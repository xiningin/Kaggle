{"cell_type":{"f540adf2":"code","41569b11":"code","2d9d2187":"code","b36b4bee":"code","d8dc96e7":"code","dd08a193":"code","ddabe34a":"code","f1aeb311":"code","35c92d7f":"code","651854e0":"code","175eeedf":"code","4a50bdb7":"code","1e9710c2":"code","b571b29d":"code","4cf41fed":"code","17685520":"code","02d51143":"code","8cff82a9":"code","83875fbd":"code","ff836a1b":"code","1b882ce8":"code","f65db0ef":"code","2589eeda":"code","32d37550":"code","26ad5820":"code","99bc5e78":"code","32a1a53e":"code","d7d23934":"code","f776dbae":"code","fb015cf3":"code","4e0961e4":"code","36c7481e":"code","5cf8cdae":"code","aedb112e":"code","0dbb81a0":"code","97bb9efa":"code","a3e23bb3":"code","a6193762":"code","6e820114":"code","5574403f":"code","51288bbd":"code","71d32567":"code","dc54ac68":"code","d7da18b0":"code","22779632":"code","a63ada58":"code","09074d87":"code","645920aa":"code","80ae956b":"code","5e1ccb28":"code","592af8db":"code","7748f18f":"code","2ac91cc1":"code","52fd325c":"code","c625fdb8":"code","e97f7126":"code","d010d0a8":"code","c8e12a06":"code","3244cb6a":"code","08a56d40":"code","10618809":"code","ca939fb3":"code","1ee76e7e":"code","fa4f5581":"code","cc30a473":"code","91c35f10":"code","d15dff7e":"code","4971389f":"code","8866ebf8":"code","542ee009":"code","a518573c":"code","f2bd4dd9":"code","613c1b06":"code","6e400d4c":"code","7d09d0c7":"code","3b885b1d":"code","e2934d15":"code","e7837eca":"code","bb352770":"code","5b81151f":"code","036c1814":"code","1636afef":"code","3f60b02a":"code","d98414e3":"code","3e56fe06":"markdown","f96772e0":"markdown","1744625c":"markdown","dd698d0a":"markdown","3a326ca8":"markdown","42ff1835":"markdown","fda446ae":"markdown","7b03b21f":"markdown","2c155ce4":"markdown","b4259ef7":"markdown","60b93cc6":"markdown","c8f75edc":"markdown","a2ddec2f":"markdown","9689c819":"markdown","8b985f8a":"markdown","1ef3971a":"markdown","07b2a195":"markdown","1b3c2c31":"markdown","3572e670":"markdown","c860142f":"markdown","58d7c472":"markdown","9421e9a3":"markdown","24736ae7":"markdown","403e2cd6":"markdown","8527f230":"markdown","3d6c2412":"markdown","877a72a3":"markdown","c229ba8d":"markdown","272db88b":"markdown","056059c4":"markdown","3c7cb45a":"markdown","ae2aec09":"markdown","abbed36f":"markdown","6f09ea4e":"markdown","c2ef8d87":"markdown","a08dcd45":"markdown","48b078d5":"markdown","18841671":"markdown"},"source":{"f540adf2":"try:\n    import mlens\nexcept ImportError:\n    !pip install mlens\n    import mlens","41569b11":"import os\nimport time\nimport pandas as pd\nimport pandas_profiling\nimport numpy as np\n\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.rcParams['figure.figsize'] = (15.0, 10.0)\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom tqdm import tqdm_notebook","2d9d2187":"# seeding\nSEED = 7\nnp.random.seed(SEED)\n\nstart = time.time()","b36b4bee":"def execution_time(start):\n    _ = time.time()\n    hours, _ = divmod(_-start, 3600)\n    minutes, seconds = divmod(_, 60)\n    print(\"Execution Time:  {:0>2} hours: {:0>2} minutes: {:05.2f} seconds\".format(int(hours),int(minutes),seconds))","d8dc96e7":"# Reading the data\ndf = pd.read_csv(\"..\/input\/pima-indians-diabetes-database\/diabetes.csv\")\ndf_name = df.columns\n\nprint('Shape of the dataframe: ', df.shape)","dd08a193":"df.head()","ddabe34a":"df.info();","f1aeb311":"df.describe()","35c92d7f":"# Basic stats\ndef basic_stats(df):\n    b = pd.DataFrame()\n    b['Missing value'] = df.isnull().sum()\n    b['N unique value'] = df.nunique()\n    b['dtype'] = df.dtypes\n    return b\n\nbasic_stats(df)","651854e0":"# plot missing values similar to missingno package\n\ndef plot_missing_values(df):\n    sns.heatmap(df.isnull().T, cbar=False)\n    \nplot_missing_values(df)","175eeedf":"# missing data replace with mode\ndef replace_missing_value(df):\n    col = df.columns\n    for i in col:\n        if df[i].isnull().sum()>0:\n            df[i].fillna(df[i].mode()[0],inplace=True)\n\n# replace_missing_value(df)","4a50bdb7":"# complete data profiling using pandas_profiling package\ndf.profile_report()","1e9710c2":"# Univariate graphs to see the distribution\ndf.hist();","b571b29d":"# Correlation Matrix\ndef correlation_matrix(df):\n    Corr = df.corr()\n\n    mask = np.zeros(Corr.shape, dtype=bool)\n    mask[np.triu_indices(len(mask))] = True\n\n    sns.heatmap(Corr, cmap = 'coolwarm', annot = True, mask = mask);\n\ncorrelation_matrix(df);","4cf41fed":"# pairplot\nsns.pairplot(df, hue=\"Outcome\", palette=\"husl\", markers=[\"o\", \"s\"], diag_kind='hist');","17685520":"# Dependent Variable Distribution\ndef dv_distribution(df, dv):\n    print(df[dv].value_counts())\n    plt.pie(df[dv].value_counts().values, labels=df[dv].value_counts().keys(), startangle=90, autopct='%.1f%%')\n    plt.title('Dependent Variable Distribution');\n\ndv_distribution(df, 'Outcome')","02d51143":"# Outliers Visualization\ndef plot_outliers(df):\n    df_name = df.columns\n    fig, axs = plt.subplots(1, len(df_name), figsize=(20, 10))\n\n    for i, col in enumerate(df_name):\n        axs[i].set_title(col)\n        axs[i].boxplot(df[col])\n    fig.suptitle('Outliers');\n\nplot_outliers(df)","8cff82a9":"# Creating Dependent and Independent variables\nX =  df[df_name[0:8]]\nY = df[df_name[8]]\n\n# Dummy Variables\n# X = pd.get_dummies(X, drop_first=True)\n# X_cv = pd.get_dummies(X_cv, drop_first=True)","83875fbd":"# Spliting data\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test =train_test_split(X, Y, test_size=0.25, random_state=0, stratify=df['Outcome'])","ff836a1b":"# convert to category\ndef category_type(df):\n    col = df.columns\n    for i in col:\n        if df[i].nunique()<=104:\n            df[i] = df[i].astype('category')\n\n#category_type(train)\n#category_type(test)","1b882ce8":"from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, make_scorer\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score\n\n# Spot-Check Algorithms (Classification)\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Spot-Check Ensemble Models (Classification)\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier, AdaBoostClassifier\n\ndef getBaselineModels():\n    models = []\n    models.append(('LR', LogisticRegression()))\n    models.append(('LDA', LinearDiscriminantAnalysis()))\n    models.append(('NB', GaussianNB()))\n    models.append(('KNN', KNeighborsClassifier()))\n    models.append(('CART', DecisionTreeClassifier()))\n    models.append(('SVM', SVC(probability=True)))\n\n    models.append(('AB', AdaBoostClassifier()))\n    models.append(('GBM', GradientBoostingClassifier()))\n    models.append(('ET', ExtraTreesClassifier()))\n    models.append(('RF', RandomForestClassifier()))\n\n    return models\n\ndef baselineModelsEval(X_train, y_train, models):\n    # Test options and evaluation metric\n    num_folds = 10\n    scoring = make_scorer(accuracy_score)\n\n    # evaluate each model in turn\n    results = {}\n    for name, model in tqdm_notebook(models):\n        kfold = StratifiedKFold(n_splits=num_folds, random_state=SEED)\n        cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n        results[name] = cv_results\n\n    return results","f65db0ef":"def scoreDataFrame(results):\n    scores = []\n    names = []\n    for k, r in results.items():\n        names.append(k)\n        scores.append(round(r.mean(),4))\n\n    scoreDataFrame = pd.DataFrame({'Model':names, 'Score': scores})\n\n    return scoreDataFrame\n\ndef plotScores(results):\n    # boxplot algorithm comparison\n    fig = plt.figure()\n    fig.suptitle('Algorithm Comparison')\n    ax = fig.add_subplot(111)\n    plt.boxplot(list(results.values()))\n    ax.set_xticklabels(list(results.keys()));","2589eeda":"models = getBaselineModels()\nresults = baselineModelsEval(X_train, y_train, models)\nplotScores(results)\nbaselineScore = scoreDataFrame(results)\nbaselineScore","32d37550":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.pipeline import Pipeline\n\ndef getScaledModel(nameOfScaler):\n    \n    if nameOfScaler == 'standard':\n        scaler = StandardScaler()\n    elif nameOfScaler =='minmax':\n        scaler = MinMaxScaler()\n\n    pipelines = []\n    pipelines.append((nameOfScaler+'LR'  , Pipeline([('Scaler', scaler),('LR'  , LogisticRegression())])))\n    pipelines.append((nameOfScaler+'LDA' , Pipeline([('Scaler', scaler),('LDA' , LinearDiscriminantAnalysis())])))\n    pipelines.append((nameOfScaler+'KNN' , Pipeline([('Scaler', scaler),('KNN' , KNeighborsClassifier())])))\n    pipelines.append((nameOfScaler+'CART', Pipeline([('Scaler', scaler),('CART', DecisionTreeClassifier())])))\n    pipelines.append((nameOfScaler+'NB'  , Pipeline([('Scaler', scaler),('NB'  , GaussianNB())])))\n    pipelines.append((nameOfScaler+'SVM' , Pipeline([('Scaler', scaler),('SVM' , SVC())])))\n    pipelines.append((nameOfScaler+'AB'  , Pipeline([('Scaler', scaler),('AB'  , AdaBoostClassifier())])))\n    pipelines.append((nameOfScaler+'GBM' , Pipeline([('Scaler', scaler),('GMB' , GradientBoostingClassifier())])  ))\n    pipelines.append((nameOfScaler+'RF'  , Pipeline([('Scaler', scaler),('RF'  , RandomForestClassifier())])))\n    pipelines.append((nameOfScaler+'ET'  , Pipeline([('Scaler', scaler),('ET'  , ExtraTreesClassifier())])))\n    \n    return pipelines","26ad5820":"#standard scaler\nmodels = getScaledModel('standard')\nresults = baselineModelsEval(X_train, y_train, models)\nplotScores(results)\nscaledScoreStandard = scoreDataFrame(results)\ncompareModels = pd.concat([baselineScore, scaledScoreStandard], axis=1)\ncompareModels","99bc5e78":"#minmax scaler\nmodels = getScaledModel('minmax')\nresults = baselineModelsEval(X_train, y_train, models)\nplotScores(results)\nscaledScoreMinMax = scoreDataFrame(results)\ncompareModels = pd.concat([baselineScore, scaledScoreStandard, scaledScoreMinMax], axis=1)\ncompareModels","32a1a53e":"df_t = df.copy()\ndf_t_name = df_t.columns","d7d23934":"def outliers(df_out, drop=False):\n    \n    #good_data = df_out.copy()\n    for nameOfFeature in df_out.columns:\n        valueOfFeature = df_out[nameOfFeature]\n        # Calculate Q1 (25th percentile of the data) for the given feature\n        Q1 = np.percentile(valueOfFeature, 25.)\n\n        # Calculate Q3 (75th percentile of the data) for the given feature\n        Q3 = np.percentile(valueOfFeature, 75.)\n\n        # Use the interquartile range to calculate an outlier step (1.5 times the interquartile range)\n        step = (Q3-Q1)*1.5\n\n        outliers = valueOfFeature[~((valueOfFeature >= Q1 - step) & (valueOfFeature <= Q3 + step))].index.tolist()\n        feature_outliers = valueOfFeature[~((valueOfFeature >= Q1 - step) & (valueOfFeature <= Q3 + step))].values\n\n        # Remove the outliers, if any were specified\n        print(\"\\n\" + \"\\u0332\".join(nameOfFeature) + \": \\n\")\n        print (\"Number of outliers (inc duplicates): {} and outliers: {}\".format(len(outliers), feature_outliers))\n\n        if drop:\n            df_out = df_out.drop(df_out.index[outliers]).reset_index(drop = True)\n            print(\"New dataset with removed outliers has shape ({}, {})\".format(*df_out.shape))\n    \n    return df_out","f776dbae":"# without drop\n_ = outliers(df_t)","fb015cf3":"# with drop\ndf_clean = outliers(df_t, drop=True)","4e0961e4":"print('df shape: {}, new df shape: {}, we lost {} rows, {}% of our data'.format(df.shape[0], df_clean.shape[0], df.shape[0]-df_clean.shape[0],\n                                                        (df.shape[0]-df_clean.shape[0])\/df.shape[0]*100))","36c7481e":"df_clean_name = df_clean.columns\n\nX_c =  df_clean[df_clean_name[0:8]]\nY_c = df_clean[df_clean_name[8]]\n\nX_train_c, X_test_c, y_train_c, y_test_c =train_test_split(X_c, Y_c, test_size=0.25, random_state=0, stratify=df_clean['Outcome'])","5cf8cdae":"models = getScaledModel('minmax')\nresults = baselineModelsEval(X_train_c, y_train_c, models)\nplotScores(results)\nscaledScoreMinMax_c = scoreDataFrame(results)\ncompareModels = pd.concat([baselineScore, scaledScoreStandard, scaledScoreMinMax, scaledScoreMinMax_c], axis=1)\ncompareModels","aedb112e":"clf = ExtraTreesClassifier(n_estimators=250, random_state=SEED)\nclf.fit(X_train_c, y_train_c);","0dbb81a0":" def featureImportance(X, y): \n    clf = ExtraTreesClassifier(n_estimators=250, random_state=SEED)\n    clf.fit(X, y)\n\n    # Plot feature importance\n    feature_importance = clf.feature_importances_\n    # make importances relative to max importance\n    feature_importance = 100.0 * (feature_importance \/ feature_importance.max())\n    sorted_idx = np.argsort(feature_importance)\n    pos = np.arange(sorted_idx.shape[0]) + .5\n    plt.figure(figsize=(5,5))\n    plt.barh(pos, feature_importance[sorted_idx], align='center')\n    plt.yticks(pos, df.columns[sorted_idx])\n    plt.xlabel('Relative Importance')\n    plt.title('Variable Importance');\n\nfeatureImportance(X_train_c, y_train_c)","97bb9efa":"import lime\nfrom lime.lime_tabular import LimeTabularExplainer","a3e23bb3":"# initialization of LIME explainer\nexplainer = LimeTabularExplainer(X_train_c.values, \n                                 mode='classification',\n                                 feature_names=X_train_c.columns,\n                                 class_names=['Diabetic', 'Not Diabetic'])","a6193762":"exp = explainer.explain_instance(X_test_c.values[0],\n                                 clf.predict_proba,\n                                 num_features=X_train_c.shape[1])\n\nexp.show_in_notebook(show_table = True)","6e820114":"import eli5\nfrom eli5.sklearn import PermutationImportance","5574403f":"eli5.show_weights(clf, feature_names=list(X_train_c.columns), top=None)","51288bbd":"eli5.show_prediction(clf, X_test_c.values[0], feature_names=list(X_train_c.columns), top=None)","71d32567":"exp = PermutationImportance(clf, random_state=0).fit(X_test_c, y_test_c)\n\neli5.show_weights(exp, feature_names=list(X_train_c.columns), top=None)","dc54ac68":"import shap\nfrom shap import TreeExplainer, KernelExplainer, LinearExplainer\nshap.initjs()","d7da18b0":"explainer = TreeExplainer(clf, X_train_c, feature_dependence='imdependent')\nshap_values = explainer.shap_values(X_test_c.values)\nshap.force_plot(explainer.expected_value[1],\n                shap_values[1],\n                X_test_c.values,\n                feature_names=X_train_c.columns)","22779632":"ssplot = shap.summary_plot(shap_values, X_test_c.values, feature_names=X_train_c.columns)","a63ada58":"# Let's check plot for 'Glucose'\n\n#shap.dependence_plot('Glucose', shap_values, X_test_c)","09074d87":"df_feature_imp = df_clean[['Glucose','BMI','Age','DiabetesPedigreeFunction','Outcome']]\ndf_feature_imp_name = df_feature_imp.columns","645920aa":"X =  df_feature_imp[df_feature_imp_name[0:df_feature_imp.shape[1]-1]]\nY = df_feature_imp[df_feature_imp_name[df_feature_imp.shape[1]-1]]\n\nX_train_im, X_test_im, y_train_im, y_test_im =train_test_split(X, Y, test_size=0.1, random_state=0,\n                                                   stratify=df_feature_imp['Outcome'])","80ae956b":"models = getScaledModel('minmax')\nresults = baselineModelsEval(X_train_im, y_train_im,models)\nplotScores(results)\nscaledScoreMinMax_im = scoreDataFrame(results)\ncompareModels = pd.concat([baselineScore,\n                           scaledScoreStandard,\n                           scaledScoreMinMax,\n                           scaledScoreMinMax_c,\n                           scaledScoreMinMax_im], axis=1)\ncompareModels","5e1ccb28":"from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom scipy.stats import uniform, randint","592af8db":"df_imp_scaled = MinMaxScaler().fit_transform(df_clean[['Glucose','BMI','Age','DiabetesPedigreeFunction','Outcome']])\ndf_imp_scaled_name = df_clean.columns\n\nX =  df_imp_scaled[:,0:4]\nY =  df_imp_scaled[:,4]\nX_train_sc, X_test_sc, y_train_sc, y_test_sc =train_test_split(X, Y, test_size=0.1, random_state=0,\n                                                   stratify=df_imp_scaled[:,4])","7748f18f":"class RandomSearch(object):\n    \n    def __init__(self,X_train,y_train,model,hyperparameters):\n        \n        self.X_train = X_train\n        self.y_train = y_train\n        self.model = model\n        self.hyperparameters = hyperparameters\n        \n    def RandomSearch(self):\n        # Create randomized search 10-fold cross validation and 100 iterations\n        cv = 10\n        clf = RandomizedSearchCV(self.model,\n                                 self.hyperparameters,\n                                 random_state=1,\n                                 n_iter=100,\n                                 cv=cv,\n                                 iid = True,\n                                 verbose=0,\n                                 n_jobs=-1,\n                                 )\n        # Fit randomized search\n        best_model = clf.fit(self.X_train, self.y_train)\n        message = (best_model.best_score_, best_model.best_params_)\n        print(\"Best: %f using %s\" % (message))\n\n        return best_model,best_model.best_params_\n    \n    def BestModelPridict(self,X_test):\n        \n        best_model,_ = self.RandomSearch()\n        pred = best_model.predict(X_test)\n        return pred","2ac91cc1":"class GridSearch(object):\n    \n    def __init__(self,X_train,y_train,model,hyperparameters):\n        \n        self.X_train = X_train\n        self.y_train = y_train\n        self.model = model\n        self.hyperparameters = hyperparameters\n        \n    def GridSearch(self):\n        # Create randomized search 10-fold cross validation and 100 iterations\n        cv = 10\n        clf = GridSearchCV(self.model,\n                                 self.hyperparameters,\n                                 cv=cv,\n                                 verbose=0,\n                                 n_jobs=-1,\n                                 )\n        # Fit randomized search\n        best_model = clf.fit(self.X_train, self.y_train)\n        message = (best_model.best_score_, best_model.best_params_)\n        print(\"Best: %f using %s\" % (message))\n\n        return best_model,best_model.best_params_\n    \n    def BestModelPridict(self,X_test):\n        \n        best_model,_ = self.GridSearch()\n        pred = best_model.predict(X_test)\n        return pred","52fd325c":"models = {LogisticRegression(): dict(C=uniform(loc=0, scale=4), penalty = ['l1', 'l2']),\n          KNeighborsClassifier(): dict(n_neighbors=[i for i in range(1, 21)]),\n          SVC(): dict(C=[0.1, 0.3, 0.5, 0.7, 0.9, 1.0, 1.3, 1.5, 1.7, 2.0], kernel=['linear', 'poly', 'rbf', 'sigmoid']),\n          DecisionTreeClassifier(): dict(max_depth=[3,None], max_features=randint(1, 4), min_samples_leaf=randint(1, 4), criterion=[\"gini\", \"entropy\"]),\n          AdaBoostClassifier(): dict(learning_rate=[.01,.05,.1,.5,1], n_estimators=[50,100,150,200,250,300]),\n          GradientBoostingClassifier(): dict(learning_rate=[.01,.05,.1,.5,1], n_estimators=[50,100,150,200,250,300]),\n          RandomForestClassifier(): dict(n_estimators=[50,100,150,200,250,300], max_depth=[5,8,15,25,30], min_samples_split=[2,5,10,15,100], min_samples_leaf = [1,2,5,10]),\n          ExtraTreesClassifier(): dict(n_estimators=[50,100,150,200,250,300], min_samples_split=[2,5,10,15,100], min_samples_leaf = [1,2,5,10])}","c625fdb8":"for model, hyperparameters in tqdm_notebook(models.items()):\n    print(\"\\u0332\".join(type(model).__name__))\n    _ = RandomSearch(X_train_sc, y_train_sc, model, hyperparameters)\n    _ = _.BestModelPridict(X_test_sc)\n    print(\"\\n\")","e97f7126":"from sklearn.ensemble import VotingClassifier\n\n# best params from parameter tuning step\nparam = {'C': 0.7678243129497218, 'penalty': 'l1'}\nmodel1 = LogisticRegression(**param)\n\nparam = {'n_neighbors': 15}\nmodel2 = KNeighborsClassifier(**param)\n\nparam = {'kernel': 'linear', 'C': 1.7}\nmodel3 = SVC(**param)\n\nparam = {'criterion': 'gini', 'max_depth': 3, 'max_features': 2, 'min_samples_leaf': 3}\nmodel4 = DecisionTreeClassifier(**param)\n\nparam = {'learning_rate': 0.05, 'n_estimators': 150}\nmodel5 = AdaBoostClassifier(**param)\n\nparam = {'learning_rate': 0.01, 'n_estimators': 100}\nmodel6 = GradientBoostingClassifier(**param)\n\nmodel7 = GaussianNB()\n\nparam = {'n_estimators': 200, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_depth': 25}\nmodel8 = RandomForestClassifier(**param)\n\nparam = {'n_estimators': 200, 'min_samples_split': 5, 'min_samples_leaf': 10}\nmodel9 = ExtraTreesClassifier(**param)\n\n\n# create the sub models\nestimators = [('LR',model1), ('KNN',model2), ('SVC',model3),\n              ('DT',model4), ('ADa',model5), ('GB',model6),\n              ('NB',model7), ('RF',model8),  ('ET',model9)]","d010d0a8":"# create the ensemble model\nkfold = StratifiedKFold(n_splits=10, random_state=SEED)\n\nensemble = VotingClassifier(estimators)\nresults = cross_val_score(ensemble, X_train_sc,y_train_sc, cv=kfold)\nvc_result = results.mean()\nprint('Accuracy on train: ',vc_result)\n\nensemble_model = ensemble.fit(X_train_sc,y_train_sc)\npred = ensemble_model.predict(X_test_sc)\nprint('Accuracy on test:' , (y_test_sc == pred).mean())","c8e12a06":"def get_models():\n    param = {'C': 0.7678243129497218, 'penalty': 'l1'}\n    model1 = LogisticRegression(**param)\n\n    param = {'n_neighbors': 15}\n    model2 = KNeighborsClassifier(**param)\n\n    param = {'kernel': 'linear', 'C': 1.7, 'probability':True}\n    model3 = SVC(**param)\n\n    param = {'criterion': 'gini', 'max_depth': 3, 'max_features': 2, 'min_samples_leaf': 3}\n    model4 = DecisionTreeClassifier(**param)\n\n    param = {'learning_rate': 0.05, 'n_estimators': 150}\n    model5 = AdaBoostClassifier(**param)\n\n    param = {'learning_rate': 0.01, 'n_estimators': 100}\n    model6 = GradientBoostingClassifier(**param)\n\n    model7 = GaussianNB()\n\n    param = {'n_estimators': 200, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_depth': 25}\n    model8 = RandomForestClassifier(**param)\n\n    param = {'n_estimators': 200, 'min_samples_split': 5, 'min_samples_leaf': 10}\n    model9 = ExtraTreesClassifier(**param)\n\n    models = {'LR':model1, 'KNN':model2, 'SVC':model3,\n              'DT':model4, 'ADa':model5, 'GB':model6,\n              'NB':model7, 'RF':model8,  'ET':model9\n              }\n\n    return models","3244cb6a":"def train_predict(model_list,xtrain, xtest, ytrain, ytest):\n    \"\"\"Fit models in list on training set and return preds\"\"\"\n    P = np.zeros((ytest.shape[0], len(model_list)))\n    P = pd.DataFrame(P)\n\n    print(\"Fitting models.\")\n    cols = list()\n    for i, (name, m) in enumerate(tqdm_notebook(models.items())):\n        m.fit(xtrain, ytrain)\n        P.iloc[:, i] = m.predict_proba(xtest)[:, 1]\n        cols.append(name)\n\n    P.columns = cols\n    return P","08a56d40":"models = get_models()\nP = train_predict(models, X_train_sc, X_test_sc, y_train_sc, y_test_sc)","10618809":"correlation_matrix(P)","ca939fb3":"correlation_matrix(P.apply(lambda predic: 1*(predic >= 0.5) - y_test_sc))","1ee76e7e":"base_learners = get_models()\n\nmeta_learner = GradientBoostingClassifier(\n    n_estimators=1000,\n    loss=\"exponential\",\n    max_features=6,\n    max_depth=3,\n    subsample=0.5,\n    learning_rate=0.001, \n    random_state=SEED\n)","fa4f5581":"from mlens.ensemble import SuperLearner\n\n# create the super learner\ndef get_super_learner():\n    ensemble = SuperLearner(scorer=accuracy_score, folds=10, random_state=SEED, verbose=True)\n    # add base models\n    ensemble.add(list(base_learners.values()), proba=True)\n    # add the meta model\n    ensemble.add_meta(meta_learner, proba=True)\n \n    return ensemble","cc30a473":"ensemble = get_super_learner()\n\n# Train the ensemble\nensemble.fit(X_train_sc, y_train_sc)\n\n# Predict the test set\np_ensemble = ensemble.predict_proba(X_test_sc)","91c35f10":"pp = []\nfor p in p_ensemble[:, 1]:\n    if p>0.5:\n        pp.append(1.)\n    else:\n        pp.append(0.)","d15dff7e":"SL_result = (y_test_sc == pp).mean()\nprint(\"Super Learner Accuracy score: %.8f\" % SL_result)","4971389f":"import joblib\n\n# Output a pickle file for the model\njoblib.dump(ensemble, 'super_learner.pkl') \n \n# Load the pickle file\nclf_load = joblib.load('super_learner.pkl')\n\n# Check that the loaded model is the same as the original\nclf_load.scorer(y_test_sc, pp) == ensemble.scorer(y_test_sc, pp)","8866ebf8":"train_df = df_clean[:500]\ntest_df = df_clean[500:]\n\ny_test = test_df['Outcome']\ntest_df.drop(columns=['Outcome'], inplace=True)\n\nfeatures = test_df.columns\ncategoricals = []","542ee009":"train_df.shape, test_df.shape","a518573c":"class Base_Model(object):\n    \n    def __init__(self, train_df, test_df, target, features, categoricals=[], n_splits=10, verbose=True):\n        self.train_df = train_df\n        self.test_df = test_df\n        self.target = target\n        self.features = features\n        self.categoricals = categoricals\n        self.n_splits = n_splits\n        self.verbose = verbose\n        self.cv = self.get_cv()\n        self.params = self.get_params()\n        self.y_pred, self.score, self.model = self.fit()\n        \n    def train_model(self, train_set, val_set):\n        raise NotImplementedError\n        \n    def get_cv(self):\n        cv = StratifiedKFold(n_splits=self.n_splits, shuffle=True, random_state=42)\n        return cv.split(self.train_df, self.train_df[self.target])\n    \n    def get_params(self):\n        raise NotImplementedError\n        \n    def convert_dataset(self, x_train, y_train, x_val, y_val):\n        raise NotImplementedError\n        \n    def convert_x(self, x):\n        return x\n        \n    def fit(self):\n        oof_pred = np.zeros((len(train_df), ))\n        y_pred = np.zeros((len(test_df), ))\n        for fold, (train_idx, val_idx) in enumerate(self.cv):\n            x_train, x_val = self.train_df[self.features].iloc[train_idx], self.train_df[self.features].iloc[val_idx]\n            y_train, y_val = self.train_df[self.target][train_idx], self.train_df[self.target][val_idx]\n            train_set, val_set = self.convert_dataset(x_train, y_train, x_val, y_val)\n            model = self.train_model(train_set, val_set)\n            conv_x_val = self.convert_x(x_val)\n            _ = np.where(model.predict(conv_x_val) > 0.5, 1, 0)\n            oof_pred[val_idx] = _.reshape(oof_pred[val_idx].shape)\n            x_test = self.convert_x(self.test_df[self.features])\n            y_pred += model.predict(x_test).reshape(y_pred.shape) \/ self.n_splits\n            print('Partial score of fold {} is: {}'.format(fold, accuracy_score(y_val, oof_pred[val_idx])))\n        loss_score = accuracy_score(self.train_df[self.target], oof_pred)\n        if self.verbose:\n            print('Our oof Accuracy is: ', loss_score)\n        return y_pred, loss_score, model","f2bd4dd9":"import lightgbm as lgb\n\nclass Lgb_Model(Base_Model):\n    \n    def train_model(self, train_set, val_set):\n        verbosity = 100 if self.verbose else 0\n        return lgb.train(self.params, train_set, valid_sets=[train_set, val_set], verbose_eval=verbosity)\n        \n    def convert_dataset(self, x_train, y_train, x_val, y_val):\n        train_set = lgb.Dataset(x_train, y_train, categorical_feature=self.categoricals)\n        val_set = lgb.Dataset(x_val, y_val, categorical_feature=self.categoricals)\n        return train_set, val_set\n        \n    def get_params(self):\n        params = {'n_estimators':10,\n                    'boosting_type': 'gbdt',\n                    'objective': 'binary',\n                    'metric': 'auc',\n                    }\n        return params","613c1b06":"lgb_model = Lgb_Model(train_df, test_df, 'Outcome', features, categoricals)","6e400d4c":"import xgboost as xgb\n\nclass Xgb_Model(Base_Model):\n    \n    def train_model(self, train_set, val_set):\n        verbosity = 0 if self.verbose else 0\n        return xgb.train(self.params, train_set, \n                         num_boost_round=5000, evals=[(train_set, 'train'), (val_set, 'val')], \n                         verbose_eval=verbosity, early_stopping_rounds=100)\n        \n    def convert_dataset(self, x_train, y_train, x_val, y_val):\n        train_set = xgb.DMatrix(x_train, y_train)\n        val_set = xgb.DMatrix(x_val, y_val)\n        return train_set, val_set\n    \n    def convert_x(self, x):\n        return xgb.DMatrix(x)\n        \n    def get_params(self):\n        params = {'colsample_bytree': 0.8,                 \n                    'learning_rate': 0.01,\n                    'max_depth': 10,\n                    'subsample': 1,\n                    'objective':'binary:hinge',\n                    'eval_metric':'auc',\n                    'min_child_weight':3,\n                    'gamma':0.25,\n                    'n_estimators':10}\n\n        return params","7d09d0c7":"xgb_model = Xgb_Model(train_df, test_df, 'Outcome', features, categoricals)","3b885b1d":"from catboost import CatBoostClassifier\n\nclass Catb_Model(Base_Model):\n    \n    def train_model(self, train_set, val_set):\n        verbosity = 0 if self.verbose else 0\n        clf = CatBoostClassifier(**self.params)\n        clf.fit(train_set['X'], train_set['y'], \n                eval_set=(val_set['X'], val_set['y']),\n                verbose=verbosity, \n                cat_features=self.categoricals)\n        return clf\n        \n    def convert_dataset(self, x_train, y_train, x_val, y_val):\n        train_set = {'X': x_train, 'y': y_train}\n        val_set = {'X': x_val, 'y': y_val}\n        return train_set, val_set\n        \n    def get_params(self):\n        params = {'loss_function': 'Logloss',\n                   'task_type': \"CPU\",\n                   'iterations': 10,\n                   'od_type': \"Iter\",\n                    'depth': 10,\n                    'colsample_bylevel': 0.5, \n                    'early_stopping_rounds': 300,\n                    'random_seed': 42,\n                    'use_best_model': True\n                    }\n        return params","e2934d15":"catb_model = Catb_Model(train_df, test_df, 'Outcome', features, categoricals)","e7837eca":"import tensorflow as tf\n\nclass Nn_Model(Base_Model):\n    \n    def __init__(self, train_df, test_df, target, features, categoricals=[], n_splits=10, verbose=True):\n        super().__init__(train_df, test_df, target, features, categoricals, n_splits, verbose)\n        \n    def train_model(self, train_set, val_set):\n        verbosity = 0 if self.verbose else 0\n        model = tf.keras.models.Sequential([\n            tf.keras.layers.Input(shape=(train_set['X'].shape[1],)),\n            tf.keras.layers.Dense(200, activation='relu'),\n            tf.keras.layers.LayerNormalization(),\n            tf.keras.layers.Dropout(0.3),\n            tf.keras.layers.Dense(100, activation='relu'),\n            tf.keras.layers.LayerNormalization(),\n            tf.keras.layers.Dropout(0.3),\n            tf.keras.layers.Dense(50, activation='relu'),\n            tf.keras.layers.LayerNormalization(),\n            tf.keras.layers.Dropout(0.3),\n            tf.keras.layers.Dense(25, activation='relu'),\n            tf.keras.layers.LayerNormalization(),\n            tf.keras.layers.Dropout(0.3),\n            tf.keras.layers.Dense(1, activation='sigmoid')\n        ])\n        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-2), loss='binary_crossentropy', metrics=['accuracy'])\n        save_best = tf.keras.callbacks.ModelCheckpoint('nn_model.w8', save_weights_only=True, save_best_only=True, verbose=0)\n        early_stop = tf.keras.callbacks.EarlyStopping(patience=20)\n        model.fit(train_set['X'], \n                train_set['y'], \n                validation_data=(val_set['X'], val_set['y']),\n                epochs=100,\n                verbose=0,\n                callbacks=[save_best, early_stop])\n        model.load_weights('nn_model.w8')\n        return model\n        \n    def convert_dataset(self, x_train, y_train, x_val, y_val):\n        train_set = {'X': x_train, 'y': y_train}\n        val_set = {'X': x_val, 'y': y_val}\n        return train_set, val_set\n        \n    def get_params(self):\n        return None","bb352770":"nn_model = Nn_Model(train_df, test_df, 'Outcome', features, categoricals)","5b81151f":"from random import choice\n\nclass Cnn_Model(Base_Model):\n    \n    def __init__(self, train_df, test_df, target, features, categoricals=[], n_splits=5, verbose=True):\n        self.create_feat_2d(features)\n        super().__init__(train_df, test_df, target, features, categoricals, n_splits, verbose)\n        \n    def create_feat_2d(self, features, n_feats_repeat=50):\n        self.n_feats = len(features)\n        self.n_feats_repeat = n_feats_repeat\n        self.mask = np.zeros((self.n_feats_repeat, self.n_feats), dtype=np.int32)\n        for i in range(self.n_feats_repeat):\n            l = list(range(self.n_feats))\n            for j in range(self.n_feats):\n                c = l.pop(choice(range(len(l))))\n                self.mask[i, j] = c\n        self.mask = tf.convert_to_tensor(self.mask)\n        print(self.mask.shape)\n       \n        \n    \n    def train_model(self, train_set, val_set):\n        verbosity = 0 if self.verbose else 0\n\n        inp = tf.keras.layers.Input(shape=(self.n_feats))\n        x = tf.keras.layers.Lambda(lambda x: tf.gather(x, self.mask, axis=1))(inp)\n        x = tf.keras.layers.Reshape((self.n_feats_repeat, self.n_feats, 1))(x)\n        x = tf.keras.layers.Conv2D(18, (50, 50), strides=50, activation='relu')(x)\n        x = tf.keras.layers.Flatten()(x)\n        x = tf.keras.layers.Dense(100, activation='relu')(x)\n        x = tf.keras.layers.LayerNormalization()(x)\n        x = tf.keras.layers.Dropout(0.3)(x)\n        x = tf.keras.layers.Dense(50, activation='relu')(x)\n        x = tf.keras.layers.LayerNormalization()(x)\n        x = tf.keras.layers.Dropout(0.3)(x)\n        out = tf.keras.layers.Dense(1)(x)\n        \n        model = tf.keras.Model(inp, out)\n    \n        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), loss='binary_crossentropy', metrics='accuracy')\n        print(model.summary())\n        save_best = tf.keras.callbacks.ModelCheckpoint('cnn_model.w8', save_weights_only=True, save_best_only=True, verbose=1)\n        early_stop = tf.keras.callbacks.EarlyStopping(patience=20)\n        model.fit(train_set['X'], \n                train_set['y'], \n                validation_data=(val_set['X'], val_set['y']),\n                epochs=100,\n                 callbacks=[save_best, early_stop])\n        model.load_weights('cnn_model.w8')\n        return model\n        \n    def convert_dataset(self, x_train, y_train, x_val, y_val):\n        train_set = {'X': x_train, 'y': y_train}\n        val_set = {'X': x_val, 'y': y_val}\n        return train_set, val_set\n        \n    def get_params(self):\n        return None","036c1814":"print(\"XGBoost Score: \", xgb_model.score)\nprint(\"LightGBM Score: \", lgb_model.score)\nprint(\"CatBoost Score: \", catb_model.score)\nprint(\"Neural Network Score: \", nn_model.score)\nprint(\"Voting Classifier Score: \", vc_result)\nprint(\"Super Learner Score: \", SL_result)","1636afef":"compareModels","3f60b02a":"#joblib.dump(xgb_model, 'xgb.pkl')\n#joblib.dump(lgb_model, 'lgb.pkl')\n#joblib.dump(catb_model, 'catb.pkl')","d98414e3":"execution_time(start)","3e56fe06":"### Scaling","f96772e0":"## Data Preprocessing and Visualization","1744625c":"## Baseline: Models Evaluation","dd698d0a":"### Stacking","3a326ca8":"#### Model evaluation on Feature Selection","42ff1835":"### LIME (Local Interpretable Model-Agnostic Explanation)","fda446ae":"### Data Preprocessing","7b03b21f":"## Save Models","2c155ce4":"<center>\n<h2> Classificaion Pipeline <\/h2>\n\n<\/center>","b4259ef7":"### LightGBM","60b93cc6":"> Local Model Interpretation","c8f75edc":"error correlations on a class prediction basis things look a bit more promising:","a2ddec2f":"### XGBoost","9689c819":"### Discriptive Statistics","8b985f8a":"> Partial Dependency Plot","1ef3971a":"> Permutation Inportance from Testing Data","07b2a195":"***`'Glucose','BMI','Age','DiabetesPedigreeFunction' columns have most effect on the data.`***","1b3c2c31":"### Grid Seach\/ Random Search","3572e670":"### ExtraTreeClassifier's feature importance","c860142f":"## Feature Engineering","58d7c472":"> **Lime - ExtraTreeClassifer**","9421e9a3":"## Parameter Tuning","24736ae7":"### Data Visualization","403e2cd6":"### SHAP (SHapley Additive exPlanations)","8527f230":"## Import Libraries","3d6c2412":"## Gradient Boosting","877a72a3":"> Local Model Interpretation","c229ba8d":"> Global Model Interpretation","272db88b":"#### Model evaluation on Cleaned Data","056059c4":"### Predict and Error Corrolation","3c7cb45a":"### CatBoost","ae2aec09":"### Removing Outlies","abbed36f":"## FeedForward Neural Network","6f09ea4e":"> Global Model Interpretation","c2ef8d87":"### VotingClassifier","a08dcd45":"## Model Interpretability (for Feature Importance\/Selection)","48b078d5":"### ELI5 (Explain Like I'm 5)","18841671":"## Ensemble Methods"}}