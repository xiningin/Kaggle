{"cell_type":{"7edb7cd6":"code","7aeb4d94":"code","5894951d":"code","ee029fbe":"code","4bcf4dda":"code","4237576f":"code","0d4ea8b0":"code","f793f96f":"code","b4de9293":"code","f1193eec":"code","dd69dbc3":"code","063135bb":"markdown","af529b16":"markdown","018650e0":"markdown","a59a716d":"markdown","813cd697":"markdown","d8c3003d":"markdown","91bd72bd":"markdown"},"source":{"7edb7cd6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7aeb4d94":"from collections import OrderedDict\nfrom fastprogress import progress_bar\nfrom pathlib import Path\nfrom sklearn.model_selection import train_test_split, ShuffleSplit\nfrom torch import nn\nfrom torch.utils.data import Dataset\nfrom torch.utils.tensorboard import SummaryWriter\n\nimport functools\nimport os\nimport pandas as pd\nimport random\nimport shutil\nimport torch\nimport torch.nn.functional as F\n\n\ntarget_cols = ['reactivity', 'deg_Mg_pH10', 'deg_pH10', 'deg_Mg_50C', 'deg_50C']\ninput_cols = ['sequence', 'structure', 'predicted_loop_type']\nerror_cols = ['reactivity_error', 'deg_error_Mg_pH10', 'deg_error_Mg_50C', 'deg_error_pH10', 'deg_error_50C']\n\ntoken_dicts = {\n    \"sequence\": {x: i for i, x in enumerate(\"ACGU\")},\n    \"structure\": {x: i for i, x in enumerate('().')},\n    \"predicted_loop_type\": {x: i for i, x in enumerate(\"BEHIMSX\")}\n}\n\n\ndef set_seed(seed: int = 42):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)","5894951d":"from sklearn.model_selection import train_test_split, ShuffleSplit\nfrom torch import nn\nfrom torch.utils.data import Dataset\n\nimport functools\n\n\nBASE_PATH = \"\/kaggle\/input\/stanford-covid-vaccine\"\nBPPS_PATH = \"..\/input\/eternafold\/bpps\/\"\nMODEL_SAVE_PATH = \"\/kaggle\/model\"\nREPLACE_PATH = \"..\/input\/eternafold\/eternafold_mfe.csv\"\n\n\ndef replace_data(train_df, test_df, replace_data_dir):\n    print(f\"using data from {replace_data_dir}\")\n\n    aux = pd.read_csv(replace_data_dir)\n    del train_df['structure']\n    del train_df['predicted_loop_type']\n    del test_df['structure']\n    del test_df['predicted_loop_type']\n    train_df = pd.merge(train_df, aux, on='id', how='left')\n    test_df = pd.merge(test_df, aux, on='id', how='left')\n    assert len(train_df) == 2400\n    assert len(test_df) == 3634\n    assert train_df['structure'].isnull().sum() == 0\n    assert train_df['predicted_loop_type'].isnull().sum() == 0\n    assert test_df['structure'].isnull().sum() == 0\n    assert test_df['predicted_loop_type'].isnull().sum() == 0\n    return train_df, test_df\n\n\ndef preprocess_inputs(df, cols):\n    return np.concatenate([preprocess_feature_col(df, col) for col in cols], axis=2)\n\n\ndef preprocess_feature_col(df, col):\n    dic = token_dicts[col]\n    dic_len = len(dic)\n    seq_length = len(df[col][0])\n    ident = np.identity(dic_len)\n    # convert to one hot\n    arr = np.array(\n        df[[col]].applymap(lambda seq: [ident[dic[x]] for x in seq]).values.tolist()\n    ).squeeze(1)\n    # shape: data_size x seq_length x dic_length\n    assert arr.shape == (len(df), seq_length, dic_len)\n    return arr\n\n\ndef preprocess(base_data, is_test=False):\n    inputs = preprocess_inputs(base_data, input_cols)\n    if is_test:\n        labels = None\n    else:\n        labels = np.array(base_data[target_cols].values.tolist()).transpose((0, 2, 1))\n        assert labels.shape[2] == len(target_cols)\n    assert inputs.shape[2] == 14\n    return inputs, labels\n\n\ndef get_bpp_feature(bpp):\n    bpp_nb_mean = 0.077522  # mean of bpps_nb across all training data\n    bpp_nb_std = 0.08914  # std of bpps_nb across all training data\n    bpp_max = bpp.max(-1)[0]\n    bpp_sum = bpp.sum(-1)\n    bpp_nb = torch.true_divide((bpp > 0).sum(dim=1), bpp.shape[1])\n    bpp_nb = torch.true_divide(bpp_nb - bpp_nb_mean, bpp_nb_std)\n    return [bpp_max.unsqueeze(2), bpp_sum.unsqueeze(2), bpp_nb.unsqueeze(2)]\n\n\n@functools.lru_cache(5000)\ndef load_from_id(id_):\n    path = Path(BPPS_PATH) \/ f\"bpps\/{id_}.npy\"\n    data = np.load(str(path))\n    return data\n\n\ndef get_distance_matrix(leng):\n    idx = np.arange(leng)\n    Ds = []\n    for i in range(len(idx)):\n        d = np.abs(idx[i] - idx)\n        Ds.append(d)\n\n    Ds = np.array(Ds) + 1\n    Ds = 1 \/ Ds\n    Ds = Ds[None, :, :]\n    Ds = np.repeat(Ds, 1, axis=0)\n\n    Dss = []\n    for i in [1, 2, 4]:\n        Dss.append(Ds ** i)\n    Ds = np.stack(Dss, axis=3)\n    print(Ds.shape)\n    return Ds\n\n\ndef get_structure_adj(df):\n    Ss = []\n    for i in range(len(df)):\n        seq_length = df[\"seq_length\"].iloc[i]\n        structure = df[\"structure\"].iloc[i]\n        sequence = df[\"sequence\"].iloc[i]\n\n        cue = []\n        a_structures = OrderedDict([\n            ((\"A\", \"U\"), np.zeros([seq_length, seq_length])),\n            ((\"C\", \"G\"), np.zeros([seq_length, seq_length])),\n            ((\"U\", \"G\"), np.zeros([seq_length, seq_length])),\n            ((\"U\", \"A\"), np.zeros([seq_length, seq_length])),\n            ((\"G\", \"C\"), np.zeros([seq_length, seq_length])),\n            ((\"G\", \"U\"), np.zeros([seq_length, seq_length])),\n        ])\n        for j in range(seq_length):\n            if structure[j] == \"(\":\n                cue.append(j)\n            elif structure[j] == \")\":\n                start = cue.pop()\n                a_structures[(sequence[start], sequence[j])][start, j] = 1\n                a_structures[(sequence[j], sequence[start])][j, start] = 1\n\n        a_strc = np.stack([a for a in a_structures.values()], axis=2)\n        a_strc = np.sum(a_strc, axis=2, keepdims=True)\n        Ss.append(a_strc)\n\n    Ss = np.array(Ss)\n    return Ss\n\n\ndef create_loader(df, batch_size=1, is_test=False):\n    features, labels = preprocess(df, is_test)\n    features_tensor = torch.from_numpy(features)\n    if labels is not None:\n        labels_tensor = torch.from_numpy(labels)\n        dataset = VacDataset(features_tensor, df, labels_tensor)\n        loader = torch.utils.data.DataLoader(dataset, batch_size, shuffle=True, drop_last=False)\n    else:\n        dataset = VacDataset(features_tensor, df, None)\n        loader = torch.utils.data.DataLoader(dataset, batch_size, shuffle=False, drop_last=False)\n    return loader\n\n\nclass VacDataset(Dataset):\n    def __init__(self, features, df, labels=None):\n        self.features = features\n        self.labels = labels\n        self.test = labels is None\n        self.ids = df[\"id\"]\n        self.score = None\n        self.structure_adj = get_structure_adj(df)\n        self.distance_matrix = get_distance_matrix(self.structure_adj.shape[1])\n        if \"score\" in df.columns:\n            self.score = df[\"score\"]\n        else:\n            df[\"score\"] = 1.0\n            self.score = df[\"score\"]\n        self.signal_to_noise = None\n        if not self.test:\n            self.signal_to_noise = df[\"signal_to_noise\"]\n            assert self.features.shape[0] == self.labels.shape[0]\n        else:\n            assert self.ids is not None\n\n    def __len__(self):\n        return len(self.features)\n\n    def __getitem__(self, index):\n        bpp = torch.from_numpy(load_from_id(self.ids[index]).copy()).float()\n        adj = self.structure_adj[index]\n        distance = self.distance_matrix[0]\n        bpp = np.concatenate([bpp[:, :, None], adj, distance], axis=2)\n        if self.test:\n            return dict(sequence=self.features[index].float(), bpp=bpp, ids=self.ids[index])\n        else:\n            return dict(sequence=self.features[index].float(), bpp=bpp,\n                        label=self.labels[index], ids=self.ids[index],\n                        signal_to_noise=self.signal_to_noise[index],\n                        score=self.score[index])\n","ee029fbe":"from torch.nn import TransformerEncoder, TransformerEncoderLayer\n\nimport math\n\n\nclass Conv1dStack(nn.Module):\n    def __init__(self, in_dim, out_dim, kernel_size=3, padding=1, dilation=1):\n        super(Conv1dStack, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv1d(in_dim, out_dim, kernel_size=kernel_size, padding=padding, dilation=dilation, bias=False),\n            nn.BatchNorm1d(out_dim),\n            nn.Dropout(0.1),\n            nn.LeakyReLU(),\n        )\n        self.res = nn.Sequential(\n            nn.Conv1d(out_dim, out_dim, kernel_size=kernel_size, padding=padding, dilation=dilation, bias=False),\n            nn.BatchNorm1d(out_dim),\n            nn.Dropout(0.1),\n            nn.LeakyReLU(),\n        )\n\n    def forward(self, x):\n        x = self.conv(x)\n        h = self.res(x)\n        return x + h\n\n\nclass Conv2dStack(nn.Module):\n    def __init__(self, in_dim, out_dim, kernel_size=3, padding=1, dilation=1):\n        super(Conv2dStack, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_dim, out_dim, kernel_size=kernel_size, padding=padding, dilation=dilation, bias=False),\n            nn.BatchNorm2d(out_dim),\n            nn.Dropout(0.1),\n            nn.LeakyReLU(),\n        )\n        self.res = nn.Sequential(\n            nn.Conv2d(out_dim, out_dim, kernel_size=kernel_size, padding=padding, dilation=dilation, bias=False),\n            nn.BatchNorm2d(out_dim),\n            nn.Dropout(0.1),\n            nn.LeakyReLU(),\n        )\n\n    def forward(self, x):\n        x = self.conv(x)\n        h = self.res(x)\n        return x + h\n\n\nclass SeqEncoder(nn.Module):\n    def __init__(self, in_dim: int):\n        super(SeqEncoder, self).__init__()\n        self.conv0 = Conv1dStack(in_dim, 128, 3, padding=1)\n        self.conv1 = Conv1dStack(128, 64, 6, padding=5, dilation=2)\n        self.conv2 = Conv1dStack(64, 32, 15, padding=7, dilation=1)\n        self.conv3 = Conv1dStack(32, 32, 30, padding=29, dilation=2)\n\n    def forward(self, x):\n        x1 = self.conv0(x)\n        x2 = self.conv1(x1)\n        x3 = self.conv2(x2)\n        x4 = self.conv3(x3)\n        x = torch.cat([x1, x2, x3, x4], dim=1)\n        # x = x.permute(0, 2, 1).contiguous()\n        # BATCH x 256 x seq_length\n        return x\n\n\nclass BppAttn(nn.Module):\n    def __init__(self, in_channel: int, out_channel: int):\n        super(BppAttn, self).__init__()\n        self.conv0 = Conv1dStack(in_channel, out_channel, 3, padding=1)\n        self.bpp_conv = Conv2dStack(5, out_channel)\n\n    def forward(self, x, bpp):\n        x = self.conv0(x)\n        bpp = self.bpp_conv(bpp)\n        # BATCH x C x SEQ x SEQ\n        # BATCH x C x SEQ\n        x = torch.matmul(bpp, x.unsqueeze(-1))\n        return x.squeeze(-1)\n\n\nclass PositionalEncoding(nn.Module):\n\n    def __init__(self, d_model, dropout=0.1, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) \/ d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0).transpose(0, 1)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        x = x + self.pe[:x.size(0), :]\n        return self.dropout(x)\n\n\nclass TransformerWrapper(nn.Module):\n    def __init__(self, dmodel=256, nhead=8, num_layers=2):\n        super(TransformerWrapper, self).__init__()\n        self.pos_encoder = PositionalEncoding(256)\n        encoder_layer = TransformerEncoderLayer(d_model=dmodel, nhead=nhead)\n        self.transformer_encoder = TransformerEncoder(encoder_layer, num_layers)\n        self.pos_emb = PositionalEncoding(dmodel)\n\n    def flatten_parameters(self):\n        pass\n\n    def forward(self, x):\n        x = x.permute((1, 0, 2)).contiguous()\n        x = self.pos_emb(x)\n        x = self.transformer_encoder(x)\n        x = x.permute((1, 0, 2)).contiguous()\n        return x, None\n\n\nclass RnnLayers(nn.Module):\n    def __init__(self, dmodel, dropout=0.3, transformer_layers: int = 2):\n        super(RnnLayers, self).__init__()\n        self.dropout = nn.Dropout(dropout)\n        self.rnn0 = TransformerWrapper(dmodel, nhead=8, num_layers=transformer_layers)\n        self.rnn1 = nn.LSTM(dmodel, dmodel \/\/ 2, batch_first=True, num_layers=1, bidirectional=True)\n        self.rnn2 = nn.GRU(dmodel, dmodel \/\/ 2, batch_first=True, num_layers=1, bidirectional=True)\n\n    def forward(self, x):\n        self.rnn0.flatten_parameters()\n        x, _ = self.rnn0(x)\n        if self.rnn1 is not None:\n            self.rnn1.flatten_parameters()\n            x = self.dropout(x)\n            x, _ = self.rnn1(x)\n        if self.rnn2 is not None:\n            self.rnn2.flatten_parameters()\n            x = self.dropout(x)\n            x, _ = self.rnn2(x)\n        return x\n\n    \nclass BaseAttnModel(nn.Module):\n    def __init__(self, transformer_layers: int = 2):\n        super(BaseAttnModel, self).__init__()\n        self.linear0 = nn.Linear(14 + 3, 1)\n        self.seq_encoder_x = SeqEncoder(18)\n        self.attn = BppAttn(256, 128)\n        self.seq_encoder_bpp = SeqEncoder(128)\n        self.seq = RnnLayers(256 * 2, dropout=0.3,\n                             transformer_layers=transformer_layers)\n\n    def forward(self, x, bpp):\n        bpp_features = get_bpp_feature(bpp[:, :, :, 0].float())\n        x = torch.cat([x] + bpp_features, dim=-1)\n        learned = self.linear0(x)\n        x = torch.cat([x, learned], dim=-1)\n        x = x.permute(0, 2, 1).contiguous().float()\n        # BATCH x 18 x seq_len\n        bpp = bpp.permute([0, 3, 1, 2]).contiguous().float()\n        # BATCH x 5 x seq_len x seq_len\n        x = self.seq_encoder_x(x)\n        # BATCH x 256 x seq_len\n        bpp = self.attn(x, bpp)\n        bpp = self.seq_encoder_bpp(bpp)\n        # BATCH x 256 x seq_len\n        x = x.permute(0, 2, 1).contiguous()\n        # BATCH x seq_len x 256\n        bpp = bpp.permute(0, 2, 1).contiguous()\n        # BATCH x seq_len x 256\n        x = torch.cat([x, bpp], dim=2)\n        # BATCH x seq_len x 512\n        x = self.seq(x)\n        return x\n\n\nclass AEModel(nn.Module):\n    def __init__(self, transformer_layers: int = 2):\n        super(AEModel, self).__init__()\n        self.seq = BaseAttnModel(transformer_layers=transformer_layers)\n        self.linear = nn.Sequential(\n            nn.Linear(256 * 2, 14),\n            nn.Sigmoid(),\n        )\n\n    def forward(self, x, bpp):\n        x = self.seq(x, bpp)\n        x = F.dropout(x, p=0.3)\n        x = self.linear(x)\n        return x\n\n\nclass FromAeModel(nn.Module):\n    def __init__(self, seq, pred_len=68, dmodel: int = 256):\n        super(FromAeModel, self).__init__()\n        self.seq = seq\n        self.pred_len = pred_len\n        self.linear = nn.Sequential(\n            nn.Linear(dmodel * 2, len(target_cols)),\n        )\n\n    def forward(self, x, bpp):\n        x = self.seq(x, bpp)\n        x = self.linear(x)\n        x = x[:, :self.pred_len]\n        return x\n","4bcf4dda":"device = torch.device('cuda')\nBATCH_SIZE = 64\nbase_train_data = pd.read_json(str(Path(BASE_PATH) \/ 'train.json'), lines=True)\nbase_test_data = pd.read_json(str(Path(BASE_PATH) \/ 'test.json'), lines=True)\nbase_train_data, base_test_data = replace_data(base_train_data, base_test_data, REPLACE_PATH)\n\npublic_df = base_test_data.query(\"seq_length == 107\").copy()\nprivate_df = base_test_data.query(\"seq_length == 130\").copy()\nprint(f\"public_df: {public_df.shape}\")\nprint(f\"private_df: {private_df.shape}\")\npublic_df = public_df.reset_index()\nprivate_df = private_df.reset_index()\n\nfeatures, _ = preprocess(base_train_data, True)\nfeatures_tensor = torch.from_numpy(features)\ndataset0 = VacDataset(features_tensor, base_train_data, None)\nfeatures, _ = preprocess(public_df, True)\nfeatures_tensor = torch.from_numpy(features)\ndataset1 = VacDataset(features_tensor, public_df, None)\nfeatures, _ = preprocess(private_df, True)\nfeatures_tensor = torch.from_numpy(features)\ndataset2 = VacDataset(features_tensor, private_df, None)\n\nloader0 = torch.utils.data.DataLoader(dataset0, BATCH_SIZE, shuffle=False, drop_last=False)\nloader1 = torch.utils.data.DataLoader(dataset1, BATCH_SIZE, shuffle=False, drop_last=False)\nloader2 = torch.utils.data.DataLoader(dataset2, BATCH_SIZE, shuffle=False, drop_last=False)","4237576f":"def learn_from_batch_ae(model, data, device):\n    seq = data[\"sequence\"].clone()\n    seq[:, :, :14] = F.dropout2d(seq[:, :, :14], p=0.3)\n    target = data[\"sequence\"][:, :, :14]\n    out = model(seq.to(device), data[\"bpp\"].to(device))\n    loss = F.binary_cross_entropy(out, target.to(device))\n    return loss\n\n\ndef train_ae(model, train_data, optimizer, lr_scheduler, epochs=10, device=\"cpu\",\n             start_epoch: int = 0, start_it: int = 0, log_path: str = \".\/logs\"):\n    print(f\"device: {device}\")\n    losses = []\n    it = start_it\n    model_save_path = Path(MODEL_SAVE_PATH)\n    start_epoch = start_epoch\n    end_epoch = start_epoch + epochs\n    min_loss = 10.0\n    min_loss_epoch = 0\n    if not model_save_path.exists():\n        model_save_path.mkdir(parents=True)\n    for epoch in progress_bar(range(start_epoch, end_epoch)):\n        print(f\"epoch: {epoch}\")\n        model.train()\n        for i, data in enumerate(train_data):\n            optimizer.zero_grad()\n            loss = learn_from_batch_ae(model, data, device)\n            loss.backward()\n            nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n            optimizer.step()\n            if lr_scheduler:\n                lr_scheduler.step()\n            loss_v = loss.item()\n            losses.append(loss_v)\n            it += 1\n        loss_m = np.mean(losses)\n        if loss_m < min_loss:\n            min_loss_epoch = epoch\n            min_loss = loss_m\n        print(f'epoch: {epoch} loss: {loss_m}')\n        losses = []\n        torch.save(optimizer.state_dict(), str(model_save_path \/ \"optimizer.pt\"))\n        torch.save(model.state_dict(), str(model_save_path \/ f\"model-{epoch}.pt\"))\n    return dict(end_epoch=end_epoch, it=it, min_loss_epoch=min_loss_epoch)\n","0d4ea8b0":"import shutil\n\n\nset_seed(123)\nshutil.rmtree(\".\/model\", True)\nshutil.rmtree(\".\/logs\", True)\nsave_path = Path(\".\/model_prediction\")\nif not save_path.exists():\n    save_path.mkdir(parents=True)\n\nlr_scheduler = None\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel = AEModel()\nmodel = model.to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\nres = dict(end_epoch=0, it=0, min_loss_epoch=0)\nepochs = [5, 5, 5, 5]\nfor e in epochs:\n    res = train_ae(model, loader0, optimizer, lr_scheduler, e, device=device,\n                   start_epoch=res[\"end_epoch\"], start_it=res[\"it\"])\n    res = train_ae(model, loader1, optimizer, lr_scheduler, e, device=device,\n                   start_epoch=res[\"end_epoch\"], start_it=res[\"it\"])\n    res = train_ae(model, loader2, optimizer, lr_scheduler, e, device=device,\n                   start_epoch=res[\"end_epoch\"], start_it=res[\"it\"])\n\nepoch = res[\"min_loss_epoch\"]\nshutil.copyfile(str(Path(MODEL_SAVE_PATH) \/ f\"model-{epoch}.pt\"), \"ae-model.pt\")","f793f96f":"def MCRMSE(y_true, y_pred):\n    colwise_mse = torch.mean(torch.square(y_true - y_pred), dim=1)\n    return torch.mean(torch.sqrt(colwise_mse), dim=1)\n\n\ndef sn_mcrmse_loss(predict, target, signal_to_noise):\n    loss = MCRMSE(target, predict)\n    weight = 0.5 * torch.log(signal_to_noise + 1.01)\n    loss = (loss * weight).mean()\n    return loss\n\n\ndef learn_from_batch(model, data, optimizer, lr_scheduler, device):\n    optimizer.zero_grad()\n    out = model(data[\"sequence\"].to(device), data[\"bpp\"].to(device))\n    signal_to_noise = data[\"signal_to_noise\"] * data[\"score\"]\n    loss = sn_mcrmse_loss(out, data[\"label\"].to(device), signal_to_noise.to(device))\n    loss.backward()\n    nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n    optimizer.step()\n    if lr_scheduler:\n        lr_scheduler.step()\n    return out, loss\n\n\ndef evaluate(model, valid_data, device):\n    model.eval()\n    loss_list = []\n    mcrmse = []\n    for i, data in enumerate(valid_data):\n        with torch.no_grad():\n            y = model(data[\"sequence\"].to(device), data[\"bpp\"].to(device))\n            mcrmse_ = MCRMSE(data[\"label\"].to(device), y)[data[\"signal_to_noise\"] > 1]\n            mcrmse.append(mcrmse_.mean().item())\n            loss = sn_mcrmse_loss(y, data[\"label\"].to(device), data[\"signal_to_noise\"].to(device))\n            loss_list.append(loss.item())\n    model.train()\n    return dict(loss=np.mean(loss_list), mcmse=np.mean(mcrmse))\n\n\ndef train(model, train_data, valid_data, optimizer, lr_scheduler, epochs=10, device=\"cpu\",\n          start_epoch: int = 0, log_path: str = \".\/logs\"):\n    print(f\"device: {device}\")\n    losses = []\n    writer = SummaryWriter(log_path)\n    it = 0\n    model_save_path = Path(MODEL_SAVE_PATH)\n    start_epoch = start_epoch\n    end_epoch = start_epoch + epochs\n    if not model_save_path.exists():\n        model_save_path.mkdir(parents=True)\n    min_eval_loss = 10.0\n    min_eval_epoch = None\n    for epoch in progress_bar(range(start_epoch, end_epoch)):\n        print(f\"epoch: {epoch}\")\n        model.train()\n        for i, data in enumerate(train_data):\n            _, loss = learn_from_batch(model, data, optimizer, lr_scheduler, device)\n            loss_v = loss.item()\n            writer.add_scalar('loss', loss_v, it)\n            losses.append(loss_v)\n            it += 1\n        print(f'epoch: {epoch} loss: {np.mean(losses)}')\n        losses = []\n\n        eval_result = evaluate(model, valid_data, device)\n        eval_loss = eval_result[\"loss\"]\n        if eval_loss <= min_eval_loss:\n            min_eval_epoch = epoch\n            min_eval_loss = eval_loss\n\n        print(f\"eval loss: {eval_loss} {eval_result['mcmse']}\")\n        writer.add_scalar(f\"evaluate\/loss\", eval_loss, epoch)\n        writer.add_scalar(f\"evaluate\/mcmse\", eval_result[\"mcmse\"], epoch)\n        model.train()\n        torch.save(optimizer.state_dict(), str(model_save_path \/ \"optimizer.pt\"))\n        torch.save(model.state_dict(), str(model_save_path \/ f\"model-{epoch}.pt\"))\n    print(f'min eval loss: {min_eval_loss} epoch {min_eval_epoch}')\n    return min_eval_epoch\n","b4de9293":"device = torch.device('cuda')\nBATCH_SIZE = 64\n#base_train_data = pd.read_json(str(Path(BASE_PATH) \/ 'train.json'), lines=True)\nsamples = base_train_data\nsave_path = Path(\".\/model_prediction\")\nif not save_path.exists():\n    save_path.mkdir(parents=True)\nshutil.rmtree(\".\/model\", True)\nshutil.rmtree(\".\/logs\", True)\nsplit = ShuffleSplit(n_splits=5, test_size=.1)\nids = samples.reset_index()[\"id\"]\nset_seed(124)\nfor fold, (train_index, test_index) in enumerate(split.split(samples)):\n    print(f\"fold: {fold}\")\n    train_df = samples.loc[train_index].reset_index()\n    val_df = samples.loc[test_index].reset_index()\n    train_loader = create_loader(train_df, BATCH_SIZE)\n    valid_loader = create_loader(val_df, BATCH_SIZE)\n    print(train_df.shape, val_df.shape)\n    ae_model = AEModel()\n    state_dict = torch.load(\".\/ae-model.pt\")\n    ae_model.load_state_dict(state_dict)\n    del state_dict\n    model = FromAeModel(ae_model.seq)\n    model = model.to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n    lr_scheduler = None\n    epoch = train(model, train_loader, valid_loader, optimizer, lr_scheduler, 200, device=device,\n                  log_path=f\"logs\/{fold}\")\n    shutil.copyfile(str(Path(MODEL_SAVE_PATH) \/ f\".\/model-{epoch}.pt\"), f\"model_prediction\/model-{fold}.pt\")\n    del model","f1193eec":"def predict_batch(model, data, device):\n    # batch x seq_len x target_size\n    with torch.no_grad():\n        pred = model(data[\"sequence\"].to(device), data[\"bpp\"].to(device))\n        pred = pred.detach().cpu().numpy()\n    return_values = []\n    ids = data[\"ids\"]\n    for idx, p in enumerate(pred):\n        id_ = ids[idx]\n        assert p.shape == (model.pred_len, len(target_cols))\n        for seqpos, val in enumerate(p):\n            assert len(val) == len(target_cols)\n            dic = {key: val for key, val in zip(target_cols, val)}\n            dic[\"id_seqpos\"] = f\"{id_}_{seqpos}\"\n            return_values.append(dic)\n    return return_values\n\n\ndef predict_data(model, loader, device, batch_size):\n    data_list = []\n    for i, data in enumerate(progress_bar(loader)):\n        data_list += predict_batch(model, data, device)\n    expected_length = model.pred_len * len(loader) * batch_size\n    assert len(data_list) == expected_length, f\"len = {len(data_list)} expected = {expected_length}\"\n    return data_list\n","dd69dbc3":"device = torch.device('cuda') if torch.cuda.is_available() else \"cpu\"\nBATCH_SIZE = 1\n#base_test_data = pd.read_json(str(Path(BASE_PATH) \/ 'test.json'), lines=True)\npublic_df = base_test_data.query(\"seq_length == 107\").copy()\nprivate_df = base_test_data.query(\"seq_length == 130\").copy()\nprint(f\"public_df: {public_df.shape}\")\nprint(f\"private_df: {private_df.shape}\")\npublic_df = public_df.reset_index()\nprivate_df = private_df.reset_index()\npub_loader = create_loader(public_df, BATCH_SIZE, is_test=True)\npri_loader = create_loader(private_df, BATCH_SIZE, is_test=True)\npred_df_list = []\nc = 0\nfor fold in range(5):\n    model_load_path = f\".\/model_prediction\/model-{fold}.pt\"\n    ae_model0 = AEModel()\n    ae_model1 = AEModel()\n    model_pub = FromAeModel(pred_len=107, seq=ae_model0.seq)\n    model_pub = model_pub.to(device)\n    model_pri = FromAeModel(pred_len=130, seq=ae_model1.seq)\n    model_pri = model_pri.to(device)\n    state_dict = torch.load(model_load_path, map_location=device)\n    model_pub.load_state_dict(state_dict)\n    model_pri.load_state_dict(state_dict)\n    del state_dict\n\n    data_list = []\n    data_list += predict_data(model_pub, pub_loader, device, BATCH_SIZE)\n    data_list += predict_data(model_pri, pri_loader, device, BATCH_SIZE)\n    pred_df = pd.DataFrame(data_list, columns=[\"id_seqpos\"] + target_cols)\n    print(pred_df.head())\n    print(pred_df.tail())\n    pred_df_list.append(pred_df)\n    c += 1\ndata_dic = dict(id_seqpos=pred_df_list[0][\"id_seqpos\"])\nfor col in target_cols:\n    vals = np.zeros(pred_df_list[0][col].shape[0])\n    for df in pred_df_list:\n        vals += df[col].values\n    data_dic[col] = vals \/ float(c)\npred_df_avg = pd.DataFrame(data_dic, columns=[\"id_seqpos\"] + target_cols)\nprint(pred_df_avg.head())\npred_df_avg.to_csv(\".\/submission.csv\", index=False)\n","063135bb":"### Model","af529b16":"This notebook is forked from:\nhttps:\/\/www.kaggle.com\/takadaat\/openvaccine-pytorch-ae-pretrain\n\nThe training data is replaced with eternafold's mfe solution","018650e0":"### Loader\n","a59a716d":"### Prediction","813cd697":"### Pretrain","d8c3003d":"### Training","91bd72bd":"create loader."}}