{"cell_type":{"139cbe1b":"code","c54d9b3d":"code","6bc48b9f":"code","d3ee3410":"code","7f3c6b1a":"code","a61addbf":"code","d4e154ee":"code","85adb0d7":"code","20f0dec2":"code","d5da332b":"code","6a4e9b52":"markdown","12e357e6":"markdown","e46f6403":"markdown","35c2001c":"markdown","0f14847b":"markdown","53bbeb79":"markdown","faec27aa":"markdown","e63b6231":"markdown","534b5fee":"markdown"},"source":{"139cbe1b":"#### This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport os\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns; sns.set()\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom matplotlib import pyplot as plt\nfrom imblearn.over_sampling import SMOTE\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.model_selection import cross_val_score\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","c54d9b3d":"# Loading credit card data from kaggle\ncc_fraud = pd.read_csv(\"\/kaggle\/input\/creditcardfraud\/creditcard.csv\")\n# show first five columns\ncc_fraud.head()","6bc48b9f":"data = cc_fraud.copy()\ndata['Text'] = [\"Non-Fruad\" if i == 0 else \"Fraud\" for i in data['Class']]\ndata = data[['Text', 'Class']].groupby('Text').count().reset_index()\n\n\nfig, ax = plt.subplots()\n\nbar_x = [1, 2]\nbar_height = data['Class']\nbar_tick_label = ['Fraud', 'Non Fraud']\nbar_label = data['Class']\n\nbar_plot = plt.bar(bar_x, bar_height, tick_label = bar_tick_label)\n\ndef autolabel(rects):\n    for idx,rect in enumerate(bar_plot):\n        height = rect.get_height()\n        ax.text(rect.get_x() + rect.get_width()\/2., 1.05*height,\n                bar_label[idx],\n                ha='center', va='bottom', rotation=0)\n\nautolabel(bar_plot)\n\nplt.ylim(0,350000)\nplt.title('Distribution of Credit Card Transactions')\nplt.show()","d3ee3410":"ax = sns.distplot(cc_fraud['Amount'])\nax.set_title(\"Distirubtion Amount Of Money Spent\")","7f3c6b1a":"cc_fraud['Amount^2'] = cc_fraud['Amount'] ** 2\ncc_fraud['Amountlog'] = np.log(cc_fraud['Amount'])\n\ncc_fraud = cc_fraud.loc[cc_fraud['Amount'] != 0]\n\nax = sns.distplot(cc_fraud['Amountlog'])\nax.set_title(\"Gaussian Distirubtion Amount Spent\")","a61addbf":"#removing outliers\ndf = cc_fraud.sample(frac = 1)\n\nfraud_df = df.loc[df['Class'] == 1]\nnon_fraud_df = df.loc[df['Class'] == 0][:2000]\n\n# print then nubmer of rows each dataset has\nprint(\"Number of Non-Fraud cases:\", len(non_fraud_df), \"Number of Fraud cases:\", len(fraud_df))\n\n# create a list of all the column names\ncolumn_names = list(fraud_df)\n\n# using the 1.5x IQR rule \nfor i in range(0, len(column_names)-1):\n    Q1 = non_fraud_df[column_names[i]].quantile(0.25)\n    Q3 = non_fraud_df[column_names[i]].quantile(0.75)\n    IQR = Q3 - Q1\n    non_fraud_df = non_fraud_df[~((non_fraud_df[column_names[i]] < (Q1 - (1.75 * IQR))) |(non_fraud_df[column_names[i]] > (Q3 + (1.75 * IQR))))]\n\n# print the number of non_fraud cases\nprint(len(fraud_df) \/ (len(non_fraud_df) + len(fraud_df)))\n\n# create a new dataframe with the same amount of fraud as non-fraud cases\nnormal_distriubted_df = pd.concat([fraud_df, non_fraud_df])\n\n# shuffle the newly created dataframe\nnew_df = normal_distriubted_df.sample(frac = 1, random_state = 32)\n\n# show number of values left\nprint(\"Number of Non-Fraud cases:\", len(non_fraud_df), \"Number of Fraud cases:\", len(fraud_df))","d4e154ee":"plt.figure(figsize=(30, 30))\nmatrix = np.triu(new_df.corr())\nsns.heatmap(new_df.corr(), annot=True, mask=matrix)","85adb0d7":"# removing the to predict value from the dataframe\nX = new_df.drop(['Class', 'Amount', 'Amount^2', 'Time'], axis = 1)\n# only select the value to predict\ny = new_df['Class']","20f0dec2":"# splitting the dataset \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 32)\n\n# chagne type from dataframe to numpy array\nX_train = X_train.values\nX_test = X_test.values\ny_train = y_train.values\ny_test = y_test.values\n\n# create a dictionary with techniques to evaluate\nclassifiers = {\n    \"LogisticRegression\": LogisticRegression(),\n    \"KNearest\": KNeighborsClassifier(),\n    \"Support Vector Classifier\": SVC(),\n    \"Decision Tree Classifier\": DecisionTreeClassifier()\n}\n\n# test techniques on train and test set using cross validation\nfor key, classifier in classifiers.items():\n    classifier.fit(X_train, y_train)\n    training_score = cross_val_score(classifier, X_train, y_train, cv = 5)\n    print(\"Classifiers: \", classifier.__class__.__name__, \"Has a training score of \", round(training_score.mean(), 2)* 100)","d5da332b":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.datasets import load_digits\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import ShuffleSplit\n\n\ndef plot_learning_curve(estimator, title, X, y, axes=None, ylim=None, cv=None,\n                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 7)):\n   \n    if axes is None:\n        _, axes = plt.subplots(1, 3, figsize=(20, 5))\n\n    axes[0].set_title(title)\n    if ylim is not None:\n        axes[0].set_ylim(*ylim)\n    axes[0].set_xlabel(\"Training examples\")\n    axes[0].set_ylabel(\"Score\")\n\n    train_sizes, train_scores, test_scores, fit_times, _ = \\\n        learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,\n                       train_sizes=train_sizes,\n                       return_times=True)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    fit_times_mean = np.mean(fit_times, axis=1)\n    fit_times_std = np.std(fit_times, axis=1)\n\n    # Plot learning curve\n    axes[0].grid()\n    axes[0].fill_between(train_sizes, train_scores_mean - train_scores_std,\n                         train_scores_mean + train_scores_std, alpha=0.1,\n                         color=\"r\")\n    axes[0].fill_between(train_sizes, test_scores_mean - test_scores_std,\n                         test_scores_mean + test_scores_std, alpha=0.1,\n                         color=\"g\")\n    axes[0].plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n                 label=\"Training score\")\n    axes[0].plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n                 label=\"Cross-validation score\")\n    axes[0].legend(loc=\"best\")\n\n    # Plot n_samples vs fit_times\n    axes[1].grid()\n    axes[1].plot(train_sizes, fit_times_mean, 'o-')\n    axes[1].fill_between(train_sizes, fit_times_mean - fit_times_std,\n                         fit_times_mean + fit_times_std, alpha=0.1)\n    axes[1].set_xlabel(\"Training examples\")\n    axes[1].set_ylabel(\"fit_times\")\n    axes[1].set_title(\"Scalability of the model\")\n\n    # Plot fit_time vs score\n    axes[2].grid()\n    axes[2].plot(fit_times_mean, test_scores_mean, 'o-')\n    axes[2].fill_between(fit_times_mean, test_scores_mean - test_scores_std,\n                         test_scores_mean + test_scores_std, alpha=0.1)\n    axes[2].set_xlabel(\"fit_times\")\n    axes[2].set_ylabel(\"Score\")\n    axes[2].set_title(\"Performance of the model\")\n\n    return plt\n\n\nfig, axes = plt.subplots(3, 2, figsize=(10, 15))\n\nX = new_df.drop('Class', axis = 1)\ny = new_df['Class']\n\ntitle = \"Logistic Regression\"\ncv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=0)\nestimator = LogisticRegression()\nplot_learning_curve(estimator, title, X, y, axes=axes[:, 0], ylim=(0.7, 1.01),\n                    cv=cv, n_jobs=4)\n\ntitle = \"Decision Tree Classifier\"\ncv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\nestimator = DecisionTreeClassifier()\nplot_learning_curve(estimator, title, X, y, axes=axes[:, 1], ylim=(0.7, 1.01),\n                    cv=cv, n_jobs=4)\n\nplt.show()","6a4e9b52":"## Loading Data","12e357e6":"### Undersampling\nIn figure two and figure three we saw that the data wasn't normally distributed. We will solve this problem by using the 1.5xIQR rule (https:\/\/en.wikipedia.org\/wiki\/Interquartile_range).","e46f6403":"**Figure 2 - Distirubtion Amount Of Money Spent** <br>\nFigure two shows that the amount of money spent per transactions is not a normal distribution but an log normal distribution.\nIn this case that means that there are many small number transationcs and a few large number transactions.","35c2001c":"<h1><center>Credit Card Fraud<\/center><\/h1>\n\n<img src=\"https:\/\/www.paymentsjournal.com\/wp-content\/uploads\/2017\/12\/Fotolia_180593142_Subscription_Monthly_M.jpg\" width=\"500\" height = \"600\">\n","0f14847b":"## Objective \nIn this notebook I'll be making use of the credit card fraud dataset that is avaiable on Kaggle (https:\/\/www.kaggle.com\/mlg-ulb\/creditcardfraud).\nThe dataset contains lables marking transactions either as a fraud case or non-fraud case.\nFurthermore, this dataset is imbalanced which means that it contains more non-fraud cases than fraud cases.\n<i><b> The objective of this notebook will therefore be to see which machine learning algorithm is best in predicting whether or not a transaction is a fraud case or not. <\/b><\/i>","53bbeb79":"# Machine Learning Techniques\nThe following machine learning techniques will be used on the undersampled dataset:\n<ol>\n    <li>LogisticRegression<\/li>\n    <li>KNearest<\/li>\n    <li>Support Vector Classifier<\/li>\n    <li>Decision Tree Classifier<\/li>\n<\/ol>\n","faec27aa":"## Splitting the data\nAs mentioned in the objective I'll be using a undersampling technique before doing a train and test split.","e63b6231":"**Figure 3 - Gaussian Distirubtion Amount Spent** <br>\nSince the distribution ofthe amount spent at first wasn't Gaussian, we transformed the data using a log function. The result, as can be seen in figure 3, is a Gaussian distribution","534b5fee":"# Exploratory Data Analysis\n**Figure 1 - Distribution of Credit Card Transactions** <br>\nAs mentioned before, the dataset is imbalenced. Namely, of the 284807 transactions only 492 (0,17%) are fraud cases."}}