{"cell_type":{"c4837352":"code","3a38e4b2":"code","c9a35b13":"code","560803d8":"code","4bee3dcd":"code","8e281615":"code","b0b42e83":"code","4a89ed3c":"code","30c3d13c":"code","b90acc2d":"code","b8c12b96":"code","9c2e9683":"code","0199dd3f":"code","e18dbaf3":"code","41de7437":"code","129aad60":"code","21242367":"code","42553aa0":"code","c7217061":"code","9bd446aa":"code","b8504e8f":"code","d5381ecf":"code","792114f9":"code","5e22ad15":"code","8d08cf9f":"code","1316b18b":"code","aab55201":"code","589a3027":"code","eab81e84":"code","caa69ae5":"code","3551e911":"code","e8b007d0":"code","23230b3f":"code","793a29b3":"code","8bcecaba":"code","3d496f75":"code","742d1329":"code","c3df00b2":"code","e40b2f37":"code","7382f317":"code","febbc1af":"code","1dc284b0":"code","4f3c138c":"markdown","58fc0f51":"markdown","26c41c3c":"markdown","de2ddc3a":"markdown","15875856":"markdown","0263a86f":"markdown","4111e8f2":"markdown","1a7068be":"markdown","8e365658":"markdown","3345d02d":"markdown","5f9ca080":"markdown","5e3dc9c6":"markdown","de7f381a":"markdown","c48a312d":"markdown","cc0e4cf3":"markdown","fdd26b11":"markdown","8efeefb3":"markdown","07ac4ee3":"markdown","a6ca4851":"markdown","3c64cccc":"markdown","cdcfa6c4":"markdown","feaf8300":"markdown","32eaab6d":"markdown","6a0ae021":"markdown","e0288a7a":"markdown","99f98f50":"markdown"},"source":{"c4837352":"!pip install venndata","3a38e4b2":"#basic library \nimport pandas as pd\nimport numpy as np\nimport math \nimport re \n\n#visualization \nimport matplotlib.pyplot as plt\nimport matplotlib\nimport matplotlib_venn as vplt\nfrom venndata import venn   \nfrom matplotlib.colors import ListedColormap\nimport seaborn as sns \nimport cufflinks as cf \n%matplotlib inline\nsns.set_style(style = 'darkgrid')\ncf.go_offline()\nsns.set_style('whitegrid')\n","c9a35b13":"#importing Data \ntrain= pd.read_csv('..\/input\/titanic\/train.csv')\ntest= pd.read_csv('..\/input\/titanic\/test.csv')\nID = test['PassengerId']\nVdf =pd.read_csv('..\/input\/titanic\/train.csv') #for visualization Purpose\ntrain.head()","560803d8":"fig,(ax1,ax2) = plt.subplots(1,2 , figsize=(15,5))\n\nsns.heatmap(train.isnull(), yticklabels = False , cmap = 'plasma', ax = ax1)\nsns.heatmap(test.isnull(), yticklabels = False , cmap = 'plasma', ax = ax2)\nprint(\"massive missing value in Cabin & test data has Missing Data in Fare\")","4bee3dcd":"Vdf\n#defining values\n\nVdf['Pclass1'] = Vdf.Pclass.apply(lambda x: 1 if x==1 else 0)\nVdf['Pclass2'] = Vdf.Pclass.apply(lambda x: 1 if x==2 else 0)\nVdf['Pclass3'] = Vdf.Pclass.apply(lambda x: 1 if x==3 else 0)\nVdf['Male'] = Vdf.Sex.apply(lambda x: 1 if x=='male' else 0)\nVdf['Female'] = Vdf.Sex.apply(lambda x: 1 if x=='female' else 0)\n#Vdf['Kids'] = Vdf.Age.apply(lambda x: 1 if x<10 else 0)\n#Vdf['Adoles'] = Vdf.Age.apply(lambda x: 1 if 11>x<10 else 0)\n#Vdf['Adults'] = Vdf.Age.apply(lambda x: 1 if 19>x<50 else 0)\n#Vdf['Elder'] = Vdf.Age.apply(lambda x: 1 if x>50 else 0)\n##-----------------------------------------------------------------\n   \n\n\ndf2 = Vdf[['Survived', 'Pclass1', 'Pclass2', 'Pclass3', 'Male', 'Female']] #,'Elder','Kids','Elder','Adoles']]\nmatplotlib.rcParams['figure.figsize'] = [10, 10]\nfineTune=False\nlabels, radii, actualOverlaps, disjointOverlaps = venn.df2areas(df2, fineTune=fineTune)\nfig, ax = venn.venn(radii, actualOverlaps, disjointOverlaps, \n                    labels=labels, labelsize='auto', \n                    cmap='viridis', fineTune=fineTune)\n\nprint('General view of survivor')","8e281615":"from IPython.display import display\nfrom PIL import Image\npath=('..\/input\/titanic-picture-ilustrive\/titanic.jpg')\ndisplay(Image.open(path))","b0b42e83":"#Survived + Survivor segmented by sex , segmented by Pclass\nfig,(ax1,ax2,ax3) = plt.subplots(1,3,figsize=(20,5))\nsns.countplot(x='Survived' , data=train, ax=ax1 ,palette=\"Dark2\")\nax1.set_title(\"Survived , 0 = Dead , 1 = Alive\")\nsns.countplot(x='Survived' , hue='Sex',data=train, ax=ax2, palette=\"Set1\")\nax2.set_title(\"Survived Segmented by Sex\")\nsns.countplot(x='Survived' , hue='Pclass',data=train, ax=ax3,palette=\"Paired\")\nax3.set_title(\"Survived Segmented by Sex\/Pclass\")","4a89ed3c":"fig,(ax1,ax2,ax3) = plt.subplots(1,3,figsize=(25,10))\nsns.countplot(x='Survived' , hue = 'Sex' , data=train[train.Age < 15], ax=ax1,palette=\"Dark2\").set_title('Age between 0-15')\nsns.countplot(x='Survived' , hue = 'Sex' , data=train[(train['Age'] > 16) & (train['Age'] < 25)], ax=ax2,palette=\"Paired\").set_title('Age between 16- 25')\nsns.countplot(x='Survived' , hue = 'Sex' , data=train[(train['Age'] > 26) & (train['Age'] < 35)], ax=ax3).set_title('Age between 26- 35')","30c3d13c":"fig,(ax1,ax2,ax3) = plt.subplots(1,3,figsize=(25,10))\nsns.countplot(x='Survived' , hue = 'Sex' , data=train[(train['Age'] > 36) & (train['Age'] < 45)], ax=ax1).set_title('Age between 36-45')\nsns.countplot(x='Survived' , hue = 'Sex' , data=train[(train['Age'] > 46) & (train['Age'] < 60)], ax=ax2,palette=\"Dark2\").set_title('Age between 46- 60')\nsns.countplot(x='Survived' , hue = 'Sex' , data=train[(train['Age'] > 61) & (train['Age'] < 80)], ax=ax3).set_title('Age between 61- 80')","b90acc2d":"#Adressing missing value in Embarked in embarked \nprint(\" Missing Values in Embarked:\",train.isnull().sum()[5])\ntrain[train['Embarked'].isna()]","b8c12b96":"#Now we have to Replace them both value \"Southampton\"\ntrain['Embarked'].fillna(\"S\", inplace = True)","9c2e9683":"train[(train.Age.isna())&(train.SibSp== 8)]","0199dd3f":"sage_f = train[(train.Age.isna())&(train.SibSp== 8)]['Name'].to_list()\ndbirth_ = [1907,1904,1895,1892,1891,1893,1897] #their ages in www.encyclopedia-titanica.org\ndbirth= 1912 - np.array(dbirth_)  #1912 Titanic accident \ndbirth","e18dbaf3":"#Diccionaty \nkeys = sage_f\nvalues = dbirth\nnew_dict = dict(zip(keys, values))\nprint(new_dict)","41de7437":"for k, v in new_dict.items():\n    train.loc[train.Name == k, 'Age'] = v\n    \nfor k, v in new_dict.items():\n    test.loc[train.Name == k, 'Age'] = v","129aad60":"sns.boxplot(x='Pclass', y='Age', data=train).set_title('Age Average per class')","21242367":"#Combining both Data set \ndata_titanic = [train,test]\n\nfor data in data_titanic: #Extracting Title \n    data['Title'] = data ['Name'].str.extract(' ([A-Za-z]+)\\.', expand = False)","42553aa0":"train.Title.value_counts().to_dict() #here I will Use Mr, Miss, Mrs, Master and the rest \"Others\"","c7217061":"tit_val = {\"Mr\": 2,\"Miss\": 1,\"Mrs\": 3,\"Master\": 0,\"Dr\": 2,\"Rev\": 2,\"Col\": 2,\"Major\": 2,\"Mlle\": 2,\"Capt\": 2,\"Jonkheer\": 2,\"Countess\": 2,\"Sir\": 2,\"Mme\": 2,\"Ms\": 1,\"Don\": 2,\"Lady\": 3}\nfor data in data_titanic:\n    data['Title'] = data ['Title'].map(tit_val)","9bd446aa":"train.head(2)","b8504e8f":"test.head(2)","d5381ecf":"#filling values\ntrain.groupby('Title')['Age'].mean().round()","792114f9":"#Now We can Fill the Age \ndef impute_age (col): \n    Age=col[0]\n    Title=col[1]\n    \n    if pd.isnull(Age):\n        \n        if Title== 0 :\n            return 5\n        elif Title == 1:\n            return 22\n        elif Title == 2:\n            return 33\n        else:\n            return 36\n    else:\n        return Age","5e22ad15":"train['Age'] = train[['Age','Title']].apply(impute_age, axis = 1)\ntest['Age'] = test[['Age','Title']].apply(impute_age, axis = 1)","8d08cf9f":"fig,(ax1) = plt.subplots(1,1 , figsize=(10,5))\nsns.countplot(x = 'Survived' , data= train , hue = 'Title', ax = ax1).set_title(\"Survivor-Dead By Title\")\nprint(\"Master: 0, Miss: 1, Mr: 2, Mrs: 3\")","1316b18b":"def cabins (col):\n    classes = col[0]\n    cabin = col[1]\n    \n    if classes == 1:\n        return 3\n    elif classes == 2:\n        return 2\n    else:\n        return 1 ","aab55201":"fig,(ax1,ax2) = plt.subplots(1,2 , figsize=(15,5))\n\nsns.heatmap(train.isnull(), yticklabels = False , cmap = 'plasma', ax = ax1)\nsns.heatmap(test.isnull(), yticklabels = False , cmap = 'plasma', ax = ax2)\nprint(\"No Missing Values\")","589a3027":"p_data= train  # I will save train as p_Data because i will try so many different algorithms ","eab81e84":"from sklearn.compose import ColumnTransformer, make_column_transformer\nfrom sklearn.preprocessing import OneHotEncoder,LabelEncoder,LabelBinarizer\n\n#Missing Values\n#print(p_data.isnull().sum())\np_data['Embarked'].fillna('S', inplace= True)\n\n#Step_1 Combine SibSp & Parch\np_data['F_A'] = p_data['SibSp']+p_data['Parch']\np_data['F_A'] =p_data.F_A.apply(lambda x :2 if x>0 else 1)\n\n#Step_2 Transform Cabin\np_data['Cabin_'] =p_data[['Pclass','Cabin']].apply(cabins, axis=1)\n\n#Step 3 Encoding Sex\nsex = pd.get_dummies(p_data['Sex'],drop_first= True)\np_data = pd.concat([p_data,sex],axis = 1)\n\n#step_4 Drop columns\np_data.drop(['PassengerId','Pclass','Name','SibSp','Parch','Ticket','Cabin','Sex'], axis = 1 , inplace = True)\np_data\n\n#Step 5 Onehot\nohc = OneHotEncoder()\nXtest= p_data.iloc[:,3].values\nXtest = Xtest.reshape(-1,1)\nEMB = ohc.fit_transform(Xtest).toarray()\nEB= pd.DataFrame(EMB,columns = ['S','C','Q'])\np_data = pd.concat([p_data ,EB], axis = 1)\np_data.drop('Embarked', axis = 1 , inplace = True)\np_data","caa69ae5":"#Missing Values\n#print(p_data.isnull().sum())\ntest['Title'].fillna(1, inplace= True)\ntest['Fare'].fillna(35.6271, inplace= True)\n\n#Step_1 Combine SibSp & Parch\ntest['F_A'] = test['SibSp']+test['Parch']\ntest['F_A'] =test.F_A.apply(lambda x :2 if x>0 else 1)\n\n#Step_2 Transform Cabin\ntest['Cabin_'] =test[['Pclass','Cabin']].apply(cabins, axis=1)\n\n#Step 3 Encoding Sex\nsex = pd.get_dummies(test['Sex'],drop_first= True)\ntest = pd.concat([test,sex],axis = 1)\n\n#step_4 Drop columns\ntest.drop(['PassengerId','Pclass','Name','SibSp','Parch','Ticket','Cabin','Sex'], axis = 1 , inplace = True)\n\n#Step 5 Change into Categorical \nohc = OneHotEncoder()\nXohc= test.iloc[:,2].values\nXohc= Xohc.reshape(-1,1)\nEMB = ohc.fit_transform(Xohc).toarray()\nEB= pd.DataFrame(EMB,columns = ['S','C','Q'])\ntest = pd.concat([test ,EB], axis = 1)\ntest.drop('Embarked', axis = 1 , inplace = True)\ntest","3551e911":"test_1 = test","e8b007d0":"from sklearn.model_selection import train_test_split,cross_val_score,cross_val_predict,cross_validate\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import *\n\n#Testing\ndata_1 = p_data  # <----- Standar Scaler\ndata_2 = p_data  # <----- MinMax\ndata_3 = p_data  # <----- \n\n\n#Difining X\nX_1=data_1.drop('Survived',axis = 1)\nX_2=data_2.drop('Survived',axis = 1)\nX_3=data_3.drop('Survived',axis = 1)\n\ny=p_data['Survived']\n\n#Splitting\nX_train, X_test, y_train, y_test = train_test_split(X_1, y, test_size=0.2, random_state=42,stratify=data_2['Cabin_'])\nX_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X_2, y, test_size=0.2, random_state=42,stratify=data_2['Cabin_'])\n\n\n# MinMax\nMX= MinMaxScaler()\nX_1 = MX.fit_transform(X_1)\nX_train = MX.fit_transform(X_train)\nX_test = MX.fit_transform(X_test)\ntest_MX = MX.fit_transform(test_1)\n\n# Standar Scalar \nSC = StandardScaler()\nX_2 = SC.fit_transform(X_2)\nX_train_2 = SC.fit_transform(X_train_2)\nX_test_2 = SC.fit_transform(X_test_2)\ntest_SC = SC.fit_transform(test_1)\n\nprint(\"Segmenting Data ....... into Two sets ....Done...!!  \")\nprint(\"Splitting Data into X_train and X_test... Done..!!  \")\nprint(\"Applying MinMax Scaler & Standard Scaler .... Done..!!\")","23230b3f":"#General Fuction to evaluate Model \n\ndef Evaluating (model,X,y,CV, criteria=True):\n    if criteria :\n        score = cross_val_score(model,X=X ,y=y ,cv=CV, scoring='accuracy', n_jobs= 4)\n        score = np.mean(score)\n        accuracy.append(score)\n    \n    else:\n        pred = cross_val_predict(model,X=X ,y=y ,cv=CV, n_jobs= 4)\n        prediction.append(pred)\n        \nprint(\"Creating General fuction to Evaluate Algorithms.....Done\")","793a29b3":"#Machine Learning\nfrom sklearn.linear_model import LogisticRegression\nimport xgboost as xgb\nfrom sklearn import svm\nfrom sklearn.ensemble import RandomForestClassifier,BaggingClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\n\nLGR = LogisticRegression()\n\nXG = xgb.XGBRFClassifier(base_score=0.5, colsample_bylevel=1, colsample_bynode=0.8,\n                colsample_bytree=0.9539552926340813, gamma=0.08955017265494192,\n                learning_rate=0.07004776526310222, max_delta_step=0,\n                max_depth=24, min_child_weight=1.667528606432285, missing=None,\n                n_estimators=230, n_jobs=1, nthread=None,\n                objective='binary:logistic', random_state=0, reg_alpha=0,\n                reg_lambda=1, scale_pos_weight=1, seed=None, silent=None,\n                subsample=0.9219040847026176, verbosity=1)\nSVM_1= svm.SVC(probability = True)\n\nSVM_2= svm.SVC()\n\nRF = RandomForestClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,\n                       criterion='gini', max_depth=10, max_features='auto',\n                       max_leaf_nodes=None, max_samples=None,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=4, min_samples_split=2,\n                       min_weight_fraction_leaf=0.0, n_estimators=600,\n                       n_jobs=None, oob_score=False, random_state=None,\n                       verbose=0, warm_start=False)\nBG = BaggingClassifier(RF)\nNB = GaussianNB()\n\naccuracy = []\nprediction =[]\n\nfor i in [LGR, XG, SVM_1, SVM_2,  RF,BG, NB]:\n      Evaluating(i,X_1,y,CV=5)        \n\nprint(\"Processing....... Done..... \")\nprint(\"Using Cross_Validation and MinMax Scaler with 5 Folds ......Check the Accuracy Below!!\")\n\nMINMAX = pd.DataFrame(accuracy, index = ['LGR','XG','SVM_1','SVM_2','RF','BG','NB'], columns = ['MINMAX_Accuracy'])","8bcecaba":"accuracy = []\nprediction =[]\n\nfor i in [LGR,XG, SVM_1, SVM_2,  RF, BG, NB]:\n      Evaluating(i,X_2,y,CV=5)\n\n\nprint(\"Processing....... Done..... \")\nprint(\"Using Cross_Validation and Standard Scaler with 5 Folds ......Check the Accuracy Below!!\")\nSCALER = pd.DataFrame(accuracy, index = ['LGR','XG','SVM_1','SVM_2','RF','BG','NB'], columns = ['SC_Accuracy'])\n\nprint(\"It seems no to have a significant change no matter what approach I use\")\n\npd.concat([MINMAX,SCALER],axis = 1)","3d496f75":"from sklearn.metrics import plot_precision_recall_curve\n\nfor i in [XG, SVM_1, SVM_2,  RF, BG, NB]:\n    i.fit(X_2,y)\n\nfig,axs= plt.subplots(2,2, figsize = (20,15))\nplot_precision_recall_curve(XG,X_2,y,ax=axs[0,0])\nplot_precision_recall_curve(SVM_1,X_2,y,ax=axs[0,1])\nplot_precision_recall_curve(BG,X_2,y,ax=axs[1,0])\nplot_precision_recall_curve(RF,X_2,y,ax=axs[1,1])\nplt.show(\"Precision VS ReCall\")","742d1329":"from sklearn.ensemble import VotingClassifier\nprint(\"Emsembling models..... RandomForest, Support Vector Machine, etc, ........\\n\")\nmodelos = [('RandomForest', RF),('BG',BG), ('SVM_1',SVM_1),('XGboost',XG)]\n\nthreshold_1 = 0.8\nthreshold_2 = 0.7\n\nVC= VotingClassifier(estimators = modelos,voting='soft',n_jobs=3)\nVC.fit(X_train_2,y_train_2)\n\nfor i in [RF, BG , SVM_1, XG,VC]:\n    \n    if i == RF:\n        i.fit(X_train_2,y_train_2)\n        predictions = i.predict_proba(X_test_2)\n        y_pred = [1 if predictions[i][1]>threshold_1 else 0  for i in range(len(predictions))]\n        \n        print(i.__class__.__name__,accuracy_score(y_test_2,y_pred))\n        \n    elif i == BG:\n        i.fit(X_train_2,y_train_2)\n        predictions = i.predict_proba(X_test_2)\n        y_pred = [1 if predictions[i][1]>threshold_2 else 0  for i in range(len(predictions))]\n        \n        print(i.__class__.__name__,accuracy_score(y_test_2,y_pred))\n        \n    else:\n        i.fit(X_train_2,y_train_2)\n        predictions = i.predict(X_test_2)\n        print(i.__class__.__name__,accuracy_score(y_test_2,y_pred))\n    \n\nprint(\"\\nTesting Vagging Classifier .... Applying Cross Validation.....!!\")\nscores = cross_val_score(estimator=VC,X=X_2,y=y,cv=5,scoring='accuracy')\nprint(\"Mean Score =\", np.mean(scores))","c3df00b2":"from sklearn.ensemble import VotingClassifier\nprint(\"Emsembling models..... RandomForest, Support Vector Machine, etc, ........\\n\")\nmodelos = [('RandomForest', RF),('BG',BG), ('SVM_1',SVM_1),('XGboost',XG)]\n\nthreshold_1 = 0.8\nthreshold_2 = 0.8\n\nVC= VotingClassifier(estimators = modelos,voting='hard',n_jobs=3 , weights = [2,4,4,4])\nVC.fit(X_train_2,y_train_2)\n\nfor i in [RF, BG , SVM_1, XG,VC]:\n    \n    if i == RF:\n        i.fit(X_train_2,y_train_2)\n        predictions = i.predict_proba(X_test_2)\n        y_pred = [1 if predictions[i][1]>threshold_1 else 0  for i in range(len(predictions))]\n        \n        print(i.__class__.__name__,accuracy_score(y_test_2,y_pred))\n        \n    elif i == BG:\n        i.fit(X_train_2,y_train_2)\n        predictions = i.predict_proba(X_test_2)\n        y_pred = [1 if predictions[i][1]>threshold_2 else 0  for i in range(len(predictions))]\n        \n        print(i.__class__.__name__,accuracy_score(y_test_2,y_pred))\n        \n    else:\n        i.fit(X_train_2,y_train_2)\n        predictions = i.predict(X_test_2)\n        print(i.__class__.__name__,accuracy_score(y_test_2,y_pred))\n    \n\nprint(\"\\nTesting Vagging Classifier .... Applying Cross Validation.....!!\")\nscores = cross_val_score(estimator=VC,X=X_2,y=y,cv=5,scoring='accuracy')\nprint(\"Mean Score =\", np.mean(scores))","e40b2f37":"# Prediction\ny_pred = VC.predict(test_SC)\n\nsub = pd.DataFrame()\nsub['PassengerId'] = ID\nsub['Survived'] = y_pred\nsub.to_csv('VC_prediction_1.csv', index=False)\nprint(\"Predicting .......! \")\nprint(\"Submission has been saved\")\nprint(\"Accurancy up to 0.77990\")","7382f317":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential \nfrom tensorflow.keras.layers import Dense,Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.optimizers import Adam\n\n#Early Stop\nearly_stop = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=25, \n        verbose=1, mode='auto', restore_best_weights=True)\n\nNN= Sequential()\n\nNN.add(Dense(9,activation = 'relu',input_shape=[9 ,]))\nNN.add(Dense(5,activation ='relu'))\nNN.add(Dropout(0.3))\nNN.add(Dense(2,activation ='relu'))\nNN.add(Dense(1,activation='sigmoid'))\nNN.compile(optimizer='adam', loss= 'binary_crossentropy', metrics = ['accuracy'] )\n\nNN.fit(x=X_train_2, y=y_train_2,validation_data = (X_test_2, y_test_2) , epochs=600,batch_size=700 ,verbose = 0)\n\n","febbc1af":"pd.DataFrame(NN.history.history).plot()\nplt.show()","1dc284b0":"threshold = 0.6\n\npredictions = NN.predict_proba(test_SC)\ny_pred = [1 if predictions[i]>threshold else 0  for i in range(len(predictions))]\n\nsub = pd.DataFrame()\nsub['PassengerId'] = ID\nsub['Survived'] = y_pred\nsub.to_csv('NN_predic_0.6 prediction_500.csv', index=False)\nprint(\"Predicting 0.7829 Accuracy.......! \")\nprint(\"Submission has been saved\")","4f3c138c":"## Age","58fc0f51":"# Machine Learning","26c41c3c":"### Test Part","de2ddc3a":"## Baseline","15875856":"**Replacing Ages\nTo fill the missing values in Age , I can easily find the average age and then apply it to all of the missing value , but instead I will filter by the Mr, Mrs , Miss, Master by doing this will be more accurate**","0263a86f":"Finding Missing Values in Embarked\n\n","4111e8f2":"## Cabin","1a7068be":"## Applying MinMax scaler and Standar Scaler ","8e365658":"### Test Part","3345d02d":"# *I know there are missing values , but in order to better understand the data I create some segmentation and them I will compare them with the new graph without NAN*","5f9ca080":"### Hard","5e3dc9c6":"# II. OBTAINING Data","de7f381a":"# From this picture I can asume \n1. People in third class has less chances to survive\n2. I get many insight and guidelines to make sure I am doing ok \n3. Any feedback it is welcome ","c48a312d":"> ## Emsembling","cc0e4cf3":"** In here i can find a balance between Recall and Precision , actually with a threshold of 0.7 using Randon Forest i got 0. 78 in the Leader board\"**","fdd26b11":"# V. Modeling ","8efeefb3":"# I. Introduction \n\n> Hi There.. Im  kind of new in Kaggle and Python and following other people recommendation I got the titanic Data set \n> It was the First Data set that I analyzed. please feel free to leave any feedback .. and thanks in advance","07ac4ee3":"**Would be easier fill the issing data with S because is the place where most of people were coming from , but since they are only two I looked for the info : Miss. Amelie boarded the Titanic at Southampton as maid to Mrs George Nelson Stone. She travelled on Mrs Stone's ticket (#113572). Mrs Stone boarded the Titanic in Southampton on 10 April 1912 and was travelling in first class with her maid Amelie Icard. She occupied cabin B-28. I found that info in www.encyclopedia-titanica.org**","a6ca4851":"# -----------------Final Thoughts-----------------\n1. Thanks for passing by .. I added Voting classifier, SVM , Xgossbost , And Neural Network , however They all have the same output 0.79 Score \n2. We can see here that the way we process data it is more important that the model that we use\n3. I Will keep Working on this but please feel free to leave some comments ","3c64cccc":"**> Filling Cabin Information : Cabin could be a great parameter but it has too many missing value, however I can notice the distribution was \"first class had the top decks (A-E)\",\"second class (D-F)\", and \"third class (E-G)\" In the image (on the notebook) , I can notice that, 3rd class was in the fron\/back , 2nd class was in the middle , 1st class on the top\nSo In think that Cabit and pclass are related if we change cabin to 1 2 3 we will have the same results**","cdcfa6c4":"## Precision Recall ","feaf8300":"# VI. Deep Learning","32eaab6d":"### Soft","6a0ae021":"# III. Exploring Data , visualization","e0288a7a":"# IV. SCRUB , Missing values - Age - Cabin , etc","99f98f50":"**We know some people was tarveling with their family, in titanic.org the information has been collected by famiy so i will do my best to filter those family that have 7 or 8 member and find their ages\n* 0    608 # traveling alone \n* 1    209 \n* 2     28\n* 4     18\n* 3     16 # 16 Family with 3 members\n* 8      7 # 7 Family with 8 members\n* 5      5**"}}