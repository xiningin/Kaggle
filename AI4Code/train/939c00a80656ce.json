{"cell_type":{"cfa980c7":"code","5ab0155b":"code","21af5935":"code","bef817e2":"code","5fc7af32":"code","834b2105":"code","33f7da0c":"code","da6ffd79":"code","6320d904":"code","cb21901d":"code","d771096a":"code","78106684":"code","f9ebdd16":"code","bd65bcfa":"code","25b2f3a7":"code","f47d6b8f":"code","55960905":"code","f969479e":"code","882334f3":"code","1963cc06":"code","e712a57a":"code","56ddc991":"code","4f50a4a3":"markdown","460e08b6":"markdown","f63861c7":"markdown","b539ceac":"markdown","cb2922dc":"markdown","70580835":"markdown","c0f40d3e":"markdown","1139c57f":"markdown","2ea0b419":"markdown","20726c68":"markdown","310a603d":"markdown","69c216f6":"markdown","e1a473d1":"markdown","d75662d3":"markdown"},"source":{"cfa980c7":"import pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom random import randint\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import f1_score\nimport itertools\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'","5ab0155b":"df=pd.read_csv(r'..\/input\/creditcardfraud\/creditcard.csv')","21af5935":"y=df.pop('Class')\ndf.pop('Time')\nx=df.values\ny=np.array(y)\ncounter=(np.count_nonzero(y))\nprint(\"Amount of data that is not fraudulent \" + str(len(y)-counter))\nprint(\"Amount that indeed is fraudulent \" + str(counter))\nprint(\"Percentage of non fraud \" +str(100*(1-(counter\/len(y)))))\nprint(\"Percentage of fraud \"+str(100*((counter\/len(y)))))","bef817e2":"x_stand=[]\ny_stand=[]\nfor i in range(len(y)):\n    if y[i]==1:\n        x_stand.append(x[i])\n        y_stand.append(1)\nfor r in range(2000):\n    i=randint(0,len(y)-1)\n    if y[i]==0:\n        y_stand.append(0)\n        x_stand.append(x[i])\n\nx_stand=np.array(x_stand)\ny_stand=np.array(y_stand)\n","5fc7af32":"epochs=90\nmodel = keras.Sequential([\n    keras.layers.Dense(64,input_shape=(29,),activation='relu'),\n    keras.layers.Dense(128, activation='relu'),\n    keras.layers.Dense(64,activation='relu'),\n    keras.layers.Dense(2, activation='softmax')\n])\nprint(model.summary())","834b2105":"model.compile(optimizer='Adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n\nhistorico=model.fit(x_stand, y_stand, epochs=epochs,validation_split=0.1,verbose=0)\n","33f7da0c":"val=historico.history\nplt.figure(0)\nplt.plot(np.arange(1,len(val['accuracy'])+1),val['accuracy'],label='acc')\nplt.plot(np.arange(1,len(val['val_accuracy'])+1),val['val_accuracy'],label='val_acc')\nplt.legend()\nplt.show()\nplt.figure(1)\nplt.plot(np.arange(1,len(val['val_loss'])+1),val['val_loss'],label='val_loss')\nplt.plot(np.arange(1,len(val['val_loss'])+1),val['loss'],label='loss')\nplt.legend()\nplt.show()","da6ffd79":"def plot_confusion_matrix(cm, classes,\n                        normalize=False,\n                        title='Confusion matrix',\n                        cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n            horizontalalignment=\"center\",\n            color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","6320d904":"x_stand=[]\ny_stand=[]\nfor i in range(len(y)):\n    if y[i]==1:\n        x_stand.append(x[i])\n        y_stand.append(1)\nfor r in range(508):\n    i=randint(0,len(y)-1)\n    if y[i]==0:\n        y_stand.append(0)\n        x_stand.append(x[i])\n\nx_stand=np.array(x_stand)\ny_stand=np.array(y_stand)\n\ncomp=model.predict(x_stand)\ncomp=np.array([np.argmax(u) for u in comp])\ncm = confusion_matrix(y_true=y_stand, y_pred=comp)\nplot_confusion_matrix(cm=cm,classes=['Non Fraudulent','Fraudulent'],title='Confusion Matrix')\nplt.show()","cb21901d":"print(\"TPR (1000 random samples in under-sampling)\")\nprint(cm[0][0]\/(cm[0][0]+cm[1][0]))\nprint(\"TNR (1000 random samples in under-sampling)\")\nprint(cm[0][1]\/(cm[0][1]+cm[1][1]))","d771096a":"plt.figure(figsize=(12,6))\ncm = confusion_matrix(y_true=y_stand, y_pred=comp)\nplot_confusion_matrix(cm=cm,classes=['Non Fraudulent','Fraudulent'],title='Confusion Matrix',normalize=True)","78106684":"cm = confusion_matrix(y_true=np.array(y), y_pred=np.array([np.argmax(u)for u in model.predict(x)]))\nplot_confusion_matrix(cm=cm,classes=['Non Fraudulent','Fraudulent'],title='Confusion Matrix')\nplt.show()\nplt.figure(figsize=(12,6))\nplot_confusion_matrix(cm=cm,classes=['Non Fraudulent','Fraudulent'],title='Confusion Matrix',normalize=True)","f9ebdd16":"print(\"TPR (All data)\")\nprint(cm[0][0]\/(cm[0][0]+cm[1][0]))\nprint(\"TNR (All data)\")\nprint(cm[0][1]\/(cm[0][1]+cm[1][1]))","bd65bcfa":"from imblearn.over_sampling import RandomOverSampler","25b2f3a7":"oversample = RandomOverSampler(sampling_strategy='minority')\nx_over, y_over = oversample.fit_resample(x, y)\ncounter=(np.count_nonzero(y_over))\nprint(\"Amount of data that is not fraudulent \" + str(len(y_over)-counter))\nprint(\"Amount that indeed is fraudulent \" + str(counter))\nprint(\"Percentage of non fraud \" +str(100*(1-(counter\/len(y_over)))))\nprint(\"Percentage of fraud \"+str(100*((counter\/len(y_over)))))","f47d6b8f":"epochs=20\nmodel = keras.Sequential([\n    keras.layers.Dense(64,input_shape=(29,),activation='relu'),\n    keras.layers.Dense(128, activation='relu'),\n    keras.layers.Dense(64,activation='relu'),\n    keras.layers.Dense(2, activation='softmax')\n])","55960905":"model.compile(optimizer='Adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n\nhistorico=model.fit(x_over, y_over, epochs=epochs,validation_split=0.25,verbose=1)\n","f969479e":"val=historico.history\nplt.figure(0)\nplt.plot(np.arange(1,len(val['accuracy'])+1),val['accuracy'],label='acc')\nplt.plot(np.arange(1,len(val['val_accuracy'])+1),val['val_accuracy'],label='val_acc')\nplt.legend()\nplt.show()\nplt.figure(1)\nplt.plot(np.arange(1,len(val['val_loss'])+1),val['val_loss'],label='val_loss')\nplt.plot(np.arange(1,len(val['val_loss'])+1),val['loss'],label='loss')\nplt.legend()\nplt.show()","882334f3":"comp=model.predict(x_over)\ncomp=np.array([np.argmax(u) for u in comp])\ncm = confusion_matrix(y_true=y_over, y_pred=comp)\nplot_confusion_matrix(cm=cm,classes=['Non Fraudulent','Fraudulent'],title='Confusion Matrix')\nplt.show()\nplt.figure(figsize=(12,6))\ncm = confusion_matrix(y_true=y_over, y_pred=comp)\nplot_confusion_matrix(cm=cm,classes=['Non Fraudulent','Fraudulent'],title='Confusion Matrix',normalize=True)","1963cc06":"print(\"TPR (Oversampling)\")\nprint(cm[0][0]\/(cm[0][0]+cm[1][0]))\nprint(\"TNR (Oversampling)\")\nprint(cm[0][1]\/(cm[0][1]+cm[1][1]))","e712a57a":"cm = confusion_matrix(y_true=np.array(y), y_pred=np.array([np.argmax(u)for u in model.predict(x)]))\nplot_confusion_matrix(cm=cm,classes=['Non Fraudulent','Fraudulent'],title='Confusion Matrix')\nplt.show()\nplt.figure(figsize=(12,6))\nplot_confusion_matrix(cm=cm,classes=['Non Fraudulent','Fraudulent'],title='Confusion Matrix',normalize=True)","56ddc991":"print(\"TPR All data Model oversampling\")\nprint(cm[0][0]\/(cm[0][0]+cm[1][0]))\nprint(\"TNR All data Model oversampling\")\nprint(cm[0][1]\/(cm[0][1]+cm[1][1]))","4f50a4a3":"The function above was taken out of the site: https:\/\/deeplizard.com\/learn\/video\/km7pxKy4UHU","460e08b6":"Now we can see that if our model only predicts non fraudulent transactions it would obtain a 99.82 % of precision and that's not what we want. We want to be able to analize the features and then be able to make assumptions about the nature of the transaction.\n\nHow can we solve this problem, the answer randomization. We are going to build a dataset that consists of mostly half fraudulent and half non fraudulent values being the half non fraudulent randomized. \n\nfor this we are going to build the code below.","f63861c7":"# Conclusion \n\nBoth our models got over 90% accuracy but the loss function is not showing such great results mainly on the validation set wich may mean that the model is overfitting and needs improvement  \n\nLooking at accuracy may be not a good way of analysing our results and the main problem with or model maybe the proportion of false positives is not good when the data get scaled.\nThis problem may be happening because of the strategy of the training wich consists of a huge under-smapling \n\nWhen we build a oversampled dataset we can see that the results improve in relation to the FP wich means that maybe oversampling may be the best strategy for this problem. But the results still needs improving.  \n\nThe main problem with our model is the prediction of non fraudulent transactions as fraudulent showing that it needs improvement focused on this area. It does get clear that this is the case by looking at the confusion matrix, such of the whole data as of the smaller group of 1000 samples. \n**Thank you for the attention**","b539ceac":"# Importing the libraries\n","cb2922dc":"Now we can see by the confusion matrix that our model in a dataset with 1000 smaples with close to 50% beeing fraudulent  samples it predicted a precision close to 96% on randomized data.","70580835":"Apresenting the results and the generalization of the network","c0f40d3e":"We can now getting this results calculate the TPR(True positive rate) of the random set and \nTNR(True negative rate)\n\nTPR=TP\/P=TP\/TP+FN\n\nTNR=TN\/N=TN\/TN+FP","1139c57f":"# Parsing the Data and analising our naiveness\n\n At the beginning of tackling this problem i tougth using the whole data because the more data the better right?\n Wrong and i am going to show you why.\n Let's see how much of the data represents a fraudulent purchase","2ea0b419":"# Trying Oversampling ","20726c68":"# Objectives \n\n* Build a neural net with a result with accuracy bigger than 90%\n* We will use TensorFlow ith keras to build our neural net","310a603d":"# Now finally a neural network\n\nWe are going to build the architecture of our neural net.","69c216f6":"Compiling the model","e1a473d1":"# Building the Dataframe","d75662d3":"Now lets try with the whole data"}}