{"cell_type":{"54d7f418":"code","3b05edb2":"code","4124a732":"code","9ea976a2":"code","a522fa12":"code","f183d60a":"code","378e862d":"code","189a9b94":"code","e1354fe9":"code","c7878be3":"code","c755efd1":"code","45a5e70f":"code","189eca8d":"code","a9a66eea":"code","e4c5f793":"code","b6a601e6":"code","a5146fc9":"code","2cf8ef3d":"code","5e5e893e":"code","1da85d60":"code","74995512":"code","7c323ddb":"code","f34bcb27":"code","5cd4841d":"code","71912813":"code","f255b17e":"code","8a966c43":"code","d3ca55b4":"code","b1c1627d":"code","c308b7b8":"code","d7cd2f3e":"code","97a4f32f":"code","2f51a424":"code","7c32e9d1":"code","dd2c0b7e":"code","eb0af460":"code","0c3640dd":"code","212db937":"code","84b6161e":"code","789d48cf":"code","ee9037b6":"code","abde735a":"code","7004a1bf":"markdown","c8025be4":"markdown","cdaea112":"markdown","671ec708":"markdown","d76b411f":"markdown","6eec1f1f":"markdown","c15b81b3":"markdown","73bd545e":"markdown","e3aba937":"markdown","9a1feee9":"markdown","41333755":"markdown","97cb4db3":"markdown","01e44650":"markdown","9705198c":"markdown","735fc011":"markdown","9db2be04":"markdown","348f2df2":"markdown","7a0ac61b":"markdown","1f81a992":"markdown","5b5b6a6f":"markdown","1435a5b9":"markdown","59396776":"markdown","971a9955":"markdown","4dec6a3a":"markdown","8ce826ca":"markdown","ac1cc105":"markdown","1714a42a":"markdown","cc25c9fe":"markdown","ee31fa7f":"markdown","da08fb2b":"markdown","a640e14c":"markdown","c800992e":"markdown"},"source":{"54d7f418":"import numpy as np\nimport pandas as pd \nimport matplotlib as mpl \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom cycler import cycler\n\nmpl.rcParams['figure.dpi'] = 120\nmpl.rcParams['axes.spines.top'] = False\nmpl.rcParams['axes.spines.right'] = False","3b05edb2":"data =pd.read_csv('..\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv')\ndata.head()","4124a732":"data.describe(include='all')","9ea976a2":"data.isnull().sum()","a522fa12":"data.groupby('gender')['bmi'].mean()","f183d60a":"data = data.fillna(data.mean())\ndata.isnull().sum()\n\n# No more missing values left ","378e862d":"# Check out all the datas dtypes before EDA\ndata.dtypes","189a9b94":"data= data.drop(columns='id')","e1354fe9":"fig = plt.figure(figsize=(14,11))\ngs = fig.add_gridspec(3,4)\nsns.set_style(\"white\")\nsns.set_context(\"poster\", font_scale = 0.5)\n\nax_gender_stroke = fig.add_subplot(gs[:2,:2])\nsns.countplot(x='gender', hue='stroke', data=data, ax=ax_gender_stroke, palette='coolwarm')\nsns.despine()\n\nax_gender_stroke = fig.add_subplot(gs[:2,2:4], sharey=ax_gender_stroke)\nsns.countplot(x='stroke', hue='gender', data=data, ax=ax_gender_stroke, palette='coolwarm')\nsns.despine()\n\n\n\nplt.show()","c7878be3":"data_delete = data[data['gender'] == 'Other'].index\ndata = data.drop(data_delete)\n\ndata.groupby(['gender', 'stroke'])['stroke'].count()","c755efd1":"fig = plt.figure(figsize=(14,11))\ngs = fig.add_gridspec(3,4)\nsns.set_style(\"white\")\nsns.set_context(\"poster\", font_scale = 0.5)\n\n\nax_gender_stroke = fig.add_subplot(gs[:2,:2])\nsns.countplot(x='ever_married', hue='stroke', data=data, ax=ax_gender_stroke, palette='coolwarm')\nsns.despine()\n\nax_gender_stroke = fig.add_subplot(gs[:2,2:4], sharey=ax_gender_stroke)\nsns.countplot(x='stroke', hue='ever_married', data=data, ax=ax_gender_stroke, palette='coolwarm')\nsns.despine()\n\n\n\nplt.show()","45a5e70f":"fig = plt.figure(figsize=(14,11))\ngs = fig.add_gridspec(3,4)\nsns.set_style(\"white\")\nsns.set_context(\"poster\", font_scale = 0.5)\n\n\nax_gender_stroke = fig.add_subplot(gs[:2,:2])\nsns.countplot(x='work_type', hue='stroke', data=data, ax=ax_gender_stroke, palette='coolwarm')\nsns.despine()\n\nax_gender_stroke = fig.add_subplot(gs[:2,2:4], sharey=ax_gender_stroke)\nsns.countplot(x='stroke', hue='work_type', data=data, ax=ax_gender_stroke, palette='coolwarm')\nsns.despine()\n\n\n\nplt.show()","189eca8d":"fig = plt.figure(figsize=(14,11))\ngs = fig.add_gridspec(3,4)\nsns.set_style(\"white\")\nsns.set_context(\"poster\", font_scale = 0.5)\n\n\nax_gender_stroke = fig.add_subplot(gs[:2,:2])\nsns.countplot(x='Residence_type', hue='stroke', data=data, ax=ax_gender_stroke, palette='coolwarm')\nsns.despine()\n\nax_gender_stroke = fig.add_subplot(gs[:2,2:4], sharey=ax_gender_stroke)\nsns.countplot(x='stroke', hue='Residence_type', data=data, ax=ax_gender_stroke, palette='coolwarm')\nsns.despine()\n\n\n\nplt.show()","a9a66eea":"data.groupby(['Residence_type', 'stroke'])['stroke'].count()","e4c5f793":"fig = plt.figure(figsize=(16,11))\ngs = fig.add_gridspec(3,4)\nsns.set_style(\"white\")\nsns.set_context(\"poster\", font_scale = 0.5)\n\n\nax_gender_stroke = fig.add_subplot(gs[:2,:2])\nsns.countplot(x='smoking_status', hue='stroke', data=data, ax=ax_gender_stroke, palette='coolwarm')\nsns.despine()\n\nax_gender_stroke = fig.add_subplot(gs[:2,2:4], sharey=ax_gender_stroke)\nsns.countplot(x='stroke', hue='smoking_status', data=data, ax=ax_gender_stroke, palette='coolwarm')\nsns.despine()\n\n\n\nplt.show()","b6a601e6":"data.groupby(['smoking_status', 'stroke'])['stroke'].count()","a5146fc9":"f,ax = plt.subplots(1,2, figsize=(20,10))\n\ndata.loc[data['stroke'] ==0]['age'].plot.hist(ax=ax[0], bins=20, edgecolor='black', color='skyblue')\nax[0].set_title('stroke = 0')\nax1 = list(range(0, 85, 5))\nax[0].set_xticks(ax1)\n\ndata[data['stroke']==1]['age'].plot.hist(ax=ax[1], color='red', bins=20, edgecolor='black')\nax[1].set_title('stroke=1')\nx2=list(range(0, 85, 5))\nax[1].set_xticks(x2)\nplt.show();","2cf8ef3d":"fig = plt.figure(figsize=(16,11))\ngs = fig.add_gridspec(3,4)\nsns.set_style(\"white\")\nsns.set_context(\"poster\", font_scale = 0.5)\n\n\nax_gender_stroke = fig.add_subplot(gs[:2,:2])\nsns.countplot(x='hypertension', hue='stroke', data=data, ax=ax_gender_stroke, palette='coolwarm')\nsns.despine()\n\nax_gender_stroke = fig.add_subplot(gs[:2,2:4], sharey=ax_gender_stroke)\nsns.countplot(x='stroke', hue='hypertension', data=data, ax=ax_gender_stroke, palette='coolwarm')\nsns.despine()\n\n\n\nplt.show()","5e5e893e":"data.groupby(['hypertension', 'stroke'])['stroke'].count()","1da85d60":"fig = plt.figure(figsize=(16,11))\ngs = fig.add_gridspec(3,4)\nsns.set_style(\"white\")\nsns.set_context(\"poster\", font_scale = 0.5)\n\n\nax_gender_stroke = fig.add_subplot(gs[:2,:2])\nsns.countplot(x='heart_disease', hue='stroke', data=data, ax=ax_gender_stroke, palette='coolwarm')\nsns.despine()\n\nax_gender_stroke = fig.add_subplot(gs[:2,2:4], sharey=ax_gender_stroke)\nsns.countplot(x='stroke', hue='heart_disease', data=data, ax=ax_gender_stroke, palette='coolwarm')\nsns.despine()\n\n\n\nplt.show()","74995512":"data.groupby(['heart_disease', 'stroke'])['stroke'].count()","7c323ddb":"sns.kdeplot('avg_glucose_level', data=data, shade=True)\nsns.set_style(\"white\")\nsns.despine()","f34bcb27":"f,ax = plt.subplots(1,2, figsize=(20,10))\n\ndata.loc[data['stroke'] ==0]['avg_glucose_level'].plot.hist(ax=ax[0], bins=20, edgecolor='black', color='skyblue')\nax[0].set_title('stroke = 0')\nax1 = list(range(30, 300, 10))\nax[0].set_xticks(ax1)\n\ndata.loc[data['stroke']==1]['avg_glucose_level'].plot.hist(ax=ax[1], color='red', bins=20, edgecolor='black')\nax[1].set_title('stroke=1')\nx2= list(range(30, 300, 10))\nax[1].set_xticks(x2)\nplt.show()","5cd4841d":"plt.hist('bmi', data=data, histtype='stepfilled',color='skyblue');","71912813":"f,ax = plt.subplots(1,2, figsize=(20,10))\n\ndata.loc[data['stroke'] ==0]['bmi'].plot.hist(ax=ax[0], bins=20, edgecolor='black', color='skyblue')\nax[0].set_title('stroke = 0')\nax1 = list(range(0, 70, 5))\nax[0].set_xticks(ax1)\n\ndata.loc[data['stroke']==1]['bmi'].plot.hist(ax=ax[1], color='red', bins=20, edgecolor='black')\nax[1].set_title('stroke=1')\nx2= list(range(0, 70, 5))\nax[1].set_xticks(x2)\nplt.show()","f255b17e":"# Categorical Features\n\ncat_columns = [c for c, t in zip(data.dtypes.index, data.dtypes) if t == 'O']\n\ndata = pd.get_dummies(data = data, columns = cat_columns)\ndata = pd.get_dummies(data = data, columns = ['hypertension'])\ndata = pd.get_dummies(data = data, columns = ['heart_disease'])\n\ndata.columns","8a966c43":"# Numerical Features\n\nnum_columns = [c for c, t in zip(data.dtypes.index, data.dtypes) if t == 'float64']\nnum_columns","d3ca55b4":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\ndata[num_columns] = scaler.fit_transform(data[num_columns])\n\ndata[num_columns]","b1c1627d":"data.head()","c308b7b8":"x = data.drop('stroke', axis=1).values\ny = data['stroke'].values","d7cd2f3e":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.4, random_state=2020,shuffle=True)\nx_valid, x_test, y_valid, y_test = train_test_split(x_test, y_test, test_size=0.5, random_state=2020,shuffle=True)","97a4f32f":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom sklearn.metrics import f1_score","2f51a424":"# 1. LogisticRegression\n\nlr = LogisticRegression()\n\nlr.fit(x_train, y_train)\n\ny_pred = lr.predict(x_valid)\n\nprint(f\"Logistic Regression F1 Score: {f1_score(y_valid, y_pred, average='micro')}\")","7c32e9d1":"# 2. Support Vector Machine\n\nsvc = SVC(probability=True)\n\nsvc.fit(x_train, y_train)\n\ny_pred = svc.predict(x_valid)\n\nprint(f\"Support Vector Machine F1 Score: {f1_score(y_valid, y_pred, average='micro')}\")","dd2c0b7e":"# 3. Rnadom Forest\n\nrf = RandomForestClassifier()\n\nrf.fit(x_train, y_train)\n\ny_pred = rf.predict(x_valid)\n\nprint(f\"RandomForest F1 Score: {f1_score(y_valid, y_pred, average='micro')}\")","eb0af460":"# 4. XGBoost\n\nxgb = XGBClassifier()\n\nxgb.fit(x_train, y_train)\n\ny_pred = xgb.predict(x_valid)\n\nprint(f\"XGBoost F1 Score: {f1_score(y_valid, y_pred, average='micro')}\")","0c3640dd":"# 5. LightGBM\n\nlgb = LGBMClassifier()\n\nlgb.fit(x_train, y_train)\n\ny_pred = lgb.predict(x_valid)\n\nprint(f\"LightGBM F1 Score: {f1_score(y_valid, y_pred, average='micro')}\")","212db937":"# 6. KNeighborsClassifier \n\nknn = KNeighborsClassifier()\n\nknn.fit(x_train, y_train)\n\ny_pred = knn.predict(x_valid)\n\nprint(f\"KNeighborsClassifier F1 Score: {f1_score(y_valid, y_pred, average='micro')}\")","84b6161e":"from sklearn.model_selection import KFold #for K-fold cross validation\nfrom sklearn.model_selection import cross_val_score #score evaluation\nfrom sklearn.model_selection import cross_val_predict #prediction\nkfold = KFold(n_splits=10, random_state=2020, shuffle=True) # k=10, split the data into 10 equal parts\nxyz=[]\naccuracy=[]\nstd=[]\nclassifiers=['Logistic Regression',\n             'SVC',\n             'Random Forest',\n             'XGB',\n             'LGBM',\n             'KNeighbors']\n\nmodels=[LogisticRegression(),\n        SVC(),\n        RandomForestClassifier(),\n        XGBClassifier(),\n        LGBMClassifier(),\n        KNeighborsClassifier()]\n\nfor i in models:\n    model = i\n    cv_result = cross_val_score(model,x,y, cv = kfold,scoring = \"accuracy\")\n    cv_result=cv_result\n    xyz.append(cv_result.mean())\n    std.append(cv_result.std())\n    accuracy.append(cv_result)\nnew_models_dataframe2=pd.DataFrame({'CV Mean':xyz,'Std':std},index=classifiers)       \nnew_models_dataframe2","789d48cf":"plt.subplots(figsize=(18,10))\nbox=pd.DataFrame(accuracy,index=[classifiers])\nbox.T.boxplot();","ee9037b6":"new_models_dataframe2['CV Mean'].plot.barh(width=0.8)\nplt.title('Average CV Mean Accuracy')\nfig=plt.gcf()\nfig.set_size_inches(8,5)\nplt.show()","abde735a":"from sklearn.ensemble import AdaBoostClassifier\nada=AdaBoostClassifier(n_estimators=200, # 200\uac1c\ub77c\ub294 \uac83\uc740 200\uac1c\ub97c \ubd99\uc5ec\uc11c \uc774\uc81c \uc2e4\ud589\uc744 \ud574\uc900\ub2e4\ub294 \uac83\uc744 \ub9d0\ud574\uc8fc\ub294 \uac83\uc774\ub2e4 \n                       random_state=0,\n                       learning_rate=0.1)\nresult=cross_val_score(ada,x,y,cv=10,scoring='accuracy')\nprint('The cross validated score for AdaBoost is:',result.mean())","7004a1bf":"## Data Check","c8025be4":"There is no difference in average according to gender, so let's replace by average value.","cdaea112":"My background was that smoking would have a significant impact on the stroke outbreak, but there's not such big difference between smoking and non smoking.\n\nBut as a percentage of smokers, we can know that when me smoke it might be more is likely to occur.","671ec708":"#### First devide the columns into **Categorical feature** and **Numerical Features**\n\n#### I am going to use dummies values to categorical features and use StandardScaler to numerical features","d76b411f":"### BMI","6eec1f1f":"Higher your glucose_level the higher you can get in stroke!","c15b81b3":"#### Wow! We get the accuracy about 96% !!! \n#### The score was better than I thought. I think I did a great job with feature engineering.\n\n## 2) Cross Validation.","73bd545e":"### Gender & Stroke","e3aba937":"### Ever Married & Stroke","9a1feee9":"### Heart Disease & Stroke ","41333755":"Most of people's bmi levels are around 20 to 30 and higher do not mean they are more likely to have a stroke.\n\n# Part2: Feature Engineering and Data Cleaning:\n","97cb4db3":"NOW! The feature engineering is clear! Next we are going to split the train-test set and go modeling ~!","01e44650":"We have gained some insights from the EDA part. But with that, we cannot accurately predict or tell whether a stroke will occur or not.. So now we will predict by using some great Classification Algorithms. Following are the algorithms I will use to make the model:\n\n1)Logistic Regression\n\n2)Support Vector Machines(Linear and radial)\n\n3)Random Forest\n\n4)K-Nearest Neighbours\n\n5)Naive Bayes\n\n6)Decision Tree\n\n7)Logistic Regression","9705198c":"## 3) Ensembling\n\nEnsembling is a good way to increase the accuracy or performance of a model. In simple words, it is the combination of various simple models to create a single powerful model.\n\nLets say we want to buy a phone and ask many people about it based on various parameters. So then we can make a strong judgement about a single product after analysing all different parameters. This is **Ensembling**, which improves the stability of the model. Ensembling can be done in ways like:\n\nI am goind to use **Boosting** ensembling method. \n\n### Boosting\n\nBoosting is an ensembling technique which uses sequential learning of classifiers. It is a step by step enhancement of a weak model.Boosting works as follows:\n\nA model is first trained on the complete dataset. Now the model will get some instances right while some wrong. Now in the next iteration, the learner will focus more on the wrongly predicted instances or give more weight to it. Thus it will try to predict the wrong instance correctly. Now this iterative process continous, and new classifers are added to the model until the limit is reached on the accuracy.\n\n#### AdaBoost(Adaptive Boosting)\n\nThe weak learner or estimator in this case is a Decsion Tree.  But we can change the dafault base_estimator to any algorithm of our choice.","735fc011":"- Delete 'Other', it can be a outlier to machine learning\n\n- There is not much difference between a man and a woman, But in proportion, males are more likely to develop in proportion","9db2be04":"Overall, people who work are more likely to get in stroke. ","348f2df2":"# Part1: Exploratory Data Analysis(EDA):\n\n1) id: unique identifier\n\n2) gender: \"Male\", \"Female\" or \"Other\"\n\n3) age: age of the patient\n\n4) hypertension: 0 if the patient doesn't have hypertension, 1 if the patient has hypertension\n\n5) heart_disease: 0 if the patient doesn't have any heart diseases, 1 if the patient has a heart disease\n\n6) ever_married: \"No\" or \"Yes\"\n\n7) work_type: \"children\", \"Govt_jov\", \"Never_worked\", \"Private\" or \"Self-employed\"\n\n8) Residence_type: \"Rural\" or \"Urban\"\n\n9) avg_glucose_level: average glucose level in blood\n\n10) bmi: body mass index\n\n11) smoking_status: \"formerly smoked\", \"never smoked\", \"smokes\" or \"Unknown\"*\n\n12) stroke: 1 if the patient had a stroke or 0 if not\n\n*Note: \"Unknown\" in smoking_status means that the information is unavailable for this patient\n\n\n\n#### First of all, we will look at features based on the target values ( 'stroke').\n","7a0ac61b":"#### Let the features be divided into two categories before we start EDA: \n\n1) **Categorical** : gender, ever_married, work_type, residence_type, smoking_status\n\n2) **Numerical** : age, hypertension, heart_disease, avg_glucose_level, bmi\n\n+ hyoertension & heart_disease have int dtypes, but we can check out that they are in categorical style ","1f81a992":"# Contents of the Notebook  :\n \n# Part1: Exploratory Data Analysis(EDA):\n#### 1)Analysis of the features.\n\n#### 2)Finding any relations or trends considering multiple features.\n\n# Part2: Feature Engineering and Data Cleaning:\n\n#### 1)Converting features into suitable form for modeling.\n\n# Part3: Predictive Modeling\n#### 1)Running Basic Algorithms.\n\n#### 2)Cross Validation.\n\n#### 3)Ensembling.\n","5b5b6a6f":"### Worktype & Stroke","1435a5b9":"### Smoking & Stroke ","59396776":"## In conclusion, an accuracy of **95%** was obtained.\n\n### Feel free to give any comments about my notebook!\n\n### Also, if my notebook was helpful, please give me an upvote !!!!!","971a9955":"### Glucose_level","4dec6a3a":"There is a greater chance of stroke among people who have been married. It can be a meaningful feature.","8ce826ca":"It's too similar to see with eyes. But I think it is not that useful feature. ","ac1cc105":"There is only missing values in 'bmi'! Let's check out \n\nDetermining the average BMI figure by gende","1714a42a":"I think ID doesn't mean much, so let's remove it.","cc25c9fe":"### Hypertension & Stroke ","ee31fa7f":"# Part3: Predictive Modeling\n## 1)Running Basic Algorithms.","da08fb2b":"### Age & Stroke \n#### Now Lets check out the numerical categories ","a640e14c":"### Residence & Stroke","c800992e":"## Train-Valid-Test split"}}