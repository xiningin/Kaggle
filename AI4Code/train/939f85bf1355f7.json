{"cell_type":{"905cfea0":"code","98eee787":"code","bcab6792":"code","46858cff":"code","f72c7791":"code","bf744d73":"code","b6788e47":"code","555a84ee":"code","b250c660":"code","50f32199":"code","4b986141":"code","99330ea8":"code","16b4ba08":"code","26908a44":"code","52cc9b88":"code","2dc2237f":"code","7b942415":"code","8bf798fc":"code","8004af8a":"code","bcb62e79":"code","f6b2a452":"code","f039b4c0":"code","0938d61c":"code","46afbe80":"code","c9607040":"markdown","4f80b9f6":"markdown","c848886e":"markdown","5a74ef2b":"markdown","8bfae11a":"markdown","3e27ba40":"markdown","e2e696d3":"markdown","c5d1c05d":"markdown","2215bf39":"markdown","c2106966":"markdown","9b10b7f0":"markdown","bb4554f0":"markdown","d4201cb6":"markdown","22c80a2e":"markdown","31d2576c":"markdown","bdd3754d":"markdown","219d9218":"markdown","3225da16":"markdown","1bca30be":"markdown","c7e8e5ea":"markdown","b05f8c49":"markdown"},"source":{"905cfea0":"import category_encoders as ce\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport pandas as pd\nimport seaborn as sns\nimport xgboost as xgb\n\nfrom IPython.display import display, Markdown\nfrom pandas_profiling import ProfileReport\nfrom sklearn.base import BaseEstimator, clone, TransformerMixin\nfrom sklearn.compose import ColumnTransformer, TransformedTargetRegressor\nfrom sklearn.impute import SimpleImputer\n\nfrom sklearn.linear_model import ElasticNetCV\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.model_selection import (\n    GridSearchCV,\n    RandomizedSearchCV,\n    train_test_split\n)\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import (\n    FunctionTransformer,\n    OneHotEncoder,\n    OrdinalEncoder,\n    QuantileTransformer,\n)\nfrom sklearn_pandas import DataFrameMapper, gen_features","98eee787":"pd.set_option('display.max_columns', None)\nsns.set()\n\ndef md(text: str):\n    display(Markdown(text))\n    \ndef rmsle(y_true, y_pred):\n    return np.sqrt(mean_squared_log_error(y_true, y_pred))\n\ndef prepend_keys(dict_: dict, prepend: str) -> dict:\n    return {f\"{prepend}{key}\": value for key, value in dict_.items()}","bcab6792":"data_dir = \"\/kaggle\/input\/house-prices-advanced-regression-techniques\"\n\ndata = pd.read_csv(os.path.join(data_dir, \"train.csv\"), index_col=\"Id\")\ntest = pd.read_csv(os.path.join(data_dir, \"test.csv\"), index_col=\"Id\")\nsample_submission = pd.read_csv(os.path.join(data_dir, \"sample_submission.csv\"))\n\nX = data.drop(columns=[\"SalePrice\"])\ny = data[\"SalePrice\"]\n\nmd(f\"The total number of training observations is: `{len(X)}`\")\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n\nmd(f\"The number of observations with which we do hyperparameter tuning is: `{len(X_train)}`\")","46858cff":"profile = ProfileReport(data, minimal=True, title='Pandas Profiling Report', html={'style':{'full_width':True}})","f72c7791":"profile.to_notebook_iframe()","bf744d73":"class FeatureAdder(TransformerMixin, BaseEstimator):\n    numeric_transforms = {\n        \"sale_age\": lambda df: df[\"YrSold\"] - df[\"YearRemodAdd\"]\n    }\n    \n    categoric_transforms = {\n        \"MSSubClass\": lambda df: df[\"MSSubClass\"].astype(\"object\"),\n    }\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        assert isinstance(X, pd.DataFrame)\n        return X.assign(**self.numeric_transforms, **self.categoric_transforms)","b6788e47":"numeric_cols = set(FeatureAdder().fit_transform(X).select_dtypes(exclude=\"object\").columns)\ncategoric_cols = set(FeatureAdder().fit_transform(X).select_dtypes(include=\"object\").columns)\n\nprint(numeric_cols)\nprint(\"=\" * 80)\nprint(categoric_cols)","555a84ee":"zero_impute_cols = {\"LotFrontage\", \"MasVnrArea\"}\nmode_impute_cols = {\"Electrical\", \"Functional\", \"KitchenQual\", \"MSZoning\", \"SaleType\", \"Utilities\"}\n\n\n# feature_imputer = ColumnTransformer(\n#     transformers=[\n#         (\"num_zero\", SimpleImputer(strategy=\"constant\", fill_value=0), list(zero_impute_cols)),\n#         (\"num_median\", SimpleImputer(strategy=\"median\"), list(numeric_cols - zero_impute_cols)),\n#         (\"cat_mode\", SimpleImputer(strategy=\"most_frequent\"), list(mode_impute_cols)),\n#         (\"cat_unknown\", SimpleImputer(strategy=\"constant\", fill_value=\"Unknown\"), list(categoric_cols - mode_impute_cols))\n#     ],\n#     remainder=\"drop\"\n# )\n\nfeature_imputer = DataFrameMapper(\n    [\n        *gen_features(\n            columns=[[col] for col in zero_impute_cols],\n            classes=[{\"class\": SimpleImputer, \"strategy\": \"constant\", \"fill_value\": 0}]\n        ),\n        *gen_features(\n            columns=[[col] for col in numeric_cols - zero_impute_cols],\n            classes=[{\"class\": SimpleImputer, \"strategy\": \"median\"}]\n        ),\n        *gen_features(\n            columns=[[col] for col in mode_impute_cols],\n            classes=[{\"class\": SimpleImputer, \"strategy\": \"most_frequent\"}]\n        ),\n        *gen_features(\n            columns=[[col] for col in categoric_cols - mode_impute_cols],\n            classes=[{\"class\": SimpleImputer, \"strategy\": \"constant\", \"fill_value\": \"Unknown\"}]\n        ),\n    ],\n    df_out=True\n)","b250c660":"target_encode_cols = {\n    \"Condition1\",\n    \"Condition2\",\n    \"Exterior1st\",\n    \"Exterior2nd\",\n    \"MSSubClass\",\n    \"Neighborhood\",\n    \"SaleType\",\n}\n\n\n# TODO: consider adding \"BsmtExposure\", [\"Gd\", \"Av\", \"Mn\", \"No\"]\nordinal_encode_categories = [\"Ex\", \"Gd\",\"TA\", \"Fa\", \"Po\", \"Unknown\"]\nordinal_encode_cols = {\n    \"BsmtCond\",\n    \"BsmtQual\",\n    \"ExterCond\",\n    \"ExterQual\",\n    \"FireplaceQu\",\n    \"GarageCond\",\n    \"GarageQual\",\n    \"HeatingQC\",\n    \"KitchenQual\",\n    \"PoolQC\",    \n}\n\n# let's write them all here once to prevent repeating ourselves\nfeature_transformers = {\n    \"ordinal\": OrdinalEncoder(categories=[ordinal_encode_categories] * len(ordinal_encode_cols)),\n    \"normal\": QuantileTransformer(n_quantiles=500, output_distribution=\"normal\"),\n    \"target\": ce.target_encoder.TargetEncoder(),\n    \"onehot\": OneHotEncoder(handle_unknown=\"ignore\"),\n}\n\nfeature_cleaner = ColumnTransformer(\n    transformers=[\n        (\n            \"num_normal\",\n            clone(feature_transformers[\"normal\"]),\n            list(numeric_cols)\n        ),\n        (\n            \"cat_ordinal\",\n            clone(feature_transformers[\"ordinal\"]),\n            list(ordinal_encode_cols)\n        ),\n        (\n            \"cat_target\",\n            clone(feature_transformers[\"target\"]),\n            list(target_encode_cols)\n        ),\n        (\n            \"cat_onehot\",\n            clone(feature_transformers[\"onehot\"]),\n            list(categoric_cols - ordinal_encode_cols - target_encode_cols)\n        ),\n    ],\n    remainder=\"drop\"\n)","50f32199":"def transform_regressor(regressor):\n    return TransformedTargetRegressor(regressor=regressor, func=np.log1p, inverse_func=np.expm1)","4b986141":"pipe = Pipeline(steps=[\n    (\"feature_adder\", FeatureAdder()),\n    (\"feature_imputer\", feature_imputer),\n    (\"feature_cleaner\", feature_cleaner),\n    (\"model\", transform_regressor(ElasticNetCV(cv=5)))\n])","99330ea8":"feature_param_grid = {\n    \"feature_cleaner__cat_ordinal\": [\n        clone(feature_transformers[\"ordinal\"]),\n        clone(feature_transformers[\"onehot\"]),\n    ],\n    \"feature_cleaner__cat_target\": [\n        clone(feature_transformers[\"target\"]),\n        clone(feature_transformers[\"onehot\"])\n    ],\n}\n\ngrid = GridSearchCV(pipe, param_grid=feature_param_grid, cv=5, scoring=\"neg_mean_squared_log_error\", n_jobs=-1)\ngrid.fit(X_train, y_train)","16b4ba08":"grid.best_params_","26908a44":"md(f\"Model score on the validation set: `{rmsle(y_test, grid.predict(X_test)):.3f}`\")\nmd(f\"Model score on the train set: `{rmsle(y_train, grid.predict(X_train)):.3f}`\")","52cc9b88":"y_train_trf = np.log1p(y_train)\ny_test_trf = np.log1p(y_test)\n\nxgb_pipe_trf = Pipeline(grid.best_estimator_.steps[:-1])\nxgb_pipe_trf.fit(X_train, y_train_trf)\n\nX_train_trf = xgb_pipe_trf.transform(X_train)\nX_test_trf = xgb_pipe_trf.transform(X_test)","2dc2237f":"xgb_fit_params = {\n    \"early_stopping_rounds\": 10,  \n    \"eval_set\" : [(X_test_trf, y_test_trf)],\n    \"verbose\": False\n}\n\nxgb_model = xgb.XGBRegressor(objective=\"reg:squaredlogerror\")","7b942415":"\"\"\"\nCredit: http:\/\/danielhnyk.cz\/how-to-use-xgboost-in-python\/\nFor the randomised parameter grid.\n\"\"\"\n\nimport scipy.stats as st\n\none_to_left = st.beta(10, 1)  \nfrom_zero_positive = st.expon(0, 50)\n\nxgb_params = {  \n    \"n_estimators\": st.randint(3, 2000),\n    \"max_depth\": st.randint(3, 60),\n    \"learning_rate\": st.uniform(0.05, 0.4),\n    \"colsample_bytree\": one_to_left,\n    \"subsample\": one_to_left,\n    \"gamma\": st.uniform(0, 10),\n    \"reg_alpha\": from_zero_positive,\n    \"min_child_weight\": from_zero_positive,\n}\n\nxgb_grid = RandomizedSearchCV(\n    xgb_model,\n    param_distributions=xgb_params,\n    n_iter=20,\n    cv=5,\n    scoring=\"neg_mean_squared_log_error\",\n)\nxgb_grid.fit(X_train_trf, y_train_trf, **xgb_fit_params)","8bf798fc":"xgb_grid.best_params_","8004af8a":"xgb_pipe = Pipeline([*xgb_pipe_trf.steps, (\"model\", transform_regressor(xgb_grid.best_estimator_))])\n\nxgb_pipe.fit(X_train, y_train, **prepend_keys(xgb_fit_params, \"model__\"))","bcb62e79":"md(f\"Model score on the validation set: `{rmsle(y_test, xgb_pipe.predict(X_test)):.3f}`\")\nmd(f\"Model score on the train set: `{rmsle(y_train, xgb_pipe.predict(X_train)):.3f}`\")","f6b2a452":"submission = pd.DataFrame({\"SalePrice\": grid.predict(test)}, index=test.index)\n\nsubmission","f039b4c0":"submission.to_csv(\"submission.csv\")","0938d61c":"from sklearn.inspection import permutation_importance\n\ndef plot_importance(estimator, X, y):\n    \"\"\"\n    Source: https:\/\/scikit-learn.org\/stable\/auto_examples\/inspection\/plot_permutation_importance.html\n    \"\"\"\n    result = permutation_importance(estimator, X, y, n_repeats=10,\n                                    random_state=42, n_jobs=-1)\n    sorted_idx = result.importances_mean.argsort()\n\n    fig, ax = plt.subplots(figsize=(15, 15))\n    ax.boxplot(result.importances[sorted_idx].T,\n               vert=False, labels=X_test.columns[sorted_idx])\n    ax.set_title(\"Permutation Importances\")\n    fig.tight_layout()\n    \n    return fig","46afbe80":"fig = plot_importance(grid, X_test, y_test)","c9607040":"# XGBoost Model","4f80b9f6":"Here we produce a set of the numeric and categorical columns. This helps to clean up our code below.","c848886e":"Next we construct our xgboost model. We can set the xgboost object function to match our competition metric\nof mean squared log error. We create a `dict` of the params that model needs to take for our evaluation set of `(X_test_trf, y_test_trf)`.","5a74ef2b":"There's lots of information from this report that we could act on. I've written down some things that caught my eye:\n\n## Target Variable\n\n- Our target variable, `SalePrice`, does not look normally distributed. A log transform would improve the modelling.\n\n## Categorical Variables\n\n- We are going to have to do some filling in of missing values. In some cases the values are __structurally missing__ ([cross reference in missing data types](https:\/\/www.displayr.com\/different-types-of-missing-data\/)). That is, if the house doesn't have a garage then the value for `GarageType` _should_ be missing.\nFor these cases we can imput the categories with a sentinel value like \"None\/Unknown\".\n- The other type of missing is those that appear to be __missing at random__. For example some values of `SaleType` are missing. For these we can impute them with an appropriate value. In this kernel I use the mode.\n- There is also a lot of features that look meaningless. For example `Street` is nearly constant. If they're of concern you can use the `sklearn.feature_selection.VarianceThreshold` to drop of these near-zero variance features. In practice, I found that it didn't improve the model meaningfully.\n- The main way to encode categorical columns in a meaningful way is to use a `OneHotEncoder`. However,there's a handful of these categorical features which have an order to them (e.g. `KitchenQual`, `BsmtQual`). We can consider encoding this information with the `OrdinalEncoder`.\n- There are some high cardinality features. Some models may struggle with encoding them as one-hot vectors. An option is to instead encode them with a `category_encoders.target.TargetEncoder`. We can try this with a fall-back to just using a `OneHotEncoder`.\n\n## Numeric Variables\n\n- We can see that most of these variables seem reasonable. Their information\nranges from being:\n    - Some measure of square footage.\n    - Some measure of quality (e.g. `OverallQuality`, `OverallCond`, `Functional`, `PoolQC`, `FenceQC`)\n    - Some time measure (e.g. `YearBuilt`, `YearRemodAdd`, `MoSold`, `YearSold`)\n    - Some counts (e.g. `BsmtFullBath`, `BsmtHalfBath`, `FullBath`)\n- The column `MSSubClass` should be changed from numeric scale and become nominal categories, as there's\nno meaning to the order of those numbers.\n- For all numeric variables, we should scale them to be on a common scale. This means that variable's\naren't unfairly favoured over another based on their __variance__ (the spread of its values from largest to smallest). Scaling them to be on a common scale helps them to be more comparable. That is, we can pick them based on what matters: their correlation with the target variable `SalePrice`. There are great tools to do this such as `StandardScaler` and `RobustScaler`. There is also value in\nnormalising the distributions of these features which can be done using either the `PowerTransformer` or\nthe `QuantileTransformer`.\n\n\n## Features to Add\n\nSome features that we can add are:\n\n- The \"age\" of the house. i.e. difference between the `YearSold` and `YearRemodAdd` (If no remodel it seems that `YearBuilt == YearRemodAdd`).","8bfae11a":"# House Prices Regression\n\n__Overview__:\n\nThis kernel is a solution to the [House prices](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques) kaggle comp. It was a way for me to learn a lot\nabout the scikit-learn `Pipeline`. Particularly, how to use it to do clean and easy feature engineering and model training (read: no \ud83c\udf5d code).\n\nI aimed to keep the modelling simple: I stuck with `ElasticNetCV` the whole time. Regularised OLS models were found to perform well from other kernels. So, I took that recommendation and focused on everything apart from the model.\n\nIf it helps at all, I'd love to hear it. Any questions let me know. If that's it then let's crack on.","3e27ba40":"We perform hyperparameter tuning for our XGBoost model. There is lots of different parameters that we could choose, which can also take a lot of time. I have used the `RandomizedSearchCV` here instead of `GridSearchCV` to save time.","e2e696d3":"We can see that the basic one-hot encoding was the best approach after all! Ah well, it was worth trying something out.\n\nWe can see below the score that our model gets on the the train dataset, and the validation dataset.","c5d1c05d":"# Imports and Settings","2215bf39":"# Conclusion\n\nFrom our simple tests above we can see that the elasticnet model is the best performing of\nthe few that we tested. There's lots more we could do such as stack several models or remove outliers. But let's leave it here for today. I hope this helped someone else in trying to write up some clean, maintable, production-ready code.","c2106966":"# Exploratory Data Analysis\n\nLet's answer the question: _What are the features we're dealing with here?_\n\nWe use the [pandas-profiling](https:\/\/github.com\/pandas-profiling\/pandas-profiling) package to\nget a quick overview of the features of our data. There's lots of info that you can dig out from the data.\nHave a look yourself below. I've made my own comments below.","9b10b7f0":"## Add\/transform features:\n\n- convert `MSSubClass` to categorical as it's currently stored as a numeric variable\n- add `sale_age`: the buildings \"age\" at time of sale\n\nThere are many other features that could be added.\n\nWe make a custom scikit-learn transformer to add these features.","bb4554f0":"# Data Loading\n\nWe are going to ensure not to touch the test set data at all during training.\nAnd will split the available training data into a train and validation set.\nThe thinking is that the generalization error for the validation set should be\ncomparable to that found with the test set.\n\nOnce we have completed any hyperparameter tuning we can retrain our chosen model\non the full data set `X`.","d4201cb6":"# Appendix\n\n## Feature importance\n\nWe can see which features in our elastic net model are most connected to our output variable. Unsurprisingly,\nthey are those features that are related to the overall quality and size of the property.","22c80a2e":"The xgboost seems good, but not as good as the elasticnet model. So we can see first-hand that XGBoost does not do as well on this challenge as ElasticNet.","31d2576c":"With the best hyperparameters found. We rebuild our pipeline from before, adding in the xgboost model within the `TransformedTargetRegressor`, and then fit the completed pipeline upon our training data.","bdd3754d":"# ElasticNet Model\n\nOur completed model pipeline is below. Super simple!","219d9218":"## Impute values\n\n- For numeric cols that are structurally missing we can impute with `0`.\nFor example `LotFrontage` and `MasVnrArea`\n- For all other numeric cols, we assume they are missing values at random.\nWe can impute their value with the median column value.\n- For categorical columns that are structurally missing we can impute with `\"None\/Unknown\"`.\nFor example `MiscFeature` and `PoolQC`.\n- For all other categorical columns, we assume they are missing at random and so attempt\nto impute with the column modes (the most frequent value).\nFor example `Functional`\n\n\nWe are using a DataFrameMapper from the `scikit_pandas` package, so that it returns a pandas DataFrame. This is required because the later steps need to use the column info. Otherwise\nwe could do this step more simply with scikit-learn's `ColumnTransformer` (code commented out below).","3225da16":"# Pipeline Creation\n\nWe want to have a series of steps in our model `Pipeline`:\n\n1. Adding features or adjusting their data type\n1. Impute missing values within features\n1. Clean features ready for modelling. For categorical this means encoding them into numbers.\nFor numeric columns it means rescaling them.\n1. Apply the model.","1bca30be":"# Cross-validation\n\nWe want to be able to try out a few different values for our model pipeline. We can do this with a grid-search using `GridSearchCV`.\n\nSubmissions are measured based on their root mean squared log error. This metric can be used as part of a grid search: `\"neg_mean_squared_log_error\"`.","c7e8e5ea":"I know I said I wouldn't look at another model type... But we're all curious aren't we? With a basic feature pipeline complete. We can try different models. We consider next the crowd-favourite __XGBoost model__.\n\nFitting this model could be as simple as changing the regressor used to `xgb.XGBRegressor()`. However, we have to jump through some hoops to be able to have the [early stopping](https:\/\/en.wikipedia.org\/wiki\/Early_stopping) work for xgboost, which is a key regularisation to help an XGBoost model perform. The `XGBRegressor` object can take in a tuple of `(X, y)` data to use as a validation set, which needs to have the same characteristics as the original `X_train` and `y_train` on which the model is being trained.\n\nTo make things simpler, I've trained the model separate from the feature pipeline and kept the feature pipeline constant (taking the best result from our grid search previously). Once we've found the best hyperparameters for the xgboost model and trained it on the data. We can slot it back into our feature processing pipeline.\n\nI create some extra data elements: the processed versions of our `X_train`, `y_train`, `X_test` and `y_test` that the model would have seen. That is, an `X` matrix with all features appropriately one-hot encoded and normalise, and a `y` vector that has been log-transformed. This way the model is training on the type of data that it will see once it is part of the pipeline. These variables are:\n\n- `X_train_trf` and `X_test_trf`, that have been transformed as expected through the feature processing steps.\n- `y_train_trf` and `y_test_trf` which has been log-transformed.","b05f8c49":"## Clean features\n\n### Numeric features:\n\nTransform the distribution of the features to be more normal.\nWe can use the `QuantileTransformer` which works for arbitrary distributions\nprovided if there is enough data.\n\n### Categorical features\n\nEncode them all with the `OneHotEncoder` or `OrdinalEncoder` or `category_encoders.target_encoder.TargetEncoder`. We don't yet know if target and ordinal encoding\nwill provide any value beyond normal one-hot. We will test later on if they help at all.\n\n### Target\n\nNormalise the distribution with a `log(1 + x)` transform. We can do this by wrapping\nany model we attempt with a `sklearn.compose.TransformedTargetRegressor(regressor, func=np.log1p, inverse_func=np.expm1)`"}}