{"cell_type":{"48fedd09":"code","2e72c683":"code","f0e2ae41":"code","5ee014da":"code","e646f497":"code","c07d63cf":"code","1cbd1bbd":"code","f59b3968":"code","ded27cc2":"code","2992b9d5":"code","4f8206d8":"code","5d8c7eb9":"code","cb067e35":"code","92d6a4bb":"code","2b53e7ad":"code","7ccd6e10":"code","56882f9e":"code","a32a9461":"code","8a60b3f4":"code","02cd8735":"code","67699137":"code","d1c948ac":"code","229adcdc":"code","8c5f2873":"code","9f839def":"code","bb31356b":"code","0776aae2":"code","19ce0800":"code","2638e0ee":"code","405cd526":"code","2ed4e56f":"code","c429cade":"code","ec46d99d":"code","c0cd1532":"code","ca353b9a":"markdown","c34371c7":"markdown","1e75085e":"markdown","5bb47b56":"markdown","0a8715f5":"markdown","aa7f7fa3":"markdown","04745f6c":"markdown","b360f2ec":"markdown","7b400e38":"markdown","2fe24949":"markdown","5aad44f1":"markdown","03d10fff":"markdown","39bb1223":"markdown","68f2191a":"markdown","0553a68e":"markdown","021f26fa":"markdown","be094405":"markdown"},"source":{"48fedd09":"import numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport soundfile as sf\nimport librosa\nimport librosa.display\nimport IPython.display as display","2e72c683":"from keras.utils import Sequence\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv1D, MaxPool1D, BatchNormalization\nfrom keras.optimizers import RMSprop,Adam\nfrom keras.applications import VGG19, VGG16, ResNet50","f0e2ae41":"import warnings\nwarnings.filterwarnings(\"ignore\")","5ee014da":"path = '\/kaggle\/input\/rfcx-species-audio-detection\/'\nos.listdir(path)","e646f497":"def read_flac_file(path, file):\n    \"\"\" Read flac audio file and return numpay array and samplerate\"\"\"\n    data, samplerate = sf.read(path+file)\n    return data, samplerate","c07d63cf":"def plot_audio_file(data, samplerate, t_min, t_max, species):\n    \"\"\" Plot the cutout for the speciec label \"\"\"\n    sr = samplerate\n    fig = plt.figure(figsize=(10, 6))\n    x = range(len(data))\n    y = data\n    plt.plot(x, y)\n    x = range(int(t_min*sr), int(t_max*sr))\n    y = data[int(t_min*sr):int(t_max*sr)]\n    plt.plot(x, y, color='red', label = 'species '+str(species))\n    plt.legend(loc='upper center')\n    plt.grid()","1cbd1bbd":"def plot_spectrogram(data, samplerate, t_min, t_max):\n    \"\"\" Plot spectrogram with mel scaling \"\"\"\n    sr = samplerate\n    data_sub = data[int(t_min*sr):int(t_max*sr)]\n    spectrogram = librosa.feature.melspectrogram(data_sub, sr=sr)\n    log_spectrogram = librosa.power_to_db(spectrogram, ref=np.max)\n    librosa.display.specshow(log_spectrogram, sr=sr, x_axis='time', y_axis='mel')","f59b3968":"def plot_bar_compare(data1, data2, name, rot=False):\n    \"\"\" Compare the distribution between train_fp and train_tp data \"\"\"\n    fig, axs = plt.subplots(1, 2, figsize=(9, 3), sharey=True)\n    \n    data1_label = data1[name].value_counts().sort_index()\n    dict_data1 = dict(zip(data1_label.keys(), ((100*(data1_label)\/len(data1.index)).tolist())))\n    data1_names = list(dict_data1.keys())\n    data1_values = list(dict_data1.values())\n    \n    data2_label = data2[name].value_counts().sort_index()\n    dict_data2 = dict(zip(data2_label.keys(), ((100*(data2_label)\/len(data2.index)).tolist())))\n    data2_names = list(dict_data2.keys())\n    data2_values = list(dict_data2.values())\n    \n    axs[0].bar(data1_names, data1_values, color='yellowgreen')\n    axs[1].bar(data2_names, data2_values, color='sandybrown')\n    axs[0].grid()\n    axs[1].grid()\n    axs[0].set_title('train_fp')\n    axs[1].set_title('train_tp')\n    axs[0].set_ylabel('%')\n    if(rot==True):\n        axs[0].set_xticklabels(data1_names, rotation=45)\n        axs[1].set_xticklabels(data2_names, rotation=45)\n    plt.show()","ded27cc2":"train_fp = pd.read_csv(path+'train_fp.csv')\ntrain_tp = pd.read_csv(path+'train_tp.csv')\nsamp_subm = pd.read_csv(path+'sample_submission.csv')","2992b9d5":"train_audio_files = os.listdir(path+'train')\ntest_audio_files = os.listdir(path+'test')","4f8206d8":"data, samplerate = read_flac_file(path+'train\/', train_audio_files[0])\ndata, samplerate, len(data)","5d8c7eb9":"print('number of false positive:', len(train_fp))\nprint('number of true positive:', len(train_tp))\nprint('number of sample submission rows:', len(samp_subm))\nprint('number of train audio files:', len(train_audio_files))\nprint('number of test audio files:', len(test_audio_files))","cb067e35":"plot_bar_compare(train_fp, train_tp, 'species_id', rot=False)","92d6a4bb":"plot_bar_compare(train_fp, train_tp, 'songtype_id', rot=False)","2b53e7ad":"train_fp[0:3]","7ccd6e10":"train_fp.describe()","56882f9e":"recording_id = '00204008d'\ndata, samplerate = read_flac_file(path+'train\/', recording_id+'.flac')","a32a9461":"display.Audio(path+'train\/'+recording_id+'.flac')","8a60b3f4":"t_min = train_fp[train_fp['recording_id']==recording_id]['t_min'][0]\nt_max = train_fp[train_fp['recording_id']==recording_id]['t_max'][0]\nlabel = train_fp[train_fp['recording_id']==recording_id]['species_id'][0]\nplot_audio_file(data, samplerate, t_min, t_max, label)","02cd8735":"plot_spectrogram(data, samplerate, t_min, t_max)","67699137":"y_train_index = [file.split('.')[0] for file in train_audio_files]\ny_train_columns = ['s'+str(i) for i in range(24)]\ny_train = pd.DataFrame(0, index=y_train_index, columns=y_train_columns)\n\nfor row in train_fp.index:\n    index = train_fp.loc[row, 'recording_id']\n    column = 's'+str(train_fp.loc[row, 'species_id'])\n    y_train.loc[index, column] = 1\n\nfor row in train_tp.index:\n    index = train_tp.loc[row, 'recording_id']\n    column = 's'+str(train_tp.loc[row, 'species_id'])\n    y_train.loc[index, column] = 1","d1c948ac":"class DataGenerator(Sequence):\n    def __init__(self, path, list_IDs, labels, batch_size):\n        self.path = path\n        self.list_IDs = list_IDs\n        self.labels = labels\n        self.batch_size = batch_size\n        self.indexes = np.arange(len(self.list_IDs))\n        \n    def __len__(self):\n        return int(np.floor(len(self.list_IDs)\/self.batch_size))\n    \n    def __getitem__(self, index):\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n        X, y = self.__data_generation(list_IDs_temp)\n        X = X.reshape((self.batch_size, 1000, 2880\/\/2))\n        return X, y\n    \n    def __data_generation(self, list_IDs_temp):\n        X = np.empty((self.batch_size, 2880000\/\/2))\n        y = np.empty((self.batch_size, 24))\n        for i, ID in enumerate(list_IDs_temp):\n            audio_file, audio_sr = read_flac_file(self.path, ID)\n            audio_file_fft = data_fft = np.abs(np.fft.fft(audio_file)[: len(audio_file)\/\/2])\n            X[i, ] = audio_file_fft\n            y[i, ] = self.labels.loc[ID.split('.')[0]]\n        return X, y","229adcdc":"batch_size = 24","8c5f2873":"epochs = 2\nlearning_rate = 1e-3","9f839def":"model = Sequential()\nmodel.add(Conv1D(128, input_shape=(1000, 2880\/\/2,), kernel_size=5, strides=4, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool1D(pool_size=(4)))\nmodel.add(Conv1D(128, kernel_size=3, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool1D(pool_size=(4)))\nmodel.add(Conv1D(128, kernel_size=3, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool1D(pool_size=(4)))\nmodel.add(Conv1D(256, kernel_size=3, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Flatten())\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dense(24, activation='softmax'))","bb31356b":"model.summary()","0776aae2":"model.compile(optimizer = Adam(lr=learning_rate),\n              loss='binary_crossentropy',\n              metrics=['binary_accuracy'])","19ce0800":"train_generator = DataGenerator(path+'train\/', train_audio_files, y_train, batch_size)","2638e0ee":"history = model.fit_generator(generator=train_generator,\n                              epochs = epochs,\n                              workers=4\n                             )","405cd526":"y_test = pd.read_csv(path+'sample_submission.csv', index_col=0)","2ed4e56f":"test_generator = DataGenerator(path+'test\/', test_audio_files, y_test, batch_size)","c429cade":"y_pred = model.predict_generator(test_generator, verbose=1)","ec46d99d":"samp_subm[samp_subm.columns[1:25]] = y_pred","c0cd1532":"samp_subm.to_csv('submission.csv', index=False)","ca353b9a":"# A Sample File","c34371c7":"# Define Model","1e75085e":"Distribution of the feature songtype_id:","5bb47b56":"# Load the CSV and Audio Data","0a8715f5":"Distribution of the feature species_id:","aa7f7fa3":"Plot [spectrogram](https:\/\/en.wikipedia.org\/wiki\/Spectrogram) with mel scaling:","04745f6c":"# Exploratory Data Analysis","b360f2ec":"# Path","7b400e38":"> Thanks to @drcapa Rico Hoffman for this amazing starter notebook from which I made some slight changes and modifications. He organized it pretty well. Thanks a lot. Go to this link to see his notebook - \nhttps:\/\/www.kaggle.com\/drcapa\/species-audio-detection-starter-keras\/","2fe24949":"# Training the model","5aad44f1":"# Predicting Test Data","03d10fff":"# Data Preprocessing","39bb1223":"# Saving the Data for Submission","68f2191a":"## Audio Data Generator","0553a68e":"Load example audio file:","021f26fa":"# Import Necessary Libraries","be094405":"There could be more than one species for one audio file:"}}