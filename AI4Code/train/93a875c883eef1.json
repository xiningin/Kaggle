{"cell_type":{"6b2dfd7a":"code","3f31ff79":"code","480b2bf2":"code","e15612de":"code","a1cb4990":"code","81574cf4":"code","871dc622":"code","1212392e":"code","c72b1cc8":"code","fad42453":"code","2e844400":"code","bffefbf4":"code","7aa1387f":"code","cf361074":"code","7bf61b19":"code","a1f90143":"code","d91ef54a":"code","62e4a263":"code","10fb2879":"code","a7def9c1":"code","b6a72832":"code","64ac5f25":"code","afd518f2":"code","f46b77f7":"code","330c4080":"code","63883a02":"code","9951f809":"code","e0a46769":"markdown","26b33483":"markdown","b309aa1c":"markdown","78312e3c":"markdown","be9784bd":"markdown","54bd33d8":"markdown","01aa56dd":"markdown","424d2768":"markdown"},"source":{"6b2dfd7a":"import os\n\nimport numpy as np\nimport pandas as pd\nfrom glob import glob\nimport tensorflow as tf\nfrom PIL import ImageFont\nfrom typing import List, Tuple\nfrom collections import Counter\nimport plotly.graph_objects as go\nfrom matplotlib import pyplot as plt\nfrom plotly.subplots import make_subplots\nfrom kaggle_datasets import KaggleDatasets\nimport seaborn as sns\nimport plotly.express as px\nimport tensorflow_addons as tfa\nfrom glob import glob","3f31ff79":"from sklearn.model_selection import GroupKFold\nfrom sklearn.model_selection import train_test_split\nfrom skmultilearn.model_selection import iterative_train_test_split","480b2bf2":"train_df = pd.read_csv(\"..\/input\/hpasinglelabelcellcsv\/singlelabelcellonly.csv\")\ntrain_df.head()","e15612de":"train_df","a1cb4990":"BASE_DIR = KaggleDatasets().get_gcs_path('hpa-single-label-cell-level-tfrecords')","81574cf4":"IMG_DIR = os.path.join(BASE_DIR , 'tfrecords\/')\n\n","871dc622":"IMG_DIR","1212392e":"TRAIN_TFRECORDS = tf.io.gfile.glob(os.path.join(IMG_DIR, '*.tfrec'))","c72b1cc8":"TRAIN_TFRECORDS","fad42453":"class TFRecordLoader:\n\n    def __init__(self, image_size: List[int], n_classes: int):\n        self.image_size = image_size\n        self.n_classes = n_classes\n        \n\n    def _parse_image(self, image):\n        image = tf.image.decode_png(image, channels=3)\n        image = tf.cast(image, dtype=tf.float32) \/ 255.0\n        image = tf.image.resize(image, self.image_size)\n        \n        return image\n\n    def _parse_label(self, label):\n        indices = tf.strings.to_number(\n            label       \n        )\n        indices =tf.cast(indices ,dtype = tf.uint8)\n        return tf.one_hot(indices, depth=self.n_classes)\n        \n\n    def _make_example(self, example):\n        feature_format = {\n            'image': tf.io.FixedLenFeature([], dtype=tf.string),\n            'image_name': tf.io.FixedLenFeature([], dtype=tf.string),\n            'target': tf.io.FixedLenFeature([], dtype=tf.string)\n        }\n        features = tf.io.parse_single_example(example, features=feature_format)\n        image = self._parse_image(features['image'])\n        image_name = features['image_name']\n        label = self._parse_label(features['target'])\n        return image,  label\n\n   \n    \n\n    def get_dataset(self, train_tfrecord_files: List[str], ignore_order: bool = False):\n        options = tf.data.Options()\n        options.experimental_deterministic = False\n        dataset = tf.data.TFRecordDataset(\n            train_tfrecord_files, num_parallel_reads=tf.data.AUTOTUNE)\n        dataset = dataset.with_options(options) if ignore_order else dataset\n        dataset = dataset.map(\n            map_func=self._make_example, num_parallel_calls=tf.data.AUTOTUNE)\n        #dataset = self._preprocess(dataset)\n        return dataset","2e844400":"class AugmentationFactory:\n\n    def __init__(self, include_flips: bool, include_rotation: bool, include_jitter: bool):\n        self.include_flips = include_flips\n        self.include_rotation = include_rotation\n        self.include_jitter = include_jitter\n\n    @staticmethod\n    def _flip_horizontal(image, seed):\n        image = tf.image.stateless_random_flip_left_right(image, seed)\n        return image\n\n    @staticmethod\n    def _flip_vertical(image, seed):\n        image = tf.image.stateless_random_flip_up_down(image, seed)\n        return image\n\n    @staticmethod\n    def _rotate(image):\n        rotation_k = tf.random.uniform((1,), minval=0, maxval=4, dtype=tf.int32)[0]\n        image = tf.image.rot90(image, k=rotation_k)\n        return image\n\n    @staticmethod\n    def _random_jitter(image, seed):\n        image = tf.image.stateless_random_saturation(image, 0.9, 1.1, seed)\n        image = tf.image.stateless_random_brightness(image, 0.075, seed)\n        image = tf.image.stateless_random_contrast(image, 0.9, 1.1, seed)\n        return image\n\n    def _map_augmentations(self, image, label):\n        seed = tf.random.uniform((2,), minval=0, maxval=100, dtype=tf.int32)\n        if self.include_flips:\n            image = self._flip_horizontal(image=image, seed=seed)\n            image = self._flip_vertical(image=image, seed=seed)\n        image = self._rotate(image=image) if self.include_rotation else image\n        image = self._random_jitter(image=image, seed=seed) if self.include_jitter else image\n        return image, label\n\n    def augment_dataset(self, dataset):\n        return dataset.map(\n            map_func=self._map_augmentations,\n            num_parallel_calls=tf.data.AUTOTUNE\n        )","bffefbf4":"loader = TFRecordLoader(\n    image_size=[224, 224], n_classes=19, \n)\ndataset = loader.get_dataset(TRAIN_TFRECORDS)\n\naugmentation_factory = AugmentationFactory(\n    include_flips=True, include_rotation=True, include_jitter=True\n)\ndataset = augmentation_factory.augment_dataset(dataset)","7aa1387f":"for x in dataset.take(1):\n    plt.imshow(x[0])","cf361074":"def get_strategy():\n    try:  # detect TPUs\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n        tf.config.experimental_connect_to_cluster(tpu)\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    except ValueError:  # detect GPUs\n        strategy = tf.distribute.MirroredStrategy()  # for GPU or multi-GPU machines\n    print(\"Number of accelerators: \", strategy.num_replicas_in_sync)\n    return strategy","7bf61b19":"# train_labels= tf.dtypes.cast(train_labels ,  dtype = tf.float32)\n# valid_labels = tf.dtypes.cast(valid_labels ,  dtype = tf.float32)","a1f90143":"#IMSIZE = (224, 240, 260, 300, 380, 456, 528, 600)\nIMSIZE = 224\n","d91ef54a":"def configure_train_dataset(augmented_dataset, shuffle_buffer: int = 128, batch_size: int = 16):\n    dataset = augmented_dataset.repeat()\n    dataset = augmented_dataset.shuffle(shuffle_buffer)\n    dataset = dataset.batch(batch_size)\n    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n    return dataset","62e4a263":"def configure_val_dataset(augmented_dataset, shuffle_buffer: int = 128, batch_size: int = 16):\n    dataset = augmented_dataset.repeat()\n    dataset = augmented_dataset.shuffle(shuffle_buffer)\n    dataset = dataset.batch(batch_size)\n    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n    return dataset","10fb2879":"def get_backbone(efficientnet_name=\"efficientnet_b0\", input_shape=(224,224,3), include_top=False, weights=\"imagenet\", pooling=\"avg\"):\n    if \"b0\" in efficientnet_name:\n        eb = tf.keras.applications.EfficientNetB0(\n            include_top=include_top, weights=weights, pooling=pooling, input_shape=input_shape\n            )\n    elif \"b1\" in efficientnet_name:\n        eb = tf.keras.applications.EfficientNetB1(\n            include_top=include_top, weights=weights, pooling=pooling, input_shape=input_shape\n            )\n    elif \"b2\" in efficientnet_name:\n        eb = tf.keras.applications.EfficientNetB2(\n            include_top=include_top, weights=weights, pooling=pooling, input_shape=input_shape\n            )\n    elif \"b3\" in efficientnet_name:\n        eb = tf.keras.applications.EfficientNetB3(\n            include_top=include_top, weights=weights, pooling=pooling, input_shape=input_shape\n            )\n    elif \"b4\" in efficientnet_name:\n        eb = tf.keras.applications.EfficientNetB4(\n            include_top=include_top, weights=weights, pooling=pooling, input_shape=input_shape\n            )\n    elif \"b5\" in efficientnet_name:\n        eb = tf.keras.applications.EfficientNetB5(\n            include_top=include_top, weights=weights, pooling=pooling, input_shape=input_shape\n            )\n    elif \"b6\" in efficientnet_name:\n        eb = tf.keras.applications.EfficientNetB6(\n            include_top=include_top, weights=weights, pooling=pooling, input_shape=input_shape\n            )\n    elif \"b7\" in efficientnet_name:\n        eb = tf.keras.applications.EfficientNetB7(\n            include_top=include_top, weights=weights, pooling=pooling, input_shape=input_shape\n            )\n    else:\n        raise ValueError(\"Invalid EfficientNet Name!!!\")\n    return eb\n\n\ndef add_head_to_bb(bb, n_classes=19, dropout=0.05, head_layer_nodes=(512,)):\n    x = tf.keras.layers.BatchNormalization()(bb.output)\n    x = tf.keras.layers.Dropout(dropout)(x)\n    \n    for n_nodes in head_layer_nodes:\n        x = tf.keras.layers.Dense(n_nodes, activation=\"relu\")(x)\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.Dropout(dropout\/2)(x)\n    \n    output = tf.keras.layers.Dense(n_classes, activation=\"sigmoid\")(x)\n    return tf.keras.Model(inputs=bb.inputs, outputs=output)\n\n\n#eb.compile(optimizer=OPTIMIZER, loss=LOSS_FN, metrics=[\"acc\", tf.keras.metrics.AUC(name=\"auc\", multi_label=True)])","a7def9c1":"\ndef is_test(x, y):\n    return x % 5 == 0\n\ndef is_train(x, y):\n    return not is_test(x, y)\n\nrecover = lambda x,y: y\n\n","b6a72832":"strategy = get_strategy()","64ac5f25":"loader = TFRecordLoader(\n    image_size=[IMSIZE, IMSIZE], n_classes=19\n    \n)\ndataset = loader.get_dataset(\n    TRAIN_TFRECORDS, ignore_order=True\n)\n\nval_dataset = dataset.enumerate() \\\n                    .filter(is_test) \\\n                    .map(recover)\n\ntrain_dataset = dataset.enumerate() \\\n                    .filter(is_train) \\\n                    .map(recover)\n        \n\naugmentation_factory = AugmentationFactory(\n    include_flips=True, include_rotation=False, include_jitter=True)\n\ntrain_dataset = augmentation_factory.augment_dataset(train_dataset)\nBATCH_SIZE = 64 * strategy.num_replicas_in_sync\ntrain_dataset = configure_train_dataset(\n    train_dataset, batch_size=BATCH_SIZE\n)\n\nval_dataset = configure_val_dataset(val_dataset, batch_size = BATCH_SIZE )\n","afd518f2":"    \nwith strategy.scope():\n    eff = get_backbone(\"b0\")\n    model = add_head_to_bb(eff, n_classes=19, dropout=0.5) \n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(),\n        loss=tf.keras.losses.BinaryCrossentropy(),\n        metrics=[tf.keras.metrics.AUC(multi_label=True)])\n        \n    model.summary()","f46b77f7":"#steps_per_epoch = train_paths.shape[0] \/\/ BATCH_SIZE\ncheckpoint = tf.keras.callbacks.ModelCheckpoint(\n    'effb7model.h5', save_best_only=True, monitor='val_loss', mode='min')\nlr_reducer = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor=\"val_loss\", patience=3, min_lr=1e-6, mode='min')\n","330c4080":"history = model.fit(\n    train_dataset, \n    epochs=10,\n    verbose=1,\n    callbacks=[checkpoint, lr_reducer],\n    \n    validation_data=val_dataset)","63883a02":"hist_df = pd.DataFrame(history.history)\nhist_df.to_csv('history.csv')","9951f809":"\nsave_locally = tf.saved_model.SaveOptions(experimental_io_device='\/job:localhost')\nmodel.save('.\/model', options=save_locally) # saving in Tensorflow's \"SavedModel\" format","e0a46769":"Since we do not have the length of our TFRecords dataset (it is a prefetch dataset) , I use the following functions to make a train\/test split in our dataset. is_test returns 1 out of every 5 examples , is_train returns the remaining 4 out of 5. This results in a 80-20 train-test split of our dataset.","26b33483":"### Training the Model","b309aa1c":"## Dependencies","78312e3c":"We won't be needing the train_df here , as the labels we need are present in the TFRecords itself. ","be9784bd":"Steps per epoch is unknown to us as the length of the dataset is unknown , however after the first epoch it is calculated automatically by TF , so the first epoch will show x\/unknown for the number of steps during the run.","54bd33d8":"Code for the model was taken from [this notebook](https:\/\/www.kaggle.com\/dschettler8845\/hpa-cellwise-classification-training\/data).\nThanks @dschettler8845 for all the amazing work in this competition! :)","01aa56dd":"# This kernel is a training baseline using the dataset provided by @ayuraj. \n# I have made the dataset into TFRecords which I shall be using for this kernel.","424d2768":"I have taken the following functions and some functions from this really neat [kernel](https:\/\/www.kaggle.com\/soumikrakshit\/hpa-baseline-on-tpu). Thanks @soumikrakshit."}}