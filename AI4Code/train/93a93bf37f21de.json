{"cell_type":{"58faa1aa":"code","eb1ec72a":"code","765f880a":"code","e7bdfae7":"code","e3632b99":"code","2c1c7174":"code","b7b972d5":"markdown","027693d8":"markdown","5971a4a5":"markdown","10222bc3":"markdown"},"source":{"58faa1aa":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom lightgbm import LGBMRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error","eb1ec72a":"%%time\ndata = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","765f880a":"%%time\ndef to_hist_func(row):\n    return np.bincount(row, minlength=30)\n\nfeatures = [f for f in data.columns if f not in ['target', 'ID']]\n\nhist_data = np.apply_along_axis(\n    func1d=to_hist_func, \n    axis=1, \n    arr=(np.log1p(data[features])).astype(int)) ","e7bdfae7":"%%time\nhist_test = np.apply_along_axis(\n    func1d=to_hist_func, \n    axis=1, \n    arr=(np.log1p(test[features])).astype(int)) ","e3632b99":"folds = KFold(n_splits=5, shuffle=True, random_state=1)\noof_preds = np.zeros(data.shape[0])\nsub_preds = np.zeros(test.shape[0])\n\nfor n_fold, (trn_, val_) in enumerate(folds.split(hist_data)):\n    reg = ExtraTreesRegressor(\n        n_estimators=1000, \n        max_features=.8,                       \n        max_depth=12, \n        min_samples_leaf=10, \n        random_state=3, \n        n_jobs=-1\n    )\n    # Fit Extra Trees\n    reg.fit(hist_data[trn_], np.log1p(data['target'].iloc[trn_]))\n    # Get OOF predictions\n    oof_preds[val_] = reg.predict(hist_data[val_])\n    # Update TEST predictions\n    sub_preds += reg.predict(hist_test) \/ folds.n_splits\n    # Display fold's score\n    print('Fold %d scores : TRN %.4f TST %.4f'\n          % (n_fold + 1,\n             mean_squared_error(np.log1p(data['target'].iloc[trn_]),\n                                reg.predict(hist_data[trn_])) ** .5,\n             mean_squared_error(np.log1p(data['target'].iloc[val_]),\n                                reg.predict(hist_data[val_])) ** .5))\n          \nprint('Full OOF score : %.4f' % (mean_squared_error(np.log1p(data['target']), oof_preds) ** .5))","2c1c7174":"test['target'] = np.expm1(sub_preds)\ntest[['ID', 'target']].to_csv('histogram_predictions.csv', index=False)","b7b972d5":"Read data?","027693d8":"Transform samples to histogram\n\nIt may sound like bins are created independently for each row when in fact not.\n\nnp.bincount  will create all integer bins up to the max value in the list, i.e. if max value is 5 it will create bins 0, 1, 2, 3, 4 and 5 columns. \n\nI set the number of bins to 30 to make sure all returned bins are contain data for 0s up to 29\n\n30 is above the max value in data and test. \n\nSo although the bin process is independent, the resulting columns are the same across rows.\n\ndoc is [here](https:\/\/docs.scipy.org\/doc\/numpy\/reference\/generated\/numpy.bincount.html)","5971a4a5":"Let's try to fit a model on this ","10222bc3":"Scripts and discussions have pointed out the fact that value was as important as features if not more. All this originated from Giba's **magic** script and post:\n - [Script](https:\/\/www.kaggle.com\/titericz\/giba-countvectorizer-d-lb-1-43)\n - [Post](https:\/\/www.kaggle.com\/c\/santander-value-prediction-challenge\/discussion\/61071)\n\nHere is a very simple and wuick script that would use the samples histogram as features.\n\nThe script uses np.apply_along_axis that is lot quicker than pd.apply !"}}