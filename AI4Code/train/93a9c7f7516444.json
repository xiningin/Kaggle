{"cell_type":{"1c854fcc":"code","1b075542":"code","a0f6dddd":"code","6b6a3fb4":"code","64190dbf":"code","848cf8bb":"code","6e3e267a":"code","6a74c5a3":"code","a90166ad":"code","734cc80a":"code","65792be8":"code","43aa381c":"code","629da535":"code","8daed935":"code","9db98895":"code","dd323154":"code","93dd706f":"code","d5228110":"code","7ea67ae6":"code","695e3c56":"code","ea7c06c9":"code","59a83694":"code","bbb1e568":"code","6b11a05b":"code","30ad85ee":"code","c6a793f0":"code","7440eb3d":"code","7d407b7e":"code","fd46d43f":"code","0dc9b3aa":"code","bd027685":"code","882c4977":"code","65425dd8":"code","56328d78":"code","1a184a51":"code","72a45d60":"code","4f34846f":"code","67429900":"code","aad1ae0f":"code","c38ca4d9":"code","d6c62a9d":"code","aa589743":"code","90a18d09":"code","9cae77bb":"code","83e22d26":"code","ffaa1035":"code","09dd6847":"code","3efb349f":"code","6b3e30b5":"code","5159f587":"code","0df46e57":"code","33bd09e2":"code","8581d01e":"code","f709d3de":"code","3fed203c":"code","fe75c14b":"code","dd92d55c":"code","31a584e1":"code","ddeda482":"code","1c005716":"code","dff77732":"code","cea7be5e":"code","5651652e":"code","5836d269":"code","7a420423":"code","9b098f7a":"code","2934c3ae":"code","c7bfd4e0":"code","8a2b0fc5":"code","5090fa38":"code","08aa441e":"code","416c6164":"code","af10f331":"code","1c0efb78":"code","852c9f99":"code","1a3eb2cb":"code","80e6f5ad":"code","e8f52bd6":"code","e8e535b0":"code","996c0906":"code","c6a4a5a6":"code","c54dfaf6":"code","b79b97aa":"code","f077f794":"code","c7e2350c":"markdown","40d6b9ba":"markdown","2603e5ca":"markdown","9bb253b0":"markdown","365ea100":"markdown","611515fa":"markdown","ae98023b":"markdown","51683241":"markdown","38003018":"markdown","2a73d48b":"markdown","b034cc62":"markdown","c00b43b6":"markdown","bdd1a7c6":"markdown","c49f0db6":"markdown","451cacae":"markdown","d38c8eb5":"markdown","bc7406e4":"markdown","4f0df128":"markdown","3925ec71":"markdown","96cdb22c":"markdown","dd029c4f":"markdown"},"source":{"1c854fcc":"import numpy as np\nimport pandas as pd","1b075542":"import matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns","a0f6dddd":"from sklearn import metrics\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report","6b6a3fb4":"import os\nprint(os.listdir(\"..\/input\"))\n\ntrain=pd.read_csv(\"..\/input\/train.csv\")","64190dbf":"train.shape","848cf8bb":"train.head(3)","6e3e267a":"#train['rooms'].value_counts().plot(kind='bar', hue='Target')\nsns.countplot(x=\"rooms\", hue= \"Target\", data=train, palette=\"coolwarm\")\nplt.xlabel(\"No. of Rooms\", fontsize=10)\nplt.ylabel(\"No. of Household\", fontsize=10)\nplt.title(\"No. of Rooms in Households in different poverty Class\", fontsize=15)\nplt.show()","6a74c5a3":"train['Target'].value_counts().plot(kind='pie',  autopct='%1.1f%%')\nplt.show()","a90166ad":"sns.countplot(x=\"r4h3\", hue= \"Target\", data=train, palette=\"coolwarm\")\nplt.xlabel(\"No. of Males\", fontsize=10)\nplt.ylabel(\"No. of Household\", fontsize=10)\nplt.title(\"No. of Males in Households in different poverty Class\", fontsize=15)\nplt.show()","734cc80a":"train.boxplot(column='r4h3', by='Target',patch_artist=True, )\nplt.grid(True)\nplt.xlabel(\"Class\")\nplt.ylabel(\"Males \")\nplt.title(\"Boxplot of Males by Poverty Class \")\nplt.suptitle(\"\")\nplt.show() ","65792be8":"sns.distplot( train[\"r4t3\"], color= 'green',  hist= True, rug= True, bins=15).grid(True)\nplt.xlabel(\"Total persons in the household\")\n#plt.ylabel(\"Males \")\nplt.title(\"Household Size \")\nplt.suptitle(\"\")\nplt.show() ","43aa381c":"sns.violinplot( x=train[\"Target\"], y=train[\"meaneduc\"], linewidth=1)\nplt.show()","629da535":"#Rent paid\nsns.violinplot( x=train[\"Target\"], y=train[\"v2a1\"], linewidth=1)\nplt.show()","8daed935":"sns.countplot(x=\"refrig\", hue= \"Target\", data=train, palette=\"coolwarm\")\nplt.xlabel(\"Refrigerators\", fontsize=10)\nplt.ylabel(\"No. of Household\", fontsize=10)\nplt.title(\"No. of Refrigerators in Households in different poverty Class\", fontsize=15)\nplt.show()","9db98895":"# Create correlation matrix\n#Subset only to the columns where parentesco1 == 1 because \n#this is the head of household, the correct label for each household.\nheads = train.loc[train['parentesco1'] == 1].copy()\ncorr_matrix = heads.corr()\n\n# Select upper triangle of correlation matrix\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n\n# Find index of feature columns with correlation greater than 0.95\nto_drop = [column for column in upper.columns if any(abs(upper[column]) > 0.95)]\n\nto_drop","dd323154":"corr_matrix.loc[corr_matrix['tamhog'].abs() > 0.9, corr_matrix['tamhog'].abs() > 0.9]","93dd706f":"train.drop(['Id','idhogar','r4t3','tamhog','tamviv','hogar_total', 'SQBmeaned', 'SQBhogar_total',\n            'SQBage','SQBescolari','SQBedjefe','SQBhogar_nin','SQBovercrowding','SQBdependency',\n            'SQBmeaned','agesq'], inplace = True, axis=1)\ntrain.shape","d5228110":"#pd.DataFrame(train.isnull().sum())","7ea67ae6":"train.columns[train.isnull().sum()!=0]","695e3c56":"#Replace the na values with the mean value of each variable\ntrain['v2a1'] = train['v2a1'].fillna((train['v2a1'].mean()))\ntrain['v18q1'] = train['v18q1'].fillna((train['v18q1'].mean()))\ntrain['rez_esc'] = train['rez_esc'].fillna((train['rez_esc'].mean()))\ntrain['meaneduc'] = train['meaneduc'].fillna((train['meaneduc'].mean()))\n#Check if any na\ntrain.columns[train.isnull().sum()!=0]","ea7c06c9":"train.select_dtypes('object').head()","59a83694":"# Converting the string to float\nyes_no_map = {'no':0,'yes':1}\ntrain['dependency'] = train['dependency'].replace(yes_no_map).astype(np.float32)\ntrain['edjefe'] = train['edjefe'].replace(yes_no_map).astype(np.float32)\ntrain['edjefa'] = train['edjefa'].replace(yes_no_map).astype(np.float32)","bbb1e568":"# Testing for String \ntrain.select_dtypes('object').head()","6b11a05b":"# Splitting data into dependent and independent variable\n# X is the independent variables matrix\nX = train.drop('Target', axis = 1)\n\n# y is the dependent variable vector\ny = train.Target","30ad85ee":"# Scaling Features\nfrom sklearn.preprocessing import StandardScaler\nss = StandardScaler()\n\nX_ss = ss.fit_transform(X)","c6a793f0":"from sklearn.decomposition import PCA","7440eb3d":"pca = PCA(n_components=0.95)\nX_PCA = pca.fit_transform(X_ss)","7d407b7e":"# split into train\/test and resample the data\nXdt_train, Xdt_test, ydt_train, ydt_test = train_test_split(X_PCA, y, random_state=1)","fd46d43f":"Xdt_test.shape","0dc9b3aa":"from sklearn.ensemble import RandomForestClassifier","bd027685":"anotherModel1 = RandomForestClassifier(n_estimators=100, max_features=2, oob_score=True, random_state=42)\nanotherModel1 = anotherModel1.fit(Xdt_train, ydt_train)","882c4977":"ydt_pred1 = anotherModel1.predict(Xdt_test)\nydt_pred1","65425dd8":"con_mat_dt = metrics.confusion_matrix(ydt_test, ydt_pred1)\n#print(con_mat_dt)\nsns.heatmap(con_mat_dt,annot=True,cmap='Blues', fmt='g')\nplt.title('RFM Confusion Matrix')\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\n#plt.grid(True)\nplt.show()","56328d78":"print('    Accuracy Report: Random Forest Model\\n', classification_report(ydt_test, ydt_pred1))","1a184a51":"from sklearn.tree import DecisionTreeClassifier","72a45d60":"anotherModel2 = DecisionTreeClassifier(max_depth=3, random_state=42)\nanotherModel2 = anotherModel2.fit(Xdt_train, ydt_train)","4f34846f":"ydt_pred2 = anotherModel2.predict(Xdt_test)\nydt_pred2","67429900":"con_mat_dt = metrics.confusion_matrix(ydt_test, ydt_pred2)\nsns.heatmap(con_mat_dt,annot=True,cmap='Greens', fmt='g')\nplt.title('Decision Tree Confusion Matrix')\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\nplt.show()","aad1ae0f":"print('    Accuracy Report: Decision Tree Model\\n', classification_report(ydt_test, ydt_pred2))","c38ca4d9":"from sklearn.ensemble import GradientBoostingClassifier as gbm","d6c62a9d":"anotherModel3 = gbm()\nanotherModel3 = anotherModel3.fit(Xdt_train, ydt_train)","aa589743":"ydt_pred3 = anotherModel3.predict(Xdt_test)\nydt_pred3","90a18d09":"con_mat_dt = metrics.confusion_matrix(ydt_test, ydt_pred3)\nsns.heatmap(con_mat_dt,annot=True,cmap='Reds', fmt='g')\nplt.title('Decision Tree Confusion Matrix')\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\nplt.show()","9cae77bb":"print('    Accuracy Report: Gradient Boost Model\\n', classification_report(ydt_test, ydt_pred3))","83e22d26":"from sklearn.neighbors import KNeighborsClassifier","ffaa1035":"anotherModel4 = KNeighborsClassifier(n_neighbors=4)\nanotherModel4 = anotherModel4.fit(Xdt_train, ydt_train)","09dd6847":"ydt_pred4 = anotherModel4.predict(Xdt_test)\nydt_pred4","3efb349f":"con_mat_dt = metrics.confusion_matrix(ydt_test, ydt_pred4)\nsns.heatmap(con_mat_dt,annot=True,cmap='YlGnBu', fmt='g')\nplt.title('K Neighbors Confusion Matrix')\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\nplt.show()","6b3e30b5":"print('    Accuracy Report: K Neighbors Model\\n', classification_report(ydt_test, ydt_pred4))","5159f587":"import lightgbm as lgb","0df46e57":"anotherModel5 = lgb.LGBMClassifier()\nanotherModel5 = anotherModel5.fit(Xdt_train, ydt_train)","33bd09e2":"ydt_pred5 = anotherModel5.predict(Xdt_test)\nydt_pred5","8581d01e":"con_mat_dt = metrics.confusion_matrix(ydt_test, ydt_pred5)\nsns.heatmap(con_mat_dt,annot=True,cmap='BuGn_r', fmt='g')\nplt.title('Light GBM Confusion Matrix')\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\nplt.show()","f709d3de":"print('    Accuracy Report: Light GBM Model\\n', classification_report(ydt_test, ydt_pred5))","3fed203c":"from sklearn.linear_model import LogisticRegression","fe75c14b":"anotherModel6 = LogisticRegression(C=0.1, penalty='l1')\nanotherModel6 = anotherModel6.fit(Xdt_train, ydt_train)","dd92d55c":"ydt_pred6 = anotherModel6.predict(Xdt_test)\nydt_pred6","31a584e1":"con_mat_dt = metrics.confusion_matrix(ydt_test, ydt_pred6)\nsns.heatmap(con_mat_dt,annot=True,cmap='Purples', fmt='g')\nplt.title('Logistic Regressioin with L1 Penalty Matrix')\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\nplt.show()","ddeda482":"print('    Accuracy Report: Logistic Regressioin with L1 Penalty Model\\n', classification_report(ydt_test, ydt_pred6))","1c005716":"from sklearn.ensemble import ExtraTreesClassifier","dff77732":"anotherModel7 = ExtraTreesClassifier()\nanotherModel7 = anotherModel7.fit(Xdt_train, ydt_train)","cea7be5e":"ydt_pred7 = anotherModel7.predict(Xdt_test)\nydt_pred7","5651652e":"con_mat_dt = metrics.confusion_matrix(ydt_test, ydt_pred7)\nsns.heatmap(con_mat_dt,annot=True,cmap='Oranges', fmt='g')\nplt.title('Logistic Extra Trees Confusion Matrix')\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\nplt.show()","5836d269":"print('    Accuracy Report: Extra Trees Model\\n', classification_report(ydt_test, ydt_pred7))","7a420423":"from xgboost.sklearn import XGBClassifier as XGB","9b098f7a":"anotherModel8 = XGB()\nanotherModel8 = anotherModel8.fit(Xdt_train, ydt_train)","2934c3ae":"ydt_pred8 = anotherModel8.predict(Xdt_test)\nydt_pred8","c7bfd4e0":"con_mat_dt = metrics.confusion_matrix(ydt_test, ydt_pred8)\nsns.heatmap(con_mat_dt,annot=True,cmap='BuGn', fmt='g')\nplt.title('XGB Confusion Matrix')\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\nplt.show()","8a2b0fc5":"print('    Accuracy Report: XGB Model\\n', classification_report(ydt_test, ydt_pred8))","5090fa38":"from bayes_opt import BayesianOptimization\nfrom skopt import BayesSearchCV as BayesSCV","08aa441e":"bayes_tuner = BayesSCV(RandomForestClassifier(n_jobs = 2),\n\n    #  Estimator parameters to be change\/tune\n    {\n        'n_estimators': (100, 500),           \n        'criterion': ['gini', 'entropy'],    \n        'max_depth': (4, 100),               \n        'max_features' : (10,64),             \n        'min_weight_fraction_leaf' : (0,0.5, 'uniform')   \n    },\n\n    # 2.13\n    n_iter=32,            \n    cv = 3               \n)","416c6164":"#bayes_cv_tuner.fit(Xdt_train, ydt_train)","af10f331":"test=pd.read_csv(\"..\/input\/test.csv\")\n#test=pd.read_table(\"E:\\\\Big Data\\\\Costa Rica\\\\Data\\\\test.csv\", engine='python', sep=',')","1c0efb78":"test.drop(['Id','idhogar','r4t3','tamhog','tamviv','hogar_total', 'SQBmeaned', 'SQBhogar_total',\n            'SQBage','SQBescolari','SQBedjefe','SQBhogar_nin','SQBovercrowding','SQBdependency',\n            'SQBmeaned','agesq'], inplace = True, axis=1)\ntest.shape","852c9f99":"test.columns[test.isnull().sum()!=0]","1a3eb2cb":"#Replace the na values with the mean value of each variable\ntest['v2a1'] = test['v2a1'].fillna((test['v2a1'].mean()))\ntest['v18q1'] = test['v18q1'].fillna((test['v18q1'].mean()))\ntest['rez_esc'] = test['rez_esc'].fillna((test['rez_esc'].mean()))\ntest['meaneduc'] = test['meaneduc'].fillna((test['meaneduc'].mean()))\n#Check if any na\ntest.columns[test.isnull().sum()!=0]","80e6f5ad":"test.select_dtypes('object').head()","e8f52bd6":"# Converting the string to float\nyes_no_map = {'no':0,'yes':1}\ntest['dependency'] = test['dependency'].replace(yes_no_map).astype(np.float32)\ntest['edjefe'] = test['edjefe'].replace(yes_no_map).astype(np.float32)\ntest['edjefa'] = test['edjefa'].replace(yes_no_map).astype(np.float32)","e8e535b0":"test_ss = ss.fit_transform(test)","996c0906":"test_ss.shape","c6a4a5a6":"pca = PCA(n_components=83)\ntest_PCA = pca.fit_transform(test_ss)","c54dfaf6":"ydt_pred41 = anotherModel4.predict(test_PCA)\nydt_pred41","b79b97aa":"unique_elements, counts_elements = np.unique(ydt_pred41, return_counts=True)\nprint(np.asarray((unique_elements, counts_elements)))","f077f794":"#Saving as tab - seperated values\nydt_pred41.tofile('submit.csv', sep='\\t')","c7e2350c":"### Filling the missing values","40d6b9ba":"### Interpretation\nThere are several variables here having to do with the size of the house:\n\nr4t3: Total persons in the household\n\ntamhog: size of the household\n\ntamviv: number of persons living in the household\n\nhhsize: household size\n\nhogar_total: total individuals in the household\n\nThese variables are all highly correlated with one another. In fact, hhsize has a perfect correlation with tamhog and hogar_total. We will remove these two variables because the information is redundant. We can also remove r4t3 because it has a near perfect correlation with hhsize.\n\nSQBhogar: Total individuals in the household and its square are highly correlated, hence no need to keep SQBhogar in data. \n\nSQBage and Age variables are highly correlated, hence no need to keep SQBage in data.\n\nSimilarly the following other 'Squared' columns are correlated to the underlying data, hence may be dropped:\nSQBescolari, escolari squared\nSQBedjefe, edjefe squared\nSQBhogar_nin, hogar_nin squared\nSQBovercrowding, overcrowding squared\nSQBdependency, dependency squared\nSQBmeaned, square of the mean years of education of adults (>=18) in the household\nagesq, Age squared\n\nId and Idhogar can also be dropped.\n\n#### The Columns are dropped before PCA so that Machine time for PCA is reduced.","2603e5ca":"### Dropping Variables","9bb253b0":"## Modelling","365ea100":"### 1. Random Forest Classifier","611515fa":"### Split in Train and Test","ae98023b":"## Feature Engineering","51683241":"The Kernel:\n\na. Explores the data and perform data visualization\n\nb. Fill in missing values using mean.\n\nc. Perform feature engineering. \n\nd. Perform PCA.\n\ne. Apply the following  estimators:\n\n        GradientBoostingClassifier\n        RandomForestClassifier\n        KNeighborsClassifier\n        ExtraTreesClassifier\n        XGBoost\n        LightGBM\n\nThis is a supervised multi-class classification machine learning problem. Labels are discrete values with 4 classes.","38003018":"### 6. Logistic Regressioin with L1 Penalty","2a73d48b":"### Variables Type having Missing values\nv2a1 - monthly rent\n\nv18q1 - number of tablets\n\nrez_esc - years behind school\n\nmeaneduc - mean education for adults\n\n#### None of them are categorical variable.","b034cc62":"### 2. Decision Tree Classifier","c00b43b6":"# Costa Rican Household Poverty Level Prediction","bdd1a7c6":"### Finding Object (String) in the Data frame and converting them to Float","c49f0db6":"### Data Preperation","451cacae":"### 4. K Neighbors Classifier","d38c8eb5":"### 3. Gradient Boost Classifier","bc7406e4":"### 5. Light GBM","4f0df128":"### 7. Extra Trees Classifier","3925ec71":"#### The maximum F1 value is obtained by Model 4. K Neighbors Classifier.\nApplying it to the test data","96cdb22c":"### 8. XGB Classifier ","dd029c4f":"## PCA"}}