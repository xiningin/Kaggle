{"cell_type":{"05bf849b":"code","cd0f1109":"code","28b38bb4":"code","319b9951":"code","23581e9b":"code","e4b93479":"code","52d15580":"code","5e6e7374":"code","3d779974":"code","89d82cbd":"code","3a94bc79":"code","cdbc5848":"code","dbf5bb25":"code","52a6cb40":"code","e02afc93":"code","47d77256":"code","1a2ea361":"code","2d2904a9":"code","d57ccb38":"code","32440e55":"code","23d5ff9e":"code","b1219313":"code","32eea007":"code","bc039db8":"code","7fb740a1":"code","8558f0e0":"code","a6179b82":"code","8e423f2f":"code","731b09fb":"code","77317e4f":"code","bb3efc89":"code","49d9396b":"code","27612d85":"code","ad48934d":"code","7c7bb806":"code","87845ec4":"code","91115316":"code","9a88106b":"code","eaee1829":"code","235f183b":"code","25338545":"code","a897d5f6":"code","95debad6":"code","cdb0c277":"code","08babc2d":"code","80e49945":"code","fc630590":"code","c24e92d5":"code","f1fc71e7":"code","cafce9a9":"code","ae848d40":"code","cd96d0db":"code","ee6226da":"code","49a495f8":"code","15aa15a4":"code","21aeadf9":"code","b068a089":"code","f34ad8ea":"code","0365c7e2":"code","8118ab8a":"code","74849581":"code","d12c4f50":"code","fb34ec82":"code","ffd68a07":"code","b804e497":"code","6feef7b0":"code","6a9c5c1a":"code","01bdbb7c":"code","70db93bf":"markdown","72a35370":"markdown"},"source":{"05bf849b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","cd0f1109":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","28b38bb4":"dat = pd.read_csv(\"\/kaggle\/input\/wineuci\/Wine.csv\", names= [ 'Cultivator','Alcohol', 'Malic acid','Ash','Alcalinity_of_ash','Magnesium',\n                                        'Total_phenols','Flavanoids','Nonflavanoid_phenols','Proanthocyanins'\n                                        ,'Color_intensity','Hue','diluted_wines','Proline'])","319b9951":"dat.head()","23581e9b":"plt.figure(figsize=(12,12))\nsns.heatmap(dat.corr(),annot=True)\nplt.show()","e4b93479":"cor= dat.corr()\n#Correlation with output variable\ncor_target = abs(cor[\"Cultivator\"])\n#Selecting highly correlated features\nrelevant_features = cor_target[cor_target>0.5]\nrelevant_features","52d15580":"i = sns.pairplot(dat, vars = ['Alcalinity_of_ash', 'Total_phenols', 'Flavanoids', 'Hue', 'diluted_wines', 'Proline'] ,hue='Cultivator', palette='husl')\nplt.show()","5e6e7374":"# Wow, all the selected features shows distinctive varaition in the proprotion to estimate the cultivator","3d779974":"# Removing the target\ndata= dat.iloc[:,1:]","89d82cbd":"from sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler","3a94bc79":"sc= StandardScaler()\ndata_scaled= sc.fit_transform(data)","cdbc5848":"num_cluster= range(1,15)\nculster_error=[]\n\nfor cluster in num_cluster:\n    model= KMeans(n_clusters=cluster,n_init=10)\n    model.fit(data_scaled)\n    label= model.labels_\n    centroids=model.cluster_centers_\n    culster_error.append(model.inertia_)","dbf5bb25":"cluster_df= pd.DataFrame({\"num_cluster\": num_cluster, \"error\": culster_error})","52a6cb40":"cluster_df","e02afc93":"# Elbow Curve\nplt.plot(cluster_df[\"num_cluster\"],cluster_df[\"error\"],marker=\"*\")\nplt.show()","47d77256":"    # Optimal Cluster point is 3\n    model= KMeans(n_clusters=3,n_init=10)\n    model.fit(data_scaled)\n","1a2ea361":"centroids= pd.DataFrame(model.cluster_centers_ , columns= list(data.columns))\ncentroids","2d2904a9":"data['Ktest_label']=model.labels_","d57ccb38":"data['Ktest_label'].value_counts()","32440e55":"dat['Cultivator'].value_counts()","23d5ff9e":"# Lets Try a base prediction with the help of logistic regression as it binary classification","b1219313":"X= data.iloc[:,:-1]\ny= data['Ktest_label']","32eea007":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split","bc039db8":"X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=1)","7fb740a1":"print(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)","8558f0e0":"y_test.value_counts()","a6179b82":"y_train.value_counts()","8e423f2f":"lr=LogisticRegression(random_state=1)\nlr.fit(X_train,y_train)","731b09fb":"test_pred= lr.predict(X_test)","77317e4f":"from sklearn.metrics import confusion_matrix,classification_report","bb3efc89":"y_test.value_counts()","49d9396b":"confusion_matrix(y_test,test_pred)","27612d85":"Precision = 23\/25 *100  # True Positive\/ True Positive and False Negative predicted\nprint(Precision)\nRecall    =  23\/24 *100\nprint(Recall)","ad48934d":"Precision = 16\/18 *100  # True Positive\/ True Positive and False Negative predicted\nprint(Precision)\nRecall    =  16\/18 *100\nprint(Recall)","7c7bb806":"Precision = 11\/11 *100  # True Positive\/ True Positive and False Negative predicted\nprint(Precision)\nRecall    =  11\/12 *100\nprint(Recall)","87845ec4":"print(classification_report(y_test,test_pred))","91115316":"# Lets give a try with the Kmean again to see whether they are classifying correctly","9a88106b":"kmeans=KMeans(n_clusters=3,n_init=10,random_state=1)\nkmeans.fit(X_train,y_train)","eaee1829":"pred_test=kmeans.predict(X_test)\npred_train=kmeans.predict(X_train)","235f183b":"print(classification_report(y_train,pred_train))","25338545":"print(classification_report(y_test,pred_test))","a897d5f6":"y_test.value_counts()","95debad6":"confusion_matrix(y_test,pred_test)","cdb0c277":"Precision = 18\/18 *100  # True Positive\/ True Positive and False Negative predicted\nprint(Precision)\nRecall    =  18\/24 *100\nprint(Recall)","08babc2d":"Precision = 13\/19 *100  # True Positive\/ True Positive and False Negative predicted\nprint(Precision)\nRecall    =  13\/18 *100\nprint(Recall)","80e49945":"Precision = 6\/17 *100  # True Positive\/ True Positive and False Negative predicted\nprint(Precision)\nRecall    =  6\/12 *100\nprint(Recall)","fc630590":"# K_Mean even though we performed Clustering the prediction is worst for both train and test data","c24e92d5":"# Hierarchical Clustering to classifiy the model better\nfrom scipy.cluster.hierarchy import dendrogram,linkage\nfrom sklearn.cluster.hierarchical import AgglomerativeClustering\nfrom scipy.stats import zscore","f1fc71e7":"X= X.apply(zscore)","cafce9a9":"plt.figure(figsize=(22,14))\nZ= linkage(X,method='ward')\ndendrogram(Z)\nplt.show()","ae848d40":"clus = AgglomerativeClustering(n_clusters=3)\nhs=clus.fit_predict(X)","cd96d0db":"clus = KMeans(n_clusters=3)\ncc=clus.fit_predict(X)","ee6226da":"df_k=data.copy(deep=True)\ndf_k['label'] = cc","49a495f8":"df_h=data.copy(deep=True)\ndf_h['label'] = hs","15aa15a4":"data","21aeadf9":"df_h['label'].value_counts() # Hirerchical Clustering","b068a089":"df_k['label'].value_counts() # Kmeans Clustering","f34ad8ea":"dat['Cultivator'].value_counts() # Actual Value","0365c7e2":"pd.DataFrame(kmeans.cluster_centers_,columns=list(X.columns)) # Estimating the centroids of the cluster","8118ab8a":"dat['Cultivator'].value_counts()","74849581":"plt.figure(figsize=(12,12))\nplt.title('Original Classes')\nsns.scatterplot(x='Flavanoids', y='diluted_wines', hue='Cultivator', data=dat,palette=\"rocket\")\nplt.show()\nplt.figure(figsize=(12,12))\nplt.title('K-Means Classes')\nsns.scatterplot(x='Flavanoids', y='diluted_wines', hue='label', data=df_k,palette=\"rocket\")\nplt.show()\nplt.figure(figsize=(12,12))\nplt.title('Hierarchical Classes')\nsns.scatterplot(x='Flavanoids', y='diluted_wines', hue='label', data=df_h,palette=\"rocket\")\nplt.show()\n","d12c4f50":"centroids= pd.DataFrame(kmeans.cluster_centers_, columns=list(X.columns))\ncentroids[['Flavanoids','diluted_wines']]","fb34ec82":"# Inference Hirerachical Clustering is performing the best compared to KMeans Clustering","ffd68a07":"# 3 plot\nfrom mpl_toolkits.mplot3d import Axes3D","b804e497":"data","6feef7b0":"df_scaled= pd.DataFrame(data_scaled,columns=list(X.columns))","6a9c5c1a":"fig = plt.figure(figsize=(10,8))\nax= Axes3D(fig, rect=[0,0,1,1],elev=20,azim=150)\nlabels = df_k['label']\nax.scatter(df_scaled['Flavanoids'],df_scaled['diluted_wines'],df_scaled['Total_phenols'],\n          c= labels.astype(np.float),edgecolor='k')\nax.w_xaxis.set_ticklabels([])\nax.w_yaxis.set_ticklabels([])\nax.w_zaxis.set_ticklabels([])\n\nax.set_xlabel(\"Flavanoids\")\nax.set_ylabel(\"diluted_wines\")\nax.set_zlabel(\"Total_phenols\")\nax.set_title(\"3D plot of KMeansClustering\")","01bdbb7c":"fig = plt.figure(figsize=(10,8))\nax= Axes3D(fig, rect=[0,0,1,1],elev=20,azim=200)\nlabels = df_h['label']\nax.scatter(df_scaled['Flavanoids'],df_scaled['diluted_wines'],df_scaled['Total_phenols'],\n          c= labels.astype(np.float),edgecolor='k')\nax.w_xaxis.set_ticklabels([])\nax.w_yaxis.set_ticklabels([])\nax.w_zaxis.set_ticklabels([])\n\nax.set_xlabel(\"Flavanoids\")\nax.set_ylabel(\"diluted_wines\")\nax.set_zlabel(\"Total_phenols\")\nax.set_title(\"3D plot of Hirerachical Clustering\")","70db93bf":"## Let's Remove the target and try to estimate the same with the clustering techniques\n### K_Mean","72a35370":"### Selcting the correlated features for visualisation\n* > Cultivator           1.000000\n* > Alcalinity_of_ash    0.517859\n* > Total_phenols        0.719163\n* > Flavanoids           0.847498\n* > Hue                  0.617369\n* > diluted_wines        0.788230\n* > Proline              0.633717"}}