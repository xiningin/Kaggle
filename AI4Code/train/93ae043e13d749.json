{"cell_type":{"78f2dd9b":"code","feb31bec":"code","0a4eb530":"code","d2dcb3b7":"code","a0ded3f5":"code","d84cd491":"code","141db2bb":"code","185029e9":"code","c6f4d9d8":"code","339a3ee6":"code","78a49da2":"code","4157279c":"code","4bcd95d1":"code","462d1a41":"code","7e07c97e":"code","26c08d27":"code","975c1ffd":"code","f9edeeb6":"code","216deb29":"markdown","60ab3001":"markdown","21cb0348":"markdown","24843cb5":"markdown","a0ddfe24":"markdown","9dc23133":"markdown","713cbaf8":"markdown"},"source":{"78f2dd9b":"# To enable plotting graphs in Jupyter notebook\n%matplotlib inline ","feb31bec":"import pandas as pd\nfrom sklearn.linear_model import LogisticRegression\n\n# importing ploting libraries\nimport matplotlib.pyplot as plt   \n\n#importing seaborn for statistical plots\nimport seaborn as sns\n\n#Let us break the X and y dataframes into training set and test set. For this we will use\n#Sklearn package's data splitting function which is based on random function\n\nfrom sklearn.model_selection import train_test_split\n\nimport numpy as np\n\n\n# calculate accuracy measures and confusion matrix\nfrom sklearn import metrics","0a4eb530":"# Since it is a data file with no header, we will supply the column names which have been obtained from the above URL \n# Create a python list of column names called \"names\"\n\n#Load the file from local directory using pd.read_csv which is a special form of read_table\n\npima_df = pd.read_csv(\"..\/input\/pima-indians-diabetes-database\/diabetes.csv\")","d2dcb3b7":"pima_df.head()","a0ded3f5":"# Let us check whether any of the columns has any value other than numeric i.e. data is not corrupted such as a \"?\" instead of \n# a number.\n\n# we use np.isreal a numpy function which checks each column for each row and returns a bool array, \n# where True if input element is real.\n# applymap is pandas dataframe function that applies the np.isreal function columnwise\n# Following line selects those rows which have some non-numeric value in any of the columns hence the  ~ symbol\n\npima_df[~pima_df.applymap(np.isreal).all(1)]","d84cd491":"# replace the missing values in pima_df with median value :Note, we do not need to specify the column names\n# every column's missing value is replaced with that column's median respectively\n#pima_df = pima_df.fillna(pima_df.median())\n#pima_df","141db2bb":"#Lets analysze the distribution of the various attributes\npima_df.describe()","185029e9":"# Let us look at the target column which is 'class' to understand how the data is distributed amongst the various values\npima_df.groupby([\"Outcome\"]).count()\n\n# Most are not diabetic. The ratio is almost 1:2 in favor or class 0.  The model's ability to predict class 0 will \n# be better than predicting class 1. ","c6f4d9d8":"# Let us do a correlation analysis among the different dimensions and also each dimension with the dependent dimension\n# This is done using scatter matrix function which creates a dashboard reflecting useful information about the dimensions\n# The result can be stored as a .png file and opened in say, paint to get a larger view \n\n#pima_df_attr = pima_df.iloc[:,0:9]\n\n#axes = pd.plotting.scatter_matrix(pima_df_attr)\n#plt.tight_layout()\n#plt.savefig('d:\\greatlakes\\pima_pairpanel.png')","339a3ee6":"# Pairplot using sns\n\n#sns.pairplot(pima_df)","78a49da2":"#data for all the attributes are skewed, especially for the variable \"Insulin\"\n\n#The mean for Insulin is 80(rounded) while the median is 30.5 which clearly indicates an extreme long tail on the right\n","4157279c":"# Attributes which look normally distributed (plas, pres, skin, and mass).\n# Some of the attributes look like they may have an exponential distribution (preg, insulin, pedi, age).\n# Age should probably have a normal distribution, the constraints on the data collection may have skewed the distribution.\n\n# There is no obvious relationship between age and onset of diabetes.\n# There is no obvious relationship between pedi function and onset of diabetes.\n","4bcd95d1":"array = pima_df.values\nX = pima_df.iloc[:,0:8]\ny = pima_df.iloc[:,8]\n#X = array[:,0:8] # select all rows and first 8 columns which are the attributes\n#Y = array[:,8]   # select all rows and the 8th column which is the classification \"Yes\", \"No\" for diabeties\ntest_size = 0.30 # taking 70:30 training and test set\nseed =1 # Random numbmer seeding for reapeatability of the code\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=seed)\n","462d1a41":"# Fit the model on 30%\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\ny_predict = model.predict(X_test)\n\ncoeff_df = pd.DataFrame(model.coef_)\ncoeff_df['intercept'] = model.intercept_\nprint(coeff_df)","7e07c97e":"model_score = model.score(X_test, y_test)\nprint(model_score)\nprint(metrics.confusion_matrix(y_test, y_predict))\n\n","26c08d27":"# Improve the model -----------------------------Iteration 2 -----------------------------------------------","975c1ffd":"# To scale the dimensions we need scale function which is part of scikit preprocessing libraries\n\nfrom sklearn import preprocessing\n\n# scale all the columns of the mpg_df. This will produce a numpy array\n#pima_df_scaled = preprocessing.scale(pima_df[0:7])\nX_train_scaled = preprocessing.scale(X_train)\nX_test_scaled = preprocessing.scale(X_test)","f9edeeb6":"# Fit the model on 30%\nmodel = LogisticRegression()\nmodel.fit(X_train_scaled, y_train)\ny_predict = model.predict(X_test_scaled)\nmodel_score = model.score(X_test_scaled, y_test)\nprint(model_score)\n\n\n# IMPORTANT: first argument is true values, second argument is predicted values\n# this produces a 2x2 numpy array (matrix)\nprint(metrics.confusion_matrix(y_test, y_predict))","216deb29":"# 2). Analyse the dataset","60ab3001":"In this example we have to rightly predict whether a native PIMA indian person has diabetes or not based on features shared","21cb0348":"# 1). Import necessary libraries and dataset","24843cb5":"    Analyzing the confusion matrix\n\nTrue Positives (TP): we correctly predicted that they do have diabetes 132\n\nTrue Negatives (TN): we correctly predicted that they don't have diabetes 48\n\nFalse Positives (FP): we incorrectly predicted that they do have diabetes (a \"Type I error\") 37\nFalsely predict positive Type I error\n\n\nFalse Negatives (FN): we incorrectly predicted that they don't have diabetes (a \"Type II error\") 14\nFalsely predict negative Type II error","a0ddfe24":"# 4). Further improve this model","9dc23133":"# 3). Model the dataset","713cbaf8":"Model score will help us determine accuracy of model"}}