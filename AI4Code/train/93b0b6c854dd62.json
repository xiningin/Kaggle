{"cell_type":{"0db7d7e0":"code","fb8a6316":"code","733ce5bc":"code","bb55fcf1":"code","c8fbd2b5":"code","90c18e33":"code","b166c633":"code","ce507120":"code","3e5f88d1":"code","ef1e0b7a":"code","5d40534d":"code","1e0598e9":"code","584410d5":"code","a83d9cec":"code","a3338ce3":"code","76d10a83":"code","a12df159":"code","c2f8ed58":"markdown","639a6c21":"markdown","15ce8b6a":"markdown","3b0b29d8":"markdown","4e624cb5":"markdown","b8ae09e5":"markdown","1520d2c7":"markdown","2d788ee7":"markdown","f7e2a041":"markdown","2e109f38":"markdown","b33ae315":"markdown","36cb948f":"markdown","64552e03":"markdown","035b4e8e":"markdown","d6375d02":"markdown"},"source":{"0db7d7e0":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\nfrom sklearn.model_selection import train_test_split\n\n\nfrom keras.utils.np_utils import to_categorical # convert to one-hot-encoding\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\nfrom keras.optimizers import RMSprop\nfrom keras.callbacks import ReduceLROnPlateau\n\nnp.random.seed(2)","fb8a6316":"# Load the data\ntrain = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")","733ce5bc":"Y_train = train[\"label\"]\nX_train = train.drop(labels = [\"label\"], axis = 1) # all except for label column\n\n\ng = sns.countplot(Y_train)\nY_train.value_counts() # ","bb55fcf1":"X_train.isnull().any().describe()","c8fbd2b5":"test.isnull().any().describe()","90c18e33":"# from 0-255 to 0-1\nX_train = X_train \/ 255.0\ntest = test \/ 255.0","b166c633":"# Reshape image  (height = 28px, width = 28px , canal = 1)\nX_train = X_train.values.reshape(-1,28,28,1)\ntest = test.values.reshape(-1,28,28,1)","ce507120":"# Encode labels to one hot vectors (ex : 2 -> [0,0,1,0,0,0,0,0,0,0])\nY_train = to_categorical(Y_train, num_classes = 10)","3e5f88d1":"random_seed = 2\nX_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.15, random_state=random_seed)","ef1e0b7a":"g = plt.imshow(X_train[0][:,:,0])","5d40534d":"# In -> [[Conv2D->relu]*2 -> MaxPool2D -> Dropout]*2 -> Flatten -> Dense -> Dropout -> Out\n\nmodel = Sequential()\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n                 activation ='relu', input_shape = (28,28,1))) #32 filters for the first conv2D layer\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n                 activation ='relu')) #32 filters for the 2nd conv2D layer\nmodel.add(MaxPool2D(pool_size=(2,2))) #the area size pooled each time\nmodel.add(Dropout(0.25)) #regularization parameter= proportion of nodes in the layer are randomly ignored (setting their weights to zero) for each training sample. \n\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'))\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Flatten()) #to convert the final feature maps into a one single 1D vector\nmodel.add(Dense(256, activation = \"relu\")) #activation function max(0,x), used to add non linearity to the network\nmodel.add(Dropout(0.5))\nmodel.add(Dense(10, activation = \"softmax\")) #outputs distribution of probability of each class","1e0598e9":"# Define the optimizer\n# loss function and an optimisation algorithm\noptimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0) # faster than Stochastic Gradient Descent ('sgd') optimizer\n","584410d5":"# Compile the model\nmodel.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])","a83d9cec":"# Set a learning rate annealer\n# reduce the LR by half if the accuracy is not improved after 3 epochs\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n                                            patience=3, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.00001)\nepochs = 10 # \nbatch_size = 86","a3338ce3":"history = model.fit(X_train, Y_train, batch_size = batch_size, epochs = epochs, \n          validation_data = (X_val, Y_val), verbose = 2)","76d10a83":"# predict results\nresults = model.predict(test)\n\n# select the index with the maximum probability\nresults = np.argmax(results,axis = 1)\n\nresults = pd.Series(results,name=\"Label\")","a12df159":"submission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\n\nsubmission.to_csv(\"cnn_mnist_datagen.csv\",index=False)","c2f8ed58":"We have ap. even number of all digits","639a6c21":"## 1.6 Splitting into training and valid\nation set","15ce8b6a":"## 1.3 Grayscale Normalization","3b0b29d8":"## 1.7 Example Images","4e624cb5":"No missing values in the train and test datasets.","b8ae09e5":"# Submitting predictions","1520d2c7":"## 1.2 Check for null values","2d788ee7":"## 1.4 Reshaping","f7e2a041":"# 2. Modeling\n## 2.1 CNN model ","2e109f38":"We need to encode these lables to one hot vectors (ex : 2 -> [0,0,1,0,0,0,0,0,0,0]).","b33ae315":"## 1.1 Load Data","36cb948f":"# 1. Data preparation","64552e03":"## 1.5 Label encoding ","035b4e8e":"All data are in 28x28x1 3D matrices now (Keras requires one more dimension in the end which correspond to channels, only one channel is needed for gray scale images.","d6375d02":"Validation accuracy = 0.9781"}}