{"cell_type":{"3dd22cc5":"code","171971c4":"code","ecce7f8f":"code","81be9d1e":"code","df5d4356":"code","9ea7827e":"code","ed509fe4":"code","61cba1c0":"code","03bc9811":"markdown","15f66f52":"markdown","3d6e2291":"markdown","99a21d4c":"markdown","83b06f48":"markdown"},"source":{"3dd22cc5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","171971c4":"train=pd.read_csv('\/kaggle\/input\/webclubrecruitment2019\/TRAIN_DATA.csv',index_col='Unnamed: 0')\ntest=pd.read_csv('\/kaggle\/input\/webclubrecruitment2019\/TEST_DATA.csv')\nID=test['Unnamed: 0']\ny=train['Class']\ntrain.drop(columns=['Class'],inplace=True)\ntest.drop(columns=['Unnamed: 0'],inplace=True)","ecce7f8f":"train.head()","81be9d1e":"missing=train.isnull().sum()\nprint(missing[missing>0])","df5d4356":"cat_cols=['V2','V3','V4','V5','V7','V8','V9','V16','V11']\nnum_cols=set(train.columns)-set(cat_cols)\nnum_cols=list(num_cols)\nprint('no. of categorical columns is {}'.format(len(cat_cols)))\nprint('no. of numerical columns is {}'.format(len(num_cols)))\nun=train[cat_cols].nunique()\nprint(un)","9ea7827e":"from sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\n\nOHE=OneHotEncoder(handle_unknown='ignore',sparse=False)\n\ncat_features_train=pd.DataFrame(OHE.fit_transform(train[cat_cols]))\ncat_features_test=pd.DataFrame(OHE.transform(test[cat_cols]))\ncat_features_train.index=train.index\ncat_features_test.index=test.index\nprint(cat_features_train.shape)\n\nnum_train=train[num_cols]\nnum_test=test[num_cols]\n\nX=pd.concat([num_train,cat_features_train],axis=1)\nX_test=pd.concat([num_test,cat_features_test],axis=1)\nprint(X.shape)\n","ed509fe4":"from xgboost import XGBClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\n\n\nxgbcl=XGBClassifier(random_state=2)\nkfold=StratifiedKFold(n_splits=3,shuffle=True,random_state=12)\ngsc=GridSearchCV(xgbcl,param_grid={'max_depth':[3,4,5,6,8],'n_estimators':[50,100,150,200,250,300],'learning_rate':[0.1,0.5,0.01,0.05]},scoring='roc_auc',cv=kfold,verbose=1,n_jobs=-1)\n\ngrid_result=gsc.fit(X,y)\nprint('best set of parameters is {}'.format(grid_result.best_params_))\n\n","61cba1c0":"pred=gsc.predict_proba(X_test)\nprint(pred.shape)\nresult=pd.DataFrame({'Id': ID , 'PredictedValue' : pred[:,1] })\nresult.to_csv('output.csv',index = False)","03bc9811":"Preprocessing","15f66f52":"Predicting probabilities, writing a csv file","3d6e2291":"using XGBClassifier and GridSearchCV to determine best n_estimators ,max_depth and learning rate.I'm using xgbclassifier since ive used xgbregressor before , and it performed much better than a random forest regressor.","99a21d4c":"separating categorical and numerical variables","83b06f48":"Checking for missing values....output shows none."}}