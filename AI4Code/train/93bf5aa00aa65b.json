{"cell_type":{"9878f15c":"code","2a1774b4":"code","d3742af5":"code","c5d3fde4":"code","ac3ec0b0":"code","104ac87b":"code","59c3a1e8":"code","640794bb":"code","57c79c46":"code","0e841bb2":"code","ca0a56f4":"code","753ed5a7":"code","e7e18ec4":"code","65896afc":"code","f7f01208":"code","6f76dbca":"code","14d11cc8":"code","b0712fa1":"code","36fd7cae":"code","3e4155d1":"code","0069e245":"code","51a583d4":"code","af596390":"code","73aac0f0":"code","21a4b834":"code","459161ac":"code","639bc706":"code","95f9d37b":"code","25f751d5":"code","bea1a194":"code","bbd85a74":"code","1f01df56":"code","28cb942c":"code","bdc8c5bf":"code","49cf4881":"code","f1da6b91":"code","b11ba77a":"code","ca029ecd":"code","7cdd040d":"code","1ac086ff":"code","034aafb3":"code","7533f17f":"code","48afcbec":"code","35ea1d3b":"code","633c6bc1":"code","e1ab5914":"code","7752c6f4":"code","01e03414":"code","23e3c2e0":"code","e33a6d34":"code","67e7bfb1":"code","a51ece49":"code","02f54397":"code","4f3a808b":"code","843c2178":"code","0f1c5f6a":"code","05579b43":"code","3d188527":"code","c9dd1d46":"code","d2cd5052":"code","408a928e":"code","6f5ffbea":"code","d7660a1c":"code","59cf32ec":"code","24b2aefa":"code","86bd29f8":"code","70a51f5e":"code","84be340b":"code","a4739bf8":"code","85cdcf74":"code","9c94a1ed":"code","46c9adde":"code","d42eab2f":"markdown","c8442bd2":"markdown","f3c922a0":"markdown","fe887883":"markdown","ff76da62":"markdown","76795ce1":"markdown","4de52d46":"markdown","3b9843a9":"markdown","bb598b7c":"markdown","7b70a5e3":"markdown","dfd5c57a":"markdown","192444ee":"markdown","55b94191":"markdown","2f223f0a":"markdown","ba0185f7":"markdown","7eb06e29":"markdown","802d1b37":"markdown","f2800fca":"markdown","9da06e96":"markdown","1637f73c":"markdown","1dfd6837":"markdown","3f9da22b":"markdown","b6b97e4b":"markdown","0830e847":"markdown","8545bf1f":"markdown","3b1ac473":"markdown","425ef42c":"markdown","98230d2e":"markdown","4a66ea72":"markdown","6ed476fd":"markdown","7edf4c74":"markdown","ee949493":"markdown","d73baca9":"markdown","af6a9c66":"markdown","ef9c2cc4":"markdown","265298f0":"markdown","32c7c8fc":"markdown","452c6b43":"markdown","28623dd1":"markdown","e3e17ce6":"markdown","1bfc3f19":"markdown","baf54ec3":"markdown","278cbac9":"markdown","9fc16a4d":"markdown"},"source":{"9878f15c":"import tensorflow\nimport tensorflow as tf\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import StratifiedKFold\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import ModelCheckpoint\n#from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\nfrom tensorflow.keras.applications.densenet import DenseNet121\n\nimport os\n#from tensorflow.python.keras.applications.nasnet import preprocess_input\nfrom tqdm import tqdm","2a1774b4":"#model_prefix = 'MobileNetV2'\nmodel_prefix = 'DenseNet121'\n\nBATCH_SIZE = 32\n\nSIZE = 224 # MobileNetV2, ResNet50V2, MobileNet, RegNetX002, MobileNetV3Large, MobileNetV3Small, EfficientNetB7, DenseNet121\n#SIZE = 331 # NASNetLarge\n#SIZE = 299 # InceptionResNetV2, InceptionV3, Xception\n\n# For mean-std measurements -> KFOLD = True\n# For model final testing -> KFOLD = False\nKFOLD = False\n\nEPOCHS = 30\nEPOCHS_KFOLD = 30\nEPOCHS_TEST = 1000","d3742af5":"models_dir = '.\/MODELS\/'\ndir_root = '..\/input\/lyme-clean-and-dirty\/Lyme_ver03\/'\ndir_original = dir_root + 'data\/'\ndir_augmented = dir_root + 'augmented_dataset\/'\ntest_df = pd.read_csv(dir_root + 'test_data.csv')\ntrain_df = pd.read_csv(dir_root + 'training_data.csv')\naugmented_df = pd.read_csv(dir_root + 'data_augmented.csv')","c5d3fde4":"\"\"\"\nmodels_dir = '\/data1\/LYME\/MODELS\/'\ndir_root = '\/data1\/LYME\/ver_03\/'\ndir_original = dir_root + 'data\/'\ndir_augmented = dir_root + 'augmented_dataset\/'\ntest_df = pd.read_csv(dir_root + 'test_data.csv')\ntrain_df = pd.read_csv(dir_root + 'training_data.csv')\naugmented_df = pd.read_csv(dir_root + 'data_augmented.csv')\n\"\"\"","ac3ec0b0":"%%time\n\ndata = {}\n\ntarget_size = (SIZE, SIZE)\n\n\nfor i in range(len(test_df['image'])):\n    image_name = dir_original + test_df['image'][i]\n    image=tf.keras.preprocessing.image.load_img(image_name, color_mode='rgb', target_size = target_size)\n    image=np.array(image)\n    data.update({test_df['image'][i]: image})\n    \n\nfor i in range(len(train_df['image'])):\n    image_name = dir_original + train_df['image'][i]\n    image=tf.keras.preprocessing.image.load_img(image_name, color_mode='rgb', target_size = target_size)\n    image=np.array(image)\n    data.update({train_df['image'][i]: image})\n\n\nfor i in range(len(augmented_df['image'])):\n    image_name = dir_augmented + augmented_df['image'][i]\n    image=tf.keras.preprocessing.image.load_img(image_name, color_mode='rgb', target_size = target_size)\n    image=np.array(image)\n    data.update({augmented_df['image'][i]: image})\n\n\n","104ac87b":"print('train_df size: ',len(train_df))\nprint('test_df size: ',len(test_df))\n#data_df = pd.concat([test_df, train_df])\ndata_df = train_df\n\ndata_df.reset_index(drop=True, inplace=True)\ndata_df = data_df.sample(frac=1, random_state=123)\ndata_df.reset_index(drop=True, inplace=True)\n\nprint('data_original_df size: ',len(data_df))","59c3a1e8":"def show_model_results(acc_per_fold, loss_per_fold, auc_per_fold):\n    # == Provide average scores ==\n    print('------------------------------------------------------------------------')\n    print('Score per fold')\n    for i in range(0, len(acc_per_fold)):\n      print('------------------------------------------------------------------------')\n      print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]} - AUC: {auc_per_fold[i]}')\n    print('------------------------------------------------------------------------')\n    print('Average scores for all folds:')\n    print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n    print(f'> AUC: {np.mean(auc_per_fold)} (+- {np.std(auc_per_fold)})')\n    print(f'> Loss: {np.mean(loss_per_fold)} (+- {np.std(loss_per_fold)})')","640794bb":"def plot_mean_std(label, line_color, fill_color, epoch_list, mean_list, std_list, ax):\n  \n  ax.plot(\n    epoch_list,\n    mean_list,\n    color=line_color,\n    label=label, \n    lw=2,\n    alpha=0.8,\n  )\n\n  upper = (mean_list + std_list)\n  lower = (mean_list - std_list)\n\n  ax.fill_between(\n    epoch_list,\n    lower,\n    upper,\n    color=fill_color,\n    alpha=0.5,\n    label=r\"$\\pm$ 1 std. dev.\",\n  )\n\n\ndef plot_train_val_mean_std(metric_label, legend_location, metric_ylim, history_train_list_means, history_train_list_stds, history_val_list_means, history_val_list_stds, ax):\n\n\n  epochs_list = range(EPOCHS)\n\n  plot_mean_std('train', 'r', \"lightcoral\", epochs_list, history_train_list_means, history_train_list_stds, ax)\n  plot_mean_std('val', 'b', \"lightsteelblue\", epochs_list, history_val_list_means, history_val_list_stds, ax)\n\n  ax.set(\n    xlim = [0, EPOCHS-1],\n    ylim = metric_ylim,\n    title = metric_label + \" - History\",\n  )\n\n  ax.legend(loc=legend_location, ncol=2)\n\n  model_name = model_prefix + '_' + data_quality + '_' + TL_name + '_E' + str(EPOCHS) + '_' + WEIGHTS_name + '_' + DA_name    \n  figures_dir = '.\/OUTPUT\/' + model_prefix + '\/' + model_name + '\/'\n  os.makedirs(figures_dir, exist_ok=True)    \n  fig_filename = model_name + '_' + metric_label + '_history_' + add_title + '.png'\n  fig_file_path = os.path.join(figures_dir, fig_filename)\n  plt.savefig(fig_file_path, dpi=300)\n\n  plt.show()  \n","57c79c46":"class Base_Model(tf.keras.Model):\n\n  def __init__(self, target_size):\n    super().__init__()\n    self.target_size = target_size\n    self.base_model = DenseNet121(\n    #self.base_model = MobileNetV2(\n        include_top=False,\n        pooling='max', \n        weights=WEIGHTS, \n        input_shape = self.target_size)\n    \n    print(target_size)\n\n    # make the weights and biases of the base model non-trainable\n    # by \"freezing\" each layer of the BASE network\n    for layer in self.layers:\n        print(layer.name)\n        layer.trainable = TRAINABLE    \n    \n    self.flat_layer = tf.keras.layers.Flatten()\n    self.dense1_layer = tf.keras.layers.Dense(512, activation='relu')\n    self.dense2_layer = tf.keras.layers.Dense(1, activation='sigmoid')\n\n  def call(self, inputs, training=False):\n    x = self.base_model(inputs)\n    x = self.flat_layer(x)\n    x = self.dense1_layer(x)\n\n\n    return self.dense2_layer(x)\n    ","0e841bb2":"class Complex_Model():\n\n  def __init__(self, result_data, result_label, test_data, test_label, \n               save_dir, save_best_model_path, epoch_num, \n               target_size = (SIZE, SIZE, 3), num_folds = 5):\n    \n    self.X = result_data\n    self.Y = result_label\n    self.x = test_data\n    self.y = test_label\n\n    self.target_size = target_size\n    self.save_dir = save_dir\n    self.save_best_model_path = save_best_model_path\n    self.epoch_num = epoch_num\n    self.num_folds = num_folds\n\n    self.acc_per_fold = []\n    self.loss_per_fold = []\n    self.auc_per_fold = []\n\n    self.history_acc_list = []\n    self.history_loss_list = []\n    self.history_auc_list = []\n    self.history_val_acc_list = []\n    self.history_val_loss_list = []\n    self.history_val_auc_list = []\n\n\n\n  def run(self):\n\n    kfold = StratifiedKFold(n_splits = self.num_folds)\n  \n\n    from tqdm import tqdm\n    for train, test in tqdm(kfold.split(self.X, self.Y)):\n\n      model = Base_Model(self.target_size)\n      #model = create_model()\n\n      #model.compile(optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.0001), \n      model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001), \n                  loss = 'binary_crossentropy', \n                  metrics = [tf.keras.metrics.AUC(name='auc'),'acc'])\n\n\n      if DA == True:  \n        train_data_aug = ImageDataGenerator(\n          rescale=1.\/255,\n          featurewise_center=True, \n          samplewise_center=True, featurewise_std_normalization=True, samplewise_std_normalization=True,\n          zca_whitening=True, zca_epsilon=1e-06, rotation_range=90, \n          width_shift_range=1.0, height_shift_range=1.0, brightness_range=[0.5, 1.5], \n          shear_range=90.0, zoom_range=1.0, channel_shift_range=0.0, \n          fill_mode='nearest', horizontal_flip=False, vertical_flip=False\n        ).flow(self.X[train], self.Y[train])\n      else:\n        train_data_aug = ImageDataGenerator(rescale=1.\/255).flow(self.X[train], self.Y[train])\n    \n      test_data_aug = ImageDataGenerator(rescale=1.\/255).flow(self.X[test], self.Y[test])\n\n      checkpoint_AUC = ModelCheckpoint(save_dir, monitor='val_auc', verbose=2, save_best_only=True, \n                                      mode='max', save_weights_only=True)\n      \n      history = model.fit(\n        train_data_aug,\n        batch_size=BATCH_SIZE,\n        epochs=self.epoch_num,\n        verbose=2,\n        callbacks = [checkpoint_AUC],\n        validation_data=test_data_aug)\n        \n      model.summary()        \n      \n      model.load_weights(save_dir)\n      \n      scores = model.evaluate(test_data_aug, verbose=0)\n\n      self.loss_per_fold.append(scores[0])\n      self.auc_per_fold.append(scores[1])\n      self.acc_per_fold.append(scores[2])\n\n      \n      self.history_acc_list.append(history.history['acc'])\n      self.history_loss_list.append(history.history['loss'])\n      self.history_auc_list.append(history.history['auc'])\n      self.history_val_acc_list.append(history.history['val_acc'])\n      self.history_val_loss_list.append(history.history['val_loss'])\n      self.history_val_auc_list.append(history.history['val_auc'])\n\n\n  def run_full(self):\n    model = Base_Model(self.target_size)\n    #model = create_model()\n    \n    model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=0.00001), \n                  loss = 'binary_crossentropy', \n                  metrics = [tf.keras.metrics.AUC(name='auc'),'acc'])\n\n    if DA == True:  \n        train_data_aug = ImageDataGenerator(\n          rescale=1.\/255,\n          featurewise_center=True, \n          samplewise_center=True, featurewise_std_normalization=True, samplewise_std_normalization=True,\n          zca_whitening=True, zca_epsilon=1e-06, rotation_range=90, \n          width_shift_range=1.0, height_shift_range=1.0, brightness_range=[0.5, 1.5], \n          shear_range=90.0, zoom_range=1.0, channel_shift_range=0.0, \n          fill_mode='nearest', horizontal_flip=False, vertical_flip=False\n        ).flow(self.X, self.Y)\n    else:\n        train_data_aug = ImageDataGenerator(rescale=1.\/255).flow(self.X, self.Y)\n    \n    test_data_aug = ImageDataGenerator(rescale=1.\/255).flow(self.x, self.y)\n\n    checkpoint_AUC = ModelCheckpoint(save_dir, monitor='val_auc', verbose=2, save_best_only=True, \n                                      mode='max', save_weights_only=True)\n      \n    history = model.fit(\n        train_data_aug,\n        batch_size=BATCH_SIZE,\n        epochs=self.epoch_num,\n        verbose=2,\n        callbacks = [checkpoint_AUC],\n        validation_data=test_data_aug)\n        \n    model.summary()        \n      \n    model.load_weights(save_dir)\n      \n    scores = model.evaluate(test_data_aug, verbose=0)\n\n    print('**** Test BEST model ****')\n    Accuracy_string = 'Acc{acc_best:.3f}'.format(acc_best=scores[2])\n    print('Accuracy = {acc_best:.4f}'.format(acc_best=scores[2]))\n    AUC_string = 'AUC{AUC_best:.3f}'.format(AUC_best=scores[1])\n    print('AUC= {AUC_best:.4f}'.format(AUC_best=scores[1]))\n    Loss_string = 'Loss{Loss_best:.3f}'.format(Loss_best=scores[0])\n    print('Loss = {Loss_best:.4f}'.format(Loss_best=scores[0]))\n    \n    best_model_metrics_filename = self.save_best_model_path + '_' + Loss_string + '_' + Accuracy_string + '_' + AUC_string  + '_best.h5'\n    model.save_weights(best_model_metrics_filename) \n    \n    return model, best_model_metrics_filename, self.x, self.y\n    \n    \n  def show_results(self):\n    show_model_results(self.acc_per_fold, self.loss_per_fold, self.auc_per_fold)\n\n\n  def history_stat_metrics(self, history_list):\n    history_list_means = np.mean(np.asarray(history_list),axis=0)\n    history_list_stds = np.std(np.asarray(history_list),axis=0)\n    return(history_list_means, history_list_stds)\n\n\n  def plot_accuracy(self):\n\n    train_acc_list_means, train_acc_list_stds = self.history_stat_metrics(self.history_acc_list)\n    val_acc_list_means, val_acc_list_stds = self.history_stat_metrics(self.history_val_acc_list)\n\n    fig, ax = plt.subplots()\n\n    plot_train_val_mean_std('Accuracy', \"lower center\", [0.01, 1.05], train_acc_list_means, \n                        train_acc_list_stds, val_acc_list_means, val_acc_list_stds, ax)\n    \n   \n  def plot_auc(self):\n\n    train_auc_list_means, train_auc_list_stds = self.history_stat_metrics(self.history_auc_list)\n    val_auc_list_means, val_auc_list_stds = self.history_stat_metrics(self.history_val_auc_list)\n\n    fig, ax = plt.subplots()\n\n    plot_train_val_mean_std('AUC', \"lower center\", [0.01, 1.05], train_auc_list_means, train_auc_list_stds, \n                        val_auc_list_means, val_auc_list_stds, ax)\n\n\n\n  def plot_loss(self):\n\n    train_loss_list_means, train_loss_list_stds = self.history_stat_metrics(self.history_loss_list)\n    val_loss_list_means, val_loss_list_stds = self.history_stat_metrics(self.history_val_loss_list)\n\n    fig, ax = plt.subplots()\n\n    plot_train_val_mean_std('Loss', \"upper center\", [0.01, 6], train_loss_list_means, train_loss_list_stds, \n                        val_loss_list_means, val_loss_list_stds, ax) \n\n\n  def folds_mean_max_auc(self):\n    return np.mean(self.acc_per_fold), np.std(self.acc_per_fold), np.mean(self.auc_per_fold), np.std(self.auc_per_fold), np.mean(self.loss_per_fold), np.std(self.loss_per_fold)  ","ca0a56f4":"def prepare_data_growing(data_df, noisy_part, limit):\n    result_label = []\n    result_data = []\n\n    dirty_pos_lyme_count = 0\n    dirty_no_lyme_count = 0\n\n    clean_pos_lyme_count = 0\n    clean_no_lyme_count = 0\n\n    clean_count = 0\n    dirty_count = 0\n    \n    dirty_limit = limit * noisy_part\n    print('dirty_limit: ', dirty_limit)\n    clean_limit = limit - dirty_limit\n    print('clean_limit: ', clean_limit)\n    \n    num_images = len(data_df['image'])\n    print('**************************')\n    print('Subset (images): ', num_images)\n    for i in range(num_images):\n        if (data_df['clean'][i] == 1) and (data_df['lyme_positive'][i] == 1) and (clean_pos_lyme_count < clean_limit):\n            result_label.append(data_df['lyme_positive'][i])\n            result_data.append(data[data_df['image'][i]])\n            clean_pos_lyme_count += 1\n            clean_count += 1\n\n        if (data_df['clean'][i] == 1) and (data_df['lyme_positive'][i] == 0) and (clean_no_lyme_count < clean_limit):\n            result_label.append(data_df['lyme_positive'][i])\n            result_data.append(data[data_df['image'][i]])\n            clean_no_lyme_count += 1\n            clean_count += 1\n       \n        if (data_df['clean'][i] == 0) and (data_df['lyme_positive'][i] == 1) and (dirty_pos_lyme_count < dirty_limit):\n            result_label.append(data_df['lyme_positive'][i])\n            result_data.append(data[data_df['image'][i]])\n            dirty_pos_lyme_count += 1\n            dirty_count += 1\n\n        if (data_df['clean'][i] == 0) and (data_df['lyme_positive'][i] == 0) and (dirty_no_lyme_count < dirty_limit):\n            result_label.append(data_df['lyme_positive'][i])\n            result_data.append(data[data_df['image'][i]])\n            dirty_no_lyme_count += 1       \n            dirty_count += 1       \n\n    print('dirty_pos_lyme_count:', dirty_pos_lyme_count)            \n    print('dirty_no_lyme_count:', dirty_no_lyme_count)\n    print('dirty_count:', dirty_count)\n\n    print('clean_pos_lyme_count:', clean_pos_lyme_count)            \n    print('clean_no_lyme_count:', clean_no_lyme_count)    \n    print('clean_count:', clean_count)\n  \n    result_data = np.asarray(result_data).astype(np.float32)\n    result_label = np.asarray(result_label).astype(np.float32)\n\n    print('**************************')\n    print('Final state of subset ...')\n    print('pos_lyme_count: ', dirty_pos_lyme_count + clean_pos_lyme_count)\n    print('no_lyme_count: ', dirty_no_lyme_count + clean_no_lyme_count)\n    print('Noise ratio: ', dirty_count\/(dirty_count + clean_count))\n    print('Growing subset (data) size: ',len(result_data))\n    print('Growing subset (label) size: ',len(result_label))\n    print('**************************')\n\n    return result_data, result_label","753ed5a7":"def plot_noise_vs(x, y, x_label, y_label, plot_title):\n  plt.plot(x, y)\n  #plt.xticks(x)\n  plt.xlabel(x_label)\n  plt.ylabel(y_label)\n  plt.title(plot_title)\n\n  model_name = model_prefix + '_' + data_quality + '_' + TL_name + '_E' + str(EPOCHS) + '_' + WEIGHTS_name + '_' + DA_name    \n  figures_dir = '.\/OUTPUT\/' + model_prefix + '\/' + model_name + '\/'\n  os.makedirs(figures_dir, exist_ok=True)    \n  fig_filename = model_name + '_' + plot_title + '.png'\n  fig_file_path = os.path.join(figures_dir, fig_filename)\n  plt.savefig(fig_file_path, dpi=300)\n\n  plt.show()","e7e18ec4":"def plot_noise_mean_std(metric_label, legend_location, metric_ylim, x_list, list_means, list_stds, ax):\n\n    plot_mean_std('Mean', 'b', \"lightsteelblue\", x_list, list_means, list_stds, ax)\n\n    ax.set(\n        xlim = [0, len(x_list)+5],\n        ylim = metric_ylim,\n        title = metric_label + \" vs \" + x_label,\n        xlabel = x_label,\n        ylabel = metric_label\n    )\n\n    ax.legend(loc=legend_location, ncol=2)\n\n    model_name = model_prefix + '_' + data_quality + '_' + TL_name + '_E' + str(EPOCHS) + '_' + WEIGHTS_name + '_' + DA_name    \n    figures_dir = '.\/OUTPUT\/' + model_prefix + '\/' + model_name + '\/'\n    os.makedirs(figures_dir, exist_ok=True)    \n    fig_filename = model_name + '_' + metric_label + \"_vs_\" + x_label_filename + '_FILL.png'\n    fig_file_path = os.path.join(figures_dir, fig_filename)\n    plt.savefig(fig_file_path, dpi=300)\n\n    plt.show()  ","65896afc":"def run_kfold():  \n    #save_dir = '.\/MODELS\/best'\n    #save_dir = os.path.dirname(checkpoint_path)\n    #os.makedirs(save_dir, exist_ok=True)\n    \n    model_name = model_prefix + '_' + data_quality + '_' + TL_name + '_E' + str(EPOCHS) + '_' + WEIGHTS_name + '_' + DA_name\n    save_model_filename = models_dir + model_prefix + '\/' + model_name\n    figures_dir = '.\/OUTPUT\/' + model_prefix + '\/' + model_name + '\/'\n    os.makedirs(figures_dir, exist_ok=True)\n    \n    #num_list = [15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65]\n    num_list = list(range(5,46,1))\n\n    metrics_list = []\n    for n in tqdm(num_list):\n        result_data, result_label = prepare_data_growing(train_df, NOISE_PART, n)\n        add_title = 'noise' + str(NOISE_PART) + '_num' + str(n)\n        model = Complex_Model(result_data=result_data, result_label=result_label, \n                              test_data=[], test_label=[],\n                              save_dir=save_dir, save_best_model_path=save_model_filename, epoch_num = EPOCHS)\n    \n        model.run()\n        model.show_results()\n        model.plot_accuracy()\n        model.plot_loss()\n        model.plot_auc()\n    \n        metrics = model.folds_mean_max_auc()\n        metrics_list.append(metrics)   \n\n    acc_mean_list = []\n    acc_std_list = []\n    auc_mean_list = []\n    auc_std_list = []\n    loss_mean_list = []\n    loss_std_list = []\n    for n in range(len(num_list)):\n        acc_mean_list.append(metrics_list[n][0])\n        acc_std_list.append(metrics_list[n][1])\n        auc_mean_list.append(metrics_list[n][2])\n        auc_std_list.append(metrics_list[n][3])\n        loss_mean_list.append(metrics_list[n][4])\n        loss_std_list.append(metrics_list[n][5])    \n        \n    # Summary\n    summary = np.column_stack((np.array(num_list),acc_mean_list,acc_std_list,auc_mean_list,auc_std_list,loss_mean_list,loss_std_list))\n    summary_df = pd.DataFrame(summary)\n    summary_df.columns = ['num', 'acc_mean', 'acc_std', 'auc_mean', 'auc_std', 'loss_mean', 'loss_std']\n    summary_df.head()\n    summary_filename = model_name + '_summary.txt'\n    summary_file_path = os.path.join(figures_dir, summary_filename)\n    summary_df.to_csv(summary_file_path, index=False)\n        \n    plot_noise_vs(num_list, acc_mean_list, x_label, 'Accuracy (mean)', 'Accuracy_mean_vs_' + x_label_filename)\n    plot_noise_vs(num_list, acc_std_list, x_label, 'Accuracy (std)', 'Accuracy_std_vs_' + x_label_filename)\n    plot_noise_vs(num_list, auc_mean_list, x_label, 'AUC (mean)', 'AUC_mean_vs_' + x_label_filename)\n    plot_noise_vs(num_list, auc_std_list, x_label, 'AUC (std)', 'AUC_std_vs_' + x_label_filename)\n    plot_noise_vs(num_list, loss_mean_list, x_label, 'Loss (mean)', 'Loss_mean_vs_' + x_label_filename)\n    plot_noise_vs(num_list, loss_std_list, x_label, 'Loss (std)', 'Loss_std_vs_' + x_label_filename)\n    \n    fig, ax = plt.subplots()\n    plot_noise_mean_std('Accuracy', \"lower center\", [0.3, 0.9], num_list, np.array(acc_mean_list), np.array(acc_std_list), ax)   \n    fig, ax = plt.subplots()\n    plot_noise_mean_std('AUC', \"upper center\", [0.5, 1.1], num_list, np.array(auc_mean_list), np.array(auc_std_list), ax)    \n    fig, ax = plt.subplots()\n    plot_noise_mean_std('Loss', \"upper center\", [-0.2, 3.6], num_list, np.array(loss_mean_list), np.array(loss_std_list), ax)    ","f7f01208":"def run_final(test_data, test_label):  \n    \n    model_name = model_prefix + '_' + data_quality + '_' + TL_name + '_E' + str(EPOCHS) + '_' + WEIGHTS_name + '_' + DA_name\n    save_model_filename = models_dir + model_prefix + '\/' + model_name\n    figures_dir = '.\/OUTPUT\/' + model_prefix + '\/' + model_name + '\/'\n    os.makedirs(figures_dir, exist_ok=True)\n    \n    print('**************************')\n    print('Training subset ...')\n    train_data, train_label = prepare_data_growing(train_df, NOISE_PART, 45)\n    print('**************************')    \n    print('Testing subset ...')\n    print('For all testing procedures use the same: test_data, test_label')    \n    #test_data, test_label = prepare_data_growing(test_df, NOISE_PART, 20)\n    \n    add_title = 'noise' + str(NOISE_PART) + '_num' + str(45)\n    model = Complex_Model(result_data=train_data, result_label=train_label, \n                          test_data=test_data, test_label=test_label,\n                          save_dir=save_dir, save_best_model_path=save_model_filename, \n                          epoch_num = EPOCHS)\n    \n    model, best_model_metrics_filename, test_data, test_label = model.run_full()\n        \n    return model, best_model_metrics_filename","6f76dbca":"import itertools\ndef plot_confusion_matrix(cm, \n                          #cm_std, # for k-fold only\n                          classes,\n                          normalize=False,\n                          title='Means',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm_sum = cm.sum(axis=1)[:, np.newaxis]\n        cm = cm.astype('float') \/ cm_sum\n#        cm_std = cm_std.astype('float') \/ cm_sum # for k-fold only\n        print(\"Confusion matrix, NORMALIZED\")\n    else:\n        print('Confusion matrix, WITHOUT normalization')\n\n    print('Mean values:')\n    print(cm)\n\n#    print('Standard deviation values:') # for k-fold only\n#    print(cm_std)\n\n    plt.title(title)\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45, fontsize = 8)\n    plt.yticks(tick_marks, classes, fontsize = 8)\n\n    fmt = '.2f' if normalize else '.0f' # 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 #+ \"\\n\" + r'$\\pm$' + format(cm_std[i, j], fmt), # for k-fold only\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\", fontsize = 7)\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n    # Check folders\n    model_name = model_prefix + '_' + data_quality + '_' + TL_name + '_E' + str(EPOCHS) + '_' + WEIGHTS_name + '_' + DA_name\n    print('model_name: ', model_name)\n    figures_dir = '.\/OUTPUT\/' + model_prefix + '\/' + model_name + '\/'\n    os.makedirs(figures_dir, exist_ok=True)\n    print('figures_dir: ', figures_dir)    \n    \n    plt.title('')     \n    fig_filename = title + '_ConfusionMatrix.eps'\n    fig_file_path = os.path.join(figures_dir, fig_filename)\n    plt.savefig(fig_file_path, bbox_inches='tight', dpi=300)\n    \n    plt.title(title)\n    fig_filename = title + '_ConfusionMatrix.png'\n    fig_file_path = os.path.join(figures_dir, fig_filename)\n    plt.savefig(fig_file_path, bbox_inches='tight', dpi=300)","14d11cc8":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\n\ndef test_model(model, best_model_metrics_filename, test_data, test_label):\n#def test_model(model, best_model_metrics_filename, test_data_aug, test_label):\n    \n    model.load_weights(best_model_metrics_filename)\n    \n    test_data_aug = ImageDataGenerator(rescale=1.\/255).flow(test_data, test_label)\n    scores = model.evaluate(test_data_aug, verbose=1)\n    \n    print('**** EXTENDED TEST *****')\n    print(' Scores for TEST subset:')\n    Loss_string = 'Loss{Loss_best:.3f}'.format(Loss_best=scores[0])\n    print('Loss = {Loss_best:.4f}'.format(Loss_best=scores[0]))\n    AUC_string = 'AUC{AUC_best:.3f}'.format(AUC_best=scores[1])\n    print('AUC= {AUC_best:.4f}'.format(AUC_best=scores[1]))\n    Accuracy_string = 'Acc{acc_best:.3f}'.format(acc_best=scores[2])\n    print('Accuracy = {acc_best:.4f}'.format(acc_best=scores[2]))    \n\n    # CM\n    print('**** CONFUSION MATRIX *****')\n    #x_test, y_test = test_data, test_label\n\n    #score = model.evaluate(x_test, y_test)\n    #print('score:', score)\n    #predicted_probs = model.predict(x_test)\n    #print('predicted_probs:', predicted_probs)\n    \n    #samples = test_data_aug.samples\n    #nb_samples = len(samples)\n    #predicted_probs = model.predict_generator(test_data_aug, steps = np.ceil(nb_samples\/32))\n    predicted_probs = model.predict(test_data\/255)\n    #print('predicted_probs:',predicted_probs)\n\n    df = pd.DataFrame(predicted_probs)\n    predicted_probs = df[0].values.tolist()\n    predicted = [round(x) for x in predicted_probs]\n    #predicted = predicted_probs #.argmax(axis=-1)\n    #print('predicted:', predicted)\n\n    expected = test_label\n    #expected = y_test\n    #expected = y_test.argmax(axis=-1)\n    #print('expected:', expected)\n\n    # Print test of confusion matrix\n    conf_matrix = confusion_matrix(expected, predicted)\n    print(conf_matrix)      \n    \n    # Check folders\n    model_name = model_prefix + '_' + data_quality + '_' + TL_name + '_E' + str(EPOCHS) + '_' + WEIGHTS_name + '_' + DA_name\n    print('model_name: ', model_name)\n    figures_dir = '.\/OUTPUT\/' + model_prefix + '\/' + model_name + '\/'\n    os.makedirs(figures_dir, exist_ok=True)\n    print('figures_dir: ', figures_dir)\n    title = model_name\n    print('title: ', title)\n\n    BINARY = True\n\n    #class_names = ['AZU','ONU', 'IZU', 'MYSLITE']\n    #class_names = subfolder_names\n    class_names = ['Lyme','NO']\n\n    # Compute confusion matrix\n    #cnf_matrix = confusion_matrix(expected, predicted)\n    np.set_printoptions(precision=4)\n\n    print('**************************')\n    # Plot non-normalized confusion matrix\n    if BINARY == True:\n        fig, ax = plt.subplots(dpi=300, figsize=(6, 3)) \n    else: \n        fig, ax = plt.subplots(dpi=300, figsize=(15, 15))\n    \n    plot_confusion_matrix(conf_matrix, \n                      #confusion_matrix_stds, # for k-fold only\n                      classes=class_names, normalize=False,\n                      #title = MODEL_NAME + '_' + TRAIN_TITLE + '_' + DA_TYPE + '_' + SAVED_MODEL + '_NOT_normalized')\n                      title = title + '_NOT_normalized')\n\n    print('**************************')\n    # Plot normalized confusion matrix\n    if BINARY == True:\n        fig, ax = plt.subplots(dpi=300, figsize=(6, 3)) \n    else: \n        fig, ax = plt.subplots(dpi=300, figsize=(15, 15))\n    plot_confusion_matrix(conf_matrix, \n                      #confusion_matrix_stds, # for k-fold only\n                      classes=class_names, normalize=True,\n                      #title = MODEL_NAME + '_' + TRAIN_TITLE + '_' + DA_TYPE + '_' + SAVED_MODEL + '_normalized')\n                      title = title + '_normalized')\n\n    plt.show()   \n    \n    TP = conf_matrix[0][0]\n    #print(TP)\n    FN = conf_matrix[0][1]\n    FP = conf_matrix[1][0]\n    TN = conf_matrix[1][1]    \n    \n    # ROC\n    from sklearn.metrics import roc_curve, auc\n\n    #fpr = dict()\n    #tpr = dict()\n    #roc_auc = dict()\n\n    fpr, tpr, _ = roc_curve(expected, predicted_probs)\n    roc_auc = auc(fpr, tpr)\n    #roc_auc_1 = roc_auc_score(expected, predicted_probs)\n\n    fig, ax = plt.subplots(figsize=(5, 5)) \n\n    ax.plot([0, 1], [0, 1], linestyle=\"--\", lw=2, color=\"r\", label=\"Random, 0.5\", alpha=0.8)\n    ax.plot(fpr, tpr,\n         #label = \"AUC, %0.4f; , %0.4f\" % (roc_auc,roc_auc_1),\n         label = \"AUC, %0.4f\" % (roc_auc),\n         color='navy', linestyle='-', linewidth=4)\n\n    ax.legend(loc=\"lower right\", title = 'AUC')\n\n    plt.title('')     \n    fig_filename = title + '_ROC.eps'\n    fig_file_path = os.path.join(figures_dir, fig_filename)\n    plt.savefig(fig_file_path, bbox_inches='tight', dpi=300)\n    \n    plt.title(title)\n    fig_filename = title + '_ROC.png'\n    fig_file_path = os.path.join(figures_dir, fig_filename)\n    plt.savefig(fig_file_path, bbox_inches='tight', dpi=300)\n\n    plt.show()   \n    \n    # Save TEST results\n    \n    # Check folders\n    model_name = model_prefix + '_' + data_quality + '_' + TL_name + '_E' + str(EPOCHS) + '_' + WEIGHTS_name + '_' + DA_name\n    print('model_name: ', model_name)\n    figures_dir = '.\/OUTPUT\/' + model_prefix + '\/' + model_name + '\/'\n    os.makedirs(figures_dir, exist_ok=True)\n    print('figures_dir: ', figures_dir)\n\n    test_CM_AUC_filename = model_name + '_CM_AUC.csv'\n    print('test_CM_AUC_filename: ', test_CM_AUC_filename)\n    test_CM_AUC_file_path = os.path.join(figures_dir, test_CM_AUC_filename)\n\n    test_ROC_filename = model_name + '_ROC.csv'\n    print('test_results_filename: ', test_ROC_filename)\n    test_ROC_file_path = os.path.join(figures_dir, test_ROC_filename)\n\n    roc_df = pd.DataFrame(fpr.T, columns=['fpr'])\n    roc_df['tpr'] = pd.DataFrame(tpr)\n    roc_df.head()\n    roc_df.to_csv(test_ROC_file_path)\n\n    cm_auc_df = pd.DataFrame(columns=['Loss', 'AUC', 'Accuracy', 'ROC_AUC', 'TP', 'FN', 'FP', 'TN'], \n                         data=[[scores[0], scores[1], scores[2], roc_auc, TP, FN, FP, TN]])\n    cm_auc_df.head()\n    cm_auc_df.to_csv(test_CM_AUC_file_path)    ","b0712fa1":"def model_version_folders(model_prefix, data_quality, TL_name, EPOCHS, WEIGHTS_name, DA_name):\n    # Check folders\n    model_name = model_prefix + '_' + data_quality + '_' + TL_name + '_E' + str(EPOCHS) + '_' + WEIGHTS_name + '_' + DA_name\n    print('model_name: ', model_name)\n    figures_dir = '.\/OUTPUT\/' + model_prefix + '\/' + model_name + '\/'\n    os.makedirs(figures_dir, exist_ok=True)\n    print('figures_dir: ', figures_dir)\n    return model_name, figures_dir","36fd7cae":"# For all testing procedures: use the same test_data, test_label\nprint('Testing subset ...')\nNOISE_PART = 0.5\ntest_data, test_label = prepare_data_growing(test_df, NOISE_PART, 20)\ntest_data_aug = ImageDataGenerator(rescale=1.\/255).flow(test_data, test_label)","3e4155d1":"# Train from ImageNet-trainable Weights\nWEIGHTS = 'imagenet'\nWEIGHTS_name = 'imagenet'\n# Train from scratch\n#WEIGHTS = None\n#WEIGHTS_name = 'Scratch'\n\nadd_title = ''\nx_label = 'Number of images'\nx_label_filename = 'Num'    \n\n#save_dir = ''\nsave_dir = models_dir + model_prefix + '\/'\nmodel_name = ''\nsave_model_filename = ''\nfigures_dir = ''","0069e245":"# Clean dataset \nNOISE_PART = 0.0\ndata_quality = 'CLEAN'\n\n# Dirty dataset \n#NOISE_PART = 1.0\n#data_quality = 'DIRTY'\n\n# Dirty & Clean dataset \n#NOISE_PART = 0.5\n#data_quality = 'DIRTY_CLEAN'","51a583d4":"# Train from ImageNet-trainable Weights Learning (IWL) -> True\n#TRAINABLE = True\n#TL_name = 'NOTL'\n# Transfer Learning (TL) -> False\nTRAINABLE = False\nTL_name = 'TL'","af596390":"%%time\n# With Data Augmentation -> True\n#DA = True\n#DA_name = 'DA' \n# Without Data Augmentation -> False\nDA = False\nDA_name = 'NODA' \n\nEPOCHS = EPOCHS_KFOLD\nmodel_name, figures_dir = model_version_folders(model_prefix, data_quality, TL_name, EPOCHS, WEIGHTS_name, DA_name)\nif KFOLD: run_kfold()","73aac0f0":"%%time\nEPOCHS = EPOCHS_TEST\nmodel, best_model_metrics_filename = run_final(test_data, test_label)\ntest_model(model, best_model_metrics_filename, test_data, test_label)","21a4b834":"%%time\n\n# With Data Augmentation -> True\nDA = True\nDA_name = 'DA' \n# Without Data Augmentation -> False\n#DA = False\n#DA_name = 'NODA' \n\nEPOCHS = EPOCHS_KFOLD\nmodel_name, figures_dir = model_version_folders(model_prefix, data_quality, TL_name, EPOCHS, WEIGHTS_name, DA_name)\nif KFOLD: run_kfold()","459161ac":"%%time\nEPOCHS = EPOCHS_TEST\nmodel, best_model_metrics_filename = run_final(test_data, test_label)\ntest_model(model, best_model_metrics_filename, test_data, test_label)","639bc706":"# Train from ImageNet-trainable Weights Learning (IWL) -> True\nTRAINABLE = True\nTL_name = 'NOTL'\n# Transfer Learning (TL) -> False\n#TRAINABLE = False\n#TL_name = 'TL'","95f9d37b":"%%time\n# With Data Augmentation -> True\n#DA = True\n#DA_name = 'DA' \n# Without Data Augmentation -> False\nDA = False\nDA_name = 'NODA' \n\nEPOCHS = EPOCHS_KFOLD\nmodel_name, figures_dir = model_version_folders(model_prefix, data_quality, TL_name, EPOCHS, WEIGHTS_name, DA_name)\nif KFOLD: run_kfold()","25f751d5":"%%time\nEPOCHS = EPOCHS_TEST\nmodel, best_model_metrics_filename = run_final(test_data, test_label)\ntest_model(model, best_model_metrics_filename, test_data, test_label)","bea1a194":"%%time\n# With Data Augmentation -> True\nDA = True\nDA_name = 'DA' \n# Without Data Augmentation -> False\n#DA = False\n#DA_name = 'NODA' \n\nEPOCHS = EPOCHS_KFOLD\nmodel_name, figures_dir = model_version_folders(model_prefix, data_quality, TL_name, EPOCHS, WEIGHTS_name, DA_name)\nif KFOLD: run_kfold()","bbd85a74":"%%time\nEPOCHS = EPOCHS_TEST\nmodel, best_model_metrics_filename = run_final(test_data, test_label)\ntest_model(model, best_model_metrics_filename, test_data, test_label)","1f01df56":"# Clean dataset \n#NOISE_PART = 0.0\n#data_quality = 'CLEAN'\n\n# Dirty dataset \nNOISE_PART = 1.0\ndata_quality = 'DIRTY'\n\n# Dirty & Clean dataset \n#NOISE_PART = 0.5\n#data_quality = 'DIRTY_CLEAN'","28cb942c":"# Train from ImageNet-trainable Weights Learning (IWL) -> True\n#TRAINABLE = True\n#TL_name = 'NOTL'\n# Transfer Learning (TL) -> False\nTRAINABLE = False\nTL_name = 'TL'","bdc8c5bf":"%%time\n# With Data Augmentation -> True\n#DA = True\n#DA_name = 'DA' \n# Without Data Augmentation -> False\nDA = False\nDA_name = 'NODA' \n\nEPOCHS = EPOCHS_KFOLD\nmodel_name, figures_dir = model_version_folders(model_prefix, data_quality, TL_name, EPOCHS, WEIGHTS_name, DA_name)\nif KFOLD: run_kfold()","49cf4881":"%%time\nEPOCHS = EPOCHS_TEST\nmodel, best_model_metrics_filename = run_final(test_data, test_label)\ntest_model(model, best_model_metrics_filename, test_data, test_label)","f1da6b91":"%%time\n# With Data Augmentation -> True\nDA = True\nDA_name = 'DA' \n# Without Data Augmentation -> False\n#DA = False\n#DA_name = 'NODA' \n\nEPOCHS = EPOCHS_KFOLD\nmodel_name, figures_dir = model_version_folders(model_prefix, data_quality, TL_name, EPOCHS, WEIGHTS_name, DA_name)\nif KFOLD: run_kfold()","b11ba77a":"%%time\nEPOCHS = EPOCHS_TEST\nmodel, best_model_metrics_filename = run_final(test_data, test_label)\ntest_model(model, best_model_metrics_filename, test_data, test_label)","ca029ecd":"# Train from ImageNet-trainable Weights Learning (IWL) -> True\nTRAINABLE = True\nTL_name = 'NOTL'\n# Transfer Learning (TL) -> False\n#TRAINABLE = False\n#TL_name = 'TL'","7cdd040d":"# With Data Augmentation -> True\n#DA = True\n#DA_name = 'DA' \n# Without Data Augmentation -> False\nDA = False\nDA_name = 'NODA' \n\nEPOCHS = EPOCHS_KFOLD\nmodel_name, figures_dir = model_version_folders(model_prefix, data_quality, TL_name, EPOCHS, WEIGHTS_name, DA_name)\nif KFOLD: run_kfold()","1ac086ff":"%%time\nEPOCHS = EPOCHS_TEST\nmodel, best_model_metrics_filename = run_final(test_data, test_label)\ntest_model(model, best_model_metrics_filename, test_data, test_label)","034aafb3":"# With Data Augmentation -> True\nDA = True\nDA_name = 'DA' \n# Without Data Augmentation -> False\n#DA = False\n#DA_name = 'NODA' \n\nEPOCHS = EPOCHS_KFOLD\nmodel_name, figures_dir = model_version_folders(model_prefix, data_quality, TL_name, EPOCHS, WEIGHTS_name, DA_name)\nif KFOLD: run_kfold()","7533f17f":"%%time\nEPOCHS = EPOCHS_TEST\nmodel, best_model_metrics_filename = run_final(test_data, test_label)\ntest_model(model, best_model_metrics_filename, test_data, test_label)","48afcbec":"# Clean dataset \n#NOISE_PART = 0.0\n#data_quality = 'CLEAN'\n\n# Dirty dataset \n#NOISE_PART = 1.0\n#data_quality = 'DIRTY'\n\n# Dirty & Clean dataset \nNOISE_PART = 0.5\ndata_quality = 'DIRTY_CLEAN'","35ea1d3b":"# Train from ImageNet-trainable Weights Learning (IWL) -> True\n#TRAINABLE = True\n#TL_name = 'NOTL'\n# Transfer Learning (TL) -> False\nTRAINABLE = False\nTL_name = 'TL'","633c6bc1":"# With Data Augmentation -> True\n#DA = True\n#DA_name = 'DA' \n# Without Data Augmentation -> False\nDA = False\nDA_name = 'NODA' \n\nEPOCHS = EPOCHS_KFOLD\nmodel_name, figures_dir = model_version_folders(model_prefix, data_quality, TL_name, EPOCHS, WEIGHTS_name, DA_name)\nif KFOLD: run_kfold()","e1ab5914":"%%time\nEPOCHS = EPOCHS_TEST\nmodel, best_model_metrics_filename = run_final(test_data, test_label)\ntest_model(model, best_model_metrics_filename, test_data, test_label)","7752c6f4":"# With Data Augmentation -> True\nDA = True\nDA_name = 'DA' \n# Without Data Augmentation -> False\n#DA = False\n#DA_name = 'NODA' \n\nEPOCHS = EPOCHS_KFOLD\nmodel_name, figures_dir = model_version_folders(model_prefix, data_quality, TL_name, EPOCHS, WEIGHTS_name, DA_name)\nif KFOLD: run_kfold()","01e03414":"%%time\nEPOCHS = EPOCHS_TEST\nmodel, best_model_metrics_filename = run_final(test_data, test_label)\ntest_model(model, best_model_metrics_filename, test_data, test_label)","23e3c2e0":"# Train from ImageNet-trainable Weights Learning (IWL) -> True\nTRAINABLE = True\nTL_name = 'NOTL'\n# Transfer Learning (TL) -> False\n#TRAINABLE = False\n#TL_name = 'TL'","e33a6d34":"# With Data Augmentation -> True\n#DA = True\n#DA_name = 'DA' \n# Without Data Augmentation -> False\nDA = False\nDA_name = 'NODA' \n\nEPOCHS = EPOCHS_KFOLD\nmodel_name, figures_dir = model_version_folders(model_prefix, data_quality, TL_name, EPOCHS, WEIGHTS_name, DA_name)\nif KFOLD: run_kfold()","67e7bfb1":"%%time\nEPOCHS = EPOCHS_TEST\nmodel, best_model_metrics_filename = run_final(test_data, test_label)\ntest_model(model, best_model_metrics_filename, test_data, test_label)","a51ece49":"# With Data Augmentation -> True\nDA = True\nDA_name = 'DA' \n# Without Data Augmentation -> False\n#DA = False\n#DA_name = 'NODA' \n\nEPOCHS = EPOCHS_KFOLD\nmodel_name, figures_dir = model_version_folders(model_prefix, data_quality, TL_name, EPOCHS, WEIGHTS_name, DA_name)\nif KFOLD: run_kfold()","02f54397":"%%time\nEPOCHS = EPOCHS_TEST\nmodel, best_model_metrics_filename = run_final(test_data, test_label)\ntest_model(model, best_model_metrics_filename, test_data, test_label)","4f3a808b":"# Train from ImageNet-trainable Weights\n#WEIGHTS = 'imagenet'\n#WEIGHTS_name = 'imagenet'\n# Train from scratch\nWEIGHTS = None\nWEIGHTS_name = 'random'","843c2178":"# Clean dataset \nNOISE_PART = 0.0\ndata_quality = 'CLEAN'\n\n# Dirty dataset \n#NOISE_PART = 1.0\n#data_quality = 'DIRTY'\n\n# Dirty & Clean dataset \n#NOISE_PART = 0.5\n#data_quality = 'DIRTY_CLEAN'","0f1c5f6a":"# Train from ImageNet-trainable Weights Learning (IWL) -> True\nTRAINABLE = True\nTL_name = 'NOTL'\n# Transfer Learning (TL) -> False\n#TRAINABLE = False\n#TL_name = 'TL'","05579b43":"# With Data Augmentation -> True\n#DA = True\n#DA_name = 'DA' \n# Without Data Augmentation -> False\nDA = False\nDA_name = 'NODA' \n\nEPOCHS = EPOCHS_KFOLD\nmodel_name, figures_dir = model_version_folders(model_prefix, data_quality, TL_name, EPOCHS, WEIGHTS_name, DA_name)\nif KFOLD: run_kfold()","3d188527":"%%time\nEPOCHS = EPOCHS_TEST\nmodel, best_model_metrics_filename = run_final(test_data, test_label)\ntest_model(model, best_model_metrics_filename, test_data, test_label)","c9dd1d46":"# With Data Augmentation -> True\nDA = True\nDA_name = 'DA' \n# Without Data Augmentation -> False\n#DA = False\n#DA_name = 'NODA' \n\nEPOCHS = EPOCHS_KFOLD\nmodel_name, figures_dir = model_version_folders(model_prefix, data_quality, TL_name, EPOCHS, WEIGHTS_name, DA_name)\nif KFOLD: run_kfold()","d2cd5052":"%%time\nEPOCHS = EPOCHS_TEST\nmodel, best_model_metrics_filename = run_final(test_data, test_label)\ntest_model(model, best_model_metrics_filename, test_data, test_label)","408a928e":"# Clean dataset \n#NOISE_PART = 0.0\n#data_quality = 'CLEAN'\n\n# Dirty dataset \nNOISE_PART = 1.0\ndata_quality = 'DIRTY'\n\n# Dirty & Clean dataset \n#NOISE_PART = 0.5\n#data_quality = 'DIRTY_CLEAN'","6f5ffbea":"# Train from ImageNet-trainable Weights Learning (IWL) -> True\nTRAINABLE = True\nTL_name = 'NOTL'\n# Transfer Learning (TL) -> False\n#TRAINABLE = False\n#TL_name = 'TL'","d7660a1c":"# With Data Augmentation -> True\n#DA = True\n#DA_name = 'DA' \n# Without Data Augmentation -> False\nDA = False\nDA_name = 'NODA' \n\nEPOCHS = EPOCHS_KFOLD\nmodel_name, figures_dir = model_version_folders(model_prefix, data_quality, TL_name, EPOCHS, WEIGHTS_name, DA_name)\nif KFOLD: run_kfold()","59cf32ec":"%%time\nEPOCHS = EPOCHS_TEST\nmodel, best_model_metrics_filename = run_final(test_data, test_label)\ntest_model(model, best_model_metrics_filename, test_data, test_label)","24b2aefa":"# With Data Augmentation -> True\nDA = True\nDA_name = 'DA' \n# Without Data Augmentation -> False\n#DA = False\n#DA_name = 'NODA' \n\nEPOCHS = EPOCHS_KFOLD\nmodel_name, figures_dir = model_version_folders(model_prefix, data_quality, TL_name, EPOCHS, WEIGHTS_name, DA_name)\nif KFOLD: run_kfold()","86bd29f8":"%%time\nEPOCHS = EPOCHS_TEST\nmodel, best_model_metrics_filename = run_final(test_data, test_label)\ntest_model(model, best_model_metrics_filename, test_data, test_label)","70a51f5e":"# Clean dataset \n#NOISE_PART = 0.0\n#data_quality = 'CLEAN'\n\n# Dirty dataset \n#NOISE_PART = 1.0\n#data_quality = 'DIRTY'\n\n# Dirty & Clean dataset \nNOISE_PART = 0.5\ndata_quality = 'DIRTY_CLEAN'","84be340b":"# Train from ImageNet-trainable Weights Learning (IWL) -> True\nTRAINABLE = True\nTL_name = 'NOTL'\n# Transfer Learning (TL) -> False\n#TRAINABLE = False\n#TL_name = 'TL'","a4739bf8":"# With Data Augmentation -> True\n#DA = True\n#DA_name = 'DA' \n# Without Data Augmentation -> False\nDA = False\nDA_name = 'NODA' \n\nEPOCHS = EPOCHS_KFOLD\nmodel_name, figures_dir = model_version_folders(model_prefix, data_quality, TL_name, EPOCHS, WEIGHTS_name, DA_name)\nif KFOLD: run_kfold()","85cdcf74":"%%time\nEPOCHS = EPOCHS_TEST\nmodel, best_model_metrics_filename = run_final(test_data, test_label)\ntest_model(model, best_model_metrics_filename, test_data, test_label)","9c94a1ed":"# With Data Augmentation -> True\nDA = True\nDA_name = 'DA' \n# Without Data Augmentation -> False\n#DA = False\n#DA_name = 'NODA' \n\nEPOCHS = EPOCHS_KFOLD\nmodel_name, figures_dir = model_version_folders(model_prefix, data_quality, TL_name, EPOCHS, WEIGHTS_name, DA_name)\nif KFOLD: run_kfold()","46c9adde":"%%time\nEPOCHS = EPOCHS_TEST\nmodel, best_model_metrics_filename = run_final(test_data, test_label)\ntest_model(model, best_model_metrics_filename, test_data, test_label)","d42eab2f":"# Model_class","c8442bd2":"### Regime = DIRTY + CLEAN + NOTL","f3c922a0":"#### Regime = CLEAN + NOTL + DA","fe887883":"## Load from Kaggle","ff76da62":"### Regime = CLEAN + NOTL","76795ce1":"#### TEST -> model","4de52d46":"### Regime = DIRTY + NOTL","3b9843a9":"#### Regime = DIRTY + NOTL + NODA","bb598b7c":"### Regime = CLEAN + TL","7b70a5e3":"#### Regime = DIRTY + NOTL + DA","dfd5c57a":"#### Regime = DIRTY + NOTL + NODA","192444ee":"# Plot functions","55b94191":"#### Regime = CLEAN + NOTL + DA","2f223f0a":"#### Regime = DIRTY + CLEAN + NOTL + DA","ba0185f7":"## Data PreProcessing ","7eb06e29":"#### Regime = DIRTY + TL + NODA","802d1b37":"#### Regime = DIRTY + CLEAN + TL + NODA","f2800fca":"### Regime = DIRTY + CLEAN + TL","9da06e96":"## Regime = DIRTY + CLEAN","1637f73c":"# Regime = CLEAN","1dfd6837":"#### Regime = DIRTY + CLEAN + NOTL + NODA","3f9da22b":"#### Regime = DIRTY + CLEAN + NOTL + NODA","b6b97e4b":"#### Regime = DIRTY + CLEAN + NOTL + DA","0830e847":"#### Regime = DIRTY + NOTL + DA","8545bf1f":"#### Regime = CLEAN + TL + DA","3b1ac473":"#### Regime = CLEAN + NOTL + NODA","425ef42c":"## Regime = DIRTY + CLEAN","98230d2e":"#### Regime = CLEAN + NOTL + NODA","4a66ea72":"## Regime = DIRTY","6ed476fd":"### Regime = CLEAN + NOTL","7edf4c74":"#### Regime = CLEAN + TL + NODA","ee949493":"#### Regime = DIRTY + CLEAN + TL + DA","d73baca9":"# Regimes","af6a9c66":"## Regime = DIRTY","ef9c2cc4":"### Regime = DIRTY + CLEAN + NOTL","265298f0":"## Regime = CLEAN","32c7c8fc":"## Basic Parameters","452c6b43":"#### Regime = DIRTY + TL + DA","28623dd1":"# From random weights","e3e17ce6":"## Load from Local Storage","1bfc3f19":"# Prepare data","baf54ec3":"### Regime = DIRTY + NOTL","278cbac9":"### Regime = DIRTY + TL","9fc16a4d":"# Different number of images"}}