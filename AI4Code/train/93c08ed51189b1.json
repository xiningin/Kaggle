{"cell_type":{"03c610af":"code","9cd8a354":"code","50842dba":"code","cbf1495e":"code","26f7bcf0":"code","6f8813b1":"code","43ed53bc":"code","ef35e18e":"code","2af73124":"code","5dc5b395":"code","18bbbe61":"code","7c75eb45":"code","0b63213e":"code","18580b5d":"code","11e03d47":"code","e6c5f722":"code","0ef033de":"code","a094b317":"code","32910d04":"code","68bacc4c":"code","0f50993f":"code","a13b5c6d":"code","bcc4daaa":"code","be96c884":"code","9fd315fb":"code","a051081b":"code","977da469":"code","105fec62":"markdown","e89f670c":"markdown","48976f78":"markdown","b36990ff":"markdown","652e16be":"markdown","170fee4e":"markdown","dd4fc95a":"markdown","b7235919":"markdown","f6426aaf":"markdown","a9d80421":"markdown","8951d75d":"markdown","94812225":"markdown","acc75977":"markdown","992334d6":"markdown","ee0079b1":"markdown","9d063143":"markdown","85476b2a":"markdown","a47e412b":"markdown","26000269":"markdown"},"source":{"03c610af":"\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","9cd8a354":"import pandas as pd \nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn import linear_model, naive_bayes, neighbors, svm","50842dba":"drugs = pd.read_csv('..\/input\/drug-classification\/drug200.csv')\ndrugs.head()","cbf1495e":"sns.distplot(drugs['Age'])","26f7bcf0":"sns.distplot(drugs['Na_to_K'])","6f8813b1":"drugs.describe()","43ed53bc":"age_groups = []\nfor i in drugs['Age']:\n    if i <= 30:\n        age_groups.append('0-30')\n    if i > 30 and i <= 40:\n        age_groups.append('30-40')\n    if i > 40 and i <= 50:\n        age_groups.append('40-50')\n    if i > 50 and i <= 60:\n        age_groups.append('50-60')\n    if i > 60:\n        age_groups.append('60+')\n\ndrugs['AgeGroup'] = age_groups","ef35e18e":"na_to_k_groups = []\nfor i in drugs[\"Na_to_K\"]:\n    if i <= 10:\n        na_to_k_groups.append('5-10')\n    if i > 10 and i <= 15:\n        na_to_k_groups.append('10-15')\n    if i > 15 and i <= 20:\n        na_to_k_groups.append('15-20')\n    if i > 20 and i <= 25:\n        na_to_k_groups.append('20-25')\n    if i > 25 and i <= 30:\n        na_to_k_groups.append('25-30')\n    if i > 30:\n        na_to_k_groups.append('30+')\n\ndrugs['Na_to_K_groups'] = na_to_k_groups","2af73124":"drugs = drugs[['AgeGroup','Sex','BP','Cholesterol','Na_to_K_groups','Drug']]\ndrugs.head()","5dc5b395":"sns.set_theme(style=\"whitegrid\")\nsns.countplot(x=\"AgeGroup\", data=drugs, palette='Spectral', order=['0-30', '30-40', '40-50', '50-60', '60+'])","18bbbe61":"sns.countplot(x='Sex', data = drugs, palette='Spectral')","7c75eb45":"sns.countplot(x=\"BP\", data=drugs, palette='Spectral')","0b63213e":"sns.countplot(x=\"Cholesterol\", data=drugs, palette='Spectral')","18580b5d":"sns.countplot(x='Na_to_K_groups', data=drugs, palette='Spectral', order = ['5-10','10-15','15-20','20-25','30+'])","11e03d47":"sns.countplot(x=\"Drug\", data=drugs, palette='Spectral')\n","e6c5f722":"x, y = drugs.values[:, :-1], drugs.values[:, -1]","0ef033de":"x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.33)","a094b317":"x_train = pd.get_dummies(pd.DataFrame(x_train))\nx_test = pd.get_dummies(pd.DataFrame(x_test))","32910d04":"x_train, y_train = SMOTE().fit_resample(x_train, y_train)","68bacc4c":"ax = sns.countplot(x=y_train, data=drugs, palette='Spectral')\nax.set(xlabel='Drug')","0f50993f":"print(\"ORIGINAL dataset:\", len(drugs), \"\\n EXTENDED dataset:\", len(y))","a13b5c6d":"log_reg = linear_model.LogisticRegression(max_iter = 5000)\nlog_reg.fit(x_train, y_train)\nlog_reg_acc = 100*log_reg.score(x_test, y_test)\nprint('Logistic Regression Predictions: \\n', log_reg.predict(x_test), '\\n Accuracy:', log_reg_acc, '%')","bcc4daaa":"nb = naive_bayes.GaussianNB()\nnb.fit(x_train, y_train)\nnb_acc = 100*nb.score(x_test, y_test)\nprint('Naive Bayes Predictions: \\n', nb.predict(x_test), '\\n Accuracy:', nb_acc, '%')\n","be96c884":"knn = neighbors.KNeighborsClassifier(n_neighbors=25)\nknn.fit(x_train, y_train)\nknn_acc = 100*knn.score(x_test, y_test)\nprint('K-Nearest Neighbours Predictions: \\n', knn.predict(x_test), '\\n Accuracy:', knn_acc, '%')","9fd315fb":"svm =svm.SVC(kernel='linear')\nsvm.fit(x_train, y_train)\nsvm_acc = 100*svm.score(x_test, y_test)\nprint('SVM Predictions: \\n', svm.predict(x_test), '\\n Accuracy:', svm_acc, '%')","a051081b":"pd.DataFrame(data={'Model': ['Logistic Regression','Gaussian Naive Bayes', 'K_Nearest Neighbours', 'Support Vector Machine(SVM)'], 'Accuracy %':[log_reg_acc, nb_acc, knn_acc, svm_acc]})","977da469":"df1 = pd.read_csv('\/kaggle\/input\/drug-classification\/drug200.csv')\ndf1.to_csv('Drug Classification100',index = False)","105fec62":"The drug frequencies are very unbalanced. For a classification problem it would be ideal to have similar numbers of the trget variable. Therefore we will use SMOTE(Synthetic Minority Oversampling Technique). SMOTE will oversample drugA, drugB, drugC and drugX to have the same numver of samples as DrugY. The systhetic aspect of this oversampling technique helps to avoid overfitting, as it's not just repeating existing data. First we need to split the dataset into training and testing sets and transform the data into dummies.","e89f670c":"# 5. Gaussian Naive Bayes ","48976f78":"the variable descriptions are as follows:\n\n-Age: Age of the patient in years\n\n-Sex: Sex of the patient- male or female \n\n-BP: Blood Pressure of patient-high, low, or normal \n\n-Cholesterol-Cholesterol level of the patient- normal or high\n\n-Na-to_K: Sodium to Potassium ratio in patient's blood \n\n-Drug: The drug type that the patient was prescirbed- drugA,drugB,drugC,drugX or drugY","b36990ff":"> **Age groups:** \n* under 30s\n* 30-40\n* 40-50\n* 50-60\n* Over 60s\n","652e16be":"we will group the Age and Sodium to Potassium Ratio(Na_to_K)values, depending on the max and min values in each column. Let's have a look at their distributions. ","170fee4e":"# 8. Summary","dd4fc95a":"# 7. Support Vector Machine","b7235919":"> **Sodium to Potassium ratios:**\n* 5-10\n* 10-15\n* 15-20\n* 20-25\n* 25-30\n* 30+","f6426aaf":" # 2.Grouping data","a9d80421":"We can see here that the minimum and maximum ages are 15 and 74 respectively and the minimum and maximum sodium to potassium ratios are 6.269 and 38.247 respectively.","8951d75d":"# 1. Imports","94812225":"We're only using SMOTE on the training data. The synthetic data shouldn't be so similar to the orginal data that it causes overfitting, but by only oversampling the training data, we will know it if does","acc75977":"Dataset has been extended by 255 ","992334d6":"* Drug Classification.\nProject goal: \nTo create a model capable of determining which drug type a patient should be prescirbed based on a number of features, uing a variety of classification algorithms. ","ee0079b1":"# 3.Visualising tha variable distributions","9d063143":"# 9. Conclusion","85476b2a":"# 4. Logistic Regression ","a47e412b":"# 6. K-Nearest Neighbours","26000269":"Logistic Regression, Naive Bayes and SVM all successfully predicted 100% of the drug types that the patients should be assigned. K-Nearest Neighbours had a much lower success rate of only 77%. This makes sense as KNN is known as a 'Lazy Learner' meaning it doesn't actually 'learn' anything in the training period - it just stores training data. Nevertheless, the other models were successful!"}}