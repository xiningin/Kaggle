{"cell_type":{"697f9c55":"code","7953e40e":"code","4334ad6b":"code","90b4afd5":"code","4c496174":"code","3f3d6844":"code","a087d9db":"code","73ed688d":"code","b1eb1bcd":"code","9f5aecb4":"code","7db9b55a":"code","339e6440":"code","0f792d68":"code","0d8acbae":"code","95e6b076":"code","472b2c23":"code","f3bde922":"code","24c975d2":"code","173c82cc":"code","50effd55":"markdown","7084fd46":"markdown","55e9d37a":"markdown","af09e6a2":"markdown","9b0d2b91":"markdown","e692de86":"markdown","6cba4d84":"markdown","559c0c26":"markdown","de0f99d8":"markdown","b7c2dd51":"markdown","3dfe5490":"markdown","4c4b0060":"markdown","1779fc9e":"markdown"},"source":{"697f9c55":"import numpy as np\nimport pandas as pd\n\n# file path\ndata_path = '\/kaggle\/input\/porto-seguro-safe-driver-prediction\/'\n\ntrain = pd.read_csv(data_path + 'train.csv', index_col='id')\ntest = pd.read_csv(data_path + 'test.csv', index_col='id')\nsubmission = pd.read_csv(data_path + 'sample_submission.csv', index_col='id')","7953e40e":"all_data = pd.concat([train, test], ignore_index=True)\nall_data = all_data.drop('target', axis=1) # Remove target value\n\nall_features = all_data.columns.tolist() # All features","4334ad6b":"# Add 'number of missing values per data' as a new feature\nall_data['num_missing'] = (all_data==-1).sum(axis=1)\n\n# Features excluding nominal feature, features with calc on tag\nremaining_features = [col for col in all_features \\\n                      if ('cat' not in col and 'calc' not in col)] \n# Add num_missin to remaining_features\nremaining_features.append('num_missing')","90b4afd5":"from sklearn.preprocessing import OneHotEncoder\n\ncat_features = [col for col in all_features if 'cat' in col] # Nominal features\n\n# Apply One-Hot encoding\nonehot_encoder = OneHotEncoder()\nencoded_cat_matrix = onehot_encoder.fit_transform(all_data[cat_features]) ","4c496174":"# Feature with 'ind' on tag\nind_features = [col for col in all_features if 'ind' in col]\n\nfirst_col=True\nfor col in ind_features:\n    if first_col:\n        all_data['mix_ind'] = all_data[col].astype(str)+'_'\n        first_col = False\n    else:\n        all_data['mix_ind'] += all_data[col].astype(str)+'_'","3f3d6844":"cat_count_features = []\nfor col in cat_features+['mix_ind']:\n    val_counts_dic = all_data[col].value_counts().to_dict()\n    all_data[f'{col}_count'] = all_data[col].apply(lambda x: val_counts_dic[x])\n    cat_count_features.append(f'{col}_count')","a087d9db":"from scipy import sparse\n\ndrop_features = ['ps_ind_14', 'ps_ind_10_bin','ps_ind_11_bin', \n                 'ps_ind_12_bin','ps_ind_13_bin','ps_car_14']\n\n# Data to remove drop_features from remaining_features, cat_count_features\nall_data_remaining = all_data[remaining_features+cat_count_features].drop(drop_features, axis=1)\n\n# Concatenate Data\nall_data_sprs = sparse.hstack([sparse.csr_matrix(all_data_remaining),\n                               encoded_cat_matrix],\n                              format='csr')","73ed688d":"num_train = train.shape[0] # Number of train data \n\n# Divide train data and test data\nX = all_data_sprs[:num_train]\nX_test = all_data_sprs[num_train:]\n\ny = train['target'].values","b1eb1bcd":"def eval_gini(y_true, y_pred):\n    # Verify that the actual and predicted values are the same size (different values raise errors)\n    assert y_true.shape == y_pred.shape\n\n    n_samples = y_true.shape[0] # Number of data\n    L_mid = np.linspace(1 \/ n_samples, 1, n_samples) # Diagonal value\n\n    # 1) Gini coefficient for predicted values\n    pred_order = y_true[y_pred.argsort()] # Sort y_true values by y_pred size\n    L_pred = np.cumsum(pred_order) \/ np.sum(pred_order) # Lorentz Curve\n    G_pred = np.sum(L_mid - L_pred) # Gini coefficient for predicted values\n\n    # 2) Gini coefficient when prediction is perfect\n    true_order = y_true[y_true.argsort()] # Sort y_true values by y_true size\n    L_true = np.cumsum(true_order) \/ np.sum(true_order) # Lorentz Curve\n    G_true = np.sum(L_mid - L_true) #  Gini coefficient when prediction is perfect\n\n    # Normalized Gini coefficient\n    return G_pred \/ G_true","9f5aecb4":"def gini_lgb(preds, dtrain):\n    labels = dtrain.get_label()\n    return 'gini', eval_gini(labels, preds), True","7db9b55a":"def gini_xgb(preds, dtrain):\n    labels = dtrain.get_label()\n    return 'gini', eval_gini(labels, preds)","339e6440":"from sklearn.model_selection import StratifiedKFold\n\n# Create Stratified K Fold Cross-Verifier\nfolds = StratifiedKFold(n_splits=5, shuffle=True, random_state=1991)","0f792d68":"max_params_lgb = {'bagging_fraction': 0.8043696643500143,\n 'feature_fraction': 0.6829323879981047,\n 'lambda_l1': 0.9264555612104627,\n 'lambda_l2': 0.9774233689434216,\n 'min_child_samples': 10,\n 'min_child_weight': 125.68433948868649,\n 'num_leaves': 28,\n 'objective': 'binary',\n 'learning_rate': 0.01,\n 'bagging_freq': 1,\n 'verbosity': 0,\n 'random_state': 1991}","0d8acbae":"import lightgbm as lgbm\n\n# One-dimensional array of probabilities for predicting validation data target values with an OOF-trained model\noof_val_preds_lgb = np.zeros(X.shape[0]) \n# One-dimensional array of probabilities for predicting test data target values with an OOF-trained model\noof_test_preds_lgb = np.zeros(X_test.shape[0]) \n\n# Train, validate, and predict models by OOF\nfor idx, (train_idx, valid_idx) in enumerate(folds.split(X, y)):\n    # The phrase that separates each fold.\n    print('#'*40, f'Fold {idx+1} out of {folds.n_splits}', '#'*40)\n    \n    # Set train data, valid data\n    X_train, y_train = X[train_idx], y[train_idx] # Train data\n    X_valid, y_valid = X[valid_idx], y[valid_idx] # Valid data\n\n    # Create lgbm dataset\n    dtrain = lgbm.Dataset(X_train, y_train) # lgbm train dataset\n    dvalid = lgbm.Dataset(X_valid, y_valid) # lgbm valid dataset\n\n    # Train LightGBM\n    lgb_model = lgbm.train(params=max_params_lgb, # Optimal Hyper-parameters\n                           train_set=dtrain, # Train data\n                           num_boost_round=1500, # Number of boosting iterations\n                           valid_sets=dvalid, # Valid data for model performance evaluation\n                           feval=gini_lgb, # Evaluation metrics for validation\n                           early_stopping_rounds=150, # Early stopping condition\n                           verbose_eval=100)\n    \n    # The number of boosting iterations when the model performs best \n    best_iter = lgb_model.best_iteration\n    # Predict probabilities using test data\n    oof_test_preds_lgb += lgb_model.predict(X_test, \n                                    num_iteration=best_iter)\/folds.n_splits\n    # OOF prediction for model performance evaluation\n    oof_val_preds_lgb[valid_idx] += lgb_model.predict(X_valid, num_iteration=best_iter)\n    \n    # Normalized Gini coefficient for oof prediction probabilities\n    gini_score = eval_gini(y_valid, oof_val_preds_lgb[valid_idx])\n    print(f'Fold {idx+1} gini score: {gini_score}\\n')","95e6b076":"max_params_xgb = {'colsample_bytree': 0.8927325521002059,\n 'gamma': 9.766883037651555,\n 'max_depth': 7,\n 'min_child_weight': 6.0577898395058085,\n 'reg_alpha': 8.136089122187865,\n 'reg_lambda': 1.385119327658532,\n 'scale_pos_weight': 1.5142072116395773,\n 'subsample': 0.717425859940308,\n 'objective': 'binary:logistic',\n 'learning_rate': 0.05,\n 'random_state': 1991}","472b2c23":"import xgboost as xgb\n\n# One-dimensional array of probabilities for predicting validation data target values with an OOF-trained model\noof_val_preds_xgb = np.zeros(X.shape[0]) \n# One-dimensional array of probabilities for predicting test data target values with an OOF-trained model\noof_test_preds_xgb = np.zeros(X_test.shape[0]) \n\n# Train, validate, and predict models by OOF\nfor idx, (train_idx, valid_idx) in enumerate(folds.split(X, y)):\n    # The phrase that separates each fold.\n    print('#'*40, f'Fold {idx+1} out of {folds.n_splits}', '#'*40)\n    \n    # Set train data, valid data\n    X_train, y_train = X[train_idx], y[train_idx]\n    X_valid, y_valid = X[valid_idx], y[valid_idx]\n\n    # Create xgboost dmatrix\n    dtrain = xgb.DMatrix(X_train, y_train)\n    dvalid = xgb.DMatrix(X_valid, y_valid)\n    dtest = xgb.DMatrix(X_test)\n    \n    watchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n\n    xgb_model = xgb.train(params=max_params_xgb, \n                           dtrain=dtrain,\n                           num_boost_round=1000,\n                           evals=watchlist,\n                          maximize=True,\n                           feval=gini_xgb,\n                           early_stopping_rounds=150,\n                           verbose_eval=100)\n\n    # The number of boosting iterations when the model performs best \n    best_iter = xgb_model.best_iteration\n    # Predict probabilities using test data\n    oof_test_preds_xgb += xgb_model.predict(dtest, \n                                    ntree_limit=best_iter)\/folds.n_splits\n    # OOF prediction for model performance evaluation\n    oof_val_preds_xgb[valid_idx] += xgb_model.predict(dvalid, ntree_limit=best_iter)\n    \n    # Normalized Gini coefficient for oof prediction probabilities\n    gini_score = eval_gini(y_valid, oof_val_preds_xgb[valid_idx])\n    print(f'Fold {idx+1} gini score: {gini_score}\\n')","f3bde922":"print('LightGBM OOF Gini Score:', eval_gini(y, oof_val_preds_lgb))","24c975d2":"print('XGBoost OOF Gini Score:', eval_gini(y, oof_val_preds_xgb))","173c82cc":"oof_test_preds = oof_test_preds_lgb * 0.6 + oof_test_preds_xgb * 0.4\nsubmission['target'] = oof_test_preds\nsubmission.to_csv('submission.csv')","50effd55":"## Feature Engineering","7084fd46":"### LightGBM","55e9d37a":"### Create new feature, the number of eigenvalues for nominal features","af09e6a2":"## Evaluation Matrics","9b0d2b91":"### Final dataset","e692de86":"# (Top 9th) LightGBM & XGBoost Ensemble Modeling\n## This is LightGBM and XGBoost ensemble modeling notebook. This model reaches the top 9th. I appreciate if you upvote!\n## I also shared [Basic and Informative EDA Notebook](https:\/\/www.kaggle.com\/werooring\/basic-eda-for-everyone). It is quite easy so that even beginners can understand\n- [reference notebook](https:\/\/www.kaggle.com\/xiaozhouwang\/2nd-place-lightgbm-solution)","6cba4d84":"## Ensemble and Submission","559c0c26":"## Modeling","de0f99d8":"### Generate missing values as a new feature","b7c2dd51":"### Create a new features `mix_ind` that combines unique values of an `ind` features","3dfe5490":"### XGBoost","4c4b0060":"### Apply One-Hot Encoding to nominal features","1779fc9e":"### OOF Valid Gini Score"}}