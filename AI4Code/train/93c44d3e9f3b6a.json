{"cell_type":{"848986cd":"code","3d51b7e4":"code","efd9d5bd":"code","b9a05e85":"code","ed57eaac":"code","ccfadd1f":"code","17a8df62":"code","4e2d3b7b":"code","661a158e":"code","744df64a":"code","47b13df2":"code","834e7b9c":"code","3f142427":"code","87eaff93":"code","db1f6ecf":"code","72768b7f":"code","eede195c":"code","b4c36b9f":"code","5c04800d":"code","db3a0436":"markdown","415e9e8b":"markdown","1bba2fb8":"markdown","2ff0664e":"markdown","8c0bbfe3":"markdown","57a273f3":"markdown","62d63d1e":"markdown","a62b45fd":"markdown"},"source":{"848986cd":"!pip install -q iterative-stratification\nimport sys; \nsys.path.insert(0,'..\/input\/timm-all-models\/pytorch-image-models-master\/pytorch-image-models-master')\n# sys.path.insert(0, '..\/input\/timm-all-models')\n\nimport torchmetrics\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nimport albumentations as A\nimport cv2\nfrom tqdm.notebook import tqdm\n\n\n\nfrom albumentations.pytorch import ToTensorV2\n\nimport timm\nfrom timm.utils.agc import adaptive_clip_grad\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport pytorch_lightning as pl\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.cuda.amp import autocast, GradScaler\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom albumentations.core.composition import Compose, OneOf\nwarnings.simplefilter(\"ignore\")\nfrom pytorch_lightning import Trainer, seed_everything\nimport torch.nn.functional as F\n\n\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\nfrom pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\nfrom sklearn import metrics\nimport itertools","3d51b7e4":"class Config:\n    seed=42\n    n_epoch=30\n    img_size= 520\n    train_path='..\/input\/plant-pathology-2021-fgvc8\/train_images'\n    test_path='..\/input\/plant-pathology-2021-fgvc8\/test_images'\n    lr = 1e-4\n    max_lr = 1e-3\n    pct_start = 0.3\n    div_factor = 1.0e+3\n    final_div_factor = 1.0e+3\n    accum = 1\n    precision = 16\n    n_fold = 3\n    num_classes=1\n    \n    \n    \n    weight_decay=0.001\n    debug=False\n    debug_sample=100\n    train_batch=32\n    test_batch=32\n    path='..\/input\/plant-pathology-2021-fgvc8\/'\n    \n        \ndevice = torch.device(\"cuda\")\n\nseed_everything(Config.seed)","efd9d5bd":"df = pd.read_csv('..\/input\/seti-breakthrough-listen\/train_labels.csv')\nprint (df.shape)\ndf['img_path'] = df['id'].apply(lambda x: f'..\/input\/seti-breakthrough-listen\/train\/{x[0]}\/{x}.npy')","b9a05e85":"class SetiDataset:\n    \n    def __init__(self, df): \n        self.image_paths = df['img_path']\n        self.targets = df['target']\n\n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self, idx):      \n        image = np.load(self.image_paths[idx]).astype(float)\n\n        targets = [self.targets[idx]]\n                \n        return {\n            \"image\": torch.tensor(image, dtype=torch.float),\n            \"targets\": torch.tensor(targets, dtype=torch.float),\n        }","ed57eaac":"class Efnetmodel(nn.Module):\n    \n    def __init__(self, model_name='efficientnet_b2', pretrained=True):\n        super().__init__()\n        self.conv1 = nn.Conv2d(6, 3, kernel_size=3, stride=1, padding=3, bias=False)\n        self.model = timm.create_model(model_name, pretrained=pretrained)\n        in_features = self.model.get_classifier().in_features\n#         self.model.fc = nn.Linear(in_features, CFG.num_classes)\n        self.model.classifier = nn.Sequential(\n            nn.Linear(in_features, in_features),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.5),\n            nn.Linear(in_features, Config.num_classes)\n        )\n        \n\n    def forward(self, x):\n        x=self.conv1(x)\n        \n        x = self.model(x)\n       \n    \n        \n        return x\n    def save(self,optim,epoch):\n        self.eval()\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': self.state_dict(),\n            'optimizer_state_dict': optim.state_dict(),\n            'loss': 0,\n            }, '.\/effnet_new_b2{}.pth'.format(epoch))\n    def load(self,optim,path):\n        checkpoint = torch.load(path)\n        self.load_state_dict(checkpoint['model_state_dict'])\n        optim.load_state_dict(checkpoint['optimizer_state_dict'])\n        self.epoch = checkpoint['epoch']\n        self.loss = checkpoint['loss']","ccfadd1f":"folds = df.copy()\nFold = StratifiedKFold(n_splits=Config.n_fold, shuffle=True, random_state=Config.seed)\nfor n, (train_index, val_index) in enumerate(Fold.split(folds, folds['target'])):\n    df_train = df.iloc[train_index]\n    df_valid = df.iloc[val_index]\ndf_train=df_train.reset_index(drop=True)\ndf_valid=df_valid.reset_index(drop=True)","17a8df62":"train_dataset=SetiDataset(df_train)\nvalid_dataset=SetiDataset(df_valid)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16,shuffle=True, num_workers=4)\nvalid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=16,shuffle=False, num_workers=4)","4e2d3b7b":"Config.steps_per_epoch = len(train_loader)\nConfig.steps_per_epoch","661a158e":"class LitPlant(pl.LightningModule):\n    def __init__(self, model):\n        super(LitPlant, self).__init__()\n        self.model = model\n#         self.metric = pl.metrics.F1(num_classes=CFG.num_classes)\n#         self.metric = torchmetrics.F1(Config.num_classes, average='weighted')\n#         self.criterion = nn.BCELoss()\n        self.criterion = nn.BCEWithLogitsLoss()\n#         self.criterion = FocalLoss()\n        self.sigmoid = nn.Sigmoid()\n        self.lr = Config.lr\n\n    def forward(self, x, *args, **kwargs):\n        return self.model(x)\n\n    def configure_optimizers(self):\n        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n#         self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=CFG.t_max, eta_min=CFG.min_lr)\n        self.scheduler = torch.optim.lr_scheduler.OneCycleLR(self.optimizer, \n                                                             epochs=Config.n_epoch, steps_per_epoch=Config.steps_per_epoch,\n                                                             max_lr=Config.max_lr, pct_start=Config.pct_start, \n                                                             div_factor=Config.div_factor, final_div_factor=Config.final_div_factor)\n        scheduler = {'scheduler': self.scheduler, 'interval': 'step',}\n\n        return [self.optimizer], [scheduler]\n\n    def training_step(self, batch, batch_idx):\n        image = batch['image']\n        target = batch['targets']\n        output = self.model(image)\n#         output = self.sigmoid(output)\n        loss = self.criterion(output, target)\n#         score = self.metric(self.sigmoid(output), target.clone().detach().to(torch.int32))\n        pred=self.sigmoid(output).clone().detach().tolist()\n        target=target.clone().detach().tolist()\n        \n        \n        \n        \n            \n        \n        \n        return {'loss':loss,'preds': pred ,'targets':target}\n    def training_epoch_end(self, training_step_outputs):\n        \n        loss_list=[]\n        pred_list=[]\n        target_list=[]\n        for out in training_step_outputs:\n            loss_list.extend([out['loss'].cpu().detach().tolist()])\n            pred_list.extend(out['preds'])\n            target_list.extend(out['targets'])\n            \n            \n        auc_score=metrics.roc_auc_score( target_list,pred_list   )\n        meanloss=sum(loss_list )\/len(loss_list )\n        \n        logs = {'train_loss': meanloss, 'train_auc': auc_score, 'lr': self.optimizer.param_groups[0]['lr']}\n        \n        self.log_dict(\n            logs,\n            on_step=False, on_epoch=True, prog_bar=True, logger=True)\n        \n            \n            \n            \n            \n            \n            \n            \n            \n    \n    def validation_step(self, batch, batch_idx):\n        image = batch['image']\n        target = batch['targets']\n        output = self.model(image)\n#         output = self.sigmoid(output)\n        loss = self.criterion(output, target)\n#         score = self.metric(self.sigmoid(output), target.clone().detach().to(torch.int32))\n        \n        \n#         valid_auc=  metrics.roc_auc_score(target.clone().detach().tolist(),self.sigmoid(output).clone().detach().tolist())\n        pred=self.sigmoid(output).clone().detach().tolist()\n        target=target.clone().detach().tolist()\n        \n        return {'loss':loss,'preds': pred ,'targets':target}\n    \n    \n    def validation_epoch_end(self, validation_step_outputs):\n        loss_list=[]\n        pred_list=[]\n        target_list=[]\n        for out in validation_step_outputs:\n            loss_list.extend([out['loss'].cpu().detach().tolist()])\n            pred_list.extend(out['preds'])\n            target_list.extend(out['targets'])\n            \n            \n        auc_score=metrics.roc_auc_score( target_list,pred_list   )\n        meanloss=sum(loss_list )\/len(loss_list )\n        \n        logs = {'valid_loss': meanloss, 'valid_auc': auc_score, }\n        \n        self.log_dict(\n            logs,\n            on_step=False, on_epoch=True, prog_bar=True, logger=True)\n            \n       ","744df64a":"\nmodel =  Efnetmodel()\nlit_model = LitPlant(model)","47b13df2":"checkpoint_callback = ModelCheckpoint(monitor='valid_auc',\n                                      save_top_k=1,\n                                      save_last=True,\n                                      save_weights_only=True,\n                                      filename='{epoch:02d}-{valid_loss:.4f}-{valid_auc:.4f}',\n                                      verbose=False,\n                                      mode='max',\n                                      dirpath='.\/')\n\ntrainer = Trainer(\n    max_epochs=Config.n_epoch,\n    gpus=[0],\n    accumulate_grad_batches=Config.accum,\n    precision=Config.precision,\n#     callbacks=[EarlyStopping(monitor='valid_loss', patience=3, mode='min')],\n    checkpoint_callback=checkpoint_callback,\n#     logger=logger,\n    weights_summary='top',\n    amp_backend='native',\n    auto_lr_find=True \n)","834e7b9c":"trainer.fit(lit_model, train_dataloader=train_loader, val_dataloaders=valid_loader)","3f142427":"# saving the model the old fasion way though pytorch lightning does this for you feel free to comment out\n\nmodel.save(lit_model.optimizer,Config.n_epoch)","87eaff93":"def evaluate(data_loader, model, device):\n    model.eval()\n    \n    final_targets = []\n    final_outputs = []\n    \n    with torch.no_grad():\n        \n        for data in tqdm(data_loader, position=0, leave=True, desc='Evaluating'):\n            inputs = data[\"image\"]\n            targets = data[\"targets\"]\n            inputs = inputs.to(device, dtype=torch.float)\n            targets = targets.to(device, dtype=torch.float)\n            \n            output = model(inputs)\n            \n            targets = targets.detach().cpu().numpy().tolist()\n            output = output.detach().cpu().numpy().tolist()\n            \n            final_targets.extend(targets)\n            final_outputs.extend(output)\n            \n    return final_outputs, final_targets","db1f6ecf":"# model=Efnetmodel()\n# optim=torch.optim.Adam(model.parameters(), lr=Config.lr)\n# model.load(optim,'..\/input\/seti-model-cheakpoint\/seti_efficientnetb2e6\/effnet_new_b26.pth')\n# model=model.to(device)","72768b7f":"submission = pd.read_csv('..\/input\/seti-breakthrough-listen\/sample_submission.csv')\nsubmission['img_path'] = submission['id'].apply(lambda x: f'..\/input\/seti-breakthrough-listen\/test\/{x[0]}\/{x}.npy')","eede195c":"test_dataset = SetiDataset(submission)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=4)","b4c36b9f":"model=model.to(device)\npredictions, valid_targets = evaluate(test_loader, model, device=device)","5c04800d":"predictions = np.array(predictions)[:, 0]\nsig = torch.nn.Sigmoid()\nouts = sig(torch.from_numpy(predictions))\nouts = outs.detach().numpy()\n\nsubmission.target = outs\nsubmission.drop(['img_path'], axis=1, inplace=True)\nsubmission.to_csv('submission.csv', index=False)","db3a0436":"# Run the cell below if you want to load weights otherwise comment it out\nI have trained effcientnet b2 for 6 epochs and the result is quite good","415e9e8b":"# MODEL\nI have chosen efficientnet b2 from timm models\nYou can choose any model from  timm.list_models(pretrained=True)","1bba2fb8":"# Config\nFell free to change the hyperparameters","2ff0664e":"# Infarence","8c0bbfe3":"# Dataset","57a273f3":"# Loading the data and adding the image_path","62d63d1e":"# Imports ","a62b45fd":"# This is a Pytorch Loghtning starter code\nI have trained efficinet-net b2 for 6 epochs and the lb score is .93\n# Room for improvement \n* Add effective augmentation\n* Train larger models from timm models for longer epochs\n\nReferance notebooks\n\n[Simple yet EffectiveB0](https:\/\/www.kaggle.com\/micheomaano\/simple-yet-effectiveb0-lb-0-96)\n\n[\u26a1Plant2021 PyTorch Lightning Starter [ Training ]\u26a1](https:\/\/www.kaggle.com\/pegasos\/plant2021-pytorch-lightning-starter-training)\n\n\nFeel free to give this notebook an upvote if it is helpful!!"}}