{"cell_type":{"a3016839":"code","20896a8c":"code","a80059e8":"code","ce00f196":"code","d21d7bba":"code","b12784ab":"code","3779bc86":"code","39d7ab52":"code","c71ec027":"code","d809d2cd":"code","9e5ce0ed":"code","46d8cdcd":"code","01bdea77":"code","affedefd":"code","22378eb7":"code","87c3db42":"code","8b38d85f":"code","ef2575b5":"code","f4df45a3":"code","8fa00da4":"code","91c74cf8":"code","cc736116":"code","acfbf189":"code","9c67ad54":"code","59933b18":"code","e0072862":"code","9889a9e2":"code","f8f2f494":"code","6ec80474":"code","40a46244":"code","8be42408":"code","4dead49d":"code","00911583":"code","7c812697":"code","00fe3b72":"code","51e3ca90":"code","d61e27c7":"code","0a39b6cb":"code","af4630be":"code","211de233":"code","68440375":"code","ee0cde3a":"code","49468c28":"code","75ae23db":"code","b927d833":"code","0aaad79e":"code","3f327de9":"code","6345e74a":"markdown","2107af8a":"markdown","ee3557ba":"markdown","14fbc31d":"markdown","4f708421":"markdown","3f6d3309":"markdown","149735c3":"markdown","667b4455":"markdown","68222a73":"markdown","570b3fed":"markdown","37fbd11e":"markdown","120a946d":"markdown","f744cb0a":"markdown","36667fac":"markdown","f2ed1fea":"markdown","55ca80a2":"markdown","62d758b4":"markdown","a43c7b1a":"markdown","fdd03ed4":"markdown","85620b55":"markdown","ccf58a6b":"markdown","e258e3a3":"markdown","f3e03377":"markdown","9bc41f95":"markdown","fbbfc36b":"markdown","50612f45":"markdown","ebfd0c82":"markdown"},"source":{"a3016839":"import os\nfrom glob import glob\nimport numpy as np\nimport math\nimport pandas as pd\nfrom tqdm.notebook import tqdm\nimport copy\nimport pickle as pkl\nimport json\nimport matplotlib.pyplot as plt\nimport multiprocessing\n\nimport scipy\nfrom scipy.ndimage import gaussian_filter1d\nfrom scipy.spatial.distance import cdist\nfrom sklearn.model_selection import GroupKFold\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import AdamW, lr_scheduler\n\nimport gc\nimport warnings\nwarnings.filterwarnings('ignore')","20896a8c":"ts_data = glob('..\/input\/iln-interpolate-waypoints-using-imu-data\/*.csv')\nts = {}\nfor site in tqdm(ts_data):\n    df = pd.read_csv(site, usecols = ['wifi_ts'])\n    site = site.split('\/')[-1].split('_')[0]\n    ts[site] = df.values","a80059e8":"data_dir = '..\/input\/indoor-with-delta'\nfiles = glob(f'{data_dir}\/*_train.csv')\n\ntrain_data = pd.read_pickle(os.path.join(data_dir, 'train_all.pkl'))\ntrain_data['timestamp'] = np.zeros(train_data.shape[0], dtype = int)","ce00f196":"ts = []\nfor file in tqdm(files):\n    df = pd.read_csv(file, usecols = ['wifi_ts', 'path']).sort_values('wifi_ts')\n    ts.append(df)\n    \nts = pd.concat(ts)\nts = ts.groupby('path').apply(lambda x: x['wifi_ts'].values).to_dict()","d21d7bba":"timestamp = []\n\nfor path in tqdm(train_data['path'].unique()):\n    timestamp.append(ts[path])\ntrain_data['timestamp'] = np.concatenate(timestamp)","b12784ab":"train_data.rename(columns = {'site': 'building'}, inplace = True)\ntrain_data['group'] = train_data['path'].map(dict(zip(train_data['path'].unique(), range(train_data['path'].nunique()))))\ntrain_data['site_path_timestamp'] = ['_'.join([i,j,str(k)]) for i, j, k in zip(train_data.building, train_data.path, train_data.timestamp)]\ntrain_data.head()","3779bc86":"print('The number of NaN values is:')\nprint(train_data.isna().any().any())\nprint('Data information:')\ntrain_data.info()","39d7ab52":"with open('..\/input\/iln-wifi-and-building-mapping\/label_encoder_bssid.pkl', 'rb') as f:\n    lbl_bssid = pkl.load(f)\n    \nwith open('..\/input\/iln-wifi-and-building-mapping\/building_map.json', 'r') as f:\n    building_map = json.load(f)\n\nbssid_map = dict(zip(lbl_bssid.classes_, lbl_bssid.transform(lbl_bssid.classes_)))","c71ec027":"# Encode BSSID\nbssid_features = [i for i in train_data.columns if i.startswith('bssid_')]\nrssi_features = [i for i in train_data.columns if i.startswith('rssi_')]\ntimegap_features = [i for i in train_data.columns if i.startswith('gap_')]\ntrain_data[bssid_features] = train_data[bssid_features].applymap(lambda x: bssid_map[x])","d809d2cd":"train_data[bssid_features][train_data[rssi_features] == -999] = 0","9e5ce0ed":"# Encode building\ntrain_data['building'] = train_data.building.map(building_map)\ntrain_data.head()","46d8cdcd":"imu_data = pd.read_pickle('..\/input\/ilnaggregated-imu\/train_imu_all.pkl')\nimu_data['path'] = [i.split('_')[-1] for i in imu_data['site_floor_path']]\nimu_data.head()","01bdea77":"users = pd.read_csv('..\/input\/retrieving-user-id-from-leaked-wifi-feature\/df.csv', usecols = ['path_id', 'user_id'])\nuser_map = dict(zip(users['path_id'], users['user_id']))","affedefd":"train_data['user_id'] = train_data['path'].map(user_map)\ntrain_data.head()","22378eb7":"def feature_extraction_wifi(x):\n    # Index\n    idx = x.index.values.astype(int)\n    \n    # Path ID\n    path = x['path'].unique()[0]\n    \n    # Building\n    building = x['building'].unique().astype(int)\n    \n    # User ID\n    user = x['user_id'].unique().astype(int)\n    \n    # BSSID\n    bssid_feature = x[bssid_features].values.astype(int)\n    \n    # RSSI\n    rssi_feature = x[rssi_features].values.astype(float)\n    for i in range(len(rssi_features)):\n        rssi_feature[:,i] = gaussian_filter1d(rssi_feature[:,i], sigma = 2)\n    \n    # Timegap\n    timegap_feature = x[timegap_features].values.astype(float)\n    \n    # Delta waypoints\n    del_waypoints = x[['delta_x_hat', 'delta_y_hat']].values.astype(float)\n    \n    # Waypoints\n    waypoints = x[['x', 'y']].values.astype(float)\n    \n    # Floor\n    floor = x['floor'].unique().astype(int) + 2\n    \n    return path, idx, building, user, bssid_feature, rssi_feature, timegap_feature, del_waypoints, waypoints, floor","87c3db42":"# Raw IMU\nacce_ = ['acce_x', 'acce_y', 'acce_z']\ngyro_ = ['gyro_x', 'gyro_y', 'gyro_z']\nmagn_ = ['magn_x', 'magn_y', 'magn_z']\nahrs_ = ['ahrs_x', 'ahrs_y', 'ahrs_z']\nimu_ = acce_ + gyro_ + magn_ + ahrs_\n\ndef feature_extraction_imu(df):\n    imu = df[imu_].values.astype(int)\n    return imu\n\nimu_group = imu_data.groupby('path').apply(feature_extraction_imu)","8b38d85f":"drop_path = train_data.path.unique()[np.in1d(train_data.path.unique(), np.array(imu_group.index))]\ntrain_data = train_data[train_data['path'].isin(drop_path.tolist())].reset_index(drop = True)","ef2575b5":"class ILN_Dataset(Dataset):\n    def __init__(self, group, imu_group, max_len_wifi = 512, max_len_imu = 25_000):\n        self.group = group\n        self.imu_group = imu_group\n        self.max_len_wifi = max_len_wifi\n        self.max_len_imu = max_len_imu\n        \n    def __len__(self):\n        return len(self.group)\n    \n    def __getitem__(self, idx):\n        # Extract sequences\n        path, idx, building, user, bssid_feature, rssi_feature, timegap_feature, del_waypoints, waypoints, floor = self.group.iloc[idx]\n        \n        # Load IMU data\n        imu = self.imu_group.loc[path]\n        \n        seq_len_imu = imu.shape[0]\n        mask_imu = np.ones(seq_len_imu)\n        mask_imu_ = np.zeros(self.max_len_imu)\n        \n        # Sequence length\n        seq_len_wifi = bssid_feature.shape[0]\n        mask = np.ones(seq_len_wifi)\n        \n        mask_ = np.zeros(self.max_len_wifi)\n        idx_ = np.zeros(self.max_len_wifi)\n        bssid_feature_ = np.zeros((self.max_len_wifi, bssid_feature.shape[1]))\n        rssi_feature_ = np.zeros((self.max_len_wifi, rssi_feature.shape[1]))\n        timegap_feature_ = np.zeros((self.max_len_wifi, timegap_feature.shape[1]))\n        del_waypoints_ = np.zeros((self.max_len_wifi, del_waypoints.shape[1]))\n        waypoints_ = np.zeros((self.max_len_wifi, waypoints.shape[1]))\n        \n        imu_ = np.zeros((self.max_len_imu, imu.shape[1]))\n        \n        if seq_len_wifi <= self.max_len_wifi:   # Pad\n            mask_[-seq_len_wifi:] = mask\n            idx_[-seq_len_wifi:] = idx\n            bssid_feature_[-seq_len_wifi:,:] = bssid_feature\n            rssi_feature_[-seq_len_wifi:,:] = rssi_feature\n            timegap_feature_[-seq_len_wifi:,:] = timegap_feature\n            del_waypoints_[-seq_len_wifi:,:] = del_waypoints\n            waypoints_[-seq_len_wifi:,:] = waypoints\n        else:    # Cut\n            mask_ = mask[-self.max_len_wifi:]\n            idx_ = idx[-self.max_len_wifi:]\n            bssid_feature_ = bssid_feature[-self.max_len_wifi:,:]\n            rssi_feature_ = rssi_feature[-self.max_len_wifi:,:]\n            timegap_feature_ = timegap_feature[-self.max_len_wifi:,:]\n            del_waypoints_ = del_waypoints[-self.max_len_wifi:,:]\n            waypoints_ = waypoints[-self.max_len_wifi:,:]\n        \n        if seq_len_imu > 0:\n            if seq_len_imu <= self.max_len_imu:   # Pad\n                mask_imu_[-seq_len_imu:] = mask_imu\n                imu_[-seq_len_imu:,:] = imu\n            else:    # Cut\n                mask_imu_ = mask_imu[-self.max_len_imu:]\n                imu_ = imu[-self.max_len_imu:,:]\n            \n        return {\n            'mask': torch.tensor(mask_, dtype = torch.bool),\n            'mask_imu': torch.tensor(mask_imu_, dtype = torch.bool),\n            'idx': torch.tensor(idx_, dtype = torch.int),\n            'imu': torch.tensor(imu_, dtype = torch.float),\n            'building': torch.tensor(building, dtype = torch.long),\n            'user': torch.tensor(user, dtype = torch.long),\n            'bssid': torch.tensor(bssid_feature_, dtype = torch.long),\n            'rssi': torch.tensor(rssi_feature_, dtype = torch.float),\n            'timegap': torch.tensor(timegap_feature_, dtype = torch.float),\n            'del_waypoints': torch.tensor(del_waypoints_, dtype = torch.float),\n            'waypoints': torch.tensor(waypoints_, dtype = torch.float),\n            'floor': torch.tensor(floor, dtype = torch.long)\n        }","f4df45a3":"def clones(module, N):\n    \"Produce N identical layers.\"\n    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n\ndef attention(query, key, value, mask=None, dropout=None):\n    \"Compute 'Scaled Dot Product Attention'\"\n    d_k = query.size(-1)\n    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n             \/ math.sqrt(d_k)\n    if mask is not None:\n        scores = scores.masked_fill(mask == 0, -1e9)\n    p_attn = F.softmax(scores, dim=-1)\n    if dropout is not None:\n        p_attn = dropout(p_attn)\n    return torch.matmul(p_attn, value), p_attn\n\n\nclass MultiHeadedAttention(nn.Module):\n    def __init__(self, d_model, nhead, dropout=0.1):\n        \"Take in model size and number of heads.\"\n        super(MultiHeadedAttention, self).__init__()\n        assert d_model % nhead == 0\n        # We assume d_v always equals d_k\n        self.d_k = d_model \/\/ nhead\n        self.nhead = nhead\n        self.linears = clones(nn.Linear(d_model, d_model, bias=False), 4) # Q, K, V, last\n        self.attn = None\n        self.dropout = nn.Dropout(p=dropout)\n\n    def forward(self, query, key, value, mask=None):\n        \"Implements Figure 2\"\n        if mask is not None:\n            # Same mask applied to all h heads.\n            mask = mask.unsqueeze(1)\n        nbatches = query.size(0)\n\n        # 1) Do all the linear projections in batch from d_model => h x d_k\n        query, key, value = \\\n            [l(x).view(nbatches, -1, self.nhead, self.d_k).transpose(1, 2)\n             for l, x in zip(self.linears, (query, key, value))]\n\n        # 2) Apply attention on all the projected vectors in batch.\n        x, self.attn = attention(query, key, value, mask=mask,\n                                 dropout=self.dropout)\n\n        # 3) \"Concat\" using a view and apply a final linear.\n        x = x.transpose(1, 2).contiguous() \\\n            .view(nbatches, -1, self.nhead * self.d_k)\n        return self.linears[-1](x)\n\n\nclass PositionwiseFeedForward(nn.Module):\n    \"Implements FFN equation.\"\n    def __init__(self, d_model, d_ff, dropout=0.1):\n        super(PositionwiseFeedForward, self).__init__()\n        self.w_1 = nn.Linear(d_model, d_ff)\n        self.w_2 = nn.Linear(d_ff, d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        return self.w_2(self.dropout(F.relu(self.w_1(x))))\n    \nclass EncoderLayer(nn.Module):\n    \"\"\"\n    Single Encoder block of SAINT\n    \"\"\"\n    def __init__(self, d_model, nhead, dim_feedforward = 1024, dropout = 0.1):\n        super().__init__()\n        self._self_attn = MultiHeadedAttention(d_model, nhead, dropout)\n        self._ffn = PositionwiseFeedForward(d_model, dim_feedforward, dropout)\n        self._layernorms = clones(nn.LayerNorm(d_model, eps=1e-6), 2)\n        self._dropout = nn.Dropout(dropout)\n\n    def forward(self, src, mask = None):\n        \"\"\"\n        query: question embeddings\n        key: interaction embeddings\n        \"\"\"\n        # self-attention block\n        src2 = self._self_attn(query=src, key=src, value=src, mask=mask)\n        src = src + self._dropout(src2)\n        src = self._layernorms[0](src)\n        src2 = self._ffn(src)\n        src = src + self._dropout(src2)\n        src = self._layernorms[1](src)\n        return src","8fa00da4":"class Lin_MultiHeadedAttention(nn.Module):\n    def __init__(self, d_model, nhead, dropout=0.1, max_len = 512, target_len = 400):\n        \"Take in model size and number of heads.\"\n        super(Lin_MultiHeadedAttention, self).__init__()\n        assert d_model % nhead == 0\n        # We assume d_v always equals d_k\n        self.d_k = d_model \/\/ nhead\n        self.nhead = nhead\n        self.linears = clones(nn.Linear(d_model, d_model, bias=False), 4) # Q, K, V, last\n        self.linear_key_value = clones(nn.Linear(max_len, 256, bias= False), 3)\n        self.linear_query = nn.Linear(max_len, target_len)\n        self.attn = None\n        self.dropout = nn.Dropout(p=dropout)\n\n    def forward(self, query, key, value, mask=None):\n        \"Implements Figure 2\"\n        if mask is not None:\n            # Same mask applied to all h heads.\n            mask = mask.unsqueeze(-1)\n            # Mask query, key and value\n            query = query * mask\n            key = key * mask\n            mask = value * mask\n        nbatches = query.size(0)\n        \n        # 1) Do all the linear projections in batch from d_model => h x d_k\n        query, key, value = \\\n            [l(x).view(nbatches, -1, self.nhead, self.d_k).transpose(1, 2)\n             for l, x in zip(self.linears, (query, key, value))]\n        \n        # 2) Linear projections\n        key, value = \\\n            [l(x).transpose(-1, -2)\n             for l, x in zip(self.linear_key_value, (key.transpose(-1,-2), value.transpose(-1,-2)))]\n        \n        query = self.linear_query(query.transpose(-1, -2)).transpose(-1, -2)\n\n        # 3) Apply attention on all the projected vectors in batch.\n        x, self.attn = attention(query, key, value, mask=None,\n                                 dropout=self.dropout)\n\n        # 4) \"Concat\" using a view and apply a final linear.\n        x = x.transpose(1, 2).contiguous() \\\n            .view(nbatches, -1, self.nhead * self.d_k)\n        return self.linears[-1](x)\n    \nclass Lin_EncoderLayer(nn.Module):\n    \"\"\"\n    Single Encoder block of SAINT\n    \"\"\"\n    def __init__(self, d_model, nhead, dim_feedforward = 1024, dropout = 0.1, max_len = 512, target_len = 400):\n        super().__init__()\n        self._self_attn = Lin_MultiHeadedAttention(d_model, nhead, dropout = dropout, max_len = max_len, target_len = target_len)\n        self._ffn = PositionwiseFeedForward(d_model, dim_feedforward, dropout)\n        self._layernorms = clones(nn.LayerNorm(d_model, eps=1e-6), 2)\n        self._dropout = nn.Dropout(dropout)\n\n    def forward(self, src, mask = None):\n        \"\"\"\n        query: question embeddings\n        key: interaction embeddings\n        \"\"\"\n        # self-attention block\n        src2 = self._self_attn(query=src, key=src, value=src, mask=mask)\n        src = self._dropout(src2)\n        src = self._layernorms[0](src)\n        src2 = self._ffn(src)\n        src = src + self._dropout(src2)\n        src = self._layernorms[1](src)\n        return src","91c74cf8":"class PositionalEncoding(nn.Module):\n\n    def __init__(self, d_model, dropout=0.1, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        self.max_len = max_len\n        self.dropout = nn.Dropout(p=dropout)\n        self.div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) \/ d_model))\n\n    def forward(self, x, mask = None):\n        if mask is not None:\n            position = torch.cumsum(mask.unsqueeze(-1), dim = 1)\n        else:\n            position = torch.repeat_interleave(torch.arange(0, self.max_len).unsqueeze(-1).unsqueeze(0), x.shape[0], dim = 0)\n        \n        pe = torch.zeros(x.shape).to(x.device)\n        div_term = self.div_term.to(x.device)\n        pe[:,:,0::2] = torch.sin(position * div_term)\n        pe[:,:,1::2] = torch.cos(position * div_term)\n        \n        x = x + pe\n        \n        return self.dropout(x)","cc736116":"class ILN_Transformer(nn.Module):\n    def __init__(self, num_feature_bssid = 100, num_feature_rssi = 100, num_building = 24, num_bssid = 216210, num_user = 27551, \n                 num_floor = 11, num_feature_imu = 12, d_model = 512, nhead = 4, max_len = 512, max_len_imu = 25_000, droprate = 0.1):\n        super().__init__()\n        self.num_feature_bssid = num_feature_bssid\n        self.num_feature_rssi = num_feature_rssi\n        self.num_building = num_building\n        self.num_bssid = num_bssid\n        self.num_user = num_user\n        self.num_floor = num_floor\n        self.d_model = d_model\n        self.nhead = nhead\n        self.max_len = max_len\n        self.max_len_imu = max_len_imu\n        self.droprate = droprate\n        \n        ############################################ Wifi ############################################\n        # Embedding layers\n        self.building_embedding = nn.Embedding(num_embeddings = self.num_building, embedding_dim = self.d_model)\n        self.user_embedding = nn.Embedding(num_embeddings = self.num_user, embedding_dim = self.d_model)\n        self.bssid_embedding = nn.Embedding(num_embeddings = self.num_bssid, embedding_dim = self.d_model \/\/ 2, padding_idx = 0)\n        \n        # Linear layers for BSSID\n        self.linear_bssid = nn.Linear(self.num_feature_bssid, 1)\n        \n        # Linear layers for RSSI and timegap\n        self.linear_rssi_timegap = nn.Linear(2, 1)\n        self.linear_rssi = nn.Linear(self.num_feature_rssi, self.d_model \/\/ 2)\n        self.layer_norm_rssi = nn.LayerNorm(self.d_model \/\/ 2)\n        self.dropout_rssi = nn.Dropout(droprate)\n        \n        ############################################ IMU ############################################\n        # Linear projection\n        self.linear_imu_rotation = nn.Sequential(\n            nn.Linear(num_feature_imu, d_model \/\/ 2),\n            nn.LayerNorm(d_model \/\/ 2),\n            nn.ReLU(),\n            nn.Dropout(droprate)\n        )\n        \n        # Positional encoder\n        self.position_imu = PositionalEncoding(d_model = self.d_model \/\/ 2, dropout = self.droprate, max_len = self.max_len_imu)   # Because the IMU sequences are super long, we need a smaller model size to fit to the memory capacity\n        \n        # Attention\n        self.attn_encoder_imu = Lin_EncoderLayer(d_model = self.d_model \/\/ 2, nhead = self.nhead, dim_feedforward = 256, dropout = self.droprate, max_len = self.max_len_imu, target_len = self.max_len)\n        \n        # self.shrink_time = nn.Linear(self.max_len_imu, self.max_len)\n        \n        # Output layer\n        self.imu_output = nn.LSTM(input_size = d_model \/\/ 2, hidden_size = d_model \/\/ 2, bidirectional = True, batch_first = True)\n        \n        ############################################ Concatenate and prediction ############################################\n        # Middle linear layers\n        self.middle_linear = nn.Sequential(\n            nn.Linear(2 * d_model, d_model),\n            nn.LayerNorm(d_model),\n            nn.ReLU(),\n            nn.Dropout(droprate)\n        )\n        \n        # LSTM\n        self.lstms = clones(nn.LSTM(input_size = self.d_model, hidden_size = self.d_model \/\/ 2, bidirectional = True, batch_first = True), 3)\n        \n        # Positional encoder\n        self.position = PositionalEncoding(d_model = self.d_model, dropout = self.droprate, max_len = self.max_len)\n        \n        # Attention\n        self.attn_encoder = EncoderLayer(d_model = self.d_model, nhead = self.nhead, dim_feedforward = 512, dropout = self.droprate)\n        \n        self.lstm = nn.LSTM(input_size = self.d_model + 2, hidden_size = self.d_model \/\/ 2, bidirectional = True, batch_first = True)\n        self.linear_after_encoder = nn.Linear(self.d_model, self.d_model)\n        self.layernorm_after_encoder = nn.LayerNorm(self.d_model)\n        \n        # Output\n        self.linear_waypoints = nn.Linear(self.d_model, 2)\n        \n        # Activation\n        self.softmax = nn.Softmax()\n        self.prelu = nn.PReLU()\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(self.droprate)\n        \n    def _get_pad_mask(self, seq, pad_idx):\n        return (seq == pad_idx).unsqueeze(-2).to(seq.device)\n    \n    def _get_subsequent_mask(self, seq):\n        sz_b, len_s = seq.size()\n        subsequent_mask = torch.triu(torch.ones((1, len_s, len_s), device = seq.device), diagonal = 1).bool()\n        return subsequent_mask\n        \n    def forward(self, mask, mask_imu, building, user, bssid, rssi, timegap, imu, del_waypoints):\n        ############################################ Wifi ############################################\n        # Embed\n        building = self.building_embedding(building)\n        user = self.user_embedding(user)\n        bssid = self.bssid_embedding(bssid)\n        \n        # BSSID\n        bssid = self.linear_bssid(bssid.transpose(-2,-1)).squeeze(-1)\n        \n        # RSSI and timegap\n        rssi = torch.cat((rssi.unsqueeze(-1), timegap.unsqueeze(-1)), dim = -1)\n        rssi = self.linear_rssi_timegap(rssi).squeeze(-1)\n        rssi = self.linear_rssi(rssi)\n        rssi = self.layer_norm_rssi(rssi)\n        rssi = self.dropout_rssi(rssi)\n        \n        ############################################ IMU ############################################\n        # Projection\n        imu = self.linear_imu_rotation(imu)\n        \n        # Positional embedding\n        imu = self.position_imu(imu, mask = mask_imu)\n        \n        # Attention\n        imu = self.attn_encoder_imu(imu, mask = mask_imu)\n        \n        # Shrink time\n        # imu = self.shrink_time(imu.transpose(-1,-2)).transpose(-1,-2)\n        \n        # Output IMU\n        imu, _ = self.imu_output(imu)\n        \n        ############################################ Concatenate and prediction ############################################\n        x_wifi = torch.cat((bssid, rssi), dim = -1)\n        \n        x = self.middle_linear(torch.cat((bssid, rssi, imu), dim = -1)) + building + user\n        \n        for i, layer in enumerate(self.lstms):\n            x, _ = layer(x)\n        \n        # Positional encoding\n        x = self.position(x, mask = mask)\n        \n        # Mask\n        mask = ~(self._get_pad_mask(mask, False) | self._get_subsequent_mask(mask))\n        \n        # Feed it to the Encoder\n        x = self.attn_encoder(x, mask)\n        \n        # Feed it to the lstm\n        x, _ = self.lstm(torch.cat((x + x_wifi + imu, del_waypoints), dim = -1))\n        \n        # Feed over one more linear layer\n        x = self.linear_after_encoder(self.dropout(self.prelu(x)))\n        x = self.layernorm_after_encoder(x)\n        x = self.dropout(self.prelu(x))\n        \n        # Concatenate floor output and x + building\n        x = self.linear_waypoints(x)\n        \n        return x","acfbf189":"def metrics(output_way, output_floor, way, floor):\n    first_term = np.mean(np.sqrt(np.sum((output_way - way)**2, axis = 1)))\n    second_term = 15 * np.mean(np.abs(output_floor - floor))\n    return first_term, second_term\n\ndef compute_cv_score(oof_):\n    output_way = oof_[['pred_x', 'pred_y']].values\n    output_floor = oof_['pred_floor'].values\n    \n    way = oof_[['true_x', 'true_y']].values\n    floor = oof_['true_floor'].values\n    \n    loss_waypoints, loss_floor = metrics(output_way, output_floor, way, floor)\n    return loss_waypoints, loss_floor","9c67ad54":"def infer_fn(model, infer_dataloader, device = 'cpu'):\n    model.eval()\n    \n    index = []\n    \n    way_pred = []\n    \n    way_true = []\n    floor_true = []\n    \n    loss = 0\n    \n    # tbar = tqdm(infer_dataloader)\n    for item in infer_dataloader:\n        padding_mask = item['mask'].to(device)\n        padding_mask_imu = item['mask_imu'].to(device)\n        imu = item['imu'].to(device)\n        building = item['building'].to(device)\n        user = item['user'].to(device)\n        idx = item['idx'].to(device)\n        bssid = item['bssid'].to(device)\n        rssi = item['rssi'].to(device)\n        timegap = item['timegap'].to(device)\n        del_waypoints = item['del_waypoints'].to(device)\n        waypoints = item['waypoints'].to(device)\n        floor = item['floor'].to(device)\n        \n        # Feed input to the model\n        with torch.no_grad():\n            output_waypoints = model(padding_mask, padding_mask_imu, building, user, bssid, rssi, timegap, imu, del_waypoints)\n        \n        # True floor\n        floor = torch.repeat_interleave(floor.unsqueeze(1), waypoints.shape[1], dim = 1)\n        \n        # Extract waypoints\n        output_waypoints = output_waypoints[padding_mask,:]\n        waypoints = waypoints[padding_mask,:]\n        \n        floor = floor[padding_mask,:] - 2\n        \n        idx = idx[padding_mask]\n        \n        # Store results\n        index.append(idx.cpu().detach().numpy())\n        \n        way_pred.append(output_waypoints.cpu().detach().numpy())\n        way_true.append(waypoints.cpu().detach().numpy())\n        \n        floor_true.append(floor.cpu().detach().numpy())\n        \n    # Stack\n    index = np.concatenate(index)\n    \n    way_pred = np.vstack(way_pred)\n    way_true = np.vstack(way_true)\n    \n    floor_true = np.vstack(floor_true)\n    \n    oof_ = pd.DataFrame(np.hstack((way_true, floor_true, way_pred, floor_true)), index = index, \n                        columns = ['true_x', 'true_y', 'true_floor', 'pred_x', 'pred_y', 'pred_floor'])\n    \n    return oof_","59933b18":"class config():\n    mode = 'infer'\n    # For training\n    split = GroupKFold(n_splits = 5)\n    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n    # For dataloader\n    batch_size = 64\n    num_workers = 4\n    # For model\n    N = 2    # Number of encoder layers\n    d_model = 256\n    nhead = 2\n    max_len = 400\n    max_len_imu = 1024\n    droprate = 0.3\n    \ncfg = config()","e0072862":"model_path = '..\/input\/iln-models'\noof_ = []\n    \nfor i, (trn_idx, val_idx) in enumerate(tqdm(cfg.split.split(train_data.iloc[:,:200], \n                                                            train_data.building, \n                                                            train_data.group))):\n    \n    val = train_data.iloc[val_idx].copy()\n    \n    # Dataloader\n    infer_group = val.groupby('group').apply(feature_extraction_wifi)\n    infer_dataset = ILN_Dataset(infer_group, imu_group, max_len_wifi = cfg.max_len, max_len_imu = cfg.max_len_imu)\n    infer_dataloader = DataLoader(infer_dataset, batch_size = cfg.batch_size, num_workers = cfg.num_workers, shuffle = False)\n    \n    ckp = torch.load(os.path.join(model_path, f'model_best_fold_{i}_imu_v4.pt'), map_location = cfg.device)\n    model = ILN_Transformer(d_model = cfg.d_model, nhead = cfg.nhead, max_len_imu = cfg.max_len_imu,\n                            max_len = cfg.max_len, droprate = cfg.droprate).to(cfg.device)\n    model.load_state_dict(ckp['model_state_dict'])\n    \n    val_pred = infer_fn(model, infer_dataloader, device = cfg.device)\n    \n    val_pred['site_path_timestamp'] = val.loc[val_pred.index, 'site_path_timestamp'].values\n    \n    oof_.append(val_pred)\n\noof_ = pd.concat(oof_)\noof_ = oof_.sort_values('site_path_timestamp').reset_index(drop = True)","9889a9e2":"oof_.to_csv('oof_wifi.csv', index = None)","f8f2f494":"loss_waypoints, loss_floor = compute_cv_score(oof_)\nprint('OOF MPE waypoints:', loss_waypoints, ', OOF MPE floor:', loss_floor, ', OOF MPE:', loss_waypoints + loss_floor)","6ec80474":"oof_[['site', 'path', 'timestamp']] = [i.split('_') for i in oof_.site_path_timestamp]\noof_['siteid_path'] = ['_'.join([i, j]) for i, j in zip(oof_['site'], oof_['path'])]\noof_.set_index('siteid_path', inplace = True)\noof_.head()","40a46244":"train_waypoints = pd.read_csv('..\/input\/indoor-location-train-waypoints\/train_waypoints.csv')\ntrain_waypoints['site_path_timestamp'] = ['_'.join([i, j, str(k)]) for i, j, k in zip(train_waypoints.site, train_waypoints.path, train_waypoints.timestamp)]\nss = train_waypoints[['site_path_timestamp', 'floor', 'x', 'y', 'site', 'path', 'timestamp']]\nss.rename(columns = {'x': 'true_x', 'y': 'true_y'}, inplace = True)\nss['pred_floor'] = ss['floor'].values\n\nsamples = pd.DataFrame(ss.groupby(['site','path'])['timestamp'].apply(lambda x: list(x)))\nbuildings = oof_.site.unique()\nsamples.head()","8be42408":"!git clone --depth 1 https:\/\/github.com\/location-competition\/indoor-location-competition-20 indoor_location_competition_20\n!rm -rf indoor_location_competition_20\/data\n\nfrom indoor_location_competition_20.io_f import read_data_file\nimport indoor_location_competition_20.compute_f as compute_f","4dead49d":"from scipy.interpolate import interp1d\nfrom scipy.ndimage.filters import uniform_filter1d\n\ncolacce = ['Unnamed: 0','acce_x','acce_y','acce_z']\ncolahrs = ['Unnamed: 0','ahrs_x','ahrs_y','ahrs_z']\n\nerror_paths = []\n\nfor building in buildings:\n    print(building)\n    paths = oof_[oof_.site == building]['path'].unique()\n    # Acceleration info\n    for path_id in paths:\n        # Load IMU data of this path_id\n        tfm = pd.read_csv(glob(f'..\/input\/ilnimu-clean\/{building}_*_{path_id}.csv')[0], usecols = ['Unnamed: 0'] + imu_)\n        # Original predicted values:\n        xy = oof_.loc[building + '_' + path_id]\n        acce_datas = np.array(tfm[colacce],dtype = np.float)\n        ahrs_datas = np.array(tfm[colahrs],dtype = np.float)\n        posi_datas = np.array(xy[['timestamp', 'pred_x', 'pred_y']], dtype = np.float).reshape(-1,3)\n        # Outlier removal:\n        xyout = uniform_filter1d(posi_datas, size = 3, axis = 0, mode = 'reflect').reshape(-1,3)\n        xydiff = np.abs(posi_datas - xyout)\n        xystd = np.std(xydiff,axis = 0) * 3\n        posi_datas = posi_datas[(xydiff [:,1] < xystd[1]) & (xydiff[:,2] < xystd[2])]\n        # Step detection:\n        step_timestamps, step_indexs, step_acce_max_mins = compute_f.compute_steps(acce_datas)\n        stride_lengths = compute_f.compute_stride_length(step_acce_max_mins)\n        # Orientation detection:\n        headings = compute_f.compute_headings(ahrs_datas)\n        step_headings = compute_f.compute_step_heading(step_timestamps, headings)\n        rel_positions = compute_f.compute_rel_positions(stride_lengths, step_headings)\n        # Running average:\n        posi_datas = uniform_filter1d(posi_datas, size = 3, axis = 0, mode = 'reflect')[0::3,:]\n        # The 1st prediction timepoint should be earlier than the 1st step timepoint.\n        try:\n            rel_positions = rel_positions[rel_positions[:,0] > posi_datas[0,0],:]\n            # If two consecutive predictions are in-between two step datapoints,\n            # the last one is removed, causing error (in the \"split_ts_seq\" function).\n            posi_index = [np.searchsorted(rel_positions[:,0], x, side = 'right') for x in posi_datas[:,0]]\n            u, i1, i2 = np.unique(posi_index, return_index = True, return_inverse = True)\n            posi_datas = np.vstack([np.mean(posi_datas[i2 == i],axis = 0) for i in np.unique(i2)])\n            # Position correction:\n            step_positions = compute_f.correct_positions(rel_positions, posi_datas)\n            # Interpolate for timestamps in the testing set:\n            t = step_positions[:,0]\n            x = step_positions[:,1]\n            y = step_positions[:,2]\n        \n            fx = interp1d(t, x, kind = 'linear', fill_value = (x[0], x[-1]), bounds_error = False) #fill_value=\"extrapolate\"\n            fy = interp1d(t, y, kind = 'linear', fill_value = (y[0], y[-1]), bounds_error = False)\n            # Output result:\n            t0 = np.array(samples.loc[(building,path_id),'timestamp'], dtype = np.float64)\n\n            ss.loc[(ss.site == building) & (ss.path == path_id), 'pred_x'] = fx(t0)\n            ss.loc[(ss.site == building) & (ss.path == path_id), 'pred_y'] = fy(t0)\n        except:\n            error_paths.append(path_id)\n            pass\n\nss = ss.dropna(axis = 0).rename(columns = {'floor': 'true_floor'}).reset_index(drop = True)\nss['timestamp'] = ss['timestamp'].astype(float)\nss.head()","00911583":"ss.to_csv('oof_waypoints_before_pp.csv', index = None)","7c812697":"loss_waypoints, loss_floor = compute_cv_score(ss)\nprint('OOF MPE waypoints:', loss_waypoints, ', OOF MPE floor:', loss_floor, ', OOF MPE:', loss_waypoints + loss_floor)","00fe3b72":"# Helper Functions\ndef split_col(df):\n    df = pd.concat([\n        df['site_path_timestamp'].str.split('_', expand=True) \\\n        .rename(columns={0:'site',\n                         1:'path',\n                         2:'timestamp'}),\n        df\n    ], axis=1).copy()\n    return df\n\nfloor_map = {\"B2\":-2, \"B1\":-1, \"F1\":0, \"F2\": 1, \"F3\":2,\n             \"F4\":3, \"F5\":4, \"F6\":5, \"F7\":6,\"F8\":7,\"F9\":8,\n             \"1F\":0, \"2F\":1, \"3F\":2, \"4F\":3, \"5F\":4, \"6F\":5,\n             \"7F\":6, \"8F\": 7, \"9F\":8}\n\ndef plot_preds(\n    site,\n    floorNo,\n    sub=None,\n    true_locs=None,\n    base=\"..\/input\/indoor-location-navigation\",\n    show_train=True,\n    show_preds=True,\n    show_smoothed=False,\n    fix_labels=True,\n    map_floor=None\n):\n    \"\"\"\n    Plots predictions on floorplan map.\n    \n    map_floor : use a different floor's map\n    \"\"\"\n    if map_floor is None:\n        map_floor = floorNo\n    # Prepare width_meter & height_meter (taken from the .json file)\n    floor_plan_filename = f\"{base}\/metadata\/{site}\/{map_floor}\/floor_image.png\"\n    json_plan_filename = f\"{base}\/metadata\/{site}\/{map_floor}\/floor_info.json\"\n    with open(json_plan_filename) as json_file:\n        json_data = json.load(json_file)\n\n    width_meter = json_data[\"map_info\"][\"width\"]\n    height_meter = json_data[\"map_info\"][\"height\"]\n\n    floor_img = plt.imread(f\"{base}\/metadata\/{site}\/{map_floor}\/floor_image.png\")\n\n    fig, ax = plt.subplots(figsize=(12, 12))\n    plt.imshow(floor_img)\n\n    if show_train:\n        true_locs = true_locs.query('site == @site and floorNo == @map_floor').copy()\n        true_locs[\"x_\"] = true_locs[\"x\"] * floor_img.shape[0] \/ height_meter\n        true_locs[\"y_\"] = (\n            true_locs[\"y\"] * -1 * floor_img.shape[1] \/ width_meter\n        ) + floor_img.shape[0]\n        true_locs.query(\"site == @site and floorNo == @map_floor\").groupby(\"path\").plot(\n            x=\"x_\",\n            y=\"y_\",\n            style=\"+\",\n            ax=ax,\n            label=\"train waypoint location\",\n            color=\"grey\",\n            alpha=0.5,\n        )\n\n    if show_preds:\n        if not show_smoothed:\n            sub = sub.query('site == @site and floorNo == @floorNo').copy()\n            sub[\"x_\"] = sub[\"pred_x\"] * floor_img.shape[0] \/ height_meter\n            sub[\"y_\"] = (\n                sub[\"pred_y\"] * -1 * floor_img.shape[1] \/ width_meter\n            ) + floor_img.shape[0]\n            for path, path_data in sub.query(\n                \"site == @site and floorNo == @floorNo\"\n            ).groupby(\"path\"):\n                path_data.plot(\n                    x=\"x_\",\n                    y=\"y_\",\n                    style=\".-\",\n                    ax=ax,\n                    title=f\"{site} - floor - {floorNo}\",\n                    alpha=1,\n                    label=path,\n                )\n                \n        else:\n            sub = sub.query('site == @site and floorNo == @floorNo').copy()\n            sub[\"x_\"] = sub[\"pred_x\"] * floor_img.shape[0] \/ height_meter\n            sub[\"y_\"] = (\n                sub[\"pred_y\"] * -1 * floor_img.shape[1] \/ width_meter\n            ) + floor_img.shape[0]\n            for path, path_data in sub.query(\n                \"site == @site and floorNo == @floorNo\"\n            ).groupby(\"path\"):\n                path_data.plot(\n                    x=\"x_\",\n                    y=\"y_\",\n                    style=\".-\",\n                    ax=ax,\n                    title=f\"{site} - floor - {floorNo}\",\n                    alpha=1,\n                    label=path,\n                )\n                \n    if fix_labels:\n        handles, labels = ax.get_legend_handles_labels()\n        by_label = dict(zip(labels, handles))\n        plt.legend(\n            by_label.values(), by_label.keys(), loc=\"center left\", bbox_to_anchor=(1, 0.5)\n        )\n        \n    return fig, ax\n\ndef sub_process(sub, train_waypoints):\n    train_waypoints['isTrainWaypoint'] = True\n    sub = sub.rename(columns = {'pred_floor': 'floor', 'pred_x': 'x', 'pred_y': 'y'})\n    #sub = split_col(sub).copy()\n    sub = sub.merge(train_waypoints[['site','floorNo','floor']].drop_duplicates(), how='left')\n    sub = sub.merge(\n        train_waypoints[['x','y','site','floor','isTrainWaypoint']].drop_duplicates(),\n        how = 'left',\n        on = ['site','x','y','floor'])\n    sub['isTrainWaypoint'] = sub['isTrainWaypoint'].fillna(False)\n    sub = sub.rename(columns = {'floor': 'pred_floor', 'x': 'pred_x', 'y': 'pred_y'})\n    return sub.copy()","51e3ca90":"sub = sub_process(ss, train_waypoints)\n\n# Plot the training Data For an example Floor\nexample_site = '5d2709bb03f801723c32852c'\nexample_floorNo = 'F4'\n\nplot_preds(example_site, example_floorNo, sub,\n           train_waypoints, show_preds=True)\nplt.show()","d61e27c7":"import math\n\norder = 3\nfs = 50.0  # sample rate, Hz\n# fs = 100\n# cutoff = 3.667  # desired cutoff frequency of the filter, Hz\ncutoff = 3\n\nstep_distance = 0.8\nw_height = 1.7\nm_trans = -5\n\nfrom scipy.signal import butter, lfilter\n\ndef butter_lowpass(cutoff, fs, order=5):\n    nyq = 0.5 * fs\n    normal_cutoff = cutoff \/ nyq\n    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n    return b, a\n\ndef butter_lowpass_filter(data, cutoff, fs, order=5):\n    b, a = butter_lowpass(cutoff, fs, order=order)\n    y = lfilter(b, a, data)\n    return y\n\ndef peak_accel_threshold(data, timestamps, threshold):\n    d_acc = []\n    last_state = 'below'\n    crest_troughs = 0\n    crossings = []\n\n    for i, datum in enumerate(data):\n        \n        current_state = last_state\n        if datum < threshold:\n            current_state = 'below'\n        elif datum > threshold:\n            current_state = 'above'\n\n        if current_state is not last_state:\n            if current_state is 'above':\n                crossing = [timestamps[i], threshold]\n                crossings.append(crossing)\n            else:\n                crossing = [timestamps[i], threshold]\n                crossings.append(crossing)\n\n            crest_troughs += 1\n        last_state = current_state\n    return np.array(crossings)\n\ndef steps_compute_rel_positions(sample_file):\n    \n    mix_acce = np.sqrt(sample_file.acce[:,1:2]**2 + sample_file.acce[:,2:3]**2 + sample_file.acce[:,3:4]**2)\n    mix_acce = np.concatenate([sample_file.acce[:,0:1], mix_acce], 1)\n    mix_df = pd.DataFrame(mix_acce)\n    mix_df.columns = [\"timestamp\",\"acce\"]\n    \n    filtered = butter_lowpass_filter(mix_df[\"acce\"], cutoff, fs, order)\n\n    threshold = filtered.mean() * 1.1\n    crossings = peak_accel_threshold(filtered, mix_df[\"timestamp\"], threshold)\n\n    step_sum = len(crossings)\/2\n    distance = w_height * 0.4 * step_sum\n\n    mag_df = pd.DataFrame(sample_file.magn)\n    mag_df.columns = [\"timestamp\",\"x\",\"y\",\"z\"]\n    \n    acce_df = pd.DataFrame(sample_file.acce)\n    acce_df.columns = [\"timestamp\",\"ax\",\"ay\",\"az\"]\n    \n    mag_df = pd.merge(mag_df,acce_df,on=\"timestamp\")\n    mag_df.dropna()\n    \n    time_di_list = []\n\n    for i in mag_df.iterrows():\n\n        gx,gy,gz = i[1][1],i[1][2],i[1][3]\n        ax,ay,az = i[1][4],i[1][5],i[1][6]\n\n        roll = math.atan2(ay,az)\n        pitch = math.atan2(-1*ax , (ay * math.sin(roll) + az * math.cos(roll)))\n\n        q = m_trans - math.degrees(math.atan2(\n            (gz*math.sin(roll)-gy*math.cos(roll)),(gx*math.cos(pitch) + gy*math.sin(roll)*math.sin(pitch) + gz*math.sin(pitch)*math.cos(roll))\n        )) -90\n        if q <= 0:\n            q += 360\n        time_di_list.append((i[1][0],q))\n\n    d_list = [x[1] for x in time_di_list]\n    \n    steps = []\n    step_time = []\n    di_dict = dict(time_di_list)\n\n    for n,i in enumerate(crossings[:,:1]):\n        if n % 2 == 1:\n            continue\n        direct_now = di_dict[i[0]]\n        dx = math.sin(math.radians(direct_now))\n        dy = math.cos(math.radians(direct_now))\n#         print(int(n\/2+1),\"\u6b69\u76ee\/x:\",dx,\"\/y:\",dy,\"\/\u89d2\u5ea6\uff1a\",direct_now)\n        steps.append((i[0],dx,dy))\n        step_time.append(i[0])\n    \n        step_dtime = np.diff(step_time)\/1000\n        step_dtime = step_dtime.tolist()\n        step_dtime.insert(0,5)\n        \n        rel_position = []\n\n        wp_idx = 0\n#         print(\"WP:\",round(sample_file.waypoint[0,1],3),round(sample_file.waypoint[0,2],3),sample_file.waypoint[0,0])\n#         print(\"------------------\")\n        for p,i in enumerate(steps):\n            step_distance = 0\n            if step_dtime[p] >= 1:\n                step_distance = w_height*0.25\n            elif step_dtime[p] >= 0.75:\n                step_distance = w_height*0.3\n            elif step_dtime[p] >= 0.5:\n                step_distance = w_height*0.4\n            elif step_dtime[p] >= 0.35:\n                step_distance = w_height*0.45\n            elif step_dtime[p] >= 0.2:\n                step_distance = w_height*0.5\n            else:\n                step_distance = w_height*0.4\n\n#             step_x += i[1]*step_distance\n#             step_y += i[2]*step_distance\n            \n            rel_position.append([i[0], i[1]*step_distance, i[2]*step_distance])\n#     print(rel_position)\n    \n    return np.array(rel_position)","0a39b6cb":"def compute_step_heading(step_timestamps, headings):\n    sin_head = np.sin(headings[:,1])\n    cos_head = np.cos(headings[:,1])\n    head = np.vstack((np.searchsorted(step_timestamps, headings[:,0]), sin_head, cos_head)).T\n    split_steps = np.split(head[:,1:], np.unique(head[:, 0], return_index = True)[1][1:])\n    step_headings = np.array([np.sum(arr, axis = 0) for arr in split_steps[:-1]])\n    step_headings = np.arctan2(step_headings[:,0], step_headings[:,1])\n    step_headings = np.vstack((step_timestamps, step_headings)).T\n    return step_headings\n\ndef compute_rel_positions(acce_datas, ahrs_datas):\n    step_timestamps, step_indexs, step_acce_max_mins = compute_f.compute_steps(acce_datas)\n    headings = compute_f.compute_headings(ahrs_datas)\n    headings[:,1] = uniform_filter1d(headings[:,1], size = 50)\n    stride_lengths = compute_f.compute_stride_length(step_acce_max_mins)\n    step_headings = compute_f.compute_step_heading(step_timestamps, headings)    # compute_f.\n    rel_positions = compute_f.compute_rel_positions(stride_lengths, step_headings)\n    return rel_positions\n\ndef correct_path(args):\n    path, path_df = args\n    \n    site = path_df.site.unique()[0]\n    floor = path_df.pred_floor.unique()[0]\n    \n    T_ref  = path_df['timestamp'].values\n    xy_hat = path_df[['pred_x', 'pred_y']].values\n    \n    # Find the file path\n    floor_name = os.listdir('..\/input\/indoor-location-navigation\/train\/' + site)\n\n    for name in floor_name:\n        if floor_map[name] == floor:\n            file = '..\/input\/indoor-location-navigation\/train\/' + site + '\/' + name + '\/' + path + '.txt'\n            break\n            \n    try:\n        example = read_data_file(file)\n    \n        rel_positions1 = compute_rel_positions(example.acce, example.ahrs)\n        rel_positions2 = steps_compute_rel_positions(example)\n        rel1 = rel_positions1.copy()\n        rel2 = rel_positions2.copy()\n        rel1[:,1:] = rel_positions1[:,1:] \/ 2\n        rel2[:,1:] = rel_positions2[:,1:] \/ 2\n        rel_positions = np.vstack([rel1,rel2])\n        rel_positions = rel_positions[np.argsort(rel_positions[:, 0])]\n\n        if T_ref[-1] > rel_positions[-1, 0]:\n            rel_positions = [np.array([[0, 0, 0]]), rel_positions, np.array([[T_ref[-1], 0, 0]])]\n        else:\n            rel_positions = [np.array([[0, 0, 0]]), rel_positions]\n        rel_positions = np.concatenate(rel_positions)\n\n        T_rel = rel_positions[:, 0]\n        delta_xy_hat = np.diff(scipy.interpolate.interp1d(T_rel, np.cumsum(rel_positions[:, 1:3], axis=0), axis=0)(T_ref), axis=0)\n\n        N = xy_hat.shape[0]\n        delta_t = np.diff(T_ref)\n        alpha = (7.2)**(-2) * np.ones(N)\n        beta  = (0.3 + 0.3 * 1e-3 * delta_t)**(-2)\n        A = scipy.sparse.spdiags(alpha, [0], N, N)\n        B = scipy.sparse.spdiags(beta, [0], N-1, N-1)\n        D = scipy.sparse.spdiags(np.stack([-np.ones(N), np.ones(N)]), [0, 1], N-1, N)\n\n        Q = A + (D.T @ B @ D)\n        c = (A @ xy_hat) + (D.T @ (B @ delta_xy_hat))\n        xy_star = scipy.sparse.linalg.spsolve(Q, c)\n\n        return pd.DataFrame({\n            'site_path_timestamp' : path_df['site_path_timestamp'],\n            'pred_floor' : path_df['pred_floor'],\n            'pred_x' : xy_star[:, 0],\n            'pred_y' : xy_star[:, 1],\n            'true_floor' : path_df['true_floor'],\n            'true_x' : path_df['true_x'],\n            'true_y' : path_df['true_y'],\n        })\n    except:\n        print('Mis-predict floor: ', file)\n        return pd.DataFrame({\n            'site_path_timestamp' : path_df['site_path_timestamp'],\n            'pred_floor' : path_df['pred_floor'],\n            'pred_x' : path_df['pred_x'],\n            'pred_y' : path_df['pred_x'],\n            'true_floor' : path_df['true_floor'],\n            'true_x' : path_df['true_x'],\n            'true_y' : path_df['true_y'],\n        })","af4630be":"processes = multiprocessing.cpu_count()\nwith multiprocessing.Pool(processes = processes) as pool:\n    dfs = pool.imap_unordered(correct_path, ss.groupby('path'))\n    dfs = tqdm(dfs)\n    dfs = list(dfs)\nss = pd.concat(dfs).sort_values('site_path_timestamp').reset_index(drop = True)","211de233":"ss.to_csv('oof_waypoints_after_Saito.csv', index = None)","68440375":"loss_waypoints, loss_floor = compute_cv_score(ss)\nprint(\"After Saito's post-processing\")\nprint('OOF MPE waypoints:', loss_waypoints, ', OOF MPE floor:', loss_floor, ', OOF MPE:', loss_waypoints + loss_floor)\n\ntmp = ss['site_path_timestamp'].apply(lambda s : pd.Series(s.split('_')))\nss['site'] = tmp[0]\nss['path'] = tmp[1]\nss['timestamp'] = tmp[2].astype(float)","ee0cde3a":"sub = sub_process(ss, train_waypoints)\n\n# Plot the training Data For an example Floor\nexample_site = '5d2709bb03f801723c32852c'\nexample_floorNo = 'F4'\n\nplot_preds(example_site, example_floorNo, sub,\n           train_waypoints, show_preds=True)\nplt.show()","49468c28":"from shapely.geometry import Polygon\nfrom shapely.ops import nearest_points\nfrom shapely.geometry import Point\n\ndef fix_prediction(args):\n    # Unpack\n    (site, floor), df = args\n    \n    # Find the file path\n    floor_name = os.listdir('..\/input\/indoor-location-navigation-scaled-geojson\/scaled_geojson\/' + site)\n    for name in floor_name:\n        if floor_map[name] == floor:\n            file = '..\/input\/indoor-location-navigation-scaled-geojson\/scaled_geojson\/' + site + '\/' + name + '\/shapely_geometry.pkl'\n            break\n            \n    # Open the corridor\n    with open(file, 'rb') as f:\n        corridor = pkl.load(f)\n        \n    # Find the outside-corridor points and force them into the corridor\n    out_corridor = []\n    out_corridor_idx = []\n    corridor_nearest_points = []\n    for i in range(df.shape[0]):\n        p = Point(df[['pred_x', 'pred_y']].iloc[i].values)\n        if not p.within(corridor):\n            out_corridor.append(p)\n            out_corridor_idx.append(df[['pred_x', 'pred_y']].index[i])\n            nearest_p, _ = nearest_points(corridor, p)\n            x, y = nearest_p.xy[0][0], nearest_p.xy[1][0]\n            corridor_nearest_points.append([x, y])\n    \n    if len(corridor_nearest_points) != 0:\n        df.loc[out_corridor_idx, ['pred_x', 'pred_y']] = np.array(corridor_nearest_points)\n    \n    return df","75ae23db":"ss[['site', 'path', 'timestamp']] = np.array([i.split('_') for i in ss.site_path_timestamp])\n\nprocesses = multiprocessing.cpu_count()\nwith multiprocessing.Pool(processes = processes) as pool:\n    dfs = pool.imap_unordered(fix_prediction, ss.groupby(['site', 'pred_floor']))\n    dfs = tqdm(dfs)\n    dfs = list(dfs)\nss = pd.concat(dfs).sort_values('site_path_timestamp')","b927d833":"ss.to_csv('oof_waypoints_after_pp.csv', index = None)","0aaad79e":"loss_waypoints, loss_floor = compute_cv_score(ss)\nprint('After Push-to-corridor post-processing')\nprint('OOF MPE waypoints:', loss_waypoints, ', OOF MPE floor:', loss_floor, ', OOF MPE:', loss_waypoints + loss_floor)","3f327de9":"sub = sub_process(ss, train_waypoints)\n\n# Plot the training Data For an example Floor\nexample_site = '5d2709bb03f801723c32852c'\nexample_floorNo = 'F4'\n\nplot_preds(example_site, example_floorNo, sub, \n           train_waypoints, show_preds = True)\nplt.show()","6345e74a":"* CV at wifi observations","2107af8a":"# Interpolate the prediction","ee3557ba":"* Visualize post-processing after Saito's post processing","14fbc31d":"# Import data","4f708421":"* Visualize after Push-to-Corridor post-processing","3f6d3309":"* Visualization functions","149735c3":"# Dataset","667b4455":"# Post-processing steps","68222a73":"# Necessary packages","570b3fed":"* IMU data","37fbd11e":"# Description\n\n##### The OOF of the model named \"model_best_fold_0_imu_v4\";\n##### The inference process is: (1) prediction at wifi observations, (ii) interpolation these prediction to the true waypoint observation, (iii) Saito's post-processing + magnetic-fields based step adjustment, (iv) push-to-corridor post-processing;\n\n##### This version: \n* $\\alpha = 7.2, \\beta = 0.3$;\n* Use a modified step-heading estimating function.\n\n##### This model score profile is:\n* OOF CV before interpolation and before post-processing (MPE at wifi observations): 6.358;\n* OOF CV after interpolation and before post-processing (MPE at waypoint observations): 6.773;\n* Public LB after interpolation and before post-processing: 6.461;\n* OOF CV after interpolation and after post-processing: ;\n* Public LB after interpolation and after post-processing: 3.937;\n* Moving average of headings, time window: 50 steps (1 second).","120a946d":"* Visualization of raw prediction","f744cb0a":"# Configuration and util functions","36667fac":"# Feature extraction","f2ed1fea":"* Drop the Wifi data points that do not have the corresponding IMU data","55ca80a2":"* Saito's post-processing","62d758b4":"* Metrics","a43c7b1a":"# Model","fdd03ed4":"# Main","85620b55":"* Inference functions","ccf58a6b":"# Wifi encoder and building map","e258e3a3":"* Add timestamp","f3e03377":"* Configuration","9bc41f95":"* Compute the MPE CV","fbbfc36b":"* Push to corridor post-processing","50612f45":"* Add user ID","ebfd0c82":"* Format the data"}}