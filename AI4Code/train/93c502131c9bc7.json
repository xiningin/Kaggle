{"cell_type":{"a92708e2":"code","e9b00b70":"code","d132460f":"code","a1556c75":"code","1fd3fd2b":"code","fd6e13ab":"code","8183738c":"code","eea7b578":"code","cfbf43a8":"code","45e84e00":"code","5cd4e804":"code","893aa700":"code","e3967f28":"code","f7bc777f":"code","53b3488e":"code","45b4d190":"code","5d9f2516":"code","c023437b":"code","506dc50b":"code","88b5ffbf":"markdown","42cbe5b1":"markdown","270965be":"markdown","7a7e65d3":"markdown","ca975ccb":"markdown","d46dde92":"markdown","a6a70dff":"markdown","e28b41d5":"markdown","4ffcb0ee":"markdown","e1894bb0":"markdown","16dcd247":"markdown","d63f1fbc":"markdown","9abc51be":"markdown","33bc6a95":"markdown"},"source":{"a92708e2":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize=(16, 9))","e9b00b70":"# Model \/ data parameters\nnum_classes = 10\ninput_shape = (28, 28, 1)\n\n# the data, split between train and test sets\n(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n\n# Scale images to the [0, 1] range\nx_train = x_train.astype(\"float32\") \/ 255\nx_test = x_test.astype(\"float32\") \/ 255\n# Make sure images have shape (28, 28, 1)\nx_train = np.expand_dims(x_train, -1)\nx_test = np.expand_dims(x_test, -1)\nprint(\"x_train shape:\", x_train.shape)\nprint(x_train.shape[0], \"train samples\")\nprint(x_test.shape[0], \"test samples\")\n\n\n# convert class vectors to binary class matrices\ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)","d132460f":"def create_mnist_model(act_fn=tf.nn.relu, input_shape=(28, 28, 1), num_classes=10):\n    model = keras.Sequential(\n    [\n        keras.Input(shape=input_shape),\n        layers.Conv2D(32, kernel_size=(3, 3), activation=act_fn, kernel_initializer=\"he_uniform\"),\n        layers.MaxPooling2D(pool_size=(2, 2)),\n        layers.Conv2D(64, kernel_size=(3, 3), activation=act_fn, kernel_initializer=\"he_uniform\"),\n        layers.MaxPooling2D(pool_size=(2, 2)),\n        layers.Flatten(),\n        layers.Dropout(0.5),\n        layers.Dense(num_classes, activation=\"softmax\"),\n    ]\n)\n    return model","a1556c75":"sample_model = create_mnist_model()\nsample_model.summary()","1fd3fd2b":"batch_size = 128\nepochs = 15\nlosses, acc = [], []\nfor i in range(20):\n    model = create_mnist_model()\n    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n    model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1, verbose=0)\n    score = model.evaluate(x_test, y_test, verbose=0)\n    print(f\"{i+1}th Test loss:\", score[0])\n    print(f\"{i+1}th Test accuracy:\", score[1])\n    losses.append(score[0])\n    acc.append(score[1])","fd6e13ab":"print(f'Average acc: {np.mean(acc)}')\nprint(f'Acc Standard Deviation: {np.std(acc)}')","8183738c":"batch_size = 128\nepochs = 15\nsin_losses, sin_acc = [], []\nfor i in range(20):\n    model = create_mnist_model(tf.math.sin)\n    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n    model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1, verbose=0)\n    score = model.evaluate(x_test, y_test, verbose=0)\n    print(f\"{i+1}th Test loss:\", score[0])\n    print(f\"{i+1}th Test accuracy:\", score[1])\n    sin_losses.append(score[0])\n    sin_acc.append(score[1])","eea7b578":"print(f'Average acc: {np.mean(sin_acc)}')\nprint(f'Acc Standard Deviation: {np.std(sin_acc)}')","cfbf43a8":"sns.distplot(sin_acc, color='b')\nsns.distplot(acc, color='r')\nplt.show()","45e84e00":"sns.distplot(sin_losses, color='b')\nsns.distplot(losses, color='r')\nplt.show()","5cd4e804":"# Model \/ data parameters\nnum_classes = 10\ninput_shape = (32, 32, 3)\n\n\n# The data, split between train and test sets:\n(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n\n# Scale images to the [0, 1] range\nx_train = x_train.astype(\"float32\") \/ 255\nx_test = x_test.astype(\"float32\") \/ 255\n\nprint(\"x_train shape:\", x_train.shape)\nprint(x_train.shape[0], \"train samples\")\nprint(x_test.shape[0], \"test samples\")\n\n# convert class vectors to binary class matrices\ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)","893aa700":"def create_cifar_model(act_fn=tf.nn.relu, input_shape=(32, 32, 3), num_classes=10):\n    model = keras.models.Sequential()\n    model.add(layers.Conv2D(32,\n                 (3, 3),\n                 padding='same',\n                 kernel_initializer=\"he_uniform\",\n                 activation=act_fn,\n                 input_shape=x_train.shape[1:]))\n    model.add(layers.Conv2D(32,\n                 (3, 3),\n                 kernel_initializer=\"he_uniform\",\n                 activation=act_fn))\n    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n    model.add(layers.Dropout(0.25))\n\n    model.add(layers.Conv2D(64,\n                 (3, 3),\n                 padding='same',\n                 kernel_initializer=\"he_uniform\",\n                 activation=act_fn))\n    model.add(layers.Conv2D(64,\n                 (3, 3),\n                 kernel_initializer=\"he_uniform\",\n                 activation=act_fn))\n    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n    model.add(layers.Dropout(0.25))\n    \n    model.add(layers.Flatten())\n    model.add(layers.Dense(512, kernel_initializer=\"he_uniform\", activation=act_fn))\n    model.add(layers.Dropout(0.5))\n    model.add(layers.Dense(num_classes, activation=\"softmax\"))\n    \n    return model","e3967f28":"sample_model = create_cifar_model()\nsample_model.summary()","f7bc777f":"epochs=15\nlosses, acc = [], []\n\nfor i in range(20):\n    model = create_cifar_model()\n    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n    model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1, verbose=0)\n    score = model.evaluate(x_test, y_test, verbose=0)\n    print(f\"{i+1}th Test loss:\", score[0])\n    print(f\"{i+1}th Test accuracy:\", score[1])\n    losses.append(score[0])\n    acc.append(score[1])","53b3488e":"print(f'Average acc: {np.mean(acc)}')\nprint(f'Acc Standard Deviation: {np.std(acc)}')","45b4d190":"epochs=15\nsin_losses, sin_acc = [], []\n\nfor i in range(20):\n    model = create_cifar_model(tf.math.sin)\n    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n    model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1, verbose=0)\n    score = model.evaluate(x_test, y_test, verbose=0)\n    print(f\"{i+1}th Test loss:\", score[0])\n    print(f\"{i+1}th Test accuracy:\", score[1])\n    sin_losses.append(score[0])\n    sin_acc.append(score[1])","5d9f2516":"print(f'Average acc: {np.mean(sin_acc)}')\nprint(f'Acc Standard Deviation: {np.std(sin_acc)}')","c023437b":"sns.distplot(sin_losses, color='b')\nsns.distplot(losses, color='r')\nplt.show()","506dc50b":"sns.distplot(sin_acc, color='b')\nsns.distplot(acc, color='r')\nplt.show()","88b5ffbf":"## Comparison","42cbe5b1":"**Conclusion:** \n\nReLU did better","270965be":"## ReLU activation","7a7e65d3":"## ReLU activation","ca975ccb":"## Sine activation","d46dde92":"This kernel is a longer, statisticaly motivated, comparison between the use of sine and ReLU as activation functions in Neural Networks, based on this [notebook](https:\/\/www.kaggle.com\/aakashnain\/siren) by [Nain](https:\/\/www.kaggle.com\/aakashnain)\n\nThe following text is a direct quote from his kernel:\n\"A new [paper](https:\/\/arxiv.org\/pdf\/2006.09661.pdf) that was recently preseneted in CVPR 2020 proposed to use `sine` activation function as compared to `relu`. As shown here in the [video](https:\/\/www.youtube.com\/watch?time_continue=3&v=Q2fLWGBeaiI&feature=emb_logo), the representations learned by the `sine` activations are remarkable as compared to any other activation. The paper also suggests that using `sine` activation, we get better convergence.\n\nTo this end, I thought to put up a few small scale experiments to check the validity of the claims. Here I am going to demonstrate the usage on `MNSIT` and `CIFAR` dataset.\"","a6a70dff":"This example has been taken from the Keras [website](https:\/\/keras.io\/examples\/vision\/mnist_convnet\/) and the following modifications were done:\n\n1. Allow the use of different activation functions, in this case we will compare `relu` (as in the original code) to `sin` (as in the siren paper)\n2. Change initializer from `glorot_uniform` to `he_uniform`, as to match both the initializer in the Siren paper ($U \\big[-\\sqrt{\\frac{6}{n}}, \\sqrt{\\frac{6}{n}}\\big]$) and the best practice of using He init for networks with ReLU activation","e28b41d5":"## Sine Activation","4ffcb0ee":"**Conclusion:** \n\nThere is no statisticaly relevant difference between the performance of the two activations ","e1894bb0":"# MNIST","16dcd247":"## Comparison","d63f1fbc":"# CIFAR-10","9abc51be":"This example has been taken from the old keras [website](https:\/\/keras.io\/examples\/cifar10_cnn\/) and the following modifications were done:\n\n1. Change activation function from `relu` to `sin`\n2. Change initializer from `glorot_uniform` to `he_uniform`","33bc6a95":"# Final Thoughts\n\n1.  No statisticaly relevant difference was found between the performance of the two functions in MNIST, and ReLU did better in CIFAR\n2.  This does not disagree with the paper's conclusion, as the application is in a more specific problem, and they do not claim that using sine as an activation will improve performance of all Neural Networks.\n\n\n**Please upvote if you liked the kernel.**"}}