{"cell_type":{"03196d8f":"code","1dc8a72f":"code","be1a79d4":"code","b7a83341":"code","16b3c9b9":"code","cfc69226":"code","0ffd1255":"code","d1ac5aa8":"code","f71292c7":"code","9ca28104":"code","e5e9c17e":"code","1f50b556":"code","b5d7b10c":"code","f3c063f3":"code","fb86a286":"code","18877c5d":"code","95c8e7e4":"code","669df0d8":"code","d1315bad":"code","1c4e4dca":"code","c50297cb":"code","c630287e":"code","1fadaf84":"code","bdb59b71":"code","4ab34abd":"code","abeda977":"code","96af9313":"code","3b4ad291":"code","e4f2afa0":"code","13f6fe7c":"code","4122e4f9":"code","29de8218":"code","b2deaf67":"code","3807d4cc":"code","bc81d163":"code","167ce78a":"code","4d378096":"code","4efb0493":"markdown","80063feb":"markdown","51c8415d":"markdown","11a53a66":"markdown","14d7bc3a":"markdown","b3b2d334":"markdown","2d33e29e":"markdown","62906b49":"markdown","dc17d289":"markdown","8befafd5":"markdown","762dd016":"markdown","b3d33b5c":"markdown","51f002b7":"markdown","bf8e5e47":"markdown","698aa101":"markdown","dfeeec9d":"markdown","ff1ac166":"markdown","67a1ddfd":"markdown","c0779fe3":"markdown","961186bb":"markdown","71fa4ae7":"markdown","900cdd43":"markdown","8064fff5":"markdown","8a2a6a70":"markdown","366888e8":"markdown","ea346de3":"markdown","f9e1ae00":"markdown","127f8ba2":"markdown","f4f6f5f6":"markdown","0be5a850":"markdown","2fb4e8cb":"markdown","aab7bf75":"markdown","b920bff0":"markdown","220721d6":"markdown","dfea0512":"markdown","b516b562":"markdown"},"source":{"03196d8f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1dc8a72f":"import matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('darkgrid')\n%matplotlib inline\n\nimport plotly.graph_objs as go\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\n\nimport warnings\nwarnings.filterwarnings('ignore')","be1a79d4":"from sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom xgboost import XGBClassifier","b7a83341":"df = pd.read_csv('..\/input\/will-customer-leave-bank\/ML_TASK_CSV.csv')","16b3c9b9":"df.head()","cfc69226":"df.shape","0ffd1255":"df.describe()","d1ac5aa8":"df.info()","f71292c7":"#Correlation Graph\nplt.figure(figsize = (10,8))\nsns.heatmap(df.corr(), annot = True)","9ca28104":"fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(14, 6))\nfemales = df[df['Gender'] == 'Female']\nmales = df[df['Gender'] == 'Male']\n\nax = sns.distplot(females[females['Exited'] == 1].Age, bins=30, label='Exited', ax=axes[0], hist_kws = {'edgecolor':'white'})\nax = sns.distplot(females[females['Exited'] == 0].Age, bins=30, label='Not Exited', ax=axes[0], hist_kws = {'edgecolor':'white'})\nax.legend()\nax.set_title('Female')\nax = sns.distplot(males[males['Exited'] == 1].Age, bins=30, label='Exited', ax=axes[1], hist_kws = {'edgecolor':'white'})\nax = sns.distplot(males[males['Exited'] == 0].Age, bins=30, label='Not Exited', ax=axes[1], hist_kws = {'edgecolor':'white'})\nax.legend()\nax.set_title('Male')","e5e9c17e":"sns.jointplot(x = 'Age', y = 'EstimatedSalary', data = df, kind = 'hex', color = 'green')","1f50b556":"sns.jointplot(x = 'Age', y = 'CreditScore', data = df, kind = 'hex', color = 'cyan')","b5d7b10c":"age = []\nfor i in df['Age']:\n    if i <= 33:\n        age.append(1)\n    elif i >33 and i <= 40:\n        age.append(2)\n    elif i > 40:\n        age.append(3)\n        \ndf['Age'] = age","f3c063f3":"df_1 = df[df['Age'] == 1]\ndf_2 = df[df['Age'] == 2]\ndf_3 = df[df['Age'] == 3]\ndf_1 = df_1['Exited'].value_counts()\ndf_2 = df_2['Exited'].value_counts()\ndf_3 = df_3['Exited'].value_counts()\n\ntrace1 = go.Bar(x = df_1.index[::-1], y = df_1.values[::-1], name = 'Age(18-33)', marker = dict(color = 'cadetblue'))\ntrace2 = go.Bar(x = df_2.index[::-1], y = df_2.values[::-1], name = 'Age(34-40)', marker = dict(color = 'teal'))\ntrace3 = go.Bar(x = df_3.index[::-1], y = df_3.values[::-1], name = 'Age(40-92)', marker = dict(color = 'seagreen'))\ndata = [trace1, trace2, trace3]\nlayout = go.Layout(height = 400, width = 700, title = 'Age Distribution')\nfig = go.Figure(data = data, layout = layout)\npy.iplot(fig)","fb86a286":"df_1 = df[df['Exited'] == 1]\ndf_2 = df[df['Exited'] == 0]\ndf_1 = df_1['Tenure'].value_counts()\ndf_2 = df_2['Tenure'].value_counts()\n\ntrace1 = go.Bar(x = df_1.index[::-1], y = df_1.values[::-1], name = 'Exited', marker = dict(color = 'cadetblue'))\ntrace2 = go.Bar(x = df_2.index[::-1], y = df_2.values[::-1], name = 'Not Exited', marker = dict(color = 'teal'))\ndata = [trace1, trace2]\nlayout = go.Layout(height = 400, width = 700, title = 'Tenure Distribution')\nfig = go.Figure(data = data, layout = layout)\npy.iplot(fig)","18877c5d":"df_1 = df[df['Gender'] == 'Male']\ndf_2 = df[df['Gender'] == 'Female']\ndf_1 = df_1['Exited'].value_counts()\ndf_2 = df_2['Exited'].value_counts()\n\ntrace1 = go.Bar(x = df_1.index[::-1], y = df_1.values[::-1], name = 'Male', marker = dict(color = 'lightseagreen'))\ntrace2 = go.Bar(x = df_2.index[::-1], y = df_2.values[::-1], name = 'Female', marker = dict(color = 'crimson'))\ndata = [trace1, trace2]\nlayout = go.Layout(height = 400, width = 700, title = 'Gender Distribution')\nfig = go.Figure(data = data, layout = layout)\npy.iplot(fig)","95c8e7e4":"labels = {'Female':0, 'Male':1}\ndf.replace({'Gender':labels}, inplace = True)","669df0d8":"df_1 = df[df['Exited'] == 1]\ndf_2 = df[df['Exited'] == 0]\ndf_1 = df_1['Geography'].value_counts()\ndf_2 = df_2['Geography'].value_counts()\n\ntrace1 = go.Bar(x = df_1.index[::-1], y = df_1.values[::-1], name = 'Exited', marker = dict(color = 'indigo'))\ntrace2 = go.Bar(x = df_2.index[::-1], y = df_2.values[::-1], name = 'Not Exited', marker = dict(color = 'green'))\ndata = [trace1, trace2]\nlayout = go.Layout(height = 400, width = 700, title = 'Geography Distribution')\nfig = go.Figure(data = data, layout = layout)\npy.iplot(fig)","d1315bad":"labels = {'Spain':1, 'France':2, 'Germany':3}\ndf.replace({'Geography':labels}, inplace = True)","1c4e4dca":"df_1 = df[df['Balance'] == 0.00]\ndf_2 = df[df['Balance'] != 0.00]\ndf_1 = df_1['Exited'].value_counts()\ndf_2 = df_2['Exited'].value_counts()\n\ntrace1 = go.Bar(x = df_1.index[::-1], y = df_1.values[::-1], name = 'Exited', marker = dict(color = 'peru'))\ntrace2 = go.Bar(x = df_2.index[::-1], y = df_2.values[::-1], name = 'Not Exited', marker = dict(color = 'darkred'))\ndata = [trace1, trace2]\nlayout = go.Layout(height = 400, width = 700, title = 'Balance Distribution')\nfig = go.Figure(data = data, layout = layout)\npy.iplot(fig)","c50297cb":"balance = []\nfor i in df['Balance']:\n    if i == 0.0:\n        balance.append(0)\n    else:\n        balance.append(1)\n        \ndf['Balance'] = balance","c630287e":"credit = []\nfor i in df['CreditScore']:\n    if i < 600:\n        credit.append(1)\n    elif i >= 600 and i < 700:\n        credit.append(0)\n    elif i >= 700:\n        credit.append(1)\n        \ndf['CreditScore'] = credit","1fadaf84":"df_1 = df[df['CreditScore'] == 1]\ndf_2 = df[df['CreditScore'] == 0]\ndf_1 = df_1['Exited'].value_counts()\ndf_2 = df_2['Exited'].value_counts()\n\ntrace1 = go.Bar(x = df_1.index[::-1], y = df_1.values[::-1], name = 'CScore(<=600 & >700)', marker = dict(color = 'chartreuse'))\ntrace2 = go.Bar(x = df_2.index[::-1], y = df_2.values[::-1], name = 'CScore(601-700)', marker = dict(color = 'coral'))\ndata = [trace1, trace2]\nlayout = go.Layout(height = 400, width = 700, title = 'CreditScore Distribution')\nfig = go.Figure(data = data, layout = layout)\npy.iplot(fig)","bdb59b71":"df_1 = df[df['Exited'] == 1]\ndf_2 = df[df['Exited'] == 0]\ndf_1 = df_1['NumOfProducts'].value_counts()\ndf_2 = df_2['NumOfProducts'].value_counts()\n\ntrace1 = go.Bar(x = df_1.index[::-1], y = df_1.values[::-1], name = 'Exited', marker = dict(color = 'deeppink'))\ntrace2 = go.Bar(x = df_2.index[::-1], y = df_2.values[::-1], name = 'Not Exited', marker = dict(color = 'yellow'))\ndata = [trace1, trace2]\nlayout = go.Layout(height = 400, width = 700, title = 'NumOfProducts Distribution')\nfig = go.Figure(data = data, layout = layout)\npy.iplot(fig)","4ab34abd":"df.drop(['RowNumber', 'CustomerId', 'Surname'], axis = 1, inplace = True)","abeda977":"#RobustScaler works best and effectively on continuous Estimated Salary data\nscaler = RobustScaler()\ndf[['EstimatedSalary']] = scaler.fit_transform(df[['EstimatedSalary']])\ndf.head()","96af9313":"X = df.drop('Exited', axis = 1)\ny = df['Exited']","3b4ad291":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 123)","e4f2afa0":"lr = LogisticRegression()\nlr.fit(X_train, y_train)\ny_pred = lr.predict(X_test)\n\nprint('Classification Report: \\n', classification_report(y_pred, y_test))\nlr_train_acc = round(lr.score(X_train, y_train) * 100, 2)\nprint('Training Accuracy: ', lr_train_acc)\nlr_test_acc = round(lr.score(X_test, y_test) * 100, 2)\nprint('Testing Accuracy: ', lr_test_acc)\nprint('Confusion Matrix: \\n', confusion_matrix(y_test, y_pred))","13f6fe7c":"knn = KNeighborsClassifier()\nknn.fit(X_train, y_train)\ny_pred = knn.predict(X_test)\n\nprint('Classification Report: \\n', classification_report(y_pred, y_test))\nknn_train_acc = round(knn.score(X_train, y_train) * 100, 2)\nprint('Training Accuracy: ', knn_train_acc)\nknn_test_acc = round(knn.score(X_test, y_test) * 100, 2)\nprint('Testing Accuracy: ', knn_test_acc)\nprint('Confusion Matrix: \\n', confusion_matrix(y_test, y_pred))","4122e4f9":"svc = SVC(C = 1, gamma = 1)\nsvc.fit(X_train, y_train)\ny_pred = svc.predict(X_test)\n\nprint('Classification Report: \\n', classification_report(y_pred, y_test))\nsvc_train_acc = round(svc.score(X_train, y_train) * 100, 2)\nprint('Training Accuracy: ', svc_train_acc)\nsvc_test_acc = round(svc.score(X_test, y_test) * 100, 2)\nprint('Testing Accuracy: ', svc_test_acc)\nprint('Confusion Matrix: \\n', confusion_matrix(y_test, y_pred))","29de8218":"dt = DecisionTreeClassifier(max_depth = 4)\ndt.fit(X_train, y_train)\ny_pred = dt.predict(X_test)\n\nprint('Classification Report: \\n', classification_report(y_pred, y_test))\ndt_train_acc = round(dt.score(X_train, y_train) * 100, 2)\nprint('Training Accuracy: ', dt_train_acc)\ndt_test_acc = round(dt.score(X_test, y_test) * 100, 2)\nprint('Testing Accuracy: ', dt_test_acc)\nprint('Confusion Matrix: \\n', confusion_matrix(y_test, y_pred))","b2deaf67":"rf = RandomForestClassifier(n_estimators = 100, max_depth = 10, n_jobs = -1)\nrf.fit(X_train, y_train)\ny_pred = rf.predict(X_test)\n\nprint('Classification Report: \\n', classification_report(y_pred, y_test))\nrf_train_acc = round(rf.score(X_train, y_train) * 100, 2)\nprint('Training Accuracy: ', rf_train_acc)\nrf_test_acc = round(rf.score(X_test, y_test) * 100, 2)\nprint('Testing Accuracy: ', rf_test_acc)\nprint('Confusion Matrix: \\n', confusion_matrix(y_test, y_pred))","3807d4cc":"adb = AdaBoostClassifier(n_estimators = 300)\nadb.fit(X_train, y_train)\ny_pred = adb.predict(X_test)\n\nprint('Classification Report: \\n', classification_report(y_pred, y_test))\nadb_train_acc = round(adb.score(X_train, y_train) * 100, 2)\nprint('Training Accuracy: ', adb_train_acc)\nadb_test_acc = round(adb.score(X_test, y_test) * 100, 2)\nprint('Testing Accuracy: ', adb_test_acc)\nprint('Confusion Matrix: \\n', confusion_matrix(y_test, y_pred))","bc81d163":"gdb = GradientBoostingClassifier(n_estimators = 200, subsample = 0.8)\ngdb.fit(X_train, y_train)\ny_pred = gdb.predict(X_test)\n\nprint('Classification Report: \\n', classification_report(y_pred, y_test))\ngdb_train_acc = round(gdb.score(X_train, y_train) * 100, 2)\nprint('Training Accuracy: ', gdb_train_acc)\ngdb_test_acc = round(gdb.score(X_test, y_test) * 100, 2)\nprint('Testing Accuracy: ', gdb_test_acc)\nprint('Confusion Matrix: \\n', confusion_matrix(y_test, y_pred))","167ce78a":"xgbc = XGBClassifier(max_depth = 3)\nxgbc.fit(X_train, y_train)\ny_pred = xgbc.predict(X_test)\n\nprint('Classification Report: \\n', classification_report(y_pred, y_test))\nxgbc_train_acc = round(xgbc.score(X_train, y_train) * 100, 2)\nprint('Training Accuracy: ', xgbc_train_acc)\nxgbc_test_acc = round(xgbc.score(X_test, y_test) * 100, 2)\nprint('Testing Accuracy: ', xgbc_test_acc)\nprint('Confusion Matrix: \\n', confusion_matrix(y_test, y_pred))","4d378096":"x = ['Logistic Regression', 'KNN', 'SVC', 'Decision Tree','Random Forest','AdaBoost','Gradient Boosting','XGBoost']\ny1 = [lr_train_acc, knn_train_acc, svc_train_acc, dt_train_acc, rf_train_acc, adb_train_acc, gdb_train_acc, xgbc_train_acc]\ny2 = [lr_test_acc, knn_test_acc, svc_test_acc, dt_test_acc, rf_test_acc, adb_test_acc, gdb_test_acc, xgbc_test_acc]\n\ntrace1 = go.Bar(x = x, y = y1, name = 'Training Accuracy', marker = dict(color = 'forestgreen'))\ntrace2 = go.Bar(x = x, y = y2, name = 'Testing Accuracy', marker = dict(color = 'lawngreen'))\ndata = [trace1,trace2]\nlayout = go.Layout(title = 'Accuracy Plot', width = 750)\nfig = go.Figure(data = data, layout = layout)\npy.iplot(fig)","4efb0493":"[Back To Contents(Click Here)](#cont)","80063feb":"[Back To Contents(Click Here)](#cont)","51c8415d":"People in Spain are tending to remain bank customer.","11a53a66":"[Back To Contents(Click Here)](#cont)","14d7bc3a":"<a id = 'sec3'><\/a>\n# Predictions","b3b2d334":"<a id = 'subsec16'><\/a>\n**AdaBoost Classifier**","2d33e29e":"[Back To Contents(Click Here)](#cont)","62906b49":"<a id = 'subsec3'><\/a>\n**Gender Feature**","dc17d289":"<a id = 'subsec1'><\/a>\n**Age Feature**","8befafd5":"1. [Importing Libraries and Reading Data](#sec1)\n2. [Feature Engineering and Data Visualization](#sec2)\n    - [Age Feature](#subsec1)\n    - [Tenure Feature](#subsec2)\n    - [Gender Feature](#subsec3)\n    - [Geography Feature](#subsec4)\n    - [Balance Feature](#subsec5)\n    - [CreditSCore Feature](#subsec6)\n    - [NumOfProducts Feature](#subsec7)\n3. [Getting Data Ready to Train](#sec3)\n4. [Predictions](#sec4)\n    - [Logistic Regression](#subsec11)\n    - [K Nearest Neighbors](#subsec12)\n    - [Support Vector Classification](#subsec13)\n    - [Decision Tree Classifier](#subsec14)\n    - [Random Forest CLassifier](#subsec15)\n    - [AdaBoost Classifier](#subsec16)\n    - [Gradient Boosting Classifier](#subsec17)\n    - [XGBoost Classifier](#subsec18)","762dd016":"<a id = 'sec2'><\/a>\n# Feature Engineering and Data Visualizations","b3d33b5c":"<a id = 'subsec11'><\/a>\n**Logistic Regression**","51f002b7":"From above plot we can conclude that there's a signinficant difference in age categories exiting bank.","bf8e5e47":"We'll try the same approach as we did with balance feature and verify by plotting distribution respectively.","698aa101":"<a id = 'subsec13'><\/a>\n**Support Vector Classification**","dfeeec9d":"From above distribution we verified that people with zero balance are more interested in leaving bank. So we'll convert continuous column into categorical with categories: 1 - Non-zero Balance, 0 - Zero Balance","ff1ac166":"<a id = 'subsec5'><\/a>\n**Balance Feature**","67a1ddfd":"We'll drop the useless features.","c0779fe3":"<a id = 'subsec15'><\/a>\n**Random Forest Classifier**","961186bb":"<a id = 'subsec18'><\/a>\n**XGBoost Classifier**","71fa4ae7":"We can classifiy age feature form continuous to categorical feature.","900cdd43":"[Back To Contents(Click Here)](#cont)","8064fff5":"<a id = 'subsec12'><\/a>\n**K Nearest Neighbors**","8a2a6a70":"[Back To Contents(Click Here)](#cont)","366888e8":"<a id = 'sec3'><\/a>\n# Getting Data Ready to Train","ea346de3":"<a id = 'subsec6'><\/a>\n**CreditScore Feature**","f9e1ae00":"# Please Upvote, any suggestions are welcome.","127f8ba2":"<a id = 'subsec2'><\/a>\n**Tenure Feature**","f4f6f5f6":"<a id = 'subsec7'><\/a>\n**NumOfProducts Feature**","0be5a850":"[Back To Contents(Click Here)](#cont)","2fb4e8cb":"<a id = 'sec1'><\/a>\n# Importing Libraries and Reading Data","aab7bf75":"<a id = 'subsec17'><\/a>\n**Gradient Boosting Classifier**","b920bff0":"<a id = 'cont'><\/a>\n# Content","220721d6":"<a id = 'subsec4'><\/a>\n**Geography Feature**","dfea0512":"<a id = 'subsec14'><\/a>\n**Decison Tree Classifier**","b516b562":"# Plotting Accuracy"}}