{"cell_type":{"95ace224":"code","1ca17ce7":"code","a15f94c6":"code","f66daa8a":"code","d873f4e8":"code","9289b889":"code","056ec021":"code","972efd09":"code","0774b521":"markdown","de85f7b2":"markdown"},"source":{"95ace224":"#Import libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score","1ca17ce7":"#Import data\nprint('Importing data...')\ntrain = pd.read_csv('..\/input\/application_train.csv')\ntest = pd.read_csv('..\/input\/application_test.csv')\n# Any results you write to the current directory are saved as output.\nprint('train shape:', train.shape)\nprint('test shape:',test.shape)","a15f94c6":"#Impute missing values (mean for numeric, mode for categorical)\nfor i in train.columns:\n    if train[i].dtype == 'object':\n      train[i] = train[i].fillna(train[i].mode().iloc[0])\n    elif (train[i].dtype != 'object'):\n      train[i] = train[i].fillna(np.mean(train[i]))\n\n\nfor i in test.columns:\n    if test[i].dtype == 'object':\n      test[i] = test[i].fillna(test[i].mode().iloc[0])\n    elif (test[i].dtype != 'object'):\n      test[i] = test[i].fillna(np.mean(test[i]))\n    \n\nprint('Nulls in train set:', train.isnull().sum().sum())\nprint('Nulls in test set:', test.isnull().sum().sum())","f66daa8a":"## label encode categorical variables\nfor col in train.columns:\n    if train[col].dtype == 'object':\n      train[col] = train[col].astype('category')\n      train[col] = train[col].cat.codes\n\nfor col in test.columns:\n    if test[col].dtype == 'object':\n      test[col] = test[col].astype('category')\n      test[col] = test[col].cat.codes","d873f4e8":"## Creating a dummy y label and drop the target variable\ntrain['set'] = 0\ntest['set'] = 1\ntrain = train.drop('TARGET',axis=1)","9289b889":"## Use a sample set from both train and test and concatenate into a single dataframe\ntrain_df = train.sample(5000, random_state=344)\ntest_df = test.sample(5000, random_state=433)\n\nall_data = train_df.append(test_df)\ny_label = all_data['set']\nall_data = all_data.drop('set',axis=1)\n\n#Make sure the new dataframe contains all the initial features\nprint('New dataframe shape:', all_data.shape)","056ec021":"## Find all the features with covariate shift. Print during the procedure and then save in array\nmodel = RandomForestClassifier(n_estimators = 50, max_depth = 5, min_samples_leaf = 5)\nfeat_to_drop = []\nfor col in all_data.columns:\n    score = cross_val_score(model,pd.DataFrame(all_data[col]),y_label,cv=2,scoring='roc_auc')\n    if np.mean(score) > 0.8:\n        feat_to_drop.append(col)\n    print(col,np.mean(score))","972efd09":"#Print number of features with covariate shift\nprint('Number of features that display a covariate shift:', len(feat_to_drop))","0774b521":"So, from the 121 initial variables we have detected 46 that probably should be excluded from our models since they display different distribution between train and test set. This may change if you change your 'roc_auc' threshold. Use this to improve your model at your own will for the time being. Very soon I will also add my model's change in performance after including it in my analysis. ","de85f7b2":"Covariate shift is the phenomenon when the distribution of train input features is different than the one in the test set features. It is mostly present in real-time production models when e.g. consumption habits or fashion affects the inputs of your models and it can have disastrous impact on the model's performance. I though of giving it a try on this competition to see if there exist features that should be excluded from our models. It seems that there are indeed many variables with a covariate shift that must be handle with care. Follow me in the process of identifying them in this kernel and then decide how to use them in your models. This kernel is work in progress and I will include the comparison between my initial model and the reduced one in a later stage.\n"}}