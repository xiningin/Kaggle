{"cell_type":{"8b865080":"code","a15ff493":"code","c740b246":"code","77d2c802":"code","c15dd17e":"code","ec97dfa6":"code","d0f3d984":"code","6a3a9dc0":"code","5f1b358d":"code","8c1ce338":"code","93c17f89":"code","305a5150":"markdown","77daaf04":"markdown","5d2c1364":"markdown","8f4cea3c":"markdown","ff4efdda":"markdown","469e7c2d":"markdown","2bf08677":"markdown","6e28ce29":"markdown","10b63209":"markdown"},"source":{"8b865080":"import numpy as np\nimport pandas as pd\nfrom pathlib import Path\nimport os.path\nimport matplotlib.pyplot as plt\nimport tensorflow as tf","a15ff493":"# Create a list with the filepaths for training and testing\ntrain_dir = Path('..\/input\/fruits\/fruits-360\/Training')\ntrain_filepaths = list(train_dir.glob(r'**\/*.jpg'))\n\ntest_dir = Path('..\/input\/fruits\/fruits-360\/Test')\ntest_filepaths = list(test_dir.glob(r'**\/*.jpg'))\n","c740b246":"def proc_img(filepath):\n    \"\"\" Create a DataFrame with the filepath and the labels of the pictures\n    \"\"\"\n\n    labels = [str(filepath[i]).split(\"\/\")[-2] \\\n              for i in range(len(filepath))]\n\n    filepath = pd.Series(filepath, name='Filepath').astype(str)\n    labels = pd.Series(labels, name='Label')\n\n    # Concatenate filepaths and labels\n    df = pd.concat([filepath, labels], axis=1)\n\n    # Shuffle the DataFrame and reset index\n    df = df.sample(frac=1).reset_index(drop = True)\n    \n    return df\n\ntrain_df = proc_img(train_filepaths)\ntest_df = proc_img(test_filepaths)\n\nprint(f'Number of pictures: {train_df.shape[0]}\\n')\nprint(f'Number of different labels: {len(train_df.Label.unique())}\\n')\nprint(f'Labels: {train_df.Label.unique()}')\n\n# The DataFrame with the filepaths in one column and the labels in the other one\ntrain_df.head(5)","77d2c802":"# Create a DataFrame with one Label of each category\ndf_unique = train_df.copy().drop_duplicates(subset=[\"Label\"]).reset_index()\n\n# Display some pictures of the dataset\nfig, axes = plt.subplots(nrows=4, ncols=10, figsize=(15, 7),\n                        subplot_kw={'xticks': [], 'yticks': []})\n\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(plt.imread(df_unique.Filepath[i]))\n    ax.set_title(df_unique.Label[i], fontsize = 12)\nplt.tight_layout(pad=0.5)\nplt.show()","c15dd17e":"train_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n    preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input,\n    validation_split=0.2\n)\n\ntest_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n    preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input\n)\n\ntrain_images = train_generator.flow_from_dataframe(\n    dataframe=train_df,\n    x_col='Filepath',\n    y_col='Label',\n    target_size=(224, 224),\n    color_mode='rgb',\n    class_mode='categorical',\n    batch_size=32,\n    shuffle=True,\n    seed=0,\n    subset='training',\n    rotation_range=30,\n    zoom_range=0.15,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.15,\n    horizontal_flip=True,\n    fill_mode=\"nearest\"\n)\n\nval_images = train_generator.flow_from_dataframe(\n    dataframe=train_df,\n    x_col='Filepath',\n    y_col='Label',\n    target_size=(224, 224),\n    color_mode='rgb',\n    class_mode='categorical',\n    batch_size=32,\n    shuffle=True,\n    seed=0,\n    subset='validation',\n    rotation_range=30,\n    zoom_range=0.15,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.15,\n    horizontal_flip=True,\n    fill_mode=\"nearest\"\n)\n\ntest_images = test_generator.flow_from_dataframe(\n    dataframe=test_df,\n    x_col='Filepath',\n    y_col='Label',\n    target_size=(224, 224),\n    color_mode='rgb',\n    class_mode='categorical',\n    batch_size=32,\n    shuffle=False\n)","ec97dfa6":"# Load the pretained model\npretrained_model = tf.keras.applications.MobileNetV2(\n    input_shape=(224, 224, 3),\n    include_top=False,\n    weights='imagenet',\n    pooling='avg'\n)\n\npretrained_model.trainable = False","d0f3d984":"inputs = pretrained_model.input\n\nx = tf.keras.layers.Dense(128, activation='relu')(pretrained_model.output)\nx = tf.keras.layers.Dense(128, activation='relu')(x)\n\noutputs = tf.keras.layers.Dense(131, activation='softmax')(x)\n\nmodel = tf.keras.Model(inputs=inputs, outputs=outputs)\n\nmodel.compile(\n    optimizer='adam',\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n\nhistory = model.fit(\n    train_images,\n    validation_data=val_images,\n    epochs=1\n)","6a3a9dc0":"# Predict the label of the test_images\npred = model.predict(test_images)\npred = np.argmax(pred,axis=1)\n\n# Map the label\nlabels = (train_images.class_indices)\nlabels = dict((v,k) for k,v in labels.items())\npred = [labels[k] for k in pred]\n\n# Display the result\nprint(f'The first 5 predictions: {pred[:5]}')","5f1b358d":"from sklearn.metrics import accuracy_score\ny_test = list(test_df.Label)\nacc = accuracy_score(y_test,pred)\nprint(f'Accuracy on the test set: {acc * 100:.2f}%')","8c1ce338":"from sklearn.metrics import classification_report\nclass_report = classification_report(y_test, pred, zero_division=1)\nprint(class_report)","93c17f89":"# Display 50 picture of the dataset with their labels\nfig, axes = plt.subplots(nrows=5, ncols=7, figsize=(20, 12),\n                        subplot_kw={'xticks': [], 'yticks': []})\n\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(plt.imread(test_df.Filepath.iloc[i]))\n    ax.set_title(f\"True: {test_df.Label.iloc[i].split('_')[0]}\\nPredicted: {pred[i].split('_')[0]}\", fontsize = 15)\nplt.tight_layout()\nplt.show()","305a5150":"# 4. Visualize the result<a class=\"anchor\" id=\"4\"><\/a>","77daaf04":"# 2. Load the Images with a generator and Data Augmentation<a class=\"anchor\" id=\"2\"><\/a>","5d2c1364":"# Table of contents\n\n[<h3>1. Loading and preprocessing<\/h3>](#1)\n\n[<h3>2. Load the Images with a generator and Data Augmentation<\/h3>](#2)\n\n[<h3>3. Train the model<\/h3>](#3)\n\n[<h3>4. Visualize the result<\/h3>](#4)\n\n[<h3>5. Example of predictions<\/h3>](#5)","8f4cea3c":"# Classify 131 fruits with a neural network and test the result","ff4efdda":"# 5. Examples of prediction<a class=\"anchor\" id=\"5\"><\/a>\n","469e7c2d":"![fruits](https:\/\/i.imgur.com\/3PndZDA.png)","2bf08677":"# 1. Loading and preprocessing<a class=\"anchor\" id=\"1\"><\/a>","6e28ce29":"# 3. Train the model<a class=\"anchor\" id=\"3\"><\/a>","10b63209":"## Dataset properties\n- Total number of images: 90483. \n- Training set size: 67692 images (one fruit or vegetable per image).\n- Test set size: 22688 images (one fruit or vegetable per image).\n- Number of classes: 131 (fruits and vegetables).\n- Image size: 100x100 pixels.\n- Filename format: imageindex100.jpg (e.g. 32100.jpg) or rimageindex100.jpg (e.g. r32100.jpg) or r2imageindex100.jpg or r3imageindex100.jpg. \"r\" stands for rotated fruit. \"r2\" means that the fruit was rotated around the 3rd axis. \"100\" comes from image size (100x100 pixels).\n- Different varieties of the same fruit (apple for instance) are stored as belonging to different classes.\n\n## How it was made\nFruits and vegetables were planted in the shaft of a low-speed motor (3 rpm) and a short movie of 20 seconds was recorded.\n\nA Logitech C920 camera was used for filming the fruits. This is one of the best webcams available.\n\nBehind the fruits, we placed a white sheet of paper as background.\n\nHowever, due to the variations in the lighting conditions, the background was not uniform and we wrote a dedicated algorithm that extracts the fruit from the background. This algorithm is of flood fill type: we start from each edge of the image and we mark all pixels there, then we mark all pixels found in the neighborhood of the already marked pixels for which the distance between colors is less than a prescribed value. We repeat the previous step until no more pixels can be marked.\n\nAll marked pixels are considered as being background (which is then filled with white) and the rest of the pixels are considered as belonging to the object.\n\nThe maximum value for the distance between 2 neighbor pixels is a parameter of the algorithm and is set (by trial and error) for each movie.\n\nPictures from the test-multiple_fruits folder were taken with a Nexus 5X phone.\n\n## Research papers\nHorea Muresan, Mihai Oltean, Fruit recognition from images using deep learning, Acta Univ. Sapientiae, Informatica Vol. 10, Issue 1, pp. 26-42, 2018.\n\nThe paper introduces the dataset and implementation of a Neural Network trained to recognize the fruits in the dataset."}}