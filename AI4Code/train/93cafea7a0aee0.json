{"cell_type":{"86046b2b":"code","71a04aee":"code","54686a1f":"code","9e1e67fc":"code","587e8a3b":"code","c33a91fd":"code","5f8aa0ba":"code","5528add3":"code","9b0dd5d8":"code","0d527533":"code","a5dc27f5":"code","08253e05":"code","9d3399f8":"code","241c9ecf":"code","e7b56d82":"code","2005e5c2":"code","5f587c5c":"code","edc099c0":"code","df95f21a":"code","29e130cf":"code","368d37af":"code","e66c2730":"code","a28d04de":"code","7a2418ee":"markdown","ce710da6":"markdown","8adb576e":"markdown","96a7d849":"markdown","1999b6b7":"markdown","3a3a1914":"markdown","9bc2958c":"markdown","5b31a899":"markdown","9de4b2fb":"markdown","86219c94":"markdown","11028263":"markdown","0030f10c":"markdown","b8845b6f":"markdown","9454a759":"markdown","7a705ae1":"markdown","369a95ec":"markdown","976b35fe":"markdown"},"source":{"86046b2b":"from IPython.display import Image\nImage(\"..\/input\/visualizemodelembeddings\/Pooler Embedding for 5-Fold RoBERTa Model.jpg\", height=500)","71a04aee":"import gc; gc.enable()\nfrom IPython.display import clear_output\nimport pandas as pd\ntrain = pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')","54686a1f":"import torch\nimport torch.nn as nn\nimport transformers\nfrom transformers import (\n    AutoModel, AutoConfig, \n    AutoTokenizer, logging\n)\nlogging.set_verbosity_error()\nlogging.set_verbosity_warning()\n\ntrain_text = train['excerpt'][:16].tolist()\nmax_seq_length = 256\n_pretrained_model = 'roberta-base'\n\nconfig = AutoConfig.from_pretrained(_pretrained_model)\nmodel = AutoModel.from_pretrained(_pretrained_model, config=config)\ntokenizer = AutoTokenizer.from_pretrained(_pretrained_model)\n\nclear_output()\n\nfeatures = tokenizer.batch_encode_plus(\n    train_text,\n    add_special_tokens=True,\n    padding='max_length',\n    max_length=max_seq_length,\n    truncation=True,\n    return_tensors='pt',\n    return_attention_mask=True\n)\n\nwith torch.no_grad():\n    outputs = model(features['input_ids'], features['attention_mask'])\npooler_output = outputs[1]\nlogits = nn.Linear(config.hidden_size, 1)(pooler_output) # regression head\n\nprint(f'Pooler Output Shape: {pooler_output.detach().numpy().shape}')\nprint(f'Logits Shape: {logits.detach().numpy().shape}')\n\ndel config, model, tokenizer, features, outputs\ngc.collect();","9e1e67fc":"import torch\nimport torch.nn as nn\nimport transformers\nfrom transformers import (\n    AutoModel, AutoConfig, \n    AutoTokenizer, logging\n)\nlogging.set_verbosity_error()\nlogging.set_verbosity_warning()\n\ntrain_text = train['excerpt'][:16].tolist()\nmax_seq_length = 256\n_pretrained_model = 'roberta-base'\n\nconfig = AutoConfig.from_pretrained(_pretrained_model)\nmodel = AutoModel.from_pretrained(_pretrained_model, config=config)\ntokenizer = AutoTokenizer.from_pretrained(_pretrained_model)\n\nclear_output()\n\nfeatures = tokenizer.batch_encode_plus(\n    train_text,\n    add_special_tokens=True,\n    padding='max_length',\n    max_length=max_seq_length,\n    truncation=True,\n    return_tensors='pt',\n    return_attention_mask=True\n)\n\nwith torch.no_grad():\n    outputs = model(features['input_ids'], features['attention_mask'])\nlast_hidden_state = outputs[0]\ncls_embeddings = last_hidden_state[:, 0]\nlogits = nn.Linear(config.hidden_size, 1)(cls_embeddings) # regression head\n\nprint(f'Last Hidden State Output Shape: {last_hidden_state.detach().numpy().shape}')\nprint(f'CLS Embeddings Output Shape: {cls_embeddings.detach().numpy().shape}')\nprint(f'Logits Shape: {logits.detach().numpy().shape}')\n\ndel config, model, tokenizer, features, outputs\ngc.collect();","587e8a3b":"import torch\nimport torch.nn as nn\nimport transformers\nfrom transformers import (\n    AutoModel, AutoConfig, \n    AutoTokenizer, logging\n)\nlogging.set_verbosity_error()\nlogging.set_verbosity_warning()\n\ntrain_text = train['excerpt'][:16].tolist()\nmax_seq_length = 256\n_pretrained_model = 'roberta-base'\n\nconfig = AutoConfig.from_pretrained(_pretrained_model)\nmodel = AutoModel.from_pretrained(_pretrained_model, config=config)\ntokenizer = AutoTokenizer.from_pretrained(_pretrained_model)\n\nclear_output()\n\nfeatures = tokenizer.batch_encode_plus(\n    train_text,\n    add_special_tokens=True,\n    padding='max_length',\n    max_length=max_seq_length,\n    truncation=True,\n    return_tensors='pt',\n    return_attention_mask=True\n)\n\nwith torch.no_grad():\n    outputs = model(features['input_ids'], features['attention_mask'])\nlast_hidden_state = outputs[0]\nattention_mask = features['attention_mask']","c33a91fd":"input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\nsum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\nsum_mask = input_mask_expanded.sum(1)\nsum_mask = torch.clamp(sum_mask, min=1e-9)\nmean_embeddings = sum_embeddings \/ sum_mask\nlogits = nn.Linear(config.hidden_size, 1)(mean_embeddings) # regression head\n\nprint(f'Last Hidden State Output Shape: {last_hidden_state.detach().numpy().shape}')\nprint(f'Mean Embeddings Output Shape: {mean_embeddings.detach().numpy().shape}')\nprint(f'Logits Shape: {logits.detach().numpy().shape}')\n\ndel config, model, tokenizer, features\ngc.collect();","5f8aa0ba":"import torch\nimport torch.nn as nn\nimport transformers\nfrom transformers import (\n    AutoModel, AutoConfig, \n    AutoTokenizer, logging\n)\nlogging.set_verbosity_error()\nlogging.set_verbosity_warning()\n\ntrain_text = train['excerpt'][:16].tolist()\nmax_seq_length = 256\n_pretrained_model = 'roberta-base'\n\nconfig = AutoConfig.from_pretrained(_pretrained_model)\nmodel = AutoModel.from_pretrained(_pretrained_model, config=config)\ntokenizer = AutoTokenizer.from_pretrained(_pretrained_model)\n\nclear_output()\n\nfeatures = tokenizer.batch_encode_plus(\n    train_text,\n    add_special_tokens=True,\n    padding='max_length',\n    max_length=max_seq_length,\n    truncation=True,\n    return_tensors='pt',\n    return_attention_mask=True\n)\n\nwith torch.no_grad():\n    outputs = model(features['input_ids'], features['attention_mask'])\nlast_hidden_state = outputs[0]\nattention_mask = features['attention_mask']","5528add3":"input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\nlast_hidden_state[input_mask_expanded == 0] = -1e9  # Set padding tokens to large negative value\nmax_embeddings = torch.max(last_hidden_state, 1)[0]\nlogits = nn.Linear(config.hidden_size, 1)(max_embeddings) # regression head\n\nprint(f'Last Hidden State Output Shape: {last_hidden_state.detach().numpy().shape}')\nprint(f'Max Embeddings Output Shape: {max_embeddings.detach().numpy().shape}')\nprint(f'Logits Shape: {logits.detach().numpy().shape}')\n\ndel config, model, tokenizer, features\ngc.collect();","9b0dd5d8":"import torch\nimport torch.nn as nn\nimport transformers\nfrom transformers import (\n    AutoModel, AutoConfig, \n    AutoTokenizer, logging\n)\nlogging.set_verbosity_error()\nlogging.set_verbosity_warning()\n\ntrain_text = train['excerpt'][:16].tolist()\nmax_seq_length = 256\n_pretrained_model = 'roberta-base'\n\nconfig = AutoConfig.from_pretrained(_pretrained_model)\nmodel = AutoModel.from_pretrained(_pretrained_model, config=config)\ntokenizer = AutoTokenizer.from_pretrained(_pretrained_model)\n\nclear_output()\n\nfeatures = tokenizer.batch_encode_plus(\n    train_text,\n    add_special_tokens=True,\n    padding='max_length',\n    max_length=max_seq_length,\n    truncation=True,\n    return_tensors='pt',\n    return_attention_mask=True\n)\n\nwith torch.no_grad():\n    outputs = model(features['input_ids'], features['attention_mask'])\nlast_hidden_state = outputs[0]","0d527533":"mean_pooling_embeddings = torch.mean(last_hidden_state, 1)\n_, max_pooling_embeddings = torch.max(last_hidden_state, 1)\nmean_max_embeddings = torch.cat((mean_pooling_embeddings, max_pooling_embeddings), 1)\nlogits = nn.Linear(config.hidden_size*2, 1)(mean_max_embeddings) # twice the hidden size\n\nprint(f'Last Hidden State Output Shape: {last_hidden_state.detach().numpy().shape}')\nprint(f'Mean-Max Embeddings Output Shape: {mean_max_embeddings.detach().numpy().shape}')\nprint(f'Logits Shape: {logits.detach().numpy().shape}')\n\ndel config, model, tokenizer, features\ngc.collect();","a5dc27f5":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport transformers\nfrom transformers import (\n    AutoModel, AutoConfig, \n    AutoTokenizer, logging\n)\nlogging.set_verbosity_error()\nlogging.set_verbosity_warning()\n\ntrain_text = train['excerpt'][:16].tolist()\nmax_seq_length = 256\n_pretrained_model = 'roberta-base'\n\nconfig = AutoConfig.from_pretrained(_pretrained_model)\nmodel = AutoModel.from_pretrained(_pretrained_model, config=config)\ntokenizer = AutoTokenizer.from_pretrained(_pretrained_model)\n\nclear_output()\n\nfeatures = tokenizer.batch_encode_plus(\n    train_text,\n    add_special_tokens=True,\n    padding='max_length',\n    max_length=max_seq_length,\n    truncation=True,\n    return_tensors='pt',\n    return_attention_mask=True\n)\n\nwith torch.no_grad():\n    outputs = model(features['input_ids'], features['attention_mask'])\nlast_hidden_state = outputs[0]","08253e05":"# first define layers\ncnn1 = nn.Conv1d(768, 256, kernel_size=2, padding=1)\ncnn2 = nn.Conv1d(256, 1, kernel_size=2, padding=1)\n\nlast_hidden_state = last_hidden_state.permute(0, 2, 1)\ncnn_embeddings = F.relu(cnn1(last_hidden_state))\ncnn_embeddings = cnn2(cnn_embeddings)\nlogits, _ = torch.max(cnn_embeddings, 2)\n\nprint(f'Last Hidden State Output Shape: {last_hidden_state.detach().numpy().shape}')\nprint(f'CNN Embeddings Output Shape: {cnn_embeddings.detach().numpy().shape}')\nprint(f'Logits Shape: {logits.detach().numpy().shape}')\n\ndel config, model, tokenizer, features\ngc.collect();","9d3399f8":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport transformers\nfrom transformers import (\n    AutoModel, AutoConfig, \n    AutoTokenizer, logging\n)\nlogging.set_verbosity_error()\nlogging.set_verbosity_warning()\n\ntrain_text = train['excerpt'][:16].tolist()\nmax_seq_length = 256\n_pretrained_model = 'roberta-base'\n\nconfig = AutoConfig.from_pretrained(_pretrained_model)\nconfig.update({'output_hidden_states':True})\nmodel = AutoModel.from_pretrained(_pretrained_model, config=config)\ntokenizer = AutoTokenizer.from_pretrained(_pretrained_model)\n\nclear_output()\n\nfeatures = tokenizer.batch_encode_plus(\n    train_text,\n    add_special_tokens=True,\n    padding='max_length',\n    max_length=max_seq_length,\n    truncation=True,\n    return_tensors='pt',\n    return_attention_mask=True\n)\n\nwith torch.no_grad():\n    outputs = model(features['input_ids'], features['attention_mask'])\nall_hidden_states = torch.stack(outputs[2])\n\nlayer_index = 11 # second to last hidden layer\ncls_embeddings = all_hidden_states[layer_index+1, :, 0] # layer_index+1 as we have 13 layers (embedding + num of blocks)\n\nlogits = nn.Linear(config.hidden_size, 1)(cls_embeddings) # regression head\n\nprint(f'Hidden States Output Shape: {all_hidden_states.detach().numpy().shape}')\nprint(f'CLS Embeddings Output Shape: {cls_embeddings.detach().numpy().shape}')\nprint(f'Logits Shape: {logits.detach().numpy().shape}')\n\ndel config, model, tokenizer, features\ngc.collect();","241c9ecf":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport transformers\nfrom transformers import (\n    AutoModel, AutoConfig, \n    AutoTokenizer, logging\n)\nlogging.set_verbosity_error()\nlogging.set_verbosity_warning()\n\ntrain_text = train['excerpt'][:16].tolist()\nmax_seq_length = 256\n_pretrained_model = 'roberta-base'\n\nconfig = AutoConfig.from_pretrained(_pretrained_model)\nconfig.update({'output_hidden_states':True})\nmodel = AutoModel.from_pretrained(_pretrained_model, config=config)\ntokenizer = AutoTokenizer.from_pretrained(_pretrained_model)\n\nclear_output()\n\nfeatures = tokenizer.batch_encode_plus(\n    train_text,\n    add_special_tokens=True,\n    padding='max_length',\n    max_length=max_seq_length,\n    truncation=True,\n    return_tensors='pt',\n    return_attention_mask=True\n)\n\nwith torch.no_grad():\n    outputs = model(features['input_ids'], features['attention_mask'])\nall_hidden_states = torch.stack(outputs[2])\n\nconcatenate_pooling = torch.cat(\n    (all_hidden_states[-1], all_hidden_states[-2], all_hidden_states[-3], all_hidden_states[-4]),-1\n)\nconcatenate_pooling = concatenate_pooling[:, 0]\n\nlogits = nn.Linear(config.hidden_size*4, 1)(concatenate_pooling) # regression head\n\nprint(f'Hidden States Output Shape: {all_hidden_states.detach().numpy().shape}')\nprint(f'Concatenate Pooling Output Shape: {concatenate_pooling.detach().numpy().shape}')\nprint(f'Logits Shape: {logits.detach().numpy().shape}')\n\ndel config, model, tokenizer, features\ngc.collect();","e7b56d82":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport transformers\nfrom transformers import (\n    AutoModel, AutoConfig, \n    AutoTokenizer, logging\n)\nlogging.set_verbosity_error()\nlogging.set_verbosity_warning()\n\ntrain_text = train['excerpt'][:16].tolist()\nmax_seq_length = 256\n_pretrained_model = 'roberta-base'\n\nconfig = AutoConfig.from_pretrained(_pretrained_model)\nconfig.update({'output_hidden_states':True})\nmodel = AutoModel.from_pretrained(_pretrained_model, config=config)\ntokenizer = AutoTokenizer.from_pretrained(_pretrained_model)\n\nclear_output()\n\nfeatures = tokenizer.batch_encode_plus(\n    train_text,\n    add_special_tokens=True,\n    padding='max_length',\n    max_length=max_seq_length,\n    truncation=True,\n    return_tensors='pt',\n    return_attention_mask=True\n)\n\nwith torch.no_grad():\n    outputs = model(features['input_ids'], features['attention_mask'])\nall_hidden_states = torch.stack(outputs[2])","2005e5c2":"class WeightedLayerPooling(nn.Module):\n    def __init__(self, num_hidden_layers, layer_start: int = 4, layer_weights = None):\n        super(WeightedLayerPooling, self).__init__()\n        self.layer_start = layer_start\n        self.num_hidden_layers = num_hidden_layers\n        self.layer_weights = layer_weights if layer_weights is not None \\\n            else nn.Parameter(\n                torch.tensor([1] * (num_hidden_layers+1 - layer_start), dtype=torch.float)\n            )\n\n    def forward(self, all_hidden_states):\n        all_layer_embedding = all_hidden_states[self.layer_start:, :, :, :]\n        weight_factor = self.layer_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(all_layer_embedding.size())\n        weighted_average = (weight_factor*all_layer_embedding).sum(dim=0) \/ self.layer_weights.sum()\n        return weighted_average\n    \nlayer_start = 9\npooler = WeightedLayerPooling(\n    config.num_hidden_layers, \n    layer_start=layer_start, layer_weights=None\n)\nweighted_pooling_embeddings = pooler(all_hidden_states)\nweighted_pooling_embeddings = weighted_pooling_embeddings[:, 0]\nlogits = nn.Linear(config.hidden_size, 1)(weighted_pooling_embeddings)\n\nprint(f'Hidden States Output Shape: {all_hidden_states.detach().numpy().shape}')\nprint(f'Weighted Pooling Output Shape: {weighted_pooling_embeddings.detach().numpy().shape}')\nprint(f'Logits Shape: {logits.detach().numpy().shape}')\n\ndel config, model, tokenizer, features\ngc.collect();","5f587c5c":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport transformers\nfrom transformers import (\n    AutoModel, AutoConfig, \n    AutoTokenizer, logging\n)\nlogging.set_verbosity_error()\nlogging.set_verbosity_warning()\n\ntrain_text = train['excerpt'][:16].tolist()\nmax_seq_length = 256\n_pretrained_model = 'roberta-base'\n\nconfig = AutoConfig.from_pretrained(_pretrained_model)\nconfig.update({'output_hidden_states':True})\nmodel = AutoModel.from_pretrained(_pretrained_model, config=config)\ntokenizer = AutoTokenizer.from_pretrained(_pretrained_model)\n\nclear_output()\n\nfeatures = tokenizer.batch_encode_plus(\n    train_text,\n    add_special_tokens=True,\n    padding='max_length',\n    max_length=max_seq_length,\n    truncation=True,\n    return_tensors='pt',\n    return_attention_mask=True\n)\n\nwith torch.no_grad():\n    outputs = model(features['input_ids'], features['attention_mask'])\nall_hidden_states = torch.stack(outputs[2])","edc099c0":"class LSTMPooling(nn.Module):\n    def __init__(self, num_layers, hidden_size, hiddendim_lstm):\n        super(LSTMPooling, self).__init__()\n        self.num_hidden_layers = num_layers\n        self.hidden_size = hidden_size\n        self.hiddendim_lstm = hiddendim_lstm\n        self.lstm = nn.LSTM(self.hidden_size, self.hiddendim_lstm, batch_first=True)\n        self.dropout = nn.Dropout(0.1)\n    \n    def forward(self, all_hidden_states):\n        ## forward\n        hidden_states = torch.stack([all_hidden_states[layer_i][:, 0].squeeze()\n                                     for layer_i in range(1, self.num_hidden_layers+1)], dim=-1)\n        hidden_states = hidden_states.view(-1, self.num_hidden_layers, self.hidden_size)\n        out, _ = self.lstm(hidden_states, None)\n        out = self.dropout(out[:, -1, :])\n        return out\n\nhiddendim_lstm = 256\npooler = LSTMPooling(config.num_hidden_layers, config.hidden_size, hiddendim_lstm)\nlstm_pooling_embeddings = pooler(all_hidden_states)\nlogits = nn.Linear(hiddendim_lstm, 1)(lstm_pooling_embeddings) # regression head\n\nprint(f'Hidden States Output Shape: {all_hidden_states.detach().numpy().shape}')\nprint(f'LSTM Pooling Output Shape: {lstm_pooling_embeddings.detach().numpy().shape}')\nprint(f'Logits Shape: {logits.detach().numpy().shape}')\n\ndel config, model, tokenizer, features\ngc.collect();","df95f21a":"import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport transformers\nfrom transformers import (\n    AutoModel, AutoConfig, \n    AutoTokenizer, logging\n)\nlogging.set_verbosity_error()\nlogging.set_verbosity_warning()\n\ntrain_text = train['excerpt'][:16].tolist()\nmax_seq_length = 256\n_pretrained_model = 'roberta-base'\n\nconfig = AutoConfig.from_pretrained(_pretrained_model)\nconfig.update({'output_hidden_states':True})\nmodel = AutoModel.from_pretrained(_pretrained_model, config=config)\ntokenizer = AutoTokenizer.from_pretrained(_pretrained_model)\n\nclear_output()\n\nfeatures = tokenizer.batch_encode_plus(\n    train_text,\n    add_special_tokens=True,\n    padding='max_length',\n    max_length=max_seq_length,\n    truncation=True,\n    return_tensors='pt',\n    return_attention_mask=True\n)\n\nwith torch.no_grad():\n    outputs = model(features['input_ids'], features['attention_mask'])\nall_hidden_states = torch.stack(outputs[2])","29e130cf":"class AttentionPooling(nn.Module):\n    def __init__(self, num_layers, hidden_size, hiddendim_fc):\n        super(AttentionPooling, self).__init__()\n        self.num_hidden_layers = num_layers\n        self.hidden_size = hidden_size\n        self.hiddendim_fc = hiddendim_fc\n        self.dropout = nn.Dropout(0.1)\n\n        q_t = np.random.normal(loc=0.0, scale=0.1, size=(1, self.hidden_size))\n        self.q = nn.Parameter(torch.from_numpy(q_t)).float()\n        w_ht = np.random.normal(loc=0.0, scale=0.1, size=(self.hidden_size, self.hiddendim_fc))\n        self.w_h = nn.Parameter(torch.from_numpy(w_ht)).float()\n\n    def forward(self, all_hidden_states):\n        hidden_states = torch.stack([all_hidden_states[layer_i][:, 0].squeeze()\n                                     for layer_i in range(1, self.num_hidden_layers+1)], dim=-1)\n        hidden_states = hidden_states.view(-1, self.num_hidden_layers, self.hidden_size)\n        out = self.attention(hidden_states)\n        out = self.dropout(out)\n        return out\n\n    def attention(self, h):\n        v = torch.matmul(self.q, h.transpose(-2, -1)).squeeze(1)\n        v = F.softmax(v, -1)\n        v_temp = torch.matmul(v.unsqueeze(1), h).transpose(-2, -1)\n        v = torch.matmul(self.w_h.transpose(1, 0), v_temp).squeeze(2)\n        return v\n\nhiddendim_fc = 128\npooler = AttentionPooling(config.num_hidden_layers, config.hidden_size, hiddendim_fc)\nattention_pooling_embeddings = pooler(all_hidden_states)\nlogits = nn.Linear(hiddendim_fc, 1)(attention_pooling_embeddings) # regression head\n\nprint(f'Hidden States Output Shape: {all_hidden_states.detach().numpy().shape}')\nprint(f'Attention Pooling Output Shape: {attention_pooling_embeddings.detach().numpy().shape}')\nprint(f'Logits Shape: {logits.detach().numpy().shape}')\n\ndel config, model, tokenizer, features\ngc.collect();","368d37af":"import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport transformers\nfrom transformers import (\n    AutoModel, AutoConfig, \n    AutoTokenizer, logging\n)\nlogging.set_verbosity_error()\nlogging.set_verbosity_warning()\n\ntrain_text = train['excerpt'][:16].tolist()\nmax_seq_length = 256\n_pretrained_model = 'roberta-base'\n\nconfig = AutoConfig.from_pretrained(_pretrained_model)\nconfig.update({'output_hidden_states':True})\nmodel = AutoModel.from_pretrained(_pretrained_model, config=config)\ntokenizer = AutoTokenizer.from_pretrained(_pretrained_model)\n\nclear_output()\n\nfeatures = tokenizer.batch_encode_plus(\n    train_text,\n    add_special_tokens=True,\n    padding='max_length',\n    max_length=max_seq_length,\n    truncation=True,\n    return_tensors='pt',\n    return_attention_mask=True\n)\n\nwith torch.no_grad():\n    outputs = model(features['input_ids'], features['attention_mask'])\nall_hidden_states = torch.stack(outputs[2])","e66c2730":"class WKPooling(nn.Module):\n    def __init__(self, layer_start: int = 4, context_window_size: int = 2):\n        super(WKPooling, self).__init__()\n        self.layer_start = layer_start\n        self.context_window_size = context_window_size\n\n    def forward(self, all_hidden_states):\n        ft_all_layers = all_hidden_states\n        org_device = ft_all_layers.device\n        all_layer_embedding = ft_all_layers.transpose(1,0)\n        all_layer_embedding = all_layer_embedding[:, self.layer_start:, :, :]  # Start from 4th layers output\n\n        # torch.qr is slow on GPU (see https:\/\/github.com\/pytorch\/pytorch\/issues\/22573). So compute it on CPU until issue is fixed\n        all_layer_embedding = all_layer_embedding.cpu()\n\n        attention_mask = features['attention_mask'].cpu().numpy()\n        unmask_num = np.array([sum(mask) for mask in attention_mask]) - 1  # Not considering the last item\n        embedding = []\n\n        # One sentence at a time\n        for sent_index in range(len(unmask_num)):\n            sentence_feature = all_layer_embedding[sent_index, :, :unmask_num[sent_index], :]\n            one_sentence_embedding = []\n            # Process each token\n            for token_index in range(sentence_feature.shape[1]):\n                token_feature = sentence_feature[:, token_index, :]\n                # 'Unified Word Representation'\n                token_embedding = self.unify_token(token_feature)\n                one_sentence_embedding.append(token_embedding)\n\n            ##features.update({'sentence_embedding': features['cls_token_embeddings']})\n\n            one_sentence_embedding = torch.stack(one_sentence_embedding)\n            sentence_embedding = self.unify_sentence(sentence_feature, one_sentence_embedding)\n            embedding.append(sentence_embedding)\n\n        output_vector = torch.stack(embedding).to(org_device)\n        return output_vector\n\n    def unify_token(self, token_feature):\n        ## Unify Token Representation\n        window_size = self.context_window_size\n\n        alpha_alignment = torch.zeros(token_feature.size()[0], device=token_feature.device)\n        alpha_novelty = torch.zeros(token_feature.size()[0], device=token_feature.device)\n\n        for k in range(token_feature.size()[0]):\n            left_window = token_feature[k - window_size:k, :]\n            right_window = token_feature[k + 1:k + window_size + 1, :]\n            window_matrix = torch.cat([left_window, right_window, token_feature[k, :][None, :]])\n            Q, R = torch.qr(window_matrix.T)\n\n            r = R[:, -1]\n            alpha_alignment[k] = torch.mean(self.norm_vector(R[:-1, :-1], dim=0), dim=1).matmul(R[:-1, -1]) \/ torch.norm(r[:-1])\n            alpha_alignment[k] = 1 \/ (alpha_alignment[k] * window_matrix.size()[0] * 2)\n            alpha_novelty[k] = torch.abs(r[-1]) \/ torch.norm(r)\n\n        # Sum Norm\n        alpha_alignment = alpha_alignment \/ torch.sum(alpha_alignment)  # Normalization Choice\n        alpha_novelty = alpha_novelty \/ torch.sum(alpha_novelty)\n\n        alpha = alpha_novelty + alpha_alignment\n        alpha = alpha \/ torch.sum(alpha)  # Normalize\n\n        out_embedding = torch.mv(token_feature.t(), alpha)\n        return out_embedding\n\n    def norm_vector(self, vec, p=2, dim=0):\n        ## Implements the normalize() function from sklearn\n        vec_norm = torch.norm(vec, p=p, dim=dim)\n        return vec.div(vec_norm.expand_as(vec))\n\n    def unify_sentence(self, sentence_feature, one_sentence_embedding):\n        ## Unify Sentence By Token Importance\n        sent_len = one_sentence_embedding.size()[0]\n\n        var_token = torch.zeros(sent_len, device=one_sentence_embedding.device)\n        for token_index in range(sent_len):\n            token_feature = sentence_feature[:, token_index, :]\n            sim_map = self.cosine_similarity_torch(token_feature)\n            var_token[token_index] = torch.var(sim_map.diagonal(-1))\n\n        var_token = var_token \/ torch.sum(var_token)\n        sentence_embedding = torch.mv(one_sentence_embedding.t(), var_token)\n\n        return sentence_embedding\n    \n    def cosine_similarity_torch(self, x1, x2=None, eps=1e-8):\n        x2 = x1 if x2 is None else x2\n        w1 = x1.norm(p=2, dim=1, keepdim=True)\n        w2 = w1 if x2 is x1 else x2.norm(p=2, dim=1, keepdim=True)\n        return torch.mm(x1, x2.t()) \/ (w1 * w2.t()).clamp(min=eps)","a28d04de":"pooler = WKPooling(layer_start=9)\nwkpooling_embeddings = pooler(all_hidden_states)\nlogits = nn.Linear(config.hidden_size, 1)(wkpooling_embeddings) # regression head\n\nprint(f'Hidden States Output Shape: {all_hidden_states.detach().numpy().shape}')\nprint(f'WKPooling Output Shape: {wkpooling_embeddings.detach().numpy().shape}')\nprint(f'Logits Shape: {logits.detach().numpy().shape}')\n\ndel config, model, tokenizer, features\ngc.collect();","7a2418ee":"<font color='#3498DB'><a id=\"section10\"><h3>Weighted Layer Pooling<\/h3><\/a><\/font>\n\n![weighted pooling](https:\/\/multithreaded.stitchfix.com\/assets\/posts\/2019-07-07-give-me-jeans\/bert.png)\n\n<font color='#3498DB'><a id=\"section1\"><h4>Introduction<\/h4><\/a><\/font>\nThe output of the last layer may not always be the best representation of the input text during the fine-tuning for downstrea tasks. \n\nFor pre-trained language models, including Transformer, the most transferable contextualized representations of input text tend to occur in the middle layers, while the top layers specialize for language modeling. Therefore, the use of the last layer\u2019s output may restrict the power of the pre-trained representation.\n\n**WeightedLayerPooling** - Token embeddings are the weighted mean of their different hidden layer representations. Weighted Layer Pooling works the best of all pooling techniques be it any given task.\n\n<font color='#3498DB'><a id=\"section1\"><h4>Implementation<\/h4><\/a><\/font>\nHere we will see how we can utilize the Weighted Layer Pooling output for downstream task. ","ce710da6":"<font color='#3498DB'><a id=\"section1\"><h2>Pooler Output<\/h2><\/a><\/font>\n\n<font color='#3498DB'><a id=\"section10\"><h3>Introduction<\/h3><\/a><\/font>\n\nThis is usually the second output from transformer and the simplest one.\n\nPooler output is the last layer hidden-state of the first token of the sequence (classification token) further processed by a Linear layer and a Tanh activation function. The Linear layer weights are trained from the next sentence prediction (classification) objective during pretraining.\n\nThe `last hidden state` is passed into the pooler and this returns the pooled output. We can deactivate pooler outputs by setting `add pooling layer to False` in model config and passing that to model.\n\n<font color='#3498DB'><a id=\"section10\"><h3>Implementation<\/h3><\/a><\/font>\n\nHere we will see how we can utilize the pooler output for downstream task. ","8adb576e":"<font color='#3498DB'><a id=\"section10\"><h3>Concatenate Pooling<\/h3><\/a><\/font>\n\n<font color='#3498DB'><a id=\"section10\"><h4>Introduction<\/h4><\/a><\/font>\n\nConcatenate Pooling is the technique where we concatenate outputs from different layers into one. In the experiments performed by BERT Authors we saw that Concatenation of Last 4 Layers gave the best results.\n\n<font color='#3498DB'><a id=\"section10\"><h4>Implementation<\/h4><\/a><\/font>\n\nHere we will see how we can utilize the Concatenate Pooling of last 4 Layers output for downstream task. ","96a7d849":" - **Step 1:** Expand Attention Mask from `[batch_size, max_len]` to `[batch_size, max_len, hidden_size]`.\n - **Step 2:** Sum Embeddings along `max_len` axis so now we have `[batch_size, hidden_size]`.\n - **Step 3:** Sum Mask along `max_len` axis. This is done so that we can ignore padding tokens.\n - **Step 4:** Take Average.","1999b6b7":"We have a combined final representation of last four layers. We can now simply take the cls token outputs, concatenate. \nThe standard pooling operation as implemented in HuggingFace Transformer for BERT, RoBERTa etc. can also be appled here. Below we simply take the `cls` token outputs and pass it from a Linear layer.","3a3a1914":"<font color='#3498DB'><center><h2>Utilizing Transformer Representations Efficiently<\/h2>- Strategies for Utilizing and Deepening Transformer Contextual Representations<\/center><\/font>\n<br>\n<center><\/center>\n\n<font color='#3498DB'><h3>Introduction<\/h3><\/font>\nWe are accustomed to the canonical way of fine-tuning: append just an additional output layer after Transformer for downstream tasks or back-end part of models which takes representations from the last layer of the pre-trained language models as the default input. \n\nHowever, due to the multi-layer structure of Transformers, different layers capture different levels of representations. They learn a rich hierarchy of linguistic information i.e. with surface features in lower layers, syntactic features in middle layers, and semantic features in higher layers.\n\n![transformer_intermediate_representation](http:\/\/jalammar.github.io\/images\/bert-feature-extraction-contextualized-embeddings.png)\n<br>\n\nThe BERT authors tested word-embedding strategies by feeding different vector combinations as input features to a BiLSTM used on a named entity recognition task and observing the resulting F1 scores. Concatenation of the last four layers produced the best results.\n\nThis is partially demonstrated by noting that the different layers of BERT encode very different kinds of information, so the appropriate pooling strategy will change depending on the application because different layers encode different kinds of information. This holds true for other variants as well.\n\n<font color='#3498DB'><h3>Overview<\/h3><\/font>\n<br>\nIn HuggingFace Transformers there are 2 main outputs and 3 if configured; that we receive after giving input_ids and attention_mask as input.\n\n - **pooler output** (batch size, hidden size) - Last layer hidden-state of the first token of the sequence\n - **last hidden state** (batch size, seq Len, hidden size) which is the sequence of hidden states at the output of the last layer.\n - **hidden states** (n layers, batch size, seq Len, hidden size) - Hidden states for all layers and for all ids.\n\nBelow, we can visualize Embeddings from 5-Fold finetuned RoBERTa [here](https:\/\/www.kaggle.com\/rhtsingh\/commonlit-readability-prize-roberta-torch-infer) and for all competition data points. The continuous targets are split into bins.\n\n*Note 1: I have included the script to generate below plot in the dataset.*  \n*Note 2: I have used `torch.no_grad` to fetch outputs from transformer in each technique since gradients gets accumulated and that results in OOM issue. Don't use this during training.*","9bc2958c":"<font color='#3498DB'><a id=\"section10\"><h3>WKPooling<\/h3><\/a><\/font>\n\n![WKPooling](https:\/\/raw.githubusercontent.com\/BinWang28\/BERT_Sentence_Embedding\/master\/figure1.png)\n\n<font color='#3498DB'><a id=\"section1\"><h4>Introduction<\/h4><\/a><\/font>\nPooling based on the paper: \"SBERT-WK: A Sentence Embedding Method ByDissecting BERT-based Word Models\".\n\nIt consists of the following\ntwo steps:\n - Determine a unified word representation for each word in a sentence by integrating its representations across layers by examining its alignment and novelty properties.  \n - Conduct a weighted average of unified word representations based on the word importance measure to yield the ultimate sentence embedding vector.\n\nYou can find more details in the paper.\n\n<font color='#3498DB'><a id=\"section1\"><h4>Implementation<\/h4><\/a><\/font>\nHere we will see how we can utilize the WK Pooling output for downstream task. \n\n*Note: SBERT-WK uses QR decomposition. torch QR decomposition is currently extremely slow when run on GPU. Hence, the tensor is first transferred to the CPU before it is applied. This makes this pooling method rather slow.*","5b31a899":"<font color='#3498DB'><a id=\"section1\"><h2>Hidden States Output<\/h2><\/a><\/font>\n\n![weightedpooling](https:\/\/miro.medium.com\/max\/2870\/1*8QtJYYJTHcAn0qQEmYsqbw.png)\n\n<font color='#3498DB'><a id=\"section10\"><h3>Introduction<\/h3><\/a><\/font>\nThis is the third and optional output from transformers. \n\nHidden States output has the output of the embeddings + one for the output of each layer of shape `(batch_size, sequence_length, hidden_size)`. Thus, combining hidden-states of the model at the output of each layer plus the initial embedding outputs will give us an output of `(n_layers, batch_size, sequence_length, hidden_size)`. \n\nThere are multiple different ways we can utilize and make use of these embeddings.\n\n<font color='#3498DB'><a id=\"section10\"><h3>CLS Layer Embeddings<\/h3><\/a><\/font>\n\n<font color='#3498DB'><a id=\"section10\"><h4>Introduction<\/h4><\/a><\/font>\n\nLike we saw in the first figure, the BERT authors tried various different layer combinations and in one of them, they use second-to-last layer outputs which performed better than last layer outputs. We have multiple application-dependent strategies for utilizing intermediate representations. \n\n*Note: To unlock Transformer for giving hidden states as output we need to pass `output_hidden_states` parameter.* \n\n<font color='#3498DB'><a id=\"section10\"><h4>Implementation<\/h4><\/a><\/font>\n\nHere we will see how we can utilize the Layer-Wise CLS embeddings output for downstream task. ","9de4b2fb":"<font color='#3498DB'><a id=\"section10\"><h3>Conv-1D Pooling<\/h3><\/a><\/font>\n\n<font color='#3498DB'><a id=\"section10\"><h4>Introduction<\/h4><\/a><\/font>\n\nConv1D pooling is a unique strategy and has been used previously in competitions like Twitter Sentiment Extraction, Jigsaw Unintended Bias in Toxicity etc. Here, we use conv1d layers to get filter unwanted features\n\nHere kernel size 2 means  it considers two tokens for convolution across features. The main idea of conv1d is that it's slides across the tokens and finds the most important tokens by exploring all the 768 features.\n\n<center><img src=\"https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F1735508%2F208ad2389a980c1d0f6dfd56407d9d23%2F1D-convolutional-example_2x.png?generation=1587806222062877&alt=media\" alt=\"conv1d\" style=\"width:400px;height:500px;\"><\/center>\n\n<font color='#3498DB'><a id=\"section10\"><h4>Implementation<\/h4><\/a><\/font>\n\nHere we will see how we can utilize the Conv1D embeddings output for downstream task. ","86219c94":"<font color='#3498DB'><a id=\"section10\"><h3>Mean Pooling<\/h3><\/a><\/font>\n\n<font color='#3498DB'><a id=\"section10\"><h4>Introduction<\/h4><\/a><\/font>\nWe can also consider the last hidden state `[batch, maxlen, hidden_state]`, the average across maxlen dimensions to get averaged\/mean embeddings.\n\nThere are multiple different ways to do this. We can simply take `torch.mean(last_hidden_state, 1)` but rather we will be implementing something different. We will make use of attention masks as well so that we can ignore padding tokens which is a better way of implementing average embeddings.\n\n<font color='#3498DB'><a id=\"section10\"><h4>Implementation<\/h4><\/a><\/font>\n\nHere we will see how we can utilize the mean pooling embeddings output for downstream task. ","11028263":"<font color='#3498DB'><a id=\"section1\"><h2>Review<\/h2><\/a><\/font>\n\nGoing back to figure 1, we can see that we have implemented all of the techniques that the BERT authors used to investigate and many more. One, can easily plug and play any of the above head \/ pooler into their model and finetune on a downstream task.\n\n![embedding_layers](http:\/\/jalammar.github.io\/images\/bert-feature-extraction-contextualized-embeddings.png)\n\n<font color='#3498DB'><a id=\"section1\"><h2>Case Study<\/h2><\/a><\/font>\nHan Xiao created an open-source project named bert-as-service on GitHub which is intended to create word embeddings for your text using BERT `bert-as-service`, by default, uses the outputs from the second-to-last layer of the model.\n\nHis observations are - \n - The embeddings start in the first layer as having no contextual information.\n - As the embeddings move deeper into the network, they pick up more and more contextual information with each layer.\n - As you approach the final layer, however, you start picking up information that is specific to BERT\u2019s pre-training tasks (the \u201cMasked Language Model\u201d (MLM) and \u201cNext Sentence Prediction\u201d (NSP)).\n    - What we want are embeddings that encode the word meaning well\u2026\n    - BERT is motivated to do this, but it is also motivated to encode anything else that would help it determine what a missing word is (MLM), or whether the second sentence came after the first (NSP).\n - The second-to-last layer is what Han settled on as a reasonable sweet spot.\n \n<font color='#3498DB'><a id=\"section112\"><h2>References and Resources<\/h2><\/a><\/font>\n \nHere I would like to acknowledge all the publications, blogs, notebooks etc. without which making this kernel would not have been possible, \n\n - **Papers:**\n   - [Deepening Hidden Representations from Pre-trained Language Models](https:\/\/arxiv.org\/pdf\/1911.01940v2.pdf)\n   - [Linguistic Knowledge and Transferability of Contextual Representations](https:\/\/www.aclweb.org\/anthology\/N19-1112.pdf)\n   - [What does BERT learn about the structure of language?](https:\/\/www.aclweb.org\/anthology\/P19-1356.pdf)\n   - [Dissecting Contextual Word Embeddings: Architecture and Representation](https:\/\/www.aclweb.org\/anthology\/D18-1179.pdf)\n   - [SDNET: CONTEXTUALIZED ATTENTION-BASED DEEP NETWORK FOR CONVERSATIONAL QUESTION ANSWERING](https:\/\/arxiv.org\/pdf\/1812.03593.pdf)\n   - [Utilizing BERT Intermediate Layers for Aspect Based Sentiment Analysis\nand Natural Language Inference](https:\/\/arxiv.org\/pdf\/2002.04815.pdf)\n   - [WHAT DO YOU LEARN FROM CONTEXT? PROBING FOR SENTENCE STRUCTURE IN CONTEXTUALIZED WORD REPRESENTATIONS](https:\/\/arxiv.org\/pdf\/1905.06316.pdf)\n   - [SBERT-WK: A Sentence Embedding Method by Dissecting BERT-based Word Models](https:\/\/arxiv.org\/pdf\/2002.06652.pdf)\n \n - **Blogs**\n   - [BERT Word Embeddings Tutorial](https:\/\/mccormickml.com\/2019\/05\/14\/BERT-word-embeddings-tutorial\/#4-appendix)\n   - [Visualize BERT sequence embeddings: An unseen way](https:\/\/towardsdatascience.com\/visualize-bert-sequence-embeddings-an-unseen-way-1d6a351e4568)\n   - [Deconstructing BERT, Part 2: Visualizing the Inner Workings of Attention](https:\/\/towardsdatascience.com\/deconstructing-bert-part-2-visualizing-the-inner-workings-of-attention-60a16d86b5c1)\n   - [Writing Math Equations in Jupyter Notebook: A Naive Introduction](https:\/\/medium.com\/analytics-vidhya\/writing-math-equations-in-jupyter-notebook-a-naive-introduction-a5ce87b9a214)\n\n - **GitHub**\n   - [Sentence Transformers: Multilingual Sentence, Paragraph, and Image Embeddings using BERT & Co.](https:\/\/github.com\/UKPLab\/sentence-transformers)\n   - [FLAIR](https:\/\/github.com\/flairNLP\/flair)\n   - [BERT Fine-tuning for Aspect Based Sentiment Analysis](https:\/\/github.com\/avinashsai\/BERT-Aspect)\n   - [Interpreting Bidirectional Encoder Representations from Transformers](https:\/\/github.com\/ganeshjawahar\/interpret_bert)\n   - [BertViz](https:\/\/github.com\/jessevig\/bertviz)\n \n - **Kaggle Kernels and Discussion**\n   - [Jigsaw Unintended Bias in Toxicity - 1st Place](https:\/\/www.kaggle.com\/c\/jigsaw-unintended-bias-in-toxicity-classification\/discussion\/103280)\n   - [Jigsaw Unintended Bias in Toxicity - 4th Place](https:\/\/www.kaggle.com\/iezepov\/wombat-inference-kernel)\n   - [Jigsaw Unintended Bias in Toxicity - 8th Place](https:\/\/www.kaggle.com\/haqishen\/jigsaw-predict)\n   - [Twitter Sentiment Extraction - 2nd Place](https:\/\/www.kaggle.com\/hiromoon166\/inference-8models-seed100101-bucketing-2-ver2)\n   - [Twitter Sentiment Extraction - 3rd Place](https:\/\/www.kaggle.com\/suicaokhoailang\/final-sub-tweet)\n   - [Twitter Sentiment Extraction - 7th Place](https:\/\/www.kaggle.com\/naivelamb\/roberta-base-ensemble)\n   - [Twitter Sentiment Extraction - TensorFlow roBERTa - [0.712]](https:\/\/www.kaggle.com\/al0kharba\/tensorflow-roberta-0-712)\n   - [Jigsaw Multilingual Toxic Comment Classification - 4th Place](https:\/\/www.kaggle.com\/c\/jigsaw-multilingual-toxic-comment-classification\/discussion\/160980)\n   - [CommonLit Readability Prize - Step 1: Create Folds](https:\/\/www.kaggle.com\/abhishek\/step-1-create-folds)\n \n<font color='#3498DB'><a id=\"section6\"><h2>Ending Notes<\/h2><\/a><\/font>\n\n- There are many other strategies which I haven't covererd and which one do further research on,\n    - Dense Pooling\n    - Word Weight (TF-IDF) Pooling\n    - Async Pooling\n    - Parallel \/ Heirarchical Aggregation\n    \n- The performance of pooling strategies will vary from task to task and there is no one technique that always performs the best. \n\n- I prefer to use WeightLayerPooling of last 4 layers.\n\n- More comprehensive repository for learning and implementing Transformers for various tasks can be found [here](https:\/\/notebooks.quantumstat.com\/), [here](https:\/\/huggingface.co\/transformers\/master\/community.html#community-notebooks) and [here](https:\/\/huggingface.co\/transformers\/notebooks.html)  \n\n<font color='#3498DB'><a id=\"section2\"><h2>Thanks & Please Do Upvote!<\/h2><\/a><\/font>","0030f10c":"<font color='#3498DB'><a id=\"section10\"><h3>Max Pooling<\/h3><\/a><\/font>\n\n![maxpooling](https:\/\/devopedia.org\/images\/article\/241\/2120.1575378105.gif)\n\n<font color='#3498DB'><a id=\"section10\"><h4>Introduction<\/h4><\/a><\/font>\nSimilar to mean pooling we will consider the last hidden state `[batch, maxlen, hidden_state]`, then take max across maxlen dimensions to get max pooling embeddings.\n\nThere are multiple different ways to do this as well. We can simply take `torch.max(last_hidden_state, 1)` but rather we will be implementing this similar to mean pooling by utilizing attention masks. \n\n<font color='#3498DB'><a id=\"section10\"><h4>Implementation<\/h4><\/a><\/font>\nHere we will see how we can utilize the max pooling embeddings output for downstream task. ","b8845b6f":"<font color='#3498DB'><a id=\"section10\"><h3>Attention Pooling<\/h3><\/a><\/font>\n\n<font color='#3498DB'><a id=\"section1\"><h4>Introduction<\/h4><\/a><\/font>\nIntuitively, attention operation can learn the contribution of each $h^i_{CLS}$. We can use a dot-product attention module to dynamically combine all intermediates:\n$$o = W^T_{h} softmax(qh^T_{CLS})h_{CLS}$$\n\nwhere $W^T_{h}$ and $q$ are learnable weights.\nFinally, we pass the pooled output o to a fullyconnected layer for label prediction:\n$$y = sof tmax(W^T_{o}o + bo)$$\n\n<font color='#3498DB'><a id=\"section1\"><h4>Implementation<\/h4><\/a><\/font>\nHere we will see how we can utilize the Attention Pooling output for downstream task. ","9454a759":"<font color='#3498DB'><a id=\"section10\"><h3>Mean-Max Pooling (Head)<\/h3><\/a><\/font>\n\n![mean_max](https:\/\/media.springernature.com\/original\/springer-static\/image\/chp%3A10.1007%2F978-3-030-58323-1_23\/MediaObjects\/498432_1_En_23_Fig1_HTML.png)\n\n<font color='#3498DB'><a id=\"section10\"><h4>Introduction<\/h4><\/a><\/font>\nMean Max Pooling or Mean Max Head is the most common technique used in many kernels and competitions. We first find mean and max-pooling embeddings, then we concatenate this to have a final representation that is twice the hidden size. The above figure b shows how this is done.\n\nWe will be implementing the Kaggle way but one can also find mean and max embeddings like above, then concatenate to have a final resultant vector which will also be the same shape but won't account for padding tokens.\n\n<font color='#3498DB'><a id=\"section10\"><h4>Implementation<\/h4><\/a><\/font>\n\nHere we will see how we can utilize the mean-max pooling embeddings output for downstream task. ","7a705ae1":"<font color='#3498DB'><a id=\"section1\"><h2>Last Hidden State Output<\/h2><\/a><\/font>\n\n![last_hidden_state](https:\/\/miro.medium.com\/max\/2120\/1*p6PgpOV74U_qLrzr-1_4Zg.png)\n\n<font color='#3498DB'><a id=\"section10\"><h3>Introduction<\/h3><\/a><\/font>\nThis is the first and default output from models. \n\nLast Hidden State output is the sequence of hidden-states at the output of the last layer of the model. The output is usually `[batch, maxlen, hidden_state]`, it can be narrowed down to `[batch, 1, hidden_state]` for `[CLS]` token, as the `[CLS]` token is 1st token in the sequence. Here , `[batch, 1, hidden_state]` can be equivalently considered as `[batch, hidden_state]`.\n\n<font color='#3498DB'><a id=\"section10\"><h3>CLS Embeddings<\/h3><\/a><\/font>\n\n![cls](http:\/\/www.mccormickml.com\/assets\/BERT\/CLS_token_500x606.png)\n\n<font color='#3498DB'><a id=\"section10\"><h4>Introduction<\/h4><\/a><\/font>\nSince Transformers are contextual model, the idea is `[CLS]` token would have captured the entire context and would be sufficient for simple downstream tasks such as classification. Hence, for tasks such as classification using sentence representations, you can use `[batch, hidden_state]`.\n\n<font color='#3498DB'><a id=\"section10\"><h4>Implementation<\/h4><\/a><\/font>\nHere we will see how we can utilize the cls embeddings output for downstream task. ","369a95ec":"We can see that the model can differentiate between the training data points pretty good just by using pooler embeddings but we still have few hard samples. For this, utilizing intermediate representations from various layers will provide better representations as it will help in incorporating more information. \n\nThe notebook will show many different ways these outputs and hidden representations can be utilized to do much more than just adding an output layer. Below are the various techniques we will be implementing.\n\n<font color='#3498DB'><h3>Contents<\/h3><\/font>\n- [**Pooler Output**](#section1)\n- [**Last Hidden State Output**](#section2)\n  - [CLS Embeddings](#None)\n    - [Introduction](#None)\n    - [Implementation](#None)\n  - [Mean Pooling](#None)\n    - [Introduction](#None)\n    - [Implementation](#None)\n  - [Max Pooling](#None)\n    - [Introduction](#None)\n    - [Implementation](#None)\n  - [Mean + Max Pooling](#None)\n    - [Introduction](#None)\n    - [Implementation](#None)\n  - [Conv1D Pooling](#None)\n    - [Introduction](#None)\n    - [Implementation](#None)\n- [**Hidden Layers Output**](#section3)\n  - [Layerwise CLS Embeddings](#None)\n    - [Introduction](#None)\n    - [Implementation](#None)\n  - [Concatenate Pooling](#None)\n    - [Introduction](#None)\n    - [Implementation](#None)\n  - [Weighted Layer Pooling](#None)\n    - [Introduction](#None)\n    - [Implementation](#None)\n  - [LSTM \/ GRU Pooling](#None)\n    - [Introduction](#None)\n    - [Implementation](#None)\n  - [Attention Pooling](#None)\n    - [Introduction](#None)\n    - [Implementation](#None)\n  - [WKPooling](#None)\n    - [Introduction](#None)\n    - [Implementation](#None)\n- [**Review**](#None)\n- [**Case Study**](#None)\n- [**References and Resources**](#None)\n- [**Ending Notes**](#None)\n\n<font color='#3498DB'><h3>What's New?<\/h3><\/font>\n1. [SWA, Apex AMP & Interpreting Transformers in Torch](https:\/\/www.kaggle.com\/rhtsingh\/swa-apex-amp-interpreting-transformers-in-torch) notebook is an implementation of the Stochastic Weight Averaging technique with NVIDIA Apex on transformers using PyTorch. The notebook also implements how to interactively interpret Transformers using LIT (Language Interpretability Tool) a platform for NLP model understanding.   \nIt has in-depth explanations and code implementations for,\n - SWA \n - Apex AMP\n - Weighted Layer Pooling\n - MADGRAD Optimizer\n - Grouped LLRD\n - Language Interpretibility Tool\n    - Attention Visualization\n    - Saliency Maps\n    - Integrated Gradients\n    - LIME \n    - Embedding Space (UMAP & PCA)\n    - Counterfactual generation\n    - And many more ...\n\n2. [On Stability of Few-Sample Transformer Fine-Tuning](https:\/\/www.kaggle.com\/rhtsingh\/on-stability-of-few-sample-transformer-fine-tuning) notebook goes over various remedies to increase few-sample fine-tuning stability and they show a significant performance improvement over simple finetuning methods. The methods explained in the notebook are - \n - Debiasing Omission In BertADAM\n - Re-Initializing Transformer Layers\n - Utilizing Intermediate Layers\n - Layer-wise Learning Rate Decay (LLRD) \n - Mixout Regularization\n - Pre-trained Weight Decay\n - Stochastic Weight Averaging. \n \n3. [Speeding up Transformer w\/ Optimization Strategies](https:\/\/www.kaggle.com\/rhtsingh\/speeding-up-transformer-w-optimization-strategies) notebook explains in-depth 5 optimization strategies with code. All these techniques are promising and can improve the model performance both in terms of speed and accuracy.\n  - Dynamic Padding and Uniform Length Batching\n  - Gradient Accumulation\n  - Freeze Embedding\n  - Numeric Precision Reduction\n  - Gradient Checkpointing  \n   \n<font color='#3498DB'><a id=\"section1\"><h2>Preliminary Setup<\/h2><\/a><\/font>\n\nImport all required libraries and import modules\/utilities.  \nWe'll be working with the  in this notebook to visualize various embedding layers.\n","976b35fe":"<font color='#3498DB'><a id=\"section10\"><h3>LSTM\/GRU Pooling<\/h3><\/a><\/font>\n\n![weighted pooling](https:\/\/media.arxiv-vanity.com\/render-output\/4765402\/x1.png)\n\n<font color='#3498DB'><a id=\"section1\"><h4>Introduction<\/h4><\/a><\/font>\nRepresentation of the hidden states $h_{CLS}$ is a special sequence: an abstract-tospecific sequence. Since LSTM network is inherently suitable for processing sequential information, we can use a LSTM network to connect all intermediate representations of the [CLS] token, and the output of the last LSTM cell is used as the final representation. Formally,\n\n$$o = h^L_{LSTM} =LSTM(h^i_{CLS}), i \u2208 [1, L]$$\n\n<font color='#3498DB'><a id=\"section1\"><h4>Implementation<\/h4><\/a><\/font>\nHere we will see how we can utilize the LSTM Pooling output for downstream task. "}}