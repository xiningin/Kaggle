{"cell_type":{"ad0e99a0":"code","b0275ef0":"code","6c51f7fe":"code","952ef47e":"code","9c29ab54":"code","f049201b":"code","05eee103":"code","863af21e":"code","4eabdbc3":"code","7f4d0ff3":"code","d4a3cd94":"code","8d71973b":"code","e19dde96":"code","6d19766f":"code","c7b09617":"code","4a695f26":"code","ae29311a":"code","410e6b5b":"markdown","cf3f16be":"markdown","1c200d75":"markdown","3a0a3601":"markdown","dbf5c601":"markdown","1f1e1a4b":"markdown","9d174f7b":"markdown","aeb86dc5":"markdown","4ea807f7":"markdown","7323cce1":"markdown","11068e58":"markdown"},"source":{"ad0e99a0":"import pandas as pd\nfrom sklearn.datasets import load_boston\n\ndata = load_boston()\ndf = pd.DataFrame(data.data, columns = data.feature_names)\ndf['target'] = data.target\ndf.head()","b0275ef0":"print(data.DESCR)","6c51f7fe":"from sklearn.model_selection import train_test_split\n\nX = data.data\ny = data.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)\n\nprint(X_train.shape, y_train.shape, X_test.shape, y_test.shape)","952ef47e":"import numpy as np\nfrom sklearn.linear_model import LinearRegression\n\nlin_reg = LinearRegression()\nlin_reg.fit(X_train, y_train)\n\nprint(\n    np.round(lin_reg.coef_,1),\n    '\\n',\n    np.round(lin_reg.intercept_, 1)\n)","9c29ab54":"from sklearn.metrics import mean_squared_error\ny_pred = lin_reg.predict(X_test)\n\nprint('R^2 Score:', lin_reg.score(X_test, y_test))\nprint('Mean squared error: ', mean_squared_error(y_test, y_pred))","f049201b":"from sklearn.ensemble import RandomForestRegressor\n\nrf_reg = RandomForestRegressor(max_depth = 4)\nrf_reg.fit(X_train, y_train)\ny_pred_rf = rf_reg.predict(X_test)\nprint(rf_reg.feature_importances_)\nprint('\\nR^2 Score:', rf_reg.score(X_test, y_test))\nprint('Mean squared error: ', mean_squared_error(y_test, y_pred_rf))","05eee103":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestRegressor\nnp.random.seed(42)\n\nrf_reg = RandomForestRegressor()\nrf_params = {'n_estimators': [10 ,15],\n             'criterion':['mse', 'mae'],\n             'max_leaf_nodes': [10, 15],\n             'max_depth': [4, 5], \n             'min_samples_split': [4, 5],\n             'max_features': [5, 6]}\n\ngrid_search = GridSearchCV(rf_reg, rf_params)\ngrid_search.fit(X_train, y_train)\nbest_rf = grid_search.best_estimator_\nprint(best_rf)\nbest_rf.score(X_test, y_test)","863af21e":"from sklearn.datasets import load_wine\n\nwine = load_wine()\nwine_df = pd.DataFrame(wine.data, columns = wine.feature_names)\nwine_df['target'] = wine.target\nwine_df.head()","4eabdbc3":"print(wine.DESCR)","7f4d0ff3":"X = wine.data\ny = wine.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, stratify = y)\nprint(X_train.shape, y_train.shape, X_test.shape, y_test.shape)","d4a3cd94":"from sklearn.linear_model import LogisticRegression\n\nlog_reg = LogisticRegression()\nlog_reg.fit(X_train, y_train)\n\nprint(\n    np.round(log_reg.coef_, 2), \n    '\\n',\n    np.round(log_reg.intercept_)\n)","8d71973b":"from sklearn.metrics import confusion_matrix, classification_report\n\nprint('Accuracy: ', log_reg.score(X_test, y_test))\ny_pred = log_reg.predict(X_test)\n\nconf_matrix = confusion_matrix(y_test, y_pred)\nprint('\\nConfusion Matrix:\\n', \n      pd.DataFrame(\n          confusion_matrix(y_test, y_pred), \n          columns = ['pred: 0', 'pred: 1', 'pred: 2'],\n          index = ['true: 0', 'true: 1', 'true: 2']), \n      '\\n\\nClassification Report:\\n', \n      classification_report(y_test, y_pred))","e19dde96":"# load data\ncity_attributes = pd.read_csv('..\/input\/city_attributes.csv')\nhumidity = pd.read_csv('..\/input\/humidity.csv')\npressure = pd.read_csv('..\/input\/pressure.csv')\ntemperature = pd.read_csv('..\/input\/temperature.csv')\nweather_description = pd.read_csv('..\/input\/weather_description.csv')\nwind_direction = pd.read_csv('..\/input\/wind_direction.csv')\nwind_speed = pd.read_csv('..\/input\/wind_speed.csv')\n\n# besides the first dataframe, the data look a lot like this:\nhumidity.head()","6d19766f":"# we can reshape these using pd.melt\nhumidity = pd.melt(humidity, id_vars = ['datetime'], value_name = 'humidity', var_name = 'City')\npressure = pd.melt(pressure, id_vars = ['datetime'], value_name = 'pressure', var_name = 'City')\ntemperature = pd.melt(temperature, id_vars = ['datetime'], value_name = 'temperature', var_name = 'City')\nweather_description = pd.melt(weather_description, id_vars = ['datetime'], value_name = 'weather_description', var_name = 'City')\nwind_direction = pd.melt(wind_direction, id_vars = ['datetime'], value_name = 'wind_direction', var_name = 'City')\nwind_speed = pd.melt(wind_speed, id_vars = ['datetime'], value_name = 'wind_speed', var_name = 'City')\n\nhumidity.head()","c7b09617":"# combine all of the dataframes created above \nweather = pd.concat([humidity, pressure, temperature, weather_description, wind_direction, wind_speed], axis = 1)\nweather = weather.loc[:,~weather.columns.duplicated()] # indexing: every row, only the columns that aren't duplicates\nweather.head()","4a695f26":"# now we can merge this with the city attributes\nweather = pd.merge(weather, city_attributes, on = 'City')\nweather.head()","ae29311a":"# create a variable for binary classification \nweather['weather_binary'] = np.where(weather['weather_description'].isin([\"sky is clear\", \"broken clouds\", \"few clouds\", \n                                                  \"scattered clouds\", \"overcast clouds\"]), 'good', 'bad')\n\n# create a variable for multi-classification\nconditions = [\n    (weather['weather_description'].isin([\"drizzle\", \"freezing_rain\", \"heavy intensity drizzle\", \n                                          \"heavy intensity rain\", \"heavy intensity shower rain\", \n                                          \"light intensity drizzle\", \"light intensity drizzle rain\", \n                                          \"light intensity shower rain\", \"light rain\", \"light shower rain\", \n                                          \"moderate rain\", \"proximity moderate rain\", \"ragged shower rain\", \n                                          \"shower drizzle\", \"very heavy rain\", \"proximity shower rain\"])),\n    (weather['weather_description'].isin([\"broken clouds\", \"overcast clouds\", \"scattered clouds\", \"few clouds\"])),\n    (weather['weather_description'].isin([\"heavy snow\", \"light rain and snow\", \"light shower sleet\", \"light snow\", \n                                          \"rain and snow\", \"shower snow\", \"sleet\", \"snow\", \"heavy shower snow\"])), \n    (weather['weather_description'].isin([\"thunderstorm with drizzle\", \"thunderstorm with heavy drizzle\", \n                                          \"thunderstorm with light drizzle\", \"thunderstorm with rain\", \n                                          \"thunderstorm with light rain\", \"heavy thunderstorm\", \n                                          \"proximity thunderstorm\", \"proximity thunderstorm with drizzle\", \n                                          \"proximity thunderstorm with rain\", \"proximity thunderstorm\", \n                                          \"thunderstorm\", \"ragged thunderstorm\"])),\n    (weather['weather_description'].isin([\"sky is clear\"]))]\n     \nchoices = ['rain', 'cloudy', 'snow', 'thunder', 'clear']\nweather['weather_broad'] = np.select(conditions, choices, default='other')\n\n# sklearn models won't work with NaN values. There are a whole suite of imputation techniques used to replace empty \n# values with the most appropriate estimate, but for the sake of these challenges, we'll just remove these cases.\n\nweather = weather.dropna()\nweather.head()","410e6b5b":"### Challenges","cf3f16be":"**Exercise 1** \n\nFit a classification model of your choice to predict `weather_binary`, and interpret its confusion matrix when applied to held-out testing data. How would you judge the model's performance? Justify your answer. ","1c200d75":"### Regression","3a0a3601":"## Classification on the Wine Dataset","dbf5c601":"**Exercise 4** \n\nTry picking a different variable than the one selected before, and use a regression method of your choice to predict it. What is the MSE on test data, and how do you interpret it? ","1f1e1a4b":"### Conclusion","9d174f7b":"**Exercise 2** \n\nFit a classification model of your choice to predict `weather_broad`. Explain the model's performance, and how your interpretation of its confusion matrix changes in a multi-class scenario.  ","aeb86dc5":"### Classification","4ea807f7":"For the following exercises, choose a continuous variable of interest in the dataset (for example, `temperature`).\n\n**Exercise 3** \n\nFor your variable of choice, fit a linear regression model using only the single variable that best explains it (how could you figure this out from the data at hand?). Perform any necessary diagnostics or analysis required to make a conclusion about its interpretation and performance. How does this change if more variables are added? Why do you think that is? ","7323cce1":"# Introduction to Modeling in Python\n\n## Regression on the Diabetes Dataset","11068e58":"In these exercises, we practiced techniques introduced in the lecture portion of the workshop on a dataset similar to what you might see during DataFest 2019. However, we were only able to cover a small portion of what modeling fully entails, and in the real world it is common for the success of a model to be driven by many factors external to which algorithm or hyperparameters were chosen. A common issue is data scarcity or quality - as is often said in regards to a machine learning model, \"garbage in, garbage out\". Oftentimes (but not always!) it is much more helpful to consider the bigger picture of the analysis at hand rather than the minutia involved in model performance.\n\n\n**Exercise 5** \n\nCan you imagine any other ways in which modeling the weather could be helpful, and potential targets to predict given the dataset at hand? If you could have additional or different data to help in this task, what would you choose? Given the models created during this workshop, how would you recommend they be applied to a real-world problem? "}}