{"cell_type":{"64788538":"code","27b7f51e":"code","831790c6":"code","8e54b1e0":"code","71c2ff42":"code","1964f641":"code","c5d31ac5":"code","f807e5fd":"code","a7a554a2":"code","6cd7bbfd":"code","578610ee":"code","4935a12b":"code","d443ce9a":"code","133ff4d4":"code","9ab8ce89":"code","2cc67d8b":"code","70bdaac6":"code","7e8a3762":"code","77901280":"code","f7c41e0e":"code","2e86958d":"code","c727cbb0":"code","81d837a7":"code","6f3693d0":"code","84e7e19d":"code","5deb71fe":"code","ddc3238b":"code","59c8d933":"code","762db46b":"code","7574ee57":"code","8ff0d68c":"code","c45389fe":"code","5af8bcb0":"code","22f40e9c":"markdown","e798ce7a":"markdown","9482f986":"markdown","53b7cb09":"markdown","66357c75":"markdown","ea4af657":"markdown","5b958997":"markdown"},"source":{"64788538":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns # data visualization library  \nimport os\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import MinMaxScaler\nfrom yellowbrick.cluster import KElbowVisualizer\nfrom scipy.cluster.hierarchy import linkage\nfrom scipy.cluster.hierarchy import dendrogram\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LassoCV\n\nimport warnings\n\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","27b7f51e":"# read data\ndata = pd.read_csv('..\/input\/ccdata\/CC GENERAL.csv')\ndata.head()","831790c6":"data.describe().T","8e54b1e0":"#Count missing variable\ndata.isnull().sum().sort_values(ascending=False)","71c2ff42":"data.loc[data.isnull().any(axis=1)].head()","1964f641":"#Fill  missing variable\ndata['MINIMUM_PAYMENTS'].fillna(data[\"PAYMENTS\"], inplace=True)\ndata['CREDIT_LIMIT'].fillna(data['CREDIT_LIMIT'].median(), inplace=True)","c5d31ac5":"# Feature Engineering\ndata[\"new_BALANCE_BALANCE_FREQUENCY\"] = data[\"BALANCE\"] * data[\"BALANCE_FREQUENCY\"]\ndata[\"new_ONEOFF_PURCHASES_PURCHASES\"] = data[\"ONEOFF_PURCHASES\"] \/ data[\"PURCHASES\"]\ndata[\"new_INSTALLMENTS_PURCHASES_PURCHASES\"] = data[\"INSTALLMENTS_PURCHASES\"] \/ data[\"PURCHASES\"]\ndata[\"new_CASH_ADVANCE_PURCHASES_PURCHASES\"] = data[\"CASH_ADVANCE\"] * data[\"CASH_ADVANCE_FREQUENCY\"]\ndata[\"new_PURCHASES_PURCHASES_FREQUENCY\"] = data[\"PURCHASES\"] * data[\"PURCHASES_FREQUENCY\"]\ndata[\"new_PURCHASES_ONEOFF_PURCHASES_FREQUENCY\"] = data[\"PURCHASES\"] * data[\"ONEOFF_PURCHASES_FREQUENCY\"]\ndata[\"new_PURCHASES_PURCHASES_TRX\"] = data[\"PURCHASES\"] \/ data[\"PURCHASES_TRX\"]\ndata[\"new_CASH_ADVANCE_CASH_ADVANCE_TRX\"] = data[\"CASH_ADVANCE\"] \/ data[\"CASH_ADVANCE_TRX\"]\ndata[\"new_BALANCE_CREDIT_LIMIT\"] = data[\"BALANCE\"] \/ data[\"CREDIT_LIMIT\"]\ndata[\"new_PAYMENTS_CREDIT_LIMIT\"] = data[\"PAYMENTS\"] \/ data[\"MINIMUM_PAYMENTS\"]","f807e5fd":"#Checking missing variable\ndata.isnull().sum().sort_values(ascending=False).head()","a7a554a2":"data.fillna(0, inplace=True)","6cd7bbfd":"#Indexing CUST_ID feature\ndata.set_index('CUST_ID', inplace=True)\ndata.head()","578610ee":"def outlier_thresholds(dataframe, variable):\n    quartile1 = dataframe[variable].quantile(0.01)\n    quartile3 = dataframe[variable].quantile(0.99)\n    interquantile_range = quartile3 - quartile1\n    up_limit = quartile3 + 1.5 * interquantile_range\n    low_limit = quartile1 - 1.5 * interquantile_range\n    return low_limit, up_limit\n\n\ndef replace_with_thresholds(dataframe, variable):\n    low_limit, up_limit = outlier_thresholds(dataframe, variable)\n    dataframe.loc[(dataframe[variable] < low_limit), variable] = low_limit\n    dataframe.loc[(dataframe[variable] > up_limit), variable] = up_limit\n    \nfor col in data.columns:\n    replace_with_thresholds(data, col)","4935a12b":"plt.figure(figsize=(10,10))\nsns.boxplot(data=data)\nplt.xticks(rotation=90)\nplt.show()","d443ce9a":"# Min Max Scaler\nnames = data.columns\nindexes = data.index\nsc = MinMaxScaler((0, 1))\ndf = sc.fit_transform(data)\ndata_scaled = pd.DataFrame(df, columns=names, index=indexes)\ndata_scaled.head()","133ff4d4":"# KMeans Clustering\nkmeans = KMeans()\nssd = []\nK = range(1, 30)\n\nfor k in K:\n    kmeans = KMeans(n_clusters=k).fit(data_scaled)\n    ssd.append(kmeans.inertia_)\n\nssd\n\nplt.plot(K, ssd, \"bx-\")\nplt.xlabel(\"Distance Residual Sums for K Values (WCSS)\")\nplt.title(\"Elbow Method for Optimum Number of Clusters\")\nplt.show()\n\nkmeans = KMeans()\nvisu = KElbowVisualizer(kmeans, k=(2, 20))\nvisu.fit(df)\nvisu.show()","9ab8ce89":"kmeans = KMeans(n_clusters=7).fit(data_scaled)\nclusters = kmeans.labels_\n\npd.DataFrame({\"Customers\": data.index, \"Clusters\": clusters})\ndata[\"cluster_no\"] = clusters\ndata.head()","2cc67d8b":"data[\"cluster_no\"] = data[\"cluster_no\"] + 1\ndata.groupby(\"cluster_no\").agg({\"cluster_no\": \"count\"})","70bdaac6":"data.groupby(\"cluster_no\").agg(np.mean)","7e8a3762":"# Plot the histogram of various clusters\nfor i in data.columns:\n  plt.figure(figsize = (35, 5))\n  for j in range(1,8):\n    plt.subplot(1,8,j+1)\n    cluster = data[data['cluster_no'] == j]\n    cluster[i].hist(bins = 20)\n    plt.title('{}    \\nCluster {} '.format(i,j))\n  \n  plt.show()","77901280":"# Average Linkage Method\nhc_average = linkage(data_scaled, \"average\")\n\nplt.figure(figsize=(20, 10))\nplt.title(\"Hierarchical Clustering\")\nplt.xlabel(\"Observations\")\nplt.ylabel(\"Distance\")\ndendrogram(hc_average,\n           leaf_font_size=10, \n           p=10,\n           show_contracted=True,\n          truncate_mode='level')\nplt.show()","f7c41e0e":"# Complete Linkage Method\nhc_complete = linkage(data_scaled, \"complete\")\n\nplt.figure(figsize=(15, 10))\nplt.title(\"Hierarchical Clustering\")\nplt.xlabel(\"Observations\")\nplt.ylabel(\"Distance\")\ndendrogram(hc_complete,\n           truncate_mode=\"lastp\",\n           p=10,\n           show_contracted=True,\n           leaf_font_size=10)\nplt.show()","2e86958d":"pca = PCA().fit(data_scaled)\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel(\"# of components\")\nplt.ylabel(\"Cumulative Variance Ratio\")\nplt.show()","c727cbb0":"pca = PCA(n_components=6)\npca_fit = pca.fit_transform(data_scaled)\npca.explained_variance_ratio_","81d837a7":"np.cumsum(pca.explained_variance_ratio_)","6f3693d0":"#Using Pearson Correlation\nplt.figure(figsize=(25,10))\ncor = data_scaled.corr()\nsns.heatmap(cor, annot=True, cmap=plt.cm.Reds)\nplt.show()","84e7e19d":"#Correlation with BALANCE variable\ncor_target = abs(cor[\"BALANCE\"])\n#Selecting highly correlated features\nrelevant_features = cor_target[cor_target>0.5]\nrelevant_features","5deb71fe":"# Model Random Forest Regression\nX = data_scaled.drop([\"BALANCE\",\"new_BALANCE_BALANCE_FREQUENCY\", \"new_BALANCE_CREDIT_LIMIT\", \"BALANCE_FREQUENCY\"],1)   #Feature Matrix\ny = data_scaled[\"BALANCE\"]          #Target Variable\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=46)\n\nrf_model = RandomForestRegressor(random_state=42).fit(X_train, y_train)\ny_pred = rf_model.predict(X_train)\nnp.sqrt(mean_squared_error(y_train, y_pred))","ddc3238b":"# Model Tuning\nrf_params = {\"max_depth\": [5, 8, None],\n             \"max_features\": [3, 5, 15],\n             \"n_estimators\": [200, 500],\n             \"min_samples_split\": [2, 5, 8]}\n\nrf_model = RandomForestRegressor(random_state=42)\nrf_cv_model = GridSearchCV(rf_model, rf_params, cv=5, n_jobs=-1, verbose=1).fit(X_train, y_train)\nrf_cv_model.best_params_","59c8d933":"rf_tuned = RandomForestRegressor(**rf_cv_model.best_params_).fit(X_train, y_train)\n\ny_pred = rf_tuned.predict(X_train)\nnp.sqrt(mean_squared_error(y_train, y_pred))","762db46b":"def plot_importance(model, features, num=len(X), save=False):\n    feature_imp = pd.DataFrame({'Value': model.feature_importances_, 'Feature': features.columns})\n    plt.figure(figsize=(10, 10))\n    sns.set(font_scale=1)\n    sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\",\n                                                                     ascending=False)[0:num])\n    plt.title('Features')\n    plt.tight_layout()\n    plt.show()\n    if save:\n        plt.savefig('importances.png')\n\n\nplot_importance(rf_tuned, X, 20)","7574ee57":"X = data_scaled.drop([\"BALANCE\",\"new_BALANCE_BALANCE_FREQUENCY\", \"new_BALANCE_CREDIT_LIMIT\", \"BALANCE_FREQUENCY\"],1)   #Feature Matrix\ny = data_scaled[\"BALANCE\"]          #Target Variable\n\nreg = LassoCV()\nreg.fit(X, y)","8ff0d68c":"print(\"Best alpha using built-in LassoCV: %f\" % reg.alpha_)\nprint(\"Best score using built-in LassoCV: %f\" %reg.score(X,y))\ncoef = pd.Series(reg.coef_, index = X.columns)","c45389fe":"print(\"Lasso picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  \n      str(sum(coef == 0)) + \" variables\")","5af8bcb0":"imp_coef = coef.sort_values()\nimport matplotlib\nmatplotlib.rcParams['figure.figsize'] = (8.0, 10.0)\nimp_coef.plot(kind = \"barh\")\nplt.title(\"Feature importance using Lasso Model\")\nplt.show()","22f40e9c":"## HIERARCHICAL CLUSTERING","e798ce7a":"## KMEANS CLUSTERING","9482f986":"## FEATURE SELECTION","53b7cb09":"## Lasso CV Feature Importances","66357c75":"## PRINCIPAL COMPONENT ANALYSIS","ea4af657":"## DATA PREPROCESSING - FEATURE ENGINEERING","5b958997":"## Random Forest Feature Importances"}}