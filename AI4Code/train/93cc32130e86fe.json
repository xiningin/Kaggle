{"cell_type":{"c01d6fb3":"code","4660e889":"code","9196bdcb":"code","f689e68e":"code","a2bb707b":"code","b644a902":"code","3d290180":"code","d97363e8":"code","3648661e":"code","381e8391":"code","93824907":"code","59ec51fe":"code","2ba4fadd":"code","4585035e":"code","70637ff3":"code","19d1e98e":"code","3a66abbd":"code","1e494dd5":"code","aa34a9f6":"code","3cd6f510":"code","b187297d":"code","a32403ca":"code","eea0de22":"code","2b90924e":"code","434bc03f":"code","ede528a4":"code","8720a3ee":"code","a3839d0e":"code","536a7b75":"code","4da62f2e":"code","e2109c06":"code","6fe2a32a":"code","9a623ca2":"code","ab87fae9":"code","9e961947":"code","f7891ec9":"code","77ebfafa":"code","2d400428":"code","e9dc9c5b":"code","d1fc662a":"code","4df4eff1":"code","8a56d71b":"code","aaa88e97":"code","ee623d13":"code","0cbdea62":"code","8eb9e9d2":"code","5e23c5d5":"code","301c3557":"code","823bdd07":"code","b41c4067":"markdown","c14f9b2b":"markdown","2e07e46d":"markdown","09cd1a82":"markdown","e45ad5c8":"markdown","f6aac7db":"markdown","3c9f40d5":"markdown","6b7166cb":"markdown","20844c4e":"markdown","28784d0d":"markdown","e48ed26f":"markdown","8e9d442b":"markdown","0b164dde":"markdown","61915ace":"markdown","65934c85":"markdown","f7a22865":"markdown","7b1f844e":"markdown","30330832":"markdown","2d04df96":"markdown","348f0699":"markdown"},"source":{"c01d6fb3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4660e889":"train = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\")","9196bdcb":"whole_df = pd.concat([train, test])","f689e68e":"whole_df","a2bb707b":"# Model Algorithms\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\nfrom xgboost import XGBClassifier\n\n# Model Helpers\nfrom scipy import stats\nfrom scipy.stats import norm, skew \nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\n\n#Visualization\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nimport seaborn as sns\n\nfrom plotly import __version__\nimport cufflinks as cf\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=True)\ncf.go_offline()\n\n#Configure Visualization Defaults\n%matplotlib inline\nmpl.style.use('ggplot')\nsns.set_style('white')\npylab.rcParams['figure.figsize'] = 12,8\npd.set_option('display.float_format', lambda x: '%.3f' % x)\n\n\n#ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')","b644a902":"# Saving ID columns of train and test for the RMSE test\ntrain_salePrice = train.loc[:, 'SalePrice']\ntrain_ID = train.loc[:, 'Id']\ntest_ID = test.loc[:, 'Id']","3d290180":"train.head(10)","d97363e8":"whole_df.set_index('Id', drop = True, inplace = True)\nraw_df = pd.concat([train, test])\nraw_df.set_index('Id', drop = True, inplace = True)","3648661e":"train['SalePrice'].iplot(kind='hist')","381e8391":"plt.subplots(figsize = (12,8))\ncorrmatrix = whole_df.corr()\ncols = corrmatrix.nlargest(15,'SalePrice').index\n#cm = np.corrcoef(train[cols].values.T)\ntop_cols_df = whole_df.loc[:,cols]\nsns.heatmap(top_cols_df.corr(), annot = True)\n","93824907":"total = whole_df.isnull().sum().sort_values(ascending = False)\npercent = (whole_df.isnull().sum()\/whole_df.isnull().count()).sort_values()\nmissing = pd.concat([total, percent], axis =1, keys = ['Total', 'Percent'])\nprint('Total Missing Values')\nmiss_cols = missing.iloc[:20].index\nmissing.iloc[:20]\n","59ec51fe":"plt.subplots(figsize = (12,9))\nsns.heatmap(whole_df[miss_cols].isnull())","2ba4fadd":"# Handle missing values for features where median\/mean or most common value doesn't make sense\n\n# Alley : data description says NA means \"no alley access\"\nwhole_df.loc[:, \"Alley\"] = whole_df.loc[:, \"Alley\"].fillna(\"None\")\n\n# BedroomAbvGr : NA most likely means 0\nwhole_df.loc[:, \"BedroomAbvGr\"] = whole_df.loc[:, \"BedroomAbvGr\"].fillna(0)\n\n# BsmtQual etc : data description says NA for basement features is \"no basement\"\nwhole_df.loc[:, \"BsmtQual\"] = whole_df.loc[:, \"BsmtQual\"].fillna(\"None\")\nwhole_df.loc[:, \"BsmtCond\"] = whole_df.loc[:, \"BsmtCond\"].fillna(\"None\")\nwhole_df.loc[:, \"BsmtExposure\"] = whole_df.loc[:, \"BsmtExposure\"].fillna(\"None\")\nwhole_df.loc[:, \"BsmtFinType1\"] = whole_df.loc[:, \"BsmtFinType1\"].fillna(\"None\")\nwhole_df.loc[:, \"BsmtFinType2\"] = whole_df.loc[:, \"BsmtFinType2\"].fillna(\"None\")\nwhole_df.loc[:, \"BsmtFullBath\"] = whole_df.loc[:, \"BsmtFullBath\"].fillna(0)\nwhole_df.loc[:, \"BsmtHalfBath\"] = whole_df.loc[:, \"BsmtHalfBath\"].fillna(0)\nwhole_df.loc[:, \"BsmtUnfSF\"] = whole_df.loc[:, \"BsmtUnfSF\"].fillna(0)\n\n# CentralAir : NA most likely means No\nwhole_df.loc[:, \"CentralAir\"] = whole_df.loc[:, \"CentralAir\"].fillna(\"N\")\n\n# Condition : NA most likely means Normal\nwhole_df.loc[:, \"Condition1\"] = whole_df.loc[:, \"Condition1\"].fillna(\"Norm\")\nwhole_df.loc[:, \"Condition2\"] = whole_df.loc[:, \"Condition2\"].fillna(\"Norm\")\n\n# EnclosedPorch : NA most likely means no enclosed porch\nwhole_df.loc[:, \"EnclosedPorch\"] = whole_df.loc[:, \"EnclosedPorch\"].fillna(0)\n\n# External stuff : NA most likely means average\nwhole_df.loc[:, \"ExterCond\"] = whole_df.loc[:, \"ExterCond\"].fillna(\"TA\")\nwhole_df.loc[:, \"ExterQual\"] = whole_df.loc[:, \"ExterQual\"].fillna(\"TA\")\n\n# Fence : data description says NA means \"no fence\"\nwhole_df.loc[:, \"Fence\"] = whole_df.loc[:, \"Fence\"].fillna(\"None\")\n\n# FireplaceQu : data description says NA means \"no fireplace\"\nwhole_df.loc[:, \"FireplaceQu\"] = whole_df.loc[:, \"FireplaceQu\"].fillna(\"None\")\nwhole_df.loc[:, \"Fireplaces\"] = whole_df.loc[:, \"Fireplaces\"].fillna(0)\n\n# Functional : data description says NA means typical\nwhole_df.loc[:, \"Functional\"] = whole_df.loc[:, \"Functional\"].fillna(\"Typ\")\n\n# GarageType etc : data description says NA for garage features is \"no garage\"\nwhole_df.loc[:, \"GarageType\"] = whole_df.loc[:, \"GarageType\"].fillna(\"None\")\nwhole_df.loc[:, \"GarageFinish\"] = whole_df.loc[:, \"GarageFinish\"].fillna(\"None\")\nwhole_df.loc[:, \"GarageQual\"] = whole_df.loc[:, \"GarageQual\"].fillna(\"None\")\nwhole_df.loc[:, \"GarageCond\"] = whole_df.loc[:, \"GarageCond\"].fillna(\"None\")\nwhole_df.loc[:, \"GarageCars\"] = whole_df.loc[:, \"GarageCars\"].fillna(0)\n\n# HalfBath : NA most likely means no half baths above grade\nwhole_df.loc[:, \"HalfBath\"] = whole_df.loc[:, \"HalfBath\"].fillna(0)\n\n# HeatingQC : NA most likely means typical\nwhole_df.loc[:, \"HeatingQC\"] = whole_df.loc[:, \"HeatingQC\"].fillna(\"TA\")\n\n# KitchenAbvGr : NA most likely means 0\nwhole_df.loc[:, \"KitchenAbvGr\"] = whole_df.loc[:, \"KitchenAbvGr\"].fillna(0)\n\n# KitchenQual : NA most likely means typical\nwhole_df.loc[:, \"KitchenQual\"] = whole_df.loc[:, \"KitchenQual\"].fillna(\"TA\")\n\n# LotFrontage : NA most likely means no lot frontage\nwhole_df.loc[:, \"LotFrontage\"] = whole_df.loc[:, \"LotFrontage\"].fillna(0)\n\n# LotShape : NA most likely means regular\nwhole_df.loc[:, \"LotShape\"] = whole_df.loc[:, \"LotShape\"].fillna(\"Reg\")\n\n# MasVnrType : NA most likely means no veneer\nwhole_df.loc[:, \"MasVnrType\"] = whole_df.loc[:, \"MasVnrType\"].fillna(\"None\")\nwhole_df.loc[:, \"MasVnrArea\"] = whole_df.loc[:, \"MasVnrArea\"].fillna(0)\n\n# MiscFeature : data description says NA means \"no misc feature\"\nwhole_df.loc[:, \"MiscFeature\"] = whole_df.loc[:, \"MiscFeature\"].fillna(\"None\")\n\n# MSZoning : NA replaced with the most frequently occuring value 'RL'\nwhole_df.loc[:, \"MSZoning\"] = whole_df.loc[:, \"MSZoning\"].fillna(whole_df.loc[:, \"MSZoning\"].mode()[0])\n\n# OpenPorchSF : NA most likely means no open porch\nwhole_df.loc[:, \"OpenPorchSF\"] = whole_df.loc[:, \"OpenPorchSF\"].fillna(0)\n\n# PavedDrive : NA most likely means not paved\nwhole_df.loc[:, \"PavedDrive\"] = whole_df.loc[:, \"PavedDrive\"].fillna(\"N\")\n\n# PoolQC : data description says NA means \"no pool\"\nwhole_df.loc[:, \"PoolQC\"] = whole_df.loc[:, \"PoolQC\"].fillna(\"None\")\nwhole_df.loc[:, \"PoolArea\"] = whole_df.loc[:, \"PoolArea\"].fillna(0)\n\n# SaleCondition : NA most likely means normal sale\nwhole_df.loc[:, \"SaleCondition\"] = whole_df.loc[:, \"SaleCondition\"].fillna(\"Normal\")\n\n# ScreenPorch : NA most likely means no screen porch\nwhole_df.loc[:, \"ScreenPorch\"] = whole_df.loc[:, \"ScreenPorch\"].fillna(0)\n\n# WoodDeckSF : NA most likely means no wood deck\nwhole_df.loc[:, \"WoodDeckSF\"] = whole_df.loc[:, \"WoodDeckSF\"].fillna(0)\nwhole_df.drop(['GarageArea', '1stFlrSF', 'TotRmsAbvGrd', 'GarageYrBlt', 'Utilities','MiscVal'], axis = 1, inplace = True)","4585035e":"total = whole_df.isnull().sum().sort_values(ascending = False)\npercent = (whole_df.isnull().sum()\/whole_df.isnull().count()).sort_values()\nmissing = pd.concat([total, percent], axis = 1, keys = ['Total', 'Percent'])\nprint('Total Missing Values')\nmiss_cols = missing.iloc[:20].index\nmissing.iloc[:20]","70637ff3":"for i in miss_cols[:7]:\n    whole_df.loc[:, i] = whole_df.loc[:, i].fillna(whole_df.loc[:, i].mode()[0]) ","19d1e98e":"total = whole_df.isnull().sum().sort_values(ascending = False)\npercent = (whole_df.isnull().sum()\/whole_df.isnull().count()).sort_values()\nmissing = pd.concat([total, percent], axis = 1, keys = ['Total', 'Percent'])\nprint('Total Missing Values')\nmissing.iloc[:20]\n","3a66abbd":"# Checking if there is any null values left in the data set\nwhole_df.isnull().values.any()","1e494dd5":"# Taking the Ordinal Features in one list\nord_fields = ['MSSubClass','ExterQual','LotShape','BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1',\n            'BsmtFinType2','HeatingQC','Functional','FireplaceQu','KitchenQual', 'GarageFinish',\n            'GarageQual','GarageCond','PoolQC','Fence']\n\n# Ordering values of each column in 'ord_fields'\norders = [ \n    #MSSubClass \n    ['20','30','40','45','50','60','70','75','80','85', '90','120','150','160','180','190'],\n    #ExterQual \n    ['Fa','TA','Gd','Ex'],\n    #LotShape \n    ['Reg','IR1' ,'IR2','IR3'], \n    #BsmtQual \n    ['None','Fa','TA','Gd','Ex'],\n    #BsmtCond\n    ['None','Po','Fa','TA','Gd','Ex'],\n    #BsmtExposure \n    ['None','No','Mn','Av','Gd'],\n    #BsmtFinType1 \n    ['None','Unf','LwQ', 'Rec','BLQ','ALQ' , 'GLQ' ],\n    #BsmtFinType2 \n    ['None','Unf','LwQ', 'Rec','BLQ','ALQ' , 'GLQ' ],\n    #HeatingQC \n    ['Po','Fa','TA','Gd','Ex'], \n    #Functional \n    ['Sev','Maj2','Maj1','Mod','Min2','Min1','Typ'], \n    #FireplaceQu \n    ['None','Po','Fa','TA','Gd','Ex'], \n    #KitchenQual \n    ['Fa','TA','Gd','Ex'],\n    #GarageFinish \n    ['None','Unf','RFn','Fin'],\n    #GarageQual \n    ['None','Po','Fa','TA','Gd','Ex'],\n    #GarageCond\n    ['None','Po','Fa','TA','Gd','Ex'],\n    #PoolQC\n    ['None','Fa','Gd','Ex'], \n    #Fence\n    ['None','MnWw','GdWo','MnPrv','GdPrv']]\n\n# Using 'OrdinalEncoder' from the sklearn preprocessing package to convert from Categorical to\n\nfrom sklearn.preprocessing import OrdinalEncoder\nfor i in range(len(orders)): \n    ord_en = OrdinalEncoder(categories = {0 : orders[i]}) \n    whole_df.loc[:, ord_fields[i]] = ord_en.fit_transform(whole_df.loc[:, ord_fields[i]].values.reshape(-1,1))","aa34a9f6":"categorical = whole_df.select_dtypes(include = [\"object\"]).columns\nnumerical = whole_df.select_dtypes(exclude = [\"object\"]).columns\n\nprint(\"Numerical features : \" + str(len(numerical)))\nprint(\"Categorical features : \" + str(len(categorical)))\nnum_df = whole_df[numerical]\ncat_df = whole_df[categorical]","3cd6f510":"# Number of unique values in the Categorical features DataFrame 'cat_df' per column\ncat_df.nunique()","b187297d":"cat_cols = cat_df.columns\nfrom sklearn.preprocessing import LabelEncoder\nfor one in cat_cols:\n    lab_enc = LabelEncoder() \n    lab_enc.fit(list(whole_df[one].values)) \n    whole_df[one] = lab_enc.transform(list(whole_df[one].values))","a32403ca":"# Checking to see if there are any categorical columns left in 'whole_df'\ncategorical = whole_df.select_dtypes(include = [\"object\"]).columns\nnumerical = whole_df.select_dtypes(exclude = [\"object\"]).columns\n\nprint(\"Numerical features : \" + str(len(numerical)))\nprint(\"Categorical features : \" + str(len(categorical)))\nnum_df = whole_df[numerical]\ncat_df = whole_df[categorical]","eea0de22":"plt.subplots(figsize = (11,7))\ncorrmatrix = raw_df.corr()\ncols = corrmatrix.nlargest(15,'SalePrice').index\n#cm = np.corrcoef(train[cols].values.T)\ntop_cols_df = raw_df.loc[:,cols]\nsns.heatmap(top_cols_df.corr(), annot = True)","2b90924e":"plt.subplots(figsize = (11,7))\ncorrmatrix = whole_df.corr()\ncols = corrmatrix.nlargest(15,'SalePrice').index\n#cm = np.corrcoef(train[cols].values.T)\ntop_cols_df = whole_df.loc[:,cols]\nsns.heatmap(top_cols_df.corr(), annot = True)","434bc03f":"whole_df.drop('SalePrice', axis = 1, inplace = True)","ede528a4":"#sns.boxplot(x = train_salePrice, y = whole_df.loc[train_ID, 'PoolQC'])\ncol = 'SalePrice'\ntrain[col].iplot(kind = 'hist')","8720a3ee":"# Creat a histogram and normal probability plot for 'Sale Price'\nsns.distplot(train_salePrice, fit = norm)\nfig = plt.figure()\nres = stats.probplot(train_salePrice, plot = plt)","a3839d0e":"# Applying the Box-Cox log transformation to the target feature with lambda = 0.15\n\nlam = 0.15\nfrom scipy.special import boxcox1p\ntrain_log_SP = boxcox1p(train_salePrice, lam)","536a7b75":"# Plotting the transformed histogram and normal probability plot of 'Sale Price'\nsns.distplot(train_log_SP, fit = norm)\nfig = plt.figure()\nres = stats.probplot(train_log_SP, plot = plt)","4da62f2e":"# Checking to see if there are any categorical columns left in 'whole_df'\ncategorical = whole_df.select_dtypes(include = [\"object\"]).columns\nnumerical = whole_df.select_dtypes(exclude = [\"object\"]).columns\n\n# Creating a table of variables and their relative skewness\nskewed = whole_df[numerical].apply(lambda x: skew(x)).sort_values(ascending = False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed})\nskewness.head(20)","e2109c06":"# We transform features with skewness larger than 0.75 to ensure the linearity of features.\nskewness = skewness[abs(skewness) > 0.75]\nprint(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n\nfrom scipy.special import boxcox1p\nskewed_features = skewness.index\n\nfor feat in skewed_features:\n    whole_df[feat] = boxcox1p(whole_df[feat], lam)","6fe2a32a":"ntrain = len(train.index)\nntrain","9a623ca2":"train = whole_df.iloc[ : ntrain]\ntest = whole_df.iloc[ntrain : ]\n\ny = train_log_SP\nX = train","ab87fae9":"from sklearn.model_selection import train_test_split","9e961947":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)","f7891ec9":"from sklearn.linear_model import LinearRegression","77ebfafa":"lm = LinearRegression()","2d400428":"lm.fit(X_train, y_train)","e9dc9c5b":"predictions = lm.predict(X_test)","d1fc662a":"sns.set_style(style = 'whitegrid')\nsns.jointplot(x = y_test, y = predictions)\n#plt.scatter(y_test, predictions)\nplt.xlabel('Y label')\nplt.ylabel('Predicted Y')","4df4eff1":"from sklearn import metrics\nprint('RMSE: ', np.sqrt(metrics.mean_squared_error(y_test,predictions)))","8a56d71b":"metrics.explained_variance_score(y_test,predictions)","aaa88e97":"lm_test = LinearRegression()","ee623d13":"lm_test.fit(X,y)","0cbdea62":"test_predictions = lm_test.predict(test)","8eb9e9d2":"len(test_predictions)","5e23c5d5":"from scipy.special import boxcox1p, inv_boxcox1p\ntest_predictions = inv_boxcox1p(test_predictions, lam)","301c3557":"sns.jointplot(x = train_ID, y = train_salePrice)\nsns.jointplot(x = test_ID, y = test_predictions)","823bdd07":"solution = pd.DataFrame({\"id\": test_ID, \"SalePrice\": test_predictions})\nsolution.to_csv(\"House_Solutions.csv\", index = False)","b41c4067":"#### A. Completing\nCompleting missing information","c14f9b2b":"Dividing the **'whole_df'** dataframe into two separate **numerical** and **categorical** feature dataframes","2e07e46d":"### Handling Missing Values","09cd1a82":"# House prices analysis using Linear Regression\nAuthor: Gan-Od Bayarbaatar","e45ad5c8":"### B. Normality Test\n\nLinear Regression is a very useful tool for a data scientist because it is easy to apply to data and comprehend the results.\n\nThe integral step for a successful Linear Regression model is to ensure the normality and linearity of the features. We analyze the skewness of our features and linearize using Box Cox Transformation.\n\nWe start with the Target variable, the Sale Price column, check its linearity and skewness.","f6aac7db":"# 2. Gather Data\nSave training and testing dataset into two separate Pandas DataFrames, then combining them into one for future data cleaning and preprocessing.","3c9f40d5":"# 5. Model Data\nWe are ready to use the Linear Regression method from Sci-Kit Learn on our linearized data.","6b7166cb":"### Model on test data\nThe RMSE score on the trainind data looks promising, so we move on and predict the house prices in our 'test' data set with a Linear Regression model fitted on the whole 'train' data set.","20844c4e":"# 4. Exploratory Data Analysis\n* A. Check correlation between features   \n* B. Normality Testing","28784d0d":"Now, we will convert the rest of the categorical features, which should all be nominal, into numerical values using LabelEncoder from the Sklearn Preprocessing package.","e48ed26f":"#### C. Converting\n\nFirst, we will be converting some of the **ordinal categorical features** into numerical values with OrdinalEncoder from the Sci-kit Learn package.\n\nThis should be done separately from the **nominal (non-ordinal) categorical features** because the orders of values in these columns matter, while in columns with nominal values, the value ordering does not provide any more information.","8e9d442b":"## 3.2 Meet and Greet Data","0b164dde":"# 3. Data Wrangling\n\n## 3.1 Import Libraries","61915ace":"In this kernel, we will predict house prices from a collection of 2930 houses in Ames, Iowa, each with 80 (23 nominal, 23 ordinal, 14 discrete, and 20 continuous) features such as 'number of bathrooms above ground', and 'size of front lawn'.  \n\nThe Ames dataset is a modern expansion on the famous Boston Housing dataset. Its extensive data and numerous features require a robust feature engineering and provides great practice for data scientists to hone their machine learning skills.\n\nThe kernel comprises of the following sections:\n- Get Data\n- Data Cleaning\n- Data Preprocessing\n- Data Modeling\n\nI will provide explanations for each section for its importance in this Linear Regression model, in the hopes  it will be comprehensible to beginners as well as experts.  \nI have found the following kernels to be incredibly useful and readable, and I encourage you to go check them out:\n\n- Comprehensive data exploration with Python by Pedro Marcelino:  \nhttps:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python#Out-liars\n\n- A study on Regression applied to the Ames dataset by Julien Cohen-Solal:  \nhttps:\/\/www.kaggle.com\/juliencs\/a-study-on-regression-applied-to-the-ames-dataset\n\n- Regularized Linear Models by Alexandru Papiu:  \nhttps:\/\/www.kaggle.com\/apapiu\/regularized-linear-models","65934c85":"### Check Skewness of all parameters\nWith a transformed 'Sale Price' column, we want to make sure the other important features are linear as well.","f7a22865":"### A. Check for Correlations\nCorrelation between features themselves leads to a big problem in ML known as Multicollinearity\nThe problems associated with Multicollinearity include:\n- coefficients will become highly sensitive to small changes in the model. (Coefficients swing wildly based on which other independent variables are in the model)\n- reduce the precision of the estimated coefficients, which in turn will undermine the statistical significance of the coefficient and weaken the statistical power of the model. The p-values will become useless when identifying independent variables that are statistically significant.","7b1f844e":"Finally, we have now handled all of our missing values, dealt with our categorical features both ordinal and nominal, and have turned all values numerical.\n\n\nWe proceed on to preparing our data for our selected model: Linear Regression.","30330832":"whole_df.drop(['GarageArea', '1stFlrSF', 'TotRmsAbvGrd', 'GarageYrBlt', 'Utilities','MiscVal'], axis = 1, inplace = True)","2d04df96":"## 3.3 Data Cleaning\n\nIn the data cleaning section, we will be: \n\n- A. **Completing** missing information  \nHandling missing values: Replace a NaN value with the appropriate replacement value - either mean or median or simply \"None\" depending on the significance and meaning it holds for the particular column\n\n- B. **Creating** new features for analysis  \n\n- C. **Converting** fields to the correct format for calculations and presentations  \nFeature Engineering: Convert categorical values into numerical values using LabelEncoder on Nominal features and OrdinalEncoder on Ordinal features.\n\n- D. **Correcting** aberrant values and outliers  (do after exploratory analysis) ","348f0699":"Eliminating highly correlated columns"}}