{"cell_type":{"667ad5b4":"code","0fd1dc2a":"code","abdd48f1":"code","9f5dce36":"code","71f0f699":"code","df0e6e11":"code","99418371":"code","ab385681":"code","bd69da1b":"code","dcd9e593":"code","0c667a50":"code","fb8b8275":"code","951b635e":"code","0ef01601":"code","abda248d":"code","7d3e40f2":"code","6026b5be":"code","176e58e0":"code","d5f87c34":"code","40ab01d3":"code","e6609cb1":"code","a7a2cc56":"code","6bd43ba3":"code","d40eb32e":"code","61d7d032":"code","1f52850e":"code","1e55ea04":"code","0c956e43":"code","3c32ff36":"code","1bd87fa2":"code","b4a00636":"code","14a31d02":"code","3448efd0":"code","16bfcfd8":"code","fb4e6b5e":"code","c101e8c8":"code","b59db4b3":"code","9d6d7028":"code","5f7e32f2":"code","a170d908":"code","49a72b08":"code","d8e73545":"markdown","71cbed7e":"markdown","85b0deee":"markdown","0f2dde32":"markdown","72cbc5b3":"markdown","facc4874":"markdown","cbaac556":"markdown","97398a90":"markdown","049564a5":"markdown","2c4dd411":"markdown","1f632906":"markdown","5846e9b1":"markdown","0d45135a":"markdown","db128acd":"markdown","9867199e":"markdown","77b8f9fd":"markdown","c4328198":"markdown","7099c94a":"markdown","f1c71a53":"markdown","5dc873a7":"markdown","ab19cdd0":"markdown","cbbfd911":"markdown","075ff581":"markdown","6b0fd487":"markdown","9b307905":"markdown","707b4014":"markdown","eff4a973":"markdown","115ae976":"markdown","2c04746c":"markdown","0ea7651d":"markdown","f0603fa4":"markdown","9318d0a9":"markdown","098f6be7":"markdown","fd4dbfe7":"markdown","3a57b4ad":"markdown","a6a1afcf":"markdown","cc240807":"markdown"},"source":{"667ad5b4":"# Load the dependencies\n\nimport numpy as np\nimport pandas as pd \nimport os\n\n#Import sklearn classes \nfrom sklearn.model_selection import train_test_split,RepeatedKFold, cross_val_score,KFold, RepeatedStratifiedKFold\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.tree import DecisionTreeClassifier,export_graphviz\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, BaggingClassifier\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n# sklearn utility to compare algorithms\nfrom sklearn import model_selection\n\n#Visualisation Libraries\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport graphviz \n\nimport warnings  \nwarnings.filterwarnings('ignore')\n\nfrom eli5 import explain_weights,show_weights\nfrom yellowbrick import ROCAUC\nfrom yellowbrick.classifier import ClassificationReport\n\nprint(\"Imported all libraries successfully\")\nprint(os.listdir(\"..\/input\"))\n\nCV_N_REPEATS=20\nBINS=10","0fd1dc2a":"df = pd.read_csv(\"..\/input\/diabetes.csv\")\ndf.head()","abdd48f1":"print('Shape of the dataset', df.shape)","9f5dce36":"# We can use the pandas_profiling library to automate most of the EDA process for us. It has been commented out for keeping the notebook concise. \n#import pandas_profiling\n#report=pandas_profiling.ProfileReport(df,check_correlation =True);\n#report.to_file(outputfile=\"eda_report.html\")","71f0f699":"plt.figure()\nplt.plot(df.Outcome,'.')\nplt.xlabel('Index')\nplt.ylabel('Outcome')","df0e6e11":"plt.figure()\nax = sns.countplot(data=df, x='Outcome');\nax.set_title(\"Seaborn countplot of Dibaetes rates within the dataset\");","99418371":"f, axes = plt.subplots(2, 4,figsize=(15,15))\nsns.set(style=\"white\", palette=\"Set3\", color_codes=True)\nsns.boxplot(  y=\"Pregnancies\", x= \"Outcome\", data=df,  orient='v', ax=axes[0,0])\nsns.boxplot(  y=\"Glucose\", x= \"Outcome\", data=df,  orient='v' , ax=axes[0,1])\nsns.boxplot(  y=\"BloodPressure\", x= \"Outcome\", data=df,  orient='v' ,ax=axes[0,2])\nsns.boxplot(  y=\"SkinThickness\", x= \"Outcome\", data=df,  orient='v' , ax=axes[0,3])\nsns.boxplot(  y=\"Insulin\", x= \"Outcome\", data=df,  orient='v' ,  ax=axes[1,0])\nsns.boxplot(  y=\"BMI\", x= \"Outcome\", data=df,  orient='v' ,  ax=axes[1,1])\nsns.boxplot(  y=\"DiabetesPedigreeFunction\", x= \"Outcome\", data=df,  orient='v' , ax=axes[1,2])\nsns.boxplot(  y=\"Age\", x= \"Outcome\", data=df,  orient='v' ,  ax=axes[1,3])\n\nf.subplots_adjust(left=0.08, right=0.98, bottom=0.05, top=0.9,\n                    hspace=0.4, wspace=0.3)\n#f.suptitle('Distribution of data')\nplt.tight_layout()","ab385681":"df_copy = df.copy(deep = True)\ndf_copy[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']] = df_copy[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']].replace(0,np.NaN)\nprint('Number of zero entries in each attribute:\\n')\nprint(df_copy.isnull().sum())","bd69da1b":"p = df.hist(figsize = (15,15))","dcd9e593":"#Impute NaN values using mean and meadian values\nprint('Imputing NaN values')\ndf_copy['Glucose'].fillna(df_copy['Glucose'].mean(), inplace = True)\ndf_copy['BloodPressure'].fillna(df_copy['BloodPressure'].mean(), inplace = True)\ndf_copy['BMI'].fillna(df_copy['BMI'].mean(), inplace = True)\ndf_copy['SkinThickness'].fillna(df_copy['SkinThickness'].median(), inplace = True)\ndf_copy['Insulin'].fillna(df_copy['Insulin'].median(), inplace = True)\nprint('Number of zero entries in each attribute:\\n')\nprint(df_copy.isnull().sum())","0c667a50":"#plot the correlation map of the dataset  \nplt.figure(figsize=(10,10))\ncorr = df_copy.corr()\ncorr.index = df_copy.columns\nsns.heatmap(corr, annot = True, cmap='RdYlGn', vmin=-1, vmax=1)\nplt.title(\"Correlation Heatmap\", fontsize=16)\nplt.show()","fb8b8275":"plt.figure()\nsns.pairplot(data=df_copy,hue='Outcome',diag_kind='kde', palette='deep');","951b635e":"# Separating the features and the target (Y) \nX = df_copy.iloc[:,0:8]\nY = df_copy.iloc[:,8]\n\n#Standardizing the features\nX= StandardScaler().fit_transform(X)\n# Fit PCA and transform X.\npca=PCA(n_components=.90)\npca.fit(X)\nprint('Variance explained by the principal components(in decreasing order): ',pca.explained_variance_ratio_)\n#print('PCA singular values: ',pca.singular_values_)\nX1=pca.transform(X)\nprint('Shape of transformed X: ',X1.shape)","0ef01601":"plt.figure()\nsns.scatterplot(X1[:,0],X1[:,1], hue=Y, palette='deep')\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.title('Scatter Plot of First two Principal Component')\nplt.show()","abda248d":"from mpl_toolkits.mplot3d import Axes3D\nplt.style.use('classic')\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.scatter(X1[:,0],X1[:,1], X1[:,2], c=df.Outcome, s=30)\nax.view_init(10, 100)\nax.set_xlabel('PC1')\nax.set_ylabel('PC2')\nax.set_zlabel('PC3')\nplt.tight_layout()\nplt.show()","7d3e40f2":"X = df_copy.iloc[:,0:8]\nY = df_copy.iloc[:,8]\nseed = 7\ntest_size = 0.20\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)\nprint(\"Shape of X_train:\", X_train.shape)\nprint(\"Shape of X_test:\", X_test.shape)","6026b5be":"dum=DummyClassifier(strategy='most_frequent')\ndum=dum.fit(X_train,y_train)\n\n#compute accuracy\nscore=dum.score(X_test, y_test)\nprint(\"Dummy Classifier Accuracy: %.2f%%\" % (score * 100.0))","176e58e0":"strategy = \"most_frequent\"\n\nscores = cross_val_score(dum,X, Y, \n                         cv=RepeatedKFold(n_repeats=CV_N_REPEATS), \n                         scoring=None) \nscores_dummy = scores.copy()\n\nscore_line = \"Scores (Accuracy) mean={0:.2f} +\/- {1:.2f} (1 s.d.)\".format(scores.mean(),scores.std())\nplt.figure(figsize=(7,7))\nfig, ax = plt.subplots()\npd.Series(scores).hist(ax=ax, bins=BINS)\nax.set_title(f\"RepeatedKFold ({len(scores)} folds) with DummyClassifier({strategy})\\n\" + score_line);\nax.set_xlabel(\"Score\")\nax.set_ylabel(\"Frequency\");","d5f87c34":"# Helper functions for graphical plotting of decision trees and to plot confusion matrix \ndef plot_tree_graph(model,columns,class_names):\n    #This function plots the constructed decision tree\n    dot_data = export_graphviz(model,feature_names=columns,class_names=class_names) \n    graph = graphviz.Source(dot_data) \n    return graph\n\n\ndef confusion_mat(y_pred,y_test):\n    plt.figure()\n    sns.set(font_scale=1.5)\n    cm = confusion_matrix(y_pred, y_test)\n    sns.heatmap(cm, annot=True, fmt='g')\n    plt.title('Confusion matrix', y=1.1)\n    plt.ylabel('Actual label')\n    plt.xlabel('Predicted label')\n    plt.show()","40ab01d3":"knn=KNeighborsClassifier(n_neighbors=11) \nknn.fit(X_train,y_train)\n\n#compute accuracy\nscores = cross_val_score(knn, X, Y, cv=RepeatedStratifiedKFold(n_repeats=CV_N_REPEATS))\nprint(f\"Accuracy mean={scores.mean():0.2f} +\/- {scores.std():0.2f} (1 s.d.)\")","e6609cb1":"dt=DecisionTreeClassifier(random_state=1, max_depth=2)\ndt=dt.fit(X_train,y_train)\ndt_scores = cross_val_score(dt, X, Y, cv=RepeatedStratifiedKFold(n_repeats=CV_N_REPEATS))\nprint(f\"Accuracy mean={dt_scores.mean():0.2f} +\/- {dt_scores.std():0.2f} (1 s.d.)\")","a7a2cc56":"plt.figure()\ngraph=plot_tree_graph(dt,X.columns,class_names=['0','1'])\ngraph","6bd43ba3":"bag=BaggingClassifier(n_estimators=100,oob_score=True) \nbag=bag.fit(X_train,y_train)\n\nbag_scores = cross_val_score(bag, X, Y, cv=RepeatedStratifiedKFold(n_repeats=CV_N_REPEATS))\nprint(\"Accuracy mean={0:0.2f} +\/- {1:0.2f} (1 s.d.)\".format(scores.mean(),scores.std()))\nprint(\"Out of bag score: {0:0.2f}\".format(bag.oob_score_*100) );","d40eb32e":"num_estimators=100\nrf = RandomForestClassifier(n_estimators=num_estimators) \nrf.fit(X_train, y_train)\n\nrf_score=rf.score(X_test, y_test)\nprint(\"Accuracy of Random Forest Classifier: {0:0.2f}\".format(rf_score * 100.0));\n\n#Make Predictions\ny_pred = rf.predict(X_test)\n#Plot the confusion matrix\nconfusion_mat(y_pred, y_test)","61d7d032":"feature_names=X_train.columns.values\nshow_weights(rf,feature_names=feature_names)","1f52850e":"scores = cross_val_score(rf, X, Y, cv=RepeatedStratifiedKFold(n_repeats=CV_N_REPEATS))\nscores_est = scores.copy()\nprint(f\"Scores mean={scores.mean():0.2f} +\/- {scores.std():0.2f} (1 s.d.)\")\n\nscore_line = f\"Scores (Accuracy) mean={scores.mean():0.2f} +\/- {scores.std():0.2f} (1 s.d.)\"\nplt.figure()\nfig, ax = plt.subplots()\npd.Series(scores).hist(ax=ax, bins=BINS)\nax.set_title(f\"RepeatedKFold ({len(scores)} folds) with RandomForest\\n\" + score_line);\nax.set_xlabel(\"Score\")\nax.set_ylabel(\"Frequency\");","1e55ea04":"plt.figure()\nfig, ax = plt.subplots()\ndf_dummy_est_scores = pd.DataFrame({'dummy': scores_dummy, 'RF': scores_est})\ndf_dummy_est_scores.plot(kind='hist', ax=ax, bins=20)\nax.set_xlabel(\"Score\")\nax.set_title(\"Dummy vs RandomForest Scores\");","0c956e43":"from sklearn.metrics import mean_squared_error\nparams={'n_estimators': 500,'learning_rate': 0.01,'max_depth': 4, 'loss':'deviance'}\ngbm=GradientBoostingClassifier(**params)\ngbm.fit(X_train, y_train)","3c32ff36":"# compute test set deviance\ntest_score = np.zeros((params['n_estimators'],), dtype=np.float64)\n\nfor i, y_pred in enumerate(gbm.staged_predict(X_test)):\n    test_score[i] = gbm.loss_(y_test, y_pred)\n\n#plot train and test set deviance against the number of estimators\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.title('GBM Deviance w.r.t Number of Estimators')\nplt.plot(np.arange(params['n_estimators']) + 1, gbm.train_score_, 'b-',label='Training Set Deviance')\nplt.plot(np.arange(params['n_estimators']) + 1, test_score, 'r-',\n         label='Test Set Deviance')\nplt.legend(loc='best')\nplt.xlabel('Boosting Iterations')\nplt.ylabel('Deviance')","1bd87fa2":"params={'n_estimators': 100,'learning_rate': 0.01,'max_depth': 4, 'loss':'deviance'}\ngbm=GradientBoostingClassifier(**params)\ngbm.fit(X_train, y_train)\n\n# make predictions for test data\ny_pred = gbm.predict(X_test)\n\n# evaluate predictions\ngbm_score = accuracy_score(y_test, y_pred)\nprint(\"Accuracy of GBM Classifier: {0:0.2f}\".format(gbm_score * 100.0));\n\n#Plot the confusion matrix\nconfusion_mat(y_pred, y_test)","b4a00636":"feature_importance = gbm.feature_importances_\n# make importances relative to max importance\nfeature_importance = 100.0 * (feature_importance \/ feature_importance.max())\nsorted_idx = np.argsort(feature_importance)\npos = np.arange(sorted_idx.shape[0]) + .5\nplt.subplot(1, 2, 2)\nplt.barh(pos, feature_importance[sorted_idx], align='center')\nplt.yticks(pos, df.columns[sorted_idx])\nplt.xlabel('Relative Importance')\nplt.title('Variable Importance')\nplt.show()","14a31d02":"from xgboost import XGBClassifier, plot_importance,to_graphviz\n\n# fit model on training data\nparam = {'max_depth': 3, 'eta': 0.8, 'subsample':1, 'objective': 'binary:logistic'}\nxgb = XGBClassifier(**param)\nxgb.fit(X_train, y_train)","3448efd0":"# make predictions for test data\ny_pred = xgb.predict(X_test)\n\n# evaluate predictions\nxgb_score = accuracy_score(y_test, y_pred)\nprint(\"Accuracy of XGB Classifier: {0:0.2f}\".format(xgb_score * 100.0));\n\n#Plot the confusion matrix\nconfusion_mat(y_pred, y_test)","16bfcfd8":"# plot feature importance using built-in function\nplt.figure()\nplot_importance(xgb,title=\"Feature importance from XGBoost model\")\nplt.show()","fb4e6b5e":"from sklearn.ensemble import VotingClassifier\n\nensemble_knn_rf_xgb=VotingClassifier(estimators= [('KNN', knn), ('Random Forest', rf),('XGBoost',xgb)], voting='hard')\nensemble_knn_rf_xgb.fit(X_train,y_train)\n\n#compute accuracy\nprint('The ensembled model with all the 3 classifiers is:',ensemble_knn_rf_xgb.score(X_test,y_test))\n\n#make predictions\ny_pred = ensemble_knn_rf_xgb.predict(X_test)\n#Plot the confusion matrix\nconfusion_mat(y_pred, y_test)","c101e8c8":"from mlxtend.classifier import StackingCVClassifier\n\nsclf = StackingCVClassifier(classifiers=[knn, rf, xgb, gbm], \n                          meta_classifier=rf)\n\nprint('10-fold cross validation:\\n')\n\nfor clf, label in zip([knn, rf, xgb, gbm, rf], \n                      ['KNearest Neighbors',\n                       'Random Forest', \n                        'XGB','GBM',\n                       'MetaClassifier']):\n\n    sclf_scores = model_selection.cross_val_score(clf, X, Y,\n                                              cv=10, scoring='accuracy')\n    print(\"Accuracy: %0.2f (+\/- %0.2f) [%s]\" % (sclf_scores.mean(), sclf_scores.std(), label))","b59db4b3":"models = []\n#models.append(('LR', LogisticRegression()))\nmodels.append(('KNN', knn))\nmodels.append(('DT', dt))\nmodels.append(('RF', rf))\nmodels.append(('GBM', gbm))\nmodels.append(('XGB', xgb))\nmodels.append(('Voting',ensemble_knn_rf_xgb))","9d6d7028":"results = []\nnames = []\n\nfor name, model in models:\n    kfold = model_selection.StratifiedKFold(n_splits=10, random_state=7)\n    cv_results = model_selection.cross_val_score(model, X, Y, cv=kfold, scoring='accuracy')\n    results.append(cv_results)\n    names.append(name)\n    msg = \"{}: {} ({})\".format(name, cv_results.mean(), cv_results.std())\n    print(msg)\n    \n#Add stacking results that we got previously\nresults.append(np.asarray(sclf_scores))\nnames.append('Stacking')","5f7e32f2":"# boxplot algorithm comparison\nfig = plt.figure(figsize=(10,6))\nfig.suptitle('Algorithm Comparison')\nax = sns.boxplot(x=names, y=results)\nplt.xlabel('Classifiers')\nplt.ylabel('Accuracy')\nplt.show()","a170d908":"visualizer = ClassificationReport(xgb,classes=['Not Diabetic','Diabetic'])\n\n#visualizer.fit(X_train, y_train)  # Fit the visualizer and the model\nvisualizer.score(X_test, y_test)   # Evaluate the model on the test data\nvisualizer.poof()                  # Draw\/show\/poof the data","49a72b08":"visualizer = ClassificationReport(ensemble_knn_rf_xgb,classes=['Not Diabetic','Diabetic'])\n\nvisualizer.score(X_test, y_test)   # Evaluate the model on the test data\nvisualizer.poof()                  # Draw\/show\/poof the data","d8e73545":"This confirms we have no zero values in our data anymore. We continue exploring the distribution of the attributes and the relation between our attributes. Below is a correlation plot from which we gauge into the linear relationship between our attributes.","71cbed7e":"Let's see how the model deviance performs w.r.t the number of estimators. Deviance is the logistic loss function used in all implementation of GBM and is the default loss function for classification problems. ","85b0deee":"We first check if the data is shuffled. We can do that by plotting the Outcome against the dataframe index (in x-axis).","0f2dde32":"### GradientBoostingClassifier (Sklearn)\nThis is an ensemble model based on the boosting paradigm, i.e. sequential model building using several weak classifiers. We start with 500 estimators and a decision tree classifier of depth 4 as our weak learner. ","72cbc5b3":"From the random forest model, we have 35 incorrectly labeled samples. In medical data analaysis, as it is usually the case, we are more concerned about the False Negatives (or Misses), i.e. diabetic samples who have been incorrectly labelled as non-diabetic. This model results in 15 such cases. \n\nWe use the eli5 library to analyse which are the most important features for our learned RF model","facc4874":"How about plotting the first 3 principal components in a 3D plot and check how well our data is classified. These 3 components together explain 61% of the variance in the data.","cbaac556":" Every algorithm is tested and results are collected and printed. We then visualise the variation in the predictions of each algorithm using a boxplot.","97398a90":"### Plot decision tree to visualize the splittling rules","049564a5":"Check the variance of our DummyClassifier's predictions","2c4dd411":"As we can see, the two classes have not been well segregated even after projecting the data into new dimensions using PCA. So I infer that using the PCA-transformed data for further model building won't help much in improving classification performance. We will continue to use the original data.","1f632906":"Along the diagonal we have distributions of the attributes for the two classes (Diabetic\/Non-Diabetic). These plots confirm that no attribute clearly separates the two distributions. \n\nIn this dataset, we have a total of 768 samples spread in 8 dimensions (8 attributes). That's a huge dimension space for such few samples. So we'll next try to reduce the dimensionality of our data into their principal components and see if the two classes can be segregated. We use PCA for dimensionality reduction.\n\n### Principal Component Analysis (PCA)\nPCA is used to decompose a multivariate dataset in a set of successive orthogonal components that explain a maximum amount of the variance.","5846e9b1":"As expected, Glucose turns out to be the most important feature for our RF model. This affirms our intuition formed during the data exploration phase. \n#### Check variance in RF prediction quality\n","0d45135a":"## Stacking\nStacking is a way of combining multiple models, that introduces the concept of a meta learner. It is less widely used than bagging and boosting. Unlike bagging and boosting, stacking may be (and normally is) used to combine models of different types.\n\nThe point of stacking is to explore a space of different models for the same problem. The idea is that you can attack a learning problem with different types of models which are capable to learn some part of the problem, but not the whole space of the problem. So you can build multiple different learners and you use them to build an intermediate prediction, one prediction for each learned model. Then you add a new model which learns from the intermediate predictions the same target. This final model is said to be stacked on the top of the others, hence the name. Thus you might improve your overall performance, and often you end up with a model which is better than any individual intermediate model.","db128acd":"To fill these Nan values the data distribution needs to be understood.","9867199e":"We check the distribution of the 8 attributes against the target variable Outcome.  ","77b8f9fd":"### Random Forest","c4328198":"## Summarize results\nWe run the algorithms once again using StratifiedK-fold cross-validation and summarize our findings. ","7099c94a":" We compare the prediction performance of our random forest classifier against the Dummy classifier. ","f1c71a53":"This model is an optimized variant of the Gradient boosting models, which at its core does the same work as the previous Gradient Boosting machine does. The difference is that XgBoost algorithm is developed with both deep consideration in terms of systems optimization and principles in machine learning. The goal of the library is to push the extreme of the computation limits of machines to provide a scalable, portable and accurate library. ","5dc873a7":"We obtained a best on average classification accuracy of approximately 77% using XgBoost and the Voting Classifier. We noted earlier from the confusion matrix of XGboost classifier that we there are 26 misclassified samples with 12 misses. One can further analyse those misclassified samples to better understand the model behaviorb. This is an extension that is not part of this notebook.\n\nWe end the analysis with classification report of our two best performing models.The classification report shows a representation of the main classification metrics on a per-class basis. This gives a deeper intuition of the classifier behavior over global accuracy which can mask functional weaknesses in one class of a multiclass problem. Visual classification reports are used to compare classification models to select models that are \u201credder\u201d, e.g. have stronger classification metrics or that are more balanced.\n\nPrecision is the ability of a classiifer not to label an instance positive that is actually negative. For each class it is defined as as the ratio of true positives to the sum of true and false positives. \n\nRecall is the ability of a classifier to find all positive instances. For each class it is defined as the ratio of true positives to the sum of true positives and false negatives.\n\nThe F1 score is a weighted harmonic mean of precision and recall such that the best score is 1.0 and the worst is 0.0.\n\nBased on the weighted F-1 score from the below two reports, XgBoost is a better classifier. So was the case when we considered number of False negatives samples. ","ab19cdd0":"## EDA and Data Cleaning","cbbfd911":"XGBoost in most cases performs better than GBM.  As we can see here the classification accuracy has increased to 83.12% than compared to GBM's 74%, but the Misses have increased from 7 in GBM to 12 with Xgboost model.  \n\nNext we plot Feature Importance based on the Xgboost model. From the feature importance graphs for all models plotted until now, we see Glucose is the most important feature. Importance order for the other features is more or less same.","075ff581":"As we can see from the plot above that the test set deviance plateaus at around 1.3 at about 100 estimators. Having more estimators would not help in predictions, in fact it would lead model to overfit the training data.\nWe set the number of estimators to 100 and retrain our GBM. We also plot the confusion matrix to check for errors. As can be seen, GBM does a good job at reducing the False Positives to 7 from 12 in the random forest model, though the number of false detections has increased to 33., False detections are samples that are not diabetic but have been classified as diabetic.","6b0fd487":"From the above plot, we notice that no particular attribute clearly differentiates the two classes, though with attribute Glucose, the two classes seem to have a noticeable difference in their samples distribution. This variable could serve as a good differentiatior. \n\nWe also note that the dataset contains samples with improbable values, such as samples which have zeros for the following attributes: BMI, Glucose, SkinThickness,Insulin,and BloodPressure. That does not make sense. Clearly, there might have been an error while recording the data or some data values might not have been recorded at all and instead filled with zeros. \n\nNext, we clean the data using basic techniques. We are not using any specialised method for data cleaning primarily because that is not the focus of this notebook. \n\n#### Data Cleaning\nFirst we replace zeros with nan since after that counting them would be easier and zeros need to be replaced with suitable values.","9b307905":"### Bagging Classifier\nThis classifier fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. \nSuch a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it.","707b4014":"Starting with a dummy classifier, we can confirm that we have made some progress in the right direction to predict for onset of diabetes. ","eff4a973":"The above plot confirms that the dataframe rows are shuffled. That is good. We next check for class imbalance. As can be seen from the below figure, there's fewer samples with diabeties than the ones not having diabetes. To tackle this imbalance, we would make use of Stratified sampling during the model building phase.","115ae976":"#### Decision tree \n\nWe next train a decision tree classifier and check the accuracy results. ","2c04746c":"## Import Libraries and Load  data","0ea7651d":"#### K-Nearest Neighbors","f0603fa4":"#### XgBoost","9318d0a9":"Pregnancies and Age are positively correlated, so are BMI and SkinThickness. As for the target variable, Glucose seems to be most linearly correlated with it. To further understand the bivariate relationships of the data, we draw pairplots.","098f6be7":"# Pima Indians Onset of Diabetes Prediction\n\nThe objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on the following 8 diagnostic measurements.\n\n    Pregnancies: Number of times pregnant\n    GlucosePlasma: glucose concentration a 2 hours in an oral glucose tolerance test\n    BloodPressure: Diastolic blood pressure (mm Hg)\n    SkinThicknessTriceps: skin fold thickness (mm)\n    Insulin: 2-Hour serum insulin (mu U\/ml)\n    BMI: Body mass index (weight in kg\/(height in m)^2)\n    DiabetesPedigreeFunction: Diabetes pedigree function\n    Age: Age (years)\n\nThe dataset includes data from 768 women beyond the age of 21. The target variable:\n\n    Outcome: Class variable (0 or 1), where 1 corresponds to having diabetes. \n\n### Goals of this notebook\n\n* Look deeper into various ensembling methods and compare their performance.\n\n**Classifiers used:**\n    * Dummy Classifiers\n    * KNN\n    * Decision Tree \n    * Bagging Classifier\n    * Random Forests\n    * Sklearn's GBM\n    * XgBoostClassifier \n    * Voting classifier\n    * Stacking Classifier\n    \n \n ### Notebook Structure\n     * Import Libraries and Load data\n     * Perform Exploratory Data Analysis and Data Cleaning\n         -Here we will also look into Principal Component Analysis\n     * Build ensemble Models \n     * Compare the different ensemble models","fd4dbfe7":"## Model Building\nIn this section, we build several machine learning models and later analyse their results.\n#### DummyClassifier\nWe start with the most basic, a dummy classifier which predicts the most frequent class at all times. This would serve as our baseline. \nSplit X,y into train and test sets; We use the original data instead of the PCA transformed data.","3a57b4ad":"#### Voting Classifier\nThe idea behind the VotingClassifier is to combine conceptually different machine learning classifiers and use a majority vote or the average predicted probabilities (soft vote) to predict the class labels. Such a classifier can be useful for a set of equally well performing model in order to balance out their individual weaknesses.\n\nWe create a voting classifier using three models: KNN, Random Forest, and XGBoost model. Results don't show any improvement over the three models. ","a6a1afcf":"Plot Feature Importance for the GBM model","cc240807":"Notice that we need 7 principal components to explain 90% of the variance in the data. Plotting the first two principal components below, which together explain approximately 47% of the total variance, we observe that the two classes have been differentiated to some extent but not enough.  "}}