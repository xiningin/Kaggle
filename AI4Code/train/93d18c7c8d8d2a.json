{"cell_type":{"d9e6c3a4":"code","2541a3ee":"code","a857a254":"code","3e3254db":"code","28539cea":"code","100694aa":"code","efbb5fb7":"code","ed27a3ef":"code","bd6c591d":"code","276854fe":"code","ad360220":"code","628854da":"code","c625a06a":"code","0d4add37":"code","b95da505":"code","87ce2956":"code","6ecd62ab":"code","c00085b7":"code","7e9e889d":"code","eb790613":"code","9b6689bc":"code","fc404bd7":"code","830d1f38":"code","6a88851a":"code","c6f7e462":"code","b173e355":"code","a6865829":"code","896dba33":"code","9ad72953":"code","f93aebdc":"code","4e55139e":"code","502676ce":"code","3585f6dd":"code","90a1745b":"code","68c2c390":"code","3c84cfe1":"code","8ae194f3":"code","2b1a3676":"code","0a62bf7a":"code","150891ea":"code","005277f2":"code","c1cd2ca5":"code","f3432997":"code","7fee8b52":"code","3252c89a":"code","3f03da5c":"code","753ddaf4":"code","e0d1cc3e":"code","4549098f":"code","8aa85c77":"code","099c1253":"code","7ee3d29c":"code","31bbd588":"code","30335256":"markdown","7af5d406":"markdown","781d4bc7":"markdown","e9867440":"markdown","b4053a3e":"markdown","f301bfa0":"markdown","f008aac8":"markdown","43f9c82f":"markdown","8e1f8b75":"markdown","c3b165ad":"markdown","856ff081":"markdown","fc2c2b68":"markdown","06f21b2b":"markdown","359dcb68":"markdown","028d87f7":"markdown","16f2f834":"markdown","f0313594":"markdown","6b8a73f8":"markdown","2fb77a3b":"markdown","72bbbeaf":"markdown","12f1ea1e":"markdown","91e876a0":"markdown","8156c248":"markdown"},"source":{"d9e6c3a4":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","2541a3ee":"train = pd.read_csv(\"..\/input\/used-electronics-price-prediction\/Train.csv\")\ntrain.head()","a857a254":"train.shape","3e3254db":"train.columns","28539cea":"train.isnull().sum()","100694aa":"train.dtypes","efbb5fb7":"train.describe()","ed27a3ef":"plt.figure(figsize=(14,5))\n\nplt.subplot(1,3,1)\ntrain.Brand.value_counts().plot(kind='bar', label = 'Brand')\nplt.legend()\n\nplt.subplot(1,3,2)\ntrain.City.value_counts().plot(kind='bar', label = 'City')\nplt.legend()\n\nplt.subplot(1,3,3)\ntrain.State.value_counts().plot(kind='bar', label = 'State')\nplt.legend()\n\n","bd6c591d":"train.Price.plot(kind='hist', color=['teal'], label=\"Price\")\nplt.legend()","276854fe":"def device_memory_category(x):\n    if (x.find(\"16gb\") != -1) or (x.find(\"16 gb\") != -1):\n        return 1\n    elif(x.find(\"32gb\") != -1) or (x.find(\"32 gb\") != -1):\n        return 2\n    elif(x.find(\"64gb\") != -1) or (x.find(\"64 gb\") != -1):\n        return 3\n    elif(x.find(\"128gb\") != -1) or (x.find(\"128 gb\") != -1):\n        return 4\n    elif(x.find(\"256gb\") != -1) or (x.find(\"256 gb\") != -1):\n        return 1\n    else:\n        return 0\n    \ndef if_iphopne_or_ipad(x):\n    if (x.find(\"iphone\") != -1) or (x.find(\"ipad\") != -1):\n        return 1\n    else:\n        return 0\n    \ndef device_condition(x):\n    if (x.find(\"good\") != -1) or (x.find(\"great\") != -1) or (x.find(\"excellent\") != -1) or (x.find(\"new\") != -1) \\\n    or (x.find(\"mint\") != -1):\n        return 1\n    else:\n        return 0\n    \ndef under_warranty(x):\n    if (x.find(\"billbox\") != -1) or (x.find(\"warranty\") != -1) or (x.find(\"boxbill\") != -1) or (x.find(\"box\") != -1) \\\n    or (x.find(\"bill box\") != -1):\n        return 1\n    else:\n        return 0\n\n","ad360220":"train[\"device_memory\"] = train[\"Model_Info\"].apply(lambda x: device_memory_category(x))\ntrain[\"phone_status\"] = train[\"Model_Info\"].apply(lambda x: if_iphopne_or_ipad(x))\ntrain[\"device_condition\"] = train[\"Model_Info\"].apply(lambda x: device_condition(x))\ntrain[\"warranty_status\"] = train[\"Additional_Description\"].apply(lambda x: under_warranty(x))","628854da":"train.head()","c625a06a":"plt.figure(figsize=(14,5))\n\nplt.subplot(2,2,1)\ntrain.device_memory.value_counts().plot(kind='bar', label = 'device_memory')\nplt.legend()\n\nplt.subplot(2,2,2)\ntrain.phone_status.value_counts().plot(kind='bar', label = 'phone_status')\nplt.legend()\n\nplt.subplot(2,2,3)\ntrain.device_condition.value_counts().plot(kind='bar', label = 'device_condition')\nplt.legend()\n\nplt.subplot(2,2,4)\ntrain.warranty_status.value_counts().plot(kind='bar', label = 'warranty_status')\nplt.legend()\n","0d4add37":"train.head()","b95da505":"train.columns","87ce2956":"train = pd.get_dummies(data=train, columns=['City','State','device_memory'], \n               prefix=['City', 'State', 'Device_memory'], drop_first=True)","6ecd62ab":"train['City_5'] = 0\ntrain['City_6'] = 0\ntrain['City_18'] = 0 \ntrain['City_19'] = 0","c00085b7":"train.shape","7e9e889d":"train.columns","eb790613":"train = train[['Brand', 'Model_Info', 'Additional_Description', 'Locality', 'Price',\n       'phone_status', 'device_condition', 'warranty_status', 'City_1',\n       'City_2', 'City_3', 'City_4', 'City_5', 'City_6', 'City_7', 'City_8', 'City_9', 'City_10',\n       'City_11', 'City_12', 'City_13', 'City_14', 'City_15', 'City_16',\n       'City_17', 'City_18', 'City_19', 'State_1', 'State_2', 'State_3', 'State_4', 'State_5',\n       'State_6', 'State_7', 'State_8', 'Device_memory_1', 'Device_memory_2',\n       'Device_memory_3', 'Device_memory_4']]","9b6689bc":"train.reset_index(drop=True, inplace=True)","fc404bd7":"x = train.drop(['Price', 'Model_Info', 'Additional_Description'], axis=1)\ny = train['Price']","830d1f38":"x_copy = x.copy()","6a88851a":"# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 100)","c6f7e462":"from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\n","b173e355":"import xgboost as xgb\nimport lightgbm as lgb","a6865829":"from sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import roc_curve, auc, classification_report, confusion_matrix, accuracy_score, roc_auc_score\nfrom sklearn.metrics import classification_report\n","896dba33":"#Define a cross validation strategy\n\n#We use the cross_val_score function of Sklearn. However this function has not a shuffle attribut, we add then one line of code, in order to shuffle the dataset prior to cross-validation\n\n#Validation function\nn_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n    rmse= np.sqrt(-cross_val_score(model, X_train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","9ad72953":"#Base models\n\n#LASSO Regression :\n#This model may be very sensitive to outliers. So we need to made it more robust on them. \n#For that we use the sklearn's Robustscaler() method on pipeline\n\nlasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))\n\n#Elastic Net Regression : again made robust to outliers\n\nENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\n\n#Kernel Ridge Regression :\n\nKRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\n\n#Gradient Boosting Regression : With huber loss that makes it robust to outliers\n\nGBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)","f93aebdc":"# XGBoost\nmodel_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)\n\n# LightGBM\nmodel_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)","4e55139e":"#Base models scores\n#Let's see how these base models perform on the data by evaluating the cross-validation rmsle error\n\nscore = rmsle_cv(lasso)\nprint(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n\nscore = rmsle_cv(ENet)\nprint(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n\nscore = rmsle_cv(KRR)\nprint(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n\nscore = rmsle_cv(GBoost)\nprint(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n\n","502676ce":"score = rmsle_cv(model_xgb)\nprint(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n\n\nscore = rmsle_cv(model_lgb)\nprint(\"LGBM score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))","3585f6dd":"class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    # we define clones of the original models to fit the data in\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        # Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n\n        return self\n    \n    #Now we do the predictions for cloned models and average them\n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1) ","90a1745b":"averaged_models = AveragingModels(models = (ENet, GBoost, KRR, lasso))\n\nscore = rmsle_cv(averaged_models)\nprint(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","68c2c390":"model = averaged_models.fit(X_train, y_train)\npredictions = model.predict(X_test)","3c84cfe1":"mean_squared_error(predictions, y_test)","8ae194f3":"#On Full Data\n\nmodel_full = averaged_models.fit(x, y)","2b1a3676":"# Lightgbm on Full Data\n\nlightgbm_full = model_lgb.fit(x, y)","0a62bf7a":"# XGBoost on Full Data\n\nmodel_xgb_full = model_xgb.fit(x, y)","150891ea":"test = pd.read_csv('..\/input\/used-electronics-price-prediction\/Test.csv')\n\ntest.shape","005277f2":"test.head(5)","c1cd2ca5":"test[\"device_memory\"] = test[\"Model_Info\"].apply(lambda x: device_memory_category(x))\ntest[\"phone_status\"] = test[\"Model_Info\"].apply(lambda x: if_iphopne_or_ipad(x))\ntest[\"device_condition\"] = test[\"Model_Info\"].apply(lambda x: device_condition(x))\ntest[\"warranty_status\"] = test[\"Additional_Description\"].apply(lambda x: under_warranty(x))","f3432997":"test = pd.get_dummies(data=test, columns=['City','State','device_memory'], \n               prefix=['City', 'State', 'Device_memory'], \n               drop_first=True)","7fee8b52":"test.columns","3252c89a":"test['City_3'] = 0\ntest['City_7'] = 0\ntest['City_9'] = 0\ntest['City_14'] = 0\ntest['City_16'] = 0\ntest['State_8'] = 0","3f03da5c":"test_for_prediction = test[['Brand', 'Locality',\n       'phone_status', 'device_condition', 'warranty_status', 'City_1',\n       'City_2', 'City_3', 'City_4', 'City_5', 'City_6', 'City_7', 'City_8', 'City_9', 'City_10',\n       'City_11', 'City_12', 'City_13', 'City_14', 'City_15', 'City_16',\n       'City_17', 'City_18', 'City_19', 'State_1', 'State_2', 'State_3', 'State_4', 'State_5',\n       'State_6', 'State_7', 'State_8', 'Device_memory_1', 'Device_memory_2',\n       'Device_memory_3', 'Device_memory_4']]","753ddaf4":"test_for_prediction.head()","e0d1cc3e":"def predict_file(model, model_instance, test_data):\n    file_name = \"Final_output_prediction_from_\" + model + \".xlsx\"\n    predictions  = model_instance.predict(test_data)\n    df_prediction_var = pd.DataFrame(predictions, columns=[\"Price\"])\n    df_prediction_var.to_excel(file_name)\n    print(\"{} created.\".format(file_name))","4549098f":"predict_file(\"stacked_model\", model_full, test_for_prediction)","8aa85c77":"predict_file(\"lightgbm_model\", lightgbm_full, test_for_prediction)","099c1253":"predict_file(\"xgboost_model\", model_xgb_full, test_for_prediction)\n","7ee3d29c":"stacked_pred = model_full.predict(test_for_prediction.values)\n\nxgb_pred = model_xgb_full.predict(test_for_prediction)\n\nlgb_pred = lightgbm_full.predict(test_for_prediction.values)\n\nensemble = stacked_pred*0.70 + xgb_pred*0.15 + lgb_pred*0.15","31bbd588":"ensemble_sub = pd.DataFrame()\nensemble_sub['Price'] = ensemble\nensemble_sub.to_excel('ensemble_submission.xlsx',index=False)","30335256":"## 4.2 Split Data","7af5d406":"## 2.1 Missing Data Analysis ","781d4bc7":"### 6.2.1 Averaged base models class","e9867440":"## 2.2 Data Type Analysis ","b4053a3e":"# Phase2: Applying Model On Test Data","f301bfa0":"# Step5: Creating Train and Test Set In Ratio 80:20","f008aac8":"# Phase1: Model Building On Training Data","43f9c82f":"# Step2: Exploratory Data Analysis","8e1f8b75":"Averaged base models score\n\nWe just average four models here ENet, GBoost, KRR and lasso. Of course we could easily add more models in the mix","c3b165ad":"## 3.1 Dummy Variable Creation","856ff081":"## 2.3 Univariate Analysis\n\nAt this stage, we explore variables one by one. Method to perform uni-variate analysis will depend on whether the variable type is categorical or continuous. Let\u2019s look at these methods and statistical measures for categorical and continuous variables individually:\n\n<b> Continuous Variables:- <\/b> In case of continuous variables, we need to understand the central tendency and spread of the variable. These are measured using various statistical metrics such as Histogram and Bar plots: ","fc2c2b68":"## 4.1 Re-setting Index Before Splitting","06f21b2b":"We live in a world that is driven by technology and electronic devices as gadgets have become a part of our daily life. It is near impossible to think of a world without smartphones or tablets. Like many kinds of goods or products, used electronic devices have a good demand in our country. In this hackathon, we challenge the data science community to predict the price of used electronic devices based on certain factors.\n\nGiven are 6 distinguishing factors that can influence the price of a used device. Your objective as a data scientist is to build a machine learning model that can predict the price of used electronic devices based on the given factors.\n\nData Description:-\nThe unzipped folder will have the following files.\n\nTrain.csv \u2013  2326 observations. <br>\nTest.csv \u2013  997 observations. <br>\nSample Submission \u2013 Sample format for the submission. <br>\n\nTarget Variable: Price","359dcb68":"### 2.3.5 Target Variable Plot","028d87f7":"# Step1: Read Data","16f2f834":"# Step6: Model Building","f0313594":"# Step4: Separating X and Y","6b8a73f8":"### 2.3.1 Discrete Variables Plot","2fb77a3b":"## Stacking models\n#### Simplest Stacking approach : Averaging base models\n\nWe begin with this simple approach of averaging base models. We build a new class to extend scikit-learn with our model and also to laverage encapsulation and code reuse (inheritance)\n\n### Averaged base models class","72bbbeaf":"## 6.2 Importing and Model Fitting","12f1ea1e":"# Problem Statement","91e876a0":"# Step3: Feature Engineering","8156c248":"Below are the steps involved to understand, clean and prepare your data for building your predictive model:\n\n1. Variable Identification\n2. Univariate Analysis\n3. Bi-variate Analysis\n4. Missing values treatment\n5. Outlier treatment\n6. Variable transformation\n7. Variable creation"}}