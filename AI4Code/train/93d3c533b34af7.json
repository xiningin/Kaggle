{"cell_type":{"55b76528":"code","4a1f6992":"code","557b5708":"code","6ec3517a":"code","a1c6e671":"code","b305b04c":"code","22fa1279":"code","b8fc0e4e":"code","4ea94b14":"code","82c21c1e":"code","af6e7641":"code","d4d5a8ac":"code","e606dfc4":"code","829c9325":"code","0eb65a23":"code","1e07a863":"code","66ee6138":"code","dd89c7ef":"code","a5ee616d":"code","a9c24706":"code","dd52ae67":"code","9df4f0ae":"code","081400e9":"code","8a2da341":"code","83779d0d":"code","6b5553e0":"code","df0740f6":"code","edc7eebf":"code","6fe2115d":"code","5b2ea95e":"code","1512ef49":"code","6452da18":"code","bd652092":"code","f4afcdc5":"code","db5bedfb":"code","cf668c3b":"code","d3c21c90":"code","ab217ad4":"code","75a73614":"code","ca49fd8e":"code","55f2c26b":"code","ba18b4d7":"code","473b2952":"code","cc49a340":"code","677c5d60":"code","122189d7":"code","4c398600":"code","77fe5ccd":"code","de92bac1":"code","0939838f":"code","7842d92e":"code","082c1071":"code","95358674":"code","ea2e850b":"code","f0a9b121":"code","8e7112fa":"code","f78f6a0c":"code","457f7f3a":"code","55df60da":"code","6d504249":"code","a2fd582a":"code","18378c0d":"code","b09bbdf1":"code","fc723242":"code","6fb146cf":"code","97cf769f":"code","6ed58e33":"code","4b07e79b":"markdown","613f8d76":"markdown","ee6ac7cd":"markdown","b6c825f3":"markdown","5ca4ed94":"markdown","5929ee1d":"markdown","4087b730":"markdown","e0f453e1":"markdown","c9fa6b11":"markdown","ee3f45f3":"markdown","f6a68e82":"markdown","ac1ab2d2":"markdown","03ac5e95":"markdown","6d844639":"markdown","add59c2d":"markdown","9486e0eb":"markdown","3c710e79":"markdown"},"source":{"55b76528":"# ! mkdir tpu_setup\n# %cd tpu_setup\n# !curl https:\/\/raw.githubusercontent.com\/pytorch\/xla\/master\/contrib\/scripts\/env-setup.py -o pytorch-xla-env-setup.py\n# !python pytorch-xla-env-setup.py --version 1.7 --apt-packages libomp5 libopenblas-dev\n# %cd ..\n# ! rm -r tpu_setup\n\n! pip install timm","4a1f6992":"import os, imp, time, copy\nimport numpy as np\nimport pandas as pd\nfrom collections import OrderedDict\nimport gc\n\nimport torch, torchvision\nfrom timm import models as timm_models\nfrom torch import nn, optim\nfrom torchvision import transforms, datasets \nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nimport albumentations as A","557b5708":"import matplotlib.pyplot as plt\nplt.rcParams.update({'font.size': 16})","6ec3517a":"if \"TPU_NAME\" in os.environ:\n    import torch_xla\n    import torch_xla.core.xla_model as xm","a1c6e671":"device = torch.device(\"cpu\")\nif torch.cuda.is_available():\n    device = torch.device(\"cuda:0\")\nelif \"TPU_NAME\" in os.environ:\n    device = xm.xla_device()\nprint(device.type, \" is available\")\ndevice ","b305b04c":"class DoubleConv(torch.nn.Module):\n    \"\"\" M\u1ed7i c\u1ee5m c\u00f3 2 conv ch\u00ednh \u0111\u1ec3 h\u1ecdc, c\u00f9ng v\u1edbi m\u1ed7i conv ch\u00ednh l\u00e0 1 batch norm \u0111\u1ec3 \u0111\u1ea3m b\u1ea3o kh\u00f4ng b\u1ecb overfitting v\u00e0 1 active relu\n    \"\"\"\n    \n    def __init__(self, in_channels, out_channels, mid_channels=None): \n        super().__init__()\n        if mid_channels is None:\n            mid_channels = out_channels\n        \n        self.double_conv = nn.Sequential(\n            nn.Conv2d(in_channels, mid_channels, kernel_size=3, stride=1, padding=1, padding_mode='replicate', bias=False),\n            nn.BatchNorm2d(mid_channels),\n            nn.ReLU(inplace=True),\n            \n            nn.Conv2d(mid_channels, out_channels, kernel_size=3, stride=1, padding=1, padding_mode='replicate', bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n    def forward(self, x):\n        return self.double_conv(x)\n\n\nclass Down(nn.Module):\n    \"\"\"Gi\u1ea3m s\u1ed1 chi\u1ec1u c\u1ee7a \u1ea3nh v\u00e0 thay \u0111\u1ed5i s\u1ed1 k\u00eanh c\u1ee7a \u1ea3nh \u0111\u1ea7u ra th\u00f4ng qua DoubleConv\"\"\"\n\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.maxpool_conv = nn.Sequential(\n            nn.MaxPool2d(2),  # Gi\u1ea3m m\u1ed9t n\u1eeda k\u00edch th\u01b0\u1edbc, gi\u1eef nguy\u00ean s\u1ed1 k\u00eanh\n            DoubleConv(in_channels, out_channels)\n        )\n\n    def forward(self, x):\n        return self.maxpool_conv(x)\n\n\nclass Up(nn.Module):\n    \"\"\"T\u0103ng s\u1ed1 chi\u1ec1u c\u1ee7a \u1ea3nh l\u00ean g\u1ea5p \u0111\u00f4i v\u00e0 thay \u0111\u1ed5i s\u1ed1 k\u00eanh c\u1ee7a \u1ea3nh \u0111\u1ea7u ra th\u00f4ng qua DoubleConv\"\"\"\n\n    def __init__(self, in_channels, skip_conn_in_channels, out_channels, mid_channels=None):\n        super().__init__()\n        self.up = nn.ConvTranspose2d(in_channels, in_channels \/\/ 2, kernel_size=2, stride=2, padding=0)  # upsample v\u00e0 gi\u1ea3m layer\n        self.conv = DoubleConv(skip_conn_in_channels + in_channels \/\/ 2, out_channels, mid_channels)\n\n    def forward(self, x_up, x_skip):\n        # B1: Upsample x_dp\n        x_up = self.up(x_up)\n        # Khi downpolling v\u00e0 conv \u1ea3nh c\u00f3 th\u1ec3 gi\u1ea3m s\u1ed1 k\u00edch th\u01b0\u1edbc 1 v\u00e0i pixel m\u00e0 ta kh\u00f3 n\u1eafm b\u1eaft \u0111\u01b0\u1ee3c, qu\u00e1 tr\u00ecnh d\u01b0\u1edbi \u0111\u00e2y s\u1eed dung padding \u0111\u1ec3 b\u00f9 \u0111\u1eafp l\u1ea1i m\u1ea5t m\u00e1t \u0111\u00f3   \n        # S\u1ef1 kh\u00e1c bi\u1ec7t g\u1eefa k\u00edch th\u01b0\u1edbc g\u1ed1c khi down v\u00e0 k\u00edch th\u01b0\u1edbc khi up\n        diffY = x_skip.size()[2] - x_up.size()[2] \n        diffX = x_skip.size()[3] - x_up.size()[3]\n        # T\u1ea1o padding bao quanh \u1ea3nh\n        x_up = nn.functional.pad(x_up, [diffX \/\/ 2, diffX - diffX \/\/ 2,  # left, right\n                        diffY \/\/ 2, diffY - diffY \/\/ 2])  # top, bottom\n        \n        # B2: H\u1ee3p nh\u1ea5t 2 \u1ea3nh x, x_dp\n        x = torch.cat([x_skip, x_up], dim=1)\n        \n        # B3: DoubleConv x\n        return self.conv(x)\n\n\nclass OutConv(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(OutConv, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n\n    def forward(self, x):\n        return self.conv(x)","22fa1279":"class UNet_backbone(nn.Module):\n    def __init__(self, n_channels, n_classes, backbone=\"seresnet50\", pretrained=False):\n        self.n_channels = n_channels\n        self.n_classes = n_classes\n        self.backbone_model = backbone\n        \n        super(UNet_backbone, self).__init__()\n\n        backbone = None\n        if self.backbone_model == \"seresnet50\":\n            backbone = timm_models.seresnet50(pretrained=pretrained)\n            channels_list = [2048, 1024, 512, 256, 64, 3]\n        elif self.backbone_model == \"seresnext50_32x4d\":\n            backbone = timm_models.seresnext50_32x4d(pretrained=pretrained)\n            channels_list = [2048, 1024, 512, 256, 64, 3]\n        elif self.backbone_model == \"senet154\":\n            backbone = timm_models.senet154(pretrained=pretrained)\n            channels_list = [2048, 1024, 512, 256, 128, 3]\n        else:\n            self.backbone_model = \"none\"\n            channels_list = [2048, 1024, 512, 256, 128, 64]\n            \n\n        # input torch.Size([5, 1, 512, 512])\n#         self.dropout_1 = nn.Dropout2d(p=0.1)\n#         self.dropout_2 = nn.Dropout2d(p=0.2)\n\n        if self.backbone_model == \"none\":\n            self.inc = DoubleConv(n_channels, 64)\n            self.down1 = Down(64, 128)\n            self.down2 = Down(128, 256)\n            self.down3 = Down(256, 512)\n            self.down4 = Down(512, 1024)\n            self.down5 = Down(1024, 2048)\n        else:\n            self.inc = nn.Sequential(OrderedDict([\n            ('conv0', nn.Conv2d(n_channels, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)),\n            ('bn0', nn.BatchNorm2d(3)),\n            ('sigmoid0', nn.Sigmoid())\n            ]))  # torch.Size([5, 3, 512, 512])\n            self.down1 = nn.Sequential(OrderedDict([\n            ('conv1', backbone.conv1),\n            ('bn1', backbone.bn1),\n            ('act1', backbone.act1)\n            ]))  # torch.Size([5, 64, 256, 256])\n            self.down2 = nn.Sequential(OrderedDict([\n            ('maxpool2', backbone.maxpool),\n            ('layer2', backbone.layer1)\n            ]))  # torch.Size([5, 256, 128, 128])\n            self.down3 = backbone.layer2  # torch.Size([5, 512, 64, 64])\n            self.down4 = backbone.layer3  # torch.Size([5, 1024, 32, 32])\n            self.down5 = backbone.layer4  # torch.Size([5, 2048, 16, 16])\n        \n        self.up1 = Up(channels_list[0], channels_list[1], channels_list[1], channels_list[0]\/\/2)  # torch.Size([5, 1024, 32, 32])\n        self.up2 = Up(channels_list[1], channels_list[2], channels_list[2], channels_list[1]\/\/2)  # torch.Size([5, 512, 64, 64])\n        self.up3 = Up(channels_list[2], channels_list[3], channels_list[3], channels_list[2]\/\/2)  # torch.Size([5, 256, 128, 128])\n        self.up4 = Up(channels_list[3], channels_list[4], channels_list[4], channels_list[3]\/\/2)  # torch.Size([5, 64, 256, 256])\n        self.up5 = Up(channels_list[4], channels_list[5], channels_list[5], channels_list[4]\/\/2)  # torch.Size([5, 3, 512, 512])\n        \n        self.outc = nn.Conv2d(channels_list[5], n_classes, kernel_size=1)  # torch.Size([5, 1, 512, 512])\n        self.sigmoid = nn.Sigmoid()\n\n        self.backbone = nn.Sequential(\n            self.down1,\n            self.down2,\n            self.down3,\n            self.down4,\n            self.down5\n        )\n    \n    def set_backbone_grad(self, is_grad):\n        for param in self.backbone.parameters():\n            param.requires_grad = is_grad\n\n    def forward(self, x):        \n        x0 = self.inc(x)\n        x1 = self.down1(x0)\n        x2 = self.down2(x1)\n        x3 = self.down3(x2)\n        x4 = self.down4(x3)\n        x5 = self.down5(x4)\n        \n        x = self.up1(x5, x4)  # torch.Size([5, 1024, 32, 32])\n        x = self.up2(x, x3)  # torch.Size([5, 512, 64, 64])\n        x = self.up3(x, x2)  # torch.Size([5, 256, 128, 128])\n        x = self.up4(x, x1)  # torch.Size([5, 64, 256, 256])\n        x = self.up5(x, x0)  # torch.Size([5, 3, 512, 512])\n        \n        out = self.outc(x)  # torch.Size([5, 1, 512, 512])\n        out = self.sigmoid(out)  # sigmoid ouput\n        \n        return out","b8fc0e4e":"# # import re\n# backbone = timm_models.seresnet50()\n# print([s[4:] for s in re.findall(\"\\n  \\(\\w+\", backbone.__str__())])\n\n# imgs, masks = next(iter(test_hm_loader))\n# t_imgs = X3_channel(dim=1)(imgs)\n# t_imgs.size()","4ea94b14":"# outputs = backbone.conv1(t_imgs)\n# print(outputs.size())\n# outputs = backbone.bn1(outputs)\n# print(outputs.size())\n# outputs = backbone.act1(outputs)\n# print(outputs.size())\n# outputs = backbone.maxpool(outputs)\n# print(outputs.size())\n# outputs = backbone.layer1(outputs)\n# print(outputs.size())\n# outputs = backbone.layer2(outputs)\n# print(outputs.size())\n# outputs = backbone.layer3(outputs)\n# print(outputs.size())\n# outputs = backbone.layer4(outputs)\n# print(outputs.size())","82c21c1e":"def train_model(model, train_loader, valid_loader, loss_f, optimizer, device='cpu', scheduler=None, num_epochs=25):\n    since = time.time()\n\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_epoch = 1\n    best_loss = 10.0\n    epoch_loss_list = []\n\n    for epoch in range(1, num_epochs + 1):\n        print('Epoch {}\/{}'.format(epoch, num_epochs), end=\" \")\n#         print('-' * 10)\n\n        # Training\n        model.train()  # Set model to training mode\n        running_loss = 0.0\n        \n        # Iterate over data.\n        for inputs, targets in train_loader:  # \n            inputs = inputs.to(device)\n            targets = targets.to(device)\n\n            # zero the parameter gradients\n            optimizer.zero_grad()\n\n            # forward\n            # track history if only in train\n            outputs = model(inputs)\n            loss = loss_f(outputs, targets)  # can grad\n            \n            # backward + optimize \n            if device.type == \"xla\":\n                loss.backward()\n                optimizer.step()\n                xm.mark_step()\n            else:\n                loss.backward()\n                optimizer.step()\n            \n            running_loss += loss * inputs.size(0)\n                                \n        train_loss = running_loss \/ len(train_loader.dataset)\n        print('Train Loss: {:.4f}'.format(train_loss.item()), end=\" \")\n        gc.collect()\n\n        # valid\n        model.eval()  # Set model to eval mode\n        running_loss = 0.0\n\n        # Iterate over data.\n        with torch.no_grad():\n            for inputs, targets in valid_loader:  # \n                inputs = inputs.to(device)\n                targets = targets.to(device)\n\n                # forward\n                # track history if only in train\n                outputs = model(inputs)\n                loss = loss_f(outputs, targets)  # can grad\n\n                # statistics\n                running_loss += loss * inputs.size(0)\n                \n        valid_loss = running_loss \/ len(valid_loader.dataset)\n\n        print('Valid Loss: {:.4f}'.format(valid_loss.item()), end=\"\\n\")\n        epoch_loss_list.append([train_loss, valid_loss])\n        \n        if scheduler is not None:\n            if isinstance(scheduler, optim.lr_scheduler.ReduceLROnPlateau):\n                scheduler.step(valid_loss)\n            else:\n                scheduler.step()\n                \n        # deep copy the model\n        if valid_loss < best_loss:\n            best_loss = valid_loss\n            best_model_wts = copy.deepcopy(model.state_dict())\n            best_epoch = epoch\n        gc.collect()\n\n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(\n        time_elapsed \/\/ 60, time_elapsed % 60))\n    print('Best val loss: {:4f}'.format(best_loss))\n\n#     # load best model weights\n#     model.load_state_dict(best_model_wts)\n    return model, epoch_loss_list, best_model_wts, best_epoch","af6e7641":"@torch.no_grad()\ndef test_model(model, test_loader, device='cpu'):\n    model = model.to(device)  # Set model to eval mode\n    model.eval()  # Set model to eval mode\n    total_dice_l = 0.0\n    total_samples = 0\n\n    # Iterate over data.\n    for inputs, masks in test_loader:  # \n        inputs = inputs.to(device)\n        masks = masks.to(device)\n\n        outputs = model(inputs)\n                \n#         outputs = nn.functional.sigmoid(outputs)\n        batch_dice_l = dice_loss_for_batch(outputs, masks)\n\n        total_dice_l += batch_dice_l.sum()\n        total_samples += len(batch_dice_l)\n\n    dice_summary = total_dice_l.item() \/ total_samples\n    \n    result = {\"dice_loss\": dice_summary,\n             }\n    return result","d4d5a8ac":"def show_loss(epoch_loss_list):\n    plt.rcParams[\"figure.figsize\"] = (8,4)\n    plt.plot(epoch_loss_list)\n    plt.legend([\"train_loss\", \"valid_loss\"], loc='upper center',bbox_to_anchor=(0.5, 1.2),\n              ncol=3, fancybox=True, shadow=True)\n    plt.ylim(0,1)\n    plt.show()","e606dfc4":"def dice_loss(pred, target, smooth=0.1):  \n    iflat = pred.contiguous().view(-1)\n    tflat = target.contiguous().view(-1)\n    intersection = (iflat * tflat).sum()\n    A_sum = torch.sum(iflat * iflat)\n    B_sum = torch.sum(tflat * tflat)\n    return 1 - ((2. * intersection + smooth) \/ (A_sum + B_sum + smooth) )\n\ndef dice_loss_for_batch(preds, targets, smooth=0.1):  \n    loss_list = []\n    for i in range(preds.size(0)):\n        loss_list.append(dice_loss(preds[i], targets[i], smooth))\n\n    loss_list = torch.tensor(loss_list)\n    return loss_list\n\nclass dice_BCE_loss:\n    def __init__(self, weight=0.2):\n        self.bce_w = weight\n        self.dice_w = 1 - weight\n        \n    def __call__(self, pred, target):\n        \"\"\"\n        dice_loss: C\u1eadp nh\u1eadp ch\u1eadm v\u1edbi mask r\u1ed7ng, khi\u1ebfn nhi\u1ec1u kho\u1ea3ng kh\u00f4ng \u0111c detected t\u1ef1 d\u01b0ng \u0111c active \n        binary_cross_entropy Gi\u1ed1ng m\u1ed9t regularization gi\u00fap gi\u00e1 tr\u1ecb \u1edf c\u00e1c kho\u1ea3ng kh\u00f4ng \u0111c detected gi\u1ea3m xu\u1ed1ng, tuy nhi\u00ean grad th\u01b0\u1eddng kh\u00e1 l\u1edbn c\u1ea3 m\u1edbi v\u00f9ng detected v\u00e0 kh\u00f4ng \n        m\u00e0 b\u00ecnh th\u01b0\u1eddng v\u00f9ng \u0111c detected th\u01b0\u1eddng kh\u00e1 nh\u1ecf so v\u1edbi \u1ea3nh khi\u1ebfn vi\u1ec7c h\u1ecdc \u0111c v\u00f9ng c\u1ea7n detected kh\u00e1 kh\u00f3\n        Vi\u1ec7c k\u1ebft h\u1ee3p 2 h\u00e0m ph\u00f9 h\u1ee3p v\u1edbi t\u1ec9 l\u1ec7 detected \/ mask_size gi\u00fap c\u1eadp nh\u1eadp hi\u1ec7u qu\u1ea3\n        \n        - Nhi\u1ec1u mask r\u1ed7ng, v\u00f9ng detected nh\u1ecf -> \u0111\u1eb7t weight nh\u1ecf -> t\u0103ng kn h\u1ecdc c\u1ee7a dice_loss\n        - To\u00e0n sd mask kh\u00f4ng r\u1ed7ng \u0111\u1ec3 train -> \u0111\u1eb7t weight x\u1ea5p x\u1ec9 detected \/ mask_size\n        - sd mask kh\u00f4ng r\u1ed7ng, v\u00f9ng detected l\u1edbn -> \u0111\u1eb7t weight c\u00e2n b\u1eb1ng ho\u1eb7c l\u1edbn\n        \"\"\"\n        dice = dice_loss(pred, target)  # C\u1eadp nh\u1eadp ch\u1eadm v\u1edbi mask r\u1ed7ng, khi\u1ebfn nhi\u1ec1u kho\u1ea3ng kh\u00f4ng \u0111c detected t\u1ef1 d\u01b0ng \u0111c active \n        bce = nn.functional.binary_cross_entropy(pred, target)  # Gi\u1ed1ng m\u1ed9t regularization gi\u00fap gi\u00e1 tr\u1ecb \u1edf c\u00e1c kho\u1ea3ng kh\u00f4ng \u0111c detected gi\u1ea3m xu\u00f4ng \n        loss = self.dice_w * dice + self.bce_w * bce\n        return loss\n    \nclass dice_BCE_loss_with_logits:\n    def __init__(self, weight=0.2):\n        self.bce_w = weight\n        self.dice_w = 1 - weight\n        \n    def __call__(self, pred, target):\n        pred = nn.functional.sigmoid(pred)\n        dice = dice_loss(pred, target)  # C\u1eadp nh\u1eadp ch\u1eadm v\u1edbi mask r\u1ed7ng, khi\u1ebfn nhi\u1ec1u kho\u1ea3ng kh\u00f4ng \u0111c detected t\u1ef1 d\u01b0ng \u0111c active \n        bce = nn.functional.binary_cross_entropy(pred, target)  # Gi\u1ed1ng m\u1ed9t regularization gi\u00fap gi\u00e1 tr\u1ecb \u1edf c\u00e1c kho\u1ea3ng kh\u00f4ng \u0111c detected gi\u1ea3m xu\u00f4ng \n        loss = self.dice_w * dice + self.bce_w * bce\n        return loss\n    \n","829c9325":"def dice_score_detail(model, test_set):\n    if isinstance(test_set, torch.utils.data.Subset):\n        loader = test_set.dataset.__loader__\n        imgs = test_set.dataset.imgs\n    else:\n        loader = test_set.__loader__\n        imgs = test_set.imgs\n    \n    predict_result = []\n    \n    model.eval() \n    with torch.no_grad():   \n        for i, img_id in enumerate(test_set.indices):\n            path = imgs[img_id]\n            img, mask = test_set.__getitem__(i)\n            \n#             plt.imshow(loader('image\/' + path))\n#             plt.show()\n#             plt.imshow(to_img(img))\n#             plt.show()\n#             plt.imshow(to_img(mask))\n#             plt.show()\n#             if i == 2:\n#                 break\n            t_img = img.to(device)\n            input = t_img.unsqueeze(0)\n            mask = mask.to(device)\n\n            output = model(input)\n            \n            score = 1 - dice_loss(output[0], mask).item()\n\n            predict_result.append([i, path, score])\n\n    result_arr = pd.DataFrame(predict_result, columns=['id', 'file', 'dice_score'])\n    result_arr.astype({'id':int, 'file': str, 'dice_score': float})    \n    return result_arr","0eb65a23":"class SegmentationDataset(Dataset):\n    def __init__(self, root, preprocessing=None, image_transform=None, mask_transform=None):\n        self.root = root\n        self.preprocessing = preprocessing\n        self.image_transform = image_transform\n        self.mask_transform = mask_transform\n        self.imgs = os.listdir(root + 'image\/')\n        self.indices = list(range(len(self.imgs)))\n    \n    def __loader__(self, file_name):\n        try:\n            return Image.open(self.root + file_name)\n        except Exception as e:\n            print(e)\n            return None\n    \n    def __getitem__(self, index):\n        file_name = self.imgs[index]\n        \n        image = self.__loader__('image\/' + file_name)\n        image = np.array(image, dtype=\"uint8\")\n        \n        if os.path.isfile(self.root + 'mask\/' + file_name):\n            mask = self.__loader__('mask\/' + file_name)\n        else:\n            mask = np.zeros(image.shape)\n        mask = np.array(mask, dtype=\"uint8\")\n\n        if self.preprocessing is not None:\n            sample = self.preprocessing(image=image, mask= mask)\n            image = sample[\"image\"]\n            mask = sample[\"mask\"]\n        \n        if self.image_transform is not None:\n            image = self.image_transform(image)\n        if self.mask_transform is not None:\n            mask = self.mask_transform(mask)\n            \n        return tuple([image, mask])\n    \n    def __len__(self):\n        return len(self.imgs)\n\n\n# class CombineDataset(Dataset):\n#     def __init__(self, datasets, suffer=False):\n#         self.datasets = {0: datasets[0]}\n#         self.lenght = len(self.datasets[0]) \n#         if len(datasets) > 1:\n#             for dataset in datasets[1:]:\n#                 self.datasets.update({self.lenght: dataset})\n#                 self.lenght += len(dataset) \n        \n#         self.id_list = list(range(self.lenght))\n#         if suffer:\n#             np.random.shuffle(self.id_list)\n        \n#     def __getitem__(self, index):\n#         index = self.id_list[index]\n#         start_index = 0\n#         for k in self.datasets.keys():\n#             if index >= k:\n#                 start_index = k\n#         dataset = self.datasets.get(start_index)\n\n#         result = dataset.__getitem__(index - start_index)\n        \n#         return result\n    \n#     def __len__(self):\n#         return self.lenght","1e07a863":"class AlbumentationTransform(object):\n    def __init__(self, transform_list=[]):\n        self.transform = A.Compose(transform_list)\n    \n    def __call__(self, image):\n        result = self.transform(image=image)[\"image\"]\n        return result\n\n    def __repr__(self):\n        return self.__class__.__name__+'()'\n\nclass InverseTransform(object):\n    def __init__(self):\n        pass\n    \n    def __call__(self, image):\n        image = np.array(image, dtype=\"uint8\")\n        image = np.clip(image, 0, 255)\n        result = 255 - image\n        return result\n\n    def __repr__(self):\n        return self.__class__.__name__+'()'\n    \n\nclass BinaryTransform(object):     \n    def __init__(self, threshold):\n        self.threshold = threshold\n        \n    def __call__(self, image):\n        result = (image > self.threshold) * 1.\n        return result\n\n    def __repr__(self):\n        return self.__class__.__name__+'()'\n    \nclass AddGaussianNoise(object):\n    def __init__(self, mean=0., std=.1):\n        self.std = std\n        self.mean = mean\n        \n    def __call__(self, image: torch.tensor):\n        result = image + torch.normal(self.mean, self.std, size=image.size())\n        return result\n    \n    def __repr__(self):\n        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)\n    \nclass X3_channel(object):\n    def __init__(self, dim=0):\n        self.dim = dim\n        \n    def __call__(self, image: torch.tensor):\n        image = torch.cat([image] * 3, dim=self.dim)\n        return image\n    \n    def __repr__(self):\n        return self.__class__.__name__ + '()'\n\nclass UnNormalize(object):\n    def __init__(self, mean, std):\n        self.mean = mean\n        self.std = std\n\n    def __call__(self, tensor):\n        for t, m, s in zip(tensor, self.mean, self.std):\n            t.mul_(s).add_(m)\n            # The normalize code -> t.sub_(m).div_(s)\n        return tensor\n    \n    def __repr__(self):\n        return self.__class__.__name__ + '()'","66ee6138":"# to_img = transforms.ToPILImage()\n# to_tensor = transforms.ToTensor()","dd89c7ef":"# img = Image.open(\"..\/input\/siimacr-pneumothorax-data-256-pixel\/have_mask\/test\/image\/1.2.276.0.7230010.3.1.4.8323329.5797.1517875190.762694.png\")\n# plt.imshow(img)\n# plt.show()\n\n# t_img = X3_channel()(to_tensor(img))\n# plt.imshow(to_img(t_img))\n# plt.show()\n","a5ee616d":"\n# c\u00e1c bi\u1ebfn \u0111\u1ed5i albumentations c\u00f3 \u1ea3nh h\u01b0\u1edfng t\u1edbi c\u1ea3 img l\u1eabn mask v\u00e0 c\u1ea7n bi\u1ebfn \u0111\u1ed5i v\u1edbi \u1ea3nh uint8\n# C\u00e1c bi\u1ebfn \u0111\u1ed5i albumentations gi\u00fap tr\u00e1nh overfitting ch\u1ec9 ap d\u1ee5ng l\u00ean train\ntrain_alb_transform = A.Compose([\n    A.HorizontalFlip(p=0.3),\n    \n    A.OneOf([\n        A.ElasticTransform(alpha=400, sigma=15, alpha_affine=15, border_mode=0,value=0, mask_value=0),\n        A.OpticalDistortion(distort_limit=0.5, shift_limit=0.05, border_mode=0,value=0, mask_value=0),\n    ], p=0.3),\n    \n    A.OneOf([\n        A.RandomCrop(height=448, width=448),\n        A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, border_mode=0,value=0, mask_value=0),\n    ], p=0.3),\n    \n    A.Resize(height=512, width=512,always_apply=True),\n])\n\n# C\u00e1c bi\u1ebfn \u0111\u1ed5i \u1ea3nh h\u01b0\u1edfng \u0111\u1ebfn \u1ea3nh\ntrain_image_transforms = transforms.Compose([\n    InverseTransform(),\n    AlbumentationTransform([\n        A.CLAHE(always_apply=True),\n        A.RandomGamma(gamma_limit=(180, 200), always_apply=True),\n    ]),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5], std=[0.25]),\n    AddGaussianNoise(0, 0.05),\n])\n\ntest_image_transforms = transforms.Compose([\n    InverseTransform(),\n    AlbumentationTransform([\n        A.CLAHE(always_apply=True),\n        A.RandomGamma(gamma_limit=(180, 200), always_apply=True),\n    ]),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5], std=[0.25]),\n])\n\n# C\u00e1c bi\u1ebfn \u0111\u1ed5i \u1ea3nh h\u01b0\u1edfng \u0111\u1ebfn mask\nmask_transforms = transforms.Compose([\n    transforms.ToTensor(),\n    BinaryTransform(0),\n])\n# un_normalize = None \n# un_normalize = UnNormalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\nun_normalize = UnNormalize(mean=[0.5], std=[0.25])\nto_img = transforms.ToPILImage()","a9c24706":"# train_image_transforms = transforms.Compose([\n#     AlbumentationTransform([\n#         A.OneOf([\n#             A.RandomGamma(gamma_limit=(60, 120)),\n#             A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2),\n#             A.CLAHE(clip_limit=4.0, tile_grid_size=(4, 4)),\n#         ], p=0.9),\n#         A.OneOf([\n#             A.Blur(blur_limit=4),\n#             A.MotionBlur(blur_limit=5),\n#             A.MedianBlur(blur_limit=5)\n#         ], p=0.5),\n#         A.HorizontalFlip(p=0.5),\n#         A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=10, border_mode=0, value=0, p=1),\n#     ]),\n#     transforms.ToTensor(),\n#     X3_channel(),\n#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n# ])\n\n# test_image_transforms = transforms.Compose([\n#     transforms.ToTensor(),\n#     X3_channel(),\n#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n# ])\n","dd52ae67":"# et = A.ElasticTransform(alpha=50, sigma=10, alpha_affine=10, border_mode=0,value=0, mask_value=0, always_apply=True)\n# gd = A.GridDistortion(always_apply=True)\n# od = A.OpticalDistortion(distort_limit=0.1, shift_limit=0.01, border_mode=0,value=0, mask_value=0, always_apply=True)\n# rc = A.RandomCrop(height=220, width=220, always_apply=True)\n# ssr = A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=10, border_mode=0,value=0, mask_value=0, always_apply=True)\n# cla = A.CLAHE(always_apply=True)\n# rgl = A.RandomGamma(gamma_limit=(45, 50), always_apply=True)\n# rgh = A.RandomGamma(gamma_limit=(200, 210), always_apply=True)\n# inv = InverseTransform()","9df4f0ae":"# img = Image.open(\"..\/input\/siimacr-pneumothorax-data-256-pixel\/have_mask\/test\/image\/1.2.276.0.7230010.3.1.4.8323329.5802.1517875190.919441.png\")\n# plt.imshow(img, cmap='bone')\n# plt.show()\n\n# t_img = np.array(img, dtype='uint8')\n# t_img = ssr(image=t_img)['image']\n# plt.imshow(to_img(t_img), cmap='bone')\n# plt.show()\n","081400e9":"# train\ntrain_val_hm_set = SegmentationDataset(\"..\/input\/siimacr-pneumothorax-data-512-pixel\/train\/have_mask\/\",\n                                    preprocessing=train_alb_transform,\n                                    image_transform=train_image_transforms,\n                                    mask_transform=mask_transforms,\n                                    )\n\n# train_val_nm_set = SegmentationDataset(\"..\/input\/siimacr-pneumothorax-data-512-pixel\/train\/no_mask\/\",\n#                                     preprocessing=train_alb_transform,\n#                                     image_transform=train_image_transforms,\n#                                     mask_transform=mask_transforms,\n#                                     )\n\ntest_hm_set = SegmentationDataset(\"..\/input\/siimacr-pneumothorax-data-512-pixel\/test\/have_mask\/\",\n                                  preprocessing=None,\n                                  image_transform=test_image_transforms,\n                                  mask_transform=mask_transforms,\n                                 )\n\ntest_nm_set = SegmentationDataset(\"..\/input\/siimacr-pneumothorax-data-512-pixel\/test\/no_mask\/\",\n                                  preprocessing=None,\n                                  image_transform=test_image_transforms,\n                                  mask_transform=mask_transforms,\n                                 )","8a2da341":"# train_val_set = torch.utils.data.ConcatDataset([train_val_hm_set, torch.utils.data.Subset(train_val_nm_set, range(len(train_val_hm_set)))])","83779d0d":"\n# Thi\u1ebft \u0111\u1eb7t k\u00edch th\u01b0\u1edbc c\u1ee7a c\u00e1c t\u1eadp d\u1eef li\u1ec7u\ntrain_size = int(0.9 * len(train_val_hm_set))\nvalid_size = len(train_val_hm_set) - train_size\ntrain_size, valid_size ","6b5553e0":"# T\u00e1ch d\u1eef li\u1ec7u th\u00e0nh c\u00e1c t\u1eadp train - valid - test\ntrain_set, valid_set = torch.utils.data.random_split(train_val_hm_set, [train_size, valid_size])\n","df0740f6":"# loader\ntrain_loader = DataLoader(train_set, batch_size=2, num_workers=4, shuffle=True)\nvalid_loader = DataLoader(valid_set, batch_size=2, num_workers=4)\n\ntest_hm_loader = DataLoader(test_hm_set, batch_size=2, num_workers=4)\n# test_nm_loader = DataLoader(test_nm_set, batch_size=3, num_workers=4)","edc7eebf":"to_img = transforms.ToPILImage()\nfig, axs = plt.subplots(2, train_loader.batch_size, figsize=(3 * train_loader.batch_size,6))\n\nimgs, masks = next(iter(train_loader))\n\nif un_normalize is not None:\n    for i, img in enumerate(imgs):\n        imgs[i] = un_normalize(img)\n        \nfor i in range(train_loader.batch_size): # 11,19,23,27,45\n    axs[0, i].imshow(to_img(imgs[i]), cmap='bone')\n    axs[1, i].imshow(to_img(masks[i]), cmap='bone')\nplt.show()    \n\n\nfig, axs = plt.subplots(2, test_hm_loader.batch_size, figsize=(3 * test_hm_loader.batch_size,6))\nimgs, masks = next(iter(test_hm_loader))\n\nif un_normalize is not None:\n    for i, img in enumerate(imgs):\n        imgs[i] = un_normalize(img)\n        \nfor i in range(test_hm_loader.batch_size): # 11,19,23,27,45\n    axs[0, i].imshow(to_img(imgs[i]), cmap='bone')\n    axs[1, i].imshow(to_img(masks[i]), cmap='bone')\nplt.show()","6fe2115d":"# out_src = \"model\/unet_seresnext50_32x4d_512px\/07_07\/\"\nmodel_name = \"unet_senet154\"\nbackbone='senet154'\nfreeze_backbone = 10\ncurrent_epoch = 0\n\n# seresnet50, seresnext50_32x4d, senet154\nmodel = UNet_backbone(n_channels=1, n_classes=1, backbone=backbone, pretrained=True)\n# model.load_state_dict(torch.load(\"..\/input\/unet-256x32-e10\/28_06\/12_unet_e52_best.pt\"))\nmodel = model.to(device)","5b2ea95e":"if freeze_backbone > 0:\n    model.set_backbone_grad(False)\n\n    loss_f = dice_BCE_loss(weight=0.5)\n    optimizer = optim.Adam(model.parameters(), lr=0.01)    \n    lr_scheduler = None\n\n    model, epoch_loss_list, best_weight, best_epoch = train_model(\n        model, \n        train_loader, valid_loader, loss_f, optimizer, device=device,\n        scheduler=lr_scheduler, num_epochs=freeze_backbone)\n\n    print(\"Best epoch:\", best_epoch)\n    show_loss(epoch_loss_list)\n    model.set_backbone_grad(True)\n\n    del loss_f, optimizer\n    gc.collect()\n\n# for param in model.parameters():\n#     print(param.requires_grad, end='|')","1512ef49":"print(test_model(model, valid_loader, device))\nprint(test_model(model, test_hm_loader, device))\n# for param in model.parameters():\n#     print(param.requires_grad, end='|')","6452da18":"train_epoch = 10\n\n# loss_f = nn.BCEWithLogitsLoss()\nloss_f = dice_BCE_loss(weight=0.5)\n\n# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\noptimizer = optim.Adam(model.parameters(), lr=0.01)\n    \nlr_scheduler = None\n# lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.9)\n# lr_scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[1,2,3,4,5,7,9,12,16], gamma=0.75)\n# lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=0.0005)\n# lr_scheduler = optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.0005, max_lr=0.01, mode='exp_range', gamma=0.9, step_size_up=2)\n\n# -------------------------- epoch 0 -> 10 --------------------------\nmodel_trained, epoch_loss_list, best_weight, best_epoch = train_model(\n    model, \n    train_loader, valid_loader, loss_f, optimizer, device=device,\n    scheduler=lr_scheduler, num_epochs=train_epoch)\n\nprint(\"Best epoch:\", current_epoch + best_epoch)\nshow_loss(epoch_loss_list)","bd652092":"print(test_model(model_trained, valid_loader, device))\nprint(test_model(model_trained, test_hm_loader, device))","f4afcdc5":"model_state = {k: v.cpu() for k, v in model_trained.state_dict().items()}\nbest_model_state = {k: v.cpu() for k, v in best_weight.items()}\n\ntorch.save(best_model_state, model_name + \"_e{}_best.pt\".format(current_epoch + best_epoch))\ncurrent_epoch += train_epoch\ntorch.save(model_state, model_name + \"_e{}.pt\".format(current_epoch))\n\ndel model_state, best_model_state","db5bedfb":"train_epoch = 10\n\nloss_f = dice_BCE_loss(weight=0.2)\n\n# # optimizer = optim.SGD(model.parameters(), lr=0.0005)\n# optimizer = optim.Adam(model_trained.parameters(), lr=0.01)\n    \n# lr_scheduler = None\n# # lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=3, eta_min=0.0005)\n\n# -------------------------- epoch 10 -> 20 --------------------------\nmodel_trained, epoch_loss_list, best_weight, best_epoch = train_model(\n    model_trained, \n    train_loader, valid_loader, loss_f, optimizer, device=device,\n    scheduler=lr_scheduler, num_epochs=train_epoch)\n\nprint(\"Best epoch:\", current_epoch + best_epoch)\nshow_loss(epoch_loss_list)","cf668c3b":"print(test_model(model_trained, valid_loader, device))\nprint(test_model(model_trained, test_hm_loader, device))","d3c21c90":"model_state = {k: v.cpu() for k, v in model_trained.state_dict().items()}\nbest_model_state = {k: v.cpu() for k, v in best_weight.items()}\n\ntorch.save(best_model_state, model_name + \"_e{}_best.pt\".format(current_epoch + best_epoch))\ncurrent_epoch += train_epoch\ntorch.save(model_state, model_name + \"_e{}.pt\".format(current_epoch))\n\ndel model_state, best_model_state","ab217ad4":"train_epoch = 10\n\n# model_trained.load_state_dict(best_weight)\n\n# loss_f = dice_BCE_loss(weight=0.2)\n\n# # optimizer = optim.SGD(model.parameters(), lr=0.0005)\n# optimizer = optim.Adam(model_trained.parameters(), lr=0.00005)\n\n# lr_scheduler = None\n\n# -------------------------- epoch 20 -> 30 --------------------------\nmodel_trained, epoch_loss_list, best_weight, best_epoch = train_model(\n    model_trained, \n    train_loader, valid_loader, loss_f, optimizer, device=device,\n    scheduler=lr_scheduler, num_epochs=train_epoch)\n\nprint(\"Best epoch:\", current_epoch + best_epoch)\nshow_loss(epoch_loss_list)","75a73614":"print(test_model(model_trained, valid_loader, device))\nprint(test_model(model_trained, test_hm_loader, device))","ca49fd8e":"model_state = {k: v.cpu() for k, v in model_trained.state_dict().items()}\nbest_model_state = {k: v.cpu() for k, v in best_weight.items()}\n\ntorch.save(best_model_state, model_name + \"_e{}_best.pt\".format(current_epoch + best_epoch))\ncurrent_epoch += train_epoch\ntorch.save(model_state, model_name + \"_e{}.pt\".format(current_epoch))\n\ndel model_state, best_model_state","55f2c26b":"train_epoch = 10\n\n# model_trained.load_state_dict(best_weight)\n\n# loss_f = dice_BCE_loss(weight=0.2)\n\n# optimizer = optim.SGD(model_trained.parameters(), lr=0.00005)\n# # optimizer = optim.Adam(model_trained.parameters(), lr=0.0005)\n\n# # lr_scheduler = None\n# lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=0.00001)\n\n# -------------------------- epoch 30 -> 40 --------------------------\nmodel_trained, epoch_loss_list, best_weight, best_epoch = train_model(\n    model_trained, \n    train_loader, valid_loader, loss_f, optimizer, device=device,\n    scheduler=lr_scheduler, num_epochs=train_epoch)\n\nprint(\"Best epoch:\", current_epoch + best_epoch)\nshow_loss(epoch_loss_list)","ba18b4d7":"model_state = {k: v.cpu() for k, v in model_trained.state_dict().items()}\nbest_model_state = {k: v.cpu() for k, v in best_weight.items()}\n\ntorch.save(best_model_state, model_name + \"_e{}_best.pt\".format(current_epoch + best_epoch))\ncurrent_epoch += train_epoch\ntorch.save(model_state, model_name + \"_e{}.pt\".format(current_epoch))\n\ndel model_state, best_model_state","473b2952":"to_img = transforms.ToPILImage()","cc49a340":"cpu_device = torch.device('cpu')\n# backbone=None\n\n# model_trained = UNet_backbone(n_channels=1, n_classes=1, backbone=backbone, pretrained=False)\n# model_trained.load_state_dict(torch.load(\"..\/input\/unet-256x32-e10\/21_06\/11_ad_unet_e40.pt\", map_location=cpu_device))\n# model_trained = model_trained.to(device)\n# model_trained.eval()\n\n# best model\nbest_model = UNet_backbone(n_channels=1, n_classes=1, backbone=backbone, pretrained=False)\nbest_model.load_state_dict(best_weight)\n# best_model.load_state_dict(torch.load(\"..\/input\/unet-256x32-e10\/28_06\/12_unet_e52_best.pt\", map_location=cpu_device))\nbest_model = best_model.to(device)\nbest_model.eval()\nprint(\"Load successful\")","677c5d60":"test_result = pd.DataFrame(index=[\"model_trained\", \"best_model\"],\n                           columns=[\"mask_dice\"] # , \"mask_bce\" , \"no_mask_dice\", \"no_mask_bce\"]\n                          )\ntest_result.index.name = \"model\"","122189d7":"result = test_model(model_trained, test_hm_loader, device)\ntest_result.loc[\"model_trained\", \"mask_dice\"] = round(result['dice_loss'], 4)\n# test_result.loc[\"model_trained\", \"mask_bce\"] = round(result['bce_loss'], 4)\n\n# result = test_model(model_trained, test_nm_loader, device)\n# test_result.loc[\"model_trained\", \"no_mask_dice\"] = round(result['dice_loss'], 4)\n# test_result.loc[\"model_trained\", \"no_mask_bce\"] = round(result['bce_loss'], 4)","4c398600":"result = test_model(best_model, test_hm_loader, device)\ntest_result.loc[\"best_model\", \"mask_dice\"] = round(result['dice_loss'], 4)\n# test_result.loc[\"best_model\", \"mask_bce\"] = round(result['bce_loss'], 4)\n\n# result = test_model(best_model, test_nm_loader, device)\n# test_result.loc[\"best_model\", \"no_mask_dice\"] = round(result['dice_loss'], 4)\n# test_result.loc[\"best_model\", \"no_mask_bce\"] = round(result['bce_loss'], 4)","77fe5ccd":"test_result  # e40->50","de92bac1":"inter_test_hm_loader = iter(test_hm_loader)","0939838f":"fig, axs = plt.subplots(test_hm_loader.batch_size, 4, figsize=(12, 3 * test_hm_loader.batch_size))\n\nimgs, masks = next(inter_test_hm_loader)\nimgs = imgs.to(device)\nmasks = masks.to(device)\n\ninputs = imgs.to(device)\n\npredict1 = model(inputs)\npredict2 = best_model(inputs)\n\nfor i in range(test_nm_loader.batch_size): # 11,19,23,27,45\n    axs[i, 0].imshow(to_img(un_normalize(imgs[i])), cmap='bone')\n    axs[i, 1].imshow(to_img(masks[i]), cmap='bone')\n    \n    inputs = imgs[i]  # .to(device)\n\n    axs[i, 2].imshow(to_img(predict1[i]), cmap='bone')\n    \n    dice_l = dice_loss(predict1[i], masks[i]).item()\n    bce_l = nn.functional.binary_cross_entropy(predict1[i], masks[i]).item()\n    axs[i, 2].set_title('dice: {:.3f} - bce: {:.3f}'.format(dice_l, bce_l))\n\n\n    axs[i,3].imshow(to_img(predict2[i]), cmap='bone')\n    \n    dice_l = dice_loss(predict2[i], masks[i]).item()\n    bce_l = nn.functional.binary_cross_entropy(predict2[i], masks[i]).item()\n    axs[i, 3].set_title('dice: {:.3f} - bce: {:.3f}'.format(dice_l, bce_l))\n\nplt.show()","7842d92e":"# inter_test_nm_loader = iter(test_nm_loader)","082c1071":"# fig, axs = plt.subplots(test_nm_loader.batch_size, 4, figsize=(12, 3 * test_nm_loader.batch_size))\n\n# imgs, masks = next(inter_test_nm_loader)\n# imgs = imgs.to(device)\n# masks = masks.to(device)\n\n# inputs = imgs.to(device)\n\n# predict1 = model_trained(inputs)\n# predict2 = best_model(inputs)\n\n# for i in range(test_nm_loader.batch_size): # 11,19,23,27,45\n#     axs[i, 0].imshow(to_img(un_normalize(imgs[i])), cmap='bone')\n#     axs[i, 1].imshow(to_img(masks[i]), cmap='bone')\n    \n#     inputs = imgs[i]  # .to(device)\n\n#     axs[i, 2].imshow(to_img(predict1[i]), cmap='bone')\n    \n#     dice_l = dice_loss(predict1[i], masks[i]).item()\n#     bce_l = nn.functional.binary_cross_entropy(predict1[i], masks[i]).item()\n#     axs[i, 2].set_title('dice: {:.3f} - bce: {:.3f}'.format(dice_l, bce_l))\n\n    \n#     axs[i,3].imshow(to_img(predict2[i]), cmap='bone')\n    \n#     dice_l = dice_loss(predict2[i], masks[i]).item()\n#     bce_l = nn.functional.binary_cross_entropy(predict2[i], masks[i]).item()\n#     axs[i, 3].set_title('dice: {:.3f} - bce: {:.3f}'.format(dice_l, bce_l))\n\n# plt.show()","95358674":"# dsd = dice_score_detail(model, train_set)\n# dsd_sorted = dsd.sort_values('dice_score').reset_index(drop=True)\n# dsd_sorted.head(), dsd_sorted.tail()","ea2e850b":"# dataset = train_set\n# dice_score_df = dsd_sorted[105:110].reset_index(drop=True)\n\n# fig, axs = plt.subplots(2,5, figsize=(25, 10))\n# for i, row in dice_score_df.head().iterrows():\n#     img, mask = dataset.__getitem__(row[0])\n#     img = un_normalize(img)\n    \n#     axs[0, i].imshow(to_img(img), cmap='bone')\n#     axs[1, i].imshow(to_img(mask), cmap='bone')\n    \n#     axs[0, i].set_title(row[1])\n#     axs[1, i].set_title(row[2])\n\n# plt.show()","f0a9b121":"# from torchvision.utils import save_image","8e7112fa":"# class SegmentationDataset_(Dataset):\n#     def __init__(self, root, preprocessing=None, image_transform=None, mask_transform=None):\n#         self.root = root\n#         self.preprocessing = preprocessing\n#         self.image_transform = image_transform\n#         self.mask_transform = mask_transform\n#         self.imgs = os.listdir(root + 'image\/')\n    \n#     def __loader__(self, file_name):\n#         try:\n#             return Image.open(self.root + file_name)\n#         except Exception as e:\n#             print(e)\n#             return None\n    \n#     def __getitem__(self, index):\n#         file_name = self.imgs[index]\n        \n#         image = self.__loader__('image\/' + file_name)\n#         image = np.array(image, dtype=\"uint8\")\n        \n#         if os.path.isfile(self.root + 'mask\/' + file_name):\n#             mask = self.__loader__('mask\/' + file_name)\n#         else:\n#             mask = np.zeros(image.shape)\n#         mask = np.array(mask, dtype=\"uint8\")\n\n#         if self.preprocessing is not None:\n#             sample = self.preprocessing(image=image, mask= mask)\n#             image = sample[\"image\"]\n#             mask = sample[\"mask\"]\n        \n#         if self.image_transform is not None:\n#             image = self.image_transform(image)\n#         if self.mask_transform is not None:\n#             mask = self.mask_transform(mask)\n            \n#         return tuple([image, mask, file_name])\n    \n#     def __len__(self):\n#         return len(self.imgs)","f78f6a0c":"# image_transforms = transforms.Compose([\n#     InverseTransform(),\n#     AlbumentationTransform([\n#         A.CLAHE(always_apply=True),\n#         A.RandomGamma(gamma_limit=(180, 200), always_apply=True),\n#     ]),\n#     transforms.ToTensor(),\n# ])\n\n# mask_transforms = transforms.Compose([\n#     transforms.ToTensor(),\n#     BinaryTransform(0),\n# ])","457f7f3a":"# train_val_hm_set = SegmentationDataset_(\"..\/input\/siimacr-pneumothorax-data-256-pixel\/have_mask\/train\/\",\n#                                     preprocessing=None,\n#                                     image_transform=image_transforms,\n#                                     mask_transform=mask_transforms,\n#                                     )\n\n# train_val_nm_set = SegmentationDataset_(\"..\/input\/siimacr-pneumothorax-data-256-pixel\/no_mask\/train\/\",\n#                                     preprocessing=None,\n#                                     image_transform=image_transforms,\n#                                     mask_transform=mask_transforms,\n#                                     )\n\n# test_hm_set = SegmentationDataset_(\"..\/input\/siimacr-pneumothorax-data-256-pixel\/have_mask\/test\/\",\n#                                   preprocessing=None,\n#                                   image_transform=image_transforms,\n#                                   mask_transform=mask_transforms,\n#                                  )\n\n# test_nm_set = SegmentationDataset_(\"..\/input\/siimacr-pneumothorax-data-256-pixel\/no_mask\/test\/\",\n#                                   preprocessing=None,\n#                                   image_transform=image_transforms,\n#                                   mask_transform=mask_transforms,\n#                                  )","55df60da":"# train_hm_loader = DataLoader(train_val_hm_set, batch_size=5, num_workers=4)\n# train_nm_loader = DataLoader(train_val_nm_set, batch_size=5, num_workers=4)\n\n# test_hm_loader = DataLoader(test_hm_set, batch_size=5, num_workers=4)\n# test_nm_loader = DataLoader(test_nm_set, batch_size=5, num_workers=4)","6d504249":"# next(iter(test_nm_loader))","a2fd582a":"# ! mkdir pre_mask pre_mask\/train pre_mask\/test pre_mask\/test\/no_mask pre_mask\/test\/have_mask pre_mask\/train\/no_mask pre_mask\/test\/have_mask\n# ! mkdir merger merger\/train merger\/test merger\/test\/no_mask merger\/test\/have_mask merger\/train\/no_mask merger\/test\/have_mask","18378c0d":"# in_out = [\n#     (test_hm_loader, \"pre_mask\/have_mask\/test\/\", \"merger\/test\/have_mask\"),\n#     (test_nm_loader, \"pre_mask\/no_mask\/test\/\", \"merger\/test\/have_mask\/\"),\n#     (train_hm_loader, \"pre_mask\/have_mask\/train\/\", \"merger\/have_mask\/test\/\"),\n#     (train_nm_loader, \"pre_mask\/no_mask\/train\/\", \"merger\/have_mask\/test\/\"),\n# ]","b09bbdf1":"# with torch.no_grad():\n#     for data_loader, pre_mask_src, merger_src in in_out:\n#         for inputs, mask, files in data_loader:\n#             inputs = inputs.to(device)\n            \n#             predicts = best_model(inputs)\n            \n#             muls = inputs * predicts\n            \n#             mergers = torch.cat([inputs, predicts, muls], dim=1)\n            \n#             for i, file in enumerate(files):\n#                 save_image(predicts[i], pre_mask_src+file)\n# #                 save_image(mergers[i], merger_src+file)\n            \n# #             fig, axs = plt.subplots(4, data_loader.batch_size, figsize=(5 * data_loader.batch_size,20))\n# #             for i in range(data_loader.batch_size):\n# #                 print(files[i])\n# #                 axs[0, i].imshow(to_img(inputs[i]), cmap='bone')\n                \n# #                 axs[1, i].imshow(to_img(predicts[i]), cmap='bone')\n\n# #                 axs[2, i].imshow(to_img(muls[i]), cmap='bone')\n                \n# #                 axs[3, i].imshow(to_img(mergers[i]))\n# #             plt.show()\n# #             break\n    \n\n                ","fc723242":"# len(os.listdir(\"pre_mask\/no_mask\/test\/\")),len(os.listdir(\"pre_mask\/no_mask\/train\/\")),len(os.listdir(\"pre_mask\/have_mask\/test\/\")),len(os.listdir(\"pre_mask\/have_mask\/train\/\")),","6fb146cf":"# len(os.listdir(\"merger\/no_mask\/test\/\")),len(os.listdir(\"merger\/no_mask\/train\/\")),len(os.listdir(\"merger\/have_mask\/test\/\")),len(os.listdir(\"merger\/have_mask\/train\/\")),","97cf769f":"# %cd pre_mask\n# ! zip -r ..\/pre_mask.zip *\n# %cd ..","6ed58e33":"# %cd merger\/\n# ! zip -r ..\/merger.zip *\n# %cd ..","4b07e79b":"# LOSS","613f8d76":"### ~dpn98: 235mb~\n### se_resnet101: 188mb\n### se_densenet121","ee6ac7cd":"# LIB","b6c825f3":"# DATASET","5ca4ed94":"### On unmask data","5929ee1d":"# VISUAL RESULT","4087b730":"# TRAINING","e0f453e1":"# MODEL","c9fa6b11":"# PROCESS DATA","ee3f45f3":"# SAVE PREDICT MASK","f6a68e82":"### Train new weight when freeze backbone","ac1ab2d2":"# CHECKING GOOD AND BAD PREDICT","03ac5e95":"# TRAIN - TEST","6d844639":"### On mask only data","add59c2d":"### Train","9486e0eb":"# LOAD DATA","3c710e79":"### TEST TRANSFORM"}}