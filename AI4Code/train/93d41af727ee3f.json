{"cell_type":{"4e34847d":"code","0c032458":"code","c0860e69":"code","4dc5ccab":"code","f34a2245":"code","d1a0482a":"code","c839b13d":"code","485dd15a":"code","dcb3eb40":"code","899170ed":"code","a5c1e42d":"code","86182ecf":"code","63d54305":"code","2dc7275b":"code","4f8d1883":"code","dd8f9838":"code","959366c4":"code","890852b5":"code","b5236073":"code","89181df4":"code","2fd4bcec":"code","b8a57aa0":"code","756327f2":"code","ae1ee70b":"code","cef57d17":"code","09fa1d32":"code","89e501e9":"code","114aa857":"code","04248f21":"code","ba67700d":"code","8677d02a":"code","d9d919d0":"code","ca970fa9":"code","559bb2e2":"code","f8dc1e3b":"code","9c1c26e3":"code","6880ee68":"code","69254e71":"code","51234063":"code","c12e55d8":"code","0ccfca80":"code","442897f9":"code","915bba4c":"code","3375b981":"code","5f158357":"code","f77da058":"code","639cfdc4":"code","0c7b46cf":"code","18388105":"code","e576a680":"code","511cf48f":"code","e8b438a7":"code","6a33c1b9":"code","f25792d5":"code","b87a351c":"code","ee631d4b":"code","e5c02f63":"code","aedda914":"code","bb91479b":"code","2733269d":"code","04646b67":"code","625d9ea4":"code","b378d450":"code","187d334d":"code","e615720e":"code","6f84ad49":"code","02ebfa9d":"code","bc50f8e0":"code","8f6db078":"code","9ad809f3":"code","4694f288":"code","7a28e1fe":"code","dfe7f8ff":"code","cd704d95":"markdown","64473052":"markdown","2bb9564b":"markdown","e934071f":"markdown","02f6c245":"markdown","8d5e97cd":"markdown","cc86817e":"markdown","b75f10bd":"markdown","c6ed7c37":"markdown","4414480b":"markdown","6f53059d":"markdown","c4bb0590":"markdown","fa1d1036":"markdown","3034496f":"markdown","25338eb5":"markdown","101f501a":"markdown","3277483b":"markdown","b7d3ae4d":"markdown","0d67032a":"markdown","639ddaca":"markdown","0099dbad":"markdown","99bb8d4e":"markdown","eff13625":"markdown","76d3fd6c":"markdown","e1e3316e":"markdown","48d12a01":"markdown","861f35f4":"markdown","2c94b488":"markdown","2666d19e":"markdown","b89bd13a":"markdown","8de9ac15":"markdown","e1e5652a":"markdown","07a27867":"markdown","56c944a3":"markdown","b7cdc547":"markdown","54c4544d":"markdown","ace171ad":"markdown","833dc6d4":"markdown","f043c6af":"markdown","83f0116a":"markdown","6ef2cf7d":"markdown","b05290b2":"markdown","3e238978":"markdown","a9b66eb9":"markdown"},"source":{"4e34847d":"%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')\nwarnings.filterwarnings('ignore', category=DeprecationWarning)\n\nimport pandas as pd\npd.options.display.max_columns = 100\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimport seaborn as sns\n\nimport pylab as plot\nparams = { \n    'axes.labelsize': \"large\",\n    'xtick.labelsize': 'large',\n    'legend.fontsize': 20,\n    'figure.dpi': 150,\n    'figure.figsize': [25, 7]\n}\nplot.rcParams.update(params)\n\n\n\n","0c032458":"# Reading the train file and saving it as a pandas data frame :\ntrain_df = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/titanic\/test.csv');\ngender_submission = pd.read_csv('\/kaggle\/input\/titanic\/gender_submission.csv') \n","c0860e69":"# View the training data\ntrain_df.head(5)","4dc5ccab":"# View the Test data\ntest_df.head(5)","f34a2245":"# View the gender data\ngender_submission.head(5)","d1a0482a":"train_df.describe()","c839b13d":"def show_data(data):\n    rows = []\n    for col in data.columns:\n        rows.append([col, data[col].isnull().sum(), data[col].nunique(), data[col].dtypes])\n    print(pd.DataFrame(rows, columns=['Col', 'Missing values', 'Unique values', 'Type']))","485dd15a":"show_data(train_df)","dcb3eb40":"train_df.info()","899170ed":"show_data(test_df)","a5c1e42d":"def bar_chart(feature):\n    survived = train_df[train_df['Survived']==1][feature].value_counts()\n    dead = train_df[train_df['Survived']==0][feature].value_counts()\n    df = pd.DataFrame([survived,dead])\n    df.index = ['Survived','Dead']\n    df.plot(kind='bar',stacked=True, figsize=(10,5))\n   ","86182ecf":"bar_chart('Sex')","63d54305":"bar_chart('Pclass')","2dc7275b":"bar_chart('SibSp')","4f8d1883":"bar_chart('Parch')","dd8f9838":"bar_chart('Embarked')","959366c4":"# cat cols, cat_but_car\n\ndef grab_col_names(dataframe, cat_th=5, car_th=20):\n    cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == \"O\"]\n    num_but_cat = [col for col in dataframe.columns if dataframe[col].dtypes != \"O\" and\n                   dataframe[col].nunique() < cat_th]\n\n    cat_but_car = [col for col in dataframe.columns if dataframe[col].dtypes == \"O\" and\n                   dataframe[col].nunique() > car_th]\n\n    cat_cols = cat_cols + num_but_cat\n    cat_cols = [col for col in cat_cols if col not in cat_but_car]\n\n    # num_cols\n    num_cols = [col for col in dataframe.columns if dataframe[col].dtypes != \"O\" and \"ID\" not in col.upper()]\n    num_cols = [col for col in num_cols if col not in num_but_cat]\n\n    return cat_cols,num_cols,cat_but_car\n\n\n\n\n","890852b5":"grab_col_names(train_df)\n","b5236073":"cat_cols, num_cols, cat_but_car = grab_col_names(train_df)","89181df4":"def cat_summary(df, plot=False):\n    for col_name in cat_cols:\n           \n        if plot == True:\n            rgb_values = sns.color_palette(\"Set2\", 6)\n            sns.set_theme(style=\"darkgrid\")\n            ax = sns.countplot(x=df[col_name], data=df, palette=rgb_values)\n            for p in ax.patches:\n                ax.annotate(f'\\n{p.get_height()}', (p.get_x()+0.2, p.get_height()), ha='center', va='top', color='white', size=10)\n            plt.show()","2fd4bcec":"cat_summary(train_df, plot=True)\n","b8a57aa0":"plt.figure(figsize=(10,6))\nsns.heatmap(train_df.drop('PassengerId',axis=1).corr(), vmax=0.6, square=True, annot=True)","756327f2":"train_test_data = [train_df, test_df] # combining train and test dataset\n\nfor dataset in train_test_data:\n    dataset['Title'] = dataset['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)    ","ae1ee70b":"train_df['Title'].value_counts()\n","cef57d17":"test_df['Title'].value_counts()\n","09fa1d32":"train_df.head()","89e501e9":"pd.crosstab(train_df['Title'], train_df['Sex'])","114aa857":"for dataset in train_test_data:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col', \\\n \t'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Other')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    \ntrain_df[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()","04248f21":"title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Other\": 5}\nfor dataset in train_test_data:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)","ba67700d":"train_df.head()\n","8677d02a":"bar_chart('Title')\n","d9d919d0":"sex_mapping = {\"male\": 0, \"female\": 1}\nfor dataset in train_test_data:\n    dataset['Sex'] = dataset['Sex'].map(sex_mapping)","ca970fa9":"train_df.head()","559bb2e2":"train_df.Embarked.unique()","f8dc1e3b":"train_df.Embarked.value_counts()","9c1c26e3":"Pclass1 = train_df[train_df['Pclass']==1]['Embarked'].value_counts()\nPclass2 = train_df[train_df['Pclass']==2]['Embarked'].value_counts()\nPclass3 = train_df[train_df['Pclass']==3]['Embarked'].value_counts()\ndf = pd.DataFrame([Pclass1, Pclass2, Pclass3])\ndf.index = ['1st class','2nd class', '3rd class']\ndf.plot(kind='bar',stacked=True, figsize=(10,5))\n","6880ee68":"for dataset in train_test_data:\n    dataset['Embarked'] = dataset['Embarked'].fillna('S')","69254e71":"train_df.head()","51234063":"embarked_mapping = {\"S\": 0, \"C\": 1, \"Q\": 2}\nfor dataset in train_test_data:\n    dataset['Embarked'] = dataset['Embarked'].map(embarked_mapping)","c12e55d8":"train_df.head()","0ccfca80":"train_df.head(50)","442897f9":"# fill missing age with median age for each title (Mr, Mrs, Miss, Others)\ntrain_df[\"Age\"].fillna(train_df.groupby(\"Title\")[\"Age\"].transform(\"median\"), inplace=True)\ntest_df[\"Age\"].fillna(test_df.groupby(\"Title\")[\"Age\"].transform(\"median\"), inplace=True)","915bba4c":"train_df.head(30)\ntrain_df.groupby(\"Title\")[\"Age\"].transform(\"median\")","3375b981":"facet = sns.FacetGrid(train_df, hue=\"Survived\",aspect=4)\nfacet.map(sns.kdeplot,'Age',shade= True)\nfacet.set(xlim=(0, train_df['Age'].max()))\nfacet.add_legend()\n \nplt.show()","5f158357":"\nfor dataset in train_test_data:\n    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age'] = 4","f77da058":"train_df.head()","639cfdc4":"bar_chart('Age')","0c7b46cf":"# fill missing Fare with median fare for each Pclass\ntrain_df[\"Fare\"].fillna(train_df.groupby(\"Pclass\")[\"Fare\"].transform(\"median\"), inplace=True)\ntest_df[\"Fare\"].fillna(test_df.groupby(\"Pclass\")[\"Fare\"].transform(\"median\"), inplace=True)\ntrain_df.head()","18388105":"facet = sns.FacetGrid(train_df, hue=\"Survived\",aspect=4)\nfacet.map(sns.kdeplot,'Fare',shade= True)\nfacet.set(xlim=(0, train_df['Fare'].max()))\nfacet.add_legend()\n \nplt.show()","e576a680":"\nfor dataset in train_test_data:\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\n","511cf48f":"train_df.head()","e8b438a7":"train_df[\"FamilySize\"] = train_df[\"SibSp\"] + train_df[\"Parch\"] + 1\ntest_df[\"FamilySize\"] = test_df[\"SibSp\"] + test_df[\"Parch\"] + 1\n","6a33c1b9":"\nfacet = sns.FacetGrid(train_df, hue=\"Survived\",aspect=4)\nfacet.map(sns.kdeplot,'FamilySize',shade= True)\nfacet.set(xlim=(0, train_df['FamilySize'].max()))\nfacet.add_legend()\nplt.xlim(0)","f25792d5":"print (train_df[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean())","b87a351c":"family_mapping = {1: 0, 2: 0.4, 3: 0.8, 4: 1.2, 5: 1.6, 6: 2, 7: 2.4, 8: 2.8, 9: 3.2, 10: 3.6, 11: 4}\nfor dataset in train_test_data:\n    dataset['FamilySize'] = dataset['FamilySize'].map(family_mapping)","ee631d4b":"train_df.head()\n","e5c02f63":"features_drop = ['Name', 'SibSp', 'Parch', 'Ticket','Cabin']\ntrain_df = train_df.drop(features_drop, axis=1)\ntest_df = test_df.drop(features_drop, axis=1)\ntrain_df = train_df.drop(['PassengerId'], axis=1)","aedda914":"train_df.head(10)","bb91479b":"test_df.head(10)","2733269d":"train_data = train_df.drop('Survived', axis=1)\ntarget = train_df['Survived']\ntest_data = test_df.drop(\"PassengerId\", axis=1).copy()\ntrain_data.shape, target.shape, test_data.shape","04646b67":"train_data.head(10)","625d9ea4":"# Importing Classifier Modules\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier","b378d450":"train_data.info()","187d334d":"from sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nk_fold = KFold(n_splits=10, shuffle=True, random_state=0)","e615720e":"clf = DecisionTreeClassifier()\nscoring = 'accuracy'\nscore1 = cross_val_score(clf, train_data, target, cv=k_fold, n_jobs=1, scoring=scoring)\nprint(score1)","6f84ad49":"# decision tree Score\nround(np.mean(score1)*100, 2)","02ebfa9d":"clf = RandomForestClassifier(n_estimators=13)\nscoring = 'accuracy'\nscore2 = cross_val_score(clf, train_data, target, cv=k_fold, n_jobs=1, scoring=scoring)\nprint(score2)","bc50f8e0":"# Random Forest Score\nround(np.mean(score2)*100, 2)","8f6db078":"clf = SVC()\nscoring = 'accuracy'\nscore3 = cross_val_score(clf, train_data, target, cv=k_fold, n_jobs=1, scoring=scoring)\nprint(score3)","9ad809f3":"round(np.mean(score3)*100,2)","4694f288":"clf = SVC()\nclf.fit(train_data, target)\n\ntest_data = test_df.drop(\"PassengerId\", axis=1).copy()\nprediction = clf.predict(test_data)","7a28e1fe":"submission = pd.DataFrame({\n        \"PassengerId\": test_df[\"PassengerId\"],\n        \"Survived\": prediction\n    })\n\nsubmission.to_csv('submission.csv', index=False)","dfe7f8ff":"submission = pd.read_csv('submission.csv')\nsubmission.head(30)","cd704d95":"#### **4.3 Embarked Feature**\nThere are empty values for some rows for Embarked column. The empty values are represented as \"nan\" in below list.","64473052":"#### **5.4 Testing**","2bb9564b":"After that, we convert the categorical Title values into numeric form.\n\n","e934071f":"#### **3.3 Let's visualize survival based on SibSp and ParCh.**","02f6c245":"As you can see above, we have added a new column named Title in the Train dataset with the Title present in the particular passenger name.","8d5e97cd":"Converting Numerical Age to Categorical Variable\n\nfeature vector map:\n- child: 0\n- young: 1\n- adult: 2\n- mid-age: 3\n- senior: 4","cc86817e":"#### **5.1 Decision Tree**\nA decision tree is a flowchart-like structure in which each internal node represents a \"test\" on an attribute (e.g. whether a coin flip comes up heads or tails), each branch represents the outcome of the test, and each leaf node represents a class label (decision taken after computing all attributes). The paths from root to leaf represent classification rules.","b75f10bd":"In this section, we select the appropriate features to train our classifier. Here, we create new features based on existing features. We also convert categorical features into numeric form.\n\nOne trick when starting a machine learning problem is to append the training set to the test set together.\n\nWe'll engineer new features using the train set to prevent information leakage. Then we'll add these variables to the test set.\n\nLet's load the train and test sets and append them together. \n\n","c6ed7c37":"#### **3.4 Let's visualize survival based on Embarked.**","4414480b":"The Chart confirms a person aboarded from C slightly more likely survived\n\nThe Chart confirms a person aboarded from Q more likely dead\n\nThe Chart confirms a person aboarded from S more likely dead\n\nLet's now stop with data exploration and switch to the next part.\n\n","6f53059d":"The Chart confirms 1st class more likely survivied than other classes\n\nThe Chart confirms 3rd class more likely dead than other classes","c4bb0590":"#### **5.3 Linear SVM**\nLinear SVM is a SVM model with linear kernel.\n\nIn the below code, LinearSVC stands for Linear Support Vector Classification.","fa1d1036":"#### **3.1 Let's visualize survival based on the gender.**","3034496f":"#### **5.2 Random Forest**\nRandom forests or random decision forests are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Random decision forests correct for decision trees' habit of overfitting to their training set.\n\nEnsemble methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone.","25338eb5":"\nThe Chart confirms a person aboarded with more than 2 siblings or spouse more likely survived\n\nThe Chart confirms a person aboarded without siblings or spouse more likely dead","101f501a":"#### **3.2 Let's visualize survival based on the classes.**\n","3277483b":"In this part, we use our knowledge of the passengers based on the features we created and then build a statistical model. You can think of this model as a box that crunches the information of any new passenger and decides whether or not he survives.\n\nThere is a wide variety of models to use, from logistic regression to decision trees and more sophisticated ones such as random forests and gradient boosted trees.","b7d3ae4d":"### **4. Feature engineering**\n","0d67032a":"Let's start by importing the useful libraries.","639ddaca":"#### **4.4 Age**\nSome age VALUES are missing\\\nLet's use Title's median age for missing Age","0099dbad":"From above, we can see that Linear SVM has the highest accuracy score.","99bb8d4e":"### References\nThis notebook is created by learning from the following notebooks:\n\n- https:\/\/www.kaggle.com\/omarelgabry\/a-journey-through-titanic\n- https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions\n- https:\/\/www.ahmedbesbes.com\/blog\/kaggle-titanic-competition","eff13625":"Ok we can clearly see some missing values here. three features have missing data (*age, cabin and embarked*) in Train data. While (*age, cabin and fare*) have missing data in Test dataset.\n\nIt's important to visualise missing values early so you know where the major holes are in your dataset.\n\nKnowing this informaiton will help with your EDA and figuring out what kind of data cleaning and preprocessing is needed.","76d3fd6c":"**Import & Loading in the data**\n","e1e3316e":"We drop unnecessary columns\/features and keep only the useful ones for our experiment. Column PassengerId is only dropped from Train set because we need PassengerId in Test set while creating Submission file to Kaggle.","48d12a01":"The Sex variable seems to be a discriminative feature. Women are more likely to survive.","861f35f4":"The Chart confirms a person aboarded with more than 2 parents or children more likely survived\n\nThe Chart confirms a person aboarded alone more likely dead","2c94b488":"About data shows that:\n\nHaving FamilySize upto 4 (from 2 to 4) has better survival chance.\\\nFamilySize = 1, i.e. travelling alone has less survival chance.\\\nLarge FamilySize (size of 5 and above) also have less survival chance.","2666d19e":"### **3. Exploratory Data Analysis**\n\nIn this section we will check the number and types of features for the dataset, if there is missing values, if there is corelation between some features, if we can remove some unnecessary features and if we can create new features from the ones we already have\n\n","b89bd13a":"**Data Descriptions**\n- PassengerID : ID of the Passenger.\n- Survived: (0 = No; 1 = Yes)\n- Pclass: Passenger Class (1 = 1st; 2 = 2nd; 3 = 3rd)\n- Name : Name of the Passenger\n- Sex: (Female \/ Male)\n- Age: Age of the Passenger.\n- Sibsp: Number of siblings\/spouses aboard\n- Parch: Number of parents\/children aboard\n- Ticket : Ticket number.\n- Fare: Passenger fare\n- Cabin: Cabin number\n- Embarked: Port of embarkation (C = Cherbourg; Q = Queenstown; S = Southampton)","8de9ac15":"The number of passengers with each Title is shown above.\n\nWe now replace some less common titles with the name \"Other\".","e1e5652a":"### **2. Collecting the data**","07a27867":"#### **4.6 SibSp & Parch Feature (FamilySize)**\nCombining SibSp & Parch feature, we create a new feature named FamilySize.\n\n","56c944a3":"Heatmap of Correlation between different features:\n\nPositive numbers = Positive correlation, i.e. increase in one feature will increase the other feature & vice-versa.\n\nNegative numbers = Negative correlation, i.e. increase in one feature will decrease the other feature & vice-versa.\n\nIn our case, we focus on which features have strong positive or negative correlation with the Survived feature.","b7cdc547":"#### **3.5 Correlating Features**","54c4544d":"**Import Libraries**","ace171ad":"#### **4.5 Fare**","833dc6d4":"### **1. Introduction**\n\nThis is my first stab at a Kaggle script. I have chosen to work with the Titanic dataset after spending some time poking around on the site and looking at other scripts made by other Kagglers for inspiration. \n\nRMS Titanic was a British passenger liner operated by the White Star Line that sank in the North Atlantic Ocean on 15 April 1912, after striking an iceberg during her maiden voyage from Southampton to New York City. Of the estimated 2,224 passengers and crew aboard, more than 1,500 died, making the sinking at the time one of the deadliest of a single ship. \nOne of the reasons of that much of loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n\nIn this project we will be working in Python on the famous Titanic dataset: from https:\/\/www.kaggle.com\/c\/titanic. To Predict if a passenger will survive the sinking or not ?\n\nThis notebook will be presented in 5 Steps:\n\n- Collecting the data\n- Exploratory data analysis\n- Feature engineering\n- Modelling\n- Testing\n","f043c6af":"### **5. Modelling & Testing**","83f0116a":"Let's check the number of passengers for each Embarked category.","6ef2cf7d":"- more than 50% of 1st class are from S embark\n- more than 50% of 2nd class are from S embark\n- more than 50% of 3rd class are from S embark\n- We find that category \"S\" has maximum passengers. Hence, we replace \"nan\" values with \"S\".","b05290b2":"#### **4.2 Sex Feature**\nWe convert the categorical value of Sex into numeric. We represent 0 as female and 1 as male.","3e238978":"We now convert the categorical value of Embarked into numeric. We represent 0 as S, 1 as C and 2 as Q.","a9b66eb9":"#### **4.1 Name Feature**\nLet's first extract titles from Name column."}}