{"cell_type":{"c7b5f3de":"code","22f36e9c":"code","80760a09":"code","8a39d472":"markdown","fe9def83":"markdown"},"source":{"c7b5f3de":"# \u0422\u0443\u0442 \u0441\u0430\u043c\u044b\u0439 \u043e\u0431\u044b\u0447\u043d\u044b\u0439 \u043a\u043e\u0434, \u0447\u0442\u043e\u0431\u044b \u0432\u0441\u0435 \u0440\u0430\u0431\u043e\u0442\u0430\u043b\u043e, \u043a\u0430\u043a \u043d\u0430\u0434\u043e. \u041d\u0438\u0447\u0435\u0433\u043e \u0438\u043d\u0442\u0435\u0440\u0435\u0441\u043d\u043e\u0433\u043e\n\nimport os\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport torch\nimport tqdm\nfrom torch.utils import data\nfrom torchvision import transforms\n\nTRAIN_SIZE = 0.8\nCROP_SIZE = 128\ninitial_path = '..\/input\/made-cv-2021-contest-01-facial-landmarks\/contest01_data'\n\nclass ThousandLandmarksDataset(data.Dataset):\n    def __init__(self, root, transforms, split=\"train\"):\n        super(ThousandLandmarksDataset, self).__init__()\n        self.root = root\n        landmark_file_name = os.path.join(root, 'landmarks.csv') if split != \"test\" \\\n            else os.path.join(root, \"test_points.csv\")\n        images_root = os.path.join(root, \"images\")\n\n        self.image_names = []\n        self.landmarks = []\n\n        with open(landmark_file_name, \"rt\") as fp:\n            num_lines = sum(1 for line in fp)\n        num_lines -= 1  # header\n\n        with open(landmark_file_name, \"rt\") as fp:\n            for i, line in tqdm.tqdm(enumerate(fp), total=num_lines + 1):\n                if i == 0:\n                    continue  # skip header\n                if split == \"train\" and i == int(TRAIN_SIZE * num_lines):\n                    break  # reached end of train part of data\n                elif split == \"val\" and i < int(TRAIN_SIZE * num_lines):\n                    continue  # has not reached start of val part of data\n                elements = line.strip().split(\"\\t\")\n                image_name = os.path.join(images_root, elements[0])\n                self.image_names.append(image_name)\n\n                if split in (\"train\", \"val\"):\n                    landmarks = list(map(np.int, elements[1:]))\n                    landmarks = np.array(landmarks, dtype=np.int).reshape((len(landmarks) \/\/ 2, 2))\n                    self.landmarks.append(landmarks)\n\n        if split in (\"train\", \"val\"):\n            self.landmarks = torch.as_tensor(self.landmarks)\n        else:\n            self.landmarks = None\n\n        self.transforms = transforms\n\n    def __getitem__(self, idx):\n        sample = {}\n        if self.landmarks is not None:\n            landmarks = self.landmarks[idx]\n            sample[\"landmarks\"] = landmarks\n\n        image = cv2.imread(self.image_names[idx])\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        sample[\"image\"] = image\n\n        if self.transforms is not None:\n            sample = self.transforms(sample)\n\n        return sample\n\n    def __len__(self):\n        return len(self.image_names)\n\n    \nclass TransformByKeys(object):\n    def __init__(self, transform, names):\n        self.transform = transform\n        self.names = set(names)\n\n    def __call__(self, sample):\n        for name in self.names:\n            if name in sample:\n                sample[name] = self.transform(sample[name])\n\n        return sample\n    \n\nclass ScaleMinSideToSize(object):\n    def __init__(self, size=(CROP_SIZE, CROP_SIZE), elem_name='image'):\n        # self.size = torch.tensor(size, dtype=torch.float)\n        self.size = np.asarray(size, dtype=np.float)\n        self.elem_name = elem_name\n\n    def __call__(self, sample):\n        h, w, _ = sample[self.elem_name].shape\n        if h > w:\n            f = self.size[0] \/ w\n        else:\n            f = self.size[1] \/ h\n\n        sample[self.elem_name] = cv2.resize(sample[self.elem_name], None, fx=f, fy=f, interpolation=cv2.INTER_AREA)\n        sample[\"scale_coef\"] = f\n\n        if 'landmarks' in sample:\n            landmarks = sample['landmarks'].reshape(-1, 2).float()\n            landmarks = landmarks * f\n            sample['landmarks'] = landmarks.reshape(-1)\n\n        return sample\n\n\nclass CropCenter(object):\n    def __init__(self, size=128, elem_name='image'):\n        self.size = size\n        self.elem_name = elem_name\n\n    def __call__(self, sample):\n        img = sample[self.elem_name]\n        h, w, _ = img.shape\n        margin_h = (h - self.size) \/\/ 2\n        margin_w = (w - self.size) \/\/ 2\n        sample[self.elem_name] = img[margin_h:margin_h + self.size, margin_w:margin_w + self.size]\n        sample[\"crop_margin_x\"] = margin_w\n        sample[\"crop_margin_y\"] = margin_h\n\n        if 'landmarks' in sample:\n            landmarks = sample['landmarks'].reshape(-1, 2)\n            landmarks -= torch.tensor((margin_w, margin_h), dtype=landmarks.dtype)[None, :]\n            sample['landmarks'] = landmarks.reshape(-1)\n\n        return sample\n\n\ntrain_transforms = transforms.Compose([\n    ScaleMinSideToSize((CROP_SIZE, CROP_SIZE)),\n    CropCenter(CROP_SIZE),\n    TransformByKeys(transforms.ToPILImage(), (\"image\",)),\n    TransformByKeys(transforms.ToTensor(), (\"image\",)),\n])\n    \ntrain_dataset = ThousandLandmarksDataset(os.path.join(initial_path, \"train\"), train_transforms, split=\"train\")","22f36e9c":"import torchvision.transforms.functional as TF\n\nclass LandmarksAugmentation:\n    def __init__(self, rotation_limit):\n        self.rotation_limit = rotation_limit\n\n    def random_rotation(self, image, landmarks):\n\n        angle = np.random.uniform(-self.rotation_limit, self.rotation_limit)\n        image = TF.rotate(image, angle)\n\n        landmarks = landmarks.reshape(-1, 2)\n        landmarks = torch.hstack((landmarks, torch.ones((landmarks.shape[0], 1))))\n        center = (CROP_SIZE \/ 2, CROP_SIZE \/ 2)\n        rad = angle * np.pi \/ 180.0\n        alpha = np.cos(rad)\n        beta = np.sin(rad)\n        M_torch = torch.tensor([[alpha, beta, (1-alpha)*center[0] - beta*center[1]],\n                                [-beta, alpha, beta*center[0] + (1-alpha)*center[1]]], dtype=torch.float32)\n        new_landmark = torch.matmul(landmarks, M_torch.T)\n\n        return image, new_landmark\n    \n    def __call__(self, sample):\n        image, landmarks = self.random_rotation(sample['image'], sample['landmarks'])\n        sample['image'] = image\n        sample['landmarks'] = landmarks.flatten()\n        return sample","80760a09":"import matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(4, 5, figsize=(25, 15))\nidx = 0\n\nfor i in range(4):\n    for j in range(5):\n        aug = LandmarksAugmentation(90)\n        augmented = aug(train_dataset[idx])\n        image = augmented['image']\n        landmarks = augmented['landmarks'].reshape(-1, 2)\n        axes[i][j].imshow(image.permute(1, 2, 0))\n        axes[i][j].scatter(landmarks[:, 0], landmarks[:, 1], color='blue', s=0.5)","8a39d472":"## \u0410\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u044f \u043d\u0430 \u043f\u043e\u0432\u043e\u0440\u043e\u0442 \u043b\u0438\u0446 \n\u0412 \u0432\u044b\u0431\u043e\u0440\u043a\u0435 \u043c\u043d\u043e\u0433\u043e \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0439 \u0441 \u043d\u0430\u043a\u043b\u043e\u043d\u043e\u043c \u043b\u0438\u0446\u0430, \u0438\u043c\u0435\u043d\u043d\u043e \u043d\u0430 \u043d\u0438\u0445 \u043e\u0431\u044b\u0447\u043d\u043e \u043c\u043e\u0434\u0435\u043b\u044c \u043e\u0448\u0438\u0431\u0430\u0435\u0442\u0441\u044f \u0431\u043e\u043b\u044c\u0448\u0435 \u0432\u0441\u0435\u0433\u043e, \u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u0431\u0443\u0434\u0435\u0442 \u043f\u043e\u043b\u0435\u0437\u043d\u043e \u0441\u043e\u0437\u0434\u0430\u0442\u044c \u0430\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u044e \u0441 \u043f\u043e\u0432\u043e\u0440\u043e\u0442\u043e\u043c \u043a\u0430\u043a \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f, \u0442\u0430\u043a \u0438 \u043e\u0441\u043d\u043e\u0432\u043d\u044b\u0445 \u0442\u043e\u0447\u0435\u043a \u043b\u0438\u0446\u0430.","fe9def83":"\u0410\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u044f \u0440\u0430\u0431\u043e\u0442\u0430\u0435\u0442, \u043c\u0430\u0441\u043a\u0430 \u043d\u0430 \u0438\u0441\u043a\u0430\u0436\u0430\u0435\u0442\u0441\u044f. \u041f\u043e\u044d\u0442\u043e\u043c\u0443 \u043c\u043e\u0436\u0435\u043c \u0441\u043c\u0435\u043b\u043e \u0432\u043a\u043b\u044e\u0447\u0430\u0442\u044c \u0435\u0435 \u0432 train_transforms. "}}