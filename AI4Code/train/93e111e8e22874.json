{"cell_type":{"a694be83":"code","c01487c0":"code","ccf2a7da":"code","249e974f":"code","075d449f":"code","db43cfa3":"code","4a5bf9b7":"code","d7d31bc1":"code","4bcc199e":"code","67ab494e":"code","59d91aa2":"code","7982735e":"code","1ddad706":"code","bbb1384e":"code","6f73f0a5":"code","d3e2a1d6":"code","bb59fb1e":"code","f2f69e58":"code","9e695f7d":"code","2aab24ae":"code","1c49f4b3":"code","3c43cc2d":"code","823e5eec":"code","9a71b56b":"code","f0dc4efa":"code","620ff2b0":"markdown","9a744cf0":"markdown","f24f0214":"markdown","e7589de9":"markdown","9cfa6e3b":"markdown","6e6ae81d":"markdown","941ff002":"markdown","babe1185":"markdown","30164679":"markdown","286644f7":"markdown","3c54dd14":"markdown","aecf78bb":"markdown","342dcb45":"markdown","9622d375":"markdown","008d5a03":"markdown","137da682":"markdown","4a32bab5":"markdown"},"source":{"a694be83":"#importing the required packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","c01487c0":"#Get the dataset\ndataset = pd.read_csv(r'..\/input\/salary-databysuperdatascienceteam\/Salary_Data.csv')","ccf2a7da":"#Get a glimpse of the Dataset\ndataset.head()","249e974f":"#Separating the independent and dependent features\nX = np.asarray(dataset['YearsExperience'].values.tolist()) #Independent feature\ny = np.asarray(dataset['Salary'].values.tolist())          #Dependent feature","075d449f":"#Reshaping the Independent and Dependent features\nX = X.reshape(len(X),1)\ny = y.reshape(len(y),1)","db43cfa3":"#Independent Feature Scaling\nX = (X - int(np.mean(X)))\/np.std(X)\n\n#Dependent Feature Scaling\ny = (y - int(np.mean(y)))\/np.std(y)","4a5bf9b7":"#Adding the feature X0 = 1, so we have the equation: y =  (W1 * X1) + (W0 * X0) \nX = np.concatenate((X,np.ones((30,1))), axis = 1)","d7d31bc1":"X","4bcc199e":"# The method \"split_data\" splits the given dataset into trainset and testset\n# This is similar to the method \"train_test_split\" from \"sklearn.model_selection\"\ndef split_data(X,y,test_size=0.2,random_state=0):\n    np.random.seed(random_state)                  #set the seed for reproducible results\n    indices = np.random.permutation(len(X))       #shuffling the indices\n    data_test_size = int(X.shape[0] * test_size)  #Get the test size\n\n    #Separating the Independent and Dependent features into the Train and Test Set\n    train_indices = indices[data_test_size:]\n    test_indices = indices[:data_test_size]\n    X_train = X[train_indices]\n    y_train = y[train_indices]\n    X_test = X[test_indices]\n    y_test = y[test_indices]\n    return X_train, y_train, X_test, y_test","67ab494e":"class linearRegression():\n\n  def __init__(self):\n    #No instance Variables required\n    pass\n\n  def forward(self,X,y,W):\n    \"\"\"\n    Parameters:\n    X (array) : Independent Features\n    y (array) : Dependent Features\/ Target Variable\n    W (array) : Weights \n\n    Returns:\n    loss (float) : Calculated Sqaured Error Loss for y and y_pred\n    y_pred (array) : Predicted Target Variable\n    \"\"\"\n    y_pred = sum(W * X)\n    loss = ((y_pred-y)**2)\/2    #Loss = Squared Error, we introduce 1\/2 for ease in the calculation\n    return loss, y_pred\n\n  def updateWeights(self,X,y_pred,y_true,W,alpha,index):\n    \"\"\"\n    Parameters:\n    X (array) : Independent Features\n    y_pred (array) : Predicted Target Variable\n    y_true (array) : Dependent Features\/ Target Variable\n    W (array) : Weights\n    alpha (float) : learning rate\n    index (int) : Index to fetch the corresponding values of W, X and y \n\n    Returns:\n    W (array) : Update Values of Weight\n    \"\"\"\n    for i in range(X.shape[1]):\n      #alpha = learning rate, rest of the RHS is derivative of loss function\n      W[i] -= (alpha * (y_pred-y_true[index])*X[index][i]) \n    return W\n\n  def train(self, X, y, epochs=10, alpha=0.001, random_state=0):\n    \"\"\"\n    Parameters:\n    X (array) : Independent Feature\n    y (array) : Dependent Features\/ Target Variable\n    epochs (int) : Number of epochs for training, default value is 10\n    alpha (float) : learning rate, default value is 0.001\n\n    Returns:\n    y_pred (array) : Predicted Target Variable\n    loss (float) : Calculated Sqaured Error Loss for y and y_pred\n    \"\"\"\n\n    num_rows = X.shape[0] #Number of Rows\n    num_cols = X.shape[1] #Number of Columns \n    W = np.random.randn(1,num_cols) \/ np.sqrt(num_rows) #Weight Initialization\n\n    #Calculating Loss and Updating Weights\n    train_loss = []\n    num_epochs = []\n    train_indices = [i for i in range(X.shape[0])]\n    for j in range(epochs):\n      cost=0\n      np.random.seed(random_state)\n      np.random.shuffle(train_indices)\n      for i in train_indices:\n        loss, y_pred = self.forward(X[i],y[i],W[0])\n        cost+=loss\n        W[0] = self.updateWeights(X,y_pred,y,W[0],alpha,i)\n      train_loss.append(cost)\n      num_epochs.append(j)\n    return W[0], train_loss, num_epochs\n\n  def test(self, X_test, y_test, W_trained):\n    \"\"\"\n    Parameters:\n    X_test (array) : Independent Features from the Test Set\n    y_test (array) : Dependent Features\/ Target Variable from the Test Set\n    W_trained (array) : Trained Weights\n    test_indices (list) : Index to fetch the corresponding values of W_trained,\n                          X_test and y_test \n\n    Returns:\n    test_pred (list) : Predicted Target Variable\n    test_loss (list) : Calculated Sqaured Error Loss for y and y_pred\n    \"\"\"\n    test_pred = []\n    test_loss = []\n    test_indices = [i for i in range(X_test.shape[0])]\n    for i in test_indices:\n        loss, y_test_pred = self.forward(X_test[i], W_trained, y_test[i])\n        test_pred.append(y_test_pred)\n        test_loss.append(loss)\n    return test_pred, test_loss\n    \n\n  def plotLoss(self, loss, epochs):\n    \"\"\"\n    Parameters:\n    loss (list) : Calculated Sqaured Error Loss for y and y_pred\n    epochs (list): Number of Epochs\n\n    Returns: None\n    Plots a graph of Loss vs Epochs\n    \"\"\"\n    plt.plot(epochs, loss)\n    plt.xlabel('Number of Epochs')\n    plt.ylabel('Loss')\n    plt.title('Plot Loss')\n    plt.show()\n  \n","59d91aa2":"#Splitting the dataset\nX_train, y_train, X_test, y_test = split_data(X,y)","7982735e":"#declaring the \"regressor\" as an object of the class LinearRegression\nregressor = linearRegression()","1ddad706":"#Training \nW_trained, train_loss, num_epochs = regressor.train(X_train, y_train, epochs=1000, alpha=0.0001)","bbb1384e":"#Testing on the Test Dataset\ntest_pred, test_loss = regressor.test(X_test, y_test, W_trained)","6f73f0a5":"#Plot the Train Loss\nregressor.plotLoss(train_loss, num_epochs)","d3e2a1d6":"print(test_loss)","bb59fb1e":"import pandas\nimport matplotlib.pyplot as plt \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_absolute_error","f2f69e58":"dataset_sk = pd.read_csv(r'..\/input\/salary-databysuperdatascienceteam\/Salary_Data.csv')\nX_sk = dataset_sk.iloc[:, :-1].values\ny_sk = dataset_sk.iloc[:, 1].values","9e695f7d":"# Splitting the dataset into the Training set and Test set\nX_train_sk, X_test_sk, y_train_sk, y_test_sk = train_test_split(X_sk, y_sk, test_size = 0.2, random_state = 0)","2aab24ae":"# Fitting Simple Linear Regression to the Training set\nregressor_sk = LinearRegression()\nregressor_sk.fit(X_train_sk, y_train_sk)","1c49f4b3":"# Predicting the Test set results\ny_pred = regressor_sk.predict(X_test_sk)","3c43cc2d":"# Visualising the Training set results\nplt.scatter(X_train_sk, y_train_sk, color = 'red')\nplt.plot(X_train_sk, regressor_sk.predict(X_train_sk), color = 'blue')\nplt.title('Salary vs Experience (Training set)')\nplt.xlabel('Years of Experience')\nplt.ylabel('Salary')\nplt.show()","823e5eec":"# Visualising the Test set results\nplt.scatter(X_test_sk, y_test_sk, color = 'red')\nplt.plot(X_train_sk, regressor_sk.predict(X_train_sk), color = 'blue')\nplt.title('Salary vs Experience (Test set)')\nplt.xlabel('Years of Experience')\nplt.ylabel('Salary')\nplt.show()","9a71b56b":"# MAE when the Sklearn Regressor is used\nmean_absolute_error(y_test_sk,regressor_sk.predict(X_test_sk))","f0dc4efa":"# MAE when the Scratch Implementation Regressor is used\nmean_absolute_error(y_test,test_pred)","620ff2b0":"### Performing Linear Regression","9a744cf0":"### Visualizing the Results","f24f0214":"## Linear Regression using Sklearn","e7589de9":"### Visualizing Results","9cfa6e3b":"### Performing the Linear Regression","6e6ae81d":"## Introduction","941ff002":"In this notebook, first, we implement Linear Regression from Scratch using Numpy without Sklearn. After the scratch implementation, we also implement the Linear Regression using Sklearn and compare the two models. The complete code is written and executed in Google Colab. No need of installing any additional packages is required. Download this notebook and upload to Google Colab to run it by yourself. Don't forget to grab your Datasets! \n\nThe Dataset used here is the **Salary_Data** dataset provided by the Super Data Science Team under their programme **Machine Learning A-Z: Hands on Python & R in Data Science**. Find the various datasets provided by them [here](https:\/\/www.superdatascience.com\/pages\/machine-learning).\n\n**Response Variable** - Years of Experience\n\n**Target Variable** - Salary\n\n**Equation Used** - y = W1 * X1 + W0 * X0\n\n**For Scratch Implementation:**\n    \n    Loss Function : Mean Squared Error\n\n    Optimization Algorithm : SGD\n\n    Weight Initialization : Xavier Initialization\n\n","babe1185":"### Get the Data and Data Preprocessing","30164679":"### Utility Methods","286644f7":"## Linear Regression from Scratch without SKlearn","3c54dd14":"### Coding the LinearRegression Class","aecf78bb":"## Comparing the Two Regressors","342dcb45":"### Data Preprocessing","9622d375":"### Importing dependencies","008d5a03":"# ML Regression Algorithms from Scratch - Linear Regression ","137da682":"There can be various reasons behind these results like choice of loss function, Optimization Algorithm used, etc. However, the prime goal of this notebook was to demonstrate the implementation of Simple Linear Regression and not to perform better than Sklearn. \n\nDefinitely, you can download this notebook and change hyperparameters, Optimization Algorithm, etc. and try to start your Machine Learning Journey.\n\n\n","4a32bab5":"### Importing the Dependencies"}}