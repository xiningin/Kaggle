{"cell_type":{"8cd9f679":"code","975c4369":"code","d1949000":"code","9002e3c4":"code","3fe0be7f":"code","9bdd581a":"code","3c1c8bd9":"code","6d17883f":"code","bc2331e5":"code","3ea7e188":"code","9257074c":"code","715e3b46":"code","ce311f3c":"code","aea45882":"code","162f0734":"code","16431eb1":"code","3a509cb2":"code","54a20400":"code","e2443a74":"code","d2a28278":"code","5b793f56":"code","df1cf975":"code","460287dd":"markdown","af5ea0cd":"markdown","42e4043b":"markdown","3ce44d79":"markdown","68f3998f":"markdown","57f2d1c9":"markdown","2691b889":"markdown","0152a56d":"markdown","5cacffe5":"markdown","2e1b5016":"markdown","5e1cae2a":"markdown","aa3011bb":"markdown"},"source":{"8cd9f679":"!pip3 install -qq optuna\nimport numpy as np \nimport pandas as pd\n\nimport xgboost\nimport catboost\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\nfrom lightgbm import LGBMRegressor\n\nfrom sklearn.linear_model import LinearRegression,Lasso, Ridge\nfrom sklearn.experimental import enable_hist_gradient_boosting \nfrom sklearn.ensemble import HistGradientBoostingRegressor, RandomForestRegressor,ExtraTreesRegressor\nfrom sklearn.model_selection import cross_val_score, KFold, train_test_split\nfrom sklearn.metrics import mean_squared_error\n\nfrom tqdm import tqdm\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport optuna\nsns.set()","975c4369":"train = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-jan-2021\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-jan-2021\/test.csv\")\nprint(train.shape)\nprint(test.shape)\n\nX, y = train.iloc[:,1:-1].values, train.iloc[:,-1].values\nfeat_names = list(train.columns[1:-1])\nkfold = KFold(n_splits=5,random_state=2021, shuffle=True)\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.15, random_state=2021)","d1949000":"train.iloc[:,1:].describe()","9002e3c4":"%%time\nfig, ax = plt.subplots(nrows=3, ncols=5, figsize=(30,15))\nfor i in range(15):\n    sns.distplot(train.iloc[:, i + 1], ax= ax[i \/\/ 5, i % 5])","3fe0be7f":"# autocorrelations\nfor i in range(1,16):\n    print(i,train.iloc[:,i].autocorr())","9bdd581a":"train.iloc[:,1:-1].diff().corrwith(train.iloc[:,-1])","3c1c8bd9":"fig, ax = plt.subplots(figsize=(20,10))\nsns.heatmap(train.iloc[:,1:].corr(),annot=True)","6d17883f":"fig, ax = plt.subplots(figsize=(20,10))\nsns.heatmap(train.iloc[:,1:].corr(method='spearman'),annot=True)","bc2331e5":"%%time\nfig, ax = plt.subplots(nrows=3, ncols=5, figsize=(30,15))\nfor i in range(14):\n    sns.scatterplot(x=X[:, i], y=y,ax= ax[i \/\/ 5, i % 5])","3ea7e188":"# Baseline \nbaseline_score = mean_squared_error(y,np.mean(y) * np.ones(y.shape[0]),squared=False)\n\nlr = LinearRegression()\nlr_scores = cross_val_score(lr,X, y, cv=kfold,scoring='neg_root_mean_squared_error')\n\nlasso = Lasso()\nlasso_scores = cross_val_score(lasso,X, y, cv=kfold,scoring='neg_root_mean_squared_error')\n\nridge = Ridge()\nridge_scores = cross_val_score(ridge,X, y, cv=kfold,scoring='neg_root_mean_squared_error')\n\nprint(-lr_scores, np.mean(-lr_scores))\nprint(-lasso_scores, np.mean(-lasso_scores))\nprint(-ridge_scores, np.mean(-ridge_scores))\n\nfig, ax = plt.subplots(ncols=4,nrows=2, figsize=(20, 10))\nridge.fit(X, y)X\nlr.fit(X,y)\nfeat_names2, coef_order = zip(*sorted(zip(feat_names,ridge.coef_), key=lambda k : abs(k[1]), reverse=True))\nsns.barplot(list(coef_order),list(feat_names2), ax=ax[0,0])\nsns.scatterplot(y - ridge.predict(X), y,ax=ax[0,1])\nsns.distplot(y - ridge.predict(X),ax=ax[0,2])\nsns.distplot(ridge.predict(X),ax=ax[0,3])\nfeat_names2, coef_order2 = zip(*sorted(zip(feat_names,lr.coef_), key=lambda k : abs(k[1]), reverse=True))\nsns.barplot(list(coef_order2),list(feat_names2), ax=ax[1,0])\nsns.scatterplot(y - lr.predict(X), y,ax=ax[1,1])\nsns.distplot(y - lr.predict(X),ax=ax[1,2])\nsns.distplot(lr.predict(X),ax=ax[1,3])\nprint(\"Ridge on full dataset\",mean_squared_error(y, ridge.predict(X), squared=False))","9257074c":"# %%time\n# hr = HistGradientBoostingRegressor(max_iter=750, max_depth=None, early_stopping=True, validation_fraction=0.1,\n#                                    learning_rate=0.08, max_leaf_nodes=128,l2_regularization=0, random_state=2021)\n# hr_scores = cross_val_score(hr,X, y, cv=kfold,scoring='neg_root_mean_squared_error')\n# print(-hr_scores, -np.mean(hr_scores))\n\n# %%time \n# from sklearn.model_selection import GridSearchCV\n# # params = {'learning_rate' : [0.05, 0.06, 0.07, 0.08, 0.1]}\n# # params = {'max_iter':[500, 750, 1000], \"max_depth\":[5,8, None]}\n# params = {'max_leaf_nodes':[64, 128, 256], \"max_depth\":[5,8, None]}\n\n\n# hr = HistGradientBoostingRegressor(learning_rate=0.08,l2_regularization=0, max_iter=750,\n#                                    early_stopping=True,validation_fraction=0.1,\n#                                    random_state=2021)\n\n\n\n# grid_search = GridSearchCV(hr, \n#                            param_grid = params, \n#                            cv=kfold,\n#                            scoring = 'neg_root_mean_squared_error', \n#                            n_jobs = -1, \n#                            verbose = 0)\n# grid_search.fit(X, y)\n\n# pd.DataFrame(grid_search.cv_results_)","715e3b46":"# %%time\n# xgbr = XGBRegressor(n_estimators=500, max_depth=3, learning_rate=0.02, objective=\"reg:squarederror\")\n# xgbc_scores = cross_val_score(xgbc,X, y, cv=kfold,scoring='neg_root_mean_squared_error')\n# print(-xgbc_scores)\n\n# %%time\n# xgb_params = {'lambda': 0.0030282073258141168, 'alpha': 0.01563845128469084, 'colsample_bytree': 0.5,\n#              'subsample': 0.7,'n_estimators': 4000, 'learning_rate': 0.01,'max_depth': 15,\n#              'random_state': 2020, 'min_child_weight': 257}\n\n# xgbr = XGBRegressor(**xgb_params)\n# xgbr.fit(X, y)","ce311f3c":"# %%time\n# cr = CatBoostRegressor(iterations=1000,verbose=False)\n# cr_scores = cross_val_score(cr,train.iloc[:,1:-1], train.iloc[:,-1], cv=kfold,scoring='neg_root_mean_squared_error')\n# print(-cr_scores, np.mean(-cr_scores))","aea45882":"# %%time\n# from lightgbm import LGBMRegressor\n\n# lb_params = {'learning_rate':0.005, 'num_iterations':5000,'objective': 'regression','metric': 'rmse','verbosity': -1,\n#              'boosting_type': 'gbdt','feature_pre_filter': False,'lambda_l1': 4.616521116348607,'lambda_l2': 1.9781272803424497,\n#              'num_leaves': 102,'feature_fraction': 0.4,'bagging_fraction': 1.0,'bagging_freq': 0,'min_child_samples': 20, \"seed\":2021}\n# lb = LGBMRegressor(**lb_params)\n# lb_scores = cross_val_score(lb,X, y, cv=kfold,scoring='neg_root_mean_squared_error')\n# print(-lb_scores, np.mean(-lb_scores))","162f0734":"# from catboost import CatBoostRegressor     \n\n# def objective(trial):\n#     param = {\n#         'learning_rate': trial.suggest_categorical('learning_rate', [0.01, 0.03, 0.05, 0.08, 0.1]),\n#         'iterations': trial.suggest_categorical(\"iterations\",[750, 1000, 2000]),\n#         'max_depth': trial.suggest_int(\"depth\", 5, 12),\n#         \"random_strength\": trial.suggest_int(\"random_strength\", 0, 100),\n#         'random_state': 2021,\n#         \"verbose\":0,\n#         \"task_type\":\"GPU\"\n#     }\n#     cbr = CatBoostRegressor(**param)  \n        \n#     cbr_scores = cross_val_score(cbr,X, y, cv=kfold,scoring='neg_root_mean_squared_error')\n    \n#     return np.mean(-cbr_scores)\n\n# study = optuna.create_study(direction=\"minimize\")\n# study.optimize(objective, n_trials=50)","16431eb1":"%%time\n# {'learning_rate': 0.01, 'iterations': 2000, 'depth': 12, 'random_strength': 52}\n\n\n# n_seeds = 10\n# rand_seeds = np.random.randint(2048, size=n_seeds)\n# print(rand_seeds)    \n\ni = 0\nfor train_index, test_index in tqdm(kfold.split(X)):\n    cr = CatBoostRegressor(verbose=False,iterations=2000, learning_rate=0.01, \n                           random_strength=52, max_depth=12,random_seed=2021)\n    cr.fit(X[train_index,:], y[train_index])\n    train[f'target{i}'] = cr.predict(X)\n    test[f'target{i}'] = cr.predict(test.iloc[:,1:].values)\n    i += 1\ntrain['target_final'] = train.loc[:,[\"target\"+str(i) for i in range(5)]].mean(axis=1)\nprint(\"In Sample: \", mean_squared_error(y, train['target_final'], squared=False))\ntest['target'] = test[[\"target\"+str(i) for i in range(5)]].mean(axis=1)\ntest[['id','target']].to_csv(\"submission.csv\",index=False)","3a509cb2":"fig, ax = plt.subplots(ncols=4,figsize=(20, 5))\nfeat_names2, coef_order = zip(*sorted(zip(feat_names,cr.get_feature_importance()), key=lambda k : abs(k[1]), reverse=True))\nsns.barplot(list(coef_order),list(feat_names2), ax=ax[0])\nsns.scatterplot(y - train['target_final'], y,ax=ax[1])\nsns.distplot(y - train['target_final'],ax=ax[2])\nsns.distplot(train['target_final'],ax=ax[3])\n\n# explainer = shap.TreeExplainer(model)\n# shap_values = explainer.shap_values(X)","54a20400":"# import tensorflow as tf\n# from tensorflow import keras\n# from tensorflow.keras import layers\n# from keras.utils import to_categorical\n# from tensorflow.keras import callbacks\n# from tensorflow.keras.optimizers import Adam\n# from tensorflow.keras.layers import Dense, Input\n# from sklearn.model_selection import train_test_split\n\n# inputs = Input(shape=(14,))\n# x = layers.Dense(128, activation=\"relu\")(inputs)\n# x = layers.Dense(64, activation=\"relu\")(x)\n# output = layers.Dense(1)(x)\n# model = keras.Model(inputs, output)    \n\n# model.compile(Adam(lr=1e-3), \"mse\", metrics=[\"mse\", tf.keras.metrics.RootMeanSquaredError()])\n\n# X_train, X_test, y_train, y_test = train_test_split(train.iloc[:,1:-1],train.iloc[:,-1],train_size=0.8, random_state=2021)\n# history = model.fit(X_train, y_train, batch_size=64, epochs=10)","e2443a74":"# # from sklearn.gaussian_process import GaussianProcessRegressor\n# # gpr = GaussianProcessRegressor()\n# # gpr_scores = cross_val_score(gpr,X, y, cv=kfold,scoring='neg_root_mean_squared_error')","d2a28278":"# from sklearn.kernel_ridge import KernelRidge\n# kr = KernelRidge()\n# kr_scores = cross_val_score(kr,X, y, cv=kfold,scoring='neg_root_mean_squared_error')","5b793f56":"# %%time\n# rf = RandomForestRegressor(n_estimators=500,max_depth=5)\n# rf_scores = cross_val_score(rf,X, y, cv=kfold,scoring='neg_root_mean_squared_error')\n# print(-hr_scores, -np.mean(rf_scores), -np.std(hr_scores))","df1cf975":"# %%time\n# rf = ExtraTreesRegressor(n_estimators=500, max_depth=5)\n# rf_scores = cross_val_score(rf,X, y, cv=kfold,scoring='neg_root_mean_squared_error')\n# print(-hr_scores, -np.mean(rf_scores), -np.std(hr_scores))","460287dd":"## Model Interpretabilitlity\n\nCould use Shap or Feature Importances\n\n**Catboost**\n+ All features look important\n+ Residuals still look bimodal\n+ https:\/\/shap.readthedocs.io\/en\/latest\/example_notebooks\/tabular_examples\/tree_based_models\/Catboost%20tutorial.html","af5ea0cd":"# Trees\n\nUnable to get 5 fold cross validation for Random Forest and Extra Trees to run in a reasonable amount of time. The previous results with linear\/Ridge regression also suggest that column subsampling may be less effective than using all features (so perhaps Boosting would be better)","42e4043b":"# Ensembling\n\n+ Ensembling from different models (XGB + CatBoost + LGBM) : https:\/\/www.kaggle.com\/shkanda\/ensemble-lgb-xgb-cat\n\n+ Averaging predictions from a model trained on K-folds : https:\/\/www.kaggle.com\/shogosuzuki\/0-69713-lightgbm-with-small-learning-rate\n\n+ Averaging from a model using different seeds?","3ce44d79":"## Bivariate Analysis\n\n+ The features appear to have a low correlation in both pearson, spearman with the target\n\n+ Some clusters of correlated features","68f3998f":"# Gradient Boosted Decision Trees\n\nSome experiments on CPU, evaluating RMSE using 5-fold CV, and fixed parameters \/ parameters from other peoples' notebooks\n\n| | XGB | CatBoost | LGBM | HistGradientBoosting |\n| -- | -- | -- | -- | -- |\n| **RMSE** | 0.70263173 | 0.6989878380818764 | 0.7001857209114114 | 0.700016686352503 |\n| **time** | 23min 14s| 5min 47s |  9.48 s  |  31s |\n| **params** | | iterations = 1000 | |  max_iter = 1000, learning_rate=0.08, max_depth |\n\n\nhttps:\/\/towardsdatascience.com\/catboost-vs-light-gbm-vs-xgboost-5f93620723db\n\n### XGBoost\n+ comparably slower on CPU\n+ https:\/\/www.kaggle.com\/hamzaghanmi\/xgboost-hyperparameter-tuning-using-optuna\n\n### CatBoost\n+ https:\/\/catboost.ai\/docs\/concepts\/python-reference_parameters-list.html\n+ https:\/\/catboost.ai\/docs\/concepts\/parameter-tuning.html\n\n\n### LightGBM\n\n+ https:\/\/lightgbm.readthedocs.io\/en\/latest\/Parameters-Tuning.html\n\n### scikit-learn's HistGradientBoosting \n\n+ https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.HistGradientBoostingRegressor.html#sklearn.ensemble.HistGradientBoostingRegressor.\n+ optimised some parameters one by one using GridSearchCV\n+ Quite fast, but seems to be performance seems to be capped at 0.7\n\n\n\n\n\n\n\n# Hyperparameter tuning\n\nThe most important hyperparmeters are probably:\n+ N_estimators \/ iterations - number of trees in boosting. greater number of trees might decrease bias but lead to overfitting\n+ max_depth - depth of each tree; complexity of the model\n+ max_leaf_nodes - lower max_leaf_nodes might imply more regularisation\n+ learning_rate - higher learning rate speeds up training\n+ l2_regularization -\n\nHow?\n\n+ Select using 5-Fold Cross Validation with GridSearchCV, RandomizedSearchCV\n+ many public notebooks favour `optuna`\n\n## Optuna\n\n+ https:\/\/www.kaggle.com\/bowaka\/tps21-optuna-lgb-fast-hyper-parameter-tunning","57f2d1c9":"## Other Models\n\n+ Gaussian Process - runs out of memory using sklearn\n+ Kernel Ridge - runs out of memory\n+ KNN, SVR - will likely have the same problems","2691b889":"## Public Leaderboard\n\n| **Method** | **params** | in-sample | **public leaderboard** |\n| -- | -- | -- | -- |\n| CatBoost + Averaging using 10 random seeds |  iterations = 500, max_depth= 8|  | 0.69856 | \n","0152a56d":"## Univariate Analysis\n\n+ The features all appear to come from non-normal distributions. Target appears to be bimodal, possibly from a mixture of gaussians\n\n+ + Features are roughly between 0 - 1, Target is roughly 0 - 10 \n\n\n+ Some have suggested that the data may be *time series*. The autocorrelation between for each column is low, and the correlation between `y` and the differenced Xs `X_{i + 1} - X_{i}` is also low so this is likely not the case\n\n+ https:\/\/www.kaggle.com\/c\/tabular-playground-series-jan-2021\/discussion\/210484\nsuggests that the distribution of the features in the training and testing sets is the same, so optimising for hyperparameters using cross validation in the training set is key\n","5cacffe5":"# Data Exploration\n\n+ Train has 300000 observations with 14 features, test has 200000 observations to predict\n\n+ There are no missing features\n\n+ Some suggest removing the outlier `y = 0` for preprocessing\n\n","2e1b5016":"# Modelling\n\n\n\nOthers have suggested a 2-step approach:\n+ Use unsupervised learning: a Gaussian Mixture Model to determine 2 clusters \/ mixtures for `y` \n+ Predict E[Y | cluster = i] usin a regression model\n\n+ https:\/\/www.kaggle.com\/iamleonie\/handling-multimodal-distributions-fe-techniques\n\n+ https:\/\/www.kaggle.com\/chrisbradley\/tab-playground-predicting-bimodal-distribution\n\n\nEvaluate models using 5 fold CV to see if there are any that seem to be better without parameter tuning\n\n\n## Linear Models\n\n\n\n| | Baseline |  Linear Regression | Lasso | Ridge |\n| -- | -- | -- | -- | -- |\n| **RMSE** | 0.7330696085805828 | 0.7262295101854237| 0.7330691414275544 | 0.7262363800486569 | \n\nhttps:\/\/scikit-learn.org\/stable\/modules\/linear_model.html#linear-model\n\n\nThe baseline is to set all the predictions to the in-sample mean: let $\\hat{y} = \\bar{y}. $For Ridge and Linear Regression, the residuals look almost the same as the original y. Ridge performs slightly better, so this may suggest that all features have *some* information as opposed to shrinking them to 0 via Lasso. Fitting on the full dataset, lasso shrinks all the coefficients to 0 (i.e. predicting mean). Overall, a nonlinear learner may be better.\n\n","5e1cae2a":"# Objective\n\nThe Objective of this task is to build a regression model to predict a target from a set of 14 features. The features are anonymised and are all *continuous*. The scoring is metric the Root Mean Squared Error (RMSE) : $$RMSE = \\sqrt{\\sum_{i = 1}^{N} (y_{i} - \\hat{y}_{i})^{2}} $$","aa3011bb":"# Other approaches\n\n## Deep Neural Network\n\nDNNs could be a worthwhile approach but have yet to explore"}}