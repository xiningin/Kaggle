{"cell_type":{"b9482f64":"code","13a10d34":"code","c0f674f1":"code","110957be":"code","8bdad27f":"code","e722c73a":"code","f54c6366":"code","e3de863f":"code","56d93736":"code","189b2ff8":"code","115a3630":"code","35488b54":"code","c3a904a0":"code","e76ab4a1":"code","97af786b":"code","a9b992a7":"code","a6f6c134":"code","d04ccaa5":"code","d47b5082":"code","ead71b18":"code","257fd127":"code","c40ec5f3":"code","1d396d7b":"code","fa982026":"code","4b2cb055":"code","128591e2":"markdown","dca87a5b":"markdown","3c687202":"markdown","2921f652":"markdown","89a056bf":"markdown","40cf29e7":"markdown","4f556919":"markdown","5b758ff2":"markdown","427e2f57":"markdown","b319aa74":"markdown","367b53d4":"markdown","0362fe84":"markdown"},"source":{"b9482f64":"pip install pretrainedmodels","13a10d34":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport os, collections\nfrom typing import Callable, List, Tuple \nfrom pathlib import Path\n\n\nimport albumentations as albu\nimport pretrainedmodels\nfrom albumentations.pytorch import ToTensor\n\nfrom catalyst.utils import split_dataframe_train_test\nfrom catalyst import utils\nfrom catalyst.data import ScalarReader, ReaderCompose, ImageReader\nfrom catalyst.data import Augmentor\nfrom catalyst.dl.runner import SupervisedRunner\nfrom catalyst.dl import utils\nfrom catalyst.dl import AUCCallback, F1ScoreCallback\nfrom catalyst.dl.callbacks import EarlyStoppingCallback, AccuracyCallback\nfrom catalyst.utils import imread\nfrom catalyst.contrib.nn import OneCycleLRWithWarmup\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn.functional import softmax\n\n","c0f674f1":"path = [\"\/kaggle\/input\/blood-cells\/dataset2-master\/dataset2-master\/images\/TEST_SIMPLE\", \n        \"\/kaggle\/input\/blood-cells\/dataset2-master\/dataset2-master\/images\/TRAIN\" ] \ndirectory = ['EOSINOPHIL', 'LYMPHOCYTE', 'MONOCYTE', 'NEUTROPHIL']\n\nis_fp16_used = False","110957be":"class_path_label = []\n\nfor p in path:\n    for direc in directory:\n        full_path = os.path.join(p, direc)\n        n_cls = directory.index(direc)\n        \n        for img in os.listdir(full_path):\n            try:\n                full_img_path = os.path.join(full_path, img)\n                class_path_label.append([direc, full_img_path, n_cls])\n                \n            except Exception as e:\n                pass \n            ","8bdad27f":"df = pd.DataFrame(class_path_label, columns=['class', 'filepath', 'label'])\ndf = df.sample(frac=1).reset_index(drop=True)\ndf.head()","e722c73a":"sns.countplot(x=df.label)","f54c6366":"train_data, valid_data = split_dataframe_train_test(df, test_size=0.2, random_state=42)\n\ntrain_data, valid_data = (train_data.to_dict('records'), valid_data.to_dict('records'))","e3de863f":"len(valid_data)","56d93736":"# ReaderCompose collects different Readers into one pipeline\nopen_fn = ReaderCompose([\n    \n    # Reads images from the `rootpath` folder \n    # using the key `input_key =\" filepath \"` (here should be the filename)\n    # and writes it to the output dictionary by `output_key=\"features\"` key\n    ImageReader(\n        input_key=\"filepath\",\n        output_key=\"features\",\n        rootpath=path\n    ),\n    \n    # Reads a number from our dataframe \n    # by the key `input_key =\" label \"` to np.long\n    # and writes it to the output dictionary by `output_key=\"targets\"` key\n    ScalarReader(\n        input_key=\"label\",\n        output_key=\"targets\",\n        default_value=-1,\n        dtype=np.int64\n    ),\n    \n    # Same as above, but with one encoding\n    ScalarReader(\n        input_key=\"label\",\n        output_key=\"targets_one_hot\",\n        default_value=-1,\n        dtype=np.int64, \n        one_hot_classes=4\n    )\n])\n","189b2ff8":"BORDER_CONSTANT = 0\nBORDER_REFLECT = 2\n\ndef pre_transforms(image_size=128):\n    # Convert the image to a square of size image_size x image_size\n    # (keeping aspect ratio)\n    result = [\n        albu.LongestMaxSize(max_size=image_size),\n        albu.PadIfNeeded(image_size, image_size, border_mode=BORDER_CONSTANT)\n    ]\n    \n    return result\n\ndef hard_transforms():\n    result = [\n        # Random shifts, stretches and turns with a 50% probability\n        albu.ShiftScaleRotate( \n            shift_limit=0.1,\n            scale_limit=0.1,\n            rotate_limit=15,\n            border_mode=BORDER_REFLECT,\n            p=0.5\n        ),\n        albu.IAAPerspective(scale=(0.02, 0.05), p=0.3),\n        # Random brightness \/ contrast with a 30% probability\n        albu.RandomBrightnessContrast(\n            brightness_limit=0.2, contrast_limit=0.2, p=0.3\n        ),\n        # Random gamma changes with a 30% probability\n        albu.RandomGamma(gamma_limit=(85, 115), p=0.3),\n        # Randomly changes the hue, saturation, and color value of the input image\n        albu.HueSaturationValue(p=0.3),\n        albu.JpegCompression(quality_lower=80),\n    ]\n    \n    return result\n\ndef post_transforms():\n    # we use ImageNet image normalization\n    # and convert it to torch.Tensor\n    return [albu.Normalize(), ToTensor()]\n\ndef compose(transforms_to_compose):\n    # combine all augmentations into one single pipeline\n    result = albu.Compose([\n      item for sublist in transforms_to_compose for item in sublist\n    ])\n    return result\n","115a3630":"train_transforms = compose([\n    pre_transforms(), \n    hard_transforms(), \n    post_transforms()\n])\nvalid_transforms = compose([pre_transforms(), post_transforms()])\n\nshow_transforms = compose([pre_transforms(), hard_transforms()])\n\n# Takes an image from the input dictionary by the key `dict_key` \n# and performs `train_transforms` on it.\ntrain_data_transforms = Augmentor(\n    dict_key=\"features\",\n    augment_fn=lambda x: train_transforms(image=x)[\"image\"]\n)\n\n\n# Similarly for the validation part of the dataset. \n# we only perform squaring, normalization and ToTensor\nvalid_data_transforms = Augmentor(\n    dict_key=\"features\",\n    augment_fn=lambda x: valid_transforms(image=x)[\"image\"]\n)","35488b54":"def get_loaders(\n    open_fn: Callable,\n    train_transforms_fn,\n    valid_transforms_fn,\n    batch_size: int = 64, \n    num_workers: int = 4,\n    sampler = None\n) -> collections.OrderedDict:\n    \"\"\"\n    Args:\n        open_fn: Reader for reading data from a dataframe\n        train_transforms_fn: Augmentor for train part\n        valid_transforms_fn: Augmentor for valid part\n        batch_size: batch size\n        num_workers: How many subprocesses to use to load data,\n        sampler: An object of the torch.utils.data.Sampler class \n            for the dataset data sampling strategy specification\n    \"\"\"\n    train_loader = utils.get_loader(\n        train_data,\n        open_fn=open_fn,\n        dict_transform=train_transforms_fn,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        shuffle=sampler is None, # shuffle data only if Sampler is not specified (PyTorch requirement)\n        sampler=sampler,\n        drop_last=True,\n    )\n\n    valid_loader = utils.get_loader(\n        valid_data,\n        open_fn=open_fn,\n        dict_transform=valid_transforms_fn,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        shuffle=False, \n        sampler=None,\n        drop_last=True,\n    )\n\n    # Catalyst expects an ordered dictionary with train\/valid\/infer loaders. \n    # The number of loaders can vary.\n    # For example, it can easily handle even some complex logic like:\n    # loaders[\"train_dataset1\"] = train_loader_1\n    # loaders[\"train_dataset2\"] = train_loader_2\n    # ....\n    # loaders[\"valid_1\"] = valid_loader_1\n    # loaders[\"valid_2\"] = valid_loader_2\n    # ...\n    # loaders[\"infer_1\"] = infer_loader_1\n    # loaders[\"infer_2\"] = infer_loader_2\n    # ...\n    \n    loaders = collections.OrderedDict()\n    loaders[\"train\"] = train_loader\n    loaders[\"valid\"] = valid_loader\n\n    return loaders\n\nif is_fp16_used:\n    batch_size = 128\nelse:\n    batch_size = 64\n\nprint(f\"batch_size: {batch_size}\")\n\nloaders = get_loaders(\n    open_fn=open_fn, \n    train_transforms_fn=train_data_transforms,\n    valid_transforms_fn=valid_data_transforms,\n    batch_size=batch_size,\n)\n","c3a904a0":"class Resnet34(nn.Module):\n    def __init__(self):\n        super(Resnet34, self).__init__()\n        self.model = pretrainedmodels.__dict__['resnet34'](pretrained='imagenet')  \n        self.l0 = nn.Linear(512, 4)\n\n    def forward(self, x):\n        bs, c, h, w = x.shape\n        x = self.model.features(x) \n        x = F.adaptive_avg_pool2d(x, 1).reshape(bs, -1)\n        op_layer_one = self.l0(x)\n        return op_layer_one\n","e76ab4a1":"num_epochs = 20\nnum_classes = 4\nclass_names = ['EOSINOPHIL', 'LYMPHOCYTE', 'MONOCYTE', 'NEUTROPHIL']\ndevice = utils.get_device()\n\n\nif is_fp16_used:\n    fp16_params = dict(opt_level=\"01\") # params for fp16\nelse:\n    fp16_params = None\n\n    \n    \n# experiment setup\nlogdir = \".\/logs\/blood_cell_simple_notebook_1\"\n# model runner\nrunner = SupervisedRunner(device=device)\n\n# model, criterion, optimizer\nmodel = Resnet34()\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.0003)\n#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=2)\nscheduler = OneCycleLRWithWarmup(\n                            optimizer,\n                            num_steps=num_epochs, \n                            lr_range=(0.005, 0.00005),\n                            warmup_steps=2,\n                            momentum_range=(0.85, 0.95)\n                            )\n\n\ncallbacks = [\n    AccuracyCallback(\n                    num_classes=num_classes,\n                    prefix = 'accuracy',\n                    ),\n    EarlyStoppingCallback(\n                         patience=2, \n                         metric=\"loss\", \n                         minimize=True, \n                         min_delta=1e-06\n                         )\n]","97af786b":"# model training\nrunner.train(\n    model=model,\n    criterion=criterion,\n    optimizer=optimizer,\n    scheduler=scheduler, \n    # our dataloaders\n    loaders=loaders,\n    num_epochs=num_epochs,\n    # We can specify the callbacks list for the experiment;\n    # For this task, we will check accuracy, AUC and F1 metrics\n    callbacks=callbacks,\n    # path to save logs\n    logdir=logdir,\n    # save our best checkpoint by AUC metric\n    #main_metric=\"auc\/mean\",\n    # AUC needs to be maximized.\n    minimize_metric=False,\n    # for FP16. It uses the variable from the very first cell\n    fp16=fp16_params,\n    # prints train logs\n    verbose=True,\n    # pipeline check\n    #check=True\n)","a9b992a7":"utils.plot_metrics(logdir=logdir)","a6f6c134":"utils.plot_metrics(logdir=logdir, step=\"epoch\", metrics=[\"accuracy01\"])","d04ccaa5":"ROOT = [\"\/kaggle\/input\/blood-cells\/dataset2-master\/dataset2-master\/images\/TEST_SIMPLE\",\n        \"\/kaggle\/input\/blood-cells\/dataset2-master\/dataset2-master\/images\/TRAIN\"]\n\nl = []\n\nfor i, p in enumerate(ROOT):\n    if i==0:\n        ALL_IMAGES1 = list(Path(p).glob(\"**\/*.jpeg\"))\n        ALL_IMAGES1 = list(filter(lambda x: not x.name.startswith(\".\"), ALL_IMAGES1))\n    if i==1:\n        ALL_IMAGES2 = list(Path(p).glob(\"**\/*.jpeg\"))\n        ALL_IMAGES2 = list(filter(lambda x: not x.name.startswith(\".\"), ALL_IMAGES2))\n        \n        ALL_IMAGES = ALL_IMAGES1 + ALL_IMAGES2\n        \n\nlen(ALL_IMAGES)","d47b5082":"def show_examples(images: List[Tuple[str, np.ndarray]]):\n    _indexes = [(i, j) for i in range(2) for j in range(2)]\n    \n    f, ax = plt.subplots(2, 2, figsize=(16, 16))\n    for (i, j), (title, img) in zip(_indexes, images):\n        ax[i, j].imshow(img)\n        ax[i, j].set_title(title)\n    f.tight_layout()\n    \n    \ndef read_random_images(paths: List[Path]) -> List[Tuple[str, np.ndarray]]:\n    data = np.random.choice(paths, size=4)\n    result = []\n    for d in data:\n        title = f\"{d.parent.name}: {d.name}\"\n        _image = imread(d)\n        result.append((title, _image))\n    \n    return result\n    ","ead71b18":"images = read_random_images(ALL_IMAGES)\nshow_examples(images)","257fd127":"def show_prediction(\n    model: torch.nn.Module, \n    class_names: List[str], \n    titles: List[str],\n    images: List[np.ndarray],\n    device: torch.device) -> None:\n    \n    with torch.no_grad():\n        tensor_ = torch.stack([\n            valid_transforms(image=image)[\"image\"]\n            for image in images]).to(device)\n        \n        \n        logits = model(tensor_)\n        probabilities = softmax(logits, dim=1)\n        predictions = probabilities.argmax(dim=1)\n    \n    images_predicted_classes = [\n        (f\"predicted: {class_names[x]} | correct: {title}\", image)\n        for x, title, image in zip(predictions, titles, images)]\n    \n    show_examples(images_predicted_classes)\n    ","c40ec5f3":"titles, images = list(zip(*read_random_images(ALL_IMAGES)))\ntitles = list(map(lambda x: x.rsplit(\":\")[0], titles))\nshow_prediction(model, class_names=class_names, titles=titles, images=images, device=device)\n","1d396d7b":"predictions = np.vstack(list(map(\n    lambda x: x[\"logits\"].cpu().numpy(), \n    runner.predict_loader(loader=loaders[\"valid\"], resume=f\"{logdir}\/checkpoints\/best.pth\")\n)))\nprint(predictions.shape)\n\nprint(\"logits: \", predictions[0])\n\nprobabilities = torch.softmax(torch.from_numpy(predictions[0]), dim=0)\nprint(\"probabilities: \", probabilities)\n\nlabel = probabilities.argmax().item()\nprint(f\"predicted: {class_names[label]}\")\n","fa982026":"predictions = np.vstack(list(map(\n    lambda x: x[\"logits\"].cpu().numpy(), \n    runner.predict_loader(loader=loaders[\"valid\"], resume=f\"{logdir}\/checkpoints\/best.pth\")\n)))\n\nlabel_dictionary = {'label_name': [], 'predicted_label': []}\n\nfor i in range(len(predictions)):\n    probabilities = torch.softmax(torch.from_numpy(predictions[i]), dim=0)\n    label = probabilities.argmax().item()\n    \n    label_dictionary['predicted_label'].append(label)\n    label_dictionary['label_name'].append(class_names[label])\n    ","4b2cb055":"df= pd.DataFrame(label_dictionary)\ndf.head()","128591e2":"# Full path of every image","dca87a5b":"# Model Training","3c687202":"# Path","2921f652":"# Get Loader Function","89a056bf":"# Visualizing some images","40cf29e7":"# Model","4f556919":"# predicting with valid data","5b758ff2":"# Training and Loss Graph","427e2f57":"# Some Variable","b319aa74":"# Transforming Image","367b53d4":"# Data Reader Compose","0362fe84":"# Train and Valid dataset"}}