{"cell_type":{"29ac2d9b":"code","d7e78a1a":"code","7b66af38":"code","f66ddcd5":"code","ee60fbe0":"code","8ff38753":"code","d40388ba":"code","18b166c0":"code","29af1840":"code","b861d5b8":"code","ebbb770b":"code","96dd29ae":"code","f84dbf5c":"code","b401e31d":"code","2f044486":"code","774c3ab5":"code","26a17d67":"code","49475fa5":"code","987200a4":"code","38c70741":"code","0720317e":"code","fb722c74":"code","34121aab":"code","dfdb1eaa":"code","42e9b83e":"markdown","73759a78":"markdown","10205832":"markdown","5a90e0bd":"markdown","53a84b0d":"markdown","54a42d86":"markdown","82dab375":"markdown"},"source":{"29ac2d9b":"import os\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport shutil\nimport tqdm","d7e78a1a":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","7b66af38":"CONTENT_DIR = '\/kaggle\/content'\nTRAIN_DIR = CONTENT_DIR + '\/train'\nVALID_DIR = CONTENT_DIR + '\/valid'\n\nif not os.path.exists(CONTENT_DIR):\n    # Extract dataset\n    import zipfile\n    with zipfile.ZipFile('\/kaggle\/input\/dogs-vs-cats\/train.zip', 'r') as zipf:\n        zipf.extractall(CONTENT_DIR)\n\n    # Split cats and dogs images to train and valid datasets\n    img_filenames = os.listdir(TRAIN_DIR)\n    dog_filenames = [fn for fn in img_filenames if fn.startswith('dog')]\n    cat_filenames = [fn for fn in img_filenames if fn.startswith('cat')]\n    dataset_filenames = train_test_split(\n        dog_filenames, cat_filenames, test_size=0.1, shuffle=True, random_state=42\n    )\n\n    # Move images\n    make_dirs = [d + a for a in ['\/dog', '\/cat'] for d in [TRAIN_DIR, VALID_DIR]]\n    for dir, fns in zip(make_dirs, dataset_filenames):\n        os.makedirs(dir, exist_ok=True)\n        for fn in tqdm.tqdm(fns):\n            shutil.move(os.path.join(TRAIN_DIR, fn), dir)\n        print('elements in {}: {}'.format(dir, len(os.listdir(dir))))","f66ddcd5":"BATCH_SIZE = 32\nIMAGE_SHAPE = 128","ee60fbe0":"# make data generators\ntrain_generator = ImageDataGenerator(rescale=1.\/255)\nvalid_generator = ImageDataGenerator(rescale=1.\/255)\ntrain_data = train_generator.flow_from_directory(\n    directory=TRAIN_DIR,\n    target_size=(IMAGE_SHAPE, IMAGE_SHAPE),\n    batch_size=BATCH_SIZE,\n    class_mode='binary',\n    shuffle=False\n)\nvalid_data = valid_generator.flow_from_directory(\n    directory=VALID_DIR,\n    target_size=(IMAGE_SHAPE, IMAGE_SHAPE),\n    batch_size=BATCH_SIZE,\n    class_mode='binary',\n    shuffle=False\n)","8ff38753":"# load xception model\nxception_model = tf.keras.applications.xception.Xception(\n    include_top=False,\n    weights='imagenet',\n    input_shape=(IMAGE_SHAPE, IMAGE_SHAPE, 3),\n    pooling='avg'\n)","d40388ba":"# look at this, it's very scary :)\n# change include_top=True -> last layer added\n# tf.keras.utils.plot_model(xception_model, dpi=48, show_shapes=True)","18b166c0":"# bottleneck features for train dataset\ntrain_bottleneck = xception_model.predict_generator(\n    train_data, train_data.n \/\/ BATCH_SIZE, verbose=1\n)\n# np.save(open('train_bottleneck.np', 'wb'), train_bottleneck)","29af1840":"# bottleneck features for valid dataset\nvalid_bottleneck = xception_model.predict_generator(\n    valid_data, valid_data.n \/\/ BATCH_SIZE, verbose=1\n)\n# np.save(open('valid_bottleneck.np', 'wb'), valid_bottleneck)","b861d5b8":"# create simple model for classification\nmodel = tf.keras.models.Sequential([\n    Dense(units=256, activation='relu', input_shape=xception_model.output_shape[1:]),\n    Dropout(0.5),\n    Dense(units=128, activation='relu'),\n    Dropout(0.5),\n    Dense(units=2, activation='softmax')\n])","ebbb770b":"model.summary()","96dd29ae":"model.compile(\n    optimizer='adam',\n    loss='sparse_categorical_crossentropy',\n    metrics=['accuracy']\n)","f84dbf5c":"# get predicted features and classify it\nEPOCHS = 10\nhistory = model.fit(\n    x=train_bottleneck,\n    y=train_data.labels[:len(train_bottleneck)],\n    batch_size=BATCH_SIZE,\n    epochs=EPOCHS,\n    validation_data=(valid_bottleneck, valid_data.labels[:len(valid_bottleneck)])\n)","b401e31d":"def show_graphs(history):\n    plt.figure(figsize=(12, 8))\n\n    plt.subplot(1, 2, 1)\n    plt.plot(history.history['accuracy'], label='train')\n    plt.plot(history.history['val_accuracy'], label='valid')\n    plt.legend(loc='lower right')\n    plt.title('Accuracy')\n\n    plt.subplot(1, 2, 2)\n    plt.plot(history.history['loss'], label='train')\n    plt.plot(history.history['val_loss'], label='valid')\n    plt.legend(loc='upper left')\n    plt.title('Loss (sparse_categorical_crossentropy)')\n\n    plt.show()\n    \nshow_graphs(history)","2f044486":"mobilenet_model = tf.keras.applications.mobilenet.MobileNet(\n    include_top=False,\n    weights='imagenet',\n    input_shape=(IMAGE_SHAPE, IMAGE_SHAPE, 3),\n    pooling='avg'\n)\nmobilenet_model.trainable = False # freeze convolutional layers","774c3ab5":"# new fully connected part of the network\ndense_model = tf.keras.models.Sequential([\n    Dense(units=1000, activation='relu'),\n    Dropout(0.5),\n    Dense(units=128, activation='relu'),\n    Dropout(0.5),\n    Dense(units=2, activation='softmax')\n])","26a17d67":"# build a new model\nmodel2 = tf.keras.models.Sequential([\n    mobilenet_model,\n    dense_model\n])","49475fa5":"model2.compile(\n    optimizer='adam',\n    loss='sparse_categorical_crossentropy',\n    metrics=['accuracy']\n)","987200a4":"EPOCHS = 10\ntrain_data.reset()\nvalid_data.reset()\nhistory = model2.fit_generator(\n    train_data,\n    steps_per_epoch=train_data.n \/\/ BATCH_SIZE,\n    epochs=EPOCHS,\n    validation_data=valid_data,\n    validation_steps=valid_data.n \/\/ BATCH_SIZE\n)","38c70741":"show_graphs(history)","0720317e":"IMAGE_SHAPE = 224\nexample_data = train_generator.flow_from_directory(\n    directory=TRAIN_DIR,\n    target_size=(IMAGE_SHAPE, IMAGE_SHAPE),\n    batch_size=BATCH_SIZE,\n    class_mode='binary',\n    shuffle=True\n)\nexample_x, example_y = example_data.next()\nexample_classes = list(example_data.class_indices.keys())\nexample_y_classes = [example_classes[int(i)] for i in example_y]","fb722c74":"mobilenet_native = tf.keras.applications.mobilenet.MobileNet(\n    include_top=True,\n    weights='imagenet',\n    input_shape=(IMAGE_SHAPE, IMAGE_SHAPE, 3),\n    pooling='avg'\n)\nmobilenet_native.compile()\nexample_pred = mobilenet_native.predict(\n    example_x\n)","34121aab":"labels_path = tf.keras.utils.get_file(\n    'ImageNetLabels.txt',\n    'https:\/\/storage.googleapis.com\/download.tensorflow.org\/data\/ImageNetLabels.txt'\n)\nimagenet_labels = np.array(open(labels_path).read().splitlines())\nresult = imagenet_labels[np.argmax(example_pred, axis=1)]","dfdb1eaa":"NUM_ROWS = 5\nNUM_COLS = 5\nNUM_IMAGES = NUM_COLS * NUM_ROWS\nplt.figure(figsize=(2*NUM_COLS, 2*NUM_ROWS))\nfor i in range(NUM_IMAGES):\n    plt.subplot(NUM_ROWS, NUM_COLS, i + 1)\n    plt.grid(False)\n    plt.xticks([])\n    plt.yticks([])\n    plt.imshow(example_x[i], cmap=plt.cm.binary)\n    plt.xlabel('{} {:.0%}\\n({})'.format(result[i], np.max(example_pred[i]), example_y_classes[i]))\nplt.tight_layout()\nplt.show()","42e9b83e":"# Load data","73759a78":"# MobileNet with native output layer\n\nOnly for fun :)\n\n**Achtung! Attention! Vnimanie!** If your datasets don't match very well, that's a bad idea!","10205832":"# Preprocessing","5a90e0bd":"**Transfer Learning via Xception model (Dogs vs. Cats)**\n\n* Part 1 [Intro to CNN (Dogs vs. Cats)](https:\/\/www.kaggle.com\/imcr00z\/intro-to-cnn-dogs-vs-cats)\n* Part 2 [Intro to CNN: Augmentation & Dropout](https:\/\/www.kaggle.com\/imcr00z\/intro-to-cnn-augmentation-dropout)\n* Part 3 [Transfer Learning (Dogs vs. Cats) 98% acc.](https:\/\/www.kaggle.com\/imcr00z\/transfer-learning-dogs-vs-cats-98-acc)\n\nIn this part i use transfer learning and pretrained models:\n* Xception (from Franc\u00b8ois Chollet). Original document [here](https:\/\/arxiv.org\/pdf\/1610.02357.pdf)\n* MobileNet via Google. Original document [here](https:\/\/arxiv.org\/pdf\/1704.04861.pdf)","53a84b0d":"# MobileNet\nIn this case i use classical path with frozen layers.","54a42d86":"# Xception\nIn this part, I extract 'bottleneck features' from an Xception model without fully connected layers and train a new fully connected model on them.\n1. load and train xception, get bottleneck features\n2. create model with dense layers\n3. train dense layers on bottleneck features","82dab375":"Look to validation loss and accuracy."}}