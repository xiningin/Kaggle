{"cell_type":{"c1ed8340":"code","c994f82e":"code","f78af64d":"code","cfccd13f":"code","5177fc08":"code","af3a6ee4":"code","ed9c277e":"markdown"},"source":{"c1ed8340":"%%bash\npip install 'kaggle-environments>=0.1.6'","c994f82e":"!pip install git+git:\/\/github.com\/wau\/keras-rl2.git --upgrade --no-deps","f78af64d":"%%writefile submission.py\n\nimport numpy as np\nimport pandas as pd\nimport random\nimport sys\nfrom os import mkdir\nfrom os.path import exists\nfrom sys import exc_info\nfrom enum import Enum, IntEnum, auto\nfrom collections import Counter\nfrom functools import reduce\nimport gym\nfrom gym import spaces\nfrom gym.utils import seeding\nfrom rl.agents.dqn import DQNAgent\nfrom rl.policy import BoltzmannQPolicy\nfrom rl.memory import SequentialMemory\nfrom rl.callbacks import TrainIntervalLogger, TrainEpisodeLogger\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\n\n\nclass Hand(IntEnum):\n    GOO = 0\n    CHOKI = 1\n    PAH = 2\n    \n    \nclass Strategy(Enum):\n    RANDOM = auto()\n    HUMAN = auto()\n    \n    \nclass RandomStrategy:\n    hands = [Hand.GOO, Hand.CHOKI, Hand.PAH]\n\n    def __init__(self, np_random):\n        self.np_random = np_random\n\n    def get_hand(self, user_hand):\n        return self.np_random.choice(RandomStrategy.hands)\n\n    def reset(self):\n        pass\n\n\nclass HumanStrategy:\n    ratio = [0.350, 0.317, 0.333]\n    hands = [Hand.GOO, Hand.CHOKI, Hand.PAH]\n\n    def __init__(self, np_random):\n        self.np_random = np_random\n\n    def get_hand(self, user_hand):\n        return self.np_random.choice(HumanStrategy.hands, p=HumanStrategy.ratio)\n\n    def reset(self):\n        pass\n    \n    \nclass RPS(gym.Env):\n    \n    action_space = spaces.Discrete(3)\n    reward_range = [0, 1]\n    observation_space = spaces.Box(low=0, high=100, shape=(2,), dtype='float32')\n\n    def __init__(self, strategy=Strategy.HUMAN):\n        super().__init__()\n\n        self.seed()\n\n        self.strategy = None\n        if strategy == Strategy.RANDOM:\n            self.strategy = RandomStrategy(self.np_random)\n            \n        self.user_hands = []\n        self.enemy_hands = []\n        self.reset()\n\n    def reset(self):\n        self.user_hand = Hand.GOO\n        self.enemy_hand = Hand.GOO\n        self.strategy.reset()\n        self.done = False\n        observation = [self.user_hand, self.enemy_hand]\n        return observation\n\n    def seed(self, seed=None):\n        self.np_random, seed = seeding.np_random(seed)\n        return [seed]\n\n    def step(self, action):\n        self.enemy_hand = self.strategy.get_hand(self.user_hand)\n        self.enemy_hands += [self.enemy_hand]\n        self.user_hand = Hand(action)\n        self.user_hands += [self.user_hand]\n\n        # draw\n        if self.user_hand == self.enemy_hand:\n            observation = [self.user_hand, self.enemy_hand]\n            reward = 0\n            self.done = True\n\n        # win case\n        elif self.user_hand == Hand.GOO and self.enemy_hand == Hand.CHOKI:\n            observation = [self.user_hand, self.enemy_hand]\n            reward = 10\n            self.done = True\n\n        elif self.user_hand == Hand.CHOKI and self.enemy_hand == Hand.PAH:\n            observation = [self.user_hand, self.enemy_hand]\n            reward = 10\n            self.done = True\n\n        elif self.user_hand == Hand.PAH and self.enemy_hand == Hand.GOO:\n            observation = [self.user_hand, self.enemy_hand]\n            reward = 10\n            self.done = True\n\n        # lose case\n        elif self.user_hand == Hand.GOO and self.enemy_hand == Hand.PAH:\n            observation = [self.user_hand, self.enemy_hand]\n            reward = -10\n            self.done = True\n\n        elif self.user_hand == Hand.CHOKI and self.enemy_hand == Hand.GOO:\n            observation = [self.user_hand, self.enemy_hand]\n            reward = -10\n            self.done = True            \n\n        elif self.user_hand == Hand.PAH and self.enemy_hand == Hand.CHOKI:\n            observation = [self.user_hand, self.enemy_hand]\n            reward = -10\n            self.done = True\n\n        return observation, reward, self.done, {}\n\n    \nclass TrainIntervalLogger2(TrainIntervalLogger):\n    def __init__(self, interval=10000):\n        super().__init__(interval=interval)\n        self.records = {}\n\n    def on_train_begin(self, logs):\n        super().on_train_begin(logs)\n        self.records['interval'] = []\n        self.records['episode_reward'] = []\n        for metrics_name in self.metrics_names:\n            self.records[metrics_name] = []\n\n    def on_step_begin(self, step, logs):\n        if self.step % self.interval == 0:\n            if len(self.episode_rewards) > 0:\n                self.records['interval'].append(self.step \/\/ self.interval)\n                self.records['episode_reward'].append(np.mean(self.episode_rewards))\n                metrics = np.array(self.metrics)\n                assert metrics.shape == (self.interval, len(self.metrics_names))\n                if not np.isnan(metrics).all():  # not all values are means\n                    means = np.nanmean(self.metrics, axis=0)\n                    assert means.shape == (len(self.metrics_names),)\n                    for name, mean in zip(self.metrics_names, means):\n                        self.records[name].append(mean)\n        super().on_step_begin(step, logs)\n        \n\nclass DQNRPS:\n    weightfile = 'dqn_{}_{}_weights.h5'\n\n    def __init__(self, strategy=Strategy.HUMAN, recycle=True):\n        print('creating model ...')\n        self.train_interval_logger = None\n\n        # Get the environment and extract the number of actions.\n        self.env = RPS(strategy=strategy)\n        self.env_name = 'rps'\n        self.weightfile = DQNRPS.weightfile.format(self.env_name, str(strategy))\n        self.nb_actions = self.env.action_space.n\n\n        # Next, we build a very simple model.\n        self.model = tf.keras.Sequential()\n        self.model.add(tf.keras.layers.Flatten(input_shape=(1,) + self.env.observation_space.shape))\n        self.model.add(tf.keras.layers.Dense(128))\n        self.model.add(tf.keras.layers.Activation('relu'))\n        self.model.add(tf.keras.layers.Dense(self.nb_actions))\n        self.model.add(tf.keras.layers.Activation('linear'))\n\n        # Finally, we configure and compile our agent.\n        # You can use every built-in Keras optimizer and even the metrics!\n        memory = SequentialMemory(limit=500, window_length=1)\n        policy = BoltzmannQPolicy(tau=1.)\n        self.dqn = DQNAgent(model=self.model, nb_actions=self.nb_actions, memory=memory,\n                            nb_steps_warmup=10, target_model_update=1e-2, policy=policy)\n        self.dqn.compile(tf.keras.optimizers.Adam(lr=1e-3), metrics=[])\n\n        self.__istrained = False\n        print('model created')\n\n        if recycle:\n            if exists(self.weightfile):\n                try:\n                    print('loading pretraining weight ...')\n                    self.dqn.load_weights(self.weightfile)\n                    self.__istrained = True\n                    print('loaded pretrained weight')\n                    return None\n                except:\n                    print('Unexpected error:', exc_info()[0])\n                    raise\n            else:\n                pass\n\n    # \u8a13\u7df4\n    def train(self, nb_steps=3000, verbose=1, visualize=False, log_interval=300):\n        if self.__istrained:\n            raise RuntimeError('this model is already traine')\n\n        print('training ...')\n\n        callbacks = []\n        if verbose == 1:\n            self.train_interval_logger = TrainIntervalLogger2(interval=log_interval)\n            callbacks.append(self.train_interval_logger)\n            verbose = 0\n        elif verbose > 1:\n            callbacks.append(TrainEpisodeLogger())\n            verbose = 0\n\n        hist = self.dqn.fit(self.env, nb_steps=nb_steps,\n                            callbacks=callbacks, verbose=verbose,\n                            visualize=visualize, log_interval=log_interval)\n        self.__istrained = True\n\n        try:\n            # After training is done, we save the final weights.\n            self.dqn.save_weights(self.weightfile, overwrite=True)\n        except:\n            print('Unexpected error:', exc_info()[0])\n            raise\n\n        return hist\n\n    def test(self, nb_episodes=10, visualize=False, verbose=1):\n        hist = self.dqn.test(self.env, nb_episodes=nb_episodes,\n                             verbose=verbose, visualize=visualize)\n        return hist\n    \n\nself_actions = np.full(1001, -1, dtype=int)\noppo_actions = np.full(1001, -1, dtype=int)\n\nd = DQNRPS(strategy=Strategy.RANDOM, recycle=False)\n\n# Training few epochs to finish with in 61 seconds.\n# https:\/\/www.kaggle.com\/c\/rock-paper-scissors\/overview\/environment-rules\nh = d.train(nb_steps=500, log_interval=100, verbose=1)\nd = DQNRPS(strategy=Strategy.RANDOM, recycle=True)\nh = d.test(nb_episodes=1000, verbose=1) \nhands = [int(i) for i in d.env.user_hands]\n\n\ndef observe_and_predict(observation, configuration):\n    \n    step = observation.step\n    global self_actions, oppo_actions\n    global hands\n    \n    if step == 0:\n        self_act = np.random.randint(3)\n        self_actions[step] = self_act\n        return self_act\n    \n    self_1s_bef = self_actions[step - 1]\n    oppo_1s_bef = observation.lastOpponentAction\n    oppo_actions[step - 1] = oppo_1s_bef\n       \n    \n    if 1 <= step:\n        self_act = hands[step]\n        self_actions[step] = self_act\n        return self_act","cfccd13f":"%%writefile random_agent.py\nimport numpy as np\ndef random_agent(observation, configuration):\n    return np.random.randint(3)","5177fc08":"from kaggle_environments import evaluate, make\nvenv = make(\"rps\", configuration={\"episodeSteps\": 1000})","af3a6ee4":"venv.reset()\nvenv.run([\"submission.py\", \"random_agent.py\"])\nvenv.render(mode=\"ipython\", width=800, height=800)","ed9c277e":"#### Thank you for viewing my first DQN Notebook.\n#### I have some nortifications, plz see below.\n\n\u30fb This shows you Simple Starter model with KerasRL.\n\n\u30fb I'm new to study RL.\n\n\u30fb This model shows simple output pattern (only goo, only choki, only pah ...) because of simple reword strategy.\n\n\u30fb I have not built nice reword strategy yet! (So, I've not submmited yet.)\n\n\u30fb I refered [this JP article](https:\/\/qiita.com\/tanuk1647\/items\/7b8c2f0d09330cbfacd2).\n\n#### Thank you!"}}