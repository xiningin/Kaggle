{"cell_type":{"c854f2f7":"code","3fb3bd1a":"code","3c69ae50":"code","4635c3cc":"code","f5695844":"code","65ae8d7f":"code","1e73cdc7":"code","c0ce173b":"code","197bba95":"code","a77c8313":"code","d95c7b35":"code","59402db2":"code","e57f8c6c":"code","8b001dd0":"code","86c069e8":"code","a890d3db":"code","370ee3b7":"code","145151de":"code","98e7a7b7":"code","7f16ec66":"code","002ef5a8":"code","3394051a":"markdown","8deb99df":"markdown","87e36399":"markdown"},"source":{"c854f2f7":"import pandas as pd\nimport glob\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cv2\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom tqdm import tqdm","3fb3bd1a":"!pip install -q pretrainedmodels","3c69ae50":"import pretrainedmodels\nimport albumentations","4635c3cc":"train_labels = pd.read_csv('..\/input\/seti-breakthrough-listen\/train_labels.csv')","f5695844":"train_labels.head()","65ae8d7f":"import torch\n\nclass ClassificationDataset:\n    \n    def __init__(self, image_paths, targets, resize=None, augmentations=None): \n        self.image_paths = image_paths\n        self.targets = targets\n        self.resize = resize\n        self.augmentations = augmentations\n\n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self, item):      \n        image = np.load(self.image_paths[item]).astype(float)\n\n        targets = self.targets[item]\n        \n        if self.resize is not None:\n            image = np.transpose(image, (1,2,0))\n            image = cv2.resize(image, dsize=self.resize, interpolation=cv2.INTER_CUBIC)        \n        \n        if self.augmentations is not None:\n            augmented = self.augmentations(image=image)\n            image = augmented[\"image\"]\n            \n        # pytorch expects CHW instead of HWC\n        image = np.transpose(image, (2, 0, 1)).astype(np.float32)\n        \n        return {\n            \"image\": torch.tensor(image, dtype=torch.float),\n            \"targets\": torch.tensor(targets, dtype=torch.long),\n        }","1e73cdc7":"train_labels['img_path'] = train_labels['id'].apply(lambda x: f'..\/input\/seti-breakthrough-listen\/train\/{x[0]}\/{x}.npy')","c0ce173b":"train_labels.head()","197bba95":"train_dataloader = ClassificationDataset(image_paths=train_labels['img_path'], targets=train_labels['target'], resize=(256, 256))","a77c8313":"def train(data_loader, model, optimizer, device):\n    \n    model.train()\n    \n    for data in tqdm(data_loader, position=0, leave=True, desc='Training'):\n        inputs = data[\"image\"]\n        targets = data['targets']\n        \n        inputs = inputs.to(device, dtype=torch.float)\n        targets = targets.to(device, dtype=torch.float)\n        \n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = nn.BCEWithLogitsLoss()(outputs, targets.view(-1, 1))\n        loss.backward()\n        optimizer.step()\n        \ndef evaluate(data_loader, model, device):\n    model.eval()\n    \n    final_targets = []\n    final_outputs = []\n    \n    with torch.no_grad():\n        \n        for data in tqdm(data_loader, position=0, leave=True, desc='Evaluating'):\n            inputs = data[\"image\"]\n            targets = data[\"targets\"]\n            inputs = inputs.to(device, dtype=torch.float)\n            targets = targets.to(device, dtype=torch.float)\n            \n            output = model(inputs)\n            \n            targets = targets.detach().cpu().numpy().tolist()\n            output = output.detach().cpu().numpy().tolist()\n            \n            final_targets.extend(targets)\n            final_outputs.extend(output)\n            \n    return final_outputs, final_targets","d95c7b35":"import torch.nn as nn\nimport pretrainedmodels\n\ndef get_model(pretrained):\n    if pretrained:\n        model = pretrainedmodels.__dict__[\"resnet18\"](pretrained='imagenet')\n    else:\n        model = pretrainedmodels.__dict__[\"resnet18\"](pretrained=None)\n        \n    model.last_linear = nn.Sequential(\n        nn.BatchNorm1d(512),\n        nn.Dropout(p=0.25),\n        nn.Linear(in_features=512, out_features=1024),\n        nn.ReLU(),\n        nn.BatchNorm1d(1024, eps=1e-05, momentum=0.1),\n        nn.Dropout(p=0.5),\n        nn.Linear(in_features=1024, out_features=1)\n    )\n    \n    return model","59402db2":"device = \"cuda\"\n\nepochs = 5\n\nimages = train_labels.img_path.values\n\ntargets = train_labels.target.values\n\nmodel = get_model(pretrained=False)\nmodel.conv1 = nn.Conv2d(6, 64, kernel_size=7, stride=2, padding=3,bias=False)\n\nmodel.to(device)\n\n# mean = (0.485, 0.456, 0.406)\n# std = (0.229, 0.224, 0.225)\n\n# aug = albumentations.Compose([albumentations.Normalize(mean, std, max_pixel_value=255.0, always_apply=True)])\naug = None\n\ntrain_images, valid_images, train_targets, valid_targets = train_test_split(images, targets, stratify=targets, random_state=42)\n\ntrain_dataset = ClassificationDataset(image_paths=train_images,\n                                     targets=train_targets,\n                                     resize=(224, 224),\n                                     augmentations=aug)\n\ntrain_loader = torch.utils.data.DataLoader(train_dataset,\n                                          batch_size=16,\n                                          shuffle=True,\n                                          num_workers=4)\n\nvalid_dataset = ClassificationDataset(image_paths=valid_images,\n                                     targets=valid_targets,\n                                     resize=(224, 224),\n                                     augmentations=aug)\n\nvalid_loader = torch.utils.data.DataLoader(valid_dataset,\n                                          batch_size=16,\n                                          shuffle=False,\n                                          num_workers=4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n\nfor epoch in range(epochs):\n    train(train_loader, model, optimizer, device=device)\n    predictions, valid_targets = evaluate(valid_loader, model, device=device)\n    roc_auc = metrics.roc_auc_score(valid_targets, predictions)\n    print(f\"Epoch={epoch}, Valid ROC AUC={roc_auc}\")","e57f8c6c":"torch.save(model.state_dict(),f'resnet18_{epochs}.pt')","8b001dd0":"submission = pd.read_csv('..\/input\/seti-breakthrough-listen\/sample_submission.csv')\nsubmission['img_path'] = submission['id'].apply(lambda x: f'..\/input\/seti-breakthrough-listen\/test\/{x[0]}\/{x}.npy')","86c069e8":"submission","a890d3db":"test_images = submission.img_path.values\n\ndummy_targets = submission.target.values\n\ntest_dataset = ClassificationDataset(image_paths=test_images,\n                                     targets=dummy_targets,\n                                     resize=(224, 224),\n                                     augmentations=aug)\n\ntest_loader = torch.utils.data.DataLoader(test_dataset,\n                                          batch_size=16,\n                                          shuffle=False,\n                                          num_workers=4)","370ee3b7":"predictions, valid_targets = evaluate(test_loader, model, device=device)","145151de":"# normalize\npredictions = np.array(predictions)\n\npredictions = (predictions - predictions.min()) \/ (predictions.max() - predictions.min())","98e7a7b7":"submission.target = predictions","7f16ec66":"submission.drop(['img_path'], axis=1, inplace=True)","002ef5a8":"submission.to_csv('submission.csv', index=False)","3394051a":"## Inference","8deb99df":"This is a simple baseline using Resnet-18. The source code on this notebook is based on @abhishek Abhishek Thakur's [AAAMLP](https:\/\/github.com\/abhi1thakur\/approachingalmost) textbook.","87e36399":"## Train"}}