{"cell_type":{"e92c716d":"code","9cb45df6":"code","9ffb3f59":"code","0d1f7e30":"code","f6897ee8":"code","38986c97":"code","558d88cf":"code","13681b0e":"code","a14d51d1":"code","922e97a4":"code","5005dc78":"code","ab433542":"code","b40791e5":"code","b0bbbad2":"code","3aa650e3":"code","47f9384c":"code","d99d9547":"code","25a4a530":"code","471dfeee":"code","be4a493f":"code","f22b7be5":"code","ce254ec4":"code","4adf6fc5":"code","634040aa":"markdown","2c552289":"markdown","7d5ba89c":"markdown","9ce65f80":"markdown","fbbed633":"markdown","2721ba3b":"markdown","114d4a48":"markdown","b88bd4e6":"markdown","a22bb7cd":"markdown","f59bd008":"markdown","7a1fa439":"markdown","1f262d51":"markdown","f68046e8":"markdown","8517f45d":"markdown","fb4d90c5":"markdown","93e1cab8":"markdown","8e7b6a66":"markdown","98aa63c4":"markdown","5a37ce08":"markdown","277e284e":"markdown","e63d96fc":"markdown","d3276639":"markdown","db6476ad":"markdown","e3e35e59":"markdown"},"source":{"e92c716d":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nimport matplotlib.colors as colors\nimport ast\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.decomposition import TruncatedSVD\n\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.metrics import silhouette_samples\n\nfrom wordcloud import WordCloud\nfrom wordcloud import get_single_color_func\n\nfrom collections import Counter\n\n\nSMALL_SIZE = 12\nMEDIUM_SIZE = 14\nBIGGER_SIZE = 16\n\nplt.rc('font', size=SMALL_SIZE)          # controls default text sizes\nplt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\nplt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\nplt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\nplt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\nplt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\nplt.rc('figure', titlesize=BIGGER_SIZE) \n\ncmap_string = 'Spectral'","9cb45df6":"# data downloaded from Kaggle: https:\/\/www.kaggle.com\/rounakbanik\/ted-talks\ndf = pd.read_csv('..\/input\/ted-talks\/ted_main.csv')\n\ndf['date'] = pd.to_datetime(df.film_date, unit='s')\n\ndf.head(3)","9ffb3f59":"print(df.shape)","0d1f7e30":"df.tags = df.tags.apply(ast.literal_eval)\ndf['tags_single_string'] = df.tags.apply(' '.join)","f6897ee8":"# get word counts and TF-IDF arrays from tags\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\n\nX = df.tags_single_string.values\n\nprint(f'The first talk keywords: [{X[0]}]')\n\ncountv = CountVectorizer()\nX_counts = countv.fit_transform(X)\n\ntfidf = TfidfTransformer()\nX_tfidf = tfidf.fit_transform(X_counts)","38986c97":"print(X_tfidf.shape)\nprint(countv.get_feature_names()[:10])","558d88cf":"pca_comps = 2\npca = PCA(n_components=pca_comps)\nx_pca = pca.fit_transform(X_tfidf.toarray())\n\nfig, ax = plt.subplots(1,1, figsize=(5,5))\nscatter, = ax.scatter(x_pca[:,0], x_pca[:,1])\nax.legend(*scatter.legend_elements())\nax.set_xlabel(f'PC 1 ({100*pca.explained_variance_ratio_[0]: 0.2f}%)')\nax.set_ylabel(f'PC 2 ({100*pca.explained_variance_ratio_[1]: 0.2f}%)')\nplt.show()","13681b0e":"feature_names = countv.get_feature_names()\nwords_to_grab = 10\n\nfig, ax = plt.subplots(1, 2, figsize=(12, 10))\nfor i, comp in enumerate(pca.components_):\n    top_words = [feature_names[ind] for ind in comp.argsort()[-1:-words_to_grab:-1]]\n    bottom_words = [feature_names[ind] for ind in comp.argsort()[words_to_grab::-1]]\n    bp_top = ax[i].barh(top_words, comp[comp.argsort()[-1:-words_to_grab:-1]], \n              color='blue', edgecolor='black')\n    bp_bottom = ax[i].barh(bottom_words, comp[comp.argsort()[words_to_grab::-1]], \n              color='red', edgecolor='black')\n    ax[i].axvline(0, linestyle='--', color='black')\n    ax[i].set_title(f'Top word loadings for PC {i+1}')\n    ax[i].set_xlabel(f'PC {i+1} loading')\n    ax[i].invert_yaxis()\nplt.subplots_adjust(wspace=0.5)\nplt.show()","a14d51d1":"# Here I define a few helper functions which will be useful throughout the rest of the notebook.\n\ndef assign_word_to_cluster(string_list, labels, vocab):\n    '''generate a counter to get word counts per cluster, \n    then assign each word to the cluster it's most often associated with'''\n    \n    texts = [' '.join(string_list.loc[labels==c].values).split(' ') \n             for c in list(set(labels))]\n\n    counters = [Counter(text) for text in texts]\n\n    word_cluster = {}\n    for word in vocab:\n        cluster_count = []\n        for i, c in enumerate(counters):\n            cluster_count.append(c[word]\/len(np.where(labels == i)))\n            word_cluster[word] = np.argmax(cluster_count)\n\n        cluster_lookup = {k: [] for k in range(1+np.max(labels))}\n        for k, v in word_cluster.items():\n            cluster_lookup[v].append(k)\n    return word_cluster, cluster_lookup\n\ndef assign_word_to_color(cluster_lookup, cmap):\n    ''' map each cluster's associated word to a color;\n    return a dictionary of {hex color}: {word_list} for passing to word cloud API'''\n    color_dict = {}\n    for i, cluster in enumerate(list(cluster_lookup.keys())):\n        color_dict[colors.to_hex(cmap(i))] = cluster_lookup[cluster]\n    return color_dict\n\ndef plot_pc_space(x_pca, pca, labels, pca_comps, cmap):\n    ''' plot observations in PC 1-2 space, colored by kmeans label'''\n    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(5,5))\n    scatter = ax.scatter(x_pca[:,0], x_pca[:,1], c=labels, cmap = cmap)\n    #ax.legend(*scatter.legend_elements(), title='Clusters')\n    ax.set_xlabel(f'PC 1 ({100*pca.explained_variance_ratio_[0]: 0.2f}%)')\n    ax.set_ylabel(f'PC 2 ({100*pca.explained_variance_ratio_[1]: 0.2f}%)')\n    ax.set_title(f'{pca_comps} PCs')\n    plt.show()\n    \n# the following function is from the WordCloud API documentation for applying colors to specific words: https:\/\/amueller.github.io\/word_cloud\/index.html\nclass SimpleGroupedColorFunc(object):\n    \"\"\"Create a color function object which assigns EXACT colors\n       to certain words based on the color to words mapping\n\n       Parameters\n       ----------\n       color_to_words : dict(str -> list(str))\n         A dictionary that maps a color to the list of words.\n\n       default_color : str\n         Color that will be assigned to a word that's not a member\n         of any value from color_to_words.\n    \"\"\"\n\n    def __init__(self, color_to_words, default_color):\n        self.word_to_color = {word: color\n                              for (color, words) in color_to_words.items()\n                              for word in words}\n\n        self.default_color = default_color\n\n    def __call__(self, word, **kwargs):\n        return self.word_to_color.get(word, self.default_color)","922e97a4":"# run silhouette score analysis for data projected onto 2 pc components\npca_comps = 2\npca = PCA(n_components=pca_comps)\nx_pca = pca.fit_transform(X_tfidf.toarray())\ncomps = np.arange(2,10,1)\nscore = []\nsilhouette_avg = []\n# get kmeans score versus number of components\nfor n_clusters in comps:\n    kmeans = KMeans(n_clusters=n_clusters).fit(x_pca)\n    score.append(kmeans.score(x_pca))\n    cluster_labels = kmeans.predict(x_pca)\n    silhouette_avg.append(silhouette_score(x_pca, cluster_labels))\n    sample_silhouette_values = silhouette_samples(x_pca, cluster_labels)\n    \n    fig, ax1 = plt.subplots(1, 1)\n    #ax0.plot(comps, score)\n    #ax0.set_xlabel('# components')\n    #ax0.set_title('k-means score')\n\n    \n    y_lower = 10\n    for i in range(n_clusters):\n        # Aggregate the silhouette scores for samples belonging to\n        # cluster i, and sort them\n        ith_cluster_silhouette_values = \\\n            sample_silhouette_values[cluster_labels == i]\n\n        ith_cluster_silhouette_values.sort()\n\n        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n        y_upper = y_lower + size_cluster_i\n\n        color = plt.cm.nipy_spectral(float(i) \/ n_clusters)\n        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n                          0, ith_cluster_silhouette_values,\n                          facecolor=color, edgecolor=color, alpha=0.7)\n\n        # Label the silhouette plots with their cluster numbers at the middle\n        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n        ax1.axvline(silhouette_avg[-1], linestyle='--', color='r')\n        ax1.set_ylabel('Observation')\n        ax1.set_title(f'Silhouette scores for k={i} clusters')\n        # Compute the new y_lower for next plot\n        y_lower = y_upper + 10  # 10 for the 0 samples\nplt.subplots_adjust(hspace=1)\nplt.show()\n\nfig, (ax0, ax1) = plt.subplots(1, 2, figsize=(10, 4))\nax0.plot(comps, silhouette_avg)\nax0.set_xlabel('# k-means clusters')\nax0.set_ylabel('Mean silhouette score')\nax0.set_title(f'silhouette score: {pca_comps} PCs')\nax1.plot(comps, -np.array(score))\nax1.set_xlabel('# k-means clusters')\nax1.set_ylabel('Mean distance to cluster center')\nax1.set_title(f'Scree plot: {pca_comps} PCs')\nplt.subplots_adjust(wspace=0.5)\nplt.show()","5005dc78":"# use 4 k-means clusters for data projected onto 2 PCs\nk_clusts = 4\nkmeans_pca = KMeans(n_clusters=k_clusts)\n\npca_comps = 2\npca = PCA(n_components=pca_comps)\nx_pca = pca.fit_transform(X_tfidf.toarray())\nkmeans_pca.fit(x_pca)\n\ncmap = plt.cm.get_cmap(cmap_string, k_clusts)  \n\nplot_pc_space(x_pca, pca, kmeans_pca.labels_, pca_comps, cmap)","ab433542":"wc, cl = assign_word_to_cluster(df.tags_single_string, kmeans_pca.labels_, countv.vocabulary_)\ncolor_dict = assign_word_to_color(cl, cmap)\n\nfor i, clusters in enumerate(list(set(kmeans_pca.labels_))):\n    cluster = np.where(kmeans_pca.labels_== clusters)\n    text = ' '.join(df['tags_single_string'].str.lower().loc[cluster].values)\n    wordcloud = WordCloud(max_font_size=50, max_words=50, background_color=\"black\", collocations=False).generate(text)\n    \n    default_color = 'grey'\n    grouped_color_func = SimpleGroupedColorFunc(color_dict, default_color)\n    #grouped_color_func = GroupedColorFunc(color_dict, default_color)\n    wordcloud.recolor(color_func=grouped_color_func)\n    \n    # Display the generated image\n    fig, ax = plt.subplots()\n    ax.imshow(wordcloud, interpolation='bilinear')\n    ax.axis(\"off\")\n    ax.set_title(f'Cluster {i}')\n    plt.show()","b40791e5":"# run silhouette score analysis for data projected onto 5 pc components\npca_comps = 5\npca = PCA(n_components=pca_comps)\nx_pca = pca.fit_transform(X_tfidf.toarray())\ncomps = np.arange(2,10,1)\nscore = []\nsilhouette_avg = []\n# get kmeans score versus number of components\nfor n_clusters in comps:\n    kmeans = KMeans(n_clusters=n_clusters).fit(x_pca)\n    score.append(kmeans.score(x_pca))\n    cluster_labels = kmeans.predict(x_pca)\n    silhouette_avg.append(silhouette_score(x_pca, cluster_labels))\n    sample_silhouette_values = silhouette_samples(x_pca, cluster_labels)\n    \n    fig, ax1 = plt.subplots(1, 1)\n    #ax0.plot(comps, score)\n    #ax0.set_xlabel('# components')\n    #ax0.set_title('k-means score')\n\n    \n    y_lower = 10\n    for i in range(n_clusters):\n        # Aggregate the silhouette scores for samples belonging to\n        # cluster i, and sort them\n        ith_cluster_silhouette_values = \\\n            sample_silhouette_values[cluster_labels == i]\n\n        ith_cluster_silhouette_values.sort()\n\n        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n        y_upper = y_lower + size_cluster_i\n\n        color = plt.cm.nipy_spectral(float(i) \/ n_clusters)\n        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n                          0, ith_cluster_silhouette_values,\n                          facecolor=color, edgecolor=color, alpha=0.7)\n\n        # Label the silhouette plots with their cluster numbers at the middle\n        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n        ax1.axvline(silhouette_avg[-1], linestyle='--', color='r')\n        ax1.set_ylabel('Observation')\n        ax1.set_title(f'Silhouette scores for k={i} clusters')\n        # Compute the new y_lower for next plot\n        y_lower = y_upper + 10  # 10 for the 0 samples\nplt.subplots_adjust(hspace=1)\nplt.show()\n\nfig, (ax0, ax1) = plt.subplots(1, 2, figsize=(10, 4))\nax0.plot(comps, silhouette_avg)\nax0.set_xlabel('# k-means clusters')\nax0.set_ylabel('Mean silhouette score')\nax0.set_title(f'silhouette score: {pca_comps} PCs')\nax1.plot(comps, -np.array(score))\nax1.set_xlabel('# k-means clusters')\nax1.set_ylabel('Mean distance to cluster center')\nax1.set_title(f'Scree plot: {pca_comps} PCs')\nplt.subplots_adjust(wspace=0.5)\nplt.show()","b0bbbad2":"# use 6 k-means clusters for data projected onto 5 PCs\n\nk_clusts = 6\nkmeans_pca = KMeans(n_clusters=k_clusts)\n\npca_comps = 5\npca = PCA(n_components=pca_comps)\nx_pca = pca.fit_transform(X_tfidf.toarray())\nkmeans_pca.fit(x_pca)\n\ncmap = plt.cm.get_cmap(cmap_string, k_clusts)  \n\nwc, cl = assign_word_to_cluster(df.tags_single_string, kmeans_pca.labels_, countv.vocabulary_)\ncolor_dict = assign_word_to_color(cl, cmap)\n\nfor i, clusters in enumerate(list(set(kmeans_pca.labels_))):\n    cluster = np.where(kmeans_pca.labels_== clusters)\n    text = ' '.join(df['tags_single_string'].str.lower().loc[cluster].values)\n    wordcloud = WordCloud(max_font_size=50, max_words=50, background_color=\"black\", collocations=False).generate(text)\n    \n    default_color = 'grey'\n    grouped_color_func = SimpleGroupedColorFunc(color_dict, default_color)\n    #grouped_color_func = GroupedColorFunc(color_dict, default_color)\n    wordcloud.recolor(color_func=grouped_color_func)\n    \n    # Display the generated image\n    fig, ax = plt.subplots()\n    ax.imshow(wordcloud, interpolation='bilinear')\n    ax.axis(\"off\")\n    ax.set_title(f'Cluster {i}')\n    plt.show()\n\nplot_pc_space(x_pca, pca, kmeans_pca.labels_, pca_comps, cmap)","3aa650e3":"from sklearn.mixture import GaussianMixture\n\n# use 4 k-means clusters for data projected onto 2 PCs\nn_clusts = 4\ngm = GaussianMixture(n_components=n_clusts, covariance_type='diag')\n\npca_comps = 2\npca = PCA(n_components=pca_comps)\nx_pca = pca.fit_transform(X_tfidf.toarray())\ngm.fit(x_pca)\ngm_labels = gm.predict(x_pca)\n\ncmap = plt.cm.get_cmap(cmap_string, n_clusts)  \n\nwc, cl = assign_word_to_cluster(df.tags_single_string, gm_labels, countv.vocabulary_)\ncolor_dict = assign_word_to_color(cl, cmap)\n\nfor i, clusters in enumerate(list(set(gm_labels))):\n    cluster = np.where(gm_labels == clusters)\n    text = ' '.join(df['tags_single_string'].str.lower().loc[cluster].values)\n    wordcloud = WordCloud(max_font_size=50, max_words=50, background_color=\"black\", collocations=False).generate(text)\n    \n    default_color = 'grey'\n    grouped_color_func = SimpleGroupedColorFunc(color_dict, default_color)\n    #grouped_color_func = GroupedColorFunc(color_dict, default_color)\n    wordcloud.recolor(color_func=grouped_color_func)\n    \n    # Display the generated image\n    fig, ax = plt.subplots()\n    ax.imshow(wordcloud, interpolation='bilinear')\n    ax.axis(\"off\")\n    ax.set_title(f'Cluster {i}')\n    plt.show()\n\nplot_pc_space(x_pca, pca, gm_labels, pca_comps, cmap)","47f9384c":"# let's only plot observations where the cluster assignment probability is greater than a threshold\np_min = 0.75\ninds = np.where(np.max(gm.predict_proba(x_pca), axis=1) > p_min)\ngm_labels_hi_prob = gm_labels[inds]\ndf_hi_prob = df.loc[inds]\n\nfor i, clusters in enumerate(list(set(gm_labels_hi_prob))):\n    cluster = np.where(gm_labels_hi_prob == clusters)\n    text = ' '.join(df_hi_prob['tags_single_string'].str.lower().iloc[cluster].values)\n    wordcloud = WordCloud(max_font_size=50, max_words=50, background_color=\"black\", collocations=False).generate(text)\n    \n    default_color = 'grey'\n    grouped_color_func = SimpleGroupedColorFunc(color_dict, default_color)\n    #grouped_color_func = GroupedColorFunc(color_dict, default_color)\n    wordcloud.recolor(color_func=grouped_color_func)\n    \n    # Display the generated image\n    fig, ax = plt.subplots()\n    ax.imshow(wordcloud, interpolation='bilinear')\n    ax.axis(\"off\")\n    ax.set_title(f'Cluster {i}')\n    plt.show()\n    \n\nplot_pc_space(x_pca[inds], pca, gm_labels_hi_prob, pca_comps, cmap)","d99d9547":"# let's only plot observations where the cluster assignment probability is lower than a threshold\np_max = 0.75\ninds = np.where(np.max(gm.predict_proba(x_pca), axis=1) < p_max)\ngm_labels_low_prob = gm_labels[inds]\ndf_low_prob = df.loc[inds]\n\nfor i, clusters in enumerate(list(set(gm_labels_low_prob))):\n    cluster = np.where(gm_labels_low_prob == clusters)\n    text = ' '.join(df_low_prob['tags_single_string'].str.lower().iloc[cluster].values)\n    wordcloud = WordCloud(max_font_size=50, max_words=50, background_color=\"black\", collocations=False).generate(text)\n    \n    default_color = 'grey'\n    grouped_color_func = SimpleGroupedColorFunc(color_dict, default_color)\n    #grouped_color_func = GroupedColorFunc(color_dict, default_color)\n    wordcloud.recolor(color_func=grouped_color_func)\n    \n    # Display the generated image\n    fig, ax = plt.subplots()\n    ax.imshow(wordcloud, interpolation='bilinear')\n    ax.axis(\"off\")\n    ax.set_title(f'Cluster {i}')\n    plt.show()\n    \nplot_pc_space(x_pca[inds], pca, gm_labels_low_prob, pca_comps, cmap)","25a4a530":"from sklearn.decomposition import NMF\n\nn_hypothesized_topics = 5\nnmf = NMF(n_components=n_hypothesized_topics, random_state=42)\nnmf.fit(X_tfidf)\n\ntop_words_to_grab = 15\ncmap = plt.cm.get_cmap(cmap_string, n_hypothesized_topics)  \n\nfeature_names = countv.get_feature_names()\n\nfig, ax = plt.subplots(n_hypothesized_topics, 1, figsize=(4, 20))\nfor i, comp in enumerate(nmf.components_):\n    top_cluster_words = [feature_names[ind] for ind in comp.argsort()[-1:-top_words_to_grab:-1]]\n    bp = ax[i].barh(top_cluster_words, comp[comp.argsort()[-1:-top_words_to_grab:-1]], \n              color=cmap(i), edgecolor='black')\n    ax[i].set_title(f'Top word loadings for cluster {i}')\n    \n    ax[i].invert_yaxis()\n\nplt.show()\nnfm_labels = nmf.transform(X_tfidf).argmax(axis=1)\n","471dfeee":"wc, cl = assign_word_to_cluster(df.tags_single_string, nfm_labels, countv.vocabulary_)\ncolor_dict = assign_word_to_color(cl, cmap)\n\nfor i, clusters in enumerate(list(set(nfm_labels))):\n    cluster = np.where(nfm_labels == clusters)\n    text = ' '.join(df['tags_single_string'].str.lower().loc[cluster].values)\n    wordcloud = WordCloud(max_font_size=50, max_words=50, background_color=\"black\", collocations=False).generate(text)\n    \n    default_color = 'grey'\n    grouped_color_func = SimpleGroupedColorFunc(color_dict, default_color)\n    #grouped_color_func = GroupedColorFunc(color_dict, default_color)\n    wordcloud.recolor(color_func=grouped_color_func)\n    \n    # Display the generated image\n    fig, ax = plt.subplots()\n    ax.imshow(wordcloud, interpolation='bilinear')\n    ax.axis(\"off\")\n    ax.set_title(f'Cluster {i}')\n    plt.show()\n\n    \npca_comps = 2\npca = PCA(n_components=pca_comps)\nx_pca = pca.fit_transform(X_tfidf.toarray())\nplot_pc_space(x_pca, pca, nfm_labels, pca_comps, cmap)","be4a493f":"cluster_names = {\n    0: 'global issues and poverty',\n    1: 'core TED',\n    2: 'scientific research',\n    3: 'art',\n    4: 'society',\n}\n\nlabels_to_cluster_names = [cluster_names[x] for x in nfm_labels]\ndf['general_category'] = labels_to_cluster_names","f22b7be5":"ax = df['views'].groupby(df.general_category).agg(['mean', 'sem']).plot(kind='barh', xerr='sem')\nax.set_ylabel('General Category')\nax.set_xlabel('Total Views')\nax.get_legend().remove()\nplt.show()","ce254ec4":"from datetime import datetime\n\ndf['video_age_in_days'] = (datetime.now()-df.date).astype('timedelta64[D]')\ndf['views_per_day'] = df['views'] \/ df['video_age_in_days']","4adf6fc5":"ax = df['views_per_day'].groupby(df.general_category).agg(['mean', 'sem']).plot(kind='barh', xerr='sem')\nax.set_ylabel('General Category')\nax.set_xlabel('Views per day')\nax.get_legend().remove()\nplt.show()","634040aa":"It looks like the data points form a central cluster with 3 vertices. \n\nBut what do principal component 1 (PC 1) and principal component 2 (PC 2) actually represent? They represent the the new dimensions which were found by PCA to be a linear combination of the original dimensions, i.e., individual words! The loadings of each PC correspond to how much each word in our original vocabulary contribute to each principal component.\n\nLet's look at the words with the highest and lowest loadings in PC 1 and PC 2 to understand what these first two principal components represent in the data.","2c552289":"Next let's try to find the clusters in which talks naturally fall. From visualizing the data in PC space, we can see that that there don't seem to be discrete clusters, but we can still try to demarcate regions that contain talks of a similar broad category, like art, science, or politics. \n\nK-means clustering is an unsupervised learning algorithm that finds k clusters in a data set by minimizing the Euclidean distance of each point to its assigned cluster center.\n\nSo we can use K-means to cluster our dimensionally reduced data, but one question is how to set the number of clusters, k, in the K-means algorithm. One way is to find the average silhouette score across all observations and choose k such that the silhouette score is maximized. The silhouette score is described in the sklearn documentation, but it is essentially weighs the inter vs. intra-cluster distances. A high score indicates more unambiguous cluster assignments, and low scores indicates ambiguous or incorrect cluster assignments. The silhouette score is defined on [-1, 1].\n\nA second method for determining the number of clusters to use is to look at the average distance of each point to its assigned cluster, also called a scree plot. This value approaches zero as the number of clusters goes to infinity, but with diminishing returns. A common metric is to look for a \"bend\" in the plot and set k equal to that.  \n\nBelow I will produce both plots.","7d5ba89c":"We see that K-means has essentially divided the observations based on physical location in PC space, which makes sense. Now let's generate word clouds for each cluster to see what topics each cluster roughly corresponds to.","9ce65f80":"Here we see that the central cluster, what I call the 'core Ted' cluster, has been broken up into 3 sub-groups, each of which leans toward one of the field-specific vertices but is perhaps slightly more general.","fbbed633":"Finally, we can also use Non-negative matrix factorization to group talks. NMF is sometimes used for 'topic modeling' or assigning documents to discrete topics, which is exactly what we have been exploring in this notebook. NMF will enable us to cluster observations as well as directly observe which words contributed to the cluster assignment. We again have to supply the number of components or topics for NMF to find, and here I will choose 5, though in theory this could be chosen in a more principled manner by combining metrics and domain knowledge.","2721ba3b":"Of course, number of views depends on how long the video has been available. Let's add a new column to the dataframe to get the age of the video and then normalize views by the age.","114d4a48":"We know the broad category that each of these clusters is representing, so let's name them and add a new column to our original dataframe to generate a high-level tag for each talk.","b88bd4e6":"And let's plot the normalized views now for each group:","a22bb7cd":"Next we will use CountVectorizer and TfidfTransformer applied to each talk's set of keywords. CountVectorizer essentially counts the number of times each word appears, and TfidfTransformer inversely weights each word by how frequently it occurs across all talks' metadata.","f59bd008":"We see the results are pretty similar to when we ran K-means using k=4. Let's use the probabilities available to use from our Gaussian mixture model to look at the words found only in talks whose cluster assignment has a probability greater than 0.75.","7a1fa439":"Alternatively, we can look at talks whose cluster assignment has less than 0.75 probability.","1f262d51":"For this analysis I will only consider the talk metadata.","f68046e8":"Now that we have done all this unsupervised analysis to group talks into high-level categories, we may be curious about whether there are any differences in engagement between these categories.\n\nLet's groupby our new high-level categories and plot how many times each category has been viewed on average.","8517f45d":"We see our dataframe has 2550 observations.  \n\nFirst let's clean up the talk keyword tags.","fb4d90c5":"Here we see that 6 clusters maximizes the silhouette score. Let's continue with a k of 6. This should allow us to gain a more nuanced view of cluster identities.","93e1cab8":"In the word clouds above I have colored each word by the cluster in which it is most frequently found. We see that the cluster 0 (colored maroon), which resides in the bottom left region of our PC space plot, contains words like global, issues, and politics, which jibes with our understanding of PC 1 from earlier. Cluster 1 (peach) resides in the center of PC space and contains words like technology, design, and culture, very generic TED-type words. And clusters 2 (green) and 3 (purple) reside in the top and bottom right of PC space, respectively. As we expect from our understanding of PC space, cluster 2 contains science, and specifically biomedical, related words, and cluster 3 contains words pertaining to art, music and entertainment.\n\nIn the analysis above, we ran K-means on our data after projecting it down onto to only 2 PCs. Perhaps we can gain a more fine-grained understanding of talk clusters if we use more PCs. Now let's use 5 PCs and re-run our cluster number analysis to select a proper k.","8e7b6a66":"As we can see from the words contributing the most positive and most negative loadings, PC 1 seems to represent an art vs. activism axis, with words like music, entertainment, and creativity contributing to a positive score. On the other hand, words like global, issues, economics, and change contribute to a negative score. \n\nPC 2 seems to represent something like a technical vs. non-technical axis, with words like science, medicine, and research contributing to positive scores and words like global, music, issues, and politics contributing to negative scores. \n\nNow we have a baseline idea of what sort of talks to expect in each region of PC space.\n","98aa63c4":"There you have it. It seems talks focusing on societal issues are viewed the most, while those focusing on global issues and poverty are viewed less.\n\nOne challenge in data analysis is that there are many ways of doing similar things, with some methods simpler, others more powerful, and others more tailored to a specific use case. Latent Dirichlet Allocation (LDA) can use be used for topic modeling, and powerful libraries exist for advanced NLP and part-of-speech tagging. Nevertheless, I hope this notebook has introduced some simple ways you can analyze text in an unsupervised manner using only sklearn. ","5a37ce08":"K-means can assign new observations to a cluster, but it does not give us information about how likely assignment to each cluster is. This might be important especially in case like this where the data lie on a continuum, where we might want to know if an observation is almost equally likely to be a part of two adjacent clusters.\n\nWe can use a Gaussian mixture model (GMM) to once again assign each observation to a cluster while also enabling us to assign probabilities to each cluster assignment. Once again choosing the number of clusters is not trivial and there are semi-quantitative ways of doing that, such as using BIC and AIC, but for now I will just use 4 clusters for data projected onto 2 PCs, as the goal here is simply to illustrate that we can generate probabilities of cluster assignment.","277e284e":"A common first step in unsupervised learning is to reduce the dimensions of your data. This will allow us to plot the data in 2 dimensions to visualize it and get a sense of what is going on. Principal component analysis (PCA) is a method which finds the axes of highest variance in the data and rotates the data so that the first dimension corresponds to the greatest variance, the second dimension corresponds to the second greatest variance, and so on.\n\nLet's first run PCA on our data to visualize the distribution of talks in this dimensionally reduced PC space.","e63d96fc":"We see our transformed keyword matrix (X_tfidf) has 2550 observations, matching our original dataframe, and 440 features, each of which correspond to a unique word. There are a total of 440 unique words across all the talk keyword sets.","d3276639":"The silhouette score peaks at 4 clusters, and maybe if you squint you can convince yourself that there is a slight bend in the scree plot at k = 4. Choosing k is not an exact science and could also be informed by our prior knowledge of expected number of clusters. In this case, let's just try setting k equal to 4 and continuing.","db6476ad":"We see that the talks that are less confidently assigned a cluster lie towards the center of PC space, and their word clouds are perhaps slightly more generic than the more confidently classified talks. Being able to assign a probability to each cluster could also be useful for generating primary and secondary labels for each data point.","e3e35e59":"# Unsupervised learning of TED talk categories\n\nIn this notebook I will primarily use sklearn to group TED talks based on metadata keywords. My goal is to present multiple ways of clustering data as an introduction to some of the options available in sklearn. There are far more advanced tools available for NLP, such as specific libraries like spaCy, NLTK, and gensim, but my goal here is sipmly to introduce some basic tools available right in sklearn for text classification and clustering."}}