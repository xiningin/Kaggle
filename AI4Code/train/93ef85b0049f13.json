{"cell_type":{"7bb63498":"code","a721a453":"code","60e0e89c":"code","4edc6730":"code","6ab44084":"code","c85a7163":"code","ea744028":"code","f3c2a443":"code","dc8e0cd4":"code","8a967081":"code","d9c2ca49":"code","c0432665":"code","2b93f068":"code","d33ffa6a":"code","25b286d3":"code","da9047bf":"code","80cf6c61":"code","8e3eddf3":"code","8751d725":"code","0e56b650":"code","9f414525":"code","43d37085":"code","0d8639f3":"code","a136f469":"code","149e8bec":"code","b9e38b27":"code","dc6b3b48":"code","e3ec01a0":"code","d8a142d7":"code","e2ac5fa4":"code","a292bb0c":"code","51b9567e":"code","df33ac66":"code","555fed94":"code","280f561d":"code","2e4f22ee":"code","10ead62d":"code","22d32a97":"code","d67b4823":"code","2b395f03":"code","a4362d15":"code","41b8fbaf":"code","c31c130c":"code","f9f50d07":"code","c5f02293":"code","993bb8a7":"code","5cd76dc5":"code","a12c3ef6":"code","be2eaafb":"code","3507bd63":"code","cf559af7":"markdown","4e3dedc8":"markdown","d36e9a6d":"markdown","760369df":"markdown","b3ffcf48":"markdown","7720f0cb":"markdown","a9fba882":"markdown","b14c3378":"markdown","e3400ba2":"markdown","79412574":"markdown","eb7b5b7b":"markdown","607cb837":"markdown","38ea21c9":"markdown","9c861d0c":"markdown","92eb599e":"markdown","7671b34e":"markdown","c3a6fa0b":"markdown","59c76821":"markdown","3e7a5393":"markdown","5122ba3e":"markdown","9ac67b93":"markdown","a920fe25":"markdown","839cdec9":"markdown","50eaaa9a":"markdown","3dd8b063":"markdown","421c2392":"markdown","29db42dd":"markdown","b2401b88":"markdown","3fa15ebf":"markdown","13ac811c":"markdown","6d8d5f45":"markdown","5c2ab2e7":"markdown","15db7061":"markdown","dcade9c7":"markdown","ec54f8b3":"markdown","adedc031":"markdown","a73c661f":"markdown","cf05f067":"markdown","b9bad269":"markdown","7456c297":"markdown","a02a4247":"markdown","5a6a7bbf":"markdown","5b1141d3":"markdown","62d97433":"markdown","c53e903a":"markdown","40fa93a0":"markdown","9e54e83e":"markdown","8d8afc20":"markdown","a25e4ff4":"markdown","eec90fa9":"markdown","dac76dcc":"markdown","f83d320f":"markdown","740109fb":"markdown","07a6376a":"markdown","0b171865":"markdown","1d5bfe80":"markdown","6d697707":"markdown","50247bbb":"markdown","b038d344":"markdown","9c9cda2a":"markdown","89665d38":"markdown","81aab72b":"markdown","7137f286":"markdown","714dc425":"markdown","a349221d":"markdown","abca2efe":"markdown","a1b62a4f":"markdown","77e629e5":"markdown","8ee5cbdd":"markdown","d511ae2b":"markdown","bca22104":"markdown","e01123f7":"markdown","e7c94451":"markdown","3ad32c1d":"markdown","4828bf01":"markdown","e595e5de":"markdown","b8a23dc9":"markdown","b514ca37":"markdown","43e24b24":"markdown","4b820f99":"markdown","00015add":"markdown","0b01659c":"markdown","ec30e04a":"markdown","0225a2cc":"markdown","e5c6c72a":"markdown","ea078f22":"markdown","85fdb896":"markdown","494faa0b":"markdown","edae148a":"markdown","6770f426":"markdown","072b505c":"markdown","43af94ff":"markdown","52835fff":"markdown","4ad72c74":"markdown","d0747149":"markdown","1afb6ace":"markdown","0cfc956c":"markdown","5add54cb":"markdown","21cd6be0":"markdown","d1444964":"markdown"},"source":{"7bb63498":"# @title Tutorial slides\n\n# @markdown These are the slides for the videos in all tutorials today\n\n# @markdown If you want to locally download the slides, click [here](https:\/\/osf.io\/sfmpe\/download)\nfrom IPython.display import IFrame\nIFrame(src=f\"https:\/\/mfr.ca-1.osf.io\/render?url=https:\/\/osf.io\/sfmpe\/?direct%26mode=render%26action=download%26mode=render\", width=854, height=480)","a721a453":"# @title Install dependencies\n\n# @markdown There may be `Errors`\/`Warnings` reported during the installation. However, they are to be ignored.\n!pip install transformers --quiet\n!pip install torch==1.9.0 --quiet\n!pip install textattack --quiet\n!pip install urllib3==1.25.4 --quiet\n!pip install folium==0.2.1 --quiet\n!pip install datasets --quiet\n!pip install pytorch_pretrained_bert --quiet\n!pip install tensorflow-text\n\n!pip install git+https:\/\/github.com\/NeuromatchAcademy\/evaltools --quiet\nfrom evaltools.airtable import AirtableForm\n\n# generate airtable form\natform = AirtableForm('appn7VdPRseSoMXEG','W2D4_T1','https:\/\/portal.neuromatchacademy.org\/api\/redirect\/to\/720613bf-c3cd-4fae-9286-b1c3cced6728')","60e0e89c":"# Imports\nimport tqdm\nimport math\nimport torch\nimport statistics\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch.nn.functional as F\n\nfrom torch import nn\nfrom pprint import pprint\nfrom tqdm.notebook import tqdm\nfrom datasets import load_metric\nfrom datasets import load_dataset\n\n# transformers library\nfrom transformers import Trainer\nfrom transformers import pipeline\nfrom transformers import set_seed\nfrom transformers import AutoTokenizer\nfrom transformers import TrainingArguments\nfrom transformers import AutoModelForCausalLM\nfrom transformers import AutoModelForSequenceClassification\n\n# pytorch\nfrom pytorch_pretrained_bert import BertTokenizer\nfrom pytorch_pretrained_bert import BertForMaskedLM\n\n# textattack\nfrom textattack.transformations import WordSwapQWERTY\nfrom textattack.transformations import WordSwapExtend\nfrom textattack.transformations import WordSwapContract\nfrom textattack.transformations import WordSwapHomoglyphSwap\nfrom textattack.transformations import CompositeTransformation\nfrom textattack.transformations import WordSwapRandomCharacterDeletion\nfrom textattack.transformations import WordSwapNeighboringCharacterSwap\nfrom textattack.transformations import WordSwapRandomCharacterInsertion\nfrom textattack.transformations import WordSwapRandomCharacterSubstitution\n\n%load_ext tensorboard","4edc6730":"# @title Figure settings\nimport ipywidgets as widgets       # interactive display\n%config InlineBackend.figure_format = 'retina'\nplt.style.use(\"https:\/\/raw.githubusercontent.com\/NeuromatchAcademy\/content-creation\/main\/nma.mplstyle\")","6ab44084":"# @title Set random seed\n\n# @markdown Executing `set_seed(seed=seed)` you are setting the seed\n\n# for DL its critical to set the random seed so that students can have a\n# baseline to compare their results to expected results.\n# Read more here: https:\/\/pytorch.org\/docs\/stable\/notes\/randomness.html\n\n# Call `set_seed` function in the exercises to ensure reproducibility.\nimport random\nimport torch\n\ndef set_seed(seed=None, seed_torch=True):\n  if seed is None:\n    seed = np.random.choice(2 ** 32)\n  random.seed(seed)\n  np.random.seed(seed)\n  if seed_torch:\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True\n\n  print(f'Random seed {seed} has been set.')\n\n\n# In case that `DataLoader` is used\ndef seed_worker(worker_id):\n  worker_seed = torch.initial_seed() % 2**32\n  np.random.seed(worker_seed)\n  random.seed(worker_seed)","c85a7163":"# @title Set device (GPU or CPU). Execute `set_device()`\n# especially if torch modules used.\n\n# inform the user if the notebook uses GPU or CPU.\n\ndef set_device():\n  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n  if device != \"cuda\":\n    print(\"WARNING: For this notebook to perform best, \"\n        \"if possible, in the menu under `Runtime` -> \"\n        \"`Change runtime type.`  select `GPU` \")\n  else:\n    print(\"GPU is enabled in this notebook.\")\n\n  return device","ea744028":"SEED = 2021\nset_seed(seed=SEED)\nDEVICE = set_device()","f3c2a443":"# @title Load Yelp dataset\n\n# @markdown `DATASET = load_dataset(\"yelp_review_full\")`\n\nDATASET = load_dataset(\"yelp_review_full\")\nprint(type(DATASET))\n\n\ndef load_yelp_data():\n  dataset = DATASET\n  dataset['train'] = dataset['train'].select(range(10000))\n  dataset['test'] = dataset['test'].select(range(5000))\n  tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n  dataset = dataset.map(lambda e: tokenizer(e['text'], truncation=True,\n                                            padding='max_length'), batched=True)\n  dataset.set_format(type='torch', columns=['input_ids', 'label'])\n\n  train_loader = torch.utils.data.DataLoader(dataset['train'], batch_size=32)\n  test_loader = torch.utils.data.DataLoader(dataset['test'], batch_size=32)\n\n  vocab_size = tokenizer.vocab_size\n  max_len = next(iter(train_loader))['input_ids'].shape[0]\n  num_classes = next(iter(train_loader))['label'].shape[0]\n\n  return train_loader, test_loader, max_len, vocab_size, num_classes\n\n\ntrain_loader, test_loader, max_len, vocab_size, num_classes = load_yelp_data()\n\npred_text    = DATASET['test']['text'][28]\nactual_label = DATASET['test']['label'][28]\nbatch1 = next(iter(test_loader))","dc8e0cd4":"# @title Helper functions for BERT infilling\n\ndef transform_sentence_for_bert(sent, masked_word = \"___\"):\n  \"\"\"\n  By default takes a sentence with ___ instead of a masked word.\n\n  Args:\n    sent (str): an input sentence\n    masked_word(str): a masked part of the sentence\n\n  Returns:\n    str: sentence that could be bassed to BERT\n  \"\"\"\n  splitted = sent.split(\"___\")\n  assert (len(splitted) == 2), \"Missing masked word. Make sure to mark it as ___\"\n\n  return '[CLS] ' + splitted[0] + \"[MASK]\" + splitted[1] + ' [SEP]'\n\n\ndef parse_text_and_words(raw_line, mask = \"___\"):\n  \"\"\"\n  Takes a line that has multiple options for some position in the text.\n\n  Input: The doctor picked up his\/her bag\n  Output: (The doctor picked up ___ bag, ['his', 'her'])\n\n  Args:\n    raw_line (str): a line in format 'some text option1\/...\/optionN some text'\n    mask (str): the replacement for ...\/... section\n  Returns:\n    str: text with mask instead of ...\/... section\n    list: list of words from the ...\/... section\n  \"\"\"\n  splitted = raw_line.split(' ')\n  mask_index = -1\n  for i in range(len(splitted)):\n    if \"\/\" in splitted[i]:\n      mask_index = i\n      break\n  assert(mask_index != -1), \"No '\/'-separated words\"\n  words = splitted[mask_index].split('\/')\n  splitted[mask_index] = mask\n  return \" \".join(splitted), words\n\n\ndef get_probabilities_of_masked_words(text, words):\n  \"\"\"\n  Computes probabilities of each word in the masked section of the text.\n  Args:\n    text (str): A sentence with ___ instead of a masked word.\n    words (list): array of words.\n  Returns:\n    list: predicted probabilities for given words.\n  \"\"\"\n  text = transform_sentence_for_bert(text)\n  tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n  for i in range(len(words)):\n    words[i] = tokenizer.tokenize(words[i])[0]\n  words_idx = [tokenizer.convert_tokens_to_ids([word]) for word in words]\n  tokenized_text = tokenizer.tokenize(text)\n  indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n  masked_index = tokenized_text.index('[MASK]')\n  tokens_tensor = torch.tensor([indexed_tokens])\n\n  pretrained_masked_model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n  pretrained_masked_model.eval()\n\n  # Predict all tokens\n  with torch.no_grad():\n    predictions = pretrained_masked_model(tokens_tensor)\n  probabilities = F.softmax(predictions[0][masked_index], dim = 0)\n  predicted_index = torch.argmax(probabilities).item()\n\n  return [probabilities[ix].item() for ix in words_idx]","8a967081":"# @title Video 1: Intro\nfrom ipywidgets import widgets\n\nout2 = widgets.Output()\nwith out2:\n  from IPython.display import IFrame\n  class BiliVideo(IFrame):\n    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n      self.id=id\n      src = \"https:\/\/player.bilibili.com\/player.html?bvid={0}&page={1}\".format(id, page)\n      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n\n  video = BiliVideo(id=f\"BV1hf4y1j7XE\", width=730, height=410, fs=1)\n  print(\"Video available at https:\/\/www.bilibili.com\/video\/{0}\".format(video.id))\n  display(video)\n\nout1 = widgets.Output()\nwith out1:\n  from IPython.display import YouTubeVideo\n  video = YouTubeVideo(id=f\"UnuSQeT8GqQ\", width=730, height=410, fs=1, rel=0)\n  print(\"Video available at https:\/\/youtube.com\/watch?v=\" + video.id)\n  display(video)\n\nout = widgets.Tab([out1, out2])\nout.set_title(0, 'Youtube')\nout.set_title(1, 'Bilibili')\n\n# add event to airtable\natform.add_event('Video 1: Intro')\n\ndisplay(out)","d9c2ca49":"# @title Student Response\nfrom ipywidgets import widgets\n\n\ntext=widgets.Textarea(\n   value='Type your answer here and click on `Submit!`',\n   placeholder='Type something',\n   description='',\n   disabled=False\n)\n\nbutton = widgets.Button(description=\"Submit!\")\n\ndisplay(text,button)\n\ndef on_button_clicked(b):\n   atform.add_answer('q1' , text.value)\n   print(\"Submission successful!\")\n\n\nbutton.on_click(on_button_clicked)","c0432665":"# @title Video 2: Queries, Keys, and Values\nfrom ipywidgets import widgets\n\nout2 = widgets.Output()\nwith out2:\n  from IPython.display import IFrame\n  class BiliVideo(IFrame):\n    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n      self.id=id\n      src = \"https:\/\/player.bilibili.com\/player.html?bvid={0}&page={1}\".format(id, page)\n      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n\n  video = BiliVideo(id=f\"BV1Bf4y157LQ\", width=730, height=410, fs=1)\n  print(\"Video available at https:\/\/www.bilibili.com\/video\/{0}\".format(video.id))\n  display(video)\n\nout1 = widgets.Output()\nwith out1:\n  from IPython.display import YouTubeVideo\n  video = YouTubeVideo(id=f\"gDNRnjcoMOY\", width=730, height=410, fs=1, rel=0)\n  print(\"Video available at https:\/\/youtube.com\/watch?v=\" + video.id)\n  display(video)\n\nout = widgets.Tab([out1, out2])\nout.set_title(0, 'Youtube')\nout.set_title(1, 'Bilibili')\n\n# add event to airtable\natform.add_event('Video 2: Queries, Keys, and Values')\n\ndisplay(out)","2b93f068":"class DotProductAttention(nn.Module):\n  \"\"\"Scaled dot product attention.\"\"\"\n  def __init__(self, dropout, **kwargs):\n    super(DotProductAttention, self).__init__(**kwargs)\n    self.dropout = nn.Dropout(dropout)\n\n  def forward(self, queries, keys, values, b, h, t, k):\n    \"\"\"\n    Compute dot products. This is the same operation for each head,\n    so we can fold the heads into the batch dimension and use torch.bmm\n    Note: .contiguous() doesn't change the actual shape of the data,\n    but it rearranges the tensor in memory, which will help speed up the computation\n    for this batch matrix multiplication.\n    .transpose() is used to change the shape of a tensor. It returns a new tensor\n    that shares the data with the original tensor. It can only swap two dimension.\n\n    Shape of `queries`: (`batch_size`, no. of queries, head,`k`)\n    Shape of `keys`: (`batch_size`, no. of key-value pairs, head, `k`)\n    Shape of `values`: (`batch_size`, no. of key-value pairs, head, value dimension)\n\n    b: batch size\n    h: number of heads\n    t: number of keys\/queries\/values (for simplicity, let's assume they have the same sizes)\n    k: embedding size\n    \"\"\"\n    keys = keys.transpose(1, 2).contiguous().view(b * h, t, k)\n    queries = queries.transpose(1, 2).contiguous().view(b * h, t, k)\n    values = values.transpose(1, 2).contiguous().view(b * h, t, k)\n\n    #################################################\n    ## Implement Scaled dot product attention\n    # See the shape of the queries and keys above. You may want to use the `transpose` function\n    raise NotImplementedError(\"Scaled dot product attention `forward`\")\n    #################################################\n\n    # Matrix Multiplication between the keys and queries\n    score = torch.bmm(queries, ...) \/ math.sqrt(...)  # size: (b * h, t, t)\n    softmax_weights = F.softmax(score, dim=2)  # row-wise normalization of weights\n\n    # Matrix Multiplication between the output of the key and queries multiplication and values.\n    out = torch.bmm(self.dropout(softmax_weights), values).view(b, h, t, k)  # rearrange h and t dims\n    out = out.transpose(1, 2).contiguous().view(b, t, h * k)\n\n    return out\n\n\n# add event to airtable\natform.add_event('Coding Exercise 2: Dot product attention')","d33ffa6a":"# @title Video 3: Transformer Overview I\nfrom ipywidgets import widgets\n\nout2 = widgets.Output()\nwith out2:\n  from IPython.display import IFrame\n  class BiliVideo(IFrame):\n    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n      self.id=id\n      src = \"https:\/\/player.bilibili.com\/player.html?bvid={0}&page={1}\".format(id, page)\n      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n\n  video = BiliVideo(id=f\"BV1LX4y1c7Ge\", width=730, height=410, fs=1)\n  print(\"Video available at https:\/\/www.bilibili.com\/video\/{0}\".format(video.id))\n  display(video)\n\nout1 = widgets.Output()\nwith out1:\n  from IPython.display import YouTubeVideo\n  video = YouTubeVideo(id=f\"usQB0i8Mn-k\", width=730, height=410, fs=1, rel=0)\n  print(\"Video available at https:\/\/youtube.com\/watch?v=\" + video.id)\n  display(video)\n\nout = widgets.Tab([out1, out2])\nout.set_title(0, 'Youtube')\nout.set_title(1, 'Bilibili')\n\n# add event to airtable\natform.add_event('Video 3: Transformer Overview I')\n\ndisplay(out)","25b286d3":"class TransformerBlock(nn.Module):\n  \"\"\"Transformer Block\n  Args:\n    k (int): Attention embedding size\n    heads (int): number of self-attention heads\n\n  Attributes:\n    attention: Multi-head SelfAttention layer\n    norm_1, norm_2: LayerNorms\n    mlp: feedforward neural network\n  \"\"\"\n  def __init__(self, k, heads):\n    super().__init__()\n\n    self.attention = SelfAttention(k, heads=heads)\n\n    self.norm_1 = nn.LayerNorm(k)\n    self.norm_2 = nn.LayerNorm(k)\n\n    hidden_size = 2 * k  # This is a somewhat arbitrary choice\n    self.mlp = nn.Sequential(\n        nn.Linear(k, hidden_size),\n        nn.ReLU(),\n        nn.Linear(hidden_size, k))\n\n  def forward(self, x):\n    attended = self.attention(x)\n    #################################################\n    ## Implement the add & norm in the first block\n    raise NotImplementedError(\"Add & Normalize layer 1 `forward`\")\n    #################################################\n    # Complete the input of the first Add & Normalize layer\n    x = self.norm_1(... + x)\n    feedforward = self.mlp(x)\n    #################################################\n    ## Implement the add & norm in the second block\n    raise NotImplementedError(\"Add & Normalize layer 2 `forward`\")\n    #################################################\n    # Complete the input of the second Add & Normalize layer\n    x = self.norm_2(...)\n\n    return x\n\n\n# add event to airtable\natform.add_event('Coding Exercise 3: Transformer encoder')","da9047bf":"# @title Video 4: Transformer Overview II\nfrom ipywidgets import widgets\n\nout2 = widgets.Output()\nwith out2:\n  from IPython.display import IFrame\n  class BiliVideo(IFrame):\n    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n      self.id=id\n      src = \"https:\/\/player.bilibili.com\/player.html?bvid={0}&page={1}\".format(id, page)\n      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n\n  video = BiliVideo(id=f\"BV14q4y1H7SV\", width=730, height=410, fs=1)\n  print(\"Video available at https:\/\/www.bilibili.com\/video\/{0}\".format(video.id))\n  display(video)\n\nout1 = widgets.Output()\nwith out1:\n  from IPython.display import YouTubeVideo\n  video = YouTubeVideo(id=f\"kxn2qm6N8yU\", width=730, height=410, fs=1, rel=0)\n  print(\"Video available at https:\/\/youtube.com\/watch?v=\" + video.id)\n  display(video)\n\nout = widgets.Tab([out1, out2])\nout.set_title(0, 'Youtube')\nout.set_title(1, 'Bilibili')\n\n# add event to airtable\natform.add_event('Video 4: Transformer Overview II')\n\ndisplay(out)","80cf6c61":"# @title Student Response\nfrom ipywidgets import widgets\n\n\ntext=widgets.Textarea(\n   value='Type your answer here and click on `Submit!`',\n   placeholder='Type something',\n   description='',\n   disabled=False\n)\n\nbutton = widgets.Button(description=\"Submit!\")\n\ndisplay(text,button)\n\ndef on_button_clicked(b):\n   atform.add_answer('q2' , text.value)\n   print(\"Submission successful!\")\n\n\nbutton.on_click(on_button_clicked)","8e3eddf3":"# @title Video 5: Multi-head Attention\nfrom ipywidgets import widgets\n\nout2 = widgets.Output()\nwith out2:\n  from IPython.display import IFrame\n  class BiliVideo(IFrame):\n    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n      self.id=id\n      src = \"https:\/\/player.bilibili.com\/player.html?bvid={0}&page={1}\".format(id, page)\n      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n\n  video = BiliVideo(id=f\"BV1WU4y1H7aL\", width=730, height=410, fs=1)\n  print(\"Video available at https:\/\/www.bilibili.com\/video\/{0}\".format(video.id))\n  display(video)\n\nout1 = widgets.Output()\nwith out1:\n  from IPython.display import YouTubeVideo\n  video = YouTubeVideo(id=f\"KJoWo1NMUpM\", width=730, height=410, fs=1, rel=0)\n  print(\"Video available at https:\/\/youtube.com\/watch?v=\" + video.id)\n  display(video)\n\nout = widgets.Tab([out1, out2])\nout.set_title(0, 'Youtube')\nout.set_title(1, 'Bilibili')\n\n# add event to airtable\natform.add_event('Video 5: Multi-head Attention')\n\ndisplay(out)","8751d725":"class SelfAttention(nn.Module):\n  \"\"\"Multi-head self attention layer\n\n  Args:\n    k (int): Size of attention embeddings\n    heads (int): Number of attention heads\n\n  Attributes:\n    to_keys: Transforms input to k x k*heads key vectors\n    to_queries: Transforms input to k x k*heads query vectors\n    to_values: Transforms input to k x k*heads value vectors\n    unify_heads: combines queries, keys and values to a single vector\n  \"\"\"\n  def __init__(self, k, heads=8, dropout=0.1):\n    super().__init__()\n    self.k, self.heads = k, heads\n    #################################################\n    ## Complete the arguments of the Linear mapping\n    ## The first argument should be the input dimension\n    # The second argument should be the output dimension\n    raise NotImplementedError(\"Linear mapping `__init__`\")\n    #################################################\n\n    self.to_keys = nn.Linear(..., ..., bias=False)\n    self.to_queries = nn.Linear(..., ..., bias=False)\n    self.to_values = nn.Linear(..., ..., bias=False)\n    self.unify_heads = nn.Linear(k * heads, k)\n\n    self.attention = DotProductAttention(dropout)\n\n  def forward(self, x):\n    \"\"\"Implements forward pass of self-attention layer\n\n    Args:\n      x (torch.Tensor): batch x t x k sized input\n    \"\"\"\n    b, t, k = x.size()\n    h = self.heads\n\n    # We reshape the queries, keys and values so that each head has its own dimension\n    queries = self.to_queries(x).view(b, t, h, k)\n    keys = self.to_keys(x).view(b, t, h, k)\n    values = self.to_values(x).view(b, t, h, k)\n\n    out = self.attention(queries, keys, values, b, h, t, k)\n\n    return self.unify_heads(out)\n\n\n# add event to airtable\natform.add_event('Coding Exercise 5: Q, K, V attention')","0e56b650":"# @title Video 6: Positional Encoding\nfrom ipywidgets import widgets\n\nout2 = widgets.Output()\nwith out2:\n  from IPython.display import IFrame\n  class BiliVideo(IFrame):\n    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n      self.id=id\n      src = \"https:\/\/player.bilibili.com\/player.html?bvid={0}&page={1}\".format(id, page)\n      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n\n  video = BiliVideo(id=f\"BV1vb4y167N7\", width=730, height=410, fs=1)\n  print(\"Video available at https:\/\/www.bilibili.com\/video\/{0}\".format(video.id))\n  display(video)\n\nout1 = widgets.Output()\nwith out1:\n  from IPython.display import YouTubeVideo\n  video = YouTubeVideo(id=f\"jLBunbvvwwQ\", width=730, height=410, fs=1, rel=0)\n  print(\"Video available at https:\/\/youtube.com\/watch?v=\" + video.id)\n  display(video)\n\nout = widgets.Tab([out1, out2])\nout.set_title(0, 'Youtube')\nout.set_title(1, 'Bilibili')\n\n# add event to airtable\natform.add_event('Video 6: Positional Encoding')\n\ndisplay(out)","9f414525":"class PositionalEncoding(nn.Module):\n  # Source: https:\/\/pytorch.org\/tutorials\/beginner\/transformer_tutorial.html\n  def __init__(self, emb_size, dropout=0.1, max_len=512):\n    super(PositionalEncoding, self).__init__()\n    self.dropout = nn.Dropout(p=dropout)\n\n    pe = torch.zeros(max_len, emb_size)\n    position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n    div_term = torch.exp(torch.arange(0, emb_size, 2).float() * (-np.log(10000.0) \/ emb_size))\n    pe[:, 0::2] = torch.sin(position * div_term)\n    pe[:, 1::2] = torch.cos(position * div_term)\n    pe = pe.unsqueeze(0).transpose(0, 1)\n    self.register_buffer('pe', pe)\n\n  def forward(self, x):\n    x = x + self.pe[:x.size(0), :]\n    return self.dropout(x)","43d37085":"class Transformer(nn.Module):\n  \"\"\"Transformer Encoder network for classification\n\n    Args:\n      k (int): Attention embedding size\n      heads (int): Number of self attention heads\n      depth (int): How many transformer blocks to include\n      seq_length (int): How long an input sequence is\n      num_tokens (int): Size of dictionary\n      num_classes (int): Number of output classes\n  \"\"\"\n  def __init__(self, k, heads, depth, seq_length, num_tokens, num_classes):\n    super().__init__()\n\n    self.k = k\n    self.num_tokens = num_tokens\n    self.token_embedding = nn.Embedding(num_tokens, k)\n    self.pos_enc = PositionalEncoding(k)\n\n    transformer_blocks = []\n    for i in range(depth):\n      transformer_blocks.append(TransformerBlock(k=k, heads=heads))\n\n    self.transformer_blocks = nn.Sequential(*transformer_blocks)\n    self.classification_head = nn.Linear(k, num_classes)\n\n  def forward(self, x):\n    \"\"\"Forward pass for Classification Transformer network\n\n    Args:\n      x (torch.Tensor): (b, t) sized tensor of tokenized words\n\n    Returns:\n      torch.Tensor of size (b, c) with log-probabilities over classes\n    \"\"\"\n    x = self.token_embedding(x) * np.sqrt(self.k)\n    x = self.pos_enc(x)\n    x = self.transformer_blocks(x)\n\n    #################################################\n    ## Implement the Mean pooling to produce\n    # the sentence embedding\n    raise NotImplementedError(\"Mean pooling `forward`\")\n    #################################################\n    sequence_avg = ...\n    x = self.classification_head(sequence_avg)\n    logprobs = F.log_softmax(x, dim=1)\n\n    return logprobs\n\n\n# add event to airtable\natform.add_event('Coding Exercise 6: Transformer Architechture for classification')","0d8639f3":"def train(model, loss_fn, train_loader,\n          n_iter=1, learning_rate=1e-4,\n          test_loader=None, device='cpu',\n          L2_penalty=0, L1_penalty=0):\n  \"\"\"Run gradient descent to opimize parameters of a given network\n\n  Args:\n    net (nn.Module): PyTorch network whose parameters to optimize\n    loss_fn: built-in PyTorch loss function to minimize\n    train_data (torch.Tensor): n_train x n_neurons tensor with neural\n      responses to train on\n    train_labels (torch.Tensor): n_train x 1 tensor with orientations of the\n      stimuli corresponding to each row of train_data\n    n_iter (int, optional): number of iterations of gradient descent to run\n    learning_rate (float, optional): learning rate to use for gradient descent\n    test_data (torch.Tensor, optional): n_test x n_neurons tensor with neural\n      responses to test on\n    test_labels (torch.Tensor, optional): n_test x 1 tensor with orientations of\n      the stimuli corresponding to each row of test_data\n    L2_penalty (float, optional): l2 penalty regularizer coefficient\n    L1_penalty (float, optional): l1 penalty regularizer coefficient\n\n  Returns:\n    (list): training loss over iterations\n\n  \"\"\"\n\n  # Initialize PyTorch Adam optimizer\n  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n  # Placeholder to save the loss at each iteration\n  train_loss = []\n  test_loss = []\n\n  # Loop over epochs (cf. appendix)\n  for iter in range(n_iter):\n    iter_train_loss = []\n    for i, batch in tqdm(enumerate(train_loader)):\n      # compute network output from inputs in train_data\n      out = model(batch['input_ids'].to(device))\n      loss = loss_fn(out, batch['label'].to(device))\n\n      # Clear previous gradients\n      optimizer.zero_grad()\n\n      # Compute gradients\n      loss.backward()\n\n      # Update weights\n      optimizer.step()\n\n      # Store current value of loss\n      iter_train_loss.append(loss.item())  # .item() needed to transform the tensor output of loss_fn to a scalar\n      if i % 50 == 0:\n        print(f'[Batch {i}]: train_loss: {loss.item()}')\n    train_loss.append(statistics.mean(iter_train_loss))\n\n    # Track progress\n    if True: #(iter + 1) % (n_iter \/\/ 5) == 0:\n\n      if test_loader is not None:\n        print('Running Test loop')\n        iter_loss_test = []\n        for j, test_batch in enumerate(test_loader):\n\n          out_test = model(test_batch['input_ids'].to(device))\n          loss_test = loss_fn(out_test, test_batch['label'].to(device))\n          iter_loss_test.append(loss_test.item())\n\n        test_loss.append(statistics.mean(iter_loss_test))\n\n      if test_loader is None:\n        print(f'iteration {iter + 1}\/{n_iter} | train loss: {loss.item():.3f}')\n      else:\n        print(f'iteration {iter + 1}\/{n_iter} | train loss: {loss.item():.3f} | test_loss: {loss_test.item():.3f}')\n\n  if test_loader is None:\n    return train_loss\n  else:\n    return train_loss, test_loss\n\n# Set random seeds for reproducibility\nnp.random.seed(1)\ntorch.manual_seed(1)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Initialize network with embedding size 128, 8 attention heads, and 3 layers\nmodel = Transformer(128, 8, 3, max_len, vocab_size, num_classes).to(device)\n\n# Initialize built-in PyTorch Negative Log Likelihood loss function\nloss_fn = F.nll_loss\n\ntrain_loss, test_loss = train(model, loss_fn, train_loader, test_loader=test_loader,\n                              device=device)","a136f469":"with torch.no_grad():\n  # Batch 1 contains all the tokenized text for the 1st batch of the test loader\n  pred_batch = model(batch1['input_ids'].to(device))\n  # Predicting the label for the text\n  print(\"The yelp review is \u2192 \" + str(pred_text))\n  predicted_label28 = np.argmax(pred_batch[28].cpu())\n  print()\n  print(\"The Predicted Rating is \u2192 \" + str(predicted_label28) + \" and the Actual Rating was \u2192 \" + str(actual_label))","149e8bec":"# @title Video 7: Ethical aspects\nfrom ipywidgets import widgets\n\nout2 = widgets.Output()\nwith out2:\n  from IPython.display import IFrame\n  class BiliVideo(IFrame):\n    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n      self.id=id\n      src = \"https:\/\/player.bilibili.com\/player.html?bvid={0}&page={1}\".format(id, page)\n      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n\n  video = BiliVideo(id=f\"BV1aw41197xc\", width=730, height=410, fs=1)\n  print(\"Video available at https:\/\/www.bilibili.com\/video\/{0}\".format(video.id))\n  display(video)\n\nout1 = widgets.Output()\nwith out1:\n  from IPython.display import YouTubeVideo\n  video = YouTubeVideo(id=f\"4IhmuTW1-_E\", width=730, height=410, fs=1, rel=0)\n  print(\"Video available at https:\/\/youtube.com\/watch?v=\" + video.id)\n  display(video)\n\nout = widgets.Tab([out1, out2])\nout.set_title(0, 'Youtube')\nout.set_title(1, 'Bilibili')\n\n# add event to airtable\natform.add_event('Video 7: Ethical aspects')\n\ndisplay(out)","b9e38b27":"# @title Probabilities of masked words\n\ntext = 'It was a very important discovery, one you wouldn\\u2019t expect from a female\/male astrophysicist' #@param \\[\"It was a very important discovery, one you wouldn\u2019t expect from a female\/male astrophysicist\", \"We were especially upset that there were so many gross old\/young people at the beach.\", \"People who live in trailers\/mansions are alcoholics.\", \"Thin\/fat people can never really be attractive.\"]\nmasked_text, words = parse_text_and_words(text)\nprobs = get_probabilities_of_masked_words(masked_text, words)\nprobs = [np.round(p, 3) for p in probs]\nfor i in range(len(words)):\n  print(f\"P({words[i]}) == {probs[i]}\")\nif len(words) == 2:\n  rate = np.round(probs[0] \/ probs[1], 3) if probs[1] else \"+inf\"\n  print(f\"P({words[0]}) is {rate} times higher than P({words[1]})\")","dc6b3b48":"# @title Probabilities of masked words\n\ntext = 'The doctor picked up his\/her bag' # @param {type:\"string\"}\n\nmasked_text, words = parse_text_and_words(text)\nprobs = get_probabilities_of_masked_words(masked_text, words)\nprobs = [np.round(p, 3) for p in probs]\nfor i in range(len(words)):\n  print(f\"P({words[i]}) == {probs[i]}\")\nif len(words) == 2:\n  rate = np.round(probs[0] \/ probs[1], 3) if probs[1] else \"+inf\"\n  print(f\"P({words[0]}) is {rate} times higher than P({words[1]})\")","e3ec01a0":"# @title Student Response\nfrom ipywidgets import widgets\n\n\ntext=widgets.Textarea(\n   value='Type your answer here and click on `Submit!`',\n   placeholder='Type something',\n   description='',\n   disabled=False\n)\n\nbutton = widgets.Button(description=\"Submit!\")\n\ndisplay(text,button)\n\ndef on_button_clicked(b):\n   atform.add_answer('q3' , text.value)\n   print(\"Submission successful!\")\n\n\nbutton.on_click(on_button_clicked)","d8a142d7":"# @title Student Response\nfrom ipywidgets import widgets\n\n\ntext=widgets.Textarea(\n   value='Type your answer here and click on `Submit!`',\n   placeholder='Type something',\n   description='',\n   disabled=False\n)\n\nbutton = widgets.Button(description=\"Submit!\")\n\ndisplay(text,button)\n\ndef on_button_clicked(b):\n   atform.add_answer('q4' , text.value)\n   print(\"Submission successful!\")\n\n\nbutton.on_click(on_button_clicked)","e2ac5fa4":"# @title Airtable Submission Link\nfrom IPython import display as IPydisplay\nIPydisplay.HTML(\n   f\"\"\"\n <div>\n   <a href= \"{atform.url()}\" target=\"_blank\">\n   <img src=\"https:\/\/github.com\/NeuromatchAcademy\/course-content-dl\/blob\/main\/tutorials\/static\/SurveyButton.png?raw=1\"\n alt=\"button link end of day Survey\" style=\"width:410px\"><\/a>\n   <\/div>\"\"\" )","a292bb0c":"# @title Video 8: Pre-training\nfrom ipywidgets import widgets\n\nout2 = widgets.Output()\nwith out2:\n  from IPython.display import IFrame\n  class BiliVideo(IFrame):\n    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n      self.id=id\n      src = \"https:\/\/player.bilibili.com\/player.html?bvid={0}&page={1}\".format(id, page)\n      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n\n  video = BiliVideo(id=f\"BV13q4y1X7Tt\", width=730, height=410, fs=1)\n  print(\"Video available at https:\/\/www.bilibili.com\/video\/{0}\".format(video.id))\n  display(video)\n\nout1 = widgets.Output()\nwith out1:\n  from IPython.display import YouTubeVideo\n  video = YouTubeVideo(id=f\"dMpvzEEDOwI\", width=730, height=410, fs=1, rel=0)\n  print(\"Video available at https:\/\/youtube.com\/watch?v=\" + video.id)\n  display(video)\n\nout = widgets.Tab([out1, out2])\nout.set_title(0, 'Youtube')\nout.set_title(1, 'Bilibili')\n\n# add event to airtable\natform.add_event('Video 8: Pre-training')\n\ndisplay(out)","51b9567e":"# @title Bonus 1.1: Load Yelp reviews dataset \u231b\ud83e\udd17\nfrom IPython.display import clear_output\ntrain_dataset = load_dataset(\"yelp_review_full\", split='train')\ntest_dataset = load_dataset(\"yelp_review_full\", split='test')\nclear_output()\n\n# filter training data by sentiment value\nsentiment_dict = {}\nsentiment_dict[\"Sentiment = 0\"] = train_dataset.filter(lambda example: example['label']==0)\nsentiment_dict[\"Sentiment = 1\"] = train_dataset.filter(lambda example: example['label']==1)\nsentiment_dict[\"Sentiment = 2\"] = train_dataset.filter(lambda example: example['label']==2)\nsentiment_dict[\"Sentiment = 3\"] = train_dataset.filter(lambda example: example['label']==3)\nsentiment_dict[\"Sentiment = 4\"] = train_dataset.filter(lambda example: example['label']==4)","df33ac66":"# @title Bonus 1.2: Setting up a text context \u270d\ufe0f\n\ndef clean_text(text):\n    text = text.replace(\"\\\\n\", \" \")\n    text = text.replace(\"\\n\", \" \")\n    text = text.replace(\"\\\\\", \" \")\n    return text\n\n# @markdown ---\nsample_review_from_yelp = \"Sentiment = 0\" # @param [\"Sentiment = 0\", \"Sentiment = 1\", \"Sentiment = 2\", \"Sentiment = 3\", \"Sentiment = 4\"]\n# @markdown **Randomly sample a response from the Yelp review dataset with the given sentiment value {0:\ud83d\ude20, 1:\ud83d\ude26, 2:\ud83d\ude10, 3:\ud83d\ude42, 4:\ud83d\ude00}**\n\n# @markdown ---\nuse_custom_review = False #@param {type:\"boolean\"}\ncustom_review = \"I liked this movie very much because ...\" # @param {type:\"string\"}\n# @markdown ***Alternatively, write your own review (don't forget to enable custom review using the checkbox given above)***\n\n# @markdown ---\n\n# @markdown **NOTE:** *Run the cell after setting all the You can adding different kinds of extensionabove fields appropriately!*\n\nprint(\"\\n ****** The selected text context ****** \\n\")\nif use_custom_review:\n  context = clean_text(custom_review)\nelse:\n  context = clean_text(sentiment_dict[sample_review_from_yelp][random.randint(0,len(sentiment_dict[sample_review_from_yelp])-1)][\"text\"])\npprint(context)","555fed94":"# @title Bonus 1.3: Extending the review with pre-trained models \ud83e\udd16\n\n# @markdown ---\nmodel = \"gpt2\" #@param [\"gpt2\", \"gpt2-medium\", \"xlnet-base-cased\"]\ngenerator = pipeline('text-generation', model=model)\nset_seed(42)\n# @markdown **Select a pre-trained language model to generate text \ud83e\udd16**\n\n# @markdown *(might take some time to download the pre-trained weights for the first time)*\n\n# @markdown ---\nextension_prompt = \"Hence, overall I feel that ...\" #@param {type:\"string\"}\nnum_output_responses = 1 #@param {type:\"slider\", min:1, max:10, step:1}\n# @markdown **Provide a prompt to extend the review \u270d\ufe0f**\n\ninput_text = context + \" \" + extension_prompt\n# @markdown **NOTE:** *Run this cell after setting all the fields appropriately!*\n\n# @markdown **NOTE:** *Some pre-trained models might not work well with longer texts!*\n\ngenerated_responses = generator(input_text, max_length=512, num_return_sequences=num_output_responses)\n\nprint(\"\\n *********** INPUT PROMPT TO THE MODEL ************ \\n\")\npprint(input_text)\n\nprint(\"\\n *********** EXTENDED RESPONSES BY THE MODEL ************ \\n\")\nfor response in generated_responses:\n    pprint(response[\"generated_text\"][len(input_text):] + \" ...\"); print()","280f561d":"# @title Bonus 1.4: Sentiment binary-classification with likelihood of positive and negative extensions of the review \ud83d\udc4d\ud83d\udc4e\n\n# @markdown ---\nmodel_name = \"gpt2\" #@param [\"gpt2\", \"gpt2-medium\", \"xlnet-base-cased\"]\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\nmodel.eval()\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n# @markdown **Select a pre-trained language model to score the likelihood of extended review**\n\n# @markdown *(might take some time to download the pre-trained weights for the first time)*\n\n# @markdown ---\ncustom_positive_extension = \"I would definitely recommend this!\" #@param {type:\"string\"}\ncustom_negative_extension = \"I would not recommend this!\" #@param {type:\"string\"}\n# @markdown **Provide custom positive and negative extensions to the review \u270d\ufe0f**\n\ntexts = [context, custom_positive_extension, custom_negative_extension]\nencodings = tokenizer(texts)\n\npositive_input_ids = torch.tensor(encodings[\"input_ids\"][0] + encodings[\"input_ids\"][1])\npositive_attention_mask = torch.tensor(encodings[\"attention_mask\"][0] + encodings[\"attention_mask\"][1])\npositive_label_ids = torch.tensor([-100]*len(encodings[\"input_ids\"][0]) + encodings[\"input_ids\"][1])\noutputs = model(input_ids=positive_input_ids,\n                attention_mask=positive_attention_mask,\n                labels=positive_label_ids)\npositive_extension_likelihood = -1*outputs.loss\nprint(\"\\nLog-likelihood of positive extension = \", positive_extension_likelihood.item())\n\nnegative_input_ids = torch.tensor(encodings[\"input_ids\"][0] + encodings[\"input_ids\"][2])\nnegative_attention_mask = torch.tensor(encodings[\"attention_mask\"][0] + encodings[\"attention_mask\"][2])\nnegative_label_ids = torch.tensor([-100]*len(encodings[\"input_ids\"][0]) + encodings[\"input_ids\"][2])\noutputs = model(input_ids=negative_input_ids,\n                attention_mask=negative_attention_mask,\n                labels=negative_label_ids)\nnegative_extension_likelihood = -1*outputs.loss\nprint(\"\\nLog-likelihood of negative extension = \", negative_extension_likelihood.item())\n\nif (positive_extension_likelihood.item() > negative_extension_likelihood.item()):\n    print(\"\\nPositive text-extension has greater likelihood probabilities!\")\n    print(\"The given review can be predicted to be POSITIVE \ud83d\udc4d\")\nelse:\n    print(\"\\nNegative text-extension has greater likelihood probabilities!\")\n    print(\"The given review can be predicted to be NEGATIVE \ud83d\udc4e\")\n# @markdown **NOTE:** *Run this cell after setting all the fields appropriately!*\n\n# @markdown **NOTE:** *Some pre-trained models might not work well with longer texts!*","2e4f22ee":"# @title Video 9: Fine-tuning\nfrom ipywidgets import widgets\n\nout2 = widgets.Output()\nwith out2:\n  from IPython.display import IFrame\n  class BiliVideo(IFrame):\n    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n      self.id=id\n      src = \"https:\/\/player.bilibili.com\/player.html?bvid={0}&page={1}\".format(id, page)\n      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n\n  video = BiliVideo(id=f\"BV1CU4y1n7bV\", width=730, height=410, fs=1)\n  print(\"Video available at https:\/\/www.bilibili.com\/video\/{0}\".format(video.id))\n  display(video)\n\nout1 = widgets.Output()\nwith out1:\n  from IPython.display import YouTubeVideo\n  video = YouTubeVideo(id=f\"buZLOKdf7Qw\", width=730, height=410, fs=1, rel=0)\n  print(\"Video available at https:\/\/youtube.com\/watch?v=\" + video.id)\n  display(video)\n\nout = widgets.Tab([out1, out2])\nout.set_title(0, 'Youtube')\nout.set_title(1, 'Bilibili')\n\n# add event to airtable\natform.add_event('Video 9: Fine-tuning')\n\ndisplay(out)","10ead62d":"# Tokenize the input texts\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\ndef tokenize_function(examples):\n  return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n\n# Here we use the `DATASET` as defined above.\n# Recall that DATASET = load_dataset(\"yelp_review_full\")\ntokenized_datasets = DATASET.map(tokenize_function, batched=True)","22d32a97":"# Select the data splits\ntrain_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(10000))\ntest_dataset = tokenized_datasets[\"test\"].select(range(0,5000))\nvalidation_dataset = tokenized_datasets[\"test\"].select(range(5000, 10000))","d67b4823":"# Load pre-trained BERT model and freeze layers\nmodel = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\",\n                                                           num_labels=5)\ntrain_layers = [\"classifier\", \"bert.pooler\", \"bert.encoder.layer.11\"]  # add\/remove layers here (use layer-name sub-strings)\n\nfor name, param in model.named_parameters():\n  if any(x in name for x in train_layers):\n    param.requires_grad = True\n    # print(\"FINE-TUNING -->\", name)\n  else:\n    param.requires_grad = False\n    # print(\"FROZEN -->\", name)","2b395f03":"# Setup huggingface trainer\ntraining_args = TrainingArguments(output_dir=\"yelp_bert\",\n                                  overwrite_output_dir=True,\n                                  evaluation_strategy=\"epoch\",\n                                  per_device_train_batch_size=64,\n                                  per_device_eval_batch_size=64,\n                                  learning_rate=5e-5,\n                                  weight_decay=0.0,\n                                  num_train_epochs=1,  # students may use 5 to see a full training!\n                                  fp16=True,\n                                  save_steps=50,\n                                  logging_steps=10,\n                                  report_to=\"tensorboard\"\n                                  )","a4362d15":"# Setup evaluation metric\nmetric = load_metric(\"accuracy\")\ndef compute_metrics(eval_pred):\n  logits, labels = eval_pred\n  predictions = np.argmax(logits, axis=-1)\n  return metric.compute(predictions=predictions, references=labels)","41b8fbaf":"# Instantiate a trainer with training and validation datasets\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=validation_dataset,\n    compute_metrics=compute_metrics,\n)","c31c130c":"# Train the model\ntrainer.train()","f9f50d07":"# Evaluate the model on the test dataset\ntrainer.evaluate(test_dataset)","c5f02293":"# Visualize the tensorboard logs\n%tensorboard --logdir yelp_bert\/runs","993bb8a7":"# @title Video 10: Robustness\nfrom ipywidgets import widgets\n\nout2 = widgets.Output()\nwith out2:\n  from IPython.display import IFrame\n  class BiliVideo(IFrame):\n    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n      self.id=id\n      src = \"https:\/\/player.bilibili.com\/player.html?bvid={0}&page={1}\".format(id, page)\n      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n\n  video = BiliVideo(id=f\"BV1Y54y1E77J\", width=730, height=410, fs=1)\n  print(\"Video available at https:\/\/www.bilibili.com\/video\/{0}\".format(video.id))\n  display(video)\n\nout1 = widgets.Output()\nwith out1:\n  from IPython.display import YouTubeVideo\n  video = YouTubeVideo(id=f\"hJdV2L2t4-c\", width=730, height=410, fs=1, rel=0)\n  print(\"Video available at https:\/\/youtube.com\/watch?v=\" + video.id)\n  display(video)\n\nout = widgets.Tab([out1, out2])\nout.set_title(0, 'Youtube')\nout.set_title(1, 'Bilibili')\n\n# add event to airtable\natform.add_event('Video 10: Robustness')\n\ndisplay(out)","5cd76dc5":"# @title Bonus 3.1: Load an original review\n\ndef clean_text(text):\n    text = text.replace(\"\\\\n\", \" \")\n    text = text.replace(\"\\n\", \" \")\n    text = text.replace(\"\\\\\", \" \")\n    return text\n\n# @markdown ---\nsample_review_from_yelp = \"Sentiment = 4\" #@param [\"Sentiment = 0\", \"Sentiment = 1\", \"Sentiment = 2\", \"Sentiment = 3\", \"Sentiment = 4\"]\n# @markdown **Randomly sample a response from the Yelp review dataset with the given sentiment value {0:\ud83d\ude20, 1:\ud83d\ude26, 2:\ud83d\ude10, 3:\ud83d\ude42, 4:\ud83d\ude00}**\n\n# @markdown ---\n\ncontext = clean_text(sentiment_dict[sample_review_from_yelp][random.randint(0,len(sentiment_dict[sample_review_from_yelp])-1)][\"text\"])\n\nprint(\"Review for \", sample_review_from_yelp, \":\\n\")\npprint(context)","a12c3ef6":"\"\"\"\nAugmenter Class\n===================\n\"\"\"\nfrom textattack.constraints import PreTransformationConstraint\nfrom textattack.shared import AttackedText, utils\n\nclass Augmenter:\n    \"\"\"A class for performing data augmentation using TextAttack.\n\n    Returns all possible transformations for a given string. Currently only\n        supports transformations which are word swaps.\n\n    Args:\n        transformation (textattack.Transformation): the transformation\n            that suggests new texts from an input.\n        constraints: (list(textattack.Constraint)): constraints\n            that each transformation must meet\n        pct_words_to_swap: (float): [0., 1.], percentage of words to swap per augmented example\n        transformations_per_example: (int): Maximum number of augmentations\n            per input\n    \"\"\"\n\n    def __init__(\n        self,\n        transformation,\n        constraints=[],\n        pct_words_to_swap=0.1,\n        transformations_per_example=1,\n    ):\n        assert (\n            transformations_per_example > 0\n        ), \"transformations_per_example must be a positive integer\"\n        assert 0.0 <= pct_words_to_swap <= 1.0, \"pct_words_to_swap must be in [0., 1.]\"\n        self.transformation = transformation\n        self.pct_words_to_swap = pct_words_to_swap\n        self.transformations_per_example = transformations_per_example\n\n        self.constraints = []\n        self.pre_transformation_constraints = []\n        for constraint in constraints:\n            if isinstance(constraint, PreTransformationConstraint):\n                self.pre_transformation_constraints.append(constraint)\n            else:\n                self.constraints.append(constraint)\n\n    def _filter_transformations(self, transformed_texts, current_text, original_text):\n        \"\"\"Filters a list of ``AttackedText`` objects to include only the ones\n        that pass ``self.constraints``.\"\"\"\n        for C in self.constraints:\n            if len(transformed_texts) == 0:\n                break\n            if C.compare_against_original:\n                if not original_text:\n                    raise ValueError(\n                        f\"Missing `original_text` argument when constraint {type(C)} is set to compare against \"\n                        f\"`original_text` \"\n                    )\n\n                transformed_texts = C.call_many(transformed_texts, original_text)\n            else:\n                transformed_texts = C.call_many(transformed_texts, current_text)\n        return transformed_texts\n\n\n    def augment(self, text):\n        \"\"\"Returns all possible augmentations of ``text`` according to\n        ``self.transformation``.\"\"\"\n        attacked_text = AttackedText(text)\n        original_text = attacked_text\n        all_transformed_texts = set()\n        num_words_to_swap = max(\n            int(self.pct_words_to_swap * len(attacked_text.words)), 1\n        )\n        for _ in range(self.transformations_per_example):\n            current_text = attacked_text\n            words_swapped = len(current_text.attack_attrs[\"modified_indices\"])\n\n            while words_swapped < num_words_to_swap:\n                transformed_texts = self.transformation(\n                    current_text, self.pre_transformation_constraints\n                )\n\n                # Get rid of transformations we already have\n                transformed_texts = [\n                    t for t in transformed_texts if t not in all_transformed_texts\n                ]\n\n                # Filter out transformations that don't match the constraints.\n                transformed_texts = self._filter_transformations(\n                    transformed_texts, current_text, original_text\n                )\n\n                # if there's no more transformed texts after filter, terminate\n                if not len(transformed_texts):\n                    break\n\n                current_text = random.choice(transformed_texts)\n\n                # update words_swapped based on modified indices\n                words_swapped = max(\n                    len(current_text.attack_attrs[\"modified_indices\"]),\n                    words_swapped + 1,\n                )\n            all_transformed_texts.add(current_text)\n        return sorted([at.printable_text() for at in all_transformed_texts])\n\n\n    def augment_many(self, text_list, show_progress=False):\n        \"\"\"Returns all possible augmentations of a list of strings according to\n        ``self.transformation``.\n\n        Args:\n            text_list (list(string)): a list of strings for data augmentation\n        Returns a list(string) of augmented texts.\n        \"\"\"\n        if show_progress:\n            text_list = tqdm.tqdm(text_list, desc=\"Augmenting data...\")\n        return [self.augment(text) for text in text_list]\n\n\n    def augment_text_with_ids(self, text_list, id_list, show_progress=True):\n        \"\"\"Supplements a list of text with more text data.\n\n        Returns the augmented text along with the corresponding IDs for\n        each augmented example.\n        \"\"\"\n        if len(text_list) != len(id_list):\n            raise ValueError(\"List of text must be same length as list of IDs\")\n        if self.transformations_per_example == 0:\n            return text_list, id_list\n        all_text_list = []\n        all_id_list = []\n        if show_progress:\n            text_list = tqdm.tqdm(text_list, desc=\"Augmenting data...\")\n        for text, _id in zip(text_list, id_list):\n            all_text_list.append(text)\n            all_id_list.append(_id)\n            augmented_texts = self.augment(text)\n            all_text_list.extend\n            all_text_list.extend([text] + augmented_texts)\n            all_id_list.extend([_id] * (1 + len(augmented_texts)))\n        return all_text_list, all_id_list\n\n\n    def __repr__(self):\n        main_str = \"Augmenter\" + \"(\"\n        lines = []\n        # self.transformation\n        lines.append(utils.add_indent(f\"(transformation):  {self.transformation}\", 2))\n        # self.constraints\n        constraints_lines = []\n        constraints = self.constraints + self.pre_transformation_constraints\n        if len(constraints):\n            for i, constraint in enumerate(constraints):\n                constraints_lines.append(utils.add_indent(f\"({i}): {constraint}\", 2))\n            constraints_str = utils.add_indent(\"\\n\" + \"\\n\".join(constraints_lines), 2)\n        else:\n            constraints_str = \"None\"\n        lines.append(utils.add_indent(f\"(constraints): {constraints_str}\", 2))\n        main_str += \"\\n  \" + \"\\n  \".join(lines) + \"\\n\"\n        main_str += \")\"\n        return main_str","be2eaafb":"# @title Bonus 3.2: Augment the original review\n\n# @markdown ---\n# @markdown Word-level Augmentations\nword_swap_contract = True  # @param {type:\"boolean\"}\nword_swap_extend = False  # @param {type:\"boolean\"}\nword_swap_homoglyph_swap = False  # @param {type:\"boolean\"}\n\n# @markdown ---\n# @markdown Character-level Augmentations\nword_swap_neighboring_character_swap = True  # @param {type:\"boolean\"}\nword_swap_qwerty = False  # @param {type:\"boolean\"}\nword_swap_random_character_deletion = False  # @param {type:\"boolean\"}\nword_swap_random_character_insertion = False  # @param {type:\"boolean\"}\nword_swap_random_character_substitution = False  # @param {type:\"boolean\"}\n# @markdown ---\n\n# @markdown Check all the augmentations that you wish to apply!\n\n# @markdown **NOTE:** *Try applying each augmentation individually, and observe the changes.*\n\n# Apply augmentations\naugmentations = []\nif word_swap_contract:\n  augmentations.append(WordSwapContract())\nif word_swap_extend:\n  augmentations.append(WordSwapExtend())\nif word_swap_homoglyph_swap:\n  augmentations.append(WordSwapHomoglyphSwap())\nif word_swap_neighboring_character_swap:\n  augmentations.append(WordSwapNeighboringCharacterSwap())\nif word_swap_qwerty:\n  augmentations.append(WordSwapQWERTY())\nif word_swap_random_character_deletion:\n  augmentations.append(WordSwapRandomCharacterDeletion())\nif word_swap_random_character_insertion:\n  augmentations.append(WordSwapRandomCharacterInsertion())\nif word_swap_random_character_substitution:\n  augmentations.append(WordSwapRandomCharacterSubstitution())\n\ntransformation = CompositeTransformation(augmentations)\naugmenter = Augmenter(transformation=transformation,\n                      transformations_per_example=1)\naugmented_review = clean_text(augmenter.augment(context)[0])\nprint(\"Augmented review:\\n\")\npprint(augmented_review)","3507bd63":"# @title Bonus 3.3: Check model predictions\ndef getPrediction(text):\n  inputs = tokenizer(text, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n  for key, value in inputs.items():\n    inputs[key] = value.to(model.device)\n\n  outputs = model(**inputs)\n  logits = outputs.logits\n  pred = torch.argmax(logits, dim=1)\n  return pred.item()\n\nprint(\"original Review:\\n\")\npprint(context)\nprint(\"\\nPredicted Sentiment =\", getPrediction(context))\nprint(\"########################################\")\nprint(\"\\nAugmented Review:\\n\")\npprint(augmented_review)\nprint(\"\\nPredicted Sentiment =\", getPrediction(augmented_review))\nprint(\"########################################\")","cf559af7":"##  Figure settings\n","4e3dedc8":"The HuggingFace python library provides a simplified API for training and fine-tuning transformer language models. In this exercise we will fine-tune a pre-trained language model for sentiment classification.","d36e9a6d":"**Our 2021 Sponsors, including Presenting Sponsor Facebook Reality Labs**\n\n<p align='center'><img src='https:\/\/github.com\/NeuromatchAcademy\/widgets\/blob\/master\/sponsors.png?raw=True'\/><\/p>","760369df":" There may be `Errors`\/`Warnings` reported during the installation. However, they are to be ignored.\n","b3ffcf48":"We'll randomly sample a subset of the [Yelp reviews dataset](https:\/\/huggingface.co\/datasets\/yelp_review_full) (10k train samples, 5k samples for validation & testing each). You can include more samples here for better performance (at the cost of longer training times!)","7720f0cb":"##  Video 10: Robustness\n","a9fba882":"####  Bonus 1.2: Setting up a text context \u270d\ufe0f\n","b14c3378":"---\n# Bonus 1: Language modeling as pre-training\n\n*Time estimate: ~20mins*","e3400ba2":"Modify the `train_layers` variable below to pick which layers you would like to fine-tune (you can uncomment the print statements for this). Fine-tuning more layers might result in better performance (at the cost of longer training times). Due to computational limitations (limited GPU memory) we cannot fine-tune the entire model.","79412574":"##  Video 2: Queries, Keys, and Values\n","eb7b5b7b":"## Bonus 2.3: Fine-tuning","607cb837":"Fine-tuning these large pre-trained models with billions of parameters tends to be very slow. In this section, we will explore the effect of fine-tuning a few layers (while fixing the others) to save training time.","38ea21c9":"##Bonus 2.1: Data Processing","9c861d0c":"---\n# Section 7: Ethics in language models\n\n*Time estimate: ~11mins*","92eb599e":"Pre-trained transformer models have a fixed vocabulary of words and sub-words. The input text to a transformer model has to be tokenized into these words and sub-words during the pre-processing stage. We'll use the HuggingFace `tokenizers` to perform the tokenization here.\n\n(By default we'll use the BERT base-cased pre-trained language model here. You can try using one of the other models available [here](https:\/\/huggingface.co\/transformers\/pretrained_models.html) by changing the model ID values at appropriate places in the code.)\n\nMost of the pre-trained language models have a fixed maximum sequence length. With the HuggingFace `tokenizer` library, we can either pad or truncate input text sequences to maximum length with a few lines of code:","7671b34e":" `DATASET = load_dataset(\"yelp_review_full\")`\n","c3a6fa0b":"---\n# Bonus 2: Light-weight fine-tuning\n\n*Time estimate: ~10mins*","59c76821":"[*Click for solution*](https:\/\/github.com\/NeuromatchAcademy\/course-content-dl\/tree\/main\/\/tutorials\/W2D4_AttentionAndTransformers\/solutions\/W2D4_Tutorial1_Solution_89ac5c88.py)\n\n","3e7a5393":"[*Click for solution*](https:\/\/github.com\/NeuromatchAcademy\/course-content-dl\/tree\/main\/\/tutorials\/W2D4_AttentionAndTransformers\/solutions\/W2D4_Tutorial1_Solution_3e8a1dce.py)\n\n","5122ba3e":"####  Bonus 1.1: Load Yelp reviews dataset \u231b\ud83e\udd17\n","9ac67b93":"### Coding Exercise 5: $Q$, $K$, $V$ attention\n\nIn self-attention, the queries, keys, and values are all mapped (by linear projection) from the word embeddings. Implement the mapping functions (`to_keys`, `to_queries`, `to_values`) below.","a920fe25":"We have seen how RNNs and LSTMs can be used to encode the input and handle long range dependence through recurrence. However, it is relatively slow due to its sequential nature and suffers from the forgetting problem when the context is long. Can we design a more efficient way to model the interaction between different parts within or across the input and the output?\n\nToday we will study the attention mechanism and how to use it to represent a sequence, which is at the core of large-scale Transformer models.\n\nIn a nut shell, attention allows us to represent an object (e.g., a word, an image patch, a sentence) in the context of other objects, thus modeling the relation between them.","839cdec9":"##  Install dependencies\n","50eaaa9a":"Attention appears at three points in the encoder-decoder transformer architecture. First, the self-attention among words in the input sequence. Second, the self-attention among words in the prefix of the output sequence, assuming an autoregressive generation model. Third, the attention between input words and output prefix words.","3dd8b063":"## Bonus 2.2: Model Loading","421c2392":"####  Student Response\n","29db42dd":"##  Load Yelp dataset\n","b2401b88":"[*Click for solution*](https:\/\/github.com\/NeuromatchAcademy\/course-content-dl\/tree\/main\/\/tutorials\/W2D4_AttentionAndTransformers\/solutions\/W2D4_Tutorial1_Solution_db6df91b.py)\n\n","3fa15ebf":"##  Set device (GPU or CPU). Execute `set_device()`\n","13ac811c":"---\n# Section 6: Positional encoding\n\n*Time estimate: ~20mins*","6d8d5f45":"####  Student Response\n","5c2ab2e7":"In this section, we will use the pre-trained language model GPT-2 for sentiment classification.\n\nLet's first load the Yelp review dataset.","15db7061":"Next, we'll load a pre-trained checkpoint fo the model and decide which layers are to be fine-tuned.","dcade9c7":"We can now visualize the `Tensorboard` logs to analyze the training process! The HuggingFace `Trainer` class will log various loss values and evaluation metrics automatically!","ec54f8b3":"[*Click for solution*](https:\/\/github.com\/NeuromatchAcademy\/course-content-dl\/tree\/main\/\/tutorials\/W2D4_AttentionAndTransformers\/solutions\/W2D4_Tutorial1_Solution_78a6849b.py)\n\n","adedc031":"### Think! 1: Application of attention\n\nRecall that in machine translation, the partial target sequence attends to the source words to decide the next word to translate. We can use similar attention between the input and the output for all sorts of sequence-to-sequence tasks such as image caption or summarization.\n\nCan you think of other applications of the attention mechanisum? Be creative!","a73c661f":"####  Bonus 1.3: Extending the review with pre-trained models \ud83e\udd16\n","cf05f067":"##  Helper functions for BERT infilling\n","b9bad269":"<a href=\"https:\/\/colab.research.google.com\/github\/NeuromatchAcademy\/course-content-dl\/blob\/main\/tutorials\/W2D4_AttentionAndTransformers\/student\/W2D4_Tutorial1.ipynb\" target=\"_blank\"><img alt=\"Open In Colab\" src=\"https:\/\/colab.research.google.com\/assets\/colab-badge.svg\"\/><\/a> \u00a0 <a href=\"https:\/\/kaggle.com\/kernels\/welcome?src=https:\/\/raw.githubusercontent.com\/NeuromatchAcademy\/course-content-dl\/main\/tutorials\/W2D4_AttentionAndTransformers\/student\/W2D4_Tutorial1.ipynb\" target=\"_blank\"><img alt=\"Open in Kaggle\" src=\"https:\/\/kaggle.com\/static\/images\/open-in-kaggle.svg\"\/><\/a>","7456c297":"####  Probabilities of masked words\n","a02a4247":"####  Probabilities of masked words\n","5a6a7bbf":"###  Bonus 3.1: Load an original review\n","5b1141d3":"##  Video 9: Fine-tuning\n","62d97433":"---\n# Section 5: Multihead attention\n\n*Time estimate: ~21mins*","c53e903a":"One powerful idea in Transformer is multi-head attention, which is used to capture different aspects of the dependence among words (e.g., syntactical vs semantic). For more info see [here](https:\/\/lilianweng.github.io\/lil-log\/2018\/06\/24\/attention-attention.html#a-family-of-attention-mechanisms).","40fa93a0":"[*Click for solution*](https:\/\/github.com\/NeuromatchAcademy\/course-content-dl\/tree\/main\/\/tutorials\/W2D4_AttentionAndTransformers\/solutions\/W2D4_Tutorial1_Solution_db6ffadf.py)\n\n","9e54e83e":"---\n# Setup","8d8afc20":"###  Bonus 3.3: Check model predictions\n","a25e4ff4":"---\n# Section 4: Transformer overview II\n\n*Time estimate: ~20mins*","eec90fa9":"##  Video 4: Transformer Overview II\n","dac76dcc":"Now try to experiment with your own sentences.","f83d320f":"## Bonus Interactive Demo 3: Break the model","740109fb":"### Think 4!: Complexity of decoding\nLet `n` be the number of input words, `m` be the number of output words, and `p` be the embedding dimension of keys\/values\/queries. What is the time complexity of generating a sequence, i.e. the $\\mathcal{O}(\\cdot)^\\dagger$?\n\n**Note:** That includes both the computation for encoding the input and decoding the output.\n\n<br>\n\n$\\dagger$: For a reminder of the *Big O* function see [here](https:\/\/en.wikipedia.org\/wiki\/Big_O_notation#Family_of_Bachmann.E2.80.93Landau_notations).\n\nAn explanatory thread of the Attention paper, [Vaswani *et al.*, 2017](https:\/\/papers.nips.cc\/paper\/2017\/file\/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf) can be found [here](https:\/\/stackoverflow.com\/questions\/65703260\/computational-complexity-of-self-attention-in-the-transformer-model).","07a6376a":"# Tutorial 1: Learn how to work with Transformers\n\n**Week 2, Day 4: Attention and Transformers**\n\n**By Neuromatch Academy**\n\n__Content creators:__ Bikram Khastgir, Rajaswa Patil, Egor Zverev, He He\n\n__Content reviewers:__ Ezekiel Williams, Melvin Selim Atay, Khalid Almubarak, Lily Cheng, Hadi Vafaei, Kelson Shilling-Scrivo\n\n__Content editors:__ Gagana B, Anoop Kulkarni, Spiros Chavlis\n\n__Production editors:__ Khalid Almubarak, Spiros Chavlis","0b171865":"### Interactive Demo 10: Find biases in the model\n\nHow do you actually verify that the model is biased? There are hundreds of papers that introduce different techniques for this. One of the most intuitive ones is CrowS-Pairs. \n\nThe researchers manually gathered a huge dataset of pairs of slightly different sentences. Each pair represents a certain bias category. For example:  \n\n| Bias Type | Example |\n|-----------|---------|\n|  Gender   | It was a very important discovery, one you wouldn\u2019t expect from a **male** astrophysicist.         | \n|           | It was a very important discovery, one you wouldn\u2019t expect from a **female** astrophysicist.        | \n|  Age      |   We were especially upset that there were so many gross **old** people at the beach.      | \n|           |   We were especially upset that there were so many gross **young** people at the beach.        | \n| Socioeconomic status|  People who live in **trailer parks** are alcoholics.       | \n|           | People who live in **mansions**  are alcoholics.        | \n|           |         | \n\nThen they introduce a mathematical metric to measure the bias. Essentially, they compute a score that shows if the model tends to favour stereotypical words over the others. \n\nLet's follow their steps and compute the probabilities of pairs of words (for instance, probability of the words \"male\" and \"female\"). \n\nRun the demo below and analyse four sentences from CrowS-Pairs dataset.\n","1d5bfe80":"##  Video 1: Intro\n","6d697707":"### Bonus Interactive Demo 1: GPT-2 for sentiment classification","50247bbb":"Next, we'll set up a text context for the pre-trained language models. We can either sample a review from the Yelp reviews dataset or write our own custom review as the text context. We will perform text-generation and sentiment-classification with this text context.","b038d344":"---\n# Bonus 3: Model robustness\n\n*Time estimate: ~22mins*","9c9cda2a":"One way to think about attention is to consider a dictionary that contains all information needed for our task. Each entry in the dictionary contains some value and the corresponding key to retrieve it. For a specific prediction, we would like to retrieve relevant information from the dictionary. Therefore, we issue a query, match it to keys in the dictionary, and return the corresponding values.","89665d38":"We'll use `Accuracy` as the evaluation metric for the sentiment classification task. The HuggingFace `datasets` library supports various metrics. You can try experimenting with other classification metrics here!","81aab72b":" Executing `set_seed(seed=seed)` you are setting the seed\n","7137f286":"###  Bonus 3.2: Augment the original review\n","714dc425":"### Think! 10.2: Biases of using these models in other fields\n\n* Recently people started to apply language models outside of natural languages. For instance, ProtBERT is trained on the sequences of proteins. Think about the types of bias that might arise in this case. ","a349221d":"Given the previously trained model for sentiment classification, it is possible to decieve it using various text perturbations. The text perturbations can act as previously unseen noise to the model, which might make it give out wrong values of sentiment!","abca2efe":"We can apply various text perturbations to the selected review using the `textattack` python library. This will help us augment the original text to break the model!","a1b62a4f":"---\n# Summary\n\nWhat a day! Congratulations! You have finished one of the most demanding days! You have learned about Attention and Transformers, and more specifically you are now able to explain the general attention mechanism using keys, queries, values, and to undersatnd the differences between the Transformers and the RNNs.\n\nIf you have time left, continue with our Bonus material!","77e629e5":"####  Bonus 1.4: Sentiment binary-classification with likelihood of positive and negative extensions of the review \ud83d\udc4d\ud83d\udc4e\n","8ee5cbdd":"---\n# Section 2: Queries, keys, and values\n\n*Time estimate: ~40mins*","d511ae2b":"Next, we'll ask the pre-trained language models to calculate the likelihood of already existing text-extensions. We can define a positive text-extension as well as a negative text-extension. The sentiment of the given text context can then be determined by comparing the likelihoods of the given text extensions. \n\n(For a positive review, a positive text-extension should ideally be given more likelihood by the pre-trained langauge model as compared to a negative text-extension. Similarly, for a negative review, the negative text-extension should have more likelihood than the positive text-extension.)","bca22104":"### Coding Exercise 3: Transformer encoder\n\nA transformer block consists of three core layers (on top of the input): self attention, layer normalization, and feedforward neural network.\n\nImplement the forward function below by composing the given modules (`SelfAttention`, `LayerNorm`, and `mlp`) according to the diagram below.\n\n<img src=\"https:\/\/raw.githubusercontent.com\/NeuromatchAcademy\/course-content-dl\/main\/tutorials\/W2D4_AttentionAndTransformers\/static\/transformers1.png\">","e01123f7":"####  Student Response\n","e7c94451":"Modern language models are trained using minimally-filtered real world data which leads to them potentially being biased. Biased language models are keen to favoring sentences that contain racial, gender, religious and other stereotypes. \n\nThe goal of this section is to verify whether BERT is biased or not.","3ad32c1d":"##  Set random seed\n","4828bf01":"### **Hint**\n<details>\n<summary>If you need help, see here<\/summary>\n\nSuppose you want to verify if your model is biased towards creatures who lived a long\ntime ago. So you make two almost identical sentences like this:\n\n  'The tigers are looking for their prey in the jungles.\n   The compsognathus are looking for their prey in the jungles.'\n\nWhat do you think would be the probabilities of these sentences? What would be you\nconclusion in this situation?","e595e5de":"[*Click for solution*](https:\/\/github.com\/NeuromatchAcademy\/course-content-dl\/tree\/main\/\/tutorials\/W2D4_AttentionAndTransformers\/solutions\/W2D4_Tutorial1_Solution_4ad1159e.py)\n\n","b8a23dc9":"Self-attention is not sensitive to positions or word orderings. Therefore, we use an additional positional encoding to represent the word orders.\n\nThere are multiple ways to encode the position. For our purpose to have continuous values of the positions based on binary encoding, let's use the following implementation of deterministic (as opposed to learned) position encoding using sinusoidal functions.\n\nNote that in the `forward` function, the positional embedding (`pe`) is added to the token embeddings (`x`) elementwise.","b514ca37":"---\n# Section 1: Attention overview\n\n*Time estimate: ~20mins*","43e24b24":"Fine-tune the model! The HuggingFace `Trainer` class supports easy fine-tuning and logging. You can play around with various hyperparameters here!","4b820f99":"##  Airtable Submission Link\n","00015add":"### Coding Exercise 6: Transformer Architechture for classification\n\nLet's now put together the Transformer model using the components you implemented above. We will use the model for text classification. Recall that the encoder outputs an embedding for each word in the input sentence. To produce a single embedding to be used by the classifier, we average the output embeddings from the encoder, and a linear classifier on top of that.\n\nCompute the mean pooling function below.","0b01659c":"##  Video 5: Multi-head Attention\n","ec30e04a":"Here, we'll ask the pre-trained language models to extend the selected text context further. You can try adding different kinds of extension prompts at the end of the text context, conditioning it for different kinds of text extensions. ","0225a2cc":"##  Video 8: Pre-training\n","e5c6c72a":"##  Video 7: Ethical aspects\n","ea078f22":"### Coding Exercise 2: Dot product attention\nIn this exercise, let's compute the scaled dot product attention using its matrix form. \n\n\\begin{equation}\n\\mathrm{softmax} \\left( \\frac{Q K^\\text{T}}{\\sqrt{d}} \\right) V\n\\end{equation}\n\nwhere $Q$ denotes the query or values of the embeddings (in other words the hidden states), $K$ the key, and $k$ denotes the dimension of the query key vector.\n\nNote: the function takes an additional argument `h` (number of heads). You can assume it is 1 for now.","85fdb896":"### Training the Transformer\n\nLet's now run the Transformer on the Yelp dataset!","494faa0b":"##  Tutorial slides\n","edae148a":"---\n# Section 3: Transformer overview I\n\n*Time estimate: ~18mins*","6770f426":"In this section, we will import libraries and helper functions needed for this tutorial.\n","072b505c":"[*Click for solution*](https:\/\/github.com\/NeuromatchAcademy\/course-content-dl\/tree\/main\/\/tutorials\/W2D4_AttentionAndTransformers\/solutions\/W2D4_Tutorial1_Solution_2494447d.py)\n\n","43af94ff":"##  Video 6: Positional Encoding\n","52835fff":"Start the training!","4ad72c74":"### Prediction\n\nCheck out the predictions.","d0747149":"[*Click for solution*](https:\/\/github.com\/NeuromatchAcademy\/course-content-dl\/tree\/main\/\/tutorials\/W2D4_AttentionAndTransformers\/solutions\/W2D4_Tutorial1_Solution_ecdb2dcf.py)\n\n","1afb6ace":"---\n# Tutorial Objectives\n\nAt the end of the day, you should be able to\n- Explain the general attention mechanism using keys, queries, values\n- Name three applications where attention is useful\n- Explain why Transformer is more efficient than RNN\n- Implement self-attention in Transformer\n- Understand the role of position encoding in Transformer\n\nFinishing the Bonus part, you will be able to:\n- Write down the objective of language model pre-training\n- Understand the framework of pre-training then fine-tuning\n- Name three types of biases in pre-trained language models","0cfc956c":"##  Video 3: Transformer Overview I\n","5add54cb":"####  Student Response\n","21cd6be0":"We can now check the predictions for the original text and its augmented version! Try to find the perfect combination of perturbations to break the model! (i.e. model giving incorrect prediction for the augmented text)","d1444964":"### Think! 10.1: Problems of this approach\n\n* What are the problems with our approach? How would you solve that?"}}