{"cell_type":{"17ff9f1d":"code","6def054f":"code","628c25f7":"code","18878a02":"code","1a4b82e7":"code","06080ee9":"code","85ffda58":"code","845dd900":"code","14aebea4":"code","142fc807":"code","4ed154e0":"code","7ddb79a3":"code","8e8b62ca":"code","b1609aca":"code","2fdf4da3":"code","70521069":"code","e5d09f44":"code","2381467d":"code","b9ea032d":"code","9a1d6164":"code","d6d7d460":"code","62c8f5ec":"code","22972ebf":"code","b8420e63":"code","1ba55ae0":"code","53c5d715":"code","6e251900":"code","ab62b3f2":"code","81363712":"code","b2b66531":"code","92fd4295":"code","19e2d7cb":"code","cef343fc":"code","98954818":"code","bd0e9da6":"code","d970cc0c":"code","3f12381b":"code","337082dd":"code","0080c93c":"code","635486d1":"code","2ab248e8":"code","cb3c7433":"code","79286acf":"code","987dbbbb":"code","74ae8c79":"code","78b92255":"code","76b56b0d":"code","f83329f3":"code","b5117242":"code","78c1611f":"code","f0121314":"code","28c45e7f":"code","4cd529f1":"code","c0968a46":"code","fa2cc5f0":"markdown","cc43aaf7":"markdown","20f34ed5":"markdown","e334a9a0":"markdown","02a50205":"markdown","54d3253e":"markdown","f8507fc9":"markdown","c6c33a75":"markdown","e285d6e6":"markdown","8af2faf7":"markdown","dfd12b0c":"markdown","d04f7fb6":"markdown","2ea43f59":"markdown","d77a84c9":"markdown","b6bd58d5":"markdown","01aa6a85":"markdown","f01d7a4b":"markdown","b9aecbb4":"markdown","7c1caa04":"markdown","59a34e8d":"markdown","4589cf85":"markdown","1b2c9f6d":"markdown","27fb19a7":"markdown","04a0546d":"markdown","9695cad7":"markdown","9a5d669f":"markdown","ae88748d":"markdown","15989dbf":"markdown","3eb74043":"markdown","85f3395c":"markdown","5422d87e":"markdown","35d7b4eb":"markdown","7adc5d69":"markdown","46525846":"markdown"},"source":{"17ff9f1d":"#utils\nimport pandas as pd\nimport numpy as np\nfrom math import ceil\nimport warnings\nwarnings.filterwarnings('ignore')\n\n#visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#pipeline\nfrom sklearn.pipeline import make_pipeline, make_union\nfrom sklearn.compose import make_column_transformer, make_column_selector\nfrom sklearn import set_config                      \n\n#preprocessing\nfrom sklearn.impute import KNNImputer, SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder, MinMaxScaler, RobustScaler, StandardScaler, FunctionTransformer, KBinsDiscretizer\n\n#feature selection\nfrom sklearn.decomposition import PCA\n\n#evaluation\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, learning_curve, GridSearchCV\nfrom sklearn.metrics import accuracy_score, classification_report, roc_curve, auc\n\n#models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom xgboost import XGBClassifier","6def054f":"filepath = '..\/input\/titanic\/train.csv'","628c25f7":"data = pd.read_csv(filepath)\ndata.head(5)","18878a02":"print(f'The train set is composed of {data.shape[0]} observations and {data.shape[1]} features.')","1a4b82e7":"print(f'Number of duplicated observations: {data.duplicated().sum()}')","06080ee9":"print('Feature | % of dictinct values\\n')\nfor feature in data.columns:\n    print(f'{feature:>12}: {round(data[feature].nunique()\/len(data), 3):>.2%}')","85ffda58":"print('Feature | % of null values\\n')\nfor feature in data.columns:\n    print(f'{feature:>12}: {round(data[feature].isna().sum()\/len(data), 3):>.2%}')","845dd900":"drop_features = ['Survived', 'PassengerId', 'Name', 'Ticket', 'Cabin']\nX = data.drop(columns = drop_features)\ny = data['Survived']","14aebea4":"cat_feature = ['Pclass', 'Sex', 'Embarked']\nnum_feature = ['Age', 'SibSp', 'Parch', 'Fare']","142fc807":"plt.figure(figsize = (10, 10))\nplt.suptitle('Discrete variables', weight = 'bold')\ni = 0\nfor feature in cat_feature:\n    i += 1 \n    plt.subplot(ceil(len(cat_feature)\/2), ceil(len(cat_feature)\/2), i)\n    sns.countplot(X[feature], hue = y)\n    plt.title(feature, weight = 'bold')\nplt.show()","4ed154e0":"plt.figure(figsize = (10, 10))\nplt.suptitle('Continous variables', weight = 'bold')\ni = 0\nfor feature in num_feature:\n    i += 1 \n    plt.subplot(ceil(len(cat_feature)\/2), ceil(len(cat_feature)\/2), i)\n    sns.distplot(a = X[feature][y.to_numpy(dtype = bool)], hist = False, label = 'Survived', kde_kws = {'bw' : 2})\n    sns.distplot(a = X[feature][np.invert(y.to_numpy(dtype = bool))], hist = False, label = 'Not survived', kde_kws = {'bw' : 2})\n    plt.legend()\n    plt.title(feature, weight = 'bold')\nplt.show()","7ddb79a3":"plt.figure(figsize = (16, 8))\nplt.suptitle('Boxplot of continuous variables', weight = 'bold')\ni = 0\nfor feature in num_feature:\n    i += 1 \n    plt.subplot(ceil(len(cat_feature)\/2), ceil(len(cat_feature)\/2), i)\n    sns.boxplot(X[feature])\nplt.show()","8e8b62ca":"plt.figure()\nplt.title('Features correlations heatmap', weight = 'bold')\nsns.heatmap(pd.concat([X, y], axis = 1).corr())\nplt.show()","b1609aca":"pp = sns.pairplot(pd.concat([X, y], axis = 1))\npp.set(xticklabels=[])","2fdf4da3":"X.info()","70521069":"X = X.astype({name:'category' for name in cat_feature})","e5d09f44":"X.info()","2381467d":"cat_pipe = make_pipeline(SimpleImputer(strategy='most_frequent'),       \n                                        OneHotEncoder(drop = 'first')) # Categorical features pipeline\n\nnum_pipe = make_pipeline(KNNImputer()) # Numerical features pipeline \n\npreprocessor = make_column_transformer((num_pipe, make_column_selector(dtype_exclude=\"category\")),\n                                       (cat_pipe, make_column_selector(dtype_include=\"category\"))) #preprocessing pipeline","b9ea032d":"# from sklearn import set_config\nset_config(display='diagram')\npreprocessor","9a1d6164":"def plot_results(model_name, y_test, y_pred, N, train_score, val_score, fpr, tpr, roc_auc):\n    plt.figure(figsize = (14, 6))\n    plt.suptitle(f'{model_name} (accuracy: {round(accuracy_score(y_test, y_pred), 3)})', weight = 'bold')\n    \n    plt.subplot(1, 2, 1)\n    plt.plot(N, train_score.mean(axis = 1), label = 'train', color = 'navy')\n    plt.plot(N, val_score.mean(axis = 1), label = 'validation', color='darkorange')\n    plt.title('Learning curve')\n    plt.xlabel('Observations')\n    plt.ylabel('Accuracy')\n    plt.legend()\n\n    plt.subplot(1, 2, 2)\n    plt.plot(fpr, tpr, color='darkorange', label='ROC curve (area = %0.2f)' % roc_auc)\n    plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC curve')\n    plt.legend(loc=\"lower right\")\n    plt.show()   \n    \n    print(classification_report(y_test, y_pred))\n\ndef evaluation(X, y, preprocessing_pipeline, models: dict, feature_selection = 'passthrough', random_state = 0, test_size = 0.2):\n    \n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_size, random_state = random_state, stratify = y)\n    cv = StratifiedKFold(n_splits=5, shuffle = True, random_state=random_state)\n    \n    for key, sub_key in models.items():\n        \n        pipe = make_pipeline(preprocessing_pipeline, feature_selection, sub_key.get('model')) #As default feature_selection is passthrough       \n        param_grid = sub_key.get('param_grid')\n        search = GridSearchCV(pipe, param_grid , cv = cv)\n        search.fit(X_train, y_train)\n        y_pred = search.best_estimator_.predict(X_test)\n        y_pred_proba = search.best_estimator_.predict_proba(X_test)\n                \n        fpr, tpr, _ = roc_curve(y_test, y_pred_proba[:,1])\n        roc_auc = auc(fpr, tpr)\n                \n        N, train_score, val_score = learning_curve(search.best_estimator_,\n                                                   X_train,\n                                                   y_train,\n                                                   cv = cv,\n                                                   scoring = 'accuracy', \n                                                   train_sizes = np.linspace(0.1, 1, 10))\n\n        plot_results(key, y_test, y_pred, N, train_score, val_score, fpr, tpr, roc_auc)\n        print(f'Parameters: {search.best_params_}')\n","d6d7d460":"seed = 42\nlr = {'LogisticRegression' : {'model': LogisticRegression(random_state = seed),\n                              'param_grid': {}}}","62c8f5ec":"evaluation(X, y, preprocessor, models = lr, random_state = seed)","22972ebf":"X['Name'] = data.Name.astype(dtype = 'category')","b8420e63":"title_names = X.Name.str.split(',', expand = True)[1].str.split('.', expand = True)[0].value_counts() > 10\nprint(title_names)","1ba55ae0":"title_list = title_names.index[title_names].tolist()","53c5d715":"def custom_title_transformer(x):\n    x = x.iloc[:,0].str.split(',',expand=True)[1].str.split(\".\", expand=True)[0]\n    x = x.apply(lambda x: 'Misc' if x not in title_list else x)\n    return x.to_frame(name = 'Title') # .to_frame() because make_column_transformer need a dataframe object in output","6e251900":"title_transformer = make_column_transformer((FunctionTransformer(custom_title_transformer), ['Name']),\n                                            remainder = 'passthrough')","ab62b3f2":"is_alone_pipe = make_pipeline(FunctionTransformer(lambda x: x.sum(axis = 1).to_frame()),\n                              FunctionTransformer(lambda x: (x >= 1).astype(int)))","81363712":"cat_pipe = make_pipeline(title_transformer,\n                         SimpleImputer(strategy = 'most_frequent'), \n                         OneHotEncoder(drop='first'))","b2b66531":"family_transformer = make_column_transformer((FunctionTransformer(lambda x: x.sum(axis = 1).to_frame(name = 'Family')), ['SibSp', 'Parch']),\n                                             remainder = 'passthrough') \n# .to_frame() because make_column_transformer need a dataframe object in output\n# remainder = 'passthrough' to indicate not to drop the columns untransformed","92fd4295":"num_2_cat_pipe = make_pipeline(KNNImputer(),\n                               KBinsDiscretizer(n_bins = 5, strategy = 'quantile'))","19e2d7cb":"num_pipe = make_pipeline(family_transformer,\n                         KNNImputer(),\n                         'passthrough')","cef343fc":"feature_engineering = make_column_transformer((num_pipe, make_column_selector(dtype_exclude=\"category\")),\n                                              (cat_pipe, make_column_selector(dtype_include=\"category\")))","98954818":"param_grid_fe = {'columntransformer__pipeline-1__knnimputer__n_neighbors': np.arange(2, 11),\n                 'columntransformer__pipeline-1__passthrough': [MinMaxScaler(),\n                                                                StandardScaler(),\n                                                                RobustScaler()]}","bd0e9da6":"lr_feature_engineering = {'LogisticRegression': {'model': LogisticRegression(random_state=seed),\n                                                 'param_grid': param_grid_fe}}","d970cc0c":"feature_engineering","3f12381b":"evaluation(X, y, feature_engineering, models = lr_feature_engineering, random_state = seed)","337082dd":"pca = PCA()\npca_range = np.arange(1, feature_engineering.fit_transform(X).shape[1] + 1) # range until the number of feature after preprocessing\n","0080c93c":"lr_pca = {'LogisticRegression' : {'model': LogisticRegression(random_state = seed),\n                                'param_grid': {'columntransformer__pipeline-1__knnimputer__n_neighbors': [3], # I keep best parameters\n                                               'columntransformer__pipeline-1__passthrough': [StandardScaler()],\n                                               'pca__n_components': pca_range}}}","635486d1":"evaluation(X, y, feature_engineering, models = lr_pca, feature_selection= pca, random_state = seed)","2ab248e8":"param_grid_ms= {'columntransformer__pipeline-1__knnimputer__n_neighbors': [3],\n                'columntransformer__pipeline-1__passthrough': [StandardScaler()]}","cb3c7433":"ensemble_models = {'RandomForestClassifier': {'model': RandomForestClassifier(random_state = seed),\n                                              'param_grid': param_grid_ms},\n                   'ExtraTreesClassifier': {'model': ExtraTreesClassifier(random_state = seed),\n                                            'param_grid': param_grid_ms},\n                   'XGBClassifier': {'model': XGBClassifier(random_state = seed),\n                                     'param_grid': param_grid_ms}}","79286acf":"evaluation(X, y, feature_engineering, models = ensemble_models, feature_selection=pca, random_state = seed)","987dbbbb":"param_grid_xgbc = {'columntransformer__pipeline-1__knnimputer__n_neighbors': [3],\n                   'columntransformer__pipeline-1__passthrough': [StandardScaler()],\n                   'xgbclassifier__n_estimators': [50, 100, 1000],\n                   'xgbclassifier__max_depth': [10, 50, 100],\n                   'xgbclassifier__colsample_bytree':[0.8, 0,9, 1],\n                   'xgbclassifier__learning_rate': [0.1, 0.05],\n                   'xgbclassifier__subsample': [1, 0.8, 0.6]}                   ","74ae8c79":"XGBC = {'XGBClassifier': {'model': XGBClassifier(random_state = seed),\n                          'param_grid': param_grid_xgbc}}","78b92255":"evaluation(X, y, feature_engineering, models = XGBC, random_state = seed)","76b56b0d":"test = pd.read_csv('..\/input\/titanic\/test.csv')","f83329f3":"final_model = XGBClassifier(random_state = 42,\n                            learning_rate = 0.05,\n                            max_depth = 10,\n                            n_estimators = 100,\n                            subsample = 0.6,\n                            colsample_bytree = 0.8)","b5117242":"X_test = test.drop(columns = ['PassengerId', 'Cabin', 'Ticket'])\nX_test = X_test.astype({name:'category' for name in ['Name', 'Embarked', 'Sex']})","78c1611f":"num_pipe_test = make_pipeline(family_transformer, KNNImputer(n_neighbors=3), StandardScaler())","f0121314":"num_pipe_test = make_pipeline(family_transformer, KNNImputer(n_neighbors=3), StandardScaler())\n\nfeature_engineering_test = make_column_transformer((num_pipe, make_column_selector(dtype_exclude=\"category\")),\n                                                   (cat_pipe, make_column_selector(dtype_include=\"category\")))\n\ntest_pipe = make_pipeline(feature_engineering_test, final_model)","28c45e7f":"test_pipe.fit(X, y)\ny_pred = test_pipe.predict(X_test)","4cd529f1":"commit = pd.DataFrame(test['PassengerId'])\ncommit['Survived'] = y_pred","c0968a46":"commit.to_csv('..\/working\/submission.csv', index = False)","fa2cc5f0":"<a id=\"p2_evaluation_function\"><\/a>\n## 2.4. Evaluation function\n\nTo test preprocessing and all next steps I need to build a evaluation function. The function `evaluation` compute accuracy score, plot ROC and learning curve and display the classification report.\n\nIn input, evaluation function take a dictionary as follows: <br>\n`{'model_name': {'model': model_function(), 'param_grid': parameters_grid}`\n\nMore informations about ROC curve: \nhttps:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html#roc-metrics <br>\nMore informations about learning curve:\nhttps:\/\/scikit-learn.org\/stable\/modules\/learning_curve.html#learning-curve","cc43aaf7":"<a id=\"p3_num_make_pipe\"><\/a>\n### 3.2.2. Make numerical pipeline","20f34ed5":"***\n<a id='p5_test'><\/a>\n## \u2699\ufe0f Model selection evaluation","e334a9a0":"<a id=\"p1_viz\"><\/a>\n### 1.3.2. Data Vizualizations\n\nHere again the analysis is basic because it is not the heart of the project, it is about having the most important information: how the variables are distributed in the data and their correlations. \n\nIn a first step I identify categorical variables and numerical variables. For each group, I plot distribution of variables depending on Survived feature. ","02a50205":"<a id=\"p1_title\"><\/a>\n# Part.1 - Data Exploration\n\n[Go back to the table of contents](#contents)\n\nFirst of all, we need to explore data to understand the problem, study features and them correlation with the outcome. ","54d3253e":"<a id=\"p1_data\"><\/a>\n## 1.2. Import data","f8507fc9":"<a id=\"p3_is_alone\"><\/a>\n### Optional: Is Alone feature\n\nMany Kaggle project build a variable Is Alone, a boolean variable indicate if the passenger had, or not, family aboard the Titanic. After tests, this variable doesn't bring any information to my model, I choose not to integrate it. However code below can help you to do so. ","c6c33a75":"***\n<a id='p5_title'><\/a>\n# Part. 5 - Model selection\n\nI test several ensemble models and I keep the best of three, XGboost classifier. ","e285d6e6":"The PCA doesn't improve accuracy. ","8af2faf7":"***\n<a id=\"p3_feature_engineering\"><\/a>\n# Part 3 - Feature engineering\n\nFor feature engineering I proceed in a same way as for preprocessing, I build 2 pipelines: one for categorial variables and one for numerical variables. \n\nIn the way of many other Kaggle projects (see [credits](#credits)), I add Name feature to extract informations about passenger (see below).\n\n<a id=\"p3_cat_pipe\"><\/a>\n## 3.1. Categorial features pipeline\n\n<a id=\"p3_titles\"><\/a>\n### 3.1.1. Titles","dfd12b0c":"***\n<a id='p3_test'><\/a>\n## \u2699\ufe0f Feature engineering evaluation","d04f7fb6":"<a id=\"p3_num_pipe\"><\/a>\n## 3.2. Numerical features pipeline\n\n<a id=\"p3_family\"><\/a>\n### 3.2.1. Aggregate SibSp and Parch\n\nSibSp and Parch are very correlate between them, it seems appropriate to add them together.","2ea43f59":"The `Cabin` (number) variable can be used for approximate the position of passenger on ship when the sinking. However, since there are many null values and `PClass` variable given to us the same information, `Cabin` variable will be exclused from analysis.\n![](https:\/\/rpmarchildon.com\/wp-content\/uploads\/2018\/06\/titanic_class_cabin_locations.png)\n\nFinally, `Survived` is dependent variable. It is a dummy variable with 1 for survived and 0 for did not survive. With its preliminary considerations, we can restrict the data.","d77a84c9":"<a id=\"p2_pipe\"><\/a>\n## 2.2. Preprocessing pipeline\n\nAs a rembember, a Sklearn pipeline concatenates results of multiple transformer objects, like StandardScaler(), SimpleImputer(), etc. There are 4 main tools to build a pipeline with Sklearn:\n\n| Original  | Shorthand | Goal |\n|---|---|---|\n|`Pipeline()`| `make_pipeline()`| Create a pipeline|\n|`ColumnTransformer()`| `make_column_transformer()`| Create a transformer with columns specified|\n|`FunctionTransformer()`| - |Create a transformer with a function specified|\n|`FeatureUnion()`| `make_union()`| Concatenate outputs of pipelines objects|\n\n\nThe shortened form allows to write without specifying a name to the object.<br>\n<ins>Exemple:<\/ins>\n`Pipeline([('scaler', StandardScaler()])` &rightarrow; `make_pipeline(StandardScaler())`\n\nThe preprocessing is very basic: replace missing value and encoding data. There are 2 features with missing values: Embarked and Age. \n\nFor Embarked I decide to use `SimpleImputer()` with most frequent strategy because there are only 2 missing value and the distribution of 3 distinct values (S, C, Q) is very unbalanced in favor of S. \n\nFor Age I choose `KNNImputer()` because I hypothesize that people with similar caracteristics are the same age. We'll see a method to challenge the hypothesize during the process of the feature engineerring optimization thanks to a grid search.\n\nFinaly, I use `make_column_selector()` with specified a dtype (include or exclude) as parameter of a `make_column_transformer()` to match each pipeline with each type of features.  ","b6bd58d5":"This notebook is a basic example of application of Sklearn pipeplines. Pipelines are useful tools to try and test your ideas throughout all of a datascience process to improve your model accuracy.\n\nWe'll see how to:<br>\n> **Define a evaluation function:** create a unique function for all steps that measure the accuracy and plot learning and ROC curves<br>\n<br>\n**Preprocess your data with pipeplines:** encoding, replace missing values and create a pipeline vizualisation<br>\n<br>\n**Do Feature engineering with pipelines:** scaling, create new columns, convert numerical features into categories\nand grid search to optimize feature engineering<br>\n<br>\n**Do feature and model selection with pipelines**<br>\n<br>\n**Do fine tuning of your model with pipelines**<br>\n\n\u26a0\ufe0f Disclaimer: this is my first Kaggle commit, any feedbacks are likeable!\n\n<a id=\"contents\"><\/a>\n# Table of Contents\n\n**1. [Part.1 - Data Exploration](#p1_title)<br>**\n1.1. [The problem](#p1_problem)<br>\n1.2. [Import data](#p1_data)<br>\n1.3. [Explorative Data Analysis](#p1_eda)<br>\n1.3.1. [Get informations](#p1_infos)<br>\n1.3.2. [Data vizualisations](#p1_viz)<br>\n\n**2. [Part. 2 - Data preprocessing](#p2_title)<br>**\n2.1. [Formating data](#p2_formating)<br>\n2.2. [Preprocessing pipeline](#p2_pipe)<br>\n2.3.1. [Pipeline vizualisation](#p2_pipe_viz)<br>\n2.4. [Evaluation function](#p2_evaluation_function)<br>\n\u2699\ufe0f [Preprocessing evaluation](#p2_test)<br>\n\n**3. [Part. 3 - Feature engineering](#p3_title)<br>**\n3.1. [Categorial features pipeline](#p3_cat_pipe)<br>\n3.1.1. [Titles](#p3_titles)<br>\nOptional: [Is Alone feature](#p3_is_alone)<br>\n3.1.2. [Make categorical pipeline](#p3_cat_make_pipe)<br>\n3.2. [Numerical features pipeline](\"p3_num_pipe\")<br>\n3.2.1. [Aggregate SibSp and Parch](#p3_family)<br>\nOptional: [Convert Age and Fare into categories](p3_cut)<br>\n3.2.2. [Make numerical pipeline](#p3_num_make_pipe)<br>\n3.3. [Feature engineering pipeline](#p3_pipe)<br>\n3.3.1. [Feature engineering grid search](#p3_grid)<br>\n\u2699\ufe0f [Feature engineering evaluation](#p3_test)<br>\n\n**4. [Part. 4 - Feature selection](#p4_title)<br>**\n\u2699\ufe0f [Feature selection evaluation](#p4_test)<br>\n\n**5. [Part.5 - Model selection](#p5_title)<br>**\n\u2699\ufe0f [Model selection evaluation](#p5_test)<br>\n\n**6. [Part.6 - Hyperparamters optimization](#p6_title)<br>**\n\u2699\ufe0f [Hyperparamters optimization evaluation](#p6_test)<br>\n\n**[Kaggle submission](#submission)<br>**\n**[Credits](#credits)<br>**\n\n# Packages","01aa6a85":"<a id=\"p2_pipe_viz\"><\/a>\n## 2.3.1. Pipeline vizualisation\n\nThe below code will help you to visualize the data pipeline in HTML format.","f01d7a4b":"***\n<a id=\"p4_title\"><\/a>\n# Part. 4 - Feature selection\n\nTo feature selection, I instantiate a feature selection object. Here it's a PCA but it's can be any others way: variance threshold, SelectKbest, etc.","b9aecbb4":"<a id=\"p1_eda\"><\/a>\n## 1.3. Explorative Data Analysis (EDA)\n\nI do a basic EDA to get the principal information about data because it's not the crux of the project. First, I check the number of observations and features, duplicated values, distincted values, missing values to eliminate useless features to analysis.\n\n<a id=\"p1_infos\"><\/a>\n### 1.3.1. Get informations","7c1caa04":"<a id=\"p1_problem\"><\/a>\n##  1.1. The problem \n\nThe sinking of the Titanic, on April 15, 1912, during her maiden voyage, lead to the death of 1502 out of 2224 passengers and crew. While there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n\nIn this challenge, we are asked to build a predictive model that answers the question: \u201cwhat sorts of people were more likely to survive?\u201d using passenger data (ie name, age, gender, socio-economic class, etc).\n\nThe metric to evaluate your model is the accuracy. Accuracy is the number of correctly predicted data points out of all the data points.","59a34e8d":"In the passenger names we can extract the Title (Mr, Mme, Dr, etc.). I keep only the titles present less than 10 times in the dataset and replace others by 'Misc'. Result, there a 5 possible titles: Mr, Miss, Mrs, Master and Misc.","4589cf85":"***\n<a id='submission'><\/a>\n# Kaggle submission","1b2c9f6d":"<a id='credits'><\/a>\n\n# Credits\n\nhttps:\/\/www.kaggle.com\/ldfreeman3\/a-data-science-framework-to-achieve-99-accuracy\n\nhttps:\/\/towardsdatascience.com\/are-you-using-pipeline-in-scikit-learn-ac4cd85cb27f","27fb19a7":"Moreover, I plot boxplot of numerical variables to check the presence of outliers.","04a0546d":"***\n<a id=\"p2_test\"><\/a>\n## \u2699\ufe0f Preprocessing evaluation\n\nFor now, I start to train a very basic model to evaluate the data preprocessing. ","9695cad7":"<a id=\"p2_title\"><\/a>\n# Part. 2 - Data Preprocessing\n\n[Go back to the table of contents](#contents)\n\nThe preprocessing pipeline is divided in two sub-pipeline: one for categorical features and one for numerical features. First of all, I need to formating data to identify each group of features. To do that, I use `pd.DataFrame.astype()`.\n\n<a id=\"p2_formating\"><\/a>\n## 2.1. Formating Data","9a5d669f":"<a id='p3_grid'><\/a>\n### 3.3.1. Feature_engineering grid search","ae88748d":"<a id='p3_pipe'><\/a>\n## 3.3. Feature engineering pipeline","15989dbf":"***\n<a id='p4_test'><\/a>\n## \u2699\ufe0f Feature selection evaluation","3eb74043":"***\n<a id='p6_test'><\/a>\n## \u2699\ufe0f Hyperparamaters optimization evaluation","85f3395c":"The `PassengerId`, `Name` and `Ticket` variables are assumed to be unique identifiers, that have no impact on the outcome variable. Thus, they will be excluded from analysis.","5422d87e":"If you want to search the best way to scale or the best imputer for missing value, you can make a grid search. To do so, you must write 'passthrough' inplace of the step you want to test (here the scaling of numerical features). ","35d7b4eb":"<a id=\"p3_cut\"><\/a>\n### Optional: convert Age and Fare into categories\n\nAfter tests, this transformation doesn't improve accuracy of my model, I choose not to integrate it. However code below can help you to do so.","7adc5d69":"***\n<a id=\"p6_title\"><\/a>\n# Part. 6 - Hyperparameters optimization","46525846":"<a id=\"p3_cat_make_pipe\"><\/a>\n### 3.1.2. Make categorical pipeline"}}