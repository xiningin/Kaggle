{"cell_type":{"ca8812c5":"code","cad231cb":"code","a16b7897":"code","c6f3c908":"code","8beda20a":"code","1702d3db":"code","94f07c62":"code","f6e9353c":"code","5c6d4379":"code","08d7ca54":"code","90bb844f":"code","f28359d4":"code","4b68d720":"code","9960c410":"code","da86c7ee":"code","d958500b":"code","054946a3":"code","a823db5c":"code","dc6fb7f0":"code","d18902df":"code","c78f7087":"code","466ce182":"code","f46019ae":"markdown","969199fb":"markdown","cbd6b8bd":"markdown","8ef30496":"markdown","41f703b4":"markdown","4dc978e8":"markdown","147a2cd9":"markdown","44f43877":"markdown","0abbdc5d":"markdown","4bb60fdf":"markdown","8e4d46c4":"markdown"},"source":{"ca8812c5":"import numpy as np # linear algebra\nimport pandas as pd # data processing,\nimport warnings    # warning to show or hide\nimport random     # For random pocess\nimport matplotlib.pyplot as plt     # Plotting Images, Graph\nfrom IPython.display import display, Markdown\n\n# Ignoring Wornings\nwarnings.filterwarnings(\"ignore\")\n\n%matplotlib inline","cad231cb":"# Reading Training Data\ntrain_data = pd.read_csv('..\/input\/train.csv')\n# Reading Testing Data\ntest_data = pd.read_csv('..\/input\/test.csv')\nprint(\"Training data informataion\")\nprint(train_data.info())\nprint(\"Training Data Shape: {0}\\nTesting Data Shape: {1}\".format(train_data.shape, test_data.shape))","a16b7897":"# Training Data Columns\ntrain_data.columns","c6f3c908":"# Get Featrues and Labels\nfeatrue = train_data.drop(['label'], axis=1)\nlabel = np.asarray(train_data[['label']])\nprint(\"Unique Digit Labels: {}\".format(np.unique(label)))","8beda20a":"# Creating custom subplot function to visualize the image\ndef plotImage(image_data, label, number_of_plots=3):\n    \n    # Generate Random number for position\n    pos_of_image = [random.randint(0, image_data.shape[0]) for p in range(0, number_of_plots*number_of_plots)]\n    \n    # Defining figure\n    fig = plt.figure(figsize=(10, 10))\n    \n    # plotting subplots\n    for pos, plotnumber in zip(pos_of_image, range(1, (number_of_plots*number_of_plots)+1)):\n        ax = fig.add_subplot(number_of_plots, number_of_plots, plotnumber)\n        \n        # pLotting image\n        ax.imshow(image_data[pos], cmap='gray')\n        ax.set_title(\"Digit-{}\".format(label[pos][0]), fontsize=14)\n        fig.tight_layout()\n\n# Converting the 1-D digit array to 2-D array.\n# Using reshape function 784 >> 28,28\nimage_array = np.reshape(np.asarray(featrue), (len(featrue), 28, 28))\n\n# Calling custom plot funcation\ndisplay(Markdown(\"### MNIST Handwritten Digits\"))\nplotImage(image_array, label, number_of_plots=5)","1702d3db":"from sklearn.preprocessing import Normalizer\n\n# Scaling Data with Normalizer\ndef norm(input_data):\n    nm = Normalizer()\n    nm.fit(input_data)\n    input_data_scale = nm.transform(featrue)\n    return input_data_scale, nm\n\nfeatrue_scale, nm = norm(featrue)\n","94f07c62":"from sklearn.decomposition import PCA\n\ncomponent_with_var = {}\n# We will use variacne range instead  of number features. Looping 784 features is slower than 10 varince range.\nvariance_list = np.arange(0.1, 1.1, 0.1)\n\nfor var in variance_list:\n    if var < 1.0:\n        # Selecting 2nd shape value which is nothing but number of components.\n        component_with_var[str(var*100)+' %'] = PCA(var).fit_transform(featrue_scale).shape[1]\n    else:\n        component_with_var[str(var*100)+' %'] = featrue_scale.shape[1]\ncomponent_with_var","f6e9353c":"variance_cover = 0.9    # 90% Of varince cover\n# Get component list\ncomponent_list = list(component_with_var.values())\n# using numpy interp function we will get number of features(components) for the 90% varince coverage.\ncomponents = int(np.interp(variance_cover, variance_list, component_list))\nprint(\"Optimum number of feature or component is {0}\".format(components))","5c6d4379":"plt.plot(variance_list, component_list)\nplt.scatter(variance_cover, components, color='red')\nplt.annotate('{}% variance covered by \\n{} features'.format(variance_cover*100,components), \n             xy=(variance_cover, components), xytext=(0.3,400),\n             arrowprops=dict(facecolor='black', shrink=0.05),\n             fontsize=14\n            )\nplt.show()","08d7ca54":"# Lets reduce the dimension of given dataset\ndef pca_transform(input_data, components):\n    pca = PCA(n_components=components)\n    pca.fit(input_data)\n    input_data_pca = pca.transform(input_data)\n    return input_data_pca, pca\nfeature_pca, pca_model = pca_transform(featrue_scale, components)","90bb844f":"from sklearn.model_selection import train_test_split\n# Train data 80%, Test data 20%\nX_train, X_test, Y_train, Y_test = train_test_split(feature_pca, label, test_size=0.2)","f28359d4":"# Calling Model\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV","4b68d720":"# Taking effective params \nrfr_grid = {\n    \"n_estimators\":[100, 200],\n    \"max_depth\":[2, 5, 10, None]\n}","9960c410":"%%time\n# Fitting model\nclf = GridSearchCV(RandomForestClassifier(),param_grid=rfr_grid,verbose=5)\nclf.fit(X_train, Y_train)","da86c7ee":"# Best params are\nclf.best_params_","d958500b":"%%time\n# Fitting with best params\nrfr = RandomForestClassifier(\n    n_estimators=200\n)\nrfr.fit(X_train,Y_train)","054946a3":"# Train score\nrfr.score(X_train,Y_train)","a823db5c":"# ValidationTest Score\nrfr.score(X_test, Y_test)","dc6fb7f0":"# Test data\ntest_data.head(3)","d18902df":"# Preprocessing Test data\ntest_scale = nm.transform(test_data)\ntest_pca = pca_model.transform(test_scale)","c78f7087":"# Predicted Digit\npredicted_digit = rfr.predict(test_pca)","466ce182":"# Submission\nsub =  pd.DataFrame(range(1,len(test_data)+1),columns=['ImageId'])\nsub['Label'] = predicted_digit\nsub.to_csv('pca_rfr_digit.csv', index=False)","f46019ae":"# Train-Test-Split\nLets split the training data into train-test data for cross validations","969199fb":"Validation Testing scores 95% ","cbd6b8bd":"# Submission","8ef30496":"## Dimensionality Reduction: Principal Component Analysis\n- The dimensionality reduction is another most important part of data preprocessing.\n- Lets say we have data of vehicles. In which type of vehicle (2-wheeler,4-wheeler), number of tyres (2 tyre,4 tyre) which are co-related to a high degree. Hence, we can reduce the number of features in such problems.\n- In this case we have 784 features. Whcih represnts the pixels values of digit. These feature are correalted so we can readuce the number of features.\n- Approch is done using Principal Component Analysis. \n- Lets calculate optimum number of feature is which covers 90% of PCA variance.","41f703b4":"# Loading Data","4dc978e8":"From above calculation we found that up to 229 we can reduce the features. Which covers 90% of variance.","147a2cd9":"# MNIST Dataset with PCA+RandomForest\n- Loading Data.\n- Visulization of Images.\n- Preprocessing.\n    - Scaling the data.\n    - Principal Component Analysis.\n- Chosing Number Of Components.\n- RandomForest classifier\n    - Tuning Model\n- Submission","44f43877":"# Preprocessing\n## Scaling the data\n- Scaling is most important when it comes to data preprocssing. \n- The independent feature which ranges in 0-1000 values makes the optimization slower i.e. training gets slower. \n- In our case the digit pixel values ranges from 0-255. Which will make our classifier system slower. \n- So to imporve the training we normalize the data using Normalizer which will convert the data in range of -1 to 1 or 0 to 1.","0abbdc5d":"Separating features and labels from traning data.","4bb60fdf":"# Visulization of Images\nLets have visulization of MNIST handwritten digit dataset.","8e4d46c4":"# RandomForsest Classifier"}}