{"cell_type":{"312909e1":"code","d199cf98":"code","a2e40260":"code","14d5c48c":"code","ce73b69c":"code","a6055cf1":"code","c2708903":"code","9863392d":"code","7f7f0a9a":"code","1a2840db":"code","d2b75b27":"code","788cd874":"code","4e8562e6":"code","06c9d0b6":"code","4c137ab7":"code","3c57720b":"code","af94136b":"code","c6ecdf95":"code","11125743":"code","cd90e680":"code","e5c70737":"code","9aab5fe4":"code","15e2d648":"code","3e685753":"code","bfce3e66":"code","9cf24800":"markdown","b4ea3677":"markdown","418042aa":"markdown","b9d49be1":"markdown","3ea537a6":"markdown"},"source":{"312909e1":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelBinarizer, MultiLabelBinarizer, MinMaxScaler\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.neighbors import NearestNeighbors\nfrom scipy.sparse import csr_matrix\nfrom sklearn.decomposition import PCA\n\npd.set_option(\"max_colwidth\", None)","d199cf98":"usecols = [\"MAL_ID\", \"Name\", \"Score\", \"Genders\", \"Type\", \"Episodes\", \"Premiered\",\n           \"Studios\", \"Source\", \"Rating\", \"Members\"]\n\nanime_data=pd.read_csv('..\/input\/anime-recommendation-database-2020\/anime.csv',usecols=usecols)\n\nprint(\"anime_data.shape:\", anime_data.shape)\n\nanime_data.head()","a2e40260":"def process_multilabel(series):\n    series = series.split(\",\")\n    if \"Unknown\" in series:\n        series.remove(\"Unknown\")\n    return series\n\nanime_data[\"Genders\"] = anime_data[\"Genders\"].map(process_multilabel)\nanime_data[\"Studios\"] = anime_data[\"Studios\"].map(process_multilabel)\nanime_data[\"Score\"] = anime_data[\"Score\"].replace(\"Unknown\", 0).astype(float)\nanime_data[\"Episodes\"] = anime_data[\"Episodes\"].replace(\"Unknown\", 0).astype(int)\n\nanime_data.head()","14d5c48c":"def preprocessing_category(df, column, is_multilabel=False):\n    # Binarise labels\n    lb = LabelBinarizer()\n    if is_multilabel:\n        lb = MultiLabelBinarizer()\n        \n    expandedLabelData = lb.fit_transform(df[column])\n    labelClasses = lb.classes_\n\n    # Create a pandas.DataFrame from our output\n    category_df = pd.DataFrame(expandedLabelData, columns=labelClasses)\n    del df[column]\n    return pd.concat([df, category_df], axis=1)\n\nanime_metadata = anime_data.copy()\nanime_metadata = preprocessing_category(anime_metadata, \"Type\")\nanime_metadata = preprocessing_category(anime_metadata, \"Premiered\")\nanime_metadata = preprocessing_category(anime_metadata, \"Studios\", is_multilabel=True)\nanime_metadata = preprocessing_category(anime_metadata, \"Source\")\nanime_metadata = preprocessing_category(anime_metadata, \"Rating\")\n\nGenders = anime_metadata[\"Genders\"]\nID_NAME = anime_metadata[[\"MAL_ID\", \"Name\"]]\n\ndel anime_metadata[\"Genders\"]\ndel anime_metadata[\"MAL_ID\"]\ndel anime_metadata[\"Name\"]\ndel anime_metadata[\"Unknown\"]\n\nanime_metadata[[\"Score\", \"Episodes\", \"Members\"]] = MinMaxScaler().fit_transform(anime_metadata[[\"Score\", \"Episodes\", \"Members\"]])\nanime_metadata = anime_metadata.values","ce73b69c":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ntfv = TfidfVectorizer(min_df=3,  max_features=None, \n            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n            ngram_range=(1, 3),\n            stop_words = 'english')\n\n# Filling NaNs with empty string\ngenres_original = anime_data['Genders'].fillna('').astype(str)\ngenres_vector_tf_idf = tfv.fit_transform(genres_original)\n\ngenres_vector_one_hot = preprocessing_category(pd.DataFrame(Genders), \"Genders\", True).values","a6055cf1":"print(\"anime_metadata.shape:\", anime_metadata.shape)\nprint(\"genres_vector_tf_idf.shape:\", genres_vector_tf_idf.shape)\nprint(\"genres_vector_one_hot.shape:\", genres_vector_one_hot.shape)","c2708903":"def get_recommended(vector, query_index, n_neighbors=10):\n    model_knn = NearestNeighbors(metric='cosine', n_neighbors=n_neighbors)\n    model_knn.fit(csr_matrix(vector))\n\n    distances, indices = model_knn.kneighbors(vector[query_index,:].reshape(1, -1), n_neighbors = n_neighbors)\n    result = []\n    for i in range(0, len(distances.flatten())):\n        index = indices.flatten()[i]\n        if index == query_index:\n            continue\n        result.append(anime_data.iloc[index])\n        \n    return pd.DataFrame(result)","9863392d":"# query_index = np.random.choice(anime_metadata.shape[0])\nquery_index = ID_NAME[ID_NAME.MAL_ID == 32281].index[0]\nanime_data.iloc[[query_index]]","7f7f0a9a":"get_recommended(anime_metadata, query_index, 10)","1a2840db":"get_recommended(genres_vector_tf_idf, query_index, 10)","d2b75b27":"get_recommended(genres_vector_one_hot, query_index, 10)","788cd874":"all_data = np.concatenate((anime_metadata, genres_vector_tf_idf.todense(), genres_vector_one_hot), axis=1)\nall_data.shape","4e8562e6":"get_recommended(all_data, query_index, 10)","06c9d0b6":"%%time\n\nreduced_all_data = PCA(n_components=250).fit_transform(all_data)\nget_recommended(reduced_all_data, query_index, 10)","4c137ab7":"usecols = [\"MAL_ID\", \"Name\", \"Genders\", \"sypnopsis\"]\nanime_data_2 = pd.read_csv('..\/input\/anime-recommendation-database-2020\/anime_with_synopsis.csv', usecols=usecols)\nanime_data_2.head()","3c57720b":"query_index_2 = anime_data_2[anime_data_2.MAL_ID == 32281].index[0]\nanime_data_2.iloc[[query_index_2]]","af94136b":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ntfv = TfidfVectorizer(min_df=3,  max_features=None, \n            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n            ngram_range=(1, 3),\n            stop_words = 'english')\n\n# Filling NaNs with empty string\n\nsynopsis_original = anime_data_2['sypnopsis'].fillna('').astype(str)\nsynopsis_vector_tf_idf = tfv.fit_transform(synopsis_original)\nsynopsis_vector_tf_idf.shape","c6ecdf95":"def get_recommended_another_df(vector, query_index, n_neighbors=10):\n    model_knn = NearestNeighbors(metric='cosine', n_neighbors=n_neighbors)\n    model_knn.fit(csr_matrix(vector))\n\n    distances, indices = model_knn.kneighbors(vector[query_index,:].reshape(1, -1), n_neighbors = n_neighbors)\n    result = []\n    for i in range(0, len(distances.flatten())):\n        index = indices.flatten()[i]\n        if index == query_index:\n            continue\n        result.append(anime_data_2.iloc[index])\n        \n    return pd.DataFrame(result)","11125743":"get_recommended_another_df(synopsis_vector_tf_idf, query_index_2, 10)","cd90e680":"%%time\n\nreduced_all_data = PCA(n_components=250).fit_transform(synopsis_vector_tf_idf.todense())\nget_recommended_another_df(reduced_all_data, query_index_2, 10)","e5c70737":"rating_data=pd.read_csv('..\/input\/anime-recommendation-database-2020\/rating_complete.csv')\n\nprint (\"rating_data.shape:\", rating_data.shape)\nprint (rating_data.info())\nrating_data.head()","9aab5fe4":"users_count = rating_data.groupby(\"user_id\").size().reset_index()\nusers_count.columns = [\"user_id\", \"anime_count\"]\n\nprint(users_count.shape)\n\nfiltered_users = users_count[users_count.anime_count >= 300]\nusers = set(filtered_users.user_id)\n\nprint(len(users))","15e2d648":"rating_data = rating_data[rating_data.user_id.isin(users)]\nprint (\"rating_data.shape:\", rating_data.shape)\nprint (rating_data.info())","3e685753":"unique_users = {int(x): i for i,x in enumerate(rating_data.user_id.unique())}\nunique_items = {int(x): i for i,x in enumerate(anime_data.MAL_ID.unique())}\n\nprint(len(unique_items), len(unique_users))\nanime_collabolative_filter = np.zeros((len(unique_items), len(unique_users)))\n\nfor user_id, anime_id, rating in rating_data.values:\n    anime_collabolative_filter[unique_items[anime_id], unique_users[user_id]] = rating","bfce3e66":"get_recommended(anime_collabolative_filter, query_index, 10)","9cf24800":"## Collaborative Filtering","b4ea3677":"## Recommend with KNN","418042aa":"### Use Synopsis and TF-IDF","b9d49be1":"# Anime Recommended System - Content Based & Collaborative Filtering\n\nThis notebook use a classic method (KNN) to recommend anime using 2 different approach: \n\n- Content Based\n- Collaborative Filtering\n\nIn Content bases, I explore 7 ways to recommend a list of anime based in 1 given anime:\n\n1. Only Metadata\n2. Using one hot encoding to embedding the genre.\n3. Using TF-IDF to embedding the genre\n4. Concatenate opction 1, 2 and 3.\n5. Apply PCA to generate reduced vector of option 4.\n6. Using TF-IDF to embedding the synopsis\n7. Apply PCA to generate reduced vector of option 6.\n\n\nI was based on the following work https:\/\/www.kaggle.com\/benroshan\/content-collaborative-anime-recommendation","3ea537a6":"## Content Based"}}