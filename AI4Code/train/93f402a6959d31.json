{"cell_type":{"84ab06d2":"code","72f09106":"code","2cf16080":"code","e0290c0e":"code","5a194eb9":"code","71697218":"code","5df0919f":"code","bdcc0d9b":"code","7a8a7f3c":"code","81594301":"code","3b140d01":"code","0ddc313d":"code","a580a493":"code","53b62721":"code","3c27ffcb":"code","70e8ad90":"code","8c3cc139":"code","1b9136c8":"code","b9db3a7a":"code","a2e9d3e0":"code","b883e583":"code","789eebee":"code","26e5b1a0":"code","6ee3cee0":"code","9cb09b20":"code","39678eef":"code","5ab061de":"code","3e4e77f1":"code","a4143594":"code","8a0e215c":"code","7a25afd7":"code","3f82ab78":"code","a488a140":"code","3b74521d":"code","1de758d7":"markdown","52e884be":"markdown","0464fa71":"markdown","1563564e":"markdown","0d4c84e4":"markdown","64f55bb0":"markdown","21426f12":"markdown","b40d33ba":"markdown","e253111e":"markdown","01d0bee0":"markdown","fe5d1a7e":"markdown","e11c48a8":"markdown","03171f4b":"markdown","b76ed72e":"markdown","c9153721":"markdown","665a524c":"markdown","87645046":"markdown","a5c13135":"markdown","36ea330e":"markdown","d81cbe4a":"markdown"},"source":{"84ab06d2":"import numpy as np\nimport pandas as pd\nimport os\nimport json\nimport re\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import ConnectionPatch\nimport plotly.express as px\nimport plotly.figure_factory as ff\nimport pycountry\nimport copy\nimport seaborn as sns\nfrom pandas_profiling import ProfileReport\nfrom collections import Counter\nfrom matplotlib import style\nfrom tqdm import tqdm","72f09106":"def get_target_dict(targets, text, paper_id, original_word_dist, word_distribution_in_paper):\n    \"\"\"\"\n    Summary:This function finds the words in the targets list in the text variable and returns \n    their count in a dictionary \n    \n    Parameters:\n    targets: The list of words which we are going to search for in a given text \n    text: the text in which we are searching for the words in targets list\n    paper_id: The id of the research papers in which we are searching. This field is considered as the key\n    original_word_dist: A dictionary which contains the count of the words in targets list as found in \n    the text variable\n    word_distribution_in_paper: A dictionary which contains the count of all words in the targets list.\n    A count of 0 is added to the ones not found in the text variable.Useful for plotting stacked bar graph\n    \"\"\"\n    word_count_new = {}\n    word_count = {}\n    for sentence in text.split('.'):\n        for word in targets:\n            if word in sentence:\n                if word in word_count: \n                    word_count[word] += sentence.count(word)\n                else:\n                    word_count[word] = sentence.count(word)\n        word_count_new = copy.deepcopy(word_count)\n        if bool(word_count_new):\n            for word in targets:\n                if word not in word_count_new:\n                    word_count_new[word] = 0\n    word_distribution_in_paper[paper_id] = word_count_new\n    original_word_dist[paper_id] = word_count #dictionary without the 0 appends\n    return original_word_dist, word_distribution_in_paper","2cf16080":"def get_word_count(targets, dataf, col1_text, col2_key):\n    \"\"\"\n    Summary: This function creates a dataframe containing sentences which has \n    any or all of the words from the targets list. This dataframe is then passed into the function\n    get_target_dict to get the count of the words in the targets list as found in the sentences.\n    \n    Parameters:\n    targets:The list of words which we are going to search for in a given text\n    dataf:Dataframe containing the original full text from which we can get the sentences \n    containing words in the targets list\n    col1_text:The field of the dataframe dataf in which we will search for the words\n    col2_key:The key which we will use to identify a paper uniquely(paper_id in our example)\n    \"\"\"\n    df_targets = dataf[dataf[col1_text].apply(lambda sentence: any(word in sentence for word in targets))] \n        \n    original_word_dist = {}\n    word_distribution_in_paper = {}\n    for index, row in df_targets.iterrows():\n        original_word_dist, word_distribution_in_paper = get_target_dict(targets, row[col1_text], row[col2_key], original_word_dist, word_distribution_in_paper)\n\n    return original_word_dist, word_distribution_in_paper","e0290c0e":"#max word count should be more than a threshold value(denoted by the varibale 'limit')\ndef get_word_distribution(word_dictionary,limit):\n    \"\"\"\n    Summary:This function accepts a word dictionary containing words and their respective counts and \n    only keeps words whose count is more than a certain limit.\n    Parameters:\n    word_dictionary:Dictionary containing words and their count\n    limit:The threshold value. If the count of a word is more than this threshold value it is \n    kept in the dictionary\n    \"\"\"\n    temp_dictionary={}\n    for paperid, lists in word_dictionary.items():\n        keep=False\n        for word, wordcount in lists.items():\n            if wordcount > limit:\n                keep=True\n                break\n        if keep:\n            temp_dictionary[paperid]=lists    \n    word_dictionary = temp_dictionary\n    return word_dictionary","5a194eb9":"def draw_plot(paper_word_distribution):\n    \"\"\"\n    This fuction accepts a dictionary and uses it to draw a stacked bar graph of \n    each research paper containing the distribution of the target list of words found\n    \"\"\"\n    labels = paper_word_distribution.keys()\n    word_count_list={}\n    count=0\n    for eachvalue in paper_word_distribution.values():\n        for key,value in eachvalue.items():\n            if key in word_count_list:\n                word_count_list[key].append(value)\n            else:\n                word_count_list[key]=[value]\n    width = 0.35\n    fig = plt.figure(figsize=(20,8))\n    ax = fig.add_subplot(111)\n\n    for key,value in word_count_list.items():\n        ax.bar(labels, value, width, label=key)\n\n    ax.set_ylabel('Word Distribution')\n    ax.set_title('Paper wise distribution of keywords')\n    ax.legend()\n    plt.show()","71697218":"def draw_plot_horizontal(paper_word_distribution):\n    \"\"\"\n    This fuction accepts a dictionary and uses it to draw a horizontal stacked bar graph of \n    each paper containing the distribution of the target list of words found\n    \"\"\"\n\n    matplotlib.rcParams.update({'font.size': 16})\n\n    labels = paper_word_distribution.keys()\n    y_pos = np.arange(len(labels))\n    word_count_list={}\n    count=0\n    for eachvalue in paper_word_distribution.values():\n        for key,value in eachvalue.items():\n            if key in word_count_list:\n                word_count_list[key].append(value)\n            else:\n                word_count_list[key]=[value]\n    fig = plt.figure(figsize=(18,9))\n    ax = fig.add_subplot(111)\n\n    for key,value in word_count_list.items():\n        h = ax.barh(y_pos, value, align='center',label=key)\n\n    ax.set_yticks(y_pos)\n    ax.set_yticklabels(labels)\n    ax.invert_yaxis()  # labels read top-to-bottom\n    ax.set_xlabel('Word Distribution')\n    ax.set_title('Paper wise distribution of keywords')\n    ax.legend()\n    plt.show()","5df0919f":"def plot_clustered_stacked(dfall, labels=None, title=\"Comparing occurence of target words between abstract and full text in COVID19 papers\",  H=\"\/\", **kwargs):\n    n_df = len(dfall)\n    n_col = len(dfall[0].columns) \n    n_ind = len(dfall[0].index)\n    axe = plt.subplot(111)\n\n    for df in dfall : \n        axe = df.plot(kind=\"bar\",\n                      linewidth=2,\n                      stacked=True,\n                      ax=axe,\n                      legend=False,\n                      grid=False,\n                      **kwargs,\n                      figsize=(20,5)) \n\n    h,l = axe.get_legend_handles_labels()\n    for i in range(0, n_df * n_col, n_col): \n        for j, pa in enumerate(h[i:i+n_col]):\n            for rect in pa.patches: \n                rect.set_x(rect.get_x() + 1 \/ float(n_df + 1) * i \/ float(n_col))\n                rect.set_hatch(H * int(i \/ n_col))      \n                rect.set_width(1 \/ float(n_df + 1))\n\n    axe.set_xticks((np.arange(0, 2 * n_ind, 2) + 1 \/ float(n_df + 1)) \/ 2.)\n    axe.set_xticklabels(df.index, rotation = 0)\n    axe.set_title(title)\n\n    #Add invisible data to add another legend\n    n=[]        \n    for i in range(n_df):\n        n.append(axe.bar(0, 0, color=\"gray\", hatch=H * i))\n\n    l1 = axe.legend(h[:n_col], l[:n_col], loc=[1.01, 0.5])\n    if labels is not None:\n        l2 = plt.legend(n, labels, loc=[1.01, 0.1]) \n    axe.add_artist(l1)\n    return axe","bdcc0d9b":"style.use(\"ggplot\")\ndirs=[\"pmc_json\",\"pdf_json\"]\ndocs=[]\ncounts=0\nfor d in dirs:\n    print(d)\n    counts = 0\n    for file in tqdm(os.listdir(f\"..\/input\/CORD-19-research-challenge\/document_parses\/{d}\")):#What is an f string?\n        file_path = f\"..\/input\/CORD-19-research-challenge\/document_parses\/{d}\/{file}\"\n        j = json.load(open(file_path,\"rb\"))\n        #Taking last 7 characters. it removes the 'PMC' appended to the beginning\n        #also paperid in pdf_json are guids and hard to plot in the graphs hence the substring\n        paper_id = j['paper_id']\n        paper_id = paper_id[-7:]\n        title = j['metadata']['title']\n\n        try:#sometimes there are no abstracts\n            abstract = j['abstract'][0]['text']\n        except:\n            abstract = \"\"\n            \n        full_text = \"\"\n        bib_entries = []\n        for text in j['body_text']:\n            full_text += text['text']\n            for csp in text['cite_spans']:\n                try:\n                    title = j['bib_entries'][csp['ref_id']]['title']\n                    bib_entries.append(title)\n                except:\n                    pass\n                \n        docs.append([paper_id, title, abstract, full_text, bib_entries])\n        #comment this below block if you want to consider all files\n        #comment block start\n        counts = counts + 1\n        if(counts >= 10000):\n            break\n        #comment block end    \ndf=pd.DataFrame(docs,columns=['paper_id','title','abstract','full_text','bib_entries']) ","7a8a7f3c":"profile = ProfileReport(df, title='Pandas Profiling Report',html={'style':{'full_width': True}}, progress_bar=False )\nprofile.to_widgets()","81594301":"incubation=df[df['full_text'].str.contains('incubation')]    \ntexts=incubation['full_text'].values\nincubation_times = []\nfor t in texts:\n    for sentence in t.split('. '):\n        if \"incubation\" in sentence:\n            num_day = 0.0\n            num_week = 0.0\n            single_day=re.findall(r\" \\d{1,2} day\",sentence)\n            single_week=re.findall(r\" \\d{1,2} week\",sentence)\n            if len(single_day) == 1 : #picked up one string\n                num_day = float(single_day[0].split(\" \")[1]) # 6 days; only extracting the no.\n            if len(single_week) == 1 :\n                num_week = float(single_week[0].split(\" \")[1])\n            if num_day or num_week:\n                incubation_times.append([sentence, num_day, num_week])    ","3b140d01":"#Renaming the columns in incubation_df.\nincubation_df = pd.DataFrame(incubation_times,columns=['sentence','days','weeks']) \ndisplay(incubation_df.loc[incubation_df['days'] != 0.0].head(5))\ndisplay(incubation_df.loc[incubation_df['weeks'] != 0.0].head(5))\nprint(f\"The mean projected incubation time in days is \", incubation_df['days'].mean(),\" days\")\nprint(f\"The mean projected incubation time in weeks is \", incubation_df['weeks'].mean(),\" weeks\")\n#Datatypes of the various columns in the incubation_df dataframe\nincubation_df.dtypes","0ddc313d":"#Displaying days and their count in the dataset\ndays_df = pd.DataFrame(incubation_df['days'].value_counts()).reset_index()\ndays_df.columns = ['Days', 'count']\ndays_df = days_df.sort_values(by='count', ascending=False)#you can also sort by 'Days'\ndisplay(days_df.head(20))#Just wanted to see the ones with double digit values","a580a493":"result_df = days_df.groupby(pd.cut(days_df[\"Days\"], np.arange(0, 90, 7))).sum()\ndisplay(result_df)\n#Groups and their counts\n#0-7 : 401\n#8-14 : 239\n#15-21 : 65\n#Over 21 :We sum up the values more than 21\ncount_21 = days_df.loc[days_df['Days'] > 21, 'count'].sum()\n#Total count of all days mentioned in the dataframe\ntotal = result_df['count'].sum()","53b62721":"font = {'family' : 'normal',\n        'weight' : 'bold',\n        'size'   : 16}\n\nmatplotlib.rc('font', **font)\n\n\n\nfig = plt.figure(figsize=(20, 9))\nax1 = fig.add_subplot(121)\nax2 = fig.add_subplot(122)\nfig.subplots_adjust(wspace=0)\n\n\n#Finding no. of incubation period which is only in days\nonly_non_zero_days = len(incubation_df[(incubation_df['days'] > 0) & (incubation_df['weeks'] == 0)])\n\n#Finding no. of incubation period which is only in weeks\nonly_non_zero_weeks = len(incubation_df[(incubation_df['days'] == 0) & (incubation_df['weeks'] > 0)])\n\n#Finding no. of incubation period which ranges from days to weeks\nboth_days_weeks = len(incubation_df[(incubation_df['days'] > 0) & (incubation_df['weeks'] > 0)])\n\n#Total no. of incubation periods either in days or weeks or both\ntotal_rows = len(incubation_df[(incubation_df['days'] > 0) | (incubation_df['weeks'] > 0)])\n\n#Pie chart parameters\nratios = [only_non_zero_days\/total_rows, only_non_zero_weeks\/total_rows, both_days_weeks\/total_rows]\nlabels = ['Incubation period in days', 'Incubation period in weeks', 'Incubation period ranging from days to weeks']\nexplode = [0.3, 0, 0]\n\n#Rotate so that first wedge is split by the x-axis\nangle = -180 * ratios[0]\ncmap = plt.get_cmap(\"tab20c\")\nouter_colors = cmap(np.array([9, 5, 1]))\npatches, texts, autotexts = ax1.pie(ratios, autopct='%1.2f%%', startangle=angle,colors=outer_colors,shadow=True,\n        labels=labels, explode=explode)\nfor text in texts:\n    text.set_fontsize(15)\nfor text in autotexts:\n    text.set_fontsize(14)\n\n#Bar chart parameters\nxpos = 0\nbottom = 0\n\n#0-7 days, 8-14 days,15-21 days, Over 21 days(4 categories)\nratios = [401\/total, 239\/total, 65\/total, count_21\/total]\nwidth = .2\ncolors = [[.1, .3, .3], [.3, .5, .5], [.5, .7, .7], [.7, .9, .9]]#colors based on values\n\nfor j in range(len(ratios)):\n    height = ratios[j]\n    ax2.bar(xpos, height, width, bottom=bottom, color=colors[j])\n    ypos = bottom + ax2.patches[j].get_height() \/ 2\n    bottom += height\n    ax2.text(xpos, ypos, \"%d%%\" % (ax2.patches[j].get_height() * 100),\n             ha='center')\n\nax2.set_title('Distribution of Incubation Period(In Days)')\nax2.legend(('0-7 days', '8-14 days', '15-21 days', 'Over 21 days'))\nax2.axis('off')\nax2.set_xlim(- 2.5 * width, 2.5 * width)\n\n#Use ConnectionPatch to draw lines between the two plots\n#Get the wedge data\ntheta1, theta2 = ax1.patches[0].theta1, ax1.patches[0].theta2\ncenter, r = ax1.patches[0].center, ax1.patches[0].r\nbar_height = sum([item.get_height() for item in ax2.patches])\n\n#Draw top connecting line\nx = r * np.cos(np.pi \/ 180 * theta2) + center[0]\ny = r * np.sin(np.pi \/ 180 * theta2) + center[1]\ncon = ConnectionPatch(xyA=(-width \/ 2, bar_height), coordsA=ax2.transData,\n                      xyB=(x, y), coordsB=ax1.transData)\ncon.set_color([0, 0, 0])\ncon.set_linewidth(4)\nax2.add_artist(con)\n\n#Draw bottom connecting line\nx = r * np.cos(np.pi \/ 180 * theta1) + center[0]\ny = r * np.sin(np.pi \/ 180 * theta1) + center[1]\ncon = ConnectionPatch(xyA=(-width \/ 2, 0), coordsA=ax2.transData,\n                      xyB=(x, y), coordsB=ax1.transData)\ncon.set_color([0, 0, 0])\nax2.add_artist(con)\ncon.set_linewidth(4)\n\nplt.show()","3c27ffcb":"df_abstract = df.loc[df['abstract'] != '']\nwith pd.option_context('display.max_colwidth', -1):\n    display(df_abstract[['paper_id','title','abstract','bib_entries']].head(1))","70e8ad90":"targets = ['vaccine','vaccination','vaccines','vaccinations'] \nlimit=10\na,b=get_word_count(targets,df,'abstract','paper_id')\npaper_word_distribution=get_word_distribution(b,limit)\ndraw_plot(paper_word_distribution)","8c3cc139":"targets = ['vaccine','vaccination','vaccines','vaccinations'] ","1b9136c8":"def get_wordcount_in_dataframe(df1, df2):\n    \"\"\"\n    Getting index from df1 and finding wordcount with same index in df2.\n    \"\"\"\n    l=[]\n    for index, row in df1.iterrows():\n        try:\n            l.append(df2.loc[index,:])\n        except:\n            data = {'vaccine':0,'vaccination':0,'vaccines':0,'vaccinations':0}\n            s = pd.Series(data,dtype='int64',name=index)\n            l.append(s)\n    df3 = pd.DataFrame(l)\n    return df3","b9db3a7a":"DF_list = list()\n\nlimit = 1\na,b = get_word_count(targets,df_abstract,'abstract','paper_id')\npaper_word_distribution1 = get_word_distribution(b,limit)\ndf1 = pd.DataFrame.from_dict(paper_word_distribution1, orient='index')\n\n#vaccine had the highest count among the target words hence sorted in descending order by vaccine\ndf1 = df1.sort_values(by='vaccine', ascending=False)\ndisplay('Table 1 :Count of target words in abstracts')\ndisplay(df1[0:10])\n#considering only first 5 values of this list for better plotting\nDF_list.append(df1[0:10])\n\nc,d = get_word_count(targets,df_abstract,'full_text','paper_id')\npaper_word_distribution2 = get_word_distribution(d,limit)\ndf2 = pd.DataFrame.from_dict(paper_word_distribution2, orient='index')\ndf2 = df2.sort_values(by='vaccine', ascending=False)\ndisplay('Table 2: Count of target words in full_text')\ndisplay(df2[0:10])\n\ndisplay('Table 3: Count of target words in full text for papers in Table 1')\ndf3 = get_wordcount_in_dataframe(df1[0:10], df2)\ndisplay(df3)\nDF_list.append(df3)\n\nplot_clustered_stacked(DF_list, cmap=plt.cm.Set2)","a2e9d3e0":"DF_list2 = list()\ntargets = ['vaccine','vaccination','vaccines','vaccinations'] \ndisplay('Table 1 :Count of target words in abstracts')\ndisplay(df1[0:10])\ndisplay('Table 2: Count of target words in full_text')\ndisplay(df2[0:10])\nDF_list2.append(df2[0:10])\ndf3 = get_wordcount_in_dataframe(df2[0:10], df1)\ndisplay('Table 3: Count of target words in full text for papers in Table 1')\ndisplay(df3)\nDF_list2.append(df3)\n\nplot_clustered_stacked(DF_list2, cmap=plt.cm.Set2)","b883e583":"targets = ['vaccine','vaccination','vaccines','vaccinations'] \nlimit=200\na,b=get_word_count(targets,df,'full_text','paper_id')\npaper_word_distribution=get_word_distribution(b,limit)","789eebee":"draw_plot(paper_word_distribution)","26e5b1a0":"targets = ['anti-viral','anti viral','genome','genome data','genome','strain'] \nlimit=200\na,b=get_word_count(targets,df,'full_text','paper_id')\npaper_word_distribution=get_word_distribution(b,limit)","6ee3cee0":"draw_plot_horizontal(paper_word_distribution)","9cb09b20":"targets=['livestock','reservoir','farmer','wildlife','host range','hosts','spillover','animal']\nlimit=150\na,b=get_word_count(targets,df,'full_text','paper_id')\npaper_word_distribution=get_word_distribution(b,limit)","39678eef":"draw_plot(paper_word_distribution)","5ab061de":"country_name=[]\nfor country in pycountry.countries:\n    country_name.append(country.name)\ntargets=country_name\nlimit=40\ncountries_mentioned,b = get_word_count(targets,df,'full_text','paper_id')","3e4e77f1":"Total_count = {}\nfor eachvalue in countries_mentioned.values():\n    Total_count = {key: Total_count.get(key, 0) + eachvalue.get(key, 0)\n          for key in set(Total_count) | set(eachvalue)}\nfor country,counts in Total_count.items():\n    Total_count[country]=[counts]","a4143594":"print (\"{:<35} {:<7}\".format('Country','Total no. of times mentioned'))\nfor k, v in Total_count.items():\n     print (\"{:<35} {:<7}\".format(k, v[0]))","8a0e215c":"country_count_df= pd.DataFrame.from_dict(Total_count)\ncountry_count_df = country_count_df.T\ncountry_count_df.columns = ['Total no. of times mentioned']\ncountry_count_df['Country'] = country_count_df.index\n#Rearranging columns\ncols = country_count_df.columns.tolist()\ncols = cols[-1:] + cols[:-1]\ncountry_count_df=country_count_df[cols]\nsorted_country_count_df = country_count_df.sort_values(by='Total no. of times mentioned', ascending=False)\n\ndf_sample = sorted_country_count_df\ncolorscale = [[0, '#4d004c'],[.5, '#f2e5ff'],[1, '#fffffe']]\n\nfig =  ff.create_table(df_sample, height_constant=20)\nfig.show()","7a25afd7":"np.random.seed(12)\ngapminder = px.data.gapminder().query(\"year==2007\")\n\nd = Total_count\n\ndata_country = pd.DataFrame(d).T.reset_index()\ndata_country.columns=['country', 'count']\n\ndf_merge=pd.merge(gapminder, data_country, how='left', on='country')\n\nfig = px.choropleth(df_merge, locations=\"iso_alpha\",\n                    color=\"count\", \n                    hover_name=\"country\",\n                    color_continuous_scale=px.colors.sequential.Plasma)\n\nfig.show()","3f82ab78":"px.scatter(df_merge, x=\"country\", y=\"count\", color=\"continent\", size=\"pop\", size_max=60,\n          hover_name=\"country\")","a488a140":"#Add all the references in a single list and find the count of the repeats\nbibs=[]\nfor item in df['bib_entries']:\n    for eachbib in item:\n        bibs.append(eachbib)\na = dict(Counter(bibs))\ndel a['']\ndf_a=pd.DataFrame.from_dict(a, orient='index',columns=['no. of times cited'])\ndf_a['no. of times cited'] = df_a['no. of times cited'].astype(str).astype(int)\nsorted_df_a=df_a.sort_values(by='no. of times cited', ascending=False)\nnew_df = sorted_df_a.loc[sorted_df_a['no. of times cited'] >= 50] #50 here is the minimum no. of times the paper has been cited\nnew_df['title'] = new_df.index.str.slice(0,30)#truncated the title","3b74521d":"#Getting Seaborn Style for Pandas Plots\ntop_n=25\nsns.set()       \nnew_df[0:top_n].reset_index().plot(\n    x = 'title', \n    y = 'no. of times cited', \n    kind='bar', \n    legend = False,\n    width=0.8\n)\nplt.ylabel(\"Number of times cited\")\nplt.xlabel(\"Paper\")\nplt.title(\"No. of citations of top \"+format(top_n)+\" papers\")\nplt.gca().yaxis.grid(linestyle=':')","1de758d7":"In this next section I look into the no. of papers with an **abstract section**. I have displayed one such paper here.","52e884be":"Below I have displayed the country names and the total no. of times they have been mentioned in the research papers","0464fa71":"Here I begin my analysis by plotting the list of searched words and their frequency as found in the 'abstract' field. **Throughout the notebook I have used the list targets to store the words I am searching for.** The count of at least one of these words in 'abstract' needs to be  more than the value of 'limit'.The 'limit' variable here is a threshold value. It has been considered in order to find papers with the highest frequency of a given word. For eg. If I searched for papers where the word 'vaccine' occurs then probably all\/most papers would have been selected. Instead I have searched for papers where the word 'vaccine' occurs more than the 'limit' no. specified.","1563564e":"I start off with a list of the libraries used and helper functions that I have used throughout the notebook","0d4c84e4":"I initially started this EDA by taking inspiration from this great [video](https:\/\/youtu.be\/S6GVXk6kbcs) by @sentdex. My reading of the json files and deciding to look into the incubation period is mostly from this video. Here I am reading the files from the two directories pmc_json and pdf_json. The fields\/attributes  'paper_id', 'title', 'abstract', 'full_text' and 'bib_entries' are then added to a dataframe for further processing. I have only considered 10000 files from each directory for faster processing time and better visibility of individual papers in the graphs plotted later in this notebook. However you can comment out the code snippet(marked below as 'comment block') to consider all the files in the given directories\n","64f55bb0":"Next I drew a Bubble Scatter Plot for mapping distribution of countries mentioned in papers. The countries are also mapped to their respective continents.\n","21426f12":"I have also tried displaying the above table using plotly for better visualization and the dataframe is sorted in descending order based on count.","b40d33ba":"I then considered the bibliographic entries.Here I looked for the papers referenced and the no. of times they have been cited. I am then plotting the papers which have been cited more than 50 times","e253111e":"Next I searched for the countries whose names appear in the full_text. I counted the frequency of occurence of country names and considered those that are greater than a certain threshold limit. ","01d0bee0":"I now plotted the above data in a **bar of pie chart**. Based on my findings so far I have divided the data into 3 groups:\n* Incubation period which is mentioned in Days\n* Incubation period which is mentioned in Weeks\n* Incubation period which is mentioned starting from Days and ends in Weeks\n* Next I explode the incubation period in days(the most frequently occuring group) to show the distribution of days. This is done in the accompaning bar chart section.","fe5d1a7e":"I proceed to create 3 tables.\n* Table 1: Displays the papers where the target words have occured **frequently in the abstract section** of the paper. These are arranged in descending order of their counts\n\n\n* Table 2: Displays the papers where the target words have occured **frequently in the full-text section** of the paper. These too are arranged in descending order of their counts.\n\n**Findings:** In my findings I see that Table 1 varies greatly from Table 2 in terms of the papers selected i.e. the papers which have a target word occuring frequently in its abstract doesnot necessarily have the word occuring frequently in its full-text. Hence Table 3 comes into play\n\n* Table 3: Displays the same paper as Table 1 but with  the count of the target words in the full-text\n\nGraph: I then used a clustered stacked bar-graph to compare Table 1 and Table 3 i.e. it will show me- if a paper has a target word occuring frequently in its abstract field then what is its count in the full-text field.","e11c48a8":"From the incubation time periods found above, I sort the days in descending order of their counts.","03171f4b":"Different list of targets searched in full_text","b76ed72e":"Next we obtain a data profiling report using Pandas Profiling. You can also save the output generated in an html file. I came to know about Pandas Profiling and many other excellent tools from this great [post](https:\/\/www.kaggle.com\/parulpandey\/10-simple-hacks-to-speed-up-your-data-analysis) by @parulpandey","c9153721":"Next I grouped the days in 7 day intervals. This is done for a succinct visualization of the data. You will see in the above data that the maximum value of days in the data considered is 90 days(Right now I have displayed only the head(), you need to remove that to see the whole dataframe). However the count does fall off considerably after 28 days.\nThe groups that I have created are:\n* 0-7 days\n* 8-14 days\n* 15-21 days\n* Over 21 days","665a524c":"I proceed to find the string 'incubation' in the dataframe. In each sentence where the string is found I am looking for the incubation period. Using regular expressions I am searching for digits ending with 'day' or 'week'. Then the mean value of the days and weeks mentioned is calculated to see what kind of time period is being talked about here.","87645046":"Plotting country and the count of occurence using gapminder ","a5c13135":"In this next section I look into the full-text field exclusively. I plot the list of targets and their frequency as found in the 'full_text' field.I have used both the horizontal stacked bar graph and the vertical one for plotting","36ea330e":"I then proceed to do the comparison the other way round i.e. I considered papers where the target words have occured frequently in the full-text section and then checked their frequency in the abstract section of the paper. In some of these cases I got an exception since many of these papers are missing an abstract. For such scenarios I have added a series of 0 values to my dataframe for that particluar paper. You can check my function get_wordcount_in_dataframe for this.","d81cbe4a":"In this kernel I tried doing an **exploratory data analysis of the json files in the CORD-19-research-challenge database** and dig deeper into certain attributes of the research papers. Here are my findings. I am very new to Data Science and this is my first EDA so if you have any suggestions on how I could improve my work I would be happy to listen.\n\nCOVID-19 is the burning topic on everyone's mind and the world's scientists and researchers are coming together to develop new ways of tackling this disease.Here I try to look into some of the scientific findings and provide a pictorial description of the data."}}