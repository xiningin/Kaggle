{"cell_type":{"28658c8b":"code","f664244d":"code","e5d46589":"code","1ea0a721":"code","eb11b1ee":"code","b3bdd89f":"code","84a8d737":"code","213572c1":"code","265fad33":"code","5a08ccd1":"markdown","340880d9":"markdown","bfe5efd8":"markdown","32e24752":"markdown","8af78b49":"markdown","9ad800b6":"markdown","933dfba7":"markdown","0e53c285":"markdown","4789879a":"markdown","beb4997d":"markdown"},"source":{"28658c8b":"from sklearn.tree import DecisionTreeRegressor\nfrom sklearn.manifold import MDS\nimport numpy as np\nimport pandas as pd\nfrom scipy import sparse\nfrom sklearn.preprocessing import LabelEncoder\nfrom tqdm import tqdm\nfrom sklearn.metrics import accuracy_score","f664244d":"def label_data(data):\n    column=data.columns\n    Type={}\n    replace_dict={}\n    for col in column:\n        temp=data[col][np.nonzero(~(data[col].isna()))[0]].values\n        unique=np.unique(temp)\n        unique=np.delete(unique,np.nan)\n        if(temp.dtype=='O'):\n            #categorical\n            Type[col]='categorical'\n            r_dict={}\n            for i in range(len(unique)):r_dict[unique[i]]=i\n            data[col]=data[col].replace(r_dict)\n            replace_dict[col]=r_dict\n        else:\n            if(str(temp.dtype).startswith('int') and len(unique)<=15):\n                Type[col]='ordinal'\n                r_dict={}\n                for i in range(len(unique)):r_dict[unique[i]]=i\n                data[col]=data[col].replace(r_dict)\n                replace_dict[col]=r_dict\n            \n            else:\n                Type[col]='continous'\n            \n    return replace_dict,Type","e5d46589":"def Bootstrapping(data,n_bootstrap):\n    #random sample from train_df of size n_bootstrapping\n    data=data.copy()\n    indices=np.random.randint(low=0,high=data.shape[0],size=n_bootstrap)\n    df_bootstarp=data.iloc[indices,:] #bootstrap dataset\n    del data\n    return df_bootstarp\n\n#apply random forest algo\ndef random_forest(data,n_features,n_trees,n_bootstrap,dt_max_dept):\n    #combination of decision tress is random forest\n    #iter over the decision tree\n    data=data.copy()\n    print(data.shape[0])\n    forest=[]\n    for i in tqdm(range(n_trees)):\n        df_bootstrap=Bootstrapping(data,n_bootstrap)\n        tree=DecisionTreeRegressor(criterion='mse',splitter='best',max_depth=None,min_samples_split=2,\n                                    min_samples_leaf=10,min_weight_fraction_leaf=0.0,max_features=None,\n                                    random_state=42,max_leaf_nodes=None,min_impurity_decrease=0.0,min_impurity_split=None,\n                                   presort='deprecated',ccp_alpha=0.0,)\n        tree.fit(df_bootstrap.iloc[:,0:-1],df_bootstrap.iloc[:,-1])\n        print(tree.score(df_bootstrap.iloc[:,0:-1],df_bootstrap.iloc[:,-1]))\n        forest.append(tree)\n    del data,df_bootstrap\n    return forest","1ea0a721":"def reverse_label(self,data,replace_dict,Type):\n    column=data.columns\n    for col in column:\n        if(Type[col]!='continous'):\n            r_dict=replace_dict[col]\n            key=r_dict.keys()\n            rev_dict={}\n            for k in key:rev_dict[r_dict[k]]=k\n            data[col]=data[col].replace(rev_dict)","eb11b1ee":"class Random_forest_imputer:\n    def __init__(self,\n               n_trees=100,\n               n_bootstrap=400,\n               criterion='mse',\n               splitter='best',\n               max_depth=None,\n               min_samples_split=2,\n               min_samples_leaf=10,\n               min_weight_fraction_leaf=0.0,\n               max_features=None,\n               random_state=42,\n               max_leaf_nodes=None,\n               min_impurity_decrease=0.0,\n               min_impurity_split=None,\n               presort='deprecated',ccp_alpha=0.0):\n        self.n_bootstrap=n_bootstrap\n        self.n_trees=n_trees\n        self.criterion=criterion\n        self.splitter=splitter\n        self.max_depth=max_depth\n        self.min_samples_split=min_samples_split\n        self.min_samples_leaf=min_samples_leaf\n        self.min_weight_fraction_leaf=min_weight_fraction_leaf\n        self.max_features=max_features\n        self.random_state=random_state\n        self.max_leaf_nodes=max_leaf_nodes\n        self.min_impurity_decrease=min_impurity_decrease\n        self.min_impurity_split=min_impurity_split\n        self.presort=presort\n        self.ccp_alpha=ccp_alpha\n    \n    def label_data(self,data):\n        column=data.columns\n        Type={}\n        replace_dict={}\n        for col in column:\n            temp=data[col][np.nonzero(~(data[col].isna()).values)[0]].values\n            unique=np.unique(temp)\n            unique=np.delete(unique,np.nan)\n            if(temp.dtype=='O'):\n                #categorical\n                Type[col]='categorical'\n                r_dict={}\n                for i in range(len(unique)):r_dict[unique[i]]=i\n                data[col]=data[col].replace(r_dict)\n                replace_dict[col]=r_dict\n            else:\n                if(str(temp.dtype).startswith('int') and len(unique)<=15):\n                    Type[col]='ordinal'\n                    r_dict={}\n                    for i in range(len(unique)):r_dict[unique[i]]=i\n                    data[col]=data[col].replace(r_dict)\n                    replace_dict[col]=r_dict\n\n                else:\n                    Type[col]='continous'\n        return data,replace_dict,Type\n    \n    def reverse_label(self,data,replace_dict,Type):\n        column=data.columns\n        for col in column:\n            if(Type[col]!='continous'):\n                r_dict=replace_dict[col]\n                key=r_dict.keys()\n                rev_dict={}\n                for k in key:rev_dict[r_dict[k]]=k\n                data[col]=data[col].replace(rev_dict)\n\n\n    def Bootstrapping(self,data):\n        #random sample from train_df of size n_bootstrapping\n        n_bootstrap=self.n_bootstrap\n        data=data.copy()\n        indices=np.random.randint(low=0,high=data.shape[0],size=n_bootstrap)\n        df_bootstarp=data.iloc[indices,:] #bootstrap dataset\n        del data\n        return df_bootstarp\n\n    #apply random forest algo\n    def random_forest(self,data):\n        #combination of decision tress is random forest\n        #iter over the decision tree\n        score=[]\n        data=data.copy()\n        print(data.shape[0])\n        forest=[]\n        for i in tqdm(range(self.n_trees)):\n            df_bootstrap=self.Bootstrapping(data)\n            tree=DecisionTreeRegressor(criterion=self.criterion,splitter=self.splitter,max_depth=self.max_depth,min_samples_split=self.min_samples_split,\n                                        min_samples_leaf=self.min_samples_leaf,min_weight_fraction_leaf=self.min_weight_fraction_leaf,max_features=self.max_features,\n                                        random_state=self.random_state,max_leaf_nodes=self.max_leaf_nodes,min_impurity_decrease=self.min_impurity_decrease,min_impurity_split=self.min_impurity_split,\n                                       presort=self.presort,ccp_alpha=self.ccp_alpha)\n            tree.fit(df_bootstrap.iloc[:,0:-1],df_bootstrap.iloc[:,-1])\n            score.append(tree.score(df_bootstrap.iloc[:,0:-1],df_bootstrap.iloc[:,-1]))\n            forest.append(tree)\n        print('Forest Accuracy',np.mean(score))\n        del data,df_bootstrap\n        return forest\n    \n    def fill_na(self,data,Type):\n        columns=data.columns\n        for col in columns:\n            unq=data[col].unique()\n            temp=data[col].values\n            if(Type[col]!='continous'):\n                value=data[col].mode().values[0]\n            else:\n                #'continous' Vairable\n                value=data[col].median()\n            data[col].fillna(value=value,inplace=True)\n    \n    def combination(self,array):\n        a=[]\n        for i in array:\n            for j in array:\n                if(i!=j):\n                    a.append([int(i),int(j)])\n        del array\n        return a\n\n    def proximity_matrix(self,data,pred,proximity):\n        ind_pred=data.index\n        pred_ind=[[pred_,ind_] for pred_,ind_ in zip(pred,ind_pred)]\n        pred_ind=np.sort(pred_ind,axis=0)\n        grp_ind=np.split(pred_ind[:,1],np.cumsum(np.unique(pred_ind[:,0],return_counts=True)[1])[:-1])\n\n        #proximity=proximity.toarray()            \n        for array in grp_ind:\n            cmb=self.combination(array)\n            for row,col in cmb:\n                proximity[row,col]+=1\n\n        return proximity\n    def main_function(self,train_df):\n        data,replace_dict,Type=self.label_data(train_df)\n        data=train_df.copy()\n        #find location of missing values\n        #before that remove the columns wich have more 50% NaN Value\n        indices_remove=np.nonzero(((train_df.isna().sum().values)\/train_df.shape[0]>=0.5)*1)\n        indices=np.delete(np.arange(data.shape[1]),indices_remove)\n        train_df=train_df.iloc[:,indices]\n        data=data.iloc[:,indices]\n        row,col=np.nonzero((data.isna().values)*1)\n        proximity=np.zeros((data.shape[0],data.shape[0]))\n        ind_nul=np.array([[r,c] for r,c in zip(row,col)])\n        #ind_nul=np.sort(ind_nul,axis=0)\n        del row,col\n        train=train_df.iloc[np.delete(np.array(train_df.index),np.nonzero(np.array(train_df.isna().sum(axis=1)))[0]),:]\n        print('creating Tree............')\n        forest=self.random_forest(train)\n        print('proximity_matrix Processing...........')\n        self.fill_na(data,Type)\n        #return data\n        for tree in tqdm(forest):\n            pred=tree.predict(data.iloc[:,0:-1])\n            proximity=self.proximity_matrix(data,pred,proximity)\n\n        del tree\n        proximity=proximity\/self.n_trees#n_trees\n        print('starts filling nan values.........')\n        for r,c in ind_nul:\n            similarity=proximity[r,:]\n            if(len(data.iloc[:,c].unique())<=15):\n                #categorical vairable\n                unique,count=np.unique(data.iloc[:,c].values,return_counts=True)\n                weighted=[]\n                for u,cnt in zip(unique,count):\n                    prob=cnt\/count.sum()\n                    vector=(data.iloc[:,c]==u)*1\n                    weighted.append([prob*(np.dot(vector,similarity))\/similarity.sum(),u])\n                weighted=np.sort(weighted,axis=0)\n                data.iloc[r,c]=weighted[-1,1]\n\n            else:\n                #continous values\n                value=np.dot(similarity,data.iloc[:,c].values)\/sum(similarity)\n                data.iloc[r,c]=value\n        \n        self.reverse_label(data,replace_dict,Type)\n        print('imputation completed')\n        #finally data is imputed\n        return data,proximity","b3bdd89f":"data=pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\nimp=Random_forest_imputer()\ndata_transformed,proximity=imp.main_function(data.copy())","84a8d737":"np.nonzero(1*(~(data[\"MSSubClass\"].isna())).values)","213572c1":"from sklearn.manifold import MDS\nproximity_diss=1-proximity \nembedding = MDS(n_components=2)\ntransformed_proximity=embedding.fit_transform(proximity_diss)","265fad33":"import matplotlib.pyplot as plt\n#ploting first 50 sample\nr=50\nx=[i[0] for i in transformed_proximity]\ny=[i[1] for i in transformed_proximity]\nx=x[0:r]\ny=y[0:r]\nfig, ax = plt.subplots()\n\nax.scatter(x, y)\n\nfor i in (np.arange(r)):\n    ax.annotate(i+1, (x[i], y[i]))","5a08ccd1":"# Reverse The Labels\n\nLabel Encoding that is done on the Categorical Vairable are reversed as given in the RAW Data.","340880d9":"Create the dataset by removing the row with NaN values and keep this dataset for training the model.\n\nNow the basic concepts comes that is the combination of Decision Tree is the Random_Forest!!\n\nInorder to create  Random Forest we need Boostrap_Dataset.\nBootsrtapping is process in which random selection of data point given the size of Boostrap_dataset.\n\nNow the Next step to fit this dataset and create the forest.","bfe5efd8":"# MDS Plot\n\n**Multidimensional scaling**(MDS) is a means of visualizing the level of similarity of individual cases of a dataset. MDS is used to translate \"information about the pairwise 'distances' among a set of n objects or individuals\"","32e24752":"#  The Main Concept......\n**Proximity_Matrix** :Lets Understand from example,In This figure, Matrix with Dimension 4x4 Each row and col represent the sample and each grid shows the similarity values. that is row=3 that is sample 3 and col=4 that is sample 4 and [3,4]=(similarity value).Proximity Matrix intialize with  all the values are zeros with size nxn, n no of sample.\n\n**Calculation of Proximity Matrix**:Above we are able to create Random Forest from Decision_trees and  data with nan values replaces with mode\/median, Mode for categorical and Median for Numerical Columns.\n\nstart prediction with Each tree and for each tree we have some prediction value based upon this values we will group the samples \n\nFor Example:\n\ndo this for all tree.\n\n\n![1.png=10x10](attachment:1.png)\n![2.png=25x25](attachment:2.png)\n![3.png](attachment:3.png)\n![4.png](attachment:4.png)\n![5.png](attachment:5.png)\nFinally we got Proximity matrix\n\nFind out the location of nan value so that  the nan values can be replaced with weighted average with proximity matrix...........","8af78b49":"Repeat the above method that is  calculating proximity matrix and get weighted average untils the values is converged.","9ad800b6":"# Final Combined Source Code","933dfba7":"# By looking the Above plot it's clear that sample which are closer are more similar and vice versa","0e53c285":"# Cluster the Data points using Random Forest and use this Information to Create the Similarity Matrix to Fill The missing Value in the Dataset\n\n\n\nSteps:\n\n1.Label The data.\n\n2.Fit RandomForest Regressor. \n\n3.Create the Proximity_matrix(similarity Matrix).\n\n4.Fill the missing Values using Proximity Matrix.\n\n5.Plot the MDP plot(Visualize the similarity between the data points).\n\nLets Start!!!!!!!!!!!\n","4789879a":"# Label Encoding.\n\nBasic Label_Encoding that is find unique element in columns and replace with it's Index value.\n\n**Note**:There is no changes to NaN values ","beb4997d":"# How a Clustering Work with the Random forest?\n\nIn Random Forest there are numbers of Trees or The collection of Decision Tree is Random forest.\nIn the end of every Decision Tree it will come to a leaf node and the Data Points which have the same Leaf node are clustered,\n\nand doing this things in metric form , proximity matrix is introduce which measure the similarity between the data points."}}