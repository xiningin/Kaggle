{"cell_type":{"18277a43":"code","71c1c2ec":"code","6d9fc749":"code","f4ff0817":"code","ebff6f3a":"code","31712cee":"code","68d2214f":"code","07faf30c":"code","8637aed7":"code","64c3efbc":"code","5a3f5b95":"code","a0be015b":"code","374afe2d":"code","ac2df05a":"code","b65a4ec0":"code","3288d137":"code","36e196fc":"code","cfdaacac":"code","6cee0156":"markdown","4a229ac5":"markdown","68004103":"markdown","4d13c2fd":"markdown","841b72a0":"markdown","fb97d1ef":"markdown","bcdd395a":"markdown","0ddcd10e":"markdown","5e83236a":"markdown","63a997dc":"markdown","5cda8a18":"markdown","9428e0f7":"markdown","f53f9aaa":"markdown","7a6e972a":"markdown","71aa2246":"markdown","f56ae0d0":"markdown","4d591803":"markdown","9da16a81":"markdown","c91ffb77":"markdown","dbbcd3eb":"markdown","e3d8f652":"markdown","4a1b63c2":"markdown"},"source":{"18277a43":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","71c1c2ec":"# Importing required python modules\n\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets, neighbors\nfrom matplotlib.colors import ListedColormap\nfrom sklearn import metrics\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","6d9fc749":"data1 = np.genfromtxt('\/kaggle\/input\/knn-dataset\/knn-data\/1.ushape.csv',delimiter=',')\ndata1","f4ff0817":"type(data1)","ebff6f3a":"# defining X and y where X having first 2 columns and y have 3rd column\n\nX = data1[:,:2]  # this is slicing\ny = data1[:,2]","31712cee":"# k value\nn_neighbors = 1\n\n# initialise KNN classifier with n_neighbors as its parameters\nknn_clf = neighbors.KNeighborsClassifier(n_neighbors=n_neighbors) # n_neighbors is a hyperparameter assign its valu very carefully\n# fitting KNN with X and y\nknn_clf.fit(X,y)","68d2214f":"# Getting x minimum value-1 and maximum value+1 from X's 1st column \nx_min,x_max = X[:,0].min()-1, X[:,0].max()+1\n# Getting  minimum value-1 and maximum value+1 from X's 2nd column \ny_min,y_max = X[:,1].min()-1,X[:,1].max()+1\n# deciding step size in mesh (grid cell size)\ngrid_space = 0.02\n\n#Creating meshgrid\nxx, yy = np.meshgrid(np.arange(x_min,x_max,grid_space),np.arange(y_min,y_max,grid_space))","07faf30c":"# predict using knn\nvalue_each_elem_of_grid = knn_clf.predict(np.c_[xx.ravel(),yy.ravel()])\nvalue_each_elem_of_grid","8637aed7":"value_each_elem_of_grid = value_each_elem_of_grid.reshape(xx.shape)\nvalue_each_elem_of_grid","64c3efbc":"# having light colors haxcode\ncmap_light = ListedColormap(['#FF6161','#00807B']) \n# having above matching shades of dark colors\ncmap_bold = ListedColormap(['#F50000','#00B8B1'])\n\nplt.figure(figsize=(18,7))\n# using pcolormesh function\nplt.pcolormesh(xx,yy,value_each_elem_of_grid,cmap=cmap_light)\n# scatter plot with given points i.e having 1st and 2nd columns of X, color = y and cmap = cmap_bold\nplt.scatter(X[:,0],X[:,1],c=y,cmap=cmap_bold)\n\n# defining scale on both axises \n\n# x limited to xx's min and max values \nplt.xlim(xx.min(),xx.max())\n# y limited to yy'ss min and max values\nplt.ylim(yy.min(),yy.max())\n\n# set the title as K values = n_neighbors\nplt.title('K values = '+str(1))\nplt.show()","5a3f5b95":"def kNN_visual_comparison(data,n_neighbors):\n    X = data[:,:2]\n    y = data[:,2]  \n    '''\n    k-NN Classifier\n    '''\n    knn_clf = neighbors.KNeighborsClassifier(n_neighbors=n_neighbors)\n    knn_clf.fit(X,y)\n    '''\n    mesh grid with grid_space = 0.02\n    '''\n    grid_space = .02\n    x_min,x_max = X[:,0].min()-1, X[:,0].max()+1 \n    y_min,y_max = X[:,1].min()-1,X[:,1].max()+1 \n    xx, yy = np.meshgrid(np.arange(x_min,x_max,grid_space),np.arange(y_min,y_max,grid_space))\n    \n    ''' \n    predicting value of each element in grid\n    '''\n    value_each_elem_of_grid = knn_clf.predict(np.c_[xx.ravel(),yy.ravel()])\n    value_each_elem_of_grid = value_each_elem_of_grid.reshape(xx.shape)\n    \n    '''\n    ploting descision boudries\n    '''\n    plt.figure(figsize=(18,8))\n    plt.pcolormesh(xx,yy,value_each_elem_of_grid,cmap=cmap_light)\n    plt.scatter(X[:,0],X[:,1],c=y,cmap=cmap_bold)\n    plt.xlim(xx.min(),xx.max())\n    plt.ylim(yy.min(),yy.max())\n    plt.title('K values = '+str(n_neighbors))\n    plt.show()","a0be015b":"data1 = np.genfromtxt('\/kaggle\/input\/knn-dataset\/knn-data\/1.ushape.csv',delimiter=',')\ndata2 = np.genfromtxt('\/kaggle\/input\/knn-dataset\/knn-data\/2.concerticcir1.csv',delimiter=',')\ndata3 = np.genfromtxt('\/kaggle\/input\/knn-dataset\/knn-data\/3.concertriccir2.csv',delimiter=',')\ndata4 = np.genfromtxt('\/kaggle\/input\/knn-dataset\/knn-data\/4.linearsep.csv',delimiter=',')\ndata5 = np.genfromtxt('\/kaggle\/input\/knn-dataset\/knn-data\/5.outlier.csv',delimiter=',')","374afe2d":"# n_neighbors\nk = [1,5,20,30,50]","ac2df05a":"# using kNN_visual_comparison function on above loaded data with k values as 1,5,20,30 and 50\nfor neighbor in k:\n    kNN_visual_comparison(data1,n_neighbors=neighbor)","b65a4ec0":"# using kNN_visual_comparison function on above loaded data with k values as 1,5,20,30 and 50\nfor neighbor in k:\n    kNN_visual_comparison(data2,n_neighbors=neighbor)","3288d137":"# using kNN_visual_comparison function on above loaded data with k values as 1,5,20,30 and 50\nfor neighbor in k:\n    kNN_visual_comparison(data3,n_neighbors=neighbor)\n","36e196fc":"# using kNN_visual_comparison function on above loaded data with k values as 1,5,20,30 and 50\nfor neighbor in k:\n    kNN_visual_comparison(data4,n_neighbors=neighbor)","cfdaacac":"# using kNN_visual_comparison function on above loaded data with k values as 1,5,10 and 20\n#for neighbor in k:\n#    kNN_visual_comparison(data5,n_neighbors=neighbor)","6cee0156":"* This will be bit time consuming ","4a229ac5":"* More about : [numpy.genfromtxt official](https:\/\/numpy.org\/doc\/stable\/reference\/generated\/numpy.genfromtxt.html)\n    * [w3resource](https:\/\/www.w3resource.com\/numpy\/input-and-output\/genfromtxt.php)\n    * Purpose of genfromtxt() is to load data from a text file, with missing values handled as specified\n* More about : [numpy.meshgrid](https:\/\/numpy.org\/doc\/stable\/reference\/generated\/numpy.meshgrid.html)\n    * Purpose of meshgrid is to create a rectangular grid out of an array of x values and an array of y values","68004103":"### For data2","4d13c2fd":"# Lets Explore Implementation\nIn below implementations I want show `how different dataset can be grouped based on their similarities`","841b72a0":"**`predicting value (either 0 or 1) of each element in the grid`**","fb97d1ef":"# `k-NN Classifier`","bcdd395a":"### For data3","0ddcd10e":"### For data4","5e83236a":"![image.png](attachment:c3d3369d-f765-4510-8316-0a7e4afac07e.png)","63a997dc":"**`converting output back to xx shape as we need it to plot Decision Boundry)`**","5cda8a18":"**In this Notebook I am using appx 10 dataset. Why 10?**\n\n**This is how I am trying to show how kNN will work on different dataset**\n\n**`I will try to explain code line by line and after that I will make a fucntion out of the explainde code as to reduce the coding task`**\n\n# AIM\n`To make you see how different dataset can be grouped based on their similarities, the kNN magic` <br>\n`These visualization are meant to understand conceptual clarity on how KNN works`","9428e0f7":"# Loading Datasets\nI am loding all datasets in different variables","f53f9aaa":"**`Creating a `mesh grid (x_min,x_max) and (y_min y_max)` with `0.02 grid spaces`,which is defined as in h variable`**","7a6e972a":"### For data5","71aa2246":"# Observation\nObserve every image carefully again, you will see that `boundary becomes smoother with increasing value of K`\n* If `K increasing to infinity` image finally becomes all blue or all red depending on total majority\n* `Training Error Rate` and `Validation Error Rate` are two parameters,One need to access different K-value\n    * this is how one can select best suitable k value for the given problem statement","f56ae0d0":"### For data1","4d591803":"# `k-Nearest Neighbor `\n`kNN` a simple, easy-to-implement `Supervised Machine Learning Algorithm` that can be used to `solve both Classification and Regression problems`<br>\n* More widely used for Classification Problems in the Industry <br>\n* Beauty behind a k-NN algorithm is that there\u2019s no need for training, learning\n\n---\n\n* K-NN algorithm `assumes similarity between new cases\/data and available cases\/data and puts new case into category that is most similar to available categories`\n    * This means when new data appears then it can be easily Classified into a well suite category by using K-NN algorithm\n* It is a `non-parametric algorithm`,which means it does not make any assumption on underlying data\n* It is also called a `lazy learner algorithm`\n    * because it does not learn from training set immediately,instead it stores dataset and at time of Classification, it performs an action on the dataset \n        * i.e it Classifies that data into a category which is much similar to new data\n        \n`Example`: <br>\n* Say you have an image of a creature that looks similar to cat and dog \n* Now you want to know either it is a cat or dog\n    * So for this identification, you can use the k-NN algorithm, as it works on a similarity measure. \n* k-NN model will find similar features of new data set to cats and dogs images and based on most similar features it will put it in either cat or dog category\n\n---\n\n* `k-NN Classification can be effectively used as an outlier detection method (i.e. fraud)`\n* `k-NN Regression can be applied to many types of Regression problems effectively, including actuarial models, environmental models, and real estate models`\n\n---\n\n* `k-NN can be used on airplanes to warn pilots and air traffic if something is going wrong on an airplane in ways that human brain cannot compute or understand`\n* can be used in many system or biological health monitoring to notify technicians if anything is going wrong","9da16a81":"**`using above defined function(kNN_visual_comparison) to observe how kNN works on different type daatsets`**","c91ffb77":"\n\nPredict ethnicity of person behind red dot. How would you do it? <br>\n\nIn this case you\u2019d look at people surrounding person. From looking at people surrounding Mr.Red Dot, you can guess that person is Asian <br>\n\n`k-NNalgorithm` basically makes a prediction based off neighboring data points. Easy right? Let\u2019s dig in a bit deeper\n\n---\n**`Training a k-NN`** <br>\n\nThere\u2019s no need for training, learning\n\n---\n**`Predicting using k-NN`**\n* Although training is fast using a kNN, predicting on a kNN takes a bit of time\n* On a new data point kNN will calculate it\u2019s distance from every single data point in dataset\n    * Most popular distance metric used is `Euclidean Distance`\n* Once every single distance is calculated k-NN algorithm will pick K-nearest data points\n---\n* `In Classification Problems` => it\u2019ll make a prediction based on class of those k-nearest datapoints\n    * In this context, it\u2019s a good idea to `choose K as an odd number to avoid dies`\n    * This predicted class will be the most frequent occuring class within K data points\n* `In Regression Problems`, it'll predict based on `mean` or `median` of those data points\n\n---\n**`Bias\/Variance Tradeoff`**  <br>\n\nIn any ML problem, our goal is to `minimize amount of bias\/variance within our model`\n* `When k=1`, we are only looking at one neighbor and assigning it its class\n    * if k=1 \n        * `variance will be high` \n        * `bias will be low` => `can be seen as overfitting on the data`\n* `When k = number of data points`\n    * `variance will be low` \n    * `bias will be high` =>  `can be seen as underfitting on the data` as in this case since we\u2019re only just assigning point to majority, within our entire dataset\n---\n\n**`Hyperparameter`** <br>\n`k`\/`n_neighbours` => how to decide values of k\n* K-Fold Cross Validation\n* Testing a variety of K\u2019s\n* Square root of n\n---\n---","dbbcd3eb":"---\n---","e3d8f652":"# Function\nNow I am intrusted in writing a fucntion which can be used on different dataset","4a1b63c2":"**`ploting Descision Boudries`**"}}