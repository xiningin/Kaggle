{"cell_type":{"ebf30f38":"code","c70a58ae":"code","088b8235":"code","fdeca524":"code","88049baa":"code","7ee08ad3":"code","95d4e0d5":"code","b08447ba":"code","50896708":"code","7c6d5384":"code","d144cede":"code","f795f363":"code","0a0ff1c6":"code","38969487":"code","c2f99825":"code","7f2fe4af":"code","5862f909":"code","1480dcd4":"code","d506dcd2":"code","3f544cf9":"code","3a220204":"code","232ae675":"code","f0347ec7":"code","96cec88e":"code","2b10327c":"code","56d3f658":"code","dd38ac5d":"code","3be61610":"code","fcfef360":"code","e84dfb04":"code","352d5d7f":"code","a7fbf9fe":"code","a9f210d1":"code","9a3eb941":"code","e12aa1a5":"code","ac604d86":"code","ded8c747":"code","7c098248":"code","f93f214a":"code","aa00ad38":"code","f0b76db9":"markdown","7e20aeca":"markdown","e7990085":"markdown","0c09bdb7":"markdown","309fe3c1":"markdown","8dfd18fd":"markdown","207db11a":"markdown","5c0e5f54":"markdown","a48a034b":"markdown","7d5aeb64":"markdown","872f5f75":"markdown","e23ed351":"markdown","83621eb9":"markdown","1203d7cd":"markdown","9dfd7b08":"markdown","c26ee1db":"markdown","85430865":"markdown","0589fc75":"markdown","73dc70d6":"markdown","6a936355":"markdown"},"source":{"ebf30f38":"import time\nimport numpy as np\nimport pandas as pd\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objs as go\n\n# install datatable\n!pip install datatable > \/dev\/null\nimport datatable as dt","c70a58ae":"# fast load the data\ntrain_datatable = dt.fread('..\/input\/jane-street-market-prediction\/train.csv')\ntrain = train_datatable.to_pandas()\ndel train_datatable\n\nexample_test_datatable = dt.fread('..\/input\/jane-street-market-prediction\/example_test.csv')\nexample_test = example_test_datatable.to_pandas()\ndel example_test_datatable\n\nfeatures = pd.read_csv(\"..\/input\/jane-street-market-prediction\/features.csv\" ,index_col=0)","088b8235":"#Function to reduce memory usage.\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df\n\ntrain = reduce_mem_usage(train)\n# example_test = reduce_mem_usage(example_test)\nfeatures = reduce_mem_usage(features)","fdeca524":"# Display the data\nprint('train:')\ndisplay(train)\n\nprint()\nprint('features:')\ndisplay(features)","88049baa":"# A better way to visualize the feature database.\nfeatures_T = (features*1).T\ndisplay(features_T.style.background_gradient(cmap='Blues'))\ndel features_T","7ee08ad3":"train_null_features = pd.DataFrame(data = train.isnull().sum() \/ len(train))\ndisplay(train_null_features.sort_values(by = 0, ascending = False).head(50))\ndel train_null_features","95d4e0d5":"# A quick description of the variables.\ntrain.describe()","b08447ba":"# Show most frequent dates.\ntrain['date'].value_counts(dropna = False).head(60)","50896708":"# Show least frequent dates.\ntrain['date'].value_counts(dropna = False).tail(60)","7c6d5384":"train['date'].value_counts(dropna = False).describe()","d144cede":"fig = plt.figure(figsize=(15,10))\n\nax = train['date'].value_counts(dropna = False).hist(density = True,\n                histtype = 'bar', bins = 10, align = 'mid', orientation = 'vertical',\n                color = 'LightGray', edgecolor = None)\nplt.grid(b = None)\nplt.xlabel('Observations per day')","f795f363":"train['weight'].value_counts(dropna = False)","0a0ff1c6":"train['weight'].describe(percentiles = [.1, .25, .5, .571, .6, .75, .8, .85, .9, .95])","38969487":"fig = plt.figure(figsize=(15,10))\n\nax = train[~np.in1d(train['weight'], 0)]['weight'].hist(density = True,\n                histtype = 'bar', bins = 10, align = 'mid', orientation = 'vertical',\n                color = 'LightGray', edgecolor = None)\nplt.grid(b = None)\nplt.xlabel('weight')\nplt.title('Weight distribution without weight = 0')","c2f99825":"# Close up \nfig = plt.figure(figsize=(15,10))\n\nax = train[(~np.in1d(train['weight'], 0)) & (train['weight'] < 1.366877e+01)]['weight'].hist(density = True,\n                histtype = 'bar', bins = 14, align = 'mid', orientation = 'vertical',\n                color = 'LightGray', edgecolor = None)\nplt.grid(b = None)\nplt.xlabel('weight')\nplt.title('Weight distribution with weight = (0, 13]')","7f2fe4af":"# Close up \nfig = plt.figure(figsize=(15,10))\n\nax = train[(~np.in1d(train['weight'], 0)) & (train['weight'] < 2)]['weight'].hist(density = True,\n                histtype = 'bar', bins = 14, align = 'mid', orientation = 'vertical',\n                color = 'LightGray', edgecolor = None)\nplt.grid(b = None)\nplt.xlabel('weight')\nplt.title('Weight distribution with weight = (0, 2]')","5862f909":"train_wo_weight0 = train[~np.in1d(train['weight'], 0)]\ndisplay(train_wo_weight0['date'].value_counts(dropna = False).describe())\nprint()\n\nfig = plt.figure(figsize=(15,10))\n\nax = train_wo_weight0['date'].value_counts(dropna = False).hist(density = True,\n                histtype = 'bar', bins = 10, align = 'mid', orientation = 'vertical',\n                color = 'LightGray', edgecolor = None)\nplt.grid(b = None)\nplt.xlabel('Observations per day')","1480dcd4":"'''\nThe number of observation per day is more stable but it is far from enough.\nLets drop the assets with low weight, or the most risky ones.\n'''\ntrain_wo_weight025 = train[train['weight'] > 0.25]\ndisplay(train_wo_weight025['date'].value_counts(dropna = False).describe())\nprint()\n\nfig = plt.figure(figsize=(15,10))\n\nax = train_wo_weight025['date'].value_counts(dropna = False).hist(density = True,\n                histtype = 'bar', bins = 10, align = 'mid', orientation = 'vertical',\n                color = 'LightGray', edgecolor = None)\nplt.grid(b = None)\nplt.xlabel('Observations per day without weight = 0')\n\ndel train_wo_weight025","d506dcd2":"# Now lets study the resp and resp 1 to 4 variables.\nprint('Missing:', train[['resp','resp_1','resp_2','resp_3','resp_4']].isnull().sum())\ntrain[['resp','resp_1','resp_2','resp_3','resp_4']].describe()","3f544cf9":"#Correlation\ndef get_redundant_pairs(df):\n    '''Get diagonal and lower triangular pairs of correlation matrix'''\n    pairs_to_drop = set()\n    cols = df.columns\n    for i in range(0, df.shape[1]):\n        for j in range(0, i+1):\n            pairs_to_drop.add((cols[i], cols[j]))\n    return pairs_to_drop\n\ncorr_matrix = train[['resp','resp_1','resp_2','resp_3','resp_4']].corr()\ncorr_table = corr_matrix.abs().unstack()\nlabels_to_drop = get_redundant_pairs(train[['resp','resp_1','resp_2','resp_3','resp_4']])\ncorr_table = corr_table.drop(labels = labels_to_drop).sort_values(ascending = False)\n\nsize_x = 10    \nsize_y = 10\nplt.figure(figsize = (size_x, size_y))\nsns.set(font_scale = 1.5)\n\nax = sns.heatmap(corr_matrix, annot = False, linewidth = 0.2, cmap='coolwarm')\nbottom, top = ax.get_ylim()\nax.set_ylim(bottom + 0.5, top - 0.5)","3a220204":"plot_list = ['resp', 'resp_1', 'resp_2', 'resp_3', 'resp_4'] \nfig = make_subplots(rows=3, cols=2)\n\ntraces = [\n    go.Histogram(\n        x=train[col], \n        nbinsx=100, \n#         name=col,\n        histnorm = 'probability',\n    ) for col in plot_list\n]\n\nfor i in range(len(traces)):\n    fig.append_trace(\n        traces[i], \n        (i \/\/ 2) + 1, \n        (i % 2) + 1\n    )\n\nfig.update_layout(\n    title_text='Resp distributions', \n    height = 900,\n    width = 800,\n)\n\nfor i in range(len(traces)):\n    fig.update_xaxes(title_text = plot_list[i], range=[-0.5, 0.5], row = (i \/\/ 2) + 1, col = (i % 2) + 1)\n    fig.update_yaxes(range=[0, 0.5], row = (i \/\/ 2) + 1, col = (i % 2) + 1)\n\nfig.show()","232ae675":"display(train_wo_weight0[['resp','resp_1','resp_2','resp_3','resp_4']].describe())\nprint()\n\ncorr_matrix = train_wo_weight0[['resp','resp_1','resp_2','resp_3','resp_4']].corr()\ncorr_table = corr_matrix.abs().unstack()\nlabels_to_drop = get_redundant_pairs(train[['resp','resp_1','resp_2','resp_3','resp_4']])\ncorr_table = corr_table.drop(labels = labels_to_drop).sort_values(ascending = False)\n\nsize_x = 10    \nsize_y = 10\nplt.figure(figsize = (size_x, size_y))\nsns.set(font_scale = 1.5)\n\nax = sns.heatmap(corr_matrix, annot = False, linewidth = 0.2, cmap='coolwarm')\nbottom, top = ax.get_ylim()\nax.set_ylim(bottom + 0.5, top - 0.5)","f0347ec7":"train_resp = pd.DataFrame(data = train[['date', 'resp']].groupby(['date'])['resp'].mean())\ntrain_resp['resp_1'] = train[['date', 'resp_1']].groupby(['date'])['resp_1'].mean()\ntrain_resp['resp_2'] = train[['date', 'resp_2']].groupby(['date'])['resp_2'].mean()\ntrain_resp['resp_3'] = train[['date', 'resp_3']].groupby(['date'])['resp_3'].mean()\ntrain_resp['resp_4'] = train[['date', 'resp_4']].groupby(['date'])['resp_4'].mean()\n\n# Creates several time periods for resp_\ntrain_resp['resp_0_lag_1'] = train_resp['resp'].shift(1)\ntrain_resp['resp_1_lag_1'] = train_resp['resp_1'].shift(1)\ntrain_resp['resp_2_lag_1'] = train_resp['resp_2'].shift(1)\ntrain_resp['resp_3_lag_1'] = train_resp['resp_3'].shift(1)\n\ntrain_resp['resp_0_2'] = train_resp.apply(lambda row: (1 + row['resp'])*(1 + row['resp_0_lag_1']) - 1, axis=1)\ntrain_resp['resp_1_2'] = train_resp.apply(lambda row: (1 + row['resp_1'])*(1 + row['resp_1_lag_1']) - 1, axis=1)\ntrain_resp['resp_2_2'] = train_resp.apply(lambda row: (1 + row['resp_2'])*(1 + row['resp_2_lag_1']) - 1, axis=1)\ntrain_resp['resp_3_2'] = train_resp.apply(lambda row: (1 + row['resp_3'])*(1 + row['resp_3_lag_1']) - 1, axis=1)\n\ntrain_resp","96cec88e":"train_resp.groupby('date')[['resp', 'resp_1', 'resp_2', 'resp_3', 'resp_4']].sum().cumsum().plot(figsize=(20,15))\nplt.title('Cumulative Sum of Different RESP\\'s', fontsize = 20)\nplt.xlabel('Date', fontsize = 15)\nplt.legend(fontsize = 15, ncol = 3, loc = 2)","2b10327c":"# Filter only feature variables.\nmask = train.columns.str.contains('feature_')\nfeatures_only = train.loc[:,mask]\n\n# Missing\ntrain_null_features = pd.DataFrame(data = features_only.isnull().sum() \/ len(train))\ntrain_null_features = train_null_features.sort_values(by = 0, ascending = False)\ntrain_null_features.columns = ['%_missing']\n\ndisplay(train_null_features.head(30))\nprint()\ndisplay(train_null_features.tail(43))","56d3f658":"train_feature_0_bar_graph = pd.DataFrame(train['feature_0'].value_counts())\n\nax = train_feature_0_bar_graph.plot(kind='bar', figsize=(15,10), width = 0.58, rot = 0,\n                                           align='center', color = 'LightGray', edgecolor = None)\n\ntotal = 0\nfor bars in ax.patches:\n    total += bars.get_height()\n\nfor p in ax.patches:\n    width = p.get_width()\n    height = p.get_height()\n    x, y = p.get_xy() \n    ax.annotate(f'{height\/total:.2%}', (x + width\/2, y + 100 + height), ha = 'center')","dd38ac5d":"train_feature_0_wo_weight0_bar_graph = pd.DataFrame(train_wo_weight0['feature_0'].value_counts())\n\nax = train_feature_0_wo_weight0_bar_graph.plot(kind='bar', figsize=(15,10), width = 0.58, rot = 0,\n                                           align='center', color = 'LightGray', edgecolor = None)\n\ntotal = 0\nfor bars in ax.patches:\n    total += bars.get_height()\n\nfor p in ax.patches:\n    width = p.get_width()\n    height = p.get_height()\n    x, y = p.get_xy() \n    ax.annotate(f'{height\/total:.2%}', (x + width\/2, y + 100 + height), ha = 'center')","3be61610":"train_feature_0_minus_one = train[np.in1d(train['feature_0'], -1)]\ntrain_feature_0_one = train[np.in1d(train['feature_0'], 1)]\n\nfig = plt.figure(figsize=(15,15))\n\nax0 = fig.add_subplot(3,1,1)\nax0 = train['resp'].hist(density = True,\n                histtype = 'bar', bins = 100, align = 'mid', orientation = 'vertical',\n                color = 'LightGray', edgecolor = None)\nplt.grid(b = None)\nplt.xlabel('resp')\nplt.xlim(-0.2, 0.2)\nplt.ylim(top = 40)\n\n\nax1 = fig.add_subplot(3,1,2)\nax1 = train_feature_0_minus_one['resp'].hist(density = True,\n                histtype = 'bar', bins = 100, align = 'mid', orientation = 'vertical',\n                color = 'LightGray', edgecolor = None)\nplt.grid(b = None)\nplt.xlabel('feature_0 = -1')\nplt.xlim(-0.2, 0.2)\nplt.ylim(top = 40)\n\nax2 = fig.add_subplot(3,1,3)\nax2 = train_feature_0_one['resp'].hist(density = True,\n                histtype = 'bar', bins = 100, align = 'mid', orientation = 'vertical',\n                color = 'LightGray', edgecolor = None)\nplt.grid(b = None)\nplt.xlabel('feature_0 = 1')\nplt.xlim(-0.2, 0.2)\nplt.ylim(top = 40)","fcfef360":"print('resp describe:')\ndisplay(train['resp'].describe())\nprint()\nprint('resp describe considering feature_0 = -1:')\ndisplay(train_feature_0_minus_one['resp'].describe())\nprint()\nprint('resp describe considering feature_0 = 1:')\ndisplay(train_feature_0_one['resp'].describe())","e84dfb04":"#Correlation\ncorr_matrix = train.corr()\ncorr_table = corr_matrix['resp'].abs()\ncorr_table = corr_table.sort_values(ascending = False)\nprint('Features most correlated with resp:')\ndisplay(corr_table.head(10))\nprint()\nprint('Features least correlated with resp:')\ndisplay(corr_table.tail(10))","352d5d7f":"corr_matrix = train_wo_weight0.corr()\ncorr_table = corr_matrix['resp'].abs()\ncorr_table = corr_table.sort_values(ascending = False)\nprint('Features most correlated with resp:')\ndisplay(corr_table.head(10))\nprint()\nprint('Features least correlated with resp:')\ndisplay(corr_table.tail(10))","a7fbf9fe":"corr_matrix_one = train_feature_0_one.corr()\ncorr_table_one = corr_matrix_one['resp'].abs()\ncorr_table_one = corr_table_one.sort_values(ascending = False)\nprint('Features most correlated with resp (considering feature_0 = 1):')\ndisplay(corr_table_one.head(10))\nprint()\nprint('Features least correlated with resp (considering feature_0 = 1):')\ndisplay(corr_table_one.tail(10))","a9f210d1":"corr_matrix_minus_one = train_feature_0_minus_one.corr()\ncorr_table_minus_one = corr_matrix_minus_one['resp'].abs()\ncorr_table_minus_one = corr_table_minus_one.sort_values(ascending = False)\nprint('Features most correlated with resp (considering feature_0 = -1):')\ndisplay(corr_table_minus_one.head(10))\nprint()\nprint('Features least correlated with resp (considering feature_0 = -1):')\ndisplay(corr_table_minus_one.tail(10))","9a3eb941":"print('Features with correlation higher than 1%:', len(corr_table[corr_table > 0.01]))\nprint('Features with correlation higher than 1% (considering feature_0 = 1):', len(corr_table_one[corr_table_one > 0.01]))","e12aa1a5":"def get_redundant_pairs(df):\n    '''Get diagonal and lower triangular pairs of correlation matrix'''\n    pairs_to_drop = set()\n    cols = df.columns\n    for i in range(0, df.shape[1]):\n        for j in range(0, i+1):\n            pairs_to_drop.add((cols[i], cols[j]))\n    return pairs_to_drop\n\ndef correlation_variables(db, features, n, correlation_cut):\n    \n    db_variables = db[features]\n\n    #Correlation\n    corr_matrix = db_variables.corr()\n    corr_table = corr_matrix.abs().unstack()\n    labels_to_drop = get_redundant_pairs(db_variables)\n    corr_table = corr_table.drop(labels = labels_to_drop).sort_values(ascending = False)\n\n\n    print(\"Top Absolute Correlations\")\n    print(corr_table[: n])\n    print()\n    print(\"Least Absolute Correlations\")\n    print(corr_table[len(corr_table) - n: ])\n    print()\n    print(\"Variables with correlation higher than\", correlation_cut, ':', len(corr_table[abs(corr_table) > correlation_cut]))\n    print(\"Percent:\", round(len(corr_table[abs(corr_table) > correlation_cut]) \/ len(corr_table), 4))\n    print()\n\n    size_x = 20     #This is a good size to visualise the heatmap saved as .png\n    size_y = 20\n    plt.figure(figsize = (size_x, size_y))\n    sns.set(font_scale = 1.5)\n\n    ax = sns.heatmap(corr_matrix, annot = False, linewidth = 0.2, cmap='coolwarm')\n    bottom, top = ax.get_ylim()\n    ax.set_ylim(bottom + 0.5, top - 0.5)\n\n    plt.tight_layout()\n    plt.savefig('correlation.png')\n\n    plt.show() ","ac604d86":"features_corr_higher_one = corr_table[corr_table > 0.01].index\nfeatures_corr_higher_one = features_corr_higher_one.drop(['resp_1', 'resp_2', 'resp_3', 'resp_4'])\ncorrelation_variables(train, features_corr_higher_one, 20, 0.9)","ded8c747":"print('Correlation considering feature_0 = 1')\nfeatures_corr_higher_one_f01 = corr_table_one[corr_table_one > 0.01].index\nfeatures_corr_higher_one_f01 = features_corr_higher_one_f01.drop(['resp_1', 'resp_2', 'resp_3', 'resp_4'])\ncorrelation_variables(train, features_corr_higher_one_f01, 35, 0.9)","7c098248":"train_feature_17_missing = train[train['feature_17'].isnull()]\nmask = train_feature_17_missing.columns.str.contains('feature_')\nfeatures_only_17_missing = train_feature_17_missing.loc[:,mask]\n\n# Missing\ntrain_null_features_feature_17 = pd.DataFrame(data = features_only_17_missing.isnull().sum() \/ len(features_only_17_missing))\ntrain_null_features_feature_17 = train_null_features_feature_17.sort_values(by = 0, ascending = False)\ntrain_null_features_feature_17.columns = ['%_missing']\n\nprint('Missings considering when feature_17 is missing:')\ndisplay(train_null_features_feature_17.head(30))\nprint()\ndisplay(train_null_features_feature_17.tail(45))","f93f214a":"print('Distribution of observations per day:')\n\nfig = plt.figure(figsize=(15,10))\n\nax = train_feature_17_missing['date'].value_counts(dropna = False).hist(density = True,\n                histtype = 'bar', bins = 100, align = 'mid', orientation = 'vertical',\n                color = 'LightGray', edgecolor = None)\nplt.grid(b = None)\nplt.xlabel('Observations per day')\n\ntrain_feature_17_missing['date'].value_counts(dropna = False).describe()","aa00ad38":"print('Distribution of resp when feature_17 is missing:')\n\nfig = plt.figure(figsize=(15,10))\n\nax = train_feature_17_missing['resp'].hist(density = True,\n                histtype = 'bar', bins = 100, align = 'mid', orientation = 'vertical',\n                color = 'LightGray', edgecolor = None)\nplt.grid(b = None)\nplt.xlabel('resp')\n\ntrain_feature_17_missing['resp'].value_counts(dropna = False).describe()","f0b76db9":"Not only our features are low correlated with resp, but also our features are high correlated between themselves.\n\nLets see what happens if we consider only the observations with feature_17 missing (our feature with the most missing values).","7e20aeca":"We can play with the weight condition to reduce the variance in the number of observation per day. However, we will not be able to find a magic condition that turns it stable.\\\nIt is a good idea to study the variance in return considering the difference in weight. If the return distribution is too different when considering weight equals to zero, it is best to drop these observations from our training. However, we can still use them to make new variables.","e7990085":"We cannot have a clear description of the variables using describe. We can only see tha we have a binary variable, feature_0, and several continuous variables that can assume negative and positive values.\n\nLets start studying the variable date.","0c09bdb7":"\u201cBuy low, sell high.\u201d It sounds so easy\u2026.\n\nIn reality, trading for profit has always been a difficult problem to solve, even more so in today\u2019s fast-moving and complex financial markets. Electronic trading allows for thousands of transactions to occur within a fraction of a second, resulting in nearly unlimited opportunities to potentially find and take advantage of price differences in real time.\n\nIn a perfectly efficient market, buyers and sellers would have all the agency and information needed to make rational trading decisions. As a result, products would always remain at their \u201cfair values\u201d and never be undervalued or overpriced. However, financial markets are not perfectly efficient in the real world.\n\nDeveloping trading strategies to identify and take advantage of inefficiencies is challenging. Even if a strategy is profitable now, it may not be in the future, and market volatility makes it impossible to predict the profitability of any given trade with certainty. As a result, it can be hard to distinguish good luck from having made a good trading decision.\n\nIn the first three months of this challenge, you will build your own quantitative trading model to maximize returns using market data from a major global stock exchange. Next, you\u2019ll test the predictiveness of your models against future market returns and receive feedback on the leaderboard.\n\nYour challenge will be to use the historical data, mathematical tools, and technological tools at your disposal to create a model that gets as close to certainty as possible. You will be presented with a number of potential trading opportunities, which your model must choose whether to accept or reject.\n\nIn general, if one is able to generate a highly predictive model which selects the right trades to execute, they would also be playing an important role in sending the market signals that push prices closer to \u201cfair\u201d values. That is, a better model will mean the market will be more efficient going forward. However, developing good models will be challenging for many reasons, including a very low signal-to-noise ratio, potential redundancy, strong feature correlation, and difficulty of coming up with a proper mathematical formulation.\n\nJane Street has spent decades developing their own trading models and machine learning solutions to identify profitable opportunities and quickly decide whether to execute trades. These models help Jane Street trade thousands of financial products each day across 200 trading venues around the world.\n\nAdmittedly, this challenge far oversimplifies the depth of the quantitative problems Jane Streeters work on daily, and Jane Street is happy with the performance of its existing trading model for this particular question. However, there\u2019s nothing like a good puzzle, and this challenge will hopefully serve as a fun introduction to a type of data science problem that a Jane Streeter might tackle on a daily basis. Jane Street looks forward to seeing the new and creative approaches the Kaggle community will take to solve this trading challenge.","309fe3c1":"There is no negative and no missing value at the weight variable.\\\nConsidering that there are no negative value, this variable hardly express the position of an asset in a portfolio. Specially, if we are working with derivatives, this means that this is a hedge found and it would be really strange a hedge found without being short in any position.\\\nOn the other hand, the highest value is 167.29. Therefore this weigh variable cannot represent the traded volume of an asset over the total trade volume of the day.\\\nOne possibility is that this variable represent a trading limit for an asset. In other words, if the weight is zero, it means that the risk management area considers the asset too risk and will not let any trade with it. However, if the weight is higher than 1, it means that we can buy the stock and have an exposure higher than our total assets. This means the weight is a good variable that tells us something about the risk of investing in the asset. If the weight is low, there is a high risk; if the weight is high, there is a low risk.\n\nLets filter the observations with weight equals to zero and see if the number of observations per day becames more stable.","8dfd18fd":"The number of observations per day is positive skewed.\\\nIt is hard to believe that in only 500 days we once had 18,000 stocks and after only 2,500. Probably this database has information about derivatives such as options that can be easily created and sold by any investor, and has a short life spam.\\\nThis is a problem because it is almost impossible to analyze the price of an option without knowing the underlying stock. We can still study the greeks of an option, but we do not know which features represent the greeks. Moreover, studying an option only with the greeks it would be a really poor study.\\\nThe ideal would be to find a way to identify these assets that disapear from our database and model only with the 2,500 assets that have observations all days. \n\nLets now sudy the weight variable.","207db11a":"It appears that feature_0 does not have a signicative impact at resp, but when considering feature_0 equal to 1 the resp variable is more concentrated at zero and has a lower variance.\n\nNow lets see how important is each feature to our resp. Lets study the correlations.","5c0e5f54":"Our conclusions stay the same with a filter in weight.\n\nLets syudy the resp by making an equal weighted index with the assets.","a48a034b":"This did not help.\\\nLets see if feature_0 can cause any changes at the correlations.","7d5aeb64":"In total there are 500 days and we can see that there are no missing in date.\\\nThe day with the most number of observations has almost 20,000 asset returns, and the day with the least observations has only 29.\\\nThe mean of observations per day is almos 4,500.\nLets plot an histogram to see it better.","872f5f75":"This model will be measured with a utility function that multiplies weight and resp. Therefore, all observations with weight equal to zero will not be contabilized. However, they still may be carrying important information for our model.\\\nWe will analyze the distributions of the variables with the whole database, and with only the observations with weight different from zero. \n\nThe only information that we have about the weight variables is that observations with weight equals to zero will not be considered. Nonetheless, we do not know what does it really means. Maybe, these observations are from long only founds and the weight variable is the asset weight in the portfolio. However, this does not make too much sense because our goal is to predict a portfolio with a positive return, and not the assets that a manager decided to buy and had a positive return. In other words, if weight express the weight of an asset in a portfolio, our predicted portfolio will not be able to carry any assets that the found manager decided not to buy. In my opinion, the weight varible is related to the liquity of the asset. Therefore, these assets will not be considered in our utility function because there were no transactions with them.\n\nThere are 5 resp variables that express the return in different time horizons. However we do not know what are these time horizons.\n\nAbout the date variable, the days considered are probably only business days (days desconsidering weekends and holidays). It would be interesting to have the exact day because the risk of carrying an asset before a weekend is higher. For example, if an terrorist attack occur in a saturday, the manager will not be able to sell its assets and will lose more money than it would in any other business day.\n\nAbout the features, we do not have any information whatsoever. We have the feature database that atribute tags for each feature, but we do not know what these tags means.\n\nThe ts_id is the id of the observation, it is not the id of an asset. Therefore, there is no information in this variable, it is just an index variable. The fact that we do not have an id per asset is unfortunate because we will not be able to calculate the intrinsic risk of an asset.\n\nIn example_test, we have only 3 days and only 15219 observations, that represents less than 1% of our 2390491 training observations.","e23ed351":"Considering feature_17 missing, we could turn the number of observations per day more stable. Therefore, probably we excluded some type of asset and it may a good idea to make a different model. However, we still have the problem of a lot of missing in our dataset. \\\nConsidering that missings values can have impotant information about the asset type, it is a good idea to work with categorical variables.\n\nIn conclusion:\\\nwe can drop observations with weight = 0;\\\nwe can make different models considering the value of feature_0 or when the value of some feature is missing;\\\nwe can drop several varialbes that are uncorrelated with resp; and\\\nwe can drop some variables that are highly correlated with another variable.","83621eb9":"Unfortunately, we have low correlations with resp. However, we can see that when feature_0 equals to one, ours correlations increase a little.\\\nLets study the correlation between features.","1203d7cd":"It is hard to believe that the only diference between resp variables is the time horizon. If that were the case, we would be able to acumulate the return of one resp variable and get as result another resp variable. Moreover, looking at the graph, it does not even look like these returns are from the same security. If we were to analyze just this graph, we would say that resp_1 is the cumulative return of a treasury bound, and resp_4 the cumulative return of a successful hedge found.\n\nLets analyze the features variables.\\\nWe already know that most features have a normal distribution, and feature_0 is a binary variable. One good notebook to see this is https:\/\/www.kaggle.com\/isaienkov\/jane-street-market-prediction-fast-understanding. \\\nWe do not have any information about the features variables. We have some tag variables that were suposed to give us some information about the features, but it is not of much help. An exemple of how these tagas variables work is:\nassume that tag_1 is true if a feature considers 10 days;\\ \ntag_2 is true if a feature considers 30 days;\\\ntag_3 is true if a feature calculates the volume.\\\nTherefore, if feature_1 is the volume in 10 days and feature_2 is the volume in 30 days, we would have tags 1 and 3 true for feature_1 and tags 2 and 3 true for feature_2.","9dfd7b08":"Unfortunately, our features have a low correlation with our reps.\\\nLets see if e can get better correlations desconsidering observations with weight equals to zero.","c26ee1db":"We can see that the distribution does not change much if we disconsider observations with weight equals to zero. Therefore, there is not any correlation between these variables.\\\nLets see if the distribution of resp changes considering feature_0 equals to one and minus one.","85430865":"It is expected that shorter periods of time have a higher variance. However, if this is true for our database, resp_1 would represent the longest period of time, resp_4 the shortest, and resp should be between resp_3 and resp_4. On the other hand, if we study the mininmum and maximun return, resp_1 should represent the shortest period, resp_4 the longest, and resp should be between resp_3 and resp_4. Finaly, studying the correlation among the resp variables, resp is closer to resp_4. Having said that, we should have in mind that our model will be used to make daily decisions, so resp probably is the daily return.\\\nIf 1, 2, 3 and 4 represent periods of time, we can assume that resp is between resp_3 and resp_4.\\\nMaybe, it is a given that if we decide to buy an asset, we must carry it for at least a month. Therefore, resp would represent the monthly return.\n\nLets repeat this study adding a filter in weight.","0589fc75":"This did not help. Lets see the distribution changes considering only observations with weight different from zero.","73dc70d6":"13 features have almost 15% of missing values.\\\n16 features have almost 3% of missing values.\\\nFor all the rest, we have less than 1% of missing values.\n\nIn the best scenario we will be able to remove these 13 variables with 15% of missing value when we do a correlation study. However, for all variables with missing values that we cannot exclude, we will use the mean or a flag if we decide to work with categorical variables. The use of a flag is more reliable.","6a936355":"13 features have almost 15% of missing values.\n16 features have almost 3% of missing values.\nFor all the rest, we have less than 1% of missing values.\n\nWe have only 42 features without any missing values.\n\nLets start studying feature_0 (that has no missing values)."}}