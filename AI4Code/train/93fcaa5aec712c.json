{"cell_type":{"c7893ed1":"code","c8c46f79":"code","2f1e9ae1":"code","c01d16dc":"code","054cb07f":"code","7e0cd691":"code","a251b32a":"code","129b9e40":"code","cb2f6ccd":"code","626902dc":"code","9ffdc788":"code","78ed6376":"code","1997ec4e":"code","b294b0ea":"code","ce27d765":"code","8a248a43":"code","df80496b":"code","51d4eb8a":"code","45d495f2":"code","57b2bfbf":"code","406c2c82":"code","b47f52fe":"code","b7d7029b":"code","9479d8cd":"code","861b5294":"code","52eff7ee":"code","0b28efd7":"code","4027d86d":"code","e68f3bd8":"markdown","32a6bb9f":"markdown","076ac931":"markdown","ea30804d":"markdown","feddd5fb":"markdown","5730a31b":"markdown","03b57faa":"markdown","6cf5bb2a":"markdown","cfa1f1c5":"markdown","bcdd0e0c":"markdown","c1fbb1b7":"markdown","18b695a0":"markdown","26eeb683":"markdown","45a84e5c":"markdown","f4d8073c":"markdown","caac2565":"markdown","a5b2a558":"markdown","3a0fe906":"markdown","1e590420":"markdown","be295e7e":"markdown","fac0fff1":"markdown"},"source":{"c7893ed1":"# Imports for common tasks, models imported later\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nimport re\nimport gensim\nfrom sklearn.metrics import fbeta_score\nfrom imblearn.combine import SMOTETomek\nimport missingno as msn\nfrom IPython.display import Image\n\n%matplotlib inline","c8c46f79":"df = pd.read_csv(\"..\/input\/cyberbully-data-formspring\/formspring.csv\")\n# See first few rows\ndf.head()","2f1e9ae1":"# missing values visualisation\nmsn.bar(df);","c01d16dc":"plt.figure(figsize = (25, 8))\nsns.countplot(df['userid'], palette='inferno')\nplt.xticks(rotation = 90);","054cb07f":"plt.figure(figsize = (25, 5))\nsns.countplot(df['asker'][:500],\n             palette = 'husl',\n             hue = df['ans1'][:500]) # For convenience, see only first few rows\nplt.xticks(rotation = -90);","7e0cd691":"## Generate Word Clouds\nbullyDF = df[df['ans1'] == 'Yes']\nWords = [str(i) for i in bullyDF['ques']]\nWordsString = (\" \".join(Words)).lower()\nWordsString = re.sub(r'[^\\w\\s]', '', WordsString)\n\n# mask\nimg_url = 'https:\/\/raw.githubusercontent.com\/Dutta-SD\/Images_Unsplash\/master\/cyber_bullying_images\/I%20AM%20FINE.png'\nfrom urllib.request import urlopen\nfrom PIL import Image\n## make mask\nimg = urlopen(img_url)\nmask = np.array(Image.open(img))\n\n# WORDCLOUD\nfrom wordcloud import WordCloud\nwc = WordCloud(width = 1200, height = 1200,\n              background_color = 'white',\n              mask = mask,\n              contour_width=3).generate(WordsString)\nplt.figure(figsize = (12, 12), facecolor = None) \nplt.imshow(wc); \nplt.axis(\"off\");","a251b32a":"# Posts with actual cyberbullying content\ndf[df['ans1'] == 'Yes'].head()","129b9e40":"df.drop(['post', 'asker'], axis = 1, inplace = True)","cb2f6ccd":"# Threshold value\nuserid_count_threshold = 3\n\n# Get row indexes which we will drop\ndrop_indexes = df[df['userid'].map(df['userid'].value_counts()) <= userid_count_threshold].index\n\ndf_dropped = df.drop(drop_indexes)","626902dc":"def impute_ans_columns(value):\n    if value == 'Yes':\n        return 1\n    return 0\n\nfor col in ['ans1', 'ans2', 'ans3']:\n    df_dropped[col] = df[col].apply(impute_ans_columns)\ndf_dropped.head()","9ffdc788":"def impute_severity_columns(value):\n    '''Value will be a string. We need to convert it to int'''\n    try:\n        return int(value)\n    except ValueError as e:\n        return 0\n\nfor col in ['severity1', 'severity2', 'severity3']:\n    df_dropped[col] = df_dropped[col].apply(impute_severity_columns)","78ed6376":"df_dropped.tail()","1997ec4e":"df_dropped['IsBully'] = (\n    (df_dropped.ans1 * df_dropped.severity1 + df_dropped.ans2 * df_dropped.severity2 + df_dropped.ans3 * df_dropped.severity3) \/ 30) >= 0.2\n\n# Remove uneccessary columns\ndf_2 = df_dropped.drop(['ans1', 'severity1','bully1','ans2','severity2','bully2','ans3','severity3','bully3'], axis = 1)","b294b0ea":"df_2.head()","ce27d765":"for col in ['ques', 'ans']:\n    df_2[col] = df_2[col].str.replace(\"&#039;\", \"'\") # Put back the apostrophe\n\n    df_2[col] = df_2[col].str.replace(\"<br>\", \"\") \n    df_2[col] = df_2[col].str.replace(\"&quot;\", \"\") \n    #df_2[col] = df_2[col].str.replace(\"<3\", \"love\")\n    \ndf_2.head()","8a248a43":"from sklearn.model_selection import train_test_split\nX, y = df_2.iloc[:, :-1], df_2.iloc[:, -1]\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.25, random_state=0, shuffle=True)\n\n# reset indices as indices hold no value for us\nX_train = X_train.reset_index(drop = True)\nX_val = X_val.reset_index(drop = True)\ny_train = y_train.reset_index(drop = True)\ny_val = y_val.reset_index(drop = True)","df80496b":"from collections import Counter\ncounts_userid = dict(Counter(X_train['userid']))\n\nfor key in counts_userid.keys():\n    # Log transform\n    counts_userid[key] = np.log10(counts_userid[key] \/ len(X_train))\n\nX_train['userid'] = X_train['userid'].map(counts_userid)\nX_val['userid'] = X_val['userid'].map(counts_userid)","51d4eb8a":"def tokenize(text):\n    stop_words = stopwords.words(\"english\")\n    lemmatizer = WordNetLemmatizer()\n    \n    # normalize case and remove punctuation\n    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", str(text).lower())\n    \n    # tokenize text\n    tokens = word_tokenize(text)\n    \n    # lemmatize andremove stop words\n    tokens = [lemmatizer.lemmatize(word).lower().strip() for word in tokens if word not in stop_words]\n\n    return tokens","45d495f2":"## getting GloVe word2vectors\n\nfrom gensim.scripts.glove2word2vec import glove2word2vec\n# convert txt to word2vec format for easy access\nglove_input_file = '..\/input\/glove-global-vectors-for-word-representation\/glove.6B.50d.txt'\nword2vec_output_file = 'glove.6B.50d.txt.word2vec'\nglove2word2vec(glove_input_file, word2vec_output_file)\n\nfrom gensim.models import KeyedVectors\n# load the Stanford GloVe model\nfilename = '.\/glove.6B.50d.txt.word2vec'\nmodel = KeyedVectors.load_word2vec_format(filename, binary=False)","57b2bfbf":"def putWordVector(text):\n    '''Returns Word Vectors for passed unclean string'''\n    clean_text = tokenize(text) # list of words\n    wordvecFinal = np.zeros((50,), dtype=np.float32)\n    \n    for word in clean_text:\n        try:\n            word_vec = model[word]\n            wordvecFinal = np.add(word_vec, wordvecFinal)\n        except KeyError as e:\n            continue\n    return wordvecFinal","406c2c82":"def addWordVectors(df, colName):\n    ''' Adds word vectors to the dframe,\n    returns a dataframe.\n    values - pandas Series\n    colName - Name of the column which contains string'''\n    df_new = df[colName].apply(putWordVector)\n    \n    columnNames = [colName + str(i) for i in range(50)]\n\n    df_new = pd.DataFrame(df_new.values.tolist(), columns = columnNames )\n    df_new = pd.concat([df, df_new], axis = 1)\n    df_new = df_new.drop([colName], axis = 1)\n    return df_new ","b47f52fe":"X_train, X_val = addWordVectors(addWordVectors(X_train, 'ques'), 'ans'), addWordVectors(addWordVectors(X_val, 'ques'), 'ans')\nX_train.head()","b7d7029b":"smk = SMOTETomek()\nX_train, y_train = smk.fit_sample(X_train.values, y_train.values)","9479d8cd":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import SelectKBest\n\nskbest = SelectKBest()\nsc = StandardScaler()\n\nX_train = skbest.fit_transform(sc.fit_transform(X_train), y_train)\nX_val = skbest.transform(sc.transform(X_val))","861b5294":"from sklearn.metrics import plot_roc_curve\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier, StackingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\n\nestimators = [\n    ('xgboost_model', XGBClassifier()),\n    ('randomForest_model', RandomForestClassifier()),\n    ('naive_bayes', GaussianNB()),\n    ('SGD', SGDClassifier())\n]\n\nfinalEstimator = LogisticRegression()","52eff7ee":"model = StackingClassifier(estimators=estimators,\n                           final_estimator = finalEstimator,\n                           cv = 5,\n                           n_jobs = -1)","0b28efd7":"predictions = model.fit(X_train, y_train).predict(X_val)","4027d86d":"plot_roc_curve(model, X_val, y_val);","e68f3bd8":"# Insights about the data set\n* userid - userid of a person giving the answer to a post\n* post - The post and it's reply. Separated by Q: and A.\n* ques and ans - The question and answer. The same information is available in post, so we drop post\n* asker - id of the person asking the question.\n* ans #, severity #, bully # - answer by mechanical turk, severity score assigned, bully word\/phrase","32a6bb9f":"## 2. severity columns\n* NaN\/None was associated with a No(0) that is 0 severity mostly.\n\nSo we will replace all NaN\/None with 0.","076ac931":"## userid-probability of occurence","ea30804d":"# Dealing with the imbalance","feddd5fb":"# Cleaning the Data\n<hr>\n\n## Userid\nUser id has some corrupted rows. Some misplaced values.\n* We find that for **counts greater than 3, the rows are not corrupted**.\n* Drop rows which are truly corrupted","5730a31b":"# Train Test Split","03b57faa":"# Text Cleaning","6cf5bb2a":"In this notebook, Let us analyse the data we have and try to come up with a classifier which might help us to deter cyberbullying to some extent.\n\n### WARNING : Strong Language Ahead!","cfa1f1c5":"![An Insight Into Cyber Bullying](https:\/\/raw.githubusercontent.com\/Dutta-SD\/Images_Unsplash\/master\/cyber_bullying_images\/cYBER_bULLYING_bANNER.png)\n\n\n# The Problem of Cyber Bullying\n> With the rise of social media and degraded morality, cyber bullying has become a terrible menace of modern times. Social Media grants us anonymity, \nwhich leads some distorted minds to think they can get away with anything. So they pick out on people, befriend them and then shower them with hate.\n    \n> The problem may seem to some to be another sort of high school bully. But cyber bullying is done on social media where information is permanant. \nSo this becomes a lifelong trauma for victims. Depression and Suicide are common after effects of this nemesis, but not much has been done to curb this menace.","bcdd0e0c":"## Fixing the text data.","c1fbb1b7":"## Inference\nLots of Bully posts are missing values. Few rows are missing in 'ques', 'ans' columns. This might suggest that missing completely at random data. \n\n## 2. Distribution of userid column","18b695a0":"# Imputing Missing Values and Replacing values\n\n## 1. ans columns\nNaN for ans columns indicates No Bully found. We will map every Yes to 1 and every No or NaN to 0","26eeb683":"# Stop saying it's okay when it is not!","45a84e5c":"# Tokenizing and Feature Engineering","f4d8073c":"# Creating the target feature\nSince in this dataset, we have not been given any target feature, we will create our own target feature.\n\n```python\nIsBully = ((ans1 * severity1) + (ans2 * severity2) + (ans3 * severity3)) \/ 30 >= 0.20\n```\n\nWe are taking 0.2 as our threshold","caac2565":"## Inference\nSome rows are corrupted whereby the answers have been filled wrong. We need to remove these(maybe?) or deal with them in some way(based on their counts).\n\n## Visualisation of asker","a5b2a558":"# Last Words:\n\nWe have seen cyberbully content and the damage that they deal. We should all grow more aware and responsible and help each other in times of need\n\n# Sources\n* [Starter Notebook](https:\/\/www.kaggle.com\/kevinlwebb\/bully-data-exploration)","3a0fe906":"# Exploration of the dataset\n\n## 1. BarPlot of missing values","1e590420":"# Modelling","be295e7e":"## Inference\nLots of unique values. Might not be suitable for predicting purposes.\n\n## WordClouds\nLet us see the vocabulary of the bullies","fac0fff1":"# Importing the Data"}}