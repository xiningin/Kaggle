{"cell_type":{"d20390e0":"code","c7931a2b":"code","1f0e895a":"code","fa334a90":"code","e7c2eb9b":"code","c04cd9de":"code","389bf937":"code","87b072e8":"code","1c7ba36a":"code","54ffe890":"code","eebe3397":"code","10242e83":"code","ad5a9227":"code","391a3da5":"code","592508fa":"code","362be54e":"code","a12611ad":"code","3636ed80":"code","ab0362dc":"code","d7f53c82":"code","ef0dc4ac":"code","0dbc525c":"code","ca04b452":"code","d6e42019":"code","f080cfa6":"code","c058a6c7":"code","a7e0de77":"code","bf5eda5d":"code","c19560d2":"code","984b9960":"code","63c1aed2":"code","4a98dcc9":"code","2bcf1de2":"code","3fa6274e":"code","43faedba":"code","66611c25":"code","da2b99b5":"code","daa73c79":"code","031aa10c":"code","5e67140c":"code","f7187647":"code","7d171d87":"code","04e10f34":"code","f0e19bd5":"code","be3eab81":"code","349e1741":"code","aa5be947":"code","f2b2e80b":"code","1410b41d":"code","885a89f7":"code","777bd3b2":"code","5145839a":"code","e77de417":"code","079b6035":"code","c54826bb":"code","5f0610e7":"code","12627d67":"code","7c95ab66":"code","923feb5a":"code","e3b2fdf9":"code","974cfafd":"code","02cc5b36":"code","f145e35a":"code","c44c129e":"code","e10ef910":"code","d7d44b33":"code","6940bcd4":"code","f413fb61":"code","bfbc60bd":"code","cb96a957":"code","0559539f":"code","e8180535":"code","db441956":"code","703e7398":"code","dd200104":"code","efb8f137":"code","0ec07666":"code","3ef62eaa":"code","102ddd37":"code","9f3f71f7":"code","aaae3a0b":"code","37ecf69c":"code","83bdc067":"code","f2cb9324":"code","c3dad0d3":"code","b354fa59":"code","602f5dc9":"code","a49e58d0":"code","6f909634":"code","775612be":"code","3816a1f4":"code","12f0f04b":"code","6be6588a":"code","d6a76a1e":"code","ef2a4a2e":"code","67f9371b":"code","db477ed8":"code","7a8eeb3a":"code","5094bae6":"code","56b2e613":"code","b6eb983f":"code","65725a70":"code","2bf90adf":"code","7c90177d":"code","7c8c5800":"markdown","0b594553":"markdown","ac320464":"markdown","8899fb59":"markdown","bc73b556":"markdown","fb051abc":"markdown","da2bcede":"markdown","bdddb638":"markdown","4277edf4":"markdown","a5e6719b":"markdown","421b4947":"markdown","192e6393":"markdown","689d26f7":"markdown","9bd9f4af":"markdown","581d2e57":"markdown","936ad238":"markdown","6ef3ef2e":"markdown","4dd94756":"markdown","c6cd0816":"markdown","23987fbe":"markdown","a9e90075":"markdown","68d28d4e":"markdown","c09f8c46":"markdown","acf7de96":"markdown","d7ee8bfa":"markdown","ba6ab320":"markdown","3cd69c8b":"markdown","ded3e4b8":"markdown","740096e9":"markdown","483409eb":"markdown","607fe28f":"markdown","bc11dfd4":"markdown","385c66e2":"markdown","fa1f7a14":"markdown","44996e75":"markdown","98bec627":"markdown","7886599e":"markdown","da1ab1bc":"markdown","152b7948":"markdown","3b4cc746":"markdown","0a91af2d":"markdown","62343a30":"markdown","6e265b61":"markdown","61e19344":"markdown","fc6252f0":"markdown","6488cb68":"markdown","82cc7b26":"markdown","f57439f4":"markdown","ef6f4ffc":"markdown","245868a6":"markdown","e7fa9dec":"markdown","612b1664":"markdown","dacfd8c0":"markdown","475f25e2":"markdown","468bfd4f":"markdown","de8b17dc":"markdown","f873aeb4":"markdown","63d41cb2":"markdown","74ed052d":"markdown","5dfe01ab":"markdown","73fbf86b":"markdown","705b3b26":"markdown","c24f45ca":"markdown","7ac3e650":"markdown","8b61b91a":"markdown","1b76b86d":"markdown","495901a8":"markdown","06797b23":"markdown","4850b910":"markdown","75273f68":"markdown","893d2d58":"markdown","bc6d1693":"markdown","a182ec1c":"markdown","eb3cdfd8":"markdown","c5316066":"markdown","4ffea863":"markdown","5a764034":"markdown","129550bd":"markdown","979b98ad":"markdown","4c321810":"markdown","85c63697":"markdown","198ecb34":"markdown","3819a7f6":"markdown","4b89ef4d":"markdown","9272b973":"markdown","745a9f87":"markdown","12e561bc":"markdown","c6ca64cb":"markdown","9c6beac0":"markdown","13a5c2dc":"markdown","931c4fbd":"markdown"},"source":{"d20390e0":"# Please switch on the TPU before running these lines.\n\n!curl https:\/\/raw.githubusercontent.com\/pytorch\/xla\/master\/contrib\/scripts\/env-setup.py -o pytorch-xla-env-setup.py\n!python pytorch-xla-env-setup.py --apt-packages libomp5 libopenblas-dev","c7931a2b":"# Imports required to use TPUs with Pytorch.\n# https:\/\/pytorch.org\/xla\/release\/1.5\/index.html\n\nimport torch_xla\nimport torch_xla.core.xla_model as xm","1f0e895a":"import pandas as pd\nimport numpy as np\nimport os\nimport gc\n\nimport random\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\n# set a seed value\ntorch.manual_seed(555)\n\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import roc_auc_score, accuracy_score\n\nimport transformers\nfrom transformers import BertTokenizer, BertForSequenceClassification \nfrom transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification\nfrom transformers import AdamW\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\nprint(torch.__version__)","fa334a90":"from transformers import BertTokenizer\n\n# Instantiate the Bert tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)","e7c2eb9b":"# How many words (tokens) does Bert have in it's vocab?\n\nlen(tokenizer.vocab)","c04cd9de":"# The vocab is an ordered dictionary - key\/value pairs.\n# This is how to see which tokens are associated with a particular word.\n\nbert_vocab = tokenizer.vocab\n\nprint(bert_vocab['[CLS]'])\nprint(bert_vocab['[SEP]'])\nprint(bert_vocab['[PAD]'])\n\nprint(bert_vocab['hello'])\nprint(bert_vocab['world'])","389bf937":"# Given a token, this is how to see what word is associated with that token.\n\nbert_keys = []\n\nfor token in tokenizer.vocab.keys():\n    \n    bert_keys.append(token)\n    \n    \nprint(bert_keys[101])\nprint(bert_keys[102])\nprint(bert_keys[0])\n\nprint(bert_keys[7592])\nprint(bert_keys[2088])","87b072e8":"# Instantiate a Bert model\n\n# model = BertForSequenceClassification.from_pretrained(\n#               'bert-base-multilingual-uncased', \n#               num_labels = 3, # The number of output labels. 2 for binary classification.  \n#               output_attentions = False,\n#               output_hidden_states = False\n#               )\n\n\n\n# outputs = model(input_ids=b_input_ids, \n#               token_type_ids=b_token_type_ids, \n#               attention_mask=b_input_mask,\n#               labels=b_labels)\n\n\n\n# These are the model inputs:\n\n#   input_ids (type: torch tensor)\n#   token_type_ids (type: torch tensor)\n#   attention_mask (type: torch tensor)\n#   labels (type: torch tensor)\n","1c7ba36a":"# 1. input_ids\n# -------------\n\n# The input_ids are the sentence or sentences represented as tokens. \n# There are a few BERT special tokens that one needs to take note of:\n\n# [CLS] - Classifier token, value: 101\n# [SEP] - Separator token, value: 102\n# [PAD] - Padding token, value: 0\n\n# Bert expects every row in the input_ids to have the special tokens included as follows:\n\n# For one sentence as input:\n# [CLS] ...word tokens... [SEP]\n\n# For two sentences as input:\n# [CLS] ...sentence1 tokens... [SEP]..sentence2 tokens... [SEP]\n\n\n# This is an example of an encoded sentence with padding (token value: 0) added. \n# We add padding (or truncate sentences) because each row in an input batch needs \n# to have the same length. The max allowed length is 512.\n\n# [101, 7592, 2045, 1012,  102,    0,    0,    0,    0,    0]\n\n\n\n# 2. token_type_ids\n# ------------------\n\n# token_type_ids are used when there are two sentences that need to be part of the input. \n# The token type ids indicate which tokens are part of sentence1 and which are part of sentence2.\n\n# This is an example:\n\n# [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n\n# The first set of zeros identify all tokens that are part of the first sentence. \n# The ones identify all tokens that are part of the second sentence. \n# The zeros on the right are the padding.\n\n\n\n# 3. attention_mask\n# ------------------\n\n# The attention mask has the same length as the input_ids. \n# It tells the model which tokens in the input_ids are words and which are padding. \n# 1 indicates a word (or special token) and 0 indicates padding.\n\n# For example:\n# Tokens: [101, 7592, 2045, 1012,  102,    0,    0,    0,    0,    0]\n# Attention mask: [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n\n\n\n# 4. labels\n# ----------\n\n# The label (target) for each row in the input_ids. \n# The labels are integers representing each target class e.g. 1, 2, 3 etc.\n\n# For example if we have three target classes (0, 1 and 2) then\n# the labels could look like this for a batch size of 8:\n\n# [0, 2, 0, 1, 2, 0, 3, 1]\n","54ffe890":"# Batch size is 8.\n\n\n# outputs = model(input_ids=b_input_ids, \n#               token_type_ids=b_token_type_ids, \n#               attention_mask=b_input_mask,\n#               labels=b_labels)\n\n\n# outputs\n# ........\n\n# The output is a tuple: (loss, preds)\n# Type: torch tensor\n\n\n# (tensor(1.1095, device='xla:1', grad_fn=<NllLossBackward>),\n#  tensor([[ 0.4005, -0.0222,  0.1946],\n#          [ 0.1117, -0.1652,  0.0208],\n#          [ 0.3866, -0.0635,  0.1842],\n#          [ 0.0423, -0.1887,  0.0691],\n#          [ 0.2817, -0.1092,  0.1111],\n#          [ 0.2353, -0.1156,  0.0977],\n#          [ 0.1253, -0.1821,  0.0402],\n#          [ 0.0879, -0.1970,  0.0718]], device='xla:1', grad_fn=<AddmmBackward>))\n\n\n\n# outputs[0]\n# ..........\n\n# tensor(1.1095, device='xla:1', grad_fn=<NllLossBackward>)\n\n\n# outputs[0].item()\n# ..................\n\n# 1.109534740447998\n\n\n# outputs[1]\n# ..........\n\n# tensor([[ 0.4005, -0.0222,  0.1946],\n#         [ 0.1117, -0.1652,  0.0208],\n#         [ 0.3866, -0.0635,  0.1842],\n#         [ 0.0423, -0.1887,  0.0691],\n#         [ 0.2817, -0.1092,  0.1111],\n#         [ 0.2353, -0.1156,  0.0977],\n#         [ 0.1253, -0.1821,  0.0402],\n#         [ 0.0879, -0.1970,  0.0718]], device='xla:1', grad_fn=<AddmmBackward>)\n","eebe3397":"# Batch size is 8.\n\n\n# preds = model(input_ids=b_input_ids, \n#               token_type_ids=b_token_type_ids, \n#               attention_mask=b_input_mask,\n#               )\n\n\n# preds\n# ------\n\n# The output is a tuple with only one value: (preds,)\n# Type: torch tensor\n\n# (tensor([[ 0.4005, -0.0222,  0.1946],\n#          [ 0.1117, -0.1652,  0.0208],\n#          [ 0.3866, -0.0635,  0.1842],\n#          [ 0.0423, -0.1887,  0.0691],\n#          [ 0.2817, -0.1092,  0.1111],\n#          [ 0.2353, -0.1156,  0.0977],\n#          [ 0.1253, -0.1821,  0.0402],\n#          [ 0.0879, -0.1970,  0.0718]], device='xla:1', grad_fn=<AddmmBackward>),)\n\n\n\n# preds[0]\n# --------\n\n# tensor([[ 0.4005, -0.0222,  0.1946],\n#         [ 0.1117, -0.1652,  0.0208],\n#         [ 0.3866, -0.0635,  0.1842],\n#         [ 0.0423, -0.1887,  0.0691],\n#         [ 0.2817, -0.1092,  0.1111],\n#         [ 0.2353, -0.1156,  0.0977],\n#         [ 0.1253, -0.1821,  0.0402],\n#         [ 0.0879, -0.1970,  0.0718]], device='xla:1', grad_fn=<AddmmBackward>)\n","10242e83":"MAX_LEN = 10 # This value could be set as 256, 512 etc.\n\nsentence1 = 'Hello there.'\n\nencoded_dict = tokenizer.encode_plus(\n            sentence1,                      # Sentence to encode.\n            add_special_tokens = True,      # Add '[CLS]' and '[SEP]'\n            max_length = MAX_LEN,           # Pad or truncate.\n            pad_to_max_length = True,\n            return_attention_mask = True,   # Construct attn. masks.\n            return_tensors = 'pt',          # Return pytorch tensors.\n           )\n\n\nencoded_dict","ad5a9227":"# These have already been converted to torch tensors.\ninput_ids = encoded_dict['input_ids'][0]\ntoken_type_ids = encoded_dict['token_type_ids'][0]\natt_mask = encoded_dict['attention_mask'][0]\n\nprint(input_ids)\nprint(token_type_ids)\nprint(att_mask)","391a3da5":"MAX_LEN = 15\n\nsentence1 = 'Hello there.'\nsentence2 = 'How are you?'\n\nencoded_dict = tokenizer.encode_plus(\n            sentence1, sentence2,           # Sentences to encode.\n            add_special_tokens = True,      # Add '[CLS]' and '[SEP]'\n            max_length = MAX_LEN,           # Pad or truncate.\n            pad_to_max_length = True,\n            return_attention_mask = True,   # Construct attn. masks.\n            return_tensors = 'pt',          # Return pytorch tensors.\n           )\n\n\nencoded_dict","592508fa":"input_ids = encoded_dict['input_ids'][0]\ntoken_type_ids = encoded_dict['token_type_ids'][0]\natt_mask = encoded_dict['attention_mask'][0]\n\n# These are torch tensors.\nprint(input_ids)\nprint(token_type_ids)\nprint(att_mask)","362be54e":"# https:\/\/huggingface.co\/transformers\/main_classes\/tokenizer.html\n# skip_special_tokens \u2013 if this is set to True, then special tokens will be replaced.\n\n# Note that do_lower_case=True in the tokenizer.\n# This is why all text is lower case.\n\na = tokenizer.decode(input_ids,\n                skip_special_tokens=False)\n\nb = tokenizer.decode(input_ids,\n                skip_special_tokens=True)\n\nprint(a)\nprint(b)","a12611ad":"from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification\n\nMODEL_TYPE = 'xlm-roberta-base'\n\ntokenizer = XLMRobertaTokenizer.from_pretrained(MODEL_TYPE)","3636ed80":"# Check the vocab size\n\ntokenizer.vocab_size","ab0362dc":"# What are the special tokens\n\ntokenizer.special_tokens_map","d7f53c82":"print('bos_token_id <s>:', tokenizer.bos_token_id)\nprint('eos_token_id <\/s>:', tokenizer.eos_token_id)\nprint('sep_token_id <\/s>:', tokenizer.sep_token_id)\nprint('pad_token_id <pad>:', tokenizer.pad_token_id)","ef0dc4ac":"# from transformers import XLMRobertaForSequenceClassification\n\n# MODEL_TYPE = 'xlm-roberta-base'\n\n# model = XLMRobertaForSequenceClassification.from_pretrained(\n#                  MODEL_TYPE, \n#                  num_labels = 3 # The number of output labels. 2 for binary classification.\n#               )\n\n\n# outputs = model(input_ids=b_input_ids, \n#                 attention_mask=b_input_mask, \n#                 labels=b_labels)\n\n\n\n# These are the model inputs:\n#   input_ids (type: torch tensor)\n#   attention_mask (type: torch tensor)\n#   labels (type: torch tensor)","0dbc525c":"# 1. input_ids\n# -------------\n\n# The input_ids are the sentence or sentences represented as tokens. \n# These are special tokens:\n\n# bos_token_id <s>: 0\n# eos_token_id <\/s>: 2\n# sep_token_id <\/s>: 2\n# pad_token_id <pad>: 1\n    \n    \n# XLM-RoBERTa expects every row in the input_ids to have the special tokens included as follows:\n\n# For one sentence as input:\n# <s> ...word tokens... <\/s>\n\n# For two sentences as input:<br>\n# <s> ...sentence1 tokens... <\/s><\/s>..sentence2 tokens... <\/s>\n\n\n# This is an example of an encoded sentence with padding (pad token value: 1). \n\n# [0, 35378, 2685, 5, 2, 1, 1, 1, 1, 1]\n\n\n\n\n# 2. token_type_ids\n# ------------------\n\n# XLM-RoBERTa does not use token_type_ids like BERT does.\n# Therefore, there's no need to create token_type_ids.\n\n\n\n# 3. attention_mask\n# ------------------\n\n# The attention mask has the same length as the input_ids. \n# It tells the model which tokens in the input_ids are works and which are padding. \n# 1 indicates a word (or special token) and 0 indicates padding.\n\n# For example, the attention mask for the above input_ids is as follows:\n# [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n\n\n# 3. labels\n# ----------\n\n# The label (target) for each row in the input_ids. \n# The labels are integers representing each target class e.g. 1, 2, 3 etc.\n\n# For example if we have three target classes (0, 1 and 2) then\n# the labels could look like this for a batch size of 8:\n\n# [0, 2, 0, 1, 2, 0, 3, 1]","ca04b452":"MAX_LEN = 10 # This value could be set as 256, 512 etc.\n\nsentence1 = 'Hello there.'\n\nencoded_dict = tokenizer.encode_plus(\n            sentence1,                \n            add_special_tokens = True,\n            max_length = MAX_LEN,     \n            pad_to_max_length = True,\n            return_attention_mask = True,  \n            return_tensors = 'pt' # return pytorch tensors\n       )\n\n\nencoded_dict","d6e42019":"# These have already been converted to torch tensors.\ninput_ids = encoded_dict['input_ids'][0]\natt_mask = encoded_dict['attention_mask'][0]\n\nprint(input_ids)\nprint(att_mask)","f080cfa6":"MAX_LEN = 15\n\nsentence1 = 'Hello there.'\nsentence2 = 'How are you?'\n\nencoded_dict = tokenizer.encode_plus(\n            sentence1, sentence2,      \n            add_special_tokens = True,\n            max_length = MAX_LEN,     \n            pad_to_max_length = True,\n            return_attention_mask = True,   \n            return_tensors = 'pt' # return pytorch tensors\n       )\n\n\nencoded_dict","c058a6c7":"input_ids = encoded_dict['input_ids'][0]\natt_mask = encoded_dict['attention_mask'][0]\n\n# These are torch tensors.\nprint(input_ids)\nprint(att_mask)","a7e0de77":"# input_ids from above\n\ninput_ids = encoded_dict['input_ids'][0]\n\nprint(input_ids)","bf5eda5d":"# https:\/\/huggingface.co\/transformers\/main_classes\/tokenizer.html\n# skip_special_tokens \u2013 if set to True, will replace special tokens.\n\na = tokenizer.decode(input_ids,\n                skip_special_tokens=False)\n\nb = tokenizer.decode(input_ids,\n                skip_special_tokens=True)\n\n\n\nprint(a)\nprint(b)","c19560d2":"MAX_LEN = 15 # This value could be set as 256, 512 etc.\n\nsentence1 = 'Hello there. How are you? Have a nice day. This is a test?'\n\n\nencoded_dict = tokenizer.encode_plus(\n            sentence1,                \n            max_length = MAX_LEN,\n            stride=0,\n            pad_to_max_length = True,\n            return_overflowing_tokens=True,\n       )\n\n\nencoded_dict","984b9960":"MAX_LEN = 15 # This value could be set as 256, 512 etc.\n\nsentence1 = 'Hello there. How are you? Have a nice day. This is a test?'\n\n\nencoded_dict = tokenizer.encode_plus(\n            sentence1,                \n            max_length = MAX_LEN,\n            stride=3,\n            pad_to_max_length = True,\n            return_overflowing_tokens=True,\n       )\n\n\nencoded_dict","63c1aed2":"# Here you can see the overlap.\n\nprint(encoded_dict['input_ids'])\nprint(encoded_dict['overflowing_tokens'])","4a98dcc9":"os.listdir('..\/input\/contradictory-my-dear-watson')","2bcf1de2":"# Load the training data.\n\npath = '..\/input\/contradictory-my-dear-watson\/train.csv'\ndf_train = pd.read_csv(path)\n\nprint(df_train.shape)\n\ndf_train.head()","3fa6274e":"# Load the test data.\n\npath = '..\/input\/contradictory-my-dear-watson\/test.csv'\ndf_test = pd.read_csv(path)\n\nprint(df_test.shape)\n\ndf_test.head()","43faedba":"from sklearn.model_selection import KFold, StratifiedKFold\n\n# shuffle\ndf = shuffle(df_train)\n\n# initialize kfold\nkf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1024)\n\n# for stratification\ny = df['label']\n\n# Note:\n# Each fold is a tuple ([train_index_values], [val_index_values])\n# fold_0, fold_1, fold_2, fold_3, fold_5 = kf.split(df, y)\n\n# Put the folds into a list. This is a list of tuples.\nfold_list = list(kf.split(df, y))\n\ntrain_df_list = []\nval_df_list = []\n\nfor i, fold in enumerate(fold_list):\n\n    # map the train and val index values to dataframe rows\n    df_train = df[df.index.isin(fold[0])]\n    df_val = df[df.index.isin(fold[1])]\n    \n    train_df_list.append(df_train)\n    val_df_list.append(df_val)\n    \n    \n\nprint(len(train_df_list))\nprint(len(val_df_list))","66611c25":"# Display one train fold\n\ndf_train = train_df_list[0]\n\ndf_train.head()","da2b99b5":"# Display one val fold\n\ndf_val = val_df_list[0]\n\ndf_val.head()","daa73c79":"MODEL_TYPE = 'bert-base-multilingual-uncased'\n\nNUM_FOLDS = 5\n\n# Saving 5 TPU models will exceed the 4.9GB disk space.\n# Therefore, will will only train on 3 folds.\nNUM_FOLDS_TO_TRAIN = 3 \n\nL_RATE = 1e-5\nMAX_LEN = 256\nNUM_EPOCHS = 3\nBATCH_SIZE = 32\nNUM_CORES = os.cpu_count()\n\nNUM_CORES","031aa10c":"# For GPU\n\n#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n#print(device)","5e67140c":"# For TPU\n\ndevice = xm.xla_device()\n\nprint(device)","f7187647":"from transformers import BertTokenizer\n\n# Load the BERT tokenizer.\nprint('Loading BERT tokenizer...')\ntokenizer = BertTokenizer.from_pretrained(MODEL_TYPE, do_lower_case=True)","7d171d87":"\nclass CompDataset(Dataset):\n\n    def __init__(self, df):\n        self.df_data = df\n\n\n\n    def __getitem__(self, index):\n\n        # get the sentence from the dataframe\n        sentence1 = self.df_data.loc[index, 'premise']\n        sentence2 = self.df_data.loc[index, 'hypothesis']\n\n        # Process the sentence\n        # ---------------------\n\n        encoded_dict = tokenizer.encode_plus(\n                    sentence1, sentence2,           # Sentences to encode.\n                    add_special_tokens = True,      # Add '[CLS]' and '[SEP]'\n                    max_length = MAX_LEN,           # Pad or truncate all sentences.\n                    pad_to_max_length = True,\n                    return_attention_mask = True,   # Construct attn. masks.\n                    return_tensors = 'pt',          # Return pytorch tensors.\n               )  \n        \n        # These are torch tensors already.\n        padded_token_list = encoded_dict['input_ids'][0]\n        att_mask = encoded_dict['attention_mask'][0]\n        token_type_ids = encoded_dict['token_type_ids'][0]\n        \n        # Convert the target to a torch tensor\n        target = torch.tensor(self.df_data.loc[index, 'label'])\n\n        sample = (padded_token_list, att_mask, token_type_ids, target)\n\n\n        return sample\n\n\n    def __len__(self):\n        return len(self.df_data)\n    \n    \n    \n    \n    \n\nclass TestDataset(Dataset):\n\n    def __init__(self, df):\n        self.df_data = df\n\n\n\n    def __getitem__(self, index):\n\n        # get the sentence from the dataframe\n        sentence1 = self.df_data.loc[index, 'premise']\n        sentence2 = self.df_data.loc[index, 'hypothesis']\n\n        # Process the sentence\n        # ---------------------\n\n        encoded_dict = tokenizer.encode_plus(\n                    sentence1, sentence2,           # Sentence to encode.\n                    add_special_tokens = True,      # Add '[CLS]' and '[SEP]'\n                    max_length = MAX_LEN,           # Pad or truncate all sentences.\n                    pad_to_max_length = True,\n                    return_attention_mask = True,   # Construct attn. masks.\n                    return_tensors = 'pt',          # Return pytorch tensors.\n               )\n        \n        # These are torch tensors already.\n        padded_token_list = encoded_dict['input_ids'][0]\n        att_mask = encoded_dict['attention_mask'][0]\n        token_type_ids = encoded_dict['token_type_ids'][0]\n               \n\n        sample = (padded_token_list, att_mask, token_type_ids)\n\n\n        return sample\n\n\n    def __len__(self):\n        return len(self.df_data)\n\n","04e10f34":"df_train = df_train.reset_index(drop=True)\ndf_val = df_val.reset_index(drop=True)","f0e19bd5":"train_data = CompDataset(df_train)\nval_data = CompDataset(df_val)\ntest_data = TestDataset(df_test)\n\n\n\ntrain_dataloader = torch.utils.data.DataLoader(train_data,\n                                        batch_size=BATCH_SIZE,\n                                        shuffle=True,\n                                       num_workers=NUM_CORES)\n\nval_dataloader = torch.utils.data.DataLoader(val_data,\n                                        batch_size=BATCH_SIZE,\n                                        shuffle=True,\n                                       num_workers=NUM_CORES)\n\ntest_dataloader = torch.utils.data.DataLoader(test_data,\n                                        batch_size=BATCH_SIZE,\n                                        shuffle=False,\n                                       num_workers=NUM_CORES)\n\n\n\nprint(len(train_dataloader))\nprint(len(val_dataloader))\nprint(len(test_dataloader))","be3eab81":"# Get one train batch\n\npadded_token_list, att_mask, token_type_ids, target = next(iter(train_dataloader))\n\nprint(padded_token_list.shape)\nprint(att_mask.shape)\nprint(token_type_ids.shape)\nprint(target.shape)","349e1741":"# Get one val batch\n\npadded_token_list, att_mask, token_type_ids, target = next(iter(val_dataloader))\n\nprint(padded_token_list.shape)\nprint(att_mask.shape)\nprint(token_type_ids.shape)\nprint(target.shape)","aa5be947":"# Get one test batch\n\npadded_token_list, att_mask, token_type_ids = next(iter(test_dataloader))\n\nprint(padded_token_list.shape)\nprint(att_mask.shape)\nprint(token_type_ids.shape)","f2b2e80b":"# Load BertForSequenceClassification, the pretrained BERT model with a single \n# linear classification layer on top. \nmodel = BertForSequenceClassification.from_pretrained(\n    MODEL_TYPE, \n    num_labels = 3, \n    output_attentions = False,\n    output_hidden_states = False)\n\n# Send the model to the device.\nmodel.to(device)","1410b41d":"# Get one train batch\n\ntrain_dataloader = torch.utils.data.DataLoader(train_data,\n                                        batch_size=8,\n                                        shuffle=True,\n                                       num_workers=NUM_CORES)\n\nbatch = next(iter(train_dataloader))\n\nb_input_ids = batch[0].to(device)\nb_input_mask = batch[1].to(device)\nb_token_type_ids = batch[2].to(device)\nb_labels = batch[3].to(device)","885a89f7":"outputs = model(b_input_ids, \n                token_type_ids=b_token_type_ids, \n                attention_mask=b_input_mask,\n                labels=b_labels)","777bd3b2":"outputs","5145839a":"# The output is a tuple: (loss, preds)\n\nlen(outputs)","e77de417":"# This is the loss.\n\noutputs[0]","079b6035":"# These are the predictions.\n\noutputs[1]","c54826bb":"preds = outputs[1].detach().cpu().numpy()\n\ny_true = b_labels.detach().cpu().numpy()\ny_pred = np.argmax(preds, axis=1)\n\ny_pred","5f0610e7":"# This is the accuracy without any fine tuning.\n\nval_acc = accuracy_score(y_true, y_pred)\n\nval_acc","12627d67":"# The loss and preds are Torch tensors\n\nprint(type(outputs[0]))\nprint(type(outputs[1]))","7c95ab66":"# For info: \n# Think in terms of fold models.\n# Fold model 0, for example, is only training on fold 0 in each epoch.\n# The same applies to the other fold models.","923feb5a":"%%time\n\n\n# Set a seed value.\nseed_val = 1024\n\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)\n\n\n\n# Store the accuracy scores for each fold model in this list.\n# [[model_0 scores], [model_1 scores], [model_2 scores], [model_3 scores], [model_4 scores]]\n# [[ecpoch 1, epoch 2, ...], [ecpoch 1, epoch 2, ...], [ecpoch 1, epoch 2, ...], [ecpoch 1, epoch 2, ...], [ecpoch 1, epoch 2, ...]]\n\n# Create a list of lists to store the val acc results.\n# The number of items in this list will correspond to\n# the number of folds that the model is being trained on.\nfold_val_acc_list = []\nfor i in range(0, NUM_FOLDS):\n    \n    # append an empty list\n    fold_val_acc_list.append([])\n    \n    \n    \n    \n\n# For each epoch...\nfor epoch in range(0, NUM_EPOCHS):\n    \n    print(\"\\nNum folds used for training:\", NUM_FOLDS_TO_TRAIN)\n    print('======== Epoch {:} \/ {:} ========'.format(epoch + 1, NUM_EPOCHS))\n    \n    # Get the number of folds\n    num_folds = len(train_df_list)\n\n    # For this epoch, store the val acc scores for each fold in this list.\n    # We will use this list to calculate the cv at the end of the epoch.\n    epoch_acc_scores_list = []\n    \n    # For each fold...\n    for fold_index in range(0, NUM_FOLDS_TO_TRAIN):\n        \n        print('\\n== Fold Model', fold_index)\n        \n        \n        # .........................\n        # Load the fold model\n        # .........................\n        \n        if epoch == 0:\n            \n            # define the model\n            model = BertForSequenceClassification.from_pretrained(\n            MODEL_TYPE, \n            num_labels = 3,       \n            output_attentions = False, \n            output_hidden_states = False,\n            )\n            \n            # Send the model to the device.\n            model.to(device)\n            \n            optimizer = AdamW(model.parameters(),\n              lr = L_RATE, \n              eps = 1e-8\n            )\n            \n        else:\n        \n            # Get the fold model\n            path_model = 'model_' + str(fold_index) + '.bin'\n            model.load_state_dict(torch.load(path_model))\n\n            # Send the model to the device.\n            model.to(device)\n        \n        \n        \n        # .....................................\n        # Set up the train and val dataloaders\n        # .....................................\n        \n        \n        # Intialize the fold dataframes\n        df_train = train_df_list[fold_index]\n        df_val = val_df_list[fold_index]\n        \n        # Reset the indices or the dataloader won't work.\n        df_train = df_train.reset_index(drop=True)\n        df_val = df_val.reset_index(drop=True)\n    \n        # Create the dataloaders\n        train_data = CompDataset(df_train)\n        val_data = CompDataset(df_val)\n\n        train_dataloader = torch.utils.data.DataLoader(train_data,\n                                                batch_size=BATCH_SIZE,\n                                                shuffle=True,\n                                               num_workers=NUM_CORES)\n\n        val_dataloader = torch.utils.data.DataLoader(val_data,\n                                                batch_size=BATCH_SIZE,\n                                                shuffle=True,\n                                               num_workers=NUM_CORES)\n    \n    \n    \n\n       \n\n        # ========================================\n        #               Training\n        # ========================================\n        \n        stacked_val_labels = []\n        targets_list = []\n\n        print('Training...')\n\n        # put the model into train mode\n        model.train()\n\n        # This turns gradient calculations on and off.\n        torch.set_grad_enabled(True)\n\n\n        # Reset the total loss for this epoch.\n        total_train_loss = 0\n\n        for i, batch in enumerate(train_dataloader):\n\n            train_status = 'Batch ' + str(i+1) + ' of ' + str(len(train_dataloader))\n\n            print(train_status, end='\\r')\n\n\n            b_input_ids = batch[0].to(device)\n            b_input_mask = batch[1].to(device)\n            b_token_type_ids = batch[2].to(device)\n            b_labels = batch[3].to(device)\n\n            model.zero_grad()        \n\n\n            outputs = model(b_input_ids, \n                        token_type_ids=b_token_type_ids, \n                        attention_mask=b_input_mask,\n                        labels=b_labels)\n\n            # Get the loss from the outputs tuple: (loss, logits)\n            loss = outputs[0]\n\n            # Convert the loss from a torch tensor to a number.\n            # Calculate the total loss.\n            total_train_loss = total_train_loss + loss.item()\n\n            # Zero the gradients\n            optimizer.zero_grad()\n\n            # Perform a backward pass to calculate the gradients.\n            loss.backward()\n            \n            # Clip the norm of the gradients to 1.0.\n            # This is to help prevent the \"exploding gradients\" problem.\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n            # Use the optimizer to update Weights\n            \n            # Optimizer for GPU\n            # optimizer.step() \n            \n            # Optimizer for TPU\n            # https:\/\/pytorch.org\/xla\/\n            xm.optimizer_step(optimizer, barrier=True)\n            \n           \n\n\n        print('Train loss:' ,total_train_loss)\n\n\n        # ========================================\n        #               Validation\n        # ========================================\n\n        print('\\nValidation...')\n\n        # Put the model in evaluation mode.\n        model.eval()\n\n        # Turn off the gradient calculations.\n        # This tells the model not to compute or store gradients.\n        # This step saves memory and speeds up validation.\n        torch.set_grad_enabled(False)\n\n\n        # Reset the total loss for this epoch.\n        total_val_loss = 0\n\n\n        for j, val_batch in enumerate(val_dataloader):\n\n            val_status = 'Batch ' + str(j+1) + ' of ' + str(len(val_dataloader))\n\n            print(val_status, end='\\r')\n\n            b_input_ids = val_batch[0].to(device)\n            b_input_mask = val_batch[1].to(device)\n            b_token_type_ids = val_batch[2].to(device)\n            b_labels = val_batch[3].to(device)      \n\n\n            outputs = model(b_input_ids, \n                    token_type_ids=b_token_type_ids, \n                    attention_mask=b_input_mask, \n                    labels=b_labels)\n\n            # Get the loss from the outputs tuple: (loss, logits)\n            loss = outputs[0]\n\n            # Convert the loss from a torch tensor to a number.\n            # Calculate the total loss.\n            total_val_loss = total_val_loss + loss.item()\n\n            # Get the preds\n            preds = outputs[1]\n\n\n            # Move preds to the CPU\n            val_preds = preds.detach().cpu().numpy()\n\n            # Move the labels to the cpu\n            targets_np = b_labels.to('cpu').numpy()\n\n            # Append the labels to a numpy list\n            targets_list.extend(targets_np)\n\n            if j == 0:  # first batch\n                stacked_val_preds = val_preds\n\n            else:\n                stacked_val_preds = np.vstack((stacked_val_preds, val_preds))\n                \n                \n                \n        # .........................................\n        # Calculate the val accuracy for this fold\n        # .........................................      \n\n\n        # Calculate the validation accuracy\n        y_true = targets_list\n        y_pred = np.argmax(stacked_val_preds, axis=1)\n\n        val_acc = accuracy_score(y_true, y_pred)\n        \n        \n        epoch_acc_scores_list.append(val_acc)\n\n\n        print('Val loss:' ,total_val_loss)\n        print('Val acc: ', val_acc)\n        \n        \n        # .........................\n        # Save the best model\n        # .........................\n        \n        if epoch == 0:\n            \n            # Save the Model\n            model_name = 'model_' + str(fold_index) + '.bin'\n            torch.save(model.state_dict(), model_name)\n            print('Saved model as ', model_name)\n            \n        if epoch != 0:\n        \n            val_acc_list = fold_val_acc_list[fold_index]\n            best_val_acc = max(val_acc_list)\n            \n            if val_acc > best_val_acc:\n                # save the model\n                model_name = 'model_' + str(fold_index) + '.bin'\n                torch.save(model.state_dict(), model_name)\n                print('Val acc improved. Saved model as ', model_name)\n                \n                \n                \n        # .....................................\n        # Save the val_acc for this fold model\n        # .....................................\n        \n        # Note: Don't do this before the above 'Save Model' code or \n        # the save model code won't work. This is because the best_val_acc will\n        # become current val accuracy.\n                \n        # fold_val_acc_list is a list of lists.\n        # Each fold model has it's own list corresponding to the fold index.\n        # Here we choose a list corresponding to the fold number and append the acc score to that list.\n        fold_val_acc_list[fold_index].append(val_acc)\n        \n            \n\n        # Use the garbage collector to save memory.\n        gc.collect()\n        \n        \n    # .............................................................\n    # Calculate the CV accuracy score over all folds in this epoch\n    # .............................................................   \n        \n        \n    # Print the average val accuracy for all 5 folds\n    cv_acc = sum(epoch_acc_scores_list)\/NUM_FOLDS_TO_TRAIN\n    print(\"\\nCV Acc:\", cv_acc)\n    \n","e3b2fdf9":"# Check that the models have been saved\n\n!ls","974cfafd":"# Display the accuracy scores for each fold model.\n# For info: \n# Fold model 0 is only training on fold 0 in each epoch.\n# The same applies to the other fold models.\n\nfold_val_acc_list","02cc5b36":"# Create the dataloader\n\ntest_data = TestDataset(df_test)\n\n\ntest_dataloader = torch.utils.data.DataLoader(test_data,\n                                        batch_size=BATCH_SIZE,\n                                        shuffle=False,\n                                       num_workers=NUM_CORES)\n\nprint(len(test_dataloader))","f145e35a":"# ========================================\n#               Test Set\n# ========================================\n\nprint('\\nTest Set...')\n\nmodel_preds_list = []\n\nprint('Total batches:', len(test_dataloader))\n\nfor fold_index in range(0, NUM_FOLDS_TO_TRAIN):\n    \n    print('\\nFold Model', fold_index)\n\n    # Load the fold model\n    path_model = 'model_' + str(fold_index) + '.bin'\n    model.load_state_dict(torch.load(path_model))\n\n    # Send the model to the device.\n    model.to(device)\n\n\n    stacked_val_labels = []\n    \n\n    # Put the model in evaluation mode.\n    model.eval()\n\n    # Turn off the gradient calculations.\n    # This tells the model not to compute or store gradients.\n    # This step saves memory and speeds up validation.\n    torch.set_grad_enabled(False)\n\n\n    # Reset the total loss for this epoch.\n    total_val_loss = 0\n\n    for j, h_batch in enumerate(test_dataloader):\n\n        inference_status = 'Batch ' + str(j + 1)\n\n        print(inference_status, end='\\r')\n\n        b_input_ids = h_batch[0].to(device)\n        b_input_mask = h_batch[1].to(device)\n        b_token_type_ids = h_batch[2].to(device)     \n\n\n        outputs = model(b_input_ids, \n                token_type_ids=b_token_type_ids, \n                attention_mask=b_input_mask)\n\n\n        # Get the preds\n        preds = outputs[0]\n\n\n        # Move preds to the CPU\n        val_preds = preds.detach().cpu().numpy()\n        \n        \n        # Stack the predictions.\n\n        if j == 0:  # first batch\n            stacked_val_preds = val_preds\n\n        else:\n            stacked_val_preds = np.vstack((stacked_val_preds, val_preds))\n\n        \n    model_preds_list.append(stacked_val_preds)\n    \n            \nprint('\\nPrediction complete.')        ","c44c129e":"model_preds_list","e10ef910":"# Sum the predictions of all fold models\nfor i, item in enumerate(model_preds_list):\n    \n    if i == 0:\n        \n        preds = item\n        \n    else:\n    \n        # Sum the matrices\n        preds = item + preds\n\n        \n# Average the predictions\navg_preds = preds\/(len(model_preds_list))\n\n\ntest_preds = np.argmax(avg_preds, axis=1)","d7d44b33":"test_preds","6940bcd4":"# Load the sample submission.\n# The row order in the test set and the sample submission is the same.\n\npath = '..\/input\/contradictory-my-dear-watson\/sample_submission.csv'\n\ndf_sample = pd.read_csv(path)\n\nprint(df_sample.shape)\n\ndf_sample.head()","f413fb61":"# Assign the preds to the prediction column\n\ndf_sample['prediction'] = test_preds\n\ndf_sample.head()","bfbc60bd":"# Create a submission csv file\ndf_sample.to_csv('submission.csv', index=False)","cb96a957":"# Check that the fold models have been saved.\n\n!ls","0559539f":"# Check the distribution of the predicted classes.\n\ndf_sample['prediction'].value_counts()","e8180535":"MODEL_TYPE = 'xlm-roberta-base'\n\n\nL_RATE = 1e-5\nMAX_LEN = 256\n\nNUM_EPOCHS = 3\nBATCH_SIZE = 32\nNUM_CORES = os.cpu_count()\n\nNUM_CORES","db441956":"#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n#print(device)","703e7398":"# Tell PyTorch to use the TPU.    \ndevice = xm.xla_device()\n\nprint(device)","dd200104":"df_train = train_df_list[0]\n\ndf_train.head()","efb8f137":"df_val = val_df_list[0]\n\ndf_val.head()","0ec07666":"from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification\n\n# xlm-roberta-large\nprint('Loading XLMRoberta tokenizer...')\ntokenizer = XLMRobertaTokenizer.from_pretrained(MODEL_TYPE)","3ef62eaa":"df_train = df_train.reset_index(drop=True)\ndf_val = df_val.reset_index(drop=True)","102ddd37":"class CompDataset(Dataset):\n\n    def __init__(self, df):\n        self.df_data = df\n\n\n\n    def __getitem__(self, index):\n\n        # get the sentence from the dataframe\n        sentence1 = self.df_data.loc[index, 'premise']\n        sentence2 = self.df_data.loc[index, 'hypothesis']\n\n        # Process the sentence\n        # ---------------------\n\n        encoded_dict = tokenizer.encode_plus(\n                    sentence1, sentence2,           # Sentences to encode.\n                    add_special_tokens = True,      # Add the special tokens.\n                    max_length = MAX_LEN,           # Pad & truncate all sentences.\n                    pad_to_max_length = True,\n                    return_attention_mask = True,   # Construct attn. masks.\n                    return_tensors = 'pt',          # Return pytorch tensors.\n               )\n        \n        # These are torch tensors.\n        padded_token_list = encoded_dict['input_ids'][0]\n        att_mask = encoded_dict['attention_mask'][0]\n        \n        # Convert the target to a torch tensor\n        target = torch.tensor(self.df_data.loc[index, 'label'])\n\n        sample = (padded_token_list, att_mask, target)\n\n\n        return sample\n\n\n    def __len__(self):\n        return len(self.df_data)\n    \n    \n    \n    \n    \n\nclass TestDataset(Dataset):\n\n    def __init__(self, df):\n        self.df_data = df\n\n\n\n    def __getitem__(self, index):\n\n        # get the sentence from the dataframe\n        sentence1 = self.df_data.loc[index, 'premise']\n        sentence2 = self.df_data.loc[index, 'hypothesis']\n\n        # Process the sentence\n        # ---------------------\n\n        encoded_dict = tokenizer.encode_plus(\n                    sentence1, sentence2,           # Sentence to encode.\n                    add_special_tokens = True,      # Add the special tokens.\n                    max_length = MAX_LEN,           # Pad & truncate all sentences.\n                    pad_to_max_length = True,\n                    return_attention_mask = True,   # Construct attn. masks.\n                    return_tensors = 'pt',          # Return pytorch tensors.\n               )\n        \n        # These are torch tensors.\n        padded_token_list = encoded_dict['input_ids'][0]\n        att_mask = encoded_dict['attention_mask'][0]\n        \n               \n\n        sample = (padded_token_list, att_mask)\n\n\n        return sample\n\n\n    def __len__(self):\n        return len(self.df_data)","9f3f71f7":"train_data = CompDataset(df_train)\nval_data = CompDataset(df_val)\ntest_data = TestDataset(df_test)\n\ntrain_dataloader = torch.utils.data.DataLoader(train_data,\n                                        batch_size=BATCH_SIZE,\n                                        shuffle=True,\n                                       num_workers=NUM_CORES)\n\nval_dataloader = torch.utils.data.DataLoader(val_data,\n                                        batch_size=BATCH_SIZE,\n                                        shuffle=True,\n                                       num_workers=NUM_CORES)\n\ntest_dataloader = torch.utils.data.DataLoader(test_data,\n                                        batch_size=BATCH_SIZE,\n                                        shuffle=False,\n                                       num_workers=NUM_CORES)\n\n\n\nprint(len(train_dataloader))\nprint(len(val_dataloader))\nprint(len(test_dataloader))","aaae3a0b":"# Get one train batch\n\npadded_token_list, att_mask, target = next(iter(train_dataloader))\n\nprint(padded_token_list.shape)\nprint(att_mask.shape)\nprint(target.shape)","37ecf69c":"# Get one val batch\n\npadded_token_list, att_mask, target = next(iter(val_dataloader))\n\nprint(padded_token_list.shape)\nprint(att_mask.shape)\nprint(target.shape)","83bdc067":"# Get one test batch\n\npadded_token_list, att_mask = next(iter(test_dataloader))\n\nprint(padded_token_list.shape)\nprint(att_mask.shape)","f2cb9324":"from transformers import XLMRobertaForSequenceClassification\n\nmodel = XLMRobertaForSequenceClassification.from_pretrained(\n    MODEL_TYPE, \n    num_labels = 3, # The number of output labels. 2 for binary classification.\n)\n\n# Send the model to the device.\nmodel.to(device)","c3dad0d3":"# Create a batch of train samples\n# We will set a small batch size of 8 so that the model's output can be easily displayed.\n\ntrain_dataloader = torch.utils.data.DataLoader(train_data,\n                                        batch_size=8,\n                                        shuffle=True,\n                                       num_workers=NUM_CORES)\n\nb_input_ids, b_input_mask, b_labels = next(iter(train_dataloader))\n\nprint(b_input_ids.shape)\nprint(b_input_mask.shape)\nprint(b_labels.shape)","b354fa59":"# Pass a batch of train samples to the model.\n\nbatch = next(iter(train_dataloader))\n\n# Send the data to the device\nb_input_ids = batch[0].to(device)\nb_input_mask = batch[1].to(device)\nb_labels = batch[2].to(device)\n\n# Run the model\noutputs = model(b_input_ids, \n                        attention_mask=b_input_mask, \n                        labels=b_labels)\n\n# The ouput is a tuple (loss, preds).\noutputs","602f5dc9":"outputs","a49e58d0":"# The output is a tuple: (loss, preds)\n\nlen(outputs)","6f909634":"# This is the loss.\n\noutputs[0]","775612be":"# These are the predictions.\n\noutputs[1]","3816a1f4":"preds = outputs[1].detach().cpu().numpy()\n\ny_true = b_labels.detach().cpu().numpy()\ny_pred = np.argmax(preds, axis=1)\n\ny_pred","12f0f04b":"# This is the accuracy without fine tuning.\n\nval_acc = accuracy_score(y_true, y_pred)\n\nval_acc","6be6588a":"# The loss and preds are Torch tensors\n\nprint(type(outputs[0]))\nprint(type(outputs[1]))","d6a76a1e":"# Define the optimizer\noptimizer = AdamW(model.parameters(),\n              lr = L_RATE, \n              eps = 1e-8 \n            )","ef2a4a2e":"# Create the dataloaders.\n\ntrain_data = CompDataset(df_train)\nval_data = CompDataset(df_val)\ntest_data = TestDataset(df_test)\n\ntrain_dataloader = torch.utils.data.DataLoader(train_data,\n                                        batch_size=BATCH_SIZE,\n                                        shuffle=True,\n                                       num_workers=NUM_CORES)\n\nval_dataloader = torch.utils.data.DataLoader(val_data,\n                                        batch_size=BATCH_SIZE,\n                                        shuffle=True,\n                                       num_workers=NUM_CORES)\n\ntest_dataloader = torch.utils.data.DataLoader(test_data,\n                                        batch_size=BATCH_SIZE,\n                                        shuffle=False,\n                                       num_workers=NUM_CORES)\n\n\n\nprint(len(train_dataloader))\nprint(len(val_dataloader))\nprint(len(test_dataloader))","67f9371b":"%%time\n\n\n# Set the seed.\nseed_val = 101\n\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)\n\n# Store the average loss after each epoch so we can plot them.\nloss_values = []\n\n\n# For each epoch...\nfor epoch in range(0, NUM_EPOCHS):\n    \n    print(\"\")\n    print('======== Epoch {:} \/ {:} ========'.format(epoch + 1, NUM_EPOCHS))\n    \n\n    stacked_val_labels = []\n    targets_list = []\n\n    # ========================================\n    #               Training\n    # ========================================\n    \n    print('Training...')\n    \n    # put the model into train mode\n    model.train()\n    \n    # This turns gradient calculations on and off.\n    torch.set_grad_enabled(True)\n\n\n    # Reset the total loss for this epoch.\n    total_train_loss = 0\n\n    for i, batch in enumerate(train_dataloader):\n        \n        train_status = 'Batch ' + str(i) + ' of ' + str(len(train_dataloader))\n        \n        print(train_status, end='\\r')\n\n\n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        b_labels = batch[2].to(device)\n\n        model.zero_grad()        \n\n\n        outputs = model(b_input_ids, \n                    attention_mask=b_input_mask,\n                    labels=b_labels)\n        \n        # Get the loss from the outputs tuple: (loss, logits)\n        loss = outputs[0]\n        \n        # Convert the loss from a torch tensor to a number.\n        # Calculate the total loss.\n        total_train_loss = total_train_loss + loss.item()\n        \n        # Zero the gradients\n        optimizer.zero_grad()\n        \n        # Perform a backward pass to calculate the gradients.\n        loss.backward()\n        \n        \n        # Clip the norm of the gradients to 1.0.\n        # This is to help prevent the \"exploding gradients\" problem.\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        \n        \n        \n        # Use the optimizer to update the weights.\n        \n        # Optimizer for GPU\n        # optimizer.step() \n        \n        # Optimizer for TPU\n        # https:\/\/pytorch.org\/xla\/\n        xm.optimizer_step(optimizer, barrier=True)\n\n    \n    print('Train loss:' ,total_train_loss)\n\n\n    # ========================================\n    #               Validation\n    # ========================================\n    \n    print('\\nValidation...')\n\n    # Put the model in evaluation mode.\n    model.eval()\n\n    # Turn off the gradient calculations.\n    # This tells the model not to compute or store gradients.\n    # This step saves memory and speeds up validation.\n    torch.set_grad_enabled(False)\n    \n    \n    # Reset the total loss for this epoch.\n    total_val_loss = 0\n    \n\n    for j, batch in enumerate(val_dataloader):\n        \n        val_status = 'Batch ' + str(j) + ' of ' + str(len(val_dataloader))\n        \n        print(val_status, end='\\r')\n\n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        b_labels = batch[2].to(device)      \n\n\n        outputs = model(b_input_ids, \n                attention_mask=b_input_mask, \n                labels=b_labels)\n        \n        # Get the loss from the outputs tuple: (loss, logits)\n        loss = outputs[0]\n        \n        # Convert the loss from a torch tensor to a number.\n        # Calculate the total loss.\n        total_val_loss = total_val_loss + loss.item()\n        \n\n        # Get the preds\n        preds = outputs[1]\n\n\n        # Move preds to the CPU\n        val_preds = preds.detach().cpu().numpy()\n        \n        # Move the labels to the cpu\n        targets_np = b_labels.to('cpu').numpy()\n\n        # Append the labels to a numpy list\n        targets_list.extend(targets_np)\n\n        if j == 0:  # first batch\n            stacked_val_preds = val_preds\n\n        else:\n            stacked_val_preds = np.vstack((stacked_val_preds, val_preds))\n\n    \n    # Calculate the validation accuracy\n    y_true = targets_list\n    y_pred = np.argmax(stacked_val_preds, axis=1)\n    \n    val_acc = accuracy_score(y_true, y_pred)\n    \n    \n    print('Val loss:' ,total_val_loss)\n    print('Val acc: ', val_acc)\n\n\n    # Save the Model\n    torch.save(model.state_dict(), 'model.pt')\n    \n    # Use the garbage collector to save memory.\n    gc.collect()","db477ed8":"for j, batch in enumerate(test_dataloader):\n        \n        inference_status = 'Batch ' + str(j+1) + ' of ' + str(len(test_dataloader))\n        \n        print(inference_status, end='\\r')\n\n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n\n\n        outputs = model(b_input_ids, \n                attention_mask=b_input_mask)\n        \n        \n        # Get the preds\n        preds = outputs[0]\n\n\n        # Move preds to the CPU\n        preds = preds.detach().cpu().numpy()\n        \n        # Move the labels to the cpu\n        targets_np = b_labels.to('cpu').numpy()\n\n        # Append the labels to a numpy list\n        targets_list.extend(targets_np)\n        \n        # Stack the predictions.\n\n        if j == 0:  # first batch\n            stacked_preds = preds\n\n        else:\n            stacked_preds = np.vstack((stacked_preds, preds))","7a8eeb3a":"stacked_preds","5094bae6":"# Take the argmax. This returns the column index of the max value in each row.\n\npreds = np.argmax(stacked_preds, axis=1)\n\npreds","56b2e613":"# Load the sample submission.\n# The row order in the test set and the sample submission is the same.\n\npath = '..\/input\/contradictory-my-dear-watson\/sample_submission.csv'\n\ndf_sample = pd.read_csv(path)\n\nprint(df_sample.shape)\n\ndf_sample.head()","b6eb983f":"# Assign the preds to the prediction column\n\ndf_sample['prediction'] = preds\n\ndf_sample.head()","65725a70":"# Create a submission csv file\n# Note that for this competition the submission file must be named submission.csv.\n# Therefore, it won't be possible to submit this csv file for leaderboard scoring.\ndf_sample.to_csv('xlmroberta_submission.csv', index=False)","2bf90adf":"# Check that the model has been saved.\n\n!ls","7c90177d":"# Check the distribution of the predicted classes.\n\ndf_sample['prediction'].value_counts()","7c8c5800":"XLM means Cross-lingual Language Model. XLM-RoBERTa (XLM-R) is a pre-trained multilingual model that outperforms multiligual BERT. One reason for this is that XLM-R was trained using a lot more data. XLM-R was also trained on 100 languages.\n\nSeveral versions of xlm roberta are available in the Transformers library. Here are two:\n\n- xlm-roberta-base\n- xlm-roberta-large\n\nThis is the link to the XLM-RoBERTa paper:<br>\nhttps:\/\/arxiv.org\/pdf\/1911.02116.pdf","0b594553":"## Create a submission csv file","ac320464":"## Decoding a sequence of tokens","8899fb59":"## 2.1. Load the Data","bc73b556":"| <a id='Appendix'><\/a>","fb051abc":"## How to use a tokenizer to auto format input data","da2bcede":"Once you understand what the input format needs to be you can write the code to manually create the input batches for training, validation and testing. However, it's also possible to use tokenizer.encode_plus to format each input row automatically. The Pytorch dataloaders in section 3 (below) use tokenizer.encode_plus when creating input batches.\n\nThis is how to automatically encode data for BERT when you have one input sentence and two input sentences:","bdddb638":"**Thank you for reading.**","4277edf4":"| <a id='Manual_formatting_of_model_input_data'><\/a>","a5e6719b":"# Section 1","421b4947":"| <a id='Overflowing_tokens_and_Stride'><\/a>","192e6393":"| <a id='Create_5_Folds'><\/a>","689d26f7":"## A6 - Helpful Resources\n\n- GLUE Explained: Understanding BERT Through Benchmarks<br>\nhttps:\/\/mccormickml.com\/2019\/11\/05\/GLUE\/\n\n- Improving Language Understanding with Unsupervised Learning<br>\nhttps:\/\/openai.com\/blog\/language-unsupervised\/\n\n- Hugging Face Transformers Github<br>\nhttps:\/\/github.com\/huggingface\/transformers\n\n- Hugging Face Summary of Models<br>\nhttps:\/\/huggingface.co\/transformers\/model_summary.html\n\n- Hugging Face - Searchable model listing<br>\nhttps:\/\/huggingface.co\/models\n\n- Bert Video Series by ChrisMcCormickAI<br>\nPart 1<br>\nhttps:\/\/www.youtube.com\/watch?v=FKlPCK1uFrc<br>\nPart 2<br>\nhttps:\/\/www.youtube.com\/watch?v=zJW57aCBCTk<br>\nPart 3<br>\nhttps:\/\/www.youtube.com\/watch?v=x66kkDnbzi4<br>\nPart 4<br>\nhttps:\/\/www.youtube.com\/watch?v=Hnvb9b7a_Ps<br>\n\n- Data Processing For Question & Answering Systems: BERT vs. RoBERTa by Abhishek Thakur<br>\nhttps:\/\/www.youtube.com\/watch?v=6a6L_9USZxg\n\n- Sentencepiece Tokenizer With Offsets For T5, ALBERT, XLM-RoBERTa And Many More by Abhishek Thakur<br>\nhttps:\/\/youtu.be\/U51ranzJBpY\n\n- PyTorch on XLA Devices - docs<br>\nhttps:\/\/pytorch.org\/xla\/release\/1.5\/index.html\n","9bd9f4af":"### Basics of BERT and XLM-RoBERTa - PyTorch\nby Marsh [ @vbookshelf ]<br>\n19 August 2020","581d2e57":"| <a id='Helpful_Resources'><\/a>","936ad238":"In this section we will train a BERT Model on three folds and train an XLM-RoBERTa model on one fold. We will use PyTorch with a single TPU. For each model we will also make a prediction on the competition test set and create a submission csv file.\n\n### A few notes on using PyTorch with a TPU\n\n- Setting up PyTorch code to use a single xla device (TPU) is easier that setting it up to use all 8 TPU cores. Just a few lines of code need to be changed to switch from a GPU to a single TPU. The speed is not as fast as using all 8 TPU cores but the model does train faster than a GPU and there's more RAM available. \n\n- Pytorch XLA does not use memory as efficiently as Tensorflow. Therefore, my code tends to consistently crash when I try to use PyTorch with an 8 core TPU setup. \n\n- There is 4.9GB of disk space available in Kaggle notebooks. What I've found is that models trained on a TPU are larger than models trained on a GPU. For example, a Bert model trained on a GPU is 600MB. However, a BERT model trained on a TPU is approx. 1GB. Therefore, when running 5 fold cross validation, trying to save all 5 fold models (1GB each) will cause the Kaggle notebook to crash because the available disk space will be exceeded. For that reason here we will be training on three folds only.\n\n-  A TPU may take a few seconds to start running. Therefore, if you run your code and you see that nothing is happening, wait a little while. Don't cancel the run because you think that something is wrong.","6ef3ef2e":"## Contents\n\n<a href='#Section_1'>Section 1<\/a><br>\n<a href='#BERT'>1.1. Explore BERT<\/a><br>\n<a href='#XLM-Roberta'>1.2. Explore XLM-RoBERTa<\/a><br>\n<a href='#Manual_formatting_of_model_input_data'>1.3. Manual formatting of model input data<\/a><br>\n<a href='#Overflowing_tokens_and_Stride'>1.4. Overflowing tokens and Stride<\/a><br>\n\n<a href='#Section_2'>Section 2<\/a><br>\n<a href='#Load_the_Data'>2.1. Load the Data<\/a><br>\n<a href='#Create_5_Folds'>2.2. Create 5 Folds<\/a><br>\n\n<a href='#Section_3'>Section 3<\/a><br>\n<a href='#Train_a_Bert_Model'>3.1. Train a BERT Model<\/a><br>\n<a href='#Train_an_XLM-Roberta_Model'>3.2. Train an XLM-RoBERTa Model <\/a><br>\n\n<a href='#Appendix'>Appendix<\/a><br>\n<a href='#Acronyms'>A1 - Acronyms<\/a><br>\n<a href='#GLUE_Datasets'>A2 - GLUE Datasets<\/a><br>\n<a href='#Datasets_Separated_by_Task'>A3 - Datasets Separated by Task<\/a><br>\n<a href='#Papers'>A4 - Papers<\/a><br>\n<a href='#NLP_Applications'>A5 - What is NLP used for?<\/a><br>\n<a href='#Helpful_Resources'>A6 - Helpful Resources<\/a><br>\n\n","4dd94756":"## Create the Dataloader","c6cd0816":"| <a id='Train_a_Bert_Model'><\/a>","23987fbe":"## 3.1. Train a Bert Model","a9e90075":"### Output during Training","68d28d4e":"## Bert Vocabulary","c09f8c46":"## A2 - GLUE Datasets\n\nGLUE (General Language Understanding Evaluation) is a performance bechmark that's used to compare the language understanding capability of machine learning models. A model's performance on 9 datasets is reduced to a single number. These are the datasets that are part of GLUE.\n\n1. MNLI -Multi-Genre Natural Language Inference\n2. QQP - Quora Question Pairs\n3. QNLI - Question Natural Langiage Inference\n4. SST-2 - Stanford Sentiment Treebank\n5. CoLA - Corpus of Linguistic Acceptability\n6. STS-B - Semantic Textual Similarity Benchmark\n7. MRPC - Microsoft Research Paraphrase Corpus\n8. RTE - Recognizing Textual Entailment\n9. WNLI - Winograd NLI\n\nMore Info:<br>\nGLUE Explained: Understanding BERT Through Benchmarks<br>\nhttps:\/\/mccormickml.com\/2019\/11\/05\/GLUE\/\n","acf7de96":"| <a id='Papers'><\/a>","d7ee8bfa":"All of the model's learned knowledge is in the [CLS] token. Therefore, if you are creating your own classification head for a BERT model, only the CLS token should feed your classification layer. The CLS vector is 768 long.","ba6ab320":"This notebook is divided into 3 Sections and an Appendix. In section 1 we will look at how to format input data for Bert and XLM-Roberta and review the ouput that these models produce. In section 2 we will load the competition data and create 5 folds. In section 3 we will fine-tune a 3 fold cv Bert model and a single fold XLM-RoBERTa model - using Pytorch with a single xla device (TPU). Finally, in the Appendix I've included some info that I'm finding helpful as I learn how to use pre-trained transformer models. ","3cd69c8b":"## Decoding a sequence of tokens","ded3e4b8":"## A3 - Datasets Separated by Task\n\na) Sentence Pair Classification Tasks<br>\nMNLI, QQP, QNLI, STS-B, MRPC, RTE, SWAG\n\nb) Single Sentence Classifications Tasks<br>\nSST-2, CoLA\n\nc) Question Answering Tasks<br>\nSQuAD (v1.1 and v2.0)\n\nd) Single Sentence Tagging Tasks<br>\nCoNLL-2003 NER","740096e9":"## Test the Model","483409eb":"## Train the Model","607fe28f":"# Section 2","bc11dfd4":"Above we see that 4 tokens were truncated. The token numbers are shown in the list called overflowing_tokens. \n\nWe can also specify a stride. This adds tokens to the front of the 'overflowing_tokens list creating an overlap. The best way to illustrate this is with an example:","385c66e2":"| <a id='Section_3'><\/a>","fa1f7a14":"## What does Bert output?","44996e75":"| <a id='Datasets_Separated_by_Task'><\/a>","98bec627":"## What input does BERT expect?\n\nBERT expects the input sentences to be formatted as a list of tokens (examples below). During training the model replaces each token with a corresponding word embedding vector. Each word vector has a length of 768. The max input length for BERT is 512 tokens.\n\nThese are two helpful resources to learn about word vectors:\n\nWhat are word embeddings?<br>\nhttps:\/\/www.youtube.com\/watch?v=Eku_pbZ3-Mw\n\nBERT Research - Ep. 2 - WordPiece Embeddings<br>\nhttps:\/\/www.youtube.com\/watch?v=zJW57aCBCTk\n\n","7886599e":"## Define the device","da1ab1bc":"## Load the data\n\nHere we will only use fold_0 for training.","152b7948":"## How to use a tokenizer to create XLM-RoBERTa input","3b4cc746":"## Introduction","0a91af2d":"## Inspect the model's output","62343a30":"## Test the dataloader","6e265b61":"# Appendix","61e19344":"## Train the Model","fc6252f0":"| <a id='BERT'><\/a>","6488cb68":"| <a id='NLP_Applications'><\/a>","82cc7b26":"## Test the model","f57439f4":"## 1.2. Explore XLM-RoBERTa","ef6f4ffc":"## Define the Optimizer","245868a6":"### For one input sentence","e7fa9dec":"## 1.4. Overflowing tokens and Stride\n\nWhen a sentence is truncated (because it's length exceeds max_length) it's possible to get the tokenizer to return the tokens that were cut off. These truncated tokens will be returned in a list called overflowing_tokens.","612b1664":"## Define the device","dacfd8c0":"## 2.2. Create 5 Folds","475f25e2":"## Process the Predictions","468bfd4f":"### Prediction output\n\nThis is the output when labels are not passed to the model.","de8b17dc":"### For two input sentences","f873aeb4":"## A5 - What is NLP used for?\n\n- Text Classification\n- Translation\n- Named Entity Recognition\n- Part of Speech Tagging\n- Question Answering\n- Text Generation\n- Language Modeling\n- Text Summarization","63d41cb2":"## 1.1. Explore BERT","74ed052d":"| <a id='GLUE_Datasets'><\/a>","5dfe01ab":"## Instantiate the Tokenizer","73fbf86b":"| <a id='Load_the_Data'><\/a>","705b3b26":"### For two input sentences","c24f45ca":"## Process the predictions\n\nHere we are ensembling the predictons of all the fold models. We add all the matrices and then take the average.","7ac3e650":"## 1.3. Manual formatting of model input data","8b61b91a":"## Inspect the model's output","1b76b86d":"## Define the Model","495901a8":"## The importance of the [CLS] token for classification problems","06797b23":"# Section 3","4850b910":"In this section we'll look at what input the Bert and XLM-RoBERTa models expect and what output they produce. We will also use a tokenizer to automatically process one sentence and a pair of sentences into the correct input format for each model. ","75273f68":"## A1 - Acronyms\n\n- NLP - Natural Language Processing\n- NLU - Natural Language Understanding\n- NLI - Natural Language Inference\n- NER - Named Entity Recognition\n- NSP - Next Sentence Prediction\n- MLM - Masked Language Model\n- PoS - Part of Speech\n- POST - Part of Speech Tagging\n- GLUE - The General Language Understanding Evaluation benchmark\n- SQuAD - Stanford Question Answering Dataset\n- SWAG - Situations With Adversarial Generations (Dataset)\n- XNLI - Cross Lingual Natural Language Inference (Dataset)\n- XLU - Cross-lingual Language Understanding\n\n","893d2d58":"## Test the dataloader","bc6d1693":"## Define the Model","a182ec1c":"In this section we will load the competition train and test data. We will also create 5 folds that can be used for cross validation.","eb3cdfd8":"## A4 - Papers\n\n- Attention is all you need<br>\nhttps:\/\/arxiv.org\/pdf\/1706.03762.pdf\n\n- BERT Paper<br>\nBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding<br>\nhttps:\/\/arxiv.org\/pdf\/1810.04805.pdf\n\n- XLMRoberta Paper<br>\nUnsupervised Cross-lingual Representation Learning at Scale<br>\nhttps:\/\/arxiv.org\/pdf\/1911.02116.pdf\n\n- GLUE Paper<br>\nhttps:\/\/arxiv.org\/abs\/1804.07461<br>\nWebsite: https:\/\/gluebenchmark.com\/\n\n- MultiNLI Paper<br>\nA Broad-Coverage Challenge Corpus for Sentence Understanding through Inference<br>\nhttps:\/\/cims.nyu.edu\/~sbowman\/multinli\/paper.pdf<br>\nWebsite: https:\/\/cims.nyu.edu\/~sbowman\/multinli\/\n\n- XNLI Paper<br>\nhttps:\/\/arxiv.org\/pdf\/1809.05053.pdf<br>\nWebsite: https:\/\/cims.nyu.edu\/~sbowman\/xnli\/\n\n- SentencePiece Paper<br>\nSentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing\nhttps:\/\/arxiv.org\/abs\/1808.06226","c5316066":"| <a id='XLM-Roberta'><\/a>","4ffea863":"BERT is a pre-trained language model that can be fine tuned to perform NLP tasks. BERT stands for Bidirectional Encoder Representations from Transformers. Bidirectional means that the model is able to read text from both left-to-right and from right-to-left. This capability helps it to understand context. BERT's other super-power is that it can understand 100 languages.\n\nYou can access BERT and other pre-trained models through a library called [Transformers](https:\/\/github.com\/huggingface\/transformers). The team at Hugging Face created this library. It contains many transformer models and tokenizers that use a common interface. The Transformers library is available in Kaggle notebooks by default - simply type: import Transformers.\n\nSeveral pre-trained BERT models are available - different sizes, monolingual and multilingual. These are a few types:\n\n- bert-base-uncased\n- bert-base-cased\n- bert-large-uncased\n- bert-large-cased\n- bert-base-multilingual-uncased \n- bert-base-multilingual-cased\n\n*uncased* - All text was converted to lower case before training the model.<br>\n*cased* - The text used to train the model was not converted to lower case.\n\nThis is a link to a full searchable listing of all model types:<br>\nhttps:\/\/huggingface.co\/models\n\nThis is a link to the BERT paper:<br>\nhttps:\/\/arxiv.org\/pdf\/1810.04805.pdf\n\nIf you are new to BERT and to pre-trained transformer models then I suggest watching these video tutorials:<br>\n\nPart 1<br>\nhttps:\/\/www.youtube.com\/watch?v=FKlPCK1uFrc<br>\nPart 2<br>\nhttps:\/\/www.youtube.com\/watch?v=zJW57aCBCTk<br>\nPart 3<br>\nhttps:\/\/www.youtube.com\/watch?v=x66kkDnbzi4<br>\nPart 4<br>\nhttps:\/\/www.youtube.com\/watch?v=Hnvb9b7a_Ps<br>\n","5a764034":"## Create a submission csv file","129550bd":"Because stride is set to 3, three tokens (5155, 5, 3293) from the end of the input_ids (excl. special token 2) are added to the front of the overflowing_tokens list. This creates an overlap between the two lists.","979b98ad":"## 3.2. Train an XLM-RoBERTa Model","4c321810":"| <a id='Train_an_XLM-Roberta_Model'><\/a>","85c63697":"| <a id='Section_1'><\/a>","198ecb34":"| <a id='Acronyms'><\/a>","3819a7f6":"### For one input sentence","4b89ef4d":"## Instantiate the tokenizer","9272b973":"| <a id='Section_2'><\/a>","745a9f87":"## Make a Test Set Prediction","12e561bc":"## Create the Dataloader","c6ca64cb":"## What input does XLM-RoBERTa expect?","9c6beac0":"## XLM-RoBERTa Vocabulary","13a5c2dc":"## Make a prediction on the test set","931c4fbd":"So far we've used tokenizer.encode_plus to automatically format input data. I've included the following resource links because there are things that one needs to be aware of when writing code for manual formatting. For example XLM-RoBERTa uses a SentencePiece-based tokenizer but BERT does not. It's also good to know what SentencePiece tokenization is and how it works.\n\nHugging Face Tokenizer docs<br>\nhttps:\/\/huggingface.co\/transformers\/main_classes\/tokenizer.html\n\nAbhishek Thakur<br>\nData Processing For Question & Answering Systems: BERT vs. RoBERTa<br>\n(Note that this video covers RoBERTa and not XLM-RoBERTa)<br>\nhttps:\/\/www.youtube.com\/watch?v=6a6L_9USZxg\n\nAbhishek Thakur<br>\nSentencepiece Tokenizer With Offsets For T5, ALBERT, XLM-RoBERTa And Many More<br>\nhttps:\/\/youtu.be\/U51ranzJBpY\n\nSentencePiece Paper<br>\nhttps:\/\/arxiv.org\/abs\/1808.06226\n\nSentencePiece Github<br>\nhttps:\/\/github.com\/google\/sentencepiece\n\nThe following Hugging face models use a SentencePiece-based tokenizer:<br>\nT5, ALBERT, CamemBERT, XLMRoBERTa and XLNet"}}