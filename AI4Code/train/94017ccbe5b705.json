{"cell_type":{"284eba67":"code","3dd3c905":"code","6ccb8693":"code","81b6ed2d":"code","dfb6ced1":"code","e5b7db95":"code","3953debb":"code","373b9765":"code","6e5a5c7c":"code","0cbfc9d9":"code","e489533b":"code","985934af":"code","fbcd9577":"code","729ec9ec":"code","108de871":"code","d4ff1f21":"code","4793ed0b":"code","e7b0b8b0":"code","1d693743":"code","21f83fbd":"code","edc46c97":"code","7e84348c":"code","56e1a80b":"code","38941e5b":"markdown","d0571a18":"markdown","a2887506":"markdown","568a48ed":"markdown","5719fb15":"markdown","8034a65b":"markdown","8eac071d":"markdown","f65e0d66":"markdown","bcb7ee3b":"markdown","deedab2f":"markdown","30f2cce6":"markdown","104373f9":"markdown"},"source":{"284eba67":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file \n\nimport datetime\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport matplotlib.gridspec as gridspec\n%matplotlib inline\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\nfrom sklearn.metrics import roc_auc_score\n\nimport xgboost as xgb\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport gc\ngc.enable()\n\nimport os\nos.chdir('\/kaggle\/input\/ieeedatapreprocessing') # Set working directory\nprint(os.listdir('\/kaggle\/input\/ieeedatapreprocessing'))","3dd3c905":"%%time\nX_train = pd.read_pickle('train_df.pkl')\nX_test = pd.read_pickle('test_df.pkl')\nprint (\"Data is loaded!\")","6ccb8693":"print('train_transaction shape is {}'.format(X_train.shape))\nprint('test_transaction shape is {}'.format(X_test.shape))","81b6ed2d":"X_train.head()","dfb6ced1":"X_test.head()","e5b7db95":"# NORMALIZE D COLUMNS\nfor i in [1,2,3,4,5,10,11,15]:\n    if i in [1,2,3,5]: continue\n    X_train['D'+str(i)] =  X_train['D'+str(i)] - X_train.TransactionDT\/np.float32(24*60*60)\n    X_test['D'+str(i)] = X_test['D'+str(i)] - X_test.TransactionDT\/np.float32(24*60*60) ","3953debb":"# FREQUENCY ENCODE TOGETHER\ndef encode_FE(df1, df2, cols):\n    for col in cols:\n        df = pd.concat([df1[col],df2[col]])\n        vc = df.value_counts(dropna=True, normalize=True).to_dict()\n        vc[-1] = -1\n        nm = col+'_FE'\n        df1[nm] = df1[col].map(vc)\n        df1[nm] = df1[nm].astype('float32')\n        df2[nm] = df2[col].map(vc)\n        df2[nm] = df2[nm].astype('float32')\n        print(nm,', ',end='')\n        \n# LABEL ENCODE\ndef encode_LE(col,train=X_train,test=X_test,verbose=True):\n    df_comb = pd.concat([train[col],test[col]],axis=0)\n    df_comb,_ = df_comb.factorize(sort=True)\n    nm = col\n    if df_comb.max()>32000: \n        train[nm] = df_comb[:len(train)].astype('int32')\n        test[nm] = df_comb[len(train):].astype('int32')\n    else:\n        train[nm] = df_comb[:len(train)].astype('int16')\n        test[nm] = df_comb[len(train):].astype('int16')\n    del df_comb; x=gc.collect()\n    if verbose: print(nm,', ',end='')\n\n# LABEL ENCODE 2\ndef encode_LE2(df1,df2,col,verbose=True):\n    df_comb = pd.concat([df1[col],df2[col]],axis=0)\n    df_comb,_ = df_comb.factorize()\n    df1[col] = df_comb[:len(df1)].astype('int32')\n    df2[col] = df_comb[len(df1):].astype('int32')\n    if verbose: print(col,', ',end='')\n        \n# GROUP AGGREGATION MEAN AND STD\ndef encode_AG(main_columns, uids, aggregations=['mean'], train_df=X_train, test_df=X_test, \n              fillna=True, usena=False):\n    # AGGREGATION OF MAIN WITH UID FOR GIVEN STATISTICS\n    for main_column in main_columns:  \n        for col in uids:\n            for agg_type in aggregations:\n                new_col_name = main_column+'_'+col+'_'+agg_type\n                temp_df = pd.concat([train_df[[col, main_column]], test_df[[col,main_column]]])\n                if usena: temp_df.loc[temp_df[main_column]==-1,main_column] = np.nan\n                temp_df = temp_df.groupby([col])[main_column].agg([agg_type]).reset_index().rename(\n                                                        columns={agg_type: new_col_name})\n\n                temp_df.index = list(temp_df[col])\n                temp_df = temp_df[new_col_name].to_dict()   \n\n                train_df[new_col_name] = train_df[col].map(temp_df).astype('float32')\n                test_df[new_col_name]  = test_df[col].map(temp_df).astype('float32')\n                \n                if fillna:\n                    train_df[new_col_name].fillna(-1,inplace=True)\n                    test_df[new_col_name].fillna(-1,inplace=True)\n                \n                print(\"'\"+new_col_name+\"'\",', ',end='')\n                \n# COMBINE FEATURES\ndef encode_CB(col1,col2,df1=X_train,df2=X_test):\n    nm = col1+'_'+col2\n    df1[nm] = df1[col1].astype(str)+'_'+df1[col2].astype(str)\n    df2[nm] = df2[col1].astype(str)+'_'+df2[col2].astype(str) \n    encode_LE(nm,verbose=False)\n    print(nm,', ',end='')\n    \n# GROUP AGGREGATION NUNIQUE\ndef encode_AG2(main_columns, uids, train_df=X_train, test_df=X_test):\n    for main_column in main_columns:  \n        for col in uids:\n            comb = pd.concat([train_df[[col]+[main_column]],test_df[[col]+[main_column]]],axis=0)\n            mp = comb.groupby(col)[main_column].agg(['nunique'])['nunique'].to_dict()\n            train_df[col+'_'+main_column+'_ct'] = train_df[col].map(mp).astype('float32')\n            test_df[col+'_'+main_column+'_ct'] = test_df[col].map(mp).astype('float32')\n            print(col+'_'+main_column+'_ct, ',end='')","373b9765":"# TRANSACTION AMT CENTS\nX_train['cents'] = (X_train['TransactionAmt'] - np.floor(X_train['TransactionAmt'])).astype('float32')\nX_test['cents'] = (X_test['TransactionAmt'] - np.floor(X_test['TransactionAmt'])).astype('float32')","6e5a5c7c":"# FREQUENCY ENCODE: ADDR1, CARD1, CARD2, CARD3, P_EMAILDOMAIN\nencode_FE(X_train,X_test,['addr1','card1','card2','card3','P_emaildomain'])\n# COMBINE COLUMNS CARD1+ADDR1, CARD1+ADDR1+P_EMAILDOMAIN\nencode_CB('card1','addr1')\nencode_CB('card1_addr1','P_emaildomain')\n# FREQUENCY ENOCDE\nencode_FE(X_train,X_test,['card1_addr1','card1_addr1_P_emaildomain'])\n# GROUP AGGREGATE\nencode_AG(['TransactionAmt','D10','D11'],['card1','card1_addr1','card1_addr1_P_emaildomain'],['mean','std'],usena=True)","0cbfc9d9":"# ADD MONTH FEATURE\nSTART_DATE = datetime.datetime.strptime('2017-11-30', '%Y-%m-%d')\nX_train['DT_M'] = X_train['TransactionDT'].apply(lambda x: (START_DATE + datetime.timedelta(seconds = x)))\nX_train['DT_M'] = (X_train['DT_M'].dt.year-2017)*12 + X_train['DT_M'].dt.month \n\nX_test['DT_M'] = X_test['TransactionDT'].apply(lambda x: (START_DATE + datetime.timedelta(seconds = x)))\nX_test['DT_M'] = (X_test['DT_M'].dt.year-2017)*12 + X_test['DT_M'].dt.month ","e489533b":"# ADD UID FEATURE\nX_train['day'] = X_train.TransactionDT \/ (24*60*60)\nX_train['uid'] = X_train.card1_addr1.astype(str)+'_'+np.floor(X_train.day-X_train.D1).astype(str)\n\nX_test['day'] = X_test.TransactionDT \/ (24*60*60)\nX_test['uid'] = X_test.card1_addr1.astype(str)+'_'+np.floor(X_test.day-X_test.D1).astype(str)\n# LABEL ENCODE\nencode_LE2(X_train,X_test,'uid',verbose=False)","985934af":"oof = np.zeros(len(X_train))\npreds = np.zeros(len(X_test))","fbcd9577":"idxT = X_train.index[:4*len(X_train)\/\/5]\nidxV = X_train.index[4*len(X_train)\/\/5:]","729ec9ec":"cols_to_drop = [\"TransactionID\", \"isFraud\", \"TransactionDT\"]\nuseful_cols = list(X_train.columns)\n\nfor col in cols_to_drop:\n    while True:\n        try:\n            useful_cols.remove(col)\n        except:\n            break","108de871":"print('NOW USING THE FOLLOWING',len(useful_cols),'FEATURES.')\nnp.array(useful_cols)","d4ff1f21":"y_train = X_train['isFraud'].copy()","4793ed0b":"skf = GroupKFold(n_splits=6)\n\nfor i, (idxT, idxV) in enumerate( skf.split(X_train, y_train, groups=X_train['DT_M']) ):\n    month = X_train.iloc[idxV]['DT_M'].iloc[0]\n    \n    print('Fold',i,'withholding month',month)\n    print(' rows of train =',len(idxT),'rows of holdout =',len(idxV))\n    \n    xgboost_magic_classifier = xgb.XGBClassifier(\n            n_estimators=15000,\n            max_depth=20,\n            learning_rate=0.02,\n            subsample=0.8,\n            eval_metric='auc',\n            colsample_bytree=0.4,\n            missing=-999,\n            tree_method='gpu_hist' \n        )   \n    \n    xgboost_magic_classifier_fit = xgboost_magic_classifier.fit(X_train[useful_cols].iloc[idxT], y_train.iloc[idxT], \n            eval_set=[(X_train[useful_cols].iloc[idxV],y_train.iloc[idxV])],\n            verbose=100, early_stopping_rounds=500)\n    \n    oof[idxV] += xgboost_magic_classifier.predict_proba(X_train[useful_cols].iloc[idxV])[:,1]\n    preds += xgboost_magic_classifier.predict_proba(X_test[useful_cols])[:,1]\/skf.n_splits\n    \n    del xgboost_magic_classifier_fit\n    x = gc.collect()","e7b0b8b0":"print(confusion_matrix(y_train, oof.round()))","1d693743":"print(classification_report(y_train, oof.round()))","21f83fbd":"feature_imp = pd.DataFrame(sorted(zip(xgboost_magic_classifier.feature_importances_,useful_cols)), columns=['Value','Feature'])\nplt.figure(figsize=(20, 10))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False).iloc[:50])\nplt.title('XGBoost cross validation Most Important Features')\nplt.tight_layout()\nplt.show()","edc46c97":"submission = pd.read_csv('sample_submission.csv', index_col='TransactionID')\nsubmission.isFraud = preds\nsubmission.head()","7e84348c":"plt.hist(submission.isFraud,bins=100)\nplt.ylim((0,5000))\nplt.title('XGBoost cross validation submission')\nplt.show()","56e1a80b":"submission.to_csv('\/kaggle\/working\/xgboost_cv_submission.csv')","38941e5b":"**Pourquoi la d\u00e9tection de fraude ?**\n> La fraude est un commerce d'un milliard de dollars et elle augmente chaque ann\u00e9e. L'enqu\u00eate mondiale de PwC sur la criminalit\u00e9 \u00e9conomique de 2018 a r\u00e9v\u00e9l\u00e9 que la moiti\u00e9 (49 %) des 7 200 entreprises interrog\u00e9es avaient \u00e9t\u00e9 victimes d'une fraude quelconque. C'est une augmentation par rapport \u00e0 l'\u00e9tude PwC de 2016, dans laquelle un peu plus d'un tiers des organisations interrog\u00e9es (36 %) avaient \u00e9t\u00e9 victimes de la criminalit\u00e9 \u00e9conomique.\n\n\nCette comp\u00e9tition est un probl\u00e8me de **classification binaire** - c'est-\u00e0-dire que notre variable cible est un attribut binaire (l'utilisateur qui fait le clic est-il frauduleux ou non ?) et notre objectif est de classer les utilisateurs en \"frauduleux\" ou \"non frauduleux\" le mieux possible.","d0571a18":"#  <a style=\"color:#6699ff\"> Team <\/a>\n- <a style=\"color:#6699ff\">Mohamed NIANG <\/a>\n- <a style=\"color:#6699ff\">Fernanda Tchouacheu <\/a>\n- <a style=\"color:#6699ff\">Sokhna Penda Toure <\/a>\n- <a style=\"color:#6699ff\">Hypolite Chokki <\/a>","a2887506":"## XGBoost classifier with cross validation","568a48ed":"# <a style=\"color:#6699ff\"> I. Introduction<\/a>","5719fb15":"## XGBoost feature engineering","8034a65b":"<h1 align=\"center\" style=\"color:#6699ff\"> DataCamp IEEE Fraud Detection <\/h1>","8eac071d":"**Load data**","f65e0d66":"<img src=\"https:\/\/github.com\/DataCampM2DSSAF\/suivi-du-data-camp-equipe-tchouacheu_toure_niang_chokki\/blob\/master\/img\/credit-card-fraud-detection.png?raw=true\" width=\"800\" align=\"center\">","bcb7ee3b":"**Seventh submission for XGBOOST with cross validation**","deedab2f":"**Feature importance for XGBoost with cross validation**","30f2cce6":"# <a style=\"color:#6699ff\">  Table of Contents<\/a> \n\n<a style=\"color:#6699ff\"> I. Introduction<\/a>\n\n<a style=\"color:#6699ff\"> II. Descriptive Statistics & Visualization<\/a>\n\n<a style=\"color:#6699ff\"> III. Preprocessing<\/a>\n\n<a style=\"color:#6699ff\"> IV. Machine Learning Models<\/a>","104373f9":"# <a style=\"color:#6699ff\"> IV. Machine Learning Models<\/a>"}}