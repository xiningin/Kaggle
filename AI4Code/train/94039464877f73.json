{"cell_type":{"393b0821":"code","09eab48e":"code","045c3962":"code","3229b1ca":"code","96d3312c":"code","f4914de8":"code","158346b1":"code","0ca13edf":"code","265758b6":"code","0945287e":"code","ff62c326":"code","003e0543":"code","68f3210c":"code","4be9571f":"code","230248d1":"code","801a25bd":"code","7eb0ba7d":"code","74e26ef9":"code","ebae44b5":"code","0bbecebb":"code","f0d6117a":"code","c751bafb":"code","46e1a1b1":"code","be4e8643":"code","233b2581":"code","f01c0c1d":"code","79b1eee1":"code","d31afd8f":"code","1d8c46ab":"code","8ce4a7ab":"code","a2ae3db6":"code","bf202810":"code","8f80684f":"code","faab3f88":"code","098bc260":"code","f7b211e7":"code","dae21782":"code","b9f34b13":"code","307c68b7":"code","fe8f7e68":"code","2fefb515":"code","65b304a8":"code","811ea203":"code","404ef51d":"code","224e724d":"code","e732d352":"code","6166c363":"code","c15684da":"code","cedc106e":"code","4eba2522":"code","1fc3e44d":"code","14124319":"code","a1ee704b":"code","5d72822c":"code","6aa67bbf":"code","65563bc6":"code","67597e81":"markdown","9050735e":"markdown","c144bfdb":"markdown","5a850d57":"markdown","a4055292":"markdown","dc33ea2c":"markdown","29389ae5":"markdown","eac7e03a":"markdown","72563ff5":"markdown","71d28663":"markdown","cbe21ed9":"markdown","64f711b7":"markdown","673bd599":"markdown","0a091d07":"markdown","7a5e6efa":"markdown","ed955bbb":"markdown","5b8d2c31":"markdown","bf586b71":"markdown","846fbace":"markdown","03ffac54":"markdown","4d60e820":"markdown","027b3095":"markdown","5734f53e":"markdown"},"source":{"393b0821":"!pip install git+https:\/\/github.com\/fastai\/fastai@2e1ccb58121dc648751e2109fc0fbf6925aa8887\n!apt update && apt install -y libsm6 libxext6","09eab48e":"%load_ext autoreload\n%autoreload 2","045c3962":"%matplotlib inline\n\nfrom fastai.imports import *\nfrom fastai.structured import *\nfrom sklearn.ensemble import RandomForestClassifier\nfrom IPython.display import display\nfrom sklearn import metrics","3229b1ca":"import seaborn as sns\nsns.set_style('whitegrid')","96d3312c":"#path = 'titanic\/'\n!ls ..\/input","f4914de8":"df = pd.read_csv('..\/input\/titanic\/train.csv')\n","158346b1":"df.head()","0ca13edf":"f,ax=plt.subplots(1,2, figsize=(18,8))\ndf[['Embarked','Survived']].groupby(['Embarked']).mean().plot.bar(ax=ax[0])\nax[0].set_title('Survived vs Embarked')\nsns.countplot('Embarked',hue='Survived',data=df,ax=ax[1])\nax[1].set_title('Sex:Survived vs Embarked')\nplt.show()","265758b6":"df.Survived.value_counts().plot(kind='bar',legend=True)","0945287e":"df.Sex.value_counts().plot(kind='bar')","ff62c326":"_=df.Pclass.value_counts().plot(kind='bar')","003e0543":"f,ax=plt.subplots(1,2, figsize=(18,5))\ndf[['Sex','Survived']].groupby(['Sex']).mean().plot.bar(ax=ax[0])\nax[0].set_title('Survived vs Sex')\nsns.countplot('Sex',hue='Survived',data=df,ax=ax[1])\nax[1].set_title('Sex:Survived vs Dead')\nplt.show()","68f3210c":"df.head()","4be9571f":"train_cats(df)","230248d1":"df,y,nas=proc_df(df, 'Survived')","801a25bd":"df.head()","7eb0ba7d":"m=RandomForestClassifier(n_jobs=-1)\nm.fit(df, y)\nm.score(df,y)","74e26ef9":"def split_vals(a, n): return a[:n].copy(), a[n:].copy()\n\nn_valid = 209\nn_trn = len(df) - n_valid\nraw_train, raw_valid = split_vals(df, n_trn)\nX_train, X_valid = split_vals(df, n_trn)\ny_train, y_valid = split_vals(y, n_trn)\n\nX_train.shape, y_train.shape, X_valid.shape","ebae44b5":"def rmse(x, y): return math.sqrt(((x-y)**2).mean())\n\ndef print_score(m):\n    res=[rmse(m.predict(X_train),y_train), rmse(m.predict(X_valid), y_valid),\n         m.score(X_train, y_train), m.score(X_valid, y_valid)]\n    \n    if hasattr(m, 'oob_score_'): res.append(m.oob_score_)\n    print(res)","0bbecebb":"m = RandomForestClassifier(n_jobs=-1)\n%time m.fit(X_train, y_train)\nprint_score(m)","f0d6117a":"m = RandomForestClassifier(n_estimators=40, n_jobs=-1, max_depth=3, bootstrap=False)\nm.fit(X_train, y_train)\nprint_score(m)","c751bafb":"m = RandomForestClassifier(n_jobs=-1, n_estimators=1, bootstrap=False)\nm.fit(X_train, y_train)\nprint_score(m)","46e1a1b1":"m =RandomForestClassifier(n_estimators=5, n_jobs=-1)\nm.fit(X_train, y_train)\nprint_score(m)","be4e8643":"m = RandomForestClassifier(n_estimators=40, n_jobs=-1)\nm.fit(X_train, y_train)\nprint_score(m)","233b2581":"m = RandomForestClassifier(n_estimators=60, n_jobs=-1)\nm.fit(X_train, y_train)\nprint_score(m)","f01c0c1d":"m = RandomForestClassifier(n_estimators=40, n_jobs=-1, oob_score=True)\nm.fit(X_train, y_train)\nprint_score(m)","79b1eee1":"m = RandomForestClassifier(n_estimators=200, n_jobs=-1, oob_score=True)\nm.fit(X_train, y_train)\nprint_score(m)","d31afd8f":"X_train, X_valid = split_vals(df, n_trn)\nm = RandomForestClassifier(n_estimators=40, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True)\nm.fit(X_train, y_train)\nprint_score(m)","1d8c46ab":"def dectree_max_depth(tree):\n    children_left = tree.children_left\n    children_right = tree.children_right\n\n    def walk(node_id):\n        if (children_left[node_id] != children_right[node_id]):\n            left_max = 1 + walk(children_left[node_id])\n            right_max = 1 + walk(children_right[node_id])\n            return max(left_max, right_max)\n        else: # leaf\n            return 1\n\n    root_node_id = 0\n    return walk(root_node_id)","8ce4a7ab":"m = RandomForestClassifier(n_estimators=40, n_jobs=-1, oob_score=True)\nm.fit(X_train, y_train)\nprint_score(m)","a2ae3db6":"t = m.estimators_[0].tree_\ndectree_max_depth(t)","bf202810":"m = RandomForestClassifier(n_estimators=40, min_samples_leaf=3, n_jobs=-1, oob_score=True)\nm.fit(X_train, y_train)\nprint_score(m)","8f80684f":"m = RandomForestClassifier(n_estimators=100, oob_score=True)\nm.fit(X_train, y_train)\nprint_score(m)","faab3f88":"m = RandomForestClassifier(n_estimators=40, n_jobs=-1, min_samples_leaf=3, oob_score=True, max_features=0.5)\nm.fit(X_train, y_train)\nprint_score(m)","098bc260":"fi=rf_feat_importance(m,df); fi","f7b211e7":"feats=['Name','Ticket','PassengerId','Embarked','Age_na','Fare']","dae21782":"df.drop(feats, axis=1, inplace=True)\n","b9f34b13":"fi.plot('cols','imp',figsize=(5,6),legend=False)","307c68b7":"X_train, X_valid = split_vals(df, n_trn)","fe8f7e68":"df.head()","2fefb515":"m = RandomForestClassifier(n_estimators=50, n_jobs=-1, min_samples_leaf=3,oob_score=True, random_state=1)\nm.fit(X_train,y_train)\nprint_score(m)","65b304a8":"m = RandomForestClassifier(n_estimators=50, n_jobs=-1, min_samples_leaf=3, oob_score=True, \n                           random_state=1, max_features=None)\nm.fit(X_train,y_train)\nprint_score(m)","811ea203":"df_test=pd.read_csv('..\/input\/titanic\/test.csv')\ndf_test.head()","404ef51d":"train_cats(df_test)","224e724d":"df_test,y_name,nas=proc_df(df_test, 'Name')","e732d352":"df_test.head()","6166c363":"feats=['Ticket','PassengerId','Embarked','Age_na','Fare_na','Fare']\n","c15684da":"df_test.drop(feats, axis=1, inplace=True)\n","cedc106e":"df_test.head()\n","4eba2522":"df.head()","1fc3e44d":"Survived=m.predict(df_test)\nSurvived.sum()","14124319":"df_sample=pd.read_csv('..\/input\/titanic\/test.csv')\n","a1ee704b":"df_sample['Survived']=pd.Series(Survived)","5d72822c":"df_sample.head()","6aa67bbf":"df_sample.to_csv('..\/input\/submit.csv',columns=['PassengerId','Survived'], index=False)","65563bc6":"submit=pd.read_csv('submit.csv')\n","67597e81":"In this competition task is find out how many passenger are survived on the ship. So, I am using random forest algorithm and fastai liabrary to find out how many passenger are survived.","9050735e":"Convert all dataframe columns into categorical and separate prediction column into another variable and find accuraccy on training data.","c144bfdb":"# Fastai Random Forest Algorithm","5a850d57":"Random Forest algorithm draw number of decision trees and aggregating them to find result. In previous random forest model creating 40 trees. I am showing one decision tree following, decision tree shows the feature name on which they classified, which algorithm is used for classification, how much sample used to draw tree and value. After drawing tree, i am trying to build model using some hyperparameter but sometime model is overfit and underfit.","a4055292":"Graph showing the how many male and female was survived as well as showing correlation between sex and survived columns. According to the graph, female was survived more than male.","dc33ea2c":"According to graph, female are more than male present in the ship.","29389ae5":"Here, I am using random forest to find passenger is survived or not and for this import necessary liabraries and titanic data","eac7e03a":"Find out importance of feature by giving model instance and dataframe to the rf_feat_importance method. After that, I decided which feature is important or not. According to my observation Name, Ticket, PassengerId, Embarked, Age_na, Fare are not important so that's why I am removing from dataframe and build model.","72563ff5":"In Pclass columns there are 3 categories named as upper class(1), second class(2) and third class(3). I am showing how many upper class, second class and thirs class present in the ship using bar graph","71d28663":"## Reducing Over-fitting","cbe21ed9":"The given data is randomised for this I am writing one simple function to split validation data from training data. Here, Validation data is 50% of testing data. 50% means 209 rows seprate from training data. ","64f711b7":"## Final model","673bd599":"Using bargraph showing how many people survived from ship. According to graph more than 50% people on the ship are died.","0a091d07":"### Tree building parameters","7a5e6efa":"## The data","ed955bbb":"## import data","5b8d2c31":"Is our validation set worse than our training set because we're over-fitting, or because the validation set is for a different time period, or a bit of both? With the existing information we've shown, we can't tell. However, random forests have a very clever trick called out-of-bag (OOB) error which can handle this (and more!)\n\nThe idea is to calculate error on the training set, but only include the trees in the calculation of a row's error where that row was not included in training that tree. This allows us to see whether the model is over-fitting, without needing a separate validation set.\n\nThis also has the benefit of allowing us to see whether our model generalizes, even if we only have a small amount of data so want to avoid separating some out to create a validation set.\n\nThis is as simple as adding one more parameter to our model constructor. We print the OOB error last in our print_score function below.","bf586b71":"## Out-of-bag ( OOB ) score","846fbace":"## Feature Importance","03ffac54":"## Testing model on test data","4d60e820":"When model behave good I am trying to tested on testing data. According to this model total 159 are survived from  testing data. In the last I am creating one submit.csv file according to the kaggle competition rule that contain only two columns named as PassengerId and Survived. ","027b3095":"\n\nShowing correlation between survived and embarked columns by using bargraph.","5734f53e":"Writting simple two functions to calculate RMSE(Root Mean Square Error) and accuracy of trainng and validation data. Here, print_score print the rmse and accuracy score of training and validation data. If we use oob_score parameter while fitting model then oob_score also print."}}