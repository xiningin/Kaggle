{"cell_type":{"3758c4a3":"code","fc72299d":"code","f30cb5a9":"code","ee1abd9a":"code","c6eb0548":"code","da829a20":"code","028f0996":"code","6695e83f":"code","bdd0a4cf":"code","d900b032":"code","60c94b4b":"code","1d11ee5c":"code","e1099f52":"markdown","495f7ded":"markdown","6217d857":"markdown","c1adeae5":"markdown","9626af06":"markdown","82ca30d4":"markdown"},"source":{"3758c4a3":"import random\nimport cv2\nfrom keras.datasets import cifar10\nfrom keras.utils import to_categorical\nfrom keras.models import Model\nfrom keras.layers import Dense, GlobalAveragePooling2D\nfrom keras.optimizers import SGD\nfrom keras.applications.inception_v3 import InceptionV3\nfrom keras.applications.resnet50 import ResNet50\nfrom keras.applications.vgg19 import VGG19\nfrom keras.applications.vgg16 import VGG16\nfrom keras.applications.xception import Xception\nimport numpy as np","fc72299d":"(x_train, y_train), (x_test, y_test) = cifar10.load_data()","f30cb5a9":"ind_train = random.sample(list(range(x_train.shape[0])), 1000)\nx_train = x_train[ind_train]\ny_train = y_train[ind_train]","ee1abd9a":"ind_test = random.sample(list(range(x_test.shape[0])), 1000)\nx_test = x_test[ind_test]\ny_test = y_test[ind_test]","c6eb0548":"def resize_data(data):\n    data_upscaled = np.zeros((data.shape[0], 320, 320, 3))\n    for i, img in enumerate(data):\n        large_img = cv2.resize(img, dsize=(320, 320), interpolation=cv2.INTER_CUBIC)\n        data_upscaled[i] = large_img\n\n    return data_upscaled","da829a20":"x_train_resized = resize_data(x_train)\nx_test_resized = resize_data(x_test)","028f0996":"y_train_hot_encoded = to_categorical(y_train)\ny_test_hot_encoded = to_categorical(y_test)","6695e83f":"def model(x_train, y_train, base_model):\n\n    # get layers and add average pooling layer\n    x = base_model.output\n    x = GlobalAveragePooling2D()(x)\n\n    # add fully-connected layer\n    x = Dense(512, activation='relu')(x)\n\n    # add output layer\n    predictions = Dense(10, activation='softmax')(x)\n\n    model = Model(inputs=base_model.input, outputs=predictions)\n\n    # freeze pre-trained model area's layer\n    for layer in base_model.layers:\n        layer.trainable = False\n\n    # update the weight that are added\n    model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n    model.fit(x_train, y_train)\n\n    # choose the layers which are updated by training\n    layer_num = len(model.layers)\n    for layer in model.layers[:int(layer_num * 0.9)]:\n        layer.trainable = False\n\n    for layer in model.layers[int(layer_num * 0.9):]:\n        layer.trainable = True\n\n    # update the weights\n    model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\n    history = model.fit(x_train, y_train, epochs=5)\n    return history","bdd0a4cf":"inception_model = InceptionV3(weights='imagenet', include_top=False)\nres_50_model = ResNet50(weights='imagenet', include_top=False)\nvgg_19_model = VGG19(weights='imagenet', include_top=False)\nvgg_16_model = VGG16(weights='imagenet', include_top=False)\nxception_model = Xception(weights='imagenet', include_top=False)","d900b032":"history_inception_v3 = model(x_train_resized, y_train_hot_encoded, inception_model)\nhistory_res_50 = model(x_train_resized, y_train_hot_encoded, res_50_model)\nhistory_vgg_19 = model(x_train_resized, y_train_hot_encoded, vgg_19_model)\nhistory_vgg_16 = model(x_train_resized, y_train_hot_encoded, vgg_16_model)\nhistory_xception = model(x_train_resized, y_train_hot_encoded, xception_model)","60c94b4b":"# check accuracy\nevaluation_inception_v3 = history_inception_v3.model.evaluate(x_test_resized,y_test_hot_encoded)\nevaluation_res_50 = history_res_50.model.evaluate(x_test_resized,y_test_hot_encoded)\nevaluation_vgg_19 = history_vgg_19.model.evaluate(x_test_resized,y_test_hot_encoded)\nevaluation_vgg_16 = history_vgg_16.model.evaluate(x_test_resized,y_test_hot_encoded)\nevaluation_xception = history_xception.model.evaluate(x_test_resized,y_test_hot_encoded)","1d11ee5c":"print(\"inception_v3:{}\".format(evaluation_inception_v3))\nprint(\"res_50:{}\".format(evaluation_res_50))\nprint(\"vgg_19:{}\".format(evaluation_vgg_19))\nprint(\"vgg_16:{}\".format(evaluation_vgg_16))\nprint(\"xception:{}\".format(evaluation_xception))","e1099f52":"limit the amount of the data","495f7ded":"resize train and  test data","6217d857":"**What is fine-tuning?**\n<p>Usually, deep learning model needs a massive amount of data for training. But it is not always easy to get enough amount of data for that. To be added, in many cases, it takes much time to make model from the viewpoint of training. I know you don\u2019t like to see one epoch of training using the time from sunrise to sunset. In some areas like image classification, you can use fine-tune method to solve this situation. <\/p>\n<p> For example, when you try to make image classification model, very deep CNN model works well(sometimes and other time not). To make that kind of model, it is necessary to prepare a huge amount of data. However, by using the model trained by other data, it is enough to add one or some layers to that model and train those. It saves much time and data. <\/p>","c1adeae5":"test data","9626af06":"make explained variable hot-encoded","82ca30d4":"read data"}}