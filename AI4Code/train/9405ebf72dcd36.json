{"cell_type":{"59dc7073":"code","fb6f1e79":"code","b865a2c8":"code","34206470":"code","2c001433":"code","ee70c1ef":"code","b3fb61b1":"code","d8362857":"code","cf4e197c":"code","bb4b68ed":"code","e0fe4e2e":"code","7a2a384d":"code","01dc0e31":"code","da39c6b7":"code","7abc79ea":"code","b7a0a722":"code","22e472a4":"code","fdc31dfc":"code","6ce456e7":"code","bd045b1f":"code","cb1cfe8c":"code","ad29fa1f":"code","6151ab15":"code","023715da":"code","13d1833a":"code","0d76caf2":"code","d39e6363":"code","cd835140":"code","4bcfd019":"code","3f5fd28a":"code","d0ca1421":"code","06d5d1f5":"code","69972437":"code","23dc905b":"code","9b2b6e41":"code","205866c3":"code","d1ae02fd":"code","4eceaa1b":"code","07a2db89":"code","4982341d":"code","ee40ca4f":"code","8308762f":"code","44d45f51":"code","d47a562f":"code","2c9472d0":"code","6699bb0e":"code","4d9c4e95":"code","681d2d28":"code","95438f44":"code","b901f0f8":"code","cfb5e86c":"code","1deb88b2":"markdown","52769281":"markdown","69094d13":"markdown","e906ad24":"markdown","14f4f474":"markdown","39aa8755":"markdown","195e3122":"markdown","145e1f13":"markdown","208b646a":"markdown","dbbd9d76":"markdown","f55d943f":"markdown","b71c0589":"markdown","56d80d8a":"markdown","fcb6489c":"markdown","6858b7fc":"markdown","f8e64683":"markdown","2b286b0b":"markdown","df9c9c8a":"markdown","30740ecf":"markdown","8c644ede":"markdown","84d21382":"markdown","9ef0d917":"markdown","dc702436":"markdown","e909a88b":"markdown","93f52946":"markdown","ecb4403a":"markdown","3b62ecb2":"markdown","f9532008":"markdown","bbc26c8c":"markdown","693a05ba":"markdown","8d0b70a2":"markdown","b33a600e":"markdown","c0a27e6f":"markdown","50967815":"markdown"},"source":{"59dc7073":"# Import Libraries\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as dates\nimport seaborn as sns\n# import chart_studio.plotly as py\nfrom datetime import datetime\nfrom scipy import stats\n# from geopy import Nominatim\n# from geopy.extra.rate_limiter import RateLimiter\n%matplotlib inline\n\npd.pandas.set_option('display.max_columns',None) # Alle Spalten der Dataframes anzeigen","fb6f1e79":"# Laden der Daten und zuweisen zu einer Variabeln um diese leichter abrufen zu k\u00f6nnen\nhousing_df = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\n\n# Struktur der Daten\nprint(housing_df.shape)\n\n\n# \u00dcberpr\u00fcfen der Merkmale und ihrer Wert-Typen\nhousing_df.info()","b865a2c8":"# Anzeigen der 5 Obersten Reihen des Datensatzes\nhousing_df.head()","34206470":"# Sammeln der genauer zu untersuchenden Merkmale in einer Liste\nfeatures_lst = ['MSZoning', 'LotArea','Neighborhood', 'BldgType', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd', 'TotalBsmtSF', '1stFlrSF', 'GrLivArea', 'FullBath', 'TotRmsAbvGrd', 'GarageCars', 'GarageArea', 'GarageCond', 'PoolArea', 'PoolQC', 'YrSold', 'SalePrice']","2c001433":"features_lst","ee70c1ef":"from IPython.display import display, HTML\n\n## Visualisieren der %-S\u00e4tze an fehlenden Werten je Merkmal\ncm = sns.light_palette(\"green\",as_cmap=True)\nplt.figure(figsize=(14,30))\n\nsns.heatmap(pd.DataFrame(housing_df.isnull().sum()\/housing_df.shape[0]*100),annot=True,cmap=sns.color_palette(\"Blues\"),\n           linewidth=2,linecolor=\"white\")\nplt.title(\"Ames Dataframe NA Values\")","b3fb61b1":"# Erstellen einer Liste von Merkmalen mit fehlenden Werten\nfeatures_with_nan=[features for features in housing_df.columns if housing_df[features].isnull().sum()>1]\n\n# Anzeigen der Gesamtanzahl der Merkmale mit fehlenden Werten und deren Prozentsatz an fehlenden Werten\nprint(\"Insgesamt weisen \", len(features_with_nan), \"Merkmale fehlende Werte auf.\\n\")\nprint(\"Folgende Merkmale sind davon betroffen:\")\nfor feature in features_with_nan:\n    print(feature, np.round(housing_df[feature].isnull().mean()*100, 2),' %')\n\n    \n# Abgleich der bisherigen Liste mit geeigneten Eingangsvariablen mit den Merkmalen die mehr als 1% Leerstellen aufweisen\nprint()\nprint(\"Merkmale die ausgeschlossen werden k\u00f6nnen:\")\nfor feature in features_lst:\n    for i in features_with_nan:\n        if (feature == i):\n            if (np.round(housing_df[i].isnull().mean()*100, 2)>=1):\n                print(feature)\n                features_lst.remove(feature)\n                break","d8362857":"features_lst","cf4e197c":"# Erstellen einer Liste numerischer Merkmale\nnumerical_features  = [feature for feature in housing_df[features_lst].columns if housing_df[feature].dtypes != 'O']\n\nprint('Anzahl numerischer Merkmale: ', len(numerical_features))\n\n# Ausgabe der 5 ersten Zeilen f\u00fcr diese Liste\nhousing_df[numerical_features].head()","bb4b68ed":"# Erstellen einer Liste der variablen mit Jahreswerten\nyear_feature = [feature for feature in features_lst if 'Yr' in feature or 'Year' in feature]\n\nhousing_df[year_feature].head()","e0fe4e2e":"# Analyse der Beziehung zwischen den Jahres Merkmalen und dem Verkaufspreis\n# Erstellen einer Kopie der Daten um den original Datensatz nicht zu ver\u00e4ndern\nyear_features_df = housing_df[year_feature].copy()\n\n# Umwandeln der Jahreswerte in Zeitabst\u00e4nde f\u00fcr eine bessere Vergleichbarkeit der Werte\nfor feature in year_feature[:2]:\n       \n    year_features_df[feature]=year_features_df['YrSold']-year_features_df[feature]\n\n# Hinzuf\u00fcgen des Verkaufspreises zum kopierten Datenset\nyear_features_df['SalePrice'] = housing_df['SalePrice']\n\n# Ausgabe eines Ausschnitts des neuen Datensets\nprint(year_features_df.head())\n\n# Analysieren der Beziehung zwischen den Merkmalen und dem Verkaufspreis\nsns.pairplot(year_features_df, diag_kind = 'kde')","7a2a384d":"# Erstellen einer Liste der diskreten Merkmale\ndiscrete_feature=[feature for feature in numerical_features if len(housing_df[feature].unique())<25 and feature not in year_feature+['Id']]\n\n# Anzahl der diskreten Merkmale\nprint(\"Discrete Variables Count: {}\".format(len(discrete_feature)))\n\n# Ausgabe der 5 ersten Zeilen f\u00fcr diese Liste\nhousing_df[discrete_feature].head()\n\n# Erstellen einer Liste die auch den Verkaufspreis beinhaltet\ndiscrete_feature_compare = discrete_feature.copy()\ndiscrete_feature_compare.append('SalePrice')\n\n# Analysieren der Beziehung zwischen den Merkmalen und dem Verkaufspreis\nsns.pairplot(housing_df[discrete_feature_compare], diag_kind = 'kde')","01dc0e31":"# Erstellen einer Liste der kontinuierlichen Merkmale\ncontinuous_feature=[feature for feature in numerical_features if feature not in discrete_feature+year_feature+['Id']]\n\n# Anzahl der selektierten Merkmale\nprint(\"Continuous feature Count {}\".format(len(continuous_feature)))\n\n\n# Ausgabe der 5 ersten Zeilen f\u00fcr diese Liste\nprint(housing_df[continuous_feature].head())\n\n# Analysieren der Beziehung zwischen den Merkmalen und dem Verkaufspreis\nsns.pairplot(housing_df[continuous_feature])","da39c6b7":"for feature in numerical_features:\n    data=housing_df[numerical_features].copy()\n    if 0 in data[feature].unique():\n        pass\n    else:\n        data[feature]=np.log(data[feature])\n        data.boxplot(column=feature)\n        plt.ylabel(feature)\n        plt.title(feature)\n        plt.show()","7abc79ea":"# Erstellen einer Liste kategorischer Merkmale\ncategorical_features=[feature for feature in housing_df[features_lst].columns if housing_df[feature].dtypes=='O']\n\n# Anzahl der selektierten Merkmale\nprint('Anzahl numerischer Merkmale: ', len(categorical_features))\n\n# Ausgabe der 5 ersten Zeilen f\u00fcr diese Liste\nhousing_df[categorical_features].head()\n\n# Anzahl der Kategorien je Merkmal (Kardinalit\u00e4t)\nfor feature in categorical_features:\n    print('Das Merkmal {} beinhaltet {} Kategorien'.format(feature,len(housing_df[feature].unique())))\n    \n# Analysieren der Beziehung zwischen den Merkmalen und dem Verkaufspreis\nfor feature in categorical_features:\n    data=housing_df.copy()\n    plt.figure(figsize=(10,5))\n    data.groupby(feature)['SalePrice'].median().plot.bar()\n    plt.xlabel(feature)\n    plt.ylabel('SalePrice')\n    plt.title(feature)\n    plt.show()","b7a0a722":"# Erstellen einer Kopie des Datensatzes um diesen nicht zu verf\u00e4lschen\nhousing_df_prepared = housing_df[features_lst].copy()\nhousing_df_prepared['SalePrice'] = housing_df['SalePrice']\nhousing_df_prepared.head()","22e472a4":"# Als erstes fehlende kategorische Werte\nfe_features_nan=[feature for feature in housing_df_prepared.columns if housing_df_prepared[feature].isnull().sum()>1 and housing_df_prepared[feature].dtypes=='O']\n\nprint(\"Es fehlen Werte in den folgenden Merkmalen; \\n\", fe_features_nan)\n\nfor feature in fe_features_nan:\n    print(\"{}: {}% missing values\".format(feature,np.round(housing_df_prepared[feature].isnull().mean(),4)))","fdc31dfc":"# Nun fehlende numerische Werte\nfe_numerical_with_nan=[feature for feature in housing_df_prepared.columns if housing_df_prepared[feature].isnull().sum()>1 and housing_df_prepared[feature].dtypes!='O']\n\nprint(\"Es fehlen Werte in den folgenden Merkmalen; \\n\", fe_features_nan)\n\nfor feature in fe_numerical_with_nan:\n    print(\"{}: {}% missing value\".format(feature,np.around(housing_df_prepared[feature].isnull().mean(),4)))","6ce456e7":"# Ersetzen der fehlenden numerischen Werte\n\nfor feature in fe_numerical_with_nan:\n    # Ersetzen der Werte mithilfe des Medians da es sich bei den fehlenden Werten um Ausreisser handelt\n    median_value=housing_df_prepared[feature].median()\n    \n    # Erstellen eines neuen features um alle nan Werte aufzunehmen\n    housing_df_prepared[feature+'nan']=np.where(housing_df_prepared[feature].isnull(),1,0)\n    housing_df_prepared[feature].fillna(median_value,inplace=True)\n    \nhousing_df_prepared[fe_numerical_with_nan].isnull().sum()","bd045b1f":"# Erstellen einer Liste der variablen mit Jahreswerten\nfe_year_feature = [feature for feature in features_lst if 'Yr' in feature or 'Year' in feature]\n\nhousing_df_prepared[fe_year_feature].head()","cb1cfe8c":"# Jahreszahlen umwandeln in vergleichbare Zeitabst\u00e4nde\n\nfor feature in fe_year_feature[:2]:\n       \n    housing_df_prepared[feature]=housing_df_prepared['YrSold']-housing_df_prepared[feature]\n    \nhousing_df_prepared[fe_year_feature].head()","ad29fa1f":"# Liste numerischer Merkmale\nfe_numerical_features = [feature for feature in housing_df_prepared.columns if housing_df_prepared[feature].dtypes != 'O']\n\n# Entfernen der Jahreswerte aus dieser Liste\nfor feature in fe_year_feature:\n    fe_numerical_features.remove(feature)\n\nprint('Number of numerical variables: ', len(fe_numerical_features))\n\n# visualise the numerical variables\nhousing_df_prepared[fe_numerical_features].head()","6151ab15":"# Entfernen des Verkaufspreises von der Liste der numerischen Werte um bei den Vorhersagen Dollarwerte zu sehen\n#fe_numerical_features.remove('SalePrice')\n\n# Anwenden des Logarithmus\nfor feature in fe_numerical_features:\n    housing_df_prepared[feature]=np.log(housing_df_prepared[feature]+1)\n        \n# visualise the numerical variables\nhousing_df_prepared[fe_numerical_features].head()","023715da":"# Erstellen einer Liste kategorischer Merkmale\ncategorical_features=[feature for feature in housing_df_prepared.columns if housing_df_prepared[feature].dtype=='O']\n\nprint(categorical_features)","13d1833a":"for feature in categorical_features:\n    temp=housing_df_prepared.groupby(feature)['SalePrice'].count()\/len(housing_df_prepared)\n    temp_df=temp[temp>0.01].index\n    housing_df_prepared[feature]=np.where(housing_df_prepared[feature].isin(temp_df),housing_df_prepared[feature],'Rare_var')","0d76caf2":"housing_df_prepared.head(100)","d39e6363":"housing_df_prepared[categorical_features].head()","cd835140":"for feature in categorical_features:\n    labels_ordered=housing_df_prepared.groupby([feature])['SalePrice'].mean().sort_values().index\n    labels_ordered={k:i for i,k in enumerate(labels_ordered,0)}\n    housing_df_prepared[feature]=housing_df_prepared[feature].map(labels_ordered)","4bcfd019":"housing_df_prepared[categorical_features].head()","3f5fd28a":"# Erstellen einer Liste der zu skalierenden Merkmale\nscaling_feature=[feature for feature in housing_df_prepared.columns if feature not in ['Id','SalePerice'] ]\nprint(len(scaling_feature))","d0ca1421":"from sklearn.preprocessing import MinMaxScaler\nscaler=MinMaxScaler()\nscaler.fit(housing_df_prepared[scaling_feature])","06d5d1f5":"scaler.transform(housing_df_prepared[scaling_feature])","69972437":"housing_df_prepared.head()","23dc905b":"# Definieren der Eingabevariablen (SalePrice als Zielvariable darf nicht unter den Eingabevariablen enthalten sein)\nfeatures_lst.remove('SalePrice')\nX = housing_df_prepared[features_lst]\n\n# Definieren der Zielvariablen\ny = housing_df_prepared['SalePrice']","9b2b6e41":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","205866c3":"### Aktuelle Features m\u00fcssen noch aufbereitet werden, String Values m\u00fcssen in floats umgewandelt werden damit das Modell mit Ihnen arbeiten kann. Betrifft die kategorischen Werte.\n\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import mean_absolute_error\n\n# Definieren des Models \nhousing_model_DT = DecisionTreeRegressor(random_state=1)\n\n# Trainieren des Modells\nhousing_model_DT.fit(X_train, y_train)\n\n# Prognosen f\u00fcr den Testdatensatz erstellen\ntest_predictions_DT = housing_model_DT.predict(X_test)\n\n# Ausgabe der obersten 10 Werte der Testprognosen\nprint('Erste Test Prognosen:', pd.DataFrame(test_predictions_DT).head(10))\n\n# Ausgabe der obersten 10 Werte des Testdatensatzes\nprint('Eigentliche Zielwerte f\u00fcr diese Immobilien: \\n', pd.DataFrame(y_test).head(10))\n\n# Berechnen des Mean Absolute Error (MAE)\n# MAE beschreibt die Abweichung der Prognose von den eigentlichen Zielwerten\nprint(mean_absolute_error(y_test, test_predictions_DT))\n\n# Scatter Plot\npd.DataFrame({'True Values': y_test, 'Predicted Values': test_predictions_DT}).plot.scatter(x='True Values', y='Predicted Values')\n\n# Residual Histogram\npd.DataFrame({'Error Values': (y_test - test_predictions_DT)}).plot.kde()","d1ae02fd":"from sklearn.ensemble import RandomForestRegressor\n\n# Definieren des Models \nhousing_model_forest= RandomForestRegressor(random_state=1)\n\n# Trainieren des Modells\nhousing_model_forest.fit(X_train, y_train)\n\n# Prognosen f\u00fcr den Testdatensatz erstellen\ntest_predictions_forest = housing_model_forest.predict(X_test)\n\n# Berechnen des Mean Absolute Error (MAE)\n# MAE beschreibt die Abweichung der Prognose von den eigentlichen Zielwerten\nprint(mean_absolute_error(y_test, test_predictions_forest))\n\n# Scatter Plot\npd.DataFrame({'True Values': y_test, 'Predicted Values': test_predictions_forest}).plot.scatter(x='True Values', y='Predicted Values')\n\n# Residual Histogram\npd.DataFrame({'Error Values': (y_test - test_predictions_forest)}).plot.kde()","4eceaa1b":"from sklearn.linear_model import LinearRegression\n\n# Definieren des Models \nhousing_model_lin_reg = LinearRegression(normalize=True)\n\n# Trainieren des Modells\nhousing_model_lin_reg.fit(X_train, y_train)\n\n# Prognosen f\u00fcr den Testdatensatz erstellen\ntest_predictions_lin_reg = housing_model_lin_reg.predict(X_test)\n\n# Berechnen des Mean Absolute Error (MAE)\n# MAE beschreibt die Abweichung der Prognose von den eigentlichen Zielwerten\nprint(mean_absolute_error(y_test, test_predictions_lin_reg))\n\n# Scatter Plot\npd.DataFrame({'True Values': y_test, 'Predicted Values': test_predictions_lin_reg}).plot.scatter(x='True Values', y='Predicted Values')\n\n# Residual Histogram\npd.DataFrame({'Error Values': (y_test - test_predictions_lin_reg)}).plot.kde()","07a2db89":"from sklearn import metrics\nfrom sklearn.model_selection import cross_val_score\n\ndef cross_val(model):\n    pred = cross_val_score(model, X, y, cv=10)\n    return pred.mean()\n\ndef print_evaluate(true, predicted):  \n    mae = metrics.mean_absolute_error(true, predicted)\n    mse = metrics.mean_squared_error(true, predicted)\n    rmse = np.sqrt(metrics.mean_squared_error(true, predicted))\n    r2_square = metrics.r2_score(true, predicted)\n    print('MAE:', mae)\n    print('MSE:', mse)\n    print('RMSE:', rmse)\n    print('R2 Square', r2_square)\n    print('__________________________________')\n    \ndef evaluate(true, predicted):\n    mae = metrics.mean_absolute_error(true, predicted)\n    mse = metrics.mean_squared_error(true, predicted)\n    rmse = np.sqrt(metrics.mean_squared_error(true, predicted))\n    r2_square = metrics.r2_score(true, predicted)\n    return mae, mse, rmse, r2_square","4982341d":"model_DT_test_pred = test_predictions_DT\nmodel_DT_train_pred = housing_model_DT.predict(X_train)\n\nprint('Test set evaluation:\\n_____________________________________')\nprint_evaluate(y_test, model_DT_test_pred)\nprint('Train set evaluation:\\n_____________________________________')\nprint_evaluate(y_train, model_DT_train_pred)","ee40ca4f":"# Feature Importance\nfeature_importance_tree = list(zip(X_train.columns ,housing_model_DT.feature_importances_))\n\ndtype = [('feature', 'S10'), ('importance', 'float')]\nfeature_importance_tree = np.array(feature_importance_tree, dtype=dtype)\nfeature_sort_tree = np.sort(feature_importance_tree, order = 'importance')[::-1]\nname, score = zip(*list(feature_sort_tree))\nprint(pd.DataFrame(feature_sort_tree[:20]))\npd.DataFrame({'name':name,'score':score})[:20].plot.bar(x='name', y='score', figsize=(12,8))\nplt.title('Gewichtung der Merkmale f\u00fcr das Modell Decision Tree')","8308762f":"# Erstellen einer Gesamtergebnis\u00fcbersicht und mit den Decision Tree Ergebnisse\nresults_df = pd.DataFrame(data=[[\"Decision Tree\", *evaluate(y_test, model_DT_test_pred) , cross_val(DecisionTreeRegressor())]], \n                          columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', \"Cross Validation\"])\n\n\nresults_df","44d45f51":"model_RF_test_pred = test_predictions_forest\nmodel_RF_train_pred = housing_model_forest.predict(X_train)\n\nprint('Test set evaluation:\\n_____________________________________')\nprint_evaluate(y_test, model_RF_test_pred)\nprint('Train set evaluation:\\n_____________________________________')\nprint_evaluate(y_train, model_RF_train_pred)","d47a562f":"# Feature Importance\nfeature_importance_forest = list(zip(X_train.columns ,housing_model_forest.feature_importances_))\n\ndtype = [('feature', 'S10'), ('importance', 'float')]\nfeature_importance_forest = np.array(feature_importance_forest, dtype=dtype)\nfeature_sort_forest = np.sort(feature_importance_forest, order = 'importance')[::-1]\nname, score = zip(*list(feature_sort_forest))\nprint(pd.DataFrame(feature_sort_forest[:20]))\npd.DataFrame({'name':name,'score':score})[:20].plot.bar(x='name', y='score', figsize=(12,8))\nplt.title('Gewichtung der Merkmale f\u00fcr das Modell Random Forest')","2c9472d0":"rf_results_df = pd.DataFrame(data=[[\"Random Forest\", *evaluate(y_test, model_RF_test_pred) , cross_val(DecisionTreeRegressor())]], \n                          columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', \"Cross Validation\"])\n\n# Erstellen einer Gesamtergebnis\u00fcbersicht und hinzuf\u00fcgen der Random Forest Ergebnisse\nresults_df = results_df.append(rf_results_df, ignore_index=True)\n\nrf_results_df","6699bb0e":"model_LR_test_pred = test_predictions_lin_reg \nmodel_LR_train_pred = housing_model_lin_reg.predict(X_train)\n\nprint('Test set evaluation:\\n_____________________________________')\nprint_evaluate(y_test, model_LR_test_pred)\nprint('Train set evaluation:\\n_____________________________________')\nprint_evaluate(y_train, model_LR_train_pred)","4d9c4e95":"lr_results_df = pd.DataFrame(data=[[\"Linear Regression\", *evaluate(y_test, model_LR_test_pred) , cross_val(LinearRegression())]], \n                          columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', \"Cross Validation\"])\n\n# Erstellen einer Gesamtergebnis\u00fcbersicht und hinzuf\u00fcgen der Linearen Regressions Ergebnisse\nresults_df = results_df.append(lr_results_df, ignore_index=True)\n\nlr_results_df","681d2d28":"# print the intercept\nprint(housing_model_lin_reg.intercept_)","95438f44":"coeff_df = pd.DataFrame(housing_model_lin_reg.coef_, X.columns, columns=['Coefficient'])\n\nprint(coeff_df.sort_values('Coefficient', ascending=False))\n\ncoeff_df['Coefficient'].sort_values(ascending=True).plot(kind='barh', figsize=(12, 8))","b901f0f8":"results_df","cfb5e86c":"results_df.set_index('Model', inplace=True)\nresults_df['R2 Square'].plot(kind='barh', figsize=(12, 8))","1deb88b2":"Aufteilen des vorliegenden Datensets in ein Trainings- und Testdatensatz. \nDas Modell wird mit Hilfe des Trainingsdatensatz trainiert und anschlie\u00dfend mit dem Testdatensatz evaluiert.","52769281":"**Fehlende Werte**","69094d13":"**Modell mit Random Forest**","e906ad24":"**Optimieren des Modells**\nDie Optimierung des bisherigen Modells w\u00e4re durch eine Anpassung der Zweige (\"leaves\") m\u00f6glich.","14f4f474":"**Vergleich der Modelle miteinander**","39aa8755":"Es zeigt sich eine Beziehung zwischen den meisten der ausgew\u00e4hlten Merkmalen und dem Verkaufspreis. \nDie Grafik f\u00fcr LotArea (Grundfl\u00e4che) wird aufgrund der Ausreisser stark verzerrt, diese m\u00fcssten f\u00fcr eine genauere Betrachtung der Beziehung ausgeschlossen werden.","195e3122":"Es zeigt sich, dass 18 Merkmale fehlende Werte beinhalten.\nVon den fehlenden Werten sind die Merkmale 'Alley', 'FireplaceQu', 'PoolQC', 'Fence' und 'MiscFeature' am st\u00e4rksten betroffen.\n\nUm eine realit\u00e4tsnahe Preisprognose zu erhalten sollten diese Werte genauer Untersucht wie diese sich auf den Verkaufspreis auswirken.\nDa hier nur die ein einfacher Leitfaden f\u00fcr die Entwicklung einer ML App zur Prognose von Hauspreisen erstellt gezeigt wird, werden diese Werte vom Training-Datenset ausgeschlossen um keine Fehler im Modell zu erzeugen. Die Merkmale 'GarageCond' und 'PoolQC' die zuvor als m\u00f6gliches Trainings-Merkmal identifiziert wurden, k\u00f6nnen somit von der Liste der Eingangsvariablen ausgeschlossen werden.\n\nDie aktuelle Liste der Eingangsvariablen lautet wie folgt:","145e1f13":"**Modell mit einer Linearen Regression**","208b646a":"**Diskrete Merkmale**","dbbd9d76":"**Entzerren der numerischen Zahlenwerte**\nDa die numerischen Variablen verzerrt sind, wird eine logarithmische Normalverteilung zur Anpassung durchgef\u00fchrt.","f55d943f":"Es zeigt sich eine Beziehung zwischen den ausgew\u00e4hlten Merkmalen und dem Verkaufspreis.","b71c0589":"Es zeigt sich, dass der 1460 H\u00e4user (Zeilen) und 81 Merkmale (Spalten) beinhaltet.\nDie Merkmale der H\u00e4user beinhalten sowohl numerische (Zahlen) als auch kategorische (Text) Werte.\nDie dem Datenset beiliegende Dokumentation (\"data description\") gibt eine genaue Erkl\u00e4rung aller Merkmale und deren Daten.\n\nBasierend auf den getroffenen Annahmen werden die folgenden Merkmale genauer untersucht:\n* MSZoning: Identifiziert die allgemeine Zoneneinteilung des Verkaufs.\n* LotArea: Grundst\u00fccksgr\u00f6\u00dfe in Quadratfu\u00df\n* Neighborhood: Physische Standorte innerhalb der Stadtgrenzen von Ames\n* BldgType: Immobilientyp\n* OverallQual: Bewertet das Gesamtmaterial und die Ausf\u00fchrung des Hauses\n* OverallCond: Bewertet den Gesamtzustand des Hauses\n* YearBuilt: Urspr\u00fcngliches Baudatum\n* YearRemodAdd: Umbaudatum (entspricht dem Baudatum, wenn keine Umbauten oder Erg\u00e4nzungen)\n* TotalBsmtSF: Gesamt-Quadratfu\u00df Kellerfl\u00e4che\n* 1stFlrSF: Quadratmeter im ersten Stock\n* GrLivArea: oberirdische Wohnfl\u00e4che (Quadratfu\u00df)\n* FullBath: AnVollst\u00e4ndige B\u00e4der \u00fcber dem Niveau\n* TotRmsAbvGrd: Gesamtzahl der Zimmer h\u00f6herwertig (ohne Badezimmer)\n* GarageCars: Gr\u00f6\u00dfe der Garage in Pkw-Kapazit\u00e4t\n* GarageArea: Garagengr\u00f6\u00dfe in Quadratmetern\n* GarageCond: Garagenzustand\n* PoolArea: Poolfl\u00e4che in Quadratmetern\n* PoolQC: Poolqualit\u00e4t\n* YrSold: Verkaufsjahr (Format YYYY)\n* SalePrice: Verkaufspreis in USD\n\n\nDie Dokumentation der Daten zeigt, dass die Datens\u00e4tze in unterschiedlichen Datenformaten vorliegen auch wenn sie vom gleichen Typ sind (z.B. LotArea in Quadratfu\u00df und PoolArea in QuadratMeter). Diese unterschiedlichen Formate k\u00f6nnen einen Einfluss auf das ML-Modell haben wie geringere Genauigkeit und neue Datens\u00e4tze m\u00fcssten genau gleiche Formatierungen aufweisen um eine konstante Prognosequalit\u00e4t zu erhalten.\n\nF\u00fcr diese Arbeit die das Vorgehen f\u00fcr die Entwicklung einer ML-App zeigt, wird f\u00fcr die Datenuntersuchung vereinfacht und nur die Korrelation dieser Werte mit der gesuchten Variable dem Verkaufspreis n\u00e4her untersucht um zu bestimmen ob diese als Teil des Trainings-Datensets f\u00fcr das ML-Modell geeignet ist.","56d80d8a":"# Evaluation","fcb6489c":"**Jahreszahlen umwandeln**","6858b7fc":"**Modell mit linearer Regression**","f8e64683":"**Model mit Decision Tree**","2b286b0b":"**Daten Analysis**\n\nIm Rahmen der Datenanalyse werden die Daten der vorselektierten Liste wie folgt untersucht:\n1. Fehlende Werte\n1. Numerische Merkmale\n   1. Zeit Merkmale (z.B. Jahreszahlen, Datumsangaben)\n   1. diskrete Merkmale\n   1. Kontinuierliche Merkmale\n   1. Verteilung der numerischen Merkmale\n   1. Ausreisser\n1. Kategorische Merkmale Merkmale\n   1. Kardinalit\u00e4t kategorischer Merkmale\n1. Beziehung zwischen unabh\u00e4ngigen (Eingabevariablen) und abh\u00e4ngigen Merkmalen (Zielvariable)","df9c9c8a":"**Pr\u00fcfen der fehlenden Werte**\n\n\u00dcberpr\u00fcfen des Prozentsatzs an NaN-Werte (fehlende Werte) in jedem Feature.","30740ecf":"**Umwandeln der kategorischen Werte von Text- in Zahlenwerte**","8c644ede":"**Skalieren der Merkmale**","84d21382":"F\u00fcr die Evaluation der Modelle erfolgt mit den g\u00e4ngigen Evalutionsmetriken f\u00fcr Regressions Probleme, diese sind:\n\n> - **Mean Absolute Error** (MAE) ist der Mittelwert der Absolutwerte der Fehler:\n$$\\frac 1n\\sum_{i=1}^n|y_i-\\hat{y}_i|$$\n\n> - **Mean Squared Error** (MSE) ist der Mittelwert der quadrierten Fehler:\n$$\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2$$\n\n> - **Root Mean Squared Error** (RMSE) ist die Quadratwurzel des Mittelwerts der quadrierten Fehler:\n$$\\sqrt{\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}$$\n\n> Vergleich dieser Metriken:\n- **MAE**  ist am einfachsten zu verstehen, da es sich um den durchschnittlichen Fehler handelt.\n- **MSE**  ist beliebter als MAE, da MSE gr\u00f6\u00dfere Fehler \"bestraft\", was in der realen Welt sich eher als n\u00fctzlich erweist.\n- **RMSE** ist noch beliebter als MSE, da RMSE in den \"y\"-Einheiten interpretiert werden kann.\n\n> Alle dieser Funktionen sind **Verlustfunktionen** mit dem Ziel deren Ergebniswert zu minimieren.","9ef0d917":"Es zeigt sich eine Beziehung zwischen den ausgew\u00e4hlten Merkmalen und dem Verkaufspreis.","dc702436":"**Entfernen seltener kategorischer Werte**\nErsetzen von kategorischen Variablen, die weniger als 1% der Beobachtungen ausmachen mit einer Sammelvariablen.","e909a88b":"## Einleitung\n\nZiel dieser ML App ist es ein Preismodell zu entwickeln, das H\u00e4userwerte in der Stadt Ames sch\u00e4tzt und Hausbesitzern oder Kaufinteressenten mehr Preis-Transparenz bietet.\n\n# Ziel\n1. Trainieren eines Modells f\u00fcr maschinelles Lernen, um zuk\u00fcnftige Immobilienverkaufspreise in Ames, Iowa vorherzusagen\n1. Erstellen eines einfachen Leitfaden f\u00fcr die Entwicklung einer ML App zur Prognose von Hauspreisen\n\n# Zielanwender\n* Privatpersonen (Kaufinteressenten)\n* Hauseigent\u00fcmer\n* KMUs im Immoblilienbereich ohne eigene Programmierer oder gr\u00f6\u00dferer IT-Abteilung\n\n# Ergebniszusammenfassung\nBasierend auf den vorliegenden Datensatz kann durch Anwendung von Regressionstechniken wie lineare Regression, Random Forest oder Gradient Boosting ein ML-Modell trainiert werden zur Vorhersage von Immobilienpreisen.\n\n\n# Inhalte dieses Notebooks\n\n* Gesch\u00e4ftsverst\u00e4ndnis\n* Daten Untersuchung: Explorative Datenanalyse der vorliegenden Daten\n* Daten Vorbereitung: Bereinigen und zusammenf\u00fchren der Daten\n* Auswahl der Merkmale: Definieren der Merkmale f\u00fcr das Trainingset des ML-Modells\n* Auswahl und Training des ML-Modells\n* Evaluation der Modell Performance\n\n# Gesch\u00e4ftsverst\u00e4ndnis\n\n**Zielgruppe:** Privatpersonen wie Kaufinteressenten oder Hauseigent\u00fcmer und in der Immobilienwirtschaft t\u00e4tige KMUs (kleine bis mittelgro\u00dfe Unternehmen)\n\n**Problemdefinition:** Genaue Bestimmung des Kauf- bzw. Verkaufspreis von Immobilien\n\n**Zielvariable:** Verkaufspreis\n\n**Grundlegende Hypothese:** Der Wert einer Immobilie ist abh\u00e4ngig von den Eigenschaften der Immobilie und deren Qualit\u00e4t, der Lage der Immobilie und der wirtschaftlichen Situation der K\u00e4ufer-\/Verk\u00e4uferparteien.\n\n**Eingabe Varibalen**\nUm die Anzahl der Eingabevariablen einzugrenzen, sowie die weitere Datenanalyse zu erleichtern und damit verbundene Aufw\u00e4nde zu reduzieren, werden folgende Annahmen getroffen:\n\n1. **Art der Immobilie (z.B. Haus oder Wohnung):** H\u00e4user sind teurer als Wohnungen\n1. **Grundfl\u00e4che der Immobilie (Grundfl\u00e4che):** Mehr Fl\u00e4che f\u00fchrt zu h\u00f6heren Preisen\n1. **Alter der Immobilie:** Neubauten sind teurer als Bestandsgeb\u00e4ude\n1. **Zustand der Immobilie:** Neuwertige Immobilien mit Materialien hoher Qualit\u00e4tsg\u00fcte l\u00e4sst sich teurer ver-kaufen als bauf\u00e4llige mit mangelnder Qualit\u00e4tsg\u00fcte.\n1. **Zeitraum der letzten Modernisierungs-\/Renovierungsma\u00dfnahmen:** Ma\u00dfnahmen die zum Immobilienerhalt beitragen f\u00fchren zu h\u00f6heren Verkaufspreisen\n1. **Anzahl der Zimmer:** gr\u00f6\u00dfere Anzahl an Zimmer f\u00fchren zu h\u00f6heren Verkaufs-preisen\n1. **Luxus- oder funktionelle Einrichtungen:** wie Garagen, Gartenfl\u00e4che oder Pools f\u00fchren zu h\u00f6heren Verkaufspreisen\n1. **Lage der Immobilie:** eine Immobilie in der Innenstadt eines Ballungszentrums (z.B. M\u00fcnchen) ist teurer als in einem kleinen Dorf auf dem Land\n1. **Nachbarschaft bzw. kulturelles Umfeld:** Abh\u00e4ngig von der umliegenden Nachbarschaft kann die Immobilie einen h\u00f6heren oder niedrigeren Verkaufspreis haben als eine vergleichbare Immobilie in einer anderen Nachbarschaft (z.B. eine Immobilie in Mitten eines sozialen Brennpunkt Viertels oder am Rande eines geruchsintensiven verarbeitenden Industriegebiets wird g\u00fcnstiger verkauft werden als in einem Wohnviertel mit gehoben Sozial- und Wohnstandards) \n1. **Verkaufsjahr:** zu Zeiten einer Wirtschaftskriese sind Immobilien g\u00fcnstiger zu er-werben als zu Zeiten von niedrigen Zinsen\n\n# Daten Untersuchung\n\n**Datensatz \"Ames Housing dataset\"**\n\nDer Datensatz \"Ames Housing dataset\" stammt von der Kaggle.com Competition \"House Prices - Advanced Regression Techniques\" und ist unter diesem Link zu finden https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\n\nDer Datensatz enth\u00e4lt 79 erkl\u00e4rte Merkmale, die ann\u00e4hernd jeden Aspekt von Wohnh\u00e4usern in Ames, Iowa (USA) beschreiben. Daher eigent er sich sehr gut daf\u00fcr ein ML-Modell basierend auf diesen Daten f\u00fcr die oben genannte Problembeschreibung zu trainieren.\n\n\n**Importieren der ben\u00f6tigten Python Libaries**","93f52946":"**Ausreisser in den numerischen Merkmalen**","ecb4403a":"# Modelling","3b62ecb2":"**Zeit Merkmale (z.B. Jahreszahlen, Datumsangaben)**","f9532008":"**Spezifizieren und trainieren eines random forest Modells**","bbc26c8c":"**Kategorische Merkmale**","693a05ba":"# Data Preparation\nW\u00e4hrend der Datenvorbereitung werden die Eingangsvariablen festgelegt und in ein passendes Format f\u00fcr das Modell gebracht.\nDa die Aufgabe des Modells ist eine Prognose f\u00fcr einen Zahlenwert zu treffen (ein sog. Regressions-Aufgabe) m\u00fcssen alle Eingangsvariablen ebenfalls Zahlenwerte sein.\n\nVorbereiten der Daten f\u00fcr die Modellierung mittels Regressionsverfahren\n\n* **Linear Assumption:** Bei der linearen Regression wird davon ausgegangen, dass die Beziehung zwischen Ihrer Eingabe und Ausgabe linear ist. Es unterst\u00fctzt nichts anderes. Dies mag offensichtlich sein, aber es ist gut, sich daran zu erinnern, wenn Sie viele Attribute haben. M\u00f6glicherweise m\u00fcssen Sie Daten transformieren, um die Beziehung linear zu machen (z. B. Log-Transformation f\u00fcr eine exponentielle Beziehung).\n* **Remove Noise:** Bei der linearen Regression wird davon ausgegangen, dass Ihre Eingabe- und Ausgabevariablen nicht verrauscht sind. Ziehen Sie in Erw\u00e4gung, Datenbereinigungsvorg\u00e4nge zu verwenden, mit denen Sie das Signal in Ihren Daten besser aufdecken und kl\u00e4ren k\u00f6nnen. Dies ist am wichtigsten f\u00fcr die Ausgabevariable und Sie m\u00f6chten Ausrei\u00dfer in der Ausgabevariable (y) nach M\u00f6glichkeit entfernen.\n* **Remove Collinearity:** Die lineare Regression passt Ihre Daten zu stark an, wenn Sie stark korrelierte Eingabevariablen haben. Ziehen Sie in Betracht, paarweise Korrelationen f\u00fcr Ihre Eingabedaten zu berechnen und die am st\u00e4rksten korrelierten zu entfernen.\n* **Gaussian Distributions:**. Die lineare Regression liefert zuverl\u00e4ssigere Vorhersagen, wenn Ihre Eingabe- und Ausgabevariablen eine Gau\u00dfsche Verteilung aufweisen. Sie k\u00f6nnen einige Vorteile erzielen, indem Sie Transformationen (z. B. log oder BoxCox) f\u00fcr Ihre Variablen verwenden, um ihre Verteilung gau\u00dfscher aussehen zu lassen.\n* **Rescale Inputs:** Die lineare Regression liefert oft zuverl\u00e4ssigere Vorhersagen, wenn Sie Eingabevariablen mithilfe von Standardisierung oder Normalisierung neu skalieren.\n\nFolgende Ma\u00dfnahmen werden im nachfolgenden durchgef\u00fchrt:\n\n1. Fehlende Werte ausgleichen\n1. Jahreszahlen in vergleichbare zeitliche Variablen (Zeitabst\u00e4nde) umwandeln\n1. Entzerren der numerischen Werte, da die unterschiedlichen Gr\u00f6\u00dfen der Merkmalswerte das Modell ungewollt beeinflussen k\u00f6nnte\n1. Entfernen von seltenen Werten in kategorische Variablen\n1. Umwandeln der kategorische Variablen von Text-Werten (string) zu Zahlen (float)\n1. Standardisieren Sie die Werte der Variablen auf den gleichen Bereich","8d0b70a2":"**Kontinuierliche Merkmale**","b33a600e":"**Specifizieren und trainieren eines Entscheidungsbaum-Modells**","c0a27e6f":"**Laden der Daten und schaffen einer \u00dcbersicht \u00fcber die Sturktur dieser**","50967815":"**Numerische Merkmale**"}}