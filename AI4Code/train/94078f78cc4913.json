{"cell_type":{"c87cc796":"code","67bf0847":"code","ef2ed11c":"code","9e4c4f91":"code","cf600afd":"code","82101e05":"code","50738d70":"code","678b5711":"code","a06cd660":"code","af54a1f0":"code","1cad9816":"code","5426ee5b":"code","d4f6e087":"code","2c4c3f0b":"code","615bccea":"code","bd054bf2":"code","ccf4c272":"code","8417cd29":"code","72875273":"code","26bbe04b":"code","0f4179b0":"code","c7c9f4b7":"code","b85ea5d1":"code","3f3295d0":"code","2be719dc":"code","fe34f94b":"code","b0a2ac54":"code","d1c5f39a":"code","afd352e6":"code","9fd80606":"code","5dc85008":"code","f24e4618":"code","6f6d7e05":"code","05217735":"code","9b11f18d":"code","03cbcca1":"code","5f2c11c3":"code","479e4b2f":"code","7f92cd7c":"code","e3537325":"code","f22fc75b":"code","2087bb67":"code","f84fa5b6":"code","351e250c":"code","d193b7a7":"code","cb6eccd2":"code","6ba656b0":"code","7337b627":"code","bc62a1ec":"code","1d08642c":"code","45f01c29":"code","bc948c20":"code","4ca89e11":"code","47b8f393":"code","305a37ae":"code","3562a634":"code","c0238f0e":"code","778c4035":"code","d6f917e1":"code","953bb783":"code","a105dd58":"code","6c3c3f03":"code","3e059dde":"code","5701af16":"code","b651364b":"code","8d1516df":"code","f206ad75":"code","e2cc38bb":"code","b6af984e":"code","213f6ec7":"code","917706fb":"code","e96c964d":"markdown","d5c98ca5":"markdown","9c9be80d":"markdown","fd97a03f":"markdown"},"source":{"c87cc796":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n#!pip install chart_studio\n#import chart_studio.plotly as py\n#import chart_studio.plotly as go\n#import plotly.plotly as py\n#import plotly.graph_objs as go\n#from plotly.offline import init_notebook_mode, iplot\n\nimport catboost\nfrom catboost import Pool\nfrom catboost import CatBoostRegressor\nfrom xgboost import XGBRegressor\nfrom xgboost import plot_importance\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","67bf0847":"sample = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/sample_submission.csv')\nshops = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/shops.csv')\nitems = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/items.csv')\nitem_categories = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/item_categories.csv')\ntrain = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/sales_train.csv', parse_dates=['date'])\ntest = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/test.csv')","ef2ed11c":"test.head(5)","9e4c4f91":"train.head(5)","cf600afd":"items.head(5)","82101e05":"item_categories.head(5)","50738d70":"train_data = train.join(items, on='item_id', rsuffix='_').join(shops, on='shop_id', rsuffix='_').join(item_categories, on='item_category_id', rsuffix='_').drop(['item_id_', 'shop_id_', 'item_category_id_'], axis=1)","678b5711":"train_data","a06cd660":"train_data.isnull().sum()","af54a1f0":"test.isnull().sum()","1cad9816":"test_shops = test.shop_id.unique()\ntrain_data = train_data[train_data.shop_id.isin(test_shops)]\ntest_items = test.item_id.unique()\ntrain_data = train_data[train_data.item_id.isin(test_items)]","5426ee5b":"train_data","d4f6e087":"#Features = ['date', 'date_block_num', 'shop_id', 'item_category_id', 'item_id', 'item_price', 'item_cnt_day']","2c4c3f0b":"train_monthly = train_data[['date', 'date_block_num', 'shop_id', 'item_category_id', 'item_id', 'item_price', 'item_cnt_day']]","615bccea":"# Group by month in this case \"date_block_num\" and aggregate features.\ntrain_monthly = train_monthly.sort_values('date').groupby(['date_block_num', 'shop_id', 'item_category_id', 'item_id'], as_index=False)\ntrain_monthly = train_monthly.agg({'item_price':['sum', 'mean'], 'item_cnt_day':['sum', 'mean','count']})\n# Rename features.\ntrain_monthly.columns = ['date_block_num', 'shop_id', 'item_category_id', 'item_id', 'item_price', 'mean_item_price', 'item_cnt', 'mean_item_cnt', 'transactions']","bd054bf2":"# Build a data set with all the possible combinations of ['date_block_num','shop_id','item_id'] so we won't have missing records.\nshop_ids = train_monthly['shop_id'].unique()\nitem_ids = train_monthly['item_id'].unique()\nempty_df = []\nfor i in range(34):\n    for shop in shop_ids:\n        for item in item_ids:\n            empty_df.append([i, shop, item])\n    \nempty_df = pd.DataFrame(empty_df, columns=['date_block_num','shop_id','item_id'])","ccf4c272":"# Merge the train set with the complete set (missing records will be filled with 0).\ntrain_monthly = pd.merge(empty_df, train_monthly, on=['date_block_num','shop_id','item_id'], how='left')\ntrain_monthly.fillna(0, inplace=True)","8417cd29":"# Extract time based features.\ntrain_monthly['year'] = train_monthly['date_block_num'].apply(lambda x: ((x\/\/12) + 2013))\ntrain_monthly['month'] = train_monthly['date_block_num'].apply(lambda x: (x % 12))","72875273":"# Grouping data for EDA.\ngp_month_mean = train_monthly.groupby(['month'], as_index=False)['item_cnt'].mean()\ngp_month_sum = train_monthly.groupby(['month'], as_index=False)['item_cnt'].sum()\ngp_category_mean = train_monthly.groupby(['item_category_id'], as_index=False)['item_cnt'].mean()\ngp_category_sum = train_monthly.groupby(['item_category_id'], as_index=False)['item_cnt'].sum()\ngp_shop_mean = train_monthly.groupby(['shop_id'], as_index=False)['item_cnt'].mean()\ngp_shop_sum = train_monthly.groupby(['shop_id'], as_index=False)['item_cnt'].sum()","26bbe04b":"train_monthly = train_monthly.query('item_cnt >= 0 and item_cnt <= 20 and item_price < 400000')","0f4179b0":"train_monthly","c7c9f4b7":"train_monthly['item_cnt_month'] = train_monthly.sort_values('date_block_num').groupby(['shop_id', 'item_id'])['item_cnt'].shift(-1)","b85ea5d1":"train_monthly['item_price_unit'] = train_monthly['item_price'] \/\/ train_monthly['item_cnt']\ntrain_monthly['item_price_unit'].fillna(0, inplace=True)","3f3295d0":"gp_item_price = train_monthly.sort_values('date_block_num').groupby(['item_id'], as_index=False).agg({'item_price':[np.min, np.max]})\ngp_item_price.columns = ['item_id', 'hist_min_item_price', 'hist_max_item_price']\n\ntrain_monthly = pd.merge(train_monthly, gp_item_price, on='item_id', how='left')","2be719dc":"train_monthly['price_increase'] = train_monthly['item_price'] - train_monthly['hist_min_item_price']\ntrain_monthly['price_decrease'] = train_monthly['hist_max_item_price'] - train_monthly['item_price']","fe34f94b":"# Min value\nf_min = lambda x: x.rolling(window=3, min_periods=1).min()\n# Max value\nf_max = lambda x: x.rolling(window=3, min_periods=1).max()\n# Mean value\nf_mean = lambda x: x.rolling(window=3, min_periods=1).mean()\n# Standard deviation\nf_std = lambda x: x.rolling(window=3, min_periods=1).std()\n\nfunction_list = [f_min, f_max, f_mean, f_std]\nfunction_name = ['min', 'max', 'mean', 'std']\n\nfor i in range(len(function_list)):\n    train_monthly[('item_cnt_%s' % function_name[i])] = train_monthly.sort_values('date_block_num').groupby(['shop_id', 'item_category_id', 'item_id'])['item_cnt'].apply(function_list[i])\n\n# Fill the empty std features with 0\ntrain_monthly['item_cnt_std'].fillna(0, inplace=True)","b0a2ac54":"lag_list = [1, 2, 3]\n\nfor lag in lag_list:\n    ft_name = ('item_cnt_shifted%s' % lag)\n    train_monthly[ft_name] = train_monthly.sort_values('date_block_num').groupby(['shop_id', 'item_category_id', 'item_id'])['item_cnt'].shift(lag)\n    # Fill the empty shifted features with 0\n    train_monthly[ft_name].fillna(0, inplace=True)","d1c5f39a":"train_monthly['item_trend'] = train_monthly['item_cnt']\n\nfor lag in lag_list:\n    ft_name = ('item_cnt_shifted%s' % lag)\n    train_monthly['item_trend'] -= train_monthly[ft_name]\n\ntrain_monthly['item_trend'] \/= len(lag_list) + 1","afd352e6":"train_set = train_monthly.query('date_block_num >= 3 and date_block_num < 28').copy()\nvalidation_set = train_monthly.query('date_block_num >= 28 and date_block_num < 33').copy()\ntest_set = train_monthly.query('date_block_num == 33').copy()\n\ntrain_set.dropna(subset=['item_cnt_month'], inplace=True)\nvalidation_set.dropna(subset=['item_cnt_month'], inplace=True)\n\ntrain_set.dropna(inplace=True)\nvalidation_set.dropna(inplace=True)\n\nprint('Train set records:', train_set.shape[0])\nprint('Validation set records:', validation_set.shape[0])\nprint('Test set records:', test_set.shape[0])\n\nprint('Train set records: %s (%.f%% of complete data)' % (train_set.shape[0], ((train_set.shape[0]\/train_monthly.shape[0])*100)))\nprint('Validation set records: %s (%.f%% of complete data)' % (validation_set.shape[0], ((validation_set.shape[0]\/train_monthly.shape[0])*100)))","9fd80606":"# Shop mean encoding.\ngp_shop_mean = train_set.groupby(['shop_id']).agg({'item_cnt_month': ['mean']})\ngp_shop_mean.columns = ['shop_mean']\ngp_shop_mean.reset_index(inplace=True)\n# Item mean encoding.\ngp_item_mean = train_set.groupby(['item_id']).agg({'item_cnt_month': ['mean']})\ngp_item_mean.columns = ['item_mean']\ngp_item_mean.reset_index(inplace=True)\n# Shop with item mean encoding.\ngp_shop_item_mean = train_set.groupby(['shop_id', 'item_id']).agg({'item_cnt_month': ['mean']})\ngp_shop_item_mean.columns = ['shop_item_mean']\ngp_shop_item_mean.reset_index(inplace=True)\n# Year mean encoding.\ngp_year_mean = train_set.groupby(['year']).agg({'item_cnt_month': ['mean']})\ngp_year_mean.columns = ['year_mean']\ngp_year_mean.reset_index(inplace=True)\n# Month mean encoding.\ngp_month_mean = train_set.groupby(['month']).agg({'item_cnt_month': ['mean']})\ngp_month_mean.columns = ['month_mean']\ngp_month_mean.reset_index(inplace=True)\n\n# Add meand encoding features to train set.\ntrain_set = pd.merge(train_set, gp_shop_mean, on=['shop_id'], how='left')\ntrain_set = pd.merge(train_set, gp_item_mean, on=['item_id'], how='left')\ntrain_set = pd.merge(train_set, gp_shop_item_mean, on=['shop_id', 'item_id'], how='left')\ntrain_set = pd.merge(train_set, gp_year_mean, on=['year'], how='left')\ntrain_set = pd.merge(train_set, gp_month_mean, on=['month'], how='left')\n# Add meand encoding features to validation set.\nvalidation_set = pd.merge(validation_set, gp_shop_mean, on=['shop_id'], how='left')\nvalidation_set = pd.merge(validation_set, gp_item_mean, on=['item_id'], how='left')\nvalidation_set = pd.merge(validation_set, gp_shop_item_mean, on=['shop_id', 'item_id'], how='left')\nvalidation_set = pd.merge(validation_set, gp_year_mean, on=['year'], how='left')\nvalidation_set = pd.merge(validation_set, gp_month_mean, on=['month'], how='left')","5dc85008":"# Create train and validation sets and labels. \nX_train = train_set.drop(['item_cnt_month', 'date_block_num'], axis=1)\nY_train = train_set['item_cnt_month'].astype(int)\nX_validation = validation_set.drop(['item_cnt_month', 'date_block_num'], axis=1)\nY_validation = validation_set['item_cnt_month'].astype(int)","f24e4618":"# Integer features (used by catboost model).\nint_features = ['shop_id', 'item_id', 'year', 'month']\n\nX_train[int_features] = X_train[int_features].astype('int32')\nX_validation[int_features] = X_validation[int_features].astype('int32')","6f6d7e05":"latest_records = pd.concat([train_set, validation_set]).drop_duplicates(subset=['shop_id', 'item_id'], keep='last')\nX_test = pd.merge(test, latest_records, on=['shop_id', 'item_id'], how='left', suffixes=['', '_'])\nX_test['year'] = 2015\nX_test['month'] = 9\nX_test.drop('item_cnt_month', axis=1, inplace=True)\nX_test[int_features] = X_test[int_features].astype('int32')\nX_test = X_test[X_train.columns]","05217735":"sets = [X_train, X_validation, X_test]\n\n# This was taking too long.\n# Replace missing values with the median of each item.\n# for dataset in sets:\n#     for item_id in dataset['item_id'].unique():\n#         for column in dataset.columns:\n#             item_median = dataset[(dataset['item_id'] == item_id)][column].median()\n#             dataset.loc[(dataset[column].isnull()) & (dataset['item_id'] == item_id), column] = item_median\n\n# Replace missing values with the median of each shop.            \nfor dataset in sets:\n    for shop_id in dataset['shop_id'].unique():\n        for column in dataset.columns:\n            shop_median = dataset[(dataset['shop_id'] == shop_id)][column].median()\n            dataset.loc[(dataset[column].isnull()) & (dataset['shop_id'] == shop_id), column] = shop_median\n            \n# Fill remaining missing values on test set with mean.\nX_test.fillna(X_test.mean(), inplace=True)","9b11f18d":"# I'm dropping \"item_category_id\", we don't have it on test set and would be a little <hard to create categories for items that exist only on test set.\nX_train.drop(['item_category_id'], axis=1, inplace=True)\nX_validation.drop(['item_category_id'], axis=1, inplace=True)\nX_test.drop(['item_category_id'], axis=1, inplace=True)","03cbcca1":"import datetime\nimport warnings","5f2c11c3":"#cat_features = [0, 1, 7, 8]\n\ncatboost_model = CatBoostRegressor(\n    iterations=500,\n    max_ctr_complexity=4,\n    random_seed=0,\n    od_type='Iter',\n    od_wait=25,\n    verbose=50,\n    depth=4\n)\n\ncatboost_model.fit(\n    X_train, Y_train,\n    #cat_features=cat_features,\n    eval_set=(X_validation, Y_validation)\n)","479e4b2f":"import matplotlib.pyplot as plt\nimport seaborn as sns","7f92cd7c":"feature_score = pd.DataFrame(list(zip(X_train.dtypes.index, catboost_model.get_feature_importance(Pool(X_train, label=Y_train)))), columns=['Feature','Score']) #, cat_features=cat_features\nfeature_score = feature_score.sort_values(by='Score', ascending=False, inplace=False, kind='quicksort', na_position='last')\n\nplt.rcParams[\"figure.figsize\"] = (19, 6)\nax = feature_score.plot('Feature', 'Score', kind='bar', color='c')\nax.set_title(\"Catboost Feature Importance Ranking\", fontsize = 14)\nax.set_xlabel('')\nrects = ax.patches\nlabels = feature_score['Score'].round(2)\n\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width()\/2, height + 0.35, label, ha='center', va='bottom')\n\nplt.show()","e3537325":"catboost_train_pred = catboost_model.predict(X_train)\ncatboost_val_pred = catboost_model.predict(X_validation)\ncatboost_test_pred = catboost_model.predict(X_test)","f22fc75b":"print('Train rmse:', np.sqrt(mean_squared_error(Y_train, catboost_train_pred)))\nprint('Validation rmse:', np.sqrt(mean_squared_error(Y_validation, catboost_val_pred)))","2087bb67":"def model_performance_sc_plot(predictions, labels, title):\n    # Get min and max values of the predictions and labels.\n    min_val = max(max(predictions), max(labels))\n    max_val = min(min(predictions), min(labels))\n    # Create dataframe with predicitons and labels.\n    performance_df = pd.DataFrame({\"Label\":labels})\n    performance_df[\"Prediction\"] = predictions\n    # Plot data\n    sns.jointplot(y=\"Label\", x=\"Prediction\", data=performance_df, kind=\"reg\", height=7)\n    plt.plot([min_val, max_val], [min_val, max_val], 'm--')\n    plt.title(title, fontsize=9)\n    plt.show()\n    \n# model_performance_sc_plot(catboost_train_pred, Y_train, 'Train')\n#model_performance_sc_plot(catboost_val_pred, Y_validation, 'Validation')","f84fa5b6":"# Use only part of features on XGBoost.\nxgb_features = ['item_cnt','item_cnt_mean', 'item_cnt_std', 'item_cnt_shifted1', \n                'item_cnt_shifted2', 'item_cnt_shifted3', 'shop_mean', \n                'shop_item_mean', 'item_trend', 'mean_item_cnt']\nxgb_train = X_train[xgb_features]\nxgb_val = X_validation[xgb_features]\nxgb_test = X_test[xgb_features]","351e250c":"xgb_model = XGBRegressor(max_depth=8, \n                         n_estimators=500, \n                         min_child_weight=1000,  \n                         colsample_bytree=0.7, \n                         subsample=0.7, \n                         eta=0.3, \n                         seed=0)\nxgb_model.fit(xgb_train, \n              Y_train, \n              eval_metric=\"rmse\", \n              eval_set=[(xgb_train, Y_train), (xgb_val, Y_validation)], \n              verbose=20, \n              early_stopping_rounds=20)","d193b7a7":"plt.rcParams[\"figure.figsize\"] = (15, 6)\nplot_importance(xgb_model)\nplt.show()","cb6eccd2":"xgb_train_pred = xgb_model.predict(xgb_train)\nxgb_val_pred = xgb_model.predict(xgb_val)\nxgb_test_pred = xgb_model.predict(xgb_test)","6ba656b0":"# model_performance_sc_plot(xgb_train_pred, Y_train, 'Train')\n#model_performance_sc_plot(xgb_val_pred, Y_validation, 'Validation')","7337b627":"# Use only part of features on random forest.\nrf_features = ['shop_id', 'item_id', 'item_cnt', 'transactions', 'year',\n               'item_cnt_mean', 'item_cnt_std', 'item_cnt_shifted1', \n               'shop_mean', 'item_mean', 'item_trend', 'mean_item_cnt']\nrf_train = X_train[rf_features]\nrf_val = X_validation[rf_features]\nrf_test = X_test[rf_features]","bc62a1ec":"rf_model = RandomForestRegressor(n_estimators=50, max_depth=7, random_state=0, n_jobs=-1)\nrf_model.fit(rf_train, Y_train)","1d08642c":"rf_train_pred = rf_model.predict(rf_train)\nrf_val_pred = rf_model.predict(rf_val)\nrf_test_pred = rf_model.predict(rf_test)","45f01c29":"# model_performance_sc_plot(rf_train_pred, Y_train, 'Train')\n#model_performance_sc_plot(rf_val_pred, Y_validation, 'Validation')","bc948c20":"# Use only part of features on linear Regression.\nlr_features = ['item_cnt', 'item_cnt_shifted1', 'item_trend', 'mean_item_cnt', 'shop_mean']\nlr_train = X_train[lr_features]\nlr_val = X_validation[lr_features]\nlr_test = X_test[lr_features]","4ca89e11":"lr_scaler = MinMaxScaler()\nlr_scaler.fit(lr_train)\nlr_train = lr_scaler.transform(lr_train)\nlr_val = lr_scaler.transform(lr_val)\nlr_test = lr_scaler.transform(lr_test)","47b8f393":"lr_model = LinearRegression(n_jobs=-1)\nlr_model.fit(lr_train, Y_train)","305a37ae":"lr_train_pred = lr_model.predict(lr_train)\nlr_val_pred = lr_model.predict(lr_val)\nlr_test_pred = lr_model.predict(lr_test)","3562a634":"print('Train rmse:', np.sqrt(mean_squared_error(Y_train, lr_train_pred)))\nprint('Validation rmse:', np.sqrt(mean_squared_error(Y_validation, lr_val_pred)))","c0238f0e":"# model_performance_sc_plot(lr_train_pred, Y_train, 'Train')\n#model_performance_sc_plot(lr_val_pred, Y_validation, 'Validation')","778c4035":"# Use only part of features on KNN.\nknn_features = ['item_cnt', 'item_cnt_mean', 'item_cnt_std', 'item_cnt_shifted1',\n                'item_cnt_shifted2', 'shop_mean', 'shop_item_mean', \n                'item_trend', 'mean_item_cnt']\n\n# Subsample train set (using the whole data was taking too long).\nX_train_sampled = X_train[:100000]\nY_train_sampled = Y_train[:100000]\n\nknn_train = X_train_sampled[knn_features]\nknn_val = X_validation[knn_features]\nknn_test = X_test[knn_features]","d6f917e1":"knn_scaler = MinMaxScaler()\nknn_scaler.fit(knn_train)\nknn_train = knn_scaler.transform(knn_train)\nknn_val = knn_scaler.transform(knn_val)\nknn_test = knn_scaler.transform(knn_test)","953bb783":"knn_model = KNeighborsRegressor(n_neighbors=9, leaf_size=13, n_jobs=-1)\nknn_model.fit(knn_train, Y_train_sampled)","a105dd58":"knn_train_pred = knn_model.predict(knn_train)\nknn_val_pred = knn_model.predict(knn_val)\nknn_test_pred = knn_model.predict(knn_test)","6c3c3f03":"print('Train rmse:', np.sqrt(mean_squared_error(Y_train_sampled, knn_train_pred)))\nprint('Validation rmse:', np.sqrt(mean_squared_error(Y_validation, knn_val_pred)))","3e059dde":"# model_performance_sc_plot(knn_train_pred, Y_train_sampled, 'Train')\n#model_performance_sc_plot(knn_val_pred, Y_validation, 'Validation')","5701af16":"# Dataset that will be the train set of the ensemble model.\nfirst_level = pd.DataFrame(catboost_val_pred, columns=['catboost'])\nfirst_level['xgbm'] = xgb_val_pred\nfirst_level['random_forest'] = rf_val_pred\nfirst_level['linear_regression'] = lr_val_pred\nfirst_level['knn'] = knn_val_pred\nfirst_level['label'] = Y_validation.values\nfirst_level.head(20)","b651364b":"# Dataset that will be the test set of the ensemble model.\nfirst_level_test = pd.DataFrame(catboost_test_pred, columns=['catboost'])\nfirst_level_test['xgbm'] = xgb_test_pred\nfirst_level_test['random_forest'] = rf_test_pred\nfirst_level_test['linear_regression'] = lr_test_pred\nfirst_level_test['knn'] = knn_test_pred\nfirst_level_test.head()","8d1516df":"meta_model = LinearRegression(n_jobs=-1)","f206ad75":"# Drop label from dataset.\nfirst_level.drop('label', axis=1, inplace=True)\nmeta_model.fit(first_level, Y_validation)\n","e2cc38bb":"ensemble_pred = meta_model.predict(first_level)\nfinal_predictions = meta_model.predict(first_level_test)","b6af984e":"print('Train rmse:', np.sqrt(mean_squared_error(ensemble_pred, Y_validation)))","213f6ec7":"#model_performance_sc_plot(ensemble_pred, Y_validation, 'Validation')","917706fb":"prediction_df = pd.DataFrame(test['ID'], columns=['ID'])\nprediction_df['item_cnt_month'] = final_predictions.clip(0., 20.)\nprediction_df.to_csv('submission.csv', index=False)\nprediction_df.head(10)","e96c964d":"def series_to_supervised(data, window=1, lag=1, dropnan=True):\n    cols, names = list(), list()\n    # Input sequence (t-n, ... t-1)\n    for i in range(window, 0, -1):\n        cols.append(data.shift(i))\n        names += [('%s(t-%d)' % (col, i)) for col in data.columns]\n    # Current timestep (t=0)\n    cols.append(data)\n    names += [('%s(t)' % (col)) for col in data.columns]\n    # Target timestep (t=lag)\n    cols.append(data.shift(-lag))\n    names += [('%s(t+%d)' % (col, lag)) for col in data.columns]\n    # Put it all together\n    agg = pd.concat(cols, axis=1)\n    agg.columns = names\n    # Drop rows with NaN values\n    if dropnan:\n        agg.dropna(inplace=True)\n    return agg","d5c98ca5":"Result= []\nfor col in Features:\n    dp = train_data.filter(like=col)\n    Result.append(dp)\n    df = pd.concat((Result),axis=1)","9c9be80d":"lag_size = (test['date'].max().date() - train['date'].max().date()).days\nprint('Max date from train set: %s' % train['date'].max().date())\nprint('Max date from test set: %s' % test['date'].max().date())\nprint('Forecast lag size', lag_size)","fd97a03f":"window = 29\nlag = lag_size\nseries = series_to_supervised(train_monthly.drop('date', axis=1), window=window, lag=lag)\nseries.head()"}}