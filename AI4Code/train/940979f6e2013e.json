{"cell_type":{"f4edae4a":"code","f60b81b1":"code","31afd0f2":"code","149f99f0":"code","cba5bb57":"code","ea962b09":"code","2ad09ba8":"code","690e0b39":"code","90788d3c":"code","4839b8cf":"code","5f5c4a24":"code","9821b8d7":"code","6c21c745":"markdown","748a3c01":"markdown","d906229a":"markdown","d1493091":"markdown","31bff4a7":"markdown","ff9e9bcc":"markdown","c68b5d64":"markdown","2846f284":"markdown","c26b1af7":"markdown","2ac7042b":"markdown","b2fb8ef9":"markdown","f188e488":"markdown"},"source":{"f4edae4a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import svm\nfrom sklearn import tree\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\ndata = pd.read_csv(\"..\/input\/data.csv\",header=0)\n# Any results you write to the current directory are saved as output.\nprint(data.columns)\nprint(data.shape)","f60b81b1":"data.drop(['Unnamed: 32', 'id'], axis=1, inplace=True)\nprint(data.columns)  # To check if columns are dropped","31afd0f2":"print(data['diagnosis'].value_counts())","149f99f0":"def accuracy_predictor(model, data):\n    train = data.drop('diagnosis', axis=1)\n    label = data['diagnosis']\n    X_train, X_test, y_train, y_test = train_test_split(train,\n                                                        label,\n                                                        test_size=0.3,\n                                                        random_state=0)\n    clf = model.fit(X_train, y_train)\n    print(\"Using all features %f\" % clf.score(X_test, y_test))","cba5bb57":"clf = svm.SVC(kernel='linear', C=1)\naccuracy_predictor(clf, data)","ea962b09":"import matplotlib.pyplot as plt\nimport seaborn as sns\n# I will make use of all features which are labelled as *.mean \nfeatures_mean = ['radius_mean', 'texture_mean', 'perimeter_mean',\n                 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n                 'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean']\ncorr = data[features_mean].corr()\nplt.figure(figsize=(8,8))\nsns.heatmap(corr, cbar = True,  square = True, annot=True, fmt= '.2f',annot_kws={'size': 8},\n           xticklabels= features_mean, yticklabels= features_mean,\n           cmap= 'coolwarm') \nplt.show()\n","2ad09ba8":"def accuracy_predictor(model, data):\n    train = data.drop('diagnosis', axis=1)\n    label = data['diagnosis']\n    X_train, X_test, y_train, y_test = train_test_split(train,\n                                                        label,\n                                                        test_size=0.3,\n                                                        random_state=0)\n    clf = model.fit(X_train, y_train)\n    print(\"Using all features %f\" % clf.score(X_test, y_test))\n    unique_mean_features = ['radius_mean', 'texture_mean', 'smoothness_mean',\n                            'compactness_mean', 'symmetry_mean',\n                            'fractal_dimension_mean']\n    new_train = data[unique_mean_features]\n    X_train, X_test, y_train, y_test = train_test_split(new_train,\n                                                        label,\n                                                        test_size=0.3,\n                                                        random_state=0)\n    clf = model.fit(X_train, y_train)\n    print(\"With independent mean features: %f\" % clf.score(X_test, y_test))\n","690e0b39":"clf = svm.SVC(kernel='linear', C=1)\naccuracy_predictor(clf, data)","90788d3c":"clf = tree.DecisionTreeClassifier()\naccuracy_predictor(clf, data)","4839b8cf":"knn = KNeighborsClassifier(n_neighbors=5,\n                           algorithm='ball_tree'\n                          )\naccuracy_predictor(knn, data)","5f5c4a24":"for neighbor in range(3, 20):\n    print(\"Iteration %d\" % neighbor)\n    knn = KNeighborsClassifier(n_neighbors=neighbor,\n                               algorithm='ball_tree'\n                               )\n    accuracy_predictor(knn, data)","9821b8d7":"from scipy.interpolate import interp1d\nimport matplotlib.pyplot as plt\n\nx = range(3, 20)\n# y1 and y2 are scores when using all features and unique features respectively.\n# I made slight changes in the function and appended the scores in two lists.\n# I am not posting that basic code.\ny1 = [0.9181286549707602, 0.9298245614035088, 0.9473684210526315, 0.9473684210526315, 0.9532163742690059, 0.9532163742690059,\n      0.9590643274853801, 0.9649122807017544, 0.9649122807017544, 0.9649122807017544,\n      0.9649122807017544, 0.9649122807017544, 0.9649122807017544, 0.9649122807017544,\n      0.9649122807017544, 0.9649122807017544, 0.9649122807017544]\ny2 = [0.8421052631578947,\n      0.8596491228070176, 0.8771929824561403, 0.8830409356725146, 0.8888888888888888,\n      0.9005847953216374, 0.9064327485380117, 0.8888888888888888, 0.8947368421052632,\n      0.8947368421052632, 0.8947368421052632, 0.9005847953216374, 0.8947368421052632,\n      0.8947368421052632, 0.9005847953216374, 0.9005847953216374, 0.9064327485380117]\nf1 = interp1d(x, y1, kind='cubic')\nf2 = interp1d(x, y2, kind='cubic')\nplt.plot(x, f1(x), '-', x, f2(x), '--')\nplt.legend(['all features', 'unique mean features'], loc='best')\nplt.show()","6c21c745":"By all features, I mean that we are using all the columns for prediction even if they are highly correlated. To make it more optimized, we will see how much different columns are correlated to each other, and make use of only useful features.","748a3c01":"Let's define a function to predict the accuracy and use that function on different models. I am going to use **train_test_split** function that conveniently splits the data into train and test data respectively. It produces four kinds of data:\nX_train & y_train are training samples and labels.\nX_test & y_test are testing samples and labels.\nThen we will set the **test_size** to 0.3 to make use of 30% of data for testing. We will fit the model on training data and later calculate its score with the testing data.","d906229a":"Now you can easily see from here which k value gives highest score, without manually looking the ouput. This process is nothing but hyper-parameter optimization. \n\nHyper-parameters are parameters that are not directly learnt within estimators. In scikit-learn they are passed as arguments to the constructor of the estimator classes.\nHere k is that parameter.\n\nYou can use different values for different model classes to understand which value gives the best possible score.\nYou can try the above function on different classification algorithms too and test which model can be used best on this data.\n\n\n\n","d1493091":"The first column is nothing but the label values that we want to predict. It has 2 classes of labels : M = malignant, B = benign. So we will predict using classification techniques of sklearn.","31bff4a7":"Decision Trees here are giving less score than SVMs, so obviously SVMs seem better. Lets try k Nearest Neighbors now.\nIn the model for kNN, we can use different algorithms like 'auto', 'ball_tree', 'kd_tree' or 'brute'. You can study which one to use and why from http:\/\/scikit-learn.org\/stable\/modules\/neighbors.html\nLet us set neighbors of 5 for our prediction.","ff9e9bcc":"This heatmap shows how highly radius, parameter and area are related. Additionally, concavity, concave points and compactness are related. So lets choose any one from each and the remaining ones and make changes in our function. ","c68b5d64":"You can see now how much score changes when we change the number of neighbors used. \nConclusions:\n\n1) Using all features, k = 10 seems to optimize the prediction giving 96.49% score.\n\n2) Using only uniques mean features, k = 9 gives 90.64%\n\nI will make simple curve to show how scores vary with this parameter k.","2846f284":"Lets try again on SVM.","c26b1af7":"Score is lower than SVMs. So you will say SVM wins. But if you notice, there is **n_neighbors** which is actually playing the game here. So we will try the prediction score taking different values for k.","2ac7042b":"This data contains 569 rows and 33 columns. As you can see, the last **Unnamed: 32** column is meaningless to us. So we need to drop it, along with the **id** column which are of no use for prediction.\n","b2fb8ef9":"Lets test our function using support vector classifier.","f188e488":"Now I will make use of different classification model i.e. Decision Trees."}}