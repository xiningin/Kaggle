{"cell_type":{"ffea368a":"code","326f5854":"code","4fa5eae5":"code","37cf5407":"code","ae75fe5f":"code","1f715cab":"code","44815d87":"code","5b6b1865":"code","c7fb09ba":"code","6f5e12b6":"code","0434d672":"code","19e974fc":"code","43f7ca12":"code","02bbf1fe":"code","f00e408e":"code","28f34dd4":"code","0a5114e1":"code","d9f91a09":"code","02c00a26":"code","db9af359":"code","e40a6a38":"code","7b2c517f":"code","64da0031":"code","7ed94d15":"code","0220b470":"code","da1e796b":"code","e88d4194":"code","917fb8e9":"code","8a90a775":"markdown","26a797a7":"markdown","1c1feabf":"markdown"},"source":{"ffea368a":"import numpy as np\nimport pandas as pd\nfrom IPython.display import display\n\n\npd.set_option(\"display.max_columns\", None)\n\ntrain_df = pd.read_csv(\"..\/input\/tabular-playground-series-dec-2021\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/tabular-playground-series-dec-2021\/test.csv\")\nsubmission_df = pd.read_csv(\"..\/input\/tabular-playground-series-dec-2021\/sample_submission.csv\")","326f5854":"display(train_df.head())","4fa5eae5":"train_df.drop([\"Id\"], axis = 1, inplace = True)","37cf5407":"print(train_df.columns[train_df.nunique() == 1])\nprint(test_df.columns[test_df.nunique() == 1])","ae75fe5f":"train_df.drop([\"Soil_Type7\", \"Soil_Type15\"], axis=1, inplace=True)\ntest_df.drop([\"Soil_Type7\", \"Soil_Type15\"], axis=1, inplace=True)","1f715cab":"from sklearn.preprocessing import LabelEncoder\n\n\nenc = LabelEncoder()\ntrain_df[\"Cover_Type\"] = enc.fit_transform(train_df[\"Cover_Type\"])","44815d87":"new_names = {\n    \"Horizontal_Distance_To_Hydrology\": \"x_dist_hydrlgy\",\n    \"Vertical_Distance_To_Hydrology\": \"y_dist_hydrlgy\",\n    \"Horizontal_Distance_To_Roadways\": \"x_dist_rdwys\",\n    \"Horizontal_Distance_To_Fire_Points\": \"x_dist_firepts\"\n}\n\ntrain_df.rename(new_names, axis=1, inplace=True)\ntest_df.rename(new_names, axis=1, inplace=True)","5b6b1865":"print(train_df[\"Aspect\"].aggregate([min, max]))\nprint(test_df[\"Aspect\"].aggregate([min, max]))","c7fb09ba":"train_df[\"Aspect\"][train_df[\"Aspect\"] < 0] += 360\ntrain_df[\"Aspect\"][train_df[\"Aspect\"] > 359] -= 360\n\ntest_df[\"Aspect\"][test_df[\"Aspect\"] < 0] += 360\ntest_df[\"Aspect\"][test_df[\"Aspect\"] > 359] -= 360","6f5e12b6":"print(train_df[\"Aspect\"].aggregate([min, max]))\nprint(test_df[\"Aspect\"].aggregate([min, max]))","0434d672":"# Manhhattan distance\ntrain_df[\"mnhttn_dist_hydrlgy\"] = np.abs(train_df[\"x_dist_hydrlgy\"]) + np.abs(train_df[\"y_dist_hydrlgy\"])\ntest_df[\"mnhttn_dist_hydrlgy\"] = np.abs(test_df[\"x_dist_hydrlgy\"]) + np.abs(test_df[\"y_dist_hydrlgy\"])\n\n# Euclidean distance\ntrain_df[\"ecldn_dist_hydrlgy\"] = (train_df[\"x_dist_hydrlgy\"]**2 + train_df[\"y_dist_hydrlgy\"]**2)**0.5\ntest_df[\"ecldn_dist_hydrlgy\"] = (test_df[\"x_dist_hydrlgy\"]**2 + test_df[\"y_dist_hydrlgy\"]**2)**0.5","19e974fc":"times = [\"9am\", \"Noon\", \"3pm\"]\nfor time in times:\n    print(train_df[f\"Hillshade_{time}\"].aggregate([min, max]))\n    print(test_df[f\"Hillshade_{time}\"].aggregate([min, max]))","43f7ca12":"for time in times:\n    train_df[f\"Hillshade_{time}\"][train_df[f\"Hillshade_{time}\"] < 0] = 0\n    test_df[f\"Hillshade_{time}\"][test_df[f\"Hillshade_{time}\"] < 0] = 0\n    \n    train_df[f\"Hillshade_{time}\"][train_df[f\"Hillshade_{time}\"] > 255] = 255\n    test_df[f\"Hillshade_{time}\"][test_df[f\"Hillshade_{time}\"] > 255] = 255","02bbf1fe":"for time in times:\n    print(train_df[f\"Hillshade_{time}\"].aggregate([min, max]))\n    print(test_df[f\"Hillshade_{time}\"].aggregate([min, max]))","f00e408e":"features_Hillshade = ['Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm']\nsoil_features = [x for x in train_df.columns if x.startswith(\"Soil_Type\")]\nwilderness_features = [x for x in train_df.columns if x.startswith(\"Wilderness_Area\")]\n\n\ndef addFeature(df):\n    df[\"Soil_Count\"] = df[soil_features].apply(sum, axis=1)\n    df[\"Wilderness_Area_Count\"] = df[wilderness_features].apply(sum, axis=1)\n    df[\"Hillshade_mean\"] = df[features_Hillshade].mean(axis=1)\n    df['amp_Hillshade'] = df[features_Hillshade].max(axis=1) - df[features_Hillshade].min(axis=1)","28f34dd4":"addFeature(train_df)\naddFeature(test_df)","0a5114e1":"cols_to_scale = [\n    \"Elevation\",\n    \"Aspect\",\n    \"mnhttn_dist_hydrlgy\",\n    \"ecldn_dist_hydrlgy\",\n    \"Slope\",\n    \"x_dist_hydrlgy\",\n    \"y_dist_hydrlgy\",\n    \"x_dist_rdwys\",\n    \"Hillshade_9am\",\n    \"Hillshade_Noon\",\n    \"Hillshade_3pm\",\n    \"x_dist_firepts\",\n    \"Soil_Count\",\n    \"Wilderness_Area_Count\",\n    \"Hillshade_mean\",\n    \"amp_Hillshade\"\n]","d9f91a09":"from sklearn.preprocessing import RobustScaler\n\nscaler = RobustScaler()\ntrain_df[cols_to_scale] = scaler.fit_transform(train_df[cols_to_scale])\ntest_df[cols_to_scale] = scaler.transform(test_df[cols_to_scale])","02c00a26":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int8','int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2\n\n    for col in df.columns:\n        col_type = df[col].dtypes\n\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n\n    if verbose:\n        print(\n            'Mem. usage decreased to {:5.2f} Mb ({:.2f}% reduction)'.format(\n                end_mem, 100 * (start_mem - end_mem) \/ start_mem\n            )\n        )\n \n    return df","db9af359":"train_df = reduce_mem_usage(train_df)\ntest_df = reduce_mem_usage(test_df)","e40a6a38":"import tensorflow as tf","7b2c517f":"INPUT_SHAPE = test_df.shape[1:]\nNUM_CLASSES = train_df[\"Cover_Type\"].nunique()","64da0031":"class self_norm_NN(tf.keras.Model):\n    \n    def __init__(self, inp_shape, num_classes, kernel_init = \"lecun_normal\", act = \"selu\"):\n        super(self_norm_NN, self).__init__()\n        self.d1 = tf.keras.layers.Dense(\n            units = 300, kernel_initializer = kernel_init, \n            activation = act, input_shape = inp_shape\n        )\n        self.bn1 = tf.keras.layers.BatchNormalization()\n        self.d2 = tf.keras.layers.Dense(\n            units = 200, kernel_initializer = kernel_init, \n            activation = act\n        )\n        self.bn2 = tf.keras.layers.BatchNormalization()\n        self.d3 = tf.keras.layers.Dense(\n            units = 100, kernel_initializer = kernel_init, \n            activation = act\n        )\n        self.bn3 = tf.keras.layers.BatchNormalization()\n        self.d4 = tf.keras.layers.Dense(\n            units = 50, kernel_initializer = kernel_init, \n            activation = act\n        )\n        self.bn4 = tf.keras.layers.BatchNormalization()\n        self.classifier = tf.keras.layers.Dense(\n            units = num_classes, \n            activation = \"softmax\"\n        )\n        \n    def call(self, input_tensor):\n        x = self.d1(input_tensor)\n        x = self.bn1(x)\n        \n        x = self.d2(x)\n        x = self.bn2(x)\n        \n        x = self.d3(x)\n        x = self.bn3(x)\n        \n        x = self.d4(x)\n        x = self.bn4(x)\n        \n        return self.classifier(x)","7ed94d15":"from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n\n\nreduce_lr = ReduceLROnPlateau(\n    monitor=\"val_loss\",\n    factor=0.5,\n    patience=5\n)\n\nearly_stop = EarlyStopping(\n    monitor=\"val_accuracy\",\n    patience=20,\n    restore_best_weights=True\n)\n\ncallbacks = [reduce_lr, early_stop]","0220b470":"test_df.drop([\"Id\"], axis = 1, inplace = True)","da1e796b":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\n\n\nX = train_df.drop(\"Cover_Type\", axis=1).values\ny = train_df[\"Cover_Type\"].values\n\ndel train_df\n\nFOLDS = 20\nEPOCHS = 200\nBATCH_SIZE = 2048\n\ntest_preds = np.zeros((1, 1))\nscores = []\n\ncv = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=42)\n\nfor fold, (train_idx, val_idx) in enumerate(cv.split(X, y)):\n    X_train, X_val = X[train_idx], X[val_idx]\n    y_train, y_val = y[train_idx], y[val_idx]\n    \n    model = self_norm_NN(INPUT_SHAPE, NUM_CLASSES)\n    \n    model.compile(\n        optimizer=\"adam\",\n        loss=\"sparse_categorical_crossentropy\",\n        metrics=[\"accuracy\"]\n    )\n    \n    model.fit(\n        X_train,\n        y_train,\n        validation_data = (X_val, y_val),\n        epochs = EPOCHS,\n        batch_size = BATCH_SIZE,\n        callbacks = callbacks,\n        verbose = True\n    )\n\n    y_pred = np.argmax(model.predict(X_val), axis = 1)\n    score = accuracy_score(y_val, y_pred)\n    scores.append(score)\n\n    test_preds = test_preds + model.predict(test_df)\n    print(f\"Fold {fold} Accuracy: {score}\")\n\nprint()\nprint(f\"Mean Accuracy: {np.mean(scores)}\")","e88d4194":"test_preds = np.argmax(test_preds, axis=1)\ntest_preds = enc.inverse_transform(test_preds)\n\nsubmission_df['Cover_Type'] = test_preds\ndisplay(submission_df.head())","917fb8e9":"submission_df.to_csv(\"submission.csv\", index = False)","8a90a775":"Aspect - values in degree ranging from 0 to 359","26a797a7":"Hillshading computes surface illumination as values from 0 to 255 based on a given compass direction to the sun (azimuth) and a certain altitude above the horizon (altitude)","1c1feabf":"Self-Normalizing Neural Networks (SNNs) are neural networks which automatically keep their activations at zero-mean and unit-variance (per neuron). This is accomplished through the use of SeLU activation function which requires LeCun Normal kernel initialization."}}