{"cell_type":{"6ca15c69":"code","0ffab8e3":"code","e6c5a914":"code","4a7a1feb":"code","2ba79b38":"code","5cf60d53":"code","a7b16055":"code","1c071a98":"code","b09df4b4":"code","b3f88e22":"code","bab4c19d":"code","d7276473":"code","2d7cf6f0":"code","ade65e64":"code","9c512656":"code","477caf7e":"code","d1fbc916":"code","e0523b16":"code","6d4e2244":"code","a89522de":"code","9f6edf89":"code","13c32403":"code","2b59c0fc":"markdown","e103975e":"markdown","b82422bb":"markdown","e2779ee3":"markdown","9f771e2f":"markdown","09e7fd9b":"markdown","188c0877":"markdown","bcb1bc9f":"markdown","2da78b79":"markdown","2f06bd34":"markdown","6ae7d541":"markdown","a4aab82d":"markdown","9a7245ea":"markdown","91a5973c":"markdown","c26aef3b":"markdown"},"source":{"6ca15c69":"import math \nfrom matplotlib import pyplot\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom numpy.linalg import inv\nfrom sklearn.decomposition import NMF\nfrom matplotlib.pyplot import plot, axis, show\nimport warnings\nwarnings.filterwarnings('ignore') # Ignore warnings\n\n# The following commands import our functions as a package\nimport sys\nsys.path.append(\"..\/src\/\")\nimport my_functions","0ffab8e3":"# Readings Data\ndata_path = \"..\/input\/movielens-25m-dataset\/ml-25m\/\"\ngenome_scores = pd.read_csv(data_path + \"genome-scores.csv\")\ngenome_tags = pd.read_csv(data_path + \"genome-tags.csv\")\nlinks = pd.read_csv(data_path + \"links.csv\")\nmovies = pd.read_csv(data_path + \"movies.csv\")\nratings = pd.read_csv(data_path + \"ratings.csv\")\ntags = pd.read_csv(data_path + \"tags.csv\")","e6c5a914":"plt.rcParams['figure.figsize'] = [8, 4] # Size of plots","4a7a1feb":"sns.countplot(x=\"rating\", data=ratings, color=\"red\")\nplt.title(\"Distribution of Ratings\")","2ba79b38":"# Print General Info of the Dataset\nprint(\"There are \" + str(len(pd.unique(tags.tag))) + \" distinct tags\")\nprint(\"There are \" + str(len(pd.unique(ratings.userId))) + \" users\")\nprint(\"There are \" + str(len(pd.unique(ratings.movieId))) + \" movies\")","5cf60d53":"########### Plot 1 ################\nplt.subplot(2, 1, 1)\nnumber_of_ratings = ratings[\"movieId\"].value_counts().to_frame()\nsns.distplot(number_of_ratings[\"movieId\"])\nplt.xlabel(\"How many times a movie have been rated\")\nplt.title(\"How many time a movie have been rated different movies\")\n########### Plot 2 ################\nplt.subplot(2, 1, 2)\nnumber_of_ratings = ratings[\"movieId\"].value_counts().to_frame()\nnumber_of_ratings = number_of_ratings[number_of_ratings[\"movieId\"]<100]\nsns.distplot(number_of_ratings[\"movieId\"])\nplt.xlabel(\"times rated\")","a7b16055":"########### Plot 1 ################\nplt.subplot(2, 1, 1)\nnumber_of_ratings = ratings[\"userId\"].value_counts().to_frame()\nsns.distplot(number_of_ratings[\"userId\"])\nplt.xlabel(\"Number of times rating\")\nplt.title(\"How many movies a user have rated\")\n########### Plot 2 ################\nplt.subplot(2, 1, 2)\nsns.distplot(number_of_ratings[number_of_ratings[\"userId\"] < 200])\nplt.xlabel(\"How many movies a user have rated\")","1c071a98":"userId = 12 # userId that we will recommend movies to\n\n# 1. Get the ratings and movies genres in same table\n\nmatrix = pd.merge(ratings,movies,on='movieId',how='left') # Left join to have movies and ratings in same table\ntable = matrix[matrix['userId'] == userId].movieId.tolist() # Movies that the userId have seen\nmatrix = matrix[matrix['userId'] == userId] \nmatrix['rating'] = 1 # Obtain a boolean, 1 if it has seen the movie, 0 if not","b09df4b4":"# 2. Split the genres categories in to multiples (To obtain the correspondant matrix)\n\nmatrix[['0', '1', '2', '3', '4', '5', '6']] =  matrix['genres'].str.split('|', expand=True)\n\n# The following part is done to stack the genres of movies vertically\n\n# Put each different category of movies in to a different dataframe\ndf0 = matrix[['movieId', '0']]\ndf1 = matrix[['movieId', '1']]\ndf2 = matrix[['movieId', '2']]\ndf3 = matrix[['movieId', '3']]\ndf4 = matrix[['movieId', '4']]\ndf5 = matrix[['movieId', '5']]\ndf6 = matrix[['movieId', '6']]\n\n# Rename Columns, so we can append them later \ndf1.rename(columns={\"1\": \"0\"}, inplace=True)\ndf2.rename(columns={\"2\": \"0\"}, inplace=True)\ndf3.rename(columns={\"3\": \"0\"}, inplace=True)\ndf4.rename(columns={\"4\": \"0\"}, inplace=True)\ndf5.rename(columns={\"5\": \"0\"}, inplace=True)\ndf6.rename(columns={\"6\": \"0\"}, inplace=True)\n\n# Append dataframes\ndf0 = df0.append(df1)\ndf0 = df0.append(df2)\ndf0 = df0.append(df3)\ndf0 = df0.append(df4)\ndf0 = df0.append(df5)\ndf0 = df0.append(df6)\n\ndf0.rename(columns={\"0\": \"genres\"}, inplace=True)\n\ndf0['rating'] = 1 # If a movie appears in the df0, it means that the user has already seen that movie","b3f88e22":"# 3. Create the matrix X= genres, Y= movieId\ndf0 = df0.pivot_table(index='movieId', columns='genres', values = 'rating', aggfunc='mean')\ndf0 = df0.fillna(0)\n\nmatrix_genres = df0\ndel df0 # Delete df0 to save space in memory\n\nmatrix_genres = matrix_genres.to_numpy()","bab4c19d":"# 4. Obtain matrix of ratings values\nsample = ratings[ratings.userId == userId]\nsample = sample.pivot_table(index=['userId'], columns='movieId', values='rating')\nsample = sample.to_numpy()[0] ","d7276473":"Id = 12 # The user taken, it is better if he has seen a lot of movies \n\nmatrix = ratings[ratings.userId == Id]\nCleanRatings = ratings.drop(ratings[ratings.userId==Id].index) # Clean matrice is the rating without the user we choose\n\ntable = matrix.movieId.tolist() # We put in a list the movies that the user has seen, so that we keep track of it\nmatrix = matrix.pivot_table(index=['userId'], columns='movieId', values='rating')\n\nratMatrice = CleanRatings[CleanRatings.userId < 10000].pivot_table(index=['userId'], columns='movieId', values='rating') #userId < x allow to select only X users, if too low some movie won't have been seen : problem\nratMatrice = ratMatrice.fillna(2.5) # We fill the voids (In the future maybe use other strategies for filling NA)\nFinalMatrice = ratMatrice[table]","2d7cf6f0":"model = NMF(n_components=10 , init='random', random_state=0, max_iter = 10000) #We make the factorisation to create the context vector (changing the number of components greatly affect the results)\nW = model.fit_transform(FinalMatrice)\nH = model.components_","ade65e64":"average_regret = []\nalphas = [0.1, 1]\niterations = 200\nt = np.linspace(1, iterations+1, iterations).astype(int)\n\n\nfor i in alphas:\n    print('Alpha = ', i)\n    Av, P, R, result = ContextualUCB(iterations, sample, 20, matrix_genres) \n    Error = [0] # We create a medium error graph\n    \n    for i in range(len(R)):\n        if i == 0:\n            Error[0] = R[0]\n        else:\n            Error.append(Error[i-1] + R[i])\n    \n    average_regret.append(Error\/t)","9c512656":"list1_linucb = []\nlist2_linucb = []\nalpha_lin_ucb = [0.00001, 0.001, 0.01, 0.1, 1]\nfor i in alpha_lin_ucb:\n    print('Alpha = ',i)\n    N,P,Av,regret = linUCB(1500,sample,i)\n    regret1 = regret[736:]\n    list1_linucb.append(regret1)\n    list2_linucb.append(regret)","477caf7e":"reward_contextual_ucb=[]\nlist1_contextual_ucb=[]\niterations=[0.1,1,5,10,30]\nIte = 200\n\nX = np.array(H.transpose())\nfor i in iterations:\n    print('Alpha = ',i)\n    Av, P, R, results = ContextualUCB(Ite,sample,20,X) #We apply the algo\n    #print(R)\n    t = np.linspace(1,Ite+1,Ite).astype(int)\n    Error = [0] #We create an medium error graph\n    for i in range(len(R)):\n        if i==0:\n            Error[0] = R[0]\n        else:\n            Error.append(Error[i-1]+R[i])\n    reward_contextual_ucb.append(Error\/t)\n    list1_contextual_ucb.append(Error[30:]\/t[30:])","d1fbc916":"plt.rcParams['figure.figsize'] = [8, 4] # Size of plots\niteration_axis = [i for i in range(200)]\n\ndf = pd.DataFrame(list(zip(iteration_axis, average_regret[0], average_regret[1])), columns =['iteration', str(alphas[0]), str(alphas[1])])\ndf = pd.melt(df, value_vars=[str(alphas[0]), str(alphas[1])], id_vars=['iteration'], var_name='alpha', value_name='Average_Regret')\n\nsns.lineplot(data=df, x=\"iteration\", y=\"Average_Regret\", hue=\"alpha\")","e0523b16":"warnings.filterwarnings( \"ignore\", module = \"matplotlib\\..*\" )\n\nplt.rcParams['figure.figsize'] = [20, 10] # Make plots bigger\n####################### Plot 1 #######################\nplt.subplot(2, 1, 1)\nalpha_0_1 = df[df[\"alpha\"] == \"0.1\"].Average_Regret.to_numpy()\nline_plot_confidence_intervals(array_to_plot=alpha_0_1, title=\"0.1 alpha\", xticks=False)\nplt.yticks(np.arange(1, 2.6, step=0.2))\n####################### Plot 2 #######################\nplt.subplot(2, 1, 2)\nalpha_1 = df[df[\"alpha\"] == \"1\"].Average_Regret.to_numpy()\nline_plot_confidence_intervals(array_to_plot=alpha_1, title=\"1 alpha\")\nplt.xlabel(\"iterations\", fontsize=17)\nplt.yticks(np.arange(1, 2.6, step=0.2))\n\nplt.savefig('.\/UCB_algorithm_genres')","6d4e2244":"plt.rcParams['figure.figsize'] = [8, 4] # Size of plots\ncolors=[\"red\", \"green\", \"orange\", \"cyan\",'yellow']\ni=0\nfor regret in list2_linucb:\n    plt.plot(regret, color=colors[i], label=alpha_lin_ucb[i])\n    i=i+1\nplt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\nplt.ylabel(\"Average Regret\")\nplt.xlabel(\"Iterations\")\nplt.title(\"Linear UCB\")\nplt.show()","a89522de":"plt.rcParams['figure.figsize'] = [20, 10] # Make plots bigger\n####################### Plot 1 #######################\nplt.subplot(2, 1, 1)\nalpha_0_1 = list2_linucb[2]\nline_plot_confidence_intervals(array_to_plot=alpha_0_1, title=\"0.1 alpha\", xticks=False)\nplt.yticks(np.arange(1, 2.6, step=0.2)) # Seeting yticks so they have the same y axis\n####################### Plot 2 #######################\nplt.subplot(2, 1, 2)\nalpha_1 = list2_linucb[4]\nline_plot_confidence_intervals(array_to_plot=alpha_1, title=\"1 alpha\")\nplt.xlabel(\"iterations\", fontsize=17)\nplt.yticks(np.arange(1, 2.6, step=0.2)) # Seeting yticks so they have the same y axis\nplt.savefig('.\/UCB_linear_algorithm')","9f6edf89":"plt.rcParams['figure.figsize'] = [8, 4] # Size of plots\ncolors=[\"red\", \"green\", \"orange\", \"cyan\",'yellow']\ni=0\nfor regret in reward_contextual_ucb:\n    plot(regret, color=colors[i], label=iterations[i])\n    i=i+1\nplt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\nplt.ylabel(\"Average Regret\")\nplt.xlabel(\"Iterations\")\nshow()","13c32403":"plt.rcParams['figure.figsize'] = [20, 10] # Make plots bigger\n####################### Plot 1 #######################\nplt.subplot(2, 1, 1)\nalpha_0_1 = list1_contextual_ucb[0]\nline_plot_confidence_intervals(array_to_plot=alpha_0_1, title=\"0.1 alpha\", xticks=False)\n#plt.yticks(np.arange(1, 2.6, step=0.2))\n####################### Plot 2 #######################\nplt.subplot(2, 1, 2)\nalpha_30 = list1_contextual_ucb[4]\nline_plot_confidence_intervals(array_to_plot=alpha_30, title=\"30 alpha\")\n#plt.yticks(np.arange(1, 2.6, step=0.2))\nplt.xlabel(\"iterations\", fontsize=17)\nplt.savefig('.\/contextual_ucb')","2b59c0fc":"# 4. Evaluation of Models","e103975e":"### 4.1 Evaluation UCB algorithm apply to genres","b82422bb":"# 2. Preprocessing","e2779ee3":"### 3.2 Training Lin UCB","9f771e2f":"# 1. Exploratory Data Analysis","09e7fd9b":"### 4.2 Evaluation Linear UCB algorithm ","188c0877":"### 3.3 Training Contextual UCB","bcb1bc9f":"We can see from the graph above that the majority of movies have been rated less than 100 times and that there are some outliers, like for example a movie that is very popular and has been rated 80000","2da78b79":"### 2.1 Preprocessing to apply UCB algorithm with Context (genres)","2f06bd34":"### 3.1 Training UCB algorithm with Context (genres)","6ae7d541":"There are definetely some outliers, because how is it possible that a user have rated 30000 movies.","a4aab82d":"### 4.3 Evaluation Contextual UCB","9a7245ea":"We tend to observe only good ratings, above 3 points. We can infer that is because when someone does not like a movie then they don't rate it.","91a5973c":"### 2.2 Preprocessing to apply Linear Bandit with Matrix Factorization","c26aef3b":"# 3. Training"}}