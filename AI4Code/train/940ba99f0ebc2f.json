{"cell_type":{"8cf5612f":"code","3053f9c9":"code","5ad4b333":"code","ed513688":"code","aa25a37c":"code","d0b6379b":"code","5d43c82c":"code","72905917":"code","2e52b935":"code","36d566da":"code","a58e911e":"code","8ec7bda6":"code","7c42b044":"code","b63e8e5f":"code","3d42cee3":"code","c56611e2":"code","db4082c7":"code","dc4e08f2":"code","ca7f7849":"code","9e5312b7":"code","1467c2b7":"code","8fcc8b13":"code","a248c442":"code","da886aaa":"code","55425834":"code","2ebd0e4d":"code","57681182":"code","dbb48d30":"code","a104aa9e":"code","036de25f":"code","51e7ab96":"code","a3c250db":"code","450883df":"code","5e1eef6a":"code","94868f25":"code","11c99235":"code","b90b3b83":"code","263f8542":"code","9b53ca79":"code","9cacd9de":"code","90640c5a":"code","e9d61ff4":"code","e2967ffd":"code","61e7fc09":"code","20b73f57":"markdown","12e6d6e8":"markdown","7e8edfb5":"markdown","42fd80d5":"markdown","99b9cc5f":"markdown","637a4c95":"markdown","2a852392":"markdown","6bf95be5":"markdown","b67db37e":"markdown","489750f7":"markdown","e7e4184a":"markdown","16c0e943":"markdown","25d194a0":"markdown","63731d8d":"markdown","8353b383":"markdown","c7438002":"markdown","a324dfe2":"markdown","286acb7d":"markdown"},"source":{"8cf5612f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3053f9c9":"#Open Dataframe\npd.set_option('display.max_rows', 1000)\npd.set_option('display.max_columns', 1000)\ndf = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndf.head()","5ad4b333":"# Checking NaN columns BEFORE Cleaning\nprint('Total Rows- BEFORE Cleaning: ', df.shape[0])\nprint('Total Columns- BEFORE Cleaning: ', df.shape[1], '\\n')\nnans = df.isnull().sum()\nnans = nans[nans!=0]\nprint('Total NaN values - BEFORE Cleaning:\\n',nans)","ed513688":"# Drop columns with >30% of missing data\/ unneccesary columns\n# 'YearBuilt' and 'YearRemodAdd' describe the same, but 'YearRemodAdd' mentions the Year of Remodel or Added structures\ndrop_cols = ['Id','Alley', 'PoolQC', 'Fence', 'MiscFeature', 'FireplaceQu','YearBuilt']","aa25a37c":"df.info(max_cols=0)","d0b6379b":"#function for cleaning the dataset\ndef clean_data(df, drop_cols):\n    \"\"\"\n    Cleaning the dataframe by:\n    -dropping columns\n    -Impute categorical columns by most frequent value\n    -Impute numerical columns by interpolating with nearest value\n    \n    Args:\n    df - dataframe, columns to drop and impute\n    drop_cols - dropping columns list\n    \n    Returns:\n    clean_df - cleaned dataframe\n    \"\"\"\n        \n    #drop columns in the list drop_cols\n    dropped_df = df.drop(drop_cols, axis=1)\n\n    #seperately selecting numerical and cateogorical columns\n    num_df = dropped_df.select_dtypes(include=['int64', 'float64'])\n    cat_df = dropped_df.select_dtypes(include='object')\n    \n    #Impute numerical columns using interpolation\/polynomial function\n    num_df.interpolate(method='nearest', inplace=True)\n    \n    #Impute categorical columns using most frequent variable\n    cols = cat_df.columns.to_list()\n    freq_vars = [cat_df[col].value_counts().idxmax() for col in cols]\n    filler = dict(zip(cols, freq_vars))\n    cat_df.fillna(value=filler, inplace=True)\n    \n       \n    #concat both imputed column types\n    clean_df = pd.concat([num_df, cat_df], axis=1)\n    \n    \n    return clean_df","5d43c82c":"clean_df = clean_data(df, drop_cols)\nclean_df.head()","72905917":"# Data check AFTER Cleaning\nprint('Total Rows- AFTER Cleaning: ', clean_df.shape[0])\nprint('Total Columns- AFTER Cleaning: ', clean_df.shape[1], '\\n')\nnans = clean_df.isnull().sum()\nnans = nans[nans!=0]\nprint('NaN columns - AFTER Cleaning:\\n', nans)","2e52b935":"clean_df.head()","36d566da":"def age_calc(df, year_cols, age_cols):\n    \"\"\"\n    Calculates the difference (in years) of supplied year columns(year_cols) with current year\n    \n    Args:\n    df - Dataframe\n    year_cols - list of year columns in df\n    age_cols - list of names of age columns to be added in df\n    \n    Returns:\n    df - Dataframe with added age_cols and dropped year_cols\n    \n    \"\"\"\n\n    from datetime import datetime\n\n    for year, age in zip(year_cols, age_cols):\n        df[year] = pd.to_datetime(df[year], format='%Y')\n        df[age] = datetime.today().year - df[year].dt.year\n        \n    df.drop(year_cols, axis=1, inplace=True)\n    \n    return df","a58e911e":"year_cols = ['YearRemodAdd','GarageYrBlt','YrSold']\nage_cols = ['HouseAge','GarageAge','SaleAge']\n\nclean_df = age_calc(clean_df, year_cols, age_cols)\nclean_df.head()","8ec7bda6":"#NORMALIZE\n#Normalize (min-max) Features\/Columns using their minimum and maximum values\ndef minmax_scale(df, cols):\n    \"\"\"\n    applies min-max scaling to columns for normalizing the numerical columns in the Dataframe\n    \n    Args:\n    df- Dataframe to normalize\n    \n    Returns:\n    df - Dataframe with normalized columns\n    \"\"\"\n    for col in cols:\n        min_val = min(df[col])\n        max_val = max(df[col])\n        df[col] = (df[col]-min_val)\/(max_val-min_val)\n    \n    return df","7c42b044":"norm_cols = clean_df.select_dtypes(include=['int64', 'float64']).columns\n\nnorm_df = minmax_scale(clean_df, norm_cols)\nnorm_df.head()","b63e8e5f":"#Import libraries\nimport seaborn as sns\nimport matplotlib.pyplot as plt","3d42cee3":"num_df = norm_df.select_dtypes(include=['int64','float64']) # columns with Numerical data\ncat_df = norm_df.select_dtypes(include=['object']) # columns with Categorical data","c56611e2":"num_df.hist(figsize=(25,50))","db4082c7":"def plot_cats(df, cat_cols, dims=(20,5)):\n    \"\"\"\n    Plots Bar graphs for categorical columns passed as arguments for studying data distribution\n   \n    Args:\n    df - input data as a dataframe\n    cat_cols - list of categorical columns\n    dims - dimensions of graphs, default=(20,5)\n   \n    Returns:\n    prints Bar graphs for the passed columns\n    \"\"\"\n\n    fig = plt.figure(figsize=dims)\n\n    plot_rows=10\n    plot_cols=4\n    plot_num=1\n\n    for col in cat_cols:\n        plt.subplot(plot_rows,plot_cols,plot_num)\n        plt.title(f\"Distribution of {col}\")\n        sns.countplot(df[col])\n        plot_num=plot_num+1\n\n    plt.show()  ","dc4e08f2":"plot_cats(clean_df, cat_df.columns.to_list(), dims=(25, 50))","ca7f7849":"num_cols = num_df.columns.to_list()\nnum_corr_df = norm_df[num_cols].corr(method='spearman')","9e5312b7":"num_corr_df['SalePrice'][abs(num_corr_df['SalePrice'])>0.5]","1467c2b7":"corr_nums = num_corr_df['SalePrice'][abs(num_corr_df['SalePrice'])>0.5].index.to_list()\ncorr_nums","8fcc8b13":"num_df[corr_nums].hist(figsize=(20,10), bins=5)","a248c442":"try_dums = pd.get_dummies(cat_df, drop_first=True, dtype='int64')\ntry_dums.info()","da886aaa":"try_dums['SalePrice'] = norm_df['SalePrice'] # adding Target columns for correlation calculation\ntry_dums.head()","55425834":"dums_cat_corr = try_dums.corr(method='spearman')\ndums_cat_corr['SalePrice'][abs(dums_cat_corr['SalePrice'])>0.5]","2ebd0e4d":"stats_cat = cat_df.copy(deep=True) # 'deep=True' prohibits changes to be transferred between copy & original\nstats_cat.head()","57681182":"stats_cat['SalePrice'] = clean_df['SalePrice'] # adding Target columns for correlation calculation\nstats_cols = stats_cat.iloc[:, :-1].columns.to_list() # Feature columns for ANOVA test, excluding Target column","dbb48d30":"total_cats = []\nfor col in stats_cols:\n    unique_cats = stats_cat[col].nunique()\n    total_cats.append(unique_cats)\nprint(f\"Totals:\\nCategorical columns: {len(stats_cols)}\\nCategorical variables: {sum(total_cats)}\")","a104aa9e":"corr_cats = ['ExterQual', 'Foundation', 'BsmtQual', 'KitchenQual', 'GarageFinish']\n\nfig, ax = plt.subplots(nrows=1, ncols=len(corr_cats), figsize=(20,10))\n\nfor cat, plot_num in zip(corr_cats,range(len(corr_cats))):\n    stats_cat.boxplot(column='SalePrice', by=cat, vert=True, figsize=(10,8), ax=ax[plot_num])","036de25f":"# Overall Feature columns with correlation > 50%\ncorr_cols = corr_nums+corr_cats\ncorr_cols = [col for col in corr_cols if col != 'SalePrice']\ncorr_cols","51e7ab96":"def hypotest_anova(df, features, target):\n    \"\"\"\n    Hypothesis test to determine if the evaluated correlations of categorical and numerical columns are eligible for Regression model development\n    \n    Args:\n    df - input dataframe\n    features - columns used as predictors\/independent variables for Regression\n    target - column used as dependent variable for Regression\n    \n    Returns:\n    model_cols - list of selected features for Regression model development\n    \"\"\"\n    from scipy.stats import f_oneway # anova test package\n    \n    model_cols = []\n    \n    for feature in features:\n            stats_data = df.groupby(feature)[target].apply(list) # grouping categorical variables by 'SalePrice' & collecting values as an array\n            f_stat, p_value = f_oneway(*stats_data)\n            \n            if p_value < 0.05:\n                model_cols.append(feature)\n                print(f\"{feature} - {target} --> CORRELATE\\nF-statistic: {f_stat}\\nP-Value: {p_value}\\n\")\n            else:\n                print(f\"{feature} - {target} --> NO CORRELATION\\nF-statistic: {f_stat}\\nP-Value: {p_value}\\n\")\n                              \n    return model_cols\n    \n    ","a3c250db":"feature_cols = hypotest_anova(norm_df, corr_cols, 'SalePrice')","450883df":"#Import neccesary libraries\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.ensemble import RandomForestRegressor, VotingRegressor, ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error as MSE\n\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer","5e1eef6a":"#Splitting dataset into Target and Features\ny = df_no_nan.SalePrice\nX = df_no_nan.drop(['SalePrice'], axis=1)\n\n#one-hot encoding categorical Features and reducing multicollinearity by dropping first category\nX_encoded = pd.get_dummies(X, drop_first=True)\n\n#Setting seed for reproducibility\nSEED = 42\n#Splitting dataset to 70% train and 30% test\nX_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.3, random_state=SEED)","94868f25":"#Instantiating Model\nrf_reg = RandomForestRegressor(n_estimators=500, n_jobs=-1,\n                               warm_start=True,\n                              random_state=SEED)\net_reg = ExtraTreesRegressor(n_estimators=500, n_jobs=-1,\n                               warm_start=True,\n                              random_state=SEED)\nvote_reg = VotingRegressor([('rf',rf_reg),('et', et_reg)], n_jobs=-1)\n","11c99235":"#Fitting the training data\nvote_reg.fit(X_train, y_train)\n\n#Predicting the test data\ny_pred = vote_reg.predict(X_test)","b90b3b83":"#Evaluating Model accuracy\nscore = vote_reg.score(X_test, y_test)*100\nrmse_test = MSE(y_test, y_pred)**(0.5)\nprint(f'Model Performance:\\n Scoring:{score}\\n rmse_score:{rmse_test}')\n#print(cross_val_score(rf_reg, X_train, y_train, cv=10, n_jobs=-1))","263f8542":"#Import test data\ntest_df = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntest_nan_cols = test_df.columns[test_df.isnull().sum()>800]\ntest_nan_cols","9b53ca79":"df_nan_cols = df.columns[df.isnull().sum()>800]\ndf_nan_cols","9cacd9de":"#Default filling of NaN values\ntest_no_nan = test_df.dropna(thresh=700, axis=1)\ntest_no_nan = test_no_nan.fillna(method='ffill')\ntest_no_nan.isnull().sum()","90640c5a":"#Impute NaN values using IterativeImputer\nimp = IterativeImputer(random_state=SEED, initial_strategy='most_frequent')\n","e9d61ff4":"#Encoding categorical data\ntest_encoded = pd.get_dummies(test_no_nan)","e2967ffd":"shapes = [df, df_no_nan, X_encoded, test_df, test_no_nan, test_encoded]\nfor i in shapes:\n    print('Shape of df:{}'.format(i.shape))","61e7fc09":"#Predicting Test data\ntest_pred = vote_reg.predict(test_encoded)\npred_df = pd.DataFrame(test_pred, columns=['SalePrice'])\npred_df.head()","20b73f57":"# Model Building\n","12e6d6e8":"> ### Boxplot visualization of correlating Categorical columns","7e8edfb5":"### Numerical columns correlation with 'SalePrice'","42fd80d5":"### Calculate the Age of the Houses","99b9cc5f":"# Data Check\n> * Descriptive Stats\n> * Column datatypes\n> * Total NaNs","637a4c95":"# Predicting Test data\n* Import Test data\n* Impute np.nan values using IterativeImputer\n* Encode categorical Features\n* Predict SalePrice","2a852392":"### Data Distribution\n> * Verify columns for Normal distribution to proceed with calculating Correlation\n> * Columns with extremely skewed distributions can be omitted\n> * Columns with Normal distribution and partial skew are fine to work with \n> * _minor adjustments to outliers if needed includes, adjusting outliers with logical business value_","6bf95be5":"> * Calculate Correlations of Numerical and Categorical columns\n> * Collect columns with Correlation > 50%\n> * Visualize correlated columns\n> * Statistically confirm Correlated columns","b67db37e":"### Hypothesis testing for correlating columns using ANOVA\n> #### Null Hypothesis - None of the Feature columns listed are correlating to the Target column","489750f7":"### Correlation\n","e7e4184a":"> ### One-Hot encoding of Categorical columns","16c0e943":"> ### Boxplot visualizations of Correlating Numerical columns","25d194a0":"# EDA\n> ### Data distribution\n> ### Correlation ","63731d8d":"### Categorical columns correlation with 'SalePrice'","8353b383":"# Data Cleaning\n> * Drop columns in drop_cols list\n> * Seperate columns into categorical and numerical \n> * Impute categorical columns using most frequent variable \n> * Impute numerical columns using interpolation\/polynomial function\n ","c7438002":"### Normalize columns","a324dfe2":"# Data Formatting\n> * Translate columns to information, such as datetime columns to Age\n> * Normalize columns","286acb7d":"> * Total categorical columns of 38, with 234 categorical variables upon interaction with 'SalePrice' column would give too many box plots to visualize \/ understand at the same time.\n> * Hence, categorical columns evaluated from correlation test are used for boxplots:\n> ['ExterQual', 'Foundation', 'BsmtQual', 'KitchenQual', 'GarageFinish']"}}