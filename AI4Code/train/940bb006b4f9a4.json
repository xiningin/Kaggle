{"cell_type":{"9d063192":"code","69435dfe":"code","89bcac17":"code","59a294ca":"code","564c7ff1":"code","2ffda8eb":"code","87f0c6fb":"code","5270887e":"code","23db6e8d":"code","6cdc632b":"code","47b5af6b":"code","b13b2ecc":"code","a010efc9":"code","72dd8e3e":"code","493e2ab1":"code","1d5ab053":"code","1add48f7":"code","32a342fb":"code","297c5b72":"code","82d4dd43":"code","c5c2e07d":"code","14798634":"code","5ddf2201":"code","f8b78433":"code","f32f299f":"code","943370cd":"code","dbac006c":"code","0b0f9d4b":"code","71f89247":"code","acb5e872":"code","f2389dc0":"code","e3fbb336":"code","43b66a2a":"code","a65461f5":"code","07ea04f5":"markdown","f76f6138":"markdown","99a4890a":"markdown","3d5d0699":"markdown","7fe35203":"markdown","59522890":"markdown","67467b58":"markdown","0ac5f1fa":"markdown"},"source":{"9d063192":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport zipfile as zf\nimport re\nimport random\nimport math\nimport matplotlib.pyplot as plt\n%matplotlib inline","69435dfe":"!pip install bert-for-tf2\n!pip install sentencepiece","89bcac17":"try:\n    %tensorflow_version 2.x\nexcept Exception:\n    pass\nimport tensorflow as tf\n\nimport tensorflow_hub as hub\n\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import load_model\nimport bert\nfrom transformers import AutoTokenizer","59a294ca":"\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","564c7ff1":"train = pd.read_csv('\/kaggle\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv.zip')\ntest_data = pd.read_csv('\/kaggle\/input\/jigsaw-toxic-comment-classification-challenge\/test.csv.zip')\ntest_labels = pd.read_csv('\/kaggle\/input\/jigsaw-toxic-comment-classification-challenge\/test_labels.csv.zip')","2ffda8eb":"train.head()","87f0c6fb":"train.info()","5270887e":"column_list = [f for f in train.columns if train.dtypes[f] != 'object']\ndf = pd.DataFrame(columns=column_list)\nfor col in column_list:\n    df.loc[0,col] = train[train[col] == 1][col].sum()\ndf['non_hate'] = train.shape[0] - df.sum(axis=1)    ","23db6e8d":"pie, ax = plt.subplots(figsize=[12,10])\nlabels = df.keys()\nplt.pie(x=df.values[0], autopct=\"%.1f\", explode=[0.05]*len(df.values[0]), labels=labels, pctdistance=0.5)\nplt.title(\"Types of Toxic Comments\", fontsize=14);\npie.savefig(\"ToxicCommentsChart.png\")","6cdc632b":"test_data.head()","47b5af6b":"test_labels.head()","b13b2ecc":"df = train.drop(columns=['id'], axis=1)","a010efc9":"def clean_comment(comment):\n    # Removing the @\n    comment = re.sub(r\"@[A-Za-z0-9]+\", ' ', comment)\n    # Removing the URL links\n    comment = re.sub(r\"https?:\/\/[A-Za-z0-9.\/]+\", ' ', comment)\n    # Keeping only letters\n    comment = re.sub(r\"[^a-zA-Z.!?']\", ' ', comment)\n    # Removing additional whitespaces\n    comment = re.sub(r\" +\", ' ', comment)\n    return comment","72dd8e3e":"df['comment_text'] = df['comment_text'].apply(lambda x: clean_comment(x))","493e2ab1":"df.head()","1d5ab053":"data_clean = df.comment_text.values\ndata_labels = df.drop(columns=['comment_text'],axis=1).values","1add48f7":"FullTokenizer = bert.bert_tokenization.FullTokenizer\n#FullTokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\nbert_layer = hub.KerasLayer(\"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-12_H-768_A-12\/1\",\n                            trainable=False)\nvocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = FullTokenizer(vocab_file, do_lower_case)","32a342fb":"def encode_sentence(sent):\n    return [\"[CLS]\"] + tokenizer.tokenize(sent)[:510] + [\"[SEP]\"]","297c5b72":"data_inputs = [encode_sentence(sentence) for sentence in data_clean]","82d4dd43":"def get_ids(tokens):\n    return tokenizer.convert_tokens_to_ids(tokens)\n\ndef get_mask(tokens):\n    return np.char.not_equal(tokens, \"[PAD]\").astype(int)\n\ndef get_segments(tokens):\n    seg_ids = []\n    current_seg_id = 0\n    for tok in tokens:\n        seg_ids.append(current_seg_id)\n        if tok == \"[SEP]\":\n            current_seg_id = 1-current_seg_id # turns 1 into 0 and vice versa\n    return seg_ids","c5c2e07d":"data_with_len = [[sent, data_labels[i], len(sent)]\n                 for i, sent in enumerate(data_inputs)]\nrandom.shuffle(data_with_len)\ndata_with_len.sort(key=lambda x: x[2])\nsorted_all = [([get_ids(sent_lab[0]),\n                get_mask(sent_lab[0]),\n                get_segments(sent_lab[0])],\n               sent_lab[1])\n              for sent_lab in data_with_len if sent_lab[2] > 7]","14798634":"# A list is a type of iterator so it can be used as generator for a dataset\nall_dataset = tf.data.Dataset.from_generator(lambda: sorted_all,\n                                             output_types=(tf.int32, tf.int32))","5ddf2201":"BATCH_SIZE = 32\nall_batched = all_dataset.padded_batch(BATCH_SIZE,\n                                       padded_shapes=((3, None), (6)),\n                                       padding_values=(0, 0))","f8b78433":"NB_BATCHES = math.ceil(len(sorted_all) \/ BATCH_SIZE)\nNB_BATCHES_TEST = NB_BATCHES \/\/ 10\nall_batched.shuffle(NB_BATCHES)\ntest_dataset = all_batched.take(NB_BATCHES_TEST)\ntrain_dataset = all_batched.skip(NB_BATCHES_TEST)","f32f299f":"class DCNNBERTEmbedding(tf.keras.Model):\n    \n    def __init__(self,\n                 nb_filters=50,\n                 FFN_units=512,\n                 nb_classes=6,\n                 dropout_rate=0.1,\n                 name=\"dcnn\"):\n        super(DCNNBERTEmbedding, self).__init__(name=name)\n        \n        self.bert_layer = hub.KerasLayer(\n            \"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-12_H-768_A-12\/1\",\n            trainable=False)\n\n        self.bigram = layers.Conv1D(filters=nb_filters,\n                                    kernel_size=2,\n                                    padding=\"valid\",\n                                    activation=\"relu\")\n        self.trigram = layers.Conv1D(filters=nb_filters,\n                                     kernel_size=3,\n                                     padding=\"valid\",\n                                     activation=\"relu\")\n        self.fourgram = layers.Conv1D(filters=nb_filters,\n                                      kernel_size=4,\n                                      padding=\"valid\",\n                                      activation=\"relu\")\n        self.pool = layers.GlobalMaxPool1D()\n        self.dense_1 = layers.Dense(units=FFN_units, activation=\"relu\")\n        self.dropout = layers.Dropout(rate=dropout_rate)\n#        if nb_classes == 2:\n        self.last_dense = layers.Dense(units=nb_classes,\n                                           activation=\"sigmoid\")\n#        else:\n#            self.last_dense = layers.Dense(units=nb_classes,\n#                                           activation=\"softmax\")\n    \n    def embed_with_bert(self, all_tokens):\n        _, embs = self.bert_layer([all_tokens[:, 0, :],\n                                   all_tokens[:, 1, :],\n                                   all_tokens[:, 2, :]])\n        return embs\n\n    def call(self, inputs, training):\n        x = self.embed_with_bert(inputs)\n\n        print(x.shape)\n\n        x_1 = self.bigram(x)\n        x_1 = self.pool(x_1)\n        x_2 = self.trigram(x)\n        x_2 = self.pool(x_2)\n        x_3 = self.fourgram(x)\n        x_3 = self.pool(x_3)\n        \n        merged = tf.concat([x_1, x_2, x_3], axis=-1) # (batch_size, 3 * nb_filters)\n        merged = self.dense_1(merged)\n        merged = self.dropout(merged, training)\n        output = self.last_dense(merged)\n        \n        return output","943370cd":"NB_FILTERS = 100\nFFN_UNITS = 256\nNB_CLASSES = 6\n\nDROPOUT_RATE = 0.5\n\nBATCH_SIZE = 32\nNB_EPOCHS = 2","dbac006c":"Dcnn = DCNNBERTEmbedding(nb_filters=NB_FILTERS,\n                         FFN_units=FFN_UNITS,\n                         nb_classes=NB_CLASSES,\n                         dropout_rate=DROPOUT_RATE)","0b0f9d4b":"Dcnn.compile(loss=\"binary_crossentropy\",\n            optimizer=\"adam\",\n            metrics=[\"accuracy\"])","71f89247":"checkpoint_path = \".\/\"\n\nckpt = tf.train.Checkpoint(Dcnn=Dcnn)\n\nckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=1)\n\nif ckpt_manager.latest_checkpoint:\n    ckpt.restore(ckpt_manager.latest_checkpoint)\n    print(\"Latest checkpoint restored!!\")","acb5e872":"class MyCustomCallback(tf.keras.callbacks.Callback):\n\n    def on_epoch_end(self, epoch, logs=None):\n        ckpt_manager.save()\n        print(\"Checkpoint saved at {}.\".format(checkpoint_path))","f2389dc0":"Dcnn.fit(train_dataset,\n         epochs=NB_EPOCHS,\n         callbacks=[MyCustomCallback()])","e3fbb336":"Dcnn.save('Bert_Dcnn_model1',save_format='tf')","43b66a2a":"new_model = load_model('Bert_Dcnn_model1')","a65461f5":"results = Dcnn.evaluate(test_dataset)\nprint(results)","07ea04f5":"## Cleaning","f76f6138":"# Model Building","99a4890a":"# Model Training","3d5d0699":"# Imports","7fe35203":"# Preprocessing","59522890":"# Evaluation","67467b58":"## Dataset Creation","0ac5f1fa":"## Tokenization"}}