{"cell_type":{"b62fcdda":"code","16bc0c16":"code","af7eb085":"code","4028ed83":"code","02cb858c":"code","95d45be0":"code","8bb9913f":"code","17a151a3":"code","2d38fe8b":"code","315a3378":"code","da5cc0a4":"code","8195bffa":"code","c00fe9a7":"code","17e8e030":"code","2c31beb0":"code","579cb79b":"code","f6c5b031":"markdown","3d122fda":"markdown","d7c01bba":"markdown","f535c888":"markdown","d3e545b9":"markdown","cbb32605":"markdown","6375b3a4":"markdown"},"source":{"b62fcdda":"!pip install kaggle-environments -U > \/dev\/null 2>&1\n# !cp -r ..\/input\/lux-ai-2021\/* .","16bc0c16":"from kaggle_environments import make\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport random\nfrom collections import namedtuple, deque\nfrom lux.game import Game\nfrom lux.game_map import RESOURCE_TYPES, Position\nfrom tqdm.notebook import tqdm\nimport json\nfrom pathlib import Path\nfrom enum import Enum\nfrom sklearn.preprocessing import normalize\nimport math\nimport os\nimport sys\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nfrom matplotlib.ticker import MultipleLocator\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\nimport tqdm\n\nimport torch\nimport numpy as np\nfrom lux.game_constants import GAME_CONSTANTS\nfrom lux.game import Game\nfrom lux import annotate,game\nfrom lux.game_map import Position","af7eb085":"!mkdir .\/models\n!mkdir .\/models\/qeval\n!mkdir .\/models\/qnext\n!mkdir .\/models\/supervised_model","4028ed83":"class config:\n    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\") # No noticable speed improvement with GPU\n    epochs = 20\n    lr = 1e-5\n    weight_decay = 1e-5\n    epsilon = .1\n    eps_decay = .999\n    eps_min = 0.00\n    gamma = 0.95\n    num_layers = 19\n    num_actions = 9\n    mem_len = 480\n    replace_cntr = 100\n    base_size = 32\n    phase = 'supervise'\n    debug_print = False","02cb858c":"class Callback():\n    def __init__(self):\n        self.loss = 0.\n        \n    def on_train_begin(self):\n        print('Begin Training')\n    \n    def on_train_end(self):\n        luxagent.save_model()\n    \n    def on_epoch_begin(self, epoch):\n        print(f'EPOCH {epoch}\/{config.epochs-1}')\n        \n    def on_epoch_end(self):\n        if config.phase == 'reinforce':\n            luxagent.qeval.optimizer.param_groups[0]['lr'] *= .5\n        \n    def on_game_begin(self):\n        self.loss = 0.\n        \n    def on_game_end(self, idx):\n        if config.phase == 'supervise':\n            steps = json_steps[-1][0]['observation']['step']\n            print(f\"{' ':3s}{game_state.map.width}x{game_state.map.height}: Loss = {self.loss\/(steps):.3e}  |  Game {idx}\")\n            luxagent.model.optimizer.param_groups[0]['lr'] *= 0.95\n        elif config.phase == 'reinforce':\n            steps = obs['step']\n            print(f\"{' ':3s}{game_state.map.width}x{game_state.map.height}: Loss = {self.loss\/(steps):.3e}  |  Reward = {reward:.4f}  |  Game {idx}\")\n        elif config.phase == 'test':\n            print(f\"{' ':3s}{game_state.map.width}x{game_state.map.height}: Game {idx}\")\n        \n        for team,player in enumerate(game_state.players):\n            workers = 0\n            carts = 0\n            for unit in player.units:\n                if unit.type==UNIT_TYPE['WORKER']:\n                    workers+=1\n                if unit.type==UNIT_TYPE['CART']:\n                    carts+=1\n            citytiles = player.city_tile_count\n            research = player.research_points\n            print(f\"{' ':6s}Team:{team}  Citytiles: {citytiles:>2d}  Workers: {workers:>2d}  Carts: {carts:>2d}  RP: {research:>3d}\")\n#         luxagent.model.initialize_hidden()\n        \n    def on_loss_begin(self):\n        pass\n    \n    def on_loss_end(self):\n        pass\n    \n    def on_step_begin(self):\n        pass\n    \n    def on_step_end(self, loss):\n        self.loss += loss","95d45be0":"class Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels=3,out_channels=1,kernel_size=(4,4),stride=1)\n        self.conv2 = nn.Conv2d(in_channels=2,out_channels=1,kernel_size=(4,4),stride=1)\n        self.conv3 = nn.Conv2d(in_channels=2,out_channels=1,kernel_size=(4,4),stride=1)\n        self.conv4 = nn.Conv2d(in_channels=9,out_channels=1,kernel_size=(4,4),stride=1)\n        self.lstm = nn.LSTM(input_size=676, hidden_size=128, num_layers=2, batch_first=True) \n        self.relu = nn.ReLU()\n        self.emb = nn.Embedding(4,4)\n        self.encoders = nn.TransformerEncoderLayer(d_model=260,nhead=10)\n        self.enc = nn.TransformerEncoder(self.encoders,num_layers=3)\n        self.dec = nn.Linear(260,config.num_actions)\n        \n        self.optimizer = optim.Adam(self.parameters(), lr=config.lr, weight_decay=config.weight_decay)\n#         self.initialize_hidden()\n        \n        self.to(config.device)\n    \n    def initialize_hidden(self):\n        self.h = torch.zeros([2, 1, 128], requires_grad=True)\n        self.c = torch.zeros([2, 1, 128], requires_grad=True)\n    \n    def forward(self, state1, state2):\n        [r_map1, c_map1, u_map1, e_types, e_info], [r_map2, c_map2, u_map2] = state1, state2\n        for idx,component in enumerate([r_map1, c_map1, u_map1, e_types, e_info, r_map2, c_map2, u_map2]):\n            component.to(config.device)\n        \n        outr1 = self.relu(self.conv1(r_map1))\n        outc1 = self.relu(self.conv2(c_map1)).permute(1,0,2,3)\n        outu1 = self.relu(self.conv3(u_map1)).permute(1,0,2,3)\n        map1 = torch.cat([outr1, outc1, outu1],dim=1)\n        map1 = self.relu(self.conv4(map1))\n        \n        outr2 = self.relu(self.conv1(r_map2))\n        outc2 = self.relu(self.conv2(c_map2)).permute(1,0,2,3)\n        outu2 = self.relu(self.conv3(u_map2)).permute(1,0,2,3)\n        map2 = torch.cat([outr2, outc2, outu2],dim=1)\n        map2 = self.relu(self.conv4(map2))\n        \n        map1 = map1.view(1,1,-1)\n        map2 = map2.view(1,1,-1)\n        map_ = torch.cat([map1, map2],dim=1)\n        \n        map_, _ = self.lstm(map_)\n        map_ = map_.view(1,-1).expand(1,len(e_types),-1)\n        \n        e_embs = self.emb(e_types)\n        embs = torch.add(e_embs, e_info).unsqueeze(0)\n        \n        out = torch.cat([embs, map_],dim=2)\n        out = self.relu(self.enc(out))\n        out = self.dec(out)\n        \n        invalid_actions = torch.zeros(out.shape)\n        \n        invalid_actions[0,e_types==UNIT_TYPE['WORKER'],:2] = -1e30\n        invalid_actions[0,e_types==UNIT_TYPE['CART'],:2] = -1e30\n        invalid_actions[0,e_types==2,2:] = -1e30\n        \n        out = torch.add(out,invalid_actions)\n        if config.debug_print:\n            print(f'preds\\n{out}')\n        out = torch.softmax(out,dim=2).squeeze(0)\n        \n        return out","8bb9913f":"Transitions = namedtuple('transitions',\n                        ('states','actions','rewards','states_','dones'))\n\nclass Replay():\n    def __init__(self):\n        self.memory = deque(maxlen=config.mem_len)\n        \n    def store_transition(self, states, actions, rewards, states_, dones):\n        self.memory.append(Transitions(states, actions, rewards, states_, dones))\n        \n    def sample_memory(self):\n        s = random.sample(self.memory, 1)\n        \n        states = s[0].states\n        actions = s[0].actions\n        rewards = s[0].rewards\n        states_ = s[0].states_\n        dones = s[0].dones\n        \n        return states, actions, rewards, states_, dones\n    \n    def __len__(self):\n        return len(self.memory)","17a151a3":"class LuxAgent():\n    def __init__(self):\n        self.initialize_model()\n        self.memory = Replay()\n        self.epsilon = config.epsilon\n        self.gamma = config.gamma\n        self.step_counter = 0\n    \n    def initialize_model(self):\n        if config.phase == 'supervise':\n#             self.model = torch.load(f'..\/input\/lux-models\/supervised_model.pth')\n#             self.model = torch.load(f'.\/models\/supervised_model\/supervised_model.pth')\n            self.model = Model()\n        elif config.phase == 'reinforce':\n            self.qeval = torch.load(f'.\/models\/supervised_model\/supervised_model.pth')\n            self.qnext = torch.load(f'.\/models\/supervised_model\/supervised_model.pth')\n        elif config.phase == 'test':\n            self.model = torch.load(f'.\/models\/supervised_model\/supervised_model.pth')\n    \n    def load_model(self):\n        if config.phase == 'supervise':\n            self.model = torch.load(f'.\/models\/supervised_model\/supervised_model.pth')\n        elif config.phase == 'reinforce':\n            self.qeval = torch.load(f'.\/models\/qeval\/lux_model_qeval.pth')\n            self.qnext = torch.load(f'.\/models\/qnext\/lux_model_qnext.pth')\n                    \n    def save_model(self):\n        if config.phase == 'supervise':\n            torch.save(self.model, f'.\/models\/supervised_model\/supervised_model.pth')\n        elif config.phase == 'reinforce':\n            torch.save(self.qeval, f'.\/models\/qeval\/lux_model_qeval.pth')\n            torch.save(self.qnext, f'.\/models\/qnext\/lux_model_qnext.pth')\n        \n    def store_memory(self, states, actions, rewards, states_, dones):\n        self.memory.store_transition(states, actions, rewards, states_, dones)\n    \n    def get_from_memory(self):\n        return self.memory.sample_memory()\n    \n    def replace_target_network(self):\n        if self.step_counter % config.replace_cntr == 0:\n            self.qnext.load_state_dict(self.qeval.state_dict())\n\n    def decrement_epsilon(self):\n        self.epsilon = self.epsilon * config.eps_decay if self.epsilon > config.eps_min else config.eps_min\n    \n    def rl_choose_actions(self, state1, state2):\n        actions = torch.argmax(self.qeval.forward(state1, state2),dim=1)\n        for idx,_ in enumerate(actions):\n            if np.random.random_sample() <= self.epsilon:\n                actions[idx] = np.random.choice(range(config.num_actions))\n        return actions\n        \n    def rl_predict(self, state):\n        state1 = state[0]\n        state2 = state[1]\n        return self.qeval.forward(state1, state2)\n    \n    def rl_predict_(self, state):\n        state1 = state[0]\n        state2 = state[1]\n        return self.qnext.forward(state1, state2)\n        \n    def reinforcement_learn(self):\n        if len(self.memory) < 1:\n            return\n        \n        self.replace_target_network()\n        \n        s, a, r, s_, d = self.get_from_memory()\n        \n        a = a.long().unsqueeze(-1)\n        q = self.rl_predict(s).gather(1,a)\n        \n        a_ = torch.argmax(self.rl_predict(s_).long(),1).unsqueeze(-1)\n        q_ = self.rl_predict_(s_).gather(1,a_)\n        q_ = r + config.gamma * q_ * (1-d)\n        \n        callback.on_loss_begin()\n        \n        criterion = nn.MSELoss()\n        loss = criterion(torch.sum(q), torch.sum(q_))\n        \n        callback.on_loss_end()\n        \n        self.qeval.optimizer.zero_grad()\n        loss.backward()\n        \n        callback.on_step_begin()\n        \n        self.qeval.optimizer.step()\n        \n        callback.on_step_end(loss.item())\n        \n        self.step_counter += 1\n        self.decrement_epsilon()\n        \n    def get_entities(self):\n        player = game_state.players[0]\n        entities = []\n        for city in player.cities.values():\n            for tile in city.citytiles:\n                entities.append(f'{tile.pos.x} {tile.pos.y}')\n        for unit in player.units:\n            entities.append(unit.id)\n        return entities\n    \n    def convert_actions(self, actions):\n        acts = []\n        a = {'bw':0, 'r':1, 'm n':2, 'm s':3, 'm e':4, 'm w':5, 'm c':6, 't':7, 'bcity':8}\n        for action in actions:\n            acts.append(a[action])\n        return acts\n    \n    def reorder_targ_actions(self, targ_action_list):\n        entities = self.get_entities()\n#         actions = [''] * len(entities)\n        num_targs = []\n        for targ_acts in targ_action_list:\n            targ_act = targ_acts.split()\n            if targ_act[0]=='m':\n                num_targs.append(f'{targ_act[0]} {targ_act[2]}')\n            else:\n                num_targs.append(targ_act[0])\n        targs = self.convert_actions(num_targs)\n#         for idx,entity in enumerate(entities):\n#             for targ in targs:\n#                 if targ<=1:\n#                     actions[idx] = targ\n#                 else:\n#                     actions[idx] = targ\n        return targs # actions, directions\n    \n    def convert_targ_actions(self, preds, targ_action_list):\n        targs = torch.zeros((preds.shape))\n        actions = self.reorder_targ_actions(targ_action_list)\n        for idx,action in enumerate(actions):\n                targs[idx,action] = 1.\n        return targs\n   \n    def sl_choose_actions(self, state1, state2):\n        actions = self.model.forward(state1, state2)\n        if config.debug_print:\n            print(actions)\n        return torch.argmax(actions,dim=1)\n            \n    def supervise_learn(self, state, last_state, targ_actions):\n        pred = self.model.forward(state, last_state)\n        targ = self.convert_targ_actions(pred, targ_actions)\n        \n        if config.debug_print:\n            print(f'preds\\n{pred}\\ntargs\\n{targ}\\n')\n        callback.on_loss_begin()\n        criterion = nn.MSELoss()\n        loss = criterion(pred, targ)\n        callback.on_loss_end()\n        \n        self.model.optimizer.zero_grad()\n        loss.backward()\n        \n        callback.on_step_begin()\n        \n        self.model.optimizer.step()\n        \n        callback.on_step_end(loss.item())\n        \n        self.decrement_epsilon()","2d38fe8b":"class WorkerObjective(Enum):\n    GatherFuel = \"gather fuel\"\n    BuildCity = \"build city\"\n    Rest = \"rest\"\n\n# Wrapper class for worker unit objects\nclass WorkerAgent:\n    def __init__(self, worker_obj, debug):\n        self.debug = debug\n        self.worker = worker_obj\n        self.objective = WorkerObjective.BuildCity\n        self.objective_changed = False\n        self.mine = None\n        self.destination = None\n        \n    # Reset worker object with latest version each turn\n    def update(self, worker_obj):\n        self.worker = worker_obj\n    \n    def _find_open_mine(self, resource_type, mines):\n        mine = mines.place_in_mine(self.worker, resource_type)\n        if mine is not None:\n            self.mine = mine\n        \n    def _get_best_fuel(self, player):\n        resource_type = RESOURCE_TYPES.WOOD\n        if player.researched_coal():\n            resource_type = RESOURCE_TYPES.COAL\n        if player.researched_uranium():\n            resource_type = RESOURCE_TYPES.URANIUM\n            \n        return resource_type\n    \n    def _at_mining_spot(self):\n        mining_spot = self.get_mining_spot()\n        if mining_spot is None:\n            return False\n        \n        return self.worker.pos == mining_spot\n    \n    def _update_mining(self, controller):\n        if self._at_mining_spot():\n            cell = controller.map.get_cell_by_pos(self.worker.pos)\n            if not cell.has_resource():\n                self.mine.report_resource_depleted(self.worker.pos, self.worker)\n                self.mine = None\n         \n        if self.objective == WorkerObjective.GatherFuel:\n            best_fuel = self._get_best_fuel(controller.player)\n            if self.mine is not None and self.mine.resource_type != best_fuel:\n                self.mine.release_worker(self.worker)\n                self._find_open_mine(best_fuel, controller.mines)\n                \n    def _handle_objective_change(self):\n        if self.objective_changed:\n            if self.mine != None:\n                self.mine.release_worker(self.worker)\n                self.mine = None\n            self.destination = None\n            self.objective_changed = False\n            if self.debug:\n                print(\"Worker\", self.worker.id, \"assigned new objective\", file=sys.stderr)\n                \n    def _handle_mine_assignment(self, controller):\n        if self.destination is None and self.mine is None and self.objective != WorkerObjective.Rest:\n            resource_type = RESOURCE_TYPES.WOOD\n            if self.objective == WorkerObjective.GatherFuel:\n                resource_type = self._get_best_fuel(controller.player)\n            self._find_open_mine(resource_type, controller.mines)\n            if self.debug:\n                if self.mine is not None:\n                    print(\"Worker\", self.worker.id, \"assigned to mining spot\", self.get_mining_spot(), file=sys.stderr)\n                else:\n                    print(\"Unable to place worker\", self.worker.id, \"in mine\", file=sys.stderr)\n                \n    def _handle_destination_arrival(self):\n        if self.destination is not None and self.worker.pos == self.destination:\n            self.destination = None  \n            if self.debug:\n                print(\"Worker\", self.worker.id, \"arrived at their destination\", file=sys.stderr)\n        \n    def _handle_destination_assignment(self, controller):\n        if self.destination is not None:\n            return\n        \n        if self.objective == WorkerObjective.Rest and not self.on_city_tile(controller.map):\n            closest_city_tile = controller.cities.get_nearest_city_tile(self.worker.pos)\n            if closest_city_tile is not None:\n                self.destination = closest_city_tile.pos\n            if self.debug:\n                print(\"Worker\", self.worker.id, \"destination set to city tile\", (self.destination.x, self.destination.y), file=sys.stderr)\n            return\n            \n        if self.worker.get_cargo_space_left() == 0: \n            if self.mine is not None:\n                self.mine.release_worker(self.worker)\n                self.mine = None\n\n            if self.debug:\n                print(\"Worker\", self.worker.id, \"is at max cargo\", file=sys.stderr)\n            if self.objective == WorkerObjective.GatherFuel:\n                nearest_city_tile = controller.cities.get_nearest_city_tile(self.worker.pos)\n                if nearest_city_tile is not None:\n                    self.destination = nearest_city_tile.pos\n            elif self.objective == WorkerObjective.BuildCity:\n                # nearest_periph = controller.cities.get_nearest_periph_pos(self.worker.pos, controller.map)\n                # if nearest_periph is not None:\n                #     self.destination = nearest_periph\n                # else:\n                self.destination = self.find_nearest_empty_tile(self.worker.pos, controller.map)\n            if self.debug:\n                print(\"Worker\", self.worker.id, \"destination changed to\", self.destination, file=sys.stderr)\n            return\n                   \n        if not self._at_mining_spot():\n            self.destination = self.get_mining_spot()\n            if self.debug:\n                print(\"Worker\", self.worker.id, \"returning to minining spot\", self.get_mining_spot(), file=sys.stderr)\n            return\n\n    # Converts directions to degrees\n    def _to_degrees(self, direction):\n        directions = [\"w\", \"s\", \"e\", \"n\"]\n        return 90 * directions.index(direction)\n\n    # Converts degrees to directions\n    def _to_dir(self, degrees):\n        directions = [\"w\", \"s\", \"e\", \"n\"]\n        return directions[int((degrees % 360) \/ 90)]\n        \n\n    # Returns the direction 90 degrees * times clockwise of direction\n    def _rotate_dir(self, direction, times):\n        if direction == \"c\":\n            return \"c\"\n        return self._to_dir(self._to_degrees(direction) + 90 * times)\n                \n        \n    def get_mining_spot(self):\n        if self.mine == None:\n            return None\n        \n        tile = self.mine.get_assigned_spot(self.worker)\n        if tile is not None:\n            return Position(tile[0], tile[1])\n        return None\n        \n    def set_objective(self, objective):\n        if self.objective == objective:\n            return\n        self.objective = objective\n        self.objective_changed = True\n        if self.debug:\n            print(\"Worker\", self.worker.id, \"has new objective\", self.objective, file=sys.stderr)\n        \n    def get_step_direction(self, game_map, steps, avoid_city=False):\n        direction = self.worker.pos.direction_to(self.destination)\n        step = step = self.worker.pos.translate(direction, 1)\n\n        if direction == \"c\" and (step.x, step.y) in steps:              # If the worker plans to stay put but is blocking the step of another worker\n            for i in range(4):\n                new_dir = self._rotate_dir(\"w\", i)\n                step = self.worker.pos.translate(new_dir, 1)\n                if (step.x, step.y) not in steps:\n                    return new_dir\n        \n        if avoid_city or (step.x, step.y) in steps:                     # Get best detour\n            cell = game_map.get_cell(step.x, step.y)\n            if cell.citytile is None and (step.x, step.y) not in steps:\n                return direction\n            \n            shortest_dist = float(\"inf\")\n            best_dir = None\n            for i in range(4):\n                new_dir = self._rotate_dir(direction, i)\n                step = self.worker.pos.translate(new_dir, 1)\n                if step.x < 0 or step.x >= game_map.width or step.y < 0 or step.y >= game_map.height or (step.x, step.y) in steps:\n                    continue\n                cell = game_map.get_cell(step.x, step.y)\n                dist = step.distance_to(self.destination) \n                if cell.citytile is None and dist < shortest_dist:\n                    best_dir = new_dir\n                    \n            if best_dir is not None:\n                return best_dir\n              \n        return direction\n            \n    \n    def on_city_tile(self, game_map):\n        tile = game_map.get_cell_by_pos(self.worker.pos)\n        return tile.citytile is None\n    \n    def find_nearest_empty_tile(self, loc, game_map):\n        if self.tile_is_empty(loc, game_map):\n            return loc\n        \n        searched = set()\n        q = [loc]\n        \n        while len(q) > 0:\n            p = q.pop(0)\n            searched.add((p.x, p.y))\n            \n            if self.tile_is_empty(p, game_map):\n                return p\n            \n            for direction in [(1, 0), (0, 1), (-1, 0), (0, -1)]:\n                neighbor = Position(p.x + direction[0], p.y + direction[1])\n                if neighbor.x >= 0 and neighbor.x < game_map.width and neighbor.y >= 0 and neighbor.y < game_map.height and (neighbor.x, neighbor.y) not in searched:\n                    q.append(neighbor)\n            \n            \n    def tile_is_empty(self, pos, game_map):\n        cell = game_map.get_cell(pos.x, pos.y)\n        return cell.citytile is None and not cell.has_resource()\n    \n    def get_action(self, controller, steps):\n        self._update_mining(controller)\n        \n        if not self.worker.can_act():\n            return None, (self.worker.pos.x, self.worker.pos.y)\n        \n        self._handle_objective_change()\n        self._handle_mine_assignment(controller)\n        self._handle_destination_arrival()\n        \n        if self.destination is None and self.objective == WorkerObjective.BuildCity and self.worker.can_build(controller.map):\n            if self.debug:\n                print(\"Worker\", self.worker.id, \"building city tile at\", self.worker.pos)\n            return self.worker.build_city(), (self.worker.pos.x, self.worker.pos.y)\n        \n        self._handle_destination_assignment(controller)\n                \n        if self.destination is not None:\n            avoid_city = self.objective == WorkerObjective.BuildCity and self.worker.get_cargo_space_left() == 0\n            step_dir = self.get_step_direction(controller.map, steps, avoid_city)\n            if self.debug:\n                print(\"Worker\", self.worker.id, \"step direction:\", step_dir, file=sys.stderr)\n            # step_dir = self.worker.pos.direction_to(self.destination)\n            step = self.worker.pos.translate(step_dir, 1)\n            return self.worker.move(step_dir), (step.x, step.y)\n        \n        return None, (self.worker.pos.x, self.worker.pos.y)\n    \nclass Workers:\n    def __init__(self, worker_list, debug):\n        self.debug = debug\n        self.workers = {}                              # Maps worker ids to WorkerAgent objs\n        self.task_proportions = [0.5, 0.5, 0.0]\n        \n        for worker in worker_list:\n            self.workers[worker.id] = WorkerAgent(worker, self.debug)\n            \n        if self.debug:\n            print(\"Workers object initialized\")\n\n        self._reassign_objectives()\n            \n    def _reassign_objectives(self):\n        num_city_builders = math.ceil(self.task_proportions[0] * len(self.workers))\n        num_fuel_gatherers = math.ceil(self.task_proportions[1] * len(self.workers))\n        worker_ids = self.workers.keys()\n        \n        for i, worker_id in enumerate(worker_ids):\n            if i < num_city_builders:\n                self.workers[worker_id].set_objective(WorkerObjective.BuildCity)\n                continue\n            if i < num_city_builders + num_fuel_gatherers:\n                self.workers[worker_id].set_objective(WorkerObjective.GatherFuel)\n                continue\n            self.workers[worker_id].set_objective(WorkerObjective.Rest)\n        \n    def update(self, worker_list):\n        if self.debug:\n            print(\"Updating workers object\")\n           \n        # Remove workers that were lost last turn\n        lost_workers = set(self.workers.keys()).difference(set([worker.id for worker in worker_list]))\n        for lost_worker in lost_workers:\n            self.workers.pop(lost_worker)\n            \n        for worker in worker_list:\n            if worker.id in self.workers:\n                self.workers[worker.id].update(worker)\n                continue\n                \n            self.workers[worker.id] = WorkerAgent(worker, self.debug)\n            self._reassign_objectives()\n            if self.debug:\n                print(\"Worker added\", file=sys.stderr)\n                \n    def update_task_proportions(self, proportions):\n        self.task_proportions = proportions\n        self._reassign_objectives()\n            \n            \n    def get_actions(self, controller):\n        actions = []\n        steps = set()\n        \n        for worker in self.workers.values():\n            action, step = worker.get_action(controller, steps)\n            steps.add(step)\n            if action is not None:\n                actions.append(action)\n                \n        return actions\n\n\nclass CityWrapper:\n    def __init__(self, city_obj, debug):\n        self.city = city_obj\n        self.debug = debug\n    \n    def get_nearest_periph_pos(self, loc, game_map):\n        if self.debug:\n            print(\"Searching for city build location\", file=sys.stderr)\n        # Return periphery tile obj closest to loc (Only works if loc is not inside city)\n        \n        # Sort tiles in city according to distance from loc\n        sorted_tiles = sorted(self.city.citytiles, key=lambda tile: tile.pos.distance_to(loc))\n\n        for tile in sorted_tiles:\n            for direction in [(1, 0), (0, 1), (-1, 0), (0, -1)]:\n                neighbor = Position(tile.pos.x + direction[0], tile.pos.y + direction[1])\n                if neighbor.x >= 0 and neighbor.x < game_map.width and neighbor.y >= 0 and neighbor.y < game_map.height:\n                    cell = game_map.get_cell(neighbor.x, neighbor.y)\n                    if cell.citytile == None and not cell.has_resource():\n                        return neighbor\n                    \n        return None\n    \n    def get_nearest_city_tile(self, loc):\n        # Return city tile closest to loc\n        shortest_dist = float(\"inf\")\n        closest = None\n        for tile in self.city.citytiles:\n            dist = tile.pos.distance_to(loc)\n            if dist < shortest_dist:\n                shortest_dist = dist\n                closest = tile\n        return closest\n    \n    def get_actions(self, controller, workers_needed):\n        actions = []\n        workers_built = 0\n        for tile in self.city.citytiles:\n            if tile.can_act():\n                if workers_needed - workers_built > 0:\n                    actions.append(tile.build_worker())\n                    workers_built += 1\n                    if self.debug:\n                        print(\"City tile\", tile.pos, \"creating new worker.\", file=sys.stderr)\n                    continue\n                actions.append(tile.research())\n        return actions, workers_built\n    \nclass CitiesWrapper:\n    def __init__(self, cities_list, debug):\n        self.cities = [CityWrapper(city, debug) for city in cities_list]\n        self.debug = debug\n        \n    def update(self, cities_list):\n        self.cities = [CityWrapper(city, self.debug) for city in cities_list]\n    \n    def get_nearest_city(self, loc):\n        # Return CityWrapper obj closest to loc\n        shortest_dist = float(\"inf\")\n        closest = None\n        \n        for city in self.cities:\n            dist = city.get_nearest_city_tile(loc).pos.distance_to(loc)\n            if dist < shortest_dist:\n                shortest_dist = dist\n                closest = city\n                \n        return closest\n    \n    def get_nearest_city_tile(self, loc):\n        # Return CityTile obj closest to loc\n        shortest_dist = float(\"inf\")\n        closest = None\n        \n        for city in self.cities:\n            tile = city.get_nearest_city_tile(loc)\n            dist = tile.pos.distance_to(loc)\n            if dist < shortest_dist:\n                shortest_dist = dist\n                closest = tile\n                \n        return closest\n    \n    def get_nearest_periph_pos(self, loc, game_map):\n        sorted_cities = sorted(self.cities, key=lambda city: city.get_nearest_city_tile(loc).pos.distance_to(loc))\n        \n        for city in sorted_cities:\n            periph = city.get_nearest_periph_pos(loc, game_map)\n            if periph is not None:\n                return periph\n            \n        return None\n    \n    def get_actions(self, controller):\n        actions = []\n        workers_needed = max(controller.state.num_city_tiles - controller.state.num_workers, 0)\n        \n        for city in self.cities:\n            city_actions, workers_built = city.get_actions(controller, workers_needed)\n            actions += city_actions\n            workers_needed -= workers_built\n            \n        return actions\n\n\nclass Mine:\n    def __init__(self, game_state, resource_tile_set, resource_type, debug):\n        self.resource_type = resource_type\n        self.resource_tiles = resource_tile_set\n        self.assigned_workers = {}                                      # Maps worker IDs to assigned worker_tile\n        #self.available_resources = 0\n        #self.cart_loc = self.get_cart_loc()\n        #self.available_work_tiles = len(self.worker_tiles)              # Number of available worker tiles\n        self.debug = debug\n    \n    def _find_cart_loc(self):\n        # Find and return the best location to park the cart\n        pass\n    \n    def _get_open_worker_tile(self, worker_pos):\n        available_tiles = list(filter(lambda tile: tile not in self.assigned_workers.values(), self.resource_tiles))\n        available_tiles = sorted(available_tiles, key=lambda tile: Position(tile[0], tile[1]).distance_to(worker_pos))\n        return available_tiles[0]\n    \n    def get_resource_tiles(self):\n        return self.resource_tiles\n    \n    def worker_assigned(self, worker_id):                               # Checks if a given worker is assigned to mine\n        return worker_id in self.assigned_workers\n    \n    def get_dist(self, loc):                                            # Returns the shortest distance between loc and all spots in mine\n        shortest_dist = float(\"inf\")\n        \n        for tile in self.resource_tiles:\n            tile_pos = Position(tile[0], tile[1])\n            dist = tile_pos.distance_to(loc)\n            if dist < shortest_dist:\n                shortest_dist = dist\n                \n        return shortest_dist\n    \n    def has_opening(self):                                              # Checks if there are any available spots in mine\n        return len(self.resource_tiles) > len(self.assigned_workers)\n    \n    def assign_worker(self, worker):\n        self.assigned_workers[worker.id] = self._get_open_worker_tile(worker.pos)\n        \n    def release_worker(self, worker):\n        if worker.id in self.assigned_workers:\n            self.assigned_workers.pop(worker.id)\n        \n    def get_assigned_spot(self, worker):\n        if worker.id in self.assigned_workers:\n            return self.assigned_workers[worker.id]\n        return None\n    \n    def report_resource_depleted(self, pos, assigned_worker):\n        self.resource_tiles.remove((pos.x, pos.y))\n        self.release_worker(assigned_worker)\n\n    \nclass Mines:\n    def __init__(self, game_state, debug):\n        self.mines = []\n        self.debug = debug\n        \n        self._build_mines(game_state)\n        \n    def _is_valid_tile(self, game_state, x, y, w, h, resource_type, searched):\n        if x < 0 or x >= w or y < 0 or y >= h or (x, y) in searched:\n            return False\n        \n        tile = game_state.map.get_cell(x, y)\n        if not tile.has_resource() or tile.resource.type != resource_type:\n            return False\n        \n        return True\n        \n    def _get_resource_cluster(self, game_state, x, y, w, h, resource_type, cluster_tiles=set(), searched=set()):\n        # Given x, y of a starting tile, search game map to find tiles of resource cluster\n        searched.add((x, y))\n        tile = game_state.map.get_cell(x, y)\n        \n        if not tile.has_resource():                             # Add tile to border set and make no recursive calls\n            return cluster_tiles, searched\n        \n        cluster_tiles.add((x, y))\n        \n        for direction in [(1, 0), (0, 1), (-1, 0), (0, -1)]:  # Call function recursively on surrounding tiles\n            new_x, new_y = x + direction[0], y + direction[1]\n            if self._is_valid_tile(game_state, new_x, new_y, w, h, resource_type, searched):\n                new_cluster_tiles, new_searched = self._get_resource_cluster(game_state, new_x, new_y, w, h, resource_type, cluster_tiles, searched)\n                cluster_tiles = cluster_tiles.union(new_cluster_tiles)\n                searched = searched.union(new_searched)\n            \n        return cluster_tiles, searched\n        \n    def _build_mines(self, game_state): \n        # Iterate over map to find clusters of resource tiles\n        w, h = game_state.map.width, game_state.map.height\n        searched = set()\n        clusters = []\n        resource_types = []\n        \n        for x in range(w):\n            for y in range(h):\n                if (x, y) in searched:\n                    continue\n                tile = game_state.map.get_cell(x, y)\n                if tile.has_resource():\n                    resource_types.append(tile.resource.type)\n                    cluster, new_searched = self._get_resource_cluster(game_state, x, y, w, h, tile.resource.type, set(), set())\n                    searched = searched.union(new_searched)\n                    clusters.append(cluster)\n        \n        # ToDo: Merge mines of same resource type that share borders\n        \n        # Build Mine objs from clusters and borders\n        for cluster, resource_type in zip(clusters, resource_types):\n            self.mines.append(Mine(game_state, cluster, resource_type, self.debug))\n        \n        if self.debug:\n            print(\"Clusters:\", clusters, file=sys.stderr)\n                    \n    def update(self, game_state):\n        # Check mines to see if they need updated\n        for mine in self.mines:\n            needs_update = mine.update_mine(game_state)\n            \n            if needs_update:\n                # Find viable cell from mine to seed cluster search\n                mine_tiles = mine.get_resource_tiles()\n                new_cluster = None\n                new_border = None\n                \n                for tile in mine_tiles:\n                    cell = game_state.map.get_cell(tile[0], tile[1])\n                \n                    if cell.has_resource():\n                        new_cluster, new_border, searched = self._get_resource_cluster(game_state, tile[0], tile[1], gamestate.map.width, gamestate.map.height, cell.resource.type, set(), set())\n                        break\n                \n                self.mines.remove(mine)\n                \n                if new_cluster is not None:\n                    self.mines.append(Mine(game_state, new_cluster, new_border, self.debug))\n    \n    def get_closest_mine(self, loc, resource_type):\n        closest_mine = None\n        shortest_dist = float(\"inf\")\n        \n        for mine in self.mines:\n            if mine.resource_type != resource_type:\n                continue\n            for tile in mine.resource_tiles:\n                tile_pos = Position(tile[0], tile[1])\n                dist = tile_pos.distance_to(loc)\n                \n                if dist < shortest_dist:\n                    shortest_dist = dist\n                    closest_mine = mine\n                    \n        return closest_mine\n    \n    def place_in_mine(self, worker, resource_type):\n        sorted_mines = sorted([mine for mine in self.mines if mine.resource_type == resource_type], key=lambda mine: mine.get_dist(worker.pos))\n        \n        for mine in sorted_mines:\n            if mine.has_opening():\n                mine.assign_worker(worker)\n                return mine \n            \n        return None\n\n\nclass State:\n    def __init__(self, game_state, player, opponent):\n        self._update_state(game_state, player, opponent)\n        \n    def _update_state(self, game_state, player, opponent):\n        self.num_workers = sum([1 if unit.is_worker() else 0 for unit in player.units])\n        self.num_carts = sum([1 if unit.is_cart() else 0 for unit in player.units])\n        self.num_city_tiles = player.city_tile_count\n        self.num_opponent_workers = sum([1 if unit.is_worker() else 0 for unit in opponent.units])\n        self.num_opponent_carts = sum([1 if unit.is_cart() else 0 for unit in opponent.units])\n        self.num_opponent_city_tiles = opponent.city_tile_count\n        self.research_points = player.research_points\n        self.opponent_research_points = opponent.research_points\n        self.turn = game_state.turn\n        \n    def get_state_vector(self):\n        return np.array([\n            self.num_workers,\n            self.num_carts,\n            self.num_city_tiles,\n            self.num_opponent_workers,\n            self.num_opponent_carts,\n            self.num_opponent_city_tiles,\n            self.research_points,\n            self.opponent_research_points,\n            self.turn\n        ]).reshape(1, -1)\n        \n\nclass Controller:\n    def __init__(self, game_state, player, opponent, debug):\n        self.debug = debug\n        self.game_state = game_state\n        self.map = game_state.map\n        self.state = State(game_state, player, opponent)\n        self.player = player\n        self.opponent = opponent\n        self.mines = Mines(game_state, debug)\n        self.workers = Workers([unit for unit in player.units if unit.is_worker()], debug)\n#         self.carts = []\n        self.cities = CitiesWrapper(self.player.cities.values(), debug)\n        \n    def update(self, game_state, player, opponent):\n        self.game_state = game_state\n        self.state._update_state(game_state, player, opponent)\n        self.map = game_state.map\n        self.player = player\n        self.opponent = opponent\n        #self.mines.update(game_state)\n        self.cities.update(self.player.cities.values())\n        self.workers.update([unit for unit in player.units if unit.is_worker()])\n        \n    def get_state_vector(self):\n        return self.state.get_state_vector()\n    \n    def apply_agent_action(self, action):\n        self.workers.update_task_proportions(action)    \n        \n    def get_actions(self):\n        if self.debug:\n            print(\"Turn\", self.game_state.turn, file=sys.stderr)\n        worker_actions = self.workers.get_actions(self)\n        city_actions = self.cities.get_actions(self)\n        return worker_actions + city_actions\n\n\ndef calculate_reward(s, s_prime, reward_weights):\n        reward_vec = (s_prime[0] - s[0]) * np.array(reward_weights)\n        return np.sum(reward_vec)\n\ngame_state = None\ncontroller = None\n\ndef base_agent(observation, configuration):\n    global game_state\n    global controller\n\n    ### Do not edit ###\n    if observation[\"step\"] == 0:\n        game_state = Game()\n        game_state._initialize(observation[\"updates\"])\n        game_state._update(observation[\"updates\"][2:])\n        game_state.id = observation.player\n        \n        player = game_state.players[observation.player]\n        opponent = game_state.players[(observation.player + 1) % 2]\n        controller = Controller(game_state, player, opponent, False)\n        \n    else:\n        game_state._update(observation[\"updates\"])\n        player = game_state.players[observation.player]\n        opponent = game_state.players[(observation.player + 1) % 2]\n        controller.update(game_state, player, opponent)\n    \n    return controller.get_actions()","315a3378":"RES = GAME_CONSTANTS['RESOURCE_TYPES']\nRP_REQ = GAME_CONSTANTS['PARAMETERS']['RESEARCH_REQUIREMENTS']\nDIR = GAME_CONSTANTS['DIRECTIONS']\nUNIT_TYPE = GAME_CONSTANTS['UNIT_TYPES']\nFUEL_RATE = GAME_CONSTANTS['PARAMETERS']['RESOURCE_TO_FUEL_RATE']\n\ndef normal(state):\n    for idx,channel in enumerate(state):\n        if idx in [1,9,14]: continue\n        state[idx] = torch.tensor(normalize(channel))\n    return state\n\ndef get_input():\n    w,h = game_state.map_width, game_state.map_height\n    s = config.base_size\n    pad = s - w\n    rp = game_state.players[0].research_points\n    \n    res_token = {'wood': 1, 'coal': 2, 'uranium': 3}\n    \n    coal_rp_rem = 0 if RP_REQ['COAL'] - rp < 0 else RP_REQ['COAL'] - rp\n    uranium_rp_rem = 0 if RP_REQ['URANIUM'] - rp < 0 else RP_REQ['URANIUM'] - rp\n    \n    rp_req = {'wood': 0, 'coal': coal_rp_rem, 'uranium': uranium_rp_rem}\n    \n    # Energy of Resources\n    resources = torch.zeros([s,s,3])\n    for row in range(h):\n        for col in range(w):\n            resource = game_state.map.map[row][col].resource\n            if resource==None:\n                continue\n            else:\n                resources[row+pad,col+pad] = torch.tensor([resource.amount,res_token[resource.type],rp_req[resource.type]])\n\n    cities = torch.zeros([2,s,s,3])\n    units = torch.zeros([2,s,s,5])\n    entity_type = []\n    entity_info = []\n    extra = []\n    for idx,player in enumerate(game_state.players):\n        for city in player.cities.values():\n            for tile in city.citytiles:\n                cities[idx,tile.pos.y+pad,tile.pos.x+pad] = torch.tensor([tile.cooldown,city.fuel,city.light_upkeep])\n                if idx==0:\n                    entity_info.append([tile.cooldown,city.fuel,tile.pos.x,tile.pos.y])\n                    if tile.cooldown<=0.:\n                        entity_type.append(2)\n                    else:\n                        extra.append(3)\n\n        for unit in player.units:\n            units[idx,unit.pos.y+pad,unit.pos.x+pad] = torch.tensor([unit.type+1,unit.cooldown,unit.cargo.wood,unit.cargo.coal,unit.cargo.uranium])\n            fuel = sum([unit.cargo.wood*FUEL_RATE['WOOD'], unit.cargo.coal*FUEL_RATE['COAL'], unit.cargo.uranium*FUEL_RATE['URANIUM']])\n            if idx==0:\n                entity_info.append([unit.cooldown,fuel,unit.pos.x,unit.pos.y])\n                if unit.cooldown<=0.:\n                    entity_type.append(unit.type)\n                else:\n                    extra.append(3)\n    \n    resources = resources.permute(2,0,1).unsqueeze(0)\n    cities = cities.permute(3,0,1,2)\n    units = units.permute(3,0,1,2)\n    entity_type = torch.tensor(entity_type + extra)\n    entity_info = torch.tensor(entity_info)\n    return resources, cities, units, entity_type, entity_info\n\ndef get_nearest_cart(player, unit):\n    nearest_unit = None\n    if len(player.units) > 1:\n        for potential_unit in player.units:\n            if potential_unit.id != unit.id:\n                dist = potential_cart.pos.distance_to(unit.pos)\n                if dist == 1:\n                    nearest_unit = potential_unit\n    return nearest_unit\n\ndef get_res(unit):\n    res = RES['WOOD']\n    max_amount = unit.cargo.wood\n    if unit.cargo.coal > max_amount:\n        res = RES['COAL']\n        max_amount = unit.cargo.coal\n    if unit.cargo.uranium > max_amount:\n        res = RES['URANIUM']\n        max_amount = unit.cargo.uranium\n    return res, max_amount\n\ndef action_strings(player, acts):\n    actions = []\n    done = False\n\n    if game_state.turn==359 or len(player.cities)==0 and len(player.units)==0:\n        done = True\n    \n    actors = []\n    for city in player.cities.values():\n        for citytile in city.citytiles:\n            actors.append(citytile)\n    for unit in player.units:\n        actors.append(unit)\n\n    for act,actor in zip(acts,actors):\n        if actor.can_act():\n            try:\n                if act==0:\n                    action = actor.build_worker()\n                    actions.append(action)\n                if act==1:\n                    action = actor.research()\n                    actions.append(action)\n                if act==2:\n                    action = actor.move(DIR['NORTH'])\n                    actions.append(action)\n                if act==3:\n                    action = actor.move(DIR['SOUTH'])\n                    actions.append(action)\n                if act==4:\n                    action = actor.move(DIR['EAST'])\n                    actions.append(action)\n                if act==5:\n                    action = actor.move(DIR['WEST'])\n                    actions.append(action)\n                if act==6:\n                    action = actor.move(DIR['CENTER'])\n                    actions.append(action)\n                if act==7 and actor.type==UNIT_TYPE['WORKER']:\n                    res, amount = get_res(actor)\n                    unit = get_nearest_unit(player, actor)\n                    if unit:\n                        action = unit.transfer(unit.id, res, amount)\n                        actions.append(action)\n                if act==8 and actor.type==UNIT_TYPE['WORKER']:\n                    action = actor.build_city()\n                    actions.append(action)\n#                 if act==9:\n#                     action = actor.build_cart()\n#                     actions.append(action)\n#                 if act==10 and actor.type==UNIT_TYPE['WORKER']:\n#                     action = actor.pillage()\n#                     actions.append(action)\n            except Exception:\n                continue\n    return actions, done","da5cc0a4":"def supervising_agent(observation, configuration):\n    global game_state, last_state, reward\n    \n    ### Do not edit ###\n    if observation[\"step\"] == 0:\n        game_state = Game()\n        game_state._initialize(observation[\"updates\"])\n        game_state._update(observation[\"updates\"][2:])\n        game_state.id = observation.player\n    else:\n        game_state._update(observation[\"updates\"])\n        \n    actions = []\n    \n    ### AI Code goes down here! ### \n    player = game_state.players[observation.player]\n    opponent = game_state.players[(observation.player + 1) % 2]\n    width, height = game_state.map.width, game_state.map.height\n    \n    true_actions = []\n    state = get_input()\n    if observation['step']==0:\n        last_state = state[:3]\n    if len(state[3])>0:\n        pred = luxagent.sl_choose_actions(state, last_state)\n        true_actions = json_steps[observation[\"step\"]+1][0]['action']\n        acts, done = action_strings(player, pred)\n        luxagent.supervise_learn(state, last_state, true_actions)\n        if config.debug_print:\n            print(f'{acts}   {true_actions}')\n    last_state = state[:3]\n    return true_actions","8195bffa":"def action_agent(observation, configuration):\n    global game_state\n    \n        ### Do not edit ###\n    if observation[\"step\"] == 0:\n        index = 0\n        game_state = Game()\n        game_state._initialize(observation[\"updates\"])\n        game_state._update(observation[\"updates\"][2:])\n        game_state.id = observation.player\n    else:\n        game_state._update(observation[\"updates\"])\n        \n    actions = []\n    \n    ### AI Code goes down here! ### \n    player = game_state.players[observation.player]\n    opponent = game_state.players[(observation.player + 1) % 2]\n    width, height = game_state.map.width, game_state.map.height\n    \n    try:\n        actions = json_steps[observation[\"step\"]+1][observation.player][\"action\"]\n    except Exception:\n        pass\n    return actions","c00fe9a7":"def supervise_train():\n    global json_steps\n    episode_dir = '..\/input\/lux-ai-toad-brigade-episodes'\n    episodes = [path for path in Path(episode_dir).glob('*.json') if 'output' not in path.name]\n    \n    callback.on_train_begin()\n    list_of_episodes = episodes[:50]\n    for idx, filepath in enumerate(list_of_episodes):\n        with open(filepath) as f:\n#             if len(list_of_episodes)-1 == idx:\n#                 config.debug_print=True\n            json_load = json.load(f)\n            seed = json_load['configuration']['seed']\n            json_steps = json_load['steps']\n            callback.on_game_begin()\n            env = make(\"lux_ai_2021\", configuration={\"loglevel\": 0, \"annotations\": True, \"seed\": seed}, debug=True)\n            steps = env.run([supervising_agent, action_agent])\n            callback.on_game_end(idx)\n#         env.render(mode=\"ipython\", width=1200, height=800)\n#         raise SystemExit(\"Stop right there!\")\n    callback.on_train_end()\n\nconfig.debug_print = False\nconfig.phase = 'supervise'\ncallback = Callback()\nluxagent = LuxAgent()\nsupervise_train()","17e8e030":"config.lr = 1e-5\nconfig.debug_print = False","2c31beb0":"class Rewards:\n    def __init__(self):\n        self.citytiles = 1\n        self.workers = 1\n        self.carts = 0\n        self.rp = 0\n    \n    def reset(self):\n        self.citytiles = 1\n        self.workers = 1\n        self.carts = 0\n        self.rp = 0\n\ndef calc_reward(player):\n    total_fuel, citytiles, workers, carts, rp = 0,0,0,0,0\n    player_id = player.team\n    \n    citytiles = player.city_tile_count\n    rp = player.research_points\n    for city in player.cities.values():\n        total_fuel += city.fuel\n    for unit in player.units:\n        workers += 1 if unit.type == UNIT_TYPE['WORKER'] else 0\n        carts += 1 if unit.type == UNIT_TYPE['CART'] else 0\n        total_fuel += unit.cargo.wood * FUEL_RATE['WOOD']\n        total_fuel += unit.cargo.coal * FUEL_RATE['COAL']\n        total_fuel += unit.cargo.uranium * FUEL_RATE['URANIUM']\n    fuel_reward = math.log(total_fuel,100) if total_fuel>0 else -1\n    city_reward = citytiles - rewards[player_id].citytiles\n    work_reward = workers - rewards[player_id].workers\n    cart_reward = carts - rewards[player_id].carts\n    rp___reward = rp - rewards[player_id].rp if rp < RP_REQ['URANIUM'] else 0\n\n    rewards[player_id].citytiles = citytiles\n    rewards[player_id].workers = workers\n    rewards[player_id].carts = carts\n    rewards[player_id].rp = rp\n    \n    return city_reward + fuel_reward + work_reward + cart_reward + rp___reward\n\ndef reinforce_agent(observation, configuration):\n    global game_state, obs, last_state, mem_last_state, reward\n    obs = observation\n    \n    ### Do not edit ###\n    if observation[\"step\"] == 0:\n        game_state = Game()\n        game_state._initialize(observation[\"updates\"])\n        game_state._update(observation[\"updates\"][2:])\n        game_state.id = observation.player\n    else:\n        game_state._update(observation[\"updates\"])\n        \n    actions = []\n    \n    ### AI Code goes down here! ### \n    player = game_state.players[observation.player]\n    opponent = game_state.players[(observation.player + 1) % 2]\n    width, height = game_state.map.width, game_state.map.height\n\n    state = get_input()\n    if observation['step']==0:\n        last_state = state[:3]\n        mem_last_state = (state, last_state)\n    pred = luxagent.rl_choose_actions(state, last_state)\n    actions, done = action_strings(player, pred)\n    \n    reward = calc_reward(player)\n    \n    mem_state = (state, last_state)\n    luxagent.store_memory(mem_state, pred, reward, mem_last_state, done)\n    luxagent.reinforcement_learn()\n    mem_last_state = (state, last_state)\n    last_state = state[:3]\n    return actions","579cb79b":"def reinforce_train():\n    global env\n    sizes = [12,16,24,32]\n    sizes = [12]\n    game_num = 0\n    \n    callback.on_train_begin()\n    for epoch in range(config.epochs):\n        callback.on_epoch_begin(epoch)\n        for idx,size in enumerate(sizes):\n            callback.on_game_begin()\n            env = make(\"lux_ai_2021\", configuration={\"loglevel\": 0, \"width\": size, \"height\": size, \"annotations\": True}, debug=True)\n            steps = env.run([reinforce_agent, \"random_agent\"])\n            rewards[0].reset()\n            rewards[1].reset()\n            callback.on_game_end(game_num)\n            game_num += 1\n        callback.on_epoch_end()\n    callback.on_train_end()\n    env.render(mode=\"ipython\", width=1200, height=800)\n\nrewards = {0:Rewards(), 1:Rewards()}\nconfig.phase = 'reinforce'\ncallback = Callback()\nluxagent = LuxAgent()\nreinforce_train()","f6c5b031":"env = make(\"lux_ai_2021\", configuration={\"loglevel\": 0, \"annotations\": True}, debug=True)\nsteps = env.run([agent, base_agent])\nenv.render(mode=\"ipython\", width=1200, height=800)","3d122fda":"%%writefile agent.py\n\nimport torch\nimport numpy as np\nfrom lux.game_constants import GAME_CONSTANTS\nfrom lux.game import Game\nfrom lux import annotate,game\nfrom lux.game_map import Position\n\nRES = GAME_CONSTANTS['RESOURCE_TYPES']\nRP_REQ = GAME_CONSTANTS['PARAMETERS']['RESEARCH_REQUIREMENTS']\nDIR = GAME_CONSTANTS['DIRECTIONS']\nUNIT_TYPE = GAME_CONSTANTS['UNIT_TYPES']\nFUEL_RATE = GAME_CONSTANTS['PARAMETERS']['RESOURCE_TO_FUEL_RATE']\n\ndef get_input():\n    w,h = game_state.map_width, game_state.map_height\n    s = config.base_size\n    pad = s - w\n    rp = game_state.players[0].research_points\n    \n    res_token = {'wood': 1, 'coal': 2, 'uranium': 3}\n    \n    coal_rp_rem = 0 if RP_REQ['COAL'] - rp < 0 else RP_REQ['COAL'] - rp\n    uranium_rp_rem = 0 if RP_REQ['URANIUM'] - rp < 0 else RP_REQ['URANIUM'] - rp\n    \n    rp_req = {'wood': 0, 'coal': coal_rp_rem, 'uranium': uranium_rp_rem}\n    \n    # Energy of Resources\n    resources = torch.zeros([s,s,3])\n    for row in range(h):\n        for col in range(w):\n            resource = game_state.map.map[row][col].resource\n            if resource==None:\n                continue\n            else:\n                resources[row+pad][col+pad] = torch.tensor([resource.amount,res_token[resource.type],rp_req[resource.type]])\n\n    cities = torch.zeros([2,s,s,3])\n    units = torch.zeros([2,s,s,5])\n    entity_type = []\n    entity_info = []\n    for idx,player in enumerate(game_state.players):\n        for city in player.cities.values():\n            for tile in city.citytiles:\n                cities[idx][tile.pos.y+pad][tile.pos.x+pad] = torch.tensor([tile.cooldown,city.fuel,city.light_upkeep])\n                entity_type.append(2)\n                entity_info.append([tile.cooldown,city.fuel,tile.pos.x,tile.pos.y])\n\n        for unit in player.units:\n            units[idx][unit.pos.y+pad][unit.pos.x+pad] = torch.tensor([unit.type+1,unit.cooldown,unit.cargo.wood,unit.cargo.coal,unit.cargo.uranium])\n            fuel = sum([unit.cargo.wood*FUEL_RATE['WOOD'], unit.cargo.coal*FUEL_RATE['COAL'], unit.cargo.uranium*FUEL_RATE['URANIUM']])\n            entity_type.append(unit.type)\n            entity_info.append([unit.cooldown,fuel,unit.pos.x,unit.pos.y])\n    \n    resources = resources.permute(2,0,1).unsqueeze(0)\n    cities = cities.permute(3,0,1,2)\n    units = units.permute(3,0,1,2)\n    entity_type = torch.tensor(entity_type)\n    entity_info = torch.tensor(entity_info)\n    return resources, cities, units, entity_type, entity_info\n\ndef get_nearest_cart(player, unit):\n    nearest_cart = None\n    if len(player.units) > 1:\n        for potential_cart in player.units:\n            if potential_cart.id[0]=='c' and potential_cart.id!=unit.id:\n                dist = potential_cart.pos.distance_to(unit.pos)\n                if dist == 1:\n                    nearest_cart = potential_cart\n    return nearest_cart\n\ndef get_res(unit):\n    res = RES['WOOD']\n    max_amount = unit.cargo.wood\n    if unit.cargo.coal > max_amount:\n        res = RES['COAL']\n        max_amount = unit.cargo.coal\n    if unit.cargo.uranium > max_amount:\n        res = RES['URANIUM']\n        max_amount = unit.cargo.uranium\n    return res, max_amount\n\ndef action_strings(player, acts):\n    actions = []\n    done = False\n\n    if game_state.turn==359 or len(player.cities)==0 and len(player.units)==0:\n        done = True\n    \n    actors = []\n    for city in player.cities.values():\n        for citytile in city.citytiles:\n            actors.append(citytile)\n    for unit in player.units:\n        actors.append(unit)\n    \n    for act,actor in zip(acts,actors):\n        if actor.can_act():\n            try:\n                if act==0:\n                    action = actor.build_worker()\n                    actions.append(action)\n                if act==1:\n                    action = actor.build_cart()\n                    actions.append(action)\n                if act==2:\n                    action = actor.research()\n                    actions.append(action)\n                if act==3:\n                    action = actor.move(DIR['NORTH'])\n                    actions.append(action)\n                if act==4:\n                    action = actor.move(DIR['SOUTH'])\n                    actions.append(action)\n                if act==5:\n                    action = actor.move(DIR['EAST'])\n                    actions.append(action)\n                if act==6:\n                    action = actor.move(DIR['WEST'])\n                    actions.append(action)\n                if act==7:\n                    action = actor.move(DIR['CENTER'])\n                    actions.append(action)\n                if act==8:\n                    res, amount = get_res(actor)\n                    cart = get_nearest_cart(player, actor)\n                    if cart:\n                        action = unit.transfer(cart.id, res, amount)\n                        actions.append(action)\n                if act==9 and actor.type==UNIT_TYPE['WORKER']:\n                    action = actor.pillage()\n                    actions.append(action)\n                if act==10 and actor.type==UNIT_TYPE['WORKER']:\n                    action = actor.build_city()\n                    actions.append(action)\n            except Exception:\n                continue\n    return actions, done\n\ndef agent(observation, configuration):\n    global game_state, model\n    \n    ### Do not edit ###\n    if observation[\"step\"] == 0:\n        game_state = Game()\n        game_state._initialize(observation[\"updates\"])\n        game_state._update(observation[\"updates\"][2:])\n        game_state.id = observation.player\n        model = torch.load('.\/models\/qeval\/lux_model_qeval.pth')\n        model.eval()\n    else:\n        game_state._update(observation[\"updates\"])\n        \n    actions = []\n    \n    ### AI Code goes down here! ### \n    player = game_state.players[observation.player]\n    opponent = game_state.players[(observation.player + 1) % 2]\n    width, height = game_state.map.width, game_state.map.height\n\n    state = get_input()\n    preds = rl_choose_actions(state)\n    actions,_ = action_strings(player, preds)\n    return actions","d7c01bba":"# Reinforcement Learning","f535c888":"!tar --exclude='*.ipynb' --exclude=\"*.pyc\" --exclude=\"*.pkl\" --exclude='models\/qnext\/*' -czf submission.tar.gz *","d3e545b9":"# Supervised Learning","cbb32605":"# David White's Rule-Based Agent","6375b3a4":"!cp -r ..\/input\/lux-ai-2021\/* ."}}