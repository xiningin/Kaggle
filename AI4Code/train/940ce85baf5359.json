{"cell_type":{"463d8adb":"code","842a0f83":"code","f47995c0":"code","5d8890e0":"code","d404107f":"code","2cdff2e6":"code","16bc68f2":"code","cef6dce9":"code","7a151c43":"code","f97e7a01":"code","fd317cc9":"code","7dcc8e8f":"code","58ec93b2":"code","c158ac41":"code","877fd832":"code","01d4d7a1":"code","3eeae1e3":"code","dfa232e4":"code","417db4c6":"code","e03fce91":"code","b4ea4fcc":"code","ff09f887":"code","bcf6dfb8":"code","5ab80055":"code","7d4c3f76":"markdown","d1cd6a0e":"markdown","47c94e45":"markdown","01b2c322":"markdown","f8d29ae0":"markdown","586a239a":"markdown","bf341cd9":"markdown","4ac19036":"markdown","76e6cbca":"markdown","d169c2de":"markdown","a4fcb80b":"markdown","ec4d27a5":"markdown","7012a94f":"markdown","6e46c890":"markdown","979a3d77":"markdown","3cc7558a":"markdown","5dc50f9b":"markdown","46fa7428":"markdown","1f585ae0":"markdown","2bf5a52e":"markdown","b3259872":"markdown","5ba0b4bb":"markdown","f0dc92f1":"markdown","5a2ce8aa":"markdown","cea71850":"markdown","55d1adca":"markdown","2beb2f86":"markdown","bb144c40":"markdown","3ca6237a":"markdown","cc629a16":"markdown","f393e013":"markdown","d42225e9":"markdown","c4bfaef7":"markdown","a1dc23ec":"markdown","7c9d1105":"markdown","bc51fe95":"markdown","5b52f54d":"markdown"},"source":{"463d8adb":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nplt.style.use('fivethirtyeight')\n%matplotlib inline","842a0f83":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.model_selection import GridSearchCV","f47995c0":"iris = pd.read_csv('..\/input\/iris\/Iris.csv')","5d8890e0":"iris.head()","d404107f":"iris.info()","2cdff2e6":"iris.describe()","16bc68f2":"print(\"The Average Sepal Length (cm) is : \",round(iris['SepalLengthCm'].mean(),2))\nprint(\"The Average Sepal Width (cm) is : \",round(iris['SepalWidthCm'].mean(),2))\nprint(\"The Average Petal Length (cm) is : \",round(iris['PetalLengthCm'].mean(),2))\nprint(\"The Average Petal Width (cm) is : \",round(iris['PetalWidthCm'].mean(),2))","cef6dce9":"iris.shape","7a151c43":"categorical = iris.select_dtypes(include=[np.object])\nprint(\"Categorical Columns:\",categorical.shape[1])\n\nnumerical = iris.select_dtypes(exclude=[np.object])\nprint(\"Numerical Columns:\",numerical.shape[1])","f97e7a01":"iris.isnull().any().any()","fd317cc9":"le = LabelEncoder()\niris['Species'] = le.fit_transform(iris['Species'])","7dcc8e8f":"iris['Species']","58ec93b2":"X = iris.iloc[:, :-1].values\ny = iris['Species']","c158ac41":"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42, stratify=y)","877fd832":"scaler = StandardScaler()\nscaler.fit(X_train)\n\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)","01d4d7a1":"classifier = KNeighborsClassifier(n_neighbors=5)\nclassifier.fit(X_train, y_train)","3eeae1e3":"y_pred = classifier.predict(X_test)","dfa232e4":"print(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))","417db4c6":"error_rate = []\n\nfor i in range(1,50):\n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train, y_train)\n    pred = knn.predict(X_test)\n    error_rate.append(np.mean(pred != y_test))","e03fce91":"plt.figure(figsize=(12, 6))\nplt.plot(range(1,50), error_rate, color='red', linestyle='dashed', marker='o',\n         markerfacecolor='blue', markersize=10)\nplt.title('Error Rate K Value')\nplt.xlabel('K Value')\nplt.ylabel('Mean Error')","b4ea4fcc":"param_grid = {'n_neighbors':np.arange(1,50)}","ff09f887":"knn = KNeighborsClassifier()\nknn_cv= GridSearchCV(knn,param_grid,cv=5)\nknn_cv.fit(X,y)","bcf6dfb8":"round(knn_cv.best_score_,2)*100","5ab80055":"knn_cv.best_params_","7d4c3f76":"## Feature Scaling\nBefore making any actual predictions, it is always a good practice to scale the features so that all of them can be uniformly evaluated.","d1cd6a0e":"<a id=\"1\"><\/a>\n<h2 style='background:black; border:10; color:white'><center>1) WHAT IS KNN?<\/center><h2>","47c94e45":"![k-nearest-neighbors-algorithm-python-scikit-learn-1.png](attachment:k-nearest-neighbors-algorithm-python-scikit-learn-1.png)\n\nYour task is to classify a new data point with 'X' into \"Blue\" class or \"Red\" class. The coordinate values of the data point are x=45 and y=50. Suppose the value of K is 3. The KNN algorithm starts by calculating the distance of point X from all the points. It then finds the 3 nearest points with least distance to point X. This is shown in the figure below. The three nearest points have been encircled.\n\n","01b2c322":"<a id=\"2\"><\/a>\n<h2 style='background:black; border:10; color:white'><center>2) THEORY <\/center><h2>","f8d29ae0":"<a id=\"7\"><\/a>\n<h2 style='background:black; border:10; color:white'><center>7) READING AND UNDERSTANDING DATA<\/center><h2>","586a239a":"The intuition behind the KNN algorithm is one of the simplest of all the supervised machine learning algorithms. It simply calculates the distance of a new data point to all other training data points. The distance can be of any type e.g Euclidean or Manhattan etc. It then selects the K-nearest data points, where K can be any integer. Finally it assigns the data point to the class to which the majority of the K data points belong.","bf341cd9":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='font-size:40px;background:black; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>TABLE OF CONTENTS<\/center><\/h2>","4ac19036":"<a id=\"10\"><\/a>\n<h2 style='background:black; border:10; color:white'><center>10) FEATURE SCALING <\/center><h2>","76e6cbca":"<a id=\"3\"><\/a>\n<h2 style='background:black; border:10; color:white'><center>3) PROS AND CONS<\/center><h2>","d169c2de":"### We have no **missing values**.","a4fcb80b":"The **K-nearest neighbors (KNN)** algorithm is a type of supervised machine learning algorithms. KNN is extremely easy to implement in its most basic form, and yet performs quite complex classification tasks. It is a lazy learning algorithm since it doesn't have a specialized training phase. Rather, it uses all of the data for training while classifying a new data point or instance. KNN is a non-parametric learning algorithm, which means that it doesn't assume anything about the underlying data. This is an extremely useful feature since most of the real world data doesn't really follow any theoretical assumption e.g. linear-separability, uniform distribution, etc.","ec4d27a5":"<a id=\"4\"><\/a>\n<h2 style='background:black; border:10; color:white'><center>4) HOW TO IMPROVE KNN? <\/center><h2>","7012a94f":"Let's see this algorithm in action with the help of a simple example. Suppose you have a dataset with two variables, which when plotted, looks like the one in the following figure.\n\n","6e46c890":"![thankyou.jpg](attachment:thankyou.jpg)","979a3d77":"The above command splits the dataset into 80% train data and 20% test data. This means that out of total 150 records, the training set will contain 120 records and the test set contains 30 of those records.","3cc7558a":"## Train Test Split\nTo avoid over-fitting, we will divide our dataset into training and test splits, which gives us a better idea as to how our algorithm performed during the testing phase. This way our algorithm is tested on un-seen data, as it would be in a production application.","5dc50f9b":"<a id=\"6\"><\/a>\n<h2 style='background:black; border:10; color:white'><center>6) IMPORTING LIBRARIES <\/center><h2>","46fa7428":"<a id=\"13\"><\/a>\n<h2 style='background:black; border:10; color:white'><center>13) MODEL PERFORMANCE USING CROSS-VALIDATION <\/center><h2>","1f585ae0":"## PROS:\n\n* It is extremely easy to implement.\n\n* As said earlier, it is lazy learning algorithm and therefore requires no training prior to making real time predictions. This makes the KNN algorithm much faster than other algorithms that require training e.g SVM, linear regression, etc.\n\n* Since the algorithm requires no training before making predictions, new data can be added seamlessly.\n\n* There are only two parameters required to implement KNN i.e. the value of K and the distance function (e.g. Euclidean or Manhattan etc.)\n\n## CONS:\n\n* The KNN algorithm doesn't work well with high dimensional data because with large number of dimensions, it becomes difficult for the algorithm to calculate distance in each dimension.\n\n* The KNN algorithm has a high prediction cost for large datasets. This is because in large datasets the cost of calculating distance between new point and each existing point becomes higher.\n\n* Finally, the KNN algorithm doesn't work well with categorical features since it is difficult to find the distance between dimensions with categorical features.","2bf5a52e":"At this point, the question arises that How to choose the optimal number of neighbors? \nAnd what are its effects on the classifier? \n\nThe number of neighbors(K) in KNN is a hyperparameter that you need choose at the time of model building. You can think of K as a controlling variable for the prediction model.\n\nResearch has shown that no optimal number of neighbors suits all kind of data sets. Each dataset has it's own requirements. In the case of a small number of neighbors, the noise will have a higher influence on the result, and a large number of neighbors make it computationally expensive. Research has also shown that a small amount of neighbors are most flexible fit which will have low bias but high variance and a large number of neighbors will have a smoother decision boundary which means lower variance but higher bias.\n\nGenerally, Data scientists choose as an **odd number if the number of classes is even**. You can also check by generating the model on different values of k and check their performance. You can also try **Elbow method** here.","b3259872":"## The algorithm works best with 2 neighbors giving accuracy of 88%","5ba0b4bb":"<a id=\"9\"><\/a>\n<h2 style='background:black; border:10; color:white'><center>9) ENCODING THE CATEGORICAL COLUMN<\/center><h2>","f0dc92f1":"From the output we can see that the mean error is zero when the value of the K is between **1 and 48**.","5a2ce8aa":"![maxresdefault.jpg](attachment:maxresdefault.jpg)","cea71850":"    \n* [WHAT IS KNN?](#1)\n* [THEORY](#2)\n* [PROS AND CONS](#3)\n* [HOW TO IMPROVE KNN?](#4)\n* [OPTIMUM NUMBER OF NEIGHBORS IN K](#5)\n* [IMPORTING LIBRARIES](#6)\n* [READING AND UNDERSTANDING DATA](#7)\n* [DATA INSPECTION](#8)\n* [ENCODING CATEGORICAL COLUMNS](#9)\n* [FEATURE SCALING](#10)\n* [MODEL BUILDING](#11)\n* [MODEL EVALUATION](#12)\n* [MODEL PERFORMANCE USING CROSS-VALIDATION](#13)","55d1adca":"<a id=\"11\"><\/a>\n<h2 style='background:black; border:10; color:white'><center>11) MODEL BUILDING <\/center><h2>","2beb2f86":"<a id=\"12\"><\/a>\n<h2 style='background:black; border:10; color:white'><center>12) MODEL EVALUATION <\/center><h2>","bb144c40":"For evaluating an algorithm, confusion matrix, precision, recall and f1 score are the most commonly used metrics.\n","3ca6237a":"### We have 1 Categorical column and 5 Numerical columns.","cc629a16":"![k-nearest-neighbors-algorithm-python-scikit-learn-2.png](attachment:k-nearest-neighbors-algorithm-python-scikit-learn-2.png)\n\nThe final step of the KNN algorithm is to assign new point to the class to which majority of the three nearest points belong. From the figure above we can see that the two of the three nearest points belong to the class \"Red\" while one belongs to the class \"Blue\". Therefore the new data point will be classified as \"Red\".","f393e013":"<a id=\"5\"><\/a>\n<h2 style='background:black; border:10; color:white'><center>5) OPTIMUM NUMBER OF NEIGHBORS IN K<\/center><h2>","d42225e9":"For better results, normalizing data on the same scale is highly recommended. Generally, the normalization range considered between 0 and 1. KNN is not suitable for the large dimensional data. In such cases, dimension needs to reduce to improve the performance. Also, handling missing values will help us in improving results.","c4bfaef7":"The first step is to import the KNeighborsClassifier class from the sklearn.neighbors library. In the second line, this class is initialized with one parameter, i.e. n_neigbours. This is basically the value for the K. There is no ideal value for K and it is selected after testing and evaluation, however to start out, 5 seems to be the most commonly used value for KNN algorithm.","a1dc23ec":"<a id=\"8\"><\/a>\n<h2 style='background:black; border:10; color:white'><center>8) DATA INSPECTION<\/center><h2>","7c9d1105":"### We have 150 rows and 6 columns.","bc51fe95":"The **X** variable contains the **attributes** while **y** contains the **labels**.","5b52f54d":"The results show that our KNN algorithm was able to classify 30 records in the test set with 100% accuracy, which is excellent. Although the algorithm performed very well with this dataset, don't expect the same results with all applications. As noted earlier, KNN doesn't always perform as well with high-dimensionality or categorical features."}}