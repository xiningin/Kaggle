{"cell_type":{"74fb8b39":"code","b4120da7":"code","e0d83f62":"code","ac7ec58f":"code","1b901441":"code","d4756d9b":"code","52ff97aa":"code","fbbfc267":"code","a74143df":"code","e76cb363":"code","c1f7d00a":"code","73134d68":"code","8d18633f":"code","21659060":"code","ba00a172":"code","7fb69e1a":"code","399f72a0":"code","5900abc3":"code","60ee0375":"code","4cbe02f8":"code","ff7eb55d":"code","786e7d6e":"code","b6adf8de":"code","7aa10010":"code","16999199":"code","0df10c9d":"code","e3aa4e16":"code","752d582d":"code","3c9d0166":"code","6e69760a":"code","95e564e8":"code","f0a72a93":"code","daafc3d1":"code","91ffce3b":"code","0181cd9e":"code","966e209f":"code","0260cf08":"code","814e16a6":"code","cf888cd8":"code","3b17a216":"code","9c71fc17":"code","57affccf":"code","37e5716b":"code","dbe86024":"code","63fde2c4":"code","0afbcbfe":"code","7bfdc20b":"code","d31143a4":"code","584e49f1":"code","182e9a88":"code","7ac04167":"code","47c81566":"markdown","677a0f15":"markdown","d53c1880":"markdown","0fafb317":"markdown","381fe722":"markdown","120b177b":"markdown"},"source":{"74fb8b39":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b4120da7":"import torch\nif torch.cuda.is_available():  \n    device = torch.device(\"cuda\")\n    print('We will use the GPU:', torch.cuda.get_device_name(0))\n    \nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")","e0d83f62":"train_data = pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")\ntest_data = pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")","ac7ec58f":"train_data.head()","1b901441":"train_data['keyword'].value_counts()","d4756d9b":"train_data['keyword'].nunique()\n","52ff97aa":"train_data['location'].value_counts()\n","fbbfc267":"train_data[\"text\"][4]","a74143df":"import seaborn as sns\nimport matplotlib as plt\nax = sns.countplot(train_data.target)","e76cb363":"import string\nimport re\nimport nltk\nnltk.download('stopwords')\nstopwords = nltk.corpus.stopwords.words('english')\n\n# data preprocessing\ndef clean_text(text):\n    text = text.lower()\n    # remove hyperlinks\n    text = re.sub(r\"http\\S+\", \"\", text)\n    # remove spcl characters\n    text = \"\".join([word for word in text if word not in string.punctuation])\n    text = re.sub(\"\\W\", \" \", str(text))\n    # remove stopwords\n    text = [word for word in text.split() if word not in stopwords]\n    # remove any numeric characters\n    text = [word for word in text if re.search(\"\\d\", word)== None]\n    # convert split to text again\n    text = ' '.join(word for word in text)\n    return text\n\n# train_data['text_clean'] = train_data['text'].apply(lambda x: clean_text(x))\ntrain_data['text_clean'] = train_data['text'].apply(clean_text)\n\ntest_data['text_clean'] = test_data['text'].apply(clean_text)","c1f7d00a":"\ntrain_data.head()","73134d68":"train_data.drop(['id','keyword','location', 'text'], axis = 1) \n","8d18633f":"test_data.head()","21659060":"test_data.drop(['id','keyword','location', 'text'], axis = 1) \n","ba00a172":"train_data[\"target\"].value_counts()\n","7fb69e1a":"data = train_data['text_clean'].values\nlabels = train_data['target'].values","399f72a0":"from transformers import ElectraTokenizer, ElectraForSequenceClassification,AdamW\nimport torch\ntokenizer = ElectraTokenizer.from_pretrained('google\/electra-base-discriminator')\nmodel = ElectraForSequenceClassification.from_pretrained('google\/electra-base-discriminator',num_labels=2)\nmodel.cuda()\n","5900abc3":"#important to know the max len of each sentence\n\nimport matplotlib.pyplot as plt\ndef plot_sentence_embeddings_length(text_list, tokenizer):\n    tokenized_texts = list(map(lambda t: tokenizer.tokenize(t), text_list))\n    tokenized_texts_len = list(map(lambda t: len(t), tokenized_texts))\n    fig, ax = plt.subplots(figsize=(8, 5));\n    ax.hist(tokenized_texts_len, bins=60);\n    ax.set_xlabel(\"Length of Comment Embeddings\");\n    ax.set_ylabel(\"Number of Comments\");\n    return max(tokenized_texts_len)\n\n\nplot_sentence_embeddings_length(data, tokenizer)","60ee0375":"token_lens = []\nfor txt in data:\n    \n    tokens = tokenizer.encode(txt, max_length=70)\n    token_lens.append(len(tokens))\n\nsns.distplot(token_lens)\nplt.xlim([0, 40]);\nplt.xlabel('Token count')","4cbe02f8":"max(token_lens)","ff7eb55d":"indices=tokenizer.batch_encode_plus(data,max_length=38,add_special_tokens=True, return_attention_mask=True,pad_to_max_length=True,truncation=True)","786e7d6e":"indices.keys()","b6adf8de":"input_ids=indices[\"input_ids\"]\nattention_masks=indices[\"attention_mask\"]","7aa10010":"from sklearn.model_selection import train_test_split\n\n# Use 99% for training and 1% for validation.\ntrain_ids, val_ids, train_labels, val_labels = train_test_split(input_ids, labels, \n                                                            random_state=42, test_size=0.2)\n# Do the same for the masks.\ntrain_masks, val_masks, _, _ = train_test_split(attention_masks, labels,\n                                             random_state=42, test_size=0.2)","16999199":"len(train_ids)\n# len(train_labels)","0df10c9d":"train_ids = torch.tensor(train_ids)\nval_ids = torch.tensor(val_ids)\ntrain_labels = torch.tensor(train_labels)\nval_labels = torch.tensor(val_labels)\ntrain_masks = torch.tensor(train_masks)\nval_masks = torch.tensor(val_masks)","e3aa4e16":"\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n#TRAINING DATA\n\ntrain_data = TensorDataset(train_ids, train_masks, train_labels)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data,sampler = train_sampler,batch_size = 32)","752d582d":"len(train_dataloader)","3c9d0166":"train_iter3 = iter(train_dataloader)\nprint(type(train_iter3))\n\nprint(len(train_iter3))","6e69760a":"#Validation Data\n\nval_data = TensorDataset(val_ids, val_masks, val_labels)\nval_sampler = RandomSampler(val_data)\nval_dataloader = DataLoader(val_data,sampler = val_sampler,batch_size = 32)","95e564e8":"optimizer = AdamW(model.parameters(),lr = 6e-6, # args.learning_rate - default is 5e-5, our notebook had 2e-5\neps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n                 )\n","f0a72a93":"from transformers import get_linear_schedule_with_warmup\n\n# Number of training epochs (authors recommend between 2 and 4)\nepochs = 5\n\n# Total number of training steps is number of batches * number of epochs.\ntotal_steps = len(train_dataloader) * epochs\n\n# Create the learning rate scheduler.\nscheduler = get_linear_schedule_with_warmup(optimizer, \n                                            num_warmup_steps = 0, \n                                            num_training_steps = total_steps)","daafc3d1":"import numpy as np\n","91ffce3b":"def flat_accuracy(preds, labels):\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return np.sum(pred_flat == labels_flat) \/ len(labels_flat)","0181cd9e":"def acc_score(y_pred, y_true):\n    # correct labels = 0\n    cor = 0\n    # loop over all the entries in test data\n    for i in range(len(y_pred)):\n        # if predicted = actual label, add 1 to correct labels\n        if(y_pred[i] == y_true[i]):\n            cor +=1\n    # return accuracy score\n    return cor\/len(y_pred)","966e209f":"import time\nimport datetime\n\ndef format_time(elapsed):\n    '''\n    Takes a time in seconds and returns a string hh:mm:ss\n    '''\n    # Round to the nearest second.\n    elapsed_rounded = int(round((elapsed)))\n    \n    # Format as hh:mm:ss\n    return str(datetime.timedelta(seconds=elapsed_rounded))","0260cf08":"import random\n\n# This training code is based on the `run_glue.py` script here:\n# https:\/\/github.com\/huggingface\/transformers\/blob\/5bfcd0485ece086ebcbed2d008813037968a9e58\/examples\/run_glue.py#L128\n\n\n# Set the seed value all over the place to make this reproducible.\nseed_val = 42\n\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)","814e16a6":"# Store the average loss after each epoch so we can plot them.\nloss_values = []\n","cf888cd8":"for epoch_i in range(epochs):\n    print(\"epoch is\" + str(epoch_i))\n    print(\"training...\")\n    t0 = time.time()\n    total_loss = 0\n    model.train()\n    for step,batch in enumerate(train_dataloader): # total steps are 191... runs from step 0 to steps 190\n        print(\"step\",step)\n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        b_labels = batch[2].to(device)\n        model.zero_grad()\n        outputs = model(b_input_ids,token_type_ids = None, attention_mask = b_input_mask,labels = b_labels)\n        loss = outputs[0]\n        total_loss += loss.item()\n        loss. backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        scheduler.step()\n        avg_train_loss = total_loss \/ len(train_dataloader)\n        print(\"avg_train_loss\",avg_train_loss)\n        loss_values.append(avg_train_loss)\n        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n        print(\"Training complete!\")\n","3b17a216":"model.eval()\n\npred = []\ntrue = []\neval_acc = 0\nnb_eval_steps = 0\n\nfor batch in val_dataloader:\n    batch = tuple(t.to(device) for t in batch)\n    \n    b_input_ids, b_input_mask, b_labels = batch\n    with torch.no_grad():\n        outputs = model(b_input_ids,token_type_ids = None, attention_mask = b_input_mask)\n        logits = outputs[0]\n        \n        logits = logits.detach().cpu().numpy()\n        label_ids = b_labels.to('cpu').numpy()\n        \n        pred.append(logits)\n        true.append(label_ids)\n        temp_eval_acc = flat_accuracy(logits,label_ids)\n        eval_acc += temp_eval_acc\n        nb_eval_steps += 1\n        \nprint(\"  Accuracy: {0:.2f}\".format(eval_acc\/nb_eval_steps))","9c71fc17":"# Combine the predictions for each batch into a single list of 0s and 1s.\nflat_predictions = [item for sublist in pred for item in sublist]\nflat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n# Combine the correct labels for each batch into a single list.\nflat_true_labels = [item for sublist in true for item in sublist]","57affccf":"from sklearn.metrics import classification_report\nprint(classification_report(flat_predictions,flat_true_labels))","37e5716b":"model.eval()\n\npred = []\ntrue = []\neval_acc = 0\nnb_eval_steps = 0\n\nfor batch in val_dataloader:\n    batch = tuple(t.to(device) for t in batch)\n    \n    b_input_ids, b_input_mask, b_labels = batch\n    with torch.no_grad():\n        outputs = model(b_input_ids,token_type_ids = None, attention_mask = b_input_mask)\n        logits = outputs[0]\n        \n        logits = logits.detach().cpu().numpy()\n        label_ids = b_labels.to('cpu').numpy()\n        \n        pred.append(logits)\n        true.append(label_ids)\n        temp_eval_acc = flat_accuracy(logits,label_ids)\n        eval_acc += temp_eval_acc\n        nb_eval_steps += 1\n        \nprint(\"  Accuracy: {0:.2f}\".format(eval_acc\/nb_eval_steps))","dbe86024":"test_data = test_data.text.values\n\nindices=tokenizer.batch_encode_plus(test_data,max_length=38,add_special_tokens=True, return_attention_mask=True,pad_to_max_length=True,truncation=True)","63fde2c4":"input_ids = indices[\"input_ids\"]\natt_mask = indices[\"attention_mask\"]","0afbcbfe":"test_ids = torch.tensor(input_ids)\ntest_mask = torch.tensor(att_mask)","7bfdc20b":"batch_size = 32\n\n\nprediction_data = TensorDataset(test_ids, test_mask)\nprediction_sampler = SequentialSampler(prediction_data)\nprediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\n","d31143a4":"len(test_ids)","584e49f1":"model.eval()\n\npredictions = []\n\nfor batch in prediction_dataloader:\n    batch = tuple(t.to(device) for t in batch)\n    b_input_ids, b_input_mask = batch\n    \n    with torch.no_grad():\n        outputs = model(b_input_ids,token_type_ids = None, attention_mask = b_input_mask)\n        logits = outputs[0]\n        \n        logits = logits.detach().cpu().numpy()\n        \n        predictions.append(logits)\n        \n        \nflat_predictions = [item for sublist in predictions for item in sublist]\nflat_predictions = np.argmax(flat_predictions, axis=1).flatten()","182e9a88":"sample_sub=pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')\nsubmit=pd.DataFrame({'id':sample_sub['id'].values.tolist(),'target':flat_predictions})\nsubmit.to_csv('submission.csv',index=False)","7ac04167":"submit.head()","47c81566":"Create DATALOADER for training and testing set","677a0f15":"Testing","d53c1880":"ELECTRA ","0fafb317":"Validation","381fe722":"convert data to tensors","120b177b":"From the graph we can conclude that the max number of tweets have less than 30 tokens. so let us take the max_len as 36."}}