{"cell_type":{"67e6b9a6":"code","ede98f00":"code","b504676a":"code","ffccf694":"code","f4ce48a8":"code","fc710853":"code","39cc286a":"code","81d135d2":"code","20696887":"code","a67fd9f8":"code","fd78ad1b":"code","6bab208f":"code","408a0f41":"code","36d4a1b1":"code","09571c18":"code","3eb1c7a2":"code","e90e8287":"code","6c7556fc":"code","5708e4a8":"code","5cef77f1":"code","07e4c7af":"markdown","ea8405e6":"markdown","10032ed9":"markdown","fc006837":"markdown","9b6b5661":"markdown","5e010579":"markdown","6a54b35b":"markdown","e90a9c14":"markdown","0771cf21":"markdown"},"source":{"67e6b9a6":"%load_ext autoreload\n%autoreload 2","ede98f00":"from IPython.core.interactiveshell import InteractiveShell\nfrom IPython.display import HTML\n\nInteractiveShell.ast_node_interactivity = 'all'\n\nimport warnings\nwarnings.filterwarnings('ignore', category = RuntimeWarning)\nwarnings.filterwarnings('ignore', category = UserWarning)\n\nimport pandas as pd\nimport numpy as np\n#from utils import get_data, generate_output, guess_human, seed_sequence, get_embeddings, find_closest\n\nfrom keras.models import load_model\nfrom keras.models import Sequential, load_model\nfrom keras.layers import LSTM, Dense, Dropout, Embedding, Masking\nfrom keras.optimizers import Adam\nfrom keras.utils import Sequence\nfrom keras.preprocessing.text import Tokenizer\n\nfrom sklearn.utils import shuffle\n\nfrom IPython.display import HTML\n\nfrom itertools import chain\nfrom keras.utils import plot_model\nimport numpy as np\nimport pandas as pd\nimport random\nimport json\nimport re\nimport csv\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","b504676a":"#data = pd.read_csv('..\/input\/the-first-claim-of-patents\/semiconductor_patents_claim1.csv')\n#data.head()","ffccf694":"def format_sequence(s):\n    \"\"\"Add spaces around punctuation.\"\"\"\n    \n    # Add spaces around punctuation\n    s =  re.sub(r'(?<=[^\\s0-9])(?=[.,;?])', r' ', s)\n    \n    # Remove references to figures\n    s = re.sub(r'\\((\\d+)\\)', r'', s)\n    \n    # Remove double spaces\n    s = re.sub(r'\\s\\s', ' ', s)\n    return s\n\ndef create_train_valid(features,\n                       labels,\n                       num_words,\n                       train_fraction=0.7):\n    \"\"\"Create training and validation features and labels.\"\"\"\n    \n    # Randomly shuffle features and labels\n    features, labels = shuffle(features, labels)\n\n    # Decide on number of samples for training\n    train_end = int(train_fraction * len(labels))\n\n    train_features = np.array(features[:train_end])\n    valid_features = np.array(features[train_end:])\n\n    train_labels = labels[:train_end]\n    valid_labels = labels[train_end:]\n\n    # Convert to arrays\n    X_train, X_valid = np.array(train_features), np.array(valid_features)\n\n    # Using int8 for memory savings\n    y_train = np.zeros((len(train_labels), num_words), dtype=np.int8)\n    y_valid = np.zeros((len(valid_labels), num_words), dtype=np.int8)\n\n    # One hot encoding of labels\n    for example_index, word_index in enumerate(train_labels):\n        y_train[example_index, word_index] = 1\n\n    for example_index, word_index in enumerate(valid_labels):\n        y_valid[example_index, word_index] = 1\n\n    # Memory management\n    import gc\n    gc.enable()\n    del features, labels, train_features, valid_features, train_labels, valid_labels\n    gc.collect()\n\n    return X_train, X_valid, y_train, y_valid\ndef cleanser(txts):\n    ##input 'txts': a list of text\n    cleansed1 = [' '.join(i.split(' ')[1:-1]) if i[-3:]=='...' else ' '.join(i.split(' ')[1:]) for i in txts]\n    \n    ### Add spaces to the two sides of punctuations\n    cleansed2 = [re.sub(r'(?<=[^\\s0-9])(?=[.,;:?])', r' ', i) for i in cleansed1]\n    #print(cleansed2)\n    cleansed3 = [re.sub(r'(?<=[.,;:?])(?=[^\\s0-9])', r' ', i) for i in cleansed2]\n    #print(cleansed3)\n    return cleansed3\n\ndef make_sequences(texts,\n                   training_length=50,\n                   lower=True,\n                   filters='!\"#$%&()*+,-.\/:;<=>?@[\\\\]^_`{|}~\\t\\n'):\n    \"\"\"Turn a set of texts into sequences of integers\"\"\"\n    # Data cleaning before tokenizaion\n    texts = cleanser(texts)\n    \n    # Create the tokenizer object and train on texts\n    tokenizer = Tokenizer(lower=lower, filters=filters)\n    tokenizer.fit_on_texts(texts)\n\n    # Create look-up dictionaries and reverse look-ups\n    word_idx = tokenizer.word_index\n    idx_word = tokenizer.index_word\n    num_words = len(word_idx) + 1\n    word_counts = tokenizer.word_counts\n\n    print(f'There are {num_words} unique words.')\n\n    # Convert text to sequences of integers\n    sequences = tokenizer.texts_to_sequences(texts)\n\n    # Limit to sequences with more than training length tokens\n    seq_lengths = [len(x) for x in sequences]\n    over_idx = [\n        i for i, l in enumerate(seq_lengths) if l > (training_length + 20)\n    ]\n\n    new_texts = []\n    new_sequences = []\n\n    # Only keep sequences with more than training length tokens\n    for i in over_idx:\n        new_texts.append(texts[i])\n        new_sequences.append(sequences[i])\n\n    training_seq = []\n    labels = []\n\n    # Iterate through the sequences of tokens\n    for seq in new_sequences:\n\n        # Create multiple training examples from each sequence\n        for i in range(training_length, len(seq)):\n            # Extract the features and label\n            extract = seq[i - training_length:i + 1]\n\n            # Set the features and label\n            training_seq.append(extract[:-1])\n            labels.append(extract[-1])\n\n    print(f'There are {len(training_seq)} training sequences.')\n\n    # Return everything needed for setting up the model\n    return word_idx, idx_word, num_words, word_counts, new_texts, new_sequences, training_seq, labels\n\ndef get_data2(file, filters='\"#$%&*+\/:<=>?@[\\\\]^_`{|}~\\t\\n', training_len=50,\n             lower=False):\n    \"\"\"Retrieve formatted training and validation data from a file\"\"\"\n    \n    data = pd.read_csv(file, parse_dates=['Application No.']).dropna(subset = ['First Claim'])\n    #claims = [format_sequence(a) for a in list(data['First Claim'])]\n    claims = [a for a in list(data['First Claim'])]\n    word_idx, idx_word, num_words, word_counts, texts, sequences, features, labels = make_sequences(\n        claims, training_len, lower, filters)\n    X_train, X_valid, y_train, y_valid = create_train_valid(features, labels, num_words)\n    training_dict = {'X_train': X_train, 'X_valid': X_valid, \n                     'y_train': y_train, 'y_valid': y_valid}\n    return training_dict, word_idx, idx_word, sequences\n\n\nfilters='\"#$%&*+\/<=>?@[\\\\]^_`{|}~\\t\\n' # ,;.: are recoginized valid tokens\ntraining_len=50\nlower=False\n\n### read dataset \ndata = pd.read_csv('..\/input\/semiconductor_patents_claim1_cleansed2.csv')\n#data = pd.read_csv('..\/input\/patent-abstract\/neural_network_patent_query.csv')\ntraining_dict, word_idx, idx_word, sequences = get_data2('..\/input\/semiconductor_patents_claim1_cleansed2.csv', filters=filters, training_len = 50)\n","f4ce48a8":"data.head()\nclms = [i for i in data['First Claim']]\nword_idx, idx_word, num_words, word_counts, texts, sequences, features, labels = make_sequences(\n        clms, training_len, lower, filters)","fc710853":"from keras.models import Sequential, load_model\nfrom keras.layers import LSTM, Dense, Dropout, Embedding, Masking, Bidirectional\nfrom keras.optimizers import Adam\n\nfrom keras.utils import plot_model","39cc286a":"def rnn_construction(model, word_idx):\n    # Embedding layer\n    model.add(\n            Embedding(\n                input_dim=len(word_idx) + 1,\n                output_dim=100,\n                weights=None,\n                trainable=True))\n\n    # Recurrent layer\n    model.add(\n        LSTM(\n            64, return_sequences=False, dropout=0.1,\n            recurrent_dropout=0.1))\n\n    # Fully connected layer\n    model.add(Dense(128, activation='relu'))\n\n    # Dropout for regularization\n    model.add(Dropout(0.2))\n\n    # Output layer\n    model.add(Dense(len(word_idx) + 1, activation='softmax'))\n\n    # Compile the model\n    model.compile(\n        optimizer='adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n\n    model.summary()\n    \n    return model\n\nmodel = Sequential()\nmodel = rnn_construction(model,word_idx)","81d135d2":"h = model.fit(training_dict['X_train'], training_dict['y_train'], epochs = 50, batch_size = 2048, \n          validation_data = (training_dict['X_valid'], training_dict['y_valid']), \n          verbose = 1)","20696887":"# Check the model performance\nprint('Model Performance: Log Loss and Accuracy on training data')\nmodel.evaluate(training_dict['X_train'], training_dict['y_train'], batch_size = 2048)\n\nprint('\\nModel Performance: Log Loss and Accuracy on validation data')\nmodel.evaluate(training_dict['X_valid'], training_dict['y_valid'], batch_size = 2048)","a67fd9f8":"# Save the model\nmodel.save('model_rnn.h5')","fd78ad1b":"### source: https:\/\/github.com\/WillKoehrsen\/recurrent-neural-networks\/blob\/master\/notebooks\/utils.py\nfrom keras.models import load_model\nfrom keras.models import Sequential, load_model\nfrom keras.layers import LSTM, Dense, Dropout, Embedding, Masking, Flatten\nfrom keras.optimizers import Adam\nfrom keras.utils import Sequence\nfrom keras.preprocessing.text import Tokenizer\n\nfrom sklearn.utils import shuffle\n\nfrom IPython.display import HTML\n\nfrom itertools import chain\nfrom keras.utils import plot_model\nimport numpy as np\nimport pandas as pd\nimport random\nimport json\nimport re\n\ndef remove_spaces(s):\n    \"\"\"Remove spaces around punctuation\"\"\"\n    s = re.sub(r'\\s+([.,;:?])', r'\\1', s)\n    return s\n\ndef generate_output(model,\n                    sequences,\n                    idx_word,\n                    seed_length=50,\n                    new_words_min=50,\n                    new_words_max = 1000,\n                    diversity=1,\n                    n_gen=1):\n    \"\"\"Generate `new_words` words of output from a trained model and format into HTML.\"\"\"\n\n    # Choose a random sequence\n    seq = random.choice(sequences)\n    #print([idx_word[i] for i in seq])\n    \n    # Choose a random starting point\n    seed_idx = random.randint(0, len(seq) - seed_length - 10)\n    seed_idx = 0\n    \n    # Ending index for seed\n    end_idx = seed_idx + seed_length\n    \n    dot_idx = word_idx['.']\n    \n    gen_list = []\n\n    # Extract the seed sequence\n    seed = seq[seed_idx:end_idx]\n    #print(' '.join([idx_word[i] for i in seed]))\n    \n    generated = list()\n\n    next_idx = -1\n    window = seed.copy()\n    # Keep adding new words\n    for i in range(new_words_max):\n        # Check the termination condition:\n        if next_idx == dot_idx and i >= new_words_min:\n            break\n\n        # Make a prediction from the seed\n        preds = model.predict(np.array(window).reshape(1, -1))[0].astype(\n            np.float64)\n\n        # Diversify\n        preds = np.log(preds) \/ diversity\n        exp_preds = np.exp(preds)\n\n        # Softmax\n        preds = exp_preds \/ sum(exp_preds)\n\n        # Choose the next word\n        probas = np.random.multinomial(1, preds, 1)[0]\n\n        next_idx = np.argmax(probas)\n\n        # New seed adds on old word\n        window = window[1:] + [next_idx]\n        #seed += [next_idx]\n        #print(next_idx)\n        generated.append(next_idx)\n            \n    # Find the actual entire sequence\n    #print(i, len(generated))\n    actual_seq = seq[end_idx:end_idx + len(generated)]\n\n    # Decode sequences into words\n    a_text = [idx_word[j] for j in actual_seq]\n    gen_text = [idx_word[j] for j in generated]\n    #print(len(actual_seq), len(generated))\n    \n    original_txt = remove_spaces(' '.join([idx_word[i] for i in seed]))\n    #gen_txt = remove_spaces(' '.join(gen_list[0]))\n    gen_txt = '< --- >' + remove_spaces(' '.join(gen_text))\n    actual_txt = '< --- >' + remove_spaces(' '.join(a_text))\n    return [original_txt, gen_txt, actual_txt]\n    #return original_sequence, gen_list, a\n\n","6bab208f":"original, gen, actual = generate_output(model, sequences, idx_word, seed_length = 50, new_words_min = 30, new_words_max = 200,diversity = 1.50)\nprint('Original seed text:')\nprint(original)\nprint('RNN generated text:')\nprint(gen)\nprint('Actual text:')\nprint(actual)","408a0f41":"def mlp_construction(model, word_idx, training_len):\n    # Embedding layer\n    model.add(\n            Embedding(\n                input_dim=len(word_idx) + 1,\n                output_dim=100,\n                weights=None,\n                trainable=True,\n                input_length = training_len))\n    \n    model.add(Flatten())\n    \n    # Fully connected layer\n    model.add(Dense(128, activation='relu'))\n\n    # Dropout for regularization\n    model.add(Dropout(0.5))\n\n    # Output layer\n    model.add(Dense(len(word_idx) + 1, activation='softmax'))\n\n    # Compile the model\n    model.compile(\n        optimizer='adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n\n    model.summary()\n    \n    return model\n\nlen(word_idx)\nmodel_mlp = Sequential()\nmodel_mlp = mlp_construction(model_mlp, word_idx, training_len)\n\n### Model fitting\nimport time\nstart_time = time.time()\nmodel_mlp.fit(training_dict['X_train'], training_dict['y_train'], epochs = 50, batch_size = 2048, \n          validation_data = (training_dict['X_valid'], training_dict['y_valid']), \n          verbose = 1)\nprint('training time = ', (time.time()-start_time)\/60.,'mins')\n\n### Check the model performance\nprint('Model Performance: Log Loss and Accuracy on training data')\nmodel_mlp.evaluate(training_dict['X_train'], training_dict['y_train'], batch_size = 2048)\n\nprint('\\nModel Performance: Log Loss and Accuracy on validation data')\nmodel_mlp.evaluate(training_dict['X_valid'], training_dict['y_valid'], batch_size = 2048)\n","36d4a1b1":"# Save the model\nmodel_mlp.save('model_mlp.h5')","09571c18":"### Check the text generated by RNN and MLP\noriginal, gen, actual = generate_output(model, sequences, idx_word, seed_length = 50, new_words_min = 30, new_words_max = 200,diversity = 1.50)\nprint('Original seed text:')\nprint(original)\nprint('RNN generated text:')\nprint(gen)\nprint('Actual text:')\nprint(actual)","3eb1c7a2":"original, gen, actual = generate_output(model_mlp, sequences, idx_word, seed_length = 50, new_words_min = 30, new_words_max = 200,diversity = 1.50)\nprint('Original seed text:')\nprint(original)\nprint('RNN generated text:')\nprint(gen)\nprint('Actual text:')\nprint(actual)","e90e8287":"def bi_construction(model, word_idx,lstm_cells=64):\n    # Embedding layer\n    model.add(\n            Embedding(\n                input_dim=len(word_idx) + 1,\n                output_dim=100,\n                weights=None,\n                trainable=True))\n    '''\n    # Recurrent layer\n    model.add(\n        LSTM(\n            lstm_cells, return_sequences=False, dropout=0.1,\n            recurrent_dropout=0.1))\n    '''\n    # Bi-directional LSTM layer\n    model.add(\n    Bidirectional(\n        LSTM(\n            lstm_cells,\n            return_sequences=False,\n            dropout=0.1,\n            recurrent_dropout=0.1)))\n    \n    # Fully connected layer\n    model.add(Dense(128, activation='relu'))\n\n    # Dropout for regularization\n    model.add(Dropout(0.2))\n\n    # Output layer\n    model.add(Dense(len(word_idx) + 1, activation='softmax'))\n\n    # Compile the model\n    model.compile(\n        optimizer='adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n\n    model.summary()\n    \n    return model\n\nmodel_bi = Sequential()\nmodel_bi = bi_construction(model_bi, word_idx)","6c7556fc":"### Model fitting\nimport time\nstart_time = time.time()\nmodel_bi.fit(training_dict['X_train'], training_dict['y_train'], epochs = 50, batch_size = 2048, \n          validation_data = (training_dict['X_valid'], training_dict['y_valid']), \n          verbose = 1)\nprint('training time = ', (time.time()-start_time)\/60.,'mins')\n\n### Check the model performance\nprint('Model Performance: Log Loss and Accuracy on training data')\nmodel_bi.evaluate(training_dict['X_train'], training_dict['y_train'], batch_size = 2048)\n\nprint('\\nModel Performance: Log Loss and Accuracy on validation data')\nmodel_bi.evaluate(training_dict['X_valid'], training_dict['y_valid'], batch_size = 2048)\n\n# Save the model\nmodel_bi.save('model_bi.h5')","5708e4a8":"def generate_output(model,\n                    sequences,\n                    idx_word,\n                    seed_length=50,\n                    new_words=50,\n                    diversity=1,\n                    return_output=False,\n                    n_gen=1):\n    \"\"\"Generate `new_words` words of output from a trained model and format into HTML.\"\"\"\n\n    # Choose a random sequence\n    seq = random.choice(sequences)\n\n    # Choose a random starting point\n    seed_idx = random.randint(0, len(seq) - seed_length - 10)\n    # Ending index for seed\n    end_idx = seed_idx + seed_length\n\n    gen_list = []\n\n    for n in range(n_gen):\n        # Extract the seed sequence\n        seed = seq[seed_idx:end_idx]\n        original_sequence = [idx_word[i] for i in seed]\n        generated = seed[:] + ['#']\n\n        # Find the actual entire sequence\n        actual = generated[:] + seq[end_idx:end_idx + new_words]\n\n        # Keep adding new words\n        for i in range(new_words):\n\n            # Make a prediction from the seed\n            preds = model.predict(np.array(seed).reshape(1, -1))[0].astype(\n                np.float64)\n\n            # Diversify\n            preds = np.log(preds) \/ diversity\n            exp_preds = np.exp(preds)\n\n            # Softmax\n            preds = exp_preds \/ sum(exp_preds)\n\n            # Choose the next word\n            probas = np.random.multinomial(1, preds, 1)[0]\n\n            next_idx = np.argmax(probas)\n\n            # New seed adds on old word\n            seed = seed[1:] + [next_idx]\n            #seed += [next_idx]\n            # print(len(seed))\n            generated.append(next_idx)\n\n        # Showing generated and actual abstract\n        n = []\n\n        for i in generated:\n            n.append(idx_word.get(i, '< --- >'))\n\n        gen_list.append(n)\n\n    a = []\n\n    for i in actual:\n        a.append(idx_word.get(i, '< --- >'))\n\n    a = a[seed_length:]\n\n    gen_list = [gen[seed_length:seed_length + len(a)] for gen in gen_list]\n\n    if return_output:\n        return original_sequence, gen_list, a\n\n    # HTML formatting\n    seed_html = ''\n    seed_html = addContent(seed_html, header(\n        'Seed Sequence', color='darkblue'))\n    seed_html = addContent(seed_html,\n                           box(remove_spaces(' '.join(original_sequence))))\n\n    gen_html = ''\n    gen_html = addContent(gen_html, header('RNN Generated', color='darkred'))\n    gen_html = addContent(gen_html, box(remove_spaces(' '.join(gen_list[0]))))\n\n    a_html = ''\n    a_html = addContent(a_html, header('Actual', color='darkgreen'))\n    a_html = addContent(a_html, box(remove_spaces(' '.join(a))))\n\n    return seed_html, gen_html, a_html\n\ndef guess_human(model, sequences, idx_word, seed_length=50):\n    \"\"\"Produce 2 RNN sequences and play game to compare to actaul.\n       Diversity is randomly set between 0.5 and 1.25\"\"\"\n    \n    new_words = np.random.randint(10, 50)\n    diversity = np.random.uniform(0.5, 1.25)\n    sequence, gen_list, actual = generate_output(model, sequences, idx_word, seed_length, new_words,\n                                                 diversity=diversity, return_output=True, n_gen = 2)\n    gen_0, gen_1 = gen_list\n    \n    output = {'sequence': remove_spaces(' '.join(sequence)),\n              'computer0': remove_spaces(' '.join(gen_0)),\n              'computer1': remove_spaces(' '.join(gen_1)),\n              'human': remove_spaces(' '.join(actual))}\n    \n    print(f\"Seed Sequence: {output['sequence']}\\n\")\n    \n    choices = ['human', 'computer0', 'computer1']\n          \n    selected = []\n    i = 0\n    while len(selected) < 3:\n        choice = random.choice(choices)\n        selected.append(choice)\n        print(f'\\nOption {i + 1} {output[choice]}')\n        choices.remove(selected[-1])\n        i += 1\n    \n    print('\\n')\n    guess = int(input('Enter option you think is human (1-3): ')) - 1\n    print('\\n')\n    \n    if guess == np.where(np.array(selected) == 'human')[0][0]:\n        print('*' * 3 + 'Correct' + '*' * 3 + '\\n')\n        print('-' * 60)\n        print('Ordering: ', selected)\n    else:\n        print('*' * 3 + 'Incorrect' + '*' * 3 + '\\n')\n        print('-' * 60)\n        print('Correct Ordering: ', selected)\n          \n    print('Diversity', round(diversity, 2))","5cef77f1":"guess_human(model, sequences, idx_word)","07e4c7af":"# Comparison w\/ recurrence-absent neural network(multi-layer perceptron, MLP)\n* Comparison w\/ the baseline model w\/o recurrent property, normal multi-layer percetron, by validation accuracy and generated text.\n\n","ea8405e6":"# Conclusions\n    Recurrent property really plays an important part in text generation. Moreover, the bi-directionality can further improve the accuracy a little.\n| Type of neural network | Test Accuracy |\n| --- | --- | --- |\n| MLP | 0.326 |\n| RNN | 0.365 |\n| Bidirectional-RNN | 0.370 |\n\n# Future work\ntry GRU next time.","10032ed9":"# Generate Output until dot appears\n* Once the model is trained, text generator will keep generating text until dot appears.\n","fc006837":"# Defining Model Architecture\n","9b6b5661":"# Train the model\n\n","5e010579":"## Try bidirectional LSTM\n","6a54b35b":"## Human-or-machine game\n","e90a9c14":"# Patent Claim1 Generator - Model Comparison\n\nThis notebook is the refinement on [William Koehrsen's work](https:\/\/github.com\/WillKoehrsen\/recurrent-neural-networks\/blob\/master\/notebooks\/Quick%20Start%20to%20Recurrent%20Neural%20Networks.ipynb). The refinement has three points:\n* Once the model is trained, text generator will keep generating text until dot appears.\n* Comparison w\/ the baseline model w\/o recurrent property, normal multi-layer percetron, and bi-directional RNN by validation accuracy and generated text.\n","0771cf21":"# Load the data\n"}}