{"cell_type":{"a90f3ef7":"code","b0a42666":"code","c02a5e22":"code","d1f8dc4a":"code","c7d96ec8":"code","1af88ce9":"code","3b36e2ca":"code","68220093":"code","c2041e0f":"code","06baf4f2":"code","9c3a4ce7":"code","57faa7a3":"code","f30c87ed":"code","8312157e":"code","24908c77":"code","0e5d62be":"code","f3d4a0d2":"code","cea977e7":"code","c26e9297":"code","8fca7155":"code","b6794fd0":"code","44304b7d":"code","4acfae5b":"code","a93de0bb":"code","c875154a":"code","8685d220":"code","4d6642c3":"code","71de1a41":"code","e0297410":"code","763f5979":"markdown","553d09ff":"markdown","27720e5f":"markdown","e31f0937":"markdown","506de1aa":"markdown","640a22c7":"markdown","13d94c25":"markdown"},"source":{"a90f3ef7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b0a42666":"# Load required libraries\n\nimport missingno\n\n#Data Visualization \nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom wordcloud import WordCloud\n\n# sklearn\nfrom sklearn.base import TransformerMixin\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.metrics import accuracy_score, recall_score, plot_confusion_matrix\nfrom sklearn.pipeline import Pipeline\n\n#Preprocessing\nimport nltk\nfrom nltk.corpus import gutenberg, stopwords\nfrom nltk.collocations import *\nfrom nltk import word_tokenize\nimport string\nimport re\n\n#Spacy\nimport spacy\nfrom spacy.lang.en.stop_words import STOP_WORDS\nfrom spacy.lang.en import English\n\n#Data Modeling\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\n#Data Evaluation \n\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.pipeline import Pipeline as imbpipe\nfrom imblearn.over_sampling import RandomOverSampler, SMOTE\nfrom sklearn.model_selection import train_test_split, \\\n                                    cross_validate , \\\n                                    GridSearchCV, \\\n                                    cross_val_predict\n\n# Ignore any warnings\nimport warnings;\nwarnings.filterwarnings('ignore')\n\n# Set random_state\nseed = 42","c02a5e22":"df=pd.read_csv('\/kaggle\/input\/real-or-fake-fake-jobposting-prediction\/fake_job_postings.csv', index_col=0)\ndf.head()","d1f8dc4a":"# Taken from: https:\/\/www.dataquest.io\/blog\/tutorial-text-classification-in-python-using-spacy\/ \n\n# Custom transformer \nclass predictors(TransformerMixin):\n    def transform(self, X, **transform_params):\n        # Cleaning Text\n        return [clean_text(text) for text in X]\n        \n    def fit(self, X, y=None, **fit_params):\n        return self\n    \n    def get_params(self, deep=True):\n        return {}\n\n# Basic function to clean the text\ndef clean_text(text):\n    \n    # Removing spaces and converting text into lowercase\n    return text.strip().lower()\n\n# Create our list of punctuation marks\npunctuations = string.punctuation\n\n#remove weird numbers\ndef remove_number(text):\n    numbers_to_remove = ['1', '2', '3', '4','5', '6', '7', '8', '9', '0', \"\\xa0\" ]\n    clean_text = text\n    for num in numbers_to_remove:\n        clean_text = clean_text.replace(num, ' ')\n    \n    return clean_text\n\n\ndef remove_punctuations(text):\n    for punctuation in punctuations:\n        text = text.replace(punctuation, ' ')\n    return text\n\n#removes weird symbols\ndef remove_symbols(text):\n    symbols_to_remove = [\"\\xa0\", \" \\n \", '\\n', \" <div> \", \"<li>\", '<br>', '<\/ul>' , '<b>',\" d \",\" y \",\" q \",\"<\/li>\", '<ul>','<\/b>', '<\/div>','<div', \" p \", ' jz ', ' iu ','class=' ,'jz'\\\n                         'jobDescriptionText' , 'jobdescriptiontext', 'class','jobsearch']\n                       \n    clean_text = text\n    for sym in symbols_to_remove:\n        clean_text = clean_text.replace(sym, ' ')\n    \n    return clean_text\n\n","c7d96ec8":"# checking missing data in our dataframe.\nmissingno.matrix(df,color=(0.1, 0.6, 0.7))","1af88ce9":"df.fraudulent.value_counts()","3b36e2ca":"fig, axes = plt.subplots(ncols=2, figsize=(17, 5), dpi=100)\nplt.tight_layout()\n\ndf[\"fraudulent\"].value_counts().plot(kind='pie', ax=axes[0], labels=['Real Post', 'Fraudulent Post'])\ntemp = df[\"fraudulent\"].value_counts()\nsns.barplot(temp.index, temp, ax=axes[1])\n\naxes[0].set_ylabel(' ')\naxes[1].set_ylabel(' ')\naxes[1].set_xticklabels([\"Real Post\", \"Fraudulent Post\"])\n\naxes[0].set_title('Target Distribution in Dataset', fontsize=15)\naxes[1].set_title('Target Count in Dataset', fontsize=15)\n\nplt.show()","68220093":"# new cols to check nulls\ndf['has_benefit'] = pd.notnull(df[\"benefits\"])\ndf['has_requirements'] = pd.notnull(df[\"requirements\"])\ndf.head(1)","c2041e0f":"# Location\ndf['location'] = df['location'].fillna(' ')\ndf['location'] = df['location'].apply(lambda x: x[3:6] )\ndf['location'] = df['location'].replace(' , ','unknown' )","06baf4f2":"# Fill nulls with 'no benefit listed'\ndf['benefits'] = df['benefits'].fillna('no benefit listed')\n\n# Fill nulls with 'no requirement listed'\ndf['requirements'] = df['requirements'].fillna('norequirementlisted')\n\n# Fill nulls with 'no description listed'\ndf['description'] = df['description'].fillna('unknown')\n\n# Fill nulls with 'no required_experience listed'\ndf['required_experience'] = df['required_experience'].fillna('unknown')\n\n# Fill nulls with 'no company_profile listed'\ndf['company_profile' ]= df['company_profile'].fillna('unknown')\n\n# Fill nulls with 'no companylogo'\ndf.has_company_logo.replace({0:\"nocompanylogo\",1:'hascompanylogo'},inplace=True)","9c3a4ce7":"# merging all relevant cols to one `text`\ndf['text'] = '  '+ df['title'] +'  '+ df['description'] +'  '+ df['requirements']+'  '+ \\\n                   df['benefits']+'  '+ df['required_experience'] + '  ' + df['has_company_logo' ] + '  ' + \\\n                   df['company_profile' ] + '  '\ndf.head(1)","57faa7a3":"# types of words are frequent in fraudulent and not fraudulent\n\nfraud_jobs_text = df[df.fraudulent==1].text\nactual_jobs_text = df[df.fraudulent==0].text","f30c87ed":"STOPWORDS = spacy.lang.en.stop_words.STOP_WORDS\nplt.figure(figsize = (16,14))\nwc = WordCloud(min_font_size = 3,  \n               max_words = 100, \n               width = 1600, \n               height = 800,\n               background_color='white',\n               stopwords = STOPWORDS).generate(str(\" \".join(fraud_jobs_text)))\nplt.imshow(wc,interpolation = 'bilinear')\nplt.axis(\"off\");","8312157e":"STOPWORDS = spacy.lang.en.stop_words.STOP_WORDS\nplt.figure(figsize = (16,14))\nwc = WordCloud(min_font_size = 3,  \n               max_words = 100, \n               width = 1600, \n               height = 800, \n               background_color='white',\n               stopwords = STOPWORDS).generate(str(\" \".join(actual_jobs_text)))\nplt.imshow(wc,interpolation = 'bilinear')\nplt.axis(\"off\");","24908c77":"# only need target and text\ndata = df[[\"fraudulent\", \"text\"]]\ndata","0e5d62be":"# Create our list of punctuation marks\npunctuations = string.punctuation\n\n# Create our list of stopwords\nnlp = spacy.load(\"en_core_web_sm\")\nstop_words = spacy.lang.en.stop_words.STOP_WORDS\n\n# Load English tokenizer, tagger, parser, NER and word vectors\nparser = English()","f3d4a0d2":"# Apply to the DF series\ndata.text = data.text.apply(remove_punctuations)\n\n#remove weird numbers\ndata.text = data.text.apply(remove_number)\ndata.text","cea977e7":"# All lowercase\ndata.text = data.text.str.lower()\n# Adding State last to keep it Caps\ndata.text = data['text'] +'  '+ df['location']+'  '","c26e9297":"# creating our bag of words\nbow_vector = CountVectorizer(tokenizer = word_tokenize, ngram_range=(1,3),lowercase=True)\nbow_vector","8fca7155":"# splitting our data in train and test\nX_train, X_test, y_train, y_test = train_test_split(data.text, data.fraudulent, \n                                                    test_size=0.25, random_state=seed)","b6794fd0":"clf = RandomForestClassifier(random_state=seed)\n\n# Create pipeline using Bag of Words\npipe = Pipeline([(\"cleaner\", predictors()),\n                 ('vectorizer', bow_vector),\n                 ('classifier', clf)])\n\n# fitting our model\npipe.fit(X_train,y_train)","44304b7d":"# Predicting with a test dataset\npredicted = pipe.predict(X_test)\n\n# Model Accuracy\nprint(\"Random Forest Accuracy:\", accuracy_score(y_test, predicted))\nprint(\"Random Forest Recall:\", recall_score(y_test, predicted))","4acfae5b":"plot_confusion_matrix(pipe, \n                      X_test, y_test, \n                      values_format=' ')","a93de0bb":"imb_log_model = imbpipe(steps=[\n                         ('vectorizer', CountVectorizer(stop_words=stop_words)),\n                         ('over',RandomOverSampler(sampling_strategy='minority')),\n                         ('RFC',RandomForestClassifier(random_state=seed))])\n# fitting the model\nimb_log_model.fit(X_train,y_train)","c875154a":"# Predicting with a test\npredicted = imb_log_model.predict(X_test)\n\n# Model Accuracy\nprint(\"Accuracy:\", accuracy_score(y_test, predicted))\nprint(\"Recall:\", recall_score(y_test, predicted))","8685d220":"plot_confusion_matrix(imb_log_model, \n                      X_test, y_test, \n                      cmap='Blues', \n                      values_format=' ')","4d6642c3":"under_log_model = imbpipe(steps=[\n                         ('tfid',CountVectorizer(stop_words=stop_words)),\n                         ('under',RandomUnderSampler(sampling_strategy='majority')),\n                         ('RFC',RandomForestClassifier(random_state=seed))])\n\n# fitting the model\nunder_log_model.fit(X_train,y_train)","71de1a41":"# Predicting with a test\npredicted = under_log_model.predict(X_test)\n\n# Model Accuracy\nprint(\"Accuracy:\", accuracy_score(y_test, predicted))\nprint(\"Recall:\", recall_score(y_test, predicted))","e0297410":"plot_confusion_matrix(under_log_model, \n                      X_test, y_test, \n                      cmap='Blues', \n                      values_format=' ')","763f5979":"Functions :","553d09ff":"### Dealing with Imbalance Oversampling\n- CountVectorizer and over","27720e5f":"Taken from: https:\/\/www.dataquest.io\/blog\/tutorial-text-classification-in-python-using-spacy\/","e31f0937":"## Machine Learning -> Modelling","506de1aa":"## First Baseline Model Random Forest","640a22c7":"## Undersampling","13d94c25":"- Dataset contains lots of missing values which need to be taken care of.\n\n### Target\n- Fraudulent"}}