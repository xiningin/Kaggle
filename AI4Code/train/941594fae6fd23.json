{"cell_type":{"54d0df6b":"code","5d943894":"code","a5c0e10e":"code","17c53d44":"code","5a8b43e1":"code","695d877e":"code","510a6107":"code","91e7ef4b":"code","266112ac":"code","d522b207":"code","9c75ca53":"code","24a5c140":"code","5f90f005":"code","1ee01470":"code","1b37ebbe":"code","aae5114c":"code","1f3d5e8b":"code","1f2dc8e2":"code","2bdff036":"code","a532f50d":"code","fa84e8e2":"code","3640c75d":"code","825ed35c":"code","04ac0829":"code","7d488529":"code","85483f74":"code","15e0fe5d":"code","3b429db5":"code","ad448c71":"code","6839d58a":"code","a62a8960":"code","89341965":"code","20efb774":"code","b33e11f9":"code","a78672cd":"code","56f08e15":"code","ffd42813":"code","c88703d0":"code","035b6d6f":"code","aabd16e3":"code","0a7dcae8":"code","08290e98":"code","3fc0a6a6":"code","98194f19":"code","a3a0824a":"code","ba5449e5":"code","b1d7cc7b":"code","2693c3e3":"code","bfd5a92f":"code","e5b6e51c":"code","d7d873dd":"code","f70f56e5":"code","53568045":"code","dc2db6a5":"code","16e1849b":"code","591f132b":"code","2fe13b2b":"code","451c7642":"code","0daea4af":"code","10c941d8":"code","e35f3005":"code","8bd3ec35":"code","69e621dc":"code","e934b094":"code","aee80fbf":"code","478a0537":"code","1b25ee35":"code","c9dbeab2":"markdown"},"source":{"54d0df6b":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.simplefilter(\"ignore\")","5d943894":"# directly reading the dataset without downloading by using url\n\ndf=pd.read_csv('https:\/\/storage.googleapis.com\/kagglesdsdata\/datasets\/596958\/1073629\/Placement_Data_Full_Class.csv?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20211225%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20211225T071003Z&X-Goog-Expires=259199&X-Goog-SignedHeaders=host&X-Goog-Signature=672442df52ad9f5263d341c7a9753e1d55f1b7ef6ad958d84401149f6e760341b8de0fabf965cadb5e8b8f4c05181060b7eb997b8989317b88d4ed720b2c8ab61981b2a116bdde023156a40125e32d2f49e69ffd4a961eabe39b3b707da2762c6dc2855cdddd6bd6446c14eda41bb0d5f9493d794da3212b96be576ce8884018c55e6aa42bdb67d75c1274a0d79c5ad15edb8d738a874a7e087e42c0f73b9fba5fa457580f737f9f7ef07f653f71b1bd436317e2523a4e4d8194041e84705aa22f7ef0cd98ac879ae24d1c3b8895cfc8708b9d537ba7ec2caa15f053f008f36ab92f6416e4a87361b4fb59fa48303044c5ba07f298de01a55ba64521d4f4f988')\ndf.head()","a5c0e10e":"df.describe()","17c53d44":"df.isna().sum()","5a8b43e1":"df = df.fillna(df.mean())\ndf.head()","695d877e":"df.isna().sum()","510a6107":"# top salary by degree\n\ntop_salary = df[df.status == 'Placed'].groupby(['degree_t','specialisation']).max('salary') \ntop_salary ","91e7ef4b":"# top salary by gender\n\ntop_gen_salary = df[df.status == 'Placed'].groupby(['gender']).max('salary') \ntop_gen_salary ","266112ac":"df.info()","d522b207":"df.gender.dtypes","9c75ca53":"cat = []\nnum = []\nfor i in df.columns:\n    if df[i].dtypes=='O':\n            cat.append(i)\n    else:\n        num.append(i)\nprint('The numerical columns are',num)\nprint('The catagorical columns are',cat)","24a5c140":"# let's check for the outliers in the dataset","5f90f005":"plt.figure(figsize=(14,25))\nfor i,j in zip(num,range(1,len(num)+1)):\n    plt.subplot(5,2,j)\n    sns.boxplot(df[i],color='lightblue')\nplt.show()","1ee01470":"# as we can see in the diagram that most of the outliers are in the dataset are present in salary\n# note: the perfectly normalized data can have upto 0.3% of outliers ","1b37ebbe":" # displot to check the normalization before removing the outliers\n    \nplt.figure(figsize=(14,25)) \nfor i,j in zip(num,range(1,len(num)+1)):\n    plt.subplot(5,2,j)\n    sns.kdeplot(df[i],shade=True,color='darkblue')\nplt.show()","aae5114c":"df[num].skew()","1f3d5e8b":"# salary column is so skewed than any other, usual skewness value should be -1 to 1","1f2dc8e2":"# Outliers Treatment (to reduce skewness and to normalize the data)\n\nQ1 = df.quantile(0.25)\nQ3 = df.quantile(0.75)\nIQR = Q3 - Q1\ndf = df[~((df<(Q1-1.5*IQR)) | (df>(Q3+1.5*IQR))).any(axis=1)]\ndf.head()","2bdff036":"# check the normalization after removing the outliers\n\nplt.figure(figsize=(14,25))\nfor i,j in zip(num,range(1,len(num)+1)):\n    plt.subplot(5,2,j)\n    sns.kdeplot(df[i],shade=True,color='darkblue')\nplt.show()","a532f50d":"df[num].skew() ","fa84e8e2":"# skewness after clearning the outliers, if you see that all the columns are normalized","3640c75d":"# Heatmap diagram\n\nplt.figure(figsize=(10,7))\nsns.heatmap(df.corr(),annot=True, cmap=\"Purples\")\nplt.show()","825ed35c":"# jointplot\n\nsns.jointplot(df.degree_p,df.salary,kind = 'hex',color='lightblue')\nplt.show()","04ac0829":"# pair plot\n\ng = sns.pairplot(df, diag_kind=\"kde\")\ng.map_lower(sns.kdeplot, levels=4, color=\".2\")\nplt.show()","7d488529":"from sklearn.preprocessing import LabelEncoder","85483f74":"le = LabelEncoder()","15e0fe5d":"# cat --> catagorical columns\n\nfor i in cat:\n    df[i] = le.fit_transform(df[i])\ndf.head()","3b429db5":"x = df.drop('status',1)\ny = df['status']","ad448c71":"from sklearn.model_selection import train_test_split","6839d58a":"xtrain, xtest, ytrain, ytest = train_test_split(x,y,test_size=0.3,random_state=42)","a62a8960":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report","89341965":"lr = LogisticRegression()\ndt = DecisionTreeClassifier()\nrn = RandomForestClassifier()\nknn = KNeighborsClassifier()\ngb = GaussianNB()\nsgd = SGDClassifier()","20efb774":"#LogisticRegression type 1","b33e11f9":"lr.fit(xtrain,ytrain)\nlr.predict(xtest)\nprint(lr.score(xtest,ytest))","a78672cd":"#LogisticRegression type 2","56f08e15":"lr.fit(xtrain,ytrain)\nypred = lr.predict(xtest)\nprint(accuracy_score(ypred,ytest))","ffd42813":"print('confusion matrix: \\n',confusion_matrix(ypred,ytest),'\\n')\nTP, FP, FN, TN = confusion_matrix(ypred,ytest).ravel()\nprint('TP:',TP,'FP:',FP,'FN:',FN,'TN:',TN)\nprint('Calculated accuracy:',(TP + TN) \/ (TP + TN + FP + FN),'\\n')","c88703d0":"print('classification report: \\n',classification_report(ytest,ypred))","035b6d6f":"#DecisionTreeClassifier type 1","aabd16e3":"dt.fit(xtrain,ytrain)\ndt.predict(xtest)\ndt.score(xtest,ytest)","0a7dcae8":"#DecisionTreeClassifier type 2","08290e98":"dt.fit(xtrain,ytrain)\nypred= dt.predict(xtest)\naccuracy_score(ypred,ytest)","3fc0a6a6":"print('confusion matrix: \\n',confusion_matrix(ypred,ytest),'\\n')\nTP, FP, FN, TN = confusion_matrix(ypred,ytest).ravel()\nprint('TP:',TP,'FP:',FP,'FN:',FN,'TN:',TN)\nprint('Calculated accuracy:',(TP + TN) \/ (TP + TN + FP + FN),'\\n')","98194f19":"print('classification report: \\n',classification_report(ytest,ypred))","a3a0824a":"#RandomForestClassifier type 1 ","ba5449e5":"rn.fit(xtrain,ytrain)\nrn.predict(xtest)\nrn.score(xtest,ytest)","b1d7cc7b":"#RandomForestClassifier type 2","2693c3e3":"rn.fit(xtrain,ytrain)\nypred = rn.predict(xtest)\naccuracy_score(ypred,ytest)","bfd5a92f":"print('confusion matrix: \\n',confusion_matrix(ypred,ytest),'\\n')\nTP, FP, FN, TN = confusion_matrix(ypred,ytest).ravel()\nprint('TP:',TP,'FP:',FP,'FN:',FN,'TN:',TN)\nprint('Calculated accuracy:',(TP + TN) \/ (TP + TN + FP + FN),'\\n')","e5b6e51c":"print('classification report: \\n',classification_report(ytest,ypred))","d7d873dd":"#KNeighborsClassifier type 1","f70f56e5":"knn.fit(xtrain,ytrain)\nknn.predict(xtest)\nknn.score(xtest,ytest)","53568045":"#KNeighborsClassifier type 2","dc2db6a5":"knn.fit(xtrain,ytrain)\nypred = knn.predict(xtest)\naccuracy_score(ypred,ytest)","16e1849b":"print('confusion matrix: \\n',confusion_matrix(ypred,ytest),'\\n')\nTP, FP, FN, TN = confusion_matrix(ypred,ytest).ravel()\nprint('TP:',TP,'FP:',FP,'FN:',FN,'TN:',TN)\nprint('Calculated accuracy:',(TP + TN) \/ (TP + TN + FP + FN),'\\n')","591f132b":"print('classification report: \\n',classification_report(ytest,ypred))","2fe13b2b":"#GaussianNB type 1","451c7642":"gb.fit(xtrain,ytrain)\ngb.predict(xtest)\ngb.score(xtest,ytest)","0daea4af":"#GaussianNB type 2","10c941d8":"gb.fit(xtrain,ytrain)\nypred = gb.predict(xtest)\naccuracy_score(ypred,ytest)","e35f3005":"print('confusion matrix: \\n',confusion_matrix(ypred,ytest),'\\n')\nTP, FP, FN, TN = confusion_matrix(ypred,ytest).ravel()\nprint('TP:',TP,'FP:',FP,'FN:',FN,'TN:',TN)\nprint('Calculated accuracy:',(TP + TN) \/ (TP + TN + FP + FN),'\\n')","8bd3ec35":"print('classification report: \\n',classification_report(ytest,ypred))","69e621dc":"# All alogrithums in classification model that are used above","e934b094":"li = [lr,rn,dt,knn,gb]                   # ML in most easiest way (in single cell) -- basic level of ml\nfor i in li: \n    print(i)\n    i.fit(xtrain,ytrain)\n    i.predict(xtest)\n    print('accuracy:',i.score(xtest,ytest))\n    if i==gb:\n        continue\n    print('*'*29)","aee80fbf":"li = [lr,rn,dt,knn,gb]                         # Usual ML in single cell (advanced level)\ndi = {}\nfor i in li:\n    i.fit(xtrain,ytrain)\n    ypred = i.predict(xtest)\n    print(i,\":\",accuracy_score(ypred,ytest),'\\n')\n    di.update({str(i):i.score(xtest,ytest)})\n    print('Confusion matrix: \\n',confusion_matrix(ypred,ytest),'\\n')\n    TP, FP, FN, TN = confusion_matrix(ypred,ytest).ravel()\n    print('TP:',TP,'FP:',FP,'FN:',FN,'TN:',TN)\n    print('Calculated accuracy:',(TP + TN) \/ (TP + TN + FP + FN),'\\n')\n    print('Classification report: \\n',classification_report(ytest,ypred))\n    if i==gb:\n        continue\n    print('*'*55)","478a0537":"# Accuracy value 1 is the highest of call using gaussianNb algorithms\n# 100% prediction is done using above all code\n# I have also created a plot for reference based on prediction value and algorithms used above","1b25ee35":"plt.figure(figsize=(11, 6))\nplt.title(\"Algorithm vs Accuracy\", fontweight='bold')\nplt.xlabel(\"Algorithm\")\nplt.ylabel(\"Accuracy\")\nplt.plot(di.keys(),di.values(),marker='o',color='plum',linewidth=3,markersize=10,\n         markerfacecolor='slategray',markeredgecolor='slategray')\nplt.show()","c9dbeab2":"As we can see from above graph the base ML algorithm(logistic regression) accuracy is 85% and highest ML alogrithm (GaussianNB) accuracy is 100% "}}