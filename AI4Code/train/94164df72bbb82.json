{"cell_type":{"dfc6a5a9":"code","fc5f5945":"code","9baae0d3":"code","644bd974":"code","ea9de720":"code","dd8b49be":"code","b8d05172":"code","ff1a35ab":"code","e287104c":"code","8f91a0b2":"code","39cdcc55":"code","46a619bd":"code","3c874e7a":"code","fafc3eab":"code","7ecaacc8":"code","bb9c10dd":"code","2146e684":"code","4ac61a02":"code","1e3ef958":"code","51f398b5":"code","ec3ba2f1":"code","31a741e6":"code","e50631fd":"code","0bd7e5d5":"code","93dfb4cd":"code","593cb6e0":"code","7304554c":"code","7cb0c5ec":"code","56efce87":"code","8f20d59a":"code","8b38f295":"code","627a08a8":"code","72ad7e82":"code","f7026f05":"code","5afc3a7a":"code","18ed0047":"code","12bc33b6":"code","8e5935dd":"code","e4caa987":"code","96992c62":"code","fc24d3c0":"code","0458d919":"code","c4577684":"code","d9dea5e4":"code","85e58729":"code","1714b5c3":"code","e0d328bb":"code","5da926b6":"code","15fb4bbc":"code","011903cb":"code","b623210e":"code","b9c898a3":"code","5802edb8":"code","d20b8abf":"code","e127e21d":"code","57806806":"code","05f345d1":"code","62b77262":"code","03dbcce1":"code","9a2fa486":"code","6f886d14":"code","cca9a689":"code","eb462cdd":"code","1b89324a":"code","ad5ce76f":"code","10c0f43c":"code","a2f2c1c3":"code","761476d3":"code","d586f043":"code","51e7955f":"code","c603378d":"code","92fcfa48":"code","6b78f7c4":"code","073049bc":"code","a008ff87":"code","e530d80e":"code","8928129f":"code","b90340b0":"code","40b127cd":"markdown","ab08cdd5":"markdown","e5159963":"markdown","e02b806b":"markdown","21b20950":"markdown","8af7f294":"markdown","17d2bd94":"markdown","ef33b2bb":"markdown","31ef5541":"markdown","cc4f1175":"markdown","bd7d3841":"markdown","80a021aa":"markdown","2a4e3cab":"markdown","e8cc0f65":"markdown","ff321acb":"markdown","8a34b396":"markdown","b9af9f48":"markdown","2ec153d5":"markdown","d7d7d2d7":"markdown","cc521b6d":"markdown","08603c7b":"markdown","5d57425c":"markdown","433df7e8":"markdown","3f44e12c":"markdown","669a05de":"markdown","b00f3eca":"markdown","f673bb00":"markdown","d679adc4":"markdown","95f276e7":"markdown","6f29f48b":"markdown","e4d0fb2c":"markdown","48633978":"markdown","90a45e82":"markdown","330c3e8a":"markdown","89d5bf67":"markdown","5543c533":"markdown","f6d1df0e":"markdown"},"source":{"dfc6a5a9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fc5f5945":"# DataFrame\nimport pandas as pd\n\n# Matplot\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Scikit-learn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\nfrom sklearn.manifold import TSNE\nfrom sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n# Summary\nfrom sklearn import datasets\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import ElasticNet\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import scale\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline, FeatureUnion,make_pipeline\nfrom sklearn.metrics import classification_report\n\nfrom sklearn.naive_bayes import MultinomialNB\n\nfrom scipy.stats import randint\n\n# tensor-Keras\nimport tensorflow as tf \nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Activation, Dense, Dropout, Embedding, Flatten, Conv1D, MaxPooling1D, LSTM\nfrom tensorflow.keras import utils\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n#from textstat.textstat import textstatistics,legacy_round\n# nltk\nimport nltk\nfrom nltk.corpus import stopwords\nfrom  nltk.stem import SnowballStemmer\n\n# Word2vec\nimport gensim\n\n# Utility\nimport re\nimport numpy as np\nimport os\nfrom collections import Counter\nimport logging\nimport time\nimport pickle\nimport itertools\nimport emoji\nimport spacy\nimport string\nimport warnings\nimport scipy.io\nprint(\"TensorFlow version: \", tf.__version__)\n\nwarnings.filterwarnings('ignore')","9baae0d3":"# Import the hashing vectorizer\nfrom sklearn.feature_extraction.text import HashingVectorizer\n# Import functional utilities\nfrom sklearn.preprocessing import FunctionTransformer, MaxAbsScaler\nfrom sklearn.pipeline import FeatureUnion\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_regression","644bd974":"# Import the datasets\ntrain_raw= pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\ntest_raw = pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')","ea9de720":"\ntrain_raw.head()","dd8b49be":"test_raw.head()","b8d05172":"print(train_raw.shape)\ntrain_raw.columns.values","ff1a35ab":"train_raw.info()","e287104c":"train_raw.target.unique() ","8f91a0b2":"fig, ax = plt.subplots()\nfig.suptitle(\"target\", fontsize=12)\ntrain_raw[\"target\"].reset_index().groupby(\"target\").count().sort_values(by= \n       \"index\").plot(kind=\"barh\", legend=False, \n        ax=ax).grid(axis='x')\nplt.show()","39cdcc55":"s = pd.Series(train_raw.excerpt[0:10], dtype=\"string\")\n# Concatenating a single Series into a string\nt=s.str.cat(sep=',')\n# Import the word cloud function  \n \nfrom wordcloud import WordCloud, STOPWORDS\n# Create and generate a word cloud image \nmy_cloud =  WordCloud(background_color='white', stopwords=STOPWORDS).generate(t)\n\n# Display the generated wordcloud image\nplt.imshow(my_cloud, interpolation='bilinear') \nplt.axis(\"off\")\n# Don't forget to show the final image\nplt.show()","46a619bd":"### Longest and shortest reviews\nlength_reviews = train_raw.excerpt.str.len()\n\n# How long is the longest review\nprint(max(length_reviews))\nlength_reviews = train_raw.excerpt.str.len()\n\n# How long is the shortest review\nprint(min(length_reviews))","3c874e7a":"train_raw.excerpt.isnull().sum()","fafc3eab":"train_raw.excerpt.isna().sum()","7ecaacc8":"import seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nsns.heatmap(train_raw.isnull(),cbar=False,yticklabels=False,cmap = 'viridis')","bb9c10dd":"import seaborn as sns","2146e684":"train_raw.dropna()\ntrain_raw.drop_duplicates()","4ac61a02":"import spacy\n## call model\nner = spacy.load(\"en_core_web_lg\")\n## tag text\ntxt = train_raw.excerpt.iloc[0]\ndoc = ner(txt)\n## display result\nspacy.displacy.render(doc, style=\"ent\")","1e3ef958":"## tag text and exctract tags into a list\ntrain_raw[\"tags\"] = train_raw.excerpt.apply(lambda x: [(tag.text, tag.label_) \n                             for tag in ner(x).ents] )","51f398b5":"train_raw[\"tags\"]","ec3ba2f1":"import collections\n## utils function to count the element of a list\ndef utils_lst_count(lst):\n    dic_counter = collections.Counter()\n    for x in lst:\n        dic_counter[x] += 1\n    dic_counter = collections.OrderedDict( \n                     sorted(dic_counter.items(), \n                     key=lambda x: x[1], reverse=True))\n    lst_count = [ {key:value} for key,value in dic_counter.items() ]\n    return lst_count\n\n## count tags\ntrain_raw[\"tags\"] = train_raw[\"tags\"].apply(lambda x: utils_lst_count(x))\n\n## utils function create new column for each tag category\ndef utils_ner_features(lst_dics_tuples, tag):\n    if len(lst_dics_tuples) > 0:\n        tag_type = []\n        for dic_tuples in lst_dics_tuples:\n            for tuple in dic_tuples:\n                type, n = tuple[1], dic_tuples[tuple]\n                tag_type = tag_type + [type]*n\n                dic_counter = collections.Counter()\n                for x in tag_type:\n                    dic_counter[x] += 1\n        return dic_counter[tag]\n    else:\n        return 0\n\n## extract features\ntags_set = []\nfor lst in train_raw[\"tags\"].tolist():\n     for dic in lst:\n        for k in dic.keys():\n            tags_set.append(k[1])\ntags_set = list(set(tags_set))\nfor feature in tags_set:\n     train_raw[\"tags_\"+feature] = train_raw[\"tags\"].apply(lambda x: \n                             utils_ner_features(x, feature))\n\n","31a741e6":"## print result\ntrain_raw.head()","e50631fd":"tags_list = train_raw[\"tags\"].sum()\nmap_lst = list(map(lambda x: list(x.keys())[0], tags_list))\ndtf_tags = pd.DataFrame(map_lst, columns=['tag','type'])\ndtf_tags[\"count\"] = 1\ndtf_tags = dtf_tags.groupby(['type',  \n                'tag']).count().reset_index().sort_values(\"count\", \n                 ascending=False)\nfig, ax = plt.subplots()\nfig.suptitle(\"Top frequent tags\", fontsize=12)\nsns.barplot(x=\"count\", y=\"tag\", hue=\"type\", \n            data=dtf_tags.iloc[:20,:], dodge=False, ax=ax)\nax.grid(axis=\"x\")\nplt.show()","0bd7e5d5":"## predict wit NER\ntxt = train_raw[\"excerpt\"].iloc[0]\nentities = ner(txt).ents\n## tag text\ntagged_txt = txt\nfor tag in entities:\n    tagged_txt = re.sub(tag.text, \"_\".join(tag.text.split()), \n                        tagged_txt) \n## show result\nprint(tagged_txt)","93dfb4cd":"! pip install --no-index --no-deps   ..\/input\/textstat\/Pyphen-0.10.0-py3-none-any.whl","593cb6e0":"! pip install --no-index --no-deps   ..\/input\/textstat\/textstat-0.7.0-py3-none-any.whl","7304554c":"import textstat\nfrom textstat.textstat import textstatistics,legacy_round","7cb0c5ec":"nlp = spacy.load('en_core_web_sm')","56efce87":"textstatistics().syllable_count('nameyehfd,nbmjf,')","8f20d59a":"class ReadabilityFeatures(BaseEstimator, TransformerMixin):\n    def count_regex(self, pattern, tweet):\n        return len(re.findall(pattern, tweet))\n    \n    # Splits the text into sentences, using\n    # Spacy's sentence segmentation which can\n    # be found at https:\/\/spacy.io\/usage\/spacy-101\n    def break_sentences(self, text):\n        #nlp = spacy.load('en_core_web_sm')\n        doc = nlp(text)\n        return list(doc.sents)\n    # Returns Number of Words in the text\n    def word_count(self,text):\n        sentences = self.break_sentences(text)\n        words = 0\n        for sentence in sentences:\n            words += len([token for token in sentence])\n        return words\n    # Returns the number of sentences in the text\n    def sentence_count(self,text):\n        sentences = self.break_sentences(text)\n        return len(sentences)\n    # Returns average sentence length\n    def avg_sentence_length(self,text):\n        words = self.word_count(text)\n        sentences = self.sentence_count(text)\n        average_sentence_length = float(words \/ sentences)\n        return average_sentence_length\n    # Textstat is a python package, to calculate statistics from\n    # text to determine readability,\n    # complexity and grade level of a particular corpus.\n    # Package can be found at https:\/\/pypi.python.org\/pypi\/textstat\n    def syllables_count(self,word):\n        word=word.lower()\n        return textstatistics().syllable_count(word)\n    # Returns the average number of syllables per\n    # word in the text\n    def avg_syllables_per_word(self, text):\n        syllable = self.syllables_count(text)\n        words = self.word_count(text)\n        ASPW = float(syllable) \/ float(words)\n        return legacy_round(ASPW, 1)\n    \n    # Return total Difficult Words in a text\n    def difficult_words(self,text):\n\n        #nlp = spacy.load('en_core_web_sm')\n        doc = nlp(text)\n        # Find all words in the text\n        words = []\n        sentences = self.break_sentences(text)\n        for sentence in sentences:\n            words += [str(token) for token in sentence]\n\n        # difficult words are those with syllables >= 2\n        # easy_word_set is provide by Textstat as\n        # a list of common words\n        diff_words_set = set()\n\n        for word in words:\n            syllable_count = textstatistics().syllable_count(word)\n            if word not in nlp.Defaults.stop_words and syllable_count >= 2:\n                diff_words_set.add(word)\n\n            return len(diff_words_set)\n    # A word is polysyllablic if it has more than 3 syllables\n    # this functions returns the number of all such words\n    # present in the text\n    def poly_syllable_count(self,text):\n        count = 0\n        words = []\n        sentences = self.break_sentences(text)\n        for sentence in sentences:\n            words += [token for token in sentence]\n\n\n        for word in words:\n            syllable_count = self.syllables_count(word)\n            if syllable_count >= 3:\n                count += 1\n        return count\n    def flesch_reading_ease(self, text):\n        \"\"\"\n            Implements Flesch Formula:\n            Reading Ease score = 206.835 - (1.015 \u00d7 ASL) - (84.6 \u00d7 ASW)\n            Here,\n            ASL = average sentence length (number of words\n                    divided by number of sentences)\n            ASW = average word length in syllables (number of syllables\n                    divided by number of words)\n        \"\"\"\n        FRE = 206.835 - float(1.015 * self.avg_sentence_length(text)) -\\\n            float(84.6 * self.avg_syllables_per_word(text))\n        return legacy_round(FRE, 2)\n    \n    def gunning_fog(self,text):\n        per_diff_words = (self.difficult_words(text) \/ self.word_count(text) * 100) + 5\n        grade = 0.4 * (self.avg_sentence_length(text) + per_diff_words)\n        return grade\n    def smog_index(self,text):\n        \"\"\"\n            Implements SMOG Formula \/ Grading\n            SMOG grading = 3 + ?polysyllable count.\n            Here,\n            polysyllable count = number of words of more\n            than two syllables in a sample of 30 sentences.\n        \"\"\"\n\n        #if self.sentence_count(text) >= 3:\n         #   poly_syllab = self.poly_syllable_count(text)\n          #  SMOG = (1.043 * (30*(poly_syllab \/ self.sentence_count(text)))**0.5) \\\n            #        + 3.1291\n           # return legacy_round(SMOG, 1)\n        #else:\n         #   return 0  \n        return textstat.smog_index(text)    \n     \n\n    def dale_chall_readability_score1(self,text):\n        \"\"\"\n            Implements Dale Challe Formula:\n            Raw score = 0.1579*(PDW) + 0.0496*(ASL) + 3.6365\n            Here,\n                PDW = Percentage of difficult words.\n                ASL = Average sentence length\n        \"\"\"\n        words = self.word_count(text)\n        # Number of words not termed as difficult words\n        count = words - self.difficult_words(text)\n        if words > 0:\n\n            # Percentage of words not on difficult word list\n\n            per = float(count) \/ float(words) * 100\n\n        # diff_words stores percentage of difficult words\n        diff_words = 100 - per\n\n        raw_score = (0.1579 * diff_words) + \\\n                    (0.0496 * self.avg_sentence_length(text))\n\n        # If Percentage of Difficult Words is greater than 5 %, then;\n        # Adjusted Score = Raw Score + 3.6365,\n        # otherwise Adjusted Score = Raw Score\n\n        if diff_words > 5:\t\n\n            raw_score += 3.6365\n\n        return legacy_round(score, 2)\n    def dale_chall_readability_score(self,text):\n         return textstat.dale_chall_readability_score(text)\n        \n    def fit(self, X, y=None, **fit_params):\n            # fit method is used when specific operations need to be done on the train data, but not on the test data\n        return self\n    \n    def transform(self, X, **transform_params):\n        sentence_count=X.apply(lambda x: self.sentence_count(x)) \n        avg_sentence_length=X.apply(lambda x: self.avg_sentence_length(x)) \n        syllables_count=X.apply(lambda x: self.syllables_count(x)) \n        avg_syllables_per_word=X.apply(lambda x: self.avg_syllables_per_word(x)) \n        difficult_words=X.apply(lambda x: self.difficult_words(x)) \n        #poly_syllable_count=X.apply(lambda x: self.poly_syllable_count(x)) \n        flesch_reading_ease=X.apply(lambda x: self.flesch_reading_ease(x)) \n        gunning_fog=X.apply(lambda x: self.gunning_fog(x)) \n        smog_index=X.apply(lambda x: self.smog_index(x)) \n        dale_chall_readability_score=X.apply(lambda x: self.dale_chall_readability_score(x)) \n        count_words = X.apply(lambda x: self.count_regex(r'\\w+', x)) \n        count_mentions = X.apply(lambda x: self.count_regex(r'@\\w+', x))\n        count_hashtags = X.apply(lambda x: self.count_regex(r'#\\w+', x))\n        count_char=X.apply(lambda x: sum(len(word) for word in str(x).split(\" \")))\n        count_capital_words = X.apply(lambda x: self.count_regex(r'\\b[A-Z]{2,}\\b', x))\n        count_excl_quest_marks = X.apply(lambda x: self.count_regex(r'!|\\?', x))\n        count_urls = X.apply(lambda x: self.count_regex(r'http.?:\/\/[^\\s]+[\\s]?', x))\n        # We will replace the emoji symbols with a description, which makes using a regex for counting easier\n        # Moreover, it will result in having more words in the tweet\n        count_emojis = X.apply(lambda x: emoji.demojize(x)).apply(lambda x: self.count_regex(r':[a-z_&]+:', x))\n        \n        df = pd.DataFrame({'count_words': count_words\n                           , 'count_mentions': count_mentions\n                           , 'count_hashtags': count_hashtags\n                           , 'count_capital_words': count_capital_words\n                           , 'count_excl_quest_marks': count_excl_quest_marks\n                           , 'count_urls': count_urls\n                           , 'count_emojis': count_emojis\n                           ,'count_char':count_char\n                           ,'sentence_count':sentence_count\n                           ,'avg_sentence_length':avg_sentence_length\n                           ,'syllables_count':syllables_count\n                           ,'avg_syllables_per_word':avg_syllables_per_word\n                           ,'difficult_words':difficult_words\n                           ,'flesch_reading_ease':flesch_reading_ease\n                           #,'poly_syllable_count':poly_syllable_count\n                           ,'gunning_fog':gunning_fog\n                           ,'smog_index':smog_index\n                           ,'dale_chall_readability_score':dale_chall_readability_score\n                          })\n        \n        return df","8b38f295":"class TextCounts(BaseEstimator, TransformerMixin):\n    \n    def count_regex(self, pattern, tweet):\n        return len(re.findall(pattern, tweet))\n    \n    def fit(self, X, y=None, **fit_params):\n        # fit method is used when specific operations need to be done on the train data, but not on the test data\n        return self\n    \n    def transform(self, X, **transform_params):\n        count_words = X.apply(lambda x: self.count_regex(r'\\w+', x)) \n        count_mentions = X.apply(lambda x: self.count_regex(r'@\\w+', x))\n        count_hashtags = X.apply(lambda x: self.count_regex(r'#\\w+', x))\n        count_char=X.apply(lambda x: sum(len(word) for word in str(x).split(\" \")))\n        count_capital_words = X.apply(lambda x: self.count_regex(r'\\b[A-Z]{2,}\\b', x))\n        count_excl_quest_marks = X.apply(lambda x: self.count_regex(r'!|\\?', x))\n        count_urls = X.apply(lambda x: self.count_regex(r'http.?:\/\/[^\\s]+[\\s]?', x))\n        # We will replace the emoji symbols with a description, which makes using a regex for counting easier\n        # Moreover, it will result in having more words in the tweet\n        count_emojis = X.apply(lambda x: emoji.demojize(x)).apply(lambda x: self.count_regex(r':[a-z_&]+:', x))\n        \n        df = pd.DataFrame({'count_words': count_words\n                           , 'count_mentions': count_mentions\n                           , 'count_hashtags': count_hashtags\n                           , 'count_capital_words': count_capital_words\n                           , 'count_excl_quest_marks': count_excl_quest_marks\n                           , 'count_urls': count_urls\n                           , 'count_emojis': count_emojis\n                           ,'count_char':count_char\n                          })\n        \n        return df","627a08a8":" train_raw.target.describe()","72ad7e82":"sns.histplot(data=train_raw, x=\"target\", kde=True)\n","f7026f05":"tc = ReadabilityFeatures()\ndf_eda = tc.fit_transform(train_raw[\"excerpt\"][0:2])\ndf_eda","5afc3a7a":"tc = ReadabilityFeatures()\ndf_eda = tc.fit_transform(train_raw[\"excerpt\"])\n# Add target to df_eda\ndf_eda['target'] = train_raw.target.astype(float)\ndf_eda['excerpt'] =train_raw[\"excerpt\"]","18ed0047":"# Specify the boundaries of the bins\nbins = [ -3.676268,-1.690320, -0.202540,1.711390]\n# Bin labels\nlabels = [ 'Low', 'Medium', 'High']\n# Bin the continuous variable ConvertedSalary using these boundaries\ndf_eda['target_binned'] = pd.cut(df_eda['target'], \n                                         bins=bins,labels=labels )\n","12bc33b6":"plt.figure(figsize=(12,6))\nsns.countplot(x='target_binned',data=df_eda)","8e5935dd":"KAGGLE_ENV = os.getcwd == '\/kaggle\/working'\ndef show_dist(df, col):\n    print('Descriptive stats for {}'.format(col))\n    print('-'*(len(col)+22))\n    print(df.groupby('target_binned')[col].describe())\n    bins = np.arange(df[col].min(), df[col].max() + 1)\n    g = sns.FacetGrid(df, col='target_binned', size=5, hue='target_binned', palette=\"PuBuGn_d\")\n    g = g.map(sns.distplot, col, kde=False, norm_hist=True, bins=bins)\n    plt.show()\n    if not KAGGLE_ENV: g.savefig( col + '_dist.png')","e4caa987":"show_dist(df_eda, 'count_char')","96992c62":"show_dist(df_eda, 'count_words')","fc24d3c0":"show_dist(df_eda, 'count_mentions')","0458d919":"show_dist(df_eda, 'count_hashtags')","c4577684":"show_dist(df_eda, 'count_capital_words')","d9dea5e4":"show_dist(df_eda, 'count_excl_quest_marks')","85e58729":"show_dist(df_eda, 'count_urls')","1714b5c3":"show_dist(df_eda, 'count_emojis')","e0d328bb":"show_dist(df_eda, 'difficult_words')","5da926b6":"nlp = spacy.load('en_core_web_sm')\nclass CleanText(BaseEstimator, TransformerMixin):\n   \n    def remove_mentions(self, input_text):\n        return re.sub(r'@\\w+', '', input_text)\n    \n    def remove_urls(self, input_text):\n        return re.sub(r'http.?:\/\/[^\\s]+[\\s]?', '', input_text)\n    \n    def emoji_oneword(self, input_text):\n        # By compressing the underscore, the emoji is kept as one word\n        return input_text.replace('_','')\n    \n    def remove_punctuation(self, input_text):\n        # Make translation table\n        punct = string.punctuation\n        trantab = str.maketrans(punct, len(punct)*' ')  # Every punctuation symbol will be replaced by a space\n        return input_text.translate(trantab)\n\n    def remove_digits(self, input_text):\n        return re.sub('\\d+', '', input_text)\n    \n    def to_lower(self, input_text):\n        return input_text.lower()\n    \n    def remove_stopwords(self, input_text):\n        stopwords_list = stopwords.words('english')\n        # Some words which might indicate a certain sentiment are kept via a whitelist\n        whitelist = [\"n't\", \"not\", \"no\"]\n        words = input_text.split() \n        clean_words = [word for word in words if (word not in stopwords_list or word in whitelist) and len(word) > 1] \n        return \" \".join(clean_words) \n    \n    def Lemmatizing(self, input_text):\n        # Create our list of stopwords\n        # Lemmatizing each token and converting each token into lowercase\n       \n        # use word_tokenize to tokenize the sentences\n        mytokens = nlp(input_text)\n        mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n        return \" \".join(mytokens)\n    \n    def fit(self, X, y=None, **fit_params):\n        return self\n    \n    def transform(self, X, **transform_params):\n        clean_X = X.apply(self.remove_mentions).apply(self.remove_urls).apply(self.emoji_oneword).apply(self.remove_punctuation).apply(self.remove_digits).apply(self.to_lower).apply(self.remove_stopwords).apply(self.Lemmatizing)\n        return clean_X","15fb4bbc":"df_eda['excerpt'].head()","011903cb":"type(df_eda['excerpt'])","b623210e":"ct = CleanText()\nsr_clean = ct.fit_transform(df_eda['excerpt'])","b9c898a3":"sr_clean.head(5)","5802edb8":"empty_clean = sr_clean == ''\nprint('{} records have no words left after text cleaning'.format(sr_clean[empty_clean].count()))\nsr_clean.loc[empty_clean] = '[no_text]'","d20b8abf":"# Transform the list of sentences into a list of words\nall_words = ' '.join(sr_clean).split(' ')\n#all_words=[w for w in DOC ]\n# Get number of unique words\nvocab_size = len(set(all_words))\nprint(vocab_size)\n# Count all unique words\neap_counts = collections.Counter(all_words )# Count all unique words\nlen(eap_counts)","e127e21d":"eap_common_words = [word[0] for word in eap_counts.most_common(25)]\neap_common_counts = [word[1] for word in eap_counts.most_common(25)]\n\n# Use spooky background\nplt.style.use('dark_background')\nplt.figure(figsize=(15, 12))\n\nsns.barplot(x=eap_common_words, y=eap_common_counts)\nplt.title('Most Common Words in our text ')\nplt.show()","57806806":"df_model = df_eda\ndf_model['clean_text'] = sr_clean\ndf_model.columns.tolist()","05f345d1":"df_model.target","62b77262":"df_model['clean_text']","03dbcce1":"df_model.columns.tolist()","9a2fa486":"df_model=df_model.dropna()\nX=df_model[['count_words',\n 'count_mentions',\n 'count_hashtags',\n 'count_capital_words',\n 'count_excl_quest_marks',\n 'count_urls',\n 'count_emojis',\n 'count_char',\n 'sentence_count',\n 'avg_sentence_length',\n 'syllables_count',\n 'avg_syllables_per_word',\n 'difficult_words',\n 'flesch_reading_ease',\n 'gunning_fog',\n 'smog_index',\n 'dale_chall_readability_score',\n 'clean_text']]\ny=df_model.target","6f886d14":"X.shape","cca9a689":"y.shape","eb462cdd":"#X_train.info()","1b89324a":"np.random.seed(0)\n# select the float columns\nnum_columns = X.select_dtypes(include=['int64','float64']).columns\n# select non-numeric columns\ncat_columns = X.select_dtypes(exclude=['int64','float64']).columns\nprint(num_columns)\nprint(cat_columns)","ad5ce76f":"# Column selector \nclass ColumnExtractor(TransformerMixin, BaseEstimator):\n    def __init__(self, cols):\n        self.cols = cols\n\n    def transform(self, X, **transform_params):\n        return X[self.cols]\n\n    def fit(self, X, y=None, **fit_params):\n        return self\n# Outlier Handle \nclass OutlierReplace(BaseEstimator,TransformerMixin):\n    def __init__(self,factor=1.5):\n        self.factor = factor\n\n    def outlier_removal(self,X,y=None):\n        X = pd.Series(X).copy()\n        qmin=X.quantile(0.05)\n        qmax=X.quantile(0.95)\n        q1 = X.quantile(0.25)\n        q3 = X.quantile(0.75)\n        iqr = q3 - q1\n        lower_bound = q1 - (self.factor * iqr)\n        upper_bound = q3 + (self.factor * iqr)\n        #X.loc[((X < lower_bound) | (X > upper_bound))] = np.nan \n        X.loc[X < lower_bound] = qmin\n        X.loc[X > upper_bound] = qmax\n        return pd.Series(X)\n\n    def fit(self,X,y=None):\n        return self\n\n    def transform(self,X,y=None):\n        return X.apply(self.outlier_removal)  ","10c0f43c":"get_text_data = FunctionTransformer(lambda x: x['clean_text'], validate=False)\nget_numeric_data = FunctionTransformer(lambda x: x[num_columns], validate=False)\ndata_preprocess_feature_extraction =FeatureUnion(\n            transformer_list = [\n                ('numeric_features', Pipeline([\n                    ('selector', get_numeric_data),\n                    ('Outlier', OutlierReplace()),\n                    ('Imputer',SimpleImputer(strategy='median',add_indicator=True)),\n                    ('Scaler',StandardScaler())\n                    \n                ])),\n                ('text_features', Pipeline([\n                    ('selector', get_text_data),\n                    ('vectorizer', TfidfVectorizer(ngram_range=(1, 2), binary=True)),\n                    ('dim_red', SelectKBest(f_regression, k=10))\n                ]))\n             ]\n        )","a2f2c1c3":"xprep= data_preprocess_feature_extraction.fit_transform(X,y)\nxprep","761476d3":"from sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n\n\ncross_validation_design = KFold(n_splits=3,\n                                shuffle=True,\n                                random_state=77)\n\ncross_validation_design","d586f043":"#RF_MODEL['gridsearch'].cv_results_","51e7955f":"from sklearn.base import BaseEstimator, TransformerMixin\nfrom scipy import sparse\nfrom itertools import combinations\n\n\nclass SparseInteractions(BaseEstimator, TransformerMixin):\n    def __init__(self, degree=2, feature_name_separator=\"_\"):\n        self.degree = degree\n        self.feature_name_separator = feature_name_separator\n    \n    def fit(self, X, y=None):\n        return self\n        \n    def transform(self, X):\n        if not sparse.isspmatrix_csc(X):\n            X = sparse.csc_matrix(X)\n            \n        if hasattr(X, \"columns\"):\n            self.orig_col_names = X.columns\n        else:\n            self.orig_col_names = np.array([str(i) for i in range(X.shape[1])])\n            \n        spi = self._create_sparse_interactions(X)\n        return spi\n    \n    \n    def get_feature_names(self):\n        return self.feature_names\n    \n    def _create_sparse_interactions(self, X):\n        out_mat = []\n        self.feature_names = self.orig_col_names.tolist()\n        \n        for sub_degree in range(2, self.degree + 1):\n            for col_ixs in combinations(range(X.shape[1]), sub_degree):\n                # add name for new column\n                name = self.feature_name_separator.join(self.orig_col_names[list(col_ixs)])\n                self.feature_names.append(name)\n                \n                # get column multiplications value\n                out = X[:, col_ixs[0]]    \n                for j in col_ixs[1:]:\n                    out = out.multiply(X[:, j])\n\n                out_mat.append(out)\n\n        return sparse.hstack([X] + out_mat)\n    ","c603378d":"from xgboost import XGBRegressor\nfrom sklearn import metrics\nfrom numpy import mean \nfrom numpy import absolute\nfrom sklearn.model_selection import KFold\nfrom sklearn.feature_selection import SelectKBest, chi2\nXGB_pipe=Pipeline([('features', data_preprocess_feature_extraction),\n                   ('int', SparseInteractions(degree=2)),\n                   ('scale', MaxAbsScaler()),\n                   ('XGB',  XGBRegressor(verbosity=0,n_estimators=120, n_jobs=6)) ])\n\nscores = cross_val_score(XGB_pipe, X, y, scoring='neg_mean_absolute_error', cv=cross_validation_design, n_jobs=-1)\n# convert scores to positive\nscores = absolute(scores)\n# summarize the result\ns_mean = mean(scores)\nprint('Mean MAE: %.3f' % (s_mean))","92fcfa48":"XGB_pipe.fit(X, y)\nXGB_pipe.score(X, y)","6b78f7c4":"from sklearn.linear_model import Ridge\n#ridge_model = Ridge(fit_intercept=True, normalize=False)\nridge_model_pipe=Pipeline([('features', data_preprocess_feature_extraction),\n                   ('int', SparseInteractions(degree=2)),\n                   ('scale', MaxAbsScaler()),\n                   ('ridge',  Ridge(fit_intercept=True, normalize=False)) ])\n\n#scores = cross_val_score(ridge_model_pipe, X, y, scoring='neg_mean_absolute_error', cv=cross_validation_design, n_jobs=-1)\n# convert scores to positive\n#scores = absolute(scores)\n# summarize the result\n#s_mean = mean(scores)\n#print('Mean MAE: %.3f' % (s_mean))","073049bc":"from sklearn.linear_model import LinearRegression\nLinearRegression_pipe=Pipeline([('features', data_preprocess_feature_extraction),\n                   ('int', SparseInteractions(degree=2)),\n                   ('scale', MaxAbsScaler()),\n                   ('ridge', LinearRegression()) ])\n#scores = cross_val_score(LinearRegression_pipe, X, y, scoring='neg_mean_absolute_error', cv=cross_validation_design, n_jobs=-1)\n# convert scores to positive\n#scores = absolute(scores)\n# summarize the result\n#s_mean = mean(scores)\n#print('Mean MAE: %.3f' % (s_mean))","a008ff87":"ct = CleanText()\ntest_clean = ct.fit_transform(test_raw['excerpt'])\ntc = ReadabilityFeatures()\ntest_eda = tc.fit_transform(test_raw[\"excerpt\"])\ntest_model = test_eda\ntest_model['clean_text'] = test_clean\ntest_model.columns.tolist()","e530d80e":"test_model.shape","8928129f":"\npreds = XGB_pipe.predict(test_model)   \n# use the method pipeline.predict on X_test data to predict the labels\n\nmy_submission = pd.DataFrame({'id': test_raw.id, 'target': preds.ravel()})\n# you could use any filename. We choose submission here\nmy_submission.to_csv('submission.csv', index=False)\n","b90340b0":"my_submission","40b127cd":"It could be interesting to see how the TextStats variables relate to the class variable. Therefore we write a function **show_dist** that provides descriptive statistics and a plot per target class.","ab08cdd5":"cv = CountVectorizer()\nbow = cv.fit_transform(sr_clean)\nword_freq = dict(zip(cv.get_feature_names(), np.asarray(bow.sum(axis=0)).ravel()))\nword_counter = collections.Counter(word_freq)\nword_counter_df = pd.DataFrame(word_counter.most_common(20), columns = ['word', 'freq'])\n\nfig, ax = plt.subplots(figsize=(12, 10))\nbar_freq_word = sns.barplot(x=\"word\", y=\"freq\", data=word_counter_df, palette=\"PuBuGn_d\", ax=ax)\nplt.show();\nif not KAGGLE_ENV: bar_freq_word.get_figure().savefig('bar_freq_word.png')","e5159963":"Moving forward with another useful application of NER: do you remember when we removed stop words losing the word \u201cWill\u201d from the name of \u201cWill Smith\u201d? An interesting solution to that problem would be replacing \u201cWill Smith\u201d with \u201cWill_Smith\u201d, so that it won\u2019t be affected by stop words removal. Since going through all the texts in the dataset to change names would be impossible, let\u2019s use SpaCy for that. As we know, SpaCy can recognize a person name, therefore we can use it for name detection and then modify the string.","e02b806b":"## Hyperparameter tuning and cross-validation\nAs we will see below, the vectorizers and classifiers all have configurable parameters. In order to chose the best parameters, we need to evaluate on a separate validation set that was not used during the training. However, using only one validation set may not produce reliable validation results. Due to chance you might have a good model performance on the validation set. If you would split the data otherwise, you might end up with other results. To get a more accurate estimation, we perform **cross-validation**. \n\nWith cross-validation the data is split into a train and validation set multiple times. The evaluation metric is then averaged over the different folds. Luckily, GridSearchCV applies cross-validation out-of-the-box.\n\nTo find the best parameters for both a vectorizer and classifier, we create a **Pipeline**. All this is put into a function for ease of use.\n\n### Evaluation metrics\nBy default GridSearchCV uses the default scorer to compute the *best_score_*. For both the MultiNomialNb and LogisticRegression this default scoring metric is the accuracy. \nfor calssification we have : \n\nIn our function *grid_vect* we additionally generate the *classification_report* on the test data. This provides some interesting metrics **per target class**, which might be more appropriate here. These metrics are the **precision, recal and F1 score.**\n\n* **Precision: ** Of all rows we predicted to be a certain class, how many did we correctly predict?\n* **Recall: ** Of all rows of a certain class, how many did we correctly predict?\n* **F1 score: ** Harmonic mean of Precision and Recall.\n\nPrecision and Recall can be calculated with the elements of the [confusion matrix](https:\/\/en.wikipedia.org\/wiki\/Confusion_matrix)\n\nfor our case we have regression : \n\nSubmissions are scored on the root mean squared error. RMSE is defined as:\n\n      RMSE=\u221a1nn\u2211i=1(yi\u2212\u02c6yi)2\n\nwhere \u02c6y is the predicted value, y is the original value, and n is the number of rows in the test data.","21b20950":"# Let's bin Target","8af7f294":"# Import the hashing vectorizer\nfrom sklearn.feature_extraction.text import HashingVectorizer\n# Import functional utilities\nfrom sklearn.preprocessing import FunctionTransformer, MaxAbsScaler\nfrom sklearn.pipeline import FeatureUnion\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_regression\n# Perform preprocessing\nget_text_data = FunctionTransformer(combine_text_columns, validate=False)\nget_numeric_data = FunctionTransformer(lambda x: x[NUMERIC_COLUMNS], validate=False)\n# Instantiate the winning model pipeline: pl\npl = Pipeline([\n        ('union', FeatureUnion(\n            transformer_list = [\n                ('numeric_features', Pipeline([\n                    ('selector', get_numeric_data),\n                    ('imputer', Imputer())\n                ])),\n                ('text_features', Pipeline([\n                    ('selector', get_text_data),\n                    ('vectorizer', HashingVectorizer(token_pattern=TOKENS_ALPHANUMERIC,\n                                                     non_negative=True, norm=None, binary=False,\n                                                     ngram_range=(1, 2))),\n                    ('dim_red', SelectKBest(f_regression, k=300))\n                ]))\n             ]\n        )),\n        ('int', SparseInteractions(degree=2)),\n        ('scale', MaxAbsScaler()),\n        ('clf', OneVsRestClassifier(LogisticRegression()))\n    ])","17d2bd94":"# Step 2 find the best model in iterative way\n## Pipeline 1 : RandomForest Classifer","ef33b2bb":"**NOTE: **One side-effect of text cleaning is that some rows do not have any words left in their text. For the CountVectorizer and TfIdfVectorizer this does not really pose a problem. However, for the Word2Vec algorithm this causes an error. There are different strategies that you could apply to deal with these missing values.\n\n* Remove the complete row, but in a production environment this is not really desirable.\n* Impute the missing value with some placeholder text like *[no_text]*\n* Word2Vec: use the average of all vectors\n\nHere we will impute with a placeholder text.","31ef5541":"# Text variable\nTo analyze the text variable we create a class **TextCounts**. In this class we compute some basic statistics on the text variable. This class can be used later in a Pipeline, as well.\n\n* **count_words** : number of words in the tweet\n* **count_mentions** : referrals to other Twitter accounts, which are preceded by a @\n* **count_hashtags** : number of tag words, preceded by a #\n* **count_capital_words** : number of uppercase words, could be used to *\"shout\"* and express (negative) emotions\n* **count_excl_quest_marks** : number of question or exclamation marks\n* **count_urls** : number of links in the tweet, preceded by http(s)\n* **count_emojis** : number of emoji, which might be a good indication of the sentiment","cc4f1175":"RF_MODEL['gridsearch'].best_params_","bd7d3841":"# NLP PIPELINE\n\nThere are mainly 4 stages of an NLP pipeline :  \n\n## Exploratory Data Analysis\n## Text Processing\n    Cleaning\n    Normalization\n    Tokenize\n    Stop word removal\n    Stemming and Lemmatization\n    POS and NER\n    \n## Feature Extraction\n    Bag of Words\n    TF-IDF\n    word2vec\n    Glove\n\n##  Modeling\n    Model\n    Train\n    Predict ","80a021aa":"# Submission ","2a4e3cab":"# Set up your environment & Import libraries and modules","e8cc0f65":"# Linear Regression: ","ff321acb":"rfpipe1=Pipeline([ ('features', data_preprocess_feature_extraction),\n                               ('rf', RandomForestRegressor(random_state=7,n_estimators=150))])\nrfpipe1.fit(X, y)","8a34b396":" from sklearn.ensemble import RandomForestRegressor\n\nRF_MODEL = {}\n\n# define pipe \nRF_MODEL['pipeline']=Pipeline([\n                               ('features', data_preprocess_feature_extraction),\n                               ('rf', RandomForestRegressor(random_state=7))])\n\n# Define hyperparams\nRF_MODEL['hyperparams'] = {}\nRF_MODEL['hyperparams']['rf__n_estimators'] = [ 100]\n#RF_MODEL['hyperparams']['rf__max_features'] = [None, 'sqrt', 'log2', .1, .25, .50, .75, .85]\n#RF_MODEL['hyperparams']['rf__max_depth'] = [None, 4, 7, 10, 20]\n\n# Search d'HP\nRF_MODEL['gridsearch'] = GridSearchCV(\n    estimator=RF_MODEL['pipeline'],\n    param_grid=RF_MODEL['hyperparams'],\n    scoring='neg_mean_squared_error',\n    cv=cross_validation_design\n)\n\nRF_MODEL['gridsearch'].fit(X, y)","b9af9f48":"##  Outlier remove ","2ec153d5":"# Complete Pipe \n![image.png](attachment:c269b891-42bc-4797-bd31-e8639d0c2ed3.png)","d7d7d2d7":"## Longest and shortest reviews","cc521b6d":"# Creating test data\nTo evaluate the trained models we'll need a **test set**. Evaluating on the train data would not be correct because the models are trained to minimize their cost function. \n\nFirst we combine the TextCounts variables with the CleanText variable.\n\n**NOTE: **Initially, I made the mistake to do execute TextCounts and CleanText in the GridSearchCV below. This took too long as it applies these functions each run of the GridSearch. It suffices to run them only once.","08603c7b":"# Named-Entity Recognition\nNER (Named-entity recognition) is the process to tag named entities mentioned in unstructured text with pre-defined categories such as person names, organizations, locations, time expressions, quantities, etc.\nTraining a NER model is really time-consuming because it requires a pretty rich dataset. Luckily there is someone who already did this job for us. One of the best open source NER tools is SpaCy. It provides different NLP models that are able to recognize several categories of entities.\n\n\nI will give an example using the SpaCy model en_core_web_lg (the large model for English trained on web data) on our usual headline (raw text, not preprocessed)","5d57425c":"from sklearn.compose import make_column_transformer\n\ncat_features = make_pipeline(\n  TfidfVectorizer(ngram_range=(1, 2))\n    #,SelectKBest(f_regression, k=300)\n)\n\nnum_features= make_pipeline( OutlierReplace(),\n                    SimpleImputer(strategy='median',add_indicator=True),\n    StandardScaler()\n)\n\n\ndata_preprocess_feature_extraction = make_column_transformer(\n    ( cat_features , cat_columns),\n    ( num_features, num_columns)\n)","433df7e8":"# Feature Engineer \n## Bag of words ","3f44e12c":"# Complete preprocess pipe : \n![image.png](attachment:e913bfce-f47a-4ecb-ad71-f89b29fcf2ea.png)\n","669a05de":"# Compute and print metrics\nprint(\"RMSE: {}\".format(rfpipe1.score(X, y)))\n","b00f3eca":"# Phase - Find the Best Pipeline\u00b6\n## Step 1 : Define CrossValidation Strategy :","f673bb00":"from xgboost import XGBRegressor\nfrom sklearn import metrics\nfrom sklearn.feature_selection import SelectKBest, chi2\n # Deine the pipe :\nXGB_pipe=Pipeline([('features', data_preprocess_feature_extraction),\n                   ('int', SparseInteractions(degree=2)),\n                   ('scale', MaxAbsScaler()),\n                   ('XGB',  XGBRegressor(verbosity=0)) ])\n#  Define our search space for grid search\nsearch_space = [\n  {\n    'XGB__n_estimators': [50, 100],\n    'XGB__learning_rate': [0.01],\n    #'XGB__max_depth': range(3, 10),\n    #'XGB__colsample_bytree': [i\/10.0 for i in range(1, 3)],\n    #'XGB__gamma': [i\/10.0 for i in range(3)],\n    #'XGB__score_func': [chi2],\n    #'XGB__k': [10],\n  }\n]\nxgb_MODEL = GridSearchCV(estimator = XGB_pipe,param_grid = search_space, scoring='neg_mean_squared_error', cv=cross_validation_design)\nxgb_MODEL.fit(X,y)","d679adc4":"# Readability Index\nReadability is the ease with which a reader can understand a written text. In natural language, the readability of text depends on its content (the complexity of its vocabulary and syntax). It focuses on the words we choose, and how we put them into sentences and paragraphs for the readers to comprehend. \n\nOur main objective in writing is to pass along information that both the writer and the reader think is worthwhile. If we fail to convey that information, our efforts are wasted. In order to engage the reader, it\u2019s critical to present information to them that they\u2019ll gladly keep reading and be able to understand clearly. So, it is required that the content be easy enough to read and understand thus is as readable as possible. There are various available Difficulty Scales with their own difficulty determining formulae.\n\nThis article illustrates various traditional readability formulae available for readability score evaluation. In Natural Language Processing, sometimes it is required to analyse words and sentences to determine the difficulty of the text. Readability Scores are generally grade levels on particular scales, which rates the text as to whats the difficulty of that particular text. It assists the writer in improving the text to make it understandable for a larger audience, thus making content engaging. \n\nVarious available Readabilty Score Determination Methods\/Formulae:\n\n    The Dale\u2013Chall formula \n    The Gunning fog formula \n     Fry readability graph \n    McLaughlin\u2019s SMOG formula \n    The FORCAST formula \n     Readability and newspaper readership \n     Flesch Scores \n     \nmore informations :https:\/\/en.wikipedia.org\/wiki\/Readability     ","95f276e7":"So df_model now contains several variables. However, our vectorizers (see below) will only need the *clean_text* variable. The TextCounts variables can be added as such. To specifically select columns, I wrote the class **ColumnExtractor** below. This can be used in the Pipeline afterwards.","6f29f48b":"# Exploratory Data Analysis\nIt\u2019s always a good idea (and frankly, a mandatory step) to first explore the data we have. This helps us not only unearth hidden patterns, but gain a valuable overall insight into what we are working with.\n\n####  Data Insights","e4d0fb2c":"bow.shape","48633978":"## Pipe2 Xgboost  ","90a45e82":"#  LinearRegression","330c3e8a":"In order to go deeper into the analysis, we need to unpack the column \u201ctags\u201d we created in the previous code. Let\u2019s plot the most frequent tags for one of the headline categories:","89d5bf67":"#  X and y ","5543c533":"# Text Preprocessing\nData preprocessing is the phase of preparing raw data to make it suitable for a machine learning model. \n\nFor NLP, that includes text cleaning, stopwords removal, stemming and lemmatization.\n\nText cleaning steps vary according to the type of data and the required task. Generally, the string is converted to lowercase and punctuation is removed before text gets tokenized. Tokenization is the process of splitting a string into a list of strings (or \u201ctokens\u201d).\nI will put all those preprocessing steps into a single function and apply it to the whole dataset\n\n## Cleaning\nBefore we start using the tweets' text we clean it. We'll do the this in the class CleanText:\n- remove the **mentions**, as we want to make the model generalisable.\n- remove the **hash tag sign** (#) but not the actual tag as this may contain information\n- set all words to **lowercase**\n- remove all **punctuations**, including the question and exclamation marks\n- remove the **urls** as they do not contain useful information and we did not notice a distinction in the number of urls used between the sentiment classes\n- make sure the converted **emojis** are kept as one word. \n- remove **digits**\n- remove **stopwords**\n- apply the **Lem** to keep the lem  of the words\n\nhttps:\/\/srinivas-yeeda.medium.com\/preprocessing-for-natural-language-processing-498df071ab6e","f6d1df0e":"## Summary Statistics\n### Check missing values"}}