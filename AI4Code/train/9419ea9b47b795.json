{"cell_type":{"6f1862b6":"code","9e3f191f":"code","2b2997d7":"code","78b5f105":"code","38054adf":"code","d67a5ef8":"code","d4b6f39f":"code","41903196":"code","a259af17":"code","19bd7162":"code","2d882989":"code","9240a74f":"code","636248c1":"code","00b6137a":"code","8bca2ccc":"code","35c49c1c":"code","4f84ab58":"code","5ee1d2be":"code","91fe8887":"code","14fbb447":"code","4a6969b3":"code","ef42fc40":"code","625136dc":"code","fb858393":"code","f3d24d38":"code","fbfbd413":"code","bed24a43":"code","a761d033":"code","e4faea23":"code","f52f81ca":"code","5152e476":"code","2a771f98":"code","e9cfa1b0":"markdown","22c20a38":"markdown","9e9dde44":"markdown","1d0f43bd":"markdown","c7041674":"markdown","1a850bc6":"markdown","6745d793":"markdown","4db11b92":"markdown","aaee63c2":"markdown","7f35e411":"markdown","ecfed96a":"markdown","82cf21f0":"markdown","4295c2ce":"markdown","ab31f722":"markdown","bb608174":"markdown","1f042838":"markdown","d6751af9":"markdown","73ad4bc7":"markdown"},"source":{"6f1862b6":"import pandas as pd\nimport numpy as np\nimport datetime\nimport keras\nimport matplotlib.pyplot as plt # basic plotting\nimport tensorflow as tf","9e3f191f":"IBM_data = pd.read_csv('\/kaggle\/input\/stock-time-series-20050101-to-20171231\/IBM_2006-01-01_to_2018-01-01.csv')\nprint(IBM_data.shape)\nIBM_data","2b2997d7":"IBM_data['Date'] = pd.to_datetime(IBM_data['Date'])\nIBM_data.set_index('Date', inplace = True)\nts = IBM_data['Close']\nts","78b5f105":"plt.figure(figsize = (10,5))\nplt.plot(ts)\nplt.title('Data series')\nplt.show()","38054adf":"# Data frame for metrics\nmetricsDF = pd.DataFrame(columns=['mse', 'mae'])\n\n#Functions to evaluate results and save metrics\ndef evalModel(forecast):\n    mse = keras.metrics.mean_squared_error(test.values.reshape(-1), forecast).numpy()\n    mae = keras.metrics.mean_absolute_error(test.values.reshape(-1), forecast).numpy()\n    return mse, mae\n\ndef addMetrics(metricsDF, modelName, forecast):\n    metricsDF.loc[modelName] = list(evalModel(forecast))\n    return metricsDF\n","d67a5ef8":"# Train-validation split\nsplit_time = 2400\ntrain = ts[:split_time]\ntest = ts[split_time:]\n\nplt.figure(figsize=(10,5))\nplt.xlabel('Date')\nplt.ylabel('Close Price')\nplt.title('Train Series')\nplt.plot(train);\nplt.grid()\nplt.show()\n\nplt.figure(figsize=(10,5))\nplt.xlabel('Date')\nplt.ylabel('Close Price')\nplt.title('Test series')\nplt.plot(test);\nplt.grid()\nplt.show()","d4b6f39f":"def moving_average_forecast(series, window_size):\n    # If window_size =1 is equivalent to naive forecast\n    forecast = []\n    for time in range(len(series) - window_size):\n        forecast.append(series[time:time + window_size].mean())\n    return np.array(forecast)","41903196":"mov_avg = moving_average_forecast(ts.values.reshape(-1), 10)[split_time - 10:]\n\nplt.figure(figsize=(16,8))\nplt.xlabel('Date')\nplt.plot(test)\nplt.plot(test.index, mov_avg)\nplt.legend(['Series', 'Forecast'])\nplt.title('Moving Average Forecast')\nplt.grid()\nplt.show()","a259af17":"addMetrics(metricsDF, \"movAverage\", mov_avg)\nmetricsDF","19bd7162":"diff_ts = (ts[1:].values.reshape(-1) - ts[: -1].values.reshape(-1))\ndiff_mov_avg = moving_average_forecast(diff_ts, 6)[split_time - 7:]\n\nplt.figure(figsize=(16,8))\nplt.xlabel('Date')\nplt.ylabel('Diferenced Close prices')\nplt.plot(test.index, diff_ts[split_time -1 :])\nplt.plot(test.index, diff_mov_avg)\nplt.legend(['Series', 'Forecast'])\nplt.grid()\nplt.show()","2d882989":"diff_mov_avg_plus_past = ts[split_time - 1: -1].values.reshape(-1) + diff_mov_avg\n\nplt.figure(figsize=(16,8))\nplt.xlabel('Date')\nplt.ylabel('Close prices')\nplt.plot(test)\nplt.plot(test.index, diff_mov_avg_plus_past)\nplt.legend(['Series', 'Forecast'])\nplt.grid()\nplt.show()","9240a74f":"addMetrics(metricsDF, \"diffMovAverage\", diff_mov_avg_plus_past)\nmetricsDF\n","636248c1":"class deepNeuralNetwork():\n    def __init__(self, timeSeries, train, test, window_size, batch_size):\n        self.ts = timeSeries\n        self.train = train\n        self.test = test\n        self.window_size = window_size\n        self.batch_size = batch_size\n\n\n    def define_NN(self, layerSize):\n        self.model = tf.keras.models.Sequential([\n                    tf.keras.layers.Dense(layerSize, input_shape=[self.window_size], activation=\"relu\"), \n                    tf.keras.layers.Dense(layerSize, activation=\"relu\"), \n                    tf.keras.layers.Dense(1)\n                ])\n\n\n    def gen_win_dataset(self, series):\n        ds = tf.data.Dataset.from_tensor_slices(series.values.reshape(-1))\n        ds = ds.window(self.window_size + 1, shift=1, drop_remainder=True)\n        ds = ds.flat_map(lambda window: window.batch(self.window_size + 1))\n        ds = ds.shuffle(1000).map(lambda window: (window[:-1], window[-1]))\n        ds = ds.batch(self.batch_size).prefetch(1)\n        return ds\n\n    \n    def findOptimalLR(self):\n        lr_schedule = tf.keras.callbacks.LearningRateScheduler(\n                lambda epoch: 1e-8 * 10**(epoch \/ self.window_size))\n        optimizer = tf.keras.optimizers.SGD(lr=1e-8, momentum=0.9)\n        self.model.compile(loss=\"mse\", optimizer=optimizer)\n        ts_dataset = self.gen_win_dataset(self.train)\n        self.LRfinderHistory = self.model.fit(ts_dataset, epochs=100, callbacks=[lr_schedule], verbose=0)\n\n    \n    def plotOptimalLoss(self, axis):\n        lrs = 1e-8 * (10 ** (np.arange(100) \/ self.window_size))\n        plt.semilogx(lrs, self.LRfinderHistory.history[\"loss\"])\n        plt.axis(axis)\n        plt.show()\n\n    \n    def trainModel(self, learningRate, numEpochs):\n        Early_stop_cb = tf.keras.callbacks.EarlyStopping(monitor = 'loss' ,patience=20)\n        optimizer = tf.keras.optimizers.SGD(lr=learningRate, momentum=0.9)\n        self.model.compile(loss=\"mse\", optimizer=optimizer)\n        ts_dataset = self.gen_win_dataset(self.train)\n        self.modelHistory = self.model.fit(\n            ts_dataset,\n            epochs=numEpochs,\n            callbacks=[Early_stop_cb],\n            verbose=0\n        )\n\n    \n    def plotLoss(self, axis):\n        loss = self.modelHistory.history['loss']\n        epochs = range(len(loss))\n        plt.plot(epochs, loss, label='Training Loss')\n        plt.axis(axis)\n        plt.show()\n\n        \n    def forecast(self):\n        #Note: forecast is made using ALL timeseries but return only the correspoing to test set\n        data_array = self.ts.values.reshape(-1)\n        forecast = []\n        for time in range(len(data_array)-self.window_size):\n            pred_array = data_array[time:time + self.window_size][np.newaxis]\n            forecast.append(self.model.predict(pred_array))\n\n        forecast = forecast[2400-self.window_size:]\n        self.forecastResults = np.array(forecast)[:, 0, 0]\n\n\n    def plotForecast(self):\n        plt.figure(figsize=(16,8))\n        plt.xlabel('Date')\n        plt.ylabel('Stock Price')\n        plt.plot(self.test)\n        plt.plot(self.test.index, self.forecastResults)\n        plt.legend(['Series', 'Forecast'])\n        plt.grid()\n        plt.show()\n               \n","00b6137a":"deepNN = deepNeuralNetwork(\n    timeSeries=ts,\n    train= train,\n    test = test,\n    window_size= 20,\n    batch_size= 32\n)\n","8bca2ccc":"tf.keras.backend.clear_session()\ntf.random.set_seed(51)\nnp.random.seed(51)\n\ndeepNN.define_NN(10)\ndeepNN.findOptimalLR()\ndeepNN.plotOptimalLoss([1e-8, 1e-3, 0, 40])","35c49c1c":"tf.keras.backend.clear_session()\ntf.random.set_seed(51)\nnp.random.seed(51)\n\ndeepNN.define_NN(10)\ndeepNN.trainModel(learningRate = 2e-6, numEpochs = 500)\ndeepNN.plotLoss([0,300, 0, 40])","4f84ab58":"tf.keras.backend.clear_session()\ntf.random.set_seed(51)\nnp.random.seed(51)\n\ndeepNN.define_NN(10)\ndeepNN.trainModel(learningRate = 1e-7, numEpochs = 500)\ndeepNN.plotLoss([0,300, 0, 40])","5ee1d2be":"deepNN.forecast()\ndeepNN.plotForecast()","91fe8887":"addMetrics(metricsDF, modelName= \"DNN\", forecast=deepNN.forecastResults)\nmetricsDF","14fbb447":"class RNN():\n    def __init__(self, timeSeries, train, test, window_size, batch_size):\n        self.ts = timeSeries\n        self.train = train\n        self.test = test\n        self.window_size = window_size\n        self.batch_size = batch_size\n\n\n    def define_NN(self, layerSize):\n        self.model = tf.keras.models.Sequential([\n            tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1),\n                                input_shape=[None]),\n            tf.keras.layers.SimpleRNN(layerSize, return_sequences=True),\n            tf.keras.layers.SimpleRNN(layerSize),\n            tf.keras.layers.Dense(1),\n            tf.keras.layers.Lambda(lambda x: x * 100.0)\n            ])\n\n\n    def gen_win_dataset(self, series):\n        ds = tf.data.Dataset.from_tensor_slices(series.values.reshape(-1))\n        ds = ds.window(self.window_size + 1, shift=1, drop_remainder=True)\n        ds = ds.flat_map(lambda window: window.batch(self.window_size + 1))\n        ds = ds.shuffle(1000).map(lambda window: (window[:-1], window[-1]))\n        ds = ds.batch(self.batch_size).prefetch(1)\n        return ds\n\n    \n    def findOptimalLR(self):\n        lr_schedule = tf.keras.callbacks.LearningRateScheduler(\n                lambda epoch: 1e-8 * 10**(epoch \/ self.window_size))\n        optimizer = tf.keras.optimizers.SGD(lr=1e-8, momentum=0.9)\n        self.model.compile(loss=tf.keras.losses.Huber(),\n                            optimizer=optimizer,\n                            metrics=[\"mae\"])\n        ts_dataset = self.gen_win_dataset(self.train)\n        self.LRfinderHistory = self.model.fit(ts_dataset, epochs=100, callbacks=[lr_schedule], verbose=0)\n\n    \n    def plotOptimalLoss(self, axis):\n        lrs = 1e-8 * (10 ** (np.arange(100) \/ self.window_size))\n        plt.semilogx(lrs, self.LRfinderHistory.history[\"loss\"])\n        plt.axis(axis)\n        plt.show()\n\n    \n    def trainModel(self, learningRate, numEpochs, patience = 20):\n        Early_stop_cb = tf.keras.callbacks.EarlyStopping(monitor = 'loss' ,patience= patience)\n        optimizer = tf.keras.optimizers.SGD(lr=learningRate, momentum=0.9)\n        self.model.compile(loss=tf.keras.losses.Huber(),\n                            optimizer=optimizer,\n                            metrics=[\"mae\"])\n        ts_dataset = self.gen_win_dataset(self.train)\n        self.modelHistory = self.model.fit(\n            ts_dataset,\n            epochs=numEpochs,\n            callbacks=[Early_stop_cb],\n            verbose=0\n        )\n\n    \n    def plotLoss(self, axis):\n        loss = self.modelHistory.history['loss']\n        epochs = range(len(loss))\n        plt.plot(epochs, loss, label='Training Loss')\n        plt.axis(axis)\n        plt.show()\n\n        \n    def forecast(self):\n        #Note: forecast is made using ALL timeseries but return only the correspoing to test set\n        data_array = self.ts.values.reshape(-1)\n        forecast = []\n        for time in range(len(data_array)-self.window_size):\n            pred_array = data_array[time:time + self.window_size][np.newaxis]\n            forecast.append(self.model.predict(pred_array))\n\n        forecast = forecast[2400-self.window_size:]\n        self.forecastResults = np.array(forecast)[:, 0, 0]\n\n\n    def plotForecast(self):\n        plt.figure(figsize=(16,8))\n        plt.xlabel('Date')\n        plt.ylabel('Stock Price')\n        plt.plot(self.test)\n        plt.plot(self.test.index, self.forecastResults)\n        plt.legend(['Series', 'Forecast'])\n        plt.grid()\n        plt.show()\n        ","4a6969b3":"recurrentNN = RNN(\n    timeSeries = ts,\n    train = train,\n    test = test,\n    window_size = 20,\n    batch_size = 32\n)","ef42fc40":"tf.keras.backend.clear_session()\ntf.random.set_seed(51)\nnp.random.seed(51)\n\nrecurrentNN.define_NN(layerSize=40)\nrecurrentNN.findOptimalLR()\nrecurrentNN.plotOptimalLoss([1e-8, 1e-2, 0, 40])","625136dc":"tf.keras.backend.clear_session()\ntf.random.set_seed(51)\nnp.random.seed(51)\n\nrecurrentNN = RNN(\n    timeSeries = ts,\n    train = train,\n    test = test,\n    window_size = 20,\n    batch_size = 32\n)\n\nrecurrentNN.define_NN(layerSize=40)\nrecurrentNN.trainModel(learningRate=1e-8, numEpochs=1000)\nrecurrentNN.plotLoss([0,800, 0, 50])","fb858393":"recurrentNN.forecast()\nrecurrentNN.plotForecast()","f3d24d38":"addMetrics(metricsDF, 'RNN', recurrentNN.forecastResults)\nmetricsDF","fbfbd413":"class LSTM():\n    def __init__(self, timeSeries, train, test, window_size, batch_size):\n        self.ts = timeSeries\n        self.train = train\n        self.test = test\n        self.window_size = window_size\n        self.batch_size = batch_size\n\n\n    def define_NN(self, layerSize):\n        self.model = tf.keras.models.Sequential([\n                tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1),\n                                    input_shape=[None]),\n                tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(layerSize, return_sequences=True)),\n                tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(layerSize)),\n                tf.keras.layers.Dense(1),\n                tf.keras.layers.Lambda(lambda x: x * 100.0)\n            ])\n\n\n    def gen_win_dataset(self, series):\n        ds = tf.data.Dataset.from_tensor_slices(series.values.reshape(-1))\n        ds = ds.window(self.window_size + 1, shift=1, drop_remainder=True)\n        ds = ds.flat_map(lambda window: window.batch(self.window_size + 1))\n        ds = ds.shuffle(1000).map(lambda window: (window[:-1], window[-1]))\n        ds = ds.batch(self.batch_size).prefetch(1)\n        return ds\n\n    \n    def findOptimalLR(self):\n        lr_schedule = tf.keras.callbacks.LearningRateScheduler(\n                lambda epoch: 1e-8 * 10**(epoch \/ self.window_size))\n        optimizer = tf.keras.optimizers.SGD(lr=1e-8, momentum=0.9)\n        self.model.compile(loss=tf.keras.losses.Huber(),\n                            optimizer=optimizer,\n                            metrics=[\"mae\"])\n        ts_dataset = self.gen_win_dataset(self.train)\n        self.LRfinderHistory = self.model.fit(ts_dataset, epochs=100, callbacks=[lr_schedule], verbose=0)\n\n    \n    def plotOptimalLoss(self, axis):\n        lrs = 1e-8 * (10 ** (np.arange(100) \/ self.window_size))\n        plt.semilogx(lrs, self.LRfinderHistory.history[\"loss\"])\n        plt.axis(axis)\n        plt.show()\n\n    \n    def trainModel(self, learningRate, numEpochs, patience = 20):\n        Early_stop_cb = tf.keras.callbacks.EarlyStopping(monitor = 'loss' ,patience= patience)\n        optimizer = tf.keras.optimizers.SGD(lr=learningRate, momentum=0.9)\n        self.model.compile(loss=tf.keras.losses.Huber(),\n                            optimizer=optimizer,\n                            metrics=[\"mae\"])\n        ts_dataset = self.gen_win_dataset(self.train)\n        self.modelHistory = self.model.fit(\n            ts_dataset,\n            epochs=numEpochs,\n            callbacks=[Early_stop_cb],\n            verbose=0\n        )\n\n    \n    def plotLoss(self, axis):\n        loss = self.modelHistory.history['loss']\n        epochs = range(len(loss))\n        plt.plot(epochs, loss, label='Training Loss')\n        plt.axis(axis)\n        plt.show()\n\n        \n    def forecast(self):\n        #Note: forecast is made using ALL timeseries but return only the correspoing to test set\n        data_array = self.ts.values.reshape(-1)\n        forecast = []\n        for time in range(len(data_array)-self.window_size):\n            pred_array = data_array[time:time + self.window_size][np.newaxis]\n            forecast.append(self.model.predict(pred_array))\n\n        forecast = forecast[2400-self.window_size:]\n        self.forecastResults = np.array(forecast)[:, 0, 0]\n\n\n    def plotForecast(self):\n        plt.figure(figsize=(16,8))\n        plt.xlabel('Date')\n        plt.ylabel('Stock Price')\n        plt.plot(self.test)\n        plt.plot(self.test.index, self.forecastResults)\n        plt.legend(['Series', 'Forecast'])\n        plt.grid()\n        plt.show()","bed24a43":"longSTMNN = LSTM(\n    timeSeries = ts,\n    train = train,\n    test = test,\n    window_size = 20,\n    batch_size = 32\n)","a761d033":"tf.keras.backend.clear_session()\ntf.random.set_seed(51)\nnp.random.seed(51)\n\nlongSTMNN.define_NN(layerSize=32)\nlongSTMNN.findOptimalLR()\nlongSTMNN.plotOptimalLoss([1e-8, 1e-2, 0, 40])","e4faea23":"tf.keras.backend.clear_session()\ntf.random.set_seed(51)\nnp.random.seed(51)\n\nlongSTMNN = LSTM(\n    timeSeries = ts,\n    train = train,\n    test = test,\n    window_size = 20,\n    batch_size = 32\n)\n\nlongSTMNN.define_NN(layerSize=32)\nlongSTMNN.trainModel(learningRate=1e-6, numEpochs=500)\nlongSTMNN.plotLoss([0,500, 0, 50])","f52f81ca":"tf.keras.backend.clear_session()\ntf.random.set_seed(51)\nnp.random.seed(51)\n\nlongSTMNN = LSTM(\n    timeSeries = ts,\n    train = train,\n    test = test,\n    window_size = 20,\n    batch_size = 32\n)\n\nlongSTMNN.define_NN(layerSize=32)\nlongSTMNN.trainModel(learningRate=1e-7, numEpochs=500)\nlongSTMNN.plotLoss([0,500, 0, 50])","5152e476":"longSTMNN.forecast()\nlongSTMNN.plotForecast() ","2a771f98":"addMetrics(metricsDF, 'LSTM', longSTMNN.forecastResults)\nmetricsDF","e9cfa1b0":"## Recurrent Neural Network\n\nA Recurrent Neural Network, or RNN is a neural network that contains recurrent layers.These are designed to sequentially processes sequence of inputs.\n\n\nWith an RNN, you can feed it in batches of sequences, and it will output a batch of forecasts.\n\n\n![image.png](attachment:2fb1066c-165f-453d-9049-72652d040e14.png)\n\n\nWhat it looks like there's lots of cells, there's actually only one, and it's used repeatedly to compute the outputs. \n\nAt each time step, the memory cell takes the input value for that step.","22c20a38":"<a id=\"data-load\"><\/a>\n## Data Load \n","9e9dde44":"## Table of Contents\n\n\n* [Data Load](#data-load)\n* [Basic Forecasting](#basic-forecasting)\n    - [Moving average](#mov-avg)\n    - [Moving average with differentiation](#mov-avg-diff)\n* [Advanced Forecasting](#Advanced-forecast)\n    - [Deep Neural Network](#NN)\n    - [Recurrent NN](#RNN)\n    - [Long Short Term Memory NN](#LSTM)\n\n","1d0f43bd":"Seems a more stable convergence, let's check the performance on the tests set:","c7041674":"We have improve a lot with respect to the previous MA forecasting!  \n\n**Note**: in the previous graph with the differentiated values we can see the presence of heteroscedasticity in the data (that is, non constant variance). This is a tipical behaviour in Stock time series.   ","1a850bc6":"Seems like we are getting much higher loss with RNN, lets pick a learning rate of 8e-8 and train it for a higher numb of epochs","6745d793":"<a id=\"mov-avg-diff\"><\/a>\n## Moving Average Forecast with differentiation\n\nLet's try to improve last result by taking into account the trends on the data.\nIn order todo so, we will apply the moving average over the differentiated series, and the give back the trends by adding the past lags to the series.\n\nApplying differentiation to the series we remove trends in the data, as we can see on the following graphic, which contains the moving average forecast over the differentiated series:","4db11b92":"<a id=\"Advanced-forecast\"><\/a>\n# Advanced Forecasting\n\n<a id=\"NN\"><\/a>\n## Deep Neural Network model\n\nNext we will forecast our time series using a classic depp NN model with fully connected layers.\n\nFirst, we need to transform our data into a dataset wich can be feed into the model. In order to do that, we will take as input variables 'windows' of data from the series, and the following value on the series as the target variable.\n\nAfter that, we will define the model (choose number and size of layers, activation funcions, etc..) and find the optimal learning rate. \n\n**Note**: *To make code more readable, we will group all functions one single class*\n ","aaee63c2":"Optimal learning rate seems to be at around 2e-6, so let's train our model for longer time with that learning rate:","7f35e411":"Optimal Lr given before seems too unstable, let's try with a lower one.","ecfed96a":"We are reaching plateau at around 3 *mae* with the current parameters altought we reduce the learing rate. \n\n","82cf21f0":"<a id=\"basic-forecasting\"><\/a>\n# Basic forecasting\n\nIn order to start making some forecasts (not complex ones), first we need to train\/test split our data.\nWe'll take the first 2400 registers as train set and the rest of the series as test set, wich give us approximatelly an 80\/20 ratio.","4295c2ce":"## LSTM Neural Network\n\nIn RNN the recurrent cells calculate y's as outputs as well as a state vector, which that fed into the cell along with the next X.\n\nThe impact of this is that while state is a factor in subsequent calculations, its impacts can diminish greatly over timestamps. \nIn order to avoid this, LSTM add a cell state to the layer architecture of RNN.  \n\n![image.png](attachment:2ca9c3ec-8077-44c0-80b7-f6338407959e.png)","ab31f722":"Altought we supposed that 2e-6 was the optimal learning rate, loss function shows some inestability on gradien descent process. Let's check if a lower learning rate could give us better results:","bb608174":"<a id=\"mov-avg\"><\/a>\n## Forecasting with moving average\n\nMoving average is a useful tool when analyzing time series. It can give us some hints about seasonal and\/or trend behaviour in the series, especially in really noisy ones.\n\nWe can also use it to forecast values of a time series. Personally I think is a bit of a 'naive' forecasting, but we can get surprisingly good rosults in some cases.\n\nWe will use the *simple moving average* over the previous k data point for this purpose, wich has the following equation:\n$$ SMA_k = \\frac{p_{n-k+1}+p_{n-k+2}+\\ldots +p_n}{k} = \\frac{1}{k} \\sum_{i = n-k+1}^n p_i $$","1f042838":"And giving back the trend we get the following.","d6751af9":"We obtain metrics really similar to the Moving Average approximation! Maybe with some fine-tuning of the NN layers we could improve this metrics but, for now, let's move onto the next model.","73ad4bc7":"We have improve our metrics with respect to the DNN model, however we had to train longer our model and the computation time was also higher per epoch.\n\nWe could try to improve this metrics by fine-tunnig the model but, for now, let's move into the next model!"}}