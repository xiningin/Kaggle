{"cell_type":{"b293b3a5":"code","ce6e40c2":"code","8317e6cd":"code","aa47c446":"code","5bf6b636":"code","ef8a0bc7":"code","96f7c141":"code","1189eef2":"code","1452e5a4":"code","2858f1ed":"code","b15c1b70":"code","b6ef5f5c":"code","269d6c26":"code","efb21e04":"code","43428de7":"code","0713c27b":"code","6914e64a":"code","1fb72904":"code","485c80d6":"code","0fe9c00e":"code","365e5e48":"code","4dccf77d":"code","d3f78b40":"code","caac4e7b":"code","113e1f44":"code","dc9e5c03":"code","96df5a55":"code","02eab8a2":"code","3676aa9f":"code","ea77be46":"markdown","99ff6bbc":"markdown","c949c247":"markdown","4050a1a1":"markdown","b6451bc2":"markdown","409cb970":"markdown","4436721a":"markdown","73a9ea64":"markdown","fae92df7":"markdown","23da51b2":"markdown","11a0d9ec":"markdown","00e503c0":"markdown","3ad82e42":"markdown","2e91b700":"markdown"},"source":{"b293b3a5":"# getting started with the model \n# importing required libraries\/packages \n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#import time for training details\nfrom time import time\nt0 = time()\n\nimport warnings\nwarnings.filterwarnings('ignore')","ce6e40c2":"# Importing and Reading the Dataset\ndf_wine= pd.read_csv('..\/input\/winequalitywhitecsv\/winequality-white.csv')","8317e6cd":"df_wine_row_count, df_wine_column_count=df_wine.shape\nprint('Total number of rows:', df_wine_row_count)\nprint('Total number of columns:', df_wine_column_count)","aa47c446":"# check to see if there are any missing entries\ndf_wine.info()","5bf6b636":"df_wine.head().iloc[:5]","ef8a0bc7":"df_wine.isna().sum()","96f7c141":"print (\"Unique values are:\\n\",df_wine.nunique())","1189eef2":"#checking Datatypes\ndf_wine.dtypes","1452e5a4":"#correlation map for features\nf,ax = plt.subplots(figsize=(9, 9))\nax.set_title('Correlation map for variables')\nsns.heatmap(df_wine.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax,cmap=\"PuRd\")","2858f1ed":"#Getting an idea about the distribution of wine quality \np = sns.countplot(data=df_wine, x = 'quality', palette='muted')","b15c1b70":"df_wine['quality'].describe()","b6ef5f5c":"#'fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar',\n#'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density',\n# 'pH', 'sulphates', 'alcohol', 'quality'\n#Getting an idea about the distribution of wine quality \n\np = sns.barplot(data=df_wine, x = 'quality',y='alcohol', palette='muted')","269d6c26":"p = sns.barplot(data=df_wine, x = 'quality',y='volatile acidity',palette='muted')","efb21e04":"#Grouping the wine based on grade\n# Defining 'grade' of wine\n\n#Good wine\ndf_wine['grade'] = 1 \n\n#Bad wine\ndf_wine.grade[df_wine.quality < 6.5] = 0 \n\n#sns.set(style=\"whitegrid\")\n#p = sns.countplot(data=df_wine, x='grade', palette='muted')\n\n#set plotsize and colors\n\nplt.figure(figsize = (6,6))\ncolors = ['lightcoral', 'rosybrown']\n\nlabels = df_wine.grade.value_counts().index\nexplode = (0.05,0.05)\nplt.pie(df_wine.grade.value_counts(), autopct='%1.1f%%',colors=colors,explode = explode)\ncentre_circle = plt.Circle((0,0),0.70,fc='white')\nfig = plt.gcf()\nfig.gca().add_artist(centre_circle)\nplt.legend(labels, loc=\"Best\")\nplt.axis('equal')\nplt.title('White Wine Quality Distribution')\nplt.show()\n#Show mean quality of white wine and quality distribution\n\nprint('The amount of good quality white wine is ',round(df_wine.grade.value_counts(normalize=True)[1]*100,1),'%.')\nprint(\"mean white wine quality = \",df_wine[\"quality\"].mean())","43428de7":"# plot to see how pH is varying in the grade of white wine\n\nplt.figure(figsize=(6,6))\nax = sns.lineplot(x=\"pH\", y=\"quality\", hue=\"grade\", data=df_wine,markers=True)","0713c27b":"df_wine['grade'].value_counts() #prints counts of good and bad white wine","6914e64a":"#Checking once more for column names\ndf_wine.columns","1fb72904":"#Defining X and y\nX = df_wine.drop(['quality'], axis=1)\ny = df_wine['quality']","485c80d6":"# creating dataset split for prediction\nfrom sklearn.model_selection import train_test_split\nX_train, X_test , y_train , y_test = train_test_split(X,y,test_size=0.2,random_state=42) # 80-20 split\n\n# Checking split \nprint('X_train:', X_train.shape)\nprint('y_train:', y_train.shape)\nprint('X_test:', X_test.shape)\nprint('y_test:', y_test.shape)","0fe9c00e":"# 1. Using Random Forest Classifier\nt0 = time()\n# Load random forest classifier \nfrom sklearn.ensemble import RandomForestClassifier\n\n# Create a random forest Classifier\nclf = RandomForestClassifier(n_jobs=2, random_state=0)\n\n# Train the Classifier\/fitting the model\nclf.fit(X_train, y_train)\n\n# predict the response\ny_pred = clf.predict(X_test)\nacc_rf = round(clf.score(X_test,y_test) * 100, 2)\n#Import scikit-learn metrics module for accuracy calculation\nfrom sklearn import metrics\n\n# evaluate accuracy\nprint(\"Random Forest Classifier Accuracy:\",metrics.accuracy_score(y_test, y_pred)*100,\"%\")\nprint('Training time', round(time() - t0, 3), 's')","365e5e48":"#2. Gaussian Naive Bayes Classifier\nt0 = time()\n#Import Gaussian Naive Bayes model\nfrom sklearn.naive_bayes import GaussianNB\n\n#Create a Gaussian Classifier\ngnb = GaussianNB()\n\n# Train the Classifier\/fitting the model\ngnb.fit(X_train, y_train)\n\n# predict the response\ny_pred = gnb.predict(X_test)\nacc_gnb = round(gnb.score(X_test,y_test) * 100, 2)\n\n#Import scikit-learn metrics module for accuracy calculation\nfrom sklearn import metrics\n\n# evaluate accuracy\nprint(\"Naive Bayes Accuracy:\",metrics.accuracy_score(y_test, y_pred)*100,\"%\")\nprint('Training time', round(time() - t0, 3), 's')","4dccf77d":"#import Decision Tree Classifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree\n\n# Create Decision Tree classifer object\nclf = DecisionTreeClassifier(max_depth=10)\n\n# Train the Classifier\/fitting the model\nclf = clf.fit(X_train,y_train)\n\n# predict the response\ny_pred = clf.predict(X_test)\nacc_dt = round(clf.score(X_test,y_test) * 100, 2)\n#Import scikit-learn metrics module for accuracy calculation\nfrom sklearn.metrics import accuracy_score \n\n# evaluate accuracy\nprint (\"Decision Tree Accuracy:\", metrics.accuracy_score(y_test, y_pred)*100,\"%\")\nprint('Training time', round(time() - t0, 3), 's')","d3f78b40":"#kNN\nimport sys, os\n\n# Import kNN classifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# instantiate learning model (k = 3)\nknn = KNeighborsClassifier(n_neighbors=3)\n\n# Train the Classifier\/fitting the model\nknn.fit(X_train, y_train)\n\n# predict the response\ny_pred = knn.predict(X_test)\nacc_knn = round(knn.score(X_test,y_test) * 100, 2)\n#Import scikit-learn metrics module for accuracy calculation\nfrom sklearn import metrics\n\n# evaluate accuracy\nprint(\"kNN Accuracy:\",metrics.accuracy_score(y_test, y_pred)*100,\"%\")\nprint('Training time', round(time() - t0, 3), 's')","caac4e7b":"#Support Vector Machines trial\nimport sys, os\n\n#Import svm model\nfrom sklearn import svm\nfrom sklearn.svm import SVC\n\n#Create a svm Classifier\nclf = SVC(C=1, kernel='rbf')\n\n# Train the Classifier\/fitting the model\nclf.fit(X_train, y_train)\n\n# predict the response\ny_pred = clf.predict(X_test)\nacc_svm = round(clf.score(X_test,y_test) * 100, 2)\n\n# evaluate accuracy\nprint(\"SVM Accuracy:\",metrics.accuracy_score(y_test, y_pred)*100,\"%\")\nprint('Training time', round(time() - t0, 3), 's')","113e1f44":"# visualizing accuracies for all ML Algorithms using Matplotlib\npredictors_group = ('Random Forest', 'GaussianNB', 'DecisionTree','kNN','SVM')\nx_pos = np.arange(len(predictors_group))\naccuracies1 = [acc_rf, acc_gnb, acc_dt,acc_svm,acc_knn]\n    \nplt.bar(x_pos, accuracies1, align='center', alpha=0.5, color='purple')\nplt.xticks(x_pos, predictors_group, rotation='vertical')\nplt.ylabel('Accuracy (%)')\nplt.title('Classifier Accuracies')\nplt.show()","dc9e5c03":"#printing top three accuracies\n\nprint('Decision Tree:', acc_dt,'%')\nprint('Random Forest:', acc_rf,'%')\nprint('GaussianNB:',acc_gnb,'%')","96df5a55":"# importing the model for prediction\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\n# creating list of tuple wth model and its name  \nmodels = []\nmodels.append(('DT',DecisionTreeClassifier()))\nmodels.append(('RF',RandomForestClassifier()))\nmodels.append(('GNB',GaussianNB()))","02eab8a2":"# Import Cross Validation \nfrom sklearn.model_selection import cross_val_score\n\n# simulate splitting a dataset of 1000 observations into 5 folds\nfrom sklearn.model_selection import KFold\nkf = KFold(n_splits=5, random_state=42, shuffle=True)\nkf.get_n_splits(X)\n# print(kf)\n\nacc = []   # All Algorithm\/model accuracies\nnames = []    # All model name\n\nfor name, model in models:\n    \n    acc_of_model = cross_val_score(model, X_train, y_train, cv=kf, scoring='accuracy') # kFolds =5 without shuffling\n    \n    acc.append(acc_of_model) # appending Accuray of different model to acc List\n    \n    names.append(name)# appending name of models\n    Acc =name,round(acc_of_model.mean()*100,2) # printing Output \n    print(Acc)","3676aa9f":"# Plotting all accuracies together for comparison\n\nlabels = ['Decision Tree', 'Random Forest','Gaussian NB']\n\nNoCV =[69.49 ,77.65,66.43] # accuracy before Cross Validation\nCV=[69.24, 75.14, 65.8] # accuracy after Cross Validation\n\nx = np.arange(len(labels))  # the label locations\nwidth = 0.25  # the width of the bars\n\nf, ax = plt.subplots(figsize=(8,6)) \np1 = ax.bar(x - width\/2, CV, width, label='After Cross Validation', color='purple')\np2 = ax.bar(x + width\/2, NoCV, width, label='Before Cross Validation', color='m')\n\n# Add some text for labels and title \nax.set_ylabel('Accuracies')\nax.set_title('Accuracy comparison')\nax.set_xticks(x)\nplt.xticks()\nax.set_xticklabels(labels)\nax.legend(loc='top right')\nplt.show()","ea77be46":"### <h3 style=\"background-color:#bf0fff;color:white;text-align: center;padding-top: 5px;padding-bottom: 5px;\"><strong><centre>Plotting all accuracies \ud83d\udcca <\/centre><\/strong><\/h3>","99ff6bbc":"### <h3 style=\"background-color:#bf0fff;color:white;text-align: center;padding-top: 5px;padding-bottom: 5px;\"><strong><centre>Importing Libraries & Packages \ud83d\udcda <\/centre><\/strong><\/h3>","c949c247":"<div style=\"background-color:#c5f9d7;color:#103783;text-align: center;padding-top: 2px;padding-bottom: 5px;\"><centre><br><strong>On comparison, the accuracies do not seem to drastically change, but do seem a bit reduced after cross validation which means that  K-fold cross validation is giving a better approximation for those accuracies.\nFrom the bar plot, Random Forest appears to be providing the better accuracies followed by Decision Tree in predicting the white wine quality. \n    Using this outcome, Random forest and Decision Tree can be applied to the dataset.<\/strong><\/centre><\/div>","4050a1a1":"<blockquote>\ud83d\udcccThe purpose of this notebook is to check the application of learned machine learning techniques of Data Exploration, Algorithm selection to an available Dataset for Learning & Educational purpose.<\/blockquote>","b6451bc2":"### <h3 style=\"background-color:#bf0fff;color:white;text-align: center;padding-top: 5px;padding-bottom: 5px;\"><strong><centre>Dataset split for prediction \u23f3 <\/centre><\/strong><\/h3>","409cb970":"<blockquote>\ud83d\udccc Note: From the figure & code above, it appears that target variable \"quality\" (range from 3 to 9), values 5 and 6 make up more than 60% of the dataset indicating that this class is unbalanced.<\/blockquote>","4436721a":"<blockquote>\ud83d\udccc This model uses the famous Wine Quality Data Set provided by UCI Repository.\n\nDataset downloaded from UCI Repository at https:\/\/archive.ics.uci.edu\/ml\/datasets\/Wine+Quality\n\n\nSource:\n\nPaulo Cortez, University of Minho, Guimar\u00e3es, Portugal, http:\/\/www3.dsi.uminho.pt\/pcortez\nA. Cerdeira, F. Almeida, T. Matos and J. Reis, Viticulture Commission of the Vinho Verde Region(CVRVV), Porto, Portugal @2009\n    \nUsing White Wine Dataset for model training and validation\n<\/blockquote>","73a9ea64":"Note: Algorithm training times are checked only for information purpose and not used anywhere else in the model.","fae92df7":"<div style=\"background-color:#c5f9d7;color:#103783;text-align: center;padding-top: 2px;padding-bottom: 2px;\"><centre><br><strong>Now, Predicting White Wine Quality using:<\/strong><br>\n    1. Random Forest Classifier<br>\n    2. Gaussian Naive Bayes Classifier<br>\n    3. Decision Tree Classifier<br>\n    4. KNeighbors Classifier<br>\n    5. SVM Classifier <\/centre><\/div>","23da51b2":"![White Wine Quality Testing(1).jpg](attachment:93058c70-23f3-4e5f-8b8c-6c40fbd337d2.jpg)","11a0d9ec":"### <h3 style=\"background-color:#bf0fff;color:white;text-align: center;padding-top: 5px;padding-bottom: 5px;\"><strong><centre>Data Exploration for the Dataset \ud83d\udd0d <\/centre><\/strong><\/h3>","00e503c0":"### <h3 style=\"background-color:#bf0fff;color:white;text-align: center;padding-top: 5px;padding-bottom: 5px;\"><strong><centre>Checking for missing values \u270f\ufe0f <\/centre><\/strong><\/h3>","3ad82e42":"### <h3 style=\"background-color:#bf0fff;color:white;text-align: center;padding-top: 5px;padding-bottom: 5px;\"><strong><centre>Reading the dataset \ud83d\udcdd <\/centre><\/strong><\/h3>","2e91b700":"### <h3 style=\"background-color:#bf0fff;color:white;text-align: center;padding-top: 5px;padding-bottom: 5px;\"><strong><centre>\ud83d\udcc9 Reducing overfitting using Cross Validation for top three Algorithms -> RF, DT and GNB <\/centre><\/strong><\/h3>"}}