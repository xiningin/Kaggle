{"cell_type":{"b391aabf":"code","047f71d7":"code","0831e491":"code","89952750":"markdown","30d42076":"markdown","8a23f6a9":"markdown","d5d63174":"markdown","3c5c8005":"markdown"},"source":{"b391aabf":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn import linear_model\nfrom numpy.linalg import inv,det,multi_dot,norm\nfrom sklearn.metrics import confusion_matrix\nfrom itertools import combinations\nimport warnings\nwarnings.filterwarnings('ignore')","047f71d7":"class LinearRegression:  \n    def __init__(self):\n        self.x=None\n        self.y=None        \n        self.weights = None\n        self.bias = None\n        self.cost= [ ]        \n        \n    def Error(self,x,y):\n        r,c=x.shape\n        x=np.array(x)\n        y=np.array(y).reshape(r,1)\n        for i in range(len(self.bias)):\n            fx = np.dot(x, self.weights[i]) + self.bias[i]      \n        \n        \n        MSE=(1\/len(x))* np.sum(np.square(fx - y))       \n        return MSE   \n\n    def kfold_linear(self,x,y):\n        kf = KFold(n_splits=5) \n        x=np.array(x)\n        y=np.array(y)      \n        \n        MinErr=[ ]\n        alpha_range = [10**i for i in range(-6,3)]\n        for j in alpha_range :\n            l=[ ]\n            for train_index,test_index in kf.split(x,y):\n                #print(x.size,y.size)\n                self.Linear_Train(x,y,j,2000,10**+100)                \n                pred= self.Error(x[test_index],y[test_index])\n                l.append(pred)\n            MinErr.append(sum(l)\/len(l))\n            print(f\"Learning rate: {j} mean error is : ---{sum(l)\/len(l)} \")\n        \n        k= np.argmin(MinErr)        \n        optimal_alpha=(alpha_range[k])\n        print(\" \")\n        print(\"optimal Learning rate is-->{}\".format(optimal_alpha))\n        return optimal_alpha\n   \n    \n    def Linear_Train(self,x,y,alpha,itr,eps):  \n        r,c=x.shape\n        x=np.array(x)\n        y=np.array(y).reshape(r,1)\n        \n        w=np.zeros((c,1))\n        w_list=[w]\n        bias=[ ]\n        cost_list=[ ]\n        #for hyperplane parameters\n        self.weights={ }\n        self.bias={ }  \n                    \n        w0=0\n        if (r>=c):\n                                       #Gradient Decent\n            for i in range(itr):\n                fx = np.dot(x,w)+ w0            \n                Err = fx - y  \n                w = w - (alpha)* np.dot(x.T,Err)            \n                w0 = w0 - alpha * np.sum(Err)            \n                cost = 0.5 * (1\/r) *np.sum(np.square(Err))         \n                cost_list.append(cost)            \n                w_list.append(w)\n                bias.append(w0)\n                if cost > eps:\n                    break \n            self.weights[0]=w\n            self.bias[0]=w0            \n            self.cost=cost_list\n            \n        elif(r<c):    \n                                      #Lagrangian_method             \n            z = np.ones(r)\n            x=np.hstack((z,x))\n            Xtrans=np.transpose(x)                       \n            a=np.dot(Xtrans,inv(np.dot(x,Xtrans)) )\n            W=np.dot(a, y)\n            self.weights={ }\n            self.bias={ }  \n                    #for hyperplane parameters\n                  \n            self.weights[0]=W[1,:]\n            self.bias[0]=W[0]   \n        \n        \n    def Linear_Test(self,x,y):\n        r,c=x.shape\n        x=np.array(x)\n        y=np.array(y).reshape(r,1)\n        for i in range(len(self.bias)):\n            z= np.dot(x, self.weights[i]) + self.bias[i] \n            \n        MSE=(1\/len(x))* np.sum(np.square(z - y))            \n        print(\"\")\n        print('Mean Square Error is-->{}'.format(MSE))","0831e491":"  def plot_decison_boundary(x_test,y_test,weight,bias):\n        r,c = x_test.shape\n        if(c<2):           \n            #  2D hyperplane plotting\n            x=x_test.iloc[:,0]            \n            x=np.array(x).reshape(r,1)           \n            fig=plt.figure(figsize=(8, 6))\n            for i in range(len(bias)):\n                y_cal= bias[i]+ x @ weight[i]                \n                print(\"-----------------------Plotting Hyperplane---------------------------------\")\n                plt.plot(x,y_cal)\n            \n            plt.scatter(x_test, y_test, color= 'y' ,label=\"Actual data\")\n            plt.xlabel('x_1',fontsize = 10)\n            plt.ylabel('x_2',fontsize = 10)\n            plt.title(\"Hyperplane\")\n            plt.legend(loc='best')\n            plt.show()\n            \n            \n        elif(c==2):\n            # 3D hyperplane plotting\n            fig=plt.figure(figsize=(8, 6))\n            ax = fig.add_subplot(111, projection = '3d')                              \n            x11=x_test.iloc[:,0] \n            \n            x11=np.array(x11).reshape(r,1)             \n            x21=x_test.iloc[:,1]\n            x21=np.array(x21).reshape(r,1) \n            y = np.array(y_test).reshape(r,1) \n            \n            tmp = np.linspace(-40,40,3)\n            for i in range(len(bias)):\n                x1,x2 = np.meshgrid(tmp,tmp)\n                z = lambda x1,x2: (bias[i]+(x11*weight[i][0])+(x21*weight[i][1]))\n                ax.plot_surface(x11, x21, z(x1, x2))            \n            ax.scatter3D(x11, x21, y, color= 'y' ,label=\"Actual data\")\n            ax.set_xlabel('X1',fontsize = 10)\n            ax.set_ylabel('X2',fontsize = 10)\n            ax.set_zlabel('X3',fontsize = 10)\n            plt.title(\"Hyperplane\")\n            plt.legend(loc='best')\n            plt.show()\n            \n        else:\n            #plot is in higher dimenstion\n            pass","89952750":"# Importing Liberaries","30d42076":"# Plotting Linear Regression Hyperplane (2D\/3D)","8a23f6a9":"![simple_regression](http:\/\/rasbt.github.io\/mlxtend\/user_guide\/regressor\/LinearRegression_files\/simple_regression.png)","d5d63174":"# Model Class LinearRegression","3c5c8005":"# Linear Regression\nLinear regression attempts to model the relationship between two variables by fitting a linear equation to observed data. One variable is considered to be an explanatory variable,and the other is considered to be a dependent variable. For example, a modeler might want to relate the weights of individuals to their heights using a linear regression model."}}