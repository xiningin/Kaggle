{"cell_type":{"cf0744e7":"code","2e85c547":"code","a0b1c279":"code","b0694f7c":"code","164cbb71":"code","edfb3700":"code","e8b02d8a":"code","db842e05":"code","eca1d225":"code","9ee694d5":"code","37e9347f":"code","c5dcf21c":"code","2cc13952":"markdown","ed8af9ae":"markdown","251bfe49":"markdown","fa4dc585":"markdown","495c9a3a":"markdown","01f28bc8":"markdown","c4e82e16":"markdown","fee8ade9":"markdown","a59b5f57":"markdown","2b1aaf4f":"markdown","535ff2d9":"markdown","cb0c4386":"markdown","ed11654d":"markdown","e3f0cb94":"markdown"},"source":{"cf0744e7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import plot_roc_curve\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.neighbors import KNeighborsClassifier\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2e85c547":"df=pd.read_csv(r'\/kaggle\/input\/heart-attack-analysis-prediction-dataset\/heart.csv')\ndf.head()","a0b1c279":"print(df.isna().sum())","b0694f7c":"print(df.duplicated().sum())","164cbb71":"df.drop_duplicates(inplace=True)\nprint(df.duplicated().sum())","edfb3700":"sns.set_theme()\nsns.pairplot(df,hue='output')\nplt.legend()","e8b02d8a":"plt.figure(figsize=(13,13))\nsns.heatmap(df.corr(),color='red',annot=True)","db842e05":"fig,axes=plt.subplots(2,2,figsize=(20,20))\n\nsns.kdeplot(ax=axes[0,0],x='slp',hue='output',data=df)\nwidths=[2,2]\ng=sns.barplot(ax=axes[0,1],y='thalachh',x='output',hue='output',data=df)\ng.legend(loc='center')\n\nsns.countplot(ax=axes[1,0],x='cp',hue='output',data=df)\n\nsns.swarmplot(ax=axes[1,1],y='oldpeak',x='output',hue='output',data=df)","eca1d225":"Scaleme= StandardScaler()\nfeatures=df.drop(columns='output')\noutput=df['output']\nX_train, X_test, y_train, y_test = train_test_split(features, output, test_size = 0.2, random_state = 42)\nX_train=Scaleme.fit_transform(X_train)\nX_test=Scaleme.transform(X_test)\nClassifier=LogisticRegression(random_state=45)\nmodel=Classifier.fit(X_train,y_train)\ny_pred=Classifier.predict(X_test)\n\n","9ee694d5":"print(\"The accuracy of the Logistic Regression model is : \", accuracy_score(y_test, y_pred.round())*100, \"%\")\ndef cmcrcheck(X_test,y_test,y_pred,model):\n    \n    print(classification_report(y_test,y_pred))\n    cm= confusion_matrix(y_test,y_pred)\n    cmdf=pd.DataFrame(index=[0,1],columns=[0,1],data=cm)\n    fig,axes=plt.subplots(figsize=(5,5))\n    g=sns.heatmap(cmdf,annot=True,cmap='Greens',fmt='.0f',ax=axes,cbar=False)\n    g.set_xlabel('Predicted Value')\n    g.set_ylabel('True Value')\n   \n    plot_roc_curve(model,X_test,y_test)\n    plt.show()\n    \n    \ncmcrcheck(X_test,y_test,y_pred,model)\n\n    ","37e9347f":"model = KNeighborsClassifier(n_neighbors = 7)\n  \nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n  \n\nfrom sklearn.metrics import classification_report, confusion_matrix\nprint(confusion_matrix(y_test, y_pred))\n\nprint(\"The accuracy of KNN is : \", accuracy_score(y_test, y_pred.round())*100, \"%\")\n\ncmcrcheck(X_test,y_test,y_pred,model)\n","c5dcf21c":"id=pd.DataFrame(X_test)\noutput = pd.DataFrame({'Id': id.index,\n                       'Output': y_pred})\noutput.to_csv('submission.csv', index=False)","2cc13952":"#### Also dubbed as the lazy algorithm for its lack of emphasize on training. I can relate to this algorithm on a spiritual level lol. \n\n#### The KneighborClassifier simply takes in the number of neighbours you want it to check for and classify the new data based on its neighbors. It does so by calculting the Euclidean distance (shortest distance) between the new data and its neighbors to make out the closest ones and takes a popularity vote to classify the new data.","ed8af9ae":"#### Before we use a classification model on our features and output we need to do feature scaling to minimize any potential bias that can affect our model.\n\n![](https:\/\/datascience.foundation\/img\/pdf_images\/Equation4_Gradient-descent-update-rule.png)\n\n#### I dont mean to scare you but the 'x' in the equation implies that the gradient descent optimization technique (thats what that equation does) that is used by  many of the popular classification algorithms(including the one I am planning to use) are greatly  influenced by our feature variables. And as you can notice from our  dataset trtbps, chol and thalachh have their values in the hundreds while cp, slp, thall, just to name a few are single digits. For this reason we have to scale their  values. We can do this by either normalizing or standardising them. I've chosen to standardize them  but I'll hopefully update this notebook to show you how the results may vary if we normalize them in the coming days. ","251bfe49":"##### We notice a smoother curve the second time although covering roughly the same amount of area as our Logistic Regression model. The confusion matrix however shows a clear indication as to why the model has a higher accuracy score than the first. ","fa4dc585":"#### Lets pair these columns up and pick the really fancy ones\n","495c9a3a":"### **Variable Description**\n\n#### **Age** : Age of the patient\n\n#### **Sex** : Sex of the patient\n\n#### **Exang** : exercise induced angina (1 = yes; 0 = no)\n\n#### **Ca** : number of major vessels (0-3)\n\n#### **Cp** : Chest Pain type chest pain type\n\n* Value 1: typical angina\n* Value 2: atypical angina\n* Value 3: non-anginal pain\n* Value 4: asymptomatic\n\n#### **trtbps** : resting blood pressure (in mm Hg)\n\n#### **Chol** : cholestoral in mg\/dl fetched via BMI sensor\n\n#### **Fbs** : (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false)\n\n#### **Rest_ecg** : resting electrocardiographic results\n\n* Value 0: normal\n* Value 1: having ST-T wave abnormality (T wave inversions and\/or ST elevation or depression of > 0.05 mV)\n* Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria\n\n#### **Thalach** : maximum heart rate achieved\n\n#### **Target** : 0= less chance of heart attack 1= more chance of heart attack","01f28bc8":"#### Who doesn't love a complete dataset!\nI hope I dont mess this up","c4e82e16":"#### Any and all feedback is appreciated. Hope this was informative. Thanks for reading homie!","fee8ade9":"#### Lets drop that and move on","a59b5f57":"# Logistic Regression Model","2b1aaf4f":"# Dr. House in the House\n","535ff2d9":"#### That was a lot but now lets make a heatmap of the correaltion matrix and put 1 and 2 together to get this rolling.","cb0c4386":"### Quick Observations\n#### 1. **thalachh** values tend to be **higher** for patients who are more **likely** to have heart attacks.\n#### 2. **Non anginal chest pains** are relatively **high** for patients who are **likely** to have a heart attack\n#### 3. the **oldspeak** distribution for both probabilities of patients really **compliment** each other.","ed11654d":"# KNN Classifier","e3f0cb94":"#### looks like the variables **slp,thalach,cp and oldpeak** seem to show relatively strong positive\/negative correlation. Lets take a closer look ~~with Seth Meyers~~ "}}