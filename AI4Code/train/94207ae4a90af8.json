{"cell_type":{"85c0cb11":"code","62df583d":"code","39733998":"code","1b8bd9c6":"code","27b4974c":"code","37b09858":"code","f556cdb9":"code","f09e39c6":"code","a2449c24":"code","85fdeea3":"code","47cdc11f":"code","ef3bd613":"code","adeb8505":"code","8d58c8c7":"code","d832345b":"code","d8838746":"code","c2623153":"code","d7c2ade0":"code","4a30a0ad":"code","1e7df16c":"code","532bba5f":"code","79b075d1":"code","9ec2b7ab":"code","f9ed0dac":"markdown","b550cbcd":"markdown","29bdfca5":"markdown","2bae25af":"markdown","6315eb91":"markdown","d18929ca":"markdown","62b20940":"markdown","5b79ad54":"markdown","96f41b94":"markdown","b93e1a44":"markdown","84ca5dd4":"markdown","3595db17":"markdown","bdbc6665":"markdown","6bf0eddd":"markdown","7eb8b12a":"markdown","910dcbb9":"markdown","9f08a6e6":"markdown","fe828d0f":"markdown"},"source":{"85c0cb11":"#Import necessory libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import  confusion_matrix,classification_report,precision_recall_curve\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly \nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","62df583d":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","39733998":"# Get the CSV data here and print head\ndf = pd.read_csv(\"\/kaggle\/input\/breast-cancer\/breast cancer.csv\",index_col=0)\ndf.head()","1b8bd9c6":"#print summary\nprint ('Shape        ------>', df.shape)\nprint ('Each Column and data type and its count','\\n')\nprint ( df.info())","27b4974c":"#DROP ALERT 1 : Unnamed:32 column has all nulls. Safe to remove the column.\ndf = df.drop(['Unnamed: 32'],axis=1)","37b09858":"# Dataframe statistics\ndf.describe()","f556cdb9":"# Validate each class to understand if the dataset is imbalanced.\n\nprint ('Total Belign Tumor (B)    :  {} and its percentage is {} %'.format(df.diagnosis.value_counts()[0], round(df.diagnosis.value_counts()[0]\/df.diagnosis.value_counts().sum()*100,2)) )\nprint ('Total Malignant Tumor (M) :  {} and its percentage is {} %'.format(df.diagnosis.value_counts()[1], round(df.diagnosis.value_counts()[1]\/df.diagnosis.value_counts().sum()*100,2)) )","f09e39c6":"#Plot each class freequency\nplt.figure(figsize=(6,4))\nsns.countplot(x='diagnosis',data=df,palette='rocket')","a2449c24":"df_corr = df.corr().abs()\nmask = np.zeros_like(df_corr)\nmask[np.triu_indices_from(mask)] = True\n\nplt.figure(figsize=(25, 20))\nsns.heatmap(df_corr, mask=mask, vmin=-1, vmax=1, annot=True,\n            square=True, center=0, linewidths=.5)\nplt.tight_layout()","85fdeea3":"#Shape of df_corr (just to cross verify)\nprint ('Correlation matrix Shape =  ', df_corr.shape)","47cdc11f":"scaler = StandardScaler()\nX = df.drop('diagnosis', axis=1)\nX_scaled = scaler.fit_transform(X)","ef3bd613":"pca = PCA()\npca.fit_transform(X_scaled)","adeb8505":"plt.figure()\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('Principal Components')\nplt.ylabel('Variance Covered')\nplt.title('PCA')\nplt.show()","8d58c8c7":"pca = PCA(n_components=13)\nnew_pcomp = pca.fit_transform(X_scaled)\nprinci_comp = pd.DataFrame(new_pcomp,columns=['PC1','PC2','PC3','PC4','PC5','PC6','PC7','PC7','PC9','PC10','PC11','PC12','PC13'])\nprinci_comp","d832345b":"\ndf['diagnosis'] = df['diagnosis'].replace({'M':1,'B':0})","d8838746":"y = df['diagnosis']","c2623153":"x_train,x_test,y_train,y_test = train_test_split(princi_comp,y,test_size = 0.25, random_state= 355)\n\nclf = LogisticRegression()\nclf.fit(x_train,y_train)\ny_pred = clf.predict(x_test)","d7c2ade0":"print (classification_report(y_test,y_pred,digits=2))","4a30a0ad":"precision, recall, _ = precision_recall_curve(y_test, y_pred)\nplt.step(recall, precision, color='r', alpha=0.8,\n         where='post')\nplt.fill_between(recall, precision, step='post', alpha=0.2,\n                 color='b')\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision vs Recall')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\nplt.show()","1e7df16c":"cfm = confusion_matrix(y_test,y_pred)\ncfm","532bba5f":"tp = cfm[0,0]\nfp = cfm[0,1]\nfn = cfm[1,0]\ntn = cfm[1,1]\nprint ('True Positive  >', tp)\nprint ('False Positive >', fp)\nprint ('False Negetive >', fn)\nprint ('True Negetive  >', tn)","79b075d1":"ax=plt.subplot()\nsns.heatmap(cfm,annot=True,ax=ax,linewidths=5,linecolor='b',center=0)\nax.set_xlabel('Predicted Labels');ax.set_ylabel('True Labels')\nax.set_title('Confusion matrix')\nax.xaxis.set_ticklabels(['Belign','Malignant'])\nax.yaxis.set_ticklabels(['Belign','Malignant'])\nplt.show()","9ec2b7ab":"print ('Final prediction >', round((tp+tn)\/(len(y_pred))*100,2))","f9ed0dac":"As we can see each feature data scaled differently. Let's go ahead and scale the data","b550cbcd":"![image.jpg](attachment:image.jpg)","29bdfca5":"Using heatmap we got to know there are many features have colinearity. We are going to use PCA to pick top components which can explain maximum variance and later we use same components for training and testing. ","2bae25af":"Finally generating confusion matrix","6315eb91":"Seperating labelled data","d18929ca":"Hello Kagglers!!\n- This is my first simple kernel. Hope you will go through it and give your feedback. ","62b20940":"Let's figure out the right number of components by plotting explained_variance_ratio_","5b79ad54":"Plot heatmap for confusion matrix ","96f41b94":"Generate Classification report to find Precision,Recall and F1 score.","b93e1a44":"**2. About Data**\nAttribute Information:\n* id\n* diagnosis: M = malignant, B = benign\n* Columns 3 to 32\n\nTen real-valued features are computed for each cell nucleus:\n\n* radius: distances from center to points on the perimeter\n* texture: standard deviation of gray-scale values\n* perimeter\n* area\n* smoothness: local variation in radius lengths\n* compactness: perimeter^2 \/ area - 1.0\n* concavity: severity of concave portions of the contour\n* concave points: number of concave portions of the contour\n* symmetry\n* fractal dimension: \"coastline approximation\" - 1\nThe mean, standard error, and \"worst\" or largest (mean of the three largest values) of these features were computed for each image, resulting in 30 features. For instance, field 3 is Mean Radius, field 13 is Radius SE, field 23 is Worst Radius.","84ca5dd4":"Plot Precision vs Recall curve","3595db17":"Around 13 Principal Components are able to explain > 95 % variance. It's safe to pick starting 13 PC's.\n* Let's create those 13 components for further analysis.","bdbc6665":"Our model is able to predict with 97.9% accuracy of the test data. You can use many other techniques to find the best hyperparams to improve the accuracy.\n\nThank you so much for going through this report. I have been thinking to start my journey in kaggle from last few months, finally I am here.This is my first ever Kernel on Kaggle. Hoping you find it useful. If, please leave your comments and give thumbs up. Happy learning Kagglers. You people are awesome.:)","6bf0eddd":"Model Buidling and prediction","7eb8b12a":"Seems no other columns have nulls. It's safe to proceed.","910dcbb9":"Replace Label column (diagnosis) into binary codes.","9f08a6e6":"**Diagnosis is a label column and has 2 classes 'M' (Malignant) and 'B' (Belign) tumor.**\n1. As there are many features, let's try to see if we can reduce the features using different techniques.\n2. Let's plot heatmap to visualize and find the coefficient of multicollinearity.","fe828d0f":"\nDataset is not imbalanced, we are good to go."}}