{"cell_type":{"7c6fe75f":"code","eb924de9":"code","2887ad53":"code","72beebd9":"code","c7dee47b":"code","ee40abe3":"code","9a8829ad":"markdown","8a0b7c1f":"markdown","0f74eb34":"markdown","ec08d423":"markdown","11cfc5c9":"markdown","5eece101":"markdown"},"source":{"7c6fe75f":"import re\nfrom pathlib import Path\n\nfrom tqdm.auto import tqdm\n\n\n# Simple sentence splitting function\n\nalphabets= \"([A-Za-z])\"\nprefixes = re.compile(\"(Mr|St|Mrs|Ms|Dr|Prof|Capt|Cpt|Lt|Mt)[.]\")\nsuffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\nstarters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\nacronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\nwebsites = re.compile(\"[.](co|net|org|io|gov|edu|us)\")\netal = re.compile(r\"(\\bet al)[.]\")\nurls = re.compile(\"(www)[.]\")\ndigits =  re.compile(\"[.]([0-9])\")\n\ndef split_into_sentences(text):\n    text = \" \" + text + \"  \"\n    text = text.replace(\"\\n\",\" \")\n    text = prefixes.sub(\"\\\\1<prd>\",text)\n    text = websites.sub(\"<prd>\\\\1\",text)\n    text = urls.sub(\"\\\\1<prd>\",text)\n    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n    text = etal.sub(\"\\\\1<prd>\", text)\n    text = digits.sub(\"<prd>\\\\1\",text)\n    if \"\u201d\" in text: text = text.replace(\".\u201d\",\"\u201d.\")\n    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n    text = text.replace(\".\",\".<stop>\")\n    text = text.replace(\"?\",\"?<stop>\")\n    text = text.replace(\"!\",\"!<stop>\")\n    text = text.replace(\"<prd>\",\".\")\n    sentences = text.split(\"<stop>\")\n    if sentences[-1] == '':\n        sentences = sentences[:-1]\n    sentences = [s.strip() for s in sentences]\n    return sentences","eb924de9":"with open(\"train.txt\", \"w\") as train_file:\n    for filename in tqdm(Path(\"..\/input\/feedback-prize-2021\/train\").glob(\"*.txt\"), total=15594):\n        with open(filename) as current_file:\n            text = current_file.read().replace(\"\\n\", \" \")\n        sentences = split_into_sentences(text)\n        \n        cleaned_sentences = []\n        \n        for idx, sent in enumerate(sentences):\n            \n            # if a sentence is short, add it to the previous one\n            if idx > 0 and len(sent) < 20:\n                cleaned_sentences[-1] = cleaned_sentences[-1].strip() + \" \" + sent\n            else:\n                cleaned_sentences.append(sent)\n        \n        train_file.write(\"\\n\".join(cleaned_sentences)+\"\\n\")\n        \n    for filename in tqdm(Path(\"..\/input\/feedback-prize-2021\/test\").glob(\"*.txt\")):\n        # Weird way of making sure to not put back-to-back \"\\n\"            \n        with open(filename) as current_file:\n            text = current_file.read().replace(\"\\n\", \" \")\n        sentences = split_into_sentences(text)\n        \n        cleaned_sentences = []\n        \n        for idx, sent in enumerate(sentences):\n            \n            # if a sentence is short, add it to the previous one\n            if idx > 0 and len(sent) < 20:\n                cleaned_sentences[-1] = cleaned_sentences[-1].strip() + \" \" + sent\n            else:\n                cleaned_sentences.append(sent)\n        \n        train_file.write(\"\\n\".join(cleaned_sentences)+\"\\n\")","2887ad53":"from datasets import load_dataset\n\ndataset = load_dataset(\"text\", data_files=\"train.txt\", split=\"train\")\n\ndataset = dataset.filter(lambda x: len(x[\"text\"]) > 20) # remove short ones\ndataset","72beebd9":"def remove_duplicates(example):\n    if example[\"text\"] in seen:\n        return False\n    seen.add(example[\"text\"])\n    return True\n\nseen = set()\n\ndataset = dataset.filter(remove_duplicates)\ndataset","c7dee47b":"with open(\"train.txt\", \"w\") as fp:\n    for i, line in enumerate(dataset[\"text\"]):\n        if i == 0:\n            fp.write(line.strip())\n        else:\n            fp.write(\"\\n\"+line.strip())","ee40abe3":"with open(\"train.txt\") as fp:\n    \n    for i, line in enumerate(fp.readlines()):\n        if i > 100: break\n        print(line)","9a8829ad":"#### Use both train and test documents. Split into sentences, merge small sentences with larger ones, write to file","8a0b7c1f":"#### load into dataset object and remove short lines","0f74eb34":"#### writing final file","ec08d423":"#### checking final file","11cfc5c9":"## This is the notebook used to create a line-by-line dataset to pre-train language models for the Feedback Prize competition.\n\nSee pre-training notebook here: https:\/\/www.kaggle.com\/nbroad\/feedback-prize-pre-training-pt-gpu  \nDataset available here: https:\/\/www.kaggle.com\/nbroad\/feedback-prize-linebyline-text-dataset","5eece101":"#### remove duplicates"}}