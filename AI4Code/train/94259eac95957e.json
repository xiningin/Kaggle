{"cell_type":{"a7cb6847":"code","a06f9de5":"code","63f58bf4":"code","e489b9c7":"code","f51bb557":"code","52d712de":"code","45e048d8":"code","52fc8323":"code","91116aac":"code","af54a7b2":"code","0aa02c71":"code","06411fc9":"code","c92f9e3c":"code","51f4133b":"code","0e62c5aa":"code","9ef32f17":"code","e70e1132":"code","481e4f91":"code","7bcdfd9b":"code","d108dd11":"code","de809016":"code","558f6995":"code","d08b1e93":"code","0c837fe4":"code","5085228c":"code","8028f291":"code","6a3ac7e4":"code","99c48298":"code","17b50bc5":"code","ed6674bf":"code","85c97288":"code","96aa9e13":"code","52a97d4f":"code","22770a3a":"code","78068388":"code","128edccb":"code","d7eda067":"code","b369ac00":"code","f4de4d89":"code","ff1cf5a8":"markdown","83cfd893":"markdown","4576fbdd":"markdown","4d540be0":"markdown","b08b7ebf":"markdown","fda0832e":"markdown","7eb18942":"markdown","bb3f2096":"markdown","4000c18b":"markdown"},"source":{"a7cb6847":"import pandas as pd\nimport numpy as np\nimport matplotlib.pylab as plt\nimport seaborn as sns\n\nfrom datetime import date, datetime\n\nfrom sklearn.model_selection import KFold\nimport lightgbm as lgb\nfrom sklearn.metrics import mean_absolute_error\nplt.style.use('ggplot')\nimport warnings\nwarnings.filterwarnings('ignore')","a06f9de5":"train = pd.read_parquet('..\/input\/kaggle-pog-series-s01e01\/train.parquet')\ntest = pd.read_parquet('..\/input\/kaggle-pog-series-s01e01\/test.parquet')\nss = pd.read_csv('..\/input\/kaggle-pog-series-s01e01\/sample_submission.csv')","63f58bf4":"train.head()","e489b9c7":"train['categoryId'].value_counts()","f51bb557":"train.isnull().sum()","52d712de":"test.isna().sum()","45e048d8":"train.shape","52fc8323":"\ndef features(df, train=True):\n    \"\"\"\n    Adds features to training or test set.\n    \"\"\"\n    df['publishedAt'] = pd.to_datetime(df['publishedAt'])\n    df['trending_date'] = pd.to_datetime(df['trending_date'], utc=True)\n    \n    df['category_duration'] = df['categoryId'].map(df.groupby('categoryId')['duration_seconds'].mean().to_dict())\n    \n    # Feature 1 - Age of video\n    df['totaldayssinceupload'] = (pd.to_datetime(date(2021, 12, 31), utc=True)-df['publishedAt']) \\\n        .dt.total_seconds().astype('int')\n        \n    df['video_age_seconds'] = (df['trending_date'] - df['publishedAt']) \\\n        .dt.total_seconds().astype('int')\n    \n    # Trending day of week As a category\n    df['trending_dow'] = df['trending_date'].dt.day_name()\n    \n    df['trending_dow']= df['trending_dow'].astype('category')\n    \n    df['published_dow'] = df['publishedAt'].dt.day_name()\n    df['published_dow']= df['published_dow'].astype('category')\n    \n    df['categoryId'] = df['categoryId'].astype('category')\n    \n    df['channel_occurance'] = df['channelId'].map(\n        df['channelId'].value_counts().to_dict())\n\n    df['channel_unique_video_count'] = df['channelId'].map(\n        df.groupby('channelId')['video_id'].nunique().to_dict())\n    \n    df['video_occurance_count'] = df.groupby('video_id')['trending_date'] \\\n        .rank().astype('int')\n    df['day_of_week'] = df['publishedAt'].dt.day_name()\n    \n    return df","91116aac":"FEATURES = ['video_age_seconds',\n            'trending_dow',\n            'published_dow',\n            'duration_seconds',\n            'categoryId',\n            'totaldayssinceupload',\n            'comments_disabled',\n            'ratings_disabled',\n            'channel_occurance',\n            'channel_unique_video_count',\n            'video_occurance_count',            \n]\n\nTARGET = ['target']","af54a7b2":"df_train = features(train)\ndf_train = df_train[FEATURES]","0aa02c71":"df_train.head()","06411fc9":"#get dummies in case for XGBoost model\ndf_train_xg=pd.get_dummies(df_train,drop_first=True)\ndf_train_xg.head()","c92f9e3c":"df_train.isnull().sum()","51f4133b":"df_train.shape","0e62c5aa":"df_train.dtypes","9ef32f17":"target=train['target']","e70e1132":"df_train_xg.dtypes","481e4f91":"from sklearn.model_selection import train_test_split\nX_train, X_valid, Y_train, Y_valid = train_test_split(df_train_xg, target)","7bcdfd9b":"X_valid.dtypes","d108dd11":"from xgboost import XGBRegressor\nfrom sklearn.model_selection import GridSearchCV","de809016":"from lightgbm import LGBMRegressor","558f6995":"# get a stacking ensemble of models\ndef get_stacking():\n    # define the base models\n    level0 = list()\n    level0.append(('lgbm',LGBMRegressor(n_estimators=50_000,learning_rate=0.1,objective='mae',metric=['mae'],importance_type='gain')))\n    level0.append(('lgbm2',LGBMRegressor(n_estimators=75_000,learning_rate=0.25,objective='mae',metric=['mae'],importance_type='gain')))\n    # define meta learner model\n    level1 = XGBRegressor(learning_rate=0.5,max_depth=8,n_estimators=250)\n    # define the stacking ensemble\n    model = StackingRegressor(estimators=level0, final_estimator=level1, cv=5)\n    return model","d08b1e93":"from numpy import std\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedKFold","0c837fe4":"def evaluate_model(model, X, y):\n    cv = RepeatedKFold(n_splits=3, n_repeats=2, random_state=1)\n    scores = cross_val_score(model, X, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1, error_score='raise')\n    return scores","5085228c":"from matplotlib import pyplot\nfrom numpy import mean","8028f291":"from sklearn.ensemble import StackingRegressor\n# get the models to evaluate\nmodel = get_stacking()\nscores = evaluate_model(model, X_train, Y_train)\n","6a3ac7e4":"scores","99c48298":"ss.head()","17b50bc5":"test.head()","ed6674bf":"df_test = features(test)\ndf_test = df_test[FEATURES]","85c97288":"df_test=pd.get_dummies(df_test,drop_first=True)\ndf_test.head()","96aa9e13":"df_test.columns","52a97d4f":"X_train.columns","22770a3a":"model.fit(X_train,Y_train)","78068388":"df_test['categoryId_15']=0\ndf_test['categoryId_29']=0","128edccb":"df_test.columns","d7eda067":"pred=model.predict(df_test)","b369ac00":"df = pd.DataFrame({\n    \"id\": test[\"id\"],\n    \"target\": np.array(pred)\n}).to_csv(\"submission.csv\", index=False)","f4de4d89":"pd.read_csv('submission.csv')","ff1cf5a8":"# Stacking Ensemble","83cfd893":"# Create Submission","4576fbdd":"# Train Validation Split","4d540be0":"# Pog Competition\n- Ensemble Learning","b08b7ebf":"# Model Training","fda0832e":"# Feature Engineering","7eb18942":"# Train LGBM Model","bb3f2096":"<a href=\"submission.csv\"> Download File <\/a>","4000c18b":"# Look at Fold Feature Importances"}}