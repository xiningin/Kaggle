{"cell_type":{"e64a0da0":"code","b2654b2e":"code","52e719e7":"code","8b6efe5e":"code","5a9cc864":"code","a374e878":"code","d89aa051":"code","7a3edeca":"code","85875328":"code","de224801":"code","3d8ab04a":"code","42d6028d":"code","c2d1da4e":"code","20541d61":"code","c4294df3":"code","620b146f":"code","8c8005df":"code","62026ff0":"code","120d7192":"code","88c5d12f":"code","d9ad40e2":"code","ded84dea":"code","3f69e226":"code","5bb8b4f1":"code","b8253e68":"code","1c6e7c34":"code","55611879":"code","21d0d474":"code","7c7e429a":"code","c2abfca4":"code","84cce6ee":"code","02cf0738":"code","9719d65d":"code","ddbe0359":"code","ca519f4e":"code","a097abae":"code","0f2749b3":"code","722213c2":"code","39a29cbd":"code","ac77d9e8":"code","7ccb85a8":"code","61bd5fbc":"code","4bcadac9":"code","42769e8a":"code","16faad5d":"code","ceb03472":"code","7933afe3":"code","f45a2f93":"code","4fa7197d":"code","684b8bc5":"markdown","95be232b":"markdown","59e93850":"markdown","75de2616":"markdown","aa47663f":"markdown","52ad428a":"markdown","a42ef7df":"markdown","53d342c5":"markdown","8a23e2ea":"markdown","9d643e25":"markdown","1125cda8":"markdown","1f95f6cb":"markdown","ef4eeb85":"markdown","4943441c":"markdown","85147e2f":"markdown","c7a6d116":"markdown","08b1500e":"markdown","502ef187":"markdown","d22cff8d":"markdown","8b75b4c9":"markdown","4c967c95":"markdown","1ff8ba9a":"markdown","47c16b33":"markdown"},"source":{"e64a0da0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b2654b2e":"import matplotlib.pyplot as plt\nfrom mpl_toolkits import mplot3d\nfrom sklearn.multiclass import OneVsOneClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.datasets import load_iris\nfrom sklearn.preprocessing import StandardScaler\n\n%matplotlib inline","52e719e7":"data = load_iris()","8b6efe5e":"labels = data['target_names']\ndescription = data['DESCR']\nfeature_names = data['feature_names']\n\nprint(f\"This is a dataset with {len(data.data)} samples.\\n{len(labels)} classes.\\n{len(feature_names)} features for each sample.\")\nprint(f\"Feature names: {feature_names}\")\nprint(f\"Label Names:{labels}\")\nprint(f\"---------\\nSample input feature vector: {data['data'][0]}\\nLabel: {data['target'][0]}\")","5a9cc864":"x, y = data['data'][:,2],data['data'][:,3] # petal length and petal width \nc = data['target']\n\nfig, ax = plt.subplots(figsize=(8,6))\nscatter = ax.scatter(x, y, c=c,label='x')\n\n# produce a legend with the unique colors from the scatter\nlegend1 = ax.legend(*scatter.legend_elements(),\n                    loc=\"best\", title=\"Classes\",fontsize=16)\nax.add_artist(legend1)\nplt.title(\"0-Setosa,1-Versioclor,2-Virginica\",fontsize=16)\nplt.xlabel('Petal Length(cm)',fontsize=18)\nplt.ylabel('Petal Width(cm)',fontsize=18)\nplt.show()","a374e878":"c = []\nfor i in data['target']:\n    if i == 0:\n        c.append('red')\n    elif i==1:\n        c.append('green')\n    else:\n        c.append('blue')","d89aa051":"data['feature_names']","7a3edeca":"fig = plt.figure(figsize=(5,5))\nax = mplot3d.Axes3D(fig)\n\n# Data for a three-dimensional line\nsepal_width,petal_length,petal_width  = data['data'][:,0],data['data'][:,1],data['data'][:,2]\nplt.title('3D Plot')\nax.set_xlabel('Sepal Width(cm)')\nax.set_ylabel('Petal Length(cm)')\nax.set_zlabel('Petal Width(cm)')\nax.scatter(sepal_width,petal_length,petal_width,c=c)\n\n\nplt.show()","85875328":"# for this simplistic example I am just using one feature: sepal length\nx = data['data'][:,:2]\ny = (data['target']==0).astype('int')","de224801":"# rough plot to see what we're dealing with \nplt.figure(figsize=(7,7))\nfor i in range(len(x)):\n    if y[i] == 1:\n        c = 'red'\n        marker = '^'\n    else:\n        c = 'green'\n        marker = '.'\n    plt.plot(x[i,0],x[i,1],c=c,marker=marker)\nplt.xlabel('Sepal Length(cm)',fontsize=14)\nplt.ylabel('Sepal Width(cm)',fontsize=14)\nplt.title(\"Red Square - Iris Setosa | Green Dot - Not iris Setosa\",fontsize=12)\nplt.show()","3d8ab04a":"lr_binary = LogisticRegression() \nlr_binary.fit(x,y)","42d6028d":"lr_binary.__dict__","c2d1da4e":"# let us check the score now\nlr_binary.score(x,y)","20541d61":"sepal_l = data['data'][:,0] #sepal length","c4294df3":"single_feature_lr_binary = LogisticRegression()","620b146f":"single_feature_lr_binary.fit(sepal_l.reshape(-1,1),y)","8c8005df":"y_probab = single_feature_lr_binary.predict_proba(np.linspace(4.5,8.0,1000).reshape(-1,1))","62026ff0":"# h(x) = theta_0 + theta_1*X\ntheta_0,theta_1 = single_feature_lr_binary.intercept_,single_feature_lr_binary.coef_\nprint(theta_0.shape)\nprint(theta_1.shape)","120d7192":"single_feature_lr_binary.decision_function(sepal_l[0].reshape(-1,1))","88c5d12f":"def h(theta_0,theta_1,x):\n    return theta_0 + theta_1*x","d9ad40e2":"single_feature_lr_binary.decision_function([[4.5],[8]])","ded84dea":"single_feature_lr_binary.score(sepal_l.reshape(-1,1),y)","3f69e226":"# new regularization constant \nlr_single_feature_high_regularization = LogisticRegression(C=0.5) # C is inverse of Regulariation Strength\nlr_single_feature_low_regularization = LogisticRegression(C=10) \ncrazy_regularized = LogisticRegression(C=0.01)","5bb8b4f1":"lr_single_feature_high_regularization.fit(sepal_l.reshape(-1,1),y)\nlr_single_feature_low_regularization.fit(sepal_l.reshape(-1,1),y)","b8253e68":"crazy_regularized.fit(sepal_l.reshape(-1,1),y)","1c6e7c34":"no_regularization = LogisticRegression(C=10**5)","55611879":"y_prob_high = lr_single_feature_high_regularization.predict_proba(np.linspace(4.5,8.0,1000).reshape(-1,1))\ny_prob_low = lr_single_feature_low_regularization.predict_proba(np.linspace(4.5,8.0,1000).reshape(-1,1))","21d0d474":"plt.figure(figsize=(10,10))\n\nplt.subplot(221)\nplt.title(\"C=1.0\")\nplt.plot(sepal_l,y,'ro')\nplt.plot(np.linspace(4.5,8.0,1000),y_probab[:,0],label=\"P(Not Iris Setosa) | X \")\nplt.plot(np.linspace(4.5,8.0,1000),y_probab[:,1],label=\"P(Iris Setosa) | X \")\nplt.legend(loc='best')\nplt.xlabel('Sepal Length(in cm)')\n\n# C = 10 , LOW REGULARIZATION \nplt.subplot(222)\nplt.title(\"C = 10\")\nplt.plot(sepal_l,y,'ro')\nplt.plot(np.linspace(4.5,8.0,1000),y_prob_low[:,0],label=\"P(Not Iris Setosa) | X \")\nplt.plot(np.linspace(4.5,8.0,1000),y_prob_low[:,1],label=\"P(Iris Setosa) | X \",color='black')\nplt.legend(loc='best')\nplt.xlabel('Sepal Length(in cm)')\n\n\n# C = 0.5 , HIGH REULARIZATION \nplt.subplot(223)\nplt.title(\"C = 0.5\")\nplt.plot(sepal_l,y,'ro')\nplt.plot(np.linspace(4.5,8.0,1000),y_prob_high[:,0],label=\"P(Not Iris Setosa) | X \")\nplt.plot(np.linspace(4.5,8.0,1000),y_prob_high[:,1],label=\"P(Iris Setosa) | X \",color='black')\nplt.legend(loc='best')\nplt.xlabel('Sepal Length(in cm)')\nplt.show()\n\nplt.tight_layout()","7c7e429a":"# scores \nprint(lr_single_feature_high_regularization.score(sepal_l.reshape(-1,1),y))\nprint(lr_single_feature_low_regularization.score(sepal_l.reshape(-1,1),y))","c2abfca4":"crazy_regularized.score(sepal_l.reshape(-1,1),y)","84cce6ee":"no_regularization.fit(sepal_l.reshape(-1,1),y)","02cf0738":"no_regularization.score(sepal_l.reshape(-1,1),y)","9719d65d":"X = data['data'] # using all 4 features \ny = data['target'] # labels","ddbe0359":"lr_multi_class = LogisticRegression(multi_class='ovr') # we set the `multi_class` attribute to 'ovr'\nlr_multi_class.fit(X,y)","ca519f4e":"y_hat = lr_multi_class.predict(X) # make predictions","a097abae":"lr_multi_class.score(X,y)","0f2749b3":"plt.figure(figsize=(6,6))\nplt.title(\"OvR | Green+ = Correct Prediction | Redx = Incorrect | Mean Accuracy: 95.3%\",fontsize=14)\nfor i in range(len(X)):\n    # correctly predicted\n    if y[i] == y_hat[i]:\n        plt.plot(i,X[i,0],'g+')\n    else:\n        plt.plot(i,X[i,0],'rx')\nplt.show()\n","722213c2":"\nmodel = LogisticRegression()\novo = OneVsOneClassifier(model)\n# fit\novo.fit(X,y)\n# score\nprint(\"Score:\",ovo.score(X,y))\n# predictions\ny_hat = ovo.predict(X)\n","39a29cbd":"plt.figure(figsize=(6,6))\nplt.title(\"OvO | Green+ = Correct Prediction | Redx = Incorrect | Mean Accuracy: 95.3%\",fontsize=14)\nfor i in range(len(X)):\n    # correctly predicted\n    if y[i] == y_hat[i]:\n        plt.plot(i,X[i,0],'g+')\n    else:\n        plt.plot(i,X[i,0],'rx')\nplt.show()\n","ac77d9e8":"lr_muticlasss_multinomial = LogisticRegression(multi_class='multinomial',max_iter=125)\nlr_muticlasss_multinomial.fit(X,y)\nprint(lr_muticlasss_multinomial.score(X,y))","7ccb85a8":"y_hat = lr_muticlasss_multinomial.predict(X)","61bd5fbc":"plt.figure(figsize=(6,6))\nplt.title(\"OvO | Green+ = Correct Prediction | Redx = Incorrect | Mean Accuracy: 95.3%\",fontsize=14)\nfor i in range(len(X)):\n    # correctly predicted\n    if y[i] == y_hat[i]:\n        plt.plot(i,X[i,0],'g+')\n    else:\n        plt.plot(i,X[i,0],'rx')\nplt.show()\n","4bcadac9":"X_scaled = StandardScaler().fit_transform(X)","42769e8a":"X_scaled.std(axis=0)","16faad5d":"lr_scaled_features = LogisticRegression(multi_class=\"ovr\") # one vs rest\nlr_scaled_features.fit(X_scaled,y)","ceb03472":"# score on unscale\nlr_scaled_features.score(X_scaled,y)","7933afe3":"lr_ovo_scaled = LogisticRegression()\novo_scaled = OneVsOneClassifier(lr_ovo_scaled)\novo_scaled.fit(X_scaled,y)","f45a2f93":"ovo_scaled.score(X_scaled,y)","4fa7197d":"lr_scaled_multinomial = LogisticRegression(multi_class=\"multinomial\")\nlr_scaled_multinomial.fit(X_scaled,y)\nlr_scaled_multinomial.score(X_scaled,y)","684b8bc5":"# Visually Inspecting where the model mis-classified instances","95be232b":"# 3.Multinomial ","59e93850":"# Logistic Regression With Iris Dataset\n\nIn this notebook I will be demonstrating how to use the scikit-learn API for learning a Logistic Regression Classifier on the Iris Dataset. \n\n# Contents\n1. A [binary](#binary) classifier which learns a decision boundary between Iris Setosa speicies and Not Iris Setosa.  \n2. A multi-class classifier for all three species.(using OvR and Softmax)","75de2616":"# Standard Scaler to the rescue!\nWe haven't scaled our data yet, gradient descent converges faster when the data is scaled!  \n\nSource: https:\/\/www.coursera.org\/lecture\/machine-learning\/gradient-descent-in-practice-i-feature-scaling-xx3Da","aa47663f":"# 4. One-vs-Rest with Scaled Features","52ad428a":"# 1. One-vs-Rest Heuristic \nHere we train C binary classifiers, fc(x), where the data from class c is treated as positive, and the data from all the other classes is treated as negative.","a42ef7df":"## 1. Binary Classifier ","53d342c5":"### Seeing if the data is linearly seperable or not","8a23e2ea":"# Comparing three models with different regularization constants","9d643e25":"### Load Data","1125cda8":"# Using a single feature + labels are iris setosa and not iris setosa","1f95f6cb":"# Multi-Class Classification","ef4eeb85":"### Importing Libraries","4943441c":"## 0.893 is not really a good score, so let us try to tweak our model to make this score better..","85147e2f":"# 5 One-vs-One with scaled features","c7a6d116":"# 6. Mutlinomial with scaled features","08b1500e":"# Binary classifier\nFeature(s) - Sepal Length, Sepal Width\n\nTarget - Iris Setosa or Not Iris Setosa(1,0)","502ef187":"### Exploring Data","d22cff8d":"<a id=\"binary\"><\/a>","8b75b4c9":"### 3-D Plot","4c967c95":"From the plot above we can clearly see that this data is linearly seperable! ","1ff8ba9a":"# 2. One-vs-One Heuristic","47c16b33":"# Exploring attributes of the LogisticRegression() object"}}