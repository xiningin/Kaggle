{"cell_type":{"c452af92":"code","a0a01665":"code","3a5674d6":"code","2ad79b13":"code","9dcebeb3":"code","e900fd8a":"code","804b01d5":"code","f3008b70":"code","02857548":"code","1dba4b4a":"code","44b5b7e9":"code","77744555":"code","e97c2224":"code","b1b7afe1":"code","b5f3fb1c":"code","fda7ae64":"code","3086e31f":"code","f608cfa4":"code","37a0d6bd":"code","5689dc95":"code","ce59d5fa":"code","5a6db093":"code","e8ae1bc7":"code","ecc78a73":"code","12e6d984":"code","1a58c893":"code","5c00668d":"code","d67149c4":"code","09ed62b6":"code","270bcfbc":"code","621fd16a":"markdown","2610a1cd":"markdown","501e2cd4":"markdown","96a49dbc":"markdown","fe9e0282":"markdown","c9e96ac7":"markdown","3e7ebf62":"markdown","f1845c41":"markdown","3e99191f":"markdown","86d88934":"markdown","e34145f2":"markdown","1d2f292c":"markdown","3009a8c8":"markdown","7393ef3b":"markdown","7beb3bdf":"markdown","a79ba7e0":"markdown","43b1489d":"markdown","fbcbfad5":"markdown","b56590f2":"markdown","e47f47a3":"markdown","b4b6b3d7":"markdown","4843221b":"markdown","ad3f5e95":"markdown","9637947c":"markdown","799f396d":"markdown","40de9479":"markdown","e90de244":"markdown","3389e9cd":"markdown"},"source":{"c452af92":"%reload_ext autoreload\n%autoreload 2\n%matplotlib inline","a0a01665":"from fastai.vision import *\nfrom fastai.datasets import *\nfrom fastai.metrics import *\n\nimport numpy as np\nimport pandas as pd\nimport os\nimport re\n\nimport matplotlib.pyplot as plt","3a5674d6":"PATH = \"..\/input\/\"\npath_img = f'{PATH}train\/'","2ad79b13":"os.listdir(PATH)","9dcebeb3":"# os.listdir(path_img)","e900fd8a":"fnames = get_image_files(path_img)\nfnames[:5]","804b01d5":"img = plt.imread(f'{fnames[-1]}')\nplt.imshow(img);","f3008b70":"img.shape","02857548":"img[:4,:4]","1dba4b4a":"np.random.seed(33)\npattern = re.compile(r'\/([^\/]+)\\.\\d+.jpg$')","44b5b7e9":"data = ImageDataBunch.from_name_re(\n    path_img, fnames, pattern, ds_tfms=get_transforms(), size=150, bs=32\n                                  ).normalize(imagenet_stats)","77744555":"data.show_batch(rows=3, figsize=(7,6))","e97c2224":"print(data.classes)\nlen(data.classes),data.c","b1b7afe1":"learn = create_cnn(data, models.resnet34, metrics=accuracy, path='.\/')","b5f3fb1c":"learn.fit_one_cycle(1) # Aqui falta LR","fda7ae64":"learn.save('stage-1')","3086e31f":"interp = ClassificationInterpretation.from_learner(learn)\n\nlosses,idxs = interp.top_losses()\n\nlen(data.valid_ds)==len(losses)==len(idxs)","f608cfa4":"interp.plot_top_losses(9, figsize=(15,11))","37a0d6bd":"interp.plot_confusion_matrix(figsize=(6,6), dpi=80)","5689dc95":"interp.most_confused(min_val=2)","ce59d5fa":"learn.lr_find()","5a6db093":"learn.recorder.plot()","e8ae1bc7":"learn.unfreeze()","ecc78a73":"learn.fit_one_cycle(2, max_lr=slice(1e-5,1e-3))","12e6d984":"data = ImageDataBunch.from_name_re(path_img, fnames, pattern, ds_tfms=get_transforms(),\n                                   size=150, bs=16).normalize(imagenet_stats)","1a58c893":"learn = create_cnn(data, models.resnet50, metrics=accuracy, path='.\/')","5c00668d":"learn.lr_find()\nlearn.recorder.plot()","d67149c4":"learn.fit_one_cycle(4)","09ed62b6":"learn.save('stage-1-50')","270bcfbc":"learn.unfreeze()\nlearn.fit_one_cycle(3, max_lr=slice(1e-5,1e-2))","621fd16a":"Vamos a crear una soluci\u00f3n para la competicion \"Dogs vs Cats\" de Kaggle. Hay 25000 im\u00e1genes de perros y gatos, de las cuales 12500 son para el validation set. \n\nEn el momento en que la competici\u00f3n fue lanzada (2013), el estado del arte estaba en el 80% de precisi\u00f3n. Por lo que si batimos esta marca, estar\u00edamos construyendo un modelo top del a\u00f1o 2013.\n","2610a1cd":"## Deep learning: la soluci\u00f3n 'definitiva' ##\n\n**Una combinaci\u00f3n de capa lineal seguida de una funci\u00f3n no lineal a nivel de elementos nos permite crear formas arbitrariamente complejas; esta es la esencia del teorema de aproximaci\u00f3n universal.**\n\n![](https:\/\/cdn-images-1.medium.com\/max\/800\/1*R4qix1l4TjKOrLkrA4t6EA.png)","501e2cd4":"## Probemos con ResNet-50","96a49dbc":"## Ejemplos de clasificadores de imagenes ##\n\n* AlphaGo.\n\n* Splunk.com: detectando transacciones fraudulentas a partir del movimiento del rat\u00f3n\n\n* Google deepmind reduci\u00f3  en un 40% la factura de la luz de los centros de datos de Google. [Enlace](https:\/\/deepmind.com\/blog\/deepmind-ai-reduces-google-data-centre-cooling-bill-40\/)\n\n* Diagnosticando cancer de pulm\u00f3n\n![](https:\/\/cdn-images-1.medium.com\/max\/800\/1*_E0tiKelpZ3_7u0rOo6T5A.png)\n\n* Otro ejemplo:\n\n![](https:\/\/cdn-images-1.medium.com\/max\/800\/1*BFG_B7UpS3AvJxE6lH0lug.gif)","fe9e0282":"Las imagenes tienen esta pinta:","c9e96ac7":"1. Usamos `lr_find()` para encontrar el learning rate m\u00e1s alto en el que el modelo est\u00e1 claramente mejorando.\n1. Entrenar la \u00faltima capa 1 o 2 epochs.\n1. Learner.Unfreeze\n1. Entrenar toda la red neuronal con cycle_mult=2 hasta que tengamos over-fitting","3e7ebf62":"# Analizando e intepretando los resultados. *ClassificationInterpretation*","f1845c41":"\u00a1Ha mejorado bastante!","3e99191f":"## **98% de precisi\u00f3n** con unas pocas lineas, increible ##\n\n**Con este resultado, habr\u00edamos ganado la competici\u00f3n de Kaggle del 2013** con unos pocos segundos y 3 l\u00edneas de c\u00f3digo:\n\n![](https:\/\/i.gyazo.com\/b3cc85c6f5cfcd2c096fa884e6ba60ab.png)\n\nEl estado del arte previo a esta competici\u00f3n estaba en una precisi\u00f3n  del 80%. Gracias a esta competici\u00f3n el SOTA di\u00f3 un gran salto al 98.9% de precisi\u00f3n. Ahora, 4 a\u00f1os m\u00e1s tarde y gracias a fastai, podemos acercarnos much\u00edsimo e incluso batir dicho resultado en apenas unos minutos y con muy pocas lineas de c\u00f3digo.\n","86d88934":"## \u00bfQu\u00e9 es una red neuronal? ##","e34145f2":"Cuando hemos cargado el modelo **resnet-34** implicitamente fastai ha *congelado*  todos los layers de la red neuronal **excepto la \u00faltima**. Es decir, fastai no permite que los pesos de las capas congeladas se reajusten y s\u00f3lo ha dejado que se entrene la \u00faltima capa del modelo.\n\nEs improbable que las primeras capas de la ResNet-34  necesiten ser reentrenadas, ya que estas capas detectan las formas m\u00e1s b\u00e1sicas de una imagen:\n* La primera capa detecta bordes\n* La segunda capa reconoce curvas y esquinas.\n\nPor lo tanto, no necesitan ser modificadas.\n\n<img src=\"https:\/\/image.slidesharecdn.com\/practicaldeeplearning-160329181459\/95\/practical-deep-learning-16-638.jpg\" width=\"500\">\n\nSin embargo, no ocurre lo mismo con las \u00faltimas capas, las cuales es m\u00e1s probable que necesiten reentrenarse\n\n**Unfreezing**: cuando descongelamos todas las capas","1d2f292c":"## La librer\u00eda fast.ai ##\n\nJeremy ha construido una librer\u00eda potent\u00edsima en la que, con tan solo 3 l\u00edneas de c\u00f3digo, podemos construir un clasificador de im\u00e1genes de calidad *world-class*, consiguiendo resultados top con muy pocas l\u00edneas de c\u00f3digo.\n\nLa librer\u00eda fast.ai auna todas las mejores pr\u00e1cticas de deep learning que van saliendo a la luz. Cada vez que sale alg\u00fan nuevo paper con alguna t\u00e9cnica prometedora, Jeremy lo implementa y lo prueba. Si ve que funciona correctamente, lo adapta ala librer\u00eda para que la podamos usar de manera super sencilla, automatizando la mayor\u00eda de cosas y encarg\u00e1ndose de toda la parte engorrosa del deep learning.\n\nFast.ai est\u00e1 escrita sobre la librer\u00eda PyTorch. La mayor\u00eda de gente la \u00fanica librer\u00eda que conoce es TensorFlow. Pero Jeremy dice que, hoy en d\u00eda, la mayor\u00eda de investigadores que \u00e9l conoce utilizan PyTorch.\n\nFast.ai junta todas las mejores pr\u00e1cticas del deep learning y las pone a disposici\u00f3n de todo el mundo de manera gratuita.\n\n","3009a8c8":"## Competici\u00f3n: 'Dogs vs Cats'","7393ef3b":"[\u00bfC\u00f3mo funciona una CNN? KERNELS](http:\/\/setosa.io\/ev\/image-kernels\/)","7beb3bdf":"### Unfreeze y fine-tuning ###","a79ba7e0":"## El modelo: ResNet-34 pre-entrenado","43b1489d":"## Unfreezing, fine-tuning y learning-rates","fbcbfad5":"### Arquitectura de la ResNet-34 ### \n[Visualizacion del modelo](http:\/\/ethereon.github.io\/netscope\/#\/gist\/db945b393d40bfa26006)\n![](https:\/\/cdn-images-1.medium.com\/max\/800\/1*4tlPOipWjcwIoNUlQ6IWFQ.png)","b56590f2":"![](https:\/\/cdn-images-1.medium.com\/max\/800\/1*RPakI9UqMTYmGIm4ELhh6w.png)","e47f47a3":"### Learning rate ## \nJeremy ha implementado una t\u00e9cnica para encontrar el learning rate m\u00e1s adecuado para cada problema. Es de un paper que encontr\u00f3, que no mucha gente conoce y que es muy util a la hora de afinar al m\u00e1ximo nuestro modelo.l","b4b6b3d7":"## Resumen: los pasos para construir un clasificador top mundial","4843221b":"## Visualizando una CNN ##","ad3f5e95":"Con tan solo **1 linea** tenemos nuestro modelo pre-entrenado:\n\nNota: La siguiente l\u00ednea descarga de internet el modelo ResNet-34 (arquitectura y pesos) y los guarda en la variables 'learn'. Necesitamos el par\u00e1metro *path* para indicarle en qu\u00e9 carpeta guardar dicho modelo. ","9637947c":"# HispanIA DL: 1 (fastai v3)","799f396d":"## Organizando nuestros datos (imagenes y labels) mediante ImageDataBunch","40de9479":"## Echamos un vistazo a las imagenes","e90de244":"Utilizaremos una CNN (convolutional neural network) pre-entrenada, una red neuronal **creada** y **entrenada** por otra persona que resolv\u00eda un problema de una naturaleza parecida. \n\nResNet esta basado en [las capas residuales](https:\/\/github.com\/KaimingHe\/deep-residual-networks). Estas capas residuales son como las capas convecionales pero con la peculiaridad de que el input en crudo pasa a ser parte del output.","3389e9cd":"Lo \u00fanico que tenemos que saber por ahora es que una red neuronal es una funci\u00f3n que puede resolver cualquier problema con una precisi\u00f3n proporcional al n\u00famero de par\u00e1metros que tenga dicha red (Teorema de Aproximaci\u00f3n Universal)\n\nUna red neuronal consiste en una cantidad determinada de capas de funciones lineales simples entremezcladas por otras capas de funciones no-lineales simples.\n\n![](https:\/\/cdn-images-1.medium.com\/max\/800\/1*0YOpyzGWkrS4VW3ntJRQ5Q.png)"}}