{"cell_type":{"4da296d1":"code","a1226207":"code","a6821306":"code","852116b1":"code","e9435021":"code","6e6f01a5":"code","5d3dc1e3":"code","c09233fd":"code","bfbecf18":"code","a31f6151":"code","809babc2":"code","a693c3e3":"code","ea9f4b22":"code","ed463793":"code","08dd614a":"code","857d9b51":"code","5e6a1078":"code","20936b0d":"code","70fd33a3":"code","9d7291a6":"code","043276a3":"code","88a9ac26":"code","a484d81a":"code","c6889a09":"code","edd80f49":"code","c3982cac":"code","f1986030":"code","f4b6ea48":"markdown","638aa9d4":"markdown","61f09f34":"markdown","e2d9c8b4":"markdown","902a42c3":"markdown","ab4d4063":"markdown","aeb6616d":"markdown","b4e3d938":"markdown","be544054":"markdown","5eecac60":"markdown","ed1ab035":"markdown","feb9e8ed":"markdown","4d87d174":"markdown"},"source":{"4da296d1":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport numpy as np\n","a1226207":"def make_teacher_model(version='v3'):\n    if version == 'v1':\n        # Create the teacher\n        teacher = keras.Sequential(\n            [\n                keras.Input(shape=(32, 32, 3)),\n                layers.Conv2D(256, (3, 3), strides=(2, 2), padding=\"same\"),\n                layers.LeakyReLU(alpha=0.2),\n                layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\"),\n                layers.Conv2D(512, (3, 3), strides=(2, 2), padding=\"same\"),\n                layers.Flatten(),\n                layers.Dense(10),\n            ],\n            name=\"teacher\",\n        )\n\n    if version == 'v2':\n        # add multiple conv blocks\n        teacher = keras.Sequential(\n            [\n                keras.Input(shape=(32, 32, 3)),\n                layers.Conv2D(256, (3, 3), strides=(2, 2), padding=\"same\"),\n                layers.LeakyReLU(alpha=0.2),\n                layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\"),\n                layers.Conv2D(256, (3, 3), strides=(2, 2), padding=\"same\"),\n                layers.LeakyReLU(alpha=0.2),\n                layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\"),\n                layers.Conv2D(512, (3, 3), strides=(2, 2), padding=\"same\"),\n                layers.LeakyReLU(alpha=0.2),\n                layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\"),\n                layers.Flatten(),\n                layers.Dense(128, activation='relu'),\n                layers.Dense(10),\n            ],\n            name=\"teacher\",\n        )\n        \n    if version == 'v3':\n        # add dropout\n        teacher = keras.Sequential(\n            [\n                keras.Input(shape=(32, 32, 3)),\n                layers.Conv2D(256, (3, 3), strides=(2, 2), padding=\"same\"),\n                layers.LeakyReLU(alpha=0.2),\n                layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\"),\n                layers.Dropout(0.2),\n                layers.Conv2D(256, (3, 3), strides=(2, 2), padding=\"same\"),\n                layers.LeakyReLU(alpha=0.2),\n                layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\"),\n                layers.Dropout(0.3),\n                layers.Conv2D(512, (3, 3), strides=(2, 2), padding=\"same\"),\n                layers.LeakyReLU(alpha=0.2),\n                layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\"),\n                layers.Dropout(0.4),\n                layers.Flatten(),\n                layers.Dense(128, activation='relu'),\n                layers.Dropout(0.5),\n                layers.Dense(10),\n            ],\n            name=\"teacher\",\n        )\n\n    return teacher","a6821306":"# teacher = make_teacher_model(version='v2')\nteacher = make_teacher_model(version='v3')\n","852116b1":"\n# Prepare the train and test dataset.\nbatch_size = 64\n# (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n","e9435021":"x_train.shape, y_train.shape","6e6f01a5":"x_test.shape, y_test.shape","5d3dc1e3":"y_train[0]","c09233fd":"x_train[0]\n","bfbecf18":"# Normalize data\nx_train = x_train.astype(\"float32\") \/ 255.0\nx_train = np.reshape(x_train, (-1, 32, 32, 3))\n\nx_test = x_test.astype(\"float32\") \/ 255.0\nx_test = np.reshape(x_test, (-1, 32, 32, 3))\n","a31f6151":"x_train.shape, y_train.shape","809babc2":"x_train[0]\n","a693c3e3":"# Train teacher as usual\nteacher.compile(\n    optimizer=keras.optimizers.Adam(),\n    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n)\n\n","ea9f4b22":"# Train and evaluate teacher on data.\nhistory  = teacher.fit(x_train, \n            y_train, \n            epochs=20,\n            batch_size=batch_size,\n            validation_data=(x_test, y_test), \n            verbose=1)            \n","ed463793":"teacher.evaluate(x_test, y_test)\n","08dd614a":"# Create the student\nstudent_scratch = keras.Sequential(\n    [\n        keras.Input(shape=(32, 32, 3)),\n        layers.Conv2D(32, (3, 3), strides=(2, 2), padding=\"same\"),\n        layers.LeakyReLU(alpha=0.2),\n        layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\"),\n        layers.Conv2D(32, (3, 3), strides=(2, 2), padding=\"same\"),\n        layers.LeakyReLU(alpha=0.2),\n        layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\"),\n        layers.Dropout(0.2),\n        layers.Flatten(),\n        layers.Dense(16, activation='relu'),\n        layers.Dropout(0.5),\n        layers.Dense(10),\n    ],\n    name=\"student_scratch\",\n)\n\n\n","857d9b51":"# Train student as doen usually\nstudent_scratch.compile(\n    optimizer=keras.optimizers.Adam(),\n    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n)\n\n\n","5e6a1078":"# Train and evaluate student trained from scratch.\nhistory2 = student_scratch.fit(x_train, \n                               y_train, \n                               epochs=20,\n                                batch_size=batch_size,\n                                validation_data=(x_test, y_test), \n                                verbose=1)            \n\n","20936b0d":"student_scratch.evaluate(x_test, y_test)\n","70fd33a3":"class Distiller(keras.Model):\n    def __init__(self, student, teacher):\n        super(Distiller, self).__init__()\n        self.teacher = teacher\n        self.student = student\n\n    def compile(\n        self,\n        optimizer,\n        metrics,\n        student_loss_fn,\n        distillation_loss_fn,\n        alpha=0.1,\n        temperature=3,\n    ):\n        \"\"\" Configure the distiller.\n        Args:\n            optimizer: Keras optimizer for the student weights\n            metrics: Keras metrics for evaluation\n            student_loss_fn: Loss function of difference between student\n                predictions and ground-truth\n            distillation_loss_fn: Loss function of difference between soft\n                student predictions and soft teacher predictions\n            alpha: weight to student_loss_fn and 1-alpha to distillation_loss_fn\n            temperature: Temperature for softening probability distributions.\n                Larger temperature gives softer distributions.\n        \"\"\"\n        super(Distiller, self).compile(optimizer=optimizer, metrics=metrics)\n        self.student_loss_fn = student_loss_fn\n        self.distillation_loss_fn = distillation_loss_fn\n        self.alpha = alpha\n        self.temperature = temperature\n\n    def train_step(self, data):\n        # Unpack data\n        x, y = data\n\n        # Forward pass of teacher\n        teacher_predictions = self.teacher(x, training=False)\n\n        with tf.GradientTape() as tape:\n            # Forward pass of student\n            student_predictions = self.student(x, training=True)\n\n            # Compute losses\n            student_loss = self.student_loss_fn(y, student_predictions)\n            distillation_loss = self.distillation_loss_fn(\n                tf.nn.softmax(teacher_predictions \/ self.temperature, axis=1),\n                tf.nn.softmax(student_predictions \/ self.temperature, axis=1),\n            )\n            loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss\n\n        # Compute gradients\n        trainable_vars = self.student.trainable_variables\n        gradients = tape.gradient(loss, trainable_vars)\n\n        # Update weights\n        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n\n        # Update the metrics configured in `compile()`.\n        self.compiled_metrics.update_state(y, student_predictions)\n\n        # Return a dict of performance\n        results = {m.name: m.result() for m in self.metrics}\n        results.update(\n            {\"student_loss\": student_loss, \"distillation_loss\": distillation_loss}\n        )\n        return results\n\n    def test_step(self, data):\n        # Unpack the data\n        x, y = data\n\n        # Compute predictions\n        y_prediction = self.student(x, training=False)\n\n        # Calculate the loss\n        student_loss = self.student_loss_fn(y, y_prediction)\n\n        # Update the metrics.\n        self.compiled_metrics.update_state(y, y_prediction)\n\n        # Return a dict of performance\n        results = {m.name: m.result() for m in self.metrics}\n        results.update({\"student_loss\": student_loss})\n        return results\n","9d7291a6":"# Create the student\nstudent = keras.Sequential(\n    [\n        keras.Input(shape=(32, 32, 3)),\n        layers.Conv2D(32, (3, 3), strides=(2, 2), padding=\"same\"),\n        layers.LeakyReLU(alpha=0.2),\n        layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\"),\n        layers.Conv2D(32, (3, 3), strides=(2, 2), padding=\"same\"),\n        layers.LeakyReLU(alpha=0.2),\n        layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\"),\n        layers.Dropout(0.2),\n        layers.Flatten(),\n        layers.Dense(16, activation='relu'),\n        layers.Dropout(0.5),\n        layers.Dense(10),\n    ],\n    name=\"student\",\n)\n\n\n","043276a3":"# Initialize and compile distiller\ndistiller = Distiller(student=student, teacher=teacher)\n","88a9ac26":"distiller.compile(\n    optimizer=keras.optimizers.Adam(),\n    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n    student_loss_fn=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    distillation_loss_fn=keras.losses.KLDivergence(),\n    alpha=0.1,\n    temperature=10,\n)\n","a484d81a":"# Distill teacher to student\nhistory3 = distiller.fit(x_train, \n                         y_train, \n                         epochs=20,\n                         batch_size=batch_size,\n                         validation_data=(x_test, y_test), \n                         verbose=1)            \n\n\n","c6889a09":"\n\n# Evaluate student on test dataset\ndistiller.evaluate(x_test, y_test)\n","edd80f49":"# Initialize and compile distiller\ndistiller = Distiller(student=student, teacher=teacher)\n\ndistiller.compile(\n    optimizer=keras.optimizers.Adam(),\n    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n    student_loss_fn=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    distillation_loss_fn=keras.losses.KLDivergence(),\n    alpha=0.5,\n    temperature=10,\n)\n\n\n# Distill teacher to student\nhistory3b = distiller.fit(x_train, \n                         y_train, \n                         epochs=20,\n                         batch_size=batch_size,\n                         validation_data=(x_test, y_test), \n                         verbose=1)            \n\n\n\n\n# Evaluate student on test dataset\ndistiller.evaluate(x_test, y_test)\n\n","c3982cac":"# Initialize and compile distiller\ndistiller = Distiller(student=student, teacher=teacher)\n\ndistiller.compile(\n    optimizer=keras.optimizers.Adam(),\n    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n    student_loss_fn=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    distillation_loss_fn=keras.losses.KLDivergence(),\n    alpha=0.2,\n    temperature=10,\n)\n\n\n# Distill teacher to student\nhistory3c = distiller.fit(x_train, \n                         y_train, \n                         epochs=20,\n                         batch_size=batch_size,\n                         validation_data=(x_test, y_test), \n                         verbose=1)            \n\n\n\n\n# Evaluate student on test dataset\ndistiller.evaluate(x_test, y_test)\n\n","f1986030":"# Initialize and compile distiller\ndistiller = Distiller(student=student, teacher=teacher)\n\ndistiller.compile(\n    optimizer=keras.optimizers.Adam(),\n    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n    student_loss_fn=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    distillation_loss_fn=keras.losses.KLDivergence(),\n    alpha=0.9,\n    temperature=10,\n)\n\n\n# Distill teacher to student\nhistory3d = distiller.fit(x_train, \n                         y_train, \n                         epochs=20,\n                         batch_size=batch_size,\n                         validation_data=(x_test, y_test), \n                         verbose=1)            \n\n\n\n\n# Evaluate student on test dataset\ndistiller.evaluate(x_test, y_test)\n\n","f4b6ea48":"### Distiller\n\nThe custom Distiller() class, overrides the Model methods train_step, test_step, and compile(). In order to use the distiller, we need:\n\n1: Define and train a teacher model - This is a large convnet model for image classification.\n\n2: Define a student model to train - This is a smaller convent model for image classification.\n\nA student loss function on the difference between student predictions and ground-truth\n\nA distillation loss function, along with a temperature, on the difference between the soft student predictions and the soft teacher labels\n\nAn alpha factor to weight the student and distillation loss\n\nAn optimizer for the student and (optional) metrics to evaluate performance\n\nIn the train_step method, we perform a forward pass of both the teacher and student, calculate the loss with weighting of the student_loss and distillation_loss by alpha and 1 - alpha, respectively, and perform the backward pass. Note: only the student weights are updated, and therefore we only calculate the gradients for the student weights.\n\nIn the test_step method, we evaluate the student model on the provided dataset.","638aa9d4":"## Prepare the dataset\n\nPossible datasets:\n\n[MNIST](https:\/\/keras.io\/api\/datasets\/mnist\/), \n\n(Not used )\n\n---------\n\n[CIFAR-10](https:\/\/keras.io\/api\/datasets\/cifar10\/)\n\nThis is a dataset of 50,000 32x32 color training images and 10,000 test images, labeled over 10 categories. \n\nhttps:\/\/www.cs.toronto.edu\/~kriz\/cifar.html\n\nThe classes are completely mutually exclusive. There is no overlap between automobiles and trucks. \"Automobile\" includes sedans, SUVs, things of that sort. \"Truck\" includes only big trucks. Neither includes pickup trucks.\n\nPixel values range from 0 to 255.\n\n\nBoth the student and teacher are trained on the training set and evaluated on\nthe test set.\n","61f09f34":"## Create student and teacher models\n\nInitialy, we create a teacher model and a smaller student model. Both models are\nconvolutional neural networks and created using `Sequential()`,\nbut could be any Keras model.\n\n\nQ: Why the original teacher (v1) model not use the softmax activation - https:\/\/github.com\/keras-team\/keras-io\/issues\/755 ?\n\nhttps:\/\/datascience.stackexchange.com\/questions\/73093\/what-does-from-logits-true-do-in-sparsecategoricalcrossentropy-loss-function\n\n\nThe from_logits=True attribute inform the loss function that the output values generated by the model are not normalized, a.k.a. logits.\nIn other words, the softmax function has not been applied on them to produce a probability distribution. Therefore, the output layer in this case does not have a softmax activation function:\n\nso basically what it means is if softmax layer is not being added at the last layer then we need to have the from_logits=True to indicate the probabilities are not normalized \n","e2d9c8b4":"### teacher best perf = \n\n76%","902a42c3":"### student_scratch best perf = \n\n63%","ab4d4063":"### Code starts here","aeb6616d":"### Next task:\n\ntrain student with distillation\n\ncan this increase the score for student?\n\n\n\n    ","b4e3d938":"### play with hyperparam for alpha","be544054":"### Ideas taken from:\n\nKnowledge Distillation  Explained with Keras Example | #MLConcepts\n(https:\/\/www.youtube.com\/watch?v=0ZS2lLsZwBY)\n\nhttps:\/\/keras.io\/examples\/vision\/knowledge_distillation\/\n\nhttps:\/\/github.com\/keras-team\/keras-io\/blob\/master\/examples\/vision\/knowledge_distillation.py\n\n\nKnowledge Distillation - Keras Code Examples | Henry AI Labs\n(https:\/\/www.youtube.com\/watch?v=Y2K13XDqwiM)\n(https:\/\/www.youtube.com\/watch?v=gZPUGje1PCI)\n\n\n\n#### TODO: \n\nhttps:\/\/huggingface.co\/docs\/transformers\/model_doc\/distilbert\n\n\n\n    \n    ","5eecac60":"### Main idea:\n\nKnowledge Distillation is a procedure for model compression, in which a small (student) model is trained to match a large pre-trained (teacher) model. \n\nKnowledge is transferred from the teacher model to the student by minimizing a loss function, aimed at matching softened teacher logits as well as ground-truth labels.\n\n","ed1ab035":"### Next task:\n\n## Train student from scratch for comparison\n\ndefine student \n\ntrain student without distillation (student_scratch)\n\n\nWe can also train an equivalent student model from scratch without the teacher, in order\nto evaluate the performance gain obtained by knowledge distillation.\n\n\n\n    ","feb9e8ed":"## Train the teacher\nIn knowledge distillation we assume that the teacher is trained and fixed. Thus, we start\nby training the teacher model on the training set in the usual way.\n\n\n### Model params:\nhttps:\/\/stackoverflow.com\/questions\/44477489\/keras-difference-between-categorical-accuracy-and-sparse-categorical-accuracy\n\nin categorical_accuracy you need to specify your target (y) as one-hot encoded vector (e.g. in case of 3 classes, when a true class is second class, y should be (0, 1, 0). \n\nIn sparse_categorical_accuracy you need should only provide an integer of the true class (in the case from previous example - it would be 1 as classes indexing is 0-based).\n\n\n### Improve the teacher model: \n\nhttps:\/\/machinelearningmastery.com\/how-to-develop-a-cnn-from-scratch-for-cifar-10-photo-classification\/\n\n\nIt is better to use a separate validation dataset, e.g. by splitting the train dataset into train and validation sets. We will not split the data in this case, and instead use the test dataset as a validation dataset to keep the example simple.\n\nusing multi layers of conv2d+maxpool helps >5% - against 1 block\n\nhttps:\/\/stackoverflow.com\/questions\/63989328\/can-i-combine-conv2d-and-leakyrelu-into-a-single-layer\n\nadding Leakyrelu after conv2d helped: around 4% boost\n\nadding dropout - reduces the rate\/extent of overfitting\n\n","4d87d174":"### effect of alpha:\n\nalpha values taken were 0.1, 0.2. 0.5, 0.9\n\nloss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss\n\nhigher alpha suggests high student loss effect and low distilation effect.\n\n\ni.e. low alpha should give better results, since it can use distillation better.\n\nso far that is not so evident...\n\n"}}