{"cell_type":{"ab7db8a3":"code","6f7e5b06":"code","4821657a":"code","cd00dfb2":"code","2899a385":"code","2b2c4181":"code","b693b100":"code","0ed9634f":"code","cb830368":"code","fd5df9f2":"code","2756195b":"code","dd25faf7":"code","b3257a74":"code","da55f6ea":"code","39317eff":"code","1357d28a":"code","3f2036ae":"markdown","99d43ca4":"markdown","4f3ec22e":"markdown","502b35fe":"markdown","f362c1fd":"markdown","585d80ee":"markdown","23bec8f3":"markdown","04f1f4c8":"markdown","5ef0f084":"markdown","8f54ab52":"markdown","a28396fb":"markdown","6bcbc00f":"markdown"},"source":{"ab7db8a3":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport plotly.express as ex\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\nimport matplotlib.pyplot as plt\nimport plotly\nplotly.offline.init_notebook_mode (connected = True)\nimport seaborn as sns\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom wordcloud import WordCloud,STOPWORDS\nfrom sklearn.preprocessing import LabelEncoder\n\nsns.set_style('darkgrid')\n\nplt.rc('figure',figsize=(20,11))","6f7e5b06":"g_data = pd.read_csv('\/kaggle\/input\/top-850-guitar-tabs\/gutiarDB.csv')\ng_data.head(3)","4821657a":"#preprocessing\n\ng_data['Song Rating'] = g_data['Song Rating'].apply(lambda x: int(''.join(x.split(','))))\ng_data['Song Hits'] = g_data['Song Hits'].apply(lambda x: int(''.join(x.split(','))))\ng_data.Capo = g_data.Capo.replace({' 6th fre':' 6th fret',' 2nd fre':' 2nd fret',' 5th fre':' 5th fret',\n                                  ' 7th fre':' 7th fret',' 3rd fre':' 3rd fret',' 1st fre':' 1st fret',\n                                  ' 4th fre':' 4th fret'})\n\ng_data['Difficulty'].replace({'advance':'advanced','intermediat':'intermediate','novic':'novice'},inplace=True)\n\ndiff_dum = pd.get_dummies(g_data['Difficulty'],prefix='Difficulty')\ndiff_dum.drop(columns=['Difficulty_intermediate'],inplace=True)\ng_data = pd.concat([g_data,diff_dum],axis=1)\ng_data.drop(columns=['Difficulty'],inplace=True)\n\n\n\ng_data.head(3)","cd00dfb2":"ax = sns.countplot(g_data['Page Type'])\nax.set_title('Distribution Of Different Page Types',fontsize=18)\nax.set_xticklabels(ax.get_xticklabels(),fontsize=14)\nplt.show()","2899a385":"fig=ex.scatter(x=g_data['Song Hits'],y=g_data['Song Rating'],color=g_data['Page Type'])\nfig.update_layout(xaxis_title='Number Of Song Hits',yaxis_title='Song Rating Count')\nfig.show()","2b2c4181":"plt.subplot(2,1,1)\nax = sns.kdeplot(g_data['Song Hits'])\nax.set_xlabel(\"Number Of Views\",fontsize=20)\nax.set_ylabel(\"Density\",fontsize=20)\ntextstr = '\\n'.join(\n        (r'$\\mu=%.2f$' % (g_data['Song Hits'].mean(),), r'$\\mathrm{median}=%.2f$' % (g_data['Song Hits'].median(),),\n         r'$\\sigma=%.2f$' % (g_data['Song Hits'].std(),)))\nprops = dict(boxstyle='round', facecolor='green', alpha=0.5)\nax.text(0.75, 0.95, textstr, transform=ax.transAxes, fontsize=14,\n            verticalalignment='top', bbox=props)\n\nax.set_title('Distribution Of View Coutns Across Our Samples',fontsize=21)\nplt.show()\n\nplt.subplot(2,1,2)\ng_data['Song Hits'] =np.log(g_data['Song Hits'])\nax = sns.kdeplot(g_data['Song Hits'])\nax.set_xlabel(\"Number Of Views\",fontsize=20)\nax.set_ylabel(\"Density\",fontsize=20)\ntextstr = '\\n'.join(\n        (r'$\\mu=%.2f$' % (g_data['Song Hits'].mean(),), r'$\\mathrm{median}=%.2f$' % (g_data['Song Hits'].median(),),\n         r'$\\sigma=%.2f$' % (g_data['Song Hits'].std(),)))\nprops = dict(boxstyle='round', facecolor='green', alpha=0.5)\nax.text(0.75, 0.95, textstr, transform=ax.transAxes, fontsize=14,\n            verticalalignment='top', bbox=props)\n\nax.set_title('Distribution Of View Coutns Across Our Samples (After Log Transformation)',fontsize=21)\nplt.show()\n","b693b100":"plt.subplot(2,1,1)\nax = sns.kdeplot(g_data['Song Rating'])\nax.set_xlabel(\"Number Of Views\",fontsize=20)\nax.set_ylabel(\"Density\",fontsize=20)\ntextstr = '\\n'.join(\n        (r'$\\mu=%.2f$' % (g_data['Song Rating'].mean(),), r'$\\mathrm{median}=%.2f$' % (g_data['Song Rating'].median(),),\n         r'$\\sigma=%.2f$' % (g_data['Song Hits'].std(),)))\nprops = dict(boxstyle='round', facecolor='green', alpha=0.5)\nax.text(0.75, 0.95, textstr, transform=ax.transAxes, fontsize=14,\n            verticalalignment='top', bbox=props)\n\nax.set_title('Distribution Of View Coutns Across Our Samples',fontsize=21)\nplt.show()\n\ng_data['Song Rating'] = np.log(g_data['Song Rating'])\n\nplt.subplot(2,1,2)\nax = sns.kdeplot(g_data['Song Rating'])\nax.set_xlabel(\"Number Of Views\",fontsize=20)\nax.set_ylabel(\"Density\",fontsize=20)\ntextstr = '\\n'.join(\n        (r'$\\mu=%.2f$' % (g_data['Song Rating'].mean(),), r'$\\mathrm{median}=%.2f$' % (g_data['Song Rating'].median(),),\n         r'$\\sigma=%.2f$' % (g_data['Song Hits'].std(),)))\nprops = dict(boxstyle='round', facecolor='green', alpha=0.5)\nax.text(0.75, 0.95, textstr, transform=ax.transAxes, fontsize=14,\n            verticalalignment='top', bbox=props)\n\nax.set_title('Distribution Of View Coutns Across Our Samples (After Log Transformation)',fontsize=21)\nplt.show()","0ed9634f":"capo_encoder = LabelEncoder()\npage_type_encoder = LabelEncoder()\nkey_encoder  = LabelEncoder()\ntuning_encoder  = LabelEncoder()\ndifficulty_encoder  = LabelEncoder()\n\nge_data = g_data.copy()\n\nge_data['Capo'] =  capo_encoder.fit_transform(g_data['Capo'])\nge_data['Page Type'] =  page_type_encoder.fit_transform(g_data['Page Type'])\nge_data['Key'] =  key_encoder.fit_transform(g_data['Key'])\nge_data['Tuning'] =  tuning_encoder.fit_transform(g_data['Tuning'])\n\nge_data.head(3)","cb830368":"artists = ge_data.groupby(by='Artist').count()\nartists = artists.sort_values(by= 'Song Name',ascending=False)\nartists = artists[:30]\nartists = artists.rename(columns={'Song Name':'Number Of Songs'})\nex.pie(artists,values='Number Of Songs',names=artists.index,title='Top 30 Artists')","fd5df9f2":"gez_data = ge_data.copy()\ngez_data['Key'] = key_encoder.inverse_transform(ge_data['Key'])\ngez_data.head(3)","2756195b":"artists = gez_data.groupby(by='Key').count()\nartists = artists.sort_values(by= 'Artist',ascending=False)\nartists = artists[:5]\nartists = artists.rename(columns={'Song Name':'Number Of Songs'})\nex.pie(artists,values='Number Of Songs',names=artists.index,title='Top 30 Keys')\n","dd25faf7":"ex.pie(g_data,names='Capo',title='Proportions of songs with a certain capo requirement')","b3257a74":"stopwords = list(STOPWORDS)\n\nwords = ''\nfor name in ge_data['Song Name']:\n    tokens = name.lower().split(' ')\n    words += ' '.join(tokens)+' '\n\n\nwordcloud = WordCloud(width = 800, height = 800, \n                background_color ='white', \n                stopwords = stopwords, \n                min_font_size = 10).generate(words) \n  \nplt.figure(figsize = (25, 15), facecolor = None) \nplt.imshow(wordcloud) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \n  \nplt.show() ","da55f6ea":"sid = SentimentIntensityAnalyzer()\n\nword_count = WordCloud().process_text(words)\nword_count =  {k: v for k, v in sorted(word_count.items(), key=lambda item: item[1])}\nword_list = list(word_count.items())[-30:]\nword_list = [word for word,count in word_list]\nsent = ' '.join(word_list)\npscores = sid.polarity_scores(sent)\ndfs = pd.DataFrame(pscores,index=[1])\ndfs = dfs.T\ndfs= dfs.reset_index()\ndfs = dfs.rename(columns={'index':'Type',1:'Value'})\ndfs = dfs.drop(3)\nex.line_polar(dfs,r='Value',theta='Type',line_close=True)","39317eff":"def get_pos_sentiment(sir):\n    return sid.polarity_scores(sir)['pos']\ndef get_neg_sentiment(sir):\n    return sid.polarity_scores(sir)['neg']\ndef get_neu_sentiment(sir):\n    return sid.polarity_scores(sir)['neu']\n\nge_data['Positive_Sentiment'] = ge_data['Song Name'].apply(get_pos_sentiment)\nge_data['Negative_Sentiment'] = ge_data['Song Name'].apply(get_neg_sentiment)\nge_data['Neutral_Sentiment'] = ge_data['Song Name'].apply(get_neu_sentiment)\nge_data['Song_Name_Length'] = ge_data['Song Name'].apply(lambda x : len(x))","1357d28a":"fig = make_subplots(rows=2, cols=1,shared_xaxes=True,subplot_titles=('Perason Correaltion',  'Spearman Correaltion'))\ncolorscale=     [[1.0              , \"rgb(165,0,38)\"],\n                [0.8888888888888888, \"rgb(215,48,39)\"],\n                [0.7777777777777778, \"rgb(244,109,67)\"],\n                [0.6666666666666666, \"rgb(253,174,97)\"],\n                [0.5555555555555556, \"rgb(254,224,144)\"],\n                [0.4444444444444444, \"rgb(224,243,248)\"],\n                [0.3333333333333333, \"rgb(171,217,233)\"],\n                [0.2222222222222222, \"rgb(116,173,209)\"],\n                [0.1111111111111111, \"rgb(69,117,180)\"],\n                [0.0               , \"rgb(49,54,149)\"]]\n\ns_val =ge_data.corr('pearson')\ns_idx = s_val.index\ns_col = s_val.columns\ns_val = s_val.values\nfig.add_trace(\n    go.Heatmap(x=s_col,y=s_idx,z=s_val,name='pearson',showscale=False,xgap=1,ygap=1,colorscale=colorscale),\n    row=1, col=1\n)\n\n\ns_val =ge_data.corr('spearman')\ns_idx = s_val.index\ns_col = s_val.columns\ns_val = s_val.values\nfig.add_trace(\n    go.Heatmap(x=s_col,y=s_idx,z=s_val,xgap=1,ygap=1,colorscale=colorscale),\n    row=2, col=1\n)\n\nfig.update_layout(height=700, width=900, title_text=\"Locations That Contribute The Most To Our Cut-Offs\")\nfig.show()","3f2036ae":"<a id=\"1.1\"><\/a>\n<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Introduction<\/h3>","99d43ca4":"### We can see that most of our song pages are 'chords' and a much less are actuacl 'tabs' ","4f3ec22e":"### We see a high but not surprising correlation between the number of views on a certain song tab and the number of ratings, but the smaller correlations reveal that maybe there is some interesting behavior which indeed needs to be tested for example we see that the difficulty of the song is correlated with the songs rating as well as page type with difficulty.","502b35fe":"![](https:\/\/images.unsplash.com\/photo-1591025810539-a321000cda85?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=1350&q=80)\n\n\nMany key features turn a song into a popular and highly rated one, but what features show the most significance when it comes to learning to play that song on a guitar.\nWhat drives guitar players the most to take the time and learn the song they love, is it the difficulty of the song? or maybe it is the tuning? \nIn the following Kernel we will explore the data and try to uncover hidden patterns, try and find out can we predict the amount of song 'Hits' a tab will get depending on other features in our data set.\n\nBefore diving into the analysis lets first clearly define all our goals.\n\n## Question We Will Investigate\n1) What are the most popular artists in our data?\n\n2) What Key has the highest ratings?\n\n3) What Tuning has the highest rating?\n\n4) How does the capo position affect the ratings\n\n5) What effect do tunings have on the average ratings?\n\n6) Are there any word which shows up in most of the song names?\n\n\n","f362c1fd":"### The top 30 words in our song names range between neutral sentiment and positive where neutral is more dominant but the negative sentiment is at zero, meaning there are no negative or sad named songs in most of our list.","585d80ee":"### Almost 50% of the songs in our data set are in Db key!","23bec8f3":"### We see that 8.5% of the song in our list are taylor swifts and ed sheerans.","04f1f4c8":"<a id=\"1.1\"><\/a>\n<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Exploratory Data Analysis<\/h3>","5ef0f084":"### Similarly to the number of views, the number of ratings is positively skewed with large outliers which cause the skewness ","8f54ab52":"### It seems that the most common words in our song names are positive and happy we will test the sentiment of the 30 most common words as well to make a clear statement.We also see the word 'acoustic' as the most common word which is not surprising as many songs have an acoustic version and it may appear in many of or songs as a part of there name.","a28396fb":"<a id=\"1.1\"><\/a>\n<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Libraries And Utilities<\/h3>","6bcbc00f":"### The number of views has some outliers or 'black swans' we can see that our distribution is positively skewed and the distribution curve itself is quite narrow around the mode. "}}