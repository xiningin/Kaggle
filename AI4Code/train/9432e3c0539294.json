{"cell_type":{"1bbb0119":"code","ecda35f1":"code","eab59c9b":"code","6666eef1":"code","8ec89d22":"code","ac8bcb0d":"code","4d5dcc4a":"code","06bc5ef9":"code","1135cf81":"code","6eeb71d8":"code","aa15f732":"code","3dfe9a46":"code","9a13e5a7":"code","b4be264c":"code","135600b4":"code","1c91c057":"code","bf7b3345":"code","67737ac3":"code","36207cd6":"code","2e719532":"code","e3bdb186":"code","409cf782":"code","dc90e187":"code","a05c38b8":"code","2dd30535":"code","65ff7cc8":"code","f77b9881":"code","81f86b49":"code","80148f00":"markdown","d731fd73":"markdown","892451e0":"markdown","27f6610f":"markdown","0b491655":"markdown","20b2c325":"markdown","73538c85":"markdown","88701f9a":"markdown","48004b5a":"markdown","45ff0ed2":"markdown","516e6ae5":"markdown","0075be3a":"markdown","6583867b":"markdown","4348af38":"markdown","071c878e":"markdown","6bc3914d":"markdown","10e96451":"markdown","b1e1b7f9":"markdown","1aa25494":"markdown","fd2a268b":"markdown","f3c8dc1f":"markdown","d88bfe20":"markdown"},"source":{"1bbb0119":"pip install textstat","ecda35f1":"%matplotlib inline\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\ntqdm.pandas()\nimport spacy\nnlp = spacy.load('en_core_web_lg')\nfrom spacy.lang.en.stop_words import STOP_WORDS\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nnlp_sm = spacy.load('en_core_web_sm')\nimport textstat as ts\nvocab = list(nlp_sm.vocab.strings)\nimport seaborn as sns\nimport gc\nimport re","eab59c9b":"%%time\ndf = pd.read_csv('..\/input\/10000-books-and-their-genres-standardized\/books_and_genres.csv', index_col=0)","6666eef1":"df.head()","8ec89d22":"# Uncomment this line when not testing\ndf = df.iloc[:1750, :]","ac8bcb0d":"df['text'] = df['text'].astype(str)\ndf['text_len'] = df.text.apply(len)\ndf.drop(df[df.text_len < 1000].index, inplace=True)\ndf.drop('text_len', axis=1, inplace=True)\nmax_char = 1000000 # spaCy can only allocate enough RAM for this many chars in a doc\ndf.text = df.text.apply(lambda t:t[:max_char])","4d5dcc4a":"df = df.reset_index()\ndf.head()","06bc5ef9":"pip install langdetect","1135cf81":"from langdetect import detect\nfrom langdetect import detect_langs","6eeb71d8":"before_filter = len(df)","aa15f732":"df['lang'] = df.text.apply(detect)\ndf.drop(df[df.lang != 'en'].index, inplace=True)","3dfe9a46":"print(f'{before_filter - len(df)} books removed from the dataset for non-English language')","9a13e5a7":"def reformatGenres(genres):\n    l_genres = list(filter(None, re.split(r\"[\\{\\}\\,\\s\\']\", genres)))\n    # Combine history and historical tags since they refer to the same genre\n    r_genres = list({e if e!='history' else 'historical' for e in l_genres})\n    return r_genres","b4be264c":"df.genres = df.loc[:, 'genres'].apply(reformatGenres)","135600b4":"genres = [g for r in df.genres for g in r]","1c91c057":"genres_counts = pd.Series(genres).value_counts()\nprint(genres_counts)\ngenres_counts.plot(figsize=(15,5), title='Genre total count in dataset')\nplt.show()","bf7b3345":"# This line was originally used but for future use and simplicity its result is hard coded\n# [k for k, v in genres_counts.items() if v >= 2000]\npopular_genres = ['fiction', 'classics', 'historical', '20th-century', 'non-fiction', 'literature', 'historical-fiction', 'romance', 'fantasy', 'adventure']\nprint(f'{len(popular_genres)} genres have over 2000 counts in dataset.\\n\\nThey are:\\n\\n{popular_genres}')","67737ac3":"df.genres = df.genres.apply(lambda genres:[g for g in genres if g in popular_genres])\ndf['genre_count'] = df.genres.apply(len)\ndf = df[df.genre_count > 0]\ndf.drop('genre_count', axis=1, inplace=True)","36207cd6":"genres = [g for r in df.genres for g in r]\ngenres_counts = pd.Series(genres).value_counts()\nprint(genres_counts)\nax = genres_counts.plot(figsize=(15,5), kind='bar')\nax.set_title('Genre total count in dataset', fontdict={'fontsize':22}, pad=20)\nplt.show()","2e719532":"ax = genres_counts.plot(figsize=(20, 10), kind='pie', ylabel='')\nax.set_title('Genre count distribution pi chart', fontdict={'fontsize':22}, pad=20)\nplt.show()","e3bdb186":"df.head()","409cf782":"tfidf_vectorizer = TfidfVectorizer(vocabulary=vocab)\nunique_pos = ['ADJ','ADP','ADV','AUX','CCONJ','DET','INTJ','NOUN','NUM','PART','PRON','PROPN','PUNCT','SCONJ','SPACE','SYM','VERB','X']\ndef preprocess_text(text):\n    # reading complexity scores\n    flesch_re = ts.flesch_reading_ease(text)\n    flesch_k = ts.flesch_kincaid_grade(text)\n    smog = ts.smog_index(text)\n    \n    # spacy text cleaning, tokenizing, etc. \n    doc = nlp(text)\n    preprocessed_text = ' '.join([token.lemma_.strip().lower() for token in doc\n                    if token.is_alpha and not(token.is_stop)])\n    \n    # spaCy vectorization\n    doc_vector = nlp(preprocessed_text).vector\n    \n    # Term frequency-inverse document frequency vectorization\n    tfidf_vector = tfidf_vectorizer.fit_transform([preprocessed_text])\n    tfidf_vector = tfidf_vector.toarray()[0]\n    \n    # POS counts\n    pos = [[token.pos_] for token in doc]\n    df1 = pd.DataFrame(data=pos, columns=['POS'])\n    pos_counts = df1.POS.value_counts(normalize=True)\n    # split into columns named after unique_pos\n    pos_cols = [pos_counts[k] if k in pos_counts.keys() else 0 for k in unique_pos]\n    \n    # replace the text with nan to save space\n    filler_text = np.nan\n    \n    ret_data = filler_text, flesch_re, flesch_k, smog, *pos_cols, doc_vector, tfidf_vector\n\n    return ret_data","dc90e187":"df.memory_usage(deep=True)","a05c38b8":"processed_data = df.text.progress_apply(preprocess_text)","2dd30535":"df[['text', 'flesch_reading_ease', 'flesch_kincaid_grade', 'smog_index', *unique_pos, 'doc_vectr', 'tfidf_vectr']] = pd.DataFrame([*processed_data])","65ff7cc8":"df.memory_usage(deep=True)","f77b9881":"df.head()","81f86b49":"import pickle\n\nwith open('output_df.p', 'wb') as f:\n    pickle.dump(df, f)","80148f00":"Some rows have text that, for whatever reason, is not a `str` type. Cast these to `str`. \nspaCy's `nlp()` can only handle documents of a certain size before the RAM requirments get too hefty. To avoid errors being thrown, limit the book size (represented in the `text` column) to 1,000,000 characters. This is an ample amount since the average book has less than 500,000 characters in it (https:\/\/www.quora.com\/How-many-characters-of-text-letters-are-in-an-average-book). \n\nAlso, discard books that have less than 1,000 characters as they are insufficient.","d731fd73":"Now, for each example, remove genre tags not in the list shown above.","892451e0":"Call the `preprocess_text` method on the text data from our dataset.","27f6610f":"Now all of the genres that aren't used at least 2000 times in this dataset have been removed and any entries that no longer had genres were removed. Only 10 genres are now in use.","0b491655":"## Preprocess and feature engineer the text\nThis function below takes a spaCy doc or token and filters out stop words and punctuation, strips extra spacing, and lemmatizes words of the text.\nThe `preprocess_text` function below performs the following preprocessing and feature-engineering tasks:\n\n__Preprocessing__\n_* Most of the preprocessing steps are performed via [spaCy's](https:\/\/spacy.io\/) automated web-trained large English core pipeline*_\n- Lemmatize text\n- Remove upper capitalization\n- Strip excess spaces and carriage returns\n- Remove punctuation\n- Remove non-alpha characters\n- Remove stop words\n- Tokenize the text\n\n__Feature Engineering__\n- Vectorize the text via [spaCy's](https:\/\/spacy.io\/) `doc.vector()` method\n- Vectorize the text via [Sklearn's TF-IDF vectorizor](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.TfidfVectorizer.html)\n- Compute reading complexity scores via [textstat](https:\/\/pypi.org\/project\/textstat\/)\n- Compute the relative Part-of-speech frequencies via [spaCy's linguistic features](https:\/\/spacy.io\/usage\/linguistic-features)\n\n\n__Note__: _The preprocessing and feature-engineering steps are taken together to save huge amounts of data. Doing it this way always us to delete the full text for each row and retain the preprocessed and feature-engineered representations of the full text. Otherwise, saving the `spaCy` `doc` objects alone would take more than 200 GB of memory that would have to be stored in RAM. After attempting to solve this dilemma through various other methods, I resorted to this method. Additionally, I needed to obtain the reading complexity scores via `textstat` and these scores take into account punctuation, capitilization, etc., basically, the input text for these algorithms needed to be raw book text, not preprocessed text. Combining the preprocessing and feature engineering steps allows for this to easily be done in one function._","20b2c325":"## Reformat book text","73538c85":"Here is an alternative visualization of the distribution","88701f9a":"<hr>\n\n# 1. Import libraries and dataset\nI'll be using several libraries quite frequently so I'll import them below. I also will import the 10000-books-and-their-genres-standardized dataset mentioned in the notebook summary.","48004b5a":"### Visuals of genre tag distribution\nHere is a bar graph showing the new distribution of genres in the dataset. ","45ff0ed2":"Here is a sample of the dataset:","516e6ae5":"Based on the graph above, using the 10 most popular genre tags, which almost all have over 2,000 occurences in the dataset, will clean up the genre tags to make classification more feasible later on in the notebook.\n\n__Note__: _Originally, I used code to obtain the 10 genre tags I would keep in the dataset, but after obtaining these via code, I hard-coded them back in with some manual selection modifications_","0075be3a":"Create or replace columns in our dataframe with the results.","6583867b":"## Reduce the number of genre tags\nMulti-label classification problems are already very difficult\u2014to help make this problem more feasible, I'll reduce the number of unique genre tags used to a more moderate number.","4348af38":"Save the results in a pickled file for future use or reference.","071c878e":"Get all of the genre tags from every book in the dataset into a list.","6bc3914d":"## Remove non-English books\nNon-English books won't be processed correctly by our preprocessing algorithms.","10e96451":"# Feature Engineering and Classification\n## Summary\nThis notebook uses the [10000-books-and-their-genres-standardized](https:\/\/www.kaggle.com\/michaelrussell4\/10000-books-and-their-genres-standardized) dataset. This data includes around 10,000 full-length books and their associated genres obtained from GoodReads. \n\nIn this notebook, I clean and preprocess the text of each book in the dataset, filter out books with insufficient length or genre tags, and then convert the text via several different feature-engineering techniques and, after fitting the several new datasets obtained from the previous step to several machine-learning algorithms, I compare the results with the intention of evaluating different eature-engineering techniques and how they affect a machine-learning algorithms' performance in predicting multi-label targets.\n\nThe overall objective of this notebook is to compare two common methods for feature engineering text against several novel feature-engineering methods. The task is unique in this objective as well as in the endeavor to create multi-label classifiers on such a dataset, i.e., predict one or more genres for a given text where, in this case, the text is an entire book! Following are descriptions of the feature engineering methods referred to:\n1. [spaCy's](https:\/\/spacy.io\/) document vectorization.\n- spaCy creates a vector of a word or document based on a pretrained language model. In my case, I'll be using the `en_core_web_lg` model which breaks down to an English, Web-data trained, large model. The vectorized form of a document or word attempts to encapsulate the meaning of the word or document based on word semantic and meaning relationships, e.g., the vectors for words whose meanings are related, like 'dog' and 'canine', will depict such relationship via the vector similarities. spaCy is an incredible tool in this regard and makes such vectorization easy to implement.\n2. [Sklearn's TF-IDF vectorizor](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.TfidfVectorizer.html)\n- Scikit learn, or Sklearn, provides a method for transforming preprocessed text to a term frequency-inverse document frequency vectorized model. For more information on this method, see https:\/\/en.wikipedia.org\/wiki\/Tf%E2%80%93idf.\n3. Novel techniques\n- Part of speech relative frequencies.\n- Reading complexity scores obtained via 3 well-established algorithms implemented through [textstat](https:\/\/pypi.org\/project\/textstat\/).\n\n-\n\n*I'd like to thank the following for their contributions to the libraries used in this notebook.*\n- [Scikit-learn](https:\/\/scikit-learn.org\/)\n- [spaCy](https:\/\/spacy.io\/)","b1e1b7f9":"Get the counts of how many times each genre appears in the dataset and plot the results.","1aa25494":"## Restructure genre data\nThe genre data was a set before it was ported to a `csv` file but when it was ported it changed the set of genres to a string representation. This code translates the genres into a list of genres for each row.","fd2a268b":"@article{scikit-learn,\n title={Scikit-learn: Machine Learning in {P}ython},\n author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.\n         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.\n         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and\n         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},\n journal={Journal of Machine Learning Research},\n volume={12},\n pages={2825--2830},\n year={2011}\n}","f3c8dc1f":"Because the dataset is over 3 GB, I only use a portion while testing and developing. Uncomment this line when running the whole dataset.","d88bfe20":"<hr>\n\n# 2. Preprocessing Text and Feature Engineering"}}