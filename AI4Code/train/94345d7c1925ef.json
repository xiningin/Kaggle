{"cell_type":{"90c69e3a":"code","d88e9c46":"code","895cb4c1":"code","158722e6":"code","3d68b84e":"code","37e4d54d":"code","36db3241":"code","a3cb4c3c":"code","7a99c4b4":"code","f40dc671":"code","7fe41e87":"code","3d966e50":"code","246c4a70":"code","10cd89df":"code","29a4460d":"code","10bf80f1":"code","d8c7b8ef":"code","4102d763":"code","a6d7fd2e":"code","0dc30e32":"code","5179ddfb":"code","d27fbcf8":"code","9c884772":"code","9413b572":"code","8bbb07c5":"code","dbc30a76":"code","1dba04b0":"code","a734bfc0":"code","f76a9cac":"code","5201b38f":"markdown","d786c518":"markdown","5e0b7803":"markdown","4c8358e3":"markdown","ea501436":"markdown","c31dfa9d":"markdown","e01ce2c1":"markdown","035b4543":"markdown","88a08d31":"markdown","c3d1a9d7":"markdown","8f53bc96":"markdown","bc82206f":"markdown","e76984e9":"markdown","87437646":"markdown","656e0c90":"markdown","ee92bc64":"markdown","54cebe96":"markdown","d6970387":"markdown","955dc9be":"markdown","762d35d7":"markdown","b2a0836e":"markdown","bf9f0284":"markdown","45d609ce":"markdown","e84022cb":"markdown","cefd8ec1":"markdown","7eca17c8":"markdown","e5047e50":"markdown"},"source":{"90c69e3a":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\n\nnp.random.seed(42)","d88e9c46":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans, SpectralClustering, AgglomerativeClustering, AffinityPropagation, estimate_bandwidth, MeanShift, DBSCAN\nfrom sklearn.metrics import silhouette_score\nfrom yellowbrick.cluster import SilhouetteVisualizer\nfrom scipy.cluster.hierarchy import dendrogram, linkage","895cb4c1":"df = pd.read_csv(\"..\/input\/customer-segmentation-tutorial-in-python\/Mall_Customers.csv\")\n\ndf.sample(10)","158722e6":"df.info()","3d68b84e":"df.describe()","37e4d54d":"df.rename(columns = {\"Annual Income (k$)\": \"AnnualIncome\", \"Spending Score (1-100)\": \"SpendingScore\"}, inplace = True)","36db3241":"df","a3cb4c3c":"def feature_distribution(df, col):\n    \n    skewness = np.round(df[col].skew(), 3)\n    kurtosis = np.round(df[col].kurtosis(), 3)\n\n    fig, axes = plt.subplots(1, 3, figsize = (18, 6))\n    \n    sns.kdeplot(data = df, x = col, fill = True, ax = axes[0], color = \"#603F83\", linewidth=2)\n    sns.boxplot(data = df, y = col, ax = axes[1], color = \"#603F83\",\n                linewidth = 2, flierprops = dict(marker = \"x\", markersize = 3.5))\n    stats.probplot(df[col], plot = axes[2])\n\n    axes[0].set_title(\"Distribution \\nSkewness: \" + str(skewness) + \"\\nKurtosis: \" + str(kurtosis))\n    axes[1].set_title(\"Boxplot\")\n    axes[2].set_title(\"Probability Plot\")\n    fig.suptitle(\"For Feature:  \" + col)\n    \n    for ax in axes:\n        ax.set_facecolor(\"#C7D3D4FF\")\n        ax.grid(linewidth = 0.1)\n    \n    axes[2].get_lines()[0].set_markerfacecolor('#8157AE')\n    axes[2].get_lines()[0].set_markeredgecolor('#603F83')\n    axes[2].get_lines()[0].set_markeredgewidth(0.1)\n    axes[2].get_lines()[1].set_color('#F1480F')\n    axes[2].get_lines()[1].set_linewidth(3)\n    \n    sns.despine(top = True, right = True, left = True, bottom = True)\n    plt.show()","7a99c4b4":"for col in [\"Age\", \"AnnualIncome\", \"SpendingScore\"]:\n    feature_distribution(df, col)","f40dc671":"for col in [\"Age\", \"AnnualIncome\", \"SpendingScore\"]:\n    scaler = StandardScaler()\n    df[col] = scaler.fit_transform(df[col].values.reshape(-1, 1))","7fe41e87":"X = df[[\"AnnualIncome\", \"SpendingScore\"]]","3d966e50":"palette = [\"#280283\", \"#82005B\", \"#008B97\", \"#F1480F\", \"#9D9301\",  \"#4C00FF\", \n           \"#FF007B\", \"#00EAFF\", \"#9736FF\", \"#FFEE00\", \"#8992F3\"]","246c4a70":"fig, axes = plt.subplots(3, 3, figsize = (25, 15), sharex = True, sharey = True)\naxes = axes.ravel()\n\nfor i in range(2, 11):\n    \n    kmeans = KMeans(n_clusters = i, random_state = 42) \n    kmeans.fit(X)\n    cluster = kmeans.labels_\n    \n    sns.scatterplot(x = X[\"AnnualIncome\"], y = X[\"SpendingScore\"], hue = cluster,\n                    palette = palette[: len(set(cluster))], ax = axes[i-2],  edgecolor = None)\n    \n    sns.scatterplot(x = kmeans.cluster_centers_[:, 0], y = kmeans.cluster_centers_[:, 1], \n                    s = 200, color = \"#C0EB00\", label = \"Centroids\", marker = \"X\", edgecolor = \"black\", ax = axes[i-2])\n    \n    axes[i-2].set_facecolor(\"#C7D3D4\")\n    axes[i-2].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n    axes[i-2].set_title(\"KMeans with \" + str(i) + \" Clusters\")\n    fig.suptitle(\"Segmentation using KMeans \\nwith different Clusters\")\n    \nplt.tight_layout()","10cd89df":"sse = {}\n\nfor k in range(1, 11):\n    \n    kmeans = KMeans(n_clusters = k, random_state = 42)\n    kmeans.fit(X)\n    sse[k] = kmeans.inertia_\n    \n    \nfig, ax = plt.subplots(figsize = (12, 8))\n\nax.set_facecolor(\"#C7D3D4\")\nplt.title('The Elbow Method')\nplt.xlabel('k')\nplt.ylabel('SSE')\n\nsns.pointplot(x = list(sse.keys()), y = list(sse.values()))\n\nplt.show()","29a4460d":"silhouette_coefficients = {}\n\nfor k in range(2, 11):\n    \n    kmeans = KMeans(n_clusters=k, random_state = 42)\n    kmeans.fit(X)\n    silhouette_coefficients[k] = silhouette_score(X, kmeans.labels_)","10bf80f1":"fig, ax = plt.subplots(figsize = (12, 8))\n\nax.set_facecolor(\"#C7D3D4\")\n\nsns.pointplot(x = list(silhouette_coefficients.keys()), y = list(silhouette_coefficients.values()))\n\nplt.title(\"Silhouette Scores\")\nplt.xlabel(\"k\"); plt.ylabel(\"SSE\")\n\nplt.show()","d8c7b8ef":"for i in range(2, 11):\n    \n    fig, axes = plt.subplots(1, 2, figsize = (20, 6))\n\n    km = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)\n    km.fit(X)\n    cluster = km.labels_\n    \n    visualizer = SilhouetteVisualizer(km, colors = palette[: len(set(cluster))], ax = axes[0])\n    visualizer.fit(X)\n    \n    sns.scatterplot(x = X[\"AnnualIncome\"], y = X[\"SpendingScore\"], hue = cluster,\n                    palette = palette[: len(set(cluster))], ax = axes[1], edgecolor = None)\n    \n    sns.scatterplot(x = km.cluster_centers_[:, 0], y = km.cluster_centers_[:, 1],\n                    s = 250, color = '#C0EB00', label = 'Centroids', marker = \"X\", ax = axes[1], edgecolor = \"black\")\n    \n    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n    \n    axes[0].set_title(\"Silhouette Plot for \" + str(i) + \" Clusters\" + \"\\nSilhouette score : \" + str(silhouette_score(X, cluster).round(3)))\n    axes[1].set_title(\"Customer Segmentations for \" + str(i) + \" Clusters\" )    \n    \n    for ax in axes:\n        ax.set_facecolor(\"#C7D3D4\")","4102d763":"kmeans = KMeans(n_clusters = 5, random_state = 42) \nkmeans.fit(X)\ncluster = kmeans.labels_\n\nfig, ax = plt.subplots(figsize = (15, 10))\n\nsns.scatterplot(x = X[\"AnnualIncome\"], y = X[\"SpendingScore\"], hue = cluster, palette = palette[: len(set(cluster))])\n\nsns.scatterplot(x = kmeans.cluster_centers_[:, 0], y = kmeans.cluster_centers_[:, 1],\n                    s = 250, color = '#C0EB00', label = 'Centroids', marker = \"X\", ax = ax, edgecolor = \"black\")\n\nax.set_title(\"Segmentation with KMeans - 5 Clusters\")\nax.set_facecolor(\"#C7D3D4\")\nplt.show()","a6d7fd2e":"fig, axes = plt.subplots(1, 2, figsize = (18, 7))\n\nfor ax in axes:\n    ax.set_facecolor(\"#C7D3D4\")\n    ax.set_xlabel(\"Clusters\")\n    \nsns.boxplot(x = cluster, y = \"AnnualIncome\", data = X, ax = axes[0])\nsns.boxplot(x = cluster, y = \"SpendingScore\", data = X, ax = axes[1])\n\n\nplt.show()","0dc30e32":"fig, axes = plt.subplots(2, 2, figsize = (20, 15), sharex = True, sharey = True)\naxes = axes.ravel()\n\n\nfor i, link in enumerate([\"ward\", \"complete\", \"average\", \"single\"]):\n    \n    aggc = AgglomerativeClustering(n_clusters = 5, affinity = 'euclidean', linkage = link)  \n    aggc.fit(X)\n    cluster = aggc.labels_\n    \n    ax = axes[i]\n    ax.set_facecolor(\"#C7D3D4\")\n    \n    sns.scatterplot(x = X[\"AnnualIncome\"], y = X[\"SpendingScore\"], hue = cluster, palette = palette[: len(set(cluster))], ax = ax)\n    \n    ax.set_title(\"Linkage Method: \" + link.capitalize())\n    ax.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n    sns.despine(left = True, bottom = True)","5179ddfb":"fig, ax = plt.subplots(figsize = (20, 10))\n\nax.set_facecolor(\"#C7D3D4\")\nax.set_title(\"Dendrograms\")  \n\ndend = dendrogram(linkage(X, method = \"ward\"), labels = df.CustomerID.values)","d27fbcf8":"spectral = SpectralClustering(n_clusters = 5, random_state = 42, n_jobs = -1)\n\nspectral.fit(X)\n\ncluster = spectral.labels_","9c884772":"fig, ax = plt.subplots(figsize = (15, 10))\nax.set_facecolor(\"#C7D3D4\")\n\nsns.scatterplot(x = X[\"AnnualIncome\"], y = X[\"SpendingScore\"], hue = cluster, palette = palette[: len(set(cluster))])\n\nax.set_title(\"Segmentation with Spectral Clustering - 5 Clusters\")\nplt.show()","9413b572":"afp = AffinityPropagation(random_state = 42, max_iter = 500)\n\nafp.fit(X)\n\ncluster = afp.labels_","8bbb07c5":"fig, ax = plt.subplots(figsize = (15, 10))\nax.set_facecolor(\"#C7D3D4\")\n\nsns.scatterplot(x = X[\"AnnualIncome\"], y = X[\"SpendingScore\"], hue = cluster, \n                palette = palette[: len(set(cluster))], edgecolor = None, ax = ax)\n\nsns.scatterplot(x = afp.cluster_centers_[:, 0], y = afp.cluster_centers_[:, 1],\n                s = 250, color = '#C0EB00', label = 'Centroids', marker = \"X\", ax = ax, edgecolor = \"black\")\n\nax.set_title(\"Segmentation with Affinity Propagation\")\nplt.legend(bbox_to_anchor = (1.05, 1), loc = 2, borderaxespad = 0.)\nplt.show()","dbc30a76":"bandwidth = estimate_bandwidth(X, quantile = 0.1, random_state = 42, n_jobs = -1)\n\nms = MeanShift(bandwidth = bandwidth, n_jobs = -1)\n\nms.fit(X)\n\ncluster = ms.labels_","1dba04b0":"fig, ax = plt.subplots(figsize = (15, 10))\nax.set_facecolor(\"#C7D3D4\")\n\nsns.scatterplot(x = X[\"AnnualIncome\"], y = X[\"SpendingScore\"], hue = cluster, \n                palette = palette[: len(set(cluster))], edgecolor = None)\n\nsns.scatterplot(x = ms.cluster_centers_[:, 0], y = ms.cluster_centers_[:, 1],\n                s = 250, color = '#C0EB00', label = 'Centroids', marker = \"X\", edgecolor = \"black\")\n\nax.set_title(\"Segmentation with Mean Shift\")\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.show()","a734bfc0":"dbscan = DBSCAN(eps = 0.3, n_jobs = -1, min_samples = 5)\n\ndbscan.fit(X)\n\ncluster = dbscan.labels_","f76a9cac":"fig, ax = plt.subplots(figsize = (15, 10))\nax.set_facecolor(\"#C7D3D4\")\n\nsns.scatterplot(x = X[\"AnnualIncome\"], y = X[\"SpendingScore\"], hue = cluster, palette = palette[: len(set(cluster))])\n\nax.set_title(\"Segmentation with DBSCAN\")\nplt.legend(bbox_to_anchor = (1.05, 1), loc = 2, borderaxespad = 0.)\nplt.show()","5201b38f":"So cluster number that have better silhouette score will be right choice.","d786c518":"*In hierarchical clustering, it illustrates the arrangement of the clusters produced by the corresponding analyses.\n\n*Everitt, Brian (1998). Dictionary of Statistics. Cambridge, UK: Cambridge University Press. p. 96. ISBN 0-521-59346-8.","5e0b7803":"**Cluster 0** --> <font color='red'>Low<\/font> Income, <font color='green'>High<\/font> Spend\n\n**Cluster 1** --> Mid Income, Mid Spend\n\n**Cluster 2** --> <font color='green'>High<\/font> Income, <font color='red'>Low<\/font> Spend\n\n**Cluster 3** --> <font color='red'>Low<\/font> Income, <font color='red'>Low<\/font> Spend\n\n**Cluster 4** --> <font color='green'>High<\/font> Income, <font color='green'>High<\/font> Spend","4c8358e3":"https:\/\/scikit-learn.org\/stable\/modules\/clustering.html#hierarchical-clustering\n\n[****](https:\/\/scikit-learn.org\/stable\/modules\/clustering.html#hierarchical-clustering) Agglomerative Clustering object performs a hierarchical clustering using a bottom up approach: each observation starts in its own cluster, and clusters are successively merged together. \n\nThe linkage criteria determines the metric used for the merge strategy.\n\n**Ward** minimizes the sum of squared differences within all clusters. It is a variance-minimizing approach and in this sense is similar to the k-means objective function but tackled with an agglomerative hierarchical approach.\n\n**Maximum or complete** linkage minimizes the maximum distance between observations of pairs of clusters.\n\n**Average** linkage minimizes the average of the distances between all observations of pairs of clusters.\n\n**Single** linkage minimizes the distance between the closest observations of pairs of clusters.","ea501436":"# 6) DBSCAN","c31dfa9d":"I choose only two features for model. I keep only \"**AnnualIncome**\" and \"**SpendingScore**\" features that are most related features for customer segmentation. And having two features gives a chance to create better visualizations and understand segmentations well.","e01ce2c1":"If we look at above plots, we can see four different clustering. Even ward and complete clustering looks similar, but they are not same.","035b4543":"# 2) Hierarchical clustering","88a08d31":"# Elbow Method","c3d1a9d7":"# 2.1) Agglomerative Clustering","8f53bc96":"We chose the optimal cluster number. Let's create a model with 5 clusters and analyse it.","bc82206f":"# 2.2) Dendrogram","e76984e9":"# 3) Spectral Clustering","87437646":"We will plot sum of squared distances w.r.t. cluster. As cluster numbers increase, sse tends to zero.\n\nTo choose right number of clusters, we think this plot like an arm. So, elbow point will be our number of clusters.","656e0c90":"https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.cluster.DBSCAN.html#sklearn.cluster.DBSCAN\n\n[****](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.cluster.DBSCAN.html#sklearn.cluster.DBSCAN)**DBSCAN** - **Density-Based Spatial Clustering of Applications with Noise**\n\nFinds core samples of high density and expands clusters from them. \n\nGood for data which contains clusters of similar density.\n\n","ee92bc64":"https:\/\/scikit-learn.org\/stable\/modules\/clustering.html#hierarchical-clustering\n\n[****](https:\/\/scikit-learn.org\/stable\/modules\/clustering.html#hierarchical-clustering)Hierarchical clustering is a general family of clustering algorithms that build nested clusters by merging or splitting them successively. \n\nThis hierarchy of clusters is represented as a tree (or dendrogram). \n\nThe root of the tree is the unique cluster that gathers all the samples, the leaves being the clusters with only one sample. ","54cebe96":"Normalizing features before applying clustering techniques is an important step.","d6970387":"# 5) Mean Shift","955dc9be":"# 1) KMeans","762d35d7":"As we can see below, we can create lots of KMeans models with using different numbers of clusters. Well, how to choose optimum clusters?\n\nWe have two option.","b2a0836e":"# Silhouette Coefficients","bf9f0284":"Boxplots will help us to specify characteristics of clusters.","45d609ce":"Generally, I didn't make comments. I will probably update the kernel.","e84022cb":"Elbow plot shows us 5 is optimal cluster number. Before 5, SSE decreases significantly when cluster number increase. After then 5, we have a little decrease on SSE when cluster numbers increase.","cefd8ec1":"Silhouette coefficients or silhouette score is a metric that shows the quality of clustering.\n\n[****](https:\/\/en.wikipedia.org\/wiki\/Silhouette_(clustering))The silhouette value is a measure of how similar an object is to its own cluster (cohesion) compared to other clusters (separation). \n\nThe silhouette ranges from \u22121 to +1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters. \n\nIf most objects have a high value, then the clustering configuration is appropriate. \n\nIf many points have a low or negative value, then the clustering configuration may have too many or too few clusters.\n\n","7eca17c8":"If we look at above graph, 5 will be an optimal cluster number because it has best silhouette score.","e5047e50":"# 4) Affinity Propagation"}}