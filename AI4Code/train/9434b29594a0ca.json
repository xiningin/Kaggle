{"cell_type":{"2606c442":"code","ceef135c":"code","ea2ec2a1":"code","bc023d95":"code","485e883f":"code","06d6a0f7":"code","133791b2":"code","9cfa4c47":"code","892ee17d":"code","6d7eca57":"code","108f30a6":"code","9eb37f71":"code","1ed50f33":"code","5c439c73":"code","8b306b3d":"code","30555a41":"code","2eb50327":"code","ebd93d56":"code","88974332":"code","9e1f46a0":"code","3eaabadf":"code","a97abbe8":"code","b62b3bb4":"code","7fbc7a4c":"code","2612c406":"code","311cc444":"code","2945ccf2":"code","fdab0e22":"code","f9064281":"code","9518f2bb":"code","575dc8c8":"code","ed47e38a":"code","20df52bf":"code","e25abe45":"code","70105206":"code","352bb38f":"code","605e1994":"code","823155d0":"code","f5ee77a4":"code","37a4ace6":"code","4a861cb8":"code","df71eeb4":"code","0a4cca31":"code","42a455cc":"code","4ad17ea1":"code","2b49e410":"code","a1ea794f":"code","32217c69":"code","7f60089a":"code","22a529e3":"code","55fee067":"code","a399e5a8":"code","0ead9c5a":"code","5aff130e":"code","ac13c62a":"code","79ecb42d":"code","f4e0bd73":"code","653a2ff9":"code","fb46686f":"code","e1afe782":"markdown","edad5010":"markdown","f6cbb634":"markdown","57e1ed28":"markdown","cc520c42":"markdown","922b6417":"markdown","46a49383":"markdown","031b9441":"markdown","30a132c1":"markdown","6bef6c15":"markdown","8732b458":"markdown","3f580f26":"markdown","955c3a77":"markdown","a8cd9976":"markdown","c53b1f78":"markdown","df5e3c87":"markdown","5f2b0d7c":"markdown","529e3db8":"markdown","d56c85be":"markdown","25cb356f":"markdown","4a9760e0":"markdown","293cd11f":"markdown","425800d9":"markdown","ac5a3f19":"markdown","3c2f38f3":"markdown","3596616a":"markdown","fb53f120":"markdown","b8953615":"markdown","2c2d3ae7":"markdown","c4be7969":"markdown","60e4edc4":"markdown","bbdbb28e":"markdown","f78ea619":"markdown","768dde08":"markdown","90ccdb84":"markdown","61d2f500":"markdown","1763cb39":"markdown","67162ce8":"markdown","0f9bd75f":"markdown","308f0773":"markdown","43c1ecd3":"markdown","e7ae2b59":"markdown","7c29851f":"markdown","3efd2423":"markdown","7f3b79e8":"markdown","c3dd8483":"markdown","6df47630":"markdown","2780b38b":"markdown","8a00ddb9":"markdown","da22d446":"markdown","ce10c5ff":"markdown","911db69d":"markdown","e48a1718":"markdown","b9aab725":"markdown","8e49d9cb":"markdown","c7e6814f":"markdown","347ff181":"markdown","f3e16113":"markdown","a53241f1":"markdown","5a2854f6":"markdown","21b91373":"markdown","7b8b22fc":"markdown","30092433":"markdown"},"source":{"2606c442":"# Visualizations\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport missingno as msno\nimport matplotlib.pyplot as plt\n\n# Plotly\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\n\n# Text manipulations\nimport re\nimport string\nimport unicodedata\n\n# Natural Language Toolkit\nfrom nltk import word_tokenize\nfrom nltk.stem import SnowballStemmer\nfrom nltk.corpus import stopwords\n\n# WordCloud\nfrom PIL import Image\nfrom wordcloud import WordCloud\n\n# Scikit-learn\nfrom sklearn.svm import LinearSVC\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import cross_val_score, train_test_split, learning_curve\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix, plot_confusion_matrix, f1_score\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import TruncatedSVD, LatentDirichletAllocation\n\n#XGBoost model\nimport xgboost as xgb\n\n# Gensim\nimport gensim\nimport gensim.corpora as corpora\nfrom gensim.utils import simple_preprocess\nfrom gensim.models import CoherenceModel\nimport pyLDAvis\nimport pyLDAvis.gensim_models\n\n# Imbalanced-Learn Library\nfrom imblearn.under_sampling import RandomUnderSampler, AllKNN, NearMiss\nfrom imblearn.over_sampling import RandomOverSampler, BorderlineSMOTE , SMOTE, ADASYN\nfrom imblearn.combine import SMOTEENN, SMOTETomek\n\n# Keras\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.layers.embeddings import Embedding\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.optimizers import Adam\nfrom keras.losses import SparseCategoricalCrossentropy\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\nfrom keras.layers import (LSTM, Embedding, BatchNormalization, Dense, \n                        Dropout, Bidirectional, GlobalMaxPool1D)\n\n# Warning library\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Others\nimport time\nfrom pprint import pprint\nfrom collections import Counter\n\n# Some colors for prints\nclass print_color:\n   PURPLE = '\\033[95m'\n   CYAN = '\\033[96m'\n   DARKCYAN = '\\033[36m'\n   BLUE = '\\033[94m'\n   GREEN = '\\033[92m'\n   YELLOW = '\\033[93m'\n   RED = '\\033[91m'\n   BOLD = '\\033[1m'\n   UNDERLINE = '\\033[4m'\n   END = '\\033[0m'","ceef135c":"train = pd.read_csv(\"..\/input\/covid-19-nlp-text-classification\/Corona_NLP_train.csv\")\ntest = pd.read_csv(\"..\/input\/covid-19-nlp-text-classification\/Corona_NLP_test.csv\")\nprint(f\"Train shape : {train.shape}\\nTest shape  : {test.shape}\")","ea2ec2a1":"train.head()","bc023d95":"test.head()","485e883f":"print(train.isna().sum())\nmsno.bar(train, color=px.colors.qualitative.D3[0], sort=\"ascending\", figsize=(10,5), fontsize=13)\nplt.show()","06d6a0f7":"print(test.isna().sum())\nmsno.bar(test, color=px.colors.qualitative.D3[2], sort=\"ascending\", figsize=(10,5), fontsize=13)\nplt.show()","133791b2":"val_counts = train[\"Sentiment\"].value_counts()\n\nfig = make_subplots(\n        rows=1, cols=2,\n        specs=[[{\"type\": \"pie\"}, {\"type\": \"bar\"}]],\n        subplot_titles=(\"Pie\", \"Bar\")\n    )\n\nfig.add_trace(\n     px.pie(val_counts, \n            names=val_counts.index, \n            values=val_counts.values,\n            hole=0.4,\n            opacity=0.9\n    ).data[0],\n     row=1, col=1\n    \n).update_traces(\n    textposition='inside', textinfo='percent+label'\n).update_layout(\n    title=dict(text='<b>Sentiment distribution<b>', x=0.48), \n    legend=dict(x=0, y=-0.05, orientation='h')\n)\n\nfig.add_trace(\n     go.Bar(x=val_counts.index, \n            y=val_counts.values,\n            marker_color=px.colors.qualitative.Set2,\n            showlegend=False\n    ),\n     row=1, col=2\n)\n\nfig['layout']['annotations'][0].update(text='Sentiment', x=0.225, y=0.47, showarrow=False, font_size=15, opacity=0.9)\n\nfig.show()","9cfa4c47":"#20 locations that appear the most\ncounts = train[\"Location\"].value_counts().sort_values(ascending=False)[:20]\n\ndf = train.loc[train[\"Location\"].isin(counts.index)]\ndf = df.groupby(by=[\"Location\",\"Sentiment\"])[\"OriginalTweet\"].count().reset_index(name=\"Count\")\n\nfig = px.bar(df, x='Location', y='Count', color='Sentiment', \n             title=\"Top 20 <b>locations that appear the most<b>\", \n             color_discrete_sequence=[\"#F96C6C\", \"#F9B06C\", \"#F9F96C\", \"#AFF96C\", \"#3DF961\"],\n             category_orders={\n                 \"Location\": counts.index, \n                 \"Sentiment\": [\"Extremely Negative\",\"Negative\",\"Neutral\",\"Positive\",\"Extremely Positive\"]\n             }\n        )\n\nfig.show()","892ee17d":"counts = pd.to_datetime(train[\"TweetAt\"]).value_counts().sort_index()\nfig = px.bar(counts, x=counts.index, y=counts.values, title=\"<b>Number of tweets by date<b>\")\nfig.update_layout(xaxis_title=\"Date\", yaxis_title=\"Number of tweets\")\nfig.show()","6d7eca57":"df = train.copy()\ndf['tweet_len'] = df[\"OriginalTweet\"].apply(lambda x : len(x))\ndf['tweet_word_count'] = df[\"OriginalTweet\"].apply(lambda x : len(x.split(' ')))\ndf.head()","108f30a6":"fig = make_subplots(rows=1, cols=2, \n                    subplot_titles=(\"Distribution of <b>tweets length<b>\", \"Distribution of the <b>number of words<b>\"))\n\ndisplot1 = ff.create_distplot(\n    [df['tweet_len']], \n    ['tweet_len'], \n    bin_size=10, \n    show_rug=False,\n    colors=['#7FA6EE']\n)\n\ndisplot2 = ff.create_distplot(\n    [df['tweet_word_count']], \n    ['tweet_word_count'], \n    bin_size=10, \n    show_rug=False,\n    colors=['#EFD07F']\n)\n\n# tweet_len distribution plot\nfig.add_trace(\n    go.Histogram(displot1['data'][0]), \n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(displot1['data'][1], line=dict(color='blue', width=0.5)), \n    row=1, col=1\n)\n\n# tweet_word_count distribution plot\nfig.add_trace(\n    go.Histogram(displot2['data'][0]), \n    row=1, col=2\n)\n\nfig.add_trace(\n    go.Scatter(displot2['data'][1], line=dict(color='orange', width=0.5)), \n    row=1, col=2\n)\n\nfig.update_xaxes(title_text = \"Length\", row=1, col=1)\nfig.update_xaxes(title_text = \"Number\", row=1, col=2)\nfig.update_layout(showlegend=False)\n\nfig.show()","9eb37f71":"def pretreatment(text, lang='english', keepStopWords=set(), stem=True, lemma=False, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n    #Make text lowercase\n    text = text.lower()\n    #Remove text in square brackets\n    text = re.sub('\\[.*?\\]', '', text)\n    #Remove links\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    #Remove punctuations\n    punc = string.punctuation  # !\"#$%&'()*+,-.\/:;<=>?@[\\]^_`{|}~\n    punc += '\\n\\r\\t'\n    text = text.translate(str.maketrans(punc, ' ' * len(punc)))\n    #Remove numbers\n    text = re.sub('[0-9]+', '', text)\n    #Removal of accents and non-standard characters\n    text = unicodedata.normalize('NFD', text).encode('ascii', 'ignore').decode(\"utf-8\")\n    \n    #Lemmatization\n    if lemma :\n        tokens = word_tokenize(text)\n        doc = nlp(\" \".join(tokens)) \n        text = ' '.join([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n        \n    #Stemming    \n    if stem :\n        stemmer = SnowballStemmer(lang)\n        tokens = word_tokenize(text)\n        text = ' '.join([stemmer.stem(item) for item in tokens])\n    \n    #Remove stop words\n    stopwords_list = set(stopwords.words(lang)) - keepStopWords\n    words = word_tokenize(text)\n    text = ' '.join([word for word in words if word not in stopwords_list])\n    \n    return text","1ed50f33":"import spacy\n!python3 -m spacy download en\nnlp = spacy.load('en', disable=['parser', 'ner']) # for lemmatization","5c439c73":"text = \"\"\"   \\tHi I will test my CLEANING FUNCTION.\\n\\n\n[I want to thank you for taking your time to read this notebook] \\n\n* Here is the link to this notebook : https:\/\/www.kaggle.com\/pascalzg\/coronavirus-tweets-nlp !!\nWhat do you think of this notebook? ?? =)\\n\nIt's 7:09 pm, I have to take a break to go eat.\\n\nHere are some French words with accents: J'ai mang\u00e9 une baguette et c'\u00e9tait tr\u00e8s bon !! miaaam\n\u03b1 \u03b2 \u03b3 \u03b4 \u03b5  \u03b6\u03b7 \u03b8\nI ate cheese.\nIs what I write amazing, incredible, unmissable ? I hope so because that is the end of this text HAHA.\n\"\"\"\n\nprint(f\"{print_color.BOLD}{print_color.UNDERLINE}Original text :{print_color.END}\\n\\n{text}\\n\")\nprint(f\"{print_color.BOLD}{print_color.UNDERLINE}Cleaned text :{print_color.END}\\n\\n{pretreatment(text, stem=True, lemma=False)}\")","8b306b3d":"keep_words = {'not', 'could', 'would'} # Can be useful in Sentiment Analysis\n\ndf['tweet_clean'] = df['OriginalTweet'].apply(pretreatment, keepStopWords=keep_words, stem=True, lemma=False)\ntest['tweet_clean'] = test['OriginalTweet'].apply(pretreatment, keepStopWords=keep_words, stem=True, lemma=False)\n\ndf[[\"OriginalTweet\", \"tweet_clean\"]].head()","30555a41":"replace_map = {\"Sentiment\": {\"Extremely Positive\" : \"Positive\", \n                             \"Extremely Negative\" : \"Negative\"} \n              }\n\ndf[\"target\"] = pd.DataFrame(df[\"Sentiment\"]).replace(replace_map)\ntest[\"target\"] = pd.DataFrame(test[\"Sentiment\"]).replace(replace_map)\n\nprint(f\"{df['Sentiment'].value_counts()}\\n\\n{df['target'].value_counts()}\")","2eb50327":"sentiment_counts = df[\"Sentiment\"].value_counts()\ntarget_counts = df[\"target\"].value_counts()\n\nfig = make_subplots(\n        rows=1, cols=2,\n        specs=[[{\"type\": \"bar\"}, {\"type\": \"bar\"}]],\n        subplot_titles=(\"Sentiment distribution\", \"Target distribution\")\n    )\n\nfig.add_trace(\n     go.Bar(x=sentiment_counts.index, \n            y=sentiment_counts.values,\n            marker_color=['#37E984','#F89898','#F4F77F','#4DC215','#E73F3F'],\n            showlegend=False\n    ),\n     row=1, col=1\n)\n\nfig.add_trace(\n     go.Bar(x=target_counts.index, \n            y=target_counts.values,\n            marker_color=['#37E984','#F89898','#F4F77F'],\n            showlegend=False\n    ),\n     row=1, col=2\n)\n\nfig.show()","ebd93d56":"target_map = {\"Negative\" : 0, \"Neutral\" : 1, \"Positive\" : 2}\n\ndf[\"encoded_target\"] = df['target'].map(target_map)\ntest[\"encoded_target\"] = test['target'].map(target_map)\n\ndf[[\"target\", \"encoded_target\"]].head()","88974332":"X_train = df[\"tweet_clean\"]\ny_train = df[\"encoded_target\"]\nX_test = test[\"tweet_clean\"]\ny_test = test[\"encoded_target\"]","9e1f46a0":"def get_top_grams(corpus, ngram_range=(1,1), nbwords=10):\n    vect = CountVectorizer(ngram_range=ngram_range)\n    counts = vect.fit_transform(corpus).toarray().sum(axis=0)\n    \n    # Sort the index of the nbwords most frequent words (note : argsort() is an ascending sort)\n    argsort_descending = counts.argsort()[::-1][:nbwords]\n    names = np.array(vect.get_feature_names())[argsort_descending]\n    counts_sorted = counts[argsort_descending]\n    return names, counts_sorted\n\ndef plot_bar(names, counts_sorted, color_groups, title):\n    fig = go.Figure(go.Bar(x=counts_sorted, \n                           y=names,\n                           orientation='h',\n                           marker_color = ['#097394']*color_groups[0] + ['#52B1CE']*color_groups[1] + ['#A8E0F2']*color_groups[2]),\n                   )\n    \n    fig.update_traces(marker_line_color = '#077092', opacity=0.8)\n    fig['layout']['yaxis']['autorange'] = \"reversed\"\n    \n    fig.update_layout(title=title)\n    fig.show()","3eaabadf":"nb_words = 15\nnames, counts = get_top_grams(df[\"tweet_clean\"], ngram_range=(1,1), nbwords=nb_words)\nplot_bar(names, counts, (1,4,nb_words), f\"Top {nb_words} <b>Unigram<b>\")","a97abbe8":"nb_words = 15\nnames, counts = get_top_grams(df[\"tweet_clean\"], ngram_range=(2,2), nbwords=nb_words)\nplot_bar(names, counts, (1,5,nb_words), f\"Top {nb_words} <b>Bigrams<b>\")","b62b3bb4":"nb_words = 15\nnames, counts = get_top_grams(df[\"tweet_clean\"], ngram_range=(3,3), nbwords=nb_words)\nplot_bar(names, counts, (2,4,nb_words), f\"Top {nb_words} <b>Trigrams<b>\")","7fbc7a4c":"# https:\/\/amueller.github.io\/word_cloud\/generated\/wordcloud.WordCloud.html\n\nmask = np.array(Image.open('..\/input\/wordcloudmasks\/mask.jpg'))\n\nwordcloud = WordCloud(background_color='white',\n                      max_words=100,\n                      mask=mask)\n\nwordcloud.generate(' '.join(text for text in df['tweet_clean']))\n\nplt.figure(figsize=(20,10))\nplt.title(\"Top words\", fontdict={'size': 20})\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")  \nplt.show()","2612c406":"# TF-IDF Vectorizer\nvectorizer = TfidfVectorizer(ngram_range=(1, 2)) # Unigrams + Bigrams\nvectors = vectorizer.fit_transform(df['tweet_clean'])\n\nprint(\"Number of texts :\",vectors.shape[0],\"\\nNumber of words (Unigrams + Bigrams) :\", vectors.shape[1])\nprint(\"Number of non-zero entries :\",vectors.nnz)\nprint(\"Sparsity measurement :\",int(vectors.nnz\/float(vectors.shape[0])),\"active words per text out of\", vectors.shape[1],\"!\")\n\n# To find the words\nprint('\\n',[(i,vectorizer.get_feature_names()[i]) for i in np.random.randint(vectors.shape[1], size=5)])","311cc444":"n_clusters = 3\nn_words = 6\nseed = 0\n\nkmeans = KMeans(n_clusters=n_clusters, random_state=seed, max_iter=10)\nkmeans.fit(vectors)\n\n# Find the cluster number for each tweet\nlabels = {k:[] for k in range (n_clusters)}\nfor i, label in enumerate(kmeans.labels_):\n    labels[label].append(i)\n\n# Join all texts of the same cluster\ntext_clusters = {k:\"\" for k in range (n_clusters)}\nfor k in range(n_clusters):\n    text_clusters[k] = ' '.join(df['tweet_clean'].iloc[labels[k]])\n    \n# Most common words for each cluster\nfor k, text in text_clusters.items():\n    print(f\"{n_words} most common words for cluster {k}:  {Counter(text.split()).most_common(n_words)}\")\nprint()\n    \n# WordClouds\nfig, axs = plt.subplots(2, 2, figsize=(15,8))\nfig.suptitle(\"WordClouds\", fontweight='bold', fontsize=18)\nfor i in range(n_clusters):\n    wordcloud = WordCloud(background_color='white', max_words=100).generate(text_clusters[i])\n    axs[i\/\/2][i%2].set_title(f\"Cluster {i}\", fontdict = {'fontsize':15, 'fontweight':'bold'})\n    axs[i\/\/2][i%2].imshow(wordcloud, interpolation='bilinear')\n    axs[i\/\/2][i%2].axis(\"off\")     \nfig.delaxes(axs[1][1])\nplt.show()","2945ccf2":"# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.decomposition.TruncatedSVD.html\n\nn_clusters = 3\nn_words = 6\nseed = 0\n\nsvd = TruncatedSVD(n_components=n_clusters, n_iter=10, random_state=seed)\nreduc_svd = svd.fit_transform(vectors)\n\nlabels = np.argmax(reduc_svd, axis=1)\n\n# Find the cluster number for each tweet\nlabels = {k:[] for k in range (n_clusters)}\nfor i, label in enumerate(labels):\n    labels[label].append(i)\n\n# Join all texts of the same cluster\ntext_clusters = {k:\"\" for k in range (n_clusters)}\nfor k in range(n_clusters):\n    text_clusters[k] = ' '.join(df['tweet_clean'].iloc[labels[k]])\n    \n# Most common words for each cluster\nfor k, text in text_clusters.items():\n    print(f\"{n_words} most common words for cluster {k}:  {Counter(text.split()).most_common(n_words)}\")\nprint()   \n\n# WordClouds\nfig, axs = plt.subplots(2, 2, figsize=(15,8))\nfig.suptitle(\"WordClouds\", fontweight='bold', fontsize=18)\nfor i in range(n_clusters):\n    wordcloud = WordCloud(background_color='white', max_words=100).generate(text_clusters[i])\n    axs[i\/\/2][i%2].set_title(f\"Cluster {i}\", fontdict = {'fontsize':15, 'fontweight':'bold'})\n    axs[i\/\/2][i%2].imshow(wordcloud, interpolation='bilinear')\n    axs[i\/\/2][i%2].axis(\"off\")     \nfig.delaxes(axs[1][1])\nplt.show()","fdab0e22":"n_clusters = 3\nn_words = 6\nseed = 0\n\nlda = LatentDirichletAllocation(n_components=n_clusters, max_iter=10, random_state=seed)\nreduc_lda = lda.fit_transform(vectors)\n\nlabels = np.argmax(reduc_lda, axis=1)\n\n# Find the cluster number for each tweet\nlabels = {k:[] for k in range (n_clusters)}\nfor i, label in enumerate(labels):\n    labels[label].append(i)\n\n# Join all texts of the same cluster\ntext_clusters = {k:\"\" for k in range (n_clusters)}\nfor k in range(n_clusters):\n    text_clusters[k] = ' '.join(df['tweet_clean'].iloc[labels[k]])\n    \n# Most common words for each cluster\nfor k, text in text_clusters.items():\n    print(f\"{n_words} most common words for cluster {k}:  {Counter(text.split()).most_common(n_words)}\")\nprint()   \n\n# WordClouds\nfig, axs = plt.subplots(2, 2, figsize=(15,8))\nfig.suptitle(\"WordClouds\", fontweight='bold', fontsize=18)\nfor i in range(n_clusters):\n    wordcloud = WordCloud(background_color='white', max_words=100).generate(text_clusters[i])\n    axs[i\/\/2][i%2].set_title(f\"Cluster {i}\", fontdict = {'fontsize':15, 'fontweight':'bold'})\n    axs[i\/\/2][i%2].imshow(wordcloud, interpolation='bilinear')\n    axs[i\/\/2][i%2].axis(\"off\")     \nfig.delaxes(axs[1][1])\nplt.show()","f9064281":"def tokenization(texts):\n    for text in texts:\n        yield(simple_preprocess(str(text)))  # deacc=True removes punctuations\n\n# List of tokenized tweets\ndata_words = list(tokenization(df['tweet_clean']))\nprint(\"Tokenization of the first tweet :\",data_words[0])\n\n# Create Dictionary\nid2word = corpora.Dictionary(data_words)\n\n# Create Corpus\ntexts = data_words\n\n# Term Document Frequency\ncorpus = [id2word.doc2bow(text) for text in texts]\nprint(\"and his Term Document Frequency :\",corpus[0])\n\n# Human readable format of corpus (term-frequency)\n[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]","9518f2bb":"seed = 0\n# Build LDA model\nlda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n                                           id2word=id2word,\n                                           num_topics=3, \n                                           random_state=seed,\n                                           update_every=1,\n                                           chunksize=100,\n                                           passes=10,\n                                           alpha='auto',\n                                           per_word_topics=True)\n\n# Print the most significant topics\npprint(f\"The most significant topics :\\n{lda_model.print_topics()}\")\ndoc_lda = lda_model[corpus]","575dc8c8":"# WordClouds\ntopics = lda_model.show_topics(formatted=False)\n\nfig, axs = plt.subplots(2, 2, figsize=(15,8))\nfig.suptitle(\"WordClouds\", fontweight='bold', fontsize=18)\nfor i in range(n_clusters):\n    wordcloud = WordCloud(background_color='white', max_words=100).generate_from_frequencies(dict(topics[i][1]))\n    axs[i\/\/2][i%2].set_title(f\"Cluster {i}\", fontdict = {'fontsize':15, 'fontweight':'bold'})\n    axs[i\/\/2][i%2].imshow(wordcloud, interpolation='bilinear')\n    axs[i\/\/2][i%2].axis(\"off\")     \nfig.delaxes(axs[1][1])\nplt.show()","ed47e38a":"# Compute Perplexity\nprint('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n\n# Compute Coherence Score\ncoherence_model_lda = CoherenceModel(model=lda_model, texts=data_words, dictionary=id2word, coherence='c_v')\ncoherence_lda = coherence_model_lda.get_coherence()\nprint('\\nCoherence Score: ', coherence_lda)","20df52bf":"# Visualize the topics\n# pyLDAvis.enable_notebook()\n# vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word) # sort_topics=False, to not reorder by size\n# vis","e25abe45":"%%time\n\n# Pipelines\npip_uni = Pipeline([ ('countUni', CountVectorizer(ngram_range=(1, 1))), ('clf', LinearSVC()) ])\npip_bi = Pipeline([ ('countBi', CountVectorizer(ngram_range=(2, 2))), ('clf', LinearSVC()) ])\npip_tri = Pipeline([ ('countTri', CountVectorizer(ngram_range=(3, 3))), ('clf', LinearSVC()) ])\npip = Pipeline([ ('countUniBi', CountVectorizer(ngram_range=(1, 2))), ('clf', LinearSVC()) ])\n\n# Lists\nnames = [\"Unigrams\",\"Bigrams\",\"Trigrams\",\"Uni+Bigrams\"]\npipelines = [pip_uni, pip_bi, pip_tri, pip]\nres = []\n\n# Results\nfor i, p in enumerate(pipelines) :\n    val_score = cross_val_score(p, df['tweet_clean'], df[\"encoded_target\"]).mean()\n    res.append({'n_grams':names[i],'Score':val_score})\n\nresults = pd.DataFrame(res).sort_values(by='Score', ascending=False)\nresults.style.background_gradient(\"Blues\")","70105206":"%%time\n\n# Pipelines\npip_uni = Pipeline([ ('tfidfUni', TfidfVectorizer(ngram_range=(1, 1))), ('clf', LinearSVC()) ])\npip_bi = Pipeline([ ('tfidfBi', TfidfVectorizer(ngram_range=(2, 2))), ('clf', LinearSVC()) ])\npip_bi = Pipeline([ ('tfidfTri', TfidfVectorizer(ngram_range=(3, 3))), ('clf', LinearSVC()) ])\npip = Pipeline([ ('tfidfUniBi', TfidfVectorizer(ngram_range=(1, 2))), ('clf', LinearSVC()) ])\n\n# Lists\nnames = [\"Unigrams\",\"Bigrams\",\"Trigrams\",\"Uni+Bigrams\"]\npipelines = [pip_uni, pip_bi, pip_tri, pip]\nres = []\n\n# Results\n\nfor i, p in enumerate(pipelines) :\n    val_score = cross_val_score(p, df['tweet_clean'], df[\"encoded_target\"]).mean()\n    res.append({'n_grams':names[i],'Score':val_score})\n\nresults = pd.DataFrame(res).sort_values(by='Score', ascending=False)\nresults.style.background_gradient(\"Blues\")","352bb38f":"texts = [text.split() for text in df[\"tweet_clean\"]]\n\n# the following configuration is the default configuration\nw2v_cbow = gensim.models.word2vec.Word2Vec(sentences=texts,\n                                        vector_size=100, window=5,     ### here we train a cbow model \n                                        min_count=5,                      \n                                        sample=0.001, workers=3,\n                                        sg=0, hs=0, negative=5,        ### set sg to 1 to train a sg model\n                                        cbow_mean=1,\n                                        epochs=5)\n\n# print(w2v_cbow.wv.key_to_index) # get vocabulary","605e1994":"words = ['covid', 'market', 'famili']\nn = 5\nfor word in words :\n    print(f\"Top {n} similar words with {word} :\\n{w2v_cbow.wv.most_similar(word, topn=n)}\\n\\n\")","823155d0":"print(w2v_cbow.wv.most_similar(positive=['covid','futur'], negative=['increas'], topn=n))","f5ee77a4":"print(w2v_cbow.predict_output_word(['impact'],topn=n))","37a4ace6":"texts = [text.split() for text in df[\"tweet_clean\"]]\n\nw2v_sg = gensim.models.word2vec.Word2Vec(sentences=texts,\n                                vector_size=100, window=5,               ### here we train a sg model \n                                min_count=5,                      \n                                sample=0.001, workers=3,\n                                sg=1, hs=0, negative=5,        ### set sg to 1 to train a sg model\n                                cbow_mean=1,\n                                epochs=5)\n\n# print(w2v_sg.wv.key_to_index) # get vocabulary","4a861cb8":"words = ['covid', 'market', 'famili']\nn = 5\nfor word in words :\n    print(f\"Top {n} similar words with {word} :\\n{w2v_sg.wv.most_similar(word, topn=n)}\\n\\n\")","df71eeb4":"print(w2v_sg.wv.most_similar(positive=['covid','futur'], negative=['increas'], topn=n))","0a4cca31":"print(w2v_sg.predict_output_word(['impact'],topn=n))","42a455cc":"print(\"Mod\u00e8le CBOW, great and good:\",w2v_cbow.wv.similarity(\"great\",\"good\"))\nprint(\"Mod\u00e8le CBOW, great and bad:\",w2v_cbow.wv.similarity(\"great\",\"bad\"))\nprint(\"Mod\u00e8le SG, great and good:\",w2v_sg.wv.similarity(\"great\",\"good\"))\nprint(\"Mod\u00e8le SG, great and bad:\",w2v_sg.wv.similarity(\"great\",\"bad\"))","4ad17ea1":"def vectorize(text, model, mean=False):\n    \"\"\" This function should vectorize one tweet.\"\"\"\n    text = text.split()\n    vec = np.zeros(model.vector_size)\n    cpt = 0\n    for word in text:\n        if word in model.wv.key_to_index:\n            vec += model.wv.get_vector(word)\n            cpt += 1\n            \n    if (mean == True) and (cpt != 0):\n        return vec \/ cpt\n\n    return vec\n\ndef evaluation(models, models_name, X_train, X_test, y_train, y_test):\n    res = []\n    svc = LinearSVC(dual=False)\n    for mi in range(len(models)):\n        x_train = [vectorize(text, models[mi]) for text in X_train]\n        x_test = [vectorize(text, models[mi]) for text in X_test]\n        svc.fit(x_train, y_train)\n        ypred = svc.predict(x_test)\n        res.append({'Model':models_name[mi],'Aggregation':'sum','Score':accuracy_score(y_test, ypred)})\n        \n        X_mean = [vectorize(text, models[mi], True) for text in X_train]\n        X_test_mean = [vectorize(text, models[mi], True) for text in X_test]\n        svc.fit(X_mean, y_train)\n        ypred = svc.predict(X_test_mean)\n        res.append({'Model':models_name[mi],'Aggregation':'mean','Score':accuracy_score(y_test, ypred)})\n        \n    return res \n\n# Let's see what a tweet vector looks like.\nprint(vectorize(X_train.iloc[0], w2v_sg))","2b49e410":"%%time\nmodels = [w2v_cbow, w2v_sg]\nnames = [\"w2v_cbow\", \"w2v_sg\"]\nres = evaluation(models, names, X_train, X_test, y_train, y_test)\npd.DataFrame(res).sort_values(by='Score', ascending=False).style.background_gradient(\"Blues\")","a1ea794f":"%%time\n\n# Evaluation function\ndef eval_model(clf, names, list_xtrain, list_ytrain, x_test, y_test) :\n    \"\"\" Returns a sorted DataFrame with an f1-score for each (classifier - resampling technique). \"\"\"\n    results = []\n    for name, x, y in zip(names, list_xtrain, list_ytrain) :\n        start = time.time()\n        clf.fit(x, y)\n        pred = clf.predict(x_test)\n        results.append({'Name': name, 'F1-score':f1_score(y_test, pred, average='micro')})\n        print(f\"{print_color.BOLD}{name}{print_color.END} execution time : {print_color.DARKCYAN}{time.time() - start} seconds{print_color.END}\")\n    return pd.DataFrame(results).sort_values(by='F1-score', ascending=False)\n\n# TF-IDF Vectorizer\nvectorizer = TfidfVectorizer(ngram_range=(1, 2))\nX_train_tfidf = vectorizer.fit_transform(X_train, y_train)\nX_test_tfidf = vectorizer.transform(X_test)\n\n# List of re-sampling techniques\nnames = [\"RandUnderSampler\", \"NearMiss\", \"AllKNN\", \"RandOverSampler\", \"BorderSMOTE\", \"SMOTE\", \"ASASYN\", \"SMOTEENN\", \"SMOTETomek\"]\nunder_strategy = {0: 7713, 1: 7713, 2: 7713}\nover_strategy = {0: 18046, 1: 18046, 2: 18046}\nseed = 142\n\ntechniques = [RandomUnderSampler(sampling_strategy=under_strategy, random_state=seed), NearMiss(version=3), AllKNN(), #Undersampling\n             RandomOverSampler(sampling_strategy=over_strategy, random_state=seed), BorderlineSMOTE(random_state=seed), #Oversampling\n             SMOTE(random_state=seed), ADASYN(sampling_strategy='minority', random_state=seed), #Oversampling\n             SMOTEENN(random_state=seed), SMOTETomek(random_state=seed)] #Combined techniques\n\n# Re-sampling\nlist_xtrain = []\nlist_ytrain = []\n\nfor i, tech in enumerate(techniques):\n    start = time.time()\n    x, y = tech.fit_resample(X_train_tfidf, y_train)\n    list_xtrain.append(x)\n    list_ytrain.append(y)\n    print(f\"{print_color.BOLD}{names[i]}{print_color.END} execution time : {print_color.GREEN}{time.time() - start} seconds{print_color.END}\")","32217c69":"# Random Undersampling\nsampling_strategy = {0: 15398, 1: 7713, 2: 15398}\nundersample = RandomUnderSampler(sampling_strategy=sampling_strategy)\nX_under, y_under = undersample.fit_resample(X_train_tfidf, y_train)\n\n# Borderline SMOTE (Oversampling)\nsm = BorderlineSMOTE()\nX_under_sm, y_under_sm = sm.fit_resample(X_under, y_under)\n\n# More data\nnames += [\"Original\", \"Undersampling\", \"Under+SMOTE\"]\nlist_xtrain += [X_train_tfidf, X_under, X_under_sm]\nlist_ytrain += [y_train, y_under, y_under_sm]         ","7f60089a":"%%time\nlinSVC = LinearSVC(random_state=seed)\nres = eval_model(linSVC, names, list_xtrain, list_ytrain , X_test_tfidf, y_test)\nres.style.background_gradient(\"Greens\")","22a529e3":"# Best data\nxtrain_best = list_xtrain[res.index[0]]\nytrain_best = list_ytrain[res.index[0]]\nprint(names[res.index[0]])","55fee067":"%%time\n\nxgboost = xgb.XGBClassifier(num_class=3, learning_rate=0.1, max_depth=10,\n                            use_label_encoder=False, eval_metric='mlogloss')\n\nxgboost.fit(xtrain_best, ytrain_best)\n\n# Predictions\ny_pred = xgboost.predict(X_test_tfidf)\n\n# Plot confusion matrix\nplot_confusion_matrix(xgboost, X_test_tfidf, y_test)\n\n# Print Classification report\nprint(f\"\\nClassification Report :\\n{classification_report(y_test, y_pred)}\")","a399e5a8":"%%time\n\nlinSVC = LinearSVC(random_state=seed)\n\n# Fit the pipeline\nlinSVC.fit(xtrain_best, ytrain_best)\n\n# Predictions\ny_pred = linSVC.predict(X_test_tfidf)\n\n# Plot confusion matrix\nplot_confusion_matrix(linSVC, X_test_tfidf, y_test)\n\n# Print Classification report\nprint(f\"\\nClassification Report :\\n{classification_report(y_test, y_pred)}\")","0ead9c5a":"texts = df[\"tweet_clean\"]\ntarget = df[\"encoded_target\"]\ntokenizer_obj=Tokenizer()\ntokenizer_obj.fit_on_texts(texts)\nsequences=tokenizer_obj.texts_to_sequences(texts)\n\ntweet_pad = pad_sequences(sequences,\n                          truncating='post',\n                          padding='post')\n\nvocab_length = len(tokenizer_obj.word_index) + 1\nlongest_train = max(texts, key=lambda sentence: len(word_tokenize(sentence)))\nlength_long_sentence = len(word_tokenize(longest_train))","5aff130e":"# Split data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    tweet_pad, \n    target, \n    test_size=0.25\n)","ac13c62a":"embeddings_dictionary = dict()\nembedding_dim = 100\n\n# Load GloVe 100D embeddings\nwith open('..\/input\/glove6b100dtxt\/glove.6B.100d.txt') as fp:\n    for line in fp.readlines():\n        records = line.split()\n        word = records[0]\n        vector_dimensions = np.asarray(records[1:], dtype='float32')\n        embeddings_dictionary [word] = vector_dimensions\n        \nembedding_matrix = np.zeros((vocab_length, embedding_dim))\n\nfor word, index in tokenizer_obj.word_index.items():\n    embedding_vector = embeddings_dictionary.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[index] = embedding_vector\n        \nembedding_matrix","79ecb42d":"# Model from https:\/\/www.kaggle.com\/mariapushkareva\/nlp-disaster-tweets-with-glove-and-lstm\/data\n\ndef glove_lstm():\n    model = Sequential()\n    \n    model.add(Embedding(\n        input_dim=embedding_matrix.shape[0], \n        output_dim=embedding_matrix.shape[1], \n        weights = [embedding_matrix], \n        input_length=length_long_sentence\n    ))\n    \n    model.add(Bidirectional(LSTM(\n        length_long_sentence, \n        return_sequences = True, \n        recurrent_dropout=0.2\n    )))\n    \n    model.add(GlobalMaxPool1D())\n    model.add(BatchNormalization())\n    model.add(Dropout(0.5))\n    model.add(Dense(length_long_sentence, activation = \"relu\"))\n    model.add(Dropout(0.5))\n    model.add(Dense(length_long_sentence, activation = \"relu\"))\n    model.add(Dropout(0.5))\n    model.add(Dense(3, activation = \"softmax\"))\n    \n    loss = SparseCategoricalCrossentropy()\n    optimizer = Adam(learning_rate = 1e-3)\n\n    model.compile(optimizer=optimizer,loss=loss,metrics=['accuracy'])\n    \n    return model\n\nmodel = glove_lstm()\nmodel.summary()","f4e0bd73":"# Load the model and train!!\n\nmodel = glove_lstm()\n\ncheckpoint = ModelCheckpoint(\n    'model.h5', \n    monitor = 'val_loss', \n    verbose = 1, \n    save_best_only = True\n)\nreduce_lr = ReduceLROnPlateau(\n    monitor = 'val_loss', \n    factor = 0.2, \n    verbose = 1, \n    patience = 5,                        \n    min_lr = 0.001\n)\nhistory = model.fit(\n    X_train, \n    y_train, \n    epochs = 10,\n    batch_size = 32,\n    validation_data = (X_test, y_test),\n    verbose = 1,\n    callbacks = [reduce_lr, checkpoint]\n)\n","653a2ff9":"fig, ax = plt.subplots(1,2,figsize=(15,5))\nax[0].set_title(\"Loss\")\nax[0].plot(history.history['loss'], label=\"Training loss\")\nax[0].plot(history.history['val_loss'], label=\"validation loss\")\nax[0].legend(loc='best')\n\nax[1].set_title(\"Accuracy\")\nax[1].plot(history.history['accuracy'], label=\"Training accuracy\")\nax[1].plot(history.history['val_accuracy'],label=\"Validation accuracy\")\nax[1].legend(loc='best')\nplt.legend()\nplt.show()","fb46686f":"pred = np.argmax(model.predict(X_test), axis = 1)\nprint(f\"\\nClassification Report :\\n{classification_report(y_test, pred)}\")\nsns.heatmap(confusion_matrix(y_test, pred), annot=True, fmt='d', cmap=\"YlGnBu\")\nplt.show()","e1afe782":"<a id='linsvc'><\/a>\n<h2 style=\"text-align:center; background-color:#3CD8A5; color:white; border-radius: 50px 15px\">\n   Linear Support Vector Classification\n<\/h2>","edad5010":"### Cleaning tweets","f6cbb634":"### Train","57e1ed28":"### TweetAt","cc520c42":"### Unigrams","922b6417":"### Predict output word\n\n* **predict_output_word(context_words_list, topn=10)**\n> Get the probability distribution of the center word given context words.<br\/>\n> **Note :** this performs a CBOW-style propagation, even in SG models, and doesn\u2019t quite weight the surrounding words the same as in training \u2013 so it\u2019s just one crude way of using a trained model as a predictor.","46a49383":"Tweets are generally from **London** and the **United States**.","031b9441":"[Back to table of contents](#table_contents)\n\n<a id='eda'><\/a>\n<h1 style=\"text-align:center; background-color:#4DBDE3; color:white; border-radius: 50px 15px\">\n   EDA\n<\/h1>","30a132c1":"### Test function","6bef6c15":"<a id='lstm'><\/a>\n<h2 style=\"text-align:center; background-color:#3CD8A5; color:white; border-radius: 50px 15px\">\n   LSTM\n<\/h2>","8732b458":"* The target column is still **not balanced**.\n* Now, let's **encode** our target column.","3f580f26":"### Test","955c3a77":"### Bigrams","a8cd9976":"Some values are missing in the **Location** columns, but these columns will not be important to us.","c53b1f78":"<a id='tfidfvect'><\/a>\n<h2 style=\"text-align:center; background-color:#3CD8A5; color:white; border-radius: 50px 15px\">\n   TF-IDF Vectorizer\n<\/h2>\n\n**[Tf-idf vectorizer](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.TfidfVectorizer.html)** considers the global weighting of words.","df5e3c87":"<a id='load_data'><\/a>\n<h1 style=\"text-align:center; background-color:#4DBDE3; color:white; border-radius: 50px 15px\">\n    Load data\n<\/h1>","5f2b0d7c":"[Back to table of contents](#table_contents)\n\n<a id='data_preprocessing'><\/a>\n<h1 style=\"text-align:center; background-color:#4DBDE3; color:white; border-radius: 50px 15px\">\n   Data Preprocessing\n<\/h1>","529e3db8":"<a id='countvect'><\/a>\n<h2 style=\"text-align:center; background-color:#3CD8A5; color:white; border-radius: 50px 15px\">\n   Count Vectorizer\n<\/h2>\n\n**[Count vectorizer](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.CountVectorizer.html)** counts the number of times the words appear.","d56c85be":"### **[Latent Dirichlet Allocation - Gensim](https:\/\/radimrehurek.com\/gensim\/models\/ldamodel.html)**\n\n> Step-by-step explanation : https:\/\/www.machinelearningplus.com\/nlp\/topic-modeling-gensim-python\/\n\n* **gensim.utils.simple_preprocess(doc, deacc=False, min_len=2, max_len=15)**\n> Convert a document into a **list of tokens**.<br\/>\n> This lowercases, tokenizes, de-accents (optional).\n\n* **gensim.corpora.Dictionary().doc2bow(document, allow_update=False, return_missing=False)**\n> Convert document into the **bag-of-words (BoW)** format = list of (token_id, token_count) tuples.","25cb356f":"### Sentiment classification\n\nSince we have only **word vectors** and that **sentences are made of multiple words**, we need to **aggregate** them.","4a9760e0":"We can see that many tweets date from **May**.","293cd11f":"<a id='cbow'><\/a>\n<h2 style=\"text-align:center; background-color:#3CD8A5; color:white; border-radius: 50px 15px\">\n   Continuous Bag of Word (CBOW)\n<\/h2>\n\n> **Word2Vec** is composed of two distinct language models (CBOW and SG), optimized to quickly learn word vectors.<br\/>\n> **[Gensim](https:\/\/radimrehurek.com\/gensim\/models\/word2vec.html)** has one of Word2Vec fastest implementation.\n\n**CBOW** predicts a word given its context.","425800d9":"<a id='xgboost'><\/a>\n<h2 style=\"text-align:center; background-color:#3CD8A5; color:white; border-radius: 50px 15px\">\n   XGBoost\n<\/h2>","ac5a3f19":"<a id='libraries'><\/a>\n<h1 style=\"text-align:center; background-color:#4DBDE3; color:white; border-radius: 50px 15px\">\n    Import libraries\n<\/h1>","3c2f38f3":"<a id='top_words'><\/a>\n<h2 style=\"text-align:center; background-color:#3CD8A5; color:white; border-radius: 50px 15px\">\n   Top Words \n<\/h2>","3596616a":"<a id='missing_val'><\/a>\n<h2 style=\"text-align:center; background-color:#3CD8A5; color:white; border-radius: 50px 15px\">\n   Missing Values\n<\/h2>","fb53f120":"### Most similar words\n\n* **most_similar(positive=None, negative=None, topn=10, ...)**\n> Find the top-N most similar keys. Positive keys contribute positively towards the similarity, negative keys negatively.<br\/>\n> This method computes **cosine similarity** between a simple mean of the projection weight vectors of the given keys and the vectors for each key in the model.<br\/>\nThe most famous exemple is: `vec(king) - vec(man) + vec(woman) => vec(queen)`","b8953615":"[Back to table of contents](#table_contents)\n\n<a id='tokens_vis'><\/a>\n<h1 style=\"text-align:center; background-color:#4DBDE3; color:white; border-radius: 50px 15px\">\n   Tokens visualization\n<\/h1>","2c2d3ae7":"**It seems that our function works !**","c4be7969":"### Lemmatizer\n* **[spacy.load()](https:\/\/spacy.io\/api\/top-level#spacy.load)**\n> Load a pipeline using the name of an installed package, a string path or a Path-like object.\n\n**Useful links :** \n* https:\/\/spacy.io\/usage\/processing-pipelines\n* https:\/\/spacy.io\/api\/token#attributes","60e4edc4":"* **log_perplexity(chunk, total_docs=None)**\n> Calculate and return per-word likelihood bound, using a chunk of documents as evaluation corpus.\n\n* **[CoherenceModel()](https:\/\/radimrehurek.com\/gensim\/models\/coherencemodel.html).get_coherence()**\n> Get coherence value based on pipeline parameters.","bbdbb28e":"**Let's add some others data :**\n* **Original** (not balanced data)\n* **Undersampling :** Random undersampling of **majority class to middle class**\n* **Under+SMOTE :** Random undersampling of **majority class to middle class** + Oversampling (*BorderlineSMOTE*) of **minority class to middle class**","f78ea619":"[Back to table of contents](#table_contents)\n\n<a id='modeling'><\/a>\n<h1 style=\"text-align:center; background-color:#4DBDE3; color:white; border-radius: 50px 15px\">\n   Modeling\n<\/h1>","768dde08":"<h1 style=\"text-align:center; background-color:#4DBDE3; font-weight:bold; color:white;border-radius: 50px 15px\">Coronavirus tweets<\/h1>","90ccdb84":"### Predict output word\n\n* **predict_output_word(context_words_list, topn=10)**\n> Get the probability distribution of the center word given context words.<br\/>\n> **Note :** this performs a CBOW-style propagation, even in SG models, and doesn\u2019t quite weight the surrounding words the same as in training \u2013 so it\u2019s just one crude way of using a trained model as a predictor.","61d2f500":"<a id='lsa'><\/a>\n<h2 style=\"text-align:center; background-color:#3CD8A5; color:white; border-radius: 50px 15px\">\n   Latent semantic analysis (LSA)\n<\/h2>","1763cb39":"As before, **unigrams** and **unigrams+bigrams** are the most efficient.","67162ce8":"[Back to table of contents](#table_contents)\n\n<a id='vectorization'><\/a>\n<h1 style=\"text-align:center; background-color:#4DBDE3; color:white; border-radius: 50px 15px\">\n   Vectorization\n<\/h1>\n\nThere are different ways to vectorize a text, for example, by frequency or by prediction :\n\n* **by frequency :**\n    * **CountVectorizer (scikit-learn) :**\n        * counts the number of times the words appear\n        * favors the most frequent words\n        * ignores a bit the rare words that could have been interesting<br\/><br\/>\n    * **TfidfVectorizer (scikit-learn) :**\n        * considers the global weighting of words\n        * penalizes the most frequent words\n        * does not understand the semantic meaning of words <br\/><br\/>\n* **by prediction :**\n    * Solves the problem of the **semantic meaning** of words\n    * Uses **embeddings** to vectorize texts before classification<br\/>\n    * **Continuous Bag of Word (CBOW) :**\n        * predicts a **word given its context**<br\/>\n    * **Skip-Gram (SG) :**\n        * predicts a **context given a word**\n\n","0f9bd75f":"<a id='kmeans'><\/a>\n<h2 style=\"text-align:center; background-color:#3CD8A5; color:white; border-radius: 50px 15px\">\n   KMeans\n<\/h2>","308f0773":"<a id='class_rebalancing'><\/a>\n<h2 style=\"text-align:center; background-color:#3CD8A5; color:white; border-radius: 50px 15px\">\n   Class rebalancing\n<\/h2>\n\n> **[Distribution of classes](#target_process) : { 0 : 15398 , 1 : 7713 , 2 : 18046 }**\n\nSince our **classes are not balanced**, we have to try to remedy this so that the models can learn properly.<br\/>\n\n___\n\nTo deal with imbalanced datasets, we can do **undersampling** or **oversampling** techniques, as :\n\n* **[Undersampling](https:\/\/imbalanced-learn.org\/stable\/references\/under_sampling.html) :** *RandomUnderSampler, NearMiss, EditedNearestNeighbours, AllKNN, TomekLinks, ...*\n* **[Oversampling](https:\/\/imbalanced-learn.org\/stable\/references\/over_sampling.html) :** *RandomOverSampler, SMOTE, ADASYN, SMOTENC, BorderlineSMOTE, ...*\n* **[Combine under and over sampling](https:\/\/imbalanced-learn.org\/stable\/references\/combine.html) :** *SMOTEENN, SMOTETomek*\n> *Useful article : https:\/\/medium.com\/analytics-vidhya\/re-sampling-imbalanced-training-corpus-for-sentiment-analysis-c9dc97f9eae1*\n\n* For **data augmentation in NLP**, there are also : *Synonym Replacement, Random Deletion, Random Swap, Random Insertion, ...*\n> *You can take a look at this notebook : https:\/\/www.kaggle.com\/swarajshinde\/eda-data-augmentation-techniques-for-text-nlp*\n\n---\n\nSo, we will try to see which techniques will work best for our case with LinearSVC.","43c1ecd3":"<a id='pretreatment'><\/a>\n<h2 style=\"text-align:center; background-color:#3CD8A5; color:white; border-radius: 50px 15px\">\n   Pretreatment \/ Corpus cleaning\n<\/h2>","e7ae2b59":"<a id='learn_embeddings'><\/a>\n<h2 style=\"text-align:center; background-color:#3CD8A5; color:white; border-radius: 50px 15px\">\n   Testing the learned embeddings\n<\/h2>\n\nIs **great** really closer to **good** than to **bad** ?","7c29851f":"**To review different concepts in NLP, I looked at different notebooks and I really liked this one :**\n* https:\/\/www.kaggle.com\/andreshg\/commonlit-a-complete-analysis\n\n**Go take a look if you have time !**","3efd2423":"<a id='wordcloud'><\/a>\n<h2 style=\"text-align:center; background-color:#3CD8A5; color:white; border-radius: 50px 15px\">\n   WordCloud \n<\/h2>","7f3b79e8":"<a id='target_process'><\/a>\n<h2 style=\"text-align:center; background-color:#3CD8A5; color:white; border-radius: 50px 15px\">\n   Target Processing\n<\/h2>\n\nTo make the prediction easier, we will reduce the number of classes for our target by renaming some classes as :\n* Extremely Positive -> Positive\n* Extremely Negative -> Negative","c3dd8483":"Thanks to **@AndresHG** for the following code.\n\n* **[tf.keras.preprocessing.text.Tokenizer](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/preprocessing\/text\/Tokenizer)**\n> Text tokenization utility class.\n\n* **[tf.keras.preprocessing.sequence.pad_sequences](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/preprocessing\/sequence\/pad_sequences)**\n> Pads sequences to the same length.","6df47630":"[Back to table of contents](#table_contents)\n\n<a id='clustering'><\/a>\n<h1 style=\"text-align:center; background-color:#4DBDE3; color:white; border-radius: 50px 15px\">\n   Clustering\n<\/h1>","2780b38b":"As we can see, this dataset is **unbalanced**.","8a00ddb9":"<a id='lda'><\/a>\n<h2 style=\"text-align:center; background-color:#3CD8A5; color:white; border-radius: 50px 15px\">\n   Latent Dirichlet Allocation (LDA)\n<\/h2>","da22d446":"#### **Results (F1-score)**","ce10c5ff":"* **[pyLDAvis.gensim_models](https:\/\/github.com\/bmabey\/pyLDAvis\/blob\/master\/pyLDAvis\/gensim_models.py).prepare(topic_model, corpus, dictionary, doc_topic_dist=None, **kwargs)**\n> Transforms the Gensim TopicModel and related corpus and dictionary into the data structures needed for the visualization.","911db69d":"The data augmentation with **RandomOverSampler** looks good.","e48a1718":"<a id='table_contents'><\/a>\n<h1 style=\"text-align:center; background-color:#4DBDE3; color:white; border-radius: 50px 15px\">\n   \ud83d\udccb&ensp; Table of Contents &ensp;\ud83d\udccb \n<\/h1>\n\n* **[Import libraries](#libraries)**\n* **[Load data](#load_data)**\n* **[EDA](#eda)**\n    * [Missing values](#missing_val)\n    * [Target visualization](#target_vis)\n    * [Variables visualization](#var_vis)\n* **[Data Preprocessing](#data_preprocessing)**\n    * [Pretreatment \/ Corpus cleaning](#pretreatment)\n    * [Target processing](#target_process)\n* **[Tokens visualization](#tokens_vis)**\n    * [Top Words](#top_words)\n    * [WordCloud](#wordcloud)\n* **[Clustering](#clustering)**\n    * [KMeans](#kmeans)\n    * [Latent semantic analysis (LSA)](#lsa)\n    * [Latent Dirichlet Allocation (LDA)](#lda)\n* **[Vectorization](#vectorization)**\n    * [Count Vectorizer](#countvect)\n    * [TF-IDF Vectorizer](#tfidfvect)\n    * [Continuous Bag of Word (CBOW)](#cbow)\n    * [Skip-Gram (SG)](#sg)\n    * [Test learnt embeddings](#learn_embeddings)\n* **[Modeling](#modeling)**\n    * [Class rebalancing (Re-sampling)](#class_rebalancing)\n    * [XGBoost](#xgboost)\n    * [Linear Support Vector Classification](#linsvc)\n    * [LSTM](#lstm)","b9aab725":"### Trigrams","8e49d9cb":"<a id='sg'><\/a>\n<h2 style=\"text-align:center; background-color:#3CD8A5; color:white; border-radius: 50px 15px\">\n   Skip-Gram (SG)\n<\/h2>\n\n**SG** predicts a context given a word.","c7e6814f":"<a id='target_vis'><\/a>\n<h2 style=\"text-align:center; background-color:#3CD8A5; color:white; border-radius: 50px 15px\">\n   Target visualization\n<\/h2>","347ff181":"* **print_topics(num_topics=20, num_words=10)**\n> Get the most significant topics (alias for show_topics() method)","f3e16113":"### **[Latent Dirichlet Allocation : sklearn](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.decomposition.LatentDirichletAllocation.html)**","a53241f1":"### OriginalTweet","5a2854f6":"<a id='var_vis'><\/a>\n<h2 style=\"text-align:center; background-color:#3CD8A5; color:white; border-radius: 50px 15px\">\n   Variables visualization\n<\/h2>","21b91373":"### Most similars words\n\n* **most_similar(positive=None, negative=None, topn=10, ...)**\n> Find the top-N most similar keys. Positive keys contribute positively towards the similarity, negative keys negatively.<br\/>\n> This method computes **cosine similarity** between a simple mean of the projection weight vectors of the given keys and the vectors for each key in the model.<br\/>\nThe most famous exemple is: `vec(king) - vec(man) + vec(woman) => vec(queen)`.","7b8b22fc":"We can see that we get the best score by taking into account **Unigrams + Bigrams**.<br\/>\nTaking just the unigrams is also good.","30092433":"### Location"}}