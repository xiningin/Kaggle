{"cell_type":{"eecd21f6":"code","7e883f82":"code","4a5ffe30":"code","fda1074e":"code","7f86dcca":"code","a0b60d70":"code","9b15d978":"code","240727e8":"code","e088c985":"code","cefc9256":"code","28b6d79e":"code","7e7ce253":"code","32d0f12e":"markdown","b4bd23b2":"markdown","1c5489d2":"markdown","d332f82d":"markdown","2e5d1259":"markdown","1d2aebd4":"markdown","5e2e0021":"markdown","47d6f1e1":"markdown","97cf59a4":"markdown","d1a78073":"markdown","44cb08bb":"markdown","62ffc129":"markdown"},"source":{"eecd21f6":"import pandas  as pd\nimport numpy   as np\nfrom sklearn.metrics import accuracy_score\n\nsolution   = pd.read_csv('..\/input\/submission-solution\/submission_solution.csv')\ntrain_data = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_data  = pd.read_csv('..\/input\/titanic\/test.csv')\nX_test     = pd.get_dummies(test_data)","7e883f82":"predictions = np.round(np.random.random((len(test_data)))).astype(int)\nprint(\"The score is %.5f\" % accuracy_score( solution['Survived'] , predictions ) )","4a5ffe30":"predictions = np.zeros((418), dtype=int)\nprint(\"The score is %.5f\" % accuracy_score( solution['Survived'] , predictions ) )","fda1074e":"from sklearn.metrics import balanced_accuracy_score\nprint(\"The balanced accuracy score is %.5f\" % balanced_accuracy_score( solution['Survived'] , predictions ) )","7f86dcca":"predictions = np.zeros((418), dtype=int)\n# now use our model\nsurvived_df = X_test[(X_test[\"Sex_female\"]==1)]\n\nfor i in survived_df.index:\n    predictions[i] = 1 # the 1's are now the survivors\n    \nprint(\"The score is %.5f\" % accuracy_score( solution['Survived'] , predictions ) )","a0b60d70":"predictions = np.zeros((418), dtype=int)\n# now use our model\nsurvived_df = X_test[((X_test[\"Pclass\"] ==1)|(X_test[\"Pclass\"] ==2)) & (X_test[\"Sex_female\"]==1 )]\n\nfor i in survived_df.index:\n    predictions[i] = 1 # the 1's are now the survivors\n    \nprint(\"The score is %.5f\" % accuracy_score( solution['Survived'] , predictions ) )","9b15d978":"predictions = np.zeros((418), dtype=int)\n# now use our model\nsurvived_df = X_test[((X_test[\"Embarked_S\"] ==1)|(X_test[\"Embarked_C\"] ==1)) & (X_test[\"Sex_female\"]==1 )]\n\nfor i in survived_df.index:\n    predictions[i] = 1 # the 1's are now the survivors\n    \nprint(\"The score is %.5f\" % accuracy_score( solution['Survived'] , predictions ) )","240727e8":"test = test_data","e088c985":"test['Boy'] = (test.Name.str.split().str[1] == 'Master.').astype('int')\ntest['Survived'] = [1 if (x == 'female') else 0 for x in test['Sex']]     \ntest.loc[(test.Boy == 1), 'Survived'] = 1                                 \ntest.loc[((test.Pclass == 3) & (test.Embarked == 'S')), 'Survived'] = 0","cefc9256":"predictions = test['Survived']\nprint(\"The score is %.5f\" % accuracy_score( solution['Survived'] , predictions ) )","28b6d79e":"male = train_data.query('Sex==\"male\"')\nmale_ticket_survivability = male.groupby('Ticket').Survived.mean()\n\nfemale = train_data.query('Sex==\"female\"')\nfemale_ticket_survivability = female.groupby('Ticket').Survived.mean()\n\ndef predict(passenger:pd.Series):\n    if passenger.Sex == 'male':\n        if male_ticket_survivability.get(passenger.Ticket, 0) > .5:\n            return 1\n        else:\n            return 0\n    else: # female\n        ticket_survivability = female_ticket_survivability.get(passenger.Ticket)\n        if ticket_survivability is not None:\n            if ticket_survivability < .5:\n                return 0\n            else:\n                return 1\n        elif passenger.Pclass == 3 and passenger.SibSp+passenger.Parch > 3:\n            return 0\n        return 1\n\npredictions = [int(predict(test_data.iloc[i])) for i in range(len(test_data))]\nprint(\"The score is %.5f\" % accuracy_score( solution['Survived'] , predictions ) )","7e7ce253":"output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('submission.csv', index=False)","32d0f12e":"# 78.5%\nThis masterpiece is from the wonderful notebook [\"Three lines of code for Titanic Top 20%\"](https:\/\/www.kaggle.com\/vbmokin\/three-lines-of-code-for-titanic-top-20) written by kaggle Master [Vitalii Mokin](https:\/\/www.kaggle.com\/vbmokin). His model is the following:\n\n* **All the women survived, and all the men died**\n* **All boys ('Master') from 1st and 2nd class survived**\n* **Everybody in 3rd class that embarked at Southampton ('S') died.**\n\nI shall make a copy of the `test_data` dataframe to maintain the original code as it is in his notebook, as well as preserving `test_data` for future small and simple models:\n","b4bd23b2":"This model is what is known as the [Zero Rule](https:\/\/machinelearningcatalogue.com\/algorithm\/alg_zero-rule.html) classifier (aka. **ZeroR** or **0-R**), and it simply consists of the majority class of the dataset. It is against this baseline (and not the random model above) that one should compare the performance of all other models based on this data. Any model that does not beat this score has something *very* wrong with it.\n\nThe **no survivors** model is also useful in another respect; it provides us with an indication as to whether the data is imbalanced or not. If the data were perfectly 'balanced' we would have as many survivors as those who did not survive, and the [accuracy score](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.accuracy_score.html) would be `50%`. However, for highly imbalanced datasets then the accuracy score evaluation metric can be misleading. For example, imagine a scenario in which only 42 passengers in the test data survived, then the **no survivors** model would have an accuracy score of `90%` before we even start modelling. Clearly in such a situation the accuracy score is no longer fit for purpose and an alternative must be found.\n\nIn such a case we can use the scikit-learn [balanced_accuracy_score](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.balanced_accuracy_score.html), which is calculated using \n\n$$\\texttt{balanced-accuracy} = \\frac{1}{2}\\left( \\frac{TP}{TP + FN} + \\frac{TN}{TN + FP}\\right )$$\n\ni.e. the arithmetic mean of sensitivity (true positive rate) and specificity (true negative rate). \n\nFor example:","1c5489d2":"# $\\approx$50%\nThis is the average result of the magnificently minimalist notebook [\"Titanic Random Survival Prediction\"](https:\/\/www.kaggle.com\/tarunpaparaju\/titanic-random-survival-prediction) by kaggle Grandmaster [Tarun Paparaju](https:\/\/www.kaggle.com\/tarunpaparaju):","d332f82d":"# 80.4%\nThe notebook [\"Titanic using Name only\"](https:\/\/www.kaggle.com\/cdeotte\/titanic-using-name-only-0-81818) by [Chris Deotte](https:\/\/www.kaggle.com\/cdeotte), as the name suggests, only makes use of the names data. It is somewhat long, so I refer the interested reader to examine the notebook itself. It consists of two rules:\n\n* Predict all males die except boys in families where all females and boys live.\n* Predict all females live except those in families where all females and boys die.","2e5d1259":"we can see that any imbalance has now been compensated for. That said, in this case `62%` isn't too bad, so we shall continue using the standard accuracy score metric.\n# 76.6%\n\n### Only the women survived\nThis is essentially the `gender_submission.csv` file that comes free with the competition.","1d2aebd4":"# 62.2%\n### No survivors\n\nThis model represents the most terrible scenario; there are no survivors. This model actually correctly guesses the fate of 260 of the 418 passengers, which is a stark reminder of the tragedy that was the Titanic.","5e2e0021":"# 77.5%\n### Only women from 1st and 2nd class survive:","47d6f1e1":"### Only women from who embarked in either Cherbourg or Southampton survive:","97cf59a4":"# How to create a submission.csv\nIf you wish to submit any of these predictions to the competition simply use this snippet of code to output a `submission.csv` file:","d1a78073":"# 78.7%\nThe following model is thanks to [Thariq Nugrohotomo](https:\/\/www.kaggle.com\/thariqnugrohotomo), published in the aptly-named notebook [\"without machine learning - part 3b\"](https:\/\/www.kaggle.com\/thariqnugrohotomo\/without-machine-learning-part-3b):","44cb08bb":"# Titanic: Applying the [KISS principle](https:\/\/en.wikipedia.org\/wiki\/KISS_principle) (Keep It Small and Simple)\n\n> *KISS is a design principle noted by the U.S. Navy in 1960. The KISS principle states that most systems work best if they are kept simple rather than made complicated; therefore, simplicity should be a key goal in design, and unnecessary complexity should be avoided.*\n\n## Explainable AI\nRecently the [National Institute of Standards and Technology (NIST)](https:\/\/www.nist.gov\/) published a draft paper [\"Four Principles of Explainable Artificial Intelligence\"](https:\/\/nvlpubs.nist.gov\/nistpubs\/ir\/2020\/NIST.IR.8312-draft.pdf), which are as follows:\n\n* **Explanation:** *Systems deliver accompanying evidence or reason(s) for all outputs.*\n* **Meaningful:** *Systems provide explanations that are understandable to individual users.*\n* **Explanation Accuracy:** *The explanation correctly reflects the system\u2019s process for generating the output.* \n* **Knowledge Limits:** *The system only operates under conditions for which it was designed or when the system reaches a sufficient confidence in its output.*\n\nIn this notebook we list a selection of simple but meaningful models, *i.e.* <font color='red'>models that you can explain to your boss whilst in the elevator.<\/font>\n\n## Explainability and the GDPR\nBeing able to easily explain how a model works, or how a decision was made based on the model, is not a mere intelectual nicety; in fact the [EU General Data Protection Regulation (GDPR) 2016\/679](https:\/\/eur-lex.europa.eu\/eli\/reg\/2016\/679), Article 15(1)(h) states:\n\n> \"*The data subject shall have the right to obtain... ...meaningful information about the logic involved, as well as the significance and the envisaged consequences of such processing*\"\n\nalso, in Article 22:\n\n> \"*The data subject shall have the right to obtain... ...human intervention on the part of the controller, to express his or her point of view and to contest the decision.*\"\n\nIn order to comply with this, the data scientist must be able to clearly explain how any decision was originally arrived at. \n\nNon-compliance with the GDPR by a company can result in serious consequences, and it is part of the job of a data scientist to mitigate such risks for their employers. (For those interested the website [GDPR Enforcement Tracker](https:\/\/www.enforcementtracker.com\/) has a partial list of fines that have been imposed).\n\n## How the scores are calculated\nIn order to avoid submitting each of these models to the competition for scoring I shall make use of the [ground truth file](https:\/\/www.kaggle.com\/martinglatz\/submission-solution), in conjunction with the [scikit-learn accuracy_score](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.accuracy_score.html).\n\n## Set up the essentials","62ffc129":"## Please feel free to mention more KISS models to add!"}}