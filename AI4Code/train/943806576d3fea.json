{"cell_type":{"071356f0":"code","cd0c56de":"code","f9c44747":"code","a4a81750":"code","36b7a1f8":"code","50fe58e0":"code","a740a0bb":"code","95c8e809":"code","99a3399b":"code","36ca80c2":"code","2710ccae":"code","ca745d27":"code","26235536":"code","4d48509e":"code","df3c8565":"code","6c250cf0":"code","9553076d":"code","ab402dff":"code","3fa4533d":"code","19061bfc":"code","f7758840":"code","104720ab":"code","908c0716":"code","8c185c68":"code","6f5e6245":"code","f55d0e5e":"code","e04108b5":"markdown","8549b5e7":"markdown","4f62a497":"markdown","0e8997ec":"markdown","279f64aa":"markdown"},"source":{"071356f0":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\n\n%matplotlib inline","cd0c56de":"data = pd.read_csv(\"\/kaggle\/input\/2020-general-election-polls\/county_statistics.csv\")","f9c44747":"# mostly numerical data apart from the state and county cols\n# a lot of null entries in the numerical data, will need to impute median values into the data\ndata.info()","a4a81750":"# there are a lot of null values, we deal with these by imputing the median values of each feature\nfrom sklearn.impute import SimpleImputer\nimputer = SimpleImputer(strategy=\"median\")\nnumerical_data = data[['lat', 'long', 'cases', 'deaths', 'TotalPop', 'Men', 'Women', \n          'Hispanic', 'White', 'Black', 'Native', 'Asian', 'Pacific', 'VotingAgeCitizen', \n          'Income', 'IncomeErr', 'IncomePerCap', 'IncomePerCapErr', 'Poverty', 'ChildPoverty', \n          'Professional', 'Service', 'Office', 'Construction', 'Production', 'Drive', 'Carpool', \n          'Transit', 'Walk', 'OtherTransp', 'WorkAtHome', 'MeanCommute', 'Employed', 'PrivateWork', \n          'PublicWork', 'SelfEmployed', 'FamilyWork', 'Unemployment','percentage16_Donald_Trump',\n          'percentage16_Hillary_Clinton','total_votes16', 'votes16_Donald_Trump',\n          'votes16_Hillary_Clinton', 'percentage20_Donald_Trump', 'percentage20_Joe_Biden',\n          'total_votes20', 'votes20_Donald_Trump','votes20_Joe_Biden']]\nimputer.fit(numerical_data)","36b7a1f8":"imputed_data = imputer.transform(numerical_data)\nimputed_data = pd.DataFrame(imputed_data, columns=numerical_data.columns,index=numerical_data.index)","50fe58e0":"# adding the non-nuumerical columns back in\nimputed_data[['county', 'state']] = data[['county', 'state']]","a740a0bb":"X = imputed_data[['county', 'state', 'lat', 'long', 'cases', 'deaths', 'TotalPop', 'Men', 'Women', \n          'Hispanic', 'White', 'Black', 'Native', 'Asian', 'Pacific', 'VotingAgeCitizen', \n          'Income', 'IncomeErr', 'IncomePerCap', 'IncomePerCapErr', 'Poverty', 'ChildPoverty', \n          'Professional', 'Service', 'Office', 'Construction', 'Production', 'Drive', 'Carpool', \n          'Transit', 'Walk', 'OtherTransp', 'WorkAtHome', 'MeanCommute', 'Employed', 'PrivateWork', \n          'PublicWork', 'SelfEmployed', 'FamilyWork', 'Unemployment','percentage16_Donald_Trump','percentage16_Hillary_Clinton','total_votes16', \n          'votes16_Donald_Trump','votes16_Hillary_Clinton']]\ny = imputed_data[['county', 'state', 'percentage20_Donald_Trump',\n          'percentage20_Joe_Biden', 'total_votes20', 'votes20_Donald_Trump','votes20_Joe_Biden']]","95c8e809":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","99a3399b":"X_train.describe()","36ca80c2":"# unnecessary id col which can be removed\nX_train.head()","2710ccae":"# convert some cols to percentages to remove any implicit dependence on other variables\nX_train['Men%'] = X_train['Men'] \/ X_train['TotalPop']\nX_train['cases%'] = X_train['cases'] \/ X_train['TotalPop']\nX_train['deaths%'] = X_train['deaths'] \/ X_train['TotalPop']\nX_train['Employed%'] = X_train['Employed'] \/ X_train['TotalPop']","ca745d27":"full_train = pd.concat([X_train,y_train],axis=1)\ncorr_matrix = full_train.corr()\nplt.figure(figsize = (10,8))\nax = sns.heatmap(abs(corr_matrix))","26235536":"# variables that produce interesting correlations to Trump's percentage of the vote are:\n# percentage16_Donald_Trump (though obvious), Construction, Transit, Production, Self-employed,\n# White, Asian, IncomePerCap \ncorr_matrix['percentage20_Donald_Trump'].sort_values()","4d48509e":"sns.countplot(data=full_train,x='percentage20_Donald_Trump')","df3c8565":"# relatively even spread of percentages apart from the much more numerous 60.4%. After looking at the \n# imputation statistics, this is the median of the data that was inserted at the start of the project.\nfull_train[['percentage20_Donald_Trump']].value_counts()","6c250cf0":"# average value included in the imputer stats, so we know the reason for this very common value\nimputer.statistics_","9553076d":"# create histograms for each feature to determine any skews or potential weaknesses in the data\nlarge_features = {'lat', 'long', 'TotalPop', 'Men', 'Women', 'Hispanic', 'White', 'Black', \n                  'Native', 'Asian', 'Pacific', 'VotingAgeCitizen', 'Income', 'IncomeErr', \n                  'IncomePerCap', 'IncomePerCapErr', 'Poverty', 'ChildPoverty', 'Professional', \n                  'Service', 'Office', 'Construction', 'Production', 'Drive', 'Carpool', 'Transit',\n                  'Walk', 'OtherTransp', 'WorkAtHome', 'MeanCommute', 'Employed', 'PrivateWork', \n                  'PublicWork', 'SelfEmployed', 'FamilyWork', 'Unemployment', 'Men%', 'Employed%'}\nlarge_features = list(large_features)\nrows = 40\ncols = 1\nfig = plt.figure(figsize=(15, 200))\nfor index, feature in enumerate(large_features):\n    ax = fig.add_subplot(rows, cols, index+1)\n    ax.grid(axis=\"y\", linewidth=1, zorder=0)\n    sns.histplot(x=feature, data=full_train, alpha=1, linewidth=1.5, zorder=2,bins=50)\n    ax.set_xlabel(feature, fontsize=14, fontfamily=\"serif\", labelpad=7)\n    ax.locator_params(axis='x', nbins=10)\n\nfig.text(x=0.05, y=1.01, s=\"Large Features Distributions\", fontsize=22, fontweight=\"bold\")\nfig.tight_layout(w_pad=2, h_pad=1.5)\nfig.show()","ab402dff":"import tensorflow as tf\nfrom tensorflow import keras\nfrom scipy.stats import reciprocal\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom keras.callbacks import EarlyStopping","3fa4533d":"# adding the new features the test set\nX_test['Men%'] = X_test['Men'] \/ X_test['TotalPop']\nX_test['cases%'] = X_test['cases'] \/ X_test['TotalPop']\nX_test['deaths%'] = X_test['deaths'] \/ X_test['TotalPop']\nX_test['Employed%'] = X_test['Employed'] \/ X_test['TotalPop']","19061bfc":"#removed -> 'lat', 'long', 'cases', 'deaths',\nnew_cols = ['TotalPop','Men', 'Women','Employed', 'Hispanic', 'White', 'Black', \n                  'Native', 'Asian', 'Pacific', 'VotingAgeCitizen', 'Income', 'IncomeErr', \n                  'IncomePerCap', 'IncomePerCapErr', 'Poverty', 'ChildPoverty', 'Professional', \n                  'Service', 'Office', 'Construction', 'Production', 'Drive', 'Carpool', 'Transit',\n                  'Walk', 'OtherTransp', 'WorkAtHome', 'MeanCommute', 'PrivateWork', \n                  'PublicWork', 'SelfEmployed', 'FamilyWork', 'Unemployment','Men%', 'Employed%',\n                  'percentage16_Donald_Trump','percentage16_Hillary_Clinton','total_votes16',\n                  'votes16_Donald_Trump','votes16_Hillary_Clinton']\nX_train_filtered = X_train[new_cols]\nX_test_filtered = X_test[new_cols]","f7758840":"# function to generalise building network, will use with a random search to time-tune parameters\ndef build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3, input_shape=[len(X_train_filtered.columns),]):\n    model = keras.Sequential()\n    model.add(keras.layers.InputLayer(input_shape))\n    model.add(keras.layers.BatchNormalization())\n    for layer in range(n_hidden):\n        model.add(keras.layers.Dense(n_neurons,activation=\"relu\"))\n    model.add(keras.layers.Dense(1))\n    optimizer = keras.optimizers.SGD(learning_rate=learning_rate)\n    model.compile(optimizer=optimizer, metrics=['accuracy'], loss=\"mean_squared_error\")\n    return model\n\n# wrap function in keras wrapper\nkeras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)\n\n# early stopping callback\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=5)\n\n# run search on model\ndef run_model_search():\n    param_distribs = {\n        \"n_hidden\": [0, 1, 2, 3, 4],\n        \"n_neurons\": np.arange(1,150),\n        \"learning_rate\": reciprocal(3e-4,3e-2),\n        }\n    \n    rnd_search_cv = RandomizedSearchCV(keras_reg, param_distribs, n_iter=30, cv=3, verbose=0)\n    \n    rnd_search_cv.fit(\n        np.asarray(X_train_filtered).astype(\"float32\"),\n        np.asarray(y_train['percentage20_Donald_Trump']).astype(\"float32\"),\n        epochs=15, validation_split=0.3,callbacks=[es], verbose=0)\n    \n    return rnd_search_cv.best_params_","104720ab":"best_params = run_model_search()","908c0716":"# final model based on optimum input values from random search\nfinal_model = build_model(n_hidden=best_params['n_hidden'], \n                          n_neurons=best_params['n_neurons'], \n                          learning_rate=best_params['learning_rate'],\n                          input_shape=[len(X_train_filtered.columns),])","8c185c68":"final_model.summary()","6f5e6245":"history = final_model.fit(np.asarray(X_train_filtered).astype(\"float32\"),\n        np.asarray(y_train['percentage20_Donald_Trump']).astype(\"float32\"),\n        epochs=10, validation_split=0.3,callbacks=[es])","f55d0e5e":"# plotting loss function result against epoch\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss Value')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper right')\nplt.show()","e04108b5":"Model election data based on information by county including covid stats.\nData is from 2017 so if we focus on affecting factors of the 2020 election results, including the results of 2016.\n\nData source: https:\/\/www.kaggle.com\/etsc9287\/2020-general-election-polls","8549b5e7":"# Covid and Election Data","4f62a497":"# Exploratory Analysis","0e8997ec":"# Neural Network","279f64aa":"**Well distributed features:**\n\nlat, long, Office, Employed%, MeanCommute\n\n**Well distributed with some skew:**\n\nUnemployment, SelfEmployed, Professional, Production, Construction, Carpool, WorkAtHome, Poverty, Drive, IncomePerCap, White, PrivateWork, ChilePoverty, Service, PublicWork, Income, Mem%\n\n**Very tail heavy:**\n\nHispanic, Employed, deaths, cases, Walk, Native, Women, Transit, FamilyWork, Asian, OtherTransp, IncomeErr, TotalPop, VotingAgeCitizen, Pacific, Men, deaths%, IncomePerCapErr, Black, cases%\n\n\nThe data isnt perfect, with a lot of tails"}}