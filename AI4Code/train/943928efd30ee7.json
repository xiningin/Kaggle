{"cell_type":{"8331c70e":"code","7019d401":"code","d53891e3":"code","c7aecb21":"code","5ca4636f":"code","ad7d14cd":"code","06fb4fb5":"code","14666e00":"code","98ebaab4":"code","16a45b90":"code","5c5a3178":"code","9a33cae7":"code","ce6afcbe":"code","617b503e":"code","83e52302":"code","c39b919d":"code","a4c4ab47":"code","12589d40":"code","502935ba":"code","6688ac35":"markdown","615b6586":"markdown","0cc8f23f":"markdown","d5ac412c":"markdown","7ba90033":"markdown","2e2aa357":"markdown","058ece34":"markdown","9d8c8813":"markdown","55762ac2":"markdown","900411c8":"markdown","5656c891":"markdown","00039e9b":"markdown","92f7d4d5":"markdown","030eb865":"markdown","3f7a3229":"markdown","3d767997":"markdown","cc2c08a7":"markdown","f475667f":"markdown"},"source":{"8331c70e":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.layers import Dense, Activation,Dropout,Conv2D, MaxPooling2D,BatchNormalization\nfrom tensorflow.keras.optimizers import Adam, Adamax\nfrom tensorflow.keras.metrics import categorical_crossentropy\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Model, load_model, Sequential\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import imshow\nimport time\nimport os\nfrom PIL import Image\nfrom IPython.core.display import display, HTML\ndisplay(HTML(\"<style>.container { width:100% !important; }<\/style>\"))","7019d401":"import logging\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # FATAL\nlogging.getLogger('tensorflow').setLevel(logging.FATAL)","d53891e3":"dir=r'..\/input\/10-monkey-species'\ntrain_dir=r'..\/input\/10-monkey-species\/training\/training'\ntest_dir=r'..\/input\/10-monkey-species\/validation\/validation'\n","c7aecb21":"def print_in_color(txt_msg,fore_tupple,back_tupple,):\n    #prints the text_msg in the foreground color specified by fore_tupple with the background specified by back_tupple \n    #text_msg is the text, fore_tupple is foregroud color tupple (r,g,b), back_tupple is background tupple (r,g,b)\n    rf,gf,bf=fore_tupple\n    rb,gb,bb=back_tupple\n    msg='{0}' + txt_msg\n    mat='\\33[38;2;' + str(rf) +';' + str(gf) + ';' + str(bf) + ';48;2;' + str(rb) + ';' +str(gb) + ';' + str(bb) +'m' \n    print(msg .format(mat))\n    print('\\33[0m') # returns default print color to back to black\n    return","5ca4636f":"classes=os.listdir(train_dir) # class names are the names of the sub directories\nclass_count=len(classes) # determine number of classes\nbatch_size=80 # set training batch size\nrand_seed=123\nstart_epoch=0 # specify starting epoch\nepochs=20 # specify the number of epochs to run\nimg_size=224 # use 224 X 224 images compatible with mobilenet model\nlr=.01 # specify initial learning rate","ad7d14cd":"def get_bs(dir,b_max):\n    # dir is the directory containing the samples, b_max is maximum batch size to allow based on your memory capacity\n    # you only want to go through test and validation set once per epoch this function determines needed batch size ans steps per epoch\n    length=0\n    dir_list=os.listdir(dir)\n    for d in dir_list:\n        d_path=os.path.join (dir,d)\n        length=length + len(os.listdir(d_path))\n    batch_size=sorted([int(length\/n) for n in range(1,length+1) if length % n ==0 and length\/n<=b_max],reverse=True)[0]  \n    return batch_size,int(length\/batch_size)","06fb4fb5":"test_batch_size, test_steps=get_bs(test_dir,100)","14666e00":"train_gen=ImageDataGenerator(preprocessing_function=keras.applications.mobilenet.preprocess_input, horizontal_flip=True, rotation_range=40,\n                             width_shift_range=0.2,  height_shift_range=0.2, shear_range=0.2, validation_split=.2 ).flow_from_directory(\n    train_dir,  target_size=(img_size, img_size), batch_size=batch_size, seed=rand_seed, class_mode='categorical', subset='training')\n\n\nvalid_gen=ImageDataGenerator(preprocessing_function=keras.applications.mobilenet.preprocess_input,validation_split=.2) .flow_from_directory(train_dir, \n                    target_size=(img_size, img_size), batch_size=batch_size,\n                    class_mode='categorical',color_mode='rgb', shuffle=False, subset='validation')\ntest_gen=ImageDataGenerator(preprocessing_function=keras.applications.mobilenet.preprocess_input).flow_from_directory(test_dir,\n                    target_size=(img_size, img_size), batch_size=test_batch_size,\n                    class_mode='categorical',color_mode='rgb', shuffle=False )\ntest_file_names=test_gen.filenames  # save list of test files names to be used later\nlabels=test_gen.labels # save test labels to be used later\n\n","98ebaab4":"images,labels=next(train_gen)\nplt.figure(figsize=(20, 20))\nfor i in range(25):\n    plt.subplot(5, 5, i + 1)\n    image=(images[i]+1 )\/2\n    plt.imshow(image)\n    index=int(labels[i][1])\n    plt.title(classes[index], color='white')\n    plt.axis('off')\nplt.show()","16a45b90":"mobile = tf.keras.applications.mobilenet.MobileNet( include_top=False, input_shape=(img_size, img_size,3), pooling='max', weights='imagenet', dropout=.5) \nfor layer in mobile.layers:\n    layer.trainable=False\nx=mobile.layers[-1].output # this is the last layer in the mobilenet model the global max pooling layer\nx=keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001 )(x)\nx=Dense(512, activation='relu')(x)\nx=Dropout(rate=.3, seed = 123)(x)\nx=Dense(64, activation='relu')(x)\nx=Dropout(rate=.3, seed = 123)(x)\npredictions=Dense (len(classes), activation='softmax')(x)\nmodel = Model(inputs=mobile.input, outputs=predictions)    \n\nmodel.compile(Adamax(lr=lr), loss='categorical_crossentropy', metrics=['accuracy']) ","5c5a3178":"class LRA(keras.callbacks.Callback):\n    best_weights=model.get_weights() # set a class vaiable so weights can be loaded after training is completed\n    def __init__(self, patience=2, threshold=.95, factor=.5):\n        super(LRA, self).__init__()\n        self.patience=patience # specifies how many epochs without improvement before learning rate is adjusted\n        self.threshold=threshold # specifies training accuracy threshold when lr will be adjusted based on validation loss\n        self.factor=factor # factor by which to reduce the learning rate\n        self.lr=float(tf.keras.backend.get_value(model.optimizer.lr)) # get the initiallearning rate and save it in self.lr\n        self.highest_tracc=0.0 # set highest training accuracy to 0\n        self.lowest_vloss=np.inf # set lowest validation loss to infinity\n        self.count=0\n        msg='\\n Starting Training - Initializing Custom Callback'\n        print_in_color (msg, (244, 252, 3), (55,65,80))\n        \n    def on_epoch_end(self, epoch, logs=None):  # method runs on the end of each epoch\n        lr=float(tf.keras.backend.get_value(self.model.optimizer.lr)) # get the current learning rate\n        v_loss=logs.get('val_loss')  # get the validation loss for this epoch\n        acc=logs.get('accuracy')  # get training accuracy        \n        if acc < self.threshold: # if training accuracy is below threshold adjust lr based on training accuracy\n            if acc>self.highest_tracc: # training accuracy improved in the epoch\n                msg= f'\\n training accuracy improved from  {self.highest_tracc:7.2f} to {acc:7.2f} learning rate held at {lr:9.6f}'\n                print_in_color(msg, (0,255,0), (55,65,80))\n                self.highest_tracc=acc # set new highest training accuracy\n                LRA.best_weights=model.get_weights() # traing accuracy improved so save the weights\n                count=0 # set count to 0 since training accuracy improved\n                if v_loss<self.lowest_vloss:\n                    self.lowest_vloss=v_loss                    \n            else:  # training accuracy did not improve check if this has happened for patience number of epochs if so adjust learning rate\n                if self.count>=self.patience -1:\n                    self.lr= lr* self.factor # adjust the learning by factor\n                    tf.keras.backend.set_value(model.optimizer.lr, self.lr) # set the learning rate in the optimizer\n                    self.count=0 # reset the count to 0\n                    if v_loss<self.lowest_vloss:\n                        self.lowest_vloss=v_loss\n                    msg=f'\\nfor epoch {epoch +1} training accuracy did not improve for {self.patience } consecutive epochs, learning rate adjusted to {lr:9.6f}'\n                    print_in_color(msg, (255,0,0), (55,65,80))\n                else:\n                    self.count=self.count +1\n                    msg=f'\\nfor  epoch {epoch +1} training accuracy did not improve, patience count incremented to {self.count}'\n                    print_in_color(msg, (255,255,0), (55,65,80))\n        else: # training accuracy is above threshold so adjust learning rate based on validation loss\n            if v_loss< self.lowest_vloss: # check if the validation loss improved\n                msg=f'\\n for epoch {epoch+1} validation loss improved from  {self.lowest_vloss:7.4f} to {v_loss:7.4}, saving best weights'\n                print_in_color(msg, (0,255,0), (55,65,80))\n                self.lowest_vloss=v_loss # replace lowest validation loss with new validation loss                \n                LRA.best_weights=model.get_weights() # validation loss improved so save the weights\n                self.count=0 # reset count since validation loss improved               \n            else: # validation loss did not improve\n                if self.count>=self.patience-1:\n                    self.lr=self.lr * self.factor\n                    msg=f' \\nfor epoch {epoch+1} validation loss failed to improve for {self.patience} consecutive epochs, learning rate adjusted to {self.lr:9.6f}'\n                    self.count=0 # reset counter\n                    print_in_color(msg, (255,0,0), (55,65,80))\n                    tf.keras.backend.set_value(model.optimizer.lr, self.lr) # set the learning rate in the optimizer\n                else: \n                    self.count =self.count +1 # increment the count\n                    msg=f' \\nfor epoch {epoch+1} validation loss did not improve patience count incremented to {self.count}'\n                    print_in_color(msg, (255,255,0), (55,65,80))","9a33cae7":"def tr_plot(tr_data):\n    #Plot the training and validation data\n    history=tr_data.history\n    tacc=results.history['accuracy']\n    tloss=results.history['loss']\n    vacc=results.history['val_accuracy']\n    vloss=results.history['val_loss']\n    Epoch_count=len(tloss)\n    Epochs=[]\n    for i in range (0,Epoch_count):\n        Epochs.append(i+1)\n    index_loss=np.argmin(vloss)#  this is the epoch with the lowest validation loss\n    val_lowest=vloss[index_loss]\n    index_acc=np.argmax(vacc)\n    val_highest=vacc[index_acc]\n    plt.style.use('fivethirtyeight')\n    sc_label='best epoch= '+ str(index_loss+1)\n    vc_label='best epoch= '+ str(index_acc + 1)\n    fig,axes=plt.subplots(nrows=1, ncols=2, figsize=(20,8))\n    axes[0].plot(Epochs,tloss, 'r', label='Training loss')\n    axes[0].plot(Epochs,vloss,'g',label='Validation loss' )\n    axes[0].scatter(index_loss+1,val_lowest, s=150, c= 'blue', label=sc_label)\n    axes[0].set_title('Training and Validation Loss')\n    axes[0].set_xlabel('Epochs')\n    axes[0].set_ylabel('Loss')\n    axes[0].legend()\n    axes[1].plot (Epochs,tacc,'r',label= 'Training Accuracy')\n    axes[1].plot (Epochs,vacc,'g',label= 'Validation Accuracy')\n    axes[1].scatter(index_acc+1,val_highest, s=150, c= 'blue', label=vc_label)\n    axes[1].set_title('Training and Validation Accuracy')\n    axes[1].set_xlabel('Epochs')\n    axes[1].set_ylabel('Accuracy')\n    axes[1].legend()\n    plt.tight_layout\n    #plt.style.use('fivethirtyeight')\n    plt.show()\n","ce6afcbe":"callbacks=[LRA()]\nresults=model.fit(x=train_gen,  epochs=epochs, verbose=1, callbacks=callbacks,  validation_data=valid_gen,\n                 shuffle=True,  initial_epoch=start_epoch)","617b503e":"tr_plot(results)","83e52302":"model.set_weights(LRA.best_weights)\nacc=model.evaluate( test_gen, batch_size=test_batch_size, verbose=1, steps=test_steps)[1]* 100\nmsg=f'accuracy on the test set is {acc:5.2f} %'\nprint_in_color(msg, (0,255,0),(55,65,80))","c39b919d":"kaggle_dir=r'.\/'\nsave_loc=os.path.join(kaggle_dir, str(acc)[:str(acc).rfind('.')+3] + '.h5')\nmodel.save(save_loc)","a4c4ab47":"errors=int(len(test_file_names)*(1-acc\/100)) +1\nrows=int(errors\/5)+1\nheight=rows * 5\npreds=model.predict(test_gen, batch_size=test_batch_size, verbose=0, steps=test_steps)\nmsg='shown below are images that were improperly classified'\nprint_in_color(msg,(0,255,0),(55,65,80))\nplt.figure(figsize=(20,height ))\nj=1\nfor i,p in enumerate(preds):\n    index=np.argmax(p)\n    prob=p[index]\n    label=test_gen.labels[i]\n    if index != label:\n        img_path=os.path.join(test_dir,test_file_names[i] )\n        plt.subplot(rows, 5, j)\n        j=j + 1\n        if j>50:\n            break\n        img = Image.open(img_path)\n        img=img.resize((224,224))     \n        plt.axis('off')\n        plt.title(test_file_names[i])\n        plt.imshow(np.asarray(img)) \nplt.show()\n\n","12589d40":"total_epochs=epochs + 10\nfor layer in model.layers:\n    layer.trainable=True\nresults=model.fit(x=train_gen,  epochs=total_epochs, verbose=1, callbacks=callbacks,  validation_data=valid_gen,\n                 shuffle=True,  initial_epoch=epochs)\n","502935ba":"model.set_weights(LRA.best_weights)\nacc=model.evaluate( test_gen, batch_size=test_batch_size, verbose=1, steps=test_steps)[1]* 100\nmsg=f'accuracy on the test set is {acc:5.2f} %'\nprint_in_color(msg, (0,255,0),(55,65,80))","6688ac35":"### Define method to plot results of training","615b6586":"### Create subclass of callback class as custom callback to adjust learning rate and save best weights\npatience is an integer that specifies how many consecutive epoch can occur until learning rate is adjusted\nthreshold is a float. It specifies that if training accuracy is above this level learning rate will be adjusted based on validation loss\nfactor is a float <1 that specifies the factor by which the current learning rate will be multiplied by\nclass variable LRA.best_weights stores the model weights for the epoch with the lowest validation loss\nafter train set the model weights with model.load_weights(LRA.best_weights) then do predictions on the test set","0cc8f23f":"### Show some of the training images","d5ac412c":"### Determine test and validation batch size and steps to go through the samples only one per epoch","7ba90033":"### define train, test and validation directories and directory to which the model will be saved","2e2aa357":"### Fine train the model","058ece34":"### Define training, validation and test image generators","9d8c8813":"### surpress annoying tensorflow warnings","55762ac2":"### Import needed modules","900411c8":"### Train the model using the custom callback","5656c891":"### Save the model  as file named acc.h5 where acc is the model accuracy on the test set","00039e9b":"### Plot the model loss and accuracy data","92f7d4d5":"### Define method to print text in specified forground and background rgb colors","030eb865":"### Make predictions on test set and show up to 50 images max that were incorrectly classified","3f7a3229":"###  Load the model with the saved best_weights and evaluate model against test set","3d767997":"### Define the model","cc2c08a7":"### Define a method that determines the test and validation batch size and steps so we go through samples only once","f475667f":"### specify initial parameters"}}