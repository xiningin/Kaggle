{"cell_type":{"b9b452ad":"code","77acf7df":"code","44ebfd89":"code","5d7ee395":"code","611863ad":"code","b3673ed2":"code","22c1f800":"code","bf176cc7":"code","80f9fc92":"code","0d89b509":"code","e078bb38":"code","2b421fd1":"code","3d06c723":"code","08762ebe":"code","84c769b8":"code","d416289e":"code","83ecaca9":"code","7d9fc08f":"code","11bce60b":"code","3180bb34":"code","dce8a21a":"code","f1943418":"code","dcc41b03":"code","0124414e":"code","816060ae":"code","044caf44":"code","fe387e14":"code","4657ce8f":"code","5d62c2c5":"code","9f1004f7":"code","a8269051":"code","aba39048":"markdown","23c95f93":"markdown","f82bb949":"markdown","bf1ebae1":"markdown","6bbc9b64":"markdown","1c9418f1":"markdown","e8fd3999":"markdown","bfc2f291":"markdown","e749f4eb":"markdown"},"source":{"b9b452ad":"!nvidia-smi","77acf7df":"!pip install -qU pip\n!pip install -qU torchsummary","44ebfd89":"import os\nimport random\n\nimport torch\nimport torch.nn as nn\nimport torch.backends.cudnn as cudnn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\n\nimport torchvision\nimport torchvision.transforms as T\nfrom torchvision.utils import make_grid, save_image\nfrom torchvision.datasets import ImageFolder\n\nfrom torchsummary import summary\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom IPython.display import HTML\n\n%matplotlib inline","5d7ee395":"# move to Device\n\ndef get_default_device():\n    \"\"\"Pick GPU if available, else CPU\"\"\"\n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    else: \n        return torch.device('cpu')\n\ndevice = get_default_device()\ndevice\n\ndef to_device(data, device):\n    if isinstance(data, (list, tuple)):\n        return [to_device(x, device) for x in data]\n    return data.to(device, non_blocking=True)\n\n\nclass DeviceDataLoader():\n\n    def __init__(self, dl, device):\n        self.dl = dl\n        self.device = device\n\n    def __iter__(self):\n        for b in self.dl:\n            yield to_device(b, self.device)\n    \n    def __len__(self):\n        return len(self.dl)\n\n\n\n# Denormalize images\ndef denorm(x):\n    out = (x + 1) \/ 2\n    return out.clamp(0, 1)\n\n# Visualization\ndef show_images(images, nmax=64):\n    fig, ax = plt.subplots(figsize=(8, 8))\n    ax.set_xticks([]); ax.set_yticks([])\n    ax.imshow(make_grid(denorm(images.cpu().detach()[:nmax]), nrow=8).permute(1, 2, 0))\n\ndef show_batch(dl, nmax=64):\n    for images, _ in dl:\n        show_images(images, nmax)\n        break\n\n\ndef save_samples(index, model, latent_tensors, show=True, verbose=False):\n    fake_images = model(latent_tensors)\n    fake_fname = 'generated-images-{0:0=4d}.png'.format(index)\n    save_image(denorm(fake_images), os.path.join(sample_dir, fake_fname), nrow=8)\n    if verbose:\n        print('Saving', fake_fname)\n    if show:\n        show_images(fake_images)","611863ad":"dataset_path = r\"..\/input\/animefacedataset\"\n\nLATENT_DIMS = 100\nBATCH_SIZE = 128\nLEARNING_RATE = 0.0002\nEPOCHS = 25\n\nIMG_SIZE = 64\nIMG_CHANNELS = 3\nCAPACITY_G = 64\nCAPACITY_D = 64\n\nBETA1 = 0.5\nBETA2 = 0.999\n\nreal_label = 1.\nfake_label = 0.","b3673ed2":"dataset = ImageFolder(root=dataset_path,\n                      transform=T.Compose([\n                               T.Resize(IMG_SIZE),\n                               T.CenterCrop(IMG_SIZE),\n                               T.ToTensor(),\n                               T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n                           ]))\n\ndata_loader = DataLoader(dataset, \n                         BATCH_SIZE,\n                         shuffle=True, \n                         pin_memory=True,\n                         num_workers=4)\n\ndata_loader = DeviceDataLoader(data_loader, device)","22c1f800":"show_batch(data_loader)","bf176cc7":"len(dataset), len(data_loader)","80f9fc92":"# # custom weights initialization called on generator and discriminator\n\n# def weights_init(model):\n#     classname = model.__class__.__name__\n#     if classname.find('conv') != -1:\n#         torch.nn.init.normal_(model.weight, 0.0, 0.02)\n#     elif classname.find('batch_norm') != -1:\n#         torch.nn.init.normal_(model.weight, 0.0, 0.02)\n#         torch.nn.init.zeros_(model.bias)","0d89b509":"class Generator(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        \n        # INPUT: (BATCH_SIZE, LATENT_DIMS, 1, 1)\n        \n        self.conv_transpose_1 = nn.ConvTranspose2d(in_channels=LATENT_DIMS, out_channels=CAPACITY_G*8, kernel_size=4, stride=1, padding=0, bias=False)\n        self.conv_transpose_2 = nn.ConvTranspose2d(in_channels=CAPACITY_G*8, out_channels=CAPACITY_G*4, kernel_size=4, stride=2, padding=1, bias=False)\n        self.conv_transpose_3 = nn.ConvTranspose2d(in_channels=CAPACITY_G*4, out_channels=CAPACITY_G*2, kernel_size=4, stride=2, padding=1, bias=False)\n        self.conv_transpose_4 = nn.ConvTranspose2d(in_channels=CAPACITY_G*2, out_channels=CAPACITY_G, kernel_size=4, stride=2, padding=1, bias=False)\n        self.conv_transpose_5 = nn.ConvTranspose2d(in_channels=CAPACITY_G, out_channels=IMG_CHANNELS, kernel_size=4, stride=2, padding=1, bias=False)\n        \n        self.batch_norm_1 = nn.BatchNorm2d(CAPACITY_G*8)\n        self.batch_norm_2 = nn.BatchNorm2d(CAPACITY_G*4)\n        self.batch_norm_3 = nn.BatchNorm2d(CAPACITY_G*2)\n        self.batch_norm_4 = nn.BatchNorm2d(CAPACITY_G)\n    \n        self.relu_activation = nn.ReLU(True)\n        self.tanh_activation = nn.Tanh()\n\n    def forward(self, xb):\n        \n        xb = self.conv_transpose_1(xb)\n        xb = self.batch_norm_1(xb)\n        xb = self.relu_activation(xb)\n\n        xb = self.conv_transpose_2(xb)\n        xb = self.batch_norm_2(xb)\n        xb = self.relu_activation(xb)\n\n        xb = self.conv_transpose_3(xb)\n        xb = self.batch_norm_3(xb)\n        xb = self.relu_activation(xb)\n\n        xb = self.conv_transpose_4(xb)\n        xb = self.batch_norm_4(xb)\n        xb = self.relu_activation(xb)\n\n        xb = self.conv_transpose_5(xb)\n        xb = self.tanh_activation(xb)\n\n        return xb","e078bb38":"test_generator_model = Generator()\n# test_generator_model.apply(weights_init)\ntest_generator_model = to_device(test_generator_model, device)\nsummary(test_generator_model, (LATENT_DIMS, 1, 1))","2b421fd1":"test_= torch.randn(1, LATENT_DIMS, 1, 1, device=device)\ntest_generator_model(test_).shape","3d06c723":"xb = torch.randn(BATCH_SIZE, LATENT_DIMS, 1, 1, device=\"cpu\") # random latent tensors\nfake_images = Generator()(xb)\nshow_images(fake_images.cpu().detach())","08762ebe":"class Discriminator(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # INPUT: (BATCH_SIZE, IMG_CHANNELS, IMG_SIZE, IMG_SIZE)\n\n        self.conv1 = nn.Conv2d(in_channels=IMG_CHANNELS, out_channels=CAPACITY_D, kernel_size=4, stride=2, padding=1, bias=False)\n        self.conv2 = nn.Conv2d(in_channels=CAPACITY_D, out_channels=CAPACITY_D*2, kernel_size=4, stride=2, padding=1, bias=False)\n        self.conv3 = nn.Conv2d(in_channels=CAPACITY_D*2, out_channels=CAPACITY_D*4, kernel_size=4, stride=2, padding=1, bias=False)\n        self.conv4 = nn.Conv2d(in_channels=CAPACITY_D*4, out_channels=CAPACITY_D*8, kernel_size=4, stride=2, padding=1, bias=False)\n        self.conv5 = nn.Conv2d(in_channels=CAPACITY_D*8, out_channels=1, kernel_size=4, stride=1, padding=0, bias=False)\n        \n        self.batch_norm_2 = nn.BatchNorm2d(CAPACITY_D*2)\n        self.batch_norm_3 = nn.BatchNorm2d(CAPACITY_D*4)\n        self.batch_norm_4 = nn.BatchNorm2d(CAPACITY_D*8)\n\n        self.leaky_activation = nn.LeakyReLU(0.2, inplace=True)\n        self.sigmoid_activation = nn.Sigmoid()\n\n    \n    def forward(self, xb):\n        xb = self.conv1(xb)        \n        xb = self.leaky_activation(xb)\n    \n        xb = self.conv2(xb)\n        xb = self.batch_norm_2(xb)\n        xb = self.leaky_activation(xb)\n        \n        xb = self.conv3(xb)\n        xb = self.batch_norm_3(xb)\n        xb = self.leaky_activation(xb)\n\n        xb = self.conv4(xb)\n        xb = self.batch_norm_4(xb)\n        xb = self.leaky_activation(xb)\n\n        xb = self.conv5(xb)\n        xb = self.leaky_activation(xb)\n        \n        xb = xb.view(xb.size(0), -1)\n\n        xb = self.sigmoid_activation(xb)\n        return xb","84c769b8":"test_discriminator_model = Discriminator()\n# test_discriminator_model.apply(weights_init)\ntest_discriminator_model = to_device(test_discriminator_model, device)\nsummary(test_discriminator_model, (3, 64, 64))","d416289e":"test_= torch.randn(1, 3, 64, 64, device=device)\ntest_discriminator_model(test_).shape","83ecaca9":"def training_generator(generator, discriminator, optimizer_g):\n\n    optimizer_g.zero_grad()\n\n    targets = torch.ones(BATCH_SIZE, 1, device=device)\n\n    # Generate fake images\n    latent = torch.randn(BATCH_SIZE, LATENT_DIMS, 1, 1, device=device)\n    fake_images = generator(latent)\n    \n    # Try to fool the discriminator\n    preds = discriminator(fake_images)\n    loss = F.binary_cross_entropy(preds, targets)\n    \n    loss.backward()\n\n    # Update generator weights\n    optimizer_g.step()\n    \n    return loss.item()    ","7d9fc08f":"def training_discriminator(generator, discriminator, real_images, optimizer_d):\n    optimizer_d.zero_grad()\n    \n    inp_batch_size = real_images.size(0)\n    # Pass real images through discriminator\n\n    # PART 1: D(x)\n    real_targets = torch.full((inp_batch_size, 1), real_label, device=device)  # saves memory # BATCH_SIZE, 1 -> 1\n    real_preds = discriminator(real_images)\n    real_loss = F.binary_cross_entropy(real_preds, real_targets)\n    \n    real_loss.backward()\n\n    real_score = torch.mean(real_preds).item() # D_x\n    \n    # ------------------------------------------\n    # PART 2: 1 - D_G_z\n\n    # Generate fake images\n    latent = torch.randn(inp_batch_size, LATENT_DIMS, 1, 1, device=device)\n    fake_images = generator(latent)\n\n    # Pass fake images through discriminator\n    fake_targets = real_targets.fill_(fake_label) # saves memory # BATCH_SIZE, 1 -> 0\n\n    fake_preds = discriminator(fake_images)\n    fake_loss = F.binary_cross_entropy(fake_preds, fake_targets)\n\n    fake_loss.backward()\n\n    fake_score = torch.mean(fake_preds).item() # D_G_z1\n\n    # Update discriminator weights\n    loss = real_loss + fake_loss\n\n    \n    optimizer_d.step()\n    \n    return loss.item(), real_score, fake_score","11bce60b":"sample_dir = '.\/generated'\n!rm -rf {sample_dir}\n\nos.makedirs(sample_dir)","3180bb34":"# fixed latents for visualizing progress\n\nfixed_latents = torch.randn(64, LATENT_DIMS, 1, 1, device=device)","dce8a21a":"save_samples(0, to_device(Generator(), device), fixed_latents, show=True, verbose=True)","f1943418":"def fit(generator, discriminator, epochs, lr):\n\n    torch.cuda.empty_cache()\n\n    generator_loss = []\n    discriminator_loss = []\n    generator_score = []\n    discriminator_score = []\n    \n    # create optimizers \n    optimizer_generator = torch.optim.Adam(generator.parameters(),\n                                            lr=lr,\n                                            betas=(BETA1, BETA2))\n    \n    optimizer_discriminator = torch.optim.Adam(discriminator.parameters(),\n                                            lr=lr,\n                                            betas=(BETA1, BETA2))\n    \n\n    for epoch in range(epochs):\n\n        # Losses & scores\n        losses_d = []\n        losses_g = []\n        real_scores = []\n        fake_scores = []\n\n        for real_images, _ in data_loader:\n\n            # train discriminator \n            \n            loss_d, real_score, fake_score = training_discriminator(generator, \n                                                                    discriminator,\n                                                                    real_images,\n                                                                    optimizer_discriminator)\n\n            # train generator\n            loss_g = training_generator(generator, \n                                        discriminator,\n                                        optimizer_generator)\n            \n            # save current batch metrics\n            losses_d.append(loss_d)\n            losses_g.append(loss_g)\n            real_scores.append(real_score)\n            fake_scores.append(fake_score)\n\n        # calculate epoch metric from all the batches\n        losses_d_mean = np.mean(np.array(losses_d))\n        losses_g_mean = np.mean(np.array(losses_g))\n        real_scores_mean = np.mean(np.array(real_scores))\n        fake_scores_mean = np.mean(np.array(fake_scores))\n\n\n        # for plotting\n        discriminator_loss.append(losses_d_mean)\n        generator_loss.append(losses_g_mean)\n        discriminator_score.append(real_scores_mean)\n        generator_score.append(fake_scores_mean)\n\n        # Log losses & scores\n        print(f\"Epoch [{epoch+1:03}\/{epochs}], loss_g: {losses_g_mean:.5f}, loss_d: {losses_d_mean:.5f}, real_score: {real_scores_mean:.5f}, fake_score: {fake_scores_mean:.5f}\")\n    \n        # Save generated images\n        with torch.no_grad():\n            save_samples(epoch+1, generator, fixed_latents, show=False, verbose=False)\n    \n    return generator_loss, discriminator_loss, discriminator_score, generator_score","dcc41b03":"# Model Initialization and weights setup\ngenerator_model = Generator()\n# generator_model.apply(weights_init)\ngenerator_model = to_device(generator_model, device)\n\ndiscriminator_model = Discriminator()\n# discriminator_model.apply(weights_init)\ndiscriminator_model = to_device(discriminator_model, device)","0124414e":"history = fit(generator_model, discriminator_model, EPOCHS, LEARNING_RATE)","816060ae":"# log and plot metrics\n\nlosses_g, losses_d, real_scores, fake_scores = history","044caf44":"plt.plot(losses_d, '-')\nplt.plot(losses_g, '-')\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.legend(['Discriminator', 'Generator'])\n_ = plt.title('Losses')","fe387e14":"plt.plot(real_scores, '-')\nplt.plot(fake_scores, '-')\nplt.xlabel('epoch')\nplt.ylabel('score')\nplt.legend(['Real', 'Fake'])\n_ = plt.title('Scores')","4657ce8f":"# save model\ntorch.save(generator_model.to(\"cpu\"), 'generator.pt')\ntorch.save(discriminator_model.to(\"cpu\"), 'discriminator.pt')","5d62c2c5":"import imageio\nimport glob\nfrom PIL import Image \nfrom numpy import asarray\nimport IPython.display as disp\n\ngenerated_file = '.\/anime.gif'\n\n\nfilenames = glob.glob('.\/generated\/*.png')\nfilenames = sorted(filenames)\nimgs = [asarray(Image.open(img)) for img in filenames]\nimageio.mimsave(generated_file, imgs)\n\nwith open(generated_file,'rb') as file:\n    disp.display(disp.Image(file.read()))","9f1004f7":"from PIL import Image\nImage.open('.\/generated\/generated-images-0015.png')","a8269051":"import cv2\n\nvid_fname = 'anime.avi'\n\nfiles = [os.path.join(sample_dir, f) for f in os.listdir(sample_dir) if 'generated' in f]\nfiles.sort()\n\nout = cv2.VideoWriter(vid_fname,cv2.VideoWriter_fourcc(*'MP4V'), 1, (530,530))\n[out.write(cv2.imread(fname)) for fname in files]\nout.release()","aba39048":"# Training","23c95f93":"# Utility Functions","f82bb949":"# Model Design","bf1ebae1":"# Prepare dataset","6bbc9b64":"## Generator Network","1c9418f1":"## Weight Initialization\n \n- as mentioned in DCGAN paper\n","e8fd3999":"## Discriminator network","bfc2f291":"# Training Setup","e749f4eb":"# Constants"}}