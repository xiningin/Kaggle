{"cell_type":{"077d44d8":"code","025579e4":"code","45795b60":"code","02fa6dd3":"code","f94f589a":"code","22f71d9c":"code","8ff9527b":"code","38302a31":"code","51a55316":"code","912f8a3a":"code","cba4910b":"code","d15e50ac":"code","dd704855":"code","aab6c4f4":"code","e1a97307":"code","56f25ac7":"code","f00845e1":"code","142ed64c":"code","1042adfe":"code","1fab4c65":"code","4b9b1313":"code","05407a73":"code","fe3e7b3a":"code","3a7d76d4":"code","0af716ef":"code","cc3645c2":"code","6de7331e":"code","ea902afb":"code","965d4ed2":"code","835a24c8":"code","66fe65a6":"code","df29dc00":"code","23765c4c":"code","9248d046":"code","b4c73ae2":"code","341a8a25":"code","fa4fef64":"markdown","1aab481e":"markdown","229ccbe8":"markdown","8ea83798":"markdown","ad510b9a":"markdown","96e2f65b":"markdown","8dc31adf":"markdown","af6c038a":"markdown","ace47dde":"markdown","86402eb3":"markdown","d0953bb7":"markdown","53de48a7":"markdown","5a4d2e88":"markdown"},"source":{"077d44d8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","025579e4":"import pandas_summary as ps\n\n# Data processing, metrics and modeling\nfrom sklearn.model_selection import train_test_split, KFold, StratifiedKFold, train_test_split, RandomizedSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.base import BaseEstimator\nfrom sklearn.cluster import KMeans\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LassoCV, RidgeCV, HuberRegressor, ElasticNetCV\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn import metrics\n\n\nfrom scipy.stats import norm\n\n# Lgbm\nimport lightgbm as lgb\n\n# Support warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Plots\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom matplotlib import rcParams","45795b60":"pd.set_option('display.max_columns', 500)\npd.set_option('display.max_rows', 500)\n\nfolder = '\/kaggle\/input\/mercedes-benz-greener-manufacturing\/'","02fa6dd3":"train_df = pd.read_csv(folder + 'train.csv.zip')\ntest_df = pd.read_csv(folder + 'test.csv.zip')\nsub_df = pd.read_csv(folder + 'sample_submission.csv.zip')\n\nprint('train_df: ', train_df.shape)\nprint('test_df: ', test_df.shape)\nprint('sub_df: ', sub_df.shape)","f94f589a":"train_df.head()","22f71d9c":"dfs_train = ps.DataFrameSummary(train_df)\nprint('categoricals: ', dfs_train.categoricals.tolist())\nprint('numerics: ', dfs_train.numerics.tolist())\ndfs_train.summary()","8ff9527b":"cat_cols = dfs_train.categoricals.tolist()","38302a31":"test_df.head()","51a55316":"dfs_test = ps.DataFrameSummary(test_df)\nprint('categoricals: ', dfs_test.categoricals.tolist())\nprint('numerics: ', dfs_test.numerics.tolist())\ndfs_test.summary()","912f8a3a":"ps.DataFrameSummary(train_df[['y']]).summary().T","cba4910b":"plt.figure(figsize=(12,5))\nsns.distplot(train_df['y'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train_df['y'])\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution');","d15e50ac":"train_df['y'] = np.log(train_df['y'])","dd704855":"ps.DataFrameSummary(train_df[['y']]).summary().T","aab6c4f4":"plt.figure(figsize=(12,5))\nsns.distplot(train_df['y'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train_df['y'])\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution');","e1a97307":"train_df = train_df[(train_df['y'] > np.percentile(train_df['y'], 0.5)) & (train_df['y'] < np.percentile(train_df['y'], 99.5))]","56f25ac7":"plt.figure(figsize=(12,5))\nsns.distplot(train_df['y'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train_df['y'])\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution');","f00845e1":"y = train_df['y']\ntrain_df.drop(['y'], axis=1, inplace=True)","142ed64c":"# VERY BAD","1042adfe":"class MeanEncoding(BaseEstimator):\n    \"\"\"   In Mean Encoding we take the number \n    of labels into account along with the target variable \n    to encode the labels into machine comprehensible values    \"\"\"\n    \n    def __init__(self, feature, C=0.1):\n        self.C = C\n        self.feature = feature\n        \n    def fit(self, X_train, y_train):\n        \n        df = pd.DataFrame({'feature': X_train[self.feature], 'target': y_train}).dropna()\n        \n        self.global_mean = df.target.mean()\n        mean = df.groupby('feature').target.mean()\n        size = df.groupby('feature').target.size()\n        \n        self.encoding = (self.global_mean * self.C + mean * size) \/ (self.C + size)\n    \n    def transform(self, X_test):\n        \n        X_test[self.feature] = X_test[self.feature].map(self.encoding).fillna(self.global_mean).values\n        \n        return X_test\n    \n    def fit_transform(self, X_train, y_train):\n        \n        df = pd.DataFrame({'feature': X_train[self.feature], 'target': y_train}).dropna()\n        \n        self.global_mean = df.target.mean()\n        mean = df.groupby('feature').target.mean()\n        size = df.groupby('feature').target.size()\n        self.encoding = (self.global_mean * self.C + mean * size) \/ (self.C + size)\n        \n        X_train[self.feature] = X_train[self.feature].map(self.encoding).fillna(self.global_mean).values\n        \n        return X_train","1fab4c65":"for f in cat_cols:\n    me = MeanEncoding(f, C=0.99)\n    me.fit(train_df, y)\n    train_df = me.transform(train_df)\n    test_df = me.transform(test_df)","4b9b1313":"train_df.head()","05407a73":"km = KMeans(n_clusters=2, random_state=13)\nkm.fit(pd.DataFrame(y))\ny_clust = km.predict(pd.DataFrame(y))","fe3e7b3a":"pd.Series(y_clust).value_counts(normalize=True)","3a7d76d4":"X_train, X_val, y_train, y_val, y_train_clust, y_val_clust = train_test_split(\n    train_df, y, pd.Series(y_clust), \n    test_size=0.25,\n    stratify=y_clust,\n    random_state=777\n)","0af716ef":"y_train_clust.value_counts(normalize=True)","cc3645c2":"scaler = StandardScaler()\nscaler.fit(X_train)\nX_train_sc = pd.DataFrame(scaler.transform(X_train))\nX_val_sc = pd.DataFrame(scaler.transform(X_val))\ntest_df_sc = pd.DataFrame(scaler.transform(test_df))","6de7331e":"pca = PCA(n_components=2)\npca.fit(X_train_sc)\ntrain_pca_transformed = pca.transform(X_train_sc)","ea902afb":"plt.figure(figsize=(10, 10))\nplt.scatter(train_pca_transformed[:, 0], train_pca_transformed[:, 1], c=y_train_clust);","965d4ed2":"lasso = LassoCV(max_iter=9999)\nlasso.fit(X_train_sc, y_train)\nlasso_train_pred = lasso.predict(X_train_sc)\nlasso_val_pred = lasso.predict(X_val_sc)\nprint('train', metrics.r2_score(y_train, lasso_train_pred), 'val', metrics.r2_score(y_val, lasso_val_pred))","835a24c8":"ridge = RidgeCV()\nridge.fit(X_train_sc, y_train)\nridge_train_pred = ridge.predict(X_train_sc)\nridge_val_pred = ridge.predict(X_val_sc)\nprint('train', metrics.r2_score(y_train, ridge_train_pred), 'val', metrics.r2_score(y_val, ridge_val_pred))","66fe65a6":"enet = ElasticNetCV()\nenet.fit(X_train_sc, y_train)\nenet_train_pred = enet.predict(X_train_sc)\nenet_val_pred = enet.predict(X_val_sc)\nprint('train', metrics.r2_score(y_train, enet_train_pred), 'val', metrics.r2_score(y_val, enet_val_pred))","df29dc00":"huber = HuberRegressor(alpha=0.05)\nhuber.fit(X_train_sc, y_train)\nhuber_train_pred = huber.predict(X_train_sc)\nhuber_val_pred = huber.predict(X_val_sc)\nprint('train', metrics.r2_score(y_train, huber_train_pred), 'val', metrics.r2_score(y_val, huber_val_pred))","23765c4c":"rf = RandomForestRegressor(n_estimators=5)\nrf.fit(X_train_sc, y_train)\nrf_train_pred = rf.predict(X_train_sc)\nrf_val_pred = rf.predict(X_val_sc)\nprint('train', metrics.r2_score(y_train, rf_train_pred), 'val', metrics.r2_score(y_val, rf_val_pred))","9248d046":"sub_df['y'] = np.round(np.exp(lasso.predict(test_df_sc)), 4)","b4c73ae2":"sub_df.head()","341a8a25":"sub_df.to_csv('sub.csv', index=False)","fa4fef64":"## 6. Make mean target encoding for categorical feature\n\nLet us consider the above table (A simple binary classification). \n\n$$ MeanTargetEnc_i = {((GlobalMean * C) + (Mean_i * Size)) \\over (C + Size)} $$\n\nInstead of finding the mean of the targets, we can also focus on median and other statistical correlations\u2026.These are broadly called target encodings","1aab481e":"# 7. Cluster stratify split","229ccbe8":"# 5. Drop outliers by percentile","8ea83798":"# 11. predict","ad510b9a":"# 9. Visualize our dateset","96e2f65b":"### Let's logarithm the value of Y","8dc31adf":"# 4. Get target","af6c038a":"# 0. Starts","ace47dde":"# 10. Models","86402eb3":"# 2. Options","d0953bb7":"# 8. Scailing","53de48a7":"# 1. Import","5a4d2e88":"# 3. Read CSV"}}