{"cell_type":{"c882986b":"code","4c903625":"code","c1bb6eba":"code","c744bf69":"code","45697673":"code","f96724b4":"code","2636009d":"code","ffce50ca":"code","a16dd15f":"code","592b5549":"code","d87e736f":"code","8713a81b":"code","dd74b98e":"code","7d7e1848":"code","339d262e":"code","60e2557e":"code","fbf3c20e":"markdown","b6e8436d":"markdown","11c2d676":"markdown","657f5892":"markdown","e7a009ab":"markdown","85b5d17d":"markdown","f47dcc34":"markdown","6dc7411b":"markdown","caa997e9":"markdown","9a8bc236":"markdown","4b60eb1e":"markdown","b7a1ec49":"markdown","94d3b518":"markdown","4b75d88f":"markdown"},"source":{"c882986b":"!pip install torch==1.7.1 --quiet\n!pip install pytorch_lightning==1.4.5 --quiet\n# !export TOKENIZERS_PARALLELISM=true","4c903625":"import os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"","c1bb6eba":"# Helper libraries\nimport os\nimport gc\ngc.enable()\nimport math\nimport json\nimport time\nimport random\nimport multiprocessing\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\nfrom sklearn import model_selection\nfrom collections import OrderedDict\n\n#Pytorch, transformers\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import Parameter\nimport torch.optim as optim\nfrom torch.utils.data import (\n    Dataset, DataLoader,\n    SequentialSampler, RandomSampler\n)\nfrom torch.utils.data.distributed import DistributedSampler\n\nimport transformers\nfrom transformers import (\n    WEIGHTS_NAME,\n    AdamW,\n    AutoConfig,\n    AutoModel,\n    AutoTokenizer,\n    get_cosine_schedule_with_warmup,\n    get_linear_schedule_with_warmup,\n    logging,\n    MODEL_FOR_QUESTION_ANSWERING_MAPPING,\n)\nlogging.set_verbosity_warning()\nlogging.set_verbosity_error()\n\n\n#Import pytorch lightning: \nimport pytorch_lightning as pl\nfrom pytorch_lightning import Trainer, seed_everything\nfrom pytorch_lightning import Callback\nfrom pytorch_lightning.loggers import CSVLogger\nfrom pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\nfrom pytorch_lightning.callbacks import LearningRateMonitor\n\nMODEL_CONFIG_CLASSES = list(MODEL_FOR_QUESTION_ANSWERING_MAPPING.keys())\nMODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)","c744bf69":"train = pd.read_csv('..\/input\/chaii-hindi-and-tamil-question-answering\/train.csv')\ntest = pd.read_csv('..\/input\/chaii-hindi-and-tamil-question-answering\/test.csv')\nexternal_mlqa = pd.read_csv('..\/input\/mlqa-hindi-processed\/mlqa_hindi.csv')\nexternal_xquad = pd.read_csv('..\/input\/mlqa-hindi-processed\/xquad.csv')\nexternal_train = pd.concat([external_mlqa, external_xquad])","45697673":"def create_folds(data: pd.DataFrame, num_splits: int) -> pd.DataFrame:\n    '''This function is used for creating the folds for k-fold splits'''\n\n    # we create a new column called kfold and fill it with -1\n    data[\"kfold\"] = -1\n\n    # the next step is to randomize the rows of the data\n    data = data.sample(frac=1).reset_index(drop=True)\n\n    # initiate the kfold class from model_selection module\n    kf = model_selection.StratifiedKFold(n_splits=num_splits, random_state=42, shuffle=True)\n    \n    for f, (t_, v_) in enumerate(kf.split(X=data, y=data['language'])):\n        data.loc[v_, 'kfold'] = f\n\n    # return dataframe with folds\n    return data","f96724b4":"def set_seed(seed):\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n\n\ndef optimal_num_of_loader_workers():\n    num_cpus = multiprocessing.cpu_count()\n    num_gpus = torch.cuda.device_count()\n    optimal_value = min(num_cpus, num_gpus*4) if num_gpus else num_cpus - 1\n    return optimal_value\n","2636009d":"# Necessary helper function: \ndef convert_answers(row):\n    return {'answer_start': [row[0]], 'text': [row[1]]}","ffce50ca":"train = create_folds(train, num_splits=5)\nexternal_train[\"kfold\"] = -1\nexternal_train['id'] = list(np.arange(1, len(external_train)+1))\ndf = pd.concat([train, external_train]).reset_index(drop=True)\n\ndf['answers'] = df[['answer_start', 'answer_text']].apply(convert_answers, axis=1)","a16dd15f":"class Config(dict):\n    # model\n    model_type = 'xlm_roberta'\n    model_name_or_path = \"deepset\/xlm-roberta-base-squad2\"\n    config_name = \"deepset\/xlm-roberta-base-squad2\"\n    fp16 = True\n    # fp16_opt_level = \"O1\"\n    gradient_accumulation_steps = 2\n\n    # tokenizer\n    tokenizer_name = \"deepset\/xlm-roberta-base-squad2\"\n    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n    max_seq_length = 384\n    doc_stride = 128\n\n    # train\n    epochs = 30\n    train_batch_size = 24\n    eval_batch_size = 24\n\n    # optimizer\n    optimizer_type = 'AdamW'\n    learning_rate = 3e-5\n    weight_decay = 1e-2\n    epsilon = 1e-8\n    max_grad_norm = 1.0\n\n    # scheduler\n    decay_name = 'linear-warmup'\n    warmup_ratio = 0.1\n\n    # logging\n    logging_steps = 10\n\n    # evaluate\n    output_dir = 'output'\n    seed = 42","592b5549":"def prepare_train_features(example, args):\n    tokenizer = args.tokenizer\n    example[\"question\"] = example[\"question\"].lstrip()\n    tokenized_example = tokenizer(\n        example[\"question\"],\n        example[\"context\"],\n        truncation=\"only_second\",\n        max_length=args.max_seq_length,\n        stride=args.doc_stride,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n\n    sample_mapping = tokenized_example.pop(\"overflow_to_sample_mapping\")\n    offset_mapping = tokenized_example.pop(\"offset_mapping\")\n\n    features = []\n    for i, offsets in enumerate(offset_mapping):\n        feature = {}\n\n        input_ids = tokenized_example[\"input_ids\"][i]\n        attention_mask = tokenized_example[\"attention_mask\"][i]\n\n        feature['input_ids'] = input_ids\n        feature['attention_mask'] = attention_mask\n        feature['offset_mapping'] = offsets\n\n        cls_index = input_ids.index(tokenizer.cls_token_id)\n        sequence_ids = tokenized_example.sequence_ids(i)\n\n        sample_index = sample_mapping[i]\n        answers = example[\"answers\"]\n\n        if len(answers[\"answer_start\"]) == 0:\n            feature[\"start_position\"] = cls_index\n            feature[\"end_position\"] = cls_index\n        else:\n            start_char = answers[\"answer_start\"][0]\n            end_char = start_char + len(answers[\"text\"][0])\n\n            token_start_index = 0\n            while sequence_ids[token_start_index] != 1:\n                token_start_index += 1\n\n            token_end_index = len(input_ids) - 1\n            while sequence_ids[token_end_index] != 1:\n                token_end_index -= 1\n\n            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n                feature[\"start_position\"] = cls_index\n                feature[\"end_position\"] = cls_index\n            else:\n                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n                    token_start_index += 1\n                feature[\"start_position\"] = token_start_index - 1\n                while offsets[token_end_index][1] >= end_char:\n                    token_end_index -= 1\n                feature[\"end_position\"] = token_end_index + 1\n\n        features.append(feature)\n    return features\n\n\ndef make_loader(fold, args):\n    train_set, valid_set = df[df['kfold']!=fold], df[df['kfold']==fold]\n    \n    train_features, valid_features = [[] for _ in range(2)]\n    for i, row in train_set.iterrows():\n        train_features += prepare_train_features(row, args)\n    for i, row in valid_set.iterrows():\n        valid_features += prepare_train_features(row, args)\n\n    train_dataset = DatasetRetriever(train_features)\n    valid_dataset = DatasetRetriever(valid_features)\n    print(f\"Num examples Train= {len(train_dataset)}, Num examples Valid={len(valid_dataset)}\")\n    \n    train_sampler = RandomSampler(train_dataset)\n    valid_sampler = SequentialSampler(valid_dataset)\n\n    train_dataloader = DataLoader(\n        train_dataset,\n        batch_size=args.train_batch_size,\n        sampler=train_sampler,\n        num_workers=optimal_num_of_loader_workers(),\n        pin_memory=True,\n        drop_last=False \n    )\n\n    valid_dataloader = DataLoader(\n        valid_dataset,\n        batch_size=args.eval_batch_size, \n        sampler=valid_sampler,\n        num_workers=optimal_num_of_loader_workers(),\n        pin_memory=True, \n        drop_last=False\n    )\n\n    return train_dataloader, valid_dataloader","d87e736f":"class DatasetRetriever(Dataset):\n    def __init__(self, features, mode='train'):\n        super(DatasetRetriever, self).__init__()\n        self.features = features\n        self.mode = mode\n        \n    def __len__(self):\n        return len(self.features)\n    \n    def __getitem__(self, item):   \n        feature = self.features[item]\n        if self.mode == 'train':\n            return {\n                'input_ids':torch.tensor(feature['input_ids'], dtype=torch.long),\n                'attention_mask':torch.tensor(feature['attention_mask'], dtype=torch.long),\n                'offset_mapping':torch.tensor(feature['offset_mapping'], dtype=torch.long),\n                'start_position':torch.tensor(feature['start_position'], dtype=torch.long),\n                'end_position':torch.tensor(feature['end_position'], dtype=torch.long)\n            }\n        else:\n            return {\n                'input_ids':torch.tensor(feature['input_ids'], dtype=torch.long),\n                'attention_mask':torch.tensor(feature['attention_mask'], dtype=torch.long),\n                'offset_mapping':feature['offset_mapping'],\n                'sequence_ids':feature['sequence_ids'],\n                'id':feature['example_id'],\n                'context': feature['context'],\n                'question': feature['question']\n            }","8713a81b":"def loss_fn(preds, labels):\n    start_preds, end_preds = preds\n    start_labels, end_labels = labels\n    start_loss = nn.CrossEntropyLoss(ignore_index=-1)(start_preds, start_labels)\n    end_loss = nn.CrossEntropyLoss(ignore_index=-1)(end_preds, end_labels)\n    total_loss = (start_loss + end_loss) \/ 2\n    return total_loss","dd74b98e":"class Model(pl.LightningModule):\n\n    def __init__(self, fold, args):\n        super(Model, self).__init__()\n        self.config = args\n        self.learning_rate = args.learning_rate\n        self.model_config = AutoConfig.from_pretrained(self.config.config_name)\n        self.model = AutoModel.from_pretrained(self.config.model_name_or_path, config=self.model_config)\n        self.qa_outputs = nn.Linear(self.model_config.hidden_size, 2)\n        self.dropout = nn.Dropout(self.model_config.hidden_dropout_prob)\n        self._init_weights(self.qa_outputs)\n\n        train_dataloader, val_dataloader= make_loader(fold, args)\n\n        self._train_dataloader = train_dataloader\n        self._val_dataloader = val_dataloader\n        self.all_targets=[]\n        self.all_preds=[]\n        self.train_loss=0\n        self.val_loss=0\n        self.t_data_size=0\n        self.v_data_size=0\n        self.automatic_optimization = True\n\n    def forward(self, input_ids, attention_mask):\n        '''The forward step performs the next step for the model while training.'''\n        outputs = self.model(\n            input_ids,\n            attention_mask=attention_mask,\n        )\n\n        sequence_output = outputs[0]\n        pooled_output = outputs[1]\n\n        # sequence_output = self.dropout(sequence_output)\n        qa_logits = self.qa_outputs(sequence_output)\n\n        start_logits, end_logits = qa_logits.split(1, dim=-1)\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n\n        return start_logits, end_logits\n    \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.model_config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n\n    def configure_optimizers(self):\n        param_optimizer = list(self.model.named_parameters())\n        no_decay = [\"bias\", \"LayerNorm.weight\",]\n        optimizer_grouped_parameters = [\n            {\n                \"params\": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n                \"weight_decay_rate\": self.config.weight_decay\n            },\n            {\n                \"params\": [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n                \"weight_decay_rate\": 0.0\n            },\n        ]\n        optimizer = AdamW(\n            optimizer_grouped_parameters,\n            lr=self.learning_rate,\n            eps = self.config.epsilon,\n            correct_bias=True\n        )\n\n        # Defining LR Scheduler\n        self.num_training_steps= len(self._train_dataloader)*self.config.epochs\n        self.num_warmup_steps= self.num_training_steps * self.config.warmup_ratio\n\n        scheduler = get_linear_schedule_with_warmup(\n            optimizer, num_warmup_steps = self.num_warmup_steps, num_training_steps= self.num_training_steps\n        )\n\n        self.scheduler=scheduler\n        self.optimizer=optimizer\n        \n        return {\n            'optimizer': self.optimizer,\n            'lr_scheduler': {\n                'scheduler': self.scheduler,\n                'monitor': 'val_loss',\n            }\n        }\n    \n    \n    def training_step(self, batch, batch_idx):\n        input_ids = batch[\"input_ids\"]\n        attention_mask = batch[\"attention_mask\"]\n        targets_start = batch[\"start_position\"]\n        targets_end = batch['end_position']\n        \n        # NOTE: Pass the data to self(param1, param2) instead of self.model(param1, param2) as this allows data to return in tensor format to the loss function, instead of strings. \n        # This was a critical point, as without this, there were continuous errors to get the loss function working.\n        outputs_start, outputs_end = self(input_ids, attention_mask=attention_mask)\n        loss = loss_fn((outputs_start, outputs_end), (targets_start, targets_end))\n    \n        self.train_loss += loss.item() * input_ids.size(0)\n        self.t_data_size += input_ids.size(0)\n        \n        epoch_loss = self.train_loss\/self.t_data_size\n        self.log('train_loss', epoch_loss, on_epoch=True, prog_bar=True, logger=True)\n        tqdm_dict = {'train_loss': loss}\n        return OrderedDict({\n            \"loss\": loss,\n            \"progress_bar\": tqdm_dict,\n            \"log\": tqdm_dict\n        })\n    \n    def validation_step(self, batch, batch_idx):\n        input_ids = batch[\"input_ids\"]\n        attention_mask = batch[\"attention_mask\"]\n        targets_start = batch[\"start_position\"]\n        targets_end = batch['end_position']\n        \n        #Note: Same as training step, pass to self instead of self.model\n        outputs_start, outputs_end = self(input_ids, attention_mask=attention_mask)\n        loss = loss_fn((outputs_start, outputs_end), (targets_start, targets_end))\n        \n        self.val_loss += loss.item() * input_ids.size(0)\n        self.v_data_size += input_ids.size(0)\n        \n        epoch_loss = self.val_loss\/self.v_data_size\n        logs = {\n            \"val_loss\": epoch_loss,\n        }\n        self.log_dict(logs, on_epoch=True, prog_bar=True, logger=True)\n        return logs\n    \n    def validation_end(self, outputs):\n        val_loss = sum([out[\"val_loss\"] for out in outputs]) \/ len(outputs)\n        result = {\"progress_bar\": tqdm_dict, \"log\": tqdm_dict, \"val_loss\": val_loss}\n        return result\n   \n    def train_dataloader(self):\n        return self._train_dataloader\n\n    def val_dataloader(self):\n        return self._val_dataloader\n","7d7e1848":"lr_monitor = LearningRateMonitor(logging_interval='step')\nlogger = CSVLogger(save_dir='logs\/')\n# Checkpoint\nckpt = ModelCheckpoint(\n    monitor='val_loss',\n    save_top_k=1,\n    save_last=True,\n    save_weights_only=True,\n    filename='checkpoint\/{epoch:02d}-{val_loss:.4f}',\n    verbose=False,\n    mode='min',\n)\n# Earlystopping\nearlystopping = EarlyStopping(monitor='val_loss', patience=5, mode='min')","339d262e":"#fold1 train\nfor fold in range(1):\n    print(\"\\n\" + '-'*50)\n    print(f'FOLD: {fold}')\n\n    model = Model(fold, Config())\n    # instrument experiment with W&B\n    # wandb_logger = WandbLogger(project='Chaii_QA', log_model='all',job_type='train')\n    trainer = pl.Trainer(\n        logger=logger,\n        max_epochs=Config().epochs,\n        callbacks=[ckpt, lr_monitor], # earlystopping\n        gpus=1,\n        precision=16,\n        accumulate_grad_batches=12,\n        val_check_interval=0.25,\n        weights_summary='top',\n        auto_lr_find=True,\n    )\n    \n    lr_find_kwargs = dict(min_lr=1e-5, max_lr=1e-3, num_training=35)\n    trainer.tune(model, lr_find_kwargs=lr_find_kwargs)\n    print(f\"LR: {model.learning_rate}\")\n    \n    # log gradients and model topology\n    # wandb_logger.watch(model)\n    trainer.fit(model)\n    # wandb.finish()","60e2557e":"metrics = pd.read_csv(f'{trainer.logger.log_dir}\/metrics.csv')\ndisplay(metrics.head())\n\naggreg_metrics = []\nagg_col = \"epoch\"\nfor i, dfg in metrics.groupby(agg_col):\n    agg = dict(dfg.mean())\n    agg[agg_col] = i\n    aggreg_metrics.append(agg)\n\ndf_metrics = pd.DataFrame(aggreg_metrics)\ndf_metrics[['train_loss_step', 'val_loss']].plot(grid=True, legend=True, xlabel=agg_col)\n# df_metrics[['valid_f1', 'valid_acc', 'train_acc']].plot(grid=True, legend=True, xlabel=agg_col)","fbf3c20e":"# Dataset class and Dataloader","b6e8436d":"All of this training was done with XLM-Roberta-Base, as that fit easily into the GPU reuirements for Kaggle. This notebook was trained on just one fold to showcase the way a pytorch lighning framework can be used for the quesiton answering task. The same can also be modified to use for any particular task need. \n\n","11c2d676":"# Introduction\n\n**THIS is adjusted fork of https:\/\/www.kaggle.com\/hoshi7\/chaii-pytorch-lightining-w-b**\n\nThe model building aspect was taken from: https:\/\/www.kaggle.com\/rhtsingh\/chaii-qa-5-fold-xlmroberta-torch-fit\n\nOver the course of the month, I have learnt a lot about transformers and pytorch. During one such lesson, I stumbled across Pytorch-lightning and how it can create a general framework for the pytorch deep learning model that we are building. \nKeeping that in mind, I set across learning about how to structure regular pytorch code into lightning code. This is one such attempt at that, with WanDB to showcase the ML-OPS part of the training. ","657f5892":"Also, over the course of a lot of learning, I realised that most of the tutorials that can be found easily on the web generally use a prepackaged model such as BertSequenceClassification, this made it difficult to figure out where the _forward_ step should go, or where the linear layers should have gone. Overcoming that obstacle was the main challenge of this notebook, along with finding a way to fit the model on Kaggle's less optimal memory provision. \n\nThus, this notebook highlights upon: \n1. Use of custom model (XLM Roberta) with pytorch lightning. \n2. Weights and Biases to showcase the ML-OPS\n\n\n","e7a009ab":"## Preparing Data","85b5d17d":"## Installing and Importing Libraries\n\nInstalling pytorch lightning and torch to a newer version as torch 1.7.0 has an error while training with lightning. ","f47dcc34":"# Loading data","6dc7411b":"![image.jpg](data:image\/png;base64,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)","caa997e9":"# Run Model with Pytorch lightning","9a8bc236":"If you found this useful, please consider upvoting. Drop a comment if there are some potential tricks to optimise the model better. Thanks a lot! ","4b60eb1e":"## Necessary Functions\n\nThe index of necessary functions in order: \n- **create_folds**: Kfold data prep\n- **fix_all_seeds**: Allows us to reproduce it: Code from: https:\/\/www.kaggle.com\/rhtsingh\/chaii-qa-5-fold-xlmroberta-torch-fit\n- **optimal_num_of_loader_workers**: Find the optimal number of workers based on config. Code from: https:\/\/www.kaggle.com\/rhtsingh\/chaii-qa-5-fold-xlmroberta-torch-fit","b7a1ec49":"# Defining the model with Pytorch Lightning\n","94d3b518":"# Outputs","4b75d88f":"# Defining Configuration"}}