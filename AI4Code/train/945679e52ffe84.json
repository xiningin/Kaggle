{"cell_type":{"c194b37c":"code","c1a6c2e9":"code","6e4863ab":"code","fc0ebf8a":"code","c2c5b944":"code","28125698":"code","f10441ef":"code","975e727b":"code","e67d3495":"code","9847de3b":"code","b8973ffc":"code","18d6ba51":"markdown","f2cb6fea":"markdown","fb6e6f02":"markdown","308024b4":"markdown","9b081623":"markdown"},"source":{"c194b37c":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os","c1a6c2e9":"# Kaggle environments.\n!git clone https:\/\/github.com\/Kaggle\/kaggle-environments.git\n!cd kaggle-environments && pip install .\n\n# GFootball environment.\n!apt-get update -y\n!apt-get install -y libsdl2-gfx-dev libsdl2-ttf-dev\n\n# Make sure that the Branch in git clone and in wget call matches !!\n!git clone -b v2.6 https:\/\/github.com\/google-research\/football.git\n!mkdir -p football\/third_party\/gfootball_engine\/lib\n\n!wget https:\/\/storage.googleapis.com\/gfootball\/prebuilt_gameplayfootball_v2.6.so -O football\/third_party\/gfootball_engine\/lib\/prebuilt_gameplayfootball.so\n!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install .\n\n# keras reinforcement learning\n!pip install reinforcement_learning_keras","6e4863ab":"import keras\nimport math\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\n\nclass RLNAgent:\n    def __init__(self):\n        self.memory = []\n        self.rewards = []\n        self.gamma = 0.3\n        self.epsilon = 0.7\n        self.epsilon_decay = 0.85\n        self.epsilon_min = 0.05\n        self.learning_rate = 0.0002\n        self._build_model()\n    \n    # setting mechanic learning model\n    def _build_model(self):\n        model = Sequential()\n        model.add(Dense(157, input_dim = 103, activation = 'linear'))\n        #model.add(Dropout(0.1))\n        model.add(Dense(101, activation = 'tanh'))\n        model.add(Dense(83, activation = 'tanh'))\n        model.add(Dense(29, activation = 'tanh'))\n        model.add(Dense(18, activation = 'tanh'))\n        model.compile(loss='mean_squared_error', optimizer = keras.optimizers.Adam(lr=self.learning_rate))\n        self.model = model\n    \n    # transfor json type to (1,103) array\n    def state_as_sample103(self, obs):\n        sample103 = np.concatenate((\n            np.array(obs['left_team'][obs['active']]).flatten(),\n            np.array(obs['left_team_direction'][obs['active']]).flatten(),\n            np.array(obs['ball']).flatten(),\n            np.array(obs['ball_direction']).flatten(),\n            np.array(obs['left_team'][obs['active']]).flatten() - np.array(obs['ball']).flatten()[0:1],\n            np.array(obs['left_team']).flatten(),\n            np.array(obs['left_team_direction']).flatten(),\n            np.array(obs['right_team']).flatten(),\n            np.array(obs['right_team_direction']).flatten(),\n            (lambda x: [1,0,0] if x == -1 else ([0,1,0] if x == 0 else [0,0,1]))(obs['ball_owned_team'])\n        ))\n        return sample103.reshape((1,103))\n    \n    # remember the play state\n    def remember(self, state, predict, action, reward, next_state):\n        self.memory.append((state, predict, action, reward, next_state))\n        self.rewards.append(reward)\n\n    # get the model predict for the player's best action\n    def act_predict(self, state):\n        return self.model.predict(state)\n    \n    # using reward to train the model\n    def replay(self, batch_size, maxreward):\n        batches = np.arange(1, len(self.memory), 1)\n        memory_max_length = len(self.memory)\n        \n        for i in batches:\n            state, predict, action, reward, next_state = self.memory[i]\n\n            # add noise\n            target_f = (predict * (1-self.epsilon)) + (np.random.rand(1,18) * self.epsilon)\n            \n            # the training target value is the addition rewards with follow-up (1.5s) effect            \n            followup_rewards = 0\n            for j in range(i, min(i + 30, memory_max_length), 1):\n                followup_rewards = followup_rewards + (self.rewards[j] * math.exp( (i - j) * 2 ))\n            \n            #if followup_rewards < 1 and followup_rewards > 0 and (self.rewards[i] - self.rewards[i-1]) < 0:\n            #    continue\n            #print(\"{}, followup_rewards:{}\".format(i,followup_rewards))\n            \n            target = target_f[0][action] + (followup_rewards * 0.01) + (self.rewards[i] - self.rewards[i-1]) * 0.5\n            \n            # limit the max target\n            if target > 0.8:\n                target = 0.8\n\n            target_f[0][action] = target\n            \n            # training\n            self.model.fit(state, target_f, epochs = 1, verbose = 0)\n        \n        if self.epsilon > self.epsilon_min:\n            self.epsilon *= self.epsilon_decay\n                        \n    def save(self):\n        self.model.save('rl_model')\n        \n    def load(self):\n        self.model = keras.models.load_model('rl_model')","fc0ebf8a":"from kaggle_environments import make\nimport copy\nimport random\n\nenv = make(\"football\",\n           configuration={\"save_video\": False, \n                          \"scenario_name\": \"11_vs_11_kaggle\", \n                          \"running_in_notebook\": True})","c2c5b944":"def closed_contestant_num(obs, controlled_player_pos):\n    closed_num = 0\n    for i in range(1, len(obs[\"right_team\"])):\n        if abs(controlled_player_pos[0] - obs[\"right_team\"][i][0]) < 0.08 and abs(controlled_player_pos[1] - obs[\"right_team\"][i][1]) < 0.04 :\n            closed_num = closed_num + 1\n    return closed_num\n\ndef rule_based_agent(obs):\n    controlled_player_pos = obs['left_team'][obs['active']]\n    if obs['ball_owned_player'] == obs['active'] and obs['ball_owned_team'] == 0:\n        closed_people = closed_contestant_num(obs, controlled_player_pos)\n        if controlled_player_pos[0] > 0.5 and closed_people < 4:\n            return 12\n        if controlled_player_pos[0] > 0.5 and controlled_player_pos[1] > 0.3 and closed_people > 5:\n            return 3\n        if controlled_player_pos[0] > 0.5 and controlled_player_pos[1] < -0.3 and closed_people > 5:\n            return 7\n        return 5\n    else:        \n        if obs['ball'][0] > controlled_player_pos[0] + 0.05:\n            return 5\n        if obs['ball'][0] < controlled_player_pos[0] - 0.05:\n            return 1\n        if obs['ball'][1] > controlled_player_pos[1] + 0.05:\n            return 7\n        if obs['ball'][1] < controlled_player_pos[1] - 0.05:\n            return 3\n        return 16","28125698":"agent = RLNAgent()\n\nepisodes = 150  # playing the game up to {episodes} times\nsteps = 2000  # each game with {steps} steps\n\nfor e in range(episodes):\n    \n    env.reset()\n    agent.memory = []\n    agent.rewards = []\n    \n    trainer = env.train([None, \"run_right\"])\n    trainer.reset()\n    \n    obs = env.state[0]['observation']['players_raw'][0]\n    maxreward = -10\n    \n    for time_t in range(steps):    # simulating the game step by step\n        \n        action = 0\n        \n        state = agent.state_as_sample103(obs)\n        predict = agent.act_predict(state)\n        action = np.argmax(predict)\n        \n        # learning from rule based agent coach\n        if e < 10:\n            action = rule_based_agent(obs)\n            predict = np.reshape([(lambda x, y: 1 if x==y else 0)(x, action) for x in range(18)], (1,18))\n        \n        next_obs, reward, done, info = trainer.step([action])\n        \n        reward = - 10 if reward == None else reward\n            \n        next_obs = next_obs['players_raw'][0]\n        next_state = agent.state_as_sample103(next_obs)\n        \n        # if we steal the ball, the reward will gain 0.05 point\n        if obs['ball_owned_team'] == 1 and next_obs['ball_owned_team'] == 0:\n            reward += 0.05\n\n        # if we get the ball, the reward will gain with the right moving distince\n        if obs['ball_owned_team'] == 0:\n            reward += ( next_obs['ball'][0] - obs['ball'][0] ) * 0.1 - abs( next_obs['ball'][1] ) * 0.01\n\n        # if active player far from the ball, the reward will lose by the moving distance\n        distance = abs(obs['left_team'][obs['active']][0] - obs['ball'][0]) + abs(obs['left_team'][obs['active']][1] - obs['ball'][1])\n        next_distance = abs(next_obs['left_team'][next_obs['active']][0] - next_obs['ball'][0]) + abs(next_obs['left_team'][next_obs['active']][1] - next_obs['ball'][1])\n        \n        if abs(next_obs['left_team'][next_obs['active']][0] - obs['left_team'][obs['active']][0]) < 0.01 and abs(next_obs['left_team'][next_obs['active']][1] - obs['left_team'][obs['active']][1]) < 0.01 :\n            reward -= 0.5\n        \n        if next_obs['ball_owned_team'] != 0:\n            reward += - distance - (next_distance - distance) * 2\n\n        # if the next action is shooting the ball, the reward will gain 0.2 point\n        if obs['ball_owned_team'] == 0 and obs['left_team'][obs['active']][0] > 0.5 and action == 12:\n            reward += 0.2\n            \n        if obs['left_team'][obs['active']][0] < 0 and action == 12:\n            reward -= 0.05\n            \n        # if we go out the square, the reward will lose 2 point\n        if obs['left_team'][obs['active']][0] > 0.98 or obs['left_team'][obs['active']][0] < - 0.98 or obs['left_team'][obs['active']][1] < - 0.39 or obs['left_team'][obs['active']][1] > 0.39:\n            reward -= 2\n        \n        agent.remember(state, predict, action, reward, next_state)\n        \n        obs = copy.deepcopy(next_obs)\n        \n        if maxreward < reward:\n            maxreward = reward\n\n    print(\"episode: {}\/{}, score: {}, action: {}, probability: {}\".format(e + 1, episodes, maxreward, action, np.max(predict,axis = 1)))\n    agent.replay(steps, maxreward)","f10441ef":"agent.save()","975e727b":"%%writefile main.py\n# for making a video\n\nimport numpy as np # linear algebra\nfrom kaggle_environments.envs.football.helpers import *\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n\n\nclass QNAgent:\n    def __init__(self):\n        self.load()\n        \n    def act(self, state):\n        act_values = self.model.predict(state)\n        return np.argmax(act_values[0])\n        \n    def load(self):\n        self.model = keras.models.load_model('\/kaggle\/working\/rl_model')\n\n    def state_as_sample103(self, obs):\n        sample103 = np.concatenate((\n            np.array(obs['left_team'][obs['active']]).flatten(),\n            np.array(obs['left_team_direction'][obs['active']]).flatten(),\n            np.array(obs['ball']).flatten(),\n            np.array(obs['ball_direction']).flatten(),\n            np.array(obs['left_team'][obs['active']]).flatten() - np.array(obs['ball']).flatten()[0:1],\n            np.array(obs['left_team']).flatten(),\n            np.array(obs['left_team_direction']).flatten(),\n            np.array(obs['right_team']).flatten(),\n            np.array(obs['right_team_direction']).flatten(),\n            (lambda x: [1,0,0] if x == -1 else ([0,1,0] if x == 0 else [0,0,1]))(obs['ball_owned_team'])\n        ))\n        return sample103.reshape((1,103))\n\n\nqagent = QNAgent()\n\nActionDic = [Action.Idle,\n             Action.Left,\n             Action.TopLeft,\n             Action.Top,\n             Action.TopRight,\n             Action.Right,\n             Action.BottomRight,\n             Action.Bottom,\n             Action.BottomLeft,\n             Action.LongPass,\n             Action.HighPass,\n             Action.ShortPass,\n             Action.Shot,\n             Action.Sprint,\n             Action.ReleaseDirection,\n             Action.ReleaseSprint,\n             Action.Slide,\n             Action.Dribble,\n             Action.ReleaseDribble\n            ]\n\n@human_readable_agent\ndef agent(obs):\n    state = qagent.state_as_sample103(obs)\n    action = qagent.act(state)    \n    return ActionDic[action]","e67d3495":"from kaggle_environments import make\n\nenv = make(\"football\", configuration={\"save_video\": True, \"scenario_name\": \"11_vs_11_kaggle\", \"running_in_notebook\": True})\noutput = env.run([\"\/kaggle\/working\/main.py\", \"run_right\"])[-1]\n\nprint('Left player: reward = %s, status = %s, info = %s' % (output[0]['reward'], output[0]['status'], output[0]['info']))\nprint('Right player: reward = %s, status = %s, info = %s' % (output[1]['reward'], output[1]['status'], output[1]['info']))\n\nenv.render(mode=\"human\", width=800, height=600)","9847de3b":"%%writefile main.py\n# for submition\n\nimport numpy as np # linear algebra\nfrom kaggle_environments.envs.football.helpers import *\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n\n\nclass QNAgent:\n    def __init__(self):\n        self.load()\n        \n    def act(self, state):\n        act_values = self.model.predict(state)\n        return np.argmax(act_values[0])\n        \n    def load(self):\n        self.model = keras.models.load_model('\/kaggle_simulations\/agent\/rl_model')\n\n    def state_as_sample103(self, obs):\n        sample103 = np.concatenate((\n            np.array(obs['left_team'][obs['active']]).flatten(),\n            np.array(obs['left_team_direction'][obs['active']]).flatten(),\n            np.array(obs['ball']).flatten(),\n            np.array(obs['ball_direction']).flatten(),\n            np.array(obs['left_team'][obs['active']]).flatten() - np.array(obs['ball']).flatten()[0:1],\n            np.array(obs['left_team']).flatten(),\n            np.array(obs['left_team_direction']).flatten(),\n            np.array(obs['right_team']).flatten(),\n            np.array(obs['right_team_direction']).flatten(),\n            (lambda x: [1,0,0] if x == -1 else ([0,1,0] if x == 0 else [0,0,1]))(obs['ball_owned_team'])\n        ))\n        return sample103.reshape((1,103))\n\n\nqagent = QNAgent()\n\nActionDic = [Action.Idle,\n             Action.Left,\n             Action.TopLeft,\n             Action.Top,\n             Action.TopRight,\n             Action.Right,\n             Action.BottomRight,\n             Action.Bottom,\n             Action.BottomLeft,\n             Action.LongPass,\n             Action.HighPass,\n             Action.ShortPass,\n             Action.Shot,\n             Action.Sprint,\n             Action.ReleaseDirection,\n             Action.ReleaseSprint,\n             Action.Slide,\n             Action.Dribble,\n             Action.ReleaseDribble\n            ]\n\n@human_readable_agent\ndef agent(obs):\n    state = qagent.state_as_sample103(obs)\n    action = qagent.act(state)    \n    return ActionDic[action]","b8973ffc":"!tar -czvf submission.tar.gz main.py rl_model","18d6ba51":"This notebook is a experimental model of reinforcement learning.","f2cb6fea":"# Playing Games and Train Model","fb6e6f02":"## Rule based agent as the starting point of reinforcement learning","308024b4":"# Setting Model","9b081623":"## Making the simulation environment"}}