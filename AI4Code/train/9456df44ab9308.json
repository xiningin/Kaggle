{"cell_type":{"9bf4ed3c":"code","1ba001f7":"code","615a1c41":"code","344c9c43":"code","184fb909":"code","858a82f7":"code","8db489aa":"code","db4450f4":"code","e328a9f2":"code","58e85f18":"code","03d6fd40":"code","1487f993":"code","663c9a71":"code","8269fd14":"code","aa214c86":"code","05e647d9":"markdown","70e1c6db":"markdown","f7a4142c":"markdown","e24fb826":"markdown","534ab11a":"markdown","70284c6d":"markdown"},"source":{"9bf4ed3c":"!pip install -qq git+https:\/\/www.github.com\/ildoonet\/tf-pose-estimation","1ba001f7":"!pip install -qq pycocotools","615a1c41":"%load_ext autoreload\n%autoreload 2\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.rcParams[\"figure.figsize\"] = (8, 8)\nplt.rcParams[\"figure.dpi\"] = 125\nplt.rcParams[\"font.size\"] = 14\nplt.rcParams['font.family'] = ['sans-serif']\nplt.rcParams['font.sans-serif'] = ['DejaVu Sans']\nplt.style.use('ggplot')\nsns.set_style(\"whitegrid\", {'axes.grid': False})","344c9c43":"%matplotlib inline\nimport tf_pose\nimport cv2\nfrom glob import glob\nfrom tqdm import tqdm_notebook\nfrom PIL import Image\nimport numpy as np\nimport os\ndef video_gen(in_path):\n    c_cap = cv2.VideoCapture(in_path)\n    while c_cap.isOpened():\n        ret, frame = c_cap.read()\n        if not ret:\n            break\n        yield c_cap.get(cv2.CAP_PROP_POS_MSEC), frame[:, :, ::-1]\n    c_cap.release()","184fb909":"video_paths = glob('..\/input\/1105-inputsports\/*.mp4')\t#\u8abf\u6574\u6a94\u6848\u8def\u5f91\nprint(video_paths)","858a82f7":"video_paths = glob('..\/input\/1105-inputsports\/*.mp4')\t#\u8abf\u6574\u6a94\u6848\u8def\u5f91\nc_video = video_gen(video_paths[0])\nfor _ in range(200):\t\t\t\t\t\t\t\t\t#\u64f7\u53d6\u524d200\u5f35\n    c_ts, c_frame = next(c_video)\nplt.imshow(c_frame)","8db489aa":"from tf_pose.estimator import TfPoseEstimator\nfrom tf_pose.networks import get_graph_path, model_wh\ntfpe = tf_pose.get_estimator()","db4450f4":"humans = tfpe.inference(npimg=c_frame, upsample_size=4.0)\nprint(humans)","e328a9f2":"new_image = TfPoseEstimator.draw_humans(c_frame[:, :, ::-1], humans, imgcopy=False)\nfig, ax1 = plt.subplots(1, 1, figsize=(10, 10))\nax1.imshow(new_image[:, :, ::-1])","58e85f18":"body_to_dict = lambda c_fig: {'bp_{}_{}'.format(k, vec_name): vec_val \n                              for k, part_vec in c_fig.body_parts.items() \n                              for vec_name, vec_val in zip(['x', 'y', 'score'],\n                                                           (part_vec.x, 1-part_vec.y, part_vec.score))}\nc_fig = humans[0]\nbody_to_dict(c_fig)","03d6fd40":"MAX_FRAMES = 13000\nbody_pose_list = []\nfor vid_path in tqdm_notebook(video_paths, desc='Files'):\n    c_video = video_gen(vid_path)\n    c_ts, c_frame = next(c_video)\n    out_path = os.path.split(vid_path)[1].replace('.mp4','_out.mp4')\n    out = cv2.VideoWriter(out_path,\n                          cv2.VideoWriter_fourcc(*'mp4v'),\n                          10, \n                          (c_frame.shape[1], c_frame.shape[0]))\n    for (c_ts, c_frame), _ in zip(c_video, \n                                  tqdm_notebook(range(MAX_FRAMES), desc='Frames')):\n        bgr_frame = c_frame[:,:,::-1]\n        humans = tfpe.inference(npimg=bgr_frame, upsample_size=4.0)\n        for c_body in humans:\n            body_pose_list += [dict(video=out_path, time=c_ts, **body_to_dict(c_body))]\n        new_image = TfPoseEstimator.draw_humans(bgr_frame, humans, imgcopy=False)\n        out.write(new_image)\n    out.release()\n","1487f993":"import pandas as pd\nbody_pose_df = pd.DataFrame(body_pose_list)\nbody_pose_df.describe()","663c9a71":"fig, m_axs = plt.subplots(1, 2, figsize=(15, 5))\nfor c_ax, (c_name, c_rows) in zip(m_axs, body_pose_df.groupby('video')):\n    for i in range(17):\n        c_ax.plot(c_rows['time'], c_rows['bp_{}_y'.format(i)], label='x {}'.format(i))\n    c_ax.legend()\n    c_ax.set_title(c_name)","8269fd14":"fig, m_axs = plt.subplots(1, 2, figsize=(15, 5))\nfor c_ax, (c_name, n_rows) in zip(m_axs, body_pose_df.groupby('video')):\n    for i in range(17):\n        c_rows = n_rows.query('bp_{}_score>0.6'.format(i)) # only keep confident results\n        c_ax.plot(c_rows['bp_{}_x'.format(i)], c_rows['bp_{}_y'.format(i)], label='BP {}'.format(i))\n    c_ax.legend()\n    c_ax.set_title(c_name)","aa214c86":"# output body-pose .csv for analysis\nbody_pose_df.to_csv('body_pose_ufc.csv', index=False)","05e647d9":"## [Code](https:\/\/www.kaggle.com\/ashkhagan\/pose-estimate)\nThe kernel shows how to use the [tf_pose_estimation](https:\/\/github.com\/ildoonet\/tf-pose-estimation) package in Python on a series of running videos.","70e1c6db":"## *Download .csv & .mp4 for sport analysis*","f7a4142c":"## Save Files","e24fb826":"## Output Body-Pose to .csv ","534ab11a":"## Repro [Github](https:\/\/www.github.com\/ildoonet\/tf-pose-estimation)\nInstall tf_pose and pycocotools","70284c6d":"# Pose Estimate"}}