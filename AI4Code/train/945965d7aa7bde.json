{"cell_type":{"24b31886":"code","c9b251d0":"code","b318b999":"code","843b323f":"code","32283cc6":"code","bf3a8545":"code","b9b05e64":"code","b5d2d7fa":"code","5d904ab0":"code","8523c4d4":"code","f0329af3":"code","0ad9402a":"code","50df6f6a":"code","6966e2f1":"code","d3c8a4d5":"code","d6d9a2f8":"code","de57cf4a":"code","69c80cfc":"code","338abe46":"code","86388464":"code","e02c09da":"code","45144951":"code","32172b32":"markdown","e0e400d5":"markdown","8ed5cbf0":"markdown","b90010da":"markdown","9c1bf42b":"markdown","462b0a15":"markdown","d586f4c2":"markdown","70b6a560":"markdown"},"source":{"24b31886":"# 1. Python open()\n\nimport os\nwith open('..\/input\/poetry\/adele.txt','r') as f:\n    adele = f.read()","c9b251d0":"# 2. Web Scraping\n\nimport requests\nfrom bs4 import BeautifulSoup","b318b999":"from time import sleep\nsleep(5)","843b323f":"url = 'https:\/\/www.udacity.com\/courses\/all'\npage = requests.get(url)","32283cc6":"# There are two methods of parsing the html page. 1st - Regular Expressions. 2nd - BeautifulSoup\n# Regular Expression:\n\nimport re","bf3a8545":"re.findall('[^.,?\/''\"\":;!@#$%&*() ]+', page.text)","b9b05e64":"pattern = re.compile(r'<,.*?>')\n# page_clean = print(pattern.sub(' ', page.text)) # Replaces patterns with space","b5d2d7fa":"# Beautiful Soup\n\nsoup = BeautifulSoup(page.text, 'html5lib')\nprint(soup.getText())","5d904ab0":"soup = BeautifulSoup(page.content, 'html.parser')\nfor i in soup.select('a'):\n    print(i.getText())","8523c4d4":"#1. Lower Case\n\nadele = adele.lower()\n\n#2. Punctuation Removal\nimport string\npunc = string.punctuation\nadele = ''.join([char for char in adele if char not in punc]) #for loops may be inefficient sometimes\n","f0329af3":"adele[:500]","0ad9402a":"#Removing newlines\n#strip function does not remove whitespaces from the middle part of the text. It removes white spaces only from the \n#beginnings and end. Let's use replace to get rid of the newlines.\n\nadele = adele.replace('\\n', ' ')\nadele[:500]","50df6f6a":"# using regular expression to get rid of punctuations:\nwith open('..\/input\/poetry\/beatles.txt') as g:\n    beatles = g.read()\n\n#Normalization\n#Lower case\n\nbeatles = beatles.lower()\n\n#Removing Punctuation\nimport re\nbeatles = re.sub('[^a-zA-Z0-9\\s]','',beatles) #replacing not a-z, A-Z, 0-9 and whitespaces with ''.\n","6966e2f1":"beatles = beatles.replace('\\n', ' ')","d3c8a4d5":"#1. split()\n\nadele_words = adele.split()\nadele_words[:5]","d6d9a2f8":"#2. nltk\n\nfrom nltk.tokenize import word_tokenize\nadele_nltk_word =word_tokenize(adele)\nadele_nltk_word[:2]","de57cf4a":"from nltk.corpus import stopwords\n\nadele_stopwords_removed = [w for w in adele_nltk_word if w not in stopwords.words('english')]\nadele_stopwords_removed[:5]","69c80cfc":"from nltk import pos_tag\nadele_pos_tag = pos_tag(adele_stopwords_removed)\nadele_pos_tag[:5]","338abe46":"from nltk import ne_chunk\nadele_named_entity = ne_chunk(adele_pos_tag)","86388464":"print(ne_chunk(pos_tag(word_tokenize(\"Don't Bullshit me\"))))","e02c09da":"print(ne_chunk(pos_tag(word_tokenize(\"This is bullshit\"))))","45144951":"from nltk.stem import LancasterStemmer\nfrom nltk.stem import PorterStemmer\n\nps = PorterStemmer()\nlist(map(ps.stem,adele_stopwords_removed))","32172b32":"# 3. Normalization","e0e400d5":"# P5M1L5 Text Processing - Capturing Text Data\n","8ed5cbf0":"# 5. Stopwords Removal","b90010da":"# 7. Named Entity Recognition","9c1bf42b":"### Steps of Natural Language Processing:\n1. Capturing Text Data (file, web scraping etc)\n2. Cleaning Raw Data (removing tags (of html), extracting useful text from html, removing extra white spaces,etc)\n3. Normalization - Lowercase, removing punctuation\n4. Tokenization - Token is a fancy word for 'words'. Generally we break text document in words.\n5. Stop Word removal\n6. Part of Speech Tagging\n7. Named Entity Recognition\n8. Stemming and Lemmatization\n    * Stemming doesn't use dictionary. Changing, Changed, Change ----- converted to chang\n    * Lammatization uses dictionary. Changing, Changed, Change ------- converted to change","462b0a15":"# 4. Tokenization \n2 methods:\n1. str.split() - but it converts Dr. to 'Dr'\n2. nltk - it's smart enough to consider Dr. as Dr.","d586f4c2":"# 8. Stemming and Lemmatization\n\n1. Stemming:\n       It does not require any context. The words are stripped off the suffixes based on the fixed rules. Sometimes, \n       it produces words without meaninng. Two methods for English is available in nltk. PorterStemmer and\n       LancasterStemmer. Porter is older one, follows 5 rules. Lancaster is the newer one, follows 120 rules. Lancaster \n       causes overstemming due to aggressive iterations.\n2. Lemmatization:\n        It connects to wordnet database and produces meaningful words.","70b6a560":"# 6. Parts of Speech Tagging"}}