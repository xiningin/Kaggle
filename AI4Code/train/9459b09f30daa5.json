{"cell_type":{"a462edba":"code","ef2ca52f":"code","82199301":"code","c0c85d6d":"code","5616528c":"code","c2058add":"code","80bac3a2":"code","b124cd71":"code","439e91d0":"code","e39520e0":"code","d2064043":"code","de8942c1":"code","978c18e5":"code","a28acd09":"code","f38e0844":"code","1f921794":"code","991c349a":"code","723aec8c":"code","83a8da93":"code","048840ff":"code","3d89a6d1":"code","b26c220f":"code","78294db2":"code","d3aafbfc":"code","781b4501":"markdown","53f05907":"markdown","5d856c1f":"markdown","0ec4147f":"markdown","194487a4":"markdown","ae766d6f":"markdown","3b307162":"markdown","74fee1f8":"markdown","806da428":"markdown","b0335766":"markdown","bf32efeb":"markdown","27ee16e0":"markdown","2d8aee04":"markdown","e3f8b273":"markdown","e50beb80":"markdown","835a8bb1":"markdown","461eeb1e":"markdown"},"source":{"a462edba":"%%HTML\n<a id=\"Analysis\"><\/a>\n<center>\n<iframe width=\"700\" height=\"315\" src=\"https:\/\/www.youtube.com\/embed\/FqxllA_HCR8\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" style=\"position: relative;top: 0;left: 0;\" allowfullscreen ng-show=\"showvideo\"><\/iframe>\n<\/center>\n","ef2ca52f":"import numpy as np\nimport pandas as pd\n\n# for vis\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder, OrdinalEncoder\n","82199301":"path = '..\/input\/house-prices-advanced-regression-techniques'\ntrain = pd.read_csv(path + r'\/train.csv', )\ntest = pd.read_csv(path + r'\/test.csv')\n\ntrain.name = 'train'\ntest.name = 'test'\n\n# keeping testing id for submission in the future\ntest_id = test.Id\nfor df in [train, test]:\n    df.drop(columns = ['Id'], inplace = True)\n    \ndf_concat = pd.concat([train, test], axis = 0).reset_index(drop = True)\ndf_concat.name = 'both dfs'\n\ndf_concat.loc[:train.shape[0], 'which'] = 'train'\ndf_concat.loc[train.shape[0]:, 'which'] = 'test'","c0c85d6d":"print('Number of training examples: ', train.shape[0])\nprint('Number of testing examples: ', test.shape[0])\nprint('Testing set to the whole: ', round(test.shape[0]\/df_concat.shape[0], 2))\nprint('Number of features:', test.shape[1])\nf_y = min(df_concat['YrSold'])\nf_m = min(df_concat[df_concat['YrSold'] == f_y].MoSold)\nprint('Selling records start from {}\\{}'.format(f_y, f_m))","5616528c":"fig = make_subplots(rows=1, cols=2)\nfig.add_trace(\n    go.Bar(name='Sum', x=train.groupby('YrSold').sum().index\n           , y = train.groupby('YrSold').sum().SalePrice),row=1, col=1\n)\nfig.add_trace(\n    go.Bar(name='Count', x=train.groupby('YrSold').count().index,\n           y=train.groupby('YrSold').count().SalePrice),row=1, col=2\n)\n\nfig.update_yaxes(title_text=\"sum\", row=1, col=1)\nfig.update_yaxes(title_text=\"count\", row=1, col=2)\nfig.update_xaxes(title_text=\"year\", row=1, col=1)\nfig.update_xaxes(title_text=\"year\", row=1, col=2)\nfig.update_layout(title_text=\"How Much?How Many?\", height=500)\nfig.show()\n","c2058add":"features = ['OverallQual', 'MoSold'] # add the name of the column to this list\n\nsns.set_style('whitegrid')\nfor feature in features:    \n    sns.catplot(x=str(feature), kind=\"count\", palette=\"PuRd_r\",\n                data=df_concat, col = \"which\")","80bac3a2":"def detect_nulls(*args, n=30):\n    df_null = pd.DataFrame()\n    for df in args:\n        df_null_n = df.isnull().sum()\n        df_null_n = pd.DataFrame(np.c_[df_null_n, np.round(df_null_n\/len(df) *100, 2)],\n                                 columns=['count', 'percentage'], index = df_null_n.index)\n        \n        df_null_n.sort_values(by = df_null_n.columns[0], ascending=False, inplace = True)\n        \n        df_null_n.columns = pd.MultiIndex.from_product([[str(df.name)], df_null_n.columns])\n        df_null = pd.concat([df_null, df_null_n], axis = 1)\n   \n    df_null = df_null.loc[[i for i in df_null.index if (df_null.loc[i, :] != 0).any() & (i not in ['SalePrice', 'which'])]]\n    num_nulls = len(df_null)\n    \n    print('How many features had at least one null value?',num_nulls)\n    print('{} most empty features of all:'.format(n))\n    return df_null[:n]","b124cd71":"df_null = detect_nulls(df_concat, train, test, n = 80) \ndf_null","439e91d0":"# making our dataset ready for vis\ndf_null = detect_nulls(df_concat, train, test, n = 81).reset_index().melt(id_vars = ['index'])\n# if you have question how this works just \n# print the df_null and you'll notice how it's done\n\nfig = px.bar(x = df_null.loc[df_null.variable_1 == 'percentage','index'],\n             y = df_null.loc[df_null.variable_1 == 'percentage'].value,\n             text = df_null.loc[df_null.variable_1 == 'count'].value,\n             title = 'Precentage of missing values per feature and dataset',\n             color = df_null.loc[df_null.variable_1 == 'count'].variable_0)\nfig.update_xaxes(title = 'Feature')\nfig.update_yaxes(title = 'Percentage')\nfig.show()","e39520e0":"features = ['PoolQC', 'MiscFeature', 'Alley']\norders = [[\"None\", \"Fa\", \"Gd\", \"Ex\"], None,None]\n\nfor i, feature in enumerate(features):\n    \n    \n    fig, ax = plt.subplots(figsize=(10, 5))\n    ax = sns.stripplot(x=feature, y=\"SalePrice\", data=train.fillna(\"None\"),\n                       order=orders[i], size = 8)\n    ax.set_title('number {} most empty feature!'.format(i+1))","d2064043":"df_concat.drop(columns = ['PoolQC'], inplace = True)","de8942c1":"# fields about the Garage\nfor field in ['GarageType', 'GarageFinish','GarageQual', 'GarageCond',\n              'BsmtFinType1','BsmtQual','BsmtCond', 'BsmtExposure', 'BsmtFinType1', \n              'BsmtFinType2','MiscFeature','Alley','Fence','FireplaceQu',\n               'MasVnrType' ] :\n    df_concat[field].fillna('None',inplace=True)\n    \nfor field in ['MasVnrArea','BsmtFullBath','BsmtHalfBath'\n              ,'BsmtFinSF1','GarageCars','GarageArea','TotalBsmtSF',\n             'BsmtUnfSF','BsmtFinSF2','GarageYrBlt','TotalBsmtSF']:\n    df_concat[field].fillna(0,inplace=True) \n","978c18e5":"df_concat['LotFrontage'] = df_concat.groupby('Neighborhood')['LotFrontage']\\\n                          .transform(lambda x: x.fillna(x.mean()))\nfor feature in ['MSZoning', 'Electrical']:\n    df_concat[feature] = df_concat.groupby('Neighborhood')[feature]\\\n                        .transform(lambda x: x.fillna(x.mode()[0]))\n    \n# There's only one null in each of the following that we fill\n# with the most frequent\nfor field in ['SaleType','Exterior1st','Exterior2nd',]:\n    df_concat[field].fillna(df_concat[field].mode()[0],inplace=True)","a28acd09":"df_concat.Functional.fillna('Typ',inplace=True)\ndf_concat.KitchenQual.fillna('TA',inplace=True)","f38e0844":"detect_nulls(df_concat)","1f921794":"df_concat.Utilities.value_counts()","991c349a":"df_concat.drop(columns = ['Utilities'], inplace = True)","723aec8c":"### ordinal that labelencoder detects their orders like years...\nordinal_fields_with_labelencoder=['LandSlope','YearBuilt','YearRemodAdd',\n                                  'CentralAir','GarageYrBlt','PavedDrive',\n                                  'YrSold']\n\n### ordinal with labelencoder...\nfor field in ordinal_fields_with_labelencoder:\n    le = LabelEncoder()\n    df_concat[field] = le.fit_transform(df_concat[field].values)\n    ","83a8da93":"features_that_are_already_ordinal = ['OverallQual','OverallCond','MoSold',\n                                     'FullBath','KitchenAbvGr','TotRmsAbvGrd']\n","048840ff":"### ordinal features that need to be sorted with ordinal encoder...\nfields_that_need_to_be_ordered = [\n              'MSSubClass','ExterQual','LotShape','BsmtQual','BsmtCond',\n              'BsmtExposure','BsmtFinType1', 'BsmtFinType2','HeatingQC',\n              'Functional','FireplaceQu','KitchenQual', 'GarageFinish',\n              'GarageQual','GarageCond','Fence'\n                                    ]\nfor field in  fields_that_need_to_be_ordered:\n    df_concat[field] = df_concat[field].astype(str)\n\n\norders=[#msclass\n    ['20','30','40','45','50','60','70','75','80','85', '90','120','150','160','180','190'],\n    #ExterQual\n    ['Po','Fa','TA','Gd','Ex'],\n    #LotShape\n    ['Reg','IR1' ,'IR2','IR3'],\n    #BsmtQual\n    ['None','Fa','TA','Gd','Ex'],\n    #BsmtCond\n    ['None','Po','Fa','TA','Gd','Ex'],\n    #BsmtExposure\n    ['None','No','Mn','Av','Gd'],\n    #BsmtFinType1\n    ['None','Unf','LwQ', 'Rec','BLQ','ALQ' , 'GLQ' ],\n    #BsmtFinType2\n   ['None','Unf','LwQ', 'Rec','BLQ','ALQ' , 'GLQ' ],\n    #HeatingQC\n    ['Po','Fa','TA','Gd','Ex'],\n    #Functional\n   ['Sev','Maj2','Maj1','Mod','Min2','Min1','Typ'],\n    #FireplaceQu\n    ['None','Po','Fa','TA','Gd','Ex'],\n    #KitchenQual\n    ['Fa','TA','Gd','Ex'],\n    #GarageFinish\n    ['None','Unf','RFn','Fin'],\n    #GarageQual\n    ['None','Po','Fa','TA','Gd','Ex'],\n    #GarageCond\n    ['None','Po','Fa','TA','Gd','Ex'],\n    #PoolQC\n    #['None','Fa','Gd','Ex'],\n    #Fence\n    ['None','MnWw','GdWo','MnPrv','GdPrv'] ]\n   \n\n### ordinal features with specific order.....\nfor i in range(len(orders)):\n\n    ord_en = OrdinalEncoder(categories = {0:orders[i]})\n    df_concat.loc[:,fields_that_need_to_be_ordered[i]] = ord_en.fit_transform(df_concat.loc[:,fields_that_need_to_be_ordered[i]].values.reshape(-1,1))\n","3d89a6d1":"numerical_cols = df_concat._get_numeric_data().columns\ncategorical_cols = list(set(df_concat.columns) - set(numerical_cols))\n\n# all the following features are categorical without any order\n# then we infer that we can use get_dummies\ncategorical_cols","b26c220f":"df_concat=pd.get_dummies(df_concat.drop(columns = ['which', 'SalePrice']))\ndf_concat.head()","78294db2":"import numpy as np\nfrom sklearn.impute import KNNImputer\n\nimputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\nimputed_df = pd.DataFrame(imputer.fit_transform(df_concat))\nimputed_df.columns = df_concat.columns\nimputed_df.name = 'imputed'\ndetect_nulls(imputed_df)","d3aafbfc":"from sklearn.experimental import enable_iterative_imputer  \n\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.linear_model import BayesianRidge\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\n\nestimators = [\n    BayesianRidge(),\n    DecisionTreeRegressor(max_features='sqrt', random_state=0),\n    ExtraTreesRegressor(n_estimators=10, random_state=0),\n    KNeighborsRegressor(n_neighbors=15)\n]\n\nimputer = IterativeImputer(random_state=0, estimator=estimators[1])\nimputer.fit(df_concat)\nimputed_df = pd.DataFrame(imputer.transform(df_concat))\nimputed_df.columns = df_concat.columns\nimputed_df.name = 'imputed'\ndetect_nulls(imputed_df)","781b4501":"\nNow that we understood how important it is to take care of the data and look for signs of trouble, even in the null values, let's import the dataset, give them name to distinguish them later and make a concatinated dataframe that can show us what's in each very easily.\n\nYou can learn about the code and packages more in the second video. Ask me questions in here or in the comments on YouTube.","53f05907":"## Now the data is literary ready to use for any Scikit learn model!!!\n\nOfcourse you can be content with what you have now, but if you want to see a few more tools and tricks, stick around for this section ;) For filling null values we also have the following packages. The knn imputer that looks at all the examples and tries to impute the closest value to the one that is missing. Also we have enable_iterative_imputer which can use different algorithms to predict the missing ones. For each you can use the code bellow to impute whatever column in our dataset after we've converted all features into digits. Just make sure to run the first cells to re-import the dataset and not the already imputed dataset.","5d856c1f":"**Missing Completely at Random(MCAR)**: This means that there is no relationship between the missing values themselves and any other feature or observation in the dataset. These are just missing randomly and without any pattern. Like when a questionnaire is lost or some parts of a survey paper are soaked and you cannot read what it's saying. Therefore we can simply ignore them.\n\n**Missing at Random(MAR)**: Means there is a pattern and relationship between the missing values and the observed values, like when the survey is about mental and physical health, male participants are less likely to know their waist circumference or boob size. We can again ignore these missing values and drop the feature or examples from our dataset. In this case we say that missing values are only related to the observed features(gender).\n\n**Missing Not at Random(MNAR)**: Well, this is a missing value that cannot and should not be ignored. We have to model and see when were the values missing and what was the reason for it. Like in the previous scenario, men might not answer questions about depression because of their depression. In this case we say that the missing values are related to themselves(depression) as well as to the observed features(gender).","0ec4147f":"## How to handle them? How to know what to do with them?!\n\n When handling these null values we have to divide them into three categories. [Reference](https:\/\/cjasn.asnjournals.org\/content\/9\/7\/1328.abstract)\n \n![Untitled.png](attachment:Untitled.png)","194487a4":"**Manually handling each column**\n\nThere are certain columns that can be easily filled without much hard work. \"Functional\" and \"KitchenQual\" are two features that have only 2 and 1 missing values that can be filled with the assumption of them being like most other typical houses. This assumption won't hurt our model when our missing values are very much smaller than the observed values.","ae766d6f":"Now we'll plot the values to have a better understanding. Just make sure to click on the legends on the top right to look at the values related to each dataset respectively.","3b307162":"As for the PoolQual, we can see that there is no very obvious trend going on here. In fact even if we just put all these houses in one category (having pool) and all others in another(No pool) we won't get anything special either. Additionally, because there is only very small evidence that having pool can have an impact on the price and testing set has even less houses that have pool, this field will be ignored specially when we use algorithms like Lasso. So our best bet is to just simply remove this feature and go onto other fields.\n\nIn other fields such as Alley or Miscellaneous features, I had the intention of removing, though due to the number of examples that had these features available, I thought I'll leave them be and see what will happen. After constructing the model you can decide what to do by experimenting more.\n## 2. Filling missing values\n\nI remember when I started working on this dataset on Kaggle, I read about a couple of different ways that you could still have the feature by filling them in with some assumed values. Some of these ways are na\u00efve, some are complicated and some take time and are manually done.  In this section we'll look at all and see which ones work best in which cases.\n\n### Filling with 0 and None \ud83d\udc4c\n \nwhen we look at fields like \"GarageType\", \"GarageQual\" or \"BsmtQual\", we see that these are fields related to a property in a house like Garage or Basement. Well, what would happen if there was no garage or basement? the fields would be value of null as the data description tells us. (NA = No Garage) So for not having null, we simply fill them all with None.\n\nThis is also True for the \"GarageArea\", \"GarageCars\", \"BsmtFinSF1\" as we fill them with 0.","74fee1f8":"This is the first notebook of a series of kernels I'm going to publish on this dataset in which I want to learn and teach by exploring Machine learning to it's fullest by reading books like [100-page ml book](http:\/\/themlbook.com\/) and [Hands-on Machine Learning with Scikit learn and Tensorflow](https:\/\/www.amazon.com\/Hands-Machine-Learning-Scikit-Learn-TensorFlow\/dp\/1492032646) and also papers. If you like to follow along or see my content I publish, subscribe to my YouTube channel or follow me on Kaggle. Let's start \ud83e\udd13 \ud83d\ude0e\n\n## Question: Why do we care about missing values?\n\nSo many of the most famous algorithms(e.g. all the Scikit learn algorithms) can't operate when there are missing values in the dataset. Also We cannot get accurate results when an important feature is missing.\n\nThe least that we can do is to either get rid of these features or fill them with something that can be similar to the value that had to be present or something that is a typical value for that field, so we can both have that feature and let the algorithm use other features to distinguish this example from others and make a prediction based on whatever else we have. So let's see what we can do for them with stories...","806da428":"In this Kernel I will focus on null values but just to have an idea of the dataset, you can play with the plots bellow to get a feeling for the data. You can add fields that you want to analyze to the list in the second plot and plot their distribution in the testing and training set.","b0335766":"# Let's Detect what is missing \ud83e\udd28","bf32efeb":"# What do they represent and What can we do?\n\n## 1. simplest approach: Dropping either the examples or columns with null value \ud83c\udfcc\ud83c\udffb\u200d\n\nAs we can see in the table, 34 columns have null values.\nif we drop all these null values, the model that we make at the end will be a robust. Although, when the percentage of missing values is high, our model will preform poorly due to the information loss. In our case there are Some features like PoolQC (Pool Quality) and MiscFeatures(Miscellaneous features) that are mostly empty and as we can see in the data description, null in these features simply mean there is no pool or extra feature in the property. So we should either set these null values to zero  and see whether their presence makes any difference, or simply remove them and get more memory and speed for our algorithms.\n\n**But to drop or not to drop?**\ud83e\udd26\u200d\n\nThere are certain questions that can help us with this definition. In this competition, we want to predict the price at which a house is sold. So one way to see if having this feature makes any difference, is by plotting houses with pool and without pool against their sales price. Does having pool mean any specific thing here? What about miscellaneous features and Alley?","27ee16e0":"<div class=\"alert alert-info\" role=\"alert\">\nRead more:\n<h2>Why do men kill themselves more than women?<\/h2>\n<a href=\"https:\/\/en.wikipedia.org\/wiki\/Gender_differences_in_suicide\" style=\"color:Black;\">Suicide gender paradox<\/a>. Male suicide rates are explained in terms of traditional gender roles. Male gender roles tend to emphasize greater levels of strength, independence, risk-taking behavior, economic status, and individualism. Reinforcement of this gender role often prevents males from seeking help for suicidal feelings and depression.<\/div>","2d8aee04":"So now we want to see how we can use a na\u00efve approach and turn it into something meaningful. Let's look at two features \"LotFrontage\" and \"MSZoning\":\n\n* ***MSZoning***: Identifies the general zoning classification of the sale. (Its values are; Agriculture, Commercial, Industrial, etc.)\n* ***LotFrontage***: Linear feet of street connected to property \n* ***Electrical***: Electrical system(Standard Circuit, Mixed, etc.)\n\nThe irst two features are aspects of a property that are not only related to one specific example, but instead a feature common among a neighborhood. So hopefully we can decide on its value based on how other houses in that neighborhood are. Are they commercial or agriculture? How big are this area's streets? \n\nAlso if we group all the examples by their neighborhood and Electrical Systems, we see that most houses in one area are using the same Electrical system, so we can use this approach for this feature too. **But we will tackle these values with other methods so you'll have a wide range of tools available at your hand and see which works best.**\n","e3f8b273":"The only thing that is left is Utilities. This column tells us what utilities are available and its unique values and their fequency are shown bellow. As we can see there is no more than one different value in this field which shows how useless this field is for distinguishing between examples. We don't want unpredictive features in our dataset so we simply drop the whole column.","e50beb80":"## 3. Use a model that is compatible with missing values\n\nThere are some algorithms that are robust to missing values, meaning that when you run a model, they will ignore them or not take them into account. For example KNN(the k nearest neighborhood algorithm) can ignore the missing feature when it's calculating the distance between examples. The Na\u00efve Bayes algorithm can ignore the missing examples when calculating a probability for a special class.\n\nBut if you're using scikit learn you don't have this option of not filling or dropping null values because the algorithms can't simply handle null values.\n\n## 4. Categorical and numeric values and Using an algorithm for missing values\n\n![mis2.png](attachment:mis2.png)\nIn order to construct a model we have to do a little bit of preprocessing to help the model understand our features. In general, we have two types of data; numerical and categorical.\n\n- numeric values are like the size of a house, how many stories it has and so on.\n- categorical values that split into two groups:\nordinal variables (categorical data with a certain order, like the quality of household material that starts from \"Poor\" and ends with \"Excellent\")\nnominal variables (categorical data with no specific order, like different types of roof material that can be \"Metal\", \"Roll\", \"Gravel\" and etc.)\n\nAs you may expect we don't change anything about numeric ones as they can be easily understood by our mode. But we have some tools like One-Hot-encoder or Label-Encoder in Scikit-learn that help us change these variables into a numerical format that's readable for our models.\n\nSome categorical data like CentralAir(yes and no values) and YrSold(starting from 1872 to 2010) are already in numbers so the label encoder can detect the order and give them digits starting from 0(e.g. the first year appearing in the dataset).\nBut some other features, like the KitchQual, don't have such advantage because they're in letters(the quality can be Poor, Fair, Typical, Good and Excellent). For that we can simply type the order in a list and then use an Ordinal Encoder to encode each variable based on them.\nAlso there are some categorical variables that don't need any change as they are in order and in a satisfying rage. like the OverallQual(overal quality) that is between 1 and 10 and the MoSold(month sold) that is between 1 and 12.\n","835a8bb1":"\n### Forward fill and backward fill \ud83d\udc48 \ud83d\udc49\n\npersonally, I  rarely use this method as it is very na\u00efve and it only works if you have an apparent pattern in your data. For example, you can use this approach if your data can be sorted by date and you  want to fill the null values in the date column with the previous or the next observation (which is called Last Observation Carried Forward -LOCF). But in our data here, I prefer not to use this method.\n\n### filling with Mean and Median \ud83d\ude4c\n\nFilling with mean(for numeric values) or median(for categorical) is one of the methods that is very straight forward and simple in which we only give all the missing values, something normal and typical to prevent dropping them and also letting other features decide how we should distinguish these examples from the others. So we try to predict the label by relying mostly on the other features. We can use tools like the simple imputer from Scikit learn library to do the following.\n\nBut before using this method, here is one important question:\n\n<div class=\"alert alert-success\" role=\"alert\">\n<center><h2>Can we use the testing set to find out how we should impute the null values?<\/h2><\/center>\n    \n    \nIf you look at the discussions panel in this competition, you'll find that so many people have argued that you should only use the training examples to, for example, find the mean of a column's values and save this value to use it on the testing set too. As they say, using both the training and testing datasets to find the mean is called data leakage. I was myself a little confused and thought that we should be using both datasets to have a better mean value. So I did a little research to see what is the best approach.\n\nIn the <a href=\"https:\/\/www.amazon.com\/Hands-Machine-Learning-Scikit-Learn-TensorFlow\/dp\/1491962291\" style=\"color:Blue;\">Hands-On Machine Learning with Scikit-Learn and TensorFlow book<\/a>, the author suggests the following:\n    \n    \n<blockquote style=\"color:Black;\">[...] you should compute the median value on the training set, and use it to fill the missing values in the training set, but also don't forget to save the median value that you have computed. You will need it later to replace missing values in the test set when you want to evaluate your system, and also once the system goes live to replace missing values in new data. <\/blockquote>\n    \nSo he said that we shouldn't use both. But in fact, in a Kaggle competition, where you have both your training and testing set ready and fixed and no new examples are come from new experiments or records, we do not need to take this into account. We can calculate the mean or median globally without being concerned about new examples. You can read about this <a href=\"https:\/\/stats.stackexchange.com\/questions\/320085\/when-imputing-missing-values-in-a-test-set-should-the-new-values-come-from-the\" style=\"color:Blue;\"> more on stackoverflow.<\/a><\/div>","461eeb1e":"I hope this kernel gave you the intuition and tools you need for your own dataset and projects. Make sure to upvote if it was hopeful or keep along with these series in which I try to explore machine learning with the house price dataset!"}}