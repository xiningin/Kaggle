{"cell_type":{"03b6a4a3":"code","35635e1b":"code","579f3eeb":"code","ad5ca6ce":"code","0e40052a":"code","5111c6f3":"code","2167cbb2":"code","e501ad72":"code","4a1898d3":"code","04130a21":"code","f5b4deb9":"markdown","5b3457f1":"markdown","53d2d516":"markdown","cd0b10e4":"markdown","a1dcf963":"markdown","864676ba":"markdown"},"source":{"03b6a4a3":"!pip install torchviz\n%matplotlib inline\nimport os\nimport cv2\nimport seaborn \nimport numpy as np\nimport pandas as pd \nimport random\nimport tqdm\nfrom tqdm import notebook\nimport albumentations as A\nfrom albumentations import pytorch\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as functional\nimport torchvision\nfrom torchviz import make_dot","35635e1b":"data_dir = '\/kaggle\/input\/segmentation-full-body-mads-dataset\/segmentation_full_body_mads_dataset_1192_img'\nimages_dir = os.path.join(data_dir, 'images')\nmasks_dir = os.path.join(data_dir, 'masks')\nworking_dir = '\/kaggle\/working'\nweights_path = os.path.join(working_dir, 'person_segmentation_unet4_weights.pth')\nlogs_path = os.path.join(working_dir, 'logs')","579f3eeb":"class Generator(object):\n    def __init__(self, images_dir, masks_dir, batch_size, is_augmentation, is_binary = False, shuffle = True, rescale = 1.00, target_size = (128, 128)):\n        super(Generator, self).__init__()\n        self.images_dir = images_dir\n        self.masks_dir = masks_dir\n        self.is_binary = is_binary\n        self.batch_size = batch_size\n        self.rescale = rescale\n        self.shuffle = shuffle\n        self.is_augmentation = is_augmentation\n        self.target_size = target_size\n        self.image_filenames = [os.path.join(self.images_dir, filename) for filename in os.listdir(self.images_dir)]\n        self.mask_filenames = [os.path.join(self.masks_dir, filename) for filename in os.listdir(self.masks_dir)]\n        self.current_step = 0\n        self.count_images = len(self.image_filenames)\n        self.available_steps = int(self.count_images \/\/ self.batch_size)\n        \n        self.transforms = A.Compose([\n            A.Rotate(65), \n            A.OneOf([\n                A.RGBShift(), A.HueSaturationValue()\n            ]),\n            A.OneOf([\n                A.CLAHE(), A.RandomBrightnessContrast(), A.RandomGamma()\n            ]), \n        ])\n    \n    def augmentate(self, batch):\n        batch = batch.astype(np.uint8)\n        batch = [self.transforms(image = image, mask = mask) for (image, mask) in batch]\n        batch = np.array([(transformed['image'], transformed['mask']) for transformed in batch], dtype = np.float32)\n        return batch\n            \n    def generate_batch(self):\n        start = self.current_step * self.batch_size\n        stop = (self.current_step + 1) * self.batch_size\n        image_filenames_batch = self.image_filenames[start:stop]\n        mask_filenames_batch = self.mask_filenames[start:stop]\n        \n        # batch of original images from directory\n        images_batch = [cv2.imread(filename) for filename in image_filenames_batch]\n        masks_batch = [cv2.imread(filename) for filename in mask_filenames_batch]\n        \n        # change channels order to rgb\n        images_batch = np.array([cv2.cvtColor(image, cv2.COLOR_BGR2RGB) for image in images_batch])\n        masks_batch = np.array([cv2.cvtColor(mask, cv2.COLOR_BGR2RGB) for mask in masks_batch])\n        \n        # resize images \n        images_batch = np.array([(cv2.resize(images_batch[index], self.target_size), cv2.resize(masks_batch[index], self.target_size)) for index in range(len(images_batch))])\n        \n        # augmentation\n        if self.is_augmentation:\n            images_batch = self.augmentate(images_batch)\n        \n         # set 'channel_first' order\n        images_batch = np.array([(np.moveaxis(image, -1, 0), np.moveaxis(mask, -1, 0)) for (image, mask) in images_batch])\n        \n        # rescaling \n        images_batch \/= self.rescale\n        \n        #resampling \n        images, masks = np.moveaxis(images_batch, 1, 0)\n        images, masks = np.array(images), np.array(masks)\n       \n        # set binary format for image if required\n        if self.is_binary:\n            masks = np.array([np.expand_dims(mask[0], axis = 0) for mask in masks])\n        \n        images, masks = torch.Tensor(images), torch.Tensor(masks)\n        \n        return images, masks\n    \n    def __next__(self):\n        if self.current_step > self.available_steps:\n            self.current_step = 0\n        images, masks = self.generate_batch()\n        self.current_step += 1\n        return images, masks\n    \n    def __len__(self):\n        return self.available_steps","ad5ca6ce":"def show_examples(num_cols):\n    stacks = []\n    dataloader = Generator(images_dir = images_dir, masks_dir = masks_dir, is_binary = False, batch_size = 8, is_augmentation = True, rescale = 255.0)\n    for iteration in range(num_cols):\n        images, masks = next(dataloader)\n        images, masks = images.numpy(), masks.numpy()\n        images, masks = np.concatenate(np.moveaxis(images, 1, -1)), np.concatenate(np.moveaxis(masks, 1, -1))\n        embedded = images - masks\n        stack = np.hstack([images, masks, embedded])\n        stacks.append(stack)\n    result = np.hstack(stacks)\n    plt.figure(figsize = (30, 30))\n    plt.axis('off')\n    plt.imshow(result)\nshow_examples(2)","0e40052a":"class DoubleConv(nn.Module):\n    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n\n    def __init__(self, in_channels, out_channels, mid_channels=None):\n        super().__init__()\n        if not mid_channels:\n            mid_channels = out_channels\n        self.double_conv = nn.Sequential(\n            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(mid_channels),\n            nn.LeakyReLU(0.1),\n            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.LeakyReLU(0.1)\n        )\n\n    def forward(self, x):\n        return self.double_conv(x)\n\n\nclass Down(nn.Module):\n    \"\"\"Downscaling with maxpool then double conv\"\"\"\n\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.maxpool_conv = nn.Sequential(\n            nn.MaxPool2d(2),\n            DoubleConv(in_channels, out_channels)\n        )\n\n    def forward(self, x):\n        return self.maxpool_conv(x)\n\n\nclass Up(nn.Module):\n    \"\"\"Upscaling then double conv\"\"\"\n\n    def __init__(self, in_channels, out_channels, bilinear=True):\n        super().__init__()\n\n        # if bilinear, use the normal convolutions to reduce the number of channels\n        if bilinear:\n            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n            self.conv = DoubleConv(in_channels, out_channels, in_channels \/\/ 2)\n        else:\n            self.up = nn.ConvTranspose2d(in_channels, in_channels \/\/ 2, kernel_size=2, stride=2)\n            self.conv = DoubleConv(in_channels, out_channels)\n\n    def forward(self, x1, x2):\n        x1 = self.up(x1)\n        # input is CHW\n        diffY = x2.size()[2] - x1.size()[2]\n        diffX = x2.size()[3] - x1.size()[3]\n\n        x1 = functional.pad(x1, [diffX \/\/ 2, diffX - diffX \/\/ 2,\n                        diffY \/\/ 2, diffY - diffY \/\/ 2])\n        x = torch.cat([x2, x1], dim=1)\n        return self.conv(x)\n\n\nclass OutConv(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(OutConv, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n\n    def forward(self, x):\n        return self.conv(x)\n    \nclass UNet(nn.Module):\n    def __init__(self, n_channels, n_classes, bilinear=True):\n        super(UNet, self).__init__()\n        self.n_channels = n_channels\n        self.n_classes = n_classes\n        self.bilinear = bilinear\n\n        self.inc = DoubleConv(n_channels, 2**5)\n        self.down1 = Down(2**5, 2**6)\n        self.down2 = Down(2**6, 2**7)\n        self.down3 = Down(2**7, 2**8)\n        factor = 2 if bilinear else 1\n        self.down4 = Down(2**8, 2**9 \/\/ factor)\n        self.up1 = Up(2**9, 2**8 \/\/ factor, bilinear)\n        self.up2 = Up(2**8, 2**7 \/\/ factor, bilinear)\n        self.up3 = Up(2**7, 2**6 \/\/ factor, bilinear)\n        self.up4 = Up(2**6, 2**5, bilinear)\n        self.outc = OutConv(2**5, n_classes)\n\n    def forward(self, x):\n        x1 = self.inc(x)\n        x2 = self.down1(x1)\n        x3 = self.down2(x2)\n        x4 = self.down3(x3)\n        x5 = self.down4(x4)\n        x = self.up1(x5, x4)\n        x = self.up2(x, x3)\n        x = self.up3(x, x2)\n        x = self.up4(x, x1)\n        logits = self.outc(x)\n        return logits    ","5111c6f3":"unet = UNet(n_channels=3, n_classes=1, bilinear=True)\nx = torch.zeros(8, 3, 128, 128, dtype=torch.float, requires_grad=False)\nout = unet(x)\nmake_dot(out, params=dict(list(unet.named_parameters())))","2167cbb2":"optimizer = torch.optim.Adam(params = unet.parameters(), \n                             lr=1e-4, \n                             betas=(0.9, 0.999), \n                             eps=1e-08, \n                             weight_decay=0, \n                             amsgrad=False)\n\ncriterion = torch.nn.BCEWithLogitsLoss()\n\nhistory = dict(train_loss = [], \n               train_dice_coeff = [], \n#                test_loss = [], \n#                test_dice_coeff = []\n              )\n\ndef dice_coeff(pred, target):\n    pred = (pred > 0).float()\n    return 2. * (pred*target).sum() \/ (pred+target).sum()\n            \ndef training(model, epochs, batch_size):\n \n    train_generator = Generator(images_dir = images_dir, masks_dir = masks_dir, is_binary = True, batch_size = batch_size, is_augmentation = True, rescale = 255.0)\n#     test_generator = Generator(images_dir = val_images_dir, batch_size = batch_size, is_augmentation = False, rescale = 255.0)\n    \n    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n    model.to(device)\n    \n    main_pbar = tqdm.notebook.tqdm(range(epochs))\n    main_pbar.set_description('common progress ')\n    \n    for epoch in main_pbar:\n        running_params = dict(train_loss = [], \n                               train_dice_coeff = [], \n#                                test_loss = [], \n#                                test_dice_coeff = []\n                             )\n        train_pbar = tqdm.notebook.tqdm(range(len(train_generator)))\n        \n        for step in train_pbar:\n            \n            # obtain batches of data \n            train_images, train_masks = next(train_generator)\n            train_images, train_masks = train_images.to(device), train_masks.to(device)\n            \n            # zero optimizer gradients\n            optimizer.zero_grad()\n            \n            # forward + backward + optimize\n            train_predictions = model(train_images)\n            # estimate loss and backward\n            train_loss = criterion(train_predictions, train_masks)\n            train_loss.backward()\n            \n            # estimate dice coeff\n            train_dice_coeff = dice_coeff(pred = train_predictions, target = train_masks)\n            \n            # optimize\n            optimizer.step()\n        \n        \n       \n#             with torch.no_grad():\n#                 # get batches of data\n#                 test_images, test_masks = next(test_generator)\n#                 test_images, test_masks = test_images.to(device), test_masks.to(device)\n#                 # calculate outputs by running images through the network\n#                 test_predictions = model(test_images)\n#                 # calculate loss without backpropagation\n#                 test_loss = criterion(test_predictions, test_masks)\n#                 # calculate dice coeff\n#                 test_dice_coeff = dice_coeff(pred = test_predictions, target = test_masks)\n                \n            \n            # set current values in dictionary to store\n            current_metrics = dict(train_loss = [train_loss.item(), ], \n                                   train_dice_coeff = [train_dice_coeff.item(), ], \n#                                    test_loss = [test_loss.item(),], \n#                                    test_dice_coeff = [test_dice_coeff.item(),]\n                                  )\n            \n            # update dictionary with current epochs\n            running_params.update(current_metrics)\n            # estimate mean values for each metrics\n            mean_metrics = dict(zip(running_params.keys(), [(sum(tensor) \/ (step + 1)) for tensor in running_params.values()]))\n            # update progress bar\n            train_pbar.set_postfix(mean_metrics)\n            # clear gpu memory\n            torch.cuda.empty_cache()\n        \n        history.update(running_params)\n        best_loss = max(history['train_loss'])\n        best_loss_index = history['train_loss'].index(best_loss)\n        current_loss_index = history['train_loss'].index(train_loss.item())\n        if abs(current_loss_index - best_loss_index) >= 5:\n            for param_group in optim.param_groups:\n                if param_group['lr'] * 0.1 > 1e-6:\n                    print('reduce learning rate to', {param_group['lr'] * 0.1})\n                    param_group['lr'] *= 0.1\n\ntraining(model = unet, epochs = 20, batch_size = 32)\ntorch.save(unet.state_dict(), weights_path)","e501ad72":"model = UNet(n_channels=3, n_classes=1, bilinear=True)\nmodel.load_state_dict(torch.load(weights_path))","4a1898d3":"model_parameters = filter(lambda p: p.requires_grad, model.parameters())\nparams = sum([np.prod(p.size()) for p in model_parameters])\nprint(f'parameters : {params}')","04130a21":"def show_final_results(num_cols, model, is_binary = True):\n    generator = Generator(images_dir = images_dir, \n                          masks_dir = masks_dir, \n                          is_binary = True,\n                           batch_size = 8, \n                           is_augmentation = True, \n                           rescale = 255.0)\n    result = []\n    \n    \n    for iteration in range(num_cols):\n        images, masks = next(generator)\n        prediction = model(images)\n        prediction = prediction.cpu().detach().numpy()\n        prediction = np.moveaxis(prediction, 1, -1)\n        prediction[prediction < 0] = 0.0\n        prediction[prediction > 0] = 1.0\n        \n        # processing\n        images, masks = images.cpu().detach(), masks.cpu().detach()\n        masks = np.moveaxis(masks.numpy(), 1, -1)\n        if is_binary:\n            masks = np.array([cv2.merge([mask[:, :, 0],] * 3) for mask in masks])\n            prediction = np.array([cv2.merge([pred[:, :, 0],] * 3) for pred in prediction])\n        \n        images = np.moveaxis(images.numpy(), 1, -1)\n        prediction = np.concatenate(prediction)\n        images = np.concatenate(images)\n        masks = np.concatenate(masks)\n        no_person = images - masks\n        no_background = (prediction) * images\n        outputs = np.hstack([images, masks, prediction, no_person, no_background])\n        result.append(outputs)\n        \n    result = np.hstack(result)\n    plt.figure(figsize = (30, 30))\n    plt.axis('off')\n    plt.imshow(result)\n\nshow_final_results(num_cols = 1, model = model)","f5b4deb9":"# Custom Generator Compatible with Torch","5b3457f1":"# Define path variables","53d2d516":"# Network","cd0b10e4":"# Training","a1dcf963":"# import dependencies","864676ba":"![image.png](attachment:d2dae276-ede7-4ecb-8620-1176f3b4f683.png)"}}