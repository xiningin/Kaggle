{"cell_type":{"9012e1ef":"code","7c73eafc":"code","1c8bba59":"code","af14b2d2":"code","2112af54":"code","55c0372c":"code","655fe430":"code","5f89586b":"code","f4355323":"code","eb7dadc0":"code","52788fa5":"code","8f5101bf":"code","648f3d30":"code","40b6cd3d":"code","b0b71891":"code","7f4e605a":"code","16bee5fd":"code","28577dd8":"code","e646d974":"code","e023919e":"code","149a8f49":"code","3d5e0e0f":"code","db25f4d0":"code","bfa35d32":"code","be63cf2c":"code","b2fbb7f6":"code","fe9951da":"code","d21a20d9":"code","48eedfc0":"code","ce3006fa":"code","cd49a3ec":"code","9aa7180b":"code","2554c2c7":"code","9e02de88":"code","3df01ab9":"code","2bb7117d":"code","98704f2c":"code","080e64b1":"code","1fdbaa0e":"code","c7444df9":"code","7ddf49c0":"code","883d7242":"code","d03d9fe4":"code","82af68ea":"code","127e3f58":"code","c3d4a13d":"code","9a18c18c":"code","607b9c82":"code","46e6967d":"code","602ab15a":"code","2f51b06c":"code","586d76e8":"code","bf4b44d8":"code","5f176da9":"code","5275a679":"code","6ff3fc56":"code","a903fd68":"code","ef1b575b":"code","c999ab87":"code","ba571945":"code","66e26607":"code","db6b2845":"code","c9187671":"code","58375d34":"code","78611ffb":"code","0874d9aa":"code","4498e050":"code","17802270":"code","4723498d":"code","1ece8d98":"code","d7667053":"code","cd8e0baa":"code","09cc3fa6":"code","fac535c8":"code","65353410":"code","da3c662a":"code","e24099c2":"code","fc7d5156":"code","9f4a05a5":"code","b145607b":"code","9395c89b":"code","99d4764a":"code","67b64e4d":"code","57c7e7f0":"code","d62b3950":"code","cfefd6f6":"code","c7ee04dd":"code","fd506d8a":"code","479b76e9":"code","49dfa0c8":"code","30eb7c4b":"code","03a5fbde":"code","aaa5486e":"code","0606a83e":"code","873b3cf1":"code","735f5489":"code","c9f45303":"code","27bf6359":"code","cd866d25":"code","35753962":"code","a3b05b09":"code","88d4836e":"code","a5c181e7":"code","fb1f291a":"code","7715793f":"code","ccbd6f81":"code","558c3925":"code","9b8e9aed":"code","e7453453":"code","ceb60d0e":"code","492aab83":"code","8c2bf2b5":"code","0593a8a5":"code","42ef3497":"code","b824228b":"code","adad8ddd":"code","06ae30ee":"code","11a6c22b":"code","6a74a808":"code","f2b8ea67":"markdown","3a3b289c":"markdown","3653c353":"markdown","719a96a6":"markdown","74aea197":"markdown","dc56e9d6":"markdown","a4682dd2":"markdown","5367dd55":"markdown","0fe5c1ec":"markdown","db3e9c36":"markdown","55ca02c5":"markdown","cf3b8a92":"markdown","e9f81067":"markdown","136f9376":"markdown","fa7c308c":"markdown","b89f464f":"markdown","88b8f510":"markdown","9cb44c6d":"markdown","5dd5b63a":"markdown","9da66709":"markdown","d6cf1141":"markdown","eee0d9a0":"markdown","d21230ed":"markdown","bf0b0947":"markdown","a7e13e98":"markdown","dae6a3dd":"markdown","f192fbd6":"markdown","53c6210b":"markdown","c75099e4":"markdown","9b5ea84f":"markdown","7d8a493f":"markdown","aa2c24bd":"markdown","362b8e98":"markdown","eba8bb8b":"markdown","0b09f1c6":"markdown","137e47e2":"markdown","62cb6b47":"markdown","321080bf":"markdown","aab250b0":"markdown","0bf43e3c":"markdown","5a03755c":"markdown","c8993ce8":"markdown","96ec4afe":"markdown"},"source":{"9012e1ef":"#import libraries\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings","7c73eafc":"#suppress warnings\nwarnings.filterwarnings(\"ignore\")","1c8bba59":"#import data\nheart_data = pd.read_csv('..\/input\/heart-disease-uci\/heart.csv')","af14b2d2":"#look at formatting of entries\nheart_data.head()","2112af54":"#display null values and data types\nheart_data.info()","55c0372c":"#null values for ca\nheart_data[heart_data.ca==4]","655fe430":"#null percentage for ca\nprint('Percentage of ca null: {}%'.format((len(heart_data[heart_data.ca==4])\/len(heart_data.ca))*100))","5f89586b":"#null values for thal\nheart_data[heart_data.thal==0]","f4355323":"#null percentage for thal\nprint('Percentage of thal null: {}%'.format((len(heart_data[heart_data.thal==0])\/len(heart_data.thal))*100))","eb7dadc0":"#numerical features\nnumerical = [\n    'age',\n    'trestbps',\n    'chol',\n    'thalach',\n    'oldpeak',\n]\n\n#categorical features\ncategorical = [\n    'sex',\n    'cp',\n    'fbs',\n    'restecg',\n    'exang',\n    'slope',\n    'ca',\n    'thal',\n    'target'\n]","52788fa5":"#look at distribution of data\nheart_data.describe()","8f5101bf":"#look at number of outliers greater than or equal to 3 std from mean\nheart_data[np.abs(stats.zscore(heart_data)) >= 3]","648f3d30":"#look at number of outliers greater than or equal to 4 std from mean\nheart_data[np.abs(stats.zscore(heart_data)) >= 4]","40b6cd3d":"#an outlier who is a 67 year old female with a cholesterol greater than six std from mean\nheart_data[np.abs(stats.zscore(heart_data)) >= 6]","b0b71891":"#oldpeak outlier visualized\nsns.boxplot(x=heart_data['oldpeak'], palette='Set1')\nplt.xlabel('oldpeak')","7f4e605a":"#cholesterol outlier visualized\nsns.boxplot(x=heart_data['chol'], palette='Set1')\nplt.xlabel('chol')","16bee5fd":"#look at numerical data distribution\nfor i in heart_data[numerical].columns:\n    plt.hist(heart_data[numerical][i], color='steelblue', edgecolor='black')\n    plt.xticks()\n    plt.xlabel(i)\n    plt.ylabel('number of people')\n    plt.show()","28577dd8":"#look at categorical data distribution\nfor i in heart_data[categorical].columns:\n    sns.barplot(edgecolor='black',x=heart_data[categorical][i].value_counts().index,y=heart_data[categorical][i].value_counts(),palette='Set1')\n    plt.xlabel(i)\n    plt.ylabel('number of people')\n    plt.show()","e646d974":"#heat map to see numerical correlations, pearson measures monotonic relationship (numerical or ordinal categorical)\nplt.figure(figsize=(16, 6))\nsns.heatmap(heart_data[numerical].corr(method='pearson'), vmin=-1, vmax=1, annot=True,cmap='coolwarm')\nplt.title('Pearson Correlation Heatmap for Numerical Variables', fontdict={'fontsize':12}, pad=12);","e023919e":"#look at how target is distributed among variables\nsns.pairplot(heart_data,hue='target',palette='Set1')\nplt.legend()\nplt.show()","149a8f49":"#thalach vs age\nsns.lmplot(x='age', y='thalach', data=heart_data, palette='Set1')\n\n#settings to display all markers\nxticks, xticklabels = plt.xticks()\nxmin = (3*xticks[0] - xticks[1])\/2.\nxmax = (3*xticks[-1] - xticks[-2])\/2.\nplt.xlim(xmin, xmax)\nplt.xticks(xticks)\n\nplt.show()","3d5e0e0f":"#thalach vs age with target hue\nsns.lmplot(x='age', y='thalach', hue='target', data=heart_data,palette='Set1')\n\n#settings to display all markers\nxticks, xticklabels = plt.xticks()\nxmin = (8*xticks[0] - xticks[1])\/2.\nxmax = (3*xticks[-1] - xticks[-2])\/2.\nplt.xlim(xmin, xmax)\nplt.xticks(xticks)\n\nplt.show()","db25f4d0":"#age vs target\nsns.violinplot(x='target', y='age', data=heart_data, palette='Set1')\nplt.show()","bfa35d32":"#cp distribution with exang hue\nsns.histplot(discrete=True,x=\"cp\", hue=\"exang\", data=heart_data, stat=\"count\", multiple=\"stack\",palette='Set1')\n\nplt.ylabel('number of people')\nplt.show()","be63cf2c":"#cp distribution with target hue\nsns.histplot(discrete=True, x=\"cp\", hue=\"target\", data=heart_data, stat=\"count\", multiple=\"stack\",palette='Set1')\n\nplt.ylabel('number of people')\nplt.xticks(ticks=[0,1,2,3])\nplt.show()","b2fbb7f6":"#thalach vs exang\nsns.violinplot(x='exang', y='thalach', data=heart_data, palette='Set1')\nplt.show()","fe9951da":"#thalach vs exang with target hue\nsns.violinplot(x='exang', y='thalach', data=heart_data, palette='Set1', hue='target')\nplt.show()","d21a20d9":"#thalach vs exang with target hue\nsns.swarmplot(y=heart_data['thalach'],\n              x=heart_data['exang'], hue=heart_data['target'],palette='Set1')\n\nplt.show()","48eedfc0":"#thalach vs target\nsns.violinplot(x='target', y='thalach', data=heart_data, palette='Set1')\nplt.show()","ce3006fa":"#exang distribution with target hue\nsns.histplot(discrete=True, x=\"exang\", hue=\"target\", data=heart_data, stat=\"count\", multiple=\"stack\",palette='Set1')\nplt.ylabel('number of people')\nplt.xticks(ticks=[0,1])\nplt.show()","cd49a3ec":"#oldpeak vs slope\nsns.violinplot(x='slope', y='oldpeak', data=heart_data, palette='Set1')\nplt.show","9aa7180b":"#oldpeak vs target\nsns.violinplot(x='target', y='oldpeak', data=heart_data, palette='Set1')\nplt.show()","2554c2c7":"#distribution of slope with target hue\nsns.histplot(discrete=True, x=\"slope\", hue=\"target\", data=heart_data, stat=\"count\", multiple=\"stack\",palette='Set1')\n\nplt.ylabel('number of people')\nplt.xticks(ticks=[0,1,2])\nplt.show()","9e02de88":"#distribution of ca with target hue\nsns.histplot(discrete=True, x=\"ca\", hue=\"target\", data=heart_data, stat=\"count\", multiple=\"stack\",palette='Set1')\n\nplt.ylabel('number of people')\nplt.xticks(ticks=[0,1,2,3,4])\nplt.show()","3df01ab9":"#distribution of thal with target hue\nsns.histplot(discrete=True, x=\"thal\", hue=\"target\", data=heart_data, stat=\"count\", multiple=\"stack\",palette='Set1')\n\nplt.ylabel('number of people')\nplt.xticks(ticks=[0,1,2,3])\nplt.show()","2bb7117d":"#creating arrays that meet critera for risk factors\nage_sex_risk = heart_data.loc[(heart_data.sex == 0) & (heart_data.age >= 50) |\n                                   (heart_data.sex == 1) & (heart_data.age >= 45) ]\n\nhigh_blood_pressure_risk = heart_data.loc[heart_data.trestbps >= 130]\n\nhigh_cholesterol_risk = heart_data.loc[heart_data.chol >= 240]\n\ndiabetes_risk = heart_data.loc[heart_data.fbs == 1]","98704f2c":"#creating a new column called 'risk factors' which counts the number of risk factors each patient has\nrisk_factors_indices = np.concatenate((age_sex_risk.index,\n                                       high_blood_pressure_risk.index,\n                                       high_cholesterol_risk.index,\n                                       diabetes_risk.index))\n\nrisk_factor_counts = np.bincount(risk_factors_indices)\n\nrisk_factors = pd.DataFrame(risk_factor_counts)\n\nrisk_factors['risk factors']=risk_factors\n\nrisk_factors['target'] = heart_data['target'].copy()","080e64b1":"#distribution of risk factors with target hue\nsns.histplot(discrete=True, x=\"risk factors\", hue=\"target\", data=risk_factors, stat=\"count\", multiple=\"stack\",palette='Set1')\n\nplt.ylabel('number of people')\nplt.xticks(ticks=[0,1,2,3,4])\nplt.show()","1fdbaa0e":"#remove target variable from categorical array\ncategorical.remove('target')\n\n#change dtype of categorical features to object\nheart_data[categorical]=heart_data[categorical].astype('object')\n\n#copy of variables and target\nX = heart_data.copy()\ny = X.pop('target')","c7444df9":"X.info()","7ddf49c0":"X_mi = X.copy()","883d7242":"#label encoding for categorical variables\nfor colname in X_mi.select_dtypes(\"object\"):\n    X_mi[colname], _ = X_mi[colname].factorize()\n\n#all discrete features have int dtypes\ndiscrete_features = X_mi.dtypes == int","d03d9fe4":"discrete_features","82af68ea":"#some continuous variables also have int dtypes\ndiscrete_features[numerical] = False","127e3f58":"#use classification since the target variable is discrete\nfrom sklearn.feature_selection import mutual_info_classif\n\n#define a function to produce mutual information scores\ndef make_mi_scores(X_mi, y, discrete_features):\n    mi_scores = mutual_info_classif(X_mi, y, discrete_features=discrete_features)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X_mi.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores\n\n#compute mutual information scores\nmi_scores = make_mi_scores(X_mi, y, discrete_features)\nmi_scores","c3d4a13d":"#define a function to plot mutual information scores\ndef plot_mi_scores(scores):\n    scores = scores.sort_values(ascending=True)\n    width = np.arange(len(scores))\n    ticks = list(scores.index)\n    plt.barh(width, scores, color='steelblue', edgecolor='black')\n    plt.yticks(width, ticks)\n    plt.title(\"Mutual Information Scores\")\n\n#plot the scores\nplt.figure(dpi=100, figsize=(8, 5))\nplot_mi_scores(mi_scores)","9a18c18c":"#plot selling_price against car_name\nsns.violinplot(y=X_mi.thal, x=y, palette='Set1');","607b9c82":"#import libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler","46e6967d":"#get feature names\nX = pd.concat([X[numerical],pd.get_dummies(X[categorical], drop_first=True)],axis=1)\nfeature_names = X.columns\n\n# train\/test split with stratify making sure classes are evenlly represented across splits\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, train_size=0.75, random_state=1)\n\n#define scaler\nscaler=MinMaxScaler()\n\n#apply preprocessing to split data with scaler\nX_train[numerical] = scaler.fit_transform(X_train[numerical])\nX_test[numerical] = scaler.transform(X_test[numerical])","602ab15a":"#import ml algorithms\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import tree\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom numpy import mean, std\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier","2f51b06c":"#naive Bayes with five-fold cross validation\ngnb = GaussianNB()\ncv = cross_val_score(gnb,X_train,y_train,cv=5)\nprint(mean(cv), '+\/-', std(cv))","586d76e8":"#logistic regression with five-fold cross validation\nlr = LogisticRegression(max_iter = 2000)\ncv = cross_val_score(lr,X_train,y_train,cv=5)\nprint(mean(cv), '+\/-', std(cv))","bf4b44d8":"#decession tree with five-fold cross validation\ndt = tree.DecisionTreeClassifier(random_state = 1)\ncv = cross_val_score(dt,X_train,y_train,cv=5)\nprint(mean(cv), '+\/-', std(cv))","5f176da9":"#k-nearest neighbors classifier with five-fold cross validation\nknn = KNeighborsClassifier()\ncv = cross_val_score(knn,X_train,y_train,cv=5)\nprint(mean(cv), '+\/-', std(cv))","5275a679":"#random forest classifier with five-fold cross validation\nrf = RandomForestClassifier(random_state = 1)\ncv = cross_val_score(rf,X_train,y_train,cv=5)\nprint(mean(cv), '+\/-', std(cv))","6ff3fc56":"#support vector classifier with five-fold cross validation\nsvc = SVC(probability = True)\ncv = cross_val_score(svc,X_train,y_train,cv=5)\nprint(mean(cv), '+\/-', std(cv))","a903fd68":"#xgboost classifier with five-fold cross validation\nxgb = XGBClassifier(use_label_encoder=False, eval_metric='error', random_state =1)\ncv = cross_val_score(xgb,X_train,y_train,cv=5)\nprint(mean(cv), '+\/-', std(cv))","ef1b575b":"#ml algorithm tuner\nfrom sklearn.model_selection import GridSearchCV \nfrom sklearn.model_selection import RandomizedSearchCV \n\n#performance reporting function\ndef clf_performance(classifier, model_name):\n    print(model_name)\n    print('Best Score: {} +\/- {}'.format(str(classifier.best_score_),str(classifier.cv_results_['std_test_score'][classifier.best_index_])))\n    print('Best Parameters: ' + str(classifier.best_params_))","c999ab87":"#naive Bayes performance tuner\ngnb = GaussianNB()\nparam_grid = {\n              'var_smoothing': np.logspace(0,-10, num=100)\n             }\nclf_lr = GridSearchCV(gnb, param_grid = param_grid, cv = 5, n_jobs = -1)\nbest_clf_gnb = clf_lr.fit(X_train,y_train)\nclf_performance(best_clf_gnb,'Naive Bayes')","ba571945":"#logistic regression performance tuner\nlr = LogisticRegression()\nparam_grid = {'max_iter' : [15000],\n              'C' : np.arange(.5,1.5,.1)\n             }\nclf_lr = GridSearchCV(lr, param_grid = param_grid, cv = 5, n_jobs = -1)\nbest_clf_lr = clf_lr.fit(X_train,y_train)\nclf_performance(best_clf_lr,'Logistic Regression')","66e26607":"#decision tree performance tuner\ndt = tree.DecisionTreeClassifier(random_state = 1)\nparam_grid = {\n             'criterion':['gini','entropy'],\n             'max_depth': np.arange(1, 15)\n             }\nclf_dt = GridSearchCV(dt, param_grid = param_grid, cv = 5, n_jobs = -1)\nbest_clf_dt = clf_dt.fit(X_train,y_train)\nclf_performance(best_clf_dt,'Decision Tree')","db6b2845":"#k-nearest neighbors classifier performance tuner\nknn = KNeighborsClassifier()\nparam_grid = {\n              'n_neighbors' : np.arange(15,20,1),\n              'weights' : ['uniform', 'distance'],\n              'algorithm' : ['auto', 'ball_tree','kd_tree','brute'],\n              'p' : [2,3,4,5]\n             }\nclf_knn = GridSearchCV(knn, param_grid = param_grid, cv = 5, n_jobs = -1)\nbest_clf_knn = clf_knn.fit(X_train,y_train)\nclf_performance(best_clf_knn,'K-Nearest Neighbors Classifier')","c9187671":"#random forest performance tuner\nrf = RandomForestClassifier(random_state = 1)\nparam_grid =  {\n                'n_estimators': [310], \n                'bootstrap': [True,False], #bagging (T) vs. pasting (F)\n                'max_depth': [1],\n                'max_features': ['auto','sqrt'],\n                #'min_samples_leaf': [1],\n                #'min_samples_split': [1]\n              }\nclf_rf_rnd = GridSearchCV(rf, param_grid = param_grid, cv = 5, n_jobs = -1)\nbest_clf_rf_rnd = clf_rf_rnd.fit(X_train,y_train)\nclf_performance(best_clf_rf_rnd,'Random Forest')","58375d34":"#support vector classifier performance tuner\nsvc = SVC(probability = True, random_state = 1)\nparam_grid = {\n              'kernel': ['rbf'],\n              'gamma': [1, 1e-1, 1e-2, 1e-3, 1e-4],\n              'C': np.arange(70,85,1)\n             }\nclf_svc = GridSearchCV(svc, param_grid = param_grid, cv = 5, n_jobs = -1)\nbest_clf_svc = clf_svc.fit(X_train,y_train)\nclf_performance(best_clf_svc,'Support Vector Classifier')","78611ffb":"#xgboost classifier performance tuner\nxgb = XGBClassifier(use_label_encoder=False, eval_metric='error', random_state =1)\nparam_grid = {\n              'max_depth': [9],\n              'n_estimators': [37],\n              'learning_rate': [1.2]\n             }\nclf_xgb = GridSearchCV(xgb, param_grid = param_grid, cv = 5, n_jobs = -1)\nbest_clf_xgb = clf_xgb.fit(X_train,y_train)\nclf_performance(best_clf_xgb,'XGBoost Classifier')","0874d9aa":"#stacking def\ndef get_stacking():\n    # define the base models\n    level0 = list()\n    level0.append(('gnb', GaussianNB()))\n    level0.append(('dt', tree.DecisionTreeClassifier(random_state = 1)))\n    level0.append(('lr', LogisticRegression()))\n    level0.append(('knn', KNeighborsClassifier()))\n    level0.append(('rf', RandomForestClassifier(random_state = 1)))\n    level0.append(('svc', SVC(probability = True, random_state = 1)))\n    level0.append(('xgb', XGBClassifier(use_label_encoder=False, eval_metric='error', random_state =1)))\n    # define meta learner model\n    level1 = LogisticRegression()\n    # define the stacking ensemble\n    stacking_model = StackingClassifier(estimators=level0, final_estimator=level1, cv=5)\n    return stacking_model\n\n#models def\ndef get_models():\n    models = dict()\n    models['gnb'] = GaussianNB()\n    models['dt'] = tree.DecisionTreeClassifier(random_state = 1)\n    models['lr'] = LogisticRegression()\n    models['knn'] = KNeighborsClassifier()\n    models['rf'] = RandomForestClassifier(random_state = 1)\n    models['svc'] = SVC(probability = True, random_state = 1)\n    models['xgb'] = XGBClassifier(use_label_encoder=False, eval_metric='error', random_state =1)\n    models['stacking'] = get_stacking()\n    return models\n\n#cross validate models and print results\nmodels = get_models()\nresults, names = list(),list()\nprint('Mean accuracy:')\nfor name, model in models.items():\n    scores = cross_val_score(model,X_train,y_train, scoring='accuracy', cv=5, n_jobs=-1)\n    results.append(scores)\n    names.append(name)\n    print('>%s %.3f +\/- %.3f' % (name, mean(scores), std(scores)))","4498e050":"#stacking def\ndef get_stacking():\n    # define the base models\n    level0 = list()\n    level0.append(('gnb', GaussianNB(var_smoothing= 0.19630406500402708)))\n    #level0.append(('dt', tree.DecisionTreeClassifier(random_state = 1, criterion= 'gini', max_depth= 1)))\n    #level0.append(('lr', LogisticRegression(C= 0.9999999999999999, max_iter= 15000)))\n    #level0.append(('knn', KNeighborsClassifier(algorithm= 'auto', n_neighbors= 19, p= 4, weights= 'uniform')))\n    #level0.append(('rf', RandomForestClassifier(random_state = 1, bootstrap= False, max_depth= 1, max_features= 'auto', n_estimators= 310)))\n    level0.append(('svc', SVC(probability = True, random_state = 1,C= 70, gamma = 0.001, kernel= 'rbf')))\n    #level0.append(('xgb', XGBClassifier(use_label_encoder=False, eval_metric='error', random_state =1, learning_rate= 1.2, max_depth= 9, n_estimators= 37)))\n    # define meta learner model\n    level1 = LogisticRegression()\n    # define the stacking ensemble\n    stacking_model = StackingClassifier(estimators=level0, final_estimator=level1, cv=5)\n    return stacking_model\n\n#models def\ndef get_models():\n    models = dict()\n    models['gnb'] = GaussianNB(var_smoothing= 0.19630406500402708)\n    #models['dt'] = tree.DecisionTreeClassifier(random_state = 1, criterion= 'gini', max_depth= 1)\n    #models['lr'] = LogisticRegression(C= 0.9999999999999999, max_iter= 15000)\n    #models['knn'] = KNeighborsClassifier(algorithm= 'auto', n_neighbors= 19, p= 4, weights= 'uniform')\n    #models['rf'] = RandomForestClassifier(random_state = 1, bootstrap= False, max_depth= 1, max_features= 'auto', n_estimators= 310)\n    models['svc'] = SVC(probability = True, random_state = 1,C= 70, gamma = 0.001, kernel= 'rbf')\n    #models['xgb'] = XGBClassifier(use_label_encoder=False, eval_metric='error', random_state =1, learning_rate= 1.2, max_depth= 9, n_estimators= 37)\n    models['stacking'] = get_stacking()\n    return models\n\n#cross validate models and print results\nmodels = get_models()\nresults, names = list(),list()\nprint('Mean accuracy:')\nfor name, model in models.items():\n    scores = cross_val_score(model,X_train,y_train, scoring='accuracy', cv=5, n_jobs=-1)\n    results.append(scores)\n    names.append(name)\n    print('>%s %.3f +\/- %.3f' % (name, mean(scores), std(scores)))","17802270":"#stacking def\ndef get_hard_voting():\n    # define the base models\n    level0 = list()\n    level0.append(('gnb', GaussianNB()))\n    level0.append(('dt', tree.DecisionTreeClassifier(random_state = 1)))\n    level0.append(('lr', LogisticRegression()))\n    level0.append(('knn', KNeighborsClassifier()))\n    level0.append(('rf', RandomForestClassifier(random_state = 1)))\n    level0.append(('svc', SVC(probability = True, random_state = 1)))\n    level0.append(('xgb', XGBClassifier(use_label_encoder=False, eval_metric='error', random_state =1)))\n    # define meta learner model\n    hard_voting_model = VotingClassifier(estimators=level0, voting='hard')\n    return hard_voting_model\n\n#models def\ndef get_models():\n    models = dict()\n    models['gnb'] = GaussianNB()\n    models['dt'] = tree.DecisionTreeClassifier(random_state = 1)\n    models['lr'] = LogisticRegression()\n    models['knn'] = KNeighborsClassifier()\n    models['rf'] = RandomForestClassifier(random_state = 1)\n    models['svc'] = SVC(probability = True, random_state = 1)\n    models['xgb'] = XGBClassifier(use_label_encoder=False, eval_metric='error', random_state =1)\n    models['hv'] = get_hard_voting()\n    return models\n\n#cross validate models and print results\nmodels = get_models()\nresults, names = list(),list()\nprint('Mean accuracy:')\nfor name, model in models.items():\n    scores = cross_val_score(model,X_train,y_train, scoring='accuracy', cv=5, n_jobs=-1)\n    results.append(scores)\n    names.append(name)\n    print('>%s %.3f +\/- %.3f' % (name, mean(scores), std(scores)))","4723498d":"#stacking def\ndef get_hard_voting():\n    # define the base models\n    level0 = list()\n    level0.append(('gnb', GaussianNB(var_smoothing= 0.19630406500402708)))\n    #level0.append(('dt', tree.DecisionTreeClassifier(random_state = 1, criterion= 'gini', max_depth= 1)))\n    level0.append(('lr', LogisticRegression(C= 0.9999999999999999, max_iter= 15000)))\n    #level0.append(('knn', KNeighborsClassifier(algorithm= 'auto', n_neighbors= 19, p= 4, weights= 'uniform')))\n    #level0.append(('rf', RandomForestClassifier(random_state = 1, bootstrap= False, max_depth= 1, max_features= 'auto', n_estimators= 310)))\n    level0.append(('svc', SVC(probability = True, random_state = 1,C= 70, gamma = 0.001, kernel= 'rbf')))\n    #level0.append(('xgb', XGBClassifier(use_label_encoder=False, eval_metric='error', random_state =1, learning_rate= 1.2, max_depth= 9, n_estimators= 37)))\n    hard_voting_model = VotingClassifier(estimators=level0, voting='hard')\n    return hard_voting_model\n\n#models def\ndef get_models():\n    models = dict()\n    models['gnb'] = GaussianNB(var_smoothing= 0.19630406500402708)\n    #models['dt'] = tree.DecisionTreeClassifier(random_state = 1, criterion= 'gini', max_depth= 1)\n    models['lr'] = LogisticRegression(C= 0.9999999999999999, max_iter= 15000)\n    #models['knn'] = KNeighborsClassifier(algorithm= 'auto', n_neighbors= 19, p= 4, weights= 'uniform')\n    #models['rf'] = RandomForestClassifier(random_state = 1, bootstrap= False, max_depth= 1, max_features= 'auto', n_estimators= 310)\n    models['svc'] = SVC(probability = True, random_state = 1,C= 70, gamma = 0.001, kernel= 'rbf')\n    #models['xgb'] = XGBClassifier(use_label_encoder=False, eval_metric='error', random_state =1, learning_rate= 1.2, max_depth= 9, n_estimators= 37)\n    models['hv'] = get_hard_voting()\n    return models\n\n#cross validate models and print results\nmodels = get_models()\nresults, names = list(),list()\nprint('Mean accuracy:')\nfor name, model in models.items():\n    scores = cross_val_score(model,X_train,y_train, scoring='accuracy', cv=5, n_jobs=-1)\n    results.append(scores)\n    names.append(name)\n    print('>%s %.3f +\/- %.3f' % (name, mean(scores), std(scores)))","1ece8d98":"#stacking def\ndef get_soft_voting():\n    # define the base models\n    level0 = list()\n    level0.append(('gnb', GaussianNB()))\n    level0.append(('dt', tree.DecisionTreeClassifier(random_state = 1)))\n    level0.append(('lr', LogisticRegression()))\n    level0.append(('knn', KNeighborsClassifier()))\n    level0.append(('rf', RandomForestClassifier(random_state = 1)))\n    level0.append(('svc', SVC(probability = True, random_state = 1)))\n    level0.append(('xgb', XGBClassifier(use_label_encoder=False, eval_metric='error', random_state =1)))\n    # define meta learner model\n    soft_voting_model = VotingClassifier(estimators=level0, voting='soft')\n    return soft_voting_model\n\n#models def\ndef get_models():\n    models = dict()\n    models['gnb'] = GaussianNB()\n    models['dt'] = tree.DecisionTreeClassifier(random_state = 1)\n    models['lr'] = LogisticRegression()\n    models['knn'] = KNeighborsClassifier()\n    models['rf'] = RandomForestClassifier(random_state = 1)\n    models['svc'] = SVC(probability = True, random_state = 1)\n    models['xgb'] = XGBClassifier(use_label_encoder=False, eval_metric='error', random_state =1)\n    models['sv'] = get_soft_voting()\n    return models\n\n#cross validate models and print results\nmodels = get_models()\nresults, names = list(),list()\nprint('Mean accuracy:')\nfor name, model in models.items():\n    scores = cross_val_score(model,X_train,y_train, scoring='accuracy', cv=5, n_jobs=-1)\n    results.append(scores)\n    names.append(name)\n    print('>%s %.3f +\/- %.3f' % (name, mean(scores), std(scores)))","d7667053":"#stacking def\ndef get_soft_voting():\n    # define the base models\n    level0 = list()\n    level0.append(('gnb', GaussianNB(var_smoothing= 0.19630406500402708)))\n    #level0.append(('dt', tree.DecisionTreeClassifier(random_state = 1, criterion= 'gini', max_depth= 1)))\n    #level0.append(('lr', LogisticRegression(C= 0.9999999999999999, max_iter= 15000)))\n    #level0.append(('knn', KNeighborsClassifier(algorithm= 'auto', n_neighbors= 19, p= 4, weights= 'uniform')))\n    #level0.append(('rf', RandomForestClassifier(random_state = 1, bootstrap= False, max_depth= 1, max_features= 'auto', n_estimators= 310)))\n    level0.append(('svc', SVC(probability = True, random_state = 1,C= 70, gamma = 0.001, kernel= 'rbf')))\n    #level0.append(('xgb', XGBClassifier(use_label_encoder=False, eval_metric='error', random_state =1, learning_rate= 1.2, max_depth= 9, n_estimators= 37)))\n    # define meta learner model\n    soft_voting_model = VotingClassifier(estimators=level0, voting='soft')\n    return soft_voting_model\n\n#models def\ndef get_models():\n    models = dict()\n    models['gnb'] = GaussianNB(var_smoothing= 0.19630406500402708)\n    #models['dt'] = tree.DecisionTreeClassifier(random_state = 1, criterion= 'gini', max_depth= 1)\n    #models['lr'] = LogisticRegression(C= 0.9999999999999999, max_iter= 15000)\n    #models['knn'] = KNeighborsClassifier(algorithm= 'auto', n_neighbors= 19, p= 4, weights= 'uniform')\n    #models['rf'] = RandomForestClassifier(random_state = 1, bootstrap= False, max_depth= 1, max_features= 'auto', n_estimators= 310)\n    models['svc'] = SVC(probability = True, random_state = 1,C= 70, gamma = 0.001, kernel= 'rbf')\n    #models['xgb'] = XGBClassifier(use_label_encoder=False, eval_metric='error', random_state =1, learning_rate= 1.2, max_depth= 9, n_estimators= 37)\n    models['sv'] = get_soft_voting()\n    return models\n\n#cross validate models and print results\nmodels = get_models()\nresults, names = list(),list()\nprint('Mean accuracy:')\nfor name, model in models.items():\n    scores = cross_val_score(model,X_train,y_train, scoring='accuracy', cv=5, n_jobs=-1)\n    results.append(scores)\n    names.append(name)\n    print('>%s %.3f +\/- %.3f' % (name, mean(scores), std(scores)))","cd8e0baa":"#baggingclassifier baseline\nbagging_model = BaggingClassifier(base_estimator=RandomForestClassifier(),\n                                     bootstrap=True,\n                                     random_state=1,\n                                     n_jobs=-1\n                                     )\n\nbagging_model.fit(X_train , y_train)\n\ncv = cross_val_score(bagging_model, X_train, y_train, cv=5)\nprint(mean(cv), '+\/-', std(cv))","09cc3fa6":"#baggingclassifier tuning\nbagging_model = BaggingClassifier(base_estimator=RandomForestClassifier(),\n                                     bootstrap=True,\n                                     random_state=1,\n                                     n_estimators=20,\n                                     max_samples=50,\n                                     n_jobs=-1,\n                                     )\n\nbagging_model.fit(X_train , y_train)\n\ncv = cross_val_score(bagging_model, X_train, y_train, cv=5)\nprint(mean(cv), '+\/-', std(cv))","fac535c8":"#baggingclassifier (pasting) baseline\npasting_model = BaggingClassifier(base_estimator=RandomForestClassifier(),\n                                     bootstrap=False,\n                                     random_state=1,\n                                     n_jobs=-1\n                                     )\n\npasting_model.fit(X_train , y_train)\n\ncv = cross_val_score(pasting_model, X_train, y_train, cv=5)\nprint(mean(cv), '+\/-', std(cv))","65353410":"#baggingclassifier (pasting) tuner\npasting_model = BaggingClassifier(base_estimator=RandomForestClassifier(random_state = 1, bootstrap=True,max_depth=7, max_features='auto', n_estimators=340),\n                                     bootstrap=False,\n                                     random_state=1,\n                                     n_estimators=20,\n                                     max_samples=50,\n                                     n_jobs=-1,\n                                     )\n\npasting_model.fit(X_train , y_train)\n\ncv = cross_val_score(pasting_model, X_train, y_train, cv=5)\nprint(mean(cv), '+\/-', std(cv))","da3c662a":"#addboostclassifier baseline\nadaboost_model = AdaBoostClassifier(base_estimator=RandomForestClassifier(),\n                                       random_state=1)\n\nadaboost_model.fit(X_train , y_train)\n\ncv = cross_val_score(adaboost_model, X_train, y_train, cv=5)\nprint(mean(cv), '+\/-', std(cv))","e24099c2":"#adaboostclassifier tuning\nadaboost_model = AdaBoostClassifier(base_estimator=RandomForestClassifier(random_state = 1, bootstrap=True,max_depth=7, max_features='auto', n_estimators=340),\n#                                        learning_rate=1,\n                                       random_state=1)\n\nadaboost_model.fit(X_train , y_train)\n\ncv = cross_val_score(adaboost_model, X_train, y_train, cv=5)\nprint(mean(cv), '+\/-', std(cv))","fc7d5156":"#import evaluation tools\nfrom sklearn.metrics import accuracy_score,precision_score, matthews_corrcoef, confusion_matrix, classification_report\nimport scikitplot as skplt","9f4a05a5":"#create support vector classifier model with tuned parameters\nsvc = SVC(probability = True, random_state = 1,C= 70, gamma = 0.001, kernel= 'rbf')\nsvc.fit(X_train,y_train)\ny_pred1 = svc.predict(X_test)\n\n#assess accuracy\nprint('SVC test accuracy: {}'.format(accuracy_score(y_test, y_pred1)))","b145607b":"#support vector classifier confusion matrix\n#create and reshape confusion matrix data\nmatrix = confusion_matrix(y_test, y_pred1)\nmatrix = matrix.astype('float') \/ matrix.sum(axis=1)[:, np.newaxis]\n\n#plot as heatmap\nplt.figure(figsize=(16,7))\nsns.set(font_scale=1.4)\nsns.heatmap(matrix, annot=True, annot_kws={'size':10},\n            cmap=sns.color_palette('Reds'), linewidths=0.2, vmin=0, vmax=1)\n\n#plot settings\nclass_names = ['Heart disease', 'No heart disease']\ntick_marks = np.arange(len(class_names))\ntick_marks2 = tick_marks + 0.5\nplt.xticks(tick_marks, class_names, rotation=25)\nplt.yticks(tick_marks2, class_names, rotation=0)\nplt.xlabel('Predicted label')\nplt.ylabel('True label')\nplt.title('Confusion Matrix for Support Vector Classifier')\nplt.show()","9395c89b":"#support vector classifier sensitivity and specificity calculations\ntotal=sum(sum(matrix))\n\nprint('SVC')\nsensitivity = matrix[0,0]\/(matrix[0,0]+matrix[1,0])\nprint('Sensitivity : ', sensitivity )\n\nspecificity = matrix[1,1]\/(matrix[1,1]+matrix[0,1])\nprint('Specificity : ', specificity)","99d4764a":"#view the support vector classification report\nprint('SVC')\nprint(classification_report(y_test, y_pred1))","67b64e4d":"#lift curve for support vector classifier\ntarget_prob = svc.predict_proba(X_test)\nskplt.metrics.plot_lift_curve(y_test, target_prob)\nplt.show()","57c7e7f0":"#Matthews correlation coefficient for SVC\nprint('SVC MCC: {}'.format(matthews_corrcoef(y_test, y_pred1)))","d62b3950":"#create stacking classifier model\nstacking_model = get_stacking()\nstacking_model.fit(X_train,y_train)\ny_pred2 = stacking_model.predict(X_test)\n\n#assess accuracy\nprint('StackingClassifier test accuracy: {}'.format(accuracy_score(y_test, y_pred2)))","cfefd6f6":"#stacking classifier confusion matrix\n#create and reshape confusion matrix data\nmatrix = confusion_matrix(y_test, y_pred2)\nmatrix = matrix.astype('float') \/ matrix.sum(axis=1)[:, np.newaxis]\n\n#plot as heatmap\nplt.figure(figsize=(16,7))\nsns.set(font_scale=1.4)\nsns.heatmap(matrix, annot=True, annot_kws={'size':10},\n            cmap=sns.color_palette('Reds'), linewidths=0.2, vmin=0, vmax=1)\n\n#plot settings\nclass_names = ['Heart disease', 'No heart disease']\ntick_marks = np.arange(len(class_names))\ntick_marks2 = tick_marks + 0.5\nplt.xticks(tick_marks, class_names, rotation=25)\nplt.yticks(tick_marks2, class_names, rotation=0)\nplt.xlabel('Predicted label')\nplt.ylabel('True label')\nplt.title('Confusion Matrix for Stacking Classifier')\nplt.show()","c7ee04dd":"#stacking classifier sensitivity and specificity calculations\ntotal=sum(sum(matrix))\n\nprint('StackingClassifier')\nsensitivity = matrix[0,0]\/(matrix[0,0]+matrix[1,0])\nprint('Sensitivity : ', sensitivity )\n\nspecificity = matrix[1,1]\/(matrix[1,1]+matrix[0,1])\nprint('Specificity : ', specificity)","fd506d8a":"#view the stacking classification report\nprint('StackingClassifier')\nprint(classification_report(y_test, y_pred2))","479b76e9":"#lift curve for the stacking model\ntarget_prob = stacking_model.predict_proba(X_test)\nskplt.metrics.plot_lift_curve(y_test, target_prob)\nplt.show()","49dfa0c8":"#Matthews correlation coefficient for StackingClassifier\nprint('StackingClassifier MCC: {}'.format(matthews_corrcoef(y_test, y_pred2)))","30eb7c4b":"#create hard voting classifier model\nhv_model = get_hard_voting()\nhv_model.fit(X_train,y_train)\ny_pred3 = hv_model.predict(X_test)\n\n#assess accuracy\nprint('Hard VotingClassifier test accuracy: {}'.format(accuracy_score(y_test, y_pred3)))","03a5fbde":"#hard voting classifier confusion matrix\n#create and reshape confusion matrix data\nmatrix = confusion_matrix(y_test, y_pred3)\nmatrix = matrix.astype('float') \/ matrix.sum(axis=1)[:, np.newaxis]\n\n#plot as heatmap\nplt.figure(figsize=(16,7))\nsns.set(font_scale=1.4)\nsns.heatmap(matrix, annot=True, annot_kws={'size':10},\n            cmap=sns.color_palette('Reds'), linewidths=0.2, vmin=0, vmax=1)\n\n#plot settings\nclass_names = ['Heart disease', 'No heart disease']\ntick_marks = np.arange(len(class_names))\ntick_marks2 = tick_marks + 0.5\nplt.xticks(tick_marks, class_names, rotation=25)\nplt.yticks(tick_marks2, class_names, rotation=0)\nplt.xlabel('Predicted label')\nplt.ylabel('True label')\nplt.title('Confusion Matrix for Hard VotingClassifier')\nplt.show()","aaa5486e":"#hard voting classifier sensitivity and specificity calculations\ntotal=sum(sum(matrix))\n\nprint('Hard VotingClassifier')\nsensitivity = matrix[0,0]\/(matrix[0,0]+matrix[1,0])\nprint('Sensitivity : ', sensitivity )\n\nspecificity = matrix[1,1]\/(matrix[1,1]+matrix[0,1])\nprint('Specificity : ', specificity)","0606a83e":"#view the hard voting classification report\nprint('Hard VotingClassifier')\nprint(classification_report(y_test, y_pred3))","873b3cf1":"#Matthews correlation coefficient for Hard VotingClassifier\nprint('Hard VotingClassifier MCC: {}'.format(matthews_corrcoef(y_test, y_pred3)))","735f5489":"#create soft voting classifier model\nsv_model = get_soft_voting()\nsv_model.fit(X_train,y_train)\ny_pred4 = sv_model.predict(X_test)\n\n#assess accuracy\nprint('Soft VotingClassifier test accuracy: {}'.format(accuracy_score(y_test, y_pred4)))","c9f45303":"#soft voting classifier confusion matrix\n#create and reshape confusion matrix data\nmatrix = confusion_matrix(y_test, y_pred4)\nmatrix = matrix.astype('float') \/ matrix.sum(axis=1)[:, np.newaxis]\n\n#plot as heatmap\nplt.figure(figsize=(16,7))\nsns.set(font_scale=1.4)\nsns.heatmap(matrix, annot=True, annot_kws={'size':10},\n            cmap=sns.color_palette('Reds'), linewidths=0.2, vmin=0, vmax=1)\n\n#plot settings\nclass_names = ['Heart disease', 'No heart disease']\ntick_marks = np.arange(len(class_names))\ntick_marks2 = tick_marks + 0.5\nplt.xticks(tick_marks, class_names, rotation=25)\nplt.yticks(tick_marks2, class_names, rotation=0)\nplt.xlabel('Predicted label')\nplt.ylabel('True label')\nplt.title('Confusion Matrix for Soft VotingClassifier')\nplt.show()","27bf6359":"#soft voting classifier sensitivity and specificity calculations\ntotal=sum(sum(matrix))\n\nprint('Soft VotingClassifier')\nsensitivity = matrix[0,0]\/(matrix[0,0]+matrix[1,0])\nprint('Sensitivity : ', sensitivity )\n\nspecificity = matrix[1,1]\/(matrix[1,1]+matrix[0,1])\nprint('Specificity : ', specificity)","cd866d25":"#view the soft voting classification report\nprint('Soft VotingClassifier')\nprint(classification_report(y_test, y_pred4))","35753962":"#lift curve for the soft voting model\ntarget_prob = sv_model.predict_proba(X_test)\nskplt.metrics.plot_lift_curve(y_test, target_prob)\nplt.show()","a3b05b09":"#Matthews correlation coefficient for Soft VotingClassifier\nprint('Soft VotingClassifier MCC: {}'.format(matthews_corrcoef(y_test, y_pred4)))","88d4836e":"#create naive bayes model\ngnb = GaussianNB(var_smoothing= 0.19630406500402708)\ngnb.fit(X_train,y_train)\ny_pred5 = gnb.predict(X_test)\n\n#assess accuracy\nprint('GaussianNB test accuracy: {}'.format(accuracy_score(y_test, y_pred5)))","a5c181e7":"#naive bayes confusion matrix\n#create and reshape confusion matrix data\nmatrix = confusion_matrix(y_test, y_pred5)\nmatrix = matrix.astype('float') \/ matrix.sum(axis=1)[:, np.newaxis]\n\n#plot as heatmap\nplt.figure(figsize=(16,7))\nsns.set(font_scale=1.4)\nsns.heatmap(matrix, annot=True, annot_kws={'size':10},\n            cmap=sns.color_palette('Reds'), linewidths=0.2, vmin=0, vmax=1)\n\n#plot settings\nclass_names = ['Heart disease', 'No heart disease']\ntick_marks = np.arange(len(class_names))\ntick_marks2 = tick_marks + 0.5\nplt.xticks(tick_marks, class_names, rotation=25)\nplt.yticks(tick_marks2, class_names, rotation=0)\nplt.xlabel('Predicted label')\nplt.ylabel('True label')\nplt.title('Confusion Matrix for GaussianNB')\nplt.show()","fb1f291a":"#naive bayes sensitivity and specificity calculations\ntotal=sum(sum(matrix))\n\nprint('GaussianNB')\nsensitivity = matrix[0,0]\/(matrix[0,0]+matrix[1,0])\nprint('Sensitivity : ', sensitivity )\n\nspecificity = matrix[1,1]\/(matrix[1,1]+matrix[0,1])\nprint('Specificity : ', specificity)","7715793f":"#view the niave bayes report\nprint('GaussianNB')\nprint(classification_report(y_test, y_pred5))","ccbd6f81":"#lift curve for the pasting model\ntarget_prob = gnb.predict_proba(X_test)\nskplt.metrics.plot_lift_curve(y_test, target_prob)\nplt.show()","558c3925":"print('GaussianNB MCC: {}'.format(matthews_corrcoef(y_test, y_pred5)))","9b8e9aed":"#plot ROC curve for best classifiers\nfrom sklearn import metrics\n\npred_prob1 = svc.predict_proba(X_test)\npred_prob2 = stacking_model.predict_proba(X_test)\npred_prob4 = sv_model.predict_proba(X_test)\npred_prob5 = pasting_model.predict_proba(X_test)\nfpr1, tpr1, thresholds1 = metrics.roc_curve(y_test, pred_prob1[:,1],pos_label=1)\nfpr2, tpr2, thresholds2 = metrics.roc_curve(y_test, pred_prob2[:,1],pos_label=1)\nfpr4, tpr4, thresholds2 = metrics.roc_curve(y_test, pred_prob4[:,1],pos_label=1)\nfpr5, tpr5, thresholds3 = metrics.roc_curve(y_test, pred_prob5[:,1],pos_label=1)\n\nfig, ax = plt.subplots(figsize=(16, 10))\nax.plot(fpr1, tpr1, label='SVC')\nax.plot(fpr2, tpr2, label='Stacking')\nax.plot(fpr4, tpr4, label='Soft Voting')\nax.plot(fpr5, tpr5, label='GaussianNB')\nax.plot([0, 1], [0, 1], transform=ax.transAxes, ls=\"--\", c=\".3\")\nplt.legend()\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.rcParams['font.size'] = 12\nplt.title('ROC curve for heart disease classifiers')\nplt.xlabel('False Positive Rate (1 - Specificity)')\nplt.ylabel('True Positive Rate (Sensitivity)')\nplt.grid(True)","e7453453":"#calculate AUC for classifiers\nprint('SVC AUC: {}'.format(metrics.auc(fpr1, tpr1)))\nprint('Stacking AUC: {}'.format(metrics.auc(fpr2, tpr2)))\nprint('Soft Voting AUC: {}'.format(metrics.auc(fpr4, tpr4)))\nprint('GaussianNB AUC: {}'.format(metrics.auc(fpr5, tpr5)))","ceb60d0e":"#import libraries\nfrom matplotlib import pyplot as plt\nfrom pdpbox import pdp, get_dataset, info_plots\nimport eli5\nfrom eli5.sklearn import PermutationImportance\nfrom sklearn import tree\nimport graphviz\nimport shap","492aab83":"#fit the model\nhv_model.fit(X_train,y_train)\n\n#make prediction\ny_pred = hv_model.predict(X_test)","8c2bf2b5":"#determine feature weights\nperm = PermutationImportance(hv_model, random_state=1).fit(X_test, y_test)\neli5.show_weights(perm, feature_names = list(feature_names), top=len(feature_names))","0593a8a5":"#build SVC model\nsvc_model = SVC(probability = True, random_state = 1, C= 100, gamma= 0.01, kernel = 'rbf').fit(X_train,y_train)","42ef3497":"#create object that can calculate shap values\nexplainer = shap.KernelExplainer(svc_model.predict_proba, X_train)\n\npred_data = pd.DataFrame(X_test)\n\npred_data.columns = feature_names\n\ndata_for_prediction = pred_data\n\n#calculate Shap values\nshap_values = explainer.shap_values(data_for_prediction)\n\nshap.initjs()\nshap.summary_plot(shap_values[1], data_for_prediction)","b824228b":"#fit the model\nstacking_model.fit(X_train,y_train)\n\n#make prediction\ny_pred = stacking_model.predict(X_test)","adad8ddd":"#determine feature weights\nperm = PermutationImportance(stacking_model, random_state=1).fit(X_test, y_test)\neli5.show_weights(perm, feature_names = list(feature_names), top=len(feature_names))","06ae30ee":"#create decision tree model with tuned parameters\ndt = tree.DecisionTreeClassifier(criterion= 'entropy', max_depth= 3)\ndt.fit(X_train,y_train)\ny_pred = dt.predict(X_test)\n\n#dt accuracy print\nprint('dt test accuracy: {}'.format(accuracy_score(y_test, y_pred)))","11a6c22b":"#look at decision tree\n#value tells how many records from each category entered the box (i.e., [# of records = 0, # of records = 1])\ntree_graph = tree.export_graphviz(dt, out_file=None, feature_names=feature_names)\ngraphviz.Source(tree_graph)","6a74a808":"#create object that can calculate shap values\nexplainer = shap.TreeExplainer(dt)\n\npred_data = pd.DataFrame(X_test)\n\npred_data.columns = feature_names\n\ndata_for_prediction = pred_data\n\n#calculate Shap values\nshap_values = explainer.shap_values(data_for_prediction)\n\n#create summary plot\nshap.initjs()\nshap.summary_plot(shap_values[1], data_for_prediction)","f2b8ea67":"**Baseline**","3a3b289c":"**Baseline**","3653c353":"# Modeling","719a96a6":"**SVC**","74aea197":"**Hyperparameter tuning**","dc56e9d6":"**Hyperparameter tuning**","a4682dd2":"**Hyperparameter tuning**","5367dd55":"**Decision Tree**","0fe5c1ec":"**Mutual information**","db3e9c36":"# AdaBoostClassifier","55ca02c5":"# Soft VotingClassifier","cf3b8a92":"# Evaluating the best models","e9f81067":"**Baseline**","136f9376":"# Stacking Classifier","fa7c308c":"# EDA","b89f464f":"# Feature Importance","88b8f510":"# End-to-end Heart Disease Prediction Project\n\n[Front-end](https:\/\/predict-heart-diseases.herokuapp.com\/)\n\n[GitHub repo](https:\/\/github.com\/MichaelBryantDS\/heart-disease-pred)","9cb44c6d":"# Hard VotingClassifier","5dd5b63a":"**Risk factors feature**","9da66709":"**Naive Bayes**","d6cf1141":"# BaggingClassifier","eee0d9a0":"**Hyperparameter tuning**","d21230ed":"# Peparing data for ML","bf0b0947":"**Soft VotingClassifier**","a7e13e98":"**Baseline**","dae6a3dd":"**StackingClassifier**","f192fbd6":"**Hard VotingClassifier**","53c6210b":"**StackingClassifier**","c75099e4":"**Finding correlations with a heat map and visualizations**","9b5ea84f":"**Baseline**","7d8a493f":"**SVC**","aa2c24bd":"**Best model**: Hard VotingClassifier\n\n* Accuracy: 0.8553\n* Sensitivity: 0.8717\n* Specificity: 0.8367\n* Precision: 0.8571\n* MCC: 0.7084","362b8e98":"# BaggingClassifier (Pasting)","eba8bb8b":"# Productionization\n\nI created a [front-end](https:\/\/predict-heart-diseases.herokuapp.com\/) using this model using Flask and Heroku to help doctors diagnose heart disease in patients with angina.\n\nSee the [GitHub repo](https:\/\/github.com\/MichaelBryantDS\/heart-disease-pred) for more information.","0b09f1c6":"**Hyperparameter tuning**","137e47e2":"**ROC\/AUC**","62cb6b47":"**Import libraries and data**","321080bf":"**Data distribution and outliers**","aab250b0":"**Baseline**","0bf43e3c":"**Hyperparameter tuning**","5a03755c":"**Hard VotingClassifier**","c8993ce8":"**Baseline**","96ec4afe":"**Hyperparameter tuning**"}}