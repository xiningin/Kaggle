{"cell_type":{"a19d379f":"code","edd5fa2e":"code","735d8db5":"code","c74dc550":"code","8d8f6a75":"code","e6f40ec4":"code","a6ed39dd":"code","3c9236f9":"code","3e1dfe33":"code","095abea9":"code","603074d5":"code","ee15a639":"code","031784d9":"code","1531198f":"code","158df01b":"code","8cdf17aa":"code","8dad810e":"markdown","603b6812":"markdown","d9d52081":"markdown","2f9d436a":"markdown","af6102b3":"markdown","9b005a36":"markdown","965f0df7":"markdown","ec642a62":"markdown","27c58ead":"markdown","db65036c":"markdown","aea50384":"markdown","9a171b64":"markdown","e70be0bf":"markdown","a9ab3980":"markdown","6504a47f":"markdown","c7b4e538":"markdown","7c1f9d49":"markdown"},"source":{"a19d379f":"\nimport cv2\nimport numpy as np # linear algebra\nimport keras\nfrom keras import backend as K\nfrom keras import layers\nimport random\nimport os\nprint(os.listdir(\"..\/input\"))\n\n","edd5fa2e":"train_dir = '..\/input\/cityscapes_data\/cityscapes_data\/train\/'\nval_dir = '..\/input\/cityscapes_data\/cityscapes_data\/val\/'\nheight = 256\nwidth = 512\nchannels=3\n\n","735d8db5":"IDS_train=[]\nfor path in os.listdir(train_dir):\n      \n       IDS_train.append(train_dir + '\/' + path)\nIDS_val=[]\nprint(IDS_train[-5:])\nfor path in os.listdir(val_dir):\n       \n       IDS_val.append(val_dir + '\/' + path)\nprint(IDS_val[-5:])\nl=len(IDS_train)\npartition={'train':IDS_train, 'val':IDS_val}\n","c74dc550":"import matplotlib.pyplot as plt\n\n\ndef show_images(images):\n  # images=(no of images, width, height, channels)\n  n_x = np.int(np.sqrt(images.shape[0]))\n  n_y = np.int(np.sqrt(images.shape[0]))\n  #x and y coordinates in the images we will display\n  coor_x =images.shape[1]\n  coor_y =images.shape[2]\n  \n  figure = np.zeros((coor_x * n_x, coor_y * n_y, images.shape[3]))\n\n  for i in range(n_x):  \n    for j in range(n_y):  \n      ind = i+n_x*j\n      if ind >= images.shape[0]:\n        break\n      image = images[ind, :,:,:]\n      figure[i * coor_x: (i + 1) * coor_x, j * coor_y: (j + 1) * coor_y] = image\n\n  plt.figure(figsize=(20, 10))\n  plt.imshow(np.squeeze(figure))\n  ax = plt.gca()\n  \n  ax.grid(False)\n\n  plt.show()\n","8d8f6a75":"random.shuffle(IDS_val)\nim= np.empty((10, 256,512, 3))\n\nfor j in range(10):\n    x=cv2.imread(IDS_val[j])\n   \n    im[j]=x\/255 #normalizing\n \n\nshow_images(im)","e6f40ec4":"\n\nimg_input = keras.Input(shape=(height, height, channels))\n#Encoder\nx = layers.Conv2D(16, (3, 3), activation='relu', padding='same')(img_input)\nx = layers.MaxPooling2D((2, 2), padding='same')(x)\nx = layers.Conv2D(8, (3, 3), activation='relu', padding='same')(x)\nx = layers.MaxPooling2D((2, 2), padding='same')(x)\nx = layers.Conv2D(8, (3, 3), activation='relu', padding='same')(x)\nx = layers.MaxPooling2D((2, 2), padding='same')(x)\nx = layers.Conv2D(4, (3, 3), activation='relu', padding='same')(x)\nx = layers.MaxPooling2D((2, 2), padding='same')(x)\n\n\n\n#Decoder\nx = layers.Conv2D(4, (3, 3), activation='relu', padding='same')(x)\nx = layers.UpSampling2D((2, 2))(x)\nx = layers.Conv2D(8, (3, 3), activation='relu', padding='same')(x)\nx = layers.UpSampling2D((2, 2))(x)\nx = layers.Conv2D(8, (3, 3), activation='relu', padding='same')(x)\nx = layers.UpSampling2D((2, 2))(x)\nx = layers.Conv2D(16, (3, 3), activation='relu', padding='same')(x)\nx = layers.UpSampling2D((2, 2))(x)\nx = layers.Conv2D(3, (3, 3), activation='sigmoid', padding='same')(x)\n\n\n\n\n\n\nimg_output = x\n\nautoencoder = keras.models.Model(img_input, img_output)\nautoencoder.summary()\nautoencoder.compile(optimizer='rmsprop', loss='mse')\n\n","a6ed39dd":"class DataGenerator(keras.utils.Sequence):\n    'Generates data for Keras'\n    def __init__(self, list_IDs, batch_size=16, dim=(256,256), n_channels=3,\n                shuffle=True):\n        'Initialization'\n        self.dim = dim\n        self.batch_size = batch_size\n        self.list_IDs = list_IDs\n        self.n_channels = n_channels\n        self.shuffle = shuffle\n        self.on_epoch_end()\n\n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        return int(np.floor(len(self.list_IDs) \/ self.batch_size))\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        # Generate indexes of the batch\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n\n        # Find list of IDs\n        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n        #print(list_IDs_temp)\n\n        # Generate data\n        X = self.__data_generation(list_IDs_temp)\n\n        return X\n\n    def on_epoch_end(self):\n        'Updates indexes after each epoch'\n        self.indexes = np.arange(len(self.list_IDs))\n        if self.shuffle == True:\n            np.random.shuffle(self.indexes)\n\n    def __data_generation(self, list_IDs_temp):\n        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n        # Initialization\n        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n        \n        Z = np.empty((self.batch_size, 256,512, self.n_channels))\n        # Generate data\n        for i, ID in enumerate(list_IDs_temp):\n            # Store sample\n            Z[i]= cv2.imread(ID)\n            X[i] = np.asarray(Z[i][:,:256,:]\/255)\n\n\n        return X,X","3c9236f9":"# Generators\ntrain_gen = DataGenerator(IDS_train)\nval_gen = DataGenerator(IDS_val)\n","3e1dfe33":"#We assign early stopping in case we overfit\nearly_stop=keras.callbacks.EarlyStopping(monitor='val_loss',patience=2)\nhistory = autoencoder.fit_generator(train_gen,\n                              steps_per_epoch=train_gen.__len__(),\n                              epochs=5, verbose=1,\n                              validation_data=val_gen, callbacks=[early_stop], validation_steps=val_gen.__len__())","095abea9":"#plot Loss\n\nimport matplotlib.pyplot as plt\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(len(loss))\n\nplt.figure()\n\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()","603074d5":"random.shuffle(IDS_val)\nim= np.empty((32, 256,256, 3))\n\nmsk=np.empty((32, 256,256, 3))\nfor j in range(32):\n    x=cv2.imread(IDS_val[j])\n   \n    im[j]=x[:,:height,:]\/255 #normalizing\n    msk[j]=x[:,height:,:]\/255\n    msk=np.asarray(msk)\n\n\n\nae_images = autoencoder.predict(im)\nshow_images(ae_images)\nshow_images(im)","ee15a639":"#discriminator\n\ndiscriminator_input = layers.Input(shape=(height, height, channels))\n\n\nx = layers.Conv2D(128, 3)(discriminator_input)\nx = layers.BatchNormalization()(x)\nx = layers.LeakyReLU()(x)\n\nx = layers.Conv2D(128, 4, strides=2)(x)\nx = layers.BatchNormalization()(x)\nx = layers.LeakyReLU()(x)\n\nx = layers.Conv2D(128, 4, strides=2)(x)\nx = layers.BatchNormalization()(x)\nx = layers.LeakyReLU()(x)\n\nx = layers.Conv2D(128, 4, strides=2)(x)\nx = layers.BatchNormalization()(x)\nx = layers.LeakyReLU()(x)\n\nx = layers.Flatten()(x)\n\n\nx = layers.Dropout(0.5)(x)\n\n\ndiscriminator_output = layers.Dense(1, activation='sigmoid')(x)\n\ndiscriminator = keras.models.Model(discriminator_input, discriminator_output)\ndiscriminator.summary()\n\n# To stabilize training, we use learning rate decay\n# and gradient clipping (by value) in the optimizer.\ndiscriminator_optimizer = keras.optimizers.RMSprop(lr=0.0008, clipvalue=1.0, decay=1e-8)\ndiscriminator.compile(optimizer=discriminator_optimizer, loss='binary_crossentropy')","031784d9":"#We define the adversarially trained autoencoder using the discriminator\n#When adversarialy training the autoencoder, we do not want to update the discriminator\n# (will only apply to the `gan` model)\n\ngan_input = keras.Input(shape=(height, height, channels,))\nmain_output = autoencoder(gan_input)\nauxiliary_output=discriminator(main_output)\ndiscriminator.trainable = False\ngan = keras.models.Model(gan_input,  outputs=[main_output, auxiliary_output])\n\ngan_optimizer = keras.optimizers.RMSprop(lr=0.0004, clipvalue=1.0, decay=1e-8)\ngan.compile(optimizer=gan_optimizer,loss=['mse','binary_crossentropy'], loss_weights=[.7, .3])\ngan.summary()","1531198f":"# Start training loop\nbatch_size = 32\ndef batch_generator(IDS):\n    batch=np.empty((batch_size, height,height, channels))\n    \n    for j in range(len(IDS)):\n        x=cv2.imread(IDS[j])\n        x=x[:,:height,:]\n        batch[j]=np.asarray(x)\/255\n    return batch\nstart = 0\nepoch = 0\nnum_epochs=5\nadv_loss = np.zeros((num_epochs, 3))\nval_adv_loss = np.zeros((num_epochs, 3))\ndisc_loss = np.zeros((num_epochs, 1))\nwhile epoch < num_epochs:\n    #Step 1:train the discriminator\n    #We take random images\n    random.shuffle(IDS_train)\n    stop=start+batch_size\n    IDS_t=IDS_train[start:stop]\n    real_images=batch_generator(IDS_t)\n\n\n    #We take arbitrary generated images\n    random.shuffle(IDS_train)\n    stop=start+batch_size\n    IDS_t=IDS_train[start:stop]\n    generated_images = autoencoder.predict(real_images)\n    \n    #We combine them\n    combined_images = np.concatenate([generated_images, real_images])\n\n    # Assemble labels discriminating real from fake images\n    labels = np.concatenate([np.ones((batch_size, 1)),\n                             np.zeros((batch_size, 1))])  # 1=fake , 0=real\n    # Add random noise to the labels\n    labels += 0.05 * np.random.random(labels.shape)\n\n    #We feed them to the discriminator to recognize real from fake images\n    d_loss = discriminator.train_on_batch(combined_images, labels)\n\n    #Step 2: Update autoencoder to fool discriminator\n    # Make misleading targets 'all real images'\n    misleading_targets = np.zeros((batch_size, 1))\n\n\n    #training data that includes random_images with target generated images and misleading_targets\n    a_loss = gan.train_on_batch(real_images, [generated_images, misleading_targets])\n    \n    #Computing validation loss\n    #We shuffle the valuation ID's\n    random.shuffle(IDS_val)\n    IDS_v=IDS_val[:batch_size]\n    real_images_v=batch_generator(IDS_v)\n    generated_images_v=autoencoder.predict(real_images_v)\n\n    labels_v = np.zeros((batch_size, 1))  # 1=fake, 0=real\n    v_loss= gan.test_on_batch(real_images_v,[generated_images_v,labels_v])                         \n\n    start += batch_size\n    if start > len(IDS_train) - batch_size:\n        start = 0\n        \n        # We print metrics\n        print('discriminator loss at epoch %s: %s' % (epoch, d_loss))\n  \n        disc_loss[epoch,:] = d_loss\n\n        print('adversarial loss at epoch %s: %s' % (epoch, a_loss))\n\n        adv_loss[epoch,:] = a_loss\n    \n        val_adv_loss[epoch,:] = v_loss\n        print('val_adv loss at epoch %s: %s' % (epoch, v_loss))\n        epoch+=1","158df01b":"x_train=batch_generator(IDS_val[0:25])\nae_images, discrim = gan.predict(x_train)\nshow_images(ae_images)\nshow_images(x_train)\n\nloss1 = adv_loss\nval_loss = val_adv_loss\nepochs = range(num_epochs)\n\nplt.figure()\n\nplt.plot(epochs, disc_loss, 'bo', label='Discriminator Training loss')\nplt.plot(epochs, loss1, 'mo', label='Training loss')\nplt.plot(epochs, val_loss, 'co', label='Validation loss')\n\nplt.title('Losses')\nplt.legend()\n\nplt.show()","8cdf17aa":"''' Any comments are more than welcome. Thank you for taking the time to read my kernel.'''","8dad810e":"Lets plot some figures.","603b6812":"Let's take a look at some images outputed from the autoencoder.","d9d52081":"We define a batch generator class which we will fit the model with using fitgenerator in Keras.  I used the code that can be found here https:\/\/stanford.edu\/~shervine\/blog\/keras-how-to-generate-data-on-the-fly , and tweaked it a little to fit our purposes. In particular, this is where the image cropping (only taking the left side) happens.","2f9d436a":"Let's import the necessary packages","af6102b3":"We train the model for 15 epochs and fit it to the corresconding batch generator object.","9b005a36":"> 1) Training the discriminator. 2) Train the autoencoder to fool the discriminator","965f0df7":"We specify the training and validation directories.","ec642a62":"Let's plot the losses.","27c58ead":"Lets see how the images look like.","db65036c":"Lets make the training and validation batch generators","aea50384":"AUTOENCODER","9a171b64":"We build the autoencoder. The latent space will be 128 dimensional, half of the length of each side of every picture.","e70be0bf":"We a function to eventually show some images.","a9ab3980":"  The Cityscapes image dataset, has  2975 training images, where each image is 256x512. The left hand side corresponds to the cityscape  and the right hand side is the masking. In this notebook we will only be using the cityscape side, so we will be cropping the images discarding the masking. \n  The purpose is to creat an Autoencoder which is trained adversarially with cityscapes. That is, we would like to construct a small dimensional subspace, such that the information is compressed and then build a decoder, which upsamples elements from the given space to look somewhat \"similar\"  to the original picture. We will do this by creating a discriminator, which distinguishes real from generated images (using the autoencoder). We feed to the network a mix of real and generated images with misleading labels declaring all of the images as real. Then we update the weights of the autoencoder to  possibly fool the discriminator into thinking that generated images are real. \n  \n  * Disclaimer: This is still a work in progress, the decoded images do not yet seem to improve after adversarial training. Any suggestions on how to improve or correct the model are more than welcome.","6504a47f":"We create a list of training and validation ID's having the path to each picture. This is how we will feed the images to the network.","c7b4e538":"We define the generative adversarial network which will train the autoencoder.","7c1f9d49":"We build the discriminator, this will learn to differentiate real images (value 0) from generated images (value 1)."}}