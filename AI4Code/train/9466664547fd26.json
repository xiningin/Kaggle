{"cell_type":{"a611a0d3":"code","c2765daa":"code","29345f1e":"code","74146b03":"code","a9e5a5ed":"code","68c5e623":"code","4b898ca9":"code","bccb7799":"code","a1f96fd4":"code","94bec3a3":"code","f6181a8f":"code","a2e1b4f5":"code","6c49158c":"code","a9949724":"code","aedc3b51":"code","68e7db24":"code","6d3c540a":"code","00f2dff7":"code","c79e0bd8":"code","805d7956":"code","9d6ae76a":"code","5a0f167c":"code","b136550e":"code","4cd4bd0f":"code","f0cefa17":"code","da398965":"code","b5686454":"code","1a0b2917":"code","d104f08b":"code","3fc7b4a3":"code","3ee1c031":"code","dc82a6a3":"code","6e8d6683":"code","90db47e5":"code","04113ec6":"code","570d412f":"code","8255becd":"code","1f0ada9c":"code","e33aef21":"code","35c7f721":"markdown","c0f544fe":"markdown","f5d8a9ba":"markdown","59fed96f":"markdown","7b60206f":"markdown","2654e053":"markdown","2d7e5fa3":"markdown","a288637c":"markdown","6540938e":"markdown","e844ef8e":"markdown","38be5750":"markdown","09d1c0a9":"markdown","fc290a75":"markdown","de827a70":"markdown","65204803":"markdown"},"source":{"a611a0d3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('\/kaggle\/input\/'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c2765daa":"!pip install -q imageio\n!pip install -q git+https:\/\/github.com\/tensorflow\/docs","29345f1e":"import matplotlib.pyplot as plt\nfrom PIL import Image\nimport imageio\nimport pathlib\nimport time\nimport PIL\nimport glob\nfrom IPython import display\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\ntf.__version__","74146b03":"test_image = np.array(Image.open('..\/input\/celeba-dataset\/img_align_celeba\/img_align_celeba\/000023.jpg'))\nprint(test_image.shape)\nplt.imshow(test_image)","a9e5a5ed":"data_dir = pathlib.Path('..\/input\/celeba-dataset\/img_align_celeba\/img_align_celeba\/')\nimage_count = len(list(data_dir.glob('*.jpg')))\nprint(\"Total images : \",image_count)","68c5e623":"IMAGE_HEIGHT = 104\nIMAGE_WIDTH = 88\nGENERATOR_IMAGE_HEIGHT = 13\nGENERATOR_IMAGE_WIDTH = 11\nNOISE_DIM = 100\nVAL_SPLIT = 0.2","4b898ca9":"list_data = tf.data.Dataset.list_files(str(data_dir\/'*.jpg'), shuffle=False)\nlist_data = list_data.shuffle(image_count,reshuffle_each_iteration=False)","bccb7799":"for f in list_data.take(5):\n    print(f.numpy())","a1f96fd4":"val_size = int(image_count*VAL_SPLIT)\ntrain_ds_list = list_data.skip(val_size)\nval_ds_list = list_data.take(val_size)","94bec3a3":"# A funtion to plot the preprocessed image\ndef plot_image(processed_image):\n    processed_image = processed_image * 255\n    img = tf.cast(processed_image, dtype=tf.uint8)\n    plt.imshow(img)","f6181a8f":"def get_image(path):\n    img = tf.io.read_file(path)\n    img = tf.image.decode_jpeg(img, channels = 3)\n    img = tf.image.resize(img, [IMAGE_HEIGHT, IMAGE_WIDTH])\n    img = img \/ 255\n    return img\n\nplot_image(get_image('..\/input\/celeba-dataset\/img_align_celeba\/img_align_celeba\/000026.jpg'))","a2e1b4f5":"train_ds = train_ds_list.map(get_image)\nval_ds = val_ds_list.map(get_image)","6c49158c":"BATCH_SIZE = 32","a9949724":"def configure_for_performance(ds):\n#     ds = ds.cache()\n    ds = ds.shuffle(buffer_size=1000)\n    ds = ds.batch(BATCH_SIZE)\n    ds = ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n    return ds\n\ntrain_ds = configure_for_performance(train_ds)\nval_ds = configure_for_performance(val_ds)","aedc3b51":"class Upsample(tf.keras.Model):\n    def __init__(self, filters, kernel, stride ):\n        super(Upsample,self).__init__()\n        self.filters = filters\n        self.conv2DT = layers.Conv2DTranspose(self.filters, kernel_size = (kernel,kernel), strides=(stride,stride), padding='same', use_bias = 'false')\n        self.batchnorm = layers.BatchNormalization()\n        self.lrelu = layers.LeakyReLU()\n    \n    def call(self, inputs):\n        # input shape - (batch_size, x , y, z)\n        x = self.conv2DT(inputs)\n        x = self.batchnorm(x)\n        out = self.lrelu(x)\n        return out\n        # output shape - (batch_size, 2x , 2y, self.filters)","68e7db24":"# For the sake of understanding consider initial filters to be 256 and generator image height and generator image width to be 13 and 11 respectively \n# and follow the comments\nclass Generator(tf.keras.Model):\n    #                   13      11      256          5\n    def __init__(self,height, width, init_filters, kernel):\n        super(Generator,self).__init__()\n        self.dense = layers.Dense(height*width*init_filters, use_bias=False, input_shape=(NOISE_DIM,))\n        self.batchnorm = layers.BatchNormalization()\n        self.lrelu = layers.LeakyReLU()\n        self.reshape = layers.Reshape((height, width, init_filters))\n        self.upsample1 = Upsample(init_filters,kernel,1)\n        self.upsample2 = Upsample(init_filters\/2,kernel,2)\n        self.upsample3 = Upsample(init_filters\/4,kernel,2)\n        self.convtranspose = layers.Conv2DTranspose(3,(kernel,kernel), strides=(2,2), use_bias='false',padding='same', activation='sigmoid')\n        \n    def call(self, inputs):\n        # input shape - (batch_size, 100)\n        x = self.dense(inputs)\n        # shape - (batch_size, 13*11*256)\n        x = self.batchnorm(x)\n        x = self.lrelu(x)\n        x = self.reshape(x)\n        # shape - (batch_size, 13, 11, 256)\n        x = self.upsample1(x)\n        # shape - (batch_size, 13, 11, 256)\n        x = self.upsample2(x)\n        # shape - (batch_size, 26, 22, 128)\n        x = self.upsample3(x)\n        # shape - (batch_size, 52, 44, 64)\n        out = self.convtranspose(x)\n        # shape - (batch_size, 104, 88, 3)\n        return out","6d3c540a":"generator = Generator(GENERATOR_IMAGE_HEIGHT,GENERATOR_IMAGE_WIDTH,256,5)","00f2dff7":"# Testing how generator generates now\nnoise = tf.random.normal([1,NOISE_DIM])\ngenerated_image = generator(noise,training=False)\nprint(\"Image Shape: \",generated_image.shape)\nplt.imshow(generated_image[0])","c79e0bd8":"generator.build((1,100))\ngenerator.summary()","805d7956":"# This is one CNN block during downnsampling\nclass CNNBlock(tf.keras.Model):\n    def __init__(self, filters, kernel, stride):\n        super(CNNBlock, self).__init__()\n        self.conv = layers.Conv2D(filters, kernel_size = (kernel,kernel), strides = (stride, stride), padding='same')\n        self.lrelu = layers.LeakyReLU()\n        self.dropout = layers.Dropout(0.3)\n        \n    def call(self, inputs):\n        x = self.conv(inputs)\n        x = self.lrelu(x)\n        out = self.dropout(x)\n        return out","9d6ae76a":"# For the sake of understanding consider initial filters to be 32 and image height and image width to be 104 and 88 respectively \n# and follow the comments\nclass Discriminator(tf.keras.Model):\n    #                      32          5\n    def __init__(self, init_filters, kernel):\n        super(Discriminator,self).__init__()\n        self.input_layer = layers.InputLayer(input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH,3))\n        self.cnn1 = CNNBlock(init_filters, kernel, 1)\n        self.cnn2 = CNNBlock(init_filters*2, kernel, 2)\n        self.cnn3 = CNNBlock(init_filters*4, kernel, 1)\n        self.cnn4 = CNNBlock(init_filters*8, kernel, 1)\n        self.flatten = layers.Flatten()\n        self.dense = layers.Dense(1)\n        \n    def call(self, inputs):\n        # shape - (batch_size, 104, 88,3)\n        x = self.input_layer(inputs)\n        # shape - (batch_size, 104, 88,3)\n        x = self.cnn1(x)\n        # shape - (batch_size, 104, 88, 32)\n        x = self.cnn2(x)\n        # shape - (batch_size, 52, 44, 64)\n        x = self.cnn3(x)\n        # shape - (batch_size, 52, 44, 128)\n        x = self.cnn4(x)\n        # shape - (batch_size, 52, 44, 256)\n        x = self.flatten(x)\n        # shape - (batch_size, 52 * 44 * 256)\n        out = self.dense(x)\n        # shape - (batch_size, 1)\n        return out","5a0f167c":"discriminator = Discriminator(32, 5)","b136550e":"discriminator.build((BATCH_SIZE,IMAGE_HEIGHT, IMAGE_WIDTH,3))\ndiscriminator.summary()","4cd4bd0f":"#Loss functions for generator and discriminator\ncross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits = True)\n\ndef generator_loss(fake_output):\n    return cross_entropy(tf.ones_like(fake_output), fake_output)\n\ndef discriminator_loss(real_output, fake_output):\n    real_loss = cross_entropy(tf.ones_like(real_output),real_output)\n    fake_loss = cross_entropy(tf.zeros_like(fake_output),fake_output)\n    return real_loss + fake_loss","f0cefa17":"#Optimizers\ngenerator_optimizer = tf.keras.optimizers.Adam(1e-4)\ndiscriminator_optimizer = tf.keras.optimizers.Adam(1e-4)","da398965":"checkpoint_dir = '.\/training_checkpoints'\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\ncheckpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n                                 discriminator_optimizer=discriminator_optimizer,\n                                 generator=generator,\n                                 discriminator=discriminator)","b5686454":"EPOCHS = 40\nnum_examples_to_generate = 16\n\nseed = tf.random.normal([num_examples_to_generate, NOISE_DIM])","1a0b2917":"seed = tf.random.normal([num_examples_to_generate, NOISE_DIM])","d104f08b":"# This is used to generate images during training and saving them for final GIF\ndef generate_and_save_images(model, epoch,step, test_input):\n    # Notice `training` is set to False.\n    # This is so all layers run in inference mode (batchnorm).\n    predictions = model(test_input, training=False)\n\n    fig = plt.figure(figsize=(4, 4))\n\n    for i in range(predictions.shape[0]):\n        plt.subplot(4, 4, i+1)\n        image = tf.cast(predictions[i, :, :]*255.0,dtype=tf.uint8)\n        plt.imshow(image)\n        plt.axis('off')\n    \n    plt.savefig('image_at_epoch_{:04d}_step_{:06d}.png'.format(epoch, step))\n    plt.show()","3fc7b4a3":"@tf.function\ndef train_step(images):\n    noise = tf.random.normal([BATCH_SIZE,NOISE_DIM])\n    \n    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n        generated_images = generator(noise, training=True)\n        \n        fake_output = discriminator(generated_images, training=True)\n        real_output = discriminator(images, training=True)\n        \n        gen_loss = generator_loss(fake_output)\n        disc_loss = discriminator_loss(real_output, fake_output)\n        \n    gradients_of_gen = gen_tape.gradient(gen_loss, generator.trainable_variables)\n    gradients_of_disc= disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n    \n    generator_optimizer.apply_gradients(zip(gradients_of_gen, generator.trainable_variables))\n    discriminator_optimizer.apply_gradients(zip(gradients_of_disc, discriminator.trainable_variables))","3ee1c031":"def train(dataset, epochs):\n    for epoch in range(epochs):\n        start = time.time()\n        \n        for i,image_batch in enumerate(dataset):\n            train_step(image_batch)\n#             if (i%100==0):\n#                 display.clear_output(wait=True)\n#                 print(\"{}th batch done\".format(i))\n#             if (i%500==0):\n#                 display.clear_output(wait=True)\n#                 generate_and_save_images(generator, epoch, i, seed)\n        \n        display.clear_output(wait=True)\n        generate_and_save_images(generator, epoch, 9999, seed)\n        \n#         checkpoint.save(file_prefix = checkpoint_prefix)\n        generator.save_weights('generator_weights_epoch_{}'.format(epoch))\n            \n        print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n\n    display.clear_output(wait=True)\n    generate_and_save_images(generator, epoch, 55000, seed)","dc82a6a3":"# I use this section to clear the GPU memory so that I can reuse GPU again\n# !pip install GPUtil\n\n# from GPUtil import showUtilization as gpu_usage\n# gpu_usage()\n\n# import torch\n# torch.cuda.empty_cache()\n\n# gpu_usage()\n# from numba import cuda\n# cuda.select_device(0)\n# cuda.close()\n# gpu_usage()","6e8d6683":"start = time.time()\ntrain(train_ds, EPOCHS)\nprint('Total Training Time is {} sec'.format(time.time()-start))","90db47e5":"# lat = tf.train.latest_checkpoint('..\/input\/traincheckpointceleb\/')\n# ch = tf.train.Checkpoint()\n# ch.restore(lat)","04113ec6":"# # checkpoint.restore(tf.train.latest_checkpoint('..\/input\/traincheckpointceleb\/'))\n# check = checkpoint.restore(tf.train.latest_checkpoint('..\/input\/traincheckpointceleb\/'))","570d412f":"# !ls ..\/input\/training-checkpoint-celeb-dataset\/","8255becd":"def display_image(epoch_no):\n    return PIL.Image.open('image_at_epoch_{:04d}.png'.format(epoch_no))\n\nanim_file = 'dcgan.gif'\n\nwith imageio.get_writer(anim_file, mode='I') as writer:\n    filenames = glob.glob('image*.png')\n    filenames = sorted(filenames)\n    for filename in filenames:\n        image = imageio.imread(filename)\n        writer.append_data(image)\n    image = imageio.imread(filename)\n    writer.append_data(image)\n\nimport tensorflow_docs.vis.embed as embed\nembed.embed_file(anim_file)","1f0ada9c":"generator.load_weights('..\/input\/generator-weights-39\/generator_weights_epoch_39')\nrandom = tf.random.normal([1,100])\nplt.imshow(tf.reshape(random,(10,10)))\nplt.show()\nimage = generator.predict(random)\nplt.imshow(image[0])","e33aef21":"# generator.save_weights('generator_weights_checkpoint')\ndiscriminator.save_weights('discrimininator_weights_checkpoint')","35c7f721":"### Checkpoint creation\n\nDisable for now","c0f544fe":"Tensorflow's Dataset module has been used because the images are fetched on the go and the module takes care of the corner cases. The noise dimension is basically the input noise length that is fed to the generator\n\nEven though we do not need validation set actually for generation, We choose validation dataset to be 20% of the total just to keep an open option on a scenario where we might need to use the unseen dataset. Also, reducing training size also decreased training time :) ","f5d8a9ba":"## Generator and Discriminator models","59fed96f":"#### The discriminator is punished if the prediction is not near to 1 for real images and if the prediction is not 0 for fake\/generator images. This is basically to improve discriminator to learn generated images\n#### But for the generator the idea is to trick the discriminator. So for a generated image, if the descriminator predicts it to be 1 that means generator has won. So we pass the generated output through discriminator and punish the generator if the output from discriminator for the generated image is not near to 1","7b60206f":"Loading the directory of the image","2654e053":"**The batch size is chosen considering memory limitation. Batch size of more than 32 would fail to allocate necessary memory**","2d7e5fa3":"Some of the variable are used so that the code can be easily tuned if we need to conduct multiple experiments.\n\n### How did I choose the height of width of image and also the generator ?\n- Having to train on full image has better advantages in generating better faces. I did try to train on full images. But due to the memory constraints and also the limited training time, the image width and height was taken. The idea was to approximately reduce image size by half.\n- Reducing image height and width by half would give the new image shape to be (109, 89 , 3). This should have been the size of the image which it should have been resized to.\n- But I had an approximate idea that, in my generators I would upsample the initial random input 3 times. So since the height and width of the inputs double on each upsample (strides=2), 3 upsample means the image increases its height and width 8 times. So the final output of the generator should have height and width divisible by 8. Hence I chose (104,88,3) as the input shape. The height and width are the nearest multiples of 8 which are lesser than my target i.e. 109 and 89.\n- So now the generator's initial height and width should be 8 times lesser than final output i.e\n - generator_height = final_image_height\/8 = 104\/8 = 13\n - generator_width = final_image_width\/8 = 88\/8 = 11","a288637c":"# Celebrity Face Generation\n\n### This kernel uses Generative Adversarial Networks (GAN) i.e. DCGAN here to generate faces. The faces are generated using a generator and a discriminator. Both these models are written using Keras model inheritence which helps to separate out blocks and reuse the blocks in multiple places","6540938e":"## Final image generation and training GIF creation","e844ef8e":"#### Tensorflow offers configuration to prefetch images  to save image loading time. The below function optimizes the overall training performance","38be5750":"**The below function is used to fetch images while we are dynamically fetching images during training**","09d1c0a9":"## Training","fc290a75":"## Data Preprocessing\n\nHere we can a sample image to understand the images in the dataset","de827a70":"### Epochs and function for image generation","65204803":"#### This is one block of Upsampling done in the generator model. Each Conv2DTranspose layer here is upsampling by a factor of 2"}}