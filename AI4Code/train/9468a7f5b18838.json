{"cell_type":{"ced44a33":"code","6b29f9ce":"code","ae8fc847":"code","48558632":"code","adabeacd":"code","4e3c0f14":"code","d166e6c4":"code","35161383":"code","b1687f82":"code","eec06d92":"code","d6064571":"code","6d527c2a":"code","9903dc6b":"code","9ce79a02":"code","7baaa0b1":"code","936b1174":"code","e6e65640":"code","315ca3ab":"code","1a005290":"code","9c971bd0":"code","50a23ca4":"code","0f6ccc98":"code","26842180":"code","6942e9f8":"code","188fcfa4":"code","c884ba6a":"code","06834cba":"code","54c1201c":"code","0ff44941":"code","a0f3108e":"code","12f8f22e":"code","d2a6c94e":"code","03a8a27e":"code","450dc66d":"markdown","01321d09":"markdown","1fab9eba":"markdown","36aa7933":"markdown","8e682aa5":"markdown","694a884f":"markdown","8c5a3975":"markdown","52a7451e":"markdown"},"source":{"ced44a33":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6b29f9ce":"df = pd.read_csv(\"\/kaggle\/input\/pima-indians-diabetes-database\/diabetes.csv\")","ae8fc847":"df.head(10)","48558632":"from matplotlib import pyplot as plt\nimport seaborn as sns\n\n#sns.pairplot(df, hue = \"Outcome\")","adabeacd":"df.shape","4e3c0f14":"df.Outcome.value_counts()","d166e6c4":"df.describe()","35161383":"data_train = df.sample(frac=0.8, random_state=1)          # 80% des donn\u00e9es avec frac=0.8\ndata_test = df.drop(data_train.index)     # le reste des donn\u00e9es pour le test","b1687f82":"X_train = data_train.drop(['Outcome'], axis=1)\ny_train = data_train['Outcome']\nX_test = data_test.drop(['Outcome'], axis=1)\ny_test = data_test['Outcome']","eec06d92":"plt.figure(figsize=(9,9))\n\nlogistique = lambda x: np.exp(x)\/(1+np.exp(x))   \n\nx_range = np.linspace(-10,10,50)       \ny_values = logistique(x_range)\n\nplt.plot(x_range, y_values, color=\"red\")","d6064571":"from sklearn.linear_model import LogisticRegression","6d527c2a":"lr = LogisticRegression(solver='liblinear')\nlr.fit(X_train,y_train)","9903dc6b":"y_lr = lr.predict(X_test)","9ce79a02":"from sklearn.metrics import accuracy_score, confusion_matrix","7baaa0b1":"lr_score = accuracy_score(y_test, y_lr)\nprint(lr_score)","936b1174":"from sklearn import tree\ndtc = tree.DecisionTreeClassifier()\ndtc.fit(X_train,y_train)\ny_dtc = dtc.predict(X_test)\nprint(accuracy_score(y_test, y_dtc))","e6e65640":"plt.figure(figsize=(30,30))\ntree.plot_tree(dtc, feature_names=X_train.columns, class_names=['benin','malin'], fontsize=14, filled=True)  ","315ca3ab":"dtc1 = tree.DecisionTreeClassifier(max_depth = 3, min_samples_leaf = 20)\ndtc1.fit(X_train,y_train)","1a005290":"plt.figure(figsize=(30,30))\ntree.plot_tree(dtc1, feature_names=X_train.columns, class_names=['benin','malin'], fontsize=14, filled=True) ","9c971bd0":"y_dtc1 = dtc1.predict(X_test)\nprint(accuracy_score(y_test, y_dtc1))","50a23ca4":"def replace_0(df,col) :\n    df1 = df.copy()\n    n = df.shape[0]\n    m = df[col].mean()\n    s = df[col].std()\n    for i in range(len(df.index)):\n        if df.loc[i,col] == 0 :\n            df1.loc[i,col] = np.random.normal(m, s);\n    return df1","0f6ccc98":"lista = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']\nfor column in lista :\n    df = replace_0(df, column)","26842180":"data_train = df.sample(frac = 0.8, random_state = 1) # 80% des donn\u00e9es avec frac=0.8\ndata_test = df.drop(data_train.index)  ","6942e9f8":"X_train = data_train.drop(['Outcome'], axis=1)\ny_train = data_train['Outcome']\nX_test = data_test.drop(['Outcome'], axis=1)\ny_test = data_test['Outcome']","188fcfa4":"plt.figure(figsize=(9,9))\n\nlogistique = lambda x: np.exp(x)\/(1+np.exp(x))   \n\nx_range = np.linspace(-10,10,50)       \ny_values = logistique(x_range)\n\nplt.plot(x_range, y_values, color=\"red\")","c884ba6a":"lr = LogisticRegression(solver='liblinear')\nlr.fit(X_train,y_train)","06834cba":"y_lr = lr.predict(X_test)","54c1201c":"lr_score = accuracy_score(y_test, y_lr)\nprint(lr_score)","0ff44941":"from sklearn import tree\ndtc = tree.DecisionTreeClassifier()\ndtc.fit(X_train,y_train)\ny_dtc = dtc.predict(X_test)\nprint(accuracy_score(y_test, y_dtc))","a0f3108e":"plt.figure(figsize=(30,30))\ntree.plot_tree(dtc, feature_names=X_train.columns, class_names=['benin','malin'], fontsize=14, filled=True)  ","12f8f22e":"dtc1 = tree.DecisionTreeClassifier(max_depth = 3, min_samples_leaf = 20)\ndtc1.fit(X_train,y_train)","d2a6c94e":"plt.figure(figsize=(30,30))\ntree.plot_tree(dtc1, feature_names=X_train.columns, class_names=['benin','malin'], fontsize=14, filled=True) ","03a8a27e":"y_dtc1 = dtc1.predict(X_test)\nprint(accuracy_score(y_test, y_dtc1))","450dc66d":"# Machine Learning","01321d09":"# Visualisation","1fab9eba":"# Arbres de d\u00e9cision","36aa7933":"# Score et matrice de confusion","8e682aa5":"# R\u00e9gression logistique","694a884f":"# Correction","8c5a3975":"# R\u00e9gression logistique","52a7451e":"# Arbres de d\u00e9cision"}}