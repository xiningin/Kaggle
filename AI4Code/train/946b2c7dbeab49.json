{"cell_type":{"bdced2e1":"code","c56d9003":"code","13789eff":"code","3c4ff800":"code","1e14a1fc":"code","de960ecc":"code","c9b86b4d":"code","e1abefee":"code","0b05b1cf":"code","fa9b1d06":"code","94ac8272":"code","d00fe137":"code","223493cc":"code","0fb72afb":"code","f1308c56":"code","8862ff97":"code","2cfb3ac2":"code","112bee92":"code","6e71fae0":"markdown","1b6d265a":"markdown","251cc4ac":"markdown","54f258d7":"markdown","aaabb57d":"markdown","4342ea09":"markdown","cb3cbf80":"markdown","efe5fe81":"markdown","194ca218":"markdown","c9b8d12c":"markdown","9af38e30":"markdown","61938141":"markdown","09f3d7e9":"markdown","dd5f3d21":"markdown","0574be0d":"markdown","1e42fd8c":"markdown","80e078be":"markdown","34561bb9":"markdown","b8e623b9":"markdown","82b484ed":"markdown","797d8ebd":"markdown"},"source":{"bdced2e1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c56d9003":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt      \n%matplotlib inline \nimport seaborn as sns","13789eff":"df = pd.read_csv('\/kaggle\/input\/pima-indians-diabetes-database\/diabetes.csv')\ndf.shape","3c4ff800":"df.head()","1e14a1fc":" # Check if there are any null values in data set\n\ndf.isnull().values.any()","de960ecc":"columns = list(df)[0:-1] \ndf[columns].hist(stacked=False, bins=100, figsize=(12,30), layout=(14,2)); ","c9b86b4d":"df.corr()","e1abefee":"def plot_corr(df, size=11):\n    corr = df.corr()\n    fig, ax = plt.subplots(figsize=(size, size))\n    ax.matshow(corr)\n    plt.xticks(range(len(corr.columns)), corr.columns)\n    plt.yticks(range(len(corr.columns)), corr.columns)\n    \nplot_corr(df)","0b05b1cf":"sns.pairplot(df,diag_kind='kde')","fa9b1d06":"n_true = len(df.loc[df['Outcome'] == True])\nn_false = len(df.loc[df['Outcome'] == False])\nprint(\"Number of true cases: {0} ({1:2.2f}%)\".format(n_true, (n_true \/ (n_true + n_false)) * 100 ))\nprint(\"Number of false cases: {0} ({1:2.2f}%)\".format(n_false, (n_false \/ (n_true + n_false)) * 100))","94ac8272":"from sklearn.model_selection import train_test_split\n\nX = df.drop('Outcome',axis=1)  # Predictor feature columns (8 X m)\nY = df['Outcome']              # Predicted class (1=True, 0=False) (1 X m)\n\nx_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=1)\n# 1 is just any random seed number\n\nx_train.head()","d00fe137":"print(\"{0:0.2f}% data is in training set\".format((len(x_train)\/len(df.index)) * 100))\nprint(\"{0:0.2f}% data is in test set\".format((len(x_test)\/len(df.index)) * 100))","223493cc":"print(\"Original Diabetes True Values    : {0} ({1:0.2f}%)\".format(len(df.loc[df['Outcome'] == 1]), (len(df.loc[df['Outcome'] == 1])\/len(df.index)) * 100))\nprint(\"Original Diabetes False Values   : {0} ({1:0.2f}%)\".format(len(df.loc[df['Outcome'] == 0]), (len(df.loc[df['Outcome'] == 0])\/len(df.index)) * 100))\nprint(\"\")\nprint(\"Training Diabetes True Values    : {0} ({1:0.2f}%)\".format(len(y_train[y_train[:] == 1]), (len(y_train[y_train[:] == 1])\/len(y_train)) * 100))\nprint(\"Training Diabetes False Values   : {0} ({1:0.2f}%)\".format(len(y_train[y_train[:] == 0]), (len(y_train[y_train[:] == 0])\/len(y_train)) * 100))\nprint(\"\")\nprint(\"Test Diabetes True Values        : {0} ({1:0.2f}%)\".format(len(y_test[y_test[:] == 1]), (len(y_test[y_test[:] == 1])\/len(y_test)) * 100))\nprint(\"Test Diabetes False Values       : {0} ({1:0.2f}%)\".format(len(y_test[y_test[:] == 0]), (len(y_test[y_test[:] == 0])\/len(y_test)) * 100))\nprint(\"\")","0fb72afb":"x_train.head()","f1308c56":"from sklearn.impute import SimpleImputer\nrep_0 = SimpleImputer(missing_values=0, strategy=\"mean\")\ncols=x_train.columns\nx_train = pd.DataFrame(rep_0.fit_transform(x_train))\nx_test = pd.DataFrame(rep_0.fit_transform(x_test))\n\nx_train.columns = cols\nx_test.columns = cols\n\nx_train.head()","8862ff97":"from sklearn import metrics\n\nfrom sklearn.linear_model import LogisticRegression\n\n# Fit the model on train\nmodel = LogisticRegression(solver=\"liblinear\")\nmodel.fit(x_train, y_train)\n#predict on test\ny_predict = model.predict(x_test)\n\n\ncoef_df = pd.DataFrame(model.coef_)\ncoef_df['intercept'] = model.intercept_\nprint(coef_df)","2cfb3ac2":"model_score = model.score(x_test, y_test)\nprint(model_score)","112bee92":"cm = metrics.confusion_matrix(y_test, y_predict, labels=[1, 0])\n\ndf_cm = pd.DataFrame(cm, index = [i for i in [\"1\",\"0\"]],\n                  columns = [i for i in [\"Predict 1\",\"Predict 0\"]])\nplt.figure(figsize = (7,5))\nsns.heatmap(df_cm, annot=True)","6e71fae0":"\n## Evaluation :-\n\n* True Positives (TP): we correctly predicted that no of people have diabetes = 48\n\n* True Negatives (TN): we correctly predicted that no of people who don't have diabetes = 132\n\n* False Positives (FP): we incorrectly predicted that no of people who do have diabetes (a \"Type I error\"). 14 Falsely predict positive, Type I error\n\n* False Negatives (FN): we incorrectly predicted that no of people who don't have diabetes (a \"Type II error\"). 37 Falsely predict negative Type II error\n\n\u200b","1b6d265a":"## Logistic Regression Model","251cc4ac":"## Lets check split of data","54f258d7":"# Logistic Regression Model \n\nWe will use Logistic Regression, to model the \"Pima Indians Diabetes\" data set. This model will predict which people are likely to develop diabetes.\n\n\nThe objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage.","aaabb57d":"## Spliting the data\n* ### We will use 70% of data for training and 30% for testing.","4342ea09":"## Data Preparation\n\n* As we checked missing values earlier but haven't got any. But there can be lots of entries with 0 values. We must need to take care of those as well.","cb3cbf80":"#### We can see lots of entries with 0 value above.","efe5fe81":"## Calculate diabetes ratio of True\/False from outcome variable","194ca218":"### Model Score ","c9b8d12c":"## Load and Review Data","9af38e30":"### However we can plot correlation in graphical representation so below is a function for that","61938141":"## Plotting Histogram for first 8 columns. Excluding the outcome column.","09f3d7e9":"# Though I am a beginner in Kaggle any comment or tip is helpful. If you like my kernel give me an upvote.","dd5f3d21":"## Now lets check diabetes True\/False ratio in split data\n\n","0574be0d":"#### So we have 34.90% people in current data set who have diabetes and rest of 65.10% doesn't have diabetes. Its a good distribution True\/False cases of diabetes in data.","1e42fd8c":"## Import Libraries ","80e078be":"## The Confusion Matrix","34561bb9":"## BiVariate Plots","b8e623b9":"   ## Identify Correlation in data","82b484ed":"#### In above plot yellow colour represents maximum correlation and blue colour represents minimum correlation.","797d8ebd":"## Replace 0s with serial mean"}}