{"cell_type":{"8bbd23f6":"code","e81b057e":"code","6579fb75":"code","6df7e920":"code","269fc4ed":"code","a8357ef6":"code","a5d5e8ad":"code","22e2b3b7":"code","0519aeac":"code","053c08bf":"code","328a3e2e":"code","7ccc9b35":"code","2e76fc6d":"code","3b64ffe2":"code","d530315b":"code","827b4707":"code","c66b9b51":"code","71237e09":"code","8eae191c":"code","857d7642":"code","14b4831e":"code","4a5fff50":"code","2e2448ac":"code","4c275653":"code","2de4452a":"code","515a6751":"code","8f478af5":"code","d36033da":"code","c41ffd58":"code","ac34f794":"code","848ac03d":"code","74fc4d55":"code","ef976a23":"code","974c0631":"code","8b4ca54d":"code","d0fccb28":"code","9544b11a":"code","c3ff431c":"code","99488299":"code","b339bf87":"code","6c959efc":"code","837d2bf7":"code","92ae409a":"code","b4c94326":"code","22b56575":"code","043824b5":"code","dca0a121":"code","5f41ce87":"code","19c5804a":"code","5ee09592":"code","8815ddbd":"code","f45f5d52":"code","e9bec115":"code","6e2afc7d":"code","48d54b06":"code","e7c8ea93":"code","eddaaac7":"code","f0d047cf":"code","01827c37":"code","6fbe4e69":"markdown","1307d8a8":"markdown","5d20a7b7":"markdown","47cc024a":"markdown","2e70a878":"markdown","04e26962":"markdown","aa0617e7":"markdown","559ae44f":"markdown","8a40e283":"markdown","4364ae4f":"markdown","13797f6f":"markdown","b9acf2a3":"markdown","af45975b":"markdown","d2551cb0":"markdown","85540e8f":"markdown","d15fcc66":"markdown","96b595ae":"markdown","d0586b0b":"markdown","562db728":"markdown","299817a9":"markdown","d8d2e7cd":"markdown","845c3259":"markdown","810bda4b":"markdown","b4a5ee42":"markdown","3fab17a6":"markdown","94f3084f":"markdown","89759823":"markdown","a316cfed":"markdown","4967b713":"markdown","1fcd8707":"markdown","c80455b5":"markdown","4ef8e32e":"markdown","b94e677c":"markdown"},"source":{"8bbd23f6":"# Common imports\nimport sys\nimport sklearn\nimport numpy as np\nimport os\n\n# To plot pretty figures\n%matplotlib inline\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nmpl.rc('axes', labelsize=14)\nmpl.rc('xtick', labelsize=12)\nmpl.rc('ytick', labelsize=12)\n\n# Where to save the figures\nPROJECT_ROOT_DIR = \".\"\nCHAPTER_ID = \"end_to_end_project\"\nIMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\nos.makedirs(IMAGES_PATH, exist_ok=True)\n\ndef save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n    print(\"Saving figure\", fig_id)\n    if tight_layout:\n        plt.tight_layout()\n    plt.savefig(path, format=fig_extension, dpi=resolution)\n\n# Ignore useless warnings (see SciPy issue #5998)\nimport warnings\nwarnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")","e81b057e":"import os\nimport tarfile\nimport urllib","6579fb75":"import pandas as pd\n\nhousing_path = \"\/kaggle\/input\/california-housing-prices\/housing.csv\"\n\nhousing = pd.read_csv(housing_path)","6df7e920":"housing.head()","269fc4ed":"housing.info()","a8357ef6":"housing[\"ocean_proximity\"].value_counts()","a5d5e8ad":"housing.describe()","22e2b3b7":"%matplotlib inline\nimport matplotlib.pyplot as plt\nhousing.hist(bins=50, figsize=(20,15))\nsave_fig(\"attribute_histogram_plots\")\nplt.show()","0519aeac":"from sklearn.model_selection import train_test_split\ntrain_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)","053c08bf":"test_set.head()","328a3e2e":"housing[\"median_income\"].hist()","7ccc9b35":"housing[\"income_cat\"] = pd.cut(housing[\"median_income\"],\n                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n                               labels=[1, 2, 3, 4, 5])","2e76fc6d":"housing[\"income_cat\"].value_counts()","3b64ffe2":"housing[\"income_cat\"].hist()","d530315b":"from sklearn.model_selection import StratifiedShuffleSplit\n\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\nfor train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n    strat_train_set = housing.loc[train_index]\n    strat_test_set = housing.loc[test_index]","827b4707":"\nstrat_test_set[\"income_cat\"].value_counts() \/ len(strat_test_set)","c66b9b51":"housing[\"income_cat\"].value_counts() \/ len(housing)","71237e09":"def income_cat_proportions(data):\n    return data[\"income_cat\"].value_counts() \/ len(data)\n\ntrain_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\n\ncompare_props = pd.DataFrame({\n    \"Overall\": income_cat_proportions(housing),\n    \"Stratified\": income_cat_proportions(strat_test_set),\n    \"Random\": income_cat_proportions(test_set),\n}).sort_index()\ncompare_props[\"Rand. %error\"] = 100 * compare_props[\"Random\"] \/ compare_props[\"Overall\"] - 100\ncompare_props[\"Strat. %error\"] = 100 * compare_props[\"Stratified\"] \/ compare_props[\"Overall\"] - 100","8eae191c":"compare_props","857d7642":"for set_ in (strat_train_set, strat_test_set):\n    set_.drop(\"income_cat\", axis=1, inplace=True)","14b4831e":"# create a copy of our training set so we can play with it without harming it\n","4a5fff50":"housing = strat_train_set.copy()","2e2448ac":"# visualize the high-density areas in California\nhousing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.1)\nsave_fig(\"better_visualization_plot\")","4c275653":"# let's visualize the housing prices\n## the radius of each circle represents the district's population the color reprents the price\n## color map ranges from blue (low values) to red (high prices)\nhousing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4,\n    s=housing[\"population\"]\/100, label=\"population\", figsize=(10,7),\n    c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"), colorbar=True,\n    sharex=False)\nplt.legend()\nsave_fig(\"housing_prices_scatterplot\")","2de4452a":"# Download the California image\nimages_path = os.path.join(PROJECT_ROOT_DIR, \"images\", \"end_to_end_project\")\nos.makedirs(images_path, exist_ok=True)\nDOWNLOAD_ROOT = \"https:\/\/raw.githubusercontent.com\/ageron\/handson-ml2\/master\/\"\nfilename = \"california.png\"\nprint(\"Downloading\", filename)\nurl = DOWNLOAD_ROOT + \"images\/end_to_end_project\/\" + filename\nurllib.request.urlretrieve(url, os.path.join(images_path, filename))","515a6751":"import matplotlib.image as mpimg\ncalifornia_img=mpimg.imread(os.path.join(images_path, filename))\nax = housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", figsize=(10,7),\n                       s=housing['population']\/100, label=\"Population\",\n                       c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"),\n                       colorbar=False, alpha=0.4,\n                      )\nplt.imshow(california_img, extent=[-124.55, -113.80, 32.45, 42.05], alpha=0.5,\n           cmap=plt.get_cmap(\"jet\"))\nplt.ylabel(\"Latitude\", fontsize=14)\nplt.xlabel(\"Longitude\", fontsize=14)\n\nprices = housing[\"median_house_value\"]\ntick_values = np.linspace(prices.min(), prices.max(), 11)\ncbar = plt.colorbar(ticks=tick_values\/prices.max())\ncbar.ax.set_yticklabels([\"$%dk\"%(round(v\/1000)) for v in tick_values], fontsize=14)\ncbar.set_label('Median House Value', fontsize=16)\n\nplt.legend(fontsize=16)\nsave_fig(\"california_housing_prices_plot\")\nplt.show()","8f478af5":"corr_matrix = housing.corr()","d36033da":"corr_matrix[\"median_house_value\"].sort_values(ascending=False)","c41ffd58":"housing[\"rooms_per_household\"] = housing[\"total_rooms\"]\/housing[\"households\"]\nhousing[\"bedrooms_per_room\"] = housing[\"total_bedrooms\"]\/housing[\"total_rooms\"]\nhousing[\"population_per_household\"]=housing[\"population\"]\/housing[\"households\"]","ac34f794":"corr_matrix = housing.corr()\ncorr_matrix[\"median_house_value\"].sort_values(ascending=False)","848ac03d":"housing = strat_train_set.drop(\"median_house_value\", axis=1) # drop labels for training set\nhousing_labels = strat_train_set[\"median_house_value\"].copy()","74fc4d55":"# Scikit-Learn provides a handy class to take care of missing values\nfrom sklearn.impute import SimpleImputer\nimputer = SimpleImputer(strategy=\"median\")","ef976a23":"# Remove the text attribute because median can only be calculated on numerical attributes\nhousing_num = housing.select_dtypes(include=[np.number])\nimputer.fit(housing_num)\nimputer.statistics_","974c0631":"# Transform the training set and set it back into a pandas DataFrame\nX = imputer.transform(housing_num)\nhousing_tr = pd.DataFrame(X, columns=housing_num.columns,index=housing.index)","8b4ca54d":"# let's preprocess the categorical input feature\nhousing_cat = housing[[\"ocean_proximity\"]]\nhousing_cat.head(10)","d0fccb28":"from sklearn.preprocessing import OrdinalEncoder\n\nordinal_encoder = OrdinalEncoder()\nhousing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)\nhousing_cat_encoded[:10]","9544b11a":"# we can get a list of categories\nordinal_encoder.categories_","c3ff431c":"from sklearn.preprocessing import OneHotEncoder\n\ncat_encoder = OneHotEncoder(sparse=False)\nhousing_cat_1hot = cat_encoder.fit_transform(housing_cat)\nhousing_cat_1hot","99488299":"cat_encoder.categories_","b339bf87":"# Let's create a custom transformer to add extra attributes\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\n# column index\nrooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6\n\nclass CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n    def __init__(self, add_bedrooms_per_room=True): # no *args or **kargs\n        self.add_bedrooms_per_room = add_bedrooms_per_room\n    def fit(self, X, y=None):\n        return self  # nothing else to do\n    def transform(self, X):\n        rooms_per_household = X[:, rooms_ix] \/ X[:, households_ix]\n        population_per_household = X[:, population_ix] \/ X[:, households_ix]\n        if self.add_bedrooms_per_room:\n            bedrooms_per_room = X[:, bedrooms_ix] \/ X[:, rooms_ix]\n            return np.c_[X, rooms_per_household, population_per_household,\n                         bedrooms_per_room]\n        else:\n            return np.c_[X, rooms_per_household, population_per_household]\n\nattr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)\nhousing_extra_attribs = attr_adder.transform(housing.values)","6c959efc":"col_names = \"total_rooms\", \"total_bedrooms\", \"population\", \"households\"\nrooms_ix, bedrooms_ix, population_ix, households_ix = [\n    housing.columns.get_loc(c) for c in col_names] # get the column indices","837d2bf7":"housing_extra_attribs = pd.DataFrame(\n    housing_extra_attribs,\n    columns=list(housing.columns)+[\"rooms_per_household\", \"population_per_household\"],\n    index=housing.index)\nhousing_extra_attribs.head()","92ae409a":"# let's build a pipeline for preprocessing the numerical attributes\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nnum_pipeline = Pipeline([\n        ('imputer', SimpleImputer(strategy=\"median\")),\n        ('attribs_adder', CombinedAttributesAdder()),\n        ('std_scaler', StandardScaler()),\n    ])\n\nhousing_num_tr = num_pipeline.fit_transform(housing_num)","b4c94326":"housing_num_tr","22b56575":"# we can build a pipeline for both numerical and categorical attributes preprocessing\nfrom sklearn.compose import ColumnTransformer\n\nnum_attribs = list(housing_num)\ncat_attribs = [\"ocean_proximity\"]\n\nfull_pipeline = ColumnTransformer([\n        (\"num\", num_pipeline, num_attribs),\n        (\"cat\", OneHotEncoder(), cat_attribs),\n    ])\n\nhousing_prepared = full_pipeline.fit_transform(housing)\n\nhousing_prepared ","043824b5":"housing_prepared.shape","dca0a121":"from sklearn.linear_model import LinearRegression\n\nlin_reg = LinearRegression()\nlin_reg.fit(housing_prepared, housing_labels)","5f41ce87":"# Measure the RMSE on the whole training set\nfrom sklearn.metrics import mean_squared_error\n\nhousing_predictions = lin_reg.predict(housing_prepared)\nlin_mse = mean_squared_error(housing_labels, housing_predictions)\nlin_rmse = np.sqrt(lin_mse)\nlin_rmse","19c5804a":"# Define and fit the model\nfrom sklearn.tree import DecisionTreeRegressor\n\ntree_reg = DecisionTreeRegressor(random_state=42)\ntree_reg.fit(housing_prepared, housing_labels)","5ee09592":"housing_predictions = tree_reg.predict(housing_prepared)\ntree_mse = mean_squared_error(housing_labels, housing_predictions)\ntree_rmse = np.sqrt(tree_mse)\ntree_rmse","8815ddbd":"from sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(tree_reg, housing_prepared, housing_labels,\n                         scoring=\"neg_mean_squared_error\", cv=10)\ntree_rmse_scores = np.sqrt(-scores)","f45f5d52":"def display_scores(scores):\n    print(\"Scores:\", scores)\n    print(\"Mean:\", scores.mean())\n    print(\"Standard deviation:\", scores.std())\n\ndisplay_scores(tree_rmse_scores)","e9bec115":"lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels,\n                             scoring=\"neg_mean_squared_error\", cv=10)\nlin_rmse_scores = np.sqrt(-lin_scores)\ndisplay_scores(lin_rmse_scores)","6e2afc7d":"# define and fit the model\nfrom sklearn.ensemble import RandomForestRegressor\n\nforest_reg = RandomForestRegressor(n_estimators=100, random_state=42)\nforest_reg.fit(housing_prepared, housing_labels)","48d54b06":"housing_predictions = forest_reg.predict(housing_prepared)\nforest_mse = mean_squared_error(housing_labels, housing_predictions)\nforest_rmse = np.sqrt(forest_mse)\nforest_rmse","e7c8ea93":"# cross validation score\n\nfrom sklearn.model_selection import cross_val_score\n\nforest_scores = cross_val_score(forest_reg, housing_prepared, housing_labels,\n                                scoring=\"neg_mean_squared_error\", cv=10)\nforest_rmse_scores = np.sqrt(-forest_scores)\ndisplay_scores(forest_rmse_scores)","eddaaac7":"## 5. Evaluate the system on the Test Set","f0d047cf":"X_test = strat_test_set.drop(\"median_house_value\", axis=1)\ny_test = strat_test_set[\"median_house_value\"].copy()\n\nX_test_prepared = full_pipeline.transform(X_test)\nfinal_predictions = forest_reg.predict(X_test_prepared)\n\nfinal_mse = mean_squared_error(y_test, final_predictions)\nfinal_rmse = np.sqrt(final_mse)","01827c37":"from scipy import stats\n\nconfidence = 0.95\nsquared_errors = (final_predictions - y_test) ** 2\nnp.sqrt(stats.t.interval(confidence, len(squared_errors) - 1,\n                         loc=squared_errors.mean(),\n                         scale=stats.sem(squared_errors)))","6fbe4e69":"## 3. Prepare the data for Machine Learning algorithms","1307d8a8":"But to avoid that the algorithms assume that two nearby values are more similar than two distant ones, we will instead create dummy values with scikit-learn's tools OneHotEncoder.","5d20a7b7":"That's right, the Decision Tree model is overfitting so badly that it performs worst than the Linear Regression model.","47cc024a":"### Looking for Correlations","2e70a878":"This step allowed us to estimate the performance of our decision tree model and see that it actually doesn't seem more accurate than the Linear Regression model, but we can compute the same scores to be sure.","04e26962":"### Decision Tree Regressor","aa0617e7":"## 0. Setup","559ae44f":"First, let's revert to a clean training set and let's also separate the predictors and the labels.","8a40e283":"## 2. Discover and visualize the data to gain insights","4364ae4f":"### Data Cleaning","13797f6f":"Another quick way to get a feel of the type of data you are dealing with is to plot a histogram for each numerical attribute.","b9acf2a3":"We will try three models; Linear Regression, Decision Tree Regressor and Random Forest Regressor.","af45975b":"This project is part of the book Hands-On-Machine Learning with Scikit-Learn, Keras & TensorFlow by Aur\u00e9lien G\u00e9ron.","d2551cb0":"Machine learning algorithms don't perform well when the input numerical attributes have very different scales, we will fix this as we create pipelines with the past steps to facilitate future analysis.","85540e8f":"Let's look at the other fields. The describe() method shows a summary of the numerical attributes.","d15fcc66":"### Random Forest Regressor","96b595ae":"Each row represents one district. There are 10 attributes: longitude, latitude, housing_median_age, total_rooms, total_bedrooms, population, household, median_income, median_house_value, and ocean_proximity.\n\nThe info() method is useful to get a quick description of the data, in particular the total number of rows, each attribute's type, and the number of nonull values.","d0586b0b":"Making the assumptionthat the median income is a very important attribute to predict median housing prices, we want to ensure that the test set is representative of the various categories of incomes in the whole dataset.","562db728":"## 1. Get the data","299817a9":"There are 20,640 instances in the dataset. Notice that the total_bedrooms attribute has only 20.433 nonnull values, meaning that 207 districts are missing this feature.\n\nAll attributes are numerical, except ocean_proximity field. Its type is object, so it could hold any kind of Python object, but as we saw earlier, it's a text attribute, and since it's repeated values, it might be it's probably a categorical attribute. We can find out which ones using the value_counts() method:","d8d2e7cd":"From this data we can see that the most promising attribute to predict the median house value is the median income.","845c3259":"### Experiment with Attribute Combinations","810bda4b":"It's impossible there's no error, it's very likely that the model has badly overfit the data. We will evaluate this assumption with a cross-validations","b4a5ee42":"There are a few things you might notice in these histograms:\n* The median income attribute, the housing median age and the median house value, has been scaled and capped at the highest and lowest value.\n* These attributes have very different scales.\n* Many histograms are tail-heavy.","3fab17a6":"## 4. Select and train a model","94f3084f":"### Create a Test Set","89759823":"Since the dataset is not too large, we can easily compute the standard correlation coefficient between every pair of attributes:","a316cfed":"# California Housing Prices Prediction","4967b713":"Now we need to deal with the text attributes; the ocean_proximity feature, which is a categorical variable with text attributes. Most machine learning algorithms prefer to work with numbers, so let's convert these categories from text to numbers by using the scikit-learn's OrdinalEncoder class.","1fcd8707":"### Linear Regressor","c80455b5":"Most Machine Learning algorithms cannot work with missing features, so let's create a new function to take care of them.","4ef8e32e":"### Feature scaling","b94e677c":"Random Forests look very promising. However, the score on the training set is still much lower than on the validation sets, meaning that the model is still overfitting the training set. Possible solutions would be to simplify the model, constrain it, or get a lot more training data. But we will leave that for future Mar."}}