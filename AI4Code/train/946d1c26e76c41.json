{"cell_type":{"da22db21":"code","30effa68":"code","01fbf20a":"code","94432567":"code","034891c9":"code","818de7cb":"code","cc1737e1":"code","aca933e4":"code","e03191df":"code","326951bf":"code","7cfcefef":"code","19f3a7ac":"code","c3688628":"code","119f4dfd":"code","6aa037a8":"code","2267851d":"code","2e660ef7":"code","ba20c5a0":"code","974d1181":"code","9dd3e242":"code","cacc82a4":"code","921d1a15":"code","230483a1":"code","51616ddf":"code","151544bb":"code","7d84b009":"code","7fbefc40":"code","620fe3f5":"code","02966299":"code","539c5b88":"code","574d2b1e":"code","d3f9145e":"code","f2f941bd":"code","375e58f1":"code","0f217931":"code","b5fc1f1d":"code","3b10cc9b":"code","bf47665a":"code","d28600b9":"code","fa421f8f":"code","21e64552":"code","d7118c73":"code","4b165d05":"code","f75e6791":"code","e7c35db2":"code","ae31b623":"code","7d9587e2":"code","2f1197f5":"markdown","f0196d3c":"markdown","c445551b":"markdown","a85f0cd4":"markdown","724c798e":"markdown","5dec7a27":"markdown","7f23b03e":"markdown","36a5d1b0":"markdown","401c5bef":"markdown","0a637cab":"markdown","91094a22":"markdown","78221b5c":"markdown","80099c67":"markdown","0fba403b":"markdown","83ee9995":"markdown","fbf35981":"markdown","63247b5a":"markdown","9e1621b4":"markdown","01d29bc5":"markdown","48ac622d":"markdown","8ff16323":"markdown","538ec264":"markdown","472b0c86":"markdown","951406a7":"markdown"},"source":{"da22db21":"#importing the libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","30effa68":"#importing the train, test and submission data files\ntrain_data = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_data = pd.read_csv('..\/input\/titanic\/test.csv')\nsub = pd.read_csv('..\/input\/titanic\/gender_submission.csv')\n\ntrain_data.head()","01fbf20a":"test_data.head()","94432567":"sub.head()","034891c9":"#making a copy of train and test data \ntrain = train_data.copy()\ntest = test_data.copy()","818de7cb":"train_data.info() #quick review of how many and what type of data is present in our train data","cc1737e1":"train_data.isnull().sum() #checking for total null values present in the train data","aca933e4":"train = train_data[['PassengerId','Pclass','Sex','Age','SibSp','Parch','Embarked','Survived']]\ntrain","e03191df":"sns.countplot(train['Pclass']);","326951bf":"print('Number of passengers in class 1, 2 and 3:')\nprint(train['Pclass'].value_counts())","7cfcefef":"sns.countplot(train['Sex']);","19f3a7ac":"print('Number of passengers with respect to gender:')\nprint(train['Sex'].value_counts())","c3688628":"a = train['Age'].value_counts()\nage = pd.DataFrame(a)\n\nage['Count'] = age['Age']\nage['Age'] = age.index\n\n\n\nsns.distplot(age);","119f4dfd":"train.groupby('Sex').Age.plot(kind='kde');","6aa037a8":"plt.hist(age.Age, bins = range(0, 80, 5))\nplt.show()","2267851d":"train_a = train.groupby([\"Age\"]).size().reset_index(name='Count')\n\ny= train_a['Count']\nx= train_a['Age']\n\nplt.bar(x, y);","2e660ef7":"print('Maximum Age of Passenger: ',age['Age'].max())\nprint('Minimum Age of Passenger: ',age['Age'].min())","ba20c5a0":"sns.countplot(train['SibSp']);","974d1181":"print('Distribuition of total number of sublings or spouses a passenger has on board titanic:')\nprint(train['SibSp'].value_counts())","9dd3e242":"sns.countplot(train['Parch']);","cacc82a4":"print('Distribuition of total number of parents or children a passenger has on board titanic:')\nprint(train['Parch'].value_counts())","921d1a15":"sns.countplot(train['Embarked']);","230483a1":"print('Distribution of total number of passengers embarked from C = Cherbourg, Q = Queenstown, S = Southampton')\nprint(train['Embarked'].value_counts())","51616ddf":"Pclass_Sur = sns.catplot(x='Pclass', y='Survived', hue='Sex', data=train, height=6, kind='bar', palette='summer')\nPclass_Sur.set_ylabels(\"survival probability\");","151544bb":"SibSp_Sur = sns.catplot(x='SibSp', y='Survived', hue='Sex', data=train, height=6, kind='bar', palette='autumn')\nSibSp_Sur.set_ylabels(\"survival probability\");","7d84b009":"Parch_Sur = sns.catplot(x='Parch', y='Survived', hue='Sex', data=train, height=6, kind='bar', palette='autumn')\nParch_Sur.set_ylabels(\"survival probability\");","7fbefc40":"Embark_Sur = sns.catplot(x='Embarked', y='Survived', hue='Sex', data=train, height=6, kind='bar', palette='spring')\nEmbark_Sur.set_ylabels(\"survival probability\");","620fe3f5":"#Distributing age into different age groups\nbins = [ 0, 5, 12, 18, 24, 35, 60, np.inf]\nlabel = ['Infants','Children','Teens','Young Adults','Adults', 'Old', ' Very Old']\ntrain['Age Groups'] = pd.cut(train[\"Age\"], bins, labels = label)\n\nAge_Sur = sns.catplot(x='Age Groups', y='Survived', hue='Sex', data=train, height=6, kind='bar', palette='spring')\nAge_Sur.set_ylabels(\"survival probability\");","02966299":"#numeric features in the data\nnumeric_attributes = train.select_dtypes(include = [np.number])\nnumeric_attributes","539c5b88":"#categorical features in the data\ncat_attributes = train.select_dtypes(include = object)\ncat_attributes","574d2b1e":"train_data.drop(['PassengerId','Name','Cabin', 'Ticket', 'Fare'],axis = 1, inplace = True)\n","d3f9145e":"#replacing missing values with mode\nmode = train_data['Embarked'].mode()\n#mode = 'S'\ntrain_data['Embarked'] = train_data['Embarked'].fillna(mode) \n\n#converting categorical values to numerical using one hot encoding\nimport category_encoders as ce\nencoder=ce.OneHotEncoder(cols=['Sex','Embarked'],handle_unknown='return_nan',return_df=True,use_cat_names=True)\ntrain_data = encoder.fit_transform(train_data)\nprint('Embarked Column divided into Embarked_S, Embarked_C, Embarked_Q after performing One Hot Encoding:')\nprint(train_data)\n\n# replacing missing values with using IterativeImputer\nfrom sklearn.experimental import enable_iterative_imputer \nfrom sklearn.impute import IterativeImputer\n\nage = pd.DataFrame(train_data['Age'])\nage = IterativeImputer().fit_transform(age)\ntrain_data['Age'] = age\n\n\n#scaling data\nfrom sklearn.preprocessing import StandardScaler\n\nscale = StandardScaler().fit(train_data[['Age','Pclass','SibSp','Parch','Survived']])\ntrain_data[['Age','Pclass','SibSp','Parch','Survived']] = scale.transform(train_data[['Age','Pclass','SibSp','Parch','Survived']])\n","f2f941bd":"test_data.drop(['PassengerId','Name','Cabin', 'Ticket', 'Fare'],axis = 1, inplace = True)\n\n#replacing missing values with mode\nmode = 'S'\ntest_data['Embarked'] = test_data['Embarked'].fillna(mode)\n\n#converting categorical values to numerical using one hot encoding\nimport category_encoders as ce\n\nencoder=ce.OneHotEncoder(cols=['Sex','Embarked'],handle_unknown='return_nan',return_df=True,use_cat_names=True)\ntest_data = encoder.fit_transform(test_data)\n\n# replacing missing values with using IterativeImputer\nfrom sklearn.experimental import enable_iterative_imputer \nfrom sklearn.impute import IterativeImputer\n\nage = pd.DataFrame(test_data['Age'])\nage = IterativeImputer().fit_transform(age)\ntest_data['Age'] = age\n\n#scaling data\nfrom sklearn.preprocessing import StandardScaler\n\nscale = StandardScaler().fit(train_data[['Age','Pclass','SibSp','Parch']])\ntest_data[['Age','Pclass','SibSp','Parch']] = scale.transform(test_data[['Age','Pclass','SibSp','Parch']])","375e58f1":"from sklearn.model_selection import train_test_split\n\nfeatures = train_data.drop('Survived',1) #features that will support in predicting accurate values\ntarget = train_data['Survived'] #target column \n\nx_train, x_test, y_train, y_test = train_test_split(features, target, test_size = 0.20, random_state = 0)","0f217931":"#importing machine learning models\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom xgboost import XGBClassifier","b5fc1f1d":"logreg = LogisticRegression()\nlogreg.fit(x_train.astype('int'), y_train.astype('int'))\n\ny_pred = logreg.predict(x_test.astype('int'))\n\nacc_logreg = round(logreg.score(x_test.astype('int'), y_test.astype('int')) * 100, 2)\n\nprint(acc_logreg)","3b10cc9b":"sgd = linear_model.SGDClassifier(max_iter=5, tol=None)\nsgd.fit(x_train.astype('int'), y_train.astype('int'))\n\ny_pred = sgd.predict(x_test.astype('int'))\n\nacc_sgd = round(sgd.score(x_test.astype('int'), y_test.astype('int')) * 100, 2)\n\nprint(acc_sgd)","bf47665a":"random_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(x_train.astype('int'), y_train.astype('int'))\n\ny_pred = random_forest.predict(x_test.astype('int'))\n\nacc_random_forest = round(random_forest.score(x_test.astype('int'), y_test.astype('int')) * 100, 2)\n\nprint(acc_random_forest)","d28600b9":"knn = KNeighborsClassifier(n_neighbors = 3) \nknn.fit(x_train.astype('int'), y_train.astype('int'))  \ny_pred = knn.predict(x_test.astype('int')) \nacc_knn = round(knn.score(x_test.astype('int'), y_test.astype('int')) * 100, 2)\nprint(acc_knn)","fa421f8f":"gaussian = GaussianNB() \ngaussian.fit(x_train.astype('int'), y_train.astype('int'))  \ny_pred = gaussian.predict(x_test.astype('int'))  \nacc_gaussian = round(gaussian.score(x_test.astype('int'), y_test.astype('int')) * 100,2)\nprint(acc_gaussian)\n","21e64552":"perceptron = Perceptron(max_iter=5)\nperceptron.fit(x_train.astype('int'), y_train.astype('int'))\n\ny_pred = perceptron.predict(x_test.astype('int'))\n\nacc_perceptron = round(perceptron.score(x_test.astype('int'), y_test.astype('int')) * 100, 2)\nprint(acc_perceptron)","d7118c73":"linear_svc = LinearSVC()\nlinear_svc.fit(x_train.astype('int'), y_train.astype('int'))  \ny_pred = linear_svc.predict(x_test.astype('int'))  \nacc_linear_svc = round(linear_svc.score(x_test.astype('int'), y_test.astype('int')) * 100, 2)\nprint(acc_linear_svc)","4b165d05":"decision_tree = DecisionTreeClassifier() \ndecision_tree.fit(x_train.astype('int'), y_train.astype('int'))  \ny_pred = decision_tree.predict(x_test.astype('int'))  \nacc_decision_tree = round(decision_tree.score(x_test.astype('int'), y_test.astype('int')) * 100, 2)\nprint(acc_decision_tree)","f75e6791":"xg_boost = XGBClassifier()\nxg_boost.fit(x_train.astype('int'), y_train.astype('int'))\ny_pred = xg_boost.predict(x_test.astype('int')) \nacc_xg_boost = round(xg_boost.score(x_test.astype('int'), y_test.astype('int')) * 100, 2)\nprint(acc_xg_boost)","e7c35db2":"#printing accuracy results of all the models together in the order of best result first \n\nresults = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron', \n              'Stochastic Gradient Decent', \n              'Decision Tree','XG Boost Classifier'],\n    'Score': [acc_linear_svc, acc_knn, acc_logreg, \n              acc_random_forest, acc_gaussian, acc_perceptron, \n              acc_sgd, acc_decision_tree,acc_xg_boost]})\nresult_df = results.sort_values(by='Score', ascending=False)\nresult_df = result_df.set_index('Score')\nresult_df.head(9)","ae31b623":"test_data = pd.DataFrame(test_data)\nembarked_q = pd.DataFrame(test_data['Embarked_Q'])\ntest_data.drop('Embarked_Q',1, inplace = True)\ntest_data['Embarked_Q'] = embarked_q['Embarked_Q']\n","7d9587e2":"prediction = xg_boost.predict(test_data)\noutput = pd.DataFrame({'PassengerId': test['PassengerId'],\n                       'Survived': prediction})\noutput.to_csv('new_sub.csv', index=False)\n\n","2f1197f5":"# Data Preprocessing ","f0196d3c":"**Looking at the train data, we can see that we have:** \n* 'PassengerId',\n* 'Survived': 0 - Not Survived\n              1 - Survived \n* 'Plass': 1 = 1st, 2 = 2nd, 3 = 3rd\n* 'Name': Name of the Passenger\n* 'Sex': Male, Female \n* 'Age': Age of the passenger,\n* 'SibSp': Number of siblings \/ spouses aboard the Titanic\n* 'Parch': Number of parents \/ children aboard the Titanic\n* 'Ticket': Ticket number\n* 'Fare': Passenger fare\n* 'Cabin': Cabin number\n* 'Embarked': Port of Embarkation\n            C = Cherbourg, Q = Queenstown, S = Southampton","c445551b":"**This is how the submission file should look like.**","a85f0cd4":"5. Gaussian NB","724c798e":"4. K Neighbors Classifier","5dec7a27":"9. XGB Classifier","7f23b03e":"# Let us explore the Titanic Surivial Data to know what is the possibility of a person to survive on Titanic!","36a5d1b0":"3. Random Forest Classifier","401c5bef":" \n# Model Building","0a637cab":"2. SGD Classifier","91094a22":"While, exploring the data in the above steps we found that two columns, 'Age' and 'Embarked'has missing values. 'Age' has 177 and 'Embarked' has 2 missing values. \n\n1. To fill the missing values in 'Age', we can replace the missing values by using iterative imputer.\n2. Since only 2 values are missing in 'Embarked' column we can replace it with mode value.","78221b5c":"![](https:\/\/i.ytimg.com\/vi\/uwDz1PNV2Js\/maxresdefault.jpg)","80099c67":"Performing similar operations on 'test_data'","0fba403b":"Let us begin with exploring our data!","83ee9995":"1. Logistic Regression","fbf35981":"#  Exploratory Data Analysis","63247b5a":"8. Decision Tree Classifier","9e1621b4":"Visualization of age distribution of passengers on board Titanic","01d29bc5":"6. Perceptron","48ac622d":"Trying out different machine learning models to find out best model for making predictions","8ff16323":"**Looking at the test data, we can see that we have same columns has train_data except for 'Survived' column whoes values are to be predicted.**","538ec264":"In order to train our model to give accurate prediction we must remove unwanted columns from the data set. We can drop 'PassengerId'as it will not affect the prediction. Similarly name of the passenger also not have much effect on the prediction. 'Cabin', 'Ticket' and 'Fare' columns direct to the 'Pclass' column. As the cabin, ticket and fare will tell about which class does the passenger belong to. Therefore we can drop this columns.","472b0c86":"7. Linear SVC","951406a7":"**If you like my notebook, please upvote it. Suggestions to make this notebook better are welcomed!**"}}