{"cell_type":{"f9ea144f":"code","fd8a20ad":"code","f4b60e6e":"code","b20b66fe":"code","4998231a":"code","2afb9f32":"code","544b8e86":"code","36f2c9af":"code","59260882":"code","cd203279":"code","5dfcf846":"code","b39861d4":"code","a275303a":"code","81e3a613":"code","a3a32d12":"code","47904a8e":"code","a54b18d6":"code","d41872f7":"code","918642a2":"code","938bad36":"code","474173d4":"code","fd6f63a1":"code","eee504d7":"code","4e7432d8":"code","072451cf":"code","cb841177":"code","73ae4d5d":"code","c97da940":"code","5c7d3326":"code","ec17bd87":"code","18746567":"code","682e1de3":"code","3eeeb4ab":"code","0de277ee":"code","ea9c25cd":"code","77fba0d2":"code","e487c5eb":"code","61d2c0c3":"code","3257877d":"code","7f7ba9d1":"code","28c6f8e6":"code","94a5fa94":"code","0f9c15f9":"code","eacb7b59":"code","f12f6c1e":"code","25bb8035":"code","55c3918c":"code","1b34a8a2":"code","3b720077":"code","2c037c85":"code","47995391":"code","af6bcc41":"code","631006ce":"code","c3db0ccd":"code","26f6760c":"markdown","e6c58e55":"markdown","70e79913":"markdown","a6d8f250":"markdown","9d06139f":"markdown","10fccac0":"markdown","249ccbef":"markdown","903a640b":"markdown","18874882":"markdown","cb9e7560":"markdown","ed9c6015":"markdown","5aefc9d6":"markdown","cb9a4c26":"markdown","3ce30bd0":"markdown","1c18e1df":"markdown","d639a593":"markdown","c8a8681b":"markdown","aee2f600":"markdown","f296e240":"markdown","007d5177":"markdown","2829b24d":"markdown","242e6092":"markdown","ce3653a8":"markdown","ea8c1b89":"markdown","b181684b":"markdown","6a984b76":"markdown","c6db2adf":"markdown","d35b59d8":"markdown","363741a9":"markdown","07c2aa17":"markdown","a5dabc45":"markdown"},"source":{"f9ea144f":"import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import SelectKBest, f_regression\nfrom sklearn.linear_model import Lasso\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline","fd8a20ad":"train_path = '..\/input\/home-data-for-ml-course\/train.csv'\ntest_path = '..\/input\/home-data-for-ml-course\/test.csv'","f4b60e6e":"train = pd.read_csv(train_path, index_col='Id')\ntest_X = pd.read_csv(test_path, index_col='Id')    # test contains only inputs, no targets","b20b66fe":"target = 'SalePrice'\nfeatures = train.columns\nfeatures = features.drop(target)","4998231a":"valid_frac = 0.2\ntrain_X, train_y = train[features], train[target]\ntrain_X, valid_X, train_y, valid_y = train_test_split(train_X, train_y, test_size=valid_frac)","2afb9f32":"print(train_X.info())","544b8e86":"plt.figure(figsize=(7, 7))\nplt.title('Null values in the dataset are shown in white')\nsns.heatmap(train_X.isnull(), cbar=False)","36f2c9af":"null_count_threshold = 0.1   # Columns with more than null_count_threshold percent nulls will be dropped\nnum_rows = train_X.shape[0]\nfor feature in features:\n    null_count = train_X[feature].isna().sum()\n    if(null_count > null_count_threshold * num_rows):\n        train_X = train_X.drop(feature, axis=1)\n        valid_X = valid_X.drop(feature, axis=1)\n        test_X = test_X.drop(feature, axis=1)","59260882":"categorical_columns = list(train_X.select_dtypes(include=['object']).columns)\nnumeric_columns = list(train_X.select_dtypes(include=['int', 'float']).columns)","cd203279":"print(len(categorical_columns))","5dfcf846":"print(len(numeric_columns))","b39861d4":"imputer = SimpleImputer(strategy='most_frequent')\ntrain_X[categorical_columns] = imputer.fit_transform(train_X[categorical_columns])\nvalid_X[categorical_columns] = imputer.transform(valid_X[categorical_columns])\ntest_X[categorical_columns] = imputer.transform(test_X[categorical_columns])","a275303a":"imputer = SimpleImputer(strategy='mean')\ntrain_X[numeric_columns] = imputer.fit_transform(train_X[numeric_columns])\nvalid_X[numeric_columns] = imputer.transform(valid_X[numeric_columns])\ntest_X[numeric_columns] = imputer.transform(test_X[numeric_columns])","81e3a613":"# There should now be no NaNs in train_X, valid_X and test_X.\nprint(f'train_X null count: {train_X.isna().sum().sum()}')\nprint(f'valid_X null count: {valid_X.isna().sum().sum()}')\nprint(f'test_X null count: {test_X.isna().sum().sum()}')","a3a32d12":"# Number of unique values in each categorical column\nprint('categorical_column'.ljust(25, ' ') + 'num_unique')\nfor categorical_column in categorical_columns:\n    print(categorical_column.ljust(25, ' ') + str(train_X[categorical_column].nunique()))","47904a8e":"num_unique_threshold = 5    # Categorical columns with more than num_unique_threshold unique values will be dropped\nfor categorical_column in categorical_columns:\n    num_unique = train_X[categorical_column].nunique()\n    if(num_unique > num_unique_threshold):\n        train_X = train_X.drop(categorical_column, axis=1)\n        valid_X = valid_X.drop(categorical_column, axis=1)\n        test_X = test_X.drop(categorical_column, axis=1)\n        categorical_columns.remove(categorical_column)","a54b18d6":"corrs = train_X.corrwith(train_y).abs()","d41872f7":"plt.figure(figsize=(8, 7))\nplt.xlabel('Correlation (absolute magnitude)')\nplt.title('Correlation of numerical features with target')\nsns.barplot(x=corrs, y=numeric_columns)","918642a2":"print('numeric_column'.ljust(25, ' ') + 'correlation')\nfor numeric_column, corr in zip(numeric_columns, corrs):\n    print(numeric_column.ljust(25, ' ') + str(round(corr, 3)))","938bad36":"# Removing numeric variables with corr < corr_threshold\ncorr_threshold = 0.2\nfor numeric_column, stddev in zip(numeric_columns, corrs):\n    if(corr < corr_threshold):\n        train_X = train_X.drop(numeric_column, axis=1)\n        numeric_columns.remove(numeric_column)","474173d4":"print(len(numeric_columns))","fd6f63a1":"corr_matrix = train_X[numeric_columns].corr().abs()","eee504d7":"plt.figure(figsize=(10, 8))\nplt.title('Correlation map (absolute values)')\nsns.heatmap(corr_matrix)","4e7432d8":"stddevs = train_X[numeric_columns].std(axis=0)","072451cf":"plt.figure(figsize=(8, 7))\nplt.xlabel('Standard deviation (log scale)')\nplt.title('Standard deviations of numerical variables')\nsns.barplot(x=stddevs, y=numeric_columns, log=True)","cb841177":"print('numeric_column'.ljust(25, ' ') + 'stddev')\nfor numeric_column, stddev in zip(numeric_columns, stddevs):\n    print(numeric_column.ljust(25, ' ') + str(round(stddev, 3)))","73ae4d5d":"# Removing numeric variables with standard deviation < stddev_threshold\nstddev_threshold = 1\nfor numeric_column, stddev in zip(numeric_columns, stddevs):\n    if(stddev < stddev_threshold):\n        train_X = train_X.drop(numeric_column, axis=1)\n        numeric_columns.remove(numeric_column)","c97da940":"print(len(numeric_columns))","5c7d3326":"# Numerical columns\ntrain_X_numerical = train_X[numeric_columns].values","ec17bd87":"# Ordinal encoded categorical columns\nordinal_encoder = OrdinalEncoder()\ntrain_X_categorical_oe = ordinal_encoder.fit_transform(train_X[categorical_columns])","18746567":"print(train_X_numerical)\nprint(f'train_X_numerical.shape: {train_X_numerical.shape}')\nprint(f'Number of numerical features = {len(numeric_columns)}')","682e1de3":"print(train_X_categorical_oe)\nprint(f'train_X_categorical_oe.shape: {train_X_categorical_oe.shape}')\nprint(f'Number of categorical features = {len(categorical_columns)}')","3eeeb4ab":"alphas = np.linspace(0, 10000000, 50)    # alpha parameter for LASSO","0de277ee":"# Rows --> Values of alpha, Columns --> Coefficients for a particular value of alpha\ncoef_trails = list()\n\n# Keeps track of the number of alpha values for which the ith coefficient is non-zero\ncoef_lifetimes = np.zeros((train_X_numerical.shape[1]))\n\nfor alpha in alphas:\n    regressor = Lasso(alpha=alpha)\n    regressor.fit(train_X_numerical, train_y)\n    coef_trails.append(regressor.coef_)\n    \n    for i, coef in enumerate(regressor.coef_):\n        if(coef != 0):\n            coef_lifetimes[i] += 1\n            \ncoef_trails = np.array(coef_trails)","ea9c25cd":"sns.set_palette(sns.color_palette('hls', coef_trails.shape[1]))\nplt.figure(figsize=(8, 7))\nplt.title('Coefficient trails for numerical variables')\nplt.xlabel('alpha parameter for LASSO')\nplt.ylabel('Coefficient value')\nplt.ylim([None, 300])\nfor i in range(coef_trails.shape[1]):\n    plt.plot(alphas, coef_trails[:, i], label=numeric_columns[i])\nplt.legend()","77fba0d2":"plt.figure(figsize=(8, 7))\nplt.xlabel('Coefficient lifetime')\nplt.xticks([])    # Setting xticks to empty list as actual lifetime is irrelevant, only relative order of lifetime matters\nsns.barplot(x=coef_lifetimes, y=numeric_columns)","e487c5eb":"alphas = np.linspace(0, 100000, 50)    # alpha parameter for LASSO","61d2c0c3":"# Rows --> Values of alpha, Columns --> Coefficients for a particular value of alpha\ncoef_trails = list()\n\n# Keeps track of the number of alpha values for which the ith coefficient is non-zero\ncoef_lifetimes = np.zeros((train_X_categorical_oe.shape[1]))\n\nfor alpha in alphas:\n    regressor = Lasso(alpha=alpha)\n    regressor.fit(train_X_categorical_oe, train_y)\n    coef_trails.append(regressor.coef_)\n    \n    for i, coef in enumerate(regressor.coef_):\n        if(coef != 0):\n            coef_lifetimes[i] += 1\n            \ncoef_trails = np.array(coef_trails)","3257877d":"sns.set_palette(sns.color_palette('hls', coef_trails.shape[1]))\nplt.figure(figsize=(9, 8))\nplt.title('Coefficient trails for categorical variables')\nplt.xlabel('alpha parameter for LASSO')\nplt.ylabel('Coefficient value')\nfor i in range(coef_trails.shape[1]):\n    plt.plot(alphas, coef_trails[:, i], label=categorical_columns[i])\nplt.legend()","7f7ba9d1":"plt.figure(figsize=(8, 7))\nplt.xlabel('Coefficient lifetime')\nplt.xticks([])    # Setting xticks to empty list as actual lifetime is irrelevant, only relative order of lifetime matters\nsns.barplot(x=coef_lifetimes, y=categorical_columns)","28c6f8e6":"selector = SelectKBest(f_regression, k=train_X_numerical.shape[1])\nselector.fit(train_X_numerical, train_y)","94a5fa94":"# pvalues for each of the numerical variables in numeric_columns\npvalues = selector.pvalues_\n\n# List of tuples of categorical variable to its pvalue\nvariable_pvalues = [(numeric_column, pvalues[i]) for i, numeric_column in enumerate(numeric_columns)]\n\n# Sorting variable_pvalues in order of pvalue\nvariable_pvalues.sort(key=lambda x: x[1])\n\nprint('numeric_variable'.ljust(25, ' ') + 'pvalue (sorted)')\nfor variable, pvalue in variable_pvalues:\n    print(variable.ljust(25, ' ') + str(pvalue))","0f9c15f9":"selector = SelectKBest(f_regression, k=train_X_categorical_oe.shape[1])\nselector.fit(train_X_categorical_oe, train_y)","eacb7b59":"# pvalues for each of the categorical variables in categorical_columns\npvalues = selector.pvalues_\n\n# List of tuples of categorical variable to its pvalue\nvariable_pvalues = [(categorical_column, pvalues[i]) for i, categorical_column in enumerate(categorical_columns)]\n\n# Sorting variable_pvalues in order of pvalue\nvariable_pvalues.sort(key=lambda x: x[1])\n\nprint('categorical_variable'.ljust(25, ' ') + 'pvalue (sorted)')\nfor variable, pvalue in variable_pvalues:\n    print(variable.ljust(25, ' ') + str(pvalue))","f12f6c1e":"final_numeric_columns = [    'LotArea', \n                             '1stFlrSF', \n                             'GarageArea', \n                             'BsmtFinSF1', \n                             'BsmtUnfSF', \n                             'FullBath', \n                             'TotRmsAbvGrd', \n                             'YearRemodAdd', \n                             'OverallCond']\nfinal_categorical_columns = ['HeatingQC', \n                             'BsmtQual', \n                             'GarageType', \n                             'KitchenQual', \n                             'LotShape', \n                             'ExterQual', \n                             'BsmtExposure', \n                             'GarageFinish', \n                             'Electrical']\n\nfinal_columns = final_numeric_columns + final_categorical_columns","25bb8035":"mean = train_X[final_numeric_columns].mean(axis=0)\nstddev = train_X[final_numeric_columns].std(axis=0)\ntrain_X[final_numeric_columns] = (train_X[final_numeric_columns] - mean) \/ stddev\nvalid_X[final_numeric_columns] = (valid_X[final_numeric_columns] - mean) \/ stddev\ntest_X[final_numeric_columns] = (test_X[final_numeric_columns] - mean) \/ stddev","55c3918c":"train_X_final = train_X[final_columns]\nvalid_X_final = valid_X[final_columns]\ntest_X_final = test_X[final_columns]","1b34a8a2":"# One hot encoding categorical features in train, valid and test by fitting on train\none_hot_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n\n# Categorical columns one hot encoded as NumPy arrays\ntrain_X_one_hot = one_hot_encoder.fit_transform(train_X_final[final_categorical_columns])\nvalid_X_one_hot = one_hot_encoder.transform(valid_X_final[final_categorical_columns])\ntest_X_one_hot = one_hot_encoder.transform(test_X_final[final_categorical_columns])\n\n# Converting one hot encoded NumPy arrays to Pandas DataFrame\ntrain_X_one_hot = pd.DataFrame(train_X_one_hot)\nvalid_X_one_hot = pd.DataFrame(valid_X_one_hot)\ntest_X_one_hot = pd.DataFrame(test_X_one_hot)\n\n# One hot encoding created NumPy arrays, so indexing was lost, reindexing\ntrain_X_one_hot.index = train_X_final.index\nvalid_X_one_hot.index = valid_X_final.index\ntest_X_one_hot.index = test_X_final.index\n\n# Removing the original categorical features columns as they have been one hot encoded\ntrain_X_final = train_X_final.drop(final_categorical_columns, axis=1)\nvalid_X_final = valid_X_final.drop(final_categorical_columns, axis=1)\ntest_X_final = test_X_final.drop(final_categorical_columns, axis=1)\n\n# Adding the one hot encoded columns to the final dataframes\ntrain_X_final = pd.concat([train_X_final, train_X_one_hot], axis=1)\nvalid_X_final = pd.concat([valid_X_final, valid_X_one_hot], axis=1)\ntest_X_final = pd.concat([test_X_final, test_X_one_hot], axis=1)","3b720077":"print(f'Numeric column count = {len(final_numeric_columns)}')\n\n# Number of one hot columns should equal the total number of unique classes in each categorical feature\none_hot_column_count = train_X[final_categorical_columns].nunique().sum()\nprint(f'One hot column count = {one_hot_column_count}')\n\nprint(f'Categorical column count = {len(final_categorical_columns)}')\n\n# Total column count = Number of numeric columns + Number of one hot columns\nprint(f'Total column count must be = {len(final_numeric_columns) + one_hot_column_count}')","2c037c85":"print(train_X_final.shape)\nprint(valid_X_final.shape)\nprint(test_X_final.shape)","47995391":"categories = one_hot_encoder.categories_\nprint(categories)","af6bcc41":"i = 0\n# Provides mapping from old one hot columns to new names for those columns\nrename_dict = dict()\nfor category_name, category in zip(final_categorical_columns, categories):\n    for category_class in category:\n        rename_dict[i] = category_name + '_' + category_class\n        i += 1\nprint(rename_dict)","631006ce":"train_X_final = train_X_final.rename(columns=rename_dict)\nvalid_X_final = valid_X_final.rename(columns=rename_dict)\ntest_X_final = test_X_final.rename(columns=rename_dict)","c3db0ccd":"train_final = pd.concat([train_X_final, train_y], axis=1).sort_index()\nvalid_final = pd.concat([valid_X_final, valid_y], axis=1).sort_index()\n\"\"\"\ntrain_final.to_csv('data\/train_final.csv')\nvalid_final.to_csv('data\/valid_final.csv')\ntest_X_final.to_csv('data\/test_X_final.csv')\n\"\"\"","26f6760c":"#### Selecting best categorical variables","e6c58e55":"### Removing numeric variables that have low correlation with target\n\nVariables that have low correlation with the target have low predictive power. \n\nA low correlation is usually considered to be r < 0.3. \nRefer: https:\/\/www.westga.edu\/academics\/research\/vrc\/assets\/docs\/scatterplots_and_correlation_notes.pdf","70e79913":"## Handling null values","a6d8f250":"Features with low variance do a bad job of prediction.","9d06139f":"Comparing the results from the univariate tests and LASSO, we can conclude that the following are likely the most significant features:\n\nNumerical:\nFrom LASSO: 'LotArea', '1stFlrSF', 'GarageArea', 'BsmtFinSF1', 'BsmtUnfSF'\nFrom univariate: 'GarageArea', '1stFlrSF', 'FullBath', 'TotRmsAbvGrd', 'YearRemodAdd', 'BsmtFinSF1', 'OpenPorchSF', 'LotArea', 'BsmtUnfSF', 'PoolArea', 'OverallCond'\nOverall: 'LotArea', '1stFlrSF', 'GarageArea', 'BsmtFinSF1', 'BsmtUnfSF', 'FullBath', 'TotRmsAbvGrd', 'YearRemodAdd', 'OverallCond'\n\nCategorical:\nFrom LASSO: 'HeatingQC', 'BsmtQual', 'GarageType', 'KitchenQual', 'LotShape', 'ExterQual', 'BsmtExposure', 'Electrical'\nFrom univariate: 'ExterQual', 'BsmtQual', 'KitchenQual', 'GarageFinish', 'HeatingQC', 'BsmtExposure', 'GarageType', 'LotShape', 'Electrical'\nOverall: 'HeatingQC', 'BsmtQual', 'GarageType', 'KitchenQual', 'LotShape', 'ExterQual', 'BsmtExposure', 'GarageFinish', 'Electrical'","10fccac0":"Renaming the categorical columns in the format \\<feature_name>\\_\\<category_name>.","249ccbef":"The longer a coefficient of a feature is alive, the more relevant the feature is.\n'LotArea', '1stFlrSF', 'GarageArea', 'BsmtFinSF1' and 'BsmtUnfSF' seem to be the most relevant features, which makes sense.","903a640b":"### Removing variables with low variance","18874882":"As a sanity check, checking that the number of features in train_X_numerical match the total number of numerical features.","cb9e7560":"### Filling NaNs in categorical columns using most frequent occurrence for each column","ed9c6015":"Performing some sanity checks before saving the dataframes to csv files.","5aefc9d6":"# Exploratory Data Analysis and Feature Selection","cb9a4c26":"#### Selecting best numerical variables","3ce30bd0":"## Feature selection","1c18e1df":"### Using LASSO for feature selection\n\nApplying LASSO separately on numerical and categorical variables.","d639a593":"### Adding together numeric and categorical columns","c8a8681b":"### Removing numeric variables that are highly correlated\n\nKeeping only one among two features that are highly correlated is a useful way to reduce dimensionality.\n\nA high correlation is usually considered to be r > 0.7. \nRefer: https:\/\/www.westga.edu\/academics\/research\/vrc\/assets\/docs\/scatterplots_and_correlation_notes.pdf","aee2f600":"#### LASSO for categorical variables","f296e240":"### Univariate tests","007d5177":"#### LASSO for numerical variables","2829b24d":"Ordinal encoding the categorical features before applying LASSO.","242e6092":"### Extracting categorical and numeric columns","ce3653a8":"### Standard scaling numeric columns","ea8c1b89":"p < 0.05 is considered statistically significant.","b181684b":"### Filling NaNs in numeric columns using mean for each column","6a984b76":"Dropping columns with large number of nulls in train. Dropping corresponding columns from valid and test too.","c6db2adf":"## Generating final training, validation and test sets","d35b59d8":"## Splitting training set into training set and validation set","363741a9":"As a sanity check, checking that the number of features in train_X_numerical match the total number of numerical features.","07c2aa17":"Dropping categorical variables with too many unique classes.","a5dabc45":"The longer a coefficient of a feature is alive, the more relevant the feature is. 'HeatingQC', 'BsmtQual', 'GarageType', 'KitchenQual', 'LotShape' and 'ExterQual' seem to be the most relevant features."}}