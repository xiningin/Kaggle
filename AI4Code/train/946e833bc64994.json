{"cell_type":{"abc64152":"code","701066b9":"code","125350ad":"code","cc74d627":"code","a335d760":"code","5e6bf0e7":"code","ffa6786d":"code","cf2043e9":"code","fec4d4f7":"code","372f99a9":"code","90ca29b8":"code","6b3877a8":"code","6d7dd96f":"code","9ef7f8ab":"code","5e7bd61d":"code","1c20e89c":"markdown","0e65108d":"markdown","1444da7f":"markdown","b8093e4b":"markdown","2876335f":"markdown","e5bdc3e3":"markdown","9c28e710":"markdown","6fe74ab8":"markdown","ba29254a":"markdown","50fd5154":"markdown","cb5e4c66":"markdown","acbadd7d":"markdown","e65326ab":"markdown","bb4333b7":"markdown","2c496d70":"markdown","68f7a9a1":"markdown","6e5f2c3e":"markdown","504dbc4e":"markdown"},"source":{"abc64152":"from IPython.core.magic import register_cell_magic\n\n@register_cell_magic\ndef write_and_run(line, cell):\n    argz = line.split()\n    file = argz[-1]\n    mode = 'w'\n    if len(argz) == 2 and argz[0] == '-a':\n        mode = 'a'\n    with open(file, mode) as f:\n        f.write(cell)\n    get_ipython().run_cell(cell)","701066b9":"%%write_and_run submission.py\n# This will run the cell and re-write the `submission.py` file.\n\nfrom abc import ABC, abstractmethod\nimport random\nimport functools\nimport numpy as np\n\nimport collections\nfrom sklearn.tree import DecisionTreeClassifier\n\n\n# Set this to True only if you're debugging your agent\nDEBUG_MODE = True","125350ad":"%%write_and_run -a submission.py\n# Append the content of this cell to `submission.py`\n\n#----------------------------------------------------------\n#  CONSTANTS\n#----------------------------------------------------------\n\nNUM_TO_MOVE = ['R', 'P', 'S']\nMOVE_TO_NUM = {'R': 0, 'P': 1, 'S': 2}\n\nBEAT = {'R': 'P', 'P': 'S', 'S': 'R', None: None}\nCEDE = {'R': 'S', 'P': 'R', 'S': 'P', None: None}\nDNA_ENCODE = {\n    'RP': 'a', 'PS': 'b', 'SR': 'c',\n    'PR': 'd', 'SP': 'e', 'RS': 'f',\n    'RR': 'g', 'PP': 'h', 'SS': 'i'}","cc74d627":"%%write_and_run -a submission.py\n# Run the cell and append its contentto `submission.py`\n\n#----------------------------------------------------------\n#  SYMMETRIC HISTORY STORAGE\n#----------------------------------------------------------\n\nclass HistoryHolder:\n  \"\"\"Holds the sequence of moves since the start of the game\"\"\"\n  def __init__(self):\n    self.our_moves = ''\n    self.his_moves = ''\n    self.dna_moves = ''\n\n  def add_moves(self, our_move, his_move):\n    self.our_moves += our_move\n    self.his_moves += his_move\n    self.dna_moves += DNA_ENCODE[our_move + his_move]\n\n  def __len__(self):\n    if DEBUG_MODE:\n      assert len(self.our_moves) == len(self.his_moves)\n      assert len(self.our_moves) == len(self.dna_moves)\n    return len(self.our_moves)\n\n\nclass HolisticHistoryHolder:\n  \"\"\"Holds actual history and the history in opponent's shoes\"\"\"\n  def __init__(self):\n    self.actual_history = HistoryHolder()\n    self.mirror_history = HistoryHolder()\n\n  def add_moves(self, our_move, his_move):\n    self.actual_history.add_moves(our_move, his_move)\n    self.mirror_history.add_moves(his_move, our_move)\n\n  def __len__(self):\n    if DEBUG_MODE:\n      assert len(self.actual_history) == len(self.mirror_history)\n    return len(self.actual_history)","a335d760":"%%write_and_run -a submission.py\n# Run the cell and append its contentto `submission.py`\n\nclass BaseAtomicStrategy(ABC):\n  \"\"\"Interface for all atomic strategies\"\"\"\n\n  @abstractmethod\n  def __call__(self, history):\n    \"\"\"Returns an action to take, given the game history\"\"\"\n    pass","5e6bf0e7":"%%write_and_run -a submission.py\n# Run the cell and append its contentto `submission.py`\n\n\ndef shift_action(action, shift):\n  shift = shift % 3\n  if shift == 0: return action\n  elif shift == 1: return BEAT[action]\n  elif shift == 2: return CEDE[action]\n\n\ndef generate_meta_strategy_pair(atomic_strategy_cls,\n                                *args, **kwargs):\n  \"\"\"Generate pair of strategy and anti-strategies\"\"\"\n  actual_atomic = atomic_strategy_cls(*args, **kwargs)\n  def _actual_strategy(holistic_history):\n    return actual_atomic(holistic_history.actual_history)\n\n  mirror_atomic = atomic_strategy_cls(*args, **kwargs)\n  def _mirror_strategy(holistic_history):\n    move = mirror_atomic(holistic_history.mirror_history)\n    return BEAT[move]\n  return _actual_strategy, _mirror_strategy","ffa6786d":"%%write_and_run -a submission.py\n# Run the cell and append its contentto `submission.py`\n\n#----------------------------------------------------------\n#  SCORING FUNCTION FACTORIES\n#----------------------------------------------------------\n\ndef get_dllu_scoring(decay=1.,\n                     win_value=1.,\n                     draw_value=0.,\n                     lose_value=-1.,\n                     drop_prob=0.,\n                     drop_draw=False,\n                     clip_zero=False):\n  \"\"\"Returns a DLLU score (daniel.lawrence.lu\/programming\/rps\/)\n\n  Adds 1 to previous score if we won, subtract if we lose the\n  round. Previous score is multiplied by a decay parameter >0.\n  Thus, if the opponent occasionally switches strategies, this\n  should be able to cope.\n\n  If a predictor loses even once, its score is reset to zero\n  with some probability. This allows for much faster response\n  to opponents with switching strategies.\n  \"\"\"\n  def _scoring_func(score, our_move, his_move):\n    if our_move == his_move:\n      retval = decay * score + draw_value\n    elif our_move == BEAT[his_move]:\n      retval = decay * score + win_value\n    elif our_move == CEDE[his_move]:\n      retval = decay * score + lose_value\n\n    if drop_prob > 0. and random.random() < drop_prob:\n      if our_move == CEDE[his_move]:\n        score = 0.\n      elif drop_draw and our_move == his_move:\n        score = 0.\n\n    if clip_zero: retval = max(0., retval)\n    return retval\n\n  return _scoring_func","cf2043e9":"%%write_and_run -a submission.py\n# Run the cell and append its contentto `submission.py`\n\n#----------------------------------------------------------\n#  STRATEGY 1: RFIND\n#----------------------------------------------------------\n\nclass RFindStrategy(BaseAtomicStrategy):\n  def __init__(self, limit=None, src='his'):\n    self.limit = limit\n    self.src = src\n\n  def __call__(self, history):\n    if len(history) == 0:\n      return NUM_TO_MOVE[random.randint(0, 2)]\n\n    # Type of lookback sequence\n    if self.src == 'his':\n      sequence = history.his_moves\n    elif self.src == 'our':\n      sequence = history.our_moves\n    elif self.src == 'dna':\n      sequence = history.dna_moves\n    else:\n      raise ValueError(f'Invalid `src` value (got {self.src}')\n\n    # Define lookback window\n    length = len(history)\n    if self.limit == None:\n      lb = length\n    else:\n      lb = min(length, self.limit)\n\n    # RFind choose action\n    while lb >= 1 and \\\n        not sequence[length - lb:length] in sequence[0:length - 1]:\n      lb -= 1\n    if lb >= 1:\n      if random.random() < 0.6:\n        idx = sequence.rfind(\n            sequence[length - lb:length], 0, length - 1)\n      elif random.random() < 0.5:\n        idx = sequence.rfind(\n            sequence[length - lb:length], 0, length - 1)\n        idx2 = sequence.rfind(\n            sequence[length - lb:length], 0, idx)\n        if idx2 != -1:\n          idx = idx2\n      else:\n        idx = sequence.find(\n            sequence[length - lb:length], 0, length - 1)\n\n      return BEAT[history.his_moves[idx + lb]]\n    else:\n      return random.choice('RPS')\n","fec4d4f7":"%%write_and_run -a submission.py\n# Run the cell and append its contentto `submission.py`\n\n#----------------------------------------------------------\n#  KUMOKO AGENT\n#----------------------------------------------------------\n\nclass KumokoV1:\n  def __init__(self):\n    \"\"\"Define scoring functions and strategies\"\"\"\n    self.strategies = []\n    self.proposed_actions = []\n    self.proposed_meta_actions = []\n    self.our_last_move = None\n    self.holistic_history = HolisticHistoryHolder()\n\n    # Add DLLU's scoring methods from his blog\n    # https:\/\/daniel.lawrence.lu\/programming\/rps\/\n    dllu_scoring_configs = [\n        # decay, win_val, draw_val, lose_val, drop_prob, drop_draw, clip_zero\n        [ 0.80,  3.00,    0.00,     -3.00,    0.00,      False,     False    ],\n        [ 0.87,  3.30,    -0.90,    -3.00,    0.00,      False,     False    ],\n        [ 1.00,  3.00,    0.00,     -3.00,    1.00,      False,     False    ],\n        [ 1.00,  3.00,    0.00,     -3.00,    1.00,      True,      False    ],\n    ]\n    self.scoring_funcs = [\n        get_dllu_scoring(*cfg)\n        for cfg in dllu_scoring_configs]\n\n    # Add RFind strategies (2 meta-strategies P0 and P'0 for each)\n    limits = [50, 20, 10]\n    sources = ['his', 'our', 'dna']\n    for limit in limits:\n      for source in sources:\n        self.strategies.extend(\n            generate_meta_strategy_pair(RFindStrategy,\n                                        *(limit, source)))\n\n    # Add initial scores for each strategy in the list\n    self.scores = 3. * np.ones(\n        shape=(len(self.scoring_funcs),\n               3 * len(self.strategies)))\n    self.proposed_actions = [\n      random.choice('RPS')\n      for _ in range(self.scores.shape[1])]\n\n    # Add meta-scores for each of the scoring function\n    self.meta_scoring_func = get_dllu_scoring(\n        decay=0.94,\n        win_value=1.0,\n        draw_value=0.0,\n        lose_value=-1.0,\n        drop_prob=0.87,\n        drop_draw=False,\n        clip_zero=True)\n\n    self.meta_scores = 3. * np.ones(\n        shape=(len(self.scoring_funcs)))\n    self.proposed_meta_actions = [\n        random.choice('RPS')\n        for _ in range(self.meta_scores.shape[0])]\n\n  def next_action(self, our_last_move, his_last_move):\n    \"\"\"Generate next move based on opponent's last move\"\"\"\n\n    # Force last move, so that we can use Kumoko as part of\n    # a larger meta-agent\n    self.our_last_move = our_last_move\n\n    # Update game history with the moves from previous\n    # game step\n    if his_last_move is not None:\n      if DEBUG_MODE:\n        assert self.our_last_move is not None\n      self.holistic_history.add_moves(\n          self.our_last_move, his_last_move)\n\n    # Update score for the previous game step\n    if his_last_move is not None and \\\n        len(self.proposed_actions) > 0:\n\n      if DEBUG_MODE:\n        assert len(self.proposed_actions) == \\\n          3 * len(self.strategies)\n        assert len(self.proposed_meta_actions) == \\\n          len(self.meta_scores)\n        assert self.scores.shape[0] == \\\n          len(self.scoring_funcs)\n\n      # Meta-strategy selection score\n      for sf in range(len(self.scoring_funcs)):\n        for pa in range(len(self.proposed_actions)):\n          self.scores[sf, pa] = self.scoring_funcs[sf](\n              self.scores[sf, pa],\n              self.proposed_actions[pa],\n              his_last_move)\n\n      # Selector selection score\n      for sf in range(len(self.scoring_funcs)):\n        self.meta_scores[sf] = self.meta_scoring_func(\n            self.meta_scores[sf],\n            self.proposed_meta_actions[sf],\n            his_last_move)\n\n    # Generate next move for each strategy\n    if len(self.proposed_actions) == 0:\n      self.proposed_actions = [\n          random.choice('RPS')\n          for _ in range(len(self.strategies) * 3)]\n    else:\n      for st in range(len(self.strategies)):\n        proposed_action = \\\n          self.strategies[st](self.holistic_history)\n        if proposed_action is not None:\n          self.proposed_actions[st] = proposed_action\n          self.proposed_actions[st + len(self.strategies)] = \\\n            BEAT[self.proposed_actions[st]]\n          self.proposed_actions[st + 2 * len(self.strategies)] = \\\n            CEDE[self.proposed_actions[st]]\n\n    # For each scoring function (selector), choose the\n    # action with highest score\n    best_actions_idx = np.argmax(self.scores, axis=1)\n    if DEBUG_MODE:\n      assert best_actions_idx.shape == \\\n        (len(self.scoring_funcs), )\n    self.proposed_meta_actions = [\n        self.proposed_actions[idx]\n        for idx in best_actions_idx]\n\n    # Meta-Selector: selecting the scoring function\n    if DEBUG_MODE:\n      assert len(self.meta_scores) == \\\n        len(self.proposed_meta_actions)\n    best_meta_action_idx = np.argmax(self.meta_scores)\n    self.our_last_move = \\\n      self.proposed_meta_actions[best_meta_action_idx]\n\n    return self.our_last_move","372f99a9":"%%write_and_run -a submission.py\n# Run the cell and append its contentto `submission.py`\n\n#----------------------------------------------------------\n#  GOING META WITH KUMOKO\n#----------------------------------------------------------\n\nclass MetaKumoko:\n  def __init__(self,\n               kumoko_cls,\n               kumoko_args=[],\n               kumoko_kwargs={}):\n    self.kumoko_1 = kumoko_cls(\n        *kumoko_args, **kumoko_kwargs)\n    self.kumoko_2 = kumoko_cls(\n        *kumoko_args, **kumoko_kwargs)\n    self.proposed_actions = []\n    self.scores = 3. * np.ones(shape=(6,))\n    self.scoring_func = get_dllu_scoring(\n        decay=0.94,\n        win_value=1.0,\n        draw_value=0.0,\n        lose_value=-1.0,\n        drop_prob=0.87,\n        drop_draw=False,\n        clip_zero=True)\n    self.our_last_move = None\n\n  def next_action(self, our_last_move, his_last_move):\n    \"\"\"Generate next move based on opponent's last move\"\"\"\n\n    # Force last move, so that we can use Kumoko as part of\n    # a larger meta-agent\n    self.our_last_move = our_last_move\n\n    # Score the last actions\n    if his_last_move is not None and \\\n        len(self.proposed_actions) > 0:\n      for i in range(6):\n        self.scores[i] = self.scoring_func(\n            self.scores[i],\n            self.proposed_actions[i],\n            his_last_move)\n\n    # Generate actions for Kumoko in our shoes and in the\n    # shoes of opponents (i.e. 6 meta-strategies)\n    a1 = self.kumoko_1.next_action(our_last_move, his_last_move)\n    a2 = self.kumoko_2.next_action(his_last_move, our_last_move)\n    a2 = BEAT[a2]\n    self.proposed_actions = [\n        a1, a2, BEAT[a1], BEAT[a2], CEDE[a1], CEDE[a2]]\n\n    # Selecting the best action\n    best_idx = np.argmax(self.scores)\n    self.our_last_move = self.proposed_actions[best_idx]\n    return self.our_last_move","90ca29b8":"%%write_and_run -a submission.py\n# Run the cell and append its contentto `submission.py`\n\n\n#----------------------------------------------------------\n#  FINAL AGENT IN THE COMPETITION'S FORMAT\n#----------------------------------------------------------\n\nglobal kumoko_agent\nglobal latest_action\nkumoko_agent = MetaKumoko(KumokoV1)\nlatest_action = None\n\n\ndef agent(obs, conf):\n  global kumoko_agent\n  global latest_action\n\n  if obs.step == 0:\n    s_move = kumoko_agent.next_action(None, None)\n  else:\n    s_his_last_move = NUM_TO_MOVE[obs.lastOpponentAction]\n    s_our_last_move = NUM_TO_MOVE[latest_action]\n    s_move = kumoko_agent.next_action(\n        s_our_last_move, s_his_last_move)\n\n  latest_action = MOVE_TO_NUM[s_move]\n\n  # Surprise motherfucker\n  if random.random() < 0.1 or random.randint(3, 40) > obs.step:\n    latest_action = random.randint(0, 2)\n  return latest_action","6b3877a8":"import os\nimport pandas as pd\nimport kaggle_environments\nfrom datetime import datetime\nimport multiprocessing as pymp\nfrom tqdm import tqdm\nimport ray.util.multiprocessing as raymp\n\n\n# function to return score\ndef get_result(match_settings):\n    start = datetime.now()\n    outcomes = kaggle_environments.evaluate(\n        'rps', [match_settings[0], match_settings[1]], num_episodes=match_settings[2])\n    won, lost, tie, avg_score = 0, 0, 0, 0.\n    for outcome in outcomes:\n        score = outcome[0]\n        if score > 0: won += 1\n        elif score < 0: lost += 1\n        else: tie += 1\n        avg_score += score\n    elapsed = datetime.now() - start\n    return match_settings[1], won, lost, tie, elapsed, float(avg_score) \/ float(match_settings[2])\n\n\ndef eval_agent_against_baselines(agent, baselines, num_episodes=10, use_ray=False):\n    df = pd.DataFrame(\n        columns=['wins', 'loses', 'ties', 'total time', 'avg. score'],\n        index=baselines\n    )\n    \n    if use_ray:\n        pool = raymp.Pool()\n    else:\n        pool = pymp.Pool()\n    matches = [[agent, baseline, num_episodes] for baseline in baselines]\n    \n    results = []\n    for content in tqdm(pool.imap_unordered(get_result, matches), total=len(matches)):\n        results.append(content)\n\n    for baseline_agent, won, lost, tie, elapsed, avg_score in results:\n        df.loc[baseline_agent, 'wins'] = won\n        df.loc[baseline_agent, 'loses'] = lost\n        df.loc[baseline_agent, 'ties'] = tie\n        df.loc[baseline_agent, 'total time'] = elapsed\n        df.loc[baseline_agent, 'avg. score'] = avg_score\n        \n    return df","6d7dd96f":"%%time\nwhite_belt_agents = [os.path.join('..\/input\/rps-dojo\/white_belt', agent)\n                     for agent in os.listdir('..\/input\/rps-dojo\/white_belt')]\neval_agent_against_baselines('submission.py', white_belt_agents)","9ef7f8ab":"%%time\nblue_belt_agents = [os.path.join('..\/input\/rps-dojo\/blue_belt', agent)\n                     for agent in os.listdir('..\/input\/rps-dojo\/blue_belt')]\neval_agent_against_baselines('submission.py', blue_belt_agents)","5e7bd61d":"%%time\nblack_belt_agents = [os.path.join('..\/input\/rps-dojo\/black_belt', agent)\n                     for agent in os.listdir('..\/input\/rps-dojo\/black_belt')]\neval_agent_against_baselines('submission.py', black_belt_agents)","1c20e89c":"# Combining multiple strategies is the key!\n\nAt this stage of the competition, it is clear that having a Multi-Armed bandit setting (as in [this notebook][multi_armed_bandit]) is the core framework that top competitors uses (see [this awesome discussion][strat_dis] for more information, thanks [@taahakhan][taahakhan] for starting this conversation) employs. The real trick is how to combine them in a way that outsmarts your opponent.\n\nIn this notebook, you will find a convenient framework that is very easy to expand, where you can define your own strategies and define ways to combine them, in a modular way (so you can implement and test your strategy very quickly).\n\n\n[multi_armed_bandit]: https:\/\/www.kaggle.com\/ilialar\/multi-armed-bandit-vs-deterministic-agents\n[strat_dis]: https:\/\/www.kaggle.com\/c\/rock-paper-scissors\/discussion\/201683\n[taahakhan]: https:\/\/www.kaggle.com\/taahakhan","0e65108d":"# Against White Belt Baselines\n\nCan Kumoko beat all of white belt agents?","1444da7f":"We can now implement our final agent.","b8093e4b":"# Against Black Belt Agents\n\nTesting against black belt baselines is where the real fun begins...","2876335f":"![Enough moping around](https:\/\/i.imgur.com\/gISUdnW.jpg)","e5bdc3e3":"Time to generate **Meta-Strategies** for our strategy!\n\n![Ultimate RPS Strategy](https:\/\/i.imgur.com\/YOJBUM1.jpg)","9c28e710":"# Meet Kumoko &mdash; a Meta Spider\n\n\n**Kumoko** is the main character from [Kumo desu ga nani ka](https:\/\/mangaclash.com\/manga\/kumo-desu-ga-nani-ka\/) manga (and light novel). She is a spider with multiple personalities that are all working in sync to achieve only one goal &mdash; to defeat her enemies. She is not the strongest one, but she can beat much stronger opponents by outsmarting them.\n\n![Kumoko - a Meta Spider](https:\/\/i.imgur.com\/mle6A5s.jpg)\n\n\n**Kumoko** follows the [RPSContest][rpscontest] top solution's strategies, which is essentially:\n\n- Pick a bunch of basic strategies (the most popular one that I see over and over again is similar to `rfind.py` from [Running RPSContest bots](https:\/\/www.kaggle.com\/purplepuppy\/running-rpscontest-bots) notebook).\n- For each strategy, also generate a \"mirror strategy\" &mdash; as if we're playing in opponent's shoes and trying to predict our own moves. Then play whatever beats the mirror strategy (i.e. rotation by 1).\n- Maybe also generate rotation strategies for each strategy and its mirror strategies.\n\n[rpscontest]: http:\/\/www.rpscontest.com\/","6fe74ab8":"# Evaluate\n\nBefore submitting our agent, let's try it against other agents in [RPS Dojo](https:\/\/www.kaggle.com\/chankhavu\/rps-dojo). The multi-processing evaluation script is copied from that notebook as well.\n\n![All opponents have gathered](https:\/\/i.imgur.com\/ObqrxxM.jpg)","ba29254a":"For easier implementation of our strategies, we define an object with simple interface for holding and retrieving history of game moves.","50fd5154":"# Let's go straight into the implementation!\n\nBut before we start, I recommend using the following magic instead of `%%writefile`. This will allow you to write-and-run the cell using magic `%%write_and_run some.py` or append-and-run using `%%write_and_run -a some.py`.\n\nThis is more interactive and you would be able to actually debug your agent without having to un-comment all of the `%%writefile` magic in your cells. Got it from [this StackOverflow answer](https:\/\/stackoverflow.com\/a\/60658965\/4789373).","cb5e4c66":"Some utility functions:","acbadd7d":"Instead of using numbers, we will represent our actions in `char` type: `\"R\"` for Rock, `\"P\"` for Paper, and `\"S\"` for Scissors.","e65326ab":"# Against Blue Belt Agents\n\nPutting Kumoko to a real test","bb4333b7":"# That's it!\n\nI want to thank everyone from [this discussion](https:\/\/www.kaggle.com\/c\/rock-paper-scissors\/discussion\/201683) for sharing their insights and approaches!\n\n![Ok, pause](https:\/\/i.imgur.com\/bFfj6xR.jpg)","2c496d70":"**Meta-Strategies** and **Multiple Scoring Schemes** is not meta enough for you? **LET'S GO FULL META!**\n\n![It's happening](https:\/\/i.imgur.com\/YBPlE6h.jpg)","68f7a9a1":"The family of scoring functions is modelled similarly to the [Centrifugal Bumblepuppy](https:\/\/daniel.lawrence.lu\/programming\/rps\/) by [Daniel Lu](https:\/\/www.kaggle.com\/purplepuppy).","6e5f2c3e":"Let's pack some **LOVE** into our base strategies <3\n\n![Let's pack some love](https:\/\/i.imgur.com\/r9NzvHE.png)\n\nWe will use `rfind` (from [Running RPSContest bots](https:\/\/www.kaggle.com\/purplepuppy\/running-rpscontest-bots) notebook) with multiple lengths and different versions of the history as our base strategy.","504dbc4e":"Base interface for the strategies is very simple and allows a lot of flexibility:"}}