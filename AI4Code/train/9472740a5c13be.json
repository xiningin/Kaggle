{"cell_type":{"77cf87d4":"code","1af5016d":"code","c24ac749":"code","88343279":"code","9d495296":"code","e5d5da0d":"code","56868dc2":"code","5e08248b":"code","5906abb1":"code","58addbc4":"code","9fe9147f":"code","6c3a5492":"code","c4ca09d0":"code","0dac402f":"code","d4597855":"code","2c3fdcc2":"code","2c68c329":"code","30136e8e":"code","a1cb4eed":"code","104a522e":"code","20f68d36":"code","38adcd90":"code","c2283967":"code","ccf666f9":"code","e3c8c960":"code","457eec44":"code","c6e578fb":"markdown","73f7b59b":"markdown","16890b45":"markdown","9907eef6":"markdown","76bc039d":"markdown","4e432fca":"markdown","5f18ed4d":"markdown","158f6fd9":"markdown","794d28ae":"markdown","2e183e6f":"markdown","42877fa3":"markdown","7dcd1689":"markdown","c9e0ad07":"markdown","dff7acf2":"markdown","3b284f7d":"markdown","bc947da0":"markdown","c67aeecb":"markdown","7dd0251b":"markdown","20e80b43":"markdown","1636e6fd":"markdown","1ebe88b7":"markdown","7a7ee066":"markdown","f5dcae4b":"markdown","d9852011":"markdown","51913f27":"markdown","a81256ef":"markdown"},"source":{"77cf87d4":"import tensorflow as tf\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline","1af5016d":"datasets = tf.keras.datasets.fashion_mnist\n(x_train_full, y_train_full), (x_test, y_test) = datasets.load_data()","c24ac749":"print('Shape Of the Datasets: ')\nprint('X - Train Datasets Shape : \\t\\t', x_train_full.shape)\nprint('X - Test Datasets Shape : \\t\\t', x_test.shape)\nprint()\nprint('Y - Train Datasets Shape : \\t\\t', y_train_full.shape)\nprint('Y - Test Datasets Shape : \\t\\t', y_test.shape)\nprint('')\nprint('Data Type of the Datasets: ')\nprint('X - Train Datasets data - type : \\t', x_train_full.dtype)\nprint('X - Test Datasets data - type : \\t', x_test.dtype)\nprint()\nprint('Y - Train Datasets data - type : \\t', y_train_full.dtype)\nprint('Y - Test Datasets data - type : \\t', y_test.dtype)","88343279":"x_validation, x_train = x_train_full[:5000] \/255.0, x_train_full[5000:]\/255.0\ny_validation, y_train = y_train_full[:5000] , y_train_full[5000:]","9d495296":"class_names= ['T-shirt\/top', 'Trouser', 'Pullover',\n             'Dress', 'Coat', 'Sandal', 'Shirt',\n             'Sneaker','Bag', 'Ankel boot']","e5d5da0d":"class_names[y_train[0]]","56868dc2":"model = tf.keras.Sequential([tf.keras.layers.Flatten(input_shape = [28, 28]),\n                            tf.keras.layers.Dense(300, activation = 'relu'),\n                            tf.keras.layers.Dense(100, activation = 'relu'),\n                            tf.keras.layers.Dense(10, activation = 'softmax')\n                            ])","5e08248b":"model.summary()","5906abb1":"model.layers","58addbc4":"hidden1 = model.layers[1]","9fe9147f":"hidden1.name","6c3a5492":"model.get_layer('dense') is hidden1","c4ca09d0":"weights, biases = hidden1.get_weights()","0dac402f":"weights","d4597855":"weights.shape","2c3fdcc2":"biases","2c68c329":"biases.shape","30136e8e":"model.compile(loss=tf.losses.sparse_categorical_crossentropy,\n             optimizer = 'sgd',\n             metrics=['accuracy'])","a1cb4eed":"history  = model.fit(x_train, y_train, epochs=30,\n                    validation_data=(x_validation, y_validation))","104a522e":"pd.DataFrame(history.history).plot(figsize = (10, 8))\nplt.grid(True)\nplt.gca().set_ylim(0, 1)\nplt.show()","20f68d36":"model.evaluate(x_test, y_test)","38adcd90":"x_new = x_test[:3]\ny_probabilities = model.predict(x_new)\ny_probabilities.round(2)","c2283967":"y_predict = model.predict_classes(x_new)\ny_predict","ccf666f9":"np.array(class_names)[y_predict]","e3c8c960":"y_new = y_test[:3]\ny_new","457eec44":"np.array(class_names)[y_new]","c6e578fb":"All the parameters of a layer can be accessed using its get_weights() and\nset_weights() methods. For a Dense layer, this includes both the connection weights\nand the bias terms:","73f7b59b":"# Class Names ","16890b45":"<b style=\"color:red\">Note that Dense layers often have a lot of parameters. For example, the first hidden\nlayer has 784 \u00d7 300 connection weights, plus 300 bias terms, which adds up to\n235,500 parameters! This gives the model quite a lot of flexibility to fit the training\ndata, but it also means that the model runs the risk of overfitting, especially when you\ndo not have a lot of training data. We will come back to this later.<\/b>","9907eef6":"<b style=\"color:blue\"> Instead of passing a validation set using the validation_data\nargument, you could set validation_split to the ratio of the\ntraining set that you want Keras to use for validation. For example,\nvalidation_split=0.1 tells Keras to use the last 10% of the data\n(before shuffling) for validation. <\/b>","76bc039d":"With MNIST, when the label is equal to 5, it means that the image represents the handwritten digit 5.","4e432fca":"# Let's explains this above code line by line.\n* The first line creates a Sequential model. This is the simplest kind of Keras\nmodel for neural networks that are just composed of a single stack of layers connected\nsequentially. This is called the Sequential API.\n* Next, we build the first layer and add it to the model. It is a Flatten layer whose\nrole is to convert each input image into a 1D array: if it receives input data X, it\ncomputes X.reshape(-1, 1). This layer does not have any parameters; it is just\nthere to do some simple preprocessing. Since it is the first layer in the model, you\nshould specify the input_shape, which doesn\u2019t include the batch size, only the\nshape of the instances. Alternatively, you could add a keras.layers.InputLayer\nas the first layer, setting input_shape=[28,28].\n* Next we add a Dense hidden layer with 300 neurons. It will use the ReLU activation\nfunction. Each Dense layer manages its own weight matrix, containing all the\nconnection weights between the neurons and their inputs. It also manages a vector\nof bias terms (one per neuron). When it receives some input data, it use this equation (\n<b style=\"color:blue\">Computing the outputs of a fully connected layer\nhW, b X = \u03d5 XW + b<\/b>)\n* Then we add a second Dense hidden layer with 100 neurons, also using the ReLU\nactivation function.\n* Finally, we add a Dense output layer with 10 neurons (one per class), using the\nsoftmax activation function (because the classes are exclusive).\n","5f18ed4d":"# Summary Method\nThe model\u2019s summary() method displays all the model\u2019s layers,14 including each layer\u2019s\nname (which is automatically generated unless you set it when creating the layer), its\noutput shape (None means the batch size can be anything), and its number of parameters.\nThe summary ends with the total number of parameters, including trainable and\nnon-trainable parameters","158f6fd9":"When Loading MNIST or fashion MNIST using keras rather than sklearn, one <b style=\"color:red\"> Important <\/b> difference is that every images is represented as a 28x28 array rather than a 1D array of size 784. Moreover, the pixel intensities are represented as integers (from 0 to 255 ) rather than floats (from 0.0- to 255.0). Let's take a look at the shape adna data type of the trining set:","794d28ae":"# Plot the model","2e183e6f":"Using loss=\"sparse_categorical_crossentropy\" is equivalent to\nusing loss=keras.losses.sparse_categorical_crossentropy.\nSimilarly, specifying optimizer=\"sgd\" is equivalent to specifying\noptimizer=keras.optimizers.SGD(), and metrics=[\"accuracy\"]\nis equivalent to metrics=[keras.metrics.sparse_categori\ncal_accuracy] (when using this loss). We will use many other losses,\noptimizers, and metrics in this book; for the full lists, see\nhttps:\/\/keras.io\/losses, https:\/\/keras.io\/optimizers, and https:\/\/\nkeras.io\/metrics.","42877fa3":"Note that the dataset us already split into <b style=\"color:blue\">training datasets<\/b> and <b style=\"color:blue\">test datasets<\/b>, but there is no <b style=\"color:blue\">validation set<\/b>, so we'll create one now. Additonally,science we are gogong to train the neural network using Gradient Descent, we must scale the input features. For simplicty, we'll scale the pixel intensities down to the 0-1 range by dividing them by 255.0 ( <b style=\"color:red\"> This also converts them to floats <\/b>)","7dcd1689":"If you want to convert sparse labels (i.e., class indices) to one-hot\nvector labels, use the keras.utils.to_categorical() function. To\ngo the other way round, use the np.argmax() function with\naxis=1.\n\nRegarding the optimizer, \"sgd\" means that we will train the model using simple Stochastic\nGradient Descent. In other words, Keras will perform the backpropagation\nalgorithm described earlier (i.e., reverse-mode autodiff plus Gradient Descent). We\nwill discuss more efficient optimizers in Chapter 11 (they improve the Gradient\nDescent part, not the autodiff).\n\nWhen using the SGD optimizer, it is important to tune the learning\nrate. So, you will generally want to use optimizer=keras.optimiz\ners.SGD(lr=???) to set the learning rate, rather than opti\nmizer=\"sgd\", which defaults to lr=0.01.\n\nFinally, since this is a classifier, it\u2019s useful to measure its \"accuracy\" during training\nand evaluation.","c9e0ad07":"# Explation \nThis code requires some explanation. First, we use the \"sparse_categorical_cross\nentropy\" loss because we have sparse labels (i.e., for each instance, there is just a target\nclass index, from 0 to 9 in this case), and the classes are exclusive. If instead we\nhad one target probability per class for each instance (such as one-hot vectors, e.g.\n[0., 0., 0., 1., 0., 0., 0., 0., 0., 0.] to represent class 3), then we would\nneed to use the \"categorical_crossentropy\" loss instead. If we were doing binary\nclassification (with one or more binary labels), then we would use the \"sigmoid\" (i.e.,\nlogistic) activation function in the output layer instead of the \"softmax\" activation\nfunction, and we would use the \"binary_crossentropy\" loss.","dff7acf2":"# Training and Evaluating the Model","3b284f7d":"# Relu Activation\nSpecifying activation=\"relu\" is equivalent to specifying activa\ntion=keras.activations.relu. Other activation functions are\navailable in the keras.activations package, we will use many of\nthem in this book. See https:\/\/keras.io\/activations\/ for the full list.","bc947da0":"# Weights","c67aeecb":"# Creating the Model","7dd0251b":"# Using the model to Predictions","20e80b43":"<b style=\"color:red\">Notice that the Dense layer initialized the connection weights randomly (which is\nneeded to break symmetry, as we discussed earlier), and the biases were initialized to\nzeros, which is fine. If you ever want to use a different initialization method, you can\nset kernel_initializer (kernel is another name for the matrix of connection weights) or bias_initializer when creating the layer. We will discuss initializers\nfurther in Chapter 11, but if you want the full list, see https:\/\/keras.io\/initializers\/. <\/b>","1636e6fd":"# Evaluating the Model\n","1ebe88b7":"First we need to load a dataset. In this tutorial  we will tackle Fashion MNIST datasets. This datasets contains 70,000 grayscale images 28 x 28 pixels each with 10 classes.But the images represent fashion items  rather than handwritten digits, so each class is more diverse, and the problem truns out to be significantly more challenging than MNIST.\nFor Example, a simple linear model model reaches about 92% accuracy on MNIST, but only about 83% on Fashion MNIST.","7a7ee066":"We pass it the input features (X_train) and the target classes (y_train), as well as the\nnumber of epochs to train (or else it would default to just 1, which would definitely\nnot be enough to converge to a good solution). We also pass a validation set (this is\noptional). Keras will measure the loss and the extra metrics on this set at the end of\neach epoch, which is very useful to see how well the model really performs. If the performance\non the training set is much better than on the validation set, \nyour model is probably overfitting the training set (or there is a bug, such as a data mismatch\nbetween the training set and the validation set).\nAnd that\u2019s it! The neural network is trained.15 At each epoch during training, Keras\ndisplays the number of instances processed so far (along with a progress bar), the\nmean training time per sample, and the loss and accuracy (or any other extra metrics\nyou asked for) on both the training set and the validation set. You can see that the\ntraining loss went down, which is a good sign, and the validation accuracy reached\n89.26% after 30 epochs. That\u2019s not too far from the training accuracy, so there does\nnot seem to be much overfitting going on.","f5dcae4b":"# Biases","d9852011":"If the training set was very skewed, with some classes being overrepresented and others\nunderrepresented, it would be useful to set the class_weight argument when\ncalling the fit() method, which would give a larger weight to underrepresented\nclasses and a lower weight to overrepresented classes. These weights would be used by\nKeras when computing the loss. If you need per-instance weights, set the sam\nple_weight argument (if both class_weight and sample_weight are provided, Keras\nmultiplies them). Per-instance weights could be useful if some instances were labeled\nby experts while others were labeled using a crowdsourcing platform: you might want\nto give more weight to the former. You can also provide sample weights (but not class\nweights) for the validation set by adding them as a third item in the validation_data\ntuple.","51913f27":"Here, the classifier actually classified all three images correctly.","a81256ef":"# Load datasets from keras"}}