{"cell_type":{"35711c0a":"code","7fccadae":"code","df39e59f":"code","3d39d6ea":"code","06445824":"code","279fefe9":"code","485eeb7a":"code","1094ea88":"code","49810137":"code","17461b82":"code","e9e9bcb6":"code","8dd7457d":"code","b2b9cc3c":"code","704cd323":"code","40a2b47b":"code","e925c9de":"code","22ec0fcd":"code","69b241e9":"code","83b9fd3e":"code","5f0a7445":"code","9b5ca4f0":"code","fc1e92cd":"code","88fd483e":"code","30472647":"code","c27a71a6":"markdown","22462c37":"markdown","04e129d5":"markdown","33261709":"markdown"},"source":{"35711c0a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7fccadae":"import matplotlib.pyplot as plt #\uc2dc\uac01\ud654 \ub3c4\uad6c module\npd.set_option('max_columns',150,'max_rows',20) # \uacb0\uacfc\uac12\uc744 \uba87\uc904\uae4c\uc9c0 \ucd9c\ub825\ud560\uc9c0 \uc124\uc815\n\n# #\uac80\uc740 \ubc30\uacbd\uc5d0\uc11c \uadf8\ub798\ud504 \uc694\uc18c\ub4e4\uc744 \ubc1d\uc740 \uc0c9\uc73c\ub85c \ubcc0\ud658 (dark mode\uc5d0\uc11c\ub9cc \uc0ac\uc6a9)\n# params = {\"text.color\" : \"g\",\n#           \"ytick.color\" : \"w\",\n#           \"xtick.color\" : \"w\",\n#           \"axes.titlecolor\" : \"w\",\n#           \"axes.labelcolor\" : \"w\",\n#           \"axes.edgecolor\" : \"w\"}\n# plt.rcParams.update(params)\n","df39e59f":"# train = pd.read_csv('\/kaggle\/input\/dlp-private-competition-2nd-ai-study\/train.csv') # train data \ud638\ucd9c\n# test = pd.read_csv('\/kaggle\/input\/dlp-private-competition-2nd-ai-study\/test.csv') # test data \ud638\ucd9c\n# submission = pd.read_csv('\/kaggle\/input\/dlp-private-competition-2nd-ai-study\/submission.csv') # submission data \ud638\ucd9c\n\n# train.head() # train data \ud615\ud0dc \uc704\uc5d0\uc11c 5\uc904\uae4c\uc9c0 \uc870\ud68c, \ub370\uc774\ud130 \uc804\ucc98\ub9ac\ub97c \uc2dc\uc791\ud558\uae30 \uc804 \uac01 column description \ud655\uc778\n\ntrain = pd.read_csv('\/kaggle\/input\/dlp-private-competition-2nd-ai-study\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/dlp-private-competition-2nd-ai-study\/test.csv')\nsubmission = pd.read_csv('\/kaggle\/input\/dlp-private-competition-2nd-ai-study\/submission.csv')\n\ntrain.head()","3d39d6ea":"test.head() #test set \ud615\ud0dc \ud655\uc778\n\n\n\n\n","06445824":"# train.shape, test.shape, submission.shape # 3\uac00\uc9c0 set\uc758 \ud06c\uae30(\ud589,\uc5f4 \uac1c\uc218) \ud655\uc778\n\n\ntrain.shape, test.shape, submission.shape","279fefe9":"print(round(train.isnull().sum() \/ len(train) *100,2).sort_values(ascending=False)[:10])\nprint(round(test.isnull().sum() \/ len(train) *100,2).sort_values(ascending=False)[:10])","485eeb7a":"# isnull().sum(): \uac01 columns\uc758 \ube48 \uac12\uc758 \uac1c\uc218 \ud655\uc778\n# len(train): train \uc758 \ud589 \uac1c\uc218\n# * 100: \ube44\uc728\uc744 \ubc31\ubd84\uc728\ub85c \ubc14\uafd4\uc90c\n# round(train, n) data\ub97c \uc18c\uc22b\uc810 n\ubc88\uc9f8 \uc790\ub9ac\uae4c\uc9c0 \ud45c\uae30\n# sort_values(ascending=False): data \ub0b4\ub9bc\ucc28\uc21c \uc815\ub82c\n# [:10] \uac00\uc7a5 \ud070 10\uac1c data \ud45c\uc2dc\nprint(round(train.isnull().sum() \/ len(train) *100,2).sort_values(ascending=False)[:10])\nprint(round(test.isnull().sum() \/ len(train) *100,2).sort_values(ascending=False)[:10])\n\n\n","1094ea88":"# null_col: train data\uc5d0\uc11c \ube48\uac12\uc774 \uac00\uc7a5 \ub9ce\uc740 10\uac1c column \uc9c0\uc815\n# for(\ubc18\ubcf5\ubb38)\ubb38\uc73c\ub85c train, test set\uc758 \ube48 data\ub97c \uac01 column\uc758 \ucd5c\ube48\uac12(mode)\uc73c\ub85c \ucc44\uc6cc\uc90c\n# [0]: \ucd5c\ube48\uac12 \uc911 \uac00\uc7a5 \uc55e\uc5d0 \ub098\uc628 \uac12 \uc0ac\uc6a9\n\nnull_col = ['C78','C50','C18','C82','C38','C5','C88','C76','C110','C10']\nfor each in null_col:\n    train[each] = train[each].fillna(train[each].mode()[0])\n    test[each] = test[each].fillna(test[each].mode()[0])\n\n\n","49810137":"# # column\uc911 index\uc640 END_TM\uc740 \ubc18\ub3c4\uccb4 \ub450\uaed8\uc5d0 \uc601\ud5a5\uc744 \ubbf8\uce58\uae30 \ud798\ub4e4\uae30 \ub54c\ubb38\uc5d0 \uc0ad\uc81c\n\n# train = train.drop(['index', 'END_TM'],1)\n# test = test.drop(['index', 'END_TM'],1)\n\n# train.head() # Column\uc774 \uc798 \uc9c0\uc6cc\uc84c\ub098 \ud655\uc778\n\ntrain = train.drop(['index', 'END_TM'],1)\ntest = test.drop(['index', 'END_TM'],1)\n\ntrain.head()\n\n","17461b82":"# # data.unique(): \uac01 column\uc5d0 \ubb34\uc2a8 \uac12\ub4e4\uc774 \uc788\ub294\uc9c0(\ubaa9\ub85d) list\ub85c \ud655\uc778\n\n# print(train['A'].unique())\n# print(test['A'].unique())\n\nprint(train['A'].unique())\nprint(test['A'].unique())","e9e9bcb6":"# # set \ud615\ud0dc\ub85c test set A\uc5f4\uc5d0\ub294 \uc788\uc9c0\ub9cc train set A\uc5f4\uc5d0\ub294 \uc5c6\ub294 \uc694\uc18c\ub97c \uc120\ubcc4\n# # \ud559\uc2b5\ub3c4 \uc548 \uc2dc\ud0a4\uace0 \uc608\uce21\uc744 \ud558\ub77c\uace0 \ud560 \uc218\ub294 \uc5c6\uc74c\n\n# only_test = set(test['A']).difference(set(train['A']))\n# only_test\n\nonly_test = set(test['A']).difference(set(train['A']))\nonly_test","8dd7457d":"# # \ud559\uc2b5\uc5d0 \ubd80\uc801\ud569\ud558\ubbc0\ub85c A\uc5f4 \uc0ad\uc81c\n\n# train = train.drop(['A'],1)\n# test = test.drop(['A'],1)\n\ntrain = train.drop(['A'],1)\ntest = test.drop(['A'],1)\n\ntrain.head()\n","b2b9cc3c":"# # B\uc5f4\ub3c4 \ub611\uac19\uc774 .unique()\ub85c \ubaa9\ub85d \uc870\ud68c\n\nprint(train['B'].unique())\nprint(test['B'].unique())\n\n\n","704cd323":"# # one hot encoding\uc744 \ud1b5\ud574 \ubb38\uc790 \ub370\uc774\ud130\ub97c \uc22b\uc790 \ub370\uc774\ud130\ub85c \ubcc0\ud658 (\ucef4\ud4e8\ud130\uac00 \uc77d\uc744 \uc218 \uc788\uac8c)\n\n# train_B = pd.get_dummies(train['B'], prefix='B') # \ubaa9\ub85d \uc218 \ub9cc\ud07c columns \uc0dd\uc131\n# train = pd.concat([train, train_B], axis=1) # \uc22b\uc790\ub85c \ub098\ud0c0\ub0b8 column\ub4e4\uc744 train data\uc5d0 \uc5f4 \ubc29\ud5a5 \ubd99\uc784\n# train = train.drop(['B','B_F','B_J','B_H','B_K','B_G','B_D','B_C','B_E'],1)\n# # \uc22b\uc790\ub85c \ubc14\uafd4\uc92c\uc73c\ub2c8 B\uc5f4 \uc0ad\uc81c, \ubaa8\ub378\uc5d0 \ub123\uae30 \uc704\ud574 train, test\uc758 column \uac1c\uc218\uac00 \uac19\uc544\uc57c \ud574\uc11c\n# # one hot encoding \ud6c4 train\uc5d0 \ub0a8\ub294 B_F~B_E\uc5f4 \uc0ad\uc81c\n# train.head() # \ubcc0\ud658\ub410\ub294\uc9c0 \ud655\uc778\ntrain_B = pd.get_dummies(train['B'], prefix='B')\ntrain = pd.concat([train, train_B], axis=1)\ntrain = train.drop(['B','B_F','B_J','B_H','B_K','B_G','B_D','B_C','B_E'],1)\ntrain.head()","40a2b47b":"# test_B = pd.get_dummies(test['B'], prefix='B') # test\uc5d0\ub3c4 one hot encoding \uc801\uc6a9\n# test = pd.concat([test, test_B], axis=1)\n# test = test.drop(['B'],1)\n\n# test.head()\n\ntest_B = pd.get_dummies(test['B'], prefix='B')\ntest = pd.concat([test, test_B], axis=1)\ntest = test.drop(['B'],1)\ntest.head()\n","e925c9de":"# # [Feature Importance Finding Process - It takes 5~20min to run]\n# features = pd.DataFrame(0., columns = ['MSE_train','MSE_valid','diff_train','diff_valid','diff_sum' ],\n#                            index = ['C'+str(x) for x in range(1,118)])\n\n# from sklearn.model_selection import train_test_split\n# import lightgbm as lgb\n# from sklearn.metrics import mean_squared_error\n# lgbm = lgb.LGBMRegressor (objective = 'regression', num_leaves=144, \n#                           learning_rate=0.005,n_estimators=720, max_depth=13,\n#                           metric='rmse', is_training_metric=True, max_bin=55,\n#                           bagging_fraction=0.8, verbose=-1, bagging_freq=5, feature_fraction=0.9)\n\n# for each in range(1,118):\n#     train_tmp = train.drop('C'+str(each),1)\n    \n#     y = train_tmp['Y']\n#     X = train_tmp.drop(['Y'],1)\n    \n#     X_train, X_valid, y_train, y_valid = train_test_split (X,y, random_state=0)\n\n#     lgbm.fit(X_train, y_train)\n    \n#     print(str(each))\n#     pred_train = lgbm.predict(X_train)\n#     train_mse = mean_squared_error(pred_train, y_train)\n#     print(train_mse)\n    \n#     pred_valid = lgbm.predict(X_valid)\n#     valid_mse = mean_squared_error(pred_valid, y_valid)\n#     print(valid_mse)\n    \n#     ['MSE_train', 'MSE_valid', 'diff_train', 'diff_valid', 'diff_sum']\n#     features['MSE_train']['C'+str(each)] = train_mse\n#     features['MSE_valid']['C'+str(each)] = valid_mse  \n#     features['diff_train']['C' + str(each)] = 8.576 - train_mse\n#     features['diff_valid']['C' + str(each)] = 16.414 - valid_mse\n#     features['diff_sum']['C' + str(each)] = (8.576 - train_mse)+(16.414 - valid_mse)\n\n# features.to_csv(\"result_feature_importance.csv\", index=False)    \n# plt.plot(features)\n# plt.show()","22ec0fcd":"# # feature importance \ud655\uc778 \uacb0\uacfc, C59\uc5f4\uc740 \uc608\uce21\uc758 \uc131\ub2a5\uc744 \ub5a8\uc5b4\ub728\ub9ac\ubbc0\ub85c \uc5f4 \uc0ad\uc81c\n# train = train.drop(['C59'],1)\n# test = test.drop(['C59'],1)","69b241e9":"# train['Y'].describe() # \uc608\uce21\ud560 Y\uac12\uc758 \ud3c9\uade0, \ud45c\uc900\ud3b8\ucc28, \ucd5c\ub313\uac12, 4\ubd84\uc704\uac12, \ucd5c\uc19f\uac12 \ud655\uc778\ntrain['Y'].describe()\n\n","83b9fd3e":"fig, ax = plt.subplots() # \uc608\uce21\ud560 Y\uac12\uc758 \ubd84\ud3ec \uc2dc\uac01\ud654\nax.scatter(x=train.index, y=train['Y'])\nplt.ylabel('Thickness', fontsize=13)\nplt.show()\n\n\n","5f0a7445":"#  # \ub450\uaed8\uac12\uc774 0\uc778 \ubc18\ub3c4\uccb4\ub294 \uc138\uc0c1\uc5d0 \uc874\uc7ac\ud558\uc9c0 \uc54a\uc544\uc11c Y=0\uc778 \ud589 \uc0ad\uc81c\n# print(train.shape)\n# count = 0\n# for x in range(0,11618):\n#     if train['Y'][x] == 0:\n#         train = train.drop(x)\n#         count +=1\n\n# train.shape, count # Y=0\uc778 \ud589 \uc0ad\uc81c \uc804\ud6c4 shape \ube44\uad50, \uc0ad\uc81c\ub41c \ud589\uc758 \uac1c\uc218 count\n\nprint(train.shape)\ncount = 0\nfor x in range(0,11618):\n    if train['Y'][x] == 0:\n        train = train.drop(x)\n        count +=1\ntrain.shape, count        \n\n","9b5ca4f0":"from sklearn.model_selection import train_test_split # train set \uc744 4\uac1c\ub85c \ubd84\ud560\ud574\uc8fc\ub294 \ubaa8\ub4c8\nimport lightgbm as lgb # \uc694\uc998 \uc120\ud638\ud558\ub294 \uac00\ubccd\uace0 \uc131\ub2a5\uc88b\uc740 \ubaa8\ub378\nfrom sklearn.metrics import mean_squared_error # \ud3c9\uac00\ucc99\ub3c4\uc778 MSE \ubaa8\ub4c8\n\n# y = train['Y']  # \uacb0\uacfc\uac12\n# X = train.drop(['Y'],1) # \uacb0\uacfc\uac12\uc744 \uc81c\uc678\ud55c \uc608\uce21\uc5d0 \uc0ac\uc6a9\ub418\ub294 \uc790\ub8cc\n\ny = train['Y']\nX = train.drop(['Y'],1)\n\n#\ud6c8\ub828\uc7ac\ub8cc,\uac80\uc99d\uc7ac\ub8cc, \ud6c8\ub828 \ub2f5, \uac80\uc99d \ub2f5 \uc73c\ub85c 4\ubd84\ud560 (\ubcf4\ud1b5 \ud6c8\ub828\uc6a9 70%, \uac80\uc99d\uc6a9 30%)\nX_train, X_valid, y_train, y_valid = train_test_split (X,y, random_state=0)\n\n#\uc0ac\uc6a9\ud560 \ubaa8\ub378(Light GBM) \uc815\uc758\nlgbm = lgb.LGBMRegressor (objective = 'regression', num_leaves=144,\n                             learning_rate=0.005,n_estimators=720, max_depth=13,\n                             metric='rmse', is_training_metric=True, max_bin=55,\n                             bagging_fraction=0.8, verbose=-1, bagging_freq=5, feature_fraction=0.9)\n\n# # \ubaa8\ub378\ub85c \uc608\uce21(fitting)\n# lgbm.fit(X_train, y_train)\nlgbm.fit(X_train, y_train)\n\n# #\ud6c8\ub828 \uacb0\uacfc\uac12\uacfc \uc2e4\uc81c\uac12\uc758 \ud3b8\ucc28 \uc81c\uacf1 \ud3c9\uade0\uc73c\ub85c \ud3c9\uac00\npred_train = lgbm.predict(X_train)\ntrain_mse = mean_squared_error(pred_train, y_train)\nprint('Train_mse: ', train_mse)\n\n# #\uac80\uc99d \uacb0\uacfc\uac12\uacfc \uc2e4\uc81c\uac12\uc758 \ud3b8\ucc28 \uc81c\uacf1 \ud3c9\uade0\uc73c\ub85c \ud3c9\uac00\npred_valid = lgbm.predict(X_valid)\nvalid_mse = mean_squared_error(pred_valid, y_valid)\nprint('Valid_mse: ', valid_mse)","fc1e92cd":"# #test \ub370\uc774\ud130\ub97c \ubaa8\ub378\uc5d0 \uc801\uc6a9\ud574\uc11c \uc608\uce21\npred_test = lgbm.predict(test)\nsubmission.head() # submission \uc758 \ud615\ud0dc \ud655\uc778\n\n\n","88fd483e":"pred_test\nsubmission_final = pd.concat([submission, pred_test], axis=1)","30472647":"# submission\uc758 Y\uac12\uc774 \ub2e4 0\uc774\ubbc0\ub85c Y\uc5f4 \uba3c\uc800 \uc81c\uac70\nsubmission = submission.drop('Y',1)\npred_test = pd.DataFrame(pred_test)  # DataFrame \uc73c\ub85c \ubc14\uafd4\uc8fc\uae30 \uc804\uc5d4 pred_test \uac00 array \ud615\ud0dc\n\n# test set\uc758 \uc608\uce21\uac12\uc744 submission Y\uc5f4 \ube80 \uc790\ub9ac\uc5d0 \ubcd1\ud569, axis=1 : \uc5f4 \ubc29\ud5a5\uc744 \uc758\ubbf8\nsubmission_final = pd.concat([submission, pred_test], axis=1)\n# \ubcd1\ud569\ud55c \uc608\uce21\uac12 column \uc774\ub984\uc744 Y\ub85c \uc9c0\uc815\nsubmission_final.columns = ['index','Y']\n# \uc81c\ucd9c\ud560 csv \ud615\ud0dc\uc758 \ud30c\uc77c \uc0dd\uc131\nsubmission_final.to_csv('submission_final.csv', index=False)\n# \uacb0\uacfc\ud30c\uc77c\uc774 \uc54c\ub9de\uc740 \ud615\ud0dc\ub85c \ub098\uc654\ub294\uc9c0 \ud655\uc778\nsubmission_final.head()\n\n\n\n","c27a71a6":"# Model Training & Scoring","22462c37":"# Result Submission ","04e129d5":"# Feature Engineering","33261709":"# Load Dataset"}}