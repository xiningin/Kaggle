{"cell_type":{"55886cf4":"code","06bcb14e":"code","b57be850":"code","4ebb68d0":"code","4aac5257":"code","ab099cd4":"code","bc957d1e":"code","9838f24f":"code","b84ee459":"code","b00cf700":"code","12358494":"code","b4ff6945":"code","24d83f01":"code","86fd7a01":"code","e698d377":"code","b8be0ea8":"code","d8b75d0e":"code","ba527e2b":"code","358d6e5e":"code","3a700cc0":"code","f745990a":"code","3f9d121b":"code","db00caa0":"code","0d367961":"code","35dd55a2":"code","8c2312a6":"code","f0e39c5d":"code","f35a9302":"code","4effd0cd":"code","fbc0a911":"code","cd2fe779":"code","7bf33d67":"code","33514d5a":"code","4fdf2e51":"code","78d85d28":"code","b92d1593":"code","f4231edd":"code","0da8e018":"code","a3721467":"code","6065a742":"code","ae7bce87":"code","aa87759b":"code","086de01a":"code","30cda1a9":"code","09f47951":"code","a65a35ea":"code","4118d138":"code","8f75ef53":"code","79c25896":"code","9de503f9":"code","0c6c1682":"code","c0720775":"code","d3879955":"code","285a6a9e":"code","f61ef8d8":"markdown","c416c722":"markdown","4bfd0cdf":"markdown","f20cd706":"markdown","03d2eb05":"markdown","0a358364":"markdown","117b2ff8":"markdown","c43cc3f4":"markdown","de6333d1":"markdown","284cc271":"markdown","42b3f474":"markdown","806d1602":"markdown","dd011e47":"markdown","df7c7c52":"markdown","3a8caa81":"markdown","ba9ee550":"markdown","f67dc648":"markdown","0c27ed6a":"markdown","7f9db378":"markdown","2a2c85a9":"markdown","3871c866":"markdown","31f22d62":"markdown","1a7e775b":"markdown","088d0c69":"markdown","a3664772":"markdown","00614029":"markdown"},"source":{"55886cf4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","06bcb14e":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport folium\nfrom folium.plugins import HeatMap as foliumHeatMapPlugin\nimport statsmodels.api as sm\nfrom numpy.polynomial.polynomial import polyfit\nfrom sklearn.linear_model import LinearRegression\nimport tensorflow as tf\n\nfrom sklearn.model_selection import train_test_split\n\nfrom tensorflow import keras\nfrom keras.layers import Dense, Activation, BatchNormalization","b57be850":"\nwildfitePath = '..\/input\/california-wildfire-incidents-20132020\/California_Fire_Incidents.csv'\nwildfireData = pd.read_csv(wildfitePath)","4ebb68d0":"wildfireData.head(5).style","4aac5257":"wildfireData.info()","ab099cd4":"wildfireData.describe()","bc957d1e":"sums = wildfireData.groupby(['ArchiveYear']).AcresBurned.sum()\nplt.bar(sums.index,sums)\nplt.title('Acres Burned by Year')\nplt.xlabel('Year')\nplt.ylabel('Acres Burned')\nplt.show()","9838f24f":"wildfireData['log_value'] = np. log1p(wildfireData['AcresBurned'])\ngraph = sns.stripplot(x = 'ArchiveYear', y = 'log_value',data = wildfireData, jitter=True, alpha = .30)\ngraph.set(xlabel ='Year', ylabel = 'log Acres Burned', title ='Year Vs log Acres Burned')\nplt.show()","b84ee459":"graph = sns.stripplot(x = 'ArchiveYear', y = 'AcresBurned',data = wildfireData, jitter=True, alpha = .30)\ngraph.set(xlabel ='Year', ylabel = 'Acres Burned', title ='Acres Burned by Year')\nplt.show()","b00cf700":"df2013 = wildfireData.loc[wildfireData['ArchiveYear'] == 2013,'AcresBurned'].values\ndf2014 = wildfireData.loc[wildfireData['ArchiveYear'] == 2014,'AcresBurned'].values\ndf2015 = wildfireData.loc[wildfireData['ArchiveYear'] == 2015,'AcresBurned'].values\ndf2016 = wildfireData.loc[wildfireData['ArchiveYear'] == 2016,'AcresBurned'].values\ndf2017 = wildfireData.loc[wildfireData['ArchiveYear'] == 2017,'AcresBurned'].values\ndf2018 = wildfireData.loc[wildfireData['ArchiveYear'] == 2018,'AcresBurned'].values\ndf2019 = wildfireData.loc[wildfireData['ArchiveYear'] == 2019,'AcresBurned'].values\n\nplt.boxplot([df2013,df2014,df2015,df2016,df2017,df2018,df2019],labels=['2013','2014','2015','2016','2017','2018','2019'])\nplt.show()","12358494":"df2013 = wildfireData.loc[wildfireData['ArchiveYear'] == 2013,'log_value'].values\ndf2014 = wildfireData.loc[wildfireData['ArchiveYear'] == 2014,'log_value'].values\ndf2015 = wildfireData.loc[wildfireData['ArchiveYear'] == 2015,'log_value'].values\ndf2016 = wildfireData.loc[wildfireData['ArchiveYear'] == 2016,'log_value'].values\ndf2017 = wildfireData.loc[wildfireData['ArchiveYear'] == 2017,'log_value'].values\ndf2018 = wildfireData.loc[wildfireData['ArchiveYear'] == 2018,'log_value'].values\ndf2019 = wildfireData.loc[wildfireData['ArchiveYear'] == 2019,'log_value'].values\n\nplt.boxplot([df2013,df2014,df2015,df2016,df2017,df2018,df2019],labels=['2013','2014','2015','2016','2017','2018','2019'])\nplt.show()","b4ff6945":"totalAcresBurned = np.sum(wildfireData['AcresBurned'])\nprint('Table Grouped by Year  \\n',sums)\nprint('Total Acres Burned = ',totalAcresBurned)","24d83f01":"RecordCount = wildfireData.groupby(['ArchiveYear']).UniqueId.count()\nplt.bar(RecordCount.index,RecordCount)\nplt.title('Number of Fire Instances By Year')\nplt.xlabel('Year')\nplt.ylabel('Number of Fire Instances')\nplt.show()","86fd7a01":"MajorIncident = wildfireData[wildfireData.MajorIncident == True]\nMajorIncidentCount = MajorIncident.groupby(['ArchiveYear']).MajorIncident.count()\n\nplt.bar(MajorIncidentCount.index,MajorIncidentCount)\nplt.title('Number of Major Fire Instances By Year')\nplt.xlabel('Year')\nplt.ylabel('Number of Major Fire Instances')\nplt.show()","e698d377":"RecordCountTotal = np.sum(RecordCount)\nMajorIncidentTotal = np.sum(MajorIncidentCount)\n\nprint('Number of Incidents Table \\n',RecordCount)\nprint('Total Number of Incidents',RecordCountTotal)\nprint('Number of Major Incidents Table \\n',MajorIncidentCount)\nprint('Total Number of Incidents',MajorIncidentTotal)","b8be0ea8":"onegraph, ax = plt.subplots()\n\nStructuresDestroyedCount = wildfireData.groupby(['ArchiveYear']).StructuresDestroyed.sum()\nax.plot(StructuresDestroyedCount.index,StructuresDestroyedCount, label = 'Number of Structures Destroyed')\n\nStructuresDamagedCount = wildfireData.groupby(['ArchiveYear']).StructuresDamaged.sum()\nax.plot(StructuresDamagedCount.index,StructuresDamagedCount, label = 'Number of Structures Damaged')\n\nStructuresThreatenedCount = wildfireData.groupby(['ArchiveYear']).StructuresThreatened.sum()\nax.plot(StructuresThreatenedCount.index,StructuresThreatenedCount, label = 'Number of Structures Threatened')\n\nax.set_xlabel('Year')\nax.set_ylabel('Structors Affected')\nax.set_title('Number of Structors Affected')\nax.legend()","d8b75d0e":"colors = (0,0,0)\nplt.scatter(wildfireData.AcresBurned, wildfireData.StructuresDestroyed,c=colors, alpha=0.1)\nplt.show()\n","ba527e2b":"plt.scatter(wildfireData.AcresBurned, wildfireData.StructuresDamaged,c=colors, alpha=0.1)\nplt.show()","358d6e5e":"plt.scatter(wildfireData.AcresBurned, wildfireData.StructuresThreatened,c=colors, alpha=0.1)\nplt.show()","3a700cc0":"onegraph, ax = plt.subplots()\n\nFatalitiesCount = wildfireData.groupby(['ArchiveYear']).Fatalities.sum()\nax.plot(FatalitiesCount.index,FatalitiesCount, label = 'Fatalities')\n\nInjuriesCount = wildfireData.groupby(['ArchiveYear']).Injuries.sum()\nax.plot(InjuriesCount.index,InjuriesCount, label = 'Injuries')\n\nax.set_xlabel('Year')\nax.set_ylabel('People Affected')\nax.set_title('People affected by Fires')\nax.legend()","f745990a":"colors = (0,0,0)\nplt.scatter(wildfireData.AcresBurned, wildfireData.Fatalities,c=colors, alpha=0.1)\nplt.show()","3f9d121b":"plt.scatter(wildfireData.AcresBurned, wildfireData.Injuries,c=colors, alpha=0.1)\nplt.show()","db00caa0":"plt.scatter(wildfireData.StructuresDestroyed, wildfireData.Injuries,c=colors, alpha=0.1)\nplt.show()","0d367961":"wildfireData['LengthofBurn'] = (pd.to_datetime(wildfireData.Extinguished) - pd.to_datetime(wildfireData.Started)).astype('timedelta64[h]')\nwildfireData['LengthofBurn']\n\ncleanedDates = wildfireData[(wildfireData.LengthofBurn > 0) & (wildfireData.LengthofBurn < 400000)]\nprint(cleanedDates.LengthofBurn.describe())","35dd55a2":"colors = (0,0,0)\n\nplt.scatter(cleanedDates.ArchiveYear, cleanedDates.LengthofBurn,c=colors, alpha=0.1)\n\nplt.title('Length of Burn by Year')\nplt.xlabel('Year')\nplt.ylabel('Length of Burn')\nplt.legend()\n\nplt.show()","8c2312a6":"plt.scatter(cleanedDates.AcresBurned, cleanedDates.LengthofBurn,c=colors, alpha=0.1)\nplt.show()","f0e39c5d":"AcresBurnedSum = wildfireData.groupby(['Counties']).AcresBurned.sum()\nUniqueIdCount = wildfireData.groupby(['Counties']).UniqueId.count()\nMajorIncident = wildfireData[wildfireData.MajorIncident == True]\nMajorIncidentCount = MajorIncident.groupby(['Counties']).MajorIncident.count()\n\nf = plt.figure()\nf.set_figwidth(15)\nf.set_figheight(5)\nplt.bar(AcresBurnedSum.index,AcresBurnedSum)\nplt.title('Acres Burned By County')\nplt.xlabel('County')\nplt.xticks(rotation=90)\nplt.ylabel('Acres Burned')\nplt.show()","f35a9302":"f = plt.figure()\nf.set_figwidth(15)\nf.set_figheight(5)\nplt.bar(UniqueIdCount.index,UniqueIdCount)\nplt.title('Number of Instances By County')\nplt.xlabel('County')\nplt.xticks(rotation=90)\nplt.ylabel('Number of Instances')\nplt.show()","4effd0cd":"f = plt.figure()\nf.set_figwidth(15)\nf.set_figheight(5)\nplt.bar(MajorIncidentCount.index,MajorIncidentCount)\nplt.title('Number of Major Fire Instances By County')\nplt.xlabel('County')\nplt.xticks(rotation=90)\nplt.ylabel('Number of Major Fire Instances')\nplt.show()","fbc0a911":"print(AcresBurnedSum)\nprint(UniqueIdCount)\nprint(wildfireData.Counties.unique())","cd2fe779":"# Longitude: 114\u00b0 8' W to 124\u00b0 24' W\n# Latitude: 32\u00b0 30' N to 42\u00b0 N\ncali = wildfireData[(wildfireData.Longitude < -115) & (wildfireData.Longitude < 125)]\ncali = cali[(cali.Latitude<43) & (cali.Latitude > 33)]\ncali = cali[cali.CanonicalUrl != '\/incidents\/2013\/8\/6\/tram-fire\/'] #Make mine\n\n\nplt.scatter(cali.Longitude,cali.Latitude,c=cali.Counties.astype('category').cat.codes, cmap = 'magma')\n\nplt.title('California Lat & Long')\nplt.xlabel('Longitude')\nplt.ylabel('Latitude')\n\nplt.show()\n","7bf33d67":"#Remove values that I have concerns about then run graphs again\n    #1636 Original Number of records\n    #1426 New Number of records\n\nprint(cleanedDates.CalFireIncident.describe())\n\n#Duration to long\/Short errors\nwildfireData = wildfireData[(wildfireData.LengthofBurn > 0) & (wildfireData.LengthofBurn < 400000)]\n\n#Area location errors\nwildfireData = wildfireData[(wildfireData.Longitude < -115) & (wildfireData.Longitude < 125)]\nwildfireData = wildfireData[(wildfireData.Latitude<43) & (wildfireData.Latitude > 33)]\n\nwildfireData.info()\nwildfireData.describe()","33514d5a":"x = np.array(wildfireData['AcresBurned']).reshape((-1, 1))\ny = np.array(wildfireData['Fatalities'])\nx = np.nan_to_num(x, copy=True, posinf=0)\ny = np.nan_to_num(y, copy=True, posinf=0)\n\nmodel = LinearRegression()\nmodel.fit(x, y)\nmodel = LinearRegression().fit(x, y)\nr_sq = model.score(x, y)\nintercept = model.intercept_\nslope = model.coef_\nprint('r score coefficient:', r_sq)\nprint('intercept:', intercept)\nprint('slope:', slope)\n\nplt.scatter(wildfireData.AcresBurned, wildfireData.Fatalities,c=colors, alpha=0.1)\ny = slope*x+intercept\nplt.plot(x, y)\nplt.plot()\nplt.show()","4fdf2e51":"x = np.array(wildfireData['AcresBurned']).reshape((-1, 1))\ny = np.array(wildfireData['Injuries'])\nx = np.nan_to_num(x, copy=True, posinf=0)\ny = np.nan_to_num(y, copy=True, posinf=0)\n\nmodel = LinearRegression()\nmodel.fit(x, y)\nmodel = LinearRegression().fit(x, y)\nr_sq = model.score(x, y)\nintercept = model.intercept_\nslope = model.coef_\nprint('r score coefficient:', r_sq)\nprint('intercept:', intercept)\nprint('slope:', slope)\n\nplt.scatter(x, y,c=colors, alpha=0.1)\ny = slope*x+intercept\nplt.plot(x, y)\nplt.plot()\nplt.show()","78d85d28":"x = np.array(wildfireData['AcresBurned']).reshape((-1, 1))\ny = np.array(wildfireData['StructuresThreatened'])\nx = np.nan_to_num(x, copy=True, posinf=0)\ny = np.nan_to_num(y, copy=True, posinf=0)\n\nmodel = LinearRegression()\nmodel.fit(x, y)\nmodel = LinearRegression().fit(x, y)\nr_sq = model.score(x, y)\nintercept = model.intercept_\nslope = model.coef_\nprint('r score coefficient:', r_sq)\nprint('intercept:', intercept)\nprint('slope:', slope)\n\nplt.scatter(x, y,c=colors, alpha=0.1)\ny = slope*x+intercept\nplt.plot(x, y)\nplt.plot()\n\nplt.title('Regression Analysis of Acres Burned vs Structures Threatened')\nplt.xlabel('Acres Burned')\nplt.ylabel('Structures Threatened')\nplt.show()","b92d1593":"x = np.array(wildfireData['AcresBurned']).reshape((-1, 1))\ny = np.array(wildfireData['StructuresDamaged'])\nx = np.nan_to_num(x, copy=True, posinf=0)\ny = np.nan_to_num(y, copy=True, posinf=0)\n\nmodel = LinearRegression()\nmodel.fit(x, y)\nmodel = LinearRegression().fit(x, y)\nr_sq = model.score(x, y)\nintercept = model.intercept_\nslope = model.coef_\nprint('r score coefficient:', r_sq)\nprint('intercept:', intercept)\nprint('slope:', slope)\n\nplt.scatter(x, y,c=colors, alpha=0.1)\ny = slope*x+intercept\nplt.plot(x, y)\nplt.plot()\nplt.show()","f4231edd":"x = np.array(wildfireData['AcresBurned']).reshape((-1, 1))\ny = np.array(wildfireData['StructuresDestroyed'])\nx = np.nan_to_num(x, copy=True, posinf=0)\ny = np.nan_to_num(y, copy=True, posinf=0)\n\nmodel = LinearRegression()\nmodel.fit(x, y)\nmodel = LinearRegression().fit(x, y)\nr_sq = model.score(x, y)\nintercept = model.intercept_\nslope = model.coef_\nprint('r score coefficient:', r_sq)\nprint('intercept:', intercept)\nprint('slope:', slope)\n\nplt.scatter(x, y,c=colors, alpha=0.1)\ny = slope*x+intercept\nplt.plot(x, y)\nplt.plot()\nplt.show()","0da8e018":"x = np.array(wildfireData['AcresBurned']).reshape((-1, 1))\ny = np.array(wildfireData['StructuresDamaged'])\nx = np.nan_to_num(x, copy=True, posinf=0)\ny = np.nan_to_num(y, copy=True, posinf=0)\n\nmodel = LinearRegression()\nmodel.fit(x, y)\nmodel = LinearRegression().fit(x, y)\nr_sq = model.score(x, y)\nintercept = model.intercept_\nslope = model.coef_\nprint('r score coefficient:', r_sq)\nprint('intercept:', intercept)\nprint('slope:', slope)\n\nplt.scatter(x, y,c=colors, alpha=0.1)\ny = slope*x+intercept\nplt.plot(x, y)\nplt.plot()\nplt.show()","a3721467":"wildfireDataNoSTR = wildfireData.drop(columns=['AdminUnit','CanonicalUrl','ConditionStatement','ControlStatement','Counties','CountyIds','Extinguished','FuelType','Location','Name','SearchDescription','SearchKeywords','Started','Status','UniqueId','Updated'], axis=1)\n\nwildfireDataNoSTR = wildfireDataNoSTR.fillna(0)\n\nwildfireDataNoSTR['Active'] = wildfireDataNoSTR['Active'].astype(int)\nwildfireDataNoSTR['CalFireIncident'] = wildfireDataNoSTR['CalFireIncident'].astype(int)\nwildfireDataNoSTR['Featured'] = wildfireDataNoSTR['Featured'].astype(int)\nwildfireDataNoSTR['Final'] = wildfireDataNoSTR['Final'].astype(int)\nwildfireDataNoSTR['MajorIncident'] = wildfireDataNoSTR['MajorIncident'].astype(int)\nwildfireDataNoSTR['Public'] = wildfireDataNoSTR['Public'].astype(int)\n\n#Droping useless columns\nwildfireDataNoSTR = wildfireDataNoSTR.drop(columns=['Active','Public','StructuresEvacuated','PercentContained'],axis=1)\n\nwildfireDataNoSTR.info()\nwildfireDataNoSTR.describe()","6065a742":"wildfireDataNoSTR1 = tf.keras.utils.normalize(wildfireDataNoSTR, axis = 1)\n\ntrain, test = train_test_split(wildfireDataNoSTR1, test_size=0.2)\n\ny_train = train.AcresBurned\nx_train = train.drop('AcresBurned', 1)\n\ny_test = test.AcresBurned\nx_test = test.drop('AcresBurned', 1)\n\nwildfireDataNoSTR1.AcresBurned.max() #410203\nwildfireDataNoSTR1.AcresBurned.min() #0","ae7bce87":"#rework\n\nmodel = tf.keras.models.Sequential()\nmodel.add(tf.keras.layers.Flatten())\n\n\nmodel.add(tf.keras.layers.Dense(128, activation = tf.nn.sigmoid))\nmodel.add(tf.keras.layers.Dense(128, activation = tf.nn.sigmoid)),BatchNormalization(axis=1),\nmodel.add(tf.keras.layers.Dense(410204, activation = tf.nn.sigmoid))\n\nmodel.compile(optimizer= 'SGD', loss = 'mean_squared_error',metrics = ['sparse_categorical_accuracy','accuracy'])\n\nmodel.fit(x_train, y_train, epochs = 10)","aa87759b":"wildfireDataMean1 = wildfireDataNoSTR\n\nwildfireDataMean1['OverUnderAverageAreaBurn'] = [1 if i > wildfireDataMean1['AcresBurned'].mean() else 0 for i in wildfireDataMean1.AcresBurned]\n\nprint(wildfireDataMean1['AcresBurned'].mean())\n\nwildfireDataMean1 = wildfireDataMean1.drop(columns=['AcresBurned'])\n\nwildfireDataMean1","086de01a":"wildfireDataMean1 = tf.keras.utils.normalize(wildfireDataMean1, axis = 1)\n\ntrain, test = train_test_split(wildfireDataMean1, test_size=0.2)\n\ntrain = tf.keras.utils.normalize(train, axis = 1)\n\ny_train = train.OverUnderAverageAreaBurn\nx_train = train.drop('OverUnderAverageAreaBurn', 1)\n\ny_test = test.OverUnderAverageAreaBurn\nx_test = test.drop('OverUnderAverageAreaBurn', 1)","30cda1a9":"model = tf.keras.models.Sequential()\nmodel.add(tf.keras.layers.Flatten())\n\nmodel.add(tf.keras.layers.Dense(32, input_dim=8, kernel_initializer='normal', activation = tf.nn.sigmoid))\nmodel.add(tf.keras.layers.Dense(32, activation = tf.nn.sigmoid)),\nmodel.add(tf.keras.layers.Dense(1, activation = tf.nn.sigmoid))\n\n#model.compile(loss='mse', optimizer='adam', metrics=['mse','mae'])\nmodel.compile(optimizer= 'adam', loss = 'binary_crossentropy',metrics = ['accuracy'])\n\nmodel.fit(x_train, y_train, epochs=10, batch_size=32)\n\n#90% accuracy to determin over under in mean because the training data has 90% under and 10% over so it is not learning just assuming that it is under","09f47951":"wildfireData50 = wildfireDataNoSTR\nwildfireData50['OverUnder50AreaBurn'] = [1 if i > 100 else 0 for i in wildfireData50.AcresBurned]\n\nwildfireData50 = wildfireData50.drop(columns=['AcresBurned'])\n\nwildfireData50","a65a35ea":"train, test = train_test_split(wildfireData50, test_size=0.2)\n\ntrain = tf.keras.utils.normalize(train, axis = 1)\n\ny_train = train.OverUnder50AreaBurn\nx_train = train.drop('OverUnder50AreaBurn', 1)\n\ny_test = test.OverUnder50AreaBurn\nx_test = test.drop('OverUnder50AreaBurn', 1)","4118d138":"x = np.array(wildfireData50)\ny = np.array(wildfireData50['OverUnder50AreaBurn'])\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, random_state = 42)\ny_train = np.reshape(y_train, (-1,1))\n\nmodel = tf.keras.models.Sequential()\nmodel.add(tf.keras.layers.Flatten())\n\nmodel.add(tf.keras.layers.Dense(3,input_dim=5, kernel_initializer='normal', activation = tf.nn.sigmoid))\nmodel.add(tf.keras.layers.Dense(32, activation = tf.nn.sigmoid)),\nmodel.add(tf.keras.layers.Dense(1, activation = tf.nn.sigmoid))\n\n#model.compile(loss='mse', optimizer='adam', metrics=['mse','mae'])\nmodel.compile(optimizer= 'adam', loss = 'binary_crossentropy',metrics = ['accuracy'])\n\nmodel.fit(x_train, y_train, epochs=10, batch_size=10)\n\ny_pred = model.predict(x_test)\n\nimport matplotlib.pyplot as plt \nplt.scatter(y_test, y_pred)\nplt.xlabel('True Values')\nplt.ylabel('Predictions')","8f75ef53":"predictions = model.predict([x_test])\n#y_test.count()\n#predictions.size\n\nnumtrues = 0\nprediction = 0\nfor i in range(y_test.size): \n   \n    print(y_test[i])\n    if predictions[i] <= .5:\n        prediction = 0\n    else:\n        prediction = 1\n        \n    print(prediction)\n    \n    if y_test[i] == prediction:\n        print(True)\n        numtrues += 1\n    else:\n        print(False)\n\nprint(numtrues\/y_test.size)\n","79c25896":"print(y_test)\nprint(predictions)","9de503f9":"wildfireDataNoSTR.info()\nwildfireDataNoSTR.describe()\n    #print(np.argmax(predictions[index]))","0c6c1682":"forestfirePath = '..\/input\/forest-forest-dataset\/forestfires.csv'\nforestfireData = pd.read_csv(forestfirePath)\nforestfireData.head(5).style","c0720775":"forestfireData.info()\n\nforestfireData.describe()","d3879955":"forestfireData['areaMean'] = [1 if i > .520000 else 0 for i in forestfireData.area]\n\nfor i,x in enumerate(forestfireData['day']):\n    if x == 'mon':\n        forestfireData.loc[i,'day'] = int(1)\n    elif x == 'tue':\n        forestfireData.at[i,'day'] = int(2)\n    elif x == 'wed':\n        forestfireData.at[i,'day'] = int(3)\n    elif x == 'thu':\n        forestfireData.at[i,'day'] = int(4)\n    elif x == 'fri':\n        forestfireData.at[i,'day'] = int(5)\n    elif x == 'sat':\n        forestfireData.at[i,'day'] = int(6)\n    elif x == 'sun':\n        forestfireData.at[i,'day'] = int(7)\n        \n        \nfor i,x in enumerate(forestfireData['month']):\n    if x == 'jan':\n        forestfireData.loc[i,'month'] = int(1)\n    elif x == 'feb':\n        forestfireData.at[i,'month'] = int(2)\n    elif x == 'mar':\n        forestfireData.at[i,'month'] = int(3)\n    elif x == 'apr':\n        forestfireData.at[i,'month'] = int(4)\n    elif x == 'may':\n        forestfireData.at[i,'month'] = int(5)\n    elif x == 'jun':\n        forestfireData.at[i,'month'] = int(6)\n    elif x == 'jul':\n        forestfireData.at[i,'month'] = int(7)\n    elif x == 'aug':\n        forestfireData.at[i,'month'] = int(8)\n    elif x == 'sep':\n        forestfireData.at[i,'month'] = int(9)\n    elif x == 'oct':\n        forestfireData.at[i,'month'] = int(10)\n    elif x == 'nov':\n        forestfireData.at[i,'month'] = int(11)\n    elif x == 'dec':\n        forestfireData.at[i,'month'] = int(12)\n\nforestfireDatatrain = forestfireData.drop(columns=['area'])\n\nforestfireDatatrain = forestfireDatatrain.astype({'day': np.float, 'month': np.float, 'areaMean': np.float})\n\nforestfireDatatrain = forestfireDatatrain.astype(np.float)\n\nforestfireDatatrain.info()\n\nforestfireDatatrain.describe()","285a6a9e":"x = np.array(forestfireDatatrain.drop(columns=['areaMean']))\ny = np.array(forestfireData['areaMean'])\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, random_state = 42)\ny_train = np.reshape(y_train, (-1,1))\n\nx_train = tf.convert_to_tensor(x_train)\nx_test = tf.convert_to_tensor(x_test)\ny_train = tf.convert_to_tensor(y_train)\ny_test = tf.convert_to_tensor(y_test)\n\nmodel = tf.keras.models.Sequential()\nmodel.add(tf.keras.layers.Flatten())\n\nmodel.add(tf.keras.layers.Dense(32,input_dim=12, kernel_initializer='normal', activation = tf.nn.sigmoid))\nmodel.add(tf.keras.layers.Dense(32, activation = tf.nn.sigmoid)),\nmodel.add(tf.keras.layers.Dense(1, activation = tf.nn.sigmoid))\n\n#model.compile(loss='mse', optimizer='adam', metrics=['mse','mae'])\nmodel.compile(optimizer= 'adam', loss = 'binary_crossentropy',metrics = ['accuracy'])\n\nmodel.fit(x_train, y_train, epochs=10, batch_size=32)\n\ny_pred = model.predict(x_test)\n\nimport matplotlib.pyplot as plt \nplt.scatter(y_test, y_pred)\nplt.xlabel('True Values')\nplt.ylabel('Predictions')","f61ef8d8":"# County data","c416c722":"# Fatalities","4bfd0cdf":"# Looking at instances","f20cd706":"# Attempt 2","03d2eb05":"# Attempt 3","0a358364":"# Attempt 1","117b2ff8":"val_loss, val_acc = model.evaluate(x_test, y_test)\nprint(val_loss, val_acc)","c43cc3f4":"# Import California forest fire data","de6333d1":"There is a strong corrolation beween fatalities and structors destroyed","284cc271":"# Prep data","42b3f474":"# Looking at Acrage affected","806d1602":"# Structures Damaged","dd011e47":"# Length of burn","df7c7c52":"# Looking at affects on people","3a8caa81":"# Quick View of the data","ba9ee550":"# Looking at Structers","f67dc648":"# Structures Damaged","0c27ed6a":"# Structures Destroyed","7f9db378":"# Bring in other data to compare","2a2c85a9":"Import Statements","3871c866":"# Latitude and Lonitude","31f22d62":"# Prep data","1a7e775b":"# Structures Threatened","088d0c69":"# Injuries","a3664772":"# Regression Analysis","00614029":"# Machine Learning"}}