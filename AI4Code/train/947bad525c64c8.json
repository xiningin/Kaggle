{"cell_type":{"68ac3b43":"code","19a1bfaf":"code","85c398fc":"code","901b28b9":"code","18e12e05":"code","9091bcc7":"code","4015d90c":"code","6147c0d8":"markdown","90a014fb":"markdown","703abee0":"markdown","63406112":"markdown","da25e503":"markdown","5422b51c":"markdown"},"source":{"68ac3b43":"import pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\nimport math\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\nfrom sklearn.ensemble import IsolationForest\nimport pickle\nimport warnings\nwarnings.filterwarnings('ignore')\nimport matplotlib.ticker as plticker\nfrom tqdm import tqdm\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn import svm\nfrom sklearn.cluster import KMeans\n\n!pip install emmv\nfrom emmv import emmv_scores\nfrom sklearn.cluster import KMeans\nfrom numpy import sqrt, array, random, argsort","19a1bfaf":"class AnamolyDetection():\n    def __init__(self, contamination = 0.07):\n        self.model_base_path = '.\/'\n    \n    def preporcess_data(self, data, rolling_windows_size = 15):\n        data.sort_values(by='date')\n        trans_dataset = data[['date', 'account_id', 'type', 'operation', 'amount']]\n        \n        \n        trans_dataset['date'] = pd.to_datetime(trans_dataset['date'], format='%y%m%d')\n        trans_dataset['account_id'] = trans_dataset['account_id'].astype(str)\n\n        \n        # Change the transaction from Czech types to English.\n        replace_trans_type = {'PRIJEM': 'CREDIT', 'VYDAJ': 'DEBIT', 'VYBER': 'DEBIT'}\n        trans_dataset['type'] = trans_dataset['type'].replace(replace_trans_type)\n\n        # Change the transaction Operation from Czech to English.\n        replace_trans_operation = {'VYBER KARTOU' : 'ATM_CARD', 'VKLAD' : 'CASH',\n        'PREVOD Z UCTU' : 'TRANSFER', 'VYBER' : 'CASH',\n        'PREVOD NA UCET' : 'TRANSFER'}\n\n        trans_dataset['operation'] = trans_dataset['operation'].replace(replace_trans_operation)\n        \n        trans_dataset = pd.get_dummies(trans_dataset, columns = ['type','operation'])\n        \n        trans_dataset = trans_dataset.set_index('date')\n        trans_dataset['sum_'+str(rolling_windows_size)+'_days_amount'] = trans_dataset.groupby('account_id')['amount'].transform(lambda s: s.rolling(timedelta(days=rolling_windows_size)).mean())\n        trans_dataset['sum_'+str(rolling_windows_size)+'_days_CREDIT'] = trans_dataset.groupby('account_id')['type_CREDIT'].transform(lambda s: s.rolling(timedelta(days=rolling_windows_size)).sum())\n        trans_dataset['sum_'+str(rolling_windows_size)+'_days_DEBIT'] = trans_dataset.groupby('account_id')['type_DEBIT'].transform(lambda s: s.rolling(timedelta(days=rolling_windows_size)).sum())\n        trans_dataset['sum_'+str(rolling_windows_size)+'_days_CASH'] = trans_dataset.groupby('account_id')['operation_CASH'].transform(lambda s: s.rolling(timedelta(days=rolling_windows_size)).sum())\n        trans_dataset['sum_'+str(rolling_windows_size)+'_days_TRANSFER'] = trans_dataset.groupby('account_id')['operation_TRANSFER'].transform(lambda s: s.rolling(timedelta(days=rolling_windows_size)).sum())\n        trans_dataset['sum_'+str(rolling_windows_size)+'_days_ATM_CARD'] = trans_dataset.groupby('account_id')['operation_ATM_CARD'].transform(lambda s: s.rolling(timedelta(days=rolling_windows_size)).sum())\n\n        trans_dataset.drop(['amount', 'type_CREDIT', 'type_DEBIT', 'operation_CASH', 'operation_TRANSFER', 'operation_ATM_CARD'], axis=1, inplace=True)\n        return trans_dataset\n        \n    def detect_anomaly(self, trans_data, account_id, model_name = 'IF' ,show_anamoly_points = False, save_model=False):\n        trans_dataset = trans_data.copy()\n        trans_dataset.drop(['account_id'], axis=1, inplace=True)\n        \n        \n        if model_name == 'IF':\n            model = IsolationForest(contamination=0.07, random_state=38)\n        elif model_name == 'LOF':\n            model = LocalOutlierFactor(contamination=0.07, novelty=True)\n        elif model_name == 'OCSVM':\n            model = svm.OneClassSVM(nu=0.07)\n        \n        model.fit(trans_dataset)\n        \n        emmv_score = emmv_scores(model, trans_dataset)\n        \n        if save_model == True:\n            filename = self.model_base_path + str(account_id) +'.sav'\n            pickle.dump(model, open(filename, 'wb'))\n            \n        suspecious = model.predict(trans_dataset)\n        scores = model.decision_function(trans_dataset)\n        \n        trans_dataset['scores']  = scores\n        trans_dataset['anamoly'] = suspecious\n        \n        if show_anamoly_points == True:\n            self.show_anamoly_points(trans_dataset)\n            \n        trans_dataset['account_id'] = account_id\n        trans_dataset['date'] = trans_dataset.index\n        trans_dataset['em'] = emmv_score['em']\n        trans_dataset['mv'] = emmv_score['mv']\n        \n        return trans_dataset\n    \n    \n    def detect_anomaly_knn(self, trans_data, account_id, show_anamoly_points = False):\n        trans_dataset = trans_data.copy()\n        trans_dataset.drop(['account_id'], axis=1, inplace=True)\n        \n        kmeans = KMeans(n_clusters = 1).fit(y)\n        center = kmeans.cluster_centers_\n        distance = sqrt((trans_dataset - center)**2)\n        \n        trans_dataset['scores']  = distance\n        trans_dataset['anamoly'] = 1\n        \n        if show_anamoly_points == True:\n            self.show_anamoly_points(trans_dataset)\n            \n        trans_dataset['account_id'] = account_id\n        trans_dataset['date'] = trans_dataset.index\n        trans_dataset['em'] = 0\n        trans_dataset['mv'] = 0\n        \n        total_samples = trans_dataset.shape[0]\n        n_instace_anomaly = total_samples * 0.07\n        n_instace_anomaly = int(n_instace_anomaly) * -1\n                \n        if n_instace_anomaly < 1:\n            pass\n        else:\n            order_index = argsort(analysed_dataset['scores'], axis = 0)\n            indexes = order_index[n_instace_anomaly:]\n            trans_dataset.loc[indexes,'anamoly'] = -1\n            trans_dataset = trans_dataset.sort_values('date')\n        \n        return trans_dataset\n            \n    def show_anamoly_points(self, test_data):\n        # visualisation of anomaly throughout time (viz 1)\n        fig, ax = plt.subplots(figsize=(17,8))\n        \n        plt.yticks(rotation = 0)\n        plt.xticks(rotation= 90)\n        \n        trans_dataset_visualization = test_data.copy()\n        trans_dataset_visualization['date'] = test_data.index\n        \n        trans_dataset_visualization['date'] = trans_dataset_visualization['date'].dt.strftime('%Y-%m-%d')\n        \n        suspecious_data = trans_dataset_visualization.loc[trans_dataset_visualization['anamoly'] == -1, ['date', 'scores']] #anomaly\n        ax.plot(trans_dataset_visualization['date'], trans_dataset_visualization['scores'], color='blue')\n        ax.scatter(suspecious_data['date'], suspecious_data['scores'], color='red')\n        \n        loc = plticker.MultipleLocator(base=1.0) # this locator puts ticks at regular intervals\n        ax.xaxis.set_major_locator(loc)\n        \n        plt.show()","85c398fc":"obj_anamoly_detect = AnamolyDetection()\ntrans_dataset = pd.read_csv('..\/input\/bank-data-loan-default\/trans.asc', delimiter=';', low_memory=False)\ntrans_dataset = pd.DataFrame(trans_dataset)\npre_processed_data = obj_anamoly_detect.preporcess_data(trans_dataset)","901b28b9":"analysed_dataset = pd.DataFrame()\n\naccount_list = set(list(trans_dataset['account_id']))\nfor account_id in tqdm(account_list):\n    account_id = str(account_id)\n    account_trans_data = pre_processed_data[pre_processed_data['account_id'] == account_id]\n    analyzed_data = obj_anamoly_detect.detect_anomaly(account_trans_data, \n                                                      account_id,\n                                                      model_name = 'IF',\n                                                      show_anamoly_points = False, \n                                                      save_model=False)\n    analysed_dataset = analysed_dataset.append(analyzed_data, ignore_index=True)\n    \nanalysed_dataset.to_csv('IF_analysed_dataset.csv', index=False)","18e12e05":"analysed_dataset = pd.DataFrame()\n\naccount_list = set(list(trans_dataset['account_id']))\nfor account_id in tqdm(account_list):\n    account_id = str(account_id)\n    account_trans_data = pre_processed_data[pre_processed_data['account_id'] == account_id]\n    analyzed_data = obj_anamoly_detect.detect_anomaly_knn(account_trans_data, \n                                                      account_id,\n                                                      show_anamoly_points = False)\n    analysed_dataset = analysed_dataset.append(analyzed_data, ignore_index=True)\n    \nanalysed_dataset.to_csv('LOF_analysed_dataset.csv', index=False)","9091bcc7":"analysed_dataset = pd.DataFrame()\n\naccount_list = set(list(trans_dataset['account_id']))\nfor account_id in tqdm(account_list):\n    account_id = str(account_id)\n    account_trans_data = pre_processed_data[pre_processed_data['account_id'] == account_id]\n    analyzed_data = obj_anamoly_detect.detect_anomaly(account_trans_data, \n                                                      account_id,\n                                                      model_name = 'OCSVM',\n                                                      show_anamoly_points = False, \n                                                      save_model=False)\n    analysed_dataset = analysed_dataset.append(analyzed_data, ignore_index=True)\n    \nanalysed_dataset.to_csv('OCSVM_analysed_dataset.csv', index=False)","4015d90c":"analysed_dataset = pd.DataFrame()\n\naccount_list = set(list(trans_dataset['account_id']))\nfor account_id in tqdm(account_list):\n    account_id = str(account_id)\n    account_trans_data = pre_processed_data[pre_processed_data['account_id'] == account_id]\n    analyzed_data = obj_anamoly_detect.detect_anomaly(account_trans_data, \n                                                      account_id,\n                                                      model_name = 'LOF',\n                                                      show_anamoly_points = False, \n                                                      save_model=False)\n    analysed_dataset = analysed_dataset.append(analyzed_data, ignore_index=True)\n\n    \nanalysed_dataset.to_csv('KMEANS_analysed_dataset.csv', index=False)","6147c0d8":"# Preprocessing","90a014fb":"# Isolation Forest","703abee0":"# Local Outlier Factor","63406112":"# One Class SVM","da25e503":"# KNN","5422b51c":"# KMeans"}}