{"cell_type":{"dc5c8c77":"code","8ce59e65":"code","5d4142ec":"code","c1124545":"code","5cfa340e":"code","85aa9483":"code","7c3cb2b9":"code","e5a73ce5":"code","9b1756d7":"code","fc8b26e4":"code","03b20a18":"code","f638e037":"code","e0bf2295":"code","dbeca119":"code","2dcfa53f":"markdown","da5d1edb":"markdown","4b481d67":"markdown"},"source":{"dc5c8c77":"%%writefile keywords.txt\nairplane\nbanana\ncat","8ce59e65":"# Search engine to use while scraping the images. \nsearch_engine = \"all\" # [google, bing, yahoo, duckduckgo, all]\n\n# Number of images per keyword. All images are downloaded when set to None\nnum_images = None\n\n# Add a prefix and\/or suffix to all the keywords\nprefix = None\nsuffix = None\n\n# Threshold for removing similar images\nsimilarity_threshold = 0.98\n\n# Output directory to store the scraped images\nout_dir = \"images\"","5d4142ec":"!pip install imagehash","c1124545":"import os, platform, sys, argparse, glob\nimport time\nimport base64\nimport hashlib\nfrom io import BytesIO\nfrom html.parser import HTMLParser\nfrom urllib.parse import quote, unquote\n\nfrom tqdm import tqdm\nimport urllib3\nimport PIL.Image as Image\n\nimport numpy as np\nimport imagehash","5cfa340e":"import warnings\nwarnings.filterwarnings('ignore', message='Unverified HTTPS request')","85aa9483":"!pip install selenium\nfrom selenium import webdriver","7c3cb2b9":"!apt-get update \n!apt install chromium-chromedriver -y","e5a73ce5":"sys.path.insert(0,'\/usr\/lib\/chromium-browser\/chromedriver')","9b1756d7":"# Instantiate and connect to the chrome driver \ndef setup_browser():\n\toptions = webdriver.ChromeOptions()\n\toptions.add_argument('--headless')\n\toptions.add_argument('--no-sandbox')\n\toptions.add_argument('--disable-dev-shm-usage')\n\tbrowser = webdriver.Chrome('chromedriver', options=options)\n\n\treturn browser","fc8b26e4":"# Get the SHA-256 hash of a file\ndef sha256(fname, size=4096):\n \n\tsha256_hash = hashlib.sha256()\n\twith open(fname, 'rb') as f:\n\t\tfor byte_block in iter(lambda: f.read(4096), b\"\"):\n\t\t\tsha256_hash.update(byte_block)\n\t\t\n\treturn sha256_hash.hexdigest()\n\n# Find difference between files using SHA-256 and remove duplicates\ndef remove_duplicate_images(directory):\n\tprint(\"\\nChecking for duplicate images by comparing SHA-256 hash\")\n\tflag = False\n\n\tfile_list = glob.glob(f\"{directory}\/*.png\")\n\n\tunique = []\n\tfor file in file_list:\n\t\tfilehash = sha256(file)\n\n\t\tif filehash not in unique:\n\t\t\tunique.append(filehash)\n\t\telse:\n\t\t\tprint(f\"Removing duplicate image: {file}\")\n\t\t\tos.remove(file)\t\n\t\t\tflag = True\n\t\t\t\n\tif flag == False:\n\t\tprint(\"No duplicate images found\")\n\n\n# Get a combined perceptual hashs of a image\ndef get_perceptual_hash(img_path):\n\timg = Image.open(img_path)\n\n\thashes = [\n\t\timagehash.average_hash,\n\t\timagehash.phash,\n\t\timagehash.dhash,\n\t\timagehash.whash,\n\t]\n\n\tcombined_hash = np.array([h(img).hash for h in hashes]).flatten()\n\tcombined_hash = np.where(combined_hash==True, 1, 0)\n\n\treturn combined_hash\n\n# Compare combined perceptual hashs of two images\ndef compare_hash(hash1, hash2):\n\tassert len(hash1) == len(hash2)\n\n\tcount = 0\n\tfor i in range(len(hash1)):\n\t\tif hash1[i] == hash2[i]:\n\t\t\tcount +=1\n\n\treturn count\/len(hash1)\n\n# Remove similar images using perceptual hashs\ndef remove_similar_images(directory, similarity_threshold=0.98):\n\tprint(\"\\nChecking for similar images\")\n\tfile_list = glob.glob(f\"{directory}\/*.png\")\n\n\tfound = False\n\tunique = []\n\tfor file in file_list:\n\t\tfilehash = get_perceptual_hash(file)\n\n\t\tflag = False\n\t\tfor each in unique:\n\t\t\tsimilarity = compare_hash(each, filehash)\n\n\t\t\tif similarity >= similarity_threshold:\n\t\t\t\tflag = True\n\t\t\t\tfound = True\n\t\t\t\tbreak\n\n\t\tif flag:\n\t\t\tprint(f\"Removing similar image: {file}\")\n\t\t\tos.remove(file)\n\t\telse:\n\t\t\tunique.append(filehash)\n\n\tif not found:\n\t\tprint(\"No similar images found\")","03b20a18":"\n# Class to extract the value of specific HTML tag attribute\nclass Extractor(HTMLParser):\n  src = []\n  tag_attr = None\n  def handle_starttag(self, tag, attrs):\n    if tag == \"img\":\n      for each in attrs:\n        if each[0] == self.tag_attr:\n          self.src.append(each[1])\n\n# Create output directory if it does not exist\ndef create_output_directory(keyword, out_dir=None):\n\tif out_dir == None:\n\t\tos.makedirs(keyword, exist_ok=True)\t\t\n\telse:\n\t\tos.makedirs(out_dir, exist_ok=True)\n\t\tos.makedirs(f\"{out_dir}\/{keyword}\", exist_ok=True)\n\ndef add_prefix_suffix(keyword, prefix=None, suffix=None):\n\tif prefix != None:\n\t\tkeyword = prefix + \" \" + keyword\n\n\tif suffix != None:\n\t\tkeyword = keyword + \" \" + suffix\n\t\n\treturn keyword\n\ndef filter_src_format(src_list):\n\tfiltered = []\n\n\tfor each_src in src_list:\n\t\tif \".png\" in each_src or \".jpg\" in each_src or \".jpeg\" in each_src:\n\t\t\tfiltered.append(each_src)\n\t\telif \"\/png\" in each_src or \"\/jpg\" in each_src or \"\/jpeg\" in each_src:\n\t\t\tfiltered.append(each_src)\n\t\telif \"https:\" in each_src:\n\t\t\tfiltered.append(each_src)\n\t\telse:\n\t\t\tcontinue\n\n\treturn filtered\n\n\ndef get_img_data(url, src):\n\tif \"https:\" in src or \"www.\" in src:\n\t\tresponse = http.request('GET', src)\n\t\timg_data = BytesIO(response.data)\n\n\telif src.endswith(\".png\") or src.endswith(\".jpg\"):\n\t\tbase_url = urlparse(url).netloc\n\t\tsrc = base_url + src\n\n\t\tresponse = http.request('GET', src)\n\t\timg_data = BytesIO(response.data)\n\n\telse:\n\t\tsrc = src.split(',')[-1]\n\t\timg_data = base64.b64decode(src)\n\t\timg_data = BytesIO(img_data)\n\n\treturn img_data\n\n# Search for the specified keyword using the specified search engine, load the url on chrome \n# using chromedriver, extract certain attribute values and then collect the images\ndef scrape_images_search_engine(keyword, search_engine, output_directory, num_images=None):\t\t\n\tprint(f\"\\nSearch engine: {search_engine}\")\n\n\tsearch_engine_urls = {\n\t\t\"google\" : f\"https:\/\/www.google.com\/search?tbm=isch&q={quote(keyword)}\",\n\t\t\"bing\" : f\"https:\/\/www.bing.com\/images\/search?q={quote(keyword)}\",\n\t\t\"yahoo\" : f\"https:\/\/images.search.yahoo.com\/search\/images?p={quote(keyword)}\",\n\t\t\"duckduckgo\": f\"https:\/\/duckduckgo.com\/?q={quote(keyword)}&iax=images&ia=images\"\n\t}\n\turl = search_engine_urls[search_engine]\n\tprint(f\"URL: {url}\")\n\n\tbrowser.get(url)\n\ttime.sleep(2)\n\n\tscroll_count = {\"google\": 3, \"bing\": 3, \"yahoo\": 1, \"duckduckgo\": 5}\n\tfor _ in range(scroll_count[search_engine]):\n\t\tbrowser.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n\t\tbrowser.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n\t\ttime.sleep(2)\n\n\textractor.src = []\n\n\tif search_engine == \"google\":\n\t\textractor.tag_attr = \"data-src\"\n\t\textractor.feed(browser.page_source)\n\t\t\n\t\textractor.tag_attr = \"src\"\n\t\textractor.feed(browser.page_source)\n\n\telif search_engine == \"bing\":\n\t\textractor.tag_attr = \"src\"\n\t\textractor.feed(browser.page_source)\n\n\t\textractor.src = list(map(lambda x: x.split('?w')[0], extractor.src))\n\t\textractor.src = list(set(extractor.src))\n\n\t\tfiltered = []\n\t\tfor each in extractor.src:\n\t\t\tif \"OIP\" in each:\n\t\t\t\tfiltered.append(each)\n\n\t\textractor.src = filtered\t\n\t\t\n\telif search_engine == \"yahoo\":\n\t\textractor.tag_attr = \"src\"\n\t\textractor.feed(browser.page_source)\n\n\t\textractor.src = list(map(lambda x: x.split('&')[0], extractor.src))\n\t\textractor.src = list(set(extractor.src))\n\n\telif search_engine == \"duckduckgo\":\n\t\textractor.tag_attr = \"src\"\n\t\textractor.feed(browser.page_source)\n\n\t\textractor.src = list(map(lambda x: unquote(x.split('?')[-1][2:]), extractor.src))\n\n\textractor.src = filter_src_format(extractor.src)\n\tlen_src = len(extractor.src)\n\tprint(f\"Number of images found: {len_src}\")\n\n\tif num_images != None:\n\t\tsrc_list = extractor.src[:num_images]\n\telse:\n\t\tsrc_list = extractor.src\n\n\tcount = 0\n\tfor each_src in tqdm(src_list):\n\t\ttry:\n\t\t\timg_data = get_img_data(url, each_src)\n\n\t\t\timage = Image.open(img_data).convert(\"RGBA\")\n\t\t\timage.save(f\"{output_directory}\/{search_engine[0]}-{count+1}.png\")\n\t\t\t\t\n\t\t\tcount+=1\n\n\t\texcept:\n\t\t\tprint(f\"Something went wrong while scraping the image at URL:\\n{each_src}\")\n\n\tif num_images != None:\n\t\tprint(f\"Downloaded {count}\/{num_images} images\")\n\telse:\n\t\tprint(f\"Downloaded {count}\/{len_src} images\")","f638e037":"browser = setup_browser()\nextractor = Extractor()\nhttp = urllib3.PoolManager()\n\nif search_engine not in ['google', 'bing', 'yahoo', 'duckduckgo', 'all']:\n  raise Exception(\"Search engine needs to be one of the following: google, bing, yahoo, duckduckgo or all\")\n\nif num_images != None:\n  num_images = [num_images \/\/ 4 + (1 if x < num_images % 4 else 0)  for x in range (4)]\n\nwith open(\"keywords.txt\", 'r') as infile:\n  keywords = infile.read().splitlines()\n\n  for each in keywords:\n    if each != \"\":\n      print('\\n' + '-' * 100)\n      \n      keyword = add_prefix_suffix(each, prefix=prefix, suffix=suffix).strip()\n      print(f\"Keyword: {keyword}\")\n\n      create_output_directory(keyword, out_dir)\n      output_directory = keyword if out_dir == None else f\"{out_dir}\/{keyword}\"\n\n      if search_engine != \"all\":\n        scrape_images_search_engine(keyword=keyword, search_engine=search_engine, output_directory=output_directory, num_images=num_images)\n      else:\n        if num_images != None:\n          for i, each_se in enumerate(['google', 'bing', 'yahoo', 'duckduckgo']):\n            scrape_images_search_engine(keyword=keyword, search_engine=each_se, output_directory=output_directory, num_images=num_images[i])\n        else:\n          for each_se in ['google', 'bing', 'yahoo', 'duckduckgo']:\n            scrape_images_search_engine(keyword=keyword, search_engine=each_se, output_directory=output_directory, num_images=num_images)\n\n      remove_duplicate_images(output_directory)\n      remove_similar_images(output_directory, similarity_threshold)\n      print('-' * 100)\n    \n      time.sleep(2)\n\nbrowser.quit()","e0bf2295":"!zip images.zip images\/*\/*\n!rm -rf images","dbeca119":"# !cp images.zip \/path\/to\/directory\/images.zip","2dcfa53f":"-----","da5d1edb":"# Image-Scraper\n### Scrape images from the web\n-----","4b481d67":"Enter the list of keywords in the cell below\n"}}