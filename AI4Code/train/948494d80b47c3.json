{"cell_type":{"9d8417a5":"code","aca19ce0":"code","4fa51086":"code","06396d98":"code","389596fa":"code","f0aec99c":"code","cfba12e3":"code","54c57d8b":"code","95537bba":"code","6193522d":"code","aee5cfeb":"code","165fa1f5":"code","157ff35b":"code","50088cbd":"code","03d88505":"code","668b086c":"code","fb7db39b":"code","e8ba1e06":"markdown","cd9a9d3a":"markdown","9b1f8af1":"markdown","e9687ed1":"markdown","558eb222":"markdown","6bcb8b7e":"markdown","d072b751":"markdown","9d8cdd6d":"markdown","c726f4ed":"markdown","522fecb0":"markdown"},"source":{"9d8417a5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nimport nltk\n# nltk.download('stopwords')\n# nltk.download('punkt')\nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize \nstop_words = set(stopwords.words('english')) \nimport tensorflow as tf\nfrom sklearn.metrics import roc_auc_score\nfrom keras import models, layers, Model\nfrom keras.preprocessing import text, sequence\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","aca19ce0":"## Clean Punctuation & Stopwords\nclass clean_text:\n\tdef __init__(self, text):\n\t\tself.text = text\n\t\n\t# Remove Punctuation\n\tdef rm_punct(text):\n\t\tpunct = set([p for p in \"\/-'?!.,#$%\\'()*+-\/:;<=>@[\\\\]^_`{|}~`\" + '\"\"\u201c\u201d\u2019' + '\u221e\u03b8\u00f7\u03b1\u2022\u00e0\u2212\u03b2\u2205\u00b3\u03c0\u2018\u20b9\u00b4\u00b0\u00a3\u20ac\\\u00d7\u2122\u221a\u00b2\u2014\u2013&'])\n\t\ttext = [t for t in text if t not in punct]\n\t\t\t\n\t\treturn \"\".join(text)\n\n\t# Remove Stopwords\n\tdef rm_stopwords(text):\n\t\tword_tokens = word_tokenize(text)   \n\t\tresult = [w for w in word_tokens if w not in stop_words]\n\t\t\t\t\n\t\treturn \" \".join(result)\n\ndef Embedding_CuDNNLSTM_model(max_words, max_len):\n    sequence_input = layers.Input(shape=(None, ))\n    x = layers.Embedding(max_words, 128, input_length=max_len)(sequence_input)\n    x = layers.SpatialDropout1D(0.3)(x)\n    x = layers.Bidirectional(layers.CuDNNLSTM(64, return_sequences=True))(x)\n    x = layers.Bidirectional(layers.CuDNNLSTM(64, return_sequences=True))(x)\n    \n    avg_pool1d = layers.GlobalAveragePooling1D()(x)\n    max_pool1d = layers.GlobalMaxPool1D()(x)\n    \n    x = layers.concatenate([avg_pool1d, max_pool1d])\n    x = layers.Dense(32, activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n    output = layers.Dense(1, activation='sigmoid')(x)\n    \n    model = models.Model(sequence_input, output)\n    \n    return model\n    \n\ndef auroc(y_true, y_pred):\n\treturn tf.py_func(roc_auc_score, (y_true, y_pred), tf.double)","4fa51086":"## load data\ntrain_data = pd.read_csv('..\/input\/train.csv')\ntest_data = pd.read_csv('..\/input\/test.csv')\nprint(train_data.shape)\nprint(test_data.shape)","06396d98":"train_data.head()","389596fa":"train_df = train_data[['id','comment_text','target']]\ntest_df = test_data.copy()\n\n# set index\ntrain_df.set_index('id', inplace=True)\ntest_df.set_index('id', inplace=True)\n\n# y_label\ntrain_y_label = np.where(train_df['target'] >= 0.5, 1, 0) # Label 1 >= 0.5 \/ Label 0 < 0.5\ntrain_df.drop(['target'], axis=1, inplace=True)","f0aec99c":"# ratio by Class\nCounter(train_y_label)","cfba12e3":"train_data['comment_text'].head(20)","54c57d8b":"# remove punctuation \ntrain_df['comment_text'] = train_df['comment_text'].apply(lambda x: clean_text.rm_punct(x))\ntest_df['comment_text'] = test_df['comment_text'].apply(lambda x: clean_text.rm_punct(x))\n# remove stopwords\nX_train = train_df['comment_text'].apply(lambda x: clean_text.rm_stopwords(x))\nX_test = test_df['comment_text'].apply(lambda x: clean_text.rm_stopwords(x))","95537bba":"## tokenize\nmax_words = 100000\ntokenizer = text.Tokenizer(num_words=max_words)\ntokenizer.fit_on_texts(X_train)\n\n# texts_to_sequences\nsequences_text_train = tokenizer.texts_to_sequences(X_train)\nsequences_text_test = tokenizer.texts_to_sequences(X_test)","6193522d":"sequences_text_train[:5]","aee5cfeb":"# add padding\nmax_len = max(len(l) for l in sequences_text_train)\npad_train = sequence.pad_sequences(sequences_text_train, maxlen=max_len)\npad_test = sequence.pad_sequences(sequences_text_test, maxlen=max_len)","165fa1f5":"pad_train[:5]","157ff35b":"## embedding_lstm models \nmodel = Embedding_CuDNNLSTM_model(max_words, max_len)\n\n# model compile\nmodel.compile(optimizer='adam',\n\t\t\t loss='binary_crossentropy', metrics=['acc', auroc])\nmodel.summary()","50088cbd":"# keras.callbacks\ncallbacks_list = [\n\t\tReduceLROnPlateau(\n\t\t\tmonitor='val_auroc', patience=2, factor=0.1, mode='max'),\t# val_loss\uac00 patience\ub3d9\uc548 \ud5a5\uc0c1\ub418\uc9c0 \uc54a\uc73c\uba74 \ud559\uc2b5\ub960\uc744 0.1\ub9cc\ud07c \uac10\uc18c (new_lr = lr * factor)\n\t\tEarlyStopping(\n\t\t\tpatience=5, monitor='val_auroc', mode='max', restore_best_weights=True)\n]\n\nhistory = model.fit(pad_train, train_y_label,\n\t\t\t\t\tepochs=7, batch_size=1024,\n\t\t\t\t\tcallbacks=callbacks_list, \n\t\t\t\t\tvalidation_split=0.3, verbose=2)","03d88505":"# plot score by epochs\nauroc = history.history['auroc']\nval_auroc = history.history['val_auroc']\nepochs = range(1, len(auroc)+1)\n\nplt.figure(figsize=(7,3))\nplt.plot(epochs, auroc, 'b', label='auroc')\nplt.plot(epochs, val_auroc, 'r', label='validation auroc')","668b086c":"## predict test_set\ntest_pred = model.predict(pad_test)","fb7db39b":"sample_result = pd.DataFrame()\nsample_result['id'] = test_df.index\nsample_result['prediction'] = test_pred\n\n## submit sample_submission.csv\nsample_result.to_csv('submission.csv', index=False)","e8ba1e06":"## 6. Embedding + LSTM model","cd9a9d3a":"## 5. Tokenize","9b1f8af1":"## 7. submit submission.csv","e9687ed1":"## 3. View text data","558eb222":"## 4. Remove Punctuation & Stopwords","6bcb8b7e":"## 1. Load Data","d072b751":"#### Train model","9d8cdd6d":"## 2. Set index & target label","c726f4ed":"## Define Function","522fecb0":"#### Predict test set"}}