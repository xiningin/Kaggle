{"cell_type":{"44f7786b":"code","810772c1":"code","a17d240f":"code","9b6aa218":"code","58e286f1":"code","88166df7":"code","0ca5220c":"code","f5f618ed":"code","2eb1e5f2":"code","67cd06f7":"code","110b2707":"code","937fb25c":"code","eef432f6":"code","574ec94d":"code","cd54df4f":"code","6d2543a1":"code","f55c5e9c":"code","efce1a45":"code","7511abc5":"code","034b223b":"code","c2179433":"code","e1a670c3":"code","8e28e750":"code","e2ea58e9":"code","447e013c":"code","d927361d":"code","1446ba8c":"code","a3afe182":"code","a7a09fbc":"code","6915aa68":"code","f587b28a":"code","ae0b1478":"code","9a0966f8":"code","d46a3dfe":"code","a05ee9dd":"code","2d7524a9":"code","b9456fef":"code","2d7f36e1":"code","8587a524":"code","a667b096":"code","b0478298":"code","e54acfa2":"code","94d05d60":"code","69eb0013":"code","6a21fcb6":"code","fdec3a42":"markdown","20315f9f":"markdown","ec19eacc":"markdown","917929aa":"markdown","4107f5d4":"markdown","c7c8aafb":"markdown","99fe41a9":"markdown","9f2fb4ff":"markdown","ab59be66":"markdown","a2590dd4":"markdown","abcdeecf":"markdown","b2ec4d68":"markdown","73f97b26":"markdown","d2c1158d":"markdown","5b9db233":"markdown","2fd991a0":"markdown","8734ee62":"markdown","a7900a31":"markdown","4ef34b6f":"markdown","bb69d3cd":"markdown"},"source":{"44f7786b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","810772c1":"stroke_data = pd.read_csv('..\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv')\nstroke_data.head()","a17d240f":"stroke_data.info()","9b6aa218":"import seaborn as sns\nimport matplotlib.pyplot as plt\n","58e286f1":"sns.kdeplot(data = stroke_data[stroke_data[\"stroke\"]==0],x = \"age\",shade = True,label = \"Healthy\")\nsns.kdeplot(data = stroke_data[stroke_data[\"stroke\"]==1],x = \"age\",shade = True,label = \"Stroke\")\nplt.legend()\n","88166df7":"\nsns.kdeplot(data = stroke_data[stroke_data['stroke'] == 0], x = 'avg_glucose_level', shade = True,   label = \"Healthy\")\nsns.kdeplot(data = stroke_data[stroke_data['stroke'] == 1], x = 'avg_glucose_level', shade = True,   label = \"Stroke\")\nplt.legend()","0ca5220c":"sns.kdeplot(x = stroke_data.stroke,y = stroke_data.age,hue = stroke_data.Residence_type,shade = True,alpha = 0.5)","f5f618ed":"sns.displot(x = stroke_data.stroke,y = stroke_data.age,hue = stroke_data.work_type,kind = 'kde',fill = True,alpha = 0.5)","2eb1e5f2":"h = stroke_data[stroke_data['stroke']==0].smoking_status.value_counts()\ns = stroke_data[stroke_data['stroke']==1].smoking_status.value_counts()\nplt.subplot(2,1,1)\nplt.bar(h.index,height = h.values,width = 0.2,label = \"healthy\",color = \"green\")\nplt.legend()\nplt.subplot(2,1,2)\nplt.bar(s.index,height = s.values,width = 0.2,label = \"stroke\",color = \"magenta\")\nplt.legend()\n\n\n\n#sns.histplot(data = stroke_data,x = 'stroke',y = 'age',hue = 'smoking_status')","67cd06f7":"sns.displot(data = stroke_data,x = 'age',y = 'gender',hue = 'stroke')","110b2707":"sns.kdeplot(data = stroke_data[stroke_data['stroke'] == 0], x = 'avg_glucose_level', shade = True,   label = \"Healthy\")\nsns.kdeplot(data = stroke_data[stroke_data['stroke'] == 1], x = 'avg_glucose_level', shade = True,   label = \"Stroke\")\nplt.legend()","937fb25c":"sns.kdeplot(data = stroke_data,x = 'bmi',y = 'age',hue = 'stroke',fill = True,alpha = 0.8)\nplt.text(-5,-70,\"Conclusion \\n    For people who had a stroke, the bmi seems to have a major role for people aged 40-80 with single dominating peak \\n   Younger people tend to have lower bmi and older people tend to have bmi in the range 25-35 indicated by the double peak.\",{'color':\"red\",'fontfamily':\"serif\",'fontsize':14,'fontweight':5,'linespacing':1.5})","eef432f6":"sns.kdeplot(data = stroke_data,x = 'age',y = 'avg_glucose_level',hue = 'stroke',fill = True,alpha = 0.8)\nplt.text(-20,-150,\"Conclusion \\n Aged people with irregular blood glucose(high or low) tend to be prone to stroke\",{'color':\"red\",'fontfamily':\"serif\",'fontsize':14,'fontweight':5,'linespacing':1.5})","574ec94d":"sns.kdeplot(data = stroke_data,x = 'bmi',y = 'avg_glucose_level',hue = 'stroke',fill = True,alpha = 0.8)\n","cd54df4f":"correlation_matrix = stroke_data.corr()\ncorrelation_matrix[\"stroke\"]","6d2543a1":"stroke_labels = stroke_data[\"stroke\"].copy()\nstroke_data_drop = stroke_data.drop([\"bmi\",\"Residence_type\",\"id\",\"stroke\"],axis = 1)","f55c5e9c":"stroke_data_drop.head()","efce1a45":"import sklearn\nfrom sklearn.preprocessing import OneHotEncoder\nstroke_data_cat = stroke_data_drop[[\"gender\",\"ever_married\",\"work_type\",\"smoking_status\"]]\nencoder = OneHotEncoder()\nstroke_1hot = encoder.fit_transform(stroke_data_cat)\nstroke_1hot","7511abc5":"encoder.categories_","034b223b":"from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\n\ncat_attribs = [\"gender\",\"ever_married\",\"work_type\",\"smoking_status\"]\nnum_attributes = [\"age\",\"hypertension\",\"heart_disease\",\"avg_glucose_level\"]\n\nnum_pipeline = Pipeline([('std_scaler',StandardScaler())])\nfull_pipeline = ColumnTransformer([(\"num\",num_pipeline,num_attributes),(\"cat\",OneHotEncoder(),cat_attribs)])\nstroke_prepared = full_pipeline.fit_transform(stroke_data_drop)","c2179433":"from sklearn.model_selection import StratifiedShuffleSplit\n\nsss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=1)\n\nfor train_index, test_index in sss.split(stroke_prepared, stroke_labels):\n    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    X_train, X_test = stroke_prepared[train_index], stroke_prepared[test_index]\n    y_train, y_test = stroke_labels[train_index], stroke_labels[test_index]\n\n#X_train, X_test, y_train, y_test = train_test_split(stroke_prepared,stroke_labels, test_size=0.2, random_state=1)","e1a670c3":"from sklearn.neural_network import MLPClassifier\nclf = MLPClassifier(hidden_layer_sizes=(1000,500,500,100,10), activation = 'logistic',solver='adam', alpha=0.0001, batch_size='auto',max_iter=200, shuffle=True, random_state=1, verbose=False)\nclf.fit(X_train,y_train)","8e28e750":"from sklearn.model_selection import cross_val_predict\ny_pred = cross_val_predict(clf,X_train,y_train,cv = 3)\nfrom sklearn.metrics import mean_squared_error\n#clf.score(X_test,y_test)\n","e2ea58e9":"from sklearn.metrics import confusion_matrix\nconfusion_matrix(y_train,y_pred)","447e013c":"from sklearn.linear_model import SGDClassifier\nsgd_clf = SGDClassifier(random_state = 2)\ny_sgd_pred = cross_val_predict(sgd_clf,X_train,y_train,cv=3)\nconfusion_matrix(y_train,y_sgd_pred)\n","d927361d":"y_scores = cross_val_predict(sgd_clf,X_train,y_train,cv=3,method = \"decision_function\")","1446ba8c":"from sklearn.metrics import precision_recall_curve\nprecisions,recalls,thresholds = precision_recall_curve(y_train,y_scores)\nplt.plot(thresholds,precisions[:-1],'b--',label = \"Precision\")\nplt.plot(thresholds,recalls[:-1],'g-',label = \"Recall\")\nplt.legend()\nplt.show()","a3afe182":"plt.plot(recalls,precisions)\nplt.xlabel(\"Recall\",fontsize = 14)\nplt.ylabel(\"Precision\",fontsize = 14)","a7a09fbc":"threshold_90_recall = thresholds[np.argmax(recalls >= 0.90)]\nthreshold_90_recall","6915aa68":"y_train_pred_90 = (y_scores >= threshold_90_recall)\ny_train_pred_90","f587b28a":"from sklearn.metrics import recall_score,precision_score\nrecall_score(y_train,y_train_pred_90)\nprecision_score(y_train,y_train_pred_90)","ae0b1478":"from sklearn.metrics import roc_curve\nfpr,tpr,thresholds = roc_curve(y_train,y_scores)\nplt.tight_layout()\nplt.plot(fpr,tpr)\nplt.plot([0,1],[0,1],'k--')\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate (Recall)\")","9a0966f8":"from sklearn.ensemble import RandomForestClassifier as RFC\nforest_clf = RFC(random_state = 2)\ny_train_pred_forest = cross_val_predict(forest_clf,X_train,y_train,cv=5)\nconfusion_matrix(y_train,y_train_pred_forest)","d46a3dfe":"y_proba_forest = cross_val_predict(forest_clf,X_train,y_train,cv=5,method = \"predict_proba\")\ny_scores_forest = y_proba_forest[:,-1]\nfpr_forest,tpr_forest,thresholds_forest = roc_curve(y_train,y_scores_forest)\nprec_forest,rec_forest,thresholds_forest = precision_recall_curve(y_train,y_scores_forest)\nplt.plot(thresholds_forest,prec_forest[:-1],'b--',label = \"precision\")\nplt.plot(thresholds_forest,rec_forest[:-1],'g-',label = \"recall\")\nplt.xlabel(\"threshold\")\nplt.figure()\nplt.plot(fpr_forest,tpr_forest)\nplt.plot([0,1],[0,1],'k--')","a05ee9dd":"from sklearn.metrics import roc_auc_score #area under the roc curve\nroc_auc_score(y_train,y_scores_forest)","2d7524a9":"# Using SMOTE\nfrom imblearn.over_sampling import SMOTE\nsampler = SMOTE(random_state = 42)\nX = stroke_prepared\ny = stroke_labels\nX,y= sampler.fit_resample(X,y.values.ravel())\ny_cat = pd.DataFrame({'stroke':y})\nsns.countplot(data = y_cat, x = 'stroke', y= None)\nplt.show()","b9456fef":"for train_index, test_index in sss.split(X, y):\n    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n\ny_pred_clf = cross_val_predict(clf,X_train,y_train,cv = 3)\nconfusion_matrix(y_train,y_pred_clf)","2d7f36e1":"y_scores_clf = cross_val_predict(clf,X_train,y_train,cv=3,method = \"predict_proba\")","8587a524":"y_scores = y_scores_clf[:,-1]\nfpr,tpr,thresholds = roc_curve(y_train,y_scores)\nprec,rec,thresholds = precision_recall_curve(y_train,y_scores)\nplt.plot(thresholds,prec[:-1],'b--',label = \"precision\")\nplt.plot(thresholds,rec[:-1],'g-',label = \"recall\")\nplt.xlabel(\"threshold\")\nplt.figure()\nplt.plot(fpr,tpr)\nplt.plot([0,1],[0,1],'k--')\na_roc = roc_auc_score(y_train,y_scores)\nplt.text(0.6,0.2,\"A_roc = {:3f}\".format(a_roc),{'fontsize': 16})","a667b096":"y_scores = cross_val_predict(sgd_clf,X_train,y_train,cv=3,method = \"decision_function\")","b0478298":"fpr,tpr,thresholds = roc_curve(y_train,y_scores)\nprec,rec,thresholds = precision_recall_curve(y_train,y_scores)\nplt.plot(thresholds,prec[:-1],'b--',label = \"precision\")\nplt.plot(thresholds,rec[:-1],'g-',label = \"recall\")\nplt.xlabel(\"threshold\")\nplt.figure()\nplt.plot(fpr,tpr)\nplt.plot([0,1],[0,1],'k--')\na_roc = roc_auc_score(y_train,y_scores)\nplt.text(0.6,0.2,\"A_roc = {:3f}\".format(a_roc),{'fontsize':16})","e54acfa2":"y_proba_forest = cross_val_predict(forest_clf,X_train,y_train,cv=5,method = \"predict_proba\")\ny_scores = y_proba_forest[:,-1]\n","94d05d60":"fpr,tpr,thresholds = roc_curve(y_train,y_scores)\nprec,rec,thresholds = precision_recall_curve(y_train,y_scores)\nplt.plot(thresholds,prec[:-1],'b--',label = \"precision\")\nplt.plot(thresholds,rec[:-1],'g-',label = \"recall\")\nplt.xlabel(\"threshold\")\nplt.figure()\nplt.plot(fpr,tpr)\nplt.plot([0,1],[0,1],'k--')\na_roc = roc_auc_score(y_train,y_scores)\nplt.text(0.6,0.2,\"A_roc = {:3f}\".format(a_roc),{'fontsize':16})","69eb0013":"y_proba_forest_test = cross_val_predict(forest_clf,X_test,y_test,cv=5,method = \"predict_proba\")\ny_scores = y_proba_forest_test[:,-1]","6a21fcb6":"fpr,tpr,thresholds = roc_curve(y_test,y_scores)\nprec,rec,thresholds = precision_recall_curve(y_test,y_scores)\nplt.plot(thresholds,prec[:-1],'b--',label = \"precision\")\nplt.plot(thresholds,rec[:-1],'g-',label = \"recall\")\nplt.xlabel(\"Threshold\")\nplt.ylabel(\"Precission\/Recall\")\nplt.figure()\nplt.plot(fpr,tpr)\nplt.plot([0,1],[0,1],'k--')\na_roc = roc_auc_score(y_test,y_scores)\nplt.text(0.5,0.2,\"A_roc = {:3f}\".format(a_roc),{'fontsize':16})\nplt.title(\"Test Prediction A_ROC score\")","fdec3a42":"# Test Data Predictions\nSince the Random Forest was the best model among all the three models tested we will use that model for Test Data prediction and evaluate our scores.","20315f9f":"## Precision and Recall\n* Precision is the accuracy of the model. How many ***Positive*** are actually ***Positives*** for \"stroke\"? The accuracy is given as:\n\n$$\\frac{True Positive}{True Positive + False Positive}$$\n\n* Recall is how many of the actual \"stroke\" are actually classified as \"stroke\" by the model?\nNote that actual \"stroke\" is composed in both the ***True Positive*** and the ***False Negative***\n$$\\frac{True Positive}{True Positive + False Negative}$$\n","ec19eacc":"# Data Augmentation and Training.\nUsing Data Augmentation from [Nikunj Malpani's](https:\/\/www.kaggle.com\/nikunjmalpani\/stroke-prediction-step-by-step-guide) notebook.","917929aa":"Let's now observe how our earlier models perfrom on the new dataset.","4107f5d4":"# Cleaning the data","c7c8aafb":"# Machine Learning Classifier","99fe41a9":"The area under the ROC curve give us the performance of the classifier. A perfect classifier will have a value of 1 and a random classifier will have a value of 0.5 as indicated by the diagona; line above. The RandomFroest Classifier performs slightly better than the previous model.\n","9f2fb4ff":"## One hot encoding the categorical variables","ab59be66":"## Conclusion\n* People worked in govt jobs are not prone to stroke","a2590dd4":"## Using a different model to try classification\n* From the sklearn, we use the stochastic gradient descent model to try and improve our prediction","abcdeecf":"## Confusion Matrix\nThe confusion Matrix shows that the classifier is not at all doing it job. This is because, the data is highly skewed. 95% of the data falls under the \"non-stroke\" category. Only 5% data is labelled \"stroke\". This makes it possible for even the dumbest classifier algorithm to score atleast high since all it has to do is classify everyone are a \"non-stroke\" candidate and still stands at getting 95% accuracy. This also shows that accuracy of prediction is not a measure for this particular type of problems.\n\n\nThe confusion matrix on the other hand gives us a clearer picture. The model correctly classifies 3889 data as \"non-stroke\" ($\\textit{True-Negative}$) but it also clasifies the 199 \"stroke\" cases as \"non stroke\" ($\\textit{False-Negative}$). Note that both the \"non-stroke\" classified as \"stroke\"($\\textit{False-Positive}$) and the \"stroke\" classified correctly ($\\textit{True-Positive}$) are zero. This emphasises the point discussed above.\n\nThis gives us reason to develop either even powerful models or manipulate the data in such a was as to get a better model out of it. A powerful model with such small data will be prone to overfit. Therefore we need to augment our data to get a better model.","b2ec4d68":"## Conclusion:\n* For people who had a stroke, the bmi seems to have a major role for people aged 40-80 with single dominating peak\n* Younger people tend to have lower bmi and older people tend to have bmi in the range 25-35 indicated by the double peak.","73f97b26":"## Conclusions:\n* Most stroke patients are roughly above 40 years of age and married.\n* Chances of getting a stroke is less for people below the age of 25\n* People below the age of 30 don't get married that often","d2c1158d":"We see that because of the disparity in the data, as the ***Recall*** increases, there is a drastic change in ***Precision***. Since this model might be used for diagnostic purposes, we can afford to err on the side of caution. What it means is we can afford to have ***false positives for*** \"stroke\" but when we miss ***false negatives*** someone's life may be danger. Therefore what we can do here is change the threshold of the model so that the precision of the model will take a hit but we make sure that most of the people prone to \"stroke\" are identitifed. Lets say we want the Recall value at 90% and see how the model behaves.","5b9db233":"The higher the recall (TPR) more the false positive (FPR) that the classifier produces. A good classifier stays as far away from the line as possible (top-left corner)","2fd991a0":"# Some visual analysis","8734ee62":"## Conclusion:\n* Doesn't matter if you are in urban or rural area.","a7900a31":"# Conclusion\nThe Random Forest Classifier has excellent performance for the test data.","4ef34b6f":"We see that our precision takes a hard hit. We might as well classify everyone as \"stroke\" prone. This is very bad model and negates the whole purpose of building a model in the first place. Let's explore more avenues to come up with a resaonable model.","bb69d3cd":"# Read and check data"}}