{"cell_type":{"7d698e01":"code","323c904c":"code","6cea0faa":"code","b539fa36":"code","f95f295d":"code","576e1255":"code","cff4834d":"code","58ef60d1":"code","251bd7c9":"code","6cb5e84c":"code","8212d72f":"code","0808a173":"code","a6348841":"code","aa83d323":"code","4cc42f1f":"code","57496072":"code","dcb5301c":"code","7ce15891":"code","a696fe02":"code","110992e0":"code","383e5548":"code","7529f4cb":"code","20d90f55":"code","4462064d":"code","14d14a1a":"code","0fd53dce":"code","23722e40":"code","a1091331":"code","ef761f38":"code","eb57f6da":"code","5f0d5d7c":"code","7921bfad":"code","bd611c67":"code","d4b8f0de":"code","e9f7cc5b":"code","cb38d119":"code","13aad1b3":"code","f44d6366":"code","c8670569":"code","d5ae4fae":"code","f26c9f92":"code","93677938":"code","be69912c":"code","3634acba":"code","44becc0d":"code","24cbf43d":"code","3536aa2d":"code","ea77d096":"code","90cdee0a":"code","e8e50a3b":"code","3d5d9abf":"code","fd030958":"code","3fdf7493":"code","6af27104":"code","2e0ff510":"code","8010e531":"code","9036ec99":"code","8de2e017":"code","1159e10f":"code","10af3191":"code","59fcf103":"code","ddb51aaa":"code","de3ea09a":"code","cf8aa9a1":"code","10a8d224":"code","46a53314":"code","ac0e4721":"code","e9962c25":"code","1c47f59f":"code","78088a10":"code","439a244b":"code","d25b3e5a":"code","f82247b7":"code","e87aeb8d":"code","6f85996b":"code","01c98671":"code","6d3813dd":"code","a7d362a6":"code","a7b8e5fc":"code","90b4a8b3":"code","8b299c65":"code","d5369d4f":"code","9a2a6352":"code","ca79e329":"code","21077d61":"code","97413200":"markdown","76cc16f3":"markdown","e64ac14c":"markdown","39cb2835":"markdown","8063c820":"markdown","702170be":"markdown","ac3731a0":"markdown","f793dd73":"markdown","187341cf":"markdown","cdeb672f":"markdown","dd532557":"markdown","c857b96f":"markdown","fc1b96a0":"markdown","b919c3fa":"markdown","4780360d":"markdown","99d6f583":"markdown","bedda6ad":"markdown","2921829b":"markdown","163cc52e":"markdown","c57ece3c":"markdown","f2331dcc":"markdown","cee16b7f":"markdown","8c62cbbb":"markdown","df000fd1":"markdown","dacd2344":"markdown","3cbe92cf":"markdown","f95657e5":"markdown","1979a183":"markdown","c16f171f":"markdown","c40ed8de":"markdown","fbf2ad0d":"markdown","f522c1dd":"markdown","f6413fbd":"markdown","b51750d0":"markdown","e87046fb":"markdown","c51ca6c5":"markdown","4d3d63ac":"markdown","da1e84e5":"markdown","8dccc19c":"markdown","edf525df":"markdown","8ffa4085":"markdown","89ad82b9":"markdown","7f1b5bef":"markdown","2a0d3b5f":"markdown","fdc7d080":"markdown","f1588d4f":"markdown","3e39d775":"markdown","57ec7ea2":"markdown","a61a7b36":"markdown","beadcd56":"markdown","b3210625":"markdown","0e11d91d":"markdown","22576b62":"markdown","f05f3175":"markdown"},"source":{"7d698e01":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np","323c904c":"data=pd.read_csv('..\/input\/world-university-rankings\/cwurData.csv')\nprint(data.head())\nprint(data.info())","6cea0faa":"data","b539fa36":"print(data.tail())","f95f295d":"print(data.info())","576e1255":"print(data[['institution', 'score', 'world_rank']].sort_values('score', ascending=False))","cff4834d":"data.shape","58ef60d1":"#verify NaN data\ndata.isna().any()","251bd7c9":"institutions=data['institution'].unique()\nprint('Univerisities in this dataset: ' + str(len(institutions)))\nstandford=data[data['institution']=='Stanford University']\nplt.plot(standford.year,standford.score)\nplt.title('Ranking of Standford over the years')\nplt.show()","6cb5e84c":"print(institutions)","8212d72f":"data.describe()","0808a173":"data.describe(include='O')","a6348841":"data[data.institution=='Arizona State University']","aa83d323":"pd.pivot_table(data , index= ['year','country'])\n#mean data over the yeats and countries","4cc42f1f":"from collections import Counter\ncountries=Counter(data.country)\ncountries=list(countries.items())\ncountries_observations=pd.DataFrame(countries,columns=['Country','Observation'])\ncountries_observations.index+=1\ncountries_observations\n","57496072":"plt.plot(data.score[data.year==2015])\nplt.xlabel('Institution')\nplt.ylabel('Score')\nplt.show()","dcb5301c":"import plotly.express as px\n\nfig = px.scatter(data.query(\"year==2015\"), x=\"world_rank\", y=\"quality_of_education\",\n\t         size=\"influence\", color=\"country\",\n                 hover_name=\"institution\", log_x=False, size_max=60)\nfig.show()\n#lees is better","7ce15891":"from wordcloud import WordCloud\nplt.subplots(figsize=(10,10))\nwordcloud = WordCloud(background_color='black', colormap='Set2', collocations=False,  width=512, height=384 ).generate(\" \".join(data.country))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.title('Countries visualization', fontsize=20)\nplt.show()","a696fe02":"plt.figure(figsize=(10,10))\nsns.heatmap(data.corr(),annot=True)","110992e0":"#from previus code \ndata.isnull().any()","383e5548":"#data that have missinf broad impact values\nprint(data[data.broad_impact.isnull()])","7529f4cb":"#creating new DataFrame to prevent erros on the original data due to manipulation\ndataF=data.copy()\ndf=data.copy()\ndataF.head()","20d90f55":"#filling NaN values on broad_impact with the mean of broad impact\ndataF['broad_impact'] = dataF['broad_impact'].fillna(dataF['broad_impact'].mean())\ndf['broad_impact'] = df['broad_impact'].fillna(df['broad_impact'].mean())\n\nprint('Does some data have missing values?: \\n'+str(dataF.isnull().any()))","4462064d":"dataF.info()","14d14a1a":"#cleaning data that may we only need\nrank_universities=data[['country','institution','score','year']]\nprint(rank_universities.head(10))","0fd53dce":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nencoder = LabelEncoder()\ndf['country'] = encoder.fit_transform(df['country'])\n#removing unnecesary data\ndf.drop('institution', axis=1, inplace=True)\ndf.drop('year', axis=1, inplace=True)\nscaler = StandardScaler()\ndf.head()\n","23722e40":"#split the data set\nscaler = StandardScaler()\ny = df['world_rank']\nX = df.drop('world_rank', axis=1)\nX = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=101)\n","a1091331":"#verify if the score follows a normal distribution\nfrom statsmodels.graphics.gofplots import qqplot\nqqplot(dataF.broad_impact,fit=True, line='s')","ef761f38":"#verify if the world rank have normal dist\nqqplot(dataF.world_rank,fit=True, line='s')","eb57f6da":"dataF.skew()","5f0d5d7c":"dataF.kurt()","7921bfad":"dataF.corr()","bd611c67":"dataF.cov()","d4b8f0de":" #Hormonic Mean of broad impact\n import statistics as st\n print('The hormonic mean of the broad impact is: '+str(st.harmonic_mean(dataF.broad_impact)))\n #dataF.broad_impact.mean()","e9f7cc5b":"dataF.apply(max)","cb38d119":"dataF.apply(min)","13aad1b3":"data_over_years=dataF.groupby(['year']).mean()\ndata_over_years","f44d6366":"over_years=data.groupby(['institution','year']).mean()\nover_years.loc['National Autonomous University of Mexico']","c8670569":"#Top 10 countries with high Score on 2015\n\ndataF[dataF.year==2015].sort_values('world_rank',ascending=True)[['country','institution','world_rank', 'score']][:10]","d5ae4fae":"# Top 10 University broad impact on 2015\ndataF[dataF.year==2015].sort_values('broad_impact',ascending=True)[['country','institution','world_rank','broad_impact','score']][:10]","f26c9f92":"dataF[dataF.year==2015].sort_values('broad_impact',ascending=True)[['country','institution','world_rank','broad_impact','score']][:10]","93677938":"year_count = dataF.year.value_counts()\ntemp=pd.DataFrame(year_count)\ntemp=temp.reset_index()\nplt.bar(x='index',height='year',data=temp)\nplt.xticks(np.arange(2012,2016,1))\nplt.show()","be69912c":"# number of world ranks on USA in 2015\nUSA=dataF[(dataF.country=='USA') & (dataF.year==2015)]\nplt.hist(USA.world_rank,bins=20)\nplt.title('USA #Rankings')\nplt.xlabel('Rank')\nplt.ylabel('Observations')\nplt.show()","3634acba":"import plotly.graph_objs as go\nfrom plotly.offline import init_notebook_mode, iplot, plot\n#form USA slcing 2015\nUSA_2015=USA[USA.year==2015]#.iloc[:100,:]\n# Creating trace1\ntrace1 = go.Scatter(\n                    x = USA_2015.world_rank,                         \n                    y = USA_2015.publications,\n                    mode = \"lines\",\n                    name = \"publications\",\n                    marker = dict(color = 'rgba(16, 112, 2, 0.8)'),\n                    text= USA_2015.institution)\n# Creating trace2\ntrace2 = go.Scatter(\n                    x = USA_2015.world_rank,\n                    y = USA_2015.broad_impact,\n                    mode = \"lines+markers\",\n                    name = \"broad impact\",\n                    marker = dict(color = 'rgba(80, 26, 80, 0.8)'),    \n                   text= USA_2015.institution)                          \ndata = [trace1, trace2]\nlayout = dict(title = 'Broad impact and publications vs World Rank in the USA on 2015',\n              xaxis= dict(title= 'World Rank',ticklen= 1,zeroline= False)      \n             )\nfig = dict(data = data, layout = layout)\niplot(fig)","44becc0d":"# Countries in the top 100 in 2015\n# data preparation\ntop_100_2015=dataF[(dataF.world_rank<=100) & (dataF.year==2015)]# df2016\n\npie1 = top_100_2015.world_rank\npie1_list = top_100_2015.world_rank  # str(2,4) => str(2.4) = > float(2.4) = 2.4\nlabels = top_100_2015.country","24cbf43d":"# figure\nfig = {\n  \"data\": [\n    {\n      \"values\": pie1_list,\n      \"labels\": labels,\n      \"domain\": {\"x\": [0, .5]},\n      \"name\": \"# in the top 100\",\n      \"hoverinfo\":\"label+percent+name\",\n      \"hole\": .3,                                   \n      \"type\": \"pie\"\n    },],\n  \"layout\": {\n        \"title\":\"Countries on the top 100 in 2015 \",\n\n    }\n}\niplot(fig)","3536aa2d":"#broad impact-world ra|nk ratio 2013 and 2014\nratiodf=(dataF[(dataF.year>=2014) & (dataF.year<=2015)]).copy()\nratiodf['broad_wRank']=ratiodf.broad_impact\/ratiodf.world_rank","ea77d096":"trace1 = go.Histogram(\n    x=ratiodf[ratiodf.year==2014].broad_wRank,\n    opacity=0.75,\n    name = \"2014\",\n    marker=dict(color='rgba(171, 50, 96, 0.6)'))\ntrace2 = go.Histogram(\n    x=ratiodf[ratiodf.year==2015].broad_wRank,\n    opacity=0.75,\n    name = \"2015\",\n    marker=dict(color='rgba(12, 50, 196, 0.6)'))\n\ndata = [trace1, trace2]\nlayout = go.Layout(barmode='overlay',                           \n                   title=' broad impact-world ra|nk ratio in 2014 and 2015',\n                   xaxis=dict(title='broad impact-world ra|nk ratio'),\n                   yaxis=dict( title='Count'),\n)\nfig = go.Figure(data=data, layout=layout)\niplot(fig)\n","90cdee0a":"\ntrace0 = go.Box(\n    y=dataF.publications,\n    name = 'Institutions Publications ',\n    marker = dict(\n        color = 'rgb(12, 12, 140)',\n    )\n)\ntrace1 = go.Box(\n    y=dataF.patents,\n    name = 'Institutions Patents',\n    marker = dict(\n        color = 'rgb(12, 128, 128)',\n    )\n)\ndata = [trace0, trace1]\niplot(data)","e8e50a3b":"sns.lmplot(x='world_rank', y='broad_impact',data=dataF)","3d5d9abf":"from sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\ncross = cross_val_score(model, X_test, y_test, cv=4)\ncross.mean()","fd030958":"from sklearn.linear_model import LogisticRegression\nfrom sklearn import tree\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn import svm\nfrom sklearn import decomposition\n\n#Import validators\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn import metrics \n#Import Hyperparameter Tuning\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.pipeline import Pipeline\n\n","3fdf7493":"from sklearn.svm import SVR","6af27104":"SV=SVR()\nSV.fit(X_train,y_train)\nprint(SV.score(X_test,y_test))","2e0ff510":"y_predSV = SV.predict(X_test)","8010e531":"EXP_1=pd.DataFrame(y_test)\nEXP_1['Predicted SVR']=y_predSV\nEXP_1['Difference SVR']=abs(y_test-y_predSV)\nEXP_1","9036ec99":"print('MAE:', metrics.mean_absolute_error(y_test, y_predSV))\nprint('MSE:', metrics.mean_squared_error(y_test, y_predSV))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_predSV)))","8de2e017":"plt.scatter(y_test,y_predSV)","1159e10f":"sns.distplot(y_test-y_predSV)","10af3191":"from sklearn.ensemble import RandomForestRegressor\nRFR = RandomForestRegressor()\nRFR.fit(X_train,y_train)","59fcf103":"print(RFR.score(X_test,y_test))","ddb51aaa":"y_predRFR = RFR.predict(X_test)","de3ea09a":"plt.scatter(y_test,y_predRFR)","cf8aa9a1":"EXP_2=pd.DataFrame(y_test)\nEXP_2['Predicted RFR']=y_predRFR\nEXP_2['Difference RFR']=abs(y_test-y_predRFR)\nEXP_2","10a8d224":"max(abs(y_test-y_predRFR))","46a53314":"sns.distplot(y_test - y_predRFR)","ac0e4721":"print('MAE:', metrics.mean_absolute_error(y_test, y_predRFR))\nprint('MSE:', metrics.mean_squared_error(y_test, y_predRFR))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_predRFR)))","e9962c25":"DTC=DecisionTreeRegressor()\nDTC.fit(X_train, y_train)","1c47f59f":"print(DTC.score(X_test,y_test))","78088a10":"y_predDTC = DTC.predict(X_test)","439a244b":"plt.scatter(y_test,y_predDTC)","d25b3e5a":"EXP_3=pd.DataFrame(y_test)\nEXP_3['Predicted DTR']=y_predDTC\nEXP_3['Difference DTR']=abs(y_test-y_predDTC)\nEXP_3","f82247b7":"max((y_test-y_predDTC))","e87aeb8d":"sns.distplot(y_test - y_predDTC)","6f85996b":"print('MAE:', metrics.mean_absolute_error(y_test, y_predDTC))\nprint('MSE:', metrics.mean_squared_error(y_test, y_predDTC))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_predDTC)))","01c98671":"from sklearn.model_selection import GridSearchCV\nparam_grid = { 'C':[0.1,1,5,10],'kernel':['linear'],'degree':[1,2,3,4,5,6],'gamma': [1, 0.1, 0.01, 0.001, 0.0001]}\ngrid = GridSearchCV(SVR(),param_grid)\ngrid.fit(X_train,y_train)\nprint(grid.best_params_)\nprint(grid.score(X_test,y_test))","6d3813dd":"print(grid.best_params_)\nprint(grid.score(X_test,y_test))","a7d362a6":"SV=SVR(C=1,degree=1,gamma=1,kernel='linear')\nSV.fit(X_train,y_train)\nprint(SV.score(X_test,y_test))","a7b8e5fc":"y_predSV = SV.predict(X_test)","90b4a8b3":"plt.scatter(y_test,y_predSV)","8b299c65":"sns.distplot(y_test-y_predSV)","d5369d4f":"EXP_1_tuned=pd.DataFrame(y_test)\nEXP_1_tuned['Predicted SVR tuned']=y_predSV\nEXP_1_tuned['Difference SVR tuned']=abs(y_test-y_predSV)\nEXP_1_tuned","9a2a6352":"print('MAE:', metrics.mean_absolute_error(y_test, y_predSV))\nprint('MSE:', metrics.mean_squared_error(y_test, y_predSV))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_predSV)))","ca79e329":"system=pd.concat([EXP_1_tuned,(EXP_2.drop('world_rank',axis=1)),(EXP_3.drop('world_rank',axis=1))],axis=1)\nsystem","21077d61":"X_test","97413200":"###**using Support Vector Regression**","76cc16f3":"##a. Correct the data. b. Replace, impute, or delete missing values\n","e64ac14c":"## 1.-  Plotly Tutorial for Beginners by DATAI Team \n(https:\/\/www.kaggle.com\/kanncaa1\/plotly-tutorial-for-beginners)\n\nThis project was the most voted on Kaggle, it was chosen because it explores a lot of the plotly library, and it uses a lot of examples. The main goal of DATAI\u00b4s project wasn't to win Kaggle competition but to make a tutorial. \n\nIn this Kernel they only use the Plotly library which is a Python graphing library that makes interactive, publication-quality graphs online. The DATAI Team made examples of how to make line plots, scatter plots, area charts, bar charts, error bars, box plots, histograms, heatmaps, subplots, multiple-axes, polar charts, and bubble charts.\n\nAs being a tutorial for Plotly, this notebook is lacking other parts of a formal project project (like Machine learning models) they just lock under the data exploration stage.\n\nWe can use this project to guide to the correct use of plotly library. Plotly is so useful because it provides interactive visualizations of the data, one important criteria when working with complex DataSets.\n\nOne interesting module seen on this Kernel is the Word Cloud, the word cloud works in a simple way: the more a specific word appears in a source of textual data, the bigger and bolder it appears in the word cloud; it can be useful to represent some visual data\n","39cb2835":"In this Kernel we can see a very complete and exhaustive work, since the DataSet is very old, there is hardly anything new to contribute. but of the projects worked with this DataSet it is considered that this may be one of the most complete","8063c820":"**Strong correlation between: publications, influnece, citations and broad_impact with the World rank**","702170be":"### Support Vector Regression","ac3731a0":"#Exploratory data analysis","f793dd73":"We can see on the data that they take surveys two times every year until 2015","187341cf":"\n## c. Summarize your work and main learnings.\n","cdeb672f":"## Using RandomForest Regression\n","dd532557":"# Related Works ","c857b96f":"## c. Show graphs","fc1b96a0":"# Introduction ","b919c3fa":"## Decision Tree Regressor\n","4780360d":"## b. Describe the main contributions of your solution","99d6f583":"Use the pandas, seaborn, sklearn, Matplotlib and plotly libraries to do a deep analysis and use ML models to predict future world university rankings; and practice what was learned as a data science","bedda6ad":"## **D.- Apply statistical techniques**","2921829b":"## a.  Extensively describe what your proposals are and  what are the things you are planning to do different. ","163cc52e":"# Modelling and Evaluation ","c57ece3c":"# Data understanding\n","f2331dcc":"## a. Stating if the main goal and specific goals were  reached.","cee16b7f":"## b & c. Train different Machine Learning Models & the results of the models. \n","8c62cbbb":"A lot of the knowledge learned in the introductory data science course was put into practice in this kernel.\n\nIt was possible to work first-hand with new ML models, different forms of visualization and it was possible to work with a real impact problem\n\nThis was a great startup project to the world of data science","df000fd1":"The problem when choosing a university is to look at the different sources to see which one is the best, the World University Rankings is using data about University Rankings from multiple sources, each of the sources is using a different hierarchization system. Besides the World University Ranking systems, there are also information about the schools and countries, about the education expenditure, and education attainment\n\nRanking universities is a difficult, political, and controversial practice. There are hundreds of different national and international university ranking systems, many of which disagree with each other. This dataset contains three global university rankings from very different places.\n\nOf all the universities in the world, which are the best?\n","dacd2344":"Build a ML algorithm that be able to predict the world rank outcome of a certain university, learn about how to correctly implement different machine learning models and share this project so it can be useful for other people ","3cbe92cf":"## 5.- Starter: World University Rankings 57003967-c by audrey meadows\n\n(https:\/\/www.kaggle.com\/audreymeadows\/starter-world-university-rankings-57003967-c)\n\nTo begin this exploratory analysis, this project first uses matplotlib to import libraries and define functions for plotting the data. Depending on the data, not all plots will be made.\n\nThis Kernel was automatically made by a bot, it says on the introductions lines \u201cautomatically-generated kernel with starter code demonstrating how to read in the data and begin exploring\u201d.\n\nPerhaps it was created by a bot, but it offers a good first look to understanding the content of the DataSets.\n\nAt the beginning of  the kernel the user defines some plotting functions to visualize the data over the kernel, he defines the plotPerColumnDistribution(),plotCorrelationMatrix() and  plotScatterMatrix(). \n\nOn the 7th cell of the kernel we can see that the user takes a first look at the cwur dataset, on the 9th cell the author made a correlation matrix over the data (the result was very interesting, it provides a first insight of the data).\nOver the 11th cell he started working with education_expenditure_supplementary_dataset, which couldn't make any EDA because the author didn't perform data cleaning .  \n","f95657e5":"## C. Apply feature scaling","1979a183":"The DataSet worked on this project was provided by The Center for World University Rankings, it is a leading consulting organization providing policy advice, strategic insights, and consulting services to governments and universities to improve educational and research outcomes.","c16f171f":"## e. Fine-Tune your model. ","c40ed8de":"# Main Contributions and  Conclusions  \n","fbf2ad0d":"## b. State your null and alternative hypothesis. ","f522c1dd":"##b. Apply descriptive statistics\n\n","f6413fbd":"Different ML models were built as expected, in addition to being able to predict what the world rank of a specific university will be, it was possible to learn how the different ML models work.\n\nIt is planned to share this Kernel so that it can be useful for other people","b51750d0":"## 3.- University Ranking Prediction by Shubham Kamble\n\n(https:\/\/www.kaggle.com\/shubham8983\/university-ranking-prediction)\n\nThis Kernel by Kable, S. is very interesting. It uses the numpy, pandas, matplotlib, seaborn and sklearn libraries in order to provide useful information. \n\nKamble used the cwur DataSet over this project, he started by taking a peak look of the data, followed by a statistical analysis of the DataFrame.\n\nRegarding the data preparation part, the author cleans the string columns by passing them through a label encoder which serves to unify the data into a numerical data.\n\nKamble realizes that there are NaN values in the broad impact, so he proceeds to fill in the gaps with the arithmetic mean of it.(A good approach according to the book hands on machine learning). Different literatures directly suggest ignoring this feature, but later in this kernel it was noticed that there is a high correlation between this feature and the result to be predicted\nThe author uses different ML models: Linear Regression, Random Forest Regression and XGBoost. It is important to highlight that this problem (World University Rankings) is not a classification problem, but a regression problem.\n","e87046fb":"Null Hypothesis: the features determine the world ranking of universities\n\nAlternative Hypothesis: The world ranking Universities Is Arbitrarily Assigned","c51ca6c5":"> Select five Kaggle Projects that have contributed  to the solution of the problem and for each project justify your choice.","4d3d63ac":"##  b. Main goal of the project.","da1e84e5":"## f. Evaluate your system on the test set","8dccc19c":"\n##a. Share firsts looks of data as it is","edf525df":"  As we can see, thanks to this project it was possible to have a deep analysis of the dataset of The Center for World University Rankings for the period 2012-2015.\n  The DataSet worked here is outdated but it certainly served its purpose, from the survey we can see a high dominance of US universities (as we seen in the word map)\n  From the information gathered as a product of this project, it was discovered that publications, influence, citations and broad impact have a lot of impact in terms of decision-making for the world ranking of the universities.\n  This ML project is a regression problem, given that from the features it was required to estimate the world ranking, unlike the classification models where it is to categorize into categories based on the different features.\n  The best models used were: Random Forest Regression and Decision Tree Regressor, both with a score of 99 with the default hyperparameters; while the Support Vector Regression model had a score of 77, so the the GridSearchCV had to be implemented for tuning the hyperparameters.\n  Undoubtedly, the most complicated part of this project was to work with new libraries since, in addition to investigating how to use those libraries, they were also implemented according to the needs of this project\n\n\n\n\n","8ffa4085":"What is planned to be different here is to change people's techniques from previous projects with the various characteristics of the pandas library to make it look cleaner, and how it can be used for data analysis. By data analysis we mean various techniques for cleaning, handling, preprocessing and visualizing features.\nMost of the kernels that work with these DataSets are somewhat basic (due to the nature of the information presented), we are looking to do a deep data analysis implementing different graphics and visual representation and other techniques never used before\nAnother proposal presented here is the implementation of different ML models, we will seek to see how the models perform and compare them with the information in the future\n","89ad82b9":"# Data Cleaning and  Preparation","7f1b5bef":">**Given the results, there is not enough evidence to reject the null hypothesis; so it is possible to predict the world rank of universities**","2a0d3b5f":"## 4.- World Uni Rankings - Beginner Project by Imer Cardona\n\n(https:\/\/www.kaggle.com\/imercardona\/world-uni-rankings-beginner-project\/)\n\nThis Kernel is a first approach to data analysis and visualization. It provides different perspectives to other DataSets not mentioned above.This kernel can be very useful if we wanted to work with other DataSets.\n\nThe libraries used in this kernel are: pandas, numpy, matplotlib and seaborn.\nAt first, the author imports the timesData, cwurData and shanghaiData into different DataFrames, after that he proceeds to slice every DataFrame into different DataFrames by year (2012, 2013, 2014 & 2015). At cell 3 we can see that Cardona made a DataFrame query in order to provide the top 10 institutions over the years. This information can be useful if we wanted to know how the institutions evolved over the years.\n\nOne important cell to highlight is the 8th cell, here we can see 9 universities and their score but with a twist, it combines all 3 DataFrames into one graph; we can see how different organizations grade those institutes \n\n","fdc7d080":"## d. Describe the source of data.","f1588d4f":"## **E.- Create new features**","3e39d775":"# Proposal ","57ec7ea2":"## a. Description of the problem. ","a61a7b36":"## a. Make use of Cross-validation. \n\n\n\n","beadcd56":"we can see on the data that the score may determinate the world rank, over with other factors","b3210625":"Columns description:\n\nworld_rank: variable to predict, less is better\n\ninstitution: name of the university\n\ncountry: location of the univesity\n\nnational_rank: univerity rank for each country\n\nquality_of_education: quakity of education\n\nalumni_employment: alumni employment\n\nquality_of_faculty: quality of the unversity \n\npublications: number of publications\n\ninfluence: influence of the university\n\ncitations: number of citations\n\nbroad_impact: broad impact of the univesity\n\npatents: number of patents\n\nscore: overal score, more is beter\n\nyear: year of the sourvey","0e11d91d":"## 2.-World university rankings by Aysenur Hilal \n (https:\/\/www.kaggle.com\/utrufe\/world-university-rankings)\n\nIn this project Hilal used pandas and numpy to make data visualization and exploratory data analysis of the ocwur DataSet. It's a beginner project with a good analysis, however Hilal forgot to do the data cleaning and preparation; we can see through the project some missing data. The project only uses the data to make visual representations so the missing data wouldn't be a problem, if Hilal wanted to use machine learning he would have some choices (as the book: Hands-On Machine learning suggest p. 55):\n\n>Decide whether we want to ignore this attribute altogether, ignore these instances, fill in the missing values (e.g., with the median), or train one model with the feature and one model without it.\n\nThis project was majorly based on making understandable data-analysis representations through visual graphics. This project can be very useful, because it serves as a tool to see the approach that different users have taken\nIn this kernel we can observe different approaches that the author had, despite the lack of an ML model, it can be used as a basis to try to give a different approach from the author\n\n","22576b62":"## c. Describe the specific goals of the project.","f05f3175":"# Discussion of Results \n\n\n\n> a. Extensively interpret your final results. \n\n\n\n"}}