{"cell_type":{"a966b3e1":"code","23087e7e":"code","d6bb9503":"code","56089068":"code","78b76807":"code","e4f36b75":"code","06586f89":"code","cd656107":"code","b6f7b518":"code","ab11e4d9":"code","89a4cd91":"code","248e0a06":"code","3e752ddf":"code","d804e62a":"code","ef2536e6":"code","c8294d5e":"code","aa4a1490":"code","14a9040e":"code","23ae2b34":"code","01b20423":"code","dd82aa2e":"code","bad1e6bf":"code","d5187f26":"code","b348f4be":"code","6e64f7de":"code","e1c4f752":"code","210d9820":"code","aa189aba":"code","3d0a38f1":"code","29059f8e":"code","7f83cba3":"code","5c93ba4e":"code","9b5f57d3":"code","392f4f73":"code","0adc6d5e":"code","7a7d1c29":"code","a94dd351":"code","34017005":"code","8d1e6d37":"code","8f08ca13":"code","e4dd8e15":"code","be575a53":"code","5f362980":"code","fc2bc93d":"code","f3ac0d2b":"code","aa7dd32a":"code","b3662d59":"code","71e7b0a0":"code","03839e37":"code","7fcc509f":"code","7e10824c":"code","965179f1":"code","d44cfb50":"code","5a797aa0":"code","88efd7b9":"code","31fd175e":"code","515f31aa":"code","11cb3841":"code","ce978939":"code","4aac9fea":"code","253ac78b":"code","06c6f1db":"code","e4bb0d54":"code","8f91f173":"code","83c58db8":"code","8d7b7a50":"code","9b4c6c0f":"code","13170a97":"code","8a90acaa":"code","67fc1ef1":"code","79262132":"code","b7499e2d":"code","fec2cd3d":"code","66bfa733":"code","f1fbd513":"code","36165be6":"code","5c23bc3b":"code","eb41acbe":"code","9d3579c5":"code","0d5678ef":"code","83ab1168":"code","00bcd4c5":"code","cc28c97a":"code","848e1b8b":"code","92473bb8":"code","598b3580":"code","5912aff4":"code","b8c4dd61":"code","ffa8eff4":"code","61cad473":"code","35d263be":"code","158eeede":"code","3a08cf56":"code","01dec21b":"code","99c8788d":"code","94b74298":"code","c5ab31cc":"code","d95ffd8d":"code","7f249285":"code","f01244dd":"code","8b573304":"code","edc59c1e":"code","1b850cdf":"code","2be0f382":"code","6edc1f5e":"code","0dbad17f":"code","32e52e7b":"code","93e5c661":"code","6ea908bc":"code","26386ffc":"code","fafbe219":"code","fa853003":"code","fb4d9125":"code","2b6861a5":"code","9a636fff":"code","aed18c3f":"markdown","7abf1df9":"markdown","b6b7ee41":"markdown","0bece919":"markdown","17230f03":"markdown","9eef4213":"markdown","abd0e94e":"markdown","86a2b7b1":"markdown","0d4dfd7c":"markdown","74a4c45f":"markdown","1685c0e7":"markdown","80d697f2":"markdown","1a728399":"markdown","1044fea2":"markdown","23d4ef5d":"markdown","80146c53":"markdown","c2c14e26":"markdown","c1cebfbe":"markdown","1adf4725":"markdown","4b5c453f":"markdown","2ec2916b":"markdown","1dccfce6":"markdown","c5fc5cf2":"markdown","efa58b18":"markdown","5f5b8c60":"markdown","4096401e":"markdown","fc5641d1":"markdown","00c55432":"markdown","eaa70e00":"markdown","4a45f5c4":"markdown","fb81591d":"markdown","782ea34f":"markdown","d11a4bff":"markdown","37f020ac":"markdown","6f93308d":"markdown","a8ab6d94":"markdown","4e2314cd":"markdown","c2625b98":"markdown","157d56e0":"markdown","904ddc62":"markdown","fd3d1de2":"markdown","235261fb":"markdown","c025dcad":"markdown","24dcf03d":"markdown","f53fd15e":"markdown","6168b751":"markdown","db1c7bef":"markdown","e109c0e4":"markdown","2a9e060b":"markdown","9206a38b":"markdown","e685dbab":"markdown","dcaf8447":"markdown","c16c10d1":"markdown","91424755":"markdown","a2c7e48e":"markdown","3a93ee09":"markdown","8568146c":"markdown","51108aee":"markdown","1df78279":"markdown","1a034432":"markdown","d76c8043":"markdown","2c7f8bff":"markdown","8b088d6c":"markdown","35da9d48":"markdown","51b8530a":"markdown","880892bd":"markdown","8e91eadb":"markdown","f18cf0b6":"markdown","3c9f5de3":"markdown","71d0a73f":"markdown","393f2f54":"markdown","ee5b0e90":"markdown","843a545f":"markdown","a0dd9bbf":"markdown","84fd6304":"markdown","5049a221":"markdown","be015f2f":"markdown","521fe891":"markdown","b1aa6eb5":"markdown","438d71bc":"markdown","570e78dc":"markdown","719b4276":"markdown","9e4031a0":"markdown","db9ec240":"markdown","6b8e9ff8":"markdown","38d5abb2":"markdown","13e5d380":"markdown","6e94e47b":"markdown","ac5273ea":"markdown","ba4a0418":"markdown","51883a37":"markdown","91b662ed":"markdown","5855843b":"markdown","69d18f2b":"markdown","80d024f5":"markdown","86b0878a":"markdown","547d818c":"markdown","a1cb8c9c":"markdown"},"source":{"a966b3e1":"import math\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import cross_val_score","23087e7e":"import os \nos.listdir('..\/input')","d6bb9503":"dataTrain = pd.read_csv('..\/input\/train.csv')\nprint(dataTrain.shape)\ndataTrain.head()","56089068":"XdataTrain = dataTrain.iloc[:, 1:-1]\nYdataTrain = dataTrain['median_house_value']\nIDdataTrain = dataTrain['Id']","78b76807":"dataTest = pd.read_csv('..\/input\/test.csv')\nprint(dataTest.shape)\nXdataTest = dataTest.iloc[:, 1:]\nIDdataTest = dataTest['Id']\ndataTest.head()","e4f36b75":"dataTrain.mean()[1:-1]","06586f89":"plot = dataTrain.mean()[4:-1].plot('bar')","cd656107":"plot = dataTrain.mean()[4:-2].plot('bar')","b6f7b518":"dataTrain.std()[1:-1]","ab11e4d9":"plot = dataTrain.std()[4:-1].plot('bar')","89a4cd91":"plot = dataTrain.mean()[3:-2].plot('bar')","248e0a06":"plot = dataTrain.std()[4:-2].plot('bar')","3e752ddf":"plot = plt.matshow(dataTrain.corr())","d804e62a":"plot = dataTrain.corr().iloc[1:-1, -1].plot('bar')","ef2536e6":"plot = dataTrain.iloc[:, 3:-1].plot(kind='hist', bins=100, legend=True, alpha=0.3, xlim=(-1000, 70000))","c8294d5e":"plot = dataTrain.iloc[:, 3:6].plot(kind='hist', bins=100, legend=True, alpha=0.3, xlim=(-100, 6000))","aa4a1490":"plot = dataTrain.iloc[:, 6:-1].plot(kind='hist', bins=100, legend=True, alpha=0.3, xlim=(-1000, 70000))","14a9040e":"import matplotlib.colors as mcolors\nfor i in list(XdataTrain):\n    plt.hist2d(XdataTrain[i], YdataTrain, bins=100, norm=mcolors.PowerNorm(0.3))\n    plt.title(i)\n    plt.show()","23ae2b34":"plot = dataTrain['median_house_value'].plot(kind='hist', bins=100, alpha=0.5)","01b20423":"dataTrain['median_house_value'].max()","dd82aa2e":"(dataTrain['median_house_value']==500001).sum()","bad1e6bf":"dataTrain['median_house_value'].min()","d5187f26":"(dataTrain['median_house_value']==14999).sum()","b348f4be":"XbalTrain = XdataTrain.copy()\nYbalTrain = YdataTrain.copy()","6e64f7de":"maxValor = (XdataTrain[(YdataTrain==500001)])\nlenMax = len(XdataTrain[(YdataTrain==500001)])\n\nnp.random.seed(3508)\n\nremove_n = int(4 * lenMax \/ 5)\ndrop_indices = np.random.choice(maxValor.index, remove_n, replace=False)","e1c4f752":"XbalTrain = XbalTrain.drop(drop_indices)\nYbalTrain = YbalTrain.drop(drop_indices)\n\nXbalTrain = XbalTrain.dropna()\nYbalTrain = YbalTrain.dropna()\n\nprint(XbalTrain.shape)\nprint(YbalTrain.shape)","210d9820":"plot = YbalTrain.plot(kind='hist', bins=100, alpha=0.5)","aa189aba":"X2grauTrain = XbalTrain.copy()","3d0a38f1":"headers = list(XdataTrain)\nfor i in headers:\n    for j in headers:\n        if i==j:\n            X2grauTrain[i+'^2'] = X2grauTrain[i] * X2grauTrain[j]\n            break\n        else:\n            X2grauTrain[i+'x'+j] = X2grauTrain[i] * X2grauTrain[j]","29059f8e":"X2grauTrain.head()","7f83cba3":"X3grauTrain = X2grauTrain.copy()","5c93ba4e":"for i in headers:\n    for j in headers:\n            if i==j:\n                for k in headers:\n                    if k==j:\n                        X2grauTrain[i+'^3'] = X3grauTrain[i] * X3grauTrain[j] * X3grauTrain[k]\n                        break\n                    else:\n                        X2grauTrain[i+'^2x'+k] = X3grauTrain[i] * X3grauTrain[j] * X3grauTrain[k]\n                break\n            else:\n                for k in headers:\n                    if k==j:\n                        X2grauTrain[i+'x'+j+'^2'] = X3grauTrain[i] * X3grauTrain[j] * X3grauTrain[k]\n                        break\n                    else:\n                        X2grauTrain[i+'x'+j+'x'+k] = X3grauTrain[i] * X3grauTrain[j] * X3grauTrain[k]","9b5f57d3":"X2grauTrain.head()","392f4f73":"Xpos1Train = XbalTrain.copy()","0adc6d5e":"Xpos1Train = Xpos1Train[['longitude', 'latitude']]","7a7d1c29":"Xpos1Train.head()","a94dd351":"Xpos2Train = Xpos1Train.copy()","34017005":"headers = list(Xpos1Train)\nfor i in headers:\n    for j in headers:\n        if i==j:\n            Xpos2Train[i+'^2'] = Xpos2Train[i] * Xpos2Train[j]\n            break\n        else:\n            Xpos2Train[i+'x'+j] = Xpos2Train[i] * Xpos2Train[j]","8d1e6d37":"Xpos2Train.head()","8f08ca13":"Xpos3Train = Xpos2Train.copy()","e4dd8e15":"for i in headers:\n    for j in headers:\n            if i==j:\n                for k in headers:\n                    if k==j:\n                        Xpos3Train[i+'^3'] = Xpos3Train[i] * Xpos3Train[j] * Xpos3Train[k]\n                        break\n                    else:\n                        Xpos3Train[i+'^2x'+k] = Xpos3Train[i] * Xpos3Train[j] * Xpos3Train[k]\n                break\n            else:\n                for k in headers:\n                    if k==j:\n                        Xpos3Train[i+'x'+j+'^2'] = Xpos3Train[i] * Xpos3Train[j] * Xpos3Train[k]\n                        break\n                    else:\n                        Xpos3Train[i+'x'+j+'x'+k] = Xpos3Train[i] * Xpos3Train[j] * Xpos3Train[k]","be575a53":"Xpos3Train.head()","5f362980":"XnoPos1Train = XbalTrain.copy()","fc2bc93d":"XnoPos1Train = XnoPos1Train.iloc[:, 2:]","f3ac0d2b":"XnoPos1Train.head()","aa7dd32a":"XnoPos2Train = XnoPos1Train.copy()","b3662d59":"headers = list(XnoPos1Train)\nfor i in headers:\n    for j in headers:\n        if i==j:\n            XnoPos2Train[i+'^2'] = XnoPos2Train[i] * XnoPos2Train[j]\n            break\n        else:\n            XnoPos2Train[i+'x'+j] = XnoPos2Train[i] * XnoPos2Train[j]","71e7b0a0":"XnoPos2Train.head()","03839e37":"XnoPos3Train = XnoPos2Train.copy()","7fcc509f":"for i in headers:\n    for j in headers:\n            if i==j:\n                for k in headers:\n                    if k==j:\n                        XnoPos3Train[i+'^3'] = XnoPos3Train[i] * XnoPos3Train[j] * XnoPos3Train[k]\n                        break\n                    else:\n                        XnoPos3Train[i+'^2x'+k] = XnoPos3Train[i] * XnoPos3Train[j] * XnoPos3Train[k]\n                break\n            else:\n                for k in headers:\n                    if k==j:\n                        XnoPos3Train[i+'x'+j+'^2'] = XnoPos3Train[i] * XnoPos3Train[j] * XnoPos3Train[k]\n                        break\n                    else:\n                        XnoPos3Train[i+'x'+j+'x'+k] = XnoPos3Train[i] * XnoPos3Train[j] * XnoPos3Train[k]","7e10824c":"XnoPos3Train.head()","965179f1":"from sklearn.metrics import make_scorer","d44cfb50":"def RMSLErelu(Y, Ypred):\n    n = len(Y)\n    soma = 0\n    Y = np.array(Y)\n    for i in range(len(Y)):\n        #Caso Ypred[i] for negativo eu trato como se ele fosse 0\n        if Ypred[i] > 0:\n            soma += ( math.log(Ypred[i]+1) - math.log(Y[i]+1) )**2\n        else:\n            soma += math.log(Y[i]+1)**2\n    return math.sqrt(soma \/ n)\nscorerRelu = make_scorer(RMSLErelu)","5a797aa0":"def RMSLEabs(Y, Ypred):\n    n = len(Y)\n    soma = 0\n    Y = np.array(Y)\n    for i in range(len(Y)):\n        soma += ( math.log( abs(Ypred[i]) + 1 ) - math.log( Y[i] + 1 ) )**2\n    return math.sqrt(soma \/ n)\nscorerAbs = make_scorer(RMSLEabs)","88efd7b9":"from sklearn.linear_model import LinearRegression","31fd175e":"clf = LinearRegression()\nprint(clf)\nscores = cross_val_score(clf, XbalTrain,YbalTrain, cv=10, scoring=scorerRelu)\nprint(scores)\nprint(scores.mean())","515f31aa":"clf = LinearRegression()\nprint(clf)\nscores = cross_val_score(clf, XbalTrain, YbalTrain, cv=10, scoring=scorerAbs)\nprint(scores)\nprint(scores.mean())","11cb3841":"from sklearn.linear_model import Lasso","ce978939":"clf = Lasso()\nprint(clf)\nscores = cross_val_score(clf, XbalTrain, YbalTrain, cv=10, scoring=scorerRelu)\nprint(scores)\nprint(scores.mean())","4aac9fea":"clf = Lasso()\nprint(clf)\nscores = cross_val_score(clf, XbalTrain, YbalTrain, cv=10, scoring=scorerAbs)\nprint(scores)\nprint(scores.mean())","253ac78b":"from sklearn.linear_model import Ridge","06c6f1db":"clf = Ridge()\nprint(clf)\nscores = cross_val_score(clf, XbalTrain, YbalTrain, cv=10, scoring=scorerRelu)\nprint(scores)\nprint(scores.mean())","e4bb0d54":"clf = Ridge()\nprint(clf)\nscores = cross_val_score(clf, XbalTrain, YbalTrain, cv=10, scoring=scorerAbs)\nprint(scores)\nprint(scores.mean())","8f91f173":"clf = LinearRegression()\nprint(clf)\nscores = cross_val_score(clf, X2grauTrain,YbalTrain, cv=10, scoring=scorerRelu)\nprint(scores)\nprint(scores.mean())","83c58db8":"clf = LinearRegression()\nprint(clf)\nscores = cross_val_score(clf, X2grauTrain,YbalTrain, cv=10, scoring=scorerAbs)\nprint(scores)\nprint(scores.mean())","8d7b7a50":"from sklearn.linear_model import LogisticRegression","9b4c6c0f":"clf = LinearRegression()\nprint(clf)\nscores = cross_val_score(clf, X3grauTrain,YbalTrain, cv=10, scoring=scorerRelu)\nprint(scores)\nprint(scores.mean())","13170a97":"clf = LinearRegression()\nprint(clf)\nscores = cross_val_score(clf, X3grauTrain,YbalTrain, cv=10, scoring=scorerAbs)\nprint(scores)\nprint(scores.mean())","8a90acaa":"clf = LinearRegression()\nprint(clf)\nscores = cross_val_score(clf, Xpos1Train,YbalTrain, cv=10, scoring=scorerAbs)\nprint(scores)\nprint(scores.mean())","67fc1ef1":"clf = LinearRegression()\nprint(clf)\nscores = cross_val_score(clf, Xpos2Train,YbalTrain, cv=10, scoring=scorerAbs)\nprint(scores)\nprint(scores.mean())","79262132":"clf = LinearRegression()\nprint(clf)\nscores = cross_val_score(clf, Xpos3Train,YbalTrain, cv=10, scoring=scorerAbs)\nprint(scores)\nprint(scores.mean())","b7499e2d":"clf = LinearRegression()\nprint(clf)\nscores = cross_val_score(clf, XnoPos1Train,YbalTrain, cv=10, scoring=scorerAbs)\nprint(scores)\nprint(scores.mean())","fec2cd3d":"clf = LinearRegression()\nprint(clf)\nscores = cross_val_score(clf, XnoPos2Train,YbalTrain, cv=10, scoring=scorerAbs)\nprint(scores)\nprint(scores.mean())","66bfa733":"clf = LinearRegression()\nprint(clf)\nscores = cross_val_score(clf, XnoPos3Train,YbalTrain, cv=10, scoring=scorerAbs)\nprint(scores)\nprint(scores.mean())","f1fbd513":"from sklearn.neighbors import KNeighborsRegressor","36165be6":"from tqdm import tqdm\n\naccuracies = {}\n\nPtam = 5\nKtam = 25\n\naccZ = np.empty([Ptam, Ktam])*0\n\nfor k in tqdm(range(1,Ktam+1)):\n    for p in range(1,Ptam+1):\n        knn = KNeighborsRegressor(n_neighbors=k, p=p)\n        scores = cross_val_score(knn, XbalTrain, YbalTrain, cv=10, scoring=scorerAbs)\n        \n        accuracies[(k,p)] = scores.mean()\n        accZ[p-1][k-1] = scores.mean()","5c23bc3b":"accuraciesSorted = list(accuracies.items())\naccuraciesSorted.sort(key=lambda x: x[1])\n\naccuraciesSorted[:10]","eb41acbe":"from mpl_toolkits.mplot3d.axes3d import Axes3D, get_test_data","9d3579c5":"fig = plt.figure(figsize=(20,5))\nax = fig.add_subplot(1, 3, 1, projection='3d')\n\naccK = np.arange(1, Ktam+1)\naccP = np.arange(1, Ptam+1)\n\naccK, accP = np.meshgrid(accK, accP)\n\nax.plot_wireframe(accK, accP, accZ)\n\nax = fig.add_subplot(1, 3, 2, projection='3d')\nax.plot_wireframe(accK, accP, accZ)\nax.view_init(30, 30)\n\nax = fig.add_subplot(1, 3, 3, projection='3d')\nax.plot_wireframe(accK, accP, accZ)\nax.view_init(20,90)","0d5678ef":"accuracies = {}\n\nfor k in tqdm(range(1,100)):\n    knn = KNeighborsRegressor(n_neighbors=k, p=1)\n    scores = cross_val_score(knn, XbalTrain, YbalTrain, cv=10, scoring=scorerAbs)\n    accuracies[k] = scores.mean()","83ab1168":"accuraciesSorted = list(accuracies.items())\naccuraciesSorted.sort(key=lambda x: x[1])\n\naccuraciesSorted[:10]","00bcd4c5":"accX = sorted(list(accuracies.keys()))\naccY = [accuracies[i] for i in accX]\nplot = plt.plot(accX, accY)","cc28c97a":"plot = plt.plot(accX[4:30], accY[4:30], 'bo', accX[4:30], accY[4:30])","848e1b8b":"Esq = np.polyfit(accX[4:10], accY[4:10], 1)\nDir = np.polyfit(accX[10:30], accY[10:30], 1)\nprint(round((Esq[1]-Dir[1])\/(Dir[0]-Esq[0])))","92473bb8":"knn = KNeighborsRegressor(n_neighbors=10, p=1)\nprint(knn)\nscores = cross_val_score(knn, XbalTrain, YbalTrain, cv=10, scoring=scorerAbs)\nprint(scores)\nprint(scores.mean())","598b3580":"accuracies = {}\n\nfor k in tqdm(range(1,100)):\n    knn = KNeighborsRegressor(n_neighbors=k, p=2)\n    scores = cross_val_score(knn, Xpos1Train,YbalTrain, cv=10, scoring=scorerAbs)\n    accuracies[k] = scores.mean()","5912aff4":"accuraciesSorted = list(accuracies.items())\naccuraciesSorted.sort(key=lambda x: x[1])\n\naccuraciesSorted[:10]","b8c4dd61":"accX = sorted(list(accuracies.keys()))\naccY = [accuracies[i] for i in accX]\nplot = plt.plot(accX, accY)","ffa8eff4":"plot = plt.plot(accX[3:10], accY[3:10], 'bo', accX[3:10], accY[3:10])","61cad473":"knn = KNeighborsRegressor(n_neighbors=6, p=2)\nprint(knn)\nscores = cross_val_score(knn, Xpos1Train,YbalTrain, cv=10, scoring=scorerAbs)\nprint(scores)\nprint(scores.mean())","35d263be":"from sklearn.tree import DecisionTreeRegressor","158eeede":"scores = {}\nfor i in tqdm(range(1,26)):\n    clf = DecisionTreeRegressor(max_depth=i, random_state=3508)\n    scores[i] = cross_val_score(clf, XbalTrain, YbalTrain, cv=10, scoring=scorerRelu).mean()\n\nclf = DecisionTreeRegressor(random_state=3508)\nclf.fit(XdataTrain, YdataTrain)\nscores[i+1] = cross_val_score(clf, XbalTrain, YbalTrain, cv=10, scoring=scorerRelu).mean()","3a08cf56":"scrX = list(scores.keys())\nscrY = list(scores.values())\n\nplot = plt.plot(scrX, scrY, 'bo', scrX, scrY)","01dec21b":"plot = plt.plot(scrX[7:13], scrY[7:13], 'bo', scrX[7:13], scrY[7:13])","99c8788d":"scores = {}\nfor i in tqdm(range(2,102)):\n    clf = DecisionTreeRegressor(max_depth=10, min_samples_split=i, random_state=3508)\n    scores[i] = cross_val_score(clf, XbalTrain, YbalTrain, cv=10, scoring=scorerRelu).mean()","94b74298":"scrX = list(scores.keys())\nscrY = list(scores.values())\n\nplot = plt.plot(scrX, scrY, 'bo', scrX, scrY)","c5ab31cc":"plot = plt.plot(scrX[10:90], scrY[10:90], 'bo', scrX[10:90], scrY[10:90])","d95ffd8d":"scores = {}\nfor i in tqdm(range(1,51)):\n    clf = DecisionTreeRegressor(max_depth=10, min_samples_split=50, min_samples_leaf=i, random_state=3508)\n    scores[i] = cross_val_score(clf, XbalTrain, YbalTrain, cv=10, scoring=scorerRelu).mean()","7f249285":"scrX = list(scores.keys())\nscrY = list(scores.values())\n\nplot = plt.plot(scrX, scrY, 'bo', scrX, scrY)","f01244dd":"plot = plt.plot(scrX[5:30], scrY[5:30], 'bo', scrX[5:30], scrY[5:30])","8b573304":"scores = {}\nfor i in tqdm(range(1,51)):\n    clf = DecisionTreeRegressor(max_depth=i, min_samples_split=50, min_samples_leaf=15, random_state=3508)\n    scores[i] = cross_val_score(clf, XbalTrain, YbalTrain, cv=10, scoring=scorerRelu).mean()\n\nclf = DecisionTreeRegressor(min_samples_split=50, min_samples_leaf=15, random_state=3508)\nscores[i+1] = cross_val_score(clf, XbalTrain, YbalTrain, cv=10, scoring=scorerRelu).mean()\n    \nscrX = list(scores.keys())\nscrY = list(scores.values())\n\nplot = plt.plot(scrX, scrY, 'bo', scrX, scrY)\nplt.show()\nplot = plt.plot(scrX[7:13], scrY[7:13], 'bo', scrX[7:13], scrY[7:13])\nplt.show()","edc59c1e":"from sklearn.ensemble import BaggingRegressor","1b850cdf":"import time","2be0f382":"'''scores = {}\ntemps = []\nt = time.time()\nfor i in tqdm(range(1, 121, 10)):\n    clf = BaggingRegressor(DecisionTreeRegressor(random_state=3508), random_state=1165842557, n_estimators=i)\n    scores[i] = cross_val_score(clf, XbalTrain, YbalTrain, cv=10, scoring=scorerRelu).mean()\n    temps.append(time.time()-t)\n    t = time.time()\n\nscrX = list(scores.keys())\nscrY = list(scores.values())\n\nplot = plt.plot(scrX, scrY, 'bo', scrX, scrY, scrX, temps)\nplt.title('Score pela quantidade de \u00e1rvores no Bagging')\n\nplt.plot(scrX, temps, 'bo', scrX, temps)\nplt.title('Tempo de treinamento pela quantidade \\nde \u00e1rvores no Bagging')'''\npass","6edc1f5e":"clf = BaggingRegressor(DecisionTreeRegressor(random_state=3508), random_state=1165842557, n_estimators=80)\nprint(clf)\nscore = cross_val_score(clf, XbalTrain, YbalTrain, cv=10, scoring=scorerRelu)\nprint(score)\nprint(score.mean())","0dbad17f":"scores = {}\ntemps = []\nt = time.time()\nfor i in tqdm(range(1, 21)):\n    clf = BaggingRegressor(DecisionTreeRegressor(max_depth=i, random_state=3508), random_state=1165842557, n_estimators=50)\n    scores[i] = cross_val_score(clf, XbalTrain, YbalTrain, cv=10, scoring=scorerRelu).mean()\n    temps.append(time.time()-t)\n    t = time.time()\nclf = BaggingRegressor(DecisionTreeRegressor(random_state=3508), random_state=1165842557, n_estimators=50)\nscores[i+1] = cross_val_score(clf, XbalTrain, YbalTrain, cv=10, scoring=scorerRelu).mean()\ntemps.append(time.time()-t)\n\nscrX = list(scores.keys())\nscrY = list(scores.values())\n\nplt.plot(scrX, scrY, 'bo', scrX, scrY)\nplt.show()\nplt.plot(scrX, temps, 'bo', scrX, temps)\nplt.show()","32e52e7b":"from sklearn.ensemble import RandomForestRegressor","93e5c661":"clf = RandomForestRegressor(n_estimators=100, max_features='sqrt', random_state=1165842557, )\nprint(clf)\nscore = cross_val_score(clf, XbalTrain, YbalTrain, cv=10, scoring=scorerRelu)\nprint(score)\nprint(score.mean())","6ea908bc":"from sklearn.ensemble import GradientBoostingRegressor","26386ffc":"scores = {}\ntemps = []\nt = time.time()\nfor i in tqdm(range(50, 250, 10)):\n    clf = GradientBoostingRegressor(max_depth=2, n_estimators=i)\n    scores[i] = cross_val_score(clf, XbalTrain, YbalTrain, cv=10, scoring=scorerRelu).mean()\n    temps.append(time.time()-t)\n    t = time.time()\nclf = GradientBoostingRegressor(max_depth=2)\nscores[i+1] = cross_val_score(clf, XbalTrain, YbalTrain, cv=10, scoring=scorerRelu).mean()\ntemps.append(time.time()-t)\n\nscrX = list(scores.keys())\nscrY = list(scores.values())\n\nplt.plot(scrX, scrY, 'bo', scrX, scrY)\nplt.show()\nplt.plot(scrX, temps, 'bo', scrX, temps)\nplt.show()","fafbe219":"scores = {}\ntemps = []\nt = time.time()\nfor i in tqdm(range(1, 11)):\n    clf = GradientBoostingRegressor(max_depth=i, n_estimators=140)\n    scores[i] = cross_val_score(clf, XbalTrain, YbalTrain, cv=10, scoring=scorerRelu).mean()\n    temps.append(time.time()-t)\n    t = time.time()\nclf = GradientBoostingRegressor(n_estimators=140)\nscores[i+1] = cross_val_score(clf, XbalTrain, YbalTrain, cv=10, scoring=scorerRelu).mean()\ntemps.append(time.time()-t)\n\nscrX = list(scores.keys())\nscrY = list(scores.values())\n\nplt.plot(scrX, scrY, 'bo', scrX, scrY)\nplt.show()\nplt.plot(scrX, temps, 'bo', scrX, temps)\nplt.show()","fa853003":"clf = GradientBoostingRegressor(max_depth=8, n_estimators=140)\nclf.fit(Xpos1Train,YbalTrain)\nYdataPred = clf.predict(XdataTest.iloc[:, :2])","fb4d9125":"ID = list(IDdataTest)\nYdataPred = list(YdataPred)","2b6861a5":"submission = np.array([ID, YdataPred])\n\nsubmission = pd.DataFrame(submission.T, columns=['Id', 'median_house_value'])\nsubmission['Id'] = submission['Id'].astype(int)","9a636fff":"submission.to_csv('out.csv', index=False)","aed18c3f":"A ideia vem de tentar achar a dist\u00e2ncia real entre os im\u00f3veis","7abf1df9":"Parece que o sistema valoriza valores de K altos e P=1","b6b7ee41":"#### abs","0bece919":"#### 5.4.1.1 Como um colocar um limite na profundidade altera o score?","17230f03":"N\u00e3o consigo vizualizar direito, selecionar cada por\u00e7\u00e3o de dados com m\u00e9dia parecida","9eef4213":"#### ReLu","abd0e94e":"## 3.5 Retirando latitude e longitude, em 1\u00ba, 2\u00ba e 3\u00ba graus","86a2b7b1":"#### 2\u00ba Grau","0d4dfd7c":"Para os par\u00e2metros inicias, o melhor valor para a profundidade m\u00e1xima \u00e9 claramente 10","74a4c45f":"### 5.1.8 Regress\u00e3o Linear retirando latitude e longitude, em 1\u00ba, 2\u00ba e 3\u00ba graus","1685c0e7":"### 5.1.7 Regress\u00e3o Linear com apenas latitude e longitude, em 1\u00ba, 2\u00ba e 3\u00ba graus","80d697f2":"#### 3\u00ba Grau","1a728399":"### 5.1.2 LASSO","1044fea2":"#### 1\u00ba Grau","23d4ef5d":"## 2.1 M\u00e9dia e desvio padr\u00e3o","80146c53":"## 4.1 Aplicando ReLu nos dados","c2c14e26":"#### 3\u00ba Grau","c1cebfbe":"## 4.2 Aplicando abs nos dados","1adf4725":"## 2.2 Matriz de correla\u00e7\u00e3o ","4b5c453f":"A mudan\u00e7a no min_samples_leaf n\u00e3o altera muito o score, com o valor \u00f3timo perto de 15","2ec2916b":"## 1.2 Carregando dados de teste","1dccfce6":"### 5.2.3 Parece que os valoes \u00f3timos de K e P para o modelo s\u00e3o 10 e 1","c5fc5cf2":"Existe uma quantidade muito grande de im\u00f3veis com median_house_value = 500001, provavelmente todos os im\u00f3veis com valor acima de 500,000 d\u00f3lares s\u00e3o tabelados como 500,001 d\u00f3lares","efa58b18":"#### ReLu","5f5b8c60":"A mudan\u00e7a no min_samples_split n\u00e3o altera muito o score, com o valor \u00f3timo perto de 50","4096401e":"## 2.3 Distribui\u00e7\u00e3o dos dados dependendo dos feartures","fc5641d1":"# 3. Tratando os dados","00c55432":"Fazendo as predi\u00e7\u00f5es","eaa70e00":"## 5.3 KNN Regressor s\u00f3 com latitude o longitude e P=2","4a45f5c4":"### 5.1.4 Conclus\u00e3o","fb81591d":"#### 2\u00ba Grau","782ea34f":"O valor \u00f3timo de K no caso \u00e9 claramente 6","d11a4bff":"#### 5.4.2.2 An\u00e1lise de como a profundidade m\u00e1xima das \u00e1rvores modifica o score","37f020ac":"#### 5.4.2.1 An\u00e1lise de como a quantidade de regressores modifica o score, com uma delta igual a 10","6f93308d":"#### ReLu","a8ab6d94":"### 5.1.6 Regress\u00e3o Linear com dados 'em 3\u00ba Grau'","4e2314cd":"### 5.3.2 Parece que os valoes \u00f3timos de K para o modelo \u00e9 6","c2625b98":"#### 2\u00ba Grau","157d56e0":"## 2.4 Distribui\u00e7\u00e3o dos dados dependendo dos feartures e da target","904ddc62":"#### 3\u00ba Grau","fd3d1de2":"#### M\u00e9dia","235261fb":"O score stabiliza perto da profundidade m\u00e1xima perto de 20","c025dcad":"Parece que o mesmo ocorre com im\u00f3veis com valor menor a 15,000 d\u00f3lares sendo tabelados como tendo 14,999 d\u00f3lares","24dcf03d":"# 4. Criando um scorer","f53fd15e":"### 5.3.1 Achando o valor \u00f3timo de K","6168b751":"#### 1\u00ba Grau","db1c7bef":"Fazendo a predi\u00e7\u00e3o","e109c0e4":"#### 3\u00ba Grau","2a9e060b":"#### 5.4.4.2 Acompanhando o comportamento dependendo da profundidade das \u00e1rvores, com 140 \u00e1rvores","9206a38b":"# 2. Vizualizando os dados ","e685dbab":"Chutar qualquer valor de um im\u00f3vel \u00e9 melhor do que falar que o seu valor \u00e9 zero, logo, a partir de agora ser\u00e1 usado apenas o m\u00e9todo de abs para se calcular o score de treino dos modelos.\n\nNo final, caso necess\u00e1rio, ser\u00e1 aplicado abs nos valores de YdataPred","dcaf8447":"O valor \u00f3timo parece estar entre 5 e 30","c16c10d1":"### 5.4.3 Random Forrest","91424755":"## 5.1 Regress\u00e3o Linear","a2c7e48e":"#### 2\u00ba Grau","3a93ee09":"### 5.4.4 Gradient Tree Boosting","8568146c":"#### 1\u00ba Grau","51108aee":"### 5.1.5 Regress\u00e3o Linear com dados 'em 2\u00ba Grau'","1df78279":"O limite de U$50.000,00 np valor da casa causa um pico de observa\u00e7\u00f5es com median_house_value=500001, para se balancear os dados \u00e9 retirado uma por\u00e7\u00e3o das observa\u00e7\u00f5es com median_house_value=500001","1a034432":"O score come\u00e7a alto e diminuindo, depois volta a aumentar, se estabiliando por volta 0.340","d76c8043":"### 5.1.1 M\u00edminos erros quadrados","2c7f8bff":"## 2.5 Analisando a distribui\u00e7\u00e3o do Target","8b088d6c":"#### abs","35da9d48":"## 5.4 \u00c1rvores de Decis\u00e7\u00e3o","51b8530a":"## 3.4 Apenas latitude e longitude, em 1\u00ba, 2\u00ba e 3\u00ba graus","880892bd":"Agora o score esbiliza no valor m\u00ednimo, com o fato de colocar um limine na profundidade da \u00e1rvore n\u00e3o \u00e9 uma altera\u00e7\u00e3o necess\u00e1rio ao modelo","8e91eadb":"O m\u00e9todo utilizado para encontrar os parametros tem pouca varia\u00e7\u00e3o no score final, com apenas o m\u00e9todo de m\u00ednimos quadrados sendo usado a partir de agora","f18cf0b6":"## 3.3 Fazendo uma \"polin\u00f4mio de 3\u00ba Grau\" com os dados","3c9f5de3":"#### abs","71d0a73f":"## 3.2 Fazendo uma \"polin\u00f4mio de 2\u00ba Grau\" com os dados","393f2f54":"#### 5.4.3.1 Testando com as configura\u00e7\u00f5es b\u00e1sicas","ee5b0e90":"#### ReLu","843a545f":"#### 5.4.1.2 Como aumentar o n\u00famero m\u00ednimo de observa\u00e7\u00f5es num n\u00f3 necess\u00e1ria para ocorrer um split altera o score?","a0dd9bbf":"# 5. Predi\u00e7\u00f5es","84fd6304":"## 3.1 Balancenado os dados","5049a221":"### 5.2.2 Tentando achar o melhor valor de K para P=1","be015f2f":"### 5.4.1 Uma \u00fanica \u00e1rvode de regress\u00e3o","521fe891":"#### abs","b1aa6eb5":"#### ReLu","438d71bc":"Observando os valores com score m\u00e1ximo","570e78dc":"Posso aproximar uma linha para os pontos antre 6 10 e outra linha para os pontos entre 10 e 30 e pegar o K do ponto de intersec\u00e7\u00e3o dessas linhas","719b4276":"### 5.2.1 Overview de como o score se comporta sobre diferentes K's e P's","9e4031a0":"#### abs","db9ec240":"### Obs: Intuitivamente, aplicar abs ao inv\u00e9s de ReLu trar\u00e1 melhores resultados","6b8e9ff8":"## 1.1 Carregando dados de treino","38d5abb2":"#### 1\u00ba Grau","13e5d380":"## 5.2 KNN Regressor","6e94e47b":"### 5.4.2 Bagging Trees","ac5273ea":"#### 5.4.1.3 Como aumentar o n\u00famero m\u00ednimo de observa\u00e7\u00f5es necess\u00e1rias num n\u00f3 para ele virar um n\u00f3 folha altera o score?","ba4a0418":"Plotando as acuraciso dependendo de K e P","51883a37":"# 6. Predi\u00e7\u00e3o dos dados de teste com o m\u00e9todo de melhor score","91b662ed":"#### 5.4.4.1 Acompanhando o comportamento dependendo da varia\u00e7\u00e3o da quantidade de \u00e1rvores, com a profundidade igual a 2","5855843b":"Fazendo a predi\u00e7\u00e3o","69d18f2b":"#### Std","80d024f5":"O score diminui rapidamente quando se altera os valores da quantidae n de \u00e1rvores para valores desse n pequenos, apos isso a score continua a diminuir, por\u00e9m muito mais devagar. O tempo neces\u00e1rio de treinamento cresce rapidamente linearmente com a quantidade de \u00e1rvores, fazendo com que o score n\u00e3o vala a pena para o tempo necess\u00e1rio para o treinamento.\n\nEsses gr\u00e1ficos foram criados com o c\u00f3digo comentado, o c\u00f3digo foi colocado para n\u00e3o rodar automaticamente pelo fato dele ser muito pesado. Apenas uma das itera\u00e7\u00f5es foi colocada para rodar como prova de que o c\u00f3digo funciona.\n\n![title](BaggQuantTree.png)","86b0878a":"### 5.1.3 RIDGE","547d818c":"# 1. Carregando os dados","a1cb8c9c":"#### 5.4.1.3 As altera\u00e7\u00f5es feitas modificaram o limite \u00f3timo para a profundidade da \u00e1rvore?"}}