{"cell_type":{"09b794ae":"code","8f292067":"code","6c06730e":"code","40cec532":"code","30bc2818":"code","5a7c7f68":"code","98a2bd33":"code","8d15ca97":"markdown","f3b0309c":"markdown","ee6dddaa":"markdown","b3b585ac":"markdown"},"source":{"09b794ae":"from kaggle_environments.envs.hungry_geese.hungry_geese import Observation, Configuration, Action, row_col\nfrom kaggle_environments import evaluate, make, utils\nimport numpy as np # linear algebra","8f292067":"%%writefile tiny_agent.py\nfrom kaggle_environments.envs.hungry_geese.hungry_geese import Observation, Configuration, Action, row_col\n\ndef agent(obs_dict, config_dict):\n    observation = Observation(obs_dict)\n    configuration = Configuration(config_dict)\n\n    player_index = observation.index\n    player_goose = observation.geese[player_index]\n    player_head = player_goose[0]\n    \n    #px, pc = row_col(player_head, configuration.columns)\n    return \"EAST\"   ","6c06730e":"%%writefile test_agent.py\nfrom kaggle_environments.envs.hungry_geese.hungry_geese import Observation, Configuration, Action, row_col\n##agent \n\n\nprev_step = None\n\ndef agent(obs_dict, config_dict):\n    #print(config_dict)\n    global prev_step\n    \n    legend = {\n    1: \"SOUTH\",\n    2: \"NORTH\",\n    3: \"EAST\",\n    4: \"WEST\"\n    }\n    \n    \n    observation = Observation(obs_dict)\n    configuration = Configuration(config_dict)\n    #print(observation)\n    #print(configuration)\n\n    player_index = observation.index\n    player_goose = observation.geese[player_index]\n    #print(player_index)\n    #print(player_goose)\n\n    player_head = player_goose[0]\n    \n    px, py = row_col(player_head, configuration.columns)\n    \n    \n    foods = []\n    for food in observation.food:\n        x, y = row_col(food, configuration.columns)\n        foods.append((x,y))\n\n    #print(foods[0])\n    fx, fy = foods[0]\n    \n\n    ##note that (0,0):= top left corner\n    ##note x is vertical, y is horizontal\n    \n    action = 3\n    if (fx > px) and (prev_step != 2):\n        action = 1 #South\n    if (fx < px) and (prev_step != 1):\n        action = 2 #North\n    if (fy > py) and (prev_step != 4):\n        action = 3 #East\n    if (fy < py) and (prev_step != 3):\n        action = 4 #West\n    \n    \n    prev_step = action\n    return legend[action]##Action.East.name\n        ","40cec532":"%%writefile hungry_agent.py\nfrom kaggle_environments.envs.hungry_geese.hungry_geese import Observation, Configuration, Action, row_col\nimport numpy as np\n\n##Note that due to examples the x,y directions are flipped relative to normal games\/apps\n##we could just flip it if needed.\ndef donut_world(x,y):\n    ##7\/2 = 3.5 -> 4\n    ##11\/2 = 5.5 -> 6\n    if x > 4 : x -= 4\n    if x < -4 : x += 4\n    if y > 6 : y -= 6\n    if y < -6 : y += 6\n    return (x,y)\n\ndef distance(v1, v2): ##v := (x,y)\n    dx = (v1[0]-v2[0])\n    dy = (v1[1]-v2[1])\n    dx,dy = donut_world(dx,dy)\n    return np.abs(dx) + np.abs(dy)\n\ndef find_closest(vectors, point):\n    max_d = np.Inf\n    close_point = vectors[0]   \n    for v in vectors:\n        d = distance(v, point)\n        if d < max_d:\n            max_d = d\n            close_point = v\n    return close_point, max_d\n            \n\ndef from_to(v1, v2): \n    return donut_world(v2[0]-v1[0], v2[1]-v1[1])\n\ndef vector_add(v1,v2):\n    return donut_world(v1[0]+v2[0], v1[1]+v2[1])\n\ndef vector_to_dir(vector):\n    sgn0 = np.sign(vector[0])\n    sgn1 = np.sign(vector[1])\n    #vector = (sgn0, sgn1)\n    if (sgn0 != 0) and (sgn1 != 0):\n        #we need to choose which direction we want to go in\n        return (sgn0, 0) #for simplicity choose vertical direction\n    \n    return (sgn0, sgn1)\n\nlegend2 = {\n    (1,0): \"SOUTH\",\n    (-1,0): \"NORTH\",\n    (0,1): \"EAST\",\n    (0,-1): \"WEST\"\n}\n\nprev_step = None\n\ndef agent(obs_dict, config_dict):\n    observation = Observation(obs_dict)\n    configuration = Configuration(config_dict)\n    player_index = observation.index\n    player_goose = observation.geese[player_index]\n    player_head = player_goose[0]\n    player = (px, py) = row_col(player_head, configuration.columns)\n    print(\"step = \", observation.step)\n    global legend2\n    global prev_step\n    \n    foods =[]\n    for food in observation.food:\n        foods.append(row_col(food, configuration.columns))\n    \n    ##we find the closest food\n    close_food = (0,0)\n    close_food, dist = find_closest(foods, player)\n    \n    print(f\"closest food at [{close_food[0]}, {close_food[1]}]; at distance [{dist}]\")\n    print(player)\n    print(close_food)\n    delta = from_to(player, close_food)\n    print(f\"delta = {delta}\")\n    \n    step = vector_to_dir(delta)\n    print(\"step towards \", step)\n    if (prev_step != None) and vector_add(prev_step, step) == (0,0):\n        ##we can't move inwards to our body\n        step = (step[1], step[0])\n    \n    prev_step = step\n    return legend2[step]\n\n","30bc2818":"%%writefile better_agent.py\nfrom kaggle_environments.envs.hungry_geese.hungry_geese import Observation, Configuration, Action, row_col\nimport numpy as np\n\n##We will mark food on the world by a 1\n##player position = 5\n##enemy goose by a -1\n##enemy potential points by a -2\n\n\n##Note that due to examples the x,y directions are flipped relative to normal games\/apps\n##we could just flip it if needed.\ndef donut_world(x,y):\n    ##7\/2 = 3.5 -> 4\n    ##11\/2 = 5.5 -> 6\n    if x > 4 : x -= 4\n    if x < -4 : x += 4\n    if y > 6 : y -= 6\n    if y < -6 : y += 6\n    return (x,y)\n\ndef distance(v1, v2): ##v := (x,y)\n    dx = (v1[0]-v2[0])\n    dy = (v1[1]-v2[1])\n    dx,dy = donut_world(dx,dy)\n    return np.abs(dx) + np.abs(dy)\n\ndef find_closest(vectors, point):\n    max_d = np.Inf\n    close_point = vectors[0]   \n    for v in vectors:\n        d = distance(v, point)\n        if d < max_d:\n            max_d = d\n            close_point = v\n    return close_point, max_d\n            \n\ndef from_to(v1, v2): \n    return donut_world(v2[0]-v1[0], v2[1]-v1[1])\n\ndef vector_add(v1,v2):\n    return donut_world(v1[0]+v2[0], v1[1]+v2[1])\n\nlegend2 = {\n    (1,0): \"SOUTH\",\n    (-1,0): \"NORTH\",\n    (0,1): \"EAST\",\n    (0,-1): \"WEST\"\n}\n \n\n\ndef vector_to_dir(vector):\n    sgn0 = np.sign(vector[0])\n    sgn1 = np.sign(vector[1])\n    #vector = (sgn0, sgn1)\n    if (sgn0 != 0) and (sgn1 != 0):\n        #we need to choose which direction we want to go in\n        return (sgn0, 0) #for simplicity choose vertical direction\n    \n    return (sgn0, sgn1)\n        \n\nprev_step = None\ndef agent(obs_dict, config_dict):\n    observation = Observation(obs_dict)\n    configuration = Configuration(config_dict)\n    player_index = observation.index\n    player_goose = observation.geese[player_index]\n    player_head = player_goose[0]\n    player = (px, py) = row_col(player_head, configuration.columns)\n    print(\"step = \", observation.step)\n    global legend2\n    global prev_step\n   \n    world = np.zeros((configuration.rows, configuration.columns))\n    for p in player_goose:\n        pos = row_col(p, configuration.columns)\n        world[pos[0], pos[1]] = 5\n    \n    foods =[]\n    for food in observation.food:\n        foods.append(row_col(food, configuration.columns))\n        world[foods[-1][0], foods[-1][1]] = 1\n    \n    for i, goose in enumerate(observation.geese): ##looping through all geese\n        if i == player_index: continue ##that's use -> pointless (for now)\n        if len(goose) == 0: continue ##enemy doesn't exist\n            \n        for j in goose:\n            x,y = row_col(j, configuration.columns)\n            world[x,y] = -1\n            \n    #print(world)\n    \n    ##we find the closest food\n    close_food = (0,0)\n    close_food, dist = find_closest(foods, player)\n    \n    print(f\"closest food at [{close_food[0]}, {close_food[1]}]; at distance [{dist}]\")\n    delta = from_to(player, close_food)\n    print(f\"delta = {delta}\")\n    \n    step = vector_to_dir(delta)\n    print(\"step towards \", step)\n    if (prev_step != None) and vector_add(prev_step, step) == (0,0):\n        ##we can't move inwards to our body\n        step = (step[1], step[0])\n        \n    prev_step = step\n    return legend2[step]\n    ","5a7c7f68":"##testing environment\nenv = make(\"hungry_geese\", debug=True)","98a2bd33":"env.reset()\nenv.run([\"better_agent.py\", \"hungry_agent.py\"])\nenv.render(mode=\"ipython\", width=500, height=450)","8d15ca97":"# Next Steps (Still deterministic)\n\nWe would preferably want to make an agent\/goose that knows about the world. So we need to make it have a \"memory\" of the grid it lives in. We can store a Matrix of encoded numbers to know the current state of the board\/world.\n\nNext we could consider all goose bodies as obstacles. This means we could apply very standard pathfinding algorithms (A*) to find our way to our objective even with many goose being in the way.\n\nNext we could try to determine what other gooses\/players can\/would do so we can account for their actions. eg.: we their goose is closer to our target why bother going for it. Or to block their way of reaching their goal.\n\nBut preferably we would want to train an agent with the above defined world state for it to find out what is the best action to do.","f3b0309c":"# The Facts \/ Plan\n\n### Facts\nFirst things first let's get some facts out of the way. The game is set on a (7,11) grid, where if we go left on the left edge we reappear on the right edge aka. Donut World https:\/\/infinityplusonemath.wordpress.com\/2017\/02\/18\/asteroids-on-a-donut\/. This means that whenever we take direction we need to acount for the symmetry of the world.\n\nOur player\/goose can only move in 4 direction (Up, Down, Left, Right) = (NORTH, SOUTH, WEST, EAST). This means that if we want to find things closest to our goose we should use the 1-norm\/Manhattan distance https:\/\/en.wikipedia.org\/wiki\/Taxicab_geometry. To effectively find distances.\n\n### Plan\nThe easiest and simplest \"smart\" goose to make is the \"greedy\" agent\/goose. This just goes toward the closest reward\/food without worrying about opponents or obstacles (not like we have any obstacles here).\n\nWe can easily make this agent with good reusable function for distance and direction calculation for more complicated agents later.","ee6dddaa":"# Running the Environment \/ FIGHT!!!","b3b585ac":"# Agent sample code"}}