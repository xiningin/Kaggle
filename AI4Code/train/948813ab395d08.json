{"cell_type":{"7decf23d":"code","e2a29403":"code","124c9afb":"code","a1ed34e7":"code","17922af2":"code","acd975dc":"code","2625ffea":"code","4454926b":"code","a9d75826":"code","fd69a602":"code","1ec78535":"code","416602ec":"code","d1db3f33":"code","78465630":"code","3e137f21":"code","1fa2f91f":"code","1769871c":"code","a2194e1d":"code","b3a5622e":"code","c06e9c1d":"code","be10f564":"code","48377d8a":"code","39de1786":"code","769e9c73":"code","d14fc5cf":"code","b0099390":"code","e36fabc3":"code","fc9bf598":"code","58fbb170":"code","108610b8":"code","f716175e":"code","67f62105":"code","4e62522b":"code","7dbe6618":"code","c6d3641b":"code","45ec52b0":"code","0cbc06b4":"code","2d988da1":"code","56de3b4f":"code","5f3034b3":"code","48cc5669":"code","b849a04c":"markdown","f660a97e":"markdown","d0805c55":"markdown","52a6db61":"markdown","26372022":"markdown","b5b08076":"markdown","d3f02fb6":"markdown","499b55df":"markdown","b50ad1fc":"markdown","cbab42d2":"markdown","49b0fd4e":"markdown","10b0dfd1":"markdown","ef2ee9c8":"markdown","85264853":"markdown","bb000ea5":"markdown","5e6e1542":"markdown","b0f320ba":"markdown","87f925e8":"markdown","7f7397b4":"markdown","a2d844de":"markdown","301aaad8":"markdown","f31fa314":"markdown","bb509c30":"markdown"},"source":{"7decf23d":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt; plt.rcdefaults()\n\n# Feature Engineering\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n\n# Classification Models\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, VotingClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import SGDClassifier, LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# Hyperparameter Tuning\nfrom sklearn.model_selection import cross_val_score, GridSearchCV, RandomizedSearchCV, StratifiedKFold\n\n# Performance Measures\nfrom sklearn.metrics import accuracy_score\n\n# Global Variables\nrnd_state = 42\nskfold = StratifiedKFold(n_splits=5)","e2a29403":"train_data = pd.read_csv(\"..\/input\/train.csv\")\ntest_data = pd.read_csv(\"..\/input\/test.csv\")\ntrain_data.head()","124c9afb":"# Stratified sampling is adopted from https:\/\/github.com\/ageron\/handson-ml\n\ntrain_data[\"Age_categories\"] = np.ceil(train_data[\"Age\"]\/1.5) \n# Dividing the Fare by 1.5 to limit the number of categories and rounding up with ceil to have discrete categories\ntrain_data[\"Age_categories\"].where (train_data[\"Age_categories\"] <2, 2.0, inplace=True)\ntrain_data[\"Age_categories\"].where (train_data[\"Age_categories\"] >50, 50.0, inplace=True)\n#keeping only the categories younger than 5o years and older than 2 years and merging the other categories into category 50 and 2\n\nsplit = StratifiedShuffleSplit (n_splits=1, test_size=0.2, random_state= rnd_state)\nfor train_index, test_index in split.split(train_data, train_data[\"Age_categories\"]):\n    strat_train_set = train_data.loc[train_index]\n    strat_test_set = train_data.loc[test_index]\n\n# Now remove the Age_categories so that the data is back to its original state\nfor set_ in (strat_train_set, strat_test_set):\n    set_.drop(\"Age_categories\", axis=1, inplace=True)  \n\n# We are interested in predicting the Survival, which means Survival is the target feature and needs to be dropped from the validation set \nstrat_train_set_p = strat_train_set.drop (\"Survived\", axis=1)\nsurvival_labels = strat_train_set[\"Survived\"].copy()\n\nstrat_test_set_p = strat_test_set.drop (\"Survived\", axis=1)\nvalidation_labels = strat_test_set[\"Survived\"].copy()","a1ed34e7":"strat_train_set_p.info() ","17922af2":"def extract_title (dataset):\n    dataset['Title'] = dataset.Name.apply(lambda name: name.split(',')[1].split('.')[0].strip())\n    # normalize the titles\n    normalized_titles = {\n        \"Capt\":       \"other\",\n        \"Col\":        \"other\",\n        \"Major\":      \"other\",\n        \"Jonkheer\":   \"other\",\n        \"Don\":        \"Mr\",\n        \"Sir\" :       \"Mr\",\n        \"Dr\":         \"other\",\n        \"Rev\":        \"other\",\n        \"the Countess\":\"other\",\n        \"Dona\":       \"Mrs\",\n        \"Mme\":        \"Mrs\",\n        \"Mlle\":       \"Miss\",\n        \"Ms\":         \"Mrs\",\n        \"Mr\" :        \"Mr\",\n        \"Mrs\" :       \"Mrs\",\n        \"Miss\" :      \"Miss\",\n        \"Master\" :    \"Master\",\n        \"Lady\" :      \"Mrs\",\n        \"NaN\" :       \"other\"  \n    }\n    # map the titles to new categories \n    dataset.Title = dataset.Title.map(normalized_titles)\n    return dataset","acd975dc":"def data_cleaner (dataset):\n    # you can add a list of feature engineering functions here\n    # For example, I created 6 age buckets for my age values and used the age as a categorical attribute but it didnt help my accuracy much so I dropped it from this notebook.\n    \n    # Extracting titles from the name\n    dataset = extract_title (dataset)\n    return dataset","2625ffea":"class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n    def __init__(self, add_familySize = False): # no *args or **kargs\n        self.add_familySize = add_familySize\n    def fit(self, X, y=None):\n        return self  # nothing else to do\n    def transform(self, X, y=None):\n        familySize = X[:, Sibsp_ix] + X[:, Parch_ix]\n        if self.add_familySize:\n            return np.c_[X, familySize]\n        else:\n            return np.c_[X]\n        \nSibsp_ix, Parch_ix = 2, 3 # column index from the dataset, index starts from 0\n\n# I created one preprocessing pipelines for processing both numeric and categorical data.\nnumeric_features = ['Age','Fare', 'SibSp', 'Parch']\nnumeric_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('attribs_adder', CombinedAttributesAdder(add_familySize = False)),\n    ('scaler', StandardScaler())\n])\n\ncategorical_features = ['Title', 'Sex', 'Pclass', 'Embarked']\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='constant', fill_value='S')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=False))])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features)\n    ])","4454926b":"train_set_prepared = preprocessor.fit_transform(data_cleaner(strat_train_set_p))\n\nvalidation_set_prepared = preprocessor.fit_transform(data_cleaner(strat_test_set_p))","a9d75826":"train_set_prepared.shape","fd69a602":"log_clf = LogisticRegression(random_state=rnd_state, solver='lbfgs')\nlog_clf.fit(train_set_prepared, survival_labels)","1ec78535":"log_pred = log_clf.predict(validation_set_prepared).astype(int)","416602ec":"log_cross_scores = cross_val_score(log_clf, train_set_prepared, survival_labels, cv=skfold, scoring=\"accuracy\")\nlog_scores = (log_cross_scores.mean() + accuracy_score (log_pred, validation_labels))\/2\nlog_scores","d1db3f33":"sgd_clf = SGDClassifier(max_iter=60, penalty = None, eta0=0.1, random_state=rnd_state, tol =1e-3)\nsgd_clf.fit(train_set_prepared, survival_labels)","78465630":"sgd_cross_scores = cross_val_score(sgd_clf, train_set_prepared, survival_labels, cv=skfold, scoring=\"accuracy\")\nsgd_pred = sgd_clf.predict(validation_set_prepared).astype(int)\nsgd_scores = (sgd_cross_scores.mean() + accuracy_score (sgd_pred, validation_labels))\/2\nsgd_scores","3e137f21":"#svm_clf = LinearSVC(loss=\"hinge\", random_state=42)\n#svm_clf = SVC (kernel=\"poly\", degree=3, coef0=1, C=5)\n\nsvm_clf = SVC (gamma='auto')\nsvm_clf.fit (train_set_prepared, survival_labels)","1fa2f91f":"svm_cross_scores = cross_val_score(svm_clf, train_set_prepared, survival_labels, cv=skfold, scoring=\"accuracy\")\nsvm_pred = svm_clf.predict(validation_set_prepared).astype(int)\nsvm_scores = (svm_cross_scores.mean() + accuracy_score (svm_pred, validation_labels))\/2\nsvm_scores","1769871c":"# # This takes some time to run, uncomment the section to run\n# param_grid = {\n#     'C':[1,10,100,1000],\n#     'gamma':[1,0.1,0.001,0.0001], \n#     'kernel':['linear','rbf']}\n\n# grid_search_svm = GridSearchCV(SVC(), param_grid, refit = True, verbose=2)\n\n# grid_search_svm.fit (titanic_train_prepared, Survival_labels)","a2194e1d":"# grid_search_svm.best_params_","b3a5622e":"svm_grid_clf = SVC (kernel=\"rbf\", gamma=0.1, C=10) #best parameters after grid search\nsvm_grid_clf.fit (train_set_prepared, survival_labels)\n\nsvm_grid_cross_scores = cross_val_score(svm_grid_clf, train_set_prepared, survival_labels, cv=skfold, scoring=\"accuracy\")\nsvm_pred = svm_grid_clf.predict(validation_set_prepared).astype(int)\nsvm_grid_scores = (svm_grid_cross_scores.mean() + accuracy_score (svm_pred, validation_labels))\/2\nsvm_grid_scores","c06e9c1d":"forest_clf = RandomForestClassifier(random_state=rnd_state, n_estimators=10)\nforest_clf.fit (train_set_prepared, survival_labels)","be10f564":"forest_clf.feature_importances_","48377d8a":"param_grid = [\n    # try 12 (3\u00d74) combinations of hyperparameters\n    {'n_estimators': [200,  250], 'max_features': [10, 17]},\n    # then try 6 (2\u00d73) combinations with bootstrap set as False\n    {'bootstrap': [False], 'n_estimators': [200, 250], 'max_features': [10, 12, 17]},\n  ]\n\n# train across 5 folds, that's a total of (12+6)*5=90 rounds of training \ngrid_forest_clf = GridSearchCV(forest_clf, param_grid, cv=skfold,\n                           scoring='accuracy', return_train_score=True)\n\ngrid_forest_clf.fit(train_set_prepared, survival_labels)","39de1786":"grid_forest_clf.best_params_","769e9c73":"grid_forest_pred = grid_forest_clf.predict(validation_set_prepared).astype(int)\ngrid_forest_scores = (grid_forest_clf.best_score_ + accuracy_score (grid_forest_pred, validation_labels))\/2\ngrid_forest_scores","d14fc5cf":"from scipy.stats import randint\nparam_distribs = {\n        'n_estimators': randint(low=100, high=300),\n        'max_features': randint(low=8, high=17),\n    }\n\nrnd_forest_clf = RandomizedSearchCV(forest_clf, param_distributions=param_distribs,\n                                n_iter=10, cv=skfold, scoring='accuracy', random_state=rnd_state)\nrnd_forest_clf.fit(train_set_prepared, survival_labels)","b0099390":"rnd_forest_clf.best_params_","e36fabc3":"rnd_forest_pred = rnd_forest_clf.predict(validation_set_prepared).astype(int)\nrnd_forest_scores = (rnd_forest_clf.best_score_ + accuracy_score (rnd_forest_pred, validation_labels))\/2\nrnd_forest_scores","fc9bf598":"knn_clf = KNeighborsClassifier()\nknn_clf.fit(train_set_prepared, survival_labels) \n\nknn_cross_scores = cross_val_score(knn_clf, train_set_prepared, survival_labels, cv=skfold, scoring=\"accuracy\")\nknn_pred = knn_clf.predict(validation_set_prepared).astype(int)\nknn_scores = (knn_cross_scores.mean() + accuracy_score (knn_pred, validation_labels))\/2\nknn_scores","58fbb170":"param_grid = [{'weights': [\"uniform\", \"distance\"], 'n_neighbors': [3, 4, 5]}]\n\ngrid_knn_clf = GridSearchCV(knn_clf, param_grid, cv=5, verbose=3, n_jobs=-1)\ngrid_knn_clf.fit(train_set_prepared, survival_labels)\ngrid_knn_clf.best_params_","108610b8":"grid_knn_cross_scores = cross_val_score(grid_knn_clf, train_set_prepared, survival_labels, cv=skfold, scoring=\"accuracy\")\ngrid_knn_pred = grid_knn_clf.predict(validation_set_prepared).astype(int)\ngrid_knn_scores = (grid_knn_cross_scores.mean() + accuracy_score (grid_knn_pred, validation_labels))\/2\ngrid_knn_scores","f716175e":"ext_clf = ExtraTreesClassifier()\n\nparam_grid = {\"max_depth\": [None],\n              \"max_features\": [10, 17],\n              \"min_samples_split\": [2, 3, 10],\n              \"min_samples_leaf\": [1, 3, 10],\n              \"bootstrap\": [False, True],\n              \"n_estimators\" :[50,100,200],\n              \"criterion\": [\"gini\"]}\n\n# Cross validate model with Kfold stratified cross val\n#kfold = StratifiedKFold(n_splits=10)\ngrid_ext_clf = GridSearchCV(ext_clf,param_grid, cv=skfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\ngrid_ext_clf.fit(train_set_prepared, survival_labels)\n\ngrid_ext_clf.best_params_","67f62105":"grid_ext_pred = grid_ext_clf.predict(validation_set_prepared).astype(int)\n\ngrid_ext_scores = (grid_ext_clf.best_score_ + accuracy_score (grid_ext_pred, validation_labels))\/2\ngrid_ext_scores","4e62522b":"ada_clf = AdaBoostClassifier(\n    DecisionTreeClassifier(random_state=rnd_state, max_depth=2),\n    random_state = rnd_state)\n\nparam_grid = {\"base_estimator__criterion\" : [\"gini\", \"entropy\"],\n              \"base_estimator__splitter\" :   [\"best\", \"random\"],\n              \"algorithm\" : [\"SAMME\",\"SAMME.R\"],\n              \"n_estimators\" :[10, 100, 200, 250],\n              \"learning_rate\":  [0.05, 0.5, 1.5, 2.5]}\n\ngrid_ada_clf = GridSearchCV(ada_clf, param_grid, cv=skfold, scoring=\"accuracy\", n_jobs= -1, verbose = 1)\ngrid_ada_clf.fit(train_set_prepared, survival_labels)\ngrid_ada_clf.best_params_\n\nada_pred = grid_ada_clf.predict(validation_set_prepared).astype(int)\ngrid_ada_scores = (grid_ada_clf.best_score_ + accuracy_score (ada_pred, validation_labels))\/2\ngrid_ada_scores","7dbe6618":"gb_clf = GradientBoostingClassifier(random_state=rnd_state)\n\nparam_grid = {\n              'n_estimators' : [25, 50 ,75, 100, 200],\n              'learning_rate': [0.005 ,0.05, 0.5, 1.5],\n              'max_depth': [2, 4, 6, 8],\n              'max_features': [10, 12, 17] \n              }\ngrid_gb_clf = GridSearchCV(gb_clf, param_grid, cv=skfold, scoring=\"accuracy\", n_jobs= -1, verbose = 1)\ngrid_gb_clf.fit(train_set_prepared, survival_labels)\n\ngb_pred = grid_gb_clf.predict(validation_set_prepared).astype(int)\ngrid_gb_scores = (grid_gb_clf.best_score_ + accuracy_score (gb_pred, validation_labels))\/2","c6d3641b":"grid_gb_clf.best_params_","45ec52b0":"%matplotlib inline\nplt.figure(figsize=(8, 4))\nplt.plot(['Logistic', 'SGD', 'SVM', 'Random Forest', 'KNN', 'Extra-Tree', 'AdaBoost', 'Gradient Boost'], [log_scores, sgd_scores, svm_scores, rnd_forest_scores, knn_scores, grid_ext_scores, grid_ada_scores, grid_gb_scores], 'ro')\n\nplt.show()","0cbc06b4":"voting_clf = VotingClassifier(\n    estimators=[('rf', rnd_forest_clf), ('svc', svm_clf), ('knn', grid_knn_clf)],\n    voting='hard')\n\nvoting_clf.fit (train_set_prepared, survival_labels)","2d988da1":"voting_cross_scores = cross_val_score(voting_clf, train_set_prepared, survival_labels, cv=skfold, scoring=\"accuracy\")\nvoting_pred = voting_clf.predict(validation_set_prepared).astype(int)\nvoting_scores = (voting_cross_scores.mean() + accuracy_score (voting_pred, validation_labels))\/2\nvoting_scores ","56de3b4f":"%matplotlib inline\nplt.figure(figsize=(8, 4))\nplt.plot(['SVM', 'Random Forest', 'KNN', 'Ensemble'], [svm_scores, rnd_forest_scores, grid_knn_scores, voting_scores], 'ro')\n\nplt.show()","5f3034b3":"test_set_prepared = preprocessor.fit_transform(data_cleaner(test_data))\npredictions = voting_clf.predict(test_set_prepared).astype(int)","48cc5669":"#set ids as PassengerId and predict survival \nids = test_data['PassengerId']\noutput = pd.DataFrame({ 'PassengerId' : ids, 'Survived': predictions })\noutput.to_csv('submission.csv', index=False)","b849a04c":"It seems like the best models so far are the SVM, Random Forest, and the KNN. let's try one last thing, the combination of SVM and Random Forest.","f660a97e":"### Randomized Search on a Random Forest","d0805c55":"# Step 1: Importing data","52a6db61":"The test data does not contain survival labels: your goal is to train the best model you can using the training data, then make your predictions on the test data and upload them to Kaggle to see your final score. This code gives you slightly more than 80% accuracy.\n\n# Step 2: Creating a Validation Set \n\nWe use the validation set to test out the performance of the training models. The simplest way to create a validation set is to use train_test_split function and specify the percentage of data that you would want to allocate for validation. This will splits the training data into two sets of training and validation. For example, the code below creates a validation set with 20% of the data.\n\ntrain_set, validation_set, survival_labels, validation_labels = train_test_split(train_data, train_data[\"Survived\"], test_size=0.20, random_state=42)\n\nNow that you know the simple way, let's apply a more sophisticated split that tries to preserve the distribution of the features we care more about. This is particularly helpful in situation where an expert tells you a feature is more important than others, or you run a set of feature importance analysis and realize one or two categories are more important than the others. Dont worry about the feature importance for now. we will revisit in step 4. Based on my analysis, Fare and Age are the highest correlated features to survival. \n\nI used StratifiedShuffleSplit applied on 'age' to ensure my validation set has a similar distribution to the initial training set with respect to age. I tried the fare as well, but the accuracy of the results stayed pretty much the same!","26372022":"## d. Random Forest Classifier","b5b08076":" \nThe first one is Age, the second one is fare, that's how I calculated the most important factors when building the validation set","d3f02fb6":"## c. SVM","499b55df":"## f. Extra-Trees","b50ad1fc":"## b. Stochastic Gradient Descent (SGD) Classifier","cbab42d2":"## h. Gradient Boost ","49b0fd4e":"Based on our analysis, it seems like the ensemble method has the highest accuracy. Let's apply this method to the test set and submit the scores to Keggle!\n\n# Step 6. Predicting the survival on test data","10b0dfd1":"## e. KNN","ef2ee9c8":"### Grid Search on a Random Forest","85264853":"**Age**, **Fare**, **SibSP**, **Parch** contain numerical value, which means their value need to be scaled and the missing values can be filled with median, mean, or most frequent value.\n\nI added a combination of SibSp and Parch as a new column that represents family size, but it didnt change the accuracy much. You can turn this on and off by setting add_familySize to False\n\n## Creating a processing pipeline","bb000ea5":"Training data now has 17 columns:\n    Age\n    Fare\n    SibSP\n    Parch\n    Title (Mr, Mrs, Miss, Master, Other)\n    Sex (F, M)\n    Pclass (1, 2, 3)\n    Embarked (C, S, Q)\n\nif you set add_familySize = True, then you will have one extra column for the family size.\n\nPlease note all the numerical values are scaled using StandardScaler(). This is particularly important as some of the classification models only work on scaled data.\n\nYou can keep working on this section. but I think we are good for now.\n\n\n# Step 4: Classification\nIn this step, we implement the following classification models and compare their accuracies.\n\n    - Logistic Classifier\n    - Stochastic Gradient Descent (SGD) Classifier\n    - SVM\n    - Bagging Classifier\n    - Extra Trees Classifier\n    - Voting Classifier\n    - AdaBoost Classifier\n    - GradientBoosting Classifier\n    - Decision Tree Classifier\n    - Random Forest Classifier\n    - KNN\n    \nThe accuracy of each model is calculated based on a cross evaluation across a number of splits as well as a prediction on the validation set.\n\n## a. Logistic Classifier","5e6e1542":"I used a Stratified folds for cross validation.\n\nThe size of the validation set is 0.20% of the entire training set, which means if we set the number of blocks in cross validation to 4 blocks, the size of each block will be the same size as the validation set. I used a combination of accuracy score from cross_val_score and the validation set as the accuracy score of the model. You can use your own!","b0f320ba":"# Step 7. Creating the Kaggle submission file","87f925e8":"## g. AdaBoost","7f7397b4":"It looks like **Cabin** is missing a lot of values. Since we dont have much information about Cabin and about 70% of attributes are missing, we can drop it from the analyses for now. **PassengerId** and **Ticket** may also be dropped as they dont contribute to survival.\n\nThe column **Name** includes the title of the passengers. We can try extracting the passenger's titles from their name to see if there is a correlation between survivals and the titles:","a2d844de":"# Step 5: Comparing Models \/ Ensemble Learning","301aaad8":"There are three other categorical features on the list:\n(1) **Sex** has two values of male and female, \n(2) **Embarked** has three values of C, S, and Q, and \n(3) **Pclass** has three values of 1, 2, 3\nOur pipeline will directly handle the values of these attributes.","f31fa314":"# Step 3: Cleaning Data \/ Feature Engineering\nIn this step, we take care of filling missing attributes, scaling, and combining features. Some classifiers only work on scaled data, so its important to make sure the training data is scaled before getting fed to the classifier for training.\n\nIn this step, we use the pipeline implementation of Scikit-learn 0.20 to create a reusable preprocessing pipeline. \nLet's start with exploring the type of features and number of missing attributes.","bb509c30":"# Classification 101 - Step By Step Predicting Survival on Titanic | Kaggle\n\nMaryam Ashoori \n\n_Requirements: Scikit-learn 0.20_\n![](https:\/\/i.imgur.com\/QoK4WwK.png)\n\nIn this notebook, we review and compare the performance of following classifiers on Titanic data.\n\n- Logistic Classifier\n- Stochastic Gradient Descent (SGD) Classifier\n- Support Vector Machine (SVM)\n- Bagging Classifier\n- Extra Trees Classifier\n- Random Forest Classifier\n- k-Nearest Neighbors\n- AdaBoost Classifier\n- GradientBoosting Classifier\n- Voting Classifier\n\n# Step 1: Setup and importing data\n## a. Preparing the environment\nFirst thing first, let's import all the libraries, packages, and modules that we need for running these models.\nI like to organize and list all dependancies at the top of the code. This will allow us to review in one glance what's happening in the code."}}