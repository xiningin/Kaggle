{"cell_type":{"2602732d":"code","4663c0d9":"code","3d71d6d8":"code","d3ee115f":"code","b1965c1e":"code","694e498c":"code","69a44253":"markdown","1573cec7":"markdown","7bfdd7e0":"markdown","5eb97324":"markdown","a1513659":"markdown","3958137f":"markdown","1a91260b":"markdown","f49521e1":"markdown"},"source":{"2602732d":"######################\n# Environment        #\n######################\n\nimport warnings\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg \nimport seaborn as sns\nfrom scipy.stats import uniform, randint\nfrom xgboost import XGBClassifier\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.compose import make_column_transformer\n\nwarnings.filterwarnings(\"ignore\")\n\n######################\n# Dataframe          #\n######################\n\nshots = pd.read_csv(\"\/kaggle\/input\/shots-20072018\/shots_2007-2018.csv\")\nshots.head(15)\n\n# Loading the picture for the heatmap\nrink = mpimg.imread(\"\/kaggle\/input\/hockey-rinks\/hockey_rink_half.jpg\")","4663c0d9":"######################\n# Light data work    #\n######################\n\n# Only keeping post-lockout seasons\nshots=shots[shots.season>2012]\n\n# Dropping missed shots and empty net goals.\nshots = shots[shots.shotWasOnGoal == 1]\nshots = shots[shots.shotOnEmptyNet != 1]\n\n# Droping shots without goalie information or goal dummy\nshots = shots.dropna(axis=0, subset=[\"goalieNameForShot\", \"goal\"])\n\n# Generating name, year identifiers\nshots[\"goalie\"] = shots[\"goalieNameForShot\"] + \"_\" + shots[\"season\"].astype(str)\nshots[\"shooter\"] = shots[\"shooterName\"] + \"_\" + shots[\"season\"].astype(str)\nshots[\"shooterTeam\"] = shots[\"teamCode\"] + \"_\" + shots[\"season\"].astype(str)\n\n# Generating a shooter goal\/shots ratio\nratio_1 = shots.loc[:,(\"goal\",\"shooter\")]\nratio_1 = ratio_1.groupby(\"shooter\", as_index=False).mean()\nratio_1 = ratio_1.rename(columns={\"goal\": \"shooter_ratio\"})\nshots=shots.merge(ratio_1, on = \"shooter\")\n\n# Generating a shooting team goal\/shots ratio\nratio_2 = shots.loc[:,(\"goal\",\"shooterTeam\")]\nratio_2 = ratio_2.groupby(\"shooterTeam\", as_index=False).mean()\nratio_2 = ratio_2.rename(columns={\"goal\": \"team_ratio\"})\nshots=shots.merge(ratio_2, on = \"shooterTeam\")\n\n# Generating squared features\nshots[\"shotAngleAdjusted_squared\"] = shots[\"shotAngleAdjusted\"] ** 2\nshots[\"arenaAdjustedShotDistance_squared\"] = shots[\"arenaAdjustedShotDistance\"] ** 2\n\n############################\n# Variables assignation    #\n############################\n\ny = shots[\"goal\"]\n\ngoalie = shots[\"goalie\"]\n\nX = shots[[\"season\", \"isPlayoffGame\", \"period\", \"isHomeTeam\", \"playerPositionThatDidEvent\", \"shooterLeftRight\", \"shotAngleAdjusted\", \"shotAngleAdjusted_squared\", \"shotAnglePlusRebound\", \"shotAngleReboundRoyalRoad\", \"shotType\", \"shotRebound\", \"shotAnglePlusReboundSpeed\", \"offWing\", \"speedFromLastEvent\",\"arenaAdjustedShotDistance\", \"arenaAdjustedShotDistance_squared\", \"shotRush\", \"timeSinceLastEvent\", \"lastEventCategory\", \"timeSinceFaceoff\", \"shooterTimeOnIce\", \"shooterTimeOnIceSinceFaceoff\", \"shootingTeamForwardsOnIce\", \"shootingTeamDefencemenOnIce\", \"shootingTeamAverageTimeOnIce\", \"shootingTeamAverageTimeOnIceOfForwardsSinceFaceoff\", \"defendingTeamForwardsOnIce\", \"defendingTeamDefencemenOnIce\", \"defendingTeamAverageTimeOnIce\", \"defendingTeamAverageTimeOnIceSinceFaceoff\", \"shooter_ratio\", \"team_ratio\"]]\n\ncategorical = [\"season\", \"isPlayoffGame\", \"period\", \"isHomeTeam\", \"playerPositionThatDidEvent\",\n       \"shooterLeftRight\", \"shotType\", \"shotRebound\", \"offWing\",\n       \"shotRush\", \"lastEventCategory\"]\n\nnumeric = [\"shotAngleAdjusted\", \"shotAngleAdjusted_squared\", \"shotAnglePlusRebound\", \"shotAngleReboundRoyalRoad\", \"shotAnglePlusReboundSpeed\", \"arenaAdjustedShotDistance\", \"arenaAdjustedShotDistance_squared\", \"timeSinceLastEvent\", \"speedFromLastEvent\", \"timeSinceFaceoff\", \"shooterTimeOnIce\", \"shooterTimeOnIceSinceFaceoff\", \"shootingTeamForwardsOnIce\", \"shootingTeamDefencemenOnIce\", \"shootingTeamAverageTimeOnIce\", \"shootingTeamAverageTimeOnIceOfForwardsSinceFaceoff\", \"defendingTeamForwardsOnIce\", \"defendingTeamDefencemenOnIce\", \"defendingTeamAverageTimeOnIce\", \"defendingTeamAverageTimeOnIceSinceFaceoff\"]","3d71d6d8":"##############\n#   xgboost  #\n##############\n\n# Preprocessor\npreprocessor = make_column_transformer((make_pipeline(SimpleImputer(strategy=\"most_frequent\"), \n                                                      OneHotEncoder(handle_unknown=\"ignore\")), categorical), \n                                       (make_pipeline(SimpleImputer(strategy=\"median\"), \n                                                      StandardScaler()), numeric))\n\n# Parameters for random hyperparameters testing\n\nparams = {\n    \"xgbclassifier__colsample_bytree\": uniform(0.7, 0.3),\n    \"xgbclassifier__gamma\": uniform(0, 0.5),\n    \"xgbclassifier__learning_rate\": uniform(0.03, 0.3),\n    \"xgbclassifier__max_depth\": randint(2, 6),\n    \"xgbclassifier__n_estimators\": randint(100, 150),\n    \"xgbclassifier__subsample\": uniform(0.6, 0.4)\n}\n\n# Fitting the model, with random hyperparameter tuning\n\nmodel = make_pipeline(preprocessor,\n                        RandomizedSearchCV(XGBClassifier(), \n                                           param_distributions=params, random_state=69, n_iter=5, cv=3, n_jobs=-1)\n                        )\n\nmodel.fit(X, y)\n\n# Retrieving the predictions\nprob_goal = pd.Series(model.predict_proba(X)[:,1])","d3ee115f":"#########################\n#   Some visualisation  #\n#########################\n\n# Ploting the distribution of the predicted values\nsns.kdeplot(prob_goal)\nplt.show()","b1965c1e":"# Preparing the heatmap\nhm_data = shots.loc[:,(\"arenaAdjustedXCord\", \"arenaAdjustedYCord\")]\nhm_data[\"goal\"] = prob_goal\nhm_data.loc[hm_data.arenaAdjustedXCord < 0, \"arenaAdjustedYCord\"] = -hm_data.arenaAdjustedYCord\nhm_data[\"arenaAdjustedXCord\"]=hm_data[\"arenaAdjustedXCord\"].abs()\n\nbinsx = 8\nbinsy = 8\n\nhm_data=hm_data.groupby([pd.cut(hm_data.arenaAdjustedYCord, binsy), \n                 pd.cut(hm_data.arenaAdjustedXCord, binsx)])[\"goal\"].mean().unstack()\n\n# Ploting the heatmap\nplt.clf()\nhm = sns.heatmap(hm_data, cmap=\"YlOrRd\", robust = True, yticklabels=False, vmin=0.07,\n                   xticklabels=False, alpha = 0.4, zorder = 2)\nhm.imshow(rink,\n          aspect = hm.get_aspect(),\n          extent = hm.get_xlim() + hm.get_ylim(),\n          zorder = 1)\n\nplt.title(\"Predicted goal probability\")\nplt.show()","694e498c":"############################################\n#  getting the average goalie performance  #\n############################################\n\n# I get a df with one row per shot\ndf = shots[[\"goalie\", \"goal\", \"season\"]]\ndf = pd.concat([df, prob_goal], axis=1)\ndf = df.rename(columns={0:\"prob_goal\"})\n\n# Grouping by goalie + season\ndf2 = df.groupby([\"goalie\", \"season\"]).agg([\"sum\", \"count\"])\ndf2.columns = df2.columns.droplevel(1)\ndf2.columns = [\"goals\", \"shots\",\"avg_goalie_goals\",\"x\"]\ndf2 = df2.drop(columns = \"x\")\ndf2 = df2.reset_index()\n\n# Computing save shot percentage (svs%) and average goalie svs%\ndf2[\"svs%\"] = df2[\"shots\"] - df2[\"goals\"]\ndf2[\"svs%\"] = df2[\"svs%\"]\/df2[\"shots\"]\n\ndf2[\"svs%_average\"] = df2[\"shots\"] - df2[\"avg_goalie_goals\"]\ndf2[\"svs%_average\"] = df2[\"svs%_average\"]\/df2[\"shots\"]\n\ndf2[\"diff\"] = df2[\"svs%\"] - df2[\"svs%_average\"]\n\n# Computing ranks\ndf2[\"svs%_rank\"] = df2.groupby(\"season\")[\"svs%\"].rank(\"dense\", ascending=False).astype(int)\ndf2[\"good_ranking\"] = df2.groupby(\"season\")[\"diff\"].rank(\"dense\", ascending=False).astype(int)\n\ndf2[\"rank_gain\"] = df2[\"svs%_rank\"] - df2[\"good_ranking\"]\n\n'''\n\ndf2 = df2.sort_values(\"svs%\", ascending=False).reset_index(drop=True)\ndf2[\"svs%_ranking\"] = df2.index+1\n\ndf2 = df2.sort_values(\"diff\", ascending=False).reset_index(drop=True)\ndf2[\"good_ranking\"] = df2.index+1\n\n'''\n\n# Getting the number of game played\ngames = shots[[\"goalie\", \"game_id\"]].groupby(\"goalie\").agg(pd.Series.nunique).reset_index()\ngames = games.rename(columns={\"game_id\":\"games_played\"})\n\ndf2 = pd.merge(left=df2, right=games, how=\"left\", on=\"goalie\")\n# Reputting the goalie name\ndf2 = pd.merge(left=df2, right=shots[[\"goalie\", \"goalieNameForShot\"]], how=\"left\", on=\"goalie\")\ndf2 = df2.drop(\"goalie\", axis=1)\ndf2 = df2.rename(columns={\"goalieNameForShot\":\"goalie\"})\n\n# OUTPUT\ndf3 = pd.concat([df2[\"goalie\"], df2.iloc[:,:-1]], axis=1)\ndf3 = df3.sort_values(by=[\"season\", \"good_ranking\"])\ndf3 = df3.round({\"avg_goalie_goals\": 2, \"svs%\": 4, \"svs%_average\": 4, \"diff\": 4})\n\ndf3.to_csv(\"\/kaggle\/working\/rankings.csv\", index=False)\n\ndf4 = df3.loc[df3[\"games_played\"]>= 24, :]\ndf4.to_csv(\"\/kaggle\/working\/rankings_more_than_24_games.csv\", index=False)","69a44253":"# Discussion\n\n\nWORK IN PROGRESS","1573cec7":"# Robustness\n\nWORK IN PROGRESS","7bfdd7e0":"# Xgboost\nBoosting machines with random trees work well for prediction. We do not do much to get the best precision posssible since we want to keep it computationally cheap. It is also unlikely to be very usefull since this specification (xgboost with 50 iterations of random hypertuning, 5-fold cross-validation) is already quite precise and we do not see why the prediction errors would favor one goalie over another. This kind of error analysis is important in real life work, but it's only a fun side project for us and we do not wish to spend weeks on it.\n\nThe light_gbm algorithm could have also been a good candidate, and most likely a better one. In fact, we don't know why we did not use it, since we use it much more often in our real work.","5eb97324":"The most used measurements of goalie performance in Hockey are limited at best. For example, the save shot percentage, measured as the ratio of saved shots over the total amount of shots in a game, does not take the shots\" charateristics into consideration, leading to overestimating the performances by goalies that play in teams with great defensive skills. Shots that originate from close from the goalie a for example much harder to stop than shots from a bigger distance.\n\nOne way to solve this issue is to use the characteristics of a shot (distance from the goalie, type of shot, whether it is a rebound, etc.) to make a prediction on its probability to be a goal. The higher the probability, the more dangerous the shot. Summing these probabilities over a game or a season gives the expected amound of goals for the average goalie in the league. If your favorite goalie gave less goals than that, than he did better than the league average. And the lower this number, the better he did.\n\nIn this notebook, we produce such a ranking. We first use the shots data from the [*MoneyPuck* database](http:\/\/moneypuck.com\/data.htm) to build a prediction machine for the NHL shots' probability to be a goal. Unsurprisingly, we use a gradient boosting algorithm from the xgboost library. We then produce an NHL goalies ranking based on their difference between the amount of goals they gave in a season and the amount of goals the average NHL goalie would have let if he received the very same shots.\n\n# Hypothesis\n\nWe must warn the reader that the validity of our ranking is based on some assumptions, the most important one being that the goalies do not have an impact on the quality of the shots they receive. It is not unreasonable to assume that it is not the case: goalies do to passively wait for shots, they intercept passes, intimidate their opponents with their reputation\/confidence and often make passes. They also try to deviate the shots outside the dangerous zones or to freeze them in order to avoid dangerous rebounds. One way to alleviate this concern is to hypothesize that while goalies do have an impact on the shots they receive, their ability to do so is tighly correlated with their talent to stop these shots. In that case, our method would simply understimate the difference between the good and the bad goalies, but not affect their ranking.\n\nWe explore the validity of this hypothesis in the section \"Robustness\" (WORK IN PROGRESS).\n\n# Setting up the environment and loading the data\n\nWe first load our environment, working with the industry standard modules. The flat file can also be accessed from this [link](http:\/\/peter-tanner.com\/moneypuck\/downloads\/shots_2007-2018.zip).","a1513659":"# SHAP\n\nWORK IN PROGRESS","3958137f":"# Final thoughts\nWORK IN PROGRESS","1a91260b":"# Data work\nThis part does minor feature engineering and identifies the categorical and numerical features. It is worth noting that we rename goalies to include the season in their name (e.g. \"Carey Price_2018\"). We do the same thing for the shooting player and its team. Not only this will make it easier to compute the season performances, it makes sense to think that the teams, players and goalies are not static, but evolve over time. To put it simply: I'd rather receive a shot by the Brandon Gallagher from 2012 than by today's Gallagher.\n\nWe also include the squared terms of shot distance and angle, in order to help the model attribute more danger to shots from a medium distance, or more from short and long distance only. On could argue that it is a good strategy for traditionnal models like a linear regression, but that it is not that useful for tree-based models (esp. boosting models). It is something that we could test some day.\n\nMore importantly, we avoid using features that would predict too well the goalie or the defending team to train the model. This way, we get the dangerousness of a shot given an average NHL goalie, which is the element that we will need for the ranking.","f49521e1":"# Ex post evaluation of the model"}}