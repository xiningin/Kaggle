{"cell_type":{"cb44fac7":"code","868b1798":"code","41a75ea7":"code","bd4b4f9c":"code","3caeaeee":"code","9e27004d":"code","e4b318ec":"code","d29d2c39":"code","a798e43e":"code","cb5fe08a":"code","0a9733ad":"code","3ced8e59":"code","cbae93d6":"code","c169dd07":"code","1aee3c84":"code","8f0de675":"code","d898a8ee":"code","2b5686e2":"code","cd28debe":"code","3d18ee2d":"code","785251d6":"code","181dde67":"code","bd19f832":"code","d207791b":"code","b7a8db0c":"code","96f5dfa0":"code","842118c6":"code","5c3461bc":"code","1c1be1bb":"code","413750fd":"code","6e015956":"code","e1d6c80d":"code","3d7d5a52":"markdown","286b6c36":"markdown","09d66b70":"markdown","e78e50bf":"markdown","43dde58b":"markdown","732c4100":"markdown","bb3a069e":"markdown","eeeb5cc9":"markdown","e1cc9134":"markdown","16d8bd31":"markdown","332d1c7f":"markdown","ec6533ac":"markdown"},"source":{"cb44fac7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","868b1798":"train = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntrain.head()","41a75ea7":"train.info()","bd4b4f9c":"train[\"Age\"] = train[\"Age\"].fillna(int(np.mean(train[\"Age\"])))\ntrain[\"Embarked\"] = train[\"Embarked\"].fillna(train[\"Embarked\"].mode()[0])\ntrain[\"FamilySize\"] = train['SibSp'] + train['Parch']\ntrain[\"IsAlone\"] = train[\"FamilySize\"].apply(lambda x: 1 if x == 0 else 0)\ntrain[\"Title\"] = train[\"Name\"].apply(lambda x: x.split(\",\")[1].split(\".\")[0])\ntrain[\"Title\"] = train[\"Title\"].str.strip()\nimp_titles = ['Mr', 'Mrs', 'Miss', 'Master', 'Dr', 'Rev']\ntrain[\"Title\"] = train[\"Title\"].apply(lambda x: x if x in imp_titles else \"misc\")\ntrain[\"actualFare\"] = train[\"Fare\"]\/train[\"FamilySize\"].replace(0,1)\n","3caeaeee":"def get_age_bin(x):\n    if x<=10:\n        return \"kids\"\n    elif x>10  and x<=20:\n        return \"teens\"\n    elif x>20 and x<=40:\n        return \"Adults\"\n    elif x>40 and x<=65:\n        return \"Mid Age\"\n    else:\n        return \"Old\"\n    \ntrain[\"AgeGroup\"] = train[\"Age\"].apply(get_age_bin)","9e27004d":"import seaborn as sns\nplt = train.groupby(\"AgeGroup\")[\"Survived\"].mean().reset_index()\nsns.lineplot(x = plt[\"AgeGroup\"], y = plt[\"Survived\"])\n","e4b318ec":"grpby = train.groupby([\"Pclass\", \"IsAlone\", \"AgeGroup\"])[\"Survived\"].agg([\"count\", \"sum\"]).reset_index()\ngrpby[\"per\"] = grpby[\"sum\"]\/grpby[\"count\"]\ngrpby = grpby.sort_values(by = \"per\", ascending=False)\ngrpby","d29d2c39":"grpby[\"dict_key\"] = grpby[\"Pclass\"].astype(str) + \"#\" + grpby[\"IsAlone\"].astype(str) + \"#\" + grpby[\"AgeGroup\"].astype(str)\ngrpby_dict = {i:j for i,j in zip(grpby[\"dict_key\"], grpby[\"per\"])}\ndef get_combi(x):\n    global grpby_dict\n    comb = str(x[\"Pclass\"]) + \"#\" + str(x[\"IsAlone\"]) + \"#\" + str(x[\"AgeGroup\"])\n    if grpby_dict[comb]<=0.70:\n        return 0\n#    elif grpby_dict[comb]>0.30 and grpby_dict[comb]<=0.50:\n#        return 1\n#    elif grpby_dict[comb]>0.50 and grpby_dict[comb]<=0.80:\n#        return 2\n    else:\n        return 1\n    \ntrain[\"combi\"] = train[[\"Pclass\", \"IsAlone\", \"AgeGroup\"]].apply(lambda x: get_combi(x), axis=1)\n","a798e43e":"pd.crosstab(train[\"Survived\"], [train[\"Pclass\"], train[\"IsAlone\"]])","cb5fe08a":"!pip install sweetviz","0a9733ad":"import sweetviz as sv\nreport = sv.analyze(train, \"Survived\")\nreport.show_html(\"\/kaggle\/working\/EDA.html\")","3ced8e59":"#cols = ['Pclass', 'Sex', 'Age', 'Fare', \"FamilySize\", \"HaveCabin\", \"Title\", \"actualFare\"]\ncols = ['Pclass', 'Sex', 'AgeGroup', \"Title\", \"actualFare\", \"combi\"]\ncols_to_encode = train[cols].select_dtypes(include=['object']).copy()\ncols_to_encode = list(cols_to_encode.columns)","cbae93d6":"test = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntest[\"Age\"] = test[\"Age\"].fillna(int(np.mean(test[\"Age\"])))\ntest[\"Fare\"] = test[\"Fare\"].fillna(np.mean(test[\"Fare\"]))\ntest[\"FamilySize\"] = test['SibSp'] + test['Parch']\ntest[\"IsAlone\"] = test[\"FamilySize\"].apply(lambda x: 1 if x == 0 else 0)\ntest[\"HaveCabin\"] = test[\"Cabin\"].fillna(\"\").apply(lambda x: 0 if x == \"\" else 1)\ntest[\"Title\"] = test[\"Name\"].apply(lambda x: x.split(\",\")[1].split(\".\")[0])\ntest[\"Title\"] = test[\"Title\"].str.strip()\ntest[\"Title\"] = test[\"Title\"].apply(lambda x: x if x in imp_titles else \"misc\")\ntest[\"actualFare\"] = test[\"Fare\"]\/test[\"FamilySize\"].replace(0,1)\ntest[\"AgeGroup\"] = test[\"Age\"].apply(get_age_bin)\ntest[\"combi\"] = test[[\"Pclass\", \"IsAlone\", \"AgeGroup\"]].apply(lambda x: get_combi(x), axis=1)","c169dd07":"from category_encoders.m_estimate import MEstimateEncoder\nfrom sklearn.model_selection import train_test_split\nMEE_encoder = MEstimateEncoder()\n\ndef get_x_and_y(train, test, y_col=\"Survived\"):\n    global cols, MEE_encoder\n    y = train[y_col]\n    train = train[cols].copy()\n    test = test[cols].copy()\n    train_mee = MEE_encoder.fit_transform(train, y) \n    test_mee = MEE_encoder.transform(test)\n    return train_mee, test_mee\n\n\ntrain1, test1 = get_x_and_y(train, test)\nX = train1.copy()\nY = train['Survived'].copy()\nx_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state=0)","1aee3c84":"from sklearn import svm\nclf = svm.SVC()\nclf.fit(x_train, y_train)\ny_pred = clf.predict(x_test)\nnp.mean(y_pred == y_test)","8f0de675":"from sklearn.model_selection import GridSearchCV\nfrom skopt.space import Real, Categorical, Integer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nimport catboost as cat\nfrom lightgbm import LGBMClassifier\n\n\ndef random_forest(train_x, train_y, test_x, test_y):\n    rf = RandomForestClassifier()\n\n    # Create the parameter grid based on the results of random search \n    rf_grid = {\n        'max_depth': [50, 70, 110],\n        'max_features': [3, 4, 6],\n        'min_samples_leaf': [2, 3, 4, 5],\n        'min_samples_split': [3,4,6],\n        'n_estimators': [100, 300, 2000, 1000]\n    }\n\n\n    grid_search = GridSearchCV(estimator = rf, param_grid = rf_grid,\n                              cv = 3, n_jobs = -1, verbose = 2)\n    grid_search.fit(train_x, train_y)\n\n    parameters = grid_search.best_params_\n    \n    print(parameters)\n    rf = RandomForestClassifier(**parameters)\n    rf.fit(train_x, train_y)\n    y_pred = rf.predict(test_x)\n    importances = rf.feature_importances_\n    indices = np.argsort(importances)\n    sns.barplot(x = np.arange(len(indices)), y = importances[indices])\n    return np.mean(y_pred == test_y),\n\n\ndef xg_boost(train_x, train_y, test_x, test_y):\n    xgb = XGBClassifier(n_estimators=100, learning_rate=0.1, objective = 'binary:logistic',\n                        colsample_bytree = 0.5, max_depth = 10, alpha = 1, gamma = 0, booster = 'gbtree')\n\n    xgb_grid = {'n_estimators':[50, 100, 200, 500],\n                'learning_rate':[0.01, 0.1, 0.25,  0.75],\n                'colsample_bytree':[0.1, 0.3, 0.5],\n                'max_depth':[8, 12, 14],\n                'alpha': [1, 2, 3],\n                'objective': ['binary:logistic'],\n                'gamma':[0],\n                'booster': ['gbtree']}\n\n    grid_search = GridSearchCV(estimator = xgb, param_grid = xgb_grid,\n                                  cv = 3, n_jobs = -1, verbose = 2)\n    grid_search.fit(train_x, train_y)\n\n    parameters = grid_search.best_params_\n    \n    print(parameters)\n    xgb = XGBClassifier(**parameters)\n    xgb.fit(train_x, train_y)\n    y_pred = xgb.predict(test_x)\n    return np.mean(y_pred == test_y)\n\ndef cat_boost(train_x, train_y, test_x, test_y):\n    ctb = cat.CatBoostClassifier(learning_rate=0.75, n_estimators=100,\n                                 subsample=0.5, loss_function='Logloss',\n                                 depth = 8, l2_leaf_reg = 2, bagging_temperature = 1.0,\n                                 )\n    \n    cat_grid = {'n_estimators': [100, 500, 1000],\n                'depth': [1, 3, 7],\n                'learning_rate': [0.1, 0.01, 0.05],\n                'bagging_temperature': [1.0],\n                'l2_leaf_reg': [2, 30],\n                'scale_pos_weight': [0.01, 1.0],\n                'subsample':[0.1, 0.3, 0.5],\n                'loss_function' : ['Logloss']}\n\n\n    grid_search = GridSearchCV(estimator = ctb, param_grid = cat_grid,\n                              cv = 3, n_jobs = -1, verbose = 2)\n    grid_search.fit(train_x, train_y)\n    \n    parameters = grid_search.best_params_\n    \n    print(parameters)\n    ctb = cat.CatBoostClassifier(**parameters)\n    ctb.fit(train_x, train_y)\n    y_pred = ctb.predict(test_x)\n    return np.mean(y_pred == test_y)\n\ndef light_gbm(train_x, train_y, test_x, test_y):\n    lgbm = LGBMClassifier()\n    lgbm_grid = {'n_estimators': [100, 500, 1000],\n                 'boosting_type':['rf', 'gbdt'],\n                'max_depth': [1, 3, 7],\n                'learning_rate': [0.01, 0.1,0.25],\n                'objective': [ 'binary'],\n                'reg_alpha': [0.1, 0.5],\n                'reg_lambda': [0.1, 0.5],\n                'subsample':[0.1, 0.3, 0.5]\n                }\n\n\n    grid_search = GridSearchCV(estimator = lgbm, param_grid = lgbm_grid,\n                              cv = 3, n_jobs = -1, verbose = 2)\n    grid_search.fit(train_x, train_y)\n    \n    parameters = grid_search.best_params_\n    print(parameters)\n    \n    lgbm = LGBMClassifier(**parameters)\n    lgbm.fit(x_train, y_train)\n    y_pred = lgbm.predict(x_test)\n    return np.mean(y_pred == y_test)\n","d898a8ee":"random_forest(x_train, y_train, x_test, y_test)","2b5686e2":"importances = rf.feature_importances_\nindices = np.argsort(importances)","cd28debe":"xg_boost(x_train, y_train, x_test, y_test)","3d18ee2d":"cat_boost(x_train, y_train, x_test, y_test)","785251d6":"light_gbm(x_train, y_train, x_test, y_test)","181dde67":"train_x.columns","bd19f832":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\n\nmodel = Sequential()\nmodel.add(Dense(27, input_dim=8, activation='relu'))\nmodel.add(Dense(36, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\n# Compile model\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.fit(train_x, train_y, epochs=200)\ny_pred = model.predict(test_x)","d207791b":"test_loss, test_acc = model.evaluate(test_x, test_y)\nprint('Test accuracy:', test_acc)","b7a8db0c":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(x_train, y_train)\ny_pred = lr.predict(x_test)\nnp.mean(y_pred == y_test)","96f5dfa0":"from vecstack import stacking\nfrom sklearn.metrics import accuracy_score\n\nseed = 0\n\nbase_models = [ctb, rf, lr]\nS_train, S_test = stacking(base_models,                # list of base models\n                           train_x, train_y, test_x,   # data\n                           regression = False,          # We need regression - set to True)\n                                                       \n                           mode = 'oof_pred_bag',      # mode: oof for train set, predict test \n                                                       # set in each fold and vote\n                           needs_proba = False,        # predict class labels (if you need \n                                                       # probabilities - set to True) \n                           save_dir = None,            # do not save result and log (to save \n                                                       # in current dir - set to '.')\n                           metric = accuracy_score,# metric: callable\n                           n_folds = 5,               # number of folds\n                           stratified = False,         # stratified split for folds\n                           shuffle = True,             # shuffle the data\n                           random_state =  seed,       # ensure reproducibility\n                           verbose = 1)    \n\nsuper_learner = xgb\nsuper_learner.fit(S_train, train_y)\nStack_pred = super_learner.predict(S_test)","842118c6":"test1.info()","5c3461bc":"S_train, S_test = stacking(base_models,                # list of base models\n                           train1, train[\"Survived\"], test1,   # data\n                           regression = False,          # We need regression - set to True)\n                                                       \n                           mode = 'oof_pred_bag',      # mode: oof for train set, predict test \n                                                       # set in each fold and vote\n                           needs_proba = False,        # predict class labels (if you need \n                                                       # probabilities - set to True) \n                           save_dir = None,            # do not save result and log (to save \n                                                       # in current dir - set to '.')\n                           metric = accuracy_score,# metric: callable\n                           n_folds = 5,               # number of folds\n                           stratified = False,         # stratified split for folds\n                           shuffle = True,             # shuffle the data\n                           random_state =  seed,       # ensure reproducibility\n                           verbose = 1)    \n\nsuper_learner = xgb\nsuper_learner.fit(train1, train[\"Survived\"])\nStack_pred = super_learner.predict(test1)","1c1be1bb":"from sklearn.model_selection import GridSearchCV\nfrom skopt.space import Real, Categorical, Integer\n# Create the parameter grid based on the results of random search \n\n\nxgb_grid = {'n_estimators':[50, 100, 150, 300],\n            'learning_rate':[0.1, 0.01, 0.05],\n            'colsample_bytree':[0.1, 0.3, 0.5],\n            'max_depth':[10, 12, 14],\n            'alpha': [1, 2, 3]}\n\ncat_grid = {'n_estimators': [100, 500, 1000],\n            'depth': [1, 7, 8],\n            'learning_rate': [0.1, 0.01, 0.05],\n            'bagging_temperature': [0.0, 1.0],\n            'border_count': [1, 255],\n            'l2_leaf_reg': [2, 30],\n            'scale_pos_weight': [0.01, 1.0]}\n\n\ngrid_search = GridSearchCV(estimator = ctb, param_grid = cat_grid,\n                          cv = 3, n_jobs = -1, verbose = 2)\ngrid_search.fit(train_x, train_y)\n","413750fd":"grid_search.best_params_","6e015956":"test[\"predictions\"] = pd.Series(Stack_pred)\ntest[\"predictions\"].isnull().sum()","e1d6c80d":"\nsubmissionFile = test[[\"PassengerId\", \"predictions\"]].copy()\nsubmissionFile = submissionFile.rename(columns = {\"predictions\": \"Survived\"})\nsubmissionFile.to_csv(\"\/kaggle\/working\/predictions.csv\", index = False)","3d7d5a52":"Applying RandomForest Classifier","286b6c36":"Applying model Stacking","09d66b70":"Tuning the models","e78e50bf":"Applying SVM","43dde58b":"Creating the submission file","732c4100":"Applying ANN","bb3a069e":"Making data imputations","eeeb5cc9":"Loading the files","e1cc9134":"Checking columns where there are null values","16d8bd31":"Applying LogisticRegression","332d1c7f":"Encoding the datasets to fit in the models","ec6533ac":"Applying the same pre-processing on the test set"}}