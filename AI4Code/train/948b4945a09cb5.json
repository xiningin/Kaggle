{"cell_type":{"7e71116d":"code","9b683d3f":"code","57525bc1":"code","052f5652":"code","d64cc19e":"code","c0754eed":"code","1a7d8e78":"code","d1a96553":"code","d438c66f":"code","348463b4":"code","544b905a":"code","271a613a":"code","67c74297":"code","f3182745":"code","0210daa9":"code","515c238a":"code","330e1d4f":"code","3fb0576d":"code","e06a25e6":"code","4bde09d8":"code","7a7e92f1":"code","98586c52":"code","8496ddfb":"code","d32917f7":"code","7941683e":"code","f08e14db":"code","4d5354eb":"code","b6441f8a":"code","f425c9c9":"code","81afecb1":"code","dac1252b":"code","b4471798":"code","e990fbee":"code","8a170845":"markdown","15f2c2e9":"markdown","14f4ff07":"markdown","c556a606":"markdown","df88c47e":"markdown","ad917d8b":"markdown","4e7c09fb":"markdown","00706b08":"markdown","44fee799":"markdown"},"source":{"7e71116d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9b683d3f":"# Let's import some libraries\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n","57525bc1":"# Load the dataset:\n\ndf = pd.read_csv('\/kaggle\/input\/pima-indians-diabetes-database\/diabetes.csv')\ndf.head()","052f5652":"# shape of the dataset\nprint(\"Shape of the Dataset:\",df.shape)","d64cc19e":"# describe\ndf.describe()","c0754eed":"# info\ndf.info()","1a7d8e78":"# columns\ndf.columns","d1a96553":"# Let's plot the y variable\n\nplt.figure(figsize=(5,4))\nsns.distplot(df['Outcome'],color = 'red')\nplt.show()","d438c66f":"# Let's plot the relation X variables and y variable\n\nplt.figure(figsize=(5,4))\nsns.barplot(df['Outcome'],df['Outcome'].value_counts(),color = 'red')\nplt.show()","348463b4":"df['Outcome'].value_counts()","544b905a":"# Let's start modelling\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score,confusion_matrix,classification_report \nfrom sklearn.preprocessing import StandardScaler","271a613a":"## Let'sdivide the dataset into X and y\ny = df.pop('Outcome')\nX = df","67c74297":"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.3,random_state = 20)","f3182745":"print(\"X Train shape :\", X_train.shape)\nprint(\"X Test shape  :\", X_test.shape)\nprint(\"Y Train shape :\", y_train.shape)\nprint(\"Y Test shape  :\", y_test.shape)","0210daa9":"logreg = LogisticRegression()\nlogreg.fit(X_train,y_train)","515c238a":"y_pred = logreg.predict(X_test)\nresult = pd.DataFrame({'Actual ': y_test,'Predicted':y_pred})\nresult.head()","330e1d4f":"cm = confusion_matrix(y_pred,y_test)\ncr = classification_report(y_pred,y_test)\nacc = accuracy_score(y_pred,y_test)\n\nprint(\"**** Before Scaling ****\")\nprint(\"Accuracy After Scaling:\", acc)\nprint(\"Classification Report :\\n\",cr)\nprint(\"confusion Matrix      :\\n\",cm)","3fb0576d":"tn = cm[0][0]\nfp = cm[0][1]\nfn = cm[1][0]\ntp = cm[1][1]\n\nrecall = tp\/(tp+fn)*100\nprecision = tp\/(tp+fp)*100\nspecificity = tn\/(tn+fp)*100\n\nprint(\"**** Before Scaling ****\")\nprint(\"Recall\/Sensitivity :\",recall)\nprint(\"Precision          :\",precision)\nprint(\"Specificity        :\",specificity)","e06a25e6":"# Scaling the X variables\nscaler = StandardScaler()\nX_scale = scaler.fit_transform(X)","4bde09d8":"X_train_scaled,X_test_scaled,y_train_scaled,y_test_scaled = train_test_split(X_scale,y,test_size = 0.3,random_state = 20)","7a7e92f1":"print(\"X Train shape :\", X_train_scaled.shape)\nprint(\"X Test shape  :\", X_test_scaled.shape)\nprint(\"Y Train shape :\", y_train_scaled.shape)\nprint(\"Y Test shape  :\", y_test_scaled.shape)","98586c52":"logreg_scale = LogisticRegression()\nlogreg_scale.fit(X_train_scaled,y_train_scaled)","8496ddfb":"y_pred_scale = logreg_scale.predict(X_test_scaled)\nresult = pd.DataFrame({'Actual ': y_test_scaled,'Predicted':y_pred_scale})\nresult.head()","d32917f7":"cm_scale = confusion_matrix(y_pred_scale,y_test_scaled)\ncr_scale = classification_report(y_pred_scale,y_test_scaled)\nacc_scale = accuracy_score(y_pred_scale,y_test_scaled)\n\nprint(\"**** After Scaling ****\")\nprint(\"Accuracy              :\", acc_scale)\nprint(\"Classification Report :\\n\",cr_scale)\nprint(\"confusion Matrix      :\\n\",cm_scale)","7941683e":"tn_scale = cm_scale[0][0]\nfp_scale = cm_scale[0][1]\nfn_scale = cm_scale[1][0]\ntp_scale = cm_scale[1][1]\n\nrecall_scale = tp_scale\/(tp_scale  + fn_scale)*100\nprecision_scale = tp_scale\/(tp_scale + fp_scale)*100\nspecificity_scale = tn_scale\/(tn_scale + fp_scale)*(100)\n\nprint(\"**** After Scaling ****\")\nprint(\"Recall\/Sensitivity :\",recall_scale)\nprint(\"Precision          :\",precision_scale)\nprint(\"Specificity        :\",specificity_scale)","f08e14db":"from imblearn.over_sampling import SMOTE","4d5354eb":"# transform the dataset\noversample = SMOTE(random_state=42)\nX_smote, y_smote = oversample.fit_resample(X, y)","b6441f8a":"X_train_smote,X_test_smote,y_train_smote,y_test_smote = train_test_split(X_smote,y_smote,test_size = 0.3,random_state = 20)","f425c9c9":"print(\"X Train shape :\", X_train_smote.shape)\nprint(\"X Test shape  :\", X_test_smote.shape)\nprint(\"Y Train shape :\", y_train_smote.shape)\nprint(\"Y Test shape  :\", y_test_smote.shape)","81afecb1":"logreg_smote = LogisticRegression()\nlogreg_smote.fit(X_train_smote,y_train_smote)","dac1252b":"y_pred_smote = logreg_smote.predict(X_test_smote)\nresult = pd.DataFrame({'Actual ': y_test_smote,'Predicted':y_pred_smote})\nresult.head()","b4471798":"cm_smote = confusion_matrix(y_pred_smote,y_test_smote)\ncr_smote = classification_report(y_pred_smote,y_test_smote)\nacc_smote = accuracy_score(y_pred_smote,y_test_smote)\n\nprint(\"**** After applying SMOTE ****\")\nprint(\"Accuracy              :\", acc_smote)\nprint(\"Classification Report :\\n\",cr_smote)\nprint(\"confusion Matrix      :\\n\",cm_smote)","e990fbee":"tn_smote = cm_smote[0][0]\nfp_smote = cm_smote[0][1]\nfn_smote = cm_smote[1][0]\ntp_smote = cm_smote[1][1]\n\nrecall_smote = tp_smote\/(tp_smote + fn_smote)*100\nprecision_smote = tp_smote\/(tp_smote + fp_smote)*100\nspecificity_smote = tn_smote\/(tn_smote + fp_smote)*(100)\n\nprint(\"**** After applying SMOTE ****\")\nprint(\"Recall\/Sensitivity :\",recall_smote)\nprint(\"Precision          :\",precision_smote)\nprint(\"Specificity        :\",specificity_smote)","8a170845":"### After scaling :","15f2c2e9":"I have experimented in this kernel with following things to check the how it effects the :\n    1. Without doind any scaling on the data\n    2. After scaling the data\n    3. Using SMOTE for handling class imbalance","14f4ff07":"### After treating class imbalance","c556a606":"## I hope you like the kernel :) Please upvote if you like :P Will update the notebook with some other ML Algorithms","df88c47e":"### Before Scaling :","ad917d8b":"### Let's see some statistical details :","4e7c09fb":"## Pima Indians Diabetes Database","00706b08":"### Let's plot some visualisation and look at some insights","44fee799":"**Context:**\n\nThis dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. **The objective of the dataset is to diagnostically predict whether or not a patient has diabetes,** based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage.\n\n**Content:**\n\nThe datasets consists of several medical predictor variables and one target variable,**Outcome**. Predictor variables includes the number of pregnancies the patient has had, their BMI, insulin level, age, and so on."}}