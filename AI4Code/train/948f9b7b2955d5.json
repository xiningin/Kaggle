{"cell_type":{"37a9910d":"code","e3c0a023":"code","0bf1182e":"code","8d99aad1":"code","fd20b571":"code","aba4e311":"code","569979fe":"code","5b235b8d":"code","736608a7":"code","19595129":"code","4679c1a6":"code","945e7966":"code","97c7ccab":"code","cb0192dc":"code","04427565":"code","2eab4789":"code","6138a769":"code","77f7ee40":"code","a17d004e":"code","8ca99271":"code","0838ef72":"code","6075285b":"code","89874c9d":"code","0ee434e2":"code","88af2591":"code","35506136":"code","6371bde8":"code","32deff17":"code","0b43a11f":"code","b0746cac":"code","0589668c":"code","0bbf52a4":"code","926ad51e":"code","83d931c6":"code","5b97d259":"code","a63c3126":"code","10d619e4":"code","f28f73ec":"code","08d3c4d8":"code","7a3e31b5":"code","46580af8":"code","01da266e":"markdown"},"source":{"37a9910d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nnp.random.seed(444)\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom pandas.plotting import scatter_matrix\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import f1_score, classification_report, confusion_matrix, accuracy_score\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\/\"))\n\n\nfigure_size = (15,15)\n\n# Any results you write to the current directory are saved as output.","e3c0a023":"df = pd.read_csv('..\/input\/heart.csv')","0bf1182e":"def create_confusion_matrix(predictions, labels):\n    \n    confusion_matrix = {\"TP\": 0, \"TN\": 0, \"FP\": 0, \"FN\": 0}\n    \n    label_list = list(labels.values.flatten())\n\n    label_index = 0\n    \n    for row in predictions:\n\n        if row == label_list[label_index]:\n            \n            if row == 1:\n                confusion_matrix[\"TP\"] += 1\n            else:\n                confusion_matrix[\"TN\"] += 1\n        else:\n            if row == 1:\n                confusion_matrix[\"FP\"] += 1\n            else:\n                confusion_matrix[\"FN\"] += 1\n        label_index += 1\n\n    print(confusion_matrix)\n    print(\"n:{}\".format(label_index))\n    return confusion_matrix\n\ndef perf(model):\n    try:\n        preds = model.predict_classes(X_test)\n    except:\n        preds = model.predict(X_test)\n        \n    print(classification_report(y_test, preds))\n    create_confusion_matrix(preds, y_test)\n    accuracy_score(preds, y_test)\n\n\n    ","8d99aad1":"df.head()","fd20b571":"df.hist('cp', figsize=(7,7))","aba4e311":"df.hist(\"age\", bins=75)","569979fe":"unique_ages = pd.unique(df.age)\nprint(\"Number of unique ages: {}\".format(len(unique_ages)))\nunique_ages.sort(kind='mergesort')\ndisplay(unique_ages)","5b235b8d":"df.target.hist()","736608a7":"display(df.describe())\ndisplay(df.isna().sum())","19595129":"df.hist(bins=75, figsize=figure_size)","4679c1a6":"scatter_matrix(df, figsize=figure_size)","945e7966":"%matplotlib inline\ncorrellations = df.corr()\nfig = plt.figure(figsize=figure_size)\nax = fig.add_subplot(111)\ncax = ax.matshow(correllations, vmin=-1, vmax=1)\nfig.colorbar(cax)\nnames = df.columns\nticks = np.arange(0,len(names),1)\ndisplay(names)\nax.set_xticks(ticks)\nax.set_yticks(ticks)\nax.set_xticklabels(names)\nax.set_yticklabels(names)\nplt.show()","97c7ccab":"df.drop(['chol', 'fbs', 'ca', 'thal'], axis=1, inplace=True)","cb0192dc":"df.info()","04427565":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\ny = df.target\nX  = df.drop('target', axis=1)\nscaler = StandardScaler()\n# Fit the scaler to the data contained in 'X'\nscaler.fit(X)\n# Use scaler.transform() on the X and store the results below\nscaled_X = scaler.transform(X)\n\npca = PCA()\n# call pca.fit() on the data stored in 'scaled_data'.\npca.fit(scaled_X)\nx_pca = pca.transform(scaled_X)\n# pca_k_means = KMeans(n_clusters=len(X.columns))\n# pca_k_means.fit(x_pca)","2eab4789":"\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.5)","6138a769":"def comp(model):\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['mse', 'mae', 'mape', 'cosine'])\n    model.summary()\n    return model\n\ndef fit(model):\n    model.fit(X_train, list(y_train), batch_size=64, epochs=80, verbose=1, validation_data=(X_test, list(y_test)))\n    return model","77f7ee40":"from sklearn.metrics import classification_report, confusion_matrix,accuracy_score","a17d004e":"clf = DecisionTreeClassifier()\nfitted = clf.fit(X_train, list(y_train))\nperf(fitted)","8ca99271":"from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\nlogreg.fit(X_train,list(y_train))\nperf(logreg)\n# display(logregs.coef_)\n# display(logregs.intercept_)\n","0838ef72":"display(logreg.predict([X_test.iloc[5]]))\n# df.to_csv('..\/input\/saved.csv')\n","6075285b":"from sklearn import linear_model","89874c9d":"clf = linear_model.SGDClassifier(max_iter=1000, tol=1e-3)\nclf.fit(X_train,list(y_train))\nperf(clf)","0ee434e2":"from sklearn.naive_bayes import GaussianNB","88af2591":"clf = GaussianNB()\nclf.fit(X_train, list(y_train))\nperf(clf)","35506136":"from sklearn.cluster import KMeans","6371bde8":"from keras.models import Sequential\nfrom keras.layers import Dense, LeakyReLU\nfrom keras.utils import to_categorical","32deff17":"small_model = Sequential()\nsmall_model.add(Dense(9, input_shape=(9,)))\nsmall_model.add(Dense(4, activation='relu'))\nsmall_model.add(Dense(1, activation='sigmoid'))\ncomp(small_model)\nfit(small_model)","0b43a11f":"perf(small_model)","b0746cac":"small_relu_nn = Sequential()\nsmall_relu_nn.add(Dense(9, input_shape=(9,)))\nsmall_relu_nn.add(LeakyReLU(alpha=0.05))\nsmall_relu_nn.add(Dense(4))\nsmall_relu_nn.add(LeakyReLU(alpha=0.05))\nsmall_relu_nn.add(Dense(1, activation='sigmoid'))\n\ncomp(small_relu_nn)\nfit(small_relu_nn)","0589668c":"perf(small_relu_nn)","0bbf52a4":"med_model = Sequential()\nmed_model.add(Dense(9, input_shape=(9,)))\nmed_model.add(LeakyReLU(alpha=0.05))\nmed_model.add(Dense(6))\nmed_model.add(LeakyReLU(alpha=0.05))\nmed_model.add(Dense(2))\nmed_model.add(LeakyReLU(alpha=0.05))\nmed_model.add(Dense(1, activation='sigmoid'))\n\ncomp(med_model)\nfit(med_model)","926ad51e":"perf(med_model)","83d931c6":"med_leaky_model = Sequential()\nmed_leaky_model.add(Dense(9, input_shape=(9, )))\nmed_leaky_model.add(LeakyReLU(alpha=0.05))\nmed_leaky_model.add(Dense(6))\nmed_leaky_model.add(LeakyReLU(alpha=0.05))\nmed_leaky_model.add(Dense(3))\nmed_leaky_model.add(LeakyReLU(alpha=0.05))\nmed_leaky_model.add(Dense(3))\nmed_leaky_model.add(LeakyReLU(alpha=0.05))\nmed_leaky_model.add(Dense(1))\n\ncomp(med_leaky_model)\nfit(med_leaky_model)","5b97d259":"perf(med_leaky_model)","a63c3126":"from keras.layers import Dropout\ntail_model = Sequential()\ntail_model.add(Dense(9, activation='relu', input_shape=(9,)))\ntail_model.add(Dense(18, activation='relu'))\ntail_model.add(Dense(9, activation='relu'))\ntail_model.add(Dense(7, activation='relu'))\ntail_model.add(Dense(4, activation='relu'))\ntail_model.add(Dense(1, activation='sigmoid'))\n\ncomp(tail_model)\nfit(tail_model)","10d619e4":"perf(tail_model)","f28f73ec":"from keras.layers import Dropout, LeakyReLU\nleakly_large_model = Sequential()\nleakly_large_model.add(Dense(16, input_shape=(9,)))\nleakly_large_model.add(LeakyReLU(alpha=0.05))\nleakly_large_model.add(Dense(12))\nleakly_large_model.add(LeakyReLU(alpha=0.05))\nleakly_large_model.add(Dense(8))\nleakly_large_model.add(LeakyReLU(alpha=0.05))\nleakly_large_model.add(Dropout(.5))\nleakly_large_model.add(Dense(4))\nleakly_large_model.add(LeakyReLU(alpha=0.05))\nleakly_large_model.add(Dense(1, activation='sigmoid'))\n\ncomp(leakly_large_model)\nfit(leakly_large_model)","08d3c4d8":"perf(leakly_large_model)","7a3e31b5":"from keras.layers import Dropout\ndroppy_model = Sequential()\ndroppy_model.add(Dense(9, activation='relu', input_shape=(9,)))\ndroppy_model.add(Dense(4, activation='relu'))\ndroppy_model.add(Dropout(0.2))\ndroppy_model.add(Dense(1, activation='sigmoid'))\n\ncomp(droppy_model)\nfit(droppy_model)","46580af8":"perf(droppy_model)","01da266e":"This comes from the documentation for the original dataset from which this was derived. It is lightly edited for clarity.\n\n1. age: age in years\n2. sex: sex (1 = male; 0 = female)\n3. cp: chest pain type\n    * Value 1: typical angina\n    * Value 2: atypical angina\n    * Value 3: non-anginal pain\n    * Value 4: asymptomatic\n4. trestbps: resting blood pressure (in mm Hg on admission to the hospital)\n5. chol: serum cholestoral in mg\/dl\n6. fbs: (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false)\n7. restecg: resting electrocardiographic results\n    * Value 0: normal\n    * Value 1: having ST-T wave abnormality (T wave inversions and\/or ST elevation or depression of > 0.05 mV)\n    * Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria\n8. thalach: maximum heart rate achieved\n9. exang: exercise induced angina (1 = yes; 0 = no)\n10. oldpeak = ST depression induced by exercise relative to rest\n11. slope: the slope of the peak exercise ST segment\n    * Value 1: upsloping\n    * Value 2: flat\n    * Value 3: downsloping\n12. ca: number of major vessels (0-3) colored by flourosopy\n13. thal: 3 = normal; 6 = fixed defect; 7 = reversable defect\n\n## Notable Columns\n\nBecause I want the models to train on the most non-invasive information, I will remove difficult to retrieve information such as cholesterol level, and fasting blood sugar. \n\n"}}