{"cell_type":{"00b56b82":"code","0348e92f":"code","b30d032c":"code","27025029":"code","ba4715a6":"code","260225f8":"code","44ec06c9":"code","3549ac29":"code","88dc13fe":"code","c15a85ca":"code","dde4b457":"code","79b53215":"code","167a94e4":"code","5b2594e4":"code","e1ce67b0":"code","3fc25bfe":"markdown","419ad946":"markdown","f814a6b5":"markdown","2ec753d9":"markdown","354039ae":"markdown","4efdef55":"markdown","841d7254":"markdown","bdd4793e":"markdown","9174164c":"markdown","bdc4b0f0":"markdown","b8a96e05":"markdown","5686259c":"markdown","2ac99e51":"markdown","6b6f7516":"markdown"},"source":{"00b56b82":"import pandas as pd       \nimport matplotlib as mat\nimport matplotlib.pyplot as plt    \nimport numpy as np\nimport seaborn as sns\n%matplotlib inline\n\nimport pandas_profiling as pp\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import OrdinalEncoder\n\nfrom sklearn.metrics import mean_squared_error\n\nfrom xgboost import XGBRegressor","0348e92f":"df_train = pd.read_csv('..\/input\/30-days-of-ml\/train.csv', index_col = 'id')\nX_test = pd.read_csv('..\/input\/30-days-of-ml\/test.csv', index_col = 'id')\nsubmission = pd.read_csv('..\/input\/30-days-of-ml\/sample_submission.csv')","b30d032c":"df_train","27025029":"df_train.info()","ba4715a6":"df_train.describe()","260225f8":"X_test","44ec06c9":"X_test.info()","3549ac29":"X_test.describe()","88dc13fe":"pp.ProfileReport(df_train)","c15a85ca":"X_train = df_train.copy().drop('target', axis = 1)\nY_train = df_train['target'].copy()","dde4b457":"# List of categorical columns\nobject_cols = [col for col in X_train.columns if 'cat' in col]\n\n# ordinal-encode categorical columns\nordinal_encoder = OrdinalEncoder()\nX_train[object_cols] = ordinal_encoder.fit_transform(X_train[object_cols])\nX_test[object_cols] = ordinal_encoder.transform(X_test[object_cols])","79b53215":"def cv_function (model, param, list):\n    \n    kfold = KFold(n_splits = 10)\n    search_model = model\n    print ('Hyperparameter: ', param)\n    \n    for i in list:\n        param_dict = {param : i}\n        search_model.set_params(**param_dict)    \n        cv_score = cross_val_score(search_model, X_train, Y_train, cv=kfold, scoring='neg_root_mean_squared_error')\n        print(\"Parameter: {0:0.2f} - RMSE(SD): {1:0.4f} ({2:0.4f})\". format(i, cv_score.mean(), cv_score.std()))","167a94e4":"xgb_model = XGBRegressor(learning_rate = 0.03 ,random_state = 42, tree_method = 'gpu_hist', reg_alpha = 10)\n\nparams_xgb_list = [1000,1150,1300,1500,1750,2000]\nparam_xgb = 'n_estimators'\ncv_function(xgb_model, param_xgb, params_xgb_list)","5b2594e4":"xgb_final = XGBRegressor(n_estimators = 1500, learning_rate = 0.03 ,random_state = 42, tree_method = 'gpu_hist', reg_alpha = 10)\n\nxgb_final.fit(X_train, Y_train)\npredictions = xgb_final.predict(X_test)","e1ce67b0":"submission['target'] = predictions\nsubmission.to_csv(\"submission.csv\", index=False)\nsubmission","3fc25bfe":"## Auto EDA with Pandas Profiling ","419ad946":"There are 300000 samples on the training set and 200000 samples on the test set. Apparently, there are no missing values.","f814a6b5":"Choice: 1500 estimators","2ec753d9":"## Preparing the Data and Creating the Model","354039ae":"To speed up the training process, we can run the XGBoost on GPU. All you need is to enable the GPU (Settings menu on the right -> Accelerator -> Select GPU) and set tree_method = 'gpu_hist' on the model, as shown on the cell below.","4efdef55":"Now, we train the model with the selected number of estimators and use it to make the final predictions on the test set.","841d7254":"# <center>30 Days of Machine Learning<center>\n## <center>Starter - Auto EDA + Baseline XGBoost (on GPU)<center>\n--- \nStarter notebook for the 30 Days of ML competition. For a quick data analysis, a report is generated with Pandas Profiling (Auto EDA tool). A simple XGBoost model is created to provide a baseline score. First, we find a good number of estimators (n_estimators parameter) using a basic cross-validation scheme. Then, the model is trained with the best value and used for prediction.\n    \nGiven the size of the dataset. I\u2019ll be using GPU to speed up the training process.\n    \n\nMy other notebooks in this competition:\n\n* [30 Days of Machine Learning: LightGBM Model (+ Blend with XGBoost)](https:\/\/www.kaggle.com\/jonaspalucibarbosa\/30days-lightgbm-blend-with-xgboost\/)\n* [30 Days of Machine Learning: Random Forest on GPU (RAPIDS\/cuML))](https:\/\/www.kaggle.com\/jonaspalucibarbosa\/30days-random-forest-on-gpu-rapids)\n* [30 Days of Machine Learning: Final Stacking](https:\/\/www.kaggle.com\/jonaspalucibarbosa\/30days-final-stacking)","bdd4793e":"Since there are some categorical features, we need to encode them in order to make our predictions. For this starter notebook, I\u2019ll adapt ordinal encoding code available on Alexis Cook\u2019s Getting Started Notebook.","9174164c":"Performing an EDA is an important step to better understand the data we are working with. We can speed up this process using an Auto EDA tool. The Pandas Profiling report shows us the distribution of each feature, the interactions and correlations among them and the number of missing values. For a further feature engineering step, knowing those aspects is a crucial start.","bdc4b0f0":"## Making Predictions","b8a96e05":"## <center>If you find this notebook useful, support with an upvote!<center>","5686259c":"## Importing Packages and Datasets + First Look at the Data ","2ac99e51":"## <center>If you find this notebook useful, support with an upvote!<center>","6b6f7516":"To define the n_estimators parameter, we can perform a simple cross-validation, printing the score of each iteration."}}