{"cell_type":{"4bba763b":"code","6413c21e":"code","4fc240ce":"code","c471057c":"code","394302b2":"code","008cf8f7":"code","8bee0c78":"code","e4d38f81":"code","b0043319":"code","5fe5635c":"code","6ad3c34c":"code","d3ae62c6":"code","655120fb":"code","c65efb1b":"code","2fd1115a":"code","421cab23":"code","3a6a6790":"code","827a5c31":"code","3faf0718":"code","792598af":"code","323a464b":"code","f97c5123":"code","daac668b":"code","1a13cd0d":"code","0dc45b6c":"code","868e324d":"code","710bff2e":"code","ede44987":"code","70cbf448":"markdown","879b7e9b":"markdown","a6bdfcb6":"markdown"},"source":{"4bba763b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,GradientBoostingClassifier\n\nfrom sklearn.metrics import SCORERS\nfrom sklearn.metrics import plot_confusion_matrix,plot_roc_curve,classification_report,accuracy_score,confusion_matrix\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6413c21e":"df = pd.read_csv('\/kaggle\/input\/heart-disease-dataset-uci\/HeartDiseaseTrain-Test.csv')","4fc240ce":"Categorical_features = ['sex','chest_pain_type','fasting_blood_sugar',\n                        'rest_ecg','exercise_induced_angina','slope',\n                        'vessels_colored_by_flourosopy','thalassemia']\n\nNumerical_features = ['age','resting_blood_pressure','cholestoral','Max_heart_rate','oldpeak']","c471057c":"df.info()","394302b2":"df.isnull().sum()","008cf8f7":"# Lets check the whether data set is balanced or not !\nsns.countplot(data=df,x='target')\n# Conclusion Balanced","8bee0c78":"# Here i am checking whether age is normally distributed or not \n# so for this purpose i have done hypothesis testing\n\nsns.histplot(data=df,x='age',kde=True)","e4d38f81":"stat , Pvalue = stats.shapiro(df['age'])\nif Pvalue > 0.05:\n    print(\"Normally distributed\")\nelse:\n    print(\"Not a normal distribution\")","b0043319":"# QQ plot for checking normality\nstats.probplot(df['age'],dist=\"norm\", plot=plt)\nplt.show()","5fe5635c":"# Chi Square testing to see effect of gender on target\ncontigency_data = pd.crosstab(df['sex'],df['target'])\n\nstat,pvalue,dof,exp =stats.chi2_contingency(contigency_data)\n\nprint('stat=%.3f, p=%.3f' % (stat, pvalue))\nif pvalue > 0.05:\n    print('Same distribution no effect of sex on heart disease')\nelse:\n    print('There is a effect of sex on heart disease')\n    ","6ad3c34c":"sns.scatterplot(data=df,y='resting_blood_pressure',x='age',hue='target')","d3ae62c6":"for i in Categorical_features:\n    sns.countplot(data=df,x=i,hue='target')\n    plt.show()\n    \n# In sex female had more heart disease than male \n# Cheast pain : if no pain means more likely no heart disease but still some had \n# fasting blood sugar both same same heart disease \n# if rest ecg 1 then more likely to have heart disease\n# if any one had angina without excersice then they more likely had heart disease\n# downslope = highly likely to have heart disease\n# 0 colour means highly likely to have heart disease","655120fb":"for i in Numerical_features:\n    sns.catplot(data=df,x='target',y=i,kind='box')","c65efb1b":"df = pd.get_dummies(df,drop_first=True)","2fd1115a":"X = df.drop('target',axis=1)\ny = df['target']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=101)\nX_test,X_validate,y_test,y_validate = train_test_split(X_test, y_test, test_size=0.50, random_state=42)\n\n","421cab23":"scaler = StandardScaler()\n\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\nX_validate_scaled = scaler.transform(X_validate)","3a6a6790":"def trainModel(model,X_train,X_test,y_train,y_test):\n    \n    model.fit(X_train,y_train)\n    y_pred = model.predict(X_test)\n    \n    print(classification_report(y_test,y_pred))\n    print(plot_confusion_matrix(model,X_test,y_test))\n    print(plot_roc_curve(model,X_test,y_test))\n    \ndef trainModelGrid(model,X_train,X_test,y_train,y_test):\n    \n    model.fit(X_train,y_train)\n    y_pred = model.predict(X_test)\n    print(model.best_params_)\n    print(classification_report(y_test,y_pred))\n    print(plot_confusion_matrix(model,X_test,y_test))","827a5c31":"model = LogisticRegression()\ntrainModel(model,X_train_scaled,X_test_scaled,y_train,y_test)","3faf0718":"# Because In this I can accept more +ve than false -negative\nmodel = LogisticRegression()\nmodel.fit(X_train_scaled,y_train)\nprob = model.predict_proba(X_test_scaled)[:,1]\ny_pred = np.where(prob >=0.4,1,0) # Choosing custom thresold\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))","792598af":"model = SVC()\ntrainModel(model,X_train_scaled,X_test_scaled,y_train,y_test)","323a464b":"# After Running this I got C=50 \n# See next cell\n\"\"\"model = SVC()\n\nparam = {'C':[1.0,2,5,10,50,100],\n    'kernel': ['rbf','linear'],\n    'degree':[3,4],\n    'gamma':['scale','auto']}\n\ngridModel = GridSearchCV(model,param_grid=param,scoring='accuracy',cv=5,verbose=2)\ntrainModelGrid(gridModel,X_train_scaled,X_test_scaled,y_train,y_test)\"\"\"","f97c5123":"model = SVC(C=50,probability=True)\nmodel.fit(X_train_scaled,y_train)\nprob = model.predict_proba(X_test_scaled)[:,1]\ny_pred = np.where(prob >=0.4,1,0) # Choosing custom thresold\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))","daac668b":"model = KNeighborsClassifier(1)\ntrainModel(model,X_train_scaled,X_test_scaled,y_train,y_test)","1a13cd0d":"model = RandomForestClassifier()\ntrainModel(model,X_train_scaled,X_test_scaled,y_train,y_test)","0dc45b6c":"model = AdaBoostClassifier()\ntrainModel(model,X_train_scaled,X_test_scaled,y_train,y_test)","868e324d":"model = GradientBoostingClassifier()\ntrainModel(model,X_train_scaled,X_test_scaled,y_train,y_test)","710bff2e":"model = GradientBoostingClassifier(n_estimators=130,learning_rate=1)\nmodel.fit(X_train_scaled,y_train)\nprob = model.predict_proba(X_test_scaled)[:,1]\ny_pred = np.where(prob >=0.3,1,0) # Choosing custom thresold\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))","ede44987":"# Here I will Not tune model This is final accuracy of the model\n\nmodel = RandomForestClassifier(n_estimators=120)\nmodel.fit(X_train_scaled,y_train)\nprob = model.predict_proba(X_validate_scaled)[:,1]\n\ny_pred = np.where(prob >=0.4,1,0) # Choosing custom thresold\n\nprint(classification_report(y_validate,y_pred))\nprint(confusion_matrix(y_validate,y_pred))","70cbf448":"# Model Training and testing with Train-Test-Validation split","879b7e9b":"## EDA","a6bdfcb6":"## Feature Seperation"}}