{"cell_type":{"097779f3":"code","2c3b0012":"code","8050b07e":"code","a1782b20":"code","6b4f56f7":"code","6cbde469":"code","f9ac8c6c":"code","5edb5daf":"code","2d0b0d6e":"code","324ffd30":"code","491ae92a":"code","032bd2ee":"code","4fd4fc26":"code","a019873c":"code","f28ab3c7":"code","c97050b0":"code","f99ea862":"code","c6e7b60a":"code","ca801734":"code","3d54e6e7":"code","b7d78427":"code","5c542da4":"code","28deff50":"code","658fd7ec":"code","0da91738":"code","54da5039":"code","a5f01ee0":"code","494c379c":"markdown","f801e81d":"markdown","984267fd":"markdown"},"source":{"097779f3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","2c3b0012":"!pip install pyspark","8050b07e":"from pyspark.sql import SparkSession\n\n# Create my_spark\nmy_spark = SparkSession.builder.getOrCreate()\n\n# Print my_spark\nprint(my_spark)","a1782b20":"print(my_spark.catalog.listTables())","6b4f56f7":"# Create pd_temp\n#pd_temp = pd.DataFrame(np.random.random(10))\n\n# Read a dataframe\nfile_path = \"..\/input\/flights.csv\"\n\n# Create spark_temp from pd_temp\nflights = my_spark.read.csv(file_path, header=True)\n\n# Show the data\nflights.show()\n\n# Examine the tables in the catalog\nprint(my_spark.catalog.listTables())\n\n# Add flights to the catalog\nflights.createOrReplaceTempView(\"flights\")\n\n# Examine the tables in the catalog again\nprint(my_spark.catalog.listTables())","6cbde469":"# Add duration_hrs\nflights = flights.withColumn(\"duration_hrs\", flights.air_time\/60)","f9ac8c6c":"flights.toPandas().shape[0]","5edb5daf":"flights.limit(flights.toPandas().shape[0]).toPandas()[\"duration_hrs\"].hist()","2d0b0d6e":"!pip install pyspark_dist_explore\n# https:\/\/github.com\/Bergvca\/pyspark_dist_explore\/","324ffd30":"import numpy as np\nimport pyspark.sql.functions as F\nimport matplotlib.pyplot as plt\nimport seaborn as sns","491ae92a":"# Read a dataframe\nfile_path = \"..\/input\/planes.csv\"\n\n# Create spark_temp from pd_temp\nplanes = my_spark.read.csv(file_path, header=True)\n\n# Rename year column\nplanes = planes.withColumnRenamed(\"year\", \"plane_year\")\n\n# Join the DataFrames\nmodel_data = flights.join(planes, on=\"tailnum\", how=\"leftouter\")","032bd2ee":"# Cast the columns to integers\nmodel_data = model_data.withColumn(\"arr_delay\", model_data.arr_delay.cast(\"integer\"))\nmodel_data = model_data.withColumn(\"air_time\", model_data.air_time.cast(\"integer\"))\nmodel_data = model_data.withColumn(\"month\", model_data.month.cast(\"integer\"))\nmodel_data = model_data.withColumn(\"plane_year\", model_data.plane_year.cast(\"integer\"))","4fd4fc26":"# Create the column plane_age\nmodel_data = model_data.withColumn(\"plane_age\", model_data.year - model_data.plane_year)","a019873c":"# Create is_late\nmodel_data = model_data.withColumn(\"is_late\", model_data.arr_delay > 0)\n\n# Convert to an integer\nmodel_data = model_data.withColumn(\"label\", model_data.is_late.cast(\"integer\"))\n\n# Remove missing values\nmodel_data = model_data.filter(\"arr_delay is not NULL and dep_delay is not NULL and air_time is not NULL and plane_year is not NULL\")","f28ab3c7":"from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n\n# Create a StringIndexer\ncarr_indexer = StringIndexer(inputCol=\"carrier\", outputCol=\"carrier_index\")\n\n# Create a OneHotEncoder\ncarr_encoder = OneHotEncoder(inputCol=\"carrier_index\", outputCol=\"carrier_fact\")","c97050b0":"# Create a StringIndexer\ndest_indexer = StringIndexer(inputCol=\"dest\", outputCol=\"dest_index\")\n\n# Create a OneHotEncoder\ndest_encoder = OneHotEncoder(inputCol=\"dest_index\", outputCol=\"dest_fact\")","f99ea862":"# Make a VectorAssembler\nvec_assembler = VectorAssembler(inputCols=[\"month\", \"air_time\", \"carrier_fact\", \"dest_fact\", \"plane_age\"], outputCol=\"features\")","c6e7b60a":"# Import Pipeline\nfrom pyspark.ml  import  Pipeline\n\n# Make the pipeline\nflights_pipe = Pipeline(stages=[dest_indexer, dest_encoder, carr_indexer, carr_encoder, vec_assembler])","ca801734":"# Fit and transform the data\npiped_data = flights_pipe.fit(model_data).transform(model_data)","3d54e6e7":"piped_data.toPandas().head(3)","b7d78427":"# Split the data into training and test sets\ntraining, test = piped_data.randomSplit([.7, .3])","5c542da4":"# Import LogisticRegression\nfrom pyspark.ml.classification import LogisticRegression\n\n# Create a LogisticRegression Estimator\nlr = LogisticRegression()","28deff50":"# Import the evaluation submodule\nimport pyspark.ml.evaluation as evals\n\n# Create a BinaryClassificationEvaluator\nevaluator = evals.BinaryClassificationEvaluator(metricName=\"areaUnderROC\")","658fd7ec":"# Import the tuning submodule\nimport pyspark.ml.tuning as tune\n\n# Create the parameter grid\ngrid = tune.ParamGridBuilder()\n\n# Add the hyperparameter\ngrid = grid.addGrid(lr.regParam, np.arange(0, .1, .01))\ngrid = grid.addGrid(lr.elasticNetParam, [0, 1])\n\n# Build the grid\ngrid = grid.build()","0da91738":"# Create the CrossValidator\ncv = tune.CrossValidator(estimator=lr,\n               estimatorParamMaps=grid,\n               evaluator=evaluator\n               )","54da5039":"# Call lr.fit()\nbest_lr = lr.fit(training)\n\n# Print best_lr\nprint(best_lr)","a5f01ee0":"# Use the model to predict the test set\ntest_results = best_lr.transform(test)\n\n# Evaluate the predictions\nprint(evaluator.evaluate(test_results))","494c379c":"# Personal PySpark Excersices\nI am learnning Spark and PySpark by myself, feel free to suggest ideas for improvements","f801e81d":"https:\/\/github.com\/Bergvca\/pyspark_dist_explore\/","984267fd":"#### duration_hrs histogram"}}