{"cell_type":{"9911c34f":"code","c82137c0":"code","7cbcedfc":"code","67f2011e":"code","e61c0d92":"code","3c393807":"code","41a8da40":"code","c3c2d3ab":"code","c62f2496":"code","8447c38d":"code","3a51addf":"code","fddaab93":"code","028b64ec":"code","9fb60961":"code","0da43030":"code","c88ae3f8":"code","08da88d3":"code","320c12dd":"code","8b2ed154":"code","8d49e5d8":"code","0edab038":"code","a0d7577e":"code","c2040a80":"code","e6dcb8d5":"code","586235d0":"code","1b375fef":"code","d415a043":"code","5813fe97":"code","8154968f":"code","73f7a16a":"code","18c0192b":"code","30a72501":"code","6b8a9120":"code","1bf7c004":"code","eec12bdd":"code","774bea51":"code","d30d26d3":"code","5b534c9a":"code","d403ce71":"code","09fc7268":"code","258eed22":"code","2819d260":"code","eb22fa2f":"code","324fc9c8":"code","826f8497":"code","703c1bb7":"code","a5a3e0a2":"code","cac8bffb":"code","083999c3":"code","87e2721a":"code","442235e4":"code","b0d84176":"code","21e3a063":"code","f8934259":"code","90b6e757":"code","e8a2b5fb":"code","a2cab90e":"code","4ca20eae":"code","01f82791":"code","c29f3b45":"code","8676f9d8":"code","6447acee":"code","a20bd007":"code","6465ce90":"code","4b859aba":"code","eba44db3":"code","5518a90c":"code","6fa4a54f":"code","1fb6bbae":"code","8e621261":"code","6675a625":"code","0ad5771f":"code","6f315c7a":"code","9efa4cf0":"code","7d4ba5a8":"code","5f45a5e2":"code","fb8e25e8":"markdown","2104bdcf":"markdown","15486730":"markdown","d1d568cf":"markdown","daef3d2d":"markdown","bc7d534a":"markdown","6ec7c6db":"markdown","49cf0a20":"markdown","30ca5627":"markdown","a67b93e8":"markdown","9aa85146":"markdown","5a5b08e5":"markdown","fb7f672f":"markdown","0e7c7707":"markdown","92277663":"markdown","0eb39510":"markdown"},"source":{"9911c34f":"import pandas as pd\n# Load the transfer learning tweet dataset\ndf = pd.read_csv('..\/input\/twitterdata\/finalSentimentdata2.csv')\ndf.head()","c82137c0":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","7cbcedfc":"#checking if there any NAN value or not\ndf.isnull().sum()","67f2011e":"# droping the unnecesary colmns \ndf.drop(['Unnamed: 0'], axis='columns', inplace=True)","e61c0d92":"df.columns","3c393807":"df['sentiment'].value_counts()","41a8da40":"\nsns.set_style(\"whitegrid\")\nplt.figure(figsize=(10,5))\nsns.countplot(df['sentiment'])","c3c2d3ab":"\nfrom sklearn.preprocessing import LabelEncoder\nscaler=LabelEncoder()\ndf['sentiment']=scaler.fit_transform(df['sentiment'])","c62f2496":"#1 for fear\n#3 for sad\n#0 for anger\n#2 for joy\ndf['sentiment'].value_counts()","8447c38d":"df.columns","3a51addf":"df['text'][:5]","fddaab93":"df.shape","028b64ec":"#making the copy of original datasets\nmessage=df.copy()","9fb60961":"message.head(3)","0da43030":"message.isnull().sum()","c88ae3f8":"message['text'][0]","08da88d3":"para=''''agree the poor in india are treated badly their poors \nseek a living in singapore and are treated like citizens they \nare given free medical treatment given food daily sim cards\nto call home to tell their family that they are fine if covid \n19 case treated foc in hospitals'''","320c12dd":"import nltk\nimport re\n\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nps = PorterStemmer()\ncorpus = []\n\nreview=re.sub(r'http\\S+',' ',para) #removing all the link releted text\nreview = re.sub('[^a-zA-Z]', ' ', review)# removing all the element except a-z and A-Z\nreview = review.lower()#lowering the text\nreview = review.split()\n\n#removing all the stopwords and then stemming the text \nreview = [ps.stem(word) for word in review if not word in stopwords.words('english')]\nreview = ' '.join(review)\ncorpus.append(review)","8b2ed154":"corpus","8d49e5d8":"from nltk.stem import WordNetLemmatizer\nlem=WordNetLemmatizer()\n\ncorpus_lem=[]\nreview=re.sub(r'http\\S+',' ',para)#removing all the link releted text\nreview = re.sub('[^a-zA-Z]', ' ', review)# removing all the element except a-z and A-Z\nreview = review.lower()#lowering the text\nreview = review.split()\n\n#removing all the stopwords and then lemmatizing the text \nreview=[lem.lemmatize(word) for word in review if word not in set(stopwords.words('english'))]\nreview=' '.join(review)\ncorpus_lem.append(review)\n    \ncorpus_lem","0edab038":"#stemming and cleaning\ncorpus_stem = []\nfor i in range(0, len(message)):\n    review=re.sub(r'http\\S+',' ',message['text'][i])\n    review = re.sub('[^a-zA-Z]', ' ', review)\n    review = review.lower()\n    review = review.split()\n    \n    review = [ps.stem(word) for word in review if not word in stopwords.words('english')]\n    review = ' '.join(review)\n    corpus_stem.append(review)","a0d7577e":"len(message)","c2040a80":"df2=pd.DataFrame(df['sentiment'],index=None)","e6dcb8d5":"df2['stemming_text']=corpus_stem","586235d0":"df2.head(4)","1b375fef":"\nfrom wordcloud import WordCloud\nanger_text = df2[df2['sentiment'] == 0]\nall_words = ' '.join([text for text in anger_text.stemming_text])\nwordcloud = WordCloud(width= 1000, height= 800,\n                          max_font_size = 120,\n                          collocations = False).generate(all_words)\nplt.figure(figsize=(10,7))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.title(\"Most Used anger words\", fontsize=20)\nplt.axis(\"off\")\nplt.show()","d415a043":"\nfrom wordcloud import WordCloud\nfear_text = df2[df2['sentiment'] == 1]\nall_words = ' '.join([text for text in fear_text.stemming_text])\nwordcloud = WordCloud(width= 1000, height= 800,\n                          max_font_size = 120,\n                          collocations = False).generate(all_words)\nplt.figure(figsize=(10,7))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.title(\"Most Used fear words\", fontsize=20)\nplt.axis(\"off\")\nplt.show()","5813fe97":"from wordcloud import WordCloud\njoy_text = df2[df2['sentiment'] == 2]\nall_words = ' '.join([text for text in joy_text.stemming_text])\nwordcloud = WordCloud(width= 1000, height= 800,\n                          max_font_size = 120,\n                          collocations = False).generate(all_words)\nplt.figure(figsize=(10,7))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.title(\"Most Used joy words\", fontsize=20)\nplt.axis(\"off\")\nplt.show()","8154968f":"from wordcloud import WordCloud\nsad_text = df2[df2['sentiment'] == 3]\nall_words = ' '.join([text for text in sad_text.stemming_text])\nwordcloud = WordCloud(width= 1000, height= 800,\n                          max_font_size = 120,\n                          collocations = False).generate(all_words)\nplt.figure(figsize=(10,7))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.title(\"Most Used sad words\", fontsize=20)\nplt.axis(\"off\")\nplt.show()","73f7a16a":"from nltk import tokenize\ntoken_space = tokenize.WhitespaceTokenizer()\ndef counter(text, column_text, quantity):\n    all_words = ' '.join([text for text in text[column_text]])\n    token_phrase = token_space.tokenize(all_words)\n    frequency = nltk.FreqDist(token_phrase)\n    df_frequency = pd.DataFrame({\"Word\": list(frequency.keys()),\n                                   \"Frequency\": list(frequency.values())})\n    df_frequency = df_frequency.nlargest(columns = \"Frequency\", n = quantity)\n    plt.figure(figsize=(15,8))\n    ax = sns.barplot(data = df_frequency, x = \"Word\", y = \"Frequency\", color = 'yellow')\n    ax.set(ylabel = \"Count\")\n    plt.xticks(rotation='vertical')\n    plt.show()\n    \n    \n","18c0192b":"#frequency of most anger words\ncounter(df2[df2['sentiment'] == 0], 'stemming_text', 20)","30a72501":"#frequency of most fear words\ncounter(df2[df2['sentiment'] == 1], 'stemming_text', 20)","6b8a9120":"#frequency of most joy words\ncounter(df2[df2['sentiment'] == 2], 'stemming_text', 20)","1bf7c004":"#frequency of most sad words\ncounter(df2[df2['sentiment'] == 3], 'stemming_text', 20)","eec12bdd":"#lammetizing and cleaning\n\ncorpus_lemmetize = []\nfor i in range(0, len(message)):\n    review=re.sub(r'http\\S+',' ',message['text'][i])\n    review = re.sub('[^a-zA-Z]', ' ', review)\n    review = review.lower()\n    review = review.split()\n    \n    review = [lem.lemmatize(word) for word in review if word not in set(stopwords.words('english'))]\n    review = ' '.join(review)\n    corpus_lemmetize.append(review)","774bea51":"corpus_lemmetize[:2]","d30d26d3":"len(corpus_lemmetize)","5b534c9a":"#countvectoriser with stemming\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nCV=CountVectorizer(max_features=5000, ngram_range=(1,3))\nx_stem=CV.fit_transform(corpus_stem).toarray()\nx_stem","d403ce71":"print(x_stem.shape)","09fc7268":"#countvectorizer with lemmetizing\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nCV=CountVectorizer(max_features=5000)\nx_lem=CV.fit_transform(corpus_lemmetize).toarray()\nx_lem","258eed22":"print(x_lem.shape)","2819d260":"#TF-IDF for stemming\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ntf_stem=TfidfVectorizer()\nx_tf_stem=tf_stem.fit_transform(corpus_stem)\nprint(x_tf_stem.shape)\n","eb22fa2f":"#TF-IDF for lemmatizing\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ntf_stem=TfidfVectorizer()\nx_tf_lem=tf_stem.fit_transform(corpus_lemmetize)\nprint(x_tf_lem.shape)","324fc9c8":"y=df['sentiment']\ny[:5]","826f8497":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test= train_test_split(x_stem,y,test_size=0.2)","703c1bb7":"X_train.shape, X_test.shape, y_train.shape, y_test.shape","a5a3e0a2":"from sklearn.naive_bayes import MultinomialNB\nmodel=MultinomialNB().fit(X_train,y_train)","cac8bffb":"model.score(X_test,y_test)","083999c3":"from sklearn.metrics import confusion_matrix\ny_pred=model.predict(X_test)\ncm=confusion_matrix(y_test,y_pred)\ncm","87e2721a":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))","442235e4":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nplt.figure(figsize=(10,5))\nsns.heatmap(cm, annot=True, fmt='d')\nplt.xlabel('Predicted')\nplt.ylabel('True')","b0d84176":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test= train_test_split(x_lem,y,test_size=0.2)","21e3a063":"X_train.shape, X_test.shape, y_train.shape, y_test.shape","f8934259":"from sklearn.naive_bayes import MultinomialNB\nmodel=MultinomialNB().fit(X_train,y_train)","90b6e757":"model.score(X_test,y_test)","e8a2b5fb":"from sklearn.metrics import confusion_matrix\ny_pred=model.predict(X_test)\ncm=confusion_matrix(y_test,y_pred)\ncm","a2cab90e":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))","4ca20eae":"plt.figure(figsize=(10,5))\nsns.heatmap(cm, annot=True, fmt='d')\nplt.xlabel('Predicted')\nplt.ylabel('True')","01f82791":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test= train_test_split(x_tf_stem,y,test_size=0.2)","c29f3b45":"X_train.shape, X_test.shape, y_train.shape, y_test.shape","8676f9d8":"from sklearn.naive_bayes import MultinomialNB\nmodel=MultinomialNB().fit(X_train,y_train)","6447acee":"model.score(X_test,y_test)","a20bd007":"from sklearn.metrics import confusion_matrix\ny_pred=model.predict(X_test)\ncm=confusion_matrix(y_test,y_pred)\ncm","6465ce90":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))","4b859aba":"plt.figure(figsize=(10,5))\nsns.heatmap(cm, annot=True, fmt='d')\nplt.xlabel('Predicted')\nplt.ylabel('True')","eba44db3":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test= train_test_split(x_tf_lem,y,test_size=0.2)","5518a90c":"x_tf_lem.shape","6fa4a54f":"from sklearn.naive_bayes import MultinomialNB\nmodel=MultinomialNB().fit(X_train,y_train)","1fb6bbae":"model.score(X_test,y_test)","8e621261":"from sklearn.metrics import confusion_matrix\ny_pred=model.predict(X_test)\ncm=confusion_matrix(y_test,y_pred)\ncm","6675a625":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))","0ad5771f":"plt.figure(figsize=(10,5))\nsns.heatmap(cm, annot=True, fmt='d')\nplt.xlabel('Predicted')\nplt.ylabel('True')\n","6f315c7a":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test= train_test_split(x_stem,y,test_size=0.2)\n\nclassifier=MultinomialNB(alpha=0.1)\n\nfrom sklearn import metrics\nimport numpy as np\nprevious_score=0\nfor alpha in np.arange(0,1,0.1):\n    sub_classifier=MultinomialNB(alpha=alpha)\n    sub_classifier.fit(X_train,y_train)\n    y_pred=sub_classifier.predict(X_test)\n    score = metrics.accuracy_score(y_test, y_pred)\n    if score>previous_score:\n        classifier=sub_classifier\n    print(\"Alpha: {}, Score : {}\".format(alpha,score))","9efa4cf0":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import  DecisionTreeClassifier\nfrom sklearn.svm import SVC\n\nfrom sklearn.model_selection import cross_val_score, GridSearchCV\nfrom sklearn.metrics import classification_report,confusion_matrix","7d4ba5a8":"model_params = {\n    'svm': {\n        'model': SVC(gamma='auto'),\n        'params' : {\n            'C': [1,10,20,25,30,40],\n            'kernel': ['rbf','linear']\n        }  \n    },\n    'random_forest': {\n        'model': RandomForestClassifier(),\n        'params' : {\n            'n_estimators': [1,5,10,15,20,25,30]\n        }\n    },\n    'logistic_regression' : {\n        'model': LogisticRegression(solver='liblinear'),\n        'params': {\n            'C': [1,5,10,15,20,25]\n        }\n    },\n    'decision_tree': {\n        'model': DecisionTreeClassifier(),\n        'params':{\n            'criterion':['gini','entropy']\n        }\n    }\n}","5f45a5e2":"scores = []\n\nX_train, X_test, y_train, y_test= train_test_split(x_stem,y,test_size=0.2)\n\nfor model_name, mp in model_params.items():\n    clf =  GridSearchCV(mp['model'], mp['params'], cv=5, return_train_score=False)\n    clf.fit(X_train,y_train)\n    scores.append({\n        'model': model_name,\n        'best_score': clf.best_score_,\n        'best_params': clf.best_params_\n    })\n    \ndf_score = pd.DataFrame(scores,columns=['model','best_score','best_params'])\ndf_score","fb8e25e8":"# Hence, It can conclude that Logistic Regression with the accuracy of 0.69","2104bdcf":"# Now let's check with other classifier algorithm like DecisionTreeClassifier, RandamForestClassifier, SVM, LogisticRegression by using GridSearchCV and crossvalidation","15486730":"# Most commonly used Anger words ","d1d568cf":"# Most commonly used Fear words","daef3d2d":"# Now lets apply this in whole datasets for cleaning the text.","bc7d534a":"# when alpha=0.6 its gives the maximumn accuracy of 0.6747572815533981","6ec7c6db":"# Most commonly used Joy words","49cf0a20":"#  Making the model by using Countvectorizer and Stemming","30ca5627":"# Making the model by using TF-IDF and lemmetizing","a67b93e8":"# Let's take a single paragraph for example message['text'][0] and check how the cleaning is going on after that it will be done in whole datasets.","9aa85146":"# Let's check the accuracy after applying hyperparameter in MultinomialNB","5a5b08e5":"# Its seems that 80% cleaning is done by stemming now let's check it my lemmatizing","fb7f672f":"# Most commonly used Sad words","0e7c7707":"# Making the model by using Lemmetizing and Countvectorizer","92277663":"# Making the model by using TF-IDF and stemming","0eb39510":"# stemming and cleaning the text"}}