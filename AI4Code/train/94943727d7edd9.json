{"cell_type":{"98c6b683":"code","acde5a78":"code","182e9ebe":"code","cf82142d":"code","66910af5":"code","aa17ef4f":"code","439dd231":"code","e4fb76c2":"code","bef4c1a8":"code","04684169":"code","48d51a96":"code","785b3de5":"code","f7c336f0":"code","5751d1b8":"code","1373339b":"code","0d2d9e6e":"code","57172a2a":"code","5a007e6d":"code","36d2d7e9":"code","321b9c7a":"code","ff3c8912":"code","7de8031b":"code","a41c2837":"code","dc88ff3e":"code","5a37dd6b":"code","5b5aaca0":"code","845c83c8":"code","a4c00168":"code","6cd965dc":"code","9d2c9365":"code","23238a99":"code","dd12a037":"code","0084aeb6":"code","8965d703":"code","638049ab":"code","1cd1d116":"code","fd235e94":"code","a063cf25":"code","94541522":"code","3a8d9862":"markdown","41ce1bdf":"markdown","a4cb8582":"markdown","d2b30a71":"markdown","f6816f2f":"markdown","923e5b4c":"markdown","87ce4722":"markdown","3caee49b":"markdown","9d0d064b":"markdown","826c7626":"markdown","930e3e1d":"markdown","329f5530":"markdown","aeeab0c7":"markdown","483153cb":"markdown","71d3ac6b":"markdown","adbbb725":"markdown","39d88bde":"markdown","5c2bc2c9":"markdown","99784e60":"markdown","89103290":"markdown"},"source":{"98c6b683":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport seaborn as sns\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense , Dropout , Lambda, Flatten\nfrom keras.optimizers import Adam ,RMSprop\nfrom sklearn.model_selection import train_test_split\nfrom keras import  backend as K\nfrom keras.preprocessing.image import ImageDataGenerator\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))","acde5a78":"# load  train and test data\ntrain_data = pd.read_csv(\"..\/input\/train.csv\")\ntest_data= pd.read_csv(\"..\/input\/test.csv\")","182e9ebe":"print(train_data.shape)\nprint(test_data.shape)","cf82142d":"train_data.head()","66910af5":"test_data.head()","aa17ef4f":"train_data.info()","439dd231":"test_data.info()","e4fb76c2":"X_train = (train_data.iloc[:,1:].values).astype('float32') # all pixel values\ny_train = train_data.iloc[:,0].values.astype('int32') # only labels i.e targets digits\n# free some space\ndel train_data\nX_test = test_data.values.astype('float32')","bef4c1a8":"X_train","04684169":"y_train","48d51a96":"sns.countplot(y_train)","785b3de5":"plt.hist(y_train)","f7c336f0":"#Convert train datset to (num_images, img_rows, img_cols) format \nX_train = X_train.reshape(X_train.shape[0], 28, 28)\n\nfor i in range(1,9):\n    plt.subplot(330 + (i+1))\n    plt.imshow(X_train[i], cmap=plt.get_cmap('gray'))\n    plt.title(y_train[i]);","5751d1b8":"#expand 1 more dimention as 1 for colour channel \nX_train = X_train.reshape(X_train.shape[0], 28, 28,1)\nX_train.shape","1373339b":"X_test = X_test.reshape(X_test.shape[0], 28, 28,1)\nX_test.shape","0d2d9e6e":"mean_px = X_train.mean().astype(np.float32)\nstd_px = X_train.std().astype(np.float32)\n\ndef standardize(x): \n    return (x-mean_px)\/std_px","57172a2a":"from keras.utils.np_utils import to_categorical\ny_train= to_categorical(y_train)\nnum_classes = y_train.shape[1]\nnum_classes","5a007e6d":"plt.title(y_train[4])\nplt.plot(y_train[4])\nplt.xticks(range(10));","36d2d7e9":"# fix random seed for reproducibility\nseed = 43\nnp.random.seed(seed)","321b9c7a":"from keras.models import  Sequential\nfrom keras.layers.core import  Lambda , Dense, Flatten, Dropout\nfrom keras.callbacks import EarlyStopping\nfrom keras.layers import BatchNormalization, Convolution2D , MaxPooling2D","ff3c8912":"model= Sequential()\nmodel.add(Lambda(standardize,input_shape=(28,28,1)))\nmodel.add(Flatten())\nmodel.add(Dense(10, activation='softmax'))\nprint(\"input shape \",model.input_shape)\nprint(\"output shape \",model.output_shape)\n\n","7de8031b":"from keras.optimizers import RMSprop\nmodel.compile(optimizer=RMSprop(lr=0.001),\n loss='categorical_crossentropy',\n metrics=['accuracy'])","a41c2837":"from keras.preprocessing import image\ngen = image.ImageDataGenerator()","dc88ff3e":"from sklearn.model_selection import train_test_split\nX = X_train\ny = y_train\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.10, random_state=42)\nbatches = gen.flow(X_train, y_train, batch_size=64)\nval_batches=gen.flow(X_val, y_val, batch_size=64)","5a37dd6b":"history=model.fit_generator(generator=batches, steps_per_epoch=batches.n, epochs=3, \n                    validation_data=val_batches, validation_steps=val_batches.n)","5b5aaca0":"history_dict = history.history\nhistory_dict.keys()","845c83c8":"loss_values = history_dict['loss']\nval_loss_values = history_dict['val_loss']\nepochs = range(1, len(loss_values) + 1)\n\n# \"bo\" is for \"blue dot\"\nplt.plot(epochs, loss_values, 'bo')\n# b+ is for \"blue crosses\"\nplt.plot(epochs, val_loss_values, 'b+')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\n\nplt.show()\n\n","a4c00168":"acc_values = history_dict['acc']\nval_acc_values = history_dict['val_acc']\n\nplt.plot(epochs, acc_values, 'bo')\nplt.plot(epochs, val_acc_values, 'b+')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\n\nplt.show()","6cd965dc":"def get_fc_model():\n    model = Sequential([\n        Lambda(standardize, input_shape=(28,28,1)),\n        Flatten(),\n        Dense(512, activation='relu'),\n        Dense(10, activation='softmax')\n        ])\n    model.compile(optimizer='Adam', loss='categorical_crossentropy',\n                  metrics=['accuracy'])\n    return model","9d2c9365":"fc = get_fc_model()\nfc.optimizer.lr=0.01","23238a99":"history=fc.fit_generator(generator=batches, steps_per_epoch=batches.n, epochs=1, \n                    validation_data=val_batches, validation_steps=val_batches.n)\n","dd12a037":"from keras.layers import Convolution2D, MaxPooling2D\n\ndef get_cnn_model():\n    model = Sequential([\n        Lambda(standardize, input_shape=(28,28,1)),\n        Convolution2D(32,(3,3), activation='relu'),\n        Convolution2D(32,(3,3), activation='relu'),\n        MaxPooling2D(),\n        Convolution2D(64,(3,3), activation='relu'),\n        Convolution2D(64,(3,3), activation='relu'),\n        MaxPooling2D(),\n        Flatten(),\n        Dense(512, activation='relu'),\n        Dense(10, activation='softmax')\n        ])\n    model.compile(Adam(), loss='categorical_crossentropy',\n                  metrics=['accuracy'])\n    return model","0084aeb6":"model= get_cnn_model()\nmodel.optimizer.lr=0.01","8965d703":"history=model.fit_generator(generator=batches, steps_per_epoch=batches.n, epochs=1, \n                    validation_data=val_batches, validation_steps=val_batches.n)\n\n","638049ab":"gen =ImageDataGenerator(rotation_range=8, width_shift_range=0.08, shear_range=0.3,\n                               height_shift_range=0.08, zoom_range=0.08)\nbatches = gen.flow(X_train, y_train, batch_size=64)\nval_batches = gen.flow(X_val, y_val, batch_size=64)","1cd1d116":"model.optimizer.lr=0.001\nhistory=model.fit_generator(generator=batches, steps_per_epoch=batches.n, epochs=1, \n                    validation_data=val_batches, validation_steps=val_batches.n)","fd235e94":"from keras.layers.normalization import BatchNormalization\n\ndef get_bn_model():\n    model = Sequential([\n        Lambda(standardize, input_shape=(28,28,1)),\n        Convolution2D(32,(3,3), activation='relu'),\n        BatchNormalization(axis=1),\n        Convolution2D(32,(3,3), activation='relu'),\n        MaxPooling2D(),\n        BatchNormalization(axis=1),\n        Convolution2D(64,(3,3), activation='relu'),\n        BatchNormalization(axis=1),\n        Convolution2D(64,(3,3), activation='relu'),\n        MaxPooling2D(),\n        Flatten(),\n        BatchNormalization(),\n        Dense(512, activation='relu'),\n        BatchNormalization(),\n        Dense(10, activation='softmax')\n        ])\n    model.compile(Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n    return model","a063cf25":"model= get_bn_model()\nmodel.optimizer.lr=0.01\nhistory=model.fit_generator(generator=batches, steps_per_epoch=batches.n, epochs=1, \n                    validation_data=val_batches, validation_steps=val_batches.n)","94541522":"predictions = model.predict_classes(X_test, verbose=0)\n\nsubmissions=pd.DataFrame({\"ImageId\": list(range(1,len(predictions)+1)),\n                         \"Label\": predictions})\nsubmissions.to_csv(\"result.csv\", index=False, header=True)","3a8d9862":"# Digit Recognizer using CNN\nSani Kamal\n\n13-08-2018\n\n### Introduction\nThis is not the best way to classify digits! This notebook is rather meant to be for someone who might not know where to start. As an ml beginner, I am trying to understand various components of Deep Learning. I am designing neural net on MNIST handwritten digits images to identify their correct label i.e number in image. Any suggestions for improvement or comments on poor coding practices are appreciated!\n\nLets start!!","41ce1bdf":"### Linear Model","a4cb8582":"### Cross Validation","d2b30a71":"\n### Compile network\n\nBefore making network ready for training we have to make sure to add below things:\n\n   1. A loss function: to measure how good the network is\n\n   2. An optimizer: to update network as it sees more data and reduce loss value\n\n   3. Metrics: to monitor performance of network\n\n","f6816f2f":"### One Hot encoding of labels.\n\nA one-hot vector is a vector which is 0 in most dimensions, and 1 in a single dimension. In this case, the nth digit will be represented as a vector which is 1 in the nth dimension.\n\nFor example, 3 would be [0,0,0,1,0,0,0,0,0,0].\n","923e5b4c":"\n\nOh its 1 !\n","87ce4722":" The output variable is an integer from 0 to 9. This is a multiclass classification problem.","3caee49b":"Lets create a simple model from Keras Sequential layer.\n\n  1 .  Lambda layer performs simple arithmetic operations like sum, average, exponentiation etc.\n\n 2 .  In 1st layer of the model we have to define input dimensions of our data in (rows,columns,colour channel) format.\n\n 3 .  Flatten will transform input into 1D array.\n\nDense is fully connected layer that means all neurons in previous layers will be connected to all neurons in fully connected layer. In the last layer we have to specify output dimensions\/classes of the model. Here it's 10, since we have to output 10 different digit labels.\n\n","9d0d064b":"### Loading the data\n* We use panda's** read_csv **to read **train.csv** into a dataframe.\n* Then we separate our images and labels for supervised learning.\n* We also do a **train_test_split **to break our data into two sets, one for training and one for testing.","826c7626":"## Designing Neural Network Architecture","930e3e1d":"\n\nMore to come . Please upvote if you find it useful.\n","329f5530":"## Preprocessing the digit images\n### Feature Standardization\n\nIt is important preprocessing step. It is used to centre the data around zero mean and unit variance.\n","aeeab0c7":"\n### Data Augmentation\n\nDifferent data aumentation techniques are as follows:\n\n   1 . Cropping\n   \n   2 . Rotating\n   \n   3 .  Scaling\n   \n   4 . Translating\n   \n   5 . Flipping\n   \n   6 . Adding Gaussian noise to input images etc.\n\n","483153cb":"### Import all required libraries","71d3ac6b":"\n## Fully Connected Model\n\nNeurons in a fully connected layer have full connections to all activations in the previous layer, as seen in regular Neural Networks. Adding another Dense Layer to model.\n","adbbb725":"### Convolutional Neural Network\n\nCNNs are extremely efficient for images.\n","39d88bde":"### Adding Batch Normalization\n\nBN helps to fine tune hyperparameters more better and train really deep neural networks.\n","5c2bc2c9":"Lets plot 5th label.\n","99784e60":"\n### Submitting Predictions to Kaggle.\n\nMake sure you use full train dataset here to train model and predict on test set.\n","89103290":"### Data Visualization\n\nLets look at 8 images from data set with their labels.\n"}}