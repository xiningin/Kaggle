{"cell_type":{"e887ee42":"code","b9690053":"code","662e8b71":"code","8c5cbe38":"code","82f83def":"code","8ecca43f":"code","baf6dcf2":"code","540fb889":"code","b90eb06e":"code","d57dce25":"code","3b44a5b9":"code","960aa53c":"code","15339906":"code","f250a487":"code","5080f85c":"code","5b3fd55c":"code","ea2e2966":"code","cfb78912":"code","9862cd2b":"code","1a0ff00e":"code","012153f3":"markdown","2ffded67":"markdown","9166951d":"markdown","7da8ec82":"markdown","dd8b35b1":"markdown","e5da3ce2":"markdown","1fe770da":"markdown","c1db3b0a":"markdown","eb5c0fd3":"markdown","ae4c9cfc":"markdown"},"source":{"e887ee42":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold\nimport warnings\nimport time\nimport sys\nimport datetime\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport gc\nimport os\nfrom sklearn.metrics import mean_squared_error\nwarnings.simplefilter(action='ignore', category=FutureWarning)","b9690053":"new_transactions = pd.read_csv('..\/input\/new_merchant_transactions.csv')\nnew_transactions.head(5)\n# new_transactions.dtypes","662e8b71":"new_transactions['authorized_flag'] = new_transactions['authorized_flag'].map({'Y':1, 'N':0})\n","8c5cbe38":"def aggregate_new_transactions(new_trans):    \n    agg_func = {\n        'authorized_flag': ['sum', 'mean'],\n        'merchant_id': ['nunique'],\n        'city_id': ['nunique'],\n        'purchase_amount': ['sum', 'median', 'max', 'min', 'std'],\n        'installments': ['sum', 'median', 'max', 'min', 'std'],\n        'month_lag': ['min', 'max']\n        }\n    agg_new_trans = new_trans.groupby(['card_id']).agg(agg_func)\n    print(agg_new_trans.columns.values)\n    agg_new_trans.columns = ['new_' + '_'.join(col).strip() \n                           for col in agg_new_trans.columns.values]\n    agg_new_trans.reset_index(inplace=True)\n    df = (new_trans.groupby('card_id').size().reset_index(name='new_transactions_count'))\n    agg_new_trans = pd.merge(df, agg_new_trans, on='card_id', how='left')    \n    return agg_new_trans\n\nnew_trans = aggregate_new_transactions(new_transactions)\nnew_trans[:10]","82f83def":"gc.collect()\nhistorical_transactions = pd.read_csv('..\/input\/historical_transactions.csv')\nhistorical_transactions[:5]","8ecca43f":"historical_transactions['authorized_flag'] = \\\n    historical_transactions['authorized_flag'].map({'Y':1, 'N':0})","baf6dcf2":"def aggregate_historical_transactions(history):\n    \n    history.loc[:, 'purchase_date'] = pd.DatetimeIndex(history['purchase_date']).\\\n                                      astype(np.int64) * 1e-9\n    \n    agg_func = {\n        'authorized_flag': ['sum', 'mean'],\n        'merchant_id': ['nunique'],\n        'city_id': ['nunique'],\n        'purchase_amount': ['sum', 'median', 'max', 'min', 'std'],\n        'installments': ['sum', 'median', 'max', 'min', 'std'],\n        'purchase_date': [np.ptp],\n        'month_lag': ['min', 'max']\n        }\n    agg_history = history.groupby(['card_id']).agg(agg_func)\n    agg_history.columns = ['hist_' + '_'.join(col).strip() \n                           for col in agg_history.columns.values]\n    agg_history.reset_index(inplace=True)\n    \n    df = (history.groupby('card_id')\n          .size()\n          .reset_index(name='hist_transactions_count'))\n    \n    agg_history = pd.merge(df, agg_history, on='card_id', how='left')\n    \n    return agg_history\n\nhistory = aggregate_historical_transactions(historical_transactions)\nhistory[:10]","540fb889":"def read_data(input_file):\n    df = pd.read_csv(input_file)\n    df['first_active_month'] = pd.to_datetime(df['first_active_month'])\n    df['year'] = df['first_active_month'].dt.year\n    df['month'] = df['first_active_month'].dt.month\n    df['elapsed_time'] = (datetime.date(2018, 2, 1) - df['first_active_month'].dt.date).dt.days\n    return df\ntrain = read_data('..\/input\/train.csv')\ntest = read_data('..\/input\/test.csv')\n\ntarget = train['target']\ndel train['target']\ntrain.head(5)","b90eb06e":"train = pd.merge(train, history, on='card_id', how='left')\ntest = pd.merge(test, history, on='card_id', how='left')\n\ntrain = pd.merge(train, new_trans, on='card_id', how='left')\ntest = pd.merge(test, new_trans, on='card_id', how='left')\ntrain.shape\ntrain.head(5)","d57dce25":"use_cols = [col for col in train.columns if col not in ['card_id', 'first_active_month']]","3b44a5b9":"# \u9009\u51fa\u7279\u5f81\u5217\ntrain = train[use_cols]\ntest = test[use_cols]","960aa53c":"# features = list(train[use_cols].columns)\n# features ==use_cols\ncategorical_feats = [col for col in use_cols if 'feature_' in col]","15339906":"for col in categorical_feats:\n    print(col, 'have', train[col].value_counts().shape[0], 'categories.')","f250a487":"# make 3 categorical features to one hot encoding\nfrom sklearn.preprocessing import LabelEncoder\nfor col in categorical_feats:\n    print(col)\n    lbl = LabelEncoder()\n    lbl.fit(list(train[col].values.astype('str')) + list(test[col].values.astype('str')))\n    train[col] = lbl.transform(list(train[col].values.astype('str')))\n    test[col] = lbl.transform(list(test[col].values.astype('str')))","5080f85c":"df_all = pd.concat([train, test])\ndf_all = pd.get_dummies(df_all, columns=categorical_feats)\nlen_train = train.shape[0]\ntrain = df_all[:len_train]\ntest = df_all[len_train:]","5b3fd55c":"lgb_params = {\"objective\" : \"regression\", \"metric\" : \"rmse\", \n               \"max_depth\": 7, \"min_child_samples\": 20, \n               \"reg_alpha\": 1, \"reg_lambda\": 1,\n               \"num_leaves\" : 64, \"learning_rate\" : 0.001, \n               \"subsample\" : 0.8, \"colsample_bytree\" : 0.8, \n               \"verbosity\": -1}\n# k fold \nFOLDs = KFold(n_splits=5, shuffle=True, random_state=1989) \n\noof_lgb = np.zeros(len(train))\npredictions_lgb = np.zeros(len(test))\n\nfeatures_lgb = list(train.columns)\nfeature_importance_df_lgb = pd.DataFrame()\n\nfor fold_, (trn_idx, val_idx) in enumerate(FOLDs.split(train)):\n    trn_data = lgb.Dataset(train.iloc[trn_idx], label=target.iloc[trn_idx])\n    val_data = lgb.Dataset(train.iloc[val_idx], label=target.iloc[val_idx])\n\n    print(\"LGB \" + str(fold_) + \"-\" * 50)\n    num_round = 2000\n    clf = lgb.train(lgb_params, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=200, early_stopping_rounds = 2000)\n    oof_lgb[val_idx] = clf.predict(train.iloc[val_idx], num_iteration=clf.best_iteration)\n\n    fold_importance_df_lgb = pd.DataFrame()\n    fold_importance_df_lgb[\"feature\"] = features_lgb\n    fold_importance_df_lgb[\"importance\"] = clf.feature_importance()\n    fold_importance_df_lgb[\"fold\"] = fold_ + 1\n    feature_importance_df_lgb = pd.concat([feature_importance_df_lgb, fold_importance_df_lgb], axis=0)\n    predictions_lgb += clf.predict(test, num_iteration=clf.best_iteration) \/ FOLDs.n_splits\n    \n\nprint(np.sqrt(mean_squared_error(oof_lgb, target)))","ea2e2966":"import xgboost as xgb\n\nxgb_params = {'eta': 0.001, 'max_depth': 7, 'subsample': 0.8, 'colsample_bytree': 0.8, \n          'objective': 'reg:linear', 'eval_metric': 'rmse', 'silent': True}\n\n\n\nFOLDs = KFold(n_splits=2, shuffle=True, random_state=1989)\n\noof_xgb = np.zeros(len(train))\npredictions_xgb = np.zeros(len(test))\n\n\nfor fold_, (trn_idx, val_idx) in enumerate(FOLDs.split(train)):\n    trn_data = xgb.DMatrix(data=train.iloc[trn_idx], label=target.iloc[trn_idx])\n    val_data = xgb.DMatrix(data=train.iloc[val_idx], label=target.iloc[val_idx])\n    watchlist = [(trn_data, 'train'), (val_data, 'valid')]\n    print(\"xgb \" + str(fold_) + \"-\" * 50)\n    num_round = 2000\n    xgb_model = xgb.train(xgb_params, trn_data, num_round, watchlist, early_stopping_rounds=100, verbose_eval=200)\n    oof_xgb[val_idx] = xgb_model.predict(xgb.DMatrix(train.iloc[val_idx]), ntree_limit=xgb_model.best_ntree_limit+50)\n\n    predictions_xgb += xgb_model.predict(xgb.DMatrix(test), ntree_limit=xgb_model.best_ntree_limit+50) \/ FOLDs.n_splits\n\nnp.sqrt(mean_squared_error(oof_xgb, target))","cfb78912":"print('lgb', np.sqrt(mean_squared_error(oof_lgb, target)))\nprint('xgb', np.sqrt(mean_squared_error(oof_xgb, target)))\ntotal_sum = 0.5 * oof_lgb + 0.5 * oof_xgb\nprint(\"CV score: {:<8.5f}\".format(mean_squared_error(total_sum, target)**0.5))","9862cd2b":"cols = (feature_importance_df_lgb[[\"feature\", \"importance\"]]\n        .groupby(\"feature\")\n        .mean()\n        .sort_values(by=\"importance\", ascending=False)[:1000].index)\n\nbest_features = feature_importance_df_lgb.loc[feature_importance_df_lgb.feature.isin(cols)]\n\nplt.figure(figsize=(14,14))\nsns.barplot(x=\"importance\",\n            y=\"feature\",\n            data=best_features.sort_values(by=\"importance\",\n                                           ascending=False))\nplt.title('LightGBM Features (avg over folds)')\nplt.tight_layout()\nplt.savefig('lgbm_importances.png')","1a0ff00e":"sub_df = pd.read_csv('..\/input\/sample_submission.csv')\nsub_df[\"target\"] = 0.5 * predictions_lgb + 0.5 * predictions_xgb\nsub_df.to_csv(\"submission.csv\", index=False)","012153f3":"# XGB","2ffded67":"# LGB","9166951d":"# New transactions","7da8ec82":"# I referred a very helpful kernel. Thanks!\n# This is  my first attemp in kaggle, It is use in  learning and exercising.\n# A few weeks later, I will submit my own mode and result. \n# Thanks again,Best Regards.\n-https:\/\/www.kaggle.com\/youhanlee\/hello-elo-ensemble-will-help-you\n-https:\/\/www.kaggle.com\/fabiendaniel\/elo-world","dd8b35b1":"# Feature preparation","e5da3ce2":"- 3 categorical features have not many categories. For now, we use one-hot encoding.","1fe770da":"- This kernel shows \n- how to use 'new_merchant_transactions.csv' \n- how to use lgb and xgb\n- how to ensemble them.","c1db3b0a":"# You know that ensemble is always answer.","eb5c0fd3":"# Merge historical transactions and new transactions","ae4c9cfc":"# historical_transactions"}}