{"cell_type":{"d7d63fae":"code","b09cbc05":"code","5e39a164":"code","413f8fcc":"code","15f148d6":"code","5183bba7":"code","939e8956":"code","3b09f6be":"code","e71a6a35":"code","080b9fd1":"code","6292b1d7":"code","0777d129":"code","3e4baa43":"code","553b38ac":"code","bf917bf8":"code","b3393261":"code","a870b9a4":"code","ed54aced":"code","0a1494db":"code","f1580caf":"code","43065587":"code","a0242957":"code","b56eacf2":"code","4800f0e6":"code","434c0670":"code","838c5ada":"code","3c0a5df2":"code","443c7356":"code","0dc32cbf":"code","db2bd2da":"code","1d8a3025":"code","0773086f":"code","9559c986":"code","624664c6":"code","cad4a3b0":"code","94ee435c":"code","d08f0991":"code","473164ac":"code","4737d43b":"code","c5c4432a":"code","94453210":"code","9586c8ad":"code","23bb1a4a":"code","537ebb56":"code","e34f735b":"code","a875e745":"code","a41b950f":"code","4756f330":"code","084a9211":"code","de6fd4b0":"code","e0bfcc35":"code","29b91d85":"code","9f688b5f":"code","de5f71ce":"code","358947a6":"code","96712408":"code","238360fd":"code","f55769ca":"code","478998e7":"code","980bbd54":"code","9d90d3b0":"code","772a3ba9":"code","7c900d61":"code","0302083d":"code","0d6499cd":"code","713a0625":"code","598c41f9":"code","8133496d":"code","0c650aa1":"code","6bf92636":"code","ce387497":"code","138bd6a8":"code","429e7664":"code","81e41cc7":"code","15de997f":"code","a06bbdc9":"code","49c137a8":"code","19ff3528":"code","58100943":"code","f18c5aeb":"code","6539ea06":"code","587ba15f":"markdown","b2c9e8e4":"markdown","9dbae96c":"markdown","ac34b2c1":"markdown","bba814e2":"markdown","4c50932b":"markdown","8140a091":"markdown","83f03afc":"markdown","283d8255":"markdown","f3d57c85":"markdown","12e46498":"markdown","f62d9686":"markdown","b2c9087e":"markdown","02504d78":"markdown","10f7f6eb":"markdown","41ee8752":"markdown","7291378d":"markdown","2a9064f1":"markdown","57db2b11":"markdown","0b362357":"markdown","58f5ed6d":"markdown","70cb68be":"markdown","86489bdc":"markdown","049947d2":"markdown","ccae3c16":"markdown","e6a1020d":"markdown","9e62c59a":"markdown","bfacb9e3":"markdown"},"source":{"d7d63fae":"import numpy as np \nimport pandas as pd ","b09cbc05":"# import library\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn import datasets\nfrom sklearn import manifold\n\n%matplotlib inline\n\nmatplotlib.rc('xtick', labelsize=20)\nmatplotlib.rc('ytick', labelsize=20)\n\nfrom sklearn import model_selection\nfrom collections import Counter\n\nprint(\"Setup Complete\")","5e39a164":"# A dataset that finally predicts the quality of red wine with multiple features\n# features : fixed acidity, volatile acidity....alcohol, quality\n\ndf = pd.read_csv('..\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv')\ndf.head()","413f8fcc":"df.describe() # quality grades are distributed from 3 to 8 ","15f148d6":"# mapping\n\nquality_mapping = {\n    3: 0,\n    4: 1,\n    5: 2,\n    6: 3,\n    7: 4,\n    8: 5\n}\n\ndf.loc[:, 'quality'] = df.quality.map(quality_mapping)\ndf.head()","5183bba7":"# data mix and reset index \n\ndf = df.sample(frac=1).reset_index(drop=True) \ndf.shape # 1,599 x 12 matrix ","939e8956":"# train : 1,000, test : 599\n\ndf_train = df.head(1000)\ndf_test = df.tail(599)","3b09f6be":"# decision Tree \n\nfrom sklearn import tree\nfrom sklearn import metrics\n\nclf = tree.DecisionTreeClassifier(max_depth=3)\n\ncols = ['fixed acidity',\n       'volatile acidity',\n       'citric acid',\n       'residual sugar',\n       'chlorides',\n       'free sulfur dioxide',\n       'total sulfur dioxide',\n       'density',\n       'pH',\n       'sulphates',\n       'alcohol']\n\nclf.fit(df_train[cols], df_train.quality)","e71a6a35":"# Confirmation of accuracy values of learning data and test data: The basic accuracy of the metric (when the data has a Gaussian distribution form)\n\ntrain_predictions = clf.predict(df_train[cols])\ntest_predictions = clf.predict(df_test[cols])\n\ntrain_accuracy = metrics.accuracy_score(df_train.quality, train_predictions)\ntest_accuracy = metrics.accuracy_score(df_test.quality, test_predictions)\n\ntrain_accuracy, test_accuracy","080b9fd1":"# Learning and test accuracy initialized to 0.5 \n\ntrain_accuracies = [0.5]\ntest_accuracies = [0.5]\n\n# Learning in depth from 1 to 24 \n\nfor depth in range(1, 25):\n    clf = tree.DecisionTreeClassifier(max_depth=depth)\n    clf.fit(df_train[cols], df_train.quality)          # supervised learning\n    \n    train_predictions = clf.predict(df_train[cols])\n    test_predictions = clf.predict(df_test[cols])\n    \n    train_accuracy = metrics.accuracy_score(df_train.quality, train_predictions)\n    test_accuracy = metrics.accuracy_score(df_test.quality, test_predictions)\n    \n    # Update the accuracy for each depth \n    train_accuracies.append(train_accuracy)\n    test_accuracies.append(test_accuracy)","6292b1d7":"plt.figure(figsize=(10, 5))\nsns.set_style('whitegrid')\nplt.plot(train_accuracies, label='train accuracy')\nplt.plot(test_accuracies, label='test accuracy')\nplt.legend(loc='upper left', prop={'size': 15})\nplt.xticks(range(0, 26, 5))\nplt.xlabel('max_depth', size=20)\nplt.ylabel('accuracy', size=20)\nplt.show()","0777d129":"# k-fold Cross Validation code \n\n'''\nif __name__ == \"__main__\":\n    df = pd.read_csv # load data\n    \n    df['kfold'] = -1\n    \n    # reset index \n    df = df.sample(frac=1).reset_index(drop=True)\n    \n    # Initialize the kfold class (divide into five) \n    kf = model_selection.KFold(n_splits=5)\n    \n    # Set the kfold column to the fold ID \n    for fold, (trn_, val_) in enumerate(kf.split(X=df)):\n        df.loc[val_, 'kfold'] = fold\n        \n    # Save the new data as a CSV file along with the kfold column\n    df.to_csv(\"train_folds.csv\", index=False)\n'''","3e4baa43":"# Looking at the distribution of the red wine label values, it can be seen that the values of the multi-label are quite skewed.   \n# (It is concentrated in quality 2 and 3)    \n\nb = sns.countplot(x='quality', data=df)\nb.set_xlabel('quality', fontsize=20)\nb.set_ylabel('count', fontsize=20)","553b38ac":"# Stratified k-fold Cross Validation code \n\n'''\nif __name__ == \"__main__\":\n    df = pd.read_csv(\ub370\uc774\ud130 \ubd88\ub7ec\uc624\uae30)\n    \n    df['kfold'] = -1\n\n    df = df.sample(frac=1).reset_index(drop=True)\n    \n    y = df.target.values\n    \n    kf = model_selection.StratifiedKFold(n_splits=5)\n\n    for f, (t_, v_) in enumerate(kf.split(X=df, y=y)):\n        df.loc[v_, 'kfold'] = f\n\n    df.to_csv(\"train_folds.csv\", index=False)\n'''","bf917bf8":"# Stratified KFold common code for use in regression \n\ndef create_folds(data):\n    \n    data['kfold'] = -1\n    \n    data = data.sample(frac=1).reset_index(drop=True)\n    \n    # Calculate the number of groups to divide the target variable through the Sturge rule (np.floor (rounded), np.round (rounded) both can be used) \n    num_bins = int(np.floor(1 + np.log2(len(data))))\n    \n    data.loc[:, \"bins\"] = pd.cut(data['target'], bins=num_bins, labels=False)\n    \n    kf = model_selection.StratifiedKFold(n_splits=5)\n    \n    # Fill in the kfold column and use the group index, not the original target variable value.\n    for f, (t_, v_) in enumerate(kf.split(X=data, y=data.bins.values)):\n        data.loc[v_, 'kfold'] = f\n    \n    data = data.drop(\"bins\", axis=1)\n    \n    return data\n\nif __name__ == '__main__':\n\n    X, y = datasets.make_regression(\n    n_samples=15000, n_features=100, n_targets=1)\n    \n    df = pd.DataFrame(X, columns=[f\"f_{i}\" for i in range(X.shape[1])])\n    df.loc[:, 'target'] = y\n    \n    df = create_folds(df)","b3393261":"df.describe()","a870b9a4":"df.tail(5)","ed54aced":"# Check if it's well divided into kfolds (0-4 divided into 5 folds)\n\nc = sns.countplot(x='kfold', data=df)\nc.set_xlabel('kfold', fontsize=20)\nc.set_ylabel('count', fontsize=20)","0a1494db":"# accuracy\ndef accuracy(y_true, y_pred):\n    \"\"\"\n    :param y_true : a list of target values\n    :param y_pred : a list of predict values\n    :return : accuracy\n    \"\"\"\n    correct_counter = 0\n    for yt, yp in zip(y_true, y_pred):\n        if yt == yp:\n            correct_counter += 1\n    return correct_counter \/ len(y_true)","f1580caf":"from sklearn import metrics\n\nl1 = [0,1,1,1,0,0,0,1]\nl2 = [0,1,0,1,0,1,0,0]\n\nmetrics.accuracy_score(l1, l2), accuracy(l1, l2) ","43065587":"def true_positive(y_true, y_pred):\n    ''' \n    :param y_true : a list of target values\n    :param y_pred : a list of predict values\n    :return : no.TP      \n    '''\n    tp = 0\n    for yt, yp in zip(y_true, y_pred): # zip : Cut the same length list into the same index and return the list. \n        if yt == 1 and yp == 1:\n            tp += 1\n    return tp\n\ndef true_negative(y_true, y_pred):\n    tn = 0\n    for yt, yp in zip(y_true, y_pred):\n        if yt == 0 and yp == 0:\n            tn += 1\n    return tn \n\ndef false_positive(y_true, y_pred):\n    fp = 0\n    for yt, yp in zip(y_true, y_pred):\n        if yt == 0 and yp == 1:\n            fp += 1\n    return fp\n\ndef false_negative(y_true, y_pred):\n    fn = 0\n    for yt, yp in zip(y_true, y_pred):\n        if yt == 1 and yp == 0:\n            fn += 1\n    return fn","a0242957":"l1, l2","b56eacf2":"# TP, FP, FN, and TN can be obtained by adding the above list to the function. \n\ntrue_positive(l1, l2), false_positive(l1, l2)","4800f0e6":"false_negative(l1, l2), true_negative(l1, l2)","434c0670":"def accuracy_v2(y_true, y_pred):\n    '''\n    A function that calculates accuracy using tp\/tn\/fp\/fn : (tp + tn) \/ (tp + tn + fp + fn)\n    '''\n    tp = true_positive(y_true, y_pred)\n    tn = true_negative(y_true, y_pred)\n    fp = false_positive(y_true, y_pred)\n    fn = false_negative(y_true, y_pred)\n    \n    accuracy_score = (tp + tn) \/ (tp + tn + fp + fn) # The proportion of true in total.\n    return accuracy_score","838c5ada":"accuracy(l1, l2)","3c0a5df2":"accuracy_v2(l1, l2)","443c7356":"metrics.accuracy_score(l1, l2)","0dc32cbf":"def precision(y_true, y_pred):\n    '''\n    A function for calculating precision: tp \/ (tp + fp)  \n    Probability when the model is predicted to be positive.\n    '''\n    tp = true_positive(y_true, y_pred)\n    fp = false_positive(y_true, y_pred)\n    precision = tp \/ (tp + fp)\n    return precision","db2bd2da":"precision(l1, l2)","1d8a3025":"def recall(y_true, y_pred):\n    '''\n    A function for calculating a recall: tp \/ (tp + fn)\n    The proportion of positive ones that are positively.\n    '''\n    tp = true_positive(y_true, y_pred)\n    fn = false_negative(y_true, y_pred)\n    recall = tp \/ (tp + fn)\n    return recall","0773086f":"recall(l1, l2)","9559c986":"# precision - recall curve\n\ny_true = [0,0,0,1,0,0,0,0,0,0,\n         1,0,0,0,0,0,0,0,1,0]\n\ny_pred = [0.02638,0.1111,0.316,\n         0.050,0.019,0.1755,\n         0.1595,0.0382,0.1163,\n         0.0793,0.0858,0.3909,\n         0.2726,0.0344,0.0464,\n         0.0354,0.1852,0.0593,\n         0.6197,0.3306]","624664c6":"precisions = []\nrecalls = []\nthresholds = [0.049,0.059,0.0793,\n             0.0858,0.1111,0.1163,\n             0.1595,0.1755,0.1852,\n             0.2726,0.3162,0.3306,\n             0.3909,0.6197]\n\nfor i in thresholds:\n    temp_prediction = [1 if x >= i else 0 for x in y_pred] # If y_pred is greater than thresholds, it means that it is predicted to be 1. \n    p = precision(y_true, temp_prediction)\n    r = recall(y_true, temp_prediction)\n    precisions.append(p)\n    recalls.append(r)","cad4a3b0":"plt.figure(figsize=(7, 7))\nplt.plot(recalls, precisions)\nplt.xlabel('Recall', fontsize=15)\nplt.ylabel('Precision', fontsize=15)","94ee435c":"def f1(y_true, y_pred):\n    '''\n    f1 score\n    '''\n    p = precision(y_true, y_pred)\n    r = recall(y_true, y_pred)\n    score = 2 * p * r \/ (p + r)\n    return score","d08f0991":"y_true = [0,0,0,1,0,0,0,0,0,0,\n         1,0,0,0,0,0,0,0,1,0]\ny_pred = [0,0,1,0,0,0,1,0,0,0,\n         1,0,0,0,0,0,0,0,1,0]\n\nf1(y_true, y_pred), metrics.f1_score(y_true, y_pred) # \uc218\uc2dd\uc73c\ub85c \ub9cc\ub4e0 \ud568\uc218\uc640 sklearn \uc758 metric library \uc758 f1_score \uac12\uc774 \uc77c\uce58 ","473164ac":"def tpr(y_true, y_pred):\n    '''\n    tpr calculation \n    '''\n    return recall(y_true, y_pred)","4737d43b":"def fpr(y_true, y_pred):\n    '''\n    fpr calculation\n    '''\n    fp = false_positive(y_true, y_pred)\n    tn = true_negative(y_true, y_pred)\n    return fp \/ (tn + fp)","c5c4432a":"tpr_list = []\nfpr_list = []\n\ny_true = [0,0,0,0,1,0,1,\n         0,0,1,0,1,0,0,1]\n\n# The probability that the sample target value is 1\ny_pred = [0.1,0.3,0.2,0.6,0.8,0.05,\n         0.9,0.5,0.3,0.66,0.3,0.2,\n         0.85,0.15,0.99]\n\nthresholds = [0,0.1,0.2,0.3,0.4,0.5,\n             0.6,0.7,0.8,0.85,0.90,0.99,1.0]","94453210":"for thresh in thresholds:\n    temp_pred = [1 if x >= thresh else 0 for x in y_pred] # If y_pred is greater than thresholds, it means that it is predicted to be 1. \n    temp_tpr = tpr(y_true, temp_pred)\n    temp_fpr = fpr(y_true, temp_pred)\n    tpr_list.append(temp_tpr)\n    fpr_list.append(temp_fpr)","9586c8ad":"# ROC CURVE (FPR - TPR CURVE) : \ub2f9\uc5f0\ud788 \uba74\uc801\uc774 \ub113\uc740 \uac83\uc774 \uc815\ud655\ud788 \uc608\uce21\ud588\ub2e4\ub294 \uc758\ubbf8\uc784. FPR - TPR \uc774 \uc644\ubcbd\ud788 \ubc18\ub300\uc774\ubbc0\ub85c\nthresholds, tpr_list, fpr_list\n\nplt.figure(figsize=(7, 7))\nplt.plot(fpr_list, tpr_list)\nplt.xlabel('FPR', fontsize=15)\nplt.ylabel('TPR', fontsize=15)","23bb1a4a":"metrics.roc_auc_score(y_true, y_pred)","537ebb56":"tp_list = []\nfp_list = []\n\ny_true = [0,0,0,0,1,0,1,\n         0,0,1,0,1,0,0,1]\n\n# \uc0d8\ud50c \ud0c0\uac9f \uac12\uc774 1 \uc77c \ud655\ub960\ny_pred = [0.1,0.3,0.2,0.6,0.8,0.05,\n         0.9,0.5,0.3,0.66,0.3,0.2,\n         0.85,0.15,0.99]\n\nthresholds = [0,0.1,0.2,0.3,0.4,0.5,\n             0.6,0.7,0.8,0.85,0.90,0.99,1.0]\n\nfor thresh in thresholds:\n    temp_pred = [1 if x >= thresh else 0 for x in y_pred]\n    temp_tp = true_positive(y_true, temp_pred)\n    temp_fp = false_positive(y_true, temp_pred)\n    tp_list.append(temp_tp)\n    fp_list.append(temp_fp)","e34f735b":"thresholds, tp_list, fp_list","a875e745":"plt.figure(figsize=(7, 7))\nplt.plot(fp_list, tp_list)\nplt.xlabel('FPR', fontsize=15)\nplt.ylabel('TPR', fontsize=15)\n\n# AUC is the metric of biased binary classification: the highest reference point at the point FPR 0.2, TPR 4 on the graph below.","a41b950f":"def log_loss(y_true, y_proba):\n    '''\n    \ub85c\uadf8 \uc190\uc2e4 \uacc4\uc0b0 \ud568\uc218\n    :param y_true : \ud0c0\uac9f \uac12 \ubaa9\ub85d\n    :param y_proba : \uc608\uce21 \uac12 \ubaa9\ub85d\n    :return : \ub85c\uadf8 \uc190\uc2e4\n    '''\n    \n    # \uadf9\ub2e8\uc801 \ud655\ub960 \uac12 \uc870\uc815\uc5d0 \ud544\uc694\ud55c epsilon \uac12 \n    epsilon = 1e-15\n    loss = [] \n    \n    for yt, yp in zip(y_true, y_proba):\n        # \ud655\ub960\uac12 \uc870\uc815 \n        # 0 --> 1e-15\n        # 1 --> 1-1e-15\n        yp = np.clip(yp, epsilon, 1-epsilon) # np.clip(\ubc30\uc5f4, \ucd5c\uc18c\uac12, \ucd5c\ub300\uac12 \uae30\uc900) \n        \n        # \ud558\ub098\uc758 \uc0d8\ud50c\uc5d0 \ub300\ud55c \ub85c\uadf8 \uc190\uc2e4 \uacc4\uc0b0 \n        temp_loss = -1.0 * (yt * np.log(yp) + (1 - yt) * np.log(1 - yp))\n        loss.append(temp_loss)\n    \n    return np.mean(loss)","4756f330":"a = np.arange(-5, 5)\na","084a9211":"np.clip(a, -1, 2), np.clip(a, -2, 6)","de6fd4b0":"a","e0bfcc35":"# \uc6d0\ubcf8 \ubcc0\ud654 \nnp.clip(a, -1, 4, out=a), a","29b91d85":"# log_loss test\ny_true = [0,0,0,0,1,0,1,\n         0,0,1,0,1,0,0,1]\n\n# The probability that the sample target value is 1.\ny_proba = [0.1,0.3,0.2,0.6,0.8,0.05,\n         0.9,0.5,0.3,0.66,0.3,0.2,\n         0.85,0.15,0.99]\n\nlog_loss(y_true, y_proba), metrics.log_loss(y_true, y_proba)","9f688b5f":"def macro_precision(y_true, y_pred):\n    '''\n    a function of calculating the macro precision\n    '''\n    # \ud0c0\uac9f \uac12\uc758 \ubaa9\ub85d\uc5d0\uc11c \uace0\uc720 \uac12 \uac1c\uc218\ub97c \ucc3e\uc544 \ubc94\uc8fc\uc758 \uac1c\uc218\ub97c \uad6c\ud568. \n    num_classes = len(np.unique(y_true)) \n    \n    # \uc815\ubc00\ub3c4\ub97c 0 \uc73c\ub85c \ucd08\uae30\ud654 \n    precision = 0 \n    \n    for class_ in range(num_classes):\n        # \ud604\uc7ac \ubc94\uc8fc \uc678\uc5d4 \ubaa8\ub450 \uc74c\uc758 \ubc94\uc8fc\ub85c \uac04\uc8fc \n        temp_true = [1 if p == class_ else 0 for p in y_true]\n        temp_pred = [1 if p == class_ else 0 for p in y_pred]\n        \n        # \ud604\uc7ac \ubc94\uc8fc\uc5d0 \ub300\ud55c tp, fp \uacc4\uc0b0 \n        tp = true_positive(temp_true, temp_pred)\n        fp = false_positive(temp_true, temp_pred)\n        \n        # \ud604\uc7ac \ubc94\uc8fc\uc5d0\uc11c \uc815\ubc00\ub3c4 \uacc4\uc0b0 \n        temp_precision = tp \/ (tp + fp)\n        \n        # \ubaa8\ub4e0 \ubc94\uc8fc\uc5d0 \ub300\ud55c \uc815\ubc00\ub3c4 \ud569\uc0b0 \n        precision += temp_precision\n        \n    # \ubaa8\ub4e0 \ubc94\uc8fc\uc758 \uc815\ubc00\ub3c4 \ud569\uc744 \ubc94\uc8fc\uc758 \uac1c\uc218\ub85c \ub098\ub204\uc5b4 \ub9e4\ud06c\ub85c \ud3c9\uade0 \uc815\ubc00\ub3c4 \uacc4\uc0b0 \n    precision \/= num_classes\n    return precision","de5f71ce":"def micro_precision(y_true, y_pred):\n    '''\n    Micro average precision calculation\n    '''\n    num_classes = len(np.unique(y_true))\n    \n    # Initialize tp and fp \n    tp = 0\n    fp = 0\n    \n    # Calculate for all categories \n    for class_ in range(num_classes):\n        temp_true = [1 if p == class_ else 0 for p in y_true]\n        temp_pred = [1 if p == class_ else 0 for p in y_pred]\n        \n        # \ud604\uc7ac \ubc94\uc8fc\uc5d0 \ub300\ud55c tp \uacc4\uc0b0\ud558\uc5ec \uc804\uccb4 tp update\n        tp += true_positive(temp_true, temp_pred)\n        \n        # \ud604\uc7ac \ubc94\uc8fc\uc5d0 \ub300\ud55c fp \uacc4\uc0b0\ud558\uc5ec \uc804\uccb4 fp update\n        fp += false_positive(temp_true, temp_pred)\n    \n    # \uc804\uccb4 \uc815\ubc00\ub3c4 \uacc4\uc0b0\n    precision = tp \/ (tp + fp)\n    return precision ","358947a6":"def weighted_precision(y_true, y_pred):\n    '''\n    A function to find weighted mean precision.\n    '''\n    num_classes = len(np.unique(y_true))\n    \n    # \ub2e4\uc74c\uacfc \uac19\uc740 \ubc94\uc8fc: \uc0d8\ud50c \uac1c\uc218 \uc0ac\uc804\uc744 \ub9cc\ub4ec.\n    # {0: 20, 1:15, 2:21}\n    class_counts = Counter(y_true)\n    \n    precision = 0\n    \n    for class_ in range(num_classes):\n        # \ud604\uc7ac \ubc94\uc8fc \uc678\uc5d4 \uc74c \uac04\uc8fc \n        temp_true = [1 if p == class_ else 0 for p in y_true]\n        temp_pred = [1 if p == class_ else 0 for p in y_pred]\n        \n        # \ud604\uc7ac \ubc94\uc8fc\uc5d0\uc11c tp, fp \uacc4\uc0b0 \n        tp = true_positive(temp_true, temp_pred)\n        fp = false_positive(temp_true, temp_pred)\n        \n        # \ud604\uc7ac \ubc94\uc8fc\uc758 \uc815\ubc00\ub3c4 \uacc4\uc0b0 \n        temp_precision = tp \/ (tp + fp)\n        \n        # \uc815\ubc00\ub3c4\uc5d0 \ud604\uc7ac \ubc94\uc8fc\uc758 \uc0d8\ud50c \uc218 \uacf1\ud568. \n        weighted_precision = class_counts[class_] * temp_precision\n        \n        # \uc804\uccb4 \uc815\ubc00\ub3c4\uc5d0 \ub354\ud568. \n        precision += weighted_precision\n        \n    # \uc804\uccb4 \uc815\ubc00\ub3c4\ub97c \uc804\uccb4 \uc0d8\ud50c \uc218\ub85c \ub098\ub204\uc5b4 \uac00\uc911 \ud3c9\uade0 \uc815\ubc00\ub3c4\ub97c \uad6c\ud568. \n    overall_precision = precision \/ len(y_true)\n    return overall_precision     ","96712408":"y_true = [0,1,2,0,1,2,0,2,2]\ny_pred = [0,2,1,0,2,1,0,0,2]\n\nmacro_precision(y_true, y_pred), metrics.precision_score(y_true, y_pred, average='macro')","238360fd":"micro_precision(y_true, y_pred), metrics.precision_score(y_true, y_pred, average='micro')","f55769ca":"weighted_precision(y_true, y_pred), metrics.precision_score(y_true, y_pred, average='weighted')","478998e7":"def weighted_f1(y_true, y_pred):\n    '''\n    Weighted mean f1 function\n    :param y_true : A list of target values\n    :param y_pred : A list of prediction values\n    :return : Weighted mean f1\n    '''\n    num_classes = len(np.unique(y_true))\n    \n    # Category: Sample Count Dictionary \n    # {0:20, 1:15, 2:21}\n    class_counts = Counter(y_true)\n    \n    f1 = 0\n    \n    for class_ in range(num_classes):\n        temp_true = [1 if p == class_ else 0 for p in y_true]\n        temp_pred = [1 if p == class_ else 0 for p in y_pred]\n        \n        p = precision(temp_true, temp_pred)\n        r = recall(temp_true, temp_pred)\n        \n        # Calculate f1 in the current category \n        if p + r != 0:\n            temp_f1 = 2 * p * r \/ (p + r)\n        else:\n            temp_f1 = 0\n        \n        # Multiply f1 by the number of samples in the current category.\n        weighted_f1 = class_counts[class_] * temp_f1\n        \n        f1 += weighted_f1\n        \n    # The weighted average f1 is obtained by dividing the total f1 by the total number of samples. \n    overall_f1 = f1 \/ len(y_true)\n    \n    return overall_f1    ","980bbd54":"y_true = [0,1,2,0,1,2,0,2,2]\ny_pred = [0,2,1,0,2,1,0,0,2]\n\nweighted_f1(y_true, y_pred), metrics.f1_score(y_true, y_pred, average='weighted')","9d90d3b0":"# confusion matrix : The more numbers are concentrated on the diagonal, the better algorithm (indicating the difference between prediction and practice)\n\ncm = metrics.confusion_matrix(y_true, y_pred)\n\nplt.figure(figsize=(10, 10))\ncmap = sns.cubehelix_palette(50, hue=0.05, rot=0, light=0.9, dark=0, as_cmap=True)\nsns.set(font_scale=2.5)\nsns.heatmap(cm, annot=True, cmap=cmap, cbar=False)\nplt.ylabel('Actual Labels', fontsize=20)\nplt.xlabel('Predicted Labels', fontsize=20)","772a3ba9":"# P@k precision is different from the previous precision.\n# When there is an actual\/prediction category in a sample, the number of prediction categories included in the actual category among the top k is divided by k.","7c900d61":"def pk(y_true, y_pred, k):\n    '''\n    A function of obtaining P@k for one sample.\n    :param y_true: \ud0c0\uac9f \uac12 \ubaa9\ub85d\n    :param y_pred: \uc608\uce21 \ubc94\uc8fc \uac12 \ubaa9\ub85d\n    :param k: k\n    :return: P@k\n    '''\n    # k \uac00 0 \uc774\uba74 0 \uc744 \ubc18\ud658, k \ub294 \ud56d\uc0c1 \uc591\uc218 \n    if k == 0:\n        return 0 \n    \n    # \uc0c1\uc704-k \uc608\uce21\uc5d0\ub9cc \uad00\uc2ec\n    y_pred = y_pred[:k]\n    \n    # \uc608\uce21 \ubc94\uc8fc \ubaa9\ub85d\uc744 set \uc73c\ub85c \ubcc0\ud658\n    pred_set = set(y_pred)\n    \n    # \ud0c0\uac9f \uac12 \ubaa9\ub85d\uc744 set \uc73c\ub85c \ubcc0\ud658\n    true_set = set(y_true)\n    \n    # \uad50\uc9d1\ud569\uc744 \uac00\uc9d0 \n    common_values = pred_set.intersection(true_set) # \uad50\uc9d1\ud569 \n    \n    # \uad50\uc9d1\ud569\uc758 \ud06c\uae30\ub97c k \ub85c \ub098\ub208 \uac12\uc744 \ubc18\ud658 \n    return len(common_values) \/ len(y_pred[:k])","0302083d":"def apk(y_true, y_pred, k):\n    '''\n    A function of obtaining AP@k for one sample. \n    :param y_true\n    :param y_pred\n    :param k\n    :return AP@k\n    '''\n    # p@k \ubaa9\ub85d\uc744 \ucd08\uae30\ud654 \n    pk_values = []\n    \n    # \ubaa8\ub4e0 k \uac12\uc5d0 \ub300\ud574 \uacc4\uc0b0 \n    for i in range(1, k+1):\n        pk_values.append(pk(y_true, y_pred, i)) # \uc704\uc5d0\uc11c \ub9cc\ub4e0 pk \ud568\uc218\ub97c \uc0ac\uc6a9\n        \n    # \ubaa9\ub85d\uc774 \ube44\uc5b4 \uc788\uc73c\uba74 0 \uc744 \ubc18\ud658 \n    if len(pk_values) == 0:\n        return 0\n    \n    # \uc544\ub2c8\uba74 \ubaa9\ub85d\uc758 \uac12\uc758 \ud569\uc744 \uc804\uccb4 \uac12\uc758 \uac1c\uc218\ub85c \ub098\ub208 \uac12\uc744 \ubc18\ud658 \n    return sum(pk_values) \/ len(pk_values)","0d6499cd":"# Calculate AP@k for each sample when there is an actual\/prediction label\n\ny_true = [\n    [1,2,3],\n    [0,2],\n    [1],\n    [2,3],\n    [1,0],\n    []\n]\n\ny_pred = [\n    [0,1,2],\n    [1],\n    [0,2,3],\n    [2,3,4,0],\n    [0,1,2],\n    [0]\n]\n\nfor i in range(len(y_true)):\n    for j in range(1, 4):\n        print(\n            f\"\"\"\n            y_true={y_true[i]}, \n            y_pred={y_pred[i]},\n            AP@{j} = {apk(y_true[i], y_pred[i], k=j)}\n            \"\"\"\n        )","713a0625":"# \ucd94\ucc9c \uc2dc\uc2a4\ud15c\uc758 \ud3c9\uac00 \ubc29\ubc95\uc73c\ub85c \uc544\uc8fc \uc720\uc6a9 : \uc815\ubc00\ub3c4\uc640 RECALL \uacfc \ub2e4\ub974\uac8c \ucd94\ucc9c \uc2dc\uc2a4\ud15c\uc758 \ud3c9\uac00\uc5d0\ub294 \ubb34\uc5c7\uc744 \uc55e\uc5d0 \ub098\uc5f4\ud588\ub294\uc9c0\uac00 \ub9e4\uc6b0 \uc911\uc694 (\ub9e4\ucd9c\uacfc \uc5f0\uad00), \ub530\ub77c\uc11c MAP \uc5d0\ub294 \n# K \ub77c\ub294 \uac12\uc744 \uc124\uc815\ud558\uac8c \ub418\uba70, \uc774 K \uac12\uc740 \ucd94\ucc9c \uc2dc\uc2a4\ud15c\uc744 \uc5b4\ub514\uae4c\uc9c0 \ud3c9\uac00 \ud560 \uac83\uc778\uac00\ub97c \uacb0\uc815\ud558\ub294 Index \uac12\uc784. \n\ndef mapk(y_true, y_pred, k):\n    '''\n    A function of obtaining MAP@k for one sample.\n    '''\n    apk_values = []\n    \n    for i in range(len(y_true)):\n        # Calculate the apk value for all samples and add it to the list\n        apk_values.append(\n        apk(y_true[i], y_pred[i], k=k)\n        )\n    \n    return sum(apk_values) \/ len(apk_values)","598c41f9":"y_true = [\n    [1,2,3],\n    [0,2],\n    [1],\n    [2,3],\n    [1,0],\n    []\n]\n\ny_pred = [\n    [0,1,2],\n    [1],\n    [0,2,3],\n    [2,3,4,0],\n    [0,1,2],\n    [0]\n]","8133496d":"mapk(y_true, y_pred, k=1)","0c650aa1":"mapk(y_true, y_pred, k=2)","6bf92636":"mapk(y_true, y_pred, k=3)","ce387497":"mapk(y_true, y_pred, k=4)","138bd6a8":"def apk(actual, predicted, k=10):\n    '''\n    A function to find AP@k.\n    \n    This function calculates AP@k from two lists.\n    The AP@k value increases when the right prediction value is in front of the list by weighting the order of the prediction values. \n    \n    Parameters\n    ----------\n    actual : list (Target value list, order doesn't matter)\n    predicted : list (prediction list, order correlation, It is related to sales because it is often used recommendation system.)\n    k : int, optional (Maximum number of predicted values to be used)\n    \n    Returns\n    -------\n    score : double AP@k\n    '''\n    \n    if len(predicted) > k :\n        predicted = predicted[:k]\n    \n    score = 0.0\n    num_hits = 0.0\n    \n    for i,p in enumerate(predicted):\n        if p in actual and p not in predicted[:1]:\n            num_hits += 1.0\n            score += num_hits \/ (i+1.0)\n    \n    if not actual:\n        return 0.0\n    \n    return score \/ min(len(actual), k)             ","429e7664":"def mean_absolute_error(y_true, y_pred):\n    '''\n    A function to find MAE \n    :param y_true : A list of target values\n    :param y_pred : A list of prediction values\n    :return : mae\n    '''\n    \n    error = 0\n    \n    for yt, yp in zip(y_true, y_pred):\n        error += np.abs(yt - yp)\n    \n    return error \/ len(y_true)\n\n# \ub610\ub294 \uc544\ub798\uc640 \uac19\uc774 \uc9e7\uac8c \ucd5c\uc801\ud654\ub3c4 \uac00\ub2a5 (\ud559\uc2b5\uc744 \uc704\ud574 \uc77c\ubd80\ub7ec low level \uba54\ud2b8\ub9ad \uc218\uc900\uc744 \uc774\ud574\ud574\uc57c \ud568.)\n\ndef mae_np(y_true, y_pred):\n    return np.mean(np.abs(y_true - y_pred))","81e41cc7":"def mean_squared_error(y_true, y_pred):\n    '''\n    A function to find MSE\n    :param y_true : A list of target values\n    :param y_pred : A list of prediction values\n    :return : mse\n    '''\n    error = 0\n    \n    for yt, yp in zip(y_true, y_pred):\n        error += (yt - yp) ** 2\n    \n    return error \/ len(y_true)","15de997f":"def mean_squared_log_error(y_true, y_pred):\n    '''\n    A function to find MSLE \n    '''\n    \n    error = 0\n    \n    for yt, yp in zip(y_true, y_pred):\n        error += (np.log(1 + yt) - np.log(1 + yp)) ** 2\n    \n    return error \/ len(y_true)","a06bbdc9":"def root_mean_squared_log_error(y_true, y_pred):\n    '''\n    A function to find RMSLE \n    '''\n    \n    error = 0\n    \n    for yt, yp in zip(y_true, y_pred):\n        error += np.sqrt((np.log(1 + yt) - np.log(1 + yp)) ** 2)\n    \n    return error \/ len(y_true)","49c137a8":"def mean_percentage_error(y_true, y_pred):\n    '''\n    A function to find MPE\n    '''\n    \n    error = 0\n    \n    for yt, yp in zip(y_true, y_pred):\n        error += (yt - yp) \/ yt\n    \n    return error \/ len(y_true)","19ff3528":"def mean_absolute_percentage_error(y_true, y_pred):\n    '''\n    A function to find MAPE \n    '''\n    \n    error = 0\n    \n    for yt, yp in zip(y_true, y_pred):\n        error += np.abs(yt - yp) \/ yt\n        \n    return error \/ len(y_true)","58100943":"def r2(y_true, y_pred):\n    '''\n    r2 \n    '''\n    \n    # \ubaa9\ud45c \uac12\uc758 \ud3c9\uade0\n    mean_true_value = np.mean(y_true)\n    \n    # \ubd84\uc790, \ubd84\ubaa8\ub97c 0 \uc73c\ub85c \ucd08\uae30\ud654 \n    numerator = 0\n    denominator = 0\n    \n    for yt, yp in zip(y_true, y_pred):\n        # \ubd84\uc790 \uc5c5\ub370\uc774\ud2b8\n        numerator += (yt - yp) ** 2\n        # \ubd84\ubaa8 \uc5c5\ub370\uc774\ud2b8\n        denominator += (yt - mean_true_value) ** 2\n        \n    ratio = numerator \/ denominator \n    return 1 - ratio ","f18c5aeb":"y_true = [1,2,3,1,2,3,1,2,3]\ny_pred = [2,1,3,1,2,3,3,1,2]\n\nmetrics.cohen_kappa_score(y_true, y_pred, weights='quadratic'), metrics.accuracy_score(y_true, y_pred)","6539ea06":"def mcc(y_true, y_pred):\n    '''\n    A function of finding mcc in binary classification\n    '''\n    tp = true_positive(y_true, y_pred)\n    tn = true_negative(y_true, y_pred)\n    fp = false_positive(y_true, y_pred)\n    fn = false_negative(y_true, y_pred)\n    \n    numerator = (tp * tn) - (fp * fn)\n    denomirator = ((tp + fp) * (fn + tn) * (fp + tn) * (tp + fn))\n    denomirator = denumerator ** 0.5\n    \n    return numerator \/ denominator","587ba15f":"* 1-FPR is specificity, TNR (True Negative Rate)  \n  \n* The general determination reference point is 0.5, but other reference points may be used, such as 0.4, 0.6.","b2c9e8e4":"* TPR (True Positive Rate) = TP \/ (TP + FN)\n* The same definition as recall","9dbae96c":"### Another regression metric, R^2","ac34b2c1":"* K-fold is available for metrics with the same label as above (of course is rare).","bba814e2":"* Matthew's Correction Coefficient (MCC): A value between -1 and 1 means perfect prediction, -1 means opposite prediction, and 0 means random prediction.\n* Metrics available for biased category distribution  \n* The definition is found in the function below.  ","4c50932b":"Map reference site : https:\/\/danthetech.netlify.app\/DataScience\/evaluation-metrics-for-recommendation-system","8140a091":"### Understanding RMSLE\n* RMSLE Stands for Root Mean Squared Logarithmic Error. This metric is best to use when targets having exponential growth,   \n  such as population counts, average sales of a commodity over a span of years, etc[Source]  \n\n    - **RMLSE is: log(actual+1)\u2212log(predicted+1)**  \n    \n      ![](https:\/\/imgur.com\/kYlnoV9.png)  \n      ![](https:\/\/cdn-images-1.medium.com\/max\/800\/1*AwltG2QujO3NCwsTNO_BTw.png)","83f03afc":"* Mean absolute error (MAE): the mean of absolute error (|actual value - predicted value|)\n* Mean square error (MSE): mean of square error (|actual value - predicted value|^2) (regression model is widely used)\n* root mean square error (RMSE): MSE is square root (regression model is used a lot)\n* MSLE (mean square logarithmic error): the mean of the square log error.\n* RMLSE (root mean square logarithmic error): square root of MSLE\n\n* mean percentage error (MPE): average of percent error (actual value - predicted value \/ actual value)\n* MAPE (mean absolute percentage error): absolute value of MPE","283d8255":"### Stratified KFold for use in regression.  \n  \n* Divide the target variable into groups.  \n* When the data size is large (10,000 pieces), the data is divided into 10 to 20.  \n* If the size is small, it can be divided into 1+log2(N) units by applying the storage rule.  ","f3d57c85":"* **Stratified k-fold** : If the distribution of target variables is significantly biased (e.g., 90% is 1, 10% 0), k-fold cross-validation is dangerous.  \nA sample with a target variable of 0 may accidentally enter a group, which may cause great confusion in learning (underperformance).  \nIn this case, k-fold (Stratified k-fold) for each layer must be used, and each group consists of the same target variable distribution as the original data.  \n  \n=> Unless you arbitrarily create the same number of labels, such as digit (e.g., 0 ~ 9 numeric separation), most of the problems must be distinguished by Stratified k-fold.","12e46498":"#### Memorizing easily\n\n* TP (True Positive): Positive prediction and correct (prediction correct)\n* TN (True Negative) : Negative prediction and correct (Correct prediction)\n* False Positive (FP): Positive prediction and incorrect (prediction)\n* False Negative (FN): Negative and incorrect (predicted incorrectly)","f62d9686":"## Cross Validation \n* The step to ensure that the model learns data accurately without overfit.     \n  \n    - Hold-out method: How to learn with data from one part of the model and measure performance with the rest (fold verification (repetitive) learning takes a long time because the data size is very large. Use or time series data)  \n    \n    Find model optimal parameters with verification set and evaluate model performance through test set (accuracy, precision, recall, AUC, f1 score, log loss, P@k, AP@k, MAP@k, etc..)    \n      \n    ![](https:\/\/img1.daumcdn.net\/thumb\/R1280x0\/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FR3OZR%2FbtqCLMql9Xi%2FFxunJuaRdrWmhJZipT0De0%2Fimg.png)\n    \n    - k-fold : Divide the dataset into k pieces and use only one subset for testing, and use the remaining k-1 for learning, which is repeated k times.   \n\n    Average performance indicators repeated k times to evaluate model performance.  \n      \n    ![](https:\/\/blog.kakaocdn.net\/dn\/bkdnwQ\/btqCIDVFH14\/VCKjIli1V1KWCb90VKtzBK\/img.png)\n    - stratified k-fold : Explain later\n    - Leave-one-out (LOO) : If k is equal to the number of samples in k-fold, all groups use only one sample for verification and learn with all data except it. \n    \n    (Since the calculation time is getting longer, we only need to use small data)\n    - Group k-fold\n    \n* **All cross-validation methods except Stratified k-fold validation are also available for regression**","b2c9087e":"* Visualization confirms: The highest train intensity was obtained at about max_depth 14, but the accuracy of the test set was almost unchanged. (overfitting)  \n* Overfit starts at approximately max_step 5 level: train loss decreases (accuracy increases) test loss begins at a point where it does not decrease or rather increases (accuracy stagnates or decreases).  \n* In this case, this model becomes a useless model because the prediction is 0.6 or less accurate in real situations.  \n  \n* For example, if you select customers with this model (label 0, 1) and issue credit cards, credit card companies will not be able to filter out a large number of credit defaulters in advance. => Cost increase (cost increase)  ","02504d78":"* To predict the actual category in binary classification, you need to set the reference point.  \n- Prediction Category = Prediction Probability >= Reference Point  \n\n* ROC curves can be used to select a reference point.  ","10f7f6eb":"* AUC = 1: Perfect model\n* AUC = 0 : A completely wrong model (requires inverting the prediction probability p to 1-p)\n* AUC = 0.5 : Random model.\n* AUC = 0 to 0.5 : A model that is less than random.","41ee8752":"* TPR, recall is sensitivity\n* FPR (False Positive Rate) : FP \/ (TN + FP)","7291378d":"### Since the accuracy is changed by the parameter value (max_step) of clf, how does the accuracy change according to the variation of this value?","2a9064f1":"* **f1 score** is a harmonic mean of precision + recall (perfect f1 has 1 between 0 and 1)\n* f1 = 2 PR \/ (P + R)\n* f1 = 2 TP \/ (2 TP + FP + FN)\n\n    - precision = tp \/ (tp + fp)\n    - recall = tp \/ (tp + fn)\n    - **precision + recall = f1** ","57db2b11":"### Precision and recall can be calculated for each category of multiple categories.  \n* Macro-mean precision: Calculate the precision average after calculating the category precision, respectively.  \n* Micro-mean precision: Calculate precision after summing TP and FP across all categories.  \n* Weighted average precision: After calculating the precision of each category, the weighted average of precision is calculated with the weight of the number of samples corresponding to each category.  ","0b362357":"## Metric","58f5ed6d":"### Remember that uncertain predictions (around 50% probability values) increase log loss.","70cb68be":"## multi label classification : Multiple labels \n* precision at k (P@k)\n* average precision at k (AP@k)\n* MAP@k\n* logloss","86489bdc":"* Measure how well the predicted value describes the actual value.  \n* If it's close to 1, explain it well. If it's close to 0, it means it's not.  \n* R2 becomes negative when the predicted value has an opposite correlation with the actual value.  \n\n* definition\n- Understand with the code below  ","049947d2":"### Reference : np.clip ","ccae3c16":"## Advanced metric.\n\n* QWK (quadratic weighted kappa): Measure the agreement between the two ratings  \n* The actual value of the rating is 0 to N, and the predicted value is also in the same range.  \n* Since the target variable, GPA, is a categorical variable, multiple classification problem.  \n* Consent is defined as how close the actual and predicted ratings are, and if it is high, it is 1 and if it is low, it is close to 0.  ","e6a1020d":"### Choosing the right metric for a given machine learning model is the first start of model performance improvement.","9e62c59a":"* Log loss is an average value of log loss for individual samples, and a large penalty is given to a prediction value with a large error.\n     - **log loss = -(y x log(p) + (1-y) x log(1-p))**","bfacb9e3":"## Metrics in regression learning"}}