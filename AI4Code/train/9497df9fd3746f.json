{"cell_type":{"3de753e6":"code","999fc2e6":"code","4ea8c9cf":"code","c4737a75":"code","d7e57f0f":"code","2dd00012":"markdown","09f3d695":"markdown","28e38510":"markdown"},"source":{"3de753e6":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_iris\nfrom sklearn.decomposition import PCA","999fc2e6":"# Loading dataset...\niris = load_iris()\n# Dimensions matrix\nX = iris.data\n# Labels\ny = iris.target\ntarget_names = iris.target_names\n\n# Principal Components Analysis\n# Using PCA to switch from 4 dim to 2 dim\npca = PCA(n_components = 2)\n# Transform X to a new space (2-dim)\nX_r = pca.fit(X).transform(X)","4ea8c9cf":"# Shape of X_r\nX_r.shape","c4737a75":"# Percentage of variance explained by each of the selected components.\nprint(\n    \"Percentage of variance explained by each of the selected components : {}\".format(\n        pca.explained_variance_ratio_\n        )\n    )","d7e57f0f":"colors = ['red', 'blue', 'green']\n\nplt.figure()\nfor color, i, target_name, in zip(colors, [0, 1, 2], target_names):\n  plt.scatter(\n      X_r[y == i, 0], X_r[y == i, 1],\n      color = color,\n      alpha = 0.8, lw = 2,\n      label = target_name\n      )\nplt.legend(loc = 'best', shadow = False, scatterpoints = 1)\nplt.title('Iris en 2D')","2dd00012":"Thus, PCA has been useful in reducing the dimensionality of datasets without losing many information","09f3d695":"The first principal component contains 92% of the variance, and the second represents 5% of the variance. Therefore, this means that only two characteristics are sufficient to explain 97% of the variance in the dataset!","28e38510":"Hi! For this application, we will use PCA (Principal Components Analysis) to reduce the dimensions of a dataset (iris dataset) so that it can be easily viewed. If you are not yet working on the classification of the iris, I suggest [this](https:\/\/www.kaggle.com\/raisssaid\/classification-logistic-lda-qda-svm-knn-and-dtree) notebook."}}