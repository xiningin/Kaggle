{"cell_type":{"edad7eaf":"code","10fb58a7":"code","5b4a500a":"code","bffe0490":"code","f1a8b12c":"code","17196a91":"code","cd64e9de":"code","da1ab4de":"code","6bad7129":"code","984d1ad6":"markdown","6721622e":"markdown","c3942a13":"markdown","8a016aef":"markdown","327119d5":"markdown","e1ebe6bf":"markdown","d426aa2e":"markdown","cdac6840":"markdown","8fd188ab":"markdown","548c81d0":"markdown","1cb50124":"markdown","f08ab76f":"markdown","70a36c88":"markdown"},"source":{"edad7eaf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n# install below 2 package as they are not installed by default\n!pip install vaderSentiment\n!pip install find_job_titles\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\n\n# Any results you write to the current directory are saved as output.\n\nstrBulletinsPath = '..\/input\/cityofla\/CityofLA\/Job Bulletins\/'\n#strCSVPath=\"C:\/Work\/Bulletins.csv\"\narrFileList  = os.listdir(strBulletinsPath)\nstrAlterReq = \"REQUIREMENTS\/MINIMUM QUALIFICATION\"\ndicSectionCounter = dict() #Count each section's occurance\n\ndef get_second_part(strLine):\n    strPart = strLine.split(':')\n    if len(strPart) > 1:\n        return strPart[1].replace(' ','')\n    else:\n        return 0\n\narrCSV = []\nstrNextKeyWord = ''\narrKeyWord = ['ANNUAL SALARY',\n              'DUTIES',\n              'REQUIREMENT\/MINIMUM QUALIFICATION',\n              'REQUIREMENTS',\n              'PROCESS NOTE',\n              'WHERE TO APPLY',\n              'NOTE',\n              'APPLICATION DEADLINE',\n              'SELECTION PROCESS',\n              'QUALIFICATIONS REVIEW',\n              'NOTICE'\n              ]\n\nfor objFile in arrFileList:\n    blnHeader = False #In case some files got first few lines empty\n    blnFlag = False #For same file, once checked header empty lines, the rest empty lines in the middle of the file will not be processed specially\n    blnClassCd = False #One file has many class code, only first one required\n    strPath = os.path.join(strBulletinsPath, objFile)\n    fileHandle = open(strPath, encoding = \"ISO-8859-1\")\n\n    objItem = { 'FILE_NAME': objFile }\n    for kw in arrKeyWord:\n        objItem[kw] = ''  #Initial value blank\n        \n    for i, strLine in enumerate(fileHandle.readlines()):\n        if not blnHeader and not blnFlag and strLine.strip() != \"\":\n            blnHeader = True\n            blnFlag = True\n        if i == 0 or blnHeader:\n            # JOB_CLASS_TITLE\n            objItem['JOB_CLASS_TITLE'] = strLine\n            blnHeader = False\n            continue\n        if 'Class Code' in strLine and not blnClassCd:\n             # JOB_CLASS_CODE\n            objItem['JOB_CLASS_CODE'] = get_second_part(strLine)\n            blnClassCd = True\n            continue\n        if 'Open Date' in strLine or 'Open date' in strLine:\n            objItem['OPEN_DATE'] = get_second_part(strLine)\n            continue\n        blnNewKeyword = False\n        for kw in arrKeyWord:\n            if kw in strLine:\n                strNextKeyWord = kw\n                blnNewKeyword = True\n                if kw != 'NOTE':\n                    if len(dicSectionCounter) == 0:\n                        dicSectionCounter[kw] = 1\n                    else:\n                        if kw in dicSectionCounter:\n                            dicSectionCounter[kw] = dicSectionCounter[kw]+1\n                        else:\n                            dicSectionCounter[kw] = 1\n                break\n            #in case in CSV, the title is not \"REQUIREMENT\/MINIMUM QUALIFICATIONS\"    but \"REQUIREMENTS\/MINIMUM QUALIFICATION\"\n            if not blnNewKeyword:\n                if strAlterReq in strLine:\n                    strNextKeyWord = \"REQUIREMENT\/MINIMUM QUALIFICATION\"               \n                    blnNewKeyword = True\n                    break\n        if blnNewKeyword:\n            continue\n        if strNextKeyWord == '':\n            continue\n        objItem[strNextKeyWord] += strLine\n    arrCSV.append(objItem)\n    blnHeader = False\n    blnFlag = False\n    blnClassCd = False\n                \ndf = pd.DataFrame(arrCSV)\n\ndef clean_line(objRow):\n    for col in df.columns:\n        objRow[col] = str(objRow[col]).replace('\\n','').replace('\\n52','')\n    return objRow\n\ndf = df.apply(clean_line, axis=1)\n#Here can generate CSV file.\n#df.to_csv(strCSVPath)\ndf.head()","10fb58a7":"import textblob\nimport pandas as pd\nimport numpy as np\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\nimport nltk\nimport matplotlib.pyplot as plt\nfrom nltk.tokenize import word_tokenize, RegexpTokenizer\nfrom wordcloud import WordCloud\nfrom collections import Counter\n\nanalyzer = SentimentIntensityAnalyzer()\n\n#Read CSV\n#strCSVPath=\"C:\/Work\/Bulletins.csv\"\n#df = pd.read_csv(strCSVPath)\ndf1 = df\n# Below fields are which we need to analyze\narrCols = ['DUTIES', 'NOTE', 'NOTICE', 'PROCESS NOTE', 'QUALIFICATIONS REVIEW', 'REQUIREMENT\/MINIMUM QUALIFICATION', 'REQUIREMENTS', 'SELECTION PROCESS', 'WHERE TO APPLY']\n\narrNegative=[]\narrItem=[]\narrKeyWord = ['FILE_NAME', \n              'ITEM',\n              'TEXT',\n              'POLARITY',\n              'SUBJECTIVITY'\n              ]\n\ndef get_sentiment(row):\n    row['polarity'] = 0\n    row['subjectivity'] = 0\n\n    for col in arrCols:\n        row[col] = str(row[col]).replace('.','. ').replace('\/',' \/ ') \n        polarity_col = col + '_Polarity'\n        subjectivity_col = col + '_Subjectivity'\n        #get each section's polarity and subjectivity value\n        blob = textblob.TextBlob(row[col])\n        row[polarity_col] = blob.sentiment.polarity\n        row[subjectivity_col] = blob.sentiment.subjectivity\n        if row[polarity_col] < 0:\n            objItem = dict()\n            for kw in arrKeyWord:\n                objItem[kw] = ''  #Initial value blank        \n            objItem['FILE_NAME'] = row['FILE_NAME']\n            objItem['ITEM'] = col\n            objItem['TEXT'] = row[col]\n            objItem['POLARITY'] = row[polarity_col]\n            objItem['SUBJECTIVITY'] = row[subjectivity_col]\n            arrNegative.append(objItem)\n            arrItem.append(col)\n\n        row['polarity'] = row['polarity'] + blob.sentiment.polarity\n        row['subjectivity'] = row['subjectivity'] + blob.sentiment.subjectivity\n    return row\n\ndf1 = df1.apply(get_sentiment, axis=1)\nNeg = pd.DataFrame(arrNegative)\nNeg.head()\n#Neg.to_csv('C:\/Work\/Negative.csv')","5b4a500a":"# lets see the histograms for a starter\n#%matplotlib notebook\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n#plt.axis(\"on\")\nfrom scipy import stats\n\nfor col in arrCols:\n    #Here we only show the graphs for \"SELECTION PROCESS\" as one example.\n    if col == \"SELECTION PROCESS\":\n        sns.set(color_codes=True)\n        fig, ax = plt.subplots(nrows=1, ncols=2)\n        sns.distplot(tuple(df1[col + '_Polarity']), ax=ax[0])\n        sns.distplot(tuple(df1[col + '_Subjectivity']), ax=ax[1])\n\n        g = sns.jointplot(tuple(df1[col + '_Polarity']), tuple(df1[col + '_Subjectivity']), kind=\"scatter\", height=7, space=0)\n        plt.show()","bffe0490":"#This is the bar chart\narrSection = []\ncnt = Counter(arrItem)\nfor key, value in cnt.items():\n    dicTemp = dict()\n    dicTemp['Section'] = key\n    dicTemp['Counter'] = value\n    arrSection.append(dicTemp)\n\nBar = pd.DataFrame(arrSection)\nplt.figure(figsize=(7,5))\nsns.barplot(x='Counter',y='Section',palette='rocket',data=Bar) \nplt.title('Negative Sections')\nplt.xlabel(\"Count\")\nplt.ylabel('Section')\nplt.gcf().subplots_adjust(left=0.3)","f1a8b12c":"neg_word_list=[]\ndef get_word_sentiment(text):\n    tokenized_text = nltk.word_tokenize(text)\n    return tokenized_text\n    \n\ndicFreq = dict()\n\n#use anoteher package SentimentIntensityAnalyzer to get the most negative words\nfor item in arrNegative:\n    if item['ITEM'] == 'REQUIREMENTS' or item['ITEM'] == 'REQUIREMENT\/MINIMUM QUALIFICATION':\n        for word in get_word_sentiment(item['TEXT']):\n            if (analyzer.polarity_scores(word)['compound']) <= -0.1:\n                if len(dicFreq) == 0:\n                    dicFreq[word]=1         \n                else:\n                    if word in dicFreq:\n                        dicFreq[word] = dicFreq[word]+1\n                    else:\n                        dicFreq[word]=1\n#use word cloud to draw the word board\nwc = WordCloud(relative_scaling=1, background_color='white',\n        max_words=50)\n\nwordcloud = wc.generate_from_frequencies(dicFreq)\nfig = plt.figure(1, figsize=(12, 12))\n#fig.set_size_inches(18.5, 10.5)\nplt.figure()\nplt.axis('off')\n#fig.suptitle('Most negative Words in Selection Process', fontsize=20)\nfig.subplots_adjust(top=2.3)\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.show()","17196a91":"neg_word_list=[]\ndicFreq = dict()\n\n#use anoteher package SentimentIntensityAnalyzer to get the most negative words\nfor item in arrNegative:\n    for word in get_word_sentiment(item['TEXT']):\n        if (analyzer.polarity_scores(word)['compound']) <= -0.1:\n            if len(dicFreq) == 0:\n                dicFreq[word]=1         \n            else:\n                if word in dicFreq:\n                    dicFreq[word] = dicFreq[word]+1\n                else:\n                    dicFreq[word]=1\n\n#use word cloud to draw the word board\nwc = WordCloud(relative_scaling=1, background_color='white',\n        max_words=50)\n\nwordcloud = wc.generate_from_frequencies(dicFreq)\nfig = plt.figure(1, figsize=(12, 12))\n#fig.set_size_inches(18.5, 10.5)\nplt.figure()\nplt.axis('off')\n#fig.suptitle('Most negative Words', fontsize=20)\nfig.subplots_adjust(top=2.3)\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.show()","cd64e9de":"arrSecCnt = []\nfor key, value in dicSectionCounter.items():\n    dicTemp = dict()\n    dicTemp['Section'] = key\n    dicTemp['Counter'] = value\n    arrSecCnt.append(dicTemp)\n\nBar = pd.DataFrame(arrSecCnt)\nBar.head()\nplt.figure(figsize=(7,5))\nplt.gcf().subplots_adjust(left=0.3)\nsns.barplot(x='Counter',y='Section',palette='vlag',data=Bar) \nplt.title('Sections Counter')\nplt.xlabel(\"Count\")\nplt.ylabel('Section')","da1ab4de":"!pip install geotext\nfrom geotext import GeoText\n\ndicCity = dict()\n# use GeoText to get city name in requirement section\nfor item in arrCSV:\n    places = GeoText(item['REQUIREMENT\/MINIMUM QUALIFICATION'] + item['REQUIREMENTS'])\n    lstCity = places.cities\n    if len(lstCity) > 0:\n        for city in lstCity:\n            if len(dicCity) == 0:\n                dicCity[city]=1         \n            else:\n                if city in dicCity:\n                    dicCity[city] = dicCity[city]+1\n                else:\n                    dicCity[city]=1\n\nprint('City mentioned times')\nprint('------------------------------------')\nfor key, value in dicCity.items():\n    print(key + '    '+str(value)+'\\n')\nprint('Example:' + '\\n')\nfor item in arrCSV:\n    places = GeoText(item['REQUIREMENT\/MINIMUM QUALIFICATION'])\n    lstCity = places.cities\n    if 'Los Angeles' in lstCity:\n        print(item['REQUIREMENT\/MINIMUM QUALIFICATION'])\n        break","6bad7129":"import pandas as pd\nfrom find_job_titles import FinderAcora\nfrom graphviz import Digraph\n\n#Read CSV\n#strCSVPath=\"C:\/Work\/Bulletins.csv\"\ndot = Digraph(comment='Promotions')\n\nfinder=FinderAcora()\nstrReq = \"REQUIREMENT\/MINIMUM QUALIFICATION\"\narrRelation=[]\narrFinal=[]\n\ndef get_promotion(row):\n    strLine = str(row [strReq]) #only check Requirement\/minimum qualification section to get the promotion path\n    # usually the promotion sentences start with \"as a\" and finish with \"with\"\n    if strLine.find(\"as a\") > 0:\n        objItem = dict() \n        # only one previous position required\n        #if strLine.find(\"or\") < 0:\n        strTemp = strLine[strLine.find(\"as a\"):strLine.find(\"with\")-1]\n        strTemp = strTemp.replace(\"as an\", \"as a\").replace(\"as a \",\"\")\n        if strTemp != \"\":\n            for m in finder.findall(strTemp):\n                objItem['Job1'] = row[\"JOB_CLASS_TITLE\"].lstrip().rstrip().upper()\n                objItem['Job2'] = m[2].lstrip().rstrip().upper()\n                arrRelation.append(objItem)\n    return row\n    \ndf = df.apply(get_promotion, axis=1)\n\npro = pd.DataFrame(arrRelation)\n#pro.head()\n#pro.to_csv('C:\/Work\/Promotion.csv')\npro = pro.drop_duplicates()\n#Job1 is higher role and Job1 is the role which could promote to Job1\nfor index, row in pro.iterrows():\n# as the image is too large, here just take police related jobs as an example\n    if row[\"Job2\"].startswith(\"POLICE\"):\n        dot.edge(row[\"Job2\"], row[\"Job1\"], label='Promotion')\ndot","984d1ad6":"Every text file has many sections, but seems the format is not strictly followed. Below chart shows the counter of the occurance of each section. \nWe can see some sections are missing in lots of job bulletin files. Develop a format and keep all the files aligned would improve the quality of the job bulletins.","6721622e":"2.  Analyze negetive contents in job bulletins\n\nNow we will get all the negative sections in each file and put them together to a dataframe.\nThis dataframe can be exported to csv as well, all the sections in each job file having poloarity less than 0 are exported.","c3942a13":"Now let's draw two charts per sections of the bulletin text file, and identify the Polarity and Subjectivity in each section, which will help us to understand which section contains the most negative contents.\nFrom conclusion we can see requirements and qulification requirements parts contain most nagative contents. It is not surprising as these two parts describe all the requirements of a job.\nHere we only show the graphs for \"SELECTION PROCESS\" as one example.","8a016aef":"3. Diversity and Quality Improvement\n\nIn each file's bottom, we can find a sentence \"The City of Los Angeles does not discriminate on the basis of race, religion, national origin, sex, age, marital status, sexual orientation, gender identity, gender expression, disability, creed, color, ancestry, medical condition (cancer), or Acquired Immune Deficiency Syndrome.\". Generally the job bulletins encourage to have more diverse candidates.","327119d5":"Use wordcloud to check the most negative words in \"REQUIREMENTS\" and \"REQUIREMENT\/MINIMUM QUALIFICATION\" section. They are mainly about the jobs tasks to deal with complains, criminial daily.","e1ebe6bf":"Below is the city name mentioned times in requirements and requirement\/minimum qualification sections. Ignore the interference words, we can see Los Angeles been mentioned **430** times.\nAs totally 683 text files as data source, most of the jobs are looking for condidates having Los Angeles local work experence. To widen condidate pool and find better resources, also improve diversity, we should consider more candidates who don't have local experience.","d426aa2e":"1.  Generating CSV file from bulletin text files.","cdac6840":"* I would like to make my special thanks to Dr. Fadi who was very supportive and helpful as well as my classmate Bhimasen who gave me precious advice during this competition.","8fd188ab":"This chart shows the amount of all the sections having negative contents. It is selection process.","548c81d0":"Then by using the same method, we get the most frequent negative words used in all bulletins. Here instead of using textblob, we use vaderSentiment to get the most negative 50 words. Most of them are coming from \"SELECTION PROCESS\". \nMaybe it's good to only mention what kind of skills will be selected, not lack of some skills will be disqualified. This can be applied in Diversity and Quality section.","1cb50124":"Below code use Python to export CSV reading from text bulletins files. Then draw charts and graphs to analyze the negtive sections and keywords. Then analyze the second question - diversity and quality. Finally draw a chart to show the promotion path of all the roles.","f08ab76f":"4.   Promotion Path\nDraw a chart to show the promotion path. \nAs the image is too large, here just take police related jobs as an example","70a36c88":"Conclusion for improvements: \n(1) Design unified format for Job bulletin\n(2) According to Sentiment Analysis, improve the job description\n(3) Only mention what kind of skills will be selected, not lack of some skills will be disqualified. \n(4) As they already did in job bulletins, keeping the following sentence in job bulletin is also a good choice.\n \"The City of Los Angeles does not discriminate on the basis of race, religion, national origin, sex, age, marital status, sexual orientation, gender identity, gender expression, disability, creed, color, ancestry, medical condition (cancer), or Acquired Immune Deficiency Syndrome.\"\n(5) Working experience out of Los Angeles should be accepted.\n"}}