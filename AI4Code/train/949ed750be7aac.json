{"cell_type":{"ac19c334":"code","8e76737d":"code","51b6b355":"code","bf7601c7":"code","7a12f900":"code","bc866f5f":"code","78e606c6":"code","1f675102":"code","3911b503":"code","7a738fed":"code","77dea41d":"code","3e45fac3":"code","c72efdea":"code","a56c73df":"code","704567ad":"code","8c3579a7":"code","656c23a6":"code","3bbd08f3":"code","bd3c37f1":"code","071c96bb":"code","e166852e":"code","435cf47e":"code","1472f0d9":"code","40de9811":"code","92d5af8e":"code","26f75d9c":"code","d98173bb":"code","3ae81564":"code","136fba24":"code","8469f0d7":"code","6cd7870e":"code","9e8056ca":"code","f855265c":"code","1725dd48":"code","b16515ab":"code","6c12833b":"code","51ebbf73":"code","88567ee3":"markdown","37790ca2":"markdown","5f9150de":"markdown","388f2c76":"markdown","2f1180ed":"markdown","20250b2b":"markdown","41dc48c4":"markdown","8fe99af9":"markdown","439d0c8b":"markdown","df9c4866":"markdown","76cc79c0":"markdown","9bd5aafc":"markdown","3cd438dc":"markdown","ac11e9b8":"markdown","5bb772c6":"markdown","ee67ac2f":"markdown","06d26c99":"markdown","f80d8121":"markdown","7ea42a74":"markdown","cf94d045":"markdown","d9cc2acc":"markdown","5c22e682":"markdown","262ad60b":"markdown","e4333b01":"markdown"},"source":{"ac19c334":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","8e76737d":"#Load the data\ndf = pd.read_csv(\"\/kaggle\/input\/telecom-churn\/telecom_churn.csv\")\ndf.sample(5)","51b6b355":"#check missing values\ndf.info()","bf7601c7":"import matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport math","7a12f900":"#default churn rate\nlabels = 'Churn', \"Stay\"\nsizes = [df.Churn[df['Churn'] == 1].count(), df.Churn[df['Churn'] == 0].count()]\nexplode = (0.1, 0)\n\nfig1, ax1 = plt.subplots(figsize=(8, 6))\nax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%', shadow=True)\nax1.axis('equal')\n\nplt.title(\"Proportion of customer churned and retained\")\n\nplt.show()","bc866f5f":"from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\nfrom sklearn.model_selection import cross_val_score\n\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\n\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom sklearn.metrics import precision_recall_curve, auc, roc_curve","78e606c6":"#dat preperation\nlabel = df[\"Churn\"]\ndf_train1 = df.iloc[:, 1:].copy()\nfeature_names = list(df_train1.columns.values)\n\n#I seperate the data into train, valiation and test. We will reserve the test set till the end to test the performance of the best model.\n\n#set, testset\nX_trainval, X_test, y_trainval, y_test = train_test_split(df_train1, label, test_size = 0.2, random_state=1)\n#train, validation set split\nX_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, test_size = 0.2, random_state=1)","1f675102":"#dummy model\ndummy1 = DummyClassifier(random_state=1).fit(X_train, y_train)\npred_dummy1 = dummy1.predict(X_val)\nprint(\"randomly guessing score: {:.2f}\".format(dummy1.score(X_val,y_val)))\ndummy2 = DummyClassifier(strategy = 'most_frequent').fit(X_train, y_train)\npred_dummy2 = dummy2.predict(X_val)\nprint(\"guess all customers will stay score: {:.2f}\".format(dummy2.score(X_val,y_val)))","3911b503":"#tree model\ntree = DecisionTreeClassifier(random_state=1).fit(X_train, y_train)\nprint(\"Decision Tree validation score: {:.2f}\".format(tree.score(X_val, y_val)))\ntree_crossval = cross_val_score(tree, X_trainval, y_trainval)\nprint(\"Decision Tree cross-validation score: {:.2f}\".format(tree_crossval.mean()))\n\n#tree model with simple tune\ntree_tune = DecisionTreeClassifier(random_state=1, max_depth = 6).fit(X_train, y_train)\n#I picked 6 by just trying out different numbers. We can also use GridSearchCV to select the best parameters but I am a bit lazy to do so. \nprint(\"Decision Tree validation score after tune: {:.2f}\".format(tree_tune.score(X_val, y_val)))\npred_tree = tree.predict(X_val)\ntree_crossval_tune = cross_val_score(tree_tune, X_trainval, y_trainval)\nprint(\"Decision Tree cross-validation score (tune): {:.2f}\".format(tree_crossval_tune.mean()))","7a738fed":"print(\"With original dataset\")\n#tree model with simple tune\ntree_tune = DecisionTreeClassifier(random_state=1, max_depth = 6).fit(X_train, y_train)\n#I picked 6 by just trying out different numbers. We can also use GridSearchCV to select the best parameters but I am a bit lazy to do so. \nprint(\"Decision Tree (max_depth 6) validation score: {:.2f}\".format(tree_tune.score(X_val, y_val)))\npred_tree = tree.predict(X_val)\ntree_crossval_tune = cross_val_score(tree_tune, X_trainval, y_trainval)\nprint(\"Decision Tree (max_depth 6) cross-validation score max_depth 6: {:.2f}\".format(tree_crossval_tune.mean()))\n\n#random forest model\nforest = RandomForestClassifier(random_state=1, max_depth=8).fit(X_train, y_train)\npred_forest = forest.predict(X_val)\nprint(\"\\nRandom Forest (max_depth 8) validation score: {:.2f}\".format(forest.score(X_val, y_val)))\nforest_crossval = cross_val_score(forest, X_trainval, y_trainval)\nprint(\"Random Forest (max_depth 8) cross-validation score: {:.2f}\".format(forest_crossval.mean()))\n\n#Gradient Boosting model\ngradient = GradientBoostingClassifier(random_state=1).fit(X_train, y_train)\npred_gradient = gradient.predict(X_val)\n#print(\"Gradient Boosting train score: {:.2f}\".format(gradient.score(X_train, y_train)))\nprint(\"\\nGradient Boosting validation score: {:.2f}\".format(gradient.score(X_val, y_val)))\ngradient_crossval = cross_val_score(gradient, X_trainval, y_trainval)\nprint(\"Grandient Boosting cross-validation score: {:.2f}\".format(gradient_crossval.mean()))\n\n#XG bossting model\nxg = XGBClassifier().fit(X_train, y_train)\npred_xg = xg.predict(X_val)\n#print(\"XGBoost train score: {:.2f}\".format(xg.score(X_train, y_train)))\nprint(\"\\nXGBoost validation score: {:.2f}\".format(xg.score(X_val, y_val)))\nxg_crossval = cross_val_score(xg, X_trainval, y_trainval)\nprint(\"XGBoost cross-validation score: {:.2f}\".format(xg_crossval.mean()))","77dea41d":"print(\"With original dataset\")\n#confusion matrix for dummy\nconfusion_dummy = confusion_matrix(y_val, pred_dummy2)\nprint(\"confusion matrix for dummy model:\\n{}\".format(confusion_dummy))\n\n#confusion matrix for trees \nconfusion_tree=confusion_matrix(y_val, pred_tree)\nprint(\"confusion matrix for decision tree (max_depth 6):\\n{}\".format(confusion_tree))\n\nconfusion_forest=confusion_matrix(y_val, pred_forest)\nprint(\"confusion matrix for random forest (max_depth 8):\\n{}\".format(confusion_forest))\n\nconfusion_gradient=confusion_matrix(y_val, pred_gradient)\nprint(\"confusion matrix for gradient boost:\\n{}\".format(confusion_gradient))\n\nconfusion_xg=confusion_matrix(y_val, pred_xg)\nprint(\"confusion matrix for xg boosting:\\n{}\".format(confusion_xg))","3e45fac3":"#classification_report\n#dummy\nprint(\"dummy classificatoin report\\n\")\nprint(classification_report(y_val, pred_dummy2, target_names = [\"Stay\", \"Churn\"]))\nprint(\"\\nrandom forest classification report\\n\")\nprint(classification_report(y_val, pred_forest, target_names = [\"Stay\", \"Churn\"]))","c72efdea":"from sklearn.utils import resample","a56c73df":"# Separate majority and minority classes\ndf_maj = df[df.Churn==0]\ndf_min = df[df.Churn==1]\n\nprint(\"The minority sample size is: {}\".format(len(df_min))) #483\n\n# Downsample majority class\ndf_maj_ds = resample(df_maj, replace=False,    # sample without replacement\n                             n_samples=483,     # to match minority class\n                             random_state=1) # reproducible results\n# Combine minority class with downsampled majority class\ndf_ds = pd.concat([df_maj_ds, df_min])\n \n# Display new class counts\ndf_ds.Churn.value_counts()\n","704567ad":"#dat preperation\nlabel2 = df_ds[\"Churn\"]\ndf_ds_train = df_ds.iloc[:, 1:].copy()\nfeature_names2 = list(df_ds_train.columns.values)\n#set, testset split\nXds_train, Xds_val, yds_train, yds_val = train_test_split(df_ds_train, label2, test_size = 0.2, random_state=1)","8c3579a7":"print(\"Downsampling\")\n#tree after downsample\ntree_ds = DecisionTreeClassifier(random_state=1, max_depth = 4).fit(Xds_train, yds_train)\nprint(\"Tree (max_depth 4) validation score: {:.2f}\".format(tree_ds.score(X_val, y_val)))\ntree_ds_crossval = cross_val_score(tree_ds, X_val, y_val)\nprint(\"Tree (max_depth 4) cross-validation score after ds: {:.2f}\".format(tree_ds_crossval.mean()))\n\n#random forest model after downsampled\nforest_ds = RandomForestClassifier(random_state=1).fit(Xds_train, yds_train)\nprint(\"\\nRandom Forest validation score: {:.2f}\".format(forest_ds.score(X_val, y_val)))\nforest_ds_crossval = cross_val_score(forest_ds, X_val, y_val)\nprint(\"Random Forest cross-validation score after ds: {:.2f}\".format(forest_ds_crossval.mean()))\n\ngradient_ds = GradientBoostingClassifier(random_state=1).fit(Xds_train, yds_train)\nprint(\"\\nGradient Boosting validation score: {:.2f}\".format(gradient_ds.score(X_val, y_val)))\ngradient_ds_crossval = cross_val_score(gradient_ds, X_val, y_val)\nprint(\"Gradient Boosting cross-validation score after ds: {:.2f}\".format(gradient_ds_crossval.mean()))\n\nxg_ds = XGBClassifier(random_state=1).fit(Xds_train, yds_train)\nprint(\"\\nXG Boosting validation score: {:.2f}\".format(xg_ds.score(X_val, y_val)))\nxg_ds_crossval = cross_val_score(xg_ds, X_val, y_val)\nprint(\"XG Boosting cross-validation score after ds: {:.2f}\".format(xg_ds_crossval.mean()))","656c23a6":"#confusion matrix for random forest downsampled\npred_tree_ds = tree_ds.predict(X_val)\nconfusion_tree_ds=confusion_matrix(y_val, pred_tree_ds)\nprint(\"\\nconfusion matrix for tree(max_depth 4) after downsampling:\\n{}\".format(confusion_tree_ds))\n\npred_forest_ds = forest_ds.predict(X_val)\nconfusion_forest_ds=confusion_matrix(y_val, pred_forest_ds)\nprint(\"\\nconfusion matrix for random forest after downsampling:\\n{}\".format(confusion_forest_ds))\n\npred_gradient_ds = gradient_ds.predict(X_val)\nconfusion_gradient_ds=confusion_matrix(y_val, pred_gradient_ds)\nprint(\"\\nconfusion matrix for gradient boosting after downsampling:\\n{}\".format(confusion_gradient_ds))\n\npred_xg_ds = xg_ds.predict(X_val)\nconfusion_xg_ds=confusion_matrix(y_val, pred_xg_ds)\nprint(\"\\nconfusion matrix for xg boosting after downsampling:\\n{}\".format(confusion_xg_ds))","3bbd08f3":"df_train = X_train.copy()","bd3c37f1":"df_train[\"Churn\"] = y_train","071c96bb":"# Separate majority and minority classes\ndf_maj2 = df_train[df_train.Churn==0]\ndf_min2 = df_train[df_train.Churn==1]\n\nprint(\"The majority sample size is: {}\".format(len(df_maj2))) #1829\n\n\n# Upsample majority class\ndf_min_up = resample(df_min2, replace=True,    # sample without replacement\n                             n_samples=1829,     # to match minority class\n                             random_state=1) # reproducible results\n\n# Combine minority class with downsampled majority class\ndf_up = pd.concat([df_min_up, df_maj2])\n \n# Display new class counts\ndf_up.Churn.value_counts()","e166852e":"Xup_train = df_up.iloc[:, :-1].copy()\n\nyup_train = df_up[\"Churn\"]","435cf47e":"tree_up = DecisionTreeClassifier(random_state=1, max_depth = 4).fit(Xup_train, yup_train)\nprint(\"Tree(max_depth 4) validation score after ups: {:.2f}\".format(tree_up.score(X_val, y_val)))\ntree_up_crossval = cross_val_score(tree_up, X_val, y_val)\nprint(\"Tree(max_depth 4) cross-validation score ups: {:.2f}\".format(tree_up_crossval.mean()))\n\nforest_up = RandomForestClassifier(random_state=1).fit(Xup_train, yup_train)\nprint(\"\\nRandom Forest validation score after ups: {:.2f}\".format(forest_up.score(X_val, y_val)))\nforest_up_crossval = cross_val_score(forest_up, X_val, y_val)\nprint(\"Random Forest cross-validation score after ups: {:.2f}\".format(forest_up_crossval.mean()))\n\ngradient_up = GradientBoostingClassifier(random_state=1).fit(Xup_train, yup_train)\nprint(\"\\nGradient Boosting validation score after ups: {:.2f}\".format(gradient_up.score(X_val, y_val)))\ngradient_up_crossval = cross_val_score(gradient_up, X_val, y_val)\nprint(\"Gradient Boosting cross-validation score: {:.2f}\".format(gradient_up_crossval.mean()))\n\nxg_up = XGBClassifier(random_state=1).fit(Xup_train, yup_train)\nprint(\"\\nXG Boosting validation score after ups: {:.2f}\".format(xg_up.score(X_val, y_val)))\nxg_up_crossval = cross_val_score(xg_up, X_val, y_val)\nprint(\"XG Boosting cross-validation score: {:.2f}\".format(xg_up_crossval.mean()))\n","1472f0d9":"pred_tree_up = tree_up.predict(X_val)\nconfusion_tree_up=confusion_matrix(y_val, pred_tree_up)\nprint(\"\\nconfusion matrix for tree(max_depth 4) after upsamling:\\n{}\".format(confusion_tree_up))\n\npred_forest_up = forest_up.predict(X_val)\nconfusion_forest_up=confusion_matrix(y_val, pred_forest_up)\nprint(\"\\nconfusion matrix for random forest after upsamling:\\n{}\".format(confusion_forest_up))\n\npred_gradient_up = gradient_up.predict(X_val)\nconfusion_gradient_up=confusion_matrix(y_val, pred_gradient_up)\nprint(\"\\nconfusion matrix for gradient after upsamling:\\n{}\".format(confusion_gradient_up))\n\npred_xg_up = xg_up.predict(X_val)\nconfusion_xg_up=confusion_matrix(y_val, pred_xg_up)\nprint(\"\\nconfusion matrix for xg after upsamling:\\n{}\".format(confusion_xg_up))","40de9811":"RF5 = RandomForestClassifier(class_weight={0: 1, 1: 5}, max_depth=4, random_state=1) #change the weight of class 1 to be 5 times bigger\nrf_weighted = RF5.fit(X_train, y_train)\nprint(\"Random Forest(max_depth 4) validation score after weighted 1:5: {:.2f}\".format(rf_weighted.score(X_val, y_val)))\nrf_weighted_crossval = cross_val_score(rf_weighted, X_val, y_val)\nprint(\"Random Forest(max_depth 4) cross-validation score after weighted 1:5: {:.2f}\".format(rf_weighted_crossval.mean()))\n\npred_rf_weighted = rf_weighted.predict(X_val)\nconfusion_rf_weighted=confusion_matrix(y_val, pred_rf_weighted)\nprint(\"\\nconfusion matrix for random forest (max_depth 4) after weighted 1:5:\\n{}\".format(confusion_rf_weighted))\n\nprint(\"\\n classification report for weights 1:5\")\nprint(classification_report(y_val, pred_rf_weighted, target_names = [\"Stay\", \"Churn\"]))","92d5af8e":"RF50 = RandomForestClassifier(class_weight={0: 1, 1: 50}, max_depth=4) #change it to be 50 times\nrf_weighted2 = RF50.fit(X_train, y_train)\n\npred_rf_weighted2 = rf_weighted2.predict(X_val)\nconfusion_rf_weighted2=confusion_matrix(y_val, pred_rf_weighted2)\nprint(\"\\nconfusion matrix for random forest(max_depth 4) after weighted 1:50:\\n{}\".format(confusion_rf_weighted2))\n\nprint(\"\\nclassification report for weights 1:50\")\nprint(classification_report(y_val, pred_rf_weighted2, target_names = [\"Stay\", \"Churn\"]))","26f75d9c":"RFauto = RandomForestClassifier(class_weight='balanced', random_state = 1, max_depth=8)\n\nrf_auto = RFauto.fit(X_train, y_train)\nprint(\"Random Forest(max_depth 8) validation score after balanced: {:.2f}\".format(rf_auto.score(X_val, y_val)))\nrfauto_crossval = cross_val_score(rf_auto, X_val, y_val)\nprint(\"Random Forest(max_depth 8) cross-validation score after balanced: {:.2f}\".format(rfauto_crossval.mean()))\n\ntree_auto = DecisionTreeClassifier(random_state=1, class_weight = 'balanced', max_depth=6).fit(X_train, y_train)\nprint(\"\\nDecision Tree(max_depth 6) validation score after balanced: {:.2f}\".format(tree_auto.score(X_val, y_val)))\ntreeauto_crossval = cross_val_score(tree_auto, X_trainval, y_trainval)\nprint(\"Decision Tree(max_depth 6) cross-validation after balanced: {:.2f}\".format(treeauto_crossval.mean()))","d98173bb":"\npred_rfauto = rf_auto.predict(X_val)\nconfusion_rfauto=confusion_matrix(y_val, pred_rfauto)\nprint(\"\\nconfusion matrix for random forest(max_depth 8) after balanced:\\n{}\".format(confusion_rfauto))\n\n#print(\"\\nclassification report for random forest balanced classes\")\n#print(classification_report(y_val, pred_rfauto, target_names = [\"Stay\", \"Churn\"]))\n\npred_treeauto = tree_auto.predict(X_val)\nconfusion_treeauto=confusion_matrix(y_val, pred_treeauto)\nprint(\"\\nconfusion matrix for tree(max_depth 6) after balanced classes:\\n{}\".format(confusion_treeauto))","3ae81564":"from sklearn.preprocessing import StandardScaler\nfrom imblearn.over_sampling import SMOTE","136fba24":"sm = SMOTE(sampling_strategy='auto', k_neighbors=2, random_state=1)\nXsm_train, ysm_train = sm.fit_resample(X_train, y_train)","8469f0d7":"#tree after smote\ntree_sm = DecisionTreeClassifier(random_state=1, max_depth = 5).fit(Xsm_train, ysm_train)\nprint(\"Tree(max_depth 5) validation score after smote: {:.2f}\".format(tree_up.score(X_val, y_val)))\ntree_sm_crossval = cross_val_score(tree_sm, X_val, y_val)\nprint(\"Tree(max_depth 5) cross-validation score after smote: {:.2f}\".format(tree_up_crossval.mean()))\n\n#random forest model after downsampled\nforest_sm = RandomForestClassifier(random_state=1, max_depth=8).fit(Xsm_train, ysm_train)\nprint(\"\\nRandom Forest(max_depth 8) validation score after smote: {:.2f}\".format(forest_sm.score(X_val, y_val)))\nforest_sm_crossval = cross_val_score(forest_sm, X_val, y_val)\nprint(\"Random Forest(max_depth 8) cross-validation score after smote: {:.2f}\".format(forest_sm_crossval.mean()))\n\ngradient_sm = GradientBoostingClassifier(random_state=1).fit(Xsm_train, ysm_train)\nprint(\"\\nGradient Boosting validation score after smote: {:.2f}\".format(gradient_up.score(X_val, y_val)))\ngradient_sm_crossval = cross_val_score(gradient_sm, X_val, y_val)\nprint(\"Gradient Boosting cross-validation score after smote: {:.2f}\".format(gradient_sm_crossval.mean()))\n\nxg_sm = XGBClassifier(random_state=1).fit(Xsm_train, ysm_train)\nprint(\"\\nXG Boosting validation score after smote: {:.2f}\".format(xg_sm.score(X_val, y_val)))\nxg_sm_crossval = cross_val_score(xg_sm, X_val, y_val)\nprint(\"XG Boosting cross-validation score after smote: {:.2f}\".format(xg_sm_crossval.mean()))","6cd7870e":"print(\"SMOTE\")\npred_tree_sm = tree_sm.predict(X_val)\nconfusion_tree_sm=confusion_matrix(y_val, pred_tree_sm)\nprint(\"\\nconfusion matrix for tree(max_depth 5) :\\n{}\".format(confusion_tree_sm))\n\npred_forest_sm = forest_sm.predict(X_val)\nconfusion_forest_sm=confusion_matrix(y_val, pred_forest_sm)\nprint(\"\\nconfusion matrix for forest(max_depth 8) :\\n{}\".format(confusion_forest_sm))\n\npred_gradient_sm = gradient_sm.predict(X_val)\nconfusion_gradient_sm=confusion_matrix(y_val, pred_gradient_sm)\nprint(\"\\nconfusion matrix for gradient:\\n{}\".format(confusion_gradient_sm))\n\npred_xg_sm = xg_sm.predict(X_val)\nconfusion_xg_sm=confusion_matrix(y_val, pred_xg_sm)\nprint(\"\\nconfusion matrix for xg:\\n{}\".format(confusion_xg_sm))","9e8056ca":"fig= plt.subplots(figsize=(8, 6))\nprecision_gd, recall_gd, thresholds_gd = precision_recall_curve(y_val, gradient_sm.predict_proba(X_val)[:, 1])\nplt.plot(precision_gd, recall_gd, label=\"gd\")\n\nclose_default_gd = np.argmin(np.abs(thresholds_gd - 0.5))\nplt.plot(precision_gd[close_default_gd], recall_gd[close_default_gd], '^', c='k', markersize=10, label='threshold 0.5 rf', fillstyle=\"none\", mew=2)\nplt.xlabel(\"Precision\")\nplt.ylabel(\"Recall\")\nplt.legend(loc=\"best\")","f855265c":"threshold = 0.38 #after different trails, this is the best\n\npredicted_proba = gradient_sm.predict_proba(X_val)\npredicted = (predicted_proba [:,1] >= threshold).astype('int')\n\n#compare the accuracy scores\naccuracy_adj = accuracy_score(y_val, predicted)\nprint(\"accurcy rate with 0.38 threshold {}\".format(str(round(accuracy_adj,4,)*100))+\"%\")\n\naccuracy = accuracy_score(y_val, pred_gradient_sm)\nprint(\"accurcy rate with 0.5 threshold {}\".format(str(round(accuracy,4,)*100))+\"%\")\n\n#confusion matrix compare\nconfusion_gd=confusion_matrix(y_val, predicted)\nprint(\"confusion matrix with new threshold:\\n{}\".format(confusion_gd))\n\npred_gradient_sm = gradient_sm.predict(X_val)\nconfusion_gd_sm=confusion_matrix(y_val, pred_gradient_sm)\nprint(\"\\nconfusion matrix original:\\n{}\".format(confusion_gd_sm))\n\n#classification_report\nprint(\"\\nrandom forest classification report with adjuested threshold\\n\")\nprint(classification_report(y_val, predicted, target_names = [\"Stay\", \"Churn\"]))","1725dd48":"fig= plt.subplots(figsize=(8, 6))\nprecision_gd, recall_gd, thresholds_gd = precision_recall_curve(y_val, gradient_sm.predict_proba(X_val)[:, 1])\nplt.plot(precision_gd, recall_gd, label=\"gd\")\n\nclose_default_gd = np.argmin(np.abs(thresholds_gd - 0.4))\nplt.plot(precision_gd[close_default_gd], recall_gd[close_default_gd], '^', c='k', markersize=10, label='threshold 0.5 rf', fillstyle=\"none\", mew=2)\nplt.xlabel(\"Precision\")\nplt.ylabel(\"Recall\")\nplt.legend(loc=\"best\")","b16515ab":"print(\"Random Forest final test score: {:.2f}\".format(forest_ds.score(X_test, y_test)))\nforesttest_crossval = cross_val_score(forest_ds, df_train1, label)\nprint(\"Random Forest final cross-validation test score: {:.2f}\".format(foresttest_crossval.mean()))\n\ntest_forest = forest_ds.predict(X_test)\nconfusion_foresttest=confusion_matrix(y_test, test_forest)\nprint(\"\\nconfusion matrix:\\n{}\".format(confusion_foresttest))\n\nforesttest_proba = forest_ds.predict_proba(X_test)\n\nforesttest = (foresttest_proba [:,1] >= 0.56).astype('int')\n\nforestaccuracy_test = accuracy_score(y_test, foresttest)\n#test_crossval2 = cross_val_score(test, df_train1, label)\nprint(\"\\naccurcy rate with test data with 0.56 threshold is {}\".format(str(round(forestaccuracy_test,4,)*100))+\"%\")\n#print(\"cross-validation rate with test data is {}\".format(str(round(test_crossval2,4,)*100))+\"%\")\n      \nprint(\"\\nrandom forest classification report with adjuested threshold\\n\")\nprint(classification_report(y_test, foresttest, target_names = [\"Stay\", \"Churn\"]))","6c12833b":"print(\"Random Forest final test score: {:.2f}\".format(forest_sm.score(X_test, y_test)))\nforestsmtest_crossval = cross_val_score(forest_sm, df_train1, label)\nprint(\"Random Forest final cross-validation test score for smote: {:.2f}\".format(forestsmtest_crossval.mean()))\n\ntest_forestsm = forest_sm.predict(X_test)\nconfusion_forestsmtest=confusion_matrix(y_test, test_forestsm)\nprint(\"\\nconfusion matrix:\\n{}\".format(confusion_forestsmtest))\n\nforestsmtest_proba = forest_sm.predict_proba(X_test)\n\nforestsmtest = (forestsmtest_proba [:,1] >= 0.56).astype('int')\n\nforestsmaccuracy_test = accuracy_score(y_test, forestsmtest)\n#test_crossval2 = cross_val_score(test, df_train1, label)\nprint(\"\\naccurcy rate with test data with 0.56 threshold is {}\".format(str(round(forestsmaccuracy_test,4,)*100))+\"%\")\n#print(\"cross-validation rate with test data is {}\".format(str(round(test_crossval2,4,)*100))+\"%\")\n      \nprint(\"\\nrandom forest classification report with adjuested threshold\\n\")\nprint(classification_report(y_test, forestsmtest, target_names = [\"Stay\", \"Churn\"]))","51ebbf73":"print(\"Gradient Boosting final test score: {:.2f}\".format(gradient_sm.score(X_test, y_test)))\ngdtest_crossval = cross_val_score(gradient_sm, df_train1, label)\nprint(\"Gradient Boosting final cross-validation test score: {:.2f}\".format(gdtest_crossval.mean()))\n\ntest_gd = gradient_sm.predict(X_test)\ngdconfusion_test=confusion_matrix(y_test, test_gd)\nprint(\"\\nconfusion matrix:\\n{}\".format(gdconfusion_test))\n\ntest_gd_original = gradient.predict(X_test)\ngdoconfusion_test=confusion_matrix(y_test, test_gd_original)\nprint(\"\\nconfusion matrix without sm:\\n{}\".format(gdoconfusion_test))\n\ngdtest_proba = gradient_sm.predict_proba(X_test)\n\ngdtest = (gdtest_proba [:,1] >= 0.38).astype('int')\n\ngdconfusion_test2=confusion_matrix(y_test, gdtest)\nprint(\"\\nconfusion matrix with 0.38 threshold:\\n{}\".format(gdconfusion_test2))\n\ngdaccuracy_test = accuracy_score(y_test, gdtest)\nprint(\"\\naccurcy rate with test data with 0.38 threshold is {}\".format(str(round(gdaccuracy_test,4,)*100))+\"%\")\n\n      \nprint(\"\\ngradient boost classification report with adjuested threshold\\n\")\nprint(classification_report(y_test, gdtest, target_names = [\"Stay\", \"Churn\"]))\n\ntest_gdo = gradient.predict(X_test)\nprint(\"\\ngradient boost classification report original\\n\")\nprint(classification_report(y_test, test_gdo, target_names = [\"Stay\", \"Churn\"]))","88567ee3":"<a id=\"4.31\"><\/a>\n### 4.3.1 balanced class_weight for RF and Tree","37790ca2":"<a id=\"2\"><\/a>\n# 2. Data Overview","5f9150de":"<a id=\"4.4\"><\/a>\n### 4.4 SMOTE\n","388f2c76":"<a id=\"3\"><\/a>\n# 3. Model fitting with original data","2f1180ed":"<a id=\"5\"><\/a>\n# 5. precision vs. recall curve","20250b2b":"Based on the confusion matrix, the our RF model makes more False Negative mistakes, which means customers will churn but the model says it will stay. It is because we have so many \"Stay\" in the sample that the model is focusing on getting the \"Stay\" class right, not the \"Churn\" class. We will need to rebalance the dataset. ","41dc48c4":"<a id=\"4.3\"><\/a>\n### 4.3 Manually changing the weights for RF","8fe99af9":"### Baseline Churn","439d0c8b":"False positive performance all improved but we also sacrificed for predicting class \"1\" due to the loss of information","df9c4866":"# Table of Content:\n\n[1. Introduction](#1)\n\n[2. Data Overview](#2)\n\n[3. Basic Model Fitting](#3)\n* [3.1 Model evaluation](#3.1) \n\n[4. Models With Resampling](#4)\n* [4.1 Downsample](#4.1)\n* [4.2 Upsample](#4.2)\n* [4.3 Class weight for Random Forest (Manually)](#4.3)\n    * [4.31 Balanced Class for RF and DecisionTree](#4.31)    \n* [4.4 Smote](#4.4)\n\n[5. Precision vs. Recall curve](#5)\n\n[6. Final Test](#6)","76cc79c0":"By keep copying the data in small sample size, it overfit those data point. That's why for testing, the False Positive rate is still high","9bd5aafc":"<a id=\"6\"><\/a>\n# 6. final test","3cd438dc":"<a id=\"4\"><\/a>\n# 4. Models With Resamplings","ac11e9b8":"### Different Tree models","5bb772c6":"<a id=\"3.1\"><\/a>\n# 3.1 Basic model Evaluation","ee67ac2f":"### Baseline Model","06d26c99":"<a id=\"1\"><\/a>\n# 1. Introduction","f80d8121":"From the classification report aboved, the difference between the model's ability to predict \"Stay\" vs. to predict \"Churn is even more obvious. Here are some reminders what \"precision\" and \"recall\" are:\n* Precision: True Positive \/ (True Positive + False Positive). Let's translate to Englis and let's use the example of \"Churn\", which is 0.88. It means: The times the model rightly predicted customers will churn \/ (rightly predict churn + predict churn but the client stay) is 88% \n* Recall: True Positive \/ (True Positive + False Negative). Let's translate to Englis and let's use the example of \"Churn\", which is 0.63. It means: The times the model rightly predicted customers will churn \/ (rightly predict churn + predict stay but the client churn) is 63% ","7ea42a74":"Performance for gradient and xg are very consistent. It's due to how their algorithms were designed. For the other tree methods, you can see the False positive rate improved a lot compare to when we used upsample. ","cf94d045":"- No missing Data - Great!\n- No Categorical variables - Great!\n\nAmbigious varaible: Customer Service Call \n1. I wonder if the customer service call is the average numbers of call\/month, or the most recent month's call or the total amount of calls that customer have made. \n2. We don't have any ideas on when the calls were made. 5 recent customers service calls can be an indicator to predict the churn, while 5 customer service calls that were made a year ago will not be an indicator\n","d9cc2acc":"<a id=\"4.1\"><\/a>\n### 4.1 Downsample","5c22e682":"<a id=\"4.2\"><\/a>\n### 4.2 Upsample ","262ad60b":"Looks like we can maintain the same recall rate and improve the overall accuracy with an adjustment of the rate of threshold.","e4333b01":"### Goal: the pros and cons of different techniques for handling inbalanced dataset\n\n### Variables: \n* Churn: 1 if customer cancelled service, 0 if not\n* AccountWeeks: number of weeks customer has had active account\n* ContractRenewal: 1 if customer recently renewed contract, 0 if not\n* DataPlan: 1 if customer has data plan, 0 if not\n* DataUsage: gigabytes of monthly data usage\n* CustServCalls: number of calls into customer service\n* DayMins: average daytime minutes per month\n* DayCalls: average number of daytime calls\n* MonthlyCharge: average monthly bill\n* OverageFee: largest overage fee in last 12 months\n* RoamMins: average number of roaming minutes"}}