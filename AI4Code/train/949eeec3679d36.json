{"cell_type":{"cfdba9f2":"code","9efa63c7":"markdown"},"source":{"cfdba9f2":"import os\nimport time\nimport pickle\nimport random\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom sklearn.metrics import log_loss, roc_auc_score\n\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\nfrom torch.nn import CrossEntropyLoss, MSELoss\nfrom torch.nn.modules.loss import _WeightedLoss\nimport torch.nn.functional as F\n\npd.set_option('display.max_columns', 100)\npd.set_option('display.max_rows', 100)\n\nDATA_PATH = '..\/input\/jane-street-market-prediction\/'\n\n# GPU_NUM = 8\nBATCH_SIZE = 8192# * GPU_NUM\nEPOCHS = 200\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nEARLYSTOP_NUM = 3\nNFOLDS = 5\n\nTRAIN = True\nCACHE_PATH = '.\/'\n\ntrain = pd.read_csv(f'{DATA_PATH}\/train.csv')\n\ndef save_pickle(dic, save_path):\n    with open(save_path, 'wb') as f:\n    # with gzip.open(save_path, 'wb') as f:\n        pickle.dump(dic, f)\n\ndef load_pickle(load_path):\n    with open(load_path, 'rb') as f:\n    # with gzip.open(load_path, 'rb') as f:\n        message_dict = pickle.load(f)\n    return message_dict\n\nclass EarlyStopping:\n    def __init__(self, patience=7, mode=\"max\", delta=0.001):\n        self.patience = patience\n        self.counter = 0\n        self.mode = mode\n        self.best_score = None\n        self.early_stop = False\n        self.delta = delta\n        if self.mode == \"min\":\n            self.val_score = np.Inf\n        else:\n            self.val_score = -np.Inf\n\n    def __call__(self, epoch_score, model, model_path):\n\n        if self.mode == \"min\":\n            score = -1.0 * epoch_score\n        else:\n            score = np.copy(epoch_score)\n\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(epoch_score, model, model_path)\n        elif score < self.best_score: #  + self.delta\n            self.counter += 1\n            # print('EarlyStopping counter: {} out of {}'.format(self.counter, self.patience))\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            # ema.apply_shadow()\n            self.save_checkpoint(epoch_score, model, model_path)\n            # ema.restore()\n            self.counter = 0\n\n    def save_checkpoint(self, epoch_score, model, model_path):\n        if epoch_score not in [-np.inf, np.inf, -np.nan, np.nan]:\n            # print('Validation score improved ({} --> {}). Saving model!'.format(self.val_score, epoch_score))\n            # if not DEBUG:\n            torch.save(model.state_dict(), model_path)\n        self.val_score = epoch_score\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\nseed_everything(seed=42)\n\nfeat_cols = [f'feature_{i}' for i in range(130)]\n\nif TRAIN:\n    train = train.loc[train.date > 85].reset_index(drop=True)\n\n    train['action'] = (train['resp'] > 0).astype('int')\n    train['action_1'] = (train['resp_1'] > 0).astype('int')\n    train['action_2'] = (train['resp_2'] > 0).astype('int')\n    train['action_3'] = (train['resp_3'] > 0).astype('int')\n    train['action_4'] = (train['resp_4'] > 0).astype('int')\n    valid = train.loc[(train.date >= 450) & (train.date < 500)].reset_index(drop=True)\n    train = train.loc[train.date < 450].reset_index(drop=True)\ntarget_cols = ['action', 'action_1', 'action_2', 'action_3', 'action_4']\n\nif TRAIN:\n    df = pd.concat([train[feat_cols], valid[feat_cols]]).reset_index(drop=True)\n    f_mean = df.mean()\n    f_mean = f_mean.values\n    np.save(f'{CACHE_PATH}\/f_mean_online.npy', f_mean)\n\n    train.fillna(df.mean(), inplace=True)\n    valid.fillna(df.mean(), inplace=True)\nelse:\n    f_mean = np.load(f'{CACHE_PATH}\/f_mean_online.npy')\n\n##### Making features\n# https:\/\/www.kaggle.com\/lucasmorin\/running-algos-fe-for-fast-inference\/data\n# eda:https:\/\/www.kaggle.com\/carlmcbrideellis\/jane-street-eda-of-day-0-and-feature-importance\n# his example:https:\/\/www.kaggle.com\/gracewan\/plot-model\ndef fillna_npwhere_njit(array, values):\n    if np.isnan(array.sum()):\n        array = np.where(np.isnan(array), values, array)\n    return array\n\nclass RunningEWMean:\n    def __init__(self, WIN_SIZE=20, n_size=1, lt_mean=None):\n        if lt_mean is not None:\n            self.s = lt_mean\n        else:\n            self.s = np.zeros(n_size)\n        self.past_value = np.zeros(n_size)\n        self.alpha = 2 \/ (WIN_SIZE + 1)\n\n    def clear(self):\n        self.s = 0\n\n    def push(self, x):\n\n        x = fillna_npwhere_njit(x, self.past_value)\n        self.past_value = x\n        self.s = self.alpha * x + (1 - self.alpha) * self.s\n\n    def get_mean(self):\n        return self.s\n\nif TRAIN:\n    all_feat_cols = [col for col in feat_cols]\n\n    train['cross_41_42_43'] = train['feature_41'] + train['feature_42'] + train['feature_43']\n    train['cross_1_2'] = train['feature_1'] \/ (train['feature_2'] + 1e-5)\n    valid['cross_41_42_43'] = valid['feature_41'] + valid['feature_42'] + valid['feature_43']\n    valid['cross_1_2'] = valid['feature_1'] \/ (valid['feature_2'] + 1e-5)\n\n    all_feat_cols.extend(['cross_41_42_43', 'cross_1_2'])\n\n##### Model&Data fnc\nclass SmoothBCEwLogits(_WeightedLoss):\n    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n        super().__init__(weight=weight, reduction=reduction)\n        self.smoothing = smoothing\n        self.weight = weight\n        self.reduction = reduction\n\n    @staticmethod\n    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n        assert 0 <= smoothing < 1\n        with torch.no_grad():\n            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n        return targets\n\n    def forward(self, inputs, targets):\n        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n            self.smoothing)\n        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n\n        if  self.reduction == 'sum':\n            loss = loss.sum()\n        elif  self.reduction == 'mean':\n            loss = loss.mean()\n\n        return loss\n\nclass MarketDataset:\n    def __init__(self, df):\n        self.features = df[all_feat_cols].values\n\n        self.label = df[target_cols].values.reshape(-1, len(target_cols))\n\n    def __len__(self):\n        return len(self.label)\n\n    def __getitem__(self, idx):\n        return {\n            'features': torch.tensor(self.features[idx], dtype=torch.float),\n            'label': torch.tensor(self.label[idx], dtype=torch.float)\n        }\n\n\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.batch_norm0 = nn.BatchNorm1d(len(all_feat_cols))\n        self.dropout0 = nn.Dropout(0.2)\n\n        dropout_rate = 0.2\n        hidden_size = 256\n        self.dense1 = nn.Linear(len(all_feat_cols), hidden_size)\n        self.batch_norm1 = nn.BatchNorm1d(hidden_size)\n        self.dropout1 = nn.Dropout(dropout_rate)\n\n        self.dense2 = nn.Linear(hidden_size+len(all_feat_cols), hidden_size)\n        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n        self.dropout2 = nn.Dropout(dropout_rate)\n\n        self.dense3 = nn.Linear(hidden_size+hidden_size, hidden_size)\n        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n        self.dropout3 = nn.Dropout(dropout_rate)\n\n        self.dense4 = nn.Linear(hidden_size+hidden_size, hidden_size)\n        self.batch_norm4 = nn.BatchNorm1d(hidden_size)\n        self.dropout4 = nn.Dropout(dropout_rate)\n\n        self.dense5 = nn.Linear(hidden_size+hidden_size, len(target_cols))\n\n        self.Relu = nn.ReLU(inplace=True)\n        self.PReLU = nn.PReLU()\n        self.LeakyReLU = nn.LeakyReLU(negative_slope=0.01, inplace=True)\n        # self.GeLU = nn.GELU()\n        self.RReLU = nn.RReLU()\n\n    def forward(self, x):\n        x = self.batch_norm0(x)\n        x = self.dropout0(x)\n\n        x1 = self.dense1(x)\n        x1 = self.batch_norm1(x1)\n        # x = F.relu(x)\n        # x = self.PReLU(x)\n        x1 = self.LeakyReLU(x1)\n        x1 = self.dropout1(x1)\n\n        x = torch.cat([x, x1], 1)\n\n        x2 = self.dense2(x)\n        x2 = self.batch_norm2(x2)\n        # x = F.relu(x)\n        # x = self.PReLU(x)\n        x2 = self.LeakyReLU(x2)\n        x2 = self.dropout2(x2)\n\n        x = torch.cat([x1, x2], 1)\n\n        x3 = self.dense3(x)\n        x3 = self.batch_norm3(x3)\n        # x = F.relu(x)\n        # x = self.PReLU(x)\n        x3 = self.LeakyReLU(x3)\n        x3 = self.dropout3(x3)\n\n        x = torch.cat([x2, x3], 1)\n\n        x4 = self.dense4(x)\n        x4 = self.batch_norm4(x4)\n        # x = F.relu(x)\n        # x = self.PReLU(x)\n        x4 = self.LeakyReLU(x4)\n        x4 = self.dropout4(x4)\n\n        x = torch.cat([x3, x4], 1)\n\n        x = self.dense5(x)\n\n        return x\n\ndef train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n    model.train()\n    final_loss = 0\n\n    for data in dataloader:\n        optimizer.zero_grad()\n        features = data['features'].to(device)\n        label = data['label'].to(device)\n        outputs = model(features)\n        loss = loss_fn(outputs, label)\n        loss.backward()\n        optimizer.step()\n        if scheduler:\n            scheduler.step()\n\n        final_loss += loss.item()\n\n    final_loss \/= len(dataloader)\n\n    return final_loss\n\ndef inference_fn(model, dataloader, device):\n    model.eval()\n    preds = []\n\n    for data in dataloader:\n        features = data['features'].to(device)\n\n        with torch.no_grad():\n            outputs = model(features)\n\n        preds.append(outputs.sigmoid().detach().cpu().numpy())\n\n    preds = np.concatenate(preds).reshape(-1, len(target_cols))\n\n    return preds\n\ndef utility_score_bincount(date, weight, resp, action):\n    count_i = len(np.unique(date))\n    # print('weight: ', weight)\n    # print('resp: ', resp)\n    # print('action: ', action)\n    # print('weight * resp * action: ', weight * resp * action)\n    Pi = np.bincount(date, weight * resp * action)\n    t = np.sum(Pi) \/ np.sqrt(np.sum(Pi ** 2)) * np.sqrt(250 \/ count_i)\n    u = np.clip(t, 0, 6) * np.sum(Pi)\n    return u\n\nif TRAIN:\n    train_set = MarketDataset(train)\n    train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n    valid_set = MarketDataset(valid)\n    valid_loader = DataLoader(valid_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n\n    start_time = time.time()\n    for _fold in range(NFOLDS):\n        print(f'Fold{_fold}:')\n        seed_everything(seed=42+_fold)\n        torch.cuda.empty_cache()\n        device = torch.device(\"cuda:0\")\n        model = Model()\n        model.to(device)\n        # model = nn.DataParallel(model)\n\n        optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n        # optimizer = Nadam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n        # optimizer = Lookahead(optimizer=optimizer, k=10, alpha=0.5)\n        scheduler = None\n        # scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3,\n        #                                                 max_lr=1e-2, epochs=EPOCHS, steps_per_epoch=len(train_loader))\n        # loss_fn = nn.BCEWithLogitsLoss()\n        loss_fn = SmoothBCEwLogits(smoothing=0.005)\n\n        model_weights = f\"{CACHE_PATH}\/online_model{_fold}.pth\"\n        es = EarlyStopping(patience=EARLYSTOP_NUM, mode=\"max\")\n        for epoch in range(EPOCHS):\n            train_loss = train_fn(model, optimizer, scheduler, loss_fn, train_loader, device)\n\n            valid_pred = inference_fn(model, valid_loader, device)\n            valid_auc = roc_auc_score(valid[target_cols].values, valid_pred)\n            valid_logloss = log_loss(valid[target_cols].values, valid_pred)\n            valid_pred = np.median(valid_pred, axis=1)\n            valid_pred = np.where(valid_pred >= 0.5, 1, 0).astype(int)\n            valid_u_score = utility_score_bincount(date=valid.date.values, weight=valid.weight.values,\n                                                   resp=valid.resp.values, action=valid_pred)\n            print(f\"FOLD{_fold} EPOCH:{epoch:3} train_loss={train_loss:.5f} \"\n                      f\"valid_u_score={valid_u_score:.5f} valid_auc={valid_auc:.5f} \"\n                      f\"time: {(time.time() - start_time) \/ 60:.2f}min\")\n            es(valid_auc, model, model_path=model_weights)\n            if es.early_stop:\n                print(\"Early stopping\")\n                break\n        # torch.save(model.state_dict(), model_weights)\n    if True:\n        valid_pred = np.zeros((len(valid), len(target_cols)))\n        for _fold in range(NFOLDS):\n            torch.cuda.empty_cache()\n            device = torch.device(\"cuda:0\")\n            model = Model()\n            model.to(device)\n            model_weights = f\"{CACHE_PATH}\/online_model{_fold}.pth\"\n            model.load_state_dict(torch.load(model_weights))\n\n            valid_pred += inference_fn(model, valid_loader, device) \/ NFOLDS\n        auc_score = roc_auc_score(valid[target_cols].values, valid_pred)\n        logloss_score = log_loss(valid[target_cols].values, valid_pred)\n\n        valid_pred = np.median(valid_pred, axis=1)\n        valid_pred = np.where(valid_pred >= 0.5, 1, 0).astype(int)\n        valid_score = utility_score_bincount(date=valid.date.values, weight=valid.weight.values, resp=valid.resp.values,\n                                             action=valid_pred)\n        print(f'{NFOLDS} models valid score: {valid_score}\\tauc_score: {auc_score:.4f}\\tlogloss_score:{logloss_score:.4f}')","9efa63c7":"**This is the training code of this kernel https:\/\/www.kaggle.com\/a763337092\/pytorch-resnet-starter-inference?scriptVersionId=52736172\nUpvote if it helps!!!**"}}