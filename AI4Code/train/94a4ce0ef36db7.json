{"cell_type":{"d6ad4ab2":"code","f477157b":"code","a2f838f8":"code","e3e74d0b":"code","ec373f62":"code","6769e6f0":"code","5caa6c1c":"code","73951604":"code","08aba263":"code","4c6ce380":"code","b5ea0324":"code","c10bedfd":"code","cf95dd85":"code","62ce62f7":"code","f47076ed":"code","f399d3a0":"code","c821be80":"code","cd711fe9":"code","174a0e4f":"code","3d82d0c6":"code","098eb2eb":"code","15ee1082":"code","c4b95fa1":"code","3124fccf":"code","503da9ca":"code","ccc4cb5a":"code","9e09ae91":"code","fb28618b":"code","c5e4b557":"code","482bf9f7":"code","788b5437":"code","f80209d0":"code","a8aae76b":"code","96c04570":"markdown","2bddd142":"markdown","b90fc8f2":"markdown","1bf44982":"markdown","35dd4b23":"markdown","f249f5cf":"markdown","c131a586":"markdown","32a487e8":"markdown","896cfb93":"markdown","a8aca52b":"markdown","6959d50d":"markdown","ca26d679":"markdown","613174cc":"markdown","35e228d9":"markdown","d9392390":"markdown","0f87c4e6":"markdown","7b37355c":"markdown","63bc639a":"markdown","b9c1072e":"markdown","94c1cf97":"markdown","a8629f70":"markdown"},"source":{"d6ad4ab2":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport time\nimport matplotlib.ticker as ticker\nimport plotly.express as px\nimport warnings\nwarnings.filterwarnings('ignore')","f477157b":"df = pd.read_csv('..\/input\/sample-sales-data\/sales_data_sample.csv', engine='python')\ndf.head()","a2f838f8":"df.info()","e3e74d0b":"print(df.isnull().sum())","ec373f62":"to_drop = ['ADDRESSLINE2','STATE','POSTALCODE','TERRITORY']\ndf = df.drop(to_drop, axis = 1)","6769e6f0":"df['ORDERDATE'] = pd.to_datetime(df['ORDERDATE'])\ndft = pd.DataFrame(df)","5caa6c1c":"df.sort_values(by = ['ORDERDATE'], inplace = True)\ndf.set_index('ORDERDATE', inplace = True)","73951604":"top_customer = df.groupby(['CUSTOMERNAME']).sum().sort_values('SALES', ascending = False).head(20)\ntop_customer = top_customer[['SALES']].round(3)\ntop_customer.reset_index(inplace = True)","08aba263":"plt.figure(figsize = (15,5))\nplt.title('20 Most Valueable Customer (2003 - 2005)', fontsize = 18)\nplt.bar(top_customer['CUSTOMERNAME'], top_customer['SALES'], color = '#37C6AB', edgecolor = 'black', linewidth = 1)\nplt.xlabel('Customer Name', fontsize = 15)\nplt.ylabel('Revenue', fontsize = 15)\nplt.xticks(fontsize = 12, rotation = 90)\nplt.yticks(fontsize = 12)\nfor k, v in top_customer['SALES'].items():\n    if v > 600000:\n        plt.text(k, v-270000, '$' + str(v), fontsize = 12, rotation = 90, color = 'black', ha = 'center')\n    else:\n        plt.text(k, v+ 50000, '$' + str(v), fontsize = 12, rotation = 90, color = 'black', ha = 'center')","4c6ce380":"top_country = df.groupby(['COUNTRY']).sum().sort_values('SALES', ascending = False).head(20)\ntop_country = top_country[['SALES']].round(3)\ntop_country.reset_index(inplace = True)","b5ea0324":"plt.figure(figsize = (15,5))\nplt.title('20 Highest Revenue by Country (2003 - 2005)', fontsize = 18)\nplt.bar(top_country['COUNTRY'], top_country['SALES'], color = '#37C6AB', edgecolor = 'black', linewidth = 1)\nplt.xlabel('Country', fontsize = 15)\nplt.ylabel('Revenue', fontsize = 15)\nplt.xticks(fontsize = 12, rotation = 90)\nplt.yticks(fontsize = 12)\nfor k, v in top_country['SALES'].items():\n    if v > 3000000:\n        plt.text(k, v-1200000, '$' + str(v), fontsize = 12, rotation = 90, color = 'black', ha = 'center')\n    else:\n        plt.text(k, v+100000, '$' + str(v), fontsize = 12, rotation = 90, color = 'black', ha = 'center')","c10bedfd":"top_city = df.groupby(['CITY']).sum().sort_values('SALES', ascending = False).head(20)\ntop_city = top_city[['SALES']].round(3)\ntop_city.reset_index(inplace = True)","cf95dd85":"plt.figure(figsize = (15,5))\nplt.title('20 Highest Revenue by City (2003 - 2005)', fontsize = 18)\nplt.bar(top_city['CITY'], top_city['SALES'], color = '#37C6AB', edgecolor = 'black', linewidth = 1 )\nplt.xlabel('City', fontsize = 15)\nplt.ylabel('Revenue', fontsize = 15)\nplt.xticks(fontsize = 12, rotation = 90)\nplt.yticks(fontsize = 12)\nfor k, v, in top_city['SALES'].items():\n    if v > 800000:\n        plt.text(k, v-350000, '$' + str(v), fontsize = 12, rotation = 90, color = 'black', ha = 'center')\n    else:\n        plt.text(k, v+35000, '$' + str(v), fontsize = 12, rotation = 90, color = 'black', ha = 'center')","62ce62f7":"top_product = df.groupby(['PRODUCTLINE']).sum().sort_values('SALES', ascending = False)\ntop_product = top_product[['SALES']]\ntop_product.reset_index(inplace = True)\ntotal_revenue_product = top_product['SALES'].sum()\ntotal_revenue_product = str(int(total_revenue_product))\ntotal_revenue_product = '$' + total_revenue_product","f47076ed":"plt.rcParams['figure.figsize'] = (13,7)\nplt.rcParams['font.size'] = 12.0\nplt.rcParams['font.weight'] = 6\ndef autopct_format(values):\n    def my_format(pct):\n        total = sum(values)\n        val = int(round(pct*total\/100.0))\n        return ' ${v:d}'.format(v = val)\n    return my_format\ncolors = ['#ff9999','#66b3ff','#99ff99','#ffcc99','#55B4B0','#E15D44','#009B77']\nexplode = (0.05,0.05,0.05,0.05,0.05,0.05,0.05)\nfig1, ax1 = plt.subplots()\npie1 = ax1.pie(top_product['SALES'], colors = colors, labels = top_product['PRODUCTLINE'], autopct = autopct_format(top_product['SALES']), startangle = 90, explode = explode)\nfraction_text_list = pie1[2]\nfor text in fraction_text_list:\n    text.set_rotation(315)\ncenter_circle = plt.Circle((0,0), 0.80, fc = 'white')\nfig = plt.gcf()\nfig.gca().add_artist(center_circle)\nax1.axis('equal')\nlabel = ax1.annotate('Total Revenue \\n' + str(total_revenue_product), color = 'red', xy = (0,0), fontsize = 12, ha  ='center')\nplt.tight_layout()\nplt.show()","f399d3a0":"plt.figure(figsize = (10,10))\ncorr_matrix = df.corr()\nsns.heatmap(corr_matrix, annot = True)","c821be80":"print('Order Date Description\\n')\nprint(dft['ORDERDATE'].describe(datetime_is_numeric=True))","cd711fe9":"dft.sort_values(by = ['ORDERDATE'], inplace = True, ascending = True)\ndft.set_index('ORDERDATE', inplace = True)\nnew_data = pd.DataFrame(dft['SALES'])\nnew_data.head()","174a0e4f":"new_data.plot()","3d82d0c6":"new_data = pd.DataFrame(new_data['SALES'].resample('D').mean())\nnew_data = new_data.interpolate(method = 'linear')","098eb2eb":"train, test, validation = np.split(new_data['SALES'].sample(frac = 1), [int(.6*len(new_data['SALES'])), int(.8*len(new_data['SALES']))])","15ee1082":"print('Train Dataset')\nprint(train)\nprint('Test Dataset')\nprint(test)\nprint('Validation Dataset')\nprint(validation)","c4b95fa1":"from statsmodels.tsa.stattools import adfuller\n#statsmodel provied addfuller()\ndata1 = new_data.iloc[:,0].values\nadf = adfuller(data1) \n\nprint(adf)\nprint('\\nADF = ', str(adf[0]))\nprint('\\np-value = ', str(adf[1]))\nprint('\\nCritical Values: ')\n\nfor key, val in adf[4].items():\n    print(key,':',val)\n    if adf[0] < val:\n        print('Null Hypothesis Rejected. Time Series is Stationary')\n    else:\n        print('Null Hypothesis Accepted. Time Series is not Stationary')","3124fccf":"from pylab import rcParams\nrcParams['figure.figsize'] = 20, 10\n\nimport statsmodels.api as sm\ndecomposition = sm.tsa.seasonal_decompose(new_data, model = 'additive')\n\nfig = decomposition.plot()\nplt.show()","503da9ca":"import itertools\np = d = q = range(0, 2) \npdq = list(itertools.product(p, d, q))\nseasonal_pdq_comb = [(i[0], i[1], i[2], 12) for i in list(itertools.product(p, d, q))]\nprint('Examples of parameter combinations for Seasonal ARIMA:')\nprint('SARIMA: {} x {}'.format(pdq[1], seasonal_pdq_comb[1]))\nprint('SARIMA: {} x {}'.format(pdq[1], seasonal_pdq_comb[2]))\nprint('SARIMA: {} x {}'.format(pdq[2], seasonal_pdq_comb[3]))\nprint('SARIMA: {} x {}'.format(pdq[2], seasonal_pdq_comb[4]))","ccc4cb5a":"for parameters in pdq:\n    for seasonal_param in seasonal_pdq_comb:\n        try:\n            mod = sm.tsa.statespace.SARIMAX(new_data,\n                                            order=parameters,\n                                            seasonal_param_order=seasonal_param,\n                                            enforce_stationarity=False,\n                                            enforce_invertibility=False)\n            results = mod.fit()\n            print('SARIMA{}x{}12 - AIC:{}'.format(parameters, seasonal_param, results.aic))\n        except:\n            continue","9e09ae91":"mod = sm.tsa.statespace.SARIMAX(new_data,\n                                order=(1, 1, 1),\n                                seasonal_order=(1, 1, 1, 12),\n                                enforce_stationarity=False,\n                                enforce_invertibility=False)\nresults = mod.fit()\nprint(results.summary().tables[1])","fb28618b":"results.plot_diagnostics(figsize=(16, 8)\nplt.show()","c5e4b557":"pred = results.get_prediction(start=pd.to_datetime('2003-01-06'), dynamic=False)\npred_val = pred.conf_int()\nax = new_data['2002':].plot(label='observed')\npred.predicted_mean.plot(ax=ax, label='One-step ahead Forecast', alpha=.7, figsize=(14, 7))\nax.fill_between(pred_val.index,\n                pred_val.iloc[:, 0],\n                pred_val.iloc[:, 1], color='k', alpha=.2)\nax.set_xlabel('ORDERDATE')\nax.set_ylabel('SALES')\nplt.legend()\nplt.show()","482bf9f7":"y_forecasted = pred.predicted_mean\ny_truth = new_data['SALES']\n\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\n\nmse = mean_squared_error(y_forecasted, y_truth)\nrmse = sqrt(mse)\nprint('The Mean Squared Error of the forecasts is {}'.format(round(rmse, 2)))","788b5437":"forecast = results.forecast(steps=7)\nprint(forecast.astype('float'))","f80209d0":"forecast = forecast.astype('float')\nforecast_df = forecast.to_frame()\nforecast_df.reset_index(level=0, inplace=True)\nforecast_df.columns = ['Prediction Date', 'Predicted Sales']\nprediction = pd.DataFrame(forecast_df).to_csv('prediction.csv',index=False)","a8aae76b":"df = pd.read_csv('.\/prediction.csv')\ndf.plot()","96c04570":"Out of sample forecast:\n\nTo forecast sales values after some time period of the given data. In our case, we have to forecast sales with time period of 7 days.","2bddd142":"## Import the dataset and preview","b90fc8f2":"**Find out 20 Most Valuable Customers**\n\nThe Most Valuable Customers are the customer who are the most profitable for a company (have a big sales on them). These customers buy more or higher-value than the other customers.","1bf44982":"Now that we know our time series is data is stationary. Let us begin with model training for forecasting the sales. We have chosen SARIMA model to forecast the sales.\n\nSeasonal Autoregressive Integrated Moving Average, SARIMA or Seasonal ARIMA, is an extension of ARIMA that supports univariate time series data with a seasonal componen","35dd4b23":"## Preprocessing\n* Check null value and solve it (Since there are lot of null values in ADDRESSLINE2, STATE, POSTALCODE, and TERRITORY, then I will drop them. COUNTRY and CITY will represent the order geographical information\n* Sorting data by 'ORDERDATE' column\n* Convert 'ORDERDATE' column to pandas datetime format\n* Setting the index to be 'ORDERDATE' column","f249f5cf":"A series is said to be stationary when its mean and variance do not change over time. From the above distribution of the sales it is not clear whether the sales distribution is stationary or not. Let us perform some stationarity tests to check whether the time series is stationary or not.","c131a586":"# Time Series Analysis","32a487e8":"**Correlation Test**\n\nPlotting correlation matrix to see the overview of how the features are related to one another","896cfb93":"**From the above values of mean and variance, it can be inferred that their is not much difference in the three values of mean and variance, indicating that the series is stationary. However, to verify our observations, let us perform a standard stationarity test, called Augmented Dicky Fuller test.**","a8aca52b":"**Find out 20 Highest Revenue by Country**\n\nHere are The Top 20 Country which generated the highest revenue","6959d50d":"**Augmented Dicky Fuller Test**\n\n* The Augmented Dickey-Fuller test is a type of statistical test alsocalled a unit root test.The base of unit root test is that it helps in determining how strongly a time series is defined by a trend.\n* The null hypothesis of the test is that the time series can be represented by a unit root, that it is not stationary. The alternate hypothesis (rejecting the null hypothesis) is that the time series is stationary.\n    - Null Hypothesis(H0): Time series is not stationary\n    - Alternate Hypothesis (H1): Time series is stationary\n* This result is interpreted using the p-value from the test.\n    - p-value > 0.05: Fail to reject the null hypothesis (H0), the data has a unit root and is non-stationary.\n    - p-value <= 0.05: Reject the null hypothesis (H0), the data does not have a unit root and is stationary.","ca26d679":"## Visualize Prediction Result","613174cc":"**Find out 20 Highest Revenue by City**\n\nThen visualized revenue by city. Here are th Top 20 City which generated the highest revenue","35e228d9":"## Sales Foecast for Next 7 Days","d9392390":"# Sales Forecasting using ARIMA","0f87c4e6":"**Observations**\n\n* There is high co-relation in ORDERNUMBER and YEAR_ID, and between QTR_ID and MONTH_ID\n* +velly correlated between SALES, QUANTITYORDERED, PRICEEACH and MSRP\n* YEAR_ID is -velly correlated to QTR_ID and MONTH_ID","7b37355c":"Then visualized which products give the highest revenue","63bc639a":"### Method 1\nTo check for stationarity by comparing the change in mean and variance over time, let us split teh data into train, test, and validation","b9c1072e":"# Exploratory Data Analysis and Visualization","94c1cf97":"### Method 2 - Augmented Dicky Fuller Test","a8629f70":"## Checking for Stationary"}}