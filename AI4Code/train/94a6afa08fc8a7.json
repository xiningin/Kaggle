{"cell_type":{"43efe4c2":"code","2ab62131":"code","c78c1f58":"code","da3be10a":"code","e58205f0":"code","8b8d3094":"code","2f79b3cf":"code","e1932d29":"code","c6f8b9fe":"code","c05cc25c":"code","7a1f689d":"code","c9430a11":"code","6cc707c2":"markdown"},"source":{"43efe4c2":"#pandas and plot tool\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n#Common Model Algorithms\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\nfrom xgboost import XGBClassifier\n\n#Common Model Helpers\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\n\n#ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')","2ab62131":"#Loading dataset\nwine = pd.read_csv('..\/input\/winequality-red.csv')","c78c1f58":"#Let's check how the data is distributed\nwine.head()","da3be10a":"#Information about the data columns\nwine.info()","e58205f0":"#Check the data quality\nprint('wine:Columns with null value:\\n', wine.isnull().sum())","8b8d3094":"#Review correlation heatmap of dataset\ndef correlation_heatmap(df):\n    _ , ax = plt.subplots(figsize =(14, 12))\n    \n    _ = sns.heatmap(\n        df.corr(), \n        center=0,\n        cmap=\"Blues\",\n        ax=ax,\n        linewidths=0.1,\n        annot=True, \n        annot_kws={'fontsize':14 }\n    )\n    plt.title('Correlation of Features', y=1.05, size=15)\n\ncorrelation_heatmap(wine)","2f79b3cf":"g = sns.FacetGrid(wine, col='quality', hue='quality', col_wrap=3, height=4)\ng.map(plt.scatter, 'alcohol','volatile acidity',s=150,alpha=0.8, edgecolors='w')\nplt.legend()","e1932d29":"#Making binary classificaion for the response variable.\n#Dividing wine as good and bad by giving the limit for the quality\nbins = (0, 6.5, 10)\ngroup_names = ['bad', 'good']\nwine['quality_bin'] = pd.cut(wine['quality'], bins = bins, labels = group_names)","c6f8b9fe":"#Now lets assign a labels to our quality variable\nlabel_quality = LabelEncoder()","c05cc25c":"#Bad becomes 0 and good becomes 1 \nwine['quality_bin'] = label_quality.fit_transform(wine['quality_bin'])\nwine['quality_bin'].value_counts()","7a1f689d":"#Define X and Y\n#Define y veriable as target\nTarget=['quality_bin']\n\n#Define x variables##remove SibSp based on the correlation\nx=['fixed acidity','volatile acidity','citric acid','residual sugar','chlorides','free sulfur dioxide','total sulfur dioxide','density','pH','sulphates','alcohol']","c9430a11":"#Machine Learning Algorithm (MLA) Selection and Initialization\nMLA = [\n    #Ensemble Methods\n    ensemble.AdaBoostClassifier(),\n    ensemble.BaggingClassifier(),\n    ensemble.ExtraTreesClassifier(),\n    ensemble.GradientBoostingClassifier(),\n    ensemble.RandomForestClassifier(),\n\n    #Gaussian Processes\n    gaussian_process.GaussianProcessClassifier(),\n    \n    #GLM\n    linear_model.LogisticRegressionCV(),\n    linear_model.PassiveAggressiveClassifier(),\n    linear_model.RidgeClassifierCV(),\n    linear_model.SGDClassifier(),\n    \n    #Navies Bayes\n    naive_bayes.BernoulliNB(),\n    naive_bayes.GaussianNB(),\n    \n    #Nearest Neighbor\n    neighbors.KNeighborsClassifier(),\n    \n    #Trees    \n    tree.DecisionTreeClassifier(),\n    tree.ExtraTreeClassifier(),\n    \n    #Discriminant Analysis\n    discriminant_analysis.LinearDiscriminantAnalysis(),\n    discriminant_analysis.QuadraticDiscriminantAnalysis(),\n\n    #xgboost: \n    XGBClassifier()    \n    ]\n\n#split dataset in cross-validation with this splitter class\ncv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0 ) # run model 10x with 60\/30 split intentionally leaving out 10%\n\n#create table to compare MLA metrics\nMLA_columns = ['MLA Name','MLA Train Accuracy Mean', 'MLA Test Accuracy Mean', 'MLA Test Accuracy 3*STD' ,'MLA Time']\nMLA_compare = pd.DataFrame(columns = MLA_columns)\n\n#create table to compare MLA predictions\nMLA_predict = wine[Target]\n\n#index through MLA and save performance to table\nrow_index = 1\nfor alg in MLA:\n\n    #set name and parameters\n    MLA_name = alg.__class__.__name__\n    MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n    \n    #score model with cross validation\n    cv_results = model_selection.cross_validate(alg, wine[x], wine[Target], cv  = cv_split)\n\n    MLA_compare.loc[row_index, 'MLA Time'] = cv_results['fit_time'].mean()\n    MLA_compare.loc[row_index, 'MLA Train Accuracy Mean'] = cv_results['train_score'].mean()\n    MLA_compare.loc[row_index, 'MLA Test Accuracy Mean'] = cv_results['test_score'].mean()   \n    #if this is a non-bias random sample, then +\/-3 standard deviations (std) from the mean, should statistically capture 99.7% of the subsets\n    MLA_compare.loc[row_index, 'MLA Test Accuracy 3*STD'] = cv_results['test_score'].std()*3   #let's know the worst that can happen!\n    \n\n    #save MLA predictions - see section 6 for usage\n    alg.fit(wine[x], wine[Target])\n    MLA_predict[MLA_name] = alg.predict(wine[x])\n    \n    row_index+=1\n\n    \n#print and sort table\nMLA_compare.sort_values(by = ['MLA Test Accuracy Mean'], ascending = False, inplace = True)\nMLA_compare","6cc707c2":"## **Let's do some plotting to know how the data columns are distributed in the dataset**"}}