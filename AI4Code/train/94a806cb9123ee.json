{"cell_type":{"cd09140b":"code","f14c8a4c":"code","211ebe66":"code","f2e80189":"code","081c1869":"code","0c0b6102":"code","0ec257ae":"code","62162caf":"code","ff43af2e":"code","c012f63f":"code","d3e217b2":"code","286be644":"code","856c7b29":"markdown","0a9ab2dd":"markdown","b36c0303":"markdown","02d127b2":"markdown","9d68e42c":"markdown","2ac8caf1":"markdown","aa72545c":"markdown","976c1ee2":"markdown","197b5e2c":"markdown","eb1aa064":"markdown","1612b229":"markdown"},"source":{"cd09140b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f14c8a4c":"SAMPLE_DATASET = \"..\/input\/facial-expression-recognition-challenge\/icml_face_data.csv\/icml_face_data.csv\"\n#print(SAMPLE_DATASET)\nNUM_CLASSES = 7\nTRAIN_HDF5 = \".\/train.hdf5\"\nVAL_HDF5 = \".\/val.hdf5\"\nTEST_HDF5 = \".\/test.hdf5\"\nMODEL_FILE = \".\/model.h5\"\nOUTPUT_PATH = \".\/\"\nBATCH_SIZE = 128\nDATASET_MEAN_FILE = OUTPUT_PATH + \"\/rgb_mean.json\"\nMODEL_FILE = OUTPUT_PATH + \"model.h5\"\n","211ebe66":"from tensorflow.keras.callbacks import Callback\nimport os\n\n\nclass EpochCheckpoint(Callback):\n    def __init__(self,output_path,every=5,start_at=0):\n        super(Callback,self).__init__()\n\n        self.output_path = output_path\n        self.every = every\n        self.start_epoch = start_at\n\n\n    def on_epoch_end(self, epoch, logs={}):\n        if (self.start_epoch + 1)% self.every ==0:\n            p = os.path.sep.join([self.output_path,\n                                  \"epoch_{}.hdf5\".format(self.start_epoch + 1)])\n            self.model.save(p, overwrite = True)\n            self.start_epoch += 1","f2e80189":"from tensorflow.keras.preprocessing.image import img_to_array\n\nclass ImageToArrayPreprocesor:\n    def __init__(self,data_format=None):\n        self.data_format = data_format\n\n    def preprocess(self,image):\n        return img_to_array(image, data_format=self.data_format)","081c1869":"import os\nimport h5py\n\n\nclass HDF5DatasetWriter:\n    def __init__(self, dims, output_path, data_key=\"images\", buf_size=1000):\n        if os.path.exists(output_path):\n            raise ValueError(\"\u60a8\u63d0\u4f9b\u7684\u8f93\u51fa\u6587\u4ef6{}\u5df2\u7ecf\u5b58\u5728\uff0c\u8bf7\u624b\u52a8\u5220\u9664\".format(output_path))\n        self.db = h5py.File(output_path, \"w\")\n        self.data = self.db.create_dataset(data_key, dims, dtype=\"float\")\n        self.labels = self.db.create_dataset(\"labels\", (dims[0],), dtype=\"int\")\n\n        self.buf_size = buf_size\n        self.buffer = {\"data\": [], \"labels\": []}\n        self.idx = 0\n\n    def add(self, raw, label):\n        self.buffer[\"data\"].extend(raw)\n        self.buffer[\"labels\"].extend(label)\n        if len(self.buffer[\"data\"]) >= self.buf_size:\n            self.flush()\n\n    def flush(self):\n        i = self.idx + len(self.buffer[\"data\"])\n        self.data[self.idx:i] = self.buffer[\"data\"]\n        self.labels[self.idx:i] = self.buffer[\"labels\"]\n        self.idx = i\n        self.buffer = {\"data\": [], \"labels\": []}\n\n    def store_class_labels(self, class_labels):\n        dt = h5py.special_dtype(vlen=str)\n        label_dim = (len(class_labels),)\n        label_set = self.db.create_dataset(\"label_names\", label_dim, dtype=dt)\n        label_set[:] = class_labels\n\n    def close(self):\n        if len(self.buffer[\"data\"]) > 0:\n            self.flush()\n        self.db.close()","0c0b6102":"from keras.utils.np_utils import to_categorical\nimport numpy as np\nimport h5py\n\n\nclass HDF5DatasetGenerator:\n    def __init__(self, db_file, batch_size, preprocessors=None, aug=None, binarize=True, classes=2):\n        self.batch_size = batch_size\n        # \u6570\u636e\u9884\u5904\u7406\u5668\u5217\u8868\n        self.preprocessor = preprocessors\n        # \u6570\u636e\u589e\u5f3a\u5904\u7406\u5668\u5217\u8868\n        self.aug = aug\n        self.binarize = binarize\n        self.classes = classes\n        self.db = h5py.File(db_file,'r')\n        self.numImages = self.db[\"labels\"].shape[0]\n\n    def generator(self, passes=np.inf):\n        epochs = 0\n\n        while epochs < passes:\n            for i in np.arange(0, self.numImages, self.batch_size):\n                images = self.db[\"images\"][i:i + self.batch_size]\n                labels = self.db[\"labels\"][i:i + self.batch_size]\n\n                if self.binarize:\n                    labels = to_categorical(labels, self.classes)\n                if self.preprocessor is not None:\n                    processed_image = []\n                    for image in images:\n                        for p in self.preprocessor:\n                            image = p.preprocess(image)\n                        processed_image.append(image)\n                    images = np.array(processed_image)\n                if self.aug is not None:\n                    (images, labels) = next(self.aug.flow(images, labels, batch_size=self.batch_size))\n\n                    yield images, labels\n                epochs += 1\n\n    def close(self):\n        self.db.close()","0ec257ae":"from tensorflow.keras.callbacks import BaseLogger\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport json\nimport os\n\n\nclass TrainingMonitor(BaseLogger):\n    def __init__(self,fig_path,json_path=None, start_at =0):\n        super(TrainingMonitor, self).__init__()\n        self.history = {}\n        self.fig_path = fig_path\n        self.json_path = json_path\n        self.start_at = start_at\n\n    def on_train_begin(self, logs={}):\n        if self.json_path is not None:\n            if os.path.exists(self.json_path):\n                self.history = json.loads(open(self.json_path).read())\n\n                if self.start_at > 0:\n                    for k in self.history.keys():\n                        self.history[k] = self.history[k][:self.start_at]\n\n\n    def on_epoch_end(self, epoch, logs={}):\n        for (k,v) in logs.items():\n            log = self.history.get(k, [])\n            log.append(v)\n            self.history[k] =  log\n\n        if self.json_path is not None:\n            f = open(self.json_path, \"w\")\n            f.write(json.dumps(self.history))\n            f.close()\n\n\n        if len(self.history[\"loss\"]) >1:\n            N = np.arange(0, len(self.history[\"loss\"]))\n            plt.style.use(\"ggplot\")\n            plt.figure()\n            plt.plot(N, self.history[\"loss\"], label=\"train_loss\")\n            plt.plot(N, self.history[\"val_loss\"], label=\"val_loss\")\n            plt.plot(N, self.history[\"accuracy\"], label=\"train_acc\")\n            plt.plot(N, self.history[\"val_accuracy\"], label=\"val_acc\")\n            epochs = len(self.history[\"loss\"])\n            plt.title(\"Training Loss & Accuracy [Epoch {}]\".format(epochs))\n            plt.xlabel(\"Epoch #\")\n            plt.ylabel(\"Loss\/Accuracy\")\n            plt.legend()\n            plt.savefig(self.fig_path)\n            plt.close()","62162caf":"import numpy as np\n# from utils.HDF5DatasetWriter import HDF5DatasetWriter\n# from config import setting\nprint(\"[\u4fe1\u606f] \u52a0\u8f7dcsv\u683c\u5f0f\u6570\u636e\u96c6\u6587\u4ef6\")\n\nfile = open(SAMPLE_DATASET)\nfile.__next__()#\u8df3\u8fc7\u7b2c\u4e00\u884c\n(train_images, train_label) = ([], [])\n(val_images, val_label) = ([], [])\n(test_images, test_label) = ([], [])\ncount_by_label_train = {}\ncount_by_label_val = {}\ncount_by_label_test = {}\nfor row in file:\n    (label, usage, image) = row.strip().split(\",\")\n    label = int(label)\n    image = np.array(image.split(\" \"), dtype=\"uint8\")\n    image = image.reshape((48, 48))\n\n    if usage == \"Training\":\n        train_images.append(image)\n        train_label.append(label)\n        count = count_by_label_train.get(label, 0)\n        count_by_label_train[label] = count + 1\n\n    elif usage == \"PublicTest\":\n        val_images.append(image)\n        val_label.append(label)\n        count = count_by_label_val.get(label, 0)\n        count_by_label_val[label] = count + 1\n\n    elif usage == \"PrivateTest\":\n        test_images.append(image)\n        test_label.append(label)\n        count = count_by_label_test.get(label, 0)\n        count_by_label_test[label] = count + 1\n\nfile.close()\nprint(\"[\u4fe1\u606f] \u8bad\u7ec3\u96c6\u6837\u672c\u6570\u91cf\uff1a{}\".format(len(train_images)))\nprint(\"[\u4fe1\u606f] \u6821\u9a8c\u96c6\u6837\u672c\u6570\u91cf\uff1a{}\".format(len(val_images)))\nprint(\"[\u4fe1\u606f] \u6d4b\u8bd5\u96c6\u6837\u672c\u6570\u91cf\uff1a{}\".format(len(test_images)))\n#\u8bad\u7ec3\u96c6\u6837\u672c\u5206\u5e03\nprint(count_by_label_train)\n#\u6821\u6b63\u96c6\u6837\u672c\u5206\u5e03\nprint(count_by_label_val)\n#\u6d4b\u8bd5\u96c6\u6837\u672c\u5206\u5e03\nprint(count_by_label_test)\n\ndatasets = [(train_images,train_label,TRAIN_HDF5),\n            (val_images,val_label,VAL_HDF5),\n            (test_images,test_label,TEST_HDF5)]\n\nfor (images,labels,outputPath) in datasets:\n    print(\"[\u4fe1\u606f]\u6784\u5efa{}...\".format(outputPath))\n    writer = HDF5DatasetWriter((len(images),48,48),outputPath)\n\n    for (image,label) in zip(images,labels):\n        writer.add([image],[label])\n\n    writer.close()","ff43af2e":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D\nfrom tensorflow.keras.layers import MaxPooling2D\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.layers import Activation\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras import backend\n\n\nclass MiniVGG11Net:\n    @staticmethod\n    def build(width, height, channel, classes, reg=0.0002):\n        \"\"\"\n        \u6839\u636e\u8f93\u5165\u6837\u672c\u7684\u7ef4\u5ea6\uff08width\u3001height\u3001channel\uff09\uff0c\u5206\u7c7b\u6570\u91cf\uff0c\u6b63\u5219\u5316\u56e0\u5b50\u521b\u5efaMiniVGG16\u7f51\u7edc\u6a21\u578b\n        Args:\n            width:   \u8f93\u5165\u6837\u672c\u7684\u5bbd\u5ea6\n            height:  \u8f93\u5165\u6837\u672c\u7684\u9ad8\u5ea6\n            channel: \u8f93\u5165\u6837\u672c\u7684\u901a\u9053\n            classes: \u5206\u7c7b\u6570\u91cf\n            reg:     \u6b63\u5219\u5316\u56e0\u5b50\n\n        Returns:\n           \u7f51\u7edc\u6a21\u578b\u5bf9\u8c61\n\n        \"\"\"\n\n        model = Sequential(name=\"MiniVGG13\")\n\n        # \u7f3a\u7701\u8f93\u5165\u683c\u5f0f\u4e3a\u901a\u9053\u540e\u7f6e\uff08\"channels_last\"\uff09\n        shape = (height, width, channel)\n        channel_dimension = -1\n\n        # \u5982\u679c\u8f93\u5165\u683c\u5f0f\u4e3a\u901a\u9053\u524d\u7f6e\n        # \u91cd\u65b0\u8bbe\u7f6e\u8f93\u5165\u683c\u5f0f\u548c\u901a\u9053\u4f4d\u7f6e\u6307\u793a\n        if backend.image_data_format() == \"channels_first\":\n            shape = (channel, height, width)\n            channel_dimension = 1\n\n        # \u7b2c\u4e00\u5377\u79ef\u5757\n        model.add(Conv2D(64, (3, 3), input_shape=shape, padding=\"same\", kernel_regularizer=l2(reg)))\n        model.add(Activation(\"relu\"))\n        # model.add(Conv2D(64, (3, 3), padding=\"same\", kernel_regularizer=l2(reg)))\n        # model.add(Activation(\"relu\"))\n        model.add(BatchNormalization(axis=channel_dimension))\n        model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n        # model.add(Dropout(0.35))\n\n        # \u7b2c\u4e8c\u5377\u79ef\u5757\n        model.add(Conv2D(128, (3, 3), padding=\"same\", kernel_regularizer=l2(reg)))\n        model.add(Activation(\"relu\"))\n        # model.add(Conv2D(128, (3, 3), padding=\"same\", kernel_regularizer=l2(reg)))\n        # model.add(Activation(\"relu\"))\n        model.add(BatchNormalization(axis=channel_dimension))\n        model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n        # model.add(Dropout(0.35))\n\n        # \u7b2c\u4e09\u5377\u79ef\u5757\n        model.add(Conv2D(256, (3, 3), padding=\"same\", kernel_regularizer=l2(reg)))\n        model.add(Activation(\"relu\"))\n        model.add(Conv2D(256, (3, 3), padding=\"same\", kernel_regularizer=l2(reg)))\n        model.add(Activation(\"relu\"))\n        # model.add(Conv2D(256, (3, 3), padding=\"same\", kernel_regularizer=l2(reg)))\n        # model.add(Activation(\"relu\"))\n        model.add(BatchNormalization(axis=channel_dimension))\n        model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n        # model.add(Dropout(0.35))\n\n        # \u7b2c\u56db\u5377\u79ef\u5757\n        model.add(Conv2D(512, (3, 3), padding=\"same\", kernel_regularizer=l2(reg)))\n        model.add(Activation(\"relu\"))\n        model.add(Conv2D(512, (3, 3), padding=\"same\", kernel_regularizer=l2(reg)))\n        model.add(Activation(\"relu\"))\n        # model.add(Conv2D(512, (3, 3), padding=\"same\", kernel_regularizer=l2(reg)))\n        # model.add(Activation(\"relu\"))\n        model.add(BatchNormalization(axis=channel_dimension))\n        model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n        # model.add(Dropout(0.35))\n\n        # \u7b2c\u4e00\u5168\u8fde\u63a5\u5c42\n        model.add(Flatten())\n        model.add(Dense(1024, kernel_regularizer=l2(reg)))\n        model.add(Activation(\"relu\"))\n        model.add(Dropout(0.5))\n        # model.add(BatchNormalization(axis=channel_dimension))\n\n        # \u7b2c\u4e8c\u5168\u8fde\u63a5\u5c42\n        model.add(Dense(256, kernel_regularizer=l2(reg)))\n        model.add(Activation(\"relu\"))\n        model.add(Dropout(0.5))\n        # model.add(BatchNormalization(axis=channel_dimension))\n\n        # \u7b2c\u4e09\u5168\u8fde\u63a5\u5c42\n        model.add(Dense(classes, kernel_regularizer=l2(reg)))\n        model.add(Activation(\"softmax\"))\n\n        return model\n\n\n# \u6d4b\u8bd5MiniVGG13\u7c7b\u5b9e\u4f8b\u5316\u5e76\u8f93\u51faMiniVGG13\u6a21\u578b\u7684\u6982\u8981\u4fe1\u606f\nif __name__ == \"__main__\":\n    model = MiniVGG11Net.build(width=48, height=48, channel=1, classes=7, reg=0.0002)\n    print(model.summary())\n   ","c012f63f":"import matplotlib\n# from config import setting\n# from utils.ImageToArrayPreprocessor import ImageToArrayPreprocessor\n# from utils.TrainingMonitor import TrainingMonitor\n# from utils.HDF5DatasetGenerator import HDF5DatasetGenerator\n# from MiniVGG11 import MiniVGG13Net\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.optimizers import Adam\nimport os\nfrom tensorflow.keras.preprocessing.image import img_to_array\n\nclass ImageToArrayPreprocessor:\n    def __init__(self,data_format=None):\n        self.data_format = data_format\n\n    def preprocess(self,image):\n        return img_to_array(image, data_format=self.data_format)\nmatplotlib.use(\"Agg\")\n\ntrain_aug = ImageDataGenerator(rotation_range=10,\n                   zoom_range = 0.1,\n                   rescale=1 \/ 255.0,\n                   fill_mode=\"nearest\")\nval_aug = ImageDataGenerator(rescale=1\/255.0)\n\niap = ImageToArrayPreprocessor()\n\ntrain_gen = HDF5DatasetGenerator(TRAIN_HDF5,\n                                 BATCH_SIZE,\n                                 aug=train_aug,\n                                 preprocessors=[iap],\n                                 classes=NUM_CLASSES)\nval_gen = HDF5DatasetGenerator(VAL_HDF5,\n                                 BATCH_SIZE,\n                                 aug=val_aug,\n                                 preprocessors = [iap],\n                                 classes=NUM_CLASSES)\n\nopt = Adam(lr = 1e-3)\nmodel = MiniVGG11Net.build(width=48,height=48,channel=1,classes=NUM_CLASSES)\nmodel.compile(loss=\"categorical_crossentropy\",optimizer=opt,metrics=[\"accuracy\"])\nfig_path = os.path.sep.join([OUTPUT_PATH, \"{}.png\".format(os.getpid())])\njson_path = os.path.sep.join([OUTPUT_PATH,\"MiniVGG11.json\"])\ncallbacks = [TrainingMonitor(fig_path=fig_path)]\nmodel.fit_generator(train_gen.generator(),\n                    steps_per_epoch=train_gen.numImages\/\/BATCH_SIZE,\n                    validation_data=val_gen.generator(),\n                    validation_steps=val_gen.numImages \/\/ BATCH_SIZE,\n                    epochs=50,\n                    max_queue_size=BATCH_SIZE*2,\n                    callbacks=callbacks,\n                    verbose=1)\nprint(\"[\u4fe1\u606f] \u4fdd\u5b58\u6a21\u578b...\")\nmodel.save(MODEL_FILE,overwrite=True)\ntrain_gen.close()\nval_gen.close()","d3e217b2":"#from config import setting\n#from utils.ImageToArrayPreprocessor import ImageToArrayPreprocesor\n#from utils.HDF5DatasetGenerator import HDF5DatasetGenerator\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import load_model\n\ntestAug = ImageDataGenerator(rescale=1\/255.0)\niap = ImageToArrayPreprocesor()\ntestGen = HDF5DatasetGenerator(TEST_HDF5,\n                               BATCH_SIZE,\n                               aug=testAug,\n                               preprocessors=[iap],\n                               classes=NUM_CLASSES)\nprint(\"[\u4fe1\u606f] \u52a0\u8f7d\u7f51\u7edc\u6a21\u578b...\")\nmodel = load_model(MODEL_FILE)\n\n# \u8bc4\u4f30\n(loss, acc) = model.evaluate_generator(testGen.generator(),\n                                     steps=testGen.numImages\/\/BATCH_SIZE,\n                                     max_queue_size=BATCH_SIZE*2)\nprint(\"[\u4fe1\u606f] \u6d4b\u8bd5\u96c6\u51c6\u786e\u7387\uff1a{:.2f}%\".format(acc*100))\ntestGen.close()\n\n","286be644":"'''#import cv2\n#import imutils\n#import numpy as np\n#from tensorflow.keras.models import load_model\n#from tensorflow.keras.preprocessing.image import img_to_array\n\nface_detector = cv2.CascadeClassifier('..\/input\/haarcascade-frontalface-defaultxml\/haarcascade_frontalface_default.xml')\nmodel = load_model(\".\/model.h5\")\nprint(face_detector)\n\nEMOTIONS = ['Angry', 'Disgust', 'Scared', 'Harry', 'Sad', 'Surprise', 'Neutral']\n\n# \u5f00\u542f\u6444\u50cf\u5934\ncapture = cv2.VideoCapture(0)\n\n# \u6301\u7eed\u91c7\u96c6\u6444\u50cf\u5934\u56fe\u50cf\u5e27\nwhile True:\n    ret, frame = capture.read()\n    # \u5e27\u56fe\u50cf\u7f29\u5c0f\u5e76\u7070\u5ea6\u5316\n    frame = imutils.resize(frame, width=300)\n    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n\n    # \u753b\u5e03\n    canvas = np.zeros((240,300,3),dtype=\"uint8\")\n\n    frameClone = frame.copy()\n\n    # \u68c0\u6d4b\u4eba\u8138\n    rects = face_detector.detectMultiScale(\n        gray,scaleFactor=1.1,\n        minNeighbors=5,\n        minSize=(30,30),\n        flags=cv2.CASCADE_SCALE_IMAGE)\n\n    if len(rects) > 0:\n        # \u5bf9\u68c0\u6d4b\u5230\u7684\u591a\u4e2a\u4eba\u8138\u6846\u964d\u5e8f\u6392\u5e8f\n        rect = sorted(rects, reverse=True,\n                      key=lambda x:(x[2]-x[0])*(x[3]-x[1]))[0]\n        (fX, fY, fW, fH) = rect\n        roi =  gray[fY:fY+fH,fX:fX+fW]\n        roi = cv2.resize(roi,(48,48))\n        roi = roi.astype(\"float\") \/ 255.0\n        roi = img_to_array(roi)\n        roi = np.expand_dims(roi,axis=0)\n\n        predicts = model.predict(roi)[0]\n        label = EMOTIONS[predicts.argmax()]\n        cv2.putText(frameClone,label,(fX,fY-10),\n                    cv2.FONT_HERSHEY_SIMPLEX,0.5,(0, 0, 255),1, cv2.LINE_AA)\n        cv2.rectangle(frameClone,(fX,fY),(fX+fW,fY+fH),\n                      (0,0,255),1,cv2.LINE_AA)\n        for (i,(emotion,prob)) in enumerate(zip(EMOTIONS,predicts)):\n            text = \"{}: {:.2f}%\".format(emotion,prob*100)\n            w = int (prob*300)\n            cv2.rectangle(canvas,(5,(i*32)+5),(5+w,(i*32)+32),(0,0,255),-1)\n            cv2.putText(\n                canvas,\n                text,\n                (10,(i*32)+23),\n                cv2.FONT_HERSHEY_SIMPLEX,\n                0.5,\n                (255,255,255),\n                1,\n                cv2.LINE_AA\n            )\n#    cv2.imshow('Emotion Detection', frameClone)\n#    cv2.imshow(\"Result\",canvas)\n\n#    if cv2.waitKey(1) == 27:\n        break\n\n#capture.release()\n#cv2.destroyAllWindows()'''\n","856c7b29":"HDF5DatasetWriter","0a9ab2dd":"setting","b36c0303":"ImageToArrayPreprocessor","02d127b2":"build_hdf5","9d68e42c":"training","2ac8caf1":"EpochCheckpoint","aa72545c":"evaluating","976c1ee2":"TrainingMonitor","197b5e2c":"HDF5DatasetGenerator","eb1aa064":"mini_vgg_11","1612b229":"emotion_detector"}}