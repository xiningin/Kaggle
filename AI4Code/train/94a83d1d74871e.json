{"cell_type":{"5792d95e":"code","127e2324":"code","39baa9a4":"code","000a6c67":"code","e5e562c4":"code","2c627837":"code","002662da":"code","88476130":"code","e80503c9":"code","e29176d0":"code","4fca72b2":"code","3e1ca340":"code","05f4f1c0":"code","c46589fb":"code","8720fdbb":"code","16fe60aa":"code","2936733b":"code","ca5228b4":"code","a4787bb0":"markdown","f515ad51":"markdown","1c383c74":"markdown","d26cf051":"markdown","5390949d":"markdown","82bfeadb":"markdown","bdad901a":"markdown","58bd8ee2":"markdown"},"source":{"5792d95e":"import os, shutil\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport random\n\nimport tensorflow as tf\n\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, Dense, Dropout, Flatten, MaxPooling2D\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras import optimizers","127e2324":"base_dir = \"..\/input\/state-farm-distracted-driver-detection\/\"\nimg_folder = os.path.join(base_dir, 'imgs\/')\ntrain_imgs = os.path.join(img_folder, 'train\/')\ntest_imgs = os.path.join(img_folder, 'test\/')\ndriver_imgs_list = pd.read_csv(os.path.join(base_dir, 'driver_imgs_list.csv'))\nsample_sub = pd.read_csv(os.path.join(base_dir, 'sample_submission.csv'))","39baa9a4":"driver_imgs_list.head()","000a6c67":"driver_imgs_list.shape","e5e562c4":"sample_sub.head()","2c627837":"classes = ['c0', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7', 'c8', 'c9']\n\nclass_def = {'c0': 'safe driving',\n'c1': 'texting - right',\n'c2': 'talking on the phone - right',\n'c3': 'texting - left',\n'c4': 'talking on the phone - left',\n'c5': 'operating the radio',\n'c6': 'drinking',\n'c7': 'reaching behind',\n'c8': 'hair and makeup',\n'c9': 'talking to passenger'}","002662da":"fig = plt.figure(figsize=(20, 18))\ncolumns = 5\nrows = 4\nfor i in range(1, columns*rows +1):\n    pic_idx = random.randint(0, driver_imgs_list.shape[0])\n    im = Image.open(r\"..\/input\/state-farm-distracted-driver-detection\/imgs\/train\/\"+ \n                    str(driver_imgs_list.loc[pic_idx, 'classname']) +'\/' \n                    +str(driver_imgs_list.loc[pic_idx, 'img' ]))\n    fig.add_subplot(rows, columns, i)\n    plt.imshow(im)\n    plt.title('State of driving: ' + class_def[(driver_imgs_list.loc[pic_idx, 'classname'])])\n    plt.axis('off')\nplt.tight_layout()\nplt.show()","88476130":"# size of each image is (640, 480)\nim = Image.open(\"..\/input\/state-farm-distracted-driver-detection\/imgs\/train\/c7\/img_100702.jpg\")\nw, h = im.size\nprint(w, h)","e80503c9":"train_datagen = ImageDataGenerator(rescale=1.\/255)\ntest_datagen = ImageDataGenerator(rescale=1.\/255)","e29176d0":"image_size = (256, 256)\nval_frac = 0.12\nbatch_size = 16\ntrain_dir = \"..\/input\/state-farm-distracted-driver-detection\/imgs\/train\/\"\n\ntrain_generator = tf.keras.preprocessing.image_dataset_from_directory(train_dir,\n                                                             labels = 'inferred',\n                                                             label_mode='categorical',\n                                                             image_size=image_size,\n                                                             batch_size=batch_size,\n                                                             seed=1,\n                                                             shuffle=True,\n                                                             validation_split=val_frac,\n                                                             subset='training')\nval_generator = tf.keras.preprocessing.image_dataset_from_directory(train_dir,\n                                                             labels = 'inferred',\n                                                             label_mode='categorical',\n                                                             image_size=image_size,\n                                                             batch_size=batch_size,\n                                                             seed=1,\n                                                             shuffle=True,\n                                                             validation_split=val_frac,\n                                                             subset='validation')","4fca72b2":"def normalize(image,label):\n    image = tf.cast(image\/255. ,tf.float32)\n    return image,label\n\ntrain_generator = train_generator.map(normalize)\nval_generator = val_generator.map(normalize)","3e1ca340":"for data_batch, labels_batch in train_generator:\n    print('data batch shape:', data_batch.shape)\n    print('labels batch shape:', labels_batch.shape)\n    break","05f4f1c0":"model = Sequential()\nmodel.add(Conv2D(32, (3, 3), activation='relu',input_shape=(256, 256, 3)))\nmodel.add(MaxPooling2D((2, 2)))\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D((2, 2)))\nmodel.add(Conv2D(128, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D((2, 2)))\nmodel.add(Conv2D(128, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D((2, 2)))\nmodel.add(Conv2D(256, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D((2, 2)))\nmodel.add(Conv2D(512, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D((2, 2)))\nmodel.add(Flatten())\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dense(10, activation='softmax'))","c46589fb":"model.summary()","8720fdbb":"model.compile('rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])","16fe60aa":"history = model.fit(\ntrain_generator,\nsteps_per_epoch=240,\nepochs=10,\nvalidation_data=val_generator,\nvalidation_steps=60)","2936733b":"model.save('distracted_driver.h5')","ca5228b4":"tr_loss = history.history['loss']\ntr_acc = history.history['accuracy']\nval_loss = history.history['val_loss']\nval_acc = history.history['val_accuracy']\nepochs = range(1, len(tr_loss)+1)\n\nplt.clf()\nfig, (ax1, ax2) = plt.subplots(1,2, figsize=(18,5))\nfig\n\nax1.plot(epochs, tr_loss, 'b', label='Training loss')\nax1.plot(epochs, val_loss, 'r', label='Validation loss')\nax1.set_title('Training & Validation loss')\nax1.set_xlabel('epochs')\nax1.set_ylabel('loss')\nax1.legend()\n\nax2.plot(epochs, tr_acc, 'b', label='Training acc')\nax2.plot(epochs, val_acc, 'r', label='Validation acc')\nax2.set_title('Training & Validation acc')\nax2.set_xlabel('epochs')\nax2.set_ylabel('accuracy')\nax2.legend()\nplt.show()","a4787bb0":"img_path = \"..\/input\/state-farm-distracted-driver-detection\/imgs\/\"\ntest_generator = test_datagen.flow_from_directory(\n    directory=img_path,\n    target_size=image_size,\n    color_mode=\"rgb\",\n    batch_size=1,\n    class_mode='categorical',\n    shuffle=False,\n    classes=['test']\n)","f515ad51":"Display 20 sample images","1c383c74":"pred_df.to_csv('predictions.csv', index=False)","d26cf051":"preds = model.predict(test_generator, steps=79726)","5390949d":"# Problem description\n\nAccording to the CDC motor vehicle safety division, one in five car accidents is caused by a distracted driver. Sadly, this translates to 425,000 people injured and 3,000 people killed by distracted driving every year.\n\nState Farm hopes to improve these alarming statistics, and better insure their customers, by testing whether dashboard cameras can automatically detect drivers engaging in distracted behaviors. Given a dataset of 2D dashboard camera images, State Farm is challenging Kagglers to classify each driver's behavior. Are they driving attentively, wearing their seatbelt, or taking a selfie with their friends in the backseat?","82bfeadb":"callback = tf.keras.callbacks.EarlyStopping(monitor='loss', mode='min', patience=4, verbose=1)","bdad901a":"# Prediction on test images & submission","58bd8ee2":"test_ids = sorted(os.listdir(test_imgs))\npred_df = pd.DataFrame(columns = ['img','c0', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7', 'c8', 'c9'])\nfor i in range(len(preds)):    \n    pred_df.loc[i, 'img'] = test_ids[i]\n    pred_df.loc[i, 'c0':'c9'] = preds[i]"}}