{"cell_type":{"717cc3e8":"code","9cd1bdab":"code","3c7135a3":"code","43ab9896":"code","716c1dd0":"code","3291e4d1":"code","31c344d6":"code","c2e01264":"code","169aea5d":"code","357c5aae":"code","4265f924":"code","b30125a9":"code","1352dfad":"code","85fad62c":"code","f44debd3":"code","c8b6b63f":"code","102ec63b":"code","6dd4a0b6":"code","a1708dea":"code","994a099f":"code","485c6b1c":"code","1b7af200":"code","24c53942":"code","f98351b1":"code","0b115ebd":"code","cefb7553":"code","954d92c9":"code","2bac16b0":"code","e61a7e29":"code","459eecf7":"code","b0965fdc":"code","fe0f1252":"code","cfc6171c":"code","cde2bd5c":"code","7f12913c":"code","6479823c":"code","a5e969ea":"code","95dedae2":"code","2c755f2e":"code","7858d3d5":"code","74879d3f":"code","fe1ea948":"code","07c39d7e":"code","bfae264f":"code","33e5bfab":"code","0a40da00":"code","79ea572f":"code","0c07b6a7":"code","61a3110d":"code","130760c0":"code","3b10cf2b":"code","cb5631a6":"code","bc451e70":"code","da4e638d":"code","a589c629":"code","53ec6d62":"code","6df2ab85":"markdown","295d1aeb":"markdown","9470e13f":"markdown","417942b8":"markdown","1f9ce897":"markdown","6d4f2412":"markdown","dee39b5a":"markdown","7198ddcc":"markdown","7254bc84":"markdown","22ef7d26":"markdown","1bfbbb32":"markdown","3623d991":"markdown","869e3b08":"markdown","9b2e4703":"markdown","4992d5d4":"markdown","4d0290d3":"markdown","01b7a9e6":"markdown","96428e78":"markdown","109336a7":"markdown","dc5e24cf":"markdown","2bc44b75":"markdown","f0c12804":"markdown"},"source":{"717cc3e8":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, MinMaxScaler,  StandardScaler\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.metrics import mean_squared_log_error, make_scorer","9cd1bdab":"train_df = pd.read_csv('\/kaggle\/input\/seoul-bike-rental-ai-pro-iti\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/seoul-bike-rental-ai-pro-iti\/test.csv')","3c7135a3":"train_df.shape","43ab9896":"train_df.dtypes","716c1dd0":"train_df.info()","3291e4d1":"train_df.describe()","31c344d6":"train_df.duplicated().count()","c2e01264":"train_df.isna().sum()","169aea5d":"plt.bar(train_df['Seasons'], train_df['y'])\nplt.xlabel('Seasons')\nplt.ylabel('Number of bikes')\nplt.show()","357c5aae":"len(train_df['Hour'])","4265f924":"plt.figure(figsize = [10, 5])\n\ny = list(train_df.groupby('Hour')['y'].sum()\/1000)\n\nplt.bar(range(24) , y)\nplt.xticks(range(24), rotation=90)\nplt.xlabel('Hour')\nplt.ylabel('Number of bikes (1000)')\nplt.show()","b30125a9":"# Box plots to check outliers existence\n\nplt.figure(figsize = [20, 5])\nbase_color = sns.color_palette()[0]\n \nplt.subplot(1, 3, 1)\n# plot Seasons VS y\n_ = sns.boxplot(data=train_df, x='Seasons', y='y', color=base_color)\n\nplt.subplot(1, 3, 2)\n# plot Holiday VS y\n_ = sns.boxplot(data=train_df, x='Holiday', y='y', color=base_color)\n\nplt.subplot(1, 3, 3)\n# Functioning Day VS y\n_ = sns.boxplot(data=train_df, x='Functioning Day', y='y', color=base_color)","1352dfad":"df_for_plot = train_df.iloc[:, 2:12]\nplt.figure(figsize = [12, 8])\nsns.heatmap(df_for_plot.corr(),annot = True, cmap = 'viridis_r', fmt = '.2f');","85fad62c":"cols = train_df.columns[5:12]\nsamples = np.random.choice(train_df.shape[0], 200, replace = False)\ndiamonds_samp = df_for_plot.loc[samples,:]\nplt.figure(figsize = [15, 15],  tight_layout = 5)\n\nfor i in range(len(cols)):\n    plt.subplot(3, 3, i+1)\n    plt.scatter(data = diamonds_samp, x = 'Temperature(\ufffdC)', y = 'y', c = cols[i], cmap = 'viridis_r')\n    plt.colorbar(label = cols[i])\n    plt.xlabel('Temperatur')\n    plt.ylabel('Number of bikes')\n    plt.title(cols[i] + 'as a third dimension')","f44debd3":"def handle_outliers(df, col, min_, max_):\n\n    # Quantile will return a tuple\n    min_threshold, max_threshold = df[col].quantile([min_ ,max_])\n    col_mean = df[col].mean()\n    new_col = []\n    \n    for v in df[col]:\n        if(v <= max_threshold) and (v >= min_threshold):\n            new_col.append(v)\n        else:\n            new_col.append(col_mean)\n            \n    df[col] = new_col\n    return df","c8b6b63f":"def min_max_scaling(train_df,test_df, lst_cols):\n    transformer = MinMaxScaler().fit(train_df[lst_cols])\n    train_df[lst_cols] = transformer.transform(train_df[lst_cols])\n    test_df[lst_cols] = transformer.transform(test_df[lst_cols])\n    return train_df,test_df","102ec63b":"def standard_scaling(train_df,test_df, lst_cols):\n    transformer = StandardScaler().fit(train_df[lst_cols])\n    train_df[lst_cols] = transformer.transform(train_df[lst_cols])\n    test_df[lst_cols] = transformer.transform(test_df[lst_cols])\n    return train_df,test_df","6dd4a0b6":"def one_hot_encoding(df, lst_cols):\n    encoder = OneHotEncoder(sparse=False, handle_unknown='ignore').fit(df[lst_cols])\n    encoded_cols = list(encoder.get_feature_names(lst_cols))\n    df[encoded_cols] = encoder.transform(df[lst_cols])\n    return df\n","a1708dea":"train_df = pd.read_csv('\/kaggle\/input\/seoul-bike-rental-ai-pro-iti\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/seoul-bike-rental-ai-pro-iti\/test.csv')","994a099f":"# 1- Handle Date\nfor df in [train_df, test_df] :\n    df['Date'] = pd.to_datetime(df['Date'], format='%d\/%m\/%Y')\n    df['Year'] = df['Date'].dt.year.astype('int')\n    df['Month'] = df['Date'].dt.month.astype(int)\n    df['Day'] = df['Date'].dt.day.astype(int)\n    df['Weekday'] = df[\"Date\"].dt.weekday.astype(int)\n#     df['weekofmonth'] = df['Date'].apply(lambda d: (d.day-1) \/\/ 7 + 1)","485c6b1c":"# 2- Hour Status\nfor df in [train_df, test_df] :\n    \n    #df[\"hour_status\"]=[0 if i in [0,1,2,3,4,5,6,7,9,10,11] else 1 for i in df[\"Hour\"]]\n    df.loc[df.Hour.isin([8 , 17 , 18 , 19 , 20 , 21]) , 'hour_status' ]  = 4\n    df.loc[df.Hour.isin([16, 22, 15, 14]) , 'hour_status' ]  = 3\n    df.loc[df.Hour.isin([13, 12 , 23 , 9 , 7 , 11 ]) , 'hour_status' ]  = 2\n    df.loc[df.Hour.isin([0,10,1]) , 'hour_status' ]  = 1\n    df.loc[df.Hour.isin([2,6,3,5,4]) , 'hour_status' ]  = 0\n    df.hour_status = df.hour_status.astype('int')","1b7af200":"# 3- Encodng \nfor df in [train_df, test_df] : \n    df = one_hot_encoding(df,['Seasons'])\n    df['Seasons'] =  df['Seasons'].map({'Winter': 1,'Spring':3,'Autumn':4, 'Summer':5})    \n    df['Holiday'] = df['Holiday'].astype('category').cat.codes\n    df['Functioning Day'] = df['Functioning Day'].astype('category').cat.codes","24c53942":"train_df.head()","f98351b1":"train_df[\"Holiday\"].dtype","0b115ebd":"#  Feature Extraction\nfor df in [train_df, test_df] : \n    df['is_rainy'] = [0 if x == 0 else 1 for x in df['Rainfall(mm)']] \n    df['is_snowy'] = [0 if x == 0 else 1 for x in df['Snowfall (cm)']]\n    df['is_weekend'] = [1 if x in [5,6] else 0 for x in df['Weekday']]  # sat & sun\n\n","cefb7553":"train_df = train_df[train_df['Functioning Day']==1]","954d92c9":"# 3- Drop Outliers\n# train_df = train_df[np.abs(train_df['y']-train_df['y'].mean())<=(3*train_df['y'].std())] \n# train_df = train_df[train_df['Wind speed (m\/s)'] < 7]\ntrain_df = train_df[train_df['Humidity(%)'] != 0.0]\n","2bac16b0":"# havg\ndf_agg = train_df.loc[train_df['is_weekend']== 0].groupby(['Hour']).agg({'y': 'mean'})\/10\nnotweekend = df_agg.apply(lambda x: x.sort_values(ascending=False))\nnotweekend = notweekend['y'].to_dict()\nfor df in [train_df, test_df] :\n    for i in range(0,24):\n        df.loc[(df['Hour'] == i) & (df['is_weekend'] == 0)  ,'havg'] = notweekend[i]","e61a7e29":"# havg\ndf_agg = train_df.loc[train_df['is_weekend']==1].groupby(['Hour']).agg({'y': 'mean'})\/10\nisweekend = df_agg.apply(lambda x: x.sort_values(ascending=False))\nisweekend = isweekend['y'].to_dict()\nfor df in [train_df, test_df] :\n    for i in range(0,24):\n            df.loc[(df['Hour'] == i) & (df['is_weekend'] == 1)  ,'havg'] = isweekend[i]","459eecf7":"numeric_cols = train_df.select_dtypes(include=np.number).columns.tolist()\nnumeric_cols","b0965fdc":"for df in [train_df, test_df]:   \n    df[\"Relative_humidity\"]=100*(np.exp((17.625*df['Dew point temperature(\ufffdC)'])\/(243.04+df['Dew point temperature(\ufffdC)']))\/np.exp((17.625*df['Temperature(\ufffdC)'])\/(243.04+df['Temperature(\ufffdC)'])))\n    df[\"Week of year\"] = df['Date'].dt.isocalendar().week.astype(int)","fe0f1252":"bins = [0, 151, 2001]\nlabels = [1, 0] # 1: low_visibility[0-150], 0: normal visibility [150-2000]\nfor df in [train_df, test_df]:\n    df[\"low_visibility\"] = pd.cut(df[\"Visibility (10m)\"], bins=bins, labels=labels).astype(float)\n    \n","cfc6171c":"# 0: low humidity[0-60], 1: high humidity [60-100]\nfor df in [train_df, test_df]:\n    df[\"High humidity\"] =[1 if i >=35 else 0 for i in df['Humidity(%)']]","cde2bd5c":"for df in [train_df,test_df]:\n    df[\"Wind speed (m\/s)\"]=np.log(df[\"Wind speed (m\/s)\"]+1)\n    df[\"Visibility (10m)\"]=np.log(df[\"Visibility (10m)\"]+1)\n    df[\"Solar Radiation (MJ\/m2)\"]=np.log(df[\"Solar Radiation (MJ\/m2)\"]+1)","7f12913c":"train_df['y'] = np.log(train_df['y']+1)\ntrain_df['y']","6479823c":"import matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\ncorr_matrix = train_df.corr()\nmask = np.triu(np.ones_like(corr_matrix,dtype = bool))\nplt.figure(figsize = (20,10))\nsns.heatmap(corr_matrix, fmt = '0.1f', cmap = 'Blues', mask = mask, annot = True)\nplt.title(\"Correlation Analysis\")","a5e969ea":"from sklearn.ensemble import RandomForestRegressor\nregr = RandomForestRegressor(max_depth=20, random_state=0)\nregr.fit(train_df.drop(columns=['ID','y','Date']), train_df['y'])\n\n\nimportance_df = pd.DataFrame({\n        'feature': train_df.drop(columns=['ID','y','Date']).columns,\n        'importance': regr.feature_importances_\n}).sort_values('importance', ascending=False)\n\n\nplt.title('Feature Importance')\nsns.barplot(data=importance_df.head(len(train_df.columns)-3), x='importance', y='feature');","95dedae2":"train_df.columns","2c755f2e":"#commented means dropped\nkeep_cols = [\n   'ID', \n#     'Date', \n    'y',\n    \"High humidity\",\n#     'Hour_sin',\n#      'Hour_cos',\n#     'Month_sin',\n#     'Month_cos',\n    \"low_visibility\",\n    #\"Day\/Night\",\n    #\"Cool temperature\",\n    #\"Week of year\",\n    #'HI',\n    #'new_hour',\n    'Hour', \n    'Temperature(\ufffdC)', \n    'Humidity(%)',\n    #'Relative_humidity',\n    'Wind speed (m\/s)', \n    #'Visibility (10m)', \n    #'Dew point temperature(\ufffdC)',\n    'Solar Radiation (MJ\/m2)', \n    'Rainfall(mm)', \n    #'Snowfall (cm)', \n    #'Seasons',\n    'Holiday', \n#     'Functioning Day', \n     'Year', \n    'Month', \n    'Day', \n    #'Weekday',\n     #'hour_status', \n    'Seasons_Autumn', \n    'Seasons_Spring', \n    'Seasons_Summer',\n#     'Seasons_Winter', \n    'is_rainy',\n    'havg',\n   #'is_snowy', \n   #'is_weekend'\n]","7858d3d5":"drop_cols = [col for col in train_df.columns if col not in keep_cols]\ndrop_cols","74879d3f":"# Dropping Columns\n\n#to try different combinations of features without doing the preprocessing again:\ntrain = train_df.copy()\ntest = test_df.copy()\n\nfor df in [train, test] :\n    for col in drop_cols:\n        if col in df.columns:\n            df.drop(col, axis=1, inplace = True)  ","fe1ea948":"import sklearn.metrics as metrics","07c39d7e":"rmsle = lambda y_true, y_pred: np.sqrt(mean_squared_log_error(y_true, y_pred))","bfae264f":"rmsle_exp = lambda y_true, y_pred: rmsle(np.exp(y_true),np.exp(y_pred))","33e5bfab":"# for models that have 'scoring' parameter\nscorer = make_scorer(rmsle_exp, greater_is_better=False)","0a40da00":"def plot_feature_importances(model):\n    importance_df = pd.DataFrame({\n        'feature': X_train.columns,\n        'importance': model.feature_importances_\n    }).sort_values('importance', ascending=False)\n\n\n    plt.title('Feature Importance')\n    sns.barplot(data=importance_df.head(len(X_train.columns)), x='importance', y='feature');","79ea572f":"def predict(ml_model):          \n    model=ml_model.fit(X_train,y_train)\n    # print('Training score : {}'.format(model.score(X_train,y_train)))\n    \n    y_prediction=model.predict(X_val)\n    print('predictions are: \\n {}'.format(y_prediction[:10]))\n         \n    res2=rmsle_exp(y_prediction, y_val)\n    print('\\nrmsle: {}'.format(res2))\n    \n    r2_score=metrics.r2_score(y_val,y_prediction)\n    print('r2 score: {}'.format(r2_score))\n    \n    y_pred_train=model.predict(X_train)\n    r2_score=metrics.r2_score(y_train,y_pred_train)\n    print('r2 score on training set: {}'.format(r2_score))\n    \n    print('\\nMAE:',metrics.mean_absolute_error(y_val,y_prediction))\n    print('MSE:',metrics.mean_squared_error(y_val,y_prediction))\n    print('RMSE:',np.sqrt(metrics.mean_squared_error(y_val,y_prediction)))\n    \n    sns.distplot(y_val-y_prediction)\n    return ml_model","0c07b6a7":"train_data = train[~train['Day'].isin([1,2,18,19,20])].drop(columns = 'Day')\nX_train = train_data.drop(columns=['ID', 'y'])\ny_train = train_data['y']\n\nval_data = train[train['Day'].isin([1,2,18,19,20])].drop(columns = 'Day')\nX_val = val_data.drop(columns=['ID', 'y'])\ny_val = val_data['y']","61a3110d":"# from sklearn.model_selection import train_test_split\n\n# train_data, val_data = train_test_split(train, test_size=0.2, random_state=42)\n\n# X_train = train_data.drop(columns=['ID', 'y'])\n# y_train = train_data['y']\n\n# X_val = val_data.drop(columns=['ID', 'y'])\n# y_val = val_data['y']","130760c0":"from sklearn.ensemble import GradientBoostingRegressor\nimport xgboost\nfrom xgboost import XGBRegressor\nxgb = XGBRegressor(random_state=0,\n                             n_estimators=881,\n                             max_depth=15,\n                            eta=0.01,\n                            subsample=.1,\n                            colsample_bytree=.5,\n             num_parallel_tree=1,\n                   eval_metric =\"rmsle\",\n                   objective ='reg:linear',\n                 )\nxgb.fit(X_train, y_train)","3b10cf2b":"rmsle_exp(y_train, xgb.predict(X_train))","cb5631a6":"rmsle_exp(y_val, xgb.predict(X_val))\n","bc451e70":"xgboost.plot_importance(xgb)\n","da4e638d":"from sklearn.ensemble import GradientBoostingRegressor\nimport xgboost\nfrom xgboost import XGBRegressor\nxgb = XGBRegressor(random_state=0,\n                             n_estimators=881,\n                             max_depth=15,\n                            eta=0.01,\n                            subsample=.1,\n                            colsample_bytree=.5,\n             num_parallel_tree=1,\n                   eval_metric =\"rmsle\",\n                   objective ='reg:linear',            \n                 )\nxgb.fit(train.drop(columns=['ID', 'y','Day']), train['y'])","a589c629":"X_test = test.drop(columns=['ID','Day'])\ny_test_predicted = xgb.predict(X_test)\n\n\n# inverse log transformation:\ny_test_predicted = np.exp(y_test_predicted)\n\n\ntest_df['y'] = y_test_predicted\ntest_df['y']=test_df['y'].apply(round)\ntest_df.loc[test_df['Functioning Day']==0,'y']=0\ntest_df.sample(20)","53ec6d62":"test_df[['ID', 'y']].to_csv('\/kaggle\/working\/submission.csv', index=False)","6df2ab85":"## Data Splitting","295d1aeb":"## Columns Dropping","9470e13f":"# Heat Map","417942b8":"## Preprocessing","1f9ce897":"#### Conclusion from the previeus visualization We got many outliers in both 'Season' and 'Holiday' features.","6d4f2412":"## Log Transform of 'y'","dee39b5a":"## Toolkit","7198ddcc":"## Toolkit","7254bc84":"# XGBoost regressor","22ef7d26":"### 3D Correlation ","1bfbbb32":"## Feature Selection using","3623d991":"# Feature Engineering","869e3b08":"# Data assessment","9b2e4703":"# File generating","4992d5d4":"# Model Buildng","4d0290d3":"## Models","01b7a9e6":"#### We don't need to predict or learn about No Functioning days because there is no renting on such days and we don't want to confuse the model","96428e78":"#### Here we can notice that the 'Hour', 'Tempereture' and 'Dew point temperature' have the heighst correlation with 'y'.","109336a7":"# Import Libraries","dc5e24cf":"# Data exploration","2bc44b75":"# xg boost regressor","f0c12804":"# Predicting"}}