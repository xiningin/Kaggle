{"cell_type":{"df716ec5":"code","fcd060c0":"code","b95030c0":"code","14a37126":"code","657425d6":"code","dd035a2b":"code","05c225cc":"code","1ffaaf7e":"code","e4320422":"code","62a3a90c":"code","f663d233":"code","a94e496c":"code","54e1b2b5":"code","9c3f9cdc":"code","1708a1b6":"code","ddc5a5a4":"code","ef6216c6":"code","134c4bdf":"code","0b86b077":"code","d388d94f":"code","3c523098":"code","59b903a5":"code","443e73e4":"code","f3993169":"code","c719cee6":"code","a61a7e55":"code","03fee447":"code","c2295e31":"code","2ecbf31b":"code","ac116230":"code","b951667c":"code","d0cf1435":"markdown","66be74f3":"markdown","68bc1dc4":"markdown","4befbebd":"markdown","55828cee":"markdown","8118a18c":"markdown","642a5aa9":"markdown","5b20bbb5":"markdown","9f3b1907":"markdown","b79d0809":"markdown","d9da0f81":"markdown","60cb7ea2":"markdown","2f1e42cf":"markdown","c8b78778":"markdown","fe9e2f19":"markdown","be853369":"markdown","2cc8fed6":"markdown","88f6ef36":"markdown","2ab635a6":"markdown","ec6dabd5":"markdown","d5ac743a":"markdown"},"source":{"df716ec5":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score\nfrom collections import Counter\nfrom IPython.core.display import display, HTML\nsns.set_style('darkgrid')","fcd060c0":"from sklearn.datasets import load_boston\nboston_dataset = load_boston()\ndataset = pd.DataFrame(boston_dataset.data, columns = boston_dataset.feature_names)","b95030c0":"dataset.head()","14a37126":"dataset['MEDV'] = boston_dataset.target","657425d6":"dataset.head()","dd035a2b":"dataset.isnull().sum()","05c225cc":"X = dataset.iloc[:, 0:13].values\ny = dataset.iloc[:, 13].values.reshape(-1,1)","1ffaaf7e":"# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 25)","e4320422":"print(\"Shape of X_train: \",X_train.shape)\nprint(\"Shape of X_test: \", X_test.shape)\nprint(\"Shape of y_train: \",y_train.shape)\nprint(\"Shape of y_test\",y_test.shape)","62a3a90c":"corr = dataset.corr()\n#Plot figsize\nfig, ax = plt.subplots(figsize=(10, 10))\n#Generate Heat Map, allow annotations and place floats in map\nsns.heatmap(corr, cmap='RdBu', annot=True, fmt=\".2f\")\n#Apply xticks\nplt.xticks(range(len(corr.columns)), corr.columns);\n#Apply yticks\nplt.yticks(range(len(corr.columns)), corr.columns)\n#show plot\nplt.show()","f663d233":"sns.pairplot(dataset)\nplt.show()","a94e496c":"from sklearn.linear_model import LinearRegression\nregressor_linear = LinearRegression()\nregressor_linear.fit(X_train, y_train)","54e1b2b5":"from sklearn.metrics import r2_score\n\n# Predicting Cross Validation Score the Test set results\ncv_linear = cross_val_score(estimator = regressor_linear, X = X_train, y = y_train, cv = 10)\n\n# Predicting R2 Score the Train set results\ny_pred_linear_train = regressor_linear.predict(X_train)\nr2_score_linear_train = r2_score(y_train, y_pred_linear_train)\n\n# Predicting R2 Score the Test set results\ny_pred_linear_test = regressor_linear.predict(X_test)\nr2_score_linear_test = r2_score(y_test, y_pred_linear_test)\n\n# Predicting RMSE the Test set results\nrmse_linear = (np.sqrt(mean_squared_error(y_test, y_pred_linear_test)))\nprint(\"CV: \", cv_linear.mean())\nprint('R2_score (train): ', r2_score_linear_train)\nprint('R2_score (test): ', r2_score_linear_test)\nprint(\"RMSE: \", rmse_linear)","9c3f9cdc":"from sklearn.preprocessing import PolynomialFeatures\npoly_reg = PolynomialFeatures(degree = 2)\nX_poly = poly_reg.fit_transform(X_train)\npoly_reg.fit(X_poly, y_train)\nregressor_poly2 = LinearRegression()\nregressor_poly2.fit(X_poly, y_train)","1708a1b6":"from sklearn.metrics import r2_score\n\n# Predicting Cross Validation Score the Test set results\ncv_poly2 = cross_val_score(estimator = regressor_poly2, X = X_train, y = y_train, cv = 10)\n\n# Predicting R2 Score the Train set results\ny_pred_poly2_train = regressor_poly2.predict(poly_reg.fit_transform(X_train))\nr2_score_poly2_train = r2_score(y_train, y_pred_poly2_train)\n\n# Predicting R2 Score the Test set results\ny_pred_poly2_test = regressor_poly2.predict(poly_reg.fit_transform(X_test))\nr2_score_poly2_test = r2_score(y_test, y_pred_poly2_test)\n\n# Predicting RMSE the Test set results\nrmse_poly2 = (np.sqrt(mean_squared_error(y_test, y_pred_poly2_test)))\nprint('CV: ', cv_poly2.mean())\nprint('R2_score (train): ', r2_score_poly2_train)\nprint('R2_score (test): ', r2_score_poly2_test)\nprint(\"RMSE: \", rmse_poly2)","ddc5a5a4":"from sklearn.linear_model import Ridge\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\n\nsteps = [\n    ('scalar', StandardScaler()),\n    ('poly', PolynomialFeatures(degree=2)),\n    ('model', Ridge(alpha=3.8, fit_intercept=True))\n]\n\nridge_pipe = Pipeline(steps)\nridge_pipe.fit(X_train, y_train)","ef6216c6":"from sklearn.metrics import r2_score\n\n# Predicting Cross Validation Score the Test set results\ncv_ridge = cross_val_score(estimator = ridge_pipe, X = X_train, y = y_train.ravel(), cv = 10)\n\n# Predicting R2 Score the Test set results\ny_pred_ridge_train = ridge_pipe.predict(X_train)\nr2_score_ridge_train = r2_score(y_train, y_pred_ridge_train)\n\n# Predicting R2 Score the Test set results\ny_pred_ridge_test = ridge_pipe.predict(X_test)\nr2_score_ridge_test = r2_score(y_test, y_pred_ridge_test)\n\n# Predicting RMSE the Test set results\nrmse_ridge = (np.sqrt(mean_squared_error(y_test, y_pred_ridge_test)))\nprint('CV: ', cv_ridge.mean())\nprint('R2_score (train): ', r2_score_ridge_train)\nprint('R2_score (test): ', r2_score_ridge_test)\nprint(\"RMSE: \", rmse_ridge)","134c4bdf":"from sklearn.linear_model import Lasso\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\n\nsteps = [\n    ('scalar', StandardScaler()),\n    ('poly', PolynomialFeatures(degree=2)),\n    ('model', Lasso(alpha=0.012, fit_intercept=True, max_iter=3000))\n]\n\nlasso_pipe = Pipeline(steps)\nlasso_pipe.fit(X_train, y_train)","0b86b077":"from sklearn.metrics import r2_score\n\n# Predicting Cross Validation Score\ncv_lasso = cross_val_score(estimator = lasso_pipe, X = X_train, y = y_train, cv = 10)\n\n# Predicting R2 Score the Test set results\ny_pred_lasso_train = lasso_pipe.predict(X_train)\nr2_score_lasso_train = r2_score(y_train, y_pred_lasso_train)\n\n# Predicting R2 Score the Test set results\ny_pred_lasso_test = lasso_pipe.predict(X_test)\nr2_score_lasso_test = r2_score(y_test, y_pred_lasso_test)\n\n# Predicting RMSE the Test set results\nrmse_lasso = (np.sqrt(mean_squared_error(y_test, y_pred_lasso_test)))\nprint('CV: ', cv_lasso.mean())\nprint('R2_score (train): ', r2_score_lasso_train)\nprint('R2_score (test): ', r2_score_lasso_test)\nprint(\"RMSE: \", rmse_lasso)","d388d94f":"# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc_X = StandardScaler()\nsc_y = StandardScaler()\nX_scaled = sc_X.fit_transform(X_train)\ny_scaled = sc_y.fit_transform(y_train.reshape(-1,1))","3c523098":"# Fitting the SVR Model to the dataset\nfrom sklearn.svm import SVR\nregressor_svr = SVR(kernel = 'rbf', gamma = 'scale')\nregressor_svr.fit(X_scaled, y_scaled.ravel())","59b903a5":"from sklearn.metrics import r2_score\n\n# Predicting Cross Validation Score\ncv_svr = cross_val_score(estimator = regressor_svr, X = X_scaled, y = y_scaled.ravel(), cv = 10)\n\n# Predicting R2 Score the Train set results\ny_pred_svr_train = sc_y.inverse_transform(regressor_svr.predict(sc_X.transform(X_train)))\nr2_score_svr_train = r2_score(y_train, y_pred_svr_train)\n\n# Predicting R2 Score the Test set results\ny_pred_svr_test = sc_y.inverse_transform(regressor_svr.predict(sc_X.transform(X_test)))\nr2_score_svr_test = r2_score(y_test, y_pred_svr_test)\n\n# Predicting RMSE the Test set results\nrmse_svr = (np.sqrt(mean_squared_error(y_test, y_pred_svr_test)))\nprint('CV: ', cv_svr.mean())\nprint('R2_score (train): ', r2_score_svr_train)\nprint('R2_score (test): ', r2_score_svr_test)\nprint(\"RMSE: \", rmse_svr)","443e73e4":"# Fitting the Decision Tree Regression Model to the dataset\nfrom sklearn.tree import DecisionTreeRegressor\nregressor_dt = DecisionTreeRegressor(random_state = 0)\nregressor_dt.fit(X_train, y_train)","f3993169":"from sklearn.metrics import r2_score\n\n# Predicting Cross Validation Score\ncv_dt = cross_val_score(estimator = regressor_dt, X = X_train, y = y_train, cv = 10)\n\n# Predicting R2 Score the Train set results\ny_pred_dt_train = regressor_dt.predict(X_train)\nr2_score_dt_train = r2_score(y_train, y_pred_dt_train)\n\n# Predicting R2 Score the Test set results\ny_pred_dt_test = regressor_dt.predict(X_test)\nr2_score_dt_test = r2_score(y_test, y_pred_dt_test)\n\n# Predicting RMSE the Test set results\nrmse_dt = (np.sqrt(mean_squared_error(y_test, y_pred_dt_test)))\nprint('CV: ', cv_dt.mean())\nprint('R2_score (train): ', r2_score_dt_train)\nprint('R2_score (test): ', r2_score_dt_test)\nprint(\"RMSE: \", rmse_dt)","c719cee6":"# Fitting the Random Forest Regression to the dataset\nfrom sklearn.ensemble import RandomForestRegressor\nregressor_rf = RandomForestRegressor(n_estimators = 500, random_state = 0)\nregressor_rf.fit(X_train, y_train.ravel())","a61a7e55":"from sklearn.metrics import r2_score\n\n# Predicting Cross Validation Score\ncv_rf = cross_val_score(estimator = regressor_rf, X = X_scaled, y = y_train.ravel(), cv = 10)\n\n# Predicting R2 Score the Train set results\ny_pred_rf_train = regressor_rf.predict(X_train)\nr2_score_rf_train = r2_score(y_train, y_pred_rf_train)\n\n# Predicting R2 Score the Test set results\ny_pred_rf_test = regressor_rf.predict(X_test)\nr2_score_rf_test = r2_score(y_test, y_pred_rf_test)\n\n# Predicting RMSE the Test set results\nrmse_rf = (np.sqrt(mean_squared_error(y_test, y_pred_rf_test)))\nprint('CV: ', cv_rf.mean())\nprint('R2_score (train): ', r2_score_rf_train)\nprint('R2_score (test): ', r2_score_rf_test)\nprint(\"RMSE: \", rmse_rf)","03fee447":"models = [('Linear Regression', rmse_linear, r2_score_linear_train, r2_score_linear_test, cv_linear.mean()),\n          ('Polynomial Regression (2nd)', rmse_poly2, r2_score_poly2_train, r2_score_poly2_test, cv_poly2.mean()),\n          ('Ridge Regression', rmse_ridge, r2_score_ridge_train, r2_score_ridge_test, cv_ridge.mean()),\n          ('Lasso Regression', rmse_lasso, r2_score_lasso_train, r2_score_lasso_test, cv_lasso.mean()),\n          ('Support Vector Regression', rmse_svr, r2_score_svr_train, r2_score_svr_test, cv_svr.mean()),\n          ('Decision Tree Regression', rmse_dt, r2_score_dt_train, r2_score_dt_test, cv_dt.mean()),\n          ('Random Forest Regression', rmse_rf, r2_score_rf_train, r2_score_rf_test, cv_rf.mean())   \n         ]","c2295e31":"predict = pd.DataFrame(data = models, columns=['Model', 'RMSE', 'R2_Score(training)', 'R2_Score(test)', 'Cross-Validation'])\npredict","2ecbf31b":"f, axe = plt.subplots(1,1, figsize=(18,6))\n\npredict.sort_values(by=['Cross-Validation'], ascending=False, inplace=True)\n\nsns.barplot(x='Cross-Validation', y='Model', data = predict, ax = axe)\n#axes[0].set(xlabel='Region', ylabel='Charges')\naxe.set_xlabel('Cross-Validaton Score', size=16)\naxe.set_ylabel('Model')\naxe.set_xlim(0,1.0)\nplt.show()","ac116230":"f, axes = plt.subplots(2,1, figsize=(14,10))\n\npredict.sort_values(by=['R2_Score(training)'], ascending=False, inplace=True)\n\nsns.barplot(x='R2_Score(training)', y='Model', data = predict, palette='Blues_d', ax = axes[0])\n#axes[0].set(xlabel='Region', ylabel='Charges')\naxes[0].set_xlabel('R2 Score (Training)', size=16)\naxes[0].set_ylabel('Model')\naxes[0].set_xlim(0,1.0)\n\npredict.sort_values(by=['R2_Score(test)'], ascending=False, inplace=True)\n\nsns.barplot(x='R2_Score(test)', y='Model', data = predict, palette='Reds_d', ax = axes[1])\n#axes[0].set(xlabel='Region', ylabel='Charges')\naxes[1].set_xlabel('R2 Score (Test)', size=16)\naxes[1].set_ylabel('Model')\naxes[1].set_xlim(0,1.0)\n\nplt.show()","b951667c":"predict.sort_values(by=['RMSE'], ascending=False, inplace=True)\n\nf, axe = plt.subplots(1,1, figsize=(18,6))\nsns.barplot(x='Model', y='RMSE', data=predict, ax = axe)\naxe.set_xlabel('Model', size=16)\naxe.set_ylabel('RMSE', size=16)\n\nplt.show()","d0cf1435":"### <span id=\"7\"><\/span> ** Linear Regression **","66be74f3":"Columns:\n- **CRIM: ** Per capita crime rate by town\n- **ZN: ** Proportion of residential land zoned for lots over 25,000 sq. ft\n- **INDUS: ** Proportion of non-retail business acres per town\n- **CHAS : ** Charles River dummy variable (1 if tract bounds river; 0 otherwise)\n- **NOX: ** Nitric oxide concentration (parts per 10 million)\n- **RM: ** Average number of rooms per dwelling\n- **AGE: ** Proportion of owner-occupied units built prior to 1940\n- **DIS: ** Weighted distances to five Boston employment centers\n- **RAD: ** Index of accessibility to radial highways\n- **PTRATIO: ** Pupil-teacher ratio by town\n- **B: ** 1000(Bk \u2014 0.63)\u00b2, where Bk is the proportion of [people of African American descent] by town\n- **LSTAT: ** Percentage of lower status of the population\n- **MEDV: ** Median value of owner-occupied homes in $1000s","68bc1dc4":"## <span id=\"2\"><\/span> ** 2. Importing Libraries and Reading the Dataset **","4befbebd":"As you seen, there isn't \"MEDV\" column that we will try to predict. Let's add the column to our dataset.","55828cee":"### <span id=\"8\"><\/span> ** Polynomial Regression - 2nd degree **","8118a18c":"## <span id=\"3\"><\/span> ** 3. Data Analysis **","642a5aa9":"In this kernel, I have built 7 regression models using Boston Housing Dataset. These are linear, polynomial, ridge, lasso,  svr, decision tree and random forest regression. Then measured and visualized the performance of the models. Please make a comment and let me know how to improve model performance, visualization or something in this kernel. This will also help me on my future analysis.\n\n<b><font color=\"red\">Don't forget to <\/font><\/b> <b><font color=\"green\">UPVOTE <\/font><\/b> if you liked this kernel, thank you. \ud83d\ude42\ud83d\udc4d","5b20bbb5":"### <span id=\"4\"><\/span> ** Data Preprocessing **","9f3b1907":"### <span id=\"15\"><\/span> ** Visualizing Model Performance **","b79d0809":"### <span id=\"10\"><\/span> ** Lasso Regression **","d9da0f81":"Are there missing values? There isn't any missing values as shown below.","60cb7ea2":"### <span id=\"5\"><\/span> ** Visualizing Data **","2f1e42cf":"### <span id=\"12\"><\/span> ** Decision Tree Regression **","c8b78778":"## <span id=\"1\"><\/span> ** 1. Overview **","fe9e2f19":"### <span id=\"9\"><\/span> ** Ridge Regression **","be853369":"## <span id=\"6\"><\/span> ** 4. Regression Models **","2cc8fed6":"## <span id=\"14\"><\/span> ** 5. Measuring the Error **","88f6ef36":"### <span id=\"13\"><\/span> ** Random Forest Regression **","2ab635a6":"### <span id=\"11\"><\/span> ** Support Vector Regression **","ec6dabd5":"<hr\/>\n[**Tolgahan Cepel**](https:\/\/www.kaggle.com\/tolgahancepel)\n<hr\/>\n<font color=green>\n1. [Overview](#1)\n1. [Importing Libraries and Reading the Dataset](#2)\n1. [Data Analysis](#3) \n    * [Data Preprocessing](#4) \n    * [Visualizing Data](#5) \n1. [Regression Models](#6) \n    * [Linear Regression](#7) \n    * [Polynomial Regression - 2nd degree](#8)\n    * [Ridge Regression](#9)\n    * [Lasso Regression](#10)\n    * [Support Vector Regression](#11)\n    * [Decision Tree Regression](#12) \n    * [Random Forest Regression](#13)\n1. [Measuring the Error](#14)\n    * [Visualizing Models Performance](#15)\n1. [Conclusion](#16)\n<hr\/>","d5ac743a":"## <span id=\"16\"><\/span> ** 6. Conclusion **"}}