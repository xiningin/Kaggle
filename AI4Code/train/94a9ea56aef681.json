{"cell_type":{"7100cd60":"code","c58d42ce":"code","6838d818":"code","c51fe017":"code","9db7c1ad":"code","3071e960":"code","fa007651":"code","419c1ba5":"code","a94b6b89":"code","354760c0":"code","af133cc5":"code","104bb03e":"code","34220c25":"code","dd72800e":"code","c94216fd":"code","e05196c4":"code","409d8839":"code","2b1e457f":"code","070dd65c":"code","96d4a9e8":"code","c697590f":"code","a55ec012":"code","122af956":"code","94d45894":"code","9397b8d5":"code","4bfcb98a":"markdown","6a87df42":"markdown","a12653e2":"markdown","1098736f":"markdown","8f71f0e8":"markdown","6e5874ce":"markdown","c6adcd19":"markdown","47463f99":"markdown","c1a9da05":"markdown","096d753c":"markdown","9cdd6caa":"markdown","cd126b8c":"markdown"},"source":{"7100cd60":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nprint(tf.__version__)\nimport os\nimport shutil\nimport matplotlib.pyplot as plt","c58d42ce":"train = pd.read_csv('..\/input\/plant-pathology-2020-fgvc7\/train.csv')\ntest = pd.read_csv('..\/input\/plant-pathology-2020-fgvc7\/test.csv')\n\ntarget = train[['healthy', 'multiple_diseases', 'rust', 'scab']]\ntest_ids = test['image_id']\n\ntrain_len = train.shape[0]\ntest_len = test.shape[0]\n\ntrain.describe()","6838d818":"print(\"Shape of train data: \" + str(train.shape))\nprint(\"Shape of test data: \" + str(test.shape))\n\ntrain_len = train.shape[0]\ntest_len = test.shape[0]","c51fe017":"from keras.preprocessing.image import load_img\nfrom keras.preprocessing.image import img_to_array\nfrom tqdm.notebook import tqdm\n\npath = '..\/input\/plant-pathology-2020-fgvc7\/images\/'\nsize = 224\n\ntrain_images = np.ndarray(shape=(train_len, size, size, 3))\nfor i in tqdm(range(train_len)):\n  img = load_img(path + f'Train_{i}.jpg', target_size=(size, size))\n  train_images[i] = np.uint8(img_to_array(img))\n\ntest_images = np.ndarray(shape=(test_len, size, size, 3))\nfor i in tqdm(range(test_len)):\n  img = load_img(path + f'Test_{i}.jpg', target_size=(size, size))\n  test_images[i] = np.uint8(img_to_array(img))\n\ntrain_images.shape, test_images.shape","9db7c1ad":"for i in range(4):\n\tplt.subplot(220 + 1 + i)\n\tplt.title(train['image_id'][i])\n\tplt.imshow(np.uint8(train_images[i]))\nplt.show()","3071e960":"for i in range(4):\n\tplt.subplot(220 + 1 + i)\n\tplt.title(test['image_id'][i])\n\tplt.imshow(np.uint8(test_images[i]))\nplt.show()\nplt.savefig('test_images.png')","fa007651":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(train_images, target.to_numpy(), test_size=0.1, random_state=289) \n\nx_train.shape, x_test.shape, y_train.shape, y_test.shape","419c1ba5":"from imblearn.over_sampling import RandomOverSampler\n\nros = RandomOverSampler(random_state=289)\n\nx_train, y_train = ros.fit_resample(x_train.reshape((-1, size * size * 3)), y_train)\nx_train = x_train.reshape((-1, size, size, 3))\nx_train.shape, y_train.shape","a94b6b89":"import gc\n\ndel train_images\ngc.collect()","354760c0":"from keras_preprocessing.image import ImageDataGenerator\n\nbatch_size = 8\n\ntrain_datagen = ImageDataGenerator(samplewise_center = True,\n                                   samplewise_std_normalization = True,\n                                   horizontal_flip = True,\n                                   vertical_flip = True,\n                                   rotation_range=20)\n\ntrain_generator = train_datagen.flow(\n    x = x_train, \n    y = y_train,\n    batch_size = batch_size)\n\nvalidation_datagen = ImageDataGenerator(samplewise_center = True,\n                                        samplewise_std_normalization = True)\n\nvalidation_generator = validation_datagen.flow(\n    x = x_test, \n    y = y_test,\n    batch_size = batch_size)","af133cc5":"idx = np.random.randint(8)\nx, y = train_generator.__getitem__(idx)\nplt.title(y[idx])\nplt.imshow(x[idx])\nplt.savefig('processed_img.png')","104bb03e":"base_model = tf.keras.applications.ResNet50(include_top = False, weights='imagenet', input_shape=(size, size, 3))\n\ndef create_model():\n    model = tf.keras.Sequential([\n      base_model,\n      tf.keras.layers.GlobalAveragePooling2D(),\n      tf.keras.layers.Dense(4, activation='softmax')\n      ])\n    model.compile(\n        loss = 'kullback_leibler_divergence', \n        optimizer = 'adam', \n        metrics = ['accuracy'])\n    return model\n\nmodel = create_model()\n\nmodel.summary()","34220c25":"epochs = 100\nsteps_per_epoch = x_train.shape[0] \/\/ batch_size\nvalidation_steps = x_test.shape[0] \/\/ batch_size\nprint(steps_per_epoch)","dd72800e":"es = tf.keras.callbacks.EarlyStopping(patience=15, restore_best_weights=True, verbose=1)\nmc = tf.keras.callbacks.ModelCheckpoint('model.hdf5', save_best_only=True, verbose=0)\nrlr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', patience=10, verbose=1)\n\nstart_lr = 0.00001\nmin_lr = 0.00001\nmax_lr = 0.00005\nrampup_epochs = 20\nsustain_epochs = 15\nexp_decay = .8\n\ndef lrfn(epoch):\n  if epoch < rampup_epochs:\n    return (max_lr - start_lr)\/rampup_epochs * epoch + start_lr\n  elif epoch < rampup_epochs + sustain_epochs:\n    return max_lr\n  else:\n    return min_lr\n    \nlr = tf.keras.callbacks.LearningRateScheduler(lambda epoch: lrfn(epoch), verbose=True)\n\nrang = np.arange(epochs)\ny = [lrfn(x) for x in rang]\nplt.plot(rang, y)\nprint('Learning rate per epoch:')","c94216fd":"history = model.fit(\n    x = train_generator,  \n    validation_data = validation_generator,\n    epochs = epochs,\n    steps_per_epoch = steps_per_epoch,\n    validation_steps = validation_steps,\n    verbose=1,\n    callbacks=[es, lr, mc, rlr])","e05196c4":"# Plot training & validation accuracy values\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n# Plot training & validation loss values\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper right')\nplt.show()","409d8839":"train_err = (1-history.history['accuracy'][-1])*100\nvalidation_err = (1-history.history['val_accuracy'][-1])*100\nprint(\"Train set error \" + str(train_err))\nprint(\"Validation set error \" + str(validation_err))","2b1e457f":"test_datagen = ImageDataGenerator(samplewise_center = True,\n                                 samplewise_std_normalization = True)\n\ntest_generator = test_datagen.flow(\n    x = test_images,\n    shuffle = False)","070dd65c":"probabilities = model.predict(test_generator, steps = len(test_generator))\nprint(probabilities[:,0].mean()*100)\nprint(probabilities[:,1].mean()*100)\nprint(probabilities[:,2].mean()*100)\nprint(probabilities[:,3].mean()*100)","96d4a9e8":"base_model.trainable = True\nmodel.summary()","c697590f":"model.compile(\n        loss = 'kullback_leibler_divergence', \n        optimizer = tf.keras.optimizers.Adam(1e-5), \n        metrics = ['accuracy'])","a55ec012":"es = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss',\n    patience=15,\n    restore_best_weights=True)","122af956":"epochs = 10\nhistory = model.fit(\n    x = train_generator,  \n    validation_data = validation_generator,\n    epochs = epochs,\n    steps_per_epoch = steps_per_epoch,\n    validation_steps = validation_steps,\n    verbose=1,\n    callbacks = [es])","94d45894":"# Plot training & validation accuracy values\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n# Plot training & validation loss values\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper right')\nplt.show()","9397b8d5":"train_err = (1-history.history['accuracy'][-1])*100\nvalidation_err = (1-history.history['val_accuracy'][-1])*100\nprint(\"Train set error \" + str(train_err))\nprint(\"Validation set error \" + str(validation_err))","4bfcb98a":"# Keras Model\nLet's create the model. Here we use ResNet50 for image classification as our base model. The last two layers are not included and these two layers are trained. ResNet50 is built to classify between 1000 classes, but we only need 4. The rest of the layers use pre-trained weights, which we will fine-tune later on once we get a convergence on the last two layers.","6a87df42":"Let's split out data into train and test sets for the model.","a12653e2":"# Results\nWe plot the train and validation accuracy and loss to see how the model did over the epochs.","1098736f":"Ah, we see the multiple_diseases label has drastically less images than the rest of the labels. Once we load the images in raw data form, we'll use scikitlearn to randomly over sample so we can fix this class imbalance.\n\nNow let's load the image data.\n","8f71f0e8":"# Loading Data and Preprocessing\n\nHere we load the data and take a look at what we're dealing with.","6e5874ce":"# Fine Tuning the Model\nHere we fine tune the model by unfreezing the layers in the pre-trained ResNet50 model by setting the base_model to trainable.","c6adcd19":" Let's take a look at what the images look like.","47463f99":"# Prediction\n\nHere we feed the test image set into the model.predict function and see how our model does.","c1a9da05":"Now use RandomOverSampler to fix our class imbalance in the multiple diseases class.","096d753c":"Now we prepare the data for going into a Keras deep learning model. Here I use the ImageDataGenerator to also give us more images by using the parameters to rotate, horizontally flip, and vertically flip. Also the image is rescaled by 1\/255 to normalize the raw data so that the activation functions work properly.","9cdd6caa":"Let's set up some callbacks. \nCallbacks:\n\n**EarlyStopping** - stop early if the validation loss has stopped improving\n\n**ModelCheckpoint** - save the model every epoch and save the best weights\n\n**ReduceLROnPlateau** - reduce learning rate when validation loss has stopped improving\n\n**LearningRateScheduler** - set learning rate to ramp up during early epochs","cd126b8c":"# Plant Pathology 2020 - FGVC7\n\nIdentify the category of foliar diseases in apple trees\n\nKaggle competition - https:\/\/www.kaggle.com\/c\/plant-pathology-2020-fgvc7\/submit\n"}}