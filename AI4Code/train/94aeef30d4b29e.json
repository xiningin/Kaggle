{"cell_type":{"54b82e14":"code","b7a3c759":"code","d7994678":"code","82aba244":"code","5ab13e0b":"code","43c19e3d":"code","4e2c82cb":"code","11d98b99":"code","942e1a3a":"code","16cc1632":"code","0ce079f7":"code","ae5682da":"code","484e772c":"code","9d2e4de5":"code","1c069a4a":"code","c23c0608":"code","074a39cd":"code","0bca8bf1":"code","fa66689a":"code","70894eee":"code","57e1845c":"code","37400a2a":"code","7852fb39":"code","dba76fb2":"code","fc9b13d5":"code","44fa651a":"code","a8d176fa":"code","ebbd42f5":"code","c0b65638":"code","35dfc190":"code","fd0bbfa7":"code","8a225880":"code","488c17ab":"code","d34fcc88":"code","5ca69307":"code","3559eba4":"code","d23f8e55":"code","ac810b3a":"code","05a1be2e":"code","5c1108cc":"code","2a7eee67":"code","a7c9d80f":"code","e59a9fd9":"code","38e0ccc9":"code","048b59d6":"code","912e70aa":"code","ee81d1af":"code","55f523c5":"code","3e70b366":"code","7b617a76":"code","86186117":"code","d96da0bc":"code","784f9a30":"code","e8d4235c":"code","0a65bbba":"code","8508bfc9":"code","2ba77dcf":"code","43109b92":"code","03f3f07d":"code","b1963b59":"code","caa411aa":"code","3924cbc1":"code","8db7449d":"code","12e05b9b":"code","a48f229b":"code","9765167f":"code","72830814":"code","281ba657":"code","548c26fe":"code","fe64b195":"code","ca9a0d7e":"code","839dd52a":"code","d5306acf":"code","85d9af64":"code","f50cee00":"code","beb884b3":"code","9dba9da2":"code","69a53bd6":"code","67232f75":"code","9b81f943":"code","1259537b":"code","2b15d664":"code","6e1b47eb":"code","10064132":"code","d258f225":"code","24a28a19":"code","277d3ba2":"code","a86c1492":"code","9f521b0d":"code","72d0e04e":"code","bc6933da":"code","66e66594":"code","5d572557":"code","0b0a8b8c":"code","18ba8338":"code","ed946059":"code","d685a74e":"code","0013d44a":"code","05624d14":"code","02c69198":"code","9cc29d31":"code","9c9c1e59":"code","e5d2cf59":"code","1977da9c":"code","ee8b5826":"code","9e451a30":"code","70769f20":"code","f3989c43":"code","b87d8a21":"code","d26280e4":"code","cf38e321":"code","edac2a48":"code","4bbe1b7f":"code","7f7ce8cb":"code","38c3b0ba":"code","ecda9edf":"code","7968354b":"code","ac6f33d9":"code","115bab93":"code","4ddf3de7":"code","0efe97a8":"code","02086807":"code","065985ee":"code","ec3d4661":"code","2bde79b4":"code","60401a96":"code","30067e06":"code","ef26bbff":"markdown","4cf9be79":"markdown","a9715eff":"markdown","ff41bd8c":"markdown","908aca81":"markdown","9d020097":"markdown","c1bdbe53":"markdown","0ae3c73e":"markdown","4f9c0ef7":"markdown","91add802":"markdown","ad8b945e":"markdown","ee57c7df":"markdown","84e1b138":"markdown","9d887830":"markdown","f076ce2b":"markdown","1d430147":"markdown","d7349818":"markdown","3bf0b999":"markdown","83c5431c":"markdown","c68b5d5a":"markdown","1b1cfd4d":"markdown","620f14a0":"markdown","d141bea0":"markdown","7382c6ad":"markdown","0ca56ed8":"markdown","9a2533a3":"markdown","6b096e9b":"markdown","14146428":"markdown","48699a36":"markdown","5300d3e2":"markdown","d5ded783":"markdown","8de27531":"markdown","8682076b":"markdown","908cd9ed":"markdown","51d2ca02":"markdown","66bf3d75":"markdown","2418f5d4":"markdown","452be6f4":"markdown","1b8dfcb3":"markdown","b6825061":"markdown","3f4d7886":"markdown","6c8b301d":"markdown","992f88e1":"markdown","28463079":"markdown","1b145fec":"markdown","9512f493":"markdown","9ca848c6":"markdown","bfaf7bf6":"markdown","efbe884f":"markdown","aa511e0c":"markdown","570b594b":"markdown","a026cc14":"markdown","8583c65b":"markdown","215a656e":"markdown","fa666090":"markdown","b84aadf6":"markdown","8f78fe85":"markdown","ffae61d0":"markdown","0da7eafc":"markdown","7d75d056":"markdown","eb6cbccf":"markdown","ad9c546a":"markdown","ff248fb2":"markdown","7d5dd9da":"markdown","a6ca26f7":"markdown","76bf9894":"markdown"},"source":{"54b82e14":"import pandas as pd\nimport numpy as np\nimport seaborn as sb\nimport matplotlib.pyplot as plt\nimport math\nimport re\nimport time\nimport warnings\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\nfrom nltk.corpus import stopwords\n\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.preprocessing import normalize\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.manifold import TSNE\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import confusion_matrix, normalized_mutual_info_score, accuracy_score, log_loss\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import StratifiedKFold, train_test_split, GridSearchCV\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.naive_bayes import MultinomialNB, GaussianNB\n\nfrom imblearn.over_sampling import SMOTE\n\nfrom collections import Counter, defaultdict\n\nfrom scipy.sparse import hstack\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom mlxtend.classifier import StackingClassifier","b7a3c759":"#training_variants is a comma separated file\ntraining_variants_df = pd.read_csv('..\/input\/training_variants')\n\n#training_text is separated by \/\/ delimeter\ntraining_text_df = pd.read_csv('..\/input\/training_text', sep = \"\\|\\|\", engine = \"python\", names = \n                              [\"ID\", \"TEXT\"], skiprows = 1)","d7994678":"#checking the first rows for training variants\ntraining_variants_df.head()","82aba244":"#checking the shape of the data for training_Variants\ntraining_variants_df.shape","5ab13e0b":"#getting info from the file\ntraining_variants_df.info()","43c19e3d":"#checking the first rows for training_text\ntraining_text_df.head()","4e2c82cb":"#getting info from training_text\ntraining_text_df.info()","11d98b99":"training_variants_df.Class.unique()","942e1a3a":"#removing the stop words from the text, I will be using the natural language toolkit to help me\nstop_words = set(stopwords.words('english'))","16cc1632":"#create a function to preprocess it\ndef data_preprocess(input_text, ind, col):\n    #remove int values from the text data\n    if type(input_text) is not int:\n        string = \"\"\n        #replacing special characters with space\n        input_text = re.sub('[^a-zA-Z0-9\\n]', ' ', str(input_text))\n        #replacing multiple spaces with single space\n        input_text = re.sub('\\s+', ' ', str(input_text))\n        #make all lower case\n        input_text = input_text.lower()\n        \n        for word in input_text.split():\n            #keep everything but the stop words\n            if not word in stop_words:\n                string += word + \" \"\n        \n        training_text_df[col][ind] = string","0ce079f7":"for index, row, in training_text_df.iterrows():\n    if type(row['TEXT']) is str:\n        data_preprocess(row['TEXT'], index, 'TEXT')","ae5682da":"training_merged = pd.merge(training_variants_df, training_text_df,on=\"ID\",how = 'left')\ntraining_merged.head()\n","484e772c":"#since I am checking along the columns I will pass axis = 1\ntraining_merged[training_merged.isnull().any(axis = 1)]","9d2e4de5":"training_merged.loc[training_merged['TEXT'].isnull(), 'TEXT'] = training_merged['Gene'] + ' ' + training_merged['Variation']","1c069a4a":"training_merged[training_merged.isnull().any(axis = 1)]","c23c0608":"#making sure the columns gene and variation have no spaces in them, replacing them with underscore\ny_real = training_merged['Class'].values\ntraining_merged.Gene = training_merged.Gene.str.replace('\\s+', '_')\ntraining_merged.Variation = training_merged.Variation.str.replace('\\s+', '_')","074a39cd":"#splitting data into test set\nX_train, x_test, y_train, y_test = train_test_split(training_merged, y_real, stratify = y_real, test_size = 0.2)\n\n#splitting the data into training set\nX_train, x_crossval, y_train, y_crossval = train_test_split(X_train, y_train, stratify = y_train, test_size = 0.2)\n","0bca8bf1":"print('Data points in X_train: ', X_train.shape[0])\nprint('Data points in x_test: ', x_test.shape[0])\nprint('Data points in cross-val: ', x_crossval.shape[0])","fa66689a":"#checking how data was distributed\ntrain_class_distributed = X_train['Class'].value_counts().sort_index()\ntest_class_distributed = x_test['Class'].value_counts().sort_index()\ncrossval_class_distributed = x_crossval['Class'].value_counts().sort_index()","70894eee":"for a, b, c in zip(train_class_distributed, test_class_distributed, crossval_class_distributed):\n    print(a, b, c)","57e1845c":"#visualizing the distribution\ntrain_class_distributed.plot(kind = 'bar')\n\nplt.xlabel('Class')\nplt.ylabel('Data Points per Class')\nplt.title('Y distribution in train')\nplt.show()","37400a2a":"#checking the distribution per percentage of data in the classes\nsorted_y = np.argsort(-train_class_distributed.values)\nfor i in sorted_y:\n    print('Number of data points in class', i+1, ':',train_class_distributed.values[i], '(', np.round((train_class_distributed.values[i]\/X_train.shape[0]*100), 3), '%)')\n\nn_data_points = []\nn_class = []\npercent_data = []\nfor i in sorted_y:\n    n_class.append(i+1)\n    n_data_points.append(train_class_distributed.values[i])\n    percent_data.append(np.round((train_class_distributed.values[i]\/X_train.shape[0]*100), 3))\n","7852fb39":"#plotting the results for the distribution\nfigureObject, axesObject = plt.subplots()\naxesObject.pie(percent_data, labels = n_class, autopct='%1.1f', startangle=90)\ncentre_circle = plt.Circle((0,0),0.70,fc='white')\nfig = plt.gcf()\nfig.gca().add_artist(centre_circle)\nwedges, texts = plt.pie(percent_data, shadow=True, startangle=90)\naxesObject.axis('equal')\nplt.tight_layout()\nplt.show()","dba76fb2":"#checking the distribution per percentage of data in the classes, trying to get fancy with my graphs, failing but still feeling pretty proud lol\n#need to come back to this after I am not saturated of looking at it\nsorted_y = np.argsort(-train_class_distributed.values)\nfor i in sorted_y:\n    print('Number of data points in class', i+1, ':',train_class_distributed.values[i], '(', np.round((train_class_distributed.values[i]\/X_train.shape[0]*100), 3), '%)')\n\nn_data_points = []\nn_class = []\npercent_data = []\nfor i in sorted_y:\n    n_class.append(i+1)\n    n_data_points.append(train_class_distributed.values[i])\n    percent_data.append(np.round((train_class_distributed.values[i]\/X_train.shape[0]*100), 3))\n\n\nfigureObject, axesObject = plt.subplots()\naxesObject.pie(percent_data, labels = n_class, autopct='%1.1f', startangle=90)\ncentre_circle = plt.Circle((0,0),0.70,fc='white')\nfig = plt.gcf()\nfig.gca().add_artist(centre_circle)\nwedges, texts = plt.pie(percent_data, shadow=True, startangle=90)\nbbox_props = dict(boxstyle=\"square,pad=0.3\", fc=\"w\", ec=\"k\", lw=0.72)\nkw = dict(arrowprops=dict(arrowstyle=\"-\"),\n          bbox=bbox_props, zorder=0, va=\"center\")\n\nfor i, p in enumerate(wedges):\n    ang = (p.theta2 - p.theta1)\/2. + p.theta1\n    y = np.sin(np.deg2rad(ang))\n    x = np.cos(np.deg2rad(ang))\n    horizontalalignment = {-1: \"right\", 1: \"left\"}[int(np.sign(x))]\n    connectionstyle = \"angle,angleA=0,angleB={}\".format(ang)\n    kw[\"arrowprops\"].update({\"connectionstyle\": connectionstyle})\n    plt.annotate(percent_data[i], xy=(x, y), xytext=(1.35*np.sign(x), 1.4*y),\n                horizontalalignment=horizontalalignment, **kw)\n","fc9b13d5":"#I need to generate 9 random numbers (because we have 9 classes) and their sum should total 9 (again, 9 classes) so \n#the overall probability can total 1\ntest_len = x_test.shape[0]\ncrossval_len = x_crossval.shape[0]","44fa651a":"#creating an output array with the same size as the data I am using for cross validation\ny_predicted_crossval = np.zeros((crossval_len, 9))\n\nfor i in range (crossval_len):\n    rand_probab = np.random.rand(1,9)\n    y_predicted_crossval[i] = ((rand_probab\/sum(sum(rand_probab)))[0])\n\nprint (\"Log loss on the cross validation data using random model\", log_loss(y_crossval, y_predicted_crossval, eps = 1e-15))","a8d176fa":"#checking for the error in the test set\n#creating an output array again\ny_predicted_test = np.zeros((test_len, 9))\n\nfor i in range(test_len):\n    rand_probab = np.random.rand(1,9)\n    y_predicted_test[i] = ((rand_probab\/sum(sum(rand_probab)))[0])\n    \nprint (\"Log loss on the test data using random model\", log_loss(y_test, y_predicted_test, eps = 1e-15))","ebbd42f5":"#using argmax to find the maximum probability \ny_predicted = np.argmax(y_predicted_test, axis = 1)","c0b65638":"#check the output, should be 600 and something values\ny_predicted","35dfc190":"#the index on the previous output seemed to start at 0, let's correct it so it matches the classes proposed on the \n#problem statement\ny_predicted = y_predicted + 1\ny_predicted","fd0bbfa7":"C = confusion_matrix(y_test, y_predicted)","8a225880":"labels = [1,2,3,4,5,6,7,8,9]\nplt.figure(figsize = (20,7))\nsb.heatmap(C, annot = True, cmap = 'YlGnBu', fmt = '.3f', xticklabels = labels, yticklabels = labels)\nplt.xlabel('Predicted Classes')\nplt.ylabel('Actual Classes')\nplt.show()","488c17ab":"B = (C\/C.sum(axis = 0))","d34fcc88":"plt.figure(figsize = (20,7))\nsb.heatmap(B, annot = True, cmap = 'YlGnBu', fmt = '.3f', xticklabels = labels, yticklabels = labels)\nplt.xlabel('Predicted Classes')\nplt.ylabel('Actual Classes')\nplt.show()","5ca69307":"A =(((C.T)\/(C.sum(axis=1))).T)","3559eba4":"plt.figure(figsize=(20,7))\nsb.heatmap(A, annot=True, cmap=\"YlGnBu\", fmt=\".3f\", xticklabels=labels, yticklabels=labels)\nplt.xlabel('Predicted Class')\nplt.ylabel('Original Class')\nplt.show()","d23f8e55":"unique_genes = X_train['Gene'].value_counts()\nprint('Number of Unique Genes: ', unique_genes.shape[0])\nprint (unique_genes.head(10))","ac810b3a":"s = sum(unique_genes.values)\nh = (unique_genes.values\/s)\nc = np.cumsum(h)\nplt.plot(c, label = \"Cumulative distribution of genes\")\n#plt.ylim(0, 1.0)\n#plt.xlim(0, 200)\nplt.grid()\nplt.legend()\nplt.show()","05a1be2e":"#one-hot encoding of the Gene feature\ngene_vectorizer = CountVectorizer()\ntrain_gene_feature_onehotCoding = gene_vectorizer.fit_transform(X_train['Gene'])\ntest_gene_feature_onehotCoding = gene_vectorizer.transform(x_test['Gene'])\ncrossval_gene_feature_onehotCoding = gene_vectorizer.transform(x_crossval['Gene'])\n\ntrain_gene_feature_onehotCoding.shape","5c1108cc":"#name of the columns after one-hot encoding\ngene_vectorizer.get_feature_names()","2a7eee67":"# get_gv_fea_dict: Get Gene variation Feature Dictionary\ndef get_gv_fea_dict(alpha, feature, df):\n\n    value_count = X_train[feature].value_counts()\n    \n    # gv_dict : Gene Variation Dict, which contains the probability array for each gene\/variation\n    gv_dict = dict()\n    \n    # denominator will contain the number of time that particular feature occured in whole data\n    for i, denominator in value_count.items():\n        # vec will contain (p(yi==1\/Gi) probability of gene\/variation belongs to perticular class\n        # vec is 9 diamensional vector\n        vec = []\n        for k in range(1,10):\n\n            cls_cnt = X_train.loc[(X_train['Class']==k) & (X_train[feature]==i)]\n            \n            # cls_cnt.shape[0](numerator) will contain the number of time that particular feature occured in whole data\n            vec.append((cls_cnt.shape[0] + alpha*10)\/ (denominator + 90*alpha))\n\n        # we are adding the gene\/variation to the dict as key and vec as value\n        gv_dict[i]=vec\n    return gv_dict","a7c9d80f":"# Get Gene variation feature\ndef get_gv_feature(alpha, feature, df):\n    gv_dict = get_gv_fea_dict(alpha, feature, df)\n    # value_count is similar in get_gv_fea_dict\n    value_count = X_train[feature].value_counts()\n    \n    # gv_fea: Gene_variation feature, it will contain the feature for each feature value in the data\n    gv_fea = []\n    # for every feature values in the given data frame we will check if it is there in the train data then we will add the feature to gv_fea\n    # if not we will add [1\/9,1\/9,1\/9,1\/9,1\/9,1\/9,1\/9,1\/9,1\/9] to gv_fea\n    for index, row in df.iterrows():\n        if row[feature] in dict(value_count).keys():\n            gv_fea.append(gv_dict[row[feature]])\n        else:\n            gv_fea.append([1\/9,1\/9,1\/9,1\/9,1\/9,1\/9,1\/9,1\/9,1\/9])\n\n    return gv_fea","e59a9fd9":"#response-coding of the Gene feature\n# alpha is used for laplace smoothing\nalpha = 1\n# train gene feature\ntrain_gene_feature_responseCoding = np.array(get_gv_feature(alpha, \"Gene\", X_train))\n# test gene feature\ntest_gene_feature_responseCoding = np.array(get_gv_feature(alpha, \"Gene\", x_test))\n# cross validation gene feature\ncv_gene_feature_responseCoding = np.array(get_gv_feature(alpha, \"Gene\", x_crossval))","38e0ccc9":"train_gene_feature_responseCoding.shape","048b59d6":"# We need a hyperparemeter for SGD classifier.\nalpha = [10 ** x for x in range(-5, 1)]","912e70aa":"#now to implementing a logistic regression using an sgd classifier and the calibrated classifier\n#I will be using Ridge regression adds \u201csquared magnitude\u201d of coefficient as penalty term to the loss function.\n\ncv_log_error_array = []\nfor i in alpha:\n    clf = SGDClassifier(alpha=i, penalty='l2', loss='log', random_state=42)\n    clf.fit(train_gene_feature_onehotCoding, y_train)\n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(train_gene_feature_onehotCoding, y_train)\n    y_predicted = sig_clf.predict_proba(crossval_gene_feature_onehotCoding)\n    cv_log_error_array.append(log_loss(y_crossval, y_predicted, labels=clf.classes_, eps=1e-15))\n    print('For values of alpha = ', i, \"The log loss is:\",log_loss(y_crossval, y_predicted, labels=clf.classes_, eps=1e-15))\n    ","ee81d1af":"#I will make a visual check for the alphas\nfig, ax = plt.subplots()\nax.plot(alpha, cv_log_error_array,c='g')\nfor i, txt in enumerate(np.round(cv_log_error_array,3)):\n    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],cv_log_error_array[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()","55f523c5":"#now to use the best alpha to compute the log-loss\nbest_alpha = np.argmin(cv_log_error_array)\n\n\nclf = SGDClassifier(alpha = alpha[best_alpha], penalty = 'l2', loss = 'log', random_state = 42)\nclf.fit(train_gene_feature_onehotCoding, y_train)\nsig_clf = CalibratedClassifierCV(clf, method = \"sigmoid\")\nsig_clf.fit(train_gene_feature_onehotCoding, y_train)\n\ny_predicted = sig_clf.predict_proba(train_gene_feature_onehotCoding)\nprint('best alpha = ', alpha[best_alpha], \" train log loss :\",log_loss(y_train, y_predicted, labels=clf.classes_, eps=1e-15))\ny_predicted = sig_clf.predict_proba(crossval_gene_feature_onehotCoding)\nprint('best alpha = ', alpha[best_alpha], \" cross validation log loss :\",log_loss(y_crossval, y_predicted, labels=clf.classes_, eps=1e-15))\ny_predicted = sig_clf.predict_proba(test_gene_feature_onehotCoding)\nprint('best alpha = ', alpha[best_alpha], \" test log loss :\",log_loss(y_test, y_predicted, labels=clf.classes_, eps=1e-15))\n","3e70b366":"test_coverage=x_test[x_test['Gene'].isin(list(set(X_train['Gene'])))].shape[0]\ncv_coverage=x_crossval[x_crossval['Gene'].isin(list(set(X_train['Gene'])))].shape[0]\n\nprint('1. In test data',test_coverage, 'out of',x_test.shape[0], \":\",(test_coverage\/x_test.shape[0])*100)\nprint('2. In cross validation data',cv_coverage, 'out of ',x_crossval.shape[0],\":\" ,(cv_coverage\/x_crossval.shape[0])*100)","7b617a76":"from wordcloud import WordCloud\n#from matplotlib_venn import venn\nimport matplotlib_venn as venn2\nfrom matplotlib_venn import venn2\n#from matplotlib_venn_wordcloud import venn3_wordcloud\ntrain_items = pd.unique(X_train['Gene'])\ntest_items = pd.unique(x_test['Gene'])\ncrossval_items = pd.unique(x_crossval['Gene'])\n\nin_train_test = set(test_items).intersection(set(train_items))\nin_train_crossval = set(crossval_items).intersection(set(train_items))\nin_test_crossval = set(crossval_items).intersection(set(test_items))\n\nonly_train = set(train_items).difference(set(test_items))\nonly_train2 = set(train_items).difference(set(crossval_items))\nexclusive_train = set(only_train).difference(set(only_train2))\n\nonly_test = set(test_items).difference(set(train_items))\nonly_test2 = set(test_items).difference(set(crossval_items))\nexclusive_test = set(only_test).difference(set(only_test2))\n\nonly_crossval = set(crossval_items).difference(set(train_items))\nonly_crossval2 = set(crossval_items).difference(set(test_items))\nexclusive_crossval = set(only_crossval).difference(set(only_crossval2))\n\n#quick print out to check if things make sense\nprint(\"Items in train: \", len(train_items), \" Overlap train-test: \", len(in_train_test), \" Overlap train-crossval: \",\n      len(in_train_crossval), \" Exclusive to train: \", len(exclusive_train))\n","86186117":"figure, axes = plt.subplots(1, 3, figsize = (15,15), squeeze = False)\n#overlap between train and test\nv = venn2(subsets = (len(train_items), len(in_train_test), len(test_items)), \n      set_labels=('Train', 'Test'), ax = axes[0][0]);\n\n\n#overlap between train and crossval\nv2 = venn2(subsets = (len(train_items), len(in_train_crossval), len(crossval_items)), \n      set_labels=('Train', 'Crossval'), ax = axes[0][1]);\n\n#overlap between test and crossval\nv2 = venn2(subsets = (len(test_items), len(in_test_crossval), len(crossval_items)), \n      set_labels=('Test', 'Crossval'), ax = axes[0][2]);\n\nplt.tight_layout()\nplt.show()\n\nprint('Unique genes in train: ', len(only_train), ' Unique genes in test: ', len(only_test))\nprint('Unique genes in train: ', len(only_train2), ' Unique genes in crossval: ', len(only_crossval))","d96da0bc":"unique_variations = X_train['Variation'].value_counts()\nprint('Number of variations: ', unique_variations.shape[0])\n\nprint(unique_variations.head(10))\n#most abundant variations visualization\nunique_variations.head(10).plot(kind = 'bar')\n\nplt.xlabel('Type of variation')\nplt.ylabel('Abundance of variation')\nplt.title('Top 10 Gene Variations found in the dataset')\nplt.show()\n","784f9a30":"#I will take a further look at the gene variation classifications\nunique_variations.head(30)","e8d4235c":"#Looking at the distribution of the variations\ns = sum(unique_variations.values)\nh = unique_variations.values\/s\nc = np.cumsum(h)\nprint(c)\nplt.plot(c,label = 'Cumulative distribution of variations')\nplt.legend()\nplt.show()\n","0a65bbba":"variation_vectorizer = CountVectorizer()\ntrain_variation_feature_onehotCoding = variation_vectorizer.fit_transform(X_train['Variation'])\ntest_variation_feature_onehotCoding = variation_vectorizer.transform(x_test['Variation'])\ncv_variation_feature_onehotCoding = variation_vectorizer.transform(x_crossval['Variation'])\n\ntrain_variation_feature_onehotCoding.shape","8508bfc9":"# alpha is used for laplace smoothing\nalpha = 1\n# train variation feature\ntrain_variation_feature_responseCoding = np.array(get_gv_feature(alpha, \"Variation\", X_train))\n# test variation feature\ntest_variation_feature_responseCoding = np.array(get_gv_feature(alpha, \"Variation\", x_test))\n# cross validation variation feature\ncv_variation_feature_responseCoding = np.array(get_gv_feature(alpha, \"Variation\", x_crossval))\n\ntrain_variation_feature_responseCoding.shape","2ba77dcf":"# We need a hyperparemeter for SGD classifier.\nalpha = [10 ** x for x in range(-5, 1)]","43109b92":"cv_log_error_array=[]\nfor i in alpha:\n    clf = SGDClassifier(alpha=i, penalty='l2', loss='log', random_state=42)\n    clf.fit(train_variation_feature_onehotCoding, y_train)\n    \n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(train_variation_feature_onehotCoding, y_train)\n    y_predicted = sig_clf.predict_proba(cv_variation_feature_onehotCoding)\n    \n    cv_log_error_array.append(log_loss(y_crossval, y_predicted, labels=clf.classes_, eps=1e-15))\n    print('For values of alpha = ', i, \"The log loss is:\",log_loss(y_crossval, y_predicted, labels=clf.classes_, eps=1e-15))","03f3f07d":"# pltotting to select best alpha\nfig, ax = plt.subplots()\nax.plot(alpha, cv_log_error_array,c='g')\nfor i, txt in enumerate(np.round(cv_log_error_array,3)):\n    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],cv_log_error_array[i]))\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()","b1963b59":"best_alpha = np.argmin(cv_log_error_array)\nclf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\nclf.fit(train_variation_feature_onehotCoding, y_train)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_variation_feature_onehotCoding, y_train)\n\ny_predicted = sig_clf.predict_proba(train_variation_feature_onehotCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, y_predicted, labels=clf.classes_, eps=1e-15))\ny_predicted = sig_clf.predict_proba(cv_variation_feature_onehotCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(y_crossval, y_predicted, labels=clf.classes_, eps=1e-15))\ny_predicted = sig_clf.predict_proba(test_variation_feature_onehotCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, y_predicted, labels=clf.classes_, eps=1e-15))","caa411aa":"test_coverage=x_test[x_test['Variation'].isin(list(set(X_train['Variation'])))].shape[0]\ncv_coverage=x_crossval[x_crossval['Variation'].isin(list(set(X_train['Variation'])))].shape[0]","3924cbc1":"print('1. In test data',test_coverage, 'out of',x_test.shape[0], \":\",(test_coverage\/x_test.shape[0])*100)\nprint('2. In cross validation data',cv_coverage, 'out of ',x_crossval.shape[0],\":\" ,(cv_coverage\/x_crossval.shape[0])*100)","8db7449d":"#starting with a function to count each word in the text\ndef extract_dictionary_paddle(cls_text):\n    dictionary = defaultdict(int)\n    for index, row in cls_text.iterrows():\n        for word in row['TEXT'].split():\n            dictionary[word] +=1\n    return dictionary","12e05b9b":"#test if a particular key was already found in the dictionary\n\ndef get_text_responsecoding(df):\n    text_feature_responseCoding = np.zeros((df.shape[0],9))\n    for i in range(0,9):\n        row_index = 0\n        for index, row in df.iterrows():\n            sum_prob = 0\n            for word in row['TEXT'].split():\n                sum_prob += math.log(((dict_list[i].get(word,0)+10 )\/(total_dict.get(word,0)+90)))\n            text_feature_responseCoding[row_index][i] = math.exp(sum_prob\/len(row['TEXT'].split()))\n            row_index += 1\n    return text_feature_responseCoding","a48f229b":"# building a CountVectorizer with all the words that occured minimum 3 times in train data\n\ntext_vectorizer = CountVectorizer(min_df=3)\ntrain_text_feature_onehotCoding = text_vectorizer.fit_transform(X_train['TEXT'])\n\n# getting all the feature names (words)\ntrain_text_features= text_vectorizer.get_feature_names()\n\n# train_text_feature_onehotCoding.sum(axis=0).A1 will sum every row and returns (1*number of features) vector\ntrain_text_fea_counts = train_text_feature_onehotCoding.sum(axis=0).A1\n\n# zip(list(text_features),text_fea_counts) will zip a word with its number of times it occured\ntext_fea_dict = dict(zip(list(train_text_features),train_text_fea_counts))\n\n\nprint(\"Total number of unique words in train data :\", len(train_text_features))","9765167f":"dict_list = []\n# dict_list =[] contains 9 dictoinaries each corresponds to a class\n\nfor i in range(1,10):\n    cls_text = X_train[X_train['Class']==i]\n    # build a word dict based on the words in that class\n    dict_list.append(extract_dictionary_paddle(cls_text))\n    # append it to dict_list\n\n# dict_list[i] is build on i'th  class text data\n# total_dict is buid on whole training text data\n\ntotal_dict = extract_dictionary_paddle(X_train)","72830814":"confuse_array = []\nfor i in train_text_features:\n    ratios = []\n    max_val = -1\n    for j in range(0,9):\n        ratios.append((dict_list[j][i]+10 )\/(total_dict[i]+90))\n    confuse_array.append(ratios)\nconfuse_array = np.array(confuse_array)","281ba657":"from collections import Counter\nword_could_dict=Counter(text_fea_dict)\nwordcloud = WordCloud(width = 1000, height = 500).generate_from_frequencies(word_could_dict)\n\nplt.figure(figsize=(15,8))\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.show()","548c26fe":"#response coding the text features\n\ntrain_text_feature_responseCoding  = get_text_responsecoding(X_train)\ntest_text_feature_responseCoding  = get_text_responsecoding(x_test)\ncv_text_feature_responseCoding  = get_text_responsecoding(x_crossval)","fe64b195":"#converting row values so they can have a total sum of 1\n\ntrain_text_feature_responseCoding = (train_text_feature_responseCoding.T\/train_text_feature_responseCoding.sum(axis=1)).T\ntest_text_feature_responseCoding = (test_text_feature_responseCoding.T\/test_text_feature_responseCoding.sum(axis=1)).T\ncv_text_feature_responseCoding = (cv_text_feature_responseCoding.T\/cv_text_feature_responseCoding.sum(axis=1)).T","ca9a0d7e":"#normalizing all my features\n\ntrain_text_feature_onehotCoding = normalize(train_text_feature_onehotCoding, axis=0)\n\n\ntest_text_feature_onehotCoding = text_vectorizer.transform(x_test['TEXT'])\ntest_text_feature_onehotCoding = normalize(test_text_feature_onehotCoding, axis=0)\n\n\ncv_text_feature_onehotCoding = text_vectorizer.transform(x_crossval['TEXT'])\ncv_text_feature_onehotCoding = normalize(cv_text_feature_onehotCoding, axis=0)","839dd52a":"#we will sort the dictionaries by value (or at least their representation, dictionaries are orderless)\n\nsorted_text_fea_dict = dict(sorted(text_fea_dict.items(), key=lambda x: x[1] , reverse=True))\nsorted_text_occur = np.array(list(sorted_text_fea_dict.values()))","d5306acf":"#and now to plot the frequencies\n#c = Counter(sorted_text_occur)\n\nlabels, values = zip(*Counter(sorted_text_occur).items())\n\nindexes = np.arange(len(labels))\nwidth = 1\n\nplt.bar(indexes, values, width)\nplt.xticks(indexes + width * 0.5, labels)\nplt.show()","85d9af64":"cv_log_error_array=[]\nfor i in alpha:\n    clf = SGDClassifier(alpha=i, penalty='l2', loss='log', random_state=42)\n    clf.fit(train_text_feature_onehotCoding, y_train)\n    \n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(train_text_feature_onehotCoding, y_train)\n    y_predicted = sig_clf.predict_proba(cv_text_feature_onehotCoding)\n    cv_log_error_array.append(log_loss(y_crossval, y_predicted, labels=clf.classes_, eps=1e-15))\n    print('For values of alpha = ', i, \"The log loss is:\",log_loss(y_crossval, y_predicted, labels=clf.classes_, eps=1e-15))\n","f50cee00":"#and to proceed to the visualization of the cross-validation error for each alpha:\n\nfig, ax = plt.subplots()\nax.plot(alpha, cv_log_error_array,c='g')\n\nfor i, txt in enumerate(np.round(cv_log_error_array,3)):\n    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],cv_log_error_array[i]))\n    \nplt.title(\"Cross Validation Error per alpha\")\nplt.xlabel(\"Alpha \")\nplt.ylabel(\"Error measurement\")\nplt.show()","beb884b3":"#using the best alpha:\n\nbest_alpha = np.argmin(cv_log_error_array)\nclf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\nclf.fit(train_text_feature_onehotCoding, y_train)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_text_feature_onehotCoding, y_train)\n\ny_predicted = sig_clf.predict_proba(train_text_feature_onehotCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, y_predicted, labels=clf.classes_, eps=1e-15))\ny_predicted = sig_clf.predict_proba(cv_text_feature_onehotCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(y_crossval, y_predicted, labels=clf.classes_, eps=1e-15))\ny_predicted = sig_clf.predict_proba(test_text_feature_onehotCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, y_predicted, labels=clf.classes_, eps=1e-15))\n","9dba9da2":"#now to check how much overlap there is between text data\n\ndef get_intersec_text(df):\n    df_text_vec = CountVectorizer(min_df=3)\n    df_text_fea = df_text_vec.fit_transform(df['TEXT'])\n    df_text_features = df_text_vec.get_feature_names()\n\n    df_text_fea_counts = df_text_fea.sum(axis=0).A1\n    df_text_fea_dict = dict(zip(list(df_text_features),df_text_fea_counts))\n    len1 = len(set(df_text_features))\n    len2 = len(set(train_text_features) & set(df_text_features))\n    return len1,len2","69a53bd6":"len1,len2 = get_intersec_text(x_test)\nprint(np.round((len2\/len1)*100, 3), \"% of words from test that appear in the train set\")\nlen1,len2 = get_intersec_text(x_crossval)\nprint(np.round((len2\/len1)*100, 3), \"% of words from Cross Validation that appear in the train set\")","67232f75":"# creating a function to return the log-loss\ndef report_log_loss(train_x, train_y, test_x, test_y,  clf):\n    clf.fit(train_x, train_y)\n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(train_x, train_y)\n    sig_clf_probs = sig_clf.predict_proba(test_x)\n    return log_loss(test_y, sig_clf_probs, eps=1e-15)","9b81f943":"#function to plot the confusion matrix for y and y^\n#Y hat (written y\u0302 ) is the predicted value of y (the dependent variable) in a regression equation. \n#It can also be considered to be the average value of the response variable.\n\ndef plot_confusion_matrix(test_y, y_predicted):\n    C = confusion_matrix(test_y, y_predicted)\n    \n    A =(((C.T)\/(C.sum(axis=1))).T)\n    \n    B =(C\/C.sum(axis=0)) \n    labels = [1,2,3,4,5,6,7,8,9]\n    # representing A in heatmap format\n    print(\"-\"*20, \"Confusion matrix\", \"-\"*20)\n    plt.figure(figsize=(20,7))\n    sb.heatmap(C, annot=True, cmap=\"YlGnBu\", fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.show()\n\n    print(\"-\"*20, \"Precision matrix (Columm Sum=1)\", \"-\"*20)\n    plt.figure(figsize=(20,7))\n    sb.heatmap(B, annot=True, cmap=\"YlGnBu\", fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.show()\n    \n    # representing B in heatmap format\n    print(\"-\"*20, \"Recall matrix (Row sum=1)\", \"-\"*20)\n    plt.figure(figsize=(20,7))\n    sb.heatmap(A, annot=True, cmap=\"YlGnBu\", fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.show()\n","1259537b":"def predict_and_plot_confusion_matrix(train_x, train_y, test_x, test_y, clf):\n    clf.fit(train_x, train_y)\n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(train_x, train_y)\n    predicted_y = sig_clf.predict(test_x)\n\n    # for calculating log_loss we willl provide the array of probabilities belongs to each class\n    print(\"Log loss :\",log_loss(test_y, sig_clf.predict_proba(test_x)))\n    # calculating the number of data points that are misclassified\n    print(\"Number of mis-classified points :\", np.count_nonzero((predicted_y- test_y))\/test_y.shape[0])\n    plot_confusion_matrix(test_y, predicted_y)\n    ","2b15d664":"def mis_class_datapoints(train_x, train_y, test_x, test_y, clf):\n    clf.fit(train_x, train_y)\n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(train_x, train_y)\n    predicted_y = sig_clf.predict(test_x)\n    log_loss(test_y, sig_clf.predict_proba(test_x))\n    m_class = np.count_nonzero((predicted_y- test_y))\/test_y.shape[0]\n    return m_class","6e1b47eb":"# this function will be used just for naive bayes\n# for the given indices, we will print the name of the features\n\ndef get_impfeature_names(indices, text, gene, var, no_features):\n    gene_count_vec = CountVectorizer()\n    var_count_vec = CountVectorizer()\n    text_count_vec = CountVectorizer(min_df=3)\n    \n    gene_vec = gene_count_vec.fit(X_train['Gene'])\n    var_vec  = var_count_vec.fit(X_train['Variation'])\n    text_vec = text_count_vec.fit(X_train['TEXT'])\n    \n    fea1_len = len(gene_vec.get_feature_names())\n    fea2_len = len(var_count_vec.get_feature_names())\n    \n    word_present = 0\n    for i,v in enumerate(indices):\n        if (v < fea1_len):\n            word = gene_vec.get_feature_names()[v]\n            yes_no = True if word == gene else False\n            if yes_no:\n                word_present += 1\n                print(i, \"Gene feature [{}] present in test data point [{}]\".format(word,yes_no))\n        elif (v < fea1_len+fea2_len):\n            word = var_vec.get_feature_names()[v-(fea1_len)]\n            yes_no = True if word == var else False\n            if yes_no:\n                word_present += 1\n                print(i, \"variation feature [{}] present in test data point [{}]\".format(word,yes_no))\n        else:\n            word = text_vec.get_feature_names()[v-(fea1_len+fea2_len)]\n            yes_no = True if word in text.split() else False\n            if yes_no:\n                word_present += 1\n                print(i, \"Text feature [{}] present in test data point [{}]\".format(word,yes_no))\n\n    print(\"Out of the top \",no_features,\" features \", word_present, \"are present in query point\")","10064132":"# merging gene, variance and text features\n\ntrain_gene_var_onehotCoding = hstack((train_gene_feature_onehotCoding,train_variation_feature_onehotCoding))\ntest_gene_var_onehotCoding = hstack((test_gene_feature_onehotCoding,test_variation_feature_onehotCoding))\ncv_gene_var_onehotCoding = hstack((crossval_gene_feature_onehotCoding,cv_variation_feature_onehotCoding))\n\ntrain_x_onehotCoding = hstack((train_gene_var_onehotCoding, train_text_feature_onehotCoding)).tocsr()\ntrain_y = np.array(list(X_train['Class']))\n\ntest_x_onehotCoding = hstack((test_gene_var_onehotCoding, test_text_feature_onehotCoding)).tocsr()\ntest_y = np.array(list(x_test['Class']))\n\ncv_x_onehotCoding = hstack((cv_gene_var_onehotCoding, cv_text_feature_onehotCoding)).tocsr()\ncrossval_y = np.array(list(x_crossval['Class']))\n\n\ntrain_gene_var_responseCoding = np.hstack((train_gene_feature_responseCoding,train_variation_feature_responseCoding))\ntest_gene_var_responseCoding = np.hstack((test_gene_feature_responseCoding,test_variation_feature_responseCoding))\ncv_gene_var_responseCoding = np.hstack((cv_gene_feature_responseCoding,cv_variation_feature_responseCoding))\n\ntrain_x_responseCoding = np.hstack((train_gene_var_responseCoding, train_text_feature_responseCoding))\ntest_x_responseCoding = np.hstack((test_gene_var_responseCoding, test_text_feature_responseCoding))\ncv_x_responseCoding = np.hstack((cv_gene_var_responseCoding, cv_text_feature_responseCoding))\n","d258f225":"print(\"One hot encoding features :\")\nprint(\"(number of data points * number of features) in train data = \", train_x_onehotCoding.shape)\nprint(\"(number of data points * number of features) in test data = \", test_x_onehotCoding.shape)\nprint(\"(number of data points * number of features) in cross validation data =\", cv_x_onehotCoding.shape)","24a28a19":"print(\" Response encoding features :\")\nprint(\"(number of data points * number of features) in train data = \", train_x_responseCoding.shape)\nprint(\"(number of data points * number of features) in test data = \", test_x_responseCoding.shape)\nprint(\"(number of data points * number of features) in cross validation data =\", cv_x_responseCoding.shape)","277d3ba2":"alpha = [0.00001, 0.0001, 0.001, 0.1, 1, 10, 100,1000]\ncv_log_error_array = []\nfor i in alpha:\n    print(\"for alpha =\", i)\n    clf = MultinomialNB(alpha=i) # as is requested for the classification\n    clf.fit(train_x_onehotCoding, train_y)\n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(train_x_onehotCoding, train_y)\n    sig_clf_probs = sig_clf.predict_proba(cv_x_onehotCoding)\n    cv_log_error_array.append(log_loss(crossval_y, sig_clf_probs, labels=clf.classes_, eps=1e-15))\n    # to avoid rounding error while multiplying probabilites we use log-probability estimates\n    print(\"Log Loss :\",log_loss(crossval_y, sig_clf_probs)) ","a86c1492":"fig, ax = plt.subplots()\nax.plot(np.log10(alpha), cv_log_error_array,c='g')\nfor i, txt in enumerate(np.round(cv_log_error_array,3)):\n    ax.annotate((alpha[i],str(txt)), (np.log10(alpha[i]),cv_log_error_array[i]))\nplt.grid()\nplt.xticks(np.log10(alpha))\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()","9f521b0d":"best_alpha = np.argmin(cv_log_error_array)\nnb_alpha = alpha[best_alpha]\n\nclf = MultinomialNB(alpha=alpha[best_alpha])\nclf.fit(train_x_onehotCoding, train_y)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_x_onehotCoding, train_y)\n\n\npredict_y = sig_clf.predict_proba(train_x_onehotCoding)\nnb_train_ll_OH = (log_loss(train_y, predict_y, labels=clf.classes_, eps=1e-15))\nprint('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(train_y, predict_y, labels=clf.classes_, eps=1e-15))\n\npredict_y = sig_clf.predict_proba(cv_x_onehotCoding)\nnb_cv_ll_OH = (log_loss(crossval_y, predict_y, labels=clf.classes_, eps=1e-15))\nprint('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(crossval_y, predict_y, labels=clf.classes_, eps=1e-15))\n\npredict_y = sig_clf.predict_proba(test_x_onehotCoding)\nnb_test_ll_OH = (log_loss(test_y, predict_y, labels=clf.classes_, eps=1e-15))\nprint('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(test_y, predict_y, labels=clf.classes_, eps=1e-15))\n","72d0e04e":"clf = MultinomialNB(alpha=alpha[best_alpha])\nclf.fit(train_x_onehotCoding, train_y)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_x_onehotCoding, train_y)\nsig_clf_probs = sig_clf.predict_proba(cv_x_onehotCoding)\n# to avoid rounding error while multiplying probabilites we use log-probability estimates\nprint(\"Log Loss :\",log_loss(crossval_y, sig_clf_probs))\nnb_misclass_OH = np.count_nonzero((sig_clf.predict(cv_x_onehotCoding)- crossval_y))\/crossval_y.shape[0]\nprint(\"# missclassified points :\", np.count_nonzero((sig_clf.predict(cv_x_onehotCoding)- crossval_y))\/crossval_y.shape[0])\nplot_confusion_matrix(crossval_y, sig_clf.predict(cv_x_onehotCoding.toarray()))","bc6933da":"test_point_index = 1\nno_feature = 100\npredicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\nprint(\"Actual Class :\", test_y[test_point_index])\nindices = np.argsort(-clf.coef_)[predicted_cls-1][:,:no_feature]\nprint(\"-\"*50)\nget_impfeature_names(indices[0], x_test['TEXT'].iloc[test_point_index],x_test['Gene'].iloc[test_point_index],x_test['Variation'].iloc[test_point_index], no_feature)","66e66594":"test_point_index = 50\nno_feature = 50\npredicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\nprint(\"Actual Class :\", test_y[test_point_index])\nindices = np.argsort(-clf.coef_)[predicted_cls-1][:,:no_feature]\nprint(\"-\"*50)\nget_impfeature_names(indices[0], x_test['TEXT'].iloc[test_point_index],x_test['Gene'].iloc[test_point_index],x_test['Variation'].iloc[test_point_index], no_feature)","5d572557":"alpha = [5, 11, 15, 21, 31, 41, 51, 99]\ncv_log_error_array = []\nfor i in alpha:\n    print(\"for alpha =\", i)\n    clf = KNeighborsClassifier(n_neighbors=i)\n    clf.fit(train_x_responseCoding, train_y)\n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(train_x_responseCoding, train_y)\n    sig_clf_probs = sig_clf.predict_proba(cv_x_responseCoding)\n    cv_log_error_array.append(log_loss(crossval_y, sig_clf_probs, labels=clf.classes_, eps=1e-15))\n    # to avoid rounding error while multiplying probabilites we use log-probability estimates\n    print(\"Log Loss :\",log_loss(crossval_y, sig_clf_probs))","0b0a8b8c":"fig, ax = plt.subplots()\nax.plot(alpha, cv_log_error_array,c='g')\nfor i, txt in enumerate(np.round(cv_log_error_array,3)):\n    ax.annotate((alpha[i],str(txt)), (alpha[i],cv_log_error_array[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()","18ba8338":"best_alpha = np.argmin(cv_log_error_array)\nknn_alpha = alpha[best_alpha]\nclf = KNeighborsClassifier(n_neighbors = alpha[best_alpha])\nclf.fit(train_x_responseCoding, train_y)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_x_responseCoding, train_y)\n\npredict_y = sig_clf.predict_proba(train_x_responseCoding)\nknn_train_ll_RE = log_loss(train_y, predict_y, labels=clf.classes_, eps=1e-15)\nprint('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(train_y, predict_y, labels=clf.classes_, eps=1e-15))\n\npredict_y = sig_clf.predict_proba(cv_x_responseCoding)\nknn_cv_ll_RE = log_loss(crossval_y, predict_y, labels=clf.classes_, eps=1e-15)\nprint('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(crossval_y, predict_y, labels=clf.classes_, eps=1e-15))\n\npredict_y = sig_clf.predict_proba(test_x_responseCoding)\nknn_test_ll_RE = log_loss(test_y, predict_y, labels=clf.classes_, eps=1e-15)\nprint('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(test_y, predict_y, labels=clf.classes_, eps=1e-15))\n","ed946059":"clf = KNeighborsClassifier(n_neighbors=alpha[best_alpha])\npredict_and_plot_confusion_matrix(train_x_responseCoding, train_y, cv_x_responseCoding, crossval_y, clf)\nknn_misclass = mis_class_datapoints(train_x_responseCoding, train_y, cv_x_responseCoding, crossval_y, clf)","d685a74e":"# Lets look at few test points\nclf = KNeighborsClassifier(n_neighbors=alpha[best_alpha])\nclf.fit(train_x_responseCoding, train_y)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_x_responseCoding, train_y)\n\ntest_point_index = 1\npredicted_cls = sig_clf.predict(test_x_responseCoding[0].reshape(1,-1))\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Actual Class :\", test_y[test_point_index])\nneighbors = clf.kneighbors(test_x_responseCoding[test_point_index].reshape(1, -1), alpha[best_alpha])\nprint(\"The \",alpha[best_alpha],\" nearest neighbours of the test points belongs to classes\",train_y[neighbors[1][0]])\nprint(\"Fequency of nearest points :\",Counter(train_y[neighbors[1][0]]))","0013d44a":"clf = KNeighborsClassifier(n_neighbors=alpha[best_alpha])\nclf.fit(train_x_responseCoding, train_y)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_x_responseCoding, train_y)\n\ntest_point_index = 100\n\npredicted_cls = sig_clf.predict(test_x_responseCoding[test_point_index].reshape(1,-1))\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Actual Class :\", test_y[test_point_index])\nneighbors = clf.kneighbors(test_x_responseCoding[test_point_index].reshape(1, -1), alpha[best_alpha])\nprint(\"k value for knn is\",alpha[best_alpha],\"and the nearest neighbours of the test points belongs to classes\",train_y[neighbors[1][0]])\nprint(\"Fequency of nearest points :\",Counter(train_y[neighbors[1][0]]))","05624d14":"alpha = [10 ** x for x in range(-6, 3)]\ncv_log_error_array = []\nfor i in alpha:\n    print(\"for alpha =\", i)\n    clf = SGDClassifier(class_weight='balanced', alpha=i, penalty='l2', loss='log', random_state=42)\n    clf.fit(train_x_onehotCoding, train_y)\n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(train_x_onehotCoding, train_y)\n    sig_clf_probs = sig_clf.predict_proba(cv_x_onehotCoding)\n    cv_log_error_array.append(log_loss(crossval_y, sig_clf_probs, labels=clf.classes_, eps=1e-15))\n    # to avoid rounding error while multiplying probabilites we use log-probability estimates\n    print(\"Log Loss :\",log_loss(crossval_y, sig_clf_probs)) \n","02c69198":"fig, ax = plt.subplots()\nax.plot(alpha, cv_log_error_array,c='g')\nfor i, txt in enumerate(np.round(cv_log_error_array,3)):\n    ax.annotate((alpha[i],str(txt)), (alpha[i],cv_log_error_array[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()","9cc29d31":"best_alpha = np.argmin(cv_log_error_array)\nLR_bal_alpha = alpha[best_alpha]\nclf = SGDClassifier(class_weight='balanced', alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\nclf.fit(train_x_onehotCoding, train_y)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_x_onehotCoding, train_y)\n\npredicted_y = sig_clf.predict_proba(train_x_onehotCoding)\nLR_bal_train_ll_OH = log_loss(train_y, predicted_y, labels=clf.classes_, eps=1e-15)\nprint('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(train_y, predicted_y, labels=clf.classes_, eps=1e-15))\n\npredicted_y = sig_clf.predict_proba(cv_x_onehotCoding)\nLR_bal_cv_ll_OH = log_loss(crossval_y, predicted_y, labels=clf.classes_, eps=1e-15)\nprint('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(crossval_y, predicted_y, labels=clf.classes_, eps=1e-15))\n\npredicted_y = sig_clf.predict_proba(test_x_onehotCoding)\nLR_bal_test_ll_OH = log_loss(test_y, predicted_y, labels=clf.classes_, eps=1e-15)\nprint('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(test_y, predicted_y, labels=clf.classes_, eps=1e-15))","9c9c1e59":"clf = SGDClassifier(class_weight='balanced', alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\npredict_and_plot_confusion_matrix(train_x_onehotCoding, train_y, cv_x_onehotCoding, crossval_y, clf)\nLR_misclass = mis_class_datapoints(train_x_onehotCoding, train_y, cv_x_onehotCoding, crossval_y, clf)","e5d2cf59":"def get_imp_feature_names(text, indices, removed_ind = []):\n    word_present = 0\n    tabulate_list = []\n    incresingorder_ind = 0\n    for i in indices:\n        if i < train_gene_feature_onehotCoding.shape[1]:\n            tabulate_list.append([incresingorder_ind, \"Gene\", \"Yes\"])\n        elif i< 18:\n            tabulate_list.append([incresingorder_ind,\"Variation\", \"Yes\"])\n        if ((i > 17) & (i not in removed_ind)) :\n            word = train_text_features[i]\n            yes_no = True if word in text.split() else False\n            if yes_no:\n                word_present += 1\n            tabulate_list.append([incresingorder_ind,train_text_features[i], yes_no])\n        incresingorder_ind += 1\n    print(word_present, \"most important features are present in our query point\")\n    print(\"-\"*50)\n    print(\"The features that are most importent of the \",predicted_cls[0],\" class:\")\n    print (tabulate(tabulate_list, headers=[\"Index\",'Feature name', 'Present or Not']))","1977da9c":"# from tabulate import tabulate\nclf = SGDClassifier(class_weight='balanced', alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\nclf.fit(train_x_onehotCoding,train_y)\ntest_point_index = 1\nno_feature = 500\npredicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\nprint(\"Actual Class :\", test_y[test_point_index])\nindices = np.argsort(-clf.coef_)[predicted_cls-1][:,:no_feature]\nprint(\"-\"*50)\nget_impfeature_names(indices[0], x_test['TEXT'].iloc[test_point_index],x_test['Gene'].iloc[test_point_index],x_test['Variation'].iloc[test_point_index], no_feature)","ee8b5826":"test_point_index = 100\nno_feature = 500\npredicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\nprint(\"Actual Class :\", test_y[test_point_index])\nindices = np.argsort(-clf.coef_)[predicted_cls-1][:,:no_feature]\nprint(\"-\"*50)\nget_impfeature_names(indices[0], x_test['TEXT'].iloc[test_point_index],x_test['Gene'].iloc[test_point_index],x_test['Variation'].iloc[test_point_index], no_feature)","9e451a30":"alpha = [10 ** x for x in range(-6, 1)]\ncv_log_error_array = []\nfor i in alpha:\n    print(\"for alpha =\", i)\n    clf = SGDClassifier(alpha=i, penalty='l2', loss='log', random_state=42)\n    clf.fit(train_x_onehotCoding, train_y)\n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(train_x_onehotCoding, train_y)\n    sig_clf_probs = sig_clf.predict_proba(cv_x_onehotCoding)\n    cv_log_error_array.append(log_loss(crossval_y, sig_clf_probs, labels=clf.classes_, eps=1e-15))\n    print(\"Log Loss :\",log_loss(crossval_y, sig_clf_probs)) \n","70769f20":"fig, ax = plt.subplots()\nax.plot(alpha, cv_log_error_array,c='g')\nfor i, txt in enumerate(np.round(cv_log_error_array,3)):\n    ax.annotate((alpha[i],str(txt)), (alpha[i],cv_log_error_array[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()","f3989c43":"best_alpha = np.argmin(cv_log_error_array)\nLR_notbal_alpha = alpha[best_alpha]\nclf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\nclf.fit(train_x_onehotCoding, train_y)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_x_onehotCoding, train_y)\n\npredicted_y = sig_clf.predict_proba(train_x_onehotCoding)\nLR_notbal_train_OH = log_loss(train_y, predicted_y, labels=clf.classes_, eps=1e-15)\nprint('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(train_y, predicted_y, labels=clf.classes_, eps=1e-15))\n\npredicted_y = sig_clf.predict_proba(cv_x_onehotCoding)\nLR_notbal_cv_OH = log_loss(crossval_y, predicted_y, labels=clf.classes_, eps=1e-15)\nprint('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(crossval_y, predicted_y, labels=clf.classes_, eps=1e-15))\n\npredicted_y = sig_clf.predict_proba(test_x_onehotCoding)\nLR_notbal_test_OH = log_loss(test_y, predicted_y, labels=clf.classes_, eps=1e-15)\nprint('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(test_y, predicted_y, labels=clf.classes_, eps=1e-15))","b87d8a21":"clf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\npredict_and_plot_confusion_matrix(train_x_onehotCoding, train_y, cv_x_onehotCoding, crossval_y, clf)\nLR_notbal_misclass = mis_class_datapoints(train_x_onehotCoding, train_y, cv_x_onehotCoding, crossval_y, clf)","d26280e4":"clf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\nclf.fit(train_x_onehotCoding,train_y)\ntest_point_index = 1\nno_feature = 500\npredicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\nprint(\"Actual Class :\", test_y[test_point_index])\nindices = np.argsort(-clf.coef_)[predicted_cls-1][:,:no_feature]\nprint(\"-\"*50)\nget_impfeature_names(indices[0], x_test['TEXT'].iloc[test_point_index],x_test['Gene'].iloc[test_point_index],x_test['Variation'].iloc[test_point_index], no_feature)","cf38e321":"alpha = [10 ** x for x in range(-5, 3)]\ncv_log_error_array = []\nfor i in alpha:\n    print(\"for C =\", i)\n#     clf = SVC(C=i,kernel='linear',probability=True, class_weight='balanced')\n    clf = SGDClassifier( class_weight='balanced', alpha=i, penalty='l2', loss='hinge', random_state=42)\n    clf.fit(train_x_onehotCoding, train_y)\n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(train_x_onehotCoding, train_y)\n    sig_clf_probs = sig_clf.predict_proba(cv_x_onehotCoding)\n    cv_log_error_array.append(log_loss(crossval_y, sig_clf_probs, labels=clf.classes_, eps=1e-15))\n    print(\"Log Loss :\",log_loss(crossval_y, sig_clf_probs)) \n","edac2a48":"fig, ax = plt.subplots()\nax.plot(alpha, cv_log_error_array,c='g')\nfor i, txt in enumerate(np.round(cv_log_error_array,3)):\n    ax.annotate((alpha[i],str(txt)), (alpha[i],cv_log_error_array[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()","4bbe1b7f":"best_alpha = np.argmin(cv_log_error_array)\nSVM_alpha = alpha[best_alpha]\n#clf = SVC(C=i,kernel='linear',probability=True, class_weight='balanced')\nclf = SGDClassifier(class_weight='balanced', alpha=alpha[best_alpha], penalty='l2', loss='hinge', random_state=42)\n\nclf.fit(train_x_onehotCoding, train_y)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_x_onehotCoding, train_y)\n\npredict_y = sig_clf.predict_proba(train_x_onehotCoding)\nSVM_train_OH = log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15)\nprint('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",\n      log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\n\npredict_y = sig_clf.predict_proba(cv_x_onehotCoding)\nSVM_cv_OH = log_loss(crossval_y, predict_y, labels=clf.classes_, eps=1e-15)\nprint('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",\n      log_loss(crossval_y, predict_y, labels=clf.classes_, eps=1e-15))\n\npredict_y = sig_clf.predict_proba(test_x_onehotCoding)\nSVM_test_OH = log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15)\nprint('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",\n      log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))","7f7ce8cb":"clf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='hinge', random_state=42,class_weight='balanced')\npredict_and_plot_confusion_matrix(train_x_onehotCoding, train_y,cv_x_onehotCoding,crossval_y, clf)\nSVM_misclass = mis_class_datapoints(train_x_onehotCoding, train_y,cv_x_onehotCoding,crossval_y, clf)","38c3b0ba":"clf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='hinge', random_state=42)\nclf.fit(train_x_onehotCoding,train_y)\ntest_point_index = 1\n# test_point_index = 100\nno_feature = 500\npredicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\nprint(\"Actual Class :\", test_y[test_point_index])\nindices = np.argsort(-clf.coef_)[predicted_cls-1][:,:no_feature]\nprint(\"-\"*50)\nget_impfeature_names(indices[0], x_test['TEXT'].iloc[test_point_index],x_test['Gene'].iloc[test_point_index],x_test['Variation'].iloc[test_point_index], no_feature)","ecda9edf":"alpha = [100,200,500,1000,2000]\nmax_depth = [5, 10]\ncv_log_error_array = []\nfor i in alpha:\n    for j in max_depth:\n        print(\"for n_estimators =\", i,\"and max depth = \", j)\n        clf = RandomForestClassifier(n_estimators=i, criterion='gini', max_depth=j, random_state=42, n_jobs=-1)\n        clf.fit(train_x_onehotCoding, train_y)\n        sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n        sig_clf.fit(train_x_onehotCoding, train_y)\n        sig_clf_probs = sig_clf.predict_proba(cv_x_onehotCoding)\n        cv_log_error_array.append(log_loss(crossval_y, sig_clf_probs, labels=clf.classes_, eps=1e-15))\n        print(\"Log Loss :\",log_loss(crossval_y, sig_clf_probs)) ","7968354b":"best_alpha = np.argmin(cv_log_error_array)\nRF_alpha_OH = alpha[int(best_alpha\/2)]\nclf = RandomForestClassifier(n_estimators=alpha[int(best_alpha\/2)], criterion='gini', max_depth=max_depth[int(best_alpha%2)],\n                             random_state=42, n_jobs=-1)\nclf.fit(train_x_onehotCoding, train_y)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_x_onehotCoding, train_y)\n\npredict_y = sig_clf.predict_proba(train_x_onehotCoding)\nRF_train_OH = log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15)\nprint('For values of best estimator = ', alpha[int(best_alpha\/2)], \"The train log loss is:\",\n      log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\n\npredict_y = sig_clf.predict_proba(cv_x_onehotCoding)\nRF_cv_OH = log_loss(crossval_y, predict_y, labels=clf.classes_, eps=1e-15)\nprint('For values of best estimator = ', alpha[int(best_alpha\/2)], \"The cross validation log loss is:\",\n      log_loss(crossval_y, predict_y, labels=clf.classes_, eps=1e-15))\n\npredict_y = sig_clf.predict_proba(test_x_onehotCoding)\nRF_test_OH = log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15)\nprint('For values of best estimator = ', alpha[int(best_alpha\/2)], \"The test log loss is:\",\n      log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))","ac6f33d9":"clf = RandomForestClassifier(n_estimators=alpha[int(best_alpha\/2)], criterion='gini', max_depth=max_depth[int(best_alpha%2)],\n                             random_state=42, n_jobs=-1)\npredict_and_plot_confusion_matrix(train_x_onehotCoding, train_y,cv_x_onehotCoding,crossval_y, clf)\nRF_OH_misclass = mis_class_datapoints(train_x_onehotCoding, train_y,cv_x_onehotCoding,crossval_y, clf)","115bab93":"# test_point_index = 10\nclf = RandomForestClassifier(n_estimators=alpha[int(best_alpha\/2)], criterion='gini', max_depth=max_depth[int(best_alpha%2)], random_state=42, n_jobs=-1)\nclf.fit(train_x_onehotCoding, train_y)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_x_onehotCoding, train_y)\n\ntest_point_index = 1\nno_feature = 100\npredicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\nprint(\"Actual Class :\", test_y[test_point_index])\nindices = np.argsort(-clf.feature_importances_)\nprint(\"-\"*50)\nget_impfeature_names(indices[:no_feature], x_test['TEXT'].iloc[test_point_index],x_test['Gene'].iloc[test_point_index],x_test['Variation'].iloc[test_point_index], no_feature)","4ddf3de7":"alpha = [10,50,100,200,500,1000]\nmax_depth = [2,3,5,10]\ncv_log_error_array = []\nfor i in alpha:\n    for j in max_depth:\n        print(\"for n_estimators =\", i,\"and max depth = \", j)\n        clf = RandomForestClassifier(n_estimators=i, criterion='gini', max_depth=j, random_state=42, n_jobs=-1)\n        clf.fit(train_x_responseCoding, train_y)\n        sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n        sig_clf.fit(train_x_responseCoding, train_y)\n        sig_clf_probs = sig_clf.predict_proba(cv_x_responseCoding)\n        cv_log_error_array.append(log_loss(crossval_y, sig_clf_probs, labels=clf.classes_, eps=1e-15))\n        print(\"Log Loss :\",log_loss(crossval_y, sig_clf_probs)) \n\n\nbest_alpha = np.argmin(cv_log_error_array)\nclf = RandomForestClassifier(n_estimators=alpha[int(best_alpha\/4)], criterion='gini', max_depth=max_depth[int(best_alpha%4)], random_state=42, n_jobs=-1)\nclf.fit(train_x_responseCoding, train_y)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_x_responseCoding, train_y)\n\npredict_y = sig_clf.predict_proba(train_x_responseCoding)\nprint('For values of best alpha = ', alpha[int(best_alpha\/4)], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(cv_x_responseCoding)\nprint('For values of best alpha = ', alpha[int(best_alpha\/4)], \"The cross validation log loss is:\",log_loss(crossval_y, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(test_x_responseCoding)\nprint('For values of best alpha = ', alpha[int(best_alpha\/4)], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))","0efe97a8":"alpha = [10,50,100,200,500,1000]\nmax_depth = [2,3,5,10]\ncv_log_error_array = []\nfor i in alpha:\n    for j in max_depth:\n        print(\"for n_estimators =\", i,\"and max depth = \", j)\n        clf = RandomForestClassifier(n_estimators=i, criterion='gini', max_depth=j, random_state=42, n_jobs=-1)\n        clf.fit(train_x_responseCoding, train_y)\n        sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n        sig_clf.fit(train_x_responseCoding, train_y)\n        sig_clf_probs = sig_clf.predict_proba(cv_x_responseCoding)\n        cv_log_error_array.append(log_loss(cv_y, sig_clf_probs, labels=clf.classes_, eps=1e-15))\n        print(\"Log Loss :\",log_loss(cv_y, sig_clf_probs)) \n\n\nbest_alpha = np.argmin(cv_log_error_array)\nclf = RandomForestClassifier(n_estimators=alpha[int(best_alpha\/4)], criterion='gini', max_depth=max_depth[int(best_alpha%4)], random_state=42, n_jobs=-1)\nclf.fit(train_x_responseCoding, train_y)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_x_responseCoding, train_y)\n\npredict_y = sig_clf.predict_proba(train_x_responseCoding)\nprint('For values of best alpha = ', alpha[int(best_alpha\/4)], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(cv_x_responseCoding)\nprint('For values of best alpha = ', alpha[int(best_alpha\/4)], \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(test_x_responseCoding)\nprint('For values of best alpha = ', alpha[int(best_alpha\/4)], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))","02086807":"clf = RandomForestClassifier(max_depth=max_depth[int(best_alpha%2)], n_estimators=alpha[int(best_alpha\/2)],\n                             criterion='gini', max_features='auto', random_state=42)\npredict_and_plot_confusion_matrix(train_x_responseCoding, train_y,cv_x_responseCoding,crossval_y, clf)","065985ee":"clf = RandomForestClassifier(n_estimators=alpha[int(best_alpha\/2)], criterion='gini',\n                             max_depth=max_depth[int(best_alpha%2)], random_state=42, n_jobs=-1)\nclf.fit(train_x_responseCoding, train_y)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_x_responseCoding, train_y)\n\n\ntest_point_index = 1\nno_feature = 27\npredicted_cls = sig_clf.predict(test_x_responseCoding[test_point_index].reshape(1,-1))\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_responseCoding[test_point_index].reshape(1,-1)),4))\nprint(\"Actual Class :\", test_y[test_point_index])\nindices = np.argsort(-clf.feature_importances_)\nprint(\"-\"*50)\nfor i in indices:\n    if i<9:\n        print(\"Gene is important feature\")\n    elif i<18:\n        print(\"Variation is important feature\")\n    else:\n        print(\"Text is important feature\")","ec3d4661":"from sklearn.linear_model import LogisticRegression\nclf1 = SGDClassifier(alpha=0.001, penalty='l2', loss='log', class_weight='balanced', random_state=0)\nclf1.fit(train_x_onehotCoding, train_y)\nsig_clf1 = CalibratedClassifierCV(clf1, method=\"sigmoid\")\n\nclf2 = SGDClassifier(alpha=1, penalty='l2', loss='hinge', class_weight='balanced', random_state=0)\nclf2.fit(train_x_onehotCoding, train_y)\nsig_clf2 = CalibratedClassifierCV(clf2, method=\"sigmoid\")\n\n\nclf3 = MultinomialNB(alpha=0.001)\nclf3.fit(train_x_onehotCoding, train_y)\nsig_clf3 = CalibratedClassifierCV(clf3, method=\"sigmoid\")\n\nsig_clf1.fit(train_x_onehotCoding, train_y)\nprint(\"Logistic Regression :  Log Loss: %0.2f\" % (log_loss(crossval_y, sig_clf1.predict_proba(cv_x_onehotCoding))))\nsig_clf2.fit(train_x_onehotCoding, train_y)\nprint(\"Support vector machines : Log Loss: %0.2f\" % (log_loss(crossval_y, sig_clf2.predict_proba(cv_x_onehotCoding))))\nsig_clf3.fit(train_x_onehotCoding, train_y)\nprint(\"Naive Bayes : Log Loss: %0.2f\" % (log_loss(crossval_y, sig_clf3.predict_proba(cv_x_onehotCoding))))\nprint(\"-\"*50)\nalpha = [0.0001,0.001,0.01,0.1,1,10] \nbest_alpha = 999\nfor i in alpha:\n    lr = LogisticRegression(C=i)\n    sclf = StackingClassifier(classifiers=[sig_clf1, sig_clf2, sig_clf3], meta_classifier=lr, use_probas=True)\n    sclf.fit(train_x_onehotCoding, train_y)\n    print(\"Stacking Classifer : for the value of alpha: %f Log Loss: %0.3f\" % (i, log_loss(crossval_y, sclf.predict_proba(cv_x_onehotCoding))))\n    log_error =log_loss(crossval_y, sclf.predict_proba(cv_x_onehotCoding))\n    if best_alpha > log_error:\n        best_alpha = log_error","2bde79b4":"lr = LogisticRegression(C=0.1)\nsclf = StackingClassifier(classifiers=[sig_clf1, sig_clf2, sig_clf3], meta_classifier=lr, use_probas=True)\nsclf.fit(train_x_onehotCoding, train_y)\n\nlog_error = log_loss(train_y, sclf.predict_proba(train_x_onehotCoding))\nStack_train = log_error\nprint(\"Log loss (train) on the stacking classifier :\",log_error)\n\nlog_error = log_loss(crossval_y, sclf.predict_proba(cv_x_onehotCoding))\nStack_cv = log_error\nprint(\"Log loss (CV) on the stacking classifier :\",log_error)\n\nlog_error = log_loss(test_y, sclf.predict_proba(test_x_onehotCoding))\nStack_test = log_error\nprint(\"Log loss (test) on the stacking classifier :\",log_error)\n\nStack_misclass = np.count_nonzero((sclf.predict(test_x_onehotCoding)- test_y))\/test_y.shape[0]\nprint(\"Number of missclassified point :\", np.count_nonzero((sclf.predict(test_x_onehotCoding)- test_y))\/test_y.shape[0])\nplot_confusion_matrix(test_y, sclf.predict(test_x_onehotCoding))","60401a96":"from sklearn.ensemble import VotingClassifier\nvclf = VotingClassifier(estimators=[('lr', sig_clf1), ('svc', sig_clf2), ('rf', sig_clf3)], voting='soft')\nvclf.fit(train_x_onehotCoding, train_y)\nMax_voting_train = log_loss(train_y, vclf.predict_proba(train_x_onehotCoding))\nprint(\"Log loss (train) on the VotingClassifier :\", log_loss(train_y, vclf.predict_proba(train_x_onehotCoding)))\n\nMax_voting_cv = log_loss(crossval_y, vclf.predict_proba(cv_x_onehotCoding))\nprint(\"Log loss (CV) on the VotingClassifier :\", log_loss(crossval_y, vclf.predict_proba(cv_x_onehotCoding)))\n\nMax_voting_test = log_loss(test_y, vclf.predict_proba(test_x_onehotCoding))\nprint(\"Log loss (test) on the VotingClassifier :\", log_loss(test_y, vclf.predict_proba(test_x_onehotCoding)))\n\nMax_voting_misclass = np.count_nonzero((vclf.predict(test_x_onehotCoding)- test_y))\/test_y.shape[0]\nprint(\"Number of missclassified point :\", np.count_nonzero((vclf.predict(test_x_onehotCoding)- test_y))\/test_y.shape[0])\nplot_confusion_matrix(test_y, vclf.predict(test_x_onehotCoding))","30067e06":"summary_table = {'Model' : ['Naive Bayes-OH', 'KNN-RE', 'Log Reg-Bal', 'Log Reg-NBal', 'Linear SVM', 'Rando Forest-OH', 'Stacking Model', \n                           'Maximum v class'], \n                 'Best-a' : [nb_alpha, knn_alpha, LR_bal_alpha, LR_notbal_alpha, SVM_alpha, RF_alpha_OH, 'not applied', 'not applied' ], \n                 'Train LL' : [nb_train_ll_OH, knn_train_ll_RE, LR_bal_train_ll_OH, LR_notbal_train_OH, SVM_train_OH, RF_train_OH, Stack_train, Max_voting_train],\n                'Test LL' : [nb_test_ll_OH, knn_test_ll_RE, LR_bal_test_ll_OH, LR_notbal_test_OH, SVM_test_OH, RF_test_OH, Stack_test, Max_voting_test], \n                 'CV LL' : [nb_cv_ll_OH, knn_cv_ll_RE, LR_bal_cv_ll_OH, LR_notbal_cv_OH, SVM_cv_OH, RF_cv_OH, Stack_cv, Max_voting_cv], \n                 'Misclassified' : [nb_misclass_OH, knn_misclass, LR_misclass, LR_notbal_misclass, SVM_misclass, RF_OH_misclass, Stack_misclass, Max_voting_misclass]}  \n\nsum_tab_df = pd.DataFrame(summary_table)\n\nprint(sum_tab_df)","ef26bbff":"### Naive Bayes performs decently for this data set but, lets see what KNN brings to the table\n\n# KNN","4cf9be79":"#### for each alpha we got a different log loss and the difference between them is not very big.\n#### the smallest log-loss was For values of alpha =  0.0001 The log loss is: 1.2490817935971195\n#### so this is the alpha I will be using from now on","a9715eff":"#### precision is the number of correct results divided by the number of all returned results, looking at the values obtained in this matric it is pretty clear that we have pretty bad precision based on this model\n\n### To check the Recall matrix: recall is the number of correct results divided by the number of results that should have been returned.","ff41bd8c":"# Linear Support Vector Machines","908aca81":"### And again looking at the stability to check how many of the targets were correctly predicted","9d020097":"#### my distribution plot is having serious issues considering that I am not even reaching 20% contribution for the first 200 values->corrected this, distribution plot looks a bit better now\n\n### Converting the categorical variable \n#### 2 ways to approach it: one-hot encoding and response encoding (mean imputation)\n#### what we will be looking at is how impactful a gene is to predict a particular class \n\n#### the problem with one-hot encoding is that it makes the data sparse by creating far too many columns if the dataset is very large (response encoding is a way to deal with this)\n#### in response encoding instead of creating many columns for every row we will create just the columns with the probabilities of each particular gene belonging to a particular class (in this case it would be 9)","c1bdbe53":"### Merging the 2 dataframes to simplify all the preprocessing that is needed to be done, make id of training_variant_df match training_text_df","0ae3c73e":"# Stacking model\n### this works best for large data sets wich is not this particular case but, it doesn't hurt to give it a go","4f9c0ef7":"### testing a data point","91add802":"### we will use the best alpha to proceed with the model (minimum value for log-loss)","ad8b945e":"#### now regarding the stability of this model: if the overlap between the genes in the train and in the test set is low (and the crossval set) the model will not be very stable and that is something we also want to test","ee57c7df":"### And now to combine all 3 features","84e1b138":"#### there are 3321 rows and 4 columns (ID, gene, variation and class)","9d887830":"# ML- Naive Bayes model\n","f076ce2b":"# Personalized Medicine: Redefining Cancer Treatment\n## A kaggle data exploration\n## About the data set:\n### Once sequenced, a cancer tumor can have thousands of genetic mutations. But the challenge is distinguishing the mutations that contribute to tumor growth (drivers) from the neutral mutations (passengers). \n\n### Currently this interpretation of genetic mutations is being done manually. This is a very time-consuming task where a clinical pathologist has to manually review and classify every single genetic mutation based on evidence from text-based clinical literature.\n\n### For this competition MSKCC is making available an expert-annotated knowledge base where world-class researchers and oncologists have manually annotated thousands of mutations.\n\n### We need your help to develop a Machine Learning algorithm that, using this knowledge base as a baseline, automatically classifies genetic variations.\n\n\n### This will an exploration on various models and will evaluate the models based on log loss","1d430147":"### Results without balancing the classes ","d7349818":"### The question now should be how good is Gene to predict the classes?\n### Stochastic Gradient Descent (SGD) is a simple yet very efficient approach to discriminative learning of linear classifiers under convex loss functions such as (linear) Support Vector Machines and Logistic Regression.\n#### Stochastic gradient descent considers only 1 random point while changing weights unlike gradient descent which considers the whole training data\n\n# Calibrated classifier\n### Predicted probabilities that match the expected distribution of probabilities for each class are referred to as calibrated. The problem is, not all machine learning models are capable of predicting calibrated probabilities. A classifier can be calibrated in scikit-learn using the CalibratedClassifierCV class. There are two ways to use this class: prefit and cross-validation. You can fit a model on a training dataset and calibrate this prefit model using a hold out validation dataset.","3bf0b999":"# Random Forest Classifier","83c5431c":"#### If there is a very large difference in log-loss between train and test, which is a sign that this is just overfitted to hell\n#### The stuff here is pretty acceptable","c68b5d5a":"#### looking at the confusion matrix the diagonal values should have better values, note also the missclassified points, subtracting this to 1 would give us the actual accuracy of the model\n\n#### the precision matrix looks better but still not getting values of 1\n\n#### the recall matrix shows the same trend ","1b1cfd4d":"### log-losses are not too different from what we got in the previous models","620f14a0":"### Based on the current knowledge of Biology it is not surprising that Truncations, deletions,  amplifications, fusions and overexpressions are the most abundant since these are extremely function altering, but looking further into the other possible columns I would seem like there are some rogue classifications (like the last bar in the plot seems to also be a fusion, but, since it is appended to the actual specific type of fusion it is falling into a category by itself. I wonder if there is something I could do about cleaning things further more and harmonize the variation types.","d141bea0":"### Data training, testing and validation\n#### the data will be divided between training set, cross-validation set and test set \n#### the train set will help building the model, the cross-validation will be used to tune the hyper-parameters and refine the model, the test will be untouch ","7382c6ad":"### Interpretability: Feature Importance","0ca56ed8":"#### looks like there are almost as many variations as there are genes\n\n#### following up with response encoding","9a2533a3":"### Preparing the data for the ML portion of the analysis ","6b096e9b":"## Loading the data files","14146428":"#### rechecking for missing values","48699a36":"#### Confusion matrix usage to evaluate the quality of the output of a classifier","5300d3e2":"### Interpreting the model\n\n#### One of the things required for this data set case was that we should have some answers on why the model is predicting things the way it is, the impfeature_names is one of the defined functions that names the most relevant features by name ","d5ded783":"#### It looks like it was just the one single case for the fusion nomenclature and the remaining variations seem to be in particular base pairs for the gene sequence that lead to aa swaps","8de27531":"### We are attempting to predict the cancer class based on this data but how many classes are there described in these datasets?","8682076b":"### Initiating the evaluation of the text column","908cd9ed":"#### Doing imputation to deal with the missing values, combining gene and variation columns","51d2ca02":"### In the end I would say that the best models would be the ones with the lower log-losses like the logistic regressions that are the best models for this data set.","66bf3d75":"### Using One-hot Encoding","2418f5d4":"### RF with response encoding\n\n\n### This one is a disaster and suffers from enourmous overfitting","452be6f4":"#### what is observed when calling shapes is that many columns appeared as the number of unique genes","1b8dfcb3":"#### it looks like there are no missing values","b6825061":"### testing one point to check how interpretabble","3f4d7886":"### Starting by processing all the text data","6c8b301d":"#### The ID will be common between the 2 data files I started working on ","992f88e1":"# Logistic Regression\n### there is going to be a degreee of oversampling because some classes are more represented in terms of data\n### class 8 and class 9 had very little data","28463079":"### Now to evaluate the effect of the variation column","1b145fec":"#### based on the numbers of the distributed sets it seems like there aren't a lot in class 8(12-4-3) and class 9(24, 7, 6) but that might be just because there are less entries overall","9512f493":"#### Looking for missing values","9ca848c6":"#### looking at the word cloud it seems like many of the most abundant words are related to methodology and not the juice of the paper itself, I wonder if this exercise was limited to just the article conclusions if we could be more successful, many references to figure names, egfr can be used as a growth factor for tissue culture, 'also', 'et' and 'previously' probably refer to other authors citations (this would be hard to tackle since each publisher has their own citation format, mentions of kits (probably extractions and electophoresis) still, pretty cool to see\n\n#### very awesome to see p53 and BRCA and pten appear as top words considering that they are some very well described genes\n\n#### just spotted ras as well\n\n#### also tiny appearence of gifitinib which is an EGFR inhibitor and common cancer drug","bfaf7bf6":"#### unfortunately many of these are just filler words\n#### the class that shows the highest probability is class 4 by quite a difference","efbe884f":"### According to kaggle the submissions are evaluated based on Multi class log loss between predicted probability and the observed target","aa511e0c":"# Summary table for all the models","570b594b":"### looking at the graph we can see the contribution of those more abundant types of variations and after that the contributions are very small\n\n#### so follwing up with one-hot encoding","a026cc14":"#### the intention was good but obviously there are far too many words with differing frequencies to make this a visually appealing plot\n\n### Now to actually work on the model using just the text feature","8583c65b":"### There are 4 fields in the table:\n#### -ID: links mutation to clinical evidence\n#### -Gene: nomenclature for the gene that carries identified variation\n#### -Variation: changed aa in the gene\n#### -Class: classification of the variant based on documented papers","215a656e":"#### checking another data point ","fa666090":"#### Making a cumulative distribution plot","b84aadf6":"#### balancing all the classes and how the dataset will look like\n#### using one-hot encoding as before since we can word with high-dimensional data","8f78fe85":"## train-test-split","ffae61d0":"#### again the results are pretty poor\n\n### The next step is to manipulate some of the data that we have in our columns, as a reminder, there were 3 independent columns (gene, variation and text) in the data set and one independent column (the classes)\n\n#### at the moment the gene column is in the format of categorical data \n#### checking how many unique genes appear in the data and how many times each gene appears","0da7eafc":"# Importing Libraries","7d75d056":"# Maximum voting classifier\n#### run them all, choose the best","eb6cbccf":"#### these are great values way over 90%","ad9c546a":"#### there are 9 classes of genetic mutations\n#### since it these are discrete values we might consider this problem a classification problem\n#### within the classification problems it is not binary since we can have more than one output, so this would be a good candidate in my opinion to a multiclass classification problem","ff248fb2":"### well, that is pretty bad which means that the variation feature is not very stable, odd since the log-loss is low and pretty decent ","7d5dd9da":"#### TEXT column seems to be missing a few values (ID has 3321 and Text has 3316)","a6ca26f7":"#### Based on this heatmap the prediction algorithm is pretty bad, I'll take a quick look on the precision matrix","76bf9894":"#### the way it will look like with response encoding is as follow:\n#### alpha: laplace smoothing\n#### feature: gene, variation\n#### df: X_train, x_test, x_crossval\n#### make a vector in which:\n##### -first element: number of times it occured in a clasa +10*alpha \/ n of times it occured in the data + 90*alpha \n##### -gv_dict: look up table for stored values \n##### -gv_fea: for the training data\n\n#### one of the tools used is laplace smoothing, with laplace smoothing the goal is to increase the zero probability values to a small positive number [and correspondingly reduce other values so that the sum is still 1]. Since we are multiplying probabilities, having a probability of 0 can greatly throw off our calculations."}}