{"cell_type":{"c4a56dc0":"code","7929a9bf":"code","57bded86":"code","2175ddc3":"code","d80bfdcb":"code","df75e4ad":"code","ea0586a3":"code","612ca55a":"code","e5dbc76f":"code","c14fbb0c":"code","af4dfcd4":"code","c87b9173":"code","3daa7867":"code","a00a265e":"code","20997622":"code","f8dca089":"code","0d7b8494":"code","35132ea4":"code","713ce6c9":"code","97c2f035":"code","cd668a79":"code","d255fcdb":"code","52701571":"code","d49514a7":"code","572c9c29":"code","c4e3e735":"code","d5756474":"code","718ffe02":"code","6283c041":"markdown"},"source":{"c4a56dc0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7929a9bf":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","57bded86":"df=pd.read_csv('\/kaggle\/input\/sms-spam-collection-dataset\/spam.csv',encoding='iso-8859-1')\n\ndf","2175ddc3":"df=df.iloc[:,0:2].values","d80bfdcb":"df=pd.DataFrame(df)","df75e4ad":"df.columns=['type','text']","ea0586a3":"df","612ca55a":"y=df['type']\nx=df['text']","e5dbc76f":"from sklearn.preprocessing import LabelEncoder\nleb = LabelEncoder()\ny=leb.fit_transform(y) \n","c14fbb0c":"y","af4dfcd4":"x","c87b9173":"# library to clean data \nimport re  \nimport nltk  \nnltk.download('stopwords') \nfrom nltk.corpus import stopwords ","3daa7867":"# Stemming\nfrom nltk.stem.porter import PorterStemmer \ncorpus = []  \n  ","a00a265e":"for i in range(0, 5572):  \n      \n    review = re.sub(r'\\b[\\w\\-.]+?@\\w+?\\.\\w{2,4}\\b', 'emailaddr', df['text'][i])\n    review  = re.sub(r'(http[s]?\\S+)|(\\w+\\.[A-Za-z]{2,4}\\S*)', 'httpaddr',\n                     review)\n    review  = re.sub(r'\u00a3|\\$', 'moneysymb', review)\n    review = re.sub(\n        r'\\b(\\+\\d{1,2}\\s)?\\d?[\\-(.]?\\d{3}\\)?[\\s.-]?\\d{3}[\\s.-]?\\d{4}\\b',\n        'phonenumbr', review)\n    review  = re.sub(r'\\d+(\\.\\d+)?', 'numbr', review)\n\n    # collapse whitespace (spaces, line breaks, tabs) into a single space.\n    # eliminate any leading or trailing whitespace.\n    review  = re.sub(r'[^\\w\\d\\s]', ' ', review)\n    review = re.sub(r'\\s+', ' ', review)\n    review = re.sub(r'^\\s+|\\s+?$', '', review)\n\n    review = review.lower()  \n    review = review.split()  \n    ps = PorterStemmer()   \n    review = [ps.stem(word) for word in review \n                if not word in set(stopwords.words('english'))]  \n    review = ' '.join(review)   \n    corpus.append(review)  ","20997622":"# to extract useful ngrams and create bag of words\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n# to create bag of words model\nvectorizer = TfidfVectorizer(ngram_range=(1, 2))\nX_ngrams = vectorizer.fit_transform(corpus)\nX_ngrams.shape","f8dca089":"from sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn import svm\n\nX_train, X_test, y_train, y_test = train_test_split( X_ngrams,y,test_size=0.3)\n\nclf = svm.LinearSVC(loss='hinge',random_state=0)\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\nmetrics.f1_score(y_test, y_pred)","0d7b8494":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\ncm","35132ea4":"from sklearn.metrics import auc\nfrom sklearn.metrics import precision_recall_curve\nprecision,recall,thresholds=precision_recall_curve(y_test,y_pred)\nauc_recall_pre=auc(recall,precision)\nauc_recall_pre","713ce6c9":"from sklearn.metrics import roc_curve\nfalse_positive,true_positive,_=roc_curve(y_test,y_pred)","97c2f035":"plt.plot(false_positive,true_positive,label='Linear SVM')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC_curve')\nplt.legend()\nplt.show()","cd668a79":"# Using Artificial Neural Network","d255fcdb":"import tensorflow as tf\nann = tf.keras.models.Sequential()\n\n# Adding the input layer and the first hidden layer\nann.add(tf.keras.layers.Dense(units=10000, kernel_initializer='normal',activation='relu',input_dim=36228))\n\n# Adding the second hidden layer\nann.add(tf.keras.layers.Dense(units=5000,kernel_initializer='normal', activation='relu'))\n\n# Adding the second hidden layer\nann.add(tf.keras.layers.Dense(units=1000,kernel_initializer='normal', activation='relu'))\n\n# Adding the output layer\nann.add(tf.keras.layers.Dense(units=1, kernel_initializer='normal',activation='sigmoid'))","52701571":"# compiling model\nann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])","d49514a7":"ann.summary()","572c9c29":"\nann.fit(X_train.todense(), y_train, validation_split=0.15,batch_size = 300, epochs = 15)","c4e3e735":"y_pred=ann.predict(X_test.todense())\ny_pred","d5756474":"y_pred = (y_pred > 0.5)\nprint(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))","718ffe02":"from sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\naccuracy_score(y_test, y_pred)","6283c041":"## ANN"}}