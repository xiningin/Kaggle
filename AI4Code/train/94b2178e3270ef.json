{"cell_type":{"0badd7bb":"code","3fc66bf0":"code","40fc403c":"code","a02cb5f4":"code","db719c16":"code","58c361b9":"code","61185fac":"code","79c7602f":"code","9b284a41":"code","56feb46c":"code","a3ca86f6":"code","cd48cf93":"code","5dd2cff9":"code","a8c2d058":"code","1d9784f8":"code","d2d380f4":"code","e0122904":"code","56bc8f5b":"code","75f1338a":"code","095a288f":"code","18db9cad":"code","37682c01":"code","7b249db1":"code","0edce0ba":"code","632ae141":"code","fcbdcc1e":"markdown","1c39427d":"markdown","9c519c97":"markdown","dc14a5ed":"markdown","1591d7c0":"markdown","a4de8fcc":"markdown","c1f31b7b":"markdown","5ea99d9f":"markdown","1c478599":"markdown","f5174dbc":"markdown","c85f2e19":"markdown","a1ccc585":"markdown","0cd186b9":"markdown","9c7177e6":"markdown","c0633553":"markdown","64bf6703":"markdown","87d80ce4":"markdown","d63bd8ba":"markdown","0f7c5ac8":"markdown","3f41120f":"markdown","f6d97c90":"markdown","bafcea0a":"markdown","1decc03b":"markdown","ef048183":"markdown","ad38f31f":"markdown","1f86029b":"markdown","871cad03":"markdown"},"source":{"0badd7bb":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom scipy.stats import zscore\n\nfrom sklearn.tree import DecisionTreeClassifier # decision tree for classifier problems\nfrom sklearn.ensemble import RandomForestClassifier # random forest for classifier problems\nfrom sklearn.model_selection import train_test_split # tool to split data\n\nfrom sklearn.metrics import accuracy_score # accuracy scorer\nfrom sklearn.metrics import roc_auc_score # auc scorer\nfrom sklearn.metrics import f1_score # f1 scorer\n\nfrom sklearn.model_selection import cross_validate # cross validation tool\nfrom sklearn.model_selection import RandomizedSearchCV # randomized search for hyper parameters on cross validation\n\nfrom sklearn.impute import SimpleImputer # imputer used to replace missing data\n\n# Plotting libraries\npd.plotting.register_matplotlib_converters()\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Set random_state for consistency\nSEED = 42","3fc66bf0":"train_data = pd.read_csv(\"\/kaggle\/input\/cap-4611-spring-21-assignment-1\/train.csv\")\ntest_data = pd.read_csv(\"\/kaggle\/input\/cap-4611-spring-21-assignment-1\/test.csv\")","40fc403c":"print(train_data.info())","a02cb5f4":"print(train_data.describe())","db719c16":"print(train_data.head())","58c361b9":"print(test_data.info())","61185fac":"print(test_data.describe())","79c7602f":"print(test_data.head())","9b284a41":"bounds = np.abs(train_data - train_data.mean()) <= (3 * train_data.std())\noutliers = train_data[~bounds]\nprint(outliers.info())","56feb46c":"train_data_no_outliers = train_data[bounds].drop(columns=[\"id\", \"Bankrupt\"])\nprint(train_data_no_outliers.info())","a3ca86f6":"print(train_data.isna().values.sum())","cd48cf93":"imp = SimpleImputer(strategy='median')\ntrain_data_no_missing = pd.DataFrame(imp.fit_transform(train_data_no_outliers))\ntrain_data_no_missing.columns = train_data_no_outliers.columns\n\ntrain_data_no_missing.insert(0, \"id\", train_data[\"id\"], False)\ntrain_data_no_missing.insert(1, \"Bankrupt\", train_data[\"Bankrupt\"], True)\n\nprint(train_data_no_missing.info())","5dd2cff9":"use_data= train_data\nfeatures = use_data.columns[2:]\ntarget = use_data.columns[1]\n\nX = use_data.loc[:, features]\ny = use_data.loc[:, target]\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=SEED)","a8c2d058":"params_dt = {\"max_depth\": np.arange(2, 12),\n              \"max_features\": [\"auto\", \"sqrt\", \"log2\", None],\n              \"min_samples_leaf\": np.arange(2, 30),\n              \"criterion\": [\"gini\", \"entropy\"]}","1d9784f8":"dt = DecisionTreeClassifier(random_state=SEED)\ndt_cv = RandomizedSearchCV(dt, params_dt, scoring=[\"accuracy\", \"roc_auc\", \"f1\"], refit=\"roc_auc\", random_state=SEED, cv=10)\ndt_cv.fit(X_train, y_train)","d2d380f4":"dt_cv.set_params(refit=\"accuracy\")\ndt_cv_train_acc = dt_cv.score(X_train, y_train)\ndt_cv.set_params(refit=\"roc_auc\")\ndt_cv_train_auc = dt_cv.score(X_train, y_train)\ndt_cv.set_params(refit=\"f1\")\ndt_cv_train_f1 = dt_cv.score(X_train, y_train)","e0122904":"dt_cv.set_params(refit=\"accuracy\")\ndt_cv_test_acc = dt_cv.score(X_test, y_test)\ndt_cv.set_params(refit=\"roc_auc\")\ndt_cv_test_auc = dt_cv.score(X_test, y_test)\ndt_cv.set_params(refit=\"f1\")\ndt_cv_test_f1 = dt_cv.score(X_test, y_test)","56bc8f5b":"print(\"-DT-\")\nprint(\"--BEST PARAMS--\")\nprint(dt_cv.best_params_)\nprint(\"--Training CV--\")\nprint(\"Accuracy Score for cv data:\",dt_cv_train_acc)\nprint(\"AUC Score for cv data:\", dt_cv_train_auc)\nprint(\"F1 Score for cv data:\", dt_cv_train_f1)\nprint(\"--Testing CV--\")\nprint(\"Accuracy Score for cv data:\",dt_cv_test_acc)\nprint(\"AUC Score for cv data:\", dt_cv_test_auc)\nprint(\"F1 Score for cv data:\", dt_cv_test_f1)","75f1338a":"# dt_test_predict = dt.predict_proba(test_data[features])\ndt_cv.set_params(refit=\"roc_auc\")\ndt_test_predict = dt_cv.predict_proba(test_data[features])\ndt_submission = pd.DataFrame({'id': test_data.id, 'Bankrupt': dt_test_predict[:, 1]})\ndt_submission.to_csv('dt_submission.csv', index=False)","095a288f":"params_rf = {\"max_depth\": np.arange(1, 100),\n              \"max_features\": [\"auto\", \"sqrt\", \"log2\"],\n              \"min_samples_leaf\": np.arange(2, 30),\n              \"criterion\": [\"gini\", \"entropy\"]}","18db9cad":"rf = RandomForestClassifier(max_depth=20, min_samples_leaf=8, random_state=SEED)\nrf_cv = RandomizedSearchCV(rf, params_rf, scoring=[\"accuracy\", \"roc_auc\", \"f1\"], refit=\"roc_auc\", random_state=SEED, cv=10)\nrf_cv.fit(X_train, y_train)","37682c01":"rf_cv.set_params(refit=\"accuracy\")\nrf_cv_train_acc = rf_cv.score(X_train, y_train)\nrf_cv.set_params(refit=\"roc_auc\")\nrf_cv_train_auc = rf_cv.score(X_train, y_train)\nrf_cv.set_params(refit=\"f1\")\nrf_cv_train_f1 = rf_cv.score(X_train, y_train)","7b249db1":"rf_cv.set_params(refit=\"accuracy\")\nrf_cv_test_acc = rf_cv.score(X_test, y_test)\nrf_cv.set_params(refit=\"roc_auc\")\nrf_cv_test_auc = rf_cv.score(X_test, y_test)\nrf_cv.set_params(refit=\"f1\")\nrf_cv_test_f1 = rf_cv.score(X_test, y_test)","0edce0ba":"print(\"-RF-\")\nprint(\"--BEST PARAMS--\")\nprint(rf_cv.best_params_)\nprint(\"--Training CV--\")\nprint(\"Accuracy Score for cv data:\",rf_cv_train_acc)\nprint(\"AUC Score for cv data:\", rf_cv_train_auc)\nprint(\"F1 Score for cv data:\", rf_cv_train_f1)\nprint(\"--Testing CV--\")\nprint(\"Accuracy Score for cv data:\",rf_cv_test_acc)\nprint(\"AUC Score for cv data:\", rf_cv_test_auc)\nprint(\"F1 Score for cv data:\", rf_cv_test_f1)","632ae141":"rf_cv.set_params(refit=\"roc_auc\")\nrf_test_predict = rf_cv.predict_proba(test_data[features])\nrf_submission = pd.DataFrame({'id': test_data.id, 'Bankrupt': rf_test_predict[:, 1]})\nrf_submission.to_csv('rf_submission.csv', index=False)","fcbdcc1e":"### Missing Values\nFirst we need to check if there are any missing values on the train_data set.","1c39427d":"## Random Forest(RF)","9c519c97":"### RF Score\nIn order to validate our model we need to score it. I scored it for two different sets, the training set and the testing set that came from the split. I need to refit it to the specific type of scoring for multi-scoring.","dc14a5ed":"### DT Score Report\nI then print out a report that I made including accuracy, auc, and f1 values from both of the sets.","1591d7c0":"### RF Conclusion\nThe Random Forest seems to be the best solution for this problem.","a4de8fcc":"### Outliers\nFirst thing, regarding outliers there are a couple different methods. You can use zscores, but that would only work if there is a normal distribution. So in this case I decided that to find ouliers I needed to filter on values that are farther than +-3 standard deviations.","c1f31b7b":"These means that there are not missing values on the original set at a glance. There needs to be more thorough investigation with domain knowledge of the data set to obtain a more representative data set.\n\nThe other missing values that we need to fix is the outliers that we detected before.","5ea99d9f":"## Data Analysis\nThis section has some data analysis.","1c478599":"### RF Model Building\nWe will then use an RandomizedSearchCV using the random forest classifier and then fit it to the training data and \ntarget.","f5174dbc":"### RF Score Report\nI then print out a report that I made including accuracy, auc, and f1 values from both of the sets.","c85f2e19":"### DT Scores\n\nIn order to validate our model we need to score it. I scored it for two different sets, the training set and the testing set that came from the split. I need to refit it to the specific type of scoring for multi-scoring.","a1ccc585":"### Split Data\nHere we are narrowing the data into the target set and features set.\nThen we are going to split the obtain the features set into a train and test data set. After testing I decided that train_data provide better results than the train_data_no_outliers.","0cd186b9":"This is the validation values for the model using testing data.","9c7177e6":"## Decision Tree(DT)","c0633553":"Then we check the training data and testing data.","64bf6703":"# Assignment 1 for CAP4611 with Professor Hollander\nby Enelson Castro","87d80ce4":"### Normalization & Standardization\nTree models and ensembles don't need normalization and standardization since all we are doing on both Decision Trees and Random Forest models is comparing features and banching on the maximum Information Gain.","d63bd8ba":"### DT Conclusion\nIt seems to me that the Decision Tree is not better for auc.","0f7c5ac8":"### Reading CSV\nFirst we read in the training data and test data.","3f41120f":"## Imports\nImport the modules I will need for this assignment.","f6d97c90":"This is the validation values for the model using testing data.","bafcea0a":"This is the validation values for the model using training data.","1decc03b":"This is the validation values for the model using training data.","ef048183":"### DT Hyperparameters\nWe decide on dealing with:\n* Max Depth\n* Max Features\n* Min Samples Leaf\n* Criterion\n\nThe reason we are dealing with these hyperparameters is in order to optimize to get low variation and low based fitted model. The current ranges were decided based on pluging in and seeing the cv of the accuracy, auc, and f1 for those changes.","ad38f31f":"### DT Model Building\nWe will then use an RandomizedSearchCV using the decision tree classifier and then fit it to the training data and target.","1f86029b":"Now these are the statistical outliers for the method I am using. Then I have to build training data without outliers and without the features \"id\" and \"Bankrupt\".","871cad03":"### RF Hyperparameters\nWe decide on dealing with:\n* Max Depth\n* Max Features\n* Min Samples Leaf\n* Criterion\n\nThe reason we are dealing with these hyperparameters is in order to optimize to get low variation and low based fitted model. The current ranges were decided based on pluging in and seeing the cv of the accuracy, auc, and f1 for those changes."}}