{"cell_type":{"5f9fe9e7":"code","9bbb2bbd":"code","87e2e272":"code","cfebe2ea":"code","4fbd1c6a":"code","bf41021a":"code","a19ca78e":"code","90769293":"code","fbf44ead":"code","6236d71f":"code","80673d80":"code","76bb8a65":"code","e00bdeec":"code","4793e6b1":"markdown"},"source":{"5f9fe9e7":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nimport keras\nfrom PIL import Image\nfrom keras.preprocessing import image\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPool2D, Flatten,Dense,Dropout,BatchNormalization\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport cv2\nfrom keras.layers import Input, Lambda, Dense, Flatten\nfrom tensorflow.keras.applications import VGG16, InceptionResNetV2\nfrom keras import regularizers\nfrom tensorflow.keras.optimizers import Adam,RMSprop,SGD,Adamax","9bbb2bbd":"train_dir = '..\/input\/yoga-poses-dataset\/DATASET\/TRAIN' #directory with training images\ntest_dir = '..\/input\/yoga-poses-dataset\/DATASET\/TEST' #directory with testing images","87e2e272":"train_datagen = ImageDataGenerator(width_shift_range= 0.1,\n                                  horizontal_flip = True,\n                                  rescale = 1.\/255,\n                                  validation_split = 0.2)\ntest_datagen = ImageDataGenerator(rescale = 1.\/255,\n                                 validation_split = 0.2)","cfebe2ea":"train_generator =  train_datagen.flow_from_directory(directory = train_dir,\n                                                    target_size = (224,224),\n                                                    color_mode = 'rgb',\n                                                    class_mode = 'categorical',\n                                                    batch_size = 16,\n                                                    subset = 'training')\nvalidation_generator  = test_datagen.flow_from_directory(directory = test_dir,\n                                                  target_size = (224,224),\n                                                  color_mode = 'rgb',\n                                                  class_mode = 'categorical',\n                                                  subset = 'validation')","4fbd1c6a":"model = tf.keras.models.Sequential([\n    tf.keras.layers.Conv2D(64, (3,3), activation='relu',padding = 'Same', input_shape=(224, 224, 3)),\n    tf.keras.layers.MaxPooling2D(2, 2),\n    tf.keras.layers.Dropout(0.25),\n    #tf.keras.layers.Conv2D(128, (3,3), activation='relu',padding = 'Same'),\n    #tf.keras.layers.MaxPooling2D(2,2),\n    #tf.keras.layers.Dropout(0.25),\n    #tf.keras.layers.Conv2D(128, (3,3), activation='relu',padding = 'Same'),\n    #tf.keras.layers.MaxPooling2D(2,2),\n    #tf.keras.layers.Dropout(0.25),\n    tf.keras.layers.Conv2D(128, (3,3), activation='relu',padding = 'Same'),\n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Dropout(0.25),\n    tf.keras.layers.Conv2D(256, (3,3), activation='relu',padding = 'Same'),\n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Dropout(0.25),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(1024, activation='relu'),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(5, activation='softmax')\n])","bf41021a":"optimizer = Adam(lr=0.001)\nmodel.compile(loss='categorical_crossentropy',\n              optimizer = optimizer,\n              metrics=['accuracy'])\nepochs = 50  \nbatch_size = 16","a19ca78e":"model.summary()","90769293":"from PIL import ImageFile\nImageFile.LOAD_TRUNCATED_IMAGES = True","fbf44ead":"history = model.fit(train_generator, epochs = epochs,validation_data = validation_generator)","6236d71f":"fig , ax = plt.subplots(1,2)\ntrain_acc = history.history['accuracy']\ntrain_loss = history.history['loss']\nfig.set_size_inches(12,4)\n\nax[0].plot(history.history['accuracy'])\nax[0].plot(history.history['val_accuracy'])\nax[0].set_title('Training Accuracy vs Validation Accuracy')\nax[0].set_ylabel('Accuracy')\nax[0].set_xlabel('Epoch')\nax[0].legend(['Train', 'Validation'], loc='upper left')\n\nax[1].plot(history.history['loss'])\nax[1].plot(history.history['val_loss'])\nax[1].set_title('Training Loss vs Validation Loss')\nax[1].set_ylabel('Loss')\nax[1].set_xlabel('Epoch')\nax[1].legend(['Train', 'Validation'], loc='upper left')\n\nplt.show()","80673d80":"train_loss, train_acc = model.evaluate(train_generator)\ntest_loss, test_acc   = model.evaluate(validation_generator)\nprint(\"final train accuracy = {:.2f} , validation accuracy = {:.2f}\".format(train_acc*100, test_acc*100))","76bb8a65":"model.save('YogaNet_model_1_1.h5')","e00bdeec":"from keras.models import load_model\nfrom keras.applications.vgg16 import preprocess_input\nmodel = load_model('YogaNet_model_1.h5')\nimg = image.load_img('..\/input\/yoga-poses-dataset\/DATASET\/TEST\/goddess\/00000000.jpg', target_size=(224, 224))\nx = image.img_to_array(img)\nx = np.expand_dims(x, axis=0)\nimg_data = preprocess_input(x)\nclasses = model.predict(img_data)\nprint(classes) ","4793e6b1":"The above prediction was correct."}}