{"cell_type":{"0d5c8fe1":"code","3edb2246":"code","24fa9f8b":"code","e2121cce":"code","ab795149":"code","d209d63e":"code","9411beee":"code","45706265":"code","e596781a":"code","6cdefdea":"code","c56aa278":"code","c7d3cd6d":"code","c750091a":"code","747c8fb1":"code","f86e0bf3":"code","c69be67e":"code","b46e8642":"markdown","774f43c1":"markdown","e3393e02":"markdown","d03c42fb":"markdown","1ce04907":"markdown","59c6ac51":"markdown","a394aa25":"markdown","4f7747c5":"markdown","b72e6973":"markdown","480764f4":"markdown","f0a5db84":"markdown","5fb4883c":"markdown","10dc4ca4":"markdown","efc02c8e":"markdown"},"source":{"0d5c8fe1":"!pip install contractions","3edb2246":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport pandas as pd\nimport numpy as np\nimport re\nimport collections\nimport contractions\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.style.use('dark_background')\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nimport warnings\nwarnings.simplefilter(action='ignore', category=Warning)\nimport keras\nfrom keras.layers import Dense, Embedding, LSTM, Dropout\nfrom keras.models import Sequential\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences","24fa9f8b":"df = pd.read_csv(\"\/kaggle\/input\/sms-spam-collection-dataset\/spam.csv\", encoding='latin-1')\ndf.head()","e2121cce":"df.shape","ab795149":"df.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1, inplace=True)","d209d63e":"# renaming the columns\ndf.columns = [\"Spam or Ham\",\"Tweet\"]\ndf.head()","9411beee":"sns.countplot(df[\"Spam or Ham\"])","45706265":"df[\"Spam or Ham\"].value_counts()","e596781a":"def word_count_plot(data):\n     \n     word_counter = collections.Counter([word for sentence in data for word in sentence.split()])\n     most_count = word_counter.most_common(30)\n     \n     most_count = pd.DataFrame(most_count, columns=[\"Word\", \"Count\"]).sort_values(by=\"Count\")\n     most_count.plot.barh(x = \"Word\", y = \"Count\", color=\"green\", figsize=(10, 15))\nword_count_plot(df[\"Tweet\"])","6cdefdea":"lem = WordNetLemmatizer()\ndef preprocessing(data):\n      sms = contractions.fix(data) \n      sms = sms.lower()\n      sms = re.sub(r'https?:\/\/S+|www.S+', \"\", sms).strip() #removing url\n      sms = re.sub(\"[^a-z ]\", \"\", sms) # removing symbols and numbes\n      sms = sms.split() \n      sms = [lem.lemmatize(word) for word in sms if not word in set(stopwords.words(\"english\"))]\n      sms = \" \".join(sms)\n      return sms\nX = df[\"Tweet\"].apply(preprocessing)","c56aa278":"word_count_plot(X)","c7d3cd6d":"from sklearn.preprocessing import LabelEncoder\nlb_enc = LabelEncoder()\ny = lb_enc.fit_transform(df[\"Spam or Ham\"])","c750091a":"tokenizer = Tokenizer() \ntokenizer.fit_on_texts(X)\ntext_to_sequence = tokenizer.texts_to_sequences(X) ","747c8fb1":"max_length_sequence = max([len(i) for i in text_to_sequence])\n \npadded_sequence = pad_sequences(text_to_sequence, maxlen=max_length_sequence, \n                                    padding = \"pre\") \npadded_sequence","f86e0bf3":"VOC_SIZE = len(tokenizer.word_index)+1\ndef create_model():\n    \n      model = Sequential()\n      model.add(Embedding(VOC_SIZE, 32, input_length=max_length_sequence))\n      model.add(LSTM(100))\n      model.add(Dropout(0.4))\n      model.add(Dense(20, activation=\"relu\"))\n      model.add(Dropout(0.5))\n      model.add(Dense(1, activation = \"sigmoid\"))\n      return model\nlstm_model = create_model()\nlstm_model.compile(optimizer = \"adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"])\nlstm_model.summary()","c69be67e":"lstm_model.fit(padded_sequence, y, epochs = 5, batch_size=16, validation_split=0.2)\n","b46e8642":"#### There are some unwanted columns in our dataset, so we are removing it","774f43c1":"#### Padding the input tokenized text sequence to make all the sequence of equal length","e3393e02":"#### Word count plotting after preprocessing techniques","d03c42fb":"#### Plotting the value counts","1ce04907":"#### Encoding the output variables","59c6ac51":"#### Training the model","a394aa25":"#### Performing data preprocessing techniques","4f7747c5":"#### Both validation and training accuracy is good.\n#### Also refer my article about sms spam detection using lstm - [here](https:\/\/www.analyticsvidhya.com\/blog\/2021\/05\/sms-spam-detection-using-lstm-a-hands-on-guide\/)\n#### Do upvote if you like this notebook","b72e6973":"#### Importing the required libraries","480764f4":"#### Creating the LSTM Model","f0a5db84":"## Spam SMS Detection using LSTM","5fb4883c":"#### Creating a function for visualizing the count of words in the sms","10dc4ca4":"#### Importing the spam sms dataset","efc02c8e":"#### Tokenizing the input text using keras tokenizer"}}