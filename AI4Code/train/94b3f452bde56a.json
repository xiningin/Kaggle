{"cell_type":{"714bf825":"code","e3caae69":"code","165e59ad":"code","1d3a673c":"code","09b71df8":"code","78250e22":"code","6c285f88":"code","5088799e":"code","7e9b5f77":"code","ab229004":"code","d63d0a19":"code","2dd792ca":"code","90dbf7ee":"code","2c35dba4":"code","969be3ff":"code","3ab3d7a9":"code","596ac8dd":"code","413bd82f":"code","262a0e15":"code","05f07fb7":"code","9c13fa9b":"code","2fbbd064":"markdown","d858b45e":"markdown","fdb4cbb8":"markdown","7609df39":"markdown","f60a90e9":"markdown","9887a2c5":"markdown","5714b76a":"markdown","4a615566":"markdown","33d64fed":"markdown","b70d0ce6":"markdown","96445cdc":"markdown","826a2cf9":"markdown","57861dfd":"markdown","6467d072":"markdown","700d26c5":"markdown","179bcded":"markdown","60963674":"markdown","45cd1599":"markdown","f2b4c2f4":"markdown","1bafd56e":"markdown","35c71f1d":"markdown","ec5b14ee":"markdown","1d670021":"markdown","acc346c7":"markdown","6615c533":"markdown","321f3ca9":"markdown","fbf8807e":"markdown","f32197db":"markdown","fd615c20":"markdown","396ecf39":"markdown","af2f85cd":"markdown","050d4fe6":"markdown","c991cf63":"markdown"},"source":{"714bf825":"import numpy as np\nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt \nimport plotly.express as px\nimport os\n#Model\nfrom sklearn import preprocessing\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import accuracy_score,f1_score,confusion_matrix\nfrom sklearn.model_selection import train_test_split,cross_val_score\nfrom sklearn import svm\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\n#Optimisation\nimport pickle\nimport optuna","e3caae69":"def basic_EDA(df):\n    size = df.shape\n    sum_duplicates = df.duplicated().sum()\n    sum_null = df.isnull().sum().sum()\n    is_NaN = df. isnull()\n    row_has_NaN = is_NaN. any(axis=1)\n    rows_with_NaN = df[row_has_NaN]\n    count_NaN_rows = rows_with_NaN.shape\n    return print(\"Number of Samples: %d,\\nNumber of Features: %d,\\nDuplicated Entries: %d,\\nNull Entries: %d,\\nNumber of Rows with Null Entries: %d %.1f%%\" %(size[0],size[1], sum_duplicates, sum_null,count_NaN_rows[0],(count_NaN_rows[0] \/ df.shape[0])*100))\n\ndef summary_table(df):\n    summary = pd.DataFrame(df.dtypes,columns=['dtypes'])\n    summary = summary.reset_index()\n    summary['Name'] = summary['index']\n    summary = summary[['Name','dtypes']]\n    summary['Missing'] = df.isnull().sum().values    \n    summary['Uniques'] = df.nunique().values\n    return summary\n\ndef percentPlot(heart_df,categorical_class,title,hue):\n    j = 0\n    for i in categorical_class:    \n\n        plt.figure(figsize=(20,5))\n        ax = sns.countplot(x=i, data=heart_df,palette=\"Blues_d\", hue = hue)\n\n        for p in ax.patches:\n                percentage = '{:.1f}%'.format(100 * p.get_height()\/total)\n                x = p.get_x() + p.get_width()\/2\n                y = p.get_y() + p.get_height() + 0.02\n                ax.annotate(percentage, (x, y),\n                            ha='center', va='bottom', \n                            color='black', xytext=(0, 3),\n                            rotation = 'horizontal',\n                            textcoords='offset points')\n\n        sns.despine(top=True, right=True, left=True, bottom=False)\n        plt.xticks(rotation=0,fontsize = 12)\n        ax.set_xlabel(title[j],fontsize = 14)\n        ax.set(yticklabels=[])\n        ax.axes.get_yaxis().set_visible(False)\n        plt.title('Distribution of '+title[j],fontsize = 14,weight = 'bold')\n        j+=1\n        plt.show()\n    ","165e59ad":"heart_df = pd.read_csv('..\/input\/heart-disease-uci\/heart.csv')","1d3a673c":"basic_EDA(heart_df)","09b71df8":"summary_table(heart_df)","78250e22":"#Remove Duplicates\nheart_df.drop_duplicates(inplace = True)","6c285f88":"#Dictionaries to make the understanding of the data easier\ngender_dict = {\n    0: 'Female',\n    1: 'Male'}\ncp_dict = {\n    0:'Typical Angina',\n    1:'Atypical Angina',\n    2:'Non-anginal Pain',\n    3:'Asymptomatic'}\nslope_dict = {\n    0: 'Upsloping',\n    1:'Flat',\n    2:'Downsloping'}\nrestecg_dict = {\n    0: 'Normal',\n    1: 'Wave Abnormality',\n    2: 'Left Ventr. Hypertrophy'}\nbin_dict = {\n    0:'No',\n    1:'Yes'\n}\n","5088799e":"categorical_feat = ['sex','cp','fbs','restecg','exang','slope','ca','thal']\ncat_title = ['Gender','Chest Pain Type','Fasting Blood Sugar > 120 mg\/dl',\n         'Resting ECG','Exercise Induced Angina','Slope of ST Exercise',\n         'Number of Vessels Colored','Thalassemia' ]\n\nnumerical_feat = ['age','trestbps','chol','thalach','oldpeak']\nnum_title = ['Age', 'Resting Blood Pressure','Cholesterol','Max. Heart Rate', 'ST Depression Induced by Exercise']\n\nheart_df['sex'] = heart_df['sex'].map(gender_dict.get)\nheart_df['cp'] = heart_df['cp'].map(cp_dict.get)\nheart_df['slope'] = heart_df['slope'].map(slope_dict.get)\nheart_df['restecg'] = heart_df['restecg'].map(restecg_dict.get)\nheart_df['fbs'] = heart_df['fbs'].map(bin_dict.get)\nheart_df['exang'] = heart_df['exang'].map(bin_dict.get)\nheart_df['target'] = heart_df['target'].map(bin_dict.get)\n\ntotal = len(heart_df['target'])\n\npercentPlot(heart_df,categorical_feat,cat_title,None)","7e9b5f77":"percentPlot(heart_df,['target'],['Target'],None)\npercentPlot(heart_df,categorical_feat,cat_title,'target')","ab229004":"plt.figure(figsize=(20,8));\ng = sns.pairplot(heart_df,vars=['age','trestbps','chol','thalach','oldpeak'],hue = 'target',corner=True,\n                 plot_kws=dict(s = 8),\n                 diag_kws=dict(linewidth=0,alpha=.5));\ng.fig.suptitle('Relationship between Numeric Variables and Target',fontsize=16, weight = 'bold');","d63d0a19":"fig = px.scatter_3d(heart_df, x=\"chol\", y=\"thalach\", z=\"age\",color='target',opacity=0.9)\nfig.update_traces(marker=dict(size=5))#, selector=dict(type='scatter3d'))\nfig.update_layout(title=dict(text = 'Age, Cholesterol and Heart Rate 3D plot'))\nfig.show()","2dd792ca":"plt.figure(figsize=(20,8))\nsns.set(style=\"ticks\", font_scale = 1)\nax = sns.boxplot(x=\"sex\", y=\"age\", data=heart_df, hue = 'target')\nsns.despine(top=True, right=True, left=False, bottom=False)\nax.set_xlabel('Gender',fontsize = 14,weight = 'bold')\nax.set_ylabel('Age',fontsize = 14,weight = 'bold')\nplt.title('Age and Gender', fontsize = 16,weight = 'bold');","90dbf7ee":"heartTarget = heart_df[heart_df['target']==1]\nplt.figure(figsize=(20,8))\nsns.set(style=\"ticks\", font_scale = 1)\nax = sns.countplot(data = heart_df,x='age',palette=\"Blues_d\", hue = 'sex')\nsns.despine(top=True, right=True, left=False, bottom=False)\nax.set_xlabel('Age',fontsize = 14,weight = 'bold')\nax.set_ylabel('Count',fontsize = 14,weight = 'bold')\nplt.title('Age and Gender Distribution for Positive Heart Attack Samples', fontsize = 16,weight = 'bold');","2c35dba4":"heart_df['age_chol'] = heart_df['age'].div(heart_df['chol'])\nheart_df['chol_thalach'] = heart_df['chol'].div(heart_df['thalach'])\nheart_df['thalach_trestbps'] = heart_df['chol'].div(heart_df['trestbps'])\nheart_df['age_thalach'] = heart_df['age'].div(heart_df['thalach'])\n\nfig, axarr = plt.subplots(1,4, figsize=(16, 6))\n\nsns.set(style=\"ticks\", font_scale = 1)\nsns.despine(top=True, right=True, left=False, bottom=False)\n\ndfs = ['age_chol','chol_thalach','thalach_trestbps','age_thalach']\nz = 0\nfor j in range(0,4):\n    ax = sns.kdeplot(data = heart_df, x = dfs[z], hue = 'target',ax=axarr[j]);\n    z +=1\n        \naxarr[0].set_title(\"Age And Cholesterol Ratio\")\naxarr[1].set_title(\"Cholesterol And Heart Rate Ratio\")\naxarr[2].set_title(\"Heart Rate and Blood Pressure Ratio\")\naxarr[3].set_title(\"Age and Heart Rate Ratio\")\n\nfig.tight_layout(pad=3.0)\nplt.suptitle('Density Plots of New Features',fontsize=16, weight = 'bold');\n      \nplt.show()","969be3ff":"df = pd.read_csv('..\/input\/heart-disease-uci\/heart.csv')\nKmeanData = df.copy()\nscaler = preprocessing.StandardScaler()\nKmeanData = scaler.fit_transform(KmeanData.iloc[:,:-1])\nkmeans = KMeans(n_clusters=2)\n\ndf[\"Cluster\"] = kmeans.fit_predict(KmeanData)\n\nplt.figure(figsize=(20,8));\ng = sns.pairplot(df,vars=['age','trestbps','chol','thalach','oldpeak'],hue = 'Cluster',corner=True,\n                 plot_kws=dict(s = 8),\n                 diag_kws=dict(linewidth=0,alpha=.5));\ng.fig.suptitle('Relationship between Numeric Variables and K-Means Clusters',fontsize=16, weight = 'bold');","3ab3d7a9":"np.random.seed(42)\ndf = pd.read_csv('..\/input\/heart-disease-uci\/heart.csv')\ntarget = df.iloc[:,-1]\n\nfeature = df.drop(columns=['target'])\nX_train, X_test, y_train, y_test = train_test_split(feature, target, test_size=0.2)","596ac8dd":"optuna.logging.set_verbosity(optuna.logging.WARNING) #Disable Warnings\n\ndef create_model(trial):\n    model_type = trial.suggest_categorical('model_type', ['random-forest', 'logistic-regression'])\n        \n    if model_type == 'logistic-regression':\n        regularization = trial.suggest_uniform('logistic-regularization', 0.01, 20)\n        model = LogisticRegression( C=regularization, solver='liblinear')\n    \n    if model_type == 'random-forest':\n        rf_n_estimators = trial.suggest_int(\"rf_n_estimators\", 10, 500)\n        rf_max_depth = trial.suggest_int(\"rf_max_depth\", 1, 50)\n        rf_min_samples = trial.suggest_int(\"rf_min_samples\", 1, 50)\n        rf_min_samples_leaf = trial.suggest_int(\"rf_min_samples_leaf\", 1, 50)\n        rf_ccp_alpha = trial.suggest_float(\"rf_ccp_alpha\", 0, 1)\n\n        model = RandomForestClassifier(max_depth=rf_max_depth, \n                                        n_estimators=rf_n_estimators,\n                                        min_samples_split = rf_min_samples,\n                                        min_samples_leaf = rf_min_samples_leaf,\n                                        ccp_alpha = rf_ccp_alpha)\n        \n    if trial.should_prune():\n            raise optuna.TrialPruned()\n            \n    return model\n\n\ndef objective(trial):\n    model = create_model(trial)\n    score = cross_val_score(model, X_train, y_train, n_jobs=-1, cv=5,scoring='f1')\n    accuracy = score.mean()\n    # Save a trained model to a file.\n    with open(\"{}.pickle\".format(trial.number), \"wb\") as fout:\n        pickle.dump(model, fout)\n    return accuracy\n\nstudy = optuna.create_study(direction=\"maximize\")\nstudy.optimize(objective, n_trials=500)\nprint(f\"The best parameters are : \\n{study.best_params}\")\nprint(\"Mean F1-Score of Cross-Validation Sets: %.4f\"% (study.best_value))","413bd82f":"optuna.logging.set_verbosity(optuna.logging.WARNING) #Disable Warnings\n\ndef create_model(trial):\n    regularization = trial.suggest_uniform('logistic-regularization', 0.0001, 1)\n    #lr_intercept_scaling = trial.suggest_uniform('lr_intercept_scaling', 0.0001, 1)\n    model = LogisticRegression( C=regularization, solver='liblinear')\n    \n    if trial.should_prune():\n            raise optuna.TrialPruned()\n            \n    return model\n\ndef objective(trial):\n    model = create_model(trial)\n    score = cross_val_score(model, X_train, y_train, n_jobs=-1, cv=5,scoring='f1')\n    accuracy = score.mean()\n    # Save a trained model to a file.\n    with open(\"{}.pickle\".format(trial.number), \"wb\") as fout:\n        pickle.dump(model, fout)\n    return accuracy\n\nstudy = optuna.create_study(direction=\"maximize\");\nstudy.optimize(objective, n_trials=500);\nprint(\"Mean F1-Score of Cross-Validation Sets: %.4f\"% (study.best_value))\nprint(\"The best parameters are: \", (study.best_params))","262a0e15":"with open(\"{}.pickle\".format(study.best_trial.number), \"rb\") as fin:\n    best_clf = pickle.load(fin)\n    best_clf.fit(X_train, y_train)\n    y_pred = best_clf.predict(X_test)\nprint(\"Accuracy of Test Set: %.2f\"% (accuracy_score(y_test, y_pred)))\nprint(\"F1-Score of Test Set: %.2f\"% (f1_score(y_test, y_pred)))","05f07fb7":"categories = ['No','Yes']\n\nCMatrix_test = pd.DataFrame(confusion_matrix(y_test, y_pred), columns=categories, index =categories)\nCMatrix_train = pd.DataFrame(confusion_matrix(y_train, best_clf.predict(X_train)), columns=categories, index =categories)\n\nfig, axarr = plt.subplots(1,2, figsize=(16, 6))\n\nsns.set(style=\"ticks\", font_scale = 1)\nsns.despine(top=True, right=True, left=False, bottom=False)\n\ndfs = [CMatrix_train,CMatrix_test]\nz = 0\nfor j in range(0,2):\n    ax = sns.heatmap(dfs[z], annot = True, fmt = 'g' ,vmin = 0, vmax = 50,cmap = 'Blues',ax=axarr[j])\n\n    axarr[j].set_xlabel('Predicted',fontsize = 14,weight = 'bold')\n    axarr[j].set_xticklabels(ax.get_xticklabels(),rotation =90)\n    axarr[j].set_ylabel('Actual',fontsize = 14,weight = 'bold')    \n\n    z +=1\n        \naxarr[0].set_title(\"Confusion Matrix - Training Set\");\naxarr[1].set_title(\"Confusion Matrix - Test Set\");\n\nfig.tight_layout(pad=3.0)\nplt.suptitle('Confusion Matrix',fontsize=16, weight = 'bold');\n      \nplt.show()","9c13fa9b":"def probability_class(y_proba, true_label):\n    plt.figure(figsize=(20,5))\n    ax = sns.kdeplot(y_proba[true_label==0], label=\"No Heart Attack\",linewidth=0,alpha=.5, fill = True)    \n    ax = sns.kdeplot(y_proba[true_label==1], label=\"Heart Attack\",linewidth=0,alpha=.5,fill = True)\n    plt.xticks(np.arange(0,1, 0.1))\n    plt.xlabel(\"Predicted probability values for Positive Heart Attack\")\n    plt.ylabel(\"Density\")\n    plt.title(\"Predicted probability values against the true Target - Training Set\",fontsize=16, weight = 'bold'); \n    plt.legend();\n    \nprobability_class(best_clf.predict_proba(X_train)[:,1],y_train)","2fbbd064":"From the models chosen to be run with the Optuna library, Logistic Regression is the one which presented better results. ","d858b45e":"Finally, let's analyse the Confusion Matrix and see how our model perfomance in greater detail:","fdb4cbb8":">Optuna Code","7609df39":"* It is interesting that the KMeans algorithm has separated the samples in a similar manner, even though it was fitted without the Target column\n* This method could be used as an additional feature for the final model. However, it should be fitted only to the training set, not the whole dataset as we did to create the above visualisation ","f60a90e9":">Confusion Matrix","9887a2c5":"From the plot above we understand where the False Positives are coming from. Looking at the No Heart Attack samples, there is a significant number of them with a probability higher than 0,5. In contrast, Class 1 samples behave as expected, having a sharp descent as it gets closer to 0,5.\n\nIt is a valid strategy to change the cutoff to a higher value to increase the accuracy. However, both classes are already nicely separated and it is preferrable to use such manouvers as a last resort.\n\n## Future Work\n\nFor this application, Logistic Regression achieved almost 90% accuracy. As additional ideas to improve model performance:\n* Ensemble of different models\n* More effort on creating new features\n* Perform a deeper analysis to understand what kind of samples are being misclassified. From this study, generate synthetic samples with such characteristics to improve model generalisation","5714b76a":"* In the training set, the number of samples misclassified as Yes is expressive if compared to the number of samples misclassified as No. The same trend is not visible in the Test set\n* In a way, it is best that the model present a False Positive tendency than the contrary for this application\n\nIt is possible to visualise how our model is predicting the probability of each samples by using KDE plots. By looking at this distribution one can conclude if raising the class probability cuttof from 0,5 to a higher value we can adjust the model tendency to FP.","4a615566":"## 2.3 New Features\n\nIn the previous section we saw the Heart Rate, Cholesterol and Age were features that presented a good separation between the targets. More importantly, the scatter plots where features were plotted against the Heart Rate (thalach) presented a better sample separation. The code below creates new features by dividing one by the other:\n* age_chol: ratio of the Age and Cholesterol level of the sample\n* chol_thalach: ratio of the Heart Rate and Cholesterol level of the sample\n* thalach_trestbps: ratio of the Heart Rate and Blood Pressure level of the sample\n* age_thalach: ratio of the Age and Heart Rate of the sample","33d64fed":"* It is possible to visualise a higher concentration of target = 1 samples in the region where lower values for Age and Cholesterol and high values of Heart rate meet\n* Perhaps a new feature that takes into account these three indicators could be helpful to the model","b70d0ce6":"* Age, oldpeak (ST Depression Induced by Exercise), and Heart Rate (thalach) are the features where both targets are visually separable\n* The KDE plot for Blood Pressure (trestbps) and Cholesterol (chol) are quite similar for both targets. As such, the scatter plots for these two features are not separable by class\n* Considering the Age Feature:\n    * The KDE plot shows for class 1 a mean around the age of 50, with a secondary peak at the age of 40. For class 0, there is a narrower KDE curve, meaning more samples are closer to their mean value. The blue right-tail curve presents a mean slightly higher than class 1, ~ 55-60 years\n    * Regarding cholesterol (chol) and Heart Rate (thalach), from the ages 35 to 50 it appears to be a majority of class 1 (heart attack samples). Following what the KDE plot demonstrated\n    * The samples seem to be distributed according to the oldpeak (ST Depression Induced by Exercise) value, not the age as much. Orange (class 1) samples are more evident for the values of 0, 1 and 2 for oldepeak\n* Considering the thalac (Heart Rate) Feature:\n    * The KDE plot shows a distinct distribution for the target samples. Class 1 samples present a higher mean than class 0 samples\n    * The higher mean pattern is visible across the scatter pair plots for this feature. Note how most of the orange samples (class 1) are usually concentrated for higher values of thalach\n    \n","96445cdc":"Now let's analyse the relationship between the target and numerical data.","826a2cf9":">Optuna Code for LR","57861dfd":"### K-Means Clustering\n\nThe section below performs a quick analysis using KMeans to cluster the data. The objective is to understand if KMeans clusters can help us visualise any other patterns we may have missed.  Later on, such insights could help us create new features for the ML model.","6467d072":"* The mean Age is higher for Females considering both Targets, Yes and No Heart Attack\n* The mean age for Males with No Heart Attacks is higher than samples with Heart Attacks, which is unexpected\n* The Interquantile Range for No Heart Attacks targets presents a smaller variability if compared to the Yes class. This is valid for both genders\n* For positive Heart Attack samples, the median for Male and Females are similar. For Negative samples, Females have a higher median than Males\n\nUsing a bar plot, we can visualise the Age and Gender samples in more detail.","700d26c5":">Class Probability Plot","179bcded":"<img src=\"https:\/\/images.unsplash.com\/photo-1460672985063-6764ac8b9c74?ixid=MXwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHw%3D&ixlib=rb-1.2.1&auto=format&fit=crop&w=1355&q=80\" width=\"500\">\nPhoto by <a href=\"https:\/\/unsplash.com\/@jessedo81?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">jesse orrico<\/a> on <a href=\"https:\/\/unsplash.com\/s\/photos\/heart?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Unsplash<\/a>\n\n# Heart Attack Prediction - UCI Dataset\n\n\n## <center style=\"background-color:Gainsboro; width:40%;\">Contents<\/center>\n1. [Overview](#1.-Overview)<br>\n    1.1 [The Attributes](#1.1.-The-Attributes)<br>\n    1.2 [Acknowledgements](#1.2.-Acknowledgements)<br>\n2. [The Data](#2.-The-Model)<br>\n    2.1 [Target Feature Analysis](#2.1.-Target-Feature-Analysis)<br>\n    2.2 [Age and Gender](#2.2.-Age-and-Gender)<br>\n    2.3 [New Features](#2.2.-New-Features)<br>\n3. [Model](#3.-Model)<br>   \n4. [Results and Conclusion](#4.-Results-and-Conclusion)<br>\n\n***Please remember to upvote if you find this Notebook helpful!***","60963674":"* The features that were able to distinct the most between the classes are the ones that combined Age and Cholesterol to Heart rate (second and fourth plots)\n* The first and third plots present quite similiar distribution between classes, as such is not clear if it will be helpful to the ML model","45cd1599":"* The dataset contains a small number of samples and displays a high unbalance regarding some features\n* Almost 70% of samples are from male patients, such data could be dangerously biased toward this important feature\n* Most samples are from patients with lower fasting blood sugar (85%)\n* Other features that present a high gap  between the different feature categories are: \n    * Resting ECG - where only 1% are from type 2 (left ventricular hypertrophy). The remaining options are equally distributed\n    * Slope ST Exercise - samples are equally distributed between flat (1) and downsloping (2), with only 7% showing an upslope after the exercise\n    * Vessels Coloured - Majority of samples (~60%) from patients where none of the vessels was coloured in the fluoroscopy test","f2b4c2f4":"# 2.2 Age, Gender and Heart Attacks","1bafd56e":">Data Preparation","35c71f1d":"The above outputs show that no Data Cleansing is required, only a duplicated entry needs to be deleted. We also identify the numerical from the categorical features by looking at the Summary table. \n\nEven though they are simple, the bar plots provide a good insight regarding the model categorical features:","ec5b14ee":">Functions","1d670021":"From the scatter plots, it was possible to notice that age, thalach and chol were important features as they provided nice target separations. Could we extract any additional insight using a 3D plot from these three features?","acc346c7":"# 2. The Data\n\nFirst, we review the basic information of the dataset, i.e. number of features, samples, missing values and the necessity of any Data Cleansing. Next, bar plots are used to have a broad understanding of the categorical features. \n\nIn the following sections, we have a deeper look at the Target feature and its relationship with the remaining features. Scatter and Density plots are used to visualise patterns with numerical data. ","6615c533":">Libraries","321f3ca9":"# Key Remarks:\n\n* Overall, the dataset is balanced between both classes of the Target feature. However, there is an expressive unbalance regarding gender, where Males compose ~70% of the data\n* The Female samples are mostly from positive Heart Attacks\n* The median age for Males with No Heart Attacks is higher than for Males that had Heart Attacks\n* A higher concentration of positive Heart Attack samples appears in the region where lower values for Age and Cholesterol and higher values of Heart rate meet\n* Data Scaling, OneHotEncoding and the use of KMean for the creation of new features did not improve model generalisation\n* In the training set results, the model shows a tendency for FP\n* Best Model is Logistic Regression with 89,7% Accuracy. The models were finely tuned with Optuna Library","fbf8807e":"* The dataset contains a similar number of samples for both target categories, Yes = 1 and No = 0 Heart attack\n* Most of the Female samples (0) are from a positive heart attack condition. It could be a dataset bias or females have more predisposal for this condition. For males there is a similar number of samples for both conditions\n* Regarding chest pain, while most samples are from \"Typical Angina\",  only a quarter of these are from positive heart attacks. Patients presenting Non-anginal pain (2) seem to have a higher predisposal to present a heart attacks\n* Patients with higher blood sugar show a similar percentage for both targets. Patients with blood sugar lower than 120 mg\/dl present a higher percentage of class 1 samples. However, 80% of all data are from patients with lower blood sugar\n* Regarding the ECG test, ST-T wave abnormality (1) represents 31.5% of samples of heart attack patients \n* The number of vessels coloured by fluoroscopy also seems to be a relevant parameter. When none of the vessels is coloured there is a big difference in the distribution between both targets. ","f32197db":"# 1. Overview\n\nThe diagnosis of heart disease is done on a combination of clinical signs and test results. The importance of a correct early diagnosis is primordial to provide care and avoid casualties. Examples of types of tests to understand if a patient is suffering or at risk of a heart attack are: electrocardiograms, cardiac computerized tomography (CT) scans, blood tests and exercise stress tests. Also, the literature cites risk factors for heart attacks as high cholesterol, high blood pressure, diabetes, weight, family history and smoking. Age, gender and genetic predisposition also play a role. \n\nThe Heart Attack UCI database originally contained 76 attributes and was created to detect the presence of heart disease in patients using ML algorithms. In the original database, the target features are integers valued from 0 (no presence) to 4. However, most experiments refer to using this dataset instead, containing only fourteen of the original attributes. Experiments with the UCI database have also concentrated on simply attempting to distinguish presence (values 1,2,3,4 as 1) from absence (value 0) of heart disease. In this notebook we follow the same approach, analysing the data as a binary classification problem.\n\n# 1.1 The Attributes\n\n* age - age in years\n* sex - (1 = male; 0 = female)\n* cp - chest pain type  (Value 1: typical angina, Value 2: atypical angina, Value 3: non-anginal pain, Value 4: asymptomatic)\n* trestbps - resting blood pressure (in mm Hg on admission to the hospital)\n* chol - serum cholestoral in mg\/dl\n* fbs - (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false)\n* restecg - Resting electrocardiographic measurement (0 = normal, 1 = having ST-T wave abnormality, 2 = showing probable or definite left ventricular hypertrophy by Estes' criteria)\n* thalach - maximum heart rate achieved\n* exang - exercise induced angina (1 = yes; 0 = no)\n* oldpeak - ST depression induced by exercise relative to rest ('ST' relates to positions on the ECG plot)\n* slope - the slope of the peak exercise ST segment (Value 1: upsloping, Value 2: flat, Value 3: downsloping)\n* ca - number of major vessels (0-3) colored by flourosopy\n* thal - a blood disorder called thalassemia\n* target - have disease or not (1=yes, 0=no)\n\n# 1.2 Acknowledgements\n\nAccording to the UCI Machine Learning Repository, the dataset creators are:\n\n1. Hungarian Institute of Cardiology. Budapest: Andras Janosi, M.D.\n2. University Hospital, Zurich, Switzerland: William Steinbrunn, M.D.\n3. University Hospital, Basel, Switzerland: Matthias Pfisterer, M.D.\n4. V.A. Medical Center, Long Beach and Cleveland Clinic Foundation: Robert Detrano, M.D., Ph.D.","fd615c20":"# Results and Conclusion\n\nThe Optuna Library has optimised the hyperparameters of the four models and chose the best one. Since the previous optimisation had a large search space and it does not take much time to run the optimisation again, here we run the same code with the intent to get any improvements focusing on a smaller set of parameters. The best results are selected according to the Cross-Validation F1 metric.\n\nFor this particular application, there is not an expressive need to use the F-Score in place of Accuracy as the classes are balanced. However, since it is a serious disease, it is good to use more than one metric to evaluate how the TP and TN are being handled. Preliminary experimentations have shown that the Accuracy and F1-Score are providing similar results (both metrics near 80%).","396ecf39":"* For male samples, there are clear peaks at age 44, 52 and 57-59 years. For females, the peaks are 54, 58, 62 and 63 year old\n* 62 and 63 age groups are the only ages where females had more heart attack than males. It is surprising due to the small number of female samples\n* While the distribution of age differ between both genders, most samples of positive heart attack are from patients from the age of 50 and beyond\n* The number of heart attack for males almost doubles from 40 to 41 years of age\n\n","af2f85cd":"# 2.1 Target Feature Analysis\n\nAfter the initial analysis, let's visualise how each feature relates to the target. \n\nIt is important to keep in mind that some features, such as gender or blood sugar, have a majority of samples from one class. As such, there will be an impression that specific situations seem more favourable to a heart attack simply because there is a high number of samples. \n\nWith that in mind, the bar plots below will serve to easily visualise the categories within the features that contain more samples from target 0 or 1, e.g. the discrepancy between the number of samples for positive and negative heart attacks of female subjects","050d4fe6":"Next, let's analyse any trends between gender, age and the possibility of heart attack. The plot below shows the age of all samples that belong to class 1 (Heart Attack), segmented by gender.","c991cf63":"# 3. Model\n\nThe dataset is rather small, it is necessary to be careful regarding Overfitting. From experience, it would not be advisable to use models that are prone to overfitting, such as ANN. For this reason, we will explore other Machine Learning Models.\n\n### Preliminary Results\n* Random Forest (RF), Logistic Regression (LR), SVM and KNN models were considered at first: \n    * KNN had the worse performance and was excluded from the optimisation step\n    * SVM was also excluded as it presented similar outcomes as LR and Random Forest. However, optimising its parameters takes more time and it is more prone to overfitting than RF.\n    * A smaller search space with parameters for Random Forest and LR is more effective than trying to optimise several models and parameters at once\n* Scaling the features reduces the model accuracy\n* Encoding the categorical variables to binary values did not improve the model performance\n* The new features did not improve model performance and were commented out of the code\n\n### Basic Approach:\n* Use Optuna to find the more promising model \n* Find the best model and use Optuna once more for final hyperparameter tuning\n* Five Cross-Validation and a separate test set are used\n* Analyse results using Confusion Matrix\n* Check if ML probability cutoff is appropriate"}}