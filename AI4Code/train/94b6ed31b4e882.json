{"cell_type":{"e5d033fe":"code","10c2596b":"code","c1a50e86":"code","b0d8d3f3":"code","cfbd2392":"code","6a4b2885":"code","38751059":"code","d10a39a4":"code","0215503d":"code","b9e7b53d":"code","fe993be2":"code","bf1869a8":"code","9970c04f":"code","77ab1cbd":"code","25ef1e57":"code","caed484f":"code","81744e93":"code","e841a022":"code","2e7a6db6":"code","1dee1e0f":"code","da764519":"code","6cec2d30":"code","f4b1a540":"markdown","d45df074":"markdown","1a7bd75e":"markdown","feba4e5c":"markdown","2b66ed6f":"markdown","968a7b22":"markdown","49c8c0a7":"markdown","2b48c0d4":"markdown","f23d7f61":"markdown","e3833335":"markdown","64d61fc7":"markdown","961a3c61":"markdown","2c3e3a4d":"markdown","c6fb506c":"markdown","ce97666b":"markdown"},"source":{"e5d033fe":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom matplotlib import pyplot as plt\nfrom sklearn import preprocessing\nfrom sklearn import ensemble\nfrom sklearn import model_selection\nfrom sklearn import naive_bayes\nimport seaborn as sns\nimport os\nfrom lightgbm import LGBMClassifier\n\nsns.set_theme()","10c2596b":"root = '\/kaggle\/input\/tabular-playground-series-feb-2022\/'","c1a50e86":"train_df = pd.read_csv(os.path.join(root, 'train.csv'))\ntest_df = pd.read_csv(os.path.join(root, 'test.csv'))\n\ntrain_df.head()","b0d8d3f3":"cols = train_df.columns.drop(['row_id', 'target'])","cfbd2392":"train_df[cols].hist(figsize=(20, 250), layout=(72, 4), log=True)\nplt.show()","6a4b2885":"r = train_df[cols].corr()\n\n(\n    r[r.abs() < 1]\n    .unstack()\n    .sort_values(ascending=False, key=np.abs)\n    .drop_duplicates()\n    .reset_index()\n    .rename(columns={\n        'level_0': 'Sample 1',\n        'level_1': 'Sample 2',\n        0: 'Correlation Coefficient'\n    })[:10]\n)","38751059":"train_df.shape[0]","d10a39a4":"train_df = train_df.drop(columns=['row_id']).drop_duplicates()\ntrain_df.shape[0]","0215503d":"anomaly_model = ensemble.IsolationForest(n_estimators=10)\nanomalies = anomaly_model.fit_predict(train_df[cols])\n\ntrain_df = train_df.loc[anomalies == 1]\ntrain_df.shape[0]","b9e7b53d":"A = np.zeros(len(cols))\nT = np.zeros(len(cols))\nG = np.zeros(len(cols))\nC = np.zeros(len(cols))\n\nfor i, x in enumerate(cols):\n    A[i] = int(x.split('A')[1].split('T')[0])\n    T[i] = int(x.split('T')[1].split('G')[0])\n    G[i] = int(x.split('G')[1].split('C')[0])\n    C[i] = int(x.split('C')[1])\n    \nA \/= 10\nT \/= 10\nG \/= 10\nC \/= 10\n    \ntrain_df['A'] = np.matmul(train_df[cols].to_numpy(), A[np.newaxis].T)\ntrain_df['T'] = np.matmul(train_df[cols].to_numpy(), T[np.newaxis].T)\ntrain_df['G'] = np.matmul(train_df[cols].to_numpy(), G[np.newaxis].T)\ntrain_df['C'] = np.matmul(train_df[cols].to_numpy(), C[np.newaxis].T)\n\ntrain_df[['A', 'T', 'G', 'C', 'target']]","fe993be2":"train_df['sum'] = train_df[cols].sum(axis=1)\ntrain_df['mean'] = train_df[cols].mean(axis=1)\ntrain_df['std'] = train_df[cols].std(axis=1)\ntrain_df['min'] = train_df[cols].min(axis=1)\ntrain_df['max'] = train_df[cols].max(axis=1)\n\ntrain_df[['sum', 'mean', 'std', 'min', 'max', 'target']]","bf1869a8":"X = train_df.drop(columns=['target'])\ny = train_df['target']","9970c04f":"kfold = model_selection.StratifiedKFold(n_splits=10, shuffle=True)\nestimators = 300","77ab1cbd":"rf = ensemble.RandomForestClassifier(max_depth=12, max_leaf_nodes=63, n_estimators=estimators, n_jobs=-1)\nrf_performance = model_selection.cross_val_score(rf, X, y=y, cv=kfold, n_jobs=-1)","25ef1e57":"et = ensemble.ExtraTreesClassifier(max_depth=12, max_leaf_nodes=63, n_estimators=estimators, n_jobs=-1)\net_performance = model_selection.cross_val_score(et, X, y=y, cv=kfold, n_jobs=-1)","caed484f":"params = {\n    'n_estimators': estimators,\n    'baggin_freq': 10,\n    'bagging_fraction':0.8,\n    'max_depth': 12,\n    'num_leaves': 63,\n    'learning_rate': 0.04,\n    'n_jobs': -1\n}\n\nlgbm = LGBMClassifier(**params)\nlgbm_performance = model_selection.cross_val_score(lgbm, X, y=y, cv=kfold, n_jobs=-1)","81744e93":"plt.boxplot(\n    [rf_performance, et_performance, lgbm_performance], \n    labels=['Random Forest', 'Extra Trees', 'LightGBM'])\nplt.title('Model Performance')\nplt.ylabel('Multi-Class Accuracy')\nplt.tight_layout()\nplt.show()","e841a022":"model = lgbm.fit(X, y=y)","2e7a6db6":"for i, x in enumerate(cols):\n    A[i] = int(x.split('A')[1].split('T')[0])\n    T[i] = int(x.split('T')[1].split('G')[0])\n    G[i] = int(x.split('G')[1].split('C')[0])\n    C[i] = int(x.split('C')[1])\n    \nA \/= 10\nT \/= 10\nG \/= 10\nC \/= 10\n    \ntest_df['A'] = np.matmul(test_df[cols].to_numpy(), A[np.newaxis].T)\ntest_df['T'] = np.matmul(test_df[cols].to_numpy(), T[np.newaxis].T)\ntest_df['G'] = np.matmul(test_df[cols].to_numpy(), G[np.newaxis].T)\ntest_df['C'] = np.matmul(test_df[cols].to_numpy(), C[np.newaxis].T)\n\ntest_df['sum'] = test_df[cols].sum(axis=1)\ntest_df['mean'] = test_df[cols].mean(axis=1)\ntest_df['std'] = test_df[cols].std(axis=1)\ntest_df['min'] = test_df[cols].min(axis=1)\ntest_df['max'] = test_df[cols].max(axis=1)","1dee1e0f":"x = test_df.drop(columns=['row_id'])\ninference = model.predict(x)","da764519":"submission = pd.DataFrame({\n    'row_id': test_df['row_id'],\n    'target': inference\n})\n\nsubmission.to_csv('submission.csv', index=False)","6cec2d30":"submission.head()","f4b1a540":"### LightGBM","d45df074":"## Inference","1a7bd75e":"## Data Input","feba4e5c":"### Aggregate attributes","2b66ed6f":"## Model Evaluation","968a7b22":"## Cleaning Data\n\n### Removing duplicate rows\n\nThere are several duplicated rows in the training set. To remove bias for these rows, they are removed","49c8c0a7":"## Training","2b48c0d4":"### Most Correlated Columns","f23d7f61":"### Visualization","e3833335":"### Extra Trees","64d61fc7":"### Random Forest","961a3c61":"### Remove Anomalies","2c3e3a4d":"## Feature Engineering\n\n### Aggregate individual ATGC values","c6fb506c":"based on our experiment, it looks like LightGBM is the strongest classifier for this application.","ce97666b":"## Visualization\n\n### Histogram"}}