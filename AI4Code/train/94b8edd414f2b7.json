{"cell_type":{"4f5678e6":"code","95c18058":"code","02da8e4f":"code","9d3f4afd":"code","3383b62e":"code","5c63a34f":"code","20eec6b3":"code","27424a7e":"code","f51b2ed2":"code","e2400f99":"code","26ba71d6":"code","2b5565ff":"code","20f67ad6":"code","698d386e":"code","59456f40":"code","fe0807c3":"code","82f24cb6":"code","8e90dd3e":"code","8ee0525a":"code","ad82200e":"code","93af4821":"code","902bb54d":"code","b7921bac":"code","b5de1da1":"code","f78ef8a0":"code","140b504e":"code","ec3b8564":"code","30930fc8":"code","56ba56c2":"code","08c2f553":"code","dcb5285a":"code","3e210816":"code","3f16c95e":"code","ec3c5032":"code","f0d16406":"code","ef57709e":"code","9035e066":"code","359e7465":"code","06e310cb":"code","b5929e4d":"code","fb4e02f3":"code","039e309e":"code","492c8b46":"code","91a3fa44":"code","b99e868c":"code","630adb32":"code","3c10081f":"code","c31393b4":"code","db900d50":"code","709be6e9":"code","17b59d41":"code","80d758a4":"code","a720af45":"code","6c902486":"code","63c9e314":"code","672fb15d":"code","937f5b14":"code","d7da7a26":"code","5f5e7943":"markdown","83bd6155":"markdown","8db2ebe8":"markdown","fbf8d981":"markdown","b97ff90e":"markdown","075e4be5":"markdown","303ca446":"markdown","182a08c9":"markdown","94ea96bb":"markdown","fe927008":"markdown","61ae91e6":"markdown","5f71047c":"markdown","d30126b3":"markdown","89e72dfa":"markdown","3c6e57db":"markdown","8b0b065f":"markdown","ff5c1c04":"markdown","066bb006":"markdown","5d2cd8c2":"markdown","4a7432bd":"markdown","935e8abc":"markdown","e6c545f2":"markdown","859973f5":"markdown","21819dd8":"markdown","ea0ca4bf":"markdown","614f0c2e":"markdown","bb531292":"markdown","5de83fde":"markdown","ab26f24f":"markdown","1875a869":"markdown","defb4a2a":"markdown","965a10d9":"markdown","4abc3b3b":"markdown","7a5c2529":"markdown","8a362abf":"markdown","4bc21767":"markdown","fb23f415":"markdown"},"source":{"4f5678e6":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nfrom sklearn.model_selection import cross_val_score\n\n# To remove the scientific notation from numpy arrays\nnp.set_printoptions(suppress=True)","95c18058":"df= pd.read_csv(\"..\/input\/human-activity-recognition-with-smartphones\/test.csv\")","02da8e4f":"df.head()","9d3f4afd":"df.shape","3383b62e":"df.info()","5c63a34f":"df= df.drop_duplicates()\ndf.shape","20eec6b3":"df.isnull().sum()[df.isnull().sum()>0]","27424a7e":"X= df.drop(columns=['Activity'])\nX=X.values\nfrom sklearn.decomposition import PCA\n\npca = PCA(n_components=3)\n\n# fitting the data\npca_fit=pca.fit(X)\n\n# calculating the principal components\nreduced_X = pca_fit.transform(X)\n#561 Columns present in X are now represented by 3-Principal components present in reduced_X","f51b2ed2":"df2= pd.DataFrame(reduced_X, columns=['PC1','PC2','PC3'])\ndf2['activity']=df['Activity']\ndf2.head()","e2400f99":"df2.hist(['PC1','PC2','PC3'],figsize=(20,5))","26ba71d6":"def bar_graph(data,predictor):\n    grouped=data.groupby(predictor)\n    chart=grouped.size().plot.bar(rot=0, title='Bar Chart showing the total frequency of different '+str(predictor), figsize=(15,4))\n    chart.set_xlabel(predictor)","2b5565ff":"bar_graph(df2,'activity')","20f67ad6":"df2.activity.value_counts()","698d386e":"df2.boxplot(column=['PC1'], by='activity', figsize=(15,10),grid=False, layout=(2,1))","59456f40":"df2.boxplot(column=['PC2'], by='activity', figsize=(15,5),grid=False)","fe0807c3":"df2.boxplot(column=['PC3'], by='activity', figsize=(15,5),grid=False)","82f24cb6":"def anova_test(data,target,predictor):\n    data1=data.groupby(target)[predictor].apply(list)\n    from scipy.stats import f_oneway\n    AnovaResults = f_oneway(*data1)\n    if AnovaResults[1]<0.05:\n        print(str(predictor)+' is related with the target variable : ', AnovaResults[1])\n    else:\n        print(str(predictor)+' is NOT related with the target variable : ', AnovaResults[1])","8e90dd3e":"anova_test(df2,'activity','PC1')","8ee0525a":"anova_test(df2,'activity','PC2')","ad82200e":"anova_test(df2,'activity','PC3')","93af4821":"df2.activity.unique()","902bb54d":"activity_mapping = {'STANDING': 1,\n                'SITTING': 2,\n                'LAYING': 3,\n              'WALKING': 4,\n               'WALKING_DOWNSTAIRS': 5,\n               'WALKING_UPSTAIRS':6\n              }\n# encoding the Ordinal variable cut\ndf['Activity'] = df['Activity'].map(activity_mapping)\n\n# Checking the encoded columns\ndf['Activity'].unique()","b7921bac":"df.head()","b5de1da1":"TargetVariable='Activity'\ndf2=df.drop(columns=['Activity','subject'])\npredictor = df2.columns\nx=df[predictor].values\ny =df[TargetVariable].values\n\nfrom sklearn.preprocessing import StandardScaler\nscaler=StandardScaler()\nx=scaler.fit_transform(x)","f78ef8a0":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.30, random_state=42)\nprint(x_train.shape, x_test.shape, y_train.shape, y_test.shape)","140b504e":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC","ec3b8564":"clf = LogisticRegression(C=1)\n\n# Creating the model on Training Data\nLOG=clf.fit(x_train,y_train)\nprediction=LOG.predict(x_test)\n\n# Measuring accuracy on Testing Data\nfrom sklearn import metrics\nprint(metrics.classification_report(y_test, prediction))\nprint(metrics.confusion_matrix(y_test, prediction))\n\n# Printing the Overall Accuracy of the model\nF1_Score=metrics.classification_report(y_test, prediction).split()[-2]\nprint('Accuracy of the model:', F1_Score)","30930fc8":"clf = KNeighborsClassifier(n_neighbors=3)\n\n# Creating the model on Training Data\nKNN=clf.fit(x_train,y_train)\nprediction=KNN.predict(x_test)\n\n# Measuring accuracy on Testing Data\nfrom sklearn import metrics\nprint(metrics.classification_report(y_test, prediction))\nprint(metrics.confusion_matrix(y_test, prediction))\n\n# Printing the Overall Accuracy of the model\nF1_Score=metrics.classification_report(y_test, prediction).split()[-2]\nprint('Accuracy of the model:', F1_Score)\n","56ba56c2":"clf = DecisionTreeClassifier(max_depth=3,criterion='entropy')\n\n# Creating the model on Training Data\nDTree=clf.fit(x_train,y_train)\nprediction=DTree.predict(x_test)\n\n# Measuring accuracy on Testing Data\nfrom sklearn import metrics\nprint(metrics.classification_report(y_test, prediction))\nprint(metrics.confusion_matrix(y_test, prediction))\n\n# Printing the Overall Accuracy of the model\nF1_Score=metrics.classification_report(y_test, prediction).split()[-2]\nprint('Accuracy of the model:', F1_Score)\n\n# Plotting the feature importance for Top 10 most important columns\n%matplotlib inline\nfeature_importances = pd.Series(DTree.feature_importances_, index=predictor)\nfeature_importances.nlargest(10).plot(kind='barh')","08c2f553":"clf = RandomForestClassifier(max_depth=4, n_estimators=600,criterion='entropy')\n\n# Creating the model on Training Data\nRF=clf.fit(x_train,y_train)\nprediction=RF.predict(x_test)\n\n# Measuring accuracy on Testing Data\nfrom sklearn import metrics\nprint(metrics.classification_report(y_test, prediction))\nprint(metrics.confusion_matrix(y_test, prediction))\n\n# Printing the Overall Accuracy of the model\nF1_Score=metrics.classification_report(y_test, prediction).split()[-2]\nprint('Accuracy of the model:', F1_Score)\n\n# Plotting the feature importance for Top 10 most important columns\n%matplotlib inline\nfeature_importances = pd.Series(RF.feature_importances_, index=predictor)\nfeature_importances.nlargest(10).plot(kind='barh')","dcb5285a":"clf = SVC(C=100, gamma=0.001, kernel='rbf')\n\n# Creating the model on Training Data\nSVM=clf.fit(x_train,y_train)\nprediction=SVM.predict(x_test)\n\n# Measuring accuracy on Testing Data\nfrom sklearn import metrics\nprint(metrics.classification_report(y_test, prediction))\nprint(metrics.confusion_matrix(y_test, prediction))\n\n# Printing the Overall Accuracy of the model\nF1_Score=metrics.classification_report(y_test, prediction).split()[-2]\nprint('Accuracy of the model:', F1_Score)","3e210816":"from imblearn.over_sampling import SMOTE\nsmk=SMOTE(random_state=42)\nx_smote,y_smote=smk.fit_sample(x_train,y_train)\nprint('Resampled dataset shape %s' % Counter(y_smote))","3f16c95e":"from imblearn.over_sampling import RandomOverSampler\nros= RandomOverSampler(random_state=42)\nx_over,y_over= ros.fit_resample(x_train,y_train)\nprint('Resampled dataset shape %s' % Counter(y_over))","ec3c5032":"from imblearn.under_sampling import RandomUnderSampler\nrus= RandomUnderSampler(random_state=42)\nx_under,y_under= rus.fit_resample(x_train,y_train)\nprint('Resampled dataset shape %s' % Counter(y_under))","f0d16406":"clf = LogisticRegression(C=1)\n\n# Creating the model on Training Data\nLOG=clf.fit(x_smote,y_smote)\nprediction=LOG.predict(x_test)\n\n# Measuring accuracy on Testing Data\nfrom sklearn import metrics\nprint(metrics.classification_report(y_test, prediction))\nprint(metrics.confusion_matrix(y_test, prediction))\n\n# Printing the Overall Accuracy of the model\nF1_Score=metrics.classification_report(y_test, prediction).split()[-2]\nprint('Accuracy of the model:', F1_Score)","ef57709e":"clf = LogisticRegression(C=1)\n\n# Creating the model on Training Data\nLOG=clf.fit(x_over,y_over)\nprediction=LOG.predict(x_test)\n\n# Measuring accuracy on Testing Data\nfrom sklearn import metrics\nprint(metrics.classification_report(y_test, prediction))\nprint(metrics.confusion_matrix(y_test, prediction))\n\n# Printing the Overall Accuracy of the model\nF1_Score=metrics.classification_report(y_test, prediction).split()[-2]\nprint('Accuracy of the model:', F1_Score)","9035e066":"clf = LogisticRegression(C=1)\n\n# Creating the model on Training Data\nLOG=clf.fit(x_under,y_under)\nprediction=LOG.predict(x_test)\n\n# Measuring accuracy on Testing Data\nfrom sklearn import metrics\nprint(metrics.classification_report(y_test, prediction))\nprint(metrics.confusion_matrix(y_test, prediction))\n\n# Printing the Overall Accuracy of the model\nF1_Score=metrics.classification_report(y_test, prediction).split()[-2]\nprint('Accuracy of the model:', F1_Score)","359e7465":"clf = KNeighborsClassifier(n_neighbors=3)\n\n# Creating the model on Training Data\nKNN=clf.fit(x_smote,y_smote)\nprediction=KNN.predict(x_test)\n\n# Measuring accuracy on Testing Data\nfrom sklearn import metrics\nprint(metrics.classification_report(y_test, prediction))\nprint(metrics.confusion_matrix(y_test, prediction))\n\n# Printing the Overall Accuracy of the model\nF1_Score=metrics.classification_report(y_test, prediction).split()[-2]\nprint('Accuracy of the model:', F1_Score)","06e310cb":"clf = KNeighborsClassifier(n_neighbors=3)\n\n# Creating the model on Training Data\nKNN=clf.fit(x_over,y_over)\nprediction=KNN.predict(x_test)\n\n# Measuring accuracy on Testing Data\nfrom sklearn import metrics\nprint(metrics.classification_report(y_test, prediction))\nprint(metrics.confusion_matrix(y_test, prediction))\n\n# Printing the Overall Accuracy of the model\nF1_Score=metrics.classification_report(y_test, prediction).split()[-2]\nprint('Accuracy of the model:', F1_Score)\n","b5929e4d":"clf = KNeighborsClassifier(n_neighbors=3)\n\n# Creating the model on Training Data\nKNN=clf.fit(x_under,y_under)\nprediction=KNN.predict(x_test)\n\n# Measuring accuracy on Testing Data\nfrom sklearn import metrics\nprint(metrics.classification_report(y_test, prediction))\nprint(metrics.confusion_matrix(y_test, prediction))\n\n# Printing the Overall Accuracy of the model\nF1_Score=metrics.classification_report(y_test, prediction).split()[-2]\nprint('Accuracy of the model:', F1_Score)\n","fb4e02f3":"clf = DecisionTreeClassifier(max_depth=3,criterion='entropy')\n\n# Creating the model on Training Data\nDTree=clf.fit(x_smote,y_smote)\nprediction=DTree.predict(x_test)\n\n# Measuring accuracy on Testing Data\nfrom sklearn import metrics\nprint(metrics.classification_report(y_test, prediction))\nprint(metrics.confusion_matrix(y_test, prediction))\n\n# Printing the Overall Accuracy of the model\nF1_Score=metrics.classification_report(y_test, prediction).split()[-2]\nprint('Accuracy of the model:', F1_Score)\n\n# Plotting the feature importance for Top 10 most important columns\n%matplotlib inline\nfeature_importances = pd.Series(DTree.feature_importances_, index=predictor)\nfeature_importances.nlargest(10).plot(kind='barh')","039e309e":"clf = DecisionTreeClassifier(max_depth=3,criterion='entropy')\n\n# Creating the model on Training Data\nDTree=clf.fit(x_over,y_over)\nprediction=DTree.predict(x_test)\n\n# Measuring accuracy on Testing Data\nfrom sklearn import metrics\nprint(metrics.classification_report(y_test, prediction))\nprint(metrics.confusion_matrix(y_test, prediction))\n\n# Printing the Overall Accuracy of the model\nF1_Score=metrics.classification_report(y_test, prediction).split()[-2]\nprint('Accuracy of the model:', F1_Score)\n\n# Plotting the feature importance for Top 10 most important columns\n%matplotlib inline\nfeature_importances = pd.Series(DTree.feature_importances_, index=predictor)\nfeature_importances.nlargest(10).plot(kind='barh')","492c8b46":"clf = DecisionTreeClassifier(max_depth=3,criterion='entropy')\n\n# Creating the model on Training Data\nDTree=clf.fit(x_under,y_under)\nprediction=DTree.predict(x_test)\n\n# Measuring accuracy on Testing Data\nfrom sklearn import metrics\nprint(metrics.classification_report(y_test, prediction))\nprint(metrics.confusion_matrix(y_test, prediction))\n\n# Printing the Overall Accuracy of the model\nF1_Score=metrics.classification_report(y_test, prediction).split()[-2]\nprint('Accuracy of the model:', F1_Score)\n\n# Plotting the feature importance for Top 10 most important columns\n%matplotlib inline\nfeature_importances = pd.Series(DTree.feature_importances_, index=predictor)\nfeature_importances.nlargest(10).plot(kind='barh')","91a3fa44":"clf = RandomForestClassifier(max_depth=4, n_estimators=600,criterion='entropy')\n\n# Creating the model on Training Data\nRF=clf.fit(x_smote,y_smote)\nprediction=RF.predict(x_test)\n\n# Measuring accuracy on Testing Data\nfrom sklearn import metrics\nprint(metrics.classification_report(y_test, prediction))\nprint(metrics.confusion_matrix(y_test, prediction))\n\n# Printing the Overall Accuracy of the model\nF1_Score=metrics.classification_report(y_test, prediction).split()[-2]\nprint('Accuracy of the model:', F1_Score)\n\n# Plotting the feature importance for Top 10 most important columns\n%matplotlib inline\nfeature_importances = pd.Series(RF.feature_importances_, index=predictor)\nfeature_importances.nlargest(10).plot(kind='barh')","b99e868c":"clf = RandomForestClassifier(max_depth=4, n_estimators=600,criterion='entropy')\n\n# Creating the model on Training Data\nRF=clf.fit(x_over,y_over)\nprediction=RF.predict(x_test)\n\n# Measuring accuracy on Testing Data\nfrom sklearn import metrics\nprint(metrics.classification_report(y_test, prediction))\nprint(metrics.confusion_matrix(y_test, prediction))\n\n# Printing the Overall Accuracy of the model\nF1_Score=metrics.classification_report(y_test, prediction).split()[-2]\nprint('Accuracy of the model:', F1_Score)\n\n# Plotting the feature importance for Top 10 most important columns\n%matplotlib inline\nfeature_importances = pd.Series(RF.feature_importances_, index=predictor)\nfeature_importances.nlargest(10).plot(kind='barh')","630adb32":"clf = RandomForestClassifier(max_depth=4, n_estimators=600,criterion='entropy')\n\n# Creating the model on Training Data\nRF=clf.fit(x_under,y_under)\nprediction=RF.predict(x_test)\n\n# Measuring accuracy on Testing Data\nfrom sklearn import metrics\nprint(metrics.classification_report(y_test, prediction))\nprint(metrics.confusion_matrix(y_test, prediction))\n\n# Printing the Overall Accuracy of the model\nF1_Score=metrics.classification_report(y_test, prediction).split()[-2]\nprint('Accuracy of the model:', F1_Score)\n\n# Plotting the feature importance for Top 10 most important columns\n%matplotlib inline\nfeature_importances = pd.Series(RF.feature_importances_, index=predictor)\nfeature_importances.nlargest(10).plot(kind='barh')","3c10081f":"clf = SVC(C=100, gamma=0.001, kernel='rbf')\n\n# Creating the model on Training Data\nSVM_smote=clf.fit(x_smote,y_smote)\nprediction=SVM_smote.predict(x_test)\n\n# Measuring accuracy on Testing Datam\nfrom sklearn import metrics\nprint(metrics.classification_report(y_test, prediction))\nprint(metrics.confusion_matrix(y_test, prediction))\n\n# Printing the Overall Accuracy of the model\nF1_Score=metrics.classification_report(y_test, prediction).split()[-2]\nprint('Accuracy of the model:', F1_Score)","c31393b4":"clf = SVC(C=100, gamma=0.001, kernel='rbf')\n\n# Creating the model on Training Data\nSVM_over=clf.fit(x_over,y_over)\nprediction=SVM_over.predict(x_test)\n\n# Measuring accuracy on Testing Data\nfrom sklearn import metrics\nprint(metrics.classification_report(y_test, prediction))\nprint(metrics.confusion_matrix(y_test, prediction))\n\n# Printing the Overall Accuracy of the model\nF1_Score=metrics.classification_report(y_test, prediction).split()[-2]\nprint('Accuracy of the model:', F1_Score)","db900d50":"clf = SVC(C=100, gamma=0.001, kernel='rbf')\n\n# Creating the model on Training Data\nSVM_under =clf.fit(x_under,y_under)\nprediction=SVM_under.predict(x_test)\n\n# Measuring accuracy on Testing Data\nfrom sklearn import metrics\nprint(metrics.classification_report(y_test, prediction))\nprint(metrics.confusion_matrix(y_test, prediction))\n\n# Printing the Overall Accuracy of the model\nF1_Score=metrics.classification_report(y_test, prediction).split()[-2]\nprint('Accuracy of the model:', F1_Score)","709be6e9":"accuracy_values= cross_val_score(SVM_under, x, y, cv=10, scoring='f1_weighted')\nprint(accuracy_values)\nprint('Final Average Accuracy of the Model:',accuracy_values.mean())","17b59d41":"final_svm= SVM.fit(x,y)","80d758a4":"test= pd.read_csv('..\/input\/human-activity-recognition-with-smartphones\/test.csv')","a720af45":"test.drop(columns=['subject'],inplace=True)\ntest=test.drop_duplicates()","6c902486":"df.isnull().sum()[df.isnull().sum()>0]","63c9e314":"activity_mapping = {'STANDING': 1,\n                'SITTING': 2,\n                'LAYING': 3,\n              'WALKING': 4,\n               'WALKING_DOWNSTAIRS': 5,\n               'WALKING_UPSTAIRS':6\n              }\n# encoding the Ordinal variable cut\ntest['Activity'] = test['Activity'].map(activity_mapping)\n\n# Checking the encoded columns\ntest['Activity'].unique()","672fb15d":"test.head()","937f5b14":"TargetVariable='Activity'\ntest2= test.drop(columns=['Activity'])\npredictor = test2.columns\nx_test= test[predictor].values\ny_test = test[TargetVariable].values\n\nfrom sklearn.preprocessing import StandardScaler\nscaler=StandardScaler()\nx=scaler.fit_transform(x_test)\n\nprediction= final_svm.predict(x)\ntest['Activity_Predictions']=prediction\ntest.head()","d7da7a26":"from sklearn import metrics\nprint(metrics.classification_report(y_test, prediction))\nprint(metrics.confusion_matrix(y_test, prediction))\n\n# Printing the Overall Accuracy of the model\nF1_Score=metrics.classification_report(y_test, prediction).split()[-2]\nprint('Accuracy of the model:', F1_Score)","5f5e7943":"_K NEAREST CLASSIFIERS AFTER SAMPLING_","83bd6155":"### APPLYING DIFFERENT ALGORITHMS","8db2ebe8":"### SAMPLING TECHNIQUES: SMOTE, OVERSAMPLING, UNDERSAMPLING","fbf8d981":"__THE BEST MODEL IS SUPPORT VECTOR CLASSIFIER WITHOUT ANY SAMPLING TECHNIQUE.__\n\n_Accuracy: 99%_\n\n_Error proportion: 0.013_","b97ff90e":"_SUPPORT VECTOR CLASSIFIER AFTER SAMPLING_","075e4be5":"#### ACCURACY OF TRAIN SET : 99%\n\n#### ACCURACY OF TEST SET : 100%\n\n_MODEL: SUPPORT VECTOR CLASSIFIER AFTER UNDERSAMPLING (Because it has the highest accuracy and lowest error percentage)_\n\n_STANDARDIZED the TRAIN SET_\n","303ca446":"__The distribution of the classes is fairly balanced.__\n_____________________________________________________________________________________________________________________________","182a08c9":"### MODEL","94ea96bb":"### EXPLORATORY DATA ANALYSIS","fe927008":"__No duplicate rows were found.__","61ae91e6":"_RANDOM FOREST CLASSIFIER_","5f71047c":"_Since PC1, PC2, PC3 is continuous in nature, we will use histogram to visualize it._\n\n_For Activity, we will use bar chart because it is categorical in nature._","d30126b3":"### SPLITTING THE DATASET INTO TRAINING AND TESTING","89e72dfa":"_LOGISTIC REGRESSION_","3c6e57db":"### DEPLOYMENT OF THE MODEL","8b0b065f":"__USING PCA WE SAW THAT THE PREDICTORS ARE RELATED TO THE TARGET VARIABLE. HOWEVER WE WILL NOT USE THE PCA COLUMNS FOR MODELLING PURPOSE BECAUSE IT CAN REDUCE THE ACCURACY.__","ff5c1c04":"__The data has no missing values in the form of NaN.__","066bb006":"_SUPPORT VECTOR MACHINE_","5d2cd8c2":"GOT THE DATASET FROM KAGGLE: https:\/\/www.kaggle.com\/uciml\/human-activity-recognition-with-smartphones\n\n\"The experiments have been carried out with a group of 30 volunteers within an age bracket of 19-48 years. Each person performed six activities (WALKING, WALKINGUPSTAIRS, WALKINGDOWNSTAIRS, SITTING, STANDING, LAYING).\"\n\n__IMPLEMENTED SUPERVISED MACHINE LEARNING ALGORITHMS FOR CLASSIFICATION__","4a7432bd":"_LOGISTIC REGRESSION AFTER SAMPLING_","935e8abc":"### READING THE DATA","e6c545f2":"_RANDOM FOREST CLASSIFIER AFTER SAMPLING_","859973f5":"_None of them has extreme skewness and represent a fair distribution._","21819dd8":"### CONTEXT\n","ea0ca4bf":"__We used ANOVA test to check whether the predictors are correlated with the target variable.__","614f0c2e":"_K-NEAREST CLASSIFIER_","bb531292":"### K-FOLD CROSS VALIDATION","5de83fde":"### VIZUALIZING THE DISTRIBUTION OF THE COLUMNS","ab26f24f":"### STATISTICAL TEST FOR CORRELATION","1875a869":"**Since there are 561 predictors, we are using PCA to reduce the number of predictors which will help us in visualization.**","defb4a2a":"__The mean value of different activities is varying for all the 3 boxplots. This implies that the predictors are correlated with the target variable.__","965a10d9":"### IMPORTING REQUIRED LIBRARIES","4abc3b3b":"_DECISION TREE AFTER SAMPLING_","7a5c2529":"### VIZUALIZING THE RELATIONSHIP BETWEEN THE PREDICTORS AND THE TARGET VARIABLE ","8a362abf":"_DECISION TREE CLASSIFIER_","4bc21767":"### TREATING THE CATEGORICAL VARIABLE","fb23f415":"_Using boxplot to see the relationship between categorical target variable and continuous predictors._"}}