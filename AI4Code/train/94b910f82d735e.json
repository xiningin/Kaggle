{"cell_type":{"7c03dd70":"code","619b3666":"code","dfdeb6bb":"code","ac0303cf":"code","d2fa8807":"code","ae119629":"code","ff688fca":"code","deb61ff1":"code","8cf1efd4":"code","1bd45cb1":"code","0682f10b":"code","b09b73db":"code","4ec53da3":"code","94ce096a":"code","6567f42c":"code","21e61b0f":"code","595932c7":"code","f77c6430":"code","b518ee47":"code","e21455ad":"code","97f637c8":"code","fc3f1d4c":"code","c03ae938":"code","97b3708a":"code","24210518":"code","e42f97c4":"code","892d8b7e":"code","b40ea64b":"code","6ab06e11":"code","291bfda8":"code","5d925c18":"code","6d966445":"code","94ae6442":"code","53bc1e99":"code","2f16a587":"code","b4aa3bdb":"code","98b17109":"code","9c363c00":"code","da1a40b4":"code","8d595ae4":"markdown","8298b07f":"markdown","a27406a1":"markdown","abac0601":"markdown","baf539df":"markdown","cd3d346c":"markdown","730e4f13":"markdown","b962acf8":"markdown","56d71b2e":"markdown","a643fb55":"markdown","5bac0a5a":"markdown","96c9ea03":"markdown","6fdbb58f":"markdown","9ebf3fd9":"markdown","97627d66":"markdown","cfdf9ebb":"markdown","5d4cb29f":"markdown","eb5ce830":"markdown","328f71d9":"markdown","86e30af8":"markdown","342d8675":"markdown","b67a53ae":"markdown","2eccc84d":"markdown","d56454bf":"markdown","a4617ab4":"markdown","f27c471a":"markdown","a285f88c":"markdown","e1b6ff2e":"markdown","8ea31744":"markdown","aaa207ad":"markdown","fc440994":"markdown","640a595d":"markdown","c41a3a03":"markdown","7bd0d91a":"markdown","0d3a0427":"markdown"},"source":{"7c03dd70":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport datatable as dt\nimport scipy.stats\nfrom sklearn.metrics import mean_absolute_error\n\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nimport xgboost as xgb\n\nfrom tqdm import tqdm\nimport os","619b3666":"import janestreet\nenv = janestreet.make_env() # initialize the environment\niter_test = env.iter_test() # an iterator which loops over the test set","dfdeb6bb":"train = dt.fread('..\/input\/jane-street-market-prediction\/train.csv').to_pandas()\n","ac0303cf":"train = train[train['weight'] != 0]","d2fa8807":"print(train.shape)\nmissing_val_count_by_column = (train.isnull().sum())\nprint(missing_val_count_by_column[missing_val_count_by_column > 0])","ae119629":"train.fillna(train.mean(),inplace=True)","ff688fca":"train['action'] = (train['resp'] > 0).astype('int')\nfeatures = [c for c in train.columns if 'feature' in c]","deb61ff1":"#X = train[features[1:]]  # all except for feature_0 that indicates buying\/selling\nX = train[features]\ny = train.loc[:, 'action']","8cf1efd4":"resp_cols = ['resp_1', 'resp_2', 'resp_3', 'resp_4']\n\nweights = train['weight'].values\nresp = train['resp'].values\nresps_rest = train[resp_cols].values\ndates = train['date'].values\n\nf0 = train['feature_0'].values\n\nweights_resp = weights * abs(resp)\nweights_resp_sq = weights * (abs(resp) ** 2)","1bd45cb1":"plt.figure(figsize=(15, 7))\nplt.plot(resp)\nplt.title('Returns [resp]')\nplt.grid(True)\nplt.show()","0682f10b":"plt.figure(figsize=(15, 7))\nplt.plot(resp[0:500])\nplt.title('Returns [resp]')\nplt.grid(True)\nplt.show()","b09b73db":"plt.figure(figsize=(15, 7))\nplt.plot(resps_rest[0:500, 0])\nplt.plot(resps_rest[0:500, 1])\nplt.plot(resps_rest[0:500, 2])\nplt.plot(resps_rest[0:500, 3])\nplt.legend(['resp_1','resp_2','resp_3','resp_4'])\nplt.title('Returns [resp_1, resp_2, resp_3, resp_4]')\nplt.grid(True)\nplt.show()","4ec53da3":"plt.figure(figsize=(15, 7))\nplt.plot(resp[0:300])\nplt.plot(resps_rest[0:300, 0])\nplt.plot(resps_rest[0:300, 3])\nplt.legend(['resp','resp_1','resp_4'])\nplt.title('Returns [resp, resp_1, resp_4]')\nplt.grid(True)\nplt.show()","94ce096a":"plt.figure(figsize=(15, 7))\nplt.hist(resp, 400)\nplt.title('Returns [resp]')\nplt.grid(True)\nplt.show()","6567f42c":"print(scipy.stats.kurtosis(resp))","21e61b0f":"resp_sq = resp ** 2;\n\nfig, (ax1, ax2) = plt.subplots(1, 2,figsize=(14,5), dpi= 80)\nsm.graphics.tsa.plot_acf(resp[0:100000], ax=ax1, lags=10, alpha=0.01)\nsm.graphics.tsa.plot_acf(resp_sq[0:100000], ax=ax2, lags=10, alpha=0.01)\n\n\n# Decorate\n# lighten the borders\nax1.spines[\"top\"].set_alpha(.3); ax2.spines[\"top\"].set_alpha(.3)\nax1.spines[\"bottom\"].set_alpha(.3); ax2.spines[\"bottom\"].set_alpha(.3)\nax1.spines[\"right\"].set_alpha(.3); ax2.spines[\"right\"].set_alpha(.3)\nax1.spines[\"left\"].set_alpha(.3); ax2.spines[\"left\"].set_alpha(.3)\n\n# font size of tick labels\nax1.tick_params(axis='both', labelsize=12)\nax2.tick_params(axis='both', labelsize=12)\nplt.show()\n\nfig, (ax1, ax2) = plt.subplots(1, 2,figsize=(14,5), dpi= 80)\nsm.graphics.tsa.plot_pacf(resp[0:100000], ax=ax1, lags=10, alpha=0.01)\nsm.graphics.tsa.plot_pacf(resp_sq[0:100000], ax=ax2, lags=10, alpha=0.01)\n\n\n# Decorate\n# lighten the borders\nax1.spines[\"top\"].set_alpha(.3); ax2.spines[\"top\"].set_alpha(.3)\nax1.spines[\"bottom\"].set_alpha(.3); ax2.spines[\"bottom\"].set_alpha(.3)\nax1.spines[\"right\"].set_alpha(.3); ax2.spines[\"right\"].set_alpha(.3)\nax1.spines[\"left\"].set_alpha(.3); ax2.spines[\"left\"].set_alpha(.3)\n\n# font size of tick labels\nax1.tick_params(axis='both', labelsize=12)\nax2.tick_params(axis='both', labelsize=12)\nplt.show()","595932c7":"print(1981287*0.7)  # obs 0-1386900 -> training, 1386901-1981287 -> validation\nt_split = 1386900;","f77c6430":"print([round(np.mean(y[0:t_split].values), 3), round(np.var(y[0:t_split].values), 3)]);\nprint([round(np.mean(y[t_split:1981287].values), 3), round(np.var(y[t_split:1981287].values), 3)]);\nprint(scipy.stats.ttest_ind(y[0:t_split].values, y[t_split:1981287].values))","b518ee47":"X_train = X[0:t_split] # to be changed later\ny_train = y[0:t_split]\n\nX_val = X[t_split+1:]\ny_val = y[t_split+1:]\n\nweights_resp_train =  weights_resp[0:t_split]\nweights_resp_sq_train = weights_resp_sq[0:t_split]","e21455ad":"base_boost_tree = xgb.XGBClassifier(\n    n_estimators=500,\n    max_depth=10,\n    gamma=0.5,\n    learning_rate=0.05,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    missing=-999,\n    random_state=7,\n    tree_method='gpu_hist'  \n)\n# --> results in 0.4838","97f637c8":"base_boost_tree.fit(X_train, y_train)","fc3f1d4c":"y_pred = base_boost_tree.predict(X_val)\nbase_boost_tree_mae = mean_absolute_error(y_pred, y_val)\nprint(base_boost_tree_mae) ","c03ae938":"#base_boost_tree_1 = xgb.XGBClassifier(\n#    n_estimators=50,\n#    max_depth=10,\n#    gamma=0.5,\n#    learning_rate=0.05,\n#    subsample=0.8,\n#    colsample_bytree=0.8,\n#    missing=-999,\n#    random_state=7,\n#    tree_method='gpu_hist'  \n#)","97b3708a":"#base_boost_tree_1.fit(X_train, y_train)","24210518":"#y_pred = base_boost_tree_1.predict(X_val)\n#base_boost_tree_1_mae = mean_absolute_error(y_pred, y_val)\n#print(base_boost_tree_1_mae) ","e42f97c4":"base_boost_tree_2 = xgb.XGBClassifier(\n    n_estimators=50,\n    max_depth=10,\n    gamma=0.7,\n    learning_rate=0.05,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    missing=-999,\n    random_state=7,\n    tree_method='gpu_hist'  \n)","892d8b7e":"#base_boost_tree_2.fit(X_train, y_train)\n#y_pred = base_boost_tree_2.predict(X_val)\n#base_boost_tree_2_mae = mean_absolute_error(y_pred, y_val)\n#print(base_boost_tree_2_mae) ","b40ea64b":"base_boost_tree_3 = xgb.XGBClassifier(\n    n_estimators=50,\n    max_depth=10,\n    gamma=0.7,\n    learning_rate=0.05,\n    subsample=0.7,\n    colsample_bytree=0.8,\n    missing=-999,\n    random_state=7,\n    tree_method='gpu_hist'  \n)","6ab06e11":"#base_boost_tree_3.fit(X_train, y_train)\n#y_pred = base_boost_tree_3.predict(X_val)\n#base_boost_tree_3_mae = mean_absolute_error(y_pred, y_val)\n#print(base_boost_tree_3_mae) ","291bfda8":"weighted_XGB = xgb.XGBClassifier(\n    n_estimators=500,\n    max_depth=11,\n    gamma=0.5,\n    learning_rate=0.05,\n    subsample=0.9,\n    colsample_bytree=0.8,\n    missing=-999,\n    random_state=7,\n    tree_method='gpu_hist'  \n)","5d925c18":"#weighted_XGB.fit(X_train, y_train, sample_weight = weights_resp_train)","6d966445":"#y_pred = weighted_XGB.predict(X_val)\n#weighted_XGB_mae = mean_absolute_error(y_pred, y_val)\n#print(weighted_XGB_mae) # -> 0.4845","94ae6442":"from lightgbm import LGBMClassifier\nweighted_LGBM = LGBMClassifier(\n    n_estimators=500,\n    max_depth=10,\n    gamma=0.5,\n    learning_rate=0.05,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    missing=-999,\n    random_state=7,\n    tree_method='gpu_hist'  \n)\n# add num_leaves \n","53bc1e99":"weighted_LGBM.fit(X_train, y_train)","2f16a587":"y_pred = weighted_LGBM.predict(X_val)\nweighted_LGBM_mae = mean_absolute_error(y_pred, y_val)\nprint(weighted_LGBM_mae) ","b4aa3bdb":"#for s_weights in [weights_resp_train, weights_resp_sq_train]:\n#    weighted_LGBM.fit(X_train, y_train, sample_weight=s_weights)\n#    y_pred = weighted_LGBM.predict(X_val)\n#    weighted_LGBM_mae = mean_absolute_error(y_pred, y_val)\n#    print(weighted_LGBM_mae)","98b17109":"#final_model = base_boost_tree_3\nfinal_model = weighted_LGBM","9c363c00":"final_model.fit(X, y)","da1a40b4":"for (test_df, sample_prediction_df) in tqdm(iter_test):\n    X_test = test_df.loc[:, test_df.columns.str.contains('feature')]\n    y_preds = final_model.predict(X_test)\n    sample_prediction_df.action = y_preds\n    env.predict(sample_prediction_df)","8d595ae4":"# Making Predictions","8298b07f":"Finally, we will use all features (X) to predict resp (y) and make an action:","a27406a1":"Ok, now we an observe some variance clusterization.\n\nLet's look into other returns (resps), the same subset from the beginning of the sample:","abac0601":"## Manage missing values","baf539df":"## Define key variables\/matrices","cd3d346c":"### Whole Sample Model Fitting","730e4f13":"The means are statistically different, I have to come up with a better splitting scheme later.","b962acf8":"## Return's visualization\n\nI will have a look at different returns: resp, and other reps?","56d71b2e":"Let's look at the distribution of returns, it looks like normal, but with excess mass near 0 and probably heavy tails:","a643fb55":"# 1. Data Preparation","5bac0a5a":"Not very informative, let's look into a subset:","96c9ea03":"# 4. Baseline Model","6fdbb58f":"This change results in minor decrease of MAE but takes much less time to train.","9ebf3fd9":"Define an action variable and an exact list of features:","97627d66":"### Sample split","cfdf9ebb":"Use datatable to read the data faster:","5d4cb29f":"1) Decrease the number of estimators from 500 to 50:","eb5ce830":"### Evaluate Prediction Quality","328f71d9":"# 0. Create Environment","86e30af8":"Indeed, it is very different from the kurtosis of the normal distribution.\n\n## ACF and PACF plots\n\nI will use again data subsample (o\/w computations take too much time) to plot autocorrelation and partial autocorrelation function for returns and squared returns: ","342d8675":" # 7. LGBMClassifier with sample weights","b67a53ae":"3) Change subsample from 0.8 to 0.7:","2eccc84d":"# 6. Baseline XGBClassifier with sample weights\n(python reports that the weights are unused, skip it)","d56454bf":"2) Increase gamma from 0.5 to 0.7:","a4617ab4":"# 2. Data exploration","f27c471a":"There is not so much difference visible this way. However, resp_4 looks more volatile than resp_1. \n\nFinally, let's compare resp with resp_1 and resp_4 over even smaller period for the sake of clarity:","a285f88c":"## Read the data","e1b6ff2e":"# 5. Baseline+-","8ea31744":"Let's look at the estimate of the kurtosis:\n","aaa207ad":"All missings are for features, never for a target variable (resp) that is somewhere among first 10 columns. Good.\n\nNotably, some features have a lot of missings, let's fill them:","fc440994":"Other potentially usefull stuff:","640a595d":"First, check the shape of the dataset. We have 2 390 491 rows and 138 columns.\nSecond, check if there are any missings:","c41a3a03":"# 3. Prepare for Validation\nI would like to split the sample into training and validation portions. Unfortunately, with time series the standard random sampling into them is not appropriate. There exists a TimeSeriesSplit function\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.TimeSeriesSplit.html, but its application will take more time for training than I am ready to allocate for that right now.\n\nThus, for now let's just split manually into two parts , keeping the order of observations, 70%\/30%:","7bd0d91a":"Overall, we see that for the returns there is about no serial correlation, while for the squared returns there is certainly some serial correlation. So, there is serial (nonlinear) dependence of returns that can be exploited. \n","0d3a0427":"We see that resp is often somewhere in between of resp_1 and resp_4 in terms of volatility. Probably, resp was generated as an average of resp_1-resp_4.\n"}}