{"cell_type":{"abb6e865":"code","d1aa3c19":"code","0bc78c63":"code","336b3ba4":"code","54be9853":"code","77ed0253":"code","5ccfc143":"code","36ae6296":"code","829e594e":"code","3365af43":"code","181ba8fb":"code","e51a08e7":"code","0a078417":"code","170f9c17":"code","574cffa2":"code","7386b545":"code","0ea83ebe":"code","461b4008":"code","92c18247":"code","2a2c8a28":"code","249f39b2":"code","bbdc47a8":"code","edbf9bf2":"code","065a0e24":"code","f53fde0f":"code","2bafedd3":"code","0f16ac3c":"code","01b14ece":"code","082eac61":"code","ae904dde":"code","6ab19f8e":"code","1cf323a2":"code","bcf2bcec":"code","83456023":"code","b52a7ace":"code","8e7f615f":"code","8d373627":"code","8b45d36d":"code","ba0f804d":"code","9e009115":"markdown","e0ec549b":"markdown","aadeaf00":"markdown","bd5853ef":"markdown","7a9cae72":"markdown","8ea0e9dd":"markdown","7c1b0633":"markdown","3977726c":"markdown","9533c48e":"markdown","d7803979":"markdown","42d4aa52":"markdown","2bcf29ed":"markdown","102779b7":"markdown","f2cd2a46":"markdown","a2357bff":"markdown","d596bcc2":"markdown","87150a02":"markdown","916c5cc5":"markdown","791eddb6":"markdown","4434da7b":"markdown","ab9d6929":"markdown","e6c850c9":"markdown","e922ed7e":"markdown","7f194ae4":"markdown","0e22d3b9":"markdown","6127921d":"markdown","67b97e53":"markdown","81b80fc0":"markdown","693e0de6":"markdown","0cc95841":"markdown","d82a5c9f":"markdown","b1d3f654":"markdown","94d8bce9":"markdown","cb873358":"markdown","8dddb66a":"markdown"},"source":{"abb6e865":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nSEED = 0\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","d1aa3c19":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")","0bc78c63":"data = pd.read_csv('..\/input\/hr-analytics-job-change-of-data-scientists\/aug_train.csv')\ntest = pd.read_csv('..\/input\/hr-analytics-job-change-of-data-scientists\/aug_test.csv')","336b3ba4":"data.head(5)","54be9853":"test.head(3)","77ed0253":"data.dtypes","5ccfc143":"data.isnull().sum()","36ae6296":"#Churn vs. normal \nsns.countplot(data.target)","829e594e":"#Frequency of each category separated by label\nplt.figure(figsize=[15,18])\nfeatures = ['gender','relevent_experience','enrolled_university','education_level', 'major_discipline',\n       'experience','company_size','company_type','last_new_job']\nn=1\nfor f in features:\n    plt.subplot(5,2,n)\n    sns.countplot(x=f, hue='target', alpha=0.7, data=data)\n    plt.title(\"Countplot of {}  by target\".format(f))\n    n=n+1\nplt.tight_layout()\nplt.show()","3365af43":"np.array(data.columns[data.dtypes != object])","181ba8fb":"import copy\ndf_train=copy.deepcopy(data)\ndf_test=copy.deepcopy(test)\n\ncols=np.array(data.columns[data.dtypes != object])\nfor i in df_train.columns:\n    if i not in cols:\n        df_train[i]=df_train[i].map(str)\n        df_test[i]=df_test[i].map(str)\ndf_train.drop(columns=cols,inplace=True)\ndf_test.drop(columns=np.delete(cols,len(cols)-1),inplace=True)","e51a08e7":"df_train.columns","0a078417":"from sklearn.preprocessing import LabelEncoder\nfrom collections import defaultdict\n\n# build dictionary function\ncols=np.array(data.columns[data.dtypes != object])\nd = defaultdict(LabelEncoder)\n\n# only for categorical columns apply dictionary by calling fit_transform \ndf_train = df_train.apply(lambda x: d[x.name].fit_transform(x))\ndf_test=df_test.apply(lambda x: d[x.name].transform(x))\ndf_train[cols]=data[cols]\ndf_test[np.delete(cols,len(cols)-1)]=test[np.delete(cols,len(cols)-1)]","170f9c17":"df_train.dtypes","574cffa2":"df_test.columns","7386b545":"import matplotlib.pyplot as plt\nimport matplotlib.style as style\nimport seaborn as sns\nstyle.use('ggplot')\nsns.set_style('whitegrid')\nplt.subplots(figsize = (12,7))\n## Plotting heatmap. # Generate a mask for the upper triangle (taken from seaborn example gallery)\nmask = np.zeros_like(df_train.corr().apply(abs), dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(df_train.corr().apply(abs), cmap=sns.diverging_palette(20, 220, n=200), annot=True, mask=mask, center = 0, )\nplt.title(\"Heatmap of all the Features of Train data set\", fontsize = 25)","0ea83ebe":"# visualizing the features whigh positive and negative correlation\nf, axes = plt.subplots(nrows=3, ncols=3, figsize=(20,15))\n\nf.suptitle('Features With High Correlation', size=35)\nsns.boxplot(x=\"target\", y=\"city\", data=df_train, ax=axes[0,0])\nsns.boxplot(x=\"target\", y=\"gender\", data=df_train, ax=axes[0,1])\nsns.boxplot(x=\"target\", y='relevent_experience', data=df_train, ax=axes[0,2])\nsns.boxplot(x=\"target\", y='enrolled_university', data=df_train, ax=axes[1,0])\nsns.boxplot(x=\"target\", y='education_level', data=df_train, ax=axes[1,1])\nsns.boxplot(x=\"target\", y='company_size', data=df_train, ax=axes[1,2])\nsns.boxplot(x=\"target\", y='company_type', data=df_train, ax=axes[2,0])\nsns.boxplot(x=\"target\", y='enrollee_id', data=df_train, ax=axes[2,1])\nsns.boxplot(x=\"target\", y='training_hours', data=df_train, ax=axes[2,2])","461b4008":"counts = data.target.value_counts()\nnot_change = counts[0]\nchange = counts[1]\nperc_not_change = not_change*100\/ sum(counts)\nperc_change = change*100\/ sum(counts)\nprint('There were {} not_change ({:.2f}%) and {} change ({:.2f}%).'.format(not_change, perc_not_change, change, perc_change))","92c18247":"X=df_train.drop(columns=['target']).values\ny=df_train['target'].values","2a2c8a28":"def oversample(X, y, ss=1):\n    from collections import Counter\n    from imblearn.over_sampling import SVMSMOTE\n    from numpy import where\n\n# summarize class distribution\n    print(\"Original class distribution:\")\n    counter = Counter(y)\n    print(counter)\n    \n# transform the dataset\n    X, y = SVMSMOTE(sampling_strategy=ss,n_jobs=-1).fit_resample(X, y)\n    \n    print(\"Over sampling done using SVM SMOTE.\\nNew class distribution is:\")\n# summarize the new class distribution\n    counter = Counter(y)\n    print(counter)\n    \n    return X, y","249f39b2":"X, y = oversample(X,y)","bbdc47a8":"# imports for training and evaluation\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom lightgbm import LGBMClassifier\n\nfrom sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score, make_scorer\nfrom sklearn.model_selection import GridSearchCV","edbf9bf2":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=SEED)\nprint(\"Data splitting complete\")","065a0e24":"# helper functions\ndef evaluate(model, X, y):\n    preds = model.predict(X)\n    \n    labels = [0,1]\n    target_names = [\"not_change\",\"change\"]\n    \n    cm = confusion_matrix(y, preds)\n    cr = classification_report(y, preds, labels=labels, target_names=target_names)\n    \n    fig, ax = plt.subplots()\n    print(cr)\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap='Blues', ax=ax)\n    plt.show()\n    \n    return preds\n    \ndef test_model(model, X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test, scorer=None):\n    model.fit(X_train,y_train)\n        \n    train_preds = evaluate(model, X_train, y_train)\n    test_preds = evaluate(model, X_test, y_test)\n    \n    return train_preds, test_preds\n\ndef plot_preds(model_names, y_true, preds_list, show_value=False, scorer='accuracy'):\n    xval = model_names\n    if scorer=='accuracy':\n        yval = [accuracy_score(y_true, y_pred) for y_pred in preds_list]\n    elif scorer in ['f1 score','f1']:\n        yval = [f1_score(y_true, y_pred) for y_pred in preds_list]\n    plt.figure(figsize=(12,6))\n    plt.ylim(ymax = min(100,max(yval)*1.1), ymin = min(yval)*0.8)\n    plt.ylabel(scorer)\n    plt.xticks(rotation=45)\n    \n    s = sns.barplot(xval,yval)\n    if show_value:\n        for x,y in zip(range(len(yval)),yval):\n            s.text(x,y+0.1,round(y,2),ha=\"center\")","f53fde0f":"train_preds = dict()\ntest_preds = dict()","2bafedd3":"train_preds[\"LR\"],test_preds[\"LR\"] = test_model(LogisticRegression());","0f16ac3c":"train_preds[\"SVC\"],test_preds[\"SVC\"] = test_model(SVC());","01b14ece":"train_preds[\"KNN\"],test_preds[\"KNN\"] = test_model(KNeighborsClassifier());","082eac61":"train_preds[\"RF\"],test_preds[\"RF\"] = test_model(RandomForestClassifier());","ae904dde":"train_preds[\"LGBM\"],test_preds[\"LGBM\"] = test_model(LGBMClassifier());","6ab19f8e":"plot_preds(list(test_preds.keys()),y_test,list(test_preds.values()), 1)","1cf323a2":"# helper functions\ndef best_params(model, grid, X_train=X_train, y_train=y_train):\n    gscv=GridSearchCV(model,grid,scoring=make_scorer(f1_score),n_jobs=-1, verbose=1)\n    grid_search=gscv.fit(X_train,y_train)\n    bp = grid_search.best_params_ \n    print(\"\\nBest Params for {}:\".format(model))\n    for k in bp:\n        print(k,\":\",bp[k])\n    print()\n    return bp","bcf2bcec":"tuned_train_preds = dict()\ntuned_test_preds = dict()","83456023":"model = LogisticRegression()\n\nsolvers = ['newton-cg', 'lbfgs', 'liblinear']\npenalty = ['l2']\nc_values = [100, 10, 1.0, 0.1]\n\ngrid = dict(solver=solvers,penalty=penalty,C=c_values)\n\ntuned_train_preds[\"LR\"],tuned_test_preds[\"LR\"] = test_model(LogisticRegression(**best_params(model,grid)));","b52a7ace":"model = SVC()\n\nC = [1.0, 0.1, 0.01, 0.05, 0.001]\n\ngrid = dict(C=C)\n\ntuned_train_preds[\"SVC\"],tuned_test_preds[\"SVC\"] = test_model(SVC(**best_params(model,grid)));","8e7f615f":"model = KNeighborsClassifier()\n\nn_neighbors = [9,11,13,15]\nweights = ['uniform', 'distance']\nmetric = ['euclidean', 'manhattan', 'minkowski']\n\ngrid = dict(n_neighbors=n_neighbors,weights=weights,metric=metric)\n\ntuned_train_preds[\"KNN\"],tuned_test_preds[\"KNN\"] = test_model(KNeighborsClassifier(**best_params(model,grid)));","8d373627":"model = RandomForestClassifier()\n\nn_estimators = [50, 100, 500]\nmax_features = ['auto','sqrt', 'log2']\nmax_depth = [5,8,10,None]\nmin_samples_split = [3,5,7,9]\ngrid = dict(n_estimators=n_estimators,# max_features=max_features,\n            max_depth=max_depth,min_samples_split=min_samples_split)\n\ntuned_train_preds[\"RF\"],tuned_test_preds[\"RF\"] = test_model(RandomForestClassifier(**best_params(model,grid)));","8b45d36d":"model = LGBMClassifier()\n\nn_estimators = [40, 80, 160]\nlearning_rate = [0.01, 0.05, 0.1, 0.5]\nmax_depth = [5,7,9]\nsubsample = [0.5,0.7,0.9]\ngrid = dict(n_estimators=n_estimators,learning_rate=learning_rate,\n            max_depth=max_depth,subsample=subsample)\n\ntuned_train_preds[\"LGBM\"],tuned_test_preds[\"LGBM\"] = test_model(LGBMClassifier(**best_params(model,grid)));","ba0f804d":"plot_preds(list(tuned_test_preds.keys()),y_test,list(tuned_test_preds.values()), 1)","9e009115":"Next, let's look at the frequency of each category separated the histogram charts to check if there is any special information to distinguish whether the result of the \"target\" is 0 - Not looking for job change, OR, 1 - Looking for a job change.","e0ec549b":"### Evaluation of Tuned Models","aadeaf00":"**But, \"Categorical variables\"**\n\nWe need to deal with categorical variables so columns which have values different than numbers. \n\nA simple way of selecting all categorical columns is by checking their type.\n\nThus, In the database, only 4 columns are of numerical-data, and up to 10 columns are Categorical variables type.","bd5853ef":"### A. Logistic Regression","7a9cae72":"### C. kNN (k- Nearest Neighbors)","8ea0e9dd":"### Evaluation of Base Models","7c1b0633":"### A. Logistic Regression","3977726c":"## Dataset information:\n\n+ enrollee_id : Unique ID for enrollee\n+ city: City code\n+ citydevelopmentindex: Developement index of the city (scaled)\n+ gender: Gender of enrolee\n+ relevent_experience: Relevent experience of enrolee\n+ enrolled_university: Type of University course enrolled if any\n+ education_level: Education level of enrolee\n+ major_discipline :Education major discipline of enrolee\n+ experience: Enrolee total experience in years\n+ company_size: No of employees in current employer's company\n+ company_type : Type of current employer\n+ lastnewjob: Difference in years between previous job and current job\n+ training_hours: training hours completed\n+ target: 0 \u2013 Not looking for job change, 1 \u2013 Looking for a job change","9533c48e":"### D. Random Forest","d7803979":"### C. K-Nearest Neighbors","42d4aa52":"Borderline-SMOTE SVM\n\nHien Nguyen, et al. suggest using an alternative of Borderline-SMOTE where an SVM algorithm is used instead of a KNN to identify misclassified examples on the decision boundary.\n\nTheir approach is summarized in the 2009 paper titled \u201cBorderline Over-sampling For Imbalanced Data Classification.\u201d An SVM is used to locate the decision boundary defined by the support vectors and examples in the minority class that close to the support vectors become the focus for generating synthetic examples.","2bcf29ed":"# 2. Data Preprocessing","102779b7":"### E. LightGBM","f2cd2a46":"### D. Random Forest","a2357bff":"## Data Encoding","d596bcc2":"From the above heatmap we can clearly observe that the target has a high dependance on the city_development_index which means candidates from city with higher amount of development index tends to move towards the field of data science.","87150a02":"## Training Base Models","916c5cc5":"We will assign each categorical variable value a number, so let\u2019s say [A, B, A, F] named values will map to [1, 2, 1, 3]. To do that we will use LabelEncoder from sklearn.preprocessing package, as following.","791eddb6":"# Notebook outline:\n\n1. Data Analysis and Visualization\n    - Dataset Information\n    - Visualizations\n2. Data Preprocessing\n    - Data Encoding\n    - Deal with Imbalanced Data using SMOTE\n3. Models Training and Evaluation\n    - Splitting data into train and test set\n    - Training Base Models\n    - Evaluation of Base Models\n    - Hyperparameter Tuning\n    - Evaluation of Tuned Models","4434da7b":"## Deal with Imbalanced Data using SMOTE","ab9d6929":"# 3. Model Training and Evaluation","e6c850c9":"# 1. Data Analysis and Visualization","e922ed7e":"From these histogram charts, it can be seen, there is no special correlation between the variables with the target function to distinguish the value of the target. Furthermore, categorical variables cannot determine the correlation factor between these variables and the target function.","7f194ae4":"Some basic libraries ...","0e22d3b9":"## Visualizations","6127921d":"... and try to check the type of the data types.","67b97e53":"## Hyperparameter Tuning","81b80fc0":"I will load the dataset by using pandas the standard python approach for dealing with data.","693e0de6":"### B. SVM - Support Vector Classifier","0cc95841":"### E. Light GBM","d82a5c9f":"Now examine the results, considering the correlation between \"pseudo categorical variables\" and the \"target\" objective function.","b1d3f654":"## Splitting data into train and test set","94d8bce9":"### B. SVM","cb873358":"From this we can clearly see that the target 0 is in majority which will effect our model so we will use SMOTE (Synthetic Minority Over-sampling Technique) which will help us to create more synthetic data for the minority class 1 :)\n    ","8dddb66a":"... and a significant amount of NaN data ..."}}