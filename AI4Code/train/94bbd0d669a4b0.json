{"cell_type":{"b6316d39":"code","c3df6d5f":"code","a2a05e99":"code","0d4712c7":"code","fdf927fe":"code","f5fea7fb":"code","b4030b6e":"code","12a1eafd":"code","4967bd2d":"code","283da47d":"code","cf81882f":"code","b0e7f7bd":"code","71f395a2":"code","0955423b":"code","3cf2b659":"code","18e17e32":"code","fd15d5e9":"code","6975083d":"code","914a9d2c":"code","171edeab":"code","40fa8f26":"code","74f12dd0":"code","18570c3e":"code","26db68a7":"code","0c361669":"markdown","0140675d":"markdown","638987b5":"markdown","c7b84c05":"markdown","de034f55":"markdown","a16e0ab4":"markdown","7ceed210":"markdown"},"source":{"b6316d39":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","c3df6d5f":"import random\nfrom sklearn.metrics import mean_squared_error,roc_auc_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold,StratifiedKFold\nfrom lightgbm import LGBMClassifier\n\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom sklearn import preprocessing\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport matplotlib.pyplot as plt\n%matplotlib inline","a2a05e99":"train = pd.read_csv('\/kaggle\/input\/tabular-playground-series-mar-2021\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/tabular-playground-series-mar-2021\/test.csv')\nsub = pd.read_csv('\/kaggle\/input\/tabular-playground-series-mar-2021\/sample_submission.csv')","0d4712c7":"train.head()","fdf927fe":"test.head()","f5fea7fb":"train.info()","b4030b6e":"test.info()","12a1eafd":"#Check if there'is null values\ntrain.isnull().sum()","4967bd2d":"#Check if there'is null values\ntest.isnull().sum()","283da47d":"train.describe()","cf81882f":"categorical_cols = ['cat'+str(i) for i in range(19)]\ncontinous_cols = ['cont'+str(i) for i in range(11)]","b0e7f7bd":"# Numerical features distribution \ni = 1\nplt.figure()\nfig, ax = plt.subplots(6, 2,figsize=(20, 24))\nfor feature in continous_cols:\n    plt.subplot(6, 2,i)\n    sns.histplot(train[feature],color=\"blue\", kde=True,bins=100, label='train')\n    sns.histplot(test[feature],color=\"olive\", kde=True,bins=100, label='test')\n    plt.xlabel(feature, fontsize=9); plt.legend()\n    i += 1\nplt.show()","71f395a2":"# Categorical features distribution \ni = 1\nplt.figure()\nfig, ax = plt.subplots(10, 2,figsize=(28, 44))\nfor feature in categorical_cols:\n    plt.subplot(10, 2,i)\n    sns.histplot(train[feature],color=\"blue\", label='train')\n    sns.histplot(test[feature],color=\"olive\", label='test')\n    plt.xlabel(feature, fontsize=9); plt.legend()\n    i += 1\nplt.show()","0955423b":"#Features correlation\ncorr = train[continous_cols+['target']].corr()\ncorr.style.background_gradient(cmap='coolwarm').set_precision(2)","3cf2b659":"# Target distibution\nsns.catplot(x=\"target\", kind=\"count\", palette=\"ch:.25\", data=train)","18e17e32":"cols=categorical_cols+continous_cols\ntrain_objs_num = len(train)\ndataset = pd.concat(objs=[train[cols], test[cols]], axis=0)\ndataset_preprocessed = pd.get_dummies(dataset,columns=categorical_cols)\ntrain_preprocessed = dataset_preprocessed[:train_objs_num]\ntest_preprocessed = dataset_preprocessed[train_objs_num:]","fd15d5e9":"train_preprocessed","6975083d":"test_preprocessed","914a9d2c":"params={'bagging_freq': 1, 'reg_alpha': 0.0019599943581399, 'reg_lambda': 0.028564643120722513, 'colsample_bytree': 0.24,\n     'subsample': 0.8, 'learning_rate': 0.008, 'max_depth': 140, 'num_leaves': 100, 'min_child_samples': 228, 'max_bin': 238,\n     'cat_smooth': 75,'metric': 'auc', 'random_state': 48,'n_estimators': 20000}","171edeab":"preds = np.zeros(test.shape[0])        \nkf = StratifiedKFold(n_splits=5,random_state=48,shuffle=True)                  \nauc=[]   # list contains AUC for each fold  \nn=0   \nfor trn_idx, test_idx in kf.split(train_preprocessed,train['target']):\n    X_tr,X_val=train_preprocessed.iloc[trn_idx],train_preprocessed.iloc[test_idx]\n    y_tr,y_val=train['target'].iloc[trn_idx],train['target'].iloc[test_idx]\n    model = LGBMClassifier(**params) \n    model.fit(X_tr,y_tr,eval_set=[(X_val,y_val)],early_stopping_rounds=200,verbose=False) \n    preds+=model.predict_proba(test_preprocessed)[:, 1]\/kf.n_splits \n    auc.append(roc_auc_score(y_val, model.predict_proba(X_val)[:, 1])) \n    print(n+1,auc[n])                                                                                       \n    n+=1                      ","40fa8f26":"np.mean(auc)","74f12dd0":"# most 10 important features for lgb model\nfrom optuna.integration import lightgbm as lgb\nlgb.plot_importance(model, max_num_features=10, figsize=(10,10))\nplt.show()","18570c3e":"sub['target']=preds\nsub.to_csv('submission.csv', index=False)","26db68a7":"sub","0c361669":"# Making a Submission","0140675d":"# Modeling","638987b5":"# Hi kagglers \ud83d\ude4b\ud83c\udffb\u200d\u2642\ufe0f and Welcome to this new competition!","c7b84c05":"# One Hot Encoding for Encoding Categorical Features ","de034f55":"# I hope that you find this kernel usefull\ud83c\udfc4","a16e0ab4":"# Let's do some Exploratory Data Analysis (EDA)","7ceed210":"* As we can see the data is unbalanced that's why I'll use  **StratifiedKFold** to split data"}}