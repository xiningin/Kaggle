{"cell_type":{"51314f71":"code","883a4cd8":"code","3841776c":"code","b27e6415":"code","e26e27cf":"code","3e66b441":"code","f1c00ca5":"code","e771070b":"code","41c0b67a":"code","2684ea71":"code","758a77a0":"code","112270ed":"code","7039a50b":"code","ff6f5927":"code","eba81861":"code","68150d47":"code","830aaaff":"code","9253ebe0":"code","1fac4c0a":"code","c24f2a71":"code","9c62fbfd":"code","39e2f454":"code","e65d31f4":"code","5ef1f3b7":"code","9031cc48":"code","734faa1c":"code","d2f056f4":"code","d4a52900":"code","7cbc0b73":"code","fd892c45":"code","9548d263":"code","b8a43d07":"code","9e7b5ee5":"code","bfa423b4":"code","6a25b19f":"code","daa0e02d":"code","88a8f9e2":"code","773bb054":"code","38e07a36":"code","cf03b022":"code","ae0f5f6c":"markdown","7d63f0bb":"markdown","a25572ad":"markdown","c018dd8e":"markdown","861a244d":"markdown","e1193dec":"markdown","0f34ed75":"markdown","2be00474":"markdown","a7e0cb24":"markdown","b904fb11":"markdown"},"source":{"51314f71":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","883a4cd8":"df=pd.read_csv(\"..\/input\/titanic\/train.csv\",usecols=['Cabin','Survived'])\ndf.head()","3841776c":"\n### Replacing\ndf['Cabin'].fillna('Missing',inplace=True)\ndf.head()","b27e6415":"df['Cabin'].unique()\n","e26e27cf":"df['Cabin']=df['Cabin'].astype(str).str[0]\ndf.head()","3e66b441":"df.Cabin.unique()\n","f1c00ca5":"prob_df=df.groupby(['Cabin'])['Survived'].mean()\n","e771070b":"\nprob_df=pd.DataFrame(prob_df)\nprob_df","41c0b67a":"prob_df['Died']=1-prob_df['Survived']\n","2684ea71":"prob_df['Probability_ratio']=prob_df['Survived']\/prob_df['Died']\nprob_df.head()\n","758a77a0":"probability_encoded=prob_df['Probability_ratio'].to_dict()\n","112270ed":"df['Cabin_encoded']=df['Cabin'].map(probability_encoded)\ndf.head()","7039a50b":"df.head(20)\n","ff6f5927":"df=pd.read_csv('..\/input\/titanic\/train.csv', usecols=['Pclass','Age','Fare','Survived'])\ndf.head()","eba81861":"df['Age'].fillna(df.Age.median(),inplace=True)\n","68150d47":"df.isnull().sum()\n","830aaaff":"from sklearn.preprocessing import StandardScaler\n","9253ebe0":"\nimport matplotlib.pyplot as plt\n%matplotlib inline\n# import matplotlib.pyplot as plt\nplt.rcParams[\"figure.figsize\"] = (14,8)","1fac4c0a":"plt.hist(df['Fare'],bins=20)\n","c24f2a71":"from sklearn.preprocessing import MinMaxScaler\nmin_max=MinMaxScaler()\ndf_minmax=pd.DataFrame(min_max.fit_transform(df),columns=df.columns)\ndf_minmax.head()","9c62fbfd":"plt.hist(df_minmax['Pclass'],bins=20)\n","39e2f454":"plt.hist(df_minmax['Age'],bins=20)\n","e65d31f4":"plt.hist(df_minmax['Fare'],bins=20)\n","5ef1f3b7":"from sklearn.preprocessing import RobustScaler\nscaler=RobustScaler()\ndf_robust_scaler=pd.DataFrame(scaler.fit_transform(df),columns=df.columns)\ndf_robust_scaler.head()","9031cc48":"plt.hist(df_robust_scaler['Age'],bins=20)\n","734faa1c":"df=pd.read_csv('..\/input\/titanic\/train.csv', usecols=['Age','Fare','Survived'])\ndf.head()","d2f056f4":"df['Age']=df['Age'].fillna(df['Age'].median())\n","d4a52900":"df.isnull().sum()\n","7cbc0b73":"import scipy.stats as stat\nimport pylab","fd892c45":"#### If you want to check whether feature is guassian or normal distributed\n#### Q-Q plot\ndef plot_data(df,feature):\n    plt.figure(figsize=(14,8))\n    plt.subplot(1,2,1)\n    df[feature].hist()\n    plt.subplot(1,2,2)\n    stat.probplot(df[feature],dist='norm',plot=pylab)\n    plt.show()","9548d263":"plot_data(df,'Age')\n","b8a43d07":"\nimport numpy as np\ndf['Age_log']=np.log(df['Age'])\nplot_data(df,'Age_log')","9e7b5ee5":"df['Age_reciprocal']=1\/df.Age\nplot_data(df,'Age_reciprocal')","bfa423b4":"df['Age_sqaure']=df.Age**(1\/2)\nplot_data(df,'Age_sqaure')","6a25b19f":"df['Age_exponential']=df.Age**(1\/1.2)\nplot_data(df,'Age_exponential')","daa0e02d":"df['Age_Boxcox'],parameters=stat.boxcox(df['Age'])\n","88a8f9e2":"plot_data(df,'Age_Boxcox')\n","773bb054":"plot_data(df,'Fare')\n","38e07a36":"df['Fare_log']=np.log1p(df['Fare'])\nplot_data(df,'Fare_log')","cf03b022":"df['Fare_Boxcox'],parameters=stat.boxcox(df['Fare']+1)\nplot_data(df,'Fare_Boxcox')","ae0f5f6c":"Logarithmic Transformation\u00b6\n","7d63f0bb":"Robust Scaler\nIt is used to scale the feature to median and quantiles Scaling using median and quantiles consists of substracting the median to all the observations, and then dividing by the interquantile difference. The interquantile difference is the difference between the 75th and 25th quantile:\n\nIQR = 75th quantile - 25th quantile\n\nX_scaled = (X - X.median) \/ IQR\n\n0,1,2,3,4,5,6,7,8,9,10\n\n9-90 percentile---90% of all values in this group is less than 9 1-10 precentile---10% of all values in this group is less than 1 4-40%\n\n","a25572ad":"Reciprocal Trnasformation\u00b6\n","c018dd8e":"BoxCOx Transformation\nThe Box-Cox transformation is defined as:\n\nT(Y)=(Y exp(\u03bb)\u22121)\/\u03bb\n\nwhere Y is the response variable and \u03bb is the transformation parameter. \u03bb varies from -5 to 5. In the transformation, all values of \u03bb are considered and the optimal value for a given variable is selected.","861a244d":"Some machine learning algorithms like linear and logistic assume that the features are normally distributed -Accuracy -Performance\n\n* logarithmic transformation\n* reciprocal transformation\n* square root transformation\n* exponential transformation (more general, you can use any exponent)\n* boxcox transformation","e1193dec":"Min Max Scaling (### CNN)---Deep Learning Techniques\nMin Max Scaling scales the values between 0 to 1. X_scaled = (X - X.min \/ (X.max - X.min)","0f34ed75":"references:- https:\/\/www.youtube.com\/watch?v=B3gyVWw1UBg&feature=em-lbcastemail","2be00474":"Transformation of Features\nWhy Transformation of Features Are Required?\n\nLinear Regression---Gradient Descent ----Global Minima\nAlgorithms like KNN,K Means,Hierarichal Clustering--- Eucledian Distance\nEvery Point has some vectors and Directiom\n\nDeep Learning Techniques(Standardization, Scaling) 1.ANN--->GLobal Minima, Gradient 2.CNN 3.RNN\n\n0-255 pixels\n\nTypes Of Transformation\nNormalization And Standardization\nScaling to Minimum And Maximum values\nScaling To Median And Quantiles\nGuassian Transformation Logarithmic Transformation Reciprocal Trnasformation Square Root Transformation Exponential Trnasformation Box Cox Transformation\nStandardization\nWe try to bring all the variables or features to a similar scale. standarisation means centering the variable at zero. z=(x-x_mean)\/std\n\n","a7e0cb24":"Exponential Transformation","b904fb11":"Square Root Transformation"}}