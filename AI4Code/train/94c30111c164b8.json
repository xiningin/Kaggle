{"cell_type":{"b6c41ed8":"code","58a00adb":"code","010cd1d7":"code","ee471e9b":"code","f7d40021":"code","f8a0e25e":"code","755f31e0":"code","d628e0b6":"code","2a5cf3b6":"code","f2c3e82f":"code","a234ae25":"code","92df4a56":"code","47c28dda":"code","b80cc634":"code","2c70b943":"code","74caab95":"code","fd0ea41c":"code","e96c4a05":"code","8f157b6e":"code","4fc9ccaf":"code","b0dd0827":"code","b9de1225":"code","ff60a983":"code","118d4634":"code","84be213f":"code","3c193f07":"code","b84f1221":"code","b59b3fe9":"code","3e61f12f":"code","5d750ab5":"code","851235a6":"code","8caee80f":"code","549c44bb":"code","580a5175":"code","990cceaf":"code","f5ccb6b4":"code","957393a1":"code","7af94fe4":"code","674362d8":"code","49b48ff9":"code","a5354882":"code","d6ad64c9":"markdown","4347f1e3":"markdown","3a24a4d8":"markdown","5cc5ff55":"markdown","896863bc":"markdown","d52b0b1a":"markdown","bcd22c0c":"markdown","6d9520ad":"markdown","ecfbb2e0":"markdown","da6b1e07":"markdown","a2f6b8e8":"markdown","14a5d6c5":"markdown","6130ab00":"markdown","9b8572aa":"markdown","fa9bd0db":"markdown","b484273f":"markdown","828922f0":"markdown","5662dbff":"markdown"},"source":{"b6c41ed8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","58a00adb":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport nltk\nimport re\nimport string\nfrom nltk.corpus import stopwords\nimport gensim\nfrom gensim import parsing\nfrom wordcloud import WordCloud,STOPWORDS\n\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\nfrom sklearn.metrics import accuracy_score,confusion_matrix, classification_report\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\n","010cd1d7":"train= pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')","ee471e9b":"test=pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")","f7d40021":"train.head()","f8a0e25e":"train.info()","755f31e0":"train.isnull().sum()","d628e0b6":"train.drop(['keyword', 'location'],axis=1, inplace=True)","2a5cf3b6":"train['target'].value_counts()","f2c3e82f":"plot=sns.countplot(train['target'])\nplot.set_title(\"Count of disaster and non disaster tweets\")","a234ae25":"mylabels=[\"Non-Disaster\", \"Disaster\"]\nmycolors=['pink', 'blue']\nplt.pie(train['target'].value_counts(), labels=mylabels, colors=mycolors,autopct='%1.1f%%')\nplt.legend()\nplt.show()","92df4a56":"plt.figure(figsize = (15,15))\nwc = WordCloud(max_words = 500 , width = 1000 , height = 500 , stopwords = STOPWORDS).generate(\" \".join(train[train.target == 1].text))\nplt.imshow(wc , interpolation = 'bilinear')","47c28dda":"plt.figure(figsize = (15,15))\nwc = WordCloud(max_words = 500 , width = 1000 , height = 500 , stopwords = STOPWORDS).generate(\" \".join(train[train.target == 0].text))\nplt.imshow(wc , interpolation = 'bilinear')","b80cc634":"def transformText(text):\n  # All the necessary preprocessing on our text of choice\n    stops = set(stopwords.words(\"english\"))\n  # Convert text to lower\n    text = text.lower()\n  # Removing non ASCII chars    \n    text = re.sub(r'[^\\x00-\\x7f]',r' ',text) \n    text = re.sub('\\[[^]]*\\]', '', text)\n    text = re.sub('http','',text)\n    text= gensim.parsing.preprocessing.strip_non_alphanum(text)                       \n  # Strip multiple whitespaces\n    text = gensim.corpora.textcorpus.strip_multiple_whitespaces(text)\n  # Removing all the stopwords\n    filtered_words = [word for word in text.split() if word not in stops]\n  # Removing all the tokens with lesser than 3 characters\n    filtered_words = gensim.corpora.textcorpus.remove_short(filtered_words, minsize=3)\n  # Preprocessed text after stop words removal\n    text = \" \".join(filtered_words)\n  # Remove the punctuation\n    text = gensim.parsing.preprocessing.strip_punctuation2(text)\n  # Strip all the numerics\n    text = gensim.parsing.preprocessing.strip_numeric(text)\n  # Strip multiple whitespaces\n    text = gensim.corpora.textcorpus.strip_multiple_whitespaces(text)\n  # Stemming\n    return gensim.parsing.preprocessing.stem_text(text)","2c70b943":"train['text']=train['text'].apply(transformText)","74caab95":"train.head(10)","fd0ea41c":"texts=''.join(train['text'])\nwords=texts.split(\" \")","e96c4a05":"def draw_n_gram(words,i):\n    n_gram=(pd.Series(nltk.ngrams(words,i)).value_counts())[:15]\n    n_gram_df=pd.DataFrame(n_gram)\n    n_gram_df=n_gram_df.reset_index()\n    n_gram_df = n_gram_df.rename(columns={\"index\": \"word\", 0: \"count\"})\n    print(n_gram_df.head())\n    plt.figure(figsize = (16,9))\n    return sns.barplot(x='count',y='word', data=n_gram_df)\n","8f157b6e":"draw_n_gram(words,1)","4fc9ccaf":"draw_n_gram(words,2)","b0dd0827":"draw_n_gram(words,3)","b9de1225":"X=train['text']\ny=train['target']","ff60a983":"X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.30, random_state=1)","118d4634":"print(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","84be213f":"vectorizer=CountVectorizer()\ntransformer=TfidfTransformer()","3c193f07":"X_train_vect=vectorizer.fit_transform(X_train)\nX_train_trans=transformer.fit_transform(X_train_vect)","b84f1221":"X_train_trans.toarray()","b59b3fe9":"X_test_vect=vectorizer.transform(X_test)\nX_test_trans=transformer.transform(X_test_vect)","3e61f12f":"X_test_trans.toarray()","5d750ab5":"lr=LogisticRegression()\ndtc= DecisionTreeClassifier()\nrfc= RandomForestClassifier()\nsvm= SVC()\nknn= KNeighborsClassifier()\nnb= GaussianNB()","851235a6":"lr.fit(X_train_trans,y_train)\ndtc.fit(X_train_trans,y_train)\nrfc.fit(X_train_trans ,y_train)\nsvm.fit(X_train_trans ,y_train)\nknn.fit(X_train_trans ,y_train)\n#nb.fit(X_train_trans ,y_train)","8caee80f":"predict_lr = lr.predict(X_test_trans)\npredict_dtc = dtc.predict(X_test_trans)\npredict_rfc = rfc.predict(X_test_trans)\npredict_svm = svm.predict(X_test_trans)\npredict_knn = knn.predict(X_test_trans)","549c44bb":"acc_1=accuracy_score(predict_lr,y_test)\nprint(\"Accuracy of LogisticRegression = \" +str(acc_1))\nacc_2=accuracy_score(predict_dtc,y_test)\nprint(\"Accuracy of DecisionTreeClassifier = \" +str(acc_2))\nacc_3 =  accuracy_score(predict_rfc,y_test)\nprint(\"Accuracy of RandomForestClassifier = \" +str(acc_3))\nacc_4 = accuracy_score(predict_svm,y_test)\nprint(\"Accuracy of SupportVectorClassifier = \" +str(acc_4))\nacc_5 = accuracy_score(predict_knn,y_test)\nprint(\"Accuracy of KNearestNeighbor = \" +str(acc_5))","580a5175":"X_test=test['text']\nX_test.head()","990cceaf":"X_test_vec=vectorizer.transform(X_test)\nX_test_tran=transformer.transform(X_test_vec)","f5ccb6b4":"X_test_tran.toarray()","957393a1":"predictions= svm.predict(X_test_tran)\npredictions","7af94fe4":"submission = pd.DataFrame()","674362d8":"submission['id']= test['id']\nsubmission['target']= predictions","49b48ff9":"submission['target'].value_counts()","a5354882":"submission.to_csv('solution.csv')","d6ad64c9":"**Bi-gram Analysis**","4347f1e3":"**For test data**","3a24a4d8":"**Checking model performance**","5cc5ff55":"**Now, we predict for test data using SVC**","896863bc":"# Building Model ","d52b0b1a":"The dataset is balanced.","bcd22c0c":"**Unigram Analysis**","6d9520ad":"# Feature Extraction with Count Vectorizer and TfidfTransformer(Term Frequency-Inverse Document Frequency)","ecfbb2e0":"**Data Preprocessing**","da6b1e07":"**Tri-gram Analysis**","a2f6b8e8":"**For train data**","14a5d6c5":"**WordCloud for Non-Disaster Tweets**","6130ab00":"**Splitting data into train and test set**","9b8572aa":"**Making Predictions**","fa9bd0db":"**N-Gram Analysis**","b484273f":"**WordCloud for Disaster Tweets**","828922f0":"**Data Visualization**","5662dbff":"**Here, we see that SupportVectorClassifier gives the best accuracy score of 80.47%**"}}