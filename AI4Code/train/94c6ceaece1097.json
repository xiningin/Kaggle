{"cell_type":{"a3edf5f5":"code","20c71bf1":"code","2e787797":"code","1259ac86":"code","d41ea3e1":"code","ef7170ea":"code","9baa9f0a":"code","1f7875c4":"code","92064f5f":"code","bef0322f":"code","0e5447e2":"code","76452b0c":"code","fe945ffd":"code","cbc383c5":"code","cd5c2c77":"code","fda85f43":"code","b2475b85":"code","4df2cbcb":"code","5cbecf5c":"code","b93dba2d":"code","02c471fa":"markdown","f05184e5":"markdown","f7bead68":"markdown","ea9b588d":"markdown","fe62b283":"markdown","70e17f3b":"markdown","42561797":"markdown"},"source":{"a3edf5f5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","20c71bf1":"# Read the data and assign it as df_train and df_test \ndf_train=pd.read_csv(\"..\/input\/train.csv\")\ndf_test=pd.read_csv(\"..\/input\/test.csv\")","2e787797":"# Let's have a quick look into data.This code shows first 5 rows and all columns\ndf_train.head()","1259ac86":"# Let's have a quick look into data.This code shows first 5 rows and all columns\ndf_test.head()","d41ea3e1":"# If there is unknown,missing or unproper data, this codes shows the number of them\n# We can also learn about features such as data type of the features\ndf_train.info()","ef7170ea":"df_test.info()","9baa9f0a":"# statistical data is important to learn about balance inside or among the features.\ndf_train.describe()","1f7875c4":"# Seaborn countplot gives the number of data in the each class\nsns.countplot(x=\"target\", data=df_train)","92064f5f":"# y has target data (clases) such as 1 and 0. \ny_train_data = df_train.target.values.reshape(-1,1)\n# This means that take target and ID_code data out from the datasets and assign them to variable\nx_train_data = df_train.drop([\"target\",\"ID_code\"],axis=1)","bef0322f":"#Normalization is used to handle with unbalanced features\n#This gives the values to the features which range from zero to 1.\nx = (x_train_data - np.min(x_train_data))\/(np.max(x_train_data)-np.min(x_train_data)).values","0e5447e2":"# Preperation of testing data\nx_test_data = df_test.drop([\"ID_code\"],axis=1)\n#x_test_data.head()","76452b0c":"# Build Logistic Logistic Regression Algorithm\nfrom sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(x_train_data,y_train_data)\n\ny_lr_test_data = lr.predict(x_test_data)","fe945ffd":"ID_code_data = df_test.ID_code.values\nfrom numpy import array\nfrom numpy import vstack\nheader=[['ID_code','target']]\nlr_array = vstack((ID_code_data, y_lr_test_data)).T\nframe_lr = pd.DataFrame(lr_array, columns=header)\nprint (frame_lr)","cbc383c5":"# Build Decision Tree Classification Model\nfrom sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier(random_state = 42)\ndt.fit(x_train_data,y_train_data)\ny_dt_test_data = dt.predict(x_test_data)","cd5c2c77":"header=[['ID_code','target']]\ndt_array = vstack((ID_code_data, y_dt_test_data)).T\nframe_dt = pd.DataFrame(dt_array, columns=header)\nprint (frame_dt)","fda85f43":"# Build Random Forest Classification Model\nfrom sklearn.ensemble import RandomForestClassifier\n# n_estimators = 100 means this model will use 100 subsets.\nrf = RandomForestClassifier(n_estimators = 100,random_state = 42)\nrf.fit(x_train_data,y_train_data)\ny_rf_test_data = rf.predict(x_test_data)","b2475b85":"header=[['ID_code','target']]\nrf_array = vstack((ID_code_data, y_rf_test_data)).T\nframe_rf = pd.DataFrame(rf_array, columns=header)\nprint (frame_rf)","4df2cbcb":"# Build Naive Bayes Classification Model\nfrom sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(x_train_data,y_train_data)\ny_nb_test_data = nb.predict(x_test_data)","5cbecf5c":"header=[['ID_code','target']]\nnb_array = vstack((ID_code_data, y_nb_test_data)).T\nframe_nb = pd.DataFrame(nb_array, columns=header)\nprint (frame_nb)","b93dba2d":"# These pandas DataFrames have 2 columns as ID_code and the predicted values of y_test.\nLogistic_regression = frame_lr\nDecision_tree_classification = frame_dt\nRandom_forest_classification = frame_rf\nNaive_bayes_classification = frame_nb","02c471fa":"<a id=\"6\"><\/a> <br>\n6.Conclusion","f05184e5":"<a id=\"2\"><\/a> <br>\n2.Logistic Regression Classification\n\nIt is very powerfull algorithm to use with binary classification.\n","f7bead68":"<a id=\"3\"><\/a> <br>\n3.Decision Tree Classification\n\n\"Decision tree builds classification or regression models in the form of a tree structure. It breaks down a dataset into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed\".\n\nAccording to \u0131nformation entropy, we can determine which feature is the most important. And we put the most important one to the top of the related tree.\n\nDecision tree classification can be used for both binary and multi classes\n","ea9b588d":"<a id=\"5\"><\/a> <br>\n5.Naive Bayes Classification\n\n\"Naive Bayes classifiers are a family of simple \"probabilistic classifiers\" based on applying Bayes' theorem with strong (naive) independence assumptions between the features.\"\n\nHere we basically determine similarity range and calculate probabilty of the X point in the A feature P(A_feature|x).","fe62b283":"<a id=\"4\"><\/a> <br>\n4.Random Forest Classification\n\nThis methods basically use multiple number of decision trees and take the avarage of the results of these decision trees. And we use this avarage to determine the class of the test point.\n\nThis is one of ensamble method which uses multiple classes to predict the target, and very powerfull technique.\n","70e17f3b":"## Supervised Machine Learning Classifications\nIn this tutorial I will apply supervised machine learning classifications to the canser data sets in order to determine if tested data has heart diseases or not. I will use KNN classification, decision tree classification, random forest classification,Support vector machine, logistig regression and naive bayes algorithms. I will show also how to determine accuracy of the each classificaiton and make evaluation by using confusion matrix.\n\n1. [EDA(Exploratory Data Analaysis)](#1)\n2. [Logistic Regression Classification](#2)\n4. [Decision Tree Classification](#3)\n5. [Random Forest Classification](#4)\n7. [Naive Bayes Classification](#5)\n6. [Conclusion](#6)","42561797":"<a id=\"1\"><\/a> <br>\n1. EDA(Exploratory Data Analaysis)\n\nEDA is very important to look at what is inside the data. For example, if there is object(string) in the data, we need to change it to integer or float because sci-learn is not handling with object data. There are also missdata in the datasets, we need to handle them."}}