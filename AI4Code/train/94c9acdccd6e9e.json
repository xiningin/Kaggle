{"cell_type":{"2269333c":"code","c8060d28":"code","9ade99b3":"code","320529ed":"code","ab2acfe3":"code","0db49b5d":"code","7c073b0f":"code","1647ed00":"code","9d3da498":"code","bd7761ab":"code","6f1f8ba0":"code","d0de20f9":"code","2e39db60":"code","4f56aef1":"code","f5eb27f3":"code","9b1386d0":"code","caccfc8f":"code","a895401a":"code","b1f70b8e":"code","5dd1b675":"code","da00f109":"code","19576308":"code","af968ef4":"code","9c163490":"code","54076c3e":"code","da3daecd":"code","50feb8e3":"code","b16a38a8":"code","c8666a9b":"code","11c6ef30":"code","67d7cc1a":"code","51524709":"code","928645a5":"code","3463d57d":"code","c9177bd9":"code","f2b1dde6":"code","c373d4de":"code","45fe4ef3":"code","d5c1664f":"code","fff62ca7":"code","a0134fa9":"code","15bd4697":"code","e2c3d354":"code","71362b1e":"code","4c8af7f9":"code","bb2b84b2":"code","e6f55f35":"code","168c5f2c":"code","f0b8dd9d":"code","9ffe83bd":"code","a993ed4a":"code","06a819b6":"code","27a96608":"code","b8e8fd84":"code","a3d0fced":"code","aa0a57a9":"code","c55ee4a7":"code","1013b861":"code","e98147fe":"code","0ee62b7e":"code","e1d155d2":"code","c9b90024":"code","dbd6fd08":"code","f289625a":"code","0005096a":"code","77875c78":"code","d908cf0e":"code","345bd483":"code","e583aff9":"code","d633287f":"code","6387e0d0":"code","b494cdb9":"code","dd4fd103":"code","48fd2094":"code","4fb6771a":"code","3c83eda4":"code","9541b077":"code","2caa6d78":"code","23f33807":"code","6d9203eb":"markdown","fbb665d1":"markdown","cd1af0fd":"markdown","778b9c0a":"markdown","f97c6889":"markdown","cacde8cc":"markdown","9390b1c5":"markdown","f953a2f4":"markdown","e93202d3":"markdown","f95e6f46":"markdown","92e7e0a0":"markdown","df3da281":"markdown","5a2698be":"markdown","5de6aba2":"markdown","36b74198":"markdown","13193d19":"markdown","03f45bbe":"markdown","e3bdde78":"markdown","4206a20d":"markdown","816c9daf":"markdown","7685019c":"markdown","50bcb219":"markdown","2c944811":"markdown","a38df55a":"markdown","4dbcafb0":"markdown","d9c8b0ec":"markdown","2e243c32":"markdown","ef7097a9":"markdown","2c8f99e5":"markdown","1e02124c":"markdown","411b4165":"markdown","9eb2e922":"markdown","f2766b4f":"markdown"},"source":{"2269333c":"# Libraries\nimport numpy as np\nimport pandas as pd\npd.set_option('max_columns', None)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport datetime\nimport lightgbm as lgb\nfrom scipy import stats\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, KFold, cross_val_score, GridSearchCV, RepeatedStratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nimport os\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn import model_selection\nfrom sklearn.metrics import accuracy_score, roc_auc_score\nimport json\nimport ast\nimport time\nfrom sklearn import linear_model\nimport eli5\nfrom eli5.sklearn import PermutationImportance\nimport shap\n\nfrom mlxtend.feature_selection import SequentialFeatureSelector as SFS\nfrom mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.feature_selection import GenericUnivariateSelect, SelectPercentile, SelectKBest, f_classif, mutual_info_classif, RFE\nimport statsmodels.api as sm\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom catboost import CatBoostClassifier","c8060d28":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\ntrain.shape","9ade99b3":"train.head()","320529ed":"train[train.columns[2:]].std().plot('hist');\nplt.title('Distribution of stds of all columns');","ab2acfe3":"train[train.columns[2:]].mean().plot('hist');\nplt.title('Distribution of means of all columns');","0db49b5d":"# we have no missing values\ntrain.isnull().any().any()","7c073b0f":"print('Distributions of first 28 columns')\nplt.figure(figsize=(26, 24))\nfor i, col in enumerate(list(train.columns)[2:30]):\n    plt.subplot(7, 4, i + 1)\n    plt.hist(train[col])\n    plt.title(col)","1647ed00":"train['target'].value_counts()","9d3da498":"corrs = train.corr().abs().unstack().sort_values(kind=\"quicksort\").reset_index()\ncorrs = corrs[corrs['level_0'] != corrs['level_1']]\ncorrs.tail(10)","bd7761ab":"X_train = train.drop(['id', 'target'], axis=1)\ny_train = train['target']\nX_test = test.drop(['id'], axis=1)\nn_fold = 20\nfolds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=42)\nrepeated_folds = RepeatedStratifiedKFold(n_splits=20, n_repeats=20, random_state=42)\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","6f1f8ba0":"def train_model(X, X_test, y, params, folds=folds, model_type='lgb', plot_feature_importance=False, averaging='usual', model=None):\n    oof = np.zeros(len(X))\n    prediction = np.zeros(len(X_test))\n    scores = []\n    feature_importance = pd.DataFrame()\n    for fold_n, (train_index, valid_index) in enumerate(folds.split(X, y)):\n        # print('Fold', fold_n, 'started at', time.ctime())\n        X_train, X_valid = X[train_index], X[valid_index]\n        y_train, y_valid = y[train_index], y[valid_index]\n        \n        if model_type == 'lgb':\n            train_data = lgb.Dataset(X_train, label=y_train)\n            valid_data = lgb.Dataset(X_valid, label=y_valid)\n            \n            model = lgb.train(params,\n                    train_data,\n                    num_boost_round=2000,\n                    valid_sets = [train_data, valid_data],\n                    verbose_eval=500,\n                    early_stopping_rounds = 200)\n            \n            y_pred_valid = model.predict(X_valid)\n            y_pred = model.predict(X_test, num_iteration=model.best_iteration_)\n            \n        if model_type == 'xgb':\n            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X_tr.columns)\n            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X_tr.columns)\n\n            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n            model = xgb.train(dtrain=train_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=200, verbose_eval=500, params=params)\n            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X_tr.columns), ntree_limit=model.best_ntree_limit)\n            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X_tr.columns), ntree_limit=model.best_ntree_limit)\n        \n        if model_type == 'cat':\n            model = CatBoostClassifier(iterations=20000,  eval_metric='AUC', **params)\n            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)\n\n            y_pred_valid = model.predict(X_valid)\n            y_pred = model.predict(X_test)\n            \n            \n        if model_type == 'sklearn':\n            model = model\n            model.fit(X_train, y_train)\n            y_pred_valid = model.predict(X_valid).reshape(-1,)\n            score = roc_auc_score(y_valid, y_pred_valid)\n            # print(f'Fold {fold_n}. AUC: {score:.4f}.')\n            # print('')\n            \n            y_pred = model.predict_proba(X_test)[:, 1]\n            \n        if model_type == 'glm':\n            model = sm.GLM(y_train, X_train, family=sm.families.Binomial())\n            model_results = model.fit()\n            model_results.predict(X_test)\n            y_pred_valid = model_results.predict(X_valid).reshape(-1,)\n            score = roc_auc_score(y_valid, y_pred_valid)\n            \n            y_pred = model_results.predict(X_test)\n        \n        oof[valid_index] = y_pred_valid.reshape(-1,)\n        scores.append(roc_auc_score(y_valid, y_pred_valid))\n\n        if averaging == 'usual':\n            prediction += y_pred\n        elif averaging == 'rank':\n            prediction += pd.Series(y_pred).rank().values  \n        \n        if model_type == 'lgb':\n            # feature importance\n            fold_importance = pd.DataFrame()\n            fold_importance[\"feature\"] = X.columns\n            fold_importance[\"importance\"] = model.feature_importances_\n            fold_importance[\"fold\"] = fold_n + 1\n            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n\n    prediction \/= n_fold\n    \n    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n    \n    if model_type == 'lgb':\n        feature_importance[\"importance\"] \/= n_fold\n        if plot_feature_importance:\n            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n                by=\"importance\", ascending=False)[:50].index\n\n            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n\n            plt.figure(figsize=(16, 12));\n            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n            plt.title('LGB Features (avg over folds)');\n        \n            return oof, prediction, feature_importance\n        return oof, prediction, scores\n    \n    else:\n        return oof, prediction, scores","d0de20f9":"# A lot of people are using logreg currently, let's try\nmodel = linear_model.LogisticRegression(class_weight='balanced', penalty='l1', C=0.1, solver='liblinear')\noof_lr, prediction_lr, scores = train_model(X_train, X_test, y_train, params=None, model_type='sklearn', model=model)","2e39db60":"# A lot of people are using logreg currently, let's try\ncat_params = {'learning_rate': 0.02,\n              'depth': 5,\n              'l2_leaf_reg': 10,\n              'bootstrap_type': 'Bernoulli',\n              #'metric_period': 500,\n              'od_type': 'Iter',\n              'od_wait': 50,\n              'random_seed': 11,\n              'allow_writing_files': False}\noof_lr, prediction_lr, scores = train_model(X_train, X_test, y_train, params=cat_params, model_type='cat')","4f56aef1":"model = CatBoostClassifier(iterations=400,  eval_metric='AUC', **cat_params)\nmodel.fit(X_train, y_train, cat_features=[], use_best_model=True, verbose=200)","f5eb27f3":"model = linear_model.LogisticRegression(class_weight='balanced', penalty='l1', C=0.1, solver='liblinear')\noof_lr, prediction_lr_repeated, scores = train_model(X_train, X_test, y_train, params=None, model_type='sklearn', model=model, folds=repeated_folds)","9b1386d0":"eli5.show_weights(model, top=50)","caccfc8f":"(model.coef_ != 0).sum()","a895401a":"top_features = [i[1:] for i in eli5.formatters.as_dataframe.explain_weights_df(model).feature if 'BIAS' not in i]\nX_train = train[top_features]\nX_test = test[top_features]\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","b1f70b8e":"model = linear_model.LogisticRegression(class_weight='balanced', penalty='l1', C=0.1, solver='liblinear')\noof_lr, prediction_lr, _ = train_model(X_train, X_test, y_train, params=None, model_type='sklearn', model=model)","5dd1b675":"perm = PermutationImportance(model, random_state=1).fit(X_train, y_train)\neli5.show_weights(perm, top=50)","da00f109":"top_features = [i[1:] for i in eli5.formatters.as_dataframe.explain_weights_df(perm).feature if 'BIAS' not in i]\nX_train = train[top_features]\nX_test = test[top_features]\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","19576308":"model = linear_model.LogisticRegression(class_weight='balanced', penalty='l1', C=0.1, solver='liblinear')\noof_lr1, prediction_lr1, _ = train_model(X_train, X_test, y_train, params=None, model_type='sklearn', model=model)","af968ef4":"X_train = train.drop(['id', 'target'], axis=1)\ny_train = train['target']\nX_test = test.drop(['id'], axis=1)\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\nmodel = linear_model.LogisticRegression(class_weight='balanced', penalty='l1', C=0.1, solver='liblinear')\noof_lr, prediction_lr, _ = train_model(X_train, X_test, y_train, params=None, model_type='sklearn', model=model)","9c163490":"explainer = shap.LinearExplainer(model, X_train)\nshap_values = explainer.shap_values(X_train)\n\nshap.summary_plot(shap_values, X_train)","54076c3e":"sfs1 = SFS(model, \n           k_features=(10, 15), \n           forward=True, \n           floating=False, \n           verbose=0,\n           scoring='roc_auc',\n           cv=folds,\n          n_jobs=-1)\n\nsfs1 = sfs1.fit(X_train, y_train)","da3daecd":"fig1 = plot_sfs(sfs1.get_metric_dict(), kind='std_dev')\n\nplt.ylim([0.8, 1])\nplt.title('Sequential Forward Selection (w. StdDev)')\nplt.grid()\nplt.show()","50feb8e3":"top_features = list(sfs1.k_feature_names_)\nX_train = train[top_features]\nX_test = test[top_features]\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\nmodel = linear_model.LogisticRegression(class_weight='balanced', penalty='l1', C=0.1, solver='liblinear')\noof_lr, prediction_lr, _ = train_model(X_train, X_test, y_train, params=None, model_type='sklearn', model=model)","b16a38a8":"X_train = train.drop(['id', 'target'], axis=1)\ny_train = train['target']\nX_test = test.drop(['id'], axis=1)\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","c8666a9b":"lr = linear_model.LogisticRegression(solver='liblinear', max_iter=1000)\n\nparameter_grid = {'class_weight' : ['balanced', None],\n                  'penalty' : ['l2'],\n                  'C' : [0.001, 0.01, 0.1, 1.0, 10.0, 100.0],\n                  'solver': ['newton-cg', 'sag', 'lbfgs']\n                 }\n\ngrid_search = GridSearchCV(lr, param_grid=parameter_grid, cv=folds, scoring='roc_auc')\ngrid_search.fit(X_train, y_train)\nprint('Best score: {}'.format(grid_search.best_score_))\nprint('Best parameters: {}'.format(grid_search.best_params_))","11c6ef30":"lr = linear_model.LogisticRegression(solver='liblinear', max_iter=10000)\n\nparameter_grid = {'class_weight' : ['balanced', None],\n                  'penalty' : ['l2', 'l1'],\n                  'C' : [0.001, 0.01, 0.08, 0.1, 0.15, 1.0, 10.0, 100.0],\n                 }\n\ngrid_search = GridSearchCV(lr, param_grid=parameter_grid, cv=folds, scoring='roc_auc')\ngrid_search.fit(X_train, y_train)\nprint('Best score: {}'.format(grid_search.best_score_))\nprint('Best parameters: {}'.format(grid_search.best_params_))","67d7cc1a":"model = linear_model.LogisticRegression(class_weight='balanced', penalty='l1', C=0.1, solver='liblinear')\noof_lr, prediction_lr, scores = train_model(X_train, X_test, y_train, params=None, model_type='sklearn', model=model)","51524709":"from sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()\noof_gnb, prediction_gnb, scores_gnb = train_model(X_train, X_test, y_train, params=None, model_type='sklearn', model=gnb)","928645a5":"from sklearn.ensemble import AdaBoostClassifier\nabc = AdaBoostClassifier()\n\nparameter_grid = {'n_estimators': [5, 10, 20, 50, 100],\n                  'learning_rate': [0.001, 0.01, 0.1, 1.0, 10.0]\n                 }\n\ngrid_search = GridSearchCV(abc, param_grid=parameter_grid, cv=folds, scoring='roc_auc')\ngrid_search.fit(X_train, y_train)\nprint('Best score: {}'.format(grid_search.best_score_))\nprint('Best parameters: {}'.format(grid_search.best_params_))","3463d57d":"abc = AdaBoostClassifier(**grid_search.best_params_)\noof_abc, prediction_abc, scores_abc = train_model(X_train, X_test, y_train, params=None, model_type='sklearn', model=abc)","c9177bd9":"from sklearn.ensemble import ExtraTreesClassifier\netc = ExtraTreesClassifier()\n\nparameter_grid = {'n_estimators': [10, 50, 100, 1000],\n                  'max_depth': [None, 3, 5, 15]\n                 }\n\ngrid_search = GridSearchCV(etc, param_grid=parameter_grid, cv=folds, scoring='roc_auc', n_jobs=-1)\ngrid_search.fit(X_train, y_train)\nprint('Best score: {}'.format(grid_search.best_score_))\nprint('Best parameters: {}'.format(grid_search.best_params_))\netc = ExtraTreesClassifier(**grid_search.best_params_)\noof_etc, prediction_etc, scores_etc = train_model(X_train, X_test, y_train, params=None, model_type='sklearn', model=etc)","f2b1dde6":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier()\n\nparameter_grid = {'n_estimators': [10, 50, 100, 1000],\n                  'max_depth': [None, 3, 5, 15]\n                 }\n\ngrid_search = GridSearchCV(rfc, param_grid=parameter_grid, cv=folds, scoring='roc_auc', n_jobs=-1)\ngrid_search.fit(X_train, y_train)\nprint('Best score: {}'.format(grid_search.best_score_))\nprint('Best parameters: {}'.format(grid_search.best_params_))\nrfc = RandomForestClassifier(**grid_search.best_params_)\noof_rfc, prediction_rfc, scores_rfc = train_model(X_train, X_test, y_train, params=None, model_type='sklearn', model=rfc)","c373d4de":"from sklearn.gaussian_process import GaussianProcessClassifier\ngpc = GaussianProcessClassifier()\noof_gpc, prediction_gpc, scores_gpc = train_model(X_train, X_test, y_train, params=None, model_type='sklearn', model=gpc)","45fe4ef3":"from sklearn.svm import SVC\nsvc = SVC(probability=True, gamma='scale')\n\nparameter_grid = {'C': [0.001, 0.01, 0.1, 1.0, 10.0],\n                  'kernel': ['linear', 'poly', 'rbf'],\n                 }\n\ngrid_search = GridSearchCV(svc, param_grid=parameter_grid, cv=folds, scoring='roc_auc', n_jobs=-1)\ngrid_search.fit(X_train, y_train)\nprint('Best score: {}'.format(grid_search.best_score_))\nprint('Best parameters: {}'.format(grid_search.best_params_))\nsvc = SVC(probability=True, gamma='scale', **grid_search.best_params_)\noof_svc, prediction_svc, scores_svc = train_model(X_train, X_test, y_train, params=None, model_type='sklearn', model=svc)","d5c1664f":"from sklearn.neighbors import KNeighborsClassifier\nknc = KNeighborsClassifier()\n\nparameter_grid = {'n_neighbors': [2, 3, 5, 10, 20],\n                  'weights': ['uniform', 'distance'],\n                  'leaf_size': [5, 10, 30]\n                 }\n\ngrid_search = GridSearchCV(knc, param_grid=parameter_grid, cv=folds, scoring='roc_auc', n_jobs=-1)\ngrid_search.fit(X_train, y_train)\nprint('Best score: {}'.format(grid_search.best_score_))\nprint('Best parameters: {}'.format(grid_search.best_params_))\nknc = KNeighborsClassifier(**grid_search.best_params_)\noof_knc, prediction_knc, scores_knc = train_model(X_train, X_test, y_train, params=None, model_type='sklearn', model=knc)","fff62ca7":"from sklearn.naive_bayes import BernoulliNB\nbnb = BernoulliNB()\n\nparameter_grid = {'alpha': [0.0001, 1, 2, 10]\n                 }\n\ngrid_search = GridSearchCV(bnb, param_grid=parameter_grid, cv=folds, scoring='roc_auc', n_jobs=-1)\ngrid_search.fit(X_train, y_train)\nprint('Best score: {}'.format(grid_search.best_score_))\nprint('Best parameters: {}'.format(grid_search.best_params_))\nbnb = BernoulliNB(**grid_search.best_params_)\noof_bnb, prediction_bnb, scores_bnb = train_model(X_train, X_test, y_train, params=None, model_type='sklearn', model=bnb)","a0134fa9":"sgd = linear_model.SGDClassifier(eta0=1, max_iter=1000, tol=0.0001)\n\nparameter_grid = {'loss': ['log', 'modified_huber'],\n                  'penalty': ['l1', 'l2', 'elasticnet'],\n                  'alpha': [0.001, 0.01],\n                  'l1_ratio': [0, 0.15, 0.5, 1.0],\n                  'learning_rate': ['optimal', 'invscaling', 'adaptive']\n                 }\n\ngrid_search = GridSearchCV(sgd, param_grid=parameter_grid, cv=folds, scoring='roc_auc', n_jobs=-1)\ngrid_search.fit(X_train, y_train)\nprint('Best score: {}'.format(grid_search.best_score_))\nprint('Best parameters: {}'.format(grid_search.best_params_))\nsgd = linear_model.SGDClassifier(eta0=1, tol=0.0001, **grid_search.best_params_)\noof_sgd, prediction_sgd, scores_sgd = train_model(X_train, X_test, y_train, params=None, model_type='sklearn', model=sgd)","15bd4697":"plt.figure(figsize=(12, 8));\nscores_df = pd.DataFrame({'LogisticRegression': scores})\nscores_df['GaussianNB'] = scores_gnb\nscores_df['AdaBoostClassifier'] = scores_abc\nscores_df['ExtraTreesClassifier'] = scores_etc\nscores_df['GaussianProcessClassifier'] = scores_gpc\nscores_df['SVC'] = scores_svc\nscores_df['KNeighborsClassifier'] = scores_knc\nscores_df['BernoulliNB'] = scores_bnb\nscores_df['SGDClassifier'] = scores_sgd\nscores_df['RandomForestClassifier'] = scores_rfc\n\nsns.boxplot(data=scores_df);\nplt.xticks(rotation=45);","e2c3d354":"X_train = train.drop(['id', 'target'], axis=1)\ny_train = train['target']\nX_test = test.drop(['id'], axis=1)\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\nmodel = linear_model.LogisticRegression(class_weight='balanced', penalty='l1', C=0.1, solver='liblinear')\noof_lr, prediction_lr, _ = train_model(X_train, X_test, y_train, params=None, model_type='sklearn', model=model)","71362b1e":"# submission = pd.read_csv('..\/input\/sample_submission.csv')\n# submission['target'] = (prediction_lr + prediction_svc) \/ 2\n# submission.to_csv('submission.csv', index=False)\n\n# submission.head()","4c8af7f9":"plt.hist(prediction_lr, label='logreg');\nplt.hist(prediction_svc, label='svc');\nplt.hist((prediction_lr + prediction_svc) \/ 2, label='blend');\nplt.title('Distribution of out of fold predictions');\nplt.legend();","bb2b84b2":"from sklearn.preprocessing import PolynomialFeatures\npoly = PolynomialFeatures(2)\n\nX_train = train.drop(['id', 'target'], axis=1)\nX_test = test.drop(['id'], axis=1)\n\nX_train_poly = poly.fit_transform(X_train)\nX_test_poly = poly.transform(X_test)","e6f55f35":"cor = pd.DataFrame(X_train_poly).corrwith(y_train)","168c5f2c":"sc = []\nfor i in range(10, 510, 5):\n    top_corr_cols = list(cor.abs().sort_values().tail(i).reset_index()['index'].values)\n    X_train_poly1 = X_train_poly[:, top_corr_cols]\n    X_test_poly1 = X_test_poly[:, top_corr_cols]\n    oof_lr_poly, prediction_lr_poly, scores = train_model(X_train_poly1, X_test_poly1, y_train, params=None, model_type='sklearn', model=model)\n    sc.append(scores)","f0b8dd9d":"data = [go.Scatter(\n        x = list(range(10, 510, 5)),\n        y = [np.round(np.mean(i), 4) for i in sc],\n        name = 'CV scores'\n    )]\nlayout = go.Layout(dict(title = \"Top N poly features vs CV\",\n                  xaxis = dict(title = 'Top N features'),\n                  yaxis = dict(title = 'CV score'),\n                  ))\npy.iplot(dict(data=data, layout=layout), filename='basic-line')","9ffe83bd":"top_corr_cols = list(cor.abs().sort_values().tail(300).reset_index()['index'].values)\nX_train_poly1 = X_train_poly[:, top_corr_cols]\nX_test_poly1 = X_test_poly[:, top_corr_cols]","a993ed4a":"model = linear_model.LogisticRegression(class_weight='balanced', penalty='l1', C=0.1, solver='liblinear')\noof_lr_poly, prediction_lr_poly, scores = train_model(X_train_poly1, X_test_poly1, y_train, params=None, model_type='sklearn', model=model)","06a819b6":"submission = pd.read_csv('..\/input\/sample_submission.csv')\nsubmission['target'] = prediction_lr_poly\n# submission.to_csv('submission_poly.csv', index=False)\n\nsubmission.head()","27a96608":"X_train = train.drop(['id', 'target'], axis=1)\nX_test = test.drop(['id'], axis=1)\nX_train['300'] = X_train.std(1)\nX_test['300'] = X_test.std(1)\nscaler = StandardScaler()\nX_train[X_train.columns[:-1]] = scaler.fit_transform(X_train[X_train.columns[:-1]])\nX_test[X_train.columns[:-1]] = scaler.transform(X_test[X_train.columns[:-1]])\nmodel = linear_model.LogisticRegression(class_weight='balanced', penalty='l1', C=0.1, solver='liblinear')\noof_lr_1, prediction_lr_1, scores = train_model(X_train.values, X_test.values, y_train, params=None, model_type='sklearn', model=model)","b8e8fd84":"X_train = train.drop(['id', 'target'], axis=1)\nX_test = test.drop(['id'], axis=1)\nX_train['300'] = X_train.std(1)\nX_test['300'] = X_test.std(1)\nscaler = StandardScaler()\nX_train[X_train.columns[:-1]] = scaler.fit_transform(X_train[X_train.columns[:-1]])\nX_test[X_train.columns[:-1]] = scaler.transform(X_test[X_train.columns[:-1]])\nmodel = linear_model.LogisticRegression(class_weight='balanced', penalty='l1', C=0.1, solver='liblinear')\noof_lr_1, prediction_lr_1_repeated, scores = train_model(X_train.values, X_test.values, y_train, params=None, model_type='sklearn', model=model, folds=repeated_folds)\nsubmission = pd.read_csv('..\/input\/sample_submission.csv')\nsubmission['target'] = prediction_lr_1_repeated\nsubmission.to_csv('repeated_fold_features.csv', index=False)\n\nsubmission.head()","a3d0fced":"X_train = train.drop(['id', 'target'], axis=1)\nX_test = test.drop(['id'], axis=1)\nmain_cols = X_train.columns.tolist()","aa0a57a9":"neigh = NearestNeighbors(5, n_jobs=-1)\nneigh.fit(X_train)\n\ndists, _ = neigh.kneighbors(X_train, n_neighbors=5)\nmean_dist = dists.mean(axis=1)\nmax_dist = dists.max(axis=1)\nmin_dist = dists.min(axis=1)\n\nX_train['300'] = X_train.std(1)\nX_train = np.hstack((X_train, mean_dist.reshape(-1, 1), max_dist.reshape(-1, 1), min_dist.reshape(-1, 1)))\n\ntest_dists, _ = neigh.kneighbors(X_test, n_neighbors=5)\n\ntest_mean_dist = test_dists.mean(axis=1)\ntest_max_dist = test_dists.max(axis=1)\ntest_min_dist = test_dists.min(axis=1)\n\nX_test['300'] = X_test.std(1)\nX_test = np.hstack((X_test, test_mean_dist.reshape(-1, 1), test_max_dist.reshape(-1, 1), test_min_dist.reshape(-1, 1)))","c55ee4a7":"model = linear_model.LogisticRegression(class_weight='balanced', penalty='l1', C=0.1, solver='liblinear')\noof_lr_2, prediction_lr_2, scores = train_model(X_train, X_test, y_train, params=None, model_type='sklearn', model=model)\nsubmission['target'] = prediction_lr_2\nsubmission.to_csv('nn_features.csv', index=False)","1013b861":"submission = pd.read_csv('..\/input\/sample_submission.csv')\nsubmission['target'] = (prediction_lr_1 + prediction_lr_2) \/ 2\n# submission.to_csv('blend.csv', index=False)\n\nsubmission.head()","e98147fe":"# baseline score\nX_train = train.drop(['id', 'target'], axis=1)\nX_test = test.drop(['id'], axis=1)\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\nmodel = linear_model.LogisticRegression(class_weight='balanced', penalty='l1', C=0.1, solver='liblinear')\noof_lr_1, prediction_lr_1, scores = train_model(X_train, X_test, y_train, params=None, model_type='sklearn', model=model)","0ee62b7e":"scores_dict = {'f_classif': [], 'mutual_info_classif': []}\nfor i in range(5, 100, 5):\n    s1 = SelectPercentile(f_classif, percentile=i)\n    X_train1 = s1.fit_transform(X_train, y_train.values.astype(int))\n    X_test1 = s1.transform(X_test)\n    oof_lr_1, prediction_lr_1, scores = train_model(X_train1, X_test1, y_train, params=None, model_type='sklearn', model=model)\n    scores_dict['f_classif'].append(np.mean(scores))\n    \n    s2 = SelectPercentile(mutual_info_classif, percentile=i)\n    X_train1 = s2.fit_transform(X_train, y_train.values.astype(int))\n    X_test1 = s2.transform(X_test)\n    oof_lr_1, prediction_lr_1, scores = train_model(X_train1, X_test1, y_train, params=None, model_type='sklearn', model=model)\n    scores_dict['mutual_info_classif'].append(np.mean(scores))","e1d155d2":"data = [go.Scatter(\n        x = list(range(5, 100, 5)),\n        y = scores_dict['f_classif'],\n        name = 'CV scores f_classif'\n    ), go.Scatter(\n        x = list(range(5, 100, 5)),\n        y = scores_dict['mutual_info_classif'],\n        name = 'CV scores mutual_info_classif')]\nlayout = go.Layout(dict(title = \"Top N features by percentile vs CV\",\n                  xaxis = dict(title = 'Top N features by percentile'),\n                  yaxis = dict(title = 'CV score'),\n                  ))\npy.iplot(dict(data=data, layout=layout), filename='basic-line')","c9b90024":"scores_dict = {'f_classif': [], 'mutual_info_classif': []}\nfor i in range(10, 301, 10):\n    s1 = SelectKBest(f_classif, k=i)\n    X_train1 = s1.fit_transform(X_train, y_train.values.astype(int))\n    X_test1 = s1.transform(X_test)\n    oof_lr_1, prediction_lr_1, scores = train_model(X_train1, X_test1, y_train, params=None, model_type='sklearn', model=model)\n    scores_dict['f_classif'].append(np.mean(scores))\n    \n    s2 = SelectKBest(mutual_info_classif, k=i)\n    X_train1 = s2.fit_transform(X_train, y_train.values.astype(int))\n    X_test1 = s2.transform(X_test)\n    oof_lr_1, prediction_lr_1, scores = train_model(X_train1, X_test1, y_train, params=None, model_type='sklearn', model=model)\n    scores_dict['mutual_info_classif'].append(np.mean(scores))","dbd6fd08":"data = [go.Scatter(\n        x = list(range(10, 301, 10)),\n        y = scores_dict['f_classif'],\n        name = 'CV scores f_classif'\n    ), go.Scatter(\n        x = list(range(10, 301, 10)),\n        y = scores_dict['mutual_info_classif'],\n        name = 'CV scores mutual_info_classif')]\nlayout = go.Layout(dict(title = \"Top N features by SelectKBest vs CV\",\n                  xaxis = dict(title = 'Top N features by SelectKBest'),\n                  yaxis = dict(title = 'CV score'),\n                  ))\npy.iplot(dict(data=data, layout=layout), filename='basic-line')","f289625a":"selector = SelectKBest(f_classif, k=60)\nX_trainK = selector.fit_transform(X_train, y_train.values.astype(int))\nX_testK = selector.transform(X_test)\noof_lr_1, prediction_lr_1, scores = train_model(X_trainK, X_testK, y_train, params=None, model_type='sklearn', model=model)","0005096a":"submission = pd.read_csv('..\/input\/sample_submission.csv')\nsubmission['target'] = prediction_lr_1\n# submission.to_csv('top_n_features.csv', index=False)\n\nsubmission.head()","77875c78":"scores_list = []\nfor i in range(10, 301, 5):\n    s = RFE(model, i, step=1)\n    X_train1 = s.fit_transform(X_train, y_train.values.astype(int))\n    X_test1 = s.transform(X_test)\n    oof_lr_1, prediction_lr_1, scores = train_model(X_train1, X_test1, y_train, params=None, model_type='sklearn', model=model)\n    scores_list.append(np.mean(scores))","d908cf0e":"data = [go.Scatter(\n        x = list(range(10, 301, 5)),\n        y = scores_list,\n        name = 'CV scores RFE'\n    )]\nlayout = go.Layout(dict(title = \"Top N features by RFE vs CV\",\n                  xaxis = dict(title = 'Top N features by RFE'),\n                  yaxis = dict(title = 'CV score'),\n                  ))\npy.iplot(dict(data=data, layout=layout), filename='basic-line')","345bd483":"selector = RFE(model, 20, step=1)\nX_trainK = selector.fit_transform(X_train, y_train.values.astype(int))\nX_testK = selector.transform(X_test)\noof_lr_1, prediction_lr_rfe_20, scores = train_model(X_trainK, X_testK, y_train, params=None, model_type='sklearn', model=model)","e583aff9":"submission = pd.read_csv('..\/input\/sample_submission.csv')\nsubmission['target'] = prediction_lr_rfe_20\nsubmission.to_csv('rfe_20.csv', index=False)\n\nsubmission.head()","d633287f":"selector = SelectKBest(f_classif, k=15)\nX_trainK = selector.fit_transform(X_train, y_train.values.astype(int))\nX_testK = selector.transform(X_test)","6387e0d0":"oof_glm, prediction_glm, scores = train_model(X_trainK, X_testK, y_train, params=None, model_type='glm')","b494cdb9":"submission = pd.read_csv('..\/input\/sample_submission.csv')\nsubmission['target'] = prediction_glm\nsubmission.to_csv('glm.csv', index=False)\n\nsubmission.head()","dd4fd103":"eli5_weights = eli5.formatters.as_dataframe.explain_weights_df(model)\neli5_weights['weight'] = eli5_weights['weight'].abs()\neli5_weights = eli5_weights.sort_values('weight', ascending=False)\neli5_weights","48fd2094":"train['mean'] = train.mean(1)\ntrain['std'] = train.std(1)\ntest['mean'] = test.mean(1)\ntest['std'] = test.std(1)\n\nscores_dict = {'simple': [], 'with_std': [], 'with_mean': []}\nfor i in range(1, eli5_weights.shape[0] + 1):\n    top_features = [i[1:] for i in eli5_weights.feature if 'BIAS' not in i][:i]\n    \n    X_train = train[top_features]\n    X_test = test[top_features]\n    scaler = StandardScaler()\n    X_train = scaler.fit_transform(X_train)\n    X_test = scaler.transform(X_test)\n    oof_lr, prediction_lr, scores = train_model(X_train, X_test, y_train, params=None, model_type='sklearn', model=model)\n    scores_dict['simple'].append(np.mean(scores))\n    \n    X_train = train[top_features + ['mean']]\n    X_test = test[top_features + ['mean']]\n    scaler = StandardScaler()\n    X_train[X_train.columns[:-1]] = scaler.fit_transform(X_train[X_train.columns[:-1]])\n    X_test[X_train.columns[:-1]] = scaler.transform(X_test[X_train.columns[:-1]])\n    oof_lr, prediction_lr, scores = train_model(X_train.values, X_test.values, y_train, params=None, model_type='sklearn', model=model)\n    scores_dict['with_mean'].append(np.mean(scores))\n    \n    X_train = train[top_features + ['std']]\n    X_test = test[top_features + ['std']]\n    scaler = StandardScaler()\n    X_train[X_train.columns[:-1]] = scaler.fit_transform(X_train[X_train.columns[:-1]])\n    X_test[X_train.columns[:-1]] = scaler.transform(X_test[X_train.columns[:-1]])\n    oof_lr, prediction_lr, scores = train_model(X_train.values, X_test.values, y_train, params=None, model_type='sklearn', model=model)\n    scores_dict['with_std'].append(np.mean(scores))","4fb6771a":"data = [go.Scatter(\n        x = list(range(1, eli5_weights.shape[0] + 1)),\n        y = scores_dict['simple'],\n        name = 'Simple CV scores'\n    ), go.Scatter(\n        x = list(range(1, eli5_weights.shape[0] + 1)),\n        y = scores_dict['with_mean'],\n        name = 'With mean CV scores'\n    ), go.Scatter(\n        x = list(range(1, eli5_weights.shape[0] + 1)),\n        y = scores_dict['with_std'],\n        name = 'With std CV scores'\n    )]\nlayout = go.Layout(dict(title = \"Top N features vs CV\",\n                  xaxis = dict(title = 'Top N features'),\n                  yaxis = dict(title = 'CV score'),\n                  ))\npy.iplot(dict(data=data, layout=layout), filename='basic-line')","3c83eda4":"train['mean'] = train.mean(1)\ntrain['std'] = train.std(1)\ntest['mean'] = test.mean(1)\ntest['std'] = test.std(1)\n\nscores_dict = {'simple': [], 'with_std': [], 'with_mean': []}\nfor i in range(1, eli5_weights.shape[0] + 1):\n    top_features = [i[1:] for i in eli5_weights.feature if 'BIAS' not in i][:i]\n    \n    X_train = train[top_features]\n    X_test = test[top_features]\n    scaler = StandardScaler()\n    X_train = scaler.fit_transform(X_train)\n    X_test = scaler.transform(X_test)\n    oof_lr, prediction_lr, scores = train_model(X_train, X_test, y_train, params=None, model_type='sklearn', model=model, folds=repeated_folds)\n    scores_dict['simple'].append(np.mean(scores))\n    \n    X_train = train[top_features + ['mean']]\n    X_test = test[top_features + ['mean']]\n    scaler = StandardScaler()\n    X_train[X_train.columns[:-1]] = scaler.fit_transform(X_train[X_train.columns[:-1]])\n    X_test[X_train.columns[:-1]] = scaler.transform(X_test[X_train.columns[:-1]])\n    oof_lr, prediction_lr, scores = train_model(X_train.values, X_test.values, y_train, params=None, model_type='sklearn', model=model, folds=repeated_folds)\n    scores_dict['with_mean'].append(np.mean(scores))\n    \n    X_train = train[top_features + ['std']]\n    X_test = test[top_features + ['std']]\n    scaler = StandardScaler()\n    X_train[X_train.columns[:-1]] = scaler.fit_transform(X_train[X_train.columns[:-1]])\n    X_test[X_train.columns[:-1]] = scaler.transform(X_test[X_train.columns[:-1]])\n    oof_lr, prediction_lr, scores = train_model(X_train.values, X_test.values, y_train, params=None, model_type='sklearn', model=model, folds=repeated_folds)\n    scores_dict['with_std'].append(np.mean(scores))","9541b077":"data = [go.Scatter(\n        x = list(range(1, eli5_weights.shape[0] + 1)),\n        y = scores_dict['simple'],\n        name = 'Simple CV scores'\n    ), go.Scatter(\n        x = list(range(1, eli5_weights.shape[0] + 1)),\n        y = scores_dict['with_mean'],\n        name = 'With mean CV scores'\n    ), go.Scatter(\n        x = list(range(1, eli5_weights.shape[0] + 1)),\n        y = scores_dict['with_std'],\n        name = 'With std CV scores'\n    )]\nlayout = go.Layout(dict(title = \"Top N features vs repeated folds CV\",\n                  xaxis = dict(title = 'Top N features'),\n                  yaxis = dict(title = 'CV score'),\n                  ))\npy.iplot(dict(data=data, layout=layout), filename='basic-line')","2caa6d78":"top_features = [i[1:] for i in eli5_weights.feature if 'BIAS' not in i][:8]\n\nX_train = train[top_features + ['mean']]\nX_test = test[top_features + ['mean']]\nscaler = StandardScaler()\nX_train[X_train.columns[:-1]] = scaler.fit_transform(X_train[X_train.columns[:-1]])\nX_test[X_train.columns[:-1]] = scaler.transform(X_test[X_train.columns[:-1]])\noof_lr, prediction_lr, scores = train_model(X_train.values, X_test.values, y_train, params=None, model_type='sklearn', model=model, folds=repeated_folds)\n\nsubmission = pd.read_csv('..\/input\/sample_submission.csv')\nsubmission['target'] = prediction_lr\nsubmission.to_csv('submission_top8.csv', index=False)\n\nsubmission.head()","23f33807":"top_features = [i[1:] for i in eli5_weights.feature if 'BIAS' not in i][:10]\n\nX_train = train[top_features + ['mean']]\nX_test = test[top_features + ['mean']]\nscaler = StandardScaler()\nX_train[X_train.columns[:-1]] = scaler.fit_transform(X_train[X_train.columns[:-1]])\nX_test[X_train.columns[:-1]] = scaler.transform(X_test[X_train.columns[:-1]])\noof_lr, prediction_lr, scores = train_model(X_train.values, X_test.values, y_train, params=None, model_type='sklearn', model=model, folds=repeated_folds)\n\nsubmission = pd.read_csv('..\/input\/sample_submission.csv')\nsubmission['target'] = prediction_lr\nsubmission.to_csv('submission_top10.csv', index=False)\n\nsubmission.head()","6d9203eb":"<a id=\"shap\"><\/a>\n## SHAP\n\nAnother interesting tool is SHAP. It also provides explanations for a variety of models.","fbb665d1":"## General information\n\nIn Don't Overfit! II competition we have a binary classification task. 300 columns, 250 training samples and 79 times more samples in test data! We need to be able to build a model without overfitting.\n\nIn this kernel I'll write the following things:\n\n* EDA on the features and trying to get some insights;\n* Using permutation importance to select most impactful features;\n* Comparing various models: bayer classification, linear models, tree based models;\n* Trying various approaches to feature selection including taking top features from eli5 and shap;\n* Hyperparameter optimization for models;\n* Feature generation;\n* Other things;\n\n![](https:\/\/cdn-images-1.medium.com\/max\/1600\/1*vuZxFMi5fODz2OEcpG-S1g.png)","cd1af0fd":"We can see that There are several features with highly positive weights and more features with negative weights. In fact there are only 32 features, which are important according to ELI5. It is worth noticing though, that the model itself had 34 non-zero features, so ELI5 dropped only 2 features.. Let's try using only them for the submission!","778b9c0a":"<a id=\"bm\"><\/a>\n## Basic modelling","f97c6889":"<a id=\"model\"><\/a>\n## Modelling","cacde8cc":"Let's compare with repeated KFold.","9390b1c5":"Submitting `prediction_lr` gives 0.847 on leaderboard.","f953a2f4":"Sadly blend gives 0.831 on LB. Again no luck.","e93202d3":"From this overview we can see the following things:\n* target is binary and has some disbalance: 36% of samples belong to 0 class;\n* values in columns are more or less similar;\n* columns have std of 1 +\/- 0.1 (min and max values are 0.889, 1.117 respectively);\n* columns have mean of 0 +\/- 0.15 (min and max values are -0.2, 0.1896 respectively);","f95e6f46":"Let's try generating some features!","92e7e0a0":"<a id=\"select\"><\/a>\n## Sklearn feature selection\n\nSklearn has several methods to do feature selection. Let's try some of them!","df3da281":"<a id=\"eli5\"><\/a>\n## ELI5\n\nELI5 is a package with provides explanations for ML models. It can do this not only for linear models, but also for tree based like Random Forest or lightgbm.","5a2698be":"Let's have a look at correlations now!","5de6aba2":"Not suprisingly we overfit.","36b74198":"CV increased a bit!","13193d19":"Content\n\n* [1 Data exploration](#de)\n* [2 Basic modelling](#bm)\n* [3 ELI5](#eli5)\n* [3.1 ELI5 and permutation importance](#eli5p)\n* [4 SHAP](#shap)\n* [5 Mlextend SequentialFeatureSelector](#mlextend)\n* [6 Modelling](#model)\n* [7 Polynomial features](#poly)\n* [8 Adding statistics](#stats)\n* [9 Adding distance features](#dist)\n* [10 Sklearn feature selection](#select)\n* [11 GLM](#glm)\n* [12 Selected top_features + statistics](#selected)","03f45bbe":"Wow, if we select columns by permutation importance, CV score drops significantly. It seems it doesn't work well in out case.","e3bdde78":"Wow, we got improvement from 0.7226 to 0.7486 on CV! But this submission gives 0.845 on leaderboard. So it decreases score slightly. Let's try other things!","4206a20d":"Score became much lower. So this is also a bad idea","816c9daf":"So, parameters for logreg are optimal, let's try other models","7685019c":"We can see that correlations between features are lower that 0.3 and the most correlated feature with target has correlation of 0.37. So we have no highly correlated features which we could drop, on the other hand we could drop some columns with have little correlation with the target.","50bcb219":"<a id=\"mlextend\"><\/a>\n## Mlextend SequentialFeatureSelector","2c944811":"<a id=\"selected\"><\/a>\n## Selected top_features + statistics","a38df55a":"<a id=\"stats\"><\/a>\n## Adding statistics","4dbcafb0":"It could be difficult to interpret this plot when you see it for the first time. It shows how features impact predictions. For example for feature 33 low values have a negative impact on model predictions (zero is more likely), and high values have a positive impace (ones are more likely). Feature 217 has an opposite effect: low values have a positive impact and high values have a negative impact.\n\nBut we will need to select features manually... let's use a library for that!","d9c8b0ec":"The number of polynomial features is ~45k which is too much. We need some way to select some of them. Let's try use correlations with target.","2e243c32":"<a id=\"dglm\"><\/a>\n## GLM","ef7097a9":"<a id=\"dist\"><\/a>\n## Adding distance features","2c8f99e5":"<a id=\"de\"><\/a>\n## Data exploration","1e02124c":"<a id=\"eli5p\"><\/a>\n### Permutation importance\nThere is also another way of using eli5 - we could have a look at permutation importance. It works in the following way:\n* We fit a model;\n* We randomly shuffle one column of validation data and calculate the score;\n* If the score dropped significantly, it means that the feature is important;\n\nYou can read more about this approach here: https:\/\/www.kaggle.com\/dansbecker\/permutation-importance","411b4165":"<a id=\"poly\"><\/a>\n## Polynomial Features","9eb2e922":"And this gives 0.811 on leaderboard. Overfitting! It seems that feature selection isn't the best approach. Let's try building various models!","f2766b4f":"We can see that logistic regression is superior to most other models. Only SVC is comparable. It seems that other models either overfit or can't work on this small dataset.\n\n\nLet's try submitting a blend of them!"}}