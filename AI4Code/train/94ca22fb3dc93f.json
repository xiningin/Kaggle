{"cell_type":{"828ec3e2":"code","c37faf56":"code","e254372f":"code","b34ca2e7":"code","a64fc2e1":"code","77a45c20":"code","c0b175f4":"code","03488af2":"code","fa277619":"code","931daaa4":"code","a1d2163b":"code","13c8883d":"code","66bcb4f5":"code","321546b9":"code","4c906755":"code","00ca896d":"code","21d04c7e":"code","30354a63":"code","5a01b5ca":"markdown","48e2105b":"markdown","6ef7d606":"markdown","13d871a2":"markdown","9c224a64":"markdown","fa7dde8c":"markdown","65e3fe40":"markdown","b844df81":"markdown","8cf5f902":"markdown","6bd6f8ae":"markdown"},"source":{"828ec3e2":"import matplotlib.pyplot as plt\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.metrics.pairwise import cosine_distances as cosine\nfrom catboost import CatBoostClassifier, Pool, cv\n\nimport string\nimport re\n\nimport nltk\nfrom nltk.corpus import stopwords\nimport hunspell\n\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn import metrics\nfrom sklearn.model_selection import KFold","c37faf56":"train = pd.read_csv('..\/input\/tweet-sentiment-extraction\/train.csv')\ntest = pd.read_csv('..\/input\/tweet-sentiment-extraction\/test.csv')\nsample_sub = pd.read_csv('..\/input\/tweet-sentiment-extraction\/sample_submission.csv')\n\ntrain = train.drop(314)\ntrain.index = np.arange(27480)","e254372f":"table_empty = str.maketrans({key: None for key in (string.punctuation + ' ')})\ntable_space = str.maketrans({key: ' ' for key in string.punctuation})\n\nhobj = hunspell.HunSpell('\/usr\/share\/hunspell\/en_US.dic', '\/usr\/share\/hunspell\/en_US.aff')\n\ngood_smile = ['=P', '<3', ':)', '(:', ';)', '=D', ';D', 'xD', ':-D', '=}', '=]',\n              '(=', '^-^', ':-*', ';-)', ':]', ':>', '*-*', '^.^', '^^']\nbad_smile = ['D:', ':(', ':[', '=(', ' :\/ ', '):', 'D:', ')=', '>=[', \":'(\",\n             ':-\/', ':`(', ':-o', ':|', ':-\/', ':@', ':[', ':-|', ':o', '={']","b34ca2e7":"def TextCleaner(text):\n    text = re.sub(r'https?:\\\/\\\/\\S*', 'huperlink', text) #huperlink\n    text = re.sub('[_@]\\S*', 'username', text) #username\n    text = re.sub('\\s', ' ', text) # '\\t', '\\n' to ' '\n    text = re.sub('[\u00ec,\u00ed,\u00ee,\u00ef]', 'i', text)\n    text = re.sub('[\u00c0,\u00c1,\u00c2,\u00c3,\u00c4,\u00c5]', 'A', text)\n    text = re.sub(' w\/o', ' without', text)\n    text = re.sub(' b\/c', ' because', text)\n    text = re.sub(' w\/', ' with', text)\n    text = re.sub(' n ', ' and ', text) \n    text = re.sub(' u ', ' you ', text)\n    text = re.sub(' r ', ' are ', text)\n    text = re.sub(' u ', ' you ', text)\n    text = re.sub(' U ', ' You ', text)\n    text = re.sub(' ppl', ' peolpe ', text)\n    text = re.sub(' pls ', ' please ', text)\n    text = re.sub(' coz ', ' cause ', text)\n    text = re.sub(' cuz ', ' cause ', text)\n    text = re.sub(' cos ', ' cause ', text)\n    text = re.sub(' wat ', ' what ', text)\n    text = re.sub(' \\*\\*\\*\\* ', ' fuck ', text)\n    text = re.sub(' +', ' ', text)\n    text = re.sub(\"`\", \"'\", text)\n    text = ''.join(map(WordCleaner, \n                       re.split('(\\W+)', text.lower().strip())))\n    return text\n\ndef replace(text):\n    st = text\n    for char in set(text):\n        if char in string.punctuation:\n            continue\n        pattern = char + '{2,}'\n        st = re.sub(pattern, char, st) \n    return st\ndef replace2(text):\n    st = text\n    for char in set(text):\n        if char in string.punctuation:\n            continue\n        pattern = char + '{3,}'\n        st = re.sub(pattern, char+char, st) \n    return st\n\ndef WordCleaner(word):\n    if len(word.translate(table_empty)) == 0 or hobj.spell(word):\n        return word\n    if hobj.spell(re.sub('0', 'o', word)):\n        return re.sub('0', 'o', word)\n    if hobj.spell(replace(word)):\n        return replace(word)\n    if hobj.spell(replace2(word)):\n        return replace2(word)\n    some_ideas = hobj.suggest(word)\n    if (len(word) > 3 and len(some_ideas) > 0 and len(some_ideas[0].split()) < 2 \n        and nltk.edit_distance(word, some_ideas[0]) < 2):\n        return some_ideas[0]\n    return word\n\ndef character_filter(c):\n    if c == \"\\t\": return \" \"\n    if ord(c)<128: return c\n    if c in \"\u2260\u2022\u221e\u2122\u02c8\u0283\u028a\u0281\u0281i\u0281\u0251\u0303\u0283\u0254.\u0303\u00ba\u00aa\u00b6\u00a7\u00a1\u00a3\u00a2\u00e7\": return \"z\"\n    if c in \"\u00e0\u00e1\u00e2\u00e3\u00e4\u00e5\u00e6\": return \"a\"\n    if c in \"\u00e8\u00e9\u00ea\u00eb\": return \"e\"\n    if c in \"\u00ec\u00ed\u00ee\u00ef\": return \"i\"\n    if c in \"\u00f2\u00f3\u00f4\u00f5\u00f6\u014d\u014f\u0151\": return \"o\"\n    if c in \"\u00f9\u00fa\u00fb\u00fc\": return \"u\"\n\n    if c in \"\u00c0\u00c1\u00c2\u00c3\u00c4\u00c5\": return \"A\"\n    if c in \"\u00c8\u00c9\u00ca\u00cb\": return \"E\"\n    if c in \"\u00cc\u00cd\u00ce\u00cf\": return \"I\" \n    if c in \"\u00d2\u00d3\u00d4\u00d5\u00d6\u014c\u014e\u0150\": return \"O\"\n    if c in \"\u00d9\u00da\u00db\u00dc\": return \"U\"\n\n    return \"z\"\n\ndef text_filter(text):\n    return \"\".join(map(character_filter, text))\n\n\ndef IsOk(string):\n    if hobj.spell(string) != -1:\n        return True\n    if hobj.spell(re.sub('0', 'o', string)) != -1:\n        return True\n    if hobj.spell(replace(string)) != -1:\n        return True\n    if hobj.spell(replace2(string)) != -1:\n        return True\n    for r in range(3,len(string)-3):\n        if hobj.spell(string[:r]) and IsOk(string[r:]):\n            return True\n    some_ideas = hobj.suggest(string)\n    if len(some_ideas) > 0 and nltk.edit_distance(string, some_ideas[0]) < 2:\n        return True\n    return False\n\ndef WordTransform(word):\n    word = text_filter(word)\n    if len(word) == 0 or word[0] == '@' or word[0] == '_' or word.find('http') != -1:\n        return ''\n    a = word.find('****') \n    if a != -1:\n        word = word[:a] + 'fuck' + word[:(a+4)]\n    word = word.translate(table_empty).lower()\n    if len(word) == 0 or hobj.spell(word):\n        return word\n    if hobj.spell(re.sub('0', 'o', word)):\n        return re.sub('0', 'o', word)\n    if hobj.spell(replace(word)):\n        return replace(word)\n    if hobj.spell(replace2(word)):\n        return replace2(word)\n    for r in range(3,len(word)-3):\n        if hobj.spell(word[:r]) and IsOk(word[r:]):\n            return word[:r] + ' ' + WordTransform(word[r:])\n    some_ideas = hobj.suggest(word)\n    if len(some_ideas) > 0 and nltk.edit_distance(word, some_ideas[0]) < 2:\n        return some_ideas[0]\n    return word","a64fc2e1":"train['good'] = np.array(list(map(lambda text: sum([text.count(smile) for smile in good_smile]), \n                  train['text'])))\ntrain['bad'] = np.array(list(map(lambda text: sum([text.count(smile) for smile in bad_smile]), \n                  train['text'])))\ntest['good'] = np.array(list(map(lambda text: sum([text.count(smile) for smile in good_smile]), \n                  test['text'])))\ntest['bad'] = np.array(list(map(lambda text: sum([text.count(smile) for smile in bad_smile]), \n                  test['text'])))\n\ntrain['cleaned_text'] = list(map(TextCleaner, train['text']))\n                             \ntest['cleaned_text'] = list(map(TextCleaner, test['text']))\n\ntrain['exclamation'] = np.array(list(map(lambda x: x.count('!'), train['text'])))\ntest['exclamation'] = np.array(list(map(lambda x: x.count('!'), test['text'])))\n\ntrain['CapsLock'] = np.array(list(map(lambda text: np.array(list(map(str.isupper, text))).mean(), train['text'])))\ntest['CapsLock'] = np.array(list(map(lambda text: np.array(list(map(str.isupper, text))).mean(), test['text'])))\n\ntrain['lenght'] = np.array(list(map(len, train['cleaned_text'])))\ntest['lenght'] = np.array(list(map(len, test['cleaned_text'])))\n\ntrain['missed'] = np.array(list(map(lambda x: len(x) - x.count(' '),\n                  train['text']))) - np.array(list(map(lambda x: len(x) - x.count(' '), \n                                                       train['cleaned_text'])))\ntest['missed'] = np.array(list(map(lambda x: len(x) - x.count(' '),\n                  test['text']))) - np.array(list(map(lambda x: len(x) - x.count(' '), \n                                                       test['cleaned_text'])))\n\ntrain['url'] = np.array(list(map(lambda x: x.find('http') != -1, train['text']))).astype(int)\ntest['url'] = np.array(list(map(lambda x: x.find('http') != -1, test['text']))).astype(int)","77a45c20":"def count_len(text):\n    words = text.split(' ')\n    return len(list(filter(lambda word: len(word.translate(table_empty)) > 1 and word.count('http:\/\/') == 0 and\n                    word[0] != '@' and word[0] != '_' and word[0] != '#',\n                    words))) + 1\n\nfraction = np.array([[count_len(x['selected_text']),\n                     count_len(x['text'])] for _, x in train.iterrows()])\n\ntrain['take_all'] = (fraction[:,0] >= fraction[:,1]).astype(int)","c0b175f4":"train[:5]","03488af2":"bert_clean_embed = np.load('..\/input\/embedded-twits\/clean_embed.npy')\nbert_selected_embed = np.load('..\/input\/embedded-twits\/selected_embed.npy')\nbert_test_embed = np.load('..\/input\/embedded-twits\/test_embed.npy')\n\nembed_target =((train['sentiment'] == 'positive').astype(int) + 2 * (train['sentiment'] == 'negative').astype(int)).values\n\nlda = LinearDiscriminantAnalysis(solver='eigen')\nlda.fit(bert_selected_embed, embed_target)\n\ntrain = pd.concat([train, \n                   pd.DataFrame(lda.transform(bert_clean_embed), \n                                columns=['lda_bert_0', 'lda_bert_1', ])], \n                  axis=1, sort=False)\n\ntest = pd.concat([test, \n                   pd.DataFrame(lda.transform(bert_test_embed), \n                                columns=['lda_bert_0', 'lda_bert_1', ])], \n                  axis=1, sort=False)","fa277619":"((lda.predict(bert_clean_embed) ==  embed_target) == train['take_all']).mean()","931daaa4":"col = ['cleaned_text', \n       'sentiment',\n       'exclamation', \n       'CapsLock',\n       'missed',\n       'lenght',\n       'good',\n       'bad',\n       'lda_bert_0', 'lda_bert_1',\n       'url',\n      ]\n\ncv_dataset = Pool(data=train[col],\n                  label=train['take_all'],\n                  cat_features=['sentiment'],\n                  text_features=['cleaned_text'])","a1d2163b":"rkf = KFold(n_splits=5, random_state=42)\nfor cv_train, cv_test in rkf.split(np.arange(train.shape[0])):\n    cat = CatBoostClassifier(n_estimators=500,\n                         max_depth = 8,\n                         task_type = 'GPU',\n                         verbose = 0\n                        )\n    cat.fit(train.loc[cv_train, col], train.loc[cv_train, 'take_all'],\n        cat_features=['sentiment'],\n        text_features=['cleaned_text'])\n    print('accuracy: ' + str((cat.predict(train.loc[cv_test, col]) == train.loc[cv_test, 'take_all']).mean()))","13c8883d":"def rr(df, target):\n    for _, x in df.iterrows():\n        for word in re.split('(\\W+)', x['text'].strip()):\n            y = x.copy()\n            y['place'] = x['text'].find(word)\n            y['word'] = WordTransform(word).lower()\n            if len(y['word'].translate(table_empty)) < 3:\n                continue\n            y['missed'] = len(word) - len(y['word'])\n            y['CapsLock'] = sum(list(map(str.isupper, y['word'])))\n            if target:\n                y['target'] = int(x['selected_text'].find(word) != -1)\n                del y['selected_text']\n            del y['text']\n            yield y","66bcb4f5":"selected_train = pd.DataFrame(rr(train, True))\nselected_test = pd.DataFrame(rr(test, False))","321546b9":"#from collections import Counter\n#words_train = Counter(selected_train['word'])\n#words_test = Counter(selected_test['word'])\n#embed_train = dict(zip(list(words_train), lda.transform(model.encode(list(words_train), batch_size=128))))\n#embed_test = dict(zip(list(words_test), lda.transform(model.encode(list(words_test), batch_size=128))))\n#selected_train['bert_0'] = [embed_train[word][0] for word in selected_train['word']]\n#selected_train['bert_1'] = [embed_train[word][1] for word in selected_train['word']]\n#selected_test['bert_0'] = [embed_test[word][0] for word in selected_test['word']]\n#selected_test['bert_1'] = [embed_test[word][1] for word in selected_test['word']]","4c906755":"cat = CatBoostClassifier(n_estimators=2000,\n                         max_depth = 8,\n                         thread_count=6,\n                         task_type = 'GPU',\n                         verbose = 0)\n\ncol = ['cleaned_text', 'sentiment',\n                            'CapsLock',\n                            'missed', \n                            'lenght',\n      #                     'bert_0', 'bert_1',\n      ]\n\ncat.fit(selected_train[col], \n        selected_train['target'],\n        cat_features=['sentiment',],\n        text_features=['cleaned_text'])","00ca896d":"def sentiment_ext(i, df):\n    #emb = lda.transform(np.array(model.encode(df.loc[i,'cleaned_text'].split(), batch_size=16)))\n    tt = pd.concat([df.iloc[[i]][col]*emb.shape[0]])\n    #tt['bert_0'] = emb[:,0]\n    #tt['bert_1'] = emb[:,1]\n    res = cat.predict_proba(tt)[:,1]\n    words = df.loc[i, 'text'].split()\n    answer = ''\n    if (res < 0.5).mean() == 1:\n        answer = words[np.argmax(res)]\n    else:\n        good = np.where(res >= 0.5)[0]\n        answer = ' '.join(words[i] for i in range(good[0], good[-1] + 1))\n    return answer","21d04c7e":"test['selected_text'] = [sentiment_ext(i, test) for i in range(test.shape[0])]","30354a63":"test[['textID','selected_text']].to_csv('submission.csv',index=False)","5a01b5ca":"## Our target\n\nIn 'take_all' column we mark by '1' twits which are fully selected to needed sentiment, and by '0' others.","48e2105b":"Let's look on head of train:","6ef7d606":"We clean up texts and add some information about it.","13d871a2":"## Using of a pretrained embedding","9c224a64":"Let's predict other target, do we need to take this word to prediction?","fa7dde8c":"## Model","65e3fe40":"## What words we will take to our prediction","b844df81":"In this notebook we will use gradient boosting for sentence extraction. In the first part we change our target to binary classification. We will predict does the text completely corresponds to a sentiment.  ","8cf5f902":"In next part we'll choose parts of the texts with corresponded sentiment.","6bd6f8ae":"Let's use pretrained embeddings to predict, which sentences have positive, negative or neutral sentiment. We will add this prediction to dataset."}}