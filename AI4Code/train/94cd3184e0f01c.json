{"cell_type":{"0c0f124d":"code","ff71dc0d":"code","3fb39c15":"code","c7a8d282":"code","fc73e076":"code","112bbb79":"code","774879e2":"code","752bb422":"code","f351e520":"code","968caad0":"code","f41efdb6":"code","86171210":"code","17962c36":"code","d383e183":"code","334cd522":"code","01662ff4":"markdown","cd5d781e":"markdown","cc673296":"markdown","97fd21dc":"markdown","d155abbc":"markdown","8ee0b9ca":"markdown","e1b8f004":"markdown","7ad4de5a":"markdown","c6fcb119":"markdown","3bc9ed00":"markdown","74f2ce2d":"markdown","0684b248":"markdown","1de1c76e":"markdown","6fbab479":"markdown","76f92617":"markdown"},"source":{"0c0f124d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\ndata_dir = \"\/kaggle\/input\/heart-disease-prediction-using-logistic-regression\"\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\n\n","ff71dc0d":"data = pd.read_csv(data_dir+\"\/framingham.csv\")\ndata.drop(['education'],axis=1,inplace=True)\ndata.rename(columns={'male':'Sex_male'},inplace=True)\ndata.head()","3fb39c15":"#Check dataset for missing values\ndata.isnull().sum()","c7a8d282":"count = 0\nfor i in data.isnull().sum(axis=1):\n    if i>0:\n        count = count + 1\ndata.dropna(axis = 0 , inplace =True)\ndata.describe()","fc73e076":"#Check values again.\ndata.isnull().sum()","112bbb79":"print(data.info())\ny = data.TenYearCHD.values\nx_data = data.drop([\"TenYearCHD\"],axis = 1)\n","774879e2":"#Normalize data\nx = (x_data-np.min(x_data))\/(np.max(x_data)-np.min(x_data))","752bb422":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.2,random_state = 42)\nx_train = x_train.T\nx_test = x_test.T\ny_train = y_train.T\ny_test = y_test.T\n","f351e520":"def initialize_weights_and_bias(dimension):\n    w = np.full((dimension),0.01)\n    b = 0.0\n    return w,b\n\nw,b = initialize_weights_and_bias(14) #For debugging.","968caad0":"def sigmoid (z):\n    y_head = 1\/(1+np.exp(-z))\n    return y_head\nprint(sigmoid(0)) #debug\n","f41efdb6":"def forward_backward_propagation(w,b,x_train,y_train):\n    \n    z = np.dot(w.T,x_train) + b #\u2022In tutorial we have learned fp's formula  z = W1X1+W2X2+....WnXn + bias(b). np.Dot does matrix multiplication\n    y_head = sigmoid(z) # y_head means our predictions. We find this with sigmoid function. We implement it to the z.\n    loss =-y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head) #loss function.\n    cost = (np.sum(loss))\/(x_train.shape[1]) # cost function is the sum of loss and divide by sample count.\n    #now we will implement bp. in bp we need derivative of w's and b's\n    \n    derivative_weight = (np.dot(x_train,((y_head-y_train).T)))\/x_train.shape[1]\n    derivative_bias = derivative_bias = np.sum(y_head-y_train)\/x_train.shape[1]\n    gradients =gradients = {\"derivative_weight\": derivative_weight, \"derivative_bias\": derivative_bias}\n    return cost,gradients","86171210":"def update(w, b, x_train, y_train, learning_rate,number_of_iterarion):\n    cost_list = []\n    cost_list2 = []\n    index = []\n    \n    # updating(learning) parameters is number_of_iterarion times\n    for i in range(number_of_iterarion):\n        # make forward and backward propagation and find cost and gradients\n        cost,gradients = forward_backward_propagation(w,b,x_train,y_train)\n        cost_list.append(cost)\n        # lets update\n        w = w - learning_rate * gradients[\"derivative_weight\"]\n        b = b - learning_rate * gradients[\"derivative_bias\"]\n        if i % 10 == 0:\n            cost_list2.append(cost)\n            index.append(i)\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n            \n    # we update(learn) parameters weights and bias\n    parameters = {\"weight\": w,\"bias\": b}\n    plt.plot(index,cost_list2)\n    plt.xticks(index,rotation='vertical')\n    plt.xlabel(\"Number of Iterarion\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters, gradients, cost_list","17962c36":"def predict(w,b,x_test):\n    # x_test is a input for forward propagation\n    w = w.reshape(-1,1)\n    b = b.reshape(-1,1)\n    z = sigmoid(np.dot(w.T,x_test)+b)\n    z= z.reshape(-1,1)\n    print(w.shape)\n    Y_prediction = np.zeros((1,x_test.shape[1]))\n\n   \n    # if z is bigger than 0.5, our prediction is TenYearsCHD 1 (y_head=1),\n    # if z is smaller than 0.5, our prediction is TenYearsCHD 0 (y_head=0),\n    for i in range(z.shape[1]):\n        if z[0,i]<= 0.5:\n            Y_prediction[0,i] = 0\n        else:\n            Y_prediction[0,i] = 1\n\n    return Y_prediction","d383e183":"def logistic_regression(x_train, y_train, x_test, y_test, learning_rate ,  num_iterations):\n    # initialize\n    dimension =  x_train.shape[0]  # that is 14\n    print(dimension)\n    w,b = initialize_weights_and_bias(dimension)\n    # do not change learning rate\n    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate,num_iterations)\n    \n    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n\n    # Print test Errors\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n    \nlogistic_regression(x_train, y_train, x_test, y_test,learning_rate = 1, num_iterations = 5000)  ","334cd522":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(x_train.T,y_train.T)\nprint(\"test accuracy {}\".format(lr.score(x_test.T,y_test.T)))  ","01662ff4":"# 2.Define Sigmoid Function:\n\n\nMath Model of Sigmoid : ","cd5d781e":"![resim.png](attachment:resim.png)\nAccording to the datai team's deeplearning tutorials, Logistic regression model on picture. W's are, coefficents of each feature. Bias is intercept.\n\n* z = (w.t)x + b => z equals to (transpose of weights times input x) + bias\n* z = b + x1w1 + x2w2 + ... + xn*wn\n* y_head = sigmoid(z)( we use sigmoid to  return probabilistic values)\n\n\nNow our program steps:\n1. Initialize Parameters\n1. Define Sigmoid Function\n1. Define and Implement Forward-Backward Propagation\n1. Implement Update Function\n1. Predict Function\n1. Implement Logistic Regression\n1. BONUS : Sklearn Logistic Regression\n\n","cc673296":"Now our dataset clear. We can implement logistic regression to it. First of all, Regression is a fitting a line to datas. We need x, and y axes to implement. Y axis is the data that we will predict. X axis is remain features. For this set, our y axis feature : TenYearCHD(10 year risk of coronary heart disease). Remain features will generate our x axis.","97fd21dc":"# 7. BONUS : Sklearn Logistic Regression:\n* I know we did some effort to  predict something in this tutorial, but you can do this with sklearn with 4 lines of code :)\n","d155abbc":"Split data as train end test. We will use sklearn to do. After splitting,transposition all matrixes to multiply.","8ee0b9ca":"We have to normalize data. Because higher values can affect to lower values.","e1b8f004":"When we check the dataset, we faced some missing values.This values affect negatively our approach so we must delete them.\ndropna function return all NA values.Axis=0 representing rows.","7ad4de5a":"# 6. Logistic Regression\n* Finally we define Logistic Regression function. First we will get datas dimensions (14 for our dataset).\n* Initialize parameters(w,b) as w = 0.01, b=0.0\n* Calculate new parameters,gradients and cost_lists. We can use update function because it has fp and bp already.\n* Calculate prediction. This function gets w,b and test set.\n* Call this function. You can change iteration and learning rate to analyze our model.\n","c6fcb119":"# 4. Update Function :\n* Our upadte function take parameters w,b(to update),x_train,y_train(to learn),learning_rate(hyperparameter so we have to tune.),number_of_iteration(how many times we will try?)\n* We call fp and bp function, use new  w formula and do same things for b's. This formulas was explained in DataiTeams deep learning tutorial. You can find them on google.\n* Every 10 iteration we print them to analyze.\n* After all plot them with maplotlib.\n","3bc9ed00":"# 1. Initialize Parameters:\n* In this section we define w,b. Our function get a parameter named dimension. This is our test and train matrix's(x_test,x_train) column number. For this dataset its 14.We create the matrix that contains 0.01. Initial w value is  0.01. It is only a concept.b = 0.0\n","74f2ce2d":"We did some inital processes and define direction of dataset. After this processes, we can create dataframe with pandas. After reading dataset,we have to clear some points on dataset.\n1. First of All education feature is useless to predict  Risk of coronary heart disease. Drop it.\n2. Male feature name is nonesense.Change to the Sex_male","0684b248":"# About Dataset:\nI used this dataset in the link below :https:\/\/www.kaggle.com\/dileep070\/heart-disease-prediction-using-logistic-regression\nIn this dataset we have some values like bmi, cigarette using, cigarette per day etc.  We will use this features to predict 10 year risk of coronary heart disease. First of all you can check dataset to normalize and uderstand it. We have to change some variables to binaries or, we have to delete null datas or we have to drop a column. For example, in dataset  we have education feature. Risk of coronary heart disease independent from education. Theese are not related features.","1de1c76e":"# 3. Define and Implement Forward-Backward Propagation:\n* This function implement forward and backward propagation. 1 forward and 1 backward propagation equals 1 iteration.\n* First we multiply w's and x's. Sum of this process equals z. If we implement sigmoid function to the z we get y_head.\n* After getting y_head, we need loss. After calculate loss, we need cost function. Cost function means sum of loss formulas results. This process is forward propagation.\n* To calculate backward propagation(bp) we need derivatives of w's and b's.\n* After calculated derivatives, we append them to the dictionary named gradients.\n* In bp, we re-calculate w and b's to reduce cost function value. If our cost is too low, we can predict more accurate. This process can be done with better w and b values. When we calculate w and b we are use derivatives. Derivative means slope. If our slope is near 0,its done. This method named gradient descent.\n**","6fbab479":"# 5. Predict Function\n* When we splitted data set, we use 0.2 percent data to test,0.8 percent data to train. Now we will use this test datas(x_test) to predict some patients TenYearCHD situation.\n* When i didnt use z.reshape(-1,1) method, i got an error that : tuple index out of range error. After  I check shapes w,and z so i decided to change them according to matrix multiply prequsities. If you have an explanation about this situation feel free to contact me. I need to clearly learn it.\n* After i decide  threshold value as 0.5","76f92617":"# Homework: Logistic Regression to Predict Heart Disease\nThis notebook created for datai team homework. We have just learned logistic regression classification method to predict binary situations(ex:is cat or dog? tumor is malign or benign etc.)I try to implement this method in this notebook. I am new on machine learning so i can do some mistakes. Feel free to feedback. Greatly  Thanks.\n"}}