{"cell_type":{"f6fc9a17":"code","5494ff78":"code","50f8101f":"code","991c4d9a":"code","65417e55":"code","162daea0":"code","d394372a":"code","673d06ad":"code","f8ee1d7f":"code","0dc2f4c4":"code","b441c927":"code","7e9ef0ab":"code","210de291":"code","5ea8b27c":"code","6f5dd750":"code","66d0cb3b":"code","6651e07a":"code","6a3e259c":"code","d4e49bdf":"code","3095cd7a":"code","6a05065c":"code","ae2fd1ac":"code","48382c7d":"code","6155c5f2":"code","3bb2a7b2":"code","78fd8a64":"code","7523babc":"code","fbc17867":"code","79084d27":"code","560b6f9e":"code","e0239224":"code","d459edd8":"code","d67e211a":"code","074959e6":"code","367fb559":"code","5c638f07":"code","81a67f2e":"code","96980249":"code","bc91150c":"markdown","ff35647b":"markdown","8a270565":"markdown","edfa8f9e":"markdown","35eed494":"markdown","d8856a15":"markdown"},"source":{"f6fc9a17":"import pandas as pd\nimport numpy as np \n\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n","5494ff78":"weatherAUS = pd.read_csv('..\/input\/weather-dataset-rattle-package\/weatherAUS.csv')\nweatherAUS.head()","50f8101f":"#length of the dataset \nprint('dataset have {} rows and {} columns'.format(weatherAUS.shape[0],weatherAUS.shape[1]))","991c4d9a":"print('Name of columns:')\nfor i in weatherAUS.columns:\n    \n    print(i)","65417e55":"#viewing summary of dataset:\n\nweatherAUS.info()","162daea0":"#for all numerical variables \nweatherAUS.describe()\n\n","d394372a":"#for all categorical \nweatherAUS.describe(include = ['object'])","673d06ad":"weatherAUS.isnull().sum()","f8ee1d7f":"weatherAUS['RainTomorrow'].unique()","0dc2f4c4":"#droping rows when there is null values in target columnn..\n\nweatherAUS = weatherAUS[~weatherAUS['RainTomorrow'].isnull()]","b441c927":"#let's Start our Eda with out target variable\n\nweatherAUS['RainTomorrow'].value_counts()\/len(weatherAUS)","7e9ef0ab":"fig , ax = plt.subplots(figsize = (6,5))\nax = sns.countplot(y= 'RainTomorrow', data=weatherAUS )","210de291":"#let's save categorical variables and numerical variables name in a list\n\ncategorical = [var for var in weatherAUS.columns if weatherAUS[var].dtype =='O']\n\nNumerical = [var for var  in weatherAUS.columns if weatherAUS[var].dtype !='O' ]\n\n\nprint('There are {} categorical variables\\nThere names are: {}\\n'.format(len(categorical),categorical))\n\nprint('There are {} numerical variables\\nThere names are: {}'.format(len(Numerical),Numerical))","5ea8b27c":"100*weatherAUS[categorical].isna().sum()\/len(weatherAUS[categorical])","6f5dd750":"#Let's explore within categorical variable\n\ncat_variables_withnull = [var for var in categorical if weatherAUS[var].isna().sum() !=0]\n\nweatherAUS[cat_variables_withnull].isna().sum()\n\n","66d0cb3b":"#replacing null values of categorical variables with None.\nfor cat in cat_variables_withnull:\n    \n    weatherAUS.loc[:,cat] =  weatherAUS.loc[:,cat].fillna('None')","6651e07a":"#now our categorical variables have no null values but if we here date column is coming inside categorical column which is wrong \n#so let's change the date column data type \n\nprint(weatherAUS['Date'].dtypes)\n\nweatherAUS['Date'] = pd.to_datetime(weatherAUS['Date'])\n\nprint(weatherAUS['Date'].dtypes)\n\n\ncategorical = [var for var in weatherAUS.columns if weatherAUS[var].dtype =='O']\n\n\nprint('There are {} categorical variables\\nThere names are: {}\\n'.format(len(categorical),categorical))","6a3e259c":"#finding numerical variables which consist of Null values...\nnum_variables_withnull = [var for var in Numerical if weatherAUS[var].isna().sum() !=0]\n\nweatherAUS[num_variables_withnull].isna().sum()","d4e49bdf":"#finding the % of null values : \n\n100*weatherAUS[Numerical].isna().sum()\/len(weatherAUS[Numerical])","3095cd7a":"#checking for correlation between the variables:\ncorrelation = weatherAUS.corr()\nplt.figure(figsize=(16,12))\nplt.title('Correlation Heatmap of Rain in Australia Dataset')\nax = sns.heatmap(correlation, square=True, annot=True, fmt='.2f', linecolor='white')\nax.set_xticklabels(ax.get_xticklabels(), rotation=90)\nax.set_yticklabels(ax.get_yticklabels(), rotation=30)           \nplt.show()\n","6a05065c":"sns.pairplot(weatherAUS[Numerical], kind = 'scatter', diag_kind = 'hist', palette = 'Rainbow')\nplt.show()","ae2fd1ac":"#creating Year, Month, Week from Date column...\nweatherAUS.loc[:,'year']  = weatherAUS.loc[:,'Date'].dt.year\nweatherAUS.loc[:,'month']  = weatherAUS.loc[:,'Date'].dt.month\nweatherAUS.loc[:,'week']  = weatherAUS.loc[:,'Date'].dt.week","48382c7d":"#creating time based CV split \n\n\nfold_split =  {\n    0 : {'train': [2008] , 'test' : [2009]},\n    1 : {'train': [2008,2009] , 'test' : [2010]},\n    2 : {'train': [2008,2009,2010] , 'test' : [2011]},\n    3 : {'train': [2008,2009,2010,2011] , 'test' : [2012]},\n    4 : {'train': [2008,2009,2010,2011,2012], 'test' : [2013]},\n    5 : {'train': [2008,2009,2010,2011,2012,2013], 'test' : [2014]},\n    6 : {'train': [2008,2009,2010,2011,2012,2013,2014], 'test' : [2015]},\n    7 : {'train': [2008,2009,2010,2011,2012,2013,2014,2015], 'test' : [2016]}\n            \n}","6155c5f2":"#filling null values for numerical variables: \nfor i in num_variables_withnull:\n    if weatherAUS[i].isna().sum() >0:\n        weatherAUS.loc[:,i] = weatherAUS.groupby(\n     ['Location',\n     'WindGustDir',\n     'WindDir9am',\n     'WindDir3pm',\n     'year',\n     'month',\n     'week']\n    )[i].transform(\n        lambda grp: grp.fillna(np.mean(grp))\n    )\n        if weatherAUS[i].isna().sum() >0:\n            weatherAUS.loc[:,i] = weatherAUS.groupby(\n     ['Location',\n     'WindGustDir',\n     'WindDir9am',\n     'year',\n     'month',\n     'week']\n    )[i].transform(\n        lambda grp: grp.fillna(np.mean(grp))\n    )\n            if weatherAUS[i].isna().sum() >0:\n                weatherAUS.loc[:,i] = weatherAUS.groupby(\n     ['Location',\n     'WindGustDir',\n     'year',\n     'month',\n     'week']\n    )[i].transform(\n        lambda grp: grp.fillna(np.mean(grp))\n    )\n                if weatherAUS[i].isna().sum() >0:\n                    weatherAUS.loc[:,i] = weatherAUS.groupby(\n     ['Location',\n     'year',\n     'month',\n     'week']\n    )[i].transform(\n        lambda grp: grp.fillna(np.mean(grp))\n    )        \n                    if weatherAUS[i].isna().sum() >0:\n                        weatherAUS.loc[:,i] = weatherAUS.groupby(\n     [\n     'year',\n     'month',\n     'week']\n    )[i].transform(\n        lambda grp: grp.fillna(np.mean(grp))\n    )\n\n\n\n","3bb2a7b2":"#Creating label encoding for categorical variables:\nfrom sklearn import preprocessing\ncategorical =['Location',\n 'WindGustDir',\n 'WindDir9am',\n 'WindDir3pm',\n 'RainToday','RainTomorrow']\nfor feat in categorical:\n    lbl_enc = preprocessing.LabelEncoder()\n    weatherAUS.loc[:,feat] = lbl_enc.fit_transform(weatherAUS.loc[:,feat])","78fd8a64":"#just trying to fit Randomforest model on Different folds and printing it's Roc\nfor i in list(fold_split.keys()):\n    features = [ 'Location', 'MinTemp', 'MaxTemp', 'Rainfall', 'Evaporation',\n           'Sunshine', 'WindGustDir', 'WindGustSpeed', 'WindDir9am', 'WindDir3pm',\n           'WindSpeed9am', 'WindSpeed3pm', 'Humidity9am', 'Humidity3pm',\n           'Pressure9am', 'Pressure3pm', 'Cloud9am', 'Cloud3pm', 'Temp9am',\n           'Temp3pm', 'RainToday', 'year', 'month', 'week']\n    train = weatherAUS[weatherAUS['year'].isin(fold_split[i]['train'])]\n    test = weatherAUS[weatherAUS['year'].isin(fold_split[i]['test'])]\n    X_train = train[features]\n    y_train = train['RainTomorrow']\n    X_test = test[features]\n    y_test = test['RainTomorrow']\n    from sklearn.ensemble import RandomForestClassifier\n    model = RandomForestClassifier()\n    model.fit(X_train,y_train)\n    preds=model.predict_proba(X_test)[:,1]\n    from sklearn.metrics import accuracy_score , confusion_matrix,roc_auc_score\n    print('Roc value for fold {}: {}'.format(i,roc_auc_score(y_test,preds)))","7523babc":"#predicting probabilties :) \nfrom sklearn.ensemble import RandomForestClassifier\n\nmodel = RandomForestClassifier()\n\nmodel.fit(X_train,y_train)\npreds=model.predict_proba(X_test)[:,1]\nfrom sklearn.metrics import accuracy_score , confusion_matrix,roc_auc_score, roc_curve\nroc_auc_score(y_test,preds)","fbc17867":"#checking for confusion metrics\nconfusion_matrix(y_test,model.predict(X_test))","79084d27":"a,b,c =roc_curve(y_test,preds)\naccuracy_ls = []\n\nfor thres in c:\n    y_pred = np.where(preds>thres,1,0)\n    accuracy_ls.append(accuracy_score(y_test,y_pred,normalize=True))\n\naccuracy_df = pd.concat([pd.Series(c),pd.Series(accuracy_ls)],axis = 1 )  \naccuracy_df.columns = ['threshold','accuracy']\naccuracy_df[accuracy_df['accuracy'] == accuracy_df['accuracy'].max()]","560b6f9e":"#creating json for finding hpyerparameters :) \nparam_grid = {\n    \"n_estimators\": np.arange(100,1500,100),\n    \"max_depth\" : np.arange(1,31),\n    \"criterion\" : ['gini','entropy']\n}\nfrom sklearn import model_selection","e0239224":"g = model_selection.RandomizedSearchCV(estimator=model,param_distributions=param_grid,scoring='accuracy',verbose =10,n_jobs =1,cv=5)","d459edd8":"g.fit(X_train,y_train)","d67e211a":"#finding the best accuracy score from it :) \ng.best_score_","074959e6":"#we will get the parameters and try to fit the model again:\ng.best_estimator_.get_params()","367fb559":"#random forest with optimal parameters \nmodel = RandomForestClassifier(max_depth=25,n_estimators=1400,criterion='entropy')\nmodel.fit(X_train,y_train)\npreds=model.predict_proba(X_test)[:,1]\nroc_auc_score(y_test,preds)","5c638f07":"#finding the optimal threshold for accuracy\naccuracy_ls = []\n\nfor thres in c:\n    y_pred = np.where(preds>thres,1,0)\n    accuracy_ls.append(accuracy_score(y_test,y_pred,normalize=True))\n\naccuracy_df = pd.concat([pd.Series(c),pd.Series(accuracy_ls)],axis = 1 )  \naccuracy_df.columns = ['threshold','accuracy']\naccuracy_df[accuracy_df['accuracy'] == accuracy_df['accuracy'].max()]    ","81a67f2e":"confusion_matrix(y_test,np.where(preds>0.48,1,0))","96980249":"import xgboost as xgb\nclf = xgb.XGBClassifier()\n\nparam_grid = {\n        'silent': [False],\n        'max_depth': [6, 10, 15, 20],\n        'learning_rate': [0.001, 0.01, 0.1, 0.2, 0,3],\n        'subsample': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n        'colsample_bytree': [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n        'colsample_bylevel': [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n        'min_child_weight': [0.5, 1.0, 3.0, 5.0, 7.0, 10.0],\n        'gamma': [0, 0.25, 0.5, 1.0],\n        'reg_lambda': [0.1, 1.0, 5.0, 10.0, 50.0, 100.0],\n        'n_estimators': [100]}\n\n'''fit_params = {'eval_metric': 'mlogloss',\n              'early_stopping_rounds': 10,\n              'eval_set': [(X_test, y_test)]}'''\n\nrs_clf = model_selection.RandomizedSearchCV(clf, param_grid, n_iter=20,\n                            n_jobs=1, verbose=2, cv=2,\n                            scoring='accuracy', refit=False, random_state=42)\nprint(\"Randomized search..\")\n#search_time_start = time.time()\nrs_clf.fit(X_train, y_train)\n#print(\"Randomized search time:\", time.time() - search_time_start)\n\nbest_score = rs_clf.best_score_\nbest_params = rs_clf.best_params_\nprint(\"Best score: {}\".format(best_score))\nprint(\"Best params: \")\nfor param_name in sorted(best_params.keys()):\n    print('%s: %r' % (param_name, best_params[param_name]))","bc91150c":"droping rows where target columns (RainTomorrow) are null. ","ff35647b":"# Let's Explore the Data","8a270565":"from above we can clearly see that data contains both numerical as well as categorical variables.\nviewing it's statistical properties of dataset help to understand behaviour of the features.","edfa8f9e":"It's always better to look numerical and categorical variable separately \n\nNow,let's look into the number of null values in each features.","35eed494":"From above matrix we can see that following infernces:\n    \n    MaxTemp,MinTemp,Temp9am,Temp3pm are highly correlated.\n    \n    Sunshine, Cloud9am, Cloud3am are also highly correlated.\n    ","d8856a15":"As we can see that our target columns consist of  77.58% No and 22.42% Yes"}}