{"cell_type":{"112d6384":"code","794ebe5f":"code","6c47db2b":"code","d2aab83d":"code","a0905067":"code","1bffe1da":"code","60c8197c":"code","3d59d577":"code","109fd8ad":"code","a800056c":"code","ee83741c":"code","84b4ef43":"code","3dbb4df1":"code","512fb4bd":"code","025eb288":"code","70bc0d69":"code","15fe2f39":"code","f06bf78b":"code","1199e2bf":"code","6c3d5304":"code","29996b21":"code","c033d1b8":"code","334c49a0":"code","64791484":"code","a842010d":"code","5d2c5c84":"code","e5d64d09":"code","315ce77b":"code","eabfff17":"code","cac1b8cd":"code","44f41694":"code","0f36f1c4":"code","bbfee75a":"code","64b8cbce":"code","202b0833":"code","c0e4b14d":"code","2721e241":"code","00d2422a":"code","62985f05":"code","0cc287cd":"markdown","c52cb5e7":"markdown","e017d2ac":"markdown","1cfe1611":"markdown","eb484432":"markdown","a22f71e4":"markdown","88986bb2":"markdown","03d7dd8c":"markdown","269754d2":"markdown","ce7d0ece":"markdown","595050f4":"markdown","3089f28a":"markdown","b5928b67":"markdown","1b7ab272":"markdown","110c4754":"markdown","9145cd82":"markdown","47ca2386":"markdown","bb06c7a9":"markdown","be2430be":"markdown","8995a1eb":"markdown","5f96f044":"markdown","0385398e":"markdown","63c5d7af":"markdown","45839d06":"markdown","7a5eaf7a":"markdown","e1972622":"markdown","534929a6":"markdown","7c081d15":"markdown","57e4731b":"markdown"},"source":{"112d6384":"!pip install --upgrade pip\n!pip install --upgrade scikit-learn","794ebe5f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6c47db2b":"import pandas as pd\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n  \ntrain_data = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\n\nprint('The shape of the dataset is:', train_data.shape)","d2aab83d":"train_data.head(10)","a0905067":"train_data.info()","1bffe1da":"# This prints basic statistics for numerical columns\ntrain_data.describe()","60c8197c":"# I remember from the movie Titanic the line \"women and childre go first...\"\nwomen = train_data.loc[train_data.Sex == 'female'][\"Survived\"]\nrate_women = sum(women)\/len(women)\n\nprint(\"% of women who survived:\", rate_women)","3d59d577":"# Grab model features\/inputs and target\/output\nnumerical_features = [\"Age\", \"SibSp\", \"Parch\"]\n\ncategorical_features = [\"Pclass\", \"Sex\", 'Embarked']\n\nmodel_features = numerical_features + categorical_features \nmodel_target = \"Survived\"","109fd8ad":"from string import ascii_letters\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.set(style=\"white\")\n\n# Compute the correlation matrix\ncorr = train_data[numerical_features].corr()\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","a800056c":"for i in range(0,len(numerical_features)):\n    print(train_data[numerical_features[i]].value_counts(bins=5, sort=False))","ee83741c":"train_data[numerical_features].hist(bins=5, figsize=(20, 15))","84b4ef43":"# SibSp\ndropIndexes = train_data[train_data['SibSp'] > 4].index\ntrain_data.drop(dropIndexes , inplace=True)\n\n# Parch\ndropIndexes = train_data[train_data['Parch'] > 2].index\ntrain_data.drop(dropIndexes , inplace=True)","3dbb4df1":"train_data[numerical_features].hist(bins=5, figsize=(20, 15))","512fb4bd":"train_data[categorical_features] = train_data[categorical_features].astype('str')","025eb288":"for c in categorical_features:\n    print(c)\n    print(train_data[c].unique())\n    print(train_data[c].value_counts())","70bc0d69":"from sklearn.model_selection import train_test_split\n\ntrain_data, test_data = train_test_split(train_data, test_size=0.1, shuffle=True, random_state=42)\n\n# Print the shapes of the Train - Test Datasets\nprint('Train - Test Datasets shapes: ', train_data.shape, test_data.shape)","15fe2f39":"# checking for an imbalanced dataset\nimport matplotlib.pyplot as plt\n\ntrain_data['Survived'].value_counts().plot.bar()\nplt.show()","f06bf78b":"from sklearn.utils import shuffle\ndef up_sampling(train_data):\n    survived = train_data[train_data[model_target] == 1]\n    not_survived = train_data[train_data[model_target] == 0]\n\n    upsampled_survived = survived.sample(n=len(not_survived), replace=True, random_state=42)\n\n    train_data = pd.concat([not_survived, upsampled_survived])\n    return shuffle(train_data)\n\ndef print_sample_stats():\n    # Print the shapes of the Train - Validation - Test Datasets\n    print('Train - Test Datasets shapes: ', train_data.shape, test_data.shape)\n\n    # Print the distribution of target for the training dataset\n    print('Train Dataset class 0 samples: ', sum(train_data[model_target] == 0))\n    print('Train Dataset class 1 samples: ', sum(train_data[model_target] == 1))","1199e2bf":"# fixing imbalance with up-sampling\nprint_sample_stats()\ntrain_data = up_sampling(train_data)\nprint_sample_stats()\n\n# checking for an imbalanced dataset\nimport matplotlib.pyplot as plt\n\ntrain_data['Survived'].value_counts().plot.bar()\nplt.show()","6c3d5304":"from sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder, MinMaxScaler\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\n\n# https:\/\/scikit-learn.org\/stable\/modules\/classes.html#module-sklearn.preprocessing\n\n### COLUMN_TRANSFORMER ###\n##########################\n\n# Preprocess the numerical features\nnumerical_processor = Pipeline([\n    ('num_imputer', SimpleImputer(strategy='mean')),\n    ('num_scaler', MinMaxScaler()) # Shown in case is needed, not a must with Decision Trees but good to have with KNN\n])\n                  \n# Preprocess the categorical features\ncategorical_processor = Pipeline([\n    ('cat_imputer', SimpleImputer(strategy='constant', fill_value='missing')), # Shown in case is needed, no effect here as we already imputed with 'nan' strings\n    ('cat_encoder', OneHotEncoder(handle_unknown='ignore')) # handle_unknown tells it to ignore (rather than throw an error for) any value that was not present in the initial training set.\n])\n\n# Combine all data preprocessors from above (add more, if you choose to define more!)\n# For each processor\/step specify: a name, the actual process, and finally the features to be processed\ndata_processor = ColumnTransformer([\n    ('numerical_processing', numerical_processor, numerical_features),\n    ('categorical_processing', categorical_processor, categorical_features)\n]) ","29996b21":"# Visualize the data processing pipeline\nfrom sklearn import set_config\nset_config(display='diagram')\ndata_processor","c033d1b8":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn import set_config\nset_config(display='diagram')\n\n\n# https:\/\/scikit-learn.org\/stable\/modules\/classes.html#module-sklearn.tree\n# https:\/\/scikit-learn.org\/stable\/modules\/classes.html#module-sklearn.neighbors\n\n### PIPELINE ###\n################\n\n# Pipeline desired all data transformers, along with an estimator at the end\n# Later you can set\/reach the parameters using the names issued - for hyperparameter tuning, for example\ndt_pipeline = Pipeline([\n    ('data_processing', data_processor),\n    ('dt', DecisionTreeClassifier())\n])\n\nknn_pipeline = Pipeline([\n    ('data_processing', data_processor),\n    ('knn', KNeighborsClassifier())\n])","334c49a0":"dt_pipeline","64791484":"knn_pipeline","a842010d":"# Get train data to train the classifier\nX_train = train_data[model_features]\ny_train = train_data[model_target]\n\n# Fit the classifier to the train data\ndt_pipeline.fit(X_train, y_train)","5d2c5c84":"knn_pipeline.fit(X_train, y_train)","e5d64d09":"from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\ndef test_model_performance(X, y, model_name, model):\n    predictions = model.predict(X)\n    \n    print(confusion_matrix(y, predictions))\n    print(classification_report(y, predictions))\n    print(f\"Accuracy for {model_name}:\", accuracy_score(y, predictions))","315ce77b":"print('Model performance on the train set:')\ntest_model_performance(X_train, y_train, \"DT\", dt_pipeline)\nprint(\"-----------------------------------------------------\")\nprint('Model performance on the train set:')\ntest_model_performance(X_train, y_train, \"KNN\", knn_pipeline)","eabfff17":"from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n\n# Get test data to test the classifier\nX_test = test_data[model_features]\ny_test = test_data[model_target]\n\nprint('Model performance on the test set:')\ntest_model_performance(X_test, y_test, \"DT\", dt_pipeline)\nprint(\"-----------------------------------------------------\")\nprint('Model performance on the test set:')\ntest_model_performance(X_test, y_test, \"KNN\", knn_pipeline)","cac1b8cd":"pred_data = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\nX_data = pred_data[model_features]\nX_data.head(10) #nothing up my sleeve","44f41694":"dt_predictions = dt_pipeline.predict(X_data.head(10))\nknn_predictions = knn_pipeline.predict(X_data.head(10))\nprint(dt_predictions)\nprint(knn_predictions)","0f36f1c4":"df_predictions = X_data.head(10)\ndf_predictions[\"Decision Tree Prediction\"] = dt_predictions\ndf_predictions[\"KNN Prediction\"] = knn_predictions\ndf_predictions.head(10)","bbfee75a":"# making sure we get some different results from each model\nX_data = pred_data[model_features]\ndt_predictions = dt_pipeline.predict(X_data)\nknn_predictions = knn_pipeline.predict(X_data)\ndf_predictions = X_data\ndf_predictions[\"Decision Tree Prediction\"] = dt_predictions\ndf_predictions[\"KNN Prediction\"] = knn_predictions\n\ndropIndexes = df_predictions[df_predictions[\"Decision Tree Prediction\"] == df_predictions[\"KNN Prediction\"]].index\ndf_predictions.drop(dropIndexes , inplace=True)\ndf_predictions.head()","64b8cbce":"# Hyperparameter Tuning\nfrom sklearn.model_selection import GridSearchCV\n\n### HYPERPARAMETER SEARCH ###\n########################################\ndef run_grid_search_cv(param_grid, model):\n    grid_search = GridSearchCV(model, # Base model\n                               param_grid, # Parameters to try\n                               cv = 5, # Apply 5-fold cross validation\n                               verbose = 2, # Print summary\n                               n_jobs = -1 # Use all available processors\n                              )\n    grid_search.fit(X_train, y_train)\n    print(f\"Best params: {grid_search.best_params_}\")\n    print(f\"Best score: {grid_search.best_score_}\")\n    return grid_search","202b0833":"# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.tree.DecisionTreeClassifier.html\nparam_grid={\n    'dt__max_depth': [5, 10, 25, 50, 100], \n    'dt__min_samples_leaf': [1, 2, 4, 6, 8],\n    'dt__min_samples_split': [1, 2, 3, 4, 5],\n}\ndt_grid_search = run_grid_search_cv(param_grid, dt_pipeline)","c0e4b14d":"# Use the fitted model to make predictions on the test dataset\nprint('Model performance on the test set:')\ntest_model_performance(X_test, y_test, \"DT\", dt_grid_search)","2721e241":"# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.tree.DecisionTreeClassifier.html\nparam_grid={'knn__n_neighbors': list(range(1, 10))}\nknn_grid_search = run_grid_search_cv(param_grid, knn_pipeline)","00d2422a":"# Use the fitted model to make predictions on the test dataset\nprint('Model performance on the test set:')\ntest_model_performance(X_test, y_test, \"KNN\", knn_grid_search)","62985f05":"pred_data = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\nX_data = pred_data[model_features]\nknn_predictions = knn_grid_search.predict(X_data)\n\noutput = pd.DataFrame({'PassengerId': pred_data.PassengerId, 'Survived': knn_predictions})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","0cc287cd":"Now that our data is cleaned up we are good to split it up for training our model! What we are doing here is taking some of our data and removing it from our sight. The goal is to build up our model with most of our data and then we can test it\u2019s accuracy with the data we put aside. We\u2019re going with option **A** for our models since the models we are going to use don\u2019t require validation data for training. If you were building up a more complicated model like a neural network, you would want to go with option **B** and take another chunk of your data away for validation. We are going to use the [train_test_split](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.train_test_split.html) function to randomly set aside 10% of our data for testing later. After that, we\u2019ll print the shape of our dataframes.\n\n![](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/b\/bb\/ML_dataset_training_validation_test_sets.png)","c52cb5e7":"# Loading the data\nAfter we have found the data, we'll load it into a pandas [dataframe](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.html). Think of a dataframe as a spreadsheet in the computer\u2019s memory. We\u2019ll use it to take a look at and manipulate the data. Calling the [shape](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.shape.html) property of a dataframe will output the dataframe's size as (rows, columns).","e017d2ac":"We\u2019re able to easily see the outliers in the numerical data now. Let\u2019s remove them from our dataframe.","1cfe1611":"The numerical data is not as skewed now. Let\u2019s take a look at the categorical data next. First, I will convert all of the features I picked as categorical data into strings. I do this so that any categories that have a numerical name (<cough>class<cough>) are compared by name later instead of the value these numbers have. I don\u2019t want 3rd class to mean more to the model then 1st class.","eb484432":"# Initial Setup\nFirst we'll make sure our dependencies are up to date.","a22f71e4":"We do want to check for outliers in our data. Many machine learning models are very sensitive to the outliers in the training data, while others are less sensitive. The models we are using today can be distracted by outliers. Let\u2019s use the [value_counts](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.Series.value_counts.html#pandas.Series.value_counts) function to group unique values of our data into buckets. Let\u2019s take a look at the numerical data first.","88986bb2":"We'll need to find the dataset on disk. This is some boilerplate code that comes with every notebook.","03d7dd8c":"If you wanted to view an image instead of looking at numbers, you can use the [hist](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.hist.html?highlight=hist#pandas.DataFrame.hist) function to do so.","269754d2":"We\u2019ll add some new columns on our dataframe to show the predictions from each model","ce7d0ece":"We\u2019re not going to act on this pattern in this model. I invite you to read through the notebook and double back afterward to see what acting on it affects your model performance.\n\nWhat we are going to do now is pick out some features we think we can design a model on. These features might be what we as humans would look at to make our own decision if we had to. I picked out a few that looked interesting to me for this model. We also want to pick out the feature we want to train our model to predict.","595050f4":"A lot of the predictions are the same. We\u2019ll take out the matching predictions so that we can see the differences in the models.","3089f28a":"# Data Preparation\n\nNow that we have a list of features we want to look at, we may want to see if there is a correlation between any of them. If your dataset has perfectly positive or negative attributes then there is a high chance that the performance of the model will be impacted by a problem called \u2014 \u201cMulticollinearity\u201d. **Multicollinearity** happens when one predictor variable in a multiple regression model can be linearly predicted from the others with a high degree of accuracy. This can lead to skewed or misleading results.\n\nWe're not building a regression model (we're building a classification model), but here is how you could show correlations on your numerical features. You could use the [corr](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.corr.html) function to show negative or positive correlations in your data.","b5928b67":"So after we train and measure our models we see that the accuracy of the predictions is pretty high. That\u2019s good right? Well...we don\u2019t know yet. What could be happening is the model is \u201cremembering\u201d the results of the testing data and adjusting itself to predict for only values of the testing data. This is why we set aside 10% of our data earlier. Let\u2019s run that data though the pipeline and have the models predict values on that data. We can run a report on those values and see how well the models perform.","1b7ab272":"One last thing we can do to our models is perform a hyperparameter search. A Hyperparameter  is a parameter whose value is used to control the learning process. For example, our knn model has a single hyperparameter called k that we can adjust to see if the model performs better under different values of k. Each model you train will have different hyperparameters you can adjust, so you will need to adjust\/pick hyperparameters on a model-by-model basis. Using [GridSearchCV](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.GridSearchCV.html?highlight=gridsearchcv#sklearn.model_selection.GridSearchCV) function you can bruteforce search for the best combination of hyperparameters. After each search you can print out the best combination found and the accuracy that combination generated using the *best_params_* and *best_score_* properties.\n\nGridSearchCV will return a model that you run measurements on.","110c4754":"The last thing we need to prepare our data is we need to make sure that we have even samples of our target data. This needs to be done because some models are sensitive to a majority class and will start to \u201clearn\u201d to put data its never seen before into that class. Let\u2019s take a look to see if our data is imbalanced:","9145cd82":"Taking a quick look at the top 10 rows we can see that there are some categorical and numerical datapoints that we can work with. We can also see some missing datapoints in the data. Missing data is printed as **NaN**. To get a total count of missing datapoints, we can call the [info](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.info.html) function on the dataframe.","47ca2386":"We will also bundle these transformations into something called a \u201cdata pipeline\u201d. Data pipelines are great because they allow you to reuse your transformation later on data that you have never seen. To build a data pipeline you would use the [Pipeline](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.pipeline.Pipeline.html?highlight=pipeline#sklearn.pipeline.Pipeline) class.\n\nWe\u2019ll try to build up two different pipelines and compare their results. One model will use the [decision tree](https:\/\/en.wikipedia.org\/wiki\/Decision_tree_learning#General) algorithm and the other will use the [k-nearest neighbors(knn)](https:\/\/en.wikipedia.org\/wiki\/K-nearest_neighbors_algorithm) algorithm.\n* In knn, the output is a class membership. An object is classified by a plurality vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors.\n* In decision trees; leaves represent class labels and branches represent conjunctions of features that lead to those class labels.\nWhat we will do is build up two pipelines; one for each model that we will train. Each pipeline will use the same data transformations that we decided on earlier.\n","bb06c7a9":"# Training the Model\nAt this point we are ready to train our models! We\u2019ll train our models with the training data we generated earlier by running the [fit](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.pipeline.Pipeline.html?highlight=pipeline#sklearn.pipeline.Pipeline.fit) function on our pipelines. It\u2019s common nomenclature in machine learning to call the target feature your trying to make predictions as a **Y** value and the data you use to make those predictions on your **X** values.","be2430be":"Outliers are extreme values that we come across, where they may be influential to the model or not. When it comes to categorical data (say Gender: as in male and female). There's no way of any outlier detection in that. If you mean something like this: You take a sample of 10 with 9 males and 1 female. So you mean that \"1 female\" is an outlier? NO! It's just the composition of the sample which you have selected. If you want to get the value counts of the data you can do so like this:","8995a1eb":"# Training the Model\nLet's take a look on how the models did. The following function will print up a report that will output a confusion matrix and the value for precision and recall.\n\nA confusion matrix provides a summary of the predictive results in a classification problem. Correct and incorrect predictions are summarized in a table with their values and broken down by each class:\n![](https:\/\/miro.medium.com\/max\/746\/0*wKaznIJzZF54b87B.jpg)\n\n* Precision attempts to answer the following question: *What proportion of positive identifications was actually correct?* It is translated to TP \/ (TP + FP), or True Positive \/ Predictive Results.\n* Recall attempts to answer the following question: *What proportion of actual positives was identified correctly?* It is translated to TP \/ (TP + FN), or True Positive Actual Results.\n\nIt is difficult to compare two models with different Precision and Recall. So to make them comparable, we use **F-Score**. It is the Harmonic Mean of Precision and Recall. As compared to Arithmetic Mean, Harmonic Mean punishes the extreme values more. F-score should be high. To keep it simple you should want a F-Score closer to 1.","5f96f044":"After running the testing data though the models we see the metrics are less impressive than the metrics we got before. This is a phenomenon called overfitting and it means just what we predicted, the model is \u201cremembering\u201d our test data and adjusting itself to perform better on that data. How can you get your model to not overfit? Well you could try modifying your data to generate more features, or you could collect more data to train on. I invite you to change the features we train the model on and see how it impacts the performance and overfitting.\n\n# Making Predictions\nWe have two good models that perform pretty well. We can use them to predict on data that we haven\u2019t seen before and that don\u2019t have the target we\u2019re looking for. We\u2019ll load up some data into another dataframe and make some predictions on it using our pipelines and the predict function.","0385398e":"We run the describe function to look for outliers in our data. Outliers can impact our model's performance. Taking a look above we can look at the Fare feature for example. I would wager a ticket that was purchased for $512.33 is an outlier in 1912. Since we can see that we have outliers, we'll want to investigate the data more and remove them.\n\nPart of the data investigation is to look for patterns in the data. You can decide if you want to design a model based around the patterns you discover. For example, we could take a look and see if there\u2019s a patterns of survivability and being female:","63c5d7af":"We can see that we are missing a lot of values in the **Cabin** column. We will opt to not use that feature to train our model (colums are called features in ML. I'll be referring to columns as features for the rest of the notbook). We are missing a few rows for Age, but we can try imputing these missing values later. Let's get some descriptive statistics about the numerical fields by running the [describe](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.describe.html#pandas.DataFrame.describe) function.","45839d06":"Let's take a look at the data after we removed the outliers.","7a5eaf7a":"We\u2019ll fix this imbalance data by randomly [upsampling](https:\/\/en.wikipedia.org\/wiki\/Upsampling) data in the majority class.","e1972622":"# Data Investigation \nThe [head](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.head.html?highlight=head#pandas.DataFrame.head) function can be used to print the first *n* rows of the dataframe. Let's take a look at the top 10 rows.","534929a6":"You can visualize the transformations by running the following code:","7c081d15":"This concludes the notebook walkthrough of the Titanic data and how to build a model on the data. I invite you to try rebuilding the model after picking some features from the data and see how it compares to mine. The code below can be used to generate a csv file that you can submit for the competition.","57e4731b":"# Feature Generation and Building a Data Pipeline\n\nNow that the data has been cleaned up, we can use the data to generate more features to train our model on. An example of one such transformation is one-hot-encoding. This is when you take a categorical feature and expand it into multiple true\/false features.\n\n![](https:\/\/i.imgur.com\/mtimFxh.png)\n\nAnother transformation that can be done is missing values in numerical data and be imputed by taking the average value from the rest of the feature values. An example is if you have a dataset of people where the average height is 6 ft tall, a safe guess for missing values of height would be a value that\u2019s close to 6 ft.\n\nAnother transformation that can be done is missing values in numerical data and be imputed by taking the average value from the rest of the feature values. An example is if you have a dataset of people where the average height is 6 ft tall, a safe guess for missing values of height would be a value that\u2019s close to 6 ft. We\u2019ll perform both such transformations on our dataset using a [ColumnTransformer](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.compose.ColumnTransformer.html). "}}