{"cell_type":{"5d1c76a8":"code","1ecb226e":"code","95a915dc":"code","285969d5":"code","807eb111":"code","4acfae3e":"code","26133f0f":"code","e43f2105":"code","80d3242c":"code","43c5a8a5":"code","28f1b0db":"code","e2ebd286":"code","838954f6":"markdown","c7fdc5de":"markdown","cea6d37a":"markdown"},"source":{"5d1c76a8":"# encoding=utf8  \nimport sys\n\ntry:\n    stdin, stdout, stderr = sys.stdin, sys.stdout, sys.stderr\n    reload(sys)  \n    sys.stdin, sys.stdout, sys.stderr = stdin, stdout, stderr\n    sys.setdefaultencoding('utf8')\nexcept:\n    pass\n\nrandom_state = 101\nRS = 101\nimport time, os\n\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom scipy.sparse import csr_matrix\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.utils import class_weight\n\nfrom sklearn.svm import LinearSVC, SVC\nfrom sklearn.linear_model import RidgeClassifier, SGDClassifier, LogisticRegression\nfrom sklearn.linear_model import Perceptron, PassiveAggressiveClassifier\nfrom sklearn.naive_bayes import BernoulliNB, MultinomialNB\nfrom sklearn.neighbors import KNeighborsClassifier, NearestCentroid\nfrom sklearn.ensemble import RandomForestClassifier,ExtraTreesClassifier,VotingClassifier,GradientBoostingClassifier,AdaBoostClassifier\n\nimport pandas as pd\nimport numpy as np\nimport json, re, warnings\nimport pdb\nwarnings.filterwarnings('ignore')\nwnl = WordNetLemmatizer()\n#https:\/\/www.kaggle.com\/ashishpatel26\/multimodel-approach","1ecb226e":"def pre_processing_(recipe):\n    wnl = WordNetLemmatizer()\n    recipe = [str.lower(ingredient) for ingredient in recipe]\n    recipe = [delete_brand_(ingredient) for ingredient in recipe]\n    recipe = [delete_state_(ingredient) for ingredient in recipe]\n    recipe = [delete_comma_(ingredient) for ingredient in recipe]\n    recipe = [original_(ingredient) for ingredient in recipe]\n    recipe = [delete_space_(ingredient) for ingredient in recipe if len(delete_space_(ingredient))>2]\n    return recipe\n\n# 2. \ndef delete_brand_(ingredient):\n    ingredient = re.sub(\"country crock|i can't believe it's not butter!|bertolli|oreo|hellmann's\", '', ingredient)\n    ingredient = re.sub(\"red gold|hidden valley|original ranch|frank's|redhot|lipton\", '', ingredient)\n    ingredient = re.sub(\"recipe secrets|eggland's best|hidden valley|best foods|knorr|land o lakes\", '', ingredient)\n    ingredient = re.sub(\"sargento|johnsonville|breyers|diamond crystal|taco bell|bacardi\", '', ingredient)\n    ingredient = re.sub(\"mccormick|crystal farms|yoplait|mazola|new york style panetini\", '', ingredient)\n    ingredient = re.sub(\"ragu|soy vay|tabasco|truv\u00eda|crescent recipe creations|spice islands\", '', ingredient)\n    ingredient = re.sub(\"wish-bone|honeysuckle white|pasta sides|fiesta sides\", '', ingredient)\n    ingredient = re.sub(\"veri veri teriyaki|artisan blends|home originals|greek yogurt|original ranch\", '', ingredient)\n    ingredient = re.sub(\"jonshonville\", '', ingredient)\n    ingredient = re.sub(\"old el paso|pillsbury|progresso|betty crocker|green giant|hellmanns|hellmann\u00e2\u20ac\", '', ingredient)\n    ingredient = re.sub(\"oscar mayer deli fresh smoked\", '', ingredient)\n    return ingredient\n\n# 3. \uc7ac\ub8cc \uc190\uc9c8, \uc0c1\ud0dc\ub97c \uc81c\uac70\ud558\ub294 \ud568\uc218\ndef delete_state_(ingredient):\n    ingredient = re.sub('frozen|chopped|ground|fresh|powdered', '', ingredient)\n    ingredient = re.sub('sharp|crushed|grilled|roasted|sliced', '', ingredient)\n    ingredient = re.sub('cooked|shredded|cracked|minced|finely', '', ingredient)        \n    return ingredient\n\n# 4. \ucf64\ub9c8 \ub4a4\uc5d0 \uc788\ub294 \uc7ac\ub8cc\uc190\uc9c8\ubc29\ubc95\uc744 \uc81c\uac70\ud558\ub294 \ud568\uc218\ndef delete_comma_(ingredient):\n    ingredient = ingredient.split(',')\n    ingredient = ingredient[0]\n    return ingredient\n\n## \uadf8\uc678 \uc804\ucc98\ub9ac \ud568\uc218 (\uc22b\uc790\uc81c\uac70, \ud2b9\uc218\ubb38\uc790\uc81c\uac70, \uc6d0\ud615\uc73c\ub85c\ubcc0\uacbd)\ndef original_(ingredient):\n    # \uc22b\uc790\uc81c\uac70\n    ingredient = re.sub('[0-9]', '', ingredient)\n    # \ud2b9\uc218\ubb38\uc790 \uc81c\uac70\n    ingredient = ingredient.replace(\"oz.\", '')\n    ingredient = re.sub('[&%()\u00ae\u2122\/]', '', ingredient)\n    ingredient = re.sub('[-.]', '', ingredient)\n    # lemmatize\ub97c \uc774\uc6a9\ud558\uc5ec \ub2e8\uc5b4\ub97c \uc6d0\ud615\uc73c\ub85c \ubcc0\uacbd\n    ingredient = wnl.lemmatize(ingredient)\n\n    return ingredient\n\n# \uc591 \ub05d \uacf5\ubc31\uc744 \uc81c\uac70\ud558\ub294 \ud568\uc218\ndef delete_space_(ingredient):\n    ingredient = ingredient.strip()\n    return ingredient","95a915dc":"lemmatizer = WordNetLemmatizer()\n\nreplaceC = [('  ', ' '),('  ', ' '),('\u00fa', 'u'), ('\u00e9', 'e'), ('\u00e8', 'e'), ('\u00ee', 'i'), ('\u00e2', 'a'), ('\u00ed', 'i'), ('\u00e2\u20ac\u2122', ''), ('\u20ac', ''), ('\u00e7', 'c'), \n            ('half & half', 'halfandhalf'),('half half', 'halfandhalf'),  ('all purpose', ''), \n            ('seasoning mix', 'seasoning'), ('style seasoning', 'seasoning'), ('salt free', 'saltfree'), ('taco seasoning reduced sodium', 'taco seasoning'),\n            ('less sodium taco seasoning', 'low sodium taco seasoning'), ('old el paso', ''), ('old bay','oldbay'), \n            ('jerk rub', 'jerk'), ('taco bell home originals', ''), ('taco bell', ''), ('low sodium', ''),\n            #('cauliflorets', 'florets'), \n            ('frozen broccoli florets', 'broccoliflorets'), ('broccoli florets', 'broccoliflorets'),\n            ('high gluten bread flour', 'highglutenflour'), ('high gluten flour', 'highglutenflour'), \n            \n            ('chapatti', 'chapati'), ('corn flour', 'cornflour'), ('not low fat', ''), ('low fat', ''), \n            ('cauliflorets florets','cauliflorets'),  ('cauliflorets flowerets','cauliflorets'), ('cauliflower','cauliflorets'), ('cauliflower florets','cauliflorets'), ('caulifloretsets', 'cauliflorets'),\n            \n            ('bread flour','flour'), ('self raising flour','selfraisingflour'), ('self rising cake flour','selfraisingflour'), ('self rising flour','selfraisingflour'),\n            \n            \n           ]\n#rmC = ['\u00ae', '\u2122', ',', '.', '!', '(', ')', '%','\u2019','\"',\"'\", '-']\nrmC = u'\u00ae\u2122,.;!()[]\\'\u2019\"-_@#$%&`\/'\np0 = re.compile(r'\\s*(oz|ounc|ounce|pound|lb|inch|inches|kg|to)\\s*[^a-z]')\n\ndef preprocess_ing(Y, doReplaces=True, rmSpaces=False):\n    if len(Y)<2: return []\n    if rmSpaces: Y = [_.replace('  ',' ').replace(' ','_') for _ in Y]\n    ingredients_text = ' '.join(Y)\n    ingredients_text = ingredients_text.lower()\n    #ingredients_text = ingredients_text.replace('-', ' ').replace('\u2122', ' ').replace('\u00ae', ' ').replace(',', ' ')\n    #for rm in rmC: ingredients_text = ingredients_text.replace(rm, ' ')\n    if doReplaces:\n        for rm in rmC: \n            try:\n                #ingredients_text = ingredients_text.encode('utf-8').replace(rm.encode('utf-8'), ' ')\n                ingredients_text = ingredients_text.replace(rm, ' ')\n            except  Exception as e:\n                print (e)\n                print(rm)\n                print(ingredients_text)\n        for rc in replaceC:  ingredients_text = ingredients_text.replace(rc[0], rc[1])\n        for rc in replaceC:  ingredients_text = ingredients_text.replace(rc[0], rc[1])\n        if rmSpaces:\n            for rc in replaceC[2:]:\n                ingredients_text = ingredients_text.replace(rc[0].replace(' ','_'), rc[1].replace(' ','_'))\n\n    words = []\n    if rmSpaces: split_char='_'\n    else: split_char=' '\n    for word in ingredients_text.split(split_char):\n        word = re.sub(p0, split_char, word.strip())\n        if re.findall('[0-9]', word): continue\n        if len(word) <= 2: continue\n        if '\u2019' in word: continue\n        word = lemmatizer.lemmatize(word)\n        if len(word) > 0: words.append(word)\n    return words\n\ndef preprocess_data(X, preproMode, doReplaces=True, rmSpaces=False):\n    _X = []\n    for x in X:\n        if preproMode=='KK': x['ingredients'] = preprocess_ing(x['ingredients'], doReplaces=doReplaces, rmSpaces=rmSpaces)\n        elif preproMode=='AV': x['ingredients'] = pre_processing_(x['ingredients'])\n        _X.append(x)\n    return _X\n\n\ndef conv_secs(s):\n    mins, secs = divmod(s,60)\n    if mins>=60: hours, mins = divmod(mins,60)\n    else: hours=0\n    if hours<24: return '%02d:%02d:%02d' % (hours, mins, secs)\n    else:\n        days, hours = divmod(hours,24)\n        return '%02d-%02d:%02d:%02d' % (days, hours, mins, secs)\n\ndef do_train_fit(model, model_name, USEOVR):\n    #if 'XGB' in model_name:\n    #    model._Booster.set_param({'num_class': len(lb.classes_)})\n    #    xgb_param = model.get_xgb_params()\n    #    xgb_param['num_class'] = 3\n    t0 = time.time()\n    #if len(best_params)>0:model=model(**best_params)\n    if USEOVR: \n        model = OneVsRestClassifier(model, n_jobs=-1)\n        model_name = 'OVR_'+model_name\n    model.fit(X_train, y_train)\n    #print(\"Test the %s on the validation data\" % model_name)\n    y_pred_valid = model.predict(X_valid)\n    y_pred_train = model.predict(X_train)\n    need_time = time.time()-t0\n    print ('%s %s Acc_T: %.4f Acc_V: %.4f [%s]' % (tfidf_name, model_name, accuracy_score(y_pred_train, y_train), accuracy_score(y_pred_valid, y_valid), conv_secs(need_time)))\n    return model, model_name, y_pred_valid\n    ","285969d5":"# Dataset Preparation\nprint (\"Read Dataset ... \")\ndef read_dataset(path): \n    try:return json.load(open(path, encoding='utf-8')) \n    #except:return json.load(open(path), encoding='utf-8') \n    except:return json.load(open(path)) \n\n_HOMEFOLDER_ = os.environ['HOME']\nDATA_FOLDER = '..\/input\/'\n#DATA_FOLDER = _HOMEFOLDER_+'\/Dropbox\/Learning\/Kaggle\/WhatsCooking\/'\n\ntrain = read_dataset(DATA_FOLDER+'train.json')\nsubmission = read_dataset(DATA_FOLDER+'test.json')\n\n# prepare X and y\ntarget = [doc['cuisine'] for doc in train]\nprint (\"Label Encode the Target Variable ... \")\nlb = LabelEncoder()\ny = lb.fit_transform(target)\nclass_weights_ = class_weight.compute_class_weight('balanced', np.unique(y), y)\nclass_weights = {}\nfor i in range(len(class_weights_)): class_weights[i]=100\/class_weights_[i]\n\nX = train\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.00010, random_state=random_state)\n\nprint(len(X_train), len(X_valid))\n# Text Data Features\nprint (\"Prepare text data  ... \")\ndef generate_text(data, preproMode, doReplaces=True, rmSpaces=False):\n    data = preprocess_data(data, preproMode=preproMode, doReplaces=doReplaces, rmSpaces=rmSpaces)\n    text_data = [\" \".join(doc['ingredients']) for doc in data]\n    return text_data \n\ndoReplaces = False\nrmSpaces   = False\npreproMode = 'KK'\npreproMode = 'AV' #https:\/\/www.kaggle.com\/ashishpatel26\/up-to-all-food-and-all-models\n\ntrain_text = generate_text(X_train, preproMode=preproMode, doReplaces=doReplaces, rmSpaces=rmSpaces)\nvalid_text = generate_text(X_valid, preproMode=preproMode, doReplaces=doReplaces, rmSpaces=rmSpaces)\nsubmission_text = generate_text(submission, preproMode=preproMode, doReplaces=doReplaces, rmSpaces=rmSpaces)\n\nprint (\"Generation Done! \")","807eb111":"#pre_processing_(X_train[0]['ingredients'])","4acfae3e":"# Feature Engineering \nprint (\"TF-IDF on text data ... \")\n#tfidf_name, tfidf = 'bT_SWN_xD100_nD1_sF', TfidfVectorizer(binary=True,stop_words=None,max_df=1.0, min_df=1,sublinear_tf=False)\n#tfidf_name, tfidf = 'bF_SWN_xD090_nD2_sF', TfidfVectorizer(binary=False,stop_words=None,max_df=0.9, min_df=2,sublinear_tf=False)\n#tfidf_name, tfidf = 'bT_SWE_xD065_nD2_sF', TfidfVectorizer(binary=True,stop_words='english', max_df=0.65, min_df=2,sublinear_tf=False)\n#tfidf_name, tfidf = 'nP_bF_SWN_xD075_nD2_sT', TfidfVectorizer(binary=False,stop_words=None,max_df=0.75, min_df=2, sublinear_tf=True)\n#tfidf_name, tfidf = 'bF_SWN_xD070_nD4_sT', TfidfVectorizer(binary=False,stop_words=None,max_df=0.7, min_df=4,sublinear_tf=True)\ntfidf_name, tfidf = 'bF_SWN_xD060_nD1_sT_nl2', TfidfVectorizer(binary=False,stop_words=None,max_df=0.6, min_df=1,sublinear_tf=True, norm='l2')\n\nif rmSpaces: tfidf_name = 'RmT_'+tfidf_name\nelse: tfidf_name = 'RmF_'+tfidf_name\n    \nif doReplaces: tfidf_name = 'RpT'+tfidf_name\nelse: tfidf_name = 'RpF'+tfidf_name\n\ntfidf_name=preproMode+'_'+tfidf_name\n    \ndef tfidf_features(txt, flag):\n    if flag == \"train\":\n        x = tfidf.fit_transform(txt)\n    else:\n        x = tfidf.transform(txt)\n    x = x.astype('float16')\n    # x.sort_indices()\n    return x \n\nX_train = tfidf_features(train_text, flag=\"train\")\nX_valid = tfidf_features(valid_text, flag=\"valid\")\nX_submission = tfidf_features(submission_text, flag=\"submission\")\n\nprint (\"Feature Engineering [%s] DONE\" % tfidf_name)","26133f0f":"model_njobs = -1\nmodel_list = [ # Must be ('Model_Name', model(wanted=params), {params:for_GSearch})\n    # 0\n    ('SVC_C50_g1p4_d3_c1', \n     SVC(C = 50, gamma=1.4, kernel='rbf', degree=3, coef0=1, shrinking=True, tol=0.0001, probability=True,\n             cache_size=2000, class_weight=None, verbose=False, max_iter=-1, decision_function_shape=None, random_state=RS), \n     {'C':[30, 50, 100, 150, 220], 'gamma': [0.1, 0.75, 1, 3, 5], 'degree': [1,2,3,4], \n      'coef0': [0, 0.5, 1], 'kernel':('linear', 'rbf', 'poly')},\n    {}),\n    # 1\n    ('LR_C7_l2_NewCG_max200_dF', \n     LogisticRegression(penalty='l2', C=8,\n                        max_iter = 200, dual = False,\n                        solver='newton-cg',\n                        multi_class='ovr',\n                        n_jobs = model_njobs,\n                        random_state=RS), \n     {'C': [0.1,0.5,1,5,10,15], 'solver': ['sag', 'newton-cg', 'lbfgs'], \n      'max_iter':[100,200,300], 'dual':[True, False], 'multi_class': ['multinomial', 'ovr']},\n    {}), \n    # 2\n    ('RFC_150_None', \n     RandomForestClassifier(n_estimators=150, criterion='gini', max_depth=None, min_samples_split=2, \n                            max_features='log2', max_leaf_nodes=None, min_impurity_decrease=0.0, \n                            min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=model_njobs, \n                            random_state=RS, verbose=0, warm_start=False, class_weight=None),\n    {'n_estimators':[25, 50, 100, 150, 200], 'max_features':['sqrt', 'log2', None], },\n    {'n_estimators':150, 'max_features':'log2'}),\n    # 3\n    ('SGD_l_l2_opt_a1m5', \n     SGDClassifier(loss='log', penalty='l2', alpha=1e-05, l1_ratio=0.15, \n                   fit_intercept=True, max_iter=1000, tol=1e-03, shuffle=True, \n                   verbose=0, epsilon=0.1, n_jobs=model_njobs, random_state=RS, \n                   learning_rate='optimal', eta0=0.0, power_t=0.5, class_weight=None, \n                   warm_start=False, average=False, n_iter=None),\n     {'loss': ['hinge','log','modified_huber','squared_hinge','perceptron',], \n      'alpha': [1.0e-6,1.0e-5, 1.0e-4, 1.0e-3, 1.0e-2], 'learning_rate': ['optimal']},\n    {'loss': 'log', 'learning_rate': 'optimal', 'alpha': 1e-05}),\n    # 4\n    ('LinSVC', LinearSVC(), {}, {}),\n    \n    #5\n    ('ABC', \n     AdaBoostClassifier(base_estimator=None, n_estimators=50, learning_rate=1.0, \n                        algorithm='SAMME.R', random_state=RS), \n     {'n_estimators':[25, 50, 100, 150, 200], 'algorithm': ['SAMME.R', 'SAMME']}, {}),\n    \n    #6\n    ('GBC',\n     GradientBoostingClassifier(loss='deviance', learning_rate=0.1, n_estimators=100, subsample=1.0, \n                                criterion='friedman_mse', min_samples_split=2, max_depth=3, \n                                min_impurity_decrease=0.0, min_impurity_split=None, init=None, \n                                random_state=RS, max_features=None, verbose=0, max_leaf_nodes=None, \n                                warm_start=False, presort='auto', \n                                #validation_fraction=0.1, n_iter_no_change=None, tol=0.0001\n                               ), \n     {'loss':['deviance','exponential'], 'n_estimators':[100, 150, 200, 300], 'max_features':['sqrt', 'log2', None]}, \n     {}),\n    \n    #7\n    ('Perceptron_50', Perceptron(penalty=None, alpha=0.0001, fit_intercept=True, max_iter=None, tol=1e-4, shuffle=True, \n                                 verbose=0, eta0=1.0, n_jobs=model_njobs, random_state=RS, class_weight=None, \n                                 warm_start=False, n_iter=None),\n     {'penalty':['l2', 'l1', 'elasticnet', None], 'max_iter':[10,100,500,1000], 'eta0':[0.5,1.0,1.5]}, {}),\n    \n    #8\n    ('KNC_50', KNeighborsClassifier(n_neighbors=100, weights='uniform', algorithm='auto', leaf_size=30, \n                                                     p=2, metric='minkowski', metric_params=None, n_jobs=model_njobs),\n     {'weights':['distance', 'uniform'], 'algorithm':['ball_tree', 'kd_tree', 'brute'], 'n_neighbors': [10,20,30,50,70], \n      'leaf_size': [10,15,20,30]},\n     {}),\n    \n    #9\n    ('LinearSVC_l2_dF', LinearSVC(penalty='l2', loss='squared_hinge', dual=False, tol=1e-4, C=1.0, multi_class='ovr', \n                                  fit_intercept=True, intercept_scaling=1, class_weight=None, verbose=0, random_state=RS, max_iter=2000),\n     {'dual':[True, False], 'C': [0.1, 0.25, 0.4, 0.5, 0.6, 0.75, 1.0, 1.25, 1.5, 2, 3, 4]},{}),\n    #10\n    ('SVC_kg', SVC(C=200, kernel='rbf', degree=3,gamma=1, coef0=1, shrinking=True,tol=0.001, probability=False, \n                   cache_size=200,class_weight=None, verbose=False, max_iter=-1,decision_function_shape=None,random_state=RS), {},{}),\n    #11\n    ('RFC', \n     ExtraTreesClassifier(n_estimators=50, criterion='gini', max_depth=None, min_samples_split=2, \n                          #min_samples_leaf=1, min_weight_fraction_leaf=0.0, \n                          max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, \n                          min_impurity_split=None, bootstrap=False, oob_score=False, n_jobs=model_njobs, random_state=RS, verbose=0, \n                          warm_start=False, class_weight=None), \n    {'max_features':['sqrt', 'log2', None],'n_estimators':[100, 500],'min_samples_leaf': [10,50,100,200,500]}, {}),\n    #12\n    ('XGB', \n     #XGBClassifier(max_depth = 9, eta = 0.003, subsample = 0.7, gamma = 7, n_jobs=model_njobs,), \n     XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n       colsample_bytree=1, gamma=1, learning_rate=0.01, max_delta_step=0,\n       max_depth=12, min_child_weight=1, missing=None, n_estimators=1000,\n       n_jobs=2, nthread=None, objective='multi:softprob', random_state=0,\n       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n       silent=True, subsample=0.8),\n     {}, {}),\n]","e43f2105":"if True:\n    SEL_MODEL = 0\n    model_name, model, gs_params, best_params = model_list[SEL_MODEL]\n    #model, model_name, y_pred_valid = do_train_fit(model, model_name, False)\n    model, model_name, y_pred_valid = do_train_fit(model, model_name, True)","80d3242c":"#?XGBClassifier\n#RpFRmF_bF_SWN_xD060_nD1_sT_nl2 OVR_XGB Acc_T: 0.8636 Acc_V: 0.7748 [00:03:50]\n#dir(model)\n#model._Booster.set_param({'num_class': len(lb.classes_)})\n\n#print (model._Booster)\n","43c5a8a5":"#RUNMODE = 'KFOLD'\n#RUNMODE = 'DOSCAN_G'\n#RUNMODE = 'DOSCAN_R'\nRUNMODE = 'VOTE_S'\n#RUNMODE = 'VOTE_H'\n#RUNMODE = 'FIT'\n#RUNMODE = 'FIT_BOTH'\nSEL_MODEL = 0\n\nUSECLASSWEIGHTS = True\nUSECLASSWEIGHTS = False\n\nUSEOVR = True\n#USEOVR = False\n\nprint(\"features: \", X_train.shape)\n\nstarttime = time.time()\n\n\nmodel_name, model, gs_params, best_params = model_list[SEL_MODEL]\n\nif 'class_weight' in model.__doc__ and USECLASSWEIGHTS:\n    model_name = 'CW_'+model_name\n    model.__setattr__('class_weight', class_weights)\n\nif RUNMODE.split('_')[0] == 'DOSCAN':\n    if RUNMODE[-1]=='R':clf = RandomizedSearchCV(model, param_distributions=gs_params, n_iter=20, verbose=10, n_jobs=-1)\n    elif RUNMODE[-1]=='G':clf = GridSearchCV(model, gs_params, cv=3, verbose=10, n_jobs=-1)\n    clf.fit(X_train, y_train)\n    print('best score: ', clf.best_score_)\n    print(\"best params: \", clf.best_params_)\n    print('best est: ', clf.best_estimator_)\n    with open(\"bestparams.txt\", \"w\") as data:\n        data.write(str(clf.best_params_))\nelif RUNMODE == 'KFOLD':\n    #K-fold validation\n    from sklearn.model_selection import StratifiedKFold\n\n    kfold = StratifiedKFold(n_splits=5, random_state=random_state).split(X_train, y_train)\n    scores = []\n    for k, (train, test) in enumerate(kfold):\n        training_data = X_train[train]\n        training_data.sort_indices()\n        model.fit(training_data, y_train[train])\n\n        score = model.score(X_train[test], y_train[test])\n        scores.append(score)\n        print('Fold: %2d, Acc: %.3f' % (k+1, score))\n        \nelif RUNMODE == 'FIT':\n    model, model_name, y_pred_valid = do_train_fit(model, model_name, USEOVR)\nelif RUNMODE == 'FIT_BOTH':\n    model, model_name, y_pred_valid = do_train_fit(model, model_name, False)\n    model, model_name, y_pred_valid = do_train_fit(model, model_name, True)\nelif RUNMODE.split('_')[0] == 'VOTE':\n    SEL_MODELS = [0,1,3]\n    WEIGHTS = [5,2,2]\n    vote_est = []\n    voting = 'hard'\n    if RUNMODE[-1]=='S': voting = 'soft'\n        \n    v_model_name = 'V' + voting[0].upper()\n    for _ in SEL_MODELS:\n        vote_est.append((model_list[_][0], model_list[_][1]))\n        v_model_name += '-'+ vote_est[-1][0]\n    print('*'*66)\n    print('*  Will use total %d ests' % len(vote_est))\n    for x,_ in vote_est: print('* %s' % x)\n    model = VotingClassifier(estimators = vote_est,\n                            voting = voting,\n                            weights = WEIGHTS,\n                            n_jobs=-1)\n    if USEOVR: model = OneVsRestClassifier(model, n_jobs=4)\n    model.fit(X_train, y_train)\n    print(\"Test the model on the validation data\")\n    y_pred_valid = model.predict(X_valid)\n    print('Accuracy: %.4f' % accuracy_score(y_pred_valid, y_valid))\n    print ('%s %s' % (tfidf_name, model_name))\n\n\nprint(\"Time: \", conv_secs(time.time()-starttime))","28f1b0db":"# Predictions \nprint (\"Predict on submision data ... \")\ny_submission = model.predict(X_submission)\ny_pred = lb.inverse_transform(y_submission)\n\n# Submission\nprint (\"Generate Submission File ... \")\nsubmission_id = [doc['id'] for doc in submission]\nsub = pd.DataFrame({'id': submission_id, 'cuisine': y_pred}, columns=['id', 'cuisine'])\nsub.to_csv('%s_%s_%s.csv' %(tfidf_name, model_name, str(random_state) ), index=False)","e2ebd286":"if False:\n    im = -1\n    exclude_ids = [0,10]\n    for model_name, model, gs_params, best_params in model_list:\n        im+=1\n        if im in exclude_ids: continue\n        \n        try:\n            print('*'*66)\n            print(im, model_name)\n            #model, model_name, y_pred_valid = do_train_fit(model, model_name, False)\n            try:model, model_name, y_pred_valid = do_train_fit(model, model_name, True)\n            except:pass\n        except Exception as e:\n            print (e)\n","838954f6":"    9 RpFRmF_bF_SWN_xD070_nD4_sT OVR_LinearSVC_l2_dF Acc_T: 0.8487 Acc_V: 0.7949 [00:00:09]\n    7 RpFRmF_bF_SWN_xD070_nD4_sT OVR_Perceptron_50 Acc_T: 0.8037 Acc_V: 0.7142 [00:00:01]\n    6 RpFRmF_bF_SWN_xD070_nD4_sT OVR_GBC Acc_T: 0.8267 Acc_V: 0.7366 [00:02:50]\n    5 RpFRmF_bF_SWN_xD070_nD4_sT OVR_ABC Acc_T: 0.7419 Acc_V: 0.7212 [00:00:41]\n    4 RpFRmF_bF_SWN_xD070_nD4_sT OVR_LinSVC Acc_T: 0.8487 Acc_V: 0.7949 [00:00:03]\n    3 RpFRmF_bF_SWN_xD070_nD4_sT OVR_SGD_l_l2_opt_a1m5 Acc_T: 0.8286 Acc_V: 0.7934 [00:00:07]\n    1 RpFRmF_bF_SWN_xD070_nD4_sT OVR_LR_C7_l2_NewCG_max200_dF Acc_T: 0.8467 Acc_V: 0.7956 [00:00:07]\n    \n    9 RpFRmF_bF_SWN_xD060_nD1_sT_nl2 OVR_LinearSVC_l2_dF Acc_T: 0.8548 Acc_V: 0.7984 [00:00:05]\n    7 RpFRmF_bF_SWN_xD060_nD1_sT_nl2 OVR_Perceptron_50 Acc_T: 0.8096 Acc_V: 0.7265 [00:00:01]\n    6 RpFRmF_bF_SWN_xD060_nD1_sT_nl2 OVR_GBC Acc_T: 0.8285 Acc_V: 0.7371 [00:02:45]\n    5 RpFRmF_bF_SWN_xD060_nD1_sT_nl2 OVR_ABC Acc_T: 0.7400 Acc_V: 0.7167 [00:00:41]\n    4 RpFRmF_bF_SWN_xD060_nD1_sT_nl2 OVR_LinSVC Acc_T: 0.8548 Acc_V: 0.7984 [00:00:01]\n    3 RpFRmF_bF_SWN_xD060_nD1_sT_nl2 OVR_SGD_l_l2_opt_a1m5 Acc_T: 0.8310 Acc_V: 0.7944 [00:00:01]\n    2 RpFRmF_bF_SWN_xD060_nD1_sT_nl2 OVR_RFC_150_None Acc_T: 0.9993 Acc_V: 0.7700 [00:02:31]\n    1 RpFRmF_bF_SWN_xD060_nD1_sT_nl2 OVR_LR_C7_l2_NewCG_max200_dF Acc_T: 0.8515 Acc_V: 0.7984 [00:00:11]\n\n\n\n\n\n","c7fdc5de":"    bF_SWN_xD090_nD2_sF LR_C7_l2_NewCG_max200_dF Acc: 0.7974 [00:00:05]\n","cea6d37a":"# Tries...\n\n    OVR_SVC_50_1 bF_SWN_xD090_nD2_sF\n    0.8215 -> 0.81707\n    \n    OVR_SVC_50_1 bT_SWN_xD100_nD1_sF\n    0.8220 -> 0.80722\n    \n    nP_bF_SWN_xD075_nD2_sT_SVC_50_1\n    0.8110 -> 0.80792\n    \n    RpFRmF_bF_SWN_xD070_nD4_sT OVR_SVC_50_1 Acc: 0.8213 -> 0.81788\n    RpFRmF_bF_SWN_xD070_nD4_sT OVR_SVC_kg Acc: 0.8213  -> 0.81808\n    RpFRmF_bF_SWN_xD060_nD1_sT_nl2 OVR_RFC_150_None Acc_T: 0.9993 Acc_V: 0.7700 [00:02:41] -> 0.76629\n    RpFRmF_bF_SWN_xD060_nD1_sT_nl2 OVR_SVC_50_1 Acc_T: 0.9991 Acc_V: 0.7500 [00:21:18] ->  0.82039\n    RpFRmF_bF_SWN_xD060_nD1_sT_nl2 OVR_SGD_l_l2_opt_a1m5 Acc_T: 0.8304 Acc_V: 0.7500 [00:00:08] -> 0.78740\n    AV_RpFRmF_bF_SWN_xD060_nD1_sT_nl2 OVR_SVC_C50_g1p4_d3_c1 Acc_T: 0.9996 Acc_V: 0.7500 [00:26:02] -> 0.82220\n\n"}}