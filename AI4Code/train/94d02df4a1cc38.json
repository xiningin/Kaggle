{"cell_type":{"1ed1e204":"code","dbabad23":"code","0a4f3fd6":"code","9cadeeca":"code","22568a3e":"code","31c108c4":"code","633e63a2":"code","66d83257":"code","63dc13b2":"code","7b7be1ae":"code","653ed4a7":"code","22c185bb":"code","76d0dcab":"code","f40ec876":"code","91fcb7dd":"code","06bae6ab":"code","3d3a0b8c":"code","04ae696d":"code","6a1d0c39":"code","31eec148":"code","302c45d4":"code","329f0045":"code","d9667652":"code","80ef03cf":"code","d3fa5205":"code","11042931":"code","18264c26":"code","4b4c7e52":"code","af6662bc":"code","8e4bd620":"code","4cce78ae":"code","b95da42e":"code","99a203a1":"code","97ebf6a8":"code","5e3aea69":"code","a9f070a6":"code","eb7eb498":"code","81808ead":"code","050dcc11":"code","18ac823c":"code","8e88ce7b":"code","14b3b68a":"code","4b150777":"code","88c81672":"code","ec449120":"code","d5a62e69":"code","339bbb24":"code","7e04255e":"code","d5cbdc2e":"code","23b87918":"code","23abb8ae":"code","0e57237e":"code","481d15ec":"markdown","1ad2e9c5":"markdown","123b7295":"markdown","7f64397e":"markdown","a554053e":"markdown","d844b7a6":"markdown","91952acb":"markdown","04cc47d4":"markdown","b6fac616":"markdown","c77457ab":"markdown","d6fa60e8":"markdown","f828b22c":"markdown","3d9ac65e":"markdown","3680f21a":"markdown","196886c5":"markdown","6f94a4a2":"markdown","11f4b77b":"markdown"},"source":{"1ed1e204":"import numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix","dbabad23":"import matplotlib.pyplot as plt\nfrom sklearn.metrics import plot_confusion_matrix","0a4f3fd6":"# Specify the input data here\n# After download the full train.csv, you can use command\n# head -n 50000 train.csv train-50k.csv to get a sample\n# For running with full data, you should upload to kaggle\/or server. It needs much more memory\n# input_data = \"..\/data\/raw\/force-well-logs\/train-50k.csv\"\ninput_data = \"..\/input\/forcedataset\/force-ai-well-logs\/train.csv\"\n# test_data = \"..\/input\/forcedataset\/force-ai-well-logs\/test.csv\"","9cadeeca":"# There are two target columns, we should remove from learning\nTARGET_1 = \"FORCE_2020_LITHOFACIES_LITHOLOGY\"\nTARGET_2 = \"FORCE_2020_LITHOFACIES_CONFIDENCE\"\nWELL_NAME = 'WELL'","22568a3e":"def draw_confusion_matrix(model, X_valid, y_valid):\n    fig, ax = plt.subplots(figsize=(6,6))\n    disp = plot_confusion_matrix(model, X_valid, y_valid, normalize = None, xticks_rotation = 'vertical', ax = ax)\n    disp.ax_.set_title(\"Plot Confusion Matrix Not Normalized\")\n    fig1, ax1 = plt.subplots(figsize=(6,6))\n    disp1 = plot_confusion_matrix(model, X_valid, y_valid, normalize = 'true', values_format = \".2f\", xticks_rotation = 'vertical', ax = ax1)\n    disp1.ax_.set_title(\"Plot Confusion Matrix Normalized\")\n    plt.show()","31c108c4":"def calculate_accuracy(y_true, y_pred):\n    cm = confusion_matrix(y_true = y_true, y_pred = y_pred)\n    tp = 0\n    for i in range(len(cm)):\n        tp += cm[i][i]\n    accuracy = 1.0 * tp \/ np.sum(cm)\n    return accuracy","633e63a2":"df = pd.read_csv(input_data, sep=';', nrows = 50000)","66d83257":"df.columns","63dc13b2":"# See how many wells in the first 50k row\nprint('N wells = ', np.unique(df['WELL'].values))","7b7be1ae":"# See how many rock types\nprint('N target classes = ', np.unique(df[TARGET_1].values))","653ed4a7":"#dictionary to translate number into rock types\nlithology_keys = {30000: 'Sandstone',\n                 65030: 'Sandstone\/Shale',\n                 65000: 'Shale',\n                 80000: 'Marl',\n                 74000: 'Dolomite',\n                 70000: 'Limestone',\n                 70032: 'Chalk',\n                 88000: 'Halite',\n                 86000: 'Anhydrite',\n                 99000: 'Tuff',\n                 90000: 'Coal',\n                 93000: 'Basement'}","22c185bb":"df[TARGET_1].value_counts()","76d0dcab":"# We view the NA columns, and drop some columns with many NA\ndf.isna().sum()","f40ec876":"df.isna().sum()","91fcb7dd":"# Describe serveral stats of df\ndf.describe()","06bae6ab":"# See the dtype, because we have to remove string values columns\ndf.dtypes","3d3a0b8c":"# We drop columns which have larger than 10k NA values (20%)\n# and object values\nunused_columns = ['RSHA', 'SGR', 'NPHI', 'BS', 'DTS', 'DCAL', 'RMIC', 'ROPA', 'RXO']\nunused_columns += [WELL_NAME, 'GROUP', 'FORMATION']\n# ADD two target columns into unused columns\nunused_columns += [TARGET_1, TARGET_2]","04ae696d":"# Get the columns features\nall_columns = list(df.columns)\n\nuse_columns = [c for c in all_columns if c not in unused_columns]\nprint(use_columns)","6a1d0c39":"# We provide a simple replace nan value with mean value.\n# This is the simplest\/standard preprocess methods\n\n# with each column in use_columns\n# df[c].mean() computes the mean values without na value\n# df[c].fillna() fill with the above value, use option inplace to provide inplace replacing\nfor c in use_columns:\n    df[c].fillna(df[c].mean(), inplace=True)","31eec148":"# train_wells = list(np.unique(df['WELL'].values))\ntrain_wells = ['15\/9-13', '15\/9-15']\n# Use this condition to find out which rows in the data is select for training\ntrain_mask = df[WELL_NAME].isin(train_wells)","302c45d4":"X_train = df[train_mask][use_columns].values\ny_train = df[train_mask][TARGET_1].values\nprint(X_train.shape, y_train.shape)","329f0045":"X_valid = df[~train_mask][use_columns].values\ny_valid = df[~train_mask][TARGET_1].values\nprint(X_valid.shape, y_valid.shape)","d9667652":"penalty_matrix = np.load(\"..\/input\/penalty-matrix\/penalty_matrix.npy\")","80ef03cf":"# Position of each type of rock in the penalty_matrix\npenalty_dict = {\"Sandstone\": 0,\n                \"Sandstone\/Shale\": 1,\n                \"Shale\": 2, \n                \"Marl\": 3,\n                \"Dolomite\": 4,\n                \"Limestone\": 5,\n                \"Chalk\": 6,\n                \"Halite\": 7,\n                \"Anhydrite\": 8,\n                \"Tuff\": 9,\n                \"Coal\": 10,\n                \"Basement\": 11}","d3fa5205":"# Used for getting the right \"rock number\" from confusion matrix index\ncm_rock_idx = np.unique(df[TARGET_1].values)","11042931":"def calculate_penalty(cm = None, penalty_matrix = None, lithology_dict = None, penalty_dict = None, cm_rock_idx = None):\n    sum_penalty = 0\n    for i in range(len(cm)):\n        for j in range(len(cm)):\n            rock_i = lithology_dict[cm_rock_idx[i]]\n            rock_j = lithology_dict[cm_rock_idx[j]]\n            penalty_i = penalty_dict[rock_i]\n            penalty_j = penalty_dict[rock_j]\n            sum_penalty += cm[i][j] * penalty_matrix[penalty_i][penalty_j]\n    return -1.0 * sum_penalty \/ np.sum(cm)","18264c26":"from xgboost import XGBClassifier","4b4c7e52":"import time","af6662bc":"tree_depth_report = []","8e4bd620":"\nfor max_depth in [3,5,7,9]:\n    \n    xgb_model_maxdepth = XGBClassifier(max_depth = max_depth)\n    start_time = time.time()\n    xgb_model_maxdepth.fit(X_train, y_train)\n    runtime = time.time() - start_time\n    predict_y_train = xgb_model_maxdepth.predict(X_train)\n    predict_y_valid = xgb_model_maxdepth.predict(X_valid)\n    cm_train = confusion_matrix(y_true = y_train, y_pred = predict_y_train)\n    cm_valid = confusion_matrix(y_true = y_valid, y_pred = predict_y_valid)\n    accuracy_on_train = calculate_accuracy(y_train, predict_y_train)\n    accuracy_on_valid = calculate_accuracy(y_valid, predict_y_valid)\n    penalty_train = calculate_penalty(cm_train, penalty_matrix, lithology_keys, penalty_dict, cm_rock_idx)\n    penalty_valid = calculate_penalty(cm_valid, penalty_matrix, lithology_keys, penalty_dict, cm_rock_idx)\n    tree_depth_report.append([max_depth, accuracy_on_train, accuracy_on_valid, penalty_train, penalty_valid, runtime])\n    \n    \n    \n    ","4cce78ae":"tree_depth_report = pd.DataFrame(tree_depth_report).rename(columns = {0: 'depth', 1: 'train_accuracy', 2: 'valid_accuracy', 3: 'penalty_train', 4: 'penalty_valid', 5: 'runtime'})","b95da42e":"tree_depth_report","99a203a1":"learning_rates_report = []","97ebf6a8":"for learning_rate in [0.01, 0.03, 0.1, 0.3, 1, 3]:\n    \n    xgb_model_learning_rate = XGBClassifier(max_depth = 9, learning_rate = learning_rate)\n    start_time = time.time()\n    xgb_model_learning_rate.fit(X_train, y_train)\n    runtime = time.time() - start_time\n    predict_y_train = xgb_model_learning_rate.predict(X_train)\n    predict_y_valid = xgb_model_learning_rate.predict(X_valid)\n    cm_train = confusion_matrix(y_true = y_train, y_pred = predict_y_train)\n    cm_valid = confusion_matrix(y_true = y_valid, y_pred = predict_y_valid)\n    accuracy_on_train = calculate_accuracy(y_train, predict_y_train)\n    accuracy_on_valid = calculate_accuracy(y_valid, predict_y_valid)\n    penalty_train = calculate_penalty(cm_train, penalty_matrix, lithology_keys, penalty_dict, cm_rock_idx)\n    penalty_valid = calculate_penalty(cm_valid, penalty_matrix, lithology_keys, penalty_dict, cm_rock_idx)\n    learning_rates_report.append([learning_rate, accuracy_on_train, accuracy_on_valid, penalty_train, penalty_valid, runtime])","5e3aea69":"learning_rates_report = pd.DataFrame(learning_rates_report).rename(columns = {0: 'learning_rate', 1: 'train_accuracy', 2: 'valid_accuracy', 3: 'penalty_train', 4: 'penalty_valid', 5: 'runtime'})","a9f070a6":"learning_rates_report","eb7eb498":"n_estimators_report = []\nmax_depth = 9\nlearning_rate = 0.1","81808ead":"for n_estimators in [50, 100, 500]:\n    \n    xgb_model_nEstimators = XGBClassifier(max_depth = max_depth, learning_rate = learning_rate, n_estimators = n_estimators)\n    start_time = time.time()\n    xgb_model_nEstimators.fit(X_train, y_train)\n    runtime = time.time() - start_time\n    predict_y_train = xgb_model_nEstimators.predict(X_train)\n    predict_y_valid = xgb_model_nEstimators.predict(X_valid)\n    cm_train = confusion_matrix(y_true = y_train, y_pred = predict_y_train)\n    cm_valid = confusion_matrix(y_true = y_valid, y_pred = predict_y_valid)\n    accuracy_on_train = calculate_accuracy(y_train, predict_y_train)\n    accuracy_on_valid = calculate_accuracy(y_valid, predict_y_valid)\n    penalty_train = calculate_penalty(cm_train, penalty_matrix, lithology_keys, penalty_dict, cm_rock_idx)\n    penalty_valid = calculate_penalty(cm_valid, penalty_matrix, lithology_keys, penalty_dict, cm_rock_idx)\n    n_estimators_report.append([n_estimators, accuracy_on_train, accuracy_on_valid, penalty_train, penalty_valid, runtime])","050dcc11":"n_estimators_report = pd.DataFrame(n_estimators_report).rename(columns = {0: 'n_estimators', 1: 'train_accuracy', 2: 'valid_accuracy', 3: 'penalty_train', 4: 'penalty_valid', 5: 'runtime'})","18ac823c":"n_estimators_report","8e88ce7b":"# from xgboost import XGBClassifier","14b3b68a":"# xgb_model = XGBClassifier()","4b150777":"# xgb_model.fit(X_train, y_train)","88c81672":"# param_grid = [{'max_depth':[6,7,8], 'min_child_weight':[2,3,4]\n#                ,'objective' : ['reg:linear'],'colsample_bytree' : [0.6,0.8,1], 'learning_rate' : [0.001,0.01],\n#               'reg_lambda' : [0.1,0.5], 'n_estimators' : [100, 500]}]\n# model = xgboost.XGBClassifier()\n# grid_search = GridSearchCV(model, param_grid, cv = 7, \n#                          scoring = 'neg_mean_squared_error', \n#                          return_train_score = True, refit = True)\n# grid_search.fit(X_train, y_train)\n# xgb_reg = grid_search.best_estimator_\n# xgb_reg.fit(X_train, y_train, verbose=1)","ec449120":"# predict_y_XGB = xgb_model.predict(X_valid)","d5a62e69":"# cm_XGB = confusion_matrix(y_true = y_valid, y_pred = predict_y_XGB)","339bbb24":"# cm_XGB","7e04255e":"# draw_confusion_matrix(xgb_model, X_valid, y_valid)","d5cbdc2e":"# accuracy_XGB = calculate_accuracy(y_valid, predict_y_XGB)","23b87918":"# accuracy_XGB","23abb8ae":"# penalty_XGB = calculate_penalty(cm_XGB, penalty_matrix, lithology_keys, penalty_dict, cm_rock_idx)","0e57237e":"# print(\"XGBoost Accuracy: {},\".format(accuracy_XGB), \"Penalty Score: {}\".format(penalty_XGB))","481d15ec":"## 1a. Helper functions:","1ad2e9c5":"# 3. Preprocess and split training and validation sets.","123b7295":"We see that the model works well when tree depth = 9. So we will keep max_depth = 9 and give learning rate some experiments","7f64397e":"There are 9 targets in 50k row, several types are lack (which one?, how to find it by code?)\nWe could count the number of samples per TARGET type as follows","a554053e":"# 2. Read data and provide some analysis","d844b7a6":"After working with tree depth and learning rate, we will combine that with estimators modification","91952acb":"## 3a. Preprocess","04cc47d4":"## Number of Estimators Modification","b6fac616":"# 5.Training model and intepreting results by XgBoost","c77457ab":"# 5. XGBoost: Tuning Parameters and Results Summarization","d6fa60e8":"# Well Log Exploration","f828b22c":"# 1. Import","3d9ac65e":"# 4. Import the FORCE Competition Metrics.","3680f21a":"As you can see, the samples are IMBALANCE heavily (IMBALANCE is a very important keyword in machine learning). Generally, it is difficult to solve IMBALANCE issues","196886c5":"## Tree depth modification","6f94a4a2":"## Learning Rate Modification","11f4b77b":"## 3b. Split data by well name\nIn train and test procedure, the data is split by well name.\nSo we also follow it here"}}