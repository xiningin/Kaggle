{"cell_type":{"cf0e5fe4":"code","b0dcbf32":"code","be239ef4":"code","e3584f24":"code","90e43be0":"code","1b5dc649":"code","92d494e9":"code","46518009":"code","dfa3fae8":"code","56ca1456":"code","3801c09a":"code","360f6009":"code","e9b9620f":"code","d52f1b1a":"code","d8129aeb":"code","9658f9c4":"code","7cc70ee9":"code","bd089ff3":"code","9112109a":"code","2fa35e50":"code","f9f0a9a2":"code","6d8ed264":"code","f010e596":"code","c69f3403":"code","7f31445a":"code","7b8019f8":"code","807bd288":"code","80c69276":"code","9126cb08":"code","fe8dbfb3":"code","f739a852":"code","88df8aab":"code","a5412ad1":"code","73bedd0e":"code","0036bb21":"code","173427cd":"code","cd5e6a26":"code","a042cba2":"code","c101328b":"code","536b01ad":"code","a4d12763":"code","2c0946c8":"markdown","64c52e0f":"markdown","3b862826":"markdown","cc07cbd7":"markdown","a16a0f0a":"markdown","51058cf0":"markdown","8e898187":"markdown","210d8a3a":"markdown","e0f42a30":"markdown","52f7ec61":"markdown","38c99210":"markdown","c106e9fc":"markdown","3c5e588b":"markdown","f0c80c83":"markdown","e61c8aa8":"markdown","860969df":"markdown","a8b79a9b":"markdown","df91f291":"markdown"},"source":{"cf0e5fe4":"%%HTML\n<style type=\"text\/css\">\ndiv.h1 {\n    background-color:#e17b34; \n    color: white; \n    padding: 8px; \n    padding-right: 300px; \n    font-size: 35px; \n    max-width: 1500px; \n    margin: auto; \n    margin-top: 50px;\n}\n\ndiv.h2 {\n    background-color:#83ccd2; \n    color: white; \n    padding: 8px; \n    padding-right: 300px; \n    font-size: 35px; \n    max-width: 1500px; \n    margin: auto; \n    margin-top: 50px;\n}\n<\/style>","b0dcbf32":"import sys\nsys.path.append('..\/input\/iterativestratification')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold","be239ef4":"import numpy as np\nimport random\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport copy\nimport seaborn as sns\n\nfrom sklearn import preprocessing\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nimport warnings\nwarnings.filterwarnings('ignore')","e3584f24":"from sklearn.preprocessing import QuantileTransformer","90e43be0":"os.listdir('..\/input\/lish-moa')","1b5dc649":"train_features = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\ntrain_targets_scored = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv('..\/input\/lish-moa\/train_targets_nonscored.csv')\n\ntest_features = pd.read_csv('..\/input\/lish-moa\/test_features.csv')\nsample_submission = pd.read_csv('..\/input\/lish-moa\/sample_submission.csv')","92d494e9":"GENES = [col for col in train_features.columns if col.startswith('g-')]\nCELLS = [col for col in train_features.columns if col.startswith('c-')]","46518009":"gnum = train_features[GENES].shape[1]\ngraphs = []\n\nfor i in range(0, gnum -1 , 7):\n    #for least display.... \n    if i >= 3:\n        break\n    idxs = list(np.array([0, 1, 2, 3, 4, 5, 6]) + i)\n    \n\n    fig, axs = plt.subplots(1, 7, sharey=True)\n    for k, item in enumerate(idxs):\n        if item >=771:\n            break\n        graph = sns.distplot(train_features[GENES].values[:,item], ax=axs[k])\n        graph.set_title(f\"g-{item}\")\n        graphs.append(graph)","dfa3fae8":"cnum = train_features[CELLS].shape[1]\ngraphs = []\n\nfor i in range(0, cnum -1 , 7):\n    #for least display.... \n    if i >= 3:\n        break\n    idxs = list(np.array([0, 1, 2, 3, 4, 5, 6]) + i)\n    \n\n    fig, axs = plt.subplots(1, 7, sharey=True)\n    for k, item in enumerate(idxs):\n        if item >=100:\n            break\n        graph = sns.distplot(train_features[CELLS].values[:,item], ax=axs[k])\n        graph.set_title(f\"c-{item}\")\n        graphs.append(graph)","56ca1456":"gnum = test_features[GENES].shape[1]\ngraphs = []\n\nfor i in range(0, gnum -1 , 7):\n    #for least display.... \n    if i >= 3:\n        break\n    idxs = list(np.array([0, 1, 2, 3, 4, 5, 6]) + i)\n    \n\n    fig, axs = plt.subplots(1, 7, sharey=True)\n    for k, item in enumerate(idxs):\n        if item >=771:\n            break\n        graph = sns.distplot(test_features[GENES].values[:,item], ax=axs[k])\n        graph.set_title(f\"g-{item}\")\n        graphs.append(graph)","3801c09a":"cnum = test_features[CELLS].shape[1]\ngraphs = []\n\nfor i in range(0, cnum -1 , 7):\n    #for least display.... \n    if i >= 3:\n        break\n    idxs = list(np.array([0, 1, 2, 3, 4, 5, 6]) + i)\n    \n\n    fig, axs = plt.subplots(1, 7, sharey=True)\n    for k, item in enumerate(idxs):\n        if item >=100:\n            break\n        graph = sns.distplot(test_features[CELLS].values[:,item], ax=axs[k])\n        graph.set_title(f\"c-{item}\")\n        graphs.append(graph)","360f6009":"#RankGauss\n\nfor col in (GENES + CELLS):\n\n    transformer = QuantileTransformer(n_quantiles=100,random_state=0, output_distribution=\"normal\")\n    vec_len = len(train_features[col].values)\n    vec_len_test = len(test_features[col].values)\n    raw_vec = train_features[col].values.reshape(vec_len, 1)\n    transformer.fit(raw_vec)\n\n    train_features[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n    test_features[col] = transformer.transform(test_features[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]","e9b9620f":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(seed=42)","d52f1b1a":"gnum = train_features[GENES].shape[1]\ngraphs = []\n\nfor i in range(0, gnum -1 , 7):\n    #for least display.... \n    if i >= 3:\n        break\n    idxs = list(np.array([0, 1, 2, 3, 4, 5, 6]) + i)\n    \n\n    fig, axs = plt.subplots(1, 7, sharey=True)\n    for k, item in enumerate(idxs):\n        if item >=771:\n            break\n        graph = sns.distplot(train_features[GENES].values[:,item], ax=axs[k])\n        graph.set_title(f\"g-{item}\")\n        graphs.append(graph)","d8129aeb":"cnum = train_features[CELLS].shape[1]\ngraphs = []\n\nfor i in range(0, cnum -1 , 7):\n    #for least display.... \n    if i >= 3:\n        break\n    idxs = list(np.array([0, 1, 2, 3, 4, 5, 6]) + i)\n    \n\n    fig, axs = plt.subplots(1, 7, sharey=True)\n    for k, item in enumerate(idxs):\n        if item >=100:\n            break\n        graph = sns.distplot(train_features[CELLS].values[:,item], ax=axs[k])\n        graph.set_title(f\"c-{item}\")\n        graphs.append(graph)","9658f9c4":"gnum = test_features[GENES].shape[1]\ngraphs = []\n\nfor i in range(0, gnum -1 , 7):\n    #for least display.... \n    if i >= 3:\n        break\n    idxs = list(np.array([0, 1, 2, 3, 4, 5, 6]) + i)\n    \n\n    fig, axs = plt.subplots(1, 7, sharey=True)\n    for k, item in enumerate(idxs):\n        if item >=771:\n            break\n        graph = sns.distplot(test_features[GENES].values[:,item], ax=axs[k])\n        graph.set_title(f\"g-{item}\")\n        graphs.append(graph)","7cc70ee9":"cnum = test_features[CELLS].shape[1]\ngraphs = []\n\nfor i in range(0, cnum -1 , 7):\n    #for least display.... \n    if i >= 3:\n        break\n    idxs = list(np.array([0, 1, 2, 3, 4, 5, 6]) + i)\n    \n\n    fig, axs = plt.subplots(1, 7, sharey=True)\n    for k, item in enumerate(idxs):\n        if item >=100:\n            break\n        graph = sns.distplot(test_features[CELLS].values[:,item], ax=axs[k])\n        graph.set_title(f\"c-{item}\")\n        graphs.append(graph)","bd089ff3":"# GENES\nn_comp = 600  #<--Update\n\ndata = pd.concat([pd.DataFrame(train_features[GENES]), pd.DataFrame(test_features[GENES])])\ndata2 = (PCA(n_components=n_comp, random_state=42).fit_transform(data[GENES]))\ntrain2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_G-{i}' for i in range(n_comp)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n\n# drop_cols = [f'c-{i}' for i in range(n_comp,len(GENES))]\ntrain_features = pd.concat((train_features, train2), axis=1)\ntest_features = pd.concat((test_features, test2), axis=1)","9112109a":"#CELLS\nn_comp = 50  #<--Update\n\ndata = pd.concat([pd.DataFrame(train_features[CELLS]), pd.DataFrame(test_features[CELLS])])\ndata2 = (PCA(n_components=n_comp, random_state=42).fit_transform(data[CELLS]))\ntrain2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_C-{i}' for i in range(n_comp)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n\n# drop_cols = [f'c-{i}' for i in range(n_comp,len(CELLS))]\ntrain_features = pd.concat((train_features, train2), axis=1)\ntest_features = pd.concat((test_features, test2), axis=1)","2fa35e50":"train_features.shape","f9f0a9a2":"from sklearn.feature_selection import VarianceThreshold\n\n\nvar_thresh = VarianceThreshold(0.8)  #<-- Update\ndata = train_features.append(test_features)\ndata_transformed = var_thresh.fit_transform(data.iloc[:, 4:])\n\ntrain_features_transformed = data_transformed[ : train_features.shape[0]]\ntest_features_transformed = data_transformed[-test_features.shape[0] : ]\n\n\ntrain_features = pd.DataFrame(train_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n                              columns=['sig_id','cp_type','cp_time','cp_dose'])\n\ntrain_features = pd.concat([train_features, pd.DataFrame(train_features_transformed)], axis=1)\n\n\ntest_features = pd.DataFrame(test_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n                             columns=['sig_id','cp_type','cp_time','cp_dose'])\n\ntest_features = pd.concat([test_features, pd.DataFrame(test_features_transformed)], axis=1)\n\ntrain_features.shape\n","6d8ed264":"train = train_features.merge(train_targets_scored, on='sig_id')\ntrain = train[train['cp_type']!='ctl_vehicle'].reset_index(drop=True)\ntest = test_features[test_features['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n\ntarget = train[train_targets_scored.columns]","f010e596":"train = train.drop('cp_type', axis=1)\ntest = test.drop('cp_type', axis=1)","c69f3403":"train","7f31445a":"target_cols = target.drop('sig_id', axis=1).columns.values.tolist()","7b8019f8":"folds = train.copy()\n\nmskf = MultilabelStratifiedKFold(n_splits=7)\n\nfor f, (t_idx, v_idx) in enumerate(mskf.split(X=train, y=target)):\n    folds.loc[v_idx, 'kfold'] = int(f)\n\nfolds['kfold'] = folds['kfold'].astype(int)\nfolds","807bd288":"print(train.shape)\nprint(folds.shape)\nprint(test.shape)\nprint(target.shape)\nprint(sample_submission.shape)","80c69276":"class MoADataset:\n    def __init__(self, features, targets):\n        self.features = features\n        self.targets = targets\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n            'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)            \n        }\n        return dct\n    \nclass TestDataset:\n    def __init__(self, features):\n        self.features = features\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n        }\n        return dct\n    ","9126cb08":"def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n    model.train()\n    final_loss = 0\n    \n    for data in dataloader:\n        optimizer.zero_grad()\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n#         print(inputs.shape)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        \n        final_loss += loss.item()\n        \n    final_loss \/= len(dataloader)\n    \n    return final_loss\n\n\ndef valid_fn(model, loss_fn, dataloader, device):\n    model.eval()\n    final_loss = 0\n    valid_preds = []\n    \n    for data in dataloader:\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        \n        final_loss += loss.item()\n        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    final_loss \/= len(dataloader)\n    valid_preds = np.concatenate(valid_preds)\n    \n    return final_loss, valid_preds\n\ndef inference_fn(model, dataloader, device):\n    model.eval()\n    preds = []\n    \n    for data in dataloader:\n        inputs = data['x'].to(device)\n\n        with torch.no_grad():\n            outputs = model(inputs)\n        \n        preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    preds = np.concatenate(preds)\n    \n    return preds\n   \n    ","fe8dbfb3":"class Model(nn.Module):      # <-- Update\n    def __init__(self, num_features, num_targets, hidden_size):\n        super(Model, self).__init__()\n        self.batch_norm1 = nn.BatchNorm1d(num_features)\n        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n        \n        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n        self.dropout2 = nn.Dropout(0.4)\n        self.dense2 = nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size))\n        \n        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n        self.dropout3 = nn.Dropout(0.4)\n        self.dense3 = nn.utils.weight_norm(nn.Linear(hidden_size, num_targets))\n    \n    def forward(self, x):\n        x = self.batch_norm1(x)\n        x = F.relu(self.dense1(x))\n        \n        x = self.batch_norm2(x)\n        x = self.dropout2(x)\n        x = F.relu(self.dense2(x))\n        \n        x = self.batch_norm3(x)\n        x = self.dropout3(x)\n        x = self.dense3(x)\n        \n        return x","f739a852":"def process_data(data):\n    data = pd.get_dummies(data, columns=['cp_time','cp_dose'])\n    return data","88df8aab":"feature_cols = [c for c in process_data(folds).columns if c not in target_cols]\nfeature_cols = [c for c in feature_cols if c not in ['kfold','sig_id']]\nlen(feature_cols)","a5412ad1":"# HyperParameters\n\nDEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\nEPOCHS = 25\nBATCH_SIZE = 128\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nNFOLDS = 7              #<-- Update\nEARLY_STOPPING_STEPS = 10\nEARLY_STOP = False\n\nnum_features=len(feature_cols)\nnum_targets=len(target_cols)\nhidden_size=2048\n","73bedd0e":"def run_training(fold, seed):\n    \n    seed_everything(seed)\n    \n    train = process_data(folds)\n    test_ = process_data(test)\n    \n    trn_idx = train[train['kfold'] != fold].index\n    val_idx = train[train['kfold'] == fold].index\n    \n    train_df = train[train['kfold'] != fold].reset_index(drop=True)\n    valid_df = train[train['kfold'] == fold].reset_index(drop=True)\n    \n    x_train, y_train  = train_df[feature_cols].values, train_df[target_cols].values\n    x_valid, y_valid =  valid_df[feature_cols].values, valid_df[target_cols].values\n    \n    train_dataset = MoADataset(x_train, y_train)\n    valid_dataset = MoADataset(x_valid, y_valid)\n    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n    validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    model = Model(\n        num_features=num_features,\n        num_targets=num_targets,\n        hidden_size=hidden_size,\n    )\n    \n    model.to(DEVICE)\n    \n    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n    scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, \n                                              max_lr=1e-2, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n    \n    loss_fn = nn.BCEWithLogitsLoss()\n    \n    early_stopping_steps = EARLY_STOPPING_STEPS\n    early_step = 0\n    \n    oof = np.zeros((len(train), target.iloc[:, 1:].shape[1]))\n    best_loss = np.inf\n    \n    for epoch in range(EPOCHS):\n        \n        train_loss = train_fn(model, optimizer,scheduler, loss_fn, trainloader, DEVICE)\n        print(f\"FOLD: {fold}, EPOCH: {epoch}, train_loss: {train_loss}\")\n        valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n        print(f\"FOLD: {fold}, EPOCH: {epoch}, valid_loss: {valid_loss}\")\n        \n        if valid_loss < best_loss:\n            \n            best_loss = valid_loss\n            oof[val_idx] = valid_preds\n            torch.save(model.state_dict(), f\"FOLD{fold}_.pth\")\n        \n        elif(EARLY_STOP == True):\n            \n            early_step += 1\n            if (early_step >= early_stopping_steps):\n                break\n            \n    \n    #--------------------- PREDICTION---------------------\n    x_test = test_[feature_cols].values\n    testdataset = TestDataset(x_test)\n    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    model = Model(\n        num_features=num_features,\n        num_targets=num_targets,\n        hidden_size=hidden_size,\n\n    )\n    \n    model.load_state_dict(torch.load(f\"FOLD{fold}_.pth\"))\n    model.to(DEVICE)\n    \n    predictions = np.zeros((len(test_), target.iloc[:, 1:].shape[1]))\n    predictions = inference_fn(model, testloader, DEVICE)\n    \n    return oof, predictions\n","0036bb21":"def run_k_fold(NFOLDS, seed):\n    oof = np.zeros((len(train), len(target_cols)))\n    predictions = np.zeros((len(test), len(target_cols)))\n    \n    for fold in range(NFOLDS):\n        oof_, pred_ = run_training(fold, seed)\n        \n        predictions += pred_ \/ NFOLDS\n        oof += oof_\n        \n    return oof, predictions","173427cd":"# Averaging on multiple SEEDS\n\nSEED = [0, 1, 2, 3 ,4, 5 ,6]  #<-- Update\noof = np.zeros((len(train), len(target_cols)))\npredictions = np.zeros((len(test), len(target_cols)))\n\nfor seed in SEED:\n    \n    oof_, predictions_ = run_k_fold(NFOLDS, seed)\n    oof += oof_ \/ len(SEED)\n    predictions += predictions_ \/ len(SEED)\n\ntrain[target_cols] = oof\ntest[target_cols] = predictions\n","cd5e6a26":"train_targets_scored","a042cba2":"len(target_cols)\n","c101328b":"valid_results = train_targets_scored.drop(columns=target_cols).merge(train[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n\n\ny_true = train_targets_scored[target_cols].values\ny_pred = valid_results[target_cols].values\n\nscore = 0\nfor i in range(len(target_cols)):\n    score_ = log_loss(y_true[:, i], y_pred[:, i])\n    score += score_ \/ target.shape[1]\n    \nprint(\"CV log_loss: \", score)\n    ","536b01ad":"sub = sample_submission.drop(columns=target_cols).merge(test[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\nsub.to_csv('submission.csv', index=False)","a4d12763":"sub.shape","2c0946c8":"**test set before using RankGauss**","64c52e0f":"# <div class=\"h1\">About this notebook<\/div> \n\n### This is by far the best (C.V.+ L.B.) score Single Model Public kernel Available.\n\nI played with different Hyperparameters...and it provided me quite good score on both `C.V`. and `L.B`.\n\nI found score improvement from baseline[1] (CV: 0.01463430775798657, LB:0.01867).\n\n- **CV**:  **0.014460374728012282**\n\n- **LB**: **0.01859**\n","3b862826":"# Dataset Classes","cc07cbd7":"### References :\n\n[1] Baseline method\n\nhttps:\/\/www.kaggle.com\/nayuts\/moa-pytorch-nn-pca-rankgauss\n\n[2] Choosing no. of components in PCA\n\nhttps:\/\/www.kaggle.com\/kushal1506\/deciding-n-components-in-pca\n\n[3] About RankGauss\n\nhttps:\/\/www.kaggle.com\/c\/porto-seguro-safe-driver-prediction\/discussion\/44629","a16a0f0a":"**test set after using RankGauss**","51058cf0":"## Updates :-\n\n### 1. No.of Components in PCA\n### 2. No.of Folds\n### 3. No. of Seeds\n### 4. Dropout Layer \n### 5. Hidden Layer size","8e898187":"I'll check distributions of g-* and c-* of train and test set. They are spiky distribution rather than normal distribution. Regardless of the train and test, they look be in the same shape.","210d8a3a":"It may be a too simple idea, it appears that the gene expression data and cell viability data can be controlled by the experimenter, so it is safe to assume that these data are independent of each other.\n\nAlso, since the shape of the distribution is close to normal distribution to begin with, I don't think there is much of a problem if it is forced to be transformed into a Gaussian distribution.","e0f42a30":"# PCA features + Existing features","52f7ec61":"# Single fold training","38c99210":"We can confirme that the shapes of data got close to the normal distribution.\n\n**train set after using RankGauss**","c106e9fc":"# Preprocessing steps","3c5e588b":"**train set before using RankGauss**","f0c80c83":"# Model","e61c8aa8":"It appears that we were able to transform the distribution of each data to resemble a normal distribution, as intended.\n\nSo, let's enter the data into the benchmarking method to see the improvement.","860969df":"## <div class='h2'> If you like, please Upvote :)<\/div>\n\n### It took me Three `Exhaustive` days to achive these Updates ... So please Upvote .\n#### It motivates me to make more high score kernels and publish them. ","a8b79a9b":"# feature Selection using Variance Encoding","df91f291":"# CV folds"}}