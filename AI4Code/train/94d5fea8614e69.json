{"cell_type":{"e3c564f1":"code","d665b708":"code","c55329d8":"code","564feb30":"code","b064de76":"code","ab97e7c3":"code","179c7233":"code","aac82b69":"code","958dbb10":"markdown","86be5a80":"markdown","ebe96401":"markdown","cf3af795":"markdown","0ddbf5c2":"markdown","c26d8675":"markdown","0c397499":"markdown","bab09819":"markdown","d26589c4":"markdown","bab8dad5":"markdown","8c981fe7":"markdown","41a203d8":"markdown","af47e174":"markdown","eea3b089":"markdown","8ca15033":"markdown","dd9a3a53":"markdown"},"source":{"e3c564f1":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import log_loss\nimport numpy as np\nimport pandas as pd\n\nX = pd.read_csv(\"..\/input\/leak-in-metadata\/X_train.csv\", usecols=['T2w_Percent Phase Field of View',\n                                        'FLAIR_Echo Train Length',\n                                        'T2w_shape',\n                                        'target'])\nX.fillna(0, inplace=True)\ny = X[\"target\"].values\nX.drop(\"target\", axis=1, inplace=True)\nX = X.values\n\no = []\no2 = []\nfor fold, (train_index, val_index) in enumerate(StratifiedKFold(n_splits=200).split(X, y)):\n    X_train, X_val = X[train_index], X[val_index]\n    y_train, y_val = y[train_index], y[val_index]\n\n    regr = LogisticRegression()\n    regr.fit(X_train, y_train)\n    \n    y_pred = regr.predict_proba(X_val)[...,1]\n    \n    auc = roc_auc_score(y_val, y_pred)\n    val_loss = log_loss(y_val, y_pred)\n    \n    o.append(auc)\n    o2.append(val_loss)\n\nprint(\"Loss\", np.mean(o2), np.std(o2))\nprint(\"AUC\", np.mean(o), np.std(o))","d665b708":"o = []\no2 = []\nfor fold, (train_index, val_index) in enumerate(StratifiedKFold(n_splits=5).split(X, y)):\n    X_train, X_val = X[train_index], X[val_index]\n    y_train, y_val = y[train_index], y[val_index]\n\n    regr = LogisticRegression()\n    regr.fit(X_train, y_train)\n    \n    y_pred = regr.predict_proba(X_val)[...,1]\n    \n    auc = roc_auc_score(y_val, y_pred)\n    val_loss = log_loss(y_val, y_pred)\n    \n    o.append(auc)\n    o2.append(val_loss)\n\nprint(\"Loss\", np.mean(o2), np.std(o2))\nprint(\"AUC\", np.mean(o), np.std(o))","c55329d8":"oof_score = np.zeros((X.shape[0],))\nfor fold, (train_index, val_index) in enumerate(StratifiedKFold(n_splits=200).split(X, y)):\n    X_train, X_val = X[train_index], X[val_index]\n    y_train, y_val = y[train_index], y[val_index]\n\n    regr = LogisticRegression()\n    regr.fit(X_train, y_train)\n    \n    y_pred = regr.predict_proba(X_val)[...,1]\n    \n    oof_score[val_index] = y_pred\n\nprint(\"OOF Loss\", roc_auc_score(y, oof_score))\nprint(\"OOF AUC\", log_loss(y, oof_score))","564feb30":"oof_score = np.zeros((X.shape[0],))\nfor fold, (train_index, val_index) in enumerate(StratifiedKFold(n_splits=5).split(X, y)):\n    X_train, X_val = X[train_index], X[val_index]\n    y_train, y_val = y[train_index], y[val_index]\n\n    regr = LogisticRegression()\n    regr.fit(X_train, y_train)\n    \n    y_pred = regr.predict_proba(X_val)[...,1]\n    \n    oof_score[val_index] = y_pred\n\nprint(\"OOF Loss\", roc_auc_score(y, oof_score))\nprint(\"OOF AUC\", log_loss(y, oof_score))","b064de76":"from sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\n\nFOLDS = 60\n\nX = pd.read_csv(\"..\/input\/leak-in-metadata\/X_train.csv\", usecols=['T2w_Percent Phase Field of View',\n                                        'FLAIR_Echo Train Length',\n                                        'T2w_shape',\n                                        'target'])\nX.fillna(0, inplace=True)\ny = X[\"target\"].values\nX.drop(\"target\", axis=1, inplace=True)\nX = X.values\n\nX, X_test, y, y_test = train_test_split(X, y, test_size=0.78,\n                                        train_size=0.22, random_state=3,\n                                        shuffle=True, stratify=y)\nprint(\"train\", X.shape, \"test\", X_test.shape)\n\navg_auc_shape = []\nroc = []\nloss = []\nskf = StratifiedKFold(n_splits=FOLDS, random_state=None, shuffle=False)\noof = np.zeros((X.shape[0],), dtype=np.float64)\ny_pred_test = np.zeros((y_test.shape[0],), dtype=np.float64)\nfor train_index, val_index in tqdm(skf.split(X, y), total=FOLDS):\n    X_train, X_val = X[train_index], X[val_index]\n    y_train, y_val = y[train_index], y[val_index]\n\n    model = LogisticRegression()\n    model.fit(X_train, y_train)\n\n    y_pred = model.predict_proba(X_val)[...,1]\n\n    y_pred_test += model.predict_proba(X_test)[...,1] \/ FOLDS\n\n    oof[val_index] = y_pred\n\n    roc.append(roc_auc_score(y_val, y_pred))\n    loss.append(log_loss(y_val, y_pred))\n    avg_auc_shape.append(y_val.shape[0])\n\nprint(\"Each AUC was computed on\", np.mean(avg_auc_shape), \"samples\")\nprint(\"CV AUC\", np.mean(roc), np.std(roc))\nprint(\"CV loss\", np.mean(loss), np.std(loss))\nprint()\nprint(\"OOF AUC\", roc_auc_score(y, oof))\nprint(\"OOF loss\", log_loss(y, oof))\nprint()\nprint(\"AUC test\", roc_auc_score(y_test, y_pred_test))\nprint(\"Loss test\", log_loss(y_test, y_pred_test))","ab97e7c3":"FOLDS = 5\n\navg_auc_shape = []\nroc = []\nloss = []\nskf = StratifiedKFold(n_splits=FOLDS, random_state=None, shuffle=False)\noof = np.zeros((X.shape[0],), dtype=np.float64)\ny_pred_test = np.zeros((y_test.shape[0],), dtype=np.float64)\nfor train_index, val_index in tqdm(skf.split(X, y), total=FOLDS):\n    X_train, X_val = X[train_index], X[val_index]\n    y_train, y_val = y[train_index], y[val_index]\n\n    model = LogisticRegression()\n    model.fit(X_train, y_train)\n\n    y_pred = model.predict_proba(X_val)[...,1]\n\n    y_pred_test += model.predict_proba(X_test)[...,1] \/ FOLDS\n\n    oof[val_index] = y_pred\n\n    roc.append(roc_auc_score(y_val, y_pred))\n    loss.append(log_loss(y_val, y_pred))\n    avg_auc_shape.append(y_val.shape[0])\n\nprint(\"Each AUC was computed on\", np.mean(avg_auc_shape), \"samples\")\nprint(\"CV AUC\", np.mean(roc), np.std(roc))\nprint(\"CV loss\", np.mean(loss), np.std(loss))\nprint()\nprint(\"OOF AUC\", roc_auc_score(y, oof))\nprint(\"OOF loss\", log_loss(y, oof))\nprint()\nprint(\"AUC test\", roc_auc_score(y_test, y_pred_test))\nprint(\"Loss test\", log_loss(y_test, y_pred_test))","179c7233":"FOLDS = 60\n\navg_auc_shape = []\ncv_auc = []\ncv_loss = []\n\noof_auc = []\noof_loss = []\n\ntest_auc = []\ntest_loss = []\nfor i in tqdm(range(300)):\n    X = pd.read_csv(\"..\/input\/leak-in-metadata\/X_train.csv\", usecols=['T2w_Percent Phase Field of View',\n                                            'FLAIR_Echo Train Length',\n                                            'T2w_shape',\n                                            'target'])\n    X.fillna(0, inplace=True)\n    y = X[\"target\"].values\n    X.drop(\"target\", axis=1, inplace=True)\n    X = X.values\n\n    X, X_test, y, y_test = train_test_split(X, y, test_size=0.78,\n                                            train_size=0.22, random_state=i,\n                                            shuffle=True, stratify=y)\n    #print(\"train\", X.shape, \"test\", X_test.shape)\n\n    skf = StratifiedKFold(n_splits=FOLDS, random_state=None, shuffle=False)\n    oof = np.zeros((X.shape[0],), dtype=np.float64)\n    y_pred_test = np.zeros((y_test.shape[0],), dtype=np.float64)\n    for train_index, val_index in skf.split(X, y):\n        X_train, X_val = X[train_index], X[val_index]\n        y_train, y_val = y[train_index], y[val_index]\n\n        model = LogisticRegression()\n        model.fit(X_train, y_train)\n\n        y_pred = model.predict_proba(X_val)[...,1]\n\n        y_pred_test += model.predict_proba(X_test)[...,1] \/ FOLDS\n\n        oof[val_index] = y_pred\n\n        cv_auc.append(roc_auc_score(y_val, y_pred))\n        cv_loss.append(log_loss(y_val, y_pred))\n        avg_auc_shape.append(y_val.shape[0])\n    \n    oof_auc.append(roc_auc_score(y, oof))\n    oof_loss.append(log_loss(y, oof))\n    \n    test_auc.append(roc_auc_score(y_test, y_pred_test))\n    test_loss.append(log_loss(y_test, y_pred_test))\n\nprint(\"Each AUC was computed on\", np.mean(avg_auc_shape), \"samples\")\nprint(\"CV AUC\", np.mean(cv_auc), np.std(cv_auc))\nprint(\"CV loss\", np.mean(cv_loss), np.std(cv_loss))\nprint()\nprint(\"OOF AUC\", np.mean(oof_auc))\nprint(\"OOF loss\", np.mean(oof_loss))\nprint()\nprint(\"AUC test\", np.mean(test_auc))\nprint(\"Loss test\", np.mean(test_loss))","aac82b69":"FOLDS = 5\n\navg_auc_shape = []\ncv_auc = []\ncv_loss = []\n\noof_auc = []\noof_loss = []\n\ntest_auc = []\ntest_loss = []\nfor i in tqdm(range(300)):\n    X = pd.read_csv(\"..\/input\/leak-in-metadata\/X_train.csv\", usecols=['T2w_Percent Phase Field of View',\n                                            'FLAIR_Echo Train Length',\n                                            'T2w_shape',\n                                            'target'])\n    X.fillna(0, inplace=True)\n    y = X[\"target\"].values\n    X.drop(\"target\", axis=1, inplace=True)\n    X = X.values\n\n    X, X_test, y, y_test = train_test_split(X, y, test_size=0.78,\n                                            train_size=0.22, random_state=i,\n                                            shuffle=True, stratify=y)\n    #print(\"train\", X.shape, \"test\", X_test.shape)\n\n    skf = StratifiedKFold(n_splits=FOLDS, random_state=None, shuffle=False)\n    oof = np.zeros((X.shape[0],), dtype=np.float64)\n    y_pred_test = np.zeros((y_test.shape[0],), dtype=np.float64)\n    for train_index, val_index in skf.split(X, y):\n        X_train, X_val = X[train_index], X[val_index]\n        y_train, y_val = y[train_index], y[val_index]\n\n        model = LogisticRegression()\n        model.fit(X_train, y_train)\n\n        y_pred = model.predict_proba(X_val)[...,1]\n\n        y_pred_test += model.predict_proba(X_test)[...,1] \/ FOLDS\n\n        oof[val_index] = y_pred\n\n        cv_auc.append(roc_auc_score(y_val, y_pred))\n        cv_loss.append(log_loss(y_val, y_pred))\n        avg_auc_shape.append(y_val.shape[0])\n    \n    oof_auc.append(roc_auc_score(y, oof))\n    oof_loss.append(log_loss(y, oof))\n    \n    test_auc.append(roc_auc_score(y_test, y_pred_test))\n    test_loss.append(log_loss(y_test, y_pred_test))\n\nprint(\"Each AUC was computed on\", np.mean(avg_auc_shape), \"samples\")\nprint(\"CV AUC\", np.mean(cv_auc), np.std(cv_auc))\nprint(\"CV loss\", np.mean(cv_loss), np.std(cv_loss))\nprint()\nprint(\"OOF AUC\", np.mean(oof_auc))\nprint(\"OOF loss\", np.mean(oof_loss))\nprint()\nprint(\"AUC test\", np.mean(test_auc))\nprint(\"Loss test\", np.mean(test_loss))","958dbb10":"# Regular 5 and 200-fold CV","86be5a80":"# Distribution","ebe96401":"# OOF 5 and 200-fold CV","cf3af795":"The standard deviation is closer to the true standard deviation. However, we increased the bias. The AUC has decreased to 0.57.","0ddbf5c2":"In the last section, we only considered one dataset by using train_test_split with 1 seed. However, this is only one particular instance of the dataset. In the next experiment, we sample 300 datasets from the whole dataset distribution to get a better estimate of the CV.\n\nWe use 60 folds again.","c26d8675":"This time we see that OOF AUC < AUC but the regular CV AUC is much closer to the test dataset. Again the estimation of the variance is wrong.\n\nLet us reduce the number of folds to 5.","0c397499":"After some discussions in this [thread](https:\/\/www.kaggle.com\/c\/rsna-miccai-brain-tumor-radiogenomic-classification\/discussion\/275233) whether OOF or CV is better, I want to clarify some points by looking at the results empirically:\n\n- CV AUC gives a better bias estimate than OOF AUC (in particular for high K)\n- the high std can be explained by the bias-variance tradeoff\n\n","bab09819":"The AUC has increased from 0.68 to 0.7. In contrast, the regular 5-fold CV AUC is 0.5768147135783306.","d26589c4":"We can see that the standard deviation is extremely high. However, this is not surprising by the [bias-variance-tradeoff](https:\/\/stats.stackexchange.com\/questions\/61783\/bias-and-variance-in-leave-one-out-vs-k-fold-cross-validation). Even the proper scoring rule log loss is affected by the high number of folds. Let us reduce the folds to 5.","bab8dad5":"We have a better estimation of the standard deviation by reducing the number of folds (bias-variance tradeoff). 0.522 + 0.0622 = 0.5842 which is quite close to 0.5756.\n\nHowever, the bias is again too high. Note that OOF AUC > CV AUC.","8c981fe7":"# Train-test-split and CV","41a203d8":"Before we saw a strong effect on the bias. This time the effect is not as strong. For 60-fold CV we had 0.575583, here we have 0.57384. AUC test is 0.57694. Then |0.575583 - 0.57694| = 0.001357 (60-fold) and |0.57384 - 0.57694| = 0.0031 (5-fold CV). Hence, 60-fold CV is closer to AUC test.","af47e174":"The train dataset is 78% of the total data and the test dataset is 22% of the total data. I am using 60 folds because the dataset is too small.","eea3b089":"Again we see that CV AUC is closer to AUC test than OOF AUC. Next, we test the results with 5 folds.","8ca15033":"By using OOF, we actually get a higher AUC! OOF AUC is **0.6806**, while regular CV AUC **0.6575**. Let us look at 5-fold CV again.","dd9a3a53":"From the last section, we found out that:\n- CV AUC < OOF AUC\n- By increasing the number of folds, we increase the variance\n\nNow, I want to look at the number of folds in relation to the bias of the estimator."}}