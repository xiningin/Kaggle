{"cell_type":{"070d84e2":"code","f91b4bcd":"code","e2094d4e":"code","af9be74c":"code","d14b1b31":"code","8a605e63":"code","70305ff1":"code","af2e0142":"code","ef7c8e07":"code","131ef143":"code","ed9c333f":"code","be63e94c":"code","97326d4f":"code","ccce46ae":"code","ac9e63fa":"code","5b60fffe":"code","410848e8":"code","9f6231bb":"code","e7f93f36":"code","b1e67e4b":"code","9a5f63f9":"code","67628c53":"code","3bd21a01":"code","129c8f6b":"code","540fbdfe":"code","fecaf1ae":"code","30de313b":"code","2445de54":"code","7337b57e":"code","0a30c633":"code","347e8732":"code","e6ee3a6b":"code","b5e9ca53":"markdown","cebce865":"markdown","ffcfbd1e":"markdown","3a84d519":"markdown","c669a6b6":"markdown","0ab2efb2":"markdown","1a419a7c":"markdown","8601f083":"markdown","c76c582b":"markdown","0e98e01f":"markdown","d147298b":"markdown","e98262b2":"markdown","f6b76402":"markdown","f7b53417":"markdown","295ed0f9":"markdown","5178de8b":"markdown","50acd647":"markdown","f2b6b84b":"markdown","f0cf6db8":"markdown","3df091a3":"markdown","9e4bb02e":"markdown","a13b51a2":"markdown","3112d5b4":"markdown","f5c23634":"markdown","e0a80ae4":"markdown","dd733abf":"markdown"},"source":{"070d84e2":"# !pip uninstall nltk\n\n\n\nimport os\nwd = '\/kaggle\/working'\nos.chdir(wd)\n\nimport pandas as pd\nimport xml.etree.ElementTree as ET\nimport re\n\nimport nltk \n\nfrom nltk.tokenize import RegexpTokenizer\n\nfrom nltk.corpus import stopwords\n\nimport emoji\n\nimport string\n\nfrom nltk.stem.porter import PorterStemmer\n\nfrom nltk.corpus import stopwords\n\", \".join(stopwords.words('english'))\n\nstopwords_list_570 = []\nwith open('\/kaggle\/input\/stopword\/stopwords_en.txt') as f:\n    stopwords_list_570 = f.read().splitlines()\n\nfrom nltk.stem import SnowballStemmer\nstemmer = SnowballStemmer('english')\n\n\n","f91b4bcd":"os.chdir('\/kaggle\/input\/train-test-data-csv\/')\n\ntrain_labels = pd.read_csv('train_labels.csv')\n\ntest = pd.read_csv('test.csv')\n\n\n## test lables \n\ntest_label = pd.read_csv('\/kaggle\/input\/test-label\/test_labels.csv')\n\ntest_label.head()\n","e2094d4e":"os.chdir('\/kaggle\/input\/fit5149-2020-s1\/data')\n\n\nfile_name = []\nfor each in os.listdir():\n    file_name.append(each)\n","af9be74c":"dox = []\ndoc_id = []\n\nfor f in file_name:\n    tree = ET.parse(f)\n    root = tree.getroot()\n    txt = ''\n    for d in root.iter('document'):\n        txt += \" \"+d.text\n#  \n    dox.append(txt)\n    doc_id.append(re.sub(r'.xml','',f))\n    \n\n","d14b1b31":"data = pd.DataFrame()\n\ndata['id'] = doc_id\n\ndata['doc']= dox\n\ndata.shape","8a605e63":"### removind URL \ndata['doc']= data[\"doc\"].apply(lambda s: re.sub(r'https\\S+.*?\\s','',s)) \n\ndata['doc']= data[\"doc\"].apply(lambda s: re.sub(r'@[A-Za-z].*? ','',s))\n\n\n\n# lower case \n\ndata['doc'] = data['doc'].apply(lambda s: s.lower())\n\n# Removal of Punctuations\n\n\"!\"#$%&\\'()*+,-\\.\/:;<=>\\?@[\\\\]^_{|}~*\"\n\ndef cleaner(txt):\n    txt = re.sub(r\"[!\\*\\.\\?&'#$:;,\/%^\\(\\)+=<>@\\\"\u2026-]\",' ',txt)\n    return txt\n    \ndata['doc']= data[\"doc\"].apply(lambda s: cleaner(s))\n\n\n\n\n\n\n### Removal of stopwords\n\nSTOPWORDS = set(stopwords.words('english'))\n\ndata[\"doc\"] = data[\"doc\"].apply(lambda s: \" \".join([word for word in str(s).split() if word not in STOPWORDS]))\n\n\n### Stemming\n\nstemmer = PorterStemmer()\n\ndata['doc'] = data['doc'].apply(lambda s: \" \".join([stemmer.stem(word) for word in s.split()]))\n\n\n\n\ndata['doc'][3]\n","70305ff1":"## Lemmatization\n\n\nfrom nltk.corpus import wordnet\nfrom nltk.stem import WordNetLemmatizer\n\nlemmatizer = WordNetLemmatizer()\n\nwordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\n\ndef lemmatize_words(text):\n    pos_tagged_text = nltk.pos_tag(text.split())\n    return \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\n\ndata[\"doc\"] = data[\"doc\"].apply(lambda x: lemmatize_words(x))\n\n## Removal of Emojis\n\ndef convert_emojis(text):\n    \n    return \" \".join([ \" \".join(re.sub('[:]',' ',e).split()) for e in emoji.demojize(text).split()])\n\ndata[\"doc\"] = data[\"doc\"].apply(lambda s: convert_emojis(s))\n\ndata['doc'][3]\n","af2e0142":"data.head()\ntrain_labels.head()\n\ndf = pd.merge(data, train_labels, on='id')\n\ndf.shape\n\n\ntest_df = pd.merge(data , test_label, on='id')\n\ntest_df.head()\n\ntest_df.shape\n\nd = pd.concat([df,test_df])  ","ef7c8e07":"import sklearn\nimport numpy as np\nfrom sklearn.feature_extraction.text import CountVectorizer","131ef143":"# ##frequency, \n\n## freq\ndef freq(df):\n    from sklearn.feature_extraction.text import CountVectorizer\n    vectorizer = CountVectorizer(max_features=2000, min_df=.05, max_df=.95,ngram_range=(1,2))\n    v1 = vectorizer.fit_transform(df)\n\n    v1 = v1.toarray()\n    return v1","ed9c333f":"f_x_train = freq(d['doc'])","be63e94c":"### TF\u2013IDF\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ndef idf(df):\n    vectorizer = TfidfVectorizer(ngram_range=(1,1),max_df=0.95,min_df=.05 )\n    Xv = vectorizer.fit_transform(df)\n    Xv = Xv.toarray()\n\n    return Xv\n\n","97326d4f":"tf_x_train = idf(d['doc'])","ccce46ae":"### one-hot\n\ndef one_hot(df):\n    from sklearn.feature_extraction.text import CountVectorizer\n    from sklearn.preprocessing import Binarizer\n    freq = CountVectorizer(max_df=.95, min_df=.05,analyzer='word',ngram_range=(1,1))\n    corpus = freq.fit_transform(df)\n    onehot = Binarizer()\n    corpus = onehot.fit_transform(corpus.toarray())\n    return corpus\n","ac9e63fa":"oh_x_train= one_hot(d['doc'])","5b60fffe":"\n# from sklearn.model_selection import train_test_split\n# X_train, X_test, Y_train, Y_test = train_test_split(x_train, y_train, test_size=0.2, random_state=0)","410848e8":"y_train = d['gender'][0:3100]","9f6231bb":"from sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(n_estimators=1700, random_state=1000,max_depth=100)\nclassifier.fit(oh_x_train[0:3100,], y_train) \n\n\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\nY_pred = classifier.predict(oh_x_train[3100:,])\n\nprint(confusion_matrix(d['gender'][3100:],Y_pred))\nprint(classification_report(d['gender'][3100:],Y_pred))\nprint(accuracy_score(d['gender'][3100:], Y_pred))\n\n","e7f93f36":"classifier = RandomForestClassifier(n_estimators=3000, random_state=1000)\nclassifier.fit(f_x_train[0:3100,], y_train) \nY_pred2 = classifier.predict(f_x_train[3100:,])\n\nprint(confusion_matrix(d['gender'][3100:],Y_pred2))\nprint(classification_report(d['gender'][3100:],Y_pred2))\nprint(accuracy_score(d['gender'][3100:], Y_pred2))\n","b1e67e4b":"classifier = RandomForestClassifier(n_estimators=3000, random_state=1000)\nclassifier.fit(tf_x_train[0:3100,], y_train) \nY_pred3 = classifier.predict(tf_x_train[3100:,])\n\nprint(confusion_matrix(d['gender'][3100:],Y_pred3))\nprint(classification_report(d['gender'][3100:],Y_pred3))\nprint(accuracy_score(d['gender'][3100:], Y_pred3))\n","9a5f63f9":"Y_pred","67628c53":"labels = pd.DataFrame(Y_pred)\n\nlabels.to_csv()","3bd21a01":"\n# from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\n# Y_pred = classifier.predict(one_hot(df['doc'])[:,0:77])\n\n# print(confusion_matrix(df['gender'],Y_pred))\n# print(classification_report(df['gender'],Y_pred))\n# print(accuracy_score(df['gender'], Y_pred))\n","129c8f6b":"# from sklearn.datasets import make_hastie_10_2\n# from sklearn.ensemble import GradientBoostingClassifier\n\n# clf = GradientBoostingClassifier(n_estimators=3000, learning_rate=0.04 ,max_depth=100, random_state=1000).fit(oh_x_train[0:3100,], y_train)\n\n# Y_pred4 = clf.predict(oh_x_train[3100:,])\n\n# print(confusion_matrix(d['gender'][3100:],Y_pred4))\n# print(classification_report(d['gender'][3100:],Y_pred4))\n# print(accuracy_score(d['gender'][3100:], Y_pred4))\n\n# # clf.score(test_df['gender'], Y_test)\n","540fbdfe":"# clf = GradientBoostingClassifier(n_estimators=3000, learning_rate=0.1 ,max_depth=100, random_state=1000).fit(f_x_train[0:3100,], y_train)\n\n# Y_pred5 = clf.predict(f_x_train[3100:,])\n\n# print(confusion_matrix(d['gender'][3100:],Y_pred5))\n# print(classification_report(d['gender'][3100:],Y_pred5))\n# print(accuracy_score(d['gender'][3100:], Y_pred5))\n","fecaf1ae":"# clf = GradientBoostingClassifier(n_estimators=3000, learning_rate=0.9 ,max_depth=100, random_state=1000).fit(tf_x_train[0:3100,], y_train)\n\n# Y_pred6 = clf.predict(tf_x_train[3100:,])\n\n# print(confusion_matrix(d['gender'][3100:],Y_pred6))\n# print(classification_report(d['gender'][3100:],Y_pred6))\n# print(accuracy_score(d['gender'][3100:], Y_pred6))","30de313b":"# from sklearn.model_selection import cross_val_score\n# from sklearn.ensemble import AdaBoostClassifier\n\n# cfr = AdaBoostClassifier(n_estimators=3000, random_state=1000)\n# cfr.fit(oh_x_train[0:3100,],y_train)\n# Y_pred7 = cfr.predict(oh_x_train[3100:,])\n\n\n# print(confusion_matrix(d['gender'][3100:],Y_pred7))\n# print(classification_report(d['gender'][3100:],Y_pred7))\n# print(accuracy_score(d['gender'][3100:], Y_pred7))\n","2445de54":"# cfr = AdaBoostClassifier(n_estimators=3000, random_state=1000)\n# cfr.fit(f_x_train[0:3100,],y_train)\n# Y_pred8 = cfr.predict(f_x_train[3100:,])\n\n\n# print(confusion_matrix(d['gender'][3100:],Y_pred8))\n# print(classification_report(d['gender'][3100:],Y_pred8))\n# print(accuracy_score(d['gender'][3100:], Y_pred8))\n\n","7337b57e":"# cfr = AdaBoostClassifier(n_estimators=3000, random_state=1000)\n# cfr.fit(tf_x_train[0:3100,],y_train)\n# Y_pred9 = cfr.predict(tf_x_train[3100:,])\n\n\n# print(confusion_matrix(d['gender'][3100:],Y_pred9))\n# print(classification_report(d['gender'][3100:],Y_pred9))\n# print(accuracy_score(d['gender'][3100:],Y_pred9))\n","0a30c633":"# final = pd.DataFrame()\n# final['id'] = test_label.iloc[:,0]\n\n\n# final['gender'] = Y_pred3\n# final['onehot'] = Y_pred\n# final['tfidf'] = Y_pred2\n\n# # txt = final.to_csv()\n\n# # open('file.csv','w').write(final.to_csv())\n\n# # f = open('file',\"w+\")\n# # f.write(txt)\n\n\n# # os.chdir('filefile')\n# f = ('R','rw+')\n# final.to_csv(f,sep='\\t', encoding='utf-8')\n\n# f.write(final.to_csv())\n\n# final.to_csv('xxxx.csv',index=False)\n\nfinal.to_csv(\"final.csv\")","347e8732":"# test_label","e6ee3a6b":"# from sklearn.linear_model import LogisticRegression\n\n# clf = LogisticRegression(random_state=0).fit(idf(df['doc']), df['gender'])\n\n# Y_pred10 = clf.predict(idf(test_df['doc']))\n\n\n# print(confusion_matrix(d['gender'][3101:],Y_pred10))\n# print(classification_report(d['gender'][3101:],Y_pred10))\n# print(accuracy_score(d['gender'][3101:], Y_pred10))\n","b5e9ca53":"# GradientBoostingClassifier\n","cebce865":"## one hot","ffcfbd1e":"# Assignment 2: : Authorship Profiling (FIT5149 S1 2020)","3a84d519":"## Vectorize TF_IDF","c669a6b6":"### getting file names","0ab2efb2":"# RandomForest","1a419a7c":"## TF-IDF","8601f083":"### combining test and train datasets so we can get the same features size.","c76c582b":"### Importing Libraries","0e98e01f":"### Making Dataframe of data","d147298b":"# Text Processing Steps","e98262b2":"## TF-IDF","f6b76402":"### Reading data files","f7b53417":"## Logistis with TFIDF","295ed0f9":"## Frequency","5178de8b":"## Vectorize One Hot","50acd647":"## frequency","f2b6b84b":"### Parsing xml file and creating doc_id list","f0cf6db8":"## Vectorize Freq","3df091a3":"## one hot ","9e4bb02e":"#### Validation checking on train dataset\nfor test set. divide the train set to train and test and perform the analysis.","a13b51a2":"## one hot\n","3112d5b4":"## prediected label with one-hot encoding","f5c23634":"## TF-IDF","e0a80ae4":"## Frequency","dd733abf":"# Adaboost"}}