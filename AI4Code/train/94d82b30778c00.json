{"cell_type":{"2ee92511":"code","352b7474":"code","6e84e7d2":"code","03a254cf":"code","55b1342a":"code","b2bf6d85":"code","1fd2695e":"code","39a16404":"code","79979cd2":"code","ea870e8e":"code","f4cf6bff":"code","39606de2":"code","8fc22105":"code","a7061eaa":"code","68515c50":"code","fe56e439":"code","432d48a0":"code","6c28589f":"code","4720a6ae":"code","79a7e5cd":"code","4683260a":"code","074a3a87":"code","5add9ecd":"code","d1daf190":"code","0e85b950":"code","01b286df":"code","58577d47":"code","c2432e14":"code","6ce406d2":"code","47f66a33":"code","4f2db882":"code","c6ca8124":"code","b913433d":"code","72210fc1":"code","678e42ce":"code","695684fb":"code","5c3355c2":"code","2d79d3f8":"code","57bed911":"code","4280515f":"code","f6bd1d8a":"code","ac9a10e4":"code","036dded7":"code","38364aa3":"code","043deeb8":"code","66924c88":"code","26e87757":"code","31624b64":"code","ea6641c2":"code","ca5b62bc":"code","6f3a4c5c":"code","a684ffd9":"code","02183720":"code","1e57fe47":"code","68fd48ff":"markdown","d985d75a":"markdown","7d4d594e":"markdown","aaf1471f":"markdown","46c45fbd":"markdown","e9d71046":"markdown","062c5ad3":"markdown","e56ed926":"markdown","f6798a12":"markdown","73ab0f7c":"markdown","e14e6efb":"markdown","465b108e":"markdown","a9889f6d":"markdown","39a7bac1":"markdown","2bb174cc":"markdown","bb1d971d":"markdown","16472ff1":"markdown","5a4a4047":"markdown","d6023565":"markdown","774af711":"markdown","33d5dd4b":"markdown","16cdf9c8":"markdown","dfff7559":"markdown","5593be0f":"markdown","fd472b71":"markdown","786d2e61":"markdown","deb51d0f":"markdown"},"source":{"2ee92511":"# packages\n\n# standard\nimport numpy as np\nimport pandas as pd\nimport time\n\n# plot\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# statistics tools\nfrom statsmodels.graphics.mosaicplot import mosaic\n\n# ML\nimport h2o\nfrom h2o.estimators import H2ORandomForestEstimator\nfrom h2o.estimators import H2OGradientBoostingEstimator","352b7474":"# load data\ndf = pd.read_csv('..\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv')","6e84e7d2":"# dimensions of data\ndf.shape","03a254cf":"# column names\nprint(df.columns.tolist())","55b1342a":"df.info()","b2bf6d85":"# impute with -99\ndf.bmi = df.bmi.fillna(-99)","1fd2695e":"# rename columns\ndf.rename(columns = {'Residence_type':'residence_type'}, inplace = True)","39a16404":"# define target variable\ndf['target'] = df.stroke\ndf = df.drop(['stroke'], axis=1) # remove stroke column","79979cd2":"# select numerical features\nfeatures_num = ['age', 'avg_glucose_level','bmi']","ea870e8e":"# basic stats\ndf[features_num].describe(percentiles=[0.1,0.25,0.5,0.75,0.9])","f4cf6bff":"# plot distribution of numerical features\nfor f in features_num:\n    df[f].plot(kind='hist', bins=50)\n    plt.title(f)\n    plt.grid()\n    plt.show()","39606de2":"# pairwise scatter plot\nsns.pairplot(df[features_num], \n             kind='reg', \n             plot_kws={'line_kws':{'color':'magenta'}, 'scatter_kws': {'alpha': 0.1}})\nplt.show()","8fc22105":"# Spearman (Rank) correlation\ncorr_spearman = df[features_num].corr(method='spearman')\n\nfig = plt.figure(figsize = (6,5))\nsns.heatmap(corr_spearman, annot=True, cmap=\"RdYlGn\", vmin=-1, vmax=+1)\nplt.title('Spearman Correlation')\nplt.show()","a7061eaa":"features_cat = ['gender','hypertension','heart_disease','ever_married',\n                'work_type','residence_type','smoking_status']","68515c50":"for f in features_cat:\n    df[f].value_counts().plot(kind='bar')\n    plt.title(f)\n    plt.grid()\n    plt.show()","fe56e439":"# calc frequencies\ntarget_count = df.target.value_counts()\nprint(target_count)\nprint()\nprint('Percentage of strokes [1]:', np.round(100*target_count[1] \/ target_count.sum(),2), '%')","432d48a0":"# plot target distribution\ntarget_count.plot(kind='bar')\nplt.title('Target = Stroke')\nplt.grid()\nplt.show()","6c28589f":"# add binned version of numerical features\n\n# quantile based:\ndf['age_bin'] = pd.qcut(df['age'], q=10, precision=1)\ndf['avg_glucose_level_bin'] = pd.qcut(df['avg_glucose_level'], q=10, precision=1)\n\n# explicitly defined bins:\ndf['bmi_bin'] = pd.cut(df['bmi'], [-100,10,20,25,30,35,40,50,100])","4720a6ae":"# plot target vs features using mosaic plot\nplt_para_save = plt.rcParams['figure.figsize'] # remember plot settings\n\nfor f in features_num:\n    f_bin = f+'_bin'\n    plt.rcParams[\"figure.figsize\"] = (16,7) # increase plot size for mosaics\n    mosaic(df, [f_bin, 'target'], title='Target vs ' + f + ' [binned]')\n    plt.show()\n    \n# reset plot size again\nplt.rcParams['figure.figsize'] = plt_para_save","79a7e5cd":"# BMI - check cross table\nctab = pd.crosstab(df.bmi_bin, df.target)\nctab","4683260a":"# normalize each row to get row-wise target percentages\n(ctab.transpose() \/ ctab.sum(axis=1)).transpose()","074a3a87":"# plot target vs features using mosaic plot\nplt_para_save = plt.rcParams['figure.figsize'] # remember plot settings\n\nfor f in features_cat:\n    plt.rcParams[\"figure.figsize\"] = (8,7) # increase plot size for mosaics\n    mosaic(df, [f, 'target'], title='Target vs ' + f)\n    plt.show()\n    \n# reset plot size again\nplt.rcParams['figure.figsize'] = plt_para_save","5add9ecd":"# \"ever married\" - check cross table\nctab = pd.crosstab(df.ever_married, df.target)\nctab","d1daf190":"# normalize each row\n(ctab.transpose() \/ ctab.sum(axis=1)).transpose()","0e85b950":"# select predictors\npredictors = features_num + features_cat\nprint('Number of predictors: ', len(predictors))\nprint(predictors)","01b286df":"# start H2O\nh2o.init(max_mem_size='12G', nthreads=4) # Use maximum of 12 GB RAM and 4 cores","58577d47":"# upload data frame in H2O environment\ndf_hex = h2o.H2OFrame(df)\n\ndf_hex['target'] = df_hex['target'].asfactor()\n\n# train \/ test split (70\/30)\ntrain_hex, test_hex = df_hex.split_frame(ratios=[0.7], seed=999)\n\n# pandas versions of train\/test\ndf_train = train_hex.as_data_frame()\ndf_test = test_hex.as_data_frame()","c2432e14":"# export for potential external processing\ndf_train.to_csv('df_train.csv')\ndf_test.to_csv('df_test.csv')","6ce406d2":"# define Gradient Boosting model\nfit_1 = H2OGradientBoostingEstimator(ntrees = 100,\n                                     max_depth=4,\n                                     min_rows=10,\n                                     learn_rate=0.01, # default: 0.1\n                                     sample_rate=1,\n                                     col_sample_rate=0.7,\n                                     nfolds=5,\n                                     score_each_iteration=True,\n                                     stopping_metric='auto',\n                                     stopping_rounds=10,\n                                     seed=999)","47f66a33":"# train model\nt1 = time.time()\nfit_1.train(x=predictors,\n            y='target',\n            training_frame=train_hex)\nt2 = time.time()\nprint('Elapsed time [s]: ', np.round(t2-t1,2))","4f2db882":"# show training scoring history\nplt.rcParams['figure.figsize']=(7,4)\nfit_1.plot()","c6ca8124":"# show cross validation metrics\nfit_1.cross_validation_metrics_summary()","b913433d":"# show scoring history - training vs cross validations\nfor i in range(5):\n    cv_model_temp = fit_1.cross_validation_models()[i]\n    df_cv_score_history = cv_model_temp.score_history()\n    my_title = 'CV ' + str(1+i) + ' - Scoring History [AUC]'\n    plt.scatter(df_cv_score_history.number_of_trees,\n                y=df_cv_score_history.training_auc, \n                c='blue', label='training')\n    plt.scatter(df_cv_score_history.number_of_trees,\n                y=df_cv_score_history.validation_auc, \n                c='darkorange', label='validation')\n    plt.title(my_title)\n    plt.xlabel('Number of Trees')\n    plt.ylabel('AUC')\n    plt.ylim(0.8,1)\n    plt.legend()\n    plt.grid()\n    plt.show()","72210fc1":"# training performance\nperf_train = fit_1.model_performance(train=True)\nperf_train.plot()","678e42ce":"# cross validation performance\nperf_cv = fit_1.model_performance(xval=True)\nperf_cv.plot()","695684fb":"# on training data - automatic threshold (optimal F1 score)\nconf_train = fit_1.confusion_matrix(train=True)\nconf_train.show()","5c3355c2":"# corresponding accuracy for this threshold:\nconf_list_temp = conf_train.to_list()\nn_matrix = sum(conf_list_temp[0]) + sum(conf_list_temp[1])\nacc_t0 = (conf_list_temp[0][0]+conf_list_temp[1][1]) \/ n_matrix\nprint('Accuracy:', np.round(acc_t0,6))","2d79d3f8":"# alternatively specify threshold manually - here we try to achieve a symmetric outcome\ntt = 0.148\nconf_train_man = fit_1.confusion_matrix(train=True, thresholds=tt)\nconf_train_man.show()","57bed911":"# corresponding accuracy for manual threshold:\nconf_list_temp = conf_train_man.to_list()\nn_matrix = sum(conf_list_temp[0]) + sum(conf_list_temp[1]) \nacc_t1 = (conf_list_temp[0][0]+conf_list_temp[1][1]) \/ n_matrix\nprint('Accuracy:', np.round(acc_t1,6))","4280515f":"# check on cross validation\nconf_cv_man = fit_1.confusion_matrix(xval=True, thresholds=tt)\nconf_cv_man.show()","f6bd1d8a":"# corresponding accuracy for our manual threshold:\nconf_list_temp = conf_cv_man.to_list()\nn_matrix = sum(conf_list_temp[0]) + sum(conf_list_temp[1])\nacc_t1_CV = (conf_list_temp[0][0]+conf_list_temp[1][1]) \/ n_matrix\nprint('Accuracy:', np.round(acc_t1_CV,6))","ac9a10e4":"# basic version\nfit_1.varimp_plot()","036dded7":"# variable importance using shap values => see direction as well as severity of feature impact\nt1 = time.time()\nfit_1.shap_summary_plot(train_hex);\nt2 = time.time()\nprint('Elapsed time [s]: ', np.round(t2-t1,2))","38364aa3":"# predict on train set (extract probabilities only)\npred_train = fit_1.predict(train_hex)['p1']\npred_train = pred_train.as_data_frame().p1\n\n# and plot\nplt.hist(pred_train, bins=50)\nplt.title('Predictions on Train Set')\nplt.grid()\nplt.show()","043deeb8":"# check calibration\nfrequency_pred = sum(pred_train)\nfrequency_act = df_train.target.sum()\nprint('Predicted Frequency:', frequency_pred)\nprint('Actual Frequency   :', frequency_act)","66924c88":"# calc performance on test test\nperf_test = fit_1.model_performance(test_hex)\n\n# ROC Curve - Test Set\nperf_test.plot()","26e87757":"# confusion matrix using our manual threshold\nconf_test_man = perf_test.confusion_matrix(thresholds=tt)\nconf_test_man.show()","31624b64":"# calc accuracy for manual threshold:\nconf_list_temp = conf_test_man.to_list()\nn_matrix = sum(conf_list_temp[0]) + sum(conf_list_temp[1]) \nacc_t1_test = (conf_list_temp[0][0]+conf_list_temp[1][1]) \/ n_matrix\nprint('Accuracy:', np.round(acc_t1_test,6))","ea6641c2":"# predict on test set (extract probabilities only)\npred_test = fit_1.predict(test_hex)['p1']\npred_test = pred_test.as_data_frame().p1\n\n# and plot\nplt.hist(pred_test, bins=50)\nplt.title('Predictions on Test Set')\nplt.grid()\nplt.show()","ca5b62bc":"# connect prediction with data frame\ndf_test['prediction'] = pred_test","6f3a4c5c":"# show most endangered patients (according to our model) in test set\ndf_high_20 = df_test.nlargest(20, columns='prediction')\ndf_high_20","a684ffd9":"print('Actual cases in highest 20    :', df_high_20.target.sum())\nprint('Predicted cases in highest 20 :', np.round(df_high_20.prediction.sum(),2))","02183720":"# show least endangered patients (according to our model) in test set\ndf_low_20 = df_test.nsmallest(20, columns='prediction')\ndf_low_20","1e57fe47":"print('Actual cases in lowest 20    :', df_low_20.target.sum())\nprint('Predicted cases in lowest 20 :', np.round(df_low_20.prediction.sum(),2))","68fd48ff":"<a id='3'><\/a>\n# Numerical Features","d985d75a":"# Stroke Prediction Model (Binary Classification)\n\n### Remark: The data is strongly imbalanced in this case: We have 4861 patients with target=0 (no stroke), but only 249 (<5%) cases with target=1 (stroke). By using a trivial predictor which always returns 0 we can achieve an accuracy of 4861\/5110 = 95.13%. This sounds at first like a good performance, however, this trivial predictor is completely useless as it has absolutely no discriminative power. We can see that accuracy is not a really useful metric in the context of strongly imbalanced data. In the following we will - for the sake of completeness - evaluate also accuracy but our focus will be on AUC as performance metric instead (our trivial predictor would have an AUC of 0.5)!\n\n\n## Table of Contents\n* [Import and first glance](#1)\n* [Data Cleansing](#2)\n* [Numerical Features](#3)\n* [Categorical Features](#4)\n* [Target](#5)\n* [Build Model](#6)\n* [Evaluate on Training Data](#7)\n* [Evaluate on Test Set](#8)","7d4d594e":"<a id='1'><\/a>\n# Import and first glance","aaf1471f":"#### Quite good:  65 actual positives vs 69 predicted positives.","46c45fbd":"#### Check calibration at low end:","e9d71046":"#### \"Naive\" Interpretations based on those univariate plots:\n* Risk increases with age and glucose level (diabetes).\n* High BMI levels are also indicating higher risk.\n* A missing value for BMI (the leftmost column) seems to indicate a massively increased risk!?","062c5ad3":"### Variable Importance","e56ed926":"### Confusion Matrix","f6798a12":"#### \"Naive\" Interpretations based on those univariate plots:\n* Influence of gender seems surprisingly low\n* Hypertension and heart disease massively increase risk of stroke\n* \"Ever married\" too!?\n* Work type: Higher risk for self-employed (more stress?)\n* Residence type: Slightly higher risk for urban vs rural\n* Smoking: Highest risk for *former* smokers. Not much difference between \"smokes\" and \"never smoked\"?","73ab0f7c":"### Show examples","e14e6efb":"#### Check calibration at high end:","465b108e":"### Predictions on training data","a9889f6d":"#### Much better: 184 actual positives vs. 185 predicted positives!","39a7bac1":"### ROC Curve - Cross Validation","2bb174cc":"#### Almost 20% of the missing BMIs had a stroke! This is way higher than for the other bins.","bb1d971d":"<a id='8'><\/a>\n# Evaluate on Test Set","16472ff1":"We have missing values for BMI!","5a4a4047":"<a id='5'><\/a>\n# Target","d6023565":"#### The blue dots for BMI are probably a little bit confusing. They are based on the strongly predictive missing values which we have encoded with -99!","774af711":"<a id='7'><\/a>\n# Evaluate on Training Data","33d5dd4b":"<a id='4'><\/a>\n# Categorical Features","16cdf9c8":"### ROC Curve - Training Data","dfff7559":"### Target vs Numerical Features","5593be0f":"### Target vs Categorical Features","fd472b71":"<a id='2'><\/a>\n# Data cleansing","786d2e61":"<a id='6'><\/a>\n# Build Model","deb51d0f":"#### Selecting threshold by optimal F1 is not really helpful here, we have a big difference between actual positives (184) and predicted positives (302). Let's try to improve by selecting the threshold manually:"}}