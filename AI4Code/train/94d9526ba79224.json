{"cell_type":{"9236381a":"code","703b0f29":"code","89ced410":"code","46a25101":"code","d1c7afe8":"code","d46eb3d4":"code","37d13afb":"code","8afd975d":"code","f45bc0f1":"code","9acba437":"code","f44c0c6a":"code","3b6dacdc":"code","3e14b766":"code","f6835ed2":"code","accaa2b9":"code","7c535e5c":"code","0f3508f3":"code","43700c6f":"code","4ba5ffa2":"code","a804c3f6":"code","96d9df2a":"code","283a5efe":"code","ed71ba62":"code","39657942":"code","5d78cb51":"code","e2f80901":"code","e3c13b33":"code","985baffa":"code","b24a44bb":"code","2854c7fd":"code","03bd638a":"code","d734b0fa":"code","3aab58ca":"code","b30a45cf":"code","d7b99abc":"code","d498345a":"code","3037a33b":"code","74fbdfd4":"code","4e215ef4":"code","825175cb":"code","c24a0000":"code","c8128341":"code","35b4ecce":"code","db162ec5":"code","d4ce58f9":"code","0b6b7f7a":"code","9eaa795e":"code","c2179d62":"code","3e5c6b0d":"code","dc9c6f7c":"code","8c40ec5b":"code","4a5156c9":"code","ada5d8a5":"code","870dfb94":"code","62bac204":"code","72980c63":"code","d7a629ac":"code","6fced9c2":"code","492ffb30":"code","ca252309":"code","cbc1316a":"code","3ef095d5":"code","f499792a":"code","eb887397":"code","627f9a49":"code","d1201c1e":"code","1d6ba16b":"code","857e30b2":"code","5da7fe6a":"code","4eadba0b":"code","a38e284e":"code","916f2a47":"code","49cf8bdc":"code","284f16b4":"code","a106e013":"code","d3bd62f3":"code","fba27503":"code","0cac1164":"code","dddc8fa3":"code","e1f0cc45":"code","26a60d4a":"code","713d946b":"code","feff216a":"code","1cd224b6":"code","e5007a1d":"code","a5d7bf66":"code","7e614982":"code","95ab87fd":"code","ce4de152":"code","c1a7b60f":"code","4d85a0f8":"code","3abdcd2c":"code","ad7d4db1":"code","4d2b2b17":"code","60fd61ee":"code","33444ba1":"code","d1dafd11":"code","b02a6fd6":"code","6eae8134":"code","ef7396b7":"code","931fb6f1":"code","4b289630":"code","684f46a0":"code","c718126f":"code","411e79e4":"code","e1460c98":"code","f7777231":"code","6abec8a5":"code","700d160a":"code","4500a0ce":"code","ff1fb298":"code","b4081f1f":"code","16764ea8":"code","118c193e":"code","94f098bc":"code","ad2afdf8":"code","66170e8c":"code","665b3709":"code","53411400":"code","bfaebbd0":"code","a43e8bf5":"code","c4bf2d15":"code","e9f6a1ac":"code","ab55cf58":"code","105648d8":"code","503bae9f":"markdown","73c4cb09":"markdown","d8676324":"markdown","be7ce305":"markdown","fdd4b4b7":"markdown","51172d20":"markdown","0c18d17f":"markdown","ac9f5036":"markdown","e8b3e85e":"markdown","b3b2c5cc":"markdown","c73fd82b":"markdown","3112c062":"markdown","09632a40":"markdown","4a0d405b":"markdown","329bc9c0":"markdown","6b51d21a":"markdown","e7eb2dcc":"markdown","cac2a799":"markdown","9d59c61e":"markdown","f947ee03":"markdown","1933ba7d":"markdown","2ed6b8f2":"markdown","169ea110":"markdown","27b7fb52":"markdown","b345d633":"markdown","a5541dd3":"markdown","37649e30":"markdown","48e26a2b":"markdown","f401b01b":"markdown","cf4cf98c":"markdown","7fbb90c6":"markdown","f2cb745e":"markdown","ec4da94d":"markdown","70375dab":"markdown","5e59679b":"markdown","d47f31da":"markdown","a5f89eec":"markdown","0a27d5f2":"markdown","03284916":"markdown"},"source":{"9236381a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","703b0f29":"# From Scratch","89ced410":"import nltk","46a25101":"dir(nltk)","d1c7afe8":"from nltk.corpus import stopwords\nstopwords.words('english')[0:500:25]","d46eb3d4":"rawData=open('\/kaggle\/input\/nlp-data-set\/SMSSpamCollection.tsv').read() \nrawData[0:500]","37d13afb":"parsedData=rawData.replace(\"\\t\", \"\\n\").split(\"\\n\")\nparsedData[0:5]","8afd975d":"label_list=parsedData[0::2]\ntext_list=parsedData[1::2]","f45bc0f1":"print(label_list[0:5])\nprint(text_list[0:5])","9acba437":"print(len(label_list))\nprint(len(text_list))","f44c0c6a":"print(label_list[-5:])","3b6dacdc":"fullCorpus=pd.DataFrame({'label' : label_list[:-1], 'body_list' : text_list})\nfullCorpus.head()","3e14b766":"dataset=pd.read_csv('\/kaggle\/input\/nlp-data-set\/SMSSpamCollection.tsv' , sep='\\t', header = None)\ndataset.head()","f6835ed2":"dataset.columns=['label', 'body_list']\ndataset.head()","accaa2b9":"# shape #5568\nprint(\"Input data has {} rows and {} columns\".format(len(fullCorpus), len(fullCorpus.columns)))","7c535e5c":"# how many ham and spam\nprint(\"Out of the {} rows, {} are spam, {} are ham\". format(len(fullCorpus), \n                                                            len(fullCorpus[fullCorpus['label']=='spam']),\n                                                            len(fullCorpus[fullCorpus['label']=='ham'])))","0f3508f3":"# missing data\nprint(\"Number of missing label {}\".format(fullCorpus['label'].isnull().sum()))\nprint(\"Number of missing text {}\".format(fullCorpus['body_list'].isnull().sum()))","43700c6f":"import re","4ba5ffa2":"re_test = 'This is a made up string to test 2 different regex method'\nre_test_messy =  'This is     a made up      string to test 2       different regex method'\nre_test_messy1 = 'This-is-a-made\/up.string*to>>>>test----2\"\"\"\"\"\"different-regex-method'","a804c3f6":"# splitting a sentence into a list of words\n# 1st method\nre.split('\\s', re_test)","96d9df2a":"re.split('\\s', re_test_messy)","283a5efe":"re.split('\\s+', re_test_messy)","ed71ba62":"re.split('\\s+', re_test_messy1)","39657942":"re.split('\\W+', re_test_messy1)","5d78cb51":"# Second method\nre.findall('\\S+', re_test_messy)","e2f80901":"re.findall('\\S+', re_test)","e3c13b33":"re.findall('\\S+', re_test_messy1)","985baffa":"re.findall('\\w+', re_test_messy1)","b24a44bb":"pep8_test = 'I try to follow PEP8 guidelines'\npep7_test = 'I try to follow PEP7 guidelines'\npeep8_test = 'I try to follow PEEP8 guidelines'","2854c7fd":"re.findall('[a-z]+' , pep8_test )","03bd638a":"re.findall('[A-Z]+' , pep8_test )","d734b0fa":"re.findall('[A-Z0-9]+' , pep8_test )","3aab58ca":"re.findall('[A-Z]+[0-9]+' , pep8_test )","b30a45cf":"re.findall('[A-Z]+[0-9]+' , pep7_test )","d7b99abc":"re.findall('[A-Z]+[0-9]+' , peep8_test )","d498345a":"re.sub('[A-Z]+[0-9]+', 'PEP8 Python Styleguide',pep8_test )","3037a33b":"re.sub('[A-Z]+[0-9]+', 'PEP8 Python Styleguide',pep7_test )","74fbdfd4":"re.sub('[A-Z]+[0-9]+', 'PEP8 Python Styleguide',peep8_test )","4e215ef4":"pd.set_option('display.max_colwidth', 100)\ndata=pd.read_csv(\"\/kaggle\/input\/nlp-data-set\/SMSSpamCollection.tsv\" , sep='\\t', header = None)\ndata.columns = ['label', 'body_text']\ndata.head()","825175cb":"# How does cleaned up version look like\ndata_cleaned=pd.read_csv(\"\/kaggle\/input\/cleaned-data\/SMSSpamCollection_cleaned.tsv\" , sep='\\t')\ndata_cleaned","c24a0000":"import string\nstring.punctuation","c8128341":"\"I like NLP.\" == \"I like NLP\"","35b4ecce":"def remove_punc(text):\n    text_no_punc = [char for char in text if char not in string.punctuation]\n    return text_no_punc\ndata['body_text_clean']=data['body_text'].apply(lambda x : remove_punc(x))\ndata.head()","db162ec5":"def remove_punc(text):\n    text_no_punc = \"\".join([char for char in text if char not in string.punctuation])\n    return text_no_punc\ndata['body_text_clean']=data['body_text'].apply(lambda x : remove_punc(x))\ndata.head()","d4ce58f9":"def tokenize(text):\n    tokens = re.split(\"\\W+\", text)\n    return tokens\ndata['body_text_tokenize']=data['body_text_clean'].apply(lambda x : tokenize(x.lower()))\ndata.head()","0b6b7f7a":"'NLP'== 'nlp'","9eaa795e":"stopword = nltk.corpus.stopwords.words('english')","c2179d62":"def remove_stopwords(tokenized_list):\n    text = [ word for word in tokenized_list if word not in stopword]\n    return text\ndata['body_text_nostop']=data['body_text_tokenize'].apply(lambda x : remove_stopwords(x))\ndata.head()","3e5c6b0d":"ps=nltk.PorterStemmer()","dc9c6f7c":"dir(ps)","8c40ec5b":"print(ps.stem('grows'))\nprint(ps.stem('growing'))\nprint(ps.stem('grow'))","4a5156c9":"print(ps.stem('run'))\nprint(ps.stem('running'))\nprint(ps.stem('runner'))","ada5d8a5":"#import re\n#import string\npd.set_option('display.max_colwidth', 100)\n\nstopwords=nltk.corpus.stopwords.words('english')\ndata=pd.read_csv(\"\/kaggle\/input\/nlp-data-set\/SMSSpamCollection.tsv\" , sep='\\t', header = None)\ndata.columns = ['label', 'body_text']\ndata.head()","870dfb94":"def clean_text(text):\n    text = \"\".join([word for word in text if word not in string.punctuation])\n    tokens=re.split('\\W+', text)\n    text = [word for word in tokens if word not in stopwords]\n    return text\ndata['body_text_nostop']=data['body_text'].apply(lambda x : clean_text(x.lower()))\ndata.head()","62bac204":"def stemming(tokenized_text):\n    text = [ps.stem(word) for word in tokenized_text]\n    return text\ndata['body_text_stemmed']=data['body_text_nostop'].apply(lambda x : stemming(x))\ndata.head()","72980c63":"#ps=nltk.PorterStemmer()\nwn=nltk.WordNetLemmatizer()\n#import re\n#import string\n#stopwords=nltk.corpus.stopwords.words('english')","d7a629ac":"dir(wn)","6fced9c2":"print(ps.stem('meanness'))\nprint(ps.stem('meaning'))","492ffb30":"print(wn.lemmatize('meanness'))\nprint(wn.lemmatize('meaning'))","ca252309":"print(ps.stem('goose'))\nprint(ps.stem('geese'))","cbc1316a":"print(wn.lemmatize('goose'))\nprint(wn.lemmatize('geese'))","3ef095d5":"data=pd.read_csv(\"\/kaggle\/input\/nlp-data-set\/SMSSpamCollection.tsv\" , sep='\\t', header = None)\ndata.columns = ['label', 'body_text']\ndata.head()","f499792a":"def clean_text(text):\n    text = \"\".join([word for word in text if word not in string.punctuation])\n    tokens=re.split('\\W+', text)\n    text = [word for word in tokens if word not in stopwords]\n    return text\ndata['body_text_nostop']=data['body_text'].apply(lambda x : clean_text(x.lower()))\ndata.head()","eb887397":"def lemmatizing(tokenized_text):\n    text = [wn.lemmatize(word) for word in tokenized_text]\n    return text\ndata['body_text_lemmatized']=data['body_text_nostop'].apply(lambda x : lemmatizing(x))\ndata.head()","627f9a49":"# import string\n# import re\n#import nltk\npd.set_option('display.max_colwidth', 100)\n\nstopwords=nltk.corpus.stopwords.words('english')\nps=nltk.PorterStemmer()\n    \ndata=pd.read_csv(\"\/kaggle\/input\/nlp-data-set\/SMSSpamCollection.tsv\" , sep='\\t', header = None)\ndata.columns = ['label', 'body_text']\ndata.head()","d1201c1e":"def clean_text(text):\n    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n    tokens=re.split('\\W+', text)\n    text = [ps.stem(word) for word in tokens if word not in stopwords]\n    return text\n#data['body_text_nostop']=data['body_text'].apply(lambda x : clean_text(x.lower()))\n#data.head()","1d6ba16b":"from sklearn.feature_extraction.text import CountVectorizer\ncount_vect = CountVectorizer(analyzer = clean_text)\nx_counts=count_vect.fit_transform(data['body_text'])\nprint(x_counts.shape)\nprint(count_vect.get_feature_names())","857e30b2":"data_sample=data[0:20]\ncount_vect_sample = CountVectorizer(analyzer = clean_text)\nx_counts_sample=count_vect_sample.fit_transform(data_sample['body_text'])\nprint(x_counts_sample.shape)\nprint(count_vect_sample.get_feature_names())","5da7fe6a":"x_counts_sample","4eadba0b":"df=pd.DataFrame(x_counts_sample.toarray())\ndf.head()","a38e284e":"df.columns = count_vect_sample.get_feature_names()\ndf.head()","916f2a47":"# import string\n# import re\n#import nltk\npd.set_option('display.max_colwidth', 100)\n\nstopwords=nltk.corpus.stopwords.words('english')\nps=nltk.PorterStemmer()\n    \ndata=pd.read_csv(\"\/kaggle\/input\/nlp-data-set\/SMSSpamCollection.tsv\" , sep='\\t', header = None)\ndata.columns = ['label', 'body_text']\ndata.head()","49cf8bdc":"def clean_text(text):\n    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n    tokens=re.split('\\W+', text)\n    text = \" \".join([ps.stem(word) for word in tokens if word not in stopwords])\n    return text\ndata['cleaned_text']=data['body_text'].apply(lambda x : clean_text(x))\ndata.head()","284f16b4":"from sklearn.feature_extraction.text import CountVectorizer\nngram_vect = CountVectorizer(ngram_range=(2,2)) # 1,2,3 = unigram, bigram, trigram\nx_counts=ngram_vect.fit_transform(data['cleaned_text'])\nprint(x_counts.shape)\nprint(ngram_vect.get_feature_names())","a106e013":"data_sample= data[0:20]\nngram_vect_sample = CountVectorizer(ngram_range=(2,2)) # 1,2,3 = unigram, bigram, trigram\nx_counts_sample=ngram_vect_sample.fit_transform(data_sample['cleaned_text'])\nprint(x_counts_sample.shape)\nprint(ngram_vect_sample.get_feature_names())","d3bd62f3":"df=pd.DataFrame(x_counts_sample.toarray())\ndf.columns = ngram_vect_sample.get_feature_names()\ndf.head()","fba27503":"# import string\n# import re\n#import nltk\npd.set_option('display.max_colwidth', 100)\n\nstopwords=nltk.corpus.stopwords.words('english')\nps=nltk.PorterStemmer()\n    \ndata=pd.read_csv(\"\/kaggle\/input\/nlp-data-set\/SMSSpamCollection.tsv\" , sep='\\t', header = None)\ndata.columns = ['label', 'body_text']\ndata.head() ","0cac1164":"def clean_text(text):\n    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n    tokens=re.split('\\W+', text)\n    text = [ps.stem(word) for word in tokens if word not in stopwords]\n    return text","dddc8fa3":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf_vect=TfidfVectorizer(analyzer=clean_text)\nx_tfidf=tfidf_vect.fit_transform(data['body_text'])\nprint(x_tfidf.shape)\nprint(tfidf_vect.get_feature_names())","e1f0cc45":"data_sample= data[0:20]\ntfidf_vect_sample=TfidfVectorizer(analyzer=clean_text)\nx_tfidf_sample=tfidf_vect_sample.fit_transform(data_sample['body_text'])\nprint(x_tfidf_sample.shape)\nprint(tfidf_vect_sample.get_feature_names())","26a60d4a":"df=pd.DataFrame(x_tfidf_sample.toarray())\ndf.columns = tfidf_vect_sample.get_feature_names()\ndf.head()","713d946b":"data=pd.read_csv(\"\/kaggle\/input\/nlp-data-set\/SMSSpamCollection.tsv\" , sep='\\t', header = None)\ndata.columns = ['label', 'body_text']\ndata.head()","feff216a":"# Create feature for text message length\ndata['body_len'] = data['body_text'].apply(lambda x : len(x) - x.count(\" \"))\ndata.head()","1cd224b6":"# Create feature for % of text that is  punctuation\nimport string\ndef count_punc(text):\n    count = sum([1 for char in text if char in string.punctuation])\n    return round(count\/(len(text) - text.count(\" \")),3)*100\ndata['punc%']= data['body_text'].apply(lambda x : count_punc(x))\ndata.head()","e5007a1d":"# Evalute new features\nimport matplotlib.pyplot as plt\n%matplotlib inline","a5d7bf66":"bins= np.linspace(0,200,40)\nplt.hist(data[data['label'] == 'spam']['body_len'], bins, alpha=0.5, normed=True, label='spam')\nplt.hist(data[data['label'] == 'ham']['body_len'], bins, alpha=0.5, normed=True, label='ham')\nplt.legend(loc='best')\nplt.show()","7e614982":"bins= np.linspace(0,50,40)\nplt.hist(data[data['label'] == 'spam']['punc%'], bins, alpha=0.5, normed=True, label='spam')\nplt.hist(data[data['label'] == 'ham']['punc%'], bins, alpha=0.5, normed=True, label='ham')\nplt.legend(loc='best')\nplt.show()","95ab87fd":"bins= np.linspace(0,200,40)\nplt.hist(data['body_len'], bins)\nplt.title('Body Length Distribution')\nplt.show()","ce4de152":"bins= np.linspace(0,50,40)\nplt.hist(data['punc%'], bins)\nplt.title('Punctuation Length Distribution')\nplt.show()","c1a7b60f":"for i in [1,2,3,4,5]:\n    plt.hist((data['punc%']) ** (1\/i), bins=40)\n    plt.title('Transformation : 1\/{}'.format(str(i)))\n    plt.show()","4d85a0f8":"import nltk\nimport re \nimport string\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nstopwords= nltk.corpus.stopwords.words('english')\ndata=pd.read_csv(\"\/kaggle\/input\/nlp-data-set\/SMSSpamCollection.tsv\" , sep='\\t', header = None)\ndata.columns = ['label', 'body_text']\n\ndef count_punc(text):\n    count = sum([1 for char in text if char in string.punctuation])\n    return round(count\/(len(text) - text.count(\" \")),3)*100\n\ndata['body_len']= data['body_text'].apply(lambda x : len(x) - x.count(\" \"))\ndata['punc%']= data['body_text'].apply(lambda x : count_punc(x))\n\n\ndef clean_text(text):\n    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n    tokens= re.split('\\W+', text)\n    text = [ps.stem(word) for word in tokens if word not in stopwords]\n    return text\n\ntfidf_vect = TfidfVectorizer(analyzer=clean_text)\nx_tfidf = tfidf_vect.fit_transform(data['body_text'])\n\nx_features = pd.concat([data['body_len'], data['punc%'], pd.DataFrame(x_tfidf.toarray())], axis =1)\nx_features.head()","3abdcd2c":"from sklearn.ensemble import RandomForestClassifier","ad7d4db1":"print(dir(RandomForestClassifier))\nprint(RandomForestClassifier())","4d2b2b17":"from sklearn.model_selection import KFold, cross_val_score","60fd61ee":"rf = RandomForestClassifier(n_jobs=-1) #n_jobs will execute all the decesion tree parallel\nK_Fold = KFold(n_splits=5)\ncross_val_score(rf, x_features, data['label'], cv=K_Fold, scoring = 'accuracy', n_jobs=-1)","33444ba1":"import nltk\nimport re \nimport string\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nstopwords= nltk.corpus.stopwords.words('english')\ndata=pd.read_csv(\"\/kaggle\/input\/nlp-data-set\/SMSSpamCollection.tsv\" , sep='\\t', header = None)\ndata.columns = ['label', 'body_text']\n\ndef count_punc(text):\n    count = sum([1 for char in text if char in string.punctuation])\n    return round(count\/(len(text) - text.count(\" \")),3)*100\n\ndata['body_len']= data['body_text'].apply(lambda x : len(x) - x.count(\" \"))\ndata['punc%']= data['body_text'].apply(lambda x : count_punc(x))\n\n\ndef clean_text(text):\n    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n    tokens= re.split('\\W+', text)\n    text = [ps.stem(word) for word in tokens if word not in stopwords]\n    return text\n\ntfidf_vect = TfidfVectorizer(analyzer=clean_text)\nx_tfidf = tfidf_vect.fit_transform(data['body_text'])\n\nx_features = pd.concat([data['body_len'], data['punc%'], pd.DataFrame(x_tfidf.toarray())], axis =1)\nx_features.head()","d1dafd11":"from sklearn.metrics import precision_recall_fscore_support as score \nfrom sklearn.model_selection import train_test_split","b02a6fd6":"x_train, x_test, y_train, y_test = train_test_split(x_features, data['label'], test_size = 0.2)","6eae8134":"rf=RandomForestClassifier(n_estimators=50, max_depth=20, n_jobs=-1)\nrf_model = rf.fit(x_train, y_train)","ef7396b7":"sorted(zip(rf_model.feature_importances_, x_train.columns), reverse=True)[0:10]","931fb6f1":"y_pred=rf_model.predict(x_test)\nprecision, recall, fscore, support = score(y_test, y_pred, pos_label = 'spam', average='binary')","4b289630":"print('precision: {} \/ recall: {} \/ accuracy: {}'. format(round(precision, 3), round(recall, 3), \n                                                         round((y_pred==y_test).sum() \/ len(y_pred),3)))","684f46a0":"import nltk\nimport re \nimport string\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nstopwords= nltk.corpus.stopwords.words('english')\ndata=pd.read_csv(\"\/kaggle\/input\/nlp-data-set\/SMSSpamCollection.tsv\" , sep='\\t', header = None)\ndata.columns = ['label', 'body_text']\n\ndef count_punc(text):\n    count = sum([1 for char in text if char in string.punctuation])\n    return round(count\/(len(text) - text.count(\" \")),3)*100\n\ndata['body_len']= data['body_text'].apply(lambda x : len(x) - x.count(\" \"))\ndata['punc%']= data['body_text'].apply(lambda x : count_punc(x))\n\n\ndef clean_text(text):\n    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n    tokens= re.split('\\W+', text)\n    text = [ps.stem(word) for word in tokens if word not in stopwords]\n    return text\n\ntfidf_vect = TfidfVectorizer(analyzer=clean_text)\nx_tfidf = tfidf_vect.fit_transform(data['body_text'])\n\nx_features = pd.concat([data['body_len'], data['punc%'], pd.DataFrame(x_tfidf.toarray())], axis =1)\nx_features.head()","c718126f":"from sklearn.metrics import precision_recall_fscore_support as score \nfrom sklearn.model_selection import train_test_split","411e79e4":"x_train, x_test, y_train, y_test = train_test_split(x_features, data['label'], test_size = 0.2)","e1460c98":"def train_RF(n_est, depth):\n    rf=RandomForestClassifier(n_estimators=n_est, max_depth=depth, n_jobs=-1)\n    rf_model=rf.fit(x_train, y_train)\n    y_pred=rf_model.predict(x_test)\n    precision, recall, fscore, support = score(y_test, y_pred, pos_label='spam', average='binary')\n    print('Est: {} \/ Depth: {} ---- Precision : {} \/ Recall : {} \/ Accuracy : {}'.format(\n         n_est, depth, round(precision,3), round(recall, 3), round((y_pred==y_test).sum() \/ len(y_pred),3)))","f7777231":"for n_est in [10, 50, 100]:\n    for depth in [10 , 20 , 30, None]:\n        train_RF(n_est, depth)","6abec8a5":"import nltk\nimport re \nimport string\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nstopwords= nltk.corpus.stopwords.words('english')\ndata=pd.read_csv(\"\/kaggle\/input\/nlp-data-set\/SMSSpamCollection.tsv\" , sep='\\t', header = None)\ndata.columns = ['label', 'body_text']\n\ndef count_punc(text):\n    count = sum([1 for char in text if char in string.punctuation])\n    return round(count\/(len(text) - text.count(\" \")),3)*100\n\ndata['body_len']= data['body_text'].apply(lambda x : len(x) - x.count(\" \"))\ndata['punc%']= data['body_text'].apply(lambda x : count_punc(x))\n\n\ndef clean_text(text):\n    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n    tokens= re.split('\\W+', text)\n    text = [ps.stem(word) for word in tokens if word not in stopwords]\n    return text\n\ntfidf_vect = TfidfVectorizer(analyzer=clean_text)\nx_tfidf = tfidf_vect.fit_transform(data['body_text'])\nx_tfidf_feat = pd.concat([data['body_len'], data['punc%'], pd.DataFrame(x_tfidf.toarray())], axis =1)\n\ncount_vect = CountVectorizer(analyzer=clean_text)\nx_count = count_vect.fit_transform(data['body_text'])\nx_count_feat = pd.concat([data['body_len'], data['punc%'], pd.DataFrame(x_count.toarray())], axis =1)\n\nx_count_feat.head()","700d160a":"from sklearn.model_selection import GridSearchCV","4500a0ce":"rf=RandomForestClassifier()\nparam = {'n_estimators' : [10, 150, 130],\n        'max_depth' : [30, 60, 90 , None ] }\ngs =GridSearchCV(rf, param, cv=5, n_jobs = -1 )\ngs_fit=gs.fit(x_tfidf_feat, data['label'])\npd.DataFrame(gs_fit.cv_results_).sort_values('mean_test_score', ascending=False)[0:5]\n    ","ff1fb298":"rf=RandomForestClassifier()\nparam = {'n_estimators' : [10, 150, 130],\n        'max_depth' : [30, 60, 90 , None ] }\ngs =GridSearchCV(rf, param, cv=5, n_jobs = -1 )\ngs_fit=gs.fit(x_count_feat, data['label'])\npd.DataFrame(gs_fit.cv_results_).sort_values('mean_test_score', ascending=False)[0:5]","b4081f1f":"import nltk\nimport re \nimport string\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nstopwords= nltk.corpus.stopwords.words('english')\ndata=pd.read_csv(\"\/kaggle\/input\/nlp-data-set\/SMSSpamCollection.tsv\" , sep='\\t', header = None)\ndata.columns = ['label', 'body_text']\n\ndef count_punc(text):\n    count = sum([1 for char in text if char in string.punctuation])\n    return round(count\/(len(text) - text.count(\" \")),3)*100\n\ndata['body_len']= data['body_text'].apply(lambda x : len(x) - x.count(\" \"))\ndata['punc%']= data['body_text'].apply(lambda x : count_punc(x))\n\n\ndef clean_text(text):\n    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n    tokens= re.split('\\W+', text)\n    text = [ps.stem(word) for word in tokens if word not in stopwords]\n    return text\n\ntfidf_vect = TfidfVectorizer(analyzer=clean_text)\nx_tfidf = tfidf_vect.fit_transform(data['body_text'])\n\nx_features = pd.concat([data['body_len'], data['punc%'], pd.DataFrame(x_tfidf.toarray())], axis =1)\nx_features.head()","16764ea8":"from sklearn.ensemble import GradientBoostingClassifier","118c193e":"print(dir(GradientBoostingClassifier))\nprint(GradientBoostingClassifier())","94f098bc":"def train_GB(est, max_depth, lr):\n    GB = GradientBoostingClassifier(n_estimators=est, max_depth=max_depth, learning_rate=lr)\n    GB_model = GB.fit(x_train, y_train)\n    y_pred = GB_model.predict(x_test)\n    precision, recall, fscore, support = score(y_test, y_pred, pos_label = 'spam', average = 'binary')\n    print('Est : {} \/ Max_Depth : {} \/ LR : {} ----- Precision : {} \/ Recall : {} \/  Accuracy : {}'.format(est, max_depth, lr, \n                                                                     round(precision,3),round(recall,3),round((y_pred==y_test).sum() \/ len(y_pred),3)))\n    ","ad2afdf8":"for n_est in [50,100,150]:\n    for max_depth in [3, 7 ,11, 15]:\n        for lr in [0.01, 0.1, 1]:\n            train_GB(n_est,max_depth,lr)","66170e8c":"import nltk\nimport re \nimport string\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nstopwords= nltk.corpus.stopwords.words('english')\ndata=pd.read_csv(\"\/kaggle\/input\/nlp-data-set\/SMSSpamCollection.tsv\" , sep='\\t', header = None)\ndata.columns = ['label', 'body_text']\n\ndef count_punc(text):\n    count = sum([1 for char in text if char in string.punctuation])\n    return round(count\/(len(text) - text.count(\" \")),3)*100\n\ndata['body_len']= data['body_text'].apply(lambda x : len(x) - x.count(\" \"))\ndata['punc%']= data['body_text'].apply(lambda x : count_punc(x))\n\n\ndef clean_text(text):\n    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n    tokens= re.split('\\W+', text)\n    text = [ps.stem(word) for word in tokens if word not in stopwords]\n    return text\n\ntfidf_vect = TfidfVectorizer(analyzer=clean_text)\nx_tfidf = tfidf_vect.fit_transform(data['body_text'])\nx_tfidf_feat = pd.concat([data['body_len'], data['punc%'], pd.DataFrame(x_tfidf.toarray())], axis =1)\n\ncount_vect = CountVectorizer(analyzer=clean_text)\nx_count = count_vect.fit_transform(data['body_text'])\nx_count_feat = pd.concat([data['body_len'], data['punc%'], pd.DataFrame(x_count.toarray())], axis =1)\n\nx_count_feat.head()","665b3709":"gb = GradientBoostingClassifier()\nparam = {'n_estimators' : [100, 150], 'max_depth' : [7, 11, 15], 'learning_rate' : [0.1]}\n\ngs= GridSearchCV(gb. param, cv=5, n_jobs= -1)\ncv_fit = gs.fit(x_tfidf_feat , data['label'])\npd.DataFrame(cv_fit.cv_results_).sort_values('mean_test_score', ascending=False)[0:5]","53411400":"gb = GradientBoostingClassifier()\nparam = {'n_estimators' : [100, 150], 'max_depth' : [7, 11, 15], 'learning_rate' : [0.1]}\n\ngs= GridSearchCV(gb. param, cv=5, n_jobs= -1)\ncv_fit = gs.fit(x_count_feat , data['label'])\npd.DataFrame(cv_fit.cv_results_).sort_values('mean_test_score', ascending=False)[0:5]","bfaebbd0":"import nltk\nimport re \nimport string\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nstopwords= nltk.corpus.stopwords.words('english')\ndata=pd.read_csv(\"\/kaggle\/input\/nlp-data-set\/SMSSpamCollection.tsv\" , sep='\\t', header = None)\ndata.columns = ['label', 'body_text']\n\ndef count_punc(text):\n    count = sum([1 for char in text if char in string.punctuation])\n    return round(count\/(len(text) - text.count(\" \")),3)*100\n\ndata['body_len']= data['body_text'].apply(lambda x : len(x) - x.count(\" \"))\ndata['punc%']= data['body_text'].apply(lambda x : count_punc(x))\n\n\ndef clean_text(text):\n    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n    tokens= re.split('\\W+', text)\n    text = [ps.stem(word) for word in tokens if word not in stopwords]\n    return text","a43e8bf5":"x_train, x_test, y_train, y_test = train_test_split(data[['body_text','body_len','punc%']], data['label'] ,test_size = 0.2)","c4bf2d15":"tfidf_vect = TfidfVectorizer(analyzer=clean_text)\ntfidf_vect_fit = tfidf_vect.fit(x_train['body_text'])\n\ntfidf_train = tfidf_vect_fit.transform(x_train['body_text'])\ntfidf_test = tfidf_vect_fit.transform(x_test['body_text'])\n\nx_train_vect = pd.concat([x_train[['body_len','punc%']].reset_index(drop=True),\n         pd.DataFrame(tfidf_train.toarray())], axis = 1)\n\nx_test_vect = pd.concat([x_test[['body_len','punc%']].reset_index(drop=True),\n         pd.DataFrame(tfidf_test.toarray())], axis = 1)\nx_train_vect.head()","e9f6a1ac":"import time","ab55cf58":"rf=RandomForestClassifier(n_estimators=150, max_depth=None, n_jobs=-1)\n\nstart=time.time()\nrf_model=rf.fit(x_train_vect, y_train)\nend=time.time()\nfit_time = (end-start)\n\nstart=time.time()\ny_pred = rf_model.predict(x_test_vect)\nend=time.time()\npred_time = (end-start)\n\nprecision, recall, fscore, support = score(y_test, y_pred, pos_label='spam', average='binary')\nprint('Fit_time: {} \/ Predict_time : {} \/ Precision : {} \/ Recall : {} \/ Accuracy : {}'.format(round(fit_time,3),round(pred_time,3),round(precision,3), round(recall, 3), \n                                                                round((y_pred==y_test).sum() \/ len(y_pred),3)))\n","105648d8":"\ngb = GradientBoostingClassifier(n_estimators=150, max_depth=11)\nstart=time.time()\ngb_model=gb.fit(x_train_vect, y_train)\nend=time.time()\nfit_time = (end-start)\n\nstart=time.time()\ny_pred = gb_model.predict(x_test_vect)\nend=time.time()\npred_time = (end-start)\n\nprecision, recall, fscore, support = score(y_test, y_pred, pos_label='spam', average='binary')\nprint('Fit_time: {} \/ Predict_time : {} \/ Precision : {} \/ Recall : {} \/ Accuracy : {}'.format(round(fit_time,3),round(pred_time,3),round(precision,3), round(recall, 3), \n                                                                round((y_pred==y_test).sum() \/ len(y_pred),3)))\n","503bae9f":" # Machine Learning\n \n ## Random Forest Model","73c4cb09":"# Box Cox Power Transformation","d8676324":"# Implementation of Tokenization","be7ce305":"# Vectorizing Raw Data - process of encoding text as integers to create feature vectors","fdd4b4b7":"# N-Gram Vectorizing\n### creates a document-term matrix where counts still occupy the cell but instead of the columns representing single terms, they represent all combination of adjacent words of length n in your text.","51172d20":"### The above data is quite vast so we will apply it on smaller sample","0c18d17f":"Vector Types = 1] Count vectorization, 2] N-grams, 3] Term frequency - numeric document frequency (TF-IDF)","ac9f5036":"# Model Selection : Results","e8b3e85e":"# Gradient boosting Grid Search","b3b2c5cc":"# Replacing a specific string","c73fd82b":"# Implementation: Removing Punctuation\n\n** preprocessing data\n1. Remove Punctuation\n2. Tokenization\n3. Removing stopwords\n4. Lemmatize\/Stem","3112c062":"# Regular Expression\n* Text string for describing a search pattern\n\n## Uses\n* Identifying white spaces between words and token\n* Identifying \/creating delimiters or end-of-line escape characters\n* removing punctuations or numbers from your text\n* cleaning html tags from your text\n* Identifying some textual patterns you are interested in\n\n## Application\n* Confirming password meets criteria\n* searching url for substring \n* searching for files on your computer\n* Document scrapping","09632a40":"# Explore Random Forest Hold Out Test Set","4a0d405b":"# Evaluate GB with GridSearchCV","329bc9c0":"# Lemmatizing \n### Process of grouping together the inflected forms of a word so they can be analyzed as a single term, identified by the word's lemma\n\n### Using vocabulary analysis of words aiming to remove inflectional ending to return the dictionary for of  a word  ","6b51d21a":"# Learnig how to use regular Expression","e7eb2dcc":"# Feature Engineering\n\n### Feature Creation","cac2a799":"# Other Regex methods\n* re.search()\n* re.match()\n* re.fullmatch()\n* re.finditer()\n* re.escape()","9d59c61e":"# Exploring Dataset","f947ee03":"# Raw Data","1933ba7d":"# Remove Punctuation","2ed6b8f2":"# Build your own Grid-search","169ea110":"# GridSearch and cross-validation","27b7fb52":"# Apply CounterVectorizer","b345d633":"# Remove StopWords","a5541dd3":"# Inverse Document Frequency Weighting (TF-IDF Equation)","37649e30":"# Apply to small sample","48e26a2b":"# Machine Learning Pipeline\n\n1. Raw Text - model can't distingush words\n2. Tokenize - tell the model what to look at\n3. clean text - remove stop words \/ punctuation, stemming, etc\n4. vectorize - convert to numeric form\n5. Spam filter - system to filter emails","f401b01b":" ### create function to remove punctuation, tokenize, remove stopwords and stem","cf4cf98c":"# Process\n### Raw Text - model can't distingush words\n### Tokenize - tell the model what to look for\n### Clean text - remove stop words, punctuations, stemming, etc\n### vectorize - convert to numeric form\n### machine learning algorithm - fit\/train model\n### spam filter - systems to filter mail","7fbb90c6":"# Supplemental Data cleaning\n### Stemming - process of removing inflected(or sometimes derived) words to their word stem or root \n### Crudely chopping off the end of the word to leave only the base\n* example - Berries\/berry = Berri\n\n## Types\n\n* Porter Stemmer \n* Snowball Stemmer \n* Lancaster Stemmer\n* Regex-Based Stemmer","f2cb745e":"# Final Model Selection","ec4da94d":"### Reading in text data and why we need to clean it","70375dab":"# Lemmatize","5e59679b":"# Short cut we can use instead of the above method","d47f31da":"# Apply Tfidfvectorizer","a5f89eec":"# Sparse Matrix\n#### A matrix in which most entries are zero. in the interest of efficient storage, a sparse matrix will be stored by only storing locations of the non-zero elements.","0a27d5f2":"# Grid Search","03284916":"# Identifying features for transformation"}}