{"cell_type":{"885ef839":"code","bf7c8364":"code","87d6ccaa":"code","412e2b2d":"code","d20cc468":"code","67891875":"code","7080afe0":"code","00f121df":"code","31c7453d":"code","e8e54773":"code","32f49448":"code","ee3a7fe2":"code","6e4f01bc":"code","ce87ada4":"code","11e3bc79":"code","25df3c78":"code","f643f5fb":"code","e79e3bef":"code","ab1a49c9":"code","43fa18f2":"code","8d42d65f":"code","ebbde568":"code","276b5bb6":"markdown","f6885225":"markdown","099859ca":"markdown","600d43b0":"markdown","06729ca3":"markdown","7237d211":"markdown","5a53fb57":"markdown","c1146db9":"markdown"},"source":{"885ef839":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","bf7c8364":"data = pd.read_csv(\"..\/input\/diabetes.csv\")","87d6ccaa":"data.head()","412e2b2d":"data.info()","d20cc468":"data.isnull().values.any()","67891875":"data.describe()","7080afe0":"data.columns = map(str.lower, data.columns)\ndata.columns","00f121df":"fig, ax = plt.subplots(4,2, figsize=(16,16))\nsns.distplot(data.age, bins = 20, ax=ax[0,0]) \nsns.distplot(data.pregnancies, bins = 20, ax=ax[0,1]) \nsns.distplot(data.glucose, bins = 20, ax=ax[1,0]) \nsns.distplot(data.bloodpressure, bins = 20, ax=ax[1,1]) \nsns.distplot(data.skinthickness, bins = 20, ax=ax[2,0])\nsns.distplot(data.insulin, bins = 20, ax=ax[2,1])\nsns.distplot(data.diabetespedigreefunction, bins = 20, ax=ax[3,0]) \nsns.distplot(data.bmi, bins = 20, ax=ax[3,1]) ","31c7453d":"sns.regplot(x = data.pregnancies, y = data.glucose)","e8e54773":"sns.set(font_scale = 1.15)\nplt.figure(figsize = (14, 10))\n\nsns.heatmap(data.corr(), vmax = 1, linewidths = 0.5, fmt= '.1f',\n            square = True, annot = True, cmap = 'YlGnBu', linecolor = \"white\")\nplt.title('Correlation of Features');","32f49448":"# Normalization\n# Normalization Formula; (x - min(x))\/max(x)-min(x)\ny = data.outcome.values\nx = data.drop([\"outcome\"], axis = 1)\n\nx = (x - np.min(x))\/(np.max(x)-np.min(x)).values","ee3a7fe2":"# Train & Test Split\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)\n\nfeatures = x_train.T\nlabels = y_train.T\ntest_features = x_test.T\ntest_labels = y_test.T\n\nprint(\"features: \", features.shape)\nprint(\"labels: \", labels.shape)\nprint(\"test_features: \", test_features.shape)\nprint(\"test_labels: \", test_labels.shape)\n\n","6e4f01bc":"#Parameter Initialize \ndef initialize_weights_and_bias(dimension):\n    w = np.full((dimension, 1),0.01)\n    b= 0.0\n    return w,b","ce87ada4":"# Sigmoid Function**\n# Sigmoid Function Formula; 1\/(1+e^-x)\ndef sigmoid(z):\n    y_head = 1\/(1+np.exp(-z))\n    return y_head\n","11e3bc79":"# Forward & Backward Propagation\ndef foward_and_backward_propagation(w, b, x_train, y_train):\n    #Forward Propagation\n    z = np.dot(w.T, x_train) + b\n    y_head = sigmoid(z)\n    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n    cost = (np.sum(loss))\/x_train.shape[1]          #x_train.shape[1] is for scaling\n    \n    # Backward Propagation\n    derivative_weight = (np.dot(x_train, ((y_head-y_train).T)))\/x_train.shape[1]\n    derivative_bias = np.sum(y_head-y_train)\/x_train.shape[1]\n    gradients = {\"derivative_weight\": derivative_weight, \"derivative_bias\": derivative_bias}\n    \n    return cost, gradients","25df3c78":"#Updating Parameters\ndef update(w, b, x_train, y_train, learning_rate, number_of_iterations):\n    cost_list = []\n    cost_list2 = []\n    index = []\n    \n    # Updating (learning) parameters is number_of_iterations times\n    for i in range(number_of_iterations):\n        \n        cost, gradients = foward_and_backward_propagation(w, b, x_train, y_train)\n        cost_list.append(cost)\n        #Let's update\n        w = w - learning_rate * gradients[\"derivative_weight\"]\n        b = b - learning_rate * gradients[\"derivative_bias\"]\n        if i % 10 == 0:\n            cost_list2.append(cost)\n            index.append(i)\n            print(\"Cost after iterations %i: %f\" %(i, cost))\n            \n    # We update (learn) parameters weights and bias\n    parameters = {\"weight\": w, \"bias\": b}\n    plt.plot(index, cost_list2)\n    plt.title(\"Cost-Iteration Relation\")\n    plt.xticks(index, rotation = \"vertical\")\n    plt.xlabel(\"Number of iterations\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters, gradients, cost_list","f643f5fb":"#Prediction\ndef predict(w, b, x_test):\n    # x_test is a input for forward propagation\n    z = sigmoid(np.dot(w.T, x_test)+b)\n    y_prediction = np.zeros((1, x_test.shape[1]))\n    #\n    #\n    for i in range(z.shape[1]):\n        if z[0, i] <= 0.5:\n            y_prediction[0, i] = 0\n        else:\n            y_prediction[0, i] = 1\n            \n    return y_prediction","e79e3bef":"# Logistic Regression\ndef logistic_regression(features, labels, test_features, test_labels, learning_rate ,  num_iterations):\n    # Initialize\n    dimension =  features.shape[0]  # It is 8\n    w,b = initialize_weights_and_bias(dimension)\n    parameters, gradients, cost_list = update(w, b, features, labels, learning_rate,num_iterations)\n    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],test_features)\n    # Print test errors\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - test_labels)) * 100))\n    \nlogistic_regression(features, labels, test_features, test_labels,learning_rate = 1.5, num_iterations = 300)   ","ab1a49c9":"# Logistic Regression with Scikit-Learn\nfrom sklearn import linear_model\nlogreg = linear_model.LogisticRegression(random_state = 42,max_iter= 150)\nprint(\"test accuracy: {} \".format(logreg.fit(features.T, labels.T).score(test_features.T, test_labels.T)))","43fa18f2":"labels = labels.reshape(labels.shape[0], -1).T\ntest_labels = test_labels.reshape(test_labels.shape[0], -1).T\n\nprint(labels.shape)\nprint(test_labels.shape)","8d42d65f":"class Artificial_Neural_Network(object):\n    \n    def __init__(self, xTrain, xTest, yTrain, yTest):\n        # Define train and test data\n        self.xTrain = xTrain\n        self.xTest = xTest\n        self.yTrain = yTrain.reshape(yTrain.shape[0],-1)\n        self.yTest = yTest.reshape(yTest.shape[0],-1)\n\n        # Define hyperparameters\n        self.inputLayerSize = self.xTrain.shape[0] # nx <-> Number of samples\n        self.hiddenLayerSize = 4\n        self.outputLayerSize = self.yTrain.shape[0]\n        \n    def initializeWeightsAndBias(self): #, inputLayerSize, hiddenLayerSize, outputLayerSize):\n        \"\"\"\n        This function creates a vector of zeros of shape (inputLayerSize, 1) for w and initializes b to 0.\n\n        Argument:\n        inputLayerSize -- size of the input layer\n        hiddenLayerSize -- size of the hidden layer\n        outputLayerSize -- size of the output layer\n\n        Returns:\n        params -- python dictionary containing your parameters:\n                        W1 -- weight matrix of shape (hiddenLayerSize, inputLayerSize)\n                        b1 -- bias vector of shape (hiddenLayerSize, 1)\n                        W2 -- weight matrix of shape (outputLayerSize, hiddenLayerSize)\n                        b2 -- bias vector of shape (outputLayerSize, 1)\n        \"\"\"\n        np.random.seed(23) # We set up a seed so that your output matches ours \n                           # although the initialization is random.\n        \n        W1 = np.random.randn(self.inputLayerSize, \n                             self.hiddenLayerSize) * 0.01\n        b1 = np.zeros(shape=(self.hiddenLayerSize, 1))\n        W2 = np.random.randn(self.hiddenLayerSize,\n                             self.outputLayerSize) * 0.01\n        b2 = np.zeros(shape=(self.outputLayerSize, 1))\n        \n        # assert(isinstance(B1, float) or isinstance(B1, int))\n        \n        assert (W1.shape == (self.inputLayerSize, self.hiddenLayerSize)), \"[W1] -> Unsuitable matrix size\"\n        assert (b1.shape == (self.hiddenLayerSize, 1))\n        assert (W2.shape == (self.hiddenLayerSize, self.outputLayerSize)), \"[W2] -> Unsuitable matrix size\"\n        assert (b2.shape == (self.outputLayerSize, 1))\n        \n        parameters = {\"W1\": W1,\n                      \"b1\": b1,\n                      \"W2\": W2,\n                      \"b2\": b2}   \n        \n        return parameters\n    \n    def sigmoid(self, Z):\n        \"\"\" Apply and compute sigmoid activation function to scalar, vector, or matrix (Z)\n\n        Arguments:\n        Z -- A scalar or numpy array of any size.\n\n        Return:\n        s -- sigmoid(z)\n        \"\"\"\n        return 1\/(1+np.exp(-Z))\n    \n    def forwardPropagation(self, X, parameters):\n        \"\"\" Propogate inputs though network\n        \n        Argument:\n        X -- input data of size (inputLayerSize, m)\n        parameters -- python dictionary containing your parameters (output of initialization function)\n\n        Returns:\n        A2 -- The sigmoid output of the second activation\n        cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\"\n        \"\"\"\n        # Retrieve each parameter from the dictionary \"parameters\"\n        W1 = parameters['W1']\n        b1 = parameters['b1']\n        W2 = parameters['W2']\n        b2 = parameters['b2']\n\n        # Implement Forward Propagation to calculate A2 (probabilities)\n        Z1 = np.dot(W1.T, X) + b1\n        A1 = sigmoid(Z1)\n        Z2 = np.dot(W2.T, A1) + b2\n        yHat = self.sigmoid(Z2) # A2\n        \n        assert(yHat.shape == (1, X.shape[1]))\n    \n        cache = {\"Z1\": Z1,\n                 \"A1\": A1,\n                 \"Z2\": Z2,\n                 \"yHat\": yHat}    # A2\n    \n        return yHat, cache\n    \n    def computeCost(self, yHat, Y, parameters):\n        \"\"\" Compute cost for given X,Y, use weights already stored in class \n\n        Arguments:\n        yHat -- The sigmoid output of the second activation, of shape (1, number of examples)\n        Y -- \"true\" labels vector of shape (1, number of examples)\n        parameters -- python dictionary containing your parameters W1, b1, W2 and b2\n\n        Returns:\n        cost -- cross-entropy cost given equation (13)\n        \"\"\"\n        m = Y.shape[1] # number of example\n                      \n        # Retrieve W1 and W2 from parameters\n        W1 = parameters['W1']\n        W2 = parameters['W2']   \n                    \n        # Loss\n        logprobs = np.multiply(np.log(yHat), Y) + np.multiply((1 - Y), np.log(1 - yHat))\n        # Cost\n        cost = - (np.sum(logprobs)) \/ m     # m =  yTrain.shape[1]  is for scaling\n        \n        cost = np.squeeze(cost)     # makes sure cost is the dimension we expect. \n                                    # E.g., turns [[17]] into 17 \n        assert(isinstance(cost, float))\n                      \n        return cost\n\n    def backwardPropagation(self,parameters, cache,  X, Y):\n        \"\"\" Compute the gradients of parameters by implementing the backward propagation\n\n        Arguments:\n        parameters -- python dictionary containing our parameters \n        cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\".\n        X -- input data of shape (2, number of examples)\n        Y -- \"true\" labels vector of shape (1, number of examples)\n\n        Returns:\n        grads -- python dictionary containing your gradients with respect to different parameters\n        \"\"\"\n        m = X.shape[1]   \n                      \n        # First, retrieve W1 and W2 from the dictionary \"parameters\".       \n        W1 = parameters['W1']\n        W2 = parameters['W2']\n                      \n        # Retrieve also A1 and A2 from dictionary \"cache\".\n        A1 = cache['A1']\n        yHat = cache['yHat']                    \n                      \n        # Backward propagation: calculate dW1, db1, dW2, db2.                     \n        dZ2 = yHat - Y\n        dW2 = (1 \/ m) * np.dot(A1, dZ2.T)\n        db2 = (1 \/ m) * np.sum(dZ2, axis=1, keepdims=True)\n        dZ1 = np.multiply(np.dot(W2, dZ2), 1 - np.power(A1, 2))\n        dW1 = (1 \/ m) * np.dot(X, dZ1.T)#(1 \/ m) * np.dot(dZ1, self.xTrain.T) # MATRIS BOYUTLARINA BAK dW1 ve dW2 ICIN\n        db1 = (1 \/ m) * np.sum(dZ1, axis=1, keepdims=True)   # m is for scaling \n\n        gradients = {\"dW1\": dW1,\n                     \"db1\": db1,\n                     \"dW2\": dW2,\n                     \"db2\": db2}\n                      \n        return gradients\n    \n    def updateParameters(self, parameters, gradients, learning_rate = 0.15):\n        \"\"\"\n        Updates parameters using the gradient descent update rule given above\n\n        Arguments:\n        parameters -- python dictionary containing your parameters \n        grads -- python dictionary containing your gradients \n\n        Returns:\n        parameters -- python dictionary containing your updated parameters \n        \"\"\"\n        # Retrieve each parameter from the dictionary \"parameters\"\n        W1 = parameters['W1']\n        b1 = parameters['b1']\n        W2 = parameters['W2']\n        b2 = parameters['b2']\n\n        # Retrieve each gradient from the dictionary \"grads\"\n        dW1 = gradients['dW1']\n        db1 = gradients['db1']\n        dW2 = gradients['dW2']\n        db2 = gradients['db2']\n        \n        # Update rule for each parameter\n        W1 = W1 - learning_rate * dW1\n        b1 = b1 - learning_rate * db1\n        W2 = W2 - learning_rate * dW2\n        b2 = b2 - learning_rate * db2\n\n        parameters = {\"W1\": W1,\n                      \"b1\": b1,\n                      \"W2\": W2,\n                      \"b2\": b2}\n\n        return parameters\n                      \n    def model(self, X, Y, num_iterations=10000, print_cost=False):\n        \"\"\"\n        Arguments:\n        X -- dataset of shape (2, number of examples)\n        Y -- labels of shape (1, number of examples)\n        n_h -- size of the hidden layer\n        num_iterations -- Number of iterations in gradient descent loop\n        print_cost -- if True, print the cost every 1000 iterations\n\n        Returns:\n        parameters -- parameters learnt by the model. They can then be used to predict.\n        \"\"\"\n        np.random.seed(3)\n        \n        costStr = []\n        indexStr = []\n        \n        # Initialize parameters, then retrieve W1, b1, W2, b2. Inputs: \"n_x, n_h, n_y\". Outputs = \"W1, b1, W2, b2, parameters\".\n        parameters = self.initializeWeightsAndBias()\n\n        W1 = parameters['W1']\n        b1 = parameters['b1']\n        W2 = parameters['W2']\n        b2 = parameters['b2']\n \n        # Loop (gradient descent)\n        for i in range(0, num_iterations):\n                      \n            # Forward propagation. Inputs: \"X, parameters\". Outputs: \"A2, cache\".\n            yHat, cache = self.forwardPropagation(X, parameters)\n\n            # Cost function. Inputs: \"A2, Y, parameters\". Outputs: \"cost\".\n            cost = self.computeCost(yHat, Y, parameters)\n            \n            # Backpropagation. Inputs: \"parameters, cache, X, Y\". Outputs: \"grads\".\n            gradients = self.backwardPropagation(parameters, cache, X, Y)\n\n            # Gradient descent parameter update. Inputs: \"parameters, grads\". Outputs: \"parameters\".\n            parameters = self.updateParameters(parameters, gradients, learning_rate = 0.0001)\n\n            # Print the cost every 1000 iterations\n            if print_cost and i % 1000 == 0:\n                costStr.append(cost)\n                indexStr.append(i)\n                print (\"Cost after iteration %i: %f\" % (i, cost))\n            \n        # Plot Cost Function\n        plt.plot(indexStr,costStr)\n        plt.xticks(indexStr,rotation='vertical')\n        plt.xlabel(\"Number of Iterarion\")\n        plt.ylabel(\"Cost\")\n        plt.show()\n            \n        return parameters\n\n    def predict(self, parameters, X):\n        \"\"\"\n        Using the learned parameters, predicts a class for each example in X\n\n        Arguments:\n        parameters -- python dictionary containing your parameters \n        X -- input data of size (n_x, m)\n\n        Returns\n        predictions -- vector of predictions of our model (red: 0 \/ blue: 1)\n        \"\"\"\n        # Computes probabilities using forward propagation, and classifies to 0\/1 using 0.5 as the threshold.\n        yHat, cache = self.forwardPropagation(X, parameters)\n        predictions = np.round(yHat)\n\n        \n        return predictions","ebbde568":"ANN = Artificial_Neural_Network(features, test_features, labels, test_labels)\nparameters = ANN.model(features, labels, num_iterations = 25000, print_cost=True)\npredictions = ANN.predict(parameters, features)\nprint('Train Accuracy: %d' % float((np.dot(labels, predictions.T) + np.dot(1 - labels, 1 - predictions.T)) \/ float(labels.size) * 100) + '%')","276b5bb6":"**Artificial Neural Networks**","f6885225":"# Diabet Prediction with Logistic Regression","099859ca":"This is an alternative piece of code to check whether our dataset has NaN or not.","600d43b0":"See distribution of every features of our dataset:","06729ca3":"Let\u2019s first upload the dataset for our analysis:","7237d211":"Have you ever come across a situation where you want to predict a binary outcome like:\n\n* Whether a person is satisfied with a product or not?\n* Whether a candidate will secure admission to a graduate school or not?\n* Of the two presidential candidates who will win the election?\n* If a plane will arrive at its destination at the scheduled time?\n\nA very simple Machine Learning algorithm which will come to your rescue is Logistic Regression.\n\nLogistic Regression is a classification algorithm which is used when we want to predict a categorical variable (Yes\/No, Pass\/Fail s,mply 1\/0 situations) based on a set of independent variable(s).\n\nIn the Logistic Regression model, the log of odds of the dependent variable is modeled as a linear combination of the independent variables.\n\nLet\u2019s get more clarity on Binary Logistic Regression using a practical example in Python.\n\nConsider a situation where you are interested in classifying an individual as diabetic or non-diabetic based on features like glucose concentration, blood pressure, age etc.\n\nFor our analysis, we\u2019ll be using Pima Indians Diabetes dataset which I found it on Kaggle's database.\n\nDiabetes is the binary dependent variable in this dataset with categories\u200a\u2014\u200apos\/neg (1\/0). We have the following eight independent variables;\n\n* Pregnant: Number of times pregnant\n* Glucose: Plasma glucose concentration (glucose tolerance test)\n* Pressure: Diastolic blood pressure (mm Hg)\n* Triceps: Skin fold thickness (mm)\n* Insulin: 2-Hr serum insulin (mu U\/ml)\n* Mass: Body mass index (weight in Kg\/ (height in m)\u00b2 )\n* Pedigree: Diabetes pedigree function\n* Age: Age (years)","5a53fb57":"We are seeing details here. All columns names, how many entries it has and if there is, null entries. It is evident from the summary statistic that there are no missing values in the dataset, they are being highlighted as non-null\u2019s. Since we do not have any NaN, we do not need to remove such observations.\n\n","c1146db9":"Now print the first five lines to get information about our data. We can see features and any other general information to understand what consist of our dataset."}}