{"cell_type":{"0f7a1bd2":"code","831f0423":"code","f0d564ad":"code","20281ccf":"code","0fd82d27":"code","b7386e8c":"code","ece04f4b":"code","3e9ba099":"code","f9c0ee22":"code","f5ad5da2":"code","b20083df":"code","9a98a086":"code","c0131e6e":"code","e51f6598":"code","f35f7a42":"code","79c37834":"code","2ddf0f2c":"code","825b392c":"code","6ea720f1":"code","145b8d95":"code","98e5890e":"code","eb5a8039":"code","839adf46":"code","aa93dcd0":"code","a5e5c25e":"code","c40bf2b8":"code","43b7b1e7":"code","cda108ba":"code","ed3e2289":"code","9f1d7647":"code","2ed1070b":"code","421d1ae5":"markdown","cd93f3cd":"markdown","50c779b9":"markdown","0804fd95":"markdown","ab01b4b5":"markdown","87f346cf":"markdown","bcb630e3":"markdown","2e623106":"markdown","17368f89":"markdown","f1e7d890":"markdown","55aa7403":"markdown","590936d0":"markdown","da64d744":"markdown","9d30a58c":"markdown"},"source":{"0f7a1bd2":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n%matplotlib inline","831f0423":"df = pd.read_csv('..\/input\/pima-indians-diabetes-database\/diabetes.csv',encoding='utf8', engine='python')\ndf.head(5)","f0d564ad":"df.shape","20281ccf":"df.isnull().values.any()","0fd82d27":"df.groupby(\"Outcome\").size()  ","b7386e8c":"df.describe()","ece04f4b":"df.info()","3e9ba099":"print(\"maximum value=\",df['Glucose'].max())\nprint(\"maximum value=\",df['BloodPressure'].max())\nprint(\"maximum value=\",df['SkinThickness'].max())\nprint(\"maximum value=\",df['Insulin'].max())\nprint(\"maximum value=\",df['BMI'].max())\nprint(\"maximum value=\",df['DiabetesPedigreeFunction'].max())\nprint(\"maximum value=\",df['Age'].max())","f9c0ee22":"print(\"manimum value=\",df['Glucose'].min())\nprint(\"manimum value=\",df['BloodPressure'].min())\nprint(\"manimum value=\",df['SkinThickness'].min())\nprint(\"manimum value=\",df['Insulin'].min())\nprint(\"manimum value=\",df['BMI'].min())\nprint(\"manimum value=\",df['DiabetesPedigreeFunction'].min())\nprint(\"manimum value=\",df['Age'].min())","f5ad5da2":"f,ax=plt.subplots(1,1,figsize=(10,4))\nsns.countplot(x=\"Outcome\",data=df,palette=\"plasma\")\n#df['Outcome'].value_counts().plot(kind='bar')","b20083df":"f,ax=plt.subplots(1,1,figsize=(20,4))\nsns.countplot(x=\"Pregnancies\",data=df,hue=\"Outcome\",palette=\"plasma\")","9a98a086":"df.hist(figsize=(12,12))  ","c0131e6e":"def plot_corr(df,size=11): \n    corr = df.corr() # calling the correlation function on the datafrmae\n    fig, ax = plt.subplots(figsize=(size,size))\n    ax.matshow(corr) # color code the rectangles by correlation value\n    plt.xticks(range(len(corr.columns)),corr.columns) # draw x tickmarks\n    plt.yticks(range(len(corr.columns)),corr.columns)\nplot_corr(df)    \n","e51f6598":"df.corr()\n","f35f7a42":"df.plot(kind= 'box' , subplots=True, layout=(3,3), sharex=False, sharey=False, figsize=(12,12)) ","79c37834":"f,ax=plt.subplots(1,1,figsize=(25,4))\nsns.kdeplot(df.loc[(df['Outcome']==1), 'Glucose'], color='r', shade=True, Label='1')\nsns.kdeplot(df.loc[(df['Outcome']==0), 'Glucose'], color='g', shade=True, Label='0')\nplt.xlabel('Glucose') ","2ddf0f2c":"f,ax=plt.subplots(1,1,figsize=(25,4))\nsns.kdeplot(df.loc[(df['Outcome']==1), 'BloodPressure'], color='c', shade=True, Label='1')\nsns.kdeplot(df.loc[(df['Outcome']==0), 'BloodPressure'], color='m', shade=True, Label='0')\nplt.xlabel('BloodPressure') ","825b392c":"from pandas.plotting import scatter_matrix\nscatter_matrix(df, figsize = (20,20),color='m')\nplt.show()","6ea720f1":"sns.pairplot(df,hue = 'Outcome', vars = ['Pregnancies','Glucose','BloodPressure','SkinThickness','Insulin','BMI','DiabetesPedigreeFunction','Age'] )","145b8d95":"f,ax=plt.subplots(1,2,figsize=(20,5))\nbox1=sns.violinplot(x=\"Outcome\",y=\"Glucose\",data=df,ax=ax[0])\nbox2=sns.violinplot(x=\"Outcome\",y=\"BloodPressure\",data=df,ax=ax[1])","98e5890e":"attributes = list(df.columns[:8])  # creates a list of all paramter names\nX = df[attributes].values  # masking the parameter values\ny= df['Outcome'].values  # Just picking up values from Outcome.","eb5a8039":"from sklearn.preprocessing import StandardScaler \nsc_X = StandardScaler() \nX = sc_X.fit_transform(X) ","839adf46":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state =0)","aa93dcd0":"from sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression, RidgeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier, GradientBoostingClassifier\nfrom sklearn.calibration import CalibratedClassifierCV","a5e5c25e":"models = []\nmodels.append((\"LR\",LogisticRegression()))\nmodels.append((\"GNB\",GaussianNB()))\nmodels.append((\"KNN\",KNeighborsClassifier()))\nmodels.append((\"DecisionTree\",DecisionTreeClassifier()))\nmodels.append((\"LDA\",  LinearDiscriminantAnalysis()))\nmodels.append((\"QDA\",  QuadraticDiscriminantAnalysis()))\nmodels.append((\"AdaBoost\", AdaBoostClassifier()))\nmodels.append((\"SVM Linear\",SVC(kernel=\"linear\")))\nmodels.append((\"SVM RBF\",SVC(kernel=\"rbf\")))\nmodels.append((\"Random Forest\",  RandomForestClassifier()))\nmodels.append((\"Bagging\",BaggingClassifier()))\nmodels.append((\"Calibrated\",CalibratedClassifierCV()))\nmodels.append((\"GradientBoosting\",GradientBoostingClassifier()))\nmodels.append((\"LinearSVC\",LinearSVC()))\nmodels.append((\"Ridge\",RidgeClassifier()))","c40bf2b8":"results = []\nfor name,model in models:\n    kfold = KFold(n_splits=10, random_state=0)\n    cv_result = cross_val_score(model,X_train,y_train, cv = kfold,scoring = \"accuracy\")\n# It gives you an unbiased estimate of the actual performance you will get at runtime\n    results.append(tuple([name,cv_result.mean(), cv_result.std()]))\n    results.sort(key=lambda x: x[1], reverse = True)    \nfor i in range(len(results)):\n    print('{:20s} {:2.2f} (+\/-) {:2.2f} '.format(results[i][0] , results[i][1] * 100, results[i][2] * 100))","43b7b1e7":"from sklearn.model_selection import GridSearchCV\nmodel = SVC()\nparamaters = [\n             {'C' : [0.01, 0.1, 1, 10, 100, 1000], 'kernel' : ['linear']}   \n             ]\ngrid_search = GridSearchCV(estimator = model, \n                           param_grid = paramaters,\n                           scoring = 'accuracy',\n                           cv = 10,\n                           n_jobs = -1)\ngrid_search = grid_search.fit(X_train, y_train)\nbest_accuracy = grid_search.best_score_ \nbest_parameters = grid_search.best_params_  \nprint('Best accuracy : ', grid_search.best_score_)\nprint('Best parameters :', grid_search.best_params_  )","cda108ba":"#Predicting output for test set. \nfinal_model = SVC(C = 0.1, kernel = 'linear')\nfinal_model.fit(X_train, y_train)\ny_pred = final_model.predict(X_test)\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncf = confusion_matrix(y_test, y_pred)\nprint(cf)\nprint(accuracy_score(y_test, y_pred) * 100) \nfrom sklearn.metrics import classification_report\nreport = classification_report(y_test, y_pred)\nprint(report)\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot= True)","ed3e2289":"from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, confusion_matrix, auc","9f1d7647":"false_positive_rate, true_positive_rate, threshold = roc_curve(y_test, y_pred)\nroc_auc = auc(false_positive_rate, true_positive_rate)\nplt.figure(figsize = (10,7))\nplt.title('Receiver Operating Characteristic')\nplt.plot(false_positive_rate, true_positive_rate, color = 'red', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1], linestyle = '--')\nplt.axis('tight')\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')","2ed1070b":"train_score = final_model.score(X_train,y_train)\ntest_score = final_model.score(X_test,y_test)\nprint(f'Training Accuracy of our model is: {train_score}')\nprint(f'Test Accuracy of our model is: {test_score}')","421d1ae5":"**Pairplot**","cd93f3cd":"**Relation between pregnancies and diebetes**","50c779b9":"**Correlation matrix**","0804fd95":"**Train Test split**","ab01b4b5":"**Additional details about the attributes:**-\n\n1)Pregnancies: Number of times pregnant\n\n2)Glucose: Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n\n3)BloodPressure: Diastolic blood pressure (mm Hg)\n\n4)SkinThickness: Triceps skin fold thickness (mm)\n\n5)Insulin: 2-Hour serum insulin (mu U\/ml)\n\n6)BMI: Body mass index (weight in kg\/(height in m)^2)\n\n7)DiabetesPedigreeFunction: Diabetes pedigree function\n\n8)Age: Age (years)\n\n9)Outcome: Class variable (0 or 1)","87f346cf":"**Histogram**","bcb630e3":"**importing dataset**","2e623106":"**Count plot**","17368f89":"**Violinplot**","f1e7d890":"**Importing Dataset**","55aa7403":"**Train Test Split**","590936d0":"**KDE Plot**","da64d744":"**Boxplot**","9d30a58c":"**Scatter matrix**"}}