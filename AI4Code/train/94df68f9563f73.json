{"cell_type":{"49252b59":"code","22304757":"code","4b06cd2f":"code","2d96cecd":"code","02b9d202":"code","44e15536":"code","87212c3d":"code","be670966":"code","fd5c1154":"code","f079b6fd":"code","2811dbe8":"code","1e6ce3a3":"code","1a796a18":"code","c3c2a998":"code","ce6686ba":"code","d99c3013":"code","729dab87":"code","abb6b360":"code","9c920e5c":"code","471a0b06":"code","ddeb8381":"code","4c272515":"code","7e4c52f1":"code","90fd38aa":"code","af9c50b6":"code","78e8c0ba":"code","8501e22b":"code","a7c59b6a":"code","b38deb3b":"code","1edbbc11":"code","9c6bdaca":"code","d88350cc":"code","ee0a5eab":"code","b2e8b68e":"code","0d52a9a2":"code","955767fc":"code","37c325be":"code","e244c793":"code","75257cc8":"code","cb3cca9e":"code","e70a73fc":"code","0a2e9d44":"code","2dcd6fa2":"code","47868ebb":"code","9da9c675":"code","1a055c2a":"code","9f7b58bb":"code","745b5109":"code","0c151e6e":"markdown","b5de6eb5":"markdown","16fec37f":"markdown","24ce2d34":"markdown","69ace4ef":"markdown","f43ad235":"markdown","0b728eec":"markdown","90034b54":"markdown","cb35cb0e":"markdown","720f07dc":"markdown","6732dfe3":"markdown","d5288fed":"markdown","466c42d6":"markdown","516e855c":"markdown","8463692c":"markdown","717afdd4":"markdown","7b7ccb46":"markdown","cf2aace9":"markdown","6e999e2f":"markdown","34eeb2f2":"markdown","b5870e57":"markdown","475d2271":"markdown","92e2dbd9":"markdown","54599d0b":"markdown","ae033d2d":"markdown","a2a99943":"markdown","0cd57d80":"markdown","52173f20":"markdown","641dbf17":"markdown","0c199501":"markdown","dadd7770":"markdown","eb1b763f":"markdown","e11a5dbe":"markdown","9faf770c":"markdown","493ec88a":"markdown","8d198b37":"markdown","abe35dcc":"markdown","03159d05":"markdown","f3f27c55":"markdown","83c00228":"markdown","d43da46c":"markdown","09f85e1e":"markdown","d7b1b221":"markdown","0bf0358f":"markdown","a31fc4d9":"markdown","f15a8726":"markdown","dfb7c67d":"markdown","580bb171":"markdown","63d285e4":"markdown","e7e8a68f":"markdown","34ad21f2":"markdown","f6a208fe":"markdown","950133a3":"markdown","54435093":"markdown","a2adfab1":"markdown","7632036a":"markdown","41623c97":"markdown","47db4164":"markdown","7272731c":"markdown","6a9eeed7":"markdown","fb280705":"markdown","b7eaa1cf":"markdown"},"source":{"49252b59":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline\ndataset_exp = pd.read_csv('..\/input\/train.csv')","22304757":"dataset_exp.head()","4b06cd2f":"dataset_exp.drop('PassengerId',axis = 1, inplace = True)","2d96cecd":"dataset_exp.info()","02b9d202":"#Embarked missing value : only 2 missing, use mode here, but in the later prerprocessing, more detail will be considered.\nEmbark_mode = dataset_exp['Embarked'].mode()[0]\ndataset_exp['Embarked'].fillna(value = Embark_mode, inplace = True)","44e15536":"# transfer categorical variables to numericial\ndataset_exp['Sex'][dataset_exp['Sex']=='male']=1\ndataset_exp['Sex'][dataset_exp['Sex']=='female']=0\ndataset_exp['Sex'] = dataset_exp['Sex'].astype(np.int64)\n\n# for age, use Random Forest to predict missing value first\nage_set = dataset_exp[['Age','Survived','Pclass','Sex','SibSp','Parch','Fare']]\nX_train = age_set[age_set['Age'].notnull()].iloc[:,1:].values\nX_test = age_set[age_set['Age'].isnull()].iloc[:,1:].values\nY_train =age_set[age_set['Age'].notnull()].iloc[:,0].values\n\nfrom sklearn.ensemble import RandomForestRegressor\nregressor = RandomForestRegressor(n_estimators= 1000,random_state = 0)\nregressor.fit(X_train,Y_train)\n\nY_pred = regressor.predict(X_test)\nY_pred = np.round(Y_pred,0)\ndataset_exp.loc[dataset_exp['Age'].isnull(),'Age'] = Y_pred","87212c3d":"# we can see lots of missing value here, but how relevant is it? Should we discard this variable?\ndataset_exp['Cabin'][dataset_exp['Cabin'].isnull()]='U'","be670966":"dataset_exp.info()","fd5c1154":"# Among all the people, the proportion of sex:\n#Total population Comparison\ntotal = dataset_exp['Sex']\ntotal = total.value_counts()\n\n# Among Survivors\nsex_sur = dataset_exp['Sex'][dataset_exp['Survived']==1]\nsex_sur[sex_sur==0] ='female'\nsex_sur[sex_sur==1] ='male'\nsex_sur = sex_sur.value_counts()\n\nfig,ax = plt.subplots(1,2,figsize = (8,5))\nlabel0 = ['male','female']\nlabel1 = ['female','male']\nfig.suptitle('Whole Population')\nax[0].pie(total,autopct='%1.1f%%',labels = label0,colors = ['blue','orange'],startangle = 90,shadow=True)\nax[0].set_title('All people')\nax[1].pie(sex_sur,autopct='%1.1f%%',labels = label1,colors = ['orange','blue'],startangle = -90,shadow=True)\nax[1].set_title('Survivors')\nplt.show()\n","f079b6fd":"pc = dataset_exp[['Pclass','Survived']].groupby('Pclass').mean()\npc.reset_index(inplace = True)\nbar_width = 0.15\nind = np.arange(0.2,1.19,1\/3)\nplt.figure(figsize = (6,6))\nplt.bar(ind,pc['Survived'],width = bar_width) \nplt.xticks(ind,label = pc['Pclass'])\nfor i in range(0,len(pc['Pclass'])):\n    plt.text(ind[i],pc['Survived'][i]+0.008,str(round(100*pc['Survived'][i],2))+'%',ha = 'center',size = 9,)\nplt.title('Survival Rate of different Pclass')\nplt.xlabel('Pclass')\nplt.ylabel('Survival Rate')\nplt.show()","2811dbe8":"facet = sns.FacetGrid(data = dataset_exp,hue = 'Survived',size = 6)\nfacet.map(sns.kdeplot,'Age',shade = True)\nfacet.set(xlim=(0,max(dataset_exp['Age'][dataset_exp['Age'].notnull()])))\nfacet.add_legend()\nplt.show()\n\n# average survival rate by age\ndataset_age = dataset_exp.copy()\ndataset_age['Age'] = dataset_age['Age'].apply(lambda x:round(x,2))\n\ndataset_age['Age'] = dataset_age['Age'].apply(str)\nage_ave = dataset_age[['Age','Survived']].groupby('Age').mean()\nage_ave.reset_index(inplace = True)\nage_ave['Age'] = age_ave['Age'].apply(float)\nage_ave.sort_values('Age',inplace = True)\nage_ave['Age'] = age_ave['Age'].apply(int)\ndataset_age['Age'] = dataset_age['Age'].apply(str)\n\nplt.figure(figsize = (20,6))\nsns.barplot('Age','Survived',data = age_ave,ci=1)\nplt.title('average survival rate by age')\nplt.show()","1e6ce3a3":"fig,ax = plt.subplots(1,2,figsize = (12,6))\nfig.suptitle('Survival distribution along Age')\n\nsns.violinplot('Pclass','Age',hue = 'Survived',data = dataset_exp, split = True,ax = ax[0])\nax[0].set_title('Age ~ Pclass')\nax[0].set_yticks(range(0,100,20))\n \nsns.violinplot('Sex','Age',hue = 'Survived',data = dataset_exp, split = True,ax = ax[1])\nax[1].set_title('Age ~ Sex')\nax[1].set_yticks(range(0,100,20))\nplt.show()\n\n#Then compare the shape with population Age distribution\nfig,ax = plt.subplots(1,2,figsize = (12,6))\nfig.suptitle('Population Age Distribution')\nsns.distplot(dataset_exp['Age'],bins = 30, hist = True, kde = True,ax = ax[0],color = 'blue')\n\nsns.boxplot(y='Age',data = dataset_exp,ax=ax[1], width = 0.4)\nplt.show()\n# Same distribution ","1a796a18":"dataset_exp['Child'] = 'None'\ndataset_exp['Age'] = dataset_exp['Age'].apply(np.float64)\ndataset_exp['Child'][dataset_exp['Age']>15]='adult'\ndataset_exp['Child'][dataset_exp['Age']<=15]='child'\nsns.set({'figure.figsize':(6,6)})\nsns.barplot('Child','Survived',data  = dataset_exp)\nplt.show()","c3c2a998":"# Sib\/sp and Par\/ch\nsibsp = dataset_exp[['SibSp','Survived']]\nsibsp['SibSp'] = sibsp['SibSp'].apply(str)\nsibsp_sur = sibsp.groupby('SibSp').mean()\nsibsp_sur.reset_index(inplace = True)\n\nparch= dataset_exp[['Parch','Survived']]\nparch['Parch'] = parch['Parch'].apply(str)\nparch_sur = parch.groupby('Parch').mean()\nparch_sur.reset_index(inplace = True)\n\n# create new variable: family size = sum of these two\ndataset_exp['FamilySize'] = dataset_exp['SibSp']+dataset_exp['Parch']+1\nfam= dataset_exp[['FamilySize','Survived']]\nfam['FamilySize'] = fam['FamilySize'].apply(str)\nfam_sur = fam.groupby('FamilySize').mean()\nfam_sur.reset_index(inplace = True)\nfam_sur['FamilySize'] = fam_sur['FamilySize'].apply(int)\nfam_sur.sort_values('FamilySize',inplace = True)\nfam_sur['FamilySize'] = fam_sur['FamilySize'].apply(str)\nfam_sur.reset_index(inplace = True)\nfam_sur.drop('index',axis=1,inplace = True)\n\n# plot\nfig,ax = plt.subplots(1,3,figsize = (9,5))\nfig.suptitle('Survival Rate ~ Family members')\nsns.barplot('SibSp','Survived',data = sibsp_sur,ax=ax[0],ci=0,color = 'blue')\nax[0].set_xlabel('Number of siblings\/spouse')\nax[0].set_ylabel('Survive Rate')\n\nsns.barplot('Parch','Survived',data = parch_sur,ax=ax[1],ci=0,color = 'blue')\nax[1].set_xlabel('Number of Parent\/children')\nax[1].set_ylabel(' ')\n\nsns.barplot('FamilySize','Survived',data = fam_sur,ax=ax[2],ci=0,color = 'blue',order =fam_sur['FamilySize'])\nax[2].set_xlabel('FamilySize')\nax[2].set_ylabel(' ')\nplt.show()\n","ce6686ba":"dataset_exp['FamilySize'][dataset_exp['FamilySize']>7]=0\ndataset_exp['FamilySize'][(dataset_exp['FamilySize']==1)|((dataset_exp['FamilySize']>=5)&(dataset_exp['FamilySize']<=7))]=1\ndataset_exp['FamilySize'][(dataset_exp['FamilySize']>=2)&(dataset_exp['FamilySize']<=4)]=2\ndataset_exp['FamilySize'] = dataset_exp['FamilySize'].apply(str)\nsns.barplot('FamilySize','Survived',data = dataset_exp)\nplt.show()","d99c3013":"# General Distribution\nfare = dataset_exp['Fare']\nsns.distplot(fare,bins=70,color = 'blue')\nplt.show()\n\n# Compare the average fare between differnet pclass\nfare_pc = dataset_exp[['Fare','Pclass']]\nplt.figure(figsize = (5,10))\nsns.boxplot('Pclass','Fare',data = fare_pc)\nplt.ylim([-50,300])\nplt.yticks(range(0,300,100))\nplt.show()","729dab87":"# compare the mean fare of survivors and victims\nfare_surv = dataset_exp[dataset_exp['Survived']==1]['Fare'].mean()\nfare_vict = dataset_exp[dataset_exp['Survived']==0]['Fare'].mean()\nind = [0.5,0.7]\nfare_y = [fare_surv,fare_vict]\nplt.bar(ind,fare_y,width = 0.1)\nplt.xticks(ind,['Survivors','Victims'])\nfor i in range(0,2):\n    plt.text(ind[i],fare_y[i]+1,round(fare_y[i],2),size =10,ha = 'center')\nplt.ylabel('Average Fare')\nplt.ylim([0,60])\nplt.title('Average fare of survivors and victims')\nplt.show()\n\nfacet_fare = sns.FacetGrid(data = dataset_exp,hue = 'Survived',aspect = 2)\nfacet_fare.map(sns.kdeplot,'Fare',shade = True)\nfacet_fare.set(xlim = (0,max(dataset_exp['Fare'])))\nplt.legend(labels=['die','survive'])\nplt.title('Survival~Fare')\nplt.show()","abb6b360":"dataset_exp['Fare'][dataset_exp['Fare']<50]=0\ndataset_exp['Fare'][(dataset_exp['Fare']>=50)&(dataset_exp['Fare']<74)]=1\ndataset_exp['Fare'][(dataset_exp['Fare']>=74)&(dataset_exp['Fare']<300)]=2\ndataset_exp['Fare'][dataset_exp['Fare']>=300]=3\n\nsns.set({'figure.figsize':(8,6)})\nsns.barplot('Fare','Survived',data = dataset_exp)\nplt.show()","9c920e5c":"#Embarked: surv\/vict for each port\nsns.countplot('Embarked',hue = 'Survived', data = dataset_exp)\nplt.title('Survivals for each port')\nplt.xlabel('Port Embarked')\nplt.show()\n\nsns.pointplot(x = 'Embarked',y = 'Survived',data = dataset_exp)\nplt.title('Survivals Rate of each port')\nplt.xlabel('Port Embarked')\nplt.ylabel('Survival Rate')\nplt.show()\n# port C has the highest survival rate\n","471a0b06":"dataset_exp['Cabin'].fillna('U',inplace = True)","ddeb8381":"dataset_exp['Has_Cabin'] = dataset_exp['Cabin'].apply(lambda x : 0 if x== 'U' else 1)\ndataset_exp['Has_Cabin'] = dataset_exp['Has_Cabin'].apply(str)\nsns.barplot(x ='Has_Cabin',y = 'Survived',data = dataset_exp)\nplt.show()","4c272515":"dataset_exp['Cabin_code'] = dataset_exp['Cabin'].str.get(0)\nsns.barplot(x ='Cabin_code',y='Survived',data = dataset_exp,estimator=np.mean)\nplt.show()","7e4c52f1":"pattern = re.compile('\\w+, (.+?)[.]')\ndataset_exp['Title'] = 'None'\nfor i in range(0,len(dataset_exp['Name'])):\n    try:    \n        dataset_exp['Title'][i] = re.search(pattern,list(dataset_exp['Name'].values)[i]).group(1) \n    except:\n        continue       \n","90fd38aa":"Title_dict = dict()\nTitle_dict.update(dict.fromkeys(['Capt', 'Col', 'Major', 'Dr', 'Rev'],'Officer'))\nTitle_dict.update(dict.fromkeys(['Don', 'Sir', 'the Countess', 'Dona', 'Lady'],'Royalty'))\nTitle_dict.update(dict.fromkeys(['Mme', 'Ms', 'Mrs'],'Mrs'))\nTitle_dict.update(dict.fromkeys(['Mlle', 'Miss'],'Miss'))\nTitle_dict.update(dict.fromkeys(['Mr'],'Mr'))\nTitle_dict.update(dict.fromkeys(['Master','Jonkheer'],'Master'))\ndataset_exp['Title'] = dataset_exp['Title'].map(Title_dict)","af9c50b6":"sns.barplot('Title','Survived',data = dataset_exp)\nplt.show()","78e8c0ba":"ticket_counts = dataset_exp['Ticket'].value_counts()\nticket_unique =list(dataset_exp['Ticket'].unique())\nticket_dict = dict()\nfor i in range(0,len(ticket_unique)):\n    ticket_dict.update(dict.fromkeys([ticket_unique[i]],ticket_counts.loc[ticket_unique[i],]))\ndataset_exp['Ticket'] = dataset_exp['Ticket'].map(ticket_dict).apply(str) \nsns.barplot(x ='Ticket',y='Survived',data = dataset_exp)\nplt.show()","8501e22b":"# divide Tickets into two classes\ndataset_exp['Ticket_nums'] = dataset_exp['Ticket'].apply(int)   \ndataset_exp['Ticket_nums'][(dataset_exp['Ticket_nums']==1)|(dataset_exp['Ticket_nums']>=5)]=0 #1,5,6,7\ndataset_exp['Ticket_nums'][(dataset_exp['Ticket_nums']>=2)&(dataset_exp['Ticket_nums']<=4)]=1 #2,3,4\ndataset_exp.info()","a7c59b6a":"# concatenate train and test and preprocess them together- to have uniform format\ntrain_set = pd.read_csv('..\/input\/train.csv')\ntrain_set_y =train_set.iloc[:,1] \ntrain_set = train_set.drop('Survived',axis=1)\ntest_set = pd.read_csv('..\/input\/test.csv')\ndataset = train_set.append(test_set)\n\n#reset index\ndataset.reset_index(inplace = True)\ndataset.drop('index',axis=1,inplace = True)\ndataset.drop('PassengerId',axis = 1, inplace = True)","b38deb3b":"#Embarked missing value : looking into observations with missing Embarked port\nemb_missing = dataset[dataset['Embarked'].isnull()]\n# these two observations share variables: Pclass and Ticket, so use these variables to speculate Embarked port:\n# Tickets starts with 113:\nemb_missing_pattern = '^113.+'\nemb_missing_ticket = dataset['Ticket'].apply(lambda x:bool(re.search(emb_missing_pattern,x)))\nticket_113 = dataset[emb_missing_ticket]  #all observations with tickets starting with 113\nticket_113['Ticket'] = ticket_113['Ticket'].apply(int)\nticket_113.sort_values('Ticket',inplace = True)\nticket_113 = ticket_113[['Ticket','Embarked']] \nticket_113['Embarked'].value_counts()\n# 52 S vs 10 C, and around 1135XX, most of them are S, so here fill missing values with S\ndataset['Embarked'].fillna('C',inplace = True)\n    \n# Name : extract the title from name using regex\npattern = re.compile('\\w+, (.+?)[.]')\ndataset['Title'] = 'None'\nfor i in range(0,len(dataset['Name'])):\n    try:    \n        dataset['Title'][i] = re.search(pattern,list(dataset['Name'].values)[i]).group(1) \n    except:\n        continue\n    \ndataset.drop('Name',axis=1,inplace = True)\n\n# Title: Unify to English format and implement dummy encoding\ntitle_dict = dict()\ntitle_dict.update(dict.fromkeys(['Capt', 'Col', 'Major', 'Dr', 'Rev'], 'Officer'))\ntitle_dict.update(dict.fromkeys(['Don', 'Sir', 'the Countess', 'Dona', 'Lady'], 'Royalty'))\ntitle_dict.update(dict.fromkeys(['Mme', 'Ms', 'Mrs'], 'Mrs'))\ntitle_dict.update(dict.fromkeys(['Mlle', 'Miss'], 'Miss'))\ntitle_dict.update(dict.fromkeys(['Mr'], 'Mr'))\ntitle_dict.update(dict.fromkeys(['Master','Jonkheer'], 'Master'))\ndataset['Title'] = dataset['Title'].map(title_dict)\n\ndataset['Title'][dataset['Title'].isin(('Mr','Officer'))] = 0\ndataset['Title'][dataset['Title'] =='Master'] = 1\ndataset['Title'][dataset['Title'].isin(('Mrs','Miss','Royalty'))]=2\ndataset['Title'] = dataset['Title'].apply(int)\n\n\n","1edbbc11":"# FamilySize = Sibsp + Parch + 1, further classified as 0(familysize>7),1(familysize=1 or 5,6,7) ,2(2<=familysize<=4)\ndataset['FamilySize'] = dataset['SibSp']+dataset['Parch']+1\ndataset['FamilySize'][dataset['FamilySize']>7]=0\ndataset['FamilySize'][(dataset['FamilySize']==1)|((dataset['FamilySize']>=5)&(dataset['FamilySize']<=7))]=1\ndataset['FamilySize'][(dataset['FamilySize']>=2)&(dataset['FamilySize']<=4)]=2\ndataset.drop(['SibSp','Parch'],axis=1,inplace = True)\n\n# Fare has 1 missing value - look into the dataset:\nmissing_fare_ob = dataset[dataset['Fare'].isnull()] \n#this person's ticket is 3701, then check the similar ticket number's fare pattern:\nticket_37_pattern = r'^3\\d\\d\\d$'\nticket_37 = dataset[dataset['Ticket'].apply(lambda x:bool(re.search(ticket_37_pattern,x)))]\n# all the fare are similar, it's reasonable to use mean to fill\nmean_fare = np.mean(ticket_37[ticket_37['Fare'].notnull()]['Fare'])\ndataset['Fare'].fillna(mean_fare,inplace = True)\n\n# Cabin\n#two groups: 0(Unknown),1(known)\ndataset['Cabin'][dataset['Cabin'].isnull()] ='U'\ndataset['Cabin']=dataset['Cabin'].str.get(0)\ndataset['Cabin'] = dataset['Cabin'].apply(lambda x: 0 if x=='U' else 2 if x in ('E','D','B') else 1)\n\n\n#Ticket: same pattern as family size, classify them into groups\nticket_counts = dataset['Ticket'].value_counts()\nticket_unique =list(dataset['Ticket'].unique())\nticket_dict = dict()\nfor k in range(0,len(ticket_unique)):\n    ticket_dict.update(dict.fromkeys([ticket_unique[k]],ticket_counts.loc[ticket_unique[k]]))\ndataset['Ticket'] = dataset['Ticket'].map(ticket_dict)\ndataset['Ticket_nums'] = dataset['Ticket']\n# divide Tickets into three classes \ndataset['Ticket_nums'][(dataset['Ticket_nums']==5)|(dataset['Ticket_nums']==6)|(dataset['Ticket_nums']>=8)]=0 # 5,6,more than 8 -- group 0\ndataset['Ticket_nums'][(dataset['Ticket_nums']==1)|(dataset['Ticket_nums']==7)]=1 #1,7 --group 1\ndataset['Ticket_nums'][(dataset['Ticket_nums']>=2)&(dataset['Ticket_nums']<=5)]=2 #2,3,4 --group 2\ndataset.drop('Ticket',axis=1,inplace = True)","9c6bdaca":"#use Sex,Title,Pclass to predict Age\nage_set = dataset[['Age','Pclass','Title','Sex']]\n\n# preprocessing: Pclass Sex and Title -> dummy variables\nage_set = pd.get_dummies(age_set)\n\nage_train_x = age_set[age_set['Age'].notnull()].iloc[:,1:]\nage_train_y = age_set[age_set['Age'].notnull()].iloc[:,0]\nage_test_x = age_set[age_set['Age'].isnull()].iloc[:,1:]\n\n #Random Forest\n #Grid Search to find optimal parameters\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV\n#use grid search to find optimal parameters\n'''\nregressor_age_rf = RandomForestRegressor()\nage_rf_params = [{'n_estimators':[100,500,1000,2000],'max_depth':[4,5,6,7],'min_samples_split':[6]}]\nage_rf_grid_search = GridSearchCV(estimator=regressor_age_rf,param_grid = age_rf_params,scoring = 'neg_mean_squared_error',cv=10,n_jobs=-1)\nage_rf_gs = age_rf_grid_search.fit(age_train_x,age_train_y)\nage_rf_gs.best_params_\nage_rf_gs.best_score_\n# best params:{'max_depth': 6, 'min_samples_split': 7, 'n_estimators': 100}\n# best score:-151.83\n'''\n#these parameters end up to be best\nregressor_age_rf = RandomForestRegressor(n_estimators = 100,max_depth = 6, min_samples_split = 5, random_state=0)\nregressor_age_rf.fit(age_train_x,age_train_y)\n'''\nfeatures = list(age_train_x.columns)\nimportance = list(regressor_age_rf.feature_importances_)\nfea_imp = {'feature':features,'importance':importance}\nimp = pd.DataFrame(data =fea_imp )\nimp.sort_values('importance',inplace = True,ascending = False)\nsns.barplot(x = 'feature',y='importance',data = imp)\nplt.show()\n'''\nage_pred_y = regressor_age_rf.predict(age_test_x)\ndataset['Age_RF']=dataset['Age']\ndataset['Age_RF'][dataset['Age'].isnull()] = age_pred_y\n \n\n#Gradient Boosting\n#Grid Search to find optimal parameters\n\nfrom sklearn.ensemble import GradientBoostingRegressor\n'''\nregressor_gb = GradientBoostingRegressor()\ngb_params = [{'n_estimators':[100,1000,2000],'max_depth':[3],'min_samples_split':[2,3,4]}]\ngb_grid_search = GridSearchCV(estimator=regressor_gb,param_grid = gb_params,scoring = 'neg_mean_squared_error',cv=10,n_jobs=-1)\ngb_gs = gb_grid_search.fit(age_train_x,age_train_y)\ngb_gs.best_params_\ngb_gs.best_score_'''\n# best params: {'max_depth': 3, 'min_samples_split': 3, 'n_estimators': 100}\n# best score: -141.40474456016958\nregressor_gb = GradientBoostingRegressor(n_estimators=100,max_depth=3, min_samples_split=3,random_state=0)\nregressor_gb.fit(age_train_x,age_train_y)\n'''\n#Feature Selection\nregressor_gb.feature_importances_\nfeatures = list(age_train_x.columns)\nimportance = list(regressor_gb.feature_importances_)\nfea_imp = {'feature':features,'importance':importance}\nimp = pd.DataFrame(data =fea_imp )\nimp.sort_values('importance',inplace = True,ascending = False)\nsns.barplot(x = 'feature',y='importance',data = imp)\nplt.show()\n'''\nage_pred_y_gb = regressor_gb.predict(age_test_x)\ndataset['Age_GB']=dataset['Age']\ndataset['Age_GB'][dataset['Age'].isnull()] = age_pred_y_gb\n\n# Merging two models ---mean\ndataset['Age'] =(dataset['Age_GB']+dataset['Age_RF'])\/2\ndataset.drop(['Age_GB','Age_RF'],axis=1,inplace = True)\n\n# divide people into child and adult(>15) groups\ndataset['Child']='None'\ndataset['Child'][dataset['Age']<=15]='Child'\ndataset['Child'][dataset['Age']>15]='Adult'\ndataset.drop('Age',axis=1,inplace = True)","d88350cc":"dataset.head()","ee0a5eab":"# dummy encoding\ndataset = pd.get_dummies(dataset)\ndataset.head()","b2e8b68e":"# test_train split\ntrain_set_x = dataset.iloc[:891,:]\ntest_set_x = dataset.iloc[891:,:]\n#train_set_y has been defined at the beginning\n\nX_train = train_set_x.values\ny_train = train_set_y.values\nX_test = test_set_x.values","0d52a9a2":"from sklearn.pipeline import Pipeline,make_pipeline\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom sklearn.feature_selection import f_classif\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold","955767fc":"'''\n# K-best and Grid Search for RF\npipe=Pipeline([('kb',SelectKBest(f_classif,k='all')), \n               ('rf', RandomForestClassifier(random_state = 10, max_features = 'sqrt'))])\n\nparam_test = {'kb__k':[9],\n              'kb__score_func':[chi2],\n              'rf__n_estimators':[554], \n              'rf__max_depth':[7],\n              'rf__min_samples_split':[2],\n              'rf__min_samples_leaf':[2],\n              'rf__criterion':['entropy']\n              }\n\n\ngs_rf = GridSearchCV(estimator = pipe, param_grid = param_test, scoring='roc_auc', cv=10,n_jobs=-1)\ngs_rf.fit(X_train,y_train)\nprint(gs_rf.best_params_, gs_rf.best_score_)\n'''","37c325be":"from sklearn.model_selection import learning_curve\nkb_rf = SelectKBest(chi2,k = 9)\nclf_rf = RandomForestClassifier(random_state = 10, \n                                  n_estimators = 554,\n                                  max_depth = 7, \n                                  criterion= 'entropy',\n                                  min_samples_leaf = 2,\n                                  min_samples_split = 2,\n                                  max_features = 'sqrt')\npipeline_rf = make_pipeline(kb_rf, clf_rf)\ntrain_sizes,train_scores,test_scores = learning_curve(pipeline_rf,X_train,y_train,cv=10,train_sizes = np.linspace(0.1,1.0,5),n_jobs=-1)\ntrain_scores_mean = np.mean(train_scores,axis=1)\ntest_scores_mean = np.mean(test_scores,axis=1)\nplt.plot(train_sizes,train_scores_mean,color = 'red',marker='.',markersize = 20,linewidth = 3,label='training curve')\nplt.plot(train_sizes,test_scores_mean,color = 'green',marker='.',markersize = 20,linewidth = 3,label = 'valication curve')\nplt.ylim([0.65,1])\nplt.legend()\nplt.title('Random Forest Learning Curve')\nplt.show()","e244c793":"# RF predict \nkb_rf = SelectKBest(chi2,k = 9)\nclf_rf = RandomForestClassifier(random_state = 10, \n                                  n_estimators = 554,\n                                  max_depth = 7, \n                                  criterion= 'entropy',\n                                  min_samples_leaf = 2,\n                                  min_samples_split = 2,\n                                  max_features = 'sqrt')\nrf_pipeline = make_pipeline(kb_rf, clf_rf)","75257cc8":"from sklearn.model_selection import learning_curve\nkb_et = SelectKBest(chi2,k = 10)\nclf_et = ExtraTreesClassifier(random_state = 10, \n                                  n_estimators = 941,\n                                  max_depth = 7, \n                                  criterion= 'entropy',\n                                  min_samples_leaf = 2,\n                                  min_samples_split = 2,\n                                  max_features = 'sqrt')\n               \npipeline_et = make_pipeline(kb_et, clf_et)\ntrain_sizes,train_scores,test_scores = learning_curve(pipeline_et,X_train,y_train,cv=10,train_sizes = np.linspace(0.1,1.0,5),n_jobs=-1)\ntrain_scores_mean = np.mean(train_scores,axis=1)\ntest_scores_mean = np.mean(test_scores,axis=1)\nplt.plot(train_sizes,train_scores_mean,color = 'red',marker='.',markersize = 20,linewidth = 3,label='training curve')\nplt.plot(train_sizes,test_scores_mean,color = 'green',marker='.',markersize = 20,linewidth = 3,label = 'valication curve')\nplt.ylim([0.65,1])\nplt.legend()\nplt.title('Extra Tree Learning Curve')\nplt.show()","cb3cca9e":"# ET predict \nkb_et = SelectKBest(chi2,k = 10)\nclf_et = ExtraTreesClassifier(random_state = 10, \n                                  n_estimators = 941,\n                                  max_depth = 7, \n                                  criterion= 'entropy',\n                                  min_samples_leaf = 2,\n                                  min_samples_split = 2,\n                                  max_features = 'sqrt')\net_pipeline = make_pipeline(kb_et, clf_et)","e70a73fc":"# Feature scaling:Fare,Age,FamilySize\ndataset_scaled = dataset.copy()\ns_fare = StandardScaler()\nfare_scaled = s_fare.fit_transform(dataset_scaled['Fare'].values.reshape(-1,1))\ndataset_scaled['Fare'] = fare_scaled\n\ns_pc = StandardScaler()\npc_scaled = s_pc.fit_transform(dataset_scaled['Pclass'].values.reshape(-1,1))\ndataset_scaled['Pclass'] = pc_scaled\n\ns_cb = StandardScaler()\ncb_scaled = s_cb.fit_transform(dataset_scaled['Cabin'].values.reshape(-1,1))\ndataset_scaled['Cabin'] = cb_scaled\n\ns_tt = StandardScaler()\ntt_scaled = s_tt.fit_transform(dataset_scaled['Title'].values.reshape(-1,1))\ndataset_scaled['Title'] = tt_scaled\n\ns_fs = StandardScaler()\nfs_scaled = s_fs.fit_transform(dataset_scaled['FamilySize'].values.reshape(-1,1))\ndataset_scaled['FamilySize'] = fs_scaled\n\n\ns_tn = StandardScaler()\ntn_scaled = s_tn.fit_transform(dataset_scaled['Ticket_nums'].values.reshape(-1,1))\ndataset_scaled['Ticket_nums'] = tn_scaled\n\n# scaled datasets\ntrain_set_x_s = dataset_scaled.iloc[:891,:]\ntest_set_x_s = dataset_scaled.iloc[891:,:]\n \n# train\/test set after scaling\nX_train_s = train_set_x_s.values\ny_train_s = train_set_y.values\nX_test_s = test_set_x_s.values","0a2e9d44":"from sklearn.model_selection import learning_curve\nkb_svm = SelectKBest(f_classif,k=12)\nclf_svm = SVC(C = 1.41, gamma = 0.1, kernel = 'rbf')\n \npipeline_svm = make_pipeline(kb_svm, clf_svm)\ntrain_sizes,train_scores,test_scores = learning_curve(pipeline_svm,X_train_s,y_train_s,cv=10,train_sizes = np.linspace(0.1,1.0,5),n_jobs=-1)\ntrain_scores_mean = np.mean(train_scores,axis=1)\ntest_scores_mean = np.mean(test_scores,axis=1)\nplt.plot(train_sizes,train_scores_mean,color = 'red',marker='.',markersize = 20,linewidth = 3,label='training curve')\nplt.plot(train_sizes,test_scores_mean,color = 'green',marker='.',markersize = 20,linewidth = 3,label = 'valication curve')\nplt.ylim([0.65,1])\nplt.legend()\nplt.title('SVM Learning Curve')\nplt.show()","2dcd6fa2":"# SVM predict \nkb_svm = SelectKBest(f_classif,k=12)\nclf_svm = SVC(C = 1.41, gamma = 0.1, kernel = 'rbf')\nsvm_pipeline = make_pipeline(kb_svm, clf_svm)","47868ebb":"#Stacking: \n #LEVEL 1:  bagging RF+ET+SVM\n  # k-fold on un-scaled training set\ndef get_out_of_kfold(model,fold,train_x,train_y,test_x):\n   \n    kfold_uns = KFold(n_splits = fold,shuffle = False)\n    train_pred_results =np.array([])\n    test_pred_results =np.zeros((X_test.shape[0],))\n    for kf_data in kfold_uns.split(train_x):\n        train_index = kf_data[0]\n        test_index = kf_data[1]\n        # training set(folds-1),test set(1)\n        kf_train_x = train_x[train_index]\n        kf_train_y = train_y[train_index]\n        kf_test = train_x[test_index]\n       \n        model.fit(kf_train_x,kf_train_y)\n        train_pred_results = np.hstack((train_pred_results,model.predict(kf_test)))\n        test_pred_results =test_pred_results + model.predict(test_x) \n    test_pred_results = np.round(test_pred_results\/fold) \n    result_list = list()\n    result_list.append(train_pred_results)\n    result_list.append(test_pred_results)\n    return result_list","9da9c675":"NFold = 9  # 9 fold\nrf_list = get_out_of_kfold(rf_pipeline,NFold,X_train,y_train,X_test)\net_list = get_out_of_kfold(et_pipeline,NFold,X_train,y_train,X_test)\n#for svm, use scaled dataset\nsvm_list = get_out_of_kfold(svm_pipeline,NFold,X_train_s,y_train_s,X_test_s)\n\n# Create train\/test set for level2\nbagging_dict_train = {'RF':rf_list[0],'ET':et_list[0],'SVM':svm_list[0]}\nX_train_lv1 =pd.DataFrame(bagging_dict_train)\ny_train_lv1 = y_train\n\nbagging_dict_test = {'RF':rf_list[1],'ET':et_list[1],'SVM':svm_list[1]}\nX_test_lv2 = pd.DataFrame(bagging_dict_test)\n","1a055c2a":"# meta classifier \n# K-Nearest Neighbors\n# Learning curve on selected model\n\nfrom sklearn.model_selection import learning_curve\nkb_knn = SelectKBest(f_classif,k='all' )\nclf_knn = KNeighborsClassifier(algorithm= 'auto', n_neighbors=28, weights='uniform')\n \npipeline_knn = make_pipeline(kb_knn, clf_knn)\ntrain_sizes,train_scores,test_scores = learning_curve(pipeline_knn,X_train_lv1,y_train_lv1,cv=10,train_sizes = np.linspace(0.1,1.0,5),n_jobs=-1)\ntrain_scores_mean = np.mean(train_scores,axis=1)\ntest_scores_mean = np.mean(test_scores,axis=1)\nplt.plot(train_sizes,train_scores_mean,color = 'red',marker='.',markersize = 20,linewidth = 3,label='training curve')\nplt.plot(train_sizes,test_scores_mean,color = 'green',marker='.',markersize = 20,linewidth = 3,label = 'valication curve')\nplt.ylim([0.7,1])\nplt.legend()\nplt.title('K-NN Learning Curve')\nplt.show()\n","9f7b58bb":"# K-NN final prediction\nkb_knn = SelectKBest(f_classif,k='all' )\nclf_knn = KNeighborsClassifier(algorithm= 'auto', n_neighbors=28, weights='uniform')\npipeline_knn = make_pipeline(kb_knn, clf_knn)\npipeline_knn.fit(X_train_lv1,y_train_lv1)\nknn_pred = pipeline_knn.predict(X_test_lv2)","745b5109":"test_set_x.reset_index(inplace = True)\ntest_set_x.drop('index',axis = 1, inplace = True)\ntest_set_x['KNN_pred']=knn_pred\n# final result output\nfinal_result = pd.DataFrame()\nfinal_result['PassengerId'] = test_set['PassengerId']\nfinal_result['Survived'] = test_set_x['KNN_pred']\n#final_result.to_csv('Thoffy-submission-knn.csv')\n","0c151e6e":"### The final output is generated here.","b5de6eb5":"### Obviously, people with 2-4 family size are more likely to survive, while the survival rate of other classes are low.","16fec37f":"### We can see from the bar plot that family size 2~4,1 or 5~7, more than 7,  have quite a big gap in survival rate. Therefore, divide familysize into three classes: 0(>7), 1(1,5,6,7), 2(2,3,4). Lets plot again\uff1a","24ce2d34":"![Titanic](https:\/\/cdn.history101.com\/wp-content\/uploads\/2018\/06\/titanic___Super_Portrait.jpg)","69ace4ef":"## PART 1. Exploratory Data Analysis","f43ad235":"### Meanwhile, let's look at **Age**. ","0b728eec":"> ## PART 0. Import Packages and Data\n* data wrangling: pandas, numpy, re\n* data visualization: matplotlib, seaborn","90034b54":"> ## PART 3. Models and Prediction","cb35cb0e":"### Using this set of parameter, draw the leaning curve ","720f07dc":"### From the survival rate distribution, it's obvious that people under age 15(child) have higher rate of surviving. Therefore, divide into **Age** into adult(>15) and child(<=15) groups and see the gap between two groups.\n","6732dfe3":"### Classification: import all the packages","d5288fed":"### Second-level model selection.","466c42d6":"### For the next model SVM, it's  based on Euclidean distance, so numerical features should be scaled.","516e855c":"### First level models selection","8463692c":"### FamlilySize, Fare, Cabin, Ticket","717afdd4":"### The prediction of missing Age is a trivial step, but now all I need is an simplified approximation for EDA, so Random Forest Regressor is used to predict missing Age based on variables: **Survived, Pclass, Sex, SibSp, Parch, Fare**.","7b7ccb46":"### The last one is Cabin, which only has 204 values. It seems like worthless. However, titanic dataset is pretty small, so each feature has its cut and ought to be explored rather than discarded. For all the null values , I fill them with 'U'.","cf2aace9":"### The simplest way to start the analysis of this variable is to explore the difference between passenger with and without a cabin.","6e999e2f":"### Exactly. This is an effective way of feature engineering for the variable **Fare**.","34eeb2f2":"### After the exploratory analysis on all the variables, I've had a better understanding of all the variables and how to implement feature engineering on them. The next part is to clean the data based on the conclusions from EDA.","b5870e57":"### There are 10 variables in this dataset which might be relevant to the dependent variable** Survived**. Moreover, Three variables contain missing values(**Age**,**Cabin** and **Embarked**), so I have to handle this problem first. Let's begin with **Embarked**. Since during EDA period, what's more important is not the accuracy of prediction but the general relationship between variables, the imputation methods of some variables are simplified and may not be the final choice. ","475d2271":"## Dataset overview","92e2dbd9":"### Follow the conclusions from the previous part, the data is to be cleaned here. The reason I clean the data again here rather than use the data above are: \n### 1. Some imputation methods in the previous section was just for general relevance analysis, here these parts will be handled with more complex ways.\n### 2. In order to unify the format, the train set(excluding Survived) and test set are concatenated preprocessed together. ","54599d0b":"### This part is the final part. In order to enhance model performance, two-level stacking is used. Here I will try different models for both levels and select them based on performance and learning curve.","ae033d2d":"## Exploratory Data Analysis\n### For Exploratory Data Analysis, I need to firstly choose useful features from the dataset. Obviously, **PassengerId** acts just as an index and should be dropped.","a2a99943":"### That's it. The final score of my submission is 0.804, which means I still have lots of problems in my feature engineering and model construction. Anyway, this is my first kernel and I will continue to try different ideas!","0cd57d80":"### The next features I'm going to dig into are number of **sibling\/spouse** and **parent\/children**. Since these are related to family accompany, it might be interesting to create a new feature **FamilySize** by adding them together. ","52173f20":"### The last one is **Ticket**. I notice that there are some duplicated ticket numbers. This might mean family ticket. Thus, this variable can be grouped by number of tickets.","641dbf17":"### Just as I expected, different title has different survival rate, Mr has the lowest likelihood to survive while Mrs, Miss and Royalty have much higher survival rate. Lady and royalty have the priority, that makes sense. These categories will be grouped again in data preprocessing section.","0c199501":"### The first model is Random Forest(RF) Classifier. For this and all the models to follow, K-best Selection and Grid Search will be used to find 'best' parameters in terms of score. Then, learning curve will be drawn using this specific set of parameters to display more detail of the model. Because the running Grid Search Code is too time-consuming, here I quote them as annotation.","dadd7770":"### Some other classifiers such as K-NN and Logistic Regression has also been tried here, but the performance is bad so they are not considered. Finally, Random Forest, Extra Tree and SVM are chosen to be the ones on the first level.","eb1b763f":"### From both the score and learning curve, these two models are not bad. Therefore, predictions of these two models are recorded respectively.","e11a5dbe":"### Yes, different cabin_code has different survival rate, so Cabin can classified  into three groups: 0(Unknown),1(CGAF),2(EDB).","9faf770c":"### In the second level, many kinds of models are used including XGboost, K-NN, Logistic Regression, etc. After a lengthy process(omitted here) of model selection and evaluation, K-Nearest Neighbors is finally selected as second-level classifier.","493ec88a":"### It's of great interest to combine some features together.","8d198b37":"### The next variable is tricky. **Cabin** only has 204 values. We can just discard it, but this one may contain some valuable information, but it's hard to figure out how to fill in these missing values at first glance, so just fill in 'U' first.","abe35dcc":"### Firstly, I'm interested in figuring out if there is any discrepancy on survival rate between male and female.","03159d05":"###  While the majority of the population were male, more female survived! Therefore, **Sex** is an important feature.","f3f27c55":"### For missing values in Age, predict them with **Title**, **Pclass** and **Sex**. The model I use here is a merge of Random Forest Regressor and Gradient Boost Regressor.","83c00228":"### Extra Tree Model","d43da46c":"### To tell the truth, I'm not very familiar with the social hierarchy in Great Britian. So I searched online and chose a reasonable way to classify the titles.","09f85e1e":"### The dataset for exploratory analysis is now very tidy.","d7b1b221":"### There are  three distinct port in the variable **Embarked**. To assess this feature, compare them on survival count and rate. From the two plots below, C is the port with highest passenger survival rate.","0bf0358f":"### Another troublesome variable is **Name** b**ecause it's a long string which cannot be quantified. Here, connecting the problem with its history backgroud, I would try to compare people with different titles. To create **Title**, regular expression is used.","a31fc4d9":"### This model is good as well, predict the survival outcome with it:","f15a8726":"**[PART 0 Import Packages and Data](#PART-0.-Import-Packages-and-Data)**\n\n**[PART 1 Exploratory Data Analysis](#PART-1.-Exploratory-Data-Analysis)**\n\n**[PART 2 Data Preprocessing](#PART-2.-Data-Preprocessing)**\n\n**[PART 3 Models and Prediction](#PART-3.-Models-and-Prediction)**\n\n","dfb7c67d":"### For** Embarked**, only 2 missing values, here I use mode to fill the null values.","580bb171":"### Use 9-fold CV, throw each model into the function and output the data for second-level classifier.","63d285e4":"# Titanic survival prediction with stacked model\n**My first Titanic kernel**\n\n**[](#Part-0:-Imports,-Functions)** \n","e7e8a68f":"### The next variable is **Fare**. Firstly I'd like to  see its distribution among all passengers, and the difference among different **pclass**.","34ad21f2":"### People with cabin have higher survival rate, thus this variable is highly relevant. Do people in different Cabin code have different survival rate? To see what's going on, the first character of cabin is extracted out as a new feature **Cabin_code**.","f6a208fe":"### SVM","950133a3":"> ## PART 2. Data Preprocessing","54435093":"### Same pattern as family size! The same ticket number are very likely to be bought by the family. In a similar way, divide this variable into two classes: 2~4 and others.","a2adfab1":"### Finally, create dummy variables for all the categorical variables.","7632036a":"### As is shown is figure 1 above, most passengers' fare is below 50. Figure  2 shows that interms of **Fare**,  **pclass**1>2>3, so pclass 1 is the most high-end without doubt. Does this mean rich people who paid more on fare had some kinds of priority when the disaster happened? Let's see the average fare of survivors and victims.","41623c97":"### Looks like that's true! Therefore, based on the distribution, Fare is divided into 4 groups: 0(lowest),1,2,3(highest). Let's plot again to see if there is a significant gap between groups.","47db4164":"### Then, evaluate Pclass variable by comparing the survival rate among different pclasses. As expected, pclass 1 passengers have the highest survival rate, pclass the lowest. Thus pclass is highly relevant.","7272731c":"### Embarked, Title ","6a9eeed7":"### When it comes to the second level in model stacking, the three first-level models are handled in the same way to provide data for second-level classifier. Therefore, a function is defined here for the transformation.","fb280705":"### Now dataset contains no missing values.","b7eaa1cf":"### Import and concatenate train set and test set."}}