{"cell_type":{"e84643f9":"code","c9fb1b1d":"code","b72c5bcc":"code","e4509a10":"code","24e66d66":"code","4640f8f8":"code","86ad4bb7":"code","b9f92125":"code","f33530d8":"code","14f25247":"code","e30a49b5":"code","bd26f984":"code","e88104fb":"code","e2bc926a":"code","9017d0d5":"code","8b40f8ac":"code","e3fcbe35":"code","3c368e24":"code","e72c1b08":"code","1c3cf5dd":"code","29a6a2b6":"code","60730768":"code","c72cebf6":"code","b9312bb4":"code","8b252f37":"code","e178ff96":"code","88953010":"code","2bbd08c0":"code","574933f3":"code","c831caab":"code","46ccd36e":"code","d3f0f1a7":"code","d47b10af":"code","69caf3eb":"code","f475a819":"code","18ad7b95":"code","0a85aba2":"code","4d90417a":"code","5e77dfbc":"code","bc90c000":"code","962a2f30":"code","2f0c9c68":"code","de6071b8":"code","5e9b3ffb":"code","538d0c1a":"code","e97eb36c":"code","ef0393bd":"code","ac47a7c5":"code","72d5c5c6":"code","59f3c333":"code","95cfcf4a":"code","c10bbf40":"code","b8bd36a2":"code","7f950484":"code","4f86e9ca":"code","9c371847":"code","d3658912":"code","e9ee69d5":"code","b59f8091":"code","f01136f4":"code","40bc98f9":"code","9129852b":"code","05015733":"code","0f1fa050":"code","c03d1769":"code","e6686abe":"code","f4cc15cc":"code","37601bdf":"code","2ecad939":"code","3ad2c5ce":"code","d94fd98f":"code","70657825":"code","4512015c":"code","b08a66b4":"code","5db2a1de":"code","c6de65bf":"code","06f02759":"code","e0e731e8":"code","3a4d33f8":"code","b80196bd":"code","606635bb":"code","ebc9e84c":"code","44c6fd72":"code","db85dca0":"code","103cf7ca":"code","7fbaa0c9":"code","b87f6753":"code","b78735db":"code","f8491ae0":"code","751d3aeb":"code","f06b4101":"code","fe62195c":"code","01eb0a64":"code","d2281ecb":"code","52e3394b":"code","5a2e439a":"code","630985fb":"code","8ce0827c":"code","56519c2a":"code","bac5f5bc":"code","c728a1c8":"markdown","40bc6419":"markdown","8b6bed80":"markdown","161fab46":"markdown","f18a3a1b":"markdown","644e41da":"markdown","0b8802fa":"markdown","f576a060":"markdown","c3ffce3f":"markdown","f5886f95":"markdown","64044f58":"markdown","d3472ca2":"markdown","c9025979":"markdown","7eb8a370":"markdown","747358bf":"markdown","39296e11":"markdown","268e1d4f":"markdown","c8b61811":"markdown","1c5152cc":"markdown","bd706682":"markdown","c59b1f23":"markdown","83ba1e09":"markdown","0fb7311d":"markdown","476617b0":"markdown","a5840602":"markdown","a88ee4dd":"markdown","d2462586":"markdown","8119a8f4":"markdown","21052197":"markdown","200f8458":"markdown","bc6b9d9c":"markdown","34d08b23":"markdown","cbd4dc16":"markdown","62f91dfd":"markdown","3520d8d7":"markdown","c98b1b1d":"markdown","9e67541e":"markdown","9abf2d6a":"markdown","e9458f7a":"markdown","75f4c671":"markdown","26db9d7e":"markdown","543f99e8":"markdown","3bea5944":"markdown","5992448f":"markdown","84a9c7a0":"markdown","0202bc9e":"markdown","6748c3e0":"markdown","b07335e9":"markdown","f0c3ae11":"markdown","76cc1563":"markdown","978917a7":"markdown","fe5c3017":"markdown","c2117532":"markdown","16ac9285":"markdown","86ec81eb":"markdown","85611165":"markdown","d30a69a6":"markdown","721b6597":"markdown","86336dc0":"markdown","54803211":"markdown","1d0c1a7b":"markdown","287f9118":"markdown","a07b81d9":"markdown","ad52e02c":"markdown","f6e6a3cb":"markdown","4ec6f522":"markdown","3ff67f5b":"markdown","da519afd":"markdown","872db8b8":"markdown","00fe123b":"markdown","06b3bf8a":"markdown","6e87fa8a":"markdown","a5ac59ff":"markdown","b8b3d353":"markdown","9c4d2953":"markdown","80a464e5":"markdown","b19e0e43":"markdown","d460b3b0":"markdown","d68e064f":"markdown","79e7b664":"markdown","059d8d76":"markdown","dbe287d4":"markdown"},"source":{"e84643f9":"from platform import python_version\nprint(python_version())","c9fb1b1d":"import numpy as np \nimport pandas as pd \n\nimport matplotlib.pyplot as plt \nimport seaborn as sns \n\nfrom math import sqrt\nfrom sklearn.metrics import mean_squared_error\nfrom matplotlib.pylab import rcParams\n\n\n# TIME SERIES\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom pandas.plotting import autocorrelation_plot\nfrom statsmodels.tsa.stattools import adfuller, acf, pacf\nfrom statsmodels.graphics.tsaplots import plot_acf,plot_pacf\nfrom pandas import Timestamp\nfrom datetime import datetime\n\n#MODELLING\nimport xgboost as xgb\nfrom xgboost import XGBRegressor\nfrom xgboost import plot_importance\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom fbprophet import Prophet\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.preprocessing import LabelEncoder\n# settings\nimport os\nimport gc\nimport warnings\nwarnings.filterwarnings(\"ignore\")","b72c5bcc":"folder='\/kaggle\/input\/competitive-data-science-predict-future-sales\/'\n\ndf_cats=pd.read_csv(folder+'item_categories.csv')\ndf_items=pd.read_csv(folder+'items.csv')\ndf_sales=pd.read_csv(folder+'sales_train.csv')\ndf_shops=pd.read_csv(folder+'shops.csv')\ndf_test=pd.read_csv(folder+'test.csv')\n\n#pickles\nfolder_pickles='...\/competitive-data-science-predict-future-sales\/pickles\/'\n\n#Submission\ndf_sub=pd.read_csv(folder+'sample_submission.csv')\nsub_folder='...\/competitive-data-science-predict-future-sales\/submissions\/'","e4509a10":"def overview(df):\n    print('SHAPE:\\n',df.shape)\n    print('COLUMN NAMES:\\n', df.columns.tolist())\n    print('UNIQUE VALUES PER COLUMN:\\n', df.nunique())\n    print('COLUMNS WITH MISSING DATA:\\n',df.isnull().sum())\n    print('SAMPLE:\\n',df.sample(10))\n    print('INFO:\\n',df.info())","24e66d66":"df_sales.head()","4640f8f8":"overview(df_sales)","86ad4bb7":"sorted(list(df_sales[\"item_cnt_day\"].unique()))[:20]","b9f92125":"sorted(list(df_sales[\"item_price\"].unique()))[:20]","f33530d8":"df_sales[df_sales[\"item_price\"]<0]","14f25247":"mean = df_sales[(df_sales[\"shop_id\"] == 32) & (df_sales[\"item_id\"] == 2973) & (df_sales[\"date_block_num\"] == 4) & (df_sales[\"item_price\"] > 0)][\"item_price\"].mean()\ndf_sales.loc[df_sales.item_price < 0, 'item_price'] = mean","e30a49b5":"# =============================================================================\n# CLEANING DF_SALES\n# =============================================================================\n\n# \u042f\u043a\u0443\u0442\u0441\u043a \u041e\u0440\u0434\u0436\u043e\u043d\u0438\u043a\u0438\u0434\u0437\u0435, 56\ndf_sales.loc[df_sales.shop_id == 0, 'shop_id'] = 57\ndf_test.loc[df_test.shop_id == 0, 'shop_id'] = 57\n# \u042f\u043a\u0443\u0442\u0441\u043a \u0422\u0426 \"\u0426\u0435\u043d\u0442\u0440\u0430\u043b\u044c\u043d\u044b\u0439\"\ndf_sales.loc[df_sales.shop_id == 1, 'shop_id'] = 58\ndf_test.loc[df_test.shop_id == 1, 'shop_id'] = 58\n# \u0416\u0443\u043a\u043e\u0432\u0441\u043a\u0438\u0439 \u0443\u043b. \u0427\u043a\u0430\u043b\u043e\u0432\u0430 39\u043c\u00b2\ndf_sales.loc[df_sales.shop_id == 10, 'shop_id'] = 11\ndf_test.loc[df_test.shop_id == 10, 'shop_id'] = 11\n\n#create attribute revenue\ndf_sales['revenue']=df_sales['item_price']*df_sales['item_cnt_day']\n","bd26f984":"plt.figure(figsize = (10,4))\nsns.boxplot(x = df_sales[\"item_cnt_day\"])","e88104fb":"df_sales[df_sales['item_cnt_day']>800]","e2bc926a":"plt.figure(figsize = (10,4))\nsns.boxplot(x = df_sales[\"item_price\"])","9017d0d5":"df_sales[df_sales['item_price']>100000]","8b40f8ac":"df_sales = df_sales[(df_sales.item_price < 300000 )& (df_sales.item_cnt_day < 1000)]","e3fcbe35":"df_items.head()","3c368e24":"overview(df_items)","e72c1b08":"pd.options.display.max_rows =100\n\ngb_item_cat=df_items.groupby('item_category_id').agg({'item_id':'count'})\ngb_item_cat.sort_values('item_id',ascending=False,inplace=True)\ngb_item_cat","1c3cf5dd":"df_cats.head()","29a6a2b6":"overview(df_cats)","60730768":"df_cats","c72cebf6":"# =============================================================================\n# CLEANING DF_CATS\n# =============================================================================\n\ndf_cats['cat_type'] = df_cats['item_category_name'].str.split('-').map(lambda x: x[0])\ndf_cats['cat_subtype'] = df_cats['item_category_name'].str.split('-').map(lambda x: x[1] if len(x)>1 else x[0])\ndf_cats['cat_type_id']=LabelEncoder().fit_transform(df_cats['cat_type'])\ndf_cats['cat_subtype_id']=LabelEncoder().fit_transform(df_cats['cat_subtype'])\n\ndf_cats=df_cats[['item_category_id','cat_type_id','cat_subtype_id']]","b9312bb4":"df_shops.head()","8b252f37":"overview(df_shops)","e178ff96":"df_shops.sort_values('shop_name',ascending=False)","88953010":"# =============================================================================\n# CLEANING DF_SHOP\n# =============================================================================\n\ndf_shops['city'] = df_shops['shop_name'].str.split(' ').map(lambda x: x[0])\ndf_shops.loc[df_shops.city=='!\u042f\u043a\u0443\u0442\u0441\u043a','city']='\u042f\u043a\u0443\u0442\u0441\u043a'\ndf_shops['city_id']=LabelEncoder().fit_transform(df_shops['city'])\n\n\ndf_shops['cat_tienda'] = df_shops['shop_name'].str.split(' ').map(lambda x: x[1])\ncategory = []\nfor cat in df_shops.cat_tienda.unique():\n    if len(df_shops[df_shops.cat_tienda == cat]) >= 5:\n        category.append(cat)\ndf_shops.cat_tienda = df_shops.cat_tienda.apply( lambda x: x if (x in category) else \"other\" )\n\ndf_shops['shop_cat']=LabelEncoder().fit_transform(df_shops['cat_tienda'])\n\ndf_shops=df_shops[['city','shop_id','city_id','shop_cat']]","2bbd08c0":"df_test.head()","574933f3":"overview(df_test)","c831caab":"df_sub.head()","46ccd36e":"overview(df_sub)","d3f0f1a7":"df_sub.drop('item_cnt_month',axis=1,inplace=True)  ","d47b10af":"df_sales[\"date\"] = pd.to_datetime(df_sales[\"date\"], format = \"%d.%m.%Y\")\ndf_sales.set_index('date')","69caf3eb":"df_sales.info()","f475a819":"full_df=pd.merge(df_sales,df_shops,on=['shop_id'],how='left')\nfull_df=pd.merge(full_df,df_items,on=['item_id'],how='left')\nfull_df=pd.merge(full_df,df_cats,on=['item_category_id'],how='left')","18ad7b95":"ts_M = full_df[[\"date\", \"item_cnt_day\"]].set_index(\"date\").resample(\"M\").sum()\n\nplt.figure(figsize = (10, 6))\nplt.plot(ts_M, color = \"blue\", label = \"Monthly sales\",marker='.')\nplt.title(\"Monthly sales\")\nplt.legend();","0a85aba2":"ts_W = full_df[[\"date\", \"item_cnt_day\"]].set_index(\"date\").resample(\"W\").sum()\n\nplt.figure(figsize = (10, 6))\nplt.plot(ts_W, color = \"blue\", label = \"Weekly sales\")\nplt.title(\"Weekly sales\")\nplt.legend();","4d90417a":"fig,ax = plt.subplots(nrows=5, ncols=2,sharex=True, sharey=True,figsize=(15,15))\nfig.suptitle('Sales per Shop', fontsize=30)\nts_shop = full_df.groupby(['shop_id','date_block_num'])['item_cnt_day'].sum().reset_index()\nshop_count=0\nfor i in range(5):\n    for j in range(2):\n        for z in range(6):\n            ax[i,j].plot(ts_shop[ts_shop['shop_id']==shop_count]['date_block_num'],ts_shop[ts_shop['shop_id']==shop_count]['item_cnt_day'],alpha=.5,label='shop '+str(shop_count))\n            shop_count += 1\n            ax[i,j].legend(loc='best')\n\nfor ax in ax.flat:\n    ax.set(xlabel='months', ylabel='Sales')\n    ax.label_outer()","5e77dfbc":"items_x_cat=full_df.groupby('item_category_id').agg({'item_cnt_day':'sum'})\nitems_x_cat.reset_index(inplace=True)","bc90c000":"items_x_cat=items_x_cat.sort_values('item_cnt_day',ascending=False)\nitems_x_cat_top=items_x_cat[0:15] \nitems_x_cat_top['item_category_id']=items_x_cat_top['item_category_id'].astype(object)\nitems_x_cat_top=items_x_cat_top.reset_index(drop=True)","962a2f30":"barplot=sns.barplot(y='item_cnt_day',x='item_category_id',palette='GnBu_d',data=items_x_cat_top,order=items_x_cat_top.sort_values('item_cnt_day',ascending=False).item_category_id)\nbarplot.set(xlabel=\"item category\", ylabel = \"Sales\")","2f0c9c68":"fig,ax = plt.subplots(nrows=7, ncols=2,sharex=True, sharey=True,figsize=(20,20))\nfig.suptitle('Sales per product category', fontsize=30)\nts_cat = full_df.groupby(['item_category_id','date_block_num'])['item_cnt_day'].sum().reset_index()\ncat_count=0\nfor i in range(7):\n    for j in range(2):\n        for z in range(6):\n            ax[i,j].plot(ts_cat[ts_cat['item_category_id']==cat_count]['date_block_num'],ts_cat[ts_cat['item_category_id']==cat_count]['item_cnt_day'],alpha=.5,label='cat '+str(cat_count))\n            cat_count += 1\n            ax[i,j].legend(loc='best')\n\nfor ax in ax.flat:\n    ax.set(xlabel='months', ylabel='Sales')\n    ax.label_outer()","de6071b8":"fig, (ax1, ax2) = plt.subplots(1, 2,figsize = (20,6), dpi = 80)\nplot_acf(ts_M, ax = ax1, lags = 20)\nplot_pacf(ts_M, ax = ax2, lags = 20);","5e9b3ffb":"res=seasonal_decompose(ts_M.values,freq=12,model='additive')\nfig=res.plot()","538d0c1a":"def test_stationarity(timeseries):\n    print('Results of Dickey-Fuller Test:')\n    dftest = adfuller(timeseries, autolag='AIC')\n    dfoutput = pd.Series(dftest[0:4], index=['ADF Statistic:','p-value','#Lags Used','Number of Observations Used'])\n    for key,value in dftest[4].items():\n        dfoutput['Critical Value (%s)'%key] = value\n    print (dfoutput)","e97eb36c":"test_stationarity(ts_M)","ef0393bd":"def normalize(ts):\n    avg,dev=ts.mean(), ts.std()\n    ts=(ts-avg)\/dev\n    return ts\n\ndef remove_seasonality(ts):\n    ts= ts-ts.shift(12)\n    ts=ts.dropna()\n    return ts\n\ndef remove_trend(ts):\n    ts= ts.diff(1).dropna()\n    return ts","ac47a7c5":"# Normalize TS\nts_norm=normalize(ts_M)\nts_norm.plot()\nplt.xlabel('meses')\nplt.ylabel('Total de ventas')","72d5c5c6":"# Remove seasonality\nts_est=remove_seasonality(ts_M)\nts_est.plot()\ntest_stationarity(ts_est)","59f3c333":"#Remove trend\nts_trend=remove_trend(ts_M)\nts_trend.plot()\nplt.xlabel('meses')\nplt.ylabel('Total de ventas')\ntest_stationarity(ts_trend)","95cfcf4a":"def create_pred_list(X,Y):\n    lista_shops=X['shop_id'].tolist()\n    preds=[]\n    for shop in range(0,len(Y)):\n        preds.append(np.array([lista_shops[shop],Y[shop]]))\n    return preds\n\ndef submission_df(df_forc_items):\n    df_final=pd.merge(df_sub,df_forc_items,how='left',on=['ID'])\n    df_final.fillna(0,inplace=True)\n    return df_final\n\ndef evaluation(df_pred):\n    df_eval=pd.merge(df_valid,df_pred, on=['ID'],how='inner')\n    error = sqrt(mean_squared_error(df_eval['obs'].values, df_eval['pred'].values))\n    return error","c10bbf40":"def middleout_forecasting(predictions,months_weights): \n    month=33-months_weights\n    df_forecast=pd.DataFrame(predictions,columns=['shop_id','forecast_shop'])\n    \n    #we calculate the number of sales per item in each store of the las \"month\" months\n    sales_gb_item=df_sales[df_sales['date_block_num']>month].groupby(['shop_id','item_id'])[\"item_cnt_day\"].sum()\n    sales_gb_item=pd.DataFrame(sales_gb_item)\n    sales_gb_item.reset_index(inplace=True)\n    sales_gb_item.rename(columns={'item_cnt_day':'item_sales'},inplace=True)\n    \n    #we calculate the number of sales store of the las \"month\" months\n    sales_gb_shop=df_sales[df_sales['date_block_num']>month].groupby(['shop_id'])[\"item_cnt_day\"].sum()\n    sales_gb_shop=pd.DataFrame(sales_gb_shop)\n    sales_gb_shop.reset_index(inplace=True)\n    sales_gb_shop.rename(columns={'item_cnt_day':'shop_sales'},inplace=True)\n    \n    #we calculate the proportion of the sales for each item in every shop in the last \"month\" months\n    sales_gb_full=pd.merge(sales_gb_item,sales_gb_shop,how='left',on=['shop_id'])\n    sales_gb_full=pd.merge(sales_gb_full,df_test,how='left',on=['shop_id','item_id'])\n    sales_gb_full['weights']=sales_gb_full['item_sales']\/sales_gb_full['shop_sales']\n    sales_gb_full.drop(['item_sales','shop_sales'],axis=1,inplace=True)\n    \n    #we calculate the forecast for each item in every store\n    df_calc=pd.merge(sales_gb_full,df_forecast,how='left',on=['shop_id'])\n    df_calc['item_cnt_month']=(df_calc['weights']*df_calc['forecast_shop']).clip(0,20) #clip the result to submit\n    df_calc.drop(['shop_id','item_id','weights','forecast_shop'],axis=1,inplace=True)\n    \n    return df_calc","b8bd36a2":"#df validation, month 33\ndf_valid=full_df[full_df['date_block_num']==33].groupby(['shop_id','item_id'])[\"item_cnt_day\"].sum().clip(0,20).to_frame()\ndf_valid.reset_index(inplace=True)\n\ndf_valid=pd.merge(df_valid,df_test, on=['shop_id','item_id'],how='left').sort_values('ID')\ndf_valid.drop(['item_id','shop_id'],axis=1,inplace=True)\ndf_valid.rename(columns={'item_cnt_day':'obs'},inplace=True)\ndf_valid=df_valid[['ID','obs']]","7f950484":"ts_arima=remove_seasonality(ts_M)","4f86e9ca":"model = ARIMA(ts_arima, order=(3,1,0))\nmodel_fit = model.fit(disp=0)\nprint(model_fit.summary())\n\n# plot residual errors\nresiduals = pd.DataFrame(model_fit.resid)\nresiduals.plot()\nresiduals.plot(kind='kde')\nprint(residuals.describe())","9c371847":"# NIVEL DE TIENDA\nts_shop=df_sales.groupby([\"date_block_num\",'shop_id'])[\"item_cnt_day\"].sum()\nts_shop=ts_shop.unstack(level=1)\nts_shop=ts_shop.fillna(0)","d3658912":"closed_shops=(ts_shop[33:]==0).all()\nclosed_shops=np.array(closed_shops.index[closed_shops==True])","e9ee69d5":"def train_evaluation_arima(ts,i):\n    predictions = list()\n    \n    X=ts[i].values\n    train, test = X[0:33], X[33:]\n    history = [x for x in train]\n    print('-----------------------shop %f--------------------------' % i)\n    if (i in closed_shops or sum(history)==0):\n        for t in range(len(test)):\n            \tpredictions.append(0)\n    else:\n        for t in range(len(test)):\n            model = ARIMA(history, order=(2,1,0))\n            model_fit = model.fit(disp=0)\n            output = model_fit.predict(1, len(history)+1, typ='levels')\n            yhat = output[-1]\n            predictions.append(yhat)\n            obs = test[t]\n            history.append(obs)\n            \n            print('predicted=%f, expected=%f' % (yhat, obs))\n            error = sqrt(mean_squared_error(test, predictions))\n            print('Test MSE: %.3f' % error)\n            \n            plt.plot(history,label='obs')\n            plt.plot(output, label='pred',ls='--')\n            plt.xticks(range(0,len(history)+1))\n            plt.legend()\n            plt.title('shop '+str(i))\n            plt.show()\n        \n    predictions_series=pd.Series(predictions)\n    return np.append(np.array(i),predictions_series.values.transpose())","b59f8091":"lista_pred_arima=[]\nfor i in ts_shop.columns.tolist():\n    lista_pred_arima.append(train_evaluation_arima(ts_shop,i))","f01136f4":"df_pred_arima=middleout_forecasting(lista_pred_arima,3)\ndf_pred_arima=submission_df(df_pred_arima)\ndf_pred_arima.rename(columns={'item_cnt_month':'pred'},inplace=True)","40bc98f9":"error_arima = evaluation(df_pred_arima)\nprint('Test MSE: %.3f' % error_arima)","9129852b":"def forecast_arima_shops(ts,i):\n    predictions = list()\n    train=ts[i].values\n    \n    if (i in closed_shops or sum(train)==0):\n        predictions.append(0)\n    else:\n        model = ARIMA(train, order=(2,1,0))\n        model_fit = model.fit(disp=0)\n        output = model_fit.predict(1, len(train)+1, typ='levels')\n        yhat = output[-1]\n        predictions.append(yhat)\n\n        plt.plot(train,label='obs')\n        plt.plot(output, label='pred',ls='--')\n        plt.xticks(range(0,len(train)+1))\n        plt.legend()\n        plt.title('shop '+str(i))\n        plt.show()\n        \n    predictions_series=pd.Series(predictions)\n    return np.append(np.array(i),predictions_series.values.transpose())","05015733":"lista_pred_arima_test=[]\nfor i in ts_shop.columns.tolist():\n    lista_pred_arima_test.append(forecast_arima_shops(ts_shop,i))","0f1fa050":"df_forecast_items=middleout_forecasting(lista_pred_arima_test,3)","c03d1769":"df_final=submission_df(df_forecast_items)\ndf_final.to_csv(sub_folder+'submission_arima.csv', index=False)","e6686abe":"ts_prophet=full_df.groupby([\"date_block_num\"])[\"item_cnt_day\"].sum()\n\nts_prophet.index=pd.date_range(start = '2013-01-01',end='2015-10-01', freq = 'MS')\nts_prophet=ts_prophet.reset_index()\nts_prophet.columns=['ds','y']","f4cc15cc":"model = Prophet(yearly_seasonality=True)\nmodel.fit(ts_prophet)\n# predict for five months in the furure and MS - month start is the frequency\nfuture = model.make_future_dataframe(periods = 4, freq = 'MS',include_history=True)  \n# now lets make the forecasts\nforecast = model.predict(future)\n#Let's plot the forecast\nmodel.plot(forecast)\n#decompose the forecast\nmodel.plot_components(forecast)","37601bdf":"ts_prophet_shop=full_df.groupby([\"date\",'shop_id'])[\"item_cnt_day\"].sum()\nts_prophet_shop=ts_prophet_shop.unstack(level=1)\nts_prophet_shop=ts_prophet_shop.fillna(0)\nts_prophet_shop = ts_prophet_shop.resample(\"M\").sum()","2ecad939":"def train_evaluation_Prophet(ts,i):\n    predictions = list()\n    ts=pd.DataFrame(ts[i]).reset_index()\n    ts.columns=['ds','y']\n    \n    train = ts.loc[ts['ds'] != datetime(2015, 10, 31)]\n    test = ts.loc[ts['ds'] == datetime(2015, 10, 31)]\n    print('-----------------------shop %f--------------------------' % i)\n    \n    if (i in closed_shops or sum(train['y'].values)==0):\n        predictions.append(0)\n    else:\n        model = Prophet(yearly_seasonality=True) \n        model.fit(train)\n        future = model.make_future_dataframe(periods = 1, freq = 'MS',include_history=True)  \n        forecast = model.predict(future) \n        output = forecast['yhat']\n        yhat = output.values[-1]\n        predictions.append(yhat)\n\n        print('predicted=%f, expected=%f' % (yhat, test['y'].values))\n        error = sqrt(mean_squared_error(test['y'], predictions))\n        print('Test MSE: %.3f' % error)\n\n#         model.plot(forecast)\n        plt.plot(ts['y'],label='obs')\n        plt.plot(output, label='pred',ls='--')\n        plt.xticks(range(0,len(train)+1))\n        plt.legend()\n        plt.title('shop '+str(i))\n        plt.show()\n        \n    predictions_series=pd.Series(predictions)\n    return np.append(np.array(i),predictions_series.values.transpose())","3ad2c5ce":"lista_pred_prophet=[]\nfor i in ts_prophet_shop.columns.tolist():\n    lista_pred_prophet.append(train_evaluation_Prophet(ts_prophet_shop,i))","d94fd98f":"df_pred_prophet=middleout_forecasting(lista_pred_prophet,3)\ndf_pred_prophet=submission_df(df_pred_prophet)\ndf_pred_prophet.rename(columns={'item_cnt_month':'pred'},inplace=True)","70657825":"error_prophet = evaluation(df_pred_prophet)\nprint('Test MSE: %.3f' % error_prophet)","4512015c":"def forecast_Prophet(ts,i):\n    predictions = list()\n    train=pd.DataFrame(ts[i]).reset_index()\n    train.columns=['ds','y']\n    \n    if (i in closed_shops or sum(train['y'])==0):\n        predictions.append(0)\n    else:\n        model = Prophet(yearly_seasonality=True) \n        model.fit(train)\n        future = model.make_future_dataframe(periods = 1, freq = 'MS',include_history=True)  \n        output = model.predict(future)['yhat']\n        yhat = output.values[-1]\n        predictions.append(yhat)\n\n        plt.plot(train['y'],label='obs')\n        plt.plot(output, label='pred',ls='--')\n        plt.xticks(range(0,len(train)+1))\n        plt.legend()\n        plt.title('shop '+str(i))\n        plt.show()\n        \n    predictions_series=pd.Series(predictions)\n    return np.append(np.array(i),predictions_series.values.transpose())","b08a66b4":"lista_pred_test_prophet=[]\nfor i in ts_prophet_shop.columns.tolist():\n    lista_pred_test_prophet.append(forecast_Prophet(ts_prophet_shop,i))","5db2a1de":"df_forecast_prophet=middleout_forecasting(lista_pred_test_prophet,3)","c6de65bf":"df_forecast_prophet=submission_df(df_forecast_prophet)\ndf_forecast_prophet.to_csv(sub_folder+'submission_prophet.csv', index=False)","06f02759":"min_date = full_df[\"date\"].min()\nmax_date_sales = full_df[\"date\"].max()\nmax_date_test = datetime(2015, 11, 30)\n\ndate_range = pd.date_range(min_date, max_date_test, freq = \"M\")\n\nshops_cartesian=sorted(df_test['shop_id'].unique().tolist())\n\ncartesian_product=pd.MultiIndex.from_product([date_range,shops_cartesian],names=['date','shop_id'])","e0e731e8":"#We prepare the follwing df to fill the matrix\ndf_dates_max = full_df.groupby(['date_block_num']).agg({'date':'max'})\ndf_dates_max.reset_index(inplace=True)\n\ngb_sales = full_df.groupby(['date_block_num',\"shop_id\"]).agg({'item_cnt_day':np.sum,'revenue':np.sum})\ngb_sales.reset_index(inplace=True)\ngb_sales = gb_sales.rename(columns={'item_cnt_day': 'item_cnt_month'})\n\ngb_sales=pd.merge(gb_sales, df_dates_max,on='date_block_num',how='left')","3a4d33f8":"matrix_test=df_test.copy()\nmatrix_test[\"date_block_num\"] = 34\nmatrix_test[\"date_block_num\"] = matrix_test[\"date_block_num\"].astype(np.int8)\nmatrix_test[\"shop_id\"] = matrix_test.shop_id.astype(np.int8)\nmatrix_test.drop(['ID','item_id'],axis=1,inplace=True)","b80196bd":"# We fill the matrix\nmatrix = pd.DataFrame(index = cartesian_product).reset_index()\n\nmatrix=pd.merge(matrix,gb_sales,on=['date','shop_id'],how='left')\nmatrix=pd.merge(matrix,matrix_test,on=['date_block_num','shop_id'],how='left')\nmatrix.loc[matrix['date'] == datetime(2015, 11, 30), 'date_block_num'] = 34\nmatrix.fillna(0, inplace = True)","606635bb":"# Let's get the shop features we created earlier\nmatrix=pd.merge(matrix,df_shops,on=['shop_id'],how='left')","ebc9e84c":"# matrix.to_pickle(folder_pickles+\"matrix.pkl\")\n# matrix = pd.read_pickle(folder_pickles+\"matrix.pkl\")","44c6fd72":"def lag_feature( df,lags, cols):\n    for col in cols:\n        print(col)\n        tmp = df[[\"date_block_num\", \"shop_id\",col ]]\n        for i in lags:\n            tmp_aux = tmp.copy()\n            tmp_aux.columns = [\"date_block_num\", \"shop_id\", col + \"_lag_\"+str(i)]\n            tmp_aux.date_block_num += i\n            df = pd.merge(df, tmp_aux, on=['date_block_num','shop_id'], how='left')\n    return df","db85dca0":"# LAG FEATURES\n\n#feature 1\nmatrix_in=matrix\nmatrix_feat1 = lag_feature(matrix_in, [1,2,3], [\"item_cnt_month\"])\n\n#feature 2\ndf_group=matrix.groupby(['date_block_num','shop_id','city_id']).agg({'item_cnt_month':['sum','mean']})\ndf_group.columns = [\"date_shop_city_sum\",'date_shop_city_mean']\ndf_group.reset_index(inplace=True)\nmatrix_in=pd.merge(matrix,df_group,on=['date_block_num','shop_id','city_id'],how='left')\n\nmatrix_feat2 = lag_feature(matrix_in, [1,2,3], [\"date_shop_city_sum\",'date_shop_city_mean'])\n\n\n# feature 3\ndf_group=matrix.groupby(['date_block_num','shop_cat']).agg({'item_cnt_month':['sum','mean']})\ndf_group.columns = [\"date_shop_cat_sum\",'date_shop_cat_mean']\ndf_group.reset_index(inplace=True)\nmatrix_in=pd.merge(matrix,df_group,on=['date_block_num','shop_cat'],how='left')\n\nmatrix_feat3 = lag_feature(matrix_in, [1,2,3], [\"date_shop_cat_sum\",'date_shop_cat_mean'])","103cf7ca":"def merge_features( df,cols):\n    cols_join=['date_block_num','shop_id']\n    cols_feats=df.iloc[:,-cols:].columns.tolist()\n    df_aux=df[cols_join+cols_feats]\n    df_feats=pd.merge(matrix_feats, df_aux, on=cols_join, how='left')\n    return df_feats\n\nmatrix_feats=matrix.copy()\nmatrix_feats=merge_features(matrix_feat1, 3)\nmatrix_feats=merge_features(matrix_feat2, 8)\nmatrix_feats=merge_features(matrix_feat3, 8)","7fbaa0c9":"matrix_feats[\"year\"] = matrix_feats[\"date\"].dt.year\nmatrix_feats[\"month\"] = matrix_feats[\"date\"].dt.month\nmatrix_feats[\"days_in_month\"] = matrix_feats[\"date\"].dt.days_in_month\nmatrix_feats[\"quarter_start\"] = matrix_feats[\"date\"].dt.is_quarter_start\nmatrix_feats[\"quarter_end\"] = matrix_feats[\"date\"].dt.is_quarter_end","b87f6753":"holidays_next_month = {\n    12:8,\n    1:1,\n    2:1,\n    3:0,\n    4:2,\n    5:1,\n    6:0,\n    7:0,\n    8:0,\n    9:0,\n    10:1,\n    11:0\n}\n\nholidays_this_month = {\n    1:8,\n    2:1,\n    3:1,\n    4:0,\n    5:2,\n    6:1,\n    7:0,\n    8:0,\n    9:0,\n    10:0,\n    11:1,\n    12:0\n}\n\nmatrix_feats[\"holidays_next_month\"] = matrix_feats[\"month\"].map(holidays_next_month)\nmatrix_feats[\"holidays_this_month\"] = matrix_feats[\"month\"].map(holidays_this_month)","b78735db":"city_population = {\n'\u042f\u043a\u0443\u0442\u0441\u043a':307911, \n'\u0410\u0434\u044b\u0433\u0435\u044f':141970,\n'\u0411\u0430\u043b\u0430\u0448\u0438\u0445\u0430':450771, \n'\u0412\u043e\u043b\u0436\u0441\u043a\u0438\u0439':326055, \n'\u0412\u043e\u043b\u043e\u0433\u0434\u0430':313012, \n'\u0412\u043e\u0440\u043e\u043d\u0435\u0436':1047549,\n'\u0412\u044b\u0435\u0437\u0434\u043d\u0430\u044f':1228680, \n'\u0416\u0443\u043a\u043e\u0432\u0441\u043a\u0438\u0439':107560, \n'\u0418\u043d\u0442\u0435\u0440\u043d\u0435\u0442-\u043c\u0430\u0433\u0430\u0437\u0438\u043d':1228680, \n'\u041a\u0430\u0437\u0430\u043d\u044c':1257391, \n'\u041a\u0430\u043b\u0443\u0433\u0430':341892,\n'\u041a\u043e\u043b\u043e\u043c\u043d\u0430':140129,\n'\u041a\u0440\u0430\u0441\u043d\u043e\u044f\u0440\u0441\u043a':1083865, \n'\u041a\u0443\u0440\u0441\u043a':452976, \n'\u041c\u043e\u0441\u043a\u0432\u0430':12678079,\n'\u041c\u044b\u0442\u0438\u0449\u0438':205397, \n'\u041d.\u041d\u043e\u0432\u0433\u043e\u0440\u043e\u0434':1252236,\n'\u041d\u043e\u0432\u043e\u0441\u0438\u0431\u0438\u0440\u0441\u043a':1602915 , \n'\u041e\u043c\u0441\u043a':1178391, \n'\u0420\u043e\u0441\u0442\u043e\u0432\u041d\u0430\u0414\u043e\u043d\u0443':1125299, \n'\u0421\u041f\u0431':5398064, \n'\u0421\u0430\u043c\u0430\u0440\u0430':1156659,\n'\u0421\u0435\u0440\u0433\u0438\u0435\u0432\u041f\u043e\u0441\u0430\u0434':104579, \n'\u0421\u0443\u0440\u0433\u0443\u0442':373940, \n'\u0422\u043e\u043c\u0441\u043a':572740, \n'\u0422\u044e\u043c\u0435\u043d\u044c':744554, \n'\u0423\u0444\u0430':1115560, \n'\u0425\u0438\u043c\u043a\u0438':244668,\n'\u0426\u0438\u0444\u0440\u043e\u0432\u043e\u0439':1228680, \n'\u0427\u0435\u0445\u043e\u0432':70548, \n'\u042f\u0440\u043e\u0441\u043b\u0430\u0432\u043b\u044c':608353\n}\n\ncity_income = {\n'\u042f\u043a\u0443\u0442\u0441\u043a':70969, \n'\u0410\u0434\u044b\u0433\u0435\u044f':28842,\n'\u0411\u0430\u043b\u0430\u0448\u0438\u0445\u0430':54122, \n'\u0412\u043e\u043b\u0436\u0441\u043a\u0438\u0439':31666, \n'\u0412\u043e\u043b\u043e\u0433\u0434\u0430':38201, \n'\u0412\u043e\u0440\u043e\u043d\u0435\u0436':32504,\n'\u0412\u044b\u0435\u0437\u0434\u043d\u0430\u044f':46158, \n'\u0416\u0443\u043a\u043e\u0432\u0441\u043a\u0438\u0439':54122, \n'\u0418\u043d\u0442\u0435\u0440\u043d\u0435\u0442-\u043c\u0430\u0433\u0430\u0437\u0438\u043d':46158, \n'\u041a\u0430\u0437\u0430\u043d\u044c':36139, \n'\u041a\u0430\u043b\u0443\u0433\u0430':39776,\n'\u041a\u043e\u043b\u043e\u043c\u043d\u0430':54122,\n'\u041a\u0440\u0430\u0441\u043d\u043e\u044f\u0440\u0441\u043a':48831, \n'\u041a\u0443\u0440\u0441\u043a':31391, \n'\u041c\u043e\u0441\u043a\u0432\u0430':91368,\n'\u041c\u044b\u0442\u0438\u0449\u0438':54122, \n'\u041d.\u041d\u043e\u0432\u0433\u043e\u0440\u043e\u0434':31210,\n'\u041d\u043e\u0432\u043e\u0441\u0438\u0431\u0438\u0440\u0441\u043a':37014 , \n'\u041e\u043c\u0441\u043a':34294, \n'\u0420\u043e\u0441\u0442\u043e\u0432\u041d\u0430\u0414\u043e\u043d\u0443':32067, \n'\u0421\u041f\u0431':61536, \n'\u0421\u0430\u043c\u0430\u0440\u0430':35218,\n'\u0421\u0435\u0440\u0433\u0438\u0435\u0432\u041f\u043e\u0441\u0430\u0434':54122, \n'\u0421\u0443\u0440\u0433\u0443\u0442':73780, \n'\u0422\u043e\u043c\u0441\u043a':43235, \n'\u0422\u044e\u043c\u0435\u043d\u044c':72227, \n'\u0423\u0444\u0430':35257, \n'\u0425\u0438\u043c\u043a\u0438':54122,\n'\u0426\u0438\u0444\u0440\u043e\u0432\u043e\u0439':46158, \n'\u0427\u0435\u0445\u043e\u0432':54122, \n'\u042f\u0440\u043e\u0441\u043b\u0430\u0432\u043b\u044c':34675\n}\n\nmatrix_feats[\"city_population\"] = matrix_feats[\"city\"].map(city_population)\nmatrix_feats[\"city_income\"] = matrix_feats[\"city\"].map(city_income)","f8491ae0":"#Pickle the matrix with all the features ready\n# matrix_feats.to_pickle(folder_pickles+\"matrix_feats.pkl\")\n# matrix_feats = pd.read_pickle(folder_pickles+\"matrix_feats.pkl\")","751d3aeb":"cols_to_drop = [  \n'revenue', \n\"city\",\n\"date_shop_city_sum\",\n\"date_shop_city_mean\", \n\"date_shop_cat_sum\",\n\"date_shop_cat_mean\",   \n]","f06b4101":"# In[1]: XGBOOST REGRESSOR\nmatrix_feats = matrix_feats[matrix_feats[\"date_block_num\"] > 3]\nmatrix_feats.drop(cols_to_drop, inplace = True, axis = 1)","fe62195c":"data = matrix_feats.copy()\ndata.drop('date',axis=1,inplace=True)","01eb0a64":"X_train = data[data.date_block_num < 33].drop(['item_cnt_month'], axis=1)\nY_train = data[data.date_block_num < 33]['item_cnt_month']\nX_valid = data[data.date_block_num == 33].drop(['item_cnt_month'], axis=1)\nY_valid = data[data.date_block_num == 33]['item_cnt_month']\nX_test  = data[data.date_block_num == 34].drop(['item_cnt_month'], axis=1)","d2281ecb":"xgb_ft=xgb.XGBRegressor()\n\nparameters = {'nthread':[4],\n              'objective':['reg:linear'],\n              'learning_rate': [0.01,0.05,0.07], \n              'max_depth': [2,5,10],\n              'min_child_weight': [0.5,1.0,1.5],\n              'silent': [1],\n              'subsample': [0.2,0.7,0.9],\n              'colsample_bytree': [0.7],\n              'n_estimators': [100,1500,3000]}\n\nmodel = GridSearchCV(xgb_ft,\n                        parameters,\n                        cv = 2,\n                        n_jobs = 5,\n                        verbose=True)\n\nmodel.fit(\n    X_train, \n    Y_train, \n    eval_metric=\"rmse\", \n    eval_set=[(X_train, Y_train), (X_valid, Y_valid)], \n    verbose=True, \n    early_stopping_rounds = 10)\n","52e3394b":"estimators=pd.DataFrame({'importance':model.best_estimator_.feature_importances_.tolist(),\n                         'features':X_train.columns.tolist()})","5a2e439a":"fig, ax = plt.subplots(1,1,figsize=(10,14))\nax = sns.barplot(x=\"importance\", y=\"features\", data=estimators).set_title('Feature importance')","630985fb":"Y_pred = model.predict(X_valid)","8ce0827c":"lista_valid_xgb=create_pred_list(X_valid,Y_pred)\ndf_pred_xgb=middleout_forecasting(lista_valid_xgb,3)\ndf_pred_xgb=submission_df(df_pred_xgb)\ndf_pred_xgb_sub=df_pred_xgb.copy()\ndf_pred_xgb.rename(columns={'item_cnt_month':'pred'},inplace=True)","56519c2a":"error_xgb = evaluation(df_pred_xgb)\nprint('Test MSE: %.3f' % error_xgb)","bac5f5bc":"#df_pred_xgb_sub.to_csv(sub_folder+'submission_xgb.csv', index=False)","c728a1c8":"<a id = \"contents\"><\/a>\n# Table of Contents\n\n\n[Intro](#intro)\n\n[Quick look at the data](#quick_look)\n\n-->[sales_train](#quick_look_sales_train)\n\n-->[items](#quick_look_items)\n\n-->[item_categories](#quick_look_item_categories)\n\n-->[shops](#quick_look_shops)\n\n-->[test](#quick_look_test)\n\n-->[sample_submission](#quick_look_sample_submission)\n\n[Dealing with the time variable](#time_variable)\n\n[EDA](#EDA)\n\n-->[Total sales per month](#sales_per_month)\n\n-->[Total sales per week](#sales_per_week)\n\n-->[Total sales per month and shop](#sales_per_month_shop)\n\n-->[Total sales per month and category](#sales_per_month_category)\n\n[Time Series Analysis](#ts_analysis)\n\n-->[Autocorrelation and parcial correlation (ACF and PACF)](#autocorrelation)\n\n-->[Decomposing the TS](#decomposing)\n\n-->[Stationarity Test | Dickey-Fuller](#stationarity)\n\n-->[Remove Non-Stationarity effects](#remove_stationarity)\n\n\n[Breaf housekeeping](#housekeeping)\n\n-->[Userful functions](#useful)\n\n-->[Validation dataframe](#validationdf)\n\n[Possible approaches](#approaches)\n\n-->[Hierarchical Time Series](#hierarchical)\n\n\n[Approach 1: ARIMA](#approach1)\n\n-->[ARIMA - Quick look](#arima_quicklook)\n\n-->[ARIMA - Modelling](#arima_modelling)\n\n-->[ARIMA - Forecasting](#arima_prediction)\n\n\n[Approach 2: Prophet](#approach2)\n\n-->[Prophet - Data prepocessing](#prophet_prepro)\n\n-->[Prophet - Quick look](#prophet_quicklook)\n\n-->[Prophet - Modelling](#prophet_model)\n\n-->[Prophet - Forecasting](#prophet_prediction)\n\n\n[Approach 3: XGBOOST](#approach3)\n\n-->[Convert to supervised learning](#xgboost)\n\n-->[XGBOOST - Data prepocessing](#xgboost_prepro)\n\n-->[XGBOOST - Modelling](#xgboost_model)\n\n-->[XGBOOST - Forecasting](#xgboost_prediction)\n\n\n[Results and conclusions](#conclusions)","40bc6419":"<a id = \"arima_modelling\"><\/a>\n### ARIMA - Modelling\n[Back to Table of Contents](#contents)","8b6bed80":"<a id = \"intro\"><\/a>\n# Introduction\n[Back to Table of Contents](#contents)","161fab46":"Submission file","f18a3a1b":"After exploring these approaches the submissions on kaggle gave the following results:\n    \n      ARIMA: 1.07697\n    PROPHET: 1.12637\n        XGB: 1.09260\n    ","644e41da":"Now we are going to create the lag features using the following function:","0b8802fa":"<a id = \"quick_look_item_categories\"><\/a>\n### item_categories\n[Back to Table of Contents](#contents)","f576a060":"ARIMA stands for <b>AutoRegressive Integrated Moving Average<\/b> . There is 3 components in this model and each one corresponds to a parameter in the ARIMA model implementation in python. ARIMA(p,d,q):\n\n<b>-AR:<\/b> Autoregression. Takes into account the strength of the relationship between an observation and its previous observation at different lags. It corresponds to the <b>p<\/b> parameter or the lag order, it is the number of lags included in the model.\n\n<b>-I:<\/b> Integration. Substracts to the observation the observation at previous timestamps. Useful to make the series stationary.It corresponds to the <b>d<\/b> parameter or the degree of differencing, it is the number of times we want to substract from the observations.\n\n<b>-MA:<\/b> Moving Average. Takes into account the strenght of the relationship between an observation and its average in previous timestamps. It corresponds to the <b>q<\/b> parameter or the window size, it is the number lags we want to take into account to calculate the average.\n\nWe can try to make some of the parameters 0 to make AR models (d=q=0) or MA models (p=d=0) or ARMA models (d=0)...","c3ffce3f":"Let's import the data:","f5886f95":"Forecasting problems can be converted into supervised learning problems.This will allow us to use machine learning methods to make predictions.\n\nBefore we can do such thing, the time series must be transformed and we need to create features that will give a machine learning model the information it needs about the dependency of the target variable with the previous observations. Therefore, the features that we are going to create are \"lag features\", that contain information about the variation of a certain metric over different time steps.","64044f58":"<a id = \"approaches\"><\/a>\n# Possible approaches\n[Back to Table of Contents](#contents)","d3472ca2":"Let's add more time features that might be interesting:","c9025979":"Before we train let's get rid of the variables that are not gonna be informed in the test set (month=34). The variables that we need for the training have to be informed in the test set. So they have to be the ones that don't vary over time (we assume city population and icome as constant) and the lag features we created, because they have the information about the changing over time","7eb8a370":"In order to dissagreate the shop forecasts using the <b>TOP-DOWN approach<\/b> and give every item the proper proportion, we are gonna build the following function to calculate the weight corresponding to each item in each store. <b>We calculate the weights based on the sales of the last 3 months:<\/b>","747358bf":"Error evaluation:","39296e11":"Our ts has 3 levels of hierarchy: total sales, sales per shop and sales per shop and item. In this exercise we are asked to make predictions for the lower hierarchical level, sales per shop and item. This poses a problem since we are working on a personal laptop with not enough computing power to model at the item level. There are to, we are gonna have to work around this issue. We found this [paper published by Rob Hyndman](https:\/\/robjhyndman.com\/publications\/hierarchical-tourism\/) with an interesting approach. Without going into too much detail, the idea of the technique is the following:\n\n<b>-Bottom-up approach:<\/b> Model and predict for the lowest level of the hierarchy and then sum these predictions to produce forecast for the upper levels.\n\n<b>-Top-down approach:<\/b> Model and predict for the top level of the hierarchy and then disaggregate these down the hierarchy.\n\n<b>-Middle-out approach:<\/b> It combines bottom-up and top-down. First generates forecasts for a middle level and the apply top-down or bottom-up to generate the forecasts for the lower and upper levels.\n\n<b>We are going to use the Middle-out approach to model and predict at the shop level and then dissagregate to generate the predictions at the shop-item level.<\/b>","268e1d4f":"The purpose of this notebook is to illustrate a way to approach a time-series forecasting problem. We will analyze the time-series data available and discuss a few algorithms that will do the job. The data we will be using is from the <b>[\"Predict Future Sales\"](https:\/\/www.kaggle.com\/c\/competitive-data-science-predict-future-sales)<\/b>. We are given sales historic data for a bunch of shops and products of a large Russian sofware company and the task is to forecast the total amount of products sold in every shop for the test set.","c8b61811":"<a id = \"prophet_model\"><\/a>\n### Prophet - Modelling\n[Back to Table of Contents](#contents)","1c5152cc":"<a id = \"sales_per_week\"><\/a>\n### Total sales per week\n[Back to Table of Contents](#contents)","bd706682":"<a id = \"EDA\"><\/a>\n# EDA\n[Back to Table of Contents](#contents)","c59b1f23":"<a id = \"autocorrelation\"><\/a>\n### Autocorrelation and partial autocorrelation (ACF and PACF)\n[Back to Table of Contents](#contents)","83ba1e09":"Before we begin modelling... here we put together a df to validate the results of the models. We will use the last month (october, date_block_num=33) for validation. This df has the format of the submission file : ['ID','obs']","0fb7311d":"<a id = \"prophet_quicklook\"><\/a>\n### Prophet - Quick look\n[Back to Table of Contents](#contents)","476617b0":"We borrow this great attributes from the kaggle comunity with the population and income of the cities.","a5840602":"We use GridSearchCV to find the best parameters and do cross validation ","a88ee4dd":"<a id = \"quick_look_items\"><\/a>\n### items\n[Back to Table of Contents](#contents)","d2462586":"Write and read pickle","8119a8f4":"<a id = \"stationarity\"><\/a>\n### Stationarity Test | Dickey-Fuller\n[Back to Table of Contents](#contents)","21052197":"Let's define the following function to get a quick summary of a dataframe","200f8458":"<a id = \"housekeeping\"><\/a>\n# Breaf housekeeping \n[Back to Table of Contents](#contents)","bc6b9d9c":"<a id = \"xgboost\"><\/a>\n### Convert to supervised learning\n[Back to Table of Contents](#contents)","34d08b23":"The blue area represents de confidence interval, set to 95% by default. This suggests that lags with values outside of this area are likely correlated. We can se how there is a positive correlation with the first 6 lags and that <b>it is significant for the lags 2 and 3, which means that the sales from the previous two months have a significant correlation with the sales of the present month.<\/b>","cbd4dc16":"<a id = \"validationdf\"><\/a>\n### Validation df\n[Back to Table of Contents](#contents)","62f91dfd":"Up to this point, we have extracted some interesting new features from the raw data, we merged it all together and we know the essential characteristics of the time series we are dealing with. \n\nIn this section we are gonna explore the underlying intuition behind some comun statistical methods for time series forecasting.","3520d8d7":"<a id = \"decomposing\"><\/a>\n### Decomposing the TS\n[Back to Table of Contents](#contents)","c98b1b1d":"<a id = \"ts_analysis\"><\/a>\n# Time Series Analysis\n[Back to Table of Contents](#contents)","9e67541e":"<a id = \"prophet_prepro\"><\/a>\n### Prophet - Data prepocessing\n[Back to Table of Contents](#contents)","9abf2d6a":"They seem to be composed of a type and subtype separated by a \"-\"... blablabla...","e9458f7a":"<a id = \"xgboost_prediction\"><\/a>\n### XGBOOST - Data forecasting\n[Back to Table of Contents](#contents)","75f4c671":"Let's look for outliers","26db9d7e":"<a id = \"useful\"><\/a>\n### Useful functions\n[Back to Table of Contents](#contents)","543f99e8":"<a id = \"prophet_prediction\"><\/a>\n### Prophet - Forecasting\n[Back to Table of Contents](#contents)","3bea5944":"<a id = \"quick_look_sales_train\"><\/a>\n### sales_train\n[Back to Table of Contents](#contents)","5992448f":"We are going to build a matrix with all the possible permutations of date and shop. This makes more sense if we had the item level because we would be able to build a matrix with all the shop-item possible combinations. Since that is how we should ideally face this problem, I will keep this structure for academic purposes","84a9c7a0":"<a id = \"arima_prediction\"><\/a>\n### ARIMA - Forecasting\n[Back to Table of Contents](#contents)","0202bc9e":"Error evaluation:","6748c3e0":"No surprise here, p-value > 0.05. <b>Ts is non-stationary<\/b>","b07335e9":"<a id = \"xgboost_model\"><\/a>\n### XGBOOST - Modelling\n[Back to Table of Contents](#contents)","f0c3ae11":"<a id = \"quick_look_sample_submission\"><\/a>\n### sample_submission\n[Back to Table of Contents](#contents)","76cc1563":"<a id = \"approach2\"><\/a>\n# Approach 2: Prophet\n[Back to Table of Contents](#contents)","978917a7":"The trend show us that sales are going down. The seasonal graph removes the trend from the ts and shows us high seasonal spikes around Chritsmas time. The resuals show us what is left of the ts when you remove the trend and the seasonality effects, so hopefully the residuals are small, since we don't want other effects, aside from trend and seasonality to explain the ts","fe5c3017":"Let's first take a look at the results of applying ARIMA to the global monthly sales time series. \n\nSince we have seen previously that the time series is not stationary and requires differencing to remove the seasonality, 1 would be a good value for the parameter q. And since the autocorrelation was significat for the first 2 or 3 lags we can set the p to 3.","c2117532":"<a id = \"sales_per_month\"><\/a>\n### Total sales per month\n[Back to Table of Contents](#contents)","16ac9285":"<b>Data description in the kaggle competition:<\/b>\n\nYou are provided with daily historical sales data. The task is to forecast the total amount of products sold in every shop for the test set. Note that the list of shops and products slightly changes every month. Creating a robust model that can handle such situations is part of the challenge.\n\n<b>File descriptions<\/b>\n\n    sales_train.csv - the training set. Daily historical data from January 2013 to October 2015.\n    test.csv - the test set. You need to forecast the sales for these shops and products for November 2015.\n    sample_submission.csv - a sample submission file in the correct format.\n    items.csv - supplemental information about the items\/products.\n    item_categories.csv  - supplemental information about the items categories.\n    shops.csv- supplemental information about the shops.\n\n<b>Data fields<\/b>\n\n    ID - an Id that represents a (Shop, Item) tuple within the test set\n    shop_id - unique identifier of a shop\n    item_id - unique identifier of a product\n    item_category_id - unique identifier of item category\n    item_cnt_day - number of products sold. You are predicting a monthly amount of this measure\n    item_price - current price of an item\n    date - date in format dd\/mm\/yyyy\n    date_block_num - a consecutive month number, used for convenience. January 2013 is 0, February 2013 is 1,..., October 2015 is 33\n    item_name - name of item\n    shop_name - name of shop\n    item_category_name - name of item category\n","86ec81eb":"<a id = \"quick_look\"><\/a>\n# Quick look at the data\n[Back to Table of Contents](#contents)","85611165":"<a id = \"quick_look_test\"><\/a>\n### test\n[Back to Table of Contents](#contents)","d30a69a6":"<a id = \"remove_stationarity\"><\/a>\n### Remove Non-Stationarity effects\n[Back to Table of Contents](#contents)","721b6597":"The conditions we need to look at to determine if a ts is stationary are:\n\n <b>-The mean is constant<\/b>\n \n <b>-The standard deviation is constant<\/b>\n \n <b>-There is no seasonality<\/b>\n\nSome forecasting machine learning methods and statistical modeling methods require the ts to be stationary in order to be able to use them, that is without the effects of a trend, seasonality, volatility and other time-dependent structures. We have already seen that the ts we are working with here is not stationary. It has a downward trend and high seasonality. The Dickey-Fuller test gives us a number to measure how far off our ts is from being stationary.\n\nThe null hypothesis of the test is that the time series is not stationary and has time-dependent structures of some kind. The alternate hypothesis (rejecting the null hypothesis) is that the time series is stationary. It use the p-value to interpret the result. If the p-value > 0.05, we can reject the h0 and the data is non-stationary. If p-value <= 0.05 we can reject the h0 and conclude that the ts is stationary. ","86336dc0":"Megajoin","54803211":"<a id = \"conclusions\"><\/a>\n# Results and conclusions\n[Back to Table of Contents](#contents)","1d0c1a7b":"We include two variables, one indicating if there is a holiday in the current month and another one in the next month","287f9118":"Let's put together all the features we have:","a07b81d9":"<a id = \"sales_per_month_shop\"><\/a>\n### Total sales per month and shop\n[Back to Table of Contents](#contents)","ad52e02c":"With enough computing power we wouldn't need to use the TOP-DOWN method and we would be able to forecast at the lowest hierarchical level. If that were the case, it would be a very good idea to explore XGB with more lag features at the item level. Exploring SARIMAX with yearly seasonality would also be an interesting approach. Prophet is a very easy to use and quick way to do a forecast, it's a great option for a quick solution.","f6e6a3cb":"This techniques consists in finding how correlated a time series is with itself in prior time steps. We fit the observations in time t with the observations in t-1, t-2 , etc.  In this particular problem, it will allow us to know how correlated the number of sales in a month is to the number of sales the previous month, and to two months before, and so on... There is two effects to take into account: The direct effect and the indirect effect.\n\nLet's imagine we want to check the correlation between time t observations and time t-2 observations. The direct effect will be the correlation between t-2 to t and the correlation between t-1 and t. The indirect correlation would be between t-2 and t-1.\n\n<b>Autocorrelation:<\/b> Takes into account direct and indirect effects.\n\n<b>Partial autocorrelation:<\/b> Takes into account only the direct effects.","4ec6f522":"<a id = \"approach1\"><\/a>\n# Approach 1: ARIMA\n[Back to Table of Contents](#contents)","3ff67f5b":"Let's take a look at the different things we need to do to make a time series stationary","da519afd":"We can already see  that the total sales decrease over time and that there is an obvious yearly seasonal pattern with high spikes around Christmas time","872db8b8":"Let's import all the necessary libraries and read the csv files:","00fe123b":"<a id = \"approach3\"><\/a>\n# Approach 3: XGBOOST\n[Back to Table of Contents](#contents)","06b3bf8a":"Error evaluation:","6e87fa8a":"<a id = \"hierarchical\"><\/a>\n### Hierarchical Time Series\n[Back to Table of Contents](#contents)","a5ac59ff":"<a id = \"xgboost_prepro\"><\/a>\n### XGBOOST - Data prepocessing\n[Back to Table of Contents](#contents)","b8b3d353":"In this section we are gonna take a look at the main properties of time series. We are gonna check the stationarity of the series, decompose it into its essential components and check how strong the relationship between an observation is with the observations at prior time steps, called lags. \n\nSome of the algorithms that we will be using require the time series to be stationary, so we will have to transform it beforehand.","9c4d2953":"Here we are gonna decompose the ts into its essential compenents, the general trend, seasonal trend and residuals","80a464e5":"<a id = \"sales_per_month_category\"><\/a>\n### Total sales per month and category\n[Back to Table of Contents](#contents)","b19e0e43":"<a id = \"time_variable\"><\/a>\n# Dealing with the time variable\n[Back to Table of Contents](#contents)","d460b3b0":"We will explore this new forecasting procedure developed by Facebook. It is relatively simple to use and could be an interesting approach.\n\nIt seems to work reasonably well on messy data, it is robust to outliers and missing data and it is fast.","d68e064f":"The name of the city seems to be the first word of the shop name. We see some typos like \"!\u042f\u043a\u0443\u0442\u0441\u043a\" instead of \"\u042f\u043a\u0443\u0442\u0441\u043a\" and some shop names that seem to be duplicates: \"\u0416\u0443\u043a\u043e\u0432\u0441\u043a\u0438\u0439 \u0443\u043b. \u0427\u043a\u0430\u043b\u043e\u0432\u0430 39\u043c\u00b2\" and  \"\u0416\u0443\u043a\u043e\u0432\u0441\u043a\u0438\u0439 \u0443\u043b. \u0427\u043a\u0430\u043b\u043e\u0432\u0430 39\u043c?\". We will keep this in mind when we prepare the data","79e7b664":"<a id = \"arima_quicklook\"><\/a>\n### ARIMA - Quick look\n[Back to Table of Contents](#contents)","059d8d76":"We have the date_block_num variable that gives us a montly segmentation of the ts. We will reformat the date column anyway in order to be able to resample the date daily, weekly and yearly if necessary:","dbe287d4":"<a id = \"quick_look_shops\"><\/a>\n### shops\n[Back to Table of Contents](#contents)"}}