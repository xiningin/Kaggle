{"cell_type":{"5a8b213a":"code","c5498e92":"code","4ee45ad8":"code","b81648da":"code","00095692":"code","f499718b":"code","56ab6a7a":"code","7fbabe43":"code","ca71f09e":"code","4f4e7468":"code","21a6d443":"code","8551b06f":"code","6a9d4fc7":"code","e593c033":"code","869ceb49":"code","b979bd3c":"code","703ceddf":"code","0dd15785":"code","98ee0b68":"code","1ae2e902":"markdown","56b5a1ce":"markdown","eac0650c":"markdown","71b10e3c":"markdown","44c85fca":"markdown","80da980f":"markdown"},"source":{"5a8b213a":"#!pip install optuna\nfrom __future__ import absolute_import, division, print_function\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Input\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard, ReduceLROnPlateau\nfrom keras.backend import clear_session\nfrom keras.optimizers import RMSprop, Adam\nfrom keras import regularizers\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nimport seaborn as sns\nimport optuna\nfrom optuna.integration import TFKerasPruningCallback\n\nfrom subprocess import check_output\nimport time\nimport warnings\n\nimport os\nimport sys\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nwarnings.filterwarnings('ignore')\nwarnings.simplefilter('ignore')","c5498e92":"#This runtime value is used by Optuna to run the permutations required for hyper-parameter optimizations. \n#The results in this report were achieved over 1 hour run of the Optuna code.\nruntime = 3600","4ee45ad8":"ori = pd.read_csv(\"..\/input\/multiples\/diabetes75pc_100_times.csv\")\n\n'''\n# The below code shows how I split the original data to 75:25 ratio\n# I wrote a function to fabricate (any number of) new records as \"multiplydata_pcversion\" function\n\nori = pd.read_csv(\"..\/input\/pimadiabetescsv\/diabetes.csv\")\n\nTrain_data = ori[:576]\nTest_data = ori[576:]\nTest_data.shape[0]\n\ntrainfile = \"C:\/\/Users\/\/pg\/\/Documents\/\/datascience\/\/case studies\/\/pima\/\/data\/\/diabetes75pc.csv\"\ntestfile = \"C:\/\/Users\/\/pg\/\/Documents\/\/datascience\/\/case studies\/\/pima\/\/data\/\/diabetes25pc.csv\"\nTrain_data.to_csv(trainfile, index=False, line_terminator='\\n') \n#Output ->   ..\/input\/multiples\/diabetes75pc_100_times.csv\nTest_data.to_csv(testfile, index=False, line_terminator='\\n') \n#Output ->   ..\/input\/multiples\/diabetes25pc.csv\n\nmultiple = 50\nmultiplydata_pcversion(multiple,trainfile)\n'''\nprint(\"Check the number of records:  \", ori.shape)\nori.head()","b81648da":"sns.heatmap(ori.corr(),annot=True, cmap = 'YlGnBu')\nfig = plt.gcf()\nfig.set_size_inches(10, 8)\nplt.show()","00095692":"ax = sns.violinplot(x='Outcome', y='BloodPressure', data=ori, palette='muted', split=True)","f499718b":"pdata = ori.copy(deep=True)\nfeature_names = pdata.columns[:8]\nX = pdata[feature_names]\ny = pdata.Outcome\n\n# Features chosen based on RFECV result\nbest_features = ['Pregnancies', 'Glucose', 'BMI', 'DiabetesPedigreeFunction']\nX = StandardScaler().fit_transform(X[best_features])\n# Splitting  data into training and testing (80% \/ 20%)\nX_train, X_test, y_train, y_test = train_test_split(\n    X,\n    y,\n    test_size=0.20\n)","56ab6a7a":"tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\ntf.config.experimental_connect_to_cluster(tpu)\ntf.tpu.experimental.initialize_tpu_system(tpu)\n\n# instantiate a distribution strategy\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n\nstudy_name = \"PIMAFeatureEng\"\ncheckpoint_path = '.\/Pima26Nov.hdf5'\ncheckpoint_dir = os.path.dirname(checkpoint_path)\n\n# create checkpoint callback\ncp_callback = keras.callbacks.ModelCheckpoint(checkpoint_path,\n                                              monitor='val_accuracy',\n                                              save_weights_only=False,\n                                              save_best_only=True,\n                                              verbose=0\n                                             )\n\nwith tpu_strategy.scope():           \n    def objective(trial):\n        # Clear clutter from previous Keras session graphs.\n        clear_session()\n\n        num_epochs = 100\n            # Create callbacks for early stopping and pruning.\n        callbacks = [\n            keras.callbacks.EarlyStopping(patience=3),\n            TFKerasPruningCallback(trial, \"val_accuracy\"),\n            cp_callback\n        ]    \n        model = Sequential()\n        for i in range(3):                        \n            model.add(Dense(int(trial.suggest_discrete_uniform(\n                'FC_{}_num_hidden_units'.format(i), 16, 80, 4)),\n                            activation = \"relu\",\n                            input_dim=4\n                           )\n                     )              \n        model.add(Dense(1, activation = 'sigmoid'))\n        lr = trial.suggest_uniform(\"lr\", 1e-4, 1e-1)\n        model.compile(loss = 'binary_crossentropy', optimizer = Adam(lr=lr), metrics=['accuracy'])\n        batch_size = trial.suggest_int('Batch_size', 32, 128, 16) \n        history = model.fit(X_train, \n                           y_train,\n                           validation_data= (X_test, y_test),\n                           epochs=num_epochs,\n                           batch_size = batch_size,\n                           callbacks=callbacks,\n                           verbose=0\n                           )\n        score = model.evaluate(X_test, y_test, verbose=0)\n        return score[1]","7fbabe43":"tic = time.process_time()\n#The below line is important to see the logs while Optuna optimization\noptuna.logging.disable_default_handler()\nstudy = optuna.create_study(\n        sampler=optuna.samplers.TPESampler(\n            consider_prior=True, prior_weight=1.0, \n            consider_magic_clip=True, consider_endpoints=False, \n            n_startup_trials=10, n_ei_candidates=24, \n            seed=None), \n        pruner=optuna.pruners.SuccessiveHalvingPruner(\n            min_resource=2, reduction_factor=4, min_early_stopping_rate=1),\n        study_name = study_name, \n        direction=\"maximize\",\n)\n\n#study.optimize(objective, n_trials=100, timeout=600)\nstudy.optimize(objective, timeout=runtime)\n\ntoc = time.process_time()\nprint(\"time taken :  \")\nprint(toc-tic)\nprint(\"Number of finished trials: {}\".format(len(study.trials)))\nprint(\"Best trial number:\", study.best_trial.number)\ntrial = study.best_trial\nprint(\"  Value: {}\".format(trial.value))\nprint(\"  Params: \")\nfor key, value in trial.params.items():\n    print(\"    {}: {}\".format(key, value))\n    ","ca71f09e":"#ls {checkpoint_path}\nfor dirname, _, filenames in os.walk('.\/'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","4f4e7468":"optuna.visualization.plot_intermediate_values(study)","21a6d443":"optuna.visualization.plot_parallel_coordinate(study)","8551b06f":"#Load the optimized model which is saved at checkpoint_path\nprint(checkpoint_path)\nnew_model = keras.models.load_model(checkpoint_path)\nnew_model.summary()","6a9d4fc7":"scores = new_model.evaluate(X_test, y_test)\npy_pred  = new_model.predict(X_test)\npy_pred = np.where(py_pred > 0.5, 1, 0) #py_pred = py_pred.round()\nprint(\"Accuracy of the training\", scores[1]*100)","e593c033":"cm = confusion_matrix(y_test, py_pred)\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.set(color_codes =True)\nsns.set(font_scale=1.5)\nsns.heatmap(cm, annot=True, fmt='g')\nplt.show()","869ceb49":"from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, roc_auc_score, classification_report\n\nprint('Accuracy: {:.2f}%'.format(accuracy_score(y_test, py_pred) * 100))\nprint('Classification report:\\n\\n', classification_report(y_test, py_pred))\n","b979bd3c":"# Load the test data which was kept aside in the beginning\ntest = pd.read_csv(\"..\/input\/multiples\/diabetes25pc.csv\")\nbest_features = ['Pregnancies', 'Glucose', 'BMI', 'DiabetesPedigreeFunction']\nX1 = StandardScaler().fit_transform(test[best_features])\ny1 = test.Outcome","703ceddf":"py_pred1  = new_model.predict(X1)\npy_pred1 = np.where(py_pred1 > 0.5, 1, 0) #py_pred = py_pred.round()","0dd15785":"cm = confusion_matrix(y1, py_pred1)\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.set(color_codes =True)\nsns.set(font_scale=1.5)\nsns.heatmap(cm, annot=True, fmt='g')\nplt.show()","98ee0b68":"print('Accuracy: {:.2f}%'.format(accuracy_score(y1, py_pred1) * 100))\nprint('Classification report:\\n\\n', classification_report(y1, py_pred1))","1ae2e902":"    \n### References \/Acknowledgements\n\n####  1. Notebook by Piotr Tynecki on Pima Dataset:->    https:\/\/www.kaggle.com\/ptynecki\/pima-indians-diabetes-prediction-with-lr-84\n\n\n\n####  2. Optuna - used for hyperparameter optimization:->    https:\/\/optuna.org\/\n\n\n\n####  3. Kaggle - Thanks for providing this fantastic platform and allowing to use GPUs\/TPUs","56b5a1ce":"From the above plot we can see that BloodPressure has hardly any effect on the Outcome(Diabetic\/NonDiabetic). So we can easily drop similar other features from our model.","eac0650c":"# Pima Diabetes Dataset Analysis (98% Accuracy & 98% F1 Score)\n\n### Using Keras(Dense Layer), Optuna(Hyperparameter optimization) & RFECV (Recursive Feature Elimination)\n\n\n# Author\n### Pradeep Gurav\n##### https:\/\/www.linkedin.com\/in\/pradeepgurav\/\n\n#### November 26, 2020\n\n##### How to get Pima Diabetes Analysis results > 98%\nMost of the analysis\/models results on Kaggle are around 78%. Piotr's Analysis Notebook which was the highest so far who had achieved 84% accuracy.\n\nThere are many notebooks describing Data exploration and understanding Pima Diabetes data, so will not cover that part in this report. From the data exploration, many of the features(such as Insulin, SkinThickness etc) had overlapping distribution over the outcomes(Diabetic-\/NonDiabetic), so it was going to be hard relying on those features.\n\nTried many methods to improve the accuracy such as:\n\n    1. Deleting records with zero values\n    2. Imputing zero values using KNN, mean values\n    3. Clipping extreme values or out of range values, bucketing certain features (Age, Pregnancies)\n    4. Applying LogTransform so as to shift some of the feature's distribution towards Normal. No Good results.\n    5. Many other imputing methods including my own method (Dense Layer network - regression values)\n    6. Feature separation, using two separate models using diff features and concatenating the models\n    7. And also using Optuna to try thousands of combinations of layers, Dense layer units, lr, batch sizes. Optuna is  a fantastic tool.\n    8. Also used RFECV & PCA to identify most co-relating features\n    9. What actually contributed the most to the performance is the 'Data Augmentation' method\n\nThe below code is self explanatory, here is the summary of steps:\n\n    1. Splitted original data into 75:25. Saved both as CSVs again. The 25% dataset was not exposed to the model while training.\n    2. Used 75% dataset(576 records) and added new 76992 records to the original dataset.\n    3. Used Optuna hyperoptimization to get optimum layers and units.\n    4. Saved the best model and used the same model once Optuna optimization was completed\n    5. Used the 25% data kept aside for testing, which have 98% accuracy (same 98% for most classification parameters)\n    6. This model could lead to accuracy above 99% with a bit more effort\n       \nData files used:\n\n    1. Public known data      ..\/input\/pimadiabetescsv\/diabetes.csv\n    2. file with 100 times    ..\/input\/multiples\/diabetes75pc_100_times.csv\n    3. 25% testdata unsued in training    ..\/input\/multiples\/diabetes25pc.csv","71b10e3c":"### Discussion on Results achieved\n\n1. The report describes various methods tried to optimize the results\n2. It provides a framework which uses various tools and techniques to solve similar problems\n3. There is significant jump in accuracy improvement from 84% to 98%","44c85fca":"### Data Exploration\n\nThe below heatmap shows correlation among all the features and the Outcome.","80da980f":"Confusion Matrix shows True Positives, True Negatives, False Positives(FP) and False Negatives(FN). 0 FPs and 0 FNs is the ideal outcome."}}