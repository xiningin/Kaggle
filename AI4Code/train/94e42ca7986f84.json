{"cell_type":{"c7b9d784":"code","98ebeb34":"code","c6b1806e":"code","e06892cb":"code","04a1591e":"code","2c4a2665":"code","0e50c873":"code","6739867b":"code","a8222d27":"code","cad9cc3a":"code","39e2efc0":"code","259a8655":"code","927808c3":"code","5433efb4":"code","c43a97f8":"code","9e4020ea":"code","bd789997":"code","dd00b527":"code","7815b2a0":"code","0c7f1232":"code","bd9cc3c6":"code","0525bf91":"code","f0a80153":"code","741cfce4":"code","defe1fb1":"code","fc1687a5":"code","19cb3fc7":"code","aa9ebd63":"code","2c14840a":"code","f6ce6a58":"code","7dab0ac4":"code","3c5f9279":"code","d0b7554d":"code","861af22a":"code","eefc52d6":"code","2514fee3":"markdown","4d4f2b39":"markdown","bf5ce2cd":"markdown","b697c441":"markdown","f0399635":"markdown","64116402":"markdown","2b019862":"markdown","dc74d149":"markdown","23a9d708":"markdown"},"source":{"c7b9d784":"!pip -q install pyspellchecker","98ebeb34":"import os\nimport numpy as np\nimport pandas as pd\nimport random\nimport re\nimport string\nfrom spellchecker import SpellChecker\nfrom tqdm.notebook import tqdm\nimport warnings\n\nfrom sklearn.metrics import roc_auc_score, f1_score, matthews_corrcoef\nfrom sklearn.model_selection import StratifiedKFold\n\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.layers import Dropout, Dense, Input\nfrom tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\nfrom tensorflow.keras.models import Model\n\nfrom tokenizers import BertWordPieceTokenizer\nfrom transformers import TFBertModel, AdamWeightDecay, BertTokenizerFast","c6b1806e":"warnings.filterwarnings(\"ignore\")","e06892cb":"SEED = 42\nMAX_LEN = 192\nEPOCHS = 6\nNUM_SPLITS = 5\nLR = 3e-5","04a1591e":"os.environ['PYTHONHASHSEED']=str(SEED)\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntf.random.set_seed(SEED)","2c4a2665":"train = pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")\ntest = pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")","0e50c873":"test[\"target\"] = -1","6739867b":"df = pd.concat([train, test])","a8222d27":"PUNCT_TO_REMOVE = string.punctuation\ndef remove_punctuation(text):\n    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n\ndf[\"text\"] = df[\"text\"].apply(lambda text: remove_punctuation(text))","cad9cc3a":"def remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\ndf[\"text\"] = df[\"text\"].apply(lambda text: remove_emoji(text))","39e2efc0":"def remove_urls(text):\n    url_pattern = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    try:\n        return url_pattern.sub(r'', text)\n    except:\n        print(text)\n    \ndf[\"text\"] = df[\"text\"].apply(lambda text: remove_urls(text))","259a8655":"def remove_html(text):\n    html_pattern = re.compile('<.*?>')\n    return html_pattern.sub(r'', text)\n\ndf[\"text\"] = df[\"text\"].apply(lambda text: remove_html(text))","927808c3":"with open(\"..\/input\/slangtext\/slang.txt\", \"r\") as file:\n    chat_words_str = file.read()\n\nchat_words_map_dict = {}\nchat_words_list = []\nfor line in chat_words_str.split(\"\\n\"):\n    if line != \"\" and \"=\" in line:\n        cw = line.split(\"=\")[0]\n        cw_expanded = line.split(\"=\")[1]\n        chat_words_list.append(cw)\n        chat_words_map_dict[cw] = cw_expanded\nchat_words_list = set(chat_words_list)","5433efb4":"def chat_words_conversion(text):\n    new_text = []\n    for w in text.split():\n        if w.upper() in chat_words_list:\n            new_text.append(chat_words_map_dict[w.upper()])\n        else:\n            new_text.append(w)\n    return \" \".join(new_text)\n\ndf[\"text\"] = df[\"text\"].apply(lambda text: chat_words_conversion(text))","c43a97f8":"spell = SpellChecker()\ndef correct_spellings(text):\n    corrected_text = []\n    misspelled_words = spell.unknown(text.split())\n    for word in text.split():\n        if word in misspelled_words:\n            corrected_text.append(spell.correction(word))\n        else:\n            corrected_text.append(word)\n    return \" \".join(corrected_text)\n\ndf[\"text\"] = df[\"text\"].apply(lambda text: chat_words_conversion(text))","9e4020ea":"train = df[df[\"target\"]!=-1]\ntest = df[df[\"target\"]==-1]","bd789997":"train.shape, test.shape, df.shape","dd00b527":"def fast_encode(texts, tokenizer, chunk_size=256, maxlen=512):\n    tokenizer.enable_truncation(max_length=maxlen)\n    tokenizer.enable_padding(max_length=maxlen)\n    all_ids = []\n    \n    for i in tqdm(range(0, len(texts), chunk_size)):\n        text_chunk = texts[i:i+chunk_size].tolist()\n        encs = tokenizer.encode_batch(text_chunk)\n        all_ids.extend([enc.ids for enc in encs])\n    \n    return np.array(all_ids)","7815b2a0":"# First load the real tokenizer\ntokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n\n# Save the loaded tokenizer locally\nsave_path = 'distilbert_base_uncased\/'\nif not os.path.exists(save_path):\n    os.makedirs(save_path)\ntokenizer.save_pretrained(save_path)\n\n# Reload it with the huggingface tokenizers library\nfast_tokenizer = BertWordPieceTokenizer('distilbert_base_uncased\/vocab.txt', lowercase=True)","0c7f1232":"X_test_tokenized = fast_encode(test[\"text\"].astype(str), fast_tokenizer, maxlen=MAX_LEN)","bd9cc3c6":"def focal_loss(gamma=2., alpha=.25):\n    def focal_loss_fixed(y_true, y_pred):\n        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n        return -K.mean(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1)) - K.mean((1 - alpha) * K.pow(pt_0, gamma) * K.log(1. - pt_0))\n    return focal_loss_fixed","0525bf91":"def auc(y_true, y_pred):\n    def fallback_auc(y_true, y_pred):\n        try:\n            return roc_auc_score(y_true, y_pred)\n        except:\n            return 0.5\n    return tf.py_function(fallback_auc, (y_true, y_pred), tf.double)","f0a80153":"tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\ntf.config.experimental_connect_to_cluster(tpu)\ntf.tpu.experimental.initialize_tpu_system(tpu)\nstrategy = tf.distribute.experimental.TPUStrategy(tpu)\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync","741cfce4":"def build_model(transformer, loss='binary_crossentropy', max_len=512):\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    # last hidden state : (batch_size, sequence_length, hidden_size)\n    sequence_output = transformer(input_word_ids)[0]\n    cls_token = sequence_output[:, 0, :]\n    x = Dropout(0.35)(cls_token)\n    out = Dense(2, activation='softmax')(x)\n    \n    optimizer = tfa.optimizers.RectifiedAdam(lr=LR)\n    model = Model(inputs=input_word_ids, outputs=out)\n    model.compile(optimizer=optimizer, loss=loss, metrics=[tf.keras.metrics.AUC()])\n    \n    return model","defe1fb1":"def build_lrfn(lr_start=0.000001, lr_max=0.000004, \n               lr_min=0.0000001, lr_rampup_epochs=7, \n               lr_sustain_epochs=0, lr_exp_decay=.87):\n    lr_max = lr_max * strategy.num_replicas_in_sync\n    def lrfn(epoch):\n        if epoch < lr_rampup_epochs:\n            lr = (lr_max - lr_start) \/ lr_rampup_epochs * epoch + lr_start\n        elif epoch < lr_rampup_epochs + lr_sustain_epochs:\n            lr = lr_max\n        else:\n            lr = (lr_max - lr_min) * lr_exp_decay**(epoch - lr_rampup_epochs - lr_sustain_epochs) + lr_min\n        return lr\n    return lrfn","fc1687a5":"eas = EarlyStopping(monitor='val_auc', min_delta=0.001, patience=3,\n                    verbose=1, mode='max', baseline=None, restore_best_weights=True)\nlrfn = build_lrfn()\nlrs = LearningRateScheduler(lrfn, verbose=1)","19cb3fc7":"test_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices(X_test_tokenized)\n    .batch(BATCH_SIZE)\n)","aa9ebd63":"num_steps = len(train) \/\/ BATCH_SIZE\nfinal_preds = np.zeros((len(test)))\ntotal_preds = np.zeros((len(train)))","2c14840a":"kfold = StratifiedKFold(n_splits=NUM_SPLITS, shuffle=True, random_state=SEED)","f6ce6a58":"for fold, (train_idx, valid_idx) in enumerate(kfold.split(X=train['text'], y=train['target'])):\n    print(\"*\"*60)\n    print(\"*\"+\" \"*26+f\"FOLD {fold+1}\"+\" \"*26+\"*\")\n    print(\"*\"*60, end=\"\\n\\n\")\n\n    X_train = train.iloc[train_idx].reset_index(drop=True)\n    X_valid = train.iloc[valid_idx].reset_index(drop=True)\n    \n    y_train = X_train[\"target\"]\n    y_valid = X_valid[\"target\"]\n    \n    X_train_tokenized = fast_encode(X_train.text.astype(str), fast_tokenizer, maxlen=MAX_LEN)\n    X_valid_tokenized = fast_encode(X_valid.text.astype(str), fast_tokenizer, maxlen=MAX_LEN)\n\n    train_dataset = tf.data.Dataset.from_tensor_slices((X_train_tokenized, to_categorical(y_train)))\n    train_dataset = train_dataset.repeat()\n    train_dataset = train_dataset.shuffle(2048)\n    train_dataset = train_dataset.batch(BATCH_SIZE)\n    train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n    \n    valid_dataset = tf.data.Dataset\n    valid_dataset = valid_dataset.from_tensor_slices((X_valid_tokenized, to_categorical(y_valid)))\n    valid_dataset = valid_dataset.batch(BATCH_SIZE)\n    \n    # release memory when building models in a loop\n    K.clear_session()\n    with strategy.scope():\n        transformer_layer = TFBertModel.from_pretrained('bert-base-uncased')\n        model = build_model(transformer_layer, loss=focal_loss(gamma=1.5), max_len=MAX_LEN)\n    \n    history = model.fit(train_dataset,\n                    steps_per_epoch=num_steps,\n                    validation_data=valid_dataset,\n                    callbacks=[eas, lrs], \n                    epochs=EPOCHS)\n    \n    valid_preds = model.predict(valid_dataset)[:, 1]\n    total_preds[valid_idx] = valid_preds\n    \n    test_preds = model.predict(test_dataset)[:, 1]\n    final_preds += test_preds","7dab0ac4":"actuals = train[\"target\"].values\ntotal_preds[total_preds >= 0.5] = 1\ntotal_preds[total_preds < 0.5] = 0","3c5f9279":"print(f\"AUC: {auc(actuals, total_preds)}\")\nprint(f\"F1 Score: {f1_score(actuals, total_preds)}\")\nprint(f\"MCC: {matthews_corrcoef(actuals, total_preds)}\")","d0b7554d":"submission = pd.read_csv(\"..\/input\/nlp-getting-started\/sample_submission.csv\")","861af22a":"submission[\"target\"] = final_preds\/NUM_SPLITS\nsubmission[\"target\"] = submission[\"target\"].apply(lambda x: 1 if x>=0.5 else 0)","eefc52d6":"submission.to_csv(\"submission.csv\", index=False)","2514fee3":"## Training","4d4f2b39":"## Loss & Metric","bf5ce2cd":"## Constants","b697c441":"## Tokenizing Data","f0399635":"## Importing Libraries","64116402":"## Text Preprocessing","2b019862":"## Callbacks","dc74d149":"## Model","23a9d708":"## Submission"}}