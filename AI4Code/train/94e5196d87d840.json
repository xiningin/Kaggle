{"cell_type":{"c9425b48":"code","4377730d":"code","9bdd21d2":"code","a4b49085":"code","67c847d5":"code","28bf0d01":"code","59799527":"code","de12e2c4":"code","5906442d":"code","5817d798":"code","55661584":"code","c4eebdd1":"code","55432a8e":"code","bd4f0ced":"code","365b40d6":"code","ae7a1a35":"code","542bfa8b":"code","2af85a30":"code","266d35a7":"code","a83f6975":"code","e888198b":"code","5496bad3":"code","7cf8763e":"code","4b5263fa":"code","2068cdb1":"code","8436b32c":"code","97d7e9a2":"code","638c3ed1":"code","5b6740e0":"code","b573c6d6":"code","60d61a6c":"code","8dc9acbe":"code","f3ae459c":"code","581d6dc6":"code","d5f4e951":"code","8a25b166":"code","b0f3b129":"code","7b75a98d":"code","373474bf":"code","13cf51be":"code","54fad9e3":"code","8f932f2c":"code","5cbaaa6f":"code","5a93e173":"code","e72064c1":"markdown","14db0861":"markdown","5497bbfa":"markdown","704684ab":"markdown","a5691fdd":"markdown","2b3a661e":"markdown","f8449c99":"markdown","8edac808":"markdown","8f5ad765":"markdown","eddea2da":"markdown","3fbd4798":"markdown","448a356d":"markdown","1640e1cb":"markdown","1eb4e4e3":"markdown","c70d5142":"markdown","4f8fe9b8":"markdown","fb86c88e":"markdown","d70eb82c":"markdown","e2531f2e":"markdown","0925accd":"markdown","08334630":"markdown","e361eae0":"markdown","deb6338c":"markdown","8c41656c":"markdown","aa34385b":"markdown","b1f0ca60":"markdown","2f9c4f5f":"markdown","924232bb":"markdown","158dcfc9":"markdown","97d42b34":"markdown","f00a181f":"markdown","5d390a8b":"markdown","7cdf1e53":"markdown","a0c4d812":"markdown","8cd1b04d":"markdown","0c24f3b4":"markdown","dfbeb3e1":"markdown"},"source":{"c9425b48":"%%html\n<style> \n@import url('https:\/\/fonts.googleapis.com\/css?family=Orbitron|Roboto');\na {color: #37c9e1; font-family: 'Roboto';} \nh1 {color: #C20E69; font-family: 'Poppins'} \nh2, h3 {color: #25B89B; font-family: 'Poppins';}\nh4 {color: #818286; font-family: 'Roboto';}\n                                      \n<\/style>","4377730d":"# Main\n\nimport pandas as pd\nimport numpy as np\nimport warnings\n\n# Plots\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport plotly.offline as py\nimport plotly.graph_objs as go\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nimport plotly.tools as tls\nimport plotly.figure_factory as ff\nimport itertools\nplt.style.use('fivethirtyeight')\npy.offline.init_notebook_mode(connected=True)\n\n# Models & Others\n\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.preprocessing import scale, StandardScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.metrics import confusion_matrix, accuracy_score, mean_squared_error, r2_score, roc_auc_score, roc_curve, classification_report\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import KFold\nfrom xgboost import XGBClassifier\nwarnings.simplefilter(action=\"ignore\")\n","9bdd21d2":"df = pd.read_csv(r\"..\/input\/pima-indians-diabetes-database\/diabetes.csv\")\ndf.head()","a4b49085":"df.describe([0.10, 0.25, 0.50, 0.75, 0.90, 0.95, 0.99]).T","67c847d5":"# Data Shape\nprint(\"There are {} observation and {} features \".format(df.shape[0], df.shape[1]))","28bf0d01":"#------------COUNT-----------------------\ndef target_count():\n    trace = go.Bar( x = df['Outcome'].value_counts().values.tolist(), \n                    y = ['healthy','diabetic' ], \n                    orientation = 'h', \n                    text=df['Outcome'].value_counts().values.tolist(), \n                    textfont=dict(size=20),\n                    textposition = 'auto',\n                    opacity = 0.8,marker=dict(\n                    color=['#25B89B', '#C20E69'],\n                    line=dict(color='#FFFFFF',width=1.5)))\n\n    layout = dict(title =  'Count of Outcome variable')\n\n    fig = dict(data = [trace], layout=layout)\n    py.iplot(fig)\ntarget_count()","59799527":"def cols():\n    cat_cols = [col for col in df.columns if df[col].dtypes == \"O\"]\n    if len(cat_cols) == 0:\n        print(\"There is not Categorical Column\")\n    else:\n        print(\"Number of Categorical Column: \", len(cat_cols),\"\\n\",cat_cols)\n    \n    num_cols = [col for col in df.columns if df[col].dtypes != \"O\"]\n    if len(num_cols) == 0:\n        print(\"There is not Numerical Column\")\n    print(\"Number of Numerical Columns: \", len(num_cols),\"\\n\",num_cols)\ncols()","de12e2c4":"# Missing Values Table Function\ndef missing_values_table(dataframe):\n    \n    variables_with_na = [col for col in dataframe.columns if dataframe[col].isnull().sum() > 0]\n    n_miss = dataframe[variables_with_na].isnull().sum().sort_values(ascending=False)\n    ratio = (dataframe[variables_with_na].isnull().sum() \/ dataframe.shape[0] * 100).sort_values(ascending=False)\n    dtypes = dataframe.dtypes\n    dtypesna = dtypes.loc[(np.sum(dataframe.isnull()) != 0)]\n    missing_df = pd.concat([n_miss, np.round(ratio, 2), dtypesna], axis=1, keys=['n_miss', 'ratio', 'type'])\n    if len(missing_df)>0:\n        print(missing_df)\n        print(\"\\nThere are {} columns with missing values\\n\".format(len(missing_df)))\n    else:\n        print(\"\\nThere is no missing value\") \n\n","5906442d":"missing_values_table(df)","5817d798":"# This features can not be 0!\nmissing = [\"Glucose\", \"BMI\", \"BloodPressure\", \"SkinThickness\", \"Insulin\"]\n\nfor i in missing:\n    df[i] = np.where(df[i] == 0, np.nan, df[i])","55661584":"missing_values_table(df)","c4eebdd1":"#The missing values will be filled with the median values of each variable\ndef median_target(variable):   \n    temp = df[df[variable].notnull()]\n    temp = temp[[variable, 'Outcome']].groupby(['Outcome'])[[variable]].median().reset_index()\n    return temp","55432a8e":"#The values to be given for incomplete observations are given the median value of people who are not sick and the median values of people who are sick\ncolumns = df.columns\ncolumns = columns.drop(\"Outcome\")\nfor i in columns:\n    median_target(i)\n    df.loc[(df['Outcome'] == 0 ) & (df[i].isnull()), i] = median_target(i)[i][0]\n    df.loc[(df['Outcome'] == 1 ) & (df[i].isnull()), i] = median_target(i)[i][1]","bd4f0ced":"missing_values_table(df)","365b40d6":"df.hist(bins=20,color = \"#F19C1F\",edgecolor='white',figsize = (15,15));","ae7a1a35":"corr_matrix = df.corr()\nsns.clustermap(corr_matrix, annot = True, fmt = \".2f\", cmap = \"viridis\", figsize=(11,11))\nplt.title(\"Correlation Between Features\")\nplt.show()","542bfa8b":"# Split X and y\ny = df[\"Outcome\"]\nX = df.drop([\"Outcome\"], axis = 1)","2af85a30":"log_model = LogisticRegression().fit(X,y)\ny_pred = log_model.predict(X)\nprint(\"Accuracy Score:\", accuracy_score(y, y_pred), \"\\n\")\nprint(classification_report(y,y_pred))","266d35a7":"models = []\nmodels.append(('LR', LogisticRegression()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('RF', RandomForestClassifier()))\nmodels.append(('GBM',GradientBoostingClassifier()))\nmodels.append(('XGB', GradientBoostingClassifier()))\nmodels.append((\"LightGBM\", LGBMClassifier()))\n\n# evaluate each model in turn\nresults = []\nnames = []\n\nfor name, model in models:\n    \n        cv_results = cross_val_score(model, X, y, cv = 10, scoring= \"accuracy\")\n        results.append(cv_results)\n        names.append(name)\n        msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n        print(msg)","a83f6975":"Importance = pd.DataFrame({'Importance':LGBMClassifier().fit(X, y).feature_importances_*100}, \n                          index = X.columns)\n\nImportance.sort_values(by = 'Importance', \n                       axis = 0, \n                       ascending = True).plot(kind = 'barh', \n                                              color = '#25B89B', figsize=(10,6))\n\nplt.xlabel('LightGBM Feature Importance')\nplt.gca().legend_ = None","e888198b":"Q1 = df.Insulin.quantile(0.25)\nQ3 = df.Insulin.quantile(0.75)\nIQR = Q3-Q1\nlower = Q1 - 1.5*IQR\nupper = Q3 + 1.5*IQR\ndf.loc[df[\"Insulin\"] > upper,\"Insulin\"] = upper","5496bad3":"import seaborn as sns\nsns.boxplot(x = df[\"Insulin\"]);","7cf8763e":"# According to feature importance i'm creating new features.\ndf2 = df.copy()\n\ndf2[\"Insulin\/Age\"]=df2[\"Insulin\"]\/df2[\"Age\"]\ndf2[\"BMI\/Age\"]=df2[\"BMI\"]\/df2[\"Age\"]\ndf2[\"Pregnancies\/Age\"]=df2[\"Pregnancies\"]\/df2[\"Age\"]\ndf2[\"Ins*Glu\"]=df2[\"Insulin\"]* df2[\"Glucose\"]\ndf2.drop([\"Age\"],axis = 1, inplace = True)","4b5263fa":"df2['New_BMI'] = pd.cut(x = df['BMI'], bins = [0,18.5, 24.9, 29.9, 100], labels = [\"Underweight\", \n                                                                                  \"NormalWeight\", \n                                                                                  \"Overweight\", \n                                                                                  \"Obes\"])","2068cdb1":"df2['New_BloodPressure'] = pd.cut(x = df['BloodPressure'], bins = [0,80, 90, 120, 122], labels = [\"Normal\", \n                                                                                                \"Hyper_St1\", \n                                                                                                \"Hyper_St2\", \n                                                                                                \"Hyper_Emer\"])\n#reference: American Heart Association","8436b32c":"df2[\"New_Glucose\"] = pd.cut(x = df[\"Glucose\"], bins = [0,140,200,300], labels = [\"Normal\",\n                                                                                \"Prediabetes\",\n                                                                                \"Diabetes\"])\n#reference: https:\/\/emedicine.medscape.com\/article\/2049402-overview","97d7e9a2":"#A categorical variable creation process is performed according to the insulin value.\ndef set_insulin(row): \n    if row[\"Insulin\"] >= 100 and row[\"Insulin\"] <= 126:\n        return \"Normal\"\n    else:\n        return \"Abnormal\"","638c3ed1":"df2 = df2.assign(NewInsulinScore=df2.apply(set_insulin, axis=1))","5b6740e0":"df2.head()","b573c6d6":"def one_hot_encoder(dataframe, categorical_columns, nan_as_category=False):\n    original_columns = list(dataframe.columns)\n    dataframe = pd.get_dummies(dataframe, columns=categorical_columns,\n                               dummy_na=nan_as_category, drop_first=True)\n    new_columns = [col for col in dataframe.columns if col not in original_columns]\n    return dataframe, new_columns","60d61a6c":"categorical_columns = [col for col in df2.columns\n                           if len(df2[col].unique()) <= 10\n                      and col != \"Outcome\"]\ncategorical_columns","8dc9acbe":"df2, new_cols_ohe = one_hot_encoder(df2,categorical_columns)\nnew_cols_ohe","f3ae459c":"df2.head()","581d6dc6":"from sklearn.neighbors import LocalOutlierFactor\nlof =LocalOutlierFactor(n_neighbors= 20)\nlof.fit_predict(df2)\ndf_scores = lof.negative_outlier_factor_\nnp.sort(df_scores)[0:30]","d5f4e951":"th = np.sort(df_scores)[6]\nth","8a25b166":"#We delete those that are higher than the threshold value\ndf2 = df2[df_scores > th]\ndf2.shape","b0f3b129":"y = df2[\"Outcome\"]\nX = df2.drop([\"Outcome\"], axis = 1)","7b75a98d":"models = [('RF', RandomForestClassifier()),\n          ('GBM',GradientBoostingClassifier()),\n          ('XGBM', XGBClassifier()),\n          (\"LightGBM\", LGBMClassifier())]\n\n# evaluate each model in turn\nresults = []\nnames = []\n\nfor name, model in models:\n    kfold = KFold(n_splits=10, random_state=123456)\n    cv_results = cross_val_score(model, X, y, cv=kfold, scoring=\"accuracy\")\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)","373474bf":"# LGBM Tuned Model\n\nlgbm_tuned = LGBMClassifier(colsample_bytree = 0.5, \n                            learning_rate = 0.01,\n                            max_depth = 6,\n                            n_estimators = 500).fit(X, y)","13cf51be":"# GBM Tuned Model\n\ngbm_tuned = GradientBoostingClassifier(learning_rate = 0.1,\n                                      max_depth = 3,\n                                      n_estimators = 200,\n                                      subsample = 0.8).fit(X,y)","54fad9e3":"# XGBoost Tuned Model\n\nxgb_tuned = XGBClassifier(learning_rate = 0.01,\n                         max_depth = 3,\n                         n_estimators =1000,\n                         subsample = 1.0).fit(X,y)","8f932f2c":"# evaluate each model in turn\nmodels = [(\"GBM\", gbm_tuned),\n         (\"XGBoost\", xgb_tuned),\n         (\"LightGBM\", lgbm_tuned)]\nresults = []\nnames = []\nfor name, model in models:\n    kfold = KFold(n_splits=10, random_state=123456)\n    cv_results = cross_val_score(model, X, y, cv=10, scoring=\"accuracy\")\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)","5cbaaa6f":"feature_imp = pd.Series(lgbm_tuned.feature_importances_,\n                        index=X.columns).sort_values(ascending=False)\n\nsns.barplot(x=feature_imp, y=feature_imp.index)\nplt.xlabel('LightGBM: 0.90 Accuracy')\nplt.title(\"Feature Importance\")\nplt.show()","5a93e173":"# Checking Overfitting\nlog_model = LogisticRegression().fit(X,y)\ny_pred = log_model.predict(X)\nprint(accuracy_score(y, y_pred))\nprint(classification_report(y, y_pred))","e72064c1":"Diabetes, is a group of metabolic disorders in which there are high blood sugar levels over a prolonged period. Symptoms of high blood sugar include frequent urination, increased thirst, and increased hunger. If left untreated, diabetes can cause many complications. Acute complications can include diabetic ketoacidosis, hyperosmolar hyperglycemic state, or death. Serious long-term complications include cardiovascular disease, stroke, chronic kidney disease, foot ulcers, and damage to the eyes.\n\nThis dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. \n\nThe objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are **females at least 21 years old of Pima Indian heritage.**\n\n--------------------","14db0861":"**Details about the dataset:**\nThe datasets consists of several medical predictor variables and one target variable, Outcome. Predictor variables includes the number of pregnancies the patient has had, their BMI, insulin level, age, and so on.\n\n**Pregnancies:** Number of times pregnant  \n**Glucose:** Plasma glucose concentration a 2 hours in an oral glucose tolerance test  \n**BloodPressure:** Diastolic blood pressure (mm Hg)  \n**SkinThickness:** Triceps skin fold thickness (mm)  \n**Insulin:** 2-Hour serum insulin (mu U\/ml)  \n**BMI:** Body mass index (weight in kg\/(height in m)^2)  \n**DiabetesPedigreeFunction:** Diabetes pedigree function  \n**Age:** Age (years)  \n**Outcome:** Class variable ( 0 - 1)\n\n-----------\n\n**Number of Observation Units:** 768  \n**Variable Number:** 9\n\nResult; The model with the highest score after hyper parameter optimization was LGBM with 0.90 cross validation score.","5497bbfa":"## Outliers","704684ab":"## Edit Features","a5691fdd":"----------------\n# &#128270; LOF","2b3a661e":"## &#128202; Base Model: LightGBM Feature Importance","f8449c99":"<center><h1><strong>Pima Indians Diabetes Classification Project<\/strong><\/h1>\n<img\nsrc=\"https:\/\/cdn.britannica.com\/s:700x500\/42\/93542-050-E2B32DAB\/women-Pima-shinny-game-field-hockey.jpg\">\n<\/center>","8edac808":"<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n\n<ul>\n<li><strong>pregnancies<\/strong> mean &gt; median, <strong>right skewed<\/strong> | <strong>99% quartile value:<\/strong> 13, <strong>max value:<\/strong> 17 <strong>might be outlier<\/strong><\/li>\n<li><strong>glucose<\/strong> mean &gt; median, <strong>right skewed<\/strong> | <strong>%99<\/strong> and <strong>max<\/strong> values close each other<\/li>\n<li><strong>bloodpressure<\/strong> mean = median nearly, looks <strong>gaussian distribution<\/strong>. There is a 16 units difference between <strong>%99<\/strong> and <strong>max value. might be outlier<\/strong><\/li>\n<li><strong>skinthickness<\/strong> mean &gt; median, <strong>right skewed<\/strong> | <strong>min value:<\/strong> 0 | <strong>99% quartile value:<\/strong> 51 <strong>max value:<\/strong> 99 <strong>there is outlier<\/strong><\/li>\n<li><strong>insulin<\/strong> mean(79) &gt; median(30), <strong>right skewed<\/strong> | <strong>min value:<\/strong> 0, <strong>99% quartile value:<\/strong> 519 <strong>max value:<\/strong> 846<\/li>\n<li><strong>BMI<\/strong> mean(31) =~ median(32), looks <strong>gaussian distribution<\/strong> | <strong>min value:<\/strong> 0 | <strong>99% quartile value:<\/strong> 50, <strong>max value:<\/strong> 67<\/li>\n<li><strong>DPF<\/strong> mean(0.47) &gt; median (0.37) | <strong>min value:<\/strong> 0.078 | <strong>99% quartile value:<\/strong> 1.69 <strong>max value:<\/strong> 2.42<\/li>\n<li><strong>Age<\/strong> mean(33)&gt;median(29) <strong>right skewed<\/strong> | <strong>min value:<\/strong> 21 | <strong>99% quartile value:<\/strong> 67 <strong>max value:<\/strong> 2.42<\/li>\n\n<\/ul>\n\n<\/div>","8f5ad765":"## New Features","eddea2da":"----------------------------\n# &#9812;Final Model | LightGBM: 0.90","3fbd4798":"-------------------\n# &#9881;Model Tuning Steps","448a356d":"### - Blood Pressure Levels","1640e1cb":"## &#128270; Descriptive Statistics","1eb4e4e3":"<div class=\"alert alert-warning\" role=\"alert\" style=\"margin-top: 20px\">\n\n<h1>REPORT<\/h1>\n\n<p><strong>The aim of this study<\/strong> was to create classification models for the diabetes data set and to predict whether a person is sick by establishing models and to obtain maximum validation scores in the established models. Here the steps;<\/p>\n<p><strong>Exploratory Data Analysis:<\/strong>  The data set&#39;s structural data were checked. The types of variables in the dataset were examined. Size information of the dataset was accessed. The 0 values in the data set are missing values. Primarily these 0 values were replaced with NaN values. Descriptive statistics of the data set were examined.<\/p>\n<p><strong>Data Preprocessing section;<\/strong> The NaN values missing observations were filled with the median values of whether each variable was diabetic or not. The outliers were determined by LOF and dropped. <\/p>\n<p><strong>In model building;<\/strong> first, the base model was create and the test results were checked. Then categorical variables were edited and new features were added to the model.<\/p>\n<p><strong>During Model Building;<\/strong> Logistic Regression, KNN, CART, Random Forests, GBM, XGBoost, LightGBM like using machine learning models Cross Validation Score were calculated. <\/p>\n<p><strong>According to test results;<\/strong> GBM, XGBoost, LightGBM hyperparameter optimizations optimized to increase Cross Validation value.<\/p>\n<p><strong>The model with the highest score after Hyper Parameter optimization was LGBM with 0.90 cross validation score<\/strong><\/p>\n\n\n\n<\/div>","c70d5142":"## &#9850; Filling NA Values","4f8fe9b8":"------------\n# &#8690; One Hot Encoding","fb86c88e":"# &#127919; Objective of Kernel","d70eb82c":"<center><img\nsrc=\"https:\/\/www.oguzerdogan.com\/wp-content\/uploads\/2020\/10\/glucose.jpg\">\n<\/center>","e2531f2e":"-------------------\n# &#9823; Base Models | LightGBM","0925accd":"# &#128270; Missing Values","08334630":"### - Glucose Levels","e361eae0":"----------\n# &#9822; New Features Model | LightGBM","deb6338c":"<div class=\"alert alert-danger\" role=\"alert\">\n    In this data set <b>NA<\/b> are filled with 0\n<\/div>","8c41656c":"# &#128217; Load Libraries","aa34385b":"## Histogram","b1f0ca60":"<center><img\nsrc=\"https:\/\/www.oguzerdogan.com\/wp-content\/uploads\/2020\/10\/BMI.jpg\">\n<\/center>","2f9c4f5f":"## Correlation Between Features","924232bb":"-----------------","158dcfc9":"## &#9703; Categorical & Numerical Columns","97d42b34":"<a href=\"https:\/\/www.oguzerdogan.com\/\">\n    <img src=\"https:\/\/www.oguzerdogan.com\/wp-content\/uploads\/2020\/10\/logo_oz.png\" width=\"200\" align=\"right\">\n<\/a>","f00a181f":"## &#128918; Target Variable Counts","5d390a8b":"# &#128202; Exploratory Data Analysis","7cdf1e53":"--------------------------\n# &#128296; Feature Engineering\n","a0c4d812":"# &#128214; Read Data","8cd1b04d":"### - Insulin Levels","0c24f3b4":"<center><img\nsrc=\"https:\/\/www.oguzerdogan.com\/wp-content\/uploads\/2020\/10\/blood.jpg\">\n<\/center>\n\n","dfbeb3e1":"### - BMI Levels"}}