{"cell_type":{"cf1b3680":"code","9e970e26":"code","e9043fc0":"code","24a1d5fe":"code","435f2ebe":"code","237b96d0":"code","3875f102":"code","57602a7a":"code","7d12b257":"code","5dbe548a":"code","e941b332":"code","fd653ff7":"code","1634ec57":"code","192b8d69":"code","8face4d0":"code","29d9eb83":"code","037f15fd":"code","267113be":"code","0ce2f76b":"code","15184169":"code","cfd44684":"code","405eb3f0":"code","f20ed0b7":"code","5c03c3d5":"code","30bc9ce8":"code","999914bb":"code","6d8a617e":"code","cbfad5dd":"code","251f9a49":"code","e22c2023":"code","db52e321":"code","97136c83":"code","25906731":"code","5d5b6bbc":"code","e78dbc26":"code","d7b5322e":"code","e48730c6":"code","2b1df6af":"code","1dfe6f63":"code","815e7296":"code","3a58b3b4":"code","a81f8f76":"code","1c1567dd":"code","75524894":"code","7981112d":"code","bb79ec4d":"code","0a3c2ad6":"code","45c20894":"code","c59db22f":"code","656eb582":"code","3fa37824":"code","c8e6ff50":"markdown","6e4a31b8":"markdown","42e79d5b":"markdown","c7802575":"markdown","56680e26":"markdown","6787f029":"markdown","a114d99e":"markdown","ca2b262d":"markdown","af6a84d7":"markdown","c4baf2ab":"markdown","9c07252e":"markdown","2c572cdf":"markdown","6de1a4f2":"markdown","caf220d0":"markdown","755450ac":"markdown","2b781347":"markdown","3b63f669":"markdown","baf85c5d":"markdown"},"source":{"cf1b3680":"import pandas as pd\nimport numpy as np\nimport os\n\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nimport itertools\nimport shutil\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom numpy.random import seed\nseed(101)\nfrom tensorflow import set_random_seed\nset_random_seed(101)\n\n\nimport tensorflow\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.metrics import categorical_crossentropy\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint","9e970e26":"os.listdir('..\/input')","e9043fc0":"# Create a new directory fro new img\nbase_dir = 'base_dir'\nos.mkdir(base_dir)\n# now we create 7 folders inside (BASE DIRECTORY)'base_dir': create a path to 'base_dir' to which we will join the names of the new folders\n\n# train_dir\ntrain_dir = os.path.join(base_dir, 'train_dir')\nos.mkdir(train_dir)\n\n# val_dir\nval_dir = os.path.join(base_dir, 'val_dir')\nos.mkdir(val_dir)\n\n\n\n# create new folders inside train_dir\nnv = os.path.join(train_dir, 'nv')\nos.mkdir(nv)\nmel = os.path.join(train_dir, 'mel')\nos.mkdir(mel)\nbkl = os.path.join(train_dir, 'bkl')\nos.mkdir(bkl)\nbcc = os.path.join(train_dir, 'bcc')\nos.mkdir(bcc)\nakiec = os.path.join(train_dir, 'akiec')\nos.mkdir(akiec)\nvasc = os.path.join(train_dir, 'vasc')\nos.mkdir(vasc)\ndf = os.path.join(train_dir, 'df')\nos.mkdir(df)\n\n\n\n# create new folders inside val_dir\nnv = os.path.join(val_dir, 'nv')\nos.mkdir(nv)\nmel = os.path.join(val_dir, 'mel')\nos.mkdir(mel)\nbkl = os.path.join(val_dir, 'bkl')\nos.mkdir(bkl)\nbcc = os.path.join(val_dir, 'bcc')\nos.mkdir(bcc)\nakiec = os.path.join(val_dir, 'akiec')\nos.mkdir(akiec)\nvasc = os.path.join(val_dir, 'vasc')\nos.mkdir(vasc)\ndf = os.path.join(val_dir, 'df')\nos.mkdir(df)\n\n","24a1d5fe":"df_data = pd.read_csv('..\/input\/HAM10000_metadata.csv')\ndf_data.head()","435f2ebe":"df = df_data.groupby('lesion_id').count()\ndf = df[df['image_id'] == 1]\ndf.reset_index(inplace=True)\n\ndf.head()","237b96d0":"def identify_duplicates(x):\n    \n    unique_list = list(df['lesion_id'])\n    \n    if x in unique_list:\n        return 'no_duplicates'\n    else:\n        return 'has_duplicates'\n\ndf_data['duplicates'] = df_data['lesion_id']\n\ndf_data['duplicates'] = df_data['duplicates'].apply(identify_duplicates)\n\ndf_data.head()","3875f102":"df_data['duplicates'].value_counts()","57602a7a":"df = df_data[df_data['duplicates'] == 'no_duplicates']\ndf.shape","7d12b257":"y = df['dx']\n_, df_val = train_test_split(df, test_size=0.17, random_state=101, stratify=y)\ndf_val.shape","5dbe548a":"df_val['dx'].value_counts()","e941b332":"\ndef identify_val_rows(x):\n    # create a list of all the lesion_id's in the val set\n    val_list = list(df_val['image_id'])\n    \n    if str(x) in val_list:\n        return 'val'\n    else:\n        return 'train'\n\n\ndf_data['train_or_val'] = df_data['image_id']\ndf_data['train_or_val'] = df_data['train_or_val'].apply(identify_val_rows)\ndf_train = df_data[df_data['train_or_val'] == 'train']\n\n\nprint(len(df_train))\nprint(len(df_val))","fd653ff7":"df_train['dx'].value_counts()","1634ec57":"df_val['dx'].value_counts()","192b8d69":"# Set the image_id as the index in df_data\ndf_data.set_index('image_id', inplace=True)","8face4d0":"\nfolder_1 = os.listdir('..\/input\/ham10000_images_part_1')\nfolder_2 = os.listdir('..\/input\/ham10000_images_part_2')\n\ntrain_list = list(df_train['image_id'])\nval_list = list(df_val['image_id'])\n\nfor image in train_list:\n    \n    fname = image + '.jpg'\n    label = df_data.loc[image,'dx']\n    \n    if fname in folder_1:\n       \n        src = os.path.join('..\/input\/ham10000_images_part_1', fname)\n        dst = os.path.join(train_dir, label, fname)\n        shutil.copyfile(src, dst)\n\n    if fname in folder_2:\n        \n        src = os.path.join('..\/input\/ham10000_images_part_2', fname)\n        dst = os.path.join(train_dir, label, fname)\n        shutil.copyfile(src, dst)\nfor image in val_list:\n    \n    fname = image + '.jpg'\n    label = df_data.loc[image,'dx']\n    \n    if fname in folder_1:\n       \n        src = os.path.join('..\/input\/ham10000_images_part_1', fname)\n        dst = os.path.join(val_dir, label, fname)\n        shutil.copyfile(src, dst)\n\n    if fname in folder_2:\n        src = os.path.join('..\/input\/ham10000_images_part_2', fname)\n        dst = os.path.join(val_dir, label, fname)\n        shutil.copyfile(src, dst)\n        ","29d9eb83":"print(len(os.listdir('base_dir\/train_dir\/nv')))\nprint(len(os.listdir('base_dir\/train_dir\/mel')))\nprint(len(os.listdir('base_dir\/train_dir\/bkl')))\nprint(len(os.listdir('base_dir\/train_dir\/bcc')))\nprint(len(os.listdir('base_dir\/train_dir\/akiec')))\nprint(len(os.listdir('base_dir\/train_dir\/vasc')))\nprint(len(os.listdir('base_dir\/train_dir\/df')))","037f15fd":"print(len(os.listdir('base_dir\/val_dir\/nv')))\nprint(len(os.listdir('base_dir\/val_dir\/mel')))\nprint(len(os.listdir('base_dir\/val_dir\/bkl')))\nprint(len(os.listdir('base_dir\/val_dir\/bcc')))\nprint(len(os.listdir('base_dir\/val_dir\/akiec')))\nprint(len(os.listdir('base_dir\/val_dir\/vasc')))\nprint(len(os.listdir('base_dir\/val_dir\/df')))","267113be":"class_list = ['mel','bkl','bcc','akiec','vasc','df']\nfor item in class_list:\n    aug_dir = 'aug_dir'\n    os.mkdir(aug_dir)\n    img_dir = os.path.join(aug_dir, 'img_dir')\n    os.mkdir(img_dir)\n    img_class = item\n    img_list = os.listdir('base_dir\/train_dir\/' + img_class)\n    for fname in img_list:\n            \n            src = os.path.join('base_dir\/train_dir\/' + img_class, fname)\n            dst = os.path.join(img_dir, fname)\n            shutil.copyfile(src, dst)\n\n\n    \n    path = aug_dir\n    save_path = 'base_dir\/train_dir\/' + img_class\n\n    # Create a data generator\n    datagen = ImageDataGenerator(\n        rotation_range=180,\n        width_shift_range=0.1,\n        height_shift_range=0.1,\n        zoom_range=0.1,\n        horizontal_flip=True,\n        vertical_flip=True,\n        #brightness_range=(0.9,1.1),\n        fill_mode='nearest')\n\n    batch_size = 50\n\n    aug_datagen = datagen.flow_from_directory(path,\n                                           save_to_dir=save_path,\n                                           save_format='jpg',\n                                                    target_size=(224,224),\n                                                    batch_size=batch_size)\n\n\n\n    \n    \n    num_aug_images_wanted = 6000 # total number of images we want to have in each class    \n    num_files = len(os.listdir(img_dir))\n    num_batches = int(np.ceil((num_aug_images_wanted-num_files)\/batch_size))\n\n    for i in range(0,num_batches):\n\n        imgs, labels = next(aug_datagen)\n    shutil.rmtree('aug_dir')","0ce2f76b":"print(len(os.listdir('base_dir\/train_dir\/nv')))\nprint(len(os.listdir('base_dir\/train_dir\/mel')))\nprint(len(os.listdir('base_dir\/train_dir\/bkl')))\nprint(len(os.listdir('base_dir\/train_dir\/bcc')))\nprint(len(os.listdir('base_dir\/train_dir\/akiec')))\nprint(len(os.listdir('base_dir\/train_dir\/vasc')))\nprint(len(os.listdir('base_dir\/train_dir\/df')))","15184169":"print(len(os.listdir('base_dir\/val_dir\/nv')))\nprint(len(os.listdir('base_dir\/val_dir\/mel')))\nprint(len(os.listdir('base_dir\/val_dir\/bkl')))\nprint(len(os.listdir('base_dir\/val_dir\/bcc')))\nprint(len(os.listdir('base_dir\/val_dir\/akiec')))\nprint(len(os.listdir('base_dir\/val_dir\/vasc')))\nprint(len(os.listdir('base_dir\/val_dir\/df')))","cfd44684":"# plots images with labels within jupyter notebook\n# source: https:\/\/github.com\/smileservices\/keras_utils\/blob\/master\/utils.py\n\ndef plots(ims, figsize=(12,6), rows=5, interp=False, titles=None): # 12,6\n    if type(ims[0]) is np.ndarray:\n        ims = np.array(ims).astype(np.uint8)\n        if (ims.shape[-1] != 3):\n            ims = ims.transpose((0,2,3,1))\n    f = plt.figure(figsize=figsize)\n    cols = len(ims)\/\/rows if len(ims) % 2 == 0 else len(ims)\/\/rows + 1\n    for i in range(len(ims)):\n        sp = f.add_subplot(rows, cols, i+1)\n        sp.axis('Off')\n        if titles is not None:\n            sp.set_title(titles[i], fontsize=16)\n        plt.imshow(ims[i], interpolation=None if interp else 'none')\n        \nplots(imgs, titles=None) # titles=labels will display the image labels","405eb3f0":"# End of Data Preparation\n### ===================================================================================== ###\n# Start of Model Building","f20ed0b7":"train_path = 'base_dir\/train_dir'\nvalid_path = 'base_dir\/val_dir'\n\nnum_train_samples = len(df_train)\nnum_val_samples = len(df_val)\ntrain_batch_size = 10\nval_batch_size = 10\nimage_size = 224\n\ntrain_steps = np.ceil(num_train_samples \/ train_batch_size)\nval_steps = np.ceil(num_val_samples \/ val_batch_size)\n","5c03c3d5":"\ndatagen = ImageDataGenerator(\n    preprocessing_function= \\\n    tensorflow.keras.applications.mobilenet.preprocess_input)\n\ntrain_batches = datagen.flow_from_directory(train_path,\n                                            target_size=(image_size,image_size),\n                                            batch_size=train_batch_size)\n\nvalid_batches = datagen.flow_from_directory(valid_path,\n                                            target_size=(image_size,image_size),\n                                            batch_size=val_batch_size)\n\n# Note: shuffle=False causes the test dataset to not be shuffled\ntest_batches = datagen.flow_from_directory(valid_path,\n                                            target_size=(image_size,image_size),\n                                            batch_size=1,\n                                            shuffle=False)","30bc9ce8":"mobile = tensorflow.keras.applications.mobilenet.MobileNet()","999914bb":"mobile.summary()","6d8a617e":"type(mobile.layers)","cbfad5dd":"# How many layers does MobileNet have?\nlen(mobile.layers)","251f9a49":"# Exclude the last 5 layers of the above model This will include all layers up to and including global_average_pooling2d_1\nx = mobile.layers[-6].output\nx = Dropout(0.25)(x)\npredictions = Dense(7, activation='softmax')(x)\n\nmodel = Model(inputs=mobile.input, outputs=predictions)","e22c2023":"model.summary()","db52e321":"for layer in model.layers[:-23]:\n    layer.trainable = False","97136c83":"# Define Top2 and Top3 Accuracy\nfrom tensorflow.keras.metrics import categorical_accuracy, top_k_categorical_accuracy\n\ndef top_3_accuracy(y_true, y_pred):\n    return top_k_categorical_accuracy(y_true, y_pred, k=3)\n\ndef top_2_accuracy(y_true, y_pred):\n    return top_k_categorical_accuracy(y_true, y_pred, k=2)","25906731":"model.compile(Adam(lr=0.01), loss='categorical_crossentropy', \n              metrics=[categorical_accuracy, top_2_accuracy, top_3_accuracy])","5d5b6bbc":"# Get the labels that are associated with each index\nprint(valid_batches.class_indices)","e78dbc26":"# Add weights to try to make the model more sensitive to melanoma\n# To solve Imbalancedd data problem\n\nclass_weights={\n    0: 1.0, # akiec\n    1: 1.0, # bcc\n    2: 1.0, # bkl\n    3: 1.0, # df\n    4: 3.0, # mel \n    5: 1.0, # nv\n    6: 1.0, # vasc\n}","d7b5322e":"\nfilepath = \"model.h5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='val_top_3_accuracy', verbose=1, \n                             save_best_only=True, mode='max')\n\nreduce_lr = ReduceLROnPlateau(monitor='val_top_3_accuracy', factor=0.5, patience=2, \n                                   verbose=1, mode='max', min_lr=0.00001)\n                              \n                              \ncallbacks_list = [checkpoint, reduce_lr]\n\nhistory = model.fit_generator(train_batches, steps_per_epoch=train_steps, \n                              class_weight=class_weights,\n                    validation_data=valid_batches,\n                    validation_steps=val_steps,\n                    epochs=30, verbose=1,\n                   callbacks=callbacks_list)\n","e48730c6":"model.metrics_names","2b1df6af":"# we use last epoch \n\nval_loss, val_cat_acc, val_top_2_acc, val_top_3_acc = \\\nmodel.evaluate_generator(test_batches, \n                        steps=len(df_val))\n\nprint('val_loss:', val_loss)\nprint('val_cat_acc:', val_cat_acc)\nprint('val_top_2_acc:', val_top_2_acc)\nprint('val_top_3_acc:', val_top_3_acc)","1dfe6f63":"model.load_weights('model.h5')\n\nval_loss, val_cat_acc, val_top_2_acc, val_top_3_acc = \\\nmodel.evaluate_generator(test_batches, \n                        steps=len(df_val))\n\nprint('val_loss:', val_loss)\nprint('val_cat_acc:', val_cat_acc)\nprint('val_top_2_acc:', val_top_2_acc)\nprint('val_top_3_acc:', val_top_3_acc)","815e7296":"\nimport matplotlib.pyplot as plt\n\nacc = history.history['categorical_accuracy']\nval_acc = history.history['val_categorical_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\ntrain_top2_acc = history.history['top_2_accuracy']\nval_top2_acc = history.history['val_top_2_accuracy']\ntrain_top3_acc = history.history['top_3_accuracy']\nval_top3_acc = history.history['val_top_3_accuracy']\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.figure()\n\nplt.plot(epochs, acc, 'bo', label='Training cat acc')\nplt.plot(epochs, val_acc, 'b', label='Validation cat acc')\nplt.title('Training and validation cat accuracy')\nplt.legend()\nplt.figure()\n\n\nplt.plot(epochs, train_top2_acc, 'bo', label='Training top2 acc')\nplt.plot(epochs, val_top2_acc, 'b', label='Validation top2 acc')\nplt.title('Training and validation top2 accuracy')\nplt.legend()\nplt.figure()\nplt.plot(epochs, train_top3_acc, 'bo', label='Training top3 acc')\nplt.plot(epochs, val_top3_acc, 'b', label='Validation top3 acc')\nplt.title('Training and validation top3 accuracy')\nplt.legend()\n\n\nplt.show()","3a58b3b4":"# Get the labels of the test images.\n\ntest_labels = test_batches.classes","a81f8f76":"# We need these to plot the confusion matrix.\ntest_labels","1c1567dd":"# Print the label associated with each class\ntest_batches.class_indices","75524894":"predictions = model.predict_generator(test_batches, steps=len(df_val), verbose=1)","7981112d":"predictions.shape","bb79ec4d":"def plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()","0a3c2ad6":"# argmax returns the index of the max value in a row\ncm = confusion_matrix(test_labels, predictions.argmax(axis=1))","45c20894":"cm_plot_labels = ['akiec', 'bcc', 'bkl', 'df', 'mel','nv', 'vasc']\nplot_confusion_matrix(cm, cm_plot_labels, title='Confusion Matrix')","c59db22f":"y_pred = np.argmax(predictions, axis=1)\ny_true = test_batches.classes","656eb582":"from sklearn.metrics import classification_report\nreport = classification_report(y_true, y_pred, target_names=cm_plot_labels)\n\nprint(report)","3fa37824":"shutil.rmtree('base_dir')","c8e6ff50":"### Generate the Final Report","6e4a31b8":"**LABELS**<br>\n\nExcerpts from the paper:<br>\n The HAM10000 Dataset: A Large Collection of Multi-Source Dermatoscopic Images of Common Pigmented Skin Lesions<br>\nhttps:\/\/arxiv.org\/abs\/1803.10417\n\n **nv**\n *[6705 images]*\n **mel**\n *[1113 images]* \n**bkl**\n*[1099 images]*\n**bcc**\n*[514 images]*\n**akiec**\n*[327 images]*\n**vasc**\n*[142 images]*\n**df**\n*[115 images]*\n<br>*[Total images = 10015]*","42e79d5b":"### Modify MobileNet Model","c7802575":"### Transfer the Images into the Folders","56680e26":"### Set Up the Generators","6787f029":"### Train the Model","a114d99e":"### Create Train and Val Sets","ca2b262d":"You Can Dowenload Data From https:\/\/www.kaggle.com\/kmader\/skin-cancer-mnist-ham10000 this will be 3GB","af6a84d7":"### Visualize 50 augmented images","c4baf2ab":"### Create a stratified val set","9c07252e":"### Plot the Training Curves display the loss and accuracy curves","2c572cdf":"## Many Thanks to ITI Staff and Instructors Track AI Intake 1  \ud83d\udda4","6de1a4f2":"### Copy the train images  into aug_dir","caf220d0":"### Create the directory structure\n\nIn these folders we will store the images that will later be fed to the Keras generators. ","755450ac":"## Delete the image data directory we created to Avoid Errors","2b781347":"### Evaluate the model using the val set","3b63f669":"### Create a train set that excludes images that are in the val set","baf85c5d":"### Create a Confusion Matrix"}}