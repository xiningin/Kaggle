{"cell_type":{"565fcfcc":"code","d35b8b3e":"code","000a460c":"code","0a28d373":"code","3981466c":"code","9030ff40":"code","634d35e9":"code","a3a36619":"code","b5d35890":"code","8fb36209":"markdown","ebd04faf":"markdown","d720db2b":"markdown","5084fcb4":"markdown","0466d2d7":"markdown","6988d0f7":"markdown","80a8795d":"markdown","7c6404c0":"markdown","f0ea8a6d":"markdown","259f1f28":"markdown"},"source":{"565fcfcc":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd","d35b8b3e":"dataset = pd.read_csv('..\/input\/wine-data\/Wine.csv')\nX = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, -1].values\n","000a460c":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","0a28d373":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","3981466c":"from sklearn.decomposition import PCA\npca = PCA(n_components = 2)\nX_train = pca.fit_transform(X_train)\nX_test = pca.transform(X_test)","9030ff40":"from sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression(random_state = 0)\nclassifier.fit(X_train, y_train)","634d35e9":"from sklearn.metrics import confusion_matrix, accuracy_score\ny_pred = classifier.predict(X_test)\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\naccuracy_score(y_test, y_pred)","a3a36619":"from matplotlib.colors import ListedColormap\nX_set, y_set = X_train, y_train\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\nplt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n             alpha = 0.75, cmap = ListedColormap(('red', 'green', 'blue')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n                c = ListedColormap(('red', 'green', 'blue'))(i), label = j)\nplt.title('Logistic Regression (Training set)')\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.legend()\nplt.show()","b5d35890":"from matplotlib.colors import ListedColormap\nX_set, y_set = X_test, y_test\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\nplt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n             alpha = 0.75, cmap = ListedColormap(('red', 'green', 'blue')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n                c = ListedColormap(('red', 'green', 'blue'))(i), label = j)\nplt.title('Logistic Regression (Test set)')\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.legend()\nplt.show()","8fb36209":"**> The main goal of a PCA analysis is to identify patterns in data; PCA aims to detect the correlation between variables. If a strong correlation between variables exists, the attempt to reduce the dimensionality only makes sense.**","ebd04faf":"# **Applying PCA**","d720db2b":"# **Visualising the Test set results\ud83d\udcca**","5084fcb4":"# **Visualising the Training set results\ud83e\udda5**","0466d2d7":"# **Importing the dataset**","6988d0f7":"# **Making the Confusion Matrix\u2618\ufe0f**","80a8795d":"# **Splitting the dataset into the Training set and Test set**","7c6404c0":"# **Feature Scaling**","f0ea8a6d":"# **Training the Logistic Regression model on the Training set**","259f1f28":"# **Importing the libraries**"}}