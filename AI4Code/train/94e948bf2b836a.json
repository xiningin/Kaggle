{"cell_type":{"6d46d543":"code","94671981":"code","b00b0832":"code","5097f5fe":"code","78c3a149":"code","dc875e38":"code","9933c4c7":"code","49ee6be1":"code","d85c0acf":"code","efab5f2a":"code","1f525e27":"code","75f911e4":"code","e555ab31":"code","6c3d5c4d":"markdown","0fe7df2b":"markdown","8345aa29":"markdown","3e78aa14":"markdown","41c133df":"markdown","9d3f7012":"markdown","1b016bf7":"markdown","b61cdeb7":"markdown","82c0cf65":"markdown","ac9f1a54":"markdown","3c1a92d9":"markdown","ede19728":"markdown"},"source":{"6d46d543":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score","94671981":"data = pd.read_csv('\/kaggle\/input\/pima-indians-diabetes-database\/diabetes.csv')  # loading the pima indians diabetes dataset\n\nprint(\"Shape of the dataset: \" + str(data.shape)) # checking the shape of the dataset \nprint(\"\\n \\nThe first few rows of the dataset: \") # checking the first few rows of the dataset\ndata.head()","b00b0832":"n_missing = data.isnull().sum().sum()       # checking if there is any missing value\nif n_missing == 0: \n    print(\"There  is no missing values in the dataset\")\nelse:\n    print(\"Oops! Total \" + str(n_missing) + \" values in the dataset\")\n","5097f5fe":"dataset = data.copy() # creating a copy of the dataset, so any further change won't effect the main dataset\n\nfor col in dataset.columns[0:-1]: # now, we are normalizing the dataset along columns\n    dataset[col] = dataset[col]\/abs(dataset[col].max())\ndataset.head()","78c3a149":"train, test = train_test_split(dataset, test_size=0.3, random_state=42, shuffle=True) # splitting into train and test set","dc875e38":"# defining \"makeinput\" function which will extract the features(x) and outcome(y) as np arrays from the dataframe\ndef makeinput(df_in):\n    df = df_in.copy()\n    x = df.iloc[:, :-1].values\n    y = df.iloc[:, -1]. values\n    y = y.reshape(y.shape[0], 1)      # reshape for avoiding broadcasting\n    x0 = np.ones(x.shape[0]).reshape(x.shape[0], 1)\n    x = np.append(x0, x, axis=1)\n    return x, y","9933c4c7":"# defining \"sigmoid\" function\ndef sigmoid(x):\n    return 1\/(1 + np.exp(-x))\n\n# checking the output of \"sigmoid\" function\na = np.linspace(-5, 5, 200)\nb = sigmoid(a)\nplt.plot(a, b)\nplt.xlabel(\"x\")\nplt.ylabel(\"Sigmoid Function\")\nplt.show()    ","49ee6be1":"# defining \"hyp\" function and it will return the hypothesis \ndef hyp(theta, x):\n    return np.matmul(x, theta)","d85c0acf":"# defining \"cost\" function, it will return the loss \ndef cost(theta, x, y):\n    m = x.shape[0]\n    h = hyp(theta, x)\n    J = -1\/(m) * np.sum(y*np.log(sigmoid(h)) + (1-y)*np.log(1-sigmoid(h)))\n    return J","efab5f2a":"# defining the optimizer\n\ndef optim(theta, x, y, alpha, epochs):\n    m = x.shape[0]\n    j = np.zeros(epochs)\n    for i in range(epochs):\n        h = hyp(theta,x)\n        gd = (1\/m)*np.matmul(np.transpose(x), (sigmoid(h)-y))\n        theta = theta - alpha*gd \n        j[i] = cost(theta, x, y)\n    return theta, j","1f525e27":"# \"pred\" function will predict the hyopothesis for test set\ndef pred(theta, x): \n    h = hyp(theta, x)\n    return sigmoid(h)","75f911e4":"# Initialization of coefficient\nx_train, y_train = makeinput(train)\nm, n = x_train.shape\ntheta_init = np.zeros((n, 1))  # initializing theta\n\n# Checking the loss without optimization\nloss = cost(theta_init, x_train, y_train)\nprint(\"Loss without optimization is \" + str(loss))\n\n# Optimization with GD\n\nalpha = 0.01        # learning rate\nepochs = 50000       # no of iterations to run the loop\ntheta, j = optim(theta_init, x_train, y_train, alpha, epochs)\nloss_opt = cost(theta, x_train, y_train)\n\nprint(\"Loss after optimization is \" + str(loss_opt))\niteration = range(epochs)\nplt.plot(iteration, j)\nplt.xlabel('epochs')\nplt.ylabel('Loss')","e555ab31":"x_test, y_test = makeinput(test)\npredictions = np.round(pred(theta, x_test))\n\nprint(accuracy_score(y_test, predictions))","6c3d5c4d":"In this notebook, I've implemented end-to-end binary classification using logistic regression. Here, from scratch, we've written all the required functions like:\n\n* sigmoid()\n* cost()\n* hyp()\n* optim()\n* pred()\n\nHere, our goal is not to obtain a high accuracy, rather we'll just implement the cost and optimizer function and see if they works. We've used the pima indiian diabetes datat.\n\n***Give an upvote, if you find the notebook helpful***","0fe7df2b":"![image.png](attachment:c33a191e-6572-4e4b-a160-73fe85e7611d.png)![image.png](attachment:a93bdb90-8f0f-4408-9a21-71e633f054bb.png)","8345aa29":"# Data Collection","3e78aa14":"# Importing Required Libraries","41c133df":"# Evaluation","9d3f7012":"![image.png](attachment:9c42c7d9-d313-4884-8c9c-ef8af47d4710.png)","1b016bf7":"# Data Analysis","b61cdeb7":"# Tutorial: Logistic Regression With Python From Scratch","82c0cf65":"Here is the equations for calculating loss:\n\n![image.png](attachment:1f4c9cf2-effe-4d0c-9020-70c8f93f6f6a.png)","ac9f1a54":"**Please give your opinion in the comment section**","3c1a92d9":"# Training","ede19728":"# Defining Important Functions"}}