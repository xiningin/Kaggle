{"cell_type":{"99bb63ec":"code","adb063a1":"code","c2f2b45a":"code","9257ee2c":"code","59595a2f":"code","8b6727bd":"code","25677901":"code","d486df95":"code","8c956574":"code","010df3e6":"code","9178a921":"code","5b0d8ed2":"code","f1ce1dae":"code","8aa0f443":"code","983d7b45":"code","c389a50f":"code","ae332dae":"code","215f83ca":"code","5cd315b6":"code","064ede5c":"code","885e53f9":"code","1304d81c":"code","d6e5b430":"code","34ca7ee7":"code","0034e214":"code","f8d0a0b5":"code","283d2768":"markdown","93c5b27a":"markdown","a646d930":"markdown","b0152067":"markdown","23162818":"markdown","7af50dc3":"markdown","687aa11a":"markdown","43fc3249":"markdown","d769073d":"markdown","b321e885":"markdown","bed7f30a":"markdown","ff4a2422":"markdown","32b71cb4":"markdown"},"source":{"99bb63ec":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\npd.options.display.max_columns = 9999 #Making sure all columns appear\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","adb063a1":"FILEPATH_TRAIN = '\/kaggle\/input\/santander-value-prediction-challenge\/train.csv'\nFILEPATH_TEST = '\/kaggle\/input\/santander-value-prediction-challenge\/test.csv'","c2f2b45a":"train = pd.read_csv(FILEPATH_TRAIN, sep=',', engine='c') #Specify sep when using C engine\ntest = pd.read_csv(FILEPATH_TEST, sep=',', engine='c')\nprint(\"Shape of train:\",train.shape, '\\n',\"Shape of test:\",test.shape)","9257ee2c":"train.head()","59595a2f":"train.describe()","8b6727bd":"import matplotlib.pyplot as plt\nimport seaborn as sns","25677901":"plt.figure(figsize=(9, 7))\nplt.scatter(range(train.shape[0]), np.sort(train['target'].values))\nplt.xlabel('Index', fontsize=14)\nplt.ylabel('Target', fontsize=14)\nplt.title(\"Target Distribution\", fontsize=14)\nplt.show()","d486df95":"plt.figure(figsize=(10,8))\nsns.distplot(train['target'].values, bins=50, kde=False)\nplt.xlabel('Target', fontsize=11)\nplt.title('Target Histogram', fontsize=11)\nplt.show()","8c956574":"plt.figure(figsize=(10,8))\nsns.distplot(np.log1p(train['target'].values), bins=50, kde=False)\nplt.xlabel('Target', fontsize=11)\nplt.title('Target Histogram', fontsize=11)\nplt.show()","010df3e6":"train.isnull().sum()","9178a921":"train.isnull().sum().any()","5b0d8ed2":"unique_vals = train.nunique().reset_index() #This drops NaN values by default\nunique_vals.columns = [\"Name\", \"Uniqueness\"]\nconst_d = unique_vals[unique_vals[\"Uniqueness\"]==1]\nconst_d.shape","f1ce1dae":"str(const_d.Name.tolist())","8aa0f443":"#Ignore any warnings that arise\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom scipy.stats import spearmanr","983d7b45":"labels = []\nvalues = []\n\nfor col in train.columns:\n    if col not in [\"ID\", \"target\"]:\n        labels.append(col)\n        values.append(spearmanr(train[col].values, train['target'].values)[0])\n\ncorrelation_df = pd.DataFrame({'column_label':labels, 'correlation_val':values})        \ncorrelation_df = correlation_df.sort_values(by='correlation_val')\n\ncorrelation_df = correlation_df[(correlation_df['correlation_val']>0.1) | (correlation_df['correlation_val']<-0.1)]","c389a50f":"index = np.arange(correlation_df.shape[0])\nwidth = 0.9\nfig, ax = plt.subplots(figsize=(10,25))\nrec = ax.barh(index, np.array(correlation_df.correlation_val.values), color='r')\nax.set_yticks(index) #Set Y to index value of the df\nax.set_yticklabels(correlation_df.column_label.values, rotation='horizontal') #Define horizontal bar graph\nax.set_xlabel(\"Correlation coefficient\")\nax.set_title(\"Correlation coefficient of the variables\")\nplt.show()","ae332dae":"import seaborn as sns\n\ncolumns = correlation_df[(correlation_df['correlation_val']>0.11) | (correlation_df['correlation_val']<-0.11)].column_label.tolist()\n\ntmp = train[columns]\ncomat = tmp.corr(method='spearman') #Since we used spearman coefficient\nfig, ax = plt.subplots(figsize=(30,30))\n\nsns.heatmap(comat, square=True, cmap=\"RdYlGn\", annot=True)\nplt.title(\"Correlation Heatmap\", fontsize=18)\nplt.show()","215f83ca":"tr_x = train.drop(const_d.Name.tolist()+ [\"ID\", \"target\"], axis=1)\nte_x = test.drop(const_d.Name.tolist()+[\"ID\"], axis=1)\ntr_y = np.log1p(train['target'].values)","5cd315b6":"from sklearn import ensemble\nmodel = ensemble.ExtraTreesRegressor(n_estimators=200, max_depth=20, max_features=0.5, n_jobs=-1, random_state=0)\nmodel.fit(tr_x, tr_y)","064ede5c":"#Plot Importance factor\nfeatures = tr_x.columns.values\nimportance = model.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in model.estimators_], axis=0)\nindices = np.argsort(importance)[::-1][:20]\n\nplt.figure(figsize=(14,14))\nplt.title(\"Feature Importances\")\nplt.bar(range(len(indices)), importance[indices], color=\"b\", yerr=std[indices])\nplt.xticks(range(len(indices)), features[indices], rotation='vertical')\nplt.xlim([-1, len(indices)])\nplt.show()","885e53f9":"import lightgbm as lgb","1304d81c":"def run_lgb(train_x, train_y, val_x, val_y, test_x):\n    parameters = {\n        'objective': 'regression',\n        'metric': 'rmse',\n        'num_leaves': 30,\n        'learning_rate': 0.01,\n        'bagging_fraction': 0.7,\n        'feature_fraction': 0.7,\n        'bagging_frequency': 5,\n        'bagging_seed': 2018,\n        'verbosity': -1\n    }\n    \n    lgtrain = lgb.Dataset(train_x, label=train_y)\n    lgval = lgb.Dataset(val_x, label=val_y)\n    evals_result = {}\n    model = lgb.train(parameters, lgtrain, 1000, valid_sets=[lgval], early_stopping_rounds=100, verbose_eval=200, evals_result=evals_result)\n    \n    pred_test_y = model.predict(test_x, num_iteration=model.best_iteration)\n    \n    return pred_test_y, model, evals_result","d6e5b430":"from sklearn.model_selection import KFold","34ca7ee7":"k_fold = KFold(n_splits=5, shuffle=True, random_state=2020)\npred_test_final = 0\n\nfor d_ind, v_ind in k_fold.split(tr_x):\n    \n    d_x, v_x = tr_x.loc[d_ind, :], tr_x.loc[v_ind, :]\n    d_y, v_y = tr_y[d_ind], tr_y[v_ind]\n    pred_test, model, evals_result = run_lgb(d_x, d_y, v_x, v_y, te_x)\n    pred_test_final += pred_test\n    \npred_test_final \/= 5\npred_test_final = np.expm1(pred_test_final)","0034e214":"final_df = pd.DataFrame({\"ID\":test[\"ID\"].values, \"target\":pred_test_final})\nfinal_df.to_csv(\"submission.csv\", index=False)","f8d0a0b5":"#Feature importance for LightGBM\nfig, ax = plt.subplots(figsize=(14,20))\nlgb.plot_importance(model, max_num_features=50, height=0.8, ax=ax)\n#ax.grid(False)\nplt.title(\"LightGBM - Feature Importance\", fontsize=16)\nplt.show()","283d2768":"**Sources:**\n\nhttps:\/\/www.kaggle.com\/sudalairajkumar\/simple-exploration-baseline-santander-value\n\nhttps:\/\/www.kaggle.com\/c\/santander-value-prediction-challenge\/discussion\/59128\n\nhttps:\/\/en.wikipedia.org\/wiki\/Ordinal_data\n\nhttps:\/\/support.minitab.com\/en-us\/minitab-express\/1\/help-and-how-to\/modeling-statistics\/regression\/supporting-topics\/basics\/a-comparison-of-the-pearson-and-spearman-correlation-methods\/#:~:text=The%20Pearson%20correlation%20evaluates%20the%20linear%20relationship%20between%20two%20continuous%20variables.&text=The%20Spearman%20correlation%20coefficient%20is,evaluate%20relationships%20involving%20ordinal%20variables.\n\nhttps:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.nunique.html\n\nhttps:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.stats.spearmanr.html\n\nhttps:\/\/seaborn.pydata.org\/generated\/seaborn.heatmap.html\n\nhttps:\/\/matplotlib.org\/3.1.0\/tutorials\/colors\/colormaps.html\n\nhttps:\/\/www.geeksforgeeks.org\/matplotlib-pyplot-xlim-in-python\/#:~:text=The%20xlim()%20function%20in,limits%20of%20the%20current%20axes.&text=Parameters%3A%20This%20method%20accept%20the,set%20the%20xlim%20to%20right.\n\nhttps:\/\/numpy.org\/doc\/stable\/reference\/generated\/numpy.expm1.html","93c5b27a":"**Print the anonymised columns**","a646d930":"**Much better**","b0152067":"**Spearman correlation is better to use due to the fact that it is computed based on ranks and this data is not linear where we can use Pearson correlation**","23162818":"**Baseline Light GBM** (TODO:Tune it)","7af50dc3":"256 Unique columns are present","687aa11a":"**No null values. Always a good sign**","43fc3249":"**Creating Submission file**","d769073d":"**K fold cross validation for predictions in test set**","b321e885":"**Random values are the column names which mean the columns are anonymous**","bed7f30a":"**The Heatmap of Correlation**","ff4a2422":"**Best way to display a right skewed distribution is to use a log scale**","32b71cb4":"**Plotting of the target variable**"}}