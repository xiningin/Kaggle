{"cell_type":{"ba088d14":"code","ea64e5b4":"code","14b6c72a":"code","a3107fca":"code","2631c65f":"code","5dcca76c":"code","12242877":"code","543349f1":"code","434be96f":"code","e21ba5cb":"code","b1b893b3":"code","2dfe022e":"code","565c686f":"code","7078cf2e":"code","48659495":"code","24ada468":"code","43dd363e":"code","a95469c6":"code","92388098":"code","ed497a09":"code","9f91b1f3":"code","53ebfceb":"code","5659fd3a":"code","18c136f8":"code","8ea76170":"code","886b08e7":"code","43426b9c":"code","203f664e":"code","6e419d84":"code","09cdb058":"markdown","8e09ec8a":"markdown","b224a3e8":"markdown","0575a391":"markdown","08998876":"markdown","3309bd1c":"markdown","65c6b3fe":"markdown","9598f494":"markdown","2ce728b9":"markdown","01c1c019":"markdown","d87b9893":"markdown","f7622b97":"markdown","da170cad":"markdown","59e0d0a9":"markdown","be91a59c":"markdown","15fd037a":"markdown","bcffe7cf":"markdown","800af590":"markdown","c5ef02e8":"markdown","0de46932":"markdown","acde5d43":"markdown","7dacea7b":"markdown","e124429c":"markdown"},"source":{"ba088d14":"%matplotlib inline\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimport pandas as pd\nimport pandas_profiling\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import make_scorer\ndef precision_score_micro(y_true, y_pred):\n    return(precision_score(y_true, y_pred, average='micro'))\n\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\n\nfrom sklearn.model_selection import GridSearchCV\nimport seaborn as sns","ea64e5b4":"!conda install -y phik","14b6c72a":"import phik\nfrom phik import resources, report","a3107fca":"phik.__version__","2631c65f":"NB_SAMPLES = 150  # Avec 2000 :  perf 80%, et on d\u00e9tecte les 5 coefs importants.  Avec 150 : perf 57% et on ne d\u00e9tecte que les 3 premiers coefs   \nNB_VARIABLES = 35\n\nnp.random.seed(35)\nX = np.random.randint(0,10,(NB_SAMPLES, NB_VARIABLES))\nX","5dcca76c":"X.shape","12242877":"coefs = np.zeros([NB_VARIABLES])\ncoefs[0] = 20\ncoefs[1] = 15\ncoefs[2] = 10\ncoefs[3] = 5\n\ncoefs","543349f1":"coefs.shape","434be96f":"Y = (coefs.dot(X.T))\nY","e21ba5cb":"Y.shape","b1b893b3":"Y = pd.cut(pd.DataFrame(Y)[0], bins=5, labels=['F1','F2','F3','F4','F5']).to_numpy()","2dfe022e":"Y","565c686f":"pd.DataFrame(Y)[0].value_counts().plot.bar(title='Distribution of labels')","7078cf2e":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42, shuffle = True)","48659495":"model = RandomForestClassifier(max_depth=10, max_features=20, max_leaf_nodes=50,\n                       n_estimators=5000, random_state=42) \n\nmodel.fit(X_train, Y_train)","24ada468":"Y_predict_train = model.predict(X_train)\nY_predict_test = model.predict(X_test)\nprecision_score(Y_predict_train, Y_train, average='micro')","43dd363e":"pscore = precision_score(Y_predict_test, Y_test, average='micro')\nrscore = recall_score(Y_predict_test, Y_test, average='micro')","a95469c6":"print(f'So we trained a model that can predict the label with a precision of {pscore*100}% and a recall of {rscore*100}% on a test set of data it has never seen')\nprint('Will that be enough for us to discover which 4 variables were importance to predict the labels ?')","92388098":"pd.DataFrame(model.feature_importances_).plot.bar(title='Random forest: feature importances of our variables')","ed497a09":"df = pd.concat([pd.DataFrame(X),pd.DataFrame(Y).rename(columns={0: 'Y'})], axis=1)","9f91b1f3":"df_phik = df.phik_matrix()","53ebfceb":"df_phik","5659fd3a":"df_phik['Y'].sort_values(ascending=False)[0:10]","18c136f8":"NB_SAMPLES = 2000  # Avec 2000 :  perf 80%, et on d\u00e9tecte les 5 coefs importants.  Avec 150 : perf 57% et on ne d\u00e9tecte que les 3 premiers coefs   \nNB_VARIABLES = 35\n\nX = np.random.randint(0,10,(NB_SAMPLES, NB_VARIABLES))\n\ncoefs = np.zeros([NB_VARIABLES])\n#coefs[5:] = 0.2\ncoefs[0] = 20\ncoefs[1] = 15\ncoefs[2] = 10\ncoefs[3] = 5\n#coefs[4] = 2\n\nY = (coefs.dot(X.T))\n\nY = pd.cut(pd.DataFrame(Y)[0], bins=5, labels=['F1','F2','F3','F4','F5']).to_numpy()\n","8ea76170":"pd.DataFrame(Y)[0].value_counts().plot.bar(title='Distribution of labels')","886b08e7":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42, shuffle = True)\n\nmodel = RandomForestClassifier(max_depth=10, max_features=20, max_leaf_nodes=50,\n                       n_estimators=5000, random_state=42) \n\nmodel.fit(X_train, Y_train)\n\nY_predict_train = model.predict(X_train)\nY_predict_test = model.predict(X_test)\nprecision_score(Y_predict_train, Y_train, average='micro')\n\npscore = precision_score(Y_predict_test, Y_test, average='micro')\nrscore = recall_score(Y_predict_test, Y_test, average='micro')","43426b9c":"print(f'So we trained a model that can predict the label with a precision of {pscore*100}% and a recall of {rscore*100}% on a test set of data it has never seen')\nprint('Will that be enough for us to discover which 4 variables were importance to predict the labels ?')","203f664e":"pd.DataFrame(model.feature_importances_).plot.bar(title='Random forest: feature importances of our variables')","6e419d84":"df = pd.concat([pd.DataFrame(X),pd.DataFrame(Y).rename(columns={0: 'Y'})], axis=1)\ndf_phik = df.phik_matrix()\ndf_phik['Y'].sort_values(ascending=False)[0:10]","09cdb058":"# Can you apply machine learning on small data ?\n## I often hear people say that you need hundreds of thousands of data instances for machine learning. But is that always true ?\n\nSometimes you just don't have the resources or necessary time to gather much data.\n\nAccross the 7 projects of my training, I've been working on several hundred thousands, or millions of data instances on various topics.  \nThat's not exactly big data, but that's something you can apply machine learning to : KNN, linear regression, random forest, ...\n\nBut what if you have 150 instances only and 35 variables ? Will machine learning or statistical analysis help you solve your problem ?  \nI guess it depends on various things:\n- The nature of your problem and whether you need to discover every underlying effects on variables (and especially if you need to discover even the small ones), or only most important effects\n- The nature of variable distributions and the existence of \"rare\" values that you may miss in your sampling\n- The complexity of relation between dependant variables (= features, in machine learning) and independant variables (= labels to predict)\n- The amount of effect size\n- The number of variables having an effect : 150 instances and 5 variables is completely different from 150 instances and 35 variables\n\n# Practical test\nSo let's do a small practical test with a small toy dataset to get a feeling of what's possible to do or not with machine learning, on a small dataset.    \n\nLet's generate a random matrix of 150 lines and 35 columns : for each line of the matrix, we'll generate a label (values range from F1 to F5).\nEach label will be generated using a linear combination between the 4 first columns and a vector of coefficients.  And the 31 other columns have no effect on the label, they are completely random.\n\nThen we'll try some machine learning and statistical analysis on this data, to see if we can find which ones of the 4 columns did matter.  ","8e09ec8a":"# Conclusion of first run","b224a3e8":"=> We see that variables 0, 1, 2 have an important effect on the label: they have the highest feature importances for the random forest.  \n=> However the effect of variable 3, which is the next most important variable, could not be captured by the model as one of the highest effects: it may be that our sampling is too small for the model to capture it !","0575a391":"### Will machine learning work ?","08998876":"=> Our statistical model captured first 3 variables :   0, 1, 2.  \n=> But, like the random forest, it did not capture variable 4","3309bd1c":"We now have a set of generated labels Y having values from F1 to F5. For each instance:  X0,X1,X2,X3 participate to the calculation of Y, and X5...X34 do not participate at all.  But only us know that, not the model that will have to analyze the data","65c6b3fe":"# Run a statistical analysis (phi(k) correlation)","9598f494":"# Let's generate a random dataset of 150 instances x 35 variables","2ce728b9":"# Train a random forest model","01c1c019":"We saw that given the parameters of this test:  \n- Machine learning correctly identified the first 3 variables as the most important ones, and so did statistical analysis  \n- Both machine learning and statistical analysis failed to identify effect of variable 4 as the 4th most important one  \n- Machine learning gave you a performance metric based on the capacity of the model to predict label on data it has never seen  \n\nNB : if you run this notebook several times with different random seed, you will sometimes get different results :  \nin some runs, statistical correlation analysis fail to identify 2nd and 3rd variable, and sometimes random forest manages to identify 4th variable.","d87b9893":"## For each instance of X, let's generate a numeric label based on some coefficients","f7622b97":"## Imports","da170cad":"A random forest lets us know which variables were the most important for it to make its prediction. Let's see them :","59e0d0a9":"We saw that given the parameters of this test:  \n- Both machine learning and statistical analysis managed to discover the importance of all 4 variables\n- Machine learning gave you a performance metric that has reached above 80%","be91a59c":"We have :\\\nY0 = coef1 * X(0,0) + coef2 * X(0,1) + .... coef n * X(0,n)  \nY1 = coef1 * X(1,0) + coef2 * X(1,1) + .... coef n * X(1,n)  \n...  \nYn = coef1 * X(n, 0) + coef2 * X(n, 1) + .... coef n * X(n, n)  ","15fd037a":"# Now let's try to predict Y values with just X, without knowing the coeficients","bcffe7cf":"Let's try a statistical analysis on the same dataset to see what we can discover","800af590":"# Conclusion of second run","c5ef02e8":"Lets split X and Y :  80% will go to training set, and 20% to test set  \nOur model will only have access to training set, not test set  ","0de46932":"Let's print the top 10 correlated variables to Y :","acde5d43":"=> See that **only the first four coefficients are non zero**  \nFor each instance of random data X, in order to generate the labels Y, coef 0 will be multiplied by random X0, coef 1 will be multiplied by random X1, etc... until coef 34 multiplied by random X34\n\nSo, since we are the ones who are generating the data, **we know** that **only the first 4 variables** have a predictable effect on the label  \nThe 31 other variables are totally random \n\nAlso see that we set a coefficient of 20 for variable 0, 15 for variable 1, 10 for variable 2, 5 for variable 3 : the higher the coefficient, the more the effect will be important on resulting label.  And the easier it will be for a machine learning or statistical model to detect the effect.","7dacea7b":"# Now let's do the same tests, but with a larger dataset of 2000 instances !","e124429c":"Now let's discretize Y by creating 5 buckets of values:\\"}}