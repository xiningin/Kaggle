{"cell_type":{"e4f80f8d":"code","1141047e":"code","a14f6e31":"code","21bfcd6e":"code","eb533307":"code","a65982ec":"code","8d886197":"code","7ab39da6":"code","3fe737c7":"code","7f0b8ba4":"code","251d0fce":"code","71c4d452":"code","6da564ac":"code","9cb37b56":"code","beda59ee":"code","93550335":"code","ea955583":"code","d0bae45a":"code","c6cf0bd1":"code","b621bacc":"code","fafe4caf":"code","ecfec946":"code","b200f5e9":"code","f3dad4fb":"code","43121ae3":"code","5d16016e":"code","a1c8c7ee":"code","6b64b284":"code","9d15bc4c":"code","f3805f47":"markdown","cfb2a194":"markdown","d78fef30":"markdown","51d39295":"markdown","91ce5c66":"markdown","335b3703":"markdown","f55f6d0a":"markdown","5a7a3868":"markdown","4be4884d":"markdown","934a13e7":"markdown","8806217f":"markdown","a2547efe":"markdown","4d789ef2":"markdown","144e5759":"markdown","fd4a6a01":"markdown","81c77538":"markdown","972a67fa":"markdown","c40c017e":"markdown","f958f871":"markdown"},"source":{"e4f80f8d":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport os\nfrom keras import preprocessing\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D,Dropout,Dense,Flatten,Conv2DTranspose,BatchNormalization,LeakyReLU,Reshape\nimport tensorflow as tf\n","1141047e":"import os\ncwd = os.getcwd()\nos.chdir(cwd)\nprint(os.listdir(\"..\/input\"))","a14f6e31":"path_celeb = []\ntrain_path_celeb = \"\/kaggle\/input\/celeba-dataset\/img_align_celeba\/img_align_celeba\/\"\nfor path in os.listdir(train_path_celeb):\n    if '.jpg' in path:\n        path_celeb.append(os.path.join(train_path_celeb, path))","21bfcd6e":"new_path=path_celeb[0:50000]\n","eb533307":"len(new_path)","a65982ec":"crop = (30, 55, 150, 175) #croping size for the image so that only the face at centre is obtained\nimages = [np.array((Image.open(path).crop(crop)).resize((64,64))) for path in new_path]\n\nfor i in range(len(images)):\n    images[i] = ((images[i] - images[i].min())\/(255 - images[i].min()))\n    #images[i] = images[i]*2-1  #uncomment this if activation is tanh for generator last layer\n    \nimages = np.array(images) ","8d886197":"train_data=images","7ab39da6":"len(path_celeb)","3fe737c7":"print(train_data.shape)","7f0b8ba4":"plt.figure(figsize=(10,10))\nfig,ax=plt.subplots(2,5)\nfig.suptitle(\"Real Images\")\nidx=800\n\nfor i in range(2):\n    for j in range(5):\n            ax[i,j].imshow(train_data[idx].reshape(64,64,3))\n            #ax[i,j].set_title(\"Real Image\")\n            \n            idx+=600\n            \nplt.tight_layout()\nplt.show()","251d0fce":"X_train = train_data","71c4d452":"noise_shape = 100","6da564ac":"generator=Sequential()\ngenerator.add(Dense(4*4*512,input_shape=[noise_shape]))\ngenerator.add(Reshape([4,4,512]))\ngenerator.add(Conv2DTranspose(256, kernel_size=4, strides=2, padding=\"same\"))\ngenerator.add(LeakyReLU(alpha=0.2))\ngenerator.add(BatchNormalization())\ngenerator.add(Conv2DTranspose(128, kernel_size=4, strides=2, padding=\"same\"))\ngenerator.add(LeakyReLU(alpha=0.2))\ngenerator.add(BatchNormalization())\ngenerator.add(Conv2DTranspose(64, kernel_size=4, strides=2, padding=\"same\"))\ngenerator.add(LeakyReLU(alpha=0.2))\ngenerator.add(BatchNormalization())\ngenerator.add(Conv2DTranspose(3, kernel_size=4, strides=2, padding=\"same\",\n                                 activation='sigmoid'))","9cb37b56":"generator.summary()","beda59ee":"discriminator=Sequential()\ndiscriminator.add(Conv2D(32, kernel_size=4, strides=2, padding=\"same\",input_shape=[64,64, 3]))\ndiscriminator.add(Conv2D(64, kernel_size=4, strides=2, padding=\"same\"))\ndiscriminator.add(LeakyReLU(0.2))\ndiscriminator.add(BatchNormalization())\ndiscriminator.add(Conv2D(128, kernel_size=4, strides=2, padding=\"same\"))\ndiscriminator.add(LeakyReLU(0.2))\ndiscriminator.add(BatchNormalization())\ndiscriminator.add(Conv2D(256, kernel_size=4, strides=2, padding=\"same\"))\ndiscriminator.add(LeakyReLU(0.2))\ndiscriminator.add(Flatten())\ndiscriminator.add(Dropout(0.5))\ndiscriminator.add(Dense(1,activation='sigmoid'))","93550335":"discriminator.summary()","ea955583":"GAN =Sequential([generator,discriminator])","d0bae45a":"discriminator.compile(optimizer='adam',loss='binary_crossentropy')\ndiscriminator.trainable = False","c6cf0bd1":"GAN.compile(optimizer='adam',loss='binary_crossentropy')","b621bacc":"GAN.layers","fafe4caf":"GAN.summary()","ecfec946":"epochs = 300  #set epoch according to your training dataset size,i had chosen 50k images hence epochs are high as 300...\nbatch_size = 128","b200f5e9":"D_loss=[] #list to collect loss for the discriminator model\nG_loss=[] #list to collect loss for generator model","f3dad4fb":"with tf.device('\/gpu:0'):\n for epoch in range(epochs):\n    print(f\"Currently on Epoch {epoch+1}\")\n    \n    # For every batch in the dataset\n    for i in range(X_train.shape[0]\/\/batch_size):\n        \n        if (i)%100 == 0:\n            print(f\"\\tCurrently on batch number {i} of {len(X_train)\/\/batch_size}\")\n            \n        noise=np.random.uniform(-1,1,size=[batch_size,noise_shape])\n        \n        gen_image = generator.predict_on_batch(noise)\n        \n        train_dataset = X_train[i*batch_size:(i+1)*batch_size]\n        #train on real image\n        train_label=np.ones(shape=(batch_size,1))\n        discriminator.trainable = True\n        d_loss1 = discriminator.train_on_batch(train_dataset,train_label)\n        \n        #train on fake image\n        train_label=np.zeros(shape=(batch_size,1))\n        d_loss2 = discriminator.train_on_batch(gen_image,train_label)\n        \n        \n        noise=np.random.uniform(-1,1,size=[batch_size,noise_shape])\n        train_label=np.ones(shape=(batch_size,1))\n        discriminator.trainable = False\n        #train the generator\n        g_loss = GAN.train_on_batch(noise, train_label)\n        D_loss.append(d_loss1+d_loss2)\n        G_loss.append(g_loss)\n        \n         \n    if epoch % 5 == 0:\n        samples = 10\n        x_fake = generator.predict(np.random.normal(loc=0, scale=1, size=(samples,100)))\n\n        for k in range(samples):\n            plt.subplot(2, 5, k+1)\n            plt.imshow(x_fake[k].reshape(64,64,3))\n            plt.xticks([])\n            plt.yticks([])\n\n        \n        plt.tight_layout()\n        plt.show()\n    print('Epoch: %d,  Loss: D_real = %.3f, D_fake = %.3f,  G = %.3f' %   (epoch+1, d_loss1, d_loss2, g_loss))        \nprint('Training is complete')","43121ae3":"noise=np.random.uniform(-1,1,size=[500,noise_shape])","5d16016e":"im=generator.predict(noise)","a1c8c7ee":"for i in range(5):\n plt.figure(figsize=(7,7))   \n for k in range(20):\n            noise=np.random.uniform(-1,1,size=[100,noise_shape])\n            im=generator.predict(noise) \n            plt.subplot(5, 4, k+1)\n            plt.imshow(im[k].reshape(64,64,3))\n            plt.xticks([])\n            plt.yticks([])\n \n plt.tight_layout()\n plt.show()","6b64b284":"plt.figure(figsize=(10,10))\nplt.plot(G_loss,color='red',label='Generator_loss')\nplt.plot(D_loss,color='blue',label='Discriminator_loss')\nplt.legend()\nplt.xlabel('total batches')\nplt.ylabel('loss')\nplt.title('Model loss per batch')\nplt.show()","9d15bc4c":"import pickle\nPkl_Filename = \"DCGAN.pkl\"  \n\n","f3805f47":"# **Deep Convolution Adversarial Networks**","cfb2a194":"the last layer for generator  have sigmoid activation in this version since I was trying between sigmoid and tanh to see which outputs better results. I found sigmoid activated model having better clarity in this case, however I have tried tanh activated generator too so if anyone want to refer to that can look at version 6\/7 of this kernel where I use tanh. In case of using tanh the input pixels should be normalised between (-1,1) unlike sigmoid where it has been normalised between (0,1) .\n","d78fef30":"# **Output Visualization**","51d39295":"# **Loss Curve**","91ce5c66":"# **Data Visualization**","335b3703":"# **Training**","f55f6d0a":"approx training time for 300 epochs was somthing little more than 2 hours.","5a7a3868":"# **Discriminator**","4be4884d":"# **Generator**","934a13e7":"# **Comment if you have any queries or find anything wrong with the code**","8806217f":"# **Reference**\n\n1. https:\/\/medium.com\/coloredfeather\/generating-human-faces-using-adversarial-network-960863bc1deb blog post\n2. https:\/\/arxiv.org\/abs\/1511.06434  paper on DCGANS published in 2016\n","a2547efe":"# **Thanks**","4d789ef2":"# **Loadind and Preprocessing data**","144e5759":"# **DCGAN (combined model)**","fd4a6a01":"the dataset have more than 202k images of which only 50k are being selected for the training purpose","81c77538":"![dcgan.png](attachment:dcgan.png)","972a67fa":"# **Upvote if you like the kernel**","c40c017e":"# **Architecture of the Model**\n\nThe core to the DCGAN architecture uses a standard CNN architecture on the discriminative model. For the generator, convolutions are replaced with upconvolutions, so the representation at each layer of the generator is actually successively larger, as it mapes from a low-dimensional latent vector onto a high-dimensional image.\n\n\n\nUse batch normalization in both the generator and the discriminator.\n\nRemove fully connected hidden layers for deeper architectures.\n\nUse ReLU activation in generator for all layers except for the output, which uses Tanh.\n\nUse LeakyReLU activation in the discriminator for all layers.","f958f871":"**Note**  -- the image pixels are normalized dividing each pixel by 255 and then modified to bring within (-1,1) range by multiplying with 2 and substracting 1 since the last layer activation of generator is tanh whose range limits (-1,1)"}}