{"cell_type":{"dc803fe9":"code","387b875b":"code","efd412e7":"code","d033615b":"code","30369b0f":"code","0e005e48":"code","b49a6ff6":"code","682edfe2":"code","4febda40":"code","64029c16":"code","921de315":"code","1846c3ae":"code","c5522a00":"code","9ba52a96":"code","39fb2e5e":"code","b396294a":"code","2ee15772":"code","95a98378":"code","c73306a5":"code","9b872041":"code","6b7cd81b":"code","7a8c2372":"code","867bb06b":"code","4201cbca":"code","bc0a2391":"code","60396986":"code","4da3765b":"code","7576ba60":"code","825e2f13":"markdown","3138fa46":"markdown","cc4d1cf8":"markdown","1bf26e03":"markdown","2e8875e8":"markdown","e5d3d2c9":"markdown","e9c6ffdf":"markdown","951f2379":"markdown","b9c71acd":"markdown","ca0205b3":"markdown","91e76141":"markdown","7f10b9ab":"markdown"},"source":{"dc803fe9":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style = \"darkgrid\")\n%matplotlib inline\nimport gc\n\nimport time\nimport warnings\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\npd.set_option('float_format', '{:f}'.format)\nwarnings.filterwarnings('ignore')","387b875b":"def reduce_mem_usage(train_data):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n    to reduce memory usage.\n    \"\"\"\n    start_mem = train_data.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n\n    for col in train_data.columns:\n        col_type = train_data[col].dtype\n\n        if col_type != object:\n            c_min = train_data[col].min()\n            c_max = train_data[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    train_data[col] = train_data[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    train_data[col] = train_data[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    train_data[col] = train_data[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    train_data[col] = train_data[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    train_data[col] = train_data[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    train_data[col] = train_data[col].astype(np.float32)\n                else:\n                    train_data[col] = train_data[col].astype(np.float64)\n        else:\n            train_data[col] = train_data[col].astype('category')\n\n    end_mem = train_data.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n\n    return train_data","efd412e7":"sample = pd.read_csv(\"..\/input\/tabular-playground-series-feb-2022\/sample_submission.csv\")\ntrain  = pd.read_csv(\"..\/input\/tabular-playground-series-feb-2022\/train.csv\")\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-feb-2022\/test.csv\")\n\nsample = reduce_mem_usage(sample)\ntrain = reduce_mem_usage(train)\ntest = reduce_mem_usage(test)\n\ngc.collect()","d033615b":"train.head(3)","30369b0f":"test.head(3)","0e005e48":"sample.head(3)","b49a6ff6":"train.info()","682edfe2":"train.describe()","4febda40":"print(train.shape, test.shape, sample.shape)","64029c16":"train.isnull().sum().sum(), test.isnull().sum().sum()  # No null in train and test datasets","921de315":"all_features = list(test.columns)[1:]\nprint(all_features)","1846c3ae":"df = pd.concat([train[all_features], test[all_features]], axis=0)\nlist(zip(df.columns, df.dtypes, df.nunique()))","c5522a00":"# ['A0T0G0C10', 'A0T0G1C9', 'A0T0G9C1', 'A0T0G10C0', 'A0T1G0C9', 'A0T10G0C0', 'A1T0G9C0', 'A10T0G0C0']  each has less than 25 different values so we will use it as categorical features\ncat_features = [i for i in all_features if df[i].nunique() <25]\nnum_features = [j for j in all_features if df[j].nunique() >= 25]\nprint(cat_features)\nprint()\nprint(num_features)","9ba52a96":"print(len(cat_features),len(num_features))","39fb2e5e":"df[cat_features].describe()","b396294a":"# for col in cat_feat:\n#     print(col)\n#     print(train[col].value_counts(normalize=True))\n#     print(\"_\"*40)","2ee15772":"sample_train = train.sample(10000)\nsample_test = test.sample(10000)\ndel train\ndel test\ndel sample\ndel df\ngc.collect()","95a98378":"sample_train[cat_features].hist(figsize=(20,20))\nplt.show()\nplt.tight_layout()","c73306a5":"sample_train.drop(cat_features+[\"row_id\"], axis=1).hist(figsize=(50,50))\nplt.show()\nplt.tight_layout()","9b872041":"target_classes = list(set(sample_train.target))\ntarget_classes","6b7cd81b":"ax= sns.countplot(y=sample_train.target)\nprint(sample_train.target.value_counts()\/sample_train.shape[0])","7a8c2372":"sample_train[num_features].head(2)","867bb06b":"num_corr = sample_train[num_features]\nmask = np.triu(np.ones_like(num_corr, dtype = bool))\nplt.figure(figsize=(20,16))\nsns.heatmap(num_corr, mask = mask, cmap='magma')","4201cbca":"print(len(cat_features),len(num_features))","bc0a2391":"sample_train.head(2)","60396986":"sample_test.head(2)","4da3765b":"fig, axes = plt.subplots(4,2, figsize=(10,10))\naxes = axes.flatten()\nfor idx, ax in enumerate(axes):\n    sns.kdeplot(sample_train[cat_features[idx]], color=\"red\", label=\"train\", ax=ax)\n    sns.kdeplot(sample_test[cat_features[idx]],  color=\"green\", label=\"test\", ax=ax)\n    ax.get_yaxis().set_visible(False)\n    ax.legend()\nfig.suptitle(\"distribution of train-test cat_feat\")\nfig.tight_layout()\nplt.show()","7576ba60":"fig, axes = plt.subplots(139,2, figsize=(20,800))\naxes = axes.flatten()\nfor idx, ax in enumerate(axes):\n        sns.kdeplot(sample_train[num_features[idx]], color=\"red\", label=\"train\", ax=ax)\n        sns.kdeplot(sample_test[num_features[idx]],  color=\"green\", label=\"test\", ax=ax)\n        ax.get_yaxis().set_visible(False)\n        ax.set_title(f'f{num_features[idx]}', loc = 'right', fontsize = 12)\n        ax.legend()\nfig.suptitle(\"distribution of train-test num_feat\")\nfig.tight_layout()\nplt.show()","825e2f13":"## Now we compare the distribution of train and test set to see if they have same distribution or not.","3138fa46":"Since we have have huge dataset so we take only sample of 10000 and since the sample is random so it represents our original dataset.","cc4d1cf8":"## So classes are almost balanced","1bf26e03":"Now we look at distribution of numerical features.","2e8875e8":"### For numerical features  distribution is almost same for both train and test set except for few columns.","e5d3d2c9":"### Here we look at no of unique values in each column and it's data type.","e9c6ffdf":"We look at the distribution of categorical features in our training dataset","951f2379":"### For categorical features  distribution is not same for train and test set.","b9c71acd":"#### below function makes our dataframe memory efficient.","ca0205b3":"## Here we see that our target class is balanced.","91e76141":"### We check for if there is any null value in train or test set.","7f10b9ab":"### reduce_mem_usage function has been taken from https:\/\/www.kaggle.com\/questions-and-answers\/148011"}}