{"cell_type":{"ace2754e":"code","4d2510cf":"code","1a459d0e":"code","bf891788":"code","2164ab12":"code","c970b5af":"code","7547f16d":"code","bae5f097":"code","319bba56":"code","4a50fe39":"code","05482b95":"code","1e2548a7":"code","e08a5e11":"code","2b49b9cc":"code","cfe0fdc9":"code","7e1105ee":"code","0d66abb3":"code","072aad45":"code","09266805":"code","a47e82e6":"code","e03bdee9":"code","954ba8fe":"code","2d2bfc9e":"code","650dfbf0":"code","0fcb5055":"code","e6139b96":"code","a48e383a":"code","db88694c":"code","892262ba":"code","a6f1e85b":"code","970df67a":"code","63f7d9ff":"code","ec9cac9c":"code","7ee0b883":"code","e659aa8b":"code","3e89b3b5":"code","3701941b":"markdown","c992c596":"markdown","d1bd2f1c":"markdown","4b2adaf2":"markdown","545eaa05":"markdown","54869b03":"markdown","2aa1dc86":"markdown","b7675b88":"markdown","f909f773":"markdown","82beff43":"markdown","bf58706f":"markdown","59a73d37":"markdown","3095585a":"markdown","53ac4760":"markdown","a0354cd2":"markdown","90486803":"markdown","883bc32d":"markdown","f59515ee":"markdown","b7f26a5f":"markdown","982ab826":"markdown","50d3f9c3":"markdown","8b34e8d7":"markdown","3afa7f8f":"markdown","4dc12974":"markdown","c5fcd58f":"markdown","8dabd5d0":"markdown","3595a9a9":"markdown","99c73af3":"markdown","b81811da":"markdown","8d14b205":"markdown","2e699545":"markdown","f13c327c":"markdown","b1e6715f":"markdown","6f3f19e9":"markdown","07cfa383":"markdown"},"source":{"ace2754e":"import pandas as pd\nimport numpy as np\nimport datetime as dt\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n# Parses the datetime string into datetime objects\ndef dateparse(x):\n    try:\n        return dt.datetime.strptime(x, '%Y-%m-%d %H:%M:%S')\n    except:\n        return pd.NaT\n\n# Load cleaned data\ndf = pd.read_csv(\"..\/input\/chicago_taxi_trips_2016_05.csv\", parse_dates=['trip_start_timestamp', 'trip_end_timestamp'],  date_parser=dateparse)\nprint(df.shape)","4d2510cf":"df.head()","1a459d0e":"# Fields not needed to our problem\nto_drop = [\"taxi_id\",\n           \"pickup_census_tract\",\n           \"dropoff_census_tract\",\n           \"tips\",\n           \"trip_seconds\",\n           \"trip_miles\",\n           \"extras\",\n           \"trip_total\",\n           \"company\",\n           \"tolls\",\n           \"payment_type\",\n           \"trip_end_timestamp\"]\n\n# Drop selected fields in place\ndf.drop(to_drop, inplace=True, axis=1)","bf891788":"# For each feature I'm going to use for training, let's calculate and print\n#the number and porcentage of missing values\nfeatures = [\"trip_start_timestamp\", \"pickup_community_area\", \"dropoff_community_area\", \"fare\", \"pickup_latitude\", \"pickup_longitude\", \"dropoff_latitude\", \"dropoff_longitude\"]\nfor f in features:\n    na = df[f].isnull().sum()\n    print(f, \"->\", \"Missing values:\", na, \"Percentage:\", na\/len(df)*100)","2164ab12":"df.dropna(inplace=True)\ndf.shape","c970b5af":"# Transform the start datatime object into discrete weekday and time features\ndf['weekday'] = df['trip_start_timestamp'].map(lambda x: x.weekday())\ndf['time'] = df['trip_start_timestamp'].map(lambda x: x.hour*4 + round(x.minute\/15))\ndf.drop('trip_start_timestamp', inplace=True, axis=1)","7547f16d":"import geopy.distance\n\n# Load lookup table\nlt = pd.read_json(\"..\/input\/column_remapping.json\")\n\n# Change indices with the real value\ndf['pickup_latitude'] = df['pickup_latitude'].map(lambda x: lt.pickup_latitude[x])\ndf['pickup_longitude'] = df['pickup_longitude'].map(lambda x: lt.pickup_longitude[x])\ndf['dropoff_latitude'] = df['dropoff_latitude'].map(lambda x: lt.dropoff_latitude[x])\ndf['dropoff_longitude'] = df['dropoff_longitude'].map(lambda x: lt.dropoff_longitude[x])","bae5f097":"# Calculate lineal distance using coordinates\ndef calculate_distance(src):\n    coords_1 = (src[\"pickup_latitude\"], src[\"pickup_longitude\"])\n    coords_2 = (src[\"dropoff_latitude\"],src[\"dropoff_longitude\"])\n    return geopy.distance.distance(coords_1, coords_2).m\n\n# Generate lineal distance field\ndf['distance'] = df.apply(calculate_distance, axis='columns')","319bba56":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","4a50fe39":"sns.distplot(df[\"distance\"]);","05482b95":"# Remove detected outliers\ndf = df[df.distance != 0]\ndf = df[df.distance <= 35000]\nsns.distplot(df[\"distance\"]);\ndf[\"distance\"].describe()","1e2548a7":"# Since I've scaled the time feture to blocks of 15 minutes if divided by 4 obtain hours\nsns.distplot(df[\"time\"]\/4);\n(df[\"time\"]\/4).describe()","e08a5e11":"# Maybe some days have more trips than others\nsns.countplot(df[\"weekday\"]);\ndf[\"weekday\"].describe()","2b49b9cc":"# Number of trips for time for each weekday\nsns.violinplot(\"weekday\", \"time\", data=df)","cfe0fdc9":"# Plot dropoff latitude and longitude as map coordinates\nsns.jointplot(y=\"dropoff_latitude\", x=\"dropoff_longitude\", data=df);","7e1105ee":"# Remove detected outliers and plot again\ndf = df[df.dropoff_longitude >= -87.85]\nsns.jointplot(y=\"dropoff_latitude\", x=\"dropoff_longitude\", data=df);","0d66abb3":"# Plot pickup latitude and longitude as map coordinates\nsns.jointplot(y=\"pickup_latitude\", x=\"pickup_longitude\", data=df);","072aad45":"# Remove detected outliers and plot again\ndf = df[df.pickup_longitude >= -87.85]\nsns.jointplot(y=\"pickup_latitude\", x=\"pickup_longitude\", data=df)","09266805":"# Generate fare histogram\nsns.distplot(df[\"fare\"]);\ndf[\"fare\"].describe()","a47e82e6":"# Remove heavy outliers and plot again\ndf = df[df[\"fare\"]<=55]\nsns.distplot(df[\"fare\"]);","e03bdee9":"# Plot histogram for pickup community area\nsns.distplot(df[\"pickup_community_area\"])\ndf[\"pickup_community_area\"].describe()","954ba8fe":"# Plot dropoff community area to detect potential outliers\nsns.distplot(df[\"dropoff_community_area\"])\ndf[\"dropoff_community_area\"].describe()","2d2bfc9e":"from sklearn.model_selection import train_test_split\n\nX = df.drop('fare', axis=1)\ny = df[\"fare\"]\ny = np.asarray(y, dtype=np.float64)\n\n# Generate sets: 80% train, 20% test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1337)","650dfbf0":"from sklearn.metrics import mean_squared_error\nfrom sklearn.dummy import DummyRegressor\nfrom sklearn.model_selection import GridSearchCV\nimport seaborn as sns\n\n# Given a model calculate the train and test scores\ndef get_scores(est):\n    y_train_predict = est.predict(X_train)\n    train_predict = mean_squared_error(y_train, y_train_predict)\n    y_test_predict = est.predict(X_test)\n    test_predict = mean_squared_error(y_test, y_test_predict)\n    print(\"Train mse:\", train_predict, \"; Test mse:\", test_predict)\n    return (train_predict, test_predict)\n\n# Given a model plot the residual plot for the test dataset\ndef plot_residuals(est):\n    sns.residplot(est.predict(X_test), y_test)\n\n# Dummy model generation and score\ndummyModel = DummyRegressor(strategy=\"mean\")\ndummyModel.fit(X_train, y_train)\nget_scores(dummyModel)","0fcb5055":"from sklearn import tree\n\n# Parameters to tune\nparameters = {'min_samples_split':[64, 128, 256],\n              'min_samples_leaf':[2, 4, 16]}\n\n# Declare objects\nest_dt_r = tree.DecisionTreeRegressor()\nest_dt = GridSearchCV(est_dt_r, parameters, n_jobs=-1, cv=4, verbose=1)\n# Train and score model\nest_dt = est_dt.fit(X_train, y_train)\nget_scores(est_dt)\nplot_residuals(est_dt)","e6139b96":"# Print best parameters found by the grid search\nest_dt.best_params_","a48e383a":"from sklearn.linear_model import SGDRegressor\n\n# Parameters to tune\nparameters = {'alpha':[0.001, 0.0001, 0.00001]}\n\n# Declare objects\nest_nn_r = SGDRegressor(max_iter=10000, tol=1e-3)\nest_nn = GridSearchCV(est_nn_r, parameters, n_jobs=-1, cv=4, verbose=1)\n# Train and score model\nest_nn = est_nn.fit(X_train, y_train)\nget_scores(est_nn)\nplot_residuals(est_nn)","db88694c":"from sklearn.ensemble import RandomForestRegressor\n\n# Parameters to tune\nparameters = {'n_estimators':[10, 50, 100, 150],\n              'min_samples_split':[64, 128, 256],\n              'min_samples_leaf': [2, 4, 6]}\n\n# Declare objects\nest_rf_r = RandomForestRegressor(random_state=1337)\nest_rf = GridSearchCV(est_rf_r, parameters, cv=4, verbose=1, n_jobs=-1)\n# Train and score model\nest_rf = est_rf.fit(X_train, y_train)\nget_scores(est_rf)\nplot_residuals(est_rf)","892262ba":" # Print best parameters found by the grid search\nest_rf.best_params_","a6f1e85b":"from sklearn.ensemble import AdaBoostRegressor\n\n# Parameters to tune\nparameters = {'n_estimators':[25, 50, 100],\n              'loss':[\"square\", \"linear\"],\n              'learning_rate': [0.75, 1, 1.25]}\n\n# Declare objects\nest_ada_r = AdaBoostRegressor(random_state=1337)\nest_ada = GridSearchCV(est_ada_r, parameters, cv=4, verbose=1, n_jobs=-1)\n# Train and score model\nest_ada = est_ada.fit(X_train, y_train)\nget_scores(est_ada)\nplot_residuals(est_ada)","970df67a":"# Print best parameters found by the grid search\nest_ada.best_params_","63f7d9ff":"from sklearn.ensemble import GradientBoostingRegressor\n\n# Parameters to tune\nparameters = {'n_estimators':[50, 100, 150],\n              'min_samples_split':[64, 128, 256],\n              'min_samples_leaf': [2, 4, 6]}\n\n# Declare objects\nest_gtb_r = GradientBoostingRegressor(loss='ls')\nest_gtb = GridSearchCV(est_gtb_r, parameters, cv=4, verbose=1, n_jobs=-1)\n# Train and score model\nest_gtb = est_gtb.fit(X_train, y_train)\nget_scores(est_gtb)\nplot_residuals(est_gtb)\n# Print best parameters found by the grid search\nprint(est_gtb.best_params_)","ec9cac9c":"from sklearn.neural_network import MLPRegressor\nfrom sklearn.preprocessing import StandardScaler\n\n# Fit and transform the scaler with the train data\nscaler = StandardScaler()\nscaler.fit(X_train)\nX_train_s = scaler.transform(X_train)\nX_test_s = scaler.transform(X_test)\n\n# Define and train the model\nest_net = MLPRegressor(learning_rate=\"adaptive\", max_iter=400, hidden_layer_sizes=(150, ))\nest_net = est_net.fit(X_train_s, y_train)","7ee0b883":"# Scale the test data and generate benchmark\ny_train_predict = est_net.predict(X_train_s)\ntrain_predict = mean_squared_error(y_train, y_train_predict)\ny_test_predict = est_net.predict(X_test_s)\ntest_predict = mean_squared_error(y_test, y_test_predict)\nprint(\"Train mse:\", train_predict, \"; Validation mse:\", test_predict)\n\nsns.residplot(est_net.predict(X_test_s), y_test);","e659aa8b":"from sklearn.ensemble import RandomForestRegressor\nest = RandomForestRegressor(random_state=1337, min_samples_split=60, n_estimators=300, n_jobs=-1)              \nest = est.fit(X_train, y_train)\nget_scores(est)\nplot_residuals(est)","3e89b3b5":"from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\ny_test_predict = est.predict(X_test)\n\nmae = mean_absolute_error(y_test, y_test_predict)\nmse = mean_squared_error(y_test, y_test_predict)\nr_2 = r2_score(y_test, y_test_predict)\nprint(\"MSE -> \", mse)\nprint(\"MAE -> \", mae)\nprint(\"R^2 -> \", r_2)","3701941b":"The dummy model achieved a train score of 140.\n\n## Support Vector Machine\n\n```py\nfrom sklearn import svm\nest_svm = svm.SVR(cache_size=13000)\n# Train and score model\nest_svm = est_svm.fit(X_train, y_train)\nget_scores(est_svm)\nplot_residuals(est_svm)\n```\n\nToo slow for this dataset, stopped after 20 minutes with plenty of available memory.","c992c596":"## Generate additional geospatial data\n\nIt's not possible to calculate the final length of the trip beforehand, but we can calculate the lineal distance between our start and end position.\n\nAll latitude and longitude are encoded with a lookup table located on another file.","d1bd2f1c":"For a first model and a basic grid search the results are quite good.\n\nLooking at the residuals plot we see a clear lineal trend that will appear on almost all the models. This can mean that we missed some important explanatory variable, which is normal, since we have restricted the set to a limited subset of variables.\n\n##  Stochastic Gradient Descent","4b2adaf2":"We clearly see a big number of observation having a distance of 0m. This is likely to be bad data, so I'll remove those rows. Also, I'll consider as outliers any observation with a distance greater than 35000m.","545eaa05":"We can see two cluster-like constructions on the vertical axes. Also, there are some potential outliers at low longitude values.","54869b03":" Random forest is just a combination of smaller decisions trees than then vote. I was expecting to improve respect the single decision tree model, but the results are almost identical.\n \n ### Ada boost","2aa1dc86":"This look exactly the same plot as the dropoff case.","b7675b88":"Surprisingly we don't see any trend here. Maybe the work-related trips of the workdays is similar to the leisure one on the weekends.","f909f773":"## Score function\n\nA score function is needed to compare different models. Because in this case I'm going to use a common error function (MSE) which is included in sklearn metrics and most of the models, I don't need to define it explicitly. A helper function is implemented to calculate the scores given a model.\n\nAs a baseline for the model performance a dummy model which return the mean is created.","82beff43":"# Exploratory analysis and anomaly detection\nBecause it is a small set of variable lets analyze one by one.","bf58706f":"Here we have some heavy outliers. If we just remove the 0.4% most expensive trips:","59a73d37":" Although here only appears a single model, I have tried changing some parameters to try to improve the results, with not much difference.This fit into the good models, but still far from the decision trees.\n \n \n# Final model selecction\n \nRandom forest is selected as the final model. Let's make more plots to ensure analize the quality of the solution.","3095585a":"# Initial data cleaning\n\nBefore exploring the data let's remove the variables that won't be used and impossible or partial data.","53ac4760":"## Feature selection\n\nSince we are restricting the problem data to that which can be obtained before taking a taxi, we are gonna drop all but those variables.\n\nWe have two factors derived from the location: census and community area. Because for privacy reasons some census tracks are missing, we ar[](http:\/\/)e not going to use that factor.\n\nAs for the target variable, we are only considering the 'fare' without any extra (ie, tips).","a0354cd2":"# Evaluation\n\nFrom the last residual plot we clearly see there is a trend, so the final model have miss some\nimportant relation. This is normal, since the features set I limit this project is quite small. Despite\nthat we can see at our benchmark metric as well as some additional ones.\n\nThe determination coefficient tell us how well a model fits some data, with a maximum value of 1,\n0.93 is a very good result.\n\nWith the mean error value, we can say that in average our model with differs with the real cost of\nabout $1.5, which for the use case is low enough.\n\nFinally, our score MSE compared with the MAE indicates that may be a subset of point which our\nmodel does not fit very well (we can also see that in the residual plot). Nevertheless, 9.3 is good for\nour problem.","90486803":"\n\nAs expected source and destination are very similar. There are two groups of areas much more present than the rest.","883bc32d":"# Distance","f59515ee":"## Split timestamps into usable components\n\nBecause year and month remain constant, we are only interested on the actual weekday.\n\nObserving the time we see that seconds are always 0 and minutes always multiple of 15. I consider useful the creation of a variable 'time' with unit 15 minutes.","b7f26a5f":" This is a very bad model, order of magnitude worse than the dummy one.\n \n## Ensemble methods\n\nEnsamble methods use different models to obtain a better one. Usually active better results and less overfitting, so I'm expecting to active better results than to the previous ones.\n\n### Random forest","982ab826":"##  Decision Tree","50d3f9c3":"For 'fare', 'tolls' and the timestamps, the number of missing is low enough to delete them considering it defective data.\n\nFor the other variables we'll also need to remove them, as they are necessary for the algorithms and cannot be imputed.","8b34e8d7":" That's quite a good model, but doubles the error compared with the decision tree. \n \n ## Neural network model\n \nAs a difference with the previous models, the data have been scaled due to the sensitivity of the algorithm to this matter.","3afa7f8f":"# Model building","4dc12974":" ## Test \/ train split generation\n","c5fcd58f":"That look much better!\n\n## Time","8dabd5d0":"We can clearly see the daily trends, but not any outlier.\n\n## Weekday","3595a9a9":"## Dropoff coordinates\n","99c73af3":"## Dropoff community area\n","b81811da":"## Missing values\nSome entries have features missing values:","8d14b205":"Clearly most of the trips are short.\n\n## Pickup community area","2e699545":"## Fare","f13c327c":"This is the first time the residual analysis have a significative change. Despite this model is not bad, it's far from the best at the moment.\n\n### Gradient Tree Boosting","b1e6715f":"# How much will it cost me?\n\n## Note\n\nThis is one of my first machine learning related projects and the only objective is to learn. I've tried to do my best at the time, but it should be taken with a pinch of salt. Thus, critics are more than welcome!\n\n# Introduction\nThe final cost of a taxi trip is usually a surprise, depending on numerous factors that can not be foreseen in advance.\n\nAlthough the most important are time and distance of a route, there are many others that affect in a more indirect way, such as the traffic of a determined area, the weather, the time of the day...\n\nIn this project I have the objective of predicting the final cost of a trip considering only information you can have beforehand. Since this is a learning project I'm only going to use the data of a single month: May 2016","6f3f19e9":"## Pickup coordinates","07cfa383":"# Future work\n\nA typical approach to improve a model is to use more data. In this case I only use the data of May,\nincluding other months could improve the overall score.\n\nIn our case however, we've identified on the residual plot that an important relation is missing.\nThus, introducing new variables like the weather, holidays or especial events could minimize this\nproblem. Considering this I think this model can improve.\n\nFinally, we could try different algorithms, like a complex neural network, although this may implied\nother problems and is a bit overkill."}}