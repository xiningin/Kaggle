{"cell_type":{"b0491f3e":"code","ca2878c1":"code","3cb48350":"code","49ccdbf1":"code","11920273":"code","caba8fcc":"code","3bbb804a":"code","8cae8a43":"code","9a933825":"code","e74da49a":"code","620e31d1":"code","6099bdc7":"code","a65c3bcc":"code","93138d28":"code","a2c2616f":"code","78b50708":"code","6e755859":"code","a728709c":"code","cf7497c6":"code","f062b0a1":"code","11ed1dde":"code","ed42e3b8":"markdown","3abb80f4":"markdown","04f06153":"markdown","1dd9c76d":"markdown","9743112b":"markdown","ade2ac8d":"markdown","94a4c904":"markdown","92959c9b":"markdown","1a05f862":"markdown","9d673271":"markdown","3571c4e4":"markdown","3fb28b64":"markdown","ea79f06d":"markdown"},"source":{"b0491f3e":"# Importing libraries\nimport os\nimport pickle\nimport pandas as pd\nimport numpy as np\nimport random\nimport string\nimport re\nimport numpy as np\nfrom nltk.translate.bleu_score import sentence_bleu\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.layers import TextVectorization\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","ca2878c1":"# Tranining Hyperparameters\nbatch_size = 64\n\n# Model Hyperparameters\nembed_dim = 128\nnum_heads = 10\nlatent_dim = 2048\nvocab_size = 20000\nsequence_length = 20\ndropout = 0.2","3cb48350":"def preprocess_text(df):\n    # Lowercase the characters\n    df[\"english_sent\"] = df[\"english_sent\"].apply(lambda x : x.lower())\n    df[\"hindi_sent\"] = df[\"hindi_sent\"].apply(lambda x : x.lower())\n\n    # Rmoving URLs\n    df[\"english_sent\"] = df[\"english_sent\"].apply(lambda x : re.sub(r\"http\\S+\", \"\", x))\n    df[\"hindi_sent\"] = df[\"hindi_sent\"].apply(lambda x : re.sub(r\"http\\S+\", \"\", x))\n\n    # Removing digits\n    remove_digits = str.maketrans(\"\", \"\",string.digits)\n    df[\"english_sent\"] = df[\"english_sent\"].apply(lambda x : x.translate(remove_digits))\n    df[\"hindi_sent\"] = df[\"hindi_sent\"].apply(lambda x : x.translate(remove_digits))\n    df[\"hindi_sent\"] = df[\"hindi_sent\"].apply(lambda x : re.sub(\"[a-zA-z\u0968\u0969\u0966\u096e\u0967\u096b\u096d\u096f\u096a\u096c]\", \"\", x))\n\n    # Remove special characters\n    special = set(string.punctuation)\n    df['english_sent'] = df['english_sent'].apply(lambda x : ''.join(ch for ch in x if ch not in special))\n    df['hindi_sent'] = df['hindi_sent'].apply(lambda x : ''.join(ch for ch in x if ch not in special))\n\n    # Remove quotes\n    df['english_sent'] = df['english_sent'].apply(lambda x: re.sub(\"'\", '', x))\n    df['hindi_sent'] = df['hindi_sent'].apply(lambda x: re.sub(\"'\", '', x))\n    \n    # Remove extra spaces\n    df['english_sent'] = df['english_sent'].apply(lambda x : x.strip())\n    df['hindi_sent'] = df['hindi_sent'].apply(lambda x : x.strip())\n    df['english_sent'] = df['english_sent'].apply(lambda x : re.sub(\" +\",\" \",x))\n    df['hindi_sent'] = df['hindi_sent'].apply(lambda x : re.sub(\" +\",\" \",x))\n    \n\n    # Add [start] and [end] tags\n    df[\"hindi_sent\"] = df[\"hindi_sent\"].apply(lambda x : \"[start] \" + x + \" [end]\")","49ccdbf1":"def decode_sequence(input_sentence):\n    hindi_vocab = hindi_vectorization.get_vocabulary()\n    hindi_index_lookup = dict(zip(range(len(hindi_vocab)), hindi_vocab))\n    max_decoded_sentence_length = 20\n    \n    tokenized_input_sentence = eng_vectorization([input_sentence])\n    decoded_sentence = \"[start]\"\n    for i in range(max_decoded_sentence_length):\n        tokenized_target_sentence = hindi_vectorization([decoded_sentence])[:, :-1]\n        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n\n        sampled_token_index = np.argmax(predictions[0, i, :])\n        sampled_token = hindi_index_lookup[sampled_token_index]\n        decoded_sentence += \" \" + sampled_token\n\n        if sampled_token == \"[end]\":\n            break\n    \n    return decoded_sentence[8:-5] # Removing [start] and [end] tokens","11920273":"# For creating Dataset\ndef format_dataset(eng, hin):\n    eng = eng_vectorization(eng)\n    hindi = hindi_vectorization(hin)\n    return ({\"encoder_inputs\" : eng, \"decoder_inputs\" : hindi[:, :-1],}, hindi[:, 1:])\n\n\ndef make_dataset(df):\n    dataset = tf.data.Dataset.from_tensor_slices((df[\"english_sent\"].values, df[\"hindi_sent\"].values))\n    dataset = dataset.batch(batch_size)\n    dataset = dataset.map(format_dataset)\n    return dataset.shuffle(2048).prefetch(16).cache()\n","caba8fcc":"df = pd.read_csv(\"..\/input\/english-to-hindi-text-corpus\/Hindi_English_Truncated_Corpus.csv\")\ndf.drop([\"source\"], axis=1, inplace = True)\ndf.dropna(axis = 0, inplace = True)\ndf.rename(columns = {\"english_sentence\" : \"english_sent\", \"hindi_sentence\" : \"hindi_sent\"}, inplace = True)\ndf.head()","3bbb804a":"# Preprocess text\npreprocess_text(df)\n\n# Drop rows with Null values\ndf.drop(df[df[\"english_sent\"] == \" \"].index, inplace = True)\ndf.drop(df[df[\"hindi_sent\"] == \"[start]  [end]\"].index, inplace = True)","8cae8a43":"# Find Sentence Length\ndf[\"eng_sent_length\"] = df[\"english_sent\"].apply(lambda x : len(x.split(' ')))\ndf[\"hindi_sent_length\"] = df[\"hindi_sent\"].apply(lambda x : len(x.split(' ')))","9a933825":"# Get sentences with specific length 20\ndf = df[df[\"eng_sent_length\"] <= 20]\ndf = df[df[\"hindi_sent_length\"] <= 20]\n\n# Take 85K records for training\ndf = df.sample(n = 85000, random_state = 2048)\ndf = df.reset_index(drop = True)\n\n# Defining train, valid, test\ntrain = df.iloc[:80000]\nval = df.iloc[80000:84500]\ntest = df.iloc[84500:]","e74da49a":"# Using TextVectorization to create sentence vectors\nstrip_chars = string.punctuation + \"\u00bf\"\nstrip_chars = strip_chars.replace(\"[\", \"\")\nstrip_chars = strip_chars.replace(\"]\", \"\")\n\n\ndef custom_standardization(input_string):\n    lowercase = tf.strings.lower(input_string)\n    return tf.strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n\neng_vectorization = TextVectorization(\n    max_tokens = vocab_size, output_mode = \"int\", output_sequence_length = sequence_length\n    )\n\nhindi_vectorization = TextVectorization(\n    max_tokens = vocab_size, output_mode = \"int\", output_sequence_length = sequence_length + 1, standardize=custom_standardization\n)\n\neng_vectorization.adapt(df[\"english_sent\"].values)\nhindi_vectorization.adapt(df[\"hindi_sent\"].values)","620e31d1":"# Savng parameters and weights of both vectorizer\npickle.dump({'config': eng_vectorization.get_config(),\n             'weights': eng_vectorization.get_weights()}\n            , open(\"eng_vectorizer.pkl\", \"wb\"))\n\npickle.dump({'config': hindi_vectorization.get_config(),\n             'weights': hindi_vectorization.get_weights()}\n            , open(\"hindi_vectorizer.pkl\", \"wb\"))","6099bdc7":"train_ds = make_dataset(train)\nval_ds = make_dataset(val)","a65c3bcc":"class PositionalEmbedding(layers.Layer):\n    def __init__(self, sequence_len, vocab_size, embed_dim, **kwargs):\n        super(PositionalEmbedding, self).__init__(**kwargs)\n        self.sequence_len = sequence_len\n        self.vocab_size = vocab_size\n        self.embed_dim = embed_dim\n        self.token_embedding = layers.Embedding(\n            input_dim = vocab_size, output_dim = embed_dim\n        )\n        self.position_embedding = layers.Embedding(\n            input_dim = sequence_len, output_dim = embed_dim\n        )\n\n    def call(self, inputs):\n        length = tf.shape(inputs)[-1]\n        positions = tf.range(start = 0, limit = length, delta = 1)\n        embedded_tokens = self.token_embedding(inputs)\n        embedded_positions = self.position_embedding(positions)\n        return embedded_tokens + embedded_positions\n\n    def compute_mask(self, inputs, mask=None):\n        return tf.math.not_equal(inputs, 0)","93138d28":"class TransformerEncoder(layers.Layer):\n    def __init__(self, embed_dim, latent_dim, num_heads, dropout,**kwargs):\n        super(TransformerEncoder, self).__init__(**kwargs)\n        self.embed_dim = embed_dim\n        self.latent_dim = latent_dim\n        self.num_heads = num_heads\n        self.dropout = dropout\n        self.attention = layers.MultiHeadAttention(\n            num_heads = num_heads, key_dim = embed_dim\n        )\n        self.layer_norm1 = layers.LayerNormalization()\n        self.layer_norm2 = layers.LayerNormalization()\n        self.layer_ffn = keras.Sequential(\n            [layers.Dense(latent_dim, activation=\"relu\"), \n             layers.Dropout(dropout),\n             layers.Dense(embed_dim),]\n            )\n        self.supports_masking = True\n    \n    def call(self, inputs, mask = None):\n        if mask is not None:\n            padding_mask = tf.cast(mask[:, tf.newaxis, tf.newaxis, :], dtype=\"int32\")\n\n        attention_output = self.attention(\n            query = inputs, value = inputs, key = inputs, attention_mask = padding_mask\n        )\n        ffn_input = self.layer_norm1(inputs + attention_output)\n        ffn_output = self.layer_ffn(ffn_input)\n        return self.layer_norm2(ffn_input + ffn_output)","a2c2616f":"class TransformerDecoder(layers.Layer):\n    def __init__(self, embed_dim, latent_dim, num_heads, sropout,**kwargs):\n        super(TransformerDecoder, self).__init__(**kwargs)\n        self.embed_dim = embed_dim\n        self.latent_dim = latent_dim\n        self.num_heads = num_heads\n        self.dropout = dropout\n        self.attention1 = layers.MultiHeadAttention(\n            num_heads = num_heads, key_dim = embed_dim\n        )\n        self.attention2 = layers.MultiHeadAttention(\n            num_heads = num_heads, key_dim = embed_dim\n        )\n        self.layer_ffn = keras.Sequential(\n            [layers.Dense(latent_dim, activation=\"relu\"),\n             layers.Dropout(dropout),\n             layers.Dense(embed_dim),]\n        )\n        self.layer_norm1 = layers.LayerNormalization()\n        self.layer_norm2 = layers.LayerNormalization()\n        self.layer_norm3 = layers.LayerNormalization()\n\n        self.supports_masking = True\n    \n    def call(self, inputs, encoder_outputs, mask = None):\n        causal_mask = self.get_causal_attention_mask(inputs)\n        if mask is not None:\n            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n            padding_mask = tf.minimum(padding_mask, causal_mask)\n        \n        attention_output1 = self.attention1(\n            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask \n        )\n        out1 = self.layer_norm1(inputs + attention_output1)\n\n        attention_output2 = self.attention2(\n            query = out1, value = encoder_outputs, key = encoder_outputs, attention_mask = padding_mask\n        )\n        out2 = self.layer_norm2(out1 + attention_output2)\n\n        ffn_output = self.layer_ffn(out2)\n        return self.layer_norm3(out2 + ffn_output)\n\n    def get_causal_attention_mask(self, inputs):\n        input_shape = tf.shape(inputs)\n        batch_size, sequence_length = input_shape[0], input_shape[1]\n        i = tf.range(sequence_length)[:, tf.newaxis]\n        j = tf.range(sequence_length)\n        mask = tf.cast(i >= j, dtype=\"int32\")\n        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n        mult = tf.concat(\n            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n            axis=0,\n        )\n        return tf.tile(mask, mult)","78b50708":"encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\nx = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\nencoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads, dropout,name=\"encoder_1\")(x)\nencoder = keras.Model(encoder_inputs, encoder_outputs)\n\ndecoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\nencoded_seq_inputs = keras.Input(shape=(None, embed_dim), name=\"decoder_state_inputs\")\nx = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\nx = TransformerDecoder(embed_dim, latent_dim, num_heads, dropout,name=\"decoder_1\")(x, encoded_seq_inputs)\nx = layers.Dropout(0.4)(x)\ndecoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\ndecoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n\ndecoder_outputs = decoder([decoder_inputs, encoder_outputs])\ntransformer = keras.Model(\n    [encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\"\n)","6e755859":"transformer.summary()","a728709c":"# Defining callback functions\nearly_stopping = EarlyStopping(patience = 5,restore_best_weights=True)\n\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3)\n\n# Compiling model\ntransformer.compile(\n    optimizer = \"adam\", \n    loss=\"sparse_categorical_crossentropy\", \n    metrics = [\"accuracy\"]\n)\n\n# Training model\ntransformer.fit(train_ds, epochs = 50, validation_data = val_ds, callbacks = [early_stopping, reduce_lr])","cf7497c6":"# Saving weights of model\ntransformer.save_weights(\"eng-hin.h5\")","f062b0a1":"# Sample for testing\neng = \"how are you\"\nprint(\"English Sentence : \",eng)\nprint(\"Translated Sentence : \",decode_sequence(eng))","11ed1dde":"# Calculating BLEU score for test data\neng = test[\"english_sent\"].values\noriginal = test[\"hindi_sent\"].values\n\ntranslated = [decode_sequence(sent) for sent in eng]\nbleu = 0\n\nfor i in range(test.shape[0]):\n    bleu += sentence_bleu([original[i].split()], translated[i].split(), weights = (0.5, 0.5))\n\nprint(\"BLEU score is : \", bleu \/ test.shape[0])","ed42e3b8":"## Defining Functions","3abb80f4":"## Importing Libraries","04f06153":"## Creating Model","1dd9c76d":"## Tokenizing Sentences\n","9743112b":"### This [article](https:\/\/keras.io\/examples\/nlp\/neural_machine_translation_with_transformer\/) by Fran\u00e7ois Chollet helped a lot to understand the implementation of Transformers.","ade2ac8d":"![Encoder Block.png](attachment:bc3e9ff0-517e-48cf-b36a-5a556a7da230.png)","94a4c904":"## Creating Dataset\n","92959c9b":"## Testing Model & Calculating BLEU Score","1a05f862":"![Decoder Block.png](attachment:51ec667e-a632-41ad-92e8-481ea3a76f21.png)","9d673271":"![Transformer.png](attachment:a476764c-1f5d-4f53-b6ce-56db78a0a5fd.png)","3571c4e4":"## Defining Parameters","3fb28b64":"## Reading Data & Preprocessing Text","ea79f06d":"## Training Model"}}