{"cell_type":{"2e71a260":"code","24c57a9c":"code","2b79a4a4":"code","4b87f83d":"code","fc30afa9":"code","66973877":"code","ebbf3db7":"code","e053b4c5":"code","f417b844":"code","f8d0ada4":"code","9b277dce":"code","031195f7":"code","18bf266e":"code","efcae2a2":"code","62afe592":"code","d930fa95":"code","23b442f7":"code","596ed37a":"code","b06ea7f9":"code","fc353cdc":"code","25aaa1cc":"code","670a2fb1":"code","f55a5bcf":"code","d9e0f32b":"code","323f4ada":"code","979401a0":"code","01452765":"code","87fb61c1":"code","c16e0964":"code","3086452a":"code","e0cba7a4":"markdown","21c92b80":"markdown","9f0c9455":"markdown","6a527cc7":"markdown","431a237c":"markdown","58c260ad":"markdown","408ea0f6":"markdown","05620e6e":"markdown","a7a76eaa":"markdown","3a239f37":"markdown","57a193d3":"markdown","7cb4538f":"markdown","95b14ff5":"markdown","963512a2":"markdown","85a9d3a4":"markdown"},"source":{"2e71a260":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport scipy.spatial\nimport matplotlib.pyplot as plt\n\n\n# from transformers import T5Tokenizer, T5ForConditionalGeneration\nfrom tqdm import tqdm\nimport pandas as pd\nfrom torch.utils.data import DataLoader, SequentialSampler, TensorDataset\nimport torch\nimport pandas as pd\nfrom pprint import pprint\n\nimport sys\nimport os\nimport glob\n","24c57a9c":"PATH = '..\/input\/mohler\/mohler_dataset_edited.csv'","2b79a4a4":"dataset = pd.read_csv(PATH)\ndataset.head()","4b87f83d":"dataset.shape","fc30afa9":"!pip install --upgrade simplet5","66973877":"dataset.head()","ebbf3db7":"df = dataset.drop(['score_me', 'score_other', 'score_avg'], axis=1)\ndf.head()","e053b4c5":"q_list = \"question: \" + df['question']                          # questions list to feed the model\nn_list = df['desired_answer'] + df['student_answer'] + \" <\/s>\"  # answers list to feed the model\n\ndict_data = {'source_text': q_list,\n      'target_text': n_list}\n\ndf = pd.DataFrame(dict_data)\ndf.head()      ","f417b844":"df['source_text'][0]","f8d0ada4":"df['target_text'][0]","9b277dce":"df.shape, len(df.source_text.unique()), len(df.target_text.unique())","031195f7":"# splitting data into train and test data\ntrain_data, val_data = train_test_split(df[:-100], test_size=0.2)\ntest_data = df[-100:]\ntrain_data.shape, val_data.shape, test_data.shape","18bf266e":"%%time\nfrom simplet5 import SimpleT5\n\nmodel = SimpleT5()\nmodel.from_pretrained(model_type=\"t5\", model_name=\"t5-base\")\nmodel.train(train_df = train_data,\n            eval_df = val_data, \n            source_max_token_len=128, \n            target_max_token_len=50, \n            batch_size=8, max_epochs=3, use_gpu=True)","efcae2a2":"ls .\/outputs","62afe592":"# let's load the trained model for inferencing\nmodel.load_model(\"t5\",\".\/outputs\/SimpleT5-epoch-2-train-loss-1.3098\", use_gpu=True)","d930fa95":"test_data['source_text']","23b442f7":"q_test = test_data['source_text'][2222]\nq_ans = test_data['target_text'][2222]\n\nprint(\"Question: \", q_test)\nprint('-'*50)\nprint(\"Answer: \",q_ans)","596ed37a":"predicted_ans = model.predict(q_test)[0]\n\nprint(predicted_ans)","b06ea7f9":"y_test = [a for a in test_data['target_text'] ]\n\n# get the predicted answers\ny_pred = []\nfor q in test_data['source_text']:\n    y_pred.append(model.predict(q)[0])","fc353cdc":"from nltk.translate.bleu_score import sentence_bleu\nreference = [\n    'this is a dog'.split(),\n    'it is dog'.split(),\n    'dog it is'.split(),\n    'a dog, it is'.split() \n]\ncandidate = 'it is dog'.split()\nprint('BLEU score -> {}'.format(sentence_bleu(reference, candidate )))\n \ncandidate = 'it is a dog'.split() # differance is the char a of a dog\nprint('BLEU score -> {}'.format(sentence_bleu(reference, candidate)))","25aaa1cc":"print('BLEU score -> {}'.format(sentence_bleu('this is a dog'.split(), 'this is a dog'.split() )))\n","670a2fb1":"pprint(f'Question is:\\n {q_test}')\npprint(f'Answer is:\\n {q_ans}')\npprint(f'Predicted answer is:\\n {predicted_ans}')","f55a5bcf":"print(f\"BLEU score is: {round(sentence_bleu(q_test.split(), predicted_ans.split()),2)}\")","d9e0f32b":"# get the rouge score between test and predicted data\nbleu_score = []\nfor i in range(len(y_pred)):\n    bleu_score.append(round(sentence_bleu(y_test[i].split(), y_pred[i].split()),2))\n    \nprint(f'The BLEU scores of 100 q&a for test data is: {round(np.mean(bleu_score),2)}')","323f4ada":"!pip install rouge-score","979401a0":"from rouge_score import rouge_scorer\n\nscorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\nscores = scorer.score('The quick brown fox jumps over the lazy dog',\n                      'The quick brown dog jumps on the log.')\npprint(scores)","01452765":"pprint(f'Question is: {q_test}')\nprint('-'*50)\npprint(f'Ansewer is: {q_ans}')\nprint('-'*50)\npprint(f'Predicted answer is: {predicted_ans}')","87fb61c1":"# precision\nprint(f\"Precision is: {round(scorer.score(q_test, predicted_ans)['rouge1'][0],2)}\")\n\n# Recall\nprint(f\"Recall is: {round(scorer.score(q_test, predicted_ans)['rouge1'][1],2)}\")","c16e0964":"# get the rouge score between test and predicted data\nprecision_rouge_scores, recall_rouge_scores = [], []\nfor i in range(len(y_pred)):\n    precision_rouge_scores.append(round(scorer.score(y_test[i], y_pred[i])['rouge1'][0],2))\n    recall_rouge_scores.append(round(scorer.score(y_test[i], y_pred[i])['rouge1'][1],2))\n","3086452a":"print(f'The average precision rouge scores of 100 q&a for test data is: {round(np.mean(precision_rouge_scores),2)}')\nprint(f'The average recall rouge scores of 100 q&a for test data is: {round(np.mean(recall_rouge_scores),2)}')","e0cba7a4":"**Rouge scores for 100 Q&A**","21c92b80":"## [ROUGE score ](https:\/\/pypi.org\/project\/rouge-score\/)","9f0c9455":"# [Simple T5 Model](https:\/\/medium.com.\/geekculture\/simplet5-train-t5-models-in-just-3-lines-of-code-by-shivanand-roy-2021-354df5ae46ba)","6a527cc7":"## [BLEU SCORE](https:\/\/www.journaldev.com\/46659\/bleu-score-in-python)","431a237c":"# Evaluation data","58c260ad":"## **`Important notes Mohlar dataset`**\n\n*  **The goal of the dataset is to evaluate the\nmodel in grading the students\u2019 answers by comparing\nthem with the evaluator\u2019s desired answer. It constitutes 2273 answers from 10 assignments and 2 examinations, collected from 31 students for 80 different\nquestions.**\n\n\n* **Each answer in the assignment is graded from 0\n(not correct) to 5 (totally correct) by two evaluators,\nwho are specialized in the field of computer science.\nThe average of the two evaluators\u2019 scores is considered as the standard score of each answer. The answers in examinations are graded from 0 (not correct)\nto 10 (totally correct). To eliminate the clutter of assigned scores in the dataset, (Metzler, 2019) had normalized all the grades of examinations to 0-5. Hence,\nwe intend to use this cleaned data in our experimentation and evaluation.**\n\n* **We calculate the similarity between each student\nanswer ai j and desired answer ai with cosine similarity given in Eq 2. We normalize these scores from 0 to\n1 to scale the similarities and attain the relative measure of similarities. We consider these scores as the\nfeatures of the answers and train them with different\nregression methods.**\n\n* **ELMo performed comparatively better than other\ntransfer learning models, BERT, GPT and GPT-2.\nThese transfer learning models have exhibited poor\nresults on the Mohler dataset compared to the conventional word embeddings**\n\n","408ea0f6":"# Evaluation\n","05620e6e":"### Refrances:\n* [Mohler DataSet](https:\/\/aclanthology.org\/E09-1065.pdf)\n* [Comparative Evaluation of Pretrained Transfer Learning Models on\nAutomatic Short Answer Grading-paper](https:\/\/arxiv.org\/pdf\/2009.01303.pdf)\n\n* [Comparative Evaluation of Pretrained Transfer Learning Models on\nAutomatic Short Answer Grading-code](https:\/\/github.com\/gsasikiran\/Comparative-Evaluation-of-Pretrained-Transfer-Learning-Models-on-ASAG)\n\n* [Exploring the Limits of Transfer Learning with a Unified\nText-to-Text Transformer-paper](https:\/\/arxiv.org\/pdf\/1910.10683.pdf)\n\n* [T5 Huggingface](https:\/\/huggingface.co\/transformers\/model_doc\/t5.html)\n\n\n* [SimpleT5](https:\/\/medium.com\/geekculture\/simplet5-train-t5-models-in-just-3-lines-of-code-by-shivanand-roy-2021-354df5ae46ba)\n\n\n* [Exploring Transfer Learning with T5: the Text-To-Text Transfer Transformer](https:\/\/ai.googleblog.com\/2020\/02\/exploring-transfer-learning-with-t5.html)\n","a7a76eaa":"for running well, you should take the path of the last epoch .. for ex here \"SimpleT5-epoch-2-train-loss-1.3098\"","3a239f37":"# Dataset","57a193d3":"**Sample of evaluation**","7cb4538f":"**BLUE score for 100 Q&A**","95b14ff5":"## **`Important notes about T5`**\nFirst things first, **`T5`** has achieved the state of the art in many GLUE, SuperGLUE tasks along with **translation , **summarization, question answering, sentimant and text classification** benchmarks.\n\nincluding and **classification** tasks (e.g., **sentiment analysis**). We can even apply T5 to **regression** tasks by training it to predict the string representation of a number instead of the number itself.\n\n### Used in:\n\nTranslation, question answering, question answering, text classification, sentiment.\n\n- **It is trained using `teacher forcing`. This means that for training we always need an input sequence and a target sequence.**\n\n<!-- * **Number of attention modules:**\n    * t5-small: 6\n\n    * t5-base: 12\n\n    * t5-large: 24\n\n    * t5-3b: 24\n\n    * t5-11b: 24\n\n -->\n<!-- * **Our results illustrate that ELMo model outperformed on domain-specific ASAG compared to other\ntransfer learning models. The reasons that ELMo may\nhave worked better than the other transfer learning\nmodels on the domain-specific dataset is two-fold.\nFirstly, the assignment of different vectors for the\nsame word in different contexts. This is helpful for\nthe homonyms. Secondly, the availability of significant amount of domain data in the pre-trained corpus\ncompared to the other transfer learning models. The\npretrained data of BERT, GPT and GPT-2 models is\nextensive and the domain which we are testing is comparatively very smaller. This resulted in the similarity\nscores in the range of 10\u22125\nto 10\u22121** -->\n \n","963512a2":"**Sample of evaluation**","85a9d3a4":"* **T5 extract the answer from the question, so here we should feed the model with the question and the exact answer\/s.**\n\n* **Here we add the student answers to desired answers, and renaming the datasef columns to source_text and target_text, which is adapted for modeling.**"}}