{"cell_type":{"d5dfba81":"code","ca9f89f1":"code","b56422a2":"code","bfdfd238":"code","aa324ac8":"code","0b587436":"code","6d36fb1e":"code","2ede4d9a":"code","7c8f8d86":"code","9407dd51":"code","e9991297":"code","551e9afb":"code","3e670eb7":"code","d1652c1b":"code","56aa68d3":"code","830bbed3":"code","b133b88c":"code","a029ed74":"code","861685b8":"code","58d9806a":"code","0099900b":"code","91e85c93":"code","06acc0d7":"code","fffc986f":"code","9b1feaa0":"code","7d06fb50":"code","604160f5":"code","9f1f35b6":"code","4ad4624c":"code","93c9b8f8":"code","75c5ab3e":"code","495f3c25":"code","1c28aa7c":"code","065aeeb1":"code","3ed636de":"code","a5e6f031":"code","980a8c4d":"code","7a88905b":"code","6bbb5099":"code","52b38bf0":"code","4b855054":"code","d6bef6c1":"code","6d32ed6b":"code","6234222b":"code","d0df058d":"code","051a3a17":"code","8814a288":"code","5d15ff5d":"code","1502a5b2":"code","d3412ccb":"code","adcd3148":"code","92a24b36":"code","cd8636ab":"code","4020d2e7":"code","1e0041f9":"code","eeec93d0":"markdown","926b925b":"markdown","3396cfb4":"markdown","4452296f":"markdown","3564832d":"markdown","433b36a1":"markdown","ce48c109":"markdown","c58e6be2":"markdown","580773b9":"markdown","309f5f94":"markdown","0ceacbc0":"markdown","bfa4caf8":"markdown","b7ec994e":"markdown","e71d71d6":"markdown","9dd13e6d":"markdown","238fcc7e":"markdown","45ba6f8f":"markdown","53c81bad":"markdown","7252b1c7":"markdown","86cd4603":"markdown","8742f9c4":"markdown","424d5eca":"markdown","9fbf8e37":"markdown","a96b7b07":"markdown","74850620":"markdown","933f576e":"markdown","75b9b658":"markdown","25d6893b":"markdown"},"source":{"d5dfba81":"#import data into dataframe\nimport pandas as pd\ndata=pd.read_csv('..\/input\/income_evaluation.csv')\nimport warnings\nwarnings.filterwarnings('ignore')","ca9f89f1":"data.columns=[x.strip() for x in data.columns]\ndata=data.apply(lambda x: x.str.strip() if x.dtype=='object' else x)\ndata.head()","b56422a2":"data.describe()","bfdfd238":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nfor i, col in enumerate(data.columns):\n    plt.figure(i)\n    if (data[col].dtype=='object')|(col=='education-num')|(col=='age'):\n        plot=sns.countplot(data[col])\n        \n        for index, item in enumerate(plot.get_xticklabels()):\n            item.set_rotation(45)\n            \n            if col=='age':\n                if index % 5 != 0:\n                    item.set_visible(False)\n                \n            \n    else:\n        plot=sns.distplot(data[col])\n    plot","aa324ac8":"target=data['income']\ntarget=target.map({'<=50K':False,'>50K':True})\ntarget=target.astype('bool')\ndata=data.drop(columns=['income'])","0b587436":"#remove redundant features\ndata=data.drop(columns=['education-num'])\n","6d36fb1e":"sns.heatmap(data.select_dtypes(exclude='object').assign(income=target).corr(),annot=True)","2ede4d9a":"from dython.nominal import associations\nassociations(dataset=data.select_dtypes(include='object').assign(age=data['age'],income=target), nominal_columns='all',plot=True)\nassociations(dataset=data.select_dtypes(include='object').assign(age=data['age'],income=target),theil_u=True, nominal_columns='all',plot=True)\n","7c8f8d86":"from sklearn.preprocessing import label_binarize\n\ndef correct_dtypes(data):\n    cat_cols=data.select_dtypes('object').columns.values\n    data[cat_cols]=data[cat_cols].astype('category')\n    \n    data['sex']=data['sex'].map({'Female':0,'Male':1}).astype('bool')\n    data=data.rename(columns={'sex':'female'})\n    return data\n\ndata=correct_dtypes(data)\ndata.head()","9407dd51":"cat=data.select_dtypes(include='category')\ncat=pd.get_dummies(cat, columns=cat.columns.values,prefix=cat.columns.values)\ndata_oh=pd.concat([data.select_dtypes(exclude='category'),cat],axis=1)\n\ndata_oh.head()","e9991297":"from sklearn.tree import export_graphviz\nfrom sklearn.tree import DecisionTreeClassifier\nfrom IPython.display import Image as PImage\nfrom subprocess import check_call\nfrom PIL import Image, ImageDraw, ImageFont\nimport pydot\n\n#data_oh=data_oh.drop(columns=['fnlwgt'])\ntree=DecisionTreeClassifier().fit(data_oh,target)\n\nwith open(\"tree1.dot\", 'w') as dot:\n    dot = export_graphviz(tree,\n                          out_file=dot,\n                          max_depth = 5,\n                          impurity = True,\n                          class_names = ['<=50K','>50K'],\n                          feature_names = data_oh.columns.values,\n                          rounded = True,\n                          filled= True )\n\n    \n# Annotating chart with PIL\n(graph,) = pydot.graph_from_dot_file('tree1.dot')\n\ngraph.write_png('tree1.png')\nPImage('tree1.png')\n","551e9afb":"import numpy as np\nfeat_impt=pd.DataFrame(data=tree.feature_importances_).T\nfeat_impt.columns=data_oh.columns.values\n\nnames = list(data.columns.values)\n\nfeature_importances=pd.DataFrame()\nfor column in names:\n    value=feat_impt.filter(regex=column)\n    value=value.mean(axis=1)\n    feature_importances[column]=value\n\n#feature_importances=pd.melt(feature_importances)\np=sns.barplot(data=feature_importances)\np.set_xticklabels(p.get_xticklabels(),rotation=45)\nfeature_importances","3e670eb7":"data=data.drop(columns=['fnlwgt'])\ndata_oh=data_oh.drop(columns=['fnlwgt'])","d1652c1b":"#split data into train and test\n#we will train our model with data\/target evalulate our model at the very end with eval_data\/eval_taret\n\nfrom sklearn.model_selection import train_test_split\n\n\ndata, eval_data, target, eval_target = train_test_split(data,target,test_size=.20)","56aa68d3":"from sklearn.base import BaseEstimator, TransformerMixin\nimport numpy as np\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.impute import SimpleImputer\n\nclass TypeSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, dtype):\n        self.dtype = dtype\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        assert isinstance(X, pd.DataFrame)\n        return X.select_dtypes(include=[self.dtype])\n    \nclass StringIndexer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        assert isinstance(X, pd.DataFrame)\n        return X.apply(lambda s: s.cat.codes.replace(\n            {-1: len(s.cat.categories)}\n        ))\n\ntransformer = Pipeline([\n    ('features', FeatureUnion(n_jobs=1, transformer_list=[\n        # Part 1\n        ('boolean', Pipeline([\n            ('selector', TypeSelector('bool')),\n        ])),  # booleans close\n        \n        ('numericals', Pipeline([\n            ('selector', TypeSelector(np.number)),\n            ('imputer',SimpleImputer()),\n            ('scaler', StandardScaler()),\n            \n        ])),  # numericals close\n        \n        # Part 2\n        ('categoricals', Pipeline([\n            ('selector', TypeSelector('category')),\n            ('labeler', StringIndexer()),\n            ('encoder', OneHotEncoder()),\n        ]))  # categoricals close\n    ])),  # features close\n])  # pipeline close","830bbed3":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\nestimators=[\n    ('logistic',LogisticRegression(solver='liblinear',penalty='l2')),\n    ('lasso',LogisticRegression(solver='liblinear',penalty='l1')),\n    ('ridge',RidgeClassifier()),\n    ('elasticnet',SGDClassifier(loss='log', penalty='elasticnet')),\n    #('decision_tree',DecisionTreeClassifier()),\n    ('random_forest',RandomForestClassifier()),\n    ('xgb',XGBClassifier(ojective='reg:logistic')),\n    ('svc',LinearSVC()),\n    ('deep_nn',MLPClassifier()),\n    ('knn',KNeighborsClassifier(n_neighbors=5,weights='distance',algorithm='auto'))\n]\n\npipes={}\nfor model in estimators:\n    pipe=Pipeline(steps=[('data_prep',transformer),model])\n    pipe.fit(data,target)\n    pipes[pipe.steps[1][0]]=pipe\n\n","b133b88c":"from sklearn.model_selection import KFold,cross_validate\nfrom sklearn.metrics import make_scorer, f1_score, accuracy_score,roc_auc_score,log_loss\n\n_metrics={'f1':make_scorer(f1_score),'auc':make_scorer(roc_auc_score),\n         'accuracy':'accuracy','logloss':make_scorer(log_loss)}\n\nestimator_names=[model[0] for model in estimators]\n\ndef plot_estimators(estimators=estimator_names,n_splits=5,metrics=['f1','auc','accuracy','logloss']):\n    metrics={key : _metrics[key] for key in metrics}\n    scorers=[]\n    labels=[]\n    for pipe_name in pipes.keys():\n        if pipe_name in estimators:\n            pipe=pipes[pipe_name]\n            labels.append(pipe_name)\n            kf=KFold(n_splits)\n            model_score=cross_validate(pipe,data,target,scoring=metrics,cv=kf)\n            scorers.append(model_score)\n    \n    score_lists={}\n    for metric in metrics:\n        score_lists[metric]=[score['test_'+metric] for score in scorers]\n    \n    for  i,(title, _list) in enumerate(score_lists.items()):\n        plt.figure(i)\n        plot=sns.boxplot(data=_list).set_xticklabels(labels, rotation=45)\n        plt.title(title)\n","a029ed74":"metrics={'f1':make_scorer(f1_score),'auc':make_scorer(roc_auc_score),\n         'accuracy':'accuracy','logloss':make_scorer(log_loss)}","861685b8":"plot_estimators()","58d9806a":"from sklearn.model_selection import GridSearchCV\n\ndef tune_param(model,param_grid,refit='auc',chart=None,data=data,target=target,cv=5):\n    \n    param_grid={model+'__'+key : param_grid[key] for key in param_grid.keys()}\n\n    xgbcv=GridSearchCV(pipes[model],param_grid,scoring=metrics,refit=refit,cv=cv)\n    xgbcv.fit(data,target)\n\n    print('best score: '+str(xgbcv.best_score_))\n    print('best params: '+str(xgbcv.best_params_))\n    results=pd.DataFrame(xgbcv.cv_results_)\n    \n    if 'line' in chart:\n        for i,param in enumerate(param_grid.keys()):\n            graph_data=results[['param_'+param,'mean_test_'+refit,'mean_train_'+refit]]\n            graph_data=graph_data.rename(columns={'mean_test_'+refit:'test','mean_train_'+refit:'train'})\n            graph_data=graph_data.melt('param_'+param, var_name='type',value_name=refit)\n            plt.figure(i)\n            plot=sns.lineplot(x='param_'+param,y=refit,hue='type',data=graph_data)\n            \n    if 'heatmap' in chart:\n        assert len(param_grid) == 2,  'heatmap only works with 2 params, {} passed'.format(str(len(param_grid)))\n        \n        param1=list(param_grid.keys())[0]\n        param2=list(param_grid.keys())[1]\n\n        graph_data=results[['param_'+param1,'param_'+param2,'mean_test_'+refit]]\n        graph_data=graph_data.pivot(index='param_'+param1,columns='param_'+param2,values='mean_test_'+refit)\n        sns.heatmap(graph_data,annot=True,xticklabels=True,yticklabels=True).set(xlabel=param2,ylabel=param1)\n","0099900b":"pipes['xgb'].named_steps['xgb'].get_params()","91e85c93":"param_grid={'n_estimators': [100,200,300,400,500,600,700,800,900]} \ntune_param('xgb',param_grid,chart='line')","06acc0d7":"pipes['xgb'].set_params(**{'xgb__n_estimators': 1000})","fffc986f":"param_grid={'max_depth':[3,5,7,9]}\ntune_param('xgb',param_grid,chart='line')","9b1feaa0":"pipes['xgb'].set_params(**{'xgb__max_depth': 3})","7d06fb50":"param_grid={'learning_rate':[0.0001, 0.001, 0.01, 0.1, 0.2, 0.3]}\ntune_param('xgb',param_grid,chart='line')","604160f5":"pipes['xgb'].set_params(**{'xgb__learning_rate': 0.1})","9f1f35b6":"param_grid={'subsample':[0.8,0.85,0.9,0.95,1]}\ntune_param('xgb',param_grid,chart='line')\n","4ad4624c":"pipes['xgb'].set_params(**{'xgb__learning_rate': 0.95})","93c9b8f8":"param_grid={'colsample_bytree':[0.3,0.5,0.7,0.9,1]}\ntune_param('xgb',param_grid,chart='line')","75c5ab3e":"pipes['xgb'].set_params(**{'xgb__colsample_bytree': 0.3})","495f3c25":"param_grid={'gamma':[0,1,5]}\ntune_param('xgb',param_grid,chart='line')","1c28aa7c":"pipes['xgb'].set_params(**{'xgb__gamma': 5})\npipes['xgb'].named_steps['xgb'].get_params()","065aeeb1":"pipes['random_forest'].named_steps['random_forest'].get_params()","3ed636de":"pipes['random_forest'].set_params(**{'random_forest__n_estimators': 100})\n","a5e6f031":"param_grid={'max_depth':[3,8,13,18]}\ntune_param('random_forest',param_grid,chart='line')","980a8c4d":"#we see that test auc plateaus at around 13-14ish\n#for efficiency sake lets set max depth at 13\npipes['random_forest'].set_params(**{'random_forest__max_depth': 13})\n","7a88905b":"param_grid={'max_leaf_nodes':[5,10,15,20,25]}\ntune_param('random_forest',param_grid,chart='line')","6bbb5099":"param_grid={'n_estimators':[10,50,100,150,200,500]}\ntune_param('random_forest',param_grid,chart='line')","52b38bf0":"pipes['random_forest'].set_params(**{'random_forest__n_estimators': 50})","4b855054":"pipes['logistic'].named_steps['logistic'].get_params()","d6bef6c1":"pipes['lasso'].named_steps['lasso'].get_params()","6d32ed6b":"warnings.filterwarnings('ignore')\nparam_grid={'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000], \n           'tol':[0.001, 0.01, 0.1, 1, 10, 100, 1000]}\ntune_param('logistic',param_grid,chart='heatmap')","6234222b":"pipes['logistic'].set_params(**{'logistic__C': 1000, 'logistic__tol': 0.001})\npipes['lasso'].set_params(**{'lasso__C': 1000, 'lasso__tol': 0.001})\n","d0df058d":"param_grid={'C': [0.001, 0.01, 0.1, 1, 10]}\ntune_param('svc',param_grid,chart='linear')","051a3a17":"pipes['svc'].set_params(**{'svc__C': 1})","8814a288":"plot_estimators(['xgb','svc','logistic','lasso','random_forest'])","5d15ff5d":"from sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import ShuffleSplit\n\nt=transformer\nt.fit(data)\nt=t.transform(data)\ndef plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):\n    \"\"\"Generate a simple plot of the test and training learning curve\"\"\"\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt\n\nto_plot=['xgb','random_forest','lasso','ridge']\nfor name in pipes.keys():\n    if name in to_plot:\n        g=plot_learning_curve(pipes[name].named_steps[name],name+' learning curves',t,target,cv=KFold(4),n_jobs=4)","1502a5b2":"tree_based=['random_forest','xgb']\nfor name in pipes.keys(): \n    if name in tree_based:\n        feat_impt=pipes[name].named_steps[name].feature_importances_\n        graph_data=pd.DataFrame()\n\n        graph_data['feature']=data_oh.columns.values\n        graph_data['importance']=feat_impt\n        graph_data_top=graph_data.nlargest(30,'importance')\n\n        plt.figure(figsize=(6.4,6))\n        g=sns.barplot(y='feature',x='importance',data=graph_data_top,orient='h')\n        g.set_ylabel('Features',fontsize=12)\n        g.set_xlabel('Relative Importance')\n        g.set_title(name + \" feature importance\")\n        g.tick_params(labelsize=8)\n        ","d3412ccb":"ensemble_results=pd.DataFrame()\nfor name,pipe in pipes.items():\n    ensemble_results[name]=pipe.predict(eval_data)\nsns.heatmap(ensemble_results.corr(),annot=True)","adcd3148":"del pipes['ridge'],pipes['decision_tree'],pipes['logistic']","92a24b36":"def print_predictions(target,predictions):\n    print('auc: '+str(roc_auc_score(target,predictions)))\n    print('f1: '+str(f1_score(target,predictions)))\n    print('accuracy: '+str(accuracy_score(target,predictions)))\n    print('logloss: '+str(log_loss(target,predictions)))","cd8636ab":"from sklearn.ensemble import VotingClassifier\nfrom sklearn.base import clone\n\nestimators=[(pipe.steps[1][0],clone(pipe.steps[1][1])) for pipe in pipes.values()] \nvote=Pipeline(steps=[('data_prep',transformer),('voter',VotingClassifier(estimators))])\nvote.fit(data,target)\npredictions=vote.predict(eval_data)\n\nprint_predictions(eval_target,predictions)\n\n","4020d2e7":"for name in pipes.keys():\n    print(name)\n    predictions=pipes[name].predict(eval_data)\n    print_predictions(eval_target,predictions)\n    print()","1e0041f9":"from itertools import combinations\n\nfinal_estimators=['random_forest','xgb','deep_nn','elasticnet']\n\ncombos=[]\nfor L in range(2, len(final_estimators)+1):\n    for subset in combinations(final_estimators, L):\n        combos.append(list(subset))\n\n\ncombo_names=[]\nauc=[]\nf1=[]\nlogloss=[]\naccuracy=[]\n\nfor combo in combos:\n    estimators=[(name,clone(pipes[name].named_steps[name])) for name in combo] \n    vote=Pipeline(steps=[('data_prep',transformer),('voter',VotingClassifier(estimators))])\n    vote.fit(data,target)\n    predictions=vote.predict(eval_data)\n\n    auc.append(roc_auc_score(eval_target,predictions))\n    accuracy.append(accuracy_score(eval_target,predictions))\n    logloss.append(log_loss(eval_target,predictions))\n    f1.append(f1_score(eval_target,predictions))\n    combo_names.append(str(list(combo)))\n    \nscore=pd.DataFrame()\nscore['combo']=combo_names\nscore['auc']=auc\nscore['f1']=f1\nscore['accuracy']=accuracy\n\nscore\n","eeec93d0":"It looks like the tree is grabbing onto the fnlwgt feature pretty strongly. Earlier, we found that there was a near-zero correlation between income and fnlwgt. This feature could create a lot of noise that will overfit our models, so let's remove this it from our dataset.","926b925b":"### 2.2: Creating the Transformation Pipeline\nThe first step of our estimator pipelines will be a transformer pipeline. The transformer will complete the following transformations:\n\n    1. Numerical Data:\n        a) Imputation transformer to fill in missing values should they exist\n        b) Standard Scaler to normalize our numerical data\n        \n    2. Catecorical Data\n        a) String indexer to index each of our strings with a numerical value\n        b) One-hot encoder to allow for categorical variables to be used in our models.","3396cfb4":"#### Regression Matrix for Categorical Variables\nIn order to produce a correlation matrix for categorical variables, I utilized the Cramer's V measure, which is based on Pearson's chi-squared statistic. I also used uncertainty coefficients as an asymmetric measure between the categorical variables. You can learn more about this [here](https:\/\/towardsdatascience.com\/the-search-for-categorical-correlation-a1cf7f1888c9).\n\nThe link for the github repo of the dython package is https:\/\/github.com\/shakedzy\/dython","4452296f":"## Part 5: Ensemble\n\n### 5.1: Learning Curves\n\nSince we have a large number of features, let's plot some learning curves to help us visualize whether the model overfits in relation to the number of training examples","3564832d":"### 4.4: Parameter Tuning: SVM","433b36a1":"### 1.3: Feature Analysis","ce48c109":"## Part 4: Hyper-Parameter Tuning","c58e6be2":"### 4.1: Parameter Tuning: XGB","580773b9":"### 1.5: Feature Correlation","309f5f94":"### 3.2: Initial Model Scoring\n\nWe will test our model with initial parameters using four metrics: roc-auc, accuracy, and log_loss","0ceacbc0":"Looks like our best voter combination is xgb, deep_nn, and elasticnet.","bfa4caf8":"### 1.7: Decision Tree as EDA\nSince we will be using tree-based methods as apart of our ensemble, I want to see how a basic decision tree would interact with our dataset. I will start by by creating a one-hot encoded dataset to fit onto a decision tree classifier.","b7ec994e":"### 5.2: Tree-Based Feature Importance","e71d71d6":"## Part 3: Model Creation\n\n### 3.1: Creating Estimator Pipelines\n\nFirst, I will create several estimators to be used in our ensembele model. Next, I will create a dictionary of pipelines corresponding to each each estimator, with each including the transformer pipeline we just built as the first step. Each model pipeline will be fit with our training data.","9dd13e6d":"#### Next Step: sum all of the dummy variable importances and divide by total","238fcc7e":"### 5.5: Ensemble Creation and Final Testing\n\nLet's make a voting ensemble including all of our estimators and compare it to the performance of each individual estimator","45ba6f8f":"## Income Classification Ensemble Modeling with Sklearn Pipelines\n\nThe goal of this kernel is to use an ensemble of several different models to predict whether a person will make more or less than 50K a year. The steps of this process are the following:\n\n    1) Conduct EDA of the dataset, remove redundant features\n    2) Create format data and create sklearn pipelines to help format data for modeling\n    3) Create the models and run initial cross-validation testing\n    4) Tune hyper-parameters of all of our models\n    5) Create ensemble estimator and compare with individual estimators.\n    \n    \n#### Glossary:\n    1. EDA and Preprocessing\n        1.1. Load Data\n        1.2. Format Feature Names\n        1.3. Feature Analysis\n        1.4. Remove Redundant Features\n        1.5. Feature Correlation\n        1.6. Datatype Preprocessing\n        1.7. Decision Tree as EDA\n        \n    2. Data Transformation Pipeline\n        2.1. Create a Holdout Set\n        2.2. Creating the Pipeline\n    \n    3. Model Creation\n        3.1. Creating Estimator Pipelines\n        3.2. Initial Model Scoring\n    \n    4. Hyper-Parameter Tuning\n        4.1. Parameter Tuning: XGB\n        4.2. Parameter Tuning: Random Forest\n        4.3. Parameter Tuning: Logistic Regression\n        4.4. Parameter Tuning: SVM\n        4.5. Post-Tuning Scores\n        \n    5. Ensembling\n        5.1. Learning Curves\n        5.2. Tree-Based Feature Importance\n        5.3. Estimator Prediction Correlation\n        5.4. Estimator Selection\n        5.5. Ensemble Creation and Final Testing\n        ","53c81bad":"### 1.6: Datatype Processing\nWe will start by making sure every column of our dataframe is of either numerical, categorical, or boolean datatypes. This will allow us to pass the data through a transformation pipeline.","7252b1c7":"### 1.2: Format Feature Names","86cd4603":"### 4.5: Post-Tuning Scores\n\nLets plot our estimators we tuned to see how much the new parameters improve performance.","8742f9c4":"### 1.4: Remove Redundant Features\n\nWe will remove 'education-num' for now, as we will use the category names so we can interperet a decision tree. Before modeling we will one-hot encode the education feature.","424d5eca":"It looks like a voting classifier using all of our estimators performed worse than our best individual estimator, xgboost. I'm now going to test out several combinations of estimators to see if we can't create a voting classifier that performs better than an individual xgboost.\n\nFor the sake of computation time, I will limit our final estimators to random_forest, xgb, deep_nn, and elastic_net.","9fbf8e37":"### 5.3:  Estimator Prediction Correlation\n\nBefore we create ensemble estimators, we should see how similar our models predict our holdout set. We will do this by predicting the holdout set and then creating a correlation heatmap between the estimator predictions.","a96b7b07":"## Part 1: EDA and Preprocessing\n\n### 1.1: Load Data","74850620":"### 4.2: Parameter Tuning: Random Forest","933f576e":"### 5.4: Estimator Selection\n\nThere appears to be some difference between our estimator predicionts. I'm going to remove the decision tree, ridge, and logistic estimators because of poor performance and high correlation with other estimators.","75b9b658":"### 4.3: Parameter Tuning: Logistic Regression","25d6893b":"## Part 2: Data Transformation Pipeline\nIn order to simplify data transformation among several models, and to make sure everything is consistent, I opted to use sklearn pipelines to transform our data before we throw it into an estimator.\n\n### 2.1: Creating a Holdout Set\nWe will create a holdout set that we will use to evalulate our model at the very end. This will hopefully prevent data leakage into our models."}}