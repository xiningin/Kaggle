{"cell_type":{"d591f288":"code","5fe868b9":"code","fd213cfb":"code","18857db1":"code","f5e2560e":"code","5ddc8085":"code","674b354b":"code","d7d54265":"code","7ee5111e":"code","2336862c":"code","4ac8b082":"code","5da3f772":"code","48cc0849":"code","3ff229ea":"code","853a6337":"code","7cfaf72d":"code","b217794c":"markdown","97a876db":"markdown","000d732d":"markdown","4864ec0a":"markdown","6b6f3395":"markdown","ff44c395":"markdown","19fde418":"markdown","017b5262":"markdown","0a15877f":"markdown","9a3da0e7":"markdown","7f1c928c":"markdown","47c8bc52":"markdown","445b6ce0":"markdown","13e94f36":"markdown","e80fb0d6":"markdown","86d5f665":"markdown","3f31bb88":"markdown","c66354e2":"markdown"},"source":{"d591f288":"import pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt \nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost as xgb","5fe868b9":"def date_to_jd(date):\n#transform date (either julian or gregorian) into a julian day number\n    y = date[0]\n    mo = date[1]\n    d = date[2]\n    h = date[3]\n    mn = date[4]\n    s = date[5]\n\n    if date[:3] >= [1582,10,15]:\n        #gregorian date\n        return 367*y - (7*(y+int((mo+9)\/12)))\/\/4 - (3*(int((y+(mo-9)\/7)\/100)+1))\/\/4+(275*mo)\/\/9+d+1721028.5+h\/24+mn\/(24*60)+s\/86400\n    elif date[:3] <= [1582,10,4]:\n        #julian date\n        return 367*y - (7*(y+5001+int((mo-9)\/7)))\/\/4+(275*mo)\/\/9+d+1729776.5+h\/24+mn\/(24*60)+s\/86400\n\ndef jd_to_date(jd):\n    Z = int(jd+0.5)\n    F = (jd+0.5)%1\n    if Z < 2299161:\n        A = Z\n    else:\n        g = int((Z - 1867216.25) \/ 36524.25)\n        A = Z + 1 + g - g\/\/4 \n\n    B = A + 1524\n    C = int((B-122.1) \/ 365.25)\n    D = int(365.25 * C)\n    E = int((B-D) \/ 30.6001)\n \n    d = B - D - int(30.6001*E) + F\n    if E<14:\n        mo = E-1\n    else:\n        mo = E-13    \n\n    if mo >2:\n        y = C- 4716\n    else:\n        y = C - 4715\n    \n    return str(y)+'-'+mak_2_dig(mo)+'-'+mak_2_dig(int(d))","fd213cfb":"def mak_2_dig(x):\n#transforms all integers bewteen 0 and 99 into a 2-digit string\n    if x<10:\n        return '0'+str(x)\n    else:\n        return str(x)\n\n\ndef transf_date(s):\n    s = s.split()\n    s[1] = str(dic_months[s[1]])\n    return s[0]+':'+s[1]+':'+s[2]\n\ndic_months = {'January':1, 'February': 2, 'March':3, 'April':4, 'May':5, 'June': 6, 'July':7, 'August':8, 'September':9, 'October':10, 'November':11, 'December':12}","18857db1":"df = pd.read_csv(\"..\/input\/solar.csv\")\ndf = df.loc[:, ['Calendar Date', 'Eclipse Time', 'Eclipse Type']]\n#combine Date and Time to Time JD\ndf['Calendar Date'] = df['Calendar Date'].apply(lambda x:transf_date(x))\ndf['Time'] = df.loc[:,['Calendar Date', 'Eclipse Time']].apply(lambda x: x[0]+':'+x[1], axis = 1) \ndf = df.drop(['Calendar Date', 'Eclipse Time'], axis=1)\ndf['Time JD']=df['Time'].apply(lambda x : date_to_jd([int(j) for j in x.split(':')]))\ndel df['Time']","f5e2560e":"df = df[df['Eclipse Type'].str[0] != 'P']     #exclude partial eclipses\n##possibly exclude data prior to date\n#date_start = date_to_jd([1400, 1, 1, 0, 0, 0])   #only use data after this date\n#df = df[df['Time JD'] > date_start]\nt_between = df['Time JD'].diff().tolist()   #count days between consecutive eclipses\nt_between = t_between[1:-1]                 #drop first and last NaN","5ddc8085":"plt.hist(t_between,int(max(t_between))+1)\nplt.xlabel('Time (days)')\nplt.show()","674b354b":"set([int(d) for d in t_between])","d7d54265":"today = date_to_jd([2017, 3, 1, 0, 0,0])\n\ndf_before = df[df['Time JD'] <= today]\ndf_after = df[df['Time JD'] > today]\ndates_before = df_before['Time JD'].tolist()\ndates_after = df_after['Time JD'].tolist()\ndiff_before = [int(j) for j in t_between[:len(dates_before)-1]]  #recorded differences between past ecl.\ndiff_after =  [int(j) for j in t_between[len(dates_before)-1:]]   #differences betw. future ecl. to be predicted","7ee5111e":"L = len(diff_before)\/\/82","2336862c":"X = []\ny = []\nfor j in range(len(diff_before)-L):\n    X.append(diff_before[j:j+L])\n    y.append(diff_before[j+L]) \n \nX = np.array(X)\ny = np.array(y)","4ac8b082":"p = 0.15 #fraction used for validation\nX_val = X[int((1-p)*len(X)):]\ny_val = y[int((1-p)*len(y)):]\n\nX_train = X[:int((1-p)*len(X))]\ny_train = y[:int((1-p)*len(y))]","5da3f772":"0#lrn = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(100,100), learning_rate = 'adaptive')\n#lrn = SVC(kernel = 'linear', C=1000)     #just takes too long\nlrn = RandomForestClassifier(1000)\nlrn.fit(X_train, y_train)","48cc0849":"c = 0\ns = lrn.predict(X_val)\nfor i in range(len(y_val)):\n    if np.absolute(y_val[i] - s[i]) > 1:\n        c += 1       #count wrong predictions\nprint(\"Predictions more than 1 day off: {}%\".format(((c*1000)\/\/len(y_val))\/10))\nprint(\"Tested on {} samples\".format(len(y_val)))","3ff229ea":"lrn = xgb.XGBClassifier(learning_rate = 0.05, max_depth = 10, objective = \"reg:linear\")\nlrn.fit(X_train, y_train)\n\nc = 0\ns = lrn.predict(X_val)\nfor i in range(len(y_val)):\n    if np.absolute(y_val[i] - s[i]) > 1:\n        c += 1       #count wrong predictions\nprint(\"Predictions more than 1 day off: {}%\".format(((c*1000)\/\/len(y_val))\/10))\nprint(\"Tested on {} samples\".format(len(y_val)))","853a6337":"N_future = 10   #number of predictions into the future\nlrn.fit(X, y)   # optionally train the model again on the 'entire' past \n#prediction part: append every further prediction to the feature set\n\nxx = np.array(X[-1])\nxx = np.roll(xx,-1)\nxx[-1] = y[-1]\ny_pred = []\nd_last = dates_before[-1]\n\nfor i in range(N_future):\n    yy = lrn.predict([xx])\n    y_pred.append(yy[0])\n    xx = np.roll(xx,-1)\n    xx[-1] = yy\n\n\nprint(\"Prediction:      \", y_pred)\nprint(\"Calculated\/true: \", diff_after[:N_future])","7cfaf72d":"days_pred = [d_last + i for i in np.cumsum(y_pred)]\nprint(\"Date (predicted)   Date (calculated\/true)\")\nfor i in range(len(days_pred)):\n    print(\"  \"+jd_to_date(days_pred[i])+\"          \"+jd_to_date(dates_after[i]))","b217794c":"# Predictions\n\nI will compare some ML-Classifiers to predict the next eclipses. First, we specify 'today', which separates the data into learning and testing data, then we split the dates and times between accordingly:","97a876db":"This is pretty good - in the case where we are not too unlucky, the predictions should be accurate up to a couple of days. Of course, once a single prediction is off, all further predictions are rendered worthless. But we need to keep in mind that this is how we have designed our model - to just predict the next eclipse.","000d732d":"Among the three algorithms chosen above, the Random Forest Classifier produces the best results in a reasonable time, with an error rate greater than 5%.\n\nBut let's compare this result with a prediction model using XGBoost (optimization of the parameters is not shown here).","4864ec0a":"In order to perform some kind of validation, I split the training data again - namely, I take the last fraction of size p of the training set for this (it doesn't make sense to randomly pick the validation set, since adjacent entries of X are highly correlated):","6b6f3395":"Now we test it on the validation set X_val (and accept deviations of +-1 days):","ff44c395":"The predictor should always predict the duration to the next eclipse from the past L eclipses. We specify this L (this seems to be near optimal, which is verified through validation):","19fde418":"This is great - XGBoost has an even lower error rate! We will stick to XGBoost for future predictions.\n\nHaving tested the quality of the prediction, we train the classifier again on the entire training set X, and then predict the next eclipses. Here, I will not only attempt to predict the next eclipse, but also some more eclipses ahead. Those further predictions will be based on the previous predictions, so errors are more likely to propagate.","017b5262":"Functions for converting dates to a JD-format, and back (not important for the presenation; you can skip this):","0a15877f":"First we check the durations beween eclipses for regularities. ","9a3da0e7":"It might be worth noticing that the durations fulfill quite simple ratios: Based on l_0 = 147.5 days (the lowest of the times; almost never happening), we find that the ratios of the other durations and l_0 are roughly:\n\n6\/5, 11\/5, 12\/5, 17\/5, 23\/5","7f1c928c":"This is greater than expected - there are only very few possibilities of times between eclipses! Essentially only 6 (we ignore uncertainties of +-2 days):","47c8bc52":"By analyzing the patterns of total eclipses (i.e. non-partial) I aim to predict upcoming eclipses. The key idea here is to count the time (e.g. days) between consecutive eclipses and apply a ML-Algorithm on this pattern.","445b6ce0":"From our modern worldview (we know about the earth moving around the sun, and the moon orbiting the earth), we know that the processes causing eclipses are periodic. Therefore, it might not be surprising to find visible patterns in the occurrence of eclipses. We here look at the time elapsing between consecutive eclipses:","13e94f36":"The prediction is for the days between eclipses. In order to predict dates of eclipses, we compute the actual dates from this data:","e80fb0d6":"Some helper functions for date-conversion (skip as well):","86d5f665":"We pick and train a learning algorithm on the (1-p)-training fraction:","3f31bb88":"Let's get started. First, we read the data, drop unnecessary columns, and convert the times of eclipses to a JD-format.","c66354e2":"Then we create the training set from diff_before:"}}