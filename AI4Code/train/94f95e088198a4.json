{"cell_type":{"55a1632b":"code","e10e1480":"code","88e384a8":"code","ae4ec291":"code","59335969":"code","f3627472":"code","3f973ac4":"code","1a14d7d7":"code","a69f326c":"code","b7843059":"code","96207e77":"code","741d4b95":"code","3a640dab":"code","873e7f1a":"code","7e9d7d2a":"code","4867f0a0":"code","1e2d75ce":"code","a4eeba2b":"code","7a5f4c9f":"code","194043df":"code","ef8487db":"code","90a8e7a4":"code","24ca175f":"code","76156db5":"code","3e5c30e6":"code","35f5f238":"code","c772bb5b":"code","22bbaf22":"code","03892185":"code","6223aeb2":"code","5619c034":"code","609175c1":"code","ecd067c5":"markdown","fab66549":"markdown","349b42a0":"markdown"},"source":{"55a1632b":"import numpy as np \nimport pandas as pd \nimport seaborn as sns \nimport matplotlib.pyplot as plt\n\nfrom sklearn.neural_network import MLPRegressor\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error\n\nfrom scipy.signal import savgol_filter","e10e1480":"data_raw = pd.read_csv('..\/input\/electric-motor-temperature\/pmsm_temperature_data.csv')\ndata_copy = data_raw.copy(deep=True)","88e384a8":"data_raw.head()","ae4ec291":"# The task is to predict the 4 target variables ('pm', 'stator_yoke', 'stator_winding', 'stator_tooth') for\n# for entries with 'profile_id' equal to 65 and 72 using data with smaller values of 'profile_id'.\n# For the purposes of this notebook target variables only for entries with 'profile_id' = 65 will be predicted\n\ntarget_variables = ['pm', 'stator_yoke', 'stator_winding', 'stator_tooth']\ndrop_cols = ['profile_id']","59335969":"### 'torque' column is dropped according to the task \n\ndata_copy.drop(columns=['torque'], inplace=True)\n\n### 'profile_id' less than 65 for the training set\n\nprofile_list = np.array([i for i in data_raw['profile_id'].unique() if i < 65])","f3627472":"profile_list","3f973ac4":"cols = [item for item in list(data_copy.columns) if item not in drop_cols + target_variables]","1a14d7d7":"cols","a69f326c":"# apply MinMaxScaler in order to improve performance of MLPRegressor\n\nscaler = MinMaxScaler()\ndata_copy[cols] = scaler.fit_transform(data_copy[cols])","b7843059":"data_copy[cols]","96207e77":"data_train = data_copy[data_copy['profile_id'].isin(profile_list)]\n\ndata_test = data_copy[data_copy['profile_id'] == 65]","741d4b95":"data_train","3a640dab":"# Take a look at how predictor variables behave \nfor profile_id in [4, 11, 30, 45, 52]:\n    print('id: ', profile_id)\n    plt.figure(figsize=(26, 3))\n    temp_data = data_train[data_train['profile_id'] == profile_id]\n    i=1\n    for col in cols: \n        sub = plt.subplot(1,7,i)\n        i+=1\n        plt.plot(temp_data[col])\n        sub.set(xlabel='index', ylabel=col)\n    plt.show()","873e7f1a":"# Take a look at how target variables behave  \nfor profile_id in [4, 11, 30, 45, 52]:\n    print('id: ', profile_id)\n    plt.figure(figsize=(22, 3))\n    temp_data = data_train[data_train['profile_id'] == profile_id]\n    i=1\n    for col in target_variables: \n        sub = plt.subplot(1,4,i)\n        i+=1\n        plt.plot(temp_data[col])\n        sub.set(xlabel='index', ylabel=col)\n    plt.show()","7e9d7d2a":"corr = data_train.corr()\n\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\nf, ax = plt.subplots(figsize=(10, 10))\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","4867f0a0":"X_train = data_train.drop(columns = target_variables + drop_cols)\nY_train = data_train[target_variables]","1e2d75ce":"X_train.head()","a4eeba2b":"reg = MLPRegressor(hidden_layer_sizes=(49), max_iter=2000, activation='tanh', verbose=False, random_state=1)","7a5f4c9f":"%%time\nreg.fit(X_train, Y_train)","194043df":"X_test = data_test.drop(columns = target_variables + drop_cols)\nY_test_actual = data_test[target_variables]","ef8487db":"Y_test_prediction = reg.predict(X_test)  ","90a8e7a4":"prediction_df = pd.DataFrame(Y_test_prediction)","24ca175f":"prediction_df.head()","76156db5":"prediction_df = prediction_df.rename(columns={0: 'pm_pred', 1: 'stator_yoke_pred', \n                                              2: 'stator_winding_pred', 3: 'stator_tooth_pred'})","3e5c30e6":"Y_test_actual","35f5f238":"prediction_df = Y_test_actual.reset_index().merge(prediction_df, left_index = True, right_index=True).set_index('index')","c772bb5b":"plt.figure(figsize=(30, 7))\nprint('actual')\n\nfor idx,col in enumerate(target_variables): \n    plt.subplot(1, 4, idx+1)\n    plt.plot(prediction_df[col + '_pred'])\n    plt.plot(prediction_df[col], color='red')\n    plt.legend(loc=\"upper right\")\nplt.show()","22bbaf22":"predicted_cols = [col + '_pred' for col in target_variables] \nsmoothed_cols = [col + '_smoothed' for col in predicted_cols] ","03892185":"for column in predicted_cols:\n    prediction_df[column+'_smoothed'] = savgol_filter(prediction_df[column], \n                                                      501, 1)","6223aeb2":"prediction_df.head()","5619c034":"plt.figure(figsize=(30, 7))\nprint('actual')\n\nfor idx,col in enumerate(target_variables): \n    plt.subplot(1, 4, idx+1)\n    plt.plot(prediction_df[col + '_pred_smoothed'], label=col+': Predicted value')\n    plt.plot(prediction_df[col], color='red', lw=1 , label=col+': Actual value')\n    plt.legend(loc=\"upper right\")\nplt.show()","609175c1":"for column in target_variables:\n    print('column: ', column)\n    print('no smooth: ', mean_squared_error(Y_test_actual[column], prediction_df[column+'_pred'].to_numpy()))\n    print('smooth: ', mean_squared_error(Y_test_actual[column], prediction_df[column+'_pred_smoothed'].to_numpy()), '\\r\\n')\n    ","ecd067c5":"the predictions for the \"stator_...\" variables appear to be somewhat precise, \nhowever, the slope formed by \"pm\" values does not seem to be predicted accurately. \nA possible solution to that might be to restructure the neural network by adding more hidden layers, however, that is going to increase the computational comlexity and, therefore, training time","fab66549":"'stator_yoke', 'stator_winding' & 'stator_tooth' appear to change similarly, but in different bounds of values (e.g. with id=30 stator_winding values is between -2 and 1.5, while stator_tooth is between -2 and 0.5).\n'pm' variable seems to behave differently from the rest, which might complicate the prediction and warrant a use of a ChainRegressor: predict the 'stator_...' variables first, and include them in the prediction for 'pm'","349b42a0":"\n\nIn order to smooth the predicted values, a Savitzky-Golay filter has been applied:\nhttps:\/\/en.wikipedia.org\/wiki\/Savitzky%E2%80%93Golay_filter"}}