{"cell_type":{"f1e6af4c":"code","39214fb8":"code","018097be":"code","b150a288":"code","af47dd90":"code","bbd6af5f":"code","599c667a":"code","c4668dd0":"code","6f3f3502":"code","77eb77cb":"code","2c202131":"code","f0fc536c":"code","21ab0f05":"code","9bd83f19":"code","8af3c205":"code","c3ce48ab":"code","73c7e14b":"code","d3cd1971":"code","0b8bec6b":"code","3b8b69f1":"code","f09b4301":"code","b231e373":"code","a4a7474d":"code","d9a37b86":"code","6167057e":"code","eaa579b9":"code","7b55e47a":"code","63c4af89":"code","6a290a54":"code","f048a451":"code","c8f12a49":"code","5dd31b1a":"code","617f2caf":"code","13d622eb":"code","ad1e0143":"code","b87de0fd":"markdown","158725f3":"markdown","b0fc2782":"markdown","13f8180a":"markdown","2832f6ba":"markdown","ba82eab2":"markdown","48695346":"markdown","227bb963":"markdown","09704573":"markdown","28003c73":"markdown","0bdd2772":"markdown","0ad6b133":"markdown"},"source":{"f1e6af4c":"import os, gc\nimport numpy as np\n#from numba import njit\nimport datatable as dtable\nimport pandas as pd\n\n\nfrom sklearn.model_selection import GroupKFold\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom tqdm import tqdm\nimport torch.optim as optim\nimport fastai\nfrom fastai.tabular.all import *","39214fb8":"device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')","018097be":"%%time\n\nprint('Loading...')\ntrain = dtable.fread('..\/input\/jane-street-market-prediction\/train.csv').to_pandas()\nfeatures = [c for c in train.columns if 'feature' in c]\n\nprint('Filling...')\ntrain = train.query('weight > 0').reset_index(drop = True)\n#train[features] = train[features].fillna(method = 'ffill').fillna(0)\n\nprint('Finish.')","b150a288":"f_mean = train[features[1:]].mean()","af47dd90":"train[features[1:]] = train[features[1:]].fillna(f_mean)","bbd6af5f":"train[features].astype('float32')\ntrain['action'] = (train['resp'] > 0).astype('int')","599c667a":"np.isnan(train.values).sum()","c4668dd0":"target_column = ['action']","6f3f3502":"len(features)","77eb77cb":"#gkf = GroupKFold(n_splits = 5)\n#for fold, (tr, te) in enumerate(gkf.split(train['action'].values, train['action'].values, train['date'].values)):\n    \n    #X_tr, X_val = train.loc[tr, features], train.loc[te, features]\n    #y_tr, y_val = train.loc[tr, target_column], train.loc[te, target_column]","2c202131":"splits = RandomSplitter(valid_pct=0.2)(range_of(train))","f0fc536c":"len(splits[0])","21ab0f05":"X_tr, X_val = train.loc[splits[0], features], train.loc[splits[1], features]\ny_tr, y_val = train.loc[splits[0], target_column], train.loc[splits[1], target_column]","9bd83f19":"\"\"\"\nOlder one\nclass Fastai_Cust_Ds():\n    def __init__(self, df, cats=None, y=None):\n        df = df.copy()\n                        \n        if cats is not None: \n            self.dfcats = df[cats] #type: pandas.core.frame.DataFrame\n            self.cats = np.stack([c.values for n, c in self.dfcats.items()], axis=1).astype(np.long)\n            self.dfconts = df.drop(cats, axis=1)\n            self.conts = np.stack([c.values for n, c in self.dfconts.items()], axis=1).astype(np.float32)\n        else:\n\n            self.dfconts = df.copy()\n            self.conts = np.stack([c.values for n, c in self.dfconts.items()], axis=1).astype(np.float32)\n            self.cats = np.zeros(len(df),).astype(np.long)\n        self.y = y.values\n        \n        \n    def __len__(self): return len(self.y)\n\n    def __getitem__(self, idx):\n\n        return [self.cats[idx], self.conts[idx], self.y[idx]]\"\"\"","8af3c205":"class Fastai_Cust_Ds():\n    def __init__(self, df, cats=None, ys=None):\n        df = df.copy()\n                           \n        if cats is not None: \n            self.dfcats = df[cats] #type: pandas.core.frame.DataFrame\n            self.cats = self.dfcats.to_numpy().astype(np.long)\n            self.dfconts = df.drop(cats, axis=1)\n            self.conts = self.dfconts.to_numpy().astype(np.float32)\n        else:\n\n            self.dfconts = df.copy()\n            self.conts = self.dfconts.to_numpy().astype(np.float32)\n            self.dfcats=pd.DataFrame(index=self.dfconts.index,)\n            self.cats = self.dfcats.to_numpy().astype(np.long)\n        self.ys = ys.values\n\n    def __len__(self): return len(self.ys)\n\n    def __getitem__(self, idx):\n        return [self.cats[idx], self.conts[idx], self.ys[idx]]","c3ce48ab":"train_ds = Fastai_Cust_Ds(df=X_tr, y=y_tr)\nvalid_ds = Fastai_Cust_Ds(df=X_val, y=y_val)","73c7e14b":"from fastai.data.core import DataLoader","d3cd1971":"#train_dl = DataLoader(train_ds, batch_size = 4096, drop_last=True, shuffle=False)\n#valid_dl = DataLoader(valid_ds, batch_size = 2048, drop_last=True, shuffle=False)","0b8bec6b":"class TabDataLoader(DataLoader):\n    def __init__(self, dataset, bs=1, num_workers=0, device='cuda', shuffle=False, **kwargs):\n        \"A `DataLoader` based on a `TabDataset`\"\n        super().__init__(dataset, bs=bs, num_workers=num_workers, shuffle=shuffle, \n                         device=device, drop_last=shuffle, **kwargs)\n        self.dataset.bs=bs\n    \n    def create_item(self, s): return s\n\n    def create_batch(self, b):\n        \"Create a batch of data\"\n        cat, cont, y = self.dataset[b]\n        return tensor(cat).to(self.device), tensor(cont).to(self.device), tensor(y).to(self.device)\n        #return cat, cont, y\n    def get_idxs(self):\n        \"Get index's to select\"\n        idxs = Inf.count if self.indexed else Inf.nones\n        if self.n is not None: idxs = list(range(len(self.dataset)))\n        return idxs\n\n    def shuffle_fn(self):\n        \"Shuffle the interior dataset\"\n        rng = np.random.permutation(len(self.dataset))\n        self.dataset.cats = self.dataset.cats[rng]\n        self.dataset.conts = self.dataset.conts[rng]\n        self.dataset.ys = self.dataset.ys[rng]","3b8b69f1":"train_dl = TabDataLoader(train_ds, device='cuda', shuffle=True, bs=5000)\nvalid_dl = TabDataLoader(valid_ds, device='cuda', shuffle=False, bs=5000)","f09b4301":"from fastai.data.core import DataLoaders\ndls = DataLoaders(train_dl,valid_dl,device='cuda')","b231e373":"dls.cats.shape, dls.conts.shape, dls.y.shape","a4a7474d":"train_ds.cats","d9a37b86":"class JanStr(nn.Module):\n\n    def __init__(self):\n        super(JanStr, self).__init__() \n\n        self.layers = nn.Sequential(\n\n        LinBnDrop(130, 400, bn=True, p=0, act=Mish(), lin_first=False),\n        LinBnDrop(400, 800, bn=True, p=0.2289, act=Mish(), lin_first=False),   \n        LinBnDrop(800, 400, bn=True, p=0.2289, act=Mish(), lin_first=False),\n        LinBnDrop(400, 2, bn=False, act=None, lin_first=False),\n\n        ) \n\n    def forward(self,cat, x):\n        x = self.layers(x)\n        return F.softmax(x, dim=1)","6167057e":"model_nn = JanStr()\nmodel_nn = model_nn.to(device)","eaa579b9":"loss_func = CrossEntropyLossFlat()","7b55e47a":"roc_auc = RocAucBinary()","63c4af89":"learn = Learner(dls, model_nn, loss_func = loss_func, metrics=roc_auc)","6a290a54":"callbacks = [\n    EarlyStoppingCallback(monitor='valid_loss', min_delta=1e-5, patience=7),    \n    ReduceLROnPlateau(monitor='valid_loss', min_delta=0.00001, patience=1, min_lr=1e-8),\n    #SaveModelCallback(mode='min'),\n    SaveModelCallback(monitor='valid_loss'),\n]","f048a451":"learn.lr_find()","c8f12a49":"lr=0.00005","5dd31b1a":"learn.fit_one_cycle(20, lr, wd = 0.0001, cbs=callbacks)","617f2caf":"def fill_nan(test_df):\n    test_df=test_df[features]\n    xar=test_df[features].values\n    na_arr=np.ones((1,len(features)), dtype=float)\n    na_arr=na_arr*(999)\n    xar = xar - na_arr\n    xar = np.nan_to_num(xar, nan=-999)\n    xar = xar + na_arr\n    test_df = torch.FloatTensor(xar)\n    return test_df","13d622eb":"learn.model.eval()\nimport janestreet\nenv = janestreet.make_env()\nenv_iter = env.iter_test()","ad1e0143":"for (test_df, pred_df) in tqdm(env_iter):\n    X_test = fill_nan(test_df).cuda()\n    preds = learn.model(0,X_test).argmax(dim=1).detach().cpu().numpy()\n    eps=0.5\n\n    #action = ((test_df['weight'].values * probas[:, 1]) > 0).astype('int')\n    if test_df['weight'].item() == 0:\n        action=0\n    else:\n        action=preds\n    pred_df.action = action\n    env.predict(pred_df)","b87de0fd":"Now we create the data loader for the train and validation sets after importing the fastai DataLoader which is similar to the pytorch one.","158725f3":"New custom_ds","b0fc2782":"The LinBnDrop is a very summarizing linear model of fastai dealing with three Batchnorm1D, nn.Linear and dropout. The forward function requires two values, cats for categories and x for the features.\n","13f8180a":"As we see, the values of cats were returned with zero values. This is important for the learner to accept the dls created.\n.\n\nThere are also many useful options in both of DataLoader and DataLoaders of fastai that can be used here.  Drop_last and device are the most important.\n\n\nThe only problem of creating such a custom dataset in fastai, is that the inference can not be calculated using the learn.get_preds. Instead, we do learn.model.eval()  then we infer using preds = learn.model(0,test_df). I suppose that a small modification of the custom dataset may solve this problem.\n\n\nAs we will see in the custom model created here, its foreword function will need two values, one for the cats values, while the other one will take the x. The same must be done in the inference using  preds = learn.model(0,test_df).","2832f6ba":"I just took the first line (BatchNorm1d) away, since LinBnDrop begins already with bn.\nIf the bn is true the bias will be set to false automatically.","ba82eab2":"The fastai DataLoader is the next step, so it must be also imported.","48695346":"Instead, I applied the custom TabDataloader from Zachary Mueller's notebook, which will make a huge differce to the speed. To create the dls, the fastai **DataLoaders** is OK.","227bb963":"This notebook is an update for previous version.\u00a0\n1-I changed the custom dataset to meet other types of data including embeddings.\n2- Since the dataloader of fastai is too slow I took a small part of the code from here: https:\/\/muellerzr.github.io\/fastblog\/2020\/04\/22\/TabularNumpy.html\nand built a new dataloader. Many thanks to Zachary Mueller for his notebook.\n3- The model does not need an additional batch normalization at the beginning as it already exists in the LinBnDrop\n","09704573":"The dataloader of Fastai is too slow, so I took it out.","28003c73":"Deviding the data:\n\nThe easiest way is to use sklearn tool or even the fastai spilitter.\nActually I got better results deviding thee data with fastai splitters, where the date is not considered.","0bdd2772":"Using the fastai DataLoader and Dataoaders is useful if you like to reduce the iteration time. The idea of creating the custom dataset class is simple and generally looks like the one created for pytorch application as created here https:\/\/towardsdatascience.com\/deep-learning-using-pytorch-for-tabular-data-c68017d8b480.\n\nThe only difference is to return some value for the categories (cats), when they are not existing. np.zeros(len(df),).astype(np.long) will do this for us. Now we can return all values cats, conts and y.","0ad6b133":"# Preprocessing"}}