{"cell_type":{"e2b08c4f":"code","333efd83":"code","16eaacd0":"code","84ac5046":"code","35c1d8c8":"code","561ea8f1":"code","90f3248c":"code","c3cc27bc":"code","6c5d29c7":"code","cd88a7ee":"code","4a30c9b5":"code","1774b1ae":"code","1b8f3e7e":"code","1e1c9501":"code","aaf6c550":"code","321c39cb":"code","8c40a625":"code","212f1d00":"code","c8e3c28b":"code","9a9e0215":"code","fad5652e":"code","84b6d8e5":"code","94ebe20e":"code","52f05f93":"code","ee5fa1d1":"code","b762ab64":"code","89a31a22":"markdown","0649b7aa":"markdown","87ff2b18":"markdown","da0e48d4":"markdown","2c482493":"markdown","261acf64":"markdown","d287e21a":"markdown","ba63e3a8":"markdown"},"source":{"e2b08c4f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","333efd83":"import seaborn as sns \nimport matplotlib.pyplot as plt\n%matplotlib inline","16eaacd0":"#Read the data into a dataframe \ntrain = pd.read_csv('\/kaggle\/input\/iowa-house-prices\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/iowa-house-prices\/test.csv')\n\ntrain.drop(columns='Id', inplace=True)\ntest.drop(columns='Id', inplace = True)","84ac5046":"print(\"Train shape: {}\".format(train.shape))\nprint(\"Test shape: {}\".format(test.shape))\ntrain.head()","35c1d8c8":"leaked_columns = ['MoSold', 'YrSold','SaleType', 'SaleCondition']\n\ntrain.drop(labels = leaked_columns, inplace=True ,axis =1)\ntest.drop(labels = leaked_columns, inplace = True,axis=1)","561ea8f1":"train.isnull().sum().sort_values(ascending=False).plot(kind='bar', figsize=(35,5))","90f3248c":"train.dtypes","c3cc27bc":"# Let's look at the distribution of the target variable\ntrain['SalePrice'].describe()","6c5d29c7":"#plot the above data \nplt.figure(figsize = (16,5))\nsns.histplot(x=train['SalePrice'], kde=True)\nplt.title('Distribution of Sales Price')","cd88a7ee":"numerical_cols = [col for col in train.columns if train[col].dtype in ['int64', 'float64']]\n#checking missing values in the numerical columns\nna_count_n = train[numerical_cols].isna().sum() ","4a30c9b5":"na_count_n[na_count_n>0]","1774b1ae":"# select the column with NAs\nna_numerical_columns  = na_count_n[na_count_n>0].index\n\nna_numerical_data = train[numerical_cols].dropna()\n\nna_numerical_data\n#Plot the distribution of missing numerical data \nplt.figure(figsize= (10,6))\nsns.distplot(a = na_numerical_data[na_numerical_columns[0]])","1b8f3e7e":"plt.figure(figsize= (10,6))\nsns.distplot(a = na_numerical_data[na_numerical_columns[1]])","1e1c9501":"plt.figure(figsize= (10,6))\nsns.distplot(a = na_numerical_data[na_numerical_columns[2]])","aaf6c550":"from sklearn.impute import SimpleImputer\n\n#Prepariing the Imputation objects (there are more easier ways to apply imputation using column trasformer but for now am using this)\nmean_imputer = SimpleImputer(strategy = 'mean')\nmedian_imputer = SimpleImputer(strategy = 'median')\nmode_imputer = SimpleImputer(strategy = 'most_frequent')\n\n#creating a function that can used to apply imputer on train and test data\ndef apply_imputation(imputer, column):\n    imputed_train = imputer.fit_transform(X = train[[column]])\n    imputed_test = imputer.transform(X = test[[column]])\n    \n    return imputed_train, imputed_test\n\n#Make the Imputation\ntrain['LotFrontage'] = apply_imputation(median_imputer, 'LotFrontage')[0]\ntest['LotFrontage'] = apply_imputation(median_imputer, 'LotFrontage')[1]\n\ntrain['MasVnrArea'] = apply_imputation(mode_imputer, 'MasVnrArea')[0]\ntest['MasVnrArea'] = apply_imputation(mode_imputer, 'MasVnrArea')[1]\n\ntrain['GarageYrBlt'] = apply_imputation(mode_imputer, 'GarageYrBlt')[0]\ntest['GarageYrBlt'] = apply_imputation(mode_imputer, 'GarageYrBlt')[1]","321c39cb":"#Missing values in Categorical Colummns\ncategorical_cols = [col for col in train.columns if train[col].dtype in ['object']]\n\nna_categorical_cols = train[categorical_cols].isna().sum()\nna_categorical_cols[na_categorical_cols>0]","8c40a625":"# drop all the columns with more than 50% missing data \nto_drop = na_categorical_cols[na_categorical_cols > train.shape[0] * 0.5].index\n\ntrain.drop(labels = to_drop , axis = 1, inplace = True)\ntest.drop(labels = to_drop,axis = 1, inplace = True)\n\ncategorical_cols = list(set(categorical_cols)-set(to_drop))","212f1d00":"#Encoding the categorical features with OneHotEncoding\nfrom sklearn.preprocessing import OneHotEncoder\n\n#Create the encoder object\nencoder = OneHotEncoder(handle_unknown='ignore')","c8e3c28b":"#Imputing missing values in categorical cols first \nfor col in categorical_cols:\n    train[col] = apply_imputation(mode_imputer , col)[0] #apply_imputation is a self created function\n    test[col] = apply_imputation(mode_imputer,col)[1]\n    \n#Encoding categorical features\nencoded_train = pd.DataFrame(encoder.fit_transform(train[categorical_cols]).toarray())\nencoded_test = pd.DataFrame(encoder.transform(test[categorical_cols]).toarray())\n\n#Drop the old columns and replace it with encoded cols\ntrain.drop(columns=categorical_cols, axis=1, inplace=True)\ntest.drop(columns=categorical_cols, axis=1, inplace=True)\n\n#joiniing with encoded cols\ntrain = pd.concat([train,encoded_train],axis =1)\ntest = pd.concat([test,encoded_test],axis=1)","9a9e0215":"# Data Splitting\nfrom sklearn.model_selection import train_test_split\n\n# Models (or Algorithms)\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\n\n# Model Evaluation\nfrom sklearn.metrics import mean_absolute_error\n\n# Ignore warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)","fad5652e":"#Splitting the training datset into Trainiing and Validation Set\nX= train\ny=train['SalePrice']\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y,test_size= 0.3, random_state = 0)\n","84b6d8e5":"#Model 1 - Linear Regression\nlinear_reg = LinearRegression()\n\n#Train \nlinear_reg.fit(X = X_train, y=y_train)\n\n#predict\npredictions = linear_reg.predict(X_valid)\n\n#calculating mean_absolute error btw predictions and actual value\nmae_linear = mean_absolute_error(y_valid, predictions)\n\nprint(\"linear regression mae: {:,}\".format(mae_linear))","94ebe20e":"# ~~~~~~~~~~~~~~~~~~~~~\n# DecisionTreeRegressor\n# ~~~~~~~~~~~~~~~~~~~~~\n\ntree_model = DecisionTreeRegressor()\n\n# Train the model on training data\ntree_model.fit(X=X_train, y=y_train)\n\n# Predict on validation data\npredictions = tree_model.predict(X_valid)\n\n# Get how well it performed\nmae_tree = mean_absolute_error(y_valid, predictions)\n\nprint(\"Tree: {:,}\".format(mae_tree))","52f05f93":"\nrf_model = RandomForestRegressor()\n\n# Train the model on training data\nrf_model.fit(X=X_train, y=y_train)\n\n# Predict on validation data\npredictions = rf_model.predict(X_valid)\n\n# Get how well it performed\nmae_rf = mean_absolute_error(y_valid, predictions)\n\nprint(\"Random Forest: {:,}\".format(mae_rf))","ee5fa1d1":"\n\nxgb_model = XGBRegressor(n_estimators=600)\n\n# Train the model on training data\nxgb_model.fit(X=X_train, y=y_train)\n\n# Predict on validation data\npredictions = xgb_model.predict(X_valid)\n\n# Get how well it performed\nmae_xgb = mean_absolute_error(y_valid, predictions)\n\nprint(\"XGBoost: {:,}\".format(mae_xgb))\n","b762ab64":"my_model = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n                        colsample_bynode=1, colsample_bytree=0.8, gamma=0.0,\n                        importance_type='gain', learning_rate=0.005, \n                        max_delta_step=0, max_depth=4, min_child_weight=1, \n                        missing=None, n_estimators=5000, n_jobs=1, \n                        nthread=4, objective='reg:squarederror', random_state=0,\n                        reg_alpha=1, reg_lambda=1, scale_pos_weight=1, seed=27,\n                        silent=None, subsample=0.8, verbosity=1)\n\n# Fit on the entire training data\nmy_model.fit(X, y)","89a31a22":"#### Imputation\nThere is a SimpleImputer to impute the missing values in the dataset ","0649b7aa":"## 1. Data Preprocessing\n## 1.1 Importing libraries","87ff2b18":"There are no mising values in the target variable i.e. \"Sales Price\" but there is a lot of missing values in other columns ","da0e48d4":"#### 1.3.2 EDA\n##### Univariate Analysis","2c482493":"### Model Training\n\n1. LinearRegression\n2. Decision tree\n3. Random Forest\n4. XGBoost\n","261acf64":"Looking at the distribution of the Sales price we can tell that it is a right skewed dataset where the most of the houses is priced below 40,000 USD with few outliers at 75,500 USD","d287e21a":"### 1.2. Getting dataset ready","ba63e3a8":"### 1.3. Data preprocessing\n\n1. Check missing values\n2. EDA - analyzing the distribution of variables\n3. Feature Engineering\n..."}}