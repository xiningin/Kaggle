{"cell_type":{"b6e88b8b":"code","fca2cca0":"code","90e84892":"code","6aa6b23c":"code","4346e86f":"code","19d8d48f":"code","bb52f427":"code","52b6f67b":"code","433e20ad":"code","d1d2893d":"code","a312533b":"code","b05f67be":"code","8b2589f3":"code","b4d1afbe":"code","e6d04fb0":"code","ec41a5b8":"code","2591b8f7":"code","ec8afbcb":"code","57831845":"markdown","77bfb3af":"markdown","0f8f1f9f":"markdown","4aa1a39a":"markdown","01586d0a":"markdown","a30f98e4":"markdown","a2c1f2ff":"markdown","09d844ad":"markdown"},"source":{"b6e88b8b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","fca2cca0":"import numpy as np\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\n# We will be using the Iris Plants Database\nfrom sklearn.datasets import load_iris\nSEED = 2017\n\n# The first two classes (Iris-Setosa and Iris-Versicolour) are linear\niris = load_iris()\nidxs = np.where(iris.target<2)\nX = iris.data[idxs]\ny = iris.target[idxs]","90e84892":"plt.scatter(X[y==0][:,0],X[y==0][:,2], color='green', label='Iris-Setosa')\nplt.scatter(X[y==1][:,0],X[y==1][:,2], color='red', label='Iris-Versica')\nplt.title('Iris Plants Database')\nplt.xlabel('sepal length in cm')\nplt.ylabel('sepal width in cm')\nplt.legend()\nplt.show()","6aa6b23c":"\ndef sigmoid(x):\n    return 1 \/ (1 + np.exp(-x))","4346e86f":"X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\nweights = np.random.normal(size=X_train.shape[1])\nbias = 1\nlearning_rate = 0.1\nn_epochs = 15\ndel_w = np.zeros(weights.shape)\nhist_loss = []\nhist_accuracy = []\n\nfor i in range(n_epochs):\n    # We apply a simple step function, if the output is > 0.5 we predict 1, else 0\n    output = np.where((X_train.dot(weights)+bias)>0.5, 1, 0)\n\n    # Compute MSE\n    error = np.mean((y_train-output)**2)\n\n    # Update weights and bias\n    weights-= learning_rate * np.dot((output-y_train), X_train)\n    bias += learning_rate * np.sum(np.dot((output-y_train), X_train))\n\n    # Calculate MSE\n    loss = np.mean((output - y_train) ** 2)\n    hist_loss.append(loss)\n\n    # Determine validation accuracy\n    output_val = np.where(X_val.dot(weights)>0.5, 1, 0)\n    accuracy = np.mean(np.where(y_val==output_val, 1, 0))\n    hist_accuracy.append(accuracy)","19d8d48f":"fig = plt.figure(figsize=(8, 4))\na = fig.add_subplot(1,2,1)\nimgplot = plt.plot(hist_loss)\nplt.xlabel('epochs')\na.set_title('Training loss')\n\na=fig.add_subplot(1,2,2)\nimgplot = plt.plot(hist_accuracy)\nplt.xlabel('epochs')\na.set_title('Validation Accuracy')\nplt.show()","bb52f427":"import numpy as np\nimport pandas as pd\nfrom keras.datasets import mnist\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Activation\nfrom keras.optimizers import SGD\nfrom keras.utils import np_utils as kutils\nfrom sklearn.model_selection import train_test_split\n\nnp.random.seed(1671) # for reproducibility\n# network and training parameters\nNB_EPOCH = 200\nBATCH_SIZE = 128\nVERBOSE = 1\nNB_CLASSES = 10 # number of outputs = number of digits\nRESHAPED = 784\nOPTIMIZER = SGD() # SGD optimizer, explained later in this chapter\nN_HIDDEN = 128\nVALIDATION_SPLIT=0.2 # how much TRAIN is reserved for VALIDATION\n# data: shuffled and split between train and test sets\n#\n\ndef load_dataset(train_path='..\/input\/digit-recognizer\/train.csv',test_path='..\/input\/digit-recognizer\/test.csv'):\n    #global train,test,trainX,trainY,nb_classes\n    img_rows=28\n    img_cols=28\n    train = pd.read_csv(train_path).values # produces numpy array\n    test  = pd.read_csv(test_path).values # produces numpy array\n    print(\"Train Shape :\",train.shape)\n    print(\"Test Shape :\",test.shape)\n    trainX = train[:, 1:].reshape(train.shape[0], RESHAPED)\n    trainX = trainX.astype(float)\n    trainX \/= 255.0\n    trainY = kutils.to_categorical(train[:, 0])\n    nb_classes = trainY.shape[1]\n    print(\"TrainX Shape : \",trainX.shape)\n    print(\"Trainy shape : \",trainY.shape)\n    testX = test.reshape(test.shape[0], RESHAPED)\n    testX = testX.astype(float)\n    testX \/= 255.0\n    trainY = kutils.to_categorical(train[:, 0])\n    return trainX,trainY,testX,nb_classes\n\nX,y,test,nb_classes=load_dataset()\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","52b6f67b":"#X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784\n\n# final stage is softmax\nmodel = Sequential()\nmodel.add(Dense(NB_CLASSES, input_shape=(RESHAPED,)))\nmodel.add(Activation('softmax'))\nmodel.summary()","433e20ad":"model.compile(loss='categorical_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy'])","d1d2893d":"\nhistory = model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=NB_EPOCH, verbose=VERBOSE, \n                    validation_split=VALIDATION_SPLIT)\nscore = model.evaluate(X_test, y_test, verbose=VERBOSE)\nprint(\"Test score:\", score[0])\nprint('Test accuracy:', score[1])","a312533b":"# final stage is softmax\nmodel = Sequential()\nmodel.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))\nmodel.add(Activation('relu'))\nmodel.add(Dense(N_HIDDEN))\nmodel.add(Activation('relu'))\nmodel.add(Dense(NB_CLASSES))\nmodel.add(Activation('softmax'))\nmodel.summary()\nmodel.compile(loss='categorical_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy'])","b05f67be":"history = model.fit(X_train, y_train,batch_size=BATCH_SIZE, epochs=NB_EPOCH,\n                    verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n\nscore = model.evaluate(X_test, y_test, verbose=VERBOSE)\nprint(\"Test score:\", score[0])\nprint('Test accuracy:', score[1])","8b2589f3":"from keras.layers.core import Dropout\nDROPOUT = 0.3\n# M_HIDDEN hidden layers 10 outputs\nmodel = Sequential()\nmodel.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(DROPOUT))\nmodel.add(Dense(N_HIDDEN))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(DROPOUT))\nmodel.add(Dense(NB_CLASSES))\nmodel.add(Activation('softmax'))\nmodel.summary()\nmodel.compile(loss='categorical_crossentropy',optimizer=OPTIMIZER,metrics=['accuracy'])\nhistory = model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=NB_EPOCH,verbose=VERBOSE,\n                    validation_split=VALIDATION_SPLIT)\n\nscore = model.evaluate(X_test, y_test, verbose=VERBOSE)\nprint(\"Test score:\", score[0])\nprint('Test accuracy:', score[1])","b4d1afbe":"from sklearn.datasets import load_digits\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.metrics import f1_score, classification_report\n\nX, y = load_digits(return_X_y=True)\nclf = Perceptron(tol=1e-3, random_state=0)\nclf.fit(X, y)\nprint(\"Classification Score : \",clf.score(X, y))\npredictions = clf.predict(X)\nprint(classification_report(y, predictions))","e6d04fb0":"from sklearn.datasets import load_digits\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.metrics import f1_score, classification_report\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Activation\n\nX, y = load_digits(return_X_y=True)\n#digits = load_digits()\n\nprint(len(X))\nprint(len(y))\n\n# final stage is softmax\nmodel = Sequential()\n# Input - Layer\nmodel.add(Dense(50, activation = \"relu\", input_shape=(64,)))\n# Output- Layer\nmodel.add(Dense(10, activation = \"sigmoid\"))\n\nmodel.summary()\nmodel.compile(optimizer = \"adam\",loss = \"binary_crossentropy\", metrics = [\"accuracy\"])\n\nresults = model.fit(X, y, epochs= 20, batch_size = 500)","ec41a5b8":"# Load required libraries\nfrom sklearn import datasets\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score, classification_report\nimport numpy as np\n\n# Load the iris dataset\niris = datasets.load_iris()\n\n# Create our X and y data\nX = iris.data\ny = iris.target\n\n# Train the scaler, which standarizes all the features to have mean=0 and unit variance\nsc = StandardScaler()\nsc.fit(X)\n\n# Apply the scaler to the X training data\nX_std = sc.transform(X)\n# Split the data into 70% training data and 30% test data\nX_train, X_test, y_train, y_test = train_test_split(X_std, y, test_size=0.3)\n\n# Create a perceptron object with the parameters: 40 iterations (epochs) over the data, and a learning rate of 0.1\nmodel = Perceptron(n_iter=40, eta0=0.1, random_state=0,activation='relu')\n\n# Train the perceptron\nmodel.fit(X_train, y_train)\n\n# Apply the trained perceptron on the X data to make predicts for the y test data\ny_pred = model.predict(X_test)\n\nprint(classification_report(y_test, y_pred))","2591b8f7":"import numpy as np\nimport pandas as pd\nfrom keras.utils import np_utils as kutils\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neural_network import MLPClassifier\n\nnp.random.seed(1671) # for reproducibility\n# network and training parameters\nNB_EPOCH = 100\nRESHAPED = 784\n#BATCH_SIZE = 128\n#VERBOSE = 1\n#NB_CLASSES = 10 # number of outputs = number of digits\n#OPTIMIZER = SGD() # SGD optimizer, explained later in this chapter\n#N_HIDDEN = 128\n#VALIDATION_SPLIT=0.2 # how much TRAIN is reserved for VALIDATION\n# data: shuffled and split between train and test sets\n#\n\ndef load_dataset(train_path='..\/input\/digit-recognizer\/train.csv',test_path='..\/input\/digit-recognizer\/test.csv'):\n    #global train,test,trainX,trainY,nb_classes\n    img_rows=28\n    img_cols=28\n    train = pd.read_csv(train_path).values # produces numpy array\n    test  = pd.read_csv(test_path).values # produces numpy array\n    print(\"Train Shape :\",train.shape)\n    print(\"Test Shape :\",test.shape)\n    trainX = train[:, 1:].reshape(train.shape[0], RESHAPED)\n    trainX = trainX.astype(float)\n    trainX \/= 255.0\n    trainY = kutils.to_categorical(train[:, 0])\n    nb_classes = trainY.shape[1]\n    print(\"TrainX Shape : \",trainX.shape)\n    print(\"Trainy shape : \",trainY.shape)\n    testX = test.reshape(test.shape[0], RESHAPED)\n    testX = testX.astype(float)\n    testX \/= 255.0\n    trainY = kutils.to_categorical(train[:, 0])\n    return trainX,trainY,testX,nb_classes\n\nX,y,test,nb_classes=load_dataset()\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","ec8afbcb":"clf = MLPClassifier(hidden_layer_sizes=(100,100,100), max_iter=NB_EPOCH, alpha=0.0001,\n                     solver='sgd', verbose=True,  random_state=21,tol=0.000000001)\nclf.fit(X_train, y_train)\nprint(\"Classification Score : \",clf.score(X_train, y_train))\n\npredictions = clf.predict(X_test)\nprint(classification_report(y_test, predictions))","57831845":"**Improving the Simple Neural Net in Keras with Hidden Layers**\n\nA first improvement is to add additional layers to our network. So, after the input layer, we have a first dense layer with the N_HIDDEN neurons and an activation function relu. This additional layer is considered hidden because it is not directly connected to either the input or the output. After the first hidden layer, we have a second hidden layer, again with the N_HIDDEN neurons, followed by an output layer with 10 neurons, each of which will fire when the relative digit is recognized. ","77bfb3af":"**Further Improving the Simple Net in Keras with Dropout**\n\nA second improvement is very simple. We decide to randomly drop with the dropout probability some of the values\npropagated inside our internal dense network of hidden layers. In machine learning, this is a well-known form of regularization. Surprisingly enough, this idea of randomly dropping a few values can significantly improve classification performance:","0f8f1f9f":"**Multiplayer Perceptrons Using Sklearn and Handwritten Digit Recognizer Dataset**","4aa1a39a":"**Perceptron Algorithm from Sklearn on Digits Dataset**\n","01586d0a":"**Multiplayer Perceptrons Using Keras and Handwritten Digit Recognizer Dataset**\n\nA neural network consists of one or multiple layers of neurons, named after the biological neurons in human brains. We will demonstrate the mechanics of a single neuron by implementing a perceptron. In a perceptron, a single unit (neuron) performs all the computations.","a30f98e4":"**Basic Perceptron Implementation**\n\nFirst, we need to understand the basics of neural networks. A neural network consists of one or multiple layers of neurons, named after the biological neurons in human brains. We will demonstrate the mechanics of a single neuron by implementing a perceptron. In a perceptron, a single unit (neuron) performs all the computations.","a2c1f2ff":"**Perceptron Using Keras  on Digits Dataset**","09d844ad":"**Iris Recognition Using Perceptron with Scikit Learn**"}}