{"cell_type":{"74efd068":"code","a4dcbeca":"code","47db0bc9":"code","31c55955":"code","944d6652":"code","27b4bad7":"code","9362d75d":"code","8be9d8b6":"code","99bd99b2":"code","d6ab3bdc":"code","18bad778":"code","40f93539":"code","90f09f27":"code","eeb83a8c":"code","ff327174":"code","950f175b":"code","ae83d6c9":"code","6cafce9b":"code","cffef2c5":"code","47496033":"code","8da6f786":"code","c48e7df4":"code","efafe39d":"code","1d3010fb":"code","bc315ba7":"code","d851ec13":"code","9aca3b1a":"code","47abc595":"markdown","7b447baf":"markdown","1fc05fd9":"markdown","c9b56b14":"markdown","abc9b3dc":"markdown","73d3787e":"markdown","10177a8a":"markdown","035b5586":"markdown","92e09239":"markdown","69c428be":"markdown","21f4e439":"markdown"},"source":{"74efd068":"import geopandas as gpd\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","a4dcbeca":"df_mh_all_villages = gpd.read_file('..\/input\/mh-villages-v2w2\/MH_Villages v2W2.shp')[['DTNAME','GPNAME','VILLNAME']]\n# ['DTNAME','GPNAME','VILLNAME']\nprint(df_mh_all_villages.shape)\ndf_mh_all_villages.T","47db0bc9":"df_ListOfTalukas = pd.read_csv(\"..\/input\/paani-foundations-satyamev-jayate-water-cup\/ListOfTalukas.csv\")\nprint(df_ListOfTalukas.shape)\ndf_ListOfTalukas.T","31c55955":"df_join_district = pd.merge(df_mh_all_villages,\n                            df_ListOfTalukas,\n                           how='inner',\n                           left_on=['DTNAME','GPNAME'],\n                           right_on=['District','Taluka'],\n                           indicator=True)","944d6652":"print(df_join_district.shape)\ndf_join_district.T","27b4bad7":"# df_join_district[['_merge']].nunique()\ndf_join_district.groupby('_merge').count()","9362d75d":"df_StateLevelWinners = pd.read_csv('\/kaggle\/input\/paani-foundations-satyamev-jayate-water-cup\/StateLevelWinners.csv')\nprint(df_StateLevelWinners.shape)\ndf_StateLevelWinners.T","8be9d8b6":"_df_join_villages = pd.merge(df_mh_all_villages,\n                           df_StateLevelWinners,\n                           left_on=['DTNAME','GPNAME','VILLNAME'],\n                           right_on=['District','Taluka','Village'],\n                           indicator=True)","99bd99b2":"_df_join_villages.shape","d6ab3bdc":"df_join_villages = pd.merge(df_mh_all_villages,\n                           df_StateLevelWinners,\n                           left_on=['VILLNAME'],\n                           right_on=['Village'],\n                           indicator=True)","18bad778":"print(df_join_villages.shape)\ndf_join_villages.T","40f93539":"df_join_villages[df_join_villages['DTNAME']=='Satara']","90f09f27":"# df_fuzzy_matching = df_join_villages\n# 'DTNAME','GPNAME','VILLNAME'\ndf_mh_all_villages['merge_entities'] = df_mh_all_villages[df_mh_all_villages.columns[:3]] \\\n                                        .apply(lambda x: ','.join(x.dropna().astype(str))\n                                               ,axis=1)\n","eeb83a8c":"df_mh_all_villages.T","ff327174":"df_StateLevelWinners['merge_entities'] = df_StateLevelWinners[df_StateLevelWinners.columns[:3]] \\\n                        .apply(lambda x: ','.join(x.dropna().astype(str)), axis =1)","950f175b":"df_StateLevelWinners.T","ae83d6c9":"from fuzzywuzzy import fuzz","6cafce9b":"for ind, row1 in df_mh_all_villages[df_mh_all_villages['DTNAME']=='Satara'].iterrows():\n    for ind, row2 in df_StateLevelWinners[df_StateLevelWinners['District']=='Satara'].iterrows():\n        matching_ratio = fuzz.ratio(row1['merge_entities'], row2['merge_entities'])\n        if matching_ratio > 80:\n            print(row1['merge_entities'] + '  *  ' +\n                  row2['merge_entities'] + '  :  ' + \n                  str(matching_ratio) )","cffef2c5":"for ind, row1 in df_mh_all_villages.iterrows():\n    for ind, row2 in df_StateLevelWinners.iterrows():\n        matching_ratio = fuzz.ratio(row1['merge_entities'], row2['merge_entities'])\n        if matching_ratio > 80:\n            print(row1['merge_entities'] + '  *  ' +\n                  row2['merge_entities'] + '  :  ' + \n                  str(matching_ratio) )","47496033":"import regex as re","8da6f786":"def ngrams(string, n=3):\n#     string = fix_text(string) # fix text encoding issues\n    string = string.encode(\"ascii\", errors=\"ignore\").decode() #remove non ascii chars\n    string = string.lower() #make lower case\n    chars_to_remove = [\")\",\"(\",\".\",\"|\",\"[\",\"]\",\"{\",\"}\",\"'\"]\n    rx = '[' + re.escape(''.join(chars_to_remove)) + ']'\n    string = re.sub(rx, '', string) #remove the list of chars defined above\n    string = string.replace('&', 'and')\n    string = string.replace(',', ' ')\n    string = string.replace('-', ' ')\n    string = string.title() # normalise case - capital at start of each word\n    string = re.sub(' +',' ',string).strip() # get rid of multiple spaces and replace with a single space\n    string = ' '+ string +' ' # pad names for ngrams...\n    string = re.sub(r'[,-.\/]|\\sBD',r'', string)\n    ngrams = zip(*[string[i:] for i in range(n)])\n    return [''.join(ngram) for ngram in ngrams]\n","c48e7df4":"from sklearn.feature_extraction.text import TfidfVectorizer","efafe39d":"org_names = df_mh_all_villages['merge_entities'].unique()\nvectorizer = TfidfVectorizer(min_df=1, analyzer=ngrams)\ntf_idf_matrix = vectorizer.fit_transform(org_names)","1d3010fb":"!pip install sparse_dot_topn ","bc315ba7":"from sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport re\n\n# clean_org_names = pd.read_excel('Gov Orgs ONS.xlsx')\n# clean_org_names = clean_org_names.iloc[:, 0:6]\nmerged_entities_watercup = df_StateLevelWinners['merge_entities'].unique()\nprint('Vecorizing the data - this could take a few minutes for large datasets...')\n\nvectorizer = TfidfVectorizer(min_df=1, analyzer=ngrams, lowercase=False)\ntfidf = vectorizer.fit_transform(merged_entities_watercup)\nprint('Vecorizing completed...')\nfrom sklearn.neighbors import NearestNeighbors\nnbrs = NearestNeighbors(n_neighbors=1, n_jobs=-1).fit(tfidf)\norg_column = 'merge_entities' #column to match against in the messy data\nmerged_entities_all_villages = set(df_mh_all_villages[org_column].values) # set used for increased performance\n###matching query:\ndef getNearestN(query):\n  queryTFIDF_ = vectorizer.transform(query)\n  distances, indices = nbrs.kneighbors(queryTFIDF_)\n  return distances, indices\nimport time\nt1 = time.time()\nprint('getting nearest n...')\ndistances, indices = getNearestN(merged_entities_all_villages)\nt = time.time()-t1\nprint(\"COMPLETED IN:\", t)\nmerged_entities_all_villages = list(merged_entities_all_villages) #need to convert back to a list\nprint('finding matches...')\nmatches = []\nfor i,j in enumerate(indices):\n  temp = [round(distances[i][0],2), df_StateLevelWinners.values[j][0][5],merged_entities_all_villages[i]]\n  matches.append(temp)\nprint('Building data frame...')  \nmatches = pd.DataFrame(matches, columns=['Match confidence','entities_watercup','entities_all_villages'])\nprint('Done')","d851ec13":"matches.T","9aca3b1a":"\nmatches[matches['Match confidence'] < 0.7]\n# matches.query('Match confidence < 0.5')","47abc595":"# Pain of Entity Matching\n\n- 'Entity Matching' is common task in most of the data engineering pipeline which joins multiple datasets.    \n- Complexity of this problem could escalate as dataset coming from different sources.  \n- While working WaterCup dataset, we realise there are quite a lot of time we have names of the places typed differently in different datasets. \n- That leads us to creating a mapping of names manually, something like this:   \n`_df_ListOfTalukas = _df_ListOfTalukas.replace('Ahmednagar','Ahmadnagar') \\ . \n                                        .replace('Buldhana','Buldana') \\ \n                                        .replace('Sangli','Sangali') \\ \n                                        .replace('Nashik','Nasik')`\nOf course this is not way to go with bigger datasets and more granular mapping!\n- In this notebook we will try to address tbnis issue using various traditional and some non-traditional but innovative methods!","7b447baf":"### Lets Join on District-DTNAME for all matching non-matching records. 48926 + 184\n\nPandas join,  \nOuter for union and inner for intersection.","1fc05fd9":"#### As we see above we got some success in matching.","c9b56b14":"## 1. Fuzzy maching","abc9b3dc":"### Matching score lower is better!","73d3787e":"This is a inprogress notebook.\n\nNotebook submitted in response to task#3:\nhttps:\/\/www.kaggle.com\/bombatkarvivek\/paani-foundations-satyamev-jayate-water-cup\/tasks?taskId=348\n\nAim is to find method that will identify the correct pair of District-Taluka-Village among different datasets.","10177a8a":"### Lets try more sophosticated algorithm, TfidfVectorizer from sklearn   \n\nSource:   \nhttps:\/\/colab.research.google.com\/drive\/1qhBwDRitrgapNhyaHGxCW8uKK5SWJblW#scrollTo=xo-X_nds97UN","035b5586":"#### No records ware found when joined on Village level!","92e09239":"[](http:\/\/)### Using fuzzywuzzy.ratio lets try to identify the matcnhing entities with matching score more than 80%.   \nLets run this over small subset of data, i.e. District = Satara ","69c428be":"Of course above logic will explode if applied to entire dataset!  \nBelow loop took good amount of time to finish. ","21f4e439":"## 2. TF-IDF"}}