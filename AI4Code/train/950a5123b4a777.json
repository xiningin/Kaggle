{"cell_type":{"e7a56bc3":"code","2a792252":"code","679576d9":"code","6d042f79":"code","6ff3eefe":"code","cf21fa88":"code","7c049e84":"code","06739d19":"code","55cf926c":"code","701235b3":"code","d18a7540":"code","3ec56239":"code","5e56c596":"code","7c73d90f":"code","1a518645":"code","74e7ac22":"code","bb441706":"code","f536ef49":"code","5a4f3463":"code","9ce1acac":"code","e6edf792":"code","b084c195":"code","0e92ab4c":"code","814f0aa4":"markdown","d8508483":"markdown","d8eb2b85":"markdown","ca1bbc72":"markdown","f2c99b07":"markdown","85a8221c":"markdown","b85a812a":"markdown","2cc45cb1":"markdown","22981cc6":"markdown","beec4d18":"markdown","61f42e48":"markdown","1625dd48":"markdown","cb4c8033":"markdown","0f505a7d":"markdown","a63967c4":"markdown","e3b91825":"markdown"},"source":{"e7a56bc3":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import preprocessing","2a792252":"df_train = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ndf_test = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ndf_train.info()","679576d9":"df_test.info()","6d042f79":"# Let's first look at the distribution for survived\/not survived\nax = sns.countplot(x=\"Survived\", data=df_train)","6ff3eefe":"ax = sns.heatmap(df_train.corr(), annot=True, vmin=-1, vmax=1)","cf21fa88":"plt.title(\"Passenger Class distribution\")\nax = sns.countplot(x=\"Pclass\", data=df_train)","7c049e84":"ax = sns.catplot(x=\"Pclass\", col=\"Survived\", kind='count', data=df_train)","06739d19":"ax = sns.barplot(x=\"Pclass\", y=\"Survived\", data=df_train)","55cf926c":"# Use one hot encoding for Pclass feature\nfeatures = preprocessing.OneHotEncoder().fit(df_train['Pclass'].to_numpy().reshape(-1,1))\\\n           .transform(df_train['Pclass'].to_numpy().reshape(-1,1)).toarray()","701235b3":"ax = sns.catplot(x=\"Sex\", col=\"Survived\", kind='count', data=df_train)","d18a7540":"ax = sns.barplot(x=\"Sex\", y=\"Survived\", data=df_train)","3ec56239":"# Add gender to features\nfeature_gender = preprocessing.OneHotEncoder().fit(df_train['Sex'].to_numpy().reshape(-1,1))\\\n           .transform(df_train['Sex'].to_numpy().reshape(-1,1)).toarray()\nfeatures = np.concatenate((features, feature_gender), axis=1)","5e56c596":"ax = sns.kdeplot(df_train.loc[df_train['Survived'] == 1, 'Age'], label=\"Survived\", shade=True)\nax = sns.kdeplot(df_train.loc[df_train['Survived'] == 0, 'Age'], label=\"Not survived\", shade=True)\nax.set_xlabel(\"Age\")","7c73d90f":"# We are missing quite a lot of data for the 'Age' column --> fill with median value\nmedian_age = df_train.loc[df_train['Age'].notna(), 'Age'].median()\nfeature_age= df_train['Age'].copy()\nfeature_age[feature_age.isna()] = median_age\nfeatures = np.concatenate((features, feature_age.to_numpy().reshape((-1, 1))), axis=1)","1a518645":"ax = sns.barplot(x=df_train['SibSp'] + df_train['Parch'], y=df_train['Survived'], data=df_train)\nax.set_xlabel(\"Family size\")","74e7ac22":"feature_family_size = (df_train['SibSp'] + df_train['Parch']).to_numpy().reshape(-1,1)\nfeatures = np.concatenate((features, feature_family_size), axis=1)","bb441706":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler\n\n# get train targets\ntargets = df_train['Survived'].to_numpy()\n# We use 10-fold cross validation\nkf = KFold(n_splits=10)\n\naccuracies_lr = []\nfor train_ind, valid_ind in kf.split(features):\n    X_train, y_train = features[train_ind], targets[train_ind]\n    X_valid, y_valid = features[valid_ind], targets[valid_ind]\n    \n    # Normalize all features including the categorical values\n    scaler = StandardScaler()\n    X_train = scaler.fit(X_train).transform(X_train)\n    X_valid = scaler.transform(X_valid)\n    \n    # Instantiate and fit the logistic regression model\n    model_lr = LogisticRegression(random_state=0, max_iter=500).fit(X_train, y_train)\n    # Calculate accurcies\n    prediction = model_lr.predict(X_valid)\n    accuracies_lr.append((prediction == y_valid).mean())\n    \nprint(\"mean accuracy logistic regression:\", np.mean(accuracies_lr), \"stdDev:\", np.std(accuracies_lr))","f536ef49":"from sklearn.ensemble import RandomForestClassifier\n\naccuracies_forest = []\nfor train_ind, valid_ind in kf.split(features):\n    X_train, y_train = features[train_ind], targets[train_ind]\n    X_valid, y_valid = features[valid_ind], targets[valid_ind]\n    \n    # Normalize all features including the categorical values\n    scaler = StandardScaler()\n    X_train = scaler.fit(X_train).transform(X_train)\n    X_valid = scaler.transform(X_valid)\n    \n    # Instantiate and fit the model\n    model_forest = RandomForestClassifier(n_estimators=100, random_state=0).fit(X_train, y_train)\n    # Calcualte accurcies\n    prediction = model_forest.predict(X_valid)\n    accuracies_forest.append((prediction == y_valid).mean())\n    \nprint(\"mean accuracy random forest:\", np.mean(accuracies_forest), \"stdDev:\", np.std(accuracies_forest))","5a4f3463":"from sklearn.svm import SVC\n\naccuracies_svc = []\nfor train_ind, valid_ind in kf.split(features):\n    X_train, y_train = features[train_ind], targets[train_ind]\n    X_valid, y_valid = features[valid_ind], targets[valid_ind]\n    \n    # Normalize all features including the categorical values\n    scaler = StandardScaler()\n    X_train = scaler.fit(X_train).transform(X_train)\n    X_valid = scaler.transform(X_valid)\n    \n    # Instantiate and fit model\n    model_svc = SVC(random_state=0).fit(X_train, y_train)\n    # Calcualte accurcies\n    prediction = model_svc.predict(X_valid)\n    accuracies_svc.append((prediction == y_valid).mean())\n    \nprint(\"mean accuracy SVC:\", np.mean(accuracies_svc), \"stdDev:\", np.std(accuracies_svc))","9ce1acac":"from sklearn.ensemble import VotingClassifier\n\naccuracies_voting = []\nfor train_ind, valid_ind in kf.split(features):\n    X_train, y_train = features[train_ind], targets[train_ind]\n    X_valid, y_valid = features[valid_ind], targets[valid_ind]\n    \n    # Normalize all features including the categorical values\n    scaler = StandardScaler()\n    X_train = scaler.fit(X_train).transform(X_train)\n    X_valid = scaler.transform(X_valid)\n    \n    # Instantiate the individual models\n    model_svc = SVC(random_state=0)\n    model_forest = RandomForestClassifier(random_state=0)\n    model_lr = LogisticRegression(random_state=0, max_iter=500)\n    \n    voting_estimator = VotingClassifier(estimators=[('lr', model_lr), ('rf', model_forest), ('svc', model_svc)], voting='hard')\n    voting_estimator.fit(X_train, y_train)\n    # Calcualte accurcies\n    prediction = voting_estimator.predict(X_valid)\n    accuracies_voting.append((prediction == y_valid).mean())\n    \nprint(\"mean accuracy Voting:\", np.mean(accuracies_voting), \"stdDev:\", np.std(accuracies_voting))","e6edf792":"# Normalize all features including the categorical values\nscaler_final = StandardScaler()\nfeatures = scaler_final.fit(features).transform(features)\n# Train final model\nmodel_svc_final = SVC(random_state=0)\nmodel_forest_final = RandomForestClassifier(random_state=0)\nmodel_lr_final = LogisticRegression(random_state=0, max_iter=500)\n    \nvoting_estimator_final = VotingClassifier(estimators=[('lr', model_lr), ('rf', model_forest), ('svc', model_svc)], voting='hard')\nvoting_estimator_final.fit(features, targets)","b084c195":"# Same feature engineering as for train set\n# Pclass\nfeatures_test = preprocessing.OneHotEncoder().fit(df_test['Pclass'].to_numpy().reshape(-1,1))\\\n           .transform(df_test['Pclass'].to_numpy().reshape(-1,1)).toarray()\n# Gender\nfeature_gender_test = preprocessing.OneHotEncoder().fit(df_test['Sex'].to_numpy().reshape(-1,1))\\\n           .transform(df_test['Sex'].to_numpy().reshape(-1,1)).toarray()\nfeatures_test = np.concatenate((features_test, feature_gender_test), axis=1)\n# Age --> Use same median age as for train set\nfeature_age_test = df_test['Age'].copy()\nfeature_age_test[feature_age_test.isna()] = median_age\nfeatures_test = np.concatenate((features_test, feature_age_test.to_numpy().reshape((-1, 1))), axis=1)\n# Family size\nfeature_family_size_test = (df_test['SibSp'] + df_test['Parch']).to_numpy().reshape(-1,1)\nfeatures_test = np.concatenate((features_test, feature_family_size_test), axis=1)\n# And rescale the features\nfeatures_test = scaler_final.transform(features_test)","0e92ab4c":"# Predict for test set\npredictions_test = voting_estimator_final.predict(features_test)\n\nsubmission = pd.DataFrame({'PassengerId': df_test['PassengerId'],\n                           'Survived': predictions_test})\nsubmission.to_csv(\"submission.csv\", index=False)","814f0aa4":"As expected, the changes for you to survive are much higher when you are sitting in the first class","d8508483":"# Gender","d8eb2b85":"## Feature Engineering","ca1bbc72":"Yay, first submission ever :)","f2c99b07":"\"Children and women first\" --> expect that women have a higher survival probability","85a8221c":"I am only looking at two different models here:\n\n* Logistic Regressor\n* Random Forest\n* SVC\n* All models mentioned above combined in a voting classifier\n\nI use KFold CV to determine which model to choose","b85a812a":"Okay.....that's more deterministic than I thought","2cc45cb1":"# Passenger class","22981cc6":"Let's start with the passenger class, it is a safe bet that first class passengers should have a higher survival rate","beec4d18":"Okay since this is my first time doing this I don't have much experience with interpretation of a linear correlation map. What I think can be extracted from this is the following:\n* Pclass and Survival show the highest (linear) correlation, so including this as a feature is set\n* Survival and Fare show the second highest correlation, but Fare and Pclass are also correlated rather heavily --> passengers with a higher class paid more. Sounds logical, we won't include Fare as a feature therefore.\n* SibSp and Parch are correlated heavily --> combine them to family size\n* We will not use the 'Embarked', 'Ticket' and 'Cabin' columns","61f42e48":"Pretty balanced, so we don't have to do anything here, lets look at the correlation of the columns.","1625dd48":"# Family size","cb4c8033":"## Training the model","0f505a7d":"The Voting classifier with all three models combined won --> train the final model","a63967c4":"This is my first try at Kaggle, really cool that something like a 'tutorial competition' like this one exists. So let's get started, lets look at the data.","e3b91825":"# Age"}}