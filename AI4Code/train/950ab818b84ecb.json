{"cell_type":{"500f168f":"code","7a304152":"code","5003bc1a":"code","4ebdf0d3":"code","f494b148":"code","b9ce8622":"code","d4db9826":"code","75b993e5":"code","07767165":"code","19d20030":"code","a39cda1c":"code","b57ef24f":"code","623a418d":"code","eff33c78":"code","4ac4ce53":"code","469c8d73":"code","796ed710":"code","7517636e":"code","ce497f23":"code","6e06244a":"code","40d2d2c8":"code","68699baa":"code","0dbea142":"code","0e31b039":"code","1827de30":"code","a07849dc":"code","45014db1":"code","56af9a6a":"code","0352101a":"code","1cc48228":"code","411e12c3":"code","5bf6ede3":"code","d8fdccf7":"code","7cb1603e":"code","ec8e6b11":"code","8c7da151":"code","b6e52017":"code","8f8bf58f":"code","239ef68b":"code","a07b95c5":"code","16dce30f":"markdown","73b7beb5":"markdown","b31fe733":"markdown","75592a17":"markdown","f285bcbd":"markdown","8bed476c":"markdown","93c7f983":"markdown","f62d4484":"markdown","9bb5c3cd":"markdown","d88790d3":"markdown","1e32aa6b":"markdown","5e7c1d6c":"markdown","2f384728":"markdown","e759cc3d":"markdown","bd031abc":"markdown","918f4e1e":"markdown","bbb512f5":"markdown","553cf85a":"markdown","786d1dc3":"markdown","3498cdb3":"markdown","1dba2762":"markdown","e0296c1c":"markdown","e4bebaec":"markdown","87586e72":"markdown","276c76c4":"markdown","da681941":"markdown","0bb7265a":"markdown","c0c8adc3":"markdown","ad92cd19":"markdown","1061496a":"markdown"},"source":{"500f168f":"import numpy as np \nimport pandas as pd \nimport nltk\nimport string as s\nimport re\nfrom nltk.corpus import stopwords\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom wordcloud import WordCloud\n\nfrom sklearn.feature_extraction.text  import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics  import f1_score,accuracy_score\nfrom sklearn.metrics import  confusion_matrix\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import SGDClassifier\nfrom lightgbm import LGBMClassifier","7a304152":"train_data=pd.read_csv(\"\/kaggle\/input\/ag-news-classification-dataset\/train.csv\",header=0,names=['classid','title','desc'])\ntest_data=pd.read_csv(\"\/kaggle\/input\/ag-news-classification-dataset\/test.csv\",header=0,names=['classid','title','desc'])","5003bc1a":"train_data.head()","4ebdf0d3":"test_data.head()","f494b148":"train_x=train_data.desc[:60000]\ntest_x=test_data.desc\ntrain_y=train_data.classid[:60000]\ntest_y=test_data.classid","b9ce8622":"df=train_data[:60000]\nsns.countplot(df.classid);","d4db9826":"world = df.desc[df.classid[df.classid==1].index]","75b993e5":"plt.figure(figsize = (15,20)) ;\nwordcloud = WordCloud(min_font_size = 3,  max_words = 2500 , width = 1200 , height = 800).generate(\" \".join(world))\nplt.imshow(wordcloud,interpolation = 'bilinear');","07767165":"sports = df.desc[df.classid[df.classid==2].index]","19d20030":"plt.figure(figsize = (15,20)) ;\nwordcloud = WordCloud(min_font_size = 3,  max_words = 2500 , width = 1200 , height = 800).generate(\" \".join(sports))\nplt.imshow(wordcloud,interpolation = 'bilinear');","a39cda1c":"biz = df.desc[df.classid[df.classid==3].index]","b57ef24f":"plt.figure(figsize = (15,20)) ;\nwordcloud = WordCloud(min_font_size = 3,  max_words = 2500 , width = 1200 , height = 800).generate(\" \".join(biz))\nplt.imshow(wordcloud,interpolation = 'bilinear');","623a418d":"sci = df.desc[df.classid[df.classid==4].index]","eff33c78":"plt.figure(figsize = (15,20)) ;\nwordcloud = WordCloud(min_font_size = 3,  max_words = 2500 , width = 1200 , height = 800).generate(\" \".join(sci))\nplt.imshow(wordcloud,interpolation = 'bilinear');","4ac4ce53":"def tokenization(text):\n    lst=text.split()\n    return lst\ntrain_x=train_x.apply(tokenization)\ntest_x=test_x.apply(tokenization)","469c8d73":"def lowercasing(lst):\n    new_lst=[]\n    for  i in  lst:\n        i=i.lower()\n        new_lst.append(i) \n    return new_lst\ntrain_x=train_x.apply(lowercasing)\ntest_x=test_x.apply(lowercasing)","796ed710":"def remove_punctuations(lst):\n    new_lst=[]\n    for i in lst:\n        for  j in  s.punctuation:\n            i=i.replace(j,'')\n        new_lst.append(i)\n    return new_lst\ntrain_x=train_x.apply(remove_punctuations) \ntest_x=test_x.apply(remove_punctuations)","7517636e":"def remove_numbers(lst):\n    nodig_lst=[]\n    new_lst=[]\n\n    for i in  lst:\n        for j in  s.digits:\n            i=i.replace(j,'')\n        nodig_lst.append(i)\n    for i in  nodig_lst:\n        if  i!='':\n            new_lst.append(i)\n    return new_lst\ntrain_x=train_x.apply(remove_numbers)\ntest_x=test_x.apply(remove_numbers)","ce497f23":"def remove_stopwords(lst):\n    stop=stopwords.words('english')\n    new_lst=[]\n    for i in lst:\n        if i not in stop:\n            new_lst.append(i)\n    return new_lst\n\ntrain_x=train_x.apply(remove_stopwords)\ntest_x=test_x.apply(remove_stopwords)  ","6e06244a":"lemmatizer=nltk.stem.WordNetLemmatizer()\ndef lemmatzation(lst):\n    new_lst=[]\n    for i in lst:\n        i=lemmatizer.lemmatize(i)\n        new_lst.append(i)\n    return new_lst\ntrain_x=train_x.apply(lemmatzation)\ntest_x=test_x.apply(lemmatzation)","40d2d2c8":"train_x=train_x.apply(lambda x: ''.join(i+' ' for i in x))\ntest_x=test_x.apply(lambda x: ''.join(i+' '  for i in x))","68699baa":"tfidf=TfidfVectorizer(max_features=10000,min_df=6)\ntrain_1=tfidf.fit_transform(train_x)\ntest_1=tfidf.transform(test_x)\nprint(\"No. of features extracted\")\nprint(len(tfidf.get_feature_names()))\nprint(tfidf.get_feature_names()[:20])\n\ntrain_arr=train_1.toarray()\ntest_arr=test_1.toarray()","0dbea142":"NB_MN=MultinomialNB()\nNB_MN.fit(train_arr,train_y)\npred=NB_MN.predict(test_arr)\n\nprint(\"first 20 actual labels\")\nprint(test_y.tolist()[:20])\nprint(\"first 20 predicted labels\")\nprint(pred.tolist()[:20])","0e31b039":"def eval_model(y,y_pred):\n    print(\"F1 score of the model\")\n    print(f1_score(y,y_pred,average='micro'))\n    print(\"Accuracy of the model\")\n    print(accuracy_score(y,y_pred))\n    print(\"Accuracy of the model in percentage\")\n    print(round(accuracy_score(y,y_pred)*100,3),\"%\")","1827de30":"def confusion_mat(color):\n    cof=confusion_matrix(test_y, pred)\n    cof=pd.DataFrame(cof, index=[i for i in range(1,5)], columns=[i for i in range(1,5)])\n    sns.set(font_scale=1.5)\n    plt.figure(figsize=(8,8));\n\n    sns.heatmap(cof, cmap=color,linewidths=1, annot=True,square=True, fmt='d', cbar=False,xticklabels=['World','Sports','Business','Science'],yticklabels=['World','Sports','Business','Science']);\n    plt.xlabel(\"Predicted Classes\");\n    plt.ylabel(\"Actual Classes\");\n    ","a07849dc":"eval_model(test_y,pred)\n    \na=round(accuracy_score(test_y,pred)*100,3)","45014db1":"confusion_mat('YlGnBu')","56af9a6a":"DT=DecisionTreeClassifier()\nDT.fit(train_arr,train_y)\npred=DT.predict(test_arr)\n\nprint(\"first 20 actual labels\")\nprint(test_y.tolist()[:20])\nprint(\"first 20 predicted labels\")\nprint(pred.tolist()[:20])","0352101a":"eval_model(test_y,pred)\n    \nb=round(accuracy_score(test_y,pred)*100,3)\n","1cc48228":"confusion_mat('Blues')","411e12c3":"NB=GaussianNB()\nNB.fit(train_arr,train_y)\npred=NB.predict(test_arr)","5bf6ede3":"eval_model(test_y,pred)\n    \nc=round(accuracy_score(test_y,pred)*100,3)","d8fdccf7":"confusion_mat('Greens')","7cb1603e":"SGD=SGDClassifier()\nSGD.fit(train_arr,train_y)\npred=SGD.predict(test_arr)","ec8e6b11":"eval_model(test_y,pred)\n    \nd=round(accuracy_score(test_y,pred)*100,3)","8c7da151":"confusion_mat('Reds')","b6e52017":"lgbm=LGBMClassifier()\nlgbm.fit(train_arr,train_y)\npred=lgbm.predict(test_arr)","8f8bf58f":"eval_model(test_y,pred)\n\ne=round(accuracy_score(test_y,pred)*100,3)","239ef68b":"confusion_mat('YlOrBr')","a07b95c5":"sns.set()\nfig = plt.figure()\nax = fig.add_axes([0,0,1,1])\nModels = ['MultinomialNB', 'DecisionTree', 'GaussianNB', 'SGD','LGBM']\nAccuracy=[a,b,c,d,e]\nax.bar(Models,Accuracy,color=['#702963','#8a2be2','#9966cc','#df73ff','#702763']);\nfor i in ax.patches:\n    ax.text(i.get_x()+.1, i.get_height()-5.5, str(round(i.get_height(),2))+'%', fontsize=15, color='white')\nplt.title('Comparison of Different Classification Models');\nplt.ylabel('Accuracy');\nplt.xlabel('Classification Models');\n\nplt.show();","16dce30f":"**Function for Displaying the Confusion Matrix**\n\nThis function displays the confusion matrix of the model","73b7beb5":"## Splitting Data into Input and Label ","b31fe733":"### Importing libraries","75592a17":"### Model 4 - Stochastic Gradient Descent Classifier","f285bcbd":"# Classification of News Articles \n\nIt is a notebook for multiclass classification of News articles which are having classes numbered 1 to 4, where 1 is \"World News\", 2 is \"Sports News\", 3 is \"Business News\" and 4 is \"Science-Technology News\".\n\nI have used various models for classification of the News articles. The classification algorithms used are:-\n\n1. Multinomial Naive Bayes\n2. Decision Tree \n3. Gaussian Naive Bayes\n4. Stochastic Gradient Descent Classifier\n5. LGBM (light gradient boosting machine) Classifier\n","8bed476c":"**Function for evaluation of model**\n\nThis function finds the F1-score and Accuracy of the trained model","93c7f983":"## WordCloud of News Articles of Different Types","f62d4484":"### Evaluation of Model","9bb5c3cd":"### Evaluation of Model","d88790d3":"Representation of the distribution of data used for training i.e. 50% of the entire training dataset for the different classes.","1e32aa6b":"## Removal of Punctuation Symbols","5e7c1d6c":"### Sports News","2f384728":"## Feature Extraction\n \n Features are extracted from the dataset and TF-IDF(Term Frequency - Inverse Document Frequency) is used for this purpose.","e759cc3d":"### Model 5 - Light Gradient Boosting Classifier","bd031abc":"## Removal of Stopwords","918f4e1e":"# Training of Model\n\n### Model 1- Multinomial Naive Bayes","bbb512f5":"### Model 2 - Decision Tree Classifier","553cf85a":"### Model 3 - Gaussian Naive Bayes","786d1dc3":"## Lemmatization of Data","3498cdb3":"### Science and Technology News","1dba2762":"## Conversion of Data to Lowercase","e0296c1c":"### Evaluation of Results","e4bebaec":"## Tokenization of Data","87586e72":"### World News","276c76c4":"## Comparison of Accuracies of Different Models","da681941":"### Evaluation of Results","0bb7265a":"# Preprocessing of Data\n\nThe data is preprocessed, in NLP it is also known as text normalization. Some of the most common methods of text normalization are \n* Tokenization\n* Lemmatization\n* Stemming\n","c0c8adc3":"## Removal of Numbers(digits)","ad92cd19":"### Evaluation of Results","1061496a":"### Business News"}}