{"cell_type":{"a4cd8897":"code","68dc83ae":"code","6e5873e6":"code","efb454b5":"code","3e250748":"code","9fcd60eb":"code","7ba8820b":"code","c31ec149":"code","170a5a0d":"code","25d59633":"code","371e3906":"code","246d8577":"markdown","7fac96d1":"markdown","917b45b5":"markdown","0b1e8fc5":"markdown","47b1f34e":"markdown","35b8548d":"markdown","c8b24e34":"markdown","b25f11c5":"markdown","b493012a":"markdown","03dfe784":"markdown","c829c59d":"markdown","da946066":"markdown","65011560":"markdown","3c832732":"markdown","861bd154":"markdown","0f0b85c6":"markdown","9193f992":"markdown"},"source":{"a4cd8897":"import pandas as pd\n\npath_data = \"..\/input\/covid19-ordc-cleaned-metadata\/COVID-19-ORDC-cleaned-metadata.xlsx\"\ndata = pd.read_excel(path_data)\n\nprint(\"Metadata was read. Total number of rows in file: \" + str(len(data[\"title\"])))","68dc83ae":"path_output_data = \"\/kaggle\/working\/COVID-19-ORDC-cleaned-metadata-extended.xlsx\"\n\ncleared_abstract = []\nfor abstract in data[\"abstract\"]:\n\n    if abstract[:8] == \"Abstract\":\n        abstract = abstract[9:]\n\n    cleared_abstract.append(abstract)\ndata[\"abstract\"] = cleared_abstract\n\ndata.to_excel(path_output_data, index=False)\n\nprint(\"'Abstract' text removed\")","6e5873e6":"import re\nimport nltk\n\nstop_words = set(nltk.corpus.stopwords.words(\"english\"))\n\ndef clean_text(text):\n    # To lower\n    result = text.lower()\n    # Remove punctuations\n    result = re.sub('[^a-zA-Z]', ' ', result)\n    # remove special characters and digits\n    result = re.sub(\"(\\\\d|\\\\W)+\", \" \", result)\n    # convert to list from string\n    result = result.split()\n    # [1.] Stemming\n    ps = nltk.stem.porter.PorterStemmer()\n    # [2.] Lemmatisation\n    lem = nltk.stem.wordnet.WordNetLemmatizer()\n    result = [lem.lemmatize(word) for word in result if not word in stop_words]\n\n    return \" \".join(result)","efb454b5":"import numpy as np\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ncount_vectorizer = CountVectorizer(stop_words='english')\n\ncommon_words = []\nfor abstract in data[\"abstract\"]:\n    abstract = [clear_text(abstract)]\n    count_data = count_vectorizer.fit_transform(abstract)\n    words = count_vectorizer.get_feature_names()\n    tmp = words[::-1]\n    total_counts = np.zeros(len(words))\n    for t in count_data:\n        total_counts += t.toarray()[0]\n    count_dict = (zip(words, total_counts))\n    count_dict = sorted(count_dict, key=lambda x: x[1], reverse=True)[0:20]\n    words = [w[0] for w in count_dict]\n    common_words.append(\" \".join(words))\n\n\ndata[\"common_words\"] = common_words\n\ndata.to_excel(path_output_data, index=False)\n\nprint(\"The most common words was defined\")","3e250748":"import tensorflow as tf\nfrom transformers import BertTokenizer, TFBertModel\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = TFBertModel.from_pretrained('bert-base-uncased')\n\nlen_data = len(data[\"abstract\"])\nvectors = [\"\"]*(len_data)\n\nfor i in range(0, len_data):\n    try:\n        # collect sentence and common words\n        sentences = data[\"abstract\"][i].split(\". \")\n        common_words = data[\"common_words\"][i].split(\" \")\n        \n        # tokenize with BERT [3] tokenizer\n        encoded_sentences = []\n        tokenized_sentences = []\n        for sentence in sentences:\n            sentence = sentence[:512]\n            encoded_sentences.append(tokenizer.encode(sentence))\n            tokenized_sentences.append(tokenizer.tokenize(sentence))\n\n        index_sentences = []\n        for sentence in tokenized_sentences:\n            index_sentence = []\n            for word in common_words:\n                try:\n                    tmp = sentence.index(word)\n                    index_sentence.append(tmp+1)\n                except:\n                    continue\n            index_sentences.append(index_sentence)\n        \n        # Create embedded vectors with BERT [3]\n        abstract_vectors = tf.constant([[0]*768]).numpy()\n        for j in range(0,len(encoded_sentences)):\n            if len(index_sentences[j]) != 0:\n                input_ids = tf.constant(encoded_sentences[j])[None, :]\n                outputs = model(input_ids)\n                sentence_vectors = outputs[0][0]\n                sentence_vectors = sentence_vectors.numpy()\n                len_isj = len(index_sentences[j])\n                sentence_vectors_collected = [[0]*768] * len_isj\n                for n in range(0,len_isj):\n                    sentence_vectors_collected[n] = sentence_vectors[index_sentences[j][n]]\n\n                abstract_vectors = np.concatenate((abstract_vectors, sentence_vectors_collected), axis=0)\n\n        encoded_title = tokenizer.encode(data[\"title\"][i])\n        input_ids = tf.constant(encoded_title)[None, :]\n        outputs = model(input_ids)\n        title_vectors = outputs[0][0]\n        title_vectors = title_vectors[1:len(title_vectors)-1].numpy()\n        abstract_vectors = np.concatenate((abstract_vectors, title_vectors), axis=0)\n\n        vektor = tf.reduce_mean(abstract_vectors, 0).numpy()\n        vectors[i] = \"\\t\".join(map(str, vektor))\n\n        # chekpoint\n        if (i % 1000) == 0 and i != 0:\n            data[\"vectors\"] = vectors\n            data.to_excel(path_output_data, index=False)\n            print(\"Processed data: \" + str(i))\n    except: \n        continue\n\ndata[\"vectors\"] = vectors\ndata.to_excel(path_output_data, index=False)\n\nprint(\"Processed data: \" + str(i))\nprint(\"The Embedded vectors was created\")","9fcd60eb":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Read vectors\nnumber_of_vectors = len(data[\"vectors\"])\nvectors = np.array([[0]*768]*number_of_vectors, dtype=float)\nfor i in range(0, number_of_vectors):\n    vector = data[\"vectors\"][i].split(\"\\t\")\n    vectors[i] = np.array(vector, dtype=float)\n\n# Scale vectors\nscaler = MinMaxScaler(feature_range=[0, 1])\nvectors = scaler.fit_transform(vectors)\n\n# Fitting the PCA [4] algorithm on data\npca = PCA().fit(vectors)\nplt.figure()\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('Number of Components')\nplt.ylabel('Variance (%)')\nplt.title('Dataset Variance')\nplt.show()","7ba8820b":"pca = PCA(n_components=200)\nvectors = pca.fit_transform(vectors)\n\nprint(\"PCA is on\")","c31ec149":"import matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom scipy.spatial.distance import cdist\nfrom sklearn.preprocessing import MinMaxScaler\n\ndistortions = []\nmaxK = 100\nK = range(2, maxK, 5)\nfor k in K:\n    kmeanModel = KMeans(n_clusters=k, verbose=0)\n    kmeanModel.fit(vectors)\n    distortions.append(sum(np.min(cdist(vectors, kmeanModel.cluster_centers_, 'euclidean'), axis=1)) \/ vectors.shape[0])\n\nplt.plot(K, distortions, 'bx-')\nplt.xlabel('k')\nplt.ylabel('Distortion')\nplt.title('The Elbow Method showing the optimal k')\nplt.show()","170a5a0d":"kmeanModel = KMeans(n_clusters=15,\n                    verbose=0)\nkmeanModel.fit(vectors)\n\ndata[\"clusters\"] = kmeanModel.labels_\ndata.to_excel(path_output_data, index=False)\n\nprint(\"Custering completed\")","25d59633":"from sklearn.decomposition import LatentDirichletAllocation as LDA\n\npd.options.mode.chained_assignment = None\n\nnumber_of_documents = len(data[\"title\"])\ndata[\"words_of_topic_in_clusters\"] = [\"\"]*number_of_documents\ndata_grouped = data.groupby(['clusters'])\nfor key in data_grouped.groups:\n    group = data_grouped.groups[key]\n    texts = []\n    for index in group:\n        text = clear_text(data[\"title\"][index] + \" \" + data[\"abstract\"][index])\n        texts.append(text)\n    \n    # Create document matrix [7]\n    count_vectorizer = CountVectorizer(stop_words='english')\n    count_data = count_vectorizer.fit_transform(texts)    \n    words = count_vectorizer.get_feature_names()\n\n    number_topics = 1\n    number_words = 10\n    lda = LDA(n_components=number_topics, verbose=0)\n    lda.fit(count_data)\n\n    lda_words = \" \".join([words[i] for i in lda.components_[0].argsort()[:-number_words - 1:-1]])\n    for index in group:\n        data[\"words_of_topic_in_clusters\"][index] = lda_words\n\ndata.to_excel(path_output_data, index=False)\nprint(\"LDA completed\")","371e3906":"number_of_documents = len(data[\"title\"])\nf_vecs = open(\"\/kaggle\/working\/vecs.tsv\",\"w\",encoding=\"utf8\")\nf_meta = open(\"\/kaggle\/working\/meta.tsv\",\"w\",encoding=\"utf8\")\nf_meta.write(\"title\\tclusters\\twords_of_topic\\n\")\nfor i in range(0, number_of_documents):\n    f_vecs.write(data[\"vectors\"][i] + \"\\n\")\n    f_meta.write(data[\"title\"][i] + \"\\t\" + str(data[\"clusters\"][i]) + \"\\t\" + data[\"words_of_topic_in_clusters\"][i] + \"\\n\")\nf_vecs.close()\nf_meta.close()\n\nprint(\".tsv files creating completed\")","246d8577":"# COVID-19 Contextual Analysis with Bidirectional Encoder Representations from Transformers (BERT)","7fac96d1":"*Text cleaning*\n\nA function ('clean_text') has been defined which clean the body text.\n\nTasks of the function  are as follows:\n* To lower\n* Remove punctuations\n* Remove special characters and digits\n* Stemming\n* Lemmatisation","917b45b5":"*Reduction to 200 dimensions seems like a good choice*","0b1e8fc5":"*Define the optimal cluster number*\n\nTo determine the optimal number of clusters with k-elbow [5] method. Ideal point on curve will be correct cluster number. This point is that where the distortion start decreasing in a linear fashion.","47b1f34e":"*Create embedding vectors with BERT*\n\nThe BERT system will be used to embed the texts.\n\nThe steps is follows:\n1. Read a record\n2. Split the text of the abstract field to sentences\n3. Tokenize the sentences with BERT tokenizer\n4. Encode the sentences with BERT encoder\n5. Fit the sentence with BERT\n6. Collect the most common and title words's vectors from BERT's embedding layer\n7. Calculate mean of the vectors\n8. Repeat these steps on all records","35b8548d":"*Loading the Data*\n\nNew dataset has been created from original covid-19 nlp dataset using Excell. It includes an filtered metadata file (COVID-19-ORDC-cleaned-metadata.xlsx). The file consist of only such as records which abstract field of records longer than 200 character.\n\n[NEW DATASET](https:\/\/www.kaggle.com\/robertlakatos\/covid19-ordc-cleaned-metadata)","c8b24e34":"*Calculate the best size of PCA [4] dimension*\n\nTo speed it up the calculation the program calculate the variance. Keep the variance of 90 % is an good choose to properly number of dimensions.","b25f11c5":"*Remove 'Abstract' text all of front of abstract contents*\n\nEvery abstract field in the dataset start with an 'Abstract' text. It is an irrelevant information so it has to remove from every fields.","b493012a":"# References\n\n[[1.] Stemming](https:\/\/en.wikipedia.org\/wiki\/Stemming#cite_note-1)\n\n[[2.] Lemmatisation](https:\/\/en.wikipedia.org\/wiki\/Lemmatisation)\n\n[[3.] Bidirectional Transformers forLanguage Understanding (BERT)](https:\/\/arxiv.org\/pdf\/1810.04805.pdf)\n\n[[4.] Principal component analysis (PCA)](https:\/\/www.tandfonline.com\/doi\/abs\/10.1080\/14786440109462720#aHR0cHM6Ly93d3cudGFuZGZvbmxpbmUuY29tL2RvaS9wZGYvMTAuMTA4MC8xNDc4NjQ0MDEwOTQ2MjcyMD9uZWVkQWNjZXNzPXRydWVAQEAw)\n\n[[5.] Elbow method (clustering)](https:\/\/en.wikipedia.org\/wiki\/Elbow_method_(clustering)\n\n[[6.] k-means clustering](https:\/\/en.wikipedia.org\/wiki\/K-means_clustering#cite_note-lloyd19572-4)\n\n[[7.] Document-term matrix](https:\/\/en.wikipedia.org\/wiki\/Document-term_matrix)\n\n[[8.] Latent Dirichlet Allocation (LDA)](https:\/\/web.archive.org\/web\/20120207011313\/http:\/\/jmlr.csail.mit.edu\/papers\/volume3\/blei03a\/blei03a.pdf)\n\n[[9.] t-distributed stochastic neighbor embedding (t-SNE)](http:\/\/jmlr.org\/papers\/volume9\/vandermaaten08a\/vandermaaten08a.pdf)\n\n[[10.] projector.tensorflow.org](https:\/\/projector.tensorflow.org\/)","03dfe784":"# Table of Contents\n\n1. Loading the Data\n2. Remove 'Abstract' text all of front of abstract contents\n3. Text cleaning\n4. Define the most common words in abstracts\n5. Create embedding vectors with BERT\n6. Calculate the best size of PCA dimension\n7. Reduction to 200 dimensions is seem good choice\n8. Define the optimal cluster number\n9. Create clusters using k-mean algorithm\n10. To genearte topic words to clusters with LDA analysis\n11. To genearte .tsv files for 'projector.tensorflow.org'","c829c59d":"*Create clusters using k-mean [6] algorithm*\n\nAny cluster number between 15 and 40 seems like a good choice.","da946066":"# Approach\n\n1. Data preparation (Loading, cleaning...)\n3. Body text embedding with BERT\n4. Dimension reduction with PCA\n5. Application of the K-elbow method to define optimal cluster number\n6. Clustering with K-mean algorithm\n7. To define the topic words to the clusters\n8. Saveing data\n9. Visualizing data with PCA and t-SNE using projector.tensorflow.org","65011560":"*Define the most common words in abstracts*\n\nThe program has to calculate the most common words because these words need to the body text embedding.","3c832732":"*To genearte topic words to clusters with LDA analysis*\n\nEach cluster can be treated as a set of different topics. LDA [8] is suitable for finding words that characterize different topics.","861bd154":"*To genearte .tsv files for 'projector.tensorflow.org' [10]*\n\nprojector.tensorflow.org is an very usefull and simple online visualizaton tool from Google. We can make PCA [4] and t-SNE [9] analyzis on our data easy way with this tool. projector.tensorflow.org works with .tsv file format so before using the projector have to creat these files. The visualizaton  tool use two files. These are the meta and vectors. The vectors contains data vectors and the meta contains data describing the vectors.","0f0b85c6":"# Goal\n\nClustering the contents of the data set provided by COVID-19 Open Research Dataset using Bidirectional Encoder Representations from Transformers (BERT).\n\nThe metadata.csv was used for the analysis. The file contains metadata for more than 28,000 documents. The file includes two important fields. These are abstract and titel fields.  Help of these two fields of documents has been created an text body, which was embedded in a single vector space using BERT.\n\nThe solution of the problem is based on the idea that the elements of the embedded vector space formed from the text body of documents define the content of the text more precisely than the methods based on the document matrix. Therefore, clustering performed on such a body of text can more accurately show the content distribution of documents.\n\nFurthermore plan was that, let each cluster is labeled with keywords. \n\nPart of the solution is the source code written in python and all the outputs of the solver program as well.","9193f992":"**Principal Component Analysis of dataset (PCA) was illustrated with the help of projector.tensorflow.org**\n\n![ep_pca_1.jpg](attachment:ep_pca_1.jpg)\n\n**T-distributed Stochastic Neighbor Embedding (t-SNE) of dataset was illustrated with the help of projector.tensorflow.org**\n\n![ep_tsne_1.jpg](attachment:ep_tsne_1.jpg)\n\n![ep_tsne_2.JPG](attachment:ep_tsne_2.JPG)\n\n![ep_tsne_3.JPG](attachment:ep_tsne_3.JPG)"}}