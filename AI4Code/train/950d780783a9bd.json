{"cell_type":{"efb2a76f":"code","637b52b6":"code","f9293202":"code","c91f139d":"code","b270eee9":"code","bb4284fa":"code","20baad58":"code","a8bb02ec":"code","971a3557":"code","87ffe60e":"code","0623bffa":"code","ba1c15fd":"code","010a2b9c":"code","d84c20a9":"code","43c31587":"code","abc40db4":"code","5d52298c":"code","b0a72d22":"code","36d85147":"code","6f1755e9":"code","92fae3e4":"markdown","94c52638":"markdown","40e4a024":"markdown","507e02ce":"markdown","4a344451":"markdown","12cc2f5d":"markdown","e5ca8bac":"markdown","d4bb7e8b":"markdown","c60f15d1":"markdown","4667513f":"markdown","4d0c51fa":"markdown","c7c34c37":"markdown","2cc55169":"markdown","b5e6e828":"markdown","6beb99d9":"markdown"},"source":{"efb2a76f":"#visualization\nimport matplotlib.pyplot as plt\n#data manipulations\nimport numpy as np\nimport pandas as pd\n#pytorch\nimport torchvision\nimport torch\nimport torch.optim as optim\nimport torch.nn as nn\nimport torch.nn.functional as f\n#scraping from internet\nfrom PIL import Image\n#scraping from internet\nimport requests\nfrom io import BytesIO\n# time related\nfrom timeit import default_timer as timer","637b52b6":"# transforms for images\ntransforms = torchvision.transforms.Compose([\n    torchvision.transforms.Resize(256),\n    torchvision.transforms.CenterCrop(224),\n    torchvision.transforms.ToTensor(),\n    torchvision.transforms.RandomHorizontalFlip(p = 0.5)\n])\n\n\n# datasets\ntrainset = torchvision.datasets.ImageFolder(\"..\/input\/70-dog-breedsimage-data-set\/train\", transform = transforms)\nvalidset = torchvision.datasets.ImageFolder(\"..\/input\/70-dog-breedsimage-data-set\/valid\", transform = transforms)\ntestset = torchvision.datasets.ImageFolder(\"..\/input\/70-dog-breedsimage-data-set\/test\", transform = transforms)\n\n#batches\nbatch_size = 128\n\n# loaders for data\ntrainloader = torch.utils.data.DataLoader(trainset , batch_size=batch_size , shuffle = True)\nvalidloader = torch.utils.data.DataLoader(validset , batch_size=batch_size , shuffle = True)\ntestloader = torch.utils.data.DataLoader(testset  , batch_size=batch_size)","f9293202":"images, labels = iter(trainloader).next()\nimages, labels = images.numpy() , labels.numpy()\n\nfig = plt.figure(figsize = (15,5))\n\nfor i in range(int(batch_size\/8)):\n    ax = fig.add_subplot(2 , int(batch_size\/16) , i + 1 , xticks = [] , yticks = [])\n    ax.imshow(np.transpose(images[i] , (1,2,0)) , cmap = 'gray')\n    ax.set_title(trainset.classes[labels[i]])","c91f139d":"# Single batch\nprint(\"number of train batches : \", len(trainloader))\nprint(\"number of validation batches : \", len(validloader))\nprint(\"Size of test batches : \", len(testloader))","b270eee9":"print(\"Classes : \", trainset.classes)","bb4284fa":"# Whether to train on a gpu\nmy_gpu = torch.cuda.is_available()\nprint(f'Train on gpu: {my_gpu}')\ndevice = torch.device('cuda' if my_gpu else 'cpu')\nprint(f\"my device: {torch.cuda.get_device_name(0)}\")","20baad58":"\n\nmodel = torchvision.models.resnet18(pretrained = True)# Use ResNet 18\n#freeze model params\nfor param in model.parameters():\n    param = param.requires_grad_(False)\n\n#new layer\nmodel.fc = nn.Sequential(\n                      nn.Linear(model.fc.in_features, 256),\n                      nn.ReLU(),\n                      nn.Dropout(0.4),\n                      nn.Linear(256, len(trainset.classes)),                   \n                      nn.LogSoftmax(dim=1))\n\n\nprint(\"The new layer is : \",model.fc)\nmodel = model.to(device) #Moving the model to GPU","a8bb02ec":"lr = 1e-3\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters() , lr = lr)","971a3557":"#format class\n\nclass color_formats:\n    \"\"\"\n    Simple color formating:\n    Variables:\n        >OKCYAN - cyan printing.\n        >Bold - bold printing.\n        >UNDERLINE - underline printing.\n    \"\"\"\n    OKCYAN = '\\033[96m'\n    ENDC = '\\033[0m'\n    BOLD = '\\033[1m'\n    UNDERLINE = '\\033[4m'","87ffe60e":"def train_loop(model,\n          criterion,\n          optimizer,\n          train_loader,\n          valid_loader,\n          save_model_name,\n          max_epochs_stop=3,\n          num_epochs=20,\n          num_epochs_report=1):\n    \"\"\"Train a neural network Model\n    Args\n    --------\n        >model (Pytorch nn model): The neural network for the training process.  \n        >criterion (Pytorch loss function): Initialize the loss function.\n        >optimizer (Pytorch optimizer): Use an optimizer to compute gradients to update the weights.\n        >train_loader (Pytorch dataloader): training dataloader to iterate through.\n        >valid_loader (Pytorch dataloader): validation dataloader used for early stopping.\n        >save_model_name (str): file path to save the model state dict, file name ends with 'pt.'.\n        >max_epochs_stop (int): maximum number of epochs with no improvement in validation loss for early stopping.\n        >num_epochs (int): maximum number of training epochs if the early stopping is not activated.\n        >num_epochs_report (int): frequency of epochs to print training reports.\n    \n    outputs\n    --------\n        >model (PyTorch model): Trained neural networks with best weights.\n        >history (DataFrame): History of train and validation loss and accuracy.\n    \"\"\"\n\n    # Early stopping intialization\n    epochs_no_improve = 0\n    valid_loss_min = np.Inf  \n\n    valid_max_acc = 0\n    history = []\n\n    # Number trained epochs  (while using loaded in model weights)\n    try:\n        print(f'Model has been trained for: {model.epochs} epochs.\\n')\n    except:\n        model.epochs = 0\n        print(f'{color_formats.BOLD + color_formats.UNDERLINE}Training activated:{color_formats.ENDC}\\n')\n\n    overall_start = timer()\n\n    # Start of loop\n    for epoch in range(num_epochs):\n\n        # Track of training and validation loss for each epoch\n        train_loss = 0.0\n        valid_loss = 0.0\n\n        train_acc = 0\n        valid_acc = 0\n\n        # Set model to train\n        model.train()\n        \n        #start timer\n        start = timer()\n\n        # Training loop\n        for ii, (data, target) in enumerate(train_loader):\n            # Put the data in the gpu\n            if my_gpu:\n                data, target = data.to(device), target.to(device)\n\n            # Remove past gradients\n            optimizer.zero_grad()\n            # Predicted outputs as log probabilities\n            output = model(data)\n\n            # Loss and backpropagation of gradients\n            loss = criterion(output, target.long())\n            loss.backward()\n\n            # Update the weights\n            optimizer.step()\n\n            # Track train loss by multiplying average loss by number of examples in batch\n            train_loss += loss.item() * data.size(0)\n\n            # Calculate accuracy by finding max log probability\n            _, pred = torch.max(output, dim=1)\n            correct_tensor = pred.eq(target.data.view_as(pred))\n            # Need to convert correct tensor from int to float to average\n            accuracy = torch.mean(correct_tensor.type(torch.FloatTensor))\n            # Multiply average accuracy times the number of examples in batch\n            train_acc += accuracy.item() * data.size(0)\n\n            # Training progress tracker\n            print(\n                f'Epoch: {epoch+1}\\t{100 * (ii + 1) \/ len(train_loader):.2f}% complete. {timer() - start:.2f} seconds elapsed in epoch.',\n                end='\\r')\n\n\n        # Start validation after training loops ends.\n        else:\n            model.epochs += 1\n\n            # Deactivate the gradient tracking. \n            with torch.no_grad():\n                # Set to evaluation mode\n                model.eval()\n\n                # Validation loop\n                for data, target in valid_loader:\n                    # Put the data in the gpu\n                    if my_gpu:\n                        data, target = data.to(device), target.to(device)\n\n                    # Predicted outputs as log probabilities\n                    output = model(data)\n\n                    # Validation loss\n                    loss = criterion(output, target.long())\n                    # Multiply average loss times the number of examples in batch\n                    valid_loss += loss.item() * data.size(0)\n\n                    # Calculate accuracy of validation set\n                    _, pred = torch.max(output, dim=1)\n                    correct_tensor = pred.eq(target.data.view_as(pred))\n                    accuracy = torch.mean(\n                        correct_tensor.type(torch.FloatTensor))\n                    # Multiply average accuracy times the number of examples\n                    valid_acc += accuracy.item() * data.size(0)\n\n                # Calculate average losses\n                train_loss = train_loss \/ len(train_loader.dataset)\n                valid_loss = valid_loss \/ len(valid_loader.dataset)\n\n                # Calculate average accuracy\n                train_acc = train_acc \/ len(train_loader.dataset)\n                valid_acc = valid_acc \/ len(valid_loader.dataset)\n\n                history.append([train_loss, valid_loss, train_acc, valid_acc])\n\n                # Print training and validation results for the num_epochs_report that was set\n                if (epoch + 1) % num_epochs_report == 0:\n                    print(\n                        f'\\n\\nEpoch: {color_formats.BOLD}{epoch+1}{color_formats.ENDC} \\tTraining Loss: {color_formats.BOLD}{train_loss:.4f}{color_formats.ENDC} \\tValidation Loss: {color_formats.BOLD}{valid_loss:.4f}{color_formats.ENDC}'\n                    )\n                    print(\n                        f'\\t\\tTraining Accuracy: {color_formats.BOLD}{100 * train_acc:.2f}%{color_formats.ENDC}\\t Validation Accuracy: {color_formats.BOLD}{100 * valid_acc:.2f}%{color_formats.ENDC}\\n'\n                    )\n\n                # Save the model if validation loss decreases\n                if valid_loss < valid_loss_min:\n                    # Save model\n                    torch.save(model.state_dict(), save_model_name)\n                    # Track improvement\n                    epochs_no_improve = 0\n                    valid_loss_min = valid_loss\n                    valid_best_acc = valid_acc\n                    best_epoch = epoch + 1\n\n                # Otherwise count all consecutive epochs with no improvement.\n                else:\n                    epochs_no_improve += 1\n                    # Trigger early stopping\n                    if epochs_no_improve >= max_epochs_stop:\n                        print(\n                            f'\\n\\n{color_formats.OKCYAN}Early stopping activated!{color_formats.ENDC}\\nthe validation loss has not improved for {max_epochs_stop} epochs.\\n\\n{color_formats.BOLD + color_formats.UNDERLINE}End of training report:{color_formats.ENDC}\\n\\n\\t-Total epochs: {epoch+1} \\n\\t-Best epoch: {color_formats.BOLD}{best_epoch}{color_formats.ENDC} \\n\\t-loss: {color_formats.BOLD}{valid_loss_min:.2f}{color_formats.ENDC} \\n\\t-accuracy: {color_formats.BOLD}{100 * valid_best_acc:.2f}%\\n{color_formats.ENDC}'\n                        )\n                        total_time = timer() - overall_start\n                        print(\n                            f'{total_time:.2f} total seconds elapsed. {total_time \/ (epoch+1):.2f} seconds per epoch.'\n                        )\n\n                        # Load the best state dict\n                        model.load_state_dict(torch.load(save_model_name))\n                        # Attach the optimizer\n                        model.optimizer = optimizer\n\n                        # History update\n                        history = pd.DataFrame(\n                            history,\n                            columns=[\n                                'train_loss', 'valid_loss', 'train_acc',\n                                'valid_acc'\n                            ])\n                        return model, history\n\n\n    # Record overall time and print out Report\n    total_time = timer() - overall_start\n    print(\n        f'\\nBest epoch is epoch #{best_epoch} with loss: {valid_loss_min:.2f} and acc: {100 * valid_acc:.2f}%'\n    )\n    print(\n        f'{total_time:.2f} total seconds elapsed. {total_time \/ (epoch):.2f} seconds per epoch.'\n    )\n    # History update\n    history = pd.DataFrame(\n        history,\n        columns=['train_loss', 'valid_loss', 'train_acc', 'valid_acc'])\n    return model, history","0623bffa":"# Running the model\nmodel, history = train_loop(\n    model,\n    criterion,\n    optimizer,\n    trainloader,\n    validloader,\n    save_model_name=\".\/model1.pt\",\n    max_epochs_stop=20,\n    num_epochs=100,\n    num_epochs_report=5)","ba1c15fd":"#models logs in detail over time of training.\n\nhistory.index = [idx for idx in range(1, history.shape[0]+1)] #from first epoch\nfig, axs = plt.subplots(1, 2, figsize = (10,5))\nfig.suptitle('History Log',  size = 20)\n#ethnicity model log\n\naxs[0].plot(history[\"train_loss\"], label = \"train\")\naxs[0].plot(history[\"valid_loss\"], label = \"validation\")\naxs[0].set_xlabel(\"epochs\")\naxs[0].set_ylabel(\"loss\")\naxs[0].legend()\naxs[0].set_title('Dog breed Classifier model loss')\n\naxs[1].plot(history[\"train_acc\"], label = \"train\")\naxs[1].plot(history[\"valid_acc\"], label = \"validation\")\naxs[1].set_xlabel(\"epochs\")\naxs[1].set_ylabel(\"accuracy\")\naxs[1].legend()\naxs[1].set_title('Dog breed Classifier accuracy')\n\n\nplt.show()","010a2b9c":"def Accuracy_report(loader = None, model = None, n_classes = None):\n    \"\"\"\n    Args:\n    >loader (pytorch dataloader): the data for accuracy testing.\n    >model (pytorch model) : the neural network.\n    >n_classes (int): the number of classes.\n    \n    Output: \n    > class_acc (dict) : accuracy per classes. non existant taregts in the test set are set to nan value.\n    > acc (float): overall accuracy.\n    \"\"\"\n    my_classes = []\n\n    classes = [n_class for n_class in range(n_classes)]\n    correct_pred = {classname: 0 for classname in classes}\n    total_pred = {classname: 0 for classname in classes}\n\n    with torch.no_grad():\n        for data in loader:\n            inputs, targets = data \n            inputs = inputs.to(device)\n            targets = targets.to(device)           \n            outputs = model(inputs)    \n            _, predictions = torch.max(outputs, 1)\n\n            # collect the correct predictions for each class\n            for target, prediction in zip(targets, predictions):\n                if target == prediction:\n                    correct_pred[classes[target]] += 1\n                total_pred[classes[target]] += 1\n\n    for classname, correct_count in correct_pred.items():\n        try:\n            accuracy = 100 * float(correct_count) \/ total_pred[classname]\n\n            my_classes.append(accuracy)\n        except ZeroDivisionError:\n            my_classes.append(np.nan)\n            continue\n    \n    acc =  round(100 * float(sum(correct_pred.values())\/sum(total_pred.values())),2)\n    class_acc = dict(zip(classes,[round(mc,2) for mc in my_classes]))\n    return class_acc, acc\n\ndef test_report(class_acc,acc):\n    \"\"\"\n    Args:\n        > my_classes (dict) : accuracy per classes. non existant taregts in the test set are set to nan value.\n        > acc (float): overall accuracy.\n    \n    Output:\n        >report of test performance.\n    \"\"\"    \n    print(f\"{color_formats.BOLD + color_formats.UNDERLINE}Test Accuracy Report{color_formats.ENDC}\")\n    for key,value in class_acc.items():\n        print(f\"Class {trainset.classes[key]} has achived {color_formats.BOLD}{value}%{color_formats.ENDC} accuracy\\n\")\n    print(f\"Overall accuracy: {color_formats.BOLD}{acc}%{color_formats.ENDC}\")\n    ","d84c20a9":"m1_test_class_acc, m1_test_acc = Accuracy_report(loader = testloader,model = model, n_classes = 70)\ntest_report(m1_test_class_acc,m1_test_acc)","43c31587":"dataiter = iter(testloader)\nimages, labels = dataiter.next()\n# get predictions\npreds = np.squeeze(model(images.cuda()).data.max(1, keepdim=True)[1].cpu().numpy())\nimages = images.cpu().numpy()\n\n# plot the images in the batch, along with predicted and true labels\nfig = plt.figure(figsize=(25, 4))\nfor idx in np.arange(int(batch_size\/8)):\n    ax = fig.add_subplot(2, int(batch_size\/16), idx+1, xticks=[], yticks=[])\n    ax.imshow(np.squeeze(images[idx].transpose(1,2,0)), cmap='gray')\n    ax.set_title(\"{} ({})\".format(trainset.classes[preds[idx]], trainset.classes[labels[idx]]),\n                 color=(\"green\" if preds[idx]==labels[idx] else \"red\"))\nplt.tight_layout()","abc40db4":"url = \"https:\/\/i.pinimg.com\/474x\/95\/99\/07\/959907c103998d280743ea0ea120121b.jpg\"\nresponse = requests.get(url)\nimg = Image.open(BytesIO(response.content))\nimg","5d52298c":"transforms = torchvision.transforms.Compose([\n    torchvision.transforms.Resize(224),\n    torchvision.transforms.Resize(256),\n    torchvision.transforms.CenterCrop(224),\n    torchvision.transforms.ToTensor(),\n    torchvision.transforms.RandomHorizontalFlip(p = 0.5)\n])\n\n\ndef predictor(img, n=5):\n    \"\"\"\n    Args: \n        >img - the image to predict.\n        >n - number of top probabilities.\n    \n    Outputs:\n        >pred - the top prediction.\n        > top preds - top n predictions.\n    \"\"\"\n    #transform the image\n    img = transforms(img)\n    # get the class predicted \n    pred = int(np.squeeze(model(img.unsqueeze(0).cuda()).data.max(1, keepdim=True)[1].cpu().numpy()))\n    # the number is also the index for the class label\n    pred = trainset.classes[pred]\n    # get model log probabilities\n    preds = torch.from_numpy(np.squeeze(model(img.unsqueeze(0).cuda()).data.cpu().numpy()))\n    # convert to prediction probabilities of the top n predictions\n    top_preds = torch.topk(torch.exp(preds),n)\n    #display at an orgenized fasion\n    top_preds = dict(zip([trainset.classes[i] for i in top_preds.indices],[f\"{round(float(i)*100,2)}%\" for i in top_preds.values]))\n    return pred, top_preds","b0a72d22":"my_prediction, top_predictions = predictor(img, n=5)","36d85147":"my_prediction","6f1755e9":"top_predictions","92fae3e4":"## Organize data for neural network training","94c52638":"the overall accuracy is impressive though the low performance at 1 or 2 classes shows a room for improvement.\n\n## Visualization of predictions","40e4a024":"## Conclusion\nthis model has room for improvement.\n\nbut overall performance is impressive.\nplease check out the [webapp](https:\/\/share.streamlit.io\/rabi320\/dogbreed\/DogBreed.py) diployed using this app!\n![](https:\/\/media4.giphy.com\/media\/3oEdva9BUHPIs2SkGk\/giphy.gif)\n","507e02ce":"## Model initiation:\n\nThe model itself is based in pretrained resnet.\n\nthe weights of the resnet are frozen from gradient computing because the method being used is transfer learning.\n\nthe last layer is a sequential layer that is used to create a last array of log probabilities for prediction.","4a344451":"## Dog Breeds","12cc2f5d":"## Training Loop\n\nThis train loop function is created for optimal training of the neural network.\n### the process:\n1. Initiate timer, history log and early stoping mechanism.\n2. Begin training loop - initiate loss calculations and accuracy calculation of training and validation.\n3. After updating Loss, accuracy and weights, begin validation.\n4. calculating validation loss and accuracy and continue if the condition of lowest loss after a requiered number of epochs is not met.\n5. print statistics each repeatitive number of times.\n6. append history log for each epoch.\n7. save best epoch model weights as a desired name ending with `.pt`. \n8. calulate time and progress bar of each epoch and whole process timer.\n9. if the condition for minimus validation loss is not met, stop after a requiered number of epochs.\n10. return the log and model chosen. \n\n","e5ca8bac":"## Loss and Optimizer\n\nThe loss is Cross Entropy - ment for multiclass classification.\n\nThe optimizer is adam: parameters are params(`model.parameters()`)  learning rate(`lr`)","d4bb7e8b":"## Random image from the web\nthe function used below will determine the class of this image.","c60f15d1":"## For formating purpose the color class is initiated:","4667513f":"## Gpu information\nkaggle gpu is Tesla P100-PCIE-16GB,\ngood enough for a simple image classification.\nchecking availability is also important.","4d0c51fa":"# Dog Breed Classification\n![](https:\/\/media3.giphy.com\/media\/cfuGFUnMy4A28vtuCI\/giphy.gif)\n\n## Libraries:","c7c34c37":"The log clearly shows the epoch chosen was right before the model could overfit.\n## Accuracy report\nthis will give us the accuracy report of the model preforming here by each class and as a whole.","2cc55169":"## Some images and class labels for display:","b5e6e828":"## Visualize history log:\nthis will show us the optimiztion of the validation loss in this model.","6beb99d9":"## Batch count"}}