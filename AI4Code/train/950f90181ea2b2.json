{"cell_type":{"6d77df71":"code","1e8f0a4a":"code","b85f5dac":"code","3a834340":"code","1e9991fa":"code","3a28f575":"code","78b6192e":"code","ed3f05f4":"markdown","94247aa0":"markdown","d9035f5a":"markdown","b4401da2":"markdown"},"source":{"6d77df71":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1e8f0a4a":"train_df = pd.read_csv('\/kaggle\/input\/da-datathon-2021\/Train.csv')\ntest_df  = pd.read_csv('\/kaggle\/input\/da-datathon-2021\/Test.csv')\nsub_df   = pd.read_csv('\/kaggle\/input\/da-datathon-2021\/sample_subs.csv')\ntrain_df.head(3)","b85f5dac":"import gc\ndef encode_LE(train,test,cols,verbose=True):\n    for col in cols:\n        df_comb = pd.concat([train[col],test[col]],axis=0)\n        df_comb,_ = df_comb.factorize(sort=True)\n        nm = col\n        if df_comb.max()>32000: \n            train[nm] = df_comb[:len(train)].astype('int32')\n            test[nm] = df_comb[len(train):].astype('int32')\n        else:\n            train[nm] = df_comb[:len(train)].astype('int16')\n            test[nm] = df_comb[len(train):].astype('int16')\n        del df_comb; x=gc.collect()\n        if verbose: print(nm,', ',end='')\n\nencode_LE(train_df, test_df,['Readiness_to_battle','Dragon_attacks','Settlement_Stable','Council_Heading','Boats','Fort_to_report'])","3a834340":"target_y = train_df['Select_for_battle'].values\ncolumns2drop = ['Dothraki_id']\ntrain_df = train_df.drop(columns2drop+['Select_for_battle'],axis=1)\ntest_df  = test_df.drop(columns2drop,axis=1)\nprint(f'Shape of train_data:- {train_df.shape}')\nprint(f'Shape of train_data:- {test_df.shape}')\ntrain_df = train_df.fillna(0)\ntest_df  = test_df.fillna(0)","1e9991fa":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import *\nfrom sklearn.model_selection import train_test_split","3a28f575":"X = train_df.values\ny = target_y\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n\nmodel            = LogisticRegression()\nmodel.fit(X_train,y_train)\ny_pred_test_prob = model.predict_proba(X_test)[:,1]\n\nprint(f'ROC-AUC score:- ',roc_auc_score(y_test,y_pred_test_prob))\ny_preds_proba = model.predict_proba(test_df.values)[:,1]\ny_output      = y_preds_proba\ny_output","78b6192e":"sub_df['Select_for_battle'] = y_output\nsub_df.to_csv('trial.csv',index=False)","ed3f05f4":"## Label Encoding categorical columns\n### Things to explore :- \n1. The below function gives a boiler-plate label encoder for categorical columns. We can also use the one from sklearn\n2. Can we use One hot encoder ?? \n3. If OHE is preferred over LE, then how to handle sparsity of data?\n4. Address the concerns related to data leakage while using - 'fit' and 'transform' methods on train and test datasets","94247aa0":"## Model Development\nTo explore:- \n1. What models can be considered for the data created above ?\n2. Stratified K-fold vs Grouped K-Fold vs train-test split ?? (which one will you select here and why?)\n3. More about the metric \n4. Can we use the metric in model itself ? \n5. Can we handle data imbalance here only?","d9035f5a":"## Feature Engineering \nThe below segment provides a simple code to do the most basic feature engineering (Dropping columns to make train and test data have same columns). Creating more features is highly encouraged\n### To explore:- \n1. How can we create more features ?? \n2. Dropping redundant columns ? \n3. Better way to impute nan values ?? (or can we keep them as they are and then use a model that can handle NaN values)\n4. Data Imbalance ?","b4401da2":"## Loading train and test datasets\n\nThings to explore :- \n1. Knowing what train and test dataset is ? How to use them in real life ? \n2. What distinguishes a train data from test data (supervised learning premise) \n"}}