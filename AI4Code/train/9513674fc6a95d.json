{"cell_type":{"aab649ea":"code","53873fc1":"code","f19b4a1d":"code","506216d2":"code","b93bb467":"code","c2a378ec":"code","7d702de7":"code","a8103801":"code","19485df3":"code","b7856f67":"code","e920eeff":"code","df7f8a02":"code","d421d502":"code","75b1f837":"code","6d3204ad":"code","0672df0a":"code","698ede0f":"code","a6777bce":"markdown","f503a50d":"markdown","70c1ec2e":"markdown","d05f17e6":"markdown","043c261d":"markdown","5484b6d0":"markdown","c9f5152d":"markdown","6be4e0ff":"markdown"},"source":{"aab649ea":"import tensorflow as tf \nimport tensorflow.keras as keras \nimport tensorflow.keras.optimizers as optimizers  \nimport tensorflow.keras.layers as layers\nimport tensorflow.keras.losses as losses \nimport tensorflow.keras.metrics as metrics\nimport tensorflow.keras.regularizers as regularizers\nimport gc","53873fc1":"from keras.datasets import cifar10 \n\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()","f19b4a1d":"x_train = x_train.reshape(x_train.shape[0],-1)\nx_test  = x_test.reshape(x_test.shape[0],-1)\ny_train = y_train.reshape(-1)\ny_test = y_test.reshape(-1)","506216d2":"x_train.shape","b93bb467":"#####This is what REAL overfitting looks like!!!!##################\n\nLEARNING_RATE = tf.keras.optimizers.schedules.ExponentialDecay(\n    initial_learning_rate=0.001, decay_steps=625*10, decay_rate=0.8, staircase=False, name=None\n)\nREGULARIZATION_LAMBDA = 0.000000001\nBATCH_SIZE = 64\nN_EPOCHS = 1000\nVALIDATION_SPLIT= 0.2\n\n#3072 is the input size iirc\nmodel = keras.Sequential(\n[  \n    layers.Dense(512, activation=layers.LeakyReLU(alpha=0.1),kernel_regularizer=regularizers.L2(l2=REGULARIZATION_LAMBDA), bias_regularizer=regularizers.L2(l2=REGULARIZATION_LAMBDA)),\n    layers.Dense(128, activation=layers.LeakyReLU(alpha=0.1),kernel_regularizer=regularizers.L2(l2=REGULARIZATION_LAMBDA), bias_regularizer=regularizers.L2(l2=REGULARIZATION_LAMBDA)),\n    layers.Dense(64, activation=layers.LeakyReLU(alpha=0.1),kernel_regularizer=regularizers.L2(l2=REGULARIZATION_LAMBDA), bias_regularizer=regularizers.L2(l2=REGULARIZATION_LAMBDA)),\n    layers.Dense(32, activation=layers.LeakyReLU(alpha=0.1),kernel_regularizer=regularizers.L2(l2=REGULARIZATION_LAMBDA), bias_regularizer=regularizers.L2(l2=REGULARIZATION_LAMBDA)),\n    layers.Dense(32, activation=layers.LeakyReLU(alpha=0.1),kernel_regularizer=regularizers.L2(l2=REGULARIZATION_LAMBDA), bias_regularizer=regularizers.L2(l2=REGULARIZATION_LAMBDA)),\n    layers.Dense(32, activation=layers.LeakyReLU(alpha=0.1),kernel_regularizer=regularizers.L2(l2=REGULARIZATION_LAMBDA), bias_regularizer=regularizers.L2(l2=REGULARIZATION_LAMBDA)),\n    layers.Dense(32, activation=layers.LeakyReLU(alpha=0.1),kernel_regularizer=regularizers.L2(l2=REGULARIZATION_LAMBDA), bias_regularizer=regularizers.L2(l2=REGULARIZATION_LAMBDA)),\n    layers.Dense(32, activation=layers.LeakyReLU(alpha=0.1),kernel_regularizer=regularizers.L2(l2=REGULARIZATION_LAMBDA), bias_regularizer=regularizers.L2(l2=REGULARIZATION_LAMBDA)),\n    layers.Dense(10, activation=\"softmax\",kernel_regularizer=regularizers.L2(l2=REGULARIZATION_LAMBDA), bias_regularizer=regularizers.L2(l2=REGULARIZATION_LAMBDA))\n]\n)\n\nmodel.compile( \n    optimizer = optimizers.Adam(learning_rate=LEARNING_RATE),\n    loss= losses.SparseCategoricalCrossentropy(),\n    metrics= [metrics.SparseCategoricalAccuracy()] \n)\n\nmodel.fit(x_train, y_train, batch_size = BATCH_SIZE, epochs=N_EPOCHS, validation_split=VALIDATION_SPLIT)\n\n","c2a378ec":"# LEARNING_RATE = tf.keras.optimizers.schedules.ExponentialDecay(\n#     initial_learning_rate=0.001, decay_steps=1250*10, decay_rate=0.8, staircase=False, name=None\n# )\n# REGULARIZATION_LAMBDA = 0.000000001\n# BATCH_SIZE = 64\n# N_EPOCHS = 1000\n# VALIDATION_SPLIT= 0.2\n\n# #3072 is the input size iirc\n# model = keras.Sequential(\n# [  \n#     layers.Dense(512, activation=layers.LeakyReLU(alpha=0.1),kernel_regularizer=regularizers.L2(l2=REGULARIZATION_LAMBDA), bias_regularizer=regularizers.L2(l2=REGULARIZATION_LAMBDA)),\n#     layers.Dense(64, activation=layers.LeakyReLU(alpha=0.1),kernel_regularizer=regularizers.L2(l2=REGULARIZATION_LAMBDA), bias_regularizer=regularizers.L2(l2=REGULARIZATION_LAMBDA)),\n#     layers.Dense(32, activation=layers.LeakyReLU(alpha=0.1),kernel_regularizer=regularizers.L2(l2=REGULARIZATION_LAMBDA), bias_regularizer=regularizers.L2(l2=REGULARIZATION_LAMBDA)),\n#     layers.Dense(32, activation=layers.LeakyReLU(alpha=0.1),kernel_regularizer=regularizers.L2(l2=REGULARIZATION_LAMBDA), bias_regularizer=regularizers.L2(l2=REGULARIZATION_LAMBDA)),\n#     layers.Dense(32, activation=layers.LeakyReLU(alpha=0.1),kernel_regularizer=regularizers.L2(l2=REGULARIZATION_LAMBDA), bias_regularizer=regularizers.L2(l2=REGULARIZATION_LAMBDA)),\n#     layers.Dense(32, activation=layers.LeakyReLU(alpha=0.1),kernel_regularizer=regularizers.L2(l2=REGULARIZATION_LAMBDA), bias_regularizer=regularizers.L2(l2=REGULARIZATION_LAMBDA)),\n#     layers.Dense(10, activation=\"softmax\",kernel_regularizer=regularizers.L2(l2=REGULARIZATION_LAMBDA), bias_regularizer=regularizers.L2(l2=REGULARIZATION_LAMBDA))\n# ]\n# )\n\n# model.compile( \n#     optimizer = optimizers.Adam(learning_rate=LEARNING_RATE),\n#     loss= losses.SparseCategoricalCrossentropy(),\n#     metrics= [metrics.SparseCategoricalAccuracy()] \n# )\n\n# model.fit(x_train, y_train, batch_size = BATCH_SIZE, epochs=N_EPOCHS, validation_split=VALIDATION_SPLIT)\n\n","7d702de7":"# LEARNING_RATE = tf.keras.optimizers.schedules.ExponentialDecay(\n#     initial_learning_rate=0.0001, decay_steps=1250*10, decay_rate=0.3, staircase=False, name=None\n# )\n# REGULARIZATION_LAMBDA = 0.000001\n# BATCH_SIZE = 32\n# N_EPOCHS = 1000\n# VALIDATION_SPLIT= 0.2\n\n# #3072 is the input size iirc\n# model = keras.Sequential(\n# [\n#     layers.Dense(64, activation=layers.LeakyReLU(alpha=0.1),kernel_regularizer=regularizers.L2(l2=REGULARIZATION_LAMBDA), bias_regularizer=regularizers.L2(l2=REGULARIZATION_LAMBDA)),\n#     layers.Dense(64, activation=layers.LeakyReLU(alpha=0.1),kernel_regularizer=regularizers.L2(l2=REGULARIZATION_LAMBDA), bias_regularizer=regularizers.L2(l2=REGULARIZATION_LAMBDA)),\n#     layers.Dense(64, activation=layers.LeakyReLU(alpha=0.1),kernel_regularizer=regularizers.L2(l2=REGULARIZATION_LAMBDA), bias_regularizer=regularizers.L2(l2=REGULARIZATION_LAMBDA)),\n#     layers.Dense(64, activation=layers.LeakyReLU(alpha=0.1),kernel_regularizer=regularizers.L2(l2=REGULARIZATION_LAMBDA), bias_regularizer=regularizers.L2(l2=REGULARIZATION_LAMBDA)),\n#     layers.Dense(10, activation=\"softmax\",kernel_regularizer=regularizers.L2(l2=REGULARIZATION_LAMBDA), bias_regularizer=regularizers.L2(l2=REGULARIZATION_LAMBDA))\n# ]\n# )\n\n# model.compile( \n#     optimizer = optimizers.Adam(learning_rate=LEARNING_RATE),\n#     loss= losses.SparseCategoricalCrossentropy(),\n#     metrics= [metrics.SparseCategoricalAccuracy()] \n# )\n\n# model.fit(x_train, y_train, batch_size = BATCH_SIZE, epochs=N_EPOCHS, validation_split=VALIDATION_SPLIT)\n","a8103801":"# LEARNING_RATE = tf.keras.optimizers.schedules.ExponentialDecay(\n#     initial_learning_rate=0.0001, decay_steps=1250*10, decay_rate=0.3, staircase=False, name=None\n# )\n# REGULARIZATION_LAMBDA = 0.000001\n# BATCH_SIZE = 32\n# N_EPOCHS = 1000\n# VALIDATION_SPLIT= 0.2\n\n# #3072 is the input size iirc\n# model = keras.Sequential(\n# [\n#     layers.Dense(64, activation=layers.LeakyReLU(alpha=0.1),kernel_regularizer=regularizers.L2(l2=REGULARIZATION_LAMBDA), bias_regularizer=regularizers.L2(l2=REGULARIZATION_LAMBDA)),\n#     layers.Dense(64, activation=layers.LeakyReLU(alpha=0.1),kernel_regularizer=regularizers.L2(l2=REGULARIZATION_LAMBDA), bias_regularizer=regularizers.L2(l2=REGULARIZATION_LAMBDA)),\n#     layers.Dense(64, activation=layers.LeakyReLU(alpha=0.1),kernel_regularizer=regularizers.L2(l2=REGULARIZATION_LAMBDA), bias_regularizer=regularizers.L2(l2=REGULARIZATION_LAMBDA)),\n#     layers.Dense(10, activation=\"softmax\",kernel_regularizer=regularizers.L2(l2=REGULARIZATION_LAMBDA), bias_regularizer=regularizers.L2(l2=REGULARIZATION_LAMBDA))\n# ]\n# )\n\n# model.compile( \n#     optimizer = optimizers.Adam(learning_rate=LEARNING_RATE),\n#     loss= losses.SparseCategoricalCrossentropy(),\n#     metrics= [metrics.SparseCategoricalAccuracy()] \n# )\n\n# model.fit(x_train, y_train, batch_size = BATCH_SIZE, epochs=N_EPOCHS, validation_split=VALIDATION_SPLIT)\n\n\n# '''\n# Epoch 60\/1000\n# 1250\/1250-3s 3ms\/step - loss: 1.4689 - sparse_categorical_accuracy: 0.4873 - val_loss: 1.6387 - val_sparse_categorical_accuracy: 0.4379\n# '''","19485df3":"# LEARNING_RATE = tf.keras.optimizers.schedules.ExponentialDecay(\n#     initial_learning_rate=0.0001, decay_steps=1250*10, decay_rate=0.3, staircase=False, name=None\n# )\n# REGULARIZATION_LAMBDA = 0.000001\n# BATCH_SIZE = 32\n# N_EPOCHS = 1000\n# VALIDATION_SPLIT= 0.2\n\n# #3072 is the input size iirc\n# model = keras.Sequential(\n# [\n#     layers.Dense(128, activation=layers.LeakyReLU(alpha=0.1),kernel_regularizer=regularizers.L2(l2=REGULARIZATION_LAMBDA), bias_regularizer=regularizers.L2(l2=REGULARIZATION_LAMBDA)),\n#     layers.Dense(64, activation=layers.LeakyReLU(alpha=0.1),kernel_regularizer=regularizers.L2(l2=REGULARIZATION_LAMBDA), bias_regularizer=regularizers.L2(l2=REGULARIZATION_LAMBDA)),\n#     layers.Dense(10, activation=\"softmax\",kernel_regularizer=regularizers.L2(l2=REGULARIZATION_LAMBDA), bias_regularizer=regularizers.L2(l2=REGULARIZATION_LAMBDA))\n# ]\n# )\n\n# model.compile( \n#     optimizer = optimizers.Adam(learning_rate=LEARNING_RATE),\n#     loss= losses.SparseCategoricalCrossentropy(),\n#     metrics= [metrics.SparseCategoricalAccuracy()] \n# )\n\n# model.fit(x_train, y_train, batch_size = BATCH_SIZE, epochs=N_EPOCHS, validation_split=VALIDATION_SPLIT)\n\n\n# ''' \n# Wide variant. Next will be deep variant.\n# Epoch 62\/1000\n# 1250\/1250 - 3s 2ms\/step - loss: 1.5173 - sparse_categorical_accuracy: 0.4787 - val_loss: 1.7193 - val_sparse_categorical_accuracy: 0.4251\n# '''","b7856f67":"# LEARNING_RATE = tf.keras.optimizers.schedules.ExponentialDecay(\n#     initial_learning_rate=0.0001, decay_steps=1250*10, decay_rate=0.3, staircase=False, name=None\n# )\n# REGULARIZATION_LAMBDA = 0.000001\n# BATCH_SIZE = 32\n# N_EPOCHS = 1000\n# VALIDATION_SPLIT= 0.2\n\n# #3072 is the input size iirc\n# model = keras.Sequential(\n# [\n#     layers.Dense(64, activation=layers.LeakyReLU(alpha=0.1),kernel_regularizer=regularizers.L2(l2=REGULARIZATION_LAMBDA), bias_regularizer=regularizers.L2(l2=REGULARIZATION_LAMBDA)),\n#     layers.Dense(64, activation=layers.LeakyReLU(alpha=0.1),kernel_regularizer=regularizers.L2(l2=REGULARIZATION_LAMBDA), bias_regularizer=regularizers.L2(l2=REGULARIZATION_LAMBDA)),\n#     layers.Dense(10, activation=\"softmax\",kernel_regularizer=regularizers.L2(l2=REGULARIZATION_LAMBDA), bias_regularizer=regularizers.L2(l2=REGULARIZATION_LAMBDA))\n# ]\n# )\n\n# model.compile( \n#     optimizer = optimizers.Adam(learning_rate=LEARNING_RATE),\n#     loss= losses.SparseCategoricalCrossentropy(),\n#     metrics= [metrics.SparseCategoricalAccuracy()] \n# )\n\n# model.fit(x_train, y_train, batch_size = BATCH_SIZE, epochs=N_EPOCHS, validation_split=VALIDATION_SPLIT)\n\n\n# ''' \n# A big jump from a small step. Let's see next if making it w i d e r helps or adding another layer helps...\n# Epoch 70\/1000\n# 1250\/1250 - 3s 2ms\/step - loss: 1.6546 - sparse_categorical_accuracy: 0.4340 - val_loss: 1.8683 - val_sparse_categorical_accuracy: 0.3767\n# '''","e920eeff":"# LEARNING_RATE = tf.keras.optimizers.schedules.ExponentialDecay(\n#     initial_learning_rate=0.0001, decay_steps=1250*10, decay_rate=0.1, staircase=False, name=None\n# )\n# REGULARIZATION_LAMBDA = 0.000001\n# BATCH_SIZE = 32\n# N_EPOCHS = 1000\n# VALIDATION_SPLIT= 0.2\n\n# #3072 is the input size iirc\n# model = keras.Sequential(\n# [  \n#     layers.Dense(64, activation=layers.LeakyReLU(alpha=0.1),kernel_regularizer=regularizers.L2(l2=REGULARIZATION_LAMBDA), bias_regularizer=regularizers.L2(l2=REGULARIZATION_LAMBDA)),\n#     layers.Dense(10, activation=\"softmax\",kernel_regularizer=regularizers.L2(l2=REGULARIZATION_LAMBDA), bias_regularizer=regularizers.L2(l2=REGULARIZATION_LAMBDA))\n# ]\n# )\n\n# model.compile( \n#     optimizer = optimizers.Adam(learning_rate=LEARNING_RATE),\n#     loss= losses.SparseCategoricalCrossentropy(),\n#     metrics= [metrics.SparseCategoricalAccuracy()] \n# )\n\n# model.fit(x_train, y_train, batch_size = BATCH_SIZE, epochs=N_EPOCHS, validation_split=VALIDATION_SPLIT)\n\n\n# ''' \n# MUCH more stable learning with the LR scheduler. Now I can put high initial LRs without fear.\n# Epoch 47\/1000\n# 1250\/1250 3s 2ms\/step - loss: 2.2177 - sparse_categorical_accuracy: 0.3490 - val_loss: 2.5022 - val_sparse_categorical_accuracy: 0.3052\n# '''","df7f8a02":"# LEARNING_RATE = 0.000025\n# REGULARIZATION_LAMBDA = 0.000001\n# BATCH_SIZE = 32\n# N_EPOCHS = 1000\n# VALIDATION_SPLIT= 0.2\n\n# #3072 is the input size iirc\n# model = keras.Sequential(\n# [  \n#     layers.Dense(64, activation=layers.LeakyReLU(alpha=0.1),kernel_regularizer=regularizers.L2(l2=REGULARIZATION_LAMBDA), bias_regularizer=regularizers.L2(l2=REGULARIZATION_LAMBDA)),\n#     layers.Dense(10, activation=\"softmax\",kernel_regularizer=regularizers.L2(l2=REGULARIZATION_LAMBDA), bias_regularizer=regularizers.L2(l2=REGULARIZATION_LAMBDA))\n# ]\n# )\n\n# model.compile( \n#     optimizer = optimizers.Adam(learning_rate=LEARNING_RATE),\n#     loss= losses.SparseCategoricalCrossentropy(),\n#     metrics= [metrics.SparseCategoricalAccuracy()] \n# )\n\n# model.fit(x_train, y_train, batch_size = BATCH_SIZE, epochs=N_EPOCHS, validation_split=VALIDATION_SPLIT)\n\n\n# ''' \n# Interesting. Let's bring that sweet sweet decayed learning rate into the picture.\n# '''","d421d502":"# LEARNING_RATE = 0.0001\n# REGULARIZATION_LAMBDA = 0.000001\n# BATCH_SIZE = 64\n# N_EPOCHS = 1000\n# VALIDATION_SPLIT= 0.2\n\n# #3072 is the input size iirc\n# model = keras.Sequential(\n# [  \n#     layers.Dense(128, activation=layers.LeakyReLU(alpha=0.1),kernel_regularizer=regularizers.L2(l2=REGULARIZATION_LAMBDA), bias_regularizer=regularizers.L2(l2=REGULARIZATION_LAMBDA)),\n#     layers.Dense(10, activation=\"softmax\",kernel_regularizer=regularizers.L2(l2=REGULARIZATION_LAMBDA), bias_regularizer=regularizers.L2(l2=REGULARIZATION_LAMBDA))\n# ]\n# )\n\n# model.compile( \n#     optimizer = optimizers.Adam(learning_rate=LEARNING_RATE),\n#     loss= losses.SparseCategoricalCrossentropy(),\n#     metrics= [metrics.SparseCategoricalAccuracy()] \n# )\n\n# model.fit(x_train, y_train, batch_size = BATCH_SIZE, epochs=N_EPOCHS, validation_split=VALIDATION_SPLIT)\n\n\n# ''' \n# Results: \n\n# Still overfitting, but we can build on this a bit I think.\n\n# Epoch 110\/1000\n# 625\/625 1s 2ms-loss: 2.2424- sparse_categorical_accuracy: 0.4169 - val_loss: 2.6510 - val_sparse_categorical_accuracy: 0.3413\n# '''","75b1f837":"# LEARNING_RATE = 0.000001\n# REGULARIZATION_LAMBDA = 0.1\n# BATCH_SIZE = 64\n# N_EPOCHS = 1000\n# VALIDATION_SPLIT= 0.2\n\n# #3072 is the input size iirc\n# model = keras.Sequential(\n# [  \n#     layers.Dense(1024, activation=\"relu\",kernel_regularizer=regularizers.L2(l2=REGULARIZATION_LAMBDA), bias_regularizer=regularizers.L2(l2=REGULARIZATION_LAMBDA)),\n#     layers.Dense(10, activation=\"softmax\",kernel_regularizer=regularizers.L2(l2=REGULARIZATION_LAMBDA), bias_regularizer=regularizers.L2(l2=REGULARIZATION_LAMBDA))\n# ]\n# )\n\n# model.compile( \n#     optimizer = optimizers.Adam(learning_rate=LEARNING_RATE),\n#     loss= losses.SparseCategoricalCrossentropy(),\n#     metrics= [metrics.SparseCategoricalAccuracy()] \n# )\n\n# model.fit(x_train, y_train, batch_size = BATCH_SIZE, epochs=N_EPOCHS, validation_split=VALIDATION_SPLIT)\n\n\n# ''' \n# Results: \n\n# Massive overfitting. Trying to reduce model size first.\n\n# Epoch 83\/1000\n# 625\/625  2s 2ms\/step loss: 1.9465- sparse_categorical_accuracy: 0.7212 - val_loss: 8.7231 - val_sparse_categorical_accuracy: 0.4185\n# '''","6d3204ad":"LEARNING_RATE = 0.00001\nREGULARIZATION_LAMBDA = 0.000001\nBATCH_SIZE = 64\nN_EPOCHS = 1000\nVALIDATION_SPLIT= 0.2\n\n#3072 is the input size iirc\nmodel = keras.Sequential(\n[  \n    layers.Dense(1024, activation=\"relu\",kernel_regularizer=regularizers.L2(l2=REGULARIZATION_LAMBDA), bias_regularizer=regularizers.L2(l2=REGULARIZATION_LAMBDA)),\n    layers.Dense(10, activation=\"softmax\",kernel_regularizer=regularizers.L2(l2=REGULARIZATION_LAMBDA), bias_regularizer=regularizers.L2(l2=REGULARIZATION_LAMBDA))\n]\n)\n\nmodel.compile( \n    optimizer = optimizers.Adam(learning_rate=LEARNING_RATE),\n    loss= losses.SparseCategoricalCrossentropy(),\n    metrics= [metrics.SparseCategoricalAccuracy()] \n)\n\nmodel.fit(x_train, y_train, batch_size = BATCH_SIZE, epochs=N_EPOCHS, validation_split=VALIDATION_SPLIT)\n\n\n''' \nResults: \n\nMassive overfitting. Trying to reduce model size first.\n\nEpoch 83\/1000\n625\/625  2s 2ms\/step loss: 1.9465- sparse_categorical_accuracy: 0.7212 - val_loss: 8.7231 - val_sparse_categorical_accuracy: 0.4185\n'''","0672df0a":"# LEARNING_RATE = 0.000001\n# REGULARIZATION_LAMBDA = 0.000001\n# BATCH_SIZE = 128\n# N_EPOCHS = 1000\n# VALIDATION_SPLIT= 0.2\n\n# #3072 is the input size iirc\n# model = keras.Sequential(\n# [  \n#     layers.Dense(10, activation=\"softmax\",kernel_regularizer=regularizers.L2(l2=REGULARIZATION_LAMBDA), bias_regularizer=regularizers.L2(l2=REGULARIZATION_LAMBDA))\n# ]\n# )\n\n# model.compile( \n#     optimizer = optimizers.Adam(learning_rate=LEARNING_RATE),\n#     loss= losses.SparseCategoricalCrossentropy(),\n#     metrics= [metrics.SparseCategoricalAccuracy()] \n# )\n\n# model.fit(x_train, y_train, batch_size = BATCH_SIZE, epochs=N_EPOCHS, validation_split=VALIDATION_SPLIT)\n\n\n# ''' \n# Results: \n\n# Epoch 161\/1000\n# 313\/313 - 1s 3ms\/step-loss:33.0944-sparse_categorical_accuracy:0.2487-val_loss:33.9132-val_sparse_categorical_accuracy:0.2364\n# '''","698ede0f":"# LEARNING_RATE = 0.00001\n# REGULARIZATION_LAMBDA = 0.000001\n# BATCH_SIZE = 128\n# N_EPOCHS = 1000\n# VALIDATION_SPLIT= 0.2\n\n# #3072 is the input size iirc\n# model = keras.Sequential(\n# [  \n#     layers.Dense(1024, activation=\"relu\",kernel_regularizer=regularizers.L2(l2=REGULARIZATION_LAMBDA), bias_regularizer=regularizers.L2(l2=REGULARIZATION_LAMBDA), kernel_initializer='he_normal', bias_initializer='he_normal'),\n#     layers.Dense(512, activation=\"relu\",kernel_regularizer=regularizers.L2(l2=REGULARIZATION_LAMBDA), bias_regularizer=regularizers.L2(l2=REGULARIZATION_LAMBDA), kernel_initializer='he_normal', bias_initializer='he_normal'),\n#     layers.Dense(128, activation=\"relu\",kernel_regularizer=regularizers.L2(l2=REGULARIZATION_LAMBDA), bias_regularizer=regularizers.L2(l2=REGULARIZATION_LAMBDA), kernel_initializer='he_normal', bias_initializer='he_normal'),\n#     layers.Dense(10, activation=\"softmax\",kernel_regularizer=regularizers.L2(l2=REGULARIZATION_LAMBDA), bias_regularizer=regularizers.L2(l2=REGULARIZATION_LAMBDA), kernel_initializer='he_normal', bias_initializer='he_normal')\n# ]\n# )\n\n# model.compile( \n#     optimizer = optimizers.Adam(learning_rate=LEARNING_RATE),\n#     loss= losses.SparseCategoricalCrossentropy(),\n#     metrics= [metrics.SparseCategoricalAccuracy()] \n# )\n\n# model.fit(x_train, y_train, batch_size = BATCH_SIZE, epochs=N_EPOCHS, validation_split=VALIDATION_SPLIT)","a6777bce":"References: \n\nhttps:\/\/www.tensorflow.org\/guide\/keras\/sequential_model \n\nhttps:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/Model \n\nhttps:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/optimizers\/Adam  \n\nhttps:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/optimizers \n\nhttps:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/layers\/Dense\n\nhttps:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/losses\/CategoricalCrossentropyhttps:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/losses\/CategoricalCrossentropy \n\nhttps:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/regularizers\/Regularizerhttps:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/regularizers\/Regularizer\n","f503a50d":"## Regularization\n\nOkay, now we have obtained a way of measuring overfitting, but what can we actually do about it? \n\nRegularization is one of the ways (though data augmentation is another). \n\nTwo common approaches to regularization for NNs: \n\n1. L2 regularization: We add a small penalty to the loss function. This penalty is a constant times the square of the L2 norm of the weights.\n\n[insert link for L2 reg]\n\nBasically, large weights typically mean a very complicated decision function. To simplify the decision function, we add a penalty. The size of this penalty is the constant LAMBDA (yes, the same lambda as Lagrange multipliers. Dig in that direction!)\n\n2. Dropout regularization: \n   [To be discussed next time] -- try to read up on this!\n","70c1ec2e":"https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/datasets\/cifar10\/load_data","d05f17e6":"## Imports","043c261d":"## The CIFAR-10 dataset \n\nhttps:\/\/www.cs.toronto.edu\/~kriz\/cifar.html \n\nhttps:\/\/en.wikipedia.org\/wiki\/CIFAR-10 \n\nCIFAR-10 is an image dataset consisting of around 60,000 images in 10 classes. Take a look at the first link to get an idea of what the data represents.\n\nIt is significantly harder to train on than MNIST (http:\/\/yann.lecun.com\/exdb\/mnist\/ , https:\/\/en.wikipedia.org\/wiki\/MNIST_database ), despite the simplicity of its problem statement. \n\nOverfitting is a real risk with this dataset and the model size required to get even half-decent results is significant.","5484b6d0":"# Introduction to Tensorflow using the CIFAR-10 Dataset\n\nA notebook by Aniruddh K B | iMT2018008, IIITB","c9f5152d":"## Models diary","6be4e0ff":"## The philosophy behind separate train, dev and test sets \n\nThe whole point of a supervised learning model is that you want it to work in a very general-purpose setting.\n\nSpecifically, you want the model to work on **data that it has not seen before**.\n\nHowever, with large models, there is a very high chance that, if you don't take adequate precautions, the model will start **overfitting** to the training data.\n\nSimply put, instead of actually learning, the model memorizes the training data.\n\nThis is characterized by the model performing well on the training data but performing really poorly on the test data.\n\nThumb rule: During training, if the training accuracy (or other performance metric) continues to rise, but the test accuracy plateaus or starts to fall, that means that we're encountering overfitting.\n\nThe most basic practice is to have separate training and testing datasets so that you can easily catch overfitting. There is a whole rabbit hole here which includes KFold cross-validation, \"stratified\" as one of the variants (most useful for multiclass settings and especially so for imbalanced data, where one class is overrepresented).\n\n\nHowever, even here, there is a risk that one may end up choosing those hyperparameters that result in a good \"test\" accuracy, but don't perform that well in the real world. \n\n\nOne of the proposed solutions for this is (apart from K-Fold CV) is to use three separate datasets: train, dev and test . The workflow is: \n\n1. Train using the training dataset. \n2. Validate against the dev dataset to find the best model.\n3. As a final sanity check, evaluate the model's performance on the test dataset. Significant difference between the dev and the test dataset results should raise a red flag. "}}