{"cell_type":{"d6b2aeb6":"code","c7f0ca8a":"code","b118a1c9":"code","e7da1155":"code","3c488f87":"code","2f330bf1":"code","270ba33a":"code","a9e3be75":"code","c70303bd":"code","4966033a":"code","6b709fdf":"code","c7dd0b5b":"code","a6041ca7":"code","8024275d":"code","7bb2f3a8":"code","67ac889b":"code","9f86656c":"code","87760be2":"code","fb645469":"code","6bb928d1":"code","c74d8cf8":"code","52941e7d":"code","33630899":"code","b02c9af8":"code","9c265744":"code","b9197acc":"code","e7d3cae1":"code","6fbebe60":"markdown","7658fe62":"markdown","fddaee10":"markdown","25011f3d":"markdown","dd3b82e4":"markdown","e9988ed2":"markdown","34867d0f":"markdown","4493f7d1":"markdown","6a0380fc":"markdown","a4d5f1c4":"markdown","702299ab":"markdown"},"source":{"d6b2aeb6":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","c7f0ca8a":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom xgboost import XGBClassifier\n\n#from sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix, precision_score, recall_score","b118a1c9":"df = pd.read_csv('..\/input\/sloan-digital-sky-survey\/Skyserver_SQL2_27_2018 6_51_39 PM.csv')\ndf.head(10)","e7da1155":"df.shape","3c488f87":"df.keys()","2f330bf1":"df.isnull().sum()","270ba33a":"df.describe()","a9e3be75":"print(df['class'].unique())\nsns.countplot(df['class'])","c70303bd":"df['run'].unique()","4966033a":"df['objid'].unique()","6b709fdf":"for i in df.keys():\n    print(\"Colname:=\",i)\n    print(df[i].unique())","c7dd0b5b":"df.corr()","a6041ca7":"dictionary={'STAR':1,'GALAXY':2,'QSO':3}\ndf.replace({'class':dictionary},inplace=True)\n\n# you can use LabelEncoder here also","8024275d":"df.head()","7bb2f3a8":"y = df['class']","67ac889b":"df = df.drop(['class','rerun'], axis=1)","9f86656c":"df.drop('objid',axis=1)","87760be2":"df.head(5)","fb645469":"from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\nsdss = scaler.fit_transform(df)\n","6bb928d1":"X_train, X_test, y_train, y_test = train_test_split(sdss, y, test_size=0.2, random_state=2)","c74d8cf8":"print(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","52941e7d":"# Logistic Regression\nlr = LogisticRegression(C=2, max_iter=1500)\nlr.fit(X_train, y_train)\ny_pred = lr.predict(X_test)\n\nprint(\"Accuracy of Logistic Regression= \", accuracy_score(y_test,y_pred))","33630899":"# KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=3)\nknn.fit(X_train, y_train)\ny_pred = knn.predict(X_test)\nprint(\"Accuracy of KNeighborsClassifier = \", accuracy_score(y_test,y_pred))","b02c9af8":"# RandomForestClassifier\nrf = RandomForestClassifier(max_depth=18, n_estimators=120)\nrf.fit(X_train, y_train)\ny_pred = rf.predict(X_test)\nprint(\"Accuracy of RandomForestClassifier = \", accuracy_score(y_test,y_pred))","9c265744":"# XGBClassifier\nxgb= XGBClassifier()\nxgb.fit(X_train,y_train)\ny_pred =xgb.predict(X_test)\nprint(\"Accuracy of XGBClassifier = \", accuracy_score(y_test,y_pred))","b9197acc":"import lightgbm as lgb\n\nlgb=lgb.LGBMClassifier()\nlgb.fit(X_train,y_train)\ny_pred =lgb.predict(X_test)\nprint(\"Accuracy of lightgbm = \", accuracy_score(y_test,y_pred))","e7d3cae1":"# GradientBoostingClassifier\ngb = GradientBoostingClassifier()\ngb.fit(X_train,y_train)\ny_pred =gb.predict(X_test)\nprint(\"Accuracy of GradientBoostingClassifier = \", accuracy_score(y_test,y_pred))","6fbebe60":"Importing Libraries","7658fe62":"Understanding Dataset","fddaee10":"Dropping the columns which we donot need.","25011f3d":"Apparently we don't need 'rerun', 'objid' columns","dd3b82e4":"Here we create a dictionary which will map Class to numeric form and then replace it.\nStar will become 1 and so on.","e9988ed2":"Using minmax scaling, we reduce the scale size to betwee 0 and 1. This helps the dataset to be more accuracte and removes high range anamolies. Apparently this must be used while dealing with large range of datasets.","34867d0f":"Understanding unique values if present in columns, which will help to reduce dimensionality in future processing.","4493f7d1":"Plotting the class column","6a0380fc":"Finding the correaltion between attributes","a4d5f1c4":"**The data consists of 10,000 observations of space taken by the SDSS. Every observation is described by 17 feature columns and 1 class column which identifies it to be either a star, galaxy or quasar.\nTask to identify Star, Galaxy or Quasar.**","702299ab":"Also dropping 'objid'"}}