{"cell_type":{"79816699":"code","34ab52f9":"code","e55c8ae0":"code","d710a908":"code","b381e000":"code","f5e024ba":"code","5abf0757":"code","1a761218":"code","c0d73eff":"code","025dbddf":"code","fa79ffc8":"code","291f1a7d":"code","83415d5d":"code","93bf2fda":"code","a99b991b":"code","cd4cbd1b":"code","7bff70de":"code","a75f9186":"code","527a47f5":"code","f252b7b2":"code","992a9509":"code","d746b890":"code","1d3fc2b3":"code","eb11c7f9":"code","b45676b5":"code","46e8b724":"code","dda4489e":"code","301e521f":"markdown","143c6057":"markdown","4e71704d":"markdown","45291c03":"markdown","7d80cdd2":"markdown","2f986e72":"markdown","d6028ade":"markdown","769bbc9d":"markdown","92f41de9":"markdown","8b4b6002":"markdown","12c25e1c":"markdown","5cab8636":"markdown","6bb91ca5":"markdown","c5f2835f":"markdown","2cc8f983":"markdown","501c1858":"markdown","2ca98824":"markdown","3291a2c0":"markdown","f86e477f":"markdown","3e4146c0":"markdown","e31ebdd5":"markdown","cbeccdea":"markdown","ab3f727e":"markdown","b4cc1895":"markdown","ef922eb7":"markdown","921dc4e9":"markdown","6f99b3f5":"markdown","dba7593b":"markdown","d735a7cf":"markdown","5f7a874f":"markdown","d0c0481d":"markdown","353a2ff4":"markdown"},"source":{"79816699":"from IPython.display import IFrame\nIFrame('https:\/\/view.officeapps.live.com\/op\/view.aspx?src=https:\/\/developer.download.nvidia.com\/training\/courses\/C-AC-02-V1\/AC_CUDA_Python_1.pptx', 640, 390)","34ab52f9":"from numba import cuda\n\n# Note the use of an `out` array. CUDA kernels written with `@cuda.jit` do not return values,\n# just like their C counterparts. Also, no explicit type signature is required with @cuda.jit\n@cuda.jit\ndef add_kernel(x, y, out):\n    \n    # The actual values of the following CUDA-provided variables for thread and block indices,\n    # like function parameters, are not known until the kernel is launched.\n    \n    # This calculation gives a unique thread index within the entire grid (see the slides above for more)\n    idx = cuda.grid(1)          # 1 = one dimensional thread grid, returns a single value.\n                                # This Numba-provided convenience function is equivalent to\n                                # `cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x`\n\n    # This thread will do the work on the data element with the same index as its own\n    # unique index within the grid.\n    out[idx] = x[idx] + y[idx]","e55c8ae0":"import numpy as np\n\nn = 4096\nx = np.arange(n).astype(np.int32) # [0...4095] on the host\ny = np.ones_like(x)               # [1...1] on the host\n\nd_x = cuda.to_device(x) # Copy of x on the device\nd_y = cuda.to_device(y) # Copy of y on the device\nd_out = cuda.device_array_like(d_x) # Like np.array_like, but for device arrays\n\n# Because of how we wrote the kernel above, we need to have a 1 thread to one data element mapping,\n# therefore we define the number of threads in the grid (128*32) to equal n (4096).\nthreads_per_block = 128\nblocks_per_grid = 32","d710a908":"add_kernel[blocks_per_grid, threads_per_block](d_x, d_y, d_out)\ncuda.synchronize()\nprint(d_out.copy_to_host()) # Should be [1...4096]","b381e000":"# Refactor to be a CUDA kernel doing one thread's work.\n# Don't forget that when using `@cuda.jit`, you must provide an output array as no value will be returned.\n@cuda.jit\ndef square_device(a,out):\n    idx = cuda.grid(1)\n    out[idx] = a[idx]**2","f5e024ba":"# Leave the values in this cell fixed for this exercise\nn = 4096\n\na = np.arange(n)\nout = a**2           # `out` will only be used for testing below","5abf0757":"d_a = cuda.to_device(a)                                   # TODO make `d_a` a device array\nd_out = cuda.device_array(shape=(n,), dtype=np.float32)    # TODO: make d_out a device array\n\n\n# TODO: Update the execution configuration for the amount of work needed\nblocks = 128\nthreads = 32\n\n# TODO: Launch as a kernel with an appropriate execution configuration\nd_out = square_device[blocks,threads](d_a,d_out)","1a761218":"from IPython.display import IFrame\nIFrame('https:\/\/view.officeapps.live.com\/op\/view.aspx?src=https:\/\/developer.download.nvidia.com\/training\/courses\/C-AC-02-V1\/AC_CUDA_Python_2.pptx', 640, 390)","c0d73eff":"from numba import cuda\n\n@cuda.jit\ndef add_kernel(x, y, out):\n    \n\n    start = cuda.grid(1)\n    \n    # This calculation gives the total number of threads in the entire grid\n    stride = cuda.gridsize(1)   # 1 = one dimensional thread grid, returns a single value.\n                                # This Numba-provided convenience function is equivalent to\n                                # `cuda.blockDim.x * cuda.gridDim.x`\n\n    # This thread will start work at the data element index equal to that of its own\n    # unique index in the grid, and then, will stride the number of threads in the grid each\n    # iteration so long as it has not stepped out of the data's bounds. In this way, each\n    # thread may work on more than one data element, and together, all threads will work on\n    # every data element.\n    for i in range(start, x.shape[0], stride):\n        # Assuming x and y inputs are same length\n        out[i] = x[i] + y[i]","025dbddf":"import numpy as np\n\nn = 100000 # This is far more elements than threads in our grid\nx = np.arange(n).astype(np.int32)\ny = np.ones_like(x)\n\nd_x = cuda.to_device(x)\nd_y = cuda.to_device(y)\nd_out = cuda.device_array_like(d_x)\n\nthreads_per_block = 128\nblocks_per_grid = 30","fa79ffc8":"add_kernel[blocks_per_grid, threads_per_block](d_x, d_y, d_out)\nprint(d_out.copy_to_host()) # Remember, memory copy carries implicit synchronization","291f1a7d":"from math import hypot\n\n@cuda.jit\ndef hypot_stride(a, b, c):\n    idx = cuda.grid(1)\n    stride = cuda.gridsize(1)\n    \n    for i in range(idx, a.shape[0], stride):\n        c[i] = hypot(a[i], b[i])","83415d5d":"# You do not need to modify the contents in this cell\nn = 1000000\na = np.random.uniform(-12, 12, n).astype(np.float32)\nb = np.random.uniform(-12, 12, n).astype(np.float32)\nd_a = cuda.to_device(a)\nd_b = cuda.to_device(b)\nd_c = cuda.device_array_like(d_b)\n\nblocks = 128\nthreads_per_block = 64\n\nhypot_stride[blocks, threads_per_block](d_a, d_b, d_c)","93bf2fda":"from numpy import testing\n# This assertion will fail until you successfully implement the hypot_stride kernel above\ntesting.assert_almost_equal(np.hypot(a,b), d_c.copy_to_host(), decimal=5)","a99b991b":"%timeit np.hypot(a, b)","cd4cbd1b":"from numba import jit\n\n@jit\ndef numba_hypot(a, b):\n    return np.hypot(a, b)","7bff70de":"%timeit numba_hypot(a, b)","a75f9186":"%time hypot_stride[1, 1](d_a, d_b, d_c); cuda.synchronize()","527a47f5":"%time hypot_stride[128, 64](d_a, d_b, d_c); cuda.synchronize()","f252b7b2":"@cuda.jit\ndef thread_counter_race_condition(global_counter):\n    global_counter[0] += 1  # This is bad\n    \n@cuda.jit\ndef thread_counter_safe(global_counter):\n    cuda.atomic.add(global_counter, 0, 1)  # Safely add 1 to offset 0 in global_counter array","992a9509":"# This gets the wrong answer\nglobal_counter = cuda.to_device(np.array([0], dtype=np.int32))\nthread_counter_race_condition[64, 64](global_counter)\n\nprint('Should be %d:' % (64*64), global_counter.copy_to_host())","d746b890":"# This works correctly\nglobal_counter = cuda.to_device(np.array([0], dtype=np.int32))\nthread_counter_safe[64, 64](global_counter)\n\nprint('Should be %d:' % (64*64), global_counter.copy_to_host())","1d3fc2b3":"def cpu_histogram(x, xmin, xmax, histogram_out):\n    '''Increment bin counts in histogram_out, given histogram range [xmin, xmax).'''\n    # Note that we don't have to pass in nbins explicitly, because the size of histogram_out determines it\n    nbins = histogram_out.shape[0]\n    bin_width = (xmax - xmin) \/ nbins\n    \n    # This is a very slow way to do this with NumPy, but looks similar to what you will do on the GPU\n    for element in x:\n        bin_number = np.int32((element - xmin)\/bin_width)\n        if bin_number >= 0 and bin_number < histogram_out.shape[0]:\n            # only increment if in range\n            histogram_out[bin_number] += 1","eb11c7f9":"x = np.random.normal(size=10000, loc=0, scale=1).astype(np.float32)\nxmin = np.float32(-4.0)\nxmax = np.float32(4.0)\nhistogram_out = np.zeros(shape=10, dtype=np.int32)\n\ncpu_histogram(x, xmin, xmax, histogram_out)\n\nhistogram_out","b45676b5":"@cuda.jit\ndef cuda_histogram(x, xmin, xmax, histogram_out):\n    '''Increment bin counts in histogram_out, given histogram range [xmin, xmax).'''\n    \n    nbins = histogram_out.shape[0]\n    bin_width = (xmax - xmin) \/ nbins\n    \n    start = cuda.grid(1)\n    stride=cuda.gridsize(1)\n    \n    for i in range(start, x.shape[0], stride):\n        bin_number = np.int32((x[i] - xmin)\/bin_width)\n        if bin_number >= 0 and bin_number < histogram_out.shape[0]:\n            # only increment if in range\n            cuda.atomic.add(histogram_out, bin_number, 1)\n    ","46e8b724":"d_x = cuda.to_device(x)\nd_histogram_out = cuda.to_device(np.zeros(shape=10, dtype=np.int32))\n\nblocks = 128\nthreads_per_block = 64\n\ncuda_histogram[blocks, threads_per_block](d_x, xmin, xmax, d_histogram_out)","dda4489e":"# This assertion will fail until you correctly implement `cuda_histogram`\nnp.testing.assert_array_almost_equal(d_histogram_out.copy_to_host(), histogram_out, decimal=2)","301e521f":"![image.png](attachment:b8ebee8d-292f-426b-8447-d4937e4cd3e0.png)","143c6057":"### Exercise: Tweak the Code\n\nMake the following minor changes to the code above to see how it affects its execution. Make educated guesses about what will happen before running the code:\n\n* Decrease the `threads_per_block` variable\n* Decrease the `blocks_per_grid` variable\n* Increase the `threads_per_block` and\/or `blocks_per_grid variables`\n* Remove or comment out the `cuda.synchronize()` call\n\n### Results\n\nIn the example above, because the kernel is written so that each thread works on exactly one data element, it is essential for the number of threads in the grid equal the number of data elements.\n\nBy **reducing the number of threads in the grid**, either by reducing the number of blocks, and\/or reducing the number of threads per block, there are elements where work is left undone and thus we can see in the output that the elements toward the end of the `d_out` array did not have any values added to it. If you edited the execution configuration by reducing the number of threads per block, then in fact there are other elements through the `d_out` array that were not processed.\n\n**Increasing the size of the grid** in fact creates issues with out of bounds memory access. This error will not show in your code presently, but later in this section you will learn how to expose this error using `cuda-memcheck` and debug it.\n\nYou might have expected that **removing the synchronization point** would have resulted in a print showing that no or less work had been done. This is a reasonable guess since without a synchronization point the CPU will work asynchronously while the GPU is processing. The detail to learn here is that memory copies carry implicit synchronization, making the call to `cuda.synchronize` above unnecessary.","4e71704d":"## An Aside on Hiding Latency and Execution Configuration Choices","45291c03":"## Timing the Kernel\n\nLet's take the time to do some performance timing for the `hypot_stride` kernel. ","7d80cdd2":"## Objectives\n\nBy the time you complete this section you will be able to:\n\n* Write custom CUDA kernels in Python and launch them with an execution configuration.\n* Utilize grid stride loops for working in parallel over large data sets and leveraging memory coalescing.\n* Use atomic operations to avoid race conditions when working in parallel.","2f986e72":"### Exercise: Implement a Grid Stride Loop\n\nRefactor the following CPU scalar `hypot_stride` function to run as a CUDA Kernel utilizing a grid stride loop. ","d6028ade":"### How to Run the Assessment\n\nTake the following steps to complete this assessment:\n\n1. Using the instructions that follow, work on the cells below as you usually would for an exercise.\n","769bbc9d":"# 2- Custom CUDA Kernels in Python with Numba\n\nIn this section we will go further into our understanding of how the CUDA programming model organizes parallel work, and will leverage this understanding to write custom CUDA **kernels**, functions which run in parallel on CUDA GPUs. Custom CUDA kernels, in utilizing the CUDA programming model, require more work to implement than, for example, simply decorating a ufunc with `@vectorize`. However, they make possible parallel computing in places where ufuncs are just not able, and provide a flexibility that can lead to the highest level of performance.\n\nThis section contains three appendices for those of you interested in futher study: a variety of debugging techniques to assist your GPU programming, links to CUDA programming references, and coverage of Numba supported random number generation on the GPU.","92f41de9":"**This is a 3 Notebook Series on:**\n# Fundamental of Accelerated Computing with CUDA Python\nIn this 3 Notebook Series you will learn about:\n1. [Introduction to CUDA Python with Numba](https:\/\/www.kaggle.com\/harshwalia\/1-introduction-to-cuda-python-with-numba)\n2. [Custom CUDA Kernels in Python with Numba](https:\/\/www.kaggle.com\/harshwalia\/2-custom-cuda-kernels-in-python-with-numba)\n3. [Multidimensional Grids and Shared Memory for CUDA Python with Numba](https:\/\/www.kaggle.com\/harshwalia\/3-multidimensional-grids-shared-memory-for-cuda)","8b4b6002":"CUDA enabled NVIDIA GPUs consist of several [**Streaming Multiprocessors**](https:\/\/docs.nvidia.com\/cuda\/cuda-c-programming-guide\/index.html#hardware-implementation), or **SMs** on a die, with attached DRAM. SMs contain all required resources for the execution of kernel code including many CUDA cores. When a kernel is launched, each block is assigned to a single SM, with potentially many blocks assigned to a single SM. SMs partition blocks into further subdivisions of 32 threads called **warps** and it is these warps which are given parallel instructions to execute.\n\nWhen an instruction takes more than one clock cycle to complete (or in CUDA parlance, to **expire**) the SM can continue to do meaningful work *if it has additional warps that are ready to be issued new instructions.* Because of very large register files on the SMs, there is no time penalty for an SM to change context between issuing instructions to one warp or another. In short, the latency of operations can be hidden by SMs with other meaningful work so long as there is other work to be done.\n\n**Therefore, of primary importance to utilizing the full potential of the GPU, and thereby writing performant accelerated applications, it is essential to give SMs the ability to hide latency by providing them with a sufficient number of warps which can be accomplished most simply by executing kernels with sufficiently large grid and block dimensions.**\n\nDeciding the very best size for the CUDA thread grid is a complex problem, and depends on both the algorithm and the specific GPU's [compute capability](https:\/\/docs.nvidia.com\/cuda\/cuda-c-programming-guide\/index.html#compute-capabilities), but here are some very rough heuristics that we tend to follow and which can work well for getting started:\n\n  * The size of a block should be a multiple of 32 threads (the size of a warp), with typical block sizes between 128 and 512 threads per block.\n  * The size of the grid should ensure the full GPU is utilized where possible. Launching a grid where the number of blocks is 2x-4x the number of SMs on the GPU is a good starting place. Something in the range of 20 - 100 blocks is usually a good starting point.\n  * The CUDA kernel launch overhead does increase with the number of blocks, so when the input size is very large we find it best not to launch a grid where the number of threads equals the number of input elements, which would result in a tremendous number of blocks. Instead we use a pattern to which we will now turn our attention for dealing with large inputs.","12c25e1c":"That's much faster!","5cab8636":"### Single Threaded on the Device\n\nJust to see, let's launch our kernel in a grid with only a single thread. Here we will use `%time`, which only runs the statement once to ensure our measurement isn't affected by the finite depth of the CUDA kernel queue. We will also add a `cuda.synchronize` to be sure we don't get any innacurate times on account of returning control to the CPU, where the timer is, before the kernel completes:","6bb91ca5":"### Exercise: Accelerate a CPU Function as a Custom CUDA Kernel\n\nBelow is CPU scalar function `square_device` that could be used as a CPU ufunc. Your job is to refactor it to run as a CUDA kernel decorated with the `@cuda.jit` decorator.\n\nYou might think that making this function run on the device could be much more easily done with `@vectorize`, and you would be correct. But this scenario will give you a chance to work with all the syntax we've introduced before moving on to more complicated and realistic examples.\n\nIn this exercise you will need to:\n* Refactor the `square_device` definition to be a CUDA kernel that will do one thread's worth of work on a single element.\n* Refactor the `d_a` and `d_out` arrays below to be CUDA device arrays.\n* Modify the `blocks` and `threads` variables to appropriate values for the provided `n`.\n* Refactor the call to `square_device` to be a kernel launch that includes an execution configuration.\n\nThe assertion test below will fail until you successfully implement the above. If you get stuck, feel free to check out a [solution](..\/edit\/solutions\/square_device_solution.py).","c5f2835f":"### Parallel on the Device","2cc8f983":"Using a grid stride loop and atomic operations, implement your solution in the cell below. ","501c1858":"Ufuncs are fantastically elegant, and for any scalar operation that ought to be performed element wise on data, ufuncs are likely the right tool for the job.\n\nAs you are well aware, there are many, if not more, classes of problems that cannot be solved by applying the same function to each element of a data set. Consider, for example, any problem that requires access to more than one element of a data structure in order to calculate its output, like stencil algorithms, or any problem that cannot be expressed by a one input value to one output value mapping, such as a reduction. Many of these problems are still inherently parallelizable, but cannot be expressed by a ufunc.\n\nWriting custom CUDA kernels, while more challenging than writing GPU accelerated ufuncs, provides developers with tremendous flexibility for the types of functions they can send to run in parallel on the GPU. Furthermore, as you will begin learning in this and the next section, it also provides fine-grained control over *how* the parallelism is conducted by exposing CUDA's thread hierarchy to developers explicitly.\n\nWhile remaining purely in Python, the way we write CUDA kernels using Numba is very reminiscent of how developers write them in CUDA C\/C++. For those of you familiar with programming in CUDA C\/C++, you will likely pick up custom kernels in Python with Numba very rapidly, and for those of you learning them for the first time, know that the work you do here will also serve you well should you ever need or wish to develop CUDA in C\/C++, or even, make a study of the wealth of CUDA resources on the web that are most commonly portraying CUDA C\/C++ code.","2ca98824":"## A First CUDA Kernel\n\nLet's start with a concrete, and very simple example by rewriting our addition function for 1D NumPy arrays. CUDA kernels are compiled using the `numba.cuda.jit` decorator. `numba.cuda.jit` is not to be confused with the `numba.jit` decorator you've already learned which optimizes functions **for the CPU**.\n\nWe will begin with a very simple example to highlight some of the essential syntax. Worth mentioning is that this particular function could in fact be written as a ufunc, but we choose it here to keep the focus on learning the syntax. We will be proceeding to functions more well suited to being written as a custom kernel below. Be sure to read the comments carefully, as they provide some important information about the code.","3291a2c0":"## Summary\n\nIn this section you learned how to:\n\n* Write custom CUDA kernels in Python and launch them with an execution configuration.\n* Utilize grid stride loops for working in parallel over large data sets and leveraging memory coalescing.\n* Use atomic operations to avoid race conditions when working in parallel.","f86e477f":"## Atomic Operations and Avoiding Race Conditions\n\nCUDA, like many general purpose parallel execution frameworks, makes it possible to have race conditions in your code.  A race condition in CUDA arises when threads read to or write from a memory location that might be modified by another independent thread. Generally speaking, you need to worry about:\n\n * read-after-write hazards: One thread is reading a memory location at the same time another thread might be writing to it.\n * write-after-write hazards: Two threads are writing to the same memory location, and only one write will be visible when the kernel is complete.\n \nA common strategy to avoid both of these hazards is to organize your CUDA kernel algorithm such that each thread has exclusive responsibility for unique subsets of output array elements, and\/or to never use the same array for both input and output in a single kernel call. (Iterative algorithms can use a double-buffering strategy if needed, and switch input and output arrays on each iteration.)\n\nHowever, there are many cases where different threads need to combine results. Consider something very simple, like: \"every thread increments a global counter.\" Implementing this in your kernel requires each thread to:\n\n1. Read the current value of a global counter.\n2. Compute `counter + 1`.\n3. Write that value back to global memory.\n\nHowever, there is no guarantee that another thread has not changed the global counter between steps 1 and 3. To resolve this problem, CUDA provides **atomic operations** which will read, modify and update a memory location in one, indivisible step. Numba supports several of these functions, [described here](http:\/\/numba.pydata.org\/numba-doc\/dev\/cuda\/intrinsics.html#supported-atomic-operations).\n\nLet's make our thread counter kernel:","3e4146c0":"## The Need for Custom Kernels","e31ebdd5":"## Working on Largest Datasets with Grid Stride Loops\n\nThe following slides give a high level overview of a technique called a **grid stride loop** which will create flexible kernels where each thread is able to work on more than one data element, an essential technique for large datasets. Execute the cell to load the slides.","cbeccdea":"### Numba on the CPU\n\nNext let's see about a CPU optimized version:","ab3f727e":"### Write an Accelerated Histogramming Kernel\n\nFor this assessment, you will create an accelerated histogramming kernel. This will take an array of input data, a range, and a number of bins, and count how many of the input data elements land in each bin. Below is a working CPU implementation of histogramming to serve as an example for your work:","b4cc1895":"Hopefully not too much of a surprise that this is way slower than even the baseline CPU execution.","ef922eb7":"## A First Grid Stride Loop\n\nLet's refactor the `add_kernel` above to utilize a grid stride loop so that we can launch it to work on larger data sets flexibly while incurring the benefits of global **memory coalescing**, which allows parallel threads to access memory in contiguous chunks, a scenario which the GPU can leverage to reduce the total number of memory operations:","921dc4e9":"**Please Upvote** If you gain Some knowledge","6f99b3f5":"If you Gain some knowledge then **Please Upvote**\n\nThanks to NVIDIA Deeplearning Institute for such a Good Content.","dba7593b":"`Well i have solved the assignment for you`","d735a7cf":"### CPU Baseline\n\nFirst let's get a baseline with `np.hypot`:","5f7a874f":"## Assessment\n\nThe following exercise will require you to utilize everything you've learned so far. \n\n**Please read the directions carefully before beginning your work to complete  the assessment.**","d0c0481d":"## Introduction to CUDA Kernels\n\nWhen programming in CUDA, developers write functions for the GPU called **kernels**, which are executed, or in CUDA parlance, **launched**, on the GPU's many cores in parallel **threads**. When kernels are launched, programmers use a special syntax, called an **execution configuration** (also called a launch configuration) to describe the parallel execution's configuration.\n\nThe following slides (which will appear after executing the cell below) give a high level introduction to how CUDA kernels can be created to work on large datasets in parallel on the GPU device. Work through the slides and then you will begin writing and executing your own custom CUDA kernels, using the ideas presented in the slides.","353a2ff4":"Read slides before working on Grid Stride Loops"}}