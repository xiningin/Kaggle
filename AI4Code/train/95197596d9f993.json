{"cell_type":{"23217016":"code","be94d1a3":"code","301a89a2":"code","995eeb8b":"code","3bd82a9b":"code","3857a938":"code","0d55451b":"code","0c634e44":"code","3d6d8bb0":"code","15e032a4":"code","b240ab3a":"code","a423d312":"code","e97ad78b":"code","7bf22b4d":"code","2c599781":"code","43bd30e7":"code","4ec83bba":"code","b49f612f":"code","1be2d8c7":"code","0e5db881":"code","f635195b":"code","f1758d1c":"code","10cbe0b2":"code","5ecacca8":"code","c06996a0":"code","d5c5ec05":"code","3ac35c59":"code","e65cfdfc":"code","0ee2216d":"code","48719ce4":"code","4f72d337":"code","f4108d9b":"code","506c5dce":"markdown","23748704":"markdown","b8f892cd":"markdown","417ae3ab":"markdown","8e542c2e":"markdown","6e2264e5":"markdown","8233a4ad":"markdown","245897dc":"markdown","40b24e76":"markdown","b90f3518":"markdown","0e64b986":"markdown","9991ec8b":"markdown","aec10a72":"markdown","d44a7e50":"markdown","ee0639ac":"markdown","9b3d2656":"markdown","7b3f207f":"markdown","1f76ed24":"markdown","ebcc8b8c":"markdown","55bb4a47":"markdown"},"source":{"23217016":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport seaborn as sns # visualizations\n\nimport matplotlib.pyplot as plt # visualizations\n\nfrom scipy import stats # stats.mode and stats.norm\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","be94d1a3":"# df is the dataframe object from pandas\ndf = pd.read_csv(\"..\/input\/USvideos.csv\")\n\n# display the first 10 rows of df\ndf.head(10)","301a89a2":"# just practicing here ...\ndate_pattern = r\"[0-9]+\\.[0-9]+\\.[0-9]+\"\ndate_pattern_2017 = r\"17\\.[0-9]+\\.[0-9]+\"\ndate_pattern_november = r\"[0-9]+\\.[0-9]+\\.11\"\n\ndf_with_no_tags = df[df.tags == \"[none]\"]\ndf_only_2017_trends = df[df.trending_date.str.contains(date_pattern_2017)]\ndf_only_november_trends = df[df.trending_date.str.contains(date_pattern_2017)]\n\nnew_filter = (df.trending_date.str.contains(date_pattern))\nnew_df = df[new_filter]\n\ndf.info()\nprint(\"-\"*50)\ndf_with_no_tags.info()\nprint(\"-\"*50)\ndf_only_2017_trends.info()\nprint(\"-\"*50)\nnew_df.info()\nprint(\"-\"*50)","995eeb8b":"# regex for each month\ndate_pattern_jan = r\"[0-9]+\\.[0-9]+\\.01\" \ndate_pattern_feb = r\"[0-9]+\\.[0-9]+\\.02\" \ndate_pattern_mar = r\"[0-9]+\\.[0-9]+\\.03\" \ndate_pattern_apr = r\"[0-9]+\\.[0-9]+\\.04\" \ndate_pattern_may = r\"[0-9]+\\.[0-9]+\\.05\" \ndate_pattern_jun = r\"[0-9]+\\.[0-9]+\\.06\" \ndate_pattern_jul = r\"[0-9]+\\.[0-9]+\\.07\" \ndate_pattern_aug = r\"[0-9]+\\.[0-9]+\\.08\" \ndate_pattern_sep = r\"[0-9]+\\.[0-9]+\\.09\" \ndate_pattern_oct = r\"[0-9]+\\.[0-9]+\\.10\" \ndate_pattern_nov = r\"[0-9]+\\.[0-9]+\\.11\" \ndate_pattern_dec = r\"[0-9]+\\.[0-9]+\\.12\"","3bd82a9b":"# filters for each month (returns true if contains pattern ... else false)\njan_filter = (df.trending_date.str.contains(date_pattern_jan))\nfeb_filter = (df.trending_date.str.contains(date_pattern_feb))\nmar_filter = (df.trending_date.str.contains(date_pattern_mar))\napr_filter = (df.trending_date.str.contains(date_pattern_apr))\nmay_filter = (df.trending_date.str.contains(date_pattern_may))\njun_filter = (df.trending_date.str.contains(date_pattern_jun))\njul_filter = (df.trending_date.str.contains(date_pattern_jul))\naug_filter = (df.trending_date.str.contains(date_pattern_aug))\nsep_filter = (df.trending_date.str.contains(date_pattern_sep))\noct_filter = (df.trending_date.str.contains(date_pattern_oct))\nnov_filter = (df.trending_date.str.contains(date_pattern_nov))\ndec_filter = (df.trending_date.str.contains(date_pattern_dec))","3857a938":"# make new dataframe, append 'month' column\njan_df = df[jan_filter]\njan_df['month'] = \"jan\"\nfeb_df = df[feb_filter]\nfeb_df['month'] = \"feb\"\nmar_df = df[mar_filter]\nmar_df['month'] = \"mar\"\napr_df = df[apr_filter]\napr_df['month'] = \"apr\"\nmay_df = df[may_filter]\nmay_df['month'] = \"may\"\njun_df = df[jun_filter]\njun_df['month'] = \"jun\"\njul_df = df[jul_filter]\njul_df['month'] = \"jul\"\naug_df = df[aug_filter]\naug_df['month'] = \"aug\"\nsep_df = df[sep_filter]\nsep_df['month'] = \"sep\"\noct_df = df[oct_filter]\noct_df['month'] = \"oct\"\nnov_df = df[nov_filter]\nnov_df['month'] = \"nov\"\ndec_df = df[dec_filter]\ndec_df['month'] = \"dec\"","0d55451b":"# get info about each month\nprint(\"-\"*50+\"jan\"+\"-\"*50)\njan_df.info()\nprint(\"-\"*50+\"feb\"+\"-\"*50)\nfeb_df.info()\nprint(\"-\"*50+\"mar\"+\"-\"*50)\nmar_df.info()\nprint(\"-\"*50+\"apr\"+\"-\"*50)\napr_df.info()\nprint(\"-\"*50+\"may\"+\"-\"*50)\nmay_df.info()\nprint(\"-\"*50+\"jun\"+\"-\"*50)\njun_df.info()\nprint(\"-\"*50+\"jul\"+\"-\"*50)\njul_df.info()\nprint(\"-\"*50+\"aug\"+\"-\"*50)\naug_df.info()\nprint(\"-\"*50+\"sep\"+\"-\"*50)\nsep_df.info()\nprint(\"-\"*50+\"oct\"+\"-\"*50)\noct_df.info()\nprint(\"-\"*50+\"nov\"+\"-\"*50)\nnov_df.info()\nprint(\"-\"*50+\"dec\"+\"-\"*50)\ndec_df.info()","0c634e44":"# look at the first 5 rows of jan_df\njan_df.head()","3d6d8bb0":"# recombine all the month data\nmonth_array = [jan_df, feb_df, mar_df, apr_df, may_df, jun_df, jul_df, aug_df, sep_df, oct_df, nov_df, dec_df]\nmonth_df = pd.concat(month_array)\n\nsns.boxplot(x='month', y='views', data=month_df)","15e032a4":"# get info about jul...oct\nprint(\"-\"*50+\"jul\"+\"-\"*50)\njul_df.info()\nprint(\"-\"*50+\"aug\"+\"-\"*50)\naug_df.info()\nprint(\"-\"*50+\"sep\"+\"-\"*50)\nsep_df.info()\nprint(\"-\"*50+\"oct\"+\"-\"*50)\noct_df.info()\n","b240ab3a":"# look at the frist 5 rows of a df with just july for trending_date\ndf[(df.trending_date.str.contains(date_pattern_jul))].head()","a423d312":"# ignore regex warnings from pandas ... https:\/\/stackoverflow.com\/q\/39901550\/5411712\nimport warnings\nwarnings.filterwarnings(\"ignore\", 'This pattern has match groups')\n\nweirdValuesRegex = r\"[0-9]+\\.[0-9]+\\.([1][3-9]+|[2][0-9]|[3][0-9]|[4][0-9]|[5][0-9]|[6][0-9]|[7][0-9]|[8][0-9]|[9][0-9]|[^0][a-zA-Z]|[a-zA-Z][a-zA-Z])\"\nweirdValuesFilter = (df.trending_date.str.contains( weirdValuesRegex ))\ndf[weirdValuesFilter].head()","e97ad78b":"# filter for november in publish_time\nnew_nov_regex = r\"^[0-9]+\\-11+\\-[0-9]+T\"\nnew_nov_filter = (df.publish_time.str.contains( new_nov_regex ))\ndf[new_nov_filter].head()","7bf22b4d":"# filter for july in publish_time\nnew_jul_regex = r\"^[0-9]+\\-07+\\-[0-9]+T\"\nnew_jul_filter = (df.publish_time.str.contains( new_jul_regex ))\ndf[new_jul_filter].head()","2c599781":"# regex for each publish_time month\ndate_pattern_jan = r\"^[0-9]+\\-01+\\-[0-9]+T\"\ndate_pattern_feb = r\"^[0-9]+\\-02+\\-[0-9]+T\"\ndate_pattern_mar = r\"^[0-9]+\\-03+\\-[0-9]+T\"\ndate_pattern_apr = r\"^[0-9]+\\-04+\\-[0-9]+T\"\ndate_pattern_may = r\"^[0-9]+\\-05+\\-[0-9]+T\"\ndate_pattern_jun = r\"^[0-9]+\\-06+\\-[0-9]+T\"\ndate_pattern_jul = r\"^[0-9]+\\-07+\\-[0-9]+T\"\ndate_pattern_aug = r\"^[0-9]+\\-08+\\-[0-9]+T\"\ndate_pattern_sep = r\"^[0-9]+\\-09+\\-[0-9]+T\"\ndate_pattern_oct = r\"^[0-9]+\\-10+\\-[0-9]+T\"\ndate_pattern_nov = r\"^[0-9]+\\-11+\\-[0-9]+T\"\ndate_pattern_dec = r\"^[0-9]+\\-12+\\-[0-9]+T\"\n\n# filters for each publish_time month (returns true if contains pattern ... else false)\njan_filter = (df.publish_time.str.contains(date_pattern_jan))\nfeb_filter = (df.publish_time.str.contains(date_pattern_feb))\nmar_filter = (df.publish_time.str.contains(date_pattern_mar))\napr_filter = (df.publish_time.str.contains(date_pattern_apr))\nmay_filter = (df.publish_time.str.contains(date_pattern_may))\njun_filter = (df.publish_time.str.contains(date_pattern_jun))\njul_filter = (df.publish_time.str.contains(date_pattern_jul))\naug_filter = (df.publish_time.str.contains(date_pattern_aug))\nsep_filter = (df.publish_time.str.contains(date_pattern_sep))\noct_filter = (df.publish_time.str.contains(date_pattern_oct))\nnov_filter = (df.publish_time.str.contains(date_pattern_nov))\ndec_filter = (df.publish_time.str.contains(date_pattern_dec))\n\n# make new dataframe (overwrite old variables), append 'month' column\njan_df = df[jan_filter]\njan_df['month'] = \"jan\"\nfeb_df = df[feb_filter]\nfeb_df['month'] = \"feb\"\nmar_df = df[mar_filter]\nmar_df['month'] = \"mar\"\napr_df = df[apr_filter]\napr_df['month'] = \"apr\"\nmay_df = df[may_filter]\nmay_df['month'] = \"may\"\njun_df = df[jun_filter]\njun_df['month'] = \"jun\"\njul_df = df[jul_filter]\njul_df['month'] = \"jul\"\naug_df = df[aug_filter]\naug_df['month'] = \"aug\"\nsep_df = df[sep_filter]\nsep_df['month'] = \"sep\"\noct_df = df[oct_filter]\noct_df['month'] = \"oct\"\nnov_df = df[nov_filter]\nnov_df['month'] = \"nov\"\ndec_df = df[dec_filter]\ndec_df['month'] = \"dec\"","43bd30e7":"# recombine all the month data\nmonth_array = [jan_df, feb_df, mar_df, apr_df, may_df, jun_df, jul_df, aug_df, sep_df, oct_df, nov_df, dec_df]\nmonth_df = pd.concat(month_array)\n\nsns.boxplot(x='month', y='views', data=month_df)","4ec83bba":"# get info about jul...oct\nprint(\"-\"*50+\"jul\"+\"-\"*50)\njul_df.info()\nprint(\"-\"*50+\"aug\"+\"-\"*50)\naug_df.info()\nprint(\"-\"*50+\"sep\"+\"-\"*50)\nsep_df.info()\nprint(\"-\"*50+\"oct\"+\"-\"*50)\noct_df.info()\n","b49f612f":"jul_df.head(20) # there's only 10 videos in the month of july??? weird...","1be2d8c7":"print(str(df.shape) + \" \\n\\tis the output of `df.shape` \\n\\twhich is a \" + str(type(df.shape)) + \" \\n\\tthat tells us there are \" + str(df.shape[0]) + \" rows and \" + str(df.shape[1]) + \" columns\")\nprint(\"\\n\"+\"-\"*70+\"\\n\")\nprint(str(df.size) + \" \\n\\tis the output of `df.size` \\n\\twhich is a \" + str(type(df.size)) + \"\\n\\tthat tells us there are \" + str(df.size) + \" total elements, \\n\\taka rows*cols = \"+str(df.shape[0]) + \"*\" + str(df.shape[1]) + \" = \" +str(df.shape[0]*df.shape[1]))\nprint(\"\\n\"+\"-\"*70+\"\\n\")\nprint(str(len(df)) + \" \\n\\tis the output of `len(df)` \\n\\twhich is a \" + str(type(len(df))) + \"\\n\\tthat tells us there are \" + str(df.size) + \" rows\")\nprint(\"\\n\"+\"-\"*70+\"\\n\")\nprint(str(df.columns) + \" \\n\\n\\tis the output of `df.columns` \\n\\twhich is a \" + str(type(df.columns)) + \" \\n\\tthat shows us the column names in our dataframe... why is it not just an array? probs for extra fancy functions\")","0e5db881":"print(\"\\n\"+\"-\"*70+\"\\n\")\n\nprint(\"here are the first 5 tags in df via the command `df['tags'][:5]`\")\nprint(df['tags'][:5])\n\nprint(\"-\"*70)\nprint(\"here are the first 5 views in df via the command `df['views'][:5]`\")\nprint(df['views'][:5])\n\nprint(\"-\"*70)\nprint(\"here are the second index for tags column in df via the command `df['tags'][:1]`\\n\")\n\nprint(df['tags'][1])","f635195b":"print(\"\\n\"+\"=\"*70+\"\\n\")\n\nprint(\"here are some terms\")\nprint(\"df = dataframe ... it's basically a fancy Pandas object to represent a matrix. Why didn't they just call it a matrix? idk man.\")\nprint(\"df['views'] gives us a single column. That's known as a 'series' ... just a 1D array (or 'list' in python)\")\nprint(\"df['views'][1] gives us a single 'value' from the views array.\")\n\nprint(\"\\n\"+\"=\"*70+\"\\n\")\nprint(\"here are the first 5 views and tags in df via the command `df[['views', 'tags']][:5]`\")\nprint(df[['views', 'tags']][:5])\n\nprint(\"\\n\"+\"=\"*70+\"\\n\")\nprint(\"this is a good way to just get the 'features' you care about to optimize performance by having less data to move around.\")","f1758d1c":"print(\"\\n\"+\"=\"*70+\"\\n\")\nprint(\"here is how to sort df by views and just see the top 5 videos with LEAST views\")\nprint(df.sort_values(['views'])[['views', 'title']][:5])\n\nprint(\"\\n\"+\"=\"*70+\"\\n\")\n\nprint(\"here is how to sort df by views and just see the top 5 videos with MOST views\")\nprint(df.sort_values(['views'], ascending=False)[['views', 'title']][:5])\n","10cbe0b2":"viewsMean = np.mean(df['views'])\nviewsMedian = np.median(df['views'])\nviewsMode = stats.mode(df['views'])\nviewsStd = df['views'].std() # standard deviation\nviewsVar = df['views'].var() # variance","5ecacca8":"print(\"stats for `views`\"\n      + \"\\n\\tmean \\t=\\t\" + str(viewsMean)\n      + \"\\n\\tmedian \\t=\\t\" + str(viewsMedian)\n      + \"\\n\\tmode \\t=\\t\" + str(viewsMode) + \"\\n\\t\\t\\t\\t... meaning \" + str(viewsMode[0]) + \" shows up \" + str(viewsMode[1]) + \" times\"\n      + \"\\n\\tstd \\t=\\t\" + str(viewsStd)\n      + \"\\n\\tvar \\t=\\t\" + str(viewsVar)\n     )\n\n# histogram for views with 50 bins\nplt.hist(df['views'], 50)\nplt.show()","c06996a0":"# https:\/\/matplotlib.org\/users\/pyplot_tutorial.html\nplt.plot([1,2,3,4], [1,4,9,16], 'ro')\nplt.axis([0, 6, 0, 20]) # [xmin, xmax, ymin, ymax]\nplt.ylabel('y-axis')\nplt.xlabel('x-axis')\nplt.title('title')\nplt.grid(True)\nplt.text(1, 5, \"some text $\\mu=number,\\ \\sigma=number$\")\nplt.annotate('WOW! a point!', xy=(4, 16), xytext=(1, 17.5),\n            arrowprops=dict(facecolor='black', shrink=0.05),\n            )\nplt.show()","d5c5ec05":"# xkcd style mode ;)\nplt.xkcd() # https:\/\/matplotlib.org\/xkcd\/examples\/showcase\/xkcd.html\n# https:\/\/matplotlib.org\/users\/pyplot_tutorial.html\nplt.plot([1,2,3,4], [1,4,9,16], 'ro')\nplt.axis([0, 6, 0, 20]) # [xmin, xmax, ymin, ymax]\nplt.ylabel('y-axis')\nplt.xlabel('x-axis')\nplt.title('title')\nplt.grid(True)\nplt.text(1, 5, \"some text $\\mu=number,\\ \\sigma=number$\")\nplt.annotate('WOW! a point!', xy=(4, 16), xytext=(1, 17.5),\n            arrowprops=dict(facecolor='black', shrink=0.05),\n            )\nplt.show()\n# turn off xkcd mode\nplt.rcdefaults()","3ac35c59":"plt.plot(df['views'], stats.norm.pdf(df['views']), \"bo\")","e65cfdfc":"plt.plot(df['views'], stats.expon.pdf(df['views']), 'bo')","0ee2216d":"# shamelessly copying code...\n\nplt.xkcd() # just for fun\n\ndf_yout = df\n\ndf_yout['likes_log'] = np.log(df_yout['likes'] + 1)\ndf_yout['views_log'] = np.log(df_yout['views'] + 1)\ndf_yout['dislikes_log'] = np.log(df_yout['dislikes'] + 1)\ndf_yout['comment_log'] = np.log(df_yout['comment_count'] + 1)\n\nplt.figure(figsize = (12,6))\n\nplt.subplot(221)\ng1 = sns.distplot(df_yout['views_log'])\ng1.set_title(\"VIEWS LOG DISTRIBUITION\", fontsize=16)\n\nplt.subplot(222)\ng4 = sns.distplot(df_yout['comment_log'])\ng4.set_title(\"COMMENTS LOG DISTRIBUITION\", fontsize=16)\n\nplt.subplot(223)\ng3 = sns.distplot(df_yout['dislikes_log'], color='r')\ng3.set_title(\"DISLIKES LOG DISTRIBUITION\", fontsize=16)\n\nplt.subplot(224)\ng2 = sns.distplot(df_yout['likes_log'],color='green')\ng2.set_title('LIKES LOG DISTRIBUITION', fontsize=16)\n\nplt.subplots_adjust(wspace=0.2, hspace=0.6)\n\nplt.show()\n\nplt.rcdefaults() # no more fun\n","48719ce4":"# https:\/\/machinelearningmastery.com\/prepare-text-data-machine-learning-scikit-learn\/\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n# list of text documents\ntext = [\"The quick brown fox jumped over the lazy dog.\",\n        \"The dog.\",\n        \"The fox\"]\n# create the transform\nvectorizer = TfidfVectorizer()\n# tokenize and build vocab\nvectorizer.fit(text)\n# summarize\nprint(\"vectorizer.vocabulary_: \\n\\t\" + str(vectorizer.vocabulary_))\n# TODO: figure out what IDF means\n## IDF = Inverse Document Frequency: This downscales words that appear a lot across documents.\n## https:\/\/nlp.stanford.edu\/IR-book\/html\/htmledition\/inverse-document-frequency-1.html\nprint(\"vectorizer.idf_: \\n\\t\" + str(vectorizer.idf_))\n# encode document\nvector = vectorizer.transform([text[0]])\n# summarize encoded vector\nprint(\"vector.shape: \\n\\t\" + str(vector.shape))\nprint(\"vector.toarray(): \\n\\t\" + str(vector.toarray()))","4f72d337":"# try the same stuff as above... but use the YouTube `title` data\n# list of text documents\ntext = df['title']","f4108d9b":"# create the transform\nvectorizer = TfidfVectorizer()\n# tokenize and build vocab\nvectorizer.fit(text)\n# summarize\nprint(\"vectorizer.vocabulary_: \\n\\t\" + str(vectorizer.vocabulary_))\nprint(\"vectorizer.idf_: \\n\\t\" + str(vectorizer.idf_))\n# encode document\nvector = vectorizer.transform([text[0]])\n# summarize encoded vector\nprint(\"vector.shape: \\n\\t\" + str(vector.shape))\nprint(\"vector.toarray(): \\n\\t\" + str(vector.toarray()))","506c5dce":"# Ok, so that's November... how about July?","23748704":"# To be honest, I expected to get a stacktrace there. Wow! Cool! We can make some more progress! \n**Click** \"Output\" to see the vectorized data from the above code, but its a lot of stuff...","b8f892cd":"# Let's try `publish_time` instead? First for a month that for sure exists in `trending_date`, November. ","417ae3ab":"# This is more difficult than I thought it would be\n\n## It's definitely a log distribution. [see this Kernel](https:\/\/www.kaggle.com\/kabure\/extensive-us-youtube-eda). I wish I saw that Kernel earlier. Thanks [@Kabure](https:\/\/www.kaggle.com\/kabure)","8e542c2e":"# Okay, so now let's just set `text = df['title']` and hope it works even though `df['title']` is a  `pandas.core.series.Series` type rather than an ~~array~~ list\ud83e\udd1e","6e2264e5":"# Here's a boxplot of all the months","8233a4ad":"# Okay, let's try everything with `publish_time` ...","245897dc":"# What!? Why is July...October missing data?","40b24e76":"# Part 2: https:\/\/www.kaggle.com\/mfekadu\/youtube-kaggle-part-2\nbecause this one quite long, but I'd like to keep the record of what I learned. \n","b90f3518":"# try to filter for weird data inside `trending_date`","0e64b986":"# Looks like there's data inside those months, but much less than the other months....","9991ec8b":"## Wow, `views` is *very* **skewed right**","aec10a72":"# In this notebook, I am \n1. Learning to work with data science \/ ML libraries in python\n2. Learning to do data science \/ ML in general\n3. Hoping to gain insight into YouTube's trending videos\n\nI avoided editing this notebook to look \"professional\" and rather aimed to accurately display my thoughts and discovery process. *That results in a messy notebook, but that's okay when one is still learning* \u263a\ufe0f\n\n# Observations\n* I am looking at United States data: `..\/input\/USvideos.csv`\n* First `video_id` in the dataset has an ALL_CAPS title \"WE WANT TO TALK ABOUT OUR MARRIAGE\"... \n    * [nice clickbait, Casey Neistat](https:\/\/www.youtube.com\/watch?v=2kyS6SvSYSE)\n\n# Questions \/ Hypotheses\n\n1. Is there a correlation between ALL_CAPS `titles` and popularity (likelihood of trending)?\n* Could a \"clickbait-title-generator\" be made with deep learning?\n* \ud83c\udf1f Is there a correlation between the **month** of `trending_date` and the `views`?\n    * Which month would have the most views?\n        * Perhaps *November \/ December* because of the holidays in the United States, and therefore more people spending time indoors with internet videos playing?\n            * Would the data for other countries have similar correlations for their respective holiday seasons? \n            * Or would the correlation be the same because CA, DE, FR, GB, US, all celebrate Christmas and have New Years at the same time? \n            * Perhaps a country that does not follow the Gregorian calendar (*e.g: Ethiopia*) would have a similar but different pattern in `trending_date`... but that would require collecting more data... ugh\n        * Perhaps *Summer time* because of younger viewers who might be out of school?","d44a7e50":"# hmm... July through October still look weird","ee0639ac":"# Here's a boxplot of all the months","9b3d2656":"# Now let's filter the data set by month and append a new feature \"month\" ","7b3f207f":"# Quickly learning *Pandas*","1f76ed24":"# Exploring Stats for  `views`   column","ebcc8b8c":"## hmm... maybe an exponential PDF is better for `views` ?","55bb4a47":"# FOUND JULY! OMG FINALLY!\n\n### So that suggests that over the months July...October, a video could be published and not be trending in the same month. \n### OR, there were some problems during data collection.\n### Either way, July...October do exist! Yay!"}}