{"cell_type":{"7ac87d55":"code","21c89448":"code","587c94c4":"code","b37d1e43":"code","9048a00f":"code","b0c4f606":"code","7fc94d4e":"code","d0add502":"code","08539cb4":"code","33e83f7a":"code","086e7274":"code","8008209f":"code","f6d9142a":"code","30f4811a":"code","ceec7272":"code","e29ef6a2":"code","0f6430b4":"code","66ace85d":"code","af1d0ac0":"code","342a9ae5":"code","33510c27":"code","0c2f05b8":"code","c04e7dbf":"code","7d6f7f46":"markdown","593b2c31":"markdown","fbdcb826":"markdown","ff33d674":"markdown","378e57b7":"markdown","3ba2c8c2":"markdown","2946b8e9":"markdown","e2ca9b16":"markdown","93ddf4a3":"markdown","6b80c82f":"markdown","3a2e4c69":"markdown","f3be7fa0":"markdown","4c535386":"markdown","62f2275b":"markdown","3eedb50d":"markdown","368c36d3":"markdown","d4d42fd2":"markdown","0a719d2c":"markdown","9632ba7a":"markdown","ac86e602":"markdown","745ce451":"markdown","9fc86deb":"markdown","44a35e14":"markdown","d3fdad53":"markdown","34b31461":"markdown","5dbd5f48":"markdown"},"source":{"7ac87d55":"import numpy as np\nimport pandas as pd \n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.cluster import KMeans, AgglomerativeClustering\nfrom sklearn.mixture import GaussianMixture \n\nfrom sklearn.decomposition import PCA\n\nfrom scipy.spatial.distance import cdist\nfrom matplotlib.patches import Ellipse\n\nsns.set()","21c89448":"df = pd.read_csv('\/kaggle\/input\/ccdata\/CC GENERAL.csv')\ndf.head(3)","587c94c4":"df.set_index('CUST_ID', inplace=True)\ndf.head(3)","b37d1e43":"nan_sample = df.isnull().sum().sort_values(ascending=False)\nnan_sample = nan_sample[nan_sample > 0]\nnan_sample","9048a00f":"for i in nan_sample.index:\n    df.loc[df[i].isnull(), i] = df[i].mean()","b0c4f606":"df.isnull().sum().sort_values(ascending=False)","7fc94d4e":"df.duplicated().value_counts()","d0add502":"plt.subplots(figsize=(14, 10))\ndf.boxplot()\nplt.yscale('log')\nplt.xticks(rotation=50)\nplt.show()","08539cb4":"df.describe()","33e83f7a":"from sklearn.preprocessing import MinMaxScaler, StandardScaler, MaxAbsScaler, RobustScaler","086e7274":"options = [MinMaxScaler, StandardScaler, MaxAbsScaler, RobustScaler]\n\nvar = 0.95\npca = PCA(n_components=var, svd_solver='full')\n\nfor name_scaler in options:\n    scaler = name_scaler() \n    scaled = scaler.fit_transform(df.values)\n    \n    pca_values = pca.fit_transform(scaled)\n    print(f\"{name_scaler.__name__}\\nOptimal number: {pca.n_components_}\\nVariance: {np.sum(pca.explained_variance_ratio_)}\\n\")","8008209f":"sse = lambda values: dict((k, KMeans(n_clusters=k, max_iter=10000).fit(values).inertia_) for k in range(1, 20))\n\nfig = plt.figure(figsize=(16, len(options) * 4))\n\nfor i, name_scaler in enumerate(options):\n    scaler = name_scaler() \n    scaled = scaler.fit_transform(df.values)\n    d = sse(scaled)\n    \n    fig.add_subplot(len(options) \/\/ 2 + 1, 2, i + 1)\n    \n    plt.bar(x=d.keys(), height=d.values(), width=1, edgecolor='k', facecolor='orange', label=f\"{name_scaler.__name__}\")\n    plt.plot(list(d.keys()), list(d.values()), 'ro-')\n    plt.xlabel(\"Number of cluster\")\n#     plt.ylabel(\"SSE\")\n    plt.legend()\n\nfig.suptitle(\"Sum squared error\", fontsize=14)","f6d9142a":"def graph_clusters(method_name, array2d, name_scaler, nmin=2, nmax=9, style=plt.cm.plasma):\n    \n    count_axis_x = 3\n    count_axis_y = (nmax - nmin + 1) \/\/ 3 + 1\n    \n    f = plt.figure(figsize=(count_axis_x  * 6, count_axis_y * 5))\n\n    for i in range(nmin, nmax + 1):\n        model = method_name(n_clusters=i).fit(array2d)\n        f.add_subplot(count_axis_y, count_axis_x, i - 1)\n        plt.scatter(array2d[:, 0], array2d[:, 1], s=10, cmap=style, c=model.labels_, label=\"number of\\nclusters = \" + str(i))\n        plt.legend()\n    \n    plt.suptitle(name_scaler)\n    plt.show()","30f4811a":"pca = PCA(n_components=2)\nfor name_scaler in options:\n    graph_clusters(KMeans, pca.fit_transform(name_scaler().fit_transform(df.values)), name_scaler.__name__, 2, 10, plt.cm.viridis)","ceec7272":"pca_values = pca.fit_transform(MaxAbsScaler().fit_transform(df.values))\nkmeans = KMeans(n_clusters=4, max_iter=1000).fit(pca_values)\n\nplt.subplots(figsize=(10, 8))\nsns.scatterplot(x=\"Pca1\", y=\"Pca2\", hue=\"Cluster\", \n                     data=pd.DataFrame({'Pca1': pca_values[:, 0],\n                                        'Pca2': pca_values[:, 1],\n                                        'Cluster': kmeans.labels_}), palette=plt.cm.tab20, s=100, \n                     alpha=1, edgecolor='k', linewidth=1.2)\n\ncenters = kmeans.cluster_centers_\n\nr = [cdist(pca_values[kmeans.labels_ == i], [center]).max() for i, center in enumerate(kmeans.cluster_centers_)]\n\nplt.scatter(centers[:, 0], centers[:, 1], facecolor='green', marker='H', s=120, edgecolor='k')\nfor c, rad in zip(centers, r):\n    plt.gcf().gca().add_artist(plt.Circle(c, rad, facecolor='darkgreen', lw=3, alpha=0.25, zorder=10))\n\nplt.axis('equal')\nplt.show()","e29ef6a2":"data = pd.concat([df, pd.DataFrame(kmeans.labels_, columns=['Cluster'], index=df.index)], axis=1)\ndata = data[['Cluster'] + [col for col in data.columns if col != 'Cluster']]\n\nfor c in data.columns[1:]:\n    grid = sns.FacetGrid(data, col='Cluster', height=3, aspect=1.3)\n    grid.map(plt.hist, c, bins=20, edgecolor='k')\n    grid.set_xticklabels(rotation=40)\n    \npd.DataFrame(data['Cluster'].value_counts())","0f6430b4":"for name_scaler in options:\n    graph_clusters(AgglomerativeClustering, pca.fit_transform(name_scaler().fit_transform(df.values)), name_scaler.__name__, 2, 10, plt.cm.viridis)","66ace85d":"ag = AgglomerativeClustering(n_clusters=4, \n                             affinity='euclidean', \n                             linkage='ward').fit(pca_values)\n\nplt.subplots(figsize=(10, 8))\nsns.scatterplot(x=\"Pca1\", y=\"Pca2\", hue=\"Cluster\", \n                     data=pd.DataFrame({'Pca1': pca_values[:, 0],\n                                        'Pca2': pca_values[:, 1],\n                                        'Cluster': ag.labels_}), palette=plt.cm.tab20, s=100, \n                     alpha=1, edgecolor='k', linewidth=1.2)\n\nplt.show()","af1d0ac0":"data = pd.concat([df, pd.DataFrame(ag.labels_, columns=['Cluster_ag'], index=df.index)], axis=1)\ndata = data[['Cluster_ag'] + [col for col in data.columns if col != 'Cluster_ag']]\n\nfor c in data.columns[1:]:\n    grid = sns.FacetGrid(data, col='Cluster_ag', height=3, aspect=1.3)\n    grid.map(plt.hist, c, bins=20, edgecolor='k')\n    grid.set_xticklabels(rotation=40)\n\npd.DataFrame(data['Cluster_ag'].value_counts())","342a9ae5":"for name_scaler in options:\n    f = plt.figure(figsize=(18, 15))\n    \n    for i in range(2, 11):\n        model = GaussianMixture(n_components=i).fit(pca.fit_transform(name_scaler().fit_transform(df.values)))\n        f.add_subplot(3, 3, i - 1)\n        plt.scatter(pca_values[:, 0], pca_values[:, 1], s=10, cmap=plt.cm.magma_r, c=model.predict(pca_values), \n                    label=\"number of\\nclusters = \" + str(i))\n        plt.legend()\n        plt.suptitle(name_scaler.__name__)\n\n    plt.show()","33510c27":"def draw_ellipse(position, covariance, ax=None, **kwargs):\n    \"\"\"Draw an ellipse with a given position and covariance\"\"\"\n    ax = ax or plt.gca()\n    \n    # Convert covariance to principal axes\n    if covariance.shape == (2, 2):\n        U, s, Vt = np.linalg.svd(covariance)\n        angle = np.degrees(np.arctan2(U[1, 0], U[0, 0]))\n        width, height = 2 * np.sqrt(s)\n    else:\n        angle = 0\n        width, height = 2 * np.sqrt(covariance)\n    \n    # Draw the Ellipse\n    for nsig in range(1, 4):\n        v = np.random.randint(255, size=3)\n        rgb = plt.cm.viridis.colors\n        \n        ax.add_patch(Ellipse(position, nsig * width, nsig * height,\n                             angle, facecolor=rgb[v[nsig - 1]], edgecolor='k', **kwargs))","0c2f05b8":"gmm = GaussianMixture(n_components=4, init_params='kmeans', covariance_type='full').fit(pca_values)\n\nf, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(12, 18))\n\nsns.scatterplot(x=\"Pca1\", y=\"Pca2\", hue=\"Cluster\", \n                     data=pd.DataFrame({'Pca1': pca_values[:, 0],\n                                        'Pca2': pca_values[:, 1],\n                                        'Cluster': gmm.predict(pca_values)}), palette=plt.cm.Spectral, s=100, \n                     alpha=1, edgecolor='k', linewidth=1.2, ax=ax1)\n\n\nax2.set_yticks(ax1.get_yticks())\nax2.set_ylabel(ax1.get_ylabel())\n\nfor pos, covar, w in zip(gmm.means_, gmm.covariances_, gmm.weights_):\n    draw_ellipse(pos, covar, alpha=0.2, ax=ax2)\n\n# ax1.axis('equal')\n# ax2.axis('equal')\nf.show()","c04e7dbf":"data = pd.concat([df, pd.DataFrame(ag.labels_, columns=['Cluster_gmm'], index=df.index)], axis=1)\ndata = data[['Cluster_gmm'] + [col for col in data.columns if col != 'Cluster_gmm']]\n\nfor c in data.columns[1:]:\n    grid = sns.FacetGrid(data, col='Cluster_gmm', height=3, aspect=1.3)\n    grid.map(plt.hist, c, bins=20, edgecolor='k')\n    grid.set_xticklabels(rotation=40)\n\npd.DataFrame(data['Cluster_gmm'].value_counts())","7d6f7f46":"## AgglomerativeClustering","593b2c31":"Add cluster number and grouping by features","fbdcb826":"## Libraries","ff33d674":"Vizualize KMeans, number of clusters $\\overline{2, 10}$","378e57b7":"## Reading and preprocessing","3ba2c8c2":"Vizualize AgglomerativeClustering, number of clusters $\\overline{2, 10}$ ","2946b8e9":"Add cluster number and grouping by features","e2ca9b16":"PCA $\\leftarrow$ 2 components (for vizualize)","93ddf4a3":"Outliers in data (log scale)","6b80c82f":"Draw ellipse from: https:\/\/jakevdp.github.io\/PythonDataScienceHandbook\/05.12-gaussian-mixtures.html","3a2e4c69":"Hmm, MinMaxScaler","f3be7fa0":"Vizualize, numbers of clusters = 4","4c535386":"Vizualize GaussianMixture, number of clusters $\\overline{2, 10}$ ","62f2275b":"Error","3eedb50d":"As a result, we can say that KMeans and GMM give the best results. Despite the outliers, RobustScaler is not the best option. Try replacing the PCA.","368c36d3":"Nans","d4d42fd2":"Determine the optimal number of components for a given variance","0a719d2c":"Duplicated","9632ba7a":"## PCA","ac86e602":"Vizualize, numbers of clusters = 4 with MaxAbsScaler (min Error)","745ce451":"## KMeans","9fc86deb":"Add cluster number and grouping by features","44a35e14":"Read","d3fdad53":"Unbalanced features $\\uparrow$. Sadly","34b31461":"## GaussianMixture","5dbd5f48":"The result ~ was repeated"}}