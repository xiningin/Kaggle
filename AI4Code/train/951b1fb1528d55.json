{"cell_type":{"15459f5a":"code","7166f205":"code","4e107467":"code","c54d11f8":"code","5e02345d":"code","352fb86f":"code","9776706a":"code","16c8d5a4":"code","d464aca8":"code","bc89fe60":"code","e39efd5f":"code","c84281fb":"code","36c83470":"code","1e35161f":"code","a88edbdf":"code","3b941bb8":"code","4813f6a7":"code","bb7584bf":"code","4408ac57":"code","830cb8cd":"code","7d9d51f2":"code","97a4237c":"code","6995d02d":"code","1a9250c2":"code","2259d1a0":"code","78880cd1":"code","35f1bdc6":"code","26708a8f":"code","3aa27193":"code","c8050389":"code","d28082ac":"code","284820e1":"code","5e24aaa4":"code","769ef389":"code","87692061":"code","666fc133":"code","5fc22930":"code","ee1f41df":"code","5ed1c7b5":"code","f23754d4":"code","991acb8b":"code","ec890ad4":"code","7602e447":"code","0e2dfea6":"code","c42c9940":"code","14fb695e":"code","c8d2b893":"code","ded7cf57":"code","9999b551":"code","06170571":"code","efe7d52d":"code","eb9eaa23":"code","ffef343a":"code","796c6535":"code","a0f97dd3":"code","b1bcd5c4":"code","38cd430c":"code","553b8a69":"code","de968917":"code","6a2f9da6":"code","089e5b45":"code","60d9f82a":"code","1384081e":"code","0b11211e":"code","fa0bf27e":"code","3a11af3d":"code","d37a0c41":"code","d7006db7":"code","73f171ab":"code","d77273e5":"code","9f9711a9":"markdown","0baa8038":"markdown","35ebd500":"markdown","e47fdb77":"markdown","0508e843":"markdown","b52f6f67":"markdown","b657d55c":"markdown","3cf6c548":"markdown","630bf0eb":"markdown","808d287e":"markdown","b67142bc":"markdown","be210c08":"markdown","f98d5431":"markdown","3d2b3d9d":"markdown","87e8ee98":"markdown","9aa82235":"markdown","3b4ffb51":"markdown","614c9ccc":"markdown","6f614bfa":"markdown","24d8bd74":"markdown","d0a9b0e4":"markdown","3eb0477c":"markdown","921af7b8":"markdown","effd39a6":"markdown","a37ccbe1":"markdown","c05777c7":"markdown","14394101":"markdown","aeb4593a":"markdown","a55f2740":"markdown","a5be88d5":"markdown","5083f75e":"markdown","be83754c":"markdown","113156b5":"markdown","eefe68fa":"markdown","2fe0a17e":"markdown","bf3e568e":"markdown","682658e4":"markdown","674419ce":"markdown","9e3789c3":"markdown","fbec1e9f":"markdown","374c9f19":"markdown","3fedc2a9":"markdown","36d93b51":"markdown","29d7bfa1":"markdown","2cbd6046":"markdown","e852977f":"markdown","fa5f2137":"markdown","6f13db68":"markdown","b5d969ec":"markdown","dffde4bc":"markdown","e744f3b8":"markdown","45ed4e15":"markdown"},"source":{"15459f5a":"import pandas as pd\nimport numpy as np\n\nimport plotly.express as px\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom bs4 import BeautifulSoup as bsp\nimport datetime as dt\nfrom collections import Counter\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom wordcloud import WordCloud, STOPWORDS \n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport re\n\n\n%matplotlib inline","7166f205":"data = pd.read_csv(\"..\/input\/60k-stack-overflow-questions-with-quality-rate\/data.csv\")\ndata.head(10)","4e107467":"data.info()","c54d11f8":"data.shape","5e02345d":"data.describe(include='object').transpose()","352fb86f":"data['Body'][0]","9776706a":"data['Tags'][10]","16c8d5a4":"data['CleanBody'] = data['Body'].apply(lambda x:bsp(x,'html.parser').text.replace('\\\\','').replace('\\n','').replace('\\t','').replace('\\r',''))\ndata['Tags'] = data['Tags'].apply(lambda x:x.replace('<','').replace('>',' '))","d464aca8":"data['CreationDate'] = pd.to_datetime(data['CreationDate'],errors='coerce')\n\ndata['CreationMonth'] = data['CreationDate'].dt.month\ndata['CreationDay'] = data['CreationDate'].dt.day\ndata['CreationYear'] = data['CreationDate'].dt.year\ndata['CreationQuater'] = data['CreationDate'].dt.quarter\ndata['CreationDate'] = data['CreationDate'].dt.date","bc89fe60":"fig = px.histogram(data,x='CreationDate',title='Question\\'s Quality year by year',color='Y')\n\nfig.update_layout(height = 700)\nfig.update_xaxes(categoryorder='category descending',title='Date').update_yaxes(title='Number of Questions')\n\nfig.show()","e39efd5f":"fig = px.histogram(data,x='CreationDate',title='Number of Question year by year')\n\nfig.update_layout(height = 700)\nfig.update_xaxes(categoryorder='category descending',title='Date').update_yaxes(title='Number of Questions')\n\nfig.show()","c84281fb":"fig = px.histogram(data,x='CreationDate',title='Question\\'s Quality year by year',color='Y',nbins=6,barmode = 'group')\n\nfig.update_layout(height = 700)\nfig.update_xaxes(categoryorder='category descending',title='Date').update_yaxes(title='Number of Questions')\n\nfig.show()","36c83470":"allTags = data['Tags'].apply(lambda x: x.lower())\nallTags = allTags.values\nallTags = list(allTags)\nallTags = ''.join(allTags)\ncount = Counter(allTags.split())\n\n\ncount = pd.DataFrame(list(dict(count).items()),columns = ['Technology','Count'])\ncount.astype({'Count':'int64'})\n\ncount.sort_values('Count',axis =0,ascending = False,inplace = True)","1e35161f":"fig = px.scatter(count[:20], x = 'Technology',y='Count',size='Count',color='Count')\n\nfig.update_layout(title='Top 20 Technologies',xaxis=dict(title='Technology'),yaxis=dict(title='No. of Questions'))\n\nfig.show()","a88edbdf":"title = data['Title'].apply(lambda x:len(x.split()))\n\nfig = px.histogram(x=title.values,title='Length Distribution of  Question')\n\n\nfig.update_xaxes(title='Length Of Question').update_yaxes(title='No. Of Question')\nfig.show()","3b941bb8":"cleanBd = data['CleanBody'].apply(lambda x:len(x.split()))\n\nfig = px.histogram(x=cleanBd.values,title = 'Length Distribution of Clean Body')\n\nfig.show()","4813f6a7":"tags = data['Tags'].apply(lambda x:len(x.split()))\n\nfig = px.histogram(x= tags.values,title= ' Length Distribution of Tags')\nfig.show()","bb7584bf":"# Word Cloud with Stop words\nstopwords = set(STOPWORDS) \ndef WordCloudSW(values):\n    wordcloud = WordCloud(width = 500, height = 300, \n                background_color ='white', \n                stopwords = stopwords, \n                min_font_size = 10).generate(values) \n    \n    plt.figure(figsize=(19,9))\n\n    plt.axis('off')\n    plt.title(\"Without Stop Words\")\n    plt.imshow(wordcloud)\n    plt.show()","4408ac57":"# Word Cloud without Stop words\n\ndef word_cloud(values):\n    wordcloud = WordCloud(width = 500, height = 300, \n                background_color ='white', \n                min_font_size = 10).generate(values) \n    \n    plt.figure(figsize=(19,9))\n\n    plt.axis('off')\n    plt.title(\"With Stop Words\")\n    plt.imshow(wordcloud)\n    plt.show()","830cb8cd":"\njs = data[data['Tags'].str.contains('javascript')]['CleanBody'].values\n\njs = ' '.join(list(js))\nword_cloud(js)# with stop words","7d9d51f2":"WordCloudSW(js)# without stop words","97a4237c":"python = data[data['Tags'].str.contains('python')]['CleanBody']\npython = ''.join(list(python.values))\nword_cloud(python)","6995d02d":"WordCloudSW(python)# without stop words","1a9250c2":"# Unigrams before removing the stopwords\ndef getTopNWords(corpus,n=20):\n    vec = CountVectorizer().fit(corpus)\n    bow = vec.transform(corpus)\n    bow = bow.sum(0)\n    \n    top = [(word,bow[0,idx]) for word,idx in vec.vocabulary_.items()]\n    top = sorted(top,key = lambda x:x[1],reverse = True)\n    top = pd.DataFrame(top[:n],columns = ['Words','Count'])\n    return top","2259d1a0":"# Unigrams after removing the stopwords\ndef getTopNWordsSW(corpus , n = 20):\n    vec = CountVectorizer(stop_words= stopwords).fit(corpus)\n    bog = vec.transform(corpus)\n    \n    bog = bog.sum(axis=0)\n    bog = [(word,bog[0,idx]) for word,idx in vec.vocabulary_.items()]\n    \n    bog = sorted(bog,key = lambda x:x[1],reverse = True)\n    \n    top = pd.DataFrame(bog[:n],columns = ['Words','Count'])\n    return top","78880cd1":"\ntitle = getTopNWords(data['Title'])\n\n\nfig = px.bar(title, x='Words',y = 'Count')\nfig.update_layout(title= 'Most Ocurring Words in Title Before Removing StopWords',\n                 xaxis=dict(title='Words'),\n                 yaxis=dict(title='Counts'))\nfig.show()","35f1bdc6":"title = getTopNWordsSW(data['Title'])\n\n\nfig = px.bar(title, x='Words',y = 'Count')\nfig.update_layout(title= 'Most Ocurring Words in Title After Removing StopWords',\n                 xaxis=dict(title='Words'),\n                 yaxis=dict(title='Counts'))\nfig.show()","26708a8f":"title = getTopNWords(data['CleanBody'])\n\n\nfig = px.bar(title, x='Words',y = 'Count')\nfig.update_layout(title= 'Most Ocurring Words in CleanBody Before Removing StopWords',\n                 xaxis=dict(title='Words'),\n                 yaxis=dict(title='Counts'))\nfig.show()","3aa27193":"title = getTopNWordsSW(data['CleanBody'])\n\n\nfig = px.bar(title, x='Words',y = 'Count')\nfig.update_layout(title= 'Most Ocurring Words in CleanBody After Removing StopWords',\n                 xaxis=dict(title='Words'),\n                 yaxis=dict(title='Counts'))\nfig.show()","c8050389":"#before removing stop words\ndef bigramNWord(corpus,n = 20):\n    vec = CountVectorizer(ngram_range=(2,2)).fit(corpus)\n    bog = vec.transform(corpus)\n    \n    bog = bog.sum(0)\n    \n    bog = [(word,bog[0,idx]) for word,idx in vec.vocabulary_.items()]\n    \n    bog = sorted(bog,key = lambda x:x[1],reverse = True )\n    \n    bog = pd.DataFrame(bog[:n],columns = ['Word','Count'])\n    \n    return bog","d28082ac":"#before removing stop words\ndef bigramNWordSW(corpus,n = 20):\n    vec = CountVectorizer(ngram_range=(2,2),stop_words=stopwords).fit(corpus)\n    bog = vec.transform(corpus)\n    \n    bog = bog.sum(0)\n    \n    bog = [(word,bog[0,idx]) for word,idx in vec.vocabulary_.items()]\n    \n    bog = sorted(bog,key = lambda x:x[1],reverse = True )\n    \n    bog = pd.DataFrame(bog[:n],columns = ['Word','Count'])\n    \n    return bog","284820e1":"title = bigramNWord(data['Title'])\n\nfig= px.bar(title,x ='Word',y = 'Count')\nfig.update_layout(title = 'Bigram of Title Before Removing Stop Words',\n                 xaxis = dict(title='Bigram Words'),\n                 yaxis = dict(title='Counts'))\nfig.show()","5e24aaa4":"title = bigramNWordSW(data['Title'])\n\nfig= px.bar(title,x ='Word',y = 'Count')\nfig.update_layout(title = 'Bigram of Title After Removing Stop Words',\n                 xaxis = dict(title='Bigram Words'),\n                 yaxis = dict(title='Counts'))\nfig.show()","769ef389":"title = bigramNWord(data['CleanBody'])\n\nfig= px.bar(title,x ='Word',y = 'Count')\nfig.update_layout(title = 'Bigram of CleanBody Before Removing Stop Words',\n                 xaxis = dict(title='Bigram Words'),\n                 yaxis = dict(title='Counts'))\nfig.show()","87692061":"title = bigramNWordSW(data['CleanBody'])\n\nfig= px.bar(title,x ='Word',y = 'Count')\nfig.update_layout(title = 'Bigram of CleanBody After Removing Stop Words',\n                 xaxis = dict(title='Bigram Words'),\n                 yaxis = dict(title='Counts'))\nfig.show()","666fc133":"# for NLP \n\nimport emoji\n\nfrom nltk import pos_tag\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet,stopwords\nfrom nltk import word_tokenize,sent_tokenize\nimport nltk\nfrom nltk.tokenize import regexp_tokenize \n\n\n# for Making Prediciton Models \nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense,Flatten,LSTM,Dropout,Embedding\nfrom keras.models import Sequential,load_model\nfrom keras.callbacks import EarlyStopping\nfrom keras.utils import to_categorical\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_score,accuracy_score,confusion_matrix,classification_report,recall_score\nfrom sklearn.preprocessing import MultiLabelBinarizer\n\nfrom collections import Counter","5fc22930":"data['Y'] = data['Y'].map({'LQ_CLOSE':0,'LQ_EDIT':1,'HQ':2})\n\n\ndata['question'] = data['Title']+data['CleanBody']","ee1f41df":"slangs = {}\nslangs_df = pd.read_csv('..\/input\/allslangs\/AllSlangs.csv')\nslangs_df.drop('Unnamed: 0',1,inplace=True)\nslangs_df.dropna(inplace=True)\n\nfor index,rows in slangs_df.iterrows():slangs[str(rows['Abbreviation'].replace(' ',''))] = str(rows['FullForm'].lower()).strip(' ')","5ed1c7b5":"CONTRACTION_MAP = {\n\"ain't\": \"is not\",\n\"aren't\": \"are not\",\n\"can't\": \"cannot\",\n\"can't've\": \"cannot have\",\n\"'cause\": \"because\",\n\"could've\": \"could have\",\n\"couldn't\": \"could not\",\n\"couldn't've\": \"could not have\",\n\"didn't\": \"did not\",\n\"doesn't\": \"does not\",\n\"don't\": \"do not\",\n\"hadn't\": \"had not\",\n\"hadn't've\": \"had not have\",\n\"hasn't\": \"has not\",\n\"haven't\": \"have not\",\n\"he'd\": \"he would\",\n\"he'd've\": \"he would have\",\n\"he'll\": \"he will\",\n\"he'll've\": \"he he will have\",\n\"he's\": \"he is\",\n\"how'd\": \"how did\",\n\"how'd'y\": \"how do you\",\n\"how'll\": \"how will\",\n\"how's\": \"how is\",\n\"I'd\": \"I would\",\n\"I'd've\": \"I would have\",\n\"I'll\": \"I will\",\n\"I'll've\": \"I will have\",\n\"I'm\": \"I am\",\n\"I've\": \"I have\",\n\"i'd\": \"i would\",\n\"i'd've\": \"i would have\",\n\"i'll\": \"i will\",\n\"i'll've\": \"i will have\",\n\"i'm\": \"i am\",\n\"i've\": \"i have\",\n\"isn't\": \"is not\",\n\"it'd\": \"it would\",\n\"it'd've\": \"it would have\",\n\"it'll\": \"it will\",\n\"it'll've\": \"it will have\",\n\"it's\": \"it is\",\n\"let's\": \"let us\",\n\"ma'am\": \"madam\",\n\"mayn't\": \"may not\",\n\"might've\": \"might have\",\n\"mightn't\": \"might not\",\n\"mightn't've\": \"might not have\",\n\"must've\": \"must have\",\n\"mustn't\": \"must not\",\n\"mustn't've\": \"must not have\",\n\"needn't\": \"need not\",\n\"needn't've\": \"need not have\",\n\"o'clock\": \"of the clock\",\n\"oughtn't\": \"ought not\",\n\"oughtn't've\": \"ought not have\",\n\"shan't\": \"shall not\",\n\"sha'n't\": \"shall not\",\n\"shan't've\": \"shall not have\",\n\"she'd\": \"she would\",\n\"she'd've\": \"she would have\",\n\"she'll\": \"she will\",\n\"she'll've\": \"she will have\",\n\"she's\": \"she is\",\n\"should've\": \"should have\",\n\"shouldn't\": \"should not\",\n\"shouldn't've\": \"should not have\",\n\"so've\": \"so have\",\n\"so's\": \"so as\",\n\"that'd\": \"that would\",\n\"that'd've\": \"that would have\",\n\"that's\": \"that is\",\n\"there'd\": \"there would\",\n\"there'd've\": \"there would have\",\n\"there's\": \"there is\",\n\"they'd\": \"they would\",\n\"they'd've\": \"they would have\",\n\"they'll\": \"they will\",\n\"they'll've\": \"they will have\",\n\"they're\": \"they are\",\n\"they've\": \"they have\",\n\"to've\": \"to have\",\n\"wasn't\": \"was not\",\n\"we'd\": \"we would\",\n\"we'd've\": \"we would have\",\n\"we'll\": \"we will\",\n\"we'll've\": \"we will have\",\n\"we're\": \"we are\",\n\"we've\": \"we have\",\n\"weren't\": \"were not\",\n\"what'll\": \"what will\",\n\"what'll've\": \"what will have\",\n\"what're\": \"what are\",\n\"what's\": \"what is\",\n\"what've\": \"what have\",\n\"when's\": \"when is\",\n\"when've\": \"when have\",\n\"where'd\": \"where did\",\n\"where's\": \"where is\",\n\"where've\": \"where have\",\n\"who'll\": \"who will\",\n\"who'll've\": \"who will have\",\n\"who's\": \"who is\",\n\"who've\": \"who have\",\n\"why's\": \"why is\",\n\"why've\": \"why have\",\n\"will've\": \"will have\",\n\"won't\": \"will not\",\n\"won't've\": \"will not have\",\n\"would've\": \"would have\",\n\"wouldn't\": \"would not\",\n\"wouldn't've\": \"would not have\",\n\"y'all\": \"you all\",\n\"y'all'd\": \"you all would\",\n\"y'all'd've\": \"you all would have\",\n\"y'all're\": \"you all are\",\n\"y'all've\": \"you all have\",\n\"you'd\": \"you would\",\n\"you'd've\": \"you would have\",\n\"you'll\": \"you will\",\n\"you'll've\": \"you will have\",\n\"you're\": \"you are\",\n\"you've\": \"you have\",\n\"n't\":'not' \n}","f23754d4":"class PreprocessingData:\n  # ----------------------------- Removing Slangs from Raw Text -----------------------------------------\n  def removingSlangs(self,reviews):\n    doc = regexp_tokenize(str(reviews), \"[\\w']+\") \n\n    for token in doc:\n      if(token in CONTRACTION_MAP):\n        reviews = reviews.replace(token,CONTRACTION_MAP[token])\n      elif(token in slangs):\n        reviews = reviews.replace(token,slangs[token])\n    return reviews\n\n  \n  # ----------------------------- Part of Speech Tagging -----------------------------------------\n  def get_wordnet_pos(self,pos_tag):\n    if pos_tag.startswith('J'):\n        return wordnet.ADJ\n    elif pos_tag.startswith('V'):\n        return wordnet.VERB\n    elif pos_tag.startswith('N'):\n        return wordnet.NOUN\n    elif pos_tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return wordnet.NOUN\n\n        \n  \n  # ----------------------------- Removing StopWords and Lemmatizing Words -----------------------------------------\n  def CleaningData(self,val):\n    rex = re.sub(r'[^a-zA-Z0-9]+',' ',str(val))\n\n    pos = pos_tag(word_tokenize(rex))\n\n    filter = [WordNetLemmatizer().lemmatize(x[0],PreprocessingData.get_wordnet_pos(self,x[1])) for x in pos if x[0] not in stopwords.words('english')]\n\n    filter = ' '.join(filter)\n\n    return filter\n\n\n\n#ppd = PreprocessingData()\n#data['question']= data['question'].apply(lambda x:ppd.CleaningData(x))\n#data['question']= data['question'].apply(lambda x:ppd.removingSlangs(x))\n","991acb8b":"#data.to_csv('pp_data.csv',index = False)\ndata = pd.read_csv('..\/input\/preprocessed-data\/pp_data.csv')","ec890ad4":"allQuestions = data['question'].apply(lambda x:x.lower())\nallQuestions = list(allQuestions.values)\nallQuestions = ''.join(allQuestions)\ncount = Counter(allQuestions.split())\nlen(count) # unique vocabulary","7602e447":"vocab = 20000\nembedding_dim = 500\ntext_length = 100\ntrunc_type = 'post'\npadding_type = 'post'\noov_tok = '<OOV>' # OOV = Out of Vocabulary\ntraining_portion = .8","0e2dfea6":"token = Tokenizer(num_words=vocab,lower = True,oov_token =oov_tok )\ntoken.fit_on_texts(data['question'])\nseq = token.texts_to_sequences(data['question'])\npadded = pad_sequences(seq,maxlen = text_length,padding = 'post',truncating='post')","c42c9940":"y = to_categorical(data['Y'])\ny","14fb695e":"xTrain,xTest ,yTrain,yTest = train_test_split(padded,y,test_size = 0.2,random_state = 2)","c8d2b893":"model = Sequential()\n\nmodel.add(Embedding(vocab,embedding_dim,input_length=xTrain.shape[1]))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(250))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(20))\nmodel.add(Dense(3,activation='softmax'))\n\nmodel.compile(optimizer='adam',loss = 'categorical_crossentropy',metrics = ['accuracy'])","ded7cf57":"model.summary()","9999b551":"call = EarlyStopping(monitor='val_loss',patience = 10,verbose = 0)\n\n#model.fit(xTrain,yTrain,epochs = 20,verbose=1,callbacks=[call],validation_data=(xTest,yTest))","06170571":"#model.save('multiclass.h5')\n\nmm1 = load_model('..\/input\/weights\/multiclass.h5')","efe7d52d":"pred = mm1.predict_classes(xTest)\n\nimport numpy as np\nrounded_labels=np.argmax(yTest, axis=1)\nrounded_labels","eb9eaa23":"confusion_matrix(rounded_labels,pred)","ffef343a":"accuracy_score(rounded_labels,pred)","796c6535":"allTags = data['Tags'].apply(lambda x:x.lower())\nallTags = list(allTags.values)\nallTags = ''.join(allTags)\ncount = Counter(allTags.split())\nlen(count) # unique Tags","a0f97dd3":"count = pd.DataFrame(list(dict(count).items()),columns = ['Technology','Count'])\ncount.astype({'Count':'int64'})\n\ncount.sort_values('Count',axis =0,ascending = False,inplace = True)","b1bcd5c4":"fig = px.scatter(count, x = 'Technology',y='Count',size='Count',color='Count')\n\nfig.update_layout(title='All Technologie\\'s-Tags and their Frequency',xaxis=dict(title='Technology'),yaxis=dict(title='No. of Questions'))\n\nfig.show()","38cd430c":"count = count[count['Count']>=1000]","553b8a69":"fig = px.scatter(count, x = 'Technology',y='Count',size='Count',color='Count')\n\nfig.update_layout(title='Technologie\\'s have Frequency greater than 1000',xaxis=dict(title='Technology'),yaxis=dict(title='No. of Questions'))\n\nfig.show()","de968917":"tech = list(count['Technology'].values)\nlen(tech) # we have 23 multilabe which cover most of the questions","6a2f9da6":"def FinalTags(tags):\n  t = list()\n  for tag in tags.split():\n    if (tag.lower() in tech):\n      t.append(tag)\n  if(t==[]):\n    return None\n  else:\n    return t\ndata['Tags'] = data['Tags'].apply(FinalTags)\ndata.isna().sum()","089e5b45":"data.dropna(inplace= True,axis= 0)\ndata.isna().sum()","60d9f82a":"multi = MultiLabelBinarizer()\ny = multi.fit_transform(data['Tags'])","1384081e":"token = Tokenizer(vocab,lower=True,oov_token=oov_tok)\ntoken.fit_on_texts(data['question'])\nseq = token.texts_to_sequences(data['question'])\npadded = pad_sequences(seq,padding = 'post',truncating='post',maxlen=text_length)\n\npadded.shape","0b11211e":"xTrain,xTest,yTrain,yTest = train_test_split(padded,y,test_size = 0.2,random_state = 2)","fa0bf27e":"model = Sequential()\n\nmodel.add(Embedding(vocab,embedding_dim,input_length=xTrain.shape[1]))\nmodel.add(Dropout(0.3))\nmodel.add(LSTM(250))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(100))\nmodel.add(Dense(50))\nmodel.add(Dense(yTrain.shape[1]))\n\nmodel.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])","3a11af3d":"model.summary()","d37a0c41":"call = EarlyStopping(monitor='val_loss',verbose=0,patience=6)\n\n#model.fit(xTrain,yTrain,epochs=20,validation_data=(xTest,yTest),callbacks=[call],verbose = 1)","d7006db7":"#model.save('Labels.h5')\nmm2 = load_model('..\/input\/weights\/Labels.h5')","73f171ab":"rounded_labels=np.argmax(yTest, axis=1)\npred = mm1.predict_classes(xTest)","d77273e5":"confusion_matrix(rounded_labels,pred)","9f9711a9":"Below Bar Chart show Frequency of Question's Quality year by year ","0baa8038":"**Bigram of CleanBody After Removing Stop Words**","35ebd500":"After Checking Tags we now have 13k value as None values \n\nWe will simply drop this for further Classification","e47fdb77":"Now lets see for the Python ","0508e843":"![Screenshot%202020-09-27%20at%201.22.02%20AM.png](attachment:Screenshot%202020-09-27%20at%201.22.02%20AM.png)","b52f6f67":"**Bigram of Title After Removing Stop Words**","b657d55c":"Checking Technologies- Tags and their frequency","3cf6c548":"Checking Unique words in Questions","630bf0eb":"**Most Ocurring Words in CleanBody After Removing StopWords**","808d287e":"**First We will Perform Multi-Class Text Classification on Y Column for Quality Prediction**","b67142bc":"Now let's find out Question's Length Range and Frequency (Question's Title)","be210c08":"Acuracy Score for Multiclass Classification","f98d5431":"# Classification Using Keras LSTM","3d2b3d9d":"# Cleaning and Preprocessing Dataset","87e8ee98":"![MultiClass.png](attachment:MultiClass.png)","9aa82235":"* We have saw that most frequently asked question are of JavaScript Tag\n* So lets find which are most occuring word in JavaScript technology\n","3b4ffb51":"**Most Ocurring Words in CleanBody Before Removing StopWords**","614c9ccc":"**This Dataset contain 60,000 Stack Overflow questions from 2016-2020 and classified them into three categories**","6f614bfa":"**First Import some basice Libraries**","24d8bd74":"For Quality(Target Column) We perform Onehot Encoding using Keras API","d0a9b0e4":"Checking Tags if they are present in tech list then we will pass list in Tags Column otherwise will pass None","3eb0477c":"**Bigram of Title Before Removing Stop Words**","921af7b8":"We can see that Body and Tags Columns are in HTML Tag, so we will basicly extract only text from HTML Tag","effd39a6":"**Confusion_Matrix**","a37ccbe1":"Now let's find out Question's Length Range and Frequency (Question's Body)","c05777c7":"# Classification Using Neural Network","14394101":"**Most Ocurring Words in Title After Removing StopWords**","aeb4593a":"> We have 23 multilabel which cover most of the questions\n\n> We will save these 23 Multilabel Tags in Variable tech List","a55f2740":"Checking Frequency of Question's Quality year by year","a5be88d5":"**Natural Language Processing**\n\n> Converting Y(Column) to categorical\n\n> Concatenating Title and CleanBody Column(for Classification) together and making new column \"question\"\n\n> Removing StopWord from Question\n\n> Lemmatizing Question\n\n> Removing Social Media Slang","5083f75e":"**Reading Dataset and Previewing First 10 Values**","be83754c":"Now let's see how many tags are use in Quesitons","113156b5":"* HQ: High-quality posts with 30+ score and without a single edit.\n\n* LQ_EDIT: Low-quality posts with a negative score and with multiple community edits. However, they still remain open after the edits.\n\n* LQ_CLOSE: Low-quality posts that were closed by the community without a single edit.","eefe68fa":"Checking Tags Frequency \n\nIn which Basically we are finding out Top 20 Programming Language or Technology types of Question were asked","2fe0a17e":"# Visualization","bf3e568e":"Now Splitting Data into \n\n* xTrain,yTrain -> Data portion on which our Neural Network will trianed\n* xTest,yTest -> For Testing Purpose of our Neural Network","682658e4":"**Checking some basic information about dataset**","674419ce":"For Neural Network Text Classification we will prepare the data in form of \n\n* First we will to Tokenize the words \n* After Tokenizing we will Make the text into sequence \n* After Sequence we will perform Padding(To make sure that each Question have Same Length)","9e3789c3":"# Multilabel Classification ","fbec1e9f":"![0*UEtwA2ask7vQYW06.png](attachment:0*UEtwA2ask7vQYW06.png)","374c9f19":"**Importing Basic Libraries for NLP and Neural Network**","3fedc2a9":"**Bigram of CleanBody Before Removing Stop Words**","36d93b51":"# What we will cover in this Notebook\n\n> Cleaning and formatting dataset\n\n> Visualizing dataset and find some usefull insight out of it\n\n> Carving text for Neural Network Classification\n\n> Performing Keras LSTM for \n  * Multi-Class text Classification\n  * Multi-Label text Classification","29d7bfa1":"For models weights(h5)and Preprocessed Dataset \n\nI, First save into output file and later download and upload into input file\n\nbecause after refreshing kaggle kernel get return to it's previous state so it's remove the output file","2cbd6046":"If you Guys have any Suggestion\/s or any tips to improve classification Model or this kaggle notebook please Comment it will help me to improvise alot\n\nThank You","e852977f":"Below Graph show Amount of Questions asked year by year","fa5f2137":"**Converting CreationDate Column Value datatype from object to pandas Datetime**","6f13db68":"Remove the Tech who have less than 1000 count","b5d969ec":"**Most Ocurring Words in title with stopwords**","dffde4bc":"After Preforming all the Steps we will store Preprocessed Dataset into CSV file","e744f3b8":"To Prepare for Multilabel Classification We will use MultiLabelBinarizer for the Target Labels and same step for the Question Text","45ed4e15":"pic credit - google images"}}