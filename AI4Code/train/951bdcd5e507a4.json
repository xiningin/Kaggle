{"cell_type":{"e88b9830":"code","af387191":"code","fe65def0":"code","46b20de7":"code","d2ab72f8":"code","126e859b":"code","17c69ba2":"code","7ae791a0":"code","8f0d8541":"code","71f4bf1c":"code","9aef159a":"code","ad2be5ec":"code","59000ceb":"code","8dede1ee":"code","e99fd809":"code","c491d5e2":"code","3cce4371":"code","0c249711":"code","bd41cdc6":"code","b5b035f6":"code","8f0293e9":"code","be527c62":"code","55c2d53f":"code","ba97ef55":"code","9676264c":"code","7c5e80a7":"code","ab7f5e5c":"code","2ba1193c":"code","d7bad8b8":"code","5c7e5353":"code","f2fb98e5":"code","9a4ad2ad":"markdown","a35e4efb":"markdown","f82aa3bb":"markdown","bc49347a":"markdown","b31f6c99":"markdown","0b6e9ee7":"markdown","7fe76923":"markdown","e2992594":"markdown","6b3bad1e":"markdown","086d4208":"markdown","39a39a3c":"markdown","c499ea1c":"markdown","3896df65":"markdown","6699db40":"markdown","e982532b":"markdown","e680a888":"markdown","b0607287":"markdown","799e6743":"markdown","aad94aa8":"markdown"},"source":{"e88b9830":"import pandas as pd\nimport transformers\nimport tokenizers\nimport torch\nimport torch.nn as nn","af387191":"data = pd.read_csv('..\/input\/tweet-sentiment-extraction\/train.csv')\ndata.head()","fe65def0":"ROBERTA_PATH = '..\/input\/roberta-base'","46b20de7":"MAX_LEN = 192\nTOKENIZER = tokenizers.ByteLevelBPETokenizer(vocab_file=f\"{ROBERTA_PATH}\/vocab.json\", \n                                             merges_file=f\"{ROBERTA_PATH}\/merges.txt\", \n                                             add_prefix_space=True, \n                                             lowercase=True)","d2ab72f8":"tokens = TOKENIZER.encode(data.text.values[0])\ntokens","126e859b":"tokens.tokens","17c69ba2":"tokens.ids","7ae791a0":"tokens.type_ids","8f0d8541":"tokens.offsets","71f4bf1c":"tokens.attention_mask","9aef159a":"expt_tokenizer = tokenizers.ByteLevelBPETokenizer(vocab_file=f\"{ROBERTA_PATH}\/vocab.json\", \n                                                merges_file=f\"{ROBERTA_PATH}\/merges.txt\", \n                                                add_prefix_space=False, \n                                                lowercase=True)\n\ntemp = expt_tokenizer.encode(data.text.values[0])\ntemp.tokens, temp.ids, temp.type_ids, temp.offsets, temp.attention_mask","ad2be5ec":"expt_tokenizer.decode(expt_tokenizer.encode(\"Hello\").ids)","59000ceb":"TOKENIZER.decode(TOKENIZER.encode(\"Hello\").ids)","8dede1ee":"conf = transformers.ReformerConfig.from_pretrained(ROBERTA_PATH)\nmodel = transformers.RobertaModel.from_pretrained(ROBERTA_PATH, config=conf)","e99fd809":"ids = torch.tensor([[0] + tokens.ids + [2]])\nattention_mask = torch.tensor([[1, 1] + tokens.attention_mask])\ntoken_type_ids = torch.tensor([tokens.type_ids + [0, 0]])","c491d5e2":"output = model(ids, attention_mask=attention_mask, token_type_ids=token_type_ids)","3cce4371":"len(output)","0c249711":"output[0].shape","bd41cdc6":"output[1].shape","b5b035f6":"model = transformers.RobertaForQuestionAnswering.from_pretrained('roberta-base')","8f0293e9":"ques = \"What is the name of prime minister of India?\"\ntext = \"India is one of the largest country in the world and its current prime minister is Narendra Modi.\"","be527c62":"tok_ques = TOKENIZER.encode(ques)\ntok_text = TOKENIZER.encode(text)","55c2d53f":"len(tok_ques.ids), len(tok_text.ids)","ba97ef55":"ids = torch.tensor([[0] + tok_ques.ids + [2, 2] + tok_text.ids + [2]])\nattention_mask = torch.tensor([[1] + tok_ques.attention_mask + [1, 1] + tok_text.attention_mask + [1]])\n# roberta doesn't make use of token_type_ids so we can have a all zero tensor of correct dimension\ntoken_type_ids = torch.tensor([[0] + tok_ques.type_ids + [0, 0] + tok_text.type_ids + [0]])","9676264c":"start, end = model(ids, attention_mask=attention_mask, token_type_ids=token_type_ids)","7c5e80a7":"start = nn.Softmax()(start)\nend = nn.Softmax()(end)","ab7f5e5c":"start.shape, end.shape","2ba1193c":"n = start.shape[1]\nmax_ij = 0\n\nstart_idx = None\nend_idx = None\n\nfor i in range(14, n-2):\n    for j in range(i+1, n-1):\n        if start[0][i] + end[0][j] > max_ij:\n            max_ij = start[0][i] + end[0][j]\n            start_idx = i\n            end_idx = j","d7bad8b8":"start_idx, end_idx, max_ij","5c7e5353":"result = list(ids[0][start_idx: end_idx+1])","f2fb98e5":"TOKENIZER.decode(ids=result)","9a4ad2ad":"Here the first token is 'i' which is the first part when we split \"i'd\" in the input sentence. 'i' is the first character of the sentence so the offset should be (0, 1) but we have to take into account that a space was added by the tokenizer to the text.","a35e4efb":"# Using the question answering model from huggingface with pretrained model","f82aa3bb":"## What happens if set add_prefix_space to false?","bc49347a":"With fine tuning and better function to choose start and end index roberta can give very accurate results","b31f6c99":"# How roberta tokenizes the text?","0b6e9ee7":"# Pretrained RoBERTa output","7fe76923":"Without any fine tuning it is just way off the answer. The point of this kernel wasn't have accuracy but to show how we can use huggingface's implementation of RoBERTa.","e2992594":"The second output is the result of pooling together all of the 768 length layers of first output.","6b3bad1e":"Notice the difference in results","086d4208":"I am using [Abhishek Thakur's](https:\/\/www.kaggle.com\/abhishek) pretrained roberta base [model](https:\/\/www.kaggle.com\/abhishek\/roberta-base) and data from the contest [Tweet Sentiment Extraction](https:\/\/www.kaggle.com\/c\/tweet-sentiment-extraction) in this kernel.","39a39a3c":"All ones as there is no padding\n\nThe above are the attributes that would be needed to train roBERTa","c499ea1c":"The results are exactly the same. The reason is that we require add_prefix_space as True because tokenizer need a space to start the input string. If we set it to False then tokenizer encode and decode method will not conserve the absence of a space at the beginning of a string. Look at the example below","3896df65":"The first output is the sequenced output. One 768 sized tensor for each of the 12 tokens.","6699db40":"Note here we have an argument add_prefix_space which is set to True so the tokenizer will add a space to start of the text passed into it.","e982532b":"It seems like to every word that is splitted by the tokenizer it is adding a special character '\u0120' to the start of the first part of the splitted text.\n\nAlso it is to be noted that special tokens have not been added by the tokenizer.","e680a888":"We will be using the tokenizer from above examples","b0607287":"This kernel is to show how roberta tokenizes and how to output of roberta looks like for absolute beginners. Once you see the output of the model you can pass the output through linear layers of desired dimension according to dataset and use case.","799e6743":"# fin","aad94aa8":"Roberta doesn't use the type_ids so we will be passing all zero vector of size of length of token ids everytime"}}