{"cell_type":{"3e0d3091":"code","2eff1015":"code","22f7dec4":"code","61a794c5":"code","ab2c30cf":"code","2bd83f39":"code","0487b451":"code","eac87e59":"code","dc14393c":"code","6cb2cd8a":"code","9a89716f":"code","cd189cd9":"code","e7b29ecc":"code","010c1222":"code","0b33a4b5":"code","a03681e0":"markdown","9bed73cc":"markdown","963d9b0c":"markdown","675702f5":"markdown","6a232043":"markdown","21c98cf6":"markdown","990c3d19":"markdown","29ba8863":"markdown","fa355d40":"markdown","678fb0bf":"markdown","5b28e928":"markdown"},"source":{"3e0d3091":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2eff1015":"data = pd.read_csv(\"\/kaggle\/input\/breast-cancer-prediction-dataset\/Breast_cancer_data.csv\")\ndata.head()","22f7dec4":"data.info()","61a794c5":"data.describe()","ab2c30cf":"import seaborn as sns\n\ndata_features = data.drop(['diagnosis'], axis = 1)\nfor columnName in data_features:\n    g = sns.FacetGrid((pd.melt(data, id_vars = ['diagnosis'], value_vars = columnName)), col = \"diagnosis\")\n    sns.set(style=\"darkgrid\")\n    g.map(sns.kdeplot,\"value\")\n    g.set_axis_labels(columnName)\n","2bd83f39":"import matplotlib.pyplot as plt\n\ncorrelation_matrix = data_features.corr()\nsns.heatmap(correlation_matrix, annot=True)\nplt.show()","0487b451":"data['diagnosis'].value_counts()","eac87e59":"!pip install pycaret","dc14393c":"from pycaret.classification import *\nsetup_cl = setup(data = data, target = 'diagnosis')","6cb2cd8a":"best = compare_models(n_select = 5, sort = 'Recall')","9a89716f":"lda_model = create_model('lda')\nbagged_lgd = ensemble_model(lda_model, method = 'Bagging')","cd189cd9":"plot_model(bagged_lgd, plot = 'pr')\n","e7b29ecc":"plot_model(bagged_lgd, plot= 'confusion_matrix')\n","010c1222":"plot_model(bagged_lgd, plot = 'class_report')\n","0b33a4b5":"optimize_threshold(bagged_lgd, false_negative = -5000)","a03681e0":"Mean_radius, Mean_perimeter and mean_area are fully correlated. If we were running logistics regression, choosing these features would have been problematic since those models are based on assumption that features are completely independent. Since we are using tree based models, this wouldn't be a problem ","9bed73cc":"I will now plot density plots for diagnosed cancer and not cancer for each feature to see distinguishment. ","963d9b0c":"# OPTIMIZE THRESHOLD\nThere is an option where i can limit the false negatives to be zero here. Given its implication in such a case where breast cancer goes undetected. It takes a trained model object (a classifier) and the loss function simply represented by true positives, true negatives, false positives and false negatives. This function returns an interactive plot where loss function (y-axis) is represented as a function of different probability threshold values on x-axis. A vertical line is then shown to represent the best value of probability threshold for that specific classifier.","675702f5":"# USING PYCARET TO BUILD A QUICK PREDICTIVE MODEL\nPyCaret is an open source, low-code machine learning library in Python that allows you to go from preparing your data to deploying your model within minutes in your choice of notebook environment.\n\nThe idea is to use to resolve a problem with lesser coding effort. \nWe will be using it to:\n1. Preprocess Features\n2. Deploy and compare models\n3. Tune the models\n4. Evaluate the model results\n5. Interpret the model\n6. Make predictions!","6a232043":"# Intializing the Setup","21c98cf6":"Will look at the distribution of the target variable now","990c3d19":"Despite the provisions by PyCaret, I still like to do an EDA Myself to understand the data. We need to understand the problem before looking for its answer. In this case, we have a dataset was obtained from the University of Wisconsin Hospitals, Madison from Dr. William H. Wolberg. The features are of the lump found in patients (its radius, texture, perimeter etc). We have the diagnosis against this feature.\n\nWe will be making a model that can use these features to predict diagnosis of breast cancer using PyCaretDespite the provisions by PyCaret, I still like to do an EDA Myself to understand the data. We need to understand the problem before looking for its answer. In this case, we have dataset of ","29ba8863":"We can also see AUC, Precision, Recall etc. for ensemble models for exploration of a better model for this problem","fa355d40":"# MODEL ANALYSIS","678fb0bf":"Precision is the ratio between the True Positives and all the Positives. For our problem statement, that would be the measure of patients that we correctly identify having a breast cancer out of all the patients actually having it.\n\nThe recall is the measure of our model correctly identifying True Positives. Thus, for all the patients who actually have cancer, recall tells us how many we correctly identified as having a breast cancer.\n\nRecall in this case is extremely important, as imagine having a patient who has breast cancer but is not treated. We can see that in comparison, LDA gives us the best estimate of Recall and AUC.","5b28e928":"Looks like mean_smoothness and texture in both cases, benign or malignant tumor is almost the same. Other features however shows a clearer distinguishment. Let's look at the correlation of the features "}}