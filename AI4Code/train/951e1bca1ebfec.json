{"cell_type":{"384e8765":"code","36544daf":"code","326bb0ab":"code","64f822a8":"code","ecaed2ac":"code","d0420676":"code","32b00b26":"code","5e759585":"code","cb3442ac":"code","2840283b":"code","fd665910":"code","cf8084aa":"code","3c531b3b":"code","bb8c52fa":"code","e9084d55":"code","9629db1a":"code","4c14d2d7":"code","e16d547d":"code","1dcfe977":"code","ffde6300":"code","7fadf1d4":"code","123071d5":"code","9459d41b":"code","5cff8caa":"code","94022c3b":"code","efe1b83c":"code","0cafb1ac":"code","0da6a93c":"code","24c9979a":"code","aef22283":"code","178ab395":"code","6d105469":"code","6cc2095e":"code","5d08054b":"code","11c13d04":"code","247576b9":"code","3925daf0":"code","de8fe3cf":"code","6e069503":"code","12cbc963":"code","b08c732b":"code","74bb61ea":"code","e238092d":"code","5bd9c97c":"code","ffbf3e31":"code","43729ab2":"code","00a5c2c7":"code","c49e03f5":"code","910beebd":"code","484401a1":"code","67ca3b7a":"code","da06f596":"code","f1133a05":"code","eaf4f5b6":"code","64b6dfc6":"code","8d2cbdc6":"code","4797be36":"code","ad138322":"code","b03b18d0":"code","6389c0b9":"code","e92bbd88":"code","6c35f062":"code","e159d425":"code","36814282":"code","55a0d2a7":"code","e515664b":"code","91191066":"code","d10d0b5d":"code","5ccf2b93":"code","0b33a8d6":"code","f104e4f5":"code","90f1852c":"markdown","84236345":"markdown","fc915207":"markdown","19709f03":"markdown","4c3d1ce8":"markdown","702b473e":"markdown","fa720968":"markdown","283f813a":"markdown","bdf2a274":"markdown","104952e7":"markdown","2a334fa5":"markdown","a2c1ac60":"markdown"},"source":{"384e8765":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","36544daf":"sns.set_theme()","326bb0ab":"train_df = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/titanic\/test.csv\")","64f822a8":"train_df.head()","ecaed2ac":"print(\"Number of rows present in the dataset    :{}\".format(train_df.shape[0]))\nprint(\"Number of columns present in the dataset :{}\".format(train_df.shape[1]))","d0420676":"train_df.info()","32b00b26":"train_df.drop(\"Cabin\", axis = 1, inplace = True)","5e759585":"train_df[\"Age\"].fillna(train_df[\"Age\"].mean(), inplace = True)","cb3442ac":"train_df[train_df.isna().any(axis=1)]","2840283b":"train_df.dropna(inplace=True)\ntrain_df.duplicated().sum()","fd665910":"train_df.head()","cf8084aa":"train_df.columns","3c531b3b":"train_df = train_df[['Survived','Pclass','Sex','Age',\n                      'SibSp','Parch','Fare','Embarked']]\ndummies = pd.get_dummies(train_df[[\"Sex\",\"Embarked\"]])\ndf = pd.concat([train_df,dummies], axis=1)\ndf.drop([\"Sex\",\"Embarked\"], axis=1, inplace=True)\ndf.head()","bb8c52fa":"print(\"Number of rows present in a dataset after preprocessing     :{}\".format(df.shape[0]))\nprint(\"Number of columns present in a dataset after preprocessing  :{}\".format(df.shape[1]))","e9084d55":"df.describe()","9629db1a":"correlate = df.corr()\ncorrelate[\"Survived\"]","4c14d2d7":"df[[\"Pclass\",\"Age\",\"Fare\",\"Sex_female\",\n    \"Sex_male\",\"Embarked_C\",\"Embarked_Q\",\n    \"Embarked_Q\"]].hist(bins=50, figsize = (20,15))\nplt.show()","e16d547d":"def diag_plots(data,variable):\n    plt.figure(figsize = (10,5))\n    plt.subplot(1,2,1)\n    sns.distplot(data[variable], bins = 15)\n    \n    plt.subplot(1,2,2)\n    stats.probplot(data[variable], dist = \"norm\", plot = plt)\n    \n    plt.show()\n    ","1dcfe977":"diag_plots(df, \"Age\")","ffde6300":"diag_plots(df, \"Fare\")","7fadf1d4":"#Logarithmic Transformation on Fare\ndf[\"Log_Fare\"] = np.log(df[\"Fare\"]+1) # adding +1 because if there are any zero's, we can't perform log on zero's\ndiag_plots(df, \"Log_Fare\")","123071d5":"#Reciprocal Transformation\ndf[\"Rec_Fare\"] = 1\/(df[\"Fare\"]+1)\ndiag_plots(df, \"Rec_Fare\")","9459d41b":"#Square Root Transformation\ndf[\"Sqrt_Fare\"] = df[\"Fare\"]**(1\/2)\ndiag_plots(df, \"Sqrt_Fare\")","5cff8caa":"#Square Root Transformation\ndf[\"Exp_Fare\"] = df[\"Fare\"]**(1\/5)\ndiag_plots(df, \"Exp_Fare\")","94022c3b":"#Box cox\ndf[\"bc_fare\"], param = stats.boxcox(df.Fare+1)\nprint(\"Optimal Lambda Param: {}\".format(param))\ndiag_plots(df, \"bc_fare\")","efe1b83c":"df.columns","0cafb1ac":"X = df[['Pclass', 'Age', 'SibSp', 'Parch', 'Log_Fare', 'Sex_female',\n       'Sex_male', 'Embarked_C', 'Embarked_Q', 'Embarked_S']]\ny = df[\"Survived\"]","0da6a93c":"x_train, x_test, y_train, y_test = train_test_split(X,y,test_size = 0.2, random_state = 0)","24c9979a":"print(x_train.shape, y_train.shape)\nprint(x_test.shape, y_test.shape)","aef22283":"Log_class = LogisticRegression()\nLog_class.fit(x_train,y_train)\npredict_log = Log_class.predict(x_test)\npredict_log","178ab395":"from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n\ndef eval(y_test, predict):\n    print(\n        \"accuracy              :\\n\\n{}\".format(accuracy_score(y_test, predict)), \n        \"\\n\\nconfusion matrix      :\\n\\n{}\".format(confusion_matrix(y_test, predict)), \n        \"\\n\\nclassification report :\\n\\n{}\".format(classification_report(y_test, predict))\n    )","6d105469":"eval(y_test, predict_log)","6cc2095e":"from sklearn.linear_model import SGDClassifier\n\nsgd_clf = SGDClassifier(random_state=0)\nsgd_clf.fit(x_train,y_train)\npredict_sgd = sgd_clf.predict(x_test)\n\npredict_sgd","5d08054b":"eval(y_test, predict_sgd)","11c13d04":"dt_clf = DecisionTreeClassifier(max_depth = 11)\ndt_clf.fit(x_train, y_train)\npredict_dtc = dt_clf.predict(x_test)\npredict_dtc","247576b9":"eval(y_test, predict_dtc)","3925daf0":"rfc_clf = RandomForestClassifier(max_depth = 12, min_samples_split = 6, random_state = 0)\nrfc_clf.fit(x_train,y_train)\npredict_rfc = rfc_clf.predict(x_test)\npredict_rfc","de8fe3cf":"eval(y_test, predict_rfc)","6e069503":"param_grid = {\n    'max_depth': [80, 90, 100, 110],\n    'min_samples_split': [8, 10, 12]\n}","12cbc963":"rf_clf = RandomForestClassifier()","b08c732b":"grid_search = GridSearchCV(estimator = rf_clf, \n                           param_grid = param_grid, \n                           cv=3, \n                           n_jobs = -1, \n                           verbose = 2)\ngrid_search.fit(x_train,y_train)","74bb61ea":"grid_search.best_score_","e238092d":"grid_search.best_estimator_","5bd9c97c":"ada_clf = AdaBoostClassifier().fit(x_train,y_train)\npredict_abc = ada_clf.predict(x_test)\npredict_abc","ffbf3e31":"eval(y_test, predict_abc)","43729ab2":"ab_clf = AdaBoostClassifier()\nab_clf.get_params()","00a5c2c7":"from sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import cross_val_score","c49e03f5":"cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\nn_scores = cross_val_score(ab_clf, x_train, y_train, \n                           scoring='accuracy', cv=cv, \n                           n_jobs=-1, error_score='raise')\n\nprint('Accuracy: %.3f (%.3f)' % (np.mean(n_scores), np.std(n_scores)))","910beebd":"AdaBoostClassifier().get_params()","484401a1":"cb_clf = CatBoostClassifier(task_type = \"GPU\", \n                            eval_metric = 'Accuracy', \n                            iterations = 1000, \n                            random_state=0\n                           )\ncb_clf.fit(x_train,y_train)\npredict_cbc = cb_clf.predict(x_test)\npredict_cbc","67ca3b7a":"eval(y_test,predict_cbc)","da06f596":"cb_clf.get_params()","f1133a05":"param_grid = {\n    'iterations': [250,500,750,1000,1500],\n    'task_type': [\"GPU\"],\n    'random_state': [0]\n}","eaf4f5b6":"from scipy.stats import randint as sp_randInt\nfrom scipy.stats import uniform as sp_randFloat\nfrom sklearn.model_selection import RandomizedSearchCV","64b6dfc6":"model = CatBoostClassifier()\nparameters = {'depth'         : sp_randInt(4, 10),\n              'learning_rate' : sp_randFloat(),\n              'iterations'    : sp_randInt(10, 100)\n             }\n    \nrandm = RandomizedSearchCV(estimator=model, param_distributions = parameters, \n                               cv = 2, n_iter = 10, n_jobs=-1)\nrandm.fit(x_train, y_train)\n\n# Results from Random Search\nprint(\"========================================================\")\nprint(\" Results from Random Search \" )\nprint(\"========================================================\")    \n    \nprint(\"\\n The best estimator across ALL searched params:\\n\",\n      randm.best_estimator_)\n    \nprint(\"\\n The best score across ALL searched params:\\n\",\n      randm.best_score_)\n    \nprint(\"\\n The best parameters across ALL searched params:\\n\",\n      randm.best_params_)\n\nprint(\"========================================================\")","8d2cbdc6":"predict_rcbc = randm.predict(x_test)\neval(y_test, predict_rcbc)","4797be36":"from xgboost import XGBClassifier\nXGBClassifier()","ad138322":"xgb = XGBClassifier(max_depth=5, n_estimators = 40).fit(x_train, y_train)\npredict_xgbc = xgb.predict(x_test)\neval(y_test,predict_xgbc)","b03b18d0":"pred = [predict_log,predict_sgd,predict_dtc,predict_rfc,predict_abc,predict_cbc,predict_rcbc,predict_xgbc]\npred","6389c0b9":"print(\"|==================================================================|\\n\")\nprint(\"  The ACCURACY OF LogisticRegression is     |  {}\\n\".format(accuracy_score(y_test,predict_log)))\nprint(\"|==================================================================|\\n\")\nprint(\"  The ACCURACY OF SGDClassifier is          |  {}\\n\".format(accuracy_score(y_test,predict_sgd)))\nprint(\"|==================================================================|\\n\")\nprint(\"  The ACCURACY OF DecisionTreeClassifier is |  {}\\n\".format(accuracy_score(y_test,predict_dtc)))\nprint(\"|==================================================================|\\n\")\nprint(\"  The ACCURACY OF RandomForestClassifier is |  {}\\n\".format(accuracy_score(y_test,predict_rfc)))\nprint(\"|==================================================================|\\n\")\nprint(\"  The ACCURACY OF AdaBoostClassifier is     |  {}\\n\".format(accuracy_score(y_test,predict_abc)))\nprint(\"|==================================================================|\\n\")\nprint(\"  The ACCURACY OF CatBoostClassifier is     |  {}\\n\".format(accuracy_score(y_test,predict_rcbc)))\nprint(\"|==================================================================|\\n\")\nprint(\"  The ACCURACY OF XGBoostClassifier is      |  {}\\n\".format(accuracy_score(y_test,predict_xgbc)))\nprint(\"|==================================================================|\\n\")","e92bbd88":"test_df = pd.read_csv(\"..\/input\/titanic\/test.csv\")","6c35f062":"test_df.head()","e159d425":"test_df.info()","36814282":"test_df.drop(\"Cabin\", axis=1, inplace=True)","55a0d2a7":"test_df[\"Age\"].fillna(train_df[\"Age\"].mean(), inplace = True)\ntest_df[\"Fare\"].fillna(test_df[\"Fare\"].mean(), inplace = True)","e515664b":"passenger_id = test_df[\"PassengerId\"]\ndf = test_df[['Pclass','Sex','Age',\n              'SibSp','Parch','Fare','Embarked']]","91191066":"x = df\nx.head()","d10d0b5d":"dummies = pd.get_dummies(x[[\"Sex\",\"Embarked\"]])\ndata = pd.concat([df, dummies], axis=1)\ndata.drop([\"Sex\",\"Embarked\"], axis=1, inplace=True)\ndata.head()","5ccf2b93":"prediction = rfc_clf.predict(data)\nprediction","0b33a8d6":"output = pd.DataFrame({'PassengerId': passenger_id, 'Survived': prediction.astype(int)})\noutput","f104e4f5":"output.to_csv('submission1.csv', index=False)","90f1852c":"### **Variable Notes**\n\n**pclass**: A proxy for socio-economic status (SES)\n1st = Upper\n2nd = Middle\n3rd = Lower\n\n**age**: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n\n**sibsp**: The dataset defines family relations in this way...\n\nSibling = brother, sister, stepbrother, stepsister\n\nSpouse = husband, wife (mistresses and fianc\u00e9s were ignored)\n\n**parch**: The dataset defines family relations in this way...\n\nParent = mother, father\n\nChild = daughter, son, stepdaughter, stepson\n\nSome children travelled only with a nanny, therefore parch=0 for them.","84236345":"### **XGBOOST CLASSIFIER**","fc915207":"### **ADA BOOST CLASSIFIER**","19709f03":"### **DECISION TREE CLASSIFIER**","4c3d1ce8":"### **LOGISTIC REGRESSION**","702b473e":"### **CATBOOST CLASSIFIER**","fa720968":"### **Goal**\n\nIt is your job to predict if a passenger survived the sinking of the Titanic or not. \nFor each in the test set, you must predict a 0 or 1 value for the variable.\n\n### **Metric**\n\nYour score is the percentage of passengers you correctly predict. This is known as accuracy.\n\n### **Submission File Format**\n\nYou should submit a csv file with exactly 418 entries plus a header row. Your submission will show an error if you have extra columns (beyond PassengerId and Survived) or rows.\n\nThe file should have exactly 2 columns:\n\n* PassengerId (sorted in any order)\n* Survived (contains your binary predictions: 1 for survived, 0 for deceased)\n\n> PassengerId,Survived\n\n> 892,0\n\n> 893,1\n\n> 894,0\n\n> Etc.","283f813a":"### **RANDOM FOREST CLASSIFIER**","bdf2a274":"![Titanic Side (C)Atlantic Productions.jpg](attachment:8001cb7c-9c77-4b90-beea-d63539553067.jpg)","104952e7":"### **The Challenge**\n\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\n\nOn April 15, 1912, during her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren\u2019t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n\nIn this challenge, we ask you to build a predictive model that answers the question: \u201cwhat sorts of people were more likely to survive?\u201d using passenger data (ie name, age, gender, socio-economic class, etc).\n\nRecommended Tutorial\nWe highly recommend Alexis Cook\u2019s Titanic Tutorial that walks you through making your very first submission step by step.\n","2a334fa5":"### **STOCHASTIC GRADIENT DESCENT**","a2c1ac60":"### **What Data Will I Use in This Competition?**\n\nIn this competition, you\u2019ll gain access to two similar datasets that include passenger information like name, age, gender, socio-economic class, etc. One dataset is titled `train.csv` and the other is titled `test.csv`.\n\nTrain.csv will contain the details of a subset of the passengers on board (891 to be exact) and importantly, will reveal whether they survived or not, also known as the \u201cground truth\u201d.\n\nThe `test.csv` dataset contains similar information but does not disclose the \u201cground truth\u201d for each passenger. It\u2019s your job to predict these outcomes.\n\nUsing the patterns you find in the train.csv data, predict whether the other 418 passengers on board (found in test.csv) survived.\n\nCheck out the \u201cData\u201d tab to explore the datasets even further. Once you feel you\u2019ve created a competitive model, submit it to Kaggle to see where your model stands on our leaderboard against other Kagglers.\n\n"}}