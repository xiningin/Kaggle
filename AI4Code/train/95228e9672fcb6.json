{"cell_type":{"e9d1ad92":"code","62b3c397":"code","893db6d8":"code","5b71569b":"code","af642d16":"code","0376d3cc":"code","e192747b":"code","76f4403c":"code","7289ff29":"code","3815846d":"code","ac4ca58e":"code","425697f4":"code","e80ff13b":"code","c7a0a94b":"code","41a48ccf":"code","768038c7":"code","26bf64bf":"code","e231f716":"code","c0d89c46":"code","91cfbf86":"code","54d4431d":"code","4ff55ca6":"code","d9aed323":"code","e377b93c":"markdown","1fade51e":"markdown","2eab5e5b":"markdown","8c05cffa":"markdown","83c7adcd":"markdown","a5c92014":"markdown","b1831288":"markdown","6168a030":"markdown","f6227f0d":"markdown","8e177ac9":"markdown","95bf9dec":"markdown","3ad8b428":"markdown","e091c446":"markdown","e086391d":"markdown","b5f43aa4":"markdown","4509c61c":"markdown","8ebbcb73":"markdown","afcee125":"markdown","5e358765":"markdown","3ef8cc36":"markdown"},"source":{"e9d1ad92":"# from google.colab import files\n# files.upload() # Upload your Kaggle API Token \n# !mkdir ~\/.kaggle\n# !mv kaggle.json ~\/.kaggle\n# !chmod 600 ~\/.kaggle\/kaggle.json\n# !kaggle competitions download -c commonlitreadabilityprize\n# !unzip train.csv.zip","62b3c397":"# %%writefile setup.sh\n# export CUDA_HOME=\/usr\/local\/cuda-10.1\n# git clone https:\/\/github.com\/NVIDIA\/apex\n# cd apex\n# pip install -v --disable-pip-version-check --no-cache-dir .\/","893db6d8":"# %%capture\n# !sh setup.sh\n# !pip -q install madgrad\n# !pip -q install lit_nlp\n# !pip -q install transformers","5b71569b":"%%writefile setup.sh\ngit clone https:\/\/github.com\/NVIDIA\/apex\ncd apex\npip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" .\/\nrm -rf .\/apex","af642d16":"%%capture\n!sh setup.sh\n!pip -q install madgrad\n!pip -q install lit_nlp","0376d3cc":"import os\nimport gc\ngc.enable()\nimport math\nimport json\nimport time\nimport random\nimport multiprocessing\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm, trange\nfrom sklearn import model_selection\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import Parameter\nimport torch.optim as optim\nfrom torch.utils.data import (\n    Dataset, DataLoader,\n    SequentialSampler, RandomSampler\n)\n\ntry:\n    from apex import amp\n    APEX_INSTALLED = True\nexcept ImportError:\n    APEX_INSTALLED = False\n\nfrom madgrad import MADGRAD\n\ntry:\n    from torch.optim.swa_utils import (\n        AveragedModel, update_bn, SWALR\n    )\n    SWA_AVAILABLE = True\nexcept ImportError:\n    SWA_AVAILABLE = False\n\nimport transformers\nfrom transformers import (\n    WEIGHTS_NAME,\n    AdamW,\n    AutoConfig,\n    AutoModel,\n    AutoTokenizer,\n    get_cosine_schedule_with_warmup,\n    logging,\n    MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING,\n)\nlogging.set_verbosity_warning()\nlogging.set_verbosity_error()\n\ndef fix_all_seeds(seed):\n    np.random.seed(seed)\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\ndef optimal_num_of_loader_workers():\n    num_cpus = multiprocessing.cpu_count()\n    num_gpus = torch.cuda.device_count()\n    optimal_value = min(num_cpus, num_gpus*4) if num_gpus else num_cpus - 1\n    return optimal_value\n\nprint(f\"Apex AMP Installed :: {APEX_INSTALLED}\")\nprint(f\"SWA Available :: {SWA_AVAILABLE}\")\nMODEL_CONFIG_CLASSES = list(MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING.keys())\nMODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)","e192747b":"train = pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv', low_memory=False)\ndef create_folds(data, num_splits):\n    data[\"kfold\"] = -1\n    kf = model_selection.KFold(n_splits=num_splits, shuffle=True, random_state=2021)\n    for f, (t_, v_) in enumerate(kf.split(X=data)):\n        data.loc[v_, 'kfold'] = f\n    return data\ntrain = create_folds(train, num_splits=5)","76f4403c":"class Config:\n    # model\n    num_labels = 1\n    model_type = 'roberta'\n    model_name_or_path = 'roberta-base'\n    config_name = 'roberta-base'\n    fp16 = True if APEX_INSTALLED else False\n    fp16_opt_level = \"O1\"\n\n    # tokenizer\n    tokenizer_name = 'roberta-base'\n    max_seq_length = 250\n\n    # train\n    epochs = 10\n    train_batch_size = 24\n    eval_batch_size = 16\n\n    # optimizer\n    optimizer_type = 'MADGRAD'\n    learning_rate = 2e-5\n    weight_decay = 1e-5\n    epsilon = 1e-6\n    max_grad_norm = 1.0\n\n    # stochastic weight averaging\n    swa = True\n    swa_start = 7\n    swa_learning_rate = 1e-4\n    anneal_epochs=3 \n    anneal_strategy='cos'\n\n    # scheduler\n    decay_name = 'cosine-warmup'\n    warmup_ratio = 0.03\n\n    # logging\n    logging_steps = 10\n\n    # evaluate\n    output_dir = 'output'\n    seed = 2021","7289ff29":"class AverageMeter(object):\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n        self.max = 0\n        self.min = 1e5\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum \/ self.count\n        if val > self.max:\n            self.max = val\n        if val < self.min:\n            self.min = val","3815846d":"class DatasetRetriever(Dataset):\n    def __init__(self, data, tokenizer, max_len, is_test=False):\n        super(DatasetRetriever, self).__init__()\n        self.data = data\n        self.is_test = is_test\n        self.excerpts = self.data.excerpt.values.tolist()\n        if not self.is_test:\n            self.targets = self.data.target.values.tolist()\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, item):\n        excerpt = self.excerpts[item]\n        features = self.convert_examples_to_features(\n            excerpt, self.tokenizer, \n            self.max_len\n        )\n        features = {key : torch.tensor(value, dtype=torch.long) for key, value in features.items()}\n        if not self.is_test:\n            label = self.targets[item]\n            features['labels'] = torch.tensor(label, dtype=torch.double)\n        return features\n    \n    def convert_examples_to_features(self, example, tokenizer, max_len):\n        features = tokenizer.encode_plus(\n            example.replace('\\n', ''), \n            max_length=max_len, \n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n        )\n        return features","ac4ca58e":"class Model(nn.Module):\n    def __init__(\n        self, model_name, \n        config\n    ):\n        super(Model, self).__init__()\n        self.config = config\n        self.roberta = AutoModel.from_pretrained(\n            model_name, \n            config=config\n        )\n        self.dropout = nn.Dropout(p=0.2)\n        self.high_dropout = nn.Dropout(p=0.5)\n        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=1e-5)\n        self._init_weights(self.layer_norm)\n        self.regressor = nn.Linear(config.hidden_size, config.num_labels)\n        self._init_weights(self.regressor)\n        \n        weights_init = torch.zeros(config.num_hidden_layers + 1).float()\n        weights_init.data[:-1] = -3\n        self.layer_weights = torch.nn.Parameter(weights_init)\n \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n \n    def forward(\n        self, input_ids=None,\n        attention_mask=None, labels=None\n    ):\n        outputs = self.roberta(\n            input_ids,\n            attention_mask=attention_mask,\n        )\n        all_hidden_states = outputs[2]\n        \n        # weighted layer pooling\n        cls_embeddings = torch.stack(\n            [self.dropout(layer[:, 0]) for layer in all_hidden_states], \n            dim=2\n        )\n        cls_output = (\n            torch.softmax(self.layer_weights, dim=0) * cls_embeddings\n        ).sum(-1)\n        cls_output = self.layer_norm(cls_output)\n        \n        # multi-sample dropout\n        logits = torch.mean(\n            torch.stack(\n                [self.regressor(self.high_dropout(cls_output)) for _ in range(5)],\n                dim=0,\n            ),\n            dim=0,\n        )\n \n        # calculate loss\n        loss = None\n        if labels is not None:\n            # regression task\n            loss_fn = torch.nn.MSELoss()\n            logits = logits.view(-1).to(labels.dtype)\n            loss = torch.sqrt(loss_fn(logits, labels.view(-1)))\n        output = (logits,) + outputs[2:]\n        \n        del all_hidden_states, cls_embeddings\n        del cls_output, logits\n        gc.collect();\n        \n        return ((loss,) + output) if loss is not None else output","425697f4":"def get_optimizer_grouped_parameters(args, model):\n    no_decay = [\"bias\", \"LayerNorm.weight\"]\n    group1=['layer.0.','layer.1.','layer.2.','layer.3.']\n    group2=['layer.4.','layer.5.','layer.6.','layer.7.']    \n    group3=['layer.8.','layer.9.','layer.10.','layer.11.']\n    group_all=['layer.0.','layer.1.','layer.2.','layer.3.','layer.4.','layer.5.','layer.6.','layer.7.','layer.8.','layer.9.','layer.10.','layer.11.']\n    optimizer_grouped_parameters = [\n        {'params': [p for n, p in model.roberta.named_parameters() if not any(nd in n for nd in no_decay) and not any(nd in n for nd in group_all)],'weight_decay': args.weight_decay},\n        {'params': [p for n, p in model.roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group1)],'weight_decay': args.weight_decay, 'lr': args.learning_rate\/2.6},\n        {'params': [p for n, p in model.roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group2)],'weight_decay': args.weight_decay, 'lr': args.learning_rate},\n        {'params': [p for n, p in model.roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group3)],'weight_decay': args.weight_decay, 'lr': args.learning_rate*2.6},\n        {'params': [p for n, p in model.roberta.named_parameters() if any(nd in n for nd in no_decay) and not any(nd in n for nd in group_all)],'weight_decay': 0.0},\n        {'params': [p for n, p in model.roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group1)],'weight_decay': 0.0, 'lr': args.learning_rate\/2.6},\n        {'params': [p for n, p in model.roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group2)],'weight_decay': 0.0, 'lr': args.learning_rate},\n        {'params': [p for n, p in model.roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group3)],'weight_decay': 0.0, 'lr': args.learning_rate*2.6},\n        {'params': [p for n, p in model.named_parameters() if args.model_type not in n], 'lr':args.learning_rate*20, \"weight_decay\": 0.0},\n    ]\n    return optimizer_grouped_parameters","e80ff13b":"def make_model(args, output_attentions=False):\n    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name)\n    config = AutoConfig.from_pretrained(args.config_name)\n    config.update({'num_labels':args.num_labels})\n    config.update({\"output_hidden_states\":True})\n    if output_attentions:\n        config.update({\"output_attentions\":True})\n    model = Model(args.model_name_or_path, config=config)\n    return model, config, tokenizer\n\ndef make_optimizer(args, model):\n    optimizer_grouped_parameters = get_optimizer_grouped_parameters(args, model)\n    if args.optimizer_type == \"AdamW\":\n        optimizer = AdamW(\n            optimizer_grouped_parameters,\n            lr=args.learning_rate,\n            eps=args.epsilon,\n            correct_bias=not args.use_bertadam\n        )\n    else:\n        optimizer = MADGRAD(\n            optimizer_grouped_parameters,\n            lr=args.learning_rate,\n            eps=args.epsilon,\n            weight_decay=args.weight_decay\n        )\n    return optimizer\n\ndef make_scheduler(\n    args, optimizer, \n    num_warmup_steps, \n    num_training_steps\n):\n    if args.decay_name == \"cosine-warmup\":\n        scheduler = get_cosine_schedule_with_warmup(\n            optimizer,\n            num_warmup_steps=num_warmup_steps,\n            num_training_steps=num_training_steps\n        )\n    else:\n        scheduler = get_linear_schedule_with_warmup(\n            optimizer,\n            num_warmup_steps=num_warmup_steps,\n            num_training_steps=num_training_steps\n        )\n    return scheduler    \n\ndef make_loader(\n    args, data, \n    tokenizer, fold\n):\n    train_set, valid_set = data[data['kfold']!=fold], data[data['kfold']==fold]\n\n    train_dataset = DatasetRetriever(train_set, tokenizer, args.max_seq_length)\n    valid_dataset = DatasetRetriever(valid_set, tokenizer, args.max_seq_length)\n    print(f\"Num examples Train= {len(train_dataset)}, Num examples Valid={len(valid_dataset)}\")\n    \n    train_sampler = RandomSampler(train_dataset)\n    valid_sampler = SequentialSampler(valid_dataset)\n\n    train_dataloader = DataLoader(\n        train_dataset,\n        batch_size=args.train_batch_size,\n        sampler=train_sampler,\n        num_workers=optimal_num_of_loader_workers(),\n        pin_memory=True,\n        drop_last=False \n    )\n\n    valid_dataloader = DataLoader(\n        valid_dataset,\n        batch_size=args.eval_batch_size, \n        sampler=valid_sampler,\n        num_workers=optimal_num_of_loader_workers(),\n        pin_memory=True, \n        drop_last=False\n    )\n\n    return train_dataloader, valid_dataloader","c7a0a94b":"class Trainer:\n    def __init__(\n        self, model, tokenizer, \n        optimizer, scheduler, \n        swa_model=None, swa_scheduler=None\n    ):\n        self.model = model\n        self.tokenizer = tokenizer\n        self.optimizer = optimizer\n        self.scheduler = scheduler\n        self.swa_model = swa_model\n        self.swa_scheduler = swa_scheduler\n\n    def train(\n        self, args, \n        train_dataloader, \n        epoch, result_dict\n    ):\n        count = 0\n        losses = AverageMeter()\n        \n        self.model.zero_grad()\n        self.model.train()\n        \n        fix_all_seeds(args.seed)\n        for batch_idx, batch_data in enumerate(train_dataloader):\n            input_ids, attention_mask, labels = \\\n                batch_data['input_ids'], batch_data['attention_mask'], batch_data['labels']\n            input_ids, attention_mask, labels = \\\n                input_ids.cuda(), attention_mask.cuda(), labels.cuda()\n\n            outputs = self.model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                labels=labels\n            )\n\n            loss, logits = outputs[:2]\n            \n            if args.fp16:\n                with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n                    scaled_loss.backward()\n            else:\n                loss.backward()\n\n            count += labels.size(0)\n            losses.update(loss.item(), input_ids.size(0))\n\n            if args.fp16:\n                torch.nn.utils.clip_grad_norm_(amp.master_params(self.optimizer), args.max_grad_norm)\n            else:\n                torch.nn.utils.clip_grad_norm_(self.model.parameters(), args.max_grad_norm)\n\n            self.optimizer.step()\n            if not args.swa:\n                self.scheduler.step()\n            else:\n                if (epoch+1) < args.swa_start:\n                    self.scheduler.step()\n            self.optimizer.zero_grad()\n\n            if (batch_idx % args.logging_steps == 0) or (batch_idx+1)==len(train_dataloader):\n                _s = str(len(str(len(train_dataloader.sampler))))\n                ret = [\n                    ('Epoch: {:0>2} [{: >' + _s + '}\/{} ({: >3.0f}%)]').format(epoch, count, len(train_dataloader.sampler), 100 * count \/ len(train_dataloader.sampler)),\n                    'Train Loss: {: >4.5f}'.format(losses.avg),\n                ]\n                print(', '.join(ret))\n            \n        if args.swa and (epoch+1) >= args.swa_start:\n            self.swa_model.update_parameters(self.model)\n            self.swa_scheduler.step()\n\n        result_dict['train_loss'].append(losses.avg)\n        return result_dict","41a48ccf":"class Evaluator:\n    def __init__(self, model, swa_model):\n        self.model = model\n        self.swa_model = swa_model\n    \n    def save(self, result, output_dir):\n        with open(f'{output_dir}\/result_dict.json', 'w') as f:\n            f.write(json.dumps(result, sort_keys=True, indent=4, ensure_ascii=False))\n\n    def evaluate(self, valid_dataloader, epoch, result_dict):\n        losses = AverageMeter()\n        for batch_idx, batch_data in enumerate(valid_dataloader):\n            self.model = self.model.eval()\n            input_ids, attention_mask, labels = \\\n                batch_data['input_ids'], batch_data['attention_mask'], batch_data['labels']\n            input_ids, attention_mask, labels = \\\n                input_ids.cuda(), attention_mask.cuda(), labels.cuda()\n            with torch.no_grad():            \n                outputs = self.model(\n                    input_ids=input_ids,\n                    attention_mask=attention_mask,\n                    labels=labels\n                )\n                loss, logits = outputs[:2]\n                losses.update(loss.item(), input_ids.size(0))\n        print('----Validation Results Summary----')\n        print('Epoch: [{}] Valid Loss: {: >4.5f}'.format(epoch, losses.avg))\n        result_dict['val_loss'].append(losses.avg)        \n        return result_dict\n    \n    def swa_evaluate(self, valid_dataloader, epoch, result_dict):\n        losses = AverageMeter()\n        for batch_idx, batch_data in enumerate(valid_dataloader):\n            self.swa_model = self.swa_model.eval()\n            input_ids, attention_mask, labels = \\\n                batch_data['input_ids'], batch_data['attention_mask'], batch_data['labels']\n            input_ids, attention_mask, labels = \\\n                input_ids.cuda(), attention_mask.cuda(), labels.cuda()\n            with torch.no_grad():            \n                outputs = self.swa_model(\n                    input_ids=input_ids,\n                    attention_mask=attention_mask,\n                    labels=labels\n                )\n                loss, logits = outputs[:2]\n                losses.update(loss.item(), input_ids.size(0))\n        print('----SWA Validation Results Summary----')\n        print('Epoch: [{}] Valid Loss: {: >4.5f}'.format(epoch, losses.avg))\n        result_dict['swa_loss'].append(losses.avg)        \n        return result_dict","768038c7":"def init_training(args, data, fold):\n    fix_all_seeds(args.seed)\n    \n    if not os.path.exists(args.output_dir):\n        os.makedirs(args.output_dir)\n    \n    # model\n    model, model_config, tokenizer = make_model(args)\n    if torch.cuda.device_count() >= 1:\n        print('Model pushed to {} GPU(s), type {}.'.format(\n            torch.cuda.device_count(), \n            torch.cuda.get_device_name(0))\n        )\n        model = model.cuda() \n    else:\n        raise ValueError('CPU training is not supported')\n    \n    # data loaders for training and evaluation\n    train_dataloader, valid_dataloader = make_loader(args, data, tokenizer, fold)\n\n    # optimizer\n    optimizer = make_optimizer(args, model)\n\n    # scheduler\n    num_training_steps = len(train_dataloader) * args.epochs\n    if args.warmup_ratio > 0:\n        num_warmup_steps = int(args.warmup_ratio * num_training_steps)\n    else:\n        num_warmup_steps = 0\n    print(f\"Total Training Steps: {num_training_steps}, Total Warmup Steps: {num_warmup_steps}\")\n    scheduler = make_scheduler(args, optimizer, num_warmup_steps, num_training_steps)\n\n    # stochastic weight averaging\n    swa_model = AveragedModel(model)\n    swa_scheduler = SWALR(\n        optimizer, swa_lr=args.swa_learning_rate, \n        anneal_epochs=args.anneal_epochs, \n        anneal_strategy=args.anneal_strategy\n    )\n\n    print(f\"Total Training Steps: {num_training_steps}, Total Warmup Steps: {num_warmup_steps}, SWA Start Step: {args.swa_start}\")\n\n    # mixed precision training with NVIDIA Apex\n    if args.fp16:\n        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n    \n    result_dict = {\n        'epoch':[], \n        'train_loss': [], \n        'val_loss' : [], \n        'swa_loss': [],\n        'best_val_loss': np.inf\n    }\n\n    return (\n        model, model_config, tokenizer, optimizer, scheduler, \n        train_dataloader, valid_dataloader, result_dict,\n        swa_model, swa_scheduler\n    )","26bf64bf":"def run(data, fold):\n    args = Config()\n    model, model_config, tokenizer, optimizer, scheduler, train_dataloader, \\\n        valid_dataloader, result_dict, swa_model, swa_scheduler = init_training(args, data, fold)\n    \n    trainer = Trainer(model, tokenizer, optimizer, scheduler, swa_model, swa_scheduler)\n    evaluator = Evaluator(model, swa_model)\n\n    train_time_list = []\n    valid_time_list = []\n\n    for epoch in range(args.epochs):\n        result_dict['epoch'].append(epoch)\n\n        # Train\n        torch.cuda.synchronize()\n        tic1 = time.time()\n        result_dict = trainer.train(\n            args, train_dataloader, \n            epoch, result_dict\n        )\n        torch.cuda.synchronize()\n        tic2 = time.time() \n        train_time_list.append(tic2 - tic1)\n        \n        # Evaluate\n        torch.cuda.synchronize()\n        tic3 = time.time()\n        result_dict = evaluator.evaluate(\n            valid_dataloader, epoch, result_dict\n        )\n        torch.cuda.synchronize()\n        tic4 = time.time() \n        valid_time_list.append(tic4 - tic3)\n            \n        output_dir = os.path.join(args.output_dir, f\"checkpoint-fold-{fold}\")\n        if result_dict['val_loss'][-1] < result_dict['best_val_loss']:\n            print(\"{} Epoch, Best epoch was updated! Valid Loss: {: >4.5f}\".format(epoch, result_dict['val_loss'][-1]))\n            result_dict[\"best_val_loss\"] = result_dict['val_loss'][-1]        \n            \n            os.makedirs(output_dir, exist_ok=True)\n            torch.save(model.state_dict(), f\"{output_dir}\/pytorch_model.bin\")\n            model_config.save_pretrained(output_dir)\n            tokenizer.save_pretrained(output_dir)\n            print(f\"Saving model checkpoint to {output_dir}.\")\n    \n            #torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n            #torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n            #print(f\"Saving optimizer and scheduler states to {output_dir}.\")\n        print()\n        \n    if args.swa:\n        update_bn(train_dataloader, swa_model, device=torch.device('cuda'))\n    result_dict = evaluator.swa_evaluate(valid_dataloader, epoch, result_dict)\n    \n    evaluator.save(result_dict, output_dir)\n    torch.save(swa_model.state_dict(), f\"{output_dir}\/swa_pytorch_model.bin\")\n    \n    print()\n    print(f\"Total Training Time: {np.sum(train_time_list)}secs, Average Training Time per Epoch: {np.mean(train_time_list)}secs.\")\n    print(f\"Total Validation Time: {np.sum(valid_time_list)}secs, Average Validation Time per Epoch: {np.mean(valid_time_list)}secs.\")\n    \n    torch.cuda.empty_cache()\n    del trainer, evaluator\n    del model, model_config, tokenizer\n    del optimizer, scheduler\n    del train_dataloader, valid_dataloader, result_dict\n    del swa_model, swa_scheduler\n    gc.collect()","e231f716":"for fold in range(5):\n    print();print()\n    print('-'*50)\n    print(f'FOLD: {fold}')\n    print('-'*50)\n    run(train, fold)","c0d89c46":"import re\nfrom lit_nlp.api import dataset as lit_dataset\nfrom lit_nlp.api import types as lit_types\nfrom lit_nlp.api import model as lit_model\nfrom lit_nlp.lib import utils","91cfbf86":"# download snapshots\n! conda install -y gdown\n!gdown --id 1-RO8zoPGuX4HI1KsvH_Urjg6XxDf-Lq0\n!gdown --id 1-Xcg0lBn6yehLkQnzadrWoCdLg9IGH3-\n!gdown --id 1-UbtAiZsgCgo0SvnNa9uLzO6coPsODv7\n!gdown --id 1-Qm2BYi-STYXfJ6DAK1yb88T7RBMerCh","54d4431d":"class CommonLitData(lit_dataset.Dataset):\n    def __init__(self, df, fold, split='val'):\n        self._examples = self.load_datapoints(df, fold, split)\n    \n    def load_datapoints(self, df, fold, split):\n        if split == 'val':\n            df = df[df['kfold']==fold].reset_index()\n        else:\n            df = df[df['kfold']!=fold].reset_index()\n        return [{\n            \"excerpt\": row[\"excerpt\"],\n            \"label\": row[\"target\"],\n        } for _, row in df.iterrows()]\n\n    def spec(self):\n        return {\n            'excerpt': lit_types.TextSegment(),\n            'label': lit_types.RegressionScore(),\n        }","4ff55ca6":"class CommonLitModel(lit_model.Model):\n    compute_grads = False\n    def __init__(self, args):\n        self.model, self.config, self.tokenizer = make_model(args, output_attentions=True)\n        self.model.eval()\n\n    def max_minibatch_size(self):\n        return 8\n\n    def predict_minibatch(self, inputs):\n        encoded_input = self.tokenizer.batch_encode_plus(\n            [ex[\"excerpt\"].replace(\"\\n\", \"\") for ex in inputs],\n            add_special_tokens=True,\n            max_length=256,\n            padding=\"max_length\",\n            truncation=True,\n            return_attention_mask=True\n        )\n        encoded_input = {\n            key : torch.tensor(value, dtype=torch.long) for key, value in encoded_input.items()\n        }\n        \n        if torch.cuda.is_available():\n            self.model.cuda()\n            for tensor in encoded_input:\n                encoded_input[tensor] = encoded_input[tensor].cuda()\n    \n        with torch.set_grad_enabled(self.compute_grads):\n            outputs = self.model(encoded_input['input_ids'], encoded_input['attention_mask'])\n            if self.model.config.output_attentions:\n                logits, hidden_states, output_attentions = outputs[0], outputs[1], outputs[2]\n            else:\n                logits, hidden_states = outputs[0], outputs[1]\n\n        batched_outputs = {\n            \"input_ids\": encoded_input[\"input_ids\"],\n            \"ntok\": torch.sum(encoded_input[\"attention_mask\"], dim=1),\n            \"cls_emb\": hidden_states[-1][:, 0],\n            \"score\": torch.squeeze(logits, dim=-1)\n        }\n        \n        if self.model.config.output_attentions:\n            assert len(output_attentions) == self.model.config.num_hidden_layers\n            for i, layer_attention in enumerate(output_attentions[-2:]):\n                batched_outputs[f\"layer_{i}\/attention\"] = layer_attention\n\n        if self.compute_grads:\n            scalar_pred_for_gradients = batched_outputs[\"score\"]\n            batched_outputs[\"input_emb_grad\"] = torch.autograd.grad(\n                scalar_pred_for_gradients,\n                hidden_states[0],\n                grad_outputs=torch.ones_like(scalar_pred_for_gradients)\n            )[0]\n\n        detached_outputs = {k: v.cpu().numpy() for k, v in batched_outputs.items()}\n        for output in utils.unbatch_preds(detached_outputs):\n            ntok = output.pop(\"ntok\")\n            output[\"tokens\"] = self.tokenizer.convert_ids_to_tokens(\n                output.pop(\"input_ids\")[1:ntok - 1]\n            )\n            if self.compute_grads:\n                output[\"token_grad_sentence\"] = output[\"input_emb_grad\"][:ntok]\n            if self.model.config.output_attentions:\n                for key in output:\n                    if not re.match(r\"layer_(\\d+)\/attention\", key):\n                        continue\n                    output[key] = output[key][:, :ntok, :ntok].transpose((0, 2, 1))\n                    output[key] = output[key].copy()\n            yield output\n\n    def input_spec(self) -> lit_types.Spec:\n        return {\n            \"excerpt\": lit_types.TextSegment(),\n            \"label\": lit_types.RegressionScore()\n        }\n\n    def output_spec(self) -> lit_types.Spec:\n        ret = {\n            \"tokens\": lit_types.Tokens(),\n            \"score\": lit_types.RegressionScore(parent=\"label\"),\n            \"cls_emb\": lit_types.Embeddings()\n        }\n        if self.compute_grads:\n            ret[\"token_grad_sentence\"] = lit_types.TokenGradients(\n                align=\"tokens\"\n            )\n        if self.model.config.output_attentions:\n            for i in range(2): # self.model.config.num_hidden_layers\n                ret[f\"layer_{i}\/attention\"] = lit_types.AttentionHeads(\n                    align_in=\"tokens\", align_out=\"tokens\")\n        return ret","d9aed323":"def create_model(path):\n    args = Config()\n    args.config_name = path\n    args.model_name_or_path = path\n    args.tokenizer_name = path\n    return CommonLitModel(args)\n\ndatasets = {\n    'validation_0':CommonLitData(train, fold=0, split='val'),\n    'validation_1':CommonLitData(train, fold=1, split='val'),\n    'validation_2':CommonLitData(train, fold=2, split='val'),\n    'validation_3':CommonLitData(train, fold=3, split='val'),\n    'validation_4':CommonLitData(train, fold=4, split='val'),\n}\n\nmodels = {\n    'model_0':create_model('output\/checkpoint-fold-0\/'),\n    'model_1':create_model('output\/checkpoint-fold-1\/'),\n    'model_2':create_model('output\/checkpoint-fold-2\/'),\n    'model_3':create_model('output\/checkpoint-fold-3\/'),\n    'model_4':create_model('output\/checkpoint-fold-4\/'),\n}\n\n\nfrom lit_nlp import notebook\nwidget = notebook.LitWidget(models, datasets, height=800)\n# widget.render() -->> uncomment this line to render","e377b93c":"<font color='#3498DB'><h3>Run<\/h3><\/font>\n\nNow we load our 5-Fold Validation Data and Models, pass that to `notebook.LitWidget` and call widget.render(). \n\nThis in return will open an interface like below,","1fade51e":"<font color='#3498DB'><h3>Import Dependencies<\/h3><\/font>\n\nHere, we will import the required dependencies and few utility functions. The `optimal_num_of_loader_workers` will find the optimal number of workers for our dataloader and `fix_all_seeds` will be doing the job of reproducibility. ","2eab5e5b":"<font color='#3498DB'><h4>Kaggle Setup<\/h4><\/font>\n\n*Note: Installing NVIDIA-Apex will take some time.*","8c05cffa":"<font color='#3498DB'><h3>Trainer<\/h3><\/font>\n\nHere we define our trainer class which will be the main engine. Below we do the necessary changes required for training support with swa and apex-amp.","83c7adcd":"<font color='#3498DB'><h3>Main<\/h3><\/font>\n\n**Modules, groups, and workspaces** form the building blocks of LIT. Modules are discrete windows in which you can perform a specific set of tasks or analyses. Workspaces display combinations of modules known as groups, so you can view different visualizations and interpretability methods side-by-side.\n\n![img1](main_screen.PNG)\n\nLIT is divided into two workspaces - a Main workspace in the upper half of the interface, and a Group-based workspace in the lower half.\n\nThe Main workspace contains core modules that play a role in many analyses. By default, these include:\n\n - **Embeddings** - explore UMAP and TSNE embeddings from your model.\n - **Data Table** - explore, navigate, and make selections from your dataset.\n - **Datapoint Editor** - deep-dive into individual examples from your dataset.\n - **Slice Editor** - create and manage slices of interest from your dataset through your LIT session.\n \n<font color='#3498DB'><h3>Models and Datasets<\/h3><\/font>\n![img2](models.PNG)\nAt the very top, you\u2019ll see the LIT toolbar. Here, you can quickly check which models have been loaded, configure LIT, or share a URL to your session. Below that is a toolbar which makes it easier to perform actions applied across all of LIT. Here you can:\n\n - Select data points by relationship, or by slice.\n - Choose a feature to color data points, across all modules.\n - Track the datapoint you\u2019re looking at, navigate to the next, mark a datapoint as a favorite, or clear your selection.\n - Select the active models and dataset, including multiple models to compare.\n\n![img3](validation_data.PNG)\n\nThe footer at the very bottom of the interface will display any error messages. \n\n<font color='#3498DB'><h3>Explanations<\/h3><\/font>\n![img4](lime_attention.PNG)\nIn the Group-based workspace, modules that offer related insights are organized together under tabs. By default, LIT offers a few default groups based on common analysis workflows: performance, predictions, explanations, and counterfactuals.\n\n - Use the Performance group to compare the performance of models across the entire dataset, or on individual slices.\n - Explore model results on individual data points in the Predictions group.\n - Investigate salience maps and attention for different data points in the Explanations group.\n - Generate data points using automated generators in the Counterfactuals group, and evaluate your model on them instantly.","a5c92014":"<font color='#3498DB'><h3>Run<\/h3><\/font>\n\nThis is our main function that will stitch everything together i.e. training, evaluating, saving the best model, updating bn for our swa model, etc.\n\n*Note: Training takes ~1 hr to complete on Tesla P100-PCIE-16GB provided by Kaggle and Colab.*","b1831288":"<font color='#3498DB'><h3>Model<\/h3><\/font>\n\nHere we define our model. The model uses strategies of weighted layer pooling on cls embeddings from each layer, multi-sample dropout, layer initialization which I have explained in other kernels. ","6168a030":"<font color='#3498DB'><h3>Average Meter<\/h3><\/font>\n\nWill help us in logging metrics.","f6227f0d":"<font color='#3498DB'><h3>Grouped Optimizer Parameters & LLRD<\/h3><\/font>\n\nWe will be using Grouped-LLRD (Layer Wise Learning Rate Decay) since from my experiments this shows better peformance and generalization than simple LLRD.","8e177ac9":"<font color='#3498DB'><h3>Training Config<\/h3><\/font>\n\nThe Config class is where we define our training hyperparameters, output path, etc. We define model, tokenizer, optimizer, scheduler, swa, and training configurations.","95bf9dec":"<font color='#3498DB'><h2>Interpreting Transformers with LIT<\/h2><\/font>\n\nHere we will be implementing model interpretibility using LIT alongwith understanding of how each component works.\n\n<font color='#3498DB'><h3>How it Works?<\/h3><\/font>\nThe LitWidget object constructor takes a dict mapping model names to model objects, and a dict mapping dataset names to dataset objects. Those will be the datasets and models displayed in LIT. \n\nIt also optionally takes in a height parameter for how tall to render the LIT UI in pixels (it defaults to 1000 pixels). Running the constructor will cause the LIT server to be started in the background, loading the models and datasets and enabling the UI to be served.\n\nRender the LIT UI in an output cell by calling the render method on the LitWidget object. The LIT UI can be rendered multiple times in separate cells if desired. The widget also contains a stop method to shut down the LIT server.\n\n<font color='#E74C3C'><h5>Thing to Note<\/h5><\/font>\nLIT won't work on Kaggle [due to this issue](https:\/\/www.kaggle.com\/product-feedback\/89671). Running LIT on Kaggle notebook throws an error **kkb-production.jupyter-proxy.kaggle.net is taking too long to respond.** Kaggle has disabled this feature because it caused a large slowdown to notebook startup time.\n\nBut below code is platform-independent and will run on Google Colab, local or else. I used Colab and will be sharing snapshots here.\n\n<font color='#3498DB'><h3>Import Dependencies<\/h3><\/font>\nImport LIT specific dependencies.","3ad8b428":"<font color='#3498DB'><h3>Evaluator<\/h3><\/font>\n\nHere we define our evaluator class which we will use to evaluate our model performance and save results.\n\n*Note: We have two evaluate functions. The first is for evaluating with the original model and second is for evaluating with the swa_model after the training is completed.*","e091c446":"<font color='#3498DB'><h3>Data & Train-Validation Split<\/h3><\/font>\n\nHere we load our data and will be using the same old 5-KFold split with `random_state=2021`.","e086391d":"<font color='#3498DB'><h2>Extras<\/h2><\/font>\n\nThis section is dedicated for interpreting LR's during Stochastic Weight Averaging. Run below code to make better sense of how Learning Rate changes when using SWA,\n\n``` python\nmodel, model_config, \noptimizer = make_optimizer(args, model)\nscheduler = make_scheduler(args, optimizer, 1, 10)\nswa_scheduler = SWALR(optimizer, swa_lr=1e-6, anneal_epochs=3, anneal_strategy='cos')\nswa_start = 7\nfor epoch in range(10):\n    optimizer.step()\n    if (epoch+1) >= swa_start:\n        print(\"starting swa\", i)\n        swa_scheduler.step()\n    \n    if (epoch+1) < swa_start:\n        print('using simple scheduler')\n        scheduler.step()\n    print(optimizer.param_groups[0]['lr'])\n```\n\n<font color='#3498DB'><h2>Thanks and Plase Do Upvote!<\/h2><\/font>","b5f43aa4":"<font color='#3498DB'><h3>Initialize Training<\/h3><\/font>\n\nWe initialize all our training components using the below method. It will help us in initializing model, scheduler, optimizer, train and eval loader, mixed precision training, swa and our results dict.","4509c61c":"<font color='#2980B9'><center><h2>SWA-LP &  Interpreting Transformer Interactively <\/h2><\/center><\/font>\n\n<br>\n\n<font color='#3498DB'><h2>Introduction<\/h2><\/font>\nSWA-LP stands for Stochastic Weight Averaging for low precision training. SWA-LP can match the performance of full-precision training, even with all numbers quantized down to 8 bits. \n\nThe notebook is an implementation of the Stochastic Weight Averaging technique with NVIDIA Apex on transformers using PyTorch. The notebook also implements how to interactively interpret Transformers using LIT (Language Interpretability Tool) a platform for NLP model understanding.\n\n\n<font color='#3498DB'><h2>Idea<\/h2><\/font>\nInspired by ideas of my discussions and kernels and due to minimal resources, this kernel will explain how to Fintune Transformers with SWA and Apex AMP. It also shows how to connect various strategies like Weighted Layer Pooling, MADGRAD Optimizer, Grouped LLRD, etc.\n\nWe will also see how we can implement visualizations for salience maps, attention, as well as aggregate analysis including metrics, embedding spaces, and flexible slicing for interpreting transformer models.\n\n*Note: We will be using competition data.*\n\n<font color='#3498DB'><h2>Overview<\/h2><\/font>\nBefore we jump into the code let's have an understanding of some techniques and how they work. Even having atleast a high level idea behind each technique will help to better understand the code.\n\n<font color='#3498DB'><h3>Stochastic Weight Averaging<\/h3><\/font>\n**Paper**: [Averaging Weights Leads to Wider Optima and Better Generalization](https:\/\/arxiv.org\/pdf\/1803.05407.pdf)  \n**Blog**: [PyTorch 1.6 now includes Stochastic Weight Averaging](https:\/\/pytorch.org\/blog\/pytorch-1.6-now-includes-stochastic-weight-averaging\/)\n\nSWA produces an ensemble by combining weights of the same network at different stages of training and then uses this model with combined weights to make predictions. \n![swa](https:\/\/miro.medium.com\/max\/1766\/1*_USiR_z8PKaDuIcAs9xomw.png)\nThere are two important ingredients that make SWA work. \n\n- First, SWA uses a modified learning rate schedule so that SGD (or other optimizers such as Adam) continues to bounce around the optimum and explore diverse models instead of simply converging to a single solution. For example, we can use the standard decaying learning rate strategy for the first 75% of training time and then set the learning rate to a reasonably high constant value for the remaining 25% of the time (see Figure below). \n![swa_training](https:\/\/pytorch.org\/assets\/images\/nswapytorch2.jpg)\n- The second ingredient is to take an average of the weights (typically an equal average) of the networks traversed by SGD. For example, we can maintain a running average of the weights obtained at the end of every epoch within the last 25% of training time. After training is complete, we then set the weights of the network to the computed SWA averages.\n\n- Another important detail is the batch normalization. Batch normalization layers compute running statistics of activations during training. Note that the SWA averages of the weights are never used to make predictions during training. So the batch normalization layers do not have the activation statistics computed at the end of training. We can compute these statistics by doing a single forward pass on the train data with the SWA model.  \n![swa2](https:\/\/miro.medium.com\/max\/502\/1*Afu2bqxzC6p1BpIRTDWJtg.png)    \n\nThus we only need to train one model, and store two models in memory during training. For prediction, we only need the running average model.\n\n<font color='#3498DB'><h3>MADGRAD Optimizer<\/h3><\/font>\n**Paper**: [Adaptivity without Compromise: A Momentumized, Adaptive, Dual Averaged Gradient Method for Stochastic Optimization](https:\/\/arxiv.org\/abs\/2101.11075)\n\nMADGRAD is a novel optimization method in the family of AdaGrad adaptive gradient methods. MADGRAD shows excellent performance on deep learning optimization problems from multiple fields, including classification and image-to-image tasks in vision, and recurrent and bidirectionally-masked models in natural language processing.\n![madgrad](https:\/\/github.com\/facebookresearch\/madgrad\/raw\/master\/figures\/nlp.png?raw=true)\nFor each of these tasks, MADGRAD matches or outperforms both SGD and ADAM in test set performance, even on problems for which adaptive methods normally perform poorly.\n\n<font color='#3498DB'><h4>Things to Note<\/h4><\/font>\n - You may need to use a lower weight decay than you are accustomed to. Often 0.\n - You should do a full learning rate sweep as the optimal learning rate will be different from SGD or Adam. On NLP models gradient clipping also helped.\n\n<font color='#3498DB'><h3>Language Interpretability Tool (LIT)<\/h3><\/font>\n**Paper**: [The Language Interpretability Tool: Extensible, Interactive Visualizations and Analysis for NLP Models](https:\/\/www.aclweb.org\/anthology\/2020.emnlp-demos.15.pdf)    \n**Blog**: [The Language Interpretability Tool (LIT): Interactive Exploration and Analysis of NLP Models](https:\/\/ai.googleblog.com\/2020\/11\/the-language-interpretability-tool-lit.html)    \n**Official Page**: [Language Interpretability Tool](https:\/\/pair-code.github.io\/lit\/)  \n**Examples**: [GitHub](https:\/\/github.com\/PAIR-code\/lit\/tree\/main\/lit_nlp\/examples)\n\nLIT is an open-source platform for visualization and understanding of NLP models. LIT contains many built-in capabilities but is also customizable, with the ability to add custom interpretability techniques, metrics calculations, counterfactual generators, visualizations, and more. \n\n| Built-in capabilities | Supported task types | Framework agnostic |\n| :-: | :-: | :-: |\n| Salience maps | Classification | TensorFlow 1.x |\n| Attention visualization | Regression | TensorFlow 2.x |\n| Metrics calculations | Text generation \/ seq2seq | PyTorch |\n| Counterfactual generation | Masked language models | Notebook compatibility |\n| Model and datapoint comparison | Span labeling | Custom inference code |\n| Embedding visualization | Multi-headed models | Remote Procedure Calls |\n| And more... | And more... | And more... |\n\nLIT can be run as a standalone server, or inside of python notebook environments such as Colab and Jupyter.\n\n![lit](https:\/\/pair-code.github.io\/lit\/assets\/images\/lit-tweet.gif)\n\n<font color='#3498DB'><h3>NVIDIA Apex - AMP<\/h3><\/font>\n - In [Speeding up Transformer w\/ Optimization Strategies](https:\/\/www.kaggle.com\/rhtsingh\/speeding-up-transformer-w-optimization-strategies) notebook.\n\n<font color='#3498DB'><h3>Weighted Layers Pooling<\/h3><\/font>\n - In [Utilizing Transformer Representations Efficiently](https:\/\/www.kaggle.com\/rhtsingh\/utilizing-transformer-representations-efficiently) notebook.\n\n<font color='#3498DB'><h3>Grouped Layerwise Learning Rate Decay<\/h3><\/font>\n - In [Guide to HuggingFace Schedulers & Differential LRs](https:\/\/www.kaggle.com\/rhtsingh\/guide-to-huggingface-schedulers-differential-lrs) notebook.\n \n \n**That's pretty much everything that we need to know. We will learn about the implementation and other nitty gritty details while coding. Rest if there's still any doubt remaining, we can discuss in the comments.**\n\n<font color='#3498DB'><h2>Code<\/h2><\/font>\n\n<font color='#3498DB'><h3>Install Dependencies<\/h3><\/font>\nFirst, we will be doing the necessary setup. We will be installing NVIDIA Apex API, MADGRAD Optimizer, and Language Interpretability Tool. Below one can find the preliminary setup command for both Kaggle and Google Colab. \n\n*Note: If you're using Google Colab make sure to download kaggle.json by creating a new API token from your account.*\n\n<font color='#3498DB'><h4>Colab Setup<\/h4><\/font>","8ebbcb73":"<font color='#3498DB'><h3>Implement Model<\/h3><\/font>\nThis is our core engine and here we inherit lit_model.Model class. We override the predict_minibatch method and as a mainstream transformer, pipeline defines the model, load model weights, tokenize examples to features, and pass that to our model. Then we will add our outputs of cls_embeddings, attentions, and gradients.\n\n*Note 1: This might seem a bit complicated at first sight and took quite some time for me as well to understand completely. So if it is difficult, break down everything into pieces and run sequentially. That will help in understanding better.*\n\n*Note 2: The implementation for Saliency Maps and Integrated Gradients isn't complete so I've set compute grads to True. Will be completing it very soon.*","afcee125":"<font color='#3498DB'><h3>Dataset Retriever<\/h3><\/font>\n\nThe dataset retriever will store the samples and their corresponding labels. When called it will process the samples into features i.e. input_ids, attention_mask, and covert that to tensors for our model inputs.","5e358765":"<font color='#3498DB'><h3>Utilities<\/h3><\/font>\n\nBelow we define our utility functions which will initialize our different components.","3ef8cc36":"<font color='#3498DB'><h3>Implement Dataset<\/h3><\/font>\nWe will inherit `lit_dataset.Dataset` class for this. This class will store samples so that our model can fetch to do further preprocessing, prediction etc."}}