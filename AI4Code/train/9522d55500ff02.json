{"cell_type":{"95b777ac":"code","4d728086":"code","c9efb009":"code","2e1559d9":"code","db40e2ca":"code","c47910c4":"code","26b150fd":"code","1b61f108":"code","8fa712d8":"code","73577ac4":"code","ca7c8664":"code","d01cbab7":"code","b55f1179":"code","655df24a":"code","81efcdb1":"code","8cb53d68":"code","a7d62a90":"markdown","46df8a06":"markdown","b62cc69e":"markdown","18e3d1a1":"markdown","eb22649e":"markdown","2c229d18":"markdown","8a1e8285":"markdown","62ad6ec0":"markdown","b66fd7e5":"markdown","b31ec1b6":"markdown","b5d3d0f0":"markdown","23e01e8f":"markdown","ca02ace3":"markdown","d9d4d152":"markdown","8fa1b43e":"markdown","f8b63e4e":"markdown"},"source":{"95b777ac":"!pip install alibi","4d728086":"import tensorflow as tf\ntf.get_logger().setLevel(40) # suppress deprecation messages\ntf.compat.v1.disable_v2_behavior() # disable TF2 behaviour as alibi code still relies on TF1 constructs\nfrom tensorflow.keras.layers import Conv2D, Dense, Dropout, Flatten, MaxPooling2D, Input\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.utils import to_categorical\nimport matplotlib\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom time import time\nfrom alibi.explainers import CounterFactual\nprint('TF version: ', tf.__version__)\nprint('Eager execution enabled: ', tf.executing_eagerly()) # False","c9efb009":"(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\nprint('x_train shape:', x_train.shape, 'y_train shape:', y_train.shape)\nplt.imshow(x_test[1]); #just an example of the dataset","2e1559d9":"x_train = x_train.astype('float32') \/ 255\nx_test = x_test.astype('float32') \/ 255\nx_train = np.reshape(x_train, x_train.shape + (1,))\nx_test = np.reshape(x_test, x_test.shape + (1,))\nprint('x_train shape:', x_train.shape, 'x_test shape:', x_test.shape)\ny_train = to_categorical(y_train)\ny_test = to_categorical(y_test)\nprint('y_train shape:', y_train.shape, 'y_test shape:', y_test.shape)","db40e2ca":"xmin, xmax = -.5, .5\nx_train = ((x_train - x_train.min()) \/ (x_train.max() - x_train.min())) * (xmax - xmin) + xmin\nx_test = ((x_test - x_test.min()) \/ (x_test.max() - x_test.min())) * (xmax - xmin) + xmin","c47910c4":"def cnn_model():\n    x_in = Input(shape=(28, 28, 1))\n    x = Conv2D(filters=64, kernel_size=2, padding='same', activation='relu')(x_in)\n    x = MaxPooling2D(pool_size=2)(x)\n    x = Dropout(0.3)(x)\n\n    x = Conv2D(filters=32, kernel_size=2, padding='same', activation='relu')(x)\n    x = MaxPooling2D(pool_size=2)(x)\n    x = Dropout(0.3)(x)\n\n    x = Flatten()(x)\n    x = Dense(256, activation='relu')(x)\n    x = Dropout(0.5)(x)\n    x_out = Dense(10, activation='softmax')(x)\n\n    cnn = Model(inputs=x_in, outputs=x_out)\n    cnn.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n    return cnn","26b150fd":"cnn = cnn_model()\ncnn.summary()\ncnn.fit(x_train, y_train, batch_size=64, epochs=3, verbose=0)\ncnn.save('mnist_cnn.h5')","1b61f108":"cnn = load_model('mnist_cnn.h5')\nscore = cnn.evaluate(x_test, y_test, verbose=0)\nprint('Test accuracy: ', score[1])","8fa712d8":"X = x_test[4].reshape((1,) + x_test[0].shape)\nplt.imshow(X.reshape(28, 28));\nprint('Prediction on instance to be explained: {}'.format([np.argmax(cnn.predict(X))]))\nprint('Prediction probabilities for each class on the instance: {}'.format(cnn.predict(X)))","73577ac4":"shape = (1,) + x_train.shape[1:] #reshaping according to batch dimension, which is here 1, so 1 column is added.\ntarget_proba = 1.0\ntol = 0.01 # tolerance - want counterfactuals with p(class)>0.99\ntarget_class = 'other' # any class other than the class of the test instance (here 7).\nmax_iter = 1000\nlam_init = 1e-1\nmax_lam_steps = 10\nlearning_rate_init = 0.1\nfeature_range = (x_train.min(),x_train.max())","ca7c8664":"# initialize explainer\ncf = CounterFactual(cnn, shape=shape, target_proba=target_proba, tol=tol,\n                    target_class=target_class, max_iter=max_iter, lam_init=lam_init,\n                    max_lam_steps=max_lam_steps, learning_rate_init=learning_rate_init,\n                    feature_range=feature_range)\n\nstart_time = time()\nexplanation = cf.explain(X)\nprint('Explanation took {:.3f} sec'.format(time() - start_time))","d01cbab7":"pred_class = explanation.cf['class']\nproba = explanation.cf['proba'][0][pred_class]\n\nprint(f'Counterfactual prediction: {pred_class} with probability {proba}')\nplt.imshow(explanation.cf['X'].reshape(28, 28));","b55f1179":"n_cfs = np.array([len(explanation.all[iter_cf]) for iter_cf in range(max_lam_steps)])\nexamples = {}\nfor ix, n in enumerate(n_cfs):\n    if n>0:\n        examples[ix] = {'ix': ix, 'lambda': explanation.all[ix][0]['lambda'],\n                       'X': explanation.all[ix][0]['X']}\ncolumns = len(examples) + 1\nrows = 1\n\nfig = plt.figure(figsize=(16,6))\n\nfor i, key in enumerate(examples.keys()):\n    ax = plt.subplot(rows, columns, i+1)\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n    plt.imshow(examples[key]['X'].reshape(28,28))\n    plt.title(f'Iteration: {key}')","655df24a":"target_class = 4\ncf = CounterFactual(cnn, shape=shape, target_proba=target_proba, tol=tol,\n                    target_class=target_class, max_iter=max_iter, lam_init=lam_init,\n                    max_lam_steps=max_lam_steps, learning_rate_init=learning_rate_init,\n                    feature_range=feature_range)\n\nexplanation = start_time = time()\nexplanation = cf.explain(X)\nprint('Explanation took {:.3f} sec'.format(time() - start_time))","81efcdb1":"pred_class = explanation.cf['class']\nproba = explanation.cf['proba'][0][pred_class]\n\nprint(f'Counterfactual prediction: {pred_class} with probability {proba}')\nplt.imshow(explanation.cf['X'].reshape(28, 28));","8cb53d68":"plt.imshow((explanation.cf['X'] - X).reshape(28, 28));","a7d62a90":"The above image shows the difference between the counterfactual and the original instance. This helps in concluding that the counterfactual is removing and adding to some parts of the shirt image with some changes in the shape to result in a prediction of 4. For example, the length of the sleeves is longer on coat, thus the pixels at that point are changed accordingly to make them look longer. Thus, we know that it is working correctly.","46df8a06":"Counterfactuals provide the solution or changes in prediction to \u201cWhat if\u201d cases by showing feature-perturbed versions of the same cases. If X is an independent variable and Y is a dependent variable, counterfactual shows the effect on Y due to small changes in X. Also, it helps to calculate,what changes need to be done in X if the outcome Y is changed to Y'.","b62cc69e":"Since counterfactual is a local explanation method, we focus on one particular instance i.e. index 4 of test data which is an image of a shirt (class 6) as shown below.","18e3d1a1":"Calculate the error for evaluation of model","eb22649e":"Run counterfactual","2c229d18":"A counterfactual explanation of a prediction describes the smallest change to the feature values that changes the prediction to a predefined output.\nHere, the counterfactual starting from a class 6 which is shirt, makes perturbations in the pixels and moves towards the closest class in the data, that is in this case class 0 (t-shirt\/top). As required, the probability of the returned counterfactual is 0.99 which is quite high.","8a1e8285":"Here, we have explicitly specified the output class and the counterfactual perturbations are made accordingly.","62ad6ec0":"Since the accuracy is 98%, the model is good.","b66fd7e5":"Generate counterfactuals","b31ec1b6":"Scale, reshape and categorize(one-hot encoding) ","b5d3d0f0":"## Counterfactual instances on Fashion MNIST","23e01e8f":"Counterfactual parameters -\n\ntarget_proba: This is the probability required for the returned target class after applying counterfactual. \n\ntol: the tolerance within the target_proba, this is used to specify a range of acceptable predicted probability values for flexibility in target_proba.\n\ntarget_class: desired target class for the returned counterfactual instance. \n\nfeature_range: feature-wise min and max values for the perturbed instance.\n\nmax_iter, lam_init, max_lam_steps, learning_rate_init are all mathematical parameters used for the method.\n\nlearning_rate_init: initial learning rate\n\nlam_init: initial value of the hyperparameter \u03bb. \n\nmax_lam_steps: the number of steps (outer loops) to search for with a different value of \u03bb.","ca02ace3":"Now, we have specified the target class as 4 which means that for the outcome of model prediction to be 4 (coat), what are the changes in features(pixels here) required.","d9d4d152":"Load MNIST data","8fa1b43e":"The transformation of shirt to t-shirt over several iterations is shown in the above image. The image is not very clear but it can be seen how the counterfactual perturbations are generated to change the prediction class.","f8b63e4e":"Define and train CNN model"}}