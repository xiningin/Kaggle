{"cell_type":{"1f1c2d49":"code","1c743f02":"code","d9ab6a1a":"code","6e8337bb":"code","085e84d5":"code","95a22f88":"code","5d3f7bbc":"code","f02a8b37":"code","cbef2047":"code","dfb84c70":"code","aaa36cdb":"code","a878e841":"code","dc59cec0":"code","b475f984":"code","294893c3":"code","6bdc003e":"code","48292656":"code","d6a6a471":"code","af2b93a1":"code","a59325ed":"code","2f7005e5":"code","8a11118d":"code","015d74e3":"code","c47513ab":"code","07787e62":"code","271cd82e":"code","348872a5":"code","3f7e378a":"code","5eed87db":"code","f7ad1356":"code","237c6985":"code","02206c08":"code","e809b5c8":"code","b368d901":"code","c608f295":"code","491aed6b":"code","fcf186bb":"code","1c513ff9":"code","bb1d4a48":"code","b6c00f4d":"code","e40e50ce":"code","346886c9":"code","7af3bc33":"code","b29818b6":"code","258257a8":"code","5af75213":"code","7fd80517":"code","c45aaf8f":"code","7ffa6036":"code","30ce1261":"code","584d5d31":"code","88878629":"code","49176669":"code","9a159ade":"code","a402f508":"code","2a493a3d":"code","6e7f61a1":"code","b9beb858":"code","9095537d":"code","a3b2a9f9":"code","7834eb97":"code","4a1ade03":"code","87abc6dc":"code","c4c420c6":"code","0c81f6e3":"code","9fcfe7e2":"code","24779c0d":"code","6235c205":"code","e6253ff0":"code","abea635f":"code","2f7f43d4":"code","3365567d":"code","34e85980":"code","3817742c":"code","c9e08d01":"code","f6e7be24":"code","4e82ee08":"code","a94f907d":"code","11971aad":"code","fcaa586d":"code","4ec92a6d":"code","d2b6a5f1":"code","d54490f1":"code","f47a2722":"code","e7ff9fdd":"code","8fc3a454":"code","4ba2e0cf":"code","d1aeab32":"code","e0f56423":"code","1483f595":"code","bd8976c9":"code","f328d3cd":"code","4cadbffe":"code","78256be0":"code","df77bddd":"markdown","954dfe22":"markdown","b2e51879":"markdown","809f4647":"markdown","2e13c312":"markdown","7be2870c":"markdown","00285024":"markdown","a82c545c":"markdown","dad7a1f0":"markdown","ca054e56":"markdown","3473e491":"markdown","1d0da7b3":"markdown","6985c4ba":"markdown","341cbe5d":"markdown","34cbcb87":"markdown","19a5e103":"markdown","86076cd0":"markdown","42e626fd":"markdown","17b8afc2":"markdown","fdc176d5":"markdown","bdd4c134":"markdown","bcc4dbf8":"markdown","54c000e5":"markdown","28999303":"markdown","a78cbf49":"markdown","87944178":"markdown","ec66de11":"markdown","97989960":"markdown","96f9c488":"markdown","57e08592":"markdown","87600339":"markdown","727ba98d":"markdown","58089838":"markdown","882390ed":"markdown","80269845":"markdown","187620a6":"markdown","39d279e9":"markdown","fcdbfa05":"markdown","cb304a6f":"markdown","96d5a3ae":"markdown","ab1b2503":"markdown","352b6794":"markdown","e398a051":"markdown","7f79da47":"markdown","5a6e0dab":"markdown","0557e911":"markdown"},"source":{"1f1c2d49":"\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport scipy.stats as stat\nfrom sklearn.svm import SVC\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import PowerTransformer0\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\n\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","1c743f02":"#Data Analysis\nimport numpy as np\nimport pandas as pd\nimport scipy.stats as stat\n\n#Data Visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n#Data Preprocessing\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.preprocessing import StandardScaler\nimport scipy.stats as stat\n\n#Model Creations\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\n#Model Validation \nfrom sklearn.model_selection import  cross_val_score, StratifiedKFold, learning_curve\n\n#Hyper Parameter Tuning\nfrom sklearn.model_selection import GridSearchCV\n","d9ab6a1a":"train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","6e8337bb":"train.shape,test.shape","085e84d5":"train.info() #to check the type of data int\/float, and the vogue idea about null values","95a22f88":"test.info()  #to check the type of data int\/float, and the vogue idea about null values","5d3f7bbc":"train.head(4)","f02a8b37":"test.head(4)","cbef2047":"train.describe()","dfb84c70":"#quant_features=train.select_dtypes(exclude=['object']).columns\n#cat_features=train.select_dtypes(include=['object']).columns\nquant_feature=['Age','SibSp','Parch','Fare']\ncat_features=['Pclass','Sex','Embarked','Ticket','Cabin','Survived']","aaa36cdb":"for i in cat_features:\n    #sns.countplot(x=i,data=train)\n    sns.countplot(x=i,data=train,hue='Survived')\n    plt.show()\n    sns.barplot(x=i,y='Survived',data=train)\n    plt.show()","a878e841":"#Check the disturbution of quantiative data and might be required to standardize\/normalize it , ","dc59cec0":"for i in quant_feature:\n    sns.histplot(train[i])\n    plt.show()\n    ","b475f984":"#heatmap to check the correclation between the featues , \n#if it is highly correlated we have to ignore one of the feature\nsns.heatmap(train[quant_feature].corr(),annot=True)  ","294893c3":"x=pd.pivot_table(train,index='Survived',values=quant_feature)\nx","6bdc003e":"group_survival=train.groupby(['Survived','Pclass'])","48292656":"group_survival['Age',].mean()","d6a6a471":"\"\"\"\nCreating a new dataset by combining both the test and train data ,which will be easy to preprocess and later just before modeling split it again into train and test \nThis will avoid the tedious process of preprocessing train and test datasets seperately and avoid confusiions lateron \"\"\"\ndataset =  pd.concat(objs=[train, test], axis=0).reset_index(drop=True)","af2b93a1":"IDtest = test[\"PassengerId\"]\n\"\"\"\nThe passenger id for the test data is required as column name IDtest in future for submission \n\"\"\"","a59325ed":"dataset.isnull().sum()","2f7005e5":"dataset['Embarked']=dataset['Embarked'].fillna(dataset['Embarked'].mode()[0])  \n#since it is two Nan in Embarked, filled with the mode or maximum occurance","8a11118d":"dataset['Fare']=dataset['Fare'].fillna(dataset['Fare'].median()) \n#Nan-Fare values filled with median of the data","015d74e3":"sns.boxplot(x='Sex',y='Age',data=dataset)\n# the distribution of age data (range as well as the mean ) against Gender are amlost same.\n# we can ignore this feature to fill Age Nan.","c47513ab":"sns.boxplot(x='Pclass',y='Age',data=dataset)\n#in this we can observe the distribution of Age against different Pclass class is different , \n#so this can be considered to fill Age NaN values","07787e62":"sns.boxplot(x='Parch',y='Age',data=dataset)\n#in this we can observe the distribution of Age against different Parch class is different , \n#so this can be considered to fill Age NaN values","271cd82e":"sns.boxplot(x='SibSp',y='Age',data=dataset)\n#in this we can observe the distribution of Age against different SibSp class is different , \n#so this can be considered to fill Age NaN values","348872a5":"\nindex_NaN_age = list(dataset[\"Age\"][dataset[\"Age\"].isnull()].index)\n\nfor i in index_NaN_age :\n    age_med = dataset[\"Age\"].median()\n    age_pred = dataset[\"Age\"][((dataset['SibSp'] == dataset.iloc[i][\"SibSp\"]) &\n                               (dataset['Parch'] == dataset.iloc[i][\"Parch\"]) &\n                               (dataset['Pclass'] == dataset.iloc[i][\"Pclass\"]))].median()\n    if not np.isnan(age_pred) :\n        dataset['Age'].iloc[i] = age_pred\n    else :\n        dataset['Age'].iloc[i] = age_med","3f7e378a":"#total passengers is 1309, but unique ticket count is  only 929, that means there might be more than one passengers in one ticket or same cabin\ndataset.shape ,len(dataset['Ticket'].unique())","5eed87db":"dataset['Cabin'].unique() #This is to check the non filled nan cabins what string can be used to impute , XX is not in current data so used XX string to impute","f7ad1356":"\"\"\"Below code is used to fill the Nan cabin values with the exisitng cabin values which have same ticket number of the missing cabin(Nan Cabin)\nBut not much effective since only 14 cabins can be able to fill in that way \nso the remaining cabins considered as a seperate class named XX\n\"\"\"\nindex_NaN_cabin = list(dataset[\"Cabin\"][dataset[\"Cabin\"].isnull()].index)\nfor i in index_NaN_cabin:\n    for j in range(0,len(dataset)):\n        if dataset['Ticket'].iloc[j]==dataset['Ticket'].iloc[i]:\n            dataset['Cabin'].iloc[i]=dataset['Cabin'].iloc[j]\n            continue\n\ndataset['Cabin']=dataset['Cabin'].fillna('XX')","237c6985":"dataset.isnull().sum()","02206c08":"\"\"\"\nJust to check the behaviour name data\"\"\"\ndataset['Name'].head(5),dataset['Name'].tail(5)","e809b5c8":"title = [i.split(\",\")[1].split(\".\")[0].strip() for i in dataset[\"Name\"]]","b368d901":"dataset['Title']=pd.Series(title) #added the tittle series to the dataframe \ndataset['Title'].unique() #eheck the unique values","c608f295":"group_title=dataset.iloc[:len(train)].groupby('Title')\ngroup_title['Survived'].value_counts(normalize=True)\n","491aed6b":"\"\"\"\nBy observing above table we can find some inputs about the survival rate among the tittle groups \nso we can group the passengers according to the tittle and its survival rates\"\"\"\n\ngroup1=['Capt','Rev','Jonkheer','Don','Dona'] #grouped based on survival rate\ngroup2=['Dr', 'Major','Col'] #grouped based on survival rate\ngroup3=['the Countess','Countess', 'Sir',] #grouped based on survial rate\ngroup4=['Mr'] #title for men\ngroup5=['Master'] #title for a boys\ngroup6=[ 'Mrs', 'Miss','Mme', 'Ms','Mlle','Lady']# tittle for ladies\n\n\ndataset['Title']=dataset['Title'].replace(group1,'1')\ndataset['Title']=dataset['Title'].replace(group2,'2')\ndataset['Title']=dataset['Title'].replace(group3,'3')\ndataset['Title']=dataset['Title'].replace(group4,'4')\ndataset['Title']=dataset['Title'].replace(group5,'5')\ndataset['Title']=dataset['Title'].replace(group6,'6')\ndataset[\"Title\"] = dataset[\"Title\"].astype(int)","fcf186bb":"dataset['Title'].unique()","1c513ff9":"\n\"\"\"\nAfter grouping as above , if we check the groupby again it gives some meaning tittle class\"\"\"\ngroup_title=dataset.iloc[:len(train)].groupby('Title')\ngroup_title['Survived'].value_counts(normalize=True)","bb1d4a48":"sns.barplot(x='Title',y='Survived',data=dataset.iloc[:len(train)])\n#This became more meaning full grouping and it obvious that group 3 has highest survival rate and group 1 has least survival rate\n","b6c00f4d":"#So now drop the name colums from the dataset , adn keep title instead\ndataset.drop(labels = [\"Name\"], axis = 1, inplace = True)","e40e50ce":"# Replace the Cabin number by the type of cabin 'XX' if not\ndataset[\"Cabin\"] = pd.Series([i[0] if not pd.isnull(i) else 'X' for i in dataset['Cabin'] ])","346886c9":"sns.barplot(data=dataset,x='Cabin',y='Survived')","7af3bc33":"## Treat Ticket by extracting the ticket prefix. When there is no prefix it returns X. \n\nTicket = []\nfor i in list(dataset.Ticket):\n    if not i.isdigit() :\n        Ticket.append(i.replace(\".\",\"\").replace(\"\/\",\"\").strip().split(' ')[0]) #Take prefix\n    else:\n        Ticket.append(\"X\")\n        \ndataset[\"Ticket\"] = Ticket\ndataset[\"Ticket\"].head()","b29818b6":"dataset['Ticket'].unique()","258257a8":"dataset['Family']=dataset[\"SibSp\"] + dataset[\"Parch\"] + 1 ","5af75213":"sns.barplot(x='Family',y='Survived',data=dataset)\nplt.show()\nsns.countplot(x='Family',hue='Survived',data=dataset)\nplt.show()","7fd80517":"dataset['Family'].head()","c45aaf8f":"for i in range(0,len(dataset)):\n    if dataset['Family'].iloc[i]== 1:\n        dataset['Family'].iloc[i] = 'Single'\n    elif dataset['Family'].iloc[i] == 2:\n        dataset['Family'].iloc[i] = 'Couple'\n    elif dataset['Family'].iloc[i] >= 3 and dataset['Family'].iloc[i] <=4 :\n        dataset['Family'].iloc[i] = 'SmallFam'\n    elif dataset['Family'].iloc[i] >= 5 and dataset['Family'].iloc[i] <=7 :\n        dataset['Family'].iloc[i] = 'LargeFam'\n    elif dataset['Family'].iloc[i] > 7  :\n        dataset['Family'].iloc[i] = 'BigFam'\n        \n        ","7ffa6036":"dataset['Family'].unique()","30ce1261":"dataset.info()","584d5d31":"dataset['Fare'].skew()","88878629":"sns.histplot(dataset['Fare'],kde='True')\nprint('Skew:',dataset['Fare'].skew())","49176669":"# log norm of fare \ndataset['norm_fare'] = np.log(dataset.Fare)\nsns.histplot(dataset['norm_fare'],kde='True')\nprint('Skew:',dataset['norm_fare'].skew())","9a159ade":"#square root transformation \ndataset['sq_fare'] = dataset['Fare']**(1\/2)\nsns.histplot(dataset['sq_fare'],kde='True')\nprint('Skew:',dataset['sq_fare'].skew())","a402f508":"dataset['exp_fare']=dataset['Fare']**(1\/2.7)\nsns.histplot(dataset['exp_fare'],kde='True')\nprint('Skew:',dataset['exp_fare'].skew())","2a493a3d":"\n\"\"\"\nBelow is one of the two power transformers  yeo johnson and box cox but we cant use boxcox on 0 \nor negative values so used yeo john transformer here\"\"\"\nimport scipy.stats as stat\nfrom sklearn.preprocessing import PowerTransformer\npt = PowerTransformer()\nptdata=pd.DataFrame(pt.fit_transform(dataset['Fare'].values.reshape(-1,1)),columns=['Fare_YJ'])\nprint('skew',ptdata['Fare_YJ'].skew())\nsns.histplot(ptdata['Fare_YJ'],kde='True')","6e7f61a1":"#Assign the Yj transformed values to dataset\ndataset['Fare_transformed']=ptdata['Fare_YJ']\ndataset.drop(labels = [\"Fare\"], axis = 1, inplace = True) #drop actual Fare feature from the dataset","b9beb858":"dataset.drop(labels = ['norm_fare','exp_fare','sq_fare'], axis = 1, inplace = True) #drop transformed column","9095537d":"dataset['Fare_transformed'].min(),dataset['Fare_transformed'].max()","a3b2a9f9":"sns.histplot(dataset['Age'],kde='True')\nprint ('Skew_Age',dataset['Age'].skew())  # Age have very low skew value so not applying the transformation","7834eb97":"sns.histplot(dataset['SibSp'],kde='True')\nprint ('Skew_Sibp',dataset['SibSp'].skew()) ","4a1ade03":"# log norm of Sibsp\ndataset['log_Sib'] = np.log(dataset.SibSp)\nsns.histplot(dataset['log_Sib'],kde='True')\nprint('Skew:',dataset['log_Sib'].skew())","87abc6dc":"ptdata=pd.DataFrame(pt.fit_transform(dataset['SibSp'].values.reshape(-1,1)),columns=['sib_YJ'])\nprint('skew',ptdata['sib_YJ'].skew())\nsns.histplot(ptdata['sib_YJ'],kde='True')","c4c420c6":"dataset['Sib_norm']=ptdata['sib_YJ'] #Added Yeo-joh transformed Sibsp to the dataset\ndataset.drop(labels = [\"SibSp\"], axis = 1, inplace = True) #drop actual SibSp columns\ndataset.drop(labels = [\"log_Sib\"], axis = 1, inplace = True) #a log transformation created earlier , decicded not to use so deleting it from the dataset","0c81f6e3":"sns.histplot(dataset['Parch'],kde='True')\nprint ('Skew_Parch',dataset['Parch'].skew()) ","9fcfe7e2":"# log norm of Parch\ndataset['norm_Par'] = np.log(dataset.Parch)\nsns.histplot(dataset['norm_Par'],kde='True')\nprint('Skew:',dataset['norm_Par'].skew())\ndataset.drop(labels = [\"norm_Par\"], axis = 1, inplace = True) #drop transformed column","24779c0d":"dataset.info()","6235c205":"dataset[\"Pclass\"] = dataset[\"Pclass\"].astype(\"category\")\ndataset = pd.get_dummies(dataset, columns = [\"Pclass\"],drop_first=True,prefix=\"Pcl\")","e6253ff0":"dataset[\"Family\"] = dataset[\"Family\"].astype(\"category\")\ndataset = pd.get_dummies(dataset, columns = [\"Family\"],drop_first=True,prefix=\"Fam\")","abea635f":"dataset = pd.get_dummies(dataset, columns = [\"Cabin\"],drop_first=True,prefix=\"Cabin\")\ndataset = pd.get_dummies(dataset, columns = [\"Ticket\"],drop_first=True, prefix=\"T\")","2f7f43d4":"dataset = pd.get_dummies(dataset, columns = [\"Title\"],drop_first=True)\ndataset = pd.get_dummies(dataset, columns = [\"Embarked\"],drop_first=True, prefix=\"Em\")\ndataset = pd.get_dummies(dataset, columns = [\"Sex\"],drop_first=True, prefix=\"Emb\")","3365567d":"dataset.info()","34e85980":"pd.set_option('display.max_columns',80) #this is nothing but to display all the columns in visualization","3817742c":"dataset.describe()\n#Mean std of Age is differnet from the remaining all the features","c9e08d01":"#Apply standard scalar to Age to tranform\nfrom sklearn.preprocessing import StandardScaler \nsc = StandardScaler()\ndataset['Age'] = sc.fit_transform(pd.DataFrame(dataset['Age']))","f6e7be24":"dataset.describe()","4e82ee08":"dataset.drop(labels = [\"PassengerId\"], axis = 1, inplace = True) #drop irrelevent passenger id","a94f907d":"\n\"\"\"Split train and test datas from dataset and delete survived col from test data , \n   PN:Here test is the data we need to predict for submission , to the test data we used for regular validation purpose.\n\"\"\"\ntrain=dataset.iloc[:len(train)] #split \ntest=dataset.iloc[len(train):]\ntest.drop(labels=[\"Survived\"],axis = 1,inplace=True) #or test.drop(columns=['Survived'],inplace=True)","11971aad":"train['Survived']=train['Survived'].astype(int)\nY_train=train['Survived']\nX_train=train.drop(columns=['Survived'])","fcaa586d":"# Cross validate model with Kfold stratified cross val\nkfold = StratifiedKFold(n_splits=10)","4ec92a6d":"# Modeling step Test differents algorithms \nrandom_state = 2\nclassifiers = []\nclassifiers.append(SVC(random_state=random_state))\nclassifiers.append(DecisionTreeClassifier(random_state=random_state))\nclassifiers.append(AdaBoostClassifier(DecisionTreeClassifier(random_state=random_state),random_state=random_state,learning_rate=0.1))\nclassifiers.append(RandomForestClassifier(random_state=random_state))\nclassifiers.append(ExtraTreesClassifier(random_state=random_state))\nclassifiers.append(GradientBoostingClassifier(random_state=random_state))\nclassifiers.append(MLPClassifier(random_state=random_state))\nclassifiers.append(KNeighborsClassifier())\nclassifiers.append(LogisticRegression(random_state = random_state))\nclassifiers.append(LinearDiscriminantAnalysis())","d2b6a5f1":"cv_results = []\nfor classifier in classifiers :\n    cv_results.append(cross_val_score(classifier, X_train, y = Y_train, scoring = \"accuracy\", cv = kfold, n_jobs=4))","d54490f1":"cv_means = []\ncv_std = []\nfor cv_result in cv_results:\n    cv_means.append(cv_result.mean())\n    cv_std.append(cv_result.std())","f47a2722":"\ncv_result = pd.DataFrame({\"CrossValMeans\":cv_means,\"CrossValerrors\": cv_std,\"Algorithm\":[\"SVC\",\"DecisionTree\",\"AdaBoost\",\n\"RandomForest\",\"ExtraTrees\",\"GradientBoosting\",\"MultipleLayerPerceptron\",\"KNeighboors\",\"LogisticRegression\",\"LinearDiscriminantAnalysis\"]})\ncv_result.sort_values(by='CrossValerrors',ascending=False)\n","e7ff9fdd":"sns.barplot(x='CrossValerrors',y='Algorithm',data=cv_result.sort_values(by='CrossValerrors',ascending=False))\nplt.show()\nsns.barplot(x='CrossValMeans',y='Algorithm',data=cv_result.sort_values(by='CrossValMeans',ascending=False),)\nplt.show()","8fc3a454":"DTC = DecisionTreeClassifier()\n\nadaDTC = AdaBoostClassifier(DTC, random_state=7)\n\nada_param_grid = {\"base_estimator__criterion\" : [\"gini\", \"entropy\"],\n              \"base_estimator__splitter\" :   [\"best\", \"random\"],\n              \"algorithm\" : [\"SAMME\",\"SAMME.R\"],\n              \"n_estimators\" :[1,2],\n              \"learning_rate\":  [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3,1.5]}\n\ngsadaDTC = GridSearchCV(adaDTC,param_grid = ada_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsadaDTC.fit(X_train,Y_train)\n\nada_best = gsadaDTC.best_estimator_\ngsadaDTC.best_score_","4ba2e0cf":"ExtC = ExtraTreesClassifier()\n## Search grid for optimal parameters\nex_param_grid = {\"max_depth\": [None],\n              \"max_features\": [1, 3, 10],\n              \"min_samples_split\": [2, 3, 10],\n              \"min_samples_leaf\": [1, 3, 10],\n              \"bootstrap\": [False],\n              \"n_estimators\" :[100,300],\n              \"criterion\": [\"gini\"]}\n\ngsExtC = GridSearchCV(ExtC,param_grid = ex_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsExtC.fit(X_train,Y_train)\n\nExtC_best = gsExtC.best_estimator_\n\n# Best score\ngsExtC.best_score_","d1aeab32":"SVMC = SVC(probability=True)\nsvc_param_grid = {'kernel': ['rbf'], \n                  'gamma': [ 0.001, 0.01, 0.1, 1],\n                  'C': [1, 10, 50, 100,200,300, 1000]}\n\ngsSVMC = GridSearchCV(SVMC,param_grid = svc_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsSVMC.fit(X_train,Y_train)\nSVMC_best = gsSVMC.best_estimator_\ngsSVMC.best_score_","e0f56423":"\nRFC = RandomForestClassifier()\n\n\n## Search grid for optimal parameters\nrf_param_grid = {\"max_depth\": [None],\n              \"max_features\": [1, 3, 10],\n              \"min_samples_split\": [2, 3, 10],\n              \"min_samples_leaf\": [1, 3, 10],\n              \"bootstrap\": [False],\n              \"n_estimators\" :[100,300],\n              \"criterion\": [\"gini\"]}\n\n\ngsRFC = GridSearchCV(RFC,param_grid = rf_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsRFC.fit(X_train,Y_train)\n\nRFC_best = gsRFC.best_estimator_","1483f595":"GBC = GradientBoostingClassifier()\ngb_param_grid = {'loss' : [\"deviance\"],\n              'n_estimators' : [100,200,300],\n              'learning_rate': [0.1, 0.05, 0.01],\n              'max_depth': [4, 8],\n              'min_samples_leaf': [100,150],\n              'max_features': [0.3, 0.1] \n              }\n\ngsGBC = GridSearchCV(GBC,param_grid = gb_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsGBC.fit(X_train,Y_train)\n\nGBC_best = gsGBC.best_estimator_\n\n# Best score\ngsGBC.best_score_","bd8976c9":"test_Survived_RFC = pd.Series(RFC_best.predict(test), name=\"RFC\")\ntest_Survived_ExtC = pd.Series(ExtC_best.predict(test), name=\"ExtC\")\ntest_Survived_SVMC = pd.Series(SVMC_best.predict(test), name=\"SVC\")\ntest_Survived_AdaC = pd.Series(ada_best.predict(test), name=\"Ada\")\ntest_Survived_GBC = pd.Series(GBC_best.predict(test), name=\"GBC\")\n\n# Concatenate all classifier results\nensemble_results = pd.concat([test_Survived_RFC,test_Survived_ExtC,test_Survived_AdaC,test_Survived_GBC, test_Survived_SVMC],axis=1)\nensemble_results.head(3)","f328d3cd":"sns.heatmap(ensemble_results.corr(),annot=True,cbar=True)","4cadbffe":"votingC = VotingClassifier(estimators=[('rfc', RFC_best), ('extc', ExtC_best),\n('svc', SVMC_best), ('adac',ada_best),('gbc',GBC_best)], voting='soft', n_jobs=4)\n\nEnsemble_train = votingC.fit(X_train, Y_train)","78256be0":"output = pd.DataFrame({'PassengerId': IDtest, 'Survived': Ensemble_train.predict(test)})\noutput.to_csv('submission.csv',index=False)","df77bddd":"## Base line model selection","954dfe22":"### Age","b2e51879":"SibSp has relatively high skew so need to transform to reduce the skew and make the data normally distributed","809f4647":"### Ticket","2e13c312":"\"\"\"Based on the above result we selected the best five models as base line models for further tuning or  hyperparameter optimization and creating Ensemble model for prediction \"\"\"","7be2870c":"#### RFC","00285024":"PCA can be done to reduce the dimensions or number of features","a82c545c":"Handling NaN : AGE\nFirst need to find which are the other features have influence to age feature\nSo plot other features against Age","dad7a1f0":"## Further Normalization of Few Features","ca054e56":"\"\"\"\n1.The steps we are following here is select 10 best classification algorithms and run with default values ,\n2.Select the best 5 models by checking the score by cross validastion scores\n3.Then hypertune the best 5 models by selecting the best hyperparameters using GrivSearch\n4.Use soft voting technique to combine the outputs from all the models (Ensemble model creation) \"\"\"","3473e491":"# 1.EXPLORATORY DATA ANALYSIS (EDA)","1d0da7b3":"So belwo we created a code in to find the age Nan rows having the PClass,Parch,SibSp , have same values eg Row 5,19,\nthen find the rows of same values and fill the with the mean of that group\n\nIf we cant find such group then fill those Nan Ages with Median of the Ages.","6985c4ba":"HANDLE CATAGORICAL FEATURES INTO NUMERICAL ,ONE HOT ENCODING","341cbe5d":"# 3. FEATURE SELECTION ","34cbcb87":"### Sib \/Parch","19a5e103":"CHECK THE DATA DISTRUBITION AND SKEW OF EACH NUMERICAL FEATURES\n\nBelow we can check the skew and try different transformers so that the skew will be minimum . among all Yeo john transformer has least skew \nAlso we cant use the box cox transformer since we have some zero values in Fare.","86076cd0":"CABIN\nFew of the materials in internet shows that there are decks named A,B,C etc so there is a strong belief that the prefix in the cabin number is nothing but the \ndeck. so in that belief we can extract the initial alphabets and classify it as decks","42e626fd":"## Feature Transformations for Linear models","17b8afc2":"# SUBMISSION.","fdc176d5":"## EDA Numerical Features","bdd4c134":"### Fare","bcc4dbf8":"## Ensemble Model Creation","54c000e5":"FAMILY\nWe can observe that the chance of survival is differnet depends on the size of the family.\nhere the family is the combination of the number of parents \/ children aboard the Titanic and number of siblings \/ spouses aboard the Titanic.\nso we can find the size each family onboard by adding Sibp and Parch + 1(passenger himself)","28999303":"# 0.LIBRARIES REQUIRED","a78cbf49":"CHECK THE INFO OF THE DATA AND DECIDE WHETHER DO WE NEED TO NORMALIZE ANY FEATURES","87944178":"## EDA (Catagorical Features)","ec66de11":"## Feature Extraction","97989960":"#### Gradient Boosting","96f9c488":"# 4.MODEL CREATION","57e08592":"Feature Extraction from Name columns\nIf we observe the every names have tittle(Mr,Mrs,Rev,Countess etc) which may give the some information \nto group the namesInstead of completley omiting the name column as irrelevent we can check what information \nwe can extract from the name columns\nIf we observe the tittle of the name comes after the surname seperated with a comma.So we can split all the \nnames with respect to comma get into a list and from the list take the second element which is tittle\n","87600339":"### Parch","727ba98d":"Below plot shows the survial probability based on family class, and the most is with the familyh number 4,\nBut its not necessary that the indivitual traveller will survive , it may be most of the lone travellers are males.\nAlso we can observe that the family size beyond 5 has less chance of survival since the difficulity in mobility altogether.\nby looking below details we can divide the family data into few classes , like single, small family, medium,large,very large","58089838":"#### SVC","882390ed":"### SibSp","80269845":"for i in range(0,len(dataset)):\n    print (dataset['Family'].iloc[i])","187620a6":"# 2.FEATURE ENGINEERING","39d279e9":"#### Ada Boost","fcdbfa05":"## Catagorical Data to One-Hot Encoding","cb304a6f":"### Cabin","96d5a3ae":"Age has normally distributed data and very low skew so no need to transform the data","ab1b2503":"### Name","352b6794":"## Handling Null\/Nan values","e398a051":"#### Extra tree Classifier","7f79da47":"## Hyper Parameter Optimization","5a6e0dab":"### PCA","0557e911":"## Drop Irrrelevent Data"}}