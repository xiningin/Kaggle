{"cell_type":{"747bfaff":"code","a23ecb95":"code","b1c8af47":"code","70d40da5":"code","43c55407":"code","bf6cb738":"code","4a3cf955":"code","1dff4cca":"code","95c540d7":"code","0e85edf9":"code","e6bb2f43":"code","69e80a4f":"code","e55ee627":"code","84fabcb0":"code","0170a198":"code","99f60cf9":"code","447eca3f":"code","a894e383":"code","a41ac563":"code","4628be98":"code","4cf2b2f5":"code","95728372":"code","8674dd59":"code","a1a7b20e":"code","2eeaea18":"code","d756bb4a":"code","3f03f925":"code","9fe4298e":"code","32a1c2dd":"code","525f8b38":"code","65754036":"code","6e444a86":"code","fe3dedf9":"code","70a40670":"code","2800f198":"code","cebc0462":"code","1c6d671f":"code","010dd3c7":"code","1f7f7088":"code","45989154":"code","ee9c53d0":"code","581cc07e":"code","5ff869eb":"code","398021e9":"code","d2fe88fe":"code","5b233650":"code","533dcd1b":"markdown","bb75604e":"markdown","1165ad11":"markdown","00a98113":"markdown","750b4209":"markdown","cfc9dbee":"markdown","a407ce44":"markdown","ba499065":"markdown","b2ee8427":"markdown","34d44f37":"markdown","6d98e173":"markdown","ca095430":"markdown","55028105":"markdown","e1d1664e":"markdown","18877d4c":"markdown","15d9593f":"markdown","fb3fcddc":"markdown","a09e0290":"markdown","e6a6830d":"markdown","4255b844":"markdown","9acd7b89":"markdown","8b0c0226":"markdown","a2cf1fd7":"markdown","c47b6014":"markdown","8a0666d0":"markdown","bc182e8d":"markdown","5e62a568":"markdown","6ad74e05":"markdown","7ad2cb56":"markdown","47786998":"markdown","f5ecbb82":"markdown","9136f02e":"markdown","72a33aa9":"markdown","ec1c8000":"markdown","9554e247":"markdown","0b629e85":"markdown","10cc7104":"markdown","4aa62ece":"markdown","ecefb1ea":"markdown","93503de6":"markdown","ad7a69f1":"markdown","37bcc2de":"markdown","c74d6356":"markdown","1ac8163f":"markdown","3438936a":"markdown","1834ae34":"markdown","85508e06":"markdown","bc56d7d5":"markdown","64536333":"markdown","570af656":"markdown","5ad2986e":"markdown"},"source":{"747bfaff":"#These open source libraries are using python 3.6\nfrom __future__ import unicode_literals, print_function\nimport numpy as np \nimport pandas as pd \nimport re\nimport nltk\nimport string\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk import word_tokenize\nimport random\nimport dateutil.parser as dparser\nfrom datetime import datetime\n!pip install datefinder\nimport datefinder\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom nltk.tokenize import sent_tokenize\nfrom wordcloud import WordCloud, STOPWORDS\nfrom flashtext import KeywordProcessor\nimport os\nimport plac\nimport spacy","a23ecb95":"def load_jobopening_dataset():\n\n    data_path = \"..\/input\/data-science-for-good-city-of-los-angeles\/CityofLA\/CityofLA\/Job Bulletins\/\"\n\n    texts = []\n    positions = []\n    file_names=[]\n    for fname in sorted(os.listdir(data_path)):\n        if fname.endswith('.txt'):\n            file_names.append(fname)\n            with open(os.path.join(data_path, fname),\"rb\") as f:\n                texts.append(str(f.read()))\n                positions.append((re.split(' (?=class)', fname))[0])\n    \n    #print the length of the List of text, length of file_names and positions and make sure they are all equal\n    return (texts,positions,file_names)\n\n#job_data, positions, file_names = load_jobopening_dataset() #This code can't read texts from zip file so I would manually import the raw text files I exported from my personal computer\njob_data = pd.read_csv(\"..\/input\/raw-file\/job_data.csv\").values.tolist()\npositions =  pd.read_csv(\"..\/input\/raw-file\/positions.csv\").values.tolist()\nfile_names =  pd.read_csv(\"..\/input\/raw-file\/file_names.csv\").values.tolist()","b1c8af47":"str(job_data[0]).replace(\"\\\\r\\\\n\",\" \").replace(\"\\\\\\'s\",\"\")[:250]","70d40da5":"titles = pd.read_csv(\"..\/input\/additional-data\/job_titles.csv\", header=None)\ntitles.head()","43c55407":"data_dict=pd.read_csv(\"..\/input\/additional-data\/kaggle_data_dictionary.csv\")\ndata_dict[:5]","bf6cb738":"def Position_parser(s):\n    title_match=False\n    pos = re.findall(r'(.*?)Class Code',s)\n    pos1 = re.findall(r'(.*?)Class  Code',s)\n    if (len(pos1) > 0):\n        pos = pos1\n    if (len(pos) > 0):\n        job_title= pos[0].replace(\"b'\",\"\").replace(\"b\\\"\",\"\").replace(\"'\",\"\").replace(\"\\\\\",\"\").strip()\n        for title in titles[0]:\n            if (title.replace(\"'\",\"\")==job_title):\n                title_match=True\n                break\n    if(title_match==True):\n        return job_title\n    else:\n        return \"Invalid job title\" ","4a3cf955":"def JobCode_parser(s):\n    job_code = 0\n    code = re.findall(r'Class Code:(.*?)Open',s)\n    if (len(code)>0):\n        job_code= int(code[0].strip())\n    return job_code","1dff4cca":"def OpenDate_parser(s):\n    openDateRet=\"\"\n    openDate = re.findall(r'Open Date:(.*?)ANNUAL',s)\n    openStr=\"\"\n    if (len(openDate)>0):\n        #print(openDate)\n        openDate = openDate[0].strip()\n        openStr=re.findall(r'(?:Exam).*',openDate)\n        #print(openStr)\n    \n    matches = list(datefinder.find_dates(openDate))\n\n    if len(matches) > 0:\n        for i in range(len(matches)):\n            date = matches[i]\n            openDateRet=str(date.date())\n    return openDateRet,openStr","95c540d7":"def SalaryRange_parser(s):\n    salaryRange = re.findall(r'ANNUAL SALARY(.*?)NOTE',s)\n    salaryRange_1 = re.findall(r'ANNUAL SALARY(.*?)DUTIES',s)\n    salaryRange_2 = re.findall(r'ANNUAL SALARY(.*?)\\(flat',s)\n    len1=0\n    len2=0\n    len3=0\n    if (len(salaryRange) > 0):\n        len1 = len(salaryRange[0])\n    if (len(salaryRange_1) > 0):\n        len2 = len(salaryRange_1[0])\n    if (len(salaryRange_2) > 0):\n        len3 = len(salaryRange_2[0])\n    if ((len1 > 0) & (len2 > 0)):\n        if (len1 < len2):\n            salaryRange = salaryRange\n        else:\n            salaryRange = salaryRange_1\n        \n    if (len(salaryRange)>0):\n        salaryRange = salaryRange[0].strip()\n    return salaryRange","0e85edf9":"def Qualification_parser(s):\n    qual = re.findall(r'REQUIREMENTS\/MINIMUM QUALIFICATIONS(.*?)WHERE TO APPLY',s)\n    if (len(qual)==0):\n        qual = re.findall(r'REQUIREMENT\/MINIMUM QUALIFICATION(.*?)WHERE TO APPLY',s)\n    if (len(qual)==0):\n        qual = re.findall(r'REQUIREMENTS(.*?)WHERE TO APPLY',s)\n    if (len(qual)==0):\n        qual = re.findall(r'REQUIREMENT(.*?)WHERE TO APPLY',s)\n    if (len(qual)>0):\n        qual = qual[0].replace(\"\\\\'s\",\"'s\").strip()\n    else:\n        qual=\"\"\n    return qual","e6bb2f43":"def Education_parser(s):\n    educationMajor=\"\"\n    sentences = sent_tokenize(s)\n    selected_sentences=[sent for sent in sentences if \"major\" in word_tokenize(sent)]\n    for i in range(len(selected_sentences)):\n        major = re.findall(r'major in(.*?),',selected_sentences[i])\n        if (len(major)>0):\n            educationMajor=major[0].strip()\n    return educationMajor","69e80a4f":"def EduSemDur_parser(s):\n    educationDur=\"\"\n    sentences = sent_tokenize(s)\n    selected_sentences=[sent for sent in sentences if \"semester\" in word_tokenize(sent)]\n    for i in range(len(selected_sentences)):\n        dur = re.findall(r'(.*?)semester',selected_sentences[i])\n        #print(dur)\n        if (len(dur)>0):\n            educationDur=dur[0]+'sememster'\n    return educationDur","e55ee627":"def Duties_parser(s):\n    duties = re.findall(r'DUTIES(.*?)REQUIREMENT',s)\n    jobDuties=\"\"\n    if (len(duties)>0):\n        jobDuties= duties[0].strip()\n    return jobDuties","84fabcb0":"def eduYears_parser(s):\n    keyword_processor = KeywordProcessor()\n    education_yrs=0.0\n    keyword_processor.add_keyword('four-year')\n    keyword_processor.add_keyword('four years')\n    sentences = sent_tokenize(s)\n    selected_sentences=[sent for sent in sentences if \"degree\" in word_tokenize(sent)]\n    selected_sentences1=[sent for sent in sentences if \"Graduation\" in word_tokenize(sent)]\n\n    for i in range(len(selected_sentences)):\n        keywords_found = keyword_processor.extract_keywords(selected_sentences[i])\n        if (len(keywords_found) > 0):\n            education_yrs=4.0\n    for i in range(len(selected_sentences1)):\n        keywords_found = keyword_processor.extract_keywords(selected_sentences1[i])\n        if (len(keywords_found) > 0):\n            education_yrs=4.0\n   \n    return education_yrs","0170a198":"def expYears_parser(s):\n    keyword_processor = KeywordProcessor()\n    exp_yrs=0.0\n    keyword_processor.add_keyword('four-year')\n    keyword_processor.add_keyword('four years')\n    keyword_processor.add_keyword('three years')\n    keyword_processor.add_keyword('one year')\n    keyword_processor.add_keyword('two years')\n    keyword_processor.add_keyword('six years')\n    sentences = sent_tokenize(s)\n    selected_sentences=[sent for sent in sentences if \"experience\" in word_tokenize(sent)]\n\n    for i in range(len(selected_sentences)):\n        keywords_found = keyword_processor.extract_keywords(selected_sentences[i])\n        for i in range(len(keywords_found)):\n            if keywords_found[i]=='two years':\n                exp_yrs=2.0\n            elif keywords_found[i]=='one year':\n                exp_yrs=1.0\n            elif keywords_found[i]=='three years':\n                exp_yrs=3.0\n            elif keywords_found[i]=='six years':\n                exp_yrs=6.0\n            elif keywords_found[i]=='four years':\n                exp_yrs=4.0\n            elif keywords_found[i]=='four-year':\n                exp_yrs=4.0\n                \n    return exp_yrs","99f60cf9":"def fullTimePartTime_parser(s):\n    keyword_processor = KeywordProcessor()\n    fullTimePartTime=\"\"\n    keyword_processor.add_keyword('full-time')\n    keyword_processor.add_keyword('part-time')\n    sentences = sent_tokenize(s)\n    selected_sentences=[sent for sent in sentences if \"experience\" in word_tokenize(sent)]\n\n    for i in range(len(selected_sentences)):\n        keywords_found = keyword_processor.extract_keywords(selected_sentences[i])\n        for i in range(len(keywords_found)):\n            if keywords_found[i]=='full-time':\n                fullTimePartTime=\"FULL TIME\"\n            elif keywords_found[i]=='part-time':\n                fullTimePartTime=\"PART TIME\"\n           \n                \n    return fullTimePartTime","447eca3f":"def DL_parser(s):\n    dl = False\n    dl_valid = False\n    dl_State = \"\"\n    arr = ['driver', 'license']\n    keyword_processor = KeywordProcessor()\n    keyword_processor.add_keyword('california')\n    if any(re.findall('|'.join(arr), qual)):\n        dl = True\n    if (dl==True):\n        sentences = sent_tokenize(s)\n        selected_sentence=[sent for sent in sentences if \"driver\" in word_tokenize(sent)]\n        if (len(selected_sentence)>0):\n            words = selected_sentence[0].split()\n            selected_word = [word for word in words if \"valid\" in words]\n            if len(selected_word)>0:\n                dl_valid=True\n        for i in range(len(selected_sentence)):   \n            keywords_found = keyword_processor.extract_keywords(selected_sentence[i])\n            for i in range(len(keywords_found)):\n                if keywords_found[i]=='california':\n                    dl_State=\"CA\"\n                \n    if (dl_valid)==True:\n        dl_valid=\"R\"\n    else:\n        dl_valid=\"P\"\n    return dl_valid,dl_State","a894e383":"def Relations_parser(TEXTS, nlp, ENTITY_TYPE):\n    entities=[]\n    for text in TEXTS:\n        doc = nlp(text)\n        relations = extract_entity_relations(doc,ENTITY_TYPE)\n        for r1, r2 in relations:\n            relation=r1.text+\"-\"+r2.text\n            entities.append(relation)\n    imp_entities='::::'.join(entities)   \n    return imp_entities","a41ac563":"def College_parser(s):\n    college=\"\"\n    keyword_processor = KeywordProcessor()\n    keyword_processor.add_keyword('college or university')\n    keyword_processor.add_keyword('college')\n    keyword_processor.add_keyword('university')\n    keyword_processor.add_keyword('high school')\n    sentences = sent_tokenize(s)\n    for j in range(len(sentences)):\n        sentence = sentences[j]\n        keywords_found = keyword_processor.extract_keywords(sentence)\n        if (len(keywords_found) > 0):\n            for i in range(len(keywords_found) ):\n                if (keywords_found[i]=='college or university'):\n                    college='college or university'\n                    break\n                elif (keywords_found[i]=='college'):\n                    college='college'\n                    break\n                elif (keywords_found[i]=='university'):\n                    college='university'\n                    break\n                elif (keywords_found[i]=='high school'):\n                    college='high school'\n                    break\n    \n\n    return college","4628be98":"def filter_spans(spans):\n    get_sort_key = lambda span: (span.end - span.start, span.start)\n    sorted_spans = sorted(spans, key=get_sort_key, reverse=True)\n    result = []\n    seen_tokens = set()\n    for span in sorted_spans:\n        if span.start not in seen_tokens and span.end - 1 not in seen_tokens:\n            result.append(span)\n            seen_tokens.update(range(span.start, span.end))\n    return result\n\n\ndef extract_entity_relations(doc,entity):\n    # Merge entities and noun chunks into one token\n    seen_tokens = set()\n    spans = list(doc.ents) + list(doc.noun_chunks)\n    spans = filter_spans(spans)\n    with doc.retokenize() as retokenizer:\n        for span in spans:\n            retokenizer.merge(span)\n\n    relations = []\n    for money in filter(lambda w: w.ent_type_ == entity, doc):\n        if money.dep_ in (\"attr\", \"dobj\"):\n            subject = [w for w in money.head.lefts if w.dep_ == \"nsubj\"]\n            if subject:\n                subject = subject[0]\n                relations.append((subject, money))\n        elif money.dep_ == \"pobj\" and money.head.dep_ == \"prep\":\n            relations.append((money.head.head, money))\n    return relations","4cf2b2f5":"nltk.download('punkt')\nnlp = spacy.load(\"en_core_web_sm\")\n\njob_data_export=pd.DataFrame(columns=[\"FILE_NAME\",\"JOB_CLASS_TITLE\",\"JOB_CLASS_NO\",\"REQUIREMENT_SET_ID\",\n                                      \"REQUIREMENT_SUBSET_ID\",\"JOB_DUTIES\",\n                                      \"EDUCATION_YEARS\",\"SCHOOL_TYPE\",\"EDUCATION_MAJOR\",\"EXPERIENCE_LENGTH\",\"IMP_ENTITIES_QUAL\",\n                                     \"FULL_TIME_PART_TIME\",\"EXP_JOB_CLASS_TITLE\",\"EXP_JOB_CLASS_ALT_RESP\"\n                                     ,\"EXP_JOB_CLASS_FUNCTION\",\"COURSE_COUNT\",\"COURSE_LENGTH\",\"COURSE_SUBJECT\"\n                                     ,\"MISC_COURSE_DETAILS\",\"DRIVERS_LICENSE_REQ\",\"DRIV_LIC_TYPE\",\n                                     \"ADDTL_LIC\",\"EXAM_TYPE\",\"ENTRY_SALARY_GEN\",\"ENTRY_SALARY_DWP\",\"OPEN_DATE\",\"LEGAL_TERMS\"])\n\nfor i in range(0, len(job_data)-1):\n\n    s = str(job_data[i]).replace(\"\\\\r\\\\n\",\" \").replace(\"\\\\t\",\"\")\n    position = Position_parser(s)\n    qual = Qualification_parser(s)\n    DL_valid,DL_state = DL_parser(qual)\n    education_yrs = eduYears_parser(qual)\n    education_major = Education_parser(qual)\n    try:\n        job_code = JobCode_parser(s)\n        openDate, openStr = OpenDate_parser(s)\n    except:\n        job_code = \"NaN\"\n        openDate = \"NaN\"\n        openStr = \"NaN\"\n    salaryRange = SalaryRange_parser(s)\n    expYrs = expYears_parser(s)\n    duties = Duties_parser(s)\n    course_length = EduSemDur_parser(qual)\n    fullTimePartTime = fullTimePartTime_parser(qual)\n    imp_qual_entities=Relations_parser([qual],nlp,\"ORG\")\n    imp_qual_cardinals=Relations_parser([qual],nlp,\"CARDINAL\")\n    imp_legal_terms=Relations_parser([s],nlp,\"LAW\")\n    college = College_parser(qual)\n    job_data_export.loc[i,\"JOB_CLASS_TITLE\"]=position\n    job_data_export.loc[i,\"FILE_NAME\"]=file_names[i]\n    job_data_export.loc[i,\"DRIVERS_LICENSE_REQ\"]=DL_valid\n    job_data_export.loc[i,\"EDUCATION_YEARS\"]=education_yrs\n    job_data_export.loc[i,\"JOB_CLASS_NO\"]=job_code\n    job_data_export.loc[i,\"OPEN_DATE\"]=openDate\n    job_data_export.loc[i,\"ENTRY_SALARY_GEN\"]=salaryRange\n    job_data_export.loc[i,\"JOB_DUTIES\"]=duties\n    job_data_export.loc[i,\"EXPERIENCE_LENGTH\"]=expYrs\n    job_data_export.loc[i,\"DRIV_LIC_TYPE\"]=DL_state\n    job_data_export.loc[i,\"EDUCATION_MAJOR\"]=education_major\n    job_data_export.loc[i,\"IMP_ENTITIES_QUAL\"]=imp_qual_entities\n    job_data_export.loc[i,\"COURSE_LENGTH\"]=course_length\n    job_data_export.loc[i,\"FULL_TIME_PART_TIME\"]=fullTimePartTime\n    job_data_export.loc[i,\"SCHOOL_TYPE\"]=college\n    job_data_export.loc[i,\"MISC_COURSE_DETAILS\"]=imp_qual_cardinals\n    job_data_export.loc[i,\"LEGAL_TERMS\"]=imp_legal_terms\n    job_data_export.loc[i,\"EXAM_TYPE\"]=openStr","95728372":"job_data_export.head()","8674dd59":"job_data_export.to_csv(\"LA_job_class_export.csv\",index=False)","a1a7b20e":"job_data_export = pd.read_csv(\"..\/input\/csv-file\/LA_job_class_export.csv\")\njob_data_export.to_csv(\"LA_job_class_export.csv\",index=False)","2eeaea18":"nltk.download('stopwords')","d756bb4a":"word_count = []\nfor s in job_data:\n    word_count.append(len(str(s).split()))","3f03f925":"fig, ax = plt.subplots()\ndata = np.random.rand(1000)\n\nN, bins, patches = ax.hist(word_count, edgecolor='white', bins=50, linewidth=0)\n\nq1 = 0.25*len(patches)\n\nq3 = 0.75*len(patches)\nfor i in range(0,int(q1)):\n    patches[i].set_facecolor('b')\nfor i in range(int(q1), int(q3)):\n    patches[i].set_facecolor('g')\nfor i in range(int(q3), len(patches)):\n    patches[i].set_facecolor('r')\nplt.xlabel('Number of words')\nplt.ylabel('Number of samples')\nplt.title('Sample length distribution')\nplt.show()","9fe4298e":"stats = pd.Series(word_count)\nq = pd.DataFrame(stats.describe()[3:]).transpose()\nq","32a1c2dd":"!pip install textstat\nimport textstat\nscore_list = []\nfor text in job_data:\n    score_list.append(textstat.flesch_reading_ease(str(text)))","525f8b38":"readability=pd.DataFrame(job_data_export[\"FILE_NAME\"])\nreadability.insert(1, \"SCORE\", score_list[:len(score_list)-1], True) \nreadability.head(10)","65754036":"rstats = readability[\"SCORE\"].describe()[3:]\npd.DataFrame(rstats).transpose()","6e444a86":"exclude = set(string.punctuation) \nwpt = nltk.WordPunctTokenizer()\nstop_words = nltk.corpus.stopwords.words('english')\n#\nnewStopWords = ['city','los','angele','angeles','may']\nstop_words.extend(newStopWords)\ntable = str.maketrans('', '', string.punctuation)\n\nlemma = WordNetLemmatizer()\nporter = PorterStemmer()","fe3dedf9":"def normalize_document(doc):\n    #replace newline and tab chars\n    doc = doc.replace(\"\\\\r\\\\n\",\" \").replace(\"\\\\\\'s\",\"\").replace(\"\\t\",\" \") #.split(\"b'\")[1]\n    # tokenize document\n    tokens = doc.split()\n    # remove punctuation from each word\n    tokens = [w.translate(table) for w in tokens]\n    # convert to lower case\n    lower_tokens = [w.lower() for w in tokens]\n    #remove spaces\n    stripped = [w.strip() for w in lower_tokens]\n    # remove remaining tokens that are not alphabetic\n    words = [word for word in stripped if word.isalpha()]\n    # filter stopwords out of document\n    filtered_tokens = [token for token in words if token not in stop_words]\n    #normalized = \" \".join(lemma.lemmatize(word) for word in filtered_tokens)\n    #join the tokens back to get the original doc\n    doc = ' '.join(filtered_tokens)\n    return doc\n\nnormalize_corpus = np.vectorize(normalize_document)\n#apply the text normalization to list of job positions\nnorm_positions=[]\nfor text_sample in positions:\n    norm_positions.append(normalize_document(str(text_sample)))\n#apply the text normalization to list of job ads\nnorm_corpus=[]\nfor text_sample in job_data:\n    norm_corpus.append(normalize_document(str(text_sample)))","70a40670":"full_norm_corpus=' '.join(norm_corpus)\nstopwords = set(STOPWORDS)\nstopwords.update([\"class\", \"code\"])\n\nwordcloud = WordCloud(background_color='white', stopwords=stopwords,max_words=100).generate(full_norm_corpus)\nplt.figure(figsize=(15,10))\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.show()","2800f198":"def ngrams(sample_texts, ngram_range, num_ngrams=30):\n    \"\"\"Plots the frequency distribution of n-grams.\n\n    # Arguments\n        samples_texts: list, sample texts.\n        ngram_range: tuple (min, mplt), The range of n-gram values to consider.\n            Min and mplt are the lower and upper bound values for the range.\n        num_ngrams: int, number of n-grams to plot.\n            Top `num_ngrams` frequent n-grams will be plotted.\n    \"\"\"\n    # Create args required for vectorizing.\n    kwargs = {\n            'ngram_range': ngram_range,\n            'dtype': 'int32',\n            'strip_accents': 'unicode',\n            'decode_error': 'replace',\n            'analyzer': 'word',  # Split text into word tokens.\n    }\n    vectorizer = CountVectorizer(**kwargs)\n    vectorized_texts = vectorizer.fit_transform(sample_texts)\n\n    # This is the list of all n-grams in the index order from the vocabulary.\n    all_ngrams = list(vectorizer.get_feature_names())\n    num_ngrams = min(num_ngrams, len(all_ngrams))\n    ngrams = all_ngrams[:num_ngrams]\n\n    # Add up the counts per n-gram ie. column-wise\n    all_counts = vectorized_texts.sum(axis=0).tolist()[0]\n\n    # Sort n-grams and counts by frequency and get top `num_ngrams` ngrams.\n    all_counts, all_ngrams = zip(*[(c, n) for c, n in sorted(\n        zip(all_counts, all_ngrams), reverse=True)])\n    ngrams = list(all_ngrams)[:num_ngrams]\n    counts = list(all_counts)[:num_ngrams]\n    return ngrams, counts\n\n\nngrams4, counts4 = ngrams(norm_corpus,ngram_range=(4, 4))","cebc0462":"# Fixing random state for reproducibility\nidx = np.arange(30)\nnp.random.seed(19680801)\nplt.rcdefaults()\nfig, ax = plt.subplots()\n#horizontal\nax.barh(idx, counts4, align='center', color='g')\nax.set_yticks(idx)\nax.set_yticklabels(ngrams4, rotation=0, fontsize=8)\nax.invert_yaxis()  # labels read top-to-bottom\nax.set_xlabel('Frequencies',fontsize=\"12\")\nax.set_title('Frequency distribution of n-grams',fontsize=\"12\")\n\nplt.show()","1c6d671f":"from textblob import TextBlob\nfrom textblob.sentiments import NaiveBayesAnalyzer\nimport nltk\n#nltk.download('movie_reviews')\n#nltk.download('punkt')\n\ntext          = \"I only hire male applicants\" \n\nsent          = TextBlob(text)\n# The polarity score is a float within the range [-1.0, 1.0]\n# where negative value indicates negative text and positive\n# value indicates that the given text is positive.\npolarity      = sent.sentiment.polarity\n# The subjectivity is a float within the range [0.0, 1.0] where\n# 0.0 is very objective and 1.0 is very subjective.\nsubjectivity  = sent.sentiment.subjectivity\n\nsent          = TextBlob(text, analyzer = NaiveBayesAnalyzer())\nclassification= sent.sentiment.classification\npositive      = sent.sentiment.p_pos\nnegative      = sent.sentiment.p_neg\n\n\ndict1 = {'Polarity': polarity,'Subjectivity': subjectivity, 'Classification': classification, 'Posititve': positive, 'Negative': negative}\ndf1 = pd.Series(dict1)\ndf1\n","010dd3c7":"def pos_neg_classify(text):\n    sent          = TextBlob(text)\n    # The polarity score is a float within the range [-1.0, 1.0]\n    # where negative value indicates negative text and positive\n    # value indicates that the given text is positive.\n    polarity      = sent.sentiment.polarity\n        \n    # The subjectivity is a float within the range [0.0, 1.0] where\n    # 0.0 is very objective and 1.0 is very subjective.\n    subjectivity  = sent.sentiment.subjectivity\n    \n    sent          = TextBlob(text, analyzer = NaiveBayesAnalyzer())\n    classification= sent.sentiment.classification\n    pos_score = round(sent.sentiment.p_pos,2)\n    neg_score = round(sent.sentiment.p_neg,2)\n    if pos_score > neg_score:\n        clas = 'POSITIVE'\n    elif pos_score < neg_score:\n        clas = 'NEGATIVE'\n    else:\n        clas = 'NEUTRAL'\n    return clas\n\ndef polarity_classify(text):\n    sent          = TextBlob(text)\n    # The polarity score is a float within the range [-1.0, 1.0]\n    # where negative value indicates negative text and positive\n    # value indicates that the given text is positive.\n    polarity      = round(sent.sentiment.polarity, 1)\n    \n    if polarity > 0:\n        pol = 'EMOTIONAL POSITIVE'\n    elif polarity < 0:\n        pol = 'EMOTIONAL NEGATIVE'\n    else:\n        pol = 'EMOTIONAL NEUTRAL'\n        \n    return pol","1f7f7088":"pd.set_option('display.max_colwidth', -1)\ndf_4 = pd.DataFrame(ngrams4, columns=['N-grams Sentence'])\nclass_list4 = []\npol_list4 = []\nfor text in ngrams4:\n    classification = pos_neg_classify(text)\n    polarity = polarity_classify(text)\n    class_list4.append(classification)\n    pol_list4.append(polarity)\ndf_4[\"Content\"] = class_list4\ndf_4[\"Polarity\"] = pol_list4\ndf_4.to_csv(\"ngrams4sentiment.csv\",index=False)\ndf_4","45989154":"df_4_neg = df_4[df_4[\"Content\"]==\"NEGATIVE\"]\ndf_4_neg.to_csv(\"negative.csv\",index=False)\ndf_4_neg","ee9c53d0":"neg_4grams = df_4_neg[\"N-grams Sentence\"]\nneg_list = []\nclass_list10 = []\npol_list10 = []\nngrams10, counts10 = ngrams(norm_corpus,ngram_range=(10, 10))   \nfor text in ngrams10:\n    for neg_word in neg_4grams:\n        neg_index = text.find(neg_word)    \n        if neg_index != -1:\n            if text not in neg_list:\n                neg_list.append(text)\n                classification = pos_neg_classify(text)\n                polarity = polarity_classify(text)\n                class_list10.append(classification)\n                pol_list10.append(polarity)\n                \ndf_neg = pd.DataFrame(neg_list, columns=['N-grams Sentence'])\ndf_neg[\"Content\"] = class_list10\ndf_neg[\"Polarity\"] = pol_list10\ndf_neg.to_csv(\"Ngrams30sentiment.csv\",index=False)\ndf_neg","581cc07e":"old_jobposting = open('..\/input\/sample-text\/ADMINISTRATIVE ANALYST 1590 060118.txt', 'r')\ncontent1 = old_jobposting.read()","5ff869eb":"!pip install textstat","398021e9":"import textstat\nprint(\"Statistics of Revised Job Posting:\")\nnumword = textstat.lexicon_count(content1, removepunct=True)\nscore = textstat.flesch_reading_ease(content1)\nclass1 = pos_neg_classify(content1)\npol1         = polarity_classify(content1)\ncontentstat1 = pd.Series({\"Word counts\": numword, \"Readbility Score\": score, \"Content\": class1, \"Polarity\": pol1})\ncontentstat1","d2fe88fe":"revised_jobposting = open('..\/input\/revised-text-posting\/Revised posting.txt', 'rb')\ncontent2 = str(revised_jobposting.read()).replace(\"\\\\r\\\\n\",\" \").replace(\"\\\\\\'s\",\"\")\nprint(content2)","5b233650":"print(\"Statistics of Revised Job Posting:\")\nnumword = textstat.lexicon_count(content2, removepunct=True)\nscore = textstat.flesch_reading_ease(content2)\nclass2 = pos_neg_classify(content2)\npol2         = polarity_classify(content2)\ncontentstat2 = pd.Series({\"Word counts\": numword, \"Readbility Score\": score, \"Content\": class2, \"Polarity\": pol2})\ncontentstat2","533dcd1b":"**Provided documents:**","bb75604e":"**According to our findings, the job postings include many negative words in the requirement section. Therefore, below are 5 additional suggestions that can fix the sentiment problems in order to improve the diversity hiring at city of LA:**\n\n**1. Re-word your job posting to attract diversity:**\n\nThe current job postings include a big part talking about requirements, standard tests and applying process. It's like a teacher gives a long list of homeworks and assignments on the first day of class. Hence, those information will scare many job seekers away. As we mentioned in part 2, we should cut down the length of text to increase the readibility of the job postings. The testing requirements and application processes can definitely cut down from our posting. These processes can be included in a follow-up email to candidates who actually apply to the job.\n\nIn addition, many studies has found that the language you use in a job description can significantly attract or turn off diverse candidates from applying to the listed position. To attract more female candidates, avoid using too many \u201cmasculine\u201d words (ambitious, dominate, challenging ...) in the job postings. ","1165ad11":"**2. Does the length of text effects the readability?**","00a98113":"**1. How long (in words) is each job posting?**","750b4209":"In this section, we will mainly perform out text analysis using the NLTK library, which is a top statistical natural language processing toolkit for English language written in the Python.","cfc9dbee":"**Original job positing:**","a407ce44":"It's easier to say than done. Let's take all our suggestions into consideration and create a revised job postings. Below is a revised job sample:","ba499065":"**3. How can we fix this readability problem?**","b2ee8427":"After we cleaning up the text to extract only the most important information, we are going to plot our normalized texts for further analysis. First, we would plot a word clouds. It is a good visualization to show the most frequently used words in the job posting text. The more frequent a specific word appears in a source of textual data, the bigger and bolder it appears in the word cloud. We look at the frequent words because they appear on every single job posting. Hence, our analysis will have the bigger influence on each individual job posting. ","34d44f37":"1. This is very similar to the sample job class export file as it is used as a template for this text mining process.  After we have the expected csv file, we would need to export it to an actual csv file using the to_csv functions from pandas package.","6d98e173":"As we can see, the a negative readability score (-8.29) from original postings increases to a college-level readability score of 4.68 in the revised posting. The contents in both remain positive with the emotional tone is changed from neutral to positive in the revised version. Therefore, our suggestion does help improve the sample job posting.","ca095430":"We will use the textblob library, which has a pre-trained machine learning model, to perform our sentiment analysis. An example of the classification can be:","55028105":"1. Flesch\u2013Kincaid readability tests. (2019, May 31). Retrieved from https:\/\/en.wikipedia.org\/wiki\/Flesch\u2013Kincaid_readability_tests\n2. Natural Language Toolkit. (n.d.). Retrieved from https:\/\/www.nltk.org\/\n3. Pellarolo, M., & Pellarolo, M. (2018, February 20). Naive Bayes for Sentiment Analysis. Retrieved from https:\/\/medium.com\/@martinpella\/naive-bayes-for-sentiment-analysis-49b37db18bf8 \n4. DataScience_CityofLA. (n.d.). Retrieved from https:\/\/www.kaggle.com\/anirbank\/datascience-cityofla\n5. Simplified Text Processing. (n.d.). Retrieved from https:\/\/textblob.readthedocs.io\/en\/dev\/\n6. Garcetti, & Lapd. (2018, February 23). Mayor Garcetti and LAPD launch recruitment campaign focused on diversity. Retrieved from https:\/\/www.lamayor.org\/mayor-garcetti-and-lapd-launch-recruitment-campaign-focused-diversity","e1d1664e":"**2. Show your existing workplace diversity (or the diversity you aspire to):**\n\nGlassdoor found that 67% of job seekers use diversity is an important factor when considering companies and job offers. One of the biggest effective way to increase workplace diversity is showing diversity at your workplace itself. \nIn February of 2018, Mayor Eric Garcetti today did a really good job in creating a social media campaign that highlighted diversity in the Los Angeles Police Department and encourage more Angelenos to consider careers in the LAPD. One of the photo banner from this campaign shows different LAPD employees holding paper signs telling their unique personal stories (the picture attached below). The city of LA can work on a similar campaign with real employees from each department. \n\n![](https:\/\/storage.googleapis.com\/kagglesdsdata\/datasets\/239299\/507558\/Diversity.png?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1561828777&Signature=m4cHD6OuJzqKEHOrpi7J6UiWsGLaNNtN26WCE3OSzmlH9bWOGMqQOkpmLT%2BvlSOQx5Arh%2FOMEWXqXTqY8kwnWiUoMORXkInoEoo9GoJ%2Bs0M35cqLJVgyS5O8rWeLTaGT072H1jMsrydDWteQ4R%2FDdPTdfnn6gbM74oiSDkGCjUovmmqqDSeDsnDaKV2gd9pQosnxx0NKdE5yyC%2B9CnBUV6GNwi%2BScBIg4bfpIONMaNOS7Hrxima9BpmhoAHJ6%2FCeSZWVCSY8PjFF3HnD7alEzg2RNfdr8y0yDgEvgcMJYx2GqpRHjgUwvTpgllQJWwS4J4P7h8XluNboiFnO%2Bl31hA%3D%3D)","18877d4c":"## REFERENCES:","15d9593f":"Let's use the functions to classify whether our 4-gram term are positive ore negative:","fb3fcddc":"Our max score is 14.43, which fall in the range 0-29 (very difficult to read), and the majority of the text data are negative numbers. This can be intepreted as the job postings are beyond the highest difficulty level to read in Flesch reading-ease test. Beside the lengthy words, the job posters may include many technical jargons and difficult requirements to understand in one reading. This may scare the applicants away since it make the job position seems harder than they actually are. \n\n","a09e0290":"**3. Encourage referrals from minority employees:**\n\nCurrent employee at your organization usually has professional networks are made up of people from the similar demographic. You can leverage this diverse network advantage by encouraging minority employees (with or without bonuses) to give referrals. A great amount of referrals from minority employee will help increase your diversity hiring drastically.\n","e6a6830d":"## PART 1: JOB POSTING TEXT BULLETIN TO STRUCTURED CSV FILE:","4255b844":"Load the data of job bulletins into a List of text, also load the filenames (to be used later), derive the job position from the filename for some preliminary analysis (actual position for data dictionary will be derived from job bulletin)","9acd7b89":"First, we will need to import all the neccessary requirements:","8b0c0226":"**Creating the Parsers:**\n\nIn this section, I will create different parsers to extract different information from the text bulletin. Those information are based on the content of sample output file and the field name of data dictionary, which includes:\n1. JOB_CLASS_TITLE\n2. JOB_CLASS_NO\n3. REQUIREMENT_SET_ID\n4. REQUIREMENT_SUBSET_ID\n5. JOB_DUTIES\n6. EDUCATION_YEARS\n7. SCHOOL_TYPE\n8. EDUCATION_MAJOR\n9. EXPERIENCE_LENGTH\n10. FULL_TIME_PART_TIME\n11. EXP_JOB_CLASS_TITLE\n12. EXP_JOB_CLASS_ALT_RESP\n13. EXP_JOB_CLASS_FUNCTION\n14. COURSE_COUNT\n15. COURSE_LENGTH\n16. COURSE_SUBJECT\n17. MISC_COURSE_DETAILS\n18. DRIVERS_LICENSE_REQ\n19. DRIV_LIC_TYPE\n20. ADDTL_LIC\n21. EXAM_TYPE\n22. ENTRY_SALARY_GEN\n23. ENTRY_SALARY_DWP\n24. OPEN_DATE\n\nWe would need 24 parsers for 24 columns of the output csv file, the parsers are defined as function below:","a2cf1fd7":"After defining all the parser funcions, we would need to put the parsers in a loop so it can go through each job bullentin text file (683 postings in total) and extract the information in each posting to place them nicely in a pandas dataframe. The process is as the follows:","c47b6014":"Looking at the above job posting and comparing to the text file of the same position from the dataset, which ones would you refer to read and feel more comfortable to take action to apply for? You may already have the answer. ","8a0666d0":"**Our final revised job posting:**\n\n![Revised posting](https:\/\/storage.googleapis.com\/kagglesdsdata\/datasets\/239299\/507558\/RevisedPosing.png?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1561828811&Signature=FXMhjTk9TVKvKsRkH%2BP32rSoza8H8zkKDA%2BARzHJZKvCx1vYQa29AsKaCX24QhOw2EAwr9Xusb%2FrODchU7lK%2FMQVW0KUwNvHT9FN%2B10tnb4OW1O5ie9W2aVp0y%2Ftos%2BFm95yBm1xHvEheMnyjleQn7gN6tKTX%2B7crrh4ALrbZQdc5qUq6%2BP2SRth2cldEVie8sj%2BsfnOgekhkIWGHYxEtWn6eQvPM8fknHLjwwuzUdHBWsX%2FhOPRcaGOBVGmWpAQF281rwFqN8yB8nB%2B3bx5l%2F%2BrksBb5COvfW8MQ%2B0AySOhyvBx0gCavSFGgz%2FogptwPMjS4D224p55MCWI0%2BqDyA%3D%3D)","bc182e8d":"**5. Offer workplace flexibility:**\n\nResearch has found a strong correlation between employees\u2019 quitting and a long commute distance. Therefore, offering flexible work hours and even work from home option does not only attract more diverse candidates, it also helps reduce some operating expenses. ","5e62a568":"From the word cloud, we can see some positive and negative keywords showing up:\n\n**Positive words:** \"disability accommodation\", \"civil service\", \"open competitive\", \"equal employment\", \"employment opportunity\".\n\nThose terms provoke a positive feeling that the employers really value diversity and welcome applicants from all backgrounds, genders and situations to apply. \n\n**Negative words:** \"written test\", \"must received\", \"eligible list\", \"minimum qualification\", \"service rule\", \"reserve list\"\n\nThese terms, in contrast, give a negative impression that the employers follow very strict rules and have many requirements for the job opening. That will discourage many applicants. \n\nWe will create a n-grams function to extract longer terms and plot top 30 most frequent terms:","6ad74e05":"We also perform Lexicon normalization. The normalization process considers another type of noise in the text. For example, \"connection\", \"connected\", \"connecting\" are reduced to the common word \"connect\". It reduces derivationally related forms of a word to a common root word.","7ad2cb56":"## SENTIMENT ANALYSIS OF CITY OF LA JOB POSTINGS TO INCREASE THE DIVERSITY HIRING\n\n**Challenge:**\n\nThe City of Los Angeles faces a big hiring challenge: one third of its 50,000 workers are eligible to retire by July of 2020. The city has partnered with Kaggle to create a competition to improve the job bulletins it will use to fill all those open positions.\nThe content, tone, and format of job descriptions can influence the quality of the applicant pool. Overly-specific requirements, for example, may discourage diversity. The Los Angeles Mayor\u2019s Office wants to reimagine the city\u2019s job bulletins by using text analysis to identify where they can improve.\n\n**Goal:**\nThe goal is to convert a folder full of plain-text job postings into a structured CSV file and then to use this data to:\n1. Identify language that can negatively bias the pool of applicants; \n2. Improve the diversity and quality of the applicant pool; \n3. Make it easier to determine which promotions are available to employees in each job class.\n\n**BASICALLY, TURNING A PLAIN JOB POSTING TEXT INTO A MORE DIVERSITY ATTRACTING FLYER**\n![Final job posting](https:\/\/storage.googleapis.com\/kagglesdsdata\/datasets\/239825\/508425\/Job%20posting%20banner.png?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1561828368&Signature=X5%2BT8pPvV5DqEvdQDNTVRTG12ts8zjPY%2FkUS1ciuLGTwPfbHSeDW3JLWHTDVRa645vzBAaQ9t%2FWBV6FnjgABMwk2lT4Z68eO2sWhoItFN909HrL8GDc5gZh4%2B%2BPY7v6ILFJha579XoljQUSfo0OWmrnx%2FUIUgYiITu7dTKwlmGY0OmtCGrJMmrdbVZLUYJPcO6bT2aUnO2m0grk5Nm%2FIlPvHHCg%2FNkirlnzRDxKl10k5hqLKy%2FQ2RaweB8DFpgf0m%2B%2FsHUNVIDJHSJDFKQL%2FvqXLYNLBK0pYAXwObVZPSGLw2pG19OGp9ZV%2FzuAIBC3Fs5btby1XoIljVz7xMw7Pxw%3D%3D)\n\n\n**The Process:**\n\n[PART 1: JOB POSTING TEXT BULLETIN TO STRUCTURED CSV FILE:](https:\/\/www.kaggle.com\/vicely07\/sentiment-analysis-of-city-of-la-job-postings?scriptVersionId=16046935#PART-1:-JOB-POSTING-TEXT-BULLETIN-TO-STRUCTURED-CSV-FILE:):\n\n[PART 2: RAW TEXT ANALYSIS ](https:\/\/www.kaggle.com\/vicely07\/sentiment-analysis-of-city-of-la-job-postings?scriptVersionId=16046935#PART-2:-RAW-TEXT-ANALYSIS)\n\n[PART 3: SENTIMENT ANALYSIS](https:\/\/www.kaggle.com\/vicely07\/sentiment-analysis-of-city-of-la-job-postings?scriptVersionId=16046935#PART-3:-SENTIMENT-ANALYSIS:)\n\n[PART 4: SUGGESTIONS FOR DIVERSITY HIRING IMPROVEMENT](https:\/\/www.kaggle.com\/vicely07\/sentiment-analysis-of-city-of-la-job-postings?scriptVersionId=16046935#PART-4:-SUGGESTIONS-FOR-DIVERSITY-HIRING-IMPROVEMENT:)\n\n[PART 5: VALIDATION OF SUGGESTIONS](https:\/\/www.kaggle.com\/vicely07\/sentiment-analysis-of-city-of-la-job-postings?scriptVersionId=16046935#PART-5:-VALIDATION-OF-SUGGESTIONS:)\n\nPART 6: CONCLUSION\n\nREFERENCES\n\n**Note:**\nSince I am working on a jupyter notebook on my personal computer first, there are some incompatible sections when I uploaded my completed project to the Kaggle kernel. Some of the incompatible code sections will be executed by importing existing files from my personal computer. ","47786998":"When putting the negative 4-gram terms in context of 10-gram terms, they turn out to be all positive in content. However, we can see that the phrase \"subject review ensure minimum qualifications met candidates disqualified time determined\" is considered to be negative in emotion. It is proved to be true since the phrase sounds very demanding and make a large number of job seekers feel less confident to apply for the position. This phrase should definitely omitted from the job posting. ","f5ecbb82":"Let's see which 4-grams words are classified negative according to our machine learning model:","9136f02e":"**Polarity:** simply means emotions expressed in a sentence. The emotional scale is: Negative < 0, Neutral = 0, Positive > 0.\n\n**Subjectivity:** sentence expresses some personal feelings, views, or beliefs. We will not use this as a metric in our analysis.\n\n**Classification:** By default, the NaiveBayesClassifier uses a simple feature extractor that indicates which words in the training set (movie reviews datset) are contained in a document. The textblob model take into consideration the tone and content of the text and can classify with an accuracy of 87%. \n\nWe now will make the two functions for classification and polarity from Textblob library for later analysis on n-grams:","72a33aa9":"**1. Cleaning the data:**","ec1c8000":"In order to perform sentiment analysis, we need to clean up the raw data to extract only the essential features. With that being said, we first need to tokenize the text. Tokenization is the process of breaking down a text paragraph into smaller chunks such as words or sentence. We also need to remove unnecessary words by including them in stopword list. In this case, \"city, los angeles, and may\" are removed since they shows up as most frequent words and does not carry much weights in our classification process later.","9554e247":"## PART 2: RAW TEXT ANALYSIS ","0b629e85":"**Since our suggestion works well in improving the readbility and semtiment of the original text, let's put all our above sugestions from part 3 into a nicer and cleaner posting. Below is my detailed comments on what I changed from the original text:**\n\n![](![image.png](attachment:image.png)https:\/\/storage.googleapis.com\/kagglesdsdata\/datasets\/239299\/507558\/RevisedPostingwithComment.png?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1561320217&Signature=Ege8Bw%2FgzPqqGGgdJQolRpzRvEl%2FQmvkwDWoybKJ05n15y4X5sQqKjNgZDoDePzFYeeM6wRwzR1KSDqKC6tBg40FJ58KTEjuBVi16CdVkds23a3lfbsEU27%2FqkJZoz4pHSL%2Fi7CSJyyd%2FQyWPu28Um%2FrbIKupArrFXJqZnm2%2FddjKZUQhuvTl3lsm6Y3xRUVAnUn10yCOzTC8p%2FaAOOw5fc3tHrn8Ugzgf5deaM4aEkTo71M11nVjBnfpUPh7m%2BBxMH9J8tkLmjiDBvAMWF%2Fbyfq410QT6bW5E9lpf16ducG9xH%2Bc%2BsOFonxrY3%2FfIoD8eYA%2F7Ln6WahB%2Bsjnx0EiA%3D%3D)","10cc7104":"## PART 3: SENTIMENT ANALYSIS:","4aa62ece":"**4. Call out inclusive benefits like parental leave and childcare subsidies:**\n\nBeside salary, company benifits is the second key point that an average job seekers scan through when they read a job posting. Thereforem bring those two important pieces of information to the top of the page. Good salary, paid parental leave, childcare subsidies, paid family sick time, and even health insurance can go a long way toward supporting and attracting diversity and inclusion. It also shows the company actually care about the well-being of their employees. ","ecefb1ea":"It's quite a time-consuming process, let's see our final dataframe:","93503de6":"One research study discovered that **the average jobseeker spends just 49.7 seconds reviewing a listing before deciding it\u2019s not a fit.** Therefore, here're some suggestions to fix this problem: \n\n**Trimming down the job requirements to include only the must-haves:**\n\nStudies show that while men are likely to apply to jobs for which they meet only 60% of the qualifications, women are much more likely to hesitate unless they meet 100% of the listed requirements. When posting a postion, hiring managers might have a long list of qualifications for the role. However, in order to highlight your commitment to inclusion, it\u2019s important to narrrow down to lists to only the must-haves, because many of the qualification are in the \"preferred\" categories for a perfectly fit candidates (which are rare). By trimming down the qualification, you\u2019ll likely see an increase in applications from female candidates. If some preffered qualifications should be included in the requirement list, they should start with some soften words such as \"familiar with\", \"plus points for\", or \"if you have one of these skills\".\n\n**Avoid using jargon:**\n\nJargon and unnecessary coporate speaking language should be avoided at all cost because the job postings are written for a larger pool of general audiences. Many jargons includes terms like KPIs, procurement,P&L... Since the candidates can't understand fully the jargon and coporate terms, these word choices can make some candidates feel less qualified for a position. Jargon and corporate language in job postings will also turn off many talented young people since the posting coporation sounds like a conservative, old-fashioned institution. This problem can be easily fixed by rephrasing the words. \n\n*For example, instead of using technical, broad business terms in job description like \"core competency\", \"detail-oriented\", \"interpersonal\", aim for more universal wording, like \u201cpays attention to details\u201d or \u201cpersonable with customers.\u201d*","ad7a69f1":"We also need to create a function to filter a sequence of spans so they don't contain overlaps and another one to merge entities and noun chunks into one token:","37bcc2de":"As we can see, the terms of \"minimum qualifcations\", \"acquired\" and \"origin sex age\" are considered to be negative in content and all of them are neutral in emotion. Let's furture analyze those negative 4-gram terms by finding these terms in a list of 10-grams. This way, we can the full context where the terms are in and can have a better analysis of thoese terms:","c74d6356":"## PART 6: CONCLUSION:","1ac8163f":"## PART 4: SUGGESTIONS FOR DIVERSITY HIRING IMPROVEMENT:","3438936a":"**2. Charts of most frequent words:**","1834ae34":"In our test, we will calculate our readibility score based on the Flesch reading-ease test. The formula for the Flesch reading-ease score (FRES) test is\n\n![image.png](https:\/\/storage.googleapis.com\/kagglesdsdata\/datasets\/239843\/508454\/readscore.png?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1561828906&Signature=Vr%2BYPRetCVGLOUhdp6IM6fSIAHUkLeZQ%2FGlYICj4smiIRM7DVZlbX4aMwpy%2BhDwLiimCScn0j4AcPfjSnW5p4aVBJUh016eBQg5DeqVdIC92WhERD7qsFj1e%2BPMdscBGPIL%2FkVT8mYGse%2BtioiH%2FhoZlNzBte36IuH8NjnFriHHsxR7yf950hDEZ0mgYeF1ldNL1Z5ublPtMDVQqaOvFgxHL%2B1xuLq2qLuV23jklc%2BSRdbO8RNnAHw9LHgciTXTbZojlJSewuHtajPLY%2F9BNUG2xU0sCXQYGCl9%2FCxckSkUByMyoGS4qXqGMNpL1Oxhiy9%2ByaqPA%2F4c%2FLEX8bNWXMQ%3D%3D)\n\nHigher scores indicate material that is easier to read; lower numbers mark passages that are more difficult to read. The following table can be helpful to assess the ease of readability in a document.\nThe table is an example of values. \n\n*Note: While the maximum score is 121.22, there is no limit on how low the score can be. A negative score is valid.*","85508e06":"## PART 5: VALIDATION OF SUGGESTIONS:","bc56d7d5":"![Original posting](https:\/\/storage.googleapis.com\/kagglesdsdata\/datasets\/240430\/509531\/original%20posting.png?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1561828837&Signature=K3IyMbL9qSttVVoaJce7zJ6k%2FB0hORZleMiT5yoE1ab1AgfQAMW0YUxKhnWsGa%2FYKk0GsCblJzHxa7s8qGMtjGcaZSTiGqANqcrq6NvEx%2B7SvFfiias7EtPec5hvBP3%2BXxA2P2wg2JJKVHC3H9ZwOtodm1IS1R4nD4KbWWP9%2BDF9hNhzGI0f4nSrwUITpZTfUUL8Pm8QmYrcOuqyYV%2BxQtK%2FTafKsGBvr6V7rAmOmOE3n6oQHPb8gB0BSdfH7%2BZJYyg2qE%2B85rXlr3%2FWOALqO6%2FO86VJz7FVpkUYRkKOH6n%2FOzSXtp7QcEFxtOm7rxf%2FAoGtg952jG39bKFXMYuQGw%3D%3D)","64536333":"As we can see in the histogram and the five-number summary table, the job posting is quite lenthy in words. The shortest text has 723 words and the longest reach to 3070 words. Looking at the quantiles, we can see that less than 25% is under 1000 words and a majority of the job postings go over 1273.5. That lead to our next analysis quesion.","570af656":"In summary, we first extract the information from the plain text job postings to create a csv table. Then, we use this new csv table and the normalized text files for the purpose of calculating ngrams of most frequently appeared words, plotting the distribution of text length, word clouds and sentiment analysis of the ngrams. From our analytics, we can see that the job posting texts are lenghty in words, have low readability scores, contain many negative words. \nBy focusing on fixing the length of the text, filtering out the negative words, adding a some other suggestions on promoting the company's mission and commitment to diversity hiring, we then validate our suggestions by running the metrics from our analysis on the new text. In our last step, we add graphic and reformat the plain text into a neat and clean job posting. This new job posting can play a key part helping the city of LA to meet their goal in diversity hiring the future. ","5ad2986e":"Luckily, the function of calculating Flesch reading-ease score is already built in the textstat package. We are going to use this library to analyze the readability of our raw text:"}}