{"cell_type":{"b8665b2f":"code","6238fe31":"code","782417f6":"code","60f3f0bf":"code","845d4d89":"code","37fe7164":"code","88e9a5f0":"code","64eb7e77":"code","abbc178a":"code","e1b3f02d":"code","d4b88c47":"code","7eaa2fc3":"code","2220271f":"code","674d3cfa":"code","c9e0097b":"code","1c4e89cb":"code","95ad6ce9":"markdown","1a490adc":"markdown","730abef0":"markdown","aa6bf09a":"markdown","2f99ecd4":"markdown","112c955b":"markdown","53c85b4d":"markdown","5241672f":"markdown","70529518":"markdown"},"source":{"b8665b2f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport re\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom skopt import BayesSearchCV\nimport xgboost as xgb\nimport lightgbm as lgb \nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.metrics import accuracy_score,f1_score,roc_auc_score,confusion_matrix,roc_curve\n\nimport collections\n\n%matplotlib inline \n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6238fe31":"# Helper Functions\n\ndef extract_salutation(x):\n    salutation = x.split(\",\")[1].split(\".\")[0]\n    salutation = salutation.strip()\n    salutation = salutation.replace(\".\", \"\")\n    return salutation\n\ndef clean_salutation(x):\n    if x not in ['Mr', 'Miss', 'Mrs', 'Master', 'Dr', 'Rev']:\n        x = 'Others'\n    else:\n        x\n    return x","782417f6":"results_dict = {}","60f3f0bf":"data_train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ndata_test = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\nfull_data = data_train.append(data_test)\nprint(data_train.shape, data_test.shape, full_data.shape)","845d4d89":"def encode_data(data):\n    # Name\n    data['Salutation'] = data['Name'].apply(extract_salutation)\n    data['Salutation'] = data['Salutation'].apply(clean_salutation)\n    data['Name_length'] = data['Name'].apply(lambda x: len(x))\n\n    # Age\n    data['Age_null'] = 0\n    data['Age_null'][data['Age'].isna()] = 1\n    \n    # SibSp and Parch\n    data['Family_size'] = data['SibSp'] + data['Parch']\n    \n    # Ticket\n    data['Ticket_alphabet'] = data['Ticket'].apply(lambda x: len(re.sub('[^a-zA-Z]+', '', x)))\n    data['Ticket_length'] = data['Ticket'].apply(lambda x: len(x))\n    \n    # Cabin\n    data['Cabin_null'] = 0\n    data['Cabin_null'][data['Cabin'].isna()] = 1\n    \n    print(f\"data has shape {data.shape} after encoding\")\n    \n    return data\n\ndata_train = encode_data(data_train)","37fe7164":"y_train = data_train['Survived']\nX_train = data_train.drop(columns=['Survived'])","88e9a5f0":"categorical_cols = ['Pclass', 'Sex', 'Embarked', 'Salutation' , 'Age_null', 'Cabin_null']\nnumerical_cols = ['Age', 'Family_size', 'Fare', 'Name_length', 'Ticket_alphabet', 'Ticket_length']\n\nX_train = X_train[categorical_cols+numerical_cols]\n\nnumerical_transformer = SimpleImputer(strategy='mean')\n\ncategorical_transformer = Pipeline([\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('one_hot', OneHotEncoder(handle_unknown='ignore'))\n])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_cols),\n        ('categorical', categorical_transformer, categorical_cols)\n    ]\n)\n\nX_train_preprocessed = preprocessor.fit_transform(X_train)\n\nprint(X_train_preprocessed.shape)\n","64eb7e77":"param_grid = {\n    'n_estimators': [100,200,500,750,1000],\n    'max_depth': [3,5,7,9]\n}","abbc178a":"rf_v0 = RandomForestClassifier(n_estimators=1000, max_depth=3, random_state=0)\nrf_v0.fit(X_train_preprocessed , y_train)\n\n#extracting default parameters from benchmark model\ndefault_params = {}\ngparams = rf_v0.get_params()\n\n#default parameters have to be wrapped in lists - even single values - so GridSearchCV can take them as inputs\nfor key in gparams.keys():\n    gp = gparams[key]\n    default_params[key] = [gp]","e1b3f02d":"folds = 5\n\n# benchmark XGB model. Grid search is not performed, since only single values are provided as parameter grid.\n#However, cross-validation is still executed\nrf_v0_cv = GridSearchCV(estimator=rf_v0, scoring='roc_auc', param_grid=default_params, return_train_score=True, verbose=1, cv=folds)\nrf_v0_cv.fit(X_train_preprocessed, y_train)\n\n#results dataframe\ndf = pd.DataFrame(rf_v0_cv.cv_results_)\n\n#predictions - inputs to confusion matrix\ntrain_predictions = rf_v0_cv.predict(X_train_preprocessed)\n\n#confusion matrices\ncfm_train = confusion_matrix(y_train, train_predictions)\n\n#accuracy scores\naccs_train = accuracy_score(y_train, train_predictions)\n\n#F1 scores for each train\/test label\nf1s_train_p1 = f1_score(y_train, train_predictions, pos_label=1)\nf1s_train_p0 = f1_score(y_train, train_predictions, pos_label=0)\n\n#Area Under the Receiver Operating Characteristic Curve\ntrain_ras = roc_auc_score(y_train, rf_v0_cv.predict_proba(X_train_preprocessed)[:,1])\n\n#best parameters\nbp = rf_v0_cv.best_params_\n\n#storing computed values in results dictionary\nresults_dict['rf_v0_cv'] = {\n    'cfm_train': cfm_train,\n    'train_accuracy': accs_train,\n    'train F1-score label 1': f1s_train_p1,\n    'train F1-score label 0': f1s_train_p0,\n    'train roc auc score': train_ras,\n    'best_params': bp\n}","d4b88c47":"# # Run this part to check on feature importance\n# feature_importance = pd.DataFrame({\n#     'features': list(preprocessor.transformers_[1][1][1].get_feature_names(categorical_cols)) + numerical_cols,\n#     'importance': rf_v0.feature_importances_\n# }).sort_values(['importance'], ascending=[0])\n\n# feature_importance","7eaa2fc3":"results_dict","2220271f":"# # To try running a hyperparameter tuned model using Bayesian search CV\n\n# folds = 5\n\n# rf_v0_tune = BayesSearchCV(estimator=rf_v0, scoring='roc_auc', search_spaces=param_grid, return_train_score=True, verbose=0, cv=folds)\n# rf_v0_tune.fit(X_train_preprocessed, y_train)\n\n# #results dataframe\n# df = pd.DataFrame(rf_v0_tune.cv_results_)\n\n# #predictions - inputs to confusion matrix\n# train_predictions = rf_v0_tune.predict(X_train_preprocessed)\n\n# #confusion matrices\n# cfm_train = confusion_matrix(y_train, train_predictions)\n\n# #accuracy scores\n# accs_train = accuracy_score(y_train, train_predictions)\n\n# #F1 scores for each train\/test label\n# f1s_train_p1 = f1_score(y_train, train_predictions, pos_label=1)\n# f1s_train_p0 = f1_score(y_train, train_predictions, pos_label=0)\n\n# #Area Under the Receiver Operating Characteristic Curve\n# train_ras = roc_auc_score(y_train, rf_v0_tune.predict_proba(X_train_preprocessed)[:,1])\n\n# #best parameters\n# bp = rf_v0_tune.best_params_\n\n# #storing computed values in results dictionary\n# results_dict['rf_v0_tune'] = {\n#     'cfm_train': cfm_train,\n#     'train_accuracy': accs_train,\n#     'train F1-score label 1': f1s_train_p1,\n#     'train F1-score label 0': f1s_train_p0,\n#     'train roc auc score': train_ras,\n#     'best_params': bp\n# }","674d3cfa":"X_test = encode_data(data_test)","c9e0097b":"X_test = X_test[categorical_cols+numerical_cols]\n\nX_test_preprocessed = preprocessor.fit_transform(X_test)\n\nprint(X_test_preprocessed.shape)\n","1c4e89cb":"y_pred = rf_v0_cv.predict(X_test_preprocessed)\nsubmission = pd.DataFrame({ 'PassengerId': data_test['PassengerId'],\n                            'Survived': y_pred })\nsubmission.to_csv(\"submission.csv\", index=False)","95ad6ce9":"Final score for this version is 0.78708 (Top 10%)","1a490adc":"# Feature Engineering","730abef0":"## Hyperparameter Tuned Model","aa6bf09a":"# Clean and Process data\n- Pclass: To keep as is, one-hot encode.\n- Name: To extract and clean salutation and name length. One-hot encode salutation. \n- Sex: To keep as is, one-hot encode.\n- Age: To create dummy variable for null age value and keep non-null values as is. \n- SipSp and Parch: Combine both to obtain new variable for family size.\n- Ticket: Get number of alphabets in ticket and length of ticket.\n- Fare: To keep as is.\n- Cabin: To create dummy variable for null cabin value.\n- Embarked: To keep as is, one-hot encode. Impute null values with most frequent.","2f99ecd4":"# Preamble\n\nThe focus on this notebook will be on modelling and achieving the highest score possible (without data leakage).\nEda Notebook can be found here: https:\/\/www.kaggle.com\/reubenlee\/titanic-dataset-eda","112c955b":"# Submission","53c85b4d":"# Import data","5241672f":"# Random Forest ","70529518":"## Base Model"}}