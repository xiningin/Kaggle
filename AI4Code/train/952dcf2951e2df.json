{"cell_type":{"4d068f30":"code","59daa70c":"code","40ff8a26":"code","73c16081":"code","67b00c88":"code","ac838b3c":"code","df2c5a7c":"code","70dfc6d2":"code","66e3e695":"code","c5ef6556":"code","e7fc3c4f":"code","7544645c":"code","c47f4f8d":"code","3d04542b":"code","ae640d29":"code","d73bf400":"code","fadca6f8":"code","83f73d69":"code","6072f226":"code","c96e7e90":"code","175d33a4":"code","cfa9dd10":"code","74fa3a78":"code","424390e5":"code","e6b10904":"code","179e4663":"code","4c7f2ef0":"code","25f4551a":"code","9029c69c":"code","37f2382e":"code","5010cf1e":"markdown","021bde61":"markdown","a50d60e2":"markdown","adfcb536":"markdown","4a891ca7":"markdown","a5ca63b1":"markdown","97eed75f":"markdown","230f527c":"markdown","b224b052":"markdown","7627c076":"markdown","5360ffb2":"markdown","ec20e7fe":"markdown","1616bcc6":"markdown","30b5e8f9":"markdown","6caa5c64":"markdown","64315782":"markdown","868b75a3":"markdown","1dbe853d":"markdown","e6e01cf1":"markdown","efe1ab70":"markdown","2de04417":"markdown","f3d3c31e":"markdown","d0fa0e9a":"markdown","4af073d8":"markdown"},"source":{"4d068f30":"# some basic libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom string import punctuation\nfrom scipy.sparse import hstack\nimport warnings\n\n# natural language toolkit\nimport nltk\nnltk.download( 'popular' )\nnltk.download( 'rslp' )\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import RSLPStemmer\n\n# machine learning libraries\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils import np_utils\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.metrics import f1_score\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn import svm\nfrom xgboost import XGBClassifier","59daa70c":"df_order_reviews = pd.read_csv(\"..\/input\/brazilian-ecommerce\/olist_order_reviews_dataset.csv\")\ndf_order_reviews.head()","40ff8a26":"important_columns = ['review_comment_message', 'review_comment_title', 'review_score']\ndf_order_reviews_imptt_cols = df_order_reviews[important_columns]\ndf_order_reviews_imptt_cols.head()","73c16081":"print( 'Total of lines                         = {0:,.0f}'.format( df_order_reviews_imptt_cols.shape[0] ))\nprint( 'Total of lines without comment message = {0:,.0f}'.format( df_order_reviews_imptt_cols['review_comment_message'].isna().sum() ))\nprint( 'Total of lines without comment title   = {0:,.0f}'.format( df_order_reviews_imptt_cols['review_comment_title'].isna().sum() ))","67b00c88":"df_order_reviews_imptt_cols_na = df_order_reviews_imptt_cols.dropna(subset=['review_comment_message', 'review_comment_title']) # drop messages and titles nans\nprint( 'Total of lines left = {0:,.0f}'.format( df_order_reviews_imptt_cols_na.shape[0] ))","ac838b3c":"df = df_order_reviews_imptt_cols_na.copy()","df2c5a7c":"warnings.filterwarnings('ignore')\n\n# tokinize\ndf['tok_message'] = df.apply(lambda row: word_tokenize(row['review_comment_message'], language='portuguese'), axis=1)\ndf['tok_title'] = df.apply(lambda row: word_tokenize(row['review_comment_title'], language='portuguese'), axis=1)\nprint( 'Words examples: {0}'.format( df.iloc[0, -2] ))","70dfc6d2":"# remove stopwords\nstw = stopwords.words('portuguese')\nprint( 'Stopword examples: {0}'.format( stw[:8] ))\n\nponc = list( punctuation )\nprint( 'Punctuation examples: {0}'.format( ponc[:8] ))\n\nremove_words = set( stw + ponc )\ndf['tok_message_stw'] = df.apply(lambda row: [word for word in row['tok_message'] if word not in remove_words], axis=1)\ndf['tok_title_stw'] = df.apply(lambda row: [word for word in row['tok_title'] if word not in remove_words], axis=1)\nprint( 'Words with out stopwords examples: {0}'.format( df.iloc[0, -2] ))","66e3e695":"# stemmer words\nstemmer = RSLPStemmer()\ndf['stem_tok_message'] = df.apply(lambda row: [ stemmer.stem( word ) for word in row['tok_message_stw'] ], axis=1)\ndf['stem_tok_title'] = df.apply(lambda row: [ stemmer.stem( word ) for word in row['tok_title_stw'] ], axis=1)\nprint( 'Stemmer examples: {0}'.format( df.iloc[0, -2] ))","c5ef6556":"df.head()","e7fc3c4f":"# vectorize words (create dictionary)\nmessage_words = []\ntitle_words = []\nfor i in range( df.shape[0] ):\n    for j in range( len(df.iloc[i, -2]) ):\n        message_words.append( df.iloc[i, -2][j] )\n    for j in range( len(df.iloc[i, -1]) ):\n        title_words.append( df.iloc[i, -1][j] )\n\nvectorizer_message = CountVectorizer( analyzer='word' )\nvectorizer_title = CountVectorizer( analyzer='word' )\n\nvectorizer_message.fit( message_words )\nvectorizer_title.fit( title_words )\n\nprint( 'Amount of words in dictionary for messages = {0}'.format( len( vectorizer_message.vocabulary_ ) ))\nprint( 'Amount of words in dictionary for titles = {0}'.format( len( vectorizer_title.vocabulary_ ) ))\nprint( 'Dictionary for messages examples:\\n{0}'.format( list( vectorizer_message.vocabulary_.items() )[:8] ))\nprint( 'Dictionary for titles examples:\\n{0}'.format( list( vectorizer_title.vocabulary_.items() )[:8] ))\n\n\nwarnings.filterwarnings('default')","7544645c":"messages = df['review_comment_message'].values # input 1\ntitles = df['review_comment_title'].values # input 2\nscores = df['review_score'].values # output\nscores -= 1 # transform values 1 to 5, to 0 to 4\n\nprint( 'Total number of lines = {0:,.0f}'.format( messages.shape[0] ))","c47f4f8d":"scores_dummy = np_utils.to_categorical(scores)\nsize_scores = len(scores_dummy[0])\nprint( 'Scores examples = {0}'.format( scores[:5] ))\nprint( 'Dummy scores examples = {0}'.format( scores_dummy[:5] ))","3d04542b":"x_train_m, x_test, y_train, y_test = train_test_split( messages, scores_dummy, test_size=0.3, random_state=42 )\nx_val_m, x_test_m, y_val, y_test = train_test_split( x_test, y_test, test_size=0.5, random_state=42 )\nx_train_t, x_test, y_train, y_test = train_test_split( titles, scores_dummy, test_size=0.3, random_state=42 )\nx_val_t, x_test_t, y_val, y_test = train_test_split( x_test, y_test, test_size=0.5, random_state=42 )\n\nprint('Total number of lines for training   = {0:,.0f}'.format( x_train_m.shape[0] ))\nprint('Total number of lines for validation = {0:,.0f}'.format( x_val_m.shape[0] ))\nprint('Total number of lines for testing    = {0:,.0f}'.format( x_test_m.shape[0] ))","ae640d29":"bow_x_train_m = vectorizer_message.transform( x_train_m )\nbow_x_train_t = vectorizer_title.transform( x_train_t )\nbow_x_train = hstack([bow_x_train_m, bow_x_train_t], format='csr') # join the two bows\n#bow_x_train = bow_x_train_m # use only the commentaries\n\nbow_x_val_m = vectorizer_message.transform( x_val_m )\nbow_x_val_t = vectorizer_title.transform( x_val_t )\nbow_x_val = hstack([bow_x_val_m, bow_x_val_t], format='csr') # join the two bows\n#bow_x_val = bow_x_val_m # use only the commentaries\n\nbow_x_test_m = vectorizer_message.transform( x_test_m )\nbow_x_test_t = vectorizer_title.transform( x_test_t )\nbow_x_test = hstack([bow_x_test_m, bow_x_test_t], format='csr') # join the two bows\n#bow_x_test = bow_x_test_m # use only the commentaries\n\nprint('Bag of words format for training   = {0}'.format( bow_x_train.shape ))\nprint('Bag of words format for validation = {0}'.format( bow_x_val.shape ))\nprint('Bag of words format for testing    = {0}'.format( bow_x_test.shape ))","d73bf400":"model = Sequential()\n\nmodel.add( Dense( 2048, activation='relu', input_shape=( bow_x_train.shape[1], ))) # input layer\nmodel.add( Dense( size_scores, activation='softmax' )) # output layer\n\nmodel.compile( Adam(), loss='categorical_crossentropy', metrics=['accuracy'] )\n\nmodel.save_weights( 'initial_weights.h5' )\n\nmodel.summary()","fadca6f8":"history = model.fit( bow_x_train, y_train, epochs=50, validation_data=( bow_x_val, y_val ), verbose=2)","83f73d69":"def plot_training( history ):\n    plt.plot( history.history['loss'], 'bo', label='Training', color='green' )\n    plt.plot( history.history['val_loss'], 'bo', label='Validation' )\n    plt.title( 'Model loss function' )\n    plt.xlabel( 'Epoch' )\n    plt.ylabel( 'Loss' )\n    plt.legend()\n    plt.show()\n\n    plt.plot( history.history['accuracy'], 'bo', label='Training', color='green' )\n    plt.plot( history.history['val_accuracy'], 'bo', label='Validation' )\n    plt.title( 'Model accuracy' )\n    plt.xlabel( 'Epoch' )\n    plt.ylabel( 'Accuracy' )\n    plt.legend()\n    plt.show()\n    \n    test_metrics = model.evaluate( bow_x_test, y_test )\n    print('Loss function in test = {0:.4f}\\nAccuracy in test = {1:.4f}'.format( test_metrics[0], test_metrics[1] ))\n\nplot_training( history )","6072f226":"model.load_weights( 'initial_weights.h5' )","c96e7e90":"history = model.fit( bow_x_train, y_train, epochs=1, validation_data=( bow_x_val, y_val ), verbose=2)","175d33a4":"plot_training( history )","cfa9dd10":"predict_train = np.argmax( model.predict( bow_x_train ), axis=1 )\npredict_val = np.argmax( model.predict( bow_x_val ), axis=1 )\npredict_test = np.argmax( model.predict( bow_x_test ), axis=1 )\n\nprint( 'F1 Score for NN in training   = {0:.4f}'.format( f1_score( np.argmax( y_train, axis=1 ), predict_train, average='weighted' )))\nprint( 'F1 Score for NN in validation = {0:.4f}'.format( f1_score( np.argmax( y_val, axis=1 ), predict_val, average='weighted' )))\nprint( 'F1 Score for NN in testing    = {0:.4f}'.format( f1_score( np.argmax( y_test, axis=1 ), predict_test, average='weighted' )))","74fa3a78":"test1_m = 'N\u00e3o gostei e nunca vou recomendar'\ntest1_t = 'Horr\u00edvel'\n\ntest2_m = 'N\u00e3o assistiria de novo'\ntest2_t = 'Bem fraco'\n\ntest3_m = 'N\u00e3o impressiona nem decepciona'\ntest3_t = 'Mais ou menos'\n\ntest4_m = 'Achei bem bacana'\ntest4_t = 'Bom'\n\ntest5_m = 'Pretendo assistir mais vezes de t\u00e3o bom'\ntest5_t = 'Maravilhoso'\n\ntest_list_m = [ test1_m, test2_m, test3_m, test4_m, test5_m ]\ntest_list_t = [ test1_t, test2_t, test3_t, test4_t, test5_t ]\n\ntest_vec_m = vectorizer_message.transform( test_list_m )\ntest_vec_t = vectorizer_title.transform( test_list_t )\ntest_vec = hstack( [test_vec_m, test_vec_t], format='csr' ) # join the two bows\n\ntest_pred = model.predict( test_vec )\n\ntest_pred_max = np.argmax( test_pred, axis=1 ) + 1\n\nfor i in range( len(test_list_m) ):\n    print('Test {0} model predicted {1}'.format( i, test_pred_max[i] ))","424390e5":"x_train, x_test, y_train, y_test = train_test_split( messages, scores, test_size=0.3, random_state=42 )\nx_val, x_test, y_val, y_test = train_test_split( x_test, y_test, test_size=0.5, random_state=42 )\n\ny_train = y_train.astype('int')\ny_val = y_val.astype('int')\ny_test = y_test.astype('int')","e6b10904":"# Multinomial Naive Bayes\nnb_clf = MultinomialNB()\nnb_clf.fit( bow_x_train, y_train )","179e4663":"predict_train = nb_clf.predict( bow_x_train )\npredict_val = nb_clf.predict( bow_x_val )\npredict_test = nb_clf.predict( bow_x_test )\n\nprint( 'F1 Score for MNB in training   = {0:.4f}'.format( f1_score( y_train, predict_train, average='weighted' )))\nprint( 'F1 Score for MNB in validation = {0:.4f}'.format( f1_score( y_val, predict_val, average='weighted' )))\nprint( 'F1 Score for MNB in testing    = {0:.4f}'.format( f1_score( y_test, predict_test, average='weighted' )))","4c7f2ef0":"# Support Vector Machines Multi-class Classification\nsvm_clf = svm.SVC()\nsvm_clf.fit( bow_x_train, y_train )","25f4551a":"predict_train = svm_clf.predict( bow_x_train )\npredict_val = svm_clf.predict( bow_x_val )\npredict_test = svm_clf.predict( bow_x_test )\n\nprint( 'F1 Score for SVM in training   = {0:.4f}'.format( f1_score( y_train, predict_train, average='weighted' )))\nprint( 'F1 Score for SVM in validation = {0:.4f}'.format( f1_score( y_val, predict_val, average='weighted' )))\nprint( 'F1 Score for SVM in testing    = {0:.4f}'.format( f1_score( y_test, predict_test, average='weighted' )))","9029c69c":"# XGBoost Classifier\nxgb_clf = XGBClassifier(learning_rate = 0.01,\n                         max_depth = 10, \n                         n_estimators = 1000,\n                         objective = 'binary:logistic',\n                         verbosity =0,\n                         seed = 42,\n                         reg_lambda = 8,\n                         reg_alpha = 2,\n                         gamma = 5,\n                         subsample= 0.8,\n                         #tree_method = 'gpu_hist'\n                         )\nxgb_clf.fit( bow_x_train, y_train )","37f2382e":"predict_train = xgb_clf.predict( bow_x_train )\npredict_val = xgb_clf.predict( bow_x_val )\npredict_test = xgb_clf.predict( bow_x_test )\n\nprint( 'F1 Score for XGB in training   = {0:.4f}'.format( f1_score( y_train, predict_train, average='weighted' )))\nprint( 'F1 Score for XGB in validation = {0:.4f}'.format( f1_score( y_val, predict_val, average='weighted' )))\nprint( 'F1 Score for XGB in testing    = {0:.4f}'.format( f1_score( y_test, predict_test, average='weighted' )))","5010cf1e":"### Creating a dictionary for the words\n\nComputers don't understand words, only numbers, because of that, we will transform each word into a number, creating a dictionary of words so the machine learning (ML) model can understand it. Before doing that, we will pass through a lot of steps trying to remove as many words as we can, so our ML model can understand our language easier and processes the sentences faster.\n\nAs we have two columns of text, we will create a dictionary for each of them, so when we use as input to the model, a word in one column will have a different impact to the same word in the other column.\n\nFirst we separate each word in each row from our dataset, a processes called tokenization.","021bde61":"### Training the model\n\nTime to train","a50d60e2":"### Evaluating the model\n\nLet\u00b4s check if our model got a good training.","adfcb536":"With the input and output separated, we need to split the data to use to train, validate and test our model, and we will do that by using 70% do training, 15% to validation and 15% for testing.","4a891ca7":"With all the words sparated and stemmed, we will create our dictionary, where each word will be translated to an integer number.","a5ca63b1":"Because our model is a multiclass classification, and we have to differentiate from 5 scores, is needed to create a dummy variable as output, that is to transform a number 2 in a list [0, 0, 1, 0, 0], so the model can understand and process it. ","97eed75f":"### Preparing the data\n\nNow that we have ur dictionary of words, we will prepare our data to use in the ML model.\n\nFirst we separate input(commentaries), what the model sees, from output(score), what the model will predict.","230f527c":"### References\n\n**Commonly used libraries**\n- https:\/\/pandas.pydata.org\/docs\/\n- https:\/\/numpy.org\/\n- https:\/\/matplotlib.org\/\n\n**Natural Language Toolkit**\n- https:\/\/www.nltk.org\/\n- https:\/\/www.guru99.com\/tokenize-words-sentences-nltk.html\n- https:\/\/www.geeksforgeeks.org\/removing-stop-words-nltk-python\/\n- https:\/\/www.datacamp.com\/community\/tutorials\/stemming-lemmatization-python\n\n**Scikit-learn**\n- https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.CountVectorizer.html\n- https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.train_test_split.html\n\n**Neural Network with Tensorflow Keras**\n- https:\/\/www.tensorflow.org\/guide\/keras\/sequential_model\n\n**F1 Score**\n- https:\/\/machinelearningmastery.com\/classification-accuracy-is-not-enough-more-performance-measures-you-can-use\/#:~:text=F1%20Score,the%20precision%20and%20the%20recall\n\n**Models with Scikit-learn**\n- https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.naive_bayes.MultinomialNB.html\n- https:\/\/scikit-learn.org\/stable\/modules\/svm.html#multi-class-classification\n\n**XGBoost**\n- https:\/\/xgboost.readthedocs.io\/en\/latest\/python\/python_api.html#xgboost.XGBClassifier","b224b052":"Third and last step, we will reduce each word to its word stem removing inflection and derivation, a processes called stemming.","7627c076":"### Creating a Neural Network model\n\nNow that all the data is good to be used, we will create our model, defining our input and output layers, the optmizer(Adam), the loss function(categorical_crossentropy) and our metrics(accuracy).","5360ffb2":"### Dropping nan lines\n\nOur machine learning will be based on the commentaries columns, so if there are no values on those, we will remove those lines.","ec20e7fe":"Second we remove stopwords, this are words that don\u00b4t have much meaning in the sentence","1616bcc6":"After that we can now test the models.\n\nThe first one that we will test is the Multinomial Naive Bayes.","30b5e8f9":"### Importing reviews file\n\nWe use the pandas library ti import and manipulate the reviews file.","6caa5c64":"Let\u00b4s now do some testing by writing something.","64315782":"### All the tests\n\nBefore having the results that you saw above, I have tested a lot of different variations to this algorithm, and you can see all the results shown below (the values in the Neural Networks where taken after 50 epochs, and the values on their names are the number of neurons for the input and hidden layers), where the columns means:\n- Message: True if used the column \"review_comment_message\"\n- Title: True if used the column \"review_comment_title\"\n- Drop Na Title: True if not only dropped the lines where messages where na but the lines where title where na too\n- Amount of Output: True if the amount of cases in the output where close to each other\n- Model: model used\n- F1 Score Test: the F1 Score after training in the test\n\n![image.png](attachment:image.png)","868b75a3":"### Getting only the columns that will be used\n\nSome column in the file are not important in this context, so we will drop them and use only the commentaries and score columns.","1dbe853d":"The second one is the Support Vector Machine(SVM) for Multi-class Classification","e6e01cf1":"# Predicting Score from Commentaries in E-Commerce\n\nThis notebook purpose is to use some machine learning techniques in a brazilian ecommerce dataset, and used the https:\/\/www.kaggle.com\/marcelolafeta\/olist-data-preparation as a start\n\n### Importing libraries\n\nFirst me will import all the libraries that will be used in this notebook.","efe1ab70":"The third and last one will be the XGBoost Classifier","2de04417":"Now we can see that we have a much better result than before, and it can predict almost 70% of the scores just by looking the commentaries.\n\nLet\u00b4s check the F1 Score metric. This metric is calculated from two other metrics:\n- **precision**: is the number of True Positives divided by the number of True Positives and False Positives. Put another way, it is the number of positive predictions divided by the total number of positive class values predicted\n- **recall**: is the number of True Positives divided by the number of True Positives and the number of False Negatives. Put another way it is the number of positive predictions divided by the number of positive class values in the test data\n\nConfusion matrix:\n![image.png](attachment:image.png)\n\nThe F1 Score is equal to **2 * ( ( precision * recall ) \/ ( precision + recall ) )**, and this metric is good to see not only where the model got the right answer, but where it got wrong too.","f3d3c31e":"Our input still is a bunch of text, so we will use our dictionary to translate the words into numbers, and not only that, but we will use the same principle of dummy variable that we used in the output, where the word that corresponds to the number 2 will have a value of 1 in a list with the dictionary size, so if the sentence have the words 1, 4 and 5, the input for that sentence will be [0, 1, 0, 0, 1, 1, 0, ... , 0]","d0fa0e9a":"As we can see, the model overfitted, that means that it is doing good in the training data, but it did not generalized good enough to have the same result in the validation data. That is common in a NLP problem when we don\u00b4t have much data, and a solution for that is to cut the training in the first epochs, where the loss function in the training is almost equal to the validation.\n\nTo do that we will load the initial weights and train the model for just 1 epoch.","4af073d8":"### Trying different models\n\nWe can try to get a better result by testing other models, so let\u00b4s do that.\n\nFirst, with this others models, we do not need to use a dummy output, so we will prepare the data again."}}