{"cell_type":{"3709f7d8":"code","c8b027d0":"code","d79c1681":"code","55c2c0ab":"code","48f8878e":"code","07e8d758":"code","2a7543c9":"code","21cd26b2":"code","8025161f":"code","8f695bb3":"code","64154ae4":"code","a7d7319a":"code","60947742":"code","9eecc6a0":"code","eac9916e":"code","1330f6bb":"code","00a5620b":"code","b8a90741":"code","1c0b1389":"code","b3ffdfc2":"code","5acf151a":"code","4b75468b":"code","018d6438":"code","34e976a3":"code","5ee993f4":"markdown","5ddfa84f":"markdown","447b6a5d":"markdown","6874f469":"markdown","ce2f3512":"markdown","8cdc195a":"markdown","492f7bd9":"markdown","95e3475e":"markdown","5ce32dab":"markdown","10422d81":"markdown","a870061e":"markdown"},"source":{"3709f7d8":"# Visualiza o scatterplot de duas dimens\u00f5es\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom bokeh.sampledata.iris import flowers as floresdf\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Quantos grupos existem nos dados\nplt.scatter(x=floresdf.petal_length, y=floresdf.petal_width)\nplt.xlabel('Largura da P\u00e9tala')\nplt.ylabel('Altura da P\u00e9tala')\nplt.title('Quantos CLUSTERS existem aqui?')\nplt.show()","c8b027d0":"# Faz o gr\u00e1fico \nsns.scatterplot(x=floresdf.petal_length, y=floresdf.petal_width, hue=floresdf.species)\nplt.show()","d79c1681":"# Visualiza em Matriz\nsns.pairplot(floresdf, hue=\"species\")\nplt.show()","55c2c0ab":"flores = floresdf.iloc[:,2:4].values","48f8878e":"# Importa a biblioteca kmeans\nfrom sklearn.cluster import KMeans\n\n# Se n\u00e3o colocarmos o n\u00famero de clusters, o valor default de n\u00famero de clusters \u00e9 de 8 clusters.\nmodelokm = KMeans(n_clusters=3)\n\n# Cria o modelo de clusteriza\u00e7\u00e3o para agrupar os dados de flores\nmodelokm = modelokm.fit(flores)\n\n# Mostra quais s\u00e3o as coordenadas de cada centroid para cada dimens\u00e3o. Temos 2 dimens\u00f5es e 3 clusters.\nmodelokm.cluster_centers_","07e8d758":"# Mostra a quantidade de clusters\nmodelokm.n_clusters","2a7543c9":"# Mostra as labels\nlabels = modelokm.labels_\nlabels ","21cd26b2":"# Faz a clusteriza\u00e7\u00e3o dos dados usando o modelo criado\ngrupos = modelokm.predict(flores)","8025161f":"# Mostra o Gr\u00e1fico de Clusters\nsns.scatterplot(x=flores[:,0], y=flores[:,1], hue=labels)\nplt.show()","8f695bb3":"# Compara com o Gr\u00e1fico anterior\nsns.scatterplot(x=floresdf.petal_length, y=floresdf.petal_width, hue=floresdf.species)\nplt.show()","64154ae4":"# Visualizar os clusters com os Centroids\nplt.scatter(flores[grupos == 0, 0], flores[grupos == 0, 1], s = 100, c = 'red', label = 'Iris-setosa')\nplt.scatter(flores[grupos == 1, 0], flores[grupos == 1, 1], s = 100, c = 'blue', label = 'Iris-versicolour')\nplt.scatter(flores[grupos == 2, 0], flores[grupos == 2, 1], s = 100, c = 'green', label = 'Iris-virginica')\nplt.scatter(modelokm.cluster_centers_[:, 0], modelokm.cluster_centers_[:,1], s = 100, c = 'yellow', label = 'Centroids')\nplt.legend()\nplt.show()","a7d7319a":"# Exemplo do c\u00e1lculo da distancia euclidiana \nimport math\nd1 = (2, 0)\nd2 = (1, 3)\ndistance = math.sqrt(sum([(a - b) ** 2 for a, b in zip(d1, d2)]))\nprint(\"Dist\u00e2ncia Euclidiana do Ponto D1 ao D2: \",distance)","60947742":"# Cross-tabulation\nimport pandas as pd\ndf1 = pd.DataFrame({'labels':labels,\"species\":floresdf['species']})\nct1 = pd.crosstab(df1['labels'],df1['species'])\nct1","9eecc6a0":"plt.title(\"KMeans\")\nsns.heatmap(ct1,annot=True,cbar=False,cmap=\"Blues\")","eac9916e":"print(modelokm.inertia_)","1330f6bb":"# Cria a Curva de Cotovelo para encontrar o Numero Ideal de Clusters\nfrom sklearn.cluster import KMeans\nwcss = []\n\nfor i in range(1, 11):\n    kmeans = KMeans(n_clusters = i, max_iter = 300, n_init = 10, random_state = 0)\n    kmeans.fit(flores)\n    wcss.append(kmeans.inertia_)\n    \n# Mostra o Gr\u00e1fico\nplt.plot(range(1, 11), wcss)\nplt.title('Curva de Cotovelo')\nplt.xlabel('Numero de Clusters')\nplt.ylabel('WCSS') #within cluster sum of squares\nplt.show()","00a5620b":"# Silhueta\nfrom sklearn.metrics import silhouette_samples, silhouette_score\n\nfor i in range(2, 11):\n    clusterer = KMeans(n_clusters=i)\n    preds = clusterer.fit_predict(flores)\n    score = silhouette_score(flores, preds)\n    print('Silhueta para ' + str(i) + ' clusters : ' + str(score))","b8a90741":"modelorm = KMeans(n_init=10)","1c0b1389":"# 1. Importa os dados de clientes do supermercado\n# 2. Visualize o scatterplot em matriz com o hue = \"Gender\". D\u00e1 pra notar algum grupo?\n# 3. Crie um DataFrame apenas com as colunas Annual Income e Spending Score\n# 4. Importe o k-means\n# 5. Crie o modelo em cluster padr\u00e3o, sem especificar a quantidade K de clusters\n# 6. Crie uma curva de cotovelo. Qual seria o n\u00famero ideal de clusters?\n# 7. Crie novamente o cluster com kmeans com a quantidade de n_clusters ajustada\n# 8. Quais foram os centroids? Imprima os centroids\n# 9. Mostra o gr\u00e1fico do resultado com os centroids\n# 10. Analise o resultado. Pra qual grupo voc\u00ea direcionaria com maior prioridade a campanha de marketing?","b3ffdfc2":"# 1. Importa os dados de clientes do supermercado\nimport pandas as pd\nperfil = pd.read_csv('..\/input\/customer-segmentation-tutorial-in-python\/Mall_Customers.csv')\n\n# 2. Visualize o scatterplot em matriz com o hue = \"Gender\". D\u00e1 pra notar algum grupo?\nimport seaborn as sns\nsns.pairplot(perfil, hue=\"Gender\")","5acf151a":"# 3. Crie um DataFrame apenas com as colunas Annual Income e Spending Score\nperfil = pd.DataFrame(perfil, columns=[\"Annual Income (k$)\", \"Spending Score (1-100)\"])\n\n# 4. Importe o k-means\nfrom sklearn.cluster import KMeans\n\n# 5. Crie o modelo em cluster padr\u00e3o, sem especificar a quantidade K de clusters\nmodelokm = KMeans()\nmodelokm = modelokm.fit(perfil)","4b75468b":"# 6. Crie uma curva de cotovelo. Qual seria o n\u00famero ideal de clusters?\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nwcss = []\nfor i in range(1, 11):\n    kmeans = KMeans(n_clusters = i, max_iter = 300, n_init = 10, random_state = 0)\n    kmeans.fit(perfil)\n    wcss.append(kmeans.inertia_)\n    \n# Mostra o Gr\u00e1fico\nplt.plot(range(1, 11), wcss)\nplt.title('Curva de Cotovelo')\nplt.xlabel('Numero de Clusters')\nplt.ylabel('WCSS') #within cluster sum of squares\nplt.show()","018d6438":"# 7. Crie novamente o cluster com kmeans com a quantidade de n_clusters ajustada\nmodelokm = KMeans(n_clusters=5)\nmodelokm = modelokm.fit(perfil)\nymodelokm= modelokm.fit_predict(perfil)\n\n# 8. Quais foram os centroids? Imprima os centroids\nmodelokm.cluster_centers_","34e976a3":"# 9. Mostra o gr\u00e1fico do resultado com os centroids\nplt.scatter(perfil.values[ymodelokm == 0, 0], perfil.values[ymodelokm == 0, 1], s = 100, c = 'red', label = 'Cluster 1')\nplt.scatter(perfil.values[ymodelokm == 1, 0], perfil.values[ymodelokm == 1, 1], s = 100, c = 'blue', label = 'Cluster 2')\nplt.scatter(perfil.values[ymodelokm == 2, 0], perfil.values[ymodelokm == 2, 1], s = 100, c = 'green', label = 'Cluster 3')\nplt.scatter(perfil.values[ymodelokm == 3, 0], perfil.values[ymodelokm == 3, 1], s = 100, c = 'cyan', label = 'Cluster 4')\nplt.scatter(perfil.values[ymodelokm == 4, 0], perfil.values[ymodelokm == 4, 1], s = 100, c = 'magenta', label = 'Cluster 5')\nplt.scatter(modelokm.cluster_centers_[:, 0], modelokm.cluster_centers_[:, 1], s = 300, c = 'yellow', label = 'Centroids')\nplt.title('Clusters of customers')\nplt.xlabel('Annual Income (k$)')\nplt.ylabel('Spending Score (1-100)')\nplt.legend()\nplt.show()\n\n# 10. Analise o resultado. Pra qual grupo voc\u00ea direcionaria com maior prioridade a campanha de marketing?","5ee993f4":"# Como avaliar o modelo?\nPara medir a qualidade do modelo de cluster. Usamos a **cross-tabulation** do pandas para comparar a clusteriza\u00e7\u00e3o com a classe de cada grupo.","5ddfa84f":"# Como descobrir o n\u00famero ideal de clusters?\nUm bom cluster tem uma baixa inertia_ (SSE) e tamb\u00e9m o menor n\u00famero de clusters. N\u00e3o queremos muitos clusters. A Curva de Cotovelo ou M\u00e9todo Elbow Curve \u00e9 uma t\u00e9cnica usada para encontrar a quantidade ideal de clusters K. Este m\u00e9todo testa a vari\u00e2ncia dos dados em rela\u00e7\u00e3o ao n\u00famero de clusters. O valor ideal de K \u00e9 aquele que tem um menor Within Sum of Squares (WSS) e ao mesmo tempo o menor n\u00famero de clusters. Chamamos de **curva de cotovelo**, porque a partir do ponto que seria o \u201ccotovelo\u201d n\u00e3o existe uma discrep\u00e2ncia t\u00e3o significativa em termos de vari\u00e2ncia. Dessa forma, a melhor quantidade de clusters K seria exatamente onde o cotovelo estaria.\n\n![](https:\/\/media.giphy.com\/media\/12vVAGkaqHUqCQ\/giphy.gif)","447b6a5d":"#### Silhueta\nA silhueta \u00e9 uma medida do quanto observa\u00e7\u00f5es mais similares est\u00e3o pr\u00f3ximas entre si e, ao mesmo tempo, o quanto est\u00e3o distantes de outros clusters diferentes. Busca maior **coes\u00e3o** dentro do cluster e maior **separa\u00e7\u00e3o** entre clusters. A silhueta vai de -1 a 1, onde o maior valor indica que a observa\u00e7\u00e3o se encaixou mais dentro do cluster e se distanciou dos outros clusters. Se a silhueta for negativa, a configura\u00e7\u00e3o do cluster pode ter ou clusters de mais ou clusters de menos. A silhueta tamb\u00e9m usa a dist\u00e2ncia euclidiana para medir a dist\u00e2ncia entre as observa\u00e7\u00f5es.\n![](http:\/\/www.mtechprojects.org\/wp-content\/uploads\/2017\/12\/silhouette-coefficient.jpg)\n\nPor exemplo:\n![](https:\/\/www.researchgate.net\/profile\/Frans_Coenen\/publication\/221570710\/figure\/fig1\/AS:670029003644935@1536758771429\/Derivation-of-the-Overall-Silhouette-Coefficient-OverallSil.png)","6874f469":"# O que \u00e9 K-MEANS?\nPara a clusteriza\u00e7\u00e3o, existem v\u00e1rios algoritmos. O mais utilizado \u00e9 o K-means. O K-means costuma ser mais aplicado em dados com poucas dimens\u00f5es de dados num\u00e9ricos e continuos para organizar os dados em grupos categ\u00f3ricos. O **K-Means** tem o objetivo de dividir as observa\u00e7\u00f5es em k clusters. K \u00e9 o n\u00famero de clusters. Esta divis\u00e3o dos dados entre os clusters tem que ser de forma que os diferentes clusters fiquem mais separados entre eles, enquanto que as observa\u00e7\u00f5es dentro de cada cluster fiquem mais pr\u00f3ximas entre elas. Para isso, \u00e9 utilizada a soma dos erros quadrados ou Sum of Squared Errors (SSE) que busca minimizar a dist\u00e2ncia entre os pontos e seu centroid. Depois que o modelo \u00e9 criado, o K-means quando \u00e9 aplicado lembra a m\u00e9dia de cada cluster, tamb\u00e9m chamada de **centroids** e, com isso, encontra o centroid mais pr\u00f3ximo a cada novo dado. O centroid representa o centro do cluster.\n\n![](https:\/\/www.saedsayad.com\/images\/Clustering_kmeans_c.png)\n\nA clusteriza\u00e7\u00e3o com K-means compreende as seguintes etapas:\n1. Inicializa\u00e7\u00e3o: a localiza\u00e7\u00e3o dos centroids s\u00e3o geradas aleatoriamente\n2. Designa\u00e7\u00e3o: os K clusters s\u00e3o criados associando cada observa\u00e7\u00e3o ao seu centroid mais pr\u00f3ximo\n3. Atualiza\u00e7\u00e3o: o centroid de cada cluster se torna a nova m\u00e9dia do cluster\nEste processo \u00e9 repetido v\u00e1rias vezes at\u00e9 convergir. \n\n![](https:\/\/stanford.edu\/~cpiech\/cs221\/img\/kmeansViz.png)","ce2f3512":"### Pr\u00f3xima Aula\n[Minera\u00e7\u00e3o de Textos](https:\/\/www.kaggle.com\/debkings\/8-minera-o-de-textos-1)","8cdc195a":"---\n# Conclus\u00e3o\nTem que se tomar cuidado ao usar clusteriza\u00e7\u00e3o com dados que n\u00e3o foram padronizados. Isto porque o K-means \u00e9 muito sens\u00edvel \u00e0 escala dos dados porque utiliza a Dist\u00e2ncia Euclidiana para medir a dist\u00e2ncia. Por isso, sempre \u00e9 importante aplicar o **Scaling** antes de usar o K-means. Outra desvantagem \u00e9 que o K-Means pode ser bem **lento com grande n\u00famero de observa\u00e7\u00f5es**. Por isso, em alguns casos, utiliza-se uma amostra dos dados e n\u00e3o o conjunto inteiro por quest\u00f5es de performance. Clusteriza\u00e7\u00e3o pode ter resultado ruim se os dados tiverrem **ru\u00eddos** (overlapping) ou outliers. Um grande n\u00famero de clusters (K) pode diminuir os erros, mas tamb\u00e9m pode ter maior risco de overfitting.\n\nComo vantagem, a clusteriza\u00e7\u00e3o n\u00e3o precisa de dados previamente classificados. \n\n# Material Complementar\n1. [K-means lecture by Andrew Ng](http:\/\/cs229.stanford.edu\/notes\/cs229-notes7a.pdf)\n2. [Some Methods for Classification and Analysis of Mutivariate Observations. J. MacQueen,1967. pp.281-297](https:\/\/books.google.com.br\/books?hl=pt-BR&lr=&id=IC4Ku_7dBFUC&oi=fnd&pg=PA281&dq=Some+methods+for+classification+and+analysis+of+multivariate+observations&ots=nOXjKWGcmO&sig=I9vBrrMQWVgLKYk577zS944OA1k)\n3. [K-Means Clustering Algorithm](http:\/\/www.labri.fr\/perso\/bpinaud\/userfiles\/downloads\/hartigan_1979_kmeans.pdf)\n4. [K-Means ++](http:\/\/ilpubs.stanford.edu:8090\/778\/1\/2006-13.pdf)\n\n## Resposta dos Exerc\u00edcios","492f7bd9":"Para os datasets que n\u00e3o possuem labels, temos que usar outras formas, como usando a **Inertia**. A Inertia corresponde ao somat\u00f3rio dos erros quadr\u00e1ticos das inst\u00e2ncias de cada cluster. Assim:\n* Mede o quanto os clusters est\u00e3o separados entre eles\n* Mede a dist\u00e2ncia de cada dado para o centroid do seu cluster\n* Aplicamos `fit()` na `inertia_` em busca de minimizar a inertia na escolha dos clusters\n* Quanto mais pr\u00f3ximos entre si e do centroid, menor a inertia","95e3475e":"# Em mais detalhes: Como calculamos a dist\u00e2ncia?\nA similaridade \u00e9 a m\u00e9trica que mostra o quanto as observa\u00e7\u00f5es de um mesmo grupo est\u00e3o pr\u00f3ximas entre si, ou seja, s\u00e3o mais similares entre si. Enquanto que ao mesmo tempo mostra o quanto as observa\u00e7\u00f5es entre grupos diferentes est\u00e3o mais distantes. \n![](https:\/\/chrisjmccormick.files.wordpress.com\/2013\/08\/2d_euclidean_distance_illustration.png)\nA Dist\u00e2ncia Euclidiana \u00e9 usada para aproximar as observa\u00e7\u00f5es ao centroid mais pr\u00f3ximo. A dist\u00e2ncia entre o centroid e a observa\u00e7\u00e3o \u00e9 calculada com a dist\u00e2ncia euclidiana. Considere os pontos D1(2,0) e D2(1,3) e D4(2,2) .Ter\u00edamos:\n![](https:\/\/sunainasblog.files.wordpress.com\/2018\/03\/untitled1.png?w=620&h=210)","5ce32dab":"**[Voltar para a P\u00e1gina Inicial do Curso](https:\/\/www.kaggle.com\/c\/ml-em-python)**\n\n# **Clusteriza\u00e7\u00e3o**\nA clusteriza\u00e7\u00e3o ou agrupamento \u00e9 uma t\u00e9cnica de aprendizado de m\u00e1quina **n\u00e3o-supervisionada**. Na cria\u00e7\u00e3o do modelo de clusteriza\u00e7\u00e3o n\u00e3o usamos a target, por isso \u00e9 chamado de n\u00e3o-supervisionado. Tamb\u00e9m n\u00e3o fazemos a separa\u00e7\u00e3o do dataset entre treino e teste. Usamos todos os dados, sem fazer divis\u00f5es das linhas dos dados. No aprendizado n\u00e3o-supervisionado, chamamos as colunas do dataset de **dimens\u00f5es** e n\u00e3o mais de atributos ou vari\u00e1veis. Por exemplo, no dataset de Flores (iris dataset) temos 4 dimens\u00f5es: altura da p\u00e9tala, largura da p\u00e9tala, altura da s\u00e9pala e largura da s\u00e9pala. \n\n![](https:\/\/blog.bismart.com\/hs-fs\/hubfs\/Imported_Blog_Media\/ClassificationAndClustering\/Clustering&clasification-Animales.gif?width=900&name=Clustering&clasification-Animales.gif)\n\nCom v\u00e1rias imagens n\u00e3o rotuladas, poder\u00edamos usar clusteriza\u00e7\u00e3o para agrupar as imagens por semelhan\u00e7a. Inclusive seria poss\u00edvel identificar as anomalias, ou seja, aqueles casos que s\u00e3o diferentes de todos os demais.\n![](https:\/\/s3-us-west-2.amazonaws.com\/static.pyimagesearch.com\/face-clustering\/face_clustering_animation.gif)\n\nA Clusteriza\u00e7\u00e3o pode ser usada tanto na prepara\u00e7\u00e3o de dados para descobrir padr\u00f5es ainda desconhecidos quanto para a cria\u00e7\u00e3o de modelos com o objetivo de criar grupos, separando os dados em grupos ainda desconhecidos. A clusteriza\u00e7\u00e3o j\u00e1 \u00e9 aplicada em diversos cen\u00e1rios, como no reconhecimento de padr\u00f5es, an\u00e1lise de imagens, no agrupamento de esp\u00e9cies, na detec\u00e7\u00e3o de anomalias, na compress\u00e3o de dados, no agrupamento de clientes, na classifica\u00e7\u00e3o de documentos, no agrupamento de not\u00edcias.\n\n### O resultado pode ser bem diferente do que voc\u00ea esperava. \nSua inten\u00e7\u00e3o era em separar as imagens entre: imagens de cachorros x imagens de comida. Mas ao inv\u00e9s disso, os clusters foram criados de outra forma e o resultado foi este. \n![](https:\/\/www.vuzo.co.uk\/wp-content\/uploads\/2018\/10\/Vuzo-Blog-chihuahuas-and-muffins.jpeg)\n![](https:\/\/pbs.twimg.com\/media\/DXGH9PZX4AADYsS.jpg:large)\n![](https:\/\/pbs.twimg.com\/media\/DCKduB-UMAAIRmX.jpg)\n![](https:\/\/cookvids.co.uk\/wp-content\/uploads\/2017\/09\/krosanai__700.jpg)\n![](https:\/\/pbs.twimg.com\/media\/Cd3RI19WIAE5Dr9.jpg)\n![](https:\/\/www.lifewithdogs.tv\/wp-content\/uploads\/2016\/03\/3.11.16-Labradoodle-Fried-Chicken4.jpg)\n\n**Conte\u00fado:**\n1. O que \u00e9 K-means?\n2. Em mais detalhes: como calculamos a dist\u00e2ncia?\n3. Como avaliar o modelo?\n4. Como descobrir o n\u00famero ideal de clusters?\n5. Model Tuning\n6. Exerc\u00edcio\n7. Conclus\u00e3o\n8. Material Complementar","10422d81":"# Model Tuning\nN\u00e3o temos a garantia que os centroids ser\u00e3o iniciados de posi\u00e7\u00f5es que levem ao melhor resultado. Podemos ter um resultado que convergiu, mas que n\u00e3o levou ao resultado \u00f3timo porque os centroids do in\u00edcio que foram designados de forma rand\u00f4mica come\u00e7aram de uma posi\u00e7\u00e3o ruim. Para aliviar este problema, realizamos a execu\u00e7\u00e3o do algoritmo v\u00e1rias vezes e isso \u00e9 controlado pelo par\u00e2metro `n_init`, que por default \u00e9 10. O que representa que o algoritmo K-means ser\u00e1 iniciado 10 vezes com pontos de in\u00edcio diferentes. ","a870061e":"---\n# Exerc\u00edcio\nVoc\u00ea consegue descobrir padr\u00f5es em uma base de dados de clientes de supermercado para direcionar o marketing para determinados grupos de clientes, com base em seu perfil de compras?"}}