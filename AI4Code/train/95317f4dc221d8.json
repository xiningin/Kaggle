{"cell_type":{"826f74c3":"code","4d35fcf2":"code","acb490d2":"code","39dfac8b":"code","894c422d":"code","9971fda3":"code","241cd0b3":"code","53eedf04":"code","aba49af4":"code","4d6bd531":"code","ac3d22aa":"code","c88863a9":"code","8d9bbfbb":"code","2adef583":"markdown","6b4773bf":"markdown","a148c982":"markdown","b901c20a":"markdown","cd82f433":"markdown","a6a95eba":"markdown","d4f7473c":"markdown","40ed0c9a":"markdown","8875cfd0":"markdown"},"source":{"826f74c3":"import numpy as np\nimport torch\nfrom torch.utils.data import DataLoader, Dataset, TensorDataset\nimport sys\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils import data\nimport torch.optim as optim\nimport seaborn as sns\nfrom collections import defaultdict\nimport time\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nfrom transformers import BertForSequenceClassification, AdamW, BertConfig\nfrom transformers import get_linear_schedule_with_warmup","4d35fcf2":"# Select CPU\/GPU\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nprint(device)","acb490d2":"'''\nLoad the BERT tokenizer.\n'''\nfrom transformers import BertTokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n\n\n'''\nDataloader \nFor Training: Returns (Tweet, Input_id, Attention_mask, label)\nFor Testing: Returns (Tweet, Input_id, Attention_mask)\n'''\nclass mydataset():    \n\n    def __init__(self, classification_df, name = 'train'):\n\n        super(mydataset).__init__()\n        self.name = name\n        self.tweet = []\n        self.Y = []\n                    \n        for index,rows in classification_df.iterrows():\n\n            tweet = rows['keyword'] + rows['location'] + rows['text']\n            self.tweet.append(''.join(tweet)) \n            \n            if name == 'train' or self.name == 'valid':\n                label = rows['target']\n                self.Y.append(label)\n\n        \n        \n        '''\n        Tokenize all of the captions and map the tokens to thier word IDs, and get respective attention masks.\n        '''\n        self.input_ids, self.attention_masks = tokenize(self.tweet)\n        \n        \n    \n    def __getitem__(self,index): \n        '''\n        For Captions, Input ids and Attention mask\n        '''\n        tweet = self.tweet[index]\n        input_id = self.input_ids[index]\n        attention_masks = self.attention_masks[index]\n        \n        \n        '''\n        For Labels during training\n        '''      \n        if self.name == 'train' or self.name == 'valid' :\n            label = float(self.Y[index])\n            \n            return tweet, input_id, attention_masks, torch.as_tensor(label).long()\n\n        \n        else:\n            return tweet, input_id, attention_masks\n        \n        \n  \n    def __len__(self):\n        return len(self.tweet)\n    \n    \n            \n'''\ntokenize all of the sentences and map the tokens to their word IDs.\n'''\n\ndef tokenize(sequences):\n    \n    input_ids = []\n    attention_masks = []\n\n    # For every caption...\n    for seq in sequences:\n        \n        encoded_dict = tokenizer.encode_plus(\n                            seq,                       # Sentence to encode.\n                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n                            max_length = 32,           # Pad & truncate all sentences.\n                            truncation=True,\n                            pad_to_max_length = True,\n                            return_attention_mask = True,   # Construct attn. masks.\n                            return_tensors = 'pt',      # Return pytorch tensors.\n                       )\n\n        # Add the encoded sentence to the list.    \n        input_ids.append(encoded_dict['input_ids'])\n\n        # And its attention mask (simply differentiates padding from non-padding).\n        attention_masks.append(encoded_dict['attention_mask'])\n\n    # Convert the lists into tensors.\n    input_ids = torch.cat(input_ids, dim=0)\n    attention_masks = torch.cat(attention_masks, dim=0)\n    \n    \n    return input_ids, attention_masks\n","39dfac8b":"'''\nShuffle and split 10 percent data for Validation set\n'''\ntrain_csv = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv', keep_default_na = False)\ntrain_csv = train_csv.sample(frac=1).reset_index(drop=True)\nninetyfive_percent  = round(0.90*(len(train_csv)))\ntrain_data = train_csv.iloc[:ninetyfive_percent]\nvalid_data = train_csv.iloc[ninetyfive_percent:]\n\nprint('Number of Training samples: ', len(train_data))\nprint('Number of Validation samples: ',len(valid_data))","894c422d":"'''\nTrain Dataloader\n''' \ntrain_dataset = mydataset(train_data, name = 'train')\ntrain_dataloader = data.DataLoader(train_dataset, shuffle= True, batch_size = 32, num_workers=16,pin_memory=True)\n\n\n'''\nValidation_Dataloader\n'''\nvalidation_dataset = mydataset(valid_data, name = 'valid')\nvalidation_dataloader = data.DataLoader(validation_dataset, shuffle= True, batch_size = 32, num_workers=16,pin_memory=True)","9971fda3":"'''\nTest Dataloader\n''' \ntest_csv = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv', keep_default_na = False)\ntest_dataset = mydataset(test_csv , name = 'test')          \ntest_dataloader = data.DataLoader(test_dataset, shuffle= False, batch_size = 1, num_workers=16,pin_memory=True)","241cd0b3":"'''\nLoad BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top.\n''' \nmodel = BertForSequenceClassification.from_pretrained(\n    \"bert-base-uncased\", #12-layer BERT model, with an uncased vocab.\n    num_labels = 2, #Number of Classes\n    output_attentions = False, # Whether the model returns attentions weights.\n    output_hidden_states = False, # Whether the model returns all hidden-states.\n)\n\nmodel.to(device)","53eedf04":"def train(model, data_loader, valid_loader, criterion, optimizer, lr_scheduler, modelpath, device, epochs):\n    \n    model.train()\n\n    train_loss= []\n    valid_loss = []\n    valid_acc = []\n\n\n    for epoch in range(epochs):\n        avg_loss = 0.0\n                \n        \n        for batch_num, (tweet, input_id, attention_masks, target) in enumerate(data_loader):\n            \n            input_ids, attention_masks, target = input_id.to(device), attention_masks.to(device), target.to(device)\n                \n            '''\n            Compute output and loss from BERT\n            '''\n            loss, logits = model(input_ids, \n                             token_type_ids=None, \n                             attention_mask=attention_masks, \n                             labels=target,\n                             return_dict=False\n                                )\n\n            '''\n            Take Step\n            '''                    \n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n\n            '''\n            Clip the norm of the gradients to 1.0. This is to help prevent the \"exploding gradients\" problem.\n            '''\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n            \n            avg_loss += loss.item()\n\n            '''\n            linear_schedule_with_warmup take step after each batch\n            '''\n            lr_scheduler.step()\n                                \n            \n        training_loss = avg_loss\/len(data_loader)\n       \n        print('Epoch: ', epoch+1)            \n        print('training loss = ', training_loss)\n        train_loss.append(training_loss)\n\n        '''\n        Check performance on validation set after an Epoch\n        '''\n        validation_loss, top1_acc= test_classify(model, valid_loader, criterion, device)\n        valid_loss.append(validation_loss)\n        valid_acc.append(top1_acc)\n\n         \n        '''\n        save model checkpoint after every epoch\n        '''\n        torch.save({\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'lr_scheduler': lr_scheduler.state_dict(),\n            }, modelpath)\n        \n    return train_loss, valid_loss, valid_acc\n\n\n\n\n\n\n'''\nFunction to perform inference on validation set\nReturns: validation loss, top1 accuracy\n'''\n\ndef test_classify(model, valid_loader, criterion, device):\n    model.eval()\n    test_loss = []\n    top1_accuracy = 0\n    total = 0\n\n    for batch_num, (tweet, input_id, attention_masks, target) in enumerate(valid_loader):\n               \n        input_ids, attention_masks, target = input_id.to(device), attention_masks.to(device), target.to(device)\n            \n        '''\n        Compute output and loss from BERT\n        '''\n        loss, logits = model(input_ids, \n                         token_type_ids=None, \n                         attention_mask=attention_masks, \n                         labels=target,\n                         return_dict=False)\n\n        test_loss.extend([loss.item()]*input_id.size()[0])\n        \n        predictions = F.softmax(logits, dim=1)\n        \n        _, top1_pred_labels = torch.max(predictions,1)\n        top1_pred_labels = top1_pred_labels.view(-1)\n        \n        top1_accuracy += torch.sum(torch.eq(top1_pred_labels, target)).item()\n        total += len(target)\n\n    print('Validation Loss: {:.4f}\\tTop 1 Validation Accuracy: {:.4f}'.format(np.mean(test_loss), top1_accuracy\/total))\n        \n    return np.mean(test_loss), top1_accuracy\/total","aba49af4":"'''\nLoss Function\n'''\ncriterion = nn.CrossEntropyLoss()\n\n'''\nOptimizer\n'''\noptimizer = AdamW(model.parameters(), lr = 2e-5, eps = 1e-8)\n\n'''\nNumber of training epochs. The BERT authors recommend between 2 and 4. Increasing the number of epochs with BERT will increase overfitting the training set, as it can be seen from the loss plot later.\n'''\nnum_Epochs = 4\n\n'''\nCreate the learning rate scheduler.\nTotal number of training steps is [number of batches] x [number of epochs].\n'''\ntotal_steps = len(train_dataloader) * num_Epochs\nlr_scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0,  num_training_steps = total_steps)","4d6bd531":"modelname = 'BERT'\nmodelpath = 'saved_checkpoint_'+modelname\n\ntrain_loss, valid_loss, valid_acc = train(model, train_dataloader, validation_dataloader, criterion, optimizer, lr_scheduler, modelpath, device, epochs = num_Epochs)","ac3d22aa":"def plot_loss(epochs, train_loss, test_loss, title):\n    plt.figure(figsize=(8,8))\n    x = np.arange(1,epochs+1)\n    plt.plot(x, train_loss, label = 'Training Loss')\n    plt.plot(x, test_loss, label = 'Validation Loss')\n    plt.xlabel('Epochs', fontsize =16)\n    plt.ylabel('Loss', fontsize =16)\n    plt.title(title,fontsize =16)\n    plt.legend(fontsize=16)\n    \n    \ndef plot_acc(epochs,test_acc):\n    plt.figure(figsize=(8,8))\n    x = np.arange(1,epochs+1)\n    plt.plot(x, test_acc)\n    plt.xlabel('Epochs', fontsize =16)\n    plt.ylabel('Test Accuracy', fontsize =16)\n    plt.title('Test Accuracy v\/s Epochs',fontsize =16)\n    \n    \nsns.set_style(\"whitegrid\")\nplot_loss(num_Epochs, train_loss, valid_loss, title='Loss plot')\nplot_acc(num_Epochs, valid_acc)","c88863a9":"def predict(model, test_loader, device):\n    model.eval()\n    target = []\n    for batch_num, (captions, input_id, attention_masks) in enumerate(test_loader):\n     \n        \n        input_ids, attention_masks = input_id.to(device), attention_masks.to(device)\n            \n        '''\n        Compute prediction outputs from BERT\n        '''\n        output_dictionary = model(input_ids, \n                         token_type_ids=None, \n                         attention_mask=attention_masks, \n                         return_dict=True)\n        \n        predictions = F.softmax(output_dictionary['logits'], dim=1)\n        \n        _, top1_pred_labels = torch.max(predictions,1)\n        top1_pred_labels = top1_pred_labels.item()        \n        target.append(top1_pred_labels)\n        \n        \n    make_csv(target)\n        \n\ndef make_csv(target):\n    test = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\n    my_submission = pd.DataFrame({'id': test.id, 'target': target})\n    my_submission.to_csv('submission.csv', index=False)\n    ","8d9bbfbb":"predict(model, test_dataloader, device)","2adef583":"We will fine-tune a BERT model on our dataset to classify a tweet into one of the 2 classes. The code has the following sections:\n1. Necessary Imports\n2. Dataloader\n3. Split data into Train and Validation sets\n4. Model Definition\n5. Trainer and Inference\n6. Set Hyperparameters\n7. Start Training\n8. Visualize the statistics (Loss, Accuracy)\n9. Predict and generate submission.csv\n\nAccuracy on Test set - **81.673%**\n","6b4773bf":"# Model Definition","a148c982":"# Trainer","b901c20a":"# Predict and generate submission.csv","cd82f433":"### Install and Import necessary modules","a6a95eba":"# Time to start training","d4f7473c":"# Visualize Loss and Accuracy plots","40ed0c9a":"# Dataloader","8875cfd0":"# Hyperparameters\n"}}