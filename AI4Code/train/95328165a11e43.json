{"cell_type":{"d0813159":"code","66c08129":"code","638dff7c":"code","fd38c679":"code","9360679a":"code","e2a2ea51":"code","4e66dd89":"code","40a51135":"code","a2a89ace":"code","a14f91b1":"code","3cb29c9a":"code","8777249e":"code","5e92b7cc":"code","c00515d9":"code","568997bc":"code","2f01668d":"code","018f94d9":"code","e79ace1c":"code","226c100b":"code","6bc937ea":"code","039aecc9":"code","60d31dbe":"code","f67b342c":"code","cb006b2a":"code","342d903f":"code","1002c8af":"code","d1eee2ae":"code","e09b84e6":"code","7d42b300":"code","3084529f":"code","e0dc53b9":"markdown","746cc3ec":"markdown","2339a466":"markdown","f82af54f":"markdown","2c92d298":"markdown","46a949be":"markdown"},"source":{"d0813159":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\nimport seaborn as sns\nsns.set_style('dark')\nimport sklearn\nimport tensorflow as tf\nfrom tensorflow import keras","66c08129":"data_path = '..\/input\/housing\/housing.csv'\n\ndf = pd.read_csv(data_path)\ndf.head()","638dff7c":"df = df.sample(frac=1, random_state=42).reset_index(drop=True)\ndf.head()","fd38c679":"df.info()","9360679a":"df['total_bedrooms'] = df['total_bedrooms'].fillna(df['total_bedrooms'].mean())\ndf.info()","e2a2ea51":"all_labels = df['median_house_value']\nall_data = df.drop('median_house_value', axis=1)","4e66dd89":"all_data['ocean_proximity'].value_counts()","40a51135":"temp = pd.get_dummies(all_data['ocean_proximity'])\nall_data = pd.concat([all_data, temp], axis=1, ignore_index=False)\nall_data = all_data.drop('ocean_proximity', axis=1)\nall_data.head()","a2a89ace":"all_data = (all_data - all_data.mean())\/all_data.std()\nall_data.head()","a14f91b1":"all_data = all_data.values\nall_data[0]","3cb29c9a":"all_labels = all_labels.values\nall_labels[0]","8777249e":"train_size = int((80\/100) * all_data.shape[0])\n\ntrain_data = all_data[: train_size]\ntrain_labels = all_labels[: train_size]\n\ntest_data = all_data[train_size: ]\ntest_labels = all_labels[train_size: ]\n\nassert(len(train_data) == len(train_labels))\nassert(len(test_data) == len(test_labels))","5e92b7cc":"print(test_data[0])\nprint(test_labels[0])\n\nassert(len(train_data[0]) == len(test_data[0]))","c00515d9":"def create_model(optimizer='adam', activation='relu'):\n\n    model = keras.models.Sequential()\n    model.add(keras.layers.Dense(128, activation=activation))\n    model.add(keras.layers.Dense(64, activation=activation))\n    model.add(keras.layers.Dense(1))\n    \n    model.compile(loss='mse', optimizer=optimizer, metrics=['mae'])\n    \n    return model","568997bc":"from sklearn.model_selection import GridSearchCV\nfrom keras.wrappers.scikit_learn import KerasRegressor\n\n\nmodel = KerasRegressor(build_fn=create_model, epochs=60 , verbose=0)\n\noptimizer = ['rmsprop', 'adam']\nactivation = ['relu', 'elu']\nparam_grid = dict(optimizer=optimizer, activation=activation)\n\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1, verbose=2)\n\ngrid_result = grid.fit(train_data, train_labels)","2f01668d":"print(grid_result.best_score_)\nprint(grid_result.best_params_)","018f94d9":"best_activation = grid_result.best_params_['activation']\nbest_optimizer = grid_result.best_params_['optimizer']","e79ace1c":"num_epochs = 500\nall_mae_histories = []\nall_scores = []\nk = 4\nnum_val_samples = len(train_data) \/\/ k\n\nfor i in range(k):\n    \n    print('processing fold #', i)\n    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]\n    val_targets = train_labels[i * num_val_samples: (i + 1) * num_val_samples]\n\n    partial_train_data = np.concatenate(\n        [train_data[:i * num_val_samples],\n        train_data[(i + 1) * num_val_samples:]],\n        axis=0)\n    \n    partial_train_targets = np.concatenate(\n        [train_labels[:i * num_val_samples],\n        train_labels[(i + 1) * num_val_samples:]],\n        axis=0)\n\n    model = create_model(optimizer='adam', activation='elu')\n    \n    my_cb = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n    \n    history = model.fit(partial_train_data, partial_train_targets,\n                            validation_data=(val_data, val_targets),\n                            epochs=num_epochs, batch_size=10, verbose=1, callbacks=[my_cb])\n    \n\n    mae_history = history.history['val_mae']\n    all_mae_histories.append(mae_history)\n    \n    val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=1)\n    all_scores.append(val_mae)","226c100b":"print(all_scores)\nprint(np.mean(all_scores))","6bc937ea":"average_mae_history = [np.mean([x[i] for x in all_mae_histories]) for i in range(308)]","039aecc9":"plt.plot(range(1, len(average_mae_history) + 1), average_mae_history)\nplt.xlabel('Epochs')\nplt.ylabel('Validation MAE')\nplt.xticks(np.arange(0, 300, 5))\nplt.tight_layout()","60d31dbe":"# According to this plot, validation MAE stops improving significantly after 50 \n# epochs. Past that point, you start overfitting.\n# We now train the final model with epochs = 5\n\nmodel = create_model(optimizer='adam', activation='elu')\n\nmy_cb = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n\nhistory = model.fit(train_data, train_labels,\n    epochs=500, validation_split=0.2, batch_size=10, verbose=1, callbacks=[my_cb])","f67b342c":"history.history.keys()","cb006b2a":"num_epochs = len(history.history['loss'])\n\nx = np.arange(1, num_epochs+1)\ny1 = history.history['loss']\ny2 = history.history['val_loss']\n\nplt.plot(x, y1, y2)\nplt.legend(['loss', 'val_loss'])\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.tight_layout()","342d903f":"x = np.arange(1, num_epochs+1)\ny1 = history.history['mae']\ny2 = history.history['val_mae']\n\nplt.plot(x, y1, y2)\nplt.legend(['mae', 'val_mae'])\nplt.xlabel('Epochs')\nplt.ylabel('MAE')\nplt.tight_layout()","1002c8af":"print(test_data[0])\nprint(test_labels[0])","d1eee2ae":"res = model.predict(test_data)\nres[0]","e09b84e6":"res = res.reshape(-1)\nres","7d42b300":"from sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nimport math\n\nmse = mean_squared_error(res, test_labels)\nmae = mean_absolute_error(res, test_labels)\nprint('Mean Sqaured Error:', mse)\nprint('Root Mean Sqaured Error:', math.sqrt(mse))\nprint('Mean Absolute Error', mae)","3084529f":"n = 15\n\nx = np.arange(1, n+1)\ny_true = test_labels[: n]\ny_pred = res[: n]\n\nplt.scatter(x, y_pred)\nplt.scatter(x, y_true)\nplt.legend(['y_pred', 'y_true'])\nplt.title('Housing Price')\nplt.ylabel('Median Housing Price')\nplt.xticks(np.arange(1, n+1))\nplt.tight_layout()","e0dc53b9":"# # Let's bring in the imports and the data","746cc3ec":"# Data preparation","2339a466":"# Training and evaluating the model","f82af54f":"# Data preprocessing","2c92d298":"# Making predictions","46a949be":"# Building and compiling the model"}}