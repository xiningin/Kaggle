{"cell_type":{"8c4740f1":"code","19b83d7a":"code","7946495a":"code","43084bad":"code","c1b88fcf":"code","39cbcb86":"code","f0c3fe75":"code","cf300e1a":"code","dce9cf30":"code","d039d72c":"code","e863bd68":"code","95a535db":"code","984d4f92":"code","76bbdb32":"code","380f64b9":"code","1f9825d9":"code","d606b8a6":"code","a8e9cd46":"code","19e1c7e9":"code","697ec6a6":"code","232a0657":"code","2b9949c3":"code","0126fc7d":"code","97f0896e":"code","0c6d4362":"code","7dbb7687":"code","d2917db6":"code","6d93df5a":"code","61620168":"code","8776f81b":"code","6dade238":"code","45cde6be":"code","04e1b185":"code","918cb729":"code","51506d55":"code","b2834732":"code","17f84113":"code","9a651f27":"code","7bce25e9":"code","c905d8a2":"code","96b03432":"code","2c0e4a80":"code","749a5ea4":"code","b20f54e9":"code","3ff6609f":"code","e417e9e5":"code","dc7e45af":"code","e53c816f":"code","61ef8ade":"code","3a786816":"code","30c12640":"code","adc89997":"markdown","f32929f2":"markdown","945bf7f4":"markdown","a0e8c03b":"markdown","bfb83876":"markdown","c08bd749":"markdown","29e9c5e9":"markdown","9b473817":"markdown","0ab51e60":"markdown","003326a1":"markdown","7ca45413":"markdown","5deaaeab":"markdown","6bb6e110":"markdown","43df9af0":"markdown","325ee46c":"markdown","4c7b3bb1":"markdown","8275265e":"markdown","6d66eb39":"markdown","5a9c3de0":"markdown","f420bb74":"markdown","1ed11146":"markdown","bd3a9cfa":"markdown","b557ce70":"markdown","69a22975":"markdown","8c85c323":"markdown","3c8ddea2":"markdown","149fbe6b":"markdown","bcaa2508":"markdown","adbe896f":"markdown"},"source":{"8c4740f1":"import os\nimport re\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path","19b83d7a":"%reload_ext autoreload\n%autoreload 2","7946495a":"from fastai import *\n#from fastai.basics import *","43084bad":"PATH=Path('..\/input\/')\ntable_names = ['train', 'store', 'store_states', 'state_names', 'googletrend', 'weather', 'test']\ntables = [pd.read_csv(PATH\/f'{fname}.csv', low_memory=False) for fname in table_names]\ntrain, store, store_states, state_names, googletrend, weather, test = tables\nlen(train),len(test)","c1b88fcf":"train.StateHoliday = train.StateHoliday!='0'\ntest.StateHoliday = test.StateHoliday!='0'","39cbcb86":"def join_df(left, right, left_on, right_on=None, suffix='_y'):\n    if right_on is None: right_on = left_on\n    return left.merge(right, how='left', left_on=left_on, right_on=right_on, \n                      suffixes=(\"\", suffix))","f0c3fe75":"weather = join_df(weather, state_names, \"file\", \"StateName\")","cf300e1a":"googletrend['Date'] = googletrend.week.str.split(' - ', expand=True)[0]\ngoogletrend['State'] = googletrend.file.str.split('_', expand=True)[2]\ngoogletrend.loc[googletrend.State=='NI', \"State\"] = 'HB,NI'","dce9cf30":"def add_datepart(df, fldname, drop=True, time=False):\n    \"Helper function that adds columns relevant to a date.\"\n    fld = df[fldname]\n    fld_dtype = fld.dtype\n    if isinstance(fld_dtype, pd.core.dtypes.dtypes.DatetimeTZDtype):\n        fld_dtype = np.datetime64\n\n    if not np.issubdtype(fld_dtype, np.datetime64):\n        df[fldname] = fld = pd.to_datetime(fld, infer_datetime_format=True)\n    targ_pre = re.sub('[Dd]ate$', '', fldname)\n    attr = ['Year', 'Month', 'Week', 'Day', 'Dayofweek', 'Dayofyear',\n            'Is_month_end', 'Is_month_start', 'Is_quarter_end', 'Is_quarter_start', 'Is_year_end', 'Is_year_start']\n    if time: attr = attr + ['Hour', 'Minute', 'Second']\n    for n in attr: df[targ_pre + n] = getattr(fld.dt, n.lower())\n    df[targ_pre + 'Elapsed'] = fld.astype(np.int64) \/\/ 10 ** 9\n    if drop: df.drop(fldname, axis=1, inplace=True)","d039d72c":"add_datepart(weather, \"Date\", drop=False)\nadd_datepart(googletrend, \"Date\", drop=False)\nadd_datepart(train, \"Date\", drop=False)\nadd_datepart(test, \"Date\", drop=False)","e863bd68":"trend_de = googletrend[googletrend.file == 'Rossmann_DE']","95a535db":"store = join_df(store, store_states, \"Store\")\nlen(store[store.State.isnull()])","984d4f92":"joined = join_df(train, store, \"Store\")\njoined_test = join_df(test, store, \"Store\")\nlen(joined[joined.StoreType.isnull()]),len(joined_test[joined_test.StoreType.isnull()])","76bbdb32":"joined = join_df(joined, googletrend, [\"State\",\"Year\", \"Week\"])\njoined_test = join_df(joined_test, googletrend, [\"State\",\"Year\", \"Week\"])\nlen(joined[joined.trend.isnull()]),len(joined_test[joined_test.trend.isnull()])","380f64b9":"joined = joined.merge(trend_de, 'left', [\"Year\", \"Week\"], suffixes=('', '_DE'))\njoined_test = joined_test.merge(trend_de, 'left', [\"Year\", \"Week\"], suffixes=('', '_DE'))\nlen(joined[joined.trend_DE.isnull()]),len(joined_test[joined_test.trend_DE.isnull()])","1f9825d9":"joined = join_df(joined, weather, [\"State\",\"Date\"])\njoined_test = join_df(joined_test, weather, [\"State\",\"Date\"])\nlen(joined[joined.Mean_TemperatureC.isnull()]),len(joined_test[joined_test.Mean_TemperatureC.isnull()])","d606b8a6":"for df in (joined, joined_test):\n    for c in df.columns:\n        if c.endswith('_y'):\n            if c in df.columns: df.drop(c, inplace=True, axis=1)","a8e9cd46":"for df in (joined,joined_test):\n    df['CompetitionOpenSinceYear'] = df.CompetitionOpenSinceYear.fillna(1900).astype(np.int32)\n    df['CompetitionOpenSinceMonth'] = df.CompetitionOpenSinceMonth.fillna(1).astype(np.int32)\n    df['Promo2SinceYear'] = df.Promo2SinceYear.fillna(1900).astype(np.int32)\n    df['Promo2SinceWeek'] = df.Promo2SinceWeek.fillna(1).astype(np.int32)","19e1c7e9":"for df in (joined,joined_test):\n    df[\"CompetitionOpenSince\"] = pd.to_datetime(dict(year=df.CompetitionOpenSinceYear, \n                                                     month=df.CompetitionOpenSinceMonth, day=15))\n    df[\"CompetitionDaysOpen\"] = df.Date.subtract(df.CompetitionOpenSince).dt.days","697ec6a6":"for df in (joined,joined_test):\n    df.loc[df.CompetitionDaysOpen<0, \"CompetitionDaysOpen\"] = 0\n    df.loc[df.CompetitionOpenSinceYear<1990, \"CompetitionDaysOpen\"] = 0","232a0657":"for df in (joined,joined_test):\n    df[\"CompetitionMonthsOpen\"] = df[\"CompetitionDaysOpen\"]\/\/30\n    df.loc[df.CompetitionMonthsOpen>24, \"CompetitionMonthsOpen\"] = 24\njoined.CompetitionMonthsOpen.unique()","2b9949c3":"# If needed, uncomment:\n# ! pip install isoweek","0126fc7d":"from isoweek import Week\nfor df in (joined,joined_test):\n    df[\"Promo2Since\"] = pd.to_datetime(df.apply(lambda x: Week(\n        x.Promo2SinceYear, x.Promo2SinceWeek).monday(), axis=1).astype(pd.datetime))\n    df[\"Promo2Days\"] = df.Date.subtract(df[\"Promo2Since\"]).dt.days","97f0896e":"for df in (joined,joined_test):\n    df.loc[df.Promo2Days<0, \"Promo2Days\"] = 0\n    df.loc[df.Promo2SinceYear<1990, \"Promo2Days\"] = 0\n    df[\"Promo2Weeks\"] = df[\"Promo2Days\"]\/\/7\n    df.loc[df.Promo2Weeks<0, \"Promo2Weeks\"] = 0\n    df.loc[df.Promo2Weeks>25, \"Promo2Weeks\"] = 25\n    df.Promo2Weeks.unique()","0c6d4362":"joined.to_pickle('joined')\njoined_test.to_pickle('joined_test')","7dbb7687":"def get_elapsed(fld, pre):\n    day1 = np.timedelta64(1, 'D')\n    last_date = np.datetime64()\n    last_store = 0\n    res = []\n\n    for s,v,d in zip(df.Store.values,df[fld].values, df.Date.values):\n        if s != last_store:\n            last_date = np.datetime64()\n            last_store = s\n        if v: last_date = d\n        res.append(((d-last_date).astype('timedelta64[D]') \/ day1))\n    df[pre+fld] = res","d2917db6":"columns = [\"Date\", \"Store\", \"Promo\", \"StateHoliday\", \"SchoolHoliday\"]","6d93df5a":"#df = train[columns]\ndf = train[columns].append(test[columns])","61620168":"fld = 'SchoolHoliday'\ndf = df.sort_values(['Store', 'Date'])\nget_elapsed(fld, 'After')\ndf = df.sort_values(['Store', 'Date'], ascending=[True, False])\nget_elapsed(fld, 'Before')","8776f81b":"fld = 'StateHoliday'\ndf = df.sort_values(['Store', 'Date'])\nget_elapsed(fld, 'After')\ndf = df.sort_values(['Store', 'Date'], ascending=[True, False])\nget_elapsed(fld, 'Before')","6dade238":"fld = 'Promo'\ndf = df.sort_values(['Store', 'Date'])\nget_elapsed(fld, 'After')\ndf = df.sort_values(['Store', 'Date'], ascending=[True, False])\nget_elapsed(fld, 'Before')","45cde6be":"df = df.set_index(\"Date\")","04e1b185":"columns = ['SchoolHoliday', 'StateHoliday', 'Promo']","918cb729":"for o in ['Before', 'After']:\n    for p in columns:\n        a = o+p\n        df[a] = df[a].fillna(0).astype(int)","51506d55":"bwd = df[['Store']+columns].sort_index().groupby(\"Store\").rolling(7, min_periods=1).sum()","b2834732":"fwd = df[['Store']+columns].sort_index(ascending=False\n                                      ).groupby(\"Store\").rolling(7, min_periods=1).sum()","17f84113":"bwd.drop('Store',1,inplace=True)\nbwd.reset_index(inplace=True)","9a651f27":"fwd.drop('Store',1,inplace=True)\nfwd.reset_index(inplace=True)","7bce25e9":"df.reset_index(inplace=True)","c905d8a2":"df = df.merge(bwd, 'left', ['Date', 'Store'], suffixes=['', '_bw'])\ndf = df.merge(fwd, 'left', ['Date', 'Store'], suffixes=['', '_fw'])","96b03432":"df.drop(columns,1,inplace=True)","2c0e4a80":"df.head()","749a5ea4":"df.to_pickle('df')","b20f54e9":"df[\"Date\"] = pd.to_datetime(df.Date)","3ff6609f":"df.columns","e417e9e5":"joined = pd.read_pickle('joined')\njoined_test = pd.read_pickle('joined_test')","dc7e45af":"joined = join_df(joined, df, ['Store', 'Date'])","e53c816f":"joined_test = join_df(joined_test, df, ['Store', 'Date'])","61ef8ade":"joined = joined[joined.Sales!=0]","3a786816":"joined.reset_index(inplace=True)\njoined_test.reset_index(inplace=True)","30c12640":"joined.to_pickle('train_clean')\njoined_test.to_pickle('test_clean')","adc89997":"In addition to the provided data, we will be using external datasets put together by participants in the Kaggle competition. You can download all of them [here](http:\/\/files.fast.ai\/part2\/lesson14\/rossmann.tgz). Then you shold untar them in the dirctory to which `PATH` is pointing below.\n\nFor completeness, the implementation used to put them together is included below.","f32929f2":"The following extracts particular date fields from a complete datetime for the purpose of constructing categoricals.\n\nYou should *always* consider this feature extraction step when working with date-time. Without expanding your date-time into these additional fields, you can't capture any trend\/cyclical behavior as a function of time at any of these granularities. We'll add to every table with a date field.","945bf7f4":"We'll replace some erroneous \/ outlying data.","a0e8c03b":"We'll back this up as well.","bfb83876":"We'll do this for two more fields.","c08bd749":"In pandas you can add new columns to a dataframe by simply defining it. We'll do this for googletrends by extracting dates and state names from the given data and adding those columns.\n\nWe're also going to replace all instances of state name 'NI' to match the usage in the rest of the data: 'HB,NI'. This is a good opportunity to highlight pandas indexing. We can use `.loc[rows, cols]` to select a list of rows and a list of columns from the dataframe. In this case, we're selecting rows w\/ statename 'NI' by using a boolean list `googletrend.State=='NI'` and selecting \"State\".","29e9c5e9":"Next we want to drop the Store indices grouped together in the window function.\n\nOften in pandas, there is an option to do this in place. This is time and memory efficient when working with large datasets.","9b473817":"We add \"CompetitionMonthsOpen\" field, limiting the maximum to 2 years to limit number of unique categories.","0ab51e60":"Same process for Promo dates. You may need to install the `isoweek` package first.","003326a1":"Join weather\/state names.","7ca45413":"We turn state Holidays to booleans, to make them more convenient for modeling. We can do calculations on pandas fields using notation very similar (often identical) to numpy.","5deaaeab":"[Lesson Video Link](https:\/\/course.fast.ai\/videos\/?lesson=6)\n\n[Lesson resources and updates](https:\/\/forums.fast.ai\/t\/lesson-6-official-resources-and-updates\/31441)\n\n[Lesson chat](https:\/\/forums.fast.ai\/t\/lesson-6-in-class-discussion\/31440)\n\n[Further discussion thread](https:\/\/forums.fast.ai\/t\/lesson-6-advanced-discussion\/31442)\n\nNote: This is a mirror of the FastAI Lesson 6 Nb. \nPlease thank the amazing team behind fast.ai for creating these, I've merely created a mirror of the same here\nFor complete info on the course, visit course.fast.ai","6bb6e110":"The authors also removed all instances where the store had zero sale \/ was closed. We speculate that this may have cost them a higher standing in the competition. One reason this may be the case is that a little exploratory data analysis reveals that there are often periods where stores are closed, typically for refurbishment. Before and after these periods, there are naturally spikes in sales that one might expect. By ommitting this data from their training, the authors gave up the ability to leverage information about these periods to predict this otherwise volatile behavior.","43df9af0":"It's usually a good idea to back up large tables of extracted \/ wrangled features before you join them onto another one, that way you can go back to it easily if you need to make changes to it.","325ee46c":"Next we'll extract features \"CompetitionOpenSince\" and \"CompetitionDaysOpen\". Note the use of `apply()` in mapping a function across dataframe values.","4c7b3bb1":"Next we'll fill in missing values to avoid complications with `NA`'s. `NA` (not available) is how Pandas indicates missing values; many models have problems when missing values are present, so it's always important to think about how to deal with them. In these cases, we are picking an arbitrary *signal value* that doesn't otherwise appear in the data.","8275265e":"# Rossmann","6d66eb39":"Then set null values from elapsed field calculations to 0.","5a9c3de0":"`join_df` is a function for joining tables on specific fields. By default, we'll be doing a left outer join of `right` on the `left` argument using the given fields for each table.\n\nPandas does joins using the `merge` method. The `suffixes` argument describes the naming convention for duplicate fields. We've elected to leave the duplicate field names on the left untouched, and append a \"\\_y\" to those on the right.","f420bb74":"Now we'll merge these values onto the df.","1ed11146":"We're going to set the active index to Date.","bd3a9cfa":"## Durations","b557ce70":"Next we'll demonstrate window functions in pandas to calculate rolling quantities.\n\nHere we're sorting by date (`sort_index()`) and counting the number of events of interest (`sum()`) defined in `columns` in the following week (`rolling()`), grouped by Store (`groupby()`). We do the same in the opposite direction.","69a22975":"The Google trends data has a special category for the whole of the Germany - we'll pull that out so we can use it explicitly.","8c85c323":"## Data preparation \/ Feature engineering","3c8ddea2":"Now we can outer join all of our data into a single dataframe. Recall that in outer joins everytime a value in the joining field on the left table does not have a corresponding value on the right table, the corresponding row in the new table has Null values for all right table fields. One way to check that all records are consistent and complete is to check for Null values post-join, as we do here.\n\n*Aside*: Why note just do an inner join?\nIf you are assuming that all records are complete and match on the field you desire, an inner join will do the same thing as an outer join. However, in the event you are wrong or a mistake is made, an outer join followed by a null-check will catch it. (Comparing before\/after # of rows for inner join is equivalent, but requires keeping track of before\/after row #'s. Outer join is easier.)","149fbe6b":"Let's walk through an example.\n\nSay we're looking at School Holiday. We'll first sort by Store, then Date, and then call `add_elapsed('SchoolHoliday', 'After')`:\nThis will apply to each row with School Holiday:\n* A applied to every row of the dataframe in order of store and date\n* Will add to the dataframe the days since seeing a School Holiday\n* If we sort in the other direction, this will count the days until another holiday.","bcaa2508":"We'll be applying this to a subset of columns:","adbe896f":"It is common when working with time series data to extract data that explains relationships across rows as opposed to columns, e.g.:\n* Running averages\n* Time until next event\n* Time since last event\n\nThis is often difficult to do with most table manipulation frameworks, since they are designed to work with relationships across columns. As such, we've created a class to handle this type of data.\n\nWe'll define a function `get_elapsed` for cumulative counting across a sorted dataframe. Given a particular field `fld` to monitor, this function will start tracking time since the last occurrence of that field. When the field is seen again, the counter is set to zero.\n\nUpon initialization, this will result in datetime na's until the field is encountered. This is reset every time a new store is seen. We'll see how to use this shortly."}}