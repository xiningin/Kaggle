{"cell_type":{"eb512040":"code","96fc97b3":"code","a6b84f8d":"code","d1e0b72b":"code","892f53d1":"code","0e1b9089":"code","2230325f":"code","87de7b03":"code","f7517079":"code","c852f45a":"code","a08705b3":"code","c74e816d":"code","6c1ff435":"code","ec237be9":"code","d69163a7":"code","46804c33":"code","37874a3c":"code","a5d497ce":"code","130409a1":"code","cb4d8838":"code","b5340007":"code","658a3f83":"code","268170d1":"code","4869dea5":"code","21a78d33":"code","6d1eed50":"code","0a289665":"code","4e6cad85":"code","ca6d5f4c":"code","2534d7bf":"code","aa3ec17a":"code","0a91bf9d":"code","5407c6ad":"code","5d011c5c":"code","ace5be95":"code","16b169f2":"code","a5bfa007":"code","d3f19414":"code","e0236f6c":"markdown","809100a6":"markdown","47b25d49":"markdown","a05ae824":"markdown","d82cb431":"markdown","d38b776d":"markdown","66270338":"markdown","e0bb8a6d":"markdown","d4fe2509":"markdown","92f9d079":"markdown","2dff6e5b":"markdown","e22c2ac2":"markdown","a4c08b51":"markdown","b1464397":"markdown","3688f01d":"markdown","21d6e1e0":"markdown","f417e6e9":"markdown","65ad33d6":"markdown","fb25109c":"markdown","690b586c":"markdown","34db4277":"markdown","ad10b6aa":"markdown","f26aa73f":"markdown","035339fa":"markdown","17458bcb":"markdown","96011118":"markdown","0899bc79":"markdown","17e2dd80":"markdown","3c8fa303":"markdown","019fb82d":"markdown","3d0bb870":"markdown","5849dd0a":"markdown","d3411b47":"markdown","06290a07":"markdown","adabc203":"markdown","12e0fb87":"markdown","940a3c1e":"markdown","6aabfb9e":"markdown","8f42f3c9":"markdown","3e79c57a":"markdown"},"source":{"eb512040":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","96fc97b3":"df_train = pd.read_csv('..\/input\/train.csv')\ndf_train.describe()","a6b84f8d":"df_train.isnull().values.any()","d1e0b72b":"data = df_train.values[:,:] \nprint(data[:5]) # display the first 5 elements\nprint(data.shape) # display the dimension of the array","892f53d1":"from scipy import ndimage\nfrom enum import IntEnum\n\nclass ProcessAction(IntEnum):\n    RotateClockwise = 0\n    RotateCounterClockwise = 1\n    ShiftUp=2\n    ShiftDown=3\n    ShiftLeft=4\n    ShiftRight=5\n\nfile_name_ext=['RotateClockwise', 'RotateCounterClockwise', 'ShiftUp', 'ShiftDown', 'ShiftLeft', 'ShiftRight']","0e1b9089":"# Delete existing files if any\nimport os\nfor i in ProcessAction:\n    file_name = 'train_'+ file_name_ext[int(i)] + '.csv'\n    if os.path.isfile(file_name):\n        os.remove(file_name)","2230325f":"# Data processing methode\ndef doWork(data, action):\n    flush_limit=100\n    display_limit=1000\n    max=data.shape[0]\n    flush_count=0\n    for i in range(0,max):\n        # Create additional image for each signal image from the initial data set.\n        img=data[i,1:] # features    \n        lbl=data[i,0] # labels\n    \n        img=img.reshape((28, 28))\n        \n        if action == ProcessAction.RotateClockwise:\n            new_img= ndimage.rotate(img,-10,reshape=False)            \n\n        elif action == ProcessAction.RotateCounterClockwise:    \n            new_img= ndimage.rotate(img,10,reshape=False)\n\n        elif action == ProcessAction.ShiftUp:    \n            new_img= ndimage.shift(img,(0,28*0.1))\n\n        elif action == ProcessAction.ShiftDown:    \n            new_img= ndimage.shift(img,(0,-28*0.1))\n\n        elif action == ProcessAction.ShiftLeft:    \n            new_img= ndimage.shift(img,(28*0.1,0))\n\n        elif action == ProcessAction.ShiftRight:    \n            new_img= ndimage.shift(img,(-28*0.1,0))\n            \n        else:\n            print(\"Unkknow action \", action)\n            return;\n    \n        new_img_data=np.append([lbl],new_img.reshape((1,784))).reshape((1,785))\n\n        if (i % flush_limit == 0 and i != 0) or i == (max-1):\n\n            if i == (max-1):\n                # add the very last conversion\n                data_local = np.vstack((data_local,new_img_data))\n           \n            #-----------------------------------------------------\n            # Save data, flush every flush_limit images\n            #-----------------------------------------------------\n                        \n            df_new_train = pd.DataFrame(data_local, columns=df_train.columns)\n        \n            file_name='train_'+ file_name_ext[int(action)] + '.csv'\n                \n            if flush_count == 0:\n                df_new_train.to_csv(file_name, sep=',', mode='a', index=False)\n            else:\n                df_new_train.to_csv(file_name, sep=',', mode='a', index=False, header=False)\n\n            flush_count = flush_count+1\n\n\n        if i % flush_limit == 0:\n            data_local = new_img_data\n        else:\n            data_local = np.vstack((data_local,new_img_data))\n       \n        if i % display_limit == 0:\n            print(\"Iteration (\",action,\"): \",i,\"\/\",max)","87de7b03":"# Perform the data processing for the different actions\nimport time\nstart = time.time()\ndoWork(data,ProcessAction.RotateClockwise)\ndoWork(data,ProcessAction.RotateCounterClockwise)\ndoWork(data,ProcessAction.ShiftUp)\ndoWork(data,ProcessAction.ShiftDown)\ndoWork(data,ProcessAction.ShiftLeft)\ndoWork(data,ProcessAction.ShiftRight)\nend = time.time()\nprint(\"Process time (s): \", end - start)","f7517079":"# Merge the new data to the original data\nimport time\nstart = time.time()\nfor i in ProcessAction:\n    file_name = 'train_'+ file_name_ext[int(i)] + '.csv'\n    print(\"Add file to data:\", file_name)\n    df_tmp = pd.read_csv(file_name)\n    df_tmp.describe()\n    data_tmp = df_tmp.values[:,:] \n    print(data_tmp.shape)\n    data = np.vstack((data,data_tmp))\nprint(\"Final data set size: \",data.shape)\nend = time.time()\nprint(\"Process time (s): \", end - start)","c852f45a":"np.random.seed(6)\nnp.random.shuffle(data)\nprint(data[:5])\nprint(data.shape)","a08705b3":"X=data[:,1:] # features\ny=data[:,0] # labels\nprint(\"X size: \", X.shape)\nprint(\"y size: \", y.shape)","c74e816d":"X_train, X_test = np.split(X, [int(.8*X.shape[0])])\ny_train, y_test = np.split(y, [int(.8*y.shape[0])])\nprint(\"X training set size: \", X_train.shape)\nprint(\"X test set size: \", X_test.shape)","6c1ff435":"%matplotlib inline\nfrom matplotlib import pyplot as plt\nfrom scipy import ndimage\n\nfig = plt.figure()\n\nfor idx in range(0,9):\n    ax = fig.add_subplot(3,3,idx+1)\n    img_data=X[idx,:].reshape((28, 28))\n    plt.imshow(img_data, cmap=\"gray\")\n\nplt.show()","ec237be9":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\n# Fit only to the training data\nscaler.fit(X_train)\nStandardScaler(copy=True, with_mean=True, with_std=True)\n\n# Now apply the transformations to the data:\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)","d69163a7":"X_train_reshaped = X_train.reshape(X_train.shape[0],28, 28,1).astype( 'float32' )\nprint('Size of the input traning set: ',X_train_reshaped.shape)\nX_test_reshaped = X_test.reshape(X_test.shape[0],28, 28,1).astype( 'float32' )\nprint('Size of the input test set: ',X_test_reshaped.shape)","46804c33":"from sklearn.preprocessing import LabelBinarizer\n\nclass MyLabelBinarizer(LabelBinarizer):\n    def transform(self, y):\n        Y = super().transform(y)\n        if self.y_type_ == 'binary':\n            return np.hstack((Y, 1-Y))\n        else:\n            return Y\n\n    def inverse_transform(self, Y, threshold=None):\n        if self.y_type_ == 'binary':\n            return super().inverse_transform(Y[:, 0], threshold)\n        else:\n            return super().inverse_transform(Y, threshold)\n        \n\nlb = MyLabelBinarizer()\nprint(y_train.shape)\nprint(y_train[:5])\ny_train_bin = lb.fit_transform(y_train)\ny_test_bin = lb.fit_transform(y_test)\nprint(y_train_bin[0:5,:])","37874a3c":"# Importing libraries\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D\nfrom keras.layers import MaxPooling2D\nfrom keras.layers import Flatten\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\n\n# Initialising the CNN\nclassifier = Sequential()\n\n# Adding a first convolutional layer\nclassifier.add(Conv2D(32, (5, 5), input_shape = (28,28,1), activation = 'relu'))\nclassifier.add(MaxPooling2D(pool_size = (2, 2)))\n\n# Adding a second convolutional layer\nclassifier.add(Conv2D(32, (3, 3), activation = 'relu'))\nclassifier.add(MaxPooling2D(pool_size = (2, 2)))\n\n# Step 3 - Flattening\nclassifier.add(Flatten())\n\n# Step 4 - Full connection\nclassifier.add(Dense(units = 128, activation = 'relu'))\nclassifier.add(Dense(units = 10, activation = 'softmax')) #softmax for classification","a5d497ce":"# Compiling the CNN\nfrom keras import optimizers\nclassifier.compile(optimizer = 'adam', \n                   loss = 'categorical_crossentropy', \n                   metrics = ['accuracy'])","130409a1":"classifier.fit(X_train_reshaped, y_train_bin,\n          epochs=70,\n          batch_size= 160)","cb4d8838":"score = classifier.evaluate(X_test_reshaped, y_test_bin, batch_size=128)\nprint('Score: ',score)\nprint('Metrics: ',classifier.metrics_names)\nclassifier.summary()","b5340007":"pred_test = classifier.predict(X_test_reshaped)","658a3f83":"print(lb.classes_)\nprint(lb.y_type_)\npred_test = lb.inverse_transform(pred_test)\nprint(pred_test[:5])","268170d1":"from sklearn.metrics import classification_report,confusion_matrix, accuracy_score\nprint(confusion_matrix(y_test,pred_test))\nprint(classification_report(y_test,pred_test))\nprint('>>>>>> Accuracy score: ',accuracy_score(y_test,pred_test))","4869dea5":"print(pred_test[:5])","21a78d33":"def build_classifier(optimizer='adam'):\n    classifier = Sequential()\n    classifier.add(Conv2D(32, (3, 3), input_shape = (28,28,1), activation = 'relu'))\n    classifier.add(MaxPooling2D(pool_size = (2, 2)))\n    classifier.add(Conv2D(32, (3, 3), activation = 'relu'))\n    classifier.add(MaxPooling2D(pool_size = (2, 2)))\n    classifier.add(Flatten())\n    classifier.add(Dense(units = 128, activation = 'relu'))\n    classifier.add(Dense(units = 128, activation = 'relu'))\n    classifier.add(Dense(units = 128, activation = 'relu'))\n    classifier.add(Dense(units = 10, activation = 'softmax'))\n    classifier.compile(optimizer = optimizer, \n                   loss = 'categorical_crossentropy', \n                   metrics = ['accuracy'])\n    return classifier","6d1eed50":"# Not used at the moment\nif 0:\n    # Evaluate\n    from keras.wrappers.scikit_learn import KerasClassifier\n    from sklearn.model_selection import cross_val_score\n    classifier = KerasClassifier(build_fn = build_classifier, batch_size = 160, epochs = 20)\n    accuracies = cross_val_score(estimator = classifier, X = X_train_reshaped, y = y_train_bin, cv = 10)\n    mean = accuracies.mean()\n    variance = accuracies.std()\n    print('Mean=',mean)\n    print('Variance=',variance)","0a289665":"# Not used at the moment\nfrom sklearn.model_selection import GridSearchCV\nif 0:\n    classifier = KerasClassifier(build_fn = build_classifier)\n    parameters = {'batch_size': [150, 170],\n                  'epochs': [20, 30],\n                  'optimizer': ['adam', 'rmsprop']}\n    grid_search = GridSearchCV(estimator = classifier,\n                               param_grid = parameters,\n                               scoring = 'accuracy',\n                               cv = 10)\n    grid_search = grid_search.fit(X_train_reshaped, y_train_bin)\n    print('Best parameters: ',grid_search.best_params_)\n    print('Best score: ',grid_search.best_score_)","4e6cad85":"df_test = pd.read_csv('..\/input\/test.csv')\ndf_test.describe()","ca6d5f4c":"df_test.isnull().values.any()","2534d7bf":"X_submission = df_test.values[:,:] \nprint(X_submission[:5]) # display the first 5 elements\nprint(X_submission.shape)","aa3ec17a":"%matplotlib inline\nfrom matplotlib import pyplot as plt\n\nfig = plt.figure()\n\nfor idx in range(0,9):\n    ax = fig.add_subplot(3,3,idx+1)\n    img_data=X_submission[idx,:].reshape((28, 28))\n    plt.imshow(img_data, cmap=\"gray\")\n    \nplt.show()","0a91bf9d":"X_submission = scaler.transform(X_submission)","5407c6ad":"X_submission_reshaped = X_submission.reshape(X_submission.shape[0],28, 28,1).astype( 'float32' )","5d011c5c":"pred_submission = classifier.predict(X_submission_reshaped)","ace5be95":"pred_submission = lb.inverse_transform(pred_submission)\nprint(pred_submission[:5])","16b169f2":"idx=np.arange(1,X_submission.shape[0]+1)\nsubmussion = np.column_stack((idx , pred_submission))\nprint(submussion[:5,:]) # display the first 5 rows","a5bfa007":"columns = ['ImageId', 'Label']\ndf_submission = pd.DataFrame(submussion, columns=columns)\nprint(df_submission.head(5)) ","d3f19414":"df_submission.to_csv('Digit Recognizer Submission 3.csv', sep=',' ,index=False)","e0236f6c":"# Import Data","809100a6":"## Check, Clean and Prepocess Data","47b25d49":"## Import Data","a05ae824":"# Evaluate","d82cb431":"## Data Augmentation\nCreate additional new data based on current ones, that is increase the original training data with new picture resulting from geometric transformation of the original data. Geometric transformation will be:\n\n1. Rotation, clockwise and counter clockwise\n2. Shift, up, down, left and right  \n(inspired from the following kernel: \"[Introduction to CNN Keras](https:\/\/www.kaggle.com\/yassineghouzam\/introduction-to-cnn-keras-0-997-top-6)\" section 3.3\n\nNote: For picture processing with neural networks, it is known that increasing the data using geometric transformation has a positive effect. On the contrary, it is known also that increasing data with noisy data does not improve the resutls.\n","d38b776d":"## Input Normalization","66270338":"Compute additional model prediction results:","e0bb8a6d":"# Create Training and Test Sets\nHere I made the choice to split the original training data into two set, a training set and test test. This is because I did not implemented CNN tuning. In case of CNN tuning, the best practice is to split the original training data at minimum in three sets, training set, validation set and test set, or even more subset depending on the fine tuning approach.\n\nHere the choice has been doen to:\n1. increase the original training data with new picture resulting from geometric transformation of the original data\n2. to shuffle the data\n3. to split the data 80% training and 20% test\n\nThis is performed in the following three sections.","d4fe2509":"## Shuffle the data","92f9d079":"## Extract Data","2dff6e5b":"# Introduction\nIn the following kernel, I will use CNN with Keras to address the problem of the digital recognizer.","e22c2ac2":"Transforme the 10 binary class prediction back to a single multi class value in order to be able to compute confusion matrix:","a4c08b51":"## Input Standardization\nInputs of traning set and test set are standardized by removing the mean and scaling to unit variance:","b1464397":"# Display Data Samples","3688f01d":"The coding approach is the following:\n*  The original data set is NOT increased on the fly. Rather a separate data set with new data is created and merged to the orignal data at the very end when all the new data have been generated.\n* The new data are flushed to different CSV files, one for each geometric transformation. This is to limit the file size and also to be able to process each transformation in parallel if needed.\n* The flush is done once a certain number of new data have been generated, this number being not too large to prevent too much overhead due to array indexing and due to array stacking with numpy.vstack, but also not too small to limit the file access overhead. A good number, found with experience is 100.\n* The data processing is implemented into a single method doWork, so that if needed we can easily parallelize it.\n\nThis approach has been guided by the performance and especially with the indexing overhead in mind. Experience showed that increasing the original data set on the fly would take many hours, the following approach on the same HW takes few minutes.\n","21d6e1e0":"## Input Formatting for 2D Process\nAt the moment the input features of a training example is a 1D array of 784 features, being the pixels on one channel (grayscale).  \nFirst we need to reshape the input features so that they have the format of 2D image of 28x28 pixel on one channel. This necessary for the 2D convolution layer:","f417e6e9":"## Transforme 10 binary class prediction back to a single multi class value","65ad33d6":"## Split Data\nSplit the data into three sets: training (80%) and test (20%):","fb25109c":"Compile the model:","690b586c":"# Conclusion\nCNN performance is good but it could be better. Possible improvement ways:\n* Improve the data augmentation step with additional transformations\n* Implement a fine tuning strategy, on the hyperparameters or CNN architecture, but is the kernel the right place to execute it with the performance and time limitations?\n* Integrate in the CNN a Spatial Transformation Network (STN) module\n","34db4277":"# Build and Train Model","ad10b6aa":"Load the training data into a data frame object and displays some statistics about its content:","f26aa73f":"# Data Preprocessing","035339fa":"1. ## Create Submission Data Set","17458bcb":"Now we are extracting the data into NumPy array. This is because Keras methods needs array but also because all the preprocessing will be faster with array compare to data frame (indexing slower with data frame compare to array due to extra features provided bw the data frame).","96011118":"## Display Data Samples","0899bc79":"# Extract Data","17e2dd80":"## Input Formatting for 2D Process","3c8fa303":"# Fine Tuning","019fb82d":"Compute predictions for the test set:","3d0bb870":"Here with check if the training data contains any NULL or NaN values, possibly missing data:","5849dd0a":"Now we define the CNN model:","d3411b47":"# Create Submission on Challenge Test Set","06290a07":"## Output Label Transformation\nThe output labels of the training set and the test set have to be converted from a single class value (from 0 to 9) into 10 binary class values:","adabc203":"# Check, Clean and Prepocess Data","12e0fb87":"# Predictions and Evaluation","940a3c1e":"## Compute Predictions","6aabfb9e":"## Write CSV Submission File","8f42f3c9":"Train the model using the traning set:","3e79c57a":"Evaluate the model on the test set:"}}