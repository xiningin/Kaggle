{"cell_type":{"f4231ed5":"code","afc179a2":"code","b26f96b5":"code","262a693f":"code","ea10cce3":"code","e0493de5":"code","103e009e":"code","55a90493":"code","8f33ec58":"code","fdd288e3":"code","1568fff2":"code","524d1747":"code","ed9731b6":"code","3dccd586":"code","c1982f89":"markdown","0a916bb2":"markdown","3a5c6e01":"markdown","29406d27":"markdown","8894d546":"markdown","7a420f21":"markdown","41aa88c1":"markdown","aacc1819":"markdown","a8f93b5b":"markdown","b22a5846":"markdown","e2fbc5a0":"markdown","3e1a3b4f":"markdown","722c06ad":"markdown","1f1c48bf":"markdown","85de82a8":"markdown"},"source":{"f4231ed5":"\nimport numpy as np\nimport pandas as pd\nimport gc\nimport os\nimport random\nimport copy\nimport matplotlib.pyplot as plt\nimport pandas\nimport statsmodels\nfrom statsmodels.tsa.stattools import adfuller","afc179a2":"dateparse = lambda dates: pd.datetime.strptime(dates, '%Y-%m-%d')\ndf=pd.read_csv('\/kaggle\/input\/price-volume-data-for-all-us-stocks-etfs\/Data\/Stocks\/amzn.us.txt',parse_dates=['Date'],index_col=['Date'],date_parser=dateparse)\ndf=df.drop(columns=['Open','High','Low','Volume','OpenInt'])\ndf.plot(figsize=(15,6))\nplt.show()","b26f96b5":"adftest=adfuller(df['Close'])\nprint('ADF Statistic: %f' % adftest[0])\nprint('ADF p-value: %f' % adftest[1])","262a693f":"df_log=pd.DataFrame(np.log(df['Close']),index=df.index)\nplt.figure(figsize=(15,6))\nplt.plot(df_log)\nplt.title('Log Close')\nplt.show()","ea10cce3":"adftest=adfuller(df_log['Close'])\nprint('ADF Statistic: %f' % adftest[0])\nprint('ADF p-value: %f' % adftest[1])","e0493de5":"df['Returns']=df['Close']-df['Close'].shift(1)\ndf_log['Log_Returns']=df_log['Close']-df_log['Close'].shift(1)\nplt.figure(figsize=(15,6))\nplt.subplot(1,2,1)\nplt.plot(df['Returns'])\nplt.title('Returns')\nplt.subplot(1,2,2)\nplt.plot(df_log['Log_Returns'],'g-')\nplt.title('Log Returns')\nplt.show()","103e009e":"adftest_ret=adfuller(df['Returns'].dropna())\nprint('Returns ADF Statistic: %f' % adftest_ret[0])\nprint('Returns ADF p-value: %f' % adftest_ret[1])\nadftest_lret=adfuller(df_log['Log_Returns'].dropna())\nprint('Log-Returns ADF Statistic: %f' % adftest_lret[0])\nprint('Log-Returns ADF p-value: %f' % adftest_lret[1])","55a90493":"def getWeights(d,lags):\n    # return the weights from the series expansion of the differencing operator\n    # for real orders d and up to lags coefficients\n    w=[1]\n    for k in range(1,lags):\n        w.append(-w[-1]*((d-k+1))\/k)\n    w=np.array(w).reshape(-1,1) \n    return w\ndef plotWeights(dRange, lags, numberPlots):\n    weights=pd.DataFrame(np.zeros((lags, numberPlots)))\n    interval=np.linspace(dRange[0],dRange[1],numberPlots)\n    for i, diff_order in enumerate(interval):\n        weights[i]=getWeights(diff_order,lags)\n    weights.columns = [round(x,2) for x in interval]\n    fig=weights.plot(figsize=(15,6))\n    plt.legend(title='Order of differencing')\n    plt.title('Lag coefficients for various orders of differencing')\n    plt.xlabel('lag coefficients')\n    #plt.grid(False)\n    plt.show()\ndef ts_differencing(series, order, lag_cutoff):\n    # return the time series resulting from (fractional) differencing\n    # for real orders order up to lag_cutoff coefficients\n    \n    weights=getWeights(order, lag_cutoff)\n    res=0\n    for k in range(lag_cutoff):\n        res += weights[k]*series.shift(k).fillna(0)\n    return res[lag_cutoff:] ","8f33ec58":"plotWeights([0.1,0.9],20,5)","fdd288e3":"differences=[0.5,0.9]\nfig, axs = plt.subplots(len(differences),2,figsize=(15,6))\nfor i in range(0,len(differences)):\n    axs[i,0].plot(ts_differencing(df['Close'],differences[i],20))\n    axs[i,0].set_title('Original series with d='+str(differences[i]))\n    axs[i,1].plot(ts_differencing(df_log['Close'],differences[i],20),'g-')\n    axs[i,1].set_title('Logarithmic series with d='+str(differences[i]))\n    plt.subplots_adjust(bottom=0.01) #increasing space between plots for aestethics","1568fff2":"def cutoff_find(order,cutoff,start_lags): #order is our dearest d, cutoff is 1e-5 for us, and start lags is an initial amount of lags in which the loop will start, this can be set to high values in order to speed up the algo\n    val=np.inf\n    lags=start_lags\n    while abs(val)>cutoff:\n        w=getWeights(order, lags)\n        val=w[len(w)-1]\n        lags+=1\n    return lags","524d1747":"def ts_differencing_tau(series, order, tau):\n    # return the time series resulting from (fractional) differencing\n    lag_cutoff=(cutoff_find(order,tau,1)) #finding lag cutoff with tau\n    weights=getWeights(order, lag_cutoff)\n    res=0\n    for k in range(lag_cutoff):\n        res += weights[k]*series.shift(k).fillna(0)\n    return res[lag_cutoff:] ","ed9731b6":"#this part takes about 20 minutes to compute\npossible_d=np.divide(range(1,100),100)\ntau=1e-4\noriginal_adf_stat_holder=[None]*len(possible_d)\nlog_adf_stat_holder=[None]*len(possible_d)\n\nfor i in range(len(possible_d)):\n    original_adf_stat_holder[i]=adfuller(ts_differencing_tau(df['Close'],possible_d[i],tau))[1]\n    log_adf_stat_holder[i]=adfuller(ts_differencing_tau(df_log['Close'],possible_d[i],tau))[1]","3dccd586":"#now the plots of the ADF p-values\nfig, axs = plt.subplots(1,2,figsize=(15,6))\naxs[0].plot(possible_d,original_adf_stat_holder)\naxs[0].axhline(y=0.01,color='r')\naxs[0].set_title('ADF P-value by differencing order in the original series')\naxs[1].plot(possible_d,log_adf_stat_holder)\naxs[1].axhline(y=0.01,color='r')\naxs[1].set_title('ADF P-value by differencing order in the logarithmic series')","c1982f89":"## Getting started","0a916bb2":"The log of the series was also non-stationary, usually at this point one then differences the series or the log of the series, (hence changing the variable of analysis to the returns or log returns of the stock) so lets see how this works out","3a5c6e01":"First off, we will modify our current *ts_differencing* function, so instead of the amount of lags, it will take our cutoff value (from now on we'll call it $\\tau$) as an input and automatically calculate the correspoding amount of lags:","29406d27":"When the differencing coefficient is fractional, the series needed to calculate one data point becomes infinite, so firstly we need to find the weights corresponding to all preceding observations, and then we'll establish a cutoff, which will truncate the amount of weights we finally keep. Usually such cutoff is very low, around $10^{-4}$ or so, in order to retain as much mathematical memory as possible. First off lets visualize how such weights would behave depending on the differencing order, which we will call $d$. (Credit for [Simon Kuttruf on github](https:\/\/gist.github.com\/skuttruf\/fb82807ab0400fba51c344313eb43466) as he's the author of the functions used to implement fractional differencing).","8894d546":"We cannot reject the null hypothesis of non-stationarity. A popular way to try and circumvent this is to use the logarithm of the original series, so lets see how that works out.","7a420f21":"I'm not a particular fan of discretionary graphical analysis, so let's check both of our new series through the ADF test.","41aa88c1":"We'll analyze the closing price of AMZN stocks, but in practice you apply the same principles to any other timeseries","aacc1819":"Usually univariate time series analysis is done on a stationary series, so first off we'll check for stationarity using the ADF test","a8f93b5b":"Remember the cutoff we mentioned earlier? Yeah, we still need to find that for each of the orders of differencing to be used, or better yet, we could just create a function that finds the amount of lags needed to reach any certain cutoff with any given differencing coefficient.","b22a5846":"## Traditional Differencing","e2fbc5a0":"In the plot above we can get a feel of whats actually happening. The weight of the present value equals one, and with differencing order of $d=1$ we would have the coefficient of the first lag be exactly -1 and the rest would be zero. But since our differfencing order is fractional each of the lags has a weight, and they converge to zero in absolute value (faster convergence towards zero happens in higher orders of differencing), and so we must decide a *cutoff value* for the absolute value of the coefficients so our series is not theoretically *infinite*. Larger time series would allow smaller cutoff values in order to preserve more memory, it ends up being a trade-off between memory conservation and computational efficiency. In short time series however, you might want to stay away from low threshold values, since they can (and will) greatly diminish the size of your training set. ","3e1a3b4f":"## Fractional Differencing","722c06ad":"And now lets visualize relationship between the ADF p-value and our fractional differencing order $d$","1f1c48bf":"As we can see, the original series breaks the threshold of 0.01 for the ADF p-value with an order of differencing of about 0.7, and in the logarithmic series it happens at about 0.5, so we can surely say that taking a full difference in this case is not necessary to achieve stationarity. Using fractionally differenced variables as dependent variables or features for time series modelling could improve predictive performance in many cases because the time series is allowed to retain more memory while still being stationary, so be sure to give it a try.","85de82a8":"And *voil\u00e1*, we have stationary series that we can now work with for our forecasts. Or so we would usually say, but to me it seems that we might have *over* differentiated the series. As we can see the ADF p-value went from very high, to practically **zero**, which makes me think that maybe we can apply some level of differencing between 0 and 1, making sure the series is as stationary as we need it to be, but not more, because in the end differencing extracts mathematical memory from a series, and some memory is usually desirable in order to produce good forecasts. We'll use an approach called *fractional differencing*, which is explained in great detail on  [Marcos Lopez de Prado's *Advances in Financial Machine Learning*](https:\/\/www.amazon.com\/Advances-Financial-Machine-Learning-Marcos\/dp\/1119482089). \n"}}