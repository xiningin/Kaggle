{"cell_type":{"34a3dd3f":"code","f4923596":"code","7731d5ca":"code","697f2035":"code","019c9ad6":"code","0e7379e6":"code","52323add":"code","85e03436":"code","c3280db3":"code","0901ade5":"code","a7ffb0f4":"code","4317a3d8":"code","8c0849c0":"code","1566b731":"code","6401042d":"code","6856d66a":"code","a049e140":"code","73089830":"code","99d08df2":"code","b8986985":"code","ad88ba75":"code","b9a0b775":"code","7d8e5982":"code","dc1f710b":"code","dc199fc0":"code","01806a78":"code","1feff037":"code","80dea5bc":"code","64ebd743":"code","a11fc975":"code","1654ce9f":"code","8ff845e3":"code","eed84435":"code","27072196":"code","330a72e1":"code","9b766adc":"code","f357923a":"code","73304f01":"code","8c21dd01":"code","cebb25a9":"markdown","cf75f25f":"markdown","638a29d9":"markdown","88cde7fe":"markdown","9ef5a43a":"markdown","7b4988c8":"markdown","7f99f7b3":"markdown","55a2e338":"markdown","3487de71":"markdown","ec6e2f42":"markdown","8a5df741":"markdown","ada6ec94":"markdown","4b536035":"markdown","d445fb98":"markdown","3b1bdb7a":"markdown","375b40a0":"markdown","c4aaa2ad":"markdown","a37eeb08":"markdown","73fa9b96":"markdown","5fc3e8ed":"markdown","84d1304e":"markdown","55e3d163":"markdown","1805ba85":"markdown","d75b86c1":"markdown","63f40145":"markdown","3d0d9f08":"markdown","0bd97f2f":"markdown","fcc08dfe":"markdown","a5bac573":"markdown"},"source":{"34a3dd3f":"from collections import defaultdict\nfrom pathlib import Path\nimport random\nfrom yaml import safe_load\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import pointbiserialr\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.model_selection import train_test_split\nimport torch\nfrom torch import nn, optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup","f4923596":"sns.set_theme()\nplt.rcParams[\"figure.figsize\"] = [18, 12]\nplt.rcParams[\"font.size\"] = 16\nplt.rcParams[\"legend.fontsize\"] = 16\ncmap = plt.get_cmap(\"tab20\")","7731d5ca":"random_seed = 16\ndata_dir = Path(\"\/kaggle\/input\/udemy-courses-and-reviews\")\ncourses_file = data_dir \/ \"courses.yaml\"\nreviews_file = data_dir \/ \"reviews.yaml\"\npre_trained_model_name = \"bert-base-cased\"\nbatch_size = 8\nmax_len = 30\nepochs = 10\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","697f2035":"random.seed(random_seed)\nnp.random.seed(random_seed)\ntorch.manual_seed(random_seed);","019c9ad6":"with courses_file.open() as f:\n    courses_df = pd.json_normalize(safe_load(f))","0e7379e6":"courses_df.sample(5)","52323add":"with reviews_file.open() as f:\n    reviews_df = pd.json_normalize(safe_load(f))","85e03436":"reviews_df.sample(5)","c3280db3":"def autopct(pct, allvals):\n    absolute = int(round(pct \/ 100.* np.sum(allvals)))\n    return \"{:.1f}%\\n({:d})\".format(pct, absolute)","0901ade5":"f, ax = plt.subplots()\ninstructional_levels = courses_df.groupby(\"instructional_level_simple\").size()\ncolors = [cmap(i) for i in np.linspace(0, 1, len(instructional_levels))]\nwedges, *_ = ax.pie(\n    x=instructional_levels, \n    pctdistance=1.1,\n    autopct=lambda pct: autopct(pct, instructional_levels),\n    wedgeprops=dict(width=0.5), \n    startangle=90,\n    colors=colors,\n)\nax.legend(\n    wedges, \n    instructional_levels.index,\n    title=\"Instructional Levels\",\n    loc=\"upper right\",\n    bbox_to_anchor=(0.75, 0, 0.5, 1),\n);","a7ffb0f4":"f, ax = plt.subplots()\nprimary_categories = courses_df.groupby(\"primary_category.category_title\").size()\ncolors = [cmap(i) for i in np.linspace(0, 1, len(primary_categories))]\nwedges, *_ = ax.pie(\n    x=primary_categories, \n    pctdistance=1.1,\n    autopct=lambda pct: autopct(pct, primary_categories),\n    wedgeprops=dict(width=0.5), \n    startangle=90,\n    colors=colors,\n)\nax.legend(\n    wedges, \n    primary_categories.index,\n    title=\"Categories\",\n    loc=\"upper right\",\n    bbox_to_anchor=(1, 0, 0.5, 1),\n);","4317a3d8":"f, ax = plt.subplots()\nprimary_subcategories = courses_df.groupby(\"primary_subcategory.sub_category_title\").size()\ncolors = [cmap(i) for i in np.linspace(0, 1, len(primary_subcategories))]\nwedges, *_ = ax.pie(\n    x=primary_subcategories, \n    pctdistance=1.1,\n    autopct=lambda pct: autopct(pct, primary_subcategories),\n    wedgeprops=dict(width=0.5), \n    startangle=90,\n    colors=colors,\n)\nax.legend(\n    wedges, \n    primary_subcategories.index,\n    title=\"Subcategories\",\n    loc=\"upper right\",\n    bbox_to_anchor=(1, 0, 0.5, 1),\n);","8c0849c0":"reviews_df[\"has_content\"] = reviews_df[\"review_content\"].str.len() > 0","1566b731":"ax = sns.countplot(data=reviews_df, x=\"review_rating\", hue=\"has_content\")\nax.set_xlabel(\"Review Rating\")\nax.set_ylabel(\"Number of Reviews\")\nax.legend(title=\"Review Has Content?\");","6401042d":"ax = sns.countplot(data=reviews_df, x=\"has_content\")\nax.set_xlabel(\"Review Has Content?\")\nax.set_ylabel(\"Number of Reviews\");","6856d66a":"temp_df = reviews_df[[\"review_rating\", \"review_content\"]].copy()\ntemp_df[\"has_content\"] = (temp_df[\"review_content\"].str.len() > 0).astype(int)\ntemp_df = temp_df.drop(columns=[\"review_content\"])\ntemp_df = temp_df.groupby([\"has_content\", \"review_rating\"]).apply(lambda x: pd.Series({\"count\": len(x)})).reset_index()","a049e140":"temp_df.plot.scatter(x=\"has_content\", y=\"review_rating\", s=\"count\");","73089830":"result = pointbiserialr(temp_df[\"has_content\"], temp_df[\"review_rating\"])\nprint(f\"Correlation is {result.correlation}\")","99d08df2":"non_empty_reviews_df = reviews_df[reviews_df[\"has_content\"] == True]","b8986985":"non_empty_reviews_df.sample(5)","ad88ba75":"class SentimentClassifier(nn.Module):\n    def __init__(self, n_classes, *, model_name: str):\n        super().__init__()\n        self.bert = BertModel.from_pretrained(model_name)\n        self.drop = nn.Dropout(p=0.3)\n        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n\n    def forward(self, input_ids, attention_mask):\n        output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        output = self.drop(output[\"pooler_output\"])\n        return self.out(output)","b9a0b775":"class ReviewsDataset(Dataset):\n    def __init__(self, reviews, targets, tokenizer, max_len):\n        self.reviews = reviews\n        self.targets = targets\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.reviews)\n\n    def __getitem__(self, item):\n        review = str(self.reviews[item])\n        target = self.targets[item]\n        encoding = self.tokenizer.encode_plus(\n            review,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            return_token_type_ids=False,\n            padding=\"max_length\",\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors=\"pt\",\n        )\n\n        return {\n            \"review_text\": review,\n            \"input_ids\": encoding[\"input_ids\"].flatten(),\n            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n            \"targets\": torch.tensor(target, dtype=torch.long),\n        }","7d8e5982":"def create_data_loader(df, tokenizer, max_len, batch_size):\n    ds = ReviewsDataset(\n        reviews=df[\"review_content\"].values,\n        targets=df[\"review_rating\"].astype(\"category\").values,\n        tokenizer=tokenizer,\n        max_len=max_len,\n    )\n\n    return DataLoader(ds, batch_size=batch_size, num_workers=4)","dc1f710b":"def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n    model = model.train()\n    losses = []\n    correct_predictions = 0\n    for d in data_loader:\n        input_ids = d[\"input_ids\"].to(device)\n        attention_mask = d[\"attention_mask\"].to(device)\n        targets = d[\"targets\"].to(device)\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        _, preds = torch.max(outputs, dim=1)\n        loss = loss_fn(outputs, targets)\n        correct_predictions += torch.sum(preds == targets)\n        losses.append(loss.item())\n        loss.backward()\n        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n        scheduler.step()\n        optimizer.zero_grad()\n    return correct_predictions.double() \/ n_examples, np.mean(losses)","dc199fc0":"def eval_model(model, data_loader, loss_fn, device, n_examples):\n    model = model.eval()\n    losses = []\n    correct_predictions = 0\n    with torch.no_grad():\n        for d in data_loader:\n            input_ids = d[\"input_ids\"].to(device)\n            attention_mask = d[\"attention_mask\"].to(device)\n            targets = d[\"targets\"].to(device)\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n            _, preds = torch.max(outputs, dim=1)\n            loss = loss_fn(outputs, targets)\n            correct_predictions += torch.sum(preds == targets)\n            losses.append(loss.item())\n    return correct_predictions.double() \/ n_examples, np.mean(losses)","01806a78":"def get_predictions(model, data_loader):\n    model = model.eval()\n    review_texts = []\n    predictions = []\n    prediction_probs = []\n    real_values = []\n\n    with torch.no_grad():\n        for d in data_loader:\n            texts = d[\"review_text\"]\n            input_ids = d[\"input_ids\"].to(device)\n            attention_mask = d[\"attention_mask\"].to(device)\n            targets = d[\"targets\"].to(device)\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n            _, preds = torch.max(outputs, dim=1)\n            review_texts.extend(texts)\n            predictions.extend(preds)\n            prediction_probs.extend(outputs)\n            real_values.extend(targets)\n\n    predictions = torch.stack(predictions).cpu()\n    prediction_probs = torch.stack(prediction_probs).cpu()\n    real_values = torch.stack(real_values).cpu()\n    return review_texts, predictions, prediction_probs, real_values","1feff037":"df_train, df_test = train_test_split(\n    non_empty_reviews_df,\n    test_size=0.1,\n    stratify=non_empty_reviews_df[\"review_rating\"],\n    random_state=random_seed\n)\n\ndf_val, df_test = train_test_split(\n    df_test,\n    test_size=0.5,\n    stratify=df_test[\"review_rating\"],\n    random_state=random_seed\n)","80dea5bc":"tokenizer = BertTokenizer.from_pretrained(pre_trained_model_name)","64ebd743":"train_data_loader = create_data_loader(df_train, tokenizer, max_len, batch_size)\nval_data_loader = create_data_loader(df_val, tokenizer, max_len, batch_size)\ntest_data_loader = create_data_loader(df_test, tokenizer, max_len, batch_size)","a11fc975":"total_steps = len(train_data_loader) * epochs","1654ce9f":"model = SentimentClassifier(\n    n_classes=non_empty_reviews_df[\"review_rating\"].nunique(), \n    model_name=pre_trained_model_name\n)\nmodel = model.to(device)","8ff845e3":"optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)","eed84435":"scheduler = get_linear_schedule_with_warmup(\n  optimizer,\n  num_warmup_steps=0,\n  num_training_steps=total_steps\n)","27072196":"loss_fn = nn.CrossEntropyLoss().to(device)","330a72e1":"history = defaultdict(list)\n\nbest_accuracy = 0\n\nfor epoch in range(epochs):\n    print(f\"Epoch {epoch + 1}\/{epochs}\")\n    print(\"=\" * 130)\n    \n    train_acc, train_loss = train_epoch(\n        model, train_data_loader, loss_fn, optimizer, device, scheduler, len(df_train)\n    )\n\n    print(f\"Train:\")\n    print(f\"\\tloss: {train_loss}\")\n    print(f\"\\taccuracy: {train_acc * 100:.02f}%\")\n\n    val_acc, val_loss = eval_model(model, val_data_loader, loss_fn, device, len(df_val))\n\n    \n    print(f\"Val:\")\n    print(f\"\\tloss: {val_loss}\")\n    print(f\"\\taccuracy: {val_acc * 100:.02f}%\")\n    print()\n\n    history[\"train_acc\"].append(train_acc)\n    history[\"train_loss\"].append(train_loss)\n    history[\"val_acc\"].append(val_acc)\n    history[\"val_loss\"].append(val_loss)\n\n    if val_acc > best_accuracy:\n        torch.save(model.state_dict(), \"best_model_state.bin\")\n        best_accuracy = val_acc","9b766adc":"f, ax = plt.subplots()\nax.plot(history[\"train_acc\"], label=\"train accuracy\")\nax.plot(history[\"val_acc\"], label=\"validation accuracy\")\nax.set_title(\"Training history\")\nax.set_ylabel(\"Accuracy\")\nax.set_xlabel(\"Epoch\")\nax.legend();","f357923a":"test_acc, _ = eval_model(\n  model,\n  test_data_loader,\n  loss_fn,\n  device,\n  len(df_test)\n)\n\nprint(f\"Test set accuracy: {test_acc.item() * 100:.02f}\")","73304f01":"y_review_texts, y_pred, y_pred_probs, y_test = get_predictions(model, test_data_loader)","8c21dd01":"print(classification_report(y_test, y_pred))","cebb25a9":"This notebook showed how to load and use the data and explored it superficially.\n\nI hope it will be useful to those wanting to use this dataset.","cf75f25f":"# Data","638a29d9":"# Constants","88cde7fe":"How many reviews are there without content i.e. with just a rating?","9ef5a43a":"# Sentiment Classification","7b4988c8":"# Data Exploration","7f99f7b3":"# Resources\n\n- [Sentiment Analysis with BERT and Transformers by Hugging Face using PyTorch and Python](https:\/\/curiousily.com\/posts\/sentiment-analysis-with-bert-and-hugging-face-using-pytorch-and-python\/)","55a2e338":"First we filter out the reviews that contain no content","3487de71":"# Reviews","ec6e2f42":"We can see here a sample of the courses data.","8a5df741":"We can see that most of the ratings have a value of 5.0","ada6ec94":"We will use [the point biserial correlation](https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.stats.pointbiserialr.html) which is used to measure the relationship between a binary variable, x, and a continuous variable, y","4b536035":"# Conclusion","d445fb98":"What is the proportion of courses according their instructional level?","3b1bdb7a":"This notebook will serve as an introduction and as a short tutorial on how to use the data in the [Udemy Courses and Reviews](https:\/\/www.kaggle.com\/koutetsu\/udemy-courses-and-reviews) dataset.\n\nThe dataset contains data about Udemy courses and their corresponding reviews fetched from their [Affiliate API](https:\/\/www.udemy.com\/developers\/affiliate\/).","375b40a0":"How good is the model on the test set?","c4aaa2ad":"We can clearly see that most reviews contain only a rating.","a37eeb08":"Next we split the dataset into a training, validation and test sets","73fa9b96":"What is the distribution of ratings?","5fc3e8ed":"What is the proportion of courses by category?","84d1304e":"## Courses","55e3d163":"The correlation is fairly low which indicates that there isn't much of a relation betweena review's value and whether it has content or not","1805ba85":"We will now a fine-tune a pre-trained neural network to do sentiment classification.\n\nWe will use Bert as our base model.","d75b86c1":"What is the correlation between the rating and there being content in the review?","63f40145":"# Libraries","3d0d9f08":"And here a sample of the reviews data.","0bd97f2f":"What is the proportion of courses by subcategory?","fcc08dfe":"After that, we define a few classes and functions that will be needed to fine-tune the model.","a5bac573":"In this section, we will proceed to explore the content of the dataset through different visualizations."}}