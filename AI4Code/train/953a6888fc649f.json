{"cell_type":{"14060729":"code","0d9a3f7b":"code","13d61e13":"code","adcf0524":"code","2bd391f4":"code","3672d951":"code","de66de08":"code","791f07d0":"code","a094249c":"code","a424d5b7":"code","d7a4586d":"code","c82cd71c":"code","bb682a8a":"code","371f2525":"code","92b743ea":"code","f2e2f398":"code","7402b3c5":"code","94042f7c":"code","b5f96678":"code","227b5787":"markdown","63497ca6":"markdown","46aa12d7":"markdown","3af0ac05":"markdown","2bf7d640":"markdown","dc1cf222":"markdown","2afdf3de":"markdown"},"source":{"14060729":"! pip uninstall torch torchvision -y\n! pip install torch==1.6.0+cu101 torchvision==0.7.0+cu101 -f https:\/\/download.pytorch.org\/whl\/torch_stable.html\n!pip install -U transformers\n!pip install -U simpletransformers  ","0d9a3f7b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","13d61e13":"import os, psutil  \n\ndef cpu_stats():\n    pid = os.getpid()\n    py = psutil.Process(pid)\n    memory_use = py.memory_info()[0] \/ 2. ** 30\n    return 'memory GB:' + str(np.round(memory_use, 2))","adcf0524":"cpu_stats()","2bd391f4":"import json\n\ndata_file = '..\/input\/arxiv\/arxiv-metadata-oai-snapshot.json'\n\ndef get_metadata():\n    with open(data_file, 'r') as f:\n        for line in f:\n            yield line","3672d951":"metadata = get_metadata()\nfor paper in metadata:\n    paper_dict = json.loads(paper)\n    print('Title: {}\\n\\nAbstract: {}\\nRef: {}'.format(paper_dict.get('title'), paper_dict.get('abstract'), paper_dict.get('journal-ref')))\n#     print(paper)\n    break","de66de08":"titles = []\nabstracts = []\nyears = []\nmetadata = get_metadata()\nfor paper in metadata:\n    paper_dict = json.loads(paper)\n    ref = paper_dict.get('journal-ref')\n    try:\n        year = int(ref[-4:]) \n        if 2016 < year < 2021:\n            years.append(year)\n            titles.append(paper_dict.get('title'))\n            abstracts.append(paper_dict.get('abstract'))\n    except:\n        pass \n\nlen(titles), len(abstracts), len(years)","791f07d0":"papers = pd.DataFrame({\n    'title': titles,\n    'abstract': abstracts,\n    'year': years\n})\npapers.head()","a094249c":"del titles, abstracts, years","a424d5b7":"cpu_stats()","d7a4586d":"import logging\nlogging.basicConfig(level=logging.INFO)\ntransformers_logger = logging.getLogger(\"transformers\")\ntransformers_logger.setLevel(logging.WARNING)\n","c82cd71c":"papers = papers[['title','abstract']]\npapers.columns = ['target_text', 'input_text']\npapers = papers.dropna()","bb682a8a":"eval_df = papers.sample(frac=0.2, random_state=101)\ntrain_df = papers.drop(eval_df.index)","371f2525":"train_df.shape, eval_df.shape","92b743ea":"import logging\n\nimport pandas as pd\nfrom simpletransformers.t5 import T5Model\n\nlogging.basicConfig(level=logging.INFO)\ntransformers_logger = logging.getLogger(\"transformers\")\ntransformers_logger.setLevel(logging.WARNING)\n\ntrain_df['prefix'] = \"summarize\"\neval_df['prefix'] = \"summarize\"\n\n\nmodel_args = {\n    \"reprocess_input_data\": True,\n    \"overwrite_output_dir\": True,\n    \"max_seq_length\": 512,\n    \"train_batch_size\": 16,\n    \"num_train_epochs\": 4,\n}\n\n# Create T5 Model\nmodel = T5Model(\"t5-small\", args=model_args, use_cuda=True)\n\n# Train T5 Model on new task\nmodel.train_model(train_df)\n\n# Evaluate T5 Model on new task\nresults = model.eval_model(eval_df)\n\n# Predict with trained T5 model\n#print(model.predict([\"convert: four\"]))","f2e2f398":"results","7402b3c5":"random_num = 350\nactual_title = eval_df.iloc[random_num]['target_text']\nactual_abstract = [\"summarize: \"+eval_df.iloc[random_num]['input_text']]\npredicted_title = model.predict(actual_abstract)\n\nprint(f'Actual Title: {actual_title}')\nprint(f'Predicted Title: {predicted_title}')\nprint(f'Actual Abstract: {actual_abstract}')\n","94042f7c":"random_num = 478\nactual_title = eval_df.iloc[random_num]['target_text']\nactual_abstract = [\"summarize: \"+eval_df.iloc[random_num]['input_text']]\npredicted_title = model.predict(actual_abstract)\n\nprint(f'Actual Title: {actual_title}')\nprint(f'Predicted Title: {predicted_title}')\nprint(f'Actual Abstract: {actual_abstract}')","b5f96678":"random_num = 999\nactual_title = eval_df.iloc[random_num]['target_text']\nactual_abstract = [\"summarize: \"+eval_df.iloc[random_num]['input_text']]\npredicted_title = model.predict(actual_abstract)\n\nprint(f'Actual Title: {actual_title}')\nprint(f'Predicted Title: {predicted_title}')\nprint(f'Actual Abstract: {actual_abstract}')","227b5787":"## \ud83e\udd17Transformers - Generating Articles from Paper's Abstracts using T5 Model\nThis notebook uses T5 model - A Sequence to Sequence model fully capable to perform any text to text tasks. What does it mean - It means that T5 model can take any input text and convert it into any output text. Such Text to Text conversion is useful in NLP tasks like language translation, summarization etc.\n\nIn this notebook, we will take paper's abstracts as our input text and paper's title as output text and feed it to T5 model. So,let's dive in...\n\n","63497ca6":"**We will take last 5 years ArXiv papers (2016-2021) due to Kaggle'c compute limits**","46aa12d7":"## And We're Done ! \n**Let's see how our model performs in generating paper's titles**","3af0ac05":"**Simpletransformers implementation of T5 model expects a data to be a dataframe with 3 columns:**\n`<prefix>, <input_text>, <target_text>`\n* `<prefix>`: A string indicating the task to perform. (E.g. \"question\", \"stsb\")\n* `<input_text>`: The input text sequence (we will use Paper's abstract as `input_text`  )\n* `<target_text`: The target sequence (we will use Paper's title as `output_text` )\n    \n    \n You can read about the data format:  https:\/\/github.com\/ThilinaRajapakse\/simpletransformers#t5-transformer","2bf7d640":" **We will use `simpletransformers` library to train a T5 model**","dc1cf222":"We will install dependencies and work with latest stable pytorch 1.6","2afdf3de":"**We will training out T5 model with very bare minimum `num_train_epochs=4`, `train_batch_size=16` to  fit into Kaggle's compute limits**"}}