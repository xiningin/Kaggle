{"cell_type":{"96c1426f":"code","ad8a0d73":"code","c1b67f9e":"code","fcffc408":"code","ec9fd065":"code","b6809b57":"code","2b347ec4":"code","a9af84ea":"code","8d47c5db":"code","6985e6f7":"code","b10ca8d6":"code","4f1ad4da":"markdown","0040ff15":"markdown","240f7a63":"markdown"},"source":{"96c1426f":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn import model_selection\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","ad8a0d73":"## create fold.py\nif __name__ == \"__main__\":\n    \n    df = pd.read_csv(\"..\/input\/cat-in-the-dat\/train.csv\")\n    \n    # we carete a new column called kfold and fill it with -1\n    df['kfold'] = -1\n    \n    # the next step is to randomize the rows of the data\n    df = df.sample(frac=1).reset_index(drop=True)\n    \n    # fetch labels\n    y = df.target.values\n    \n    # initiate the kfold class from model_selection module\n    kf = model_selection.StratifiedKFold(n_splits=5)\n    \n    # fill the new kfold column\n    for fold, (train_, valid_) in enumerate(kf.split(X=df, y=y)):\n        df.loc[valid_, 'kfold'] = fold\n        \n    # save the new csv with kfold column\n    df.to_csv(\".\/cat_train_fold.csv\", index=False)\n","c1b67f9e":"df.kfold.value_counts()","fcffc408":"df[df.kfold==0].target.value_counts()","ec9fd065":"df[df.kfold==1].target.value_counts()","b6809b57":"df[df.kfold==2].target.value_counts()","2b347ec4":"df[df.kfold==3].target.value_counts()","a9af84ea":"df[df.kfold==4].target.value_counts()","8d47c5db":"df_fold = pd.read_csv('.\/cat_train_fold.csv')\ndf_fold.head()","6985e6f7":"# One of the simplest models we can build is by one-hot encoding all the data and using logistic regression\n\nfrom sklearn import linear_model\nfrom sklearn import metrics\nfrom sklearn import preprocessing\n\ndef logistic_regression(fold):\n    # load the full training data with folds\n    df = pd.read_csv('.\/cat_train_fold.csv')\n    \n    # all columns are features except id, target and kfold columns\n    features = [\n        f for f in df.columns if f not in (\"id\", \"target\", \"kfold\")\n    ]\n    # fill all NaN values with NONE\n    #note that I am converting all columns to \"strings\"\n    # it doesn't matter because all are categories\n    for col in features:\n        df.loc[:, col] = df[col].astype(str).fillna(\"NONE\")\n        \n    # get training data using folds\n    df_train = df[df.kfold != fold].reset_index(drop=True) # get validation data using folds\n    df_valid = df[df.kfold == fold].reset_index(drop=True) # initialize OneHotEncoder from scikit-learn\n\n    ohe = preprocessing.OneHotEncoder()\n\n    # fit ohe on training + validation features\n    full_data = pd.concat(\n    [df_train[features], df_valid[features]], axis=0\n    )\n    ohe.fit(full_data[features])\n\n    # transform training data\n    x_train = ohe.transform(df_train[features])\n\n    # transform validation data\n    x_valid = ohe.transform(df_valid[features])\n    \n    # initialize Logistic Regression model\n    model = linear_model.LogisticRegression(solver='liblinear')\n\n    # fit model on training data (ohe)\n    model.fit(x_train, df_train.target.values)\n\n    # predict on validation data\n    # we need the probability values as we are calculating AUC # we will use the probability of 1s\n\n    valid_preds = model.predict_proba(x_valid)[:, 1]\n\n    # get roc auc score\n    auc = metrics.roc_auc_score(df_valid.target.values, valid_preds)\n\n    # print auc\n    print(f\"Fold = {fold}, AUC = {auc}\")\n    \n\nif __name__ == \"__main__\":\n    for fold_ in range(5):\n        logistic_regression(fold_)\n    ","b10ca8d6":"from sklearn import ensemble\ndef Random_forest(fold):\n    # load the full training data with folds\n    df = pd.read_csv('.\/cat_train_fold.csv')\n    \n    # all columns are features except id, target and kfold columns\n    features = [\n        f for f in df.columns if f not in (\"id\", \"target\", \"kfold\")\n    ]\n    # fill all NaN values with NONE\n    #note that I am converting all columns to \"strings\"\n    # it doesn't matter because all are categories\n    for col in features:\n        df.loc[:, col] = df[col].astype(str).fillna(\"NONE\")\n        \n    \n    # now its time to label endoe the features\n    for col in features:\n        \n        # initialize labelEncoder for each feature column\n        lbl = preprocessing.LabelEncoder()\n        \n        # fil label encdoer on all data\n        lbl.fit(df[col])\n        \n        # transform all the data\n        df.loc[:,col] = lbl.transform(df[col])\n        \n        \n    # get training data using folds\n    df_train = df[df.kfold != fold].reset_index(drop=True) # get validation data using folds\n    df_valid = df[df.kfold == fold].reset_index(drop=True) # initialize OneHotEncoder from scikit-learn\n\n    \n    # get training data\n    x_train = df_train[features].values\n    \n    # get validation data\n    x_valid = df_valid[features].values\n    \n    # initialize random forest model\n    model = ensemble.RandomForestClassifier(n_jobs=-1)\n    model.fit(x_train, df_train.target.values)\n\n    # predict on validation data\n    # we need the probability values as we are calculating AUC \n    # we will use the probability of 1s\n\n    valid_preds = model.predict_proba(x_valid)[:, 1]\n\n    # get roc auc score\n    auc = metrics.roc_auc_score(df_valid.target.values, valid_preds)\n\n    # print auc\n    print(f\"Fold = {fold}, AUC = {auc}\")\n    \n\nif __name__ == \"__main__\":\n    for fold_ in range(5):\n        Random_forest(fold_)","4f1ad4da":"We see that AUC scores are quite stable across all folds. <\/br>\nThe average AUC is : \n0.78631449527.<\/br> Quite good for our first model!\n<\/br>\n<\/br>\n<\/hr>\nMany people will start this kind of problem with a tree-based model, such as random forest. For applying random forest in this dataset, instead of one-hot encoding, we can use label encoding and convert every feature in every column to an integer as discussed previously.\n","0040ff15":"We see that in each fold, the distribution of targets is the same. This is what we need. It can also be similar and doesn\u2019t have to be the same all the time. Now, when we build our models, we will have the same distribution of targets across every fold.","240f7a63":"Wow! Huge difference! The random forest model, without any tuning of hyperparameters, performs a lot worse than simple logistic regression.<\/br><br>\n<h3><i>\n    And this is a reason why we should always start with simple models first. A fan of random forest would begin with it here and will ignore logistic regression model thinking it\u2019s a very simple model that cannot bring any value better than random forest. That kind of person will make a huge mistake. In our implementation of random forest, the folds take a much longer time to complete compared to logistic regression. So, we are not only losing on AUC but also taking much longer to complete the training. Please note that inference is also time-consuming with random forest and it also takes much larger space.<\/i><\/h3>"}}