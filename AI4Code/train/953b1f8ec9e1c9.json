{"cell_type":{"de0f34bd":"code","ef8612d5":"code","b9330fc9":"code","8bb4efd9":"code","f1a3a0a9":"code","bb670ce3":"code","fefef953":"code","74313253":"code","abd80917":"code","1e5aad6e":"code","89210019":"code","5c2fe0e8":"code","413c2467":"code","881e25b8":"code","877be9cb":"code","69f69513":"markdown","fd9ffd0a":"markdown","ae4aed6e":"markdown","b457857e":"markdown","4dc26273":"markdown","9327f829":"markdown","783190b3":"markdown"},"source":{"de0f34bd":"import pandas as pd","ef8612d5":"%%time\ntrain1 = pd.read_csv(\"..\/input\/jane-street-market-prediction\/train.csv\")\nprint(\"Data size:\", train1.shape)","b9330fc9":"train1.head(5)","8bb4efd9":"del(train1)","f1a3a0a9":"import dask.dataframe as dd","bb670ce3":"%%time\ntrain2 = dd.read_csv(\"..\/input\/jane-street-market-prediction\/train.csv\").compute()\nprint(\"Data size:\", train2.shape)","fefef953":"train2.head(5)","74313253":"del(train2)","abd80917":"!pip install ..\/input\/python-datatable\/datatable-0.11.0-cp37-cp37m-manylinux2010_x86_64.whl\nimport datatable as dt","1e5aad6e":"%%time\ntrain3 = dt.fread(\"..\/input\/jane-street-market-prediction\/train.csv\")\nprint(\"Data size:\", train3.shape)","89210019":"train3.head(5)","5c2fe0e8":"del(train3)","413c2467":"import sys\n!cp ..\/input\/rapids\/rapids.0.15.0 \/opt\/conda\/envs\/rapids.tar.gz\n!cd \/opt\/conda\/envs\/ && tar -xzvf rapids.tar.gz > \/dev\/null\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\/python3.7\/site-packages\"] + sys.path\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\/python3.7\"] + sys.path\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\"] + sys.path \n!cp \/opt\/conda\/envs\/rapids\/lib\/libxgboost.so \/opt\/conda\/lib\/\n\nimport cudf","881e25b8":"%%time\ntrain4 = cudf.read_csv(\"..\/input\/jane-street-market-prediction\/train.csv\")\nprint(\"Data size:\", train4.shape)","877be9cb":"train4.head(5)","69f69513":"<a id=\"2\"><\/a>\n# 2-Dask\n\nA Dask DataFrame is a large parallel DataFrame composed of many smaller Pandas DataFrames, split along the index. These Pandas DataFrames may live on disk for larger-than-memory computing on a single machine, or on many different machines in a cluster. One Dask DataFrame operation triggers many operations on the constituent Pandas DataFrames\n\n## When Dask is  the best choice?\n* Manipulating large datasets, even when those datasets don\u2019t fit in memory\n* Accelerating long computations by using many cores\n* Distributed computing on large datasets with standard Pandas operations like groupby, join, and time series computations\n\n## When Dask DataFrame may not be the best choice?\n* If your dataset fits comfortably into RAM on your laptop, then you may be better off just using Pandas. There may be simpler ways to improve performance than through parallelism\n* If your dataset doesn\u2019t fit neatly into the Pandas tabular model, then you might find more use in dask.bag or dask.array\n* If you need functions that are not implemented in Dask DataFrame, then you might want to look at dask.delayed which offers more flexibility\n* If you need a proper database with all that databases offer you might prefer something like Postgres\n\nReference: https:\/\/docs.dask.org\/en\/latest\/","fd9ffd0a":"<a id=\"4\"><\/a>\n# 4-Rapids\n\nThe RAPIDS suite of open source software libraries and APIs gives you the ability to execute end-to-end data science and analytics pipelines entirely on GPUs.\n\nReference: https:\/\/docs.rapids.ai\/\n\nmake sure to turn on GPU","ae4aed6e":"# How to Read Large Dataset\n![Big+Data+Solutions+-+Alliance+Technology+Group.png](attachment:Big+Data+Solutions+-+Alliance+Technology+Group.png)\n\n\nWhen you need to read large dataset which has size greater than RAM, your system will run out of RAM while reading such a huge amount of data, and this also might lead to kernel shutdown in notebook or system crash.\n\nPython data scientists often use Pandas for working with tables. While Pandas is perfect for small to medium-sized datasets, larger ones are problematic.\n\nIn this kernel, I show how to deal with large datasets using other techniques\n\n* [1-Pandas](#1)\n* [2-Dask](#2)\n* [3-Datatable](#3)\n* [4-Rapids](#4)\n\n","b457857e":"# PLEASE UPVOTE if you like this kernel.\n","4dc26273":"<a id=\"3\"><\/a>\n# 3-Datatable\nThe Datatable is a python library for manipulating tabular data.  It is super fast, much faster than pandas and has the ability to work with out-of-memory datasets, multi-threaded data processing, and flexible API.for its performance, it is on path to become a must-use package for data manipulation in python.\n\nReference: https:\/\/datatable.readthedocs.io\/en\/latest\/index.html","9327f829":"<a id=\"1\"><\/a>\n# 1-Pandas\n\nPandas provides data structures for in-memory analytics, which makes using pandas to analyze datasets that are larger than memory datasets somewhat tricky. Even datasets that are a sizable fraction of memory become unwieldy, as some pandas operations need to make intermediate copies.\n\nThis document ([Scaling to large datasets](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/user_guide\/scale.html)) provides a few recommendations for scaling your analysis to larger datasets. It\u2019s a complement to Enhancing performance, which focuses on speeding up analysis for datasets that fit in memory.\n\n\n\nReference: https:\/\/pandas.pydata.org\/docs\/","783190b3":"## References\nhttps:\/\/www.kaggle.com\/rohanrao\/tutorial-on-reading-large-datasets"}}