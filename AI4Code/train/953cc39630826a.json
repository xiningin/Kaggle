{"cell_type":{"24a6579f":"code","9a013db3":"code","65774d15":"code","f750f398":"code","501dc1ab":"code","73405591":"code","0d4b9dc2":"code","4ae1b8a8":"code","bb863bcb":"code","14d59089":"code","a6a15ed5":"code","28971f6d":"code","a41d85d9":"code","4c8af251":"code","4249b8d4":"code","5b0b55e2":"code","b1599a6a":"code","b0db2a51":"code","b6600adc":"code","debfc10c":"code","5af0592e":"code","412281f9":"code","025bc275":"code","af8b1cef":"code","f7f8699b":"code","2b235b79":"code","ff2a872e":"code","64745ad5":"code","d01090f7":"code","5f697f2b":"code","0152b13f":"code","aa2e0632":"code","bac9a5de":"code","bfafa8fd":"code","9084d60f":"code","77a7556c":"code","2ec8ff75":"code","d70bf9b3":"code","406e9b89":"code","179e9a3e":"code","be3d5e64":"markdown","06d3f084":"markdown","ec31c4ef":"markdown","d601dd7e":"markdown","8ce30e02":"markdown","f9ae52a4":"markdown","47ec8a37":"markdown","26f266b1":"markdown","9d596acb":"markdown","4894511b":"markdown","65d78d6f":"markdown","cc8a9217":"markdown","0f7e3cbe":"markdown","b6999b73":"markdown","fdc9b861":"markdown","6ca398d6":"markdown","5add1002":"markdown","8539bd25":"markdown","feda8654":"markdown","30345f8d":"markdown","046d1899":"markdown","9a87e241":"markdown","5091df64":"markdown","3c3e41f7":"markdown","13efc7e4":"markdown","4d8fa424":"markdown","87112420":"markdown","11688e96":"markdown","1573bfe1":"markdown","cf768f04":"markdown","e673e757":"markdown","2d07d37c":"markdown","d33fd185":"markdown","b0e63c28":"markdown","a6cfee22":"markdown","e4b17db6":"markdown","4146fb9a":"markdown","43aed585":"markdown","e0718cd8":"markdown","f5632e17":"markdown","cddf0e96":"markdown","cfd4dc81":"markdown","26e44c39":"markdown","2ce8ca4f":"markdown","3791e99d":"markdown","3c681523":"markdown","4e19e6b7":"markdown","384838da":"markdown","95cc2308":"markdown","3eed95f6":"markdown","03c2ab30":"markdown","021848a4":"markdown","2edaab75":"markdown"},"source":{"24a6579f":"## packages\n\n# data processing\nimport numpy as np # linear algebra\nimport pandas as pd # data processing\n\n# visuals\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\n\n# Tree\nfrom sklearn import tree\nimport graphviz\n\n# preproessing\nfrom sklearn.preprocessing import RobustScaler,StandardScaler\nfrom sklearn.model_selection import train_test_split,GridSearchCV\n\n# SMOTE treat class imbalance\nfrom imblearn.over_sampling import (SMOTE,ADASYN)\n\n# model evaluation\nfrom sklearn import metrics\n\n# model explainablity\nimport eli5\nfrom eli5.sklearn import PermutationImportance\n\n# html\nfrom IPython.core.display import HTML\n\n# plotly offline\nfrom plotly.offline import download_plotlyjs,init_notebook_mode\ninit_notebook_mode(connected=True)\n\n# MISC\nimport warnings\nwarnings.filterwarnings(\"ignore\")","9a013db3":"df = pd.read_csv('..\/input\/heart-attack-analysis-prediction-dataset\/heart.csv')","65774d15":"df.head()","f750f398":"# rename the variables\ndf.columns = ['Age','Sex','chest_pain_type','resting_blood_pressure','serum_cholestoral_mg',\n              'fasting_blood_sugar','resting_electrocardiographic','maximum_heart_rate',\n             'exercise_induced_angina','ST_depression','slope','no._major_vessels','Thalassemia','Target']","501dc1ab":"df.head() ","73405591":"num_cols =['Age','serum_cholestoral_mg','maximum_heart_rate','resting_blood_pressure','ST_depression','Target']\n\n\ncat_cols =['slope','Sex','chest_pain_type','fasting_blood_sugar',\n           'Thalassemia','slope','no._major_vessels',\n           'resting_electrocardiographic','exercise_induced_angina']","0d4b9dc2":"# stats\nfi_df =df[num_cols]\nprint('stats of people who had higher chance of heart attack')\nfi_df[df['Target']==1].describe().T","4ae1b8a8":"fi_df =df[num_cols]\nprint('stats of people who had low chance of heart attack')\nfi_df[df['Target']==0].describe().T","bb863bcb":"# pairplot\n\nsns.set_context(context='notebook',font_scale=1)\nsns.pairplot(df.drop(cat_cols,axis=1),hue='Target');\nplt.tight_layout()","14d59089":"# plot\nplt.figure(figsize=(12,6))\nsns.set_context(context='notebook',font_scale=1.2)\nsns.heatmap(df[['Age','serum_cholestoral_mg','maximum_heart_rate',\n                'resting_blood_pressure','ST_depression']].corr('pearson'),annot=True,cmap='Blues');\nplt.title('Pearson correlation');\nplt.tight_layout();","a6a15ed5":"# Target\nsns.set_context(context='notebook',font_scale=1)\nsns.countplot(df['Target']);\nplt.tight_layout();\nprint(df['Target'].value_counts(1)*100)#target class precentage","28971f6d":"# outliers are extreme values in the variables\ndf.plot(kind='box',figsize=(14,6));\nplt.xticks(rotation=75);\nplt.title('Outliers');\nplt.tight_layout()","a41d85d9":"%%HTML\n<div class='tableauPlaceholder' id='viz1624963845064' style='position: relative'><noscript><a href='#'><img alt='All in one ' src='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;he&#47;heart_attack_dataset&#47;Allinone&#47;1_rss.png' style='border: none' \/><\/a><\/noscript><object class='tableauViz'  style='display:none;'><param name='host_url' value='https%3A%2F%2Fpublic.tableau.com%2F' \/> <param name='embed_code_version' value='3' \/> <param name='site_root' value='' \/><param name='name' value='heart_attack_dataset&#47;Allinone' \/><param name='tabs' value='no' \/><param name='toolbar' value='yes' \/><param name='static_image' value='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;he&#47;heart_attack_dataset&#47;Allinone&#47;1.png' \/> <param name='animate_transition' value='yes' \/><param name='display_static_image' value='yes' \/><param name='display_spinner' value='yes' \/><param name='display_overlay' value='yes' \/><param name='display_count' value='yes' \/><param name='language' value='en-US' \/><\/object><\/div>                <script type='text\/javascript'>                    var divElement = document.getElementById('viz1624963845064');                    var vizElement = divElement.getElementsByTagName('object')[0];                    if ( divElement.offsetWidth > 800 ) { vizElement.style.width='100%';vizElement.style.height=(divElement.offsetWidth*0.75)+'px';} else if ( divElement.offsetWidth > 500 ) { vizElement.style.width='100%';vizElement.style.height=(divElement.offsetWidth*0.75)+'px';} else { vizElement.style.width='100%';vizElement.style.height='2377px';}                     var scriptElement = document.createElement('script');                    scriptElement.src = 'https:\/\/public.tableau.com\/javascripts\/api\/viz_v1.js';                    vizElement.parentNode.insertBefore(scriptElement, vizElement);                <\/script>","4c8af251":"# missing values\nmissing_value = 100* df.isnull().sum()\/len(df)\nmissing_value = missing_value.reset_index()\nmissing_value.columns = ['variables','missing values in percentage']\nmissing_value = missing_value.sort_values('missing values in percentage',ascending=False)\n\n# barplot\nfig = px.bar(missing_value, y='missing values in percentage',x='variables',title='Missing values % in each column',\n             template='ggplot2',text='missing values in percentage');\nfig.update_traces(texttemplate='%{text:.2s}', textposition='inside')\nfig.update_layout(uniformtext_minsize=8, uniformtext_mode='hide')\n\nfig.show()","4249b8d4":"# visual check for any empyty string,-999,0\n\ndef visual_check(df):\n    \"\"\"\n    This function will print the unqiue values in each\n    variable and return list of column with zeros \n    \"\"\"\n\n    counter = 0\n    cols_with_zeros = []\n    zero_in_col= False\n    \n\n    for feature in df.columns:\n\n        counter += 1\n\n        print('\\n')\n        print('***********************************************************')\n        print(f'Col. NO.{counter} Column name: {feature}')\n        print('***********************************************************')\n        print(' ')\n        print('1. Unique vlaues:',df[feature].unique())\n        print(' ')\n\n\n        try:\n\n            print('2. Min values:',df[feature].min())\n            print(' ')\n            print('3. Max values:',df[feature].max())\n            print(' ')\n            print('4. no. unique:',df[feature].nunique())\n            print(' ')\n            print('5. value counts:')\n            print(df[feature].value_counts(1)*100)\n            print(' ')\n            print('**************************************************')\n            print('--------------------------------------------------')\n            print('\\n ')\n            \n\n            if df[feature].min()==0:\n                               \n                cols_with_zeros.append(feature)  \n\n            else:\n                zero_in_col= False\n             \n        except:\n            print('min and max unsupported')\n\n            \n    return cols_with_zeros ","5b0b55e2":"#  visual check for special characters\nvisual_check(df)","b1599a6a":"# outliers\ndf.plot(kind='box',figsize=(12,6));\nplt.xticks(rotation=80);","b0db2a51":"# model metrics function\n\ndef model_metrics(X_train, X_test, y_train, y_test, model, name):\n    \"\"\"\n    This fuction will print accuracy, F1 score\n    and confusion matrix\n    \n    \"\"\"\n\n    # model predict\n    predict_train = model.predict(X_train)\n    predict_test = model.predict(X_test)\n\n    # accuracy score\n    train_score = model.score(X_train,y_train)\n    test_score = model.score(X_test,y_test)\n\n    # f1-score\n    f1_score = metrics.f1_score(y_test, predict_test)\n\n    print(f'{name} Accuracy on Train set',train_score)\n    print(f'{name} Accuracy on Test set',test_score)\n    print(f'{name} F1-score on Test set:',f1_score)\n    print('\\n')\n    print(metrics.classification_report(y_test, predict_test))\n    print('\\n')\n\n    # confusion matrix\n    metrics.plot_confusion_matrix(model,X_test,y_test,cmap='Blues');\n    plt.grid(False)\n    plt.title(f'{name} Confusion Matrix on test set');\n    ","b6600adc":"X= df.drop('Target',axis=1)\ny= df.pop('Target')","debfc10c":"# Data split\n\nX_train, X_test,y_train,y_test = train_test_split(X,y,test_size=0.30,random_state =101)","5af0592e":"print(y_train.value_counts(1))\nprint(y_test.value_counts(1))","412281f9":"## Scaling data\n\nrc = RobustScaler()\n\nX_train = rc.fit_transform(X_train)\nX_test = rc.transform(X_test)","025bc275":"## ADASYN oversampling\n\nADASYN = ADASYN(random_state=101)\nX_train,y_train = ADASYN.fit_resample(X_train, y_train.ravel())","af8b1cef":"from sklearn.tree import DecisionTreeClassifier\n\nDT_model= DecisionTreeClassifier(max_features= 5,max_depth= 7,min_samples_split= 90,min_samples_leaf= 30,random_state=42)\n\n# fit the model\nDT_model.fit(X_train,y_train)\n\n# model score\nmodel_metrics(X_train, X_test,y_train,y_test,DT_model,'Decision_Tree')","f7f8699b":"# Random forest\nfrom sklearn.ensemble import RandomForestClassifier\n\nRF_model = RandomForestClassifier(n_estimators= 600,min_samples_split= 90,min_samples_leaf= 20,max_features= 5,max_depth= 10)\n\n# fit the model\nRF_model.fit(X_train,y_train)\n\n# model score function\nmodel_metrics(X_train, X_test,y_train,y_test,RF_model,'Random_forest')","2b235b79":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\n\nLDA = LinearDiscriminantAnalysis(solver='svd')\n\n# fit the model\nLDA.fit(X_train, y_train)\n\n# model score function\nmodel_metrics(X_train, X_test,y_train,y_test,LDA,'LDA')","ff2a872e":"from sklearn.naive_bayes import GaussianNB\n\nNB_model = GaussianNB()\n\n# fit the model\nNB_model.fit(X_train, y_train)\n\n# model score function\nmodel_metrics(X_train, X_test,y_train,y_test,NB_model,'GaussianNB')","64745ad5":"# Xgboost\nimport xgboost as xgb\n\n# create object model\nXgboost_model = xgb.XGBClassifier(learning_rate=0.01,verbosity=0)\n\n# fit the model\nXgboost_model.fit(X_train,y_train)\n\n# model score function\nmodel_metrics(X_train, X_test,y_train,y_test,Xgboost_model,'Xgboost_model')","d01090f7":"from sklearn.ensemble import AdaBoostClassifier\n\n# create object model\nAdaboost_model = AdaBoostClassifier(n_estimators=500,learning_rate=0.01)\n\n# fit the model\nAdaboost_model.fit(X_train,y_train)\n\n# model score function\nmodel_metrics(X_train, X_test,y_train,y_test,Adaboost_model,'Adaboost_model')","5f697f2b":"from sklearn.ensemble import GradientBoostingClassifier\n\n# create object model\ngradientboost_model = GradientBoostingClassifier(learning_rate=0.01,n_estimators=600,subsample=1.0)\n\n# fit the model\ngradientboost_model.fit(X_train,y_train)\n\n# model score function\nmodel_metrics(X_train, X_test,y_train,y_test,gradientboost_model,'gradientboost_model')\n","0152b13f":"from lightgbm import LGBMClassifier\n\n# create object model\nLightBGM  = LGBMClassifier(learning_rate=0.01,n_estimators=60,subsample=1.0,max_depth=5)\n\n# fit the model\nLightBGM.fit(X_train,y_train)\n\n# model score function\nmodel_metrics(X_train, X_test,y_train,y_test,LightBGM ,'LightBGM')\n","aa2e0632":"from sklearn.svm import SVC\n\n# create object model\nSVM = SVC(probability=True)\n\n# fit the model\nSVM .fit(X_train,y_train)\n\n# model score function\nmodel_metrics(X_train, X_test,y_train,y_test,SVM ,'SVM')","bac9a5de":"from sklearn.neighbors import KNeighborsClassifier\n\n# create object model\nKNN = KNeighborsClassifier()\n\n# fit the model\nKNN.fit(X_train,y_train)\n\n# model score function\nmodel_metrics(X_train, X_test,y_train,y_test,KNN ,'KNN')","bfafa8fd":"from sklearn.ensemble import VotingClassifier\n\nvoting_classifier = VotingClassifier(estimators=[\n    ('LightBGM', LightBGM),\n    ('RF_model', RF_model),\n    ('NB_model',NB_model),\n    ('Xgboost_model',Xgboost_model),\n    ('SVM',SVM),\n    ('Adaboost_model',Adaboost_model),\n    \n\n],voting='soft')\n\n# voting classifier\nvoting_classifier = voting_classifier.fit(X_train,y_train.ravel())\n\n# model score function\nmodel_metrics(X_train, X_test,y_train,y_test,voting_classifier ,'voting_classifier')","9084d60f":"models_list = [KNN,SVM,LightBGM,gradientboost_model,Adaboost_model,Xgboost_model,NB_model,LDA,RF_model,DT_model,voting_classifier]\nrecall =[]\nprecision =[]\ntest_acc = []\ntrain_acc = []\nf1_score = []\n\nfor model in models_list:\n    predict_test = model.predict(X_test)\n    predict_train = model.predict(X_train)\n    f1s = metrics.f1_score(y_test, predict_test)\n    pre = metrics.precision_score(y_test, predict_test)\n    rec = metrics.recall_score(y_test, predict_test)\n    acc_test = model.score(X_test,y_test)\n    acc_train = model.score(X_train,y_train)\n    \n    recall.append(rec)\n    precision.append(pre)\n    test_acc.append(acc_test)\n    train_acc.append(acc_train)\n    f1_score.append(f1s)\n    \nmodel_compare = pd.DataFrame({\n'Models':['KNN','SVM','LightBGM','gradientboost_model','Adaboost_model',\n          'Xgboost_model','NB_model','LDA','RF_model','DT_model','voting_classifier'],\n'recall':recall,\n'Precision':precision,\n'f1_score':f1_score,\n'Accuracy on Test':test_acc,\n'Accuracy on Train':train_acc\n})\n","77a7556c":"model_compare.sort_values(['f1_score','Precision'],ascending=[False,False],inplace=True)\nmodel_compare.style.background_gradient(cmap='coolwarm_r')","2ec8ff75":"# Decision tree\nTree_plot = tree.export_graphviz(DT_model,feature_names=df.columns)\ngraphviz.Source(Tree_plot)","d70bf9b3":"# random forest feature importance \n\nfeature_importances = pd.Series(RF_model.feature_importances_, index=df.columns);\nfeature_importances.nlargest(15).plot(kind='barh',figsize=(10,6));\nplt.title('Random forest feature_importances');","406e9b89":"# permutation importance for Voting classifier\n\nimport eli5\nfrom eli5.sklearn import PermutationImportance\n\npermutation_for_NB = PermutationImportance(voting_classifier,random_state=1,scoring='f1').fit(X_test,y_test)\neli5.show_weights(permutation_for_NB, feature_names = df.columns.tolist())","179e9a3e":"# SVM permutation importance\n\npermutation = PermutationImportance(Xgboost_model,random_state=1,scoring='f1').fit(X_test,y_test)\neli5.show_weights(permutation, feature_names = df.columns.tolist())","be3d5e64":"# Model comparison\n\n<h5> <span style=\"font-family: Arial;font-size:1.2em;color:#3366ff\"> objective here is we need choose the model with higher f1_score and with higher precision\n","06d3f084":"# EDA","ec31c4ef":"<div style=\"color:white;\n           display:fill;\n           border-radius:40px;\n           background-color:#88d8b0;\n           font-size:300%;\n           font-family:Arial;\n           letter-spacing:0.20px\">\n<p style=\"padding: 10px;\n          color:black;\n          text-align:center;\">Model comparison \n<\/p>\n<\/div>","d601dd7e":"#### <span style=\"font-family: Arial;font-size:1.2em;color:#3366ff\">10. KNeighborsClassifier","8ce30e02":"<div style=\"color:white;\n           display:fill;\n           border-radius:40px;\n           background-color:#88d8b0;\n           font-size:300%;\n           font-family:Arial;\n           letter-spacing:0.20px\">\n<p style=\"padding: 10px;\n          color:black;\n          text-align:center;\">End\n<\/p>\n<\/div>","f9ae52a4":"* <span style=\"font-family:Arial;font-size:1.2em;color:#3366ff;\"> some of the distributions are not well separated that are serum_cholestoral_mg and resting blood pressure which shows that these variables are least helpful for the model","47ec8a37":"# Packages","26f266b1":"<div style=\"color:white;\n           display:fill;\n           border-radius:40px;\n           background-color:#88d8b0;\n           font-size:300%;\n           font-family:Arial;\n           letter-spacing:0.10px\">\n<p style=\"padding: 10px;\n          color:black;\n          text-align:center;\">voting classifier\n<\/p>\n<\/div>","9d596acb":"#### <span style=\"font-family: Arial;font-size:1.2em;color:#3366ff\">6. Adaboostclassifier","4894511b":"#### <span style=\"font-family: Arial;font-size:1.2em;color:#3366ff\">8. LGBMClassifier","65d78d6f":"# Feature importance","cc8a9217":"<h4> Index\n\n* <a href=\"#Packages\">1.1.Packages<\/a>\n* <a href=\"#EDA\">1.2.EDA<\/a>\n* <a href=\"#Tableau-dashboard\">1.3.Tableau Dashboard<\/a>\n* <a href=\"#Preprocessing\">1.4.Preprocessing<\/a>\n* <a href=\"#Modeling\">1.5.Modeling<\/a>\n* <a href=\"#Feature-importance\">1.6.Feature importance\n* <a href=\"#Permutation-Importance\">1.7.Permutation Importance<\/a>\n* <a href=\"#Model-comparison\">1.8.Model comparison<\/a>\n* <a href=\"#Reference\">1.9.Reference<\/a>","0f7e3cbe":"<h5> <span style=\"font-family: Arial;font-size:1.2em;color:#3366ff\"> if you like the work hit the upvote and feel free to post any suggestions","b6999b73":"<div style=\"color:white;\n           display:fill;\n           border-radius:40px;\n           background-color:#88d8b0;\n           font-size:300%;\n           font-family:Arial;\n           letter-spacing:0.10px\">\n<p style=\"padding: 10px;\n          color:black;\n          text-align:center;\">Modeling\n<\/p>\n<\/div>","fdc9b861":"# VotingClassifier","6ca398d6":"### <span style=\"font-family: Arial;font-size:1.2em;color:#3366ff\">iv. data split","5add1002":"#### <span style=\"font-family: Arial;font-size:1.2em;color:#3366ff\">7. GradientBoostingClassifier","8539bd25":"* <span style=\"font-family: Arial;font-size:1.2em;color:#3366ff\"> The mean heart rate for those who had a higher chance of heart attack is 158.46 vs 139.10 who doesn't have a heart attack\n","feda8654":"# Thank you!","30345f8d":"* <span style=\"font-family: Arial;font-size:1.2em;color:#3366ff\">most of the variables have outliers","046d1899":"<div style=\"color:white;\n           display:fill;\n           border-radius:40px;\n           background-color:#88d8b0;\n           font-size:300%;\n           font-family:Arial;\n           letter-spacing:0.10px\">\n<p style=\"padding: 10px;\n          color:black;\n          text-align:center;\">Preprocessing\n<\/p>\n<\/div>","9a87e241":"#### <span style=\"font-family: Arial;font-size:1.2em;color:#3366ff\">9. SVM","5091df64":"#### <span style=\"font-family: Arial;font-size:1.2em;color:#3366ff\"> 4. Naive bayes","3c3e41f7":"#### <span style=\"font-family: Arial;font-size:1.2em;color:#3366ff\"> 5. Xgboost","13efc7e4":"### <span style=\"font-family: Arial;font-size:1.2em;color:#3366ff\">ii. Data cleaning","4d8fa424":"<span style=\"font-family:Arial;font-size:1.2em;color:#3366ff\">we can see that model has given importance to the feature like number of major vessels, chest pain type, Thalassemia, chest_pain_type and age","87112420":"<div style=\"color:white;\n           display:fill;\n           border-radius:40px;\n           background-color:#88d8b0;\n           font-size:300%;\n           font-family:Arial;\n           letter-spacing:0.10px\">\n<p style=\"padding: 10px;\n          color:black;\n          text-align:center;\">EDA\n<\/p>\n<\/div>","11688e96":"### <span style=\"font-family: Arial;font-size:1.2em;color:#3366ff\">vii. Oversampling","1573bfe1":"<div style=\"color:white;\n           display:fill;\n           border-radius:40px;\n           background-color:#88d8b0;\n           font-size:300%;\n           font-family:Arial;\n           letter-spacing:0.20px\">\n<p style=\"padding: 10px;\n          color:black;\n          text-align:center;\">Permutation importance\n<\/p>\n<\/div>","cf768f04":"* <span style=\"font-family: Arial;font-size:1.2em;color:#3366ff\">there are no high correlation among the variables","e673e757":"* <span style=\"font-family: Arial;font-size:1.2em;color:#3366ff\"> we can use SVM, Adaboost and Xgboost_model that has a balance of precision and recall for the model for an overall much better prediction\n* <span style=\"font-family: Arial;font-size:1.2em;color:#3366ff\"> we can use voting classifier that combines the output of the models and use the average probability of the models for prediction\n","2d07d37c":"# Data dictionary\n* Age : Age of the patient\n\n* Sex : Sex of the patient\n\n* exang: exercise induced angina (1 = yes; 0 = no)\n\n* ca: number of major vessels (0-3)\n\n* cp : Chest Pain type chest pain type\n\n* trtbps : resting blood pressure (in mm Hg)\n\n* chol : cholestoral in mg\/dl fetched via BMI sensor\n\n* fbs : (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false)\n\n* rest_ecg : resting electrocardiographic results\n\n* target : 0= less chance of heart attack 1= more chance of heart attack","d33fd185":"* <span style=\"font-family: Arial;font-size:1.2em;color:#3366ff\">Outliers are not treated as this a medical data need to proceed with caution for treating outliers \n* <span style=\"font-family: Arial;font-size:1.2em;color:#3366ff\">some of the oulier data points may be the cause of heart attack so replacing that data points may affect the predict power of the model still we can check the metrics and then we can decide\n\n","b0e63c28":"# Reference","a6cfee22":"### <span style=\"font-family: Arial;font-size:1.2em;color:#3366ff\">2. Random Forest","e4b17db6":"# Modeling\n\n##### for modeling we can use bagging, boosting techniques and voting classifier\n##### since I have't treated outlier some of the boosting models will get affected\n\n*  <span style=\"font-family: Arial;font-size:1.2em;color:#3366ff\">i. Decision Tree\n*  <span style=\"font-family: Arial;font-size:1.2em;color:#3366ff\">ii. Random Forest\n*  <span style=\"font-family: Arial;font-size:1.2em;color:#3366ff\">iii. LDA\n*  <span style=\"font-family: Arial;font-size:1.2em;color:#3366ff\">iv. Naive bayse\n* <span style=\"font-family: Arial;font-size:1.2em;color:#3366ff\">v. XGboost\n* <span style=\"font-family:Arial;font-size:1.2em;color:#3366ff\">vi. Adaboostclassifier\n* <span style=\"font-family:Arial;font-size:1.2em;color:#3366ff\">vii. GradientBoostingClassifier\n* <span style=\"font-family:Arial;font-size:1.2em;color:#3366ff\">viii. LGBMClassifier\n* <span style=\"font-family:Arial;font-size:1.2em;color:#3366ff\">ix. SVM\n* <span style=\"font-family:Arial;font-size:1.2em;color:#3366ff\">x. KNN\n  ","4146fb9a":"<h1> <span style=\"font-family: Arial;font-size:1.2em;color:#3366ff\"> Heart attack dataset","43aed585":"* <span style=\"font-family:Arial;font-size:1.2em;color:#3366ff\">Decision tree Gini sore shows that important given to no._major vessels or ca it seems that referred to Fluoroscopy it is imaging technique that analyzes the blood flow in the heart vessels","e0718cd8":"### <span style=\"font-family: Arial;font-size:1.2em;color:#3366ff\">1.Decision Tree","f5632e17":"### <span style=\"font-family: Arial;font-size:1.2em;color:#3366ff\">iii.Outliers ","cddf0e96":"* <span style=\"font-family: Arial;font-size:1.2em;color:#3366ff\"> target variable has chance of heart attack yes 54% and no 45%\n    ","cfd4dc81":"<h4> <span style=\"font-family: Arial;font-size:1.2em;color:#333333\"> Objective: To predict the chance of Heart attack","26e44c39":"# permutation importance\n\n* <span style=\"font-family: Arial;font-size:1.2em;color:#3366ff\"> Permutation importance is stated to be the decrease in a model Accuracy or any user-defined metrics when a single Independent variable is randomly shuffled and observe the difference in the output to know what are variables are given importance by the model to predict","2ce8ca4f":"*  Data dictionary: https:\/\/archive.ics.uci.edu\/ml\/datasets\/heart+disease\n* https:\/\/www.kaggle.com\/dansbecker\/permutation-importance referred Permutation Importance from Dan Becker's Notebook\n* https:\/\/www.kaggle.com\/amiiiney\/titanic-top-20-with-ensemble-votingclassifier#4--Encode-categorical-feature # voting classifier","3791e99d":"# Tableau dashboard","3c681523":"# stats","4e19e6b7":"<div style=\"color:white;\n           display:fill;\n           border-radius:40px;\n           background-color:#88d8b0;\n           font-size:300%;\n           font-family:Arial;\n           letter-spacing:0.20px\">\n<p style=\"padding: 10px;\n          color:black;\n          text-align:center;\">Index \n<\/p>\n<\/div>","384838da":"*  <span style=\"font-family: Arial;font-size:1.2em;color:#3366ff\"> no missing values are found in the data however we need to check for special characters and empty strings","95cc2308":"#### <span style=\"font-family: Arial;font-size:1.2em;color:#3366ff\">3. LDA","3eed95f6":"### <span style=\"font-family: Arial;font-size:1.2em;color:#3366ff\">vi. Scaling","03c2ab30":"<div style=\"color:white;\n           display:fill;\n           border-radius:40px;\n           background-color:#88d8b0;\n           font-size:300%;\n           font-family:Arial;\n           letter-spacing:0.20px\">\n<p style=\"padding: 10px;\n          color:black;\n          text-align:center;\">Feature importance \n<\/p>\n<\/div>","021848a4":"# Preprocessing\n\n* <span style=\"font-family: Arial;font-size:1.2em;color:#3366ff\">i. Missing Values","2edaab75":"<div style=\"color:white;\n           display:fill;\n           border-radius:40px;\n           background-color:#88d8b0;\n           font-size:300%;\n           font-family:Arial;\n           letter-spacing:0.10px\">\n<p style=\"padding: 10px;\n          color:black;\n          text-align:center;\">Tableau dashboard\n<\/p>\n<\/div>"}}