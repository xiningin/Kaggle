{"cell_type":{"c455996e":"code","e4b71b72":"code","5dffee09":"code","338c6c3d":"code","c3c0a99f":"code","9207bd8d":"code","40417271":"code","bfc32f6a":"code","1b51f3d2":"code","69807c49":"code","1c90e77b":"code","25760592":"code","3971238a":"code","ab351f05":"code","11962abf":"code","5255b3f7":"code","a03235b3":"code","47df22dc":"code","1a980202":"code","7a0306de":"code","f2440d95":"code","c27bfd2e":"markdown","e5b4af92":"markdown","3e95fa14":"markdown","84d64a6d":"markdown","2cf200be":"markdown","87fe055f":"markdown","6dc13d4b":"markdown","f21e4cad":"markdown","21dcb5f4":"markdown","c591c780":"markdown","2322c464":"markdown","77fc8d62":"markdown","97e2427a":"markdown","649c0678":"markdown","67ccc9cb":"markdown","728711b4":"markdown","5593e63d":"markdown","38eccd89":"markdown"},"source":{"c455996e":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers \nimport tensorflow_datasets as tfds\ntfds.disable_progress_bar()","e4b71b72":"embedding_layer = layers.Embedding(1000, 5)\n# Here inside the Embedding(), the first number is the input_size and second one is the output_size","5dffee09":"result = embedding_layer(tf.constant([1,2,3, 999]))\nresult.numpy()","338c6c3d":"result = embedding_layer(tf.constant([[0,1,2],[3,4,5]]))\nprint(result.shape)\nprint(result[0])\nprint(result[1])","c3c0a99f":"# (train_data, test_data), info = tfds.load('imdb_reviews\/subwords8k', \n#                                           split = (tfds.Split.TRAIN, tfds.Split.TEST),\n#                                           with_info=True, \n#                                           as_supervised=True)\n\n(train_data, test_data), info = tfds.load('imdb_reviews\/subwords32k', \n                                          split = (tfds.Split.TRAIN, tfds.Split.TEST),\n                                          with_info=True, \n                                          as_supervised=True)","9207bd8d":"encoder = info.features['text'].encoder\nencoder.subwords[50:70]","40417271":"train_batches = train_data.shuffle(1000).padded_batch(10)\ntest_batches = test_data.shuffle(1000).padded_batch(10)","bfc32f6a":"train_batch, train_labels = next(iter(train_batches))\nprint('Training data info')\nprint('Train data shape: ', train_batch.shape)\nprint('Train_data: ', train_batch.numpy())\n\n\nprint('\\n\\nTrain label info')\nprint('Train label shape: ', train_labels.shape)\nprint('Train label: ', train_labels.numpy())\n","1b51f3d2":"from IPython.display import YouTubeVideo\nYouTubeVideo('hjx-zwVdfjc', width=800, height=450)\n","69807c49":"YouTubeVideo('upto_vdrXFI', width=800, height=450)","1c90e77b":"embedding_dim = 32\n\nmodel = keras.Sequential([\n  layers.Embedding(encoder.vocab_size, embedding_dim),\n  layers.GlobalAveragePooling1D(),\n#   layers.Dense(16, activation='relu'),\n  layers.Dense(1)\n])\n\nmodel.summary()","25760592":"model.compile(optimizer='adam',\n              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              metrics=['accuracy'])","3971238a":"history = model.fit(train_batches,\n                    epochs=5,\n                    validation_data=test_batches, \n                    validation_steps=250)","ab351f05":"def visualize_training(history, lw = 3):\n    import matplotlib.pyplot as plt \n    plt.figure(figsize=(10,10))\n    plt.subplot(2,1,1)\n    plt.plot(history.history['accuracy'], label = 'training', marker = '*', linewidth = lw)\n    plt.plot(history.history['val_accuracy'], label = 'validation', marker = 'o', linewidth = lw)\n    plt.title('Accuracy Comparison')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.grid(True)\n    plt.legend(fontsize = 'x-large')\n    \n\n    plt.subplot(2,1,2)\n    plt.plot(history.history['loss'], label = 'training', marker = '*', linewidth = lw)\n    plt.plot(history.history['val_loss'], label = 'validation', marker = 'o', linewidth = lw)\n    plt.title('Loss Comparison')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend(fontsize = 'x-large')\n    plt.grid(True)\n    plt.show()\n\n#     plt.figure(figsize=(10,5))\n#     plt.plot(history.history['lr'], label = 'lr', marker = '*',linewidth = lw)\n#     plt.title('Learning Rate')\n#     plt.xlabel('Epochs')\n#     plt.ylabel('Learning Rate')\n#     plt.grid(True)\n#     plt.show()","11962abf":"visualize_training(history)","5255b3f7":"# Retrieving the layer\ne = model.layers[0]\n#Retrieving the weights learned in that layer. \nweights = e.get_weights()[0]\n# The weight matrix is basically a list type matrix. Let's convert them into a numpy array for easier visualization.\nprint(weights.shape)","a03235b3":"import io\n\nencoder = info.features['text'].encoder\n\nout_v = io.open('vecs.tsv', 'w', encoding='utf-8')\nout_m = io.open('meta.tsv', 'w', encoding='utf-8')\n\nfor num, word in enumerate(encoder.subwords):\n    vec = weights[num+1] # skip 0, it's padding.\n    out_m.write(word + \"\\n\")\n    out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\nout_v.close()\nout_m.close()","47df22dc":"YouTubeVideo('hjx-zwVdfjc', width=800, height=450)","1a980202":"\ndef get_location(word, vocab):\n    location = None\n    for index, s in enumerate(vocab):\n        if(s == word):\n            location = index\n            break\n    return location\nvocab = encoder.subwords\n\nman = weights[get_location(\"man\", vocab)]\nwoman = weights[get_location(\"woman\", vocab)]\nboy = weights[get_location(\"boy\", vocab)]\ngirl = weights[get_location(\"girl\", vocab)]","7a0306de":"what = boy - man + woman","f2440d95":"from sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np\n\nprint('Similarity between   man & what  : ', np.squeeze(cosine_similarity([what], [man])))\nprint('Similarity between   woman & what: ', np.squeeze(cosine_similarity([what], [woman])))\nprint('Similarity between   boy & what  : ', np.squeeze(cosine_similarity([what], [boy])))\nprint('Similarity between   girl & what : ', np.squeeze(cosine_similarity([what], [girl])))","c27bfd2e":"Here we observe that, with the word **gird** our query has the highest similarity i.e. highest cosine value which eventually means the lowest angle between them in the vector space. ","e5b4af92":"## Visualize the embeddings\nAfter you have run the previous segment of code, your output folder should have two files named \n* vecs.tsv\n* meta.tsv\n\nTo visualize our embeddings we will upload them to the embedding projector.\n\nOpen the [Embedding Projector](http:\/\/projector.tensorflow.org\/) (this can also run in a local TensorBoard instance).\n\n* Click on \"Load data\".\n\n* Upload the two files we created above: `vecs.tsv` and `meta.tsv`.\n\nThe embeddings you have trained will now be displayed. You can search for words to find their closest neighbors. For example, try searching for **\"beautiful\"**. You may see neighbors like **\"wonderful\"**. \n\nNote: your results may be a bit different, depending on how weights were randomly initialized before training the embedding layer.\n\nNote: experimentally, you may be able to produce more interpretable embeddings by using a simpler model. Try deleting the `Dense(16)` layer, retraining the model, and visualizing the embeddings again.\n\n\nNow in order to find or have a look at any words in the embedding, we just have to find the location of that word in the vocabulary and select that row from the embedding matrix. This mechanism is very well explained in the tutorial of Andrew Ng","3e95fa14":"As imported, the text of reviews is integer-encoded (each integer represents a specific word or word-part in the vocabulary).\nNote the trailing zeros, because the batch is padded to the longest example. Now lets have a look at the dataset summary. ","84d64a6d":"Get the encoder [(tfds.features.text.SubwordTextEncoder)](https:\/\/www.tensorflow.org\/datasets\/api_docs\/python\/tfds\/features\/text\/SubwordTextEncoder), and have a quick look at the vocabulary.\nThe \"_\" in the vocabulary represent spaces. Note how the vocabulary includes whole words (ending with \"_\") and partial words which it can use to build larger words:","2cf200be":"### Retrieve the learned embeddings\nNext, let's retrieve the word embeddings learned during training. This will be a matrix of shape `(vocab_size, embedding-dimension)`. Here observe some things carefully. \n* We are selecting only the first layer of the model because it is the embedding layer in our model descriptor. If you carefully look at the model description, you can get an idea of what I am talking about. \n\n``` Python \n#Model description\nembedding_dim=300\nmodel = keras.Sequential([\n  layers.Embedding(encoder.vocab_size, embedding_dim),\n  layers.GlobalAveragePooling1D(),\n#   layers.Dense(16, activation='relu'),\n  layers.Dense(1)\n])\n```\n\n* When we run the summary() command for the model, we saw that `Embedding()` layer had **1044800   ** parameters. Which is exactly equal to **32,650x32 = 1044800.** Therefore it was basically telling the size of the embedding matrix that it was going to learn. However for the quick testing purpose, I would set `embedding_dim = 32 or 16`. \n\n* In the summary of the model, we observe that `global_average_pooling1d` layer which is `GlobalAveragePooling1D()` has **0** parameters to learn because it was simply finding out the running average of the previous layers output. \n\n* Similarly it is possible to explore the learned parameters of the last two layers. Feel free to explore them for better understanding. ","87fe055f":"Now for comparative analysis, lets calculate the similarity of what with each of them ","6dc13d4b":"### Create a simple model\n\nI will use the [Keras Sequential API](https:\/\/www.tensorflow.org\/guide\/keras\/sequential_model) to define our model. In this case it is a [\"Continuous bag of words (CBOW)\"](https:\/\/towardsdatascience.com\/nlp-101-word2vec-skip-gram-and-cbow-93512ee24314) style model. **In the CBOW model, the distributed representations of context (or surrounding words) are combined to predict the word in the middle. The exact opposite model is the Skip-Gram model,  that is based on a single target word, the sorrounding words (contexts) are predicted** The following figure will clarify them for sure. ![](https:\/\/miro.medium.com\/max\/875\/1*cuOmGT7NevP9oJFJfVpRKA.png)\n\n* Next the Embedding layer takes the integer-encoded vocabulary and looks up the embedding vector for each word-index. These vectors are learned as the model trains. The vectors add a dimension to the output array. The resulting dimensions are: `(batch, sequence, embedding)`.\n\n* Next, a GlobalAveragePooling1D layer returns a fixed-length output vector for each example by averaging over the sequence dimension. This allows the model to handle input of variable length, in the simplest way possible.\n\n* This fixed-length output vector is piped through a fully-connected (Dense) layer with 16 hidden units.\n\n* The last layer is densely connected with a single output node. Using the sigmoid activation function, this value is a float between 0 and 1, representing a probability (or confidence level) that the review is positive.\n\nCaution: This model doesn't use masking, so the zero-padding is used as part of the input, so the padding length may affect the output.  To fix this, see the [masking and padding guide](..\/..\/guide\/keras\/masking_and_padding).","f21e4cad":"[Large Movie Review Dataset.](https:\/\/ai.stanford.edu\/~amaas\/data\/sentiment\/) This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. It contains a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional 50000 unlabeled data for use as well.\n\nOne of the common tricks is to use the total 100,000 samples to train a language model which essentially predicts the next word of a sequence given a series of words before that and later on using that model for classification. However, in this notebook, I am simply using them for demonstrating word embeddings. So, I will just work on a small subset of the whole dataset containing vocabulary size of 8,000 words. You are free to explore the other larger version of the dataset from the tensorflow dataframes on [imdb reviews](https:\/\/www.tensorflow.org\/datasets\/catalog\/imdb_reviews) ready to use. \n\n\n","21dcb5f4":"Let's find the location of the words man, woman, king, queen from the vocabulary and then retrieve their word embedding. ","c591c780":"![](https:\/\/ruder.io\/content\/images\/size\/w2000\/2016\/04\/word_embeddings_colah.png)\n# What Are Word Embeddings?\nA word embedding is a learned representation for text where words that have the same meaning have a similar representation. It is this approach to representing words and documents that may be considered one of the key breakthroughs of deep learning on challenging natural language processing problems. Word embeddings are in fact a class of techniques where individual words are represented as real-valued vectors in a predefined vector space. Each word is mapped to one vector and the vector values are learned in a way that resembles a neural network, and hence the technique is often lumped into the field of deep learning.\n\n\n## Representing text as numbers\nMachine learning models take vectors (arrays of numbers) as input. When working with text, the first thing we must do come up with a strategy to convert strings to numbers (or to \"vectorize\" the text) before feeding it to the model. In this section, we will look at three strategies for doing so.\n\n### One-hot encodings\nAs a first idea, we might \"one-hot\" encode each word in our vocabulary. Consider the sentence \"The cat sat on the mat\". The vocabulary (or unique words) in this sentence is (cat, mat, on, sat, the). To represent each word, we will create a zero vector with length equal to the vocabulary, then place a one in the index that corresponds to the word. This approach is shown in the following diagram.\n![](https:\/\/github.com\/tensorflow\/docs\/blob\/master\/site\/en\/tutorials\/text\/images\/one-hot.png?raw=1) To create a vector that contains the encoding of the sentence, we could then concatenate the one-hot vectors for each word. However, This approach is inefficient. A one-hot encoded vector is sparse (meaning, most indices are zero). Imagine we have 10,000 words in the vocabulary. To one-hot encode each word, we would create a vector where 99.99% of the elements are zero.\n\n\n### Encode each word with a unique number\nA second approach we might try is to encode each word using a unique number. Continuing the example above, we could assign 1 to \"cat\", 2 to \"mat\", and so on. We could then encode the sentence \"The cat sat on the mat\" as a dense vector like $[5, 1, 4, 3, 5, 2]$. This appoach is efficient. Instead of a sparse vector, we now have a dense one (where all elements are full).\n\nThere are two downsides to this approach, however:\n> The integer-encoding is arbitrary (it does not capture any relationship between words).\n> An integer-encoding can be challenging for a model to interpret. A linear classifier, for example, learns a single weight for each feature. Because there is no relationship between the similarity of any two words and the similarity of their encodings, this feature-weight combination is not meaningful.\n\n### Word embeddings\nWord embeddings give us a way to use an efficient, dense representation in which similar words have a similar encoding. Importantly, we do not have to specify this encoding by hand. An embedding is a dense vector of floating point values (the length of the vector is a parameter you specify). Instead of specifying the values for the embedding manually, they are trainable parameters (weights learned by the model during training, in the same way a model learns weights for a dense layer). It is common to see word embeddings that are 8-dimensional (for small datasets), up to 1024-dimensions when working with large datasets. A higher dimensional embedding can capture fine-grained relationships between words, but takes more data to learn.\n\nDiagram of an embedding\n![](https:\/\/github.com\/tensorflow\/docs\/blob\/master\/site\/en\/tutorials\/text\/images\/embedding2.png?raw=1)\nAbove is a diagram for a word embedding. Each word is represented as a 4-dimensional vector of floating point values. Another way to think of an embedding is as \"lookup table\". After these weights have been learned, we can encode each word by looking up the dense vector it corresponds to in the table.\n","2322c464":"We will now write the weights to disk. To use the Embedding Projector, we will upload two files in tab separated format: a file of vectors (containing the embedding), and a file of meta data (containing the words).","77fc8d62":"For text or sequence problems, the Embedding layer takes a 2D tensor of integers, of shape (samples, sequence_length), where each entry is a sequence of integers. It can embed sequences of variable lengths. You could feed into the embedding layer above batches with shapes $(32, 10)$ (batch of 32 sequences of length 10) or $(64, 15)$ (batch of 64 sequences of length 15).\n\nThe returned tensor has one more axis than the input, the embedding vectors are aligned along the new last axis. Pass it a $(2, 3)$ input batch and the output is $(2, 3, N)$","97e2427a":"The Embedding layer can be understood as a lookup table that maps from integer indices (which stand for specific words) to dense vectors (their embeddings). The dimensionality (or width) of the embedding is a parameter you can experiment with to see what works well for your problem, much in the same way you would experiment with the number of neurons in a Dense layer.","649c0678":"Before we jump into the practical implementation of word embeddings, we can spare a few moments to have a look at one of the best videos on the internet on learning embeddings by professor [Andrew Ng](https:\/\/en.wikipedia.org\/wiki\/Andrew_Ng). He talks about some mathematical foundation of the embedding vector and explains underlying mechanisms to convert a one hot encoding to a embedding vector.","67ccc9cb":"When you create an Embedding layer, the weights for the embedding are randomly initialized (just like any other layer). During training, they are gradually adjusted via backpropagation. Once trained, the learned word embeddings will roughly encode similarities between words (as they were learned for the specific problem your model is trained on).\n\nIf you pass an integer to an embedding layer, the result replaces each integer with the vector from the embedding table:","728711b4":"Movie reviews can be different lengths. We will use the **padded_batch** method to standardize the lengths of the reviews.","5593e63d":"### Compile and train the model","38eccd89":"When given a batch of sequences as input, an embedding layer returns a 3D floating point tensor, of shape **(samples, sequence_length, embedding_dimensionality).** To convert from this sequence of variable length to a fixed representation there are a variety of standard approaches. You could use an **RNN, Attention, or pooling layer** before passing it to a Dense layer. This tutorial uses pooling because it's simplest.\n\n## Learning embeddings from scratch"}}