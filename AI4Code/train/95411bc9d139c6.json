{"cell_type":{"6b67aeac":"code","644fc3d9":"code","32183a3a":"code","b1d55337":"code","d672a7e0":"code","f499f1f7":"code","0bdabb8f":"code","baa6d7b7":"code","f930591c":"code","d62b34b3":"code","1c911857":"code","63fcecd5":"code","b234396e":"code","9087cb6b":"code","dc9ef87f":"code","72b4d8e0":"code","30464339":"code","2b35f0b9":"code","0ae7b134":"code","9ba3f94b":"code","f27312d7":"code","9bf23ef2":"code","f6b49019":"code","ad51fc42":"code","068d3dc4":"code","fa59912b":"code","252a60da":"code","dfd7545f":"code","6f19fde7":"code","e65a72a6":"code","0088f22d":"code","948bf8ce":"code","0da7973a":"code","66cd7a03":"markdown","2344ca1c":"markdown","f738a903":"markdown","0ed082e0":"markdown","08ecc888":"markdown","726e7545":"markdown"},"source":{"6b67aeac":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom kerastuner.tuners import RandomSearch\nfrom tensorflow.keras.models import Sequential,Model\nfrom tensorflow.keras.layers import Conv2D\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.layers import MaxPool2D\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport matplotlib.pyplot as plt","644fc3d9":"#!pip install tensorflow","32183a3a":"!pip install -U keras-tuner","b1d55337":"print(tf.__version__)\nif tf.test.gpu_device_name():\n    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\nelse:\n    print(\"Please install GPU version of TF\")","d672a7e0":"train_datagenerator = ImageDataGenerator(rescale = 1.0\/255,\n                                        shear_range = 0.2,\n                                        zoom_range = 0.5,\n                                        horizontal_flip = True,\n                                        rotation_range=10,\n                                        width_shift_range=0.2,\n                                        brightness_range=[0.2,1.2]\n                                        )\ntest_datagenerator = ImageDataGenerator(rescale = 1.0\/255)","f499f1f7":"train_data = train_datagenerator.flow_from_directory('..\/input\/cotton-disease-dataset\/Cotton Disease\/train',\n                                                    target_size = (227,227),\n                                                    batch_size = 64,\n                                                    class_mode = 'categorical')","0bdabb8f":"test_data = test_datagenerator.flow_from_directory('..\/input\/cotton-disease-dataset\/Cotton Disease\/val',\n                                                  target_size = (227,227),\n                                                    batch_size = 64,\n                                                    class_mode = 'categorical')","baa6d7b7":"\"\"\"\nCreating CNN model from Scratch.\n\"\"\"\ncnn = tf.keras.models.Sequential()\n# Convolution \ncnn.add(tf.keras.layers.Conv2D(filters=64,padding = \"same\",kernel_size=3,activation='relu',input_shape=[227,227,3]))\ncnn.add(tf.keras.layers.Conv2D(filters=32,padding = \"same\",kernel_size=3,activation='relu'))\n# pooling\ncnn.add(tf.keras.layers.MaxPool2D(pool_size=2,strides=2))\n# Convolution\ncnn.add(tf.keras.layers.Conv2D(filters=16,padding = \"same\",kernel_size=3,activation='relu'))\ncnn.add(tf.keras.layers.Conv2D(filters=16,padding = \"same\",kernel_size=3,activation='relu'))\n#pooling\ncnn.add(tf.keras.layers.MaxPool2D(pool_size=2,strides=2))\n\n#flaterning\ncnn.add(tf.keras.layers.Flatten())\ncnn.add(tf.keras.layers.Dense(units=128,activation='relu'))\ncnn.add(tf.keras.layers.Dense(units=128,activation='relu'))\n# Output layer\ncnn.add(tf.keras.layers.Dense(units=4,activation='softmax'))","f930591c":"cnn.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])","d62b34b3":"history = cnn.fit(x = train_data, validation_data = test_data, epochs = 50)","1c911857":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","63fcecd5":"from tensorflow.keras.applications.vgg16 import VGG16\nfrom tensorflow.keras.applications.vgg16 import preprocess_input\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.layers import Dense\nfrom glob import glob","b234396e":"Image_size = [224,224]\n\ntrain_path = '..\/input\/cotton-disease-dataset\/Cotton Disease\/train'\ntest_path = '..\/input\/cotton-disease-dataset\/Cotton Disease\/test'","9087cb6b":"train_datavg = train_datagenerator.flow_from_directory('..\/input\/cotton-disease-dataset\/Cotton Disease\/train',\n                                                    target_size = (224,224),\n                                                    batch_size = 64,\n                                                    class_mode = 'categorical')\ntest_datavg = test_datagenerator.flow_from_directory('..\/input\/cotton-disease-dataset\/Cotton Disease\/val',\n                                                  target_size = (224,224),\n                                                    batch_size = 64,\n                                                    class_mode = 'categorical')","dc9ef87f":"vgg16 = VGG16(input_shape = Image_size + [3], weights = 'imagenet', include_top = False)","72b4d8e0":"vgg16.summary()","30464339":"# freez the pretrained structure\nfor layer in vgg16.layers:\n    layer.trainable = False","2b35f0b9":"number_of_classes = glob('..\/input\/cotton-disease-dataset\/Cotton Disease\/train\/*')\nnumber_of_classes","0ae7b134":"flatten_layer = Flatten()(vgg16.output)","9ba3f94b":"output_layer = Dense(len(number_of_classes),activation = 'softmax')(flatten_layer)","f27312d7":"vgg16_model = Model(inputs = vgg16.input,outputs = output_layer)","9bf23ef2":"vgg16_model.summary()","f6b49019":"vgg16_model.compile(loss= 'categorical_crossentropy',\n                   optimizer = 'adam',\n                   metrics = ['accuracy'])","ad51fc42":"history = vgg16_model.fit_generator(train_datavg,validation_data= test_datavg,epochs = 49,\n                                   steps_per_epoch=len(train_datavg),validation_steps=len(test_datavg))","068d3dc4":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","fa59912b":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout, Flatten,\\\n Conv2D, MaxPooling2D\nfrom keras.layers.normalization import BatchNormalization\nimport numpy as np","252a60da":"train_dataAlx = train_datagenerator.flow_from_directory('..\/input\/cotton-disease-dataset\/Cotton Disease\/train',\n                                                    target_size = (227,227),\n                                                    batch_size = 64,\n                                                    class_mode = 'categorical')","dfd7545f":"test_dataAlx = test_datagenerator.flow_from_directory('..\/input\/cotton-disease-dataset\/Cotton Disease\/val',\n                                                    target_size = (227,227),\n                                                    batch_size = 64,\n                                                    class_mode = 'categorical')","6f19fde7":"\"\"\"\nBuilding AlexNet Model.\n\"\"\"\nmodel = Sequential()\n\n# Block 1\nmodel.add(Conv2D(filters=96, input_shape=(227,227,3), kernel_size=(11,11),strides=(4,4), padding='valid'))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\nmodel.add(BatchNormalization())\n\n# Block 2\nmodel.add(Conv2D(filters=256, kernel_size=(11,11), strides=(1,1), padding='valid'))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\nmodel.add(BatchNormalization())\n\n# Block 3\nmodel.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))\nmodel.add(Activation('relu'))\nmodel.add(BatchNormalization())\n\n# Block 4\nmodel.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))\nmodel.add(Activation('relu'))\nmodel.add(BatchNormalization())\n\n# Block 5\nmodel.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='valid'))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\nmodel.add(BatchNormalization())\n\n# Flattening\nmodel.add(Flatten())\n\n# Dense Layer block 1\nmodel.add(Dense(4096, input_shape=(227*227*3,)))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.4)) # Add Dropout to prevent overfitting\nmodel.add(BatchNormalization())\n\n# Dense Layer block 2\nmodel.add(Dense(4096))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.4))\nmodel.add(BatchNormalization())\n\n# Dense Layer block 3\nmodel.add(Dense(1000))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.4))\nmodel.add(BatchNormalization())\n\n# Additional dense\n#model.add(Dense(17))\n#model.add(Activation('relu'))\n\n# Output Layer\nmodel.add(Dense(4))\nmodel.add(Activation('softmax'))","e65a72a6":"model.summary()","0088f22d":"model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n\nhistory = model.fit_generator(train_dataAlx,validation_data=test_dataAlx,epochs = 50,\n                                   steps_per_epoch=len(train_dataAlx),validation_steps=len(test_dataAlx))","948bf8ce":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","0da7973a":"import numpy as np\nfrom tensorflow.keras.preprocessing import image\npredictions = [\"diseased cotton leaf\",\"diseased cotton plant\",\"fresh cotton leaf\",\"fresh cotton plant\"]\ntest_image = image.load_img('..\/input\/cotton-disease-dataset\/Cotton Disease\/val\/fresh cotton leaf\/d (26)_iaip.jpg', target_size = (224,224))\ntest_image = image.img_to_array(test_image)\ntest_image=test_image\/255\ntest_image = np.expand_dims(test_image, axis = 0)\nresult = model.predict(test_image)\npredictions[result.argmax()]","66cd7a03":"I have used the three classic Convolution neural network models:\n\n1) CNN from Stratch ( with accuracy 84.19%)\n\n2) VGG16 ( with accuracy 95.28%)\n\n3) AlexNet ( accuracy 72.33%)\n\nThough these accuracies can be modified by tweaking up the architecture.\n\n### More modifications to come:\n\nAdding ResNet & InceptionNet aswell.","2344ca1c":"## AlexNet","f738a903":"  ## VGG16","0ed082e0":"* ## Please upvote if you like the notebook and comment your suggestion which models should be added.","08ecc888":"## CNN Implementation","726e7545":"## Generating Data"}}