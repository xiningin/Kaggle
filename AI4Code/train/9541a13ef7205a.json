{"cell_type":{"11bb037a":"code","dc19aa83":"code","74bc2406":"code","410986eb":"code","2bd0510b":"code","3a2341e5":"code","b7b50437":"code","b8884ea0":"code","53e3dadc":"code","b0e8ff12":"code","213056a3":"code","1a4ce68b":"code","e8a07c67":"code","b98711f6":"code","10b18a61":"code","490a6faa":"code","86df5840":"markdown","af5e9f9e":"markdown","b0606689":"markdown","22d8d629":"markdown","f3ba94a5":"markdown","df3b10b9":"markdown"},"source":{"11bb037a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","dc19aa83":"df_train = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ndf_train.head()","74bc2406":"df_test = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\ndf_test.head()","410986eb":"df_train.info()\nprint('_'*40)\ndf_test.info()","2bd0510b":"#Save the 'Id' column\ntrain_ID = df_train['id']\ntest_ID = df_test['id']\n\n#Now drop the  'Id' colum since it's unnecessary for  the prediction process.\ndf_train.drop(\"id\", axis = 1, inplace = True)\ndf_test.drop(\"id\", axis = 1, inplace = True)","3a2341e5":"# Check for NaN values:\nprint(df_train.isnull().sum())\ndf_test.isnull().sum()\n","b7b50437":"df_train['keyword'] = df_train['keyword'].fillna('None')\ndf_test['keyword'] = df_test['keyword'].fillna('None')","b8884ea0":"df_train['location'] = df_train['location'].fillna('None')\ndf_test['location'] = df_test['location'].fillna('None')","53e3dadc":"df_train[\"text\"]= df_train[\"keyword\"] + \" \" + df_train[\"location\"] + \" \"+df_train[\"text\"]\ndf_test[\"text\"]= df_test[\"keyword\"] + \" \" + df_test[\"location\"] + \" \"+df_test[\"text\"]\n\ndf_train=df_train.drop(\"keyword\",axis=1)\ndf_train=df_train.drop(\"location\",axis=1)\n\ndf_test=df_test.drop(\"keyword\",axis=1)\ndf_test=df_test.drop(\"location\",axis=1)","b0e8ff12":"#Remove redundant samples\ndf_train=df_train.drop_duplicates(subset=['text', 'target'], keep='first')","213056a3":"import re\ndef clean(sen):\n    sentence = re.sub(\"http[s]*:\/\/[^\\s]+\",\" \",sen)\n    # Remove punctuations and numbers\n    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n\n    # Single character removal\n    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n    \n    # Removing multiple spaces\n    sentence = re.sub(r'\\s+', ' ', sentence)\n    \n    sentence = str(sentence).lower()\n    sentence = sentence.replace(\",000,000\", \"m\").replace(\",000\", \"k\").replace(\"\u2032\", \"'\").replace(\"\u2019\", \"'\")\\\n                           .replace(\"won't\", \"will not\").replace(\"cannot\", \"can not\").replace(\"can't\", \"can not\")\\\n                           .replace(\"n't\", \" not\").replace(\"what's\", \"what is\").replace(\"it's\", \"it is\")\\\n                           .replace(\"'ve\", \" have\").replace(\"i'm\", \"i am\").replace(\"'re\", \" are\")\\\n                           .replace(\"he's\", \"he is\").replace(\"she's\", \"she is\").replace(\"'s\", \" own\")\\\n                           .replace(\"%\", \" percent \").replace(\"\u20b9\", \" rupee \").replace(\"$\", \" dollar \")\\\n                           .replace(\"\u20ac\", \" euro \").replace(\"'ll\", \" will\")\n    sentence = re.sub(r\"([0-9]+)000000\", r\"\\1m\", sentence)\n    sentence = re.sub(r\"([0-9]+)000\", r\"\\1k\", sentence)\n    sentence = re.sub(r\"\\w+:\\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\\/[^\\s\\\/]*))*\", \"\", sentence)\n    sentence = sentence.replace(\"_\", \" \")\n    \n    return sentence\n\nfrom nltk.stem.porter import PorterStemmer\nimport string\nfrom nltk.tokenize import word_tokenize\n\npstem = PorterStemmer()\ndef clean_text(text):\n    text= text.lower()\n    text= re.sub('[0-9]', '', text)\n    text  = \"\".join([char for char in text if char not in string.punctuation])\n    tokens = word_tokenize(text)\n    tokens=[pstem.stem(word) for word in tokens]\n    #tokens=[word for word in tokens if word not in stopwords.words('english')]\n    text = ' '.join(tokens)\n    return text","1a4ce68b":"df_train['text'] = df_train['text'].apply(lambda s : clean(s))\ndf_test['text'] = df_test['text'].apply(lambda s : clean(s))\ndf_train['text'] = df_train['text'].apply(lambda s : clean_text(s))\ndf_test['text'] = df_test['text'].apply(lambda s : clean_text(s))\n","e8a07c67":"X_train = df_train.loc[:,df_train.columns != 'target']  # this time we want to look at the text\ny_train = df_train['target']","b98711f6":"from sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.svm import LinearSVC\n\nfrom xgboost import XGBClassifier\n\npreprocessor = ColumnTransformer(\n     transformers=[\n         ('text', TfidfVectorizer(), 'text'),]\n    ,)\n\ntext_clf = Pipeline([('preprocessor', preprocessor),\n                     ('clf', LinearSVC(loss='hinge'),),\n])\n\n# Feed the training data through the pipeline\ntext_clf.fit(X_train, y_train) ","10b18a61":"predictions = text_clf.predict(df_test)","490a6faa":"output = pd.DataFrame({'id': test_ID,\n                       'target': predictions})\n\noutput.to_csv('submission.csv', index=False)","86df5840":"## Preprocessing","af5e9f9e":"## Data Cleaning","b0606689":"### Location\n","22d8d629":"# Simple Text Classification (TFIDF + LinearSVC)\n\nIn this notebook, a simple text classification algorithm is developed for the Classification of Disaster Tweets. \n\nSteps:\n- Read Data.\n- Data Preprocessing\n- Modelling with TF-IDF and LinearSVC","f3ba94a5":"## Missing Values\n\n### Keyword\n\nReplace NaN values by None","df3b10b9":"## Modelling\n"}}