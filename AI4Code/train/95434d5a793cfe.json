{"cell_type":{"7bc770cc":"code","e19f1500":"code","a554bd47":"code","506a1be0":"code","5cb43925":"code","af5fb4a2":"code","83a0a735":"code","1638a0b5":"code","16725aae":"markdown","1c40349c":"markdown","26516ca2":"markdown"},"source":{"7bc770cc":"! pip install --quiet datasets \n! pip install fsspec==0.9.\n! pip install wandb --upgrade","e19f1500":"import os\nimport numpy  as np\nimport pandas as pd\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\nfrom datasets import load_dataset, load_metric","a554bd47":"os.environ[\"WANDB_API_KEY\"] = \"a6d0e2833cb35ed2224f9808f275a82596530dfe\"","506a1be0":"raw_datasets = load_dataset(\"imdb\")\nraw_datasets","5cb43925":"raw_datasets['train']","af5fb4a2":"tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\ndef tokenize_function(examples):\n    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n\ntokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\ntokenized_datasets","83a0a735":"small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\nsmall_eval_dataset  = tokenized_datasets[\"test\" ].shuffle(seed=42).select(range(1000))\nfull_train_dataset  = tokenized_datasets[\"train\"]\nfull_eval_dataset   = tokenized_datasets[\"test\" ]","1638a0b5":"metric = load_metric(\"accuracy\")\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    return metric.compute(predictions=predictions, references=labels)\n\nmodel         = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)\n# training_args = TrainingArguments(\"test_trainer\")\ntraining_args = TrainingArguments(\"test_trainer\", evaluation_strategy=\"epoch\")\ntrainer       = Trainer(\n    model         = model, \n    args          = training_args, \n    train_dataset = small_train_dataset, \n    eval_dataset  = small_eval_dataset,\n    compute_metrics = compute_metrics,\n)\ntrainer.train()\ntrainer.evaluate()","16725aae":"# Preparing the datasets","1c40349c":"# Fine-tuning in PyTorch with the Trainer API","26516ca2":"# Huggingface - Fine-Tuning A Pretrained BERT Model\n\nThis notebook follows the Huggingface tutorial for fine-tuning a pretrained BERT model on the IMDB dataset\n- https:\/\/huggingface.co\/transformers\/training.html"}}