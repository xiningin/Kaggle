{"cell_type":{"cb8516c5":"code","c822e271":"code","f7d87e32":"code","ab380a88":"code","318d8b4a":"code","f77f4a00":"code","400d8f04":"code","7e64e4ff":"code","563c50b1":"code","abbe2051":"code","2f4c80d3":"code","4d73d595":"code","cb1129be":"code","0254a375":"code","f5906413":"code","15c741f9":"code","caa15edd":"code","8a0b205c":"markdown","e6c3b7b6":"markdown","9d66a31e":"markdown","6e671ae2":"markdown"},"source":{"cb8516c5":"from IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\n\n# torch\nimport torch\nimport torchaudio\nimport torchaudio.functional as F\nimport torchaudio.transforms as T\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport tensorflow_datasets as tfds\nfrom tensorflow.keras.layers.experimental import preprocessing\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D\nfrom tensorflow.keras import optimizers\n\nfrom tqdm.auto import trange, tqdm\n\nimport time\nimport threading\nimport librosa\nimport librosa.display\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport gc\n\n# Load the TensorBoard notebook extension\n%load_ext tensorboard\nMFCC_NUM = 20\nMFCC_MAX_LEN = 2000\nINSTRUMENT_TYPE = 11\nSOURCE_TYPE = 3\nPITCH_RANGE = 128 # 0-127\nVELOCITY_RANGE = 128 # 0-127\ntorchaudio.set_audio_backend(\"sox_io\")","c822e271":"class Timer:\n    def __init__(self):\n        self.s = None\n        self.e = None\n\n    def start(self):\n        self.s = time.time()\n\n    def end(self):\n        self.e = time.time()\n        total = int(self.e - self.s)\n        seconds = total % (24 * 3600)\n        hour = seconds \/\/ 3600\n        seconds %= 3600\n        minutes = seconds \/\/ 60\n        seconds %= 60\n        return f'{hour}h {minutes}min(s) {seconds}sec(s)'\ntimer = Timer()","f7d87e32":"# load test dataset\ntest_records = tf.data.TFRecordDataset('..\/input\/nsynth-music-dataset\/nsynth-test.tfrecord').prefetch(tf.data.experimental.AUTOTUNE)","ab380a88":"print('============= Test Dataset Size =============')\ntimer.start()\ntest_size = sum(1 for _ in test_records)\nprint(f'[Test Dataset]Count Finished => {test_size} items')\nprint('=============================================')\nprint(\"Garbage collector: collected\", \"%d objects.\" % gc.collect())","318d8b4a":"def _parse_function(dataset):\n    return tf.io.parse_single_example(dataset, {\n        'sample_rate': tf.io.FixedLenFeature([], tf.int64),\n        'qualities_str': tf.io.FixedLenSequenceFeature([], tf.string,allow_missing=True, default_value=''),\n        'note_str': tf.io.FixedLenFeature([], tf.string),\n        'qualities': tf.io.FixedLenSequenceFeature([], tf.int64, allow_missing=True, default_value=0),\n        'audio': tf.io.FixedLenSequenceFeature([], tf.float32, allow_missing=True, default_value=0.0),\n        'instrument_family': tf.io.FixedLenFeature([], tf.int64),\n        'pitch': tf.io.FixedLenFeature([], tf.int64),\n        'instrument_source': tf.io.FixedLenFeature([], tf.int64),\n        'instrument_str': tf.io.FixedLenFeature([], tf.string),\n        'instrument_source_str': tf.io.FixedLenFeature([], tf.string),\n        'note': tf.io.FixedLenFeature([], tf.int64),\n        'instrument': tf.io.FixedLenFeature([], tf.int64),\n        'instrument_family_str': tf.io.FixedLenFeature([], tf.string),\n        'velocity': tf.io.FixedLenFeature([], tf.int64),\n    })\n\nclass SampleRecord:\n    def __init__(self, audio, sr, instrument, source, pitch, velocity):\n        self.audio = audio\n        self.sr = sr\n        self.instrument = instrument\n        self.pitch = pitch\n        self.velocity = velocity\n        self.source = source\n\nclass SampleFeature:\n    def __init__(self, audio, mfcc, spectrogram, chroma):\n        self.audio = audio\n        self.mfcc = mfcc\n        self.spectrogram = spectrogram\n        self.chroma = chroma","f77f4a00":"@tf.autograph.experimental.do_not_convert\ndef parseSet(dataset:tf.data.Dataset, n:int = 50):\n    @tf.autograph.experimental.do_not_convert\n    def getDataset() -> (dict, int):\n        \"\"\"\n        Prepare the dataset\n        :return a dictionary containing the dataset, the size for each instrument and source type and the total samples size\n        \"\"\"\n        size = []\n        instruments = []\n        with tqdm(total=n) as pBar:\n            for _i in range(11):\n                count = 0\n                sources = []\n                subdataset = dataset.filter(lambda x: x['instrument_family'] == _i)\n                for r in subdataset:\n                    sources.append(SampleRecord(audio=r['audio'].numpy(), sr=r['sample_rate'].numpy(), instrument=r['instrument_family'].numpy(), pitch=r['pitch'].numpy(), velocity=r['velocity'].numpy(), source=r['instrument_source'].numpy()))\n                    count += 1\n                    pBar.update()\n                size.append(count)\n                instruments.append(sources)\n        print(f\"Successful prepare the dataset! | Require {n*3*11} | Actual {np.array(size).sum()}\")\n        return instruments, size\n    print(\"Loading the dataset...\")\n    parsedSet, _size = getDataset()\n    sample_size = np.array(_size).sum()\n    print(f\"============ Finish preparing the pitch data! | Total {sample_size} samples ============\")\n    return parsedSet, _size","400d8f04":"print('============= Parse Test Dataset =============')\ntimer.start()\nparsed_test = parseSet(test_records.map(_parse_function, num_parallel_calls=tf.data.experimental.AUTOTUNE), test_size)\nprint(\"Garbage collector: collected\", \"%d objects.\" % gc.collect())\nprint(f'[Test]Parse Finished! Cost {timer.end()}')\nprint('========================================')","7e64e4ff":"mel_spectrogram = T.MelSpectrogram(sample_rate=16000,n_fft=1024,win_length=None,hop_length=512,power=2.0,n_mels=128)\nmfcc_transform = T.MFCC(sample_rate=16000, n_mfcc=256, melkwargs={'n_fft': 2048, 'n_mels': 256, 'hop_length': 512})","563c50b1":"instrument_labels = ['Bass', 'Brass', 'Flute', 'Guitar','Keyboard', 'Mallet', 'Organ', 'Reed', 'String','Synth_lead', 'Vocal']\ndef extract_features(dataset_and_size):\n    def getFeatures(y, sr) -> SampleFeature:\n        \"\"\"\n\t\tGet the features with given audio and sample rate\n\t\t:param y: the audio\n\t\t:param sr: sample rate\n\t\t:return: features(mfcc, spectrogram, chroma, contrast, harmonic)\n\t\t\"\"\"\n        #Mel-frequency cepstral coefficients (MFCCs)\n        mfcc = mfcc_transform(torch.tensor(y)).tolist()\n\n        #get the mel-scaled spectrogram\n        spectrogram = mel_spectrogram(torch.tensor(y)).tolist()\n\n        #compute chroma energy\n        chroma = librosa.feature.chroma_cens(y=y, sr=sr)\n        chroma = np.mean(chroma, axis = 1)\n        return SampleFeature(y, mfcc, spectrogram, chroma)\n    _rawSet, _size = dataset_and_size\n    sample_size = np.array(_size).sum()\n    instrument_set = {\n        'mfcc': {\n            'Bass': [], \n            'Brass': [], \n            'Flute':[], \n            'Guitar':[], \n            'Keyboard':[], \n            'Mallet':[], \n            'Organ':[], \n            'Reed':[], \n            'String':[], \n            'Synth_lead':[], \n            'Vocal':[]\n        },\n        'spectrogram':{\n            'Bass': [], \n            'Brass': [], \n            'Flute':[], \n            'Guitar':[], \n            'Keyboard':[], \n            'Mallet':[], \n            'Organ':[], \n            'Reed':[], \n            'String':[], \n            'Synth_lead':[], \n            'Vocal':[]\n        },\n        'pitch':{\n            'Bass': [], \n            'Brass': [], \n            'Flute':[], \n            'Guitar':[], \n            'Keyboard':[], \n            'Mallet':[], \n            'Organ':[], \n            'Reed':[], \n            'String':[], \n            'Synth_lead':[], \n            'Vocal':[]\n        },\n        'chroma':{\n            'Bass': [], \n            'Brass': [], \n            'Flute':[], \n            'Guitar':[], \n            'Keyboard':[], \n            'Mallet':[], \n            'Organ':[], \n            'Reed':[], \n            'String':[], \n            'Synth_lead':[], \n            'Vocal':[]\n        },\n        'velocity':{\n            'Bass': [], \n            'Brass': [], \n            'Flute':[], \n            'Guitar':[], \n            'Keyboard':[], \n            'Mallet':[], \n            'Organ':[], \n            'Reed':[], \n            'String':[], \n            'Synth_lead':[], \n            'Vocal':[]\n        }\n    }\n    print(\"Extracting the features...\")\n    with tqdm(total=sample_size) as bar:\n        for record in _rawSet:\n            for eg in record:\n                feature = getFeatures(eg.audio, eg.sr)\n                instrument_set['mfcc'][instrument_labels[eg.instrument]].append(feature.mfcc)\n                instrument_set['spectrogram'][instrument_labels[eg.instrument]].append(feature.spectrogram)\n                instrument_set['chroma'][instrument_labels[eg.instrument]].append(feature.chroma)\n                instrument_set['pitch'][instrument_labels[eg.instrument]].append(eg.pitch)\n                instrument_set['velocity'][instrument_labels[eg.instrument]].append(eg.velocity)\n                bar.update()\n    return instrument_set","abbe2051":"timer.start()\nTEST = extract_features(parsed_test)\nprint(f'[TEST]Extract Finished, Cost {timer.end()}')\nprint(\"Garbage collector: collected\", \"%d objects.\" % gc.collect())","2f4c80d3":"performance_mfcc = pd.DataFrame(columns=['Model Instrument', 'Loss', 'Accuracy'])\nperformance_spectrogram = pd.DataFrame(columns=['Model Instrument', 'Loss', 'Accuracy'])\ngc.collect()","4d73d595":"for i in instrument_labels:\n    X_mfcc = np.array(TEST['mfcc'][i])\n    X_spectrogram = np.array(TEST['spectrogram'][i])\n    \n    mfcc_shape = X_mfcc.shape[1:]\n    mfcc_size = X_mfcc.shape[0]\n    spectrogram_size = X_spectrogram.shape[0]\n    spectrogram_shape = X_spectrogram.shape[1:]\n    \n    X_test_mfcc = X_mfcc.reshape(X_mfcc.shape[0], 256, 126, 1)\n    X_test_spectrogram = X_spectrogram.reshape(X_spectrogram.shape[0], 128, 126, 1)\n    Y_pitch = np.array(TEST['pitch'][i])\n    instrument = instrument_labels.index(i)\n    print(f'\\n================ Start {i} | {X_mfcc.shape[0]} test samples ================')\n    \n    if X_mfcc.shape[0] != 0:\n        \n        print(f'[{spectrogram_size}]Mel-Spectrogram, Shpe: {spectrogram_shape} | [{mfcc_size}]MFCC, Shpe: {mfcc_shape}')\n        mfcc_model = tf.keras.models.load_model(f\"..\/input\/bass-model\/{i.lower()}\/{i.lower()}_mfcc\")\n        spectrogram_model = tf.keras.models.load_model(f\"..\/input\/bass-model\/{i.lower()}\/{i.lower()}_spectrogram\")\n        test_loss_1, test_acc_1 = mfcc_model.evaluate(x=X_test_mfcc, y=np.eye(128, dtype=int)[Y_pitch].reshape((Y_pitch.shape[0], 128)))\n        performance_mfcc.loc[performance_mfcc.size] = {\n            'Model Instrument': f'{instrument_labels[instrument]}', \n            'Loss': test_loss_1, \n            'Accuracy': test_acc_1\n        }\n\n        test_loss_2, test_acc_2 = spectrogram_model.evaluate(\n            x=X_test_spectrogram, \n            y=np.eye(128, dtype=int)[Y_pitch].reshape((Y_pitch.shape[0], 128))\n        )\n\n        performance_spectrogram.loc[performance_spectrogram.size] = {\n            'Model Instrument': f'{instrument_labels[instrument]}', \n            'Loss': test_loss_2, 'Accuracy': test_acc_2\n        }\n\n        predict_mfcc = mfcc_model.predict(X_test_mfcc)\n        result = pd.DataFrame(columns=['Real Pitch', 'Predict Pitch', 'Predict Score'])\n        for rs in range(len(predict_mfcc)):\n            Max = {'index': 0, 'score': 0}\n            for pi in range(len(predict_mfcc[rs])):\n                if predict_mfcc[rs][pi] > Max['score']:\n                    Max['index'] = pi\n                    Max['score'] = predict_mfcc[rs][pi]\n            result.loc[rs] = {\n                    'Real Pitch': librosa.midi_to_note(Y_pitch[rs]), \n                    'Predict Pitch': librosa.midi_to_note(int(Max['index'])), \n                    'Predict Score': str(int(Max['score']*100)) + \"%\"\n            }\n\n        result.head()\n        result.to_csv(f'{i.lower()}_mfcc_prediction.csv')\n\n        predict_spectrogram = spectrogram_model.predict(X_test_spectrogram)\n        result = pd.DataFrame(columns=['Real Pitch', 'Predict Pitch'])\n        for rs in range(len(predict_spectrogram)):\n            index = 0\n            for pi in range(len(predict_spectrogram[rs])):\n                if predict_spectrogram[rs][pi] == 1:\n                    index = predict_spectrogram[rs][pi]\n            result.loc[rs] = {\n                'Real Pitch': librosa.midi_to_note(Y_pitch[rs]), \n                'Predict Pitch': librosa.midi_to_note(index)\n            }\n        result.head()\n        result.to_csv(f'{i.lower()}_spectrogram_prediction.csv')\n    o = gc.collect()","cb1129be":"performance_mfcc.reset_index(drop=True, inplace=True)\nperformance_mfcc.head()\nperformance_mfcc.describe()\nperformance_mfcc.to_csv(f'mfcc_report.csv')","0254a375":"performance_spectrogram.reset_index(drop=True, inplace=True)\nperformance_spectrogram.head()\nperformance_spectrogram.describe()\nperformance_spectrogram.to_csv(f'spectrogram_report.csv')","f5906413":"gc.collect()\nimport seaborn as sns\nsns.set_palette(sns.color_palette('pastel'))","15c741f9":"plt.rcParams['figure.figsize'] = (5, 5)\nperformance_mfcc['Accuracy'].plot.bar(stacked=True, alpha=0.5)   \nplt.xticks(fontsize=16, rotation=0)\nplt.xticks(fontsize=16)\nplt.yticks(fontsize=16)\nplt.legend(fontsize=16)\nplt.ylim(0, 1)\nplt.show()\n\nplt.rcParams['figure.figsize'] = (5, 5)\nperformance_spectrogram['Accuracy'].plot.bar(stacked=True, alpha=0.5)   \nplt.xticks(fontsize=16, rotation=0)\nplt.xticks(fontsize=16)\nplt.yticks(fontsize=16)\nplt.legend(fontsize=16)\nplt.ylim(0, 1)\nplt.show()","caa15edd":"plt.rcParams['figure.figsize'] = (5, 5)\nperformance_mfcc['Loss'].plot.bar(stacked=True, alpha=0.5)   \nplt.xticks(fontsize=16, rotation=0)\nplt.xticks(fontsize=16)\nplt.yticks(fontsize=16)\nplt.legend(fontsize=16)\nplt.show()\n\nplt.rcParams['figure.figsize'] = (5, 5)\nperformance_spectrogram['Loss'].plot.bar(stacked=True, alpha=0.5)   \nplt.xticks(fontsize=16, rotation=0)\nplt.xticks(fontsize=16)\nplt.yticks(fontsize=16)\nplt.legend(fontsize=16)\nplt.show()","8a0b205c":"## Loading dataset\nWe use [Nsynth](https:\/\/magenta.tensorflow.org\/datasets\/nsynth#files) to train the models.","e6c3b7b6":"# Muisc Score recognization","9d66a31e":"## Import the modules\nWe use following libs for this projects\n- tensorflow\n- librosa\n- matplotlib","6e671ae2":"## Custom timer\nUse time to count the time cost"}}