{"cell_type":{"ebf0512f":"code","be86a822":"code","f8b622d4":"code","1b8095a9":"code","8696cfcb":"code","72f62dbe":"code","afca4d4a":"code","0cb41226":"code","7880ea5a":"code","522da28e":"code","15dd0308":"code","a9a6e1d1":"code","d4be8a44":"code","ebd96712":"markdown","3758693a":"markdown","75bd75ac":"markdown","3cec23f7":"markdown","eff3f77f":"markdown","847dd770":"markdown","4a2c2cc4":"markdown","5e98d2ff":"markdown","52c482e1":"markdown","b57c7801":"markdown","deca109a":"markdown","995ac48d":"markdown","7856c439":"markdown","f635d581":"markdown","1ccfad64":"markdown","98aa0126":"markdown"},"source":{"ebf0512f":"import numpy as np\nimport pandas as pd\nimport nltk\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n\ndata = pd.read_csv('..\/input\/train.csv')","be86a822":"print(\"Hatred labeled: {}\\nNon-hatred labeled: {}\".format(\n    (data.label == 1).sum(),\n    (data.label == 0).sum()\n))","f8b622d4":"hashtags = data['tweet'].str.extractall('#(?P<hashtag>[a-zA-Z0-9_]+)').reset_index().groupby('level_0').agg(lambda x: ' '.join(x.values))\ndata.loc[:, 'hashtags'] = hashtags['hashtag']\ndata['hashtags'].fillna('', inplace=True)\n\ndata.loc[:, 'mentions'] = data['tweet'].str.count('@[a-zA-Z0-9_]+')\n\ndata.tweet = data.tweet.str.replace('@[a-zA-Z0-9_]+', '')","1b8095a9":"data.tweet = data.tweet.str.replace('[^a-zA-Z]', ' ')","8696cfcb":"from nltk.stem.snowball import SnowballStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet\nfrom nltk import pos_tag, FreqDist, word_tokenize\n\nstemmer = SnowballStemmer('english')\nlemmer = WordNetLemmatizer()\n\npart = {\n    'N' : 'n',\n    'V' : 'v',\n    'J' : 'a',\n    'S' : 's',\n    'R' : 'r'\n}\n\ndef convert_tag(penn_tag):\n    if penn_tag in part.keys():\n        return part[penn_tag]\n    else:\n        return 'n'\n\n\ndef tag_and_lem(element):\n    sent = pos_tag(word_tokenize(element))\n    return ' '.join([lemmer.lemmatize(sent[k][0], convert_tag(sent[k][1][0]))\n                    for k in range(len(sent))])\n    \n\ndata.loc[:, 'tweet'] = data['tweet'].apply(lambda x: tag_and_lem(x))\ndata.loc[:, 'hashtags'] = data['hashtags'].apply(lambda x: ' '.join([stemmer.stem(word) for word in x.split()]))\n","72f62dbe":"from wordcloud import WordCloud, STOPWORDS\nstopwords = STOPWORDS.add('amp')\n\nall_words = ' '.join(data.tweet.values)\nhatred_words = ' '.join(data[data.label == 1].tweet.values)\n\nplt.figure(figsize=(16, 8))\n\ncloud1 = WordCloud(width=400, height=400, background_color='white', stopwords=stopwords).generate(all_words)\nplt.subplot(121)\nplt.imshow(cloud1, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.title('All tweets', size=20)\n\ncloud2 = WordCloud(width=400, height=400, background_color='white', stopwords=stopwords).generate(hatred_words)\nplt.subplot(122)\nplt.imshow(cloud2, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.title('Hatred tweets', size=20)\nplt.show()","afca4d4a":"all_hashtags = FreqDist(list(' '.join(data.hashtags.values).split())).most_common(10)\nhatred_hashtags = FreqDist(list(' '.join(data[data.label==1].hashtags.values).split())).most_common(10)\nplt.figure(figsize=(14, 6))\nax = plt.subplot(121)\npd.DataFrame(all_hashtags, columns=['hashtag', 'Count']).set_index('hashtag').plot.barh(ax=ax, fontsize=12)\nplt.xlabel('# occurrences')\nplt.title('Hashtags in all tweets', size=13)\nax = plt.subplot(122)\npd.DataFrame(hatred_hashtags, columns=['hashtag', 'Count']).set_index('hashtag').plot.barh(ax=ax, fontsize=12)\nplt.xlabel('# occurrences')\nplt.ylabel('')\nplt.title('Hashtags in hatred tweets', size=13)\nplt.show()","0cb41226":"print(\"Number of mentions: {}\\nNumber of tweets having a mention: {}\\nCorrelation with label: {}\".format(\n    data.mentions.sum(),\n    len(data[data.mentions > 0]),\n    np.corrcoef(data.mentions, data.label)[0][1]\n))","7880ea5a":"data.drop('mentions', axis=1, inplace=True)","522da28e":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.corpus import stopwords\n\nvectorizer = TfidfVectorizer(stop_words=stopwords.words('english'), ngram_range=(1, 3), min_df=10)\nfeatures = vectorizer.fit_transform(data.tweet)","15dd0308":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import f1_score\n\nX_train, X_test, y_train, y_test = train_test_split(features, data.label)","a9a6e1d1":"params = {'penalty': ['l1', 'l2'], 'C': [3, 10, 30, 100, 300]}\nlrmodel = GridSearchCV(LogisticRegression(solver='liblinear', max_iter=150), param_grid=params, scoring='f1', cv=5, n_jobs=-1)\nlrmodel.fit(X_train, y_train)\nprint(\"Best parameters found were {} with F1 score of {:.2f}\".format(\n    lrmodel.best_params_,\n    lrmodel.best_score_\n))\nprobas = lrmodel.predict_proba(X_test)\nthresholds = np.arange(0.1, 0.9, 0.1)\nscores = [f1_score(y_test, (probas[:, 1] >= x).astype(int)) for x in thresholds]\nplt.plot(thresholds, scores, 'o-')\nplt.title(\"F1 score for different thresholds\")\nplt.ylabel(\"Score\")\nplt.xlabel(\"Threshold\")\nplt.show()","d4be8a44":"params = {'C': [1000, 3000, 9000, 15000]}\nsvc = GridSearchCV(SVC(kernel='rbf', gamma='auto'), param_grid=params, scoring='f1', cv=3, n_jobs=-1)\nsvc.fit(X_train, y_train)\nprint(\"Best parameters found were {} with F1 score of {:.2f}\".format(\n    svc.best_params_,\n    svc.best_score_\n))\npredictions = svc.predict(X_test)\nprint(\"\\nF1 test score for SVC: {:.2f}\".format(f1_score(y_test, predictions)))","ebd96712":"#### Hashtags\nNow let's see which hashtags are used the most in hatred tweets and in total.","3758693a":"#### Frequent words\nNow let's see what words and hashtags are the most frequenst in hate tweets and in total.","75bd75ac":"Best threshold found for our model is 0.4 so we will label any tweet with probability higher than or equal to 0.4 as hate tweet.","3cec23f7":"#### Lemmatization\nWe lemmatize tweets' words as we have the sentences and we can tag part of speeches, and will stem hashtags.  ","eff3f77f":"Classes are pretty much skewed, it's better to use F1 score as evaluation metric.","847dd770":"#### SVC","4a2c2cc4":"## Extracting features\n\n#### Hashtags and mentions\nWe'll extract hashtags for each tweet as an extra column to explore them later.   \nFor user mentions, all of the usernames have been replaced with `'user'` so we can't get any data from it, we'll just remove mentions and keep the number of mentions in each tweet as an extra features for that tweet.  ","5e98d2ff":"There is no meaningful relation between number of mentions and it looks like there is not a correlation either. So we'll remove number of mentions and won't use it as a feature.  ","52c482e1":"#### Mentions\nLet's see how many mentions are there in total and if they can be of any use","b57c7801":"## Description\nDataset contains nearly 32K tweets which are labeled based on having racist or sexist content. We are going to analyse this dataset and tweets, and by the end, create a classification model to classify tweets.   \nEach row in the dataset has 3 columns:\n* `id`: Assigned ID to this tweet by Analytics Vidhya.\n* `label`: Tweet label, 1 if tweet has hatred content and 0 otherwise.\n* `tweet`: Tweet text.  \n\nDataset is provided by [Analytics Vidhya](http:\/\/https:\/\/datahack.analyticsvidhya.com\/contest\/practice-problem-twitter-sentiment-analysis\/)  ","deca109a":"## Classifying\nWe'll build a SVC and a LogsiticRegression model for classifying our tweets.","995ac48d":"#### Tfidf vectorizing\nNow we use the frequency of each word in tweets as our features","7856c439":"#### Logistic Regression","f635d581":"How are tweets spread among these 2 classes?","1ccfad64":"#### Removing anything but the words\nNow we'll remove anything but the words (punctuations, numbers, etc). Note that this time we'll replace them with a blank space since it might be a `_` or `-` or a punctuation with no space from the next word and we don't want the words to join together.  ","98aa0126":"## Conclusion\nWe saw some of the most common words and hashtags in general and in racist\/sexist tweets, extracted features by counting word tokens and Tfidf weighting them. We used unigrams, bigrams and trigrams as tokens.   \nFinally, we built a logistic regression model and a support vector classifier to classify future tweets in these 2 classes.  "}}