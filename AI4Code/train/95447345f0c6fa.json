{"cell_type":{"3402cb57":"code","7b3946bf":"code","be275c92":"code","9910ef63":"code","401b9b9e":"code","6c846492":"code","66f88531":"code","32d98238":"code","42e9b0c8":"code","667379b6":"code","717e2f22":"code","2ff421cd":"code","ff8e36b1":"code","2e279a66":"code","05435e19":"code","309e183e":"code","75122622":"code","f115e757":"code","0d609a85":"code","238adffd":"code","e8826902":"code","58335ed0":"code","01a47923":"code","3476587d":"code","9f6472ac":"code","1e691d85":"code","3ae245e7":"code","0e7047f7":"code","69587831":"code","c228372a":"markdown"},"source":{"3402cb57":"# Keras\nfrom keras.models import Sequential\nfrom keras.layers import Bidirectional, LSTM, Dense, Embedding, SpatialDropout1D\nfrom keras.optimizers import adam\nfrom keras.callbacks import EarlyStopping\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.preprocessing.text import Tokenizer\nfrom sklearn.preprocessing import LabelEncoder\nimport keras\n\n# Regular Expression\nimport re\n\n# NLTK\nfrom nltk.tokenize import word_tokenize\nfrom nltk import FreqDist\nfrom nltk.stem import WordNetLemmatizer\n\n# EDA\nfrom string import punctuation\nimport pandas as pd\nimport numpy as np","7b3946bf":"# Data Load\ndf = pd.read_csv(\"..\/input\/movie-review-sentiment-analysis-kernels-only\/train.tsv\", sep=\"\t\")\ndf.head()","be275c92":"len(df)","9910ef63":"df.isnull().sum()","401b9b9e":"df['Sentiment'].value_counts()","6c846492":"# Preprocessing\ndf['Phrase'] = df['Phrase'].apply(lambda x: x.lower())\ndf['Phrase'] = df['Phrase'].apply((lambda x: re.sub('[^A-z\\s]','',x)))","66f88531":"lemma=WordNetLemmatizer()\ndef clean_text(text):\n    text_corpus=[]\n    for i in range(0,len(text)):\n        review = str(text[i])\n        review = [lemma.lemmatize(w) for w in word_tokenize(str(review))]\n        review = ' '.join(review)\n        text_corpus.append(review)\n    return text_corpus","32d98238":"#df['Phrase'] = df['Phrase'].map(lambda x : x if len(x.split(\" \")) > 1 else None)","42e9b0c8":"df['clean_text'] = clean_text(df['Phrase'].values)\ndf.head()","667379b6":"# Total Words\naa = ' '.join(list(df['clean_text']))\naa = list(set(aa.split(\" \")))\nlen(aa)","717e2f22":"from sklearn.utils import shuffle","2ff421cd":"df = shuffle(df)\ndf.head()","ff8e36b1":"# Tokenizer\nvocabulary_size = len(aa)\ntokenizer = Tokenizer(num_words=vocabulary_size, split=' ')\ntokenizer.fit_on_texts(df['clean_text'].values)\nsequences = tokenizer.texts_to_sequences(df['clean_text'].values)\ndata = pad_sequences(sequences)#, maxlen=45)","2e279a66":"from keras.utils.np_utils import to_categorical","05435e19":"# Encoder\nencoder = LabelEncoder()\nencoder = encoder.fit_transform(df['Sentiment'])\ntarget = to_categorical(encoder)","309e183e":"data.shape, target.shape","75122622":"from keras.backend import zeros","f115e757":"embeddings_index = dict()\nf = open('..\/input\/glove6b300dtxt\/glove.6B.300d.txt', encoding='utf-8')\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()","0d609a85":"embedding_matrix = np.zeros((vocabulary_size, 300))\nfor word, index in tokenizer.word_index.items():\n    if index > vocabulary_size - 1:\n        break\n    else:\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[index] = embedding_vector","238adffd":"from keras.layers import GRU, Dropout","e8826902":"# Model\nmodel = Sequential()\nmodel.add(Embedding(vocabulary_size, 300, input_length = data.shape[1], weights = [embedding_matrix], trainable=True))\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(Bidirectional(LSTM(256, return_sequences=True)))\nmodel.add(Bidirectional(GRU(256)))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(target.shape[1], activation='softmax'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\nprint(model.summary())","58335ed0":"early_stopping_filter = EarlyStopping(monitor='val_loss', patience=2)","01a47923":"#model.fit(data, target, validation_split=0.1, epochs=4, callbacks=[early_stopping_filter], batch_size=256)\nmodel.fit(data, target, epochs=4, callbacks=[early_stopping_filter], batch_size=256)","3476587d":"testdf = pd.read_csv(\"..\/input\/movie-review-sentiment-analysis-kernels-only\/test.tsv\", sep=\"\t\")\ntestdf.head()","9f6472ac":"testdf['Phrase'] = testdf['Phrase'].apply(lambda x: x.lower())\ntestdf['Phrase'] = testdf['Phrase'].apply((lambda x: re.sub('[^A-z\\s]','',x)))\ntestdf['clean_test'] = clean_text(testdf['Phrase'].values)","1e691d85":"test_sequences = tokenizer.texts_to_sequences(testdf['clean_test'].values)\ntest_data = pad_sequences(test_sequences, maxlen=data.shape[1])","3ae245e7":"y_pred = model.predict_classes(test_data, verbose=1)","0e7047f7":"submissiondf = pd.DataFrame({'PhraseId': testdf['PhraseId'], 'Sentiment': y_pred})\nsubmissiondf.head()","69587831":"submissiondf.to_csv(\"sampleSubmission.csv\", index=False)","c228372a":"# Testing"}}