{"cell_type":{"b53c6569":"code","8f4231f8":"code","4dd0e93a":"code","0d3b2aea":"code","b3f1100e":"code","c5e0a910":"code","d8fe927e":"code","8b141a86":"code","c3d9caa9":"code","fd6f8bda":"code","49185aa4":"code","5840ac87":"code","36349300":"code","0397858a":"code","c8090733":"code","6929674e":"code","c425653e":"code","beacd935":"code","f6ce60e7":"code","e4a29153":"code","a42afb22":"code","1fdcaf28":"code","4eb777f8":"code","bafd7f21":"code","dc3201b6":"code","de5484e2":"code","bb1a6a1c":"code","0a6262ac":"code","0632e934":"code","372c7b03":"code","3ad9ef62":"markdown","d1dc4818":"markdown","3cbd7e73":"markdown","7398699f":"markdown","e00413be":"markdown","7018efa3":"markdown","63242a15":"markdown","1d4a2bc8":"markdown","3ada3ca2":"markdown","d95a068f":"markdown","c917b6ab":"markdown","26325f65":"markdown","6f472be8":"markdown","e2be4ad7":"markdown","0dbf0665":"markdown","3d04a7fb":"markdown","94e11d44":"markdown","ce7c2c60":"markdown","23c28e05":"markdown","1000fc54":"markdown","cc0b0568":"markdown","c3979425":"markdown","4278647b":"markdown","1c365e7e":"markdown","08897763":"markdown","374f6596":"markdown","b548f5a6":"markdown","3056195a":"markdown","9b259b4b":"markdown","15792f89":"markdown","d658b44f":"markdown","d40f1f68":"markdown","f3be8d83":"markdown","e1efd5f6":"markdown","7a80a99c":"markdown","01d5dbe9":"markdown","a2272786":"markdown","a40ba2bf":"markdown","ac89a505":"markdown","eea69418":"markdown","6a9eba64":"markdown","449ca652":"markdown","ab5968c4":"markdown","a42ae2f8":"markdown","1552868e":"markdown","b34efd3f":"markdown","0957689a":"markdown"},"source":{"b53c6569":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport gc\nimport re\nimport spacy\n\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\nfrom nltk.stem import PorterStemmer, SnowballStemmer\nfrom nltk.stem.lancaster import LancasterStemmer\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras import backend as K\nfrom keras.layers import *\nfrom keras.models import *\nfrom keras.initializers import Constant\nfrom keras.utils import plot_model\nfrom keras.optimizers import Adam\n\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split, KFold, StratifiedKFold\nfrom sklearn.datasets import make_classification\n\nfrom tqdm.notebook import tqdm\nfrom IPython.core.display import display, HTML\ntqdm().pandas()\n\npd_ctx = pd.option_context('display.max_colwidth', 100)\npd.set_option('display.float_format', lambda x: '%.3f' % x)","8f4231f8":"# \u0111\u1ecdc file d\u1eef li\u1ec7u \u0111\u1ec3 train\nTRAIN_FILE = '\/kaggle\/input\/quora-insincere-questions-classification\/train.csv'\nTEST_FILE = '\/kaggle\/input\/quora-insincere-questions-classification\/test.csv'\ndf = pd.read_csv(TRAIN_FILE)\ndf.info()\n\ntest_df = pd.read_csv(TEST_FILE)\nwith pd_ctx:\n    print(\"Sincere question\")\n    display(df[df['target'] == 0].head())\n    print(\"Insincere question\")\n    display(df[df['target'] == 1].head())","4dd0e93a":"df['word_count']= df.question_text.progress_apply(lambda x: len(x.split()))\ndata_neg = df[df['target']==0]\ndata_pos = df[df['target']==1]\nstatistic = pd.merge(\n    data_neg[['word_count']].describe(percentiles=[.8, .9999]), \n    data_pos[['word_count']].describe(percentiles=[.8, .9999]), \n    left_index=True, right_index=True, suffixes=('_sincere', '_insincere')\n)\ncolLabels = statistic.columns\ncellText = statistic.round(2).values\nrowLabels = statistic.index\n\nfig, axes = plt.subplots(nrows=1, ncols=2)\naxes[0] = fig.add_axes([0,0,1,1])\naxes[0].bar(['sincere question', 'insincere question'], df.target.value_counts())\nfor p in axes[0].patches:\n    width = p.get_width()\n    height = p.get_height()\n    percent = height \/ len(df)\n    x, y = p.get_xy() \n    axes[0].annotate(f'{percent:.2%}', (x + width\/2, y + height + 0.01*len(df)), ha='center')\naxes[1].axis('off')\nmpl_table = axes[1].table(cellText = cellText, colLabels=colLabels, rowLabels = rowLabels, bbox=[2, 0, 2, 1.5], )\nmpl_table.auto_set_font_size(False)\nmpl_table.set_fontsize(14)","0d3b2aea":"def cloud(docs, title):\n    wordcloud = WordCloud(width=800, height=400, collocations=False, background_color=\"white\").generate(\" \".join(docs))\n    fig = plt.figure(figsize=(10,7), facecolor='w')\n    plt.imshow(wordcloud)\n    plt.axis('off')\n    plt.title(title, fontsize=25,color='k')\n    plt.tight_layout(pad=0)\n    plt.show()\ncloud(data_neg.question_text, \"Sincere question\")\ncloud(data_pos.question_text, \"Insincere question\")","b3f1100e":"def statistic(df):\n    stats = pd.DataFrame();\n    stats['question_text'] = df['question_text']\n    stats['sp_char_words'] = stats['question_text'].str.findall(r'[^a-zA-Z0-9 ]').str.len()\n    stats['num_capital'] = stats['question_text'].progress_map(lambda x: len([c for c in str(x) if c.isupper()]))\n    stats['num_numerics'] = stats['question_text'].progress_map(lambda x: sum(c.isdigit() for c in x))\n    stats['num_stopwords'] = stats['question_text'].progress_map(lambda x: len([c for c in str(x).lower().split() if c in STOPWORDS]))\n    return stats\n# df_stat = statistic(df)\n# df_stat[['sp_char_words','num_capital', 'num_numerics', 'num_stopwords']].describe()","c5e0a910":"# d\u1eef li\u1ec7u \u0111\u1ec3 hu\u1ea5n luy\u1ec7n\ntrain = df.sample(frac = 1,random_state=123).reset_index(drop=True) # shuffle d\u1eef li\u1ec7u\n\n# \u0110\u1ec3 demo nhanh, \u1edf \u0111\u00e2y m\u00ecnh l\u1ea5y ra 100 m\u1eabu d\u1eef li\u1ec7u sample\nsample = train.sample(n=100, random_state=123)\nwith pd_ctx:\n    display(sample)","d8fe927e":"# L\u00e0m s\u1ea1ch c\u00e2u\ncontractions= {\"i'm\": 'i am',\"i'm'a\": 'i am about to',\"i'm'o\": 'i am going to',\"i've\": 'i have',\"i'll\": 'i will',\"i'll've\": 'i will have',\"i'd\": 'i would',\"i'd've\": 'i would have',\"Whatcha\": 'What are you',\"amn't\": 'am not',\"ain't\": 'are not',\"aren't\": 'are not',\"'cause\": 'because',\"can't\": 'can not',\"can't've\": 'can not have',\"could've\": 'could have',\"couldn't\": 'could not',\"couldn't've\": 'could not have',\"daren't\": 'dare not',\"daresn't\": 'dare not',\"dasn't\": 'dare not',\"didn't\": 'did not','didn\u2019t': 'did not',\"don't\": 'do not','don\u2019t': 'do not',\"doesn't\": 'does not',\"e'er\": 'ever',\"everyone's\": 'everyone is',\"finna\": 'fixing to',\"gimme\": 'give me',\"gon't\": 'go not',\"gonna\": 'going to',\"gotta\": 'got to',\"hadn't\": 'had not',\"hadn't've\": 'had not have',\"hasn't\": 'has not',\"haven't\": 'have not',\"he've\": 'he have',\"he's\": 'he is',\"he'll\": 'he will',\"he'll've\": 'he will have',\"he'd\": 'he would',\"he'd've\": 'he would have',\"here's\": 'here is',\"how're\": 'how are',\"how'd\": 'how did',\"how'd'y\": 'how do you',\"how's\": 'how is',\"how'll\": 'how will',\"isn't\": 'is not',\"it's\": 'it is',\"'tis\": 'it is',\"'twas\": 'it was',\"it'll\": 'it will',\"it'll've\": 'it will have',\"it'd\": 'it would',\"it'd've\": 'it would have',\"kinda\": 'kind of',\"let's\": 'let us',\"luv\": 'love',\"ma'am\": 'madam',\"may've\": 'may have',\"mayn't\": 'may not',\"might've\": 'might have',\"mightn't\": 'might not',\"mightn't've\": 'might not have',\"must've\": 'must have',\"mustn't\": 'must not',\"mustn't've\": 'must not have',\"needn't\": 'need not',\"needn't've\": 'need not have',\"ne'er\": 'never',\"o'\": 'of',\"o'clock\": 'of the clock',\"ol'\": 'old',\"oughtn't\": 'ought not',\"oughtn't've\": 'ought not have',\"o'er\": 'over',\"shan't\": 'shall not',\"sha'n't\": 'shall not',\"shalln't\": 'shall not',\"shan't've\": 'shall not have',\"she's\": 'she is',\"she'll\": 'she will',\"she'd\": 'she would',\"she'd've\": 'she would have',\"should've\": 'should have',\"shouldn't\": 'should not',\"shouldn't've\": 'should not have',\"so've\": 'so have',\"so's\": 'so is',\"somebody's\": 'somebody is',\"someone's\": 'someone is',\"something's\": 'something is',\"sux\": 'sucks',\"that're\": 'that are',\"that's\": 'that is',\"that'll\": 'that will',\"that'd\": 'that would',\"that'd've\": 'that would have',\"em\": 'them',\"there're\": 'there are',\"there's\": 'there is',\"there'll\": 'there will',\"there'd\": 'there would',\"there'd've\": 'there would have',\"these're\": 'these are',\"they're\": 'they are',\"they've\": 'they have',\"they'll\": 'they will',\"they'll've\": 'they will have',\"they'd\": 'they would',\"they'd've\": 'they would have',\"this's\": 'this is',\"those're\": 'those are',\"to've\": 'to have',\"wanna\": 'want to',\"wasn't\": 'was not',\"we're\": 'we are',\"we've\": 'we have',\"we'll\": 'we will',\"we'll've\": 'we will have',\"we'd\": 'we would',\"we'd've\": 'we would have',\"weren't\": 'were not',\"what're\": 'what are',\"what'd\": 'what did',\"what've\": 'what have',\"what's\": 'what is',\"what'll\": 'what will',\"what'll've\": 'what will have',\"when've\": 'when have',\"when's\": 'when is',\"where're\": 'where are',\"where'd\": 'where did',\"where've\": 'where have',\"where's\": 'where is',\"which's\": 'which is',\"who're\": 'who are',\"who've\": 'who have',\"who's\": 'who is',\"who'll\": 'who will',\"who'll've\": 'who will have',\"who'd\": 'who would',\"who'd've\": 'who would have',\"why're\": 'why are',\"why'd\": 'why did',\"why've\": 'why have',\"why's\": 'why is',\"will've\": 'will have',\"won't\": 'will not',\"won't've\": 'will not have',\"would've\": 'would have',\"wouldn't\": 'would not',\"wouldn't've\": 'would not have',\"y'all\": 'you all',\"y'all're\": 'you all are',\"y'all've\": 'you all have',\"y'all'd\": 'you all would',\"y'all'd've\": 'you all would have',\"you're\": 'you are',\"you've\": 'you have',\"you'll've\": 'you shall have',\"you'll\": 'you will',\"you'd\": 'you would',\"you'd've\": 'you would have','jan.': 'january','feb.': 'february','mar.': 'march','apr.': 'april','jun.': 'june','jul.': 'july','aug.': 'august','sep.': 'september','oct.': 'october','nov.': 'november','dec.': 'december','I\u2019m': 'I am','I\u2019m\u2019a': 'I am about to','I\u2019m\u2019o': 'I am going to','I\u2019ve': 'I have','I\u2019ll': 'I will','I\u2019ll\u2019ve': 'I will have','I\u2019d': 'I would','I\u2019d\u2019ve': 'I would have','amn\u2019t': 'am not','ain\u2019t': 'are not','aren\u2019t': 'are not','\u2019cause': 'because','can\u2019t': 'can not','can\u2019t\u2019ve': 'can not have','could\u2019ve': 'could have','couldn\u2019t': 'could not','couldn\u2019t\u2019ve': 'could not have','daren\u2019t': 'dare not','daresn\u2019t': 'dare not','dasn\u2019t': 'dare not','doesn\u2019t': 'does not','e\u2019er': 'ever','everyone\u2019s': 'everyone is','gon\u2019t': 'go not','hadn\u2019t': 'had not','hadn\u2019t\u2019ve': 'had not have','hasn\u2019t': 'has not','haven\u2019t': 'have not','he\u2019ve': 'he have','he\u2019s': 'he is','he\u2019ll': 'he will','he\u2019ll\u2019ve': 'he will have','he\u2019d': 'he would','he\u2019d\u2019ve': 'he would have','here\u2019s': 'here is','how\u2019re': 'how are','how\u2019d': 'how did','how\u2019d\u2019y': 'how do you','how\u2019s': 'how is','how\u2019ll': 'how will','isn\u2019t': 'is not','it\u2019s': 'it is','\u2019tis': 'it is','\u2019twas': 'it was','it\u2019ll': 'it will','it\u2019ll\u2019ve': 'it will have','it\u2019d': 'it would','it\u2019d\u2019ve': 'it would have','let\u2019s': 'let us','ma\u2019am': 'madam','may\u2019ve': 'may have','mayn\u2019t': 'may not','might\u2019ve': 'might have','mightn\u2019t': 'might not','mightn\u2019t\u2019ve': 'might not have','must\u2019ve': 'must have','mustn\u2019t': 'must not','mustn\u2019t\u2019ve': 'must not have','needn\u2019t': 'need not','needn\u2019t\u2019ve': 'need not have','ne\u2019er': 'never','o\u2019': 'of','o\u2019clock': 'of the clock','ol\u2019': 'old','oughtn\u2019t': 'ought not','oughtn\u2019t\u2019ve': 'ought not have','o\u2019er': 'over','shan\u2019t': 'shall not','sha\u2019n\u2019t': 'shall not','shalln\u2019t': 'shall not','shan\u2019t\u2019ve': 'shall not have','she\u2019s': 'she is','she\u2019ll': 'she will','she\u2019d': 'she would','she\u2019d\u2019ve': 'she would have','should\u2019ve': 'should have','shouldn\u2019t': 'should not','shouldn\u2019t\u2019ve': 'should not have','so\u2019ve': 'so have','so\u2019s': 'so is','somebody\u2019s': 'somebody is','someone\u2019s': 'someone is','something\u2019s': 'something is','that\u2019re': 'that are','that\u2019s': 'that is','that\u2019ll': 'that will','that\u2019d': 'that would','that\u2019d\u2019ve': 'that would have','there\u2019re': 'there are','there\u2019s': 'there is','there\u2019ll': 'there will','there\u2019d': 'there would','there\u2019d\u2019ve': 'there would have','these\u2019re': 'these are','they\u2019re': 'they are','they\u2019ve': 'they have','they\u2019ll': 'they will','they\u2019ll\u2019ve': 'they will have','they\u2019d': 'they would','they\u2019d\u2019ve': 'they would have','this\u2019s': 'this is','those\u2019re': 'those are','to\u2019ve': 'to have','wasn\u2019t': 'was not','we\u2019re': 'we are','we\u2019ve': 'we have','we\u2019ll': 'we will','we\u2019ll\u2019ve': 'we will have','we\u2019d': 'we would','we\u2019d\u2019ve': 'we would have','weren\u2019t': 'were not','what\u2019re': 'what are','what\u2019d': 'what did','what\u2019ve': 'what have','what\u2019s': 'what is','what\u2019ll': 'what will','what\u2019ll\u2019ve': 'what will have','when\u2019ve': 'when have','when\u2019s': 'when is','where\u2019re': 'where are','where\u2019d': 'where did','where\u2019ve': 'where have','where\u2019s': 'where is','which\u2019s': 'which is','who\u2019re': 'who are','who\u2019ve': 'who have','who\u2019s': 'who is','who\u2019ll': 'who will','who\u2019ll\u2019ve': 'who will have','who\u2019d': 'who would','who\u2019d\u2019ve': 'who would have','why\u2019re': 'why are','why\u2019d': 'why did','why\u2019ve': 'why have','why\u2019s': 'why is','will\u2019ve': 'will have','won\u2019t': 'will not','won\u2019t\u2019ve': 'will not have','would\u2019ve': 'would have','wouldn\u2019t': 'would not','wouldn\u2019t\u2019ve': 'would not have','y\u2019all': 'you all','y\u2019all\u2019re': 'you all are','y\u2019all\u2019ve': 'you all have','y\u2019all\u2019d': 'you all would','y\u2019all\u2019d\u2019ve': 'you all would have','you\u2019re': 'you are','you\u2019ve': 'you have','you\u2019ll\u2019ve': 'you shall have','you\u2019ll': 'you will','you\u2019d': 'you would','you\u2019d\u2019ve': 'you would have'}\nmissing_spell = {'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor', 'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ', 'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What', 'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can', 'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doI': 'do I', 'theBest': 'the best', 'howdoes': 'how does', 'mastrubation': 'masturbation', 'mastrubate': 'masturbate', \"mastrubating\": 'masturbating', 'pennis': 'penis', 'Etherium': 'bitcoin', 'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend', 'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization', 'demonitization': 'demonetization', 'demonetisation': 'demonetization','electroneum':'bitcoin','nanodegree':'degree','hotstar':'star','dream11':'dream','ftre':'fire','tensorflow':'framework','unocoin':'bitcoin','lnmiit':'limit','unacademy':'academy','altcoin':'bitcoin','altcoins':'bitcoin','litecoin':'bitcoin','coinbase':'bitcoin','cryptocurency':'cryptocurrency','simpliv':'simple','quoras':'quora','schizoids':'psychopath','remainers':'remainder','twinflame':'soulmate','quorans':'quora','brexit':'demonetized','iiest':'institute','dceu':'comics','pessat':'exam','uceed':'college','bhakts':'devotee','boruto':'anime','cryptocoin':'bitcoin','blockchains':'blockchain','fiancee':'fiance','redmi':'smartphone','oneplus':'smartphone','qoura':'quora','deepmind':'framework','ryzen':'cpu','whattsapp':'whatsapp','undertale':'adventure','zenfone':'smartphone','cryptocurencies':'cryptocurrencies','koinex':'bitcoin','zebpay':'bitcoin','binance':'bitcoin','whtsapp':'whatsapp','reactjs':'framework','bittrex':'bitcoin','bitconnect':'bitcoin','bitfinex':'bitcoin','yourquote':'your quote','whyis':'why is','jiophone':'smartphone','dogecoin':'bitcoin','onecoin':'bitcoin','poloniex':'bitcoin','7700k':'cpu','angular2':'framework','segwit2x':'bitcoin','hashflare':'bitcoin','940mx':'gpu','openai':'framework','hashflare':'bitcoin','1050ti':'gpu','nearbuy':'near buy','freebitco':'bitcoin','antminer':'bitcoin','filecoin':'bitcoin','whatapp':'whatsapp','empowr':'empower','1080ti':'gpu','crytocurrency':'cryptocurrency','8700k':'cpu','whatsaap':'whatsapp','g4560':'cpu','payymoney':'pay money','fuckboys':'fuck boys','intenship':'internship','zcash':'bitcoin','demonatisation':'demonetization','narcicist':'narcissist','mastuburation':'masturbation','trignometric':'trigonometric','cryptocurreny':'cryptocurrency','howdid':'how did','crytocurrencies':'cryptocurrencies','phycopath':'psychopath','bytecoin':'bitcoin','possesiveness':'possessiveness','scollege':'college','humanties':'humanities','altacoin':'bitcoin','demonitised':'demonetized','bras\u00edlia':'brazilia','accolite':'accolyte','econimics':'economics','varrier':'warrier','quroa':'quora','statergy':'strategy','langague':'language','splatoon':'game','7600k':'cpu','gate2018':'gate 2018','in2018':'in 2018','narcassist':'narcissist','jiocoin':'bitcoin','hnlu':'hulu','7300hq':'cpu','weatern':'western','interledger':'blockchain','deplation':'deflation', 'cryptocurrencies':'cryptocurrency', 'bitcoin':'blockchain cryptocurrency'}\n\ndef clean_tag(x):\n    if '[math]' in x:\n        x = re.sub('\\[math\\].*?math\\]', 'math equation', x) #replacing with [MATH EQUATION]\n    if 'http' in x or 'www' in x:\n        x = re.sub('(?:(?:https?|ftp):\\\/\\\/)?[\\w\/\\-?=%.]+\\.[\\w\/\\-?=%.]+', 'url', x) #replacing with [url]\n    return x\n\ndef contraction_fix(word):\n    try:\n        a=contractions[word]\n    except KeyError:\n        a=word\n    return a\n\ndef misspell_fix(word):\n    try:\n        a=missing_spell[word]\n    except KeyError:\n        a=word\n    return a\n\n\ndef clean_text(text):\n    text = clean_tag(text) # thay th\u1ebf c\u00e1c tag th\u00e0nh t\u1eeb c\u1ed1 \u0111\u1ecbnh\n    text = \" \".join([contraction_fix(w) for w in text.split()]) # s\u1eeda t\u1eeb vi\u1ebft t\u1eaft\n    text = \" \".join([misspell_fix(w) for w in text.split()]) # s\u1eeda t\u1eeb vi\u1ebft sai ch\u00ednh t\u1ea3\n    text = re.sub(r'[^a-zA-Z0-9]', ' ', text) # Lo\u1ea1i b\u1ecf c\u00e1c k\u00fd t\u1ef1 kh\u00f4ng ph\u1ea3i ch\u1eef v\u00e0 s\u1ed1\n#     text = text.lower()\n    return text\n\ndef apply_clean_text(question_text):\n    tmp = pd.DataFrame()\n    tmp['question_text'] = question_text;\n    tmp['clean'] = tmp.question_text.progress_map(clean_text)\n    with pd_ctx:\n        display(tmp)\n    return tmp['clean']\n        \nsample['clean'] = apply_clean_text(sample.question_text)","8b141a86":"OOV_TOKEN = '<OOV>' # out of vocab token: thay th\u1ebf c\u00e1c t\u1eeb kh\u00f4ng c\u00f3 trong t\u1eeb \u0111i\u1ec3n th\u00e0nh t\u1eeb m\u00e0 ta ch\u1ecdn\ndef create_tokenizer(docs):\n    tokenizer = Tokenizer(oov_token=OOV_TOKEN)\n    tokenizer.fit_on_texts(list(docs))\n    print(\"Size of vocabulary: \", len(tokenizer.word_index))\n    return tokenizer\ntokenizer = create_tokenizer(sample['clean'])\nprint(\"20 t\u1eeb \u0111\u1ea7u ti\u00ean trong t\u1eeb \u0111i\u1ec3n:\")\nlist(tokenizer.word_index.items())[:20]","c3d9caa9":"word_sequences = tokenizer.texts_to_sequences(sample['clean'])\n# \u0110\u1ed9 d\u00e0i c\u1ee7a m\u1ed7i chu\u1ed7i\nprint(\"Length of 20 first word_sequences:\")\nprint(list(map(lambda x: len(x) ,word_sequences[:20])))\n\nprint(\"\\n20 first word_sequences:\")\nfor sequence in word_sequences[:20]:\n    print(sequence)","fd6f8bda":"MAX_SENTENCE_LENGTH = 60 # \u0110\u1ed9 d\u00e0i t\u1ed1i \u0111a c\u1ee7a chu\u1ed7i\nPADDING_TYPE = 'post' # ki\u1ec3u padding, post = cu\u1ed1i chu\u1ed7i\nTRUNCATE_TYPE = 'post'# ki\u1ec3u truncating, post = cu\u1ed1i chu\u1ed7i\ndef create_sequence(tokenizer, docs):\n    word_sequeces = tokenizer.texts_to_sequences(docs)\n    padded_word_sequences = pad_sequences(word_sequeces, maxlen=MAX_SENTENCE_LENGTH, padding=PADDING_TYPE, truncating=TRUNCATE_TYPE)\n    return padded_word_sequences\npadded_sequences = create_sequence(tokenizer, sample['clean'])\n\n    \n# \u0110\u1ed9 d\u00e0i c\u1ee7a m\u1ed7i chu\u1ed7i\nprint(\"K\u00edch th\u01b0\u1edbc m\u1ea3ng:\",padded_sequences.shape)\n\nprint(\"Length of 20 first word_sequences:\")\nprint(list(map(lambda x: len(x) ,padded_sequences[:20])))\n\nprint(\"\\n10 first word_sequences:\")\nfor sequence in padded_sequences[:10]:\n    print(sequence)\n","49185aa4":"# Th\u1ef1c hi\u1ec7n l\u00e0m s\u1ea1ch cho d\u1eef li\u00eau hu\u1ea5n luy\u1ec7n v\u00e0 d\u1eef li\u00eau test \u0111\u1ec3 chu\u1ea9n b\u1ecb qua qu\u00e1 tr\u00ecnh train\ntrainY = train.target\n\nprint(\"Clean train question\")\ntrainX_text = apply_clean_text(train.question_text)\nprint(\"Clean test question\")\ntestX_text = apply_clean_text(test_df.question_text)","5840ac87":"# Chia th\u00e0nh t\u1eadp train v\u00e0 validate\nX_train, X_val, y_train, y_val = train_test_split(trainX_text, trainY, test_size=0.2, random_state=123)","36349300":"EMBEDDING_DIM = 300\nlearning_rate = 0.001\n\ndef createModel_bidirectional_LSTM_GRU(features,embedding_matrix = None):\n    output_bias = Constant(np.log([len(data_pos)\/len(data_neg)])) # Kh\u1edfi t\u1ea1o gia tr\u1ecb \u0111\u1ea7u cho bias\n    \n    x_input = Input(shape=(MAX_SENTENCE_LENGTH))\n    if not(embedding_matrix is None):\n        embedding = Embedding(features, EMBEDDING_DIM, input_length=MAX_SENTENCE_LENGTH, weights=[embedding_matrix], trainable=False)(x_input)\n    else:\n        embedding = Embedding(features, EMBEDDING_DIM, input_length=MAX_SENTENCE_LENGTH)(x_input)\n    x = SpatialDropout1D(0.2)(embedding)\n    \n    lstm = Bidirectional(LSTM(256, return_sequences=True))(x)\n    gru = Bidirectional(GRU(128, return_sequences=True))(lstm)\n    \n    x = Concatenate()([lstm, gru])\n    x = GlobalAveragePooling1D()(x)\n    \n    x_output = Dense(1, activation='sigmoid', bias_initializer=output_bias)(x)\n    \n    model = Model(inputs=x_input, outputs=x_output)\n    opt = Adam(lr=learning_rate)\n    model.compile(loss='binary_crossentropy', optimizer= opt, metrics=[f1_m])\n    return model\n\n\n# S\u1eed d\u1ee5ng h\u00e0m n\u00e0y \u0111\u1ec3 t\u00ednh f1_score trong khi train model\n# kh\u00f4ng bi\u1ebft v\u00ec sao e d\u00f9ng metrics.f1_score l\u1ea1i b\u1ecb l\u1ed7i :(\ndef f1_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    \n    recall = true_positives \/ (possible_positives + K.epsilon())\n    precision = true_positives \/ (predicted_positives + K.epsilon())\n    \n    return 2*((precision*recall)\/(precision+recall+K.epsilon()))","0397858a":"def best_threshold(y_train,train_preds):\n    tmp = [0,0,0] # idx, cur, max\n    delta = 0\n    for tmp[0] in tqdm(np.arange(0.1, 0.9, 0.01)):\n        tmp[1] = metrics.f1_score(y_train, np.array(train_preds)>tmp[0])\n        if tmp[1] > tmp[2]:\n            delta = tmp[0]\n            tmp[2] = tmp[1]\n    return delta, tmp[2] # threshold, f1_score","c8090733":"# kh\u1edfi t\u1ea1o strategy s\u1eed d\u1ee5ng TPU \u0111\u1ec3 train model => t\u0103ng t\u1ed1c \u0111\u1ed9 train\nstrategy = None\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect() # TPU detection\n    strategy = tf.distribute.TPUStrategy(tpu)\n    print('Use TPU')\nexcept ValueError:\n    if len(tf.config.list_physical_devices('GPU')) > 0:\n        strategy = tf.distribute.MirroredStrategy()\n        print('Use GPU')\n    else:\n        strategy = tf.distribute.get_strategy()\n        print('Use CPU')","6929674e":"class_weight = {\n    0: 1,\n    1: 3,\n}\nbatch_size = 1024\nn_epochs = 30\n\nearly_stopping=tf.keras.callbacks.EarlyStopping(\n                                                monitor=\"val_loss\",\n                                                patience=3,\n                                                mode=\"min\",\n                                                restore_best_weights=True\n                                              )\n### Gi\u1ea3m learning rate khi model kh\u00f4ng \u0111\u01b0\u1ee3c c\u1ea3i thi\u00ean (c\u00e0ng h\u1ecdc c\u00e0ng ngu)\nreduce_lr=tf.keras.callbacks.ReduceLROnPlateau(\n                                                monitor=\"val_loss\",\n                                                factor=0.2,\n                                                patience=2,\n                                                verbose=1,\n                                                mode=\"auto\"\n                                            )\n\nmy_callbacks=[early_stopping,reduce_lr]\ndef train_model_and_predict(X_train, y_train, X_val, y_val, X_test, embedding_vec=None):\n    # tokenize and convert to sequence\n    print(\"Create vocab...\")\n    tokenizer = create_tokenizer(X_train)\n    print(\"Create sequences...\")\n    X_train_seq = create_sequence(tokenizer, X_train)\n    X_val_seq = create_sequence(tokenizer, X_val)\n    X_test_seq = create_sequence(tokenizer, X_test)\n    embedding_matrix = None\n    if not(embedding_vec is None):\n        embedding_matrix = load_embedding(embedding_vec, tokenizer.word_index)\n    \n    # build model and training\n    model = createModel_bidirectional_LSTM_GRU(len(tokenizer.word_index)+1, embedding_matrix)\n    model.fit(X_train_seq, y_train, batch_size=batch_size, epochs=n_epochs, validation_data=(X_val_seq, y_val), class_weight=class_weight, callbacks=my_callbacks)\n    \n    # get f1_score, threshold\n    val_pred = model.predict(X_val_seq, verbose=1, batch_size=256)\n    threshold, f1_score = best_threshold(y_val, val_pred)\n    \n    test_pred = model.predict(X_test_seq, verbose=1, batch_size=256)\n    \n    return threshold, f1_score, val_pred, test_pred","c425653e":"# train\nwith strategy.scope():\n    threshold, f1_score, val_pred, test_pred = train_model_and_predict(X_train, y_train, X_val, y_val, testX_text)\nprint(metrics.classification_report(y_val,(val_pred>threshold).astype(int)))","beacd935":"# t\u00e1ch t\u1eeb v\u00e0 vecto t\u01b0\u01a1ng \u1ee9ng v\u1edbi n\u00f3\ndef get_coefs(word, *arr): \n    return word, np.asarray(arr, dtype='float32')\n# l\u1ea5y s\u1ed1 d\u00f2ng c\u1ee7a file embeddings \ndef get_lines_count(file_name): \n    return sum(1 for _ in open(file_name, encoding=\"utf8\", errors='ignore'))\n# chuy\u1ec3n file embeddigns th\u00e0nh dict\ndef load_vec(file_name): \n    return dict(\n        get_coefs(*o.split(\" \")) \n            for o in tqdm(open(\n                file_name, encoding=\"utf8\", errors='ignore'), \n                total=get_lines_count(file_name)\n            ) if len(o) > 100\n    )","f6ce60e7":"EMBEDDING_DIM = 300 # ch\u1ecdn s\u1ed1 chi\u1ec1u c\u1ee7a vecto embedding l\u00e0 300\n\nps = PorterStemmer()\nlc = LancasterStemmer()\nsb = SnowballStemmer('english')\n\ndef load_embedding(word2vec, word2index):\n    oov_count = 0 # S\u1ed1 l\u01b0\u1ee3ng t\u1eeb kh\u00f4ng t\u00ecm th\u1ea5y vecto embeddings\n    vocab_count = 0 # S\u1ed1 l\u01b0\u1ee3ng t\u1eeb c\u00f3 vecto embeddings\n    vocab_size = len(word2index)\n    \n    embedding_weights = np.zeros((vocab_size+1, EMBEDDING_DIM)) # kh\u1edfi t\u1ea1o tr\u1ecdng s\u1ed1 weight = 0\n    unknown_vector = np.zeros((EMBEDDING_DIM,), dtype=np.float32) - 1\n    unknown_words = []\n\n    # T\u00ecm ki\u1ebfm t\u1eebng t\u1eeb trong embeddings, n\u1ebfu kh\u00f4ng th\u1ea5y th\u00ec l\u1ea7n l\u01b0\u1ee3t th\u1ef1c hi\u1ec7n c\u00e1c k\u1ef9 thu\u1eadt bi\u1ebfn \u0111\u1ed5i t\u1eeb \u0111\u1ec3 t\u00ecm ra t\u1eeb g\u1ea7n ngh\u0129a.\n    # => hi v\u1ecdng embeddings cho tr\u01b0\u1edbc ph\u1ee7 \u0111\u01b0\u1ee3c c\u00e0ng nhi\u1ec1u t\u1eeb c\u00e0ng t\u1ed1t.\n    for key, i in tqdm(word2index.items()):\n        word = key\n        if word in word2vec:\n            embedding_weights[i] = word2vec[word]\n            continue\n\n        word = key.capitalize()       \n        if word in word2vec:\n            embedding_weights[i] = word2vec[word]\n            continue\n        \n        word = key.upper()       \n        if word in word2vec:\n            embedding_weights[i] = word2vec[word]\n            continue\n        \n        word = key.lower()       \n        if word in word2vec:\n            embedding_weights[i] = word2vec[word]\n            continue\n        \n        # PorterStemmer (\"python\",\"pythoner\",\"pythoning\",\"pythoned\" => \"python\")\n        word = ps.stem(key)        \n        if word in word2vec:\n            embedding_weights[i] = word2vec[word]\n            continue\n        \n        # LancasterStemmer\n        word = lc.stem(key)        \n        if word in word2vec:\n            embedding_weights[i] = word2vec[word]\n            continue\n            \n        # SnowballStemmer (connection, connections, connective, connected, and connecting => connect)\n        word = sb.stem(key)        \n        if word in word2vec:\n            embedding_weights[i] = word2vec[word]\n            continue\n            \n        unknown_words.append(key)\n            \n        embedding_weights[i] = unknown_vector\n\n    print('Top 10 Null word embeddings: ')\n    print(unknown_words[:10])\n#      % np.sum(np.sum(embedding_weights, axis=1) == -1 * EMBEDDING_DIM)\n    print('\\nNull word embeddings: %d' % len(unknown_words))\n    print(\"There are {:.2f}% word out of embedding file\".format(100*len(unknown_words)\/vocab_size))\n    return embedding_weights","e4a29153":"GLOVE_FILE = 'glove.840B.300d\/glove.840B.300d.txt'\n!unzip -n \/kaggle\/input\/quora-insincere-questions-classification\/embeddings.zip {GLOVE_FILE} -d .\nprint('loading glove_vec')\nglove_vec = load_vec(GLOVE_FILE)","a42afb22":"with strategy.scope():\n    glove_threshold, glove_f1_score, glove_val_pred, glove_test_pred = train_model_and_predict(X_train, y_train, X_val, y_val, testX_text, glove_vec)\nprint(metrics.classification_report(y_val,(glove_val_pred>glove_threshold).astype(int)))","1fdcaf28":"# X\u00f3a \u0111\u1ec3 gi\u1ea3i ph\u00f3ng b\u1ed9 nh\u1edb\ndel glove_vec\ngc.collect()","4eb777f8":"PARA_FILE = 'paragram_300_sl999\/paragram_300_sl999.txt'\n!unzip -n \/kaggle\/input\/quora-insincere-questions-classification\/embeddings.zip {PARA_FILE} -d .\nprint('loading para_vec')\npara_vec = load_vec(PARA_FILE)","bafd7f21":"with strategy.scope():\n    para_threshold, para_f1_score, para_val_pred, para_test_pred = train_model_and_predict(X_train, y_train, X_val, y_val, testX_text, para_vec)\nprint(metrics.classification_report(y_val,(para_val_pred>para_threshold).astype(int)))","dc3201b6":"# X\u00f3a \u0111\u1ec3 gi\u1ea3i ph\u00f3ng b\u1ed9 nh\u1edb\ndel para_vec\ngc.collect()","de5484e2":"WIKI_FILE = 'wiki-news-300d-1M\/wiki-news-300d-1M.vec'\n!unzip -n \/kaggle\/input\/quora-insincere-questions-classification\/embeddings.zip {WIKI_FILE} -d .\nprint('loading wiki_vec')\nwiki_vec = load_vec(WIKI_FILE)","bb1a6a1c":"with strategy.scope():\n    wiki_threshold, wiki_f1_score, wiki_val_pred, wiki_test_pred = train_model_and_predict(X_train, y_train, X_val, y_val, testX_text, wiki_vec)\nprint(metrics.classification_report(y_val,(wiki_val_pred>wiki_threshold).astype(int)))","0a6262ac":"# X\u00f3a \u0111\u1ec3 gi\u1ea3i ph\u00f3ng b\u1ed9 nh\u1edb\ndel wiki_vec\ngc.collect()","0632e934":"val_prod = np.zeros((len(X_val),), dtype=np.float32)\n\nval_prod += 1\/3 * np.squeeze(glove_val_pred)\nval_prod += 1\/3 * np.squeeze(para_val_pred)\nval_prod += 1\/3 *np.squeeze(wiki_val_pred)\nthreshold_global, f1_global = best_threshold(y_val, val_prod)\nprint(metrics.classification_report(y_val,(val_prod>threshold_global).astype(int)))","372c7b03":"pred_prob = np.zeros((len(testX_text),), dtype=np.float32)\npred_prob += 1\/3 * np.squeeze(glove_test_pred)\npred_prob += 1\/3 * np.squeeze(para_test_pred)\npred_prob += 1\/3 * np.squeeze(wiki_test_pred)\ny_test_pre=((pred_prob>threshold_global).astype(int))\n\n## Creating the submission File\nsubmit = pd.DataFrame()\nsubmit[\"qid\"]=test_df.qid\nsubmit[\"prediction\"]=y_test_pre\nsubmit.to_csv(\"submission.csv\",index=False)","3ad9ef62":"Sau b\u01b0\u1edbc n\u00e0y, \u0111\u1ed9 d\u00e0i c\u1ee7a t\u1ea5t c\u1ea3 c\u00e1c chu\u1ed7i \u0111\u1ec1u b\u1eb1ng nhau (len = 60)\nV\u1ec1 c\u01a1 b\u1ea3n th\u00ec ph\u1ea7n ti\u1ec1n x\u1ee9 l\u00fd d\u1eef li\u1ec7u \u0111\u00e3 xong.","d1dc4818":"Nh\u1eadn x\u00e9t: Nh\u00ecn chung, d\u1eef li\u1ec7u d\u1ea1ng text ch\u1ee9a kh\u00e1 nhi\u1ec1u y\u00eau t\u1ed1 g\u00e2y nhi\u1ec5u (k\u00ed t\u1ef1 \u0111\u1eb7c bi\u1ec7t, k\u00ed t\u1ef1 s\u1ed1, ch\u1eef hoa ch\u1eef th\u01b0\u1eddng)\n\nV\u00ec v\u1eady ta c\u1ea7n c\u00f3 b\u01b0\u1edbc x\u1eed l\u00fd c\u00e1c k\u00fd t\u1ef1 \u0111\u1eb7c bi\u1ec7t, stopword (n\u1ebfu c\u1ea7n)\n\n","3cbd7e73":"## Train model with Pretrained Embeddings\nTh\u1eed nghi\u1ec7m v\u1edbi m\u00f4 h\u00ecnh k\u1ebft h\u1ee3p c\u00e1c word embeddings \u0111\u01b0\u1ee3c pretained\n\nTa x\u1ebd xem x\u00e9t c\u00e1c t\u1eadp tr\u1ecdng s\u1ed1 c\u1ee7a embedding \u0111\u01b0\u1ee3c trainning s\u1eb5n c\u00f3 c\u1ea3i thi\u1ec7n m\u00f4 h\u00ecnh kh\u00f4ng.\n\nPh\u1ea7n n\u00e0y ta s\u1ebd th\u1eed qua c\u00e1c embedding sau:\n- GloVe embedding\n- Paragram embedding\n- Wiki-news embedding","7398699f":"V\u1edbi paragram embedding, ta th\u1ea5y n\u00f3 bao ph\u1ee7 \u0111\u01b0\u1ee3c nhi\u1ec1u t\u1eeb v\u1ef1ng h\u01a1n so v\u1edbi GloVe (GloVe ~ 81%, paragram ~ 83%)\n\nTuy nhi\u00ean F1_score l\u1ea1i b\u1ecb gi\u1ea3m so v\u1edbi GloVe, nh\u01b0ng v\u1eabn hi\u1ec7u qu\u1ea3 h\u01a1n so v\u1edbi khi kh\u00f4ng s\u1eed d\u1ee5ng pretrain.","e00413be":"# 1. Kh\u1ea3o s\u00e1t d\u1eef li\u1ec7u","7018efa3":"**`create_tokenizer()`:** s\u1ebd t\u1ea1o ra m\u1ed9t t\u1eeb \u0111i\u1ec3n t\u1eeb t\u1eadp v\u0103n b\u1ea3n truy\u1ec1n v\u00e0o.\n\n`OOV_TOKEN`: l\u00e0 token cho c\u00e1c t\u1eeb m\u00e0 kh\u00f4ng xu\u1ea5t hi\u1ec7n trong t\u1eadp t\u1eeb \u0111i\u1ec3n.\n\n`word_index` l\u00e0 `dict` ch\u1ee9a \u00e1nh x\u1ea1 c\u1ee7a 1 t\u1eeb sang index c\u1ee7a n\u00f3\n\nindex c\u00e0ng nh\u1ecf => t\u1ea7n su\u1ea5t xu\u1ea5t hi\u1ec7n c\u00e0ng nhi\u1ec1u","63242a15":"## b. M\u00e3 h\u00f3a c\u1ed9t v\u0103n b\u1ea3n v\u00e0 chuy\u1ec3n \u0111\u1ed5i ch\u00fang th\u00e0nh vect\u01a1","1d4a2bc8":"Do tr\u01b0\u1eddng `question_text` l\u00e0 d\u1ea1ng v\u0103n b\u1ea3n, n\u00ean \u0111\u1ec3 d\u1ec5 hu\u1ea5n luy\u1ec7n ta c\u1ea7n m\u00e3 h\u00f3a \u0111\u1ec3 chuy\u1ec3n th\u00e0nh vecto c\u1ee7a c\u00e1c s\u1ed1 nguy\u00ean.\n\nTokenize l\u00e0 qu\u00e1 tr\u00ecnh chia nh\u1ecf v\u0103n b\u1ea3n th\u00e0nh c\u00e1c ph\u1ea7n nh\u1ecf h\u01a1n \u0111\u01b0\u1ee3c g\u1ecdi l\u00e0 token. M\u1ed7i token s\u1ebd bi\u1ec3u di\u1ec5n 1 t\u1eeb trong c\u00e2u.\n\nModule `keras.Tokenizer` s\u1ebd gi\u00fap ta th\u1ef1c hi\u1ec7n vi\u00eac tokenize.\n\nB\u1ea3n ch\u1ea5t n\u00f3 s\u1ebd duy\u1ec7t qua c\u00e1c t\u1eeb trong to\u00e0n b\u1ed9 v\u0103n b\u1ea3n \u0111\u00ea x\u00e2y d\u1ef1ng l\u00ean 1 b\u1ed9 t\u1eeb \u0111i\u1ec3n. Sau \u0111\u00f3 s\u1ebd s\u1eafp x\u1ebfp c\u00e1c t\u1eeb theo t\u1ea7n xu\u1ea5t xu\u1ea5t hi\u1ec7n c\u1ee7a n\u00f3. T\u1eeb xu\u1ea5t hi\u1ec7n c\u00e0ng nhi\u1ec1u th\u00ec c\u00f3 index c\u00e0ng th\u1ea5p. Sau \u0111\u00f3 s\u1ebd s\u1eed d\u1ee5ng t\u1eeb \u0111i\u1ec3n n\u00e0y \u0111\u1ec3 transform t\u1eebng c\u00e2u \u1edf d\u1ea1ng text sang d\u1ea1ng sequence c\u1ee7a s\u1ed1.","3ada3ca2":"**`clean_tag()`:** h\u00e0m n\u00e0y s\u1ebd lo\u1ea1i b\u1ecf c\u00e1c bi\u1ec3u th\u1ee9c to\u00e1n h\u1ecdc, thay th\u1ebf ch\u00fang b\u1eb1ng 'MATH EQUATION' v\u00e0 thay th\u1ebf c\u00e1c \u0111\u01b0\u1eddng li\u00ean k\u1ebft b\u1eb1ng 'URL'. B\u1edfi v\u00ec c\u00e1c bi\u1ec3u th\u1ee9c to\u00e1n h\u1ecdc v\u00e0 c\u00e1c \u0111\u01b0\u1eddng link th\u01b0\u1eddng kh\u00f4ng mang nhi\u1ec1u \u00fd ngh\u0129a, kh\u00f4ng nh\u1eefng v\u1eady n\u00f3 c\u00f2n c\u00f3 th\u1ec3 g\u00e2y ra nhi\u1ec5u.\n\n**`contraction_fix()`:** Chuy\u1ec3n nh\u1eefng t\u1eeb vi\u1ebft t\u1eaft th\u00e0nh ho\u00e0n ch\u1ec9nh \u0111\u1ec3 tr\u00e1nh x\u1ea3y ra s\u1ef1 hi\u1ec3u l\u1ea7m.\n\n**`misspell_fix()`:** Do d\u1eef li\u1ec7u \u0111\u01b0\u1ee3c thu th\u1eadp tr\u00ean Qoura, n\u00ean kh\u00f4ng th\u1ec3 tr\u00e1nh \u0111\u01b0\u1ee3c vi\u1ec7c b\u1ecb sai ch\u00ednh t\u1ea3. N\u00ean vi\u1ec7c s\u1eeda c\u00e1c t\u1eeb b\u1ecb sai ch\u00ednh t\u1ea3 l\u00e0 c\u1ea7n thi\u1ebft, m\u1ee5c \u0111\u00edch c\u0169ng gi\u1ed1ng v\u1edbi s\u1eeda c\u00e1c t\u1eeb vi\u1ebft t\u1eaft. Nh\u01b0ng ta kh\u00f4ng th\u1ec3 bi\u1ebft \u0111\u01b0\u1ee3c h\u1ebft c\u00e1c kh\u1ea3 n\u0103ng sai ch\u00ednh t\u1ea3 \u0111\u01b0\u1ee3c, n\u00ean m\u00ecnh s\u1ebd ch\u1ec9 s\u1eeda nh\u1eefng t\u1eeb sai ch\u00ednh t\u1ea3 th\u01b0\u1eddng g\u1eb7p, ho\u1eb7c thay th\u1ebf m\u1ed9t s\u1ed1 t\u1eeb t\u1ed1i ngh\u0129a th\u00e0nh t\u1eeb \u0111\u1ed3ng ngh\u0129a v\u1edbi n\u00f3.","d95a068f":"# 2. X\u1eed l\u00fd d\u1eef li\u1ec7u","c917b6ab":"# 3. Chu\u1ea9n b\u1ecb m\u00f4 h\u00ecnh hu\u1ea5n luy\u1ec7n\n\n## \u0110\u1ea7u ti\u00ean m\u00ecnh s\u1ebd t\u00ecm hi\u1ec3u m\u1ed9t s\u1ed1 kh\u00e1i ni\u1ec7m:\n\n`One-hot vector`: \u0110\u00e2y l\u00e0 k\u1ef9 thu\u1eadt bi\u1ec3u di\u1ec5n t\u1eeb b\u1eb1ng vector c\u00f3 s\u1ed1 chi\u1ec1u b\u1eb1ng s\u1ed1 t\u1eeb v\u1ef1ng. Vector n\u00e0y c\u00f3 duy nh\u1ea5t m\u1ed9t chi\u1ec1u c\u00f3 gi\u00e1 tr\u1ecb b\u1eb1ng 1 \u1ee9ng v\u1edbi t\u1eeb \u0111ang bi\u1ec3u di\u1ec5n, c\u00e1c v\u1ecb tr\u00ed kh\u00e1c c\u00f3 gi\u00e1 tr\u1ecb 0. V\u00ed d\u1ee5 [1,0,0,0\u20260]. Bi\u1ec3u di\u1ec5n n\u00e0y gi\u1ea3i quy\u1ebft \u0111\u01b0\u1ee3c m\u1eabu thu\u1eabn ti\u1ec1m n\u0103ng c\u1ee7a bi\u1ec3u di\u1ec5n b\u1eb1ng s\u1ed1. Tuy nhi\u00ean, nh\u01b0\u1ee3c \u0111i\u1ec3m c\u1ee7a ph\u01b0\u01a1ng ph\u00e1p n\u00e0y l\u00e0 s\u1ed1 chi\u1ec1u vector r\u1ea5t l\u1edbn, \u1ea3nh h\u01b0\u1edfng \u0111\u1ebfn qu\u00e1 tr\u00ecnh x\u1eed l\u00fd c\u0169ng nh\u01b0 l\u01b0u tr\u1eef.\n\n`Embedding`: Do s\u1ed1 l\u01b0\u1ee3ng \u0111\u1eb7c tr\u01b0ng (t\u1eeb trong t\u1eeb \u0111i\u1ec3n) l\u00e0 kh\u00e1 l\u1edbn (nh\u01b0\u1ee3c \u0111i\u1ec3m c\u1ee7a one-hot vector), n\u00ean ng\u01b0\u1eddi ta sinh ra k\u1ef9 thu\u1eadt Embedding \u0111\u1ec3 gi\u1ea3m s\u1ed1 chi\u1ec1u c\u1ee7a kh\u00f4ng gian \u0111\u1eb7c tr\u01b0ng. C\u1ee5 th\u1ec3 l\u00e0 m\u1ed7i t\u1eeb s\u1ebd \u0111\u01b0\u1ee3c bi\u1ec3u di\u1ec5n b\u1eb1ng m\u1ed9t vecto c\u00f3 s\u1ed1 chi\u1ec1u x\u00e1c \u0111\u1ecbnh.\n\nC\u00f3 2 c\u00e1ch \u0111\u1ec3 bi\u1ec3u di\u1ec5n embedding:\n- S\u1eed d\u1ee5ng vector ng\u1eabu nhi\u00ean: V\u1edbi c\u00e1ch n\u00e0y, m\u1ed7i t\u1eeb \u0111\u01b0\u1ee3c bi\u1ec3u th\u1ecb b\u1eb1ng m\u1ed9t vector c\u00f3 gi\u00e1 tr\u1ecb c\u1ee7a c\u00e1c chi\u1ec1u l\u00e0 ng\u1eabu nhi\u00ean. Do \u0111\u00f3, s\u1ed1 l\u01b0\u1ee3ng chi\u1ec1u ch\u00fang ta c\u1ea7n s\u1eed d\u1ee5ng \u00edt h\u01a1n nhi\u1ec1u so v\u1edbi s\u1eed d\u1ee5ng one-hot. V\u00ed d\u1ee5: n\u1ebfu b\u1ea1n c\u00f3 1 tri\u1ec7u t\u1eeb, b\u1ea1n c\u00f3 th\u1ec3 bi\u1ec3u th\u1ecb t\u1ea5t c\u1ea3 c\u00e1c t\u1eeb \u0111\u00f3 trong kh\u00f4ng gian 3D, m\u1ed7i t\u1eeb l\u00e0 m\u1ed9t \u0111i\u1ec3m trong kh\u00f4ng gian 3 chi\u1ec1u.\n- S\u1eed d\u1ee5ng Word embedding: \u0110\u00e2y \u0111\u01b0\u1ee3c coi l\u00e0 c\u00e1ch t\u1ed1t nh\u1ea5t \u0111\u1ec3 th\u1ec3 hi\u1ec7n c\u00e1c t\u1eeb trong v\u0103n b\u1ea3n. K\u1ef9 thu\u1eadt n\u00e0y c\u0169ng g\u00e1n m\u1ed7i t\u1eeb v\u1edbi m\u1ed9t vector, nh\u01b0ng \u01b0u vi\u1ec7t h\u01a1n k\u1ef9 thu\u1eadt vector ng\u1eabu nhi\u00ean v\u00ec c\u00e1c vector n\u00e0y \u0111\u01b0\u1ee3c t\u00ednh to\u00e1n \u0111\u1ec3 bi\u1ec3u di\u1ec5n quan h\u1ec7 t\u01b0\u01a1ng \u0111\u1ed3ng gi\u1eefa c\u00e1c t\u1eeb\n\n`SpatialDropout1D`: m\u1ed7i b\u01b0\u1edbc khi train model th\u00ec ng\u1eabu nhi\u00ean (1-p%) c\u00e1c node b\u1ecb lo\u1ea1i b\u1ecf n\u00ean model kh\u00f4ng th\u1ec3 ph\u1ee5 thu\u1ed9c v\u00e0o b\u1ea5t k\u00ec node n\u00e0o c\u1ee7a layer tr\u01b0\u1edbc m\u00e0 thay v\u00e0o \u0111\u00f3 c\u00f3 xu h\u01b0\u1edbng tr\u1ea3i \u0111\u1ec1u weight. Do \u0111\u00f3 m\u00f4 h\u00ecnh s\u1ebd c\u00f3 th\u1ec3 g\u00e2y ra nh\u1eefng \u0111\u1ed9t bi\u1ebft, c\u00f3 th\u1ec3 tho\u00e1t ra c\u00e1c l\u1ed7i m\u00f2n \u0111\u1ec3 \u0111\u1ed9t ph\u00e1. D\u00f3 \u0111\u00f3 c\u00f3 th\u1ec3 h\u1ea1n ch\u1ebf \u0111\u01b0\u1ee3c s\u1ef1 overfitting.\n\n`RNN` `(M\u1ea1ng n\u01a1 ron truy h\u1ed3i)`: \u00fd t\u01b0\u1edfng ch\u00ednh l\u00e0 s\u1eed d\u1ee5ng m\u1ed9t b\u1ed9 nh\u1edb \u0111\u1ec3 l\u01b0u l\u1ea1i th\u00f4ng tin t\u1eeb t\u1eeb nh\u1eefng b\u01b0\u1edbc t\u00ednh to\u00e1n x\u1eed l\u00fd tr\u01b0\u1edbc \u0111\u1ec3 d\u1ef1a v\u00e0o n\u00f3 c\u00f3 th\u1ec3 \u0111\u01b0a ra d\u1ef1 \u0111o\u00e1n ch\u00ednh x\u00e1c nh\u1ea5t cho b\u01b0\u1edbc d\u1ef1 \u0111o\u00e1n hi\u1ec7n t\u1ea1i, Tuy nhi\u00ean nh\u01b0\u1ee3c \u0111i\u1ec3m l\u00e0 n\u00f3 kh\u00f4ng x\u1eed l\u00ed hi\u1ec7u qu\u1ea3 \u0111\u01b0\u1ee3c c\u00e1c th\u00f4ng tin d\u00e0i h\u1ea1n (vanishing gradient)\n\n\u0110\u1ec3 \u0111\u1ed1i ph\u00f3 v\u1edbi v\u1ea5n \u0111\u1ec1 c\u1ee7a m\u1ea1ng`RNN` truy\u1ec1n th\u1ed1ng, ng\u01b0\u1eddi ta s\u1eed d\u1ee5ng Gated Recurrent Unit (GRU) v\u00e0 Long short term memory (LSTM)\n\n`LSTM\/GRU`: LSTM kh\u00f4ng kh\u00e1c m\u00f4 h\u00ecnh truy\u1ec1n th\u1ed1ng c\u1ee7a RNN,nh\u01b0ng ch\u00fang s\u1eed d\u1ee5ng h\u00e0m t\u00ednh to\u00e1n kh\u00e1c \u1edf c\u00e1c tr\u1ea1ng th\u00e1i \u1ea9n. GRU l\u00e0 m\u1ed9t phi\u00ean b\u1ea3n \u00edt ph\u1ee9c t\u1ea1p h\u01a1n LSTM.\n\n## \u0110\u1ec3 gi\u1ea3i quy\u1ebft b\u00e0i to\u00e1n hi\u1ec7n t\u1ea1i, m\u00ecnh s\u1ebd k\u1ebft h\u1ee3p c\u00e1c layer tr\u00ean th\u00e0nh m\u1ed9t m\u00f4 h\u00ecnh nh\u01b0 sau:\n\n![model.png](attachment:12c8c1ee-6ef2-4993-a6fd-2e701c3fa3e3.png)","26325f65":"GloVe l\u00e0 vi\u1ebft t\u1eaft c\u1ee7a global vectors, m\u1ed9t d\u1ef1 \u00e1n m\u00e3 ngu\u1ed3n m\u1edf c\u1ee7a Stanford nh\u1eb1m t\u1ea1o ra c\u00e1c v\u00e9c t\u01a1 bi\u1ec3u di\u1ec5n cho c\u00e1c t\u1eeb. ","6f472be8":"T\u1eadn d\u1ee5ng TPU c\u1ee7a kaggle \u0111\u1ec3 gi\u1ea3m th\u1eddi gian hu\u1ea5n luy\u1ec7n.","e2be4ad7":"**3. Wiki-news embeddings**\n\nTi\u1ebfp t\u1ee5c th\u1eed nghi\u1ec7m v\u1edbi wiki-news","0dbf0665":"**1. GloVe embeddings**","3d04a7fb":"# B\u00e1o c\u00e1o BTL m\u00f4n H\u1ecdc m\u00e1y\n- M\u00e3 l\u1edbp: 2021II_INT3405_20 (GV: Tr\u1ea7n Qu\u1ed1c Long)\n- Sinh vi\u00ean: Tr\u1ea7n Tu\u1ea5n Anh\n- MSSV: 18020149","94e11d44":"## a. Cleaning data","ce7c2c60":"D\u1ec5 d\u00e0ng th\u1ea5y \u0111\u1ed9 d\u00e0i c\u1ee7a m\u1ed7i chu\u1ed7i kh\u00f4ng b\u1eb1ng nhau (chu\u1ed7i d\u00e0i, chu\u1ed7i ng\u1eafn) => s\u1ebd g\u00e2y kh\u00f3 kh\u1eafn cho qu\u00e1 tr\u00ecnh hu\u1ea5n luy\u1ec7n m\u00f4 h\u00ecnh.\n\nTa chu\u1ea9n h\u00f3a t\u1ea5t c\u1ea3 c\u00e1c input v\u1ec1 c\u00f9ng m\u1ed9t \u0111\u1ed9 d\u00e0i c\u1ed1 \u0111\u1ecbnh b\u1eb1ng k\u00fd thu\u1eadt padding v\u00e0 truncating \n\n- `padding`: n\u1ebfu chu\u1ed7i b\u1ecb ng\u1eafn th\u00ec th\u1ef1c hi\u1ec7n padding b\u1eb1ng c\u00e1c th\u00eam 0 v\u00e0o \u0111\u0103ng sau chu\u1ed7i.\n    \n- `truncating`: n\u1ebfu chu\u1ed7i b\u1ecb d\u00e0i th\u00ec ta l\u00e0m ng\u1eafn b\u1eb1ng c\u00e1ch b\u1ecf \u0111i ph\u1ea7n d\u01b0 ra \u1edf cu\u1ed1i chu\u1ed7i.\n\nV\u1ec1 ph\u1ea7n padding v\u00e0 truncating ta s\u1ebd l\u00e0m c\u00e1c chu\u1ed7i s\u1ebd c\u00f3 c\u00f9ng m\u1ed9t \u0111\u1ed9 d\u00e0i c\u1ed1 \u0111inh, nh\u01b0ng bao nhi\u00eau th\u00ec h\u1ee3p l\u00fd ?\n\nNh\u1edb l\u1ea1i ph\u1ea7n th\u1ed1ng k\u00ea b\u00ean tr\u00ean, ta th\u1ea5y r\u1eb1ng c\u00f3 99,99% d\u1eef li\u1ec7u c\u00f3 d\u01b0\u1edbi 54 t\u1eeb. (0.01% c\u00f2n l\u1ea1i t\u01b0\u01a1ng \u1ee9ng v\u1edbi kho\u1ea3ng 100 b\u1ea3n ghi)\n\n\u0110\u1ec3 an to\u00e0n th\u00ec m\u00ecnh s\u1ebd ch\u1ecdn chi\u1ec1u d\u00e0i t\u1ed1i \u0111a l\u00e0 60 => \u0111\u1ea3m b\u1ea3o kh\u00f4ng \u1ea3nh h\u01b0\u1edfng \u0111\u1ebfn qu\u00e1 tr\u00ecnh trainning.\n\n","23c28e05":"**Th\u1ed1ng k\u00ea**","1000fc54":"**Nh\u1eadn x\u00e9t v\u1ec1 ph\u00e2n l\u1edbp d\u1eef li\u1ec7u**\n\nD\u1ef1a v\u00e0o bi\u1ec3u \u0111\u1ed3 tr\u00ean, ta nh\u1eadn th\u1ea5y d\u1eef li\u1ec7u \u0111\u01b0\u1ee3c chia th\u00e0nh 2 class: 0 (sincere question) v\u00e0 1 (insincere question)\n\n- class 0 : 1225312 d\u1eef li\u1ec7u chi\u1ebfm 93.81%\n- class 1 : 80810 d\u1eef li\u1ec7u chi\u1ebfm 6.19%\n\n=> B\u1ed9 d\u1eef li\u1ec7u \u0111\u1ec3 \u0111\u00e0o t\u1ea1o b\u1ecb m\u1ea5t c\u00e2n b\u1eb1ng (k\u1ebft qu\u1ea3 negative g\u1ea5p 15 l\u1ea7n positive)\n\nT\u1ec9 l\u1ec7 d\u1eef li\u1ec7u 15:1 th\u01b0\u1eddng s\u1ebd d\u1eabn \u0111\u1ebfn ng\u1ed9 nh\u1eadn ch\u1ea5t l\u01b0\u1ee3ng m\u00f4 h\u00ecnh. Khi \u0111\u00f3 th\u01b0\u1edbc \u0111o \u0111\u00e1nh gi\u00e1 m\u00f4 h\u00ecnh l\u00e0 \u0111\u1ed9 ch\u00ednh x\u00e1c (accuracy) c\u00f3 th\u1ec3 \u0111\u1ea1t \u0111\u01b0\u1ee3c r\u1ea5t cao m\u00e0 kh\u00f4ng c\u1ea7n t\u1edbi m\u00f4 h\u00ecnh. V\u00ed d\u1ee5, m\u1ed9t d\u1ef1 b\u00e1o ng\u1eabu nhi\u00ean \u0111\u01b0a ra t\u1ea5t c\u1ea3 \u0111\u1ec1u l\u00e0 nh\u00f3m \u0111a s\u1ed1 th\u00ec \u0111\u1ed9 ch\u00ednh x\u00e1c \u0111\u00e3 \u0111\u1ea1t \u0111\u01b0\u1ee3c l\u00e0 93%. Do \u0111\u00f3 kh\u00f4ng n\u00ean l\u1ef1a ch\u1ecdn \u0111\u1ed9 ch\u00ednh x\u00e1c l\u00e0m ch\u1ec9 s\u1ed1 \u0111\u00e1nh gi\u00e1 m\u00f4 h\u00ecnh \u0111\u1ec3 tr\u00e1nh l\u1ea1c quan sai l\u1ea7m v\u1ec1 ch\u1ea5t l\u01b0\u1ee3ng.\n\n=> Trong ph\u1ea7n b\u00e1o c\u00e1o n\u00e0y, m\u00ecnh s\u1ebd s\u1eed d\u1ee5ng ch\u1ec9 s\u1ed1 `F1_score` \u0111\u1ec3 \u0111\u00e1nh gi\u00e1 m\u00f4 h\u00ecnh.\n\n**N\u00f3i qua v\u1ec1 [F1 score](https:\/\/en.wikipedia.org\/wiki\/F-score):**\n\n`F1_score` l\u00e0 trung b\u00ecnh \u0111i\u1ec1u h\u00f2a gi\u1eefa `precision` (\u0111\u1ed9 ch\u00ednh x\u00e1c) v\u00e0 `recall` (\u0111\u1ed9 bao ph\u1ee7)\n\nPrecision: trong t\u1eadp t\u00ecm \u0111\u01b0\u1ee3c th\u00ec bao nhi\u00eau c\u00e1i (ph\u00e2n lo\u1ea1i) \u0111\u00fang.\n\nRecall: trong s\u1ed1 c\u00e1c t\u1ed3n t\u1ea1i, t\u00ecm ra \u0111\u01b0\u1ee3c bao nhi\u00eau c\u00e1i (ph\u00e2n lo\u1ea1i).\n\n\n\n\n**Nh\u1eadn x\u00e9t \u0111\u1ed9 d\u00e0i c\u1ee7a c\u00e1c c\u00e2u trong b\u1ed9 d\u1eef li\u1ec7u**\n\n- C\u00e2u ch\u00e2n th\u00e0nh\n        \u0110\u1ed9 d\u00e0i chung b\u00ecnh kho\u1ea3ng 12,5 t\u1eeb. C\u00e2u d\u00e0i nh\u1ea5t l\u00e0 134 t\u1eeb.\n        80% s\u1ed1 c\u00e2u c\u00f3 \u00edt h\u01a1n ho\u1eb7c b\u1eb1ng 15 t\u1eeb.\n        H\u1ea7u nh\u01b0 \u0111\u1ec1u c\u00f3 d\u01b0\u1edbi 53 t\u1eeb.\n- C\u00e2u kh\u00f4ng ch\u00e2n th\u00e0nh \n        \u0110\u1ed9 d\u00e0i trung b\u00ecnh l\u00e0 17.3 t\u1eeb. C\u00e2u d\u00e0i nh\u00e1t l\u00e0 64 t\u1eeb.\n        80% s\u1ed1 c\u00e2u \u00edt h\u01a1n ho\u1eb7c b\u1eb1ng 25 t\u1eeb.\n        H\u1ea7u nh\u01b0 \u0111\u1ec1u c\u00f3 d\u01b0\u1edbi 54 t\u1eeb.\n\n=> C\u00e2u kh\u00f4ng ch\u00e2n th\u00e0nh c\u00f3 xu h\u01b0\u1edbng d\u00e0i h\u01a1n c\u00e2u ch\u00e2n th\u00e0nh. V\u00e0 \u0111\u1ed9 d\u00e0i c\u1ee7a c\u00e1c c\u00e2u ch\u1ee7 y\u1ebfu nh\u1ecf h\u01a1n 60 k\u00fd t\u1ef1 (gi\u00e1 tr\u1ecb n\u00e0y s\u1ebd \u0111\u01b0\u1ee3c d\u00f9ng sau n\u00e0y)","cc0b0568":"## Chia th\u00e0nh d\u1eef li\u1ec7u th\u00e0nh 2 t\u1eadp train v\u00e0 valid","c3979425":"**\u00c1p d\u1ee5ng ph\u1ea7n ti\u1ec1n x\u1eed l\u00fd cho d\u1eef li\u1ec7u**","4278647b":"# 5. Th\u1eed nghi\u1ec7m v\u00e0 c\u1ea3i thi\u1ec7n","1c365e7e":"**Nh\u1eadn x\u00e9t**\n\n- Word Cloud l\u00e0 m\u1ed9t k\u1ef9 thu\u1eadt tr\u1ef1c quan h\u00f3a d\u1eef li\u1ec7u \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng \u0111\u1ec3 bi\u1ec3u di\u1ec5n d\u1eef li\u1ec7u v\u0103n b\u1ea3n, trong \u0111\u00f3 k\u00edch th\u01b0\u1edbc c\u1ee7a m\u1ed7i t\u1eeb cho bi\u1ebft t\u1ea7n su\u1ea5t ho\u1eb7c t\u1ea7m quan tr\u1ecdng c\u1ee7a n\u00f3.\n- 2 word cloud kh\u00e1 t\u01b0\u01a1ng t\u1ef1 nhau (khi so s\u00e1nh v\u1ec1 c\u00e1c t\u1eeb n\u1ed5i b\u1eadt)\n- C\u00e1c danh t\u1eeb ri\u00eang xu\u1ea5t hi\u1ec7n nhi\u1ec1u h\u01a1n \u1edf c\u00e2u kh\u00f4ng ch\u00e2n th\u00e0nh\n","08897763":"**2. Paragram embeddings**\n\nTi\u1ebfp t\u1ee5c th\u1eed nghi\u1ec7m v\u1edbi m\u1ed9t embedding kh\u00e1c","374f6596":" S\u1eed d\u1ee5ng wiki-news embeddings, ta th\u1ea5y s\u1ed1 l\u01b0\u1ee3ng t\u1eeb kh\u00f4ng n\u0103m trong file n\u00e0y t\u0103ng nhi\u1ec1u h\u01a1n so v\u1edbi GloVe, nh\u01b0ng b\u1ea5t hi\u1ec7u n\u0103ng c\u1ee7a m\u00f4 h\u00ecnh v\u1eabn x\u1ea5p x\u1ec9 v\u1edbi GloVe (F1_score ~ 0.69)","b548f5a6":"Nh\u01b0 \u0111\u00e3 n\u00f3i \u1edf b\u00ean tr\u00ean, ph\u1ea7n n\u00e0y s\u1ebd chuy\u1ec3n c\u00e1c c\u00e2u text th\u00e0nh c\u00e1c chu\u1ed7i s\u1ed1. M\u1ee5c \u0111\u00edch l\u00e0 \u0111\u1ec3 cho m\u00e1y c\u00f3 th\u1ec3 hi\u1ec3u v\u00e0 d\u1ec5 d\u00e0ng h\u1ecdc \u0111\u01b0\u1ee3c th\u00f4i.","3056195a":"**Import c\u00e1c module c\u1ea7n thi\u1ebft**","9b259b4b":"**K\u00edch th\u01b0\u1edbc train.csv:**\n* S\u1ed1 d\u00f2ng: 1.31 millions\n* S\u1ed1 c\u1ed9t: 3\n\n**Data field**\n+ `qid`: m\u00e3 s\u1ed1 c\u1ee7a c\u00e2u h\u1ecfi (question identifier)\n+ `quesntion_text`: n\u1ed9i dung c\u1ee7a c\u00e2u h\u1ecfi c\u1ea7n ph\u00e2n lo\u1ea1i (type: text)\n+ `target`: nh\u00e3n c\u1ee7a c\u00e2u h\u1ecfi, c\u00e2u ch\u1ee9a n\u1ed9i dung toxic c\u00f3 nh\u00e3n l\u00e0 1, ng\u01b0\u1ee3c l\u1ea1i l\u00e0 0 (type: int)\n\nKh\u00f4ng c\u00f3 d\u1eef li\u1ec7u n\u00e0o b\u1ea5t th\u01b0\u1eddng (missing, null)","15792f89":"Sau khi \u0111\u00e1nh gi\u00e1 ta th\u1ea5y d\u1eef li\u1ec7u c\u00f2n kh\u00e1 ph\u1ee9c t\u1ea1p v\u00e0 nhi\u1ec1u nhi\u1ec5u. \u0110\u1ec3 \u0111\u01a1n gi\u1ea3n ho\u00e1 d\u1eef li\u1ec7u ta c\u00f3 th\u1ec3 th\u1ef1c hi\u1ec7n m\u1ed9t s\u1ed1 b\u01b0\u1edbc sau:\n- Lo\u1ea1i b\u1ecf c\u00e1c c\u00f4ng th\u1ee9c to\u00e1n h\u1ecdc, \u0111\u01b0\u1eddng d\u1eabn\n- Chu\u1ea9n h\u00f3a c\u00e1c t\u1eeb vi\u1ebft t\u1eaft th\u00e0nh d\u1ea1ng \u0111\u1ea7y \u0111\u1ee7 c\u1ee7a n\u00f3\n- S\u1eeda l\u1ed7i m\u1ed9t s\u1ed1 t\u1eeb \u0111\u1eb7c bi\u1ec7t, ho\u1eb7c b\u1ecb sai ch\u00ednh t\u1ea3 \n- X\u00f3a b\u1ecf c\u00e1c k\u00fd t\u1ef1 \u0111\u1eb7c bi\u1ec7t (ch\u1ec9 gi\u1eef l\u1ea1i s\u1ed1 v\u00e0 ch\u1eef)\n\nRi\u00eang v\u1ec1 stopword, em nh\u00e2n th\u1ea5y r\u1eb1ng sau khi b\u1ecf th\u00ec kh\u1ea3 n\u0103ng ph\u00e2n lo\u1ea1i c\u1ee7a m\u00f4 h\u00ecnh b\u1ecb gi\u1ea3m xu\u1ed1ng, n\u00ean ph\u1ea7n x\u1eed l\u00fd d\u1eef li\u1ec7u em s\u1ebd \u0111\u1ec3 nguy\u00ean stopword","d658b44f":"Ta c\u1ea7n t\u1ea1o m\u1ed9t m\u1ea3ng ch\u01b0a weight c\u1ee7a c\u00e1c t\u1eeb t\u01b0\u01a1ng \u1ee9ng v\u1edbi `tokenizer.word_index`\n\nNh\u1eadn th\u1ea5y c\u00e1c t\u1eeb trong t\u1eeb \u0111i\u1ec3n c\u00f3 kh\u1ea3 n\u0103ng kh\u00f4ng d\u00f3 trong word embeddings. \n\n=> \u0110\u1ec3 hi\u1ec7u qu\u1ea3 nh\u1ea5t th\u00ec v\u1edbi m\u1ed7i t\u1eeb m\u00e0 ta kh\u00f4ng t\u00ecm th\u1ea5y trong word embeddings th\u00ec ta s\u1ebd th\u1eed bi\u1ebfn \u0111\u1ed5i t\u1eeb sao cho t\u00ecm \u0111\u01b0\u1ee3c vecto g\u1ea7n n\u00f3 nh\u1ea5t. B\u1edfi v\u00ec nh\u01b0 v\u1eady ch\u1eafc ch\u1eafn s\u1ebd t\u1ed1t h\u01a1n l\u00e0 kh\u00f4ng c\u00f3 th\u00f4ng tin g\u00ec\n\n\u0110\u1ec3 bi\u1ebfn \u0111\u1ed5i t\u1eeb th\u00ec ta s\u1ebd s\u1eed d\u1ee5ng m\u1ed9t s\u1ed1 k\u00fd thu\u1eadt sau:\n- Chuy\u1ec3n th\u00e0nh ch\u1eef vi\u1ebft hoa\n- Chuy\u1ec3n th\u00e0nh ch\u1eef th\u01b0\u1eddng\n- Chuy\u1ec3n th\u00e0nh ch\u1eef hoa\n- S\u1eed d\u1ee5ng c\u00e1c th\u01b0 vi\u1ec7n \u0111\u1ec3 stem t\u1eeb (c\u1eaft \u0111i m\u1ed9t ph\u1ea7n k\u00ed t\u1ef1 \u1edf cu\u1ed1i t\u1eeb)\n\nChi\u1ebfn thu\u1eadt l\u00e0 t\u00ecm \u0111\u01b0\u1ee3c c\u00e0ng nhi\u1ec1u t\u1eeb c\u00e0ng t\u1ed1t.","d40f1f68":"# N\u1ed9i dung b\u00e0i b\u00e1o c\u00e1o\n\n1. Kh\u1ea3o s\u00e1t d\u1eef li\u1ec7u\n2. X\u1eed l\u00fd d\u1eef li\u1ec7u\n3. Chu\u1ea9n b\u1ecb m\u00f4 h\u00ecnh hu\u1ea5n luy\u1ec7n\n4. Hu\u1ea5n luy\u1ec7n v\u00e0 d\u1ef1 \u0111o\u00e1n\n5. Th\u1eed nghi\u1ec7m v\u00e0 c\u1ea3i thi\u1ec7n\n6. Submit test","f3be8d83":"### T\u1ea1o t\u1eeb \u0111i\u1ec3n","e1efd5f6":"# M\u00f4 t\u1ea3 b\u00e0i to\u00e1n\n\nQuora l\u00e0 m\u1ed9t n\u1ec1n t\u1ea3ng cho ph\u00e9p m\u1ecdi ng\u01b0\u1eddi h\u1ecdc h\u1ecfi l\u1eabn nhau. Tr\u00ean Quora, m\u1ecdi ng\u01b0\u1eddi c\u00f3 th\u1ec3 \u0111\u1eb7t c\u00e2u h\u1ecfi v\u00e0 k\u1ebft n\u1ed1i v\u1edbi nh\u1eefng ng\u01b0\u1eddi kh\u00e1c, nh\u1eefng ng\u01b0\u1eddi \u0111\u00f3ng g\u00f3p th\u00f4ng tin chi ti\u1ebft \u0111\u1ed9c \u0111\u00e1o v\u00e0 c\u00e2u tr\u1ea3 l\u1eddi ch\u1ea5t l\u01b0\u1ee3ng. M\u1ed9t th\u00e1ch th\u1ee9c quan tr\u1ecdng l\u00e0 lo\u1ea1i b\u1ecf nh\u1eefng c\u00e2u h\u1ecfi thi\u1ebfu ch\u00e2n th\u00e0nh - nh\u1eefng c\u00e2u h\u1ecfi \u0111\u01b0\u1ee3c \u0111\u1eb7t ra d\u1ef1a tr\u00ean nh\u1eefng ti\u1ec1n \u0111\u1ec1 sai l\u1ea7m ho\u1eb7c c\u00f3 \u00fd \u0111\u1ecbnh \u0111\u01b0a ra m\u1ed9t tuy\u00ean b\u1ed1 h\u01a1n l\u00e0 t\u00ecm ki\u1ebfm nh\u1eefng c\u00e2u tr\u1ea3 l\u1eddi h\u1eefu \u00edch.\n\nB\u00e0i to\u00e1n \u0111\u1eb7t ra l\u00e0 ph\u00e2n lo\u1ea1i c\u00e2u h\u1ecfi tr\u00ean quora xem \u0111\u00e2u l\u00e0 ch\u00e2n th\u00e0nh hay thi\u1ebfu ch\u00e2n th\u00e0nh.\n- Input: C\u00e2u h\u1ecfi d\u01b0\u1edbi d\u1ea1ng v\u0103n b\u1ea3n\n- Output: 0\/1 (Yes\/No)","7a80a99c":"## Threshold\n\nB\u00ecnh th\u01b0\u1eddng, \u0111\u1ed1i v\u1edbi ph\u00e2n l\u1edbp nh\u1ecb ph\u00e2n k\u1ebft qu\u1ea3 d\u1ef1 \u0111o\u00e1n s\u1ebd thu\u1ed9c (0,1), threshold = 0.5 .\n\nN\u1ebfu threshold < 0.5 th\u00ec ph\u00e2n l\u1edbp l\u00e0 negative, ng\u01b0\u1ee3c l\u1ea1i th\u00ec l\u00e0 positive\n\nNh\u01b0ng trong m\u1ed9t s\u1ed1 tr\u01b0\u1eddng h\u1ee3p, gi\u00e1 tr\u1ecb 0.5 n\u00e0y c\u00f3 th\u1ec3 ch\u01b0a ph\u1ea3i l\u00e0 t\u1ed1t nh\u1ea5t. \n\nDo \u0111\u00f3, ta th\u1eed \u0111i\u1ec1u ch\u1ec9nh gi\u00e1 tr\u1ecb threshold n\u00e0y \u0111\u1ec3 t\u00ecm ra gi\u00e1 tr\u1ecb n\u00e0o m\u00e0 cho f1_score l\u00e0 t\u1ed1t nh\u1ea5t => best_threshold","01d5dbe9":"### Kh\u1ea3o s\u00e1t c\u00e1c t\u1eeb xu\u1ea5t hi\u1ec7n nhi\u1ec1u trong t\u1eeb ph\u00e2n l\u1edbp","a2272786":"`class_weight`: Nh\u1eadn th\u1ea5y t\u1ec9 l\u1ec7 d\u1eef li\u1ec7u class 0: class 1 = 15:1 (d\u1eef li\u1ec7u b\u1ecb l\u1ec7ch), \u0111\u1ec3 gi\u1ea3m thi\u1ec3u \u1ea3nh h\u01b0\u1edfng th\u00ec m\u00ecnh s\u1eed d\u1ee5ng class_weight \u0111\u1ec3 c\u00e2n b\u1eb1ng tr\u1ecdng s\u1ed1 cho h\u00e0m m\u1ea5t m\u00e1t trong khi hu\u1ea5n luy\u1ec7n. \nSau khi th\u1eed nghi\u1ec7m v\u1edbi nhi\u1ec1u class_weight kh\u00e1c nhau, e th\u1ea5y ch\u1ecdn t\u1ec9 l\u1ec7 1:3 mang l\u1ea1i hi\u1ec7u qu\u1ea3 \u1ed5n nh\u1ea5t.\n\n**Early Stop Callback:**\n\nN\u00f3i m\u1ed9t c\u00e1ch ng\u1eafn g\u1ecdn l\u00e0 Early Stop Callback d\u00f9ng \u0111\u1ec3 d\u1eebng qu\u00e1 tr\u00ecnh train s\u1edbm, sau khi m\u00e0 th\u00f4ng s\u1ed1 ch\u00fang ta quan s\u00e1t (c\u00f3 th\u1ec3 l\u00e0 validation accuracy hay validaiton loss) kh\u00f4ng \u201ckh\u00e1 l\u00ean\u201d sau m\u1ed9t v\u00e0i epochs.\n\n![early-stopping.png](attachment:2b38866d-87c3-44a1-84b3-8cfee66059ce.png)\n\n\u0110\u00e2y c\u0169ng l\u00e0 m\u1ed9t k\u1ef9 thu\u1eadt hay \u0111\u01b0\u1ee3c d\u00f9ng \u0111\u1ec3 tr\u00e1nh overfit. Kh\u00f4ng ph\u1ea3i c\u1ee9 train v\u1edbi nhi\u1ec1u epochs l\u00e0 t\u1ed1t, ch\u00fang ta train nhi\u1ec1u qu\u00e1 m\u00e0 kh\u00f4ng th\u1ea5y val_loss gi\u1ea3m n\u1eefa ho\u1eb7c val_acc t\u0103ng n\u1eefa m\u00e0 v\u1eabn ti\u1ebfp t\u1ee5c train th\u00ec s\u1ebd d\u1eabn \u0111\u1ebfn c\u00f3 kh\u1ea3 n\u0103ng overfit.\n\n**Reduce Learning rate:**\n\nKhi b\u1eaft \u0111\u1ea7u training, em s\u1ebd ch\u1ecdn learning rate l\u1edbn m\u1ed9t x\u00edu \u0111\u1ec3 t\u0103ng t\u1ed1c qu\u00e1 tr\u00ecnh train. Sau \u0111\u00f3 s\u1ebd gi\u1ea3m t\u1ed1c \u0111\u1ed9 h\u1ecdc \u0111i n\u1ebfu sau m\u1ed9t s\u1ed1 epochs, \u0111\u1ed9 l\u1ed7i tr\u00ean t\u1eadp validation kh\u00f4ng gi\u1ea3m","a40ba2bf":"M\u00f4 h\u00ecnh \u0111\u00e3 nh\u1eadn di\u1ec7n \u0111\u01b0\u1ee3c h\u1ea7u h\u1ebft c\u00e1c c\u00e2u negative.\n\nTuy nhi\u00ean \u0111\u1ed1i v\u1edbi c\u00e1c c\u00e2u positive th\u00ec \u0111\u1ed9 ch\u00ednh x\u00e1c ch\u01b0a \u0111\u01b0\u1ee3c cao l\u1eafm, \u0111\u1ed3ng th\u1eddi c\u0169ng kh\u00f4ng th\u1ef1c s\u1ef1 nh\u1ea1y (recall ~ 0.7). D\u1eabn \u0111\u1ebfn F1 score ch\u1ec9 \u0111\u01b0\u1ee3c 0.65","ac89a505":"### \u0110\u1ecdc file embeddings\n\nC\u00e1c file embedding c\u00f3 d\u1ea1ng text, m\u1ed7i d\u00f2ng ch\u1ee9a 1 word v\u00e0 k\u00e8m sau \u0111\u00f3 l\u00e0 1 vector t\u01b0\u01a1ng \u1ee9ng\n\nTa c\u1ea7n l\u1ea5y c\u00e1c word c\u00f3 trong t\u1eeb \u0111i\u1ec3n k\u00e8m v\u1edbi vecto t\u01b0\u01a1ng \u1ee9ng \u0111\u1ec3 t\u1ea1o th\u00e0nh 1 dict \u0111\u1ec3 c\u00f3 th\u1ec3 tra c\u1ee9u nhanh c\u00e1c vecto t\u01b0\u01a1ng \u1ee9ng v\u1edbi c\u00e1c t\u1eeb\n\n","eea69418":"### S\u1eed d\u1ee5ng t\u1eeb \u0111i\u1ec3n chuy\u1ec3n text th\u00e0nh sequence","6a9eba64":"# 6. Submit test","449ca652":"### Stack Models Prediction\n\nT\u1ed5ng k\u1ebft l\u1ea1i th\u00ec c\u00f3 th\u1ec3 th\u1ea5y \u0111\u01b0\u1ee3c r\u1eb1ng hi\u1ec7u n\u0103ng c\u1ee7a model v\u1edbi c\u00e1c ma tr\u1eadn embeddings Glove, Paragram v\u00e0 Wiki-news kh\u00e1 t\u01b0\u01a1ng \u0111\u1ed3ng v\u1edbi nhau, c\u1ea3 v\u1ec1 \u0111\u1ed9 ph\u1ee7, c\u1ea3 v\u1ec1 f1_score v\u00e0 nh\u1eefng y\u1ebfu t\u1ed1 kh\u00e1c.\n\nTuy nhi\u00ean t\u1ea1i sao ch\u00fang ta kh\u00f4ng k\u1ebft h\u1ee3p l\u1ea1i c\u00e1c k\u1ebft qu\u1ea3 c\u1ee7a t\u1eeb m\u00f4 h\u00ecnh b\u00ean tr\u00ean, r\u1ed3i \u0111\u01b0a ra k\u1ebft qu\u1ea3 cu\u1ed1i c\u00f9ng.\n\nTa s\u1ebd l\u1ea5y k\u1ebft qu\u1ea3 trung b\u00ecnh c\u1ee7a 3 l\u1ea7n hu\u1ea5n luy\u1ec7n b\u00ean tr\u00ean.","ab5968c4":"# 4. Hu\u1ea5n luy\u1ec7n v\u00e0 d\u1ef1 \u0111o\u00e1n\n\n","a42ae2f8":"K\u1ebft qu\u1ea3 \u0111\u01b0\u1ee3c c\u1ea3i thi\u1ec7n nh\u1eb9, F1_score t\u0103ng l\u00ean g\u1ea7n 0.7","1552868e":"Chia d\u1eef li\u00eau train th\u00e0nh 2 t\u1eadp c\u00f3 t\u1ec9 l\u1ec7 2 class b\u1eb1ng v\u1edbi ban \u0111\u1ea7u.\n- 80% \u0111\u1ec3 hu\u1ea5n luy\u1ec7n\n- 20% \u0111\u1ec3 x\u00e1c th\u1ef1c ","b34efd3f":"S\u1eed d\u1ee5ng GloVe embedding \u0111\u00e3 c\u1ea3i thi\u1ec7n \u0111\u01b0\u1ee3c hi\u1ec7u qu\u1ea3 c\u1ee7a model, F1_score t\u0103ng r\u00f5 r\u1ec7t (t\u1eeb 0.65 l\u00ean 0.69) so v\u1edbi khi kh\u00f4ng s\u1eed d\u1ee5ng.\n\nHi\u1ec7u n\u0103ng \u0111\u01b0\u1ee3c c\u1ea3i thi\u1ec7n kh\u00f4ng n\u1eb1m ngo\u00e0i d\u1ef1 \u0111o\u00e1n do vi\u1ec7c s\u1eed d\u1ee5ng tr\u1ecdng s\u1ed1 t\u1eeb file embedding n\u00f3 gi\u00fap c\u00e1c t\u1eeb g\u1ea7n ngh\u0129a v\u1edbi nhau s\u1ebd c\u00f3 bi\u1ec3u di\u1ec5n t\u01b0\u01a1ng t\u1ef1 nhau => t\u0103ng kh\u1ea3 n\u0103ng \u0111\u1ecdc hi\u1ec3u c\u1ee7a model.\n","0957689a":"## Train model without Pretrained Embeddings\nTh\u1eed nghi\u1ec7m v\u1edbi m\u00f4 h\u00ecnh v\u1edbi l\u1edbp embedding s\u1ebd kh\u00f4ng s\u1eed d\u1ee5ng b\u1ea5t k\u00ec pretrain word embeddings n\u00e0o (m\u00f4 h\u00ecnh h\u1ecdc l\u1ea1i t\u1eeb \u0111\u1ea7u)\n\nKhi \u0111\u00f3 ph\u1ea7n tr\u1ecdng s\u1ed1 c\u1ee7a layer embedding s\u1ebd \u0111\u01b0\u1ee3c kh\u1edfi t\u1ea1o ng\u1eabu nhi\u00ean, sau \u0111\u00f3 \u0111\u01b0\u1ee3c c\u1ea3i thi\u1ec7n t\u1eeb t\u1eeb trong qu\u00e1 tr\u00ecnh hu\u1ea5n luy\u1ec7n"}}