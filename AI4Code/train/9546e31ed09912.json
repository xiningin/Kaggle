{"cell_type":{"3647fe7c":"code","20af35ca":"code","dd1ef0ca":"code","0ffd98a9":"code","6bf7b5aa":"code","c90f3986":"code","4c8083b6":"code","551a84c8":"code","1d259407":"code","3408d4a9":"code","ffb79524":"code","2e1bcfe3":"code","4865ab0c":"code","81717e4f":"code","5a246d13":"markdown","17cb2774":"markdown","7ddfb641":"markdown","843b3ab2":"markdown"},"source":{"3647fe7c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","20af35ca":"!pip install bert-for-tf2\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom tensorflow.keras import layers\nimport bert","dd1ef0ca":"# Loading pretrained bert layer\nBertTokenizer = bert.bert_tokenization.FullTokenizer\nbert_layer = hub.KerasLayer(\"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-12_H-768_A-12\/1\",\n                            trainable=True)\n\n# Loading tokenizer from the bert layer\n\nvocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n\ntokenizer = BertTokenizer(vocab_file, do_lower_case)","0ffd98a9":"# load the dataset\n\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nsample_submission = pd.read_csv(\"..\/input\/nlp-getting-started\/sample_submission.csv\")\ntrain_df = pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")\n","6bf7b5aa":"# text preprosessing\n\nfrom nltk.stem import PorterStemmer #normalize word form\nfrom nltk.probability import FreqDist #frequency word count\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords #stop words\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import wordnet\nfrom nltk.probability import FreqDist \n\ndef text_cleaning_hyperlink(text,rep):\n    \n    #remove hyper link\n    return re.sub(r\"http\\S+\",\"{}\".format(rep),text) #remove hyperlink\n\n\ndef text_cleaning_punctuation(text):\n    defined_punctuation = string.punctuation.replace('#','')  # specific innovation\n    translator = str.maketrans( defined_punctuation, ' '*len( defined_punctuation)) #remove punctuation    \n    return text.translate(translator)\n\ndef text_cleaning_stopwords(text):\n    \n    stop_words = set(stopwords.words('english'))\n    word_token = word_tokenize(text)\n    filtered_sentence = [w for w in word_token if not w in stop_words]\n    return ' '.join(filtered_sentence) #return string of no stopwords\n\n\n# convert all letters into lowercase ones\ndef text_cleaning_lowercase(text):\n    return text.lower()\n\ndef text_extract(text_lst):\n    \n    txt = []\n    for i,x in enumerate(text_lst):\n        for j,p in enumerate(x):\n            txt.append(p)\n    return txt\n\n# remove digits from the text\n\ndef remove_digits(txt):\n    \n    no_digits = ''.join(i for i in txt if not i.isdigit())\n    return no_digits","c90f3986":"from nltk import pos_tag, word_tokenize\nps = PorterStemmer()\nwnl = WordNetLemmatizer()\n\n\ndef word_lemmatizer_1(sentence):\n    \n    n_sentence = []\n    for word, tag in pos_tag(word_tokenize(sentence)):\n        wntag = tag[0].lower()\n        if wntag in ['a', 'r', 'n', 'v']:\n            wntag = wntag\n        else:\n            wntag = None\n        if not wntag:\n            lemma = wnl.lemmatize(word)\n        else:\n            lemma = wnl.lemmatize(word,wntag)\n        n_sentence.append(lemma)\n    n_s = ' '.join(n_sentence)\n    return n_s","4c8083b6":"from nltk import pos_tag, word_tokenize\nps = PorterStemmer()\nwnl = WordNetLemmatizer()\n\n\ndef word_lemmatizer_2(sentence):\n    \n    n_sentence = []\n    for word, tag in pos_tag(word_tokenize(sentence)):\n        wntag = tag[0].lower()\n        if wntag in ['a', 'r', 'n']:\n            wntag = wntag\n        else:\n            wntag = None\n        if not wntag:\n            lemma = wnl.lemmatize(word)\n        else:\n            lemma = wnl.lemmatize(word,wntag)\n        n_sentence.append(lemma)\n    n_s = ' '.join(n_sentence)\n    return n_s","551a84c8":"import re\nimport string\n\n#print(string.punctuation.replace('#',''))\n\n#Clean the text\ntrain_df['text_clean'] = train_df.text.apply(lambda x: text_cleaning_hyperlink(remove_digits(x),'http'))\ntrain_df['text_clean'] = train_df['text_clean'].apply(lambda x:  text_cleaning_punctuation(x) )\ntrain_df['text_clean'] = train_df['text_clean'].apply(lambda x:  text_cleaning_stopwords(x) )\n\n#train_df['text_clean'] = train_df.text.apply(lambda x: text_cleaning_stopwords(text_cleaning_punctuation(text_cleaning_hyperlink(remove_digits(x),'http'))))\n# train_df['text_clean'] = train_df['text_clean'].apply(lambda x: list(set(x.split(' '))))\n# train_df['text_clean'] = train_df['text_clean'].apply(lambda x: ' '.join(x))\n\ntest_df['text_clean'] = test_df.text.apply(lambda x: text_cleaning_hyperlink(remove_digits(x),'http'))\ntest_df['text_clean'] = test_df['text_clean'].apply(lambda x:  text_cleaning_punctuation(x) )\ntest_df['text_clean'] = test_df['text_clean'].apply(lambda x:  text_cleaning_stopwords(x) )\n\n\n","1d259407":"# see a sample of encoding\ntext = train_df.text[0]\n\n#tokenize \ntokens_list = tokenizer.tokenize(text)\nprint(\"Text after tokenization like this:\", tokens_list)\n\n#initialize dimension\nmax_len = 25\ntext = tokens_list[:max_len-2]\ninput_sequence = [\"[CLS]\"] + text +[\"[SEP]\"]\nprint(\"Text after adding flag -[ClS] and [SEP]: \", input_sequence )\n\n\ntokens = tokenizer.convert_tokens_to_ids(input_sequence)\nprint(\"tokens to their id: \", tokens)\n\npad_len = max_len  - len(input_sequence)\ntokens += [0] * pad_len\nprint(\"tokens: \", tokens)\n\nprint(pad_len)\npad_masks = [1] * len(input_sequence) + [0] * pad_len\nprint(\"Pad Masking: \", pad_masks)\n\nsegment_ids = [0] * max_len\nprint(\"Segment Ids: \",segment_ids)","3408d4a9":"# Function to encoe the text into tokens, mask, and segment flags\n\nimport numpy as np\n\n\ndef bert_encode(texts, tokenizer, max_len=512):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n        text = tokenizer.tokenize(text)\n            \n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens += [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)\n\nMAX_LEN = 160\n\n# encode train set \n# train_input = bert_encode(train_df['text_clean'], tokenizer, max_len=MAX_LEN)\n\n# encode test set \n# test_input = bert_encode(test_df['text_clean'], tokenizer, max_len= MAX_LEN )\n\n","ffb79524":"from tensorflow.keras.layers import Input\n\n\n\ninput_word_ids = Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"input_word_ids\")\ninput_mask = Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"input_mask\")\nsegment_ids = Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"segment_ids\")\n\n    \n#output\n\nfrom tensorflow.keras.layers import Dense\n\n_, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\nclf_output = sequence_output[:, 0, :]\nout = Dense(1, activation='sigmoid')(clf_output)\n","2e1bcfe3":"#model initialization \n\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\n\nmodel = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\nmodel.compile(Adam(lr=2e-6), loss='binary_crossentropy', metrics=['accuracy'])\n\nmodel.summary()\n\n","4865ab0c":"from sklearn.model_selection import train_test_split\n\n\n# personal test\n# X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(train_df['text_clean'], train_df.target, test_size=0.3)\n# train_input = bert_encode(X_train_1, tokenizer, max_len=MAX_LEN)\n# test_input = bert_encode(X_test_1, tokenizer, max_len=MAX_LEN)\n# train_labels = y_train_1\n# test_labels = y_test_1\n\n# submission test\n\n# encode train set \ntrain_input = bert_encode(train_df.text.values, tokenizer, max_len=MAX_LEN)\n# encode  test set \ntest_input = bert_encode(test_df.text.values, tokenizer, max_len= MAX_LEN )\n\ntrain_labels = train_df.target.values\n\ntrain_history = model.fit(\n    train_input, train_labels,\n    validation_split=0.2,\n    epochs=3,\n    batch_size=32\n)\n\nmodel.save('model.h5')\n\n\n# train\n# train_history = model.fit(\n#     train_input, train_labels,\n#     validation_split=0.2,\n#     epochs=20,\n#     batch_size=32\n# )\n\n# model.save('model.h5')","81717e4f":"test_pred = model.predict(test_input)\npreds = test_pred.round().astype(int)\npreds\n\nsample_submission[\"target\"] = preds\nsample_submission.target.value_counts()\nsample_submission.to_csv(\"submission.csv\", index = False)\n\n# scores = model.evaluate(test_input, test_labels, verbose=1)\n# print(\"Accuracy:\", scores[1])\n","5a246d13":"## 3.BERT Modeling\nBasic Model.","17cb2774":"## 1.BERT Pretrained Layer\n\nFetch the pretrained BERT LAYER and load the tokenizer","7ddfb641":"Encode the dataset now","843b3ab2":"## 2. BERT Encoding\n1. each sentence is first tokenized into tokens\n2. A [CLS] token is inserted at the beginning of the first sentence; A [SEP] token is inserted at the end of each sentence\n\n3. Token that comply with the fixed vocabulary are fetched and assigend with 3 properties\n1) Token IDs - assign unique token-id from BERT's tokenizer\n2) Padding ID (MASK-id) - to indicate which elements in the sequence are tokens and which are padding elements\n3) Segement IDs - to distinguish diffrennt sentences"}}