{"cell_type":{"29fad4e9":"code","b7ac3412":"code","0adaf11c":"code","7c545b31":"code","beeb5bdf":"code","da9068fd":"code","4202ae6b":"code","2996ebc1":"code","4ca3752a":"code","767c418d":"code","4d4effb8":"code","e497ad79":"code","503120b5":"code","1687fb63":"code","3c75d320":"code","8072aa1c":"code","e8d72825":"code","aa921cce":"code","d17944e6":"code","ecc5dafb":"code","e1d691c3":"code","f26e9c38":"code","ba869441":"code","b52432bf":"code","e3ec75ef":"code","f149fa3d":"code","e9742082":"code","a1f5702b":"code","c6eab883":"code","af9ee04f":"code","0f0fcb06":"code","24ba55fd":"code","dd7ec5b5":"markdown","3e216475":"markdown","96a3d7ce":"markdown","823d7e94":"markdown","46eabdd9":"markdown","b7dc589c":"markdown","2f5069aa":"markdown","929ff668":"markdown","12f2ec26":"markdown","0f233522":"markdown","1affe988":"markdown","7b5abd02":"markdown","caa4d1bb":"markdown","11056739":"markdown","cd489d50":"markdown","4ac25e15":"markdown","142f6ac2":"markdown","13afca3c":"markdown","9e683d7b":"markdown"},"source":{"29fad4e9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b7ac3412":"train = pd.read_csv('..\/input\/hr-analytics-job-change-of-data-scientists\/aug_train.csv')\ntest = pd.read_csv('..\/input\/hr-analytics-job-change-of-data-scientists\/aug_test.csv')","0adaf11c":"missing = train.isnull().sum().sort_values(ascending=False).reset_index()\nmissing.columns = ['features','missing_num']\nmissing['percentage'] = missing['missing_num']\/train.shape[0]\nmissing","7c545b31":"print('Among a total of 13 features,' + str((missing['missing_num']>0).sum())+ ' features contains missing values.')\nprint('And ' + str((missing['percentage']>0.3).sum()) + ' features contains over 30% missing values.')","beeb5bdf":"# here, set the threshold to 30%\nthr = 0.7*train.shape[0]\ntrain2 = train.dropna(thresh = thr, axis = 1) #drop columns with too many missing val\ntrain2.isnull().sum().sort_values(ascending=False).reset_index()","da9068fd":"train2.dtypes","4202ae6b":"#gender\ntrain2.loc[train2['gender'].isnull(),'gender'] = train2['gender'].value_counts().index[0]\n\n#major_discipline\ntrain2.loc[train2['major_discipline'].isnull(),'major_discipline'] = train2['major_discipline'].value_counts().index[0]\n\n#education_level\ntrain2.loc[train2['education_level'].isnull(),'education_level'] = train2['education_level'].value_counts().index[0]\n\n#last_new_job\ntrain2.loc[train2['last_new_job'].isnull(),'last_new_job'] = train2['last_new_job'].value_counts().index[0]\n\n#enrolled_university\ntrain2.loc[train2['enrolled_university'].isnull(),'enrolled_university'] = train2['enrolled_university'].value_counts().index[0]\n\n#experience\ntrain2.loc[train2['experience'].isnull(),'experience'] = train2['experience'].value_counts().index[0]","2996ebc1":"train2.isnull().sum().sort_values(ascending=False).reset_index()","4ca3752a":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","767c418d":"train2.dtypes","4d4effb8":"#city_development_index\nplt.boxplot(train2['city_development_index'],vert=False)\nplt.show()","e497ad79":"q3 =  train2['city_development_index'].describe()['75%']\nq1 =  train2['city_development_index'].describe()['25%']\niqr = q3-q1\ntrain2.loc[train2['city_development_index'] < q1 - 1.5*iqr,'city_development_index']= q1-1.5*iqr #reset outliers","503120b5":"#training_hours \nplt.boxplot(train2['training_hours'],vert=False)\nplt.show()","1687fb63":"q3 =  train2['training_hours'].describe()['75%']\nq1 =  train2['training_hours'].describe()['25%']\niqr = q3-q1\ntrain2.loc[train2['training_hours'] < q1 - 1.5*iqr,'training_hours']= 200 #reset outliers","3c75d320":"train2.head()","8072aa1c":"#one-hot encoding\ntrain2 = pd.get_dummies(train2, columns=['gender','enrolled_university','major_discipline'])\n#ohe_test = pd.get_dummies(test, columns=['gender','enrolled_university','major_discipline'])\n#train2,test = train2.align(ohe_test,join='left',axis=1)","e8d72825":"#hash encoding\nimport category_encoders as ce\nencoder_city = ce.HashingEncoder(cols=['city'])\ncity_he = encoder_city.fit_transform(train2['city'], train2['target'])\ntrain2=train2.drop(columns=['city'])\ntrain2= pd.concat([train2, city_he],axis=1)\n\n#city_he_test = encoder_city.transform(test['city'], test['target'])\n#test = test.drop(columns=['city'])\n#test = pd.concat([test, city_he_test], axis=1)","aa921cce":"#label encoding\nfrom sklearn.preprocessing import LabelEncoder as le\nfrom collections import defaultdict\nd = defaultdict(le)\n\nle_train = train2[['relevent_experience','education_level','experience','last_new_job']].apply(lambda x: d[x.name].fit_transform(x),axis=0)\n#le_test = test[['relevent_experience','education_level','experience','last_new_job']].apply(lambda x: d[x.name].transform(x) if type(x) == str else x)","d17944e6":"train2 = train2.drop(columns=['relevent_experience','education_level','experience','last_new_job'])\ntrain2 = pd.concat([train2,le_train],axis=1)\n\n#test = test.drop(columns=['relevent_experience','education_level','experience','last_new_job'])\n#test = pd.concat([test, le_test], axis=1)","ecc5dafb":"train2.head()","e1d691c3":"from collections import Counter #summerize class distribution\nfrom imblearn.over_sampling import SMOTE\n\nX = train2.drop(columns=['target', 'enrollee_id'])\ny = train2['target']\n\n#summerize class distribution: before\ncounter = Counter(y)\nprint(counter)\n\n#Oversampling using SMOTE\nsmt = SMOTE(random_state=42)\nX,y = smt.fit_sample(X,y)\n\n#summerize class distribution: after\ncounter = Counter(y)\nprint(counter)","f26e9c38":"#create training set and test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","ba869441":"#create models\nfrom sklearn.metrics import confusion_matrix,accuracy_score\n\ndef Model(model, X_train, X_test, y_train, y_test, title):\n    \n    #train\n    model.fit(X_train, y_train)\n    \n    #predict\n    y_train_pred = model.predict(X_train)\n    y_test_pred = model.predict(X_test)\n    \n    #evaluate\n    print(title + ' - training set - accuracy score: ', accuracy_score(y_train, y_train_pred))\n    print(title + ' - test set - accuracy score: ' , accuracy_score(y_test, y_test_pred))\n    print(title + ' - training set - confusion matrix: \\n' , confusion_matrix(y_train, y_train_pred))\n    print(title + ' - test set - confusion matrix: \\n' ,confusion_matrix(y_test, y_test_pred))\n  \n    \n#find important features\n\ndef ImportantFeatures(model):\n    model.fit(X_train, y_train)\n    importances = model.feature_importances_\n    features = X_train.columns.values\n    imp = pd.DataFrame({'Features': features, 'Importance': importances})\n    imp.sort_values(by='Importance')\n    \n    return imp","b52432bf":"from sklearn.linear_model import LogisticRegression\nModel(LogisticRegression(solver='lbfgs', max_iter=10000,random_state=42),X_train, X_test, y_train, y_test, 'Logistic Regression w\/ SMOTE')","e3ec75ef":"from sklearn.svm import SVC\nModel(SVC(random_state=42), X_train, X_test, y_train, y_test, 'SVM w\/ SMOTE')","f149fa3d":"from sklearn.naive_bayes import GaussianNB\nModel(GaussianNB(), X_train, X_test, y_train, y_test, 'GaussianNB w\/ SMOTE')","e9742082":"from sklearn.neighbors import KNeighborsClassifier\nModel(KNeighborsClassifier(), X_train, X_test, y_train, y_test, 'KNN w\/ SMOTE')","a1f5702b":"from sklearn.tree import DecisionTreeClassifier\nModel(DecisionTreeClassifier(max_depth=8), X_train, X_test, y_train, y_test, 'Decision Tree w\/ SMOTE')","c6eab883":"ImportantFeatures(DecisionTreeClassifier(max_depth=14))","af9ee04f":"from sklearn.ensemble import RandomForestClassifier\nModel(RandomForestClassifier(max_features=8,n_estimators=4000,max_depth=10,random_state=42), X_train, X_test, y_train, y_test, 'RandomForest w\/ SMOTE')","0f0fcb06":"ImportantFeatures(RandomForestClassifier(max_features=8,n_estimators=4000,max_depth=10,random_state=42))","24ba55fd":"from xgboost import XGBClassifier\n\nModel(XGBClassifier(random_state=42), X_train, X_test, y_train, y_test, 'XGBoost w\/ SMOTE')","dd7ec5b5":"## 5.7 XGBoost","3e216475":"## 5.1 Logistic Regression","96a3d7ce":"# 1. Data Loading","823d7e94":"# 5. Modeling","46eabdd9":"## 5.6 Random Forest","b7dc589c":"### Fill in values for remaining features ","2f5069aa":"### Remove features that have too many missing values","929ff668":"# 6. Conclusion","12f2ec26":"=================================\n\n### Key Takeaway\n\n- how to handle missing values \n- categorical encoding (https:\/\/www.analyticsvidhya.com\/blog\/2020\/08\/types-of-categorical-data-encoding\/)\n- deal with unbalanced data (SMOTE: https:\/\/towardsdatascience.com\/5-smote-techniques-for-oversampling-your-imbalance-data-b8155bdbe2b5#:~:text=Borderline%2DSMOTE%20is%20a%20variation,boundary%20between%20the%20two%20classes.)\n\n\nLastly, huge thanks to Huynh Dong Nguyen's notebook! Learned a lot from it!","0f233522":"# 4. Oversampling Using SMOTE","1affe988":"## 5.4 KNN","7b5abd02":"## 2.2 Outliers","caa4d1bb":"# 3. Feature Engineering - Encoding\n\n- One-hot encoding for gender, enrolled_university, major_discipline (nominal)\n- Hash encoding for city (deal with high cardinality)\n- Label encoding for relevent experience,education_level, experience, last_new_job (ordinal)\n\n\nMore on Encoding: https:\/\/www.analyticsvidhya.com\/blog\/2020\/08\/types-of-categorical-data-encoding\/","11056739":"## Top 3 Models\n\n1. XGBoost (training accuracy: ~87%, test accuracy: ~83%)\n2. Logistic Regression (training\/test accuracy: ~82%)\n3. Random Forest(training accuracy: ~84%, test accuracy: ~81%)\n\n\n## The Most important Factor - city_development_index (Based on Random Forest)\n","cd489d50":"# 2. Data Preprocessing\n\n## 2.1 Missing Value\n\n### Count Missing value by features","4ac25e15":"Since features with missing values are all categorical, we can use mode or modeling (such as random forest) to fill in. Here, we simply use mode.","142f6ac2":"## 5.5 Decision Tree","13afca3c":"## 5.2 SVM","9e683d7b":"## 5.3 GaussianNB"}}