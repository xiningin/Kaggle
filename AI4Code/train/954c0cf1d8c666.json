{"cell_type":{"a4a74ef3":"code","264dde88":"code","efc92bde":"code","855fe417":"code","148d128f":"code","125f8a7d":"code","e1ff492f":"code","64946104":"code","c8c00cff":"code","83c842b1":"code","cbf0e326":"code","d2cf9f1c":"code","6444aa78":"code","665798c9":"code","ad664d49":"code","e0885803":"code","5adf8240":"code","389393c9":"code","08f966ba":"code","62fb6316":"code","76852c7f":"code","05a9ac32":"code","bbde713b":"code","546e9fb5":"code","75821b85":"code","f27ef089":"code","26ee6b43":"code","234490e0":"code","d792808b":"code","9f6bfa5e":"code","7aca7695":"code","b487a2de":"code","05c254cc":"code","3f2955f5":"code","927b9f52":"code","c53b4f1a":"code","e930df4a":"code","cb51e3cb":"code","12a30728":"code","c1720eba":"code","7e2ec571":"code","bcfffefc":"code","58ad74cc":"code","0a004d94":"code","e872a25b":"code","2a9f1287":"code","93fbdfb9":"code","37320f21":"code","b22f1d99":"code","cae78402":"markdown","eb7296a4":"markdown","48711080":"markdown","c7ed57df":"markdown","ff0540ab":"markdown","094ac510":"markdown","a4dc815b":"markdown","f462ae56":"markdown","60bc75a3":"markdown","18481772":"markdown","58b95441":"markdown","525a02a1":"markdown","564819f7":"markdown","fc6e8657":"markdown","93b02e1a":"markdown","bb208bbd":"markdown","ee61fc96":"markdown","87cbfc42":"markdown","bc803b8e":"markdown","8f20c37c":"markdown","c430f4f3":"markdown","a93e7597":"markdown","75ce7d41":"markdown","6491343f":"markdown","66c4b374":"markdown","5bb03a6c":"markdown","8455ab59":"markdown","714e8f6e":"markdown","6b8e756a":"markdown","9aa69aec":"markdown","8241df60":"markdown","d5d3c17d":"markdown","de09d766":"markdown","886fa20b":"markdown","e3124bfb":"markdown","e4a3099b":"markdown","b09a9783":"markdown","744bd8eb":"markdown","774db5d5":"markdown","5035b14e":"markdown"},"source":{"a4a74ef3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","264dde88":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom scipy.stats import norm\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n","efc92bde":"train_df = pd.read_csv('..\/input\/home-data-for-ml-course\/train.csv', index_col = 'Id')\ntest_df = pd.read_csv('..\/input\/home-data-for-ml-course\/test.csv', index_col = 'Id')","855fe417":"train_df.shape","148d128f":"train_df.info()","125f8a7d":"train_df.drop(['Alley','PoolQC','Fence','MiscFeature'], axis = 1, inplace = True)\ntest_df.drop(['Alley','PoolQC','Fence','MiscFeature'], axis = 1, inplace = True)","e1ff492f":"pd.set_option('display.max_columns', 100)\ntrain_df.describe()","64946104":"train_df.head(10)","c8c00cff":"b = int(train_df.shape[1]\/4)\nr = int(train_df.shape[1]\/4)\nc = int(train_df.shape[1]\/b)\ni = 0\nfig, ax = plt.subplots(nrows = r, ncols = c, figsize=(15, 80))\nfor row in ax:\n    for col in row:\n        try:\n            col.scatter(x = train_df[train_df.columns[i]], \n                        y = np.log(train_df['SalePrice']))\n            col.title.set_text(train_df.columns[i])\n        except:\n            temp=1\n        finally:\n            temp=1\n        i = i + 1\n        \nplt.show()","83c842b1":"train_df[(train_df['GrLivArea'] > 4000)]","cbf0e326":"#removing outliers\ntrain_df = train_df.drop(train_df[(train_df['GrLivArea'] >= 4000) & \n                       (train_df['SalePrice'] <= 200000)].index)","d2cf9f1c":"train_df[(train_df['OverallQual'] > 9)]","6444aa78":"#removing outliers\ntrain_df = train_df.drop(train_df[(train_df['OverallQual'] > 9) & \n                       (train_df['SalePrice'] < 300000)].index)","665798c9":"train_df['TotalSF'] = train_df['TotalBsmtSF'] + train_df['1stFlrSF'] + train_df['2ndFlrSF']\ntest_df['TotalSF'] = test_df['TotalBsmtSF'] + test_df['1stFlrSF'] + test_df['2ndFlrSF']\n\nplt.figure(figsize = (10,6))\nsns.regplot(x = 'TotalSF', y = np.log(train_df['SalePrice']), data = train_df)\nplt.show()","ad664d49":"train_df['age'] = train_df['YrSold'] - train_df['YearBuilt']\ntest_df['age'] = test_df['YrSold'] - test_df['YearBuilt']\n\nplt.figure(figsize = (10,6))\nsns.regplot(x = 'age', y = np.log(train_df['SalePrice']), data = train_df)\nplt.show()","e0885803":"def col_type(col):\n    if (col.nunique() <= 10) | (col.dtype == 'object'):\n        return 'cat'\n    else:\n        return 'num'\n    \ncategorical_col = [cname for cname in train_df.columns\n                  if col_type(train_df[cname]) == 'cat']\nnumerical_col = [cname for cname in train_df.columns\n                if col_type(train_df[cname]) == 'num']\n\nprint('Categorical Columns: {}'.format(categorical_col))\nprint('Numerical Columns: {}'.format(numerical_col))","5adf8240":"numerical_col.append('poolArea')\ncategorical_col.remove('PoolArea')","389393c9":"corrMat = train_df.corr()\n\nf, ax = plt.subplots(figsize = (20,20))\ncmap = sns.diverging_palette(230, 20, as_cmap = True)\nmask = np.triu(np.ones_like(corrMat, dtype = bool))\nsns.heatmap(corrMat, square = True, annot = True, cmap = cmap, mask = mask)","08f966ba":"related = corrMat[corrMat.abs() >= 0.6]\nplt.figure(figsize = (20,15))\nmask = np.triu(np.ones_like(related, dtype = bool))\nsns.heatmap(related, annot = True, square = True, mask = mask)","62fb6316":"removables = ['GarageYrBlt','GarageCars','TotRmsAbvGrd',\n              '1stFlrSF', '2ndFlrSF','GrLivArea']\ncategorical_col.remove('GarageCars')","76852c7f":"plt.figure(figsize = (5,15))\nS = corrMat[['SalePrice']]\nsns.heatmap(S, annot = True)","05a9ac32":"lowCorrFeatures = []\nfor col in corrMat.index:\n    if (col in numerical_col) & (-0.1 <= corrMat[col]['SalePrice'] <= 0.1):\n        lowCorrFeatures.append(col)\nprint(lowCorrFeatures)","bbde713b":"#removing all the removable predictors from dataframe\ntrain_df.drop(removables, axis = 1, inplace = True)\ntest_df.drop(removables, axis = 1, inplace = True)\ntrain_df.drop(lowCorrFeatures, axis = 1, inplace = True)\ntest_df.drop(lowCorrFeatures, axis = 1, inplace = True)\n\ntrain_df.shape","546e9fb5":"from sklearn.model_selection import train_test_split\n\nX = train_df.copy()\ny = X.SalePrice\nX.drop('SalePrice', axis = 1, inplace = True)\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size = 0.2, \n                                                     random_state = 42)\nX_test = test_df.copy()\n# print(X_train.shape, y_train.shape) -> (1168, 70) (1168,)","75821b85":"missing_val_cols = X_train.isnull().sum().sort_values(ascending = False)\nmissing_val_cols = missing_val_cols[missing_val_cols > 0]\nratio_of_missing = missing_val_cols \/ X_train.shape[0]\nmissing_cols = pd.concat([missing_val_cols, ratio_of_missing * 100], axis = 1,\n                        keys = ['Count','%'])\nmissing_cols","f27ef089":"# For 'Electrical', 'MasVnrArea' and 'MasVnrType', removing the entries with missing values.\ny_train.drop(X_train.loc[X_train.Electrical.isnull()].index, axis = 0, inplace = True)\nX_train.drop(X_train.loc[X_train.Electrical.isnull()].index, axis = 0, inplace = True)\n#y_train.drop(X_train.loc[X_train.MasVnrArea.isnull()].index, axis = 0, inplace = True)\n#X_train.drop(X_train.loc[X_train.MasVnrArea.isnull()].index, axis = 0, inplace = True)\n#y_train.drop(X_train.loc[X_train.MasVnrType.isnull()].index, axis = 0, inplace = True)\n#X_train.drop(X_train.loc[X_train.MasVnrType.isnull()].index, axis = 0, inplace = True)\n\n\n# For The following features, it's apparent that a missing entry is a direct indication \n# of absence of the corresponding functionality. Thus, treating NaN as a separate \n# class 'missing' would be wise.\nX_train['BsmtQual'] = X_train['BsmtQual'].fillna('missing')\nX_train['BsmtCond'] = X_train['BsmtCond'].fillna('missing')\nX_train['BsmtExposure'] = X_train['BsmtExposure'].fillna('missing')\nX_train['BsmtFinType1'] = X_train['BsmtFinType1'].fillna('missing')\nX_train['BsmtFinType2'] = X_train['BsmtFinType2'].fillna('missing')\n\nX_train['GarageCond'] = X_train['GarageCond'].fillna('missing')\nX_train['GarageQual'] = X_train['GarageQual'].fillna('missing')\nX_train['GarageFinish'] = X_train['GarageFinish'].fillna('missing')\nX_train['GarageType'] = X_train['GarageType'].fillna('missing')\n\nX_train['MasVnrArea'] = X_train['MasVnrArea'].fillna(0)\nX_train['MasVnrType'] = X_train['MasVnrType'].fillna('missing')\n\nX_train['FireplaceQu'] = X_train['FireplaceQu'].fillna('missing')\n\n# Same for test set\nX_test['BsmtQual'] = X_test['BsmtQual'].fillna('missing')\nX_test['BsmtCond'] = X_test['BsmtCond'].fillna('missing')\nX_test['BsmtExposure'] = X_test['BsmtExposure'].fillna('missing')\nX_test['BsmtFinType1'] = X_test['BsmtFinType1'].fillna('missing')\nX_test['BsmtFinType2'] = X_test['BsmtFinType2'].fillna('missing')\n\nX_test['GarageCond'] = X_test['GarageCond'].fillna('missing')\nX_test['GarageQual'] = X_test['GarageQual'].fillna('missing')\nX_test['GarageFinish'] = X_test['GarageFinish'].fillna('missing')\nX_test['GarageType'] = X_test['GarageType'].fillna('missing')\n\nX_test['Electrical'] = X_test['Electrical'].fillna('SBrkr')\nX_test['MasVnrArea'] = X_test['MasVnrArea'].fillna(0)\nX_test['MasVnrType'] = X_test['MasVnrType'].fillna('missing')\n\nX_test['FireplaceQu'] = X_test['FireplaceQu'].fillna('missing')\n\n# For GarageCond, rows with missing field overlaps with rows having missing fields for\n# other Garage related features. Same goes for BsmtCond. Thus, by deleting just these rows \n# we can effectively deal with all related missing entries.\n#X_train.drop(X_train.loc[X_train.GarageCond.isnull()].index, axis = 0, inplace = True)\n#X_train.drop(X_train.loc[X_train.BsmtCond.isnull()].index, axis = 0, inplace = True)\n#garage_removables = ['GarageCond', 'GarageType', 'GarageFinish', 'GarageQual']\n#removables.append(garage_removables)\n#X_train.drop(garage_removables, axis = 1, inplace = True)\n#X_valid.drop(garage_removables, axis = 1, inplace = True)","26ee6b43":"print(X_train.shape)\nprint(y_train.shape)","234490e0":"full_train = pd.concat([X_train,y_train], axis = 1)\nplt.figure(figsize = (12,6))\nsns.lmplot(x = 'LotFrontage', y = 'SalePrice', data = full_train)\nplt.show()","d792808b":"# Encoding categorical columns from X_train and storing in new dataframe. \ntemp_df = pd.get_dummies(X_train, columns = categorical_col, drop_first = True)\n\n# Dataframe consisting of non-null values of LotFrontage. This will serve as training set.\ndf_non_nulls = temp_df[temp_df['LotFrontage'].notnull()].copy()\nlot_y = df_non_nulls.LotFrontage.copy()\nlot_X = df_non_nulls.drop('LotFrontage', axis = 1, inplace = False)\n\nlot_x_train, lot_x_valid, lot_y_train, lot_y_valid = train_test_split(lot_X, lot_y,\n                                                                     test_size = 0.2,\n                                                                     random_state = 42)\n\n# Fitting model on the training set with target LotFrontage.\nfrom sklearn.ensemble import RandomForestRegressor\n\nrfr = RandomForestRegressor(max_depth = 25, min_samples_leaf = 2, random_state = 0)\nrfr.fit(lot_x_train,lot_y_train)\nscore = rfr.score(lot_x_valid,lot_y_valid)\n#print(score) -> 0.6335685958712058\n\n# Dataframe consisting of null values of LotFrontage. This is our test set as here we\n# don't know a single value of LotFrontage.\ndf_nulls = temp_df[temp_df['LotFrontage'].isnull()].copy()\ndf_nulls.drop('LotFrontage', axis = 1, inplace = True)\ndf_nulls['LotFrontage'] = rfr.predict(df_nulls)\n\n# Now copying the predicted values of LotFrontage back to the original training set.\nfor idx in df_nulls.index:\n    X_train.loc[X_train.index == idx, 'LotFrontage'] = df_nulls.loc[idx]['LotFrontage']","9f6bfa5e":"X_train.isnull().any().any()","7aca7695":"missing_val_cols_valid = X_valid.isnull().sum().sort_values(ascending = False)\nmissing_val_cols_valid = missing_val_cols_valid[missing_val_cols_valid > 0]\nratio_of_missing_valid = missing_val_cols_valid \/ X_valid.shape[0]\nmissing_cols_valid = pd.concat([missing_val_cols_valid, ratio_of_missing_valid * 100], axis = 1,\n                        keys = ['Count','%'])\nmissing_cols_valid","b487a2de":"y_valid.drop(X_valid.loc[X_valid.MasVnrArea.isnull()].index, axis = 0, inplace = True)\nX_valid.drop(X_valid.loc[X_valid.MasVnrArea.isnull()].index, axis = 0, inplace = True)\ny_valid.drop(X_valid.loc[X_valid.MasVnrType.isnull()].index, axis = 0, inplace = True)\nX_valid.drop(X_valid.loc[X_valid.MasVnrType.isnull()].index, axis = 0, inplace = True)\n\nX_valid['BsmtQual'] = X_valid['BsmtQual'].fillna('missing')\nX_valid['BsmtCond'] = X_valid['BsmtCond'].fillna('missing')\nX_valid['BsmtExposure'] = X_valid['BsmtExposure'].fillna('missing')\nX_valid['BsmtFinType1'] = X_valid['BsmtFinType1'].fillna('missing')\nX_valid['BsmtFinType2'] = X_valid['BsmtFinType2'].fillna('missing')\n\nX_valid['GarageCond'] = X_valid['GarageCond'].fillna('missing')\nX_valid['GarageQual'] = X_valid['GarageQual'].fillna('missing')\nX_valid['GarageFinish'] = X_valid['GarageFinish'].fillna('missing')\nX_valid['GarageType'] = X_valid['GarageType'].fillna('missing')\n\nX_valid['FireplaceQu'] = X_valid['FireplaceQu'].fillna('missing')","05c254cc":"temp_df_valid = pd.get_dummies(X_valid, columns = categorical_col, drop_first = True)\ntemp_df, temp_df_valid = temp_df.align(temp_df_valid, join = 'left', axis = 1)\n\ndf_nulls_valid = temp_df_valid[temp_df_valid.LotFrontage.isnull()].copy()\ndf_nulls_valid.drop('LotFrontage', axis = 1, inplace = True)\ndf_nulls_valid = df_nulls_valid.fillna(0)\n\ndf_nulls_valid['LotFrontage'] = rfr.predict(df_nulls_valid)\n\nfor idx in df_nulls_valid.index:\n    X_valid.loc[X_valid.index == idx, 'LotFrontage'] = df_nulls_valid.loc[idx]['LotFrontage']\n    \n\n# Same for test set\ntemp_df_test = pd.get_dummies(X_test, columns = categorical_col, drop_first = True)\ntemp_df, temp_df_test = temp_df.align(temp_df_test, join = 'left', axis = 1)\n\ndf_nulls_test = temp_df_test[temp_df_test.LotFrontage.isnull()].copy()\ndf_nulls_test.drop('LotFrontage', axis = 1, inplace = True)\ndf_nulls_test = df_nulls_test.fillna(0)\n\ndf_nulls_test['LotFrontage'] = rfr.predict(df_nulls_test)\n\nfor idx in df_nulls_test.index:\n    X_test.loc[X_test.index == idx, 'LotFrontage'] = df_nulls_test.loc[idx]['LotFrontage']","3f2955f5":"X_valid.isnull().any().any()","927b9f52":"print(categorical_col)","c53b4f1a":"numerical_col.append(['OverallQual','OverallCond'])\ncategorical_col.remove('OverallQual')\ncategorical_col.remove('OverallCond')\n\n# Updated list of categorical variables:\nprint(categorical_col)","e930df4a":"ordinal_variables = ['LotShape','Utilities','LandSlope','ExterQual','ExterCond',\n                    'BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2',\n                    'HeatingQC','KitchenQual','Functional','FireplaceQu',\n                    'GarageFinish','GarageQual','GarageCond','PavedDrive']\nnominal_variables = list(set(categorical_col) - set(ordinal_variables))\n\nfrom category_encoders.ordinal import OrdinalEncoder\noe = OrdinalEncoder(cols = ordinal_variables, mapping = [\n    {'col':'LotShape','mapping':{'IR3':0,'IR2':1,'IR1':2,'Reg':3}},\n    {'col':'Utilities','mapping':{'ELO':0,'NoSeWa':1,'NoSeWr':2,'AllPub':3}},\n    {'col':'LandSlope','mapping':{'Sev':0,'Mod':1,'Gtl':2}},\n    {'col':'ExterQual','mapping':{'Po':0,'Fa':1,'TA':2,'Gd':3,'Ex':4}},\n    {'col':'ExterCond','mapping':{'Po':0,'Fa':1,'TA':2,'Gd':3,'Ex':4}},\n    {'col':'BsmtQual','mapping':{'missing':0,'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex':5}},\n    {'col':'BsmtCond','mapping':{'missing':0,'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex':5}},\n    {'col':'BsmtExposure','mapping':{'missing':0,'No':1,'Mn':2,'Av':3,'Gd':4}},\n    {'col':'BsmtFinType1','mapping':{'missing':0,'Unf':1,'LwQ':2,'Rec':3,'BLQ':4,'ALQ':5,'GLQ':6}},\n    {'col':'BsmtFinType2','mapping':{'missing':0,'Unf':1,'LwQ':2,'Rec':3,'BLQ':4,'ALQ':5,'GLQ':6}},\n    {'col':'HeatingQC','mapping':{'Po':0,'Fa':1,'TA':2,'Gd':3,'Ex':4}},\n    {'col':'KitchenQual','mapping':{'Po':0,'Fa':1,'TA':2,'Gd':3,'Ex':4}},\n    {'col':'Functional','mapping':{'Sal':0,'Sev':1,'Maj2':2,'Maj1':3,'Mod':4,'Min2':5,'Min1':6,'Typ':7}},\n    {'col':'FireplaceQu','mapping':{'missing':0,'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex':5}},\n    {'col':'GarageFinish','mapping':{'missing':0,'Unf':1,'Rfn':2,'Fin':3}},\n    {'col':'GarageQual','mapping':{'missing':0,'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex':5}},\n    {'col':'GarageCond','mapping':{'missing':0,'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex':5}},\n    {'col':'PavedDrive','mapping':{'N':0,'P':1,'Y':2}}\n])\n\nX_train = oe.fit_transform(X_train)\nX_valid = oe.transform(X_valid)\nX_test = oe.transform(X_test)\n\n# print(X_train.shape) -> (1161, 70)\n# print(X_valid.shape) -> (290, 70)","cb51e3cb":"X_train = pd.get_dummies(X_train)\nX_valid = pd.get_dummies(X_valid)\nX_test = pd.get_dummies(X_test)\nX_train, X_valid = X_train.align(X_valid, join = 'left', axis = 1)\nX_train, X_test = X_train.align(X_test, join = 'left', axis = 1)\n\n# print(X_train.shape) -> (1161, 208)\n# print(X_valid.shape) -> (290, 208)","12a30728":"from sklearn.preprocessing import StandardScaler, RobustScaler\n\nscaler = RobustScaler() #To make dataset robust to outliers.\nrobust_X_train = scaler.fit_transform(X_train)\nrobust_X_valid = scaler.transform(X_valid)\nrobust_X_test = scaler.transform(X_test)\n\nsc = StandardScaler()\nscaled_X_train = sc.fit_transform(robust_X_train)\nscaled_X_valid = sc.transform(robust_X_valid)\nscaled_X_test = sc.transform(robust_X_test)","c1720eba":"final_X_train = pd.DataFrame(scaled_X_train, index = X_train.index, \n                             columns = X_train.columns)\nfinal_X_valid = pd.DataFrame(scaled_X_valid, index = X_valid.index,\n                             columns = X_valid.columns)\nfinal_X_test = pd.DataFrame(scaled_X_test, index = X_test.index,\n                            columns = X_test.columns)\n\nfinal_X_train.describe()","7e2ec571":"from sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.linear_model import Ridge\n\n# For parameter selection and comparison, use RandomizedSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV","bcfffefc":"dt = DecisionTreeRegressor()\nrfr = RandomForestRegressor()\nada = AdaBoostRegressor()\nxgb = XGBRegressor()\nknn = KNeighborsRegressor()\nrdg = Ridge()","58ad74cc":"from sklearn.metrics import make_scorer, mean_absolute_error, mean_squared_error\nfrom scipy.stats import uniform, randint\n\nscorer = make_scorer(mean_absolute_error) # Mean Absolute Error as scorer\n\n# # # # # # # # Decision Tree # # # # # # # #\ndt_params = {'criterion' : ['mae','friedman_mse'],\n            'splitter' : ['best', 'random'],\n            'max_depth' : np.arange(1,50),\n            'max_features' : ['auto','log2','sqrt'],\n            'min_samples_split': [2, 5, 10],\n            'min_samples_leaf' : [2, 3, 4, 5, 10, 15, 20]}\ndt_rs = RandomizedSearchCV(dt, param_distributions = dt_params,\n                          scoring = scorer, cv = 5)\n\n# # # # # # # # Random Forest # # # # # # # #\nrfr_params = {'criterion' : ['mse', 'mae'],\n              'n_estimators' : [50, 100, 200, 400, 500],\n              'max_depth' : np.arange(1,50),\n              'max_features' : ['auto','log2','sqrt'],\n              'min_samples_split': [2, 5, 10],\n              'min_samples_leaf' : [2, 3, 4, 5, 10, 20, 30, 40, 50]}\nrfr_rs = RandomizedSearchCV(rfr, param_distributions = rfr_params, \n                            scoring = scorer, cv = 5)\n\n# # # # # # # # ADA Boost # # # # # # # #\nada_params = {'n_estimators' : [50, 100, 200, 400, 500],\n              'learning_rate' : np.arange(0,1,0.01),\n              'loss' : ['linear', 'square']}\nada_rs = RandomizedSearchCV(ada, param_distributions = ada_params, \n                            scoring = scorer, cv = 5)\n\n# # # # # # # #  XG Boost # # # # # # # #\nxgb_params = {'n_estimators' : [50, 100, 200, 300, 400, 500],\n              'learning_rate' : np.arange(0,1,0.01),\n              'min_child_weight' : np.arange(1,10)}\nxgb_rs = RandomizedSearchCV(xgb, param_distributions = xgb_params, \n                            scoring = scorer, cv = 5)\n\n# # # # # # # #  K-Nearest Neighbors # # # # # # # #\nknn_params = {'n_neighbors' : np.arange(1,50),\n              'weights' : ['uniform', 'distance'],\n              'algorithm' : ['auto', 'ball_tree', 'kd_tree']}\nknn_rs = RandomizedSearchCV(knn, param_distributions = knn_params, \n                            scoring = scorer, cv = 5)\n\n# # # # # # # #  Ridge # # # # # # # #\nrdg_params = {'alpha' : np.arange(0,1,0.01),\n              'solver' : ['auto', 'lsqr', 'saga']}\nrdg_rs = RandomizedSearchCV(rdg, param_distributions = rdg_params, \n                            scoring = scorer, cv = 5)","0a004d94":"# # # # # # # # FITTING # # # # # # # #\ndt_rs.fit(final_X_train, y_train)\nrfr_rs.fit(final_X_train, y_train)\nada_rs.fit(final_X_train, y_train)\nxgb_rs.fit(final_X_train, y_train)\nknn_rs.fit(final_X_train, y_train)\nrdg_rs.fit(final_X_train, y_train)\n\n# # # # # # # # EVALUATION # # # # # # # # \nprint('Decision Tree best parameters:', dt_rs.best_params_)\nprint('Random Forest best parameters:', rfr_rs.best_params_)\nprint('Ada Boost best parameters:', ada_rs.best_params_)\nprint('XGB best parameters:', xgb_rs.best_params_)\nprint('KNN best parameters:', knn_rs.best_params_)\nprint('Ridge best parameters:', rdg_rs.best_params_)","e872a25b":"# # # # # # # # Decision Tree # # # # # # # #\ndt = DecisionTreeRegressor(criterion = dt_rs.best_params_['criterion'], \n                           splitter = dt_rs.best_params_['splitter'], \n                           max_depth = dt_rs.best_params_['max_depth'],\n                           max_features = dt_rs.best_params_['max_features'], \n                           min_samples_split = dt_rs.best_params_['min_samples_split'],\n                           min_samples_leaf = dt_rs.best_params_['min_samples_leaf'], \n                           random_state = 42)\n\n# # # # # # # # Random Forest # # # # # # # #\nrfr = RandomForestRegressor(criterion = rfr_rs.best_params_['criterion'], \n                            n_estimators = rfr_rs.best_params_['n_estimators'],  \n                            max_depth = rfr_rs.best_params_['max_depth'],\n                            max_features = rfr_rs.best_params_['max_depth'],\n                            min_samples_split = rfr_rs.best_params_['min_samples_split'], \n                            min_samples_leaf = rfr_rs.best_params_['min_samples_leaf'], \n                            random_state = 42)\n\n# # # # # # # # ADA Boost # # # # # # # #\nada = AdaBoostRegressor(n_estimators = ada_rs.best_params_['n_estimators'], \n                        learning_rate = ada_rs.best_params_['learning_rate'], \n                        loss = ada_rs.best_params_['loss'],\n                        random_state = 42)\n\n# # # # # # # #  XG Boost # # # # # # # #\nxgb = XGBRegressor(n_estimators = xgb_rs.best_params_['n_estimators'], \n                    learning_rate = xgb_rs.best_params_['learning_rate'], \n                    min_child_weight = xgb_rs.best_params_['min_child_weight'],\n                    random_state = 42, eval_metric = 'logloss')\n\n# # # # # # # #  K-Nearest Neighbors # # # # # # # #\nknn = KNeighborsRegressor(n_neighbors=  knn_rs.best_params_['n_neighbors'], \n                          weights = knn_rs.best_params_['weights'],\n                          algorithm = knn_rs.best_params_['algorithm'])\n\n# # # # # # # #  Ridge # # # # # # # #\nrdg = Ridge(alpha = rdg_rs.best_params_['alpha'],\n            solver = rdg_rs.best_params_['solver'])\n\n\n\nmodels = [(dt, 'Decision Tree'), (rfr, 'Random Forest'), \n          (ada, 'Ada Boost'), (xgb, 'XG Boost'), \n          (knn, 'K Neighbors'), (rdg, 'Ridge')]","2a9f1287":"final_X_valid = final_X_valid.fillna(0)\nfinal_X_test = final_X_test.fillna(0)","93fbdfb9":"#dataframe to keep track of scores of various models\nevaluations = pd.DataFrame({'Model' : [], 'MAE' : [], \n                            'MSE' : [], 'RMSE' : []})\n\n#function that evaluates and returns different scores obtained by a model\ndef evaluate(actual, preds):\n    mae = mean_absolute_error(actual, preds)\n    mse = mean_squared_error(actual, preds)\n    rmse  = mean_squared_error(actual, preds, squared = False)\n    return (mae, mse, rmse)\n\n# Fitting and evaluating the models one by one\nfor model, model_name in models:\n    model.fit(final_X_train, y_train)\n    preds = model.predict(final_X_valid)\n    mae, mse, rmse = evaluate(y_valid, preds)\n    cur_model = {'Model' : model_name, 'MAE' : mae, \n                 'MSE' : mse, 'RMSE' : rmse}\n    evaluations = evaluations.append(cur_model, ignore_index = True)\n    #print('Model: {} f1: {:.3f} accuracy: {:.3f}'.format(model_name, f1, accuracy))\n    \nevaluations.set_index('Model', inplace = True)\nevaluations\n","37320f21":"final_X_test.shape","b22f1d99":"# make predictions\ntest_predictions = xgb.predict(final_X_test)\n\n# convert to csv\noutput = pd.DataFrame({'Id' : final_X_test.index, 'SalePrice' : test_predictions})\noutput.to_csv('submission.csv', index = False)","cae78402":"Classifying columns as *categorical* and *numerical*:","eb7296a4":"There are a total of *1460* training examples, having 80 features and 1 target column, i.e. *SalePrice*.","48711080":"Out of these, there are many ordinal variables and many nominal ones. So, appropriate encoding techniques need to be used.\n\n**FOR ORDINAL VARIABLES:**\n    First off, we have to hand pick all ordinal variables. For that, examining the *data_description.txt* file would help.","c7ed57df":"A lot of missing values in *Alley*, *PoolQC*, *Fence*, and *MiscFeature*. Thus, it would be reasonable to remove these features.","ff0540ab":"After tuning the best suitable parameters to each and every model, we will now measure how well these models do with respect to some scoring criterias.","094ac510":"# **IMPORT LIBRARIES**","a4dc815b":"# **4. FEATURE SCALING**","f462ae56":"This concludes the part of dealing with missing values. (**Finally!!**)","60bc75a3":"**1.3 CORRELATION ANALYSIS**","18481772":"On passing through the Scalers, our Data Frame has now been converted to a numpy array. So, for convention, we will convert the array back to a Data Frame.","58b95441":"Next, we will use *RandomizedSearchCV* to define and test different combinations of parameters. This is an important step as many a times simply hypertuning the parameters can drastically improve the model performance.","525a02a1":"# **1. EXPLORATORY DATA ANALYSIS**","564819f7":"As is obvious from the above plot, there is plenty positive correlation between the two.\n\nTherefore, we need to proceed with caution while dealing with this feature.\nOne way to deal with the situation is to build a model that predicts the missing values of *LotFrontage*. Here, we treat *LotFrontage* as the dependent variable while the other features would be independent variables.","fc6e8657":"First of all, determine the amount and type of missing data in the given dataframe.","93b02e1a":"Firstly, let us look at the categorical features we have:","bb208bbd":"There are many rating variables (i.e. 1-10), which will be better off in the numerical variables set.","ee61fc96":"From the above plot:\n\n**1.** Pairs *YearBuilt* & *GarageYrBlt*, *GarageArea* & *GarageCars*, *TotRmsAbvGrd* & *GrLivArea*, *2ndFlrSF* & *GrLivArea*, *1stFlrSF* & *TotalBsmtSF*, and *TotalSF* & <*TotalBsmtSF*, *1stFlrSF*, *GrLivArea*> have high correlations and hence one from each pair can be removed. Say, we will remove *GarageYrBlt*, *GarageCars*, *TotRmsAbvGrd*, *1stFlrSF*, *2ndFlrSF*, and *GrLivArea*.\n\n**2.** Predictors *OverallQual*, *GrLivArea*, *TotalBsmtSF*, and *GarageArea* have high correlation with target *SalePrice*. Hence, these are our important features.","87cbfc42":"**FOR NOMINAL VARIABLES:** For these, we will use the good ol' one hot encoding technique.","bc803b8e":"Let's go with the following rules of thumb:\n\n**1.** Any two predictor variables having correlation coefficient greater than or equal to 0.7 can be said to have high positive correlation, and thus, any one of the two could be removed.\n\n**2.** Same goes for pairs of predictor variables having correlation coefficient less than or equal to -0.7.\n\n**3.** Predictor variables having correlation greater than 0.6 or less than -0.6 with the target *SalePrice* can be labeled as important predictors\/features.","8f20c37c":"> if any other feature whose value would not actually be available in practice at the time you\u2019d want to use the model to make a prediction, is a feature that can introduce leakage to your model","c430f4f3":"And we're done! The output file is now ready for submission.","a93e7597":"# **IMPORT DATASET**","75ce7d41":"Applying the same imputation rules as the training set:","6491343f":"**1.2 FEATURE ENGINEERING**\n\nGenerally, when we go to buy a house, we are generally concerned with two most important features, i.e. *total surface area* and *age of the building*. Therefore, we will add these two predictors before proceeding further.","66c4b374":"It goes without saying that for a dataset having 200+ features, we have to employ complex curve fitting regression techniques. Some regression techniques we are going to use are:\n\n1. Decision Tree Regressor\n2. Random Forest Regressor\n3. XG Boost Regressor\n4. Ada Boost Regressor\n5. K-Nearest-Neighbors Regressor\n6. Ridge Regressor","5bb03a6c":"**1.1 OUTLIERS**\n\nThe famous dataset consists of a few major outliers, which we will remove at the very beginning to ensure that the statistical measures aren't affected by these outliers.","8455ab59":"Many machine learning algorithms perform better when numerical input variables are scaled to a standard range.\n\nStandardizing is a popular scaling technique that subtracts the mean from values and divides by the standard deviation, transforming the probability distribution for an input variable to a standard Gaussian (zero mean and unit variance). Standardization can become skewed or biased if the input variable contains outlier values.\n\nTo overcome this, the median and interquartile range can be used when standardizing numerical input variables, generally referred to as robust scaling.","714e8f6e":"# **5. MODEL FITTING AND EVALUATION**","6b8e756a":"After analysing the effects of the outliers on the results of a simple Random Forest Regressor model, two cases stood out which deteriorated model performance:\n\n1. *GrLivArea* > 4000 and *SalePrice* < 200000\n2. *OverallQual* > 9 and *SalePrice* < 300000","9aa69aec":"Now that we have our models and their hyperparameter combinations defined, let's fit each model on the training set and evaluate their scores.","8241df60":"This gives us the best set of parameters correspoding to each model. \n\nNext, we have to tune the optimized parameters onto the respective models:","d5d3c17d":"Now, we are going to look for predictors that have very low correlation with the target *SalePrice*, i.e. features that have little to no effect on our target variable.\nFor this, we will look into our correlation matrix and extract only those features that have correlation coefficient value between -0.1 and 0.1 with the target *SalePrice*.\n\n**Note:** Dropping features based on their feature-to-target-correlation essentially is a form of feature filtering. It is important to understand that feature filtering does not necessarily improve predictive performance. This is because for complex regression models, these features might get coupled with other features and contribute to the predictive performance of the model. \n\nFor the above reason, we are not going to remove these features from the training set just yet. Instead, we will observe how a particular model would perform with and without these features. Then we can take that result to determine whether or not to keep these features.","de09d766":"With this, we are now done with data anaytics. ","886fa20b":"However, owing to very few number of houses with a pool, *PoolArea* got wrongly classified as a categorical feature.","e3124bfb":"# **3. ENCODING**","e4a3099b":"# **2. DATA CLEANING**","b09a9783":"Out of all the models, *Ridge* has shown the best results. Thus, we'll base our final test predictions on the rdg model.","744bd8eb":"Before we jump into data cleaning, it is good practice to first split up the dataset into train and validation sets. This is to ensure that data leakage doesn't seep into our model.","774db5d5":"# **6. PREDICTION**\n\nWith everything prepared, now we just have to predict target value in the test set based on the model we'd fit on the training set.","5035b14e":"For *LotFrontage*:\n\n1. Dropping the column is out the question as in the actual market scenario, measure of linear feet of street connected to the property has significant effect on the price.\n2. For this exact reason, imputing the missing values would result in nonsensical data.\n3. Deleting rows with missing field would mean throwing away about 20% of the training dataset.\n\nWe can further confirm the effect of *LotFrontage* on target variable with the help of the following plot:"}}