{"cell_type":{"c2a3aa30":"code","7ce5c350":"code","15a67785":"code","ea85d428":"code","5962e7c2":"code","ec71ce16":"code","37c4aa99":"code","ca9ad643":"code","b9e8fda0":"code","67ae7bc7":"code","5bd25b53":"code","d84a502c":"code","e911696f":"code","89c5b8b8":"code","62b1526c":"code","6b3c5189":"code","a1a18b24":"code","03b0efb3":"code","67215f77":"code","41d51825":"code","51ba7ed3":"code","8f8b8b75":"code","4acf31da":"code","52f7b857":"code","e03a689d":"code","94e0f302":"code","c02c8ad0":"code","177fb754":"code","7baf2614":"code","25849200":"code","9413d096":"code","836f191c":"code","a0885f96":"code","0d989fc5":"code","968e0633":"code","5f5221a4":"code","f87d10ae":"code","7eeb0882":"code","c67e9a3a":"code","618ae2f0":"code","1ae46e7a":"code","90209edb":"code","ee237fc7":"code","d72359fc":"code","0ef2f409":"code","bad62fe6":"code","afe22ae8":"code","e45b2cd0":"code","07576d38":"markdown","2e020038":"markdown","e6698c47":"markdown","b9e20699":"markdown","e3f02d03":"markdown","f009ee0f":"markdown","6cae7fd7":"markdown","b2895326":"markdown","efb1a86c":"markdown","5b52e86b":"markdown","ccc00f0d":"markdown","a9e8969d":"markdown","95369b9d":"markdown","46f7aab5":"markdown","bc9ccd11":"markdown","835173e9":"markdown","4b20b0b9":"markdown","2f6cb8be":"markdown","62fa18f3":"markdown","71bbcb49":"markdown","f15ff9a6":"markdown","0e506898":"markdown","7c2fefb0":"markdown","78fdfb9d":"markdown","ba96bc07":"markdown","976695b2":"markdown","d0581f83":"markdown","032748f9":"markdown","1b35fd82":"markdown","c437127b":"markdown","7d13579f":"markdown","11d61ce8":"markdown","8bab21c9":"markdown"},"source":{"c2a3aa30":"import math\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt","7ce5c350":"train = pd.read_csv('..\/input\/pokemon-datasets-for-ml\/train_pokemon.csv')\ntest = pd.read_csv('..\/input\/pokemon-datasets-for-ml\/test_pokemon.csv')","15a67785":"train.head(3)","ea85d428":"train.shape","5962e7c2":"train.describe()","ec71ce16":"# describe(include = ['O']) will show the descriptive statistics of object data types.\ntrain.describe(include=['O'])","37c4aa99":"# check for missing values\nsns.heatmap(train.isnull(),yticklabels=False,cbar=False,cmap='viridis')","ca9ad643":"def fill_type_2(cols):\n    type_2 = cols[0]\n    if pd.isnull(type_2):\n        return \"None\"\n    else:\n        return type_2","b9e8fda0":"train['Type_2'] = train[['Type_2']].apply(fill_type_2,axis=1)","67ae7bc7":"train.drop(columns=['Egg_Group_2'], inplace=True)","5bd25b53":"sns.heatmap(train.isnull(),yticklabels=False,cbar=False,cmap='viridis')","d84a502c":"legendary = train[train['isLegendary'] == 1]\nnot_legendary = train[train['isLegendary'] == 0]\n\nprint(\"Legendary: %i (%.1f%%)\"%(len(legendary), float(len(legendary))\/len(train)*100.0))\nprint(\"Not Legendary: %i (%.1f%%)\"%(len(not_legendary), float(len(not_legendary))\/len(train)*100.0))\nprint(\"Total: %i\"%len(train))","e911696f":"train.columns","89c5b8b8":"plt.figure(figsize=(25,10))\ntrain2 = train.drop(['Number','Name','hasGender','shuffle'], axis=1)\nsns.heatmap(train2.corr(), vmin= -1, vmax=1, square=True, annot=True)","62b1526c":"#boxplot of Attack vs. Legendary\nplt.figure(figsize=(8, 4))\nsns.boxplot(x='isLegendary',y='Attack',data=train, palette='rainbow')\n\n#stripplot of Attack vs. Legendary\nplt.figure(figsize=(15, 4))\nsns.stripplot(x='Type_1',y='Total',data=train, jitter=True,hue='isLegendary',palette=['r','b'],dodge=False).set_title('Type_1 Distribution on Legendary')\n\n#stripplot of Attack vs. Legendary\nplt.figure(figsize=(15, 4))\nsns.stripplot(x='Type_2',y='Total',data=train, jitter=True,hue='isLegendary',palette=['r','b'],dodge=False).set_title('Type_2 Distribution on Legendary')","6b3c5189":"type_1 = train[['Type_1','isLegendary']].groupby(['Type_1'], as_index=False).mean().set_index('Type_1')\ntype_1.sort_values(by='isLegendary',ascending=False).plot(kind='bar')","a1a18b24":"type_2 = train[['Type_2','isLegendary']].groupby(['Type_2'], as_index=False).mean().set_index('Type_2')\ntype_2.sort_values(by='isLegendary',ascending=False).plot(kind='bar')","03b0efb3":"train_test_data = [train, test]","67215f77":"for dataset in train_test_data:\n    dataset['isLegendary'] = dataset['isLegendary'].map({True: 1, False: 0}).astype(int)","41d51825":"type_1.sort_values(by='isLegendary',ascending=False)","51ba7ed3":"type_1_mapping = {\"Fire\": 1, \"Dragon\": 2, \"Electric\": 3, \"Fighting\": 4, \"Ice\": 5, \"Flying\": 6, \"Water\": 7, \"Ghost\": 8, \"Steel\": 9, \"None\": 10, \"Fairy\": 11, \"Psychic\": 12, \"Ground\": 13, \"Rock\": 14, \"Bug\": 15, \"Poison\": 16, \"Normal\": 17, \"Dark\": 18, \"Grass\": 19}\nfor dataset in train_test_data:\n    dataset['Type_1'] = dataset['Type_1'].map(type_1_mapping)\n    dataset['Type_1'] = dataset['Type_1'].fillna(0)","8f8b8b75":"type_2_mapping = {\"Fire\": 1, \"Dragon\": 2, \"Electric\": 3, \"Fighting\": 4, \"Ice\": 5, \"Flying\": 6, \"Water\": 7, \"Ghost\": 8, \"Steel\": 9, \"None\": 10, \"Fairy\": 11, \"Psychic\": 12, \"Ground\": 13, \"Rock\": 14, \"Bug\": 15, \"Poison\": 16, \"Normal\": 17, \"Dark\": 18, \"Grass\": 19}\nfor dataset in train_test_data:\n    dataset['Type_2'] = dataset['Type_2'].map(type_2_mapping)\n    dataset['Type_2'] = dataset['Type_2'].fillna(0)","4acf31da":"for dataset in train_test_data:\n    pr_male_avg = dataset['Pr_Male'].mean()\n    pr_male_std = dataset['Pr_Male'].std()\n    pr_male_null_count = dataset['Pr_Male'].isnull().sum()\n    \n    pr_male_null_random_list = np.random.uniform(pr_male_avg - pr_male_std, pr_male_avg + pr_male_std, pr_male_null_count)\n    dataset['Pr_Male'][np.isnan(dataset['Pr_Male'])] = pr_male_null_random_list\n    dataset['Pr_Male'] = dataset['Pr_Male'].astype(int)\n    \ntrain['Pr_Male_Band'] = pd.cut(train['Pr_Male'], 5)\n\nprint(train[['Pr_Male_Band', 'isLegendary']].groupby(['Pr_Male_Band'], as_index=False).mean())","52f7b857":"for dataset in train_test_data:\n    dataset.loc[ dataset['Pr_Male'] <= 0.2, 'Pr_Male'] = 0\n    dataset.loc[(dataset['Pr_Male'] > 0.2) & (dataset['Pr_Male'] <= 0.4), 'Pr_Male'] = 1\n    dataset.loc[(dataset['Pr_Male'] > 0.4) & (dataset['Pr_Male'] <= 0.6), 'Pr_Male'] = 2\n    dataset.loc[(dataset['Pr_Male'] > 0.6) & (dataset['Pr_Male'] <= 0.8), 'Pr_Male'] = 3\n    dataset.loc[ dataset['Pr_Male'] >= 1, 'Pr_Male'] = 4","e03a689d":"for dataset in train_test_data:\n    attack_avg = dataset['Attack'].mean()\n    attack_std = dataset['Attack'].std()\n    attack_null_count = dataset['Attack'].isnull().sum()\n    \n    attack_null_random_list = np.random.randint(attack_avg - attack_std, attack_avg + attack_std, attack_null_count)\n    dataset['Attack'][np.isnan(dataset['Attack'])] = attack_null_random_list\n    dataset['Attack'] = dataset['Attack'].astype(int)\n    \ntrain['Attack_Band'] = pd.cut(train['Attack'], 5)\n\nprint(train[['Attack_Band', 'isLegendary']].groupby(['Attack_Band'], as_index=False).mean())","94e0f302":"for dataset in train_test_data:\n    dataset.loc[ dataset['Attack'] <= 36, 'Attack'] = 0\n    dataset.loc[(dataset['Attack'] > 36) & (dataset['Attack'] <= 67), 'Attack'] = 1\n    dataset.loc[(dataset['Attack'] > 67) & (dataset['Attack'] <= 98), 'Attack'] = 2\n    dataset.loc[(dataset['Attack'] > 98) & (dataset['Attack'] <= 129), 'Attack'] = 3\n    dataset.loc[ dataset['Attack'] >= 129, 'Attack'] = 4","c02c8ad0":"for dataset in train_test_data:\n    defense_avg = dataset['Defense'].mean()\n    defense_std = dataset['Defense'].std()\n    defense_null_count = dataset['Defense'].isnull().sum()\n    \n    defense_null_random_list = np.random.randint(defense_avg - defense_std, defense_avg + defense_std, defense_null_count)\n    dataset['Defense'][np.isnan(dataset['Defense'])] = defense_null_random_list\n    dataset['Defense'] = dataset['Defense'].astype(int)\n    \ntrain['Defense_Band'] = pd.cut(train['Defense'], 5)\n\nprint(train[['Defense_Band', 'isLegendary']].groupby(['Defense_Band'], as_index=False).mean())","177fb754":"for dataset in train_test_data:\n    dataset.loc[ dataset['Defense'] <= 50, 'Defense'] = 0\n    dataset.loc[(dataset['Defense'] > 50) & (dataset['Defense'] <= 95), 'Defense'] = 1\n    dataset.loc[(dataset['Defense'] > 95) & (dataset['Defense'] <= 140), 'Defense'] = 2\n    dataset.loc[(dataset['Defense'] > 140) & (dataset['Defense'] <= 230), 'Defense'] = 3\n    dataset.loc[ dataset['Defense'] >= 230, 'Defense'] = 4","7baf2614":"for dataset in train_test_data:\n    cr_avg = dataset['Catch_Rate'].mean()\n    cr_std = dataset['Catch_Rate'].std()\n    cr_null_count = dataset['Catch_Rate'].isnull().sum()\n    \n    cr_null_random_list = np.random.randint(cr_avg - cr_std, cr_avg + cr_std, cr_null_count)\n    dataset['Catch_Rate'][np.isnan(dataset['Catch_Rate'])] = cr_null_random_list\n    dataset['Catch_Rate'] = dataset['Catch_Rate'].astype(int)\n    \ntrain['Catch_Rate_Band'] = pd.cut(train['Catch_Rate'], 5)\n\nprint(train[['Catch_Rate_Band', 'isLegendary']].groupby(['Catch_Rate_Band'], as_index=False).mean())","25849200":"for dataset in train_test_data:\n    dataset.loc[ dataset['Catch_Rate'] <= 53, 'Catch_Rate'] = 0\n    dataset.loc[(dataset['Catch_Rate'] > 53) & (dataset['Catch_Rate'] <= 104), 'Catch_Rate'] = 1\n    dataset.loc[(dataset['Catch_Rate'] > 104) & (dataset['Catch_Rate'] <= 154), 'Catch_Rate'] = 2\n    dataset.loc[(dataset['Catch_Rate'] > 154) & (dataset['Catch_Rate'] <= 204), 'Catch_Rate'] = 3\n    dataset.loc[ dataset['Catch_Rate'] >= 255, 'Catch_Rate'] = 4","9413d096":"train.columns","836f191c":"train_drop = ['Number', 'Name', 'Total', 'HP', 'Sp_Atk', 'Sp_Def', 'Speed', 'Generation','Color', 'hasGender', 'Egg_Group_1', 'hasMegaEvolution','Height_m', 'Weight_kg', 'Body_Style', 'shuffle','Pr_Male_Band', 'Attack_Band', 'Defense_Band', 'Catch_Rate_Band']\ntrain = train.drop(train_drop, axis=1)","a0885f96":"train.head()","0d989fc5":"test.head()","968e0633":"test_drop = ['Number', 'Name', 'Total', 'HP', 'Sp_Atk', 'Sp_Def', 'Speed', 'Generation',\n       'Color', 'hasGender', 'Egg_Group_1', 'Egg_Group_2', 'isLegendary',\n       'hasMegaEvolution', 'Height_m', 'Weight_kg', 'Body_Style',\n       'shuffle']\ntest = test.drop(test_drop, axis=1)","5f5221a4":"test.head()","f87d10ae":"X_train = train.drop('isLegendary', axis=1)\ny_train = train['isLegendary']\nX_test = test.copy()\n\nX_train.shape, y_train.shape, X_test.shape","7eeb0882":"# Importing Classifier Modules\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier","c67e9a3a":"clf = LogisticRegression()\nclf.fit(X_train, y_train)\ny_pred_log_reg = clf.predict(X_test)\nacc_log_reg = round( clf.score(X_train, y_train) * 100, 2)\nprint(str(acc_log_reg) + ' percent')","618ae2f0":"clf = SVC()\nclf.fit(X_train, y_train)\ny_pred_svc = clf.predict(X_test)\nacc_svc = round(clf.score(X_train, y_train) * 100, 2)\nprint (acc_svc)","1ae46e7a":"clf = KNeighborsClassifier(n_neighbors = 3)\nclf.fit(X_train, y_train)\ny_pred_knn = clf.predict(X_test)\nacc_knn = round(clf.score(X_train, y_train) * 100, 2)\nprint (acc_knn)","90209edb":"clf = DecisionTreeClassifier()\nclf.fit(X_train, y_train)\ny_pred_decision_tree = clf.predict(X_test)\nacc_decision_tree = round(clf.score(X_train, y_train) * 100, 2)\nprint (acc_decision_tree)","ee237fc7":"clf = RandomForestClassifier(n_estimators=100)\nclf.fit(X_train, y_train)\ny_pred_random_forest = clf.predict(X_test)\nacc_random_forest = round(clf.score(X_train, y_train) * 100, 2)\nprint (acc_random_forest)","d72359fc":"clf = GaussianNB()\nclf.fit(X_train, y_train)\ny_pred_gnb = clf.predict(X_test)\nacc_gnb = round(clf.score(X_train, y_train) * 100, 2)\nprint (acc_gnb)","0ef2f409":"clf = Perceptron(max_iter=5, tol=None)\nclf.fit(X_train, y_train)\ny_pred_perceptron = clf.predict(X_test)\nacc_perceptron = round(clf.score(X_train, y_train) * 100, 2)\nprint (acc_perceptron)","bad62fe6":"clf = SGDClassifier(max_iter=5, tol=None)\nclf.fit(X_train, y_train)\ny_pred_sgd = clf.predict(X_test)\nacc_sgd = round(clf.score(X_train, y_train) * 100, 2)\nprint (acc_sgd)","afe22ae8":"from sklearn.metrics import confusion_matrix\nimport itertools\n\nclf = RandomForestClassifier(n_estimators=100)\nclf.fit(X_train, y_train)\ny_pred_random_forest_training_set = clf.predict(X_train)\nacc_random_forest = round(clf.score(X_train, y_train) * 100, 2)\nprint (\"Accuracy: %i %% \\n\"%acc_random_forest)\n\nclass_names = ['Legendary', 'Not Legendary']\n\n# Compute confusion matrix\ncnf_matrix = confusion_matrix(y_train, y_pred_random_forest_training_set)\nnp.set_printoptions(precision=2)\n\nprint ('Confusion Matrix in Numbers')\nprint (cnf_matrix)\nprint ('')\n\ncnf_matrix_percent = cnf_matrix.astype('float') \/ cnf_matrix.sum(axis=1)[:, np.newaxis]\n\nprint ('Confusion Matrix in Percentage')\nprint (cnf_matrix_percent)\nprint ('')\n\ntrue_class_names = ['True Legendary', 'True Not Legendary']\npredicted_class_names = ['Predicted Legendary', 'Predicted Not Legendary']\n\ndf_cnf_matrix = pd.DataFrame(cnf_matrix, \n                             index = true_class_names,\n                             columns = predicted_class_names)\n\ndf_cnf_matrix_percent = pd.DataFrame(cnf_matrix_percent, \n                                     index = true_class_names,\n                                     columns = predicted_class_names)\n\nplt.figure(figsize = (15,5))\n\nplt.subplot(121)\nsns.heatmap(df_cnf_matrix, annot=True, fmt='d')\n\nplt.subplot(122)\nsns.heatmap(df_cnf_matrix_percent, annot=True)","e45b2cd0":"models = pd.DataFrame({\n    'Model': ['Logistic Regression', 'Support Vector Machines', \n              'KNN', 'Decision Tree', 'Random Forest', 'Naive Bayes', \n              'Perceptron', 'Stochastic Gradient Decent'],\n    \n    'Score': [acc_log_reg, acc_svc, \n              acc_knn,  acc_decision_tree, acc_random_forest, acc_gnb, \n              acc_perceptron, acc_sgd]\n    })\n\nmodels.sort_values(by='Score', ascending=False)","07576d38":"### Stochastic Gradient Descent (SGD)","2e020038":"### Decision Tree","e6698c47":"Like Type_1, `Type_2` can be useful to predict legendary Pokemons.","b9e20699":"### Random Forest","e3f02d03":"Apparently, some feature have no correlation with legendary. These features are `hasMegaEvolution`,`Generation`.","f009ee0f":"# Pokemon","6cae7fd7":"### Feature Selection\n\nWe drop unnecessary columns\/features and keep only the useful ones for our experiment.","b2895326":"### Catch Rate %","efb1a86c":"Because the majority of the `Egg_Group_2` is `NaN`, we will drop from the dataset as it will not be of any help. Then, we will be in a position to start investigating our data.","5b52e86b":"Let's fill the rows in `Type_2` column that are currently null with `None`. ","ccc00f0d":"## Classification & Accuracy\nDefine training and testing set.","a9e8969d":"### Attack & Defense","95369b9d":"## Correlating Features\n\nHeatmap of Correlation between different features:\n   - Positive numbers = Positive correlation, i.e. increase in one feature will increase the other feature & vice-versa.\n   - Negative numbers = Negative correlation, i.e. increase in one feature will decrease the other feature & vice-versa.\n\nIn our case, we focus on which features have strong positive or negative correlation with the Survived feature.","46f7aab5":"## Relationship between Features and Legendary\n\nIn this section, we will analyse the relationship between different features with respect to `isLegendary`.","bc9ccd11":"We are done with Feature Selection\/Engineering. Now, we are ready to train a classifier with our feature set.","835173e9":"Now, we map `Pr_Male` according to `Pr_Male_Band`.","4b20b0b9":"From the above table, we can see that Decision Tree and Random Forest classfiers have the highest accuracy score.\n\nBetween the two, we choose **Random Forest Classifier** as it has the ability to limit overfitting as compared to Decision Tree classifier.","2f6cb8be":"## Prepare data","62fa18f3":"It seems that most legendary Pokemons are also a Flying type, followed by the Dragon type. There are no legendary Poison, Fighting or Bug types. Still, `Type_1` feature can be useful to predict legendary Pokemons.","71bbcb49":"## Feature Extraction\n\nIn this section, we select the appropriate features to train our classifier. Here, we create new features based on existing features. We also convert categorical features into numeric form.","f15ff9a6":"### Type_1 vs Lengendary","0e506898":"## Data content\n\nThis database includes 21 variables per each of the 721 Pok\u00e9mon of the first six generations, plus the Pok\u00e9mon ID and its name. These variables are briefly described next:\n\n    Number. Pok\u00e9mon ID in the Pok\u00e9dex.\n    Name. Name of the Pok\u00e9mon.\n    Type_1. Primary type.\n    Type_2. Second type, in case the Pok\u00e9mon has it.\n    Total. Sum of all the base stats (Health Points, Attack, Defense, Special Attack, Special Defense, and Speed).\n    HP. Base Health Points.\n    Attack. Base Attack.\n    Defense. Base Defense.\n    Sp_Atk. Base Special Attack.\n    Sp_Def. Base Special Defense.\n    Speed. Base Speed.\n    Generation. Number of the generation when the Pok\u00e9mon was introduced.\n    isLegendary. Boolean that indicates whether the Pok\u00e9mon is Legendary or not.\n    Color. Colour of the Pok\u00e9mon according to the Pok\u00e9dex.\n    hasGender. Boolean that indicates if the Pok\u00e9mon can be classified as female or male.\n    Pr_male. In case the Pok\u00e9mon has Gender, the probability of its being male. The probability of being female is, of course, 1 minus this value.\n    EggGroup1. Egg Group of the Pok\u00e9mon.\n    EggGroup2. Second Egg Group of the Pok\u00e9mon, in case it has two.\n    hasMegaEvolution. Boolean that indicates whether the Pok\u00e9mon is able to Mega-evolve or not.\n    Height_m. Height of the Pok\u00e9mon, in meters.\n    Weight_kg. Weight of the Pok\u00e9mon, in kilograms.\n    Catch_Rate. Catch Rate.\n    Body_Style. Body Style of the Pok\u00e9mon according to the Pok\u00e9dex.","7c2fefb0":"### Gaussian Naive Bayes","78fdfb9d":"There are many classifying algorithms. Among them, we will apply the following Classification algorithms to predict a legendary Pok\u00e9mon:\n\n   - Logistic Regression\n   - Support Vector Machines (SVC)\n   - *k*-Nearest Neighbor (KNN)\n   - Decision Tree\n   - Random Forest\n   - Naive Bayes (GaussianNB)\n   - Perceptron\n   - Stochastic Gradient Descent (SGD)\n\nHere is the training and testing procedure:\n\n   - First, we train these classifiers with our training data.\n   - After that, using the trained classifier, we predict the Survival outcome of test data.\n   - Finally, we calculate the accuracy score (in percentange) of the trained classifier.\n\n**Please note**: that the accuracy score is generated based on our training dataset.","ba96bc07":"### Perceptron","976695b2":"## Confusion Matrix\n\nA confusion matrix, also known as an error matrix, is a specific table layout that allows visualization of the performance of an algorithm. Each row of the matrix represents the instances in a predicted class while each column represents the instances in an actual class and *vice versa*. The name stems from the fact that it makes it easy to see if the system is confusing two classes (*i.e.* commonly mislabelling one as another).\n\nIn predictive analytics, a table of confusion (sometimes also called a confusion matrix), is a table with two rows and two columns that reports the number of false positives, false negatives, true positives, and true negatives. This allows more detailed analysis than mere proportion of correct classifications (accuracy). Accuracy is not a reliable metric for the real performance of a classifier, because it will yield misleading results if the data set is unbalanced (that is, when the numbers of observations in different classes vary greatly). For example, if there were 95 cats and only 5 dogs in the data set, a particular classifier might classify all the observations as cats. The overall accuracy would be 95%, but in more detail the classifier would have a 100% recognition rate for the cat class but a 0% recognition rate for the dog class.\n\nHere's another guide explaining Confusion Matrix with example.\n\nIn our Pok\u00e9mon case:\n\n   - **True Positive**: The classifier predicted legendary and the Pok\u00e9mon was actually a legendary.\n\n   - **True Negative**: The classifier predicted not legendary and the Pok\u00e9mon was not a legendary.\n\n   - **False Positive**: The classifier predicted legendary but the Pok\u00e9mon was not a legendary.\n\n   - **False Negative**: The classifier predicted not legendary the Pok\u00e9mon was actually a legendary.\n    \nIn the example code below, we plot a confusion matrix for the prediction of Random Forest Classifier on our training dataset. This shows how many entries are correctly and incorrectly predicted by our classifer.","d0581f83":"### *k*-Nearest Neighbors","032748f9":"### Pr_Male\n\nWe first fill the NULL values of `Pr_Male` with a random number between (mean_Pr_Male - std_Pr_Male) and (mean_Pr_Male + std_Pr_Male). Then, we create a new column named Pr_Male_Band. This categorises Pr_Male into different ranges.","1b35fd82":"## Comparing Models\n\nLet's compare the accuracy score of all the classifier models used above.","c437127b":"After that, we convert the categorical Title values into numeric form.","7d13579f":"Now, let's do the same thing for `Type_2`. Luckily, both `Type_1` and `Type_2` have the same Pokemon types, so we can just copy & paste and replace `Type_1` for `Type_2`. ","11d61ce8":"### Logistic Regression","8bab21c9":"### Support Vector Machine (SVM)"}}