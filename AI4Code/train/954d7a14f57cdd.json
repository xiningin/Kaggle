{"cell_type":{"2518a320":"code","5933b3f3":"code","50db7f66":"code","0c275758":"code","5080bba2":"code","99233805":"code","bdb60816":"code","355b224f":"code","71994bd7":"code","622ef809":"code","0f2531a0":"code","5c2ac7e2":"code","31325543":"code","88e33db6":"code","72f59e37":"code","b086cfd7":"code","5518eab4":"code","2cb593a6":"code","23e99f41":"code","2b1af1b0":"code","6db43ba3":"markdown","4af45874":"markdown"},"source":{"2518a320":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","5933b3f3":"events = pd.read_csv('\/kaggle\/input\/datatest\/events.csv')\nevents.columns\nevents","50db7f66":"events.event.unique()","0c275758":"events_by_events = events.groupby('event')","5080bba2":"datasets = []\ndataLength = []","99233805":"datasets = []\ndataLength = []\nfor i, group in events_by_events:\n    # print(group.isna().sum())\n    group.dropna(how=\"all\", axis = 1, inplace=True)\n    group.drop(columns=[\"event\"], axis=1, inplace=True)\n    row = {\"event\": i, \"total Rows\": group.shape[0]}\n    dataLength.append(row)\n    print(row)\n    datasets.append({\"event\": i, \"group\": group})","bdb60816":"datalen_df = pd.DataFrame(dataLength)\ndatalen_df.index = datalen_df.event\ndatalen_df.sort_values(by=[\"total Rows\"], inplace=True)\ndatalen_df.head(20)","355b224f":"datalen_df.plot.barh(figsize=(30,10),x = \"event\")","71994bd7":"general_cols_Series = pd.Series(list(events.columns))","622ef809":"for i, value in enumerate(datasets):\n    value[\"columns\"] = pd.Series(list(value[\"group\"].columns))","0f2531a0":"for i, value in enumerate(datasets):\n    print(value['event'])\n    print(\"Columnas:\",list(value['group'].columns))\n    print(value['group'].info(),\"\\n\")","5c2ac7e2":"len(events['person'].unique())","31325543":"\nevents[events['event'] == 'visited site']['channel']","88e33db6":"events.groupby(['person','event']).size()","72f59e37":"user_convert = events[events['event'] == 'conversion'][['person']]\ndata_user_convert = pd.merge(events, user_convert, on='person')","b086cfd7":"user_converted_full_data = data_user_convert[['person','event']]","5518eab4":"user_converted_full_data[user_converted]","2cb593a6":"user_converted_full_data.groupby('event')","23e99f41":"import pandas as pd \n\n\ndf2 = events.groupby([\"person\",\"event\"]).size().reset_index().set_index(\"person\")\nevents_list=[]\nfor element in list(df2.index.unique()):\n    events_list.append( (list(df2.loc[element].index.unique())[0], list(df2.loc[element][\"event\"])  ) )   \n\nevents_list[0]   \n\nconversions_list = []\nfor element in events_list:\n    if \"conversion\" in element[1]:\n        conversions_list.append(element)  \n\nconversions_list[0]\n\nconversions_indices = []\nfor element in conversions_list:\n    conversions_indices.append(element[0])\nconversions_indices[:5]  \n\nconversions_events = df2.loc[conversions_indices].copy().reset_index()\nconversions_events.columns = [\"person\",\"event\",\"count\"]\nconversions_events.groupby(\"event\").mean().reset_index().sort_values(by = \"count\",ascending = False).plot(kind = \"bar\",y=\"count\" , x=\"event\",title = \"Events for converted cusotmers\")\n\ndf3 = df2.copy()\ndf3 = df3.reset_index()\nboolean = ~df3[\"person\"].isin(conversions_indices) \nnotconversions_events = df3[boolean]\nnotconversions_events.columns = [\"person\",\"event\",\"count\"]\nnotconversions_events.groupby(\"event\").mean().reset_index().sort_values(by = \"count\",ascending = False).plot(kind = \"bar\",y=\"count\" , x=\"event\",title = \"Events for not converted cusotmers\")","2b1af1b0":"events[events.event == \"visited site\"].head()\nevents[events.event == \"visited site\"].shape\nnotcolumns = [\"url\" ,\"sku\",                         \n\"model\",                       \n\"condition\",                   \n\"storage\",                    \n\"color\",                       \n\"skus\",                        \n\"search_term\",                 \n\"staticpage\",                  \n\"campaign_source\",             \n\"search_engine\"]\ncolumns = [ x for x in list(events.columns) if x not in notcolumns ]\nvisited_site = events[columns]\nvisited_site = visited_site[visited_site.event == \"visited site\"]\nvisited_site[\"browser\"] = visited_site['browser_version'].apply(lambda x: x.split(\" \")[0])\n#visited_site[\"browser\"].value_counts().plot.barh(figsize=(30,10),logx=True)\nvisited_site[visited_site['new_vs_returning'] == 'New']['person'].value_counts().sum()\/visited_site[visited_site['new_vs_returning'] == 'New']['person'].shape[0]","6db43ba3":"# David","4af45874":"We need to drop all columns by group that all columns are null"}}