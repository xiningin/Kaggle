{"cell_type":{"0e71dabf":"code","7fbd9058":"code","1457734a":"code","4088d11b":"code","16acd95f":"code","6edf6e15":"code","c1d3ec0f":"code","bc43cc91":"code","3ee9fabc":"code","9f2f1e9e":"code","f776231d":"code","509c8c47":"code","4ecdb62d":"code","037cc307":"code","140c809b":"code","6c8510d9":"code","0479baff":"code","a1702623":"code","b11002af":"code","f9fd83fd":"code","0c09e837":"code","9e5da10b":"code","d4b6ad19":"code","8cc40d4d":"code","d313648d":"code","2868fbea":"code","c6a2c294":"code","7f82029f":"code","4ccd940a":"code","44b6d0cd":"code","2cca55ca":"code","ce70a959":"code","e726bd01":"code","7c5706e8":"code","3b2fa8c2":"code","c7ac79d0":"code","96d25d7b":"code","96a6d928":"code","13ab308d":"code","a517a95e":"code","60a8e386":"code","2af399cc":"code","4b2aa233":"code","f95b07f1":"code","074a0dde":"code","8d62d49d":"code","f12a2e11":"code","b5d346d1":"code","f25b6237":"code","b43cb979":"code","07465c6a":"code","c6e25df7":"code","f65d4041":"code","04a20256":"code","aa56b006":"code","2af435da":"code","892a36d3":"code","4e9c685f":"code","2ff432e0":"code","8fc3e718":"code","71a15e9a":"code","d991447d":"code","3956ba11":"code","69c1ad84":"code","1114d5f1":"code","296a0d97":"code","2a490f12":"code","4edcc0b7":"code","1d5b4821":"code","d748b6a4":"code","5af33330":"code","ffcf9df3":"code","ce887634":"code","474b2386":"code","a3e1eeec":"code","6dfb9356":"code","32b0d24a":"code","449c2868":"code","bc24b4b4":"code","fec2dce7":"code","549354b4":"code","1d9bc662":"code","ca32c5c3":"code","7b02b489":"code","f47180e4":"code","652977ba":"markdown","ed21f5e2":"markdown","1465209e":"markdown","81cf8fc9":"markdown","8bcfc0d7":"markdown","6c85aebb":"markdown","e5d2c966":"markdown","e2613d53":"markdown","94573c1f":"markdown","3e1c3d24":"markdown","28b5d765":"markdown","62cc118d":"markdown","be1e0415":"markdown"},"source":{"0e71dabf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7fbd9058":"import seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","1457734a":"train=pd.read_csv('..\/input\/titanic\/train.csv')\nvalid=pd.read_csv('..\/input\/titanic\/test.csv')","4088d11b":"train.head()","16acd95f":"train.shape","6edf6e15":"train.dtypes","c1d3ec0f":"X=train.drop('Survived',axis=1)\ny=train['Survived']\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25,random_state=0)","bc43cc91":"df_train=pd.concat([X_train,y_train],axis=1)\ndf_test=pd.concat([X_test,y_test],axis=1)","3ee9fabc":"df_train","9f2f1e9e":"df_test","f776231d":"#performing Feature Engineering separately on train data and test data to avoid leakage and overfitting on test data\n#checkin missing values in dataset\ndf_train.isnull().sum()","509c8c47":"sns.heatmap(df_train.isnull(),yticklabels= False,cbar = False,cmap='viridis')","4ecdb62d":"sns.set_style('whitegrid')\nsns.countplot(x='Survived',data = df_train)","037cc307":"sns.set_style('whitegrid')\nsns.countplot(x='Survived',hue='Sex',data= df_train,palette='Set2')","140c809b":"sns.set_style('whitegrid')\nsns.countplot(x='Survived',hue='Pclass',data= df_train,palette='Set3')","6c8510d9":"sns.histplot(df_train['Age'].dropna(),kde= False,color='r',bins=40)","0479baff":"sns.countplot(x='SibSp',data= df_train,palette='Set1')","a1702623":"sns.histplot(df_train['Fare'],kde= False,color='r',bins=50)","b11002af":"df_train.drop('PassengerId',axis=1).corr().abs().unstack()","f9fd83fd":"df_train.drop('PassengerId',axis=1).corr()","0c09e837":"#filling missing values in Age column\ndf_train['Age']= df_train.groupby(['Pclass'])['Age'].apply(lambda cols:cols.fillna(cols.median()))","9e5da10b":"sns.heatmap(df_train.isnull(),yticklabels= False,cbar = False,cmap='viridis')","d4b6ad19":"#dropping Cabin Column coz more than half of the records have nan values\ndf_train.drop('Cabin',axis=1,inplace=True)","8cc40d4d":"df_train","d313648d":"df_train.isnull().values.sum()","2868fbea":"df_train.dropna(inplace=True)","c6a2c294":"df_train['Embarked'].value_counts()","7f82029f":"df_train['Embarked'].isnull().sum()","4ccd940a":"df_train['SibSp'].value_counts()","44b6d0cd":"df_train['Parch'].value_counts()","2cca55ca":"df_train['count']=df_train['SibSp']+df_train['Parch']\n\nsize = {\n    0:'Alone',\n    \n}\ndf_train['fm_size']=df_train['count'].map(size)\ndf_train['alone']=df_train['fm_size'] == 'Alone'","ce70a959":"fig, ax = plt.subplots(1, 2)\nfor ind, val in enumerate(['count','alone']):\n    sns.countplot(x=val, hue='Survived', data=df_train, ax=ax[ind])","e726bd01":"df_train.head()","7c5706e8":"df_train.drop(['fm_size','Name','Ticket'],inplace=True,axis=1)","3b2fa8c2":"df_train","c7ac79d0":"#One-hot encoding for categorical data and dropping first column from each feature\nsex=pd.get_dummies(df_train['Sex'],drop_first=True)\nembark=pd.get_dummies(df_train['Embarked'],drop_first=True)","96d25d7b":"df_train.drop(['Sex','Embarked'],axis=1,inplace=True)","96a6d928":"df_train = pd.concat([df_train,sex,embark],axis=1)","13ab308d":"df_train.head()","a517a95e":"df_train['alone']=pd.get_dummies(df_train['alone'],drop_first=True)","60a8e386":"df_train.head()","2af399cc":"#Now we will perform Feature engineering for test dataset similarly we did for training data to avoid data leakage and overfitting\n#checking missing values in test dataset\ndf_test.isnull().sum()","4b2aa233":"sns.heatmap(df_test.isnull(),yticklabels= False,cbar = False,cmap='viridis')","f95b07f1":"df_test.head()","074a0dde":"df_test['Age']= df_test.groupby(['Pclass'])['Age'].apply(lambda cols:cols.fillna(cols.median()))","8d62d49d":"sns.heatmap(df_test.isnull(),yticklabels= False,cbar = False,cmap='viridis')","f12a2e11":"df_test.drop('Cabin',axis=1,inplace=True)","b5d346d1":"df_test.isnull().values.sum()","f25b6237":"df_test['Embarked'].value_counts()","b43cb979":"df_test['SibSp'].value_counts()","07465c6a":"df_test['Parch'].value_counts()","c6e25df7":"df_test['count']=df_test['SibSp']+df_test['Parch']\n\nsize = {\n    0:'Alone',\n    \n}\ndf_test['fm_size']=df_test['count'].map(size)\ndf_test['alone']=df_test['fm_size'] == 'Alone'","f65d4041":"df_test.head()","04a20256":"df_test.drop(['fm_size','Name','Ticket'],inplace=True,axis=1)","aa56b006":"df_test","2af435da":"sex=pd.get_dummies(df_test['Sex'],drop_first=True)\nembark=pd.get_dummies(df_test['Embarked'],drop_first=True)","892a36d3":"df_test.drop(['Sex','Embarked'],axis=1,inplace=True)\ndf_test = pd.concat([df_test,sex,embark],axis=1)\ndf_test['alone']=pd.get_dummies(df_test['alone'],drop_first=True)\ndf_test.head()","4e9c685f":"df_test.isnull().sum()","2ff432e0":"#df_test.fillna(test['Fare'].mean(),axis=1,inplace=True)","8fc3e718":"#test.isnull().sum()","71a15e9a":"valid.isnull().sum()\nvalid['Age']= valid.groupby(['Pclass'])['Age'].apply(lambda cols:cols.fillna(cols.median()))\nvalid.drop('Cabin',axis=1,inplace=True)\nvalid.isnull().values.sum()\nvalid['Embarked'].value_counts()\nvalid['SibSp'].value_counts()\nvalid['Parch'].value_counts()\n\nvalid['count']=valid['SibSp']+valid['Parch']\n\nsize = {\n    0:'Alone',\n    \n}\nvalid['fm_size']=valid['count'].map(size)\nvalid['alone']=valid['fm_size'] == 'Alone'\n\nvalid.head()\nvalid.drop(['fm_size','Name','Ticket'],inplace=True,axis=1)\nsex=pd.get_dummies(valid['Sex'],drop_first=True)\nembark=pd.get_dummies(valid['Embarked'],drop_first=True)\nvalid.drop(['Sex','Embarked'],axis=1,inplace=True)\nvalid = pd.concat([valid,sex,embark],axis=1)\nvalid['alone']=pd.get_dummies(valid['alone'],drop_first=True)\nvalid.head()\nvalid.isnull().sum()\nvalid.fillna(valid['Fare'].mean(),axis=1,inplace=True)\nvalid.isnull().sum()","d991447d":"features=['Pclass','Age','SibSp','Parch','Fare','count','alone','male','Q','S']\nX_train=df_train[features]\ny_train=df_train['Survived']\n\nX_test=df_test[features]\ny_test=df_test['Survived']","3956ba11":"from sklearn.feature_selection import chi2\np_values=chi2(X_train,y_train)","69c1ad84":"p_values=pd.Series(p_values[1])\np_values.index=X_train.columns\np_values.sort_values(ascending=True)","1114d5f1":"p_values[p_values<0.05].index","296a0d97":"features=['Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'alone', 'male']\nX_train=df_train[features]\ny_train=df_train['Survived']\n\nX_test=df_test[features]\ny_test=df_test['Survived']","2a490f12":"from sklearn.ensemble import RandomForestClassifier\nrandom_forest = RandomForestClassifier(n_estimators=200,max_depth=5,max_features='sqrt',n_jobs=-1)\nrandom_forest.fit(X_train,y_train)","4edcc0b7":"y_pred=random_forest.predict(X_test)","1d5b4821":"y_pred","d748b6a4":"from sklearn.metrics import accuracy_score,classification_report,confusion_matrix\nprint(accuracy_score(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(classification_report(y_test,y_pred))","5af33330":"from xgboost import XGBClassifier\nxgb=XGBClassifier( use_label_encoder=False)\nxgb.fit(X_train,y_train)","ffcf9df3":"xgb_pred=xgb.predict(X_test)","ce887634":"from sklearn.metrics import accuracy_score,classification_report,confusion_matrix\nprint(accuracy_score(y_test,xgb_pred))\nprint(confusion_matrix(y_test,xgb_pred))\nprint(classification_report(y_test,xgb_pred))","474b2386":"from sklearn.model_selection import RandomizedSearchCV","a3e1eeec":"#Randomized Search CV\n\n#number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start=100,stop=600,num=6)]\n\n#number of feature to consider at every split\nmax_features =['auto','sqrt']\n\n#max number of levels in tree\nmax_depth = [int(x) for x in np.linspace(5,30,6)]\n    \n#minimum number of samples required to split a node\nmin_samples_split = [2,5,10,15,100]\n             \n#minimum number of samples required at each leaf node\nmin_samples_leaf = [1,2,5,10]","6dfb9356":"# Create the random grid\n\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf}","32b0d24a":"# Random search of parameters, using 5 fold cross validation, \n# search across 100 different combinations\nrf_random = RandomizedSearchCV(estimator = random_forest, param_distributions = random_grid,scoring='accuracy', n_iter = 10, cv = 5, verbose=2, random_state=0, n_jobs =1)","449c2868":"rf_random.fit(X_train,y_train)","bc24b4b4":"rf_random.best_params_","fec2dce7":"prediction = rf_random.predict(X_test)","549354b4":"print(accuracy_score(y_test,prediction))\nprint(confusion_matrix(y_test,prediction))\nprint(classification_report(y_test,prediction))","1d9bc662":"valid['Survived'] = rf_random.predict(valid[features])","ca32c5c3":"valid['Survived']","7b02b489":"submission=valid[['PassengerId','Survived']]","f47180e4":"submission.to_csv('.\/submission1.csv',index=False)","652977ba":"**family count**","ed21f5e2":"# correlation between columns","1465209e":"****family count****","81cf8fc9":"# *train data Feature Engineering*\n# missing values","8bcfc0d7":"# Validation Data feature Engineering","6c85aebb":"# Saving submission file as csv","e5d2c966":"train test split of training data to predict test.csv validation data","e2613d53":"# fitting Model to the dataset","94573c1f":"# test data feature engineering","3e1c3d24":"# Handling Categorical Data","28b5d765":"# Feature Selection","62cc118d":"# **predicting the validation set with random forest model**","be1e0415":"# Exploratory Data Analysis"}}