{"cell_type":{"55bb20f4":"code","3bc26099":"code","bc91a60e":"code","79400797":"code","eeadf140":"code","4cdd3395":"code","8795a1a2":"code","a4b60a9c":"code","796e3e32":"code","40b2c926":"code","2a84d5be":"code","fd7165e6":"code","769500de":"code","3ee45235":"code","4b407479":"code","f9105479":"code","b585907c":"code","697a9478":"code","89a3f2a8":"code","3c7b689d":"code","2fcc99bf":"code","8d55ffd6":"code","b69bd00c":"code","309290db":"markdown","8537dfa5":"markdown","5c58a091":"markdown","81dc5237":"markdown","030e2bdc":"markdown","49afa2ae":"markdown","169084ae":"markdown","d316de63":"markdown","10a28f43":"markdown","29c68b24":"markdown","469dda38":"markdown","7db20c10":"markdown","e96814e7":"markdown","0f98ad4d":"markdown","1131ca82":"markdown","45dd9d54":"markdown","ca8633fb":"markdown","9b9beae4":"markdown","d8dc4e04":"markdown","e3399d37":"markdown","8b539624":"markdown","487e9857":"markdown","d18a0fcf":"markdown","7ca74725":"markdown"},"source":{"55bb20f4":"# libraries \nimport matplotlib.pyplot as plt\nimport seaborn as srn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn import linear_model\nfrom sklearn.metrics import*\nfrom sklearn.model_selection import train_test_split","3bc26099":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","bc91a60e":"df=pd.read_csv('\/kaggle\/input\/suicide-rates-overview-1985-to-2016\/master.csv')\ndf","79400797":"print('shape of the dataframe is :',df.shape)","eeadf140":"for i in df.columns:\n    print(i,end=\", \")","4cdd3395":"# leakage checking\n\nlk_dat=df.isnull()\nfor col in df.columns:\n    print('leakage in '+col+' is :',len(lk_dat[lk_dat[col]==True]))","8795a1a2":"# filling the leakage\n\ndf.replace(np.NaN,-99999.0,inplace=True)\ndf","a4b60a9c":"df.drop('country-year',1,inplace=True)\ndf","796e3e32":"age=[]\nfor i in df['age']:\n    age.append(i[:-6])\ndf['age'].value_counts()","40b2c926":"age=['5-14 years','15-24 years','25-34 years','35-54 years','55-74 years','75+ years']","2a84d5be":"k=0\nfor i in age:\n    df['age'].replace(i,k,inplace=True)\n    k+=1\ndf['age']","fd7165e6":"#  function to turn categorical data into numerical data\n\ndef non_num_data(df):\n    columns=df.columns.values\n    \n    for column in columns:\n        text_digit_vals={}\n        def con_to_int(val):\n            return text_digit_vals[val]\n        \n        if df[column].dtype!=np.int64 and df[column].dtype!=np.float64:\n            column_contents=df[column].values.tolist()\n            unique_elements=set(column_contents)\n            x=0\n            for unique in unique_elements:\n                if unique not in text_digit_vals:\n                    text_digit_vals[unique]=x\n                    x+=1\n            df[column]=list(map(con_to_int,df[column]))\n    return df","769500de":"non_num_data(df)\ndf","3ee45235":"X_df=df.drop('suicides_no',1)\ny_df=df['suicides_no']","4b407479":"print(X_df.shape,y_df.shape)","f9105479":"df=df.sample(frac=1)\ndf","b585907c":"\nX_train,X_test,y_train,y_test=train_test_split(X_df,y_df,test_size=0.3)\nprint(X_train.shape,X_test.shape,y_train.shape,y_test.shape)","697a9478":"\nmodel=linear_model.LinearRegression()\nmodel.fit(X_train,y_train)\nprint('accuracy of linearregression : ',model.score(X_train,y_train))\ny_pr1=model.predict(X_test)\nprint('MSE of the data using Linear Regression : ',mean_squared_error(y_test,y_pr1))\nprint('r-squared error of the data using Linear Regression : ',1-r2_score(y_test,y_pr1))\nprint('RMSE of the data using Linear Regression : ',np.sqrt(mean_squared_error(y_test,y_pr1)))","89a3f2a8":"a=np.arange(1,len(y_test)+1,1)\nplt.scatter(a,y_test,label='true',s=5)\nplt.legend()\nplt.scatter(a,y_pr1,label='predicted',s=5)\nplt.legend()\nplt.show()","3c7b689d":"\nmodel=RandomForestRegressor()\nmodel.fit(X_train,y_train)\nprint('accuracy of randomforestregression : ',model.score(X_train,y_train))\ny_pr1=model.predict(X_test)\nprint('MSE of the data using randomforestregression : ',mean_squared_error(y_test,y_pr1))\nprint('r-squared error of the data using randomforestregression : ',1-r2_score(y_test,y_pr1))\nprint('RMSE of the data using randomforestregression : ',np.sqrt(mean_squared_error(y_test,y_pr1)))","2fcc99bf":"plt.scatter(a,y_test,label='true',s=5)\nplt.legend()\nplt.scatter(a,y_pr1,label='predicted',s=5)\nplt.legend()\nplt.show()","8d55ffd6":"X_trr=X_train.drop('HDI for year',1)\nX_ter=X_test.drop('HDI for year',1)","b69bd00c":"model.fit(X_trr,y_train)\nprint('accuracy of randomforestregression : ',model.score(X_trr,y_train))\ny_pr1=model.predict(X_ter)\nprint('MSE of the data using randomforestregression : ',mean_squared_error(y_test,y_pr1))\nprint('r-squared error of the data using randomforestregression : ',1-r2_score(y_test,y_pr1))\nprint('RMSE of the data using randomforestregression : ',np.sqrt(mean_squared_error(y_test,y_pr1)))","309290db":"LinearRegression","8537dfa5":"Regression models","5c58a091":"![file-20181012-119117-1ie1xwy.jpg](attachment:file-20181012-119117-1ie1xwy.jpg)","81dc5237":"![the%20end.jpg](attachment:the%20end.jpg)","030e2bdc":"* Re-shuffling the data","49afa2ae":"# UPVOTE if you like this EDA  :)","169084ae":"So we are going to take a dummy datset and the this from there and predcit both and see which have more accuracy. Before doing pipeline we'll create that dummy dataframe ","d316de63":"Surely it shows that droping that 'HDI for year' column increases the accuracy of the model and it concludes that heavy leakage may mislead the data and omitting that in a huge dataset might help you gain higher accuracy","10a28f43":"* train-test split \n\nAs the data is large   a 70-30 train-test split is good for prediction","29c68b24":"Creating X and Y for the data","469dda38":"# Pre-processing","7db20c10":"It is visible it has 6 different types.","e96814e7":"Data is large in size and can be predicted through Regression","0f98ad4d":"The age column is a categorical data .\n","1131ca82":"# Libraries","45dd9d54":"* Leakage checking (if any)\n* Leakage filling (if any)\n* Unnecessary column or row deletion (if any)","ca8633fb":"# Data gathering and visulaization","9b9beae4":"# Feature Engineering","d8dc4e04":"RandomForestRegressor showed huge accuracy over the model.\n\nPreviously we decided to predict the model after droping the HDI column. \n\nwe are going to execute that now.","e3399d37":"# Pipelines","8b539624":"The country-year column looks unnecessary as the data is also present in 2 other columns. we are drpoping it.","487e9857":"The HDI column hold (19456 \/ 27820) * 100 = 69.9352 % of leakage in a single column.","d18a0fcf":"Randomforestregressor","7ca74725":"Encoding other columns too"}}