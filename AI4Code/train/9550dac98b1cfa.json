{"cell_type":{"1fe417e9":"code","b461ca67":"code","f386a15c":"code","8c306739":"code","22e24a6d":"code","e86b6067":"code","e6dc95da":"code","56c6b31b":"code","393b54f0":"code","b6b18e9f":"code","e71010bf":"code","b85e956a":"code","6df62ec1":"code","06b6d8e9":"markdown","0475db3b":"markdown","09207152":"markdown","f2e6af68":"markdown","97b27238":"markdown","f6e88af1":"markdown","5a59b483":"markdown","7fb652f1":"markdown","86e31cf6":"markdown","baf47cf1":"markdown","bf7edb5a":"markdown","9102acd4":"markdown","134df940":"markdown"},"source":{"1fe417e9":"!pip install pyspark\n!pip install spark-nlp==2.0.1\n!pip install pandas","b461ca67":"import pandas as pd\npd.set_option('max_colwidth', 800)","f386a15c":"from pyspark.sql import SparkSession\n\nspark = SparkSession \\\n        .builder \\\n        .config(\"spark.jars.packages\", \"JohnSnowLabs:spark-nlp:1.8.2\") \\\n        .getOrCreate()","8c306739":"path = '..\/input\/*.csv'\ndf = spark.read.format('csv').option('header', 'true').load(path)\ndf.limit(5).toPandas()","22e24a6d":"df = df.filter('comment is not null')","e86b6067":"from pyspark.sql.functions import split, explode, desc\n\ndfWords = df.select(explode(split('comment', '\\\\s+')).alias('word')) \\\n                    .groupBy('word').count().orderBy(desc('word'))\n\ndfWords.printSchema()","e6dc95da":"dfWords.orderBy(desc('count')).limit(5).toPandas()","56c6b31b":"from com.johnsnowlabs.nlp.pretrained.pipeline.en import BasicPipeline as bp\n\ndfAnnotated = bp.annotate(df, 'comment')\ndfAnnotated.printSchema()","393b54f0":"dfPos = dfAnnotated.select(\"text\", \"pos.metadata\", \"pos.result\")\ndfPos.limit(5).toPandas()","b6b18e9f":"dfSplitPos = dfAnnotated.select(explode(\"pos\").alias(\"pos\"))\ndfSplitPos.limit(5).toPandas()","e71010bf":"NNPFilter = \"pos.result = 'NNP' or pos.result = 'NNPs'\"\ndfNNPFilter = dfSplitPos.filter(NNPFilter)\ndfNNPFilter.limit(10).toPandas()","b85e956a":"dfWordTag = dfNNPFilter.selectExpr(\"pos.metadata['word'] as word\", \"pos.result as tag\")\ndfWordTag.limit(10).toPandas()","6df62ec1":"dfCountWords = dfWordTag.groupBy('word').count().orderBy(desc('count'))\ndfCountWords.limit(20).toPandas()","06b6d8e9":"Let's create a new DataFrame with the `pos` struct","0475db3b":"I'm going to use selectExpr function to create a new DataFrame with a *word* and *tag* columns","09207152":"I want to count every word with the tag NNP or NNPs which means:\n* NNP\tProper noun, singular \n* NNPS\tProper noun, plural\n","f2e6af68":"Our DataSet is doens't say so much, but the idea is apply this method to more huge datasets, this can be works as practice to apply in other studies and projects. \n\nPlease feel free to let me know your thoughts about this and what I can do better for a next exercise. \n\nYou can reach me on Medium or Github\n\n* https:\/\/github.com\/kennycontreras\n* https:\/\/medium.com\/@kennycontreras","97b27238":"Declare a path variable and read the csv files with the `SparkSession` created before. \n\nSet a *header* option as true and *csv* format ","f6e88af1":"Our new DataFrame doesn't looks so good, as you can see, we have blank rows, pronouns, etc.\n\nOur goal is count the relevant words from posts. That's why we are going to use `NLP` library. \nNatural Languague Processing library will clasify every word from the dataset as Noun, Pronoun, Verbs, etc.","5a59b483":"* `text` original text from comment column.\n* `pos.metadata` will contain a key,value for every words.\n* `pos.result` column is an array with a bunch of tags for every word in the DataSet.\n\nHere is the list of NLP tags https:\/\/cs.nyu.edu\/grishman\/jet\/guide\/PennPOS.html\n","7fb652f1":"**Natural Languague Processing NLP using Spark and Pandas**\n\nI created this notebook to do a short demostration about this library called NLP. \nI found this exercise really fun and beginner friendly.\n\nWe are going to analize some dataset from Reddit and figure out what are the most common words. \nJust to clarify, this dataset is really small and it works just for practice but you can apply the same methods to some others datasets too. \n\nTo use this notebook you need to install \n* pyspark\n* spark-nlp\n* pandas\n\nYou can do it just running the following code in Jupyter Notebook:\n","86e31cf6":"Let's create a `SparkSession`. We're going declare a Spark package to use the NLP library and count the most common words from our dataset. ","baf47cf1":"Import `pandas` Library and set the column width to 800. ","bf7edb5a":"Finally, we have our DataSet as we want and we can start counting the most common words. ","9102acd4":"Our objective with this project is count the most common words, so we don't want null comments.\n\nLet's filter all null rows from the comment column.","134df940":"I'm going to create a new DataFrame using * explode * and * split * functions of `pyspark`.\n\nThe purpose of this is create a new column called word, this new column will contain all the words of our comments splited with spaces."}}