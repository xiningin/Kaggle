{"cell_type":{"3967585d":"code","4cea8f38":"code","0b6d939b":"code","8d77ffc9":"code","6d5328df":"code","64f9a7f8":"code","07b9d7ac":"code","c4d1bdac":"code","3c8f1849":"code","f52ef069":"code","26b185cd":"code","abf49b90":"code","11110f44":"code","22aa2a3b":"code","79d2a464":"code","1fb38db6":"code","1c379b52":"code","37ec55e2":"code","6160337e":"code","65b2c9ad":"code","47737ac1":"code","da4e2ed2":"code","7457f9d5":"code","cb7bff8f":"code","e34f052c":"code","8c021727":"code","faed5424":"code","e4cc674d":"code","1b98f3a3":"code","8313f969":"code","9ae8d4c6":"code","8ca1f337":"code","c7a8a49b":"code","845d828f":"code","1cb200f5":"code","1ecb96d3":"code","ce086c9c":"code","549f7c3e":"code","c93c3593":"code","4a6c7170":"code","d6e1bca1":"code","f1bc71be":"code","2667433f":"code","ad1e16e0":"code","3753327d":"code","bf28a9d7":"code","137f2598":"code","8fdceb4a":"markdown","60390147":"markdown","cfa9a6c8":"markdown","4e40e0d7":"markdown","ba4ba8ba":"markdown","a101f4b5":"markdown","8a22afa6":"markdown","d0eafbdc":"markdown","8325b2ab":"markdown","83353e2c":"markdown","85581c66":"markdown","166c8cb7":"markdown","7a4ab00f":"markdown","24a10444":"markdown","6929e610":"markdown","b9308214":"markdown","742e17e6":"markdown","44cf18e5":"markdown","1a5f9842":"markdown","ebd14b65":"markdown","ef5404cc":"markdown","1943196e":"markdown","cf9922b5":"markdown","1ed07c1b":"markdown","52ab7fa4":"markdown","296adcb2":"markdown","0a497b40":"markdown","c4bfc336":"markdown","bfcb86f3":"markdown","0039e017":"markdown","bbf1f32e":"markdown","ae3a681f":"markdown","aec96140":"markdown","f2b6461e":"markdown","785af1e7":"markdown","a8c9a607":"markdown","1aa407fe":"markdown","e6148332":"markdown","ad820ab4":"markdown","a61296a7":"markdown","18c032dd":"markdown","9c780d06":"markdown","ecaaf172":"markdown","32b1d826":"markdown","afd976ea":"markdown","10c01827":"markdown","ad07486d":"markdown","9d0b4630":"markdown","7a3a9d4a":"markdown","dce445cc":"markdown","8c845973":"markdown","ccf45542":"markdown"},"source":{"3967585d":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn","4cea8f38":"trainDataFrame = pd.read_csv(\"..\/input\/atividade-regressao-PMR3508\/train.csv\", index_col=['Id'])\n\ntestDataFrame = pd.read_csv(\"..\/input\/atividade-regressao-PMR3508\/test.csv\", index_col=['Id'])","0b6d939b":"trainDataFrame.head()","8d77ffc9":"trainDataFrame.shape","6d5328df":"testDataFrame.head()","64f9a7f8":"testDataFrame.shape","07b9d7ac":"trainDataFrame.describe()","c4d1bdac":"trainDataFrame.info()","3c8f1849":"testDataFrame.describe()","f52ef069":"testDataFrame.info()","26b185cd":"import numpy as np\n\nmask = np.triu(np.ones_like(trainDataFrame.corr(), dtype=np.bool))\n\nplt.figure(figsize=(10,10))\n\nsns.heatmap(trainDataFrame.corr(), mask=mask, square = True, annot=True, vmin=-1, vmax=1)\nplt.show()\n","abf49b90":"trainDataFrame.plot(kind=\"scatter\", x=\"median_income\", y=\"median_house_value\", alpha=0.4, s=trainDataFrame[\"population\"]\/50, \n             label=\"population\", c=\"median_house_value\", figsize=(10,8),\n             cmap=plt.get_cmap(\"rocket\"), colorbar=False)","11110f44":"sns.set()\nsns.pairplot(trainDataFrame, height=2)","22aa2a3b":"import plotly.express as px\n\nfig = px.scatter_mapbox(trainDataFrame, lat=\"latitude\", lon=\"longitude\", color=\"median_house_value\", size=\"population\",\n                  color_continuous_scale=px.colors.sequential.Sunsetdark, size_max=15, zoom=4,\n                  mapbox_style=\"carto-positron\")\nfig.show()","79d2a464":"trainDataFrame.plot(kind=\"scatter\", x=\"households\", y=\"median_house_value\", alpha=0.4, s=trainDataFrame[\"population\"]\/50, \n             label=\"population\", c=\"median_house_value\", figsize=(10,8),\n             cmap=plt.get_cmap(\"rocket\"), colorbar=False)","1fb38db6":"trainDataFrame.plot(kind=\"scatter\", x=\"population\", y=\"median_house_value\", alpha=0.4, s=trainDataFrame[\"population\"]\/50, \n             label=\"population\", c=\"median_house_value\", figsize=(10,8),\n             cmap=plt.get_cmap(\"rocket\"), colorbar=False)","1c379b52":"trainDataFrame.plot(kind=\"scatter\", x=\"total_bedrooms\", y=\"median_house_value\", alpha=0.4, s=trainDataFrame[\"population\"]\/50, \n             label=\"population\", c=\"median_house_value\", figsize=(10,8),\n             cmap=plt.get_cmap(\"rocket\"), colorbar=False)","37ec55e2":"trainDataFrame.plot(kind=\"scatter\", x=\"total_rooms\", y=\"median_house_value\", alpha=0.4, s=trainDataFrame[\"population\"]\/50, \n             label=\"population\", c=\"median_house_value\", figsize=(10,8),\n             cmap=plt.get_cmap(\"rocket\"), colorbar=False)","6160337e":"from geopy.distance import geodesic\n\n#cidades grandes mencionadas anteriormente\nSanDiego = (32.7174209, -117.1627714)\nSanFrancisco = (37.7790262, -122.4199061)\nSacramento = (38.575764, -121.478851)\n#Los Angeles, mesmo que n\u00e3o mencionada anteriormente, \u00e9 uma das maiores cidades do estado e tem im\u00f3veis com alto valor\nLosAngeles = (34.0536909, -118.2427666)\n\n\ndef dist_cidades(trainDataFrame):\n    home = (trainDataFrame[\"latitude\"], trainDataFrame[\"longitude\"])\n    trainDataFrame[\"city_distance\"] = min(geodesic(home, SanDiego).km, geodesic(home, SanFrancisco).km, geodesic(home, LosAngeles).km)\n    \n    return trainDataFrame","65b2c9ad":"trainDataFrame = trainDataFrame.apply(dist_cidades, axis = 1)","47737ac1":"trainDataFrame.head()","da4e2ed2":"trainDataFrame.plot(kind=\"scatter\", x=\"city_distance\", y=\"median_house_value\", alpha=0.4, s=trainDataFrame[\"population\"]\/50, \n             label=\"population\", c=\"median_house_value\", figsize=(10,8),\n             cmap=plt.get_cmap(\"rocket\"), colorbar=False)","7457f9d5":"trainDataFrame['rooms\/households'] = trainDataFrame['total_rooms']\/trainDataFrame['households']\ntrainDataFrame['bedrooms\/households'] = trainDataFrame['total_bedrooms']\/trainDataFrame['households']\ntrainDataFrame['population\/bedrooms'] = trainDataFrame['population']\/trainDataFrame['total_bedrooms']\ntrainDataFrame['population\/rooms'] = trainDataFrame['population']\/trainDataFrame['total_rooms']\ntrainDataFrame['population\/households'] = trainDataFrame['population']\/trainDataFrame['households']","cb7bff8f":"trainDataFrame.head()","e34f052c":"trainDataFrame.plot(kind=\"scatter\", x=\"rooms\/households\", y=\"median_house_value\", alpha=0.4, s=trainDataFrame[\"population\"]\/50, \n             label=\"population\", c=\"median_house_value\", figsize=(10,8),\n             cmap=plt.get_cmap(\"rocket\"), colorbar=False)","8c021727":"trainDataFrame.plot(kind=\"scatter\", x=\"bedrooms\/households\", y=\"median_house_value\", alpha=0.4, s=trainDataFrame[\"population\"]\/50, \n             label=\"population\", c=\"median_house_value\", figsize=(10,8),\n             cmap=plt.get_cmap(\"rocket\"), colorbar=False)","faed5424":"trainDataFrame.plot(kind=\"scatter\", x=\"population\/bedrooms\", y=\"median_house_value\", alpha=0.4, s=trainDataFrame[\"population\"]\/50, \n             label=\"population\", c=\"median_house_value\", figsize=(10,8),\n             cmap=plt.get_cmap(\"rocket\"), colorbar=False)","e4cc674d":"trainDataFrame.plot(kind=\"scatter\", x=\"population\/rooms\", y=\"median_house_value\", alpha=0.4, s=trainDataFrame[\"population\"]\/50, \n             label=\"population\", c=\"median_house_value\", figsize=(10,8),\n             cmap=plt.get_cmap(\"rocket\"), colorbar=False)","1b98f3a3":"trainDataFrame.plot(kind=\"scatter\", x=\"population\/households\", y=\"median_house_value\", alpha=0.4, s=trainDataFrame[\"population\"]\/50, \n             label=\"population\", c=\"median_house_value\", figsize=(10,8),\n             cmap=plt.get_cmap(\"rocket\"), colorbar=False)","8313f969":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\nlb = trainDataFrame.pop('median_house_value').values.astype(float)\n\nat = trainDataFrame\n\nat = scaler.fit_transform(at)","9ae8d4c6":"from sklearn.model_selection import cross_val_score\n\ndef rmsle(y_test,y_pred):\n    return np.sqrt(np.mean((np.log(np.abs(y_pred)+1)-np.log(np.abs(y_test)+1))**2))\n\ndef scorer(model, X_train, Y_train):\n    score = np.mean(cross_val_score(model, X_train, Y_train, cv = 10))\n    return score","8ca1f337":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\n\nX_train, xTEST, Y_train, yTEST = train_test_split(at, lb, test_size=0.3) \n\nLR = LinearRegression()\nLR.fit(X_train,Y_train)\n\npred = LR.predict(xTEST)\n\nRMSLE = rmsle(yTEST, pred)\nprint(\"RMSLE:\", RMSLE)","c7a8a49b":"from sklearn.linear_model import Ridge\n\nX_train, xTEST, Y_train, yTEST = train_test_split(at, lb, test_size=0.3) \n\nR = Ridge()\nR.fit(X_train,Y_train)\n\npred = R.predict(xTEST)\n\nRMSLE = rmsle(yTEST, pred)\nprint(\"RMSLE:\", RMSLE)","845d828f":"from sklearn.linear_model import Lasso\n\nX_train, xTEST, Y_train, yTEST = train_test_split(at, lb, test_size=0.3) \n\nL = Lasso()\nL.fit(X_train,Y_train)\n\npred = L.predict(xTEST)\n\nRMSLE = rmsle(yTEST, pred)\nprint(\"RMSLE:\", RMSLE)","1cb200f5":"from sklearn.linear_model import ElasticNet\n\nX_train, xTEST, Y_train, yTEST = train_test_split(at, lb, test_size=0.3) \n\nEN = ElasticNet()\nEN.fit(X_train,Y_train)\n\npred = EN.predict(xTEST)\n\nRMSLE = rmsle(yTEST, pred)\nprint(\"RMSLE:\", RMSLE)","1ecb96d3":"from sklearn.linear_model import Lars\n\nX_train, xTEST, Y_train, yTEST = train_test_split(at, lb, test_size=0.3) \n\nLars = Lars()\nLars.fit(X_train,Y_train)\n\npred = Lars.predict(xTEST)\n\nRMSLE = rmsle(yTEST, pred)\nprint(\"RMSLE:\", RMSLE)","ce086c9c":"from sklearn.linear_model import OrthogonalMatchingPursuit\n\nX_train, xTEST, Y_train, yTEST = train_test_split(at, lb, test_size=0.3) \n\nOMP = OrthogonalMatchingPursuit()\nOMP.fit(X_train,Y_train)\n\npred = OMP.predict(xTEST)\n\nRMSLE = rmsle(yTEST, pred)\nprint(\"RMSLE:\", RMSLE)","549f7c3e":"from sklearn.linear_model import BayesianRidge\n\nX_train, xTEST, Y_train, yTEST = train_test_split(at, lb, test_size=0.3) \n\nBR = BayesianRidge()\nBR.fit(X_train,Y_train)\n\npred = BR.predict(xTEST)\n\nRMSLE = rmsle(yTEST, pred)\nprint(\"RMSLE:\", RMSLE)","c93c3593":"from sklearn.neighbors import KNeighborsRegressor\n\nX_train, xTEST, Y_train, yTEST = train_test_split(at, lb, test_size=0.3) \n\nKNN = KNeighborsRegressor()\nKNN.fit(X_train,Y_train)\n\npred = KNN.predict(xTEST)\n\nRMSLE = rmsle(yTEST, pred)\nprint(\"RMSLE:\", RMSLE)","4a6c7170":"from sklearn.ensemble import ExtraTreesRegressor\n\nX_train, xTEST, Y_train, yTEST = train_test_split(at, lb, test_size=0.3) \n\nETR = ExtraTreesRegressor()\nETR.fit(X_train,Y_train)\n\npred = ETR.predict(xTEST)\n\nRMSLE = rmsle(yTEST, pred)\nprint(\"RMSLE:\", RMSLE)","d6e1bca1":"testDataFrame = testDataFrame.apply(dist_cidades, axis = 1)\n\ntestDataFrame['rooms\/households'] = testDataFrame['total_rooms']\/testDataFrame['households']\ntestDataFrame['bedrooms\/households'] = testDataFrame['total_bedrooms']\/testDataFrame['households']\ntestDataFrame['population\/bedrooms'] = testDataFrame['population']\/testDataFrame['total_bedrooms']\ntestDataFrame['population\/rooms'] = testDataFrame['population']\/testDataFrame['total_rooms']\ntestDataFrame['population\/households'] = testDataFrame['population']\/testDataFrame['households']","f1bc71be":"testDataFrame.head()","2667433f":"X_test = scaler.fit_transform(testDataFrame)","ad1e16e0":"ETR.fit(at,lb)\npredict = ETR.predict(X_test)\n\nprint(\"O resultado da predi\u00e7\u00e3o \u00e9:\", predict)","3753327d":"submission = pd.DataFrame(predict)\nsubmission[0] = testDataFrame.index\nsubmission[1] = predict\nsubmission.columns = ['Id','median_house_value']","bf28a9d7":"submission.head()","137f2598":"submission.to_csv('submission.csv',index = False)","8fdceb4a":"Aparentemente n\u00e3o temos valores *null* nesse dataframe de teste, mas podemos confirmar isso usando do comando info:","60390147":"Podemos ver que, sozinho, o atributo *households* n\u00e3o \u00e9 muito eficaz em nos dizer o pre\u00e7o da casa. Entretanto, podemos ver que h\u00e1 uma certa tend\u00eancia no fato de que menores valores deste atributo estejam ligados a um menor pre\u00e7o do im\u00f3vel. Por fim, podemos tamb\u00e9m concluir desse gr\u00e1fico que grande parte dos valores desse atributo se encontram entre 0-1000.\n\n*median_house_value* x *population*:","cfa9a6c8":"**2.2. Prepara\u00e7\u00e3o dos dados (Data prep)**\n\nNessa etapa iremos preparar nossos dados para serem analisados posteriormente. O principal objetivo aqui \u00e9 o tratamento dos dados faltantes. Para isso, usaremos primeiramente o comando *describe* para obter uma visualiza\u00e7\u00e3o dos dados de treino usando da estat\u00edstica descritiva:","4e40e0d7":"**6.4. ElasticNet**","ba4ba8ba":"**3.4. Plotly**\n\nVamos, nessa etapa, fazer uma esp\u00e9cie de mapa de calor da Calif\u00f3rnia, que vai ajudar na visualiza\u00e7\u00e3o espacial do pre\u00e7o das casas por regi\u00e3o do estado:","a101f4b5":"Tracemos, ent\u00e3o, gr\u00e1ficos de dispers\u00e3o desses novos atributos *versus* o pre\u00e7o m\u00e9dio do r\u00f3tulo, para avaliarmos a correla\u00e7\u00e3o entre eles:","8a22afa6":"Vamos, ent\u00e3o, visualizar a nossa base de treino:","d0eafbdc":"**3.2. Gr\u00e1fico de dispers\u00e3o: *median_house_value* x *median_income***\n\nVamos tra\u00e7ar um gr\u00e1fico para averiguarmos melhor a correla\u00e7\u00e3o entre as vari\u00e1veis *median_income* e *median_house_value*:","8325b2ab":"Vejamos como ficou o nosso dataframe de submiss\u00e3o:","83353e2c":"Vamos visualizar como esse novo dataframe se apresenta:","85581c66":"Podemos ver pelo mapa que os im\u00f3veis mais pr\u00f3ximos ao mar apresentam maior valoriza\u00e7\u00e3o, o que pode ser explicado por diversos fatores hist\u00f3ricos, ec\u00f4nomicos, culturais e tur\u00edsticos. \u00c9 curioso perceber como essa valoriza\u00e7\u00e3o \u00e9 similar com algumas regi\u00f5es do Brasil: no Rio de Janeiro, por exemplo, \u00e9 t\u00edpico im\u00f3veis que tem vista para a praia serem muito caros: o Leblon, bairro da Zona Sul do Rio de Janeiro, que possui como praia a Praia do Leblon, tem o metro quadrado mais caro do Brasil.\n\nAl\u00e9m disso, tamb\u00e9m \u00e9 percept\u00edvel que im\u00f3veis pr\u00f3ximos de\/em cidades grandes como S\u00e3o Francisco, S\u00e3o Diego e Sacramento (capital do estado da Calif\u00f3rnia) apresentam um pre\u00e7o maior. ","166c8cb7":"Desse modo, usando como m\u00e9trica para avaliar os regressores o RMSLE (Root Mean Squared Log Error), vemos que o melhor regressor \u00e9 o ExtraTreesRegressor, sendo ele que utilizaremos para nossa predi\u00e7\u00e3o.","7a4ab00f":"Por fim, vemos uma tend\u00eancia similar \u00e0s anteriores: uma casa com menos c\u00f4modos tende a ter um menor pre\u00e7o e a tend\u00eancia \u00e9 que a maioria das casas n\u00e3o tenha um elevado n\u00famero de c\u00f4modos.\n\nCom isso, terminamos a parte de visualiza\u00e7\u00e3o de dados.","24a10444":"Como pode ser visto, dos 14448 dados, nenhum apresenta nem sequer um atributo (uma coluna) com um dado faltante (*null*). Desse modo, concluimos que na base de treino, n\u00e3o temos nenhum dado faltante.\n\nFa\u00e7amos o mesmo procedimento para a base de dados de teste:","6929e610":"Desse modo, separamos o nosso dataframe de treino em atributos e r\u00f3tulo, al\u00e9m de normalizar todos os nossos atributos. Com isso, terminamos o pr\u00e9-processamento e podemos come\u00e7ar a testar os regressores!","b9308214":"# 2. Importa\u00e7\u00e3o e prepara\u00e7\u00e3o dos dados\n\n**2.1. Importa\u00e7\u00e3o dos dados**\n\nIremos, nessa etapa, importar a base de dados que usaremos na regress\u00e3o, tanto a de treino quanto a de teste. Da seguinte forma: ","742e17e6":"Vejamos como ficou a base de dados de teste:","44cf18e5":"**6.6. Orthogonal Matching Pursuit (OMP)**","1a5f9842":"# 1. Prepara\u00e7\u00e3o do ambiente de desenvolvimento\n\nIremos, nessa etapa (in\u00edcio do projeto), importar as principais bibliotecas que ser\u00e3o utilizadas no exerc\u00edcio, sendo elas: numpy, para mexermos com dataframes, matplotlib e seaborn para visualiza\u00e7\u00e3o dos dados e sklearn para a regress\u00e3o de fato.","ebd14b65":"**6.7. Bayesian Ridge**","ef5404cc":"Verificando como o dataframe se apresenta ap\u00f3s essa mudan\u00e7a:","1943196e":"Vamos agora exportar nossa submiss\u00e3o no formato .csv:","cf9922b5":"Seguindo a mesma l\u00f3gica utilizada para a base de treino, podemos concluir que a base de teste tamb\u00e9m n\u00e3o possui dados faltantes.\n\nDesse modo, n\u00e3o precisamos lidar com os dados faltantes nesse exerc\u00edcio (pois n\u00e3o existem). Assim, encerramos aqui a nossa *data prep*.","1ed07c1b":"# 3. Visualiza\u00e7\u00e3o dos dados\n\n**3.1. Mapa de calor (*Heatmap*)**\n\nInicialmente, podemos utilizar de um mapa de calor para avaliar a correla\u00e7\u00e3o entre as vari\u00e1veis do problema (tanto atributos, quanto r\u00f3tulo). Como todas as nossas vari\u00e1veis s\u00e3o num\u00e9ricas, n\u00e3o precisamos utilizar de um encoder (vari\u00e1veis categ\u00f3ricas) para visualizar as vari\u00e1veis por meio de um mapa de calor.","52ab7fa4":"Feito isso, apliquemos o ExtraTreeRegressor \u00e0 base de teste:","296adcb2":"Antes de analisarmos o mapa, devemos lembrar que quanto mais pr\u00f3ximo de 0.00 estiver o valor no mapa de calor, significa que menor \u00e9 a correla\u00e7\u00e3o entre as vari\u00e1veis. Quanto mais pr\u00f3ximo de +1.00, mais positivamente correlacionadas est\u00e3o as vari\u00e1veis, o que significa que quando uma aumenta, a outra aumenta tamb\u00e9m (em m\u00e9dia). Quanto mais pr\u00f3ximo de -1.00, mais negativamente correlacionadas est\u00e3o as vari\u00e1veis, o que significa que quando quando uma diminui, a outra diminui tamb\u00e9m (em m\u00e9dia).\n\nDesse modo, visualizando ent\u00e3o o mapa de calor, podemos tirar as seguintes conclus\u00f5es:\n\nO \"tri\u00e2ngulo\" claro ao centro do *heatmap*, indica que as vari\u00e1veis *total_bedrooms*, *population*, *households* e *total_rooms* est\u00e3o fortemente **positivamente correlacionadas**, o que \u00e9 um resultado intuitivo, pois quanto maior o n\u00famero de pessoas e fam\u00edlias na casa, maior ser\u00e1 o n\u00famero de quartos e c\u00f4modos (em m\u00e9dia). \n\nAl\u00e9m disso, um ponto interessante de se observar \u00e9 que, sem d\u00favidas, olhando o *heatmap*, a vari\u00e1vel que mais contribui sobre o resultado do pre\u00e7o da casa (median_house_value) \u00e9 o *median_income* da fam\u00edlia. O que tamb\u00e9m \u00e9 um resultado intuitivo, uma vez que ambas as vari\u00e1veis s\u00e3o **positivamente correlacionadas**, a tend\u00eancia \u00e9 que, quanto maior a renda da fam\u00edlia, mais ela ter\u00e1 condi\u00e7\u00e3o de comprar um im\u00f3vel de alto valor.","0a497b40":"Como pode ser visto por esse gr\u00e1fico, a renda da fam\u00edlia parece ter uma rela\u00e7\u00e3o de alta linearidade com o pre\u00e7o da casa, o que confirma a alta correla\u00e7\u00e3o observada no mapa de calor anteriormente.","c4bfc336":"E as dimens\u00f5es da base de treino (linhas x colunas):","bfcb86f3":"# 8. Submiss\u00e3o\n\nNessa \u00faltima etapa, iremos submeter nosso projeto \u00e0 competi\u00e7\u00e3o. Para isso, iremos transformar o array gerado com as predi\u00e7\u00f5es para a base de teste em um dataframe, para em seguida export\u00e1-lo no formato .csv","0039e017":"**6.5. Lars (Least Angle Regression)**","bbf1f32e":"**6.8. kNN**","ae3a681f":"O caso aqui \u00e9 bem similar com o anterior (*median_house_value* x *households*): tamb\u00e9m vemos uma tend\u00eancia aqui para o fato de que menores valores deste atributo gerem um menor r\u00f3tulo, entretanto, a tend\u00eancia aqui parece mais forte que a anterior. Al\u00e9m disso, vemos tamb\u00e9m uma tend\u00eancia da maior parte dos valores deste atributo ser baixa (a casa n\u00e3o ser t\u00e3o populosa) e, novamente, a tend\u00eancia aqui parece ser mais forte que no caso anterior.\n\n*median_house_value* x *total_bedrooms*","aec96140":"Vemos neste caso uma tend\u00eancia para o fato de que um im\u00f3vel com menos quarto tenha um menor pre\u00e7o, al\u00e9m do fato de que a maior parte dos im\u00f3veis n\u00e3o tem uma alta quantidade de quartos, o que parece bem realista.\n\n*median_house_value* x *total_rooms*","f2b6461e":"# Exerc\u00edcio de Regress\u00e3o - California Housing\ud83c\udfe1\n\n**Paulino Fonseca - PMR3508-2020-153**","785af1e7":"**6.3. Lasso**","a8c9a607":"**6.9. Extra Trees**","1aa407fe":"Al\u00e9m disso, antes de realizarmos a regress\u00e3o, de fato, temos que normalizar os dados da base de teste. Para isso, usaremos novamente o StandardScaler:","e6148332":"**6.1. Ordinary Least Squares**","ad820ab4":"# 7. Predi\u00e7\u00e3o#\n\nAp\u00f3s termos preparado, pr\u00e9-processado os dados e tendo escolhido o melhor regressor para o exerc\u00edcio, podemos passar para a etapa final de predi\u00e7\u00e3o do pre\u00e7o das casas de nossa base de teste. \n\nEntretanto, antes de fazermos a predi\u00e7\u00e3o, iremos primeiro adicionar as feature adicionais criadas anteriormente:","a61296a7":"**3.5. Outros gr\u00e1ficos de dispers\u00e3o**\n\nFa\u00e7amos, agora, outros *scatterplots* para avaliarmos a correla\u00e7\u00e3o de outras vari\u00e1veis importantes (que tiveram a import\u00e2ncia analisada anteriormente) para a determina\u00e7\u00e3o do *median_house_value*.\n\n*median_house_value* x *households*:","18c032dd":"**4.2. Outras features**\n\n\u00c9 importante lembrarmos do fato de que as vari\u00e1veis population, households, total_rooms e total_bedrooms apresentavam alta correla\u00e7\u00e3o. Al\u00e9m disso, essas vari\u00e1veis apresentavam certa correla\u00e7\u00e3o com o pre\u00e7o m\u00e9dio das casas. Portanto, \u00e9 natural tentarmos buscar novas vari\u00e1veis (que tenham algum significado real) a partir delas que possam ter uma correla\u00e7\u00e3o maior ainda com o pre\u00e7o m\u00e9dio das casas. Criemos ent\u00e3o essas novas vari\u00e1veis:","9c780d06":"Fa\u00e7amos o mesmo para a base de teste:","ecaaf172":"Vemos que nenhum dos atributos criados influencia significativamente no r\u00f3tulo. De todo modo, os manteremos, pois podem ajudar na regress\u00e3o.\n\nAssim, tendo adicionado todos esses atributos extras, acabamos essa etape de feature engineering.","32b1d826":"Vamos avaliar o quanto essa nova feature est\u00e1 relacionada com o pre\u00e7o da casa:","afd976ea":"Como pode ser visto pelo gr\u00e1fico, observamos uma forte rela\u00e7\u00e3o entre o atributo criado e o pre\u00e7o m\u00e9dio da casa, o que \u00e9 uma boa justificativa para o mantermos no dataframe.","10c01827":"# 6. Testando regressores\n\nNessa etapa iremos testar qual o melhor regressor para resolver o exerc\u00edcio. Faremos isso da seguinte forma:","ad07486d":"# 4. Feature Engineering\n\nNessa etapa do projeto, iremos criar algumas features que julgamos importantes para a regress\u00e3o, com base nas conclus\u00f5es obtidas da visualiza\u00e7\u00e3o de dados (etapa anterior).\n\n**4.1. *City distance***\n\nAo tra\u00e7ar o plot *Plotly* (\"mapa de calor geogr\u00e1fico\") na etapa anterior, conseguimos perceber a rela\u00e7\u00e3o entre a dist\u00e2ncia em rela\u00e7\u00e3o ao mar de um im\u00f3vel (se \u00e9 litor\u00e2neo ou n\u00e3o) e o seu pre\u00e7o. Al\u00e9m disso, percebemos tamb\u00e9m a rela\u00e7\u00e3o entre a dist\u00e3ncia em rela\u00e7\u00e3o a uma cidade grande de um im\u00f3vel e o seu pre\u00e7o. Desse modo, iremos adicionar a feature \"city distance\" ao nosso dataframe. \n\nColocando as cidades San Diego, San Francisco e Los Angeles, teremos cidades grandes e litor\u00e2neas(tr\u00eas pontos do litoral da Calif\u00f3rnia), de modo que conseguimos averiguar a quest\u00e3o de estar perto de uma cidade grande e ser ou n\u00e3o uma cidade do litoral. Isto \u00e9, im\u00f3veis pr\u00f3ximos a uma dessas tr\u00eas cidades, provavelmente, tem um alto valor. Al\u00e9m disso, tiramos tamb\u00e9m da busca a cidade Sacramento, pois, mesmo sendo uma cidade grande (capital do estado), ela n\u00e3o \u00e9 litor\u00e2nea.\n\nDesse modo, teremos o seguinte:","9d0b4630":"**6.2. Ridge Regression**","7a3a9d4a":"Aplicando a fun\u00e7\u00e3o criada ao dataframe de treino:","dce445cc":"# 5. Pr\u00e9-processamento de dados#\n\nNessa etapa, iremos realizar algumas opera\u00e7\u00f5es sobre nossa base de dados para que elas possam sofrer o processo de regress\u00e3o, de modo que esse processo tenha um resultado com menor RMSLE (Root Mean Squared Log Error) poss\u00edvel.\n\n**5.1. Normalizando os dados**\n\nIremos normalizar os dados de nossa base de treino, uma vez que os dados (todos num\u00e9ricos) apresentam intervalos muito diferentes, o que dificulta a precis\u00e3o da regress\u00e3o. Para isso, utilizaremos o StandardScaler da biblioteca scikit-learn:\n ","8c845973":"**3.3. Pairplot**\n\nVamos, agora, observar a rela\u00e7\u00e3o dos diversos atributos entre si:","ccf45542":"Podemos perceber, por esse gr\u00e1fico, a **alta correla\u00e7\u00e3o** entre a latitude e longitude do im\u00f3vel, que forma uma faixa cont\u00ednua de valores.\n\nPor outro lado, refor\u00e7ando as conclus\u00f5es que tiramos anteriormente da correla\u00e7\u00e3o entre entre *total_rooms, total_bedrooms, population e households*, o dataframe em quest\u00e3o apresenta casas que majoritariamente, n\u00e3o tem muito c\u00f4modos, nem quartos, com poucos residentes e fam\u00edlias, o que explica que o fato da maioria das casas valer menos que $400000. O que acrescenta uma nova informa\u00e7\u00e3o na correla\u00e7\u00e3o dos atributos cidados anteriormente: o fato de estarem **positivamente correlacionados com o r\u00f3tulo**."}}