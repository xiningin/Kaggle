{"cell_type":{"226b3872":"code","f45f0711":"code","ea58c94e":"code","affd8de6":"code","eb0c6173":"code","ac00f482":"code","4636cfdd":"code","d5f96e57":"code","1d2e82fe":"code","2d3e4678":"code","752894c8":"code","25eead15":"code","0a53c9e6":"code","c8c3e8e1":"code","bd7785f9":"code","ccdbbf07":"code","046ad182":"code","ede924eb":"code","7bfc8bad":"code","359f9fba":"code","fe96b9e2":"code","8e656691":"markdown","89a9f7f5":"markdown","1d82b399":"markdown","326817f2":"markdown","c178a157":"markdown","bee67c67":"markdown","6e48f569":"markdown","63431190":"markdown","cefd9172":"markdown","4ff2c38f":"markdown","7a2d3f62":"markdown","190085ac":"markdown"},"source":{"226b3872":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f45f0711":"from tensorflow.keras.datasets import mnist\nfrom matplotlib import pyplot as plt\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.callbacks import EarlyStopping, LambdaCallback\nfrom tensorflow.keras.utils import to_categorical\nimport matplotlib.pyplot as plt","ea58c94e":"(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\nx_train = x_train.astype('float')\/255.\nx_test = x_test.astype('float')\/255.\nx_train = np.reshape(x_train, (60000, 784))\nx_test = np.reshape(x_test, (10000, 784))\nx_train.shape\n\n","affd8de6":"x_train_noisy = x_train + np.random.rand(60000, 784) * 0.9\nx_test_noisy = x_test + np.random.rand(10000, 784) * 0.9\nx_train_noisy = np.clip(x_train_noisy, 0., 1.)\nx_test_noisy = np.clip(x_test_noisy, 0., 1.)","eb0c6173":"def plot(x,predictions, labels=False):\n    plt.figure(figsize=(20,2))\n    for i in range(10):\n        plt.subplot(1,10,i+1)\n        plt.imshow(x[i].reshape(28,28),cmap=\"binary\")\n        plt.xticks([])\n        plt.yticks([])\n        if labels:\n            plt.xlabel(np.argmax(p[i]))\n    plt.show()","ac00f482":"plot(x_train,None)","4636cfdd":"plot(x_train_noisy,None)","d5f96e57":"model = Sequential()\nmodel.add(Dense(units=256,activation=\"relu\", input_shape=(784,)))\nmodel.add(Dense(units=256,activation=\"relu\"))\nmodel.add(Dense(units=10,activation=\"softmax\"))\nmodel.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\",metrics=[\"accuracy\"])\nmodel.summary()","1d2e82fe":"model.fit(x=x_train, y=y_train,validation_data=(x_test,y_test), epochs=4)","2d3e4678":"model.evaluate(x_test,y_test)","752894c8":"model.evaluate(x_test_noisy,y_test)","25eead15":"input_image = Input(shape=(784,))\nencoded = Dense(units=64,activation=\"relu\")(input_image) # This will reduce the dimensionality of the image and get the most important parts\ndecoded = Dense(units=784,activation=\"sigmoid\")(encoded) # This will return 1 or 0 on the encoded pixels, so will reduce the noises.\n\nautoencoder = Model(input_image, decoded)\nautoencoder.compile(loss=\"binary_crossentropy\",optimizer=\"adam\",metrics=[\"accuracy\"])","0a53c9e6":"autoencoder.fit(x=x_train_noisy,y=x_train, epochs =100, callbacks=[EarlyStopping(monitor='val_loss', patience=5)])\n\nprint(' ***********************************************************************************')\nprint('Training is complete!')","c8c3e8e1":"predictions = autoencoder.predict(x_test_noisy)","bd7785f9":"plot(x_test_noisy, None)","ccdbbf07":"plot(predictions, None)","046ad182":"model.evaluate(predictions,y_test)","ede924eb":"noisy_image = Input(shape=(784,))\nx = autoencoder(noisy_image)\ny = model(x)\n\ndenoise_and_classify = Model(noisy_image, y)","7bfc8bad":"p = denoise_and_classify.predict(x_test_noisy)","359f9fba":"plot(x_test_noisy, p, True)","fe96b9e2":"plot(x_test_noisy, to_categorical(y_test), True)","8e656691":"## 2. Adding Noise ","89a9f7f5":"## 7. Composite Model","1d82b399":"Lets see the performance of our classifier with the denoised images. As seen below, we have almost the same accuracy as we had with the original images.","326817f2":"## 3. Building the Autoencoder","c178a157":"## 6.Denoised Images and Evaluate Performance of the Model","bee67c67":"<font color=\"green\">\nAn autoencoder is an unsupervised learning technique for neural networks that learns efficient data representations (encoding) by training the network to ignore signal \u201cnoise.\u201d Autoencoders can be used for image denoising, image compression, and, in some cases, even generation of image data.\n  \nAutoencoder gets the noisy images as input and the original images as output. This forces the model to learn the most important characteristics of the image like Principal Component Analysis.","6e48f569":"As seen our model's prediction is very low in the noisy images %97 versus %26","63431190":"We see the original images as follows:","cefd9172":"## 1. Data Preprocessing","4ff2c38f":"## 5. Training the Autoencoder","7a2d3f62":"## 4. Building the Autoencoder","190085ac":"We see the noisy images as follows:"}}