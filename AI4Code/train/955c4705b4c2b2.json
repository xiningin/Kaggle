{"cell_type":{"e1c46895":"code","ebf3275c":"code","4d33d463":"code","9f02eaad":"code","a088d323":"code","24cbd783":"code","27faaf32":"code","dbea0066":"code","3f16bb88":"code","b5e15fab":"code","f0ffa345":"code","e3db0762":"code","15ff05e4":"code","ec2dfe52":"code","c09c1098":"markdown","31c533fb":"markdown","f4426180":"markdown","de063d0d":"markdown","76ae2d41":"markdown","ffb8d0bb":"markdown","f924c7b8":"markdown","4948283f":"markdown","cf5fbcb6":"markdown","b7122c70":"markdown","034d6ba3":"markdown","fc41b015":"markdown","4dfb52e5":"markdown","c30a2103":"markdown","3d98a358":"markdown"},"source":{"e1c46895":"import os\nimport numpy as np\nimport pandas as pd\nimport xgboost as xgb\nimport collections\nimport seaborn as sns\nfrom xgboost import DMatrix\nimport matplotlib.pyplot as plt\nfrom xgboost import plot_importance, plot_tree\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import confusion_matrix, recall_score\nfrom sklearn.metrics import cohen_kappa_score\nimport lightgbm as lgb\nfrom sklearn.metrics import accuracy_score\nfrom tqdm import tqdm\nfrom scipy import signal\nimport matplotlib.pyplot as plt\nimport random\nimport numpy as np\nfrom statistics import mean, mode\n# \u30c1\u30e3\u30fc\u30c8\u7528\u8a2d\u5b9a\n# setting for chart\nimport plotly as py\nimport plotly.io as pio\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nfrom warnings import simplefilter\nsimplefilter(action='ignore', category=FutureWarning)\nsimplefilter(action='ignore', category=DeprecationWarning)\n\ninit_notebook_mode(connected=True)\nlayout=go.Layout(paper_bgcolor='rgba(0,0,0,0)', plot_bgcolor='rgba(250,250,250,0.8)')\nfig = go.Figure(layout=layout)\ntemplated_fig = pio.to_templated(fig)\npio.templates['my_template'] = templated_fig.layout.template\npio.templates.default = 'my_template'","ebf3275c":"df = pd.read_csv(\"\/kaggle\/input\/topix4\/TOPIX4.csv\", names=(\"Date\", \"Open\", \"High\", \"Low\", \"Close\")).drop(0)\nprediction_day_count = 150\ntrain_day_count = 250\ndf = df.loc[len(df) - prediction_day_count - train_day_count - 34:].reset_index().drop('index', 1)","4d33d463":"# df = df.loc[len(df) - prediction_day_count - train_day_count - 34 -1:].reset_index().drop('index', 1)\n# index_funds = ['N225', 'GSPC', 'EZU']\nindex_funds = []\n# columns_added_by_index_funds = ['N225_Close', 'GSPC_Close', 'EZU_Close']\n# columns_added_by_index_funds = ['N225_Close', 'GSPC_Close', 'EZU_Close', 'N225_RSI', 'GSPC_RSI', 'EZU_RSI']\ncolumns_added_by_index_funds = []\n# for index_fund in index_funds:\n#     index_df = pd.read_csv(\"\/kaggle\/input\/stock-price-datas\/\" + index_fund + \".csv\", names=(\"Date\", \"Open\", \"High\", \"Low\", index_fund+\"_Close\", \"Adj Close\", \"Volume\")).drop(0)\n#     index_df = index_df.drop([\"Open\", \"High\", \"Low\", \"Adj Close\", \"Volume\"], 1)\n#     index_df[\"Date\"] = index_df[\"Date\"].str.replace('-', '\/')\n#     df = pd.merge(df, index_df, on=\"Date\", how='left')\n#     if (index_fund != 'N225'):\n#         df[index_fund+\"_Close\"] = df[index_fund+\"_Close\"].shift()\n# df = df[1:]      # Because of shifting close price\n# df = df.reset_index().drop(\"index\", 1)","9f02eaad":"def flag_features(df):\n    for num in ['5', '10', '15', '30']:\n        # \u79fb\u52d5\u5e73\u5747\u7dda\u3068\u79fb\u52d5\u6a19\u6e96\u504f\u5dee\n        # moving average and std of it\n        df['SMA_' + num] = df['Close'].rolling(int(num)).mean().shift() \/ df['Close']\n        df['SMA_' + num + '_std'] = df['Close'].rolling(int(num)).std().shift() \/ df['Close']\n        col_name = 'SMA_' + num + '_sub'\n        today_col_name = 'flag_today_sma_' + num\n        yesterday_col_name = 'flag_yesterday_sma_' + num\n        df[col_name] = df['Close'].rolling(int(num)).mean().shift()\n        df.loc[df[col_name] < df['Close'], today_col_name] = 1\n        df.loc[df[col_name] >= df['Close'], today_col_name] = 0\n        df[yesterday_col_name] = df[today_col_name].shift(1)\n        # \u30d5\u30e9\u30b0\u3092\u4f5c\u6210\n        # make flag\n        df.loc[(df[yesterday_col_name] == 0) & (df[today_col_name] == 1), \"flag_sma_under_\" + num] = 1\n        df.loc[~((df[yesterday_col_name] == 0) & (df[today_col_name] == 1)), \"flag_sma_under_\" + num] = 0\n        df.loc[(df[yesterday_col_name] == 1) & (df[today_col_name] == 0), \"flag_sma_over_\" + num] = 1\n        df.loc[~((df[yesterday_col_name] == 1) & (df[today_col_name] == 0)), \"flag_sma_over_\" + num] = 0\n        df = df.drop([col_name, yesterday_col_name, today_col_name], 1)\n    up_flag = {'5' : 0, '10': 0, '15' : 0, '30' : 0}\n    down_flag = {'5' : 0, '10': 0, '15' : 0, '30' : 0}\n    # \u30d5\u30e9\u30b0\u304b\u3089\u7279\u5b9a\u306e\u8ddd\u96e2\u306b\u3042\u308b\u304b\u30c1\u30a7\u30c3\u30af\n    # check distance from flag\n    for i in range(len(df)):\n        for num in ['5', '10', '15', '30']:\n            up_column = \"up_flag_distance_\" + num\n            down_column = \"down_flag_distance_\" + num\n            df.loc[i, up_column] = 0\n            df.loc[i, down_column] = 0\n            if up_flag[num] > int(num):\n                up_flag[num] = 0\n                df.loc[i, \"up_flag_distance_\" + num] = 0\n            elif up_flag[num] > 0:\n                df.loc[i, \"up_flag_distance_\" + num] = up_flag[num]\n                up_flag[num] += 1\n\n            if down_flag[num] > int(num):\n                down_flag[num] = 0\n                df.loc[i, \"down_flag_distance_\" + num] = 0\n            elif down_flag[num] > 0:\n                df.loc[i, \"down_flag_distance_\" + num] = down_flag[num]\n                down_flag[num] += 1\n\n            if df.loc[i, \"flag_sma_under_\" + num] == 1:\n                df.loc[i, \"down_flag_distance_\" + num] = 1\n                down_flag[num] = 2\n            if df.loc[i, \"flag_sma_over_\" + num] == 1:\n                df.loc[i, \"up_flag_distance_\" + num] = 1\n                up_flag[num] = 2\n    for num in ['5', '10', '15', '30']:\n        df = df.drop([\"flag_sma_over_\" + num, \"flag_sma_under_\" + num], 1)\n    return df","a088d323":"def relative_strength_idx(df, close_column = \"Close\", n=14):\n    close = df[close_column]\n    delta = close.diff()\n    delta = delta[1:]\n    pricesUp = delta.copy()\n    pricesDown = delta.copy()\n    pricesUp[pricesUp < 0] = 0\n    pricesDown[pricesDown > 0] = 0\n    rollUp = pricesUp.rolling(n).mean()\n    rollDown = pricesDown.abs().rolling(n).mean()\n    rs = rollUp \/ rollDown\n    rsi = 100.0 - (100.0 \/ (1.0 + rs))\n    return rsi","24cbd783":"def wt( data ):\n    widths = np.arange(1, 100)\n    return signal.cwt( data, signal.ricker, widths )","27faaf32":"def change_rate(df):\n    # \u8a55\u4fa1\u306b\u7d42\u5024\u306e\u5909\u5316\u7387\u3092\u4f7f\u3046\u305f\u3081\n    # qualtile([0.2, 0.4, 0.6, 0.8])\u3067\u95be\u5024\u3092\u6c7a\u5b9a\n    # \u5927\u304d\u306a\u30d7\u30e9\u30b9\u30922\u3068\u3057\u3066\u3001\u5c0f\u3055\u306a\u30d7\u30e9\u30b9\u30921 \u2192\u3000\u5927\u304d\u306a\u30de\u30a4\u30ca\u30b9\u3092-2\n    \n    # objective variable is change rate of close\n    # devide change rate to label by quantile\n    # big plus is 2 and small plus is 1 ... bug minus is -2\n    df['Change_rate'] = df['Close'].pct_change()\n    divide = df['Change_rate'].quantile([0, 0.2, 0.4, 0.6, 0.8, 1.0])\n    df.loc[(df['Change_rate'] > divide[0.4]) & (df['Change_rate'] < divide[0.6]), 'Change_label'] = 0\n    df.loc[(df['Change_rate'] > divide[0.4]) & (df['Change_rate'] < divide[0.6]), 'weight'] = 1\n    df.loc[(df['Change_rate'] >= divide[0.2]) & (df['Change_rate'] <= divide[0.4]), 'Change_label'] = -1\n    df.loc[(df['Change_rate'] >= divide[0.2]) & (df['Change_rate'] <= divide[0.4]), 'weight'] = 1.2\n    df.loc[df['Change_rate'] < divide[0.2], 'Change_label'] = -2\n    df.loc[df['Change_rate'] < divide[0.2], 'weight'] = 1.5\n    df.loc[(df['Change_rate'] <= divide[0.8]) & (df['Change_rate'] >= divide[0.6]), 'Change_label'] = 1\n    df.loc[(df['Change_rate'] <= divide[0.8]) & (df['Change_rate'] >= divide[0.6]), 'weight'] = 1.2\n    df.loc[df['Change_rate'] > divide[0.8], 'Change_label'] = 2\n    df.loc[df['Change_rate'] > divide[0.8], 'weight'] = 1.5\n    df = df.reset_index()\n    return df.drop('index', 1)","dbea0066":"def ready_features(df):\n    # \u30c7\u30fc\u30bf\u306e\u8aad\u307f\u8fbc\u307f\n    # read csv and modify type of columns\n    df['Date'] = pd.to_datetime(df['Date'])\n    df[\"Open\"] = df['Open'].astype(float)\n    df[\"Close\"] = df['Close'].astype(float)\n    df[\"High\"] = df['High'].astype(float)\n    df[\"Low\"] = df['Low'].astype(float)\n    for index_fund in index_funds:\n        df[index_fund+\"_Close\"] = df[index_fund+\"_Close\"].astype(float)\n    # \u6307\u6570\u5e73\u6ed1\u79fb\u52d5\u5e73\u5747\n    # Exponential Moving Average\n    # \u6307\u6570\u5e73\u6ed1\u79fb\u52d5\u5e73\u5747\u306e\u65e5\u672c\u8a9e\u306e\u8aac\u660e\u2193\n    # https:\/\/media-kojirokousi.com\/exponential-moving-average\/#:~:text=%E6%8C%87%E6%95%B0%E5%B9%B3%E6%BB%91%E7%A7%BB%E5%8B%95%E5%B9%B3%E5%9D%87%E7%B7%9A(EMA)%E3%81%AF%E3%80%81%E5%BE%93%E6%9D%A5%E3%81%AE,EMA%E3%81%A8%E5%91%BC%E3%81%B0%E3%82%8C%E3%81%BE%E3%81%99%E3%80%82\n    df['EMA_9'] = df['Close'].ewm(9).mean().shift() \/ df['Close']\n    df['EMA_9_std'] = df['Close'].ewm(9).std().shift() \/ df['Close']\n    # MACD\n    # MACD\u306e\u65e5\u672c\u8a9e\u306e\u8aac\u660e\u2193\n    # https:\/\/www.sevendata.co.jp\/shihyou\/technical\/macd.html\n    EMA_12 = pd.Series(df['Close'].ewm(span=12, min_periods=12).mean())\n    EMA_26 = pd.Series(df['Close'].ewm(span=26, min_periods=26).mean())\n    df['MACD'] = pd.Series(EMA_12 - EMA_26)\/ df['Close']\n    df['MACD_signal'] = pd.Series(df.MACD.ewm(span=9, min_periods=9).mean())\/ df['Close']\n    df['RSI'] = relative_strength_idx(df).fillna(0)\n    df = flag_features(df)\n    df = change_rate(df)\n    #\u2193\u4ed6\u306e\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u30d5\u30a1\u30f3\u30c9\u306e\u30c7\u30fc\u30bf\u3082\u540c\u69d8\u306e\u7279\u5fb4\u91cf\u3092\u751f\u6210\u3057\u3066\u3044\u308b\n    #\u2193create same features from other index-fund data\n    for index_fund in index_funds:\n        close_column = index_fund+\"_Close\"\n        rsi_column = index_fund +\"_RSI\"\n        df[index_fund + '_EMA_9'] = df[close_column].ewm(9).mean().shift() \/ df[close_column]\n        df[index_fund + '_EMA_9_std'] = df[close_column].ewm(9).std().shift() \/ df[close_column]\n        EMA_12 = pd.Series(df[close_column].ewm(span=12, min_periods=12).mean())\n        EMA_26 = pd.Series(df[close_column].ewm(span=26, min_periods=26).mean())\n        df[rsi_column] = relative_strength_idx(df, close_column).fillna(0)\n        column_wave = wt(df[close_column])\n        for num in ['5', '10', '15', '30']:\n            df[close_column + '_wave_' + num] = column_wave[int(num)]\n        rsi_wave = wt(df[rsi_column])\n        for num in ['5', '10', '15', '30']:\n            df[rsi_column + '_wave_' + num] = rsi_wave[int(num)]\n\n    # wavelet\u89e3\u6790\n    # wavelet\n    for column in ['Close', 'RSI']:\n        column_wave = wt(df[column])\n        for num in ['5', '10', '15', '30', '45', '50', '75','98']:\n            df[column + '_wave_' + num] = column_wave[int(num)]\n    return df\n\n\ndf = ready_features(df)\ndf.tail()","3f16bb88":"def ready_data_for_train(df_for_ready, day_count): \n    df_for_ready = modify_weight_based_on_day_count(df_for_ready, day_count)\n    df_for_ready, train_df, valid_df, test_df = modify_and_split_data_for_xgb(df_for_ready, day_count)\n    X_train, y_train = get_label_and_explanatory_variable(train_df)\n    X_valid, y_valid = get_label_and_explanatory_variable(valid_df)\n    X_test, y_test = get_label_and_explanatory_variable(test_df)\n    eval_set = [(X_train, y_train), (X_valid, y_valid)]\n    weights = X_train[\"weight\"].astype('float')\n    X_train = X_train.drop(\"weight\", axis=1)\n    X_valid = X_valid.drop(\"weight\", axis=1)\n    X_test = X_test.drop(\"weight\", axis=1)\n    eval_set = [(X_train, y_train), (X_valid, y_valid)]\n    return eval_set, X_train, y_train, X_valid, y_valid, X_test, y_test, weights, df_for_ready, test_df\n    \ndef modify_weight_based_on_day_count(df_for_ready, day_count):\n    df_for_ready = df_for_ready.reset_index()\n    df_for_ready.loc[df_for_ready['weight'] - 0.5 > df_for_ready['weight'] - 0.7 + 1.2 * df_for_ready['index']\/len(df_for_ready), 'weight'] = df_for_ready['weight'] - 0.5\n    df_for_ready.loc[df_for_ready['weight'] - 0.5 <= df_for_ready['weight'] - 0.7 + 1.2 * df_for_ready['index']\/len(df), 'weight'] = df_for_ready['weight'] - 0.7 + 1.2 * df_for_ready['index']\/len(df_for_ready)\n    return df_for_ready.drop('index', 1)\n    \ndef modify_and_split_data_for_xgb(df_for_ready, day_count):\n    # \u524d\u65e5\u306e\u30c7\u30fc\u30bf\u304b\u3089\u3001\u7fcc\u65e5\u306e\u7d42\u5024\u3092\u4e88\u6e2c\u3059\u308b\u305f\u3081\u3001close\u3092shift\u3057\u3066\u3044\u308b\u3002\n    # \u2193we would like to predict tommorow's movement\n    df_for_ready['Change_label'] = df_for_ready['Change_label'].shift(-1)\n    df_for_ready['Change_rate'] = df_for_ready['Change_rate'].shift(-1)\n    df_for_ready['weight'] = df_for_ready['weight'].shift(-1)\n\n    df_for_ready = df_for_ready.iloc[33:] # Because of moving averages and MACD line\n    df_for_ready = df_for_ready[:-1]      # Because of shifting close price\n    df_for_ready[\"Change_label\"] = df_for_ready[\"Change_label\"].astype('int')\n    df_for_ready = df_for_ready.reset_index()\n    df_for_ready = df_for_ready.drop(['index'], 1)\n    test_df = df_for_ready.loc[len(df_for_ready) - prediction_day_count + day_count: len(df_for_ready) - prediction_day_count + day_count].copy()\n    train_df = df_for_ready.loc[: len(df_for_ready) - prediction_day_count + day_count - 1].copy()\n    train_df, valid_df = train_test_split(train_df, test_size=0.19, stratify=train_df.Change_label)\n    return df_for_ready,train_df, valid_df, test_df\ndef get_label_and_explanatory_variable(df_for_ready):\n    drop_cols = ['Date', 'Open', 'Low', 'High', 'Close', 'Change_rate']\n    df_for_ready = df_for_ready.drop(drop_cols, 1)\n    df_for_ready = df_for_ready.drop(columns_added_by_index_funds, 1)\n    label = df_for_ready['Change_label'].copy()\n    explanatory = df_for_ready.drop(columns='Change_label')\n    return explanatory, label","b5e15fab":"result = df[df.Date == \"aa\"].copy().reset_index().drop(['index'], 1)\n#\u2191\u30c7\u30fc\u30bf\u306e\u30d5\u30ec\u30fc\u30e0\u306e\u307f\u3092\u30b3\u30d4\u30fc\n#\u2191copy frame only\nensemble_count = 10\nfor i in tqdm(range(prediction_day_count)):\n    predicts = []\n    for j in range(ensemble_count):\n        eval_set, X_train, y_train, X_valid, y_valid, X_test, y_test, weights, ans_df, test_df = ready_data_for_train(df.loc[i:].copy(), i)\n        model = xgb.XGBClassifier(n_estimators=200, \n            learning_rate=0.05,\n            max_depth= 8,\n            gamma=0.02,\n            random_state= 42,\n            num_class =5,\n            objective='multi:softprob')\n        model.fit(X_train, y_train, eval_set=eval_set, verbose=False, sample_weight = weights)\n        predicts.append(model.predict(X_test)[0])\n        if ensemble_count - 1 == j:\n            test_df['pred'] = collections.Counter(predicts).most_common()[0][0]\n    result = pd.concat([result, test_df.reset_index()])\nresult = result.reset_index().drop(['index', 'level_0'], 1)\nacc = accuracy_score(result['Change_label'], result['pred'])\nprint(acc)","f0ffa345":"def simulate_based_on_predict(sim_data):\n    money_his = pd.DataFrame(columns = ['Date', 'money'])\n    money = 1\n    for i in range(0, len(result)):\n        data = sim_data.loc[i].copy()\n        today_pred = data.pred\n        Change_rate = data.Change_rate\n        if (today_pred == -2):\n            money = (1 + Change_rate * -1) * money\n        elif (today_pred == -1):\n            money = (1 + Change_rate * -0.5) * money\n        elif (today_pred == 1):\n            money = (1 + Change_rate * 0.5) * money\n        elif (today_pred == 2):\n            money = (1 + Change_rate * 1) * money\n        money_his.loc[data.Date] = [data.Date, money]\n    return money, money_his\nmoney, money_his = simulate_based_on_predict(result.copy())\nprint(money)\nfig = go.Figure(go.Scatter(x=money_his.Date, y=money_his.money, name='money_history'))\nfig.show()","e3db0762":"print(confusion_matrix(result['Change_label'], result['pred']))\nsns.heatmap(confusion_matrix(result['Change_label'], result['pred']))","15ff05e4":"fig, ax = plt.subplots(1,1,figsize=(10, 10))\nplot_importance(booster=model, ax=ax)","ec2dfe52":"fig = make_subplots(rows=1, cols=1)\nfig.add_trace(go.Scatter(x=result.Date, y=result.Change_label,\n                         name='Truth'), row=1, col=1)\n\nfig.add_trace(go.Scatter(x=result.Date,\n                         y=result.pred,\n                         name='Prediction'), row=1, col=1)\n\nfig.show()","c09c1098":"## \u65b9\u91dd\n\u904e\u53bb250\u65e5\u9593\u306e\u30c7\u30fc\u30bf\u3092\u7528\u3044\u3066xgb\u3092\u5b66\u7fd2\u3057251\u65e5\u76ee\u306e\u682a\u4fa1\u306e\u5909\u5316\u3092\u4e88\u6e2c\u3059\u308b\u3002  \n\u2192\u3053\u308c\u3092\u4e88\u6e2c\u3057\u305f\u3044\u65e5\u6570\u5206\u7e70\u308a\u8fd4\u3059\n\u4ee5\u4e0b\u540c\u69d8\u306b  \n```\npredict = []\nfor (int i = first_predict_day; i < final_day; i++) {\n  model.train(data[(i - 251) ~ i - 1])\n  predict[i] = model.predict(data[i])\n}\n```\n\u306e\u3088\u3046\u306a\u5f62\u3067\u884c\u3046\n\n## plan\ntrain XGB by recent (i - 250)~ i days data and predict (i+1)-day Change rate  \n\u2192train XGB by recent (i - 251 + 1)~ (i + 1) days data and predict (i + 1 + 1)-day Change rate  \n\u2192predict all day as above\n```\npredict = []\nfor (int i = first_predict_day; i < final_day; i++) {\n  model.train(data[(i - 251) ~ i - 1])\n  predict[i] = model.predict(data[i])\n}\n```","31c533fb":"### \u30d5\u30e9\u30b0\u306e\u4f5c\u6210 \/ make flag\n\u30b4\u30fc\u30eb\u30c7\u30f3\u30af\u30ed\u30b9\u306e\u7c21\u6613\u7248\u3067\u3059  \nsimple version of golden cross  ","f4426180":"## \u305d\u306e\u4ed6\u306e\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u30d5\u30a1\u30f3\u30c9\u306e\u30c7\u30fc\u30bf\u3092\u53d6\u5f97\u3059\u308b\/prepare other index-fund\ntopix\u4ee5\u5916\u306edata\u3082\u4e88\u6e2c\u306b\u7528\u3044\u308b\u3053\u3068\u306b\u3059\u308b\u3002  \n\u30b3\u30e1\u30f3\u30c8\u30a2\u30a6\u30c8\u3092\u3059\u308b\u304b\u3001\u3057\u306a\u3044\u304b\u306b\u3088\u3063\u3066\u7528\u3044\u308b\u304b\u3069\u3046\u304b\u3092\u6c7a\u5b9a\u3059\u308b\u3002  \nuse other index-fund data  \nswitch use or unuse by comment-out","de063d0d":"## \u30b7\u30df\u30e5\u30ec\u30fc\u30b7\u30e7\u30f3 \/ simulation\n\u640d\u76ca\u30b7\u30df\u30e5\u30ec\u30fc\u30b7\u30e7\u30f3\u3092\u4ee5\u4e0b\u306e\u5f62\u3067\u884c\u3046  \nsimulate profit and loss as below  \n\nW_0 = 1\n\nW_{t+1} =  ( 1 + r_{t+1} * w_t ) W_t\n\n### \u8a18\u53f7\n\n* W_t : \u6642\u523bt\u3067\u306e\u4fdd\u6709\u8cc7\u7523\u984d \/ money at t-time  \n* r_t : \u6642\u523b{t-1} \u304b\u3089 \u6642\u523bt\u307e\u3067\u306eTOPIX\u5909\u5316\u7387 \/ change rate from (t-1)-time to t-time  \n* w_t : \u81ea\u5206\u306e\u6295\u8cc7\u6bd4\u7387 \/ investment rate  \n\n### \u4e88\u6e2c\u3068\u6295\u8cc7\u6bd4\u7387\u306e\u95a2\u4fc2 \/ decide investment rate by prediction\n* +2 : \u5927\u5e45\u306aTOPIX\u4e0a\u6607\/big plus  => w_t =  1.0 \n* +1 : \u5c0f\u5e45\u306aTOPIX\u4e0a\u6607\/small plus  => w_t =  0.5\n*  0 : TOPIX\u5909\u5316\u306a\u3057\/no channge  \u3000=> w_t =  0.0\n* -1 : \u5c0f\u5e45\u306aTOPIX\u4e0b\u843d\/small minus  => w_t = -0.5\n* -2 : \u5927\u5e45\u306aTOPIX\u4e0b\u843d\/big minus  => w_t = -1.0","76ae2d41":"## ready features","ffb8d0bb":"## \u5b66\u7fd2\u3068\u4e88\u6e2c\/train and prediction\nxgb\u3092\u7528\u3044\u3066\u5b66\u7fd2\u3059\u308b\u3002  \n\u307e\u305f\u3001\u521d\u671f\u72b6\u614b\u306b\u4f9d\u5b58\u3057\u3066\u3057\u307e\u3046\u3053\u3068\u304b\u3089\u3001\u30a2\u30f3\u30b5\u30f3\u30d6\u30eb\u5b66\u7fd2\u3092\u884c\u3046\u3053\u3068\u306b\u3059\u308b\u3002  \npredict by xgb  \nprediction is influenced by initial state, prevent influence by using ensemble learning","f924c7b8":"### wavelet analysis \/ \u30a6\u30a7\u30fc\u30d6\u30ec\u30c3\u30c8\u89e3\u6790\nWavelet coefficient represents periodic nature of time-series data.  \n\u30a6\u30a7\u30fc\u30d6\u30ec\u30c3\u30c8\u4fc2\u6570\u306f\u6642\u7cfb\u5217\u30c7\u30fc\u30bf\u306e\u5468\u671f\u7684\u306a\u6027\u8cea\u3092\u8868\u3057\u3066\u3044\u307e\u3059\u3002","4948283f":"## \u521d\u671f\u8a2d\u5b9a\/setting\n\u76f4\u8fd1150\u65e5\u9593\u306e\u4e88\u6e2c\u3092\u884c\u3046\u3053\u3068\u306b\u3059\u308b\u3002  \npredict 150days","cf5fbcb6":"### ready simple features \/ \u5358\u7d14\u306a\u8aac\u660e\u5909\u6570\u306e\u4f5c\u6210\nMACD and EMA... is very easy to create.\nIn this method we make easy features.\n\n\u7c21\u5358\u306b\u4f5c\u6210\u3067\u304d\u308b\u7279\u5fb4\u91cf\u306f\u3053\u3053\u3067\u4f5c\u6210\u3057\u3066\u3044\u307e\u3059\u3002","b7122c70":"\ud83d\udc4d**\u3053\u306enotebook\u304c\u5f79\u306b\u7acb\u3063\u305f\u65b9\u306f\u662f\u975e\u3044\u3044\u306d\u3092\u304a\u9858\u3044\u3057\u307e\u3059**\ud83d\udc4d  \n\ud83d\udc4d**please upvote**\ud83d\udc4d  \nthis notebook is created from https:\/\/www.kaggle.com\/takahiro1127\/topix-prediction-of-topix-stock-price  \n\u3053\u306enotebook\u306f\u3053\u3061\u3089\u306enotebook https:\/\/www.kaggle.com\/takahiro1127\/topix-prediction-of-topix-stock-price \u306e\u767a\u5c55\u7cfb\u3068\u3057\u3066\u4f5c\u6210\u3057\u307e\u3057\u305f\u3002","034d6ba3":"### RSI \/ \u76f8\u5bfe\u529b\u6307\u6570\n\u76f8\u5bfe\u7684\u306b\u4e0a\u6607\u65b9\u5411\u306a\u306e\u304b\u4e0b\u964d\u65b9\u5411\u306a\u306e\u304b\u3092\u793a\u3059\u6307\u6570\u3067\u3059\u3002  \nrelative strength of up or down","fc41b015":"### make objective variable \/ \u76ee\u7684\u5909\u6570\u306b\u4f5c\u6210\nmake objective variable and assign weight based on objective variable(change label)  \n\u76ee\u7684\u95a2\u6570\u3092\u4f5c\u6210\u3057\u307e\u3059\u3002\u307e\u305f\u3001\u5909\u5316\u7387\u306b\u57fa\u3065\u3044\u3066\u5b66\u7fd2\u306e\u91cd\u307f\u4ed8\u3051\u3092\u884c\u3044\u307e\u3059\u3002\n\n#### weight\nbig change rate gives me big profit(or loss).  \nwe must focus on big change.  \nset heavy weight on big change.  \n\n\u5927\u304d\u306a\u5909\u5316\u306f\u5927\u304d\u306a\u5229\u76ca\u3084\u640d\u5931\u3092\u3082\u305f\u3089\u3059\u305f\u3081\u3001\u9593\u9055\u3048\u305f\u6642\u306e\u30ea\u30b9\u30af\u304c\u5927\u304d\u3044\u3067\u3059\u3002  \n\u3088\u3063\u3066\u3001\u5927\u304d\u306a\u5909\u5316\u307b\u3069\u4e88\u6e2c\u6027\u80fd\u304c\u3042\u304c\u308b\u3088\u3046\u306b\u91cd\u307f\u4ed8\u3051\u3092\u3057\u307e\u3059\u3002","4dfb52e5":"## EDA \/ \u30c7\u30fc\u30bf\u63a2\u7d22\ndata visualization and data analysis is here https:\/\/www.kaggle.com\/takahiro1127\/topix-prediction-of-topix-stock-price  \n\u30c7\u30fc\u30bf\u63a2\u7d22\u3068\u53ef\u8996\u5316\u306f\u3053\u3061\u3089\u306enote book\u306b\u3042\u308a\u307e\u3059\u3002\u3000https:\/\/www.kaggle.com\/takahiro1127\/topix-prediction-of-topix-stock-price \n","c30a2103":"### ready data for train \/ train\u7528\u306b\u30c7\u30fc\u30bf\u3092\u4f5c\u6210","3d98a358":"# stock price prediction\n![](https:\/\/miro.medium.com\/max\/2560\/0*R5pC0bAlYxH_nTlF.jpg)"}}