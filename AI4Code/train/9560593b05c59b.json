{"cell_type":{"f2a1785a":"code","ac57c602":"code","52c627e2":"code","e61ee37a":"code","fa0a35dd":"code","b7a64956":"code","09eaab2f":"code","aff34d62":"code","22270d82":"code","e615fcad":"code","c33551ae":"code","cf0f0b7e":"code","8f040e4b":"code","d1a319c4":"code","7641202d":"code","3e7de046":"code","d6de34f4":"code","abb47e10":"code","5c91944a":"code","884e1ed7":"code","f58c7bd4":"code","58acb79f":"code","b05aff66":"markdown"},"source":{"f2a1785a":"import numpy as np\nimport pandas as pd\nimport os\nimport tokenizers\nimport string\nimport torch\nimport transformers\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom tqdm import tqdm\nimport pickle\nimport re\nimport string\n","ac57c602":"MAX_LEN = 168\nVALID_BATCH_SIZE = 8\nEPOCHS = 5\nROBERTA_PATH = \"..\/input\/roberta-base\"\nROBERTA_PATH_NEW = \"..\/input\/rb-base\"\n\nTOKENIZER = tokenizers.ByteLevelBPETokenizer(\n    vocab_file=f\"{ROBERTA_PATH}\/vocab.json\", \n    merges_file=f\"{ROBERTA_PATH}\/merges.txt\", \n    lowercase=True,\n    add_prefix_space=True\n)","52c627e2":"class TweetModel(transformers.BertPreTrainedModel):\n    def __init__(self, conf):\n        super(TweetModel, self).__init__(conf)\n        self.roberta = transformers.RobertaModel.from_pretrained(ROBERTA_PATH, config=conf)\n        self.drop_out = nn.Dropout(0.3)\n        self.l0 = nn.Linear(768, 2)\n        torch.nn.init.normal_(self.l0.weight, std=0.02)\n    \n    def forward(self, ids, mask, token_type_ids):\n        seq, pooled = self.roberta(\n            ids,\n            attention_mask=mask,\n            token_type_ids=token_type_ids\n        )\n\n        # out = torch.cat((out[-1], out[-2]), dim=-1)\n        out = self.drop_out(seq)\n        logits = self.l0(out)\n\n        start_logits, end_logits = logits.split(1, dim=-1)\n\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n\n        return start_logits, end_logits","e61ee37a":"link_re = re.compile('http[s]?:\/\/\\S+')\nre_username = '^(\\_)\\w+'\nre_username2 = '^@(\\_)\\w+'\n\n\ndef clean_text(text, sentiment):\n    # cleaned_text = text.strip()\n    cleaned_text = \" \".join(str(text).split()).strip()\n    if sentiment != 'neutral':\n        if '_it_good' in text or '_in_love' in text or '_violence' in text:\n            return text\n\n        if re.search(re_username, cleaned_text):\n            cleaned_text = re.sub(re_username, '', cleaned_text).strip()\n        if re.search(re_username2, cleaned_text):\n            cleaned_text = re.sub(re_username2, '', cleaned_text).strip()\n    \n        if re.search(link_re, cleaned_text): \n            split_text = text.split()\n            if re.search(link_re, split_text[0]) or re.search(link_re, split_text[-1]):\n                cleaned_text = re.sub('http[s]?:\/\/\\S+', '', text).strip()\n\n        if cleaned_text.split()[0] == '-':\n            cleaned_text = cleaned_text[1:].strip()\n\n        if 'these dogs are going to die if somebody doesn`t save them!' in cleaned_text:\n            cleaned_text = 'these dogs are going to die if somebody doesn`t save them!'\n        elif cleaned_text == 'imo':\n            cleaned_text = 'Thanks'\n        elif cleaned_text == 'and_jay hi! **** your job!':\n            cleaned_text = 'hi! **** your job!'\n        elif cleaned_text == 'beckett Thanks':\n            cleaned_text = 'Thanks'\n        elif 'c Thank' in cleaned_text:\n            cleaned_text = 'Thank' \n        elif cleaned_text == 'hall no i was gutted when he wasn`t. lmao. i think i`m obsessed with him, bahaha.':\n            cleaned_text = 'no i was gutted when he wasn`t. lmao. i think i`m obsessed with him, bahaha.'\n\n    # print('Cleaned', cleaned_text)\n\n    return cleaned_text","fa0a35dd":"def process_data(tweet, selected_text, sentiment, tokenizer, max_len):\n    \n    tweet = \" \" + \" \".join(str(tweet).split())\n    selected_text = \" \" + \" \".join(str(selected_text).split())\n    \n    \n    len_st = len(selected_text) - 1\n    idx0 = None\n    idx1 = None\n\n    for ind in (i for i, e in enumerate(tweet) if e == selected_text[1]):\n        if \" \" + tweet[ind: ind+len_st] == selected_text:\n            idx0 = ind\n            idx1 = ind + len_st - 1\n            break\n\n    char_targets = [0] * len(tweet)\n    if idx0 != None and idx1 != None:\n        for ct in range(idx0, idx1 + 1):\n            char_targets[ct] = 1\n    \n    tok_tweet = tokenizer.encode(tweet)\n    input_ids_orig = tok_tweet.ids\n    tweet_offsets = tok_tweet.offsets\n    \n    target_idx = []\n    for j, (offset1, offset2) in enumerate(tweet_offsets):\n        if sum(char_targets[offset1: offset2]) > 0:\n            target_idx.append(j)\n    \n    targets_start = target_idx[0]\n    targets_end = target_idx[-1]\n\n    sentiment_id = {\n        'positive': [1313],\n        'negative': [2430],\n        'neutral': [7974]\n    }\n\n    \n    input_ids = [0] + sentiment_id[sentiment] + [2] + [2] + input_ids_orig + [2]\n    token_type_ids = [0, 0, 0, 0] + [0] * (len(input_ids_orig) + 1)\n    mask = [1] * len(token_type_ids)\n    tweet_offsets = [(0, 0)] * 4 + tweet_offsets + [(0, 0)]\n    targets_start += 4\n    targets_end += 4\n\n\n    padding_length = max_len - len(input_ids)\n    if padding_length > 0:\n        input_ids = input_ids + ([1] * padding_length)\n        mask = mask + ([0] * padding_length)\n        token_type_ids = token_type_ids + ([0] * padding_length)\n        tweet_offsets = tweet_offsets + ([(0, 0)] * padding_length)\n    \n    return {\n        'ids': input_ids,\n        'mask': mask,\n        'token_type_ids': token_type_ids,\n        'targets_start': targets_start,\n        'targets_end': targets_end,\n        'orig_tweet': tweet,\n        'orig_selected': selected_text,\n        'sentiment': sentiment,\n        'offsets': tweet_offsets\n    }","b7a64956":"\nclass TweetDataset:\n    def __init__(self, tweet, sentiment, selected_text):\n        self.tweet = tweet\n        self.sentiment = sentiment\n        self.selected_text = selected_text\n        self.tokenizer = TOKENIZER\n        self.max_len = MAX_LEN\n    \n    def __len__(self):\n        return len(self.tweet)\n\n    def __getitem__(self, item):\n        data = process_data(\n            self.tweet[item], \n            self.selected_text[item], \n            self.sentiment[item],\n            self.tokenizer,\n            self.max_len\n        )\n\n        return {\n            'ids': torch.tensor(data[\"ids\"], dtype=torch.long),\n            'mask': torch.tensor(data[\"mask\"], dtype=torch.long),\n            'token_type_ids': torch.tensor(data[\"token_type_ids\"], dtype=torch.long),\n            'targets_start': torch.tensor(data[\"targets_start\"], dtype=torch.long),\n            'targets_end': torch.tensor(data[\"targets_end\"], dtype=torch.long),\n            'orig_tweet': data[\"orig_tweet\"],\n            'orig_selected': data[\"orig_selected\"],\n            'sentiment': data[\"sentiment\"],\n            'offsets': torch.tensor(data[\"offsets\"], dtype=torch.long)\n        }","09eaab2f":"punc = \"!.\"\ndef postprocess(text, predicted_text):\n    # print(text)\n    # print(predicted_text)\n    splitted = text.split(predicted_text)[0]\n    sub = len(splitted) - len(\" \".join(splitted.split()))\n    \n#     ## new condition\n#     if sub  == 1 and text.strip() != predicted_text.strip() and text[0] == \" \":\n#         start = text.find(predicted_text)\n#         end = start + len(predicted_text)\n#         start = start - 2 if start > 1 and text[start - 1] == \" \" else start - 1\n#         end = end if (len(text) > end and text[end] == \" \") or len(text) == end else end - 1\n#         predicted_text = text[start: end]\n\n    splitted1 = text.split(predicted_text.strip())[0]\n    sub1 = len(splitted1) - len(\" \".join(splitted1.split()))\n    \n    if sub1 == 2 and text.strip() != predicted_text.strip() and text[:2] == \"  \":\n        predicted_text = predicted_text.strip()\n        start = text.find(predicted_text)\n        end = start + len(predicted_text)\n        if start == 0:\n            end = end - 2\n        else:\n            start = start - 2\n        if predicted_text[-1] in \"!.,\":\n            end = end - 1\n        predicted_text = text[start: end]\n\n    \n    elif sub > 1 and text.strip() != predicted_text.strip():\n        if len(predicted_text.split()) == 1:\n            if text[0] == \" \":\n                text = text[1:] + \" \"\n            else:\n                predicted_text = predicted_text + \" \"\n            if text[0] == \" \" and text.strip().find(predicted_text.strip()) == 0:\n                text = \" \" + text[:-1]\n            add_one = False\n            if predicted_text[-1] not in punc:\n                add_one = True\n            splitted = text.split(predicted_text)[0]\n            sub = len(splitted) - len(\" \".join(splitted.split()))\n            start = text.find(predicted_text) - sub\n            if start < 0:\n                start = text.find(predicted_text.strip())\n                add_one = False\n                predicted_text = predicted_text.strip()\n            if add_one:\n                end = start + len(predicted_text) + 1\n            else:\n                end = start + len(predicted_text)\n            predicted_text = text[start: end]\n        \n#         # start condition\n#         elif len(predicted_text.split()) > 1:\n#             splitted1 = text.split(predicted_text)\n#             if len(splitted1) == 1 or splitted1[1] == \"\":\n#                 start = text.find(predicted_text.strip()) - sub\n#                 end = len(text) + 1\n#             else:\n#                 # shift end index as well\n#                 start = text.find(predicted_text.strip()) - sub\n#                 end = start + len(predicted_text) + 1\n\n#             if start < 0:\n#                 start = 0\n#             predicted_text = text[start: end]  \n\n    return predicted_text","aff34d62":"# punc = \"!.\"\n# def postprocess(text, predicted_text):\n#     # print(text)\n#     # print(predicted_text)\n#     splitted = text.split(predicted_text)[0]\n#     sub = len(splitted) - len(\" \".join(splitted.split()))\n#     if sub > 1 and text.strip() != predicted_text.strip():\n#         if len(predicted_text.split()) == 1:\n#             if text[0] == \" \":\n#                 text = text[1:] + \" \"\n#             else:\n#                 predicted_text = predicted_text + \" \"\n#             if text[0] == \" \" and text.strip().find(predicted_text.strip()) == 0:\n#                 text = \" \" + text[:-1]\n#             add_one = False\n#             if predicted_text[-1] not in punc:\n#                 add_one = True\n#             splitted = text.split(predicted_text)[0]\n#             sub = len(splitted) - len(\" \".join(splitted.split()))\n#             start = text.find(predicted_text) - sub\n#             if start < 0:\n#                 start = text.find(predicted_text.strip())\n#                 add_one = False\n#                 predicted_text = predicted_text.strip()\n#             if add_one:\n#                 end = start + len(predicted_text) + 1\n#             else:\n#                 end = start + len(predicted_text)\n#             predicted_text = text[start: end]\n#     return predicted_text","22270d82":"def calculate_jaccard_score(\n    original_tweet, \n    target_string, \n    sentiment_val, \n    idx_start, \n    idx_end, \n    offsets,\n    verbose=False):\n    \n    if idx_end < idx_start:\n        idx_end = idx_start\n    \n    filtered_output  = \"\"\n    for ix in range(idx_start, idx_end + 1):\n        filtered_output += original_tweet[offsets[ix][0]: offsets[ix][1]]\n        if (ix+1) < len(offsets) and offsets[ix][1] < offsets[ix+1][0]:\n            filtered_output += \" \"\n\n#     if sentiment_val == \"neutral\" or len(original_tweet.split()) < 2:\n#         filtered_output = original_tweet\n        \n    if len(original_tweet.split()) < 2:\n        filtered_output = original_tweet\n        \n        \n#     if len(filtered_output.strip()) > 0:\n#         filtered_output = postprocess(original_tweet, filtered_output)\n    \n    if sentiment_val != \"neutral\" and verbose == True:\n        if filtered_output.strip().lower() != target_string.strip().lower():\n            print(\"********************************\")\n            print(f\"Output= {filtered_output.strip()}\")\n            print(f\"Target= {target_string.strip()}\")\n            print(f\"Tweet= {original_tweet.strip()}\")\n            print(\"********************************\")\n\n    jac = 0\n    return jac, filtered_output","e615fcad":"df_test = pd.read_csv(\"..\/input\/tweet-sentiment-extraction\/test.csv\")\ndf_test.loc[:, \"selected_text\"] = df_test.text.values","c33551ae":"# df_test = pd.read_csv(\"..\/input\/tweet-sentiment-extraction\/test.csv\")\n","cf0f0b7e":"device = torch.device(\"cuda\")\nmodel_config = transformers.RobertaConfig.from_pretrained(ROBERTA_PATH)\n#model_config = transformers.RobertaConfig.from_pretrained(ROBERTA_PATH)\n#model_config.output_hidden_states = True\n","8f040e4b":"MODEL_BASE_PATH_OLD = '..\/input\/roberta-base-uncased4'\n\nMODEL_BASE_PATH = '..\/input\/roberta-pp2'","d1a319c4":"ENSEMBLES = [\n    {'model': TweetModel(conf=model_config), 'state_dict': f\"{MODEL_BASE_PATH}\/model_0.bin\", 'weight': 1}, # CV: 0.705 LB:?? 0.709\n    {'model': TweetModel(conf=model_config), 'state_dict': f\"{MODEL_BASE_PATH}\/model_1.bin\", 'weight': 1}, # CV: 0.707 LB:?? 0.710\n    {'model': TweetModel(conf=model_config), 'state_dict': f\"{MODEL_BASE_PATH}\/model_2.bin\", 'weight': 1}, # CV: 0.712 LB:??\n    {'model': TweetModel(conf=model_config), 'state_dict': f\"{MODEL_BASE_PATH}\/model_3.bin\", 'weight': 1}, # CV: 0.718 LB:??\n    {'model': TweetModel(conf=model_config), 'state_dict': f\"{MODEL_BASE_PATH}\/model_4.bin\", 'weight': 1}, # CV: 0.718 LB:??\n    {'model': TweetModel(conf=model_config), 'state_dict': f\"{MODEL_BASE_PATH}\/model_5.bin\", 'weight': 1}, # CV: 0.715 LB:??\n    {'model': TweetModel(conf=model_config), 'state_dict': f\"{MODEL_BASE_PATH}\/model_6.bin\", 'weight': 1}, # CV: 0.710 LB:??\n    {'model': TweetModel(conf=model_config), 'state_dict': f\"{MODEL_BASE_PATH}\/model_7.bin\", 'weight': 1}, # CV: 0.714 LB??\n]","7641202d":"models = []\nweights = []\n\nfor val in ENSEMBLES:\n    model = val['model']\n    model.to(device)\n    model.load_state_dict(torch.load(val['state_dict']))\n    model.eval()\n    models.append(model)\n    weights.append(val['weight'])","3e7de046":"def get_best_start_end_idxs(_start_logits, _end_logits):\n    best_logit = -1000\n    best_idxs = None\n    for start_idx, start_logit in enumerate(_start_logits):\n        for end_idx, end_logit in enumerate(_end_logits[start_idx:]):\n            logit_sum = (start_logit + end_logit).item()\n            if logit_sum > best_logit:\n                best_logit = logit_sum\n                best_idxs = (start_idx, start_idx+end_idx)\n    return best_idxs","d6de34f4":"final_output = []\n\n\ntest_dataset = TweetDataset(\n    tweet=df_test.text.values,\n    sentiment=df_test.sentiment.values,\n    selected_text=df_test.selected_text.values\n)\n\ndata_loader = torch.utils.data.DataLoader(\n    test_dataset,\n    shuffle=False,\n    batch_size=VALID_BATCH_SIZE,\n    num_workers=1\n)\n\n# pp_model = pickle.load(open('..\/input\/tse-post-process\/post_process.sav', 'rb'))\n\n\nwith torch.no_grad():\n    tk0 = tqdm(data_loader, total=len(data_loader))\n    for bi, d in enumerate(tk0):\n        ids = d[\"ids\"]\n        token_type_ids = d[\"token_type_ids\"]\n        mask = d[\"mask\"]\n        sentiment = d[\"sentiment\"]\n        orig_selected = d[\"orig_selected\"]\n        orig_tweet = d[\"orig_tweet\"]\n        targets_start = d[\"targets_start\"]\n        targets_end = d[\"targets_end\"]\n        offsets = d[\"offsets\"].numpy()\n\n        ids = ids.to(device, dtype=torch.long)\n        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n        mask = mask.to(device, dtype=torch.long)\n        targets_start = targets_start.to(device, dtype=torch.long)\n        targets_end = targets_end.to(device, dtype=torch.long)\n\n        outputs_start = []\n        outputs_end = []\n        \n        for index, model in enumerate(models):\n            output_start, output_end = model(\n                ids=ids,\n                mask=mask,\n                token_type_ids=token_type_ids\n            )\n            outputs_start.append(output_start * weights[index]) \n            outputs_end.append(output_end * weights[index]) \n            \n        outputs_start = sum(outputs_start) \/ len(outputs_start) \n        outputs_end = sum(outputs_end) \/ len(outputs_end)\n    \n        \n        outputs_start = torch.softmax(outputs_start, dim=1).cpu().detach().numpy()\n        outputs_end = torch.softmax(outputs_end, dim=1).cpu().detach().numpy()\n        jaccard_scores = []\n        for px, tweet in enumerate(orig_tweet):\n            selected_tweet = orig_selected[px]\n            tweet_sentiment = sentiment[px]\n            idx_start, idx_end = get_best_start_end_idxs(outputs_start[px, :], outputs_end[px, :]) \n            \n            _, output_sentence = calculate_jaccard_score(\n                original_tweet=tweet,\n                target_string=selected_tweet,\n                sentiment_val=tweet_sentiment,\n#                 idx_start=np.argmax(outputs_start[px, :]),\n#                 idx_end=np.argmax(outputs_end[px, :]),\n                idx_start=idx_start,\n                idx_end=idx_end,\n                offsets=offsets[px]\n            )\n            \n            final_output.append(output_sentence)","abb47e10":"df_test['selected_text'] = final_output\ndf_test['selected_text'] = df_test.apply(lambda x: postprocess(x.text, x.selected_text), axis=1)","5c91944a":"def place_in_back(x):\n    splitted = x.text.split(x.selected_text)[0]\n    sub = len(splitted) - len(\" \".join(splitted.split()))\n    \n    select=x.selected_text\n    ind = x.text.find(x.selected_text.strip())-1\n    if ((ind >0) & (select.startswith('.')!=1) & (sub>0)):\n        if ((x.text[ind] in string.punctuation) & (x.sentiment!='neutral') & (sub==1)):\n            select = x.text[ind]+select\n            print(select,sub)\n        elif ((x.text[ind-1] in string.punctuation) & (x.sentiment!='neutral') &(sub==2) ):\n            select = x.text[ind-1]+\"\"+select\n            print(select,sub)\n    return select\n    ","884e1ed7":"df_test['selected_text']=df_test.apply(lambda x : place_in_back(x),axis=1)","f58c7bd4":"\nsample = pd.read_csv(\"..\/input\/tweet-sentiment-extraction\/sample_submission.csv\")\nsample.loc[:, 'selected_text'] = df_test['selected_text']\n\n\n\nsample['selected_text'] = sample['selected_text'].apply(lambda x: x.replace('!!!', '!') if len(x.split())==1 else x)\nsample['selected_text'] = sample['selected_text'].apply(lambda x: x.replace('!!!!', '!!') if len(x.split())==1 else x)\nsample['selected_text'] = sample['selected_text'].apply(lambda x: x.replace('..', '.') if len(x.split())==1 else x)\nsample['selected_text'] = sample['selected_text'].apply(lambda x: x.replace('...', '.') if len(x.split())==1 else x)\nsample['selected_text'] = sample['selected_text'].apply(lambda x: x.replace('....', '..') if len(x.split())==1 else x)\n\n\n\n\nsample.to_csv(\"submission.csv\", index=False)\n\n","58acb79f":"sample.sample(10)","b05aff66":"### datasets info\n\nroberta-pp -> preprocess + postprocess + (valideated on after preprocess) CV: 0.717+"}}