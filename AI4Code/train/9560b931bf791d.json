{"cell_type":{"934ce9ea":"code","e0fea548":"code","3dc2e33f":"code","8e7684ab":"code","714b14b3":"code","c33de06f":"code","6659cab8":"code","180a4aee":"code","085b9ed6":"code","80ee6754":"code","a95a720e":"code","e53b12ba":"code","39aa2326":"code","62ec8a8a":"markdown","2e8951ff":"markdown","6920eeb5":"markdown","4a54108d":"markdown","c4809900":"markdown"},"source":{"934ce9ea":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom IPython.display import Image\n\nfrom scipy.stats import norm\nimport scipy.stats as stats\n\nimport math\n%matplotlib inline\nfrom ipywidgets import IntProgress\nfrom IPython.display import display\n\n# plotly\nimport plotly.plotly as py\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\n# %matplotlib notebook\n\nplt.style.use('ggplot')\n\nimport warnings            \nwarnings.filterwarnings(\"ignore\") \n\ncolor = sns.color_palette()\n\npd.options.mode.chained_assignment = None\npd.options.display.max_columns = 999\n\nfrom pandas.tools.plotting import parallel_coordinates","e0fea548":"veri=pd.read_csv(\"..\/input\/iris.data.csv\", header=None)\nveri.columns=['Sepal.Length', 'Sepal.Width', 'Petal.Length', 'Petal.Width', 'Species']\nveri.head()","3dc2e33f":"parallel_coordinates(veri, 'Species', colormap=plt.get_cmap(\"Set1\"))","8e7684ab":"melt=pd.melt(veri,id_vars=\"Species\")\nsns.swarmplot(x=\"variable\", y=\"value\", hue=\"Species\", data=melt)","714b14b3":"sns.violinplot(x=\"variable\", y=\"value\", hue=\"Species\", data=melt, inner=\"quart\")","c33de06f":"sns.boxplot(x=\"variable\", y=\"value\", hue=\"Species\", data=melt)","6659cab8":"X=veri.iloc[:,2:3].values\ny=veri.iloc[:,4].values\n\nfrom sklearn.model_selection import train_test_split \nX_egitim,X_dene,y_egitim,y_dene=train_test_split(X, y, test_size=0.25,random_state=0, stratify=y)\n\nfrom sklearn.ensemble import RandomForestClassifier\nsinif=RandomForestClassifier(random_state=0,criterion=\"entropy\",n_estimators=1000)\nsinif.fit(X_egitim,y_egitim)\ny_tahmin=sinif.predict(X_dene)\n\nfrom sklearn.metrics import accuracy_score\naccuracy_score(y_tahmin, y_dene)","180a4aee":"X=veri.iloc[:,:-1].values\ny=veri.iloc[:,4].values\n\nfrom sklearn.model_selection import train_test_split \nX_egitim,X_dene,y_egitim,y_dene=train_test_split(X, y, test_size=0.25,random_state=0, stratify=y)","085b9ed6":"clf_rf = RandomForestClassifier(random_state=0)      \nclr_rf = clf_rf.fit(X_egitim,y_egitim)\ny_tahmin=clr_rf.predict(X_dene)\naccuracy_score(y_tahmin, y_dene)","80ee6754":"clf_rf_5 = RandomForestClassifier()      \nclr_rf_5 = clf_rf_5.fit(X_egitim,y_egitim)\nimportances = clr_rf_5.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in clf_rf.estimators_],\n             axis=0)\nindices = np.argsort(importances)[::-1]\n\n# Print the feature ranking\nprint(\"Feature ranking:\")\nfor f in range(X_egitim.shape[1]):\n    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))","a95a720e":"plt.title(\"Feature importances\")\nplt.bar(range(X_egitim.shape[1]), importances[indices],\n       color=\"g\", yerr=std[indices], align=\"center\")\nplt.xticks(range(X_egitim.shape[1]), veri.columns[indices],rotation=90)\nplt.xlim([-1, X_egitim.shape[1]])\nplt.show()","e53b12ba":"X=veri.iloc[:,2:4].values\ny=veri.iloc[:,4].values\n\nfrom sklearn.model_selection import train_test_split \nX_egitim,X_dene,y_egitim,y_dene=train_test_split(X, y, test_size=0.25,random_state=0, stratify=y)\n\nfrom sklearn.ensemble import RandomForestClassifier\nsinif=RandomForestClassifier(random_state=0,criterion=\"entropy\",n_estimators=1000)\nsinif.fit(X_egitim,y_egitim)\ny_tahmin=sinif.predict(X_dene)\n\nfrom sklearn.metrics import accuracy_score\naccuracy_score(y_tahmin, y_dene)","39aa2326":"X=veri.iloc[:,:-1].values\ny=veri.iloc[:,4].values\n\nfrom sklearn.model_selection import train_test_split \nX_egitim,X_dene,y_egitim,y_dene=train_test_split(X, y, test_size=0.25,random_state=0, stratify=y)\n\nfrom sklearn.feature_selection import RFECV\n\n# The \"accuracy\" scoring is proportional to the number of correct classifications\nclf_rf_4 = RandomForestClassifier() \nrfecv = RFECV(estimator=clf_rf_4, step=1, cv=5,scoring='accuracy')   #5-fold cross-validation\nrfecv = rfecv.fit(X_egitim, y_egitim)\n\nprint('Optimal number of features :', rfecv.n_features_)\nprint('Best features :', veri.columns[:-1:][rfecv.support_])\n\nprint(\"\\nS\u0131ralama:\")\nsaya\u00e7=1\nfor i in np.argsort(rfecv.ranking_):\n    print(\"{0}. {1}\".format(saya\u00e7,veri.columns[i]))\n    saya\u00e7+=1    ","62ec8a8a":"#### 2-T\u00fcm kolonlar eleme yap\u0131lmadan al\u0131nd\u0131\u011f\u0131nda %95","2e8951ff":"#### 1-G\u00f6rsel kontrol sonucu sadece PetalLength tercih edildi\u011finde sonu\u00e7 %100","6920eeb5":"#### 4-\"Recursive feature elimination with cross validation\" ile ise  her seferinde farkl\u0131 sonu\u00e7lar al\u0131n\u0131yor ve \u00e7o\u011funda \"PetalWidth\" in \u00f6nemi g\u00f6r\u00fclm\u00fcyor.","4a54108d":"**Merhabalar \u00f6znitelik se\u00e7iminde g\u00f6rselle\u015ftirme ile eleme ila \u00f6znitelik hesaplama ara\u00e7lar\u0131 aras\u0131ndaki fark\u0131 k\u0131yaslamak i\u00e7in b\u00f6yle bir kernell olu\u015fturdum.**","c4809900":"#### 3-feature_importances a bak\u0131ld\u0131\u011f\u0131nda \"PetalLength\" ikinci plana at\u0131lm\u0131\u015f, PetalWidth daha \u00f6nemli \u00e7\u0131k\u0131yor. Bu iki kolonla s\u0131n\u0131fland\u0131rma yap\u0131ld\u0131\u011f\u0131nda %97'ye \u00e7\u0131k\u0131l\u0131yor ama g\u00f6rsel kontroldeki %100 l\u00fck ba\u015far\u0131ya ula\u015f\u0131lam\u0131yor."}}