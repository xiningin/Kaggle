{"cell_type":{"66290eba":"code","2e53f0bc":"code","3fadcbfc":"code","a4ff56a5":"code","42c667cb":"code","4da06672":"code","92561000":"code","4c1abd15":"code","c1af197b":"code","2fd548f4":"code","daa01a3d":"code","a6ce4673":"code","4bfa43db":"code","f4ec0cd3":"code","2be79260":"code","d6b8d03d":"code","eb2a0d2c":"code","7c1d1f70":"code","ad749907":"code","8c44387b":"code","1537a717":"code","4a5a46bc":"code","b29c6c23":"code","dc65a881":"code","51a7c221":"code","425dac25":"code","e14c89a2":"code","f5cc31b2":"code","4964b16b":"code","4be0a6f1":"code","f7f763a9":"code","762b3d40":"code","64373e8c":"code","aee42568":"code","39132179":"code","2aedae09":"code","d73b3b6e":"code","ae1b8f5c":"code","082c6efe":"code","527ed37e":"code","6797059d":"code","afa9af20":"code","23eb631b":"code","4e5be499":"code","852e5797":"code","01b17a62":"code","923e0016":"code","a9838007":"code","0530588e":"code","0c698f94":"code","17e81c77":"code","d8fe6e60":"code","a8d0437d":"code","5e0ccca0":"code","1ef858d8":"code","7276bfb7":"code","64bb4f2c":"code","90e1aa50":"code","22215365":"code","4ba9f041":"code","086a33d7":"code","483c5637":"code","6288244d":"markdown","22d74147":"markdown","9d68ebea":"markdown","9b93ffd8":"markdown","e56100c4":"markdown"},"source":{"66290eba":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2e53f0bc":"data=pd.read_csv('\/kaggle\/input\/iba-ml1-final-project\/train.csv')","3fadcbfc":"test=pd.read_csv('\/kaggle\/input\/iba-ml1-final-project\/test.csv')\n","a4ff56a5":"submission=pd.read_csv('\/kaggle\/input\/iba-ml1-final-project\/sample_submission.csv')\n","42c667cb":"data.head()","4da06672":"#Checking null values\ndata.isnull().sum()","92561000":"#Dropping columns which don't have any review\ndata = data[~data['Review'].isnull()] \ntest = test[~test['Review'].isnull()] \ndata = data[~data['Review_Title'].isnull()]\ntest = test[~test['Review_Title'].isnull()]","4c1abd15":"#Distribution of age\nimport plotly.express as px\npx.histogram(data, x = 'Age')","c1af197b":"px.histogram(data, x = data['Rating'])","2fd548f4":"px.histogram(data, x = data['Product_Category'])\n#Clothes,knits and blouse are most widely categories","daa01a3d":"px.scatter(data, x=\"Age\", y=\"Pos_Feedback_Cnt\", facet_row=\"Recommended\", facet_col=\"Rating\",trendline=\"ols\",category_orders={\"Rating\": [1,2,3,4,5],'Recommended':[0,1]})","a6ce4673":"px.violin(data, x=\"Age\", y=\"Product_Category\", orientation=\"h\", color=\"Recommended\")","4bfa43db":"px.box(data, x=\"Age\", y=\"Division\", orientation=\"h\",color = 'Recommended')","f4ec0cd3":"err1 = data['Review'].str.extractall(\"(&amp)\")\nerr2 = data['Review'].str.extractall(\"(\\xa0)\")","2be79260":"print('with &amp',len(err1[~err1.isna()]))\nprint('with (\\xa0)',len(err2[~err2.isna()]))","d6b8d03d":"data['Review'] = data['Review'].str.replace('(&amp)','')\ndata['Review'] = data['Review'].str.replace('(\\xa0)','')","eb2a0d2c":"err1 = data['Review'].str.extractall(\"(&amp)\")\nprint('with &amp',len(err1[~err1.isna()]))\nerr2 = data['Review'].str.extractall(\"(\\xa0)\")\nprint('with (\\xa0)',len(err2[~err2.isna()]))","7c1d1f70":"!pip install TextBlob\nfrom textblob import *","ad749907":"data['polarity'] = data['Review'].map(lambda text: TextBlob(text).sentiment.polarity)","8c44387b":"px.histogram(data, x = 'polarity')","1537a717":"px.box(data, y=\"polarity\", x=\"Product_Category\", orientation=\"v\",color = 'Recommended')\n","4a5a46bc":"#Reviews with positive polarity\nsam = data.loc[data.polarity == 1,['Review']].sample(3).values\nfor i in sam:\n    print(i[0])","b29c6c23":"#Reviews with negative polarity\nsam = data.loc[data.polarity <0,['Review']].sample(3).values\nfor i in sam:\n    print(i[0])","dc65a881":"negative = (len(data.loc[data.polarity <0,['Review']].values)\/len(data))*100\npositive = (len(data.loc[data.polarity >0.5,['Review']].values)\/len(data))*100\nneutral  = len(data.loc[data.polarity >0 ,['Review']].values) - len(data.loc[data.polarity >0.5 ,['Review']].values)\nneutral = neutral\/len(data)*100","51a7c221":"from matplotlib import pyplot as plt \nplt.figure(figsize =(10, 7)) \nplt.pie([positive,negative,neutral], labels = ['Positive','Negative','Neutral'])","425dac25":"from sklearn.feature_extraction.text import CountVectorizer\n","e14c89a2":"def top_n_ngram(corpus,n = None,ngram = 1):\n    vec = CountVectorizer(stop_words = 'english',ngram_range=(ngram,ngram)).fit(corpus)\n    bag_of_words = vec.transform(corpus) #Have the count of  all the words for each review\n    sum_words = bag_of_words.sum(axis =0) #Calculates the count of all the word in the whole review\n    words_freq = [(word,sum_words[0,idx]) for word,idx in vec.vocabulary_.items()]\n    words_freq = sorted(words_freq,key = lambda x:x[1],reverse = True)\n    return words_freq[:n]","f5cc31b2":"common_words = top_n_ngram(data['Review'], 20,1)\ncv = pd.DataFrame(common_words, columns = ['Review' , 'count'])\nplt.figure(figsize =(10,5))\ncv.groupby('Review').sum()['count'].sort_values(ascending=False).plot(\nkind='bar', title='Top 20 unigrams in review after removing stop words')","4964b16b":"common_words = top_n_ngram(data['Review'], 20,2)\ncv = pd.DataFrame(common_words, columns = ['Review' , 'count'])\nplt.figure(figsize =(10,5))\ncv.groupby('Review').sum()['count'].sort_values(ascending=False).plot(\nkind='bar', title='Top 20 bigrams in review after removing stop words')","4be0a6f1":"common_words = top_n_ngram(data['Review'], 20,3)\ncv = pd.DataFrame(common_words, columns = ['Review' , 'count'])\nplt.figure(figsize =(10,5))\ncv.groupby('Review').sum()['count'].sort_values(ascending=False).plot(\nkind='bar', title='Top 20 trigrams in review after removing stop words')","f7f763a9":"blob= TextBlob(str(data['Review']))\npos = pd.DataFrame(blob.tags,columns =['word','pos'])\npos1 = pos.pos.value_counts()[:20]\nplt.figure(figsize = (10,5))\npos1.plot(kind='bar',title ='Top 20 Part-of-speech taggings')","762b3d40":"X = data.drop(columns = ['Recommended','Rating'])\ny=data[['Recommended','Rating']]","64373e8c":"import nltk\nimport re\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer","aee42568":"corpus =[]","39132179":"X.index = np.arange(len(X))","2aedae09":"corpus2 =[]\n","d73b3b6e":"test.index = np.arange(len(test))","ae1b8f5c":"for i in range(len(test)):\n    review = re.sub('[^a-zA-z]',' ',test['Review'][i])\n    review = review.lower()\n    review = review.split()\n    ps = PorterStemmer()\n    review =[ps.stem(i) for i in review if not i in set(stopwords.words('english'))]\n    review =' '.join(review)\n    corpus2.append(review)","082c6efe":"from sklearn.feature_extraction.text import CountVectorizer as CV\ncv2  = CV(max_features = 3000,ngram_range=(1,1))\ntest_cv = cv2.fit_transform(corpus2).toarray()\ny = y.values","527ed37e":"for i in range(len(X)):\n    review = re.sub('[^a-zA-z]',' ',X['Review'][i])\n    review = review.lower()\n    review = review.split()\n    ps = PorterStemmer()\n    review =[ps.stem(i) for i in review if not i in set(stopwords.words('english'))]\n    review =' '.join(review)\n    corpus.append(review)","6797059d":"from sklearn.feature_extraction.text import CountVectorizer as CV\ncv  = CV(max_features = 3000,ngram_range=(1,1))\nX_cv = cv.fit_transform(corpus).toarray()\ny = y.values","afa9af20":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_cv, y, test_size = 0.20, random_state = 0)","23eb631b":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","4e5be499":"tokenizer = Tokenizer(num_words = 3000)\ntokenizer.fit_on_texts(corpus)","852e5797":"sequences = tokenizer.texts_to_sequences(corpus)\npadded = pad_sequences(sequences, padding='post')","01b17a62":"word_index = tokenizer.word_index\ncount = 0\nfor i,j in word_index.items():\n    if count == 11:\n        break\n    print(i,j)\n    count = count+1","923e0016":"embedding_dim = 64\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(3000, embedding_dim),\n    tf.keras.layers.GlobalAveragePooling1D(),\n    tf.keras.layers.Dense(6, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\nmodel.summary()","a9838007":"num_epochs = 10\n\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])","0530588e":"from sklearn.naive_bayes import BernoulliNB\nclassifier = BernoulliNB()\nclassifier.fit(X_train, y_train[:,0])","0c698f94":"y_pred = classifier.predict(X_test)\nfrom sklearn.metrics import accuracy_score\nacc = accuracy_score(y_test[:,0], y_pred)","17e81c77":"prediction_c=classifier.predict(X_test)","d8fe6e60":"from scipy.stats import spearmanr\ncoef, p = spearmanr(y_test[:,0], prediction_c)\n\ncoef","a8d0437d":" from sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\ndlf =  RandomForestRegressor(random_state=0, n_estimators=100)\ndlf.fit(X_train, y_train[:,1])\nprint(\"classifier accuracy:\", dlf.score(X_test, y_test[:,1]))","5e0ccca0":"prediction_r=(dlf.predict(X_test)).astype(int)","1ef858d8":"from scipy.stats import spearmanr\ncoef, p = spearmanr(y_test[:,1], prediction_r)\n\ncoef","7276bfb7":"import math\nimport numpy as np","64bb4f2c":"u1c=classifier.predict(test_cv)","90e1aa50":"import numpy as np","22215365":"u2c=(dlf.predict(test_cv)).astype(int)","4ba9f041":"submission.shape","086a33d7":"ids =test['Id']\n\n\n#set the output as a dataframe and convert to csv file named submission.csv\noutput = pd.DataFrame({ 'Id' : ids, 'Rating': u2c,'Recommended': u1c })\noutput.to_csv('submission_asdfb.csv', index=False)","483c5637":"output","6288244d":"Model Building","22d74147":"# RE + Tokenizing + Stemming + Corpus Creation","9d68ebea":"# Cleaning the Text Data\n\n","9b93ffd8":"# EDA","e56100c4":"****Bag of Words Technique"}}