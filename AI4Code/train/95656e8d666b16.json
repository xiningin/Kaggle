{"cell_type":{"37d424ed":"code","656edf2c":"code","ccdb1a6e":"code","b3c20462":"code","63afebd1":"code","a680fda7":"code","f9022994":"code","731042ee":"code","bb864cac":"code","b983b79a":"code","a9847b42":"code","34ef60ed":"code","e0fc671b":"code","534ffe01":"code","0a8dc27f":"code","b8234d27":"code","ff23956a":"code","699446f7":"code","1d188494":"code","2387e46f":"code","e204057b":"code","c6261867":"code","cd5b90aa":"code","591efc4b":"code","5e3bd97a":"code","ad9a80c8":"code","3c3371f7":"code","247c820e":"code","7a759a77":"code","2ba4b393":"code","452feb0b":"code","3b864e47":"code","1b9b9724":"code","19daa315":"code","d497088a":"markdown","208da08f":"markdown","60c6508c":"markdown","f9643baa":"markdown","a79a6693":"markdown","3eb1cdd4":"markdown","0e6de682":"markdown","ec413307":"markdown","84365b87":"markdown","2b9a7462":"markdown","7ffb1324":"markdown","d1c72531":"markdown","79160455":"markdown","0fb7b34a":"markdown"},"source":{"37d424ed":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","656edf2c":"df = pd.read_csv('..\/input\/fish-market\/Fish.csv')","ccdb1a6e":"df","b3c20462":"df.isna().sum()","63afebd1":"df.duplicated().sum()","a680fda7":"df.dtypes","f9022994":"import seaborn as sns","731042ee":"sns.countplot(x='Species', data=df)","bb864cac":"X = df.drop('Species', axis=1)","b983b79a":"y = df['Species']","a9847b42":"X, y","34ef60ed":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=.25, random_state=42)","e0fc671b":" from sklearn.feature_selection import SelectKBest, f_classif\n\nselector = SelectKBest(f_classif, k=6)\nselector.fit(X_train, y_train)","534ffe01":"selector.scores_","0a8dc27f":"cols = selector.get_support(indices=True)\ncols","b8234d27":"from sklearn.preprocessing import LabelEncoder, StandardScaler","ff23956a":"encoder = LabelEncoder()\ny = encoder.fit_transform(y)\ny_map = {index:label for index,label in enumerate(encoder.classes_)}","699446f7":"y_map","1d188494":"y","2387e46f":"scaler = StandardScaler()\nX = scaler.fit_transform(X)","e204057b":"X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=.25, random_state=42)","c6261867":"from sklearn.linear_model import LogisticRegression","cd5b90aa":"log_model = LogisticRegression()\nlog_model.fit(X_train,y_train)","591efc4b":"log_model.score(X_test,y_test)","5e3bd97a":"from sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier(random_state=42).fit(X_train, y_train)\ny_pred_rfc = model.predict(X_test)\nmodel.score(X_test, y_test)","ad9a80c8":"y_pred_log = log_model.predict(X_test)","3c3371f7":"from sklearn.metrics import confusion_matrix","247c820e":"matrix = confusion_matrix(y_test, y_pred_log)","7a759a77":"from sklearn.metrics import roc_curve, roc_auc_score, f1_score, accuracy_score, precision_score, recall_score","2ba4b393":"l = ['micro', 'macro', 'weighted']\nscores = []\nfor i in l:\n    scores.append(f1_score(y_test, y_pred_log, average=i))\nscores","452feb0b":"print('Micro Precision: {:.2f}'.format(precision_score(y_test, y_pred_log, average='micro')))\nprint('Micro Recall: {:.2f}'.format(recall_score(y_test, y_pred_log, average='micro')))\nprint('Micro F1-score: {:.2f}\\n'.format(f1_score(y_test, y_pred_log, average='micro')))\n\nprint('Macro Precision: {:.2f}'.format(precision_score(y_test, y_pred_log, average='macro')))\nprint('Macro Recall: {:.2f}'.format(recall_score(y_test, y_pred_log, average='macro')))\nprint('Macro F1-score: {:.2f}\\n'.format(f1_score(y_test, y_pred_log, average='macro')))\n\nprint('Weighted Precision: {:.2f}'.format(precision_score(y_test, y_pred_log, average='weighted')))\nprint('Weighted Recall: {:.2f}'.format(recall_score(y_test, y_pred_log, average='weighted')))\nprint('Weighted F1-score: {:.2f}'.format(f1_score(y_test, y_pred_log, average='weighted')))","3b864e47":"from sklearn.metrics import classification_report\nprint('\\nClassification Report\\n')\nprint(classification_report(y_test, y_pred_log, target_names=['Class 0', 'Class 1', 'Class 2', 'Class 3', 'Class 4', 'Class 5', 'Class 6']))","1b9b9724":"import matplotlib.pyplot as plt","19daa315":"sns.heatmap(matrix, annot=True, cbar=None, cmap='Blues')\nplt.title('Confusion Matrix'), plt.tight_layout()\nplt.ylabel('True Class'), plt.xlabel('Predicted Class')\nplt.show()\nprint(y_map)","d497088a":"80% accuracy using RandomForestClassifier","208da08f":"Trying to predict the 'Species' based off the Weight, Length1, Length2, Length3, Height, and Width","60c6508c":"Splitting the data first","f9643baa":"No null values; Let's check for duplicates","a79a6693":"All columns are significant","3eb1cdd4":"Seems to be a little bit skewed","0e6de682":"85% accuracy using LogisticRegression, decent","ec413307":"### Metrics","84365b87":"Using StandardScaler on 'X'","2b9a7462":"### Exploring feature selection techniques\nNote: I know that there are already barely any features, I'm simply including this to practice and improve myself in this realm of ML","7ffb1324":"Re-splitting X_train, X_test, y_train, y_test","d1c72531":"#### Anova F-value","79160455":"First let's check for null values","0fb7b34a":"No duplicates either"}}