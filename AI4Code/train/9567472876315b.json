{"cell_type":{"e7fbac49":"code","4b91d237":"code","42c5305a":"code","16d7d8c5":"code","dee9b8d9":"code","2926c1c2":"code","e3d2b206":"code","c32d69f1":"code","e7884957":"code","fdc62baf":"code","7d502194":"code","f3a9e197":"code","bebfd42b":"code","ef8bf9f0":"code","ed7721a4":"code","f9b60571":"code","12bf1680":"code","3d0490e6":"code","24bdd697":"code","b8424f6b":"code","a45ee7da":"code","181e6eb0":"code","c41c258f":"code","52b6f706":"code","dc531ba9":"code","fe8316e5":"code","e8b3ee7a":"code","885e01ac":"code","9bcc4b05":"code","f497547a":"code","29e9ec95":"code","a1b46e48":"code","1bcd81c3":"code","25ca511a":"code","3525256b":"code","5c8682c0":"code","d5ee631f":"code","7137a8b3":"code","e2414fa7":"code","57c67f8a":"code","977d7853":"code","87115694":"code","a8f59aeb":"code","f84870ea":"code","018cfd4c":"code","95366157":"code","96170f43":"code","ead52873":"code","b3ef8a71":"code","0a685e6b":"code","257509ec":"code","e1ae01b6":"code","a76e66f2":"code","6fe8ffda":"code","62601571":"code","ea98cb1b":"code","5714d800":"code","4e4060d0":"code","cb3e419c":"code","7c593855":"code","7a37648f":"code","43db7e42":"code","bbdedbe3":"code","18c2116e":"code","26f403fd":"code","5c412835":"code","d26cc4c4":"code","0ff866d6":"code","747639a3":"code","22081a11":"code","6b88a718":"code","3c983af2":"code","253dc7c3":"code","034a9b72":"code","5f0b2932":"code","860ac18f":"code","0bc7dd1a":"code","9131f8b9":"code","b743e22f":"code","cd2c7d98":"code","d2f61e89":"code","5007b158":"code","5e8b36bf":"code","754c3f95":"code","903e2183":"code","a11ea0c7":"code","f5923901":"code","4fe1e197":"code","785cc05a":"code","a47ab5c1":"code","0042ba21":"code","daf9c3d5":"code","9aefdce1":"code","6d30d50f":"code","6310a991":"code","3d055755":"code","ee669aca":"code","113750b8":"code","8f3d8ac4":"code","f46f0e29":"code","dbf77f30":"code","dad12a7e":"code","525f08e6":"code","9c1bcc55":"code","69461539":"code","440a34de":"code","500f35ad":"markdown","718a3c4c":"markdown","b34ee79b":"markdown","68401091":"markdown","264f4960":"markdown","f60e8a42":"markdown","2e141c09":"markdown","1538e638":"markdown","e9c383fa":"markdown","1c5075db":"markdown","79b983cc":"markdown","5bf0c9d2":"markdown","4f8117a6":"markdown","f96dd124":"markdown","5144e33a":"markdown","dd98d70d":"markdown","838f647c":"markdown","e1df8a0a":"markdown","179eb184":"markdown","fea5b698":"markdown","be1cca82":"markdown","d1179501":"markdown","0d6526de":"markdown","54615525":"markdown","be3a2dd1":"markdown","27da2856":"markdown","ecdedf49":"markdown","b43435b1":"markdown","e4794b68":"markdown","b5765549":"markdown","1e4aafa6":"markdown","e8c028a3":"markdown","c2a3a794":"markdown","39381b75":"markdown","9de1ce08":"markdown","8295f54b":"markdown","8f09a9a7":"markdown","aecd341e":"markdown","5ad99738":"markdown","d5e88a50":"markdown","83929e3e":"markdown","48a3f744":"markdown","f8add5ca":"markdown","3a186949":"markdown","18247f91":"markdown","5a360bb7":"markdown","250317a5":"markdown","e3a3a3de":"markdown","0795b74b":"markdown","d27ab5b5":"markdown","5cd01821":"markdown","ae34a3cd":"markdown","c8bfe54b":"markdown","f987808c":"markdown","210de7dd":"markdown","e9768bf0":"markdown","682444d5":"markdown","3c2ccc00":"markdown","4f90531a":"markdown","b96421d4":"markdown","07de4c9e":"markdown","2ffb728f":"markdown","f867c260":"markdown","9f22e347":"markdown","c3de00ac":"markdown","0ee5d2c2":"markdown","2f2a8629":"markdown","db1bc535":"markdown","bce33c48":"markdown","84ccce5a":"markdown","355754ca":"markdown","6fc43119":"markdown","a472a5bd":"markdown","ac2e62eb":"markdown","06718fb3":"markdown","bec6d6c0":"markdown","82847e08":"markdown","44c7e5a7":"markdown","b6de7f05":"markdown","58f230c3":"markdown","99be8962":"markdown","be80b81b":"markdown","a8e6f1b5":"markdown","6ab3045d":"markdown","d5a6ae5f":"markdown","8a840774":"markdown","9ad23f79":"markdown","e8626142":"markdown","e7f41a2d":"markdown","8b0f2c74":"markdown","18fc833b":"markdown","d782716c":"markdown","4aa64115":"markdown","cca6ca99":"markdown","2ef9de62":"markdown","2983b7e6":"markdown","cbd03b6d":"markdown","e4a3396d":"markdown","e65d297b":"markdown","9d88447f":"markdown","054b79a5":"markdown","bd32f5df":"markdown","7900a13e":"markdown","3e94e393":"markdown","612f0fae":"markdown","083d808e":"markdown","36ec6c0d":"markdown","aba01980":"markdown","95638e5d":"markdown","e5abba0b":"markdown","97724c2c":"markdown","b7b43591":"markdown","53de4581":"markdown","595bad3c":"markdown","5ed06b1a":"markdown","c715a8c8":"markdown","f40a0d14":"markdown","3032eb22":"markdown","10fd14cd":"markdown","5d56c150":"markdown","69a3ff3a":"markdown","eac2595b":"markdown","56476bbb":"markdown","4189910a":"markdown","c523c27b":"markdown","d2ae1473":"markdown","c49b067c":"markdown","04cd0bcc":"markdown","22c745f6":"markdown","7acd426e":"markdown","727fcb0e":"markdown","b12310bb":"markdown","9eb86040":"markdown","1d99665f":"markdown","5c259a8c":"markdown","0ac5f810":"markdown","ae695917":"markdown","67827224":"markdown","09162c61":"markdown","423fad9b":"markdown","eb8823f8":"markdown","bc187775":"markdown","da33a103":"markdown","8a331a5d":"markdown","09346ffb":"markdown","cf17ef59":"markdown"},"source":{"e7fbac49":"from copy import deepcopy\nimport matplotlib.pyplot as plt\nimport numpy as np \nimport pandas as pd\nimport re\nimport seaborn as sns\nimport warnings\n\n%matplotlib inline\nsns.set()\nwarnings.filterwarnings(\"ignore\")","4b91d237":"df = pd.read_csv('..\/input\/pima-indians-diabetes-database\/diabetes.csv')","42c5305a":"df.head()","16d7d8c5":"df.info()","dee9b8d9":"df.describe().T","2926c1c2":"print(df['Outcome'].value_counts())\nprint(f\"Approximately {(df['Outcome'].value_counts()[0]\/df['Outcome'].size*100):.4f}% of patients don't have diabetes.\")\nprint(f\"Approximately {(df['Outcome'].value_counts()[1]\/df['Outcome'].size*100):.4f}% of patients have diabetes.\")","e3d2b206":"df.describe().T","c32d69f1":"plt.figure(figsize=(10, 8)) \nsns.heatmap(df.corr(), annot =True)","e7884957":"fig, axes = plt.subplots(4, 2, figsize=(14,14))\nfig.tight_layout(pad=4.0)\n\nfor i,j in enumerate(df.columns[:-1]):\n    sns.histplot(df[j], ax=axes[i\/\/2, i%2])","fdc62baf":"ndf = df.copy(deep = True)","7d502194":"(ndf == 0).sum()","f3a9e197":"colsToFix = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']\nndf[colsToFix] = ndf[colsToFix].replace(0, np.NaN)","bebfd42b":"ndf.isnull().sum()","ef8bf9f0":"def gimmeThemStats(dFrame):\n    \"\"\"\n    Description\n    ----\n    Outputs the general statistical description of the dataframe,\n    outputs the correlation heatmap, and outputs a distribution plot.\n    \n    Parameters\n    ----\n    dFrame(DataFrame):\n        The dataframe for which information will be displayed.\n        \n    Returns\n    ----\n    Nothing.\n    \n    \"\"\"\n    # Description\n    print(\"Descriptive Stats:\")\n    display(dFrame.describe().T)\n    \n    # Heatmap\n    plt.figure(figsize=(10, 8)) \n    plt.title(\"Heatmap\", fontsize = 'x-large')\n    sns.heatmap(dFrame.corr(), annot =True)\n    \n    # Distribution\n    ### NOTE: I changed histplot to distplot\n    fig, axes = plt.subplots(4, 2, figsize=(14,14))\n    fig.suptitle(\"Distribution Plot\", y=0.92, fontsize='x-large')\n    fig.tight_layout(pad=4.0)\n\n    for i,j in enumerate(df.columns[:-1]):\n        sns.distplot(dFrame[j], ax=axes[i\/\/2, i%2])","ed7721a4":"gimmeThemStats(ndf)","f9b60571":"dfMeanMed = ndf.copy(deep = True)","12bf1680":"# Note: colsToFix was defined earlier as a list --> ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']\n\ndfMeanMed[colsToFix].skew()","3d0490e6":"dfMeanMed['Glucose'].fillna(dfMeanMed['Glucose'].median(), inplace = True)\ndfMeanMed['SkinThickness'].fillna(dfMeanMed['SkinThickness'].median(), inplace = True)\ndfMeanMed['Insulin'].fillna(dfMeanMed['Insulin'].median(), inplace = True)\ndfMeanMed['BMI'].fillna(dfMeanMed['BMI'].median(), inplace = True)\n\ndfMeanMed['BloodPressure'].fillna(dfMeanMed['BloodPressure'].mean(), inplace = True)","24bdd697":"dfMeanMed.isnull().sum()","b8424f6b":"gimmeThemStats(dfMeanMed)\n\n# EXPAND THE OUTPUT BELOW TO SEE STATS OF `dfMeanMed`","a45ee7da":"dfMeanMed.kurt() - ndf.kurt()","181e6eb0":"from sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n\nfrom sklearn.linear_model import BayesianRidge\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor","c41c258f":"# Choose an estimator if you don't want the default BayesianRidge() estimator\nexEstimator = DecisionTreeRegressor(max_features='sqrt', random_state=42)\n# Choose a style if you don't want the default ascending order\nexStyle = 'descending'\n\n# Imputer definition\nexImputer = IterativeImputer(estimator=exEstimator, imputation_order=exStyle, random_state=42)","52b6f706":"# No, you don't have to run this, just showing you the output of it\nexImputer","dc531ba9":"# Make a copy\nedf = ndf.copy(deep = True)\n\n# Fit on the dataset\nexImputer.fit(edf)\n\n# Transform and convert to a dataframe\nexTrans = pd.DataFrame(exImputer.transform(edf), columns = edf.columns)\n\nexTrans\n\n# EXPAND OUTPUT BELOW TO VIEW THE DATAFRAME WITH ITERATIVE IMPUTATIONS","fe8316e5":"exTrans.isnull().sum()","e8b3ee7a":"gimmeThemStats(exTrans)\n\n# EXPAND OUTPUT BELOW TO VIEW STATS FOR `exTrans`","885e01ac":"exTrans.le(0).any()","9bcc4b05":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score,classification_report,confusion_matrix\n\nfrom sklearn.ensemble import ExtraTreesClassifier","f497547a":"# Define X for features, where exTrans is the dataset with iterative imputation\nX = exTrans.drop('Outcome', axis=1)\n\n# Define y\ny = exTrans['Outcome']\n\n# Split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42, stratify = y)","29e9ec95":"y_train.value_counts()","a1b46e48":"y_test.value_counts()","1bcd81c3":"# Scaler definition\nexScaler = StandardScaler()\n\n# Fit and transform the train set\nexScaledTrainSet = exScaler.fit_transform(X_train)\n# Transform the test set\nexScaledTestSet = exScaler.transform(X_test)","25ca511a":"# Model definition\nexModel = ExtraTreesClassifier()\n\n# Fit the model and training sets\nexModel.fit(exScaledTrainSet, y_train)\n\n# Predict\nexPredict = exModel.predict(exScaledTestSet)","3525256b":"exAccScore = accuracy_score(y_test, exPredict)\nprint(f\"Our model got an accuracy score of {exAccScore*100:.4f}% when using a Decision Tree Regressor estimator to Iteratively Impute\\nin a descending fashion, after predictions from Extra Trees Classifier.\")","5c8682c0":"confusion_matrix(y_test, exPredict)","d5ee631f":"print(classification_report(y_test, exPredict, target_names=['Diabetic Neg.', 'Diabetic Pos.']))","7137a8b3":"from sklearn.svm import SVC, LinearSVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier","e2414fa7":"estimatorList = [\n    BayesianRidge(),\n    DecisionTreeRegressor(max_features='sqrt', random_state=42),\n    ExtraTreesRegressor(n_estimators=10, random_state=42),\n    RandomForestRegressor(criterion='mse', n_estimators=10, random_state=42),\n    KNeighborsRegressor(n_neighbors=15)\n]\n\nimputation_styles = ['ascending', 'descending', 'roman', 'arabic', 'random']\n\nclassifications_list = [\n    LinearSVC(C= 5.0, class_weight=\"balanced\"), SVC(kernel='rbf'), GaussianNB(), \n    KNeighborsClassifier(n_neighbors=7), DecisionTreeClassifier(), RandomForestClassifier(),\n    ExtraTreesClassifier(), AdaBoostClassifier(), GradientBoostingClassifier()\n]","57c67f8a":"def imputeEm(adf, estimList, stylesList):\n    \"\"\"\n    Description\n    ----\n    Iteratively imputes missing values to a dataset by following a \n    given estimator and imputation-order pair.\n    If an estimator or imputation order throws an error, an error\n    will be printed after function call explaining why the error\n    occured. If no error is found for a given estimator or imputation\n    order, no errors will be printed in the end of the function call.\n    An error thrown for an estimator and imputation order pair won't \n    affect other pairs. You'll still get the results you sought for.\n    \n    Parameters\n    ----\n    adf (dataframe):\n        The dataframe containing missing values.\n        \n    estimList (list of models):\n        The list of estimators to use in IterativeImputer.\n    \n    stylesList (list of str):\n        The list of styles to use in IterativeImputer.\n    \n    Returns\n    ----\n    estim_name_list (list of str):\n        A list of the name of the estimator used in each iteration.\n        For example, if there are 5 imputation-order styles per each \n        estimator, then the list will contain each estimator 5 times.\n    \n    style_list (list of str):\n        A list of the name of the imputation-order used in each\n        iteration. For example, if there are 10 estimators used, \n        the list will include each imputation-order 10 times.\n        \n    imputed_df_list (list of dataframes):\n        A list of dataframes for each estimator and imputation-order\n        pair.\n    \"\"\"\n    \n    # The returned lists\n    style_list = []\n    estim_name_list = []\n    imputed_df_list = []\n    \n    # Loop through each estimator\n    for estim in range(len(estimList)):\n        \n        # Convert estimator to string format and debolish parenthesis and anything in between\n        estimstorName = re.sub(r\"\\([^()]*\\)\", '', str(estimList[estim])) \n        \n        # Loop through each imputation-order\n        for style in stylesList:\n            \n            try:\n                # Introduce Iterative Imputer with estimator and imputation_order\n                imputer = IterativeImputer(random_state=42, estimator=estimList[estim], imputation_order=style)\n                # Fit on dataframe\n                imputer.fit(adf)\n            except Exception as e:\n                print(\"==============================================================\")\n                print(f\"I wasn't able to iteratively impute with the estimator: {estimList[estim]} and imputation order: {style}.\")\n                print(f\"This was the error I've received from my master:\\n\\n{e}.\")\n                print(\"\\nI didn't let it faze me though, for now I've skipped this imputation pair.\")\n                print(\"==============================================================\\n\")\n            else:\n                estim_name_list.append(estimstorName) #Appending estimator name\n                style_list.append(style) #Appending style name\n                \n                # Transform and append the imputed dataframe to the list of imputed dataframes\n                imputed_df_list.append(pd.DataFrame(imputer.transform(adf), columns = adf.columns))\n            \n            \n    return estim_name_list, style_list, imputed_df_list","977d7853":"# It doesn't take too long, but that mainly depends on the amount of estimators you're using\ndata = imputeEm(ndf, estimatorList, imputation_styles)","87115694":"data[0] # The list containing the estimator per iteration\n\n# EXPAND THE CELL BELOW TO VIEW OUTPUT","a8f59aeb":"data[1] # The list containing the imputation order per iteration\n\n# EXPAND THE CELL BELOW TO VIEW OUTPUT","f84870ea":"# Running data[2] will output the list containing all dataframes.\n# Here, we're looking at the head() of the 7th dataframe by running the code below.\nprint(f\"Estimator: {data[0][7]}\")\nprint(f\"Imputation order: {data[1][7]}\")\ndata[2][7].head()","018cfd4c":"len(data[2])","95366157":"def invalidNumberChecker(dataList):\n    \"\"\"\n    Description\n    ----\n    This function will check for values less than or equal to 0 within a dataframe. \n    The function displays several things including: \n        1. The Dataframe number: The ith dataframe in `dataList[2][i]` in which the \n        invalid number was caught.\n        2. Estimator: Estimator used in the dataframe number.\n        3. Order: Imputation order used in the dataframe number.\n        4. A set of description for each invalid number within the dataframe which \n        includes:\n            4a. Index: The index of the dataframe where the invalid number lives.\n            4b. Column: The column that contains the invalid number.\n            4c. Value: The invalid number itself.\n        5. A dataframe display of the rows with invalid number(s).\n    \n    Parameters\n    ----\n    dataList (list of lists):\n        The list containing a list of models, list of imputation orders, and\n        list of dataframes which was obtained after running function `imputeEm`.\n    \n    Returns\n    ----\n    Nothing.\n    \n    \"\"\"\n    # Loop through every dataframe in the list of dataframes\n    for i in range(len(dataList[2])):\n        # index_list will hold the indices where invalid numbers live\n        index_list = []\n        # invalid_pairs is a list containing pairs (tuples) of rows and column names where invalid numbers live\n        invalid_pairs = dataList[2][i][colsToFix][dataList[2][i][colsToFix] <= 0].stack().index.tolist()\n        if(invalid_pairs):\n            print(f'Dataframe # {i}  --  For reference, check dataList[2][{i}], where dataList is the list obtained after running function `imputeEm`.')\n            print('--------------')\n            print(f'Estimator: {dataList[0][i]}\\nOrder: {dataList[1][i]}\\n')\n            for j in range(len(invalid_pairs)):\n                index_list.append(invalid_pairs[j][0])\n                print(f'Index: {invalid_pairs[j][0]}\\nColumn: {invalid_pairs[j][1]}')\n                print(f'Value: {dataList[2][i][invalid_pairs[j][1]].loc[invalid_pairs[j][0]]}\\n')\n            display(dataList[2][i].loc[index_list])\n            print(\"==================================================================================================\\n\\n\")\n            ","96170f43":"invalidNumberChecker(data)\n\n# EXPAND OUTPUT TO SEE INVALID NUMBERS","ead52873":"start = 0\nstop = len(imputation_styles)\ndiff = stop - start\nprint(\"Printing the mean of all values of the 62nd index for each estimator in column: Insulin\")\nprint(\"=======================================================================================\")\nfor i in range(len(estimatorList)):\n    value = 0   \n    name = re.sub(r\"\\([^()]*\\)\", '', str(estimatorList[i]))\n    for j in range(len(data[2][start:stop])):\n        value += data[2][start:stop][j].iloc[62]['Insulin']\n    start = stop\n    stop += diff\n    value \/= diff\n    \n    print(f\"Estimator: {name}\\nMean:{value}\\n\")","b3ef8a71":"for i in range(5):\n    print(f\"Dataframe #{i}\")\n    print(f\"Minimum Value: {data[2][i]['Insulin'].min()}\")\n    print(f\"Skewness: {data[2][i]['Insulin'].skew()}\\n\")","0a685e6b":"for i in range(5):\n    print(data[2][i]['Insulin'].median())","257509ec":"for i in range(5):\n    data[2][i]['Insulin'][62] = data[2][i]['Insulin'].median()","e1ae01b6":"invalidNumberChecker(data)","a76e66f2":"def produceSplits(dataList, testSize=0.2):\n    \"\"\"\n    Description\n    ----\n    Splits a list of dataframes into train and test sets based\n    on given testSize for train_test_split.\n    For each dataframe in dfList, the X_train, X_test, y_train,\n    and y_test are appended into separate lists.\n    Each split will use the same index for all dataframes in \n    dataList[2] to reduce bias when comparing results after \n    modeling.\n    \n    Parameters\n    ----\n    dataList (list of dataframes):\n        The list containing a list of models, list of imputation \n        orders, and list of dataframes which was obtained after \n        running function `imputeEm`.\n    \n    testSize (float):\n        The test_size that you want to give for train_test_split.\n        The default test_size is set to 0.2.\n        \n    Returns\n    ----\n    Xtrain_list (list of dataframes):\n        A list containing the X_train split for each dataframe\n        in dfList.\n    \n    Xtest_list (list of dataframes):\n        A list containing the X_test split for each dataframe\n        in dfList.\n        \n    ytrain_list (list of series):\n        A list containing the y_train split for each dataframe\n        in dfList.\n        \n    ytest_list (list of series):\n        A list containing the y_test split for each dataframe\n        in dfList.\n    \n    \"\"\"\n    \n    # Returned train and test splits lists\n    Xtrain_list = []\n    Xtest_list = []\n    ytrain_list = []\n    ytest_list = []\n    \n    # Loop through each dataframe in dataList[2]\n    for dFrame in range(len(dataList[2])):\n        # Inputs\n        X = dataList[2][dFrame].drop('Outcome', axis=1)\n        # Output\n        y = dataList[2][dFrame]['Outcome']\n        # Train and test split with given testSize, where the default is 0.2\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=testSize, random_state=42, stratify = y)\n        # Append the splits to each list\n        Xtrain_list.append(X_train)\n        Xtest_list.append(X_test)\n        ytrain_list.append(y_train)\n        ytest_list.append(y_test)\n    \n    return Xtrain_list, Xtest_list, ytrain_list, ytest_list","6fe8ffda":"# Production\nX_train_list, X_test_list, y_train_list, y_test_list = produceSplits(data)","62601571":"print(f\"The size of each X_train in X_train_list is {len(X_train_list[4])}\")\nprint(f\"The size of each X_test in X_test_list is {len(X_test_list[4])}\")","ea98cb1b":"X_train_list[7].head()\n\n# EXPAND OUTPUT TO SEE THE FIRST 5 ROWS","5714d800":"X_train_list[13].head()\n\n# EXPAND OUTPUT TO SEE THE FIRST 5 ROWS","4e4060d0":"def weightForMe(trainList, testList):\n    \"\"\"\n    Description\n    ----\n    Standardizes the training and testing input sets \n    using StandardScaler().\n    The training features are fit and then transformed.\n    The testing features are transformed.\n    \n    Parameters\n    ----\n    trainList (list of dataframes):\n        The list of dataframes of the training set obtained\n        after running function `produceSplits`.\n\n    testList (list of dataframes):\n        The list of dataframes of the testing set obtained\n        after running function `produceSplits`.\n        \n    Returns\n    ----\n    list_of_scaled_train_dfs (list of dataframes):\n        List of dataframes of each `X_train` after scale.\n        \n    list_of_scaled_test_dfs (list of dataframes):\n        List of dataframes of each `X_test` after scale.\n    \"\"\"\n    # Returned lists\n    list_of_scaled_train_dfs = []\n    list_of_scaled_test_dfs = []\n    \n    # Iterate through each `X_train` and `X_test` in `trainList`\n    for i in range(len(trainList)):\n        # Introducing the Scaler\n        sclr = StandardScaler()\n        \n        scaled_train_features = sclr.fit_transform(trainList[i]) # fit and transform train set\n        scaled_test_features = sclr.transform(testList[i]) # transform test set\n            \n        # For debugging purposes, I converted the scaled lists to dataframes\n        list_of_scaled_train_dfs.append(pd.DataFrame(scaled_train_features, \n                                                 index = trainList[i].index, \n                                                 columns = trainList[i].columns))\n        list_of_scaled_test_dfs.append(pd.DataFrame(scaled_test_features, \n                                                 index = testList[i].index, \n                                                 columns = testList[i].columns))\n                                   \n    return list_of_scaled_train_dfs, list_of_scaled_test_dfs","cb3e419c":"scaled_train_dfs, scaled_test_dfs = weightForMe(X_train_list, X_test_list)","7c593855":"# Here's an example\nscaled_train_dfs[6].head()","7a37648f":"def acceptingModels(XTrainList, XTestList, yTrainList, yTestList, dataList, classifierList):\n    \"\"\"\n    Description\n    ----\n    This function tests out all models listed in `classifierList`.\n    If the model isn't valid, the function prints out the invalid\n    name of the model along with it's error.\n    The function then fits the model to every train set in\n    `XTrainList` and `yTrainList` and gives a prediction based on\n    the `XTestList`. The accuracy score is then found along with\n    the diabetic positive precision, recall and f-scores, and the\n    diabetic negative prevision, recall and f-scores.\n    Data is then appended into a dictionary with columns:\n        1.  ModelName - Name of model used for predictions.\n        2.  Estimator - Name of estimator used for iterative\n              imputation.\n        3.  Order - The type of imputation order style used.\n        4.  AccuracyScore - The accuracy score of the model's\n              predictions.\n        5.  CorrectPredictionsCount - The number of predictions\n              that the model got correct.\n        6.  Total - The size of XTestList; the total number of\n              patients in the test set.\n        7.  PosPrecScore - The precision score for diabetic\n              positives.\n        8.  PosRecScore - The recall score for diabetic positives.\n        9.  PosFScore - The f1-score for diabetic positives.\n        10. NegPresScore - The precision score for diabetic\n              negatives.\n        11. NegRecScore - The recall score for diabetic negatives.\n        12. NegFScore - The f1-score for diabetic negatives.\n        13. TNPercentage - The ratio of True Negatives to total\n              (The [0][0] index of confusion matrix).\n        14. TPPercentage - The ratio of True Positives to total\n              (The [1][1] index of confusion matrix).\n        15. FNPercentage - The ratio of False Negatives to total\n              (The [1][0] index of confusion matrix).\n        16. FPPercentage - The ratio of False Positives to total\n              (The [0][1] index of confusion matrix).\n    NOTE: \n        TNPercentage + TPPercentage + \n            FNPercentage + FPPercentage = 100.\n    \n    Parameters\n    ----\n    XTrainList (list of dataframes):\n        A list containing the X_train split for each dataframe.\n        \n    XTestList (list of dataframes):\n        A list containing the X_test split for each dataframe.\n        \n    yTrainList (list of series):\n        A list containing the y_train split for each dataframe.\n        \n    yTestList (list of series):\n        A list containing the y_test split for each dataframe.\n    \n    dataList (list of lists):\n        The list containing a list of models, list of imputation \n        orders, and list of dataframes which was obtained after \n        running function `imputeEm`.\n        \n    classifierList (list of models):\n        The list of models chosen.\n    \n    Returns\n    ----\n    dic (dataframe):\n        A dataframe of dic.\n        \n    \"\"\"\n    # Introduce a dictionary\n    dic = {'ModelName': [], 'Estimator': [], 'Order': [], 'AccuracyScore':[], \n           'CorrectPredictionsCount': [], 'Total': [], 'PosPrecScore': [],\n           'PosRecScore': [], 'PosFScore': [], 'NegPrecScore': [], 'NegRecScore': [],\n           'NegFScore': [], 'TNPercentage': [], 'TPPercentage': [], \n           'FNPercentage': [], 'FPPercentage': []}\n    \n    # Deepcopy the classifierList\n    models = deepcopy(classifierList)\n    \n    # Test each models in the list to verify validation\n    for i in range(len(classifierList)):\n        try:\n            model = classifierList[i]\n            model.fit(XTrainList[0], yTrainList[0])\n        except Exception as e:\n            print(\"==============================================================\")\n            print(f\"I wasn't able to score with the model: {classifications_list[i]}\")\n            print(f\"This was the error I've received from my master:\\n\\n{e}.\")\n            print(\"\\nI didn't let it faze me though, for now I've skipped this model.\")\n            print(\"==============================================================\\n\")\n            models.remove(classifierList[i]) # Remove invalid models from list\n    \n    # Loop through all train\/test sets\n    for i in range(len(XTrainList)):\n        # Loop through all models\n        for classifier in range(len(models)):\n            # Destroy parenthesis and anything within\n            modelName = re.sub(r\"\\([^()]*\\)\", '', str(models[classifier]))\n            # Performance\n            model = models[classifier]\n            model.fit(XTrainList[i], yTrainList[i])          \n            pred = model.predict(XTestList[i])\n            # Results\n            acc_score = accuracy_score(yTestList[i], pred)\n            noOfCorrect = accuracy_score(yTestList[i], pred, normalize = False)\n            total = noOfCorrect\/acc_score\n            madConfusing = confusion_matrix(yTestList[i],pred)\n\n            dpps = madConfusing[1][1] \/ (madConfusing[1][1] + madConfusing[0][1]) # diab pos prec score\n            dprs = madConfusing[1][1] \/ (madConfusing[1][1] + madConfusing[1][0]) # diab pos rec score\n            dpfs = 2 * (dpps * dprs) \/ (dpps + dprs) # diab pos f1 score\n            dnps = madConfusing[0][0] \/ (madConfusing[0][0] + madConfusing[1][0]) # diabetic neg prec score\n            dnrs = madConfusing[0][0] \/ (madConfusing[0][0] + madConfusing[0][1]) # diab neg rec score\n            dnfs = 2 * (dnps * dnrs) \/ (dnps + dnrs) # diab neg f1 score\n            \n            # Save everything\n            dic['ModelName'].append(modelName)\n            dic['Estimator'].append(dataList[0][i])\n            dic['Order'].append(dataList[1][i])\n            dic['AccuracyScore'].append(acc_score)\n            dic['CorrectPredictionsCount'].append(noOfCorrect)\n            dic['Total'].append(total)\n            dic['PosPrecScore'].append(dpps)\n            dic['PosRecScore'].append(dprs)\n            dic['PosFScore'].append(dpfs)\n            dic['NegPrecScore'].append(dnps)\n            dic['NegRecScore'].append(dnrs)\n            dic['NegFScore'].append(dnfs)\n            dic['TNPercentage'].append(madConfusing[0][0]\/total*100)\n            dic['TPPercentage'].append(madConfusing[1][1]\/total*100)\n            dic['FNPercentage'].append(madConfusing[1][0]\/total*100)\n            dic['FPPercentage'].append(madConfusing[0][1]\/total*100)\n            \n    return pd.DataFrame.from_dict(dic)","43db7e42":"classifications_list = [\n    LinearSVC(C= 5.0, class_weight=\"balanced\"), SVC(kernel='rbf'), GaussianNB(), \n    KNeighborsClassifier(n_neighbors=7), DecisionTreeClassifier(), RandomForestClassifier(),\n    ExtraTreesClassifier(), AdaBoostClassifier(), GradientBoostingClassifier()\n]","bbdedbe3":"results = acceptingModels(scaled_train_dfs, scaled_test_dfs, y_train_list, y_test_list, data, classifications_list)","18c2116e":"results.head()","26f403fd":"len(results)","5c412835":"results.sort_values(by=['AccuracyScore'], ascending = False).head()","d26cc4c4":"display(results.groupby(['ModelName']).mean().sort_values(by=['AccuracyScore'], ascending = False).iloc[[0, 1, 2]])\ndisplay(results.groupby(['Estimator']).mean().sort_values(by=['AccuracyScore'], ascending = False).iloc[[0]])\ndisplay(results.groupby(['Order']).mean().sort_values(by=['AccuracyScore'], ascending = False).iloc[[0]])","0ff866d6":"# Sorting values by best positive recall score, then best negative recall score\ndisplay(results.groupby(['ModelName']).mean().sort_values(by=['PosRecScore', 'NegRecScore'], ascending = False).iloc[[0, 1]])","747639a3":"# Sorting values by best negative recall score, then best positive recall score\ndisplay(results.groupby(['ModelName']).mean().sort_values(by=['NegRecScore', 'PosRecScore'], ascending = False).iloc[[0, 1]])","22081a11":"results.sort_values(by=['AccuracyScore']).head()","6b88a718":"display(results.groupby(['ModelName']).mean().sort_values(by=['AccuracyScore']).iloc[[0, 1, 2]])\ndisplay(results.groupby(['Estimator']).mean().sort_values(by=['AccuracyScore']).iloc[[0]])\ndisplay(results.groupby(['Order']).mean().sort_values(by=['AccuracyScore']).iloc[[0]])","3c983af2":"# Sorting values by worst positive recall score, then worst negative recall score\ndisplay(results.groupby(['ModelName']).mean().sort_values(by=['PosRecScore', 'NegRecScore']).iloc[[0, 1]])","253dc7c3":"# Sorting values by worst negative recall score, then worst positive recall score\ndisplay(results.groupby(['ModelName']).mean().sort_values(by=['NegRecScore', 'PosRecScore']).iloc[[0, 1]])","034a9b72":"def acceptingModelsNoIterImp(XTrain, XTest, yTrain, yTest, classifierList):\n    \"\"\"\n    Description\n    ----\n    This function tests out all models listed in `classifierList`\n    without Iterative Imputation.\n    If the model isn't valid, the function prints out the invalid\n    name of the model along with it's error.\n    The function then fits the model to the train set and gives a \n    prediction based on the `XTest`. The accuracy score is then \n    found along with the diabetic positive precision, recall and \n    f-scores, and the diabetic negative prevision, recall and \n    f-scores.\n    \n    Data is then appended into a dictionary with columns:\n        1.  ModelName - Name of model used for predictions.\n        2.  AccuracyScore - The accuracy score of the model's\n              predictions.\n        3.  CorrectPredictionsCount - The number of predictions\n              that the model got correct.\n        4.  Total - The size of XTestList; the total number of\n              patients in the test set.\n        5.  PosPrecScore - The precision score for diabetic\n              positives.\n        6.  PosRecScore - The recall score for diabetic positives.\n        7.  PosFScore - The f1-score for diabetic positives.\n        8.  NegPresScore - The precision score for diabetic\n              negatives.\n        9.  NegRecScore - The recall score for diabetic negatives.\n        10. NegFScore - The f1-score for diabetic negatives.\n        11. TNPercentage - The ratio of True Negatives to total\n              (The [0][0] index of confusion matrix).\n        12. TPPercentage - The ratio of True Positives to total\n              (The [1][1] index of confusion matrix).\n        13. FNPercentage - The ratio of False Negatives to total\n              (The [1][0] index of confusion matrix).\n        14. FPPercentage - The ratio of False Positives to total\n              (The [0][1] index of confusion matrix).\n    NOTE: \n        TNPercentage + TPPercentage + \n            FNPercentage + FPPercentage = 100.\n    \n    Parameters\n    ----\n    XTrain (dataframe\/series):\n        The X_train split for the dataframe.\n        \n    XTest (dataframe\/series):\n        The X_test split for the dataframe.\n        \n    yTrain (series):\n        The y_train split for the dataframe.\n        \n    yTest (series):\n        The y_test split for the dataframe.\n        \n    classifierList (list of models):\n        The list of models chosen.\n    \n    Returns\n    ----\n    dic (dataframe):\n        A dataframe of dic.\n        \n    \"\"\"\n    # Introduce a dictionary\n    dic = {'ModelName': [], 'AccuracyScore':[],\n           'CorrectPredictionsCount': [], 'Total': [], 'PosPrecScore': [],\n           'PosRecScore': [], 'PosFScore': [], 'NegPrecScore': [], 'NegRecScore': [],\n           'NegFScore': [], 'TNPercentage': [], 'TPPercentage': [], \n           'FNPercentage': [], 'FPPercentage': []}\n    \n    # Deepcopy the classifierList\n    models = deepcopy(classifierList)\n    \n    # Test each models in the list to verify validation\n    for i in range(len(classifierList)):\n        try:\n            model = classifierList[i]\n            model.fit(XTrain, yTrain)\n        except Exception as e:\n            print(\"==============================================================\")\n            print(f\"I wasn't able to score with the model: {classifications_list[i]}\")\n            print(f\"This was the error I've received from my master:\\n\\n{e}.\")\n            print(\"\\nI didn't let it faze me though, for now I've skipped this model.\")\n            print(\"==============================================================\\n\")\n            models.remove(classifierList[i]) # Remove invalid models from list\n    \n    # Loop through all models\n    for classifier in range(len(models)):\n        # Destroy parenthesis and anything within\n        modelName = re.sub(r\"\\([^()]*\\)\", '', str(models[classifier]))\n        # Performance\n        model = models[classifier]\n        model.fit(XTrain, yTrain)          \n        pred = model.predict(XTest)\n        # Results\n        acc_score = accuracy_score(yTest, pred)\n        noOfCorrect = accuracy_score(yTest, pred, normalize = False)\n        total = noOfCorrect\/acc_score\n        madConfusing = confusion_matrix(yTest,pred)\n\n        dpps = madConfusing[1][1] \/ (madConfusing[1][1] + madConfusing[0][1]) # diab pos prec score\n        dprs = madConfusing[1][1] \/ (madConfusing[1][1] + madConfusing[1][0]) # diab pos rec score\n        dpfs = 2 * (dpps * dprs) \/ (dpps + dprs) # diab pos f1 score\n        dnps = madConfusing[0][0] \/ (madConfusing[0][0] + madConfusing[1][0]) # diabetic neg prec score\n        dnrs = madConfusing[0][0] \/ (madConfusing[0][0] + madConfusing[0][1]) # diab neg rec score\n        dnfs = 2 * (dnps * dnrs) \/ (dnps + dnrs) # diab neg f1 score\n\n        # Save everything\n        dic['ModelName'].append(modelName)\n        dic['AccuracyScore'].append(acc_score)\n        dic['CorrectPredictionsCount'].append(noOfCorrect)\n        dic['Total'].append(total)\n        dic['PosPrecScore'].append(dpps)\n        dic['PosRecScore'].append(dprs)\n        dic['PosFScore'].append(dpfs)\n        dic['NegPrecScore'].append(dnps)\n        dic['NegRecScore'].append(dnrs)\n        dic['NegFScore'].append(dnfs)\n        dic['TNPercentage'].append(madConfusing[0][0]\/total*100)\n        dic['TPPercentage'].append(madConfusing[1][1]\/total*100)\n        dic['FNPercentage'].append(madConfusing[1][0]\/total*100)\n        dic['FPPercentage'].append(madConfusing[0][1]\/total*100)\n            \n    return pd.DataFrame.from_dict(dic)","5f0b2932":"def magicWithoutIterImp(adf, testSize = 0.2):\n    \"\"\"\n    Description\n    ----\n    Splits and scales a single given dataframe using\n    `StandardScaler()`. The scaled features are then\n    inputted into `acceptingModelsNoIterImp` and out\n    comes a dataframe.\n    \n    Parameters\n    ----\n    adf (dataframe):\n        The dataframe to use for modeling.\n    \n    testSize (float):\n        The test_size that you want to give for \n        train_test_split.\n        The default test_size is set to 0.2.\n    \n    Returns\n    ----\n    save (dataframe):\n        The dataframe after running the splits in\n        `acceptingModelsNoIterImp`.\n        \n    \"\"\"\n    # Define X\n    X = adf.drop('Outcome', axis=1)\n    # Define y\n    y = adf['Outcome']\n\n    # Split\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = testSize, random_state = 42, stratify = y)\n\n    # Scale\n    sclr = StandardScaler()\n    scaled_train_features = sclr.fit_transform(X_train) \n    scaled_test_features = sclr.transform(X_test) \n\n    # Save\n    save = acceptingModelsNoIterImp(scaled_train_features, scaled_test_features, y_train, y_test, classifications_list)\n    \n    return save","860ac18f":"def finalResults(adf, saves):\n    \"\"\"\n    Description\n    ----\n    Prints the overall average of the method\/technique's \n    `AccuracyScore` column.\n    Displays the information of the best performed model\n    for the current method.\n    Saves data into a dictionary only if the name of the\n    dataframe `adf` does not exist in saves['MethodName'],\n    and finally displays whatever data is in `saves`.\n    \n    Parameters\n    ----\n    adf (dataframe):\n        The dataframe of the results of a method\/technique.\n        \n    saves (dict):\n        The dictionary containing all final results.\n        \n    Returns\n    ----\n    Nothing.\n        \n    \"\"\"\n    \n    print(f\"Overall Accuracy Score: {adf['AccuracyScore'].mean()}\") # Overall average of method\/technique's `AccuracyScore` column\n    print('Current Method\\'s Top Model:')\n    display(adf.iloc[[adf['AccuracyScore'].idxmax()]]) \n    \n    if adf.name not in saves['MethodName']:\n        issaSeries = adf.iloc[adf['AccuracyScore'].idxmax()]\n        saves['MethodName'].append(adf.name) # Dataframe name (Method\/Technique name)\n        saves['MethodAccuracy'].append(adf['AccuracyScore'].mean()) # Overall average of method\/technique's `AccuracyScore` column\n        saves['TopModelName'].append(issaSeries['ModelName'])  # Best performed model name\n        if 'Estimator' in adf.columns:\n            saves['Estimator'].append(issaSeries['Estimator']) # Estimator used for iterative imputer if applicable\n            saves['Order'].append(issaSeries['Order']) # Order used for iterative imputer if applicable\n        else:\n            saves['Estimator'].append('None')\n            saves['Order'].append('None')\n        saves['ModelAccuracy'].append(issaSeries['AccuracyScore']) # Accuracy score of the best performed model\n        saves['CorrectPredictionsCount'].append(issaSeries['CorrectPredictionsCount']) # Number of correct predictions\n        saves['Total'].append(issaSeries['Total']) # Size of test set\/total number of patients in test set\n    display(saves)","0bc7dd1a":"nextTopMethod = {'MethodName': [], 'MethodAccuracy': [], 'TopModelName': [],\n                 'Estimator': [], 'Order': [], 'ModelAccuracy': [], \n                 'CorrectPredictionsCount': [], 'Total': []}\nresults.name = 'results'\n\nnextTopMethod","9131f8b9":"finalResults(results, nextTopMethod)","b743e22f":"dfMeanMed.head()\n\n# EXPAND OUTPUT TO SEE THE FIRST 5 ROWS","cd2c7d98":"meanMed = magicWithoutIterImp(dfMeanMed)\nmeanMed.name = 'meanMed' # Give `meanMed` a name","d2f61e89":"finalResults(meanMed, nextTopMethod)","5007b158":"keepZeros = magicWithoutIterImp(df)\nkeepZeros.name = 'keepZeros'\nfinalResults(keepZeros, nextTopMethod)\n\n# EXPAND OUTPUT TO SEE THE RESULTS SO FAR","5e8b36bf":"ndf.isnull().mean() * 100","754c3f95":"cdf = ndf.copy(deep = True)\ncdf = cdf.drop(columns=['Insulin'])\ncolsToFix = ['Glucose', 'BloodPressure', 'SkinThickness', 'BMI'] # `Insulin` removed from `colsToFix`\n\ndata_list = imputeEm(cdf, estimatorList, imputation_styles)","903e2183":"invalidNumberChecker(data_list)","a11ea0c7":"X_train_list, X_test_list, y_train_list, y_test_list = produceSplits(data_list)\nX_train_scaled, X_test_scaled = weightForMe(X_train_list, X_test_list)\nnoInsulin = acceptingModels(X_train_scaled, X_test_scaled, y_train_list, y_test_list, data_list, classifications_list)","f5923901":"noInsulin.name = 'noInsulin' \nfinalResults(noInsulin, nextTopMethod)\n\n# EXPAND OUTPUT TO SEE THE RESULTS SO FAR","4fe1e197":"cdf = ndf.copy(deep = True)\ncdf = cdf.drop(columns=['SkinThickness'])\ncolsToFix = ['Glucose', 'BloodPressure', 'Insulin', 'BMI'] # `SkinThickness` removed from `colsToFix`\n\ndata_list = imputeEm(cdf, estimatorList, imputation_styles)","785cc05a":"invalidNumberChecker(data_list)\n\n# EXPAND OUTPUT BELOW TO VIEW INVALID NUMBERS","a47ab5c1":"for i in range(5):\n    data_list[2][i]['Insulin'][62] = data_list[2][i]['Insulin'].median()\ninvalidNumberChecker(data_list)","0042ba21":"X_train_list, X_test_list, y_train_list, y_test_list = produceSplits(data_list)\nX_train_scaled, X_test_scaled = weightForMe(X_train_list, X_test_list)\nnoThickSkin = acceptingModels(X_train_scaled, X_test_scaled, y_train_list, y_test_list, data_list, classifications_list)","daf9c3d5":"noThickSkin.name = 'noThickSkin' \nfinalResults(noThickSkin, nextTopMethod)\n\n# EXPAND OUTPUT TO SEE THE RESULTS SO FAR","9aefdce1":"cdf = ndf.copy(deep = True)\ncdf = cdf.drop(columns=['SkinThickness', 'Insulin'])\ncolsToFix = ['Glucose', 'BloodPressure', 'BMI'] # `Insulin` and `SkinThickness` removed from `colsToFix`\n\ndata_list = imputeEm(cdf, estimatorList, imputation_styles)","6d30d50f":"invalidNumberChecker(data_list)","6310a991":"X_train_list, X_test_list, y_train_list, y_test_list = produceSplits(data_list)\nX_train_scaled, X_test_scaled = weightForMe(X_train_list, X_test_list)\nnoInsulinThickSkin = acceptingModels(X_train_scaled, X_test_scaled, y_train_list, y_test_list, data_list, classifications_list)","3d055755":"noInsulinThickSkin.name = 'noInsulinThickSkin' \nfinalResults(noInsulinThickSkin, nextTopMethod)\n\n# EXPAND OUTPUT TO SEE THE RESULTS SO FAR","ee669aca":"cdf = ndf.copy(deep=True)\ncdf = cdf.drop(cdf[(cdf['Glucose'].isnull()) |\n                     (cdf['BloodPressure'].isnull()) | \n                     (cdf['BMI'].isnull()) | \n                     (cdf['SkinThickness'].isnull())].index)\n\ncdf.info()\n\n# EXPAND OUTPUT BELOW TO VIEW `cdf.info()`","113750b8":"cdf['Insulin'].fillna(0, inplace = True)\n\ndropRepInsZero = magicWithoutIterImp(cdf)\ndropRepInsZero.name = 'dropRepInsZero'\n\nfinalResults(dropRepInsZero, nextTopMethod)\n\n# EXPAND OUTPUT TO SEE THE RESULTS SO FAR","8f3d8ac4":"cdf = ndf.copy(deep=True)\ncdf = cdf.drop(cdf[(cdf['Glucose'].isnull()) |\n                     (cdf['BloodPressure'].isnull()) | \n                     (cdf['BMI'].isnull()) | \n                     (cdf['SkinThickness'].isnull())].index)\ncdf['Insulin'].fillna(cdf['Insulin'].median(), inplace = True)\n\ndropRepInsMed = magicWithoutIterImp(cdf)\ndropRepInsMed.name = 'dropRepInsMed'\n\nfinalResults(dropRepInsMed, nextTopMethod)\n\n# EXPAND OUTPUT TO SEE THE RESULTS SO FAR","f46f0e29":"cdf = ndf.copy(deep=True)\ncdf = cdf.drop(cdf[(cdf['Glucose'].isnull()) |\n                     (cdf['BloodPressure'].isnull()) | \n                     (cdf['BMI'].isnull()) | \n                     (cdf['SkinThickness'].isnull())].index)\n\ncolsToFix = ['Insulin'] # Remember to update `colsToFix`\ndata_list = imputeEm(cdf, estimatorList, imputation_styles)\ninvalidNumberChecker(data_list)","dbf77f30":"X_train_list, X_test_list, y_train_list, y_test_list = produceSplits(data_list)\nX_train_scaled, X_test_scaled = weightForMe(X_train_list, X_test_list)\ndropRepInsIter = acceptingModels(X_train_scaled, X_test_scaled, y_train_list, y_test_list, data_list, classifications_list)\n\ndropRepInsIter.name = 'dropRepInsIter' \nfinalResults(dropRepInsIter, nextTopMethod)\n\n# EXPAND OUTPUT TO SEE THE RESULTS SO FAR","dad12a7e":"cdf = ndf.copy(deep=True)\ncdf = cdf.drop(cdf[(cdf['Glucose'].isnull()) |\n                     (cdf['BloodPressure'].isnull()) | \n                     (cdf['BMI'].isnull()) | \n                     (cdf['SkinThickness'].isnull())].index)\n\ncdf.drop(columns=['Insulin'], inplace=True)\ncdf.info()\n\n# EXPAND OUTPUT BELOW TO VIEW `cdf.info()`","525f08e6":"dropNullRemIns = magicWithoutIterImp(cdf)\ndropNullRemIns.name = 'dropNullRemIns'\n\nfinalResults(dropNullRemIns, nextTopMethod)\n\n# EXPAND OUTPUT TO SEE THE RESULTS SO FAR","9c1bcc55":"ndf.drop(ndf[ndf['Insulin'].isnull()].index).info()","69461539":"pd.DataFrame.from_dict(nextTopMethod).sort_values(by=['MethodAccuracy'], ascending = False)","440a34de":"def gimmeThemStats(dFrame):\n    \"\"\"\n    Description\n    ----\n    Outputs the general statistical description of the dataframe,\n    outputs the correlation heatmap, and outputs a distribution plot.\n    \n    Parameters\n    ----\n    dFrame(DataFrame):\n        The dataframe for which information will be displayed.\n        \n    Returns\n    ----\n    Nothing.\n    \n    \"\"\"\n    # Description\n    print(\"Descriptive Stats:\")\n    display(dFrame.describe().T)\n    \n    # Heatmap\n    plt.figure(figsize=(10, 8)) \n    plt.title(\"Heatmap\", fontsize = 'x-large')\n    sns.heatmap(dFrame.corr(), annot =True)\n    \n    # Distribution\n    ### NOTE: I changed histplot to distplot\n    fig, axes = plt.subplots(4, 2, figsize=(14,14))\n    fig.suptitle(\"Distribution Plot\", y=0.92, fontsize='x-large')\n    fig.tight_layout(pad=4.0)\n\n    for i,j in enumerate(df.columns[:-1]):\n        sns.distplot(dFrame[j], ax=axes[i\/\/2, i%2])\n        \n        \ndef imputeEm(adf, estimList, stylesList):\n    \"\"\"\n    Description\n    ----\n    Iteratively imputes missing values to a dataset by following a \n    given estimator and imputation-order pair.\n    If an estimator or imputation order throws an error, an error\n    will be printed after function call explaining why the error\n    occured. If no error is found for a given estimator or imputation\n    order, no errors will be printed in the end of the function call.\n    An error thrown for an estimator and imputation order pair won't \n    affect other pairs. You'll still get the results you sought for.\n    \n    Parameters\n    ----\n    adf (dataframe):\n        The dataframe containing missing values.\n        \n    estimList (list of models):\n        The list of estimators to use in IterativeImputer.\n    \n    stylesList (list of str):\n        The list of styles to use in IterativeImputer.\n    \n    Returns\n    ----\n    estim_name_list (list of str):\n        A list of the name of the estimator used in each iteration.\n        For example, if there are 5 imputation-order styles per each \n        estimator, then the list will contain each estimator 5 times.\n    \n    style_list (list of str):\n        A list of the name of the imputation-order used in each\n        iteration. For example, if there are 10 estimators used, \n        the list will include each imputation-order 10 times.\n        \n    imputed_df_list (list of dataframes):\n        A list of dataframes for each estimator and imputation-order\n        pair.\n    \"\"\"\n    \n    # The returned lists\n    style_list = []\n    estim_name_list = []\n    imputed_df_list = []\n    \n    # Loop through each estimator\n    for estim in range(len(estimList)):\n        \n        # Convert estimator to string format and debolish parenthesis and anything in between\n        estimstorName = re.sub(r\"\\([^()]*\\)\", '', str(estimList[estim])) \n        \n        # Loop through each imputation-order\n        for style in stylesList:\n            \n            try:\n                # Introduce Iterative Imputer with estimator and imputation_order\n                imputer = IterativeImputer(random_state=42, estimator=estimList[estim], imputation_order=style)\n                # Fit on dataframe\n                imputer.fit(adf)\n            except Exception as e:\n                print(\"==============================================================\")\n                print(f\"I wasn't able to iteratively impute with the estimator: {estimList[estim]} and imputation order: {style}.\")\n                print(f\"This was the error I've received from my master:\\n\\n{e}.\")\n                print(\"\\nI didn't let it faze me though, for now I've skipped this imputation pair.\")\n                print(\"==============================================================\\n\")\n            else:\n                estim_name_list.append(estimstorName) #Appending estimator name\n                style_list.append(style) #Appending style name\n                \n                # Transform and append the imputed dataframe to the list of imputed dataframes\n                imputed_df_list.append(pd.DataFrame(imputer.transform(adf), columns = adf.columns))\n            \n            \n    return estim_name_list, style_list, imputed_df_list\n\n\ndef invalidNumberChecker(dataList):\n    \"\"\"\n    Description\n    ----\n    This function will check for values less than or equal to 0 within a dataframe. \n    The function displays several things including: \n        1. The Dataframe number: The ith dataframe in `dataList[2][i]` in which the \n        invalid number was caught.\n        2. Estimator: Estimator used in the dataframe number.\n        3. Order: Imputation order used in the dataframe number.\n        4. A set of description for each invalid number within the dataframe which \n        includes:\n            4a. Index: The index of the dataframe where the invalid number lives.\n            4b. Column: The column that contains the invalid number.\n            4c. Value: The invalid number itself.\n        5. A dataframe display of the rows with invalid number(s).\n    \n    Parameters\n    ----\n    dataList (list of lists):\n        The list containing a list of models, list of imputation orders, and\n        list of dataframes which was obtained after running function `imputeEm`.\n    \n    Returns\n    ----\n    Nothing.\n    \n    \"\"\"\n    # Loop through every dataframe in the list of dataframes\n    for i in range(len(dataList[2])):\n        # index_list will hold the indices where invalid numbers live\n        index_list = []\n        # invalid_pairs is a list containing pairs (tuples) of rows and column names where invalid numbers live\n        invalid_pairs = dataList[2][i][colsToFix][dataList[2][i][colsToFix] <= 0].stack().index.tolist()\n        if(invalid_pairs):\n            print(f'Dataframe # {i}  --  For reference, check dataList[2][{i}], where dataList is the list obtained after running function `imputeEm`.')\n            print('--------------')\n            print(f'Estimator: {dataList[0][i]}\\nOrder: {dataList[1][i]}\\n')\n            for j in range(len(invalid_pairs)):\n                index_list.append(invalid_pairs[j][0])\n                print(f'Index: {invalid_pairs[j][0]}\\nColumn: {invalid_pairs[j][1]}')\n                print(f'Value: {dataList[2][i][invalid_pairs[j][1]].loc[invalid_pairs[j][0]]}\\n')\n            display(dataList[2][i].loc[index_list])\n            print(\"==================================================================================================\\n\\n\")\n            \n\ndef produceSplits(dataList, testSize=0.2):\n    \"\"\"\n    Description\n    ----\n    Splits a list of dataframes into train and test sets based\n    on given testSize for train_test_split.\n    For each dataframe in dfList, the X_train, X_test, y_train,\n    and y_test are appended into separate lists.\n    Each split will use the same index for all dataframes in \n    dataList[2] to reduce bias when comparing results after \n    modeling.\n    \n    Parameters\n    ----\n    dataList (list of dataframes):\n        The list containing a list of models, list of imputation \n        orders, and list of dataframes which was obtained after \n        running function `imputeEm`.\n    \n    testSize (float):\n        The test_size that you want to give for train_test_split.\n        The default test_size is set to 0.2.\n        \n    Returns\n    ----\n    Xtrain_list (list of dataframes):\n        A list containing the X_train split for each dataframe\n        in dfList.\n    \n    Xtest_list (list of dataframes):\n        A list containing the X_test split for each dataframe\n        in dfList.\n        \n    ytrain_list (list of series):\n        A list containing the y_train split for each dataframe\n        in dfList.\n        \n    ytest_list (list of series):\n        A list containing the y_test split for each dataframe\n        in dfList.\n    \n    \"\"\"\n    \n    # Returned train and test splits lists\n    Xtrain_list = []\n    Xtest_list = []\n    ytrain_list = []\n    ytest_list = []\n    \n    # Loop through each dataframe in dataList[2]\n    for dFrame in range(len(dataList[2])):\n        # Inputs\n        X = dataList[2][dFrame].drop('Outcome', axis=1)\n        # Output\n        y = dataList[2][dFrame]['Outcome']\n        # Train and test split with given testSize, where the default is 0.2\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=testSize, random_state=42, stratify = y)\n        # Append the splits to each list\n        Xtrain_list.append(X_train)\n        Xtest_list.append(X_test)\n        ytrain_list.append(y_train)\n        ytest_list.append(y_test)\n    \n    return Xtrain_list, Xtest_list, ytrain_list, ytest_list\n\n\ndef weightForMe(trainList, testList):\n    \"\"\"\n    Description\n    ----\n    Standardizes the training and testing input sets \n    using StandardScaler().\n    The training features are fit and then transformed.\n    The testing features are transformed.\n    \n    Parameters\n    ----\n    trainList (list of dataframes):\n        The list of dataframes of the training set obtained\n        after running function `produceSplits`.\n\n    testList (list of dataframes):\n        The list of dataframes of the testing set obtained\n        after running function `produceSplits`.\n        \n    Returns\n    ----\n    list_of_scaled_train_dfs (list of dataframes):\n        List of dataframes of each `X_train` after scale.\n        \n    list_of_scaled_test_dfs (list of dataframes):\n        List of dataframes of each `X_test` after scale.\n    \"\"\"\n    # Returned lists\n    list_of_scaled_train_dfs = []\n    list_of_scaled_test_dfs = []\n    \n    # Iterate through each `X_train` and `X_test` in `trainList`\n    for i in range(len(trainList)):\n        # Introducing the Scaler\n        sclr = StandardScaler()\n        \n        scaled_train_features = sclr.fit_transform(trainList[i]) # fit and transform train set\n        scaled_test_features = sclr.transform(testList[i]) # transform test set\n            \n        # For debugging purposes, I converted the scaled lists to dataframes\n        list_of_scaled_train_dfs.append(pd.DataFrame(scaled_train_features, \n                                                 index = trainList[i].index, \n                                                 columns = trainList[i].columns))\n        list_of_scaled_test_dfs.append(pd.DataFrame(scaled_test_features, \n                                                 index = testList[i].index, \n                                                 columns = testList[i].columns))\n                                   \n    return list_of_scaled_train_dfs, list_of_scaled_test_dfs\n\n\ndef acceptingModels(XTrainList, XTestList, yTrainList, yTestList, dataList, classifierList):\n    \"\"\"\n    Description\n    ----\n    This function tests out all models listed in `classifierList`.\n    If the model isn't valid, the function prints out the invalid\n    name of the model along with it's error.\n    The function then fits the model to every train set in\n    `XTrainList` and `yTrainList` and gives a prediction based on\n    the `XTestList`. The accuracy score is then found along with\n    the diabetic positive precision, recall and f-scores, and the\n    diabetic negative prevision, recall and f-scores.\n    Data is then appended into a dictionary with columns:\n        1.  ModelName - Name of model used for predictions.\n        2.  Estimator - Name of estimator used for iterative\n              imputation.\n        3.  Order - The type of imputation order style used.\n        4.  AccuracyScore - The accuracy score of the model's\n              predictions.\n        5.  CorrectPredictionsCount - The number of predictions\n              that the model got correct.\n        6.  Total - The size of XTestList; the total number of\n              patients in the test set.\n        7.  PosPrecScore - The precision score for diabetic\n              positives.\n        8.  PosRecScore - The recall score for diabetic positives.\n        9.  PosFScore - The f1-score for diabetic positives.\n        10. NegPresScore - The precision score for diabetic\n              negatives.\n        11. NegRecScore - The recall score for diabetic negatives.\n        12. NegFScore - The f1-score for diabetic negatives.\n        13. TNPercentage - The ratio of True Negatives to total\n              (The [0][0] index of confusion matrix).\n        14. TPPercentage - The ratio of True Positives to total\n              (The [1][1] index of confusion matrix).\n        15. FNPercentage - The ratio of False Negatives to total\n              (The [1][0] index of confusion matrix).\n        16. FPPercentage - The ratio of False Positives to total\n              (The [0][1] index of confusion matrix).\n    NOTE: \n        TNPercentage + TPPercentage + \n            FNPercentage + FPPercentage = 100.\n    \n    Parameters\n    ----\n    XTrainList (list of dataframes):\n        A list containing the X_train split for each dataframe.\n        \n    XTestList (list of dataframes):\n        A list containing the X_test split for each dataframe.\n        \n    yTrainList (list of series):\n        A list containing the y_train split for each dataframe.\n        \n    yTestList (list of series):\n        A list containing the y_test split for each dataframe.\n    \n    dataList (list of lists):\n        The list containing a list of models, list of imputation \n        orders, and list of dataframes which was obtained after \n        running function `imputeEm`.\n        \n    classifierList (list of models):\n        The list of models chosen.\n    \n    Returns\n    ----\n    dic (dataframe):\n        A dataframe of dic.\n        \n    \"\"\"\n    # Introduce a dictionary\n    dic = {'ModelName': [], 'Estimator': [], 'Order': [], 'AccuracyScore':[], \n           'CorrectPredictionsCount': [], 'Total': [], 'PosPrecScore': [],\n           'PosRecScore': [], 'PosFScore': [], 'NegPrecScore': [], 'NegRecScore': [],\n           'NegFScore': [], 'TNPercentage': [], 'TPPercentage': [], \n           'FNPercentage': [], 'FPPercentage': []}\n    \n    # Deepcopy the classifierList\n    models = deepcopy(classifierList)\n    \n    # Test each models in the list to verify validation\n    for i in range(len(classifierList)):\n        try:\n            model = classifierList[i]\n            model.fit(XTrainList[0], yTrainList[0])\n        except Exception as e:\n            print(\"==============================================================\")\n            print(f\"I wasn't able to score with the model: {classifications_list[i]}\")\n            print(f\"This was the error I've received from my master:\\n\\n{e}.\")\n            print(\"\\nI didn't let it faze me though, for now I've skipped this model.\")\n            print(\"==============================================================\\n\")\n            models.remove(classifierList[i]) # Remove invalid models from list\n    \n    # Loop through all train\/test sets\n    for i in range(len(XTrainList)):\n        # Loop through all models\n        for classifier in range(len(models)):\n            # Destroy parenthesis and anything within\n            modelName = re.sub(r\"\\([^()]*\\)\", '', str(models[classifier]))\n            # Performance\n            model = models[classifier]\n            model.fit(XTrainList[i], yTrainList[i])          \n            pred = model.predict(XTestList[i])\n            # Results\n            acc_score = accuracy_score(yTestList[i], pred)\n            noOfCorrect = accuracy_score(yTestList[i], pred, normalize = False)\n            total = noOfCorrect\/acc_score\n            madConfusing = confusion_matrix(yTestList[i],pred)\n\n            dpps = madConfusing[1][1] \/ (madConfusing[1][1] + madConfusing[0][1]) # diab pos prec score\n            dprs = madConfusing[1][1] \/ (madConfusing[1][1] + madConfusing[1][0]) # diab pos rec score\n            dpfs = 2 * (dpps * dprs) \/ (dpps + dprs) # diab pos f1 score\n            dnps = madConfusing[0][0] \/ (madConfusing[0][0] + madConfusing[1][0]) # diabetic neg prec score\n            dnrs = madConfusing[0][0] \/ (madConfusing[0][0] + madConfusing[0][1]) # diab neg rec score\n            dnfs = 2 * (dnps * dnrs) \/ (dnps + dnrs) # diab neg f1 score\n            \n            # Save everything\n            dic['ModelName'].append(modelName)\n            dic['Estimator'].append(dataList[0][i])\n            dic['Order'].append(dataList[1][i])\n            dic['AccuracyScore'].append(acc_score)\n            dic['CorrectPredictionsCount'].append(noOfCorrect)\n            dic['Total'].append(total)\n            dic['PosPrecScore'].append(dpps)\n            dic['PosRecScore'].append(dprs)\n            dic['PosFScore'].append(dpfs)\n            dic['NegPrecScore'].append(dnps)\n            dic['NegRecScore'].append(dnrs)\n            dic['NegFScore'].append(dnfs)\n            dic['TNPercentage'].append(madConfusing[0][0]\/total*100)\n            dic['TPPercentage'].append(madConfusing[1][1]\/total*100)\n            dic['FNPercentage'].append(madConfusing[1][0]\/total*100)\n            dic['FPPercentage'].append(madConfusing[0][1]\/total*100)\n            \n    return pd.DataFrame.from_dict(dic)\n\n\ndef acceptingModelsNoIterImp(XTrain, XTest, yTrain, yTest, classifierList):\n    \"\"\"\n    Description\n    ----\n    This function tests out all models listed in `classifierList`\n    without Iterative Imputation.\n    If the model isn't valid, the function prints out the invalid\n    name of the model along with it's error.\n    The function then fits the model to the train set and gives a \n    prediction based on the `XTest`. The accuracy score is then \n    found along with the diabetic positive precision, recall and \n    f-scores, and the diabetic negative prevision, recall and \n    f-scores.\n    \n    Data is then appended into a dictionary with columns:\n        1.  ModelName - Name of model used for predictions.\n        2.  AccuracyScore - The accuracy score of the model's\n              predictions.\n        3.  CorrectPredictionsCount - The number of predictions\n              that the model got correct.\n        4.  Total - The size of XTestList; the total number of\n              patients in the test set.\n        5.  PosPrecScore - The precision score for diabetic\n              positives.\n        6.  PosRecScore - The recall score for diabetic positives.\n        7.  PosFScore - The f1-score for diabetic positives.\n        8.  NegPresScore - The precision score for diabetic\n              negatives.\n        9.  NegRecScore - The recall score for diabetic negatives.\n        10. NegFScore - The f1-score for diabetic negatives.\n        11. TNPercentage - The ratio of True Negatives to total\n              (The [0][0] index of confusion matrix).\n        12. TPPercentage - The ratio of True Positives to total\n              (The [1][1] index of confusion matrix).\n        13. FNPercentage - The ratio of False Negatives to total\n              (The [1][0] index of confusion matrix).\n        14. FPPercentage - The ratio of False Positives to total\n              (The [0][1] index of confusion matrix).\n    NOTE: \n        TNPercentage + TPPercentage + \n            FNPercentage + FPPercentage = 100.\n    \n    Parameters\n    ----\n    XTrain (dataframe\/series):\n        The X_train split for the dataframe.\n        \n    XTest (dataframe\/series):\n        The X_test split for the dataframe.\n        \n    yTrain (series):\n        The y_train split for the dataframe.\n        \n    yTest (series):\n        The y_test split for the dataframe.\n        \n    classifierList (list of models):\n        The list of models chosen.\n    \n    Returns\n    ----\n    dic (dataframe):\n        A dataframe of dic.\n        \n    \"\"\"\n    # Introduce a dictionary\n    dic = {'ModelName': [], 'AccuracyScore':[],\n           'CorrectPredictionsCount': [], 'Total': [], 'PosPrecScore': [],\n           'PosRecScore': [], 'PosFScore': [], 'NegPrecScore': [], 'NegRecScore': [],\n           'NegFScore': [], 'TNPercentage': [], 'TPPercentage': [], \n           'FNPercentage': [], 'FPPercentage': []}\n    \n    # Deepcopy the classifierList\n    models = deepcopy(classifierList)\n    \n    # Test each models in the list to verify validation\n    for i in range(len(classifierList)):\n        try:\n            model = classifierList[i]\n            model.fit(XTrain, yTrain)\n        except Exception as e:\n            print(\"==============================================================\")\n            print(f\"I wasn't able to score with the model: {classifications_list[i]}\")\n            print(f\"This was the error I've received from my master:\\n\\n{e}.\")\n            print(\"\\nI didn't let it faze me though, for now I've skipped this model.\")\n            print(\"==============================================================\\n\")\n            models.remove(classifierList[i]) # Remove invalid models from list\n    \n    # Loop through all models\n    for classifier in range(len(models)):\n        # Destroy parenthesis and anything within\n        modelName = re.sub(r\"\\([^()]*\\)\", '', str(models[classifier]))\n        # Performance\n        model = models[classifier]\n        model.fit(XTrain, yTrain)          \n        pred = model.predict(XTest)\n        # Results\n        acc_score = accuracy_score(yTest, pred)\n        noOfCorrect = accuracy_score(yTest, pred, normalize = False)\n        total = noOfCorrect\/acc_score\n        madConfusing = confusion_matrix(yTest,pred)\n\n        dpps = madConfusing[1][1] \/ (madConfusing[1][1] + madConfusing[0][1]) # diab pos prec score\n        dprs = madConfusing[1][1] \/ (madConfusing[1][1] + madConfusing[1][0]) # diab pos rec score\n        dpfs = 2 * (dpps * dprs) \/ (dpps + dprs) # diab pos f1 score\n        dnps = madConfusing[0][0] \/ (madConfusing[0][0] + madConfusing[1][0]) # diabetic neg prec score\n        dnrs = madConfusing[0][0] \/ (madConfusing[0][0] + madConfusing[0][1]) # diab neg rec score\n        dnfs = 2 * (dnps * dnrs) \/ (dnps + dnrs) # diab neg f1 score\n\n        # Save everything\n        dic['ModelName'].append(modelName)\n        dic['AccuracyScore'].append(acc_score)\n        dic['CorrectPredictionsCount'].append(noOfCorrect)\n        dic['Total'].append(total)\n        dic['PosPrecScore'].append(dpps)\n        dic['PosRecScore'].append(dprs)\n        dic['PosFScore'].append(dpfs)\n        dic['NegPrecScore'].append(dnps)\n        dic['NegRecScore'].append(dnrs)\n        dic['NegFScore'].append(dnfs)\n        dic['TNPercentage'].append(madConfusing[0][0]\/total*100)\n        dic['TPPercentage'].append(madConfusing[1][1]\/total*100)\n        dic['FNPercentage'].append(madConfusing[1][0]\/total*100)\n        dic['FPPercentage'].append(madConfusing[0][1]\/total*100)\n            \n    return pd.DataFrame.from_dict(dic)\n\n\ndef magicWithoutIterImp(adf, testSize = 0.2):\n    \"\"\"\n    Description\n    ----\n    Splits and scales a single given dataframe using\n    `StandardScaler()`. The scaled features are then\n    inputted into `acceptingModelsNoIterImp` and out\n    comes a dataframe.\n    \n    Parameters\n    ----\n    adf (dataframe):\n        The dataframe to use for modeling.\n    \n    testSize (float):\n        The test_size that you want to give for \n        train_test_split.\n        The default test_size is set to 0.2.\n    \n    Returns\n    ----\n    save (dataframe):\n        The dataframe after running the splits in\n        `acceptingModelsNoIterImp`.\n        \n    \"\"\"\n    # Define X\n    X = adf.drop('Outcome', axis=1)\n    # Define y\n    y = adf['Outcome']\n\n    # Split\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = testSize, random_state = 42, stratify = y)\n\n    # Scale\n    sclr = StandardScaler()\n    scaled_train_features = sclr.fit_transform(X_train) \n    scaled_test_features = sclr.transform(X_test) \n\n    # Save\n    save = acceptingModelsNoIterImp(scaled_train_features, scaled_test_features, y_train, y_test, classifications_list)\n    \n    return save\n\n\ndef finalResults(adf, saves):\n    \"\"\"\n    Description\n    ----\n    Prints the overall average of the method\/technique's \n    `AccuracyScore` column.\n    Displays the information of the best performed model\n    for the current method.\n    Saves data into a dictionary only if the name of the\n    dataframe `adf` does not exist in saves['MethodName'],\n    and finally displays whatever data is in `saves`.\n    \n    Parameters\n    ----\n    adf (dataframe):\n        The dataframe of the results of a method\/technique.\n        \n    saves (dict):\n        The dictionary containing all final results.\n        \n    Returns\n    ----\n    Nothing.\n        \n    \"\"\"\n    \n    print(f\"Overall Accuracy Score: {adf['AccuracyScore'].mean()}\") # Overall average of method\/technique's `AccuracyScore` column\n    print('Current Method\\'s Top Model:')\n    display(adf.iloc[[adf['AccuracyScore'].idxmax()]]) \n    \n    if adf.name not in saves['MethodName']:\n        issaSeries = adf.iloc[adf['AccuracyScore'].idxmax()]\n        saves['MethodName'].append(adf.name) # Dataframe name (Method\/Technique name)\n        saves['MethodAccuracy'].append(adf['AccuracyScore'].mean()) # Overall average of method\/technique's `AccuracyScore` column\n        saves['TopModelName'].append(issaSeries['ModelName'])  # Best performed model name\n        if 'Estimator' in adf.columns:\n            saves['Estimator'].append(issaSeries['Estimator']) # Estimator used for iterative imputer if applicable\n            saves['Order'].append(issaSeries['Order']) # Order used for iterative imputer if applicable\n        else:\n            saves['Estimator'].append('None')\n            saves['Order'].append('None')\n        saves['ModelAccuracy'].append(issaSeries['AccuracyScore']) # Accuracy score of the best performed model\n        saves['CorrectPredictionsCount'].append(issaSeries['CorrectPredictionsCount']) # Number of correct predictions\n        saves['Total'].append(issaSeries['Total']) # Size of test set\/total number of patients in test set\n    display(saves)\n\n    \nestimatorList = [\n    BayesianRidge(),\n    DecisionTreeRegressor(max_features='sqrt', random_state=42),\n    ExtraTreesRegressor(n_estimators=10, random_state=42),\n    RandomForestRegressor(criterion='mse', n_estimators=10, random_state=42),\n    KNeighborsRegressor(n_neighbors=15)\n]\n\nimputation_styles = ['ascending', 'descending', 'roman', 'arabic', 'random']\n\nclassifications_list = [\n    LinearSVC(C= 5.0, class_weight=\"balanced\"), SVC(kernel='rbf'), GaussianNB(), \n    KNeighborsClassifier(n_neighbors=7), DecisionTreeClassifier(), RandomForestClassifier(),\n    ExtraTreesClassifier(), AdaBoostClassifier(), GradientBoostingClassifier()\n]\n\ncolsToFix = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']","500f35ad":"We're still working with 532 rows, only difference is that now we don't have to worry about imputing anything.","718a3c4c":"### 6.4 Iterative Imputation After Removing Columns With Majority Data Missing<a id=\"6.4\"><\/a>\nAnother question I had was, what if we do iterative imputation after removing columns with majority data missing?<br>\nIf the column(s) has more than 5% of the data missing, let's see our results after only dropping single columns separately, and then finally dropping all columns with more than 5% data missing together. For the remaining missing data, we'll use Iterative Imputation.\n\n<i>NOTE:<\/i> `ndf` <i>was defined earlier on in this notebook, which is the dataset containing 0's replaced with<\/i> `NaN`.","b34ee79b":"#### 5.3.1 FUNCTION: imputeEm<a id=\"5.3.1\"><\/a>\nI'll define a function called `imputeEm` whose main purpose is to iteratively impute missing values based on a given estimator list and imputation style list.<br>\n\n\nEXPAND THE CELL BELOW TO VIEW FUNCTION AND DESCRIPTION","68401091":"# 5. Testing Other Model Sets<a id=\"5\"><\/a>\nRemember in chapter 4 when I said that modeling is the boring part? Well I kinda lied, I actually like modeling. I may not be the best at it, but I know I'm getting better. You know what they say, the older the wine the finer it gets, you know what I'm sayin'?\n\nIn this chapter, we will explore the different model sets and their results.<br>\nThe number of sets you'll have in the end is entirely up to you. For me, I'll be working with the 5 estimators, the 5 imputation order styles, and the 9 classifiers of which I spoke about in earlier chapters.\n\n<u>Estimators:<\/u>\n- Bayesian Ridge\n- KNeighbors Regressor\n- Decision Tree Regressor\n- Random Forest Regressor\n- Extra Trees Regressor\n\n<u>Imputation Orders:<\/u>\n- Ascending - From features with fewest missing values to most\n- Descending - From features with most missing values to fewest\n- Roman - Left to right\n- Arabic - Right to left\n- Random - A random order for each round\n\n<u>Models:<\/u>\n- LinearSVC\n- SVC with `kernel='rbf'`\n- GaussianNB\n- KNeighbors Classifier\n- Decision Tree Classifier\n- Random Forest Classifier\n- Extra Trees Classifier\n- Ada Boost Classifier\n- Gradient Boosting Classifier\n\nThis means that in the end, I will have 5x5 = 25 different iterative imputed datasets, and 25x9 = 225 different model-estimator-order sets, which means 225 accuracy scores.\n\nAlso, in this chapter, I'm going to be throwing a few functions at you, but I have faith that you'll be able to catch what's going on in the functions because it's simply what I did in the previous chapter, but in separate functions. Don't worry, I'll explain in short what my functions do, and the best part is, you can reuse these functions to test out your own model pairs.","264f4960":"<b>6.1.3.2 Saving our Results from the Previous Chapter<\/b><a id=\"6.1.3.2\"><\/a>\n    \nLet's save our results from the previous chapter using our new function that we've just created. <br>\nFirst, let's create a dataframe for our function, I'll call it `nextTopMethod`.<br>\nWe then have to give a name to our `results` dataframe. To keep things simple, I'll just give the same names that I've declared them with.","f60e8a42":"<b>5.3.3.2 Your Number Cannot be Completed as Dialed<\/b><a id=\"5.3.3.2\"><\/a><br>\nLet's check our numbers and dial again.","2e141c09":"### 5.3 Step 3: Iterative Imputation<a id=\"5.3\"><\/a>\nNow we handle missing data before we try anything else.","1538e638":"### 5.6 Step 6: Testing Several Different Models<a id=\"5.6\"><\/a>\nAnd finally, last, but not least, let's test several different models with our scaled lists.","e9c383fa":"Let's take a step back to understand what this hist plot is telling us.<br>\n1. A lot of patients had no pregnancy\n2. Most of the patients are younger\n3. We're dealing with skewness\n4. A LOT of data may be missing\n\nAlthough some of these information may not matter to our goal, it's best to write them down somewhere just in case you might find use for it later\/elsewhere.<br>\nAs I've mentioned earlier, I knew that having a BMI of 0 is likely impossible, but `BMI` is likely not even our main concern here, take a look at how many patients are missing `Insulin` and `SkinThickness` data. We'll certainly have to do something about this before we train our models with this data. Find out more on the [next chapter](#3), where things get interesting.","1c5075db":"#### 5.4.2 Producing Splits<a id=\"5.4.2\"><\/a>\n<i>A reason to continue on.<\/i>","79b983cc":"#### 6.1.2 FUNCTION: magicWithoutIterImp<a id=\"6.1.2\"><\/a>\nThis function is essentially an \"all in one\" package for dataframes that didn't go through an iterative imputation process. This function splits, scales and runs the scaled features set and output set on `acceptingModelsNoIterImp`.<br>\nWe don't have a function for those with iterative imputation due to the manual work that must be done when finding invalid imputations using `invalidNumberChecker`.\n<br><br>\n\nEXPAND THE CELL BELOW TO VIEW FUNCTION AND DESCRIPTION","5bf0c9d2":"Looks like the only column that will be getting a <b>mean<\/b> imputation is `BloodPressure`, `Glucose` barely made the cut.","4f8117a6":"#### 6.1.3 Saving our Results<a id=\"6.1.3\"><\/a>\nLet's save our results in some fashion so that we can compare our final results in the end.","f96dd124":"From our results, we can see that <i>although<\/i> `LinearSVC` <i>had a lower accuracy score than other models,<\/i> <b>on an average<\/b>, it performed the best when predicting which patients actually had diabetes (best positive recall score).<br>\n`SVC` (with `kernel = 'rbf'`), <b>on an average<\/b>, performed the best when predicting which patients actually did NOT have diabetes (best negative recall score).\n\nA side note, if I was a model, and all I did was mark every patient as diabetic positive, even if they didn't have it, I'd have the best positive recall score, but my accuracy score would be trash.","5144e33a":"# 6. Curiosity Didn't Kill the Cat<a id=\"6\"><\/a>\nIt just made the cat more curious.<br>\nFrom our results, Model: `KNeighborsClassifier` with Estimator: `ExtraTreesRegressor` in an `ascending` order performed the best with an accuracy score of 0.798701.\n\nA few questions I had in mind:\n   - How would our results have been if we only imputed with the mean\/median?\n   - How would our results have been if we didn't impute anything for the 0's at all?\n   - What if we dropped a whole column which contains too many missing values (`Insulin` or `SkinThickness`)?\n   - How would our results have been if we removed rows with certain rows with missing data?","dd98d70d":"#### 6.4.1 Results After Dropping Insulin Only<a id=\"6.4.1\"><\/a>\nFirst, let's create a copy of `ndf` and then drop the `Insulin` column.<br>\nSince we drop `Insulin`, it's a column we no longer have to fix, so we will also update our`colsToFix` list.<br>\nThen we impute missing data using our iterative imputation function, `imputeEm`.<br>\nWe then have to check for invalid imputations, and manually fix them if we have any occurences.\n\nAfter we handle any invalid imputations, we then split, scale and model our data, and finally save our results.","838f647c":"#### 4.4.1 A Modelling Example<a id=\"4.4.1\"><\/a>\nIn this section, I will use Extra Trees Classifer as an example. In [section 5.6](#5.6), you'll be able to see all the models in action.<br>\nFirst comes the model definition, then fitting on the scaled training set for `X` and the training set `y`, and then predicting on the test set.","e1df8a0a":"#### 3.2.1 Mean\/Median Imputation<a id=\"3.2.1\"><\/a>\nIn this subsection, let's see how the stats change when we impute the mean\/median to the missing values.<br>\nFirst I'm going to create a copy of `ndf`, the dataframe with null values, and call it `dfMeanMed`.","179eb184":"<b>6.4.3.3 Split, Scale and Model<\/b><a id=\"6.4.3.3\"><\/a>","fea5b698":"The dataframe didn't fail to output the first five records, nothing fishy going on yet. Let's see if we can catch the dataframe lacking by calling `.info()` to search for null values.","be1cca82":"<b>4.4.2.2 Confusion Matrix <\/b><a id=\"4.4.2.2\"><\/a>\n\nThe confusion matrix may confuse many but won't confuse you after reading this section.","d1179501":"We can see that `dropRepInsIter` performed the best on an average overall, and Model: `GradientBoostingClassifier` with Estimator: `ExtraTreesRegressor` imputing in an `ascending` order performed the best overall (with the `dropRepInsIter` method) with an accuracy score of 85.98%.\n\nNotice that `dropRepInsIter`, one of our hybrid method\/technique only had to make a total of 107 predictions.<br>\n`noThickSkin` performed the best on an average when having to make 154 predictions, with the Model: `GradientBoostingClassifier` and Estimator: `DecisionTreeRegressor` imputing in a `roman` order performing the best when having to make 154 predictions with an accuracy score of 82.47%.","0d6526de":"<b>5.3.3.1 FUNCTION: invalidNumberChecker<\/b><a id=\"5.3.3.1\"><\/a>\n\nI'll start off by creating another function. This function's main goal is to check every row and column pair (for column names in `colsToFix`) for values less than or equal to 0.<br>It will then display:<br>\n- Dataframe number: The i<sup>th<\/sup> dataframe in `data[2][i]` in which the invalid number was caught\n- Estimator: Estimator used in the dataframe number\n- Order: Imputation order used in the dataframe number\n- A set of description for each invalid number within the dataframe which includes:\n    - Index: The index of the dataframe where the invalid number lives\n    - Column: The column that contains the invalid number\n    - Value: The invalid number itself\n- A dataframe display of the rows with invalid number(s)\n\nIf your iterative imputations didn't impute any invalid numbers, this function will display nothing.\n\n\n<b>EXPAND THE CELL BELOW TO VIEW FUNCTION AND DESCRIPTION<\/b>","54615525":"<b>4.4.2.1 Accuracy Score for our Example<\/b><a id=\"4.4.2.1\"><\/a>","be3a2dd1":"### 8.2 All Functions<a id=\"8.2\"><\/a>\nExpand the cell below to see all functions.","27da2856":"Understanding what's changed:\n1. We're no longer having to deal with zeros in the columns we've substituted, as you can see in the `min` column in <b>Descriptive Stats<\/b> and distribution plots\n2. Pretty much every column except for `max` had some sort of change after `NaN` substitution\n3. Data is still skewed as expected\n4. More changes in the correlation heatmap in addition (changes from original dataset):\n    - `Glucose` and `Outcome` are slightly more correlated (change from 0.47 --> 0.49)\n    - `Insulin` and `SkinThickness` no longer have an important correlation (based on data, 0.44 --> 0.18)\n    - Increase in correlation between `BMI` and `SkinThickness` (0.39 --> 0.65)\n    - Increase in correlation between `Glucose` and `Insulin` (0.33 --> 0.58)\n    \n    \n    \nNow that we've substituted the zeros with `NaN`'s, let's see what we can do to impute missing values in the next section.","ecdedf49":"<b>6.4.3.2 Check for Invalid Imputations<\/b><a id=\"6.4.3.2\"><\/a>","b43435b1":"Simply, the design of this matrix is as follows:\n```\n[  [0][0] , [0][1]  ]\n[  [1][0] , [1][1]  ]\n```\nOR\n```\n[TN, FP]\n[FN, TP]\n```\n\nWhere,\n```\nTN = True Negative   --> [0][0]\nFP = False Positive  --> [0][1]\nFN = False Negative  --> [1][0]\nTP = True Positive   --> [1][1]\n```\n\nThe way I like to think of it is that each part of the confusion matrix (TN, FP, FN, TP) has their own index pair ([0][0], [0][1], [1][0], [1][1]) and the second index of each pair either stands for Positive or Negative, where [0] = negative and [1] = positive. If both index of a pair are the same number, then they're meant for each other, they're <b>true<\/b>, if the two numbers are different in the pair, they're not meant be, they're false.","e4794b68":"Looks like the only two columns we'll be dropping are `Insulin` and `SkinThickness`.","b5765549":"### 1.3 Always Read Directions<a id=\"1.3\"><\/a>\nI've had learned a valuable lesson back in like the 3rd or 4th grade, maybe it was 5th grade. Anyways, one of my teacher gave us an exam halfway through the class duration after going over a preparation for the exam. In the directions, it wrote something along the lines of, \"Do not take this exam, this is just a test to see if you're reading the directions of this exam, pretend that you're taking this exam and hand in your exam in the end of class without answering any questions.\"<br>\nNot surprisingly, I didn't read the directions and went ahead and started taking the exam. I always liked finishing the exam first, I remember this girl named Joette (not her actual name) who was very smart, so we would race to finish exams first (or maybe it was just me racing by myself), whatever the case may be, I always thrived to finish my exams before everyone else in elementary and middle school (K-8). <br>\nI got a zero on that exam.<br>\nMoral of the story, read and understand before doing.","1e4aafa6":"<b>4.4.2.3 Classification Report<\/b><a id=\"4.4.2.3\"><\/a>\n\nLet's now look at the precision score, recall score, and the f1-score. These measurements are used to determine the <i>quality<\/i> of the model's predictions.\n\n<u>Precision Score:<\/u><br>\nThe precision score is the calculation\/accuracy of correctly identified diabetic positives\/negatives from the total <i>model-claimed<\/i> number of positive\/negative diabetics.<br>\nA simple way to remember what the values from the confusion matrix the precision score uses to calculate it's values is by remembering that the precision score only deals with the same j<sup>th<\/sup> index per [i,j] pair, so the precision score's index pairs are:<br>\n`([0][1] and [1][1])` AND `([0][0] and [1][0])`.<br>\nOR another way to look at it is the precision score only deals with either positives or negatives, but not both:<br>\n`(FP and TP)` AND `(TN and FN)`.<br>\nThere are two precision scores, one for diabetic positive predictions and one for diabetic negative predictions.<br>\nThe precision score for diabetic positives is: `TP\/(TP + FP)`<br>\nThe precision score for diabetic negatives is: `TN\/(TN + FN)`<br>\n\n<u>Recall Score:<\/u><br>\nThe recall score is the calculation\/accuracy of correctly identified diabetic positives\/negatives from the total (<i>actual<\/i>) number of positive\/negative diabetics.<br>\nA simple way to remember what values from the confusion matrix the recall score uses to calculate it's values is by remembering that the recall score only sticks to the same array of the 2 arrays in the matrix, so the recall score's index pairs are:<br>\n`([0][0] and [0][1])` AND `([1][0] and [1][1])`. <br>\nOR<br>\n`(TN and FP)` AND `(FN and TP)`<br>\nIf the pair contains more than one of the same letters in TNFP, it ain't recall.<br>\nJust like precision score, there are also two recall scores.<br>\nThe recall score for diabetic positives is: `TP\/(TP + FN)`<br>\nThe recall score for diabetic negatives is: `TN\/(TN + FP)`\n\n<u>F1-Score:<\/u><br>\nThe F1 score is the calculation of the weighted average of the precision and recall score. There are also two f1-scores.<br>\nThe f1-score for diabetic positives and negatives is the same as follows: `2 * (precision * recall) \/ (precision + recall)`.\n\n\nWe can see the precision, recall and f1 scores all in one chart by  calling the `classifiation_report()` function.","e8c028a3":"Let's break this down into pieces:\n- Remember that our test set contained 154 records in total.\n- Of the 154 records, 100 of them were `0`, or diabetes negative.\n- Of the 154 records, 54 of them were `1` or diabetes positive.\n- The first array of the matrix is the total diabetes negative patients in our test set (82+18=100).\n- The second array of the matrix is the total diabetes positive patients in our test set (23+31=54).\n- In this 2x2 confusion matrix, 82 (the [0][0] index) is the `True Negative`.\n    - This means that the model correctly predicted 82 patients to be <b>truly<\/b> diabetes <b>negative<\/b>.\n- The index [0][1] is the second part of the first array, the value: 18. This value is the `False Positive`.\n    - This means that the model incorrectly predicted (claimed) 18 patients to have diabetes, when they actually didn't.\n    - Another way to think of it: 18 <b>false<\/b> predictions of diabetes <b>positive<\/b>.\n- The index [1][0] is the first part of the second array, the value: 23. This value is the `False Negative`.\n    - This means that the model incorrectly predicted (claimed) 23 patients to NOT have diabetes, when they actually did.\n    - Another way to think of it: 23 <b>false<\/b> predictions of diabetes <b>negative<\/b>.\n- The index [1][1] is the second part of the second array, the value: 31. This value is the `True Positive`.\n    - This means that the model correctly predicted (claimed) 31 patients to be <b>truly<\/b> diabetes <b>positive<\/b>.","c2a3a794":"***\nClick here to go to stats for the dataset with:\n1. [Nothing changed (original dataframe)](#2.3.2)\n2. [`NaN` substitution](#3.1.3.2)\n3. [Mean\/Median Imputation](#3.2.1.2)\n4. [Iterative Imputation](#3.2.2.4)\n***","39381b75":"We can see that there are no null values.<br>\nLet's explore the stats of this updated dataset in the next section.","9de1ce08":"<b>5.6.2.4 Our Worst Models<\/b><a id=\"5.6.2.4\"><\/a>\n\nNow let's see which models performed the worst.","8295f54b":"<b>3.2.2.4 Gimme Them Stats for the Dataset with Iterative Imputation<\/b><a id=\"3.2.2.4\"><\/a>\n\nI know you wanna see it","8f09a9a7":"Since we don't have to do any iterative imputation on `dfMeanMed`, we can use our function `magicWithoutIterImp`.<br>\nLet's also give a name to the results and save it to `nextTopMethod` using `finalResults`.","aecd341e":"`data` now contains a list of estimators, a list of styles, and a list of datasets with their imputed values.<br>\nLet's take a look at each of them to understand how indexing with `data` works.","5ad99738":"### 2.1 Importing Packages<a id=\"2.1\"><\/a>\n\nEXPAND CELL BELOW TO VIEW IMPORTED PACKAGES","d5e88a50":"<b>3.2.2.2 Estimators and Imputation Order<\/b><a id=\"3.2.2.2\"><\/a>\n\n<u>Estimators:<\/u><br>\nThere are 5 estimators that I will be testing out for iterative imputation (check [chapter 5](#5) for the complete imputation walkthrough):\n- Bayesian Ridge\n- KNeighbors Regressor\n- Decision Tree Regressor\n- Random Forest Regressor\n- Extra Trees Regressor\n\n<u>Imputation Order:<\/u><br>\n`IterativeImputer` also has a parameter called `imputation_order`, which determines the order of imputation, where the default is in `ascending` order.<br>\nThere are 5 different orders in which features can be imputed:\n- Ascending - From features with fewest missing values to most\n- Descending - From features with most missing values to fewest\n- Roman - Left to right\n- Arabic - Right to left\n- Random - A random order for each round","83929e3e":"#### 5.4.1 FUNCTION: produceSplits<a id=\"5.4.1\"><\/a>\nThis function will simply split dataframes into train and test sets.\n\nEXPAND THE CELL BELOW TO VIEW FUNCTION AND DESCRIPTION","48a3f744":"#### 2.3.1 Outcomes <a id=\"2.3.1\"><\/a>\nLet's start off with seeing how many of the 768 entries has a positive outcome (has diabetes).","f8add5ca":"#### 1.3.1 The Context and the Goal<a id=\"1.3.1\"><\/a>\nUnderstanding the context and goal is fairly simple for this project.<br>\n&emsp;<b>What we're dealing with:<\/b> A dataset which includes records of patients and some [diagnostic measurement](#1.3.2) information about the patients, and their outcome (whether or not they've got diabetes).<br>\n&emsp;<b>What must be done:<\/b> We have to figure out how we can use the diagnostic measurements to be able to predict whether or not a person has diabetes.<br>\nWhy is it important you may ask? Well, although we have many tools today to be able to pre-determine diabetes, this dataset and task is just to get your feet wet in the world of data science.","3a186949":"#### 3.1.1 Zeros are not the Heroes<a id=\"3.1.1\"><\/a>\nLet's take a look at how many 0's are in each column of the dataframe.","18247f91":"From the heatmap, we can identify a few things:\n1. `Glucose` has the best correlation to `Outcome` out of all other measurements\n2. Number of times pregnant somewhat increases with age\n3. There is somewhat of a correlation going on with `Insulin` and `SkinThickness`\n\nI always check for correlations when I do any sort of analysis on anything. I think that seeing how one thing correlates with another thing(s) is a great way, if not the best way to learn, especially if you're dealing with true data.<br>\n\"<i>Men lie, women lie, but numbers don't<\/i>,\" is a great quote by I don't exactly know who.","5a360bb7":"We can see that approximately 65.1466% (400\/614) of patients in the train set does not have diabetes, and approximately 64.9351% (100\/154) of patients in the test set does not have diabetes, which is very close to 65.1042%, which is the percentage of patients who did not have diabetes in the original dataset (thanks to my man `stratify`).\n\nNow let's move onto scaling our dataset.","250317a5":"Wow would you look at that, the data doesn't contain any null values. This must make our lives easier.\nWell, not so fast, life can't be this simple. Let's look at the statistical description of the dataframe by calling `.describe()`.\n\n\nNotice that calling `.info()` also returns the data type objects of the column values, memory usage and also the shape: 768 entries, 9 columns.","e3a3a3de":"### 5.2 Step 2: Making our Lists<a id=\"5.2\"><\/a>\nLet's make our lists.","0795b74b":"### 6.5 Hybrid Methods<a id=\"6.5\"><\/a>\nI'll be testing out some other methods which I call \"hybrid methods\".\nIt's sorta like what we did in the previous section, except now, instead of removing a column with the majority of data missing, I'm going to first remove all rows with any missing data from all but one column (either `Insulin` or `SkinThickness`).\n\nSo for example, first I'll choose `Insulin`. I'll remove any rows with missing data from the remaining columns, and then I'll do different types of imputations for the remaining missing data for `Insulin`.","d27ab5b5":"#### 6.5.2 What about Hybrid with `SkinThickness`?<a id=\"6.5.2\"><\/a>\nBefore we try this hybrid method with `SkinThickness`, we have to realize that we're going to be dropping A LOT of rows after dropping rows with missing data for`Insulin` (remember that we have 768 rows in original dataset). Let's take a look at what exactly we'll be left with to work on if we just remove the rows containing missing data for `Insulin`.","5cd01821":"# 8. Thank You<a id=\"8\"><\/a>\n\n### 8.1 Final Words<a id=\"8.1\"><\/a>\nThank you all for taking the time to read this notebook, I know, this was a long one. This was my very first notebook written on Kaggle, but as far as I can see, it won't be my last. I've spent many days working on this. Please don't forget to upvote if you think I deserve it. Feel free to use this notebook and implement your own estimator-order-model sets to see what you get! In [section 8.2](#8.2) I've pasted all of my functions used in this notebook into one cell.\n\nIf you notice an error in my notebook, please reach out to me or leave a comment and I'll fix it so I don't mislead anyone.<br>\nTo me, any criticism is positive criticism, that's one of the best ways I learn, from critics. If something in my notebook is wrong, please reach out to me and leave a comment to let everyone know so they don't learn wrong and as soon as I see wrong, I'll handle it all right.<br>\nAlthough this notebook can be downloaded from Kaggle, this notebook is also available on my Github, [click here to view my notebook on Github](https:\/\/github.com\/Gifari\/pidd-iteratively-imputing-missing-data).<br><br>\n\n\nBe sure to follow me on <b>[GitHub](https:\/\/github.com\/Gifari)<\/b> and here on <b>[Kaggle](https:\/\/www.kaggle.com\/gifarihoque)<\/b> where I'll be posting more notebooks and tutorials from time to time.<br>\nI've also created a <b>[Medium](https:\/\/gifari.medium.com\/)<\/b> account to post tutorials and guides, please follow me there too.<br>\nAlso, feel free to connect with me on <b>[LinkedIn](https:\/\/www.linkedin.com\/in\/gifari\/)<\/b>.<br>\n\n<b>[Click here to check out my online portfolio!](https:\/\/gifari.github.io\/)<\/b><br>\n\n\nI hope this was worth your time and thanks again for visiting!","ae34a3cd":"<b>6.4.1.1 Imputation on remaining missing data<\/b><a id=\"6.4.1.1\"><\/a>","c8bfe54b":"#### 5.5.2 I've Been Weighting For You<a id=\"5.5.2\"><\/a>\nNot all algorithms need to be scaled or normalized. Some algorithms converge faster if the data is scaled.<br>\nFor tutorial purposes, I'll standardize my datasets and show you the results.<br>\nTo learn more about feature scaling, [click here](https:\/\/towardsdatascience.com\/all-about-feature-scaling-bcc0ad75cb35) to read a <i>towards data science<\/i> article.\n\nLet's give existence to two.","f987808c":"#### 2.3.2 Correlations <a id=\"2.3.2\"><\/a>\nEven without knowing anything about diabetes, we can learn more about how one feature might correlate with another feature. A beautiful way to visualize this is by using seaborn's heatmap function.<br><br>\nHere is an output of the description again so you don't have to scroll further up.","210de7dd":"The size of all `X_train` in `X_train_list` is the same, the size of all `X_test` in `X_test_list` is the same, etc.<br>\nAlso each split used the same indices from each dataframe to reduce bias.<br>\nThe indices used in `X_train_list[2]` will be the same as the indices used in `X_train_list[7]`, as well as `X_train_list[13]`, etc.","e9768bf0":"#### 3.2.2 Iterative Imputation<a id=\"3.2.2\"><\/a>\nDepending on who you are, you will either learn something new in this section, or you won't.<br>\nIn the beginning, Iterative Imputation may be a lot to take in, but I'll assure you it isn't. I will do my best to keep things as clear as possible.<br><br>\n\n<b>Iterative Imputation<\/b> refers to a process where each feature is modeled as a function of the other features. Prior imputed values are used to predict subsequent features in a sequential manner ([ref](https:\/\/machinelearningmastery.com\/iterative-imputation-for-missing-values-in-machine-learning\/#:~:text=Iterative%20imputation%20refers,predicting%20subsequent%20features.)).<br>\n\nIterative Imputation can be done to impute missing data with the use of different estimators. The default estimator used when doing <i>scikit-learn's<\/i> `IterativeImputer` is the <i>`BayesianRidge()`<\/i> estimator.<br><br>\n<b>Note:<\/b> This estimator is still <b>experimental<\/b> for now: the predictions and the API might change without any deprecation cycle. To use it, you need to explicitly import `enable_iterative_imputer`.<br>\nTo learn more about `IterativeImputer`, click [here](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.impute.IterativeImputer.html).","682444d5":"Not surprising, yet once again, `DecisionTreeClassifier` and `GaussianNB` showed up on our results.<br>\n`DecisionTreeClassifier` on an average was the worst model when predicting diabetic positives, and `GuassianNB` was the worst model when predicting diabetic negatives.\n\nAlso notice how `SVC` is the second worst model when predicting diabetic positives (positive recall score) and `LinearSVC` is the second worst for predicting diabetic negatives (negative recall score). How can this be when in the end of [section 5.6.2.3](#5.6.2.3) we saw `LinearSVC` performed the best when predicting diabetic positives (had the best positive recall score) and `SVC` performed the best when predicting diabetic negatives (had the best negative recall score)?<br>\nOne explanation to this is that `LinearSVC` predicted positive for way too many patients (resulting in a good positive recall score) which brought it's negative recall score, and vice versa for `SVC`. <b><i>You can do everything, just not at the same time. Find your balance.<\/i><\/b>","3c2ccc00":"#### 5.3.3 Negative imputations?<a id=\"5.3.3\"><\/a>\n[Earlier](#negatives) I spoke a little about models imputing possible negative numbers.<br>\nIn this subsection, I'll go over how to find any imputations less than or equal to 0.","4f90531a":"<b>6.1.3.1 FUNCTION: finalResults<\/b><a id=\"6.1.3.1\"><\/a>\n\nNot sure if you can tell by now or not, I like functions.. like A LOT.\n\nThis function will print out and save the mean `AccuracyScore` of the whole dataframe. This can be used to compare\/contrast which method\/technique had the best results.<br>\nThis function will also print out and save the model\/set which performed the best on the current method\/technique.<br>\nThis function will only save the dataframe information to a dictionary if the name of the dataframe doesn't exist in the dictionary.\n\n\nEXPAND THE CELL BELOW TO VIEW FUNCTION AND DESCRIPTION","b96421d4":"### 2.4 Conclusions from Analysis<a id=\"2.4\"><\/a>\n1. We're working with data for 768 patients\n2. The data, pretty much like all data, consists of outliers\n3. The dataset has more patients who don't have diabetes (65.1042% of patients don't have diabetes in this dataset)\n4. Out of all other features, `Glucose` has the best correlation to `Outcome`\n5. Skewed distribution\n6. We may be dealing with missing data\n\n\n\n[back to table of contents](#toc)","07de4c9e":"Looks like `DecisionTreeClassifier` won the award for giving the worst predictions, or did it really?<br>\nLet's see who performed the worst on an average.","2ffb728f":"Let's output the first five records by calling `.head()` of the dataframe to see if everything looks good.","f867c260":"<b>6.4.1.3 Split, Scale and Model<\/b><a id=\"6.4.1.3\"><\/a>","9f22e347":"<b>5.3.3.3 Handling Invalid Numbers<\/b><a id=\"5.3.3.3\"><\/a><br>\n\nLooks like we got some results back for invalid numbers.<br>\nFrom the output, it seems  that `BayesianRidge()` is the only estimator that imputed invalid number(s), and it also looks like ALL imputation orders were caught imputing an invalid number on the SAME index number, for the SAME  column with very similar values.<br>\nLet's look more in-depth to see what we can do about this invalid number.\n\nFor one, we know that the other estimators didn't output an invalid number for the 62<sup>nd<\/sup> index for `Insulin`.<br>\nMaybe the 62<sup>nd<\/sup> index is a strange outlier of some sort?<br>\nLet's see if the mean of all 62<sup>nd<\/sup> index for each Estimator has any weird numbers.","c3de00ac":"#### 3.1.2 `NaN` Substitution<a id=\"3.1.2\"><\/a>","0ee5d2c2":"#### 5.3.2 Let's imputeEm!<a id=\"5.3.2\"><\/a>\nLet's look at our results after we run our new function.","2f2a8629":"#### 6.1.1 FUNCTION: acceptingModelsNoIterImp<a id=\"6.1.1\"><\/a>\n\n\nEXPAND THE CELL BELOW TO VIEW FUNCTION AND DESCRIPTION","db1bc535":"<b>5.6.2.1 Introducing a Classifications List<\/b><a id=\"5.6.2.1\"><\/a>\n\nNote: `classifications_list` was defined earlier in [section 5.2](#5.2).","bce33c48":"### 5.4 Step 4: Train & Test Split<a id=\"5.4\"><\/a>\nLet's now produce a train and test split for each dataframe.","84ccce5a":"### 2.3 Exploratory Data Analysis<a id=\"2.3\"><\/a>\nNow let's explore the data more indepth.<br>\nThis is where Person 1 will look further into the data by visualizing it in different means. Person 1 is doing this to be cautious of what he will have to deal with. Person 1 wants to prepare for any scenarios possible so that he doesn't end up being the fool. This process will help Person 1 understand the patterns in the data, find outliers, relations between variables, identify any errors, etc.","355754ca":"The skewness for the `Insulin` column is very high, let's replace the invalid numbers with the median.","6fc43119":"<a id=\"negatives\"><\/a>\nAlso, one thing to note, in some cases, some estimators will impute negative numbers. We don't want any negative numbers or any more zeros imputed back into the dataset.<br>\nAlthough we can check the minimum in the `min` column of the <b>Descriptive Stats<\/b>, if we were to have any negative values or zeros in the dataset, we can detect it using the `le()` function on our dataframe.","a472a5bd":"### 5.5 Step 5: Scaling<a id=\"5.5\"><\/a>\nNow let's do some scaling on our training and testing datasets.<br>\nRemember, we `fit_transform` the training set, and we `transform` the testing set.<br>\nFor debugging purposes, I'll convert the scaled results into a dataframe.\n\nI'll show you the weigh.","ac2e62eb":"### 8.3 References<a id=\"8.3\"><\/a>\n\n[A few moments later](https:\/\/www.youtube.com\/watch?v=mozhRTbRayc)<br>\n[GoodData Commuity](https:\/\/community.gooddata.com\/metrics-and-maql-kb-articles-43\/normality-testing-skewness-and-kurtosis-241#:~:text=these%20numerical%20measures.-,SKEWNESS,-In%20statistics%2C%20skewness)<br>\n[Image 1](https:\/\/i.kym-cdn.com\/photos\/images\/newsfeed\/000\/472\/188\/fd5.jpg)<br>\n[Image 2](https:\/\/memegenerator.net\/img\/instances\/67758912\/i-feel-it-doesnt-matter-how-it-makes-you-feel.jpg)<br>\n[IterativeImputer](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.impute.IterativeImputer.html)<br>\n[Machine Learning Mastery](https:\/\/machinelearningmastery.com\/iterative-imputation-for-missing-values-in-machine-learning\/#:~:text=Iterative%20imputation%20refers,predicting%20subsequent%20features.)\n","06718fb3":"<h1>Pima Indians Diabetes Database - Missing Data!? Where? <\/h1>\n<h3 style=\"color:gray\">Understanding how to detect data flaws and using different approaches to handle bad data<\/h3>\n<h4>A descriptive machine learning tutorial for beginners and an introduction to using machine learning to impute missing data<\/h4>\n\n[Gifari Hoque](https:\/\/gifari.github.io) - October 2021","bec6d6c0":"<b>3.1.3.2 Exploring Changes After `NaN` Substitution<\/b><a id=\"3.1.3.2\"><\/a>","82847e08":"<b>6.4.3.1 Imputation on remaining missing data<\/b><a id=\"6.4.3.1\"><\/a>","44c7e5a7":"Now if we run our function `invalidNumberChecker` again, we shouldn't receive any results.","b6de7f05":"<b>6.4.1.4 Saving our Results<\/b><a id=\"6.4.1.4\"><\/a>","58f230c3":"# Table of Contents<a id=\"toc\"><\/a>\n#### 1. [An Introduction](#1)\n   - 1.1 [Catching a Thief (A Story)](#1.1)\n   - 1.2 [Why Are You Here?](#1.2)\n   - 1.3 [Always Read Directions](#1.3)\n       - 1.3.1 [Context and Goal](#1.3.1)\n       - 1.3.2 [Diagnostic Measurements](#1.3.2)\n       \n#### 2. [Understanding the Data](#2)\n   - 2.1 [Importing Packages](#2.1)\n   - 2.2 [General Statistical Analysis](#2.2)\n   - 2.3 [Exploratory Data Analysis](#2.3)\n       - 2.3.1 [Outcomes](#2.3.1)\n       - 2.3.2 [Correlations](#2.3.2)\n       - 2.3.3 [Distributions](#2.3.3)\n   - 2.4 [Conclusions from Analysis](#2.4)\n   \n#### 3. [Cleaning the Data](#3)\n   - 3.1 [Finding and Replacing \"Bad\" Data with `NaN`](#3.1)\n       - 3.1.1 [Zeros are not the Heroes](#3.1.1)\n       - 3.1.2 [`NaN` Substitution](#3.1.2)\n       - 3.1.3 [Gimme Them Stats!](#3.1.3)\n           - 3.1.3.1 [<b>FUNCTION:<\/b> gimmeThemStats](#3.1.3.1)\n           - 3.1.3.2 [Exploring Changes After `NaN` Substitution](3.1.3.2)\n   - 3.2 [Handling Missing Data](#3.2)\n       - 3.2.1 [Mean\/Median Imputation](#3.2.1)\n           - 3.2.1.1 [You're Skewed!](#3.2.1.1)\n           - 3.2.1.2 [Gimme Them Stats for the Dataset with Mean\/Median Imputations](#3.2.1.2)\n       - 3.2.2 [Iterative Imputation](#3.2.2)\n           - 3.2.2.1 [Importing Packages](#3.2.2.1)\n           - 3.2.2.2 [An `IterativeImputer` Example](#3.2.2.2)\n\n#### 4. [Applying Models](#4)\n   - 4.1 [Importing Packages](#4.1)\n   - 4.2 [Splitting the Data](#4.2)\n   - 4.3 [Scaling the Data](#4.3)\n   - 4.4 [Modelling the Data](#4.4)\n       - 4.4.1 [A Modelling Example](#4.4.1)\n       - 4.4.2 [Prediction Analysis](#4.4.2)\n           - 4.4.2.1 [Accuracy Score for our Example](#4.4.2.1)\n           - 4.4.2.2 [Confusion Matrix](#4.4.2.2)\n           - 4.4.2.3 [Classification Report](#4.4.2.3)\n\n#### 5. [Testing other Pairs](#5)\n   - 5.1 [Step 1: Importing Packages](#5.1)\n   - 5.2 [Step 2: Making our Lists](#5.2)\n   - 5.3 [Step 3: Iterative Imputation](#5.3)\n       - 5.3.1 [<b>FUNCTION:<\/b> imputeEm](#5.3.1)\n       - 5.3.2 [Let's imputeEm!](#5.3.2)\n       - 5.3.3 [Negative imputations?](#5.3.3)\n           - 5.3.3.1 [<b>FUNCTION:<\/b> invalidNumberChecker](#5.3.3.1)\n           - 5.3.3.2 [Your Number Cannot be Completed as Dialed](#5.3.3.2)\n           - 5.3.3.3 [Handling Invalid Numbers](#5.3.3.3)\n   - 5.4 [Step 4: Train & Test Split](#5.4)\n       - 5.4.1 [<b>FUNCTION:<\/b> produceSplits](#5.4.1)\n       - 5.4.2 [Producing Splits](#5.4.2)\n   - 5.5 [Step 5: Scaling](#5.5)\n       - 5.5.1 [<b>FUNCTION:<\/b> weightForMe](#5.5.1)\n       - 5.5.2 [I've Been Weighting For You](#5.5.2)\n   - 5.6 [Step 6: Testing Several Different Models](#5.6)\n       - 5.6.1 [<b>FUNCTION:<\/b> acceptingModels](#5.6.1)\n       - 5.6.2 [PIDD's Next Top Model?](#5.6.2)\n           - 5.6.2.1 [Introducing a Classifications List](#5.6.2.1)\n           - 5.6.2.2 [The Results](#5.6.2.2)\n           - 5.6.2.3 [Our Top Models](#5.6.2.3)\n           - 5.6.2.4 [Our Worst Models](#5.6.2.4)\n           \n#### 6. [Curiosity Didn't Kill the Cat!](#6)\n   - 6.1 [Moar Functions](#6.1)\n       - 6.1.1 [<b>FUNCTION:<\/b> acceptingModelsNoIterImp](#6.1.1)\n       - 6.1.2 [<b>FUNCTION:<\/b> magicWithoutIterImp](#6.1.2)\n       - 6.1.3 [Saving our Results](#6.1.3)\n           - 6.1.3.1 [<b>FUNCTION:<\/b> finalResults](#6.1.3.1)\n           - 6.1.3.2 [Saving our Results from the Previous Chapter](#6.1.3.2)\n   - 6.2 [Only Mean\/Median Imputation](#6.2)\n   - 6.3 [Results if we didn't Replace Zeros](#6.3)\n   - 6.4 [Iterative Imputation After Removing Columns With Majority Data Missing](#6.4)\n       - 6.4.1 [Results After Dropping Insulin Only](#6.4.1)\n           - 6.4.1.1 [Imputation on remaining missing data](#6.4.1.1)\n           - 6.4.1.2 [Check for Invalid Imputations](#6.4.1.2)\n           - 6.4.1.3 [Split, Scale and Model](#6.4.1.3)\n           - 6.4.1.4 [Saving our Results](#6.4.1.4)\n       - 6.4.2 [Results After Dropping SkinThickness Only](#6.4.2)\n           - 6.4.2.1 [Imputation on remaining missing data](#6.4.2.1)\n           - 6.4.2.2 [Check for Invalid Imputations](#6.4.2.2)\n           - 6.4.2.3 [Split, Scale and Model](#6.4.2.3)\n           - 6.4.2.4 [Saving our Results](#6.4.2.4)\n       - 6.4.3 [Results After Dropping Insulin and SkinThickness](#6.4.3)\n           - 6.4.3.1 [Imputation on remaining missing data](#6.4.3.1)\n           - 6.4.3.2 [Check for Invalid Imputations](#6.4.3.2)\n           - 6.4.3.3 [Split, Scale and Model](#6.4.3.3)\n           - 6.4.3.4 [Saving our Results](#6.4.3.4)\n   - 6.5 [Hybrid Methods](#6.5)\n       - 6.5.1 [Hybrid with `Insulin`](#6.5.1)\n           - 6.5.1.1 [Dropping Rows](#6.5.1.1)\n           - 6.5.1.2 [Replacing Missing Data with Zeros](#6.5.1.2)\n           - 6.5.1.3 [Replacing Missing Data with the Median](#6.5.1.3)\n           - 6.5.1.4 [Replacing Missing Data with Iterative Imputation](#6.5.1.4)\n           - 6.5.1.5 [What if we Removed Insulin After the Rows Drop?](#6.5.1.5)\n       - 6.5.2 [What about Hybrid with `SkinThickness`?](#6.5.2)\n\n#### 7. [The Final Results](#7)\n\n#### 8. [A Thank You](#8)\n   - 8.1 [Final Words](#8.1)\n   - 8.2 [All Functions](#8.2)\n   - 8.3 [References](#8.3)","99be8962":"***\nClick here to go to stats for the dataset with:\n1. [Nothing changed (original dataframe)](#2.3.2)\n2. [`NaN` substitution](#3.1.3.2)\n3. [Mean\/Median Imputation](#3.2.1.2)\n4. [Iterative Imputation](#3.2.2.4)\n***","be80b81b":"### 5.1 Step 1: Importing Packages<a id=\"5.1\"><\/a>\nFirst steps first, let's import some packages that will be used for modeling.<br>\nEXPAND THE CELL BELOW TO VIEW IMPORTED PACKAGES","a8e6f1b5":"### 1.2 Why are you here?<a id=\"1.2\"><\/a>\nWhat's the purpose of your life? Have you figured it out yet? Is your purpose always changing, or are you at least updating it from time to time? Or maybe, think about what you wanted to have accomplished at your age a year ago, two years ago, five years ago, etc. Realistically speaking, your goals probably changed due to certain unpredictable factors.<br>\nFrom the time you were born to now, you've collected data. You've come a long way, don't let anyone try to fool you otherwise. You weren't born to become a data scientist\/analyst\/unicorn, but over time, you've collected data, analyzed that data, determined what else was needed to be collected after your analysis, and repeated the process, and I think that's a general reason to why you're here, everything that took place in your past somehow led you here, to reading this notebook.<br>\nWell, whatever the case may be, I assume you're here to learn more about machine learning, or handling bad data, or just because you're bored, and I think you've come to the right place. Though this is my first notebook I've ever publically written anywhere online for anything coding related, I'm confident I'll be able to keep you entertained while also being able to teach you what I've learned from this project (I know, it's shocking right? [Here's something](https:\/\/i.kym-cdn.com\/photos\/images\/newsfeed\/000\/472\/188\/fd5.jpg) to the 93% of the teachers I've had).<br><br>\nWithout wasting anymore of your time, let's get started.\n\n<b><i>I also have to point out that my results may vary with your results. Every run is a new run.<\/i><\/b>","6ab3045d":"<b>6.4.2.3 Split, Scale and Model<\/b><a id=\"6.4.2.3\"><\/a>\n\nNo more invalid numbers, let's finish this method off.","d5a6ae5f":"<b>5.6.2.3 Our Top Models<\/b><a id=\"5.6.2.3\"><\/a><br>\nLet's see which models got the best accuracy scores.","8a840774":"### 2.2 General Statistical Analysis<a id=\"2.2\"><\/a>\nLet's do some general statistical analysis to better understand what we will be having to work with.<br>\nThis is where Person 1 looks at the information from a broad perspective and slowly breaks it down piece by piece to better understand what he will have to work with.<br>\nFirst, we import the data as a dataframe.","9ad23f79":"#### 6.4.2 Results After Dropping SkinThickness Only<a id=\"6.4.2\"><\/a>\nSame as before, except now we deal with `SkinThickness`.","e8626142":"<b>6.5.1.3 Replacing Missing Data with the Median<\/b><a id=\"6.5.1.3\"><\/a>\n\nLet's see what happens if we replace the remaining missing data for `Insulin` with median instead.","e7f41a2d":"<b>3.2.2.1 Importing Packages<\/b><a id=\"3.2.2.1\"><\/a>\n\nEXPAND THE CELL BELOW TO VIEW IMPORTED PACKAGES","8b0f2c74":"***\nClick here to go to stats for the dataset with:\n1. [Nothing changed (original dataframe)](#2.3.2)\n2. [`NaN` substitution](#3.1.3.2)\n3. [Mean\/Median Imputation](#3.2.1.2)\n4. [Iterative Imputation](#3.2.2.4)\n***\n\n<br>\n\n[back to table of contents](#toc)","18fc833b":"Understanding what's changed from dataset containing nulls:\n1. All null values are now filled\n2. Slight (not significant) changes in <b>Descriptive Stats<\/b> after imputations\n3. More missing data resulted in more kurtosis in distribution plot from mean\/median imputation\n4. Slight changes in the correlation heatmap (changes from null dataset):\n    - Decrease in correlation between `BMI` and `SkinThickness` (0.65 --> 0.54)\n    - Decrease in correlation between `Glucose` and `Insulin` (0.58 --> 0.42)\n    \n    \n    \nLet's now go over Iterative Imputation.","d782716c":"### 4.4 Modelling the Data<a id=\"4.4\"><\/a>\nNow we'll move onto doing some modeling.<br>\nFor modeling, I will use 9 different classifiers:\n- LinearSVC\n- SVC with `kernel='rbf'`\n- GaussianNB\n- KNeighbors Classifier\n- Decision Tree Classifier\n- Random Forest Classifier\n- Extra Trees Classifier\n- Ada Boost Classifier\n- Gradient Boosting Classifier","4aa64115":"We can see that more missing data in a certain column had more impact in the distribution plot for that column. More specifically, the <i>kurtosis<\/i>, or peakedness changed. Let's see how much the kurtosis measurements changed from the null df (`ndf`) to the Mean\/Median df (`dfMeanMed`).","cca6ca99":"***\nClick here to go to stats for the dataset with:\n1. [Nothing changed (original dataframe)](#2.3.2)\n2. [`NaN` substitution](#3.1.3.2)\n3. [Mean\/Median Imputation](#3.2.1.2)\n4. [Iterative Imputation](#3.2.2.4)\n***","2ef9de62":"<b>6.4.2.2 Check for Invalid Imputations<\/b><a id=\"6.4.2.2\"><\/a>","2983b7e6":"Okay, now things are getting a little fishy. I may not be the brightest in the room, but how does one even get a BMI of 0 (`min` column)? Now that I'm looking more into it, I see that Pregnancies, Glucose, BloodPressure, SkinThickness, Insulin and Outcome are also 0.\nTo have a minimum value of 0 for Pregnancies makes sense, and we know that Outcome can be either 0 or 1.\nNow, let's say I have no idea if Glucose, BloodPressure, SkinThickness, Insulin or BMI could have a minimum value of 0, this is where I would do further research to find out.\nAfter doing research, I realized that it's almost impossible for these values to be correct. Let's write this down on the side [somewhere](#2.4) to come back to later.\n\nI sometimes use `.describe()` to also look for outliers from a broad view, for instance, someone was pregnant 17 times dude, that's an outlier, at least here in America.\nI won't be focusing too much on outliers in this notebook, but it's something to take notice of in datasets, and removing outliers can usually do more good than bad.<br><br>\nLet's move onto doing some EDA to learn more about our data.","cbd03b6d":"We can see that all other columns except for `Insulin` has 532 rows.","e4a3396d":"#### 6.4.3 Results After Dropping Insulin and SkinThickness<a id=\"6.4.3\"><\/a>\nLet's see what happens if we drop both.","e65d297b":"# 3. Cleaning the Dataset<a id=\"3\"><\/a>\nI'm no psycho, but welcome to the fun part, where I will be going over a few ways in which I will be handling the missing\/bad data in this dataset, but first, let's see how much of \"bad\" data we're dealing with.<br><br>\nThis is essentially where Person 1 will identify and handle\/fix errors in the data to be able to make better decisions. ","9d88447f":"We can see that the other 62<sup>nd<\/sup> index of the other estimators seem to be fine.<br>\nSince that's the case, let's not remove the the 62<sup>nd<\/sup> index of all estimators, and replace it with either the mean\/median based on the skewness. Since we're dealing with the first 5 dataframes in `data[2]`, this makes our job a little easier.<br>\nLet's look at the skewness value for each dataframe's `Insulin` column.","054b79a5":"Our confusion matrix is:\n```\narray([[82, 18],\n       [23, 31]])\n```\nLet's look at the diabetic <b>negatives<\/b> scores:\n- The precision score is: `82 \/ (82+23) \u2248 0.78`, where `82+23 = 105`, the number of patients the model marked as diabetic negative.\n    - This means that out of all the 105 patients the model marked to be diabetic negative, the model was approximately 78% precise.\n- The recall score is: `82 \/ (82+18) = 0.82`, where `82+18 = 100`, the number of total actual diabetic negative patients.\n    - This means that out of all the 100 actual diabetic negative patients, the model predicted 82% of them correctly.\n    \nLet's look at the diabetic <b>positives<\/b> scores:\n- The precision score is: `31 \/ (31+18) \u2248 0.63`, where `31+18 = 49`, the number of patients the model marked as diabetic positive.\n    - This means that out of all the 49 patients the model marked to be diabetic positive, the model was approximately 63% precise.\n- The recall score is: `31 \/ (31+23) \u2248 0.57`, where `31+23 = 54`, the number of total actual diabetic positive patients.\n    - This means that out of all the 54 actual diabetic positive patients, the model predicted approximately 57% of them correctly.\n    \n    \nSpeaking without knowing any more about diabetes, I would assume that a model correctly predicting the number of patients to have actually have diabetes is more important than a model incorrectly predicting a patient has diabetes when they actually don't. Let's explore other estimator\/model\/imputation-order sets in the next chapter to see which set(s) perform the best overall.\n\n<br>\n\n[back to table of contents](#toc)","bd32f5df":"<b>5.6.2.2 The Results<\/b><a id=\"5.6.2.2\"><\/a>","7900a13e":"#### 4.4.2 Prediction Analysis<a id=\"4.4.2\"><\/a>\nLet's analyze our model's predictions.","3e94e393":"Looks like nothing we haven't dealt with before, let's replace these values with the median and check for invalid numbers again just incase.","612f0fae":"<b>6.5.1.4 Replacing Missing Data with Iterative Imputation<\/b><a id=\"6.5.1.4\"><\/a>\n\nLet's see what happens if we replace the remaining missing data for `Insulin` with iterative imputation instead.","083d808e":"<b>6.4.2.4 Saving our Results<\/b><a id=\"6.4.2.4\"><\/a>","36ec6c0d":"<b>3.1.3.1 gimmeThemStats Function<\/b><a id=\"3.1.3.1\"><\/a>\n\nEXPAND THE CELL BELOW TO VIEW FUNCTION AND DESCRIPTION","aba01980":"Now, let's make a copy of the null dataframe (`ndf`) to run the imputer on, and`fit` on the dataset.<br>\nThe next step is to call `transform` which will impute the missing values. `transform` returns an array, so I will convert the array to a dataframe.","95638e5d":"# 2. Understanding the Dataset<a id=\"2\"><\/a>\nLet's start off by first understanding the data given to us.<br>\nIn this chapter, I'll import (un)necessary (what may be necessary for me, may or may not necessarily be necessary for you) packages, and then look into the general description about the data. I'll save the EDA for [later](#2.3).","e5abba0b":"### 4.3 Scaling the Data<a id=\"4.3\"><\/a>\n\nLet's now scale the dataset.<br>\nFirst we define the scaler, I will be using `StandardScaler()`. Then we fit and transform the training set, and transform the test set.","97724c2c":"You get the gist, the model got an accuracy score of approximately 73.3766%. Not a good accuracy score, but there are countless of options available, given the number of models, estimators and imputation orders we can use, maybe we can get a better score using a different combination. I'll go over how we can do this in [chapter 5](#5).\n","b7b43591":"<b>6.4.1.2 Check for Invalid Imputations<\/b><a id=\"6.4.1.2\"><\/a>","53de4581":"When we drop `Insulin`, we're left with a total of 394 rows with 1 row of missing data from columns `Glucose` and `BMI`.<br>\nWhat this means is that after we removed `Insulin`, we were left with NO more rows with missing data in the `SkinThickness` column.\n\nSo this means that there's no reason for us to use this hybrid method on `SkinThickness`.\n\nLet's now look at our final results.\n\n<br>\n\n[back to table of contents](#toc)","595bad3c":"### 4.2 Splitting the Data<a id=\"4.2\"><\/a>\nNow we'll split the data. First we define `X` as all columns except the `Outcome` column, and `y` as the `Outcome` column.<br>\nThen we do the split. I'm going to use `test_size` of 0.2. I will also set `stratify=y` for a more proportionate data split.","5ed06b1a":"From our results, `GradientBoostingClassifier` performed the best on an average when predicting on the test set.<br>\nImputations retrieved from `ExtraTreesRegressor` gave the most accurate predictions, and imputations in an `arabic` order gave the most accurate predictions.\n\nI wanted to see which model had the best recall scores, more specifically the best positive recall score.","c715a8c8":"#### 5.5.1 FUNCTION: weightForMe<a id=\"5.5.1\"><\/a>\nThis function will scale the training and testing sets using `StandardScaler()`.\n\nEXPAND THE CELL BELOW TO VIEW FUNCTION AND DESCRIPTION","f40a0d14":"From looking at the first 5 rows of our results, we can see that the models from our `classifications_list` is being used on each `Estimator` and `Order` pair to get an `AccuracyScore`.<br>\nIn total, the row count will be the number of each (as long as they're valid):<br>\n`\"Estimators\" x \"Imputation Orders\" x \"Models\"`<br>\nSince I've used 5 valid estimators, 5 valid imputation orders and 9 valid models, I should expect to get 225 different accuracy scores.","3032eb22":"<b>6.5.1.5 What if we Removed Insulin After the Rows Drop?<\/b><a id=\"6.5.1.5\"><\/a>\n\nWhat if we just dropped the `Insulin` column entirely after dropping missing data rows of other columns?","10fd14cd":"Because I've used 5 estimators and 5 imputation orders, I get a dataframe for each estimator and imputation order pair.<br>\nIn total, 25 different dataframes.","5d56c150":"### 6.1 Moar Functions<a id=\"6.1\"><\/a>\nEarlier, in [section 3.2.1.1](#3.2.1.1) we saw how skewed our missing columns were and decided on doing a mean imputation on `BloodPressure` and a median imputation on the rest.<br>\nIn this section, I'm going to throw in a few more functions to make our lives easier.<br>\nIn the [next section](#6.2),let's take our mean\/median imputated dataset (`dfMeanMed`) to the next level and test out our models after scaling.\n\nTo do this, we have to tweak our `acceptingModels` function just a little bit since we don't need to go through the Iterative Imputation process.","69a3ff3a":"#### 1.3.2 Diagnostic Measurements<a id=\"1.3.2\"><\/a>\n`Pregnancies` - Number of times patient has been pregnant (Note: dataset consists of all females aged 21+)\n\n`Glucose` - Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n\n`BloodPressure` - Diastolic blood pressure (mm Hg)\n\n`SkinThickness` - Triceps skin fold thickness (mm)\n\n`Insulin` - 2-Hour serum insulin (mu U\/ml)\n\n`BMI` - Body mass index (weight in kg\/(height in m)^2)\n\n`DiabetesPedigreeFunction` - Diabetes pedigree function\n\n`Age` - Age in years\n\n`Outcome` - Class variable (0 or 1) 268 of 768 are 1, the others are 0, where 0 indicates patient does not have diabetes","eac2595b":"#### 3.1.3 Gimme Them Stats!<a id=\"3.1.3\"><\/a>\nLet's now take a look at the description, correlation and distribution of our dataframe after converting zeros to `NaN`'s.<br>\nI'll be doing this a few more times, so I'll just create a function to keep the notebook a little more clean.","56476bbb":"That's right, `GaussianNB` and `DecisionTreeClassifier` equally performed the worst. <br>\nLooks like they're gonna have to figure out how to <i>split<\/i> the medal tonight.\n\nImputations using `BayesianRidge`, on an average, had the worst predictions.<br>\nImputations in a `random` order had the worst predictions.\n\nLet's look at the worst recall scores.","4189910a":"# 7. Final Results<a id=\"7\"><\/a>\nAnd now finally, we can see our results by calling `nextTopMethod`.<br>\nBelow, I'll write the name of the method and a description for each:\n   - `results` - The results for iterative imputation on ALL missing data (see [Chapter 5](#5))\n   - `meanMed` - The results for mean\/median imputation on ALL missing data (see [Section 6.2](#6.2))\n   - `keepZeros` - The results for original dataframe `df`, which contains original data before `NaN` replacement (see  [Section 6.3](#6.3))\n   - `noInsulin` - The results after dropping column `Insulin` and doing iterative imputation on the remaining missing data of remaining columns (see [Section 6.4.1](#6.4.1))\n   - `noThickSkin` - The results after dropping column `SkinThickness` and doing iterative imputation on the remaining missing data of remaining columns (see [Section 6.4.2](#6.4.2))\n   - `noInsulinThickSkin` - The results after dropping both columns `Insulin` and `SkinThickness` and then doing iterative imputation on the remaining missing data of remaining columns (see [Section 6.4.3](#6.4.3))\n   - `dropRepInsZero` - (Hybrid) The results after dropping all rows containing missing data for `Glucose`, `BloodPressure`, `SkinThickness`, and `BMI`, and then replacing the remaining missing data in column `Insulin` with zeros (see [Section 6.5.1.2](#6.5.1.2))\n   - `dropRepInsMed` - (Hybrid) The results after dropping all rows containing missing data for `Glucose`, `BloodPressure`, `SkinThickness`, and `BMI`, and then replacing the remaining missing data in column `Insulin` with the Median of the column (see [Section 6.5.1.3](#6.5.1.3))\n   - `dropRepInsIter` - (Hybrid) The results after dropping all rows containing missing data for `Glucose`, `BloodPressure`, `SkinThickness`, and `BMI`, and then replacing the remaining missing data in column `Insulin` through iterative imputation (see [Section 6.5.1.4](#6.5.1.4))\n   - `dropNullRemIns` - (Hybrid) The results after dropping all rows containing missing data for `Glucose`, `BloodPressure`, `SkinThickness`, and `BMI`, and then dropping column `Insulin`. (see [Section 6.5.1.5](#6.5.1.5))","c523c27b":"<b>3.2.1.1 You're Skewed!<\/b><a id=\"3.2.1.1\"><\/a>\n\nLet's see the skewness measurements of the columns with missing values to get a better idea of whether or not we should use the mean or median to impute missing values. If the data is skewed, we can use the median for a better representation for the missing values. If the data isn't skewed by much, we can replace the missing values with the mean for the column(s).<br>\nHow exactly can we tell how skewed the data has to be to consider it to be <i>more skewed than not<\/i>?<br>\nQuoting from [kristian.klima](https:\/\/community.gooddata.com\/metrics-and-maql-kb-articles-43\/normality-testing-skewness-and-kurtosis-241#:~:text=these%20numerical%20measures.-,SKEWNESS,-In%20statistics%2C%20skewness), an author from the [GoodData Commuity](https:\/\/community.gooddata.com\/):<br>\n\n>    As a general rule of thumb:<br>\n        - If skewness is less than -1 or greater than 1, the distribution is highly skewed.<br>\n        - If skewness is between -1 and -0.5 or between 0.5 and 1, the distribution is moderately skewed.<br>\n        - If skewness is between -0.5 and 0.5, the distribution is approximately symmetric.<br>\n  \n   \nFor our data, if skewness value is between -0.5 and 0.5, I'll use the mean to impute missing values, else, I will use median.","d2ae1473":"<b>6.5.1.1 Dropping Rows<\/b><a id=\"6.5.1.1\"><\/a>","c49b067c":"### 4.1 Importing Packages<a id=\"4.1\"><\/a>\nEXPAND THE CELL BELOW TO VIEW IMPORTED PACKAGES","04cd0bcc":"#### 6.5.1 Hybrid with `Insulin`<a id=\"6.5.1\"><\/a>","22c745f6":"Understanding what's changed from dataset containing nulls:\n1. All null values are now filled\n2. Slight (not significant) changes in <b>Descriptive Stats<\/b> after imputations\n3. Since different numbers were imputed, we don't see crazy kurtosis like how we did when we imputed the mean\/median, overall the distribution plots for this dataframe seem to have kept the original shape of the distribution plots of the dataframe containing nulls (`ndf`)\n4. Slight changes in the correlation heatmap (changes from null dataset):\n    - Decrease in correlation between `Glucose` and `Insulin` (0.58 --> 0.45)\n    - Very slight changes in correlations between other features\n    \n    \nNow that we're done with imputing missing values, let's go do some modeling (or modelling - however you wanna spell it).","7acd426e":"That's crazy.<br>\nA lot of zero values in `SkinThickness` and `Insulin`.<br>\nTo me, things like this is equivalent to taking tylenol for recurring headaches. You're just masking your pain just to <i>get by...<\/i><br>\nWithout worrying about `Pregnancies` and `Outcome`, let's convert all zeros in `Glucose`, `BloodPressure`, `SkinThickness`, `Insulin`, and `BMI` to  `NaN` values to manipulate this data.","727fcb0e":"<b>3.2.2.3 An `IterativeImputer` Example<\/b><a id=\"3.2.2.3\"><\/a>\n\nLet's go over an example on using `IterativeImputer`. The way I do it is as follows: define the imputer, then `fit` on the dataset, and finally impute all missing values in the dataset by calling `transform`.<br>\nFor this example, I'll use <i>Decision Tree Regressor<\/i> with a <i>descending<\/i> order.<br><br>\n\n<b>NOTE:<\/b> Although yes, it can be easier to use a pipeline to keep code more clean, for this tutorial, I'm going to demonstrate imputing, scaling and modeling without the use of pipeline.","b12310bb":"### 3.2 Handling Missing Data<a id=\"3.2\"><\/a>\nIn this section, I'm going to handle the missing data in 2 different ways:\n1. [Mean\/Median Imputation](#3.2.1)\n2. [Iterative Imputer Method](#3.2.2)","9eb86040":"# 1. Introduction <a id=\"1\"><\/a>\nWelcome to my notebook. I'm going to try to keep my work and explanations as beginner-friendly as possible.<br><br>\nMy main goal is to help you be able to detect bad\/missing data, different approaches to handling missing data (including iterative imputation with the use of machine learning), and finally comparing results of accuracy scores with multiple machine learning predictive models to see how exactly bad\/missing data affects the predictions the models make.<br><br>\n<center>======================================================<br>\n============= <b>NOTE:<\/b> The results may shock some of you!=============<br>\n======================================================<br><br><\/center>\nMy notebook will start off with introducing the context and content of the database, and then we'll dive right into understanding the data by doing EDA (exploratory data analysis). We will then do some data cleaning and wrangling, I call this process \"arguing with the data provided,\" where we will clean the data and transform it for better use.<br><br>\n\n### 1.1 Catching a Thief<a id=\"1.1\"><\/a>\nHere is a real life example (maybe):<br>\n&emsp;<b>Person 1:<\/b> What's up dude, you got intel for me?<br>\n&emsp;<b>Person 2:<\/b> Yeah bro, here's some information about the guy who stole my bike.<br>\n&emsp;<b>Person 1:<\/b> Alright, let me take a look.<br>\n\n[A few moments later](https:\/\/www.youtube.com\/watch?v=mozhRTbRayc)<br><br>\n&emsp;<b>Person 1:<\/b> Yo my man, how can you expect to get your bike back with this information? It's almost meaningless.<br>\n&emsp;<b>Person 2:<\/b> Whaaat?? I thought the more information I get for you the better it would be.<br>\n&emsp;<b>Person 1:<\/b> Well.. yeah, but I thought you'd at least organize this information for me.<br>\n&emsp;<b>Person 2:<\/b> I swear I thought it was organized bro, I spoke to at least five different people and I wrote down what each person said in separate pages.<br>\n&emsp;<b>Person 1:<\/b> All you did was collect data, but don't worry about it, I'll take it on from here.<br>\n<center><i>Two days later, Person 1 somehow bought back 3 bikes.<\/i><\/center><br><br>\nThat was probably a bad example, but whatever. I hope this story helped you understand that data collection does not always equal good data. Most of the time the data given to us is just bland and meaningless, until someone like Person 1 puts meaning to the data by analyzing it, and that's where the most time is usually spent, analyzing the data. Analyzing data helps you further determine what else needs to be added to the dataset. In the story, Person 2 got his bike stolen by someone, and so he reaches out to Person 1 for assistance. Person 1 tells Person 2 to bring some information about the thief and so, Person 2 does. Person 1 realizes that the data is just lists of information for people Person 2 spoke to, and so Person 1 takes matters into his own hand, figures out what to do with the information he got, perhaps even did some further \"data collection\" himself, and was able to out-thief (is that even a word?) the thief.<br>\n<i>Disclaimer: This story is made up for educational purposes only and is not intended to reflect anyone's life story, nor was it written to offend anyone.<\/i>","1d99665f":"From our results, we can see that the model `KNeighborsClassifier` with the estimator `ExtraTreesRegressor` in an `ascending` order had the best accuracy score of `0.798701` with a total of `123` correct predictions.<br>\n`AdaBoostClassifier` and `GradientBoostingClassifier` both came in second place and `ExtraTreesClassifer` came in third place.\n\n\nLet's see which model, estimator, and order had the best <b>average<\/b> accuracy score.","5c259a8c":"### 6.3 Results if we didn't Replace Zeros<a id=\"6.3\"><\/a>\nWhat if we didn't replace any zeros with `NaN` from the very beginning?<br>\nIt would be a bit interesting to compare our results in the very end.<br>\nFor this, let's create a copy of the original dataset `df` which contains the zeros.","0ac5f810":"### 3.1 Finding and Replacing \"Bad\" Data with `NaN`<a id=\"3.1\"><\/a>\nI'm going to start off by creating a copy of the original dataframe.<br>\nGet ready because things are gonna get dirty.","ae695917":"Looks good! Let's now move onto splitting out datasets into training and testing sets.","67827224":"<b>6.4.2.1 Imputation on remaining missing data<\/b><a id=\"6.4.2.1\"><\/a>","09162c61":"<b>6.4.3.4 Saving our Results<\/b><a id=\"6.4.3.4\"><\/a>","423fad9b":"#### 5.6.1 FUNCTION: acceptingModels<a id=\"5.6.1\"><\/a>\nThis function will test out different models on each train and test split sets.\n\nEXPAND THE CELL BELOW TO VIEW FUNCTION AND DESCRIPTION","eb8823f8":"<b>3.2.1.2 Gimme Them Stats for the Dataset with Mean\/Median Imputations<\/b><a id=\"3.2.1.2\"><\/a>\n\nLet's see some stats.","bc187775":"#### 2.3.3 Distributions <a id=\"2.3.3\"><\/a>\nNow let's check the distribution and count of all columns except for the `Outcome` column using seaborn's `.histplot()` function.","da33a103":"# 4. Modelling<a id=\"4\"><\/a>\nYou made it through, but now here's the boring part, modeling, or modelling, however you spell it, [it doesn't matter](https:\/\/memegenerator.net\/img\/instances\/67758912\/i-feel-it-doesnt-matter-how-it-makes-you-feel.jpg).\n\nFirst we'll start off by splitting the dataset, then we will scale the train and test splits separately to avoid data leakage, and finally we'll be doing some modeling.<br>\nI will be using the dataset with iterative imputations for this example.","8a331a5d":"#### 5.6.2 PIDD's Next Top Model?<a id=\"5.6.2\"><\/a>\nWho's going to be PIDD's Next Top Model? Let's get the results we've been waiting for.\n\nLet's first introduce a classifier list.","09346ffb":"<b>6.5.1.2 Replacing Missing Data with Zeros<\/b><a id=\"6.5.1.2\"><\/a>\n\nLet's see what happens if we replace the remaining missing data for `Insulin` with zeros.","cf17ef59":"### 6.2 Only Mean\/Median Imputation<a id=\"6.2\"><\/a>\n\n<i>NOTE:<\/i> `dfMeanMed` <i>was introduced in [section 3.2.1.1](#3.2.1.1).<\/i><br>\nIt is a dataframe with the mean\/median imputations.\nSince nothing else was done to this dataset, let's use it."}}