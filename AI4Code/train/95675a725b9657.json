{"cell_type":{"f0111349":"code","03875a81":"code","f62e2ad5":"code","f4920e32":"code","87ed6444":"code","725a4336":"code","26ad4b63":"code","8e76a4bb":"code","509fb44c":"code","b257a597":"code","883057b5":"code","b25bcd9f":"code","01e2b42a":"code","e43a9529":"code","a16bc4b8":"code","f3019c19":"code","f3bad1fb":"code","350563cd":"code","4783f21a":"code","f29e9d42":"code","8b588f7a":"code","e3c32048":"code","9b42f89f":"code","d10d199b":"code","ec28275c":"code","8cf33e38":"code","dd2fcdc5":"code","1bd07547":"code","a38b8b44":"code","b803122c":"code","5fb4e8fa":"code","0de6c54e":"code","2d54ab74":"code","2fbe76bd":"code","f27ebcca":"code","fa8faa33":"code","6f6ac9ec":"markdown","3c9730f8":"markdown","2e9adc10":"markdown","d9409921":"markdown","c1d81dc4":"markdown","0e7589f9":"markdown","10b9cf95":"markdown","0d600188":"markdown","c9378342":"markdown","94426568":"markdown","217f7372":"markdown","27d319ba":"markdown","4e96cad3":"markdown","f3478e5d":"markdown","266ca64b":"markdown","bd83a481":"markdown","e633cfda":"markdown","4d49604c":"markdown","638a791f":"markdown","6113d5eb":"markdown","f7ea06c4":"markdown"},"source":{"f0111349":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","03875a81":"import pandas as pd\n\n# Data import\nfeatures=pd.read_csv(\"\/kaggle\/input\/retaildataset\/Features data set.csv\")\nstores=pd.read_csv(\"\/kaggle\/input\/retaildataset\/stores data-set.csv\")\nsales=pd.read_csv(\"\/kaggle\/input\/retaildataset\/sales data-set.csv\")\n\n# Now we setup the data types of each column.\nfeatures[\"Store\"]=features[\"Store\"].astype(\"category\")\nfeatures[\"Date\"]=pd.to_datetime(features[\"Date\"],dayfirst=True)\nfeatures[\"IsHoliday\"]=features[\"IsHoliday\"].map({True:1,False:0}).astype(\"category\")\nfeatures[\"Week\"]=features[\"Date\"].dt.isocalendar().week\nfeatures[\"Year\"]=features[\"Date\"].dt.isocalendar().year\nfor i in range(1,6): # A total markdown column is created. It sums the value of the 5 markdown columns.\n  if i==1:\n    features[\"MarkDowns\"]=features[\"MarkDown%s\" % i]  \n  else:\n    features[\"MarkDowns\"]=features[\"MarkDowns\"]+features[\"MarkDown%s\" % i]  \nfeatsfloatcols=[\"Temperature\",\"Fuel_Price\",\"CPI\", \"Unemployment\", \"MarkDowns\"]+[\"MarkDown%s\" % i for i in range(1,6)]\nfeatures[featsfloatcols]=features[featsfloatcols].astype(\"float64\")\nfeatures[\"Weight\"]=features[\"IsHoliday\"].map({1:5,0:1}).astype(\"int64\") # We set up the weights of each row. Holiday weeks have a weight 5 times larger than the others.\n\nstores[\"Store\"]=stores[\"Store\"].astype(\"category\")\nstores[\"Type\"]=stores[\"Type\"].astype(\"category\")\n\nsales[\"Store\"]=sales[\"Store\"].astype(\"category\")\nsales[\"Dept\"]=sales[\"Dept\"].astype(\"category\")\nsales[\"IsHoliday\"]=sales[\"IsHoliday\"].map({True:1,False:0}).astype(\"category\")\nsales[\"Date\"]=pd.to_datetime(sales[\"Date\"],dayfirst=True)","f62e2ad5":"sales","f4920e32":"features","87ed6444":"stores","725a4336":"import numpy as np\nfuturesales=sales[[\"Store\",\"Dept\"]].drop_duplicates()\nfuturesales[\"Weekly_Sales\"]=np.nan\nfuturedates=features[features[\"Date\"]>sales[\"Date\"].max()][[\"Store\",\"Date\",\"IsHoliday\"]]\nfuturesales=futuresales.merge(futuredates,on=\"Store\", how=\"outer\")\nsalesV2=pd.concat([sales,futuresales])","26ad4b63":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nimport numpy as np\n\ndef isCategory(data,col):\n  '''\n  Check if a column is categorical.\n  '''\n  try:\n    return data[col].dtype==\"category\"\n  except:\n    return False\n\ndef CategCols(data):\n  '''\n  Retuns a list with the names of all categorical columns.\n  '''\n  return [col for col in data.columns if isCategory(data,col)==True]\n\ndef negFix0(oldval):\n  '''\n  max(0,x) function.\n  '''\n  if oldval<0: return 0\n  else: return oldval\n\ndef fixFutureWS(WS):\n  '''\n  This function assigns again all future sales as NaNs,\n  since when a sum groupby is applied, NaNs become 0.\n  '''\n  if WS==0: return np.nan\n  else: return WS\n\ndef featuresMod(markdowns, CPI_Unemp):\n  '''\n  Modifies the features dataframe in case we don't want to work with\n  columns MarkDown and CPI and Unemployment\n  '''\n  moddedFeats=features.copy()\n  for i in range(1,6):\n    if markdowns!=\"all\":\n      moddedFeats=moddedFeats.drop(\"MarkDown%s\" % i, axis=1)\n  if markdowns==False:\n    moddedFeats=moddedFeats.drop(\"MarkDowns\", axis=1)\n  if CPI_Unemp==False:\n    moddedFeats=moddedFeats.drop([\"CPI\", \"Unemployment\"], axis=1)\n  return moddedFeats.dropna()\n\ndef salesMod(negWS):\n  '''\n  Modifies the sales dataframe. Negative Weekly_Sales values can be set to 0, dropped\n  or remain untouched. In our case, we remain them untouched.\n  '''\n  moddedSales=salesV2.copy()\n  if negWS==\"Keep\":\n    pass\n  elif negWS==\"Set0\":\n    moddedSales[\"Weekly_Sales\"]=moddedSales[\"Weekly_Sales\"].apply(lambda x: negFix0(x))\n  elif negWS==\"Drop\":\n    moddedSales=moddedSales.query(\"Weekly_Sales >= 0 | Weekly_Sales != Weekly_Sales\")\n  return moddedSales\n\ndef dataMerge(feats, sales, Detail):\n  '''\n  Merges the 3 dataframes and it groups the data by store if needed.\n  In our case, the data detail has to be department-wide.\n  '''\n  if Detail==\"Store\":\n    GBSsales=sales.groupby(by=[\"Date\",\"IsHoliday\",\"Store\"]).sum().reset_index()\n    GBSsales[\"Weekly_Sales\"]=GBSsales[\"Weekly_Sales\"].apply(lambda x: fixFutureWS(x))\n    mergedData=GBSsales.merge(stores, how=\"inner\", on=\"Store\")\n  elif Detail==\"Dept\":\n    mergedData=sales.merge(stores, how=\"right\", on=\"Store\")\n  mergedData=mergedData.merge(feats, how=\"right\", on=[\"Store\",\"Date\",\"IsHoliday\"])\n  return mergedData\n\ndef dataCook(data):\n  '''\n  Generates the One-Hit-Encodings of categorical variables.\n  '''\n  cookedData=data.copy()\n  cookedData=cookedData.drop(\"Year\", axis=1)\n  for col in CategCols(cookedData):\n    OHEdata=pd.get_dummies(cookedData[col], prefix=col)\n    cookedData=cookedData.join(OHEdata)\n    cookedData=cookedData.drop(col, axis=1)\n  return cookedData\n\ndef dArray(data):\n  '''\n  Transforms the dataframes to a dictionary with numpy arrays.\n  '''\n  op={}\n  op[\"X\"]=data.drop([\"Weekly_Sales\",\"Weight\",\"Date\"],axis=1).values\n  op[\"y\"]=data[\"Weekly_Sales\"].values.reshape(-1,1)\n  op[\"w\"]=data[\"Weight\"].values.reshape(-1,1)\n  op[\"d\"]=data[\"Date\"].values.reshape(-1,1)\n  return op\n\ndef dataServe(data, testSize, splitDate, futureTest):\n  '''\n  Train-Test data split. It can do it in 3 ways::\n  - Random split of the dataset with sales values. The test size fraction is given by 'testSize'.\n  - Data split by 'splitDate'. Only the train set is shuffled.\n  - Apply past-future split if 'futureTest=True'.\n  '''\n  X_cols=data.drop([\"Weekly_Sales\",\"Weight\",\"Date\"],axis=1).columns\n  y_col=\"Weekly_Sales\"\n  w_col=\"Weight\"\n  d_col=\"Date\"\n  X_shape=(data.drop([\"Weekly_Sales\",\"Weight\",\"Date\"],axis=1).shape[1],)\n\n  if futureTest==True:\n    d0=dArray( data.query(\"Weekly_Sales == Weekly_Sales\").sample(frac=1, random_state=29) )\n    d1=dArray( data.query(\"Weekly_Sales != Weekly_Sales\") )\n    X_train, y_train, w_train, d_train  = d0[\"X\"], d0[\"y\"], d0[\"w\"], d0[\"d\"]\n    X_test, y_test, w_test, d_test = d1[\"X\"], d1[\"y\"], d1[\"w\"], d1[\"d\"]\n  elif splitDate!=None:\n    dHist=data.query(\"Weekly_Sales == Weekly_Sales\")\n    splitdate=pd.to_datetime(splitDate) \n    d0=dArray( dHist[dHist[\"Date\"] <= splitdate].sample(frac=1, random_state=29) )\n    d1=dArray( dHist[dHist[\"Date\"] > splitdate] )\n    X_train, y_train, w_train, d_train  = d0[\"X\"], d0[\"y\"], d0[\"w\"], d0[\"d\"]\n    X_test, y_test, w_test, d_test = d1[\"X\"], d1[\"y\"], d1[\"w\"], d1[\"d\"]\n  else:\n    d=dArray( data.query(\"Weekly_Sales == Weekly_Sales\") )\n    X_train, X_test , y_train, y_test , w_train, w_test, d_train, d_test = train_test_split(d[\"X\"], d[\"y\"], d[\"w\"], d[\"d\"], test_size=testSize, random_state=29)\n\n  XScaler = StandardScaler()\n  yScaler = StandardScaler()\n\n  XScaler.fit(X_train)\n  yScaler.fit(y_train)\n\n  X_train=XScaler.transform(X_train)\n  X_test=XScaler.transform(X_test)\n\n  y_train=yScaler.transform(y_train)\n  y_test=yScaler.transform(y_test)\n  \n  op={\"X\":\n      {\"scaler\":XScaler,\n       \"train\":X_train,\n       \"test\":X_test, \n       \"shape\":X_shape,\n       \"cols\":X_cols\n      },\n      \"y\":\n      {\"scaler\":yScaler,\n       \"train\":y_train,\n       \"test\":y_test,\n       \"col\":y_col\n      },\n      \"w\":\n      {\"train\":w_train,\n       \"test\":w_test,\n       \"col\":w_col    \n      },\n      \"d\":\n      {\"train\":d_train,\n       \"test\":d_test,\n       \"col\":d_col    \n      }\n      }\n  return op\n\ndef dataSetup(markdowns, CPI_Unemp, detailLvl=\"Dept\", DnegWS=\"Keep\", PipelineLvl=\"Serve\", testSize=0.2, splitDate=None, futureTest=False):\n  '''\n  Quick data setup. It can deturn the data in each processing stage.\n  To make plots we use data in 'Merge' stage, since we can use the infor of the 3 dataframes at the same time.\n  To generate the input and output for machine learning, we want to use the data in 'Serve' stage.\n  '''\n  mfeats=featuresMod(markdowns, CPI_Unemp)\n  if detailLvl==\"Store\":\n    msales=salesMod(\"Keep\")\n  else:\n    msales=salesMod(DnegWS)\n  data=dataMerge(mfeats, msales, detailLvl)\n  if PipelineLvl==\"Merge\":\n    return data\n  data=dataCook(data)\n  if PipelineLvl==\"Cook\":\n    return data\n  data=dataServe(data,testSize,splitDate,futureTest)\n  if PipelineLvl==\"Serve\":  \n    return data","8e76a4bb":"import seaborn as sns\n\nMDplotData=dataSetup(markdowns=\"all\", CPI_Unemp=False, detailLvl=\"Store\", PipelineLvl=\"Merge\")\n\nfor i in range(1,6):\n  sns.relplot(data=MDplotData, x=\"MarkDown%s\" % i,y=\"Weekly_Sales\",hue=\"Type\",col=\"IsHoliday\") \nsns.relplot(data=MDplotData, x=\"MarkDowns\",y=\"Weekly_Sales\",hue=\"Type\",col=\"IsHoliday\")","509fb44c":"from tensorflow import keras\nfrom tensorflow.keras import layers\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef prettyDate(date):\n  '''\n  Transforms datetime values to string with format 'dd\/mm\/aaaa'.\n  '''\n  date=pd.to_datetime(date)\n  day=date.day\n  month=date.month\n  year=date.year\n  return \"%02d\/%02d\/%d\" % (day,month,year)\n\nclass dataModel():\n  def __init__(self, markdowns=True, CPI_Unemp=True, detailLvl=\"Dept\", DnegWS=\"Keep\", testSize=0.2, splitDate=None, futureTest=False, validation=True, verbose=\"auto\"):\n    '''\n    Class initializer. Its parameters are::\n    - markdowns: If the network input contains the total markdown variable.\n    - CPI_Unemp: If the network input contains the CP and Unemployment variables.\n    - detailLvl: If the input data is grouped by store or not. In our case, we don't group the data.\n    - DnegWS: If we keep the negative Weekly_Sales or not. In our case, we keep them.\n    - testSize: The test fraction of the dataset the random split case.\n    - splitDate: The splitting date in case we use a date split.\n    - futureTest: If we use a past-future split.\n    - validation: If we use a validation set. In that case, we take the 10% of the train set as the validation set.\n    - verbose: The network communication with the user in the training and testing processes (see Keras API).\n    '''\n    data=dataSetup(markdowns, CPI_Unemp, detailLvl, DnegWS, \"Serve\", testSize, splitDate, futureTest)\n    self.markdowns=markdowns\n    self.CPI_Unemp=CPI_Unemp\n    self.splitDate=splitDate\n    self.futureTest=futureTest\n\n    self.detailLvl=detailLvl\n    self.validation=validation\n    self.verbose=verbose\n\n    self.X_cols=data[\"X\"][\"cols\"]\n    self.y_col=data[\"y\"][\"col\"]\n    self.w_col=data[\"w\"][\"col\"]\n    self.d_col=data[\"d\"][\"col\"]\n    self.inputShape=data[\"X\"][\"shape\"]\n\n    self.X_train=data[\"X\"][\"train\"]\n    self.y_train=data[\"y\"][\"train\"]\n    self.w_train=data[\"w\"][\"train\"]\n    self.d_train=data[\"d\"][\"train\"]\n\n    self.X_test=data[\"X\"][\"test\"]\n    self.y_test=data[\"y\"][\"test\"]\n    self.w_test=data[\"w\"][\"test\"]\n    self.d_test=data[\"d\"][\"test\"]\n\n    self.X_scaler=data[\"X\"][\"scaler\"]\n    self.y_scaler=data[\"y\"][\"scaler\"]\n\n    self.model=keras.Sequential(\n      [\n      keras.Input(shape=self.inputShape),\n      layers.Dense(name=\"Hidden1\",units=80,activation=\"sigmoid\"),\n      layers.Dense(name=\"Hidden2\",units=40,activation=\"sigmoid\"),\n      layers.Dense(name=\"Hidden3\",units=20,activation=\"selu\"), \n      layers.Dense(name=\"Output\",units=1,activation=None) \n      ]\n      )\n    self.model.compile(optimizer=\"adam\",loss=\"mean_squared_error\",metrics=[\"mean_squared_error\"])\n\n    self.y_train_pred=None\n    self.y_test_pred=None\n  \n  def fit(self, epochs=15):\n    '''\n    Trains the network with a given number of epochs.\n    '''\n    if self.validation==True: val=0.1\n    else: val=0\n    self.fitHistory=self.model.fit(\n      x=self.X_train,\n      y=self.y_train,\n      batch_size=50,\n      validation_split=val,\n      sample_weight=self.w_train,\n      epochs=epochs,\n      verbose=self.verbose\n      )\n    self.y_train_pred=self.model.predict(self.X_train)\n\n  def predict(self):\n    '''\n    Executes the test process.\n    '''\n    if self.futureTest==False and self.verbose!=0: self.model.evaluate(self.X_test,self.y_test,verbose=self.verbose)\n    self.y_test_pred=self.model.predict(self.X_test)\n\n  def evaluate(self):\n    '''\n    Returns the graphical comparison of predicted vs real sales of both train and test sets (only\n    train set if past-future split is applied or we do an overfitting test).\n    '''\n    plots=[]\n    if type(self.y_train_pred)!=type(None):\n      plots.append(\"Training set\")\n      yTrain=self.y_scaler.inverse_transform(self.y_train)\n      yTrainPred=self.y_scaler.inverse_transform(self.y_train_pred)\n    if type(self.y_test_pred)!=type(None) and self.futureTest==False:\n      plots.append(\"Testing set\")\n      yTest=self.y_scaler.inverse_transform(self.y_test)\n      yTestPred=self.y_scaler.inverse_transform(self.y_test_pred)\n\n    fig,ax=plt.subplots(ncols=len(plots), sharex=True, sharey=True,figsize=(5*len(plots),5),squeeze=False )\n    pos=0\n    for plot_ in plots:\n      if plot_==\"Training set\":\n        x=yTrain\n        y=yTrainPred\n        date0=prettyDate(self.d_train.min())\n        date1=prettyDate(self.d_train.max())\n        daterange= \"%s - %s\" % (date0,date1)\n      if plot_==\"Testing set\" and yTest.sum()==yTest.sum():\n        x=yTest\n        y=yTestPred\n        date0=prettyDate(self.d_test.min())\n        date1=prettyDate(self.d_test.max())\n        daterange= \"%s - %s\" % (date0,date1)\n      ax[0,pos].scatter(x,y,s=1)\n      ax[0,pos].set_xlabel(\"Real\")\n      ax[0,pos].set_ylabel(\"Pred\")\n      ax[0,pos].set_title(plot_ + \". %s\" % daterange)\n      ax[0,pos].plot(x,x, color=\"black\")\n      ax[0,pos].tick_params(axis='x', rotation=45)\n      pos+=1\n    title=\"Markdowns = %s, \" % self.markdowns\n    title+=\"CPI & Unemployment = %s \" % self.CPI_Unemp\n    fig.suptitle(title)\n    plt.close(fig)\n    return fig\n\n  def data(self, dset=\"both\"):\n    '''\n    Returns train and test data with their predicted output.\n    '''\n    dCol=self.d_col\n    xCols=self.X_cols\n    yCol=self.y_col\n    storecols=[col for col in xCols if \"Store_\" in col]\n    deptcols=[col for col in xCols if \"Dept_\" in col]\n\n    dTrain=self.d_train\n    xTrain=self.X_scaler.inverse_transform(self.X_train)\n    yTrain=self.y_scaler.inverse_transform(self.y_train)\n\n    dTest=self.d_test\n    xTest=self.X_scaler.inverse_transform(self.X_test)\n    yTest=self.y_scaler.inverse_transform(self.y_test)\n\n    data={}\n\n    if dset in (\"train\",\"both\"):\n      dataTrain=pd.DataFrame()\n      dataTrain[dCol]=dTrain.ravel()\n      dataTrain[xCols]=xTrain\n      dataTrain[yCol]=yTrain.ravel()\n\n      dataTrain[\"Store\"]=dataTrain[storecols].idxmax(axis=1).apply(lambda x: x.split(\"_\")[1])\n      dataTrain=dataTrain.drop(columns=storecols)\n      if self.detailLvl==\"Dept\":\n        dataTrain[\"Dept\"]=dataTrain[deptcols].idxmax(axis=1).apply(lambda x: x.split(\"_\")[1])\n        dataTrain=dataTrain.drop(columns=deptcols)\n\n      if type(self.y_train_pred)!=type(None):\n        yPredTrain=self.y_scaler.inverse_transform(self.y_train_pred)\n        dataTrain[yCol+\"_Pred\"]=yPredTrain.ravel()\n        dataTrain[yCol+\"_Pred\"]=dataTrain[yCol+\"_Pred\"].astype(\"float64\")\n      data[\"train\"]=dataTrain\n    if dset in (\"test\",\"both\"):\n      dataTest=pd.DataFrame()\n      dataTest[dCol]=dTest.ravel()\n      dataTest[xCols]=xTest\n      dataTest[yCol]=yTest.ravel()\n\n      #Es desf\u00e0 el one-hit encoding per tornar a tenir les variables categ\u00f2riques en una sola columna.\n      dataTest[\"Store\"]=dataTest[storecols].idxmax(axis=1).apply(lambda x: x.split(\"_\")[1])\n      dataTest[\"Store\"]=dataTest[\"Store\"].apply(lambda x: int(x)).astype(\"category\") \n      dataTest=dataTest.drop(columns=storecols)\n      if self.detailLvl==\"Dept\": \n        dataTest[\"Dept\"]=dataTest[deptcols].idxmax(axis=1).apply(lambda x: x.split(\"_\")[1])\n        dataTest[\"Dept\"]=dataTest[\"Dept\"].apply(lambda x: int(x)).astype(\"category\")\n        dataTest=dataTest.drop(columns=deptcols)\n\n      if type(self.y_test_pred)!=type(None):\n        yPredTest=self.y_scaler.inverse_transform(self.y_test_pred)\n        dataTest[yCol+\"_Pred\"]=yPredTest.ravel()\n        dataTest[yCol+\"_Pred\"]=dataTest[yCol+\"_Pred\"].astype(\"float64\")\n      data[\"test\"]=dataTest\n      if dset!=\"both\":\n        return data[dset]\n      else:\n        return data\n\n  def getTrainScore(self):\n    '''\n    Returns the overfitting test results.\n    '''\n    train=np.array(self.fitHistory.history['mean_squared_error'])\n    epochs=[i+1 for i in range(len(train))]\n    x_epochs=[1]+[i for i in range(5,len(train)+1,5)]\n    fig=plt.figure()\n    ax = fig.add_subplot()\n\n    ax.plot(epochs,np.sqrt(train),label=\"Training set\")\n    try:\n      val=np.array(self.fitHistory.history['val_mean_squared_error'])\n      ax.plot(epochs,np.sqrt(val),label=\"Validation set\")\n    except:\n      pass\n\n    date0=prettyDate(self.d_train.min())\n    date1=prettyDate(self.d_train.max())\n    daterange= \"%s - %s\" % (date0,date1)\n\n    ax.set_title(\"Training score. \"+daterange)\n    ax.set_xlabel(\"Epochs\")\n    ax.set_xticks(x_epochs)\n    ax.set_ylabel(\"RMSE\")\n    ax.set_xlim(left=1)\n    ax.legend()\n\n    title=\"Markdowns = %s, \" % self.markdowns\n    title+=\"CPI & Unemployment = %s \" % self.CPI_Unemp\n    fig.suptitle(title)\n\n    plt.close(fig)\n    return fig\n    \n  def getTestPred(self, epochs=15):\n    '''\n    Returns the test set sales prediction.\n    '''\n    if type(self.y_train_pred)==type(None): self.fit(epochs)\n    if type(self.y_test_pred)==type(None): self.predict()\n    if self.detailLvl==\"Store\": cols=[\"Store\"]\n    else: cols=[\"Store\", \"Dept\"]\n    cols+=[\"Date\"]\n    if self.futureTest==False: cols+=[self.y_col]\n    cols+=[self.y_col+\"_Pred\"]\n    return self.data(\"test\")[cols].round(2)","b257a597":"import pickle as p\n\ndef overfittingTest(MD,CU):\n  dModel=dataModel(markdowns=MD, CPI_Unemp=CU, futureTest=True, validation=True,verbose=0) # S'agafa tot el rang d'entrenament con a conjunt d'entrenament, aix\u00ed que s'utilitza la separaci\u00f3 passat-futur.\n  dModel.fit(epochs=50)\n  return dModel.getTrainScore()\n\nofT={}\nbmap={False:0,True:1}\nfor MD in (False,True):\n  md=bmap[MD]\n  ofT[md]={}\n  for CU in (False,True):\n    cu=bmap[CU]\n    ofT[md][cu]=overfittingTest(MD,CU)","883057b5":"# Save results variable.\nimport pickle as p\n\nwith open(\"ofT.p\",\"wb\") as file:\n  p.dump(ofT,file)","b25bcd9f":"# Load results variable.\nimport pickle as p\n\nwith open(\"ofT.p\",\"rb\") as file:\n  ofT = p.load(file)","01e2b42a":"ofT[0][0]","e43a9529":"ofT[0][1]","a16bc4b8":"ofT[1][0]","f3019c19":"ofT[1][1]","f3bad1fb":"def getsplitDate(frac):\n  if frac==None:\n    return \"2011-10-26\"\n  else:\n    fecha=pd.to_datetime(\"2011-11-11\")+(pd.to_datetime(\"2012-10-26\")-pd.to_datetime(\"2011-11-11\"))*(frac\/100)\n    return \"%d-%02d-%02d\" % (fecha.year,fecha.month,fecha.day)\n\ndef futurePredictionEval(MD,CU,frac=None):\n  spDate=getsplitDate(frac)\n  dModel=dataModel(markdowns=MD, CPI_Unemp=CU, splitDate=spDate, validation=False,verbose=0)\n  op={}\n  op[\"data\"]=dModel.getTestPred(epochs=20)\n  op[\"fig\"]=dModel.evaluate()\n  return op\n\nfpE={}\nbmap={False:0,True:1}\nfor MD in (False,True):\n  md=bmap[MD]\n  fpE[md]={}\n  for CU in (False,True):\n    cu=bmap[CU]\n    fpE[md][cu]={}\n    if MD==False:\n      fpE[md][cu]=futurePredictionEval(MD,CU)\n    else:\n      for frac in (67,80,90):\n        fpE[md][cu][frac]=futurePredictionEval(MD,CU,frac)","350563cd":"# Save results variable.\nimport pickle as p\n\nwith open(\"fpE.p\",\"wb\") as file:\n  p.dump(fpE,file)","4783f21a":"# Load results variable.\nimport pickle as p\n\nwith open(\"fpE.p\",\"rb\") as file:\n  fpE=p.load(file)","f29e9d42":"fpE[0][0][\"fig\"]","8b588f7a":"fpE[0][1][\"fig\"]","e3c32048":"fpE[1][0][67][\"fig\"]","9b42f89f":"fpE[1][0][80][\"fig\"]","d10d199b":"fpE[1][0][90][\"fig\"]","ec28275c":"fpE[1][1][67][\"fig\"]","8cf33e38":"fpE[1][1][80][\"fig\"]","dd2fcdc5":"fpE[1][1][90][\"fig\"]","1bd07547":"def fpEfix(data):\n  op=data\n  return op[[\"Store\",\"Dept\", \"Date\",\"Weekly_Sales_Pred\"]]\n\n\ndef modmean(MD0CU0,MD0CU1,MD1CU0,MD1CU1,weights):\n  pred=[MD0CU0,MD0CU1,MD1CU0,MD1CU1]\n  weight=weights\n  totalw=0\n  mean=0\n  allnans=True\n  for i in range(4):\n    if pred[i]==pred[i]:\n      totalw+=weight[i]\n      mean+=weight[i]*pred[i]\n      allnans=False\n  if allnans==False:\n    return mean\/totalw\n  else: return np.nan\n\n\nfusedPreds=salesV2.copy()\nfor MD in (0,1):\n  for CU in (0,1):\n    if MD==1:\n      rightDF=fpEfix(fpE[MD][CU][90][\"data\"])\n    else:\n      rightDF=fpEfix(fpE[MD][CU][\"data\"])\n    fusedPreds=fusedPreds.merge(rightDF, how=\"left\", on=[\"Store\",\"Dept\", \"Date\"])\n    colname=\"MD%s_CU%s\" % (MD,CU)\n    fusedPreds.rename(columns = {'Weekly_Sales_Pred':'WSPred_'+colname}, inplace=True)\nfusedPreds[\"WSPred_ModMean\"]=fusedPreds.apply(lambda x: modmean(x.WSPred_MD0_CU0,x.WSPred_MD0_CU1,x.WSPred_MD1_CU0,x.WSPred_MD1_CU1, [2,2.2,0.9,1]),axis=1)\n\nplotdata=fusedPreds.query(\"Date > '2012-09-21' & Date <= '2012-10-26'\")","a38b8b44":"# Save results variable.\nimport pickle as p\nwith open(\"fpEmean.p\",\"wb\") as file:\n  p.dump(plotdata,file)","b803122c":"# Load results variable.\nimport pickle as p\n\nwith open(\"fpEmean.p\",\"rb\") as file:\n  plotdata=p.load(file)","5fb4e8fa":"meanweights=[2,2.2,0.9,1]\nx,xlab=plotdata[\"Weekly_Sales\"],\"Real\"\ny0,y0lab=plotdata[\"WSPred_ModMean\"],\"Weightened mean\"\ny1,y1lab=plotdata[\"WSPred_MD0_CU0\"],\"MD=False, CU=False, Weight=\"+str(meanweights[0])\ny2,y2lab=plotdata[\"WSPred_MD0_CU1\"],\"MD=False, CU=True, Weight=\"+str(meanweights[1])\ny3,y3lab=plotdata[\"WSPred_MD1_CU0\"],\"MD=True, CU=False, Weight=\"+str(meanweights[2])\ny4,y4lab=plotdata[\"WSPred_MD1_CU1\"],\"MD=True, CU=True, Weight=\"+str(meanweights[3])\n\nfig, ax = plt.subplots(ncols=5,sharex=True, sharey=True,figsize=(25,5))\nax[0].set_xlabel(\"Real\")\nax[0].set_ylabel(\"Pred\")\n\nax[0].scatter(x,y0,s=1)\nax[0].plot(x,x,color=\"black\")\nax[0].set_title(y0lab)\nax[1].scatter(x,y1,s=1)\nax[1].plot(x,x,color=\"black\")\nax[1].set_title(y1lab)\nax[2].scatter(x,y2,s=1)\nax[2].plot(x,x,color=\"black\")\nax[2].set_title(y2lab)\nax[3].scatter(x,y3,s=1)\nax[3].plot(x,x,color=\"black\")\nax[3].set_title(y3lab)\nax[4].scatter(x,y4,s=1)\nax[4].plot(x,x,color=\"black\")\nax[4].set_title(y4lab)\n\ndaterange= \"28\/09\/2012- 26\/10\/2012\"\nfig.suptitle(\"Future prediction test. \"+daterange,fontsize=16)","0de6c54e":"def futurePrediction(MD,CU):\n  dModel=dataModel(markdowns=MD, CPI_Unemp=CU, validation=False, futureTest=True, verbose=0)\n  op=dModel.getTestPred(epochs=30)\n  return op\n\nfPred={}\nbmap={False:0,True:1}\nfor MD in (False,True):\n  md=bmap[MD]\n  fPred[md]={}\n  for CU in (False,True):\n    cu=bmap[CU]\n    fPred[md][cu]=futurePrediction(MD,CU)","2d54ab74":"# Save results variable.\nimport pickle as p\nwith open(\"fPred.p\",\"wb\") as file:\n  p.dump(fPred,file)","2fbe76bd":"# Load results variable.\nimport pickle as p\n\nwith open(\"fPred.p\",\"rb\") as file:\n  fPred=p.load(file)","f27ebcca":"futureSales=dataMerge(features, futuresales, Detail=\"Dept\")\n\ndef modmean(MD0CU0,MD0CU1,MD1CU0,MD1CU1,weights):\n  pred=[MD0CU0,MD0CU1,MD1CU0,MD1CU1]\n  weight=weights\n  totalw=0\n  mean=0\n  allnans=True\n  for i in range(4):\n    if pred[i]==pred[i]:\n      totalw+=weight[i]\n      mean+=weight[i]*pred[i]\n      allnans=False\n  if allnans==False:\n    return mean\/totalw\n  else: return np.nan\n\nfinalPred=futureSales.copy()\n\nfor MD in (0,1):\n  for CU in (0,1):\n    rightDF=fPred[MD][CU][[\"Store\",\"Dept\", \"Date\",\"Weekly_Sales_Pred\"]]\n    finalPred=finalPred.merge(rightDF, how=\"left\", on=[\"Store\",\"Dept\", \"Date\"])\n    colname=\"MD%s_CU%s\" % (MD,CU)\n    finalPred.rename(columns = {'Weekly_Sales_Pred':'WSPred_'+colname}, inplace=True)\nfinalPred[\"WSPred_ModMean\"]=finalPred.apply(lambda x: modmean(x.WSPred_MD0_CU0,x.WSPred_MD0_CU1,x.WSPred_MD1_CU0,x.WSPred_MD1_CU1,[1.8,2,1,1.2]),axis=1)\n\nfinalPred=finalPred.query(\"WSPred_ModMean==WSPred_ModMean\").drop([\"Weekly_Sales\",\"Weight\"],axis=1)","fa8faa33":"finalPred","6f6ac9ec":"Now we modify **sales** dataframe so it can get future **Weekly_Sales** values.","3c9730f8":"## **Function definition**\nFirst the functions used in the data processing are defined.","2e9adc10":"The networks without markdowns can predict the next year to the train set quite well, whereas the networks with markdowns, with less training records, show worse results. However, as it learns more weeks, the prediction improves, to the point that in the 90% of the available training data the prediction is quite accurate, though the test set is quite close to the end of the training set (less than a month of difference). \n\nWe can conclude that our networks have a reasonable predictive performance.","d9409921":"## **Markdown analysis**\n\nNow we will analyze the markdowns to know their contribution to sales and the way they are represented in the dataset.\n\nTo do so, columns MarkDown1-5 and their sum (MarkDowns) VS Weekly_Sales is plotted. Furthermore, we differenciate between store types and we make a plot for holiday weeks and another for regular ones.","c1d81dc4":"\nOn MarkDown1-5 plots we see 3 different groups, small markdowns (<1000), intermediate markdowns (1000<m<50000) and big markdowns (50000<). That happens on both holiday and regular weeks, so maybe each group corrensponds to a different markdown type and sometimes one particular type is put in one markdown column or another.\n\nIf all markdowns are added so that effect is removed, we see that **in holiday weeks markdown effect on sales is quite noticeable, with a strong proportional relationship, whereas in the regular weeks the effect is more homogeneous**.\n\nGiven this conclusion, we will use the total markdowns column as an input for machine learning instead of each individual column.","0e7589f9":"## **Overfitting test**\n\nEach of the 4 neural networks are tested. The error is que root mean squared error (RMSE) with the standard scaling applied. Since the real RMSE is just the same but multiplied by a constant (the data variance), it doesn't affect the result.\n\n<font size=2>*WARNING: The execution time of the first 2 plots can exceed 15 minutes. Load the pickle file with the results variable if you already have it.*<\/font> ","10b9cf95":"We see that **the weightened mean method adapts quite well to the missing data problem**.","0d600188":"# **3. Future sales prediction**\n\nFinally we make the prediction of future sales with our fully trained networks (using all their trainable data). Networks with markdowns have a training data almost 2 years shorter than the others, thought their training data is overall closer to the data we want to predict and, of course, they take into account markdowns effect on sales. With all of this, the weights will have the following values:\n\n| Network | MarkDowns | CPI & Unemployment | Weight |\n| :----: | :----: | :----: | :----: | \n| **1** | No | No | 1.8 |\n| **2** | No | Yes | 2 |  \n| **3** | Yes | No | 1 |\n| **4** | Yes | Yes | 1.2 |\n\nAlso, each network is trained with 30 epochs.","c9378342":"The validation set error of the 2 last networks already reaches a stationary regime, while the other 2 ones still have room for improvement. This is due to the fact that the 2 first plots correspond to networks with a train set date range 2 times larger than the others, so they need more epochs to converge.\n\nBesides, from 20 epochs the performance improvement rate decreases substantially.","94426568":"# **Wallmart sales prediction using 4 neural networks**\n\nOn 2014, Wallmart made a competition about data analysis of 45 stores. The competition had 2 main objectives: **Department-wide sales prediction for each store** and **markdowns effect on holiday weeks analysis**. Regarding machine learning algorithms training, holiday weeks are weighted 5 times more than the others.\n\nThe dataset composed of 3 csv files:\n\n- **\"sales.csv\"**\n\n  Contains the weekly sales records by date, store and department. It's variable columns are:\n   \n  - **Date**: Weekly record date.\n  - **Store**: The store id.\n  - **Dept**: The department id.\n  - **Weekly_Sales**: The corresponding weekly sales.\n  - **IsHoliday**: If the week is part of a main holiday (Super Bowl, Labor Day, Thanksgiving or Christmas).\n  \n  Some noticeable details about the data:\n  \n  - Some **Weekly_Sales** values are negative. That can be due to product refunds. We suppose refunds are already included on all weekly records (recorded sales = sales - refunds) so negative values are viewed as the rest.\n  - For a specific week not always there will the records of all the departments of all the stores. Some may be missing.\n\n- **\"stores.csv\"**\n\n  Contains basic info about each store (type and size). Variables:\n  - **Store**: The store id.\n  - **Type**: Store type.\n  - **Size**: Store size.\n\n- **\"features.csv\"**\n \n  Contains info about each week features:\n  - **Date**: Weekly record date.\n  - **Store**: The store id. \n  - **Temperature**: Mean temperature of the store's region.\n  - **Fuel_Price**: Fuel prize of the region.\n  - **CPI**: Consumer Prize Index of the region.\n  - **Unemployment**: Unemployment rate of the region.\n  - **MarkDown1-5**: Anonymized data about the markdowns of each store.\n \n  Noticeable details:\n  - For an specific week not always there's info about all the markdowns (sometimes some **MarkDown** columns don't have a value).\n\nDataset date range spans from **05\/02\/2010** to **26\/07\/2013**, though some variables are available in a reduced range:\n- **Weekly_Sales**: From **05\/02\/2010** to **26\/10\/2012**. The competition asked for the prediction from the end of this date range to the end of the dataset date range (**02\/11\/2012** - **26\/07\/2013**).\n- **MarkDown1-5**: From **11\/11\/2011** to **26\/07\/2013**.\n- **CPI** and **Unemployment**: From **05\/02\/2010** to **26\/04\/2013**.\n\nIn addition to the occasional lack of data commented previously.","217f7372":"#### **Without markdowns \/ With CPI & Unemployment**","27d319ba":"### **Future prediction with a weightened mean**\n\nIn ordert to make the actual prediction, we will make the weightened mean of the prediction of the available neural networks for each testing record, since we have to take into account the amount of training data and the quantity of input variables of each network.\n\nTo see if this method is reasonable, we will take the results of the previous test and we will use them to predict the last month with sales values (28\/09\/2012 - 26\/10\/2012). Networks without markdowns have training data up to 21\/10\/2011 and the others up to 21\/09\/2021.\n\nThe weights have been assigned arbitrarily with the following values:\n\n| Network | MarkDowns | CPI & Unemployment | Weight |\n| :----: | :----: | :----: | :----: | \n| **1** | No | No | 2 |\n| **2** | No | Yes | 2.2 |  \n| **3** | Yes | No | 0.8 |\n| **4** | Yes | Yes | 1 |","4e96cad3":"# **2. Machine Learning and adaptation to missing data**\n\nAs we have already seen, there are date intervals on the dataset in which some variables are missing. We can make a division of the dataset date range according to the available variables:\n\n| Range | MarkDowns | CPI & Unemployment | Weekly_Sales |\n| :----: | :----: | :----: | :----: | \n| **05\/02\/2010** - **04\/11\/2011** | No | Yes | **Yes** |\n| **11\/11\/2011** - **26\/10\/2012** | Yes\\* | Yes | **Yes** |  \n| **02\/11\/2012** - **26\/04\/2013** | Yes\\* | Yes | **No** |\n| **03\/05\/2013** - **26\/07\/2013** | Yes\\* | No | **No** |   \n\n<font size=\"2\">\\*: *There are some weeks in which one or more markdowns are missing, so we can't compute the total markdown.*<\/font>\n\nMissing data is a problem when we have to choose our algorithm input, which in our case is a neural network. The typical solution is somehow interpolate the missing data but this can distort our sample set.\nHowever, in our case, we can try to solve this problem in a more straightforward way: **creating a neural network for each missing data combination.**\n\n| Network | MarkDowns | CPI & Unemployment | Training range | Testing range |\n| :----: | :----: | :----: | :----: | :----: | \n| **1** | No | No | 05\/02\/2010 - 26\/10\/2012 | 05\/02\/2010 - 26\/07\/2013 | \n| **2** | No | Yes | 05\/02\/2010 - 26\/10\/2012 | 05\/02\/2010 - 26\/04\/2013 |  \n| **3** | Yes | No | 11\/11\/2011 - 26\/10\/2012 | 11\/11\/2011 - 26\/07\/2013 |\n| **4** | Yes | Yes | 11\/11\/2011 - 26\/10\/2012 | 11\/11\/2011 - 26\/04\/2013 |\n\nThis way, depending of the available data, we can use one or many of those 4 networks:\n\n| MarkDowns | CPI & Unemployment | Network 1 | Network 2  | Network 3 | Network 4 |    \n| :----: | :----: | :----: | :----: | :----: | :----: |\n| No | No | \u2714 | - | - | - |\n| No | Yes | \u2714 | \u2714 | - | - |\n| Yes | No | \u2714 | - | \u2714 | - |\n| Yes | Yes | \u2714 | \u2714 | \u2714 | \u2714 |\n\nIn order to test the performance of each network we will make 2 tests:\n- **Overfitting test**: All the available training range (the records with sales data) is used for the training process. 10% of the data is used as the validation set. A high number of epochs (e=50) is applied to study the evolution of the error both sets and identify a possible overfitting regime.\n- **Future prediction test**: Data is split from a given date. Data records previous to that date make the train set, whereas the following records make the test set, so we can evaluate the actual predictive performance of sales of records subsequent to the training date range.\n\n Making a simple train-test evaluation in the same date range isn't enough to evaluate the predictive performance, since in the future the variable distribution may be different, so predicting the future with present data may have extrapolation issues. For example if an input variable value increases abruptly in the future, the algorythm may have problems, since it has been trained with that variable in a different value range, with much lower values.\n \n This method shows us if a given algorithm adapts well to future data.","f3478e5d":"#### **With markdowns \/ With CPI & Unemployment**","266ca64b":"## **Original dataframes**","bd83a481":"# **1. Data cleaning and analysis**\n\nOur data processing has the following stages:\n\n- **Merge** : The 3 dataframes (stores, sales, features) are merged in order to obtain all the info of each weekly record at the same time.\n\n- **Cook**: Apply one-hit encoding to each categorical variable (Store, Dept, IsHoliday and Type).\n\n  Week variable is kept as numeric, since it has a natural order and a concept of continuity (week 34 goes after 33 and before 35). Due to its nature, its relation with sales has to be periodical (1 year = 52 weeks -> S(w): S(w+52n)=S(w), n\u03f5\u2124). We expect the algorithm to end up with a similar conclusion.\n\n- **Serve**: Train-test data split according to one of the following criteria:\n  - **Random split**: Data records with sales are shuffled and are split following a given ratio.\n  - **Date split**: Data records with sales are split according to a given date. The previous records to this date are shuffled and form the train set, the rest form the test set.\n  - **Past-future split**: The records inside the sales data range (until 26\/10\/2012) are used as a train set. The rest, the ones we have to actually predict, form the test set. \n\n  Furthermore, a standard scaling is applied  to the data following the train set data distribution. This scaling shifts and scales the data so each column has mean=0 and variance=1. This way, neural networks can learn the date with more ease.\n\nIn the analysis part, here we focus on the effect of markdowns on sales and the possible meaning of the anonymized columns.","e633cfda":"#### **Without markdowns \/ Without CPI & Unemployment**","4d49604c":"## **Future prediction test**\n\nIn this test we will evaluate the ability of our network to predict data in a date range after the training one. For our case we do 2 different evaluations:\n\n- On one side, we have the networks without markdowns, those have a wide training date range (05\/02\/2010 - 26\/10\/2012), so we can predit the last year with sales data (26\/10\/2011 - 26\/10\/2012) using prior data (05\/02\/2010 - 19\/10\/2011).\n  \n  In this case, the training test contains records of every week of the year (some of them with records of 2 different years)\n \n\n- On the other side, the train set date range is less than a year (11\/11\/2011 - 26\/10\/2012), so no matter how we do it, the test set records will fall into unlearnt weeks.\n\n  In this case, we do several date splits of the trainable data to see how the predictive performance evolves if while it learns more weeks of the year. The splits are:\n\n| % | Train set | Test set |\n| :---: | :---: | :---: |\n| 67 | 11\/11\/2011 - 29\/06\/2012 | 06\/07\/2012 - 26\/10\/2012 |\n| 80 | 11\/11\/2011 - 17\/08\/2012 | 24\/08\/2012 - 26\/10\/2012 |\n| 90 | 11\/11\/2011 - 21\/09\/2012 | 28\/09\/2012 - 26\/10\/2012 |\n\n<font size=2>*WARNING: The execution time of results can exceed 30 minutes. Load the pickle file with the results variable if you already have it.*<\/font> ","638a791f":"# **0. Data import and first data cleaning**\n\nFirst we will assign the data types of each variable.\n- Store, Dept, IsHoliday and Type will be categorical.\n- Date will be datetime.\n- Temperature, Fuel Price, CPI, Unemployment, MarkDown1-5 and Weekly_Sales will be float64.\n\nAlso, we will add the following variables:\n- **MarkDowns**: The sum of the values of the 5 markdown columns.\n- **Week**: The week of the year, extracted from **Date**.\n- **Year**: The year, extracted from **Date**.\n- **Weight**: The weight of each record.","6113d5eb":"#### **With markdowns \/ Without CPI & Unemployment**","f7ea06c4":"## **Data model class and its methods definition**\n\nFirst, the **data model** class is defined, which gets a **set of input data** and applies it to a **multilayer neural network**. Then, we define some methods which return the results of the network training and testing processes.\n\nThe hidden layers of the neural network are the same for the 4 cases, since the difference of the number of input neurons between each network is relatively small (138-141). The network has the following structure.\n\n| Layer | Neurons | Activation F. |\n| ---- | :----: | :----: |\n| Input | ~140 | sigmoid |\n| Hidden 1 | 80 | sigmoid |\n| Hidden 2 | 40 | sigmoid |\n| Hidden 3 | 20 | selu |\n| Output | 1 | - |\n\n\nSince this is a regression task with quite sparse sales values, it's not very convenient to apply an activation funtion to the output neuron."}}