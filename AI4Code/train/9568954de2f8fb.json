{"cell_type":{"78e53577":"code","cce893d1":"code","e7540884":"code","77fdbb69":"code","21bd6cf3":"code","fe30aff4":"code","caed65b2":"code","2e78870a":"code","f6cd9016":"code","e59c8b54":"code","2d3c1402":"code","436352bb":"code","77dc2bc6":"code","6cd1f34f":"code","5c7a9364":"code","fc22aa01":"code","3232ac9b":"code","29224b2a":"code","6b918b6d":"code","0ecfe519":"code","c7266f94":"code","ba6bade2":"code","9107b41e":"code","98856dbb":"markdown","09136de7":"markdown","7f51a4c1":"markdown","30d4394f":"markdown","2f1affaa":"markdown","a752d1d2":"markdown","e337ab97":"markdown","e8495129":"markdown","6b8c2a4b":"markdown","5632cac5":"markdown","cabccd21":"markdown","740d7f7d":"markdown","366aa022":"markdown","0eccf302":"markdown"},"source":{"78e53577":"!pip install pandas-profiling","cce893d1":"import logging\nimport math\nimport warnings\n\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px\nfrom pandas_profiling import ProfileReport\nfrom sklearn import model_selection\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import (accuracy_score, f1_score, precision_score,\n                             recall_score)\nfrom statsmodels.discrete.discrete_model import BinaryResultsWrapper, Logit\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","e7540884":"logging.basicConfig(level=logging.ERROR)\nwarnings.filterwarnings('ignore')","77fdbb69":"df = pd.read_csv(\"..\/input\/league-of-legends-diamond-ranked-games-10-min\/high_diamond_ranked_10min.csv\")\ndf.head()","21bd6cf3":"profile = ProfileReport(df, explorative=False, interactions={'continuous': False})\nprofile","fe30aff4":"correlation = df.corr()[\"blueWins\"]","caed65b2":"# We only keep the variables that have an absolute correlation coefficient that is greater than 0.2\nTHRESHOLD = 0.2\nfeatures = [feature for feature, corr_coef in correlation.iteritems() if abs(corr_coef) >= THRESHOLD]","2e78870a":"# We drop all the variables that are symetric i.e blueKills == redDeaths\nsymetric_features = [\"redFirstBlood\", \"redKills\", \"redDeaths\", \"redGoldDiff\", \"redExperienceDiff\"]\n# We drop all the redundant variables. For example, we do not care about the total amount of gold earned, what matters is actually the difference compared to the enemy team\nredundant_features = [\"redTotalGold\", \"redAvgLevel\", \"redTotalExperience\", \"blueTotalGold\", \"blueAvgLevel\", \"blueTotalExperience\"]\n# We drop highly correlated features. For example, the redAvgLevel is logically highly corelated with the \nmulticolinear_features = [\"redCSPerMin\", \"blueCSPerMin\", \"blueEliteMonsters\", \"redEliteMonsters\", \"blueGoldPerMin\", \"redGoldPerMin\"]\n\nfeatures = set(features) - set(symetric_features) - set(redundant_features) - set(multicolinear_features)\nfeatures.remove(\"blueWins\") # Removing target variable from features","f6cd9016":"df_features = df[features]","e59c8b54":"df_features.head()","2d3c1402":"def check_colinearity_from_correlation(dataframe: pd.DataFrame, threshold: float) -> None:\n    correlation = dataframe.corr()\n    analyzed_pair = set()\n    for index, serie in correlation.iterrows():\n        for column, value in serie.iteritems():\n            if index != column and value >= threshold and tuple(sorted((index, column))) not in analyzed_pair:\n                print(f\"There may be colinearity between {index} and {column}. Correlation : {value}\")\n            analyzed_pair.add(tuple(sorted((index, column))))\n\n\ndef check_colinearity_from_vif(dataframe: pd.DataFrame, threshold: float) -> None:\n    variance_inflation = [variance_inflation_factor(dataframe.values, i) for i in range(dataframe.shape[1])]\n    for feature, vif in zip(dataframe.columns, variance_inflation):\n        if vif >= threshold:\n            print(f\"{feature} may be colinear. VIF : {vif}\")","436352bb":"print(\"Multicolinearity check using correlation :\\n\")\ncheck_colinearity_from_correlation(df_features, threshold=0.7)\nprint(\"\\nMulticolinearity check using VIF :\\n\")\ncheck_colinearity_from_vif(df_features, 10)","77dc2bc6":"X_train, X_test, y_train, y_test = model_selection.train_test_split(\n    df_features, df[\"blueWins\"], test_size=0.2, random_state=42)\n\nmodel = Logit(y_train, X_train).fit()","6cd1f34f":"model.summary()","5c7a9364":"predictions = model.predict(X_test)\npredictions = [1 if x >= 0.5 else 0 for x in predictions]","fc22aa01":"def compute_metrics(y_true: np.array, y_pred: np.array) -> None:\n    print(f\"Accuracy = {accuracy_score(y_true, y_pred)}\")\n    print(f\"F1 = {f1_score(y_true, y_pred)}\")\n    print(f\"Recall = {recall_score(y_true, y_pred)}\")\n    print(f\"Precision = {precision_score(y_true, y_pred)}\")","3232ac9b":"compute_metrics(y_test, predictions)","29224b2a":"def filter_significant_variables(model: BinaryResultsWrapper, pvalue_threshold: float) -> list:\n    return [variable for variable, pvalue in model.pvalues.items() if pvalue <= pvalue_threshold]\n\ndef plot_variable_impact_on_odds_ratio(model: BinaryResultsWrapper, pvalue_threshold: float = 0.05) -> None:\n    significant_variables = filter_significant_variables(model, pvalue_threshold)\n    coefficients = {variable: coef for variable, coef in model.params.items() if variable in significant_variables}\n    odds = [(variable, (math.exp(coef) - 1) * 100 if coef > 0 else (-1 + math.exp(coef)) * 100) for variable, coef in coefficients.items()]\n    df = pd.DataFrame(odds, columns=[\"Variable\", \"Impact on odds of winning (in %)\"])\n    fig = px.bar(df, x=\"Impact on odds of winning (in %)\", y=\"Variable\", orientation='h')\n    fig.show()","6b918b6d":"significant_variables = filter_significant_variables(model, 0.05)\nsubset_df = df[significant_variables]\n\nprint(\"Multicolinearity check using correlation :\\n\")\ncheck_colinearity_from_correlation(subset_df, threshold=0.7)\nprint(\"\\nMulticolinearity check using VIF :\\n\")\ncheck_colinearity_from_vif(subset_df, 10)","0ecfe519":"plot_variable_impact_on_odds_ratio(model)","c7266f94":"features = significant_variables + [\"blueWardsPlaced\", \"blueTowersDestroyed\"]\ndf_features = df[features]","ba6bade2":"X_train, X_test, y_train, y_test = model_selection.train_test_split(\n    df_features, df[\"blueWins\"], test_size=0.2, random_state=42)\n\nmodel = Logit(y_train, X_train).fit()","9107b41e":"model.summary()","98856dbb":"# Feature Selection","09136de7":"# Conclusion\n\nThe main gold of this first analysis resorting to **Logistic Regression** was to be able to identify the key factors that could, within the 10 first minutes of a LoL game, impact the odds of winning of a given team.\n\nThanks to this first analysis, we were able to confirm the fact that both the **gold** generated and **experience** acquired are key factors in determining which team would win.\n\nWe also learnt that the dragon had a non-negligible effect on the odds of winning, but that this effet was asymetric has the increase in the odds of winning when a team gets the first dragon is much greater when concerning the blue than the red team.\n\nWith our Logistic Regression that has not been fine-tuned and a probabiliy threshold set at **0.5**, we managed to predict with a 73.4% accuracy the outcome of a LoL game from data about the first 10 minutes of the game which is actually pretty good when keeping in mind that in the case of this study, we were not able to grasp the effects of :\n- Tilt : This happens when players get really mad and frustrated due to the succession of bad events occuring along the game and that negatively affect their performance. It is a factor to consider since every indicators can point towards a win, it only needs a few minutes of players being tilted to drastically decrease their odds of winning\n- Nashor : As we are only analyzing the first 10 minutes here, we cannot take into account the bonus provided by the Nashor which can, in some games, completely change the situation\n- AFK : We do not have any informations about whether a player left the game. Even if everything is going very well and we are predicting high odds of winning, having one player to left the game should cause a huge drop of the winning odds\n- Champions : We are not taking into account which champions are being played by the players. Everything being equal, having a huge lead with a champion considered as \"early\" could lead to a fast snowballing of the game.\n- Repartition of the gold : We know how much gold a team earned, but we are not provided with the distribution of the amount of gold among the team. Indeed having gold equally distributed in a team should increase the odds of winning compared to a situation in which the `blueGoldDiff` is actually mostly concentrated on a single player\n","7f51a4c1":"The goal of this section is to select the features that we are going to consider to try to predict the winner of the game. \nFor this purpose, we will first rely on the correlation matrix, from there, we will then slim the features set by ensuring that they do not have an high VIF.","30d4394f":"Despite our first attempt to manually handle colinearity with our feature selection, it seems that we still have some multicolinearity problem.\n\nWe are first going to try if we manage to achieve a good performance from our first selection of variable, if not, we will eventually think about ways to remove the remaining multicollinearity","2f1affaa":"Simple interpretation of the results :\n- When blue team gets the first dragon, it increases the team's winning odds by 37% (if considering all the other variables as fixed)\n- When the red team gets the first dragon, it reduces the team's winning odds by 20.4% (if considering all the other variables as fixed)\n- For each additional minions killed by the red team, the blue team's winning odds increase by 0.42% (if considering all the other variables as fixed)\n- For each additional minions killed by the blue team, the blue team's winning odds decrease by 0.42% (if considering all the other variables as fixed)\n- Each time the blue team gets one more experience point compared to the red one, its winning ods increase by 0.02% (if considering all the other variables as fixed)\n- Each time the blue team gets one more gold compared to the red one, its winning ods increase by 0.04% (if considering all the other variables as fixed)\n\n## Things to notice\n\nWithin the first 10 minutes of the game, the dragon has a huge impact on the odds of winning, however this impact is not symetric as we saw that when blue team gets the first dragon, the increase of the odds of winning (37%) is almost the double of the increase when the red team actually gets it which makes a huge difference.\n\nParadoxically, the more minions a team kills, the lower is its odds of winning. This sounds to be counterintuitive but this fact can be explained. Indeed, one could say that the more minions a team is killing, the more the lanes are pushed meaning that it brings the enemy team next to their turrets which makes it way harder for the pushing team to get more kills compared to when they get pushed\n\nRegarding the low increase of the odds of winning for the `blueGoldDiff` and `blueTotalMinionsKilled`, this has to be considered with regards of the fact that 1 unit of those is not much at all. For example, the standard deviation those two variables are respectively 2453 and 1920. As an illustration, if blue teams gets 2453 more gold than the red team or 1920 more experience points, it respectively increases the odds of winning of the blue team by 98% (2453 * 0.04) or by 38.4% (1920 * 0.02)\n\nDespite their low coefficients, it seems, as expected, that the two most important factors, at least in the 10 first minutes of a League of Legends game are actually the amount of gold generated and the experience points acquired by the team.","a752d1d2":"We still have some variables that seem colinear but we can see that our feature selection allowed us to reduce the number of variables concerned and also lowered the VIF of the `blueTotalMinionsKilled` and `redTotalMinionsKilled` since they decrease from 122 to 87.","e337ab97":"# Importing packages","e8495129":"Despite the presence of some redundant variables, it looks like the regression is presenting a quite good accuracy even though we're only considering the first 10 minutes of the game","6b8c2a4b":"# Loading dataset and first visualization","5632cac5":"# Attempt to add some relevant yet uncorrelated features to the Logistic Regression\n\nWe filtered out a lot of features since we only kept those that have a correlation with our target variables which was greater than 0.2 but we actually think, as a LoL player that some features have been unfairly removed from our set of features, among them we have :\n- blueWardsPlaced : more wards equals to more squirmishes (supposedly positively correlated with `blueKills` and thus with `blueGoldDiff`) and less deaths (supposedly positively related with the `blueGoldDiff` since less deaths equals to less gold for the enemy team)\n- blueTowersDestroyed : More towers destroyed equals to more gold and more vision\n\nRemark : The herald could be a variable to consider, however, due to the fact that the first and main goal of the herald is to destroy turrets, we will not include it in the following regression as it may introduce some multicolinearity with `blueTowersDestroyed`","cabccd21":"The two freshly introduced variables are actually highly statistically insignificant, this is unexpected from a player perspective but actually really understandable from a statistical point of view. \n\nIndeed, these two variables are, as presented earlier, indirectly related to the amount of gold generated which is directly impacting the `blueGoldDiff` parameter which is key as we saw earlier.\n\nApart from this, an interesting point is also the coefficient associated with the `blueWardsPlaced` parameter. Indeed, despite the fact that the parameter is insignificant, it particularly present a negative coefficient which means that the more wards placed, the less the odds of winning.\n\nThis seems counterintuitive but it could be explained by the fact that :\n- More wards placed equals to potentially more wards destroyed which generates gold for the enemy team and thus reduces the `blueGoldDiff` parameter\n- More wards placed equals to more wards bought which means more gold spent in things that do not increase the fighting skills of the player's characters which probably tends to reduce the number of kills and\/or increase the number of deaths which *in fine*, reduces the `blueGoldDiff` parameter. ","740d7f7d":"As per the report, we observe that many variables are highly correlated.\n\nFor example, `redCSPerMin` is highly correlated with `redTotalMinionsKilled` which makes sense since the more CS killed per minute, the more minions killed overall.\n\nWe will probably face a multicollinearity problem here.\n\nAs a League of Legends player, we could also point out some interesting things :\n- It seems that in average, red side tends to have the first drake more often (41.3% vs 36.2%). This point is especially surprising as it is supposed to be easier for blue side to both ward the Drake's pit and to steal it.\n- It seems that in average, blue side tends to have the first herald more often (18.8% vs 16%)\n- The win probability seems to be independent on the side which is a bit surprising as the side is supposed to have a significant (while supposedly low) impact on the win probability as :\n    - Blue side picks first which can guarantee the pick of an \"OP\" champ while red side can thus counterpick. The draft aspect is not explicit here but we suppose that it does not have any impact on the win probabilities as we are analyzing SoloQ data whereas the draft effect is especially observable in competitive matches where the level is way beyond the one from Diamond.\n    - Due to the way the map has been created, the odds of getting the objectives slightly vary from one side to another\n    ","366aa022":"# Logistic Regression Fitting","0eccf302":"We can see here that we actually have a bunch of variable that we thought would be important to determine the probability of winning that actually are statistically insignificant.\n\nAmong them we can list :\n- blueDeaths\n- blueKills\n- blueFirstBlood\n- blueAssists\n- redAssists\n\nActually, this is not as unexpected as we could think at first hand. \n\nIndeed, we can see for example that the `redAssists` and `blueAssists` variables are clearly insignificant but this is expected as we saw earlier that these two were kind of redundant as they were highly correlated respectively with `blueDeaths` and `blueKills`.\n\n`blueKills` is not necessary highly correlated with another variable from our set of features however, we saw that it presented a quite high VIF (33) which could suppose that it may be colinear with another variable which we suppose is the `blueGoldDiff` variable as more kills means more gold and thus tends to increase the `blueGoldDiff` parameter.\n\nRegarding the `blueDeaths` parameter, it also presents a high p-value probably because more deaths implies more gold for the enemy team which leads to a decrease in the `blueGoldDiff` variable."}}