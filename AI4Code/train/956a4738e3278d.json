{"cell_type":{"f8391315":"code","dab4ba69":"code","c2b20a69":"code","2a8ff867":"code","954e70e8":"code","523231f0":"code","e0c45710":"code","1a4b977e":"code","4c53f6d5":"code","a7483ecf":"code","46c916d9":"code","b758339c":"code","c3eda686":"code","0f8a3048":"code","24f63907":"code","1f721f80":"code","222d8124":"code","b638b2cb":"code","d4c12f9b":"code","d3e3f59d":"code","15771ae0":"code","2d4c2f82":"code","f919919a":"code","26a05a62":"code","36533378":"code","71701af1":"code","592063f5":"code","6f7c6d0a":"code","893ab408":"code","076b9749":"code","beb9b57d":"code","20777d76":"code","13f5adf5":"code","4a0b2d7a":"code","b7877d24":"code","cbe9f70a":"code","550cd504":"code","c3b15399":"code","43f3b4b5":"markdown","4b6c32c5":"markdown","1acef818":"markdown","22e633be":"markdown","ff3e3bc9":"markdown","6d193267":"markdown","602f233a":"markdown","7c97d268":"markdown","84f41249":"markdown","17173beb":"markdown","9765546f":"markdown","aa028ea7":"markdown","d3e043c1":"markdown","3931e375":"markdown","571056be":"markdown","955f8cec":"markdown","93169bc1":"markdown","f512a88c":"markdown","9279498f":"markdown"},"source":{"f8391315":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.model_selection import GridSearchCV\n\n# Load the datasets\ntrain = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")","dab4ba69":"train.head()","c2b20a69":"train.info()","2a8ff867":"test.info()","954e70e8":"# PassengerId - We'll drop this column\ntrain.drop(['PassengerId'], axis=1, inplace=True)","523231f0":"# Survived - this is the target variable\nsns.countplot(x=train['Survived'],data=train);","e0c45710":"train[\"Age\"].fillna(train.groupby(['Pclass','Sex'])['Age'].transform(\"mean\"), inplace=True)\ntest[\"Age\"].fillna(test.groupby(['Pclass','Sex'])['Age'].transform(\"mean\"), inplace=True)","1a4b977e":"# Pclass = ticket class\nsns.countplot(x=train['Pclass'],data=train,hue='Survived');","4c53f6d5":"# Change the data type of Pclass and get dummies\n#test['Pclass'] = test['Pclass'].astype('object')\n#train['Pclass'] = train['Pclass'].astype('object')\n\ntest = pd.get_dummies(test, columns=['Pclass'])\ntrain = pd.get_dummies(train, columns=['Pclass'])","a7483ecf":"# Name\ntrain.drop(['Name'], axis=1,inplace=True)\ntest.drop(['Name'], axis=1,inplace=True)","46c916d9":"# Sex\nsns.countplot(x=train['Sex'],data=train,hue='Survived');","b758339c":"# Manually encode Sex\ntrain['Sex'].replace('male', 1, inplace=True)\ntrain['Sex'].replace('female', 0, inplace=True)\ntest['Sex'].replace('male', 1, inplace=True)\ntest['Sex'].replace('female', 0, inplace=True)","c3eda686":"# Age\ntrain['Age'].describe()","0f8a3048":"# Let's see which ages are fractional\ntrain[train['Age'] % 1 != 0].head()","24f63907":"# Impute the missing age values to the median\n#train['Age'].fillna(train['Age'].median(), inplace=True)\n#test['Age'].fillna(test['Age'].median(), inplace=True)\n","1f721f80":"# SibSp\nsns.countplot(x=train['SibSp'],data=train,hue='Survived');","222d8124":"# Parch\nsns.countplot(x=train['Parch'],data=train,hue='Survived');","b638b2cb":"# Ticket - Lets see if there are duplicate ticket numbers\ndupes = train[train.duplicated(['Ticket'])]\ndupes","d4c12f9b":"# Drop the Ticket column\ntrain.drop(['Ticket'], axis=1,inplace=True)\ntest.drop(['Ticket'], axis=1,inplace=True)","d3e3f59d":"# Fare\nsns.distplot(x=train['Fare']);","15771ae0":"sns.boxplot(x=train['Fare']);","2d4c2f82":"# Check for Fares above 300\ntrain[train['Fare'] > 300]","f919919a":"# These three outliers must be typos. They're all on the same ticket. Lets impute them with the mean Fare from rows of the same Pclass\ntest['Fare'][test['Fare'] > 300] = test['Fare'][test['Pclass_1'] == 1].mean()\ntrain['Fare'][train['Fare'] > 300] = train['Fare'][train['Pclass_1'] == 1].mean()\n\n# There were some nulls in the train set for this column, lets impute them with mean. Should probably use the mean of the same Pclass as well.\ntest['Fare'] = test['Fare'].fillna(test['Fare'].mean())\ntrain['Fare'] = train['Fare'].fillna(train['Fare'].mean())\n\nsns.boxplot(x=train['Fare']);","26a05a62":"log_transformer = FunctionTransformer(np.log1p)\n\ntest['Fare_trans'] = log_transformer.fit_transform(test['Fare'])\ntrain['Fare_trans'] = log_transformer.fit_transform(train['Fare'])\n\ntrain.drop(['Fare'], axis=1,inplace=True)\ntest.drop(['Fare'], axis=1,inplace=True)","36533378":"# Cabin\nprint(str(round(train['Cabin'].isnull().sum() \/ train.shape[0] * 100,0)) + \"% of Cabin is empty in the train set\")\nprint(str(round(test['Cabin'].isnull().sum() \/ test.shape[0] * 100,0)) + \"% of Cabin is empty in the test set\")","71701af1":"train.drop(['Cabin'], axis=1,inplace=True)\ntest.drop(['Cabin'], axis=1,inplace=True)","592063f5":"# Embarked\nsns.countplot(x=train['Embarked'],data=train,hue='Survived');","6f7c6d0a":"train.head()","893ab408":"# Impute the missing Embarked data to median and encode them\ntrain['Embarked'].fillna(\"S\", inplace=True)\ntest['Embarked'].fillna(\"S\", inplace=True)\n\n#test = pd.get_dummies(test, columns=['Embarked'])\n#train = pd.get_dummies(train, columns=['Embarked'])","076b9749":"# Drop some columns that have very low correlation to the target\ntrain.drop(['SibSp'], axis=1,inplace=True)\ntrain.drop(['Age'], axis=1,inplace=True)\ntrain.drop(['Parch'], axis=1,inplace=True)\ntrain.drop(['Embarked'], axis=1,inplace=True)\n\ntest.drop(['SibSp'], axis=1,inplace=True)\ntest.drop(['Age'], axis=1,inplace=True)\ntest.drop(['Parch'], axis=1,inplace=True)\ntest.drop(['Embarked'], axis=1,inplace=True)","beb9b57d":"# Take a look at the correlations \nsns.heatmap(train.corr(), annot=True);","20777d76":"# Setup the target variable\ny = train.Survived\n\n# Setup the train dataframe\nX = train.drop(['Survived'], axis=1)\nX_test = test.drop(['PassengerId'],axis=1)","13f5adf5":"# RandomForestClassifier\nrf_model = RandomForestClassifier(n_estimators = 70, max_leaf_nodes = 50, max_depth = 10, random_state = 82)\nrf_model.fit(X,y)","4a0b2d7a":"#rf_params = {'n_estimators': (40,50,60,70,80),\n#             'max_leaf_nodes': (35,40,45,50),\n#             'max_depth': (10,15,20,25)}\n\n#rf_gs = GridSearchCV(rf_model,rf_params,cv=10)\n#rf_gs.fit(X, y);\n\n#n_estimators = rf_gs.best_params_['n_estimators']\n#max_leaf_nodes = rf_gs.best_params_['max_leaf_nodes']\n#max_depth = rf_gs.best_params_['max_depth']\n        \n#rf_model = RandomForestClassifier(n_estimators=n_estimators, max_leaf_nodes=max_leaf_nodes, max_depth=max_depth, random_state=82)\n#rf_model.fit(X,y)\n#print(rf_gs.best_params_)","b7877d24":"# XGBoost\nxgb_model = XGBClassifier(max_depth=5, n_estimators=300, learning_rate=.05, random_state = 82, verbosity=0)\nxgb_model.fit(X,y)","cbe9f70a":"#xgb_params = {'max_depth': (3,4,5),\n#              'n_estimators': (300,400,500),\n#              'learning_rate': (0.25,0.05,0.75)}\n\n#xgb_gs = GridSearchCV(xgb_model,xgb_params,cv=10)\n#xgb_gs.fit(X, y);\n\n#max_depth = xgb_gs.best_params_['max_depth']\n#n_estimators = xgb_gs.best_params_['n_estimators'] \n#learning_rate = xgb_gs.best_params_['learning_rate']\n        \n#xgb_model = XGBClassifier(max_depth=max_depth, n_estimators=n_estimators, learning_rate=learning_rate, random_state = 82, verbosity=0);\n#xgb_model.fit(X,y);\n\n#print(xgb_gs.best_params_)","550cd504":"kfold = KFold(n_splits=10, random_state = 1, shuffle=True)\n\nresults = cross_val_score(rf_model, X, y, cv=kfold)\nprint(\"RF Accuracy: %.2f%%\" % (results.mean()*100.0))\n\nresults = cross_val_score(xgb_model, X, y, cv=kfold)\nprint(\"XGB Accuracy: %.2f%% \" % (results.mean()*100.0))","c3b15399":"#output = pd.DataFrame({'PassengerId':test.PassengerId, 'Survived':rf_preds})\n#output.to_csv('titanic_submission.csv', index=False)\n#print(\"Your submission was successfully saved!\")","43f3b4b5":"- **There are 891 passengers in the training set.**\n- **Survived is the target column.**\n- **SibSp = the number of siblings \/ spouses aboard.**\n- **Parch = The number of parents \/ children aboard.**\n- **Age, Cabin and Embarked have missing values.**\n- **Age is a float datatype .. are some ages in fractions?**","4b6c32c5":"- **A passenger was more likely to survive if they were traveling with a parent or a child.**","1acef818":"- **Less than 100 females died, while almost 500 males died.**","22e633be":"- **There are three classes of tickets (1, 2 and 3). Most of the people who did not survive had class 3 tickets.** \n- **These should be encoded so they're not used as continuous variables**.","ff3e3bc9":"- **Most of Cabin is empty, I'm going to drop it.**","6d193267":"- **I'm going to try a few different, simple models here and use GridSearchCV to find the best parameters**","602f233a":"- **I suspect the ages under 1 are typos. Nobody is going to write a childs age as .83. All the ones under 1 survived, so we'll leave them alone for now.**","7c97d268":"- **There are 210 rows that share ticket numbers. I'm not sure there's any relevance to this.**\n- **The values in the Ticket column don't seem to have any immediately recognizable patterns, so I'm going to drop it.**","84f41249":"- **The minimum age is .42, so there are fractional ages here.**","17173beb":"- **A passenger was much more likely to survive if traveling with a sibling or a spouse.**","9765546f":"- **There are 418 passengers in the test set.**\n- **Cabin is mostly nulls. Age has around 20% nulls as well.**\n- **Let's take a look at each of the variables.**","aa028ea7":"- **The Fare column is right skewed. It looks like there are outliers.**","d3e043c1":"## Build models","3931e375":"1. **EDA**\n2. **Impute data\/feature engineering**\n3. **Model building\/Tuning**\n4. **Evaluate\/score**","571056be":"## EDA","955f8cec":"- **Use KFold cross validation to get accuracy scores.**","93169bc1":"- **Around 30% of the passengers survived.**","f512a88c":"- **That's a little better, although it's still skewed to the right.**\n- **Lets transform the Fare values with a log transform and drop the original column.**","9279498f":"- **The name column seems useless. The person's title might have some correlation to the target. But I'm going to drop it for now.**"}}