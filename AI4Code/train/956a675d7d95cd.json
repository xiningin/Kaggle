{"cell_type":{"8498b1c7":"code","95ebd036":"code","bb372fea":"code","dcd24d06":"code","d05a00e1":"code","8feb2338":"code","40d50be7":"code","11e32421":"code","901d7264":"code","7189967e":"code","ea6b73cc":"code","375e7d22":"code","fe13e0e2":"code","dc433b69":"code","215deddc":"code","7f348fed":"code","70bc8756":"code","6c1235d7":"code","6256a21c":"code","c4c4f73e":"code","06adb0a7":"code","7346a497":"code","72ee0fa5":"code","cb8e8007":"code","7b22f156":"code","3f8fc1f6":"code","0a26b72c":"code","97c721e8":"code","5ff20706":"code","d7ea266c":"code","1f6a9a51":"code","abd7d62a":"code","4a6c6a52":"code","32b15657":"code","ae462f13":"code","e4aa4c9c":"code","6ab770ff":"code","6d45648d":"code","78de8354":"code","07001bac":"code","f554ca46":"markdown","a74fc4dc":"markdown","b8e2c7c2":"markdown","f3082147":"markdown","cce8cf83":"markdown","ce810b26":"markdown","ec346b14":"markdown","f418a4c9":"markdown","ea3bdfd3":"markdown","0ad8ca88":"markdown","b77a5077":"markdown","eeefa4a3":"markdown","6c1fe140":"markdown","4112ebd0":"markdown","7f73446c":"markdown","c3bac097":"markdown","4ff48fc8":"markdown","dd8866e4":"markdown","f2893bc7":"markdown","b4853073":"markdown","437c84be":"markdown","bcc48ec3":"markdown","d987fbe8":"markdown","c58bb6aa":"markdown","93a8674a":"markdown","98201f69":"markdown","42a1a895":"markdown","1af6f305":"markdown","0404d3a5":"markdown","f313dcb3":"markdown","3f5fa4dd":"markdown","0d93ffad":"markdown","e23e3c58":"markdown","35619d08":"markdown","e62e9895":"markdown"},"source":{"8498b1c7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","95ebd036":"df = pd.read_csv(\"\/kaggle\/input\/ccdata\/CC GENERAL.csv\")\ndf.info()","bb372fea":"df.isnull().sum()","dcd24d06":"df.describe()","d05a00e1":"df.head(10)","8feb2338":"df = df.drop(['CUST_ID'],axis = 1)\ndf.head(10)","40d50be7":"import matplotlib.pyplot as plt\nimport seaborn as sns","11e32421":"for column in df.columns:\n    plt.figure(figsize = (30,5))\n    sns.histplot(df[column])\n    plt.show()","901d7264":"for column in df.columns:\n    plt.figure(figsize = (30,5))\n    sns.boxplot(df[column])\n    plt.show()","7189967e":"#imputing with median values using sklearn.impute\nfrom sklearn.impute import SimpleImputer\nimputer = SimpleImputer(missing_values=np.nan, strategy='median')\n\nX = df['MINIMUM_PAYMENTS'].values.reshape(-1,1)\nX = imputer.fit_transform(X) \n\ndf['MINIMUM_PAYMENTS_NEW'] = X","ea6b73cc":"X2 = df['CREDIT_LIMIT'].values.reshape(-1,1)\nX2 = imputer.fit_transform(X2) \n\ndf['CREDIT_LIMIT_NEW'] = X2","375e7d22":"df = df.drop(['CREDIT_LIMIT','MINIMUM_PAYMENTS'],axis = 1)\ndf.info()","fe13e0e2":"df.isnull().sum().sum()","dc433b69":"sns.pairplot(df)\nplt.show()","215deddc":"plt.figure(figsize=(20,20))\ncorr_df = df.corr()\nsns.heatmap(corr_df,annot=True)\nplt.show()","7f348fed":"from sklearn.model_selection import train_test_split\ntrain_df, test_df = train_test_split(df,test_size=0.2,random_state=42)","70bc8756":"from sklearn.preprocessing import MinMaxScaler\nmm = MinMaxScaler()\ntrain_df = mm.fit_transform(train_df)\ntest_df = mm.transform(test_df)","6c1235d7":"from sklearn.preprocessing import PowerTransformer\npt = PowerTransformer()\ntrain_df = pt.fit_transform(train_df)\ntest_df = pt.transform(test_df)","6256a21c":"from sklearn.cluster import KMeans","c4c4f73e":"interclusterdistance = []\n\nfor clusters in range(1,20):\n    km = KMeans(n_clusters = clusters,init ='k-means++', max_iter=300,random_state=42)\n    km.fit(train_df)\n    interclusterdistance.append(km.inertia_)\n    \n#plotting the values\nplt.figure(figsize=(30,10))\nplt.plot(range(1, 20), interclusterdistance, marker='o', color='r')\nplt.xlabel('Number of clusters')\nplt.ylabel('Inter Cluster Distance')\nplt.show()","06adb0a7":"km = KMeans(n_clusters = 6,init ='k-means++', max_iter=300,random_state=42)\nkm.fit(train_df)\ny_pred = km.predict(train_df)","7346a497":"cluster_df = pd.DataFrame(train_df,columns = df.columns)\ncluster_df['clusters'] = y_pred\ncluster_df.head(10)","72ee0fa5":"cluster_df['clusters'].value_counts()","cb8e8007":"X = cluster_df[['BALANCE','PURCHASES']].to_numpy()","7b22f156":"interclusterdistance = []\n\nfor clusters in range(1,20):\n    km = KMeans(n_clusters = clusters,init ='k-means++', max_iter=300,random_state=42)\n    km.fit(X)\n    interclusterdistance.append(km.inertia_)\n    \n#plotting the values\nplt.figure(figsize=(30,10))\nplt.plot(range(1, 20), interclusterdistance, marker='o', color='g')\nplt.xlabel('Number of clusters')\nplt.ylabel('Inter Cluster Distance')\nplt.show()","3f8fc1f6":"km = KMeans(n_clusters = 4,init ='k-means++', max_iter=300,random_state=42)\nkm.fit(X)\ny_balance_pred = km.predict(X)","0a26b72c":"plt.scatter(X[y_balance_pred==0, 0], X[y_balance_pred==0, 1], s=100, c='red', label ='Cluster 1')\nplt.scatter(X[y_balance_pred==1, 0], X[y_balance_pred==1, 1], s=100, c='blue', label ='Cluster 2')\nplt.scatter(X[y_balance_pred==2, 0], X[y_balance_pred==2, 1], s=100, c='green', label ='Cluster 3')\nplt.scatter(X[y_balance_pred==3, 0], X[y_balance_pred==3, 1], s=100, c='yellow', label ='Cluster 4')\n\nplt.scatter(km.cluster_centers_[:, 0], km.cluster_centers_[:, 1], s=300, c='cyan', label = 'Centroids')\nplt.show()","97c721e8":"from sklearn.cluster import DBSCAN","5ff20706":"dbscan = DBSCAN(eps=2,min_samples=6)\ndbscan.fit(train_df)\ny_dbscan_pred = dbscan.labels_\ny_dbscan_pred","d7ea266c":"dbscan_df = pd.DataFrame(train_df,columns = df.columns)\ndbscan_df['clusters'] = y_dbscan_pred\ndbscan_df.head(10)","1f6a9a51":"dbscan_df['clusters'].value_counts()","abd7d62a":"X = dbscan_df[['BALANCE','PURCHASES']].to_numpy()","4a6c6a52":"dbscan = DBSCAN(eps=0.075,min_samples=2)\ndbscan.fit(X)\ny_dbscan_pred = dbscan.labels_\ny_dbscan_pred","32b15657":"dbscan_df['clusters'] = y_dbscan_pred\ndbscan_df['clusters'].value_counts()","ae462f13":"plt.figure(figsize=(10,10))\nplt.scatter(dbscan_df['BALANCE'],dbscan_df['PURCHASES'],c=dbscan_df['clusters'])\nplt.title('DBSCAN Clustering',fontsize=20)\nplt.xlabel('Feature 1',fontsize=14)\nplt.ylabel('Feature 2',fontsize=14)\nplt.show()","e4aa4c9c":"!pip install minisom","6ab770ff":"from minisom import MiniSom","6d45648d":"som = MiniSom(x = 20,y = 20, input_len = 17, sigma=0.25) \n#Here x and y are just dimensions which we will use for visualising later, input length is the number of features in X and sigma is the radius of differnt neigbourhood","78de8354":"som.train_random(train_df, 10000) ","07001bac":"from pylab import bone, pcolor, colorbar, plot, show\nbone()\npcolor(som.distance_map().T)\ncolorbar()\nmarkers = ['o', 's']\ncolors = ['r', 'g']\nfor i, x in enumerate(train_df):\n    w = som.winner(x)\n    plot(w[0] + 0.5,\n         w[1] + 0.5,\n#          markers[y[i]],\n#          markeredgecolor = colors[y[i]],\n         markerfacecolor = 'None',\n         markersize = 10,\n         markeredgewidth = 2)\nshow()","f554ca46":"**Here, we observe most of the customers fall in 3rd cluster, while 2nd cluster contains the least amount of customers.**\n<\/br>\n**Note**: Since clustering is done on mulitple columns, we cannot visualise the data as only 2 dimensional data can be visualised. ","a74fc4dc":"**We can observe and validate few observations here:**\n<ol>\n    <li>Purchases,oneoffpurchases and investment purchases are highly correlated. That is most of the purchases are oneoffpurchases<\/li>\n    <li>People dont make full payments when the balance is high<\/li>\n    <li>Purchase frequency and cash advance frequency are inversely correlated. That is as the purchase frequency is high, the number of times cash is paid in advance is less and vice-versa<\/li>\n<\/ol>\n    ","b8e2c7c2":"# 2. Data Visualization :","f3082147":"**Imputing null values :**","cce8cf83":"**Lets try to visualise using two variables like BALANCE and PURCHASES:**","ce810b26":"There are values in **different numerical ranges** which will be normalized later.","ec346b14":"# 3.3 Some differences that we observe in K-Means and DBSCAN are:","f418a4c9":"Finding optimum number of clusters for grouping:","ea3bdfd3":"DBSCAN - Density-Based Spatial Clustering of Applications with Noise. Finds core samples of high density and expands clusters from them. Good for data which contains clusters of similar density. K-Means is not capable of creating clusters of arbitary shape. This is were DBSCAN is helpful.","0ad8ca88":"**We observe some of the following trends here:**\n<ol>\n    <li>As the credit limit increase, the balance also increases hence a linear relationship<\/li>\n    <li>As the number of purchases increases, the number of \"cash in advance\" transactions decreases<\/li>\n    <li>As the credit balance is low, the purchases, oneoffpurchases and installments purchases are less. Thus validating our assumption from univariate analysis<\/li>\n    <li>Purchases, oneoffpurchases and installment purchases are all related linearly<\/li>\n    <li>As the credit balance is low, the \"cash in advance\" transactions are less<\/li>\n<\/ol>","b77a5077":"Clusters formed using K-Means are more or less spherical or convex in shape and must have same feature size. But using DBSCAN, arbitrary shaped clusters can be formed.","eeefa4a3":"# 3.1 Normalizing the values:","6c1fe140":"# 3.2 K-MEANS :","4112ebd0":"# 3.4 Self Organizing Maps (SOM):","7f73446c":"We have removed all the null values from the dataset.","c3bac097":"# 2.2 Bivariate Analysis","4ff48fc8":"# 3.2 DBSCAN :","dd8866e4":"We observe a lot of outliers here, hence **deleting these outliers is not recommended**. We could have deleted the outlier points if the number of outliers are less. Here, we will avoid deleting the outlier records. What we can do here based on column description is to normalize the data and **handle the skewness using log tranformations or power transformers**.","f2893bc7":"# 1. Extracting data and cleaning:","b4853073":"Let's first identify the **distribution** of each column :","437c84be":"Here, using eps (minimum distance between two points) as 2 and minimum samples in a cluster as 6, we get 6 clusters with majority of them being in cluster 0","bcc48ec3":"**Cust_ID** is one column which we will not be needing for model building, so lets **drop** it:","d987fbe8":"There are a **few null values**, lets confirm that:","c58bb6aa":"# 4. Please upvote this notebook if you find it helpful :)","93a8674a":"K-means clustering is more efficient for large datasets and K-means clustering is also sensitive to the number of clusters specified. Whereas in case of DBSCAN, no need to specify the number of clusters manually.","98201f69":"Considering only BALANCE variable, we can choose k = 4","42a1a895":"**We observe the following trends here :**\n<ol>\n    <li>Most credit card holders have low credit limit and maintain credit balance below 7500<\/li>\n    <li>Variable such as Purchases, OneOffPurchases, installmentpurchases and cash advances also follow the same trend as credit balance. They could all be related. That is as the credit balance is low, the purchases are also low and so on<\/li>\n    <li>Most people either don't purchase anything or they purchase very frequently<\/li>\n    <li>People who purchase in installments is more than people who purchase in one-go<\/li>\n    <li>In the last 6 months, most people have made total payments below 10000, with the minimum payments below 5000<\/li>\n    <li>Finally, most of the credit card holders own a card for more than 12 months<\/li>\n<\/ol>\n<\/br>\n\n**We can validate some of these trends further in bivariate analysis.**","1af6f305":"**Lets try to visualise using two variables like BALANCE and PURCHASES:**","0404d3a5":"One more thing to observe here is minimum payments column also contains a lot of outliers, so lets impute the null values with median. You can also delete the records with null values and proceed since, the number of records with null values are small. There is no thumbrule for handling null values or outliers.","f313dcb3":"There are very few null values in credit limit and minimum payments column. Lets impute these values later.","3f5fa4dd":"K-means Clustering does not work well with outliers and noisy datasets whereas DBSCAN works well with outlier.","0d93ffad":"Here, points which are in darker space have low mean internueron distance, that is there are closer to the cluster centers and vice-versa.","e23e3c58":"# 2.1 Univariate Analysis :","35619d08":"We observe that till k = 6 the inter cluster distance decreases significantly. Post that it decreases slightly. Lets form 6 clusters and display them:","e62e9895":"# 3. Model Building :"}}