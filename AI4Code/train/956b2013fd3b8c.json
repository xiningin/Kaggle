{"cell_type":{"5cc697ae":"code","900c86cb":"code","a84f89d1":"code","c49b211d":"code","37817fa1":"code","c025c50e":"code","077c43dc":"code","bf4a44b4":"code","d811d9ca":"code","e6c07f6f":"code","9a0c7211":"code","5c2b95ed":"code","e5534612":"code","5845ee1b":"code","b0bf8dbc":"code","6c2b9f15":"code","610de59c":"code","01d182c6":"code","7e30d324":"code","efbda25c":"code","8a7b09ed":"code","c28ef239":"code","dba58889":"code","332c71f7":"code","8ea61b45":"code","68abf70b":"code","45aa0f56":"code","3fb4cec4":"code","c8d0ea42":"code","43f8af9b":"code","db4fd071":"code","07de484f":"code","99a0af5a":"code","d08653df":"code","4e19abf3":"code","d50e9016":"code","89d866ee":"code","6585aad4":"code","2c4c46f4":"code","2778c3d6":"code","66f09acf":"code","1b5cca51":"code","78eb32ea":"code","bd800afb":"code","eac1be87":"code","70a9c975":"code","a55db3e7":"code","95a66276":"code","1d488329":"code","85b7c34b":"code","1290e873":"code","0b54809d":"code","0adbf1e7":"code","339a64e9":"code","c1d129cc":"code","d9b64839":"code","a3825f2b":"code","78571e84":"code","0ee8ffc1":"code","a7fd6bb8":"code","cc86cac9":"code","a03528b9":"code","d5d3bf3f":"code","94142a19":"code","0736aafb":"code","11bb91dd":"code","a203c707":"code","8995976d":"code","e167db2b":"code","c1601688":"code","e0a63ae5":"code","43654c45":"code","77442fd8":"code","5994eb37":"code","14aaf371":"code","be0cec5b":"code","916ae703":"code","94f7a051":"code","898198fe":"code","5b15ce84":"code","6b765278":"code","013517d6":"code","d8aa3133":"code","89c1f8ae":"code","3626ec2c":"code","2a39f583":"code","56031cc6":"code","b60f5b8c":"code","0165430b":"code","beeb5bb4":"code","3c338033":"code","cd0139d7":"code","9b903cfa":"code","72851133":"code","d59fa2d8":"code","d3f81650":"code","448fa304":"code","ce55e8d8":"code","729fe2a9":"code","9cc2e50d":"code","b073b620":"code","2dba1dae":"code","010f911d":"code","a3b644d0":"code","2e3a9192":"code","5484a507":"code","ea690649":"code","10c86834":"code","f20f816b":"code","280230f1":"code","7993154d":"code","06ee9600":"code","0926b501":"code","bc7692a0":"code","ef89d52e":"code","8bc1a23c":"code","7dd79132":"code","e7d8278c":"code","d0533fef":"code","98e0eb8a":"code","fcbc3e07":"code","27507f4a":"markdown","07985a12":"markdown","c7f43586":"markdown","c803515d":"markdown","1199c0bb":"markdown","009f9111":"markdown","c65bb512":"markdown","ce295e38":"markdown","e561b247":"markdown","0a7d786b":"markdown","f7b9de26":"markdown","73c887f3":"markdown","322df82f":"markdown","e623e125":"markdown","c2127949":"markdown","159903b4":"markdown","cb1476ab":"markdown","0bdf217c":"markdown","3b23a358":"markdown","3567d82a":"markdown","7a229b98":"markdown","030c1709":"markdown","ced456bc":"markdown","815cac36":"markdown","973fcba6":"markdown","e04cca4a":"markdown","b3b47826":"markdown","c738eaed":"markdown","d00aa7cf":"markdown","9f1e736c":"markdown","ba809627":"markdown","0df30c36":"markdown","39499c9b":"markdown","a38308fc":"markdown","08d4bfc6":"markdown","dee0b4df":"markdown","d7d31c8b":"markdown","80311f3c":"markdown","19ae2c1a":"markdown","6cdb163a":"markdown","d53e40ae":"markdown","b4f2ccb6":"markdown","d89c9954":"markdown","621140e6":"markdown","c4bbb91d":"markdown","f0209111":"markdown","e5bad22b":"markdown","b54c4486":"markdown","e1c8c002":"markdown"},"source":{"5cc697ae":"# Packages\n\nimport pandas as pd\nimport numpy as np\nimport string\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\n\n\n# Visualisation\n\nimport matplotlib.pyplot as plt \n%matplotlib inline\nimport seaborn as sns\nfrom wordcloud import WordCloud, STOPWORDS\n\n\n# Model Building\n\n    #classifiers\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import AdaBoostClassifier\n\n    #vectorizers\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n    #training features\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.model_selection import GridSearchCV\n\n    #performance measures\nfrom sklearn.metrics import accuracy_score,log_loss\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.metrics import make_scorer\n\n    #filter future warnings\n#two futre warnings occured multiple times when running cross validation and GridSearchCV have been removed\n#FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n#FutureWarning: The default value of cv will change from 3 to 5 in version 0.22\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)","900c86cb":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","a84f89d1":"# view the data\ntrain.head()","c49b211d":"test.head()","37817fa1":"print('Number of rows and columns in train data:{}' .format(train.shape))\nprint('Number of rows and columns in test data:{}' .format(test.shape))","c025c50e":"train.isnull().sum()","077c43dc":"test.isnull().sum()","bf4a44b4":"type_sum = train.groupby(['type']).count()\ntype_sum.sort_values('posts', ascending=False, inplace=True)\ntype_sum","d811d9ca":"train['word_count'] = train['posts'].apply(lambda x: len(str(x).split(\" \")))\nword_count = train.groupby('type').sum()\nword_count.sort_values('word_count', ascending=False, inplace=True)\nword_count","e6c07f6f":"#drop word_count column\ntrain = train.drop(['word_count'], axis=1)","9a0c7211":"dim = (15.0, 4.0)\nfig, ax = plt.subplots(figsize=dim)\ncmrmap = sns.color_palette('CMRmap', 16)\nsns.set_palette(cmrmap)\nsns.countplot(x='type', data=train,\n              order=['ENFJ', 'ENFP', 'ENTJ', 'ENTP', 'ESFJ', 'ESFP', 'ESTJ',\n                     'ESTP', 'INFJ', 'INFP', 'INTJ', 'INTP', 'ISFJ', 'ISFP',\n                     'ISTJ', 'ISTP'])\nplt.title('Distribution of Myers-Briggs Types in the Dataset', fontsize=16)\nplt.xlabel('Personality Type')\nplt.ylabel('Count of Posts')\nplt.xticks(fontsize=12)\nplt.yticks(fontsize=12)\nplt.show()","5c2b95ed":"# Create a binary column for each of the 4 dimension types\ntrain['Mind'] = train['type'].map(lambda x: 'Extroverted'\n                                  if x[0] == 'E' else 'Introverted')\ntrain['Energy'] = train['type'].map(lambda x: 'Intuitive'\n                                    if x[1] == 'N' else 'Sensing')\ntrain['Nature'] = train['type'].map(lambda x: 'Thinking'\n                                    if x[2] == 'T' else 'Feeling')\ntrain['Tactics'] = train['type'].map(lambda x: 'Judging'\n                                     if x[3] == 'J' else 'Perceiving')","e5534612":"# Countplot of the Introverted - Extroverted variable\nIEcolors = sns.xkcd_palette(['red', 'soft pink'])\nsns.set_palette(IEcolors)\nsns.countplot(x='Mind', data=train, order=['Introverted', 'Extroverted'])\nplt.ylim(0, 8000)\nplt.xticks(fontsize=12)\nplt.yticks(fontsize=12)\nplt.xlabel('Introverted vs Extroverted')\nplt.ylabel('Count of each Personality Type')\nplt.title('Introversion vs. Extroversion', fontsize=14)\nplt.show()\n","5845ee1b":"# Start with one review:\ndef generate_wordcloud(text, title):\n    # Create and generate a word cloud image:\n    wordcloud = WordCloud(background_color='white').generate(text)\n\n    # Display the generated image:\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis('off')\n    plt.title(title, fontsize=40)\n    plt.show()","b0bf8dbc":"# Group together posts written by those under the mind variable\nwords_of_mind = train.groupby('Mind')['posts'].apply(' '.join).reset_index()","6c2b9f15":"for i, t in enumerate(words_of_mind['Mind']):\n    text = words_of_mind.iloc[i,1]\n    generate_wordcloud(text, t)","610de59c":"# Countplot of the Intuitive - Sensing variable\nNScolors = sns.xkcd_palette(['blue', 'light blue'])\nsns.set_palette(NScolors)\nsns.countplot(x='Energy', data=train, order=['Intuitive', 'Sensing'])\nplt.title('Intuitive vs. Sensing', fontsize=14)\nplt.ylim(0, 8000)\nplt.xticks(fontsize=12)\nplt.yticks(fontsize=12)\nplt.show()","01d182c6":"words_of_energy = train.groupby('Energy')['posts'].apply(' '.join).reset_index()\nfor i, t in enumerate(words_of_energy['Energy']):\n    text = words_of_energy.iloc[i, 1]\n    generate_wordcloud(text, t)","7e30d324":"# Countplot of the Tinking - Feeling variable\nTFcolors = sns.xkcd_palette(['green', 'pale green'])\nsns.set_palette(TFcolors)\nsns.countplot(x='Nature', data=train, order=['Thinking', 'Feeling'])\nplt.title('Thinking vs. Feeling', fontsize=14)\nplt.ylim(0, 8000)\nplt.xticks(fontsize=12)\nplt.yticks(fontsize=12)\nplt.show()","efbda25c":"words_of_nature = train.groupby('Nature')['posts'].apply(' '.join).reset_index()\nfor i, t in enumerate(words_of_nature['Nature']):\n    text = words_of_nature.iloc[i, 1]\n    generate_wordcloud(text, t)","8a7b09ed":"# Countplot of Judging - Perceiving\nJPcolors = sns.xkcd_palette(['purple', 'lavender'])\nsns.set_palette(JPcolors)\nsns.countplot(x='Tactics', data=train, order=['Judging', 'Perceiving'])\nplt.title('Judging vs. Perceiving', fontsize=14)\nplt.ylim(0, 8000)\nplt.xticks(fontsize=12)\nplt.yticks(fontsize=12)\nplt.show()","c28ef239":"words_of_tactics = train.groupby('Tactics')['posts'].apply(' '.join).reset_index()\nfor i, t in enumerate(words_of_tactics['Tactics']):\n    text = words_of_tactics.iloc[i, 1]\n    generate_wordcloud(text, t)","dba58889":"def remove_delimiters (post):\n    new = post.replace('|||',' ')\n    return ' '.join(new.split())\n\ntrain['posts'] = train['posts'].apply(remove_delimiters)\ntest['posts'] = test['posts'].apply(remove_delimiters)","332c71f7":"## Remove urls\npattern_url = r'http[s]?:\/\/(?:[A-Za-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9A-Fa-f][0-9A-Fa-f]))+'\nsubs_url = r'url-web'\n\n#apply to train set\ntrain['posts'] = train['posts'].replace(to_replace = pattern_url, value = subs_url, regex = True)\n\n#apply to test set\ntest['posts'] = test['posts'].replace(to_replace = pattern_url, value = subs_url, regex = True)","8ea61b45":"train['posts'] = train['posts'].str.lower()\n\ntest['posts'] = test['posts'].str.lower()","68abf70b":"#Remove punctuation & numbers\ndef remove_punctuation(post):\n    punc_numbers = string.punctuation + '0123456789'\n    return ''.join([l for l in post if l not in punc_numbers])\n\ntrain['posts'] = train['posts'].apply(remove_punctuation)\n\ntest['posts'] = test['posts'].apply(remove_punctuation)","45aa0f56":"train.head()","3fb4cec4":"# Lematise posts\nlemmatizer = WordNetLemmatizer()\ntrain['lemma'] = [' '.join([lemmatizer.lemmatize(word) for word in text.split(' ')])for text in train['posts']]\ntest['lemma'] = [' '.join([lemmatizer.lemmatize(word) for word in text.split(' ')])for text in test['posts']]","c8d0ea42":"train.head()","43f8af9b":"#Check for stopwords train\nstop = stopwords.words('english')\ntrain['stopwords'] = train['lemma'].apply(lambda x: len([x for x in x.split() if x in stop]))\ntrain[['lemma','stopwords']].head()","db4fd071":"#Check for stopwords test\nstop = stopwords.words('english')\ntest['stopwords'] = test['lemma'].apply(lambda x: len([x for x in x.split() if x in stop]))\ntest[['lemma','stopwords']].head()","07de484f":"def remove_stop_words(word):\n    if word not in stop:\n        return word\n    else:\n        return ''","99a0af5a":"test['lemma_no_stop'] = [' '.join([remove_stop_words(word) for word in text.split(' ')])for text in test['lemma']]","d08653df":"test.head()","4e19abf3":"#Create binary classes for each of the personality characteristics\ntrain['E'] = train['type'].apply(lambda x: x[0] == 'E').astype('int')\ntrain['N'] = train['type'].apply(lambda x: x[1] == 'N').astype('int')\ntrain['T'] = train['type'].apply(lambda x: x[2] == 'T').astype('int')\ntrain['J'] = train['type'].apply(lambda x: x[3] == 'J').astype('int')","d50e9016":"train.head()","89d866ee":"mind_df = train[['lemma','E']]","6585aad4":"vect_mind = TfidfVectorizer(lowercase=True, \n                            stop_words='english', \n                            max_features=250,\n                            min_df=4,\n                            max_df=0.5\n                           )","2c4c46f4":"vect_mind.fit(mind_df['lemma'])\nX_count_mind = vect_mind.transform(mind_df['lemma'])","2778c3d6":"X_count_mind.shape","66f09acf":"vect_mind.get_feature_names()","1b5cca51":"X = X_count_mind\ny = mind_df['E']\nX_train, X_test, y_train, y_test = train_test_split(X, \n                                                    y,\n                                                    test_size =0.3,\n                                                   random_state = 42)","78eb32ea":"def scoring_function_log_loss(y_test, y_pred_test):\n    return log_loss(y_test, y_pred_test)","bd800afb":"score_log_loss = make_scorer(scoring_function_log_loss, greater_is_better = False)","eac1be87":"def tune_LogReg_model(X_train, y_train): \n    C_list = [0.001, 0.01, 0.1, 0.5, 0.75, 1, 5, 10, 25, 100]\n    penalty_list = ['l1','l2']\n\n    score = make_scorer(scoring_function_log_loss, greater_is_better = False)\n    \n    logreg = LogisticRegression()\n    \n    parameters = {'C':C_list,\n                  'penalty': penalty_list}\n    tune = GridSearchCV(logreg, parameters, scoring = score)\n    tune.fit(X_train,y_train)\n    \n    return tune","70a9c975":"best_mind_model = tune_LogReg_model(X_train, y_train)","a55db3e7":"best_mind_model.best_params_","95a66276":"mind_model = LogisticRegression(C=best_mind_model.best_params_['C'], penalty = best_mind_model.best_params_['penalty'])\nmind_model.fit(X_train, y_train)","1d488329":"y_pred_train = mind_model.predict(X_train)","85b7c34b":"accuracy_score(y_train, y_pred_train)","1290e873":"y_pred_test = mind_model.predict(X_test)","0b54809d":"accuracy_score(y_test, y_pred_test)","0adbf1e7":"confusion_matrix(y_train, y_pred_train)","339a64e9":"confusion_matrix(y_test, y_pred_test)","c1d129cc":"print(classification_report(y_train, y_pred_train))","d9b64839":"print(classification_report(y_test, y_pred_test))","a3825f2b":"log_loss(y_train, y_pred_train)","78571e84":"log_loss(y_test, y_pred_test)","0ee8ffc1":"mind_log_loss = cross_val_score(mind_model, X, y, scoring=score_log_loss,cv=4,)\nprint('Log Loss %2f' %(-1 * mind_log_loss.mean()))\n\nmind_acc = cross_val_score(mind_model, X, y, scoring='accuracy',cv=4,)\nprint('Accuracy %2f' %(mind_acc.mean()))","a7fd6bb8":"energy_df = train[['lemma','N']]","cc86cac9":"vect_energy = TfidfVectorizer(lowercase=True, \n                            stop_words='english', \n                            max_features=195,\n                            min_df=4,\n                            max_df=0.5\n                           )\nvect_energy.fit(energy_df['lemma'])\nX_count_energy = vect_energy.transform(energy_df['lemma'])\n\nX_count_energy.shape\n\nvect_energy.get_feature_names()","a03528b9":"X = X_count_energy\ny = energy_df['N']\nX_train, X_test, y_train, y_test = train_test_split(X, \n                                                    y,\n                                                    test_size =0.3,\n                                                   random_state = 42)","d5d3bf3f":"best_energy_model = tune_LogReg_model(X_train, y_train)","94142a19":"best_energy_model.best_params_","0736aafb":"energy_model = LogisticRegression(C=best_energy_model.best_params_['C'], penalty = best_energy_model.best_params_['penalty'])\nenergy_model.fit(X_train, y_train)","11bb91dd":"y_pred_train = energy_model.predict(X_train)\n\naccuracy_score(y_train, y_pred_train)","a203c707":"y_pred_test = energy_model.predict(X_test)\n\naccuracy_score(y_test, y_pred_test)","8995976d":"confusion_matrix(y_train, y_pred_train)","e167db2b":"confusion_matrix(y_test, y_pred_test)","c1601688":"print(classification_report(y_train, y_pred_train))","e0a63ae5":"print(classification_report(y_test, y_pred_test))","43654c45":"log_loss(y_train, y_pred_train)","77442fd8":"log_loss(y_test, y_pred_test)","5994eb37":"energy_log_loss = cross_val_score(energy_model, X, y, scoring=score_log_loss,cv=4)\nprint('Log Loss %2f' %(-1 * energy_log_loss.mean()))\n\nenergy_acc = cross_val_score(energy_model, X, y, scoring='accuracy',cv=4,)\nprint('Accuracy %2f' %(energy_acc.mean()))","14aaf371":"nature_df = train[['lemma','T']]","be0cec5b":"vect_nature = TfidfVectorizer(lowercase=True, \n                            stop_words='english', \n                            max_features=3900,\n                            min_df=4,\n                            max_df=0.5\n                            #ngram_range=(3,3)\n                           )\nvect_nature.fit(nature_df['lemma'])\nX_count_nature = vect_nature.transform(nature_df['lemma'])\n\nX_count_nature.shape\n\nvect_nature.get_feature_names()","916ae703":"X = X_count_nature\ny = nature_df['T']\nX_train, X_test, y_train, y_test = train_test_split(X, \n                                                    y,\n                                                    test_size =0.3,\n                                                   random_state = 42)","94f7a051":"best_nature_model = tune_LogReg_model(X_train, y_train)","898198fe":"best_nature_model.best_params_","5b15ce84":"nature_model = LogisticRegression(C=best_nature_model.best_params_['C'], penalty = best_nature_model.best_params_['penalty'])\nnature_model.fit(X_train, y_train)","6b765278":"y_pred_train = nature_model.predict(X_train)\n\naccuracy_score(y_train, y_pred_train)","013517d6":"y_pred_test = nature_model.predict(X_test)\n\naccuracy_score(y_test, y_pred_test)","d8aa3133":"confusion_matrix(y_train, y_pred_train)","89c1f8ae":"confusion_matrix(y_test, y_pred_test)","3626ec2c":"print(classification_report(y_train, y_pred_train))","2a39f583":"print(classification_report(y_test, y_pred_test))","56031cc6":"log_loss(y_train, y_pred_train)","b60f5b8c":"log_loss(y_test, y_pred_test)","0165430b":"nature_log_loss = cross_val_score(nature_model, X, y, scoring=score_log_loss,cv=4,)\nprint('Log Loss %2f' %(-1 * nature_log_loss.mean()))\n\nnature_acc = cross_val_score(nature_model, X, y, scoring='accuracy',cv=4,)\nprint('Accuracy %2f' %(nature_acc.mean()))","beeb5bb4":"tactics_df = train[['lemma','J']]","3c338033":"vect_tactics = TfidfVectorizer(lowercase=True, \n                            stop_words='english', \n                            max_features=260,\n                            min_df=4,\n                            max_df=0.5\n                           )\nvect_tactics.fit(tactics_df['lemma'])\nX_count_tactics = vect_tactics.transform(tactics_df['lemma'])\n\nX_count_tactics.shape\n\nvect_tactics.get_feature_names()","cd0139d7":"X = X_count_tactics\ny = tactics_df['J']\nX_train, X_test, y_train, y_test = train_test_split(X, \n                                                    y,\n                                                    test_size =0.3,\n                                                   random_state = 42)","9b903cfa":"best_tactics_model = tune_LogReg_model(X_train, y_train)","72851133":"best_tactics_model.best_params_","d59fa2d8":"tactics_model = LogisticRegression(C=best_tactics_model.best_params_['C'], penalty = best_tactics_model.best_params_['penalty'])\ntactics_model.fit(X_train, y_train)","d3f81650":"y_pred_train = tactics_model.predict(X_train)\n\naccuracy_score(y_train, y_pred_train)","448fa304":"y_pred_test = tactics_model.predict(X_test)\n\naccuracy_score(y_test, y_pred_test)","ce55e8d8":"confusion_matrix(y_train, y_pred_train)","729fe2a9":"confusion_matrix(y_test, y_pred_test)","9cc2e50d":"print(classification_report(y_train, y_pred_train))","b073b620":"print(classification_report(y_test, y_pred_test))","2dba1dae":"log_loss(y_train, y_pred_train)","010f911d":"log_loss(y_test, y_pred_test)","a3b644d0":"tactics_log_loss = cross_val_score(tactics_model, X, y, scoring=score_log_loss,cv=4,)\nprint('Log Loss %2f' %(-1 * tactics_log_loss.mean()))\n\ntactics_acc = cross_val_score(tactics_model, X, y, scoring='accuracy',cv=4,)\nprint('Accuracy %2f' %(tactics_acc.mean()))","2e3a9192":"test.head()","5484a507":"pred_mind_count = vect_mind.transform(test['lemma_no_stop'])\n\npred_mind_count.shape\n\nX = X_count_mind\ny = mind_df['E']\n\nfinal_mind_model = mind_model\nfinal_mind_model.fit(X, y)\n\nfinal_mind_predictions = final_mind_model.predict(pred_mind_count)\n\ntest['E_pred'] = final_mind_predictions\n\ntest.head()\n\npred_mind_df = test[['id', 'E_pred']]\n\npred_mind_df.head(10)\n\npred_mind_df.columns\n\npred_mind_df['E_pred'].value_counts().plot(kind = 'bar',color = ['darkblue','dodgerblue'])\n\n#pred_mind_df\n\n\n\n\nplt.show()\n\npred_mind_df.head(10)","ea690649":"pred_energy_count = vect_energy.transform(test['lemma_no_stop'])\n\npred_energy_count.shape\n\nX = X_count_energy\ny = energy_df['N']\n\nfinal_energy_model = energy_model\nfinal_energy_model.fit(X, y)\n\nfinal_energy_predictions = final_energy_model.predict(pred_energy_count)\n\ntest['N_pred'] = final_energy_predictions\n\npred_energy_df = test[['id', 'N_pred']]\n\npred_energy_df['N_pred'].value_counts().plot(kind = 'bar', color = ['purple','violet'])\nplt.show()\n\npred_energy_df.head(10)","10c86834":"pred_nature_count = vect_nature.transform(test['lemma_no_stop'])\n\npred_nature_count.shape\n\nX = X_count_nature\ny = nature_df['T']\n\nfinal_nature_model = nature_model\nfinal_nature_model.fit(X, y)\n\nfinal_nature_predictions = final_nature_model.predict(pred_nature_count)\n\ntest['T_pred'] = final_nature_predictions\n\npred_nature_df = test[['id', 'T_pred']]\n\npred_nature_df['T_pred'].value_counts().plot(kind = 'bar', color = ['darkgreen','yellowgreen'])\nplt.show()\n\npred_nature_df.head(10)","f20f816b":"pred_tactics_count = vect_tactics.transform(test['lemma_no_stop'])\n\npred_tactics_count.shape\n\nX = X_count_tactics\ny = tactics_df['J']\n\nfinal_tactics_model = tactics_model\nfinal_tactics_model.fit(X, y)\n\nfinal_tactics_predictions = final_tactics_model.predict(pred_tactics_count)\n\ntest['J_pred'] = final_tactics_predictions\n\npred_tactics_df = test[['id', 'J_pred']]\n\npred_tactics_df['J_pred'].value_counts().plot(kind = 'bar', color = ['black','grey'])\n\n\nplt.show()\n\npred_tactics_df.head(10)","280230f1":"my_submission = pd.merge(pred_mind_df[['id','E_pred']], pred_energy_df[['id','N_pred']], how ='inner', on ='id') \nmy_submission = pd.merge(my_submission[['id','E_pred', 'N_pred']], pred_nature_df[['id','T_pred']], how ='inner', on ='id')\nmy_submission = pd.merge(my_submission[['id','E_pred', 'N_pred','T_pred']], pred_tactics_df[['id','J_pred']], how ='inner', on ='id') ","7993154d":"my_submission.head(10)","06ee9600":"my_submission.rename(columns={'id':'id',\n                            'E_pred':'mind',\n                            'N_pred': 'energy',\n                            'T_pred': 'nature',\n                            'J_pred': 'tactics'\n                             }, \n                 inplace=True)\n\nmy_submission.head()","0926b501":"my_submission.to_csv('Classification_project_final_submission.csv', index=False)","bc7692a0":"# Create column for the predictions of each of the 4 chracteristics\nmy_submission['Mind Pred'] = my_submission['mind'].map(lambda x: 'E' if x == 1 else 'I')\nmy_submission['Energy Pred'] = my_submission['energy'].map(lambda x: 'N' if x == 1 else 'S')\nmy_submission['Nature Pred'] = my_submission['nature'].map(lambda x: 'T' if x == 1 else 'F')\nmy_submission['Tactics Pred'] = my_submission['tactics'].map(lambda x: 'J' if x == 1 else 'P')","ef89d52e":"my_submission.head()","8bc1a23c":"my_submission['Personality Pred'] = my_submission['Mind Pred'] + my_submission['Energy Pred'] + my_submission['Nature Pred']+ my_submission['Tactics Pred']","7dd79132":"my_submission[['id','Personality Pred']].head()","e7d8278c":"mbti_type = ['ENFJ','ENFP','ENTJ','ENTP','ESFJ','ESFP','ESTJ','ESTP','INFJ','INFP','INTJ','INTP','ISFJ','ISFP','ISTJ','ISTP']\nglobal_p = [ 2.5, 8.1, 1.8, 3.2, 12.3, 8.5, 8.7, 4.3, 1.5, 4.4, 2.1, 3.3, 13.8, 8.8, 11.6, 5.4]","d0533fef":"global_types = {'Type':mbti_type,'Percentage':global_p}","98e0eb8a":"global_df = pd.DataFrame(global_types)\nglobal_df","fcbc3e07":"#view posts count of each personality type\n# Countplot of the 16 personality types in the dataset\ndims1 = (15.0, 4.0)\nfig, ax = plt.subplots(figsize=dims1)\ncmrmap = sns.color_palette('CMRmap', 16)\nsns.set_palette(cmrmap)\nsns.countplot(x='type', data=train,\\\n              order=['ENFJ','ENFP','ENTJ','ENTP','ESFJ','ESFP','ESTJ','ESTP',\\\n                    'INFJ','INFP','INTJ','INTP','ISFJ','ISFP','ISTJ','ISTP'])\nplt.title('Distribution of Myers-Briggs Types in the test Dataset', fontsize=16)\nplt.xlabel('Personality Type')\nplt.ylabel('Count of Posts')\nplt.xticks(fontsize=12)\nplt.yticks(fontsize=12)\nplt.show()\n\n\n\n#view posts count of each personality type\n# Countplot of the 16 personality types in the dataset\ndims1 = (15.0, 4.0)\nfig, ax = plt.subplots(figsize=dims1)\ncmrmap = sns.color_palette(\"CMRmap\", 16)\nsns.set_palette(cmrmap)\nsns.countplot(x='Personality Pred', data=my_submission,\\\n              order=['ENFJ','ENFP','ENTJ','ENTP','ESFJ','ESFP','ESTJ','ESTP',\\\n                    'INFJ','INFP','INTJ','INTP','ISFJ','ISFP','ISTJ','ISTP'])\nplt.title('Distribution of Myers-Briggs Type Predictions', fontsize=16)\nplt.xlabel('Personality Type')\nplt.ylabel('Count of Posts')\nplt.xticks(fontsize=12)\nplt.yticks(fontsize=12)\nplt.show()\n\n\n\n#view posts count of each personality type\n# Countplot of the 16 personality types in the dataset\ndims1 = (15.0, 4.0)\nfig, ax = plt.subplots(figsize=dims1)\ncmrmap = sns.color_palette(\"CMRmap\", 16)\nsns.set_palette(cmrmap)\nsns.barplot(x='Type', y='Percentage', data=global_df, order=['ENFJ','ENFP','ENTJ','ENTP','ESFJ','ESFP','ESTJ','ESTP',\\\n                   'INFJ','INFP','INTJ','INTP','ISFJ','ISFP','ISTJ','ISTP'])\nplt.title('Approximate Distribution of Global Myers-Briggs Personality Types', fontsize=16)\nplt.xlabel('Personality Type')\nplt.ylabel('Percentage')\nplt.xticks(fontsize=12)\nplt.yticks(fontsize=12)\nplt.show()\n\n\n","27507f4a":"### Tactics","07985a12":"# Data Preprocessing","c7f43586":"# Overview","c803515d":"There are more introverts who have written posts compare to extroverts. Now let's assess the most common words spoken by each.","1199c0bb":"### Predicting mind","009f9111":"#### Is there any missing data in any of the columns?","c65bb512":"<img  src=\"https:\/\/cdn.shopify.com\/s\/files\/1\/0100\/5392\/articles\/Mouthpiece_VdayMeyersBriggs1.progressive.jpg?v=1549578121\" data-iml=\"1561547447318\" style='width:600px'>","ce295e38":"### Energy model","e561b247":"### Energy","0a7d786b":"## EDSA CPT Team Classi5","f7b9de26":"### Nature","73c887f3":"# Exploratory Data Analysis (EDA)","322df82f":"### Package Imports","e623e125":"The 'ES' personality types have the least posts, whilst the 'IN' personalities have the most posts. This suggests that the data is imbalanced.","c2127949":"Each person's posts were made up of 50 individual posts which were separated by  the string '|||'. There was an attempt to split the data,converting each post into the 50 individual posts, with the goal of increasing the size of the dataset (especially those personalities with very few posts). However this method reduced the accuracy  of the model and was abandoned. We therefore removed the delimiters as part of the preprocessing. ","159903b4":"# Model Building\n\nThis require classification machine learning techniques.\n\nInitially a multi-class classification approach was taken, in which the model would use the input data to predict each row of input as one of the 16 personality types. The results were not very good and this lead to the change in approach where each of the individual personality characteristics 'mind', 'energy', 'nature' & 'tactics' were classified separately as binary classification problems.\n\nVarious classification techniques were applied to the data\n- Logistic Regression\n- Multinomial Naive Bayes\n- Support Vector Classifier (SVC)\n- Random Forrest Classifier\n\nTwo vectorizers were used\n- CountVectorizer\n- TfidfVectorizer\n\nThe code used for Logistic Regression with TfidfVectorizer follows in this notebook.\n\n","cb1476ab":"Vectorise the words with TfidfVectorizer\n\nSeveral iterations of parameters were applied for each characteritic.\n\nFor Mind classification the best log loss result was achieved with below parameters\n- max_features = 250 (this parmeter that was adjusted the most when tuning)\n- min_df = 4 (had very little effect, when using the max_features the lower frequency words aren't selected)\n- max_df = 0.5 (consistently produced the better results with a 0.5 setting)","0bdf217c":"#### Convert words to lowercase","3b23a358":"#### Removing delimeters","3567d82a":"#### How many total words were written by each personality type?","7a229b98":"The Myers-Briggs Type Indicator (MBTI) is a psychological assessment tool used to classify people into one of 16 different personality types.  The classification system consists of a four-letter code based on four dimensions, where each letter in the code refers to the predominant trait in each dimension. The four dimensions are:\n- **Mind: Introverted (I)** or **Extraverted (E)** which describes the different attitudes people use to direct their energy (i.e. the \"inner world\" vs. one's \"outer world\").\n- **Energy: Sensing (S)** or **Intuitive (N)** which describes people's method of processing informaton (i.e. paying more attention to the patterns and possibilities seen in the information received vs. information that comes in through the five senses)\n- **Nature: Feeling (F)** or **Thinking (T)** which describes people's method for making decisions (i.e. putting more weight on objective principles and impersonal facts vs. personal concerns and the people involved).\n- **Tactics: Perceiving (P)** or **Judging (J)** which describes people's orientation to the outside world and the behaviors one exhibits (i.e. preferring a structured and decided lifestyle vs. a more flexible and adaptive lifestyle)\n\n\nThe letters associated with an individual's preferences are combined to get the Myers Briggs personality type.\nThe goal of the competition is to build and train a model that is capable of predicting labels for each of the four MBTI variables.","030c1709":"#### Remove punctuation and numbers ","ced456bc":"As observed from above, the data used to train and test the model does not represent the overall amount of personalities worldwide. Our data has more 'INFP' personality types whilst the worldwide data has the 'ISFJ' personality being more prevalent. The personality type that is smaller in number worldwide is 'INFJ' but according to our data the 'ESFJ' and 'ESTJ' personalities are the ones that are fewer. This is possibly due to the personalities that used the Personality Cafe website forum which shows that the individuals used to collect the data had a low amount of extroverted personalities(especially 'ES' personalities) and had a high amount of introverted personalities(especially 'IN' personalities)","815cac36":"While the \"INFP\" personality wrote the most posts and  wrote the most words in their posts.The personality that wrote the least posts was 'ESTJ' and the personality with the least written words was 'ESFP'. ","973fcba6":"Stopwords tend to have a negative influence over the accuracy of a model ,therefore these will need to be removed. This will be done by vectorizer in the model building section, as CountVectorizer and TfidfVectorizer have the ability of removing stopwords.\nWe remove the stopwords from the test data to speed up the anaylysis when transforming the test data to the vectorizer.","e04cca4a":"### Nature model","b3b47826":"# Conclusion","c738eaed":"# Personality Profile Prediction\n---","d00aa7cf":"#### How many posts were written by each personality type?","9f1e736c":"The 'Nature' variable containing 'Thinking - Feeling' is more balanced compared to other variables. \n\nThe word that seems to appear the most among all the personalities is the word 'think'.","ba809627":"The data has to be preprocessed with the purpose of removing noise, which negatively affects the accuracy of the model. Both the train and test data were preprocessed.","0df30c36":"1. Introduction to the data and outline goal of the competition\n2. Importing the packages and the data\n3. Data exploration\n4. Data preprocessing\n5. Building the Model\n6. Prediction on test data\n6. Conclusion","39499c9b":"### Predicting nature","a38308fc":"# Introduction","08d4bfc6":"#### Removing URLs","dee0b4df":"## Import Data","d7d31c8b":"The words in the dataset were also lemmatized.Lemmatization  returns the base or dictionary form of a word, which is known as the lemma.Lemmatization is considered to be better than stemming because lemmatization returns an actual word of the language, and is used when it is necessary to get valid words.","80311f3c":"### Mind","19ae2c1a":"Words were converted to lowercase in order to remove noise from capitalisation and to avoid having multiple copies of the same words(eg.'People vs people')(https:\/\/www.analyticsvidhya.com\/blog\/2018\/02\/the-different-methods-deal-text-data-predictive-python\/)","6cdb163a":"# Predicting on test data\n\nThe 4 characteristic models developed in the previous section where then applied to the full train dataset. The models were fit and then 4 charachteristics were predicted for each id in the test dataset. ","d53e40ae":"#### Lemmatization","b4f2ccb6":"### Predicting energy","d89c9954":"Urls don't add any value when analyzing text therefore they were removed(https:\/\/towardsdatascience.com\/understanding-feature-engineering-part-3-traditional-methods-for-text-data-f6f7d70acd41).","621140e6":"### Model conclusions\n\nOf the models that were applied the best result was achieved with LogisticRegression with TfidfVectorizer. SVC with CountVectorizer a very close second. MutlinomialNB & RandomForest performance was poor in comparison to these two.\n\n| Model               | Mind Log Loss | Energy Log Loss | Nature Log Loss | Tactics Log Loss | Average Log Loss | Kaggle Score |\n|---------------------|---------------|-----------------|-----------------|------------------|------------------|--------------|\n| Logistic Regression |     4.87      |      3.59       |      4.89       |       6.56       |       4.98       |      5.01      |\n| SVC                 |     4.87      |      4.28       |      5.52       |       6.56       |       5.31       |      5.18    |\n| Random Forest       |     6.39      |      4.46       |      9.30       |      10.65       |       7.70       |      7.91      |\n| Multinomial NB      |     7.59      |      4.54       |      6.67       |       9.73       |       7.13       |      6.42      |","c4bbb91d":"### Tactics model","f0209111":"### Predicting tactics","e5bad22b":"#### Distribution of the Personality Types","b54c4486":"Punctuation doesn\u2019t add any extra information to the text data therefore removing it will help reduce the size of the training data.","e1c8c002":"#### Are there any stopwords in the text?"}}