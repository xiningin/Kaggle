{"cell_type":{"50449e0a":"code","ee8ada04":"code","1da851f0":"code","5456a893":"code","4d20f82a":"code","20cdb3e1":"code","19b5d0a3":"code","55e0188a":"code","f7a2c9de":"code","b96b5817":"code","40c3b184":"code","5d8c3ff3":"code","93854716":"code","5929c0ad":"code","7b7aa3ea":"markdown","354912a0":"markdown","5d8792de":"markdown"},"source":{"50449e0a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\n\n#Ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ee8ada04":"df = pd.read_csv('..\/input\/arabic-article-headline-generation\/dataset\/out.csv',delimiter=',', encoding='utf8')\ndf.head()","1da851f0":"!pip install qalsadi","5456a893":"import qalsadi.lemmatizer","4d20f82a":"df.isnull().sum()","20cdb3e1":"#Code by https:\/\/pypi.org\/project\/qalsadi\/\n\n#Developper: Taha Zerrouki: http:\/\/tahadz.com taha dot zerrouki at gmail dot com\n\ntext = u\"\"\"\u0647\u0644 \u062a\u062d\u062a\u0627\u062c \u0625\u0644\u0649 \u062a\u0631\u062c\u0645\u0629 \u0643\u064a \u062a\u0641\u0647\u0645 \u062e\u0637\u0627\u0628 \u0627\u0644\u0645\u0644\u0643\u061f \u0627\u0644\u0644\u063a\u0629 \"\u0627\u0644\u0643\u0644\u0627\u0633\u064a\u0643\u064a\u0629\" (\u0627\u0644\u0641\u0635\u062d\u0649) \u0645\u0648\u062c\u0648\u062f\u0629 \u0641\u064a \u0643\u0644 \u0627\u0644\u0644\u063a\u0627\u062a \u0648\u0643\u0630\u0644\u0643 \u0627\u0644\u0644\u063a\u0629 \"\u0627\u0644\u062f\u0627\u0631\u062c\u0629\" .. \u0627\u0644\u0641\u0631\u0646\u0633\u064a\u0629 \u0627\u0644\u062a\u064a \u0646\u062f\u0631\u0633 \u0641\u064a \u0627\u0644\u0645\u062f\u0631\u0633\u0629 \u0644\u064a\u0633\u062a \u0627\u0644\u0641\u0631\u0646\u0633\u064a\u0629 \u0627\u0644\u062a\u064a \u064a\u0633\u062a\u062e\u062f\u0645\u0647\u0627 \u0627\u0644\u0646\u0627\u0633 \u0641\u064a \u0634\u0648\u0627\u0631\u0639 \u0628\u0627\u0631\u064a\u0633 .. \u0648\u0645\u0644\u0643\u0629 \u0628\u0631\u064a\u0637\u0627\u0646\u064a\u0627 \u0644\u0627 \u062a\u062e\u0637\u0628 \u0628\u0644\u063a\u0629 \u0634\u0648\u0627\u0631\u0639 \u0644\u0646\u062f\u0646 .. \u0644\u0643\u0644 \u0645\u0642\u0627\u0645 \u0645\u0642\u0627\u0644\"\"\"\nlemmer = qalsadi.lemmatizer.Lemmatizer()\n# lemmatize a word\nlemmer.lemmatize(\"\u064a\u062d\u062a\u0627\u062c\")\n      \n# lemmatize a word with a specific pos\nlemmer.lemmatize(\"\u0648\u0641\u064a\")\n    \nlemmer.lemmatize(\"\u0648\u0641\u064a\", pos=\"v\")\n        \nlemmas = lemmer.lemmatize_text(text)\nprint(lemmas)\n['\u0647\u0644', '\u0627\u062d\u062a\u0627\u062c', '\u0625\u0644\u0649', '\u062a\u0631\u062c\u0645\u0629', '\u0643\u064a', '\u062a\u0641\u0647\u0645', '\u062e\u0637\u0627\u0628', '\u0645\u0644\u0643', '\u061f', '\u0644\u063a\u0629', '\"', '\u0643\u0644\u0627\u0633\u064a\u0643\u064a', '\"(', '\u0641\u0635\u062d\u0649', ')', '\u0645\u0648\u062c\u0648\u062f', '\u0641\u064a', '\u0643\u0644', '\u0644\u063a\u0629', '\u0630\u0644\u0643', '\u0644\u063a\u0629', '\"', '\u062f\u0627\u0631\u062c', '\"..', '\u0641\u0631\u0646\u0633\u064a', '\u0627\u0644\u062a\u064a', '\u062f\u0631\u0633', '\u0641\u064a', '\u0645\u062f\u0631\u0633\u0629', '\u0644\u064a\u0633\u062a', '\u0641\u0631\u0646\u0633\u064a', '\u0627\u0644\u062a\u064a', '\u0627\u0633\u062a\u062e\u062f\u0645', '\u0646\u0627\u0633', '\u0641\u064a', '\u0634\u0648\u0627\u0631\u0639', '\u0628\u0627\u0631\u064a\u0633', '..', '\u0645\u0644\u0643', '\u0628\u0631\u064a\u0637\u0627\u0646\u064a\u0627', '\u0644\u0627', '\u062e\u0637\u0628', '\u0628\u0644\u063a\u0629', '\u0634\u0648\u0627\u0631\u0639', '\u062f\u0646\u0648', '..', '\u0643\u0644', '\u0645\u0642\u0627\u0645', '\u0645\u0642\u0627\u0644\u064a']\n     # lemmatize a text and return lemma pos\nlemmas = lemmer.lemmatize_text(text, return_pos=True)\nprint(lemmas)\n[('\u0647\u0644', 'stopword'), ('\u0627\u062d\u062a\u0627\u062c', 'verb'), ('\u0625\u0644\u0649', 'stopword'), ('\u062a\u0631\u062c\u0645\u0629', 'noun'), ('\u0643\u064a', 'stopword'), ('\u062a\u0641\u0647\u0645', 'noun'), ('\u062e\u0637\u0627\u0628', 'noun'), ('\u0645\u0644\u0643', 'noun'), '\u061f', ('\u0644\u063a\u0629', 'noun'), '\"', ('\u0643\u0644\u0627\u0633\u064a\u0643\u064a', 'noun'), '\"(', ('\u0641\u0635\u062d\u0649', 'noun'), ')', ('\u0645\u0648\u062c\u0648\u062f', 'noun'), ('\u0641\u064a', 'stopword'), ('\u0643\u0644', 'stopword'), ('\u0644\u063a\u0629', 'noun'), ('\u0630\u0644\u0643', 'stopword'), ('\u0644\u063a\u0629', 'noun'), '\"', ('\u062f\u0627\u0631\u062c', 'noun'), '\"..', ('\u0641\u0631\u0646\u0633\u064a', 'noun'), ('\u0627\u0644\u062a\u064a', 'stopword'), ('\u062f\u0631\u0633', 'verb'), ('\u0641\u064a', 'stopword'), ('\u0645\u062f\u0631\u0633\u0629', 'noun'), ('\u0644\u064a\u0633\u062a', 'stopword'), ('\u0641\u0631\u0646\u0633\u064a', 'noun'), ('\u0627\u0644\u062a\u064a', 'stopword'), ('\u0627\u0633\u062a\u062e\u062f\u0645', 'verb'), ('\u0646\u0627\u0633', 'noun'), ('\u0641\u064a', 'stopword'), ('\u0634\u0648\u0627\u0631\u0639', 'noun'), ('\u0628\u0627\u0631\u064a\u0633', 'all'), '..', ('\u0645\u0644\u0643', 'noun'), ('\u0628\u0631\u064a\u0637\u0627\u0646\u064a\u0627', 'noun'), ('\u0644\u0627', 'stopword'), ('\u062e\u0637\u0628', 'verb'), ('\u0628\u0644\u063a\u0629', 'noun'), ('\u0634\u0648\u0627\u0631\u0639', 'noun'), ('\u062f\u0646\u0648', 'verb'), '..', ('\u0643\u0644', 'stopword'), ('\u0645\u0642\u0627\u0645', 'noun'), ('\u0645\u0642\u0627\u0644\u064a', 'noun')]\n","19b5d0a3":"#Code by https:\/\/pypi.org\/project\/qalsadi\/\n\n#Developper: Taha Zerrouki: http:\/\/tahadz.com taha dot zerrouki at gmail dot com\n\nfilename=\"..\/input\/arabic-article-headline-generation\/dataset\/out.csv\"\nimport qalsadi.analex as qa\ntry:\n    myfile=open(filename)\n    text=(myfile.read()).decode('utf8');\n\n    if text == None:\n        text=u\"\u0627\u0644\u0633\u0644\u0627\u0645 \u0639\u0644\u064a\u0643\u0645\"\nexcept:\n    text=u\"\u0623\u0633\u0644\u0645\"\n    print (\"\u0648 \u0647\u0648 \u064a\u0639\u062a\u0628\u0631 \u0645\u062a\u062d\u0641 \u062c\u062f\u064a\u062f \u0627\u0644\u0639\u0647\u062f \u060c \u064a\u0642\u062f\u0645 \u0645\u062c\u0645\u0648\u0639\u0629 \u0645\u0646 \u0627\u0644\")\n\ndebug=False;\nlimit=500\nanalyzer = qa.Analex()\nanalyzer.set_debug(debug);\nresult = analyzer.check_text(text);\nprint ('----------------python format result-------')\nprint \nresult\nfor i in range(len(result)):\n#       print \"--------\u062a\u062d\u0644\u064a\u0644 \u0643\u0644\u0645\u0629  ------------\", word.encode('utf8');\n    print (\"\u064a\u0639\u062a\u0628\u0631 \u0641\u064a\u0644\u0627\u062c\u064a\u0648 \u0645\u0648\u0644 \u0645\u0646 \u0623\u0647\u0645 \u0627\u0644\u0645\u062c\u0645\u0639\u0627\u062a \u0627\u0644\u062a\u062c\u0627\u0631\u064a\u0629 \u0648 \u0623\");\n    for analyzed in  result[i]:\n        print (\"\u0627\u0644\u0645\u062a\u062d\u0641 \u0627\u0644\u0639\u0631\u0628\u064a \u0644\u0644\u0641\u0646 \u0627\u0644\u062d\u062f\u064a\u062b\");\n        print ('repr analyzed');","55e0188a":"!pip install arabic-reshaper","f7a2c9de":"import arabic_reshaper as arabic_reshaper\nfrom bidi.algorithm import get_display","b96b5817":"data=pd.read_csv(r'..\/input\/arabic-article-headline-generation\/dataset\/out.csv')","40c3b184":"#Code by MAHMOUD MOHAMED ALY ABDALLAH JOBEEL  https:\/\/www.kaggle.com\/mahmoudjobeel\/data-analysis-of-egyptian-arabic-movies\n\ndef describe_column(colNames):\n    \n    fig, ax = plt.subplots()\n\n    # hide axes\n    fig.patch.set_visible(False)\n    ax.axis('off')\n    ax.axis('tight')\n\n    plt.subplots_adjust(left=0.5, top=0.4)\n    temp=[]\n    for i in data[colNames].describe().values:\n        if type(i)==str:\n            temp.append(get_display(arabic_reshaper.reshape(i)))\n        elif type(i)==np.float64:\n            temp.append(round(i, 3))\n        else:\n            temp.append(i)\n    df = pd.DataFrame(temp, columns=[get_display(arabic_reshaper.reshape(colNames))])\n\n    ax.table(cellText=df.values, colLabels=df.columns,rowLabels=data[colNames].describe().index ,loc='center')\n\n    fig.tight_layout()\n\n    plt.show()","5d8c3ff3":"#Code by MAHMOUD MOHAMED ALY ABDALLAH JOBEEL  https:\/\/www.kaggle.com\/mahmoudjobeel\/data-analysis-of-egyptian-arabic-movies\n\ndef describe_dataframe(df):\n    for colName in df.columns:\n        describe_column(colName)","93854716":"describe_column(\"title\")","5929c0ad":"describe_column(\"index\")","7b7aa3ea":"#Qalsadi Arabic Morphological Analyzer for Python\n\nDeveloppers: Taha Zerrouki: http:\/\/tahadz.com taha dot zerrouki at gmail dot com\n\nCitation: T. Zerrouki, Qalsadi, Arabic mophological analyzer Library for python.,  https:\/\/pypi.python.org\/pypi\/qalsadi\/","354912a0":"#Thanks to MAHMOUD MOHAMED ALY ABDALLAH JOBEEL  https:\/\/www.kaggle.com\/mahmoudjobeel\/data-analysis-of-egyptian-arabic-movies\n\n#Thanks to Taha Zerrouki  https:\/\/pypi.org\/project\/qalsadi\/\n\n#Developper: Taha Zerrouki: http:\/\/tahadz.com taha dot zerrouki at gmail dot com","5d8792de":"#Above, I think it was Not what was expected to return."}}