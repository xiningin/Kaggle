{"cell_type":{"baa62d8c":"code","19df9546":"code","74f58d89":"code","ac752b22":"code","d62ceede":"code","690cc64f":"code","a0b5c335":"code","7b3ee176":"code","797b1894":"code","8912a659":"code","68d4359a":"code","51176f2e":"code","10d068a9":"code","8d223160":"code","d5de8328":"code","07b169d8":"code","ee0796fb":"code","bd688a97":"code","146729a9":"code","99b2751a":"code","7ce373d9":"code","38480920":"code","096823ad":"code","c1858752":"code","8e1c8849":"code","013b8f58":"code","edd89ac3":"code","37f1bd87":"code","a0fb6124":"code","4427357c":"code","6ed29752":"code","50f03464":"code","fb2b7c45":"code","59736afa":"code","dcdb4981":"code","52325243":"code","5526cf0b":"code","a97fa630":"code","7069c70b":"code","e2eb8d21":"code","1a370558":"code","aa3159a0":"code","1992bc5c":"code","7f283d92":"code","88cffc94":"code","e855d3d8":"code","70e55fc6":"code","ab14e7d2":"code","021d089c":"code","fdfb1da2":"code","4535ec81":"code","ad9931ae":"code","070d300c":"code","2edebe5c":"code","2aa4856a":"code","1872facc":"code","a53830c8":"code","57f9902c":"code","6d61004e":"code","2360d186":"code","6364d234":"code","54660bc0":"code","6f5c827a":"code","c448dcfc":"code","c7b75136":"code","1c3a672e":"code","1f00bf9d":"code","b64a67a9":"code","b36e91aa":"code","ba76dbce":"code","bc85c440":"code","7275f782":"code","73f7bef9":"code","62807f3a":"code","bfcbcd13":"code","352df455":"code","e8c4e8bb":"code","469090bc":"code","1676623a":"code","6539e4b0":"code","cbd55298":"code","261a550a":"code","7b20a18d":"code","44abb3c2":"code","ea902f9e":"code","40e6044a":"code","d0b89b05":"code","59740c16":"code","0df8d5e3":"code","ffe6f1da":"code","d7dd4bf4":"code","ebd510d2":"code","e422a528":"code","869516a5":"code","835efee3":"code","ae597768":"code","3da3e6b6":"code","88b2b470":"code","52b07d55":"code","0b29fa42":"code","8831ce59":"code","d064fd51":"code","c1a26c70":"code","f9868918":"code","e710d1d7":"code","196ca155":"code","2cb9cf5b":"code","6b861876":"code","e56db27f":"code","ef36da63":"code","3181dd8f":"code","dd91948b":"code","1805be57":"code","7faf0b7a":"code","8509bb7d":"code","4d0ef005":"code","a8cb2d5a":"code","3810ed75":"code","963fe6a7":"code","5618b3a5":"code","2414780a":"code","74b5b135":"code","87b99ff0":"code","69bde994":"code","74d480fd":"code","7d126c61":"code","84f0378f":"code","653e8c86":"code","77d210db":"code","c5c7b6d9":"code","37801267":"code","928e544e":"code","521febab":"code","fee101b2":"code","e99f8aa1":"code","17755bab":"code","9135645f":"code","b886c58e":"code","8829c9f1":"code","aaf9d02a":"code","9666b9b5":"code","cf4a00c4":"code","40e27192":"code","98932607":"code","4335f675":"code","5b10c548":"code","172631c8":"code","a2d8d724":"code","abe25bd5":"code","445050fc":"code","89a2a3cf":"code","7c9bb558":"code","5642de27":"code","5ef436f9":"code","d505be9a":"code","8f2c8fea":"code","db07dcea":"code","02a57977":"code","e5c983ec":"code","ed3602ae":"code","ea70867e":"code","803dd6ab":"code","c94431b9":"code","20ce12fc":"code","b53b4810":"code","f7e6e7ad":"code","df905bff":"code","f7eeaa9d":"code","c33b06a2":"code","ac82f6c1":"code","76f79767":"code","4ff8c65f":"code","a3f1e093":"code","8a9ecf57":"code","8be59044":"code","32ce7abb":"code","5abecf29":"code","115df19f":"code","ca5e3150":"code","bcfaa6a9":"code","efac369c":"code","7ecf0b6b":"code","6643601f":"code","0ead0593":"code","bc0c8cfb":"code","191f0706":"code","6a5baa55":"code","25b70578":"code","a0bd44da":"code","56f812e0":"code","f9a58f18":"code","1b1b5d63":"code","86f0a725":"code","010bc2cf":"code","ff929583":"code","e2f6a903":"code","0bf901d2":"code","a1074361":"code","5a2a778f":"code","02220d8c":"code","a6d10582":"code","13f03c06":"code","887e708a":"code","65bb1e19":"code","a344ef19":"code","687b8a25":"code","114dc734":"code","d57f99ab":"code","72a3d0ef":"code","0937a892":"code","6008dc5b":"code","c5bbfcd2":"code","a2a554f0":"code","5092538b":"code","2c1f1654":"code","e6735fc3":"code","1e6f0254":"code","25964ed5":"code","51f11ebe":"code","98cf1f4e":"code","bdc5ecfd":"code","1486912b":"code","53f6d045":"code","bf74f7db":"code","9647f22e":"code","e64c497d":"code","6c4868b2":"code","529239ee":"code","0a0d7ab4":"code","f425a324":"code","6b4526b6":"code","5eecdc44":"code","595f73d8":"code","ea1cfa05":"code","831707c3":"code","7653f540":"code","d36fbbf7":"code","fe6b7c6b":"code","382ece22":"code","351e792e":"code","03fe5bd3":"code","f0e2e559":"code","dcc481c1":"code","2cddd3d4":"code","f8729363":"code","7f43782e":"code","2bbf428a":"code","fba982aa":"code","117dba7b":"code","268f55ac":"code","e85b883c":"code","f052b785":"code","44cc8493":"code","88ccc1c1":"code","1c27bb8c":"code","2141ddfd":"code","ee2ca470":"code","1e7d2307":"code","89a2b491":"code","5e3298e8":"code","5ecb417e":"code","629dea30":"code","e946c29b":"code","ff4c8d2d":"code","7b2aa3fa":"code","282cdc3a":"code","2f6be432":"markdown","ed090c0e":"markdown","7b8215d7":"markdown","b67935f5":"markdown","1f7fd2a4":"markdown","a7d0d29d":"markdown","5b6aca48":"markdown","c3a31598":"markdown","8529f98c":"markdown","94988582":"markdown","5f3597ec":"markdown","5c578076":"markdown","f9fb3a6f":"markdown","4a9f629a":"markdown","249f3708":"markdown","87a5d8aa":"markdown","237d35ac":"markdown","677713dd":"markdown","f281e817":"markdown","1405cf05":"markdown","7c5793b6":"markdown","88785b84":"markdown","a28d7a44":"markdown","2da59a00":"markdown","04ed071c":"markdown","1fc07b2b":"markdown","368c5fff":"markdown","194fe799":"markdown","19f1afae":"markdown","fb9e64c9":"markdown","81747004":"markdown","b0c3247b":"markdown","6957fd46":"markdown","13618859":"markdown","65edf925":"markdown","5250236a":"markdown","596bac4a":"markdown","328f6d76":"markdown","555ca5bf":"markdown","cdc53a95":"markdown","fa748df5":"markdown","c456cce6":"markdown","04088870":"markdown","595a2558":"markdown","69a9eade":"markdown","8b5eed4b":"markdown","9fb56a8f":"markdown","015747e4":"markdown","c8f30a39":"markdown","7f3af09d":"markdown","d32d4f69":"markdown","97681db8":"markdown","db838643":"markdown","3e9e3e34":"markdown","6cdde5c0":"markdown","48f2f221":"markdown","354fc54e":"markdown","f5e1e189":"markdown","8a352a52":"markdown","fc18292c":"markdown","3b1bd85f":"markdown","1480ef0f":"markdown","56321f72":"markdown","d46bd330":"markdown","cfa4ae54":"markdown","f094cda0":"markdown","432a19c0":"markdown","ec66def4":"markdown","1e81509b":"markdown","3a477b47":"markdown","5eb2287a":"markdown","a91c6245":"markdown","8422b8a7":"markdown","f5917b4a":"markdown","ae8aa944":"markdown","89ec1072":"markdown","7216fce1":"markdown","3a555245":"markdown","62cb90a7":"markdown","8a4a6b16":"markdown","d8892b1b":"markdown","0084b3a8":"markdown","fe3982f3":"markdown","f75acfa2":"markdown","2bc95c45":"markdown","d090d0d4":"markdown","2ef7f8ef":"markdown","5c9eeb79":"markdown","516ef100":"markdown","5696ef2d":"markdown","39c54a2a":"markdown","7a9ed6e6":"markdown","33e9238b":"markdown","bba7d216":"markdown","70d9604c":"markdown","39a26ef8":"markdown","9addf4de":"markdown","30b3720c":"markdown","4521c5db":"markdown","726c472e":"markdown","5072f6f6":"markdown","e0bd946f":"markdown","4f7f2dcf":"markdown","eb16d314":"markdown","6ab0694f":"markdown","6bdda023":"markdown","f40471f8":"markdown","dfa351de":"markdown","c9133060":"markdown","d6e2b7dc":"markdown","6933a6fb":"markdown","d6b4d564":"markdown","fec3d608":"markdown","0adcf9d8":"markdown","1342369d":"markdown","a69f8cef":"markdown","29603c7e":"markdown","978b9f7e":"markdown","cccd515d":"markdown","a91a95d6":"markdown","518a028e":"markdown","a7660b75":"markdown","8a5a0cb4":"markdown","9f425baf":"markdown","fc67f468":"markdown","92251e40":"markdown","027f4c8d":"markdown","91c043ba":"markdown","92cfc35f":"markdown","5e8f5afa":"markdown","973cdef2":"markdown","e9e33ec2":"markdown","d362d2ce":"markdown","2831f088":"markdown","38173398":"markdown","766c7c9c":"markdown","6646c37b":"markdown","8f3ebd6f":"markdown","198adbb2":"markdown","a304fec1":"markdown","564e54b9":"markdown","386cc840":"markdown","c6d9aebd":"markdown","74550b01":"markdown"},"source":{"baa62d8c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","19df9546":"# !pip install pyforest\n\n# 1-Import Libraies\n\nimport ipywidgets\nfrom ipywidgets import interact\n\nimport numpy as np\nimport pandas as pd \n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport matplotlib.ticker as mticker\nimport squarify as sq\n\nimport scipy.stats as stats\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nimport missingno as msno \n\nimport datetime as dt\nfrom datetime import datetime\n\n# Scaling\nfrom sklearn.preprocessing import scale, StandardScaler, MinMaxScaler, RobustScaler\nfrom sklearn.preprocessing import PolynomialFeatures \nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import PowerTransformer \nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\n\n\n# Importing plotly and cufflinks in offline mode\nimport plotly.express as px\nimport cufflinks as cf\nimport plotly.offline\ncf.go_offline()\ncf.set_config_file(offline=False, world_readable=True)\n\n# !pip install termcolor\nimport colorama\nfrom colorama import Fore, Style  # maakes strings colored\nfrom termcolor import colored\nfrom termcolor import cprint\n\nfrom wordcloud import WordCloud\n\n# Ignore Warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nwarnings.warn(\"this will not show\")\n\n# Figure&Display options\nplt.rcParams[\"figure.figsize\"] = (10,6)\npd.set_option('max_colwidth',200)\npd.set_option('display.max_rows', 1000)\npd.set_option('display.max_columns', 200)\npd.set_option('display.float_format', lambda x: '%.3f' % x)","74f58d89":"## Some Useful Functions\n\n###############################################################################\n\ndef missing_values(df):\n    missing_number = df.isnull().sum().sort_values(ascending=False)\n    missing_percent = (df.isnull().sum()\/df.isnull().count()).sort_values(ascending=False)\n    missing_values = pd.concat([missing_number, missing_percent], axis=1, keys=['Missing_Number', 'Missing_Percent'])\n    return missing_values[missing_values['Missing_Number']>0]\n\n###############################################################################\n\ndef first_looking(df):\n    print(colored(\"Shape:\", attrs=['bold']), df.shape,'\\n', \n          colored('*'*100, 'red', attrs=['bold']),\n          colored(\"\\nInfo:\\n\", attrs=['bold']), sep='')\n    print(df.info(), '\\n', \n          colored('*'*100, 'red', attrs=['bold']), sep='')\n    print(colored(\"Number of Uniques:\\n\", attrs=['bold']), df.nunique(),'\\n',\n          colored('*'*100, 'red', attrs=['bold']), sep='')\n    print(colored(\"Missing Values:\\n\", attrs=['bold']), missing_values(df),'\\n', \n          colored('*'*100, 'red', attrs=['bold']), sep='')\n    print(colored(\"All Columns:\", attrs=['bold']), list(df.columns),'\\n', \n          colored('*'*100, 'red', attrs=['bold']), sep='')\n\n    df.columns= df.columns.str.lower().str.replace('&', '_').str.replace(' ', '_')\n    print(colored(\"Columns after rename:\", attrs=['bold']), list(df.columns),'\\n',\n          colored('*'*100, 'red', attrs=['bold']), sep='')  \n    print(colored(\"Columns after rename:\", attrs=['bold']), list(df.columns),'\\n',\n          colored('*'*100, 'red', attrs=['bold']), sep='')\n    print(colored(\"Descriptive Statistics \\n\", attrs=['bold']), df.describe().round(2),'\\n',\n          colored('*'*100, 'red', attrs=['bold']), sep='') # Gives a statstical breakdown of the data.\n    print(colored(\"Descriptive Statistics (Categorical Columns) \\n\", attrs=['bold']), df.describe(include=object).T,'\\n',\n          colored('*'*100, 'red', attrs=['bold']), sep='') # Gives a statstical breakdown of the data.\n    \ndef multicolinearity_control(df):\n    feature =[]\n    collinear=[]\n    for col in df.corr().columns:\n        for i in df.corr().index:\n            if (abs(df.corr()[col][i])> .9 and abs(df.corr()[col][i]) < 1):\n                    feature.append(col)\n                    collinear.append(i)\n                    print(colored(f\"Multicolinearity alert in between:{col} - {i}\", \n                                  \"red\", attrs=['bold']), df.shape,'\\n',\n                                  colored('*'*100, 'red', attrs=['bold']), sep='')\n\ndef duplicate_values(df):\n    print(colored(\"Duplicate check...\", attrs=['bold']), sep='')\n    print(\"There are\", df.duplicated(subset=None, keep='first').sum(), \"duplicated observations in the dataset.\")\n#     duplicate_values = df.duplicated(subset=None, keep='first').sum()\n#     if duplicate_values > 0:\n#         df.drop_duplicates(keep='first', inplace=True)\n#         print(duplicate_values, colored(\" Duplicates were dropped!\"),'\\n',\n#               colored('*'*100, 'red', attrs=['bold']), sep='')\n#     else:\n#         print(colored(\"There are no duplicates\"),'\\n',\n#               colored('*'*100, 'red', attrs=['bold']), sep='')     \n        \n# def drop_columns(df, drop_columns):\n#     if drop_columns !=[]:\n#         df.drop(drop_columns, axis=1, inplace=True)\n#         print(drop_columns, 'were dropped')\n#     else:\n#         print(colored('We will now check the missing values and if necessary, the related columns will be dropped!', attrs=['bold']),'\\n',\n#               colored('*'*100, 'red', attrs=['bold']), sep='')\n        \ndef drop_null(df, limit):\n    print('Shape:', df.shape)\n    for i in df.isnull().sum().index:\n        if (df.isnull().sum()[i]\/df.shape[0]*100)>limit:\n            print(df.isnull().sum()[i], 'percent of', i ,'null and were dropped')\n            df.drop(i, axis=1, inplace=True)\n            print('new shape:', df.shape)       \n    print('New shape after missing value control:', df.shape)\n    \n###############################################################################\n\n# To view summary information about the columns\n\ndef first_look(col):\n    print(\"column name    : \", col)\n    print(\"--------------------------------\")\n    print(\"Per_of_Nulls   : \", \"%\", round(df[col].isnull().sum()\/df.shape[0]*100, 2))\n    print(\"Num_of_Nulls   : \", df[col].isnull().sum())\n    print(\"Num_of_Uniques : \", df[col].nunique())\n    print(\"Duplicates     : \", df.duplicated(subset=None, keep='first').sum())\n    print(df[col].value_counts(dropna = False))\n    \n###############################################################################\n\ndef fill_most(df, group_col, col_name):\n    '''Fills the missing values with the most existing value (mode) in the relevant column according to single-stage grouping'''\n    for group in list(df[group_col].unique()):\n        cond = df[group_col]==group\n        mode = list(df[cond][col_name].mode())\n        if mode != []:\n            df.loc[cond, col_name] = df.loc[cond, col_name].fillna(df[cond][col_name].mode()[0])\n        else:\n            df.loc[cond, col_name] = df.loc[cond, col_name].fillna(df[col_name].mode()[0])\n    print(\"Number of NaN : \",df[col_name].isnull().sum())\n    print(\"------------------\")\n    print(df[col_name].value_counts(dropna=False))","ac752b22":"df0=pd.read_csv('..\/input\/online-retail-dataset\/Online Retail.csv',index_col=0)\ndf = df0.copy()\ndf.head(3)","d62ceede":"# df0 = pd.read_csv('Online Retail.csv', index_col=0)\n# df = df0.copy()\n# df.head(3) ","690cc64f":"df.tail(3) ","a0b5c335":"df.sample(5)","7b3ee176":"first_looking(df)\nduplicate_values(df)","797b1894":"df.columns","8912a659":"df['total_price'] = df['quantity'] * df['unitprice']\ndf['total_price'].head(3)","68d4359a":"print(\"There is\", df.shape[0], \"observation and\", df.shape[1], \"columns in the dataset\")","51176f2e":"cprint(\"Have a First Look to INVOICE Column\",'red')\nfirst_look('invoiceno')","10d068a9":"cprint(\"Total number of invoices for each country :\",'red')\nprint(df.groupby('country')['invoiceno'].nunique().sort_values(ascending=False))","8d223160":"fig = px.histogram(df, x = df.groupby('country')['invoiceno'].nunique().index,\n                   y = df.groupby('country')['invoiceno'].nunique().values, \n                   title = 'Invoice Counts Per Country', \n                   labels = dict(x = \"Countries\", y =\"Invoice\"))\nfig.show();","d5de8328":"df['invoiceno'].sample(10)","07b169d8":"cprint(\"The Bottom Five INVOICENO According to TOTAL_PRICE :\",'red')\ndf.groupby(['invoiceno','customerid', 'country'])['total_price'].sum().sort_values().head()","ee0796fb":"cprint(\"The Top Five INVOICENO According to TOTAL_PRICE :\",'red')\ndf.groupby(['invoiceno','customerid', 'country'])['total_price'].sum().sort_values(ascending=False).head()","bd688a97":"cprint(\"The Bottom Five INVOICENO According to QUANTITY :\",'red')\ndf.groupby(['invoiceno','customerid', 'country'])['quantity'].sum().sort_values().head()","146729a9":"cprint(\"The Top Five INVOICENO According to QUANTITY :\",'red')\ndf.groupby(['invoiceno','customerid', 'country'])['quantity'].sum().sort_values(ascending=False).head()","99b2751a":"cprint(\"INVOICENO Starts with A:\",'red')\ndf['invoiceno'].str.startswith('A').value_counts()","7ce373d9":"cprint(\"INVOICENO Starts with C:\",'red')\ndf['invoiceno'].str.startswith('C').value_counts()","38480920":"# cprint(\"INVOICENO Starts with A or C:\",'red')\n# (df['invoiceno'].str.startswith('C') | df['invoiceno'].str.startswith('A')).value_counts()\n\ncprint(\"INVOICENO Starts with A or C:\",'red')\ndf[\"invoiceno\"].str[0].str.isalpha().value_counts()","096823ad":"fig = px.pie(df, values = df['invoiceno'].str.startswith('C').value_counts(), \n             names = (df[\"invoiceno\"].str[0].str.isalpha().value_counts()).index, \n             title = 'invoiceno Starts With A or C')\nfig.show()","c1858752":"cprint(\"Have a First Look to STOCKCODE Column\",'red')\nfirst_look('stockcode')","8e1c8849":"cprint(\"Have a First Look to DESCRIPTION Column\",'red')\nfirst_look('description')","013b8f58":"cprint(\"Number of Missing Values\",'red')\ndf[['stockcode','description']].isnull().sum()","edd89ac3":"cprint(\"Croos check\",'red')\ndf[(df['stockcode'].notnull()) & (df['description'].isnull())].count()","37f1bd87":"cprint(\"Croos check\",'red')\ndf[(df['stockcode'].notnull()) & (df['description'].isnull())][['stockcode','description']].head(10)","a0fb6124":"cprint(\"Croos check\",'red')\ndf[df['stockcode']=='22139.0']['description'].value_counts(dropna=False)","4427357c":"cprint(\"Have a First Look to QUANTITY Column\",'red')\nfirst_look('quantity')","6ed29752":"cprint(\"Number of Observations That QUANTITY Values Below 0\",'red')\n(df['quantity'] < 0).value_counts()","50f03464":"fig = px.pie(df, values = (df['quantity'] < 0).value_counts(), \n             names = (df['quantity'] < 0).value_counts().index, title = 'quantity Smaller Than 0')\nfig.show()","fb2b7c45":"cprint(\"Number of Observations That QUANTITY Values Above 0\",'red')\ndf[df['quantity'] > 0][['invoiceno', 'stockcode', 'description', 'quantity', 'invoicedate', 'unitprice', 'customerid', 'country']]","59736afa":"cprint(\"Number of Observations That QUANTITY Values Equal to 0\",'red')\ndf[df['quantity'] == 0][['invoiceno', 'stockcode', 'description', 'quantity', 'invoicedate', 'unitprice', 'customerid', 'country']]","dcdb4981":"cprint(\"Number of Observations That QUANTITY Values Below 0\",'red')\ndf[df['quantity'] < 0][['invoiceno', 'stockcode', 'description', 'quantity', 'invoicedate', 'unitprice', 'customerid', 'country']]","52325243":"cprint(\"Have a First Look to UNITPRICE Column\",'red')\nfirst_look('unitprice')","5526cf0b":"cprint(\"Number of Observations That UNITPRICE Values Above 0\",'red')\ndf[df['unitprice'] > 0][['invoiceno', 'stockcode', 'description', 'quantity', 'invoicedate', 'unitprice', 'customerid', 'country']].head()","a97fa630":"cprint(\"Number of Observations That UNITPRICE Values Equal to 0\",'red')\ndf[df['unitprice']==0][['invoiceno', 'stockcode', 'description', 'quantity', 'invoicedate', 'unitprice', 'customerid', 'country']].head()","7069c70b":"cprint(\"Number of Observations That UNITPRICE Values Below 0\",'red')\ndf[df['unitprice']<0][['invoiceno', 'stockcode', 'description', 'quantity', 'invoicedate', 'unitprice', 'customerid', 'country']].head()","e2eb8d21":"cprint(\"Have a First Look to CUSTOMERID Column\",'red')\nfirst_look('customerid')","1a370558":"cprint(\"Total number of customers for each country :\",'red')\nprint(df.groupby('country')['customerid'].nunique().sort_values(ascending=False))","aa3159a0":"fig = px.histogram(df, x = df.groupby('country')['customerid'].nunique().index, \n                   y = df.groupby('country')['customerid'].nunique().values, \n                   title = 'Customer Counts by Country',\n                   labels = dict(x = \"Countries\", y =\"Customer\"))\nfig.show()","1992bc5c":"cprint(\"Customers by number of orders (Bottom 5)\",'red')\ndf.groupby(['customerid', 'country'])['invoiceno'].count().sort_values().head()","7f283d92":"cprint(\"Customers by number of orders (Top 5)\",'red')\ndf.groupby(['customerid', 'country'])['invoiceno'].count().sort_values(ascending=False).head()","88cffc94":"cprint(\"Customers according to the number of pieces in the orders (Bottom 5)\",'red')\ndf.groupby(['customerid', 'country'])['quantity'].sum().sort_values().head()","e855d3d8":"cprint(\"Customers according to the number of pieces in the orders (Top 5)\",'red')\ndf.groupby(['customerid', 'country'])['quantity'].sum().sort_values(ascending=False).head()","70e55fc6":"cprint(\"Customers according to the spendings (Bottom 5)\",'red')\ndf.groupby(['customerid', 'country'])['total_price'].sum().sort_values().head()","ab14e7d2":"cprint(\"Customers according to the spendings (Top 5)\",'red')\ndf.groupby(['customerid', 'country'])['total_price'].sum().sort_values(ascending=False).head()","021d089c":"cprint(\"Have a First Look to INVOICEDATE Column\",'red')\nfirst_look('invoicedate')","fdfb1da2":"df.invoicedate.max()","4535ec81":"df.invoicedate.min()","ad9931ae":"df[\"invoicedate\"] = pd.to_datetime(df[\"invoicedate\"])\ndf.groupby('customerid')['invoicedate'].max().sample(5)","070d300c":"df[['invoiceno', 'quantity', 'unitprice']].head(3)","2edebe5c":"df[[\"invoiceno\", \"quantity\", \"unitprice\"]].describe().T.style.background_gradient(subset=['mean','std','50%','count'], cmap='RdPu')","2aa4856a":"df[[\"invoiceno\", \"quantity\", \"unitprice\"]].describe(include=object).T","1872facc":"cprint(\"QUANTITY Below 0\",'red')\ndf[df['quantity'] < 0][['invoiceno', 'stockcode', 'description', 'quantity', 'invoicedate', 'unitprice', 'customerid', 'country']].sample(5)","a53830c8":"cprint(\"QUANTITY Equal to 0\",'red')\ndf[df['quantity'] == 0][['invoiceno', 'stockcode', 'description', 'quantity', 'invoicedate', 'unitprice', 'customerid', 'country']]","57f9902c":"cprint(\"QUANTITY Above 0\",'red')\ndf[df['quantity'] > 0][['invoiceno', 'stockcode', 'description', 'quantity', 'invoicedate', 'unitprice', 'customerid', 'country']].sample(5)","6d61004e":"cprint(\"UNITPRICE Below 0\",'red')\ndf[df['unitprice'] < 0][['invoiceno', 'stockcode', 'description', 'quantity', 'invoicedate', 'unitprice', 'customerid', 'country']]","2360d186":"cprint(\"UNITPRICE Equal to 0\",'red')\ndf[df['unitprice'] == 0][['invoiceno', 'stockcode', 'description', 'quantity', 'invoicedate', 'unitprice', 'customerid', 'country']].sample(5)","6364d234":"cprint(\"UNITPRICE Above 0\",'red')\ndf[df['unitprice'] > 0][['invoiceno', 'stockcode', 'description', 'quantity', 'invoicedate', 'unitprice', 'customerid', 'country']].sample(5)","54660bc0":"df[['unitprice', 'quantity']].describe().T.style.background_gradient(subset = ['mean','std','50%','count'], cmap='RdPu')","6f5c827a":"# df[\"invoiceno\"].str.startswith('C').sum()\n\ndf[\"invoiceno\"].str.startswith('C').value_counts()","c448dcfc":"df[\"invoiceno\"].str.startswith('C').value_counts(normalize = True)*100","c7b75136":"fig = px.pie(df, values = df['invoiceno'].str.startswith('C').value_counts(), \n             names = (df['invoiceno'].str.startswith('C').value_counts()).index, \n             title = 'invoiceno Starts With C')\nfig.show()","1c3a672e":"cprint('INVOICENO Starts with \"A\" & unitprice<0:','red')\ndf[df['unitprice']<0][['invoiceno', 'stockcode', 'description', 'quantity', 'invoicedate', 'unitprice', 'customerid', 'country']]","1f00bf9d":"cprint(\"Cancelled Orders\",'red')\ndf[df['quantity'] < 0][['invoiceno', 'stockcode', 'description', 'quantity', 'invoicedate', 'unitprice', 'customerid', 'country']].sample(10)","b64a67a9":"cprint(\"Non-Cancelled Orders\",'red')\ndf[df['quantity'] > 0][['invoiceno', 'stockcode', 'description', 'quantity', 'invoicedate', 'unitprice', 'customerid', 'country']].sample(10)","b36e91aa":"missing_values(df)","ba76dbce":"df.isnull().melt(value_name=\"missing\")","bc85c440":"plt.figure(figsize = (10, 5))\n\nsns.displot(\n    data = df.isnull().melt(value_name = \"missing\"),\n    y = \"variable\",\n    hue = \"missing\",\n    multiple = \"fill\",\n    height = 9.25)\n\nplt.axvline(0.3, color = \"r\");","7275f782":"cprint(\"Before filling missing values of DESCRIPTION column\",'red')\ndf[df['stockcode']=='22139.0']['description'].value_counts(dropna=False)","73f7bef9":"fill_most(df, 'stockcode', 'description')","62807f3a":"cprint(\"After filling missing values of DESCRIPTION column\",'red')\ndf[df['stockcode']=='22139.0']['description'].value_counts(dropna=False)","bfcbcd13":"plt.figure(figsize = (10, 5))\n\nsns.displot(\n    data = df.isnull().melt(value_name = \"missing\"),\n    y = \"variable\",\n    hue = \"missing\",\n    multiple = \"fill\",\n    height = 9.25)\n\nplt.axvline(0.3, color = \"r\");","352df455":"print(\"There is\", df.shape[0], \"observation and\", df.shape[1], \"columns in the dataset\")","e8c4e8bb":"missing_values(df)","469090bc":"df = df.dropna(subset=[\"customerid\"])\n","1676623a":"print(\"There is\", df.shape[0], \"observation and\", df.shape[1], \"columns in the dataset\")","6539e4b0":"plt.figure(figsize = (10, 5))\n\nsns.displot(\n    data = df.isnull().melt(value_name = \"missing\"),\n    y = \"variable\",\n    hue = \"missing\",\n    multiple = \"fill\",\n    height = 9.25)\n\nplt.axvline(0.3, color = \"r\");","cbd55298":"df = df[(df['unitprice'] > 0) & (df['quantity'] > 0)]","261a550a":"print(\"There is\", df.shape[0], \"observation and\", df.shape[1], \"columns in the dataset\")","7b20a18d":"print(\"There are\", df.duplicated(subset=None, keep='first').sum(), \"duplicated observations in the dataset.\")\nprint(df.duplicated(subset=None, keep='first').sum(), \"Duplicated observations are dropped!\")\ndf.drop_duplicates(keep='first', inplace=True)","44abb3c2":"print(\"There is\", df.shape[0], \"observation and\", df.shape[1], \"columns in the dataset\")","ea902f9e":"df.columns","40e6044a":"cprint(\"Unique number of invoice per customer\",'red')\ndf.groupby(\"customerid\")[\"invoiceno\"].nunique().sort_values(ascending = False)","d0b89b05":"cprint(\"Average number of unique items per order\",'red')\ndf.groupby([\"invoiceno\",\"stockcode\"])[\"quantity\"].mean()","59740c16":"cprint(\"Average number of unique items per customer\",'red')\ndf.groupby([\"customerid\",\"stockcode\"])[\"quantity\"].mean()","0df8d5e3":"cprint(\"Number of unique items per customer\",'red')\ndf.groupby(\"customerid\")[\"stockcode\"].nunique().sort_values(ascending=False)","ffe6f1da":"df.head(1)","d7dd4bf4":"cprint(\"Total revenue per country\",'red')\ndf.groupby(\"country\")['total_price'].sum().sort_values(ascending=False)","ebd510d2":"cprint(\"Number of Customer Per Country\",'red')\ndf.groupby(\"country\")['customerid'].nunique().sort_values(ascending=False)","e422a528":"df.groupby(\"customerid\")['country'].nunique().sum()","869516a5":"df['customerid'].nunique()","835efee3":"df.groupby('customerid')['country'].nunique().value_counts()","ae597768":"fig = px.histogram(df, x = df.groupby('country')['customerid'].nunique().sort_index(ascending=False).index, \n                   y = df.groupby('country')['customerid'].nunique().sort_index(ascending=False).values, \n                   title = 'Customer Counts by Country',\n                   labels = dict(x = \"Countries\", y =\"Customers\"))\nfig.show()","3da3e6b6":"cprint(\"Total Costs Per Country\",'red')\ndf.groupby('country')['total_price'].sum().sort_values(ascending=False)","88b2b470":"fig = px.histogram(df, x = df.groupby('country')['total_price'].sum().sort_values(ascending=False).index, \n                   y = df.groupby('country')['total_price'].sum().sort_values(ascending=False).values, \n                   title = 'Total Costs Per Country',\n                   labels = dict(x = \"Countries\", y =\"total_price\"))\nfig.show()","52b07d55":"df.head(3)","0b29fa42":"# from wordcloud import WordCloud\n\ncountry_text = df[\"country\"].str.split(\" \").str.join(\"_\")\nall_countries = \" \".join(country_text)\n\nwc = WordCloud(background_color=\"red\", \n               max_words=250, \n               max_font_size=256, \n               random_state=42,\n               width=800, height=400)\nwc.generate(all_countries)\nplt.figure(figsize = (12, 10))\nplt.imshow(wc)\nplt.axis('off')\nplt.show()","8831ce59":"df_uk = df[df[\"country\"]==\"United Kingdom\"]\ndf_uk.head(3)","d064fd51":"df_uk.tail(3)","c1a26c70":"df_uk.sample(3)","f9868918":"first_looking(df_uk)\nduplicate_values(df_uk)","e710d1d7":"cprint(\" Top 10 Demanded Products According to Quantity\",'red')\ndf_uk.groupby(\"stockcode\")['quantity'].sum().sort_values(ascending=False)[:10]","196ca155":"fig = px.histogram(df_uk, x = df_uk.groupby(\"stockcode\")['quantity'].sum().sort_values(ascending=False)[:10].index,\n                   y = df_uk.groupby(\"stockcode\")['quantity'].sum().sort_values(ascending=False)[:10].values, \n                   title = 'Top 10 Demanded Products According to Quantity',\n                   labels = dict(x = \"stockcode\", y =\"quantity\"))\nfig.show()","2cb9cf5b":"cprint(\"Top 10 Demanded Products According to Total_Price\",'red')\ndf_uk.groupby(\"stockcode\")['total_price'].sum().sort_values(ascending=False)[:10]","6b861876":"fig = px.histogram(df_uk, x = df_uk.groupby(\"stockcode\")['total_price'].sum().sort_values(ascending=False)[:10].index,\n                   y = df_uk.groupby(\"stockcode\")['total_price'].sum().sort_values(ascending=False)[:10].values, \n                   title = 'Top 10 Products According to Total_Price',\n                   labels = dict(x = \"stockcode\", y =\"total_price\"))\nfig.show()","e56db27f":"cprint(\"Top 10 Customers According to Total_Price\",'red')\ndf_uk.groupby(\"customerid\")['total_price'].sum().sort_values(ascending=False).head(10)","ef36da63":"cprint(\"Top 10 Customers According to Total_Price\",'red')\ndf_uk.groupby(\"customerid\")['total_price'].sum().sort_values(ascending=False).head(10).plot(kind=\"bar\", width=0.5, color='green', edgecolor='purple', figsize=(11,6));","3181dd8f":"cprint(\"Top 10 Customers According to Quantity\",'red')\ndf_uk.groupby(\"customerid\")['quantity'].sum().sort_values(ascending=False).head(10)","dd91948b":"cprint(\"Top 10 Customers According to Quantity\",'red')\ndf_uk.groupby(\"customerid\")['quantity'].sum().sort_values(ascending=False).head(10).plot(kind=\"bar\", width=0.5, color='orange', edgecolor='purple', figsize=(11,6));","1805be57":"cprint(\"Top 10 Customers According to Stockcode\",'red')\ndf_uk.groupby(\"customerid\")['stockcode'].nunique().sort_values(ascending=False).head(10)","7faf0b7a":"cprint(\"Top 10 Customers According to Stockcode\",'red')\ndf_uk.groupby(\"customerid\")['stockcode'].nunique().sort_values(ascending=False).head(10).plot(kind=\"bar\", width=0.5, color='brown', edgecolor='purple', figsize=(11,6));","8509bb7d":"import datetime as dt\nfrom datetime import datetime\nimport datetime as dt","4d0ef005":"df_uk.head(1)","a8cb2d5a":"df_uk.info()","3810ed75":"print(\"There is\", df_uk.shape[0], \"observation and\", df_uk.shape[1], \"columns in the dataset\")","963fe6a7":"df_uk[\"ref_date\"] = pd.to_datetime(df_uk['invoicedate']).dt.date.max()\ndf_uk.head(1)","5618b3a5":"df_uk['ref_date'] = pd.to_datetime(df_uk['ref_date'])","2414780a":"df_uk[\"date\"] = pd.to_datetime(df_uk[\"invoicedate\"]).dt.date\ndf_uk.head(1)","74b5b135":"df_uk['date'] = pd.to_datetime(df_uk['date'])","87b99ff0":"df_uk.groupby('customerid')[['date']].max().sample(5)","69bde994":"df_uk['last_purchase_date'] = df_uk.groupby('customerid')['invoicedate'].transform(max)\ndf_uk.head(1)","74d480fd":"df_uk['last_purchase_date'] = pd.to_datetime(df_uk['last_purchase_date']).dt.date\ndf_uk.head(1)","7d126c61":"df_uk['last_purchase_date'] = pd.to_datetime(df_uk['last_purchase_date'])","84f0378f":"df_uk.info()","653e8c86":"df_uk.groupby('customerid')[['last_purchase_date']].max().sample(5)","77d210db":"df_uk.groupby('customerid')[['last_purchase_date']].max().sample(5)","c5c7b6d9":"df_uk[\"customer_recency\"] = df_uk[\"ref_date\"] - df_uk[\"last_purchase_date\"]\ndf_uk.head(1)","37801267":"df_uk['recency_value'] = pd.to_numeric(df_uk['customer_recency'].dt.days.astype('int64'))\ndf_uk.head(1)","928e544e":"customer_recency = pd.DataFrame(df_uk.groupby('customerid')['recency_value'].min())\ncustomer_recency.rename(columns={'recency_value':'recency'}, inplace=True)\ncustomer_recency.reset_index(inplace=True)\ncustomer_recency.sample(5)","521febab":"print(\"There is\", df_uk.shape[0], \"observation and\", df_uk.shape[1], \"columns in the dataset\")","fee101b2":"df_uk.head(1)","e99f8aa1":"df_uk.drop('last_purchase_date', axis=1, inplace=True)\ndf_uk.head(1)","17755bab":"fig = px.histogram(df_uk, x = 'recency_value', title = 'Customer Regency Distribution')\nfig.show()","9135645f":"df_uk1 = df_uk.copy()\ndf_uk1.head(1)","b886c58e":"duplicate_values(df_uk1)","8829c9f1":"customer_frequency = pd.DataFrame(df_uk.groupby('customerid')['invoiceno'].nunique())\ncustomer_frequency.rename(columns={'invoiceno':'frequency'}, inplace=True)\ncustomer_frequency.reset_index(inplace=True)\ncustomer_frequency.sample(5)","aaf9d02a":"df_uk['customer_frequency'] = df_uk.groupby('customerid')['invoiceno'].transform('count')\ndf_uk.head(1)","9666b9b5":"fig = px.histogram(df_uk, x = 'customer_frequency', title = 'Customer Frequency Distribution')\nfig.show()","cf4a00c4":"customer_monetary = pd.DataFrame(df_uk.groupby('customerid')['total_price'].sum())\ncustomer_monetary.rename(columns={'total_price':'monetary'}, inplace=True)\ncustomer_monetary.reset_index(inplace=True)\ncustomer_monetary.sample(5)","40e27192":"df_uk['customer_monetary'] = df_uk.groupby('customerid')['total_price'].transform('sum')\ndf_uk.head(1)","98932607":"fig = px.histogram(df_uk, x = 'customer_monetary', title = 'Customer Monetary Distribution')\nfig.show()","4335f675":"customer_rfm = pd.merge(pd.merge(customer_recency, customer_frequency, on='customerid'), customer_monetary, on='customerid')\ncustomer_rfm.head()","5b10c548":"customer_rfm.info()","172631c8":"missing_values(customer_rfm)","a2d8d724":"duplicate_values(customer_rfm)","abe25bd5":"customer_rfm.set_index('customerid', inplace = True)\ncustomer_rfm.sample(5)","445050fc":"quantiles = customer_rfm.quantile(q = [0.25,0.50,0.75])\nquantiles","89a2a3cf":"def rec_score(x):\n    if x < 17.000:\n        return 4\n    elif 17.000 <= x < 50.000:\n        return 3\n    elif 50.000 <= x < 142.000: \n        return 2\n    else:\n        return 1","7c9bb558":"def freq_score(x):\n    if x > 5.000:\n        return 4\n    elif 5.000 >= x > 2.000:\n        return 3\n    elif 2.000 >= x > 1: \n        return 2\n    else:\n        return 1","5642de27":"def mon_score(x):\n    if x > 1571.285:\n        return 4\n    elif 1571.285 >= x > 644.975:\n        return 3\n    elif 644.975 >= x > 298.185: \n        return 2\n    else:\n        return 1","5ef436f9":"customer_rfm['recency_score'] = customer_rfm['recency'].apply(rec_score)\ncustomer_rfm['frequency_score'] = customer_rfm['frequency'].apply(freq_score)\ncustomer_rfm['monetary_score'] = customer_rfm['monetary'].apply(mon_score)\ncustomer_rfm.sample(5)","d505be9a":"customer_rfm.info()","8f2c8fea":"fig = px.pie(df, values = customer_rfm['recency_score'].value_counts(), \n             names = (customer_rfm[\"recency_score\"].value_counts()).index, \n             title = 'Recency Score Distribution')\nfig.show()","db07dcea":"fig = px.pie(df, values = customer_rfm['frequency_score'].value_counts(), \n             names = (customer_rfm[\"frequency_score\"].value_counts()).index, \n             title = 'Frequency_Score Distribution')\nfig.show()","02a57977":"fig = px.pie(df, values = customer_rfm['monetary_score'].value_counts(), \n             names = (customer_rfm[\"monetary_score\"].value_counts()).index, \n             title = 'Monetary_Score Distribution')\nfig.show()","e5c983ec":"customer_rfm['RFM_score'] = (customer_rfm['recency_score'].astype(str) + customer_rfm['frequency_score'].astype(str) + \n                            customer_rfm['monetary_score'].astype(str))\ncustomer_rfm.sample(5)","ed3602ae":"customer_rfm['RFM_score'].value_counts()","ea70867e":"customer_rfm.info()","803dd6ab":"customer_rfm['RFM_score'].value_counts()","c94431b9":"fig = px.histogram(customer_rfm, x = customer_rfm['RFM_score'].value_counts().index, \n                   y = customer_rfm['RFM_score'].value_counts().values, \n                   title = 'Customer_RFM Distribution',\n                   labels = dict(x = \"RFM_score\", y =\"counts\"))\nfig.show()","20ce12fc":"customer_rfm['RFM_level'] = customer_rfm['recency_score'] + customer_rfm['frequency_score'] + customer_rfm['monetary_score']\ncustomer_rfm.sample(5)","b53b4810":"customer_rfm['RFM_level'].value_counts()","f7e6e7ad":"print('Min value for RFM_level : ', customer_rfm['RFM_level'].min())\nprint('Max value for RFM_level : ', customer_rfm['RFM_level'].max())","df905bff":"np.sort(customer_rfm['RFM_level'].unique())","f7eeaa9d":"customer_rfm.info()","c33b06a2":"fig = px.pie(df, values = customer_rfm['RFM_level'].value_counts(), \n             names = (customer_rfm[\"RFM_level\"].value_counts()).index, \n             title = 'RFM_level Distribution')\nfig.show()","ac82f6c1":"def segments(df_rfm):\n    if df_rfm['RFM_level'] == 12 :\n        return 'champion'\n    elif (df_rfm['RFM_level'] == 11) or (df_rfm['RFM_level'] == 10 ):\n        return 'loyal_customer'\n    elif (df_rfm['RFM_level'] == 9) or (df_rfm['RFM_level'] == 8 ):\n        return 'promising'\n    elif (df_rfm['RFM_level'] == 7) or (df_rfm['RFM_level'] == 6 ):\n        return 'need_attention'\n    elif (df_rfm['RFM_level'] == 5) or (df_rfm['RFM_level'] == 4 ):\n        return 'hibernating'\n    else:  \n        return 'almost_lost'","76f79767":"customer_rfm['customer_segment'] = customer_rfm.apply(segments,axis=1)\ncustomer_rfm.sample(5)","4ff8c65f":"customer_rfm['customer_segment'].value_counts()","a3f1e093":"customer_rfm['customer_segment'].value_counts(dropna=False, normalize=True)","8a9ecf57":"customer_rfm[customer_rfm['customer_segment'] == 'champion'].sample(5)","8be59044":"customer_rfm[customer_rfm['customer_segment'] == 'loyal_customer'].sample(5)","32ce7abb":"customer_rfm[customer_rfm['customer_segment'] == 'promising'].sample(5)","5abecf29":"customer_rfm[customer_rfm['customer_segment'] == 'need_attention'].sample(5)","115df19f":"customer_rfm[customer_rfm['customer_segment'] == 'almost_lost'].sample(5)","ca5e3150":"fig = px.histogram(customer_rfm, x = customer_rfm['customer_segment'].value_counts().index, \n                   y = customer_rfm['customer_segment'].value_counts().values, \n                   title = 'Customer_Segment Distribution',\n                   labels = dict(x = \"customer_segment\", y =\"counts\"))\nfig.show()","bcfaa6a9":"fig = px.pie(df, values = customer_rfm['customer_segment'].value_counts(), \n             names = (customer_rfm[\"customer_segment\"].value_counts()).index, \n             title = 'Customer_Segment Distribution')\nfig.show()","efac369c":"avg_rfm_segment = customer_rfm.groupby('customer_segment').RFM_level.mean().sort_values(ascending=False)\navg_rfm_segment","7ecf0b6b":"fig = px.histogram(customer_rfm, x = customer_rfm.groupby('customer_segment').RFM_level.mean().sort_values(ascending=False).index, \n                   y = customer_rfm.groupby('customer_segment').RFM_level.mean().sort_values(ascending=False).values, \n                   title = 'Customer_Segment RFM_level Mean Values',\n                   labels = dict(x = \"customer_segment\", y =\"RFM_level Mean Values\"))\nfig.show()","6643601f":"size_rfm_segment = customer_rfm['customer_segment'].value_counts()\nsize_rfm_segment","0ead0593":"fig = px.histogram(customer_rfm, x = customer_rfm['customer_segment'].value_counts().index, \n                   y = customer_rfm['customer_segment'].value_counts().values, \n                   title = 'Size RFM_Segment',\n                   labels = dict(x = \"customer_segment\", y =\"Size RFM_Segment\"))\nfig.show()","bc0c8cfb":"customer_segment = pd.DataFrame(pd.concat([avg_rfm_segment, size_rfm_segment], axis=1))\ncustomer_segment.rename(columns={'RFM_level': 'avg_RFM_level', 'customer_segment': 'segment_size'}, inplace=True)\ncustomer_segment","191f0706":"#import squarify as sq\n\nfig = plt.gcf()\nax = fig.add_subplot()\nfig.set_size_inches(15, 8)\nsq.plot(sizes=customer_segment['segment_size'], \n              label=['champions',\n                     'loyal_customer',\n                     'promising',\n                     'need_attention',\n                     'hibernating', \n                     'almost_lost'], alpha=0.8 )\nplt.title(\"RFM Segments\",fontsize=18,fontweight=\"bold\")\nplt.axis('off')\nplt.show()","6a5baa55":"# import plotly.express as px\n\nfig = px.treemap(customer_segment,\n                 path=[customer_segment.index], \n                 values='segment_size', \n                 width=900, \n                 height=600)\nfig.update_layout(title=\"RFM Segments\",\n                  title_x = 0.5, title_font = dict(size=18),\n                 )\nfig.update_layout(margin = dict(t=50, l=25, r=25, b=25))\nfig.show()","25b70578":"# from wordcloud import WordCloud\n\nsegment_text = customer_rfm[\"customer_segment\"].str.split(\" \").str.join(\"_\")\nall_segments = \" \".join(segment_text)\n\nwc = WordCloud(background_color=\"orange\", \n               max_words=250, \n               max_font_size=256, \n               random_state=42,\n               width=800, height=400)\nwc.generate(all_segments)\nplt.figure(figsize = (16, 15))\nplt.imshow(wc)\nplt.title(\"RFM Segments\",fontsize=18,fontweight=\"bold\")\nplt.axis('off')\nplt.show()","a0bd44da":"customer_rfm['customer_segment'].value_counts()","56f812e0":"customer_rfm.columns","f9a58f18":"# from sklearn.cluster import KMeans\n# from sklearn.decomposition import PCA\n# from sklearn.preprocessing import LabelEncoder, MinMaxScaler","1b1b5d63":"customer_rfm.head()","86f0a725":"first_looking(customer_rfm)\nduplicate_values(customer_rfm)","010bc2cf":"customer_rfm.groupby('rfm_level').agg({'recency': ['mean','min','max','count'],\n                                       'frequency': ['mean','min','max','count'],\n                                       'monetary': ['mean','min','max','count'] }).round(1)","ff929583":"customer_rfm.corr()","e2f6a903":"plt.figure(figsize=(10,7))\nsns.heatmap(customer_rfm.corr(),annot=True, cmap=\"coolwarm\");","0bf901d2":"df_temp = customer_rfm.corr()\n\ncount = \"Done\"\nfeature =[]\ncollinear=[]\nfor col in df_temp.columns:\n    for i in df_temp.index:\n        if (df_temp[col][i]> .9 and df_temp[col][i] < 1) or (df_temp[col][i]< -.9 and df_temp[col][i] > -1) :\n                feature.append(col)\n                collinear.append(i)\n                print(Fore.RED + f\"\\033[1mmulticolinearity alert in between\\033[0m {col} - {i}\")\n        else:\n            print(f\"For {col} and {i}, there is NO multicollinearity problem\") \n\nprint(\"\\033[1mThe number of strong corelated features:\\033[0m\", count) ","a1074361":"plt.figure(figsize = (20,20))\nsns.pairplot(customer_rfm[['recency', 'frequency', 'monetary','customer_segment']], hue = 'customer_segment');","5a2a778f":"for i in customer_rfm:\n    customer_rfm[i].iplot(kind=\"box\", title=i, boxpoints=\"all\")","02220d8c":"for i in customer_rfm:\n    customer_rfm[i].iplot(kind=\"histogram\", title=i, subplots=True,bins=50)","a6d10582":"matrix = np.triu(customer_rfm[['recency','frequency','monetary']].corr())\nfig, ax = plt.subplots(figsize=(14,10)) \nsns.heatmap (customer_rfm[['recency','frequency','monetary']].corr(), annot=True, fmt= '.2f', vmin=-1, vmax=1, center=0, cmap='coolwarm',mask=matrix, ax=ax);","13f03c06":"skew_vals = customer_rfm.skew().sort_values(ascending=False)\nskew_vals","887e708a":"skew_limit = 0.75 # This is our threshold-limit to evaluate skewness. Overall below abs(1) seems acceptable for the linear models. \nskew_vals = customer_rfm.skew()\nskew_cols = skew_vals[abs(skew_vals)> skew_limit].sort_values(ascending=False)\nskew_cols ","65bb1e19":"#Interpreting Skewness \n\nfor skew in skew_vals:\n    if -0.75 < skew < 0.75:\n        print (\"A skewness value of\", '\\033[1m', Fore.GREEN, skew, '\\033[0m', \"means that the distribution is approx.\", '\\033[1m', Fore.GREEN, \"symmetric\", '\\033[0m')\n    elif  -0.75 < skew < -1.0 or 0.75 < skew < 1.0:\n        print (\"A skewness value of\", '\\033[1m', Fore.YELLOW, skew, '\\033[0m', \"means that the distribution is approx.\", '\\033[1m', Fore.YELLOW, \"moderately skewed\", '\\033[0m')\n    else:\n        print (\"A skewness value of\", '\\033[1m', Fore.RED, skew, '\\033[0m', \"means that the distribution is approx.\", '\\033[1m', Fore.RED, \"highly skewed\", '\\033[0m')","a344ef19":"customer_rfm[skew_cols.index].iplot(kind='histogram',subplots=True,bins=50)","687b8a25":"rfm_log = customer_rfm[['recency', 'frequency', 'monetary']].apply(np.log1p, axis = 1).round(3)\nrfm_log.head()","114dc734":"rfm_log.skew()","d57f99ab":"rfm_log.iplot(kind='histogram',subplots=True,bins=50);","72a3d0ef":"rfm_trans = customer_rfm[['recency', 'frequency', 'monetary']]\npt = PowerTransformer(method='yeo-johnson')\ntrans= pt.fit_transform(rfm_trans)\nrfm_trans = pd.DataFrame(trans, columns =skew_cols.index)\nrfm_trans.head()","0937a892":"rfm_trans.skew()","6008dc5b":"rfm_trans.iplot(kind='histogram',subplots=True,bins=50);","c5bbfcd2":"plt.figure(figsize = (20,20))\nsns.pairplot(rfm_trans);","a2a554f0":"fig = px.scatter(data_frame = rfm_trans)\nfig.show()","5092538b":"for i in rfm_trans:\n    rfm_trans[i].iplot(kind=\"box\", title=i, boxpoints=\"all\")","2c1f1654":"# !pip install -U matplotlib\n# from mpl_toolkits.mplot3d import Axes3D \nfig = px.scatter_3d(rfm_trans, \n                    x='recency',\n                    y='frequency',\n                    z='monetary',\n                    color='recency')\nfig.show();","e6735fc3":"pip install pyclustertend","1e6f0254":"from sklearn.cluster import KMeans, AgglomerativeClustering\nfrom pyclustertend import hopkins\nfrom sklearn.preprocessing import scale\nfrom sklearn.metrics.cluster import adjusted_rand_score\nfrom sklearn.metrics import silhouette_samples,silhouette_score\nfrom scipy.cluster.hierarchy import linkage, dendrogram","25964ed5":"hopkins(rfm_trans,rfm_trans.shape[0])","51f11ebe":"#First : Get the Best KMeans \nks = range(1,10)\ninertias=[]\nfor k in ks :\n    # Create a KMeans clusters\n    kc = KMeans(n_clusters=k,random_state=1)\n    kc.fit(rfm_trans)\n    inertias.append(kc.inertia_)\n\n# Plot ks vs inertias\nf, ax = plt.subplots(figsize=(8, 6))\nplt.plot(ks, inertias, '-o')\nplt.xlabel('Number of clusters, k')\nplt.ylabel('Inertia')\nplt.xticks(ks)\nplt.style.use('ggplot')\nplt.title('What is the Best Number for KMeans ?')\nplt.show()","98cf1f4e":"from yellowbrick.cluster import KElbowVisualizer\nkmeans = KMeans()\nvisu = KElbowVisualizer(kmeans, k = (1,10))\nvisu.fit(rfm_trans)\nvisu.show();","bdc5ecfd":"ssd =[]\n\nK = range(2,10)\n\nfor k in K:\n    model = KMeans(n_clusters=k)\n    model.fit(rfm_trans)\n    ssd.append(model.inertia_)\n    print(f'Silhouette Score for {k} clusters: {silhouette_score(rfm_trans, model.labels_)}')","1486912b":"from sklearn.cluster import KMeans\n\nfrom yellowbrick.cluster import SilhouetteVisualizer\n\nmodel_3 = KMeans(n_clusters=3, random_state=101)\nvisualizer = SilhouetteVisualizer(model_3)\nvisualizer.fit(rfm_trans)    # Fit the data to the visualizer\nvisualizer.poof();","53f6d045":"k_means_model = KMeans(n_clusters = 3, random_state = 101)\nk_means_model.fit_predict(rfm_trans)\nlabels = k_means_model.labels_\nlabels","bf74f7db":"rfm_trans['predicted_clusters'] = labels\nrfm_trans","9647f22e":"rfm_trans['predicted_clusters'].value_counts()","e64c497d":"fig = px.pie(df, values = rfm_trans['predicted_clusters'].value_counts(), \n             names = (rfm_trans['predicted_clusters'].value_counts()).index, \n             title = 'Predicted_Clusters Distribution')\nfig.show()","6c4868b2":"customer_rfm['predicted_clusters'] = labels\ncustomer_rfm","529239ee":"f, (ax1, ax2, ax3) = plt.subplots(1, 3, sharey=True, figsize=(11, 6)) # sharey=True ile y eksen labels lari ortak kullanirlar.\n\nax1.set_title('Recency-Frequency')\nax1.set_xlabel('Recency')\nax1.set_ylabel('Frequency')\nax1.scatter(rfm_trans.iloc[:,0], rfm_trans.iloc[:, 1], c=kmeans.labels_, cmap=\"rainbow\")\nax1.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300,alpha=0.9, label = 'Centroids')\n\nax2.set_title(\"Frequency-Monetary\")\nax2.set_xlabel('Frequency')\nax2.set_ylabel('Monetary')\nax2.scatter(rfm_trans.iloc[:,1], rfm_trans.iloc[:, 2], c=kmeans.labels_,cmap=\"rainbow\")\nax2.scatter(kmeans.cluster_centers_[:, 1], kmeans.cluster_centers_[:, 2], s=300,alpha=0.9, label = 'Centroids')\n\nax3.set_title(\"Recency-Monetary\")\nax3.set_xlabel('Recency')\nax3.set_ylabel('Monetary')\nax3.scatter(rfm_trans.iloc[:, 0], rfm_trans.iloc[:, 2], c=kmeans.labels_,cmap=\"rainbow\")\nax3.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 2], s=300,alpha=0.9, label = 'Centroids');","0a0d7ab4":"plt.figure(figsize=(15, 8))\nplt.scatter(rfm_trans.iloc[:, 0], rfm_trans.iloc[:, 1], c = labels, s = 50, cmap = \"viridis\")\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=200, c='red',alpha=0.5, label = 'Centroids');","f425a324":"customer_rfm.head(10)","6b4526b6":"customer_rfm['predicted_clusters'] = customer_rfm['predicted_clusters'].map({1:'almost_lost', 2:'best_customers', 0:'passer_customers'})","5eecdc44":"customer_rfm.head(10)","595f73d8":"customer_rfm['customer_segment'].value_counts()","ea1cfa05":"customer_rfm[customer_rfm['predicted_clusters']=='almost_lost']['customer_segment'].value_counts()","831707c3":"pd.crosstab(customer_rfm['predicted_clusters'], \n            customer_rfm['customer_segment']).iplot(kind = \"bar\", title = 'Compare (predicted-clusters vs customer-segment)',\n                                                   xTitle = 'clusters & segments', yTitle = 'counts')","7653f540":"fig = px.pie(df, values = customer_rfm[customer_rfm['predicted_clusters']=='almost_lost']['customer_segment'].value_counts(), \n             names = customer_rfm[customer_rfm['predicted_clusters']=='almost_lost']['customer_segment'].value_counts().index, \n             title = 'Predicted_Clusters_Almost_Lost Distribution')\nfig.show()","d36fbbf7":"customer_rfm[customer_rfm['predicted_clusters']=='passer_customers']['customer_segment'].value_counts()","fe6b7c6b":"fig = px.pie(df, values = customer_rfm[customer_rfm['predicted_clusters']=='passer_customers']['customer_segment'].value_counts(), \n             names = customer_rfm[customer_rfm['predicted_clusters']=='passer_customers']['customer_segment'].value_counts().index, \n             title = 'Predicted_Clusters_Passer_Customers Distribution')\nfig.show()","382ece22":"customer_rfm[customer_rfm['predicted_clusters']=='best_customers']['customer_segment'].value_counts()","351e792e":"fig = px.pie(df, values = customer_rfm[customer_rfm['predicted_clusters']=='best_customers']['customer_segment'].value_counts(), \n             names = customer_rfm[customer_rfm['predicted_clusters']=='best_customers']['customer_segment'].value_counts().index, \n             title = 'Predicted_Clusters_Best_Customers Distribution')\nfig.show()","03fe5bd3":"customer_rfm['customer_segment'].value_counts()","f0e2e559":"pd.crosstab(customer_rfm['customer_segment'], \n            customer_rfm['predicted_clusters']).iplot(kind=\"bar\", title = 'Compare (customer-segment vs predicted-clusters)',\n            xTitle = 'segments & clusters', yTitle = 'counts')","dcc481c1":"fig = px.pie(df, values = customer_rfm[customer_rfm['customer_segment']=='champion']['predicted_clusters'].value_counts(), \n             names = customer_rfm[customer_rfm['customer_segment']=='champion']['predicted_clusters'].value_counts().index, \n             title = 'Customer_Segment_Champion Distribution')\nfig.show()","2cddd3d4":"fig = px.pie(df, values = customer_rfm[customer_rfm['customer_segment']=='loyal_customer']['predicted_clusters'].value_counts(), \n             names = customer_rfm[customer_rfm['customer_segment']=='loyal_customer']['predicted_clusters'].value_counts().index, \n             title = 'Customer_Segment_Loyal_Customer Distribution')\nfig.show()","f8729363":"fig = px.pie(df, values = customer_rfm[customer_rfm['customer_segment']=='promising']['predicted_clusters'].value_counts(), \n             names = customer_rfm[customer_rfm['customer_segment']=='promising']['predicted_clusters'].value_counts().index, \n             title = 'Customer_Segment_Promising Distribution')\nfig.show()","7f43782e":"fig = px.pie(df, values = customer_rfm[customer_rfm['customer_segment']=='need_attention']['predicted_clusters'].value_counts(), \n             names = customer_rfm[customer_rfm['customer_segment']=='need_attention']['predicted_clusters'].value_counts().index, \n             title = 'Customer_Segment_Need_Attention Distribution')\nfig.show()","2bbf428a":"fig = px.pie(df, values = customer_rfm[customer_rfm['customer_segment']=='hibernating']['predicted_clusters'].value_counts(), \n             names = customer_rfm[customer_rfm['customer_segment']=='hibernating']['predicted_clusters'].value_counts().index, \n             title = 'Customer_Segment_Hibernating Distribution')\nfig.show()","fba982aa":"fig = px.pie(df, values = customer_rfm[customer_rfm['customer_segment']=='almost_lost']['predicted_clusters'].value_counts(), \n             names = customer_rfm[customer_rfm['customer_segment']=='almost_lost']['predicted_clusters'].value_counts().index, \n             title = 'Customer_Segment_Almost_Lost Distribution')\nfig.show()","117dba7b":"df_uk[\"invoicedate\"] = pd.to_datetime(df_uk[\"invoicedate\"])","268f55ac":"def get_month(x) : return dt.datetime(x.year,x.month,1)","e85b883c":"df_uk['invoice_month'] = df_uk['invoicedate'].apply(get_month)\ngrouping = df_uk.groupby('customerid')['invoice_month']\ndf_uk['cohort_month'] = grouping.transform('min')\ndf_uk.tail()","f052b785":"def get_date_int (dframe,column):\n    year = dframe[column].dt.year\n    month = dframe[column].dt.month\n    day = dframe[column].dt.day\n    return year, month , day ","44cc8493":"invoice_year,invoice_month,_ = get_date_int(df_uk,'invoice_month')\ncohort_year,cohort_month,_ = get_date_int(df_uk,'cohort_month')","88ccc1c1":"year_diff = invoice_year - cohort_year \nmonth_diff = invoice_month - cohort_month \n\ndf_uk['cohort_index'] = year_diff * 12 + month_diff + 1 \ndf_uk","1c27bb8c":"#Count monthly active customers from each cohort\ngrouping = df_uk.groupby(by=['cohort_month', 'cohort_index'])\ncohort_data = grouping['customerid'].apply(pd.Series.nunique)\ncohort_data","2141ddfd":"# Return number of unique elements in the object.\ncohort_data = cohort_data.reset_index()\ncohort_counts = cohort_data.pivot(index='cohort_month',columns='cohort_index',values='customerid')\ncohort_counts","ee2ca470":"# Retention table\ncohort_size = cohort_counts.iloc[:,0]\nretention = cohort_counts.divide(cohort_size,axis=0) #axis=0 to ensure the divide along the row axis \nretention.round(3) * 100 #to show the number as percentage ","1e7d2307":"plt.figure(figsize=(15, 8))\nplt.title('Retention rates')\nsns.heatmap(data=retention,annot = True,fmt = '.0%',vmin = 0.0,vmax = 0.5, cmap=\"BuPu_r\")\nplt.show()","89a2b491":"sns.lineplot(data=cohort_counts[1])","5e3298e8":"cohort_data[cohort_data[\"cohort_index\"]==1]","5ecb417e":"\ngrouping = df_uk.groupby(['cohort_month', 'cohort_index'])\ncohort_data = grouping['quantity'].mean()\ncohort_data = cohort_data.reset_index()\naverage_quantity = cohort_data.pivot(index='cohort_month',columns='cohort_index',values='quantity')\naverage_quantity.round(1)\naverage_quantity.index = average_quantity.index.date","629dea30":"cohort_data","e946c29b":"average_quantity","ff4c8d2d":"plt.figure(figsize=(15, 8))\nplt.title('Average quantity for each cohort')\nsns.heatmap(data=average_quantity,annot = True,vmin = 0.0,vmax =20,cmap=\"BuGn_r\")\nplt.show()","7b2aa3fa":"grouping = df_uk.groupby(['cohort_month', 'cohort_index'])\ncohort_data = grouping['total_price'].mean()\ncohort_data = cohort_data.reset_index()\naverage_sales = cohort_data.pivot(index='cohort_month',columns='cohort_index',values='total_price')\naverage_sales.round(1)\naverage_sales.index = average_sales.index.date\naverage_sales","282cdc3a":"plt.figure(figsize=(15, 8))\nplt.title('Average sales for each cohort')\nsns.heatmap(data=average_sales,annot = True,vmin = 0.0,vmax =20,cmap=\"BuGn_r\")\nplt.show()","2f6be432":"In the age of the internet and e-commerce, companies that do not expand their businesses online or utilize digital tools to reach their customers will run into issues like scalability and a lack of digital precsence. An important marketing strategy e-commerce businesses use for analyzing and predicting customer value is customer segmentation. Customer data is used to sort customers into group based on their behaviors and preferences.\n\n**[RFM](https:\/\/www.putler.com\/rfm-analysis\/) (Recency, Frequency, Monetary) Analysis** is a customer segmentation technique for analyzing customer value based on past buying behavior. RFM analysis was first used by the direct mail industry more than four decades ago, yet it is still an effective way to optimize our marketing.\n<br>\n<br>\nOur goal in this Notebook is to cluster the customers in our data set to:\n - Recognize who are our most valuable customers\n - Increase revenue\n - Increase customer retention\n - Learn more about the trends and behaviors of our customers\n - Define customers that are at risk\n\nWe will start with **RFM Analysis** and then compliment our findings with predictive analysis using **K-Means Clustering Algorithms.**\n\n- RECENCY (R): Time since last purchase\n- FREQUENCY (F): Total number of purchases\n- MONETARY VALUE (M): Total monetary value\n\n\n\n\nBenefits of RFM Analysis\n\n- Increased customer retention\n- Increased response rate\n- Increased conversion rate\n- Increased revenue\n\nRFM Analysis answers the following questions:\n - Who are our best customers?\n - Who has the potential to be converted into more profitable customers?\n - Which customers do we need to retain?\n - Which group of customers is most likely to respond to our marketing campaign?\n ","ed090c0e":"4. Calculate the days since last purchase","7b8215d7":"### iii. Handling with Missing Values","b67935f5":"## K-Means Implementation with rfm_trans DataFrame\n\nFor k-means, we have to set k to the number of clusters we want, but figuring out how many clusters is not obvious from the beginning. We will try different cluster numbers and check their [silhouette coefficient](http:\/\/scikit-learn.org\/stable\/auto_examples\/cluster\/plot_kmeans_silhouette_analysis.html). The silhouette coefficient for a data point measures how similar it is to its assigned cluster from -1 (dissimilar) to 1 (similar). \n<br>\n<br>\n**Note**: K-means is sensitive to initializations because they are critical to qualifty of optima found. Thus, we will use smart initialization called \"Elbow Method\".","1f7fd2a4":"## DATA\n\n\nUsing the [Online Retail dataset](https:\/\/archive.ics.uci.edu\/ml\/datasets\/Online+Retail) from the UCI Machine Learning Repository for exploratory data analysis, ***Customer Segmentation***, ***RFM Analysis***, ***K-Means Clustering*** and ***Cohort Analysis***.\n\nThis is a transnational data set which contains all the transactions occurring between 01\/12\/2010 and 09\/12\/2011 for a UK-based and registered non-store online retail. The company mainly sells unique all-occasion gifts. Many customers of the company are wholesalers.","a7d0d29d":"**Conclusion**\n- Cluster 0 : The first cluster is more related to the \"Almost Lost\" segment as they Haven\u2019t purchased for some time(R=1), but used to purchase frequently and spent a lot.\n- Cluster 1 : The second cluster belongs to the \"Best Customers\" segment which we saw earlier as they purchase recently (R=4), frequent buyers (F=4), and spent the most (M=4)\n- Cluster 2 : The third cluster can be interpreted as passer customers as their last purchase is long ago (R<=1),purchased very few (F>=2 & F < 4) and spent little (M>=4 & M < 4).Company has to come up with new strategies to make them permanent members. Low value customers","5b6aca48":"3. Visualize total cost per country","c3a31598":"1. Create plot and resize it.","8529f98c":"## Context\n\n\"Predict behavior to retain customers. We can analyze all relevant customer data and develop focused customer retention programs.\" [IBM Sample Data Sets]\n\nEach row represents a customer, each column contains customer\u2019s attributes described on the column Metadata.\n\n**The data set includes information about:**\n\n- Customers who left within the last month \u2013 the column is called Churn\n- Services that each customer has signed up for \u2013 phone, multiple lines, internet, online security, online backup, device -protection, tech support, and streaming TV and movies\n- Customer account information \u2013 how long they\u2019ve been a customer, contract, payment method, paperless billing, monthly charges, and total charges\n- Demographic info about customers \u2013 gender, age range, and if they have partners and dependents","94988582":"**stockcode & description**","5f3597ec":"### We will continue analyzing the UK transactions with customer segmentation.","5c578076":"2. Plot RFM distributions","f9fb3a6f":"### ii. Review df_uk DataFrame","4a9f629a":"# 1. Data Cleaning & Exploratory Data Analysis\n\n- Import Modules, Load Data & Data Review\n\n- Follow the Steps Below\n\n    *i. Take a look at relationships between InvoiceNo, Quantity and UnitPrice columns.*\n    \n    *ii. What does the letter \"C\" in the invoiceno column mean?*\n    \n    *iii. Handling Missing Values*\n    \n    *iv. Clean the Data from the Noise and Missing Values*\n    \n    *v. Explore the Orders*\n    \n    *vi. Explore Customers by Country*\n    \n    *vii. Explore the UK Market*","249f3708":"- Let's check it out on a stockcode. ","87a5d8aa":"**Handling with Skewness - np.log**","237d35ac":"**Let's try to get some information by examining the columns in the dataset.**","677713dd":"3. Let's see how this compares to the number of unique products per customer.","f281e817":"**Annotation:**\n\nLimitations of K-means clustering:\n\n1. There is no assurance that it will lead to the global best solution.\n2. Can't deal with different shapes(not circular) and consider one point's probability of belonging to more than one cluster.\n\nThese disadvantages of K-means show that for many datasets (especially low-dimensional datasets), it may not perform as well as we might hope.","1405cf05":"Use the variables created above to calcualte the difference in days and store them in cohort Index column.","7c5793b6":"- The invoiceno values of rows with a negative quantity value begin with C.","88785b84":"### What The Problem Is\n\nFirst of all, to observe the structure of the data and missing values, we can use exploratory data analysis and data visualization techniques.\n\nWe must do descriptive analysis. Because we must understand the relationship of the features to each other and clear the noise and missing values in the data. After that, the data set will be ready for RFM analysis.\n\nBefore starting the RFM Analysis, we will be asked to do some analysis regarding the distribution of *Orders*, *Customers* and *Countries*. These analyzes will help the company develop its sales policies and contribute to the correct use of resources.\n\nWe will notice that the UK not only has the most sales revenue, but also the most customers. So we will continue to analyze only UK transactions in the next RFM Analysis, Customer Segmentation and K-Means Clustering topics.\n\nNext, we will begin RFM Analysis, a customer segmentation technique based on customers' past purchasing behavior. \n\nBy using RFM Analysis, we can enable companies to develop different approaches to different customer segments so that they can get to know their customers better, observe trends better, and increase customer retention and sales revenues.\n\nWe will calculate the Recency, Frequency and Monetary values of the customers in the RFM Analysis we will make using the data consisting of UK transactions. Ultimately, we have to create an RFM table containing these values.\n\nIn the Customer Segmentation section, we will create an RFM Segmentation Table where we segment wer customers by using the RFM table. For example, we can label the best customer as \"Big Spenders\" and the lost customer as \"Lost Customer\".\n\nWe will segment the customers ourselves based on their recency, frequency, and monetary values. But can an **unsupervised learning** model do this better for us? we will use the K-Means algorithm to find the answer to this question. Then we will compare the classification made by the algorithm with the classification we have made werself.\n\nBefore applying K-Means Clustering, we should do data pre-processing. In this context, it will be useful to examine feature correlations and distributions. In addition, the data we apply for K-Means should be normalized.\n\nOn the other hand, we should inform the K-means algorithm about the number of clusters it will predict. we will also try the *** Elbow method *** and *** Silhouette Analysis *** to find the optimum number of clusters.\n\nAfter the above operations, we will have made cluster estimation with K-Means. we should visualize the cluster distribution by using a scatter plot. we can observe the properties of the resulting clusters with the help of the boxplot. Thus we will be able to tag clusters and interpret results.\n\nFinally, we will do Cohort Analysis with the data we used at the beginning, regardless of the analysis we have done before. Cohort analysis is a subset of behavioral analytics that takes the user data and breaks them into related groups for analysis. This analysis can further be used to do customer segmentation and track metrics like retention, churn, and lifetime value.","a28d7a44":"- Since the dataset has 'unitprice' and 'quantity' columns, I'm creating the **total_price** column. I think it can be useful for further analysis. ","2da59a00":"**Drop the observations with missing values**","04ed071c":"3. Group by CustomerID and check the last date of purchase","1fc07b2b":"### About The Features\n\n**The features in the given dataset are:**\n\n**InvoiceNo**: Invoice number. *Nominal*, a 6-digit integral number uniquely assigned to each transaction. If this code starts with letter 'c', it indicates a cancellation. \n<br>\n**StockCode**: Product (item) code. *Nominal*, a 5-digit integral number uniquely assigned to each distinct product.\n<br>\n**Description**: Product (item) name. *Nominal*. \n<br>\n**Quantity**: The quantities of each product (item) per transaction. *Numeric*.\n<br>\n**InvoiceDate**: Invoice Date and time. *Numeric*, the day and time when each transaction was generated.\n<br>\n**UnitPrice**: Unit price. *Numeric*, Product price per unit in sterling.\n<br>\n**CustomerID**: Customer number. *Nominal*, a 5-digit integral number uniquely assigned to each customer.\n<br>\n**Country**: Country name. *Nominal*, the name of the country where each customer resides.","368c5fff":"- For normalization, we used 2 different methods, log1p method and PowerTransformer(method='yeo-johnson') and got different results. Since the results obtained with PowerTransformer(method='yeo-johnson') are better, we will continue with the normalization with the PowerTransformer(method='yeo-johnson') method in the next section. ","194fe799":"We will use this function to extract the integer values for Invoice as well as Cohort Date in 3 seperate series for each of the two columns","19f1afae":"### vi. Create RFM Table\nMerge the recency, frequency and motetary dataframes","fb9e64c9":"# 3. Customer Segmentation with RFM Scores\n\n- Calculate RFM Scoring\n\n    *i. Creating the RFM Segmentation Table*\n \n\n- Plot RFM Segments","81747004":"**Check for dublicated values and get rid of them**","b0c3247b":"## Create 1st Cohort: User number & Retention Rate","6957fd46":"[The Elbow Method](https:\/\/en.wikipedia.org\/wiki\/Elbow_method_(clustering) ","13618859":"### ii. Visualize analysis of cohort 1 using seaborn and matplotlib modules","65edf925":"When we filter canceled orders by Quantity> 0 or filter non-canceled orders by Quantity <0 nothing returns, this confirms that negative values mean the order was canceled. So lets find out how many orders were cancelled?","5250236a":"5. Drop Last_Purchase_Date since we don't need it anymore","596bac4a":"## Future Engineering","328f6d76":"2. How many customers do we have in each segment?","555ca5bf":"### i. Creating the RFM Segmentation Table\n","cdc53a95":"#### The UK not only has the most sales revenue, but also the most customers. Since the majority of this data set contains orders from the UK, we can explore the UK market further by finding out what products the customers buy together and any other buying behaviors to improve our sales and targeting strategy.","fa748df5":"- Number of unique values of 'stockcode' & 'description' columns are different. Let's look why?","c456cce6":"## Create the 2nd Cohort: Average Quantity Sold","04088870":"1. Create two functions, one for Recency and one for Frequency and Monetary. For Recency, customers in the first quarter should be scored as 4, this represents the highest Recency value. Conversely, for Frequency and Monetary, customers in the last quarter should be scored as 4, representing the highest Frequency and Monetary values.","595a2558":"**invoiceno**","69a9eade":"## Data Pre-Processing and Exploring","8b5eed4b":"- When viewing the observation by way of taking example from the 'invoiceno' column, it can be seen that some invoiceno values start with A and C when quantity and total_price below 0. \n- Let's look how many of the observations starts with A or C.","9fb56a8f":"## Create the 3rd Cohort: Average Sales\n","015747e4":"### vii. Explore the UK Market\n","c8f30a39":"### i. Extract the Month of the Purchase\nFirst we will create a function, which takes any date and returns the formatted date with day value as 1st of the same month and Year.","7f3af09d":"3. Plot RFM distributions","d32d4f69":"**unitprice**","97681db8":"1. Create a scatter plot and select cluster centers","db838643":"For e-commerce organisations, cohort analysis is a unique opportunity to find out which clients are the most valuable to their business. By performing Cohort analysis we can get answers to following questions:\n\n- How much effective was a marketing campaign held in a particular time period?\n- Did the strategy employed to improve the conversion rates of Customers worked?\n- Should I focus more on retention rather than acquiring new customers?\n- Are my customer nurturing strategies effective?\n- Which marketing channels bring me the best results?\n- Is there a seasoanlity pattern in Customer behahiour?","3e9e3e34":"Fit the K-Means Algorithm with the optimal number of clusters we decided and save the model to disk.","6cdde5c0":"## Import Modules, Load Data & Data Review","48f2f221":"- According to the cross check result, the customerid values of the observations where the description column has missing values are the same.","354fc54e":"4. Define rfm_level function that tags customers by using RFM_Scrores and Create a new variable RFM_Level","f5e1e189":"#### 9288  of the orders were cancelled. Looking deeper into why these orders were cancelled may prevent future cancellations. Now let's find out what a negative UnitPrice means.\n","8a352a52":"- There are values below zero in quantity and unitprice columns.\n- Let's find the reason.","fc18292c":"![image.png](attachment:image.png)","3b1bd85f":"- The invoiceno values of rows with a negative total price value begin with C. ","1480ef0f":"1. We can use the logarithm method to normalize the values in a column.","56321f72":"### i. Pivot Cohort and Cohort Retention","d46bd330":"Since we will be performing Cohort Analysis based on transaction records of customers, the columns we will be dealing with mainly:\n- Invoice Data\n- CustomerID\n- Price\n- Quantity\n\nThe following steps will performed to generate the Cohort Chart of Retention Rate:\n- Month Extraction from InvioceDate column\n- Assigning Cohort to Each Transaction\n- Assigning Cohort Index to each transaction\n- Calculating number of unique customers in each Group of (ChortDate,Index)\n- Creating Cohort Table for Retention Rate\n- Creating the Cohort Chart using the Cohort Table\n\nThe Detailed information about each step is given below:","cfa4ae54":"2. Calculate the frequency of purchases","f094cda0":"### ii. Calculating time offset in Months i.e. Cohort Index:\nCalculating time offset for each transaction will allows us to report the metrics for each cohort in a comparable fashion.\nFirst, we will create 4 variables that capture the integer value of years, months for Invoice and Cohort Date using the get_date_int() function which we'll create it below.","432a19c0":"If the invoice number starts with the letter \"C\", it means the order was cancelled. Or those who abandon their order.","ec66def4":"2. Visualize number of customer per country","1e81509b":"### Study Structure\n\n- Data Cleaning & Exploratory Data Analysis\n- RFM Analysis\n- Customer Segmentation\n- Applying K-Means Clustering\n- Create Cohort and Conduct Cohort Analysis","3a477b47":"### iii. Visualize the Clusters","5eb2287a":"2. Visualize Cluster Id vs Recency, Cluster Id vs Frequency and Cluster Id vs Monetary using Box plot. Also evaluate the results. ","a91c6245":"### ii. Model Fitting","8422b8a7":"2. Score customers from 1 to 4 by applying the functions we have created. Also create separate score column for each value. ","f5917b4a":"5. Calculate average values for each RFM_Level, and return a size of each segment ","ae8aa944":"### i. Pivot Cohort and Cohort Retention","89ec1072":"2. What are the most popular products that are bought in the UK?","7216fce1":"**segment visualization with Plotly Matplotlib Squarify**","3a555245":"### iv. Frequency: Number of purchases\n\nTo calculate how many times a customer purchased something, we need to count how many invoices each customer has. To calculate the frequency values, follow these steps in order:","62cb90a7":"- As seen above, invoiceno values of observations with negative unitprice values start with 'A'. The letter 'A' most likely represents the word 'Abandoned' or 'Adjust'.","8a4a6b16":"**We have come to the end of the work.** \n**I hope it was useful.** \n**I would be very happy if you send your constructive and educational comments about the kernel.**","d8892b1b":"### iv. Assign the Label","0084b3a8":"### ii. Visualize analysis of cohort 2 using seaborn and matplotlib modules","fe3982f3":"### i. Define and Plot Feature Correlations","f75acfa2":"**stockcode**","2bc95c45":"1. Calculate sum total cost by customers and named \"Monetary\"","d090d0d4":"**customerid**","2ef7f8ef":"### iii. Recency: Days since last purchase\nTo calculate the recency values, follow these steps in order:\n\n1. To calculate recency, we need to choose a date as a point of reference to evaluate how many days ago was the customer's last purchase.\n2. Create a new column called Date which contains the invoice date without the timestamp\n3. Group by CustomerID and check the last date of purchase\n4. Calculate the days since last purchase\n5. Drop Last_Purchase_Date since we don't need it anymore\n6. Plot RFM distributions","5c9eeb79":"# 5. Create Cohort & Conduct Cohort Analysis\n\n- Future Engineering\n\n    *i. Extract the Month of the Purchase*\n \n    *ii. Calculating time offset in Months i.e. Cohort Index*\n \n\n- Create 1st Cohort: User Number & Retention Rate \n\n    *i. Pivot Cohort and Cohort Retention*\n \n    *ii. Visualize analysis of cohort 1 using seaborn and matplotlib*\n\n\n- Create 2nd Cohort: Average Quantity Sold \n\n    *i. Pivot Cohort and Cohort Retention*\n \n    *ii. Visualize analysis of cohort 2 using seaborn and matplotlib*\n\n\n- Create 3rd Cohort: Average Sales\n\n    *i. Pivot Cohort and Cohort Retention*\n \n    *ii. Visualize analysis of cohort 3 using seaborn and matplotlib*\n\n[Cohort Analysis](https:\/\/medium.com\/swlh\/cohort-analysis-using-python-and-pandas-d2a60f4d0a4d) is specifically useful in analyzing user growth patterns for products. In terms of a product, a cohort can be a group of people with the same sign-up date, the same usage starts month\/date, or the same traffic source.\nCohort analysis is an analytics method by which these groups can be tracked over time for finding key insights. This analysis can further be used to do customer segmentation and track metrics like retention, churn, and lifetime value.\n\nFor e-commerce organizations, cohort analysis is a unique opportunity to find out which clients are the most valuable to their business. By performing Cohort analysis we can get the following answers to the following questions:\n\n- How much effective was a marketing campaign held in a particular time period?\n- Did the strategy employ to improve the conversion rates of Customers worked?\n- Should I focus more on retention rather than acquiring new customers?\n- Are my customer nurturing strategies effective?\n- Which marketing channels bring me the best results?\n- Is there a seasonality pattern in Customer behavior?\n- Along with various performance measures\/metrics for our organization.","516ef100":"**segment visualization with WordCloud**","5696ef2d":"1. What's the total revenue per country?","39c54a2a":"- There is a difference between the number of customerids received according to countries and the number of customerids received as unique. \n- This difference is due to entering 2 different country information for 8 customers. ","7a9ed6e6":"Now we will use the function created above to convert all the invoice dates into respective month date format.","33e9238b":"- We saw that the unique value numbers of the stockcode and description columns are different, so we did the operations we saw above.\n- As can be seen above, there are 2 different descriptions for the item with stockcode 22139.0.\n- The difference between the unique value numbers is due to more than one description entered for an item and the missing value in the description column.\n- As can be seen above, while there should be a unique description value for each unique stockcode, more than one description was entered. ","bba7d216":"Let's assume that the orders which do NOT have customer ID's were not made by the customers already in the dataset because the customers who in fact made some purchases already have ID's. \n\nSo we don't want to assign these orders to those customers because this would alter the insights we draw from the data. ","70d9604c":"# 2. RFM Analysis\n\n- Follow the steps below\n\n   *i. Import Libraries*\n   \n   *ii. Review \"df_uk\" DataFrame*\n   \n   *iii. Calculate Recency*\n   \n   *iv. Calculate Frequency*\n   \n   *v. Calculate Monetary Values*\n   \n   *vi. Create RFM Table*","39a26ef8":"- Above, we have seen that observations with a missing description value have also missing customerid values. When I drop the observations that have missing customerid value from the dataset, the observations with missing description value will also be dropped.\n- Here, the following operations are performed to show how to fill the missing value with the fill_most function defined by the user. ","9addf4de":"Now that we have our customers segmented into 6 different categories, we can gain further insight into customer behavior by using predictive models in conjuction with out RFM model.\nPossible algorithms include **Logistic Regression**, **K-means Clustering**, and **K-nearest Neighbor**. We will go with [K-Means](https:\/\/towardsdatascience.com\/understanding-k-means-clustering-in-machine-learning-6a6e67336aa1) since we already have our distinct groups determined. K-means has also been widely used for market segmentation and has the advantage of being simple to implement.","30b3720c":"1. Create df_uk DataFrame","4521c5db":"### ii. Visualize analysis of cohort 3 using seaborn and matplotlib modules","726c472e":"1. Find the unique number of InvoiceNo  per customer","5072f6f6":"### iii. Data Normalization","e0bd946f":"**description**","4f7f2dcf":"## PREFACE","eb16d314":"1. Choose a date as a point of reference to evaluate how many days ago was the customer's last purchase.","6ab0694f":"### i. Import Libraries","6bdda023":"- The fill_most function finds the most repeating value from the description column values corresponding to each value in the stockcode column and fills the missing values in the description column with this most repeated value (MODE). ","f40471f8":"[Silhouette Coefficient](http:\/\/scikit-learn.org\/stable\/auto_examples\/cluster\/plot_kmeans_silhouette_analysis.html)","dfa351de":"- Invoiceno values of observations with a Quantity value below 0 start with C. ","c9133060":"2. Create a new column called Date which contains the invoice date without the timestamp","d6e2b7dc":"### vi. Explore Customers by Country","6933a6fb":"3. Now that scored each customer, we'll combine the scores for segmentation.","d6b4d564":"6. Plot RFM distributions","fec3d608":"### i. Define the Optimal Number of Clusters","0adcf9d8":"# 4. Applying K-Means Clustering\n\n- Data Pre-Processing and Exploring\n\n    *i. Define and Plot Feature Correlations*\n \n    *ii. Visualize Feature Distributions*\n \n    *iii. Data Normalization*\n\n\n- K-Means Implementation\n\n    *i. Define Optimal Cluster Number (K) by using \"Elbow Method\" and \"Silhouette Analysis\"*\n \n    *ii. Visualize the Clusters*\n \n    *iii. Assign the label*\n \n    *iv. Conclusion*","1342369d":"- Invoiceno values of observations with a unitprice value below 0 start with A.","a69f8cef":"- As seen above, invoiceno values of observations with negative unitprice values start with 'A'. The letter 'A' most likely represents the word 'Abandoned' or 'Adjust'.\n- We see that there are negative values in the Quantity and UnitPrice columns. These are possibly canceled and returned orders.","29603c7e":"### v. Conclusion\n\nCompare customer segmentation labels from the Customer Segmentation with the labels found by K-Means.","978b9f7e":"## Calculate RFM Scoring\n\nThe simplest way to create customer segments from an RFM model is by using **Quartiles**. We will assign a score from 1 to 4 to each category (Recency, Frequency, and Monetary) with 4 being the highest\/best value. The final RFM score is calculated by combining all RFM values. For Customer Segmentation, we will use the df_rfm data set resulting from the RFM analysis.\n<br>\n<br>\n**Note**: Data can be assigned into more groups for better granularity, but we will use 4 in this case.","cccd515d":"How we want to continue this analysis depends on how the business plans to use the results and the level of granularity the business stakeholders want to see in the clusters. We can also ask what range of customer behavior from high to low value customers are the stakeholders interested in exploring. From those answers, various methods of clustering can be used and applied on RFM variable or directly on the transaction data set.","a91a95d6":"2. Plot normalized data with scatter matrix or pairplot. Also evaluate results.","518a028e":"1. Make a copy of df_uk and drop duplicates","a7660b75":"**invoicedate**","8a5a0cb4":"Note That : The min for unit price = 0 and the min for Quantity with negative value","9f425baf":"### ii. Visualize Feature Distributions\n\nTo get a better understanding of the dataset, we can costruct a scatter matrix of each of the three features in the RFM data.","fc67f468":"2. What's the average number of unqiue items per order or per customer?","92251e40":"### i. Pivot Cohort and Cohort Retention","027f4c8d":"- We see that there are negative values in the Quantity and UnitPrice columns. These are possibly canceled and returned orders. Let's check it out.","91c043ba":"**quantity**","92cfc35f":"Businesses have this ever-lasting urge to understand their customers. The better you understand the customer, the better you serve them, and the higher the financial gain you receive from that customer. Since the dawn of trade, this process of understanding customers for a strategic gain has been there practiced and this task is known majorly as [Customer Segmentation](https:\/\/clevertap.com\/blog\/rfm-analysis\/).\nWell as the name suggests, Customer Segmentation could segment customers according to their precise needs. Some of the common ways of segmenting customers are based on their Recency-Frequency-Monatory values, their demographics like gender, region, country, etc, and some of their business-crafted scores. We will use Recency-Frequency-Monatory values for this case.\n\nIn this section, we will create an RFM Segmentation Table where we segment our customers by using the RFM table. For example, we can label the best customer as \"Big Spenders\" and the lost customer as \"Lost Customer\".","5e8f5afa":"Create Heatmap and evaluate the results ","973cdef2":"### v. Explore the Orders\n","e9e33ec2":"### iv. Clean the Data from the Noise and Missing Values","d362d2ce":"Using customer segmentation categories found [here](http:\/\/www.blastam.com\/blog\/rfm-analysis-boosts-sales) we can formulate different marketing strategies and approaches for customer engagement for each type of customer.\n\nNote: The author in the article scores 1 as the highest and 4 as the lowest","2831f088":"### i. Take a look at relationships between InvoiceNo, Quantity and UnitPrice columns.","38173398":"As seen above, invoiceno values of observations with negative quantity values start with 'C'. The letter 'C' most likely represents the word 'Cancelled'.","766c7c9c":"Welcome to \"RFM Customer Segmentation & Cohort Analysis Study\".\n\nIn this study, we will apply RFM Analysis and Customer Segmentation using K-Means Clustering.\n\nBefore diving into the project, please take a look at the determines and project structure.","6646c37b":"**Get rid of cancellations and negative values**","8f3ebd6f":"1. Divide the df_rfm into quarters","198adbb2":"### ii. What does the letter \"C\" in the InvoiceNo column mean?","a304fec1":"## Plot RFM Segments","564e54b9":"**Handling with Skewness - Power Transformer**","386cc840":"### v. Monetary: Total amount of money spent\n\nThe monetary value is calculated by adding together the cost of the customers' purchases.\n","c6d9aebd":"**segment visualization with Plotly TreeMap**","74550b01":"# <h3 style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align: center; border-radius:10px 10px;\" class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" role=\"tablist\" aria-controls=\"home\">RFM Analysis for Customer Segmentation<\/h3>"}}