{"cell_type":{"d4d45225":"code","207d979e":"code","466f5e3a":"code","1253316a":"code","e3f68946":"code","efc824a5":"code","48f822e7":"code","3a27f3d9":"code","a3f32d96":"code","29c07859":"code","75a9624c":"code","a8ce44a6":"code","c89270f1":"code","8881c399":"markdown","a1267d22":"markdown","69c53c07":"markdown","0045c68e":"markdown"},"source":{"d4d45225":"import glob\nimport logging\nimport os\nimport pickle\nimport random\nimport re\nimport shutil\nimport sys\nimport pathlib\nimport itertools\nimport collections\nimport functools\nfrom typing import Dict, List, Tuple\n\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom sklearn import model_selection\nfrom torch import distributed\nfrom torch.nn.utils import rnn as rnn_utils\nfrom torch.utils import data\nfrom torch.utils.data import distributed as distributed_utils\nfrom tqdm.notebook import tqdm, trange\nfrom transformers import (\n    WEIGHTS_NAME,\n    AdamW,\n    AutoConfig,\n    AutoModelWithLMHead,\n    AutoTokenizer,\n    PreTrainedModel,\n    PreTrainedTokenizer,\n    get_linear_schedule_with_warmup,\n)\n\ntry:\n    from torch.utils import tensorboard\nexcept ImportError:\n    import tensorboardX as tensorboard","207d979e":"logging.basicConfig(stream=sys.stdout, level=logging.INFO,\n                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\nlogger.info('Logger initialized')\n\n\n# Args to allow for easy conversion of python script to notebook\nclass Args():\n    def __init__(self):\n        self.output_dir = 'output-small'\n        self.model_type = 'gpt2'\n        self.model_name_or_path = 'microsoft\/DialoGPT-small'\n        self.config_name = 'microsoft\/DialoGPT-small'\n        self.tokenizer_name = 'microsoft\/DialoGPT-small'\n        self.cache_dir = 'cached'\n        self.block_size = 512\n        self.do_train = True\n        self.do_eval = True\n        self.evaluate_during_training = False\n        self.per_gpu_train_batch_size = 2\n        self.per_gpu_eval_batch_size = 2\n        self.gradient_accumulation_steps = 2\n        self.learning_rate = 5e-5\n        self.weight_decay = 0.0\n        self.adam_epsilon = 1e-8\n        self.max_grad_norm = 1.0\n        self.num_train_epochs = 6\n        self.max_steps = -1\n        self.warmup_steps = 0\n        self.logging_steps = 1000\n        self.save_steps = 3500\n        self.save_total_limit = None\n        self.eval_all_checkpoints = False\n        self.no_cuda = False\n        self.overwrite_output_dir = True\n        self.overwrite_cache = True\n        self.should_continue = False\n        self.seed = 42\n        self.local_rank = -1\n        self.fp16 = False\n        self.fp16_opt_level = 'O1'\n        self.n_gpu = torch.cuda.device_count()","466f5e3a":"class ConversationLabelDataset(data.Dataset):\n    def __init__(self, tokenizer: PreTrainedTokenizer, args, contexted_dataframes, block_size=512):\n        self.tokenizer = tokenizer\n        \n        block_size = block_size - (tokenizer.model_max_length - tokenizer.max_len_single_sentence)\n\n        directory = args.cache_dir\n        cached_features_file = os.path.join(\n            directory, args.model_type + \"_cached_lm_\" + str(block_size)\n        )\n\n        if os.path.exists(cached_features_file) and not args.overwrite_cache:\n            logger.info(\"Loading features from cached file %s\", cached_features_file)\n            with open(cached_features_file, \"rb\") as handle:\n                self.samples = pickle.load(handle)\n        else:\n            logger.info(\"Creating features from dataset file at %s\", directory)\n\n            self.samples = []\n            for df in contexted_dataframes:\n                sample = self.df_construct_conv(df, tokenizer)\n                if sample is not None:\n                    self.samples.append(sample)\n\n            logger.info(\"Saving features into cached file %s\", cached_features_file)\n            with open(cached_features_file, \"wb\") as handle:\n                pickle.dump(self.samples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, item):\n        sample = self.samples[item]\n        \n        torch_sample = {name: torch.tensor(item, dtype=torch.long) for name, item in sample.items()}\n        \n        return torch_sample\n    \n    @staticmethod\n    def df_construct_conv(df, tokenizer):\n        # columns: 'name', 'utterance'\n        with_names = df['name'] + \": \" + df['utterance']\n        tokenized = with_names.apply(tokenizer.encode)\n        tokenized.apply(lambda x: x.append(tokenizer.eos_token_id))\n        conv = np.asarray(list(itertools.chain.from_iterable(tokenized)))\n        \n        if len(conv) > 1023:\n            logger.warning(f'Skipping a large context documents')\n            return None\n\n        # mask all labels except for the final utterance\n        last_utterance_len = len(tokenizer.encode(df['utterance'].iloc[-1])) + 1  # + 1 is for the endoftext token\n        labels = conv.copy()\n        labels[:-last_utterance_len] = -100\n        \n        sample = {'input_ids': conv, 'labels': labels}\n        return sample\n\n\nclass ConversationDataset(data.Dataset):\n    def __init__(self, tokenizer: PreTrainedTokenizer, args, df, block_size=512):\n        self.tokenizer = tokenizer\n        block_size = block_size - (tokenizer.model_max_length - tokenizer.max_len_single_sentence)\n\n        directory = args.cache_dir\n        cached_features_file = os.path.join(\n            directory, args.model_type + \"_cached_lm_\" + str(block_size)\n        )\n\n        if os.path.exists(cached_features_file) and not args.overwrite_cache:\n            logger.info(\"Loading features from cached file %s\", cached_features_file)\n            with open(cached_features_file, \"rb\") as handle:\n                self.samples = pickle.load(handle)\n        else:\n            logger.info(\"Creating features from dataset file at %s\", directory)\n\n            self.samples = []\n            for _, row in df.iterrows():\n                conv = self.new_construct_conv(row, tokenizer)\n                self.samples.append(conv)\n\n            logger.info(\"Saving features into cached file %s\", cached_features_file)\n            with open(cached_features_file, \"wb\") as handle:\n                pickle.dump(self.samples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, item):\n        sample = self.samples[item]\n        torch_sample = {name: torch.tensor(item, dtype=torch.long) for name, item in sample.items()}\n        \n        decoded = self.tokenizer.decode(sample['input_ids'])\n        \n        return torch_sample\n    \n    @staticmethod\n    def new_construct_conv(row, tokenizer):\n        conv = [tokenizer.encode(x) + [tokenizer.eos_token_id] for x in row]\n        conv = list(itertools.chain.from_iterable(conv))\n        sample = {'input_ids': conv, 'labels': conv}\n        return sample\n\n\ndef set_seed(seed, n_gpu):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if args.n_gpu > 0:\n        torch.cuda.manual_seed_all(seed)","1253316a":"def _sorted_checkpoints(args, checkpoint_prefix=\"checkpoint\", use_mtime=False) -> List[str]:\n    ordering_and_checkpoint_path = []\n\n    glob_checkpoints = glob.glob(os.path.join(args.output_dir, \"{}-*\".format(checkpoint_prefix)))\n\n    for path in glob_checkpoints:\n        if use_mtime:\n            ordering_and_checkpoint_path.append((os.path.getmtime(path), path))\n        else:\n            regex_match = re.match(\".*{}-([0-9]+)\".format(checkpoint_prefix), path)\n            if regex_match and regex_match.groups():\n                ordering_and_checkpoint_path.append((int(regex_match.groups()[0]), path))\n\n    checkpoints_sorted = sorted(ordering_and_checkpoint_path)\n    checkpoints_sorted = [checkpoint[1] for checkpoint in checkpoints_sorted]\n    return checkpoints_sorted\n\n\ndef _rotate_checkpoints(args, checkpoint_prefix=\"checkpoint\", use_mtime=False) -> None:\n    if not args.save_total_limit:\n        return\n    if args.save_total_limit <= 0:\n        return\n\n    # Check if we should delete older checkpoint(s)\n    checkpoints_sorted = _sorted_checkpoints(args, checkpoint_prefix, use_mtime)\n    if len(checkpoints_sorted) <= args.save_total_limit:\n        return\n\n    number_of_checkpoints_to_delete = max(0, len(checkpoints_sorted) - args.save_total_limit)\n    checkpoints_to_be_deleted = checkpoints_sorted[:number_of_checkpoints_to_delete]\n    for checkpoint in checkpoints_to_be_deleted:\n        logger.info(\"Deleting older checkpoint [{}] due to args.save_total_limit\".format(checkpoint))\n        shutil.rmtree(checkpoint)","e3f68946":"def collate_dicts(examples: List[Dict[str, torch.Tensor]], tokenizer):\n    keys = examples[0].keys()\n\n    # List[Dict[str, torch.Tensor]] -> Dict[str, List[torch.Tensor]]\n    keyed_tensor_lists = {}\n    for key in keys:\n        tensors = []\n        for example in examples:\n            tensor = example[key]\n            tensors.append(tensor)\n        keyed_tensor_lists[key] = tensors\n\n    # keyed_tensor_lists: Dict[str, List[torch.Tensor]]\n    # pad tensors:\n    results = {}\n    for key, tensors in keyed_tensor_lists.items():\n        if tokenizer._pad_token is None:\n            padded = rnn_utils.pad_sequence(tensors, batch_first=True)\n        else:\n            padded = rnn_utils.pad_sequence(tensors, batch_first=True, padding_value=tokenizer.pad_token_id)\n        results[key] = padded\n\n    # results: Dict[str, torch.Tensor]\n    return results\n\ndef train(\n        args, train_dataset, model: PreTrainedModel, tokenizer: PreTrainedTokenizer,\n) -> Tuple[int, float]:\n    \"\"\" Train the model \"\"\"\n    if args.local_rank in [-1, 0]:\n        tb_writer = tensorboard.SummaryWriter()\n\n    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n\n    train_sampler = data.RandomSampler(\n        train_dataset\n    ) if args.local_rank == -1 else distributed_utils.DistributedSampler(\n        train_dataset\n    )\n    collate_fn = functools.partial(collate_dicts, tokenizer=tokenizer)\n    train_dataloader = data.DataLoader(\n        train_dataset, sampler=train_sampler, batch_size=args.train_batch_size, collate_fn=collate_fn, drop_last=True\n    )\n\n    if args.max_steps > 0:\n        t_total = args.max_steps\n        args.num_train_epochs = args.max_steps \/\/ (len(train_dataloader) \/\/ args.gradient_accumulation_steps) + 1\n    else:\n        t_total = len(train_dataloader) \/\/ args.gradient_accumulation_steps * args.num_train_epochs\n\n    model = model.module if hasattr(model, \"module\") else model  # Take care of distributed\/parallel training\n    model.resize_token_embeddings(len(tokenizer))\n    # add_special_tokens_(model, tokenizer)\n\n    # Prepare optimizer and schedule (linear warmup and decay)\n    no_decay = [\"bias\", \"LayerNorm.weight\"]\n    optimizer_grouped_parameters = [\n        {\n            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n            \"weight_decay\": args.weight_decay,\n        },\n        {\n            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n    ]\n    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n    )\n\n    # Check if saved optimizer or scheduler states exist\n    if (\n            args.model_name_or_path\n            and os.path.isfile(os.path.join(args.model_name_or_path, \"optimizer.pt\"))\n            and os.path.isfile(os.path.join(args.model_name_or_path, \"scheduler.pt\"))\n    ):\n        # Load in optimizer and scheduler states\n        optimizer.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"optimizer.pt\")))\n        scheduler.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"scheduler.pt\")))\n\n    if args.fp16:\n        try:\n            from apex import amp\n        except ImportError:\n            raise ImportError(\"Please install apex from https:\/\/www.github.com\/nvidia\/apex to use fp16 training.\")\n        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n\n    # multi-gpu training (should be after apex fp16 initialization)\n    if args.n_gpu > 1:\n        model = torch.nn.DataParallel(model)\n\n    # Distributed training (should be after apex fp16 initialization)\n    if args.local_rank != -1:\n        model = torch.nn.parallel.DistributedDataParallel(\n            model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True\n        )\n\n    # Train!\n    logger.info(\"***** Running training *****\")\n    logger.info(\"  Num examples = %d\", len(train_dataset))\n    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n    logger.info(\"  Instantaneous batch size per GPU = %d\", args.per_gpu_train_batch_size)\n    logger.info(\n        \"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n        args.train_batch_size\n        * args.gradient_accumulation_steps\n        * (distributed.get_world_size() if args.local_rank != -1 else 1),\n    )\n    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n    logger.info(\"  Total optimization steps = %d\", t_total)\n\n    global_step = 0\n    epochs_trained = 0\n    steps_trained_in_current_epoch = 0\n    # Check if continuing training from a checkpoint\n    if args.model_name_or_path and os.path.exists(args.model_name_or_path):\n        try:\n            # set global_step to gobal_step of last saved checkpoint from model path\n            checkpoint_suffix = args.model_name_or_path.split(\"-\")[-1].split(\"\/\")[0]\n            global_step = int(checkpoint_suffix)\n            epochs_trained = global_step \/\/ (len(train_dataloader) \/\/ args.gradient_accumulation_steps)\n            steps_trained_in_current_epoch = global_step % (len(train_dataloader) \/\/ args.gradient_accumulation_steps)\n\n            logger.info(\"  Continuing training from checkpoint, will skip to saved global_step\")\n            logger.info(\"  Continuing training from epoch %d\", epochs_trained)\n            logger.info(\"  Continuing training from global step %d\", global_step)\n            logger.info(\"  Will skip the first %d steps in the first epoch\", steps_trained_in_current_epoch)\n        except ValueError:\n            logger.info(\"  Starting fine-tuning.\")\n\n    tr_loss, logging_loss = 0.0, 0.0\n\n    model.zero_grad()\n    train_iterator = trange(\n        epochs_trained, int(args.num_train_epochs), desc=\"Epoch\", disable=args.local_rank not in [-1, 0]\n    )\n    set_seed(args.seed, args.n_gpu)  # Added here for reproducibility\n    for _ in train_iterator:\n        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=args.local_rank not in [-1, 0])\n        for step, sample_batched in enumerate(epoch_iterator):\n            inputs = sample_batched['input_ids']\n            labels = sample_batched['labels']\n\n            # Skip past any already trained steps if resuming training\n            if steps_trained_in_current_epoch > 0:\n                steps_trained_in_current_epoch -= 1\n                continue\n\n            if inputs.shape[1] > 1024: continue\n            inputs = inputs.to(args.device)\n            labels = labels.to(args.device)\n            model.train()\n            outputs = model(inputs, labels=labels)\n            loss = outputs[0]  # model outputs are always tuple in transformers (see doc)\n\n            if args.n_gpu > 1:\n                loss = loss.mean()  # mean() to average on multi-gpu parallel training\n            if args.gradient_accumulation_steps > 1:\n                loss = loss \/ args.gradient_accumulation_steps\n\n            if args.fp16:\n                with amp.scale_loss(loss, optimizer) as scaled_loss:\n                    scaled_loss.backward()\n            else:\n                loss.backward()\n\n            tr_loss += loss.item()\n            if (step + 1) % args.gradient_accumulation_steps == 0:\n                if args.fp16:\n                    torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n                else:\n                    torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n                optimizer.step()\n                scheduler.step()  # Update learning rate schedule\n                model.zero_grad()\n                global_step += 1\n\n                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n                    logger.info(\"Running evaluation\")\n                    # Log metrics\n                    if (\n                            args.local_rank == -1 and args.evaluate_during_training\n                    ):  # Only evaluate when single GPU otherwise metrics may not average well\n                        results = evaluate(args, model, tokenizer)\n                        for key, value in results.items():\n                            tb_writer.add_scalar(\"eval_{}\".format(key), value, global_step)\n                    tb_writer.add_scalar(\"lr\", scheduler.get_lr()[0], global_step)\n                    tb_writer.add_scalar(\"loss\", (tr_loss - logging_loss) \/ args.logging_steps, global_step)\n                    logging_loss = tr_loss\n\n                if args.local_rank in [-1, 0] and args.save_steps > 0 and global_step % args.save_steps == 0:\n                    checkpoint_prefix = \"checkpoint\"\n                    # Save model checkpoint\n                    output_dir = os.path.join(args.output_dir, \"{}-{}\".format(checkpoint_prefix, global_step))\n                    os.makedirs(output_dir, exist_ok=True)\n                    model_to_save = (\n                        model.module if hasattr(model, \"module\") else model\n                    )  # Take care of distributed\/parallel training\n                    model_to_save.save_pretrained(output_dir)\n                    tokenizer.save_pretrained(output_dir)\n\n                    torch.save(args, os.path.join(output_dir, \"training_args.bin\"))\n                    logger.info(\"Saving model checkpoint to %s\", output_dir)\n\n                    _rotate_checkpoints(args, checkpoint_prefix)\n\n                    torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n                    torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n                    logger.info(\"Saving optimizer and scheduler states to %s\", output_dir)\n\n            if args.max_steps > 0 and global_step > args.max_steps:\n                epoch_iterator.close()\n                break\n        if args.max_steps > 0 and global_step > args.max_steps:\n            train_iterator.close()\n            break\n\n    if args.local_rank in [-1, 0]:\n        tb_writer.close()\n\n    return global_step, tr_loss \/ global_step","efc824a5":"# Evaluation of some model\ndef evaluate(args, model: PreTrainedModel, tokenizer: PreTrainedTokenizer, eval_dataset, prefix=\"\") -> Dict:\n    # Loop to handle MNLI double evaluation (matched, mis-matched)\n    eval_output_dir = args.output_dir\n\n    os.makedirs(eval_output_dir, exist_ok=True)\n    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n\n    # Note that DistributedSampler samples randomly\n    eval_sampler = data.SequentialSampler(eval_dataset)\n    collate_fn = functools.partial(collate_dicts, tokenizer=tokenizer)\n    eval_dataloader = data.DataLoader(\n        eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size, collate_fn=collate_fn, drop_last=True\n    )\n\n    # multi-gpu evaluate\n    if args.n_gpu > 1:\n        model = torch.nn.DataParallel(model)\n\n    # Eval!\n    logger.info(\"***** Running evaluation {} *****\".format(prefix))\n    logger.info(\"  Num examples = %d\", len(eval_dataset))\n    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n    eval_loss = 0.0\n    nb_eval_steps = 0\n    model.eval()\n\n    for eval_sample_batched in tqdm(eval_dataloader, desc=\"Evaluating\"):\n        inputs = eval_sample_batched['input_ids']\n        labels = eval_sample_batched['labels']\n        inputs = inputs.to(args.device)\n        labels = labels.to(args.device)\n\n        with torch.no_grad():\n            outputs = model(inputs, labels=labels)\n            lm_loss = outputs[0]\n            eval_loss += lm_loss.mean().item()\n        nb_eval_steps += 1\n\n    eval_loss = eval_loss \/ nb_eval_steps\n    perplexity = torch.exp(torch.tensor(eval_loss))\n\n    result = {\"perplexity\": perplexity}\n\n    output_eval_file = os.path.join(eval_output_dir, prefix, \"eval_results.txt\")\n    with open(output_eval_file, \"w\") as writer:\n        logger.info(\"***** Eval results {} *****\".format(prefix))\n        for key in sorted(result.keys()):\n            logger.info(\"  %s = %s\", key, str(result[key]))\n            writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n\n    return result","48f822e7":"def run_finetune(args, model, tokenizer, train_dataset, eval_dataset):\n\n    if args.should_continue:\n        sorted_checkpoints = _sorted_checkpoints(args)\n        if len(sorted_checkpoints) == 0:\n            raise ValueError(\"Used --should_continue but no checkpoint was found in --output_dir.\")\n        else:\n            args.model_name_or_path = sorted_checkpoints[-1]\n\n    if (\n            os.path.exists(args.output_dir)\n            and os.listdir(args.output_dir)\n            and args.do_train\n            and not args.overwrite_output_dir\n            and not args.should_continue\n    ):\n        raise ValueError(\n            \"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(\n                args.output_dir\n            )\n        )\n\n    # Setup CUDA, GPU & distributed training\n    device = torch.device(\"cuda\")\n    args.device = device\n\n    logger.warning(\n        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n        args.local_rank,\n        device,\n        args.n_gpu,\n        bool(args.local_rank != -1),\n        args.fp16,\n    )\n\n    model.to(args.device)\n\n    logger.info(\"Training\/evaluation parameters %s\", args)\n\n    # Training\n    if args.do_train:\n\n        global_step, tr_loss = train(args, train_dataset, model, tokenizer,)\n        logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n\n    # Saving best-practices: if you use save_pretrained for the model and\n    # tokenizer, you can reload them using from_pretrained()\n    if args.do_train:\n        # Create output directory if needed\n        os.makedirs(args.output_dir, exist_ok=True)\n\n        logger.info(\"Saving model checkpoint to %s\", args.output_dir)\n        # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n        # They can then be reloaded using `from_pretrained()`\n        model_to_save = (\n            model.module if hasattr(model, \"module\") else model\n        )  # Take care of distributed\/parallel training\n        model_to_save.save_pretrained(args.output_dir)\n        tokenizer.save_pretrained(args.output_dir)\n\n        # Good practice: save your training arguments together with the trained model\n        torch.save(args, os.path.join(args.output_dir, \"training_args.bin\"))\n\n        # Load a trained model and vocabulary that you have fine-tuned\n        model = AutoModelWithLMHead.from_pretrained(args.output_dir)\n        tokenizer = AutoTokenizer.from_pretrained(args.output_dir)\n        model.to(args.device)\n\n    # Evaluation\n    results = {}\n    if args.do_eval and args.local_rank in [-1, 0]:\n        checkpoints = [args.output_dir]\n        if args.eval_all_checkpoints:\n            checkpoints = list(\n                os.path.dirname(c) for c in sorted(glob.glob(args.output_dir + \"\/**\/\" + WEIGHTS_NAME, recursive=True))\n            )\n            logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.WARN)  # Reduce logging\n        logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n        for checkpoint in checkpoints:\n            global_step = checkpoint.split(\"-\")[-1] if len(checkpoints) > 1 else \"\"\n            prefix = checkpoint.split(\"\/\")[-1] if checkpoint.find(\"checkpoint\") != -1 else \"\"\n\n            model = AutoModelWithLMHead.from_pretrained(checkpoint)\n            model.to(args.device)\n            result = evaluate(args, model, tokenizer, eval_dataset, prefix=prefix)\n            result = dict((k + \"_{}\".format(global_step), v) for k, v in result.items())\n            results.update(result)\n\n    return results","3a27f3d9":"# def main():\n\"\"\"\nFine-tuning the library models for language modeling on a text file (GPT, GPT-2, BERT, RoBERTa).\nGPT and GPT-2 are fine-tuned using a causal language modeling (CLM) loss while BERT and RoBERTa are fine-tuned\nusing a masked language modeling (MLM) loss.\n\"\"\"\nconfig = {\n    'data_path': '..\/input\/rickmorty-scripts\/RickAndMortyScripts.csv',\n    'joe_rogan_data_path': \"..\/input\/joe-rogan\/joe_rogan_c\/\",\n    'context_size': 1,\n    'target_name': \"Joe Rogan\",\n    'test_size': 0.1,\n    'use_guest_as_name': True,\n}\n\n# Let's look at original dataset\nall_rick = pd.read_csv(config['data_path'])\n\ncontexted = []\n\nn = config['context_size']\n\nfor i in range(n, len(all_rick['line'])):\n    row = []\n    # we additionally subtract 1, so row will contain current responce and 7 previous responces\n    for j in range(i, i - 1 - n, -1):\n        row.append(all_rick['line'][j])\n    row = list(reversed(row))\n    contexted.append(row)\n\nlogger.info(f'Dataset size: {len(contexted)}')\n\ncolumns = ['response', 'context']\ncolumns = columns + ['context\/' + str(i) for i in range(n - 1)]\n\ndf = pd.DataFrame.from_records(contexted, columns=columns)\n\ntrn_df, val_df = model_selection.train_test_split(df, test_size=config['test_size'])","a3f32d96":"df_paths = list(pathlib.Path(config['joe_rogan_data_path']).iterdir())\ndfs = []\nfor df_path in df_paths:\n    cur_df = pd.read_csv(df_path)\n    cur_df['src'] = df_path.stem\n    dfs.append(cur_df)\nall_data = pd.concat(dfs)\ndel dfs, df_path, cur_df\n\n# all_data = all_rick.rename(columns={'line': 'utterance'})\n# all_data['src'] = all_rick['season no.'].astype(str) + all_rick['episode no.'].astype(str)\n\nall_data['with_names'] = all_data['name'] + \": \" + all_data['utterance']\nall_data = all_data.dropna()\n\nif config['use_guest_as_name']:\n    all_data.loc[all_data['name'] != config['target_name'], 'name'] = 'Guest'\nall_data.head()","29c07859":"context_size = config['context_size']\ntgt_name = config['target_name']\ncontexted_dfs = []\nfor gr_name, group in all_data.groupby('src'):\n    for i in range(len(group)):\n        elem = group.iloc[i, :]\n        if elem['name'] != tgt_name:\n            continue\n        else:\n            with_context = group.iloc[max(0, i - context_size):i + 1, :]\n            contexted_dfs.append(with_context)\nnew_train_dfs, new_test_dfs = model_selection.train_test_split(contexted_dfs, test_size=config['test_size'])","75a9624c":"args = Args()\n# Set seed\nset_seed(args.seed, args.n_gpu)\nmodel_config = AutoConfig.from_pretrained(args.config_name, cache_dir=args.cache_dir)\ntokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, cache_dir=args.cache_dir)\nmodel = AutoModelWithLMHead.from_pretrained(\n    args.model_name_or_path,\n    from_tf=False,\n    config=model_config,\n    cache_dir=args.cache_dir,\n)\n\ntrain_dataset = ConversationDataset(tokenizer, args, trn_df)\neval_dataset = ConversationDataset(tokenizer, args, val_df)\n\n# def __init__(self, tokenizer: PreTrainedTokenizer, args, contexted_dataframes, block_size=512):\n\nnew_train_dataset = ConversationLabelDataset(tokenizer, args, new_train_dfs)\nnew_eval_dataset = ConversationLabelDataset(tokenizer, args, new_test_dfs)\n\nres = run_finetune(args, model, tokenizer, new_train_dataset, new_eval_dataset)\n# res = run_finetune(args, model, tokenizer, train_dataset, eval_dataset)\nprint(res)","a8ce44a6":"tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, cache_dir=args.cache_dir)\nmodel = AutoModelWithLMHead.from_pretrained('output-small')\n\ncontext = collections.deque()\nfor step in range(5):\n    while len(context) >= config['context_size']:\n        context.popleft()\n    \n    # encode the new user input, add the eos_token and return a tensor in Pytorch\n    inp = input(\">> Guest:\")\n    utterance = \"Guest: \" + inp + tokenizer.eos_token + f'{config[\"target_name\"]}:'\n    user_input_ids = tokenizer.encode(utterance, return_tensors='pt')\n    \n    context.append(user_input_ids)\n\n    # feed the context to the model\n    bot_input_ids = torch.cat(list(context), dim=-1)\n    print(f\"Debug: Decoded input: {tokenizer.decode(bot_input_ids.squeeze())}\\n\")\n\n    # generated a response while limiting the total chat history to 1000 tokens,\n    chat_history_ids = model.generate(\n        bot_input_ids, max_length=200,\n        pad_token_id=tokenizer.eos_token_id,\n        no_repeat_ngram_size=3,\n        do_sample=True,\n        top_k=100,\n        top_p=0.7,\n        temperature=0.8\n    )\n    response_ids = chat_history_ids[:, bot_input_ids.shape[-1]:]\n    context.append(response_ids)\n    decoded = tokenizer.decode(response_ids[0], skip_special_tokens=True)\n    # pretty print last ouput tokens from bot\n    print(f\"Joe Rogan: {decoded}\")","c89270f1":"jr = all_data.loc[all_data['name'] == config['target_name']]\nlens = jr['utterance'].str.split().apply(len)\nlens.plot.hist(bins=103)","8881c399":"### Train","a1267d22":"#### Run finetune","69c53c07":"### Finetune steps","0045c68e":"### Evaluation"}}