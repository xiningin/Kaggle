{"cell_type":{"b3f4fe18":"code","2effe36d":"code","bfcf4914":"code","fb9e3a06":"code","6d1e9c6f":"code","7cc361bb":"code","497a549a":"code","9ce66fbe":"code","9cf690e1":"code","3e1b4682":"code","8756f114":"code","064fe6b0":"code","bd258735":"code","b92e560e":"code","95dd3d85":"code","7ec37d87":"code","1cc87512":"code","573e5ea0":"code","ff44e76a":"code","a6252833":"code","9d438d59":"code","a8d34c3c":"code","1daaf743":"code","ddb1c243":"code","41a09493":"code","11442ce6":"code","1363df1a":"code","bd4ae747":"code","7cf14f0f":"code","d7d730a8":"code","d65d049b":"code","75c01b33":"code","bdb99914":"code","51dbe7bc":"code","c2c92d76":"code","50f70151":"code","cf0b147a":"code","f1055235":"code","07050074":"code","7d048a0c":"code","1e6a1d90":"code","d72dfdaa":"code","e58085a9":"code","d24336fb":"code","e6ebaf68":"code","fafa451c":"code","63d96302":"code","3b8b195f":"code","3b482068":"code","c80e183a":"code","16f7a756":"code","a052f948":"code","487fc174":"code","e7c3871d":"code","a250bcba":"code","d4c51007":"code","a72e1754":"code","aae4a82b":"code","a950776a":"code","9467974d":"code","ce1ba88b":"code","0b6eaab7":"code","4c6a3b81":"code","b7846f65":"code","afd7195c":"code","1cf776f5":"code","3f5134a6":"code","d7dd4302":"code","e060a44d":"code","d905f539":"code","55843f44":"code","7e1dad24":"code","c34d1317":"code","831ddc62":"code","350c5cd8":"code","c21da74e":"code","0d3ccda4":"code","8b9a78a6":"code","052670a2":"code","36aa8ced":"code","eb6aab21":"code","3f2c0855":"code","0532997d":"code","cea9e537":"code","eb20986d":"code","0658bb0d":"code","e66c9d34":"code","637e0add":"markdown","43d205b6":"markdown","5606f2d2":"markdown","fccee341":"markdown","dbf05db4":"markdown","f174f0dc":"markdown","580b5062":"markdown","9a26fcbf":"markdown","db830cf6":"markdown","1733be3a":"markdown","53bb6f19":"markdown","c870e1c2":"markdown","a557181e":"markdown","71fdfa5f":"markdown","1312661c":"markdown","f15f28cc":"markdown","08b25d4d":"markdown","598dd8ed":"markdown","c18d9328":"markdown","6f7d2e79":"markdown","e5fcc796":"markdown","d00b5f21":"markdown","d6c50fbe":"markdown","a5f00320":"markdown","c34744af":"markdown","3729c222":"markdown","36d5e214":"markdown","289fc64a":"markdown","caccc852":"markdown","1f7d251b":"markdown","e3ff05db":"markdown","a71bc391":"markdown","69e916c3":"markdown","c1f86cfc":"markdown","cd992b00":"markdown","32c54829":"markdown","c5aff530":"markdown","e35b8fe4":"markdown","ee507a9d":"markdown","8fd669bf":"markdown","d400d78a":"markdown","ef7b368d":"markdown","ca39a3c6":"markdown","b179bde4":"markdown","69277ea7":"markdown","2deec6c8":"markdown","5ccc014e":"markdown","46462b05":"markdown","ec514d5c":"markdown","6bf565c9":"markdown","c3458876":"markdown","6ca89c94":"markdown","19d075ed":"markdown","cffc1638":"markdown"},"source":{"b3f4fe18":"import sys, os, shutil, glob\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns\n\n# from tqdm import tqdm\nfrom itertools import chain\n\n# Printing models in ipynb\nfrom PIL import Image\nfrom scipy import ndimage\nfrom keras.utils.vis_utils import model_to_dot, plot_model\nfrom IPython.display import SVG\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.pipeline import Pipeline\n\nfrom skimage.io import imread, imshow, imread_collection, concatenate_images\nfrom skimage.transform import resize\n\nimport tensorflow as tf\n\nimport keras\nfrom keras import regularizers, optimizers\nfrom keras.applications import VGG16, VGG19\nfrom keras.models import Model, load_model, Sequential\nfrom keras.layers import Dense, Activation, Flatten, Dropout\nfrom keras.layers import Input, Conv2D, MaxPooling2D, BatchNormalization\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.preprocessing.image import array_to_img, img_to_array\nfrom keras.preprocessing.image import load_img\nfrom keras import backend as K\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.utils import np_utils\n\nimport warnings\nwarnings.filterwarnings('ignore')\n# OMP: Error #15: Initializing libiomp5.dylib, but found libiomp5.dylib already initialized.\n# os.environ['KMP_DUPLICATE_LIB_OK']='True'\n\nnp.random.seed(42)","2effe36d":"# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))","bfcf4914":"%ls ..\/input\/dataset\/dataset_updated","fb9e3a06":"%ls ..\/input\/dataset\/dataset_updated\/training_set\/drawings\/ | head -5","6d1e9c6f":"%ls ..\/input\/musemart\/dataset_updated\/training_set\/drawings\/ | head -5","7cc361bb":"imshow('..\/input\/musemart\/dataset_updated\/training_set\/drawings\/1677_mainfoto_05.jpg');","497a549a":"imshow('..\/input\/dataset\/dataset_updated\/training_set\/drawings\/1677_mainfoto_05.jpg');","9ce66fbe":"src_dirs_0 = ['dataset', 'musemart']\nsrc_dirs_1 = ['training_set', 'validation_set']\nsrc_dirs_2 = ['sculpture', 'iconography', 'engraving', 'drawings', 'painting']","9cf690e1":"# copying files from musemart to dataset (merge data)\nfor sub_dir in src_dirs_1:\n    for d in src_dirs_2:\n        src_dir = src_dirs_0[1] + '\/' + sub_dir + '\/' + d\n        files = os.listdir(src_dir)\n        \n        dst_dir = src_dirs_0[0] + '\/' + sub_dir + '\/' + d\n        \n        for file in files:\n            shutil.copy(os.path.join(src_dir, file), os.path.join(dst_dir, file))","3e1b4682":"img_width, img_height = 150, 150\n\ncategories = ['drawings', 'engraving', 'iconography' ,'painting' ,'sculpture']\n\ntrain_path = 'dataset\/train\/'\nvalid_path = 'dataset\/validation\/'\ntest_path  = 'dataset\/test\/'","8756f114":"def show_images_for_art(art_type=\"drawings\", num_pics=10):\n    assert art_type in categories\n    \n    pic_dir = os.path.join(train_path, art_type)\n    pic_files = [os.path.join(pic_dir, filename) for filename in os.listdir(pic_dir)]\n\n    ncols = 5\n    nrows = (num_pics - 1) \/\/ ncols + 1\n    fig, ax = plt.subplots(nrows=nrows, ncols=ncols, figsize=(16, 4))\n    \n    fig.set_size_inches((20, nrows * 5))\n    ax = ax.ravel()\n    \n    for pic, ax in zip(pic_files[:num_pics], ax):\n        img = imread(pic)\n        ax.imshow(img, resample=True)\n    \n    plt.show();\n    \nshow_images_for_art(art_type=\"drawings\")","064fe6b0":"# Just have a look at the categories itself, one image shall be ok\n\nfig, ax = plt.subplots(nrows=1, ncols=5, figsize=(16, 4))\n\nfor i, cat in enumerate(categories):\n    cat_path = os.path.join(train_path, cat)\n    img_name = os.listdir(cat_path)[0]\n    \n    img = imread(os.path.join(cat_path, img_name))\n    img = resize(img, (img_width, img_height, 3), mode='reflect')\n    \n    ax[i].imshow(img, resample=True)\n    ax[i].set_title(cat)\n    \nplt.show();","bd258735":"n_imgs = []\nfor cat in categories:\n    files = os.listdir(os.path.join(train_path, cat))\n    n_imgs += [len(files)]\n\nplt.figure(figsize=(16, 8))\nplt.bar([_ for _ in range(5)], n_imgs, tick_label=categories)\nplt.show();","b92e560e":"num_train_sample = 0\nfor i, cat in enumerate(categories):\n    cat_path = os.path.join(train_path, cat)\n    num_train_sample += len(os.listdir(cat_path))\n    \nprint('Total number of training samples: {}'.format(num_train_sample))","95dd3d85":"num_test_sample = 0\nfor i, cat in enumerate(categories):\n    cat_path = os.path.join(test_path, cat)\n    num_test_sample += len(os.listdir(cat_path))\n    \nprint('Total number of test samples: {}'.format(num_test_sample))","7ec37d87":"num_validation_sample = 0\nfor i, cat in enumerate(categories):\n    cat_path = os.path.join(valid_path, cat)\n    num_validation_sample += len(os.listdir(cat_path))\n    \nprint('Total number of validation samples: {}'.format(num_validation_sample))","1cc87512":"nb_train_samples = 2000\nnb_validation_samples = 800\nepochs = 50\nbatch_size = 16","573e5ea0":"# this is the augmentation configuration we will use for training\ntrain_datagen = ImageDataGenerator(\n        rescale=1.\/255,\n        shear_range=0.2,\n        zoom_range=0.2,\n        horizontal_flip=True)\n\n# this is the augmentation configuration we will use for testing:\n# only rescaling\ndatagen = ImageDataGenerator(rescale=1.\/255)\n\ntrain_generator = train_datagen.flow_from_directory(\n        train_path,\n        target_size=(img_width, img_height),\n        batch_size=batch_size,\n        class_mode='categorical')\n\nvalid_generator = datagen.flow_from_directory(\n        valid_path,\n        target_size=(img_width, img_height),\n        batch_size=batch_size,\n        class_mode='categorical')\n\ntest_generator = datagen.flow_from_directory(\n        test_path,\n        target_size=(img_width, img_height),\n        batch_size=batch_size,\n        class_mode='categorical')","ff44e76a":"if K.image_data_format() == 'channels_first':\n    input_shape = (3, img_width, img_height)\nelse:\n    input_shape = (img_width, img_height, 3)","a6252833":"model = Sequential([\n    Conv2D(32, (3, 3), input_shape=input_shape, activation='relu'),\n    MaxPooling2D(pool_size=(2, 2)),\n    Dropout(0.2),\n    \n    Conv2D(32, (3, 3), activation='relu'),\n    MaxPooling2D(pool_size=(2, 2)),\n    Dropout(0.2),\n    \n    Conv2D(64, (3, 3), activation='relu'),\n    MaxPooling2D(pool_size=(2, 2)),\n    Dropout(0.2),\n    \n    Flatten(),\n    Dense(128, activation='relu'),\n    Dropout(0.2),\n    Dense(64, activation='relu'),\n    Dropout(0.2),\n    Dense(5, activation='softmax')\n])\n\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])","9d438d59":"SVG(model_to_dot(model, show_layer_names=False, show_shapes=True).create(prog='dot', format='svg'))","a8d34c3c":"%%time\n\ntrain_result = model.fit_generator(\n            train_generator,\n            steps_per_epoch=num_train_sample \/\/ batch_size,\n            epochs=epochs,\n            validation_data=valid_generator,\n            validation_steps=num_validation_sample \/\/ batch_size,\n            use_multiprocessing=True)","1daaf743":"model.save('CNN_base_run1.h5')","ddb1c243":"sns.set(style=\"white\", palette=\"muted\", color_codes=True)\n\nfig, ax = plt.subplots(1, 2, figsize=(16, 6), sharex=True)\n\nax[0].plot(train_result.history['loss'], label=\"Loss\")\nax[0].plot(train_result.history['val_loss'], label=\"Validation loss\")\nax[0].set_title('Loss')\nax[0].set_xlabel('Epoch')\nax[0].set_ylabel('Loss')\nax[0].legend()\n\nax[1].plot(train_result.history['acc'], label=\"Accuracy\")\nax[1].plot(train_result.history['val_acc'], label=\"Validation accuracy\")\nax[1].set_title('Accuracy')\nax[1].set_xlabel('Epoch')\nax[1].set_ylabel('Accuracy')\nax[1].legend()\nplt.tight_layout()\n\nplt.show();","41a09493":"test_loss, test_acc = model.evaluate_generator(test_generator, steps=32)\ny_hat_test = model.predict_generator(test_generator, steps=32)\n\nprint('Generated {} predictions'.format(len(y_hat_test)))\nprint('Test accuracy: {:.2f}%'.format(test_acc * 100))","11442ce6":"# Load the VGG19 network\nvgg_model = VGG19(include_top=False, weights='imagenet', input_shape=input_shape)\nvgg_model.summary()","1363df1a":"model = Sequential([\n    vgg_model,\n    Flatten(),\n    Dense(32, activation='relu'),\n    Dense(64, activation='relu'),\n    Dense(128, activation='relu'),\n    Dense(64, activation='relu'),\n    Dense(5, activation='softmax')\n])\n\nvgg_model.trainable = False\n\n# Check what layers are trainable\nfor layer in model.layers:\n    print(layer.name, layer.trainable)\n    \n# model.summary()","bd4ae747":"SVG(model_to_dot(model, show_layer_names=False, show_shapes=True).create(prog='dot', format='svg'))","7cf14f0f":"%%time\n\n# Compilation\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# Fitting the Model\ntrain_result = model.fit_generator(\n            train_generator,\n            steps_per_epoch=num_train_sample \/\/ batch_size,\n            epochs=epochs,\n            validation_data=valid_generator,\n            validation_steps=num_validation_sample \/\/ batch_size,\n            use_multiprocessing=True)","d7d730a8":"model.save('VGG19_Feature_Engineered.h5')","d65d049b":"sns.set(style=\"white\", palette=\"muted\", color_codes=True)\n\nfig, ax = plt.subplots(1, 2, figsize=(16, 6), sharex=True)\n\nax[0].plot(train_result.history['loss'], label=\"Loss\")\nax[0].plot(train_result.history['val_loss'], label=\"Validation loss\")\nax[0].set_title('Loss')\nax[0].set_xlabel('Epoch')\nax[0].set_ylabel('Loss')\nax[0].legend()\n\nax[1].plot(train_result.history['acc'], label=\"Accuracy\")\nax[1].plot(train_result.history['val_acc'], label=\"Validation accuracy\")\nax[1].set_title('Accuracy')\nax[1].set_xlabel('Epoch')\nax[1].set_ylabel('Accuracy')\nax[1].legend()\nplt.tight_layout()\n\nplt.show();","75c01b33":"test_loss, test_acc = model.evaluate_generator(test_generator, steps=32, use_multiprocessing=True)\ny_hat_test = model.predict_generator(test_generator, steps=32, use_multiprocessing=True)\n\nprint('Generated {} predictions'.format(len(y_hat_test)))\nprint('Test accuracy: {:.2f}%'.format(test_acc * 100))","bdb99914":"# Load the VGG19 network\nvgg_model = VGG19(include_top=False, weights='imagenet', input_shape=input_shape)\nvgg_model.summary()","51dbe7bc":"%%time\ndatagen = ImageDataGenerator(rescale=1. \/ 255)\n\ngenerator = datagen.flow_from_directory(\n    train_path,\n    target_size=(img_width, img_height),\n    batch_size=batch_size,\n    class_mode='categorical',\n    shuffle=False)\n\nbottleneck_features_train = vgg_model.predict_generator(\n    generator, 500, use_multiprocessing=True)\n\n# Save the output as a numpy array\nnp.save(open('bottleneck_features_train.npy', 'wb'),\n        bottleneck_features_train)","c2c92d76":"%%time\ngenerator = datagen.flow_from_directory(\n    valid_path,\n    target_size=(img_width, img_height),\n    batch_size=batch_size,\n    class_mode='categorical',\n    shuffle=False)\n\nbottleneck_features_validation = vgg_model.predict_generator(\n    generator, 60, use_multiprocessing=True, verbose=1)\n\n# Save the output as a numpy array\nnp.save(open('bottleneck_features_validation.npy', 'wb'), \n        bottleneck_features_validation)","50f70151":"bottleneck_features_train.shape, bottleneck_features_validation.shape","cf0b147a":"train_data = np.load(open('bottleneck_features_train.npy', 'rb'))\n# train_labels = np.array([0] * 4000 + [1] * 4000)\na = np.zeros(shape=(8000, 3))\nb = np.ones(shape=(8000, 2))\ntrain_labels = np.concatenate((a, b), axis=1)\n\nvalidation_data = np.load(open('bottleneck_features_validation.npy', 'rb'))\n# validation_labels = np.array([0] * 480 + [1] * 480)\na = np.zeros(shape=(960, 3))\nb = np.ones(shape=(960, 2))\nvalidation_labels = np.concatenate((a, b), axis=1)","f1055235":"train_data.shape, train_labels.shape","07050074":"model = Sequential([\n    Flatten(input_shape=train_data.shape[1:]),\n    Dense(256, activation='relu'),\n    Dropout(0.2),\n    Dense(5, activation='softmax')\n])","7d048a0c":"model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])","1e6a1d90":"model.summary()","d72dfdaa":"%%time\ntrain_result = model.fit(train_data, train_labels,\n                         epochs=epochs,\n                         batch_size=batch_size,\n                         validation_data=(validation_data, validation_labels)\n                        )","e58085a9":"model.save_weights('bottleneck_fc_model.h5')","d24336fb":"sns.set(style=\"white\", palette=\"muted\", color_codes=True)\n\nfig, ax = plt.subplots(1, 2, figsize=(16, 6), sharex=True)\n\nax[0].plot(train_result.history['loss'], label=\"Loss\")\nax[0].plot(train_result.history['val_loss'], label=\"Validation loss\")\nax[0].set_title('Loss')\nax[0].set_xlabel('Epoch')\nax[0].set_ylabel('Loss')\nax[0].legend()\n\nax[1].plot(train_result.history['acc'], label=\"Accuracy\")\nax[1].plot(train_result.history['val_acc'], label=\"Validation accuracy\")\nax[1].set_title('Accuracy')\nax[1].set_xlabel('Epoch')\nax[1].set_ylabel('Accuracy')\nax[1].legend()\nplt.tight_layout()\n\nplt.show();","e6ebaf68":"img_width, img_height = 128, 128\ninput_shape = (img_height, img_width, 3)\n\ncategories = ['drawings', 'engraving', 'iconography' ,'painting' ,'sculpture']\n\ntrain_path = 'dataset\/train\/'\nvalid_path = 'dataset\/validation\/'\ntest_path = 'dataset\/test\/'","fafa451c":"category_embeddings = {\n    'drawings': 0,\n    'engraving': 1,\n    'iconography': 2,\n    'painting': 3,\n    'sculpture': 4\n}","63d96302":"train_data = [(file, cat) for cat in categories for file in glob.glob(train_path + cat + '\/*')]\ntest_data = [(file, cat) for cat in categories for file in glob.glob(train_path + cat + '\/*')]","3b8b195f":"train_data[:5]","3b482068":"def load_dataset(tuples_list):\n    indexes = np.arange(len(tuples_list))\n    np.random.shuffle(indexes)\n    \n    X = []\n    y = []\n    \n    cpt = 0\n    for i in range(len(indexes)):\n        t = tuples_list[indexes[i]]\n        try:\n            # skimage\n            img = imread(t[0])\n            img = resize(img, input_shape, mode='reflect')\n            X += [img]\n            \n            y_tmp = [0 for _ in range(len(categories))]\n            y_tmp[category_embeddings[t[1]]] = 1\n            y += [y_tmp]\n        except OSError:\n            pass\n        \n        cpt += 1\n        if cpt % 1000 == 0:\n            print(\"Processed {} images\".format(cpt))\n\n    return np.array(X), np.array(y)","c80e183a":"X_train, y_train = load_dataset(train_data)\nX_valid, y_valid = load_dataset(test_data)","16f7a756":"train_datagen = ImageDataGenerator(rotation_range=20, zoom_range=0.15, horizontal_flip=True)\ntrain_datagen.fit(X_train)","a052f948":"# model = Sequential([\n#     Conv2D(32, (3, 3), padding='same', input_shape=input_shape, activation='relu'),\n#     Conv2D(32, (3, 3), activation='relu'),\n#     MaxPooling2D(pool_size=(2, 2)),\n    \n#     Dropout(0.25),\n#     Conv2D(64, (3, 3), padding='same', activation='relu'),\n#     Conv2D(64, (3, 3), activation='relu'),\n#     MaxPooling2D(pool_size=(2, 2)),\n    \n#     Dropout(0.25),\n#     Flatten(),\n#     Dense(256, activation='relu'),\n#     Dropout(0.5),\n#     Dense(5, activation='sigmoid')\n# ])\nmodel = Sequential([\n    Conv2D(32, (3, 3), input_shape=input_shape, activation='relu'),\n    MaxPooling2D(pool_size=(2, 2)),\n    Dropout(0.2),\n    \n    Conv2D(32, (3, 3), activation='relu'),\n    MaxPooling2D(pool_size=(2, 2)),\n    Dropout(0.2),\n    \n    Conv2D(64, (3, 3), activation='relu'),\n    MaxPooling2D(pool_size=(2, 2)),\n    Dropout(0.2),\n    \n    Flatten(),\n    Dense(128, activation='relu'),\n    Dropout(0.2),\n    Dense(64, activation='relu'),\n    Dropout(0.2),\n    Dense(5, activation='softmax')\n])\n\nmodel.compile(optimizer='adam', loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])","487fc174":"train_generator = train_datagen.flow(X_train, y_train, batch_size=32)\ntrain_result = model.fit_generator(generator=train_generator, validation_data=(X_valid, y_valid),\n                                  epochs=50, steps_per_epoch=len(X_train)\/32, verbose=1, \n                                  use_multiprocessing=True)","e7c3871d":"fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(16, 8))\n\nax[0].plot(train_result.history['loss'], label=\"Loss\")\nax[0].plot(train_result.history['val_loss'], label=\"Validation loss\")\nax[0].set_title('Loss')\nax[0].set_xlabel('Epoch')\nax[0].set_ylabel('Loss')\nax[0].legend()\n\nax[1].plot(train_result.history['acc'], label=\"Accuracy\")\nax[1].plot(train_result.history['val_acc'], label=\"Validation accuracy\")\nax[1].set_title('Accuracy')\nax[1].set_xlabel('Epoch')\nax[1].set_ylabel('Accuracy')\nax[1].legend()\n\nplt.tight_layout()\nplt.show();","a250bcba":"# Let's look at more metrics\nfrom sklearn.metrics import classification_report\n\nX_test = []\ny_test = []\nfor t in test_data:\n    try:\n        img = skimage.io.imread(os.path.join(t[0]))\n        img = skimage.transform.resize(img, input_shape, mode='reflect')\n        X_test += [img]\n        y_test += [category_embeddings[t[1]]]\n    except OSError:\n        pass\n\nX_test = np.array(X_test)\ny_test = np.array(y_test)\n\npred = model.predict(X_test, verbose=1)\n\ny_pred = np.argmax(pred, axis=1)\nprint(classification_report(y_test, y_pred))","d4c51007":"from sklearn.metrics import confusion_matrix\n\nc_matrix = confusion_matrix(y_test, y_pred)\nplt.imshow(c_matrix, cmap=plt.cm.Blues)\nplt.title(\"Confusion matrix\")\nplt.colorbar()\nplt.show();\n\nprint(c_matrix)","a72e1754":"categories = os.listdir(\"dataset\/train\/\")\ncategories","aae4a82b":"dataset = pd.DataFrame(columns=[categories])\ndataset.insert(loc=0, column='filename', value=None)\ndataset.head()","a950776a":"df1 = dataset.copy()\ndf2 = dataset.copy()\ndf3 = dataset.copy()\ndf4 = dataset.copy()\ndf5 = dataset.copy()\ndf5.head()","9467974d":"myList = [file.split(\"\/\", 1)[1] for file in glob.glob('dataset\/train\/sculpture\/*')]\ndataset = pd.DataFrame(data=myList, columns=['filename'])\ndataset[categories] = pd.DataFrame([[1, 0, 0, 0, 0]], index=dataset.index)\n\nmyList = [file.split(\"\/\", 1)[1] for file in glob.glob('dataset\/train\/iconography\/*')]\ndf2 = pd.DataFrame(data=myList, columns=['filename'])\ndf2[categories] = pd.DataFrame([[0, 1, 0, 0, 0]], index=df2.index)\ndataset = dataset.append(df2)\n\nmyList = [file.split(\"\/\", 1)[1] for file in glob.glob('dataset\/train\/engraving\/*')]\ndf3 = pd.DataFrame(data=myList, columns=['filename'])\ndf3[categories] = pd.DataFrame([[0, 0, 1, 0, 0]], index=df3.index)\ndataset = dataset.append(df3)\n\nmyList = [file.split(\"\/\", 1)[1] for file in glob.glob('dataset\/train\/drawings\/*')]\ndf4 = pd.DataFrame(data=myList, columns=['filename'])\ndf4[categories] = pd.DataFrame([[0, 0, 0, 1, 0]], index=df4.index)\ndataset = dataset.append(df3)\n\nmyList = [file.split(\"\/\", 1)[1] for file in glob.glob('dataset\/train\/painting\/*')]\ndf5 = pd.DataFrame(data=myList, columns=['filename'])\ndf5[categories] = pd.DataFrame([[0, 0, 0, 0, 1]], index=df5.index)\ndataset = dataset.append(df5)","ce1ba88b":"dataset.shape","0b6eaab7":"dataset.reset_index(drop=True, inplace=True)","4c6a3b81":"dataset.tail()","b7846f65":"valid_dataset = pd.DataFrame(columns=[categories])\nvalid_dataset.insert(loc=0, column='filename', value=None)\nvalid_dataset.head()","afd7195c":"del df1, df2, df3, df4, df5","1cf776f5":"df1 = valid_dataset.copy()\ndf2 = valid_dataset.copy()\ndf3 = valid_dataset.copy()\ndf4 = valid_dataset.copy()\ndf5 = valid_dataset.copy()\ndf5.head()","3f5134a6":"myList = [file.split(\"\/\", 1)[1] for file in glob.glob('dataset\/validation\/sculpture\/*')]\nvalid_dataset = pd.DataFrame(data=myList, columns=['filename'])\nvalid_dataset[categories] = pd.DataFrame([[1, 0, 0, 0, 0]], index=dataset.index)\n\nmyList = [file.split(\"\/\", 1)[1] for file in glob.glob('dataset\/validation\/iconography\/*')]\ndf2 = pd.DataFrame(data=myList, columns=['filename'])\ndf2[categories] = pd.DataFrame([[0, 1, 0, 0, 0]], index=df2.index)\nvalid_dataset = valid_dataset.append(df2)\n\nmyList = [file.split(\"\/\", 1)[1] for file in glob.glob('dataset\/validation\/engraving\/*')]\ndf3 = pd.DataFrame(data=myList, columns=['filename'])\ndf3[categories] = pd.DataFrame([[0, 0, 1, 0, 0]], index=df3.index)\nvalid_dataset = valid_dataset.append(df3)\n\nmyList = [file.split(\"\/\", 1)[1] for file in glob.glob('dataset\/validation\/drawings\/*')]\ndf4 = pd.DataFrame(data=myList, columns=['filename'])\ndf4[categories] = pd.DataFrame([[0, 0, 0, 1, 0]], index=df4.index)\nvalid_dataset = valid_dataset.append(df3)\n\nmyList = [file.split(\"\/\", 1)[1] for file in glob.glob('dataset\/validation\/painting\/*')]\ndf5 = pd.DataFrame(data=myList, columns=['filename'])\ndf5[categories] = pd.DataFrame([[0, 0, 0, 0, 1]], index=df5.index)\nvalid_dataset = valid_dataset.append(df5)","d7dd4302":"valid_dataset.reset_index(drop=True, inplace=True)\nvalid_dataset.tail()","e060a44d":"valid_dataset.shape","d905f539":"test_dataset = pd.DataFrame(columns=[categories])\ntest_dataset.insert(loc=0, column='filename', value=None)\ntest_dataset.head()","55843f44":"del df1, df2, df3, df4, df5\n\ndf1 = test_dataset.copy()\ndf2 = test_dataset.copy()\ndf3 = test_dataset.copy()\ndf4 = test_dataset.copy()\ndf5 = test_dataset.copy()\ndf5.head()","7e1dad24":"myList = [file.split(\"\/\", 1)[1] for file in glob.glob('dataset\/test\/sculpture\/*')]\ntest_dataset = pd.DataFrame(data=myList, columns=['filename'])\ntest_dataset[categories] = pd.DataFrame([[1, 0, 0, 0, 0]], index=dataset.index)\n\nmyList = [file.split(\"\/\", 1)[1] for file in glob.glob('dataset\/test\/iconography\/*')]\ndf2 = pd.DataFrame(data=myList, columns=['filename'])\ndf2[categories] = pd.DataFrame([[0, 1, 0, 0, 0]], index=df2.index)\ntest_dataset = test_dataset.append(df2)\n\nmyList = [file.split(\"\/\", 1)[1] for file in glob.glob('dataset\/test\/engraving\/*')]\ndf3 = pd.DataFrame(data=myList, columns=['filename'])\ndf3[categories] = pd.DataFrame([[0, 0, 1, 0, 0]], index=df3.index)\ntest_dataset = test_dataset.append(df3)\n\nmyList = [file.split(\"\/\", 1)[1] for file in glob.glob('dataset\/test\/drawings\/*')]\ndf4 = pd.DataFrame(data=myList, columns=['filename'])\ndf4[categories] = pd.DataFrame([[0, 0, 0, 1, 0]], index=df4.index)\ntest_dataset = test_dataset.append(df3)\n\nmyList = [file.split(\"\/\", 1)[1] for file in glob.glob('dataset\/test\/painting\/*')]\ndf5 = pd.DataFrame(data=myList, columns=['filename'])\ndf5[categories] = pd.DataFrame([[0, 0, 0, 0, 1]], index=df5.index)\ntest_dataset = test_dataset.append(df5)","c34d1317":"test_dataset.reset_index(drop=True, inplace=True)\ntest_dataset.tail()","831ddc62":"test_dataset.shape","350c5cd8":"datagen=ImageDataGenerator(rescale=1.\/255.)\n\ntrain_generator=datagen.flow_from_dataframe(\n    dataframe=dataset,\n    directory=\"dataset\/\",\n    x_col=\"filename\",\n    y_col=categories,\n    batch_size=32,\n    seed=42,\n    shuffle=True,\n    class_mode=\"other\",\n    target_size=(100,100))\n\nvalid_generator=datagen.flow_from_dataframe(\n    dataframe=valid_dataset,\n    directory=\"dataset\/\",\n    x_col=\"filename\",\n    y_col=categories,\n    batch_size=32,\n    seed=42,\n    shuffle=True,\n    class_mode=\"other\",\n    target_size=(100,100))\n\ntest_generator=datagen.flow_from_dataframe(\n    dataframe=test_dataset,\n    directory=\"dataset\/\",\n    x_col=\"filename\",\n    batch_size=1,\n    seed=42,\n    shuffle=False,\n    class_mode=None,\n    target_size=(100,100))","c21da74e":"# model = Sequential([\n#     Conv2D(32, (3, 3), padding='same', input_shape=(100,100,3), activation='relu'),\n#     Conv2D(32, (3, 3), activation='relu'),\n#     MaxPooling2D(pool_size=(2, 2)),\n    \n#     Dropout(0.25),\n#     Conv2D(64, (3, 3), padding='same', activation='relu'),\n#     Conv2D(64, (3, 3), activation='relu'),\n#     MaxPooling2D(pool_size=(2, 2)),\n    \n#     Dropout(0.25),\n#     Flatten(),\n#     Dense(512, activation='relu'),\n#     Dropout(0.5),\n#     Dense(5, activation='sigmoid')\n# ])\nmodel = Sequential([\n    Conv2D(32, (3, 3), input_shape=(100,100,3), activation='relu'),\n    MaxPooling2D(pool_size=(2, 2)),\n    Dropout(0.2),\n    \n    Conv2D(32, (3, 3), activation='relu'),\n    MaxPooling2D(pool_size=(2, 2)),\n    Dropout(0.2),\n    \n    Conv2D(64, (3, 3), activation='relu'),\n    MaxPooling2D(pool_size=(2, 2)),\n    Dropout(0.2),\n    \n    Flatten(),\n    Dense(128, activation='relu'),\n    Dropout(0.2),\n    Dense(64, activation='relu'),\n    Dropout(0.2),\n    Dense(5, activation='softmax')\n])\n\nmodel.compile(optimizers.rmsprop(lr=0.0001, decay=1e-6),\n              loss=\"binary_crossentropy\", metrics=[\"accuracy\"])","0d3ccda4":"STEP_SIZE_TRAIN=train_generator.n\/\/train_generator.batch_size\nSTEP_SIZE_VALID=valid_generator.n\/\/valid_generator.batch_size\nSTEP_SIZE_TEST=test_generator.n\/\/test_generator.batch_size\n\nmodel.fit_generator(generator=train_generator,\n                    steps_per_epoch=STEP_SIZE_TRAIN,\n                    validation_data=valid_generator,\n                    validation_steps=STEP_SIZE_VALID,\n                    epochs=50\n)","8b9a78a6":"test_generator.reset()\npred = model.predict_generator(test_generator, steps=STEP_SIZE_TEST, verbose=1)","052670a2":"pred_bool = (pred >0.5)","36aa8ced":"predictions = pred_bool.astype(int)\n\nresults = pd.DataFrame(predictions, columns=categories)\nresults[\"filename\"] = test_generator.filenames\nordered_cols = [\"filename\"] + categories\n\n# To get the same column order\nresults = results[ordered_cols]\nresults.to_csv(\"results.csv\", index=False)","eb6aab21":"results.head()","3f2c0855":"input_ = Input(shape = (100,100,3))\nx = Conv2D(32, (3, 3), padding = 'same')(input_)\nx = Activation('relu')(x)\nx = Conv2D(32, (3, 3))(x)\nx = Activation('relu')(x)\nx = MaxPooling2D(pool_size = (2, 2))(x)\nx = Dropout(0.25)(x)\nx = Conv2D(64, (3, 3), padding = 'same')(x)\nx = Activation('relu')(x)\nx = Conv2D(64, (3, 3))(x)\nx = Activation('relu')(x)\nx = MaxPooling2D(pool_size = (2, 2))(x)\nx = Dropout(0.25)(x)\nx = Flatten()(x)\nx = Dense(512)(x)\nx = Activation('relu')(x)\nx = Dropout(0.5)(x)\noutput1 = Dense(1, activation = 'sigmoid')(x)\noutput2 = Dense(1, activation = 'sigmoid')(x)\noutput3 = Dense(1, activation = 'sigmoid')(x)\noutput4 = Dense(1, activation = 'sigmoid')(x)\noutput5 = Dense(1, activation = 'sigmoid')(x)\n\nmodel = Model(input_, [output1, output2, output3, output4, output5])\n\nmodel.compile(optimizers.rmsprop(lr = 0.0001, decay = 1e-6), \n              loss = [\"binary_crossentropy\", \"binary_crossentropy\", \"binary_crossentropy\",\n                      \"binary_crossentropy\", \"binary_crossentropy\"], metrics = [\"accuracy\"])","0532997d":"def generator_wrapper(generator):\n    for batch_x, batch_y in generator:\n        yield (batch_x, [batch_y[:,i] for i in range(5)])","cea9e537":"STEP_SIZE_TRAIN=train_generator.n\/\/train_generator.batch_size\nSTEP_SIZE_VALID=valid_generator.n\/\/valid_generator.batch_size\nSTEP_SIZE_TEST=test_generator.n\/\/test_generator.batch_size\n\nmodel.fit_generator(generator=generator_wrapper(train_generator),\n                    steps_per_epoch=STEP_SIZE_TRAIN,\n                    validation_data=generator_wrapper(valid_generator),\n                    validation_steps=STEP_SIZE_VALID,\n                    epochs=1, verbose=2)","eb20986d":"test_generator.reset()\npred = model.predict_generator(test_generator, steps=STEP_SIZE_TEST, verbose=1)","0658bb0d":"pred","e66c9d34":"print(type(pred[0]))","637e0add":"## Visualize Loss\/Accuracy","43d205b6":"Let's prepare the data using `flow_from_directory` to generate batches of image data (and labels) and resize all images to 128x128.\n\n**Question**: should we preprocess the images for the pre-trained VGG network?","5606f2d2":"**Note**: The first two categories seem to be close together in style and therefore have some overlap.","fccee341":"## Predict and save results in csv file","dbf05db4":"# Preprocessing","f174f0dc":"## Create dataframes from directories","580b5062":"img_width, img_height = 128, 128\ninput_shape = (img_height, img_width, 1)\n\ncategories = ['drawings', 'engraving', 'iconography' ,'painting' ,'sculpture']\n\ntrain_path = 'dataset\/train\/'\nval_path = 'dataset\/validation\/'\ntest_path = 'dataset\/test\/'","9a26fcbf":"## Build Sequential Model and fit","db830cf6":"# Create test set directory\nos.mkdir('dataset\/test\/')\n         \nfor d in src_dirs_2:\n    os.mkdir('dataset\/test\/' + d)","1733be3a":"## Visualize training history\n\nLet's display Loss and Accuravy","53bb6f19":"## Multi-label classification with a Multi-Output Model","c870e1c2":"## Evaluate test data","a557181e":"## Creating a tuple of all images and its category","71fdfa5f":"## What is the distribution across the categories?","1312661c":"Let's have a look at some of the images","f15f28cc":"%ls dataset\/*","08b25d4d":"## Evaluate test data","598dd8ed":"# Moving ~10% of the train data over to test\nimport math\n\nfor d in src_dirs_2:\n    src_dir = 'dataset\/train\/' + d\n    num = len(os.listdir(src_dir)) - math.floor(len(os.listdir(src_dir))*0.1)    \n    images = [file for file in os.listdir(src_dir) if file.endswith('.jpg')]\n    \n    dst_dir = 'dataset\/test\/' + d\n    test_img = images[num:]\n\n    for file in test_img:\n        shutil.copy(os.path.join(src_dir, file), os.path.join(dst_dir, file))","c18d9328":"# How many images do we have\nfor sub_dir in src_dirs_1:\n    for d in src_dirs_2:\n        src_dir = src_dirs_0[0] + '\/' + sub_dir + '\/' + d\n        files = os.listdir(src_dir)\n        print(\"Number of images in {}: {}\".format(src_dir, len(files)))","6f7d2e79":"# Dataset Prep","e5fcc796":"## Create generators","d00b5f21":"## TODO\n\n* Is the shape for the created label arrays correct? Doesn't look like it. Investigate.\n* Evaluate thoughts errors.","d6c50fbe":"# Rename directories\nshutil.move('dataset\/training_set\/', 'dataset\/train')\nshutil.move('dataset\/validation_set\/', 'dataset\/validation')","a5f00320":"## Fine-tuning the network ","c34744af":"# Import necessary libraries","3729c222":"# Conclusion\n\n* For sure we want to use pre-trained networks where we can\n* Being able to use train_test_split seems to me a best option even so we are dumping everything into memory.\n* In regards to the images the *drawings* and *engraving* are close together in style and therefore have some overlap in results.","36d5e214":"# Pre-Trained Network Part 2 (Experimental)","289fc64a":"## Calculate number of images in train, test and validation","caccc852":"## Visualize training history","1f7d251b":"# flow_from_dataframe approach","e3ff05db":"## Evalute test data","a71bc391":"Blog post URL: https:\/\/stephanosterburg.github.io\/deep_learning_art_images\/","69e916c3":"## Create keras model","c1f86cfc":"## Checking for invalid images","cd992b00":"The dataset contains two directories which can be compined. Also, there are some bad images (invalid) which we need to delete. While we are at it, we will re-orginize the directory structure a little bit and rename the images. This is just for us and not necessarily needed for keras. \n\nThis is a one off and can be remove from the notebook. For the time being I will leave it in here for documentation. But shouldn't be executed again, otherwise we will just see errors.","32c54829":"count = 0\n\nfor path in [train_path, test_path, val_path]:\n    for i, cat in enumerate(categories):\n        cat_path = os.path.join(path, cat)\n        images = [file for file in os.listdir(cat_path)]\n\n        for image in images:\n            try:\n                img = Image.open(os.path.join(cat_path, image))\n            except:\n                count += 1\n\n    print(\"Total bad images in {}: {}\".format(path, str(count)))","c5aff530":"SVG(model_to_dot(model, show_layer_names=False, show_shapes=True).create(prog='dot', format='svg'))","e35b8fe4":"<h1>Table of Contents<span class=\"tocSkip\"><\/span><\/h1>\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Final-Project-Submission\" data-toc-modified-id=\"Final-Project-Submission-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;<\/span>Final Project Submission<\/a><\/span><\/li><li><span><a href=\"#Import-necessary-libraries\" data-toc-modified-id=\"Import-necessary-libraries-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;<\/span>Import necessary libraries<\/a><\/span><\/li><li><span><a href=\"#Dataset-Prep\" data-toc-modified-id=\"Dataset-Prep-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;<\/span>Dataset Prep<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Checking-for-invalid-images\" data-toc-modified-id=\"Checking-for-invalid-images-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;<\/span>Checking for invalid images<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Read-in-Data\" data-toc-modified-id=\"Read-in-Data-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;<\/span>Read in Data<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#What-is-the-distribution-across-the-categories?\" data-toc-modified-id=\"What-is-the-distribution-across-the-categories?-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;<\/span>What is the distribution across the categories?<\/a><\/span><\/li><li><span><a href=\"#Calculate-number-of-images-in-train,-test-and-validation\" data-toc-modified-id=\"Calculate-number-of-images-in-train,-test-and-validation-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;<\/span>Calculate number of images in train, test and validation<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Preprocessing\" data-toc-modified-id=\"Preprocessing-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;<\/span>Preprocessing<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Create-keras-model\" data-toc-modified-id=\"Create-keras-model-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;<\/span>Create keras model<\/a><\/span><\/li><li><span><a href=\"#Save-model\" data-toc-modified-id=\"Save-model-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;<\/span>Save model<\/a><\/span><\/li><li><span><a href=\"#Visualize-training-history\" data-toc-modified-id=\"Visualize-training-history-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;<\/span>Visualize training history<\/a><\/span><\/li><li><span><a href=\"#Evalute-test-data\" data-toc-modified-id=\"Evalute-test-data-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;<\/span>Evalute test data<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Pre-Trained-Network-Part-1\" data-toc-modified-id=\"Pre-Trained-Network-Part-1-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;<\/span>Pre-Trained Network Part 1<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Fine-tuning-the-network\" data-toc-modified-id=\"Fine-tuning-the-network-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;<\/span>Fine-tuning the network<\/a><\/span><\/li><li><span><a href=\"#Save-model\" data-toc-modified-id=\"Save-model-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;<\/span>Save model<\/a><\/span><\/li><li><span><a href=\"#Visualize-training-history\" data-toc-modified-id=\"Visualize-training-history-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;<\/span>Visualize training history<\/a><\/span><\/li><li><span><a href=\"#Evaluate-test-data\" data-toc-modified-id=\"Evaluate-test-data-6.4\"><span class=\"toc-item-num\">6.4&nbsp;&nbsp;<\/span>Evaluate test data<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Pre-Trained-Network-Part-2-(Experimental)\" data-toc-modified-id=\"Pre-Trained-Network-Part-2-(Experimental)-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;<\/span>Pre-Trained Network Part 2 (Experimental)<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Visualize-training-history\" data-toc-modified-id=\"Visualize-training-history-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;<\/span>Visualize training history<\/a><\/span><\/li><li><span><a href=\"#Evaluate-test-data\" data-toc-modified-id=\"Evaluate-test-data-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;<\/span>Evaluate test data<\/a><\/span><\/li><li><span><a href=\"#TODO\" data-toc-modified-id=\"TODO-7.3\"><span class=\"toc-item-num\">7.3&nbsp;&nbsp;<\/span>TODO<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#train_test_split-approach\" data-toc-modified-id=\"train_test_split-approach-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;<\/span>train_test_split approach<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Creating-a-tuple-of-all-images-and-its-category\" data-toc-modified-id=\"Creating-a-tuple-of-all-images-and-its-category-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;<\/span>Creating a tuple of all images and its category<\/a><\/span><\/li><li><span><a href=\"#Process-data\" data-toc-modified-id=\"Process-data-8.2\"><span class=\"toc-item-num\">8.2&nbsp;&nbsp;<\/span>Process data<\/a><\/span><\/li><li><span><a href=\"#Build-Sequential-Model-and-fit\" data-toc-modified-id=\"Build-Sequential-Model-and-fit-8.3\"><span class=\"toc-item-num\">8.3&nbsp;&nbsp;<\/span>Build Sequential Model and fit<\/a><\/span><\/li><li><span><a href=\"#Visualize-Loss\/Accuracy\" data-toc-modified-id=\"Visualize-Loss\/Accuracy-8.4\"><span class=\"toc-item-num\">8.4&nbsp;&nbsp;<\/span>Visualize Loss\/Accuracy<\/a><\/span><\/li><li><span><a href=\"#Display-confusion-matrix\" data-toc-modified-id=\"Display-confusion-matrix-8.5\"><span class=\"toc-item-num\">8.5&nbsp;&nbsp;<\/span>Display confusion matrix<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#flow_from_dataframe-approach\" data-toc-modified-id=\"flow_from_dataframe-approach-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;<\/span>flow_from_dataframe approach<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Create-dataframes-from-directories\" data-toc-modified-id=\"Create-dataframes-from-directories-9.1\"><span class=\"toc-item-num\">9.1&nbsp;&nbsp;<\/span>Create dataframes from directories<\/a><\/span><\/li><li><span><a href=\"#Create-generators\" data-toc-modified-id=\"Create-generators-9.2\"><span class=\"toc-item-num\">9.2&nbsp;&nbsp;<\/span>Create generators<\/a><\/span><\/li><li><span><a href=\"#Build-Sequential-Model-and-fit\" data-toc-modified-id=\"Build-Sequential-Model-and-fit-9.3\"><span class=\"toc-item-num\">9.3&nbsp;&nbsp;<\/span>Build Sequential Model and fit<\/a><\/span><\/li><li><span><a href=\"#Predict-and-save-results-in-csv-file\" data-toc-modified-id=\"Predict-and-save-results-in-csv-file-9.4\"><span class=\"toc-item-num\">9.4&nbsp;&nbsp;<\/span>Predict and save results in csv file<\/a><\/span><\/li><li><span><a href=\"#Multi-label-classification-with-a-Multi-Output-Model\" data-toc-modified-id=\"Multi-label-classification-with-a-Multi-Output-Model-9.5\"><span class=\"toc-item-num\">9.5&nbsp;&nbsp;<\/span>Multi-label classification with a Multi-Output Model<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;<\/span>Conclusion<\/a><\/span><\/li><\/ul><\/div>","ee507a9d":"# Pre-Trained Network Part 1\n\nWe can leverage a pre-trained network like the VGG19 architecture, which is pre-trained on the ImageNet dataset. Even so the ImageNet dataset contains only \"cats\" and \"dogs\" it can be used for a more generalized problem like the Art Images.\n\nHere we will only instantiate the convolutional part of the model, everything up to the fully-connected layers. In our case we will freeze the layers of the VGG19 model. And only fine-tune the added layers.\n\nTo further improve the model we could make the last five nodes trainable. But that is for another time.","8fd669bf":"# train_test_split approach","d400d78a":"# Rename files to to something more simple\nfor sub_dir in src_dirs_1:\n    for d in src_dirs_2:\n        src_dir = src_dirs_0[0] + '\/' + sub_dir + '\/' + d\n        files = os.listdir(src_dir)\n\n        print('-' * 50)\n        print('Renaming files in ' + src_dir + '\\n')\n\n        for i, file in enumerate(files):\n            new_name = 'image.' + str(i) + '.jpg'\n            os.rename(os.path.join(src_dir, file), os.path.join(src_dir, new_name))\n#             print(os.path.join(src_dir, new_name))","ef7b368d":"count = 0\n\nfor path in [train_path, test_path, val_path]:\n    for i, cat in enumerate(categories):\n        cat_path = os.path.join(path, cat)\n        images = [file for file in os.listdir(cat_path)]\n\n        for image in images:\n            try:\n                img = Image.open(os.path.join(cat_path, image))\n            except:\n                os.remove(os.path.join(cat_path, image))\n                count += 1\n                \n    print(\"Removed {} bad images from {}\".format(str(count), path))","ca39a3c6":"We will set the `class_mode` to `None`. The generator will only yield batches of image data, which is useful to use with `model.predict_generator()`. This means that the generator will only have batches of data and no labels.","b179bde4":"test_loss, test_acc = model.evaluate_generator(test_generator, steps=32)\ny_hat_test = model.predict_generator(test_generator, steps=32)\n\nprint('Generated {} predictions'.format(len(y_hat_test)))\nprint('Test accuracy: {:.2f}%'.format(test_acc * 100))","69277ea7":"## Process data\n\nRead images and resize into memory","2deec6c8":"## Visualize training history","5ccc014e":"## Display confusion matrix","46462b05":"## Save model","ec514d5c":"# Read in Data","6bf565c9":"## Build Sequential Model and fit","c3458876":"## Save model","6ca89c94":"As we did above we will use the pre-trained VGG19 network. Only this time we will run the model on our training and validation data once and record the output in two numpy arrays. Then we will train a small fully-connected model on top of the stored features.\n\nThe reason why we are storing the features offline rather than adding our fully-connected model directly on top of a frozen convolutional base and running the whole thing, is computational effiency. Running VGG16 is expensive, especially if you're working on CPU, and we want to only do it once. \n\nNote that this prevents us from using data augmentation.","19d075ed":"Looks like that we have duplicates in dataset and musemart.","cffc1638":"Assumging that we have in train, test and validation bad images, we shall remove them."}}