{"cell_type":{"11244d77":"code","936ad6cf":"code","0890ef58":"code","6fcdf140":"code","7025b28f":"code","4d508215":"code","bc6c1d0e":"code","77552552":"code","884c47d8":"code","a07de4de":"code","8ae6d8e9":"code","04794632":"code","7c6b0c29":"code","c6ff297b":"code","6a230a65":"code","381c7d1a":"code","731f3175":"code","d5ed6698":"code","b48d06c5":"code","b50ed797":"code","42d5bbd3":"code","8f32832b":"code","b1cdaaed":"code","d5ac22ea":"code","8357a17a":"code","53bccc03":"code","e067c800":"code","fbffc639":"code","3bfd8579":"code","4bcffa33":"code","be362ece":"code","98d8d045":"code","67399f1d":"code","c5381198":"code","ce01d416":"code","9a52505f":"markdown","2ab160f9":"markdown","81126f0f":"markdown","fad2cacd":"markdown","98f73321":"markdown","2f548370":"markdown","c8735504":"markdown","eebb9e77":"markdown","60c94848":"markdown","df90dc8d":"markdown","6f0b0acd":"markdown","0f663b5d":"markdown","a81f5a60":"markdown","1a1e8b90":"markdown"},"source":{"11244d77":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","936ad6cf":"#import necessary modules\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nsns.set()","0890ef58":"#import the dataset\ncc= pd.read_csv('\/kaggle\/input\/ccdata\/CC GENERAL.csv')\ncc.head()","6fcdf140":"print(cc.shape)","7025b28f":"cc.columns","4d508215":"#Look at data using the info() function\ncc.info()","bc6c1d0e":"#Look at summary statistics of data using the describe() function\ncc.describe(include='all')","77552552":"# Let's get unique values for each category\nunique_vals = {\n    k: cc[k].unique()\n    for k in cc.columns\n}\n\nunique_vals","884c47d8":"#CUST_ID is a dataset artifact, not something useful for analysis\ncc= cc.drop(\"CUST_ID\", axis=1)","a07de4de":"cc.isnull().sum()","8ae6d8e9":"#CREDIT_LIMIT and MINIMUM_PAYMENTS have some missing value.so fill missing values with median value\n\ncc= cc.fillna(cc.median())\n\n# Checking no more NULLs in the data\nall(cc.isna().sum() == 0)","04794632":"cc.describe(include='all')","7c6b0c29":"#since all the attributes are numerical first we will understand the distributions of the data on each attributes\n\ncc.hist(figsize=(20,15))\nplt.title('Data',fontsize=12)\nplt.show()","c6ff297b":"n= len(cc.columns)\n\nplt.figure(figsize=(10,60))\nfor i in range(n):\n    plt.subplot(17,1,i+1)\n    sns.boxplot(cc[cc.columns[i]])\n    plt.title(cc.columns[i])\nplt.tight_layout()","6a230a65":"# Create the correlation matrix\ncorr = cc.corr()\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=bool))\nplt.figure(figsize=(14,10))\n# Add the mask to the heatmap\nsns.heatmap(corr, mask=mask, cmap='YlGnBu',center=0, linewidths=1, annot=True, fmt=\".2f\")\nplt.show()","381c7d1a":"cc.var().sort_values()","731f3175":"from sklearn.preprocessing import StandardScaler\n\nsc= StandardScaler()\ncc_scaled= sc.fit_transform(cc)","d5ed6698":"#checking optimal value of k using elbow method\n\nfrom sklearn.cluster import KMeans\n\nks = range(1, 15)\ninertias = []\nfor k in ks:\n    # Create a KMeans instance with k clusters: model\n    model= KMeans(n_clusters=k)\n        # Fit model to samples\n    model.fit(cc_scaled)\n        # Append the inertia to the list of inertias\n    inertias.append(model.inertia_)\n    # Plot ks vs inertias\nplt.plot(ks, inertias, '-o')\nplt.xlabel('number of clusters, k')\nplt.ylabel('inertia')\nplt.xticks(ks)\nplt.show()","b48d06c5":"clusters_df=pd.DataFrame({'num_clusters':ks,'cluster_errors':inertias})\nclusters_df","b50ed797":"#choose k = 4 for number of clusters, based on plot above. also after k=4 the slope of the line is almot constant as well.\n\nfrom sklearn.cluster import KMeans\n\nKM= KMeans(n_clusters=4)\nKM.fit(cc_scaled)\n\nKM_labels = KM.fit_predict(cc_scaled)\nKM_labels","42d5bbd3":"KM.cluster_centers_.shape","8f32832b":"print(KM.inertia_)","b1cdaaed":"cc['cluster_labels'] = KM_labels\ncc.head()","d5ac22ea":"plt.figure(figsize=(20,15))\ndf1= cc[cc.cluster_labels==0]\ndf2= cc[cc.cluster_labels==1]\ndf3= cc[cc.cluster_labels==2]\ndf4= cc[cc.cluster_labels==3]\n\n\nplt.scatter(df1['PAYMENTS'], df1['PURCHASES'], color='black')\nplt.scatter(df2['PAYMENTS'], df2['PURCHASES'], color='orange')\nplt.scatter(df3['PAYMENTS'], df3['PURCHASES'], color='purple')\nplt.scatter(df4['PAYMENTS'], df4['PURCHASES'], color='blue')\n\nplt.show()","8357a17a":"cc['cluster_labels'].value_counts()","53bccc03":"cc.groupby('cluster_labels').mean()","e067c800":"for c in cc:\n    grid= sns.FacetGrid(cc, col='cluster_labels')\n    grid.map(plt.hist, c)","fbffc639":"#t-SNE provides great visualizations when the individual samples can be labeled\n\nfrom sklearn.manifold import TSNE\nmodel = TSNE(learning_rate=200)\n\n# Apply fit_transform to samples: tsne_features\ntsne_features = model.fit_transform(cc_scaled)\n\n# Select the 0th feature: xs\nxs = tsne_features[:,0]\n# Select the 1st feature: ys\nys = tsne_features[:,1]\n\nplt.figure(figsize=(20,15))\n# Scatter plot, coloring by variety_numbers\nplt.scatter(xs, ys, c=KM_labels)\nplt.show()","3bfd8579":"from scipy.cluster.hierarchy import dendrogram, linkage\n\n#calculate the linkage: mergings\nmergings= linkage(cc_scaled, method='ward')\n\nplt.figure(figsize=(20,15))\n#Plot the dendrogram, using labels\ndendrogram(mergings, labels=KM_labels, p=5, leaf_rotation=90,leaf_font_size=10, truncate_mode='level')\n\nplt.show()","4bcffa33":"from sklearn.decomposition import PCA\n\nmodel= PCA()\n\nmodel.fit_transform(cc_scaled)","be362ece":"# Plot the explained variances\nfeatures = range(model.n_components_)\nplt.figure(figsize=(20,15))\nplt.bar(features, model.explained_variance_)\nplt.xlabel('PCA feature')\nplt.ylabel('variance')\nplt.xticks(features)\nplt.show()","98d8d045":"pca= PCA(n_components= 2)\n\npca.fit(cc_scaled)\npca_features=pca.transform(cc_scaled)\n\nprint(pca_features.shape)","67399f1d":"# Create a dataframe with the two PCA components\npca_df = pd.DataFrame(data=pca_features,columns=['pca1','pca2'])\npca_df.head()","c5381198":"# Concatenate the clusters labels to the dataframe\ndf = pd.concat([pca_df,pd.DataFrame({'cluster':KM_labels})], axis = 1)\ndf.head()","ce01d416":"plt.figure(figsize=(18,12))\nsns.scatterplot(x='pca1', y='pca2', hue='cluster', data=df, palette=['purple','orange','blue','black'])\nplt.xlabel('Principal Component 1', fontsize=13)\nplt.ylabel('Principal Component 2', fontsize=13)\nplt.show()","9a52505f":"**Variances of the PCA features**","2ab160f9":"#### Understanding the Dataset","81126f0f":"**Visualising Hierarchies**","fad2cacd":"**Transforming Features for better clustering**","98f73321":"Notice PCA reduced the high dimension features of 17 to intrinsic dimension of 2. However, the observations remains same","2f548370":"**Dimensionality reduction with PCA**","c8735504":"**Initial Observations**\n\n*     CUST_ID has unique values for each observation. Keeping this will make algorithm complex. we will ignore the columns from our analyis\n*      few of the columns have high variance\n*     There are frquency columns where the values are between 0-1. We need to find a way where we can convert them into categorical as 0-no frequently purchased, 1-not frequently purchased\n    ","eebb9e77":" It looks like PCA features 0 and 1 have significant variance. The intrinsic dimension of this dataset appears to be 2","60c94848":"Most of the features have high variance are in different scales. We need to center these variances around 0.","df90dc8d":"There are outliers present in almost all the features.","6f0b0acd":"**Principal Component Analysis**","0f663b5d":"    The info() function is critical to understand the data.\n    As you can see there are 17 numerical columns and first column as object columns","a81f5a60":"### Exploratory Data Analysis","1a1e8b90":"Cluster-0\n\nBalance is very high and gets updated frequently. Majority of purchases being done by paying cash in advance and it is being done quite frequently. Credit Limit is comparitively high. Minimum Payment done for  the purchases are high compared to others\n\nCluster 1 : \n\nLow balance but the balance gets updated frequently ie. more no. of transactions. No of purchases from the account are also quite large and majority of the purchases are done either in one go or in installments but not by paying cash in advance.\n\nCluster-2\nBalance is comparatively high and balance gets updated. No of Purchases are high. Purchases done either in one go or in installments are very high and done very frequently. Credit Limit is very high. Payments made are very high. Full Payments Percent of full payment paid by user.\n\nCluster-3\n\nBalance is high but the balance doesnt get updated. No. of purchases from the account are very low. Purchases done either in one go or in installments are verly low. Purchases are not done quite frequently. Credit Limit is very low. Payment and Minimum payment is very low"}}