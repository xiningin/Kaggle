{"cell_type":{"6d9a735a":"code","70c39d25":"code","07581503":"code","c668a2f9":"code","43a0e80e":"code","94cfa585":"code","cbe7890a":"code","557c636f":"code","39c66084":"code","69e2f8d8":"code","3e3f7a49":"code","12f580ba":"code","4ef9c178":"code","98d03403":"code","95910428":"code","a22312c7":"code","f82c85e2":"code","fb75474d":"code","2c3a622b":"code","79b5e9b2":"code","f2e336a5":"code","6b95df64":"code","526be63c":"code","5b2a5975":"code","17453de1":"markdown","a89e89c9":"markdown","bc93fd2e":"markdown","27c8ae06":"markdown","da2f1f15":"markdown","b93487ab":"markdown","09ae8eb8":"markdown"},"source":{"6d9a735a":"# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\n# To create temporary directory, type in following in the Console\n# os.chdir(\"\/kaggle\/\")\n# !mkdir temp\n# os.listdir()\n\n# Any results we write to the current directory are saved as output in '\/kaggle\/working\/' directory\nprint()\nprint(os.listdir('..'))\nprint(os.listdir('\/kaggle\/input'))\nprint(os.listdir('\/kaggle\/working\/'))\n# print(os.listdir('\/kaggle\/temp\/'))\n","70c39d25":"# Importing needed libraries\nimport numpy as np\nimport h5py\nimport cv2\n","07581503":"# Opening dataset from HDF5 binary file\n# Initiating File object\n# Opening file in reading mode by 'r'\nwith h5py.File('\/kaggle\/input\/traffic-signs-1-million-images-for-classification\/dataset_ts_original.hdf5', 'r') as f:\n    # Showing all keys in the HDF5 binary file\n    print(list(f.keys()))\n    \n    # Extracting saved arrays for training by appropriate keys\n    # Saving them into new variables    \n    x_train = f['x_train']  # HDF5 dataset\n    y_train = f['y_train']  # HDF5 dataset\n    # Converting them into Numpy arrays\n    x_train = np.array(x_train)  # Numpy arrays\n    y_train = np.array(y_train)  # Numpy arrays\n    \n    \n    # Extracting saved arrays for validation by appropriate keys\n    # Saving them into new variables \n    x_validation = f['x_validation']  # HDF5 dataset\n    y_validation = f['y_validation']  # HDF5 dataset\n    # Converting them into Numpy arrays\n    x_validation = np.array(x_validation)  # Numpy arrays\n    y_validation = np.array(y_validation)  # Numpy arrays\n    \n    \n    # Extracting saved arrays for testing by appropriate keys\n    # Saving them into new variables \n    x_test = f['x_test']  # HDF5 dataset\n    y_test = f['y_test']  # HDF5 dataset\n    # Converting them into Numpy arrays\n    x_test = np.array(x_test)  # Numpy arrays\n    y_test = np.array(y_test)  # Numpy arrays\n\n    \n# Check point\n# Showing shapes of arrays after splitting\nprint(x_train.shape)\nprint(y_train.shape)\n\nprint(x_validation.shape)\nprint(y_validation.shape)\n\nprint(x_test.shape)\nprint(y_test.shape)\n","c668a2f9":"# Implementing normalization by dividing images pixels on 255.0\n# Purpose: to make computation more efficient by reducing values between 0 and 1\nx_train_255 = x_train \/ 255.0\nx_validation_255 = x_validation \/ 255.0\nx_test_255 = x_test \/ 255.0\n","43a0e80e":"# Calculating Mean Image from training dataset\n# (!) We calculate Mean Image only from training dataset\n# And apply it to all sub-datasets\nmean_rgb_dataset_ts = np.mean(x_train_255, axis=0)  # (48, 48, 3)\n","94cfa585":"# Implementing normalization by subtracting Mean Image\n# Purpose: to centralize the data dispersion around zero, that, in turn,\n# is needed for training with respect to learnability and accuracy\n# The images themselves are no longer interpretable to human eyes\n# Pixels' values are now in some range (from negative to positive),\n# where the mean lies at zero\nx_train_255_mean = x_train_255 - mean_rgb_dataset_ts\nx_validation_255_mean = x_validation_255 - mean_rgb_dataset_ts\nx_test_255_mean = x_test_255 - mean_rgb_dataset_ts\n","cbe7890a":"# Saving Mean Image to use it later when testing\n# Initiating File object\n# Creating file with name 'mean_rgb_dataset_ts_original.hdf5'\n# Opening it in writing mode by 'w'\nwith h5py.File('mean_rgb_dataset_ts_original.hdf5', 'w') as f:\n    # Calling methods to create datasets of given shapes and types\n    # Saving Numpy array for Mean Image\n    f.create_dataset('mean', data=mean_rgb_dataset_ts, dtype='f')\n    ","557c636f":"# Saving processed Numpy arrays into new HDF5 binary file\n# Initiating File object\n# Creating file with name 'dataset_ts_rgb_255_mean_original.hdf5'\n# Opening it in writing mode by 'w'\nwith h5py.File('dataset_ts_rgb_255_mean_original.hdf5', 'w') as f:\n    # Calling methods to create datasets of given shapes and types\n    # Saving Numpy arrays for training\n    f.create_dataset('x_train', data=x_train_255_mean, dtype='f')\n    f.create_dataset('y_train', data=y_train, dtype='i')\n\n    # Saving Numpy arrays for validation\n    f.create_dataset('x_validation', data=x_validation_255_mean, dtype='f')\n    f.create_dataset('y_validation', data=y_validation, dtype='i')\n\n    # Saving Numpy arrays for testing\n    f.create_dataset('x_test', data=x_test_255_mean, dtype='f')\n    f.create_dataset('y_test', data=y_test, dtype='i')\n    ","39c66084":"# Calculating Standard Deviation from training dataset\n# (!) We calculate Standard Deviation only from training dataset\n# And apply it to all sub-datasets\nstd_rgb_dataset_ts = np.std(x_train_255_mean, axis=0)  # (48, 48, 3)\n","69e2f8d8":"# Implementing preprocessing by dividing on Standard Deviation\n# Purpose: to scale pixels' values to a smaller range, that, in turn,\n# is needed for training with respect to learnability and accuracy\nx_train_255_mean_std = x_train_255_mean \/ std_rgb_dataset_ts\nx_validation_255_mean_std = x_validation_255_mean \/ std_rgb_dataset_ts\nx_test_255_mean_std = x_test_255_mean \/ std_rgb_dataset_ts\n","3e3f7a49":"# Saving Standard Deviation to use it later when testing\n# Initiating File object\n# Creating file with name 'std_rgb_dataset_ts_original.hdf5'\n# Opening it in writing mode by 'w'\nwith h5py.File('std_rgb_dataset_ts_original.hdf5', 'w') as f:\n    # Calling methods to create datasets of given shapes and types\n    # Saving Numpy array for Mean Image\n    f.create_dataset('std', data=std_rgb_dataset_ts, dtype='f')\n    ","12f580ba":"# Saving processed Numpy arrays into new HDF5 binary file\n# Initiating File object\n# Creating file with name 'dataset_ts_rgb_255_mean_std_original.hdf5'\n# Opening it in writing mode by 'w'\nwith h5py.File('dataset_ts_rgb_255_mean_std_original.hdf5', 'w') as f:\n    # Calling methods to create datasets of given shapes and types\n    # Saving Numpy arrays for training\n    f.create_dataset('x_train', data=x_train_255_mean_std, dtype='f')\n    f.create_dataset('y_train', data=y_train, dtype='i')\n\n    # Saving Numpy arrays for validation\n    f.create_dataset('x_validation', data=x_validation_255_mean_std, dtype='f')\n    f.create_dataset('y_validation', data=y_validation, dtype='i')\n\n    # Saving Numpy arrays for testing\n    f.create_dataset('x_test', data=x_test_255_mean_std, dtype='f')\n    f.create_dataset('y_test', data=y_test, dtype='i')\n","4ef9c178":"# Check point\n# Printing some values from matrices\nprint('Original:            ', x_train_255[0, 0, :5, 0])\nprint('- Mean Image:        ', x_train_255_mean[0, 0, :5, 0])\nprint('\/ Standard Deviation:', x_train_255_mean_std[0, 0, :5, 0])\nprint()\n\n# Check point\n# Printing some values of Mean Image and Standard Deviation\nprint('Mean Image:          ', mean_rgb_dataset_ts[0, :5, 0])\nprint('Standard Deviation:  ', std_rgb_dataset_ts[0, :5, 0])\nprint()\n","98d03403":"# Converting all images to GRAY by OpenCV function\nx_train = np.array(list(map(lambda x: cv2.cvtColor(x, cv2.COLOR_RGB2GRAY), x_train)))\nx_validation = np.array(list(map(lambda x: cv2.cvtColor(x, cv2.COLOR_RGB2GRAY), x_validation)))\nx_test = np.array(list(map(lambda x: cv2.cvtColor(x, cv2.COLOR_RGB2GRAY), x_test)))\n\n# Extending dimension from (n, height, width) to (n, height, width, one channel)\nx_train = x_train[:, :, :, np.newaxis]\nx_validation = x_validation[:, :, :, np.newaxis]\nx_test = x_test[:, :, :, np.newaxis]\n\n# Check point\n# Showing shapes of Numpy arrays with GRAY images\nprint('Numpy arrays of Custom Dataset')\nprint(x_train.shape)\nprint(x_validation.shape)\nprint(x_test.shape)\nprint()\n","95910428":"# Implementing normalization by dividing images pixels on 255.0\n# Purpose: to make computation more efficient by reducing values between 0 and 1\nx_train_255 = x_train \/ 255.0\nx_validation_255 = x_validation \/ 255.0\nx_test_255 = x_test \/ 255.0\n","a22312c7":"# Calculating Mean Image from training dataset\n# (!) We calculate Mean Image only from training dataset\n# And apply it to all sub-datasets\nmean_gray_dataset_ts = np.mean(x_train_255, axis=0)  # (48, 48, 1)\n","f82c85e2":"# Implementing normalization by subtracting Mean Image\n# Purpose: to centralize the data dispersion around zero, that, in turn,\n# is needed for training with respect to learnability and accuracy\n# The images themselves are no longer interpretable to human eyes\n# Pixels' values are now in some range (from negative to positive),\n# where the mean lies at zero\nx_train_255_mean = x_train_255 - mean_gray_dataset_ts\nx_validation_255_mean = x_validation_255 - mean_gray_dataset_ts\nx_test_255_mean = x_test_255 - mean_gray_dataset_ts\n","fb75474d":"# Saving Mean Image to use it later when testing\n# Initiating File object\n# Creating file with name 'mean_gray_dataset_ts_original.hdf5'\n# Opening it in writing mode by 'w'\nwith h5py.File('mean_gray_dataset_ts_original.hdf5', 'w') as f:\n    # Calling methods to create datasets of given shapes and types\n    # Saving Numpy array for Mean Image\n    f.create_dataset('mean', data=mean_gray_dataset_ts, dtype='f')\n    ","2c3a622b":"# Saving processed Numpy arrays into new HDF5 binary file\n# Initiating File object\n# Creating file with name 'dataset_ts_gray_255_mean_original.hdf5'\n# Opening it in writing mode by 'w'\nwith h5py.File('dataset_ts_gray_255_mean_original.hdf5', 'w') as f:\n    # Calling methods to create datasets of given shapes and types\n    # Saving Numpy arrays for training\n    f.create_dataset('x_train', data=x_train_255_mean, dtype='f')\n    f.create_dataset('y_train', data=y_train, dtype='i')\n\n    # Saving Numpy arrays for validation\n    f.create_dataset('x_validation', data=x_validation_255_mean, dtype='f')\n    f.create_dataset('y_validation', data=y_validation, dtype='i')\n\n    # Saving Numpy arrays for testing\n    f.create_dataset('x_test', data=x_test_255_mean, dtype='f')\n    f.create_dataset('y_test', data=y_test, dtype='i')\n    ","79b5e9b2":"# Calculating Standard Deviation from training dataset\n# (!) We calculate Standard Deviation only from training dataset\n# And apply it to all sub-datasets\nstd_gray_dataset_ts = np.std(x_train_255_mean, axis=0)  # (48, 48, 1)\n","f2e336a5":"# Implementing preprocessing by dividing on Standard Deviation\n# Purpose: to scale pixels' values to a smaller range, that, in turn,\n# is needed for training with respect to learnability and accuracy\nx_train_255_mean_std = x_train_255_mean \/ std_gray_dataset_ts\nx_validation_255_mean_std = x_validation_255_mean \/ std_gray_dataset_ts\nx_test_255_mean_std = x_test_255_mean \/ std_gray_dataset_ts\n","6b95df64":"# Saving Standard Deviation to use it later when testing\n# Initiating File object\n# Creating file with name 'std_gray_dataset_ts_original.hdf5'\n# Opening it in writing mode by 'w'\nwith h5py.File('std_gray_dataset_ts_original.hdf5', 'w') as f:\n    # Calling methods to create datasets of given shapes and types\n    # Saving Numpy array for Mean Image\n    f.create_dataset('std', data=std_gray_dataset_ts, dtype='f')\n    ","526be63c":"# Saving processed Numpy arrays into new HDF5 binary file\n# Initiating File object\n# Creating file with name 'dataset_ts_gray_255_mean_std_original.hdf5'\n# Opening it in writing mode by 'w'\nwith h5py.File('dataset_ts_gray_255_mean_std_original.hdf5', 'w') as f:\n    # Calling methods to create datasets of given shapes and types\n    # Saving Numpy arrays for training\n    f.create_dataset('x_train', data=x_train_255_mean_std, dtype='f')\n    f.create_dataset('y_train', data=y_train, dtype='i')\n\n    # Saving Numpy arrays for validation\n    f.create_dataset('x_validation', data=x_validation_255_mean_std, dtype='f')\n    f.create_dataset('y_validation', data=y_validation, dtype='i')\n\n    # Saving Numpy arrays for testing\n    f.create_dataset('x_test', data=x_test_255_mean_std, dtype='f')\n    f.create_dataset('y_test', data=y_test, dtype='i')\n","5b2a5975":"# Check point\n# Printing some values from matrices\nprint('Original:            ', x_train_255[0, 0, :5, 0])\nprint('- Mean Image:        ', x_train_255_mean[0, 0, :5, 0])\nprint('\/ Standard Deviation:', x_train_255_mean_std[0, 0, :5, 0])\nprint()\n\n# Check point\n# Printing some values of Mean Image and Standard Deviation\nprint('Mean Image:          ', mean_gray_dataset_ts[0, :5, 0])\nprint('Standard Deviation:  ', std_gray_dataset_ts[0, :5, 0])\nprint()\n","17453de1":"* Applying pre-processing techniques to **\"original\"** version","a89e89c9":"# \ud83c\udf00 Pre-processing RGB \"original\" version of Traffic Signs","bc93fd2e":"# \ud83d\udce5 Importing needed libraries","27c8ae06":"**Design**, **Train** & **Test** deep CNN for Image Classification.\n\n**Join** the course & enjoy new opportunities to get deep learning skills:\n\n\n[https:\/\/www.udemy.com\/course\/convolutional-neural-networks-for-image-classification\/](https:\/\/www.udemy.com\/course\/convolutional-neural-networks-for-image-classification\/?referralCode=12EE0D74A405BF4DDE9B)\n\n\n![](https:\/\/github.com\/sichkar-valentyn\/1-million-images-for-Traffic-Signs-Classification-tasks\/blob\/main\/images\/slideshow_classification.gif?raw=true)","da2f1f15":"# \ud83c\udf93 Related course for classification tasks","b93487ab":"# \u26d4\ufe0f Pre-processing of Traffic Signs Dataset","09ae8eb8":"# \ud83c\udf00 Pre-processing GRAY \"original\" version of Traffic Signs"}}