{"cell_type":{"0ce036f6":"code","0882165e":"code","ca4b0d8d":"code","4d81e690":"code","b79c55fb":"code","9fc4e51a":"code","e61a084e":"code","f86387bd":"code","0689af03":"code","8cd3f4ce":"code","bebaaf34":"code","ff8ae3ef":"markdown"},"source":{"0ce036f6":"import spacy\n#load returns a language object to nlp\nnlp = spacy.load('en')\n#doc object with string input returns doc object\ndoc = nlp('My name is danny')\n#doc object is an iterable of tokens\n#tokenization first splits the tokens based on white space\n# secondly,special characters based splitting happens ,eg:don't->don and t\n#thirdly, prefix and suffix special characters are removed\n\n#When you call nlp on a text, spaCy first tokenizes the text to produce a Doc object. \n#The Doc is then processed in several different steps \u2013 this is also referred to as the processing pipeline. \n#text --> pipeline --> Doc\n#each model will specify the pipeline to use in its meta data, as a simple list containing the component names\nfor token in doc:\n    print(\"{0}\\t{1}\\t{2} \".format(token.text,token.idx,token.pos_))","0882165e":"doc1 = nlp(\"My name is danny.I love music.I love in bangalore\")\nfor sent in doc1.sents:\n    print(sent)","ca4b0d8d":"doc3 = nlp(\"My name is danny mathew\")\nfor token in doc3:\n    print(token,\" : \",token.pos_)","4d81e690":"doc4 = nlp(\"I am Danny and I will be in Arizona next week\")\nfor ent in doc4.ents:\n    print(ent.text,ent.label_)","b79c55fb":"from spacy import displacy\n \ndoc = nlp('I just bought 2 shares at 9 a.m. because the stock went up 30% in just 2 days according to the WSJ')\ndisplacy.render(doc, style='ent', jupyter=True)","9fc4e51a":"#Noun chunks are \"base noun phrases\" \u2013 flat phrases that have a noun as their head.\nYou can think of noun chunks as a noun plus the words describing the noun\ndoc5 = nlp(\"Today was a great day\")\nfor chunk in doc5.noun_chunks:\n    print(chunk.text,chunk.label_,chunk.root.text)","e61a084e":"doc = nlp('Wall Street Journal just published an interesting piece on crypto currencies')\n \nfor token in doc:\n    print(\"{0}\/{1} <--{2}-- {3}\/{4}\".format(\n        token.text, token.tag_, token.dep_, token.head.text, token.head.tag_))","f86387bd":"from spacy import displacy\n \ndoc = nlp('Today was a nice day')\ndisplacy.render(doc, style='dep', jupyter=True, options={'distance': 90})","0689af03":"from scipy import spatial\nnlp = spacy.load('en_core_web_lg')\ncosine_similarity = lambda x, y: 1 - spatial.distance.cosine(x, y)\nman = nlp.vocab['man'].vector\nwoman = nlp.vocab['woman'].vector\nqueen = nlp.vocab['queen'].vector\nking = nlp.vocab['king'].vector\n# We now need to find the closest vector in the vocabulary to the result of \"man\" - \"woman\" + \"queen\"\nmaybe_king = man - woman + queen\ncomputed_similarities = []\nfor word in nlp.vocab:\n# Ignore words without vectors\n    if not word.has_vector:\n        continue\n    similarity = cosine_similarity(maybe_king, word.vector)\n    computed_similarities.append((word, similarity))\ncomputed_similarities = sorted(computed_similarities, key=lambda item: -item[1])\nprint([w[0].text for w in computed_similarities[:10]])\n# ['Queen', 'QUEEN', 'queen', 'King', 'KING', 'king', 'KIng', 'KINGS', 'kings', 'Kings']\n","8cd3f4ce":"banana = nlp.vocab['banana']\ndog = nlp.vocab['dog']\nfruit = nlp.vocab['fruit']\nanimal = nlp.vocab['animal']\n \nprint(dog.similarity(animal), dog.similarity(fruit)) # 0.6618534 0.23552845","bebaaf34":"target = nlp(\"Cats are beautiful animals.\")\n \ndoc1 = nlp(\"Dogs are awesome.\")\ndoc2 = nlp(\"Some gorgeous creatures are felines.\")\n \nprint(target.similarity(doc1))  # 0.8901765218466683\nprint(target.similarity(doc2))","ff8ae3ef":"**Spacy features**\n\nTokenization : Segmenting text into words, punctuations marks etc.\n\nPart-of-speech (POS) Tagging : Assigning word types to tokens, like verb or noun.\n\nDependency Parsing : Assigning syntactic dependency labels, describing the relations between individual tokens, like subject or object.\n\nLemmatization : Assigning the base forms of words. For example, the lemma of \"was\" is \"be\", and the lemma of \"rats\" is \"rat\".\n\nSentence Boundary Detection (SBD) : Finding and segmenting individual sentences.\n\nNamed Entity Recognition (NER) : Labelling named \"real-world\" objects, like persons, companies or locations.\n\nSimilarity : Comparing words, text spans and documents and how similar they are to each other.\n\nText Classification : Assigning categories or labels to a whole document, or parts of a document.\n\nRule-based Matching : Finding sequences of tokens based on their texts and linguistic annotations, similar to regular expressions.\n\nTraining : Updating and improving a statistical model's predictions.\n\nSerialization : Saving objects to files or byte strings."}}