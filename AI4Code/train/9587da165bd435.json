{"cell_type":{"9dd6c1af":"code","baef43d2":"code","51fafe36":"code","01ed24da":"code","5e225e57":"code","44be9b68":"code","a7c685d1":"code","5532f9be":"code","71d0fc81":"code","21e22f42":"code","052ef030":"code","4b1fa39e":"code","625b71b9":"code","8421f7fb":"code","a1ffcc16":"code","c36108ad":"code","fc45fb1d":"code","b11c516c":"code","f4cd3906":"code","09295b02":"code","19214fd6":"code","50de65be":"code","e9418ca5":"code","06c3421c":"code","886a1240":"code","818c0bf7":"code","2f81d2eb":"code","2a10e852":"code","cc09faa6":"code","a8a078e3":"code","e8d6124a":"code","6f923091":"code","c5ca4aa8":"code","ac59a66a":"code","74f31817":"code","755df0d5":"code","d937344a":"code","14e388d1":"markdown","1d4e2c3d":"markdown","c9da7fac":"markdown","242b9572":"markdown","0c2c2c9b":"markdown"},"source":{"9dd6c1af":"import pandas as pd\nimport spacy\nfrom pandarallel import pandarallel\nfrom nltk.tokenize import word_tokenize, WhitespaceTokenizer\nfrom spacy.lang.en.stop_words import STOP_WORDS\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB","baef43d2":"nlp = spacy.load(\"en_core_web_sm\")\npandarallel.initialize(progress_bar=False)\nwst = WhitespaceTokenizer()","51fafe36":"df_train = pd.read_csv('..\/input\/ruddit-jigsaw-dataset-combined-cleaned\/toxic_train.csv')","01ed24da":"df_train.head(10)","5e225e57":"df_validate = pd.read_csv('..\/input\/jigsaw-toxic-severity-rating\/validation_data.csv')","44be9b68":"df_validate.head(10)","a7c685d1":"df_test = pd.read_csv('..\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv')","5532f9be":"df_test.head(10)","71d0fc81":"def dropNA(df):\n    return df.dropna()","21e22f42":"def removeDeletedPandas(data, column):\n    text = data[column]\n    if text.strip() == '[deleted]':\n        return True\n\ndef dropDeletedComments(df, text_column):\n    df['isDeleted'] = df.parallel_apply(removeDeletedPandas, axis=1, args=(text_column,))\n    df = df[(df['isDeleted'] != True)]\n    df = df.drop('isDeleted', axis=1)\n    return df","052ef030":"def removeNewLinesPandas(data, column):\n    text = data[column]\n    return \" \".join(text.split())\n\ndef removeNewLines(df, text_column, out_column):\n    df[out_column] = df.parallel_apply(removeNewLinesPandas, axis=1, args=(text_column,))\n    return df","4b1fa39e":"def lemmatizeSpacyPandas(data, column):\n    text = data[column]\n    doc = nlp(text)\n    text_to_return = ''\n    for token in doc:\n        text_to_return = text_to_return + token.lemma_ + \" \"\n    return text_to_return\n\ndef lemmatizeComments(df, text_column, out_column):\n    df[out_column] = df.parallel_apply(lemmatizeSpacyPandas, axis=1, args=(text_column,))\n    return df","625b71b9":"def removePunctuationsAndNumbersPandas(data, column):\n    text_split = wst.tokenize(data[column])\n    noPunctAndNumbers = ' '.join(word_tokenize(' '.join([''.join([char for char in word if char.isalpha()]).lower() for word in text_split])))\n    return noPunctAndNumbers\n\ndef removePunctuationsAndNumbers(df, text_column, out_column):\n    df[out_column] = df.parallel_apply(removePunctuationsAndNumbersPandas, axis=1, args=(text_column,))\n    return df","8421f7fb":"def removeNonAlphaPandas(data, column):\n    string = data[column]\n    space_split_str = string.split()\n    cleaned_words = []\n    for word in space_split_str:\n        cleaned_word = ''\n        for c in word:\n            if c.isalpha():\n                cleaned_word = cleaned_word + c\n        cleaned_words.append(cleaned_word)\n    \n    return \" \".join(cleaned_words)\n\ndef removeNonAlpha(df, text_column, out_column):\n    df[out_column] = df.parallel_apply(removeNonAlphaPandas, axis=1, args=(text_column,))\n    return df","a1ffcc16":"def cleanStopWordsPandas(data, column):\n    string = data[column]\n    spacy_str = nlp(string)\n    spacy_str_tokens = []\n    \n    for token in spacy_str:\n        spacy_str_tokens.append(token.text)\n        \n    filtered_str =[] \n\n    for word in spacy_str_tokens:\n        lexeme = nlp.vocab[word]\n        if lexeme.is_stop == False:\n            filtered_str.append(word) \n    \n    return \" \".join(filtered_str)\n\ndef cleanStopWords(df, text_column, out_column):\n    df[out_column] = df.parallel_apply(cleanStopWordsPandas, axis=1, args=(text_column,))\n    return df","c36108ad":"df_validate.head(10)","fc45fb1d":"df_validate = dropNA(df_validate)\ndf_validate = removeNewLines(df_validate, 'less_toxic', 'less_toxic_processed')\ndf_validate = removeNewLines(df_validate, 'more_toxic', 'more_toxic_processed')\ndf_validate = lemmatizeComments(df_validate, 'less_toxic_processed', 'less_toxic_processed')\ndf_validate = lemmatizeComments(df_validate, 'more_toxic_processed', 'more_toxic_processed')","b11c516c":"df_validate = removePunctuationsAndNumbers(df_validate, 'less_toxic_processed', 'less_toxic_processed')\ndf_validate = removePunctuationsAndNumbers(df_validate, 'more_toxic_processed', 'more_toxic_processed')\ndf_validate = removeNonAlpha(df_validate, 'less_toxic_processed', 'less_toxic_processed')\ndf_validate = removeNonAlpha(df_validate, 'more_toxic_processed', 'more_toxic_processed')\ndf_validate = cleanStopWords(df_validate, 'less_toxic_processed', 'less_toxic_processed')\ndf_validate = cleanStopWords(df_validate, 'more_toxic_processed', 'more_toxic_processed')","f4cd3906":"df_validate.head(10)","09295b02":"df_test.head(10)","19214fd6":"df_test = dropNA(df_test)\ndf_test = removeNewLines(df_test, 'text', 'processed')\ndf_test = lemmatizeComments(df_test, 'processed', 'processed')","50de65be":"df_test = removePunctuationsAndNumbers(df_test, 'processed', 'processed')\ndf_test = removeNonAlpha(df_test, 'processed', 'processed')\ndf_test = cleanStopWords(df_test, 'processed', 'processed')","e9418ca5":"df_test.head(10)","06c3421c":"df_train['isOffensive'] = df_train.parallel_apply(lambda x: 1 if (x['offensiveness_score'] > 0) else 0, axis=1)","886a1240":"df_train.head(10)","818c0bf7":"tfidfVectorizer = TfidfVectorizer()","2f81d2eb":"nb_tfidf_model = MultinomialNB(alpha=0.139)","2a10e852":"X = tfidfVectorizer.fit_transform(df_train['processed'])\nY = df_train['isOffensive']","cc09faa6":"nb_tfidf_model.fit(X, Y)","a8a078e3":"X_validate_less_toxic = tfidfVectorizer.transform(df_validate['less_toxic_processed'])\nX_validate_more_toxic = tfidfVectorizer.transform(df_validate['more_toxic_processed'])","e8d6124a":"Y_validate_less_toxic = nb_tfidf_model.predict_proba(X_validate_less_toxic)\nY_validate_more_toxic = nb_tfidf_model.predict_proba(X_validate_more_toxic)","6f923091":"(Y_validate_less_toxic[:, 1] < Y_validate_more_toxic[:, 1]).mean()","c5ca4aa8":"X_test = tfidfVectorizer.transform(df_test['processed'])","ac59a66a":"Y_test = nb_tfidf_model.predict_proba(X_test)","74f31817":"df_test['score'] = Y_test[:, 1]","755df0d5":"df_test.head(10)","d937344a":"df_test[['comment_id', 'score']].to_csv(\"submission.csv\", index=False)","14e388d1":"#### Clean Validation Data","1d4e2c3d":"#### Clean Test Data","c9da7fac":"#### Model 2: Tfidf, Binary Y, Naive Bayes","242b9572":"### Data Clean Functions","0c2c2c9b":"#### Create a Binary Field For Classification"}}