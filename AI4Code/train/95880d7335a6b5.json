{"cell_type":{"6f1ca17a":"code","6a8aa64f":"code","e7117d13":"code","ceb9e1e8":"code","3a484b2f":"code","40b9ad84":"code","17f6e609":"code","b6766b32":"code","e7d9c720":"code","f4eeb7fc":"code","604772d3":"code","7d092b84":"code","fe9c2761":"code","421cdb1e":"code","8dcb923f":"code","1febe02c":"code","d93eaf1f":"code","ed210c44":"code","3f2b4a54":"code","23604ef8":"code","76c93525":"code","49e4ef9e":"code","1dc68d4b":"code","3a49ffdd":"code","c73501d5":"code","aaa0960e":"code","a2fd1419":"code","1cdedd97":"code","5a58e154":"code","185968e8":"code","d982f7f9":"markdown","7bae6a12":"markdown","7daa481c":"markdown","3673dd81":"markdown","5c6e59e0":"markdown","c81b9946":"markdown","264e3063":"markdown","fd7919fb":"markdown","a8b6def4":"markdown","5d28a7c0":"markdown"},"source":{"6f1ca17a":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\npd.set_option('display.max_columns', 50)\npd.set_option('display.max_columns', 100)\n\ndf = pd.read_csv(\"..\/input\/videogamesales\/vgsales.csv\")\nds = df.copy()","6a8aa64f":"ds.info()","e7117d13":"ds.head(10)","ceb9e1e8":"#checking for duplicated values\nds.duplicated().sum()","3a484b2f":"#representing Platform and Genre\n# ds[\"Platform\"].value_counts().plot.pie()  --> not appropriate, too many categories\nfig, ax = plt.subplots(figsize=(10, 10))\nfig = sns.countplot(x=ds[\"Platform\"], ax=ax, order=ds[\"Platform\"].value_counts().index)\nax.set_xticklabels(fig.get_xticklabels(), rotation=45, horizontalalignment='right')\nplt.show()\n\nds[\"Genre\"].value_counts().plot.pie(labeldistance=1.1, autopct=\"%.2f%%\", radius=1.5) \nplt.show()","40b9ad84":"#Most prolific years for video games publishing\nfig, ax = plt.subplots(figsize=(15, 15))\nsns.countplot(x=ds[\"Year\"], data=ds, ax=ax)\nax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\nplt.show()","17f6e609":"#No game after 2016, so we drop these 4 years to avoid issues of representation\nds = ds[ds[\"Year\"] < 2017]","b6766b32":"#We plot the median rank per year to see what was the best year in terms of median sales \nds.groupby(\"Year\")[\"Rank\"].median().sort_values(ascending=True).plot.bar(figsize=(15, 5))","e7d9c720":"#By curiosity, we will check why 1989 and 1990 were the best year for median VG Rank\nds[(ds[\"Year\"] == 1989)| (ds[\"Year\"] ==1990)]","f4eeb7fc":"### Mean Rank per year to see if the best sales were located in a particular year\nds.groupby(\"Year\")[\"Rank\"].mean().plot.bar(figsize=(15, 5))","604772d3":"#We plot the same graph with the median to check the possible effect of very low rank wideo games\nds.groupby(\"Year\")[\"Rank\"].median().plot.bar(figsize=(15, 5))","7d092b84":"# info about publishers\nprint(\"Number of different publishers:\", ds[\"Publisher\"].unique().size)\ntop10_pub = (ds[\"Publisher\"].value_counts().head(10)\/ds.shape[0]*100).sum()\nlow500_pub = (ds[\"Publisher\"].value_counts().tail(500)\/ds.shape[0]*100).sum()\n\nprint(\"Share of the 10 biggest publishers in the VG world: {:.2f}%\"\n      .format(top10_pub))\n\nprint(\"Share of the 69 intermediate publishers in the VG world: {:.2f}%\"\n      .format(100-top10_pub-low500_pub))\n\nprint(\"Share of the 500 smallest publishers in the VG world: {:.2f}%\"\n      .format(low500_pub))\n\nprint(\"\\n\\nTop 10 publisher :\\n\", \n    ds[\"Publisher\"].value_counts().head(10))","fe9c2761":"for platform in ds[\"Platform\"].unique():\n    first_pub_yr = ds.loc[ds[\"Platform\"] == platform, \"Year\"].min() \n    last_pub_yr = ds.loc[ds[\"Platform\"] == platform, \"Year\"].max()\n    pub_span = last_pub_yr - first_pub_yr \n    print(f\"The first year of VG publishing for the {platform} is : {first_pub_yr}\")\n    print(f\"The last year of VG publishing for the {platform} is : {last_pub_yr}\")\n    print(f\"So the total publishing period is {pub_span}\\n\\n\")\n\n","421cdb1e":"### The data for the Nintendo DS makes us wonder what is the outlier from the '80s'\n### given that the DS was released on late 2004 and was stoppped in year 2013\nds.loc[(ds[\"Platform\"] == \"DS\") & ((ds[\"Year\"] < 2004) | (ds[\"Year\"] > 2014))].value_counts()","8dcb923f":"#On which platform the median VG was a hit\nds.groupby(\"Platform\")[\"Global_Sales\"].median().sort_values(ascending=False).plot.bar(figsize=(15, 5),\n                                                                                     ylabel=\"Global Sales in millions of copies per VG (median)\")","1febe02c":"#On which platform the total VG sales were the best\nds.groupby(\"Platform\")[\"Global_Sales\"].sum().sort_values(ascending=False).plot.bar(figsize=(15, 5),\n                                                                                   ylabel=\"Global Sales in millions of copies\")","d93eaf1f":"#What are the best Genre for Global_Sales in general\nds.groupby(\"Genre\")[\"Global_Sales\"].sum().sort_values(ascending=False).plot.bar(figsize=(15, 5),\n                                                                                   ylabel=\"Global Sales in millions of copies\")\nplt.show()\n#What are the best Genre for Global_Sales (median video games sales)\nds.groupby(\"Genre\")[\"Global_Sales\"].median().sort_values(ascending=False).plot.bar(figsize=(15, 5),\n                                                                                   ylabel=\"Global Sales in millions of copies per VG (median)\")\nplt.show()","ed210c44":"#What are the Top 10 Publishers for Global_Sales in general\nds.groupby(\"Publisher\")[\"Global_Sales\"].sum().sort_values(ascending=False).head(10).plot.bar(figsize=(15, 5),\n                                                                                   ylabel=\"Global Sales in millions of copies\")\nplt.show()\n#What are the Top 10 Publishers  for Global_Sales (median video games sales)\nds.groupby(\"Publisher\")[\"Global_Sales\"].median().sort_values(ascending=False).head(10).plot.bar(figsize=(15, 5),\n                                                                                   ylabel=\"Global Sales in millions of copies per VG (median)\")\nplt.show()","3f2b4a54":"# What games did these publishers released ? Except Valve (Half-Life) and Maxis (the Sims, Sim City)\n# they are mainly unknown to the average gamer. So they should basically be \"One Trick Poney\" publishers\n# ie : publishers with only one or two games that were blockbusters.\ntop10_pub_median = ds.groupby(\"Publisher\")[\"Global_Sales\"].median().sort_values(ascending=False).head(10)","23604ef8":"for pub in top10_pub_median.index:\n    print(f\"{pub} is famous for:\")\n    display(ds[ds[\"Publisher\"] == pub])","76c93525":"# Correlation between different markets\nsns.heatmap(ds.corr(), annot=True)","49e4ef9e":"#There is a high correlation between Global_Sales, NA_Sales and EU_Sales\n# This oculd be explained by the weight of NA_Sales and EU_Sales in the Global_Sales.\n# Let's confirm it by computing the distribution of the different sales:\n(ds.iloc[:, 6:].sum(axis=0))\/(ds[\"Global_Sales\"].sum(axis=0))*100","1dc68d4b":"# Japan seems to be a independant market compared to the Western market (EU and NA)\n# It's confirmed by the top 10 of JP_Sales that differs markedly from Global_Sales \nds.sort_values(by=\"JP_Sales\", ascending=False).head(10)","3a49ffdd":"# Outliers can be seen on this chart, mainly for the 80's\nsns.catplot(x=\"Year\", y=\"Global_Sales\", kind=\"bar\", data=ds, height=10, aspect=2)","c73501d5":"#I check for some differences between the whole dataset and the VG lines with missing data. \n#just to see if a dropna would have any impact on the global overview and the future modelization\nsns.pairplot(ds[ds.isna().any(axis=1)])","aaa0960e":"#First, we restart with the original dataset\nds = df.copy()","a2fd1419":"%%time\n\n#defining a preprocessing function\nfrom sklearn.preprocessing import (OneHotEncoder, StandardScaler,PolynomialFeatures,\n                                   Normalizer, MinMaxScaler, RobustScaler, PowerTransformer,\n                                QuantileTransformer)\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.linear_model import (LinearRegression, ElasticNetCV, SGDRegressor, Lasso,\n                                    LassoLarsCV)\nfrom sklearn.svm import SVR\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.pipeline import make_union, make_pipeline\n\n\n\ndef imputation(df):\n    # We remove the video Games with NaN Data and the VG after 2017. No need for true imputation here\n    \n    df = df.dropna(axis=0)\n    df = df[df[\"Year\"] < 2017]\n    \n    # We remove the name column as well, as it doesn't help the model to learn\n    df = df.drop(columns=\"Name\")\n    \n    #We remove the rank, beacause the data is redondant with sales (can cause information leak in the model\n    df = df.drop(columns=\"Rank\")\n    \n    #we remove the different sales as they are a component of  Global_Sales\n    df = df.drop(columns=[\"NA_Sales\", \"EU_Sales\", \"JP_Sales\", \"Other_Sales\"])\n    return df\n\ndef encoding(X):\n    ## No need so far\n    return 0\n\ndef preprocessing(df):\n    df = imputation(df)\n    \n    #We transform the non-numerical categories with an encoder\n    df = pd.get_dummies(df)\n    \n    #Separation of features and target\n    y = df[\"Global_Sales\"]\n    X = df.drop(\"Global_Sales\", axis=1)\n    return df, X, y\n\ndef modelisation(df):\n\n    df, X, y = preprocessing(df)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n   \n    model = make_pipeline(RobustScaler(), DecisionTreeRegressor())\n  \n    params = {\"decisiontreeregressor__max_depth\": np.arange(2, 20 ,4),\n             \"decisiontreeregressor__min_samples_split\": np.arange(2, 20 ,4),\n             \"decisiontreeregressor__min_samples_leaf\": np.arange(2, 20 ,4)}\n\n    grid = GridSearchCV(model, param_grid=params, cv=5, verbose=1, return_train_score=True)\n    grid.fit(X_train, y_train)\n    print(grid.score(X_train, y_train))\n    print(grid.best_params_)\n    print(grid.score(X_test, y_test))\n","1cdedd97":"%%time\nmodelisation(ds)","5a58e154":"def modelisation_2(df):\n\n    df, X, y = preprocessing(df)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n    \n    \n    model = make_pipeline(RobustScaler(), SGDRegressor())\n    params = {\"sgdregressor__alpha\": np.float_power(10, -np.arange(2, 7)),\n             \"sgdregressor__max_iter\": [np.ceil(10**6 \/ X_train.shape[0]), 100, 1000, 10000, 100000]} \n    #np.ceil(10**6 \/ X_train.shape[0] => empirical advice from SGDRegressor documentation\n    \n    grid = GridSearchCV(model, param_grid=params, cv=5, verbose=1, return_train_score=True)\n    grid.fit(X_train, y_train)\n    print(grid.score(X_train, y_train))\n    print(grid.best_params_)\n    print(grid.score(X_test, y_test))\n  ","185968e8":"%%time\nmodelisation_2(ds)","d982f7f9":"# 3. Develop a predictive model for VG Global_Sales<\/h1>\n","7bae6a12":"# 4. Conclusion\nThank you for your time. Any comment\/improvement will be greatly appreciated !\n\nThis was my first Kaggle analysis and notebook (and 2nd dataset overall).\nAlthough the final modelization is miserable, it was a very interesting way to learn. To be continued... :)","7daa481c":"<h3>c. Analysis of missing data<\/h3>","3673dd81":"Basically 2 outliers because \"Imagine: Makeup Artist\" was released in 2009 on DS \nand Strongest Tokyo University Shogi was released on 2007 (True Name =  Saikyou Toudai Shogi DS)","5c6e59e0":"# 1. Goal & Other Information\nThe goal of this notebook is : <br>\n1\/ to have a better overview of the best video game sales in general <br>\n2\/ Find a pattern (time, platform, genre, publisher) in the different sales <br>\n3\/ Find if there is a correlation between the different markets (EU vs US vs Others)<br>\n4\/ Train a predictive model to find the potential sales (total global_sales + potential rank) based on the data provided <br>\n5\/ Learn as many things as possible on the fly ! <br>\n\n<h3>Dataset description: <\/h3>\nFeatures: 1*int64, 4*object and 6*float64 <br>\nShape: 16597 lines and  11 columns <br> \nNull: 307 lines with missing data : 271 missing in \"Year\" and 58 in \"Publisher\" (some overlap exists). So less than 2% of missing values in the whole dataset. <br>\nAlmost no data after 2016 <br>\n\n<h2>2. Exploratory Data Analysis:<\/h2>\n\n<h3>a. by Count:<\/h3>\n2008-2009 : Years with the highest published games number <br>\nDS & PS2 : Highest number of published video games <br>\nAction & Sports : Main genres for video games <br>\n10 Publishers = 49% of games published <br>\n\n<h3>b. by Global_Sales:<\/h3>\n1989 and 1990 = best year for video games based on Mean and median Rank (so indirectly based on Global_Sales) <br>\n\nPS2 was the best platform in terms of sales (more than 1.2 billion copies sold). But the best platforms for median sales are NES and Game Boy.<br>\n\nAction games are the best seller but in average (median) Platform games are better.<br>\n\nNintendo and Electronic Arts are the best publishers in terms of total global sales but they basically dominate the market by the number of games more than the median copies sold.<br>\n\nEU Sales and NA_Sales are quite correlated (0.77), meanwhile JP_Sales is less correlated to the Western market(0.44 and 0.45 resp. to EU and NA) and Other_Sales (0.73 and 0.63 resp. to EU and NA).<br>\nJapanese market seems to be a very different market in terms of sales compare to the others. This can be seen from the top 10 VG based on JP_Sales : there are only Nintendo VG and the 10th best VG in sales is 215th on a Global_sales basis. The best performing genre in Japan is mainly Role-Player Games (6\/10 in top 10).<br>\n\nGlobal_Sales is mainly dependant from NA (49.3%) and EU Sales (27.3%).<br>\n\nThere are many outliers mainly in the early year of video game history (ie 1980-1990), probably due to some very high global sales (and some probably very low as well) and low number of video games released.\n\n<h3>c. Analysis of missing data:<\/h3>\nNo data after 2016 (only 4 games). So we will better drop the data after this year... <br>\nThe missing \"Publisher\" are mainly concerning the years 2004-2005. <br>\n\n<h2>3. Develop a predictive model for VG Global_Sales<\/h2>\nThe goal of this section is to find and train a model to predict what would have been the global_sales (hence Rank) of a potential video game based on : Publisher, Year, Platform and Genre.\nWe did a dropna on the dataset because it decreased the number of video games from 16 598 to 16 287 which represents a loss of less than 2% of data. This is an acceptable loss that preserves the dataset trend.\n\n<h2> 4. Conclusion<\/h2>\n\nGenerally the decade 1980-1990 was the best in terms of sales. <br>\n    --> Hypothesis : low published VG numbers, novelty effect & high quality game (Tetris, Super Mario Land & World for 89-90) <br>\n    Japan's Sales of a Video Games are not similar in sales to other parts of the world.","c81b9946":"## b. by Global_Sales","264e3063":"For fun, we will check which platform had the longest lifespan (Personal Computer included)","fd7919fb":"#### Scaler\nAs an estimator I first used ElasticNetCV based on the Scikit-learn model selection cheat sheet : \">50 samples\" --> \"Predicting a quantity\" --> \"<100k samples\" --> \"few features should be important\" --> 2 possiblities: ElasticNet or Lasso\n\nI tested different preprocessing steps, here are the R\u00b2 obtained with an ElasticNetCV as estimator :\n    - No preprocessing: 0.126<br>\n    - Normalizer: 0.042 <br>\n    - StandardScaler: 0.132 <br>\n    - MinMaxScaler: 0.130 <br>\n    - PowerTransformer: 0.131 <br>\n\n    \nWe have a lot of outliers (check the section \"Outliers\" above), which explains the difficulty we are having with data scaling. I therefore tested more robust transformer\n    - RobustScaler: 0.130 <br>\n    - QuantileTransformer: 0.129 <br> \n    - QuantileTransformer (with gaussian output): 0.130 <br> \n    \nFinally, I made the choice of keeping RobustScaler as it keeps the outliers but decreases significantly their weight on the overall dataset.\n\n#### Estimator choice\nAfter having chosen the RobustScaler() transformer on the data, I decided to have a look at other estimators with default parameters still basing my choice on the Regression section of the Cheat Sheet from scikit-learn. Here are the R\u00b2 found:\n    - LinearRegression: 0.132 <br>\n    - DecisionTreeRegressor: 0.624 <br>\n    - SGDRegressor: 0.125 <br>\n    - Lasso: 0 --> https:\/\/datascience.stackexchange.com\/questions\/62701\/normalisation-results-in-r2-score-of-0-lasso-regression I found \"That you get an R2 of zero suggests that the lasso penalty is large enough now to push the coefficient to zero, so that the model is just a horizontal line. \"<br> \n    - LassoLarsCV: 0.132 (I used the CV with LARS algorithme because I didn't know exactly how to optimize the lasso parameters) <br>\n    - SVR(kernel=\"rbf): 0.106 (and 1min23s of wall time !)<br>\n    - SVR(kernel=\"linear\"): 0.045 (and 1min 8s of wall time)<br>\n\n#### DecisionTreeRegressor optimization\nIt seems that the DecisionTreeRegressor is the one giving the best results. However, given that default values are: \"max_depth=None\", \"min_samples_split=2\" and \"min_samples_leaf=1\", the model is probably overfitting the train set. When looking for the final Max_depth and the number of leaves, we are very clearly overfitting (n_leaves = 8420 and max_depth = 139).\n\nWe will use a GridSearchCv to enhance the model without overfitting it too much. We will then see what it gives on the test set. I limited it to a small table : np.arange(2, 20, 4) or [2, 6, 10, 14, 18] for CPU utilization reason (the 625 fits lasted 5min22s on my computer). Here are the final results obtained\n\nR\u00b2 = 0.20171389878776647\n'decisiontreeregressor__max_depth': 18\n'decisiontreeregressor__min_samples_leaf': 18\n'decisiontreeregressor__min_samples_split': 2\n\nOnce applied to our test set, the results are not too far : R\u00b2 = 0.18453698853931988, but still very far from the R\u00b2=0.62 from the beginning.\nThis estimator seems to be overfitting our dataset. Therefore I decided to change my estimator for SGDRegressor and try to optimize it. LinearRegression was giving a better R\u00b2 at first, but as it does not have any hyperparameters to optimize, I favored SGDRegressor with a GridSearchCV.\n\n#### SGDRegressor optimization\nEven after optimization I could not obtain a R\u00b2 larger than 0.126 on the train set, but surprisingly... the best parameters obtained from GridSearchCV (sgdregressor__alpha': 0.00001, 'sgdregressor__max_iter': 1000.0} allows me to obtain a R\u00b2 of 0.163 on the test set.\n\n#### Conclusion\nThe best model obtained  after some optimization and tests was SGDRegressor, but the overall score is pretty low. This is probably due to the simple reason that a video game success has more variable than just the Year, Publisher, Platform and Genre.. Fortunately !\n","a8b6def4":"The missing \"Publisher\" are mainly concerning the years 2004-2005 and video games that were big hit in the japan.","5d28a7c0":"# 2. Exploratory Data Analysis\n## a. by Count"}}