{"cell_type":{"e5e1829a":"code","8e45c144":"code","9c0c678f":"code","96c1b290":"code","b4c56f2d":"code","5ee104ec":"code","046af9a6":"code","03ad7095":"code","f07d506c":"code","4a026d6e":"code","967a9fc4":"code","493d9021":"code","975e3c4a":"code","a7299398":"code","64fbecea":"code","8af945b0":"code","52f5d39b":"code","29bafd24":"code","6d3ff4b3":"code","576094ef":"code","a9e93efd":"code","1e8b0aee":"code","7607c808":"code","cf734e37":"code","3e61c69b":"code","2ebff20e":"code","0cf97777":"code","bc4c9bb5":"code","18a20281":"code","441e3357":"code","72e09313":"code","3faa8452":"code","986182e7":"code","910c320f":"code","23cb0d47":"code","19b010a7":"code","f64f9ba6":"code","240370d7":"code","c37f8676":"code","cf4bb448":"code","6ddd252b":"code","6753e4e9":"code","773cd5ee":"code","780f49b4":"code","2c065aca":"code","9d51af56":"code","4be06e29":"code","ff33e77f":"code","507e93ec":"code","fe929503":"code","b3a036b6":"code","a3bd9921":"code","1fe12799":"code","ea5e991b":"code","e6e34c8a":"code","1984f5f3":"code","254090b3":"code","e022445c":"code","15825b25":"code","50be0a28":"code","4236f43b":"code","6c80e818":"code","cc50c630":"code","eb5678eb":"code","097df424":"code","9aaec5a6":"code","7e09d199":"code","20b894b7":"code","ae2d64a2":"code","ccb9c5d4":"code","558ec166":"code","7808ced5":"code","334e7478":"code","74041b12":"code","d86b295d":"code","f84019a4":"code","e581168a":"code","7307631c":"code","14ad2b2a":"code","4dc9f34b":"code","1a084266":"code","98893e7c":"code","34017c91":"code","aeda9781":"code","603fa06b":"code","233cc4cd":"code","752e2c62":"code","353c3c42":"code","45f2abee":"code","5ac630be":"code","2ae10cb4":"code","ea62d47e":"code","58e6ba84":"code","86b33b80":"code","d0c97f5b":"code","8033da04":"code","d4299b1a":"code","5c14938d":"code","9019ce75":"code","4d5451b6":"code","3fb68249":"code","6215071f":"code","843ca06e":"code","12c700c5":"code","aea4966c":"code","e0a1e392":"code","3f789185":"code","206da55a":"code","a62ff976":"code","ccdae0b3":"code","c3896d30":"code","a72be189":"code","04a723c8":"code","924fef3f":"code","4fd55fee":"code","ce30688d":"code","ee24cd95":"code","80e9ea49":"code","6dc5090d":"code","476f4213":"code","b4f37634":"code","765530e9":"code","d78cb095":"code","da788c1e":"code","9105a3e2":"code","66db0d46":"code","53431e55":"code","f1bec0e0":"markdown","9e0ff2ad":"markdown","bdfbdf35":"markdown","247b9899":"markdown","25e8f6f0":"markdown","536595ad":"markdown","dd24722b":"markdown","d62c2b7d":"markdown","a43c5e00":"markdown","4703e215":"markdown","04ec349c":"markdown","ed8be472":"markdown","fd04bece":"markdown","20d87cf8":"markdown","2895edd9":"markdown","1f1dc8ce":"markdown","5c1d8b0a":"markdown","f5b68ee4":"markdown","9c841fe5":"markdown","a7635e47":"markdown","81d064ac":"markdown","8b0cace6":"markdown","a6f6e1de":"markdown","3affc6ba":"markdown","34501d22":"markdown","725ea777":"markdown","b3ddee42":"markdown","81ed92f7":"markdown","e52e4ca0":"markdown","c60ab5a7":"markdown","53cd0689":"markdown","77e4514d":"markdown","3b70a17e":"markdown","22329039":"markdown","ca6cc624":"markdown","6b2a9af9":"markdown","dfc7dbbf":"markdown","02234dfd":"markdown","b781c0f1":"markdown","b63624fd":"markdown","91c399dc":"markdown","7cad13c9":"markdown","89c3aae2":"markdown","f44c581b":"markdown","ba70ac09":"markdown","38517102":"markdown","89fdafc5":"markdown","ae5a450f":"markdown","620d84a6":"markdown","2d7dafef":"markdown","410662dc":"markdown","54a516d4":"markdown","bc254e10":"markdown","e11b7e27":"markdown","293c402a":"markdown","bdabf51c":"markdown","d7f3a229":"markdown","4fb7e4db":"markdown","588aa384":"markdown","442128dd":"markdown","c21300b5":"markdown","ad7229a5":"markdown","7df7194d":"markdown","3eba5d7f":"markdown"},"source":{"e5e1829a":"from keras.utils import np_utils \nfrom keras.datasets import mnist \nimport seaborn as sns\nfrom keras.initializers import RandomNormal\n%matplotlib notebook\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport time","8e45c144":"# https:\/\/gist.github.com\/greydanus\/f6eee59eaf1d90fcb3b534a25362cea4\n# https:\/\/stackoverflow.com\/a\/14434334\n# this function is used to update the plots for each epoch and error\ndef plt_dynamic(x, vy, ty, ax, colors=['b']):\n    ax.plot(x, vy, 'b', label=\"Validation Loss\")\n    ax.plot(x, ty, 'r', label=\"Train Loss\")\n    plt.legend()\n    plt.grid()\n    fig.canvas.draw()","9c0c678f":"# the data, shuffled and split between train and test sets \n(X_train, y_train), (X_test, y_test) = mnist.load_data()","96c1b290":"type(X_train)","b4c56f2d":"plt.imshow(X_train[218])","5ee104ec":"print(\"Number of training examples :\", X_train.shape[0], \"and each image is of shape (%d, %d)\"%(X_train.shape[1], X_train.shape[2]))\nprint(\"Number of training examples :\", X_test.shape[0], \"and each image is of shape (%d, %d)\"%(X_test.shape[1], X_test.shape[2]))","046af9a6":"# if you observe the input shape its 2 dimensional vector\n# for each image we have a (28*28) vector\n# we will convert the (28*28) vector into single dimensional vector of 1 * 784 \n\nX_train = X_train.reshape(X_train.shape[0], X_train.shape[1]*X_train.shape[2]) \nX_test = X_test.reshape(X_test.shape[0], X_test.shape[1]*X_test.shape[2]) ","03ad7095":"# after converting the input images from 3d to 2d vectors\n\nprint(\"Number of training examples :\", X_train.shape[0], \"and each image is of shape (%d)\"%(X_train.shape[1]))\nprint(\"Number of training examples :\", X_test.shape[0], \"and each image is of shape (%d)\"%(X_test.shape[1]))","f07d506c":"# An example data point\nprint(X_train[0])","4a026d6e":"# if we observe the above matrix each cell is having a value between 0-255\n# before we move to apply machine learning algorithms lets try to normalize the data\n# X => (X - Xmin)\/(Xmax-Xmin) = X\/255\n\nX_train = X_train\/255\nX_test = X_test\/255","967a9fc4":"# example data point after normlizing\nprint(X_train[0])","493d9021":"# here we are having a class number for each image\nprint(\"Class label of first image :\", y_train[0])\n\n# lets convert this into a 10 dimensional vector\n# ex: consider an image is 5 convert it into 5 => [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n# this conversion needed for MLPs \n\nY_train = np_utils.to_categorical(y_train, 10) \nY_test = np_utils.to_categorical(y_test, 10)\n\nprint(\"After converting the output into a vector : \",Y_train[0])","975e3c4a":"from keras.models import Sequential \nfrom keras.layers import Dense, Activation ","a7299398":"# some model parameters\n\noutput_dim = 10\ninput_dim = X_train.shape[1]\n\nbatch_size = 128 \nnb_epoch = 20","64fbecea":"model_2_relu = Sequential()\nmodel_2_relu.add(Dense(1024, activation='relu', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.062, seed=None)))\nmodel_2_relu.add(Dense(512, activation='relu', kernel_initializer=RandomNormal(mean=0.0, stddev=0.125, seed=None)) )\nmodel_2_relu.add(Dense(output_dim, activation='softmax'))\n\nprint(model_2_relu.summary())","8af945b0":"model_2_relu.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\nhistory = model_2_relu.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))","52f5d39b":"model_2_relu_score = model_2_relu.evaluate(X_test, Y_test, verbose=1) \nprint('Test score:', model_2_relu_score[0]) \nprint('Test accuracy:', model_2_relu_score[1])\n\nfig,ax = plt.subplots(1,1)\nax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n\n# list of epoch numbers\nx = list(range(1,nb_epoch+1))\n\nvy = history.history['val_loss']\nty = history.history['loss']\nplt_dynamic(x, vy, ty, ax)","29bafd24":"model_2_relu.layers","6d3ff4b3":"w_after = model_2_relu.get_weights()\n\nh1_w = w_after[0].flatten().reshape(-1,1)\nh2_w = w_after[2].flatten().reshape(-1,1)\nout_w = w_after[4].flatten().reshape(-1,1)\n\n\nfig = plt.figure(figsize=(10,6))\nplt.title(\"Weight matrices after model trained\")\nplt.subplot(1, 3, 1)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h1_w,color='b')\nplt.xlabel('Hidden Layer 1')\n\nplt.subplot(1, 3, 2)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h2_w, color='r')\nplt.xlabel('Hidden Layer 2 ')\n\nplt.subplot(1, 3, 3)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=out_w,color='y')\nplt.xlabel('Output Layer ')\nplt.show()","576094ef":"model_2_relu_medium = Sequential()\nmodel_2_relu_medium.add(Dense(256, activation='relu', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.062, seed=None)))\nmodel_2_relu_medium.add(Dense(128, activation='relu', kernel_initializer=RandomNormal(mean=0.0, stddev=0.125, seed=None)) )\nmodel_2_relu_medium.add(Dense(output_dim, activation='softmax'))\n\nprint(model_2_relu_medium.summary())","a9e93efd":"model_2_relu_medium.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\nhistory = model_2_relu_medium.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))","1e8b0aee":"model_2_relu_medium_score = model_2_relu_medium.evaluate(X_test, Y_test, verbose=1) \nprint('Test score:', model_2_relu_medium_score[0]) \nprint('Test accuracy:', model_2_relu_medium_score[1])\n\nfig,ax = plt.subplots(1,1)\nax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n\n# list of epoch numbers\nx = list(range(1,nb_epoch+1))\n\nvy = history.history['val_loss']\nty = history.history['loss']\nplt_dynamic(x, vy, ty, ax)","7607c808":"w_after = model_2_relu_medium.get_weights()\n\nh1_w = w_after[0].flatten().reshape(-1,1)\nh2_w = w_after[2].flatten().reshape(-1,1)\nout_w = w_after[4].flatten().reshape(-1,1)\n\n\nfig = plt.figure(figsize=(10,6))\nplt.title(\"Weight matrices after model trained\")\nplt.subplot(1, 3, 1)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h1_w,color='b')\nplt.xlabel('Hidden Layer 1')\n\nplt.subplot(1, 3, 2)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h2_w, color='r')\nplt.xlabel('Hidden Layer 2 ')\n\nplt.subplot(1, 3, 3)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=out_w,color='y')\nplt.xlabel('Output Layer ')\nplt.show()","cf734e37":"model_2_relu_small = Sequential()\nmodel_2_relu_small.add(Dense(256, activation='relu', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.062, seed=None)))\nmodel_2_relu_small.add(Dense(128, activation='relu', kernel_initializer=RandomNormal(mean=0.0, stddev=0.125, seed=None)) )\nmodel_2_relu_small.add(Dense(output_dim, activation='softmax'))\n\nprint(model_2_relu_small.summary())","3e61c69b":"model_2_relu_small.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\nhistory = model_2_relu_small.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))","2ebff20e":"model_2_relu_small_score = model_2_relu_small.evaluate(X_test, Y_test, verbose=1) \nprint('Test score:', model_2_relu_small_score[0]) \nprint('Test accuracy:', model_2_relu_small_score[1])\n\nfig,ax = plt.subplots(1,1)\nax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n\n# list of epoch numbers\nx = list(range(1,nb_epoch+1))\n\nvy = history.history['val_loss']\nty = history.history['loss']\nplt_dynamic(x, vy, ty, ax)","0cf97777":"w_after = model_2_relu_small.get_weights()\n\nh1_w = w_after[0].flatten().reshape(-1,1)\nh2_w = w_after[2].flatten().reshape(-1,1)\nout_w = w_after[4].flatten().reshape(-1,1)\n\n\nfig = plt.figure(figsize=(10,6))\nplt.title(\"Weight matrices after model trained\")\nplt.subplot(1, 3, 1)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h1_w,color='b')\nplt.xlabel('Hidden Layer 1')\n\nplt.subplot(1, 3, 2)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h2_w, color='r')\nplt.xlabel('Hidden Layer 2 ')\n\nplt.subplot(1, 3, 3)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=out_w,color='y')\nplt.xlabel('Output Layer ')\nplt.show()","bc4c9bb5":"from keras.layers.normalization import BatchNormalization\n\nmodel_batch_relu = Sequential()\n\nmodel_batch_relu.add(Dense(1024, activation='relu', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.039, seed=None)))\nmodel_batch_relu.add(BatchNormalization())\n\nmodel_batch_relu.add(Dense(512, activation='relu', kernel_initializer=RandomNormal(mean=0.0, stddev=0.55, seed=None)) )\nmodel_batch_relu.add(BatchNormalization())\n\nmodel_batch_relu.add(Dense(output_dim, activation='softmax'))\n\n\nmodel_batch_relu.summary()","18a20281":"model_batch_relu.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\nhistory = model_batch_relu.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))","441e3357":"model_batch_relu_score = model_batch_relu.evaluate(X_test, Y_test, verbose=1) \nprint('Test score:', model_batch_relu_score[0]) \nprint('Test accuracy:', model_batch_relu_score[1])\n\nfig,ax = plt.subplots(1,1)\nax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n\n# list of epoch numbers\nx = list(range(1,nb_epoch+1))\n\nvy = history.history['val_loss']\nty = history.history['loss']\nplt_dynamic(x, vy, ty, ax)","72e09313":"w_after = model_batch_relu.get_weights()\n\nh1_w = w_after[0].flatten().reshape(-1,1)\nh2_w = w_after[2].flatten().reshape(-1,1)\nout_w = w_after[4].flatten().reshape(-1,1)\n\n\nfig = plt.figure(figsize=(10,6))\nplt.title(\"Weight matrices after model trained\")\nplt.subplot(1, 3, 1)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h1_w,color='b')\nplt.xlabel('Hidden Layer 1')\n\nplt.subplot(1, 3, 2)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h2_w, color='r')\nplt.xlabel('Hidden Layer 2 ')\n\nplt.subplot(1, 3, 3)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=out_w,color='y')\nplt.xlabel('Output Layer ')\nplt.show()","3faa8452":"model_batch_relu_medium = Sequential()\n\nmodel_batch_relu_medium.add(Dense(256, activation='relu', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.039, seed=None)))\nmodel_batch_relu_medium.add(BatchNormalization())\n\nmodel_batch_relu_medium.add(Dense(128, activation='relu', kernel_initializer=RandomNormal(mean=0.0, stddev=0.55, seed=None)) )\nmodel_batch_relu_medium.add(BatchNormalization())\n\nmodel_batch_relu_medium.add(Dense(output_dim, activation='softmax'))\n\n\nmodel_batch_relu_medium.summary()","986182e7":"model_batch_relu_medium.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\nhistory = model_batch_relu_medium.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))","910c320f":"model_batch_relu_medium_score = model_batch_relu_medium.evaluate(X_test, Y_test, verbose=1) \nprint('Test score:', model_batch_relu_medium_score[0]) \nprint('Test accuracy:', model_batch_relu_medium_score[1])\n\nfig,ax = plt.subplots(1,1)\nax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n\n# list of epoch numbers\nx = list(range(1,nb_epoch+1))\n\nvy = history.history['val_loss']\nty = history.history['loss']\nplt_dynamic(x, vy, ty, ax)","23cb0d47":"w_after = model_batch_relu_medium.get_weights()\n\nh1_w = w_after[0].flatten().reshape(-1,1)\nh2_w = w_after[2].flatten().reshape(-1,1)\nout_w = w_after[4].flatten().reshape(-1,1)\n\n\nfig = plt.figure(figsize=(10,6))\nplt.title(\"Weight matrices after model trained\")\nplt.subplot(1, 3, 1)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h1_w,color='b')\nplt.xlabel('Hidden Layer 1')\n\nplt.subplot(1, 3, 2)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h2_w, color='r')\nplt.xlabel('Hidden Layer 2 ')\n\nplt.subplot(1, 3, 3)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=out_w,color='y')\nplt.xlabel('Output Layer ')\nplt.show()","19b010a7":"model_batch_relu_small = Sequential()\n\nmodel_batch_relu_small.add(Dense(256, activation='relu', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.039, seed=None)))\nmodel_batch_relu_small.add(BatchNormalization())\n\nmodel_batch_relu_small.add(Dense(128, activation='relu', kernel_initializer=RandomNormal(mean=0.0, stddev=0.55, seed=None)) )\nmodel_batch_relu_small.add(BatchNormalization())\n\nmodel_batch_relu_small.add(Dense(output_dim, activation='softmax'))\n\n\nmodel_batch_relu_small.summary()","f64f9ba6":"model_batch_relu_small.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\nhistory = model_batch_relu_small.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))","240370d7":"model_batch_relu_small_score = model_batch_relu_small.evaluate(X_test, Y_test, verbose=1) \nprint('Test score:', model_batch_relu_small_score[0]) \nprint('Test accuracy:', model_batch_relu_small_score[1])\n\nfig,ax = plt.subplots(1,1)\nax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n\n# list of epoch numbers\nx = list(range(1,nb_epoch+1))\n\nvy = history.history['val_loss']\nty = history.history['loss']\nplt_dynamic(x, vy, ty, ax)","c37f8676":"w_after = model_batch_relu_small.get_weights()\n\nh1_w = w_after[0].flatten().reshape(-1,1)\nh2_w = w_after[2].flatten().reshape(-1,1)\nout_w = w_after[4].flatten().reshape(-1,1)\n\n\nfig = plt.figure(figsize=(10,6))\nplt.title(\"Weight matrices after model trained\")\nplt.subplot(1, 3, 1)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h1_w,color='b')\nplt.xlabel('Hidden Layer 1')\n\nplt.subplot(1, 3, 2)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h2_w, color='r')\nplt.xlabel('Hidden Layer 2 ')\n\nplt.subplot(1, 3, 3)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=out_w,color='y')\nplt.xlabel('Output Layer ')\nplt.show()","cf4bb448":"# https:\/\/stackoverflow.com\/questions\/34716454\/where-do-i-call-the-batchnormalization-function-in-keras\n\nfrom keras.layers import Dropout\n\nmodel_drop = Sequential()\n\nmodel_drop.add(Dense(512, activation='relu', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.039, seed=None)))\nmodel_drop.add(BatchNormalization())\nmodel_drop.add(Dropout(0.5))\n\nmodel_drop.add(Dense(128, activation='relu', kernel_initializer=RandomNormal(mean=0.0, stddev=0.55, seed=None)) )\nmodel_drop.add(BatchNormalization())\nmodel_drop.add(Dropout(0.5))\n\nmodel_drop.add(Dense(output_dim, activation='softmax'))\n\n\nmodel_drop.summary()","6ddd252b":"model_drop.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\nhistory = model_drop.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))","6753e4e9":"model_drop_score = model_drop.evaluate(X_test, Y_test, verbose=1) \nprint('Test score:', model_drop_score[0]) \nprint('Test accuracy:', model_drop_score[1])\n\nfig,ax = plt.subplots(1,1)\nax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n\n# list of epoch numbers\nx = list(range(1,nb_epoch+1))\n\nvy = history.history['val_loss']\nty = history.history['loss']\nplt_dynamic(x, vy, ty, ax)","773cd5ee":"w_after = model_drop.get_weights()\n\nh1_w = w_after[0].flatten().reshape(-1,1)\nh2_w = w_after[2].flatten().reshape(-1,1)\nout_w = w_after[4].flatten().reshape(-1,1)\n\n\nfig = plt.figure(figsize=(10,6))\nplt.title(\"Weight matrices after model trained\")\nplt.subplot(1, 3, 1)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h1_w,color='b')\nplt.xlabel('Hidden Layer 1')\n\nplt.subplot(1, 3, 2)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h2_w, color='r')\nplt.xlabel('Hidden Layer 2 ')\n\nplt.subplot(1, 3, 3)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=out_w,color='y')\nplt.xlabel('Output Layer ')\nplt.show()","780f49b4":"# https:\/\/stackoverflow.com\/questions\/34716454\/where-do-i-call-the-batchnormalization-function-in-keras\n\nfrom keras.layers import Dropout\n\nmodel_drop_medium = Sequential()\n\nmodel_drop_medium.add(Dense(256, activation='relu', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.039, seed=None)))\nmodel_drop_medium.add(BatchNormalization())\nmodel_drop_medium.add(Dropout(0.5))\n\nmodel_drop_medium.add(Dense(128, activation='relu', kernel_initializer=RandomNormal(mean=0.0, stddev=0.55, seed=None)) )\nmodel_drop_medium.add(BatchNormalization())\nmodel_drop_medium.add(Dropout(0.5))\n\nmodel_drop_medium.add(Dense(output_dim, activation='softmax'))\n\n\nmodel_drop_medium.summary()","2c065aca":"model_drop_medium.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\nhistory = model_drop_medium.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))","9d51af56":"model_drop_medium_score = model_drop_medium.evaluate(X_test, Y_test, verbose=1) \nprint('Test score:', model_drop_medium_score[0]) \nprint('Test accuracy:', model_drop_medium_score[1])\n\nfig,ax = plt.subplots(1,1)\nax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n\n# list of epoch numbers\nx = list(range(1,nb_epoch+1))\n\nvy = history.history['val_loss']\nty = history.history['loss']\nplt_dynamic(x, vy, ty, ax)","4be06e29":"w_after = model_drop_medium.get_weights()\n\nh1_w = w_after[0].flatten().reshape(-1,1)\nh2_w = w_after[2].flatten().reshape(-1,1)\nout_w = w_after[4].flatten().reshape(-1,1)\n\n\nfig = plt.figure(figsize=(10,6))\nplt.title(\"Weight matrices after model trained\")\nplt.subplot(1, 3, 1)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h1_w,color='b')\nplt.xlabel('Hidden Layer 1')\n\nplt.subplot(1, 3, 2)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h2_w, color='r')\nplt.xlabel('Hidden Layer 2 ')\n\nplt.subplot(1, 3, 3)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=out_w,color='y')\nplt.xlabel('Output Layer ')\nplt.show()","ff33e77f":"# https:\/\/stackoverflow.com\/questions\/34716454\/where-do-i-call-the-batchnormalization-function-in-keras\n\nfrom keras.layers import Dropout\n\nmodel_drop_small = Sequential()\n\nmodel_drop_small.add(Dense(256, activation='relu', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.039, seed=None)))\nmodel_drop_small.add(BatchNormalization())\nmodel_drop_small.add(Dropout(0.5))\n\nmodel_drop_small.add(Dense(128, activation='relu', kernel_initializer=RandomNormal(mean=0.0, stddev=0.55, seed=None)) )\nmodel_drop_small.add(BatchNormalization())\nmodel_drop_small.add(Dropout(0.5))\n\nmodel_drop_small.add(Dense(output_dim, activation='softmax'))\n\n\nmodel_drop_small.summary()","507e93ec":"model_drop_small.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\nhistory = model_drop_small.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))","fe929503":"model_drop_small_score = model_drop_small.evaluate(X_test, Y_test, verbose=1) \nprint('Test score:', model_drop_small_score[0]) \nprint('Test accuracy:', model_drop_small_score[1])\n\nfig,ax = plt.subplots(1,1)\nax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n\n# list of epoch numbers\nx = list(range(1,nb_epoch+1))\n\nvy = history.history['val_loss']\nty = history.history['loss']\nplt_dynamic(x, vy, ty, ax)","b3a036b6":"w_after = model_drop_small.get_weights()\n\nh1_w = w_after[0].flatten().reshape(-1,1)\nh2_w = w_after[2].flatten().reshape(-1,1)\nout_w = w_after[4].flatten().reshape(-1,1)\n\n\nfig = plt.figure(figsize=(10,6))\nplt.title(\"Weight matrices after model trained\")\nplt.subplot(1, 3, 1)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h1_w,color='b')\nplt.xlabel('Hidden Layer 1')\n\nplt.subplot(1, 3, 2)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h2_w, color='r')\nplt.xlabel('Hidden Layer 2 ')\n\nplt.subplot(1, 3, 3)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=out_w,color='y')\nplt.xlabel('Output Layer ')\nplt.show()","a3bd9921":"model_3_relu = Sequential()\nmodel_3_relu.add(Dense(1024, activation='relu', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.062, seed=None)))\nmodel_3_relu.add(Dense(768, activation='relu', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.062, seed=None)))\nmodel_3_relu.add(Dense(512, activation='relu', kernel_initializer=RandomNormal(mean=0.0, stddev=0.125, seed=None)) )\nmodel_3_relu.add(Dense(output_dim, activation='softmax'))\n\nprint(model_3_relu.summary())","1fe12799":"model_3_relu.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\nhistory = model_3_relu.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))","ea5e991b":"model_3_relu_score = model_3_relu.evaluate(X_test, Y_test, verbose=1) \nprint('Test score:', model_3_relu_score[0]) \nprint('Test accuracy:', model_3_relu_score[1])\n\nfig,ax = plt.subplots(1,1)\nax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n\n# list of epoch numbers\nx = list(range(1,nb_epoch+1))\n\nvy = history.history['val_loss']\nty = history.history['loss']\nplt_dynamic(x, vy, ty, ax)","e6e34c8a":"model_3_relu.layers","1984f5f3":"w_after = model_3_relu.get_weights()\n\nh1_w = w_after[0].flatten().reshape(-1,1)\nh2_w = w_after[2].flatten().reshape(-1,1)\nh3_w = w_after[4].flatten().reshape(-1,1)\nout_w = w_after[6].flatten().reshape(-1,1)\n\n\nfig = plt.figure(figsize=(20,6))\nplt.title(\"Weight matrices after model trained\")\nplt.subplot(1, 4, 1)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h1_w,color='b')\nplt.xlabel('Hidden Layer 1')\n\nplt.subplot(1, 4, 2)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h2_w, color='r')\nplt.xlabel('Hidden Layer 2 ')\n\nplt.subplot(1, 4, 3)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h3_w, color='r')\nplt.xlabel('Hidden Layer 2 ')\n\nplt.subplot(1, 4, 4)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=out_w,color='y')\nplt.xlabel('Output Layer ')\nplt.show()","254090b3":"model_3_relu_medium = Sequential()\nmodel_3_relu_medium.add(Dense(256, activation='relu', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.062, seed=None)))\nmodel_3_relu_medium.add(Dense(192, activation='relu', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.062, seed=None)))\nmodel_3_relu_medium.add(Dense(128, activation='relu', kernel_initializer=RandomNormal(mean=0.0, stddev=0.125, seed=None)) )\nmodel_3_relu_medium.add(Dense(output_dim, activation='softmax'))\n\nprint(model_3_relu_medium.summary())","e022445c":"model_3_relu_medium.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\nhistory = model_3_relu_medium.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))","15825b25":"model_3_relu_medium_score = model_3_relu_medium.evaluate(X_test, Y_test, verbose=1) \nprint('Test score:', model_3_relu_medium_score[0]) \nprint('Test accuracy:', model_3_relu_medium_score[1])\n\nfig,ax = plt.subplots(1,1)\nax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n\n# list of epoch numbers\nx = list(range(1,nb_epoch+1))\n\nvy = history.history['val_loss']\nty = history.history['loss']\nplt_dynamic(x, vy, ty, ax)","50be0a28":"w_after = model_3_relu_medium.get_weights()\n\nh1_w = w_after[0].flatten().reshape(-1,1)\nh2_w = w_after[2].flatten().reshape(-1,1)\nh3_w = w_after[4].flatten().reshape(-1,1)\nout_w = w_after[6].flatten().reshape(-1,1)\n\n\nfig = plt.figure(figsize=(20,6))\nplt.title(\"Weight matrices after model trained\")\nplt.subplot(1, 4, 1)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h1_w,color='b')\nplt.xlabel('Hidden Layer 1')\n\nplt.subplot(1, 4, 2)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h2_w, color='r')\nplt.xlabel('Hidden Layer 2 ')\n\nplt.subplot(1, 4, 3)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h3_w, color='r')\nplt.xlabel('Hidden Layer 3 ')\n\nplt.subplot(1, 4, 4)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=out_w,color='y')\nplt.xlabel('Output Layer ')\nplt.show()","4236f43b":"model_3_relu_small = Sequential()\nmodel_3_relu_small.add(Dense(128, activation='relu', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.062, seed=None)))\nmodel_3_relu_small.add(Dense(96, activation='relu', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.062, seed=None)))\nmodel_3_relu_small.add(Dense(64, activation='relu', kernel_initializer=RandomNormal(mean=0.0, stddev=0.125, seed=None)) )\nmodel_3_relu_small.add(Dense(output_dim, activation='softmax'))\n\nprint(model_3_relu_small.summary())","6c80e818":"model_3_relu_small.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\nhistory = model_3_relu_small.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))","cc50c630":"model_3_relu_small_score = model_3_relu_small.evaluate(X_test, Y_test, verbose=1) \nprint('Test score:', model_3_relu_small_score[0]) \nprint('Test accuracy:', model_3_relu_small_score[1])\n\nfig,ax = plt.subplots(1,1)\nax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n\n# list of epoch numbers\nx = list(range(1,nb_epoch+1))\n\nvy = history.history['val_loss']\nty = history.history['loss']\nplt_dynamic(x, vy, ty, ax)","eb5678eb":"w_after = model_3_relu_small.get_weights()\n\nh1_w = w_after[0].flatten().reshape(-1,1)\nh2_w = w_after[2].flatten().reshape(-1,1)\nh3_w = w_after[4].flatten().reshape(-1,1)\nout_w = w_after[6].flatten().reshape(-1,1)\n\n\nfig = plt.figure(figsize=(20,6))\nplt.title(\"Weight matrices after model trained\")\nplt.subplot(1, 4, 1)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h1_w,color='b')\nplt.xlabel('Hidden Layer 1')\n\nplt.subplot(1, 4, 2)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h2_w, color='r')\nplt.xlabel('Hidden Layer 2 ')\n\nplt.subplot(1, 4, 3)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h3_w, color='r')\nplt.xlabel('Hidden Layer 3 ')\n\nplt.subplot(1, 4, 4)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=out_w,color='y')\nplt.xlabel('Output Layer ')\nplt.show()","097df424":"model_3_batch_relu = Sequential()\n\nmodel_3_batch_relu.add(Dense(1024, activation='relu', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.039, seed=None)))\nmodel_3_batch_relu.add(BatchNormalization())\n\nmodel_3_batch_relu.add(Dense(768, activation='relu', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.039, seed=None)))\nmodel_3_batch_relu.add(BatchNormalization())\n\nmodel_3_batch_relu.add(Dense(512, activation='relu', kernel_initializer=RandomNormal(mean=0.0, stddev=0.55, seed=None)) )\nmodel_3_batch_relu.add(BatchNormalization())\n\nmodel_3_batch_relu.add(Dense(output_dim, activation='softmax'))\n\n\nmodel_3_batch_relu.summary()","9aaec5a6":"model_3_batch_relu.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\nhistory = model_3_batch_relu.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))\n","7e09d199":"model_3_batch_relu_score = model_3_batch_relu.evaluate(X_test, Y_test, verbose=1) \nprint('Test score:', model_3_batch_relu_score[0]) \nprint('Test accuracy:', model_3_batch_relu_score[1])\n\nfig,ax = plt.subplots(1,1)\nax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n\n# list of epoch numbers\nx = list(range(1,nb_epoch+1))\n\nvy = history.history['val_loss']\nty = history.history['loss']\nplt_dynamic(x, vy, ty, ax)","20b894b7":"w_after = model_3_batch_relu.get_weights()\n\nh1_w = w_after[0].flatten().reshape(-1,1)\nh2_w = w_after[2].flatten().reshape(-1,1)\nh3_w = w_after[4].flatten().reshape(-1,1)\nout_w = w_after[6].flatten().reshape(-1,1)\n\n\nfig = plt.figure(figsize=(20,6))\nplt.title(\"Weight matrices after model trained\")\nplt.subplot(1, 4, 1)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h1_w,color='b')\nplt.xlabel('Hidden Layer 1')\n\nplt.subplot(1, 4, 2)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h2_w, color='r')\nplt.xlabel('Hidden Layer 2 ')\n\nplt.subplot(1, 4, 3)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h3_w, color='r')\nplt.xlabel('Hidden Layer 3 ')\n\nplt.subplot(1, 4, 4)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=out_w,color='y')\nplt.xlabel('Output Layer ')\nplt.show()","ae2d64a2":"model_3_batch_relu_medium = Sequential()\n\nmodel_3_batch_relu_medium.add(Dense(256, activation='relu', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.039, seed=None)))\nmodel_3_batch_relu_medium.add(BatchNormalization())\n\nmodel_3_batch_relu_medium.add(Dense(192, activation='relu', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.039, seed=None)))\nmodel_3_batch_relu_medium.add(BatchNormalization())\n\nmodel_3_batch_relu_medium.add(Dense(128, activation='relu', kernel_initializer=RandomNormal(mean=0.0, stddev=0.55, seed=None)) )\nmodel_3_batch_relu_medium.add(BatchNormalization())\n\nmodel_3_batch_relu_medium.add(Dense(output_dim, activation='softmax'))\n\n\nmodel_3_batch_relu_medium.summary()","ccb9c5d4":"model_3_batch_relu_medium.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\nhistory = model_3_batch_relu_medium.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))","558ec166":"model_3_batch_relu_medium_score = model_3_batch_relu_medium.evaluate(X_test, Y_test, verbose=1) \nprint('Test score:', model_3_batch_relu_medium_score[0]) \nprint('Test accuracy:', model_3_batch_relu_medium_score[1])\n\nfig,ax = plt.subplots(1,1)\nax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n\n# list of epoch numbers\nx = list(range(1,nb_epoch+1))\n\nvy = history.history['val_loss']\nty = history.history['loss']\nplt_dynamic(x, vy, ty, ax)","7808ced5":"w_after = model_3_batch_relu_medium.get_weights()\n\nh1_w = w_after[0].flatten().reshape(-1,1)\nh2_w = w_after[2].flatten().reshape(-1,1)\nh3_w = w_after[4].flatten().reshape(-1,1)\nout_w = w_after[6].flatten().reshape(-1,1)\n\n\nfig = plt.figure(figsize=(20,6))\nplt.title(\"Weight matrices after model trained\")\nplt.subplot(1, 4, 1)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h1_w,color='b')\nplt.xlabel('Hidden Layer 1')\n\nplt.subplot(1, 4, 2)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h2_w, color='r')\nplt.xlabel('Hidden Layer 2 ')\n\nplt.subplot(1, 4, 3)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h3_w, color='r')\nplt.xlabel('Hidden Layer 3 ')\n\nplt.subplot(1, 4, 4)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=out_w,color='y')\nplt.xlabel('Output Layer ')\nplt.show()","334e7478":"model_3_batch_relu_small = Sequential()\n\nmodel_3_batch_relu_small.add(Dense(1024, activation='relu', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.039, seed=None)))\nmodel_3_batch_relu_small.add(BatchNormalization())\n\nmodel_3_batch_relu_small.add(Dense(768, activation='relu', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.039, seed=None)))\nmodel_3_batch_relu_small.add(BatchNormalization())\n\nmodel_3_batch_relu_small.add(Dense(512, activation='relu', kernel_initializer=RandomNormal(mean=0.0, stddev=0.55, seed=None)) )\nmodel_3_batch_relu_small.add(BatchNormalization())\n\nmodel_3_batch_relu_small.add(Dense(output_dim, activation='softmax'))\n\n\nmodel_3_batch_relu_small.summary()","74041b12":"model_3_batch_relu_small.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\nhistory = model_3_batch_relu_small.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))","d86b295d":"model_3_batch_relu_small_score = model_3_batch_relu_small.evaluate(X_test, Y_test, verbose=1) \nprint('Test score:', model_3_batch_relu_small_score[0]) \nprint('Test accuracy:', model_3_batch_relu_small_score[1])\n\nfig,ax = plt.subplots(1,1)\nax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n\n# list of epoch numbers\nx = list(range(1,nb_epoch+1))\n\nvy = history.history['val_loss']\nty = history.history['loss']\nplt_dynamic(x, vy, ty, ax)","f84019a4":"w_after = model_3_batch_relu_small.get_weights()\n\nh1_w = w_after[0].flatten().reshape(-1,1)\nh2_w = w_after[2].flatten().reshape(-1,1)\nh3_w = w_after[4].flatten().reshape(-1,1)\nout_w = w_after[6].flatten().reshape(-1,1)\n\n\nfig = plt.figure(figsize=(20,6))\nplt.title(\"Weight matrices after model trained\")\nplt.subplot(1, 4, 1)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h1_w,color='b')\nplt.xlabel('Hidden Layer 1')\n\nplt.subplot(1, 4, 2)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h2_w, color='r')\nplt.xlabel('Hidden Layer 2 ')\n\nplt.subplot(1, 4, 3)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h3_w, color='r')\nplt.xlabel('Hidden Layer 3 ')\n\nplt.subplot(1, 4, 4)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=out_w,color='y')\nplt.xlabel('Output Layer ')\nplt.show()","e581168a":"from keras.layers import Dropout\nmodel_3_drop = Sequential()\n\nmodel_3_drop.add(Dense(1024, activation='relu', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.039, seed=None)))\nmodel_3_drop.add(BatchNormalization())\nmodel_3_drop.add(Dropout(0.5))\n\nmodel_3_drop.add(Dense(768, activation='relu', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.039, seed=None)))\nmodel_3_drop.add(BatchNormalization())\nmodel_3_drop.add(Dropout(0.5))\n\nmodel_3_drop.add(Dense(512, activation='relu', kernel_initializer=RandomNormal(mean=0.0, stddev=0.55, seed=None)) )\nmodel_3_drop.add(BatchNormalization())\nmodel_3_drop.add(Dropout(0.5))\n\nmodel_3_drop.add(Dense(output_dim, activation='softmax'))\n\n\nmodel_3_drop.summary()","7307631c":"model_3_drop.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\nhistory = model_3_drop.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))","14ad2b2a":"model_3_drop_score = model_3_drop.evaluate(X_test, Y_test, verbose=1) \nprint('Test score:', model_3_drop_score[0]) \nprint('Test accuracy:', model_3_drop_score[1])\n\nfig,ax = plt.subplots(1,1)\nax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n\n# list of epoch numbers\nx = list(range(1,nb_epoch+1))\n\nvy = history.history['val_loss']\nty = history.history['loss']\nplt_dynamic(x, vy, ty, ax)\n","4dc9f34b":"w_after = model_3_drop.get_weights()\n\nh1_w = w_after[0].flatten().reshape(-1,1)\nh2_w = w_after[2].flatten().reshape(-1,1)\nh3_w = w_after[4].flatten().reshape(-1,1)\nout_w = w_after[6].flatten().reshape(-1,1)\n\n\nfig = plt.figure(figsize=(20,6))\nplt.title(\"Weight matrices after model trained\")\nplt.subplot(1, 4, 1)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h1_w,color='b')\nplt.xlabel('Hidden Layer 1')\n\nplt.subplot(1, 4, 2)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h2_w, color='r')\nplt.xlabel('Hidden Layer 2 ')\n\nplt.subplot(1, 4, 3)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h3_w, color='r')\nplt.xlabel('Hidden Layer 3 ')\n\nplt.subplot(1, 4, 4)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=out_w,color='y')\nplt.xlabel('Output Layer ')\nplt.show()","1a084266":"model_3_drop_medium = Sequential()\n\nmodel_3_drop_medium.add(Dense(256, activation='relu', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.039, seed=None)))\nmodel_3_drop_medium.add(BatchNormalization())\nmodel_3_drop_medium.add(Dropout(0.5))\n\nmodel_3_drop_medium.add(Dense(192, activation='relu', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.039, seed=None)))\nmodel_3_drop_medium.add(BatchNormalization())\nmodel_3_drop_medium.add(Dropout(0.5))\n\nmodel_3_drop_medium.add(Dense(128, activation='relu', kernel_initializer=RandomNormal(mean=0.0, stddev=0.55, seed=None)) )\nmodel_3_drop_medium.add(BatchNormalization())\nmodel_3_drop_medium.add(Dropout(0.5))\n\nmodel_3_drop_medium.add(Dense(output_dim, activation='softmax'))\n\n\nmodel_3_drop_medium.summary()\n","98893e7c":"model_3_drop_medium.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\nhistory = model_3_drop_medium.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))","34017c91":"model_3_drop_medium_score = model_3_drop_medium.evaluate(X_test, Y_test, verbose=1) \nprint('Test score:', model_3_drop_medium_score[0]) \nprint('Test accuracy:', model_3_drop_medium_score[1])\n\nfig,ax = plt.subplots(1,1)\nax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n\n# list of epoch numbers\nx = list(range(1,nb_epoch+1))\n\nvy = history.history['val_loss']\nty = history.history['loss']\nplt_dynamic(x, vy, ty, ax)","aeda9781":"w_after = model_3_drop_medium.get_weights()\n\nh1_w = w_after[0].flatten().reshape(-1,1)\nh2_w = w_after[2].flatten().reshape(-1,1)\nh3_w = w_after[4].flatten().reshape(-1,1)\nout_w = w_after[6].flatten().reshape(-1,1)\n\n\nfig = plt.figure(figsize=(20,6))\nplt.title(\"Weight matrices after model trained\")\nplt.subplot(1, 4, 1)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h1_w,color='b')\nplt.xlabel('Hidden Layer 1')\n\nplt.subplot(1, 4, 2)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h2_w, color='r')\nplt.xlabel('Hidden Layer 2 ')\n\nplt.subplot(1, 4, 3)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h3_w, color='r')\nplt.xlabel('Hidden Layer 3 ')\n\nplt.subplot(1, 4, 4)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=out_w,color='y')\nplt.xlabel('Output Layer ')\nplt.show()","603fa06b":"model_3_drop_small = Sequential()\n\nmodel_3_drop_small.add(Dense(128, activation='relu', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.039, seed=None)))\nmodel_3_drop_small.add(BatchNormalization())\nmodel_3_drop_small.add(Dropout(0.5))\n\nmodel_3_drop_small.add(Dense(96, activation='relu', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.039, seed=None)))\nmodel_3_drop_small.add(BatchNormalization())\nmodel_3_drop_small.add(Dropout(0.5))\n\nmodel_3_drop_small.add(Dense(64, activation='relu', kernel_initializer=RandomNormal(mean=0.0, stddev=0.55, seed=None)) )\nmodel_3_drop_small.add(BatchNormalization())\nmodel_3_drop_small.add(Dropout(0.5))\n\nmodel_3_drop_small.add(Dense(output_dim, activation='softmax'))\n\n\nmodel_3_drop_small.summary()","233cc4cd":"model_3_drop_small.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\nhistory = model_3_drop_small.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))","752e2c62":"model_3_drop_small_score = model_3_drop_small.evaluate(X_test, Y_test, verbose=1) \nprint('Test score:', model_3_drop_small_score[0]) \nprint('Test accuracy:', model_3_drop_small_score[1])\n\nfig,ax = plt.subplots(1,1)\nax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n\n# list of epoch numbers\nx = list(range(1,nb_epoch+1))\n\nvy = history.history['val_loss']\nty = history.history['loss']\nplt_dynamic(x, vy, ty, ax)","353c3c42":"w_after = model_3_drop_small.get_weights()\n\nh1_w = w_after[0].flatten().reshape(-1,1)\nh2_w = w_after[2].flatten().reshape(-1,1)\nh3_w = w_after[4].flatten().reshape(-1,1)\nout_w = w_after[6].flatten().reshape(-1,1)\n\n\nfig = plt.figure(figsize=(20,6))\nplt.title(\"Weight matrices after model trained\")\nplt.subplot(1, 4, 1)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h1_w,color='b')\nplt.xlabel('Hidden Layer 1')\n\nplt.subplot(1, 4, 2)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h2_w, color='r')\nplt.xlabel('Hidden Layer 2 ')\n\nplt.subplot(1, 4, 3)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h3_w, color='r')\nplt.xlabel('Hidden Layer 3 ')\n\nplt.subplot(1, 4, 4)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=out_w,color='y')\nplt.xlabel('Output Layer ')\nplt.show()","45f2abee":"model_5_relu = Sequential()\nmodel_5_relu.add(Dense(1024, activation='relu', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.062, seed=None)))\nmodel_5_relu.add(Dense(896, activation='relu', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.062, seed=None)))\nmodel_5_relu.add(Dense(768, activation='relu', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.062, seed=None)))\nmodel_5_relu.add(Dense(640, activation='relu', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.062, seed=None)))\nmodel_5_relu.add(Dense(512, activation='relu', kernel_initializer=RandomNormal(mean=0.0, stddev=0.125, seed=None)) )\nmodel_5_relu.add(Dense(output_dim, activation='softmax'))\n\nprint(model_5_relu.summary())","5ac630be":"model_5_relu.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\nhistory = model_5_relu.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))","2ae10cb4":"model_5_relu_score = model_5_relu.evaluate(X_test, Y_test, verbose=1) \nprint('Test score:', model_5_relu_score[0]) \nprint('Test accuracy:', model_5_relu_score[1])\n\nfig,ax = plt.subplots(1,1)\nax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n\n# list of epoch numbers\nx = list(range(1,nb_epoch+1))\n\nvy = history.history['val_loss']\nty = history.history['loss']\nplt_dynamic(x, vy, ty, ax)","ea62d47e":"w_after = model_5_relu.get_weights()\n\nh1_w = w_after[0].flatten().reshape(-1,1)\nh2_w = w_after[2].flatten().reshape(-1,1)\nh3_w = w_after[4].flatten().reshape(-1,1)\nh4_w = w_after[6].flatten().reshape(-1,1)\nh5_w = w_after[8].flatten().reshape(-1,1)\nout_w = w_after[10].flatten().reshape(-1,1)\n\n\nfig = plt.figure(figsize=(20,6))\nplt.title(\"Weight matrices after model trained\")\nplt.subplot(1, 6, 1)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h1_w,color='b')\nplt.xlabel('Hidden Layer 1')\n\nplt.subplot(1, 6, 2)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h2_w, color='r')\nplt.xlabel('Hidden Layer 2 ')\n\nplt.subplot(1, 6, 3)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h3_w, color='r')\nplt.xlabel('Hidden Layer 3 ')\n\nplt.subplot(1, 6, 4)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h3_w, color='r')\nplt.xlabel('Hidden Layer 4 ')\n\nplt.subplot(1, 6, 5)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h3_w, color='r')\nplt.xlabel('Hidden Layer 5 ')\n\nplt.subplot(1, 6, 6)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=out_w,color='y')\nplt.xlabel('Output Layer ')\nplt.show()","58e6ba84":"model_5_relu_medium = Sequential()\nmodel_5_relu_medium.add(Dense(256, activation='relu', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.062, seed=None)))\nmodel_5_relu_medium.add(Dense(224, activation='relu', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.062, seed=None)))\nmodel_5_relu_medium.add(Dense(192, activation='relu', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.062, seed=None)))\nmodel_5_relu_medium.add(Dense(160, activation='relu', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.062, seed=None)))\nmodel_5_relu_medium.add(Dense(128, activation='relu', kernel_initializer=RandomNormal(mean=0.0, stddev=0.125, seed=None)) )\nmodel_5_relu_medium.add(Dense(output_dim, activation='softmax'))\n\nprint(model_5_relu_medium.summary())","86b33b80":"model_5_relu_medium.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\nhistory = model_5_relu_medium.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))","d0c97f5b":"model_5_relu_medium_score = model_5_relu_medium.evaluate(X_test, Y_test, verbose=1) \nprint('Test score:', model_5_relu_medium_score[0]) \nprint('Test accuracy:', model_5_relu_medium_score[1])\n\nfig,ax = plt.subplots(1,1)\nax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n\n# list of epoch numbers\nx = list(range(1,nb_epoch+1))\n\nvy = history.history['val_loss']\nty = history.history['loss']\nplt_dynamic(x, vy, ty, ax)\n","8033da04":"w_after = model_5_relu_medium.get_weights()\n\nh1_w = w_after[0].flatten().reshape(-1,1)\nh2_w = w_after[2].flatten().reshape(-1,1)\nh3_w = w_after[4].flatten().reshape(-1,1)\nh4_w = w_after[6].flatten().reshape(-1,1)\nh5_w = w_after[8].flatten().reshape(-1,1)\nout_w = w_after[10].flatten().reshape(-1,1)\n\n\nfig = plt.figure(figsize=(20,6))\nplt.title(\"Weight matrices after model trained\")\nplt.subplot(1, 6, 1)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h1_w,color='b')\nplt.xlabel('Hidden Layer 1')\n\nplt.subplot(1, 6, 2)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h2_w, color='r')\nplt.xlabel('Hidden Layer 2 ')\n\nplt.subplot(1, 6, 3)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h3_w, color='r')\nplt.xlabel('Hidden Layer 3 ')\n\nplt.subplot(1, 6, 4)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h3_w, color='r')\nplt.xlabel('Hidden Layer 4 ')\n\nplt.subplot(1, 6, 5)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h3_w, color='r')\nplt.xlabel('Hidden Layer 5 ')\n\nplt.subplot(1, 6, 6)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=out_w,color='y')\nplt.xlabel('Output Layer ')\nplt.show()","d4299b1a":"model_5_relu_small = Sequential()\nmodel_5_relu_small.add(Dense(128, activation='relu', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.062, seed=None)))\nmodel_5_relu_small.add(Dense(112, activation='relu', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.062, seed=None)))\nmodel_5_relu_small.add(Dense(96, activation='relu', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.062, seed=None)))\nmodel_5_relu_small.add(Dense(80, activation='relu', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.062, seed=None)))\nmodel_5_relu_small.add(Dense(64, activation='relu', kernel_initializer=RandomNormal(mean=0.0, stddev=0.125, seed=None)) )\nmodel_5_relu_small.add(Dense(output_dim, activation='softmax'))\n\nprint(model_5_relu_small.summary())","5c14938d":"model_5_relu_small.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\nhistory = model_5_relu_small.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))","9019ce75":"model_5_relu_small_score = model_5_relu_small.evaluate(X_test, Y_test, verbose=1) \nprint('Test score:', model_5_relu_small_score[0]) \nprint('Test accuracy:', model_5_relu_small_score[1])\n\nfig,ax = plt.subplots(1,1)\nax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n\n# list of epoch numbers\nx = list(range(1,nb_epoch+1))\n\nvy = history.history['val_loss']\nty = history.history['loss']\nplt_dynamic(x, vy, ty, ax)","4d5451b6":"w_after = model_5_relu_small.get_weights()\n\nh1_w = w_after[0].flatten().reshape(-1,1)\nh2_w = w_after[2].flatten().reshape(-1,1)\nh3_w = w_after[4].flatten().reshape(-1,1)\nh4_w = w_after[6].flatten().reshape(-1,1)\nh5_w = w_after[8].flatten().reshape(-1,1)\nout_w = w_after[10].flatten().reshape(-1,1)\n\n\nfig = plt.figure(figsize=(20,6))\nplt.title(\"Weight matrices after model trained\")\nplt.subplot(1, 6, 1)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h1_w,color='b')\nplt.xlabel('Hidden Layer 1')\n\nplt.subplot(1, 6, 2)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h2_w, color='r')\nplt.xlabel('Hidden Layer 2 ')\n\nplt.subplot(1, 6, 3)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h3_w, color='r')\nplt.xlabel('Hidden Layer 3 ')\n\nplt.subplot(1, 6, 4)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h3_w, color='r')\nplt.xlabel('Hidden Layer 4 ')\n\nplt.subplot(1, 6, 5)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h3_w, color='r')\nplt.xlabel('Hidden Layer 5 ')\n\nplt.subplot(1, 6, 6)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=out_w,color='y')\nplt.xlabel('Output Layer ')\nplt.show()","3fb68249":"model_5_batch_relu = Sequential()\n\nmodel_5_batch_relu.add(Dense(1024, activation='relu', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.039, seed=None)))\nmodel_5_batch_relu.add(BatchNormalization())\n\nmodel_5_batch_relu.add(Dense(896, activation='relu', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.039, seed=None)))\nmodel_5_batch_relu.add(BatchNormalization())\n\nmodel_5_batch_relu.add(Dense(768, activation='relu', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.039, seed=None)))\nmodel_5_batch_relu.add(BatchNormalization())\n\nmodel_5_batch_relu.add(Dense(640, activation='relu', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.039, seed=None)))\nmodel_5_batch_relu.add(BatchNormalization())\n\nmodel_5_batch_relu.add(Dense(512, activation='relu', kernel_initializer=RandomNormal(mean=0.0, stddev=0.55, seed=None)) )\nmodel_5_batch_relu.add(BatchNormalization())\n\nmodel_5_batch_relu.add(Dense(output_dim, activation='softmax'))\n\n\nmodel_5_batch_relu.summary()","6215071f":"model_5_batch_relu.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\nhistory = model_5_batch_relu.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))\n","843ca06e":"model_5_batch_relu_score = model_5_batch_relu.evaluate(X_test, Y_test, verbose=1) \nprint('Test score:', model_5_batch_relu_score[0]) \nprint('Test accuracy:', model_5_batch_relu_score[1])\n\nfig,ax = plt.subplots(1,1)\nax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n\n# list of epoch numbers\nx = list(range(1,nb_epoch+1))\n\nvy = history.history['val_loss']\nty = history.history['loss']\nplt_dynamic(x, vy, ty, ax)","12c700c5":"w_after = model_5_batch_relu.get_weights()\n\nh1_w = w_after[0].flatten().reshape(-1,1)\nh2_w = w_after[2].flatten().reshape(-1,1)\nh3_w = w_after[4].flatten().reshape(-1,1)\nh4_w = w_after[6].flatten().reshape(-1,1)\nh5_w = w_after[8].flatten().reshape(-1,1)\nout_w = w_after[10].flatten().reshape(-1,1)\n\n\nfig = plt.figure(figsize=(20,6))\nplt.title(\"Weight matrices after model trained\")\nplt.subplot(1, 6, 1)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h1_w,color='b')\nplt.xlabel('Hidden Layer 1')\n\nplt.subplot(1, 6, 2)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h2_w, color='r')\nplt.xlabel('Hidden Layer 2 ')\n\nplt.subplot(1, 6, 3)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h3_w, color='r')\nplt.xlabel('Hidden Layer 3 ')\n\nplt.subplot(1, 6, 4)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h3_w, color='r')\nplt.xlabel('Hidden Layer 4 ')\n\nplt.subplot(1, 6, 5)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h3_w, color='r')\nplt.xlabel('Hidden Layer 5 ')\n\nplt.subplot(1, 6, 6)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=out_w,color='y')\nplt.xlabel('Output Layer ')\nplt.show()","aea4966c":"model_5_batch_relu_medium = Sequential()\n\nmodel_5_batch_relu_medium.add(Dense(256, activation='relu', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.039, seed=None)))\nmodel_5_batch_relu_medium.add(BatchNormalization())\n\nmodel_5_batch_relu_medium.add(Dense(224, activation='relu', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.039, seed=None)))\nmodel_5_batch_relu_medium.add(BatchNormalization())\n\nmodel_5_batch_relu_medium.add(Dense(192, activation='relu', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.039, seed=None)))\nmodel_5_batch_relu_medium.add(BatchNormalization())\n\nmodel_5_batch_relu_medium.add(Dense(160, activation='relu', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.039, seed=None)))\nmodel_5_batch_relu_medium.add(BatchNormalization())\n\nmodel_5_batch_relu_medium.add(Dense(128, activation='relu', kernel_initializer=RandomNormal(mean=0.0, stddev=0.55, seed=None)) )\nmodel_5_batch_relu_medium.add(BatchNormalization())\n\nmodel_5_batch_relu_medium.add(Dense(output_dim, activation='softmax'))\n\n\nmodel_5_batch_relu_medium.summary()","e0a1e392":"model_5_batch_relu_medium.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\nhistory = model_5_batch_relu_medium.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))","3f789185":"model_5_batch_relu_medium_score = model_5_batch_relu_medium.evaluate(X_test, Y_test, verbose=1) \nprint('Test score:', model_5_batch_relu_medium_score[0]) \nprint('Test accuracy:', model_5_batch_relu_medium_score[1])\n\nfig,ax = plt.subplots(1,1)\nax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n\n# list of epoch numbers\nx = list(range(1,nb_epoch+1))\n\nvy = history.history['val_loss']\nty = history.history['loss']\nplt_dynamic(x, vy, ty, ax)","206da55a":"w_after = model_5_batch_relu_medium.get_weights()\n\nh1_w = w_after[0].flatten().reshape(-1,1)\nh2_w = w_after[2].flatten().reshape(-1,1)\nh3_w = w_after[4].flatten().reshape(-1,1)\nh4_w = w_after[6].flatten().reshape(-1,1)\nh5_w = w_after[8].flatten().reshape(-1,1)\nout_w = w_after[10].flatten().reshape(-1,1)\n\n\nfig = plt.figure(figsize=(20,6))\nplt.title(\"Weight matrices after model trained\")\nplt.subplot(1, 6, 1)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h1_w,color='b')\nplt.xlabel('Hidden Layer 1')\n\nplt.subplot(1, 6, 2)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h2_w, color='r')\nplt.xlabel('Hidden Layer 2 ')\n\nplt.subplot(1, 6, 3)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h3_w, color='r')\nplt.xlabel('Hidden Layer 3 ')\n\nplt.subplot(1, 6, 4)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h3_w, color='r')\nplt.xlabel('Hidden Layer 4 ')\n\nplt.subplot(1, 6, 5)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h3_w, color='r')\nplt.xlabel('Hidden Layer 5 ')\n\nplt.subplot(1, 6, 6)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=out_w,color='y')\nplt.xlabel('Output Layer ')\nplt.show()","a62ff976":"model_5_batch_relu_small = Sequential()\n\nmodel_5_batch_relu_small.add(Dense(128, activation='relu', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.039, seed=None)))\nmodel_5_batch_relu_small.add(BatchNormalization())\n\nmodel_5_batch_relu_small.add(Dense(112, activation='relu', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.039, seed=None)))\nmodel_5_batch_relu_small.add(BatchNormalization())\n\nmodel_5_batch_relu_small.add(Dense(96, activation='relu', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.039, seed=None)))\nmodel_5_batch_relu_small.add(BatchNormalization())\n\nmodel_5_batch_relu_small.add(Dense(80, activation='relu', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.039, seed=None)))\nmodel_5_batch_relu_small.add(BatchNormalization())\n\nmodel_5_batch_relu_small.add(Dense(64, activation='relu', kernel_initializer=RandomNormal(mean=0.0, stddev=0.55, seed=None)) )\nmodel_5_batch_relu_small.add(BatchNormalization())\n\nmodel_5_batch_relu_small.add(Dense(output_dim, activation='softmax'))\n\n\nmodel_5_batch_relu_small.summary()","ccdae0b3":"model_5_batch_relu_small.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\nhistory = model_5_batch_relu_small.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))","c3896d30":"model_5_batch_relu_small_score = model_5_batch_relu_small.evaluate(X_test, Y_test, verbose=1) \nprint('Test score:', model_5_batch_relu_small_score[0]) \nprint('Test accuracy:', model_5_batch_relu_small_score[1])\n\nfig,ax = plt.subplots(1,1)\nax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n\n# list of epoch numbers\nx = list(range(1,nb_epoch+1))\n\nvy = history.history['val_loss']\nty = history.history['loss']\nplt_dynamic(x, vy, ty, ax)","a72be189":"w_after = model_5_batch_relu_small.get_weights()\n\nh1_w = w_after[0].flatten().reshape(-1,1)\nh2_w = w_after[2].flatten().reshape(-1,1)\nh3_w = w_after[4].flatten().reshape(-1,1)\nh4_w = w_after[6].flatten().reshape(-1,1)\nh5_w = w_after[8].flatten().reshape(-1,1)\nout_w = w_after[10].flatten().reshape(-1,1)\n\n\nfig = plt.figure(figsize=(20,6))\nplt.title(\"Weight matrices after model trained\")\nplt.subplot(1, 6, 1)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h1_w,color='b')\nplt.xlabel('Hidden Layer 1')\n\nplt.subplot(1, 6, 2)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h2_w, color='r')\nplt.xlabel('Hidden Layer 2 ')\n\nplt.subplot(1, 6, 3)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h3_w, color='r')\nplt.xlabel('Hidden Layer 3 ')\n\nplt.subplot(1, 6, 4)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h3_w, color='r')\nplt.xlabel('Hidden Layer 4 ')\n\nplt.subplot(1, 6, 5)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h3_w, color='r')\nplt.xlabel('Hidden Layer 5 ')\n\nplt.subplot(1, 6, 6)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=out_w,color='y')\nplt.xlabel('Output Layer ')\nplt.show()","04a723c8":"from keras.layers import Dropout\nmodel_5_drop = Sequential()\n\nmodel_5_drop.add(Dense(1024, activation='relu', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.039, seed=None)))\nmodel_5_drop.add(BatchNormalization())\nmodel_5_drop.add(Dropout(0.5))\n\nmodel_5_drop.add(Dense(896, activation='relu', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.039, seed=None)))\nmodel_5_drop.add(BatchNormalization())\nmodel_5_drop.add(Dropout(0.5))\n\nmodel_5_drop.add(Dense(768, activation='relu', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.039, seed=None)))\nmodel_5_drop.add(BatchNormalization())\nmodel_5_drop.add(Dropout(0.5))\n\nmodel_5_drop.add(Dense(640, activation='relu', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.039, seed=None)))\nmodel_5_drop.add(BatchNormalization())\nmodel_5_drop.add(Dropout(0.5))\n\nmodel_5_drop.add(Dense(512, activation='relu', kernel_initializer=RandomNormal(mean=0.0, stddev=0.55, seed=None)) )\nmodel_5_drop.add(BatchNormalization())\nmodel_5_drop.add(Dropout(0.5))\n\nmodel_5_drop.add(Dense(output_dim, activation='softmax'))\n\n\nmodel_5_drop.summary()","924fef3f":"model_5_drop.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\nhistory = model_5_drop.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))","4fd55fee":"model_5_drop_score = model_5_drop.evaluate(X_test, Y_test, verbose=1) \nprint('Test score:', model_5_drop_score[0]) \nprint('Test accuracy:', model_5_drop_score[1])\n\nfig,ax = plt.subplots(1,1)\nax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n\n# list of epoch numbers\nx = list(range(1,nb_epoch+1))\n\nvy = history.history['val_loss']\nty = history.history['loss']\nplt_dynamic(x, vy, ty, ax)\n","ce30688d":"w_after = model_5_drop.get_weights()\n\nh1_w = w_after[0].flatten().reshape(-1,1)\nh2_w = w_after[2].flatten().reshape(-1,1)\nh3_w = w_after[4].flatten().reshape(-1,1)\nh4_w = w_after[6].flatten().reshape(-1,1)\nh5_w = w_after[8].flatten().reshape(-1,1)\nout_w = w_after[10].flatten().reshape(-1,1)\n\n\nfig = plt.figure(figsize=(20,6))\nplt.title(\"Weight matrices after model trained\")\nplt.subplot(1, 6, 1)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h1_w,color='b')\nplt.xlabel('Hidden Layer 1')\n\nplt.subplot(1, 6, 2)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h2_w, color='r')\nplt.xlabel('Hidden Layer 2 ')\n\nplt.subplot(1, 6, 3)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h3_w, color='r')\nplt.xlabel('Hidden Layer 3 ')\n\nplt.subplot(1, 6, 4)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h3_w, color='r')\nplt.xlabel('Hidden Layer 4 ')\n\nplt.subplot(1, 6, 5)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h3_w, color='r')\nplt.xlabel('Hidden Layer 5 ')\n\nplt.subplot(1, 6, 6)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=out_w,color='y')\nplt.xlabel('Output Layer ')\nplt.show()","ee24cd95":"model_5_drop_medium = Sequential()\n\nmodel_5_drop_medium.add(Dense(256, activation='relu', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.039, seed=None)))\nmodel_5_drop_medium.add(BatchNormalization())\nmodel_5_drop_medium.add(Dropout(0.5))\n\nmodel_5_drop_medium.add(Dense(224, activation='relu', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.039, seed=None)))\nmodel_5_drop_medium.add(BatchNormalization())\nmodel_5_drop_medium.add(Dropout(0.5))\n\nmodel_5_drop_medium.add(Dense(192, activation='relu', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.039, seed=None)))\nmodel_5_drop_medium.add(BatchNormalization())\nmodel_5_drop_medium.add(Dropout(0.5))\n\nmodel_5_drop_medium.add(Dense(160, activation='relu', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.039, seed=None)))\nmodel_5_drop_medium.add(BatchNormalization())\nmodel_5_drop_medium.add(Dropout(0.5))\n\nmodel_5_drop_medium.add(Dense(128, activation='relu', kernel_initializer=RandomNormal(mean=0.0, stddev=0.55, seed=None)) )\nmodel_5_drop_medium.add(BatchNormalization())\nmodel_5_drop_medium.add(Dropout(0.5))\n\nmodel_5_drop_medium.add(Dense(output_dim, activation='softmax'))\n\n\nmodel_5_drop_medium.summary()\n","80e9ea49":"model_5_drop_medium.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\nhistory = model_5_drop_medium.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))","6dc5090d":"model_5_drop_medium_score = model_5_drop_medium.evaluate(X_test, Y_test, verbose=1) \nprint('Test score:', model_5_drop_medium_score[0]) \nprint('Test accuracy:', model_5_drop_medium_score[1])\n\nfig,ax = plt.subplots(1,1)\nax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n\n# list of epoch numbers\nx = list(range(1,nb_epoch+1))\n\nvy = history.history['val_loss']\nty = history.history['loss']\nplt_dynamic(x, vy, ty, ax)","476f4213":"w_after = model_5_drop_medium.get_weights()\n\nh1_w = w_after[0].flatten().reshape(-1,1)\nh2_w = w_after[2].flatten().reshape(-1,1)\nh3_w = w_after[4].flatten().reshape(-1,1)\nh4_w = w_after[6].flatten().reshape(-1,1)\nh5_w = w_after[8].flatten().reshape(-1,1)\nout_w = w_after[10].flatten().reshape(-1,1)\n\n\nfig = plt.figure(figsize=(20,6))\nplt.title(\"Weight matrices after model trained\")\nplt.subplot(1, 6, 1)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h1_w,color='b')\nplt.xlabel('Hidden Layer 1')\n\nplt.subplot(1, 6, 2)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h2_w, color='r')\nplt.xlabel('Hidden Layer 2 ')\n\nplt.subplot(1, 6, 3)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h3_w, color='r')\nplt.xlabel('Hidden Layer 3 ')\n\nplt.subplot(1, 6, 4)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h3_w, color='r')\nplt.xlabel('Hidden Layer 4 ')\n\nplt.subplot(1, 6, 5)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h3_w, color='r')\nplt.xlabel('Hidden Layer 5 ')\n\nplt.subplot(1, 6, 6)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=out_w,color='y')\nplt.xlabel('Output Layer ')\nplt.show()","b4f37634":"model_5_drop_small = Sequential()\n\nmodel_5_drop_small.add(Dense(128, activation='relu', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.039, seed=None)))\nmodel_5_drop_small.add(BatchNormalization())\nmodel_5_drop_small.add(Dropout(0.5))\n\nmodel_5_drop_small.add(Dense(112, activation='relu', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.039, seed=None)))\nmodel_5_drop_small.add(BatchNormalization())\nmodel_5_drop_small.add(Dropout(0.5))\n\nmodel_5_drop_small.add(Dense(96, activation='relu', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.039, seed=None)))\nmodel_5_drop_small.add(BatchNormalization())\nmodel_5_drop_small.add(Dropout(0.5))\n\nmodel_5_drop_small.add(Dense(80, activation='relu', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.039, seed=None)))\nmodel_5_drop_small.add(BatchNormalization())\nmodel_5_drop_small.add(Dropout(0.5))\n\nmodel_5_drop_small.add(Dense(64, activation='relu', kernel_initializer=RandomNormal(mean=0.0, stddev=0.55, seed=None)) )\nmodel_5_drop_small.add(BatchNormalization())\nmodel_5_drop_small.add(Dropout(0.5))\n\nmodel_5_drop_small.add(Dense(output_dim, activation='softmax'))\n\n\nmodel_5_drop_small.summary()","765530e9":"model_5_drop_small.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\nhistory = model_5_drop_small.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))","d78cb095":"model_5_drop_small_score = model_5_drop_small.evaluate(X_test, Y_test, verbose=1) \nprint('Test score:', model_5_drop_small_score[0]) \nprint('Test accuracy:', model_5_drop_small_score[1])\n\nfig,ax = plt.subplots(1,1)\nax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n\n# list of epoch numbers\nx = list(range(1,nb_epoch+1))\n\nvy = history.history['val_loss']\nty = history.history['loss']\nplt_dynamic(x, vy, ty, ax)","da788c1e":"w_after = model_5_drop_small.get_weights()\n\nh1_w = w_after[0].flatten().reshape(-1,1)\nh2_w = w_after[2].flatten().reshape(-1,1)\nh3_w = w_after[4].flatten().reshape(-1,1)\nh4_w = w_after[6].flatten().reshape(-1,1)\nh5_w = w_after[8].flatten().reshape(-1,1)\nout_w = w_after[10].flatten().reshape(-1,1)\n\n\nfig = plt.figure(figsize=(20,6))\nplt.title(\"Weight matrices after model trained\")\nplt.subplot(1, 6, 1)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h1_w,color='b')\nplt.xlabel('Hidden Layer 1')\n\nplt.subplot(1, 6, 2)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h2_w, color='r')\nplt.xlabel('Hidden Layer 2 ')\n\nplt.subplot(1, 6, 3)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h3_w, color='r')\nplt.xlabel('Hidden Layer 3 ')\n\nplt.subplot(1, 6, 4)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h3_w, color='r')\nplt.xlabel('Hidden Layer 4 ')\n\nplt.subplot(1, 6, 5)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h3_w, color='r')\nplt.xlabel('Hidden Layer 5 ')\n\nplt.subplot(1, 6, 6)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=out_w,color='y')\nplt.xlabel('Output Layer ')\nplt.show()","9105a3e2":"from prettytable import PrettyTable\n    \nx = PrettyTable()\n\nx.field_names = [\"Model No\", \"Hidden Layers\", \"Batch Normalization\", \"Dropouts\", \"test loss\", \"test accuracy\"]\nx.add_row([1,\"1024-512\", \"No\", \"No\", round(model_2_relu_score[0],4), round(model_2_relu_score[1],4)] )\nx.add_row([2,\"256-128\", \"No\", \"No\", round(model_2_relu_medium_score[0],4), round(model_2_relu_medium_score[1],4)] )\nx.add_row([3,\"128-64\", \"No\", \"No\", round(model_2_relu_small_score[0],4), round(model_2_relu_small_score[1],4)] )\nx.add_row([4,\"1024-512\", \"Yes\", \"No\", round(model_batch_relu_score[0],4), round(model_batch_relu_score[1],4)] )\nx.add_row([5,\"256-128\", \"Yes\", \"No\", round(model_batch_relu_medium_score[0],4), round(model_batch_relu_medium_score[1],4)] )\nx.add_row([6,\"128-64\", \"Yes\", \"No\", round(model_batch_relu_small_score[0],4), round(model_batch_relu_small_score[1],4)] )\nx.add_row([7,\"1024-512\", \"Yes\", \"Yes\", round(model_drop_score[0],4), round(model_drop_score[1],4)] )\nx.add_row([8,\"256-128\", \"Yes\", \"Yes\", round(model_drop_medium_score[0],4), round(model_drop_medium_score[1],4)] )\nx.add_row([9,\"128-64\", \"Yes\", \"Yes\", round(model_drop_small_score[0],4), round(model_drop_small_score[1],4)] )\nprint(x)","66db0d46":"x = PrettyTable()\n\nx.field_names = [\"Model No\", \"Hidden Layers\", \"Batch Normalization\", \"Dropouts\", \"test loss\", \"test accuracy\"]\n\nx.add_row([10,\"1024-768-512\", \"No\", \"No\", round(model_3_relu_score[0],4), round(model_3_relu_score[1],4)] )\nx.add_row([11,\"256-192-128\", \"No\", \"No\", round(model_3_relu_medium_score[0],4), round(model_3_relu_medium_score[1],4)] )\nx.add_row([12,\"128-96-64\", \"No\", \"No\", round(model_3_relu_small_score[0],4), round(model_3_relu_small_score[1],4)] )\nx.add_row([13,\"1024-768-512\", \"Yes\", \"No\", round(model_3_batch_relu_score[0],4), round(model_3_batch_relu_score[1],4)] )\nx.add_row([14,\"256-192-128\", \"Yes\", \"No\", round(model_3_batch_relu_medium_score[0],4), round(model_3_batch_relu_medium_score[1],4)] )\nx.add_row([15,\"128-96-64\", \"Yes\", \"No\", round(model_3_batch_relu_small_score[0],4), round(model_3_batch_relu_small_score[1],4)] )\nx.add_row([16,\"1024-768-512\", \"Yes\", \"Yes\", round(model_3_drop_score[0],4), round(model_3_drop_score[1],4)] )\nx.add_row([17,\"256-192-128\", \"Yes\", \"Yes\", round(model_3_drop_medium_score[0],4), round(model_3_drop_medium_score[1],4)] )\nx.add_row([18,\"128-96-64\", \"Yes\", \"Yes\", round(model_3_drop_small_score[0],4), round(model_3_drop_small_score[1],4)] )\nprint(x)","53431e55":"x = PrettyTable()\n\nx.field_names = [\"Model No\", \"Hidden Layers\", \"Batch Normalization\", \"Dropouts\", \"test loss\", \"test accuracy\"]\nx.add_row([19,\"1024-896-768-640-512\", \"No\", \"No\", round(model_5_relu_score[0],4), round(model_5_relu_score[1],4)] )\nx.add_row([20,\"256-224-192-160-128\", \"No\", \"No\", round(model_5_relu_medium_score[0],4), round(model_5_relu_medium_score[1],4)] )\nx.add_row([21,\"128-112-96-80-64\", \"No\", \"No\", round(model_5_relu_small_score[0],4), round(model_5_relu_small_score[1],4)] )\nx.add_row([22,\"1024-896-768-640-512\", \"Yes\", \"No\", round(model_5_batch_relu_score[0],4), round(model_5_batch_relu_score[1],4)] )\nx.add_row([23,\"256-224-192-160-128\", \"Yes\", \"No\", round(model_5_batch_relu_medium_score[0],4), round(model_5_batch_relu_medium_score[1],4)] )\nx.add_row([24,\"128-112-96-80-64\", \"Yes\", \"No\", round(model_5_batch_relu_small_score[0],4), round(model_5_batch_relu_small_score[1],4)] )\nx.add_row([25,\"1024-896-768-640-512\", \"Yes\", \"Yes\", round(model_5_drop_score[0],4), round(model_5_drop_score[1],4)] )\nx.add_row([26,\"256-224-192-160-128\", \"Yes\", \"Yes\", round(model_5_drop_medium_score[0],4), round(model_5_drop_medium_score[1],4)] )\nx.add_row([27,\"128-112-96-80-64\", \"Yes\", \"Yes\", round(model_5_drop_small_score[0],4), round(model_5_drop_small_score[1],4)] )\nprint(x)","f1bec0e0":"## Experiment 1 (2 hidden layers 1024 512 neurons)","9e0ff2ad":"- Both test and train loss are decresing and become equal at the end of 20 epochs.\n- First layer have weights in the form of normal distribution\n- Most of the weights of the output layer are positive\n- Test loss is always less than train loss\n- Accuracy compared to prior dropout models is almost same","bdfbdf35":"## Experiment 27 (5 hidden layers 128 112 96 80 64 neurons + batch normalization + dropouts)","247b9899":"## Experiment 7 (2 hidden layers 1024 512 neurons + Batch Normalization + Dropout))","25e8f6f0":"- Both test and train loss are decresing\n- First layer have weights in the form of normal distribution\n- Most of the weights of the output layer are positive\n- Test loss is always less than train loss","536595ad":"- Train loss is decreasing continously but test loss is increasing and fluctuating\n- First two layers have weights in the form of normal distribution\n- Accuracy has increased with increase in number of hidden layers. Better than two layered nn.","dd24722b":"- Train loss is decreasing continously but test loss is increasing and fluctuating\n- First two layers have weights in the form of normal distribution\n- There is an increase in accuracy with reduction in number of neurons","d62c2b7d":"- Train loss is decreasing continously but test loss is increasing and fluctuating\n- First five layers have weights in the form of normal distribution\n- There is a slight decrease in the accuracy compared to three layer NN","a43c5e00":"- Both test and train loss are decresing.\n- Test loss is always less than train loss\n- First layer have weights in the form of normal distribution\n- Accuracy is becoming lower compared to two prior models. Means dropout works well if the number of neurons in hidden layer is more","4703e215":"- Train loss is decreasing continously but test loss is fluctuating. There is a significant gap between train and test loss.\n- The fluctuation have reduced compared to models with more neurons\n- First layer have weights in the form of normal distribution\n- Most of the weights of the output layer are positive","04ec349c":"- Train loss is decreasing continously but test loss is increasing and fluctuating. But the fluctuation has reduced with reduction in number of neurons\n- First two layers have weights in the form of normal distribution\n- There is an increase in accuracy with reduction in number of neurons","ed8be472":"## Experiment 8 (2 hidden layers 254 128 neurons + Batch Normalization + Dropout))","fd04bece":"- Using imshow to see one image in the dataset. The number in the 218th position is 6","20d87cf8":"![image.png](attachment:image.png)","2895edd9":"- Train loss is decreasing continously but test loss is increasing and fluctuating\n- Frequency of fluctuation decresed\n- First layers have weights in the form of normal distribution\n- There is a slight decrease in the accuracy compared to three layer NN","1f1dc8ce":"## Experiment 23 (5 hidden layers 256 224 192 160 128 neurons + batch normalization)","5c1d8b0a":"- Train loss is decreasing continously but test loss is increasing and fluctuating\n- Frequency of fluctuation has reduced\n- First layer have weights in the form of normal distribution","f5b68ee4":"## Experiment 21 (5 hidden layers 128 112 96 80 64 neurons)","9c841fe5":"## Experiment 14 (3 hidden layers 256 192 128 neurons + batch normalization)","a7635e47":"## Experiment 4 (2 hidden layers 1024  512 neurons + Batch Normalization)","81d064ac":"## Experiment 18 (3 hidden layers 128 96 64 neurons + batch normalization + dropouts)","8b0cace6":"## Importing Libraries","a6f6e1de":"- The X_tran and X_test are three dimentional numpy array. The 0th dimention is the number of images, 1st dimension is the width and 2nd dimension is the height","3affc6ba":"- Both test and train loss are decresing.\n- First layer have weights in the form of normal distribution\n- Accuracy is reduced from prior model. This means dropout works well if the number of neurons in each hidden layer is more.","34501d22":"- Both test and train loss are decresing.\n- First layer have weights in the form of normal distribution\n- Most of the weights of the output layer are positive\n- Test loss is always less than train loss\n- Accuracy was more for nn with more number of neurons in each layer","725ea777":"## Experiment 24 (3 hidden layers 128 112 96 80 64 neurons + batch normalization)","b3ddee42":"## Experiment 16 (3 hidden layers 1024 768 512 neurons + batch normalization + dropouts)","81ed92f7":"- Train loss is decreasing continously but test loss is increasing and fluctuating\n- Frequency of fluctuation has reduced\n- First two layers have weights in the form of normal distribution\n- Accuracy has increased with increase in number of hidden layers. Better than two layered nn.","e52e4ca0":"### Five Hidden Layer","c60ab5a7":"## Experiment 26 (5 hidden layers 256 224 192 160 128 neurons + batch normalization + dropouts)","53cd0689":"- Train loss is decreasing continously but test loss is fluctuating. There is a significant gap between train and test loss\n- First two layers have weights in the form of normal distribution\n- Most of the weights of the output layer are positive","77e4514d":"## Experiment 15 (3 hidden layers 128 96 64 neurons + batch normalization)","3b70a17e":"## Experiment 17 (3 hidden layers 256 192 128 neurons + batch normalization + dropouts)","22329039":"## Experiment 13 (3 hidden layers 1024 768 512 neurons + batch normalization)","ca6cc624":"- Both test and train loss are decresing.\n- First layer have weights in the form of normal distribution\n- Most of the weights of the output layer are positive\n- Test loss is always less than train loss\n- Accuracy was more for nn with more number of neurons in each layer","6b2a9af9":"- Both test and train loss are decresing.\n- First layer have weights in the form of normal distribution\n- Most of the weights of the output layer are positive\n- Test loss is always less than train loss\n- Accuracy was more for nn with more number of neurons in each layer","dfc7dbbf":"We have learned about the neural network basics and experimented with different optimization functions, batch normalization, dropouts etc in the last note book. If you need a quick revision please take a look in https:\/\/www.kaggle.com\/vishnurapps\/beginners-guide-to-use-keras\n\nNow we are going to check the importance of the depth of neural network and its influence in the output prediction. We will test with different depths, batch normalization and dropouts","02234dfd":"## Experiment 9 (2 hidden layers 128 64 neurons + Batch Normalization + Dropout))","b781c0f1":"## Experiment 22 (5 hidden layers 1024 896 768 640 512 neurons + batch normalization)","b63624fd":"## Experiment 20 (3 hidden layers 256 224 192 160 128 neurons)","91c399dc":"- Train loss is decreasing continously but test loss is increasing and fluctuating\n- First two layers have weights in the form of normal distribution\n- There is a slight decrease in the accuracy compared to three layer NN","7cad13c9":"### Two hidden layers","89c3aae2":"## Experiment 3 (2 hidden layers 128 64 neurons)","f44c581b":"## Experiment 11 (3 hidden layers 256 192 128 neurons)","ba70ac09":"## Experiment 6 (2 hidden layers 256 128 neurons + Batch Normalization)","38517102":"- Both test and train loss are decresing and become equal at the end of 20 epochs.\n- First layer have weights in the form of normal distribution\n- Most of the weights of the output layer are positive\n- Test loss is always less than train loss","89fdafc5":"- Train loss is decreasing continously but train loss is fluctuating. There is a significant gap between train and test loss\n- There is heavy fluctuation in the test loss but compared to prior model, its less. \n- Test loss seems to increase\n- First layers have weights in the form of normal distribution\n- There is no significant inprovement in accuracy compared to other batch normalization models","ae5a450f":"- The test loss with out using batch normalization or dropouts was heavly fluctuating.\n- This fluctuation seems to reduce with decrease in the number of neurons in in each layer.\n- Test loss was always greater than train loss.\n- The initial test loss was very high and in some models the test loss seems to increse. Especially the models having more number of neurons in each layer.\n- Similar observation was seen with batch normalization but the rate of change of test loss was less compared to models having no batch normalization.\n- Best performance was shown by models with dropouts. The test loss was always less than train loss. There was no fluctuation.\n- For models with dropouts the accuracy was more for models having more number of neurons in each hidden layer.","620d84a6":"- Train loss is decreasing continously but train loss is fluctuating. There is a significant gap between train and test loss\n- There is heavy fluctuation in the test loss but compared to prior model, its less. \n- First layers have weights in the form of normal distribution\n- There is no significant inprovement in accuracy compared to other batch normalization models","2d7dafef":"## Experiment 10 (3 hidden layers 1024 768 512 neurons)","410662dc":"- Train loss is decreasing continously but test loss is increasing and fluctuating\n- First and last layers have weights in the form of normal distribution","54a516d4":"### Three Hidden Layer","bc254e10":"## Experiment 6 (2 hidden layers 128 64 neurons + Batch Normalization))","e11b7e27":"## Experiment 19 (5 hidden layers 1024 896 768 640 512 neurons)","293c402a":"- Train loss is decreasing continously but test loss is increasing and fluctuating\n- Frequency of fluctuation has reduced compared to prior model\n- First and last layers have weights in the form of normal distribution","bdabf51c":"## Experiment 25 (3 hidden layers 1024 896 768 640 512 neurons + batch normalization + dropouts)","d7f3a229":"- Train loss is decreasing continously but test loss is increasing and fluctuating\n- First two layers have weights in the form of normal distribution","4fb7e4db":"- Both test and train loss are decresing.\n- First layer have weights in the form of normal distribution\n- Accuracy is more compared to two hidden layer NN.","588aa384":"## Experiment 12 (3 hidden layers 128 96 64 neurons)","442128dd":"## Summary","c21300b5":"## Experiment 2 (2 hidden layers 256 128 neurons)","ad7229a5":"- Train loss is decreasing continously but train loss is fluctuating. There is a significant gap between train and test loss\n- There is heavy fluctuation in the test loss\n- First layers have weights in the form of normal distribution\n- There is no significant inprovement in accuracy compared to other batch normalization models","7df7194d":"- Train loss is decreasing continously but test loss is fluctuating. There is a significant gap between train and test loss and gap is increasing\n- First layer have weights in the form of normal distribution\n- Most of the weights of the output layer are positive","3eba5d7f":"- Train loss is decreasing continously but test loss is increasing and fluctuating\n- First three layers have weights in the form of normal distribution\n- Accuracy has increased with increase in number of hidden layers. Better than two layered nn."}}