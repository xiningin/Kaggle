{"cell_type":{"b32974d1":"code","db4b527b":"code","e23fa6c2":"code","bc558dc3":"code","9fc30b1c":"code","834507d9":"code","370244ee":"code","13555a23":"code","c41d6dee":"code","ba994e6d":"code","89487e63":"code","442f6de3":"code","5e90d2f9":"code","c0189798":"code","386b9eda":"code","737ca027":"markdown","91ccdbe7":"markdown"},"source":{"b32974d1":"import optuna\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport xgboost as xgb\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nimport warnings\nwarnings.filterwarnings('ignore')","db4b527b":"path = '..\/input\/tabular-playground-series-mar-2021\/'\ntrain = pd.read_csv(path + 'train.csv')\ntest = pd.read_csv(path + 'test.csv')\nsub = pd.read_csv(path + 'sample_submission.csv')","e23fa6c2":"cat = [col for col in train.columns if 'cat' in col]\ncont = [col for col in train.columns if 'cont' in col]\nall_features = cat + cont","bc558dc3":"all_df = pd.concat([train , test]).reset_index(drop = True)\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nfor col in cat:\n    all_df[col] = le.fit_transform(all_df[col])\n    \ntrain = all_df[:train.shape[0]]\ntest = all_df[train.shape[0]:].reset_index(drop = True)","9fc30b1c":"data = train[all_features]\ntarget = train['target']","834507d9":"def objective(trial , data = data , target = target):\n    train_x , test_x , train_y , test_y = train_test_split(data , target , \\\n                test_size = 0.028059109276941666 , random_state = 42)\n    params = {\n        'eval_metric' : 'auc',\n        'booster' : 'gbtree',\n        'tree_method' : 'gpu_hist' , \n        'use_label_encoder' : False , \n        'lambda' : trial.suggest_loguniform('lambda' , 1e-5 , 1.0),\n        'alpha' : trial.suggest_loguniform('alpha' , 1e-5 , 1.0),\n        'colsample_bytree' : trial.suggest_uniform('colsample_bytree' , 0 , 1.0),\n        'subsample' : trial.suggest_uniform('subsample' , 0 , 1.0),\n        'learning_rate' : trial.suggest_uniform('learning_rate' , 0 , 0.02),\n        'n_estimators' : trial.suggest_int('n_estimators' , 1 , 9999),\n        'max_depth' : trial.suggest_int('max_depth' , 1 , 20),\n        'random_state' : trial.suggest_categorical('random_state' , [0,42,2021]),\n        'min_child_weight' : trial.suggest_int('min_child_weight' , 1 , 300),\n        'gamma' : trial.suggest_loguniform('gamma' , 1e-5 , 1.0)\n    }\n    model = xgb.XGBClassifier(**params)\n    model.fit(train_x , train_y , eval_set = [(test_x , test_y)] , early_stopping_rounds = 222 , \\\n              verbose = False)\n    preds = model.predict_proba(test_x)[: , 1]\n    auc = roc_auc_score(test_y , preds )\n    return auc","370244ee":"study = optuna.create_study(direction = 'maximize' , study_name = 'xgbclassifier')\nstudy.optimize(objective , n_trials = 60)\nprint('number of the finished trials:' , len(study.trials))\nprint('the parametors of best trial:' , study.best_trial.params)\nprint('best value:' , study.best_value)","13555a23":"optuna.visualization.plot_optimization_history(study)","c41d6dee":"optuna.visualization.plot_param_importance(study)","ba994e6d":"#Best  0.8990941303451653\nparams = {'lambda': 3.342625262710592e-05, 'alpha': 0.0005910445093857934, \\\n          'colsample_bytree': 0.42295113660344236, 'subsample': 0.8092952867076734,\\\n          'learning_rate': 0.014533634130298151, 'n_estimators': 5966, 'max_depth': 12, \\\n          'random_state': 2021, 'min_child_weight': 24, 'gamma': 0.017646631838015223}","89487e63":"params['eval_metric'] = 'auc'\nparams['booster'] = 'gbtree'\nparams['tree_method'] = 'gpu_hist'\nparams['use_label_encoder'] = False","442f6de3":"params","5e90d2f9":"preds = np.zeros(test.shape[0])\noof_predictions = np.zeros(len(data))\nkf = KFold(n_splits = 20 , random_state = 42 , shuffle = True)\nroc = []\nn = 0\nfor trn_idx , val_idx in kf.split(data , target):\n    train_x = data.iloc[trn_idx]\n    train_y = target.iloc[trn_idx]\n    val_x = data.iloc[val_idx]\n    val_y = target.iloc[val_idx]\n    \n    model = xgb.XGBClassifier(**params)\n    model.fit(train_x , train_y , eval_set = [(val_x , val_y)] , early_stopping_rounds = 100 , \\\n             verbose = False)\n    preds += model.predict_proba(test[all_features])[:,1]\/kf.n_splits\n    oof_predictions += model.predict_proba(data[all_features])[:,1]\/kf.n_splits\n    roc.append(roc_auc_score( val_y , model.predict_proba(val_x)[:,1]))\n    print(n+1 , roc[n])\n    n+=1","c0189798":"sub['target'] = preds\nsub.to_csv('xgbsubmission.csv' , index = False)","386b9eda":"output = pd.DataFrame({'id':train['id'] , 'target':oof_predictions})\noutput.to_csv('xgboof_predictions.csv' , index = False)","737ca027":"# Label Encoder","91ccdbe7":"# OPTUNA"}}