{"cell_type":{"9a607314":"code","c3792d75":"code","7e426979":"code","542f905c":"code","550fcde8":"code","b11ffb66":"code","405cf739":"code","ba1e4d7b":"code","8abde3b6":"code","41d25247":"code","5d61a545":"code","9c3f0f75":"code","845f3933":"code","c7fe1ba1":"code","47ac0d41":"code","f4379691":"code","90d96b99":"code","51beb639":"code","07363d8c":"code","eba0a01d":"code","2d24d558":"code","6fa2119a":"code","e3826980":"code","d587ac43":"code","c950e676":"code","9e592f0a":"code","e5f8df10":"code","8a3d3c1c":"code","f4807b55":"code","331295c4":"code","63abe3dd":"code","6d2efa50":"code","f3df7d48":"markdown","67794b49":"markdown","0dfcb6f5":"markdown","b7525177":"markdown","4cc51d7c":"markdown","600a671d":"markdown","a3e88809":"markdown","e8ae9492":"markdown","54ae90ec":"markdown","138440c9":"markdown"},"source":{"9a607314":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c3792d75":"train_df= pd.read_csv('\/kaggle\/input\/fashionmnist\/fashion-mnist_train.csv')\ntest_df= pd.read_csv('\/kaggle\/input\/fashionmnist\/fashion-mnist_test.csv')","7e426979":"print(train_df.shape)\nprint(test_df.shape)","542f905c":"X_train= train_df.drop(['label'],axis = 1)\nX_test=train_df['label']\ny_test=test_df.drop(['label'],axis = 1)\n\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)","550fcde8":"X_train","b11ffb66":"X_test","405cf739":"X_train = X_train.astype('float32')\ny_test = y_test.astype('float32')\nX_train \/= 255.0\ny_test \/=255.0","ba1e4d7b":"from sklearn.model_selection import train_test_split\nstate = 42\nnp.random.seed(state)\nX_train, X_val, y_train, y_val = train_test_split(X_train, X_test, test_size=0.1, random_state = state)","8abde3b6":"from sklearn.decomposition import PCA\npca = PCA(n_components=100, random_state=42)\nX_train_pca =pca.fit_transform(X_train)\nX_test_pca = pca.transform(X_val)\ny_test_pca =pca.transform(y_test)","41d25247":"X_train_pca.shape","5d61a545":"X_train_PCA1 = pd.DataFrame(X_train_pca)\nX_test_PCA1 = pd.DataFrame(X_test_pca)","9c3f0f75":"from sklearn.linear_model import LogisticRegression\nimport time\nstart = time.time()\nlog_clf = LogisticRegression(max_iter=1000,multi_class=\"multinomial\", solver=\"lbfgs\", C=1, random_state=42)\nlog_clf.fit(X_train_PCA1, y_train)\n\nend = time.time()\nlog_time = end-start\n\nprint(\"Logistic Regression Time minute: \",(log_time\/60.0))","845f3933":"from sklearn.metrics import accuracy_score\nfrom sklearn import metrics\n\ny_train_log = log_clf.predict(X_train_PCA1)\ny_pred_log = log_clf.predict(X_test_pca)\nlogistic_train = metrics.accuracy_score(y_train,y_train_log )\nlogistic_accuracy = metrics.accuracy_score(y_val, y_pred_log)\n\nprint(\"Train Accuracy score: \",(logistic_train))\nprint(\"Test Accuracy score: \",(logistic_accuracy))\nprint(metrics.classification_report(y_val, y_pred_log))","c7fe1ba1":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nconfusion_matrix_log = pd.crosstab(pd.Series(y_val.values.flatten(), name='Real' ),pd.Series(y_pred_log, name='Result'))\nplt.figure(figsize = (14,12))\nplt.title(\"Logistic Regression\")\nsns.heatmap(confusion_matrix_log, annot=True, fmt='g')\nplt.show()","47ac0d41":"from sklearn.svm import SVC\n\nstart1 = time.time()\n\nsvc_clf = SVC(C=21,kernel='poly',gamma=\"auto\")\nsvc_clf.fit(X_train_PCA1, y_train)\n\nend1 = time.time()\nsvm_time = end1-start","f4379691":"print(\"SVM Time minute: \",(svm_time\/60.0))","90d96b99":"y_train_svc = svc_clf.predict(X_train_PCA1)\ny_pred_svc = svc_clf.predict(X_test_pca)\nsvc_train = metrics.accuracy_score(y_train,y_train_svc)\nsvc_accuracy = metrics.accuracy_score(y_val, y_pred_svc)\n\nprint(\"Train Accuracy score: \",(svc_train))\nprint(\"Test Accuracy score: \",(svc_accuracy))\nprint(metrics.classification_report(y_val, y_pred_svc))","51beb639":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nconfusion_matrix_svc = pd.crosstab(pd.Series(y_val.values.flatten(), name='Real' ),pd.Series(y_pred_svc, name='Result'))\nplt.figure(figsize = (14,12))\nplt.title(\"Support Vector Regression\")\nsns.heatmap(confusion_matrix_svc, annot=True, fmt='g')\nplt.show()","07363d8c":"from sklearn.ensemble import RandomForestClassifier\n\nstart2 = time.time()\n\nrandom= RandomForestClassifier(criterion='entropy', max_depth=100, n_estimators=1000,random_state=42)\nrandom.fit(X_train_PCA1, y_train)\n\nend2 = time.time()\nforest_time = end2 - start2","eba0a01d":"print(\"Random Forest Time minute: \",(forest_time\/60.0))","2d24d558":"y_train_random = random.predict(X_train_PCA1)\ny_pred_random = random.predict(X_test_pca)\nrandom_train = metrics.accuracy_score(y_train,y_train_random)\nrandom_accuracy = metrics.accuracy_score(y_val, y_pred_random)\n\nprint(\"Train Accuracy score: \",(random_train))\nprint(\"Test Accuracy score: \",(random_accuracy))\nprint(metrics.classification_report(y_val, y_pred_random))","6fa2119a":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nconfusion_matrix_random = pd.crosstab(pd.Series(y_val.values.flatten(), name='Real' ),pd.Series(y_pred_random, name='Result'))\nplt.figure(figsize = (14,12))\nplt.title(\"Random Forest\")\nsns.heatmap(confusion_matrix_random, annot=True, fmt='g')\nplt.show()","e3826980":"from sklearn import ensemble\n\nstart3 = time.time()\n\nGradient = ensemble.GradientBoostingClassifier(n_estimators=100,random_state=42)\nGradient.fit(X_train_PCA1, y_train)\n\nend3 = time.time()\ngradient_time = end3-start3","d587ac43":"print(\"Gradient Boosting Time minute: \",(gradient_time\/60.0))","c950e676":"y_train_gradient = Gradient.predict(X_train_PCA1)\ny_pred_gradient = Gradient.predict(X_test_pca)\nGradient_train = metrics.accuracy_score(y_train,y_train_gradient)\nGradient_accuracy = metrics.accuracy_score(y_val, y_pred_gradient)\n\nprint(\"Train Accuracy score: \",(Gradient_train))\nprint(\"Test Accuracy score: \",(Gradient_accuracy))\nprint(metrics.classification_report(y_val, y_pred_gradient))","9e592f0a":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nconfusion_matrix_gradient = pd.crosstab(pd.Series(y_val.values.flatten(), name='Real' ),pd.Series(y_pred_gradient, name='Result'))\nplt.figure(figsize = (14,12))\nplt.title(\"Gradient Boosting\")\nsns.heatmap(confusion_matrix_random, annot=True, fmt='g')\nplt.show()","e5f8df10":"from xgboost import XGBClassifier\n\nstart4 = time.time()\nXGboost = XGBClassifier(use_label_encoder=False,objective=\"multi:softmax\",eval_metric=\"merror\",n_estimators=20,max_depth=10,random_state=42)\nXGboost.fit(X_train_PCA1, y_train.ravel())\n\nend4 = time.time()\nXGboost_time = end4 - start4","8a3d3c1c":"print(\"XGBoost Time minute: \",(XGboost_time\/60.0))","f4807b55":"y_train_XGBoost = XGboost.predict(X_train_PCA1)\ny_pred_XGBoost = XGboost.predict(X_test_pca)\nXGBoost_train = metrics.accuracy_score(y_train,y_train_XGBoost)\nXGBoost_accuracy = metrics.accuracy_score(y_val, y_pred_XGBoost)\n\nprint(\"Train Accuracy score: \",(XGBoost_train))\nprint(\"Test Accuracy score: \",(XGBoost_accuracy))\nprint(metrics.classification_report(y_val, y_pred_XGBoost))","331295c4":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nconfusion_matrix_XGBoost = pd.crosstab(pd.Series(y_val.values.flatten(), name='Real' ),pd.Series(y_pred_XGBoost, name='Result'))\nplt.figure(figsize = (14,12))\nplt.title(\"Gradient Boosting\")\nsns.heatmap(confusion_matrix_XGBoost, annot=True, fmt='g')\nplt.show()","63abe3dd":"Train_scores=[logistic_train,svc_train,random_train,Gradient_train,XGBoost_train]\nTest_scores=[logistic_accuracy,svc_accuracy,random_accuracy,Gradient_accuracy,XGBoost_accuracy]\nmodel_dict = {\n    'Algos': ['Logistic Regression','SVC','Random Forest Classifier','Gradient Boosting','XGBoost'],'Train Accuracy':Train_scores,'Test Accuracy':Test_scores}\n\nmodel_dataset = pd.DataFrame(model_dict)\nmodel_dataset","6d2efa50":"import plotly.graph_objects as go\n\nfig = go.Figure(data=[\n    go.Bar(name='train set', x=model_dict['Algos'], y=model_dict['Train Accuracy'],text=np.round(model_dict['Train Accuracy'],2),textposition='outside',marker_color='lightsalmon'),\n    go.Bar(name='test set', x=model_dict['Algos'], y=model_dict['Test Accuracy'],text=np.round(model_dict['Test Accuracy'],2),textposition='outside',marker_color='indianred')\n])\n\nfig.update_layout(barmode='group',title_text='Accuracy of different algos on Fashion mnist dataset',yaxis=dict(\n        title='Accuracy'))\nfig.show()","f3df7d48":"# Random Forest","67794b49":"# ALGO Comparison \n\n## 1. SVC is the best algo among the above mentioned algorithms(giving almost equal weights to training and testing set)\n## 2. Gradient boosting and random forest took almost same and much tim eto prepare the prediction model(40 min approx)\n## 3. Logistic regression and XGBoost took least time as respected to the other algorithms(Logistic: 2min approx and XGBoost: 3min approx)\n## 4. Use of algos according to there prediction models(SVC > XGBoosting > Random Forest > Gradient Boosting > Logistic Regression)\n## 5. Use of algos according to there least time taking to learn (Logistic Regression > XGBoosting > SVC > Random Forest > Gradient Boosting) ","0dfcb6f5":"# Gradient Vector Regression","b7525177":"# PCA\n","4cc51d7c":"# Support Vector Regression","600a671d":"# XgBoost","a3e88809":"# Comparing the models","e8ae9492":"# Normalization","54ae90ec":"# Logistic Regression","138440c9":"# train test split"}}