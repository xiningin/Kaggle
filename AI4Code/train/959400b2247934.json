{"cell_type":{"e2223d3c":"code","44e59e9f":"code","9d0a2ab9":"code","e3efff7a":"code","0b35b15b":"code","bda5bc38":"code","18b97c71":"code","786ade11":"code","2fdc36e4":"code","37285ff0":"code","32c707d1":"code","d524d045":"code","19cedfb2":"code","72c6d2f2":"code","7075a7c4":"code","45652812":"code","985cd35a":"code","1896b521":"code","56d30f1d":"code","ffb48556":"code","e8037b29":"code","99d4a242":"code","e119cbbd":"code","f7ef21e8":"code","c8c4e25b":"markdown","c855db2a":"markdown","c11521d6":"markdown","0ab0132b":"markdown","f1596d27":"markdown","acca4d2f":"markdown","f15bfef3":"markdown","bf66d669":"markdown","3039ed14":"markdown","36df9c57":"markdown","94c7cdcd":"markdown","6f0faaa5":"markdown","78a6ef28":"markdown","a37b097e":"markdown","9c5cd606":"markdown","d0e585af":"markdown","ec549075":"markdown","fc0eb624":"markdown","55e9a3b6":"markdown","4d562fc7":"markdown","b6387adf":"markdown","d023d152":"markdown","09e0b82c":"markdown","e153451a":"markdown","6ecc4e69":"markdown","99356b35":"markdown"},"source":{"e2223d3c":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n\n# Any results you write to the current directory are saved as output.","44e59e9f":"train_df = pd.read_csv('..\/input\/train.csv')\ntest_df = pd.read_csv('..\/input\/test.csv')\n\ntest_df_original = test_df.copy()\ntest_df.head()","9d0a2ab9":"print(\"training data\", train_df.shape)\nprint(\"training data\", test_df.shape)","e3efff7a":"train_df.info()","0b35b15b":"train_df['SalePrice'].describe()","bda5bc38":"def explore(df):\n    numerical = ['GrLivArea' , 'TotalBsmtSF' ]\n    for feature in  numerical:\n        data = pd.concat([df['SalePrice'], df[feature]], axis=1)\n        data.plot.scatter(x=feature, y='SalePrice', ylim=(0,800000));\n    \nexplore(train_df)\n","18b97c71":"def explore(df):\n    categorical = ['OverallQual' , 'YearBuilt', 'BsmtCond','BsmtCond']\n    for feature in  categorical:\n        data = pd.concat([df['SalePrice'], df[feature]], axis=1)\n        f, ax = plt.subplots(figsize=(16, 8))\n        fig = sns.boxplot(x=feature, y=\"SalePrice\", data=data)\n        fig.axis(ymin=0, ymax=800000);\n    \nexplore(train_df)","786ade11":"def heatMap(df):\n\n    corr = df.corr()\n    plt.subplots(figsize=(20,12))\n    sns.heatmap(corr, vmax=0.9, square=True)\n    \n\n    \nheatMap(train_df)","2fdc36e4":"# least correlated features\ncorr = train_df.corr()\nleast_corr_features = corr.index[abs(corr[\"SalePrice\"])<0.25]\nplt.figure(figsize=(10,10))\ng = sns.heatmap(train_df[least_corr_features].corr(),annot=True,cmap=\"RdYlGn\")\n\n## We will be deleting these features from our data set as they do not help much in making predictions","37285ff0":"least_corr_features","32c707d1":"sns.set()\ncols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', '1stFlrSF', 'YearBuilt', 'FullBath']\nsns.pairplot(train_df[cols], size = 2.0)\nplt.show();","d524d045":"plt.figure(figsize = (12, 6))\nsns.boxplot(x = 'Neighborhood', y = 'SalePrice',  data = train_df)\nxt = plt.xticks(rotation=45)","19cedfb2":"#missing data\n\ndef missing_data_total(df):\n    total = df.isnull().sum().sort_values(ascending=False)\n    percent = (df.isnull().sum()\/df.isnull().count()).sort_values(ascending=False)\n    missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    return missing_data.head(20)\n\nprint(\"missing training data\",missing_data_total(train_df))\nprint(\"missing testing data\" ,missing_data_total(test_df))    \n    \n","72c6d2f2":"# defining a function to perform transformation\n\ndef delete(df):\n    \n    # we will be deleting useless and least correlated features from our dataset\n    useless = [ 'MasVnrType', 'GarageYrBlt','GarageArea' ,'TotalBsmtSF' , 'TotRmsAbvGrd','MSSubClass', 'OverallCond', 'BsmtFinSF2', 'BsmtUnfSF',\n       'LowQualFinSF', 'BsmtFullBath', 'BsmtHalfBath', 'BedroomAbvGr',\n       'KitchenAbvGr', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea',\n       'MiscVal', 'MoSold', 'YrSold']\n    \n    for feature in useless: \n        df.drop([feature], axis = 1, inplace = True)\n        \n    return df\n\ndef fill_categorical(df):\n    \n    fill = ['PoolQC', 'MiscFeature','Alley', 'Fence','FireplaceQu','GarageType', 'GarageFinish', 'GarageQual', 'GarageCond','BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2']\n    \n    for feature in fill:\n        #filling these features with \"None\"\n        \n        df[feature] = df[feature].fillna(\"None\")\n        \n    return df\n\ndef fill_numerical(df):\n    \n    \n    fill = ['GarageCars','MasVnrArea','BsmtFinSF1']\n    \n    for feature in fill:\n        \n        #filling these features with \"None\"\n        \n        df[feature] = df[feature].fillna(0)\n        \n    return df\n\ndef fill_mode(df):\n    \n    fill=['KitchenQual', 'Electrical','MSZoning',\"Utilities\", \"Functional\",\"Exterior1st\", \"Exterior2nd\"]\n    \n    for feature in fill:\n        df[feature] = df[feature].fillna(df[feature].mode()[0])\n        \n    return df\n\n# now we will fill lot frontage\n\ndef fill_lot(df):\n    \n    df[\"LotFrontage\"] = df.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.median()))\n    return df\n    \ndef transform(df):\n    \n    df = delete(df)\n    df = fill_categorical(df)\n    df = fill_numerical(df)\n    df = fill_mode(df)\n    df = fill_lot(df)\n    return df\n    \n    \ntrain_df = transform(train_df)\ntest_df = transform(test_df)\n\n\ntrain_df.head()\n","7075a7c4":"train_df.describe(include=['O'])\n","45652812":"from sklearn.preprocessing import LabelEncoder\n\ndef encoder(df):\n    cols = ( 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC',  'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'CentralAir','PoolQC')\n    for feature in cols:\n        label = LabelEncoder() \n        label.fit(list(df[feature].values)) \n        df[feature] = label.transform(list(df[feature].values))\n    \n    print('Shape data: {}'.format(df.shape))\n  \n    return df\n\ntrain_df = encoder(train_df)\ntest_df = encoder(test_df)\n ","985cd35a":"######### Total area of the house   ######## As basement was correlated to 1stfloor area, it was deleted earlier. \n\ndef new_feature(df):\n    df['TotalSF'] = df['1stFlrSF'] + df['2ndFlrSF']\n    df.drop(['1stFlrSF', '2ndFlrSF'], axis=1, inplace=True)\n    return df\ntrain_df= new_feature(train_df)\ntest_df = new_feature(test_df)\n\ntrain_df.head()\n\n# We will drop these 2 features from data frame and include a new feature instead","1896b521":"from scipy import stats\nfrom scipy.stats import norm, skew \n\nntrain = train_df.shape[0]\nntest = test_df.shape[0]\ny_train = train_df.SalePrice.values\n\n#train_df = train_df.drop\n\nall_data = pd.concat([train_df, test_df]).reset_index(drop=True)\nall_data.drop(['SalePrice'], axis=1, inplace=True)\nprint(\"all_data size is : {}\".format(all_data.shape))\n\n\nskewed_feature = all_data.dtypes[all_data.dtypes != \"object\"].index\n\n# Check the skew of all numerical features\nskewed_feats = all_data[skewed_feature].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features for all the data (training + testing): \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nprint(skewness.head(10))\n","56d30f1d":"from scipy.special import boxcox1p\n\n# We are using Box Cox and not Logarithmic transformation, because we have imputes some features to 0 vale. So logarithmic \n#transformation of these features will not be possible \nskewness = skewness[abs(skewness) > 0.75]\nprint(\"There are {} skewed numerical features to  transform\".format(skewness.shape[0]))\nskewed_features = skewness.index\n  \nlam = 0.15 \nfor transform in skewed_features:\n    all_data[transform] = boxcox1p(all_data[transform], lam)\n      \n\n    ","ffb48556":"all_data = pd.get_dummies(all_data)\nprint(all_data.head())","e8037b29":"### Splitting features\ntrain = all_data[:ntrain]\ntest = all_data[ntrain:]\n\nprint(\"Training shape\", train.shape)\nprint(\"Testing shape\", test.shape)","99d4a242":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(train, y_train, test_size = 0.3, random_state = 200)\n\n","e119cbbd":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn.metrics import make_scorer, mean_squared_error\n\nscorer = make_scorer(mean_squared_error, False)\nparameters = {'n_estimators' : [100,150,200,250,300] , 'max_features' : [0.2, 0.1,0.3,0.05], 'min_samples_split' :[2,3,4]}\n\nclf = RandomForestRegressor(random_state = 42, n_jobs = -1)\ngrid_obj = GridSearchCV(clf, parameters, scoring =  scorer)\n\n#Fit the grid search object to the training data and find the optimal parameters using fit()\ngrid_fit = grid_obj.fit(X_train, y_train)\n\n# Get the estimator\nbest_clf = grid_fit.best_estimator_\n\nprint(best_clf)\n\n# Make predictions using the unoptimized and model\npredictions = (clf.fit(X_train, y_train)).predict(X_test)\nbest_predictions = best_clf.predict(X_test)\n\nprint(\"\\nUnOptimized Model\\n------\")\nprint(\"Final rmse score on the testing data: {:.4f}\".format(mean_squared_error(y_test, predictions)))\n\nprint(\"\\nOptimized Model\\n------\")\nprint(\"Final rmse score on the testing data: {:.4f}\".format(mean_squared_error(y_test, best_predictions)))\n\n\n\n","f7ef21e8":"pred_test = best_clf.predict(test)\nsubmission = pd.read_csv('..\/input\/sample_submission.csv')\nsubmission['SalePrice']=pred_test\nsubmission['Id']=test_df_original['Id']\n\n#converting to csv\n\npd.DataFrame(submission, columns=['Id','SalePrice']).to_csv('randomforesthouse.csv')","c8c4e25b":"** Inference **\nIt seems that both the features have linear relationship with SalesPrice. However there are some outliers in the above 2 graphs . For these points, inspite of having a very high living area and basement area, they have way too less price. \n","c855db2a":"![](http:\/\/)# Applying Box Cot transformation to the skewed features #","c11521d6":"## Building the model","0ab0132b":"> ## Feature Engineering ##\n\n1. Lable encoding categorical variables which are useful for predicting ","f1596d27":"***Exploring relationship of SalePrice with categorical variables***\n\nIn the set of most important features, although OverallQuality and Yearbuit are integers, but to understand them better, we will treat them as categorical variables, \n","acca4d2f":"## Evaluating least correlated features with SalePrice ##\n","f15bfef3":"## Finding ","bf66d669":"## Checking Skewness ##\n\nFrom our initial analysis, we found that SalePrice is right skewed due to presence of outliers. We will confirm our finding now mathematically. \n","3039ed14":"## Inferences ##\n\n1. PoolQC : Now  most of the houses do not have a pool. So the reson for huge amount of missing data is not due to missing records.  And the houses that have a pool whill most probably form an outlier. So we will fill missing values with None. \n\n2. MiscFeature : Not sure what this is. But I guess it is something which makes a house outlier. So it should be filled with None. \n\n3. Alley : It should also be filled with None. \n\n4. FireplaceQual and Fence : They should also be filled with None. \n\n5.  Garage : All the garage variables have same set of missing data. The most probable reason could be that the house does not          have any garage. And thus it does not have data for garage features. So we will fill the  missing values as None. \n   \n  So - GarageType, GarageFinish, GarageQual and GarageCond  : fill None \n         GarageYrBlt, GarageArea   = 0\n\n6.  Basement -  We can draw the same coclusion as of Garage. \n\n7. MSZoning - It does not have any missing value. \n\n8. KitchenQual, Electrical - We set mode as there's only 1 misisng value. ","36df9c57":"* Now we will see the normally distributes Sales Price Grapg","94c7cdcd":"## Submitting the prediction","6f0faaa5":"!**##  Splitting trainig data into train and split**","78a6ef28":"## Feature Transformation ##\n","a37b097e":"## Inferences ##\n\n1. Cars - We can see that with increase in no of cars, Sales Price increases upto 3. But for 4 cars, it drops significantly. Maybe not    many people have that many cars.\n2. ","9c5cd606":"## Splitting traiing and testing features","d0e585af":"*** Combining trainning + testing data**","ec549075":"***Exploring relationship of SalePrice with numerical variables***","fc0eb624":"** Plotting Neighbourhood **","55e9a3b6":"## To see categorical data ##","4d562fc7":"**# Data Exploration\n\nFirst we will start by analyzing target variable - SalesPrice \n","b6387adf":"***Getting dummy categorical variables**","d023d152":"## Missing Data##","09e0b82c":"## Analyzing SalePrice (target) variable with correlated features","e153451a":"As Mean > Median , this implies there is a skweness in the data and thus it is not a normal distribution. \n\n**Factors that can affect Normal House Price**\n1. Neighborhood\n2. YearBuilt\n3. OverallQuality\n4. BasementQuality\n5. GrLivArea\n\nAs per me these 5 should be the most important features in influencing the price of the house. ","6ecc4e69":"** Inferences **\n\n1. For these features there is strong collinearity. In fact they give almost the same information. \n         YearBuilt  & Garage Year Built(GarageYrBuilt) -(we will keep'YearBuilt')\n         GarageArea and GarageCars - (we will keep 'GarageCars' as its correlation with 'SalePrice' is higher)\n          TotalBsmtSF and 1stFlrSF - (As both have same correlation with 'SalePrice' - 1stFlrSf)\n         TotRmsAbvGrd(Total rooms above ground) and GrLivArea - ( 'GrLivArea' hias higher correlation with 'SalePrice')  \n         \n         \n         \n 2. Overall Quality , GrLivArea and GarageArea are highly correlated to Sales price. \n \n 3. The other features well correlated to SalesPrice are - TotalBsmtSF, 1stFlrSF, Full Bath","99356b35":"## Making new features ##\n\nThere are some features which we can combine to form a single feature. Like we can form a feature Total Area of the house "}}