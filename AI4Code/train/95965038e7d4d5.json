{"cell_type":{"f64c4ed1":"code","a3f22f8d":"code","695327eb":"code","37648fb3":"code","06b87210":"code","e959b14c":"code","bfadff03":"code","ebff3ff5":"code","649f9e5e":"code","71f1f241":"code","b38793ca":"code","e2a4ae09":"code","37ac422b":"code","b84955f7":"code","d7bd7f61":"code","ac9de018":"code","38a6f869":"code","4d9971b8":"code","b395f2f3":"code","1c88ac72":"code","aa9be858":"code","3dc2ec40":"code","98c96faa":"markdown","9bfc2903":"markdown","eb91d48e":"markdown","b5c1f1b1":"markdown","a95f2988":"markdown","53bd0247":"markdown","ffe93a81":"markdown","c8c49411":"markdown","7987cf06":"markdown","057343ff":"markdown","4dc2aa4f":"markdown","b61ca086":"markdown","6a556e73":"markdown","419f5721":"markdown","f80a5d89":"markdown","0247d119":"markdown","259422d9":"markdown","c32b74e8":"markdown"},"source":{"f64c4ed1":"# Import models \nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import r2_score, mean_squared_error\nimport matplotlib.pyplot as plt\nimport pandas as  pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\n\ndf = pd.read_csv(\"..\/input\/us-accidents\/US_Accidents_June20.csv\")\ndf.head()","a3f22f8d":"# With the Weather_Condition column\ndf2 = df[[\"Distance(mi)\", \n          \"Temperature(F)\", \n          \"Wind_Chill(F)\", \n          \"Humidity(%)\", \n          \"Pressure(in)\", \n          \"Visibility(mi)\", \n          \"Precipitation(in)\", \n          \"Weather_Condition\",\n          \"Severity\"]]\n\n# Without the Weather_Condition column\ndf1 = df[[\"Distance(mi)\",  \n          \"Temperature(F)\", \n          \"Wind_Chill(F)\", \n          \"Humidity(%)\", \n          \"Pressure(in)\", \n          \"Visibility(mi)\", \n          \"Precipitation(in)\",\n          \"Severity\"\n          ]]\n\ndf1.replace(-1, np.nan, inplace=True)  \ndf1 = df1.dropna()\n\ndf2.replace(-1, np.nan, inplace=True)  \ndf2 = df2.dropna()\n\n# Keep 30000 to decrease running times\ndf1 = df1[:100000] \ndf2 = df2[:100000] \n\n\nY1 = df1.Severity.values\nX1 = df1.loc[:, df1.columns != 'Severity']","695327eb":"print(Y1.shape)\nprint(X1.shape)","37648fb3":"X1.head()","06b87210":"\nsvc_c = [1,10,100]\nknn_k = [1,2,3,4,5,6,7,8,9,10]\nlogreg_c = [.001, .01, .1, 1, 10, 100, 1000]\npercep_early = [True, False]\npercep_valid = [0.1, 0.2]\ndt = [1,2,3,4,5,6,7,8,9,10]\nrf = [1,2,4,8,16,32]\n","e959b14c":"X_train1, X_test1,Y_train1,Y_test1 = train_test_split(X1, Y1, test_size=0.33, random_state=99)\n#Without weather\n\nfor i in range(len(svc_c)):\n    svc = SVC(C = svc_c[i])\n    svc.fit(X_train1, Y_train1)\n    Y_pred = svc.predict(X_test1)\n    acc_svc1 = round(svc.score(X_test1, Y_test1) * 100, 2)\n    print(\"Accuracy SVC for C = \" + str(svc_c[i]) + \": \" + str(acc_svc1))\n\nfor i in range(len(knn_k)):\n    knn = KNeighborsClassifier(n_neighbors = knn_k[i])\n    knn.fit(X_train1, Y_train1)\n    Y_pred = knn.predict(X_test1)\n    acc_knn1 = round(knn.score(X_test1, Y_test1) * 100, 2)\n    print(\"Accuracy KNN for k = \" + str(knn_k[i]) + \": \" + str(acc_knn1))\n\n\n# Logistic Regression\nfor i in range(len(logreg_c)):\n    logreg = LogisticRegression(C = logreg_c[i], max_iter = 2000)\n    logreg.fit(X_train1, Y_train1)\n    Y_pred = logreg.predict(X_test1)\n    acc_log1 = round(logreg.score(X_train1, Y_train1) * 100, 2)\n    print(\"Accuracy Log for C = \" + str(logreg_c[i]) + \": \" + str(acc_log1))\n\n\n# Gaussian Naive Bayes\n\ngaussian = GaussianNB()\ngaussian.fit(X_train1, Y_train1)\nY_pred = gaussian.predict(X_test1)\nacc_gaussian1 = round(gaussian.score(X_test1, Y_test1) * 100, 2)\nprint(\"Accuracy Gaussian: \", acc_gaussian1)\n\n# Perceptron\nfor i in range(len(percep_valid)):\n    perceptron = Perceptron(max_iter = 2000, early_stopping = percep_early[i], validation_fraction = percep_valid[i])\n    perceptron.fit(X_train1, Y_train1)\n    Y_pred = perceptron.predict(X_test1)\n    acc_perceptron1 = round(perceptron.score(X_test1, Y_test1) * 100, 2)\n    print(\"Accuracy Perceptron for early_stopping = \" + str(percep_early[i]) + \" and  validation_fraction = \" + str(percep_valid[i]) + \": \" +  str(acc_perceptron1))\n\n# Linear SVC\n\n#linear_svc = LinearSVC(max_iter = 10000)\n#linear_svc.fit(X_train1, Y_train1)\n#Y_pred = linear_svc.predict(X_test1)\n#acc_linear_svc1 = round(linear_svc.score(X_test1, Y_test1) * 100, 2)\n#print(\"Accuracy Linear SVC: \", acc_linear_svc1)\n\n# Stochastic Gradient Descent\nfor i in range(len(percep_valid)):\n    sgd = SGDClassifier(max_iter = 2000, early_stopping = percep_early[i], validation_fraction = percep_valid[i])\n    sgd.fit(X_train1, Y_train1)\n    Y_pred = sgd.predict(X_test1)\n    acc_sgd1 = round(sgd.score(X_test1, Y_test1) * 100, 2)\n    print(\"Accuracy SGD for early_stopping = \" + str(percep_early[i]) + \" and  validation_fraction = \" + str(percep_valid[i]) + \" : \" + str(acc_sgd1))\n\n# Decision Tree\nfor i in range(len(dt)):\n    decision_tree = DecisionTreeClassifier(max_depth = dt[i])\n    decision_tree.fit(X_train1, Y_train1)\n    Y_pred = decision_tree.predict(X_test1)\n    acc_decision_tree1 = round(decision_tree.score(X_test1, Y_test1) * 100, 2)\n    print(\"Accuracy Decision Tree for max_depth = \" + str(dt[i]) + \": \" + str(acc_decision_tree1))\n\n# Random Forest\nfor i in range(len(rf)):\n    random_forest = RandomForestClassifier(n_estimators= rf[i])\n    random_forest.fit(X_train1, Y_train1)\n    Y_pred = random_forest.predict(X_test1)\n    random_forest.score(X_train1, Y_train1)\n    acc_random_forest1 = round(random_forest.score(X_test1, Y_test1) * 100, 2)\n    print(\"Accuracy Random Forest for n_estimators = \" +  str(rf[i]) + \": \" + str(acc_random_forest1))","bfadff03":"df2[\"Weather_Condition\"].unique()","ebff3ff5":"# One-hot encoding\nencoded_cons = []\nfor con in df2[\"Weather_Condition\"].values:\n    if \"Rain\" in con.split(\" \"):\n        encoded_cons.append(1)\n    elif \"Snow\" in con.split(\" \"):\n        encoded_cons.append(2)\n    elif \"Fog\" in con.split(\" \"):\n        encoded_cons.append(3)\n    else:\n        encoded_cons.append(4)\n\n# New column and delete the original Weather_Condition column\ndf2['Encoded_Weather'] = encoded_cons\ndel df2[\"Weather_Condition\"]\n\ndf2","649f9e5e":"Y = df2.Severity.values\nX = df2.loc[:, df2.columns != 'Severity']","71f1f241":"X.head()","b38793ca":"print(Y.shape)\nprint(X.shape)","e2a4ae09":"X_train2, X_test2, Y_train2, Y_test2 = train_test_split(X, Y, test_size=0.33, random_state=99)\n#With weather\n\nfor i in range(len(svc_c)):\n    svc = SVC(C = svc_c[i])\n    svc.fit(X_train2, Y_train2)\n    Y_pred = svc.predict(X_test2)\n    acc_svc2 = round(svc.score(X_test2, Y_test2) * 100, 2)\n    print(\"Accuracy SVC for C = \" + str(svc_c[i]) + \": \" + str(acc_svc2))\n    #print(\"Improvement: \", acc_svc2 > acc_svc1)\n    \n\n\n\nfor i in range(len(knn_k)):\n    knn = KNeighborsClassifier(n_neighbors = knn_k[i])\n    knn.fit(X_train2, Y_train2)\n    Y_pred = knn.predict(X_test2)\n    acc_knn2 = round(knn.score(X_test2, Y_test2) * 100, 2)\n    print(\"Accuracy KNN for k = \" + str(knn_k[i]) + \": \" + str(acc_knn2))\n\n\n\n# Logistic Regression\nfor i in range(len(logreg_c)):\n    logreg = LogisticRegression(C = logreg_c[i], max_iter = 2000)\n    logreg.fit(X_train2, Y_train2)\n    Y_pred = logreg.predict(X_test2)\n    acc_log2 = round(logreg.score(X_train2, Y_train2) * 100, 2)\n    print(\"Accuracy Log for C = \" + str(logreg_c[i]) + \": \" + str(acc_log2))\n\n\n\n\n# Gaussian Naive Bayes\n\ngaussian = GaussianNB()\ngaussian.fit(X_train2, Y_train2)\nY_pred = gaussian.predict(X_test2)\nacc_gaussian2 = round(gaussian.score(X_test2, Y_test2) * 100, 2)\nprint(\"Accuracy Gaussian: \", acc_gaussian2)\n\n\n# Perceptron\n\nfor i in range(len(percep_valid)):\n    perceptron = Perceptron(max_iter = 2000, early_stopping = percep_early[i], validation_fraction = percep_valid[i])\n    perceptron.fit(X_train2, Y_train2)\n    Y_pred = perceptron.predict(X_test2)\n    acc_perceptron2 = round(perceptron.score(X_test2, Y_test2) * 100, 2)\n    print(\"Accuracy Perceptron for early_stopping = \" + str(percep_early[i]) + \" and  validation_fraction = \" + str(percep_valid[i]) + \": \" +  str(acc_perceptron2))\n\n\n\n# Linear SVC\n\n#linear_svc = LinearSVC(max_iter = 10000)\n#linear_svc.fit(X_train1, Y_train1)\n#Y_pred = linear_svc.predict(X_test1)\n#acc_linear_svc1 = round(linear_svc.score(X_test1, Y_test1) * 100, 2)\n#print(\"Accuracy Linear SVC: \", acc_linear_svc1)\n\n# Stochastic Gradient Descent\n\nfor i in range(len(percep_valid)):\n    sgd = SGDClassifier(max_iter = 2000, early_stopping = percep_early[i], validation_fraction = percep_valid[i])\n    sgd.fit(X_train2, Y_train2)\n    Y_pred = sgd.predict(X_test2)\n    acc_sgd2 = round(sgd.score(X_test2, Y_test2) * 100, 2)\n    print(\"Accuracy SGD for early_stopping = \" + str(percep_early[i]) + \" and  validation_fraction = \" + str(percep_valid[i]) + \" : \" + str(acc_sgd2))\n\n\n\n\n# Decision Tree\n\nfor i in range(len(dt)):\n    decision_tree = DecisionTreeClassifier(max_depth = dt[i])\n    decision_tree.fit(X_train2, Y_train2)\n    Y_pred = decision_tree.predict(X_test2)\n    acc_decision_tree2 = round(decision_tree.score(X_test2, Y_test2) * 100, 2)\n    print(\"Accuracy Decision Tree for max_depth = \" + str(dt[i]) + \": \" + str(acc_decision_tree2))\n\n\n\n\n# Random Forest\n\nfor i in range(len(rf)):\n    random_forest = RandomForestClassifier(n_estimators= rf[i])\n    random_forest.fit(X_train2, Y_train2)\n    Y_pred = random_forest.predict(X_test2)\n    random_forest.score(X_train2, Y_train2)\n    acc_random_forest2 = round(random_forest.score(X_test2, Y_test2) * 100, 2)\n    print(\"Accuracy Random Forest for n_estimators = \" +  str(rf[i]) + \": \" + str(acc_random_forest2))\n\n","37ac422b":"from sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn\n\ndef plot_confusion_matrix(data, labels):\n    \"\"\"Plot confusion matrix using heatmap.\n \n    Args:\n        data (list of list): List of lists with confusion matrix data.\n        labels (list): Labels which will be plotted across x and y axis.\n        output_filename (str): Path to output file.\n \n    \"\"\"\n    seaborn.set(color_codes=True)\n    plt.figure(1, figsize=(9, 6))\n \n    plt.title(\"Confusion Matrix\")\n \n    seaborn.set(font_scale=1.4)\n    ax = seaborn.heatmap(data, annot=True, cmap=\"YlGnBu\", cbar_kws={'label': 'Scale'})\n \n    ax.set_xticklabels(labels)\n    ax.set_yticklabels(labels)\n \n    ax.set(ylabel=\"True Label\", xlabel=\"Predicted Label\")\n \n#     plt.savefig(output_filename, bbox_inches='tight', dpi=300)\n    plt.show()\n\ncm = confusion_matrix(Y_pred, Y_test2) \nlabels = [1,2,3,4]\n    \n# SVC\nplot_confusion_matrix(cm, labels)  # doctest: +SKIP","b84955f7":"# With State column\ndfState = df[[\"Distance(mi)\", \n          \"Temperature(F)\", \n          \"Wind_Chill(F)\", \n          \"Humidity(%)\", \n          \"Pressure(in)\", \n          \"Visibility(mi)\", \n          \"Precipitation(in)\", \n          \"State\",\n          \"Severity\"]]\n\n# Without State column\ndfNoState = df[[\"Distance(mi)\",  \n          \"Temperature(F)\", \n          \"Wind_Chill(F)\", \n          \"Humidity(%)\", \n          \"Pressure(in)\", \n          \"Visibility(mi)\", \n          \"Precipitation(in)\",\n          \"Severity\"\n          ]]\n\ndfState.replace(-1, np.nan, inplace=True)  \ndfState = dfState.dropna()\n\ndfNoState.replace(-1, np.nan, inplace=True)  \ndfNoState = dfNoState.dropna()\n\n# Keep 30000 to decrease running times\ndfState = dfState[:100000] \ndfNoState = dfNoState[:100000] \n\nYNoState = dfNoState.Severity.values\nXNoState = dfNoState.loc[:, dfNoState.columns != 'Severity']","d7bd7f61":"# One-hot encoding\nencoded_states = []\nfor states in dfState[\"State\"].values:\n    if \"PA\" in states.split(\" \"):\n        encoded_states.append(1)\n    elif \"CA\" in states.split(\" \"):\n        encoded_states.append(2)\n    elif \"NY\" in states.split(\" \"):\n        encoded_states.append(3)\n    else:\n        encoded_states.append(4)\n\n# New column and delete the original Weather_Condition column\ndfState['Encoded_States'] = encoded_states\ndel dfState[\"State\"]\n\n\nYState = dfState.Severity.values\nXState = dfState.loc[:, dfState.columns != 'Severity']","ac9de018":"X_trainState, X_testState, Y_trainState, Y_testState = train_test_split(XState, YState, test_size=0.33, random_state=99)\n#With state\n\nfor i in range(len(svc_c)):\n    svc = SVC(C = svc_c[i])\n    svc.fit(X_trainState, Y_trainState)\n    Y_pred = svc.predict(X_testState)\n    acc_svcState = round(svc.score(X_testState, Y_testState) * 100, 2)\n    print(\"Accuracy SVC for C = \" + str(svc_c[i]) + \": \" + str(acc_svcState))\n\n\n\nfor i in range(len(knn_k)):\n    knn = KNeighborsClassifier(n_neighbors = knn_k[i])\n    knn.fit(X_trainState, Y_trainState)\n    Y_pred = knn.predict(X_testState)\n    acc_knnState = round(knn.score(X_testState, Y_testState) * 100, 2)\n    print(\"Accuracy KNN for k = \" + str(knn_k[i]) + \": \" + str(acc_knnState))\n\n\n\n\n# Logistic Regression\nfor i in range(len(logreg_c)):\n    logreg = LogisticRegression(C = logreg_c[i], max_iter = 2000)\n    logreg.fit(X_trainState, Y_trainState)\n    Y_pred = logreg.predict(X_testState)\n    acc_logState = round(logreg.score(X_trainState, Y_trainState) * 100, 2)\n    print(\"Accuracy Log for C = \" + str(logreg_c[i]) + \": \" + str(acc_logState))\n\n\n\n\n# Gaussian Naive Bayes\n\ngaussian = GaussianNB()\ngaussian.fit(X_trainState, Y_trainState)\nY_pred = gaussian.predict(X_testState)\nacc_gaussianState = round(gaussian.score(X_testState, Y_testState) * 100, 2)\nprint(\"Accuracy Gaussian: \", acc_gaussianState)\nprint(\"Improvement: \", acc_gaussianState > acc_gaussianNoState)\n\n# Perceptron\n\nfor i in range(len(percep_valid)):\n    perceptron = Perceptron(max_iter = 2000, early_stopping = percep_early[i], validation_fraction = percep_valid[i])\n    perceptron.fit(X_trainState, Y_trainState)\n    Y_pred = perceptron.predict(X_testState)\n    acc_perceptronState = round(perceptron.score(X_testState, Y_testState) * 100, 2)\n    print(\"Accuracy Perceptron for early_stopping = \" + str(percep_early[i]) + \" and  validation_fraction = \" + str(percep_valid[i]) + \": \" +  str(acc_perceptronState))\n\n\n\n# Linear SVC\n\n#linear_svc = LinearSVC(max_iter = 10000)\n#linear_svc.fit(X_train1, Y_train1)\n#Y_pred = linear_svc.predict(X_test1)\n#acc_linear_svc1 = round(linear_svc.score(X_test1, Y_test1) * 100, 2)\n#print(\"Accuracy Linear SVC: \", acc_linear_svc1)\n\n# Stochastic Gradient Descent\n\nfor i in range(len(percep_valid)):\n    sgd = SGDClassifier(max_iter = 2000, early_stopping = percep_early[i], validation_fraction = percep_valid[i])\n    sgd.fit(X_trainState, Y_trainState)\n    Y_pred = sgd.predict(X_testState)\n    acc_sgdState = round(sgd.score(X_testState, Y_testState) * 100, 2)\n    print(\"Accuracy SGD for early_stopping = \" + str(percep_early[i]) + \" and  validation_fraction = \" + str(percep_valid[i]) + \" : \" + str(acc_sgdState))\n\n\n\n\n# Decision Tree\n\nfor i in range(len(dt)):\n    decision_tree = DecisionTreeClassifier(max_depth = dt[i])\n    decision_tree.fit(X_trainState, Y_trainState)\n    Y_pred = decision_tree.predict(X_testState)\n    acc_decision_treeState = round(decision_tree.score(X_testState, Y_testState) * 100, 2)\n    print(\"Accuracy Decision Tree for max_depth = \" + str(dt[i]) + \": \" + str(acc_decision_treeState))\n\n\n\n# Random Forest\n\nfor i in range(len(rf)):\n    random_forest = RandomForestClassifier(n_estimators= rf[i])\n    random_forest.fit(X_trainState, Y_trainState)\n    Y_pred = random_forest.predict(X_testState)\n    random_forest.score(X_trainState, Y_trainState)\n    acc_random_forestState = round(random_forest.score(X_testState, Y_testState) * 100, 2)\n    print(\"Accuracy Random Forest for n_estimators = \" +  str(rf[i]) + \": \" + str(acc_random_forestState))\n\n","38a6f869":"import datetime\n\n\ndf[\"Year\"] = pd.DatetimeIndex(df[\"Start_Time\"]).year\n\n#With Year\ndfYear = df[[\"Distance(mi)\", \n          \"Temperature(F)\", \n          \"Wind_Chill(F)\", \n          \"Humidity(%)\", \n          \"Pressure(in)\", \n          \"Visibility(mi)\", \n          \"Precipitation(in)\", \n          \"Year\",\n          \"Severity\"]]\n\n# Without Year column\ndfNoYear = df[[\"Distance(mi)\",  \n          \"Temperature(F)\", \n          \"Wind_Chill(F)\", \n          \"Humidity(%)\", \n          \"Pressure(in)\", \n          \"Visibility(mi)\", \n          \"Precipitation(in)\",\n          \"Severity\"\n          ]]\n\ndfYear.replace(-1, np.nan, inplace=True)  \ndfYear = dfYear.dropna()\n\ndfNoYear.replace(-1, np.nan, inplace=True)  \ndfNoYear = dfNoYear.dropna()\n\n# Keep 30000 to decrease running times\ndfYear = dfYear[:100000] \ndfNoYear = dfNoYear[:100000] \n\nYNoYear = dfNoYear.Severity.values\nXNoYear = dfNoYear.loc[:, dfNoYear.columns != 'Severity']","4d9971b8":"YYear = dfYear.Severity.values\nXYear = dfYear.loc[:, dfYear.columns != 'Severity']","b395f2f3":"X_trainYear, X_testYear, Y_trainYear, Y_testYear = train_test_split(XYear, YYear, test_size=0.33, random_state=99)\n#With state\n\nfor i in range(len(svc_c)):\n    svc = SVC(C = svc_c[i])\n    svc.fit(X_trainYear, Y_trainYear)\n    Y_pred = svc.predict(X_testYear)\n    acc_svcYear = round(svc.score(X_testYear, Y_testYear) * 100, 2)\n    print(\"Accuracy SVC for C = \" + str(svc_c[i]) + \": \" + str(acc_svcYear))\n\n\n\n\nfor i in range(len(knn_k)):\n    knn = KNeighborsClassifier(n_neighbors = knn_k[i])\n    knn.fit(X_trainYear, Y_trainYear)\n    Y_pred = knn.predict(X_testYear)\n    acc_knnYear = round(knn.score(X_testYear, Y_testYear) * 100, 2)\n    print(\"Accuracy KNN for k = \" + str(knn_k[i]) + \": \" + str(acc_knnYear))\n\n\n\n\n# Logistic Regression\nfor i in range(len(logreg_c)):\n    logreg = LogisticRegression(C = logreg_c[i], max_iter = 2000)\n    logreg.fit(X_trainYear, Y_trainYear)\n    Y_pred = logreg.predict(X_testYear)\n    acc_logYear = round(logreg.score(X_trainYear, Y_trainYear) * 100, 2)\n    print(\"Accuracy Log for C = \" + str(logreg_c[i]) + \": \" + str(acc_logYear))\n\n\n\n\n# Gaussian Naive Bayes\n\ngaussian = GaussianNB()\ngaussian.fit(X_trainYear, Y_trainYear)\nY_pred = gaussian.predict(X_testYear)\nacc_gaussianYear = round(gaussian.score(X_testYear, Y_testYear) * 100, 2)\nprint(\"Accuracy Gaussian: \", acc_gaussianYear)\nprint(\"Improvement: \", acc_gaussianYear > acc_gaussianNoYear)\n\n# Perceptron\n\nfor i in range(len(percep_valid)):\n    perceptron = Perceptron(max_iter = 2000, early_stopping = percep_early[i], validation_fraction = percep_valid[i])\n    perceptron.fit(X_trainYear, Y_trainYear)\n    Y_pred = perceptron.predict(X_testYear)\n    acc_perceptronYear = round(perceptron.score(X_testYear, Y_testYear) * 100, 2)\n    print(\"Accuracy Perceptron for early_stopping = \" + str(percep_early[i]) + \" and  validation_fraction = \" + str(percep_valid[i]) + \": \" +  str(acc_perceptronYear))\n\n\n\n# Linear SVC\n\n#linear_svc = LinearSVC(max_iter = 10000)\n#linear_svc.fit(X_train1, Y_train1)\n#Y_pred = linear_svc.predict(X_test1)\n#acc_linear_svc1 = round(linear_svc.score(X_test1, Y_test1) * 100, 2)\n#print(\"Accuracy Linear SVC: \", acc_linear_svc1)\n\n# Stochastic Gradient Descent\n\nfor i in range(len(percep_valid)):\n    sgd = SGDClassifier(max_iter = 2000, early_stopping = percep_early[i], validation_fraction = percep_valid[i])\n    sgd.fit(X_trainYear, Y_trainYear)\n    Y_pred = sgd.predict(X_testYear)\n    acc_sgdYear = round(sgd.score(X_testYear, Y_testYear) * 100, 2)\n    print(\"Accuracy SGD for early_stopping = \" + str(percep_early[i]) + \" and  validation_fraction = \" + str(percep_valid[i]) + \" : \" + str(acc_sgdYear))\n\n\n\n\n# Decision Tree\n\nfor i in range(len(dt)):\n    decision_tree = DecisionTreeClassifier(max_depth = dt[i])\n    decision_tree.fit(X_trainYear, Y_trainYear)\n    Y_pred = decision_tree.predict(X_testYear)\n    acc_decision_treeYear = round(decision_tree.score(X_testYear, Y_testYear) * 100, 2)\n    print(\"Accuracy Decision Tree for max_depth = \" + str(dt[i]) + \": \" + str(acc_decision_treeYear))\n\n\n\n\n# Random Forest\n\nfor i in range(len(rf)):\n    random_forest = RandomForestClassifier(n_estimators= rf[i])\n    random_forest.fit(X_trainYear, Y_trainYear)\n    Y_pred = random_forest.predict(X_testYear)\n    random_forest.score(X_trainYear, Y_trainYear)\n    acc_random_forestYear = round(random_forest.score(X_testYear, Y_testYear) * 100, 2)\n    print(\"Accuracy Random Forest for n_estimators = \" +  str(rf[i]) + \": \" + str(acc_random_forestYear))\n\n","1c88ac72":"df[\"Hour\"] = pd.DatetimeIndex(df[\"Start_Time\"]).hour\n\n#With Year\ndfHour = df[[\"Distance(mi)\", \n          \"Temperature(F)\", \n          \"Wind_Chill(F)\", \n          \"Humidity(%)\", \n          \"Pressure(in)\", \n          \"Visibility(mi)\", \n          \"Precipitation(in)\", \n          \"Hour\",\n          \"Severity\"]]\n\n# Without Year column\ndfNoHour = df[[\"Distance(mi)\",  \n          \"Temperature(F)\", \n          \"Wind_Chill(F)\", \n          \"Humidity(%)\", \n          \"Pressure(in)\", \n          \"Visibility(mi)\", \n          \"Precipitation(in)\",\n          \"Severity\"\n          ]]\n\ndfHour.replace(-1, np.nan, inplace=True)  \ndfHour = dfHour.dropna()\n\ndfNoHour.replace(-1, np.nan, inplace=True)  \ndfNoHour = dfNoHour.dropna()\n\n# Keep 30000 to decrease running times\ndfHour = dfHour[:100000] \ndfNoHour = dfNoHour[:100000] \n\nYNoHour = dfNoHour.Severity.values\nXNoHour = dfNoHour.loc[:, dfNoHour.columns != 'Severity']","aa9be858":"YHour = dfHour.Severity.values\nXHour = dfHour.loc[:, dfHour.columns != 'Severity']","3dc2ec40":"X_trainHour, X_testHour, Y_trainHour, Y_testHour = train_test_split(XHour, YHour, test_size=0.33, random_state=99)\n#With state\n\n'''\nfor i in range(len(svc_c)):\n    svc = SVC(C = svc_c[i])\n    svc.fit(X_trainHour, Y_trainHour)\n    Y_pred = svc.predict(X_testHour)\n    acc_svcHour = round(svc.score(X_testHour, Y_testHour) * 100, 2)\n    print(\"Accuracy SVC for C = \" + str(svc_c[i]) + \": \" + str(acc_svcHour))\n\n\n\n\nfor i in range(len(knn_k)):\n    knn = KNeighborsClassifier(n_neighbors = knn_k[i])\n    knn.fit(X_trainHour, Y_trainHour)\n    Y_pred = knn.predict(X_testHour)\n    acc_knnHour = round(knn.score(X_testHour, Y_testHour) * 100, 2)\n    print(\"Accuracy KNN for k = \" + str(knn_k[i]) + \": \" + str(acc_knnHour))\n\n\n\n# Logistic Regression\nfor i in range(len(logreg_c)):\n    logreg = LogisticRegression(C = logreg_c[i], max_iter = 2000)\n    logreg.fit(X_trainHour, Y_trainHour)\n    Y_pred = logreg.predict(X_testHour)\n    acc_logHour = round(logreg.score(X_trainHour, Y_trainHour) * 100, 2)\n    print(\"Accuracy Log for C = \" + str(logreg_c[i]) + \": \" + str(acc_logHour))\n\n\n\n\n# Gaussian Naive Bayes\n\ngaussian = GaussianNB()\ngaussian.fit(X_trainHour, Y_trainHour)\nY_pred = gaussian.predict(X_testHour)\nacc_gaussianHour = round(gaussian.score(X_testHour, Y_testHour) * 100, 2)\nprint(\"Accuracy Gaussian: \", acc_gaussianHour)\n\n'''\n\n# Perceptron\n\nfor i in range(len(percep_valid)):\n    perceptron = Perceptron(max_iter = 2000, early_stopping = percep_early[i], validation_fraction = percep_valid[i])\n    perceptron.fit(X_trainHour, Y_trainHour)\n    Y_pred = perceptron.predict(X_testHour)\n    acc_perceptronHour = round(perceptron.score(X_testHour, Y_testHour) * 100, 2)\n    print(\"Accuracy Perceptron for early_stopping = \" + str(percep_early[i]) + \" and  validation_fraction = \" + str(percep_valid[i]) + \": \" +  str(acc_perceptronHour))\n\n\n\n# Linear SVC\n\n#linear_svc = LinearSVC(max_iter = 10000)\n#linear_svc.fit(X_train1, Y_train1)\n#Y_pred = linear_svc.predict(X_test1)\n#acc_linear_svc1 = round(linear_svc.score(X_test1, Y_test1) * 100, 2)\n#print(\"Accuracy Linear SVC: \", acc_linear_svc1)\n\n# Stochastic Gradient Descent\n\nfor i in range(len(percep_valid)):\n    sgd = SGDClassifier(max_iter = 2000, early_stopping = percep_early[i], validation_fraction = percep_valid[i])\n    sgd.fit(X_trainHour, Y_trainHour)\n    Y_pred = sgd.predict(X_testHour)\n    acc_sgdHour = round(sgd.score(X_testHour, Y_testHour) * 100, 2)\n    print(\"Accuracy SGD for early_stopping = \" + str(percep_early[i]) + \" and  validation_fraction = \" + str(percep_valid[i]) + \" : \" + str(acc_sgdHour))\n\n\n\n\n# Decision Tree\n\nfor i in range(len(dt)):\n    decision_tree = DecisionTreeClassifier(max_depth = dt[i])\n    decision_tree.fit(X_trainHour, Y_trainHour)\n    Y_pred = decision_tree.predict(X_testHour)\n    acc_decision_treeHour = round(decision_tree.score(X_testHour, Y_testHour) * 100, 2)\n    print(\"Accuracy Decision Tree for max_depth = \" + str(dt[i]) + \": \" + str(acc_decision_treeHour))\n\n\n\n# Random Forest\n\nfor i in range(len(rf)):\n    random_forest = RandomForestClassifier(n_estimators= rf[i])\n    random_forest.fit(X_trainHour, Y_trainHour)\n    Y_pred = random_forest.predict(X_testHour)\n    random_forest.score(X_trainHour, Y_trainHour)\n    acc_random_forestHour = round(random_forest.score(X_testHour, Y_testHour) * 100, 2)\n    print(\"Accuracy Random Forest for n_estimators = \" +  str(rf[i]) + \": \" + str(acc_random_forestHour))\n","98c96faa":"As there are a variety of different values indicating different weather conditions, for the sake of the main goal of this example, we will simplify it to \"Rain\", \"Snow, \"Fog\", and \"Other\". \"Rain\" will be mapped to 1, \"Snow\" is 2, \"Fog\" is 3, and \"Other\" is 4.","9bfc2903":"The project's mission is to provide assitance for this battle against car accidents with statistics-based findings and data-based analysis. To be more specific, we strive for determining the importance of each attribute toward predicting severity levels of accidents. The process is to create two similar models that predict severity level of available accidents based on a set of attributes. One model will take in all attributes except for the target attribute, for example weather conditions, and the other one will take in every attribute including the target attribute. Two sets of metric scores will be calculated and compared to see if adding the target attribute to the model will improve its performance or hurt it. Additionally, the level of influence of each target attribute will also be evaluated to find out which one plays the most important role and which one plays the least in supporting the performance of the algorithms. The target attributes are:\n\n- Weather Conditions\n- Locations\n- Time of the day\n- Time of the year\n\nFor each attribute, we will create a separate set of models. We will try to implement as many machine learning algorithms as possible. Each of the attribute listed above will be carefully processed and feeded into the models, making sure they retain their full features and hopefully are influential enough to affect the performance of the algorithms for the better or worse. ","eb91d48e":"# Project Requirements\n\nThis final project examines the level of knowledge the students have learned from the course. The following course outcomes will be checked against the content of the report:\n\nUpon successful completion of this course, a student will be able to:\n* Describe the key Python tools and libraries that related to a typical data analytics project. \n* Identify data science libraries, frameworks, modules, and toolkits in Python that efficiently implement the most common data science algorithms and techniques.\n* Apply latest Python techniques in data acquisition, transformation and predictive analytics for data science projects.\n* Discuss the underlying principles and main characteristics of the most common methods and techniques for data analytics. \n* Build data analytic and predictive models for real world data sets using existing Python libraries.\n\n** Marking will be foucsed on both presentation and content.** \n\n## Written Presentation Requirements\nThe report will be judged on the basis of visual appearance, grammatical correctness, and quality of writing, as well as its contents. Please make sure that the text of your report is well-structured, using paragraphs, full sentences, and other features of well-written presentation.\n\n## Technical Content:\n* Is the problem well defined and described thoroughly?\n* Is the size and complexity of the data set used in this project comparable to that of the example data sets used in the lectures and assignments?\n* Did the report describe the charactriatics of the data?\n* Did the report describe the goals of the data analysis?\n* Did the analysis conduct exploratory analyses on the data?\n* Did the analysis build models of the data and evaluated the performance of the models?\n* Overall, what is the rating of this project?","b5c1f1b1":"On average, there are 6 million car accidents in the U.S. every year. That's roughly 16,438 per day. Over 37,000 Americans die in automobile crashes per year, and there is an additional 3 million injured or disabled annually. Economically, traffic accidents cost the country $871 billion a year, and that was 6 years ago. These are only a few quick car crash statistics happening right now in the U.S. Even though the country is standing at 110th on the list of countries with the highest traffic-related death rate, the number can still be lowered tremendously if science-based solutions are carried out in a mission to improve the safety of the people on the roads. With a good dataset, data analysis can be an efficient method to extract useful information in order to figure out the cause and effect rules of the accidents, which will result in improved accident prevention.","a95f2988":"Pay attention that these new models that are trained on weather conditions will be tested with X_test1 and Y_test1 in order to produce an unbiased accuracy score. ","53bd0247":"In this section, we will examine the possibility of achieving the goals mentioned above. We will use 8 different models which are listed below in the code. The goal is to make sure the models work fluently without errors, quickly examine the performance of the models, and find out if the target attribute in this example improves or hurts the accuracy. Weather conditions will be the target attribute for these preliminary models.","ffe93a81":"With a good amount of data and thoroughly executed analytics, one can possibly unveil the many faces of a problem or phenomenon. Data science has been being considered the most direct and reliable way to attack a problem, tracing it to the root and predicting what and when next consequences will take place. This project will follow the same direction and try to solve a specific real-world problem: what can data analytics do to reduce the number of car accidents in the U.S. The analytics will be based on \u201cA Countrywide Traffic Accident Dataset\u201d by Moosavi, Sobhan, Mohammad Hossein Samavatian, Srinivasan Parthasarathy, and Rajiv Ramnath. In this project, we will strive for understanding the cause and effect rules of the accidents, and from that, we will try to build several machine learning models that can help with the future accidents forecasting.","c8c49411":"The results indicated that adding weather-related features to a machine learning algorithm in predicting severity of an accident did not substantially improve the accuracy of models in this PARTICULAR example. However, in order to firmly conclude that whether weather-related features, and other target features (time of the day, time of the year, locations), actually hurt the models or not, we need to take into consideration the possibility of an imbalance dataframe, df1 and df2 in this case. Not all four classes of severity levels are evidently evenly distributed in the training set. Another thing to consider is the size of the training set; 30,000 can possibly be an insufficient number considering we have quite a few attributes feeding into the models. \n\nIn our final report, we will focus more on data preprocessing to create an unbiased experiment as well as increase the training set size to fully utilize the machine learning algorithms. Other metric scores (precision, recall, and F1 score) will also be calculated along side with accuracy. Visualization will be selectively added to plot out the difference between models' performances. Lastly, we will expand the input to consist of more attributes considering the great resource of the original dataset. ","7987cf06":"### 3. Data Sources\n---\n*(Describe the origin of the data sources. What is the format of the original data? How to access the data?)*","057343ff":"### 4. The Goal(s) of the predictions\n---\n*(What are the expected results of the project?)*","4dc2aa4f":"### College of Computing and Informatics, Drexel University\n### INFO 213: Data Science Programming II\n---\n\n## Final Report\n\n## Project Title: Countrywide Car Accidents Analysis and Forecasting\n\n## Student(s): Khanh Tran, Amanjyot Singh\n\n#### Date: August 30, 2020\n---","b61ca086":"### 5. Experimental Models\n---","6a556e73":"### 2. Problem Definition\n---\n*(Define the problem that will be solved in this data analytics project.)*","419f5721":"---\n(*Use the following requirements for writing your reports. DO NOT DELETE THE CELLS BELLOW*)","f80a5d89":"As the dataset was acquired on Kaggle and because of its size, downloading it to local computers will be quite time-consuming. Using Kaggle notebook will solve this problem as we don't have to manually download the dataset to use it. Kaggle allows their users to get access to the datasets available on their website. There are currently about 3.5 million accident records in this dataset. It covers 49 states of the USA, and the data were collected from February 2016 to June 2020, using two APIs that provide streaming traffic incident (or event) data. Along with the large number of records, this dataset also provide a wide range of attributes for each accident. With 49 columns, analysts can observe and discover many faces of the accidents such as starting-ending time, exact starting-ending location, address, weather conditions, existed crossings, junctions, or bumps, etc. Our goals are planned upon this variety of features. We will also make use of pandas, numpy, matplotlib.pyplot, math, and sklearn packages of Python to effectively analyze, visualize, and model our data.\n\nAcknowledgements\n\n- Moosavi, Sobhan, Mohammad Hossein Samavatian, Srinivasan Parthasarathy, and Rajiv Ramnath. \u201cA Countrywide Traffic Accident Dataset.\u201d, 2019.\n\n- Moosavi, Sobhan, Mohammad Hossein Samavatian, Srinivasan Parthasarathy, Radu Teodorescu, and Rajiv Ramnath. \"Accident Risk Prediction based on Heterogeneous Sparse Data: New Dataset and Insights.\" In proceedings of the 27th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems, ACM, 2019.\n\nhttps:\/\/www.kaggle.com\/sobhanmoosavi\/us-accidents","0247d119":"When feeding weather conditions to the models, we have to use a technique call one-hot encoding since this is a categorical variable. In other words, we will map the values of this attribute to numerical values.","259422d9":"### 1. Introduction\n---\n*(Introduce the project and describe the objectives.)* ","c32b74e8":"Below, we have specific values that our classifier paratmeters will change to. Thus, we store our desired values in different lists and loop them."}}