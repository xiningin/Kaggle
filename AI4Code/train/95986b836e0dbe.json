{"cell_type":{"806ff374":"code","acb99776":"code","9b379320":"code","be2bef7c":"code","5c1c64d4":"code","9d658b12":"code","502c0c66":"code","8b9d8b1f":"code","a2af1ded":"code","a9a9701c":"code","8650e337":"code","90369adb":"code","100531f3":"code","4361e889":"code","1df13479":"code","6dabb4e3":"code","a89ff33e":"code","76de7711":"code","0e889171":"code","b24a9ef2":"code","1e72ab2a":"code","4caa2619":"markdown","39c5b7a3":"markdown","73d02a21":"markdown","42c975b3":"markdown","9696be8f":"markdown","d6376cb5":"markdown","58586bdf":"markdown","e91e678f":"markdown","c0e49f98":"markdown","630c1e32":"markdown","23bb38b5":"markdown","d426268d":"markdown","384afa03":"markdown","5e9c1cd6":"markdown","29047093":"markdown"},"source":{"806ff374":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\nfrom sklearn.cluster import KMeans\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","acb99776":"# Load the data\ndata = pd.read_csv('\/kaggle\/input\/iris-dataset.csv')\n# Check the data\ndata.head()","9b379320":"# create a scatter plot based on two corresponding features (sepal_length and sepal_width; OR petal_length and petal_width)\nplt.scatter(data['sepal_length'],data['sepal_width'])\n# name your axes\nplt.xlabel('Lenght of sepal')\nplt.ylabel('Width of sepal')\nplt.show()","be2bef7c":"# create a variable which will contain the data for the clustering\nx = data.copy()\n# create a k-means object with 2 clusters\nkmeans = KMeans(2)\n# fit the data\nkmeans.fit(x)","5c1c64d4":"# create a copy of data, so we can see the clusters next to the original data\nclusters = data.copy()\n# predict the cluster for each observation\nclusters['cluster_pred']=kmeans.fit_predict(x)","9d658b12":"# create a scatter plot based on two corresponding features (sepal_length and sepal_width; OR petal_length and petal_width)\nplt.scatter(clusters['sepal_length'], clusters['sepal_width'], c= clusters ['cluster_pred'], cmap = 'rainbow')\nplt.show()","502c0c66":"# import some preprocessing module\nfrom sklearn import preprocessing\n\n# scale the data for better results\nx_scaled = preprocessing.scale(data)\nx_scaled","8b9d8b1f":"# create a k-means object with 2 clusters\nkmeans_scaled = KMeans(2)\n# fit the data\nkmeans_scaled.fit(x_scaled)","a2af1ded":"# create a copy of data, so we can see the clusters next to the original data\nclusters_scaled = data.copy()\n# predict the cluster for each observation\nclusters_scaled['cluster_pred']=kmeans_scaled.fit_predict(x_scaled)","a9a9701c":"# create a scatter plot based on two corresponding features (sepal_length and sepal_width; OR petal_length and petal_width)\nplt.scatter(clusters_scaled['sepal_length'], clusters_scaled['sepal_width'], c= clusters_scaled ['cluster_pred'], cmap = 'rainbow')\nplt.show()","8650e337":"wcss = []\n# 'cl_num' is a that keeps track the highest number of clusters we want to use the WCSS method for. \n# We have it set at 10 right now, but it is completely arbitrary.\ncl_num = 10\nfor i in range (1,cl_num):\n    kmeans= KMeans(i)\n    kmeans.fit(x_scaled)\n    wcss_iter = kmeans.inertia_\n    wcss.append(wcss_iter)\nwcss","90369adb":"number_clusters = range(1,cl_num)\nplt.plot(number_clusters, wcss)\nplt.title('The Elbow Method')\nplt.xlabel('Number of clusters')\nplt.ylabel('Within-cluster Sum of Squares')\nplt.show()","100531f3":"kmeans_2 = KMeans(2)\nkmeans_2.fit(x_scaled)","4361e889":"# Remember that we are plotting the non-standardized values of the sepal length and width. \nclusters_2 = x.copy()\nclusters_2['cluster_pred']=kmeans_2.fit_predict(x_scaled)","1df13479":"plt.scatter(clusters_2['sepal_length'], clusters_2['sepal_width'], c= clusters_2 ['cluster_pred'], cmap = 'rainbow')","6dabb4e3":"kmeans_3 = KMeans(3)\nkmeans_3.fit(x_scaled)","a89ff33e":"clusters_3 = x.copy()\nclusters_3['cluster_pred']=kmeans_3.fit_predict(x_scaled)","76de7711":"plt.scatter(clusters_3['sepal_length'], clusters_3['sepal_width'], c= clusters_3 ['cluster_pred'], cmap = 'rainbow')","0e889171":"kmeans_5 = KMeans(5)\nkmeans_5.fit(x_scaled)","b24a9ef2":"clusters_5 = x.copy()\nclusters_5['cluster_pred']=kmeans_5.fit_predict(x_scaled)","1e72ab2a":"plt.scatter(clusters_5['sepal_length'], clusters_5['sepal_width'], c= clusters_5 ['cluster_pred'], cmap = 'rainbow')","4caa2619":"#### 5 Clusters","39c5b7a3":"Construct and compare the scatter plots to determine which number of clusters is appropriate for further use in our analysis. Based on the Elbow Curve, 2, 3 or 5 seem the most likely.","73d02a21":"### Clustering (scaled data)","42c975b3":"### Import the relevant libraries","9696be8f":"### Clustering (unscaled data)","d6376cb5":"It seems like 2 or 3-cluster solutions are the best.","58586bdf":"### Elbow Method (WCSS)","e91e678f":"### Load the data","c0e49f98":"#### 2 clusters","630c1e32":"### Plot the data","23bb38b5":"#### 3 Clusters","d426268d":"Looks like the two solutions are identical. That is because the original features have very similar scales to start with!","384afa03":"### Standardize the variables","5e9c1cd6":"The Iris flower dataset is one of the most popular ones for machine learning. You can read a lot about it online and have probably already heard of it: https:\/\/en.wikipedia.org\/wiki\/Iris_flower_data_set\n\nWe didn't want to use it in the lectures, but believe that it would be very interesting for you to try it out (and maybe read about it on your own).\n\nThere are 4 features: sepal length, sepal width, petal length, and petal width.","29047093":"### Understanding the Elbow Curve"}}