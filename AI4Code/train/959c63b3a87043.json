{"cell_type":{"12c1c5d5":"code","b3ec847f":"code","e66053dc":"code","dd2aaa4d":"code","d7567bd0":"code","e98d8cb0":"code","97196f5f":"code","f32d9579":"code","35deabd1":"code","85f8861c":"code","45bbfe8d":"code","e6497c39":"code","91721458":"code","bbb100b8":"code","7c583c81":"code","352902dc":"code","a0c70cf6":"code","d04a1113":"code","44310c7d":"code","15efb7cf":"code","4464a88c":"code","729c594e":"code","0befc5ba":"code","adfc7ccd":"code","b6b1536d":"markdown","c4a9d546":"markdown","2c18c34d":"markdown","85f22915":"markdown","13b0a88b":"markdown"},"source":{"12c1c5d5":"import numpy as np\nimport pandas as pd\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom scipy.stats import beta\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader,TensorDataset\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport pytorch_lightning as pl\nfrom pytorch_lightning.loggers import TensorBoardLogger,CSVLogger\nfrom pytorch_lightning.callbacks.early_stopping import EarlyStopping","b3ec847f":"# needed for deterministic output\npl.seed_everything(2)\n\n# device in which the model will be trained\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","e66053dc":"dataset = pd.read_csv(\"..\/input\/santander-customer-transaction-prediction\/train.csv\")\ndataset","dd2aaa4d":"dataset.info()","d7567bd0":"dataset.groupby(\"target\")[\"ID_code\"].count() \/ len(dataset)","e98d8cb0":"# dataset stratified split: train 60% - valid 20% - test 20%\n\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=2)\nsplit = skf.split(dataset, dataset.target)\n_,valid_index = next(split)\n_,test_index = next(split)\n\ntrain_dset = dataset.drop(valid_index).drop(test_index).reset_index(drop=True)\nvalid_dset = dataset.loc[valid_index].reset_index(drop=True)\ntest_dset = dataset.loc[test_index].reset_index(drop=True)","97196f5f":"display(train_dset.groupby(\"target\")[\"ID_code\"].count() \/ len(train_dset))\ndisplay(valid_dset.groupby(\"target\")[\"ID_code\"].count() \/ len(valid_dset))\ndisplay(test_dset.groupby(\"target\")[\"ID_code\"].count() \/ len(test_dset))","f32d9579":"input_features = dataset.columns[2:].tolist()\ntarget = \"target\"","35deabd1":"# parsing inputs as pytorch tensor dataset\n\ntrain_tensor_dset = TensorDataset(\n    torch.tensor(train_dset[input_features].values, dtype=torch.float),\n    torch.tensor(train_dset[target].values.reshape(-1,1), dtype=torch.float)\n)\n\nvalid_tensor_dset = TensorDataset(\n    torch.tensor(valid_dset[input_features].values, dtype=torch.float),\n    torch.tensor(valid_dset[target].values.reshape(-1,1), dtype=torch.float)\n)\n\ntest_tensor_dset = TensorDataset(\n    torch.tensor(test_dset[input_features].values, dtype=torch.float),\n    torch.tensor(test_dset[target].values.reshape(-1,1), dtype=torch.float) \n)","85f8861c":"class DNN(pl.LightningModule):\n\n    def __init__(self, input_dim, output_dim, nn_depth, nn_width, dropout, momentum):\n        super().__init__()\n\n        self.bn_in = nn.BatchNorm1d(input_dim, momentum=momentum)\n        self.dp_in = nn.Dropout(dropout)\n        self.ln_in = nn.Linear(input_dim, nn_width, bias=False)\n\n        self.bnorms = nn.ModuleList([nn.BatchNorm1d(nn_width, momentum=momentum) for i in range(nn_depth-1)])\n        self.dropouts = nn.ModuleList([nn.Dropout(dropout) for i in range(nn_depth-1)])\n        self.linears = nn.ModuleList([nn.Linear(nn_width, nn_width, bias=False) for i in range(nn_depth-1)])\n        \n        self.bn_out = nn.BatchNorm1d(nn_width, momentum=momentum)\n        self.dp_out = nn.Dropout(dropout\/2)\n        self.ln_out = nn.Linear(nn_width, output_dim, bias=False)\n\n        self.loss = nn.BCEWithLogitsLoss()\n\n    def forward(self, x):\n        x = self.bn_in(x)\n        x = self.dp_in(x)\n        x = nn.functional.relu(self.ln_in(x))\n\n        for bn_layer,dp_layer,ln_layer in zip(self.bnorms,self.dropouts,self.linears):\n            x = bn_layer(x)\n            x = dp_layer(x)\n            x = ln_layer(x)\n            x = nn.functional.relu(x)\n            \n        x = self.bn_out(x)\n        x = self.dp_out(x)\n        x = self.ln_out(x)\n        return x\n\n    def training_step(self, batch, batch_idx):\n        X, y = batch\n        y_hat = self.forward(X)\n        loss = self.loss(y_hat, y)\n        self.log('train_loss', loss)\n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        X, y = batch\n        y_hat = self.forward(X)\n        loss = self.loss(y_hat, y)\n        self.log('valid_loss', loss)\n        \n    def test_step(self, batch, batch_idx):\n        X, y = batch\n        y_logit = self.forward(X)\n        y_probs = torch.sigmoid(y_logit).detach().cpu().numpy()\n        loss = self.loss(y_logit, y)\n        metric = roc_auc_score(y.cpu().numpy(), y_probs)\n        self.log('test_loss', loss)\n        self.log('test_metric', metric)\n        \n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=5e-3, weight_decay=1e-4)\n        scheduler = {\n            'scheduler': ReduceLROnPlateau(\n                optimizer, \n                mode=\"min\", \n                factor=0.5, \n                patience=5, \n                min_lr=1e-5),\n            'interval': 'epoch',\n            'frequency': 1,\n            'reduce_on_plateau': True,\n            'monitor': 'valid_loss',\n        }\n        return [optimizer], [scheduler]","45bbfe8d":"model = DNN(\n    input_dim=len(input_features), \n    output_dim=1, \n    nn_depth=3, \n    nn_width=128, \n    dropout=0.2, \n    momentum=0.1\n)\n\nlogger = logger = CSVLogger(\"logs\", name=\"mlp_wo_mixup\")\n\nearly_stop_callback = EarlyStopping(\n   monitor='valid_loss',\n   min_delta=.0,\n   patience=20,\n   verbose=True,\n   mode='min'\n)\n\ntrainer = pl.Trainer(\n    callbacks=[early_stop_callback], \n    min_epochs=10, \n    max_epochs=200, \n    gpus=0, \n    logger=logger, \n    deterministic=True\n)","e6497c39":"model.summarize()","91721458":"trainer.fit(\n    model, \n    DataLoader(train_tensor_dset, batch_size=1024, shuffle=True, num_workers=4),\n    DataLoader(valid_tensor_dset, batch_size=1024, shuffle=False, num_workers=4)\n)","bbb100b8":"# AUC on validation dataset\ntrainer.test(model, DataLoader(valid_tensor_dset, batch_size=1024, shuffle=False, num_workers=4))","7c583c81":"# AUC on test dataset\ntrainer.test(model, DataLoader(test_tensor_dset, batch_size=1024, shuffle=False, num_workers=4))","352902dc":"metrics = pd.read_csv(\"logs\/mlp_wo_mixup\/version_0\/metrics.csv\")\n\ndf1 = metrics.loc[:,[\"step\",\"train_loss\"]].dropna()\ndf2 = metrics.loc[:,[\"step\",\"valid_loss\"]].dropna()\n\nplt.figure(figsize=(12,5))\nplt.plot(df1.step, df1.train_loss, \"o-\", label=\"train_loss\")\nplt.plot(df2.step, df2.valid_loss, \"o-\", label=\"valid_loss\")\nplt.grid()\nplt.legend(loc=\"best\")\nplt.show()","a0c70cf6":"alpha = 0.25\n\nx = np.linspace(beta.ppf(0.01, alpha, alpha), beta.ppf(0.99, alpha, alpha), 100)\nplt.plot(x, beta.pdf(x, alpha, alpha), 'r-', lw=5, alpha=0.6, label='beta pdf')\nplt.grid()\nplt.show()","d04a1113":"class DNN(pl.LightningModule):\n\n    def __init__(self, input_dim, output_dim, nn_depth, nn_width, dropout, momentum, alpha=0.8):\n        super().__init__()\n        \n        self.alpha = alpha\n        \n        self.bn_in = nn.BatchNorm1d(input_dim, momentum=momentum)\n        self.dp_in = nn.Dropout(dropout)\n        self.ln_in = nn.Linear(input_dim, nn_width, bias=False)\n\n        self.bnorms = nn.ModuleList([nn.BatchNorm1d(nn_width, momentum=momentum) for i in range(nn_depth-1)])\n        self.dropouts = nn.ModuleList([nn.Dropout(dropout) for i in range(nn_depth-1)])\n        self.linears = nn.ModuleList([nn.Linear(nn_width, nn_width, bias=False) for i in range(nn_depth-1)])\n        \n        self.bn_out = nn.BatchNorm1d(nn_width, momentum=momentum)\n        self.dp_out = nn.Dropout(dropout\/2)\n        self.ln_out = nn.Linear(nn_width, output_dim, bias=False)\n\n        self.loss = nn.BCEWithLogitsLoss()\n\n    def forward(self, x):\n        x = self.bn_in(x)\n        x = self.dp_in(x)\n        x = nn.functional.relu(self.ln_in(x))\n\n        for bn_layer,dp_layer,ln_layer in zip(self.bnorms,self.dropouts,self.linears):\n            x = bn_layer(x)\n            x = dp_layer(x)\n            x = ln_layer(x)\n            x = nn.functional.relu(x)\n            \n        x = self.bn_out(x)\n        x = self.dp_out(x)\n        x = self.ln_out(x)\n        return x\n\n    def training_step(self, batch, batch_idx):\n        X, y = batch\n        \n        lam = np.random.beta(self.alpha,self.alpha)\n        lam = torch.FloatTensor([lam]).to(self.device)\n        n = len(batch)\/\/2\n        X = lam * X[:n,:] + (1-lam) * X[n:,:]\n        y = lam * y[:n,:] + (1-lam) * y[n:,:]\n        \n        y_hat = self.forward(X)\n        loss = self.loss(y_hat, y)\n        self.log('train_loss', loss)\n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        X, y = batch\n        y_hat = self.forward(X)\n        loss = self.loss(y_hat, y)\n        self.log('valid_loss', loss)\n        \n    def test_step(self, batch, batch_idx):\n        X, y = batch\n        y_logit = self.forward(X)\n        y_probs = torch.sigmoid(y_logit).detach().cpu().numpy()\n        loss = self.loss(y_logit, y)\n        metric = roc_auc_score(y.cpu().numpy(), y_probs)\n        self.log('test_loss', loss)\n        self.log('test_metric', metric)\n        \n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=5e-3, weight_decay=1e-4)\n        scheduler = {\n            'scheduler': ReduceLROnPlateau(\n                optimizer, \n                mode=\"min\", \n                factor=0.5, \n                patience=5, \n                min_lr=1e-5),\n            'interval': 'epoch',\n            'frequency': 1,\n            'reduce_on_plateau': True,\n            'monitor': 'valid_loss',\n        }\n        return [optimizer], [scheduler]","44310c7d":"model = DNN(\n    input_dim=len(input_features), \n    output_dim=1, \n    nn_depth=3, \n    nn_width=128, \n    dropout=0.2, \n    momentum=0.1,\n    alpha=0.25,\n)\n\nlogger = logger = CSVLogger(\"logs\", name=\"mlp_w_mixup\")\n\nearly_stop_callback = EarlyStopping(\n   monitor='valid_loss',\n   min_delta=.0,\n   patience=20,\n   verbose=True,\n   mode='min'\n)\n\ntrainer = pl.Trainer(\n    callbacks=[early_stop_callback], \n    min_epochs=10, \n    max_epochs=200, \n    gpus=0, \n    logger=logger,\n    deterministic=True\n)","15efb7cf":"model.summarize()","4464a88c":"trainer.fit(\n    model, \n    DataLoader(train_tensor_dset, batch_size=1024, shuffle=True, num_workers=4, drop_last=True),\n    DataLoader(valid_tensor_dset, batch_size=1024, shuffle=False, num_workers=4)\n)","729c594e":"# AUC on validation dataset\ntrainer.test(model, DataLoader(valid_tensor_dset, batch_size=1024, shuffle=False, num_workers=4))","0befc5ba":"# AUC on test dataset\ntrainer.test(model, DataLoader(test_tensor_dset, batch_size=1024, shuffle=False, num_workers=4))","adfc7ccd":"metrics = pd.read_csv(\"logs\/mlp_w_mixup\/version_0\/metrics.csv\")\n\ndf1 = metrics.loc[:,[\"step\",\"train_loss\"]].dropna()\ndf2 = metrics.loc[:,[\"step\",\"valid_loss\"]].dropna()\n\nplt.figure(figsize=(12,5))\nplt.plot(df1.step, df1.train_loss, \"o-\", label=\"train_loss\")\nplt.plot(df2.step, df2.valid_loss, \"o-\", label=\"valid_loss\")\nplt.grid()\nplt.legend(loc=\"best\")\nplt.show()","b6b1536d":"***","c4a9d546":"***\n## data preparation","2c18c34d":"***\n## 3-layers MLP without mixup","85f22915":"# Assessment of a plain MLP+mixup on SCTP\n","13b0a88b":"***\n## 3-layers MLP with mixup"}}