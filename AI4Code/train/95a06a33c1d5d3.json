{"cell_type":{"e1a6baf0":"code","00d8dce1":"code","7d90c1f1":"code","74c9e358":"code","3bbf47bc":"code","f98b36b2":"code","c780dbee":"code","1ed2d051":"code","ca92d3be":"code","57daf9b8":"code","573db6da":"code","f870c3af":"code","b59a5816":"code","3228e7e9":"code","7c80d5f3":"code","7ba7506a":"code","97613475":"code","203c8210":"code","02a1023a":"code","9178734b":"code","838c9f85":"code","aca1b71f":"code","eace3728":"code","eb514af6":"code","cd766c86":"code","bcb7323f":"code","a4410c9d":"code","349d790a":"code","2c779cff":"code","c406925e":"code","2ace694b":"code","bc04a683":"code","57a69241":"code","1d10dd54":"code","303b5ef5":"code","8cc5d807":"code","96cb904b":"code","ac04c2b3":"code","c9a29c7c":"code","f2ea2319":"code","a0c43ca8":"code","f5b09f8f":"code","9e2ca7d0":"code","7b25a840":"code","0bb55fc5":"code","490e17e8":"code","a442cba2":"code","cdb2574b":"code","855fc864":"markdown","d32b829b":"markdown","50d4aa4d":"markdown","2c782bf0":"markdown","22d16430":"markdown","44691e1f":"markdown","c3443727":"markdown","c2737f69":"markdown","471f16eb":"markdown","72dfe723":"markdown","2b1524d0":"markdown","a349ccee":"markdown","149ec332":"markdown","fe0ea36b":"markdown","2d41e52a":"markdown","8daec6ea":"markdown","e317c3e5":"markdown","29ba0391":"markdown","088dcfcd":"markdown","b238a332":"markdown","c619ca5b":"markdown","480111d6":"markdown","b7209473":"markdown","dc429984":"markdown","55f2d391":"markdown","3cb9d8bb":"markdown","cb7ee0cb":"markdown","ef597171":"markdown","92734977":"markdown","c7b0a049":"markdown","e2b53658":"markdown","28b98043":"markdown","6de00e34":"markdown","a1f1d3c9":"markdown"},"source":{"e1a6baf0":"#import the Libraries\n\nimport os\nimport pandas as pd\nimport numpy as np\nimport scipy as sp\nimport random\nimport h5py\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nimport warnings\nwarnings.filterwarnings(\"ignore\")","00d8dce1":"os.listdir('..\/input\/trends-assessment-prediction\/')","7d90c1f1":"features = pd.read_csv('\/kaggle\/input\/trends-assessment-prediction\/train_scores.csv')\nloading = pd.read_csv('\/kaggle\/input\/trends-assessment-prediction\/loading.csv')\nsubmission = pd.read_csv('\/kaggle\/input\/trends-assessment-prediction\/sample_submission.csv')\nfnc = pd.read_csv(\"\/kaggle\/input\/trends-assessment-prediction\/fnc.csv\")\nreveal = pd.read_csv('..\/input\/trends-assessment-prediction\/reveal_ID_site2.csv')\nnumbers = pd.read_csv('..\/input\/trends-assessment-prediction\/ICN_numbers.csv')\nfmri_mask = '..\/input\/trends-assessment-prediction\/fMRI_mask.nii'","74c9e358":"# Installing the nilearn\n!wget https:\/\/github.com\/Chaogan-Yan\/DPABI\/raw\/master\/Templates\/ch2better.nii","3bbf47bc":"import nilearn as nl\nimport nilearn.plotting as nlplt\nimport nibabel as nib\nfrom nilearn import image\nfrom nilearn import plotting\nfrom nilearn import datasets\nfrom nilearn import surface","f98b36b2":"smri = 'ch2better.nii'\nmask_img = nl.image.load_img(fmri_mask)\n\ndef load_subject(filename, mask_img):\n    subject_data = None\n    with h5py.File(filename, 'r') as f:\n        subject_data = f['SM_feature'][()]\n    # It's necessary to reorient the axes, since h5py flips axis order\n    subject_data = np.moveaxis(subject_data, [0,1,2,3], [3,2,1,0])\n    subject_img = nl.image.new_img_like(mask_img, subject_data, affine=mask_img.affine, copy_header=True)\n\n    return subject_img","c780dbee":"files = random.choices(os.listdir('..\/input\/trends-assessment-prediction\/fMRI_train\/'), k = 3)\nfor file in files:\n    subject = os.path.join('..\/input\/trends-assessment-prediction\/fMRI_train\/', file)\n    subject_img = load_subject(subject, mask_img)\n    print(\"Image shape is %s\" % (str(subject_img.shape)))\n    num_components = subject_img.shape[-1]\n    print(\"Detected {num_components} spatial maps\".format(num_components=num_components))\n    nlplt.plot_prob_atlas(subject_img, bg_img=smri, view_type='filled_contours',\n                          draw_cross=False, title='All %d spatial maps' % num_components, threshold='auto')\n    print(\"-\"*50)","1ed2d051":"files = random.choices(os.listdir('..\/input\/trends-assessment-prediction\/fMRI_train\/'), k = 3)\nfor file in files:\n    subject = os.path.join('..\/input\/trends-assessment-prediction\/fMRI_train\/', file)\n    subject_img = load_subject(subject, mask_img)\n    print(\"Image shape is %s\" % (str(subject_img.shape)))\n    num_components = subject_img.shape[-1]\n    print(\"Detected {num_components} spatial maps\".format(num_components=num_components))\n    rsn = subject_img\n    #convert to 3d image\n    first_rsn = image.index_img(rsn, 0)\n    print(first_rsn.shape)\n    plotting.plot_stat_map(first_rsn)\n    print(\"-\"*50)","ca92d3be":"files = random.choices(os.listdir('..\/input\/trends-assessment-prediction\/fMRI_train\/'), k = 3)\nfor file in files:\n    subject = os.path.join('..\/input\/trends-assessment-prediction\/fMRI_train\/', file)\n    subject_img = load_subject(subject, mask_img)\n    print(\"Image shape is %s\" % (str(subject_img.shape)))\n    num_components = subject_img.shape[-1]\n    print(\"Detected {num_components} spatial maps\".format(num_components=num_components))\n    rsn = subject_img\n    #convert to 3d image\n    first_rsn = image.index_img(rsn, 0)\n    print(first_rsn.shape)\n    for img in image.iter_img(rsn):\n        # img is now an in-memory 3D img\n        plotting.plot_stat_map(img, threshold=3)\n    print(\"-\"*50)","57daf9b8":"files = random.choices(os.listdir('..\/input\/trends-assessment-prediction\/fMRI_train\/'), k = 3)\nfor file in files:\n    subject = os.path.join('..\/input\/trends-assessment-prediction\/fMRI_train\/', file)\n    subject_img = load_subject(subject, mask_img)\n    print(\"Image shape is %s\" % (str(subject_img.shape)))\n    num_components = subject_img.shape[-1]\n    print(\"Detected {num_components} spatial maps\".format(num_components=num_components))\n    rsn = subject_img\n    #convert to 3d image\n    first_rsn = image.index_img(rsn, 0)\n    print(first_rsn.shape)     \n    plotting.plot_glass_brain(first_rsn,display_mode='lyrz')\n    print(\"-\"*50)","573db6da":"files = random.choices(os.listdir('..\/input\/trends-assessment-prediction\/fMRI_train\/'), k = 3)\nfor file in files:\n    subject = os.path.join('..\/input\/trends-assessment-prediction\/fMRI_train\/', file)\n    subject_img = load_subject(subject, mask_img)\n    print(\"Image shape is %s\" % (str(subject_img.shape)))\n    num_components = subject_img.shape[-1]\n    print(\"Detected {num_components} spatial maps\".format(num_components=num_components))\n    rsn = subject_img\n    #convert to 3d image\n    first_rsn = image.index_img(rsn, 0)\n    print(first_rsn.shape)\n    plotting.plot_epi(first_rsn)\n    print(\"-\"*50)","f870c3af":"files = random.choices(os.listdir('..\/input\/trends-assessment-prediction\/fMRI_train\/'), k = 3)\nfor file in files:\n    subject = os.path.join('..\/input\/trends-assessment-prediction\/fMRI_train\/', file)\n    subject_img = load_subject(subject, mask_img)\n    print(\"Image shape is %s\" % (str(subject_img.shape)))\n    num_components = subject_img.shape[-1]\n    print(\"Detected {num_components} spatial maps\".format(num_components=num_components))\n    rsn = subject_img\n    #convert to 3d image\n    first_rsn = image.index_img(rsn, 0)\n    print(first_rsn.shape)\n    plotting.plot_anat(first_rsn)\n    print(\"-\"*50)","b59a5816":"files = random.choices(os.listdir('..\/input\/trends-assessment-prediction\/fMRI_train\/'), k = 3)\nfor file in files:\n    subject = os.path.join('..\/input\/trends-assessment-prediction\/fMRI_train\/', file)\n    subject_img = load_subject(subject, mask_img)\n    print(\"Image shape is %s\" % (str(subject_img.shape)))\n    num_components = subject_img.shape[-1]\n    print(\"Detected {num_components} spatial maps\".format(num_components=num_components))\n    rsn = subject_img\n    #convert to 3d image\n    first_rsn = image.index_img(rsn, 0)\n    print(first_rsn.shape)\n    plotting.plot_roi(first_rsn)\n    print(\"-\"*50)","3228e7e9":"motor_images = datasets.fetch_neurovault_motor_task()\nstat_img = motor_images.images[0]\nview = plotting.view_img_on_surf(stat_img, threshold='90%')\nview.open_in_browser()\nview","7c80d5f3":"features.head()","7ba7506a":"features.info()","97613475":"features.fillna(features.mean(),inplace=True)","203c8210":"features.info()","02a1023a":"loading.head()","9178734b":"loading.info()","838c9f85":"fnc.head()","aca1b71f":"fnc.info()","eace3728":"reveal.head()","eb514af6":"reveal.info()","cd766c86":"numbers.head()","bcb7323f":"numbers.info()","a4410c9d":"sns.heatmap(features.corr(),annot=True,linewidths=0.2) \nfig=plt.gcf()\nfig.set_size_inches(20,12)\nplt.show()","349d790a":"sns.heatmap(loading.corr(),annot=True,linewidths=0.2) \nfig=plt.gcf()\nfig.set_size_inches(20,12)\nplt.show()","2c779cff":"#main or, target element in problem\ntarget_col = ['age', 'domain1_var1', 'domain1_var2', 'domain2_var1', 'domain2_var2']","c406925e":"fig, ax = plt.subplots(1, 5, figsize=(20, 5))\nsns.distplot(features['age'], ax=ax[0],rug=True, rug_kws={\"color\": \"coral\"},\n                  kde_kws={\"color\": \"royalblue\", \"lw\": 1.5},\n                  hist_kws={\"histtype\": \"bar\", \"linewidth\": 3,\n                            \"alpha\": 1, \"color\": \"coral\"}).set_title('Age')\n\nsns.distplot(features['domain1_var1'], ax=ax[1],rug=True, rug_kws={\"color\": \"coral\"},\n                  kde_kws={\"color\": \"royalblue\", \"lw\": 1.5},\n                  hist_kws={\"histtype\": \"bar\", \"linewidth\": 3,\n                            \"alpha\": 1, \"color\": \"coral\"}).set_title('domain1_var1')\n\nsns.distplot(features['domain1_var2'], ax=ax[2],rug=True, rug_kws={\"color\": \"coral\"},\n                  kde_kws={\"color\": \"royalblue\", \"lw\": 1.5},\n                  hist_kws={\"histtype\": \"bar\", \"linewidth\": 3,\n                            \"alpha\": 1, \"color\": \"coral\"}).set_title('domain1_var2')\n\nsns.distplot(features['domain2_var1'], ax=ax[3],rug=True, rug_kws={\"color\": \"coral\"},\n                  kde_kws={\"color\": \"royalblue\", \"lw\": 1.5},\n                  hist_kws={\"histtype\": \"bar\", \"linewidth\": 3,\n                            \"alpha\": 1, \"color\": \"coral\"}).set_title('domain2_var1')\n\nsns.distplot(features['domain2_var2'], ax=ax[4],rug=True, rug_kws={\"color\": \"coral\"},\n                  kde_kws={\"color\": \"royalblue\", \"lw\": 1.5},\n                  hist_kws={\"histtype\": \"bar\", \"linewidth\": 3,\n                            \"alpha\": 1, \"color\": \"coral\"}).set_title('domain2_var2')\n\nfig.suptitle('Target Visualization', fontsize=10)","2ace694b":"plt.figure(figsize=(20,15))\ng = sns.pairplot(data=features, hue='age', palette = 'seismic',\n                 size=1.2,diag_kind = 'kde',diag_kws=dict(shade=True),plot_kws=dict(s=10) )\ng.set(xticklabels=[])","bc04a683":"train_df = features.merge(loading, on='Id', how='left')\ntrain = train_df.merge(fnc, on='Id', how='left')\ntrain.head()","57a69241":"X = train.drop(target_col, axis=1)\nX.head()","1d10dd54":"\ny = features\ny.head()","303b5ef5":"y.info()","8cc5d807":"submission['ID_num'] = submission['Id'].apply(lambda x: int(x.split('_')[0]))\ntest = pd.DataFrame({'Id': submission['ID_num'].unique()})\ndel submission['ID_num'];\ntest.head()","96cb904b":"test = test.merge(loading, on='Id', how='left')\ntest = test.merge(fnc, on='Id', how='left')\ntest.head()","ac04c2b3":"test.info()","c9a29c7c":"from tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom sklearn.model_selection import KFold,cross_val_score\nfrom tensorflow.keras.layers import Dropout\n\nmodel = Sequential()\nmodel.add(Dense(1404,input_dim=1404,kernel_initializer='normal',activation='relu'))\nmodel.add(Dense(702,kernel_initializer='normal',activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(702,kernel_initializer='normal',activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(702,kernel_initializer='normal',activation='relu'))\nmodel.add(Dense(5,kernel_initializer='normal'))\n\nmodel.compile(loss='mean_absolute_error',optimizer='adam', metrics = ['accuracy'])\n\nmodel.summary()","f2ea2319":"def lrfn(epoch):\n    LR_START = 0.00001\n    LR_MAX = 0.00005 * 8\n    LR_MIN = 0.00001\n    LR_RAMPUP_EPOCHS = 5\n    LR_SUSTAIN_EPOCHS = 0\n    LR_EXP_DECAY = .8\n    \n    if epoch < LR_RAMPUP_EPOCHS:\n        lr = (LR_MAX - LR_START) \/ LR_RAMPUP_EPOCHS * epoch + LR_START\n    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n        lr = LR_MAX\n    else:\n        lr = (LR_MAX - LR_MIN) * LR_EXP_DECAY**(epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS) + LR_MIN\n    return lr","a0c43ca8":"lr_schedule = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=1)\n\nhistory=model.fit(X.iloc[:,1:],y.iloc[:,1:],epochs=25,batch_size=32,validation_split=0.25,callbacks=[lr_schedule],verbose=2)","f5b09f8f":"loss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(loss) + 1)\nplt.plot(epochs, loss, 'y', label='Training loss')\nplt.plot(epochs, val_loss, 'r', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","9e2ca7d0":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nplt.plot(epochs, acc, 'y', label='Training acc')\nplt.plot(epochs, val_acc, 'r', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","7b25a840":"prediction = model.predict(test.iloc[:,1:])\nprediction = pd.DataFrame(prediction)\nprediction.columns = y.iloc[:,1:].columns\nprediction.head(10)","0bb55fc5":"pred = pd.DataFrame()\n\nfor target in target_col:\n    value = pd.DataFrame()\n    value['Id'] = [f'{c}_{target}' for c in test['Id'].values]\n    value['Predicted'] = prediction[target]\n    pred = pd.concat([pred, value])\n\npred.head()","490e17e8":"submission","a442cba2":"submission = pd.merge(submission, pred, on = 'Id')\nsubmission = submission[['Id', 'Predicted_y']]\nsubmission.columns = ['Id', 'Predicted']\nsubmission.to_csv('submission.csv', index=False)\nsubmission.head()\n","cdb2574b":"submission['Predicted'].hist()","855fc864":"## Plotting ROIs, or a mask...\n\nwhen mapping brain connectivities, ROIs provide the structural substrates for measuring connectivities within individual brains and for pooling data across populations. Thus, identification of reliable, reproducible and accurate ROIs is critically important for the success of brain connectivity mapping.\n\nFor more details you may visit.[click here](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC3927780\/)","d32b829b":"## References\n\n* Convolutional Neural Networks with Intermediate Loss for 3D Super-Resolution of CT and MRI Scans: [click here](https:\/\/arxiv.org\/pdf\/2001.01330)\n* Multi-Resolution 3D CNN for MRI Brain Tumor Segmentation and Survival Prediction: [click here](https:\/\/arxiv.org\/abs\/1911.08388)\n* Automatic Post-Stroke Lesion Segmentation on MR Images using 3D Residual Convolutional Neural Network: [click here](https:\/\/arxiv.org\/pdf\/1911.11209)\n* You can also check out the previuos competition occur on kaggle regard to this. [click here](https:\/\/www.kaggle.com\/c\/mlsp-2014-mri\/overview)\n* You can plot brain visualization using nilearn: [click here](https:\/\/nilearn.github.io\/plotting\/index.html)\n* Check out the various library in python for visulization of Brain MRI.[click here](https:\/\/www.kaggle.com\/c\/trends-assessment-prediction\/discussion\/148175)\n* Idea of visualization. [click here](https:\/\/www.kaggle.com\/soham1024\/visualization-using-nilearn)","50d4aa4d":"### Handling the missing Value...\n\nHandling missing values is an essential part of data cleaning and preparation process because almost all data in real life comes with some missing values.\n\nMissing values need to be handled because they reduce the quality for any of our performance metric. It can also lead to wrong prediction or classification and can also cause a high bias for any given model being used.\n\n### WHAT DO WE DO TO MISSING VALUES?\nThere are several options for handling missing values each with its own PROS and CONS. However, the choice of what should be done is largely dependent on the nature of our data and the missing values. Below is a summary highlight of several options we have for handling missing values.\n\n* DROP MISSING VALUES\n* FILL MISSING VALUES WITH TEST STATISTIC(mean, median, mode).\n* PREDICT MISSING VALUE WITH A MACHINE LEARNING ALGORITHM(knn).","2c782bf0":"## Difference between fMRI and MRI Scans\n\nMagnetic resonance imaging (MRI) of the head is a painless, noninvasive test that produces detailed images of your brain and brain stem. An MRI machine creates the images using a magnetic field and radio waves. This test is also known as a brain MRI or a cranial MRI. You will go to a hospital or radiology center to take a head MRI.\n\nAn MRI scan is different from a CT scan or an X-ray in that it doesn\u2019t use radiation to produce images. An MRI scan combines images to create a 3-D picture of your internal structures, so it\u2019s more effective than other scans at detecting abnormalities in small structures of the brain such as the pituitary gland and brain stem. Sometimes a contrast agent, or dye, can be given through an intravenous (IV) line to better visualize certain structures or abnormalities.\n\nA functional MRI (fMRI) of the brain is useful for people who might have to undergo brain surgery. An fMRI can pinpoint areas of the brain responsible for speech and language, and body movement. It does this by measuring metabolic changes that take place in your brain when you perform certain tasks. During this test, you may need to carry out small tasks, such as answering basic questions or tapping your thumb with your fingertips.\n","22d16430":"## Exploratory Data Analysis of fMRI _ NeuroImaging\n\n\n\nHuman brain research is among the most complex areas of study for scientists. We know that age and other factors can affect its function and structure, but more research is needed into what specifically occurs within the brain. With much of the research using MRI scans, data scientists are well positioned to support future insights. In particular, neuroimaging specialists look for measurable markers of behavior, health, or disorder to help identify relevant brain regions and their contribution to typical or symptomatic effects.\n\nThis is a work in progress. Real time visualisation of fMRI for diagnosis of ADHD, Alzehmeirs disease and other mental health disorders in the future scope. \nThe fMRI is collected from opensource data : https:\/\/github.com\/Chaogan-Yan\/DPABI\/blob\/master\/Templates\/ch2better.nii","44691e1f":"#### I strongly recommend to change the plot from \"Inflated\" to \"Pial\" for analyze the 3D view of Brain.","c3443727":"## Prepare the Train dataset...","c2737f69":"\n## What is NeuroImaging?\n\nNeuroimaging or brain imaging is the use of various techniques to either directly or indirectly image the structure, function, or pharmacology of the nervous system.It is a relatively new discipline within medicine, neuroscience, and psychology.Physicians who specialize in the performance and interpretation of neuroimaging in the clinical setting are neuroradiologists.\n\nNeuroimaging falls into two broad categories:\n* Structural imaging, which deals with the structure of the nervous system and the diagnosis of gross (large scale) intracranial disease (such as a tumor) and injury.\n* Functional imaging, which is used to diagnose metabolic diseases and lesions on a finer scale (such as Alzheimer's disease) and also for neurological and cognitive psychology research and building brain-computer interfaces.\n\nHuman brain research is among the most complex areas of study for scientists. We know that age and other factors can affect its function and structure, but more research is needed into what specifically occurs within the brain. With much of the research using MRI scans, data scientists are well positioned to support future insights. In particular, neuroimaging specialists look for measurable markers of behavior, health, or disorder to help identify relevant brain regions and their contribution to typical or symptomatic effects.\n\n### In this project, we have to predict age and assessment values from two domains using features derived from brain MRI images as inputs.","471f16eb":"## Visualize the Target Columns...","72dfe723":"### There lot more in visualization,and I try to explain all possible visualization in a simple way. you can also check out the above links for more details.","2b1524d0":"## Update Status...\n* Version 6 - Updating Visualisation\n* version 5 - Adding Learning SCheduler for training.Adding some more Data EDA.\n* version 4 - Adding more details about plot. And adding Discription about data.\n* version 3 - Adding More plot and adding future reading note.\n* version 2 - Adding Research work related to NeuroImaging.","a349ccee":"## Glass brain visualization\n\nGlass Brain is a tool that maps the electrical activity of your brain in realtime.The anatomically realistic 3D brain will show realtime data from electroencephalographic (EEG) signals taken from a specially-designed EEG cap.This data is mapped to the source of that electrical activity, i.e. the specific part of the brain. The underlying brain model is generated through MRI scans so that the EEG data is accurately mapped to an individual's brain model.\n\nDifferent colours are given to the different signal frequency bands to create a beautiful interactive artwork that seems to crackle with energy, showing how information is transferred (or at least estimated to do so) between different regions of the brain.","149ec332":"## Preparaing the test data...","fe0ea36b":"## Now move to the Data EDA...\n\n* Analyze the data\n* check for missing values.\n* Analyze the correlation.","2d41e52a":"## Plotting a statistical map\n\nStatistical parametric mapping or SPM is a statistical technique for examining differences in brain activity recorded during functional neuroimaging experiments.The measurement technique depends on the imaging technology (e.g., fMRI and PET). The scanner produces a 'map' of the area that is represented as voxels. Each voxel represents the activity of a specific volume in three-dimensional space. The exact size of a voxel varies depending on the technology. fMRI voxels typically represent a volume of 27 mm3 (a cube with 3mm length sides).\n\nParametric statistical models are assumed at each voxel, using the general linear model to describe the data variability in terms of experimental and confounding effects, with residual variability. Hypotheses expressed in terms of the model parameters are assessed at each voxel with univariate statistics.\n\nAnalyses may examine differences over time (i.e. correlations between a task variable and brain activity in a certain area) using linear convolution models of how the measured signal is caused by underlying changes in neural activity.\n\nBecause many statistical tests are conducted, adjustments have to be made to control for type I errors (false positives) potentially caused by the comparison of levels of activity over many voxels. A type I error would result in falsely assessing background brain activity as related to the task. Adjustments are made based on the number of resels in the image and the theory of continuous random fields in order to set a new criterion for statistical significance that adjusts for the problem of multiple comparisons.","8daec6ea":"# End Note...\n\nThis notebook is work in progress.\n\n\n\n#### <span style=\"color:red\">STAY TUNED!<\/span>","e317c3e5":"## Plotting an EPI...\n\nIn Echo-Planar Imaging (EPI)-based Magnetic Resonance Imaging (MRI), inter-subject registration typically uses the subject's T1-weighted (T1w) anatomical image to learn deformations of the subject's brain onto a template. The estimated deformation fields are then applied to the subject's EPI scans (functional or diffusion-weighted images) to warp the latter to a template space.\n\nFor further details you visit.[click here](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC5819565\/)","29ba0391":"## \"Nilearn\" format\nNilearn is a Python module for fast and easy statistical learning on NeuroImaging data. It leverages the scikit-learn Python toolbox for multivariate statistics with applications such as predictive modelling, classification, decoding, or connectivity analysis.\n\nNilearn can operate on either file names or NiftiImage objects. The later represent the data loaded in memory. In the example above, the function smooth_img returns a Nifti1Image object, which can then be readily passed to other nilearn functions.\n\nIn nilearn, we often use the term \u201cniimg\u201d as abbreviation that denotes either a file name or a NiftiImage object.\nNiimgs can be 3D or 4D. A 4D niimg may for instance represent a time series of 3D images.\n\n### The NifTi data structure (also used in Analyze files) is the standard way of sharing data in neuroimaging research. \n\nThree main components are:\n* data :- raw scans in form of a numpy array: data = nilearn.image.get_data(img)\n\n* affine :- returns the transformation matrix that maps from voxel indices of the numpy array to actual real-world locations of the brain: affine = img.affine\n\n* header:- low-level informations about the data (slice duration, etc.): header = img.header\n\n### Nilearn functions take as input argument what we call \u201cNiimg-like objects\u201d:\n\nNiimg: A Niimg-like object can be one of the following:\n\n* A string with a file path to a Nifti or Analyse image\n\n* An SpatialImage from nibabel, ie an object exposing get_fdata() method and affine attribute, typically a Nifti1Image from nibabel.\n\nNiimg-4D: Similarly, some functions require 4D Nifti-like data, which we call Niimgs or Niimg-4D. Accepted input arguments are:\n\n* A path to a 4D Nifti image\n\n* List of paths to 3D Nifti images\n\n* 4D Nifti-like object\n\n* List of 3D Nifti-like objects\n","088dcfcd":"## Creating the Models...","b238a332":"## Checking the correlation between features","c619ca5b":"## Libraires used","480111d6":"> ![nilearn_candy.png](attachment:nilearn_candy.png)","b7209473":"### File format used\n\n## What is .nii file format?\nThe [.nii] file type is primarily associated with NIfTI-1 Data Format by Neuroimaging Informatics Technology Initiative. NIfTI-1 is adapted from the widely used ANALYZE 7.5 file format. NIfTI-1 uses the empty space in the ANALYZE 7.5 header to add several new features.\n\n#### You can open the [.nii] file [here](https:\/\/filext.com\/file-extension\/NII)\n\n### How to read in Python?\nFor Python, you will need the nilearn in your system.Nilearn is a Python module for fast and easy statistical learning on NeuroImaging data. It leverages the scikit-learn Python toolbox for multivariate statistics with applications such as predictive modelling, classification, decoding, or connectivity analysis.\n\n## What is .mat file format?\nFiles with the [.mat] extension are files that are in the binary data container format that the MATLAB program uses.MAT files are categorized as data files that include variables, functions, arrays and other information. There are the so called level 4 MAT files wherein two-dimensional matrices and character strings are supported and there is also the level 5 MAT files wherein several things are included like the cell arrays, objects, multidimensional numeric arrays, structures and character arrays. MAT files are also useful when it comes to representing audio in various formats such as 16-bit signed integer, 8-bit unsigned integer and 64-bit floating point. Mathworks MATLAB is the software used to open MAT files. It is an application used to develop algorithm, visualize and analyze data as well as to compute numbers.\n\n### How to read it in Python?\nFor Python, you will need the h5py extension, which requires HDF5 on your system. The function loadmat loads all variables stored in the MAT-file into a simple Python data structure, using only Python's dict and list objects.","dc429984":"## How Features Were Obtained - (preTrained and used as input)\n\nAn unbiased strategy was utilized to obtain the provided features. This means that a separate, unrelated large imaging dataset was utilized to learn feature templates. Then, these templates were \"projected\" onto the original imaging data of each subject used for this competition using spatially constrained independent component analysis (scICA) via group information guided ICA (GIG-ICA).\n\nThe first set of features are source-based morphometry (SBM) loadings. These are subject-level weights from a group-level ICA decomposition of gray matter concentration maps from structural MRI (sMRI) scans.\n\nThe second set are static functional network connectivity (FNC) matrices. These are the subject-level cross-correlation values among 53 component timecourses estimated from GIG-ICA of resting state functional MRI (fMRI).\n\nThe third set of features are the component spatial maps (SM). These are the subject-level 3D images of 53 spatial networks estimated from GIG-ICA of resting state functional MRI (fMRI).","55f2d391":"### Importing Libraries for Brain image visualization.","3cb9d8bb":"## Plotting an anatomical image...\n\nMain Idea of this visualization technique is to provide the anatomical picture of the brain. Due to the measurement procedures, BOLD images usually have a realtively low resolution, as you want to squeeze in as many data-points along time as possible.","cb7ee0cb":"## Plotting 4D probabilistic atlas maps\n\nProbabilistic atlasing is a research strategy whose goal is to generate anatomical templates that retain quantitative information on inter-subject variations in brain architecture (Mazziotta et al., 1995). A digital probabilistic atlas of the human brain, incorporating precise statistical information on positional variability of important functional and anatomic interfaces, may rectify many current atlasing problems, since it specifically stores information on the population variability.\n\nFor further reading you may visit.[click here](http:\/\/users.loni.usc.edu\/~thompson\/prob_atlas.html)","ef597171":"## Let Create the Submission...","92734977":"## Let understand the Dataset\n\n* **fMRI_train** - a folder containing 53 3D spatial maps for train samples in [.mat] format.\n* **fMRI_test** - a folder containing 53 3D spatial maps for test samples in [.mat] format.\n* **fnc.csv** - static FNC correlation features for both train and test samples.\n* **loading.csv** - sMRI SBM loadings for both train and test samples.\n* **train_scores.csv** - age and assessment values for train samples.\n* **reveal_ID_site2.csv** - a list of subject IDs whose data was collected with a different scanner than the train samples.\n* **fMRI_mask.nii** - a 3D binary spatial map.\n* **ICN_numbers.txt** - intrinsic connectivity network numbers for each fMRI spatial map; matches FNC names.\n\n[Note] - The [.mat] files in this can be read in python using h5py,and the [.nii] file can be read in python using nilearn.","c7b0a049":"Interpreting The Heatmap The first thing to note is that only the numeric features are compared as it is obvious that we cannot correlate between alphabets or strings. Before understanding the plot, let us see what exactly correlation is.\n\nPOSITIVE CORRELATION: If an increase in feature A leads to increase in feature B, then they are positively correlated. A value 1 means perfect positive correlation.\n\nNEGATIVE CORRELATION: If an increase in feature A leads to decrease in feature B, then they are negatively correlated. A value -1 means perfect negative correlation.\n\nNow lets say that two features are highly or perfectly correlated, so the increase in one leads to increase in the other. This means that both the features are containing highly similar information and there is very little or no variance in information. This is known as MultiColinearity as both of them contains almost the same information.\n\nSo do you think we should use both of them as one of them is redundant. While making or training models, we should try to eliminate redundant features as it reduces training time and many such advantages.","e2b53658":"## BRAIN IMAGE VISUALISATION\n","28b98043":"# What next...\n\n\n* More explanition through visualization.\n* Visualisation through unity further as aN AR OR VR Application","6de00e34":"## 3D Plots of statistical maps or atlases on the cortical surface...","a1f1d3c9":"### Taking any 3 random images and visualize the plot."}}