{"cell_type":{"428a76de":"code","607c47af":"code","44d671d2":"code","74f32569":"code","2f9d081e":"code","8e7eef97":"code","c52b3a0e":"code","e39e7374":"code","7ef63edc":"code","f4e01b90":"code","f8bbaa7a":"code","ddd92f32":"code","368c6979":"code","6b9e4a6c":"code","62701149":"markdown","50bdd051":"markdown"},"source":{"428a76de":"import time\nimport operator as opt\nimport numpy as np \nimport pandas as pd \nimport os\nimport gc\nfrom contextlib import contextmanager\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport seaborn as sns\nimport featuretools as ft\nimport warnings\nwarnings.filterwarnings('ignore')","607c47af":"#### Function definition ####\n@contextmanager\ndef timer(title):\n    t0 = time.time()\n    yield\n    print(\"{} - done in {:.0f}s\".format(title, time.time() - t0))\n\ndef read_file_names(path='\/kaggle\/input'):\n    files = {}\n    for dirname, _, filenames in os.walk(path):\n        for filename in filenames:\n            files[filename] = os.path.join(dirname, filename)\n            print(filename, ' : ', os.path.join(dirname, filename))\n    return files\n\ndef load_file_into_dataframe(file_path):\n    df = pd.read_csv(file_path)\n    df = df.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x) # best practice\n    df = df.replace('?', np.nan) # best practice\n    return df\n\ndef dataframe_split_between_null_and_not_null(df):\n    not_null_df = df.dropna()\n    null_df = df.drop(not_null_df.index)\n    percentage = 100 * null_df.shape[0] \/ df.shape[0]\n    print(\"Actual dataset \", df.shape)\n    print(\"Null dataset \", null_df.shape)\n    print(\"Not null dataset \", not_null_df.shape)\n    print(f'Null percentage {percentage}%')\n    return df, null_df, not_null_df,percentage\n\ndef column_missing_state(df):\n    mis_val = df.isnull().sum()\n    mis_val_percent = 100 * mis_val \/ len(df)\n    mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n    mis_val_table = mis_val_table[mis_val_table.iloc[:,1] != 0].sort_values(1, ascending=False).round(1)\n    print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table.shape[0]) +\n              \" columns that have missing values.\")\n    return mis_val_table\n\ndef column_dropper(df, threshold=.7): # TODO: Shoul be used for generating column list to drop\n    missing_percentage_df = column_missing_state(df)\n    cols_to_drop = set()\n    for index, row in missing_percentage_df.iterrows():\n        if row[1] > 100 * threshold:\n            cols_to_drop.add(index)\n    percentage = 100 * len(cols_to_drop) \/ df.shape[1]\n    print(\"Actual dataset \", df.shape)\n    df = df.drop(list(cols_to_drop), axis=1)\n    print(\"New dataset \", df.shape)\n    print(f'Columns drop percentage {percentage}%')\n    return df, percentage\n\ndef row_dropper(df, threshold =.7):\n    col_num = df.shape[1]\n    nan_vals = dict(df.count(axis=1)) # How much column every row has\n    nan_vals = {key: value for (key, value) in nan_vals.items() if (value \/ col_num) > threshold}\n    percentage = 100 * len(nan_vals.keys()) \/ df.shape[0]\n    print(\"Actual dataset \", df.shape)\n    df = df.drop(index=nan_vals.keys())\n    print(\"New dataset \", df.shape)\n    print(f'Rows drop percentage {percentage}%')\n    return df, percentage\n\ndef drop_columns_from_dataframe(df, col_list= ['colA','colB']):\n    return df.drop(col_list, axis=1, inplace=True)\n\ndef drop_rows_based_on_indices(df, indices = None):\n    return df.drop(indices, inplace = True)\n\ndef generate_condition(df,col_name='colA', value= 'valA', com_op = opt.lt): # TODO: Should be used for comparing column with a value \n    dtype_ = df[col_name].dtype\n    value_ = pd.Series([value], dtype=dtype_)\n    indices = df[com_op(df[col_name], value_[0])].index #['eq', 'ne', 'ge', 'le', 'gt', 'lt']\n    return indices\n\ndef check_correlation_against_target_column(df, col_name, target_col_name): # TODO: Should be used for getting correlation between a target and specified column\n    return df[col_name].corr(df[target_col_name])\n\ndef correaltion_dataframe(df, target_col, positive= True): # TODO: Should be used for getting correlation dataframe based on positive and negative\n    correlations =  df.corr()[target_col].sort_values(ascending=False)\n    if positive == True:\n        return correlations[correlations > 0]\n    else:\n        return correlations[correlations < 0]\n    \ndef real_discrete_columns_list(df):\n    real = [i for i in range(len(df.iloc[0])) if type(df.iloc[0, i]) != str]\n    discrete = [i for i in range(len(df.iloc[0])) if type(df.iloc[0, i]) == str]\n    real_names = set()\n    discrete_names = set()\n    for i,v in enumerate(df.columns):\n        if i in real:\n            real_names.add(v)\n        else:\n            discrete_names.add(v)\n    return real, discrete, real_names, discrete_names\n\ndef interpolate_missing_data(df, real_names, discrete_names):\n    imputation_failed = set()\n    for col in real_names:\n        df[col] = df[col].fillna(df[col].median())\n        if df[col].isnull().sum():\n            imputation_failed.add(col)\n    for col in discrete_names:\n        df[col] = df[col].fillna(df[col].mode())\n        if df[col].isnull().sum():\n            imputation_failed.add(col)\n    for col in list(imputation_failed):\n        if col in discrete_names:\n            val = df[col].mode().values[0]\n            df[col].fillna(val,inplace=True)\n        else: \n            val = df[col].median().values[0]\n            df[col].fillna(val,inplace=True)\n    return df\n\ndef remove_outliers(data, real_names,discrete_names):\n    real_names = list(real_names)\n    discrete_names = list(discrete_names)\n    cat_data = data[discrete_names]\n    real_data = data[real_names]\n    Q1 = real_data.quantile(.25)\n    Q3 = real_data.quantile(.75)\n    IQR = Q3 - Q1\n    new_df = real_data\n    new_df_out_0 = new_df[~((new_df < (Q1 - 1.5 * IQR))|(new_df > (Q3 + 1.5 * IQR))).any(axis=1)]\n    new_df_out_1 = new_df.drop(new_df_out_0.index, inplace=False)\n    percentage = 100 * new_df_out_1.shape[0] \/ real_data.shape[0]\n    for i,col in enumerate(new_df_out_1.columns):\n        new_df_out_1[col] = new_df_out_0[col].median()\n    real_data = pd.concat([new_df_out_0, new_df_out_1], axis=0)\n    final_data = pd.concat([real_data, cat_data], axis=1)\n    percentage = 100 * final_data.shape[0] \/ data.shape[0]\n    if final_data.shape[0] != data.shape[0]:\n        print(f'Rows drop percentage {percentage}%')\n    return final_data\n\ndef outlier_check(df):\n    test_df = df\n    Q1 = test_df.quantile(.25)\n    Q3 = test_df.quantile(.75)\n    IQR = Q3 - Q1\n    test_df_out_1 = test_df[((test_df < (Q1 - 1.5 * IQR)) |(test_df > (Q3 + 1.5 * IQR))).any(axis=1)]\n    percentage = 100 * test_df_out_1.shape[0] \/ df.shape[0]\n    print(f'Outlier percentage {percentage}%')\n    return percentage\n\ndef label_encoding(df, discrete_names):\n    le_count = 0\n    le_dict = {}\n    for col in discrete_names:\n        if len(list(df[col].unique())) <= 3:\n            col_values, unique_name = pd.factorize(df[col])\n            le_dict[col] = list(unique_name)\n            df[col] = col_values\n            le_count += 1\n    print('%d columns were label encoded.' % le_count)\n    return df, le_dict\n\ndef one_hot_encoder(df, discrete_names):\n    df2 = pd.get_dummies(df, columns= discrete_names)\n    new_columns = set(df2.columns) - set(df.columns)\n    print('%d columns were one hot encoded.' % len(new_columns))\n    return df2, list(new_columns)","44d671d2":"files = read_file_names(path='\/kaggle\/input')","74f32569":"train = load_file_into_dataframe(files['application_train.csv'])\ntest = load_file_into_dataframe(files['application_test.csv'])","2f9d081e":"# shape i.e [df.shape]\n# info i.e [df.info()]\n# checking on columnwise value frequency i.e [dict(df.count(axis=0))] \n# for the purpose of column counts, min, max, mean, Q1, Q3 we can use the above dataframe i.e [df.describe()] or [df.describe(). T] [for transposed dataframe]\n# columnwise individual's datatypes (which column contains which data type) i.e [df.dtypes]\n# count column number for every datatype i.e [df.dtypes.value_counts()]\n# unique value for specified dataype's columns i.e [df.select_dtypes(dtype).apply(pd.Series.nunique, axis = 0)]\n# unique values in specified columns i.e [df[col_name].value_counts()]\n# correlation matrix i.e [df.corr()]\n# check sum of null values in every column of a df i.e [df.isnull().sum()]\n# check sum of not-null values in every column of a df i.e [df.notnull().sum()]\ntrain_row, train_col =  train.shape\ntest_row, test_col = test.shape\nprint(train_row, train_col)\nprint(test_row, test_col)","8e7eef97":"target_column_label = 'TARGET'\nidentifier_column_label = 'SK_ID_CURR'\n\n# train.drop([target_column_label], axis=1, inplace=True)\ntest['set'] = 'test'\ntest[target_column_label] = -999\ntrain['set'] = 'train'\ncombined_df = train.append(test, ignore_index=True)\nidentifier_column_value = combined_df[identifier_column_label]\ntarget_column_value = combined_df[target_column_label] \ncombined_df.drop([identifier_column_label,target_column_label], axis=1, inplace=True)","c52b3a0e":"# split dataframe between df, null df and not null df\n# check missing data statistics i.e null values | percentage of total null values \n# drop columns from dataframe with column names list\n# drop rows from dataframe with indices list\nwith timer(\"Splitting dataframe between dataframe, null-dataframe, not-nul-dataframe and percentage\"):\n    df, null_train_df, not_null_train_df, percentage = dataframe_split_between_null_and_not_null(combined_df)\nprint()\nwith timer(\"Splitting real data and discrete data columns\"):\n    real, discrete, real_names, discrete_names = real_discrete_columns_list(df)\nwith timer(\"Dropping columns based on threshold\"):\n    df, cols_percentage = column_dropper(df, threshold = .85)\nprint()\nwith timer(\"Dropping rows based on threshold\"):\n    df, rows_percentage = row_dropper(df, threshold = .9)\nwith timer(\"Missing value interpolation for the dataframe:\"):\n    df = interpolate_missing_data(df, real_names, discrete_names)\nprint()\nwith timer(\"Removing outliers from the dataframe:\"):\n    while(outlier_check(df) != 0.0):\n        df = remove_outliers(df,real_names,discrete_names)\nprint()\nwith timer(\"Label encoding the dataframe:\"):\n    df, le_dict = label_encoding(df, discrete_names)\nprint()\nwith timer(\"One hot encoding the dataframe:\"):\n    df, new_columns = one_hot_encoder(df, (set(discrete_names) - set(le_dict.keys())))\nprint()\nwith timer(\"Assigning the label and one hot encoding variable:\"):\n    label_encoded_dict = le_dict\n    label_encoded_columns = list(le_dict.keys())\n    one_hot_encoded_columns = new_columns\nprint()\n# train_stat_df = missing_values_stat_in_columns(application_train)\n# dropped_column_train_df = drop_columns_from_dataframe(application_train, col_list= ['colA','colB'])\n# dropped_rows_train_df = drop_rows_based_on_indices(application_train, indices = None)\n# missing_value_manipulation(application_train,30.0)\n\n#### Check anomalies as missing data #\n#### Check outliers as missing data #\n# Deleting rows and columns based on their missing values\n# Replacing values with mean, median and mode\n# Assigning an unique category\n# Predicting the missing value\n# Using algorithms which support missing values","e39e7374":"df[identifier_column_label] = identifier_column_value\ndf[target_column_label] = target_column_value","7ef63edc":"# creating and entity set 'es'\nes = ft.EntitySet(id = 'cr-entity')\n\n# adding a dataframe \nes = es.entity_from_dataframe(entity_id = 'df', dataframe = df, index = 'SK_ID_CURR')","f4e01b90":"default_agg_primitives =  [\"sum\", \"std\", \"max\", \"skew\", \"min\", \"mean\", \"count\", \"percent_true\", \"num_unique\", \"mode\"]\ndefault_trans_primitives =  [\"day\", \"year\", \"month\", \"weekday\", \"haversine\",'num_characters', 'num_words'] ","f8bbaa7a":"# ft.primitives.list_primitives().name.to_list()","ddd92f32":"feature_matrix, feature_names = ft.dfs(entityset = es, target_entity = 'df',trans_primitives = default_trans_primitives, agg_primitives=default_agg_primitives, max_depth = 2, features_only=False, verbose = True, n_jobs = 3)","368c6979":"# feature_matrix.columns\n# feature_matrix.head()\nfeature_matrix = feature_matrix.reindex(index=df[identifier_column_label])\nfeature_matrix = feature_matrix.reset_index()\nprint('Saving features')\nfeature_matrix.to_csv('feature_matrix.csv', index = False)","6b9e4a6c":"# !python -m pip install featuretools","62701149":"#### Automatic Feature Engineering with autofeaturetools","50bdd051":"* [EDA cheet sheet](https:\/\/github.com\/cmawer\/pycon-2017-eda-tutorial\/blob\/master\/EDA-cheat-sheet.md)\n* [Data Science for business leaders](http:\/\/datacamp-community-prod.s3.amazonaws.com\/9fb0cc3f-19fb-4be8-8d3f-68f10a95cc16)\n* [EDA](https:\/\/elitedatascience.com\/python-cheat-sheet)"}}