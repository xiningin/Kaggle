{"cell_type":{"cabe7090":"code","e8375eec":"code","cba59628":"code","91fb67ed":"code","fbc4fd52":"code","16fe9b7c":"code","ac12ed1d":"code","c936910b":"code","4f3f80d6":"code","c73b1638":"code","2b75000f":"code","a9f1e462":"code","4c440b42":"code","0666b9cf":"code","f80f914f":"code","e63fe7fb":"code","1396fbfa":"code","888ac9b7":"markdown","6a1c8a36":"markdown","12e8d51a":"markdown","826663ae":"markdown","8625e58a":"markdown","06bbf9bc":"markdown","fd8a1bad":"markdown","d40a0cd1":"markdown","6a747969":"markdown","1ee3712c":"markdown","2149a74b":"markdown","bbea154b":"markdown","d83d636a":"markdown","e9099209":"markdown","fa64da24":"markdown","9b390adc":"markdown","8c64b191":"markdown","012afdea":"markdown","8eff0283":"markdown"},"source":{"cabe7090":"# Date wrangling\nimport datetime\n\n# Data wrangling\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport gc\n\n# Package to track the optimizing of parameters \nimport time\n\n# Status tracker\nfrom tqdm import tqdm\n\n# xgboost library\nimport xgboost as xgb\nfrom xgboost import plot_importance\n\n# gbm library\nimport lightgbm as lgb\n\n# One hot encoding \nfrom sklearn import preprocessing\n\n# Dimensionality reduction\nfrom sklearn.decomposition import PCA\n\n# Hyper parameter optimization\nfrom sklearn.model_selection import KFold, TimeSeriesSplit, StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import make_scorer\nfrom hyperopt import fmin, tpe, hp, space_eval\n\n#TODO move analyzing and ploting to a class \nanalyze_data = False\nplot_data = False","e8375eec":"# Reading the training data\ntrain_identity = pd.read_csv('..\/input\/ieee-fraud-detection\/train_identity.csv')\ntrain_transaction = pd.read_csv('..\/input\/ieee-fraud-detection\/train_transaction.csv')","cba59628":"# Seeing the columns of the data\nprint(\"Identity columns:\", train_identity.columns)\nprint(\"Transaction columns:\", train_transaction.columns)\n\n# Shapes\nprint(\"Identity shape:\", train_identity.shape)\nprint(\"Transaction shape\", train_transaction.shape)","91fb67ed":"# Merging the two data sets together\nd = pd.merge(train_transaction, train_identity, on='TransactionID', how='left')\nprint(\"Shape of merged data\", d.shape)","fbc4fd52":"# Reading the test set\ntest_identity = pd.read_csv('..\/input\/ieee-fraud-detection\/test_identity.csv')\ntest_transaction = pd.read_csv('..\/input\/ieee-fraud-detection\/test_transaction.csv')\n\n# Merging to one data frame\nd_test = pd.merge(test_transaction, test_identity, on='TransactionID', how='left')\nprint(f'Test data shape: {d_test.shape}')","16fe9b7c":"# Freeing up memory\ndel test_identity, test_transaction, train_identity, train_transaction\ngc.collect()\n\n# Convergin inf to NA\nd = d.replace([np.inf, -np.inf], np.nan)\nd_test = d_test.replace([np.inf, -np.inf], np.nan)","ac12ed1d":"print(d.head(10))","c936910b":"# Balance of data\nprint('Percent of fraud transactions:', d[d['isFraud']==1].shape[0] * 100 \/d.shape[0])\nprint('Percent of good transactions:', d[d['isFraud']==0].shape[0] * 100 \/d.shape[0])","4f3f80d6":"if analyze_data:\n    features = d.columns\n    feature_info = {}\n    row_count = d.shape[0]\n\n    for feature in tqdm(features):\n        NA_count = d[d[feature].isna()].shape[0]\n        NA_share = round(NA_count * 100\/row_count, 4)\n        unique_values_count = len(d[feature].unique())\n        coltype = d[feature].dtype.kind\n\n        feature_info.update({\n            feature: {\n                'NA_count': NA_count,\n                'NA_share': NA_share,\n                'unique_values_count': unique_values_count,\n                'coltype': coltype\n            }\n        })\n    \n    feature_df = pd.DataFrame({\n        'NA_share': [feature_info.get(x).get('NA_share') for x in feature_info],\n        'NA_count': [feature_info.get(x).get('NA_count') for x in feature_info],\n        'unique_values_count': [feature_info.get(x).get('unique_values_count') for x in feature_info],\n        'coltype': [feature_info.get(x).get('coltype') for x in feature_info],\n        'feature': [x for x in feature_info.keys()]\n    }).sort_values('NA_share')\n    print(feature_df)","c73b1638":"if analyze_data:\n    na_shares = np.array([feature_info.get(x).get('NA_share') for x in feature_info])\n    labels, counts = np.unique(na_shares, return_counts=True)\n    count_df = pd.DataFrame({\n        'na_share': labels, \n        'count': counts\n    }).sort_values('na_share', ascending=False)\n    print(count_df.head(20))\n    plt.hist(na_shares)\n    plt.show()","2b75000f":"if analyze_data:\n    # Getting the columns for each of the data type\n    numeric_cols = [x for x in feature_info if feature_info.get(x).get('coltype')=='f']\n    numeric_cols += [x for x in feature_info if feature_info.get(x).get('coltype')=='i']\n\n    categorical_cols = [x for x in feature_info if (feature_info.get(x).get('coltype')=='O')]\n\n    # Removing unwanted features\n    numeric_cols = [x for x in numeric_cols if x not in ['isFraud', 'TransactionID']]\n\n    # Getting the datatypes\n    height = [len(numeric_cols), len(categorical_cols)]\n    bars = ('numeric', 'categorical')\n    y_pos = np.arange(len(bars))\n\n    # Drawing the distribution\n    plt.bar(y_pos, height)\n    plt.xticks(y_pos, bars)\n    plt.show()","a9f1e462":"if analyze_data and plot_data:\n    for i, float_col in enumerate(numeric_cols):\n        plt.figure(figsize=(16, 8))\n        sns.violinplot(x='isFraud', y=float_col, data=d, hue='isFraud').set_title(float_col)\n        plt.show()","4c440b42":"# Defining a feature engineering class\nclass featureEngineer:\n    \n    def __init__(self):\n        self.email_map = {\n        'gmail': 'google', \n        'att.net': 'att', \n        'twc.com': 'spectrum', \n        'scranton.edu': 'other', \n        'optonline.net': 'other', \n        'hotmail.co.uk': 'microsoft',\n        'comcast.net': 'other', \n        'yahoo.com.mx': 'yahoo', \n        'yahoo.fr': 'yahoo',\n        'yahoo.es': 'yahoo', \n        'charter.net': 'spectrum', \n        'live.com': 'microsoft', \n        'aim.com': 'aol', \n        'hotmail.de': 'microsoft', \n        'centurylink.net': 'centurylink',\n        'gmail.com': 'google', \n        'me.com': 'apple', \n        'earthlink.net': 'other', \n        'gmx.de': 'other',\n        'web.de': 'other', \n        'cfl.rr.com': 'other', \n        'hotmail.com': 'microsoft', \n        'protonmail.com': 'other', \n        'hotmail.fr': 'microsoft', \n        'windstream.net': 'other', \n        'outlook.es': 'microsoft', \n        'yahoo.co.jp': 'yahoo', \n        'yahoo.de': 'yahoo',\n        'servicios-ta.com': 'other', \n        'netzero.net': 'other', \n        'suddenlink.net': 'other',\n        'roadrunner.com': 'other', \n        'sc.rr.com': 'other', \n        'live.fr': 'microsoft',\n        'verizon.net': 'yahoo', \n        'msn.com': 'microsoft', \n        'q.com': 'centurylink', \n        'prodigy.net.mx': 'att', \n        'frontier.com': 'yahoo', \n        'anonymous.com': 'other', \n        'rocketmail.com': 'yahoo', \n        'sbcglobal.net': 'att', \n        'frontiernet.net': 'yahoo', \n        'ymail.com': 'yahoo', \n        'outlook.com': 'microsoft', \n        'mail.com': 'other', \n        'bellsouth.net': 'other', \n        'embarqmail.com': 'centurylink', \n        'cableone.net': 'other', \n        'hotmail.es': 'microsoft', \n        'mac.com': 'apple', \n        'yahoo.co.uk': 'yahoo',\n        'netzero.com': 'other', \n        'yahoo.com': 'yahoo', \n        'live.com.mx': 'microsoft', \n        'ptd.net': 'other', \n        'cox.net': 'other',\n        'aol.com': 'aol', \n        'juno.com': 'other', \n        'icloud.com': 'apple'\n        }\n\n        self.minmaxcols = ['TransactionAmt', 'card1', 'card2', 'card3', 'card5', 'D1', 'D15', 'dist1']\n        \n        self.logcols = ['TransactionAmt']\n\n    def feature_engineer(self, X):\n        \"\"\"\n        A function that transforms features in a given dataset by certain rules\n        \"\"\"\n        for col in X.columns:\n\n            if col in self.logcols:\n                X[col + '_log'] = np.log(X[col])\n            \n            if col in self.minmaxcols:\n                normalizer = preprocessing.MinMaxScaler()\n                X[col + '_scaled'] = normalizer.fit_transform(X[col].values.reshape(-1, 1)) \n\n            if col in ['P_emaildomain', 'R_emaildomain']:\n                X[col + '_provider'] = X[col].map(self.email_map)\n                X[col + '_suffix'] = X[col].map(lambda x: str(x).split('.')[-1])\n\n            if col == 'TransactionDT':\n                START_DATE = '2017-12-01'\n                startdate = datetime.datetime.strptime(START_DATE, \"%Y-%m-%d\")\n                X[\"Date\"] = X[col].apply(lambda x: (startdate + datetime.timedelta(seconds=x)))\n                X['Weekdays'] = X['Date'].dt.dayofweek\n                X['Hours'] = X['Date'].dt.hour\n                X['Days'] = X['Date'].dt.day\n                X['Weekdays_Hours'] = X['Weekdays'] + X['Hours']\n\n            if col == 'id_30':\n                X[col + '_cleaned'] = X[col]\n                for ops in ['windows', 'ios', 'mac', 'android', 'linux']:\n                    X.loc[X[col].str.lower().str.contains(ops, na=False), col + '_cleaned'] = ops\n\n            if col == 'id_31':\n                X[col + '_cleaned'] = np.nan\n                for ops in ['chrome', 'firefox', 'edge', 'ie', \n                            'android', 'opera', 'safari', 'samsung', \n                            'google', 'blackberry']:\n                    X.loc[X[col].str.lower().str.contains(ops, na=False), col + '_cleaned'] = ops\n                # Creating a column to check if the device is a mobile\n                X.loc[X[col].str.lower().str.contains('mobile', na=False), col + '_cleaned_mobile'] = \"1\"\n        \n        # Crude aggregations\n        X['P_emaildomain__addr1'] = X['P_emaildomain'] + '__' + X['addr1'].astype(str)\n        X['card1__card2'] = X['card1'].astype(str) + '__' + X['card2'].astype(str)\n        X['card1__addr1'] = X['card1'].astype(str) + '__' + X['addr1'].astype(str)\n        X['card2__addr1'] = X['card2'].astype(str) + '__' + X['addr1'].astype(str)\n        X['card12__addr1'] = X['card1__card2'] + '__' + X['addr1'].astype(str)    \n\n        X['TransactionAmt_to_mean_card1'] = X['TransactionAmt'] \/ X.groupby(['card1'])['TransactionAmt'].transform('mean')\n        X['TransactionAmt_to_mean_card2'] = X['TransactionAmt'] \/ X.groupby(['card2'])['TransactionAmt'].transform('mean')\n        X['TransactionAmt_to_mean_card3'] = X['TransactionAmt'] \/ X.groupby(['card3'])['TransactionAmt'].transform('mean')\n        X['TransactionAmt_to_mean_card5'] = X['TransactionAmt'] \/ X.groupby(['card5'])['TransactionAmt'].transform('mean')\n        \n        X['TransactionAmt_to_std_card1'] = X['TransactionAmt'] \/ X.groupby(['card1'])['TransactionAmt'].transform('std')\n        X['TransactionAmt_to_std_card2'] = X['TransactionAmt'] \/ X.groupby(['card2'])['TransactionAmt'].transform('std')\n        X['TransactionAmt_to_std_card3'] = X['TransactionAmt'] \/ X.groupby(['card3'])['TransactionAmt'].transform('std')\n        X['TransactionAmt_to_std_card5'] = X['TransactionAmt'] \/ X.groupby(['card5'])['TransactionAmt'].transform('std')\n        \n        return X","0666b9cf":"# Defining a class to preproces data \nclass preprocesingEngineer:\n    \"\"\"\n    Class that preprocess data before modeling\n    \"\"\"\n    def __init__(self):\n        self.numeric_cols = [\n            'TransactionAmt_log', 'card1', 'card2', 'card3', 'card5', \n            'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14',\n            'D1', 'Hours', 'dist1', 'D15', \n            'TransactionAmt_to_mean_card1', 'TransactionAmt_to_mean_card2', 'TransactionAmt_to_mean_card3', 'TransactionAmt_to_mean_card5', \n            'TransactionAmt_to_std_card1', 'TransactionAmt_to_std_card2', 'TransactionAmt_to_std_card3', 'TransactionAmt_to_std_card5',\n            'id_02'\n        ]\n        \n        self.categorical_cols = [\n            'ProductCD', \n            'P_emaildomain_provider', 'P_emaildomain_suffix', \n            'R_emaildomain_provider', 'R_emaildomain_suffix',\n            'id_30_cleaned', 'id_31_cleaned',\n            'addr1', 'addr2', \n            'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9',\n            'Weekdays', 'Days', 'Weekdays_Hours',\n            \"id_12\", \"id_13\",\"id_14\",\"id_15\",\"id_16\",\"id_17\",\"id_18\",\"id_19\",\"id_20\",\"id_21\",\"id_22\",\"id_23\",\"id_24\",\n            \"id_25\",\"id_26\",\"id_27\",\"id_28\",\"id_29\",\"id_30\",\"id_32\",\"id_34\", 'id_36', \"id_37\", \"id_38\",\n            'P_emaildomain__addr1', 'card1__card2', 'card1__addr1', 'card2__addr1',\n            'card12__addr1',\n        ]\n        \n        self.target_col = 'isFraud'\n        \n    def to_string(self, X):\n        \"\"\"\n        A method to convert categorical columns to strings\n        \"\"\"\n        for feature in self.categorical_cols:\n            X[feature] = [str(x) for x in X[feature].values]\n            \n        return X    \n    \n    def encode_labels(self, X, X_test=pd.DataFrame({})):\n        \"\"\"\n        A method to label encode categorical columns\n        \"\"\"\n        for feature in self.categorical_cols:\n            # Initiating the label encoder\n            lbl = preprocessing.LabelEncoder()\n            \n            # Getting unique column values\n            features = list(set(X[feature].values))\n            if not X_test.empty:\n                features += list(set(X[feature].values))\n            \n            # Fitting the label encoder    \n            lbl.fit(features)\n            \n            # Transforming the original feature\n            X[feature] = lbl.transform(list(X[feature].values))\n            \n        return X      \n    \n    def leave_final_cols(self, X, leave_target=False):\n        \"\"\"\n        A method to leave only the columns which will be used in modeling\n        \"\"\"\n        cols = self.numeric_cols + self.categorical_cols\n        \n        if leave_target:\n            return X[self.target_col].values, X[cols]\n        else:\n            return X[cols]            ","f80f914f":"class modelingEngineer:\n    \"\"\"\n    Class for creating and using boosting models\n    \"\"\"\n    \n    def __init__(self):\n        self.NFOLDS = 5\n        \n        self.top_ft = 50\n        \n        self.lgb_parameters={\n            'objective': 'binary',\n            'metric': 'auc',\n            'num_leaves': 600,\n            'min_child_weight': 0.03,\n            'feature_fraction': 0.27,\n            'bagging_fraction': 0.31,\n            'min_data_in_leaf': 106,\n            'n_jobs': -1,\n            'learning_rate': 0.005,\n            'max_depth': -1,\n            'tree_learner': 'serial',\n            'colsample_bytree': 0.5,\n            'subsample_freq': 1,\n            'subsample': 0.7,\n            'max_bin': 300,\n            'verbose': -1,\n            'early_stopping_rounds': 200,\n        }\n        \n    def fit_model(self, X_train, Y_train, X_test):\n        \"\"\"\n        Fits the model using cross-validation and early stopping\n        \"\"\"\n        folds = KFold(n_splits=self.NFOLDS)\n        splits = folds.split(X_train, Y_train)\n\n        feature_importances = pd.DataFrame()\n        feature_importances['feature'] = X_train.columns\n        \n        y_preds = np.zeros(X_test.shape[0])\n        \n        for fold_n, (train_index, valid_index) in enumerate(splits):\n            x_train, x_valid = X_train.iloc[train_index], X_train.iloc[valid_index]\n            y_train, y_valid = Y_train[train_index], Y_train[valid_index]\n\n            dtrain = lgb.Dataset(x_train, label=y_train)\n            dvalid = lgb.Dataset(x_valid, label=y_valid)\n\n            clf = lgb.train(self.lgb_parameters, dtrain, 10000, valid_sets = [dvalid], verbose_eval=100)\n            \n            feature_importances[f'fold_{fold_n + 1}'] = clf.feature_importance()\n\n            # Predicting the true values of Y\n            y_preds += clf.predict(X_test) \/ self.NFOLDS\n            \n            del x_train, x_valid, y_train, y_valid\n            gc.collect()\n       \n        return y_preds, feature_importances\n    \n    def plot_feature_importance(self, feature_importances):\n        feature_importances['average'] = feature_importances[[f'fold_{fold_n + 1}' for fold_n in range(self.NFOLDS)]].mean(axis=1)\n        sns.barplot(data=feature_importances.sort_values(by='average', ascending=False).head(self.top_ft), x='average', y='feature')\n        plt.show()","e63fe7fb":"# Defining the global pipeline parameters\noptimize_params = True\nN_evals = 10\nuse_gpu = False\n\nclass pipeline(featureEngineer, preprocesingEngineer, modelingEngineer):\n    \n    def __init__(self):\n        self.d_train = d\n        self.d_test = d_test\n        \n    def pipeline(self):\n        \"\"\"\n        Pipeline that creates new features, preproces and models data\n        \"\"\"\n        # Creating new features\n        fe = featureEngineer()\n        X_train = fe.feature_engineer(self.d_train)\n        X_test = fe.feature_engineer(self.d_test)\n        \n        # Preprocesing data\n        pre = preprocesingEngineer()\n        X_train = pre.to_string(X_train)\n        X_test = pre.to_string(X_test)\n\n        X_train = pre.encode_labels(X_train, X_test)\n        X_test = pre.encode_labels(X_test, X_train)\n\n        Y_train, X_train = pre.leave_final_cols(X_train, leave_target=True)\n        X_test = pre.leave_final_cols(X_test)\n        \n        # Modeling data \n        me = modelingEngineer()\n\n        # Fitting the model and getting predictions\n        y_preds, feature_importances = me.fit_model(X_train, Y_train, X_test)\n        \n        # Ploting the most important features\n        fig, ax = plt.subplots(figsize=(15, 20))\n        me.plot_feature_importance(feature_importances)\n    \n        # Forecasting the test set and creating file for submission\n        d_upload = pd.DataFrame({\n            'TransactionID': self.d_test['TransactionID'],\n            'isFraud': y_preds})\n        \n        return d_upload","1396fbfa":"# Fitting the pipeline\npipe = pipeline()\nd_upload = pipe.pipeline()\n\n# Saving for uplaod\nd_upload.to_csv('submission.csv', index=False)","888ac9b7":"# Reading data","6a1c8a36":"We will create a dict with the information about every feature. ","12e8d51a":"# Preprocesing class","826663ae":"The pipeline is as follows: \nFeature engineering -> feature preprocesing -> data modelling. \n\nThe output of the pipeline is the .csv file to upload to the competition.","8625e58a":"# Package imports","06bbf9bc":"# Pipeline class","fd8a1bad":"# Modelling class","d40a0cd1":"# Short EDA","6a747969":"Class that handles hyper parameter tuning and fitting the model. If we use GPU then the model of choice is xgboost, otherwise we use lightgbm.","1ee3712c":"## Ploting numeric features","2149a74b":"As we can see the data is unbalanced where more than 90 percent of the data falls in one class.","bbea154b":"As we can see from the head of the data, the features are messy with lots of missing values. Some of them are numeric while others seems to be factorial.","d83d636a":"## Feature analysis","e9099209":"## Distribution of Y","fa64da24":"The variable 'isFraud' is binary indicating whether the transaction if fraudulent or not. ","9b390adc":"A class to preproces data. The main tasks is to encode categorical features for the models.","8c64b191":"# Feature engineering class","012afdea":"A class to create new features from existing one. Because xgboost and lightgbm treat missing values as part of the features we do not need to worry about missing values too much. ","8eff0283":"## Running the pipeline"}}