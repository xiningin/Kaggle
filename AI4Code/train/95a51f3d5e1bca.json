{"cell_type":{"b582d8a1":"code","2a068cc0":"code","45432d23":"code","a7d53d00":"code","21048b79":"code","47d051fd":"code","f8a509ca":"code","c4955131":"code","900af04d":"code","9b038d1c":"code","f03491cf":"code","840485b2":"code","00d27158":"code","2965e0ea":"code","0a2e9a0c":"code","7e4d8b9a":"code","e842840c":"markdown","51ff7fa4":"markdown","ee94638e":"markdown","d5ab0d04":"markdown","64c7c137":"markdown","438cb552":"markdown"},"source":{"b582d8a1":"# import required packages here, like pytorch, numpy and etc.","2a068cc0":"from tqdm.notebook import tqdm","45432d23":"assert torch.cuda.is_available()","a7d53d00":"train_dataset = CIFAR10(root='\/kaggle\/working\/cifar10', download=True, train=True)\ntest_dataset = CIFAR10(root='\/kaggle\/working\/cifar10', download=True, train=False)","21048b79":"class MyConvNet(nn.Module):\n    def __init__(self, num_classes: int):\n        super(MyConvNet, self).__init__()\n        \n        self.layers = nn.Sequential(\n            # your conv layers here\n            # requirement:\n            # 1. input channel: 3\n            # 2. output channel: 64\n            # 3. kernel size: 3\n            # 4. activatoin function: ReLU\n        )\n        \n        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(64, num_classes)\n        \n    def forward(self, x):\n        out = self.layers(x)\n        out = self.avg_pool(out)\n        out = out.flatten()\n        out = self.fc(out)\n        \n        return out\n        ","47d051fd":"# define dataloader here, named `train_loader` and `test_loader` respectively","f8a509ca":"model = MyConvNet(num_classes=10)\nmodel.cuda()","c4955131":"# define optimizer\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-3, momentum=0.9)","900af04d":"criterion = nn.CrossEntropyLoss()","9b038d1c":"model.train() # important","f03491cf":"for epoch in range(50):\n    for x, y in tqdm(train_loader, desc=f'EPOCH [{epoch+1}\/50]'):\n        x, y = x.cuda(non_blocking), y.cuda(non_blocking)\n        \n        # add training scripts here\n        # 1. compute loss\n        # 2. clean old grads for the optimizer\n        # 3. invoke `backward()` for the loss\n        # 4. invoke `step()` to optimize","840485b2":"model.eval() # important","00d27158":"preds = []\ntruths = []\n\nfor x, y in tqdm(test_loader):\n    x = x.cuda(non_blocking)\n    \n    with torch.no_grad():\n        out = model(x)\n        y_pred = torch.argmax(out, dim=-1).cpu().numpy()\n        preds.append(y_pred)\n        truths.append(y.numpy())\n    \npreds = np.concatenate(preds, axis=0).reshape(-1)\ntruths = np.concatenate(truths, axis=0).reshape(-1)","2965e0ea":"from sklearn.metrics import accuracy_score","0a2e9a0c":"acc = accuracy_score(truths, preds)","7e4d8b9a":"print('Accuracy is', acc)","e842840c":"# Load Dataset","51ff7fa4":"# Training Procedure","ee94638e":"# Prepare for Training","d5ab0d04":"# Evaluation Procedure","64c7c137":"# Model Definition","438cb552":"# Import Packages"}}