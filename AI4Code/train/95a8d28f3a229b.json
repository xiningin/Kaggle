{"cell_type":{"086d8f3f":"code","829a3607":"code","5bfc75fe":"code","cccf906d":"code","57b5d7fc":"code","f183aae0":"code","462bd6a5":"code","2251dd32":"code","36f8662b":"code","826d1e25":"code","d2ecef02":"code","e19d8b66":"code","ac452ffa":"code","719693ab":"code","2e247056":"code","f27b8ca4":"code","0118b307":"code","078aeaf6":"code","eb677771":"code","b29eb4fe":"code","0f3996f3":"code","436d57fd":"code","3e4ce91e":"code","11c28bd0":"code","e6a16ace":"code","a2f80f7f":"code","520cb266":"code","742463f0":"code","d518bffc":"code","95b5cf86":"code","3cb7d8d3":"code","593a22a6":"code","e3bc5de3":"code","dae1a12a":"code","661a3728":"code","69aa5320":"code","f454a520":"code","209e457e":"code","cfee0692":"code","261edbd8":"code","dbbe2f39":"code","395f1247":"code","60f35635":"code","a6635cad":"code","8fa23b34":"code","9c9e44d1":"code","05c3b4de":"code","a87288fc":"code","114ea584":"code","6df4d32a":"code","17d0c751":"code","a1aa4d98":"code","2faa4899":"code","41bffadd":"code","a944eb1e":"code","c52e6304":"code","11f0aad8":"code","d2cf96b7":"code","bf860def":"code","3ae4d65d":"code","c7c140e4":"code","f582d6bb":"markdown","b675384b":"markdown","fdddf8b9":"markdown","aec0bbe9":"markdown","87cd3289":"markdown","08e62ac4":"markdown","615bdbdf":"markdown","f1973fe2":"markdown","32b464bc":"markdown","ca76e140":"markdown","b072e84c":"markdown","508511f0":"markdown","bf86bddb":"markdown","f2a8478d":"markdown","53b0d606":"markdown","32931d8a":"markdown","c6b10fbf":"markdown","a2693595":"markdown","eb867a73":"markdown","568c210d":"markdown","a7b8f292":"markdown","23b235b0":"markdown","d783c6c0":"markdown","2385d2b5":"markdown","3775f27b":"markdown","68744b9f":"markdown","89003572":"markdown","3eeb5c4c":"markdown","96d85025":"markdown","62c670b8":"markdown","8f940431":"markdown","cefa5273":"markdown","387d28d4":"markdown","c8c03608":"markdown","49eb4def":"markdown","e194e5bf":"markdown","8be4bb5d":"markdown","bf759ec5":"markdown","8b4318db":"markdown","b25a5295":"markdown","38f5c5b5":"markdown","d83d2c79":"markdown","97cf0bf3":"markdown","6ea7e7e8":"markdown","8ce60226":"markdown"},"source":{"086d8f3f":"import pandas as pd\nfrom pandas_profiling import ProfileReport\nimport matplotlib.pyplot as plt\nimport seaborn as sns","829a3607":"df = pd.read_csv('..\/input\/nlp-getting-started\/train.csv', index_col='id')\ndf.info()","5bfc75fe":"df.shape","cccf906d":"df.isnull().sum()","57b5d7fc":"with pd.option_context('display.max_colwidth', 400):\n    print(df.sample(5))","f183aae0":"cat_cols = list(df.select_dtypes(include='object').columns)\nnum_cols = set(df.columns) - set(cat_cols)\nprint(cat_cols)\nprint(list(num_cols))","462bd6a5":"profile = ProfileReport(df, title=\"Disaster Tweets Dataset - Uncleaned\", explorative=True)\nprofile.to_file(\"report_uncleaned.html\")\nwith pd.option_context('display.max_colwidth', 400):\n    profile.to_widgets()","2251dd32":"df['target'].value_counts()","36f8662b":"df['target'].value_counts().plot.pie()\nplt.title('Target Value')\nplt.legend(['No Disaster', 'Disaster'], bbox_to_anchor=(1.5, 1))\nplt.ylabel('')\nplt.show()","826d1e25":"with pd.option_context('display.max_colwidth', 400):\n    print(df[df['target'] == 1]['text'].sample(5))","d2ecef02":"with pd.option_context('display.max_colwidth', 400):\n    print(df[df['target'] == 0]['text'].sample(5))","e19d8b66":"df[df['target'] == 0]['keyword'].value_counts()","ac452ffa":"keyword_count_nondisaster = df[df['target'] == 0]['keyword'].value_counts()\nkeyword_count_disaster= df[df['target'] == 1]['keyword'].value_counts()\nprint(keyword_count_nondisaster)\nprint(keyword_count_disaster)","719693ab":"from wordcloud import WordCloud\n\nwordcloud_nondisaster = WordCloud(max_font_size=40).generate_from_frequencies(keyword_count_nondisaster)\nwordcloud_disaster = WordCloud(max_font_size=40).generate_from_frequencies(keyword_count_disaster)\n\nplt.figure(figsize=(20, 20), dpi=100)\nplt.subplot(1,2,1)\nplt.imshow(wordcloud_nondisaster)\nplt.title('No Disaster')\nplt.axis(\"off\")\nplt.subplot(1,2,2)\nplt.imshow(wordcloud_disaster)\nplt.title('Disaster')\nplt.axis(\"off\")\nplt.show()","2e247056":"df[df['target']==0]['keyword'].value_counts()","f27b8ca4":"df[df['target']==1]['keyword'].value_counts()","0118b307":"df['keyword_target_mean'] = df.groupby('keyword')['target'].transform('mean')\ndf.sort_values(by=['keyword_target_mean'], ascending=False).head(5)","078aeaf6":"keyword_mean = df.groupby('keyword')['keyword_target_mean'].first().sort_values(ascending=False)\nkeyword_mean","eb677771":"fig = plt.figure(figsize=(8, 72), dpi=100)\nax = sns.barplot(x=keyword_mean.values, y=keyword_mean.index)\nplt.xlabel('target mean')\nax.xaxis.set_label_position('top')\nax.xaxis.set_ticks_position('top')\nplt.grid()\nplt.title('Target Mean by Keyword')\nplt.show()","b29eb4fe":"fig = plt.figure(figsize=(8, 72), dpi=100)\nax = sns.countplot(y=df.sort_values(by='keyword_target_mean', ascending=False)['keyword'],\n              hue=df.sort_values(by='keyword_target_mean', ascending=False)['target'])\nax.xaxis.set_label_position('top')\nax.xaxis.set_ticks_position('top')\nplt.title('Number of Target Values by Keyword')\nplt.legend(['No Disaster', 'Disaster'], loc='upper right', bbox_to_anchor=(1.3, 1))\nplt.grid()\nplt.show()","0f3996f3":"df['location'].unique()","436d57fd":"df['location'].value_counts()","3e4ce91e":"df['location_target_mean'] = df.groupby('location')['target'].transform('mean')\ndf.sort_values(by=['location_target_mean'], ascending=False).head(5)","11c28bd0":"df.groupby('location')['location_target_mean'].agg(mean_='first', sum_='sum', count_='count').sort_values(by='sum_', ascending=False)","e6a16ace":"df = pd.read_csv('..\/input\/nlp-getting-started\/train.csv', index_col='id')","a2f80f7f":"df.drop(columns=['location'], inplace=True)\ndf.info()","520cb266":"df.dropna(inplace=True)\ndf.info()","742463f0":"df[~df['keyword'].str.isalpha()]","d518bffc":"import re\n(lambda s: re.sub('[^a-zA-Z]+', '', s))('wild%20fires')","95b5cf86":"df['keyword'] = df['keyword'].str.replace('[^a-zA-Z]+', '', regex=True)\ndf[~df['keyword'].str.isalpha()]","3cb7d8d3":"(lambda x: ' '.join(x.split()))(' test   hallo   ')","593a22a6":"df['text'] = df['text'].apply(lambda x: ' '.join(x.split()))","e3bc5de3":"import re\n\ndf['text_num_words'] = df['text'].apply(lambda s: len(s.split()))\ndf['text_len'] = df['text'].str.len()\ndf['text_avg_len_per_word'] = df['text_len'] \/ df['text_num_words']\ndf['text_num_hashtag'] = df['text'].str.count('#')\ndf['text_num_mentions'] = df['text'].str.count('@')\ndf['text_num_links'] = df['text'].apply(lambda x: x.count('http') + x.count('https') + x.count('www'))\ndf['text_len_non_alpha'] = df['text'].apply(lambda x: len(re.sub('[0-9a-zA-Z]+', '', x)))\ndf.head()","dae1a12a":"df.describe()","661a3728":"profile = ProfileReport(df, title=\"Disaster Tweets Dataset - Cleaned\", explorative=True)\nprofile.to_file(\"report_cleaned.html\")\nprofile.to_widgets()","69aa5320":"cols = ['text_num_words', 'text_num_hashtag', 'text_num_mentions', 'target']\nsns.pairplot(df[cols])\nplt.show()","f454a520":"import math\ncols = [c for c in df.columns if 'text_' in c]\nplt.figure(figsize=(10, 15), dpi=100)\nlayout_cols = 4\nlayout_rows = len(cols)\nfor i, c in enumerate(cols):\n    plt.subplot(layout_rows, layout_cols, layout_cols*i+1)\n    sns.histplot(data=df, x=c, hue='target', stat='probability', common_norm=False)\n    plt.grid()\n    plt.subplot(layout_rows, layout_cols, layout_cols*i+2)\n    sns.histplot(data=df, x=c, hue='target', stat='probability', cumulative=True, common_norm=False)\n    plt.grid()\n    plt.subplot(layout_rows, layout_cols, layout_cols*i+3)\n    sns.boxplot(data=df, y=c, x='target')\n    plt.grid()\n    plt.subplot(layout_rows, layout_cols, layout_cols*i+4)\n    sns.violinplot(data=df, y=c, x='target')\n    plt.grid()\nplt.tight_layout()","209e457e":"df['key_text'] = df['keyword'] + ' ' + df['text']","cfee0692":"df['key_text']","261edbd8":"from sklearn.model_selection import train_test_split\nX = df['key_text']\ny = df['target']\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.33, random_state=42)","dbbe2f39":"import spacy\nnlp = spacy.load('en_core_web_sm')","395f1247":"def my_tokenizer(sentence):\n    doc = nlp(sentence)\n    lemmas = [token.lemma_.lower() for token in doc if not token.is_stop and token.lemma_.isalpha()]\n    return lemmas","60f35635":"s = \"13,000People are whitnessing #wildfires see wwwEvacuation and www.google.de\"\nmy_tokenizer(s)","a6635cad":"from sklearn.feature_extraction.text import CountVectorizer\nbow_vectorizer = CountVectorizer(tokenizer=my_tokenizer, ngram_range=(1,1), max_features=1000)","8fa23b34":"X_train_bow = bow_vectorizer.fit_transform(X_train)\nX_val_bow = bow_vectorizer.transform(X_val)","9c9e44d1":"X_train_bow_df = pd.DataFrame(X_train_bow.toarray(), columns=bow_vectorizer.get_feature_names(), index=X_train.index).add_prefix('BOW_')\nX_val_bow_df = pd.DataFrame(X_val_bow.toarray(), columns=bow_vectorizer.get_feature_names(), index=X_val.index).add_prefix('BOW_')\nX_train_bow_df","05c3b4de":"X_train_bow_df.sum().sort_values(ascending=False)","a87288fc":"from sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression()\nclassifier.fit(X_train_bow_df, y_train)","114ea584":"print(f'Training Accuracy: {classifier.score(X_train_bow_df, y_train):.2f}')\nprint(f'Validation Accuracy: {classifier.score(X_val_bow_df, y_val):.2f}')","6df4d32a":"from sklearn.metrics import plot_confusion_matrix\nplot_confusion_matrix(classifier, X_val_bow_df, y_val, normalize='true', cmap=plt.cm.Blues)\nplt.show()","17d0c751":"from pprint import pprint\nfrom sklearn.metrics import confusion_matrix\n\nCM = confusion_matrix(y_val, classifier.predict(X_val_bow_df))\n\nTN = CM[0][0]\nFN = CM[1][0]\nTP = CM[1][1]\nFP = CM[0][1]\n\nscores = {\n    'TPR': TP \/ (TP + FN), # recall,\n    'FPR': FP \/ (FP + TN),\n    'PPV': TP \/ (TP + FP), # precision,\n    'ACC': (TP + TN) \/ (TN + FN + TP + FP),\n}\n\npprint(scores)","a1aa4d98":"from sklearn.metrics import plot_roc_curve\nplot_roc_curve(classifier, X_val_bow_df, y_val)\nplt.plot(scores['FPR'], scores['TPR'], 'or')\nplt.text(scores['FPR'] + 0.05, scores['TPR'] - 0.01, 'current configuration, threshold of 0.5')\nplt.show()","2faa4899":"from sklearn.metrics import f1_score\nf1_score(y_val, classifier.predict(X_val_bow_df))","41bffadd":"s = 'worrying sinkhole'\nX_new = bow_vectorizer.transform([s]).toarray()\nprint(classifier.predict(X_new)[0])\nprint(classifier.predict_proba(X_new)[0])","a944eb1e":"coefficients = pd.DataFrame({\"word\": list(bow_vectorizer.get_feature_names()), \"coeff\": classifier.coef_[0]})\ncoefficients.sort_values(by=['coeff'], ascending=False).head(10)","c52e6304":"coefficients.sort_values(by=['coeff'], ascending=False).tail(10)","11f0aad8":"len(classifier.coef_[0])","d2cf96b7":"classifier.intercept_[0]","bf860def":"coefficients[coefficients['word'].isin(['worry', 'sinkhole'])]","3ae4d65d":"import numpy as np\n\ndef sigmoid(x):\n    return 1 \/ (1 + math.e ** -x)\n\ndef get_proba(classifier, bow_vectorizer, sentence):\n    X_new = bow_vectorizer.transform([sentence]).toarray()[0]\n    \n    # Not needed for calculation of final prediction value, just to get more insights on the contribution of the single words\n    found_words = [i for (i, v) in zip(bow_vectorizer.get_feature_names(), X_new) if v == 1]\n    df_coefficients = pd.DataFrame({\"word\": list(bow_vectorizer.get_feature_names()), \"coeff\": classifier.coef_[0]})\n    print(df_coefficients[df_coefficients['word'].isin(found_words)])\n    print(f'bias: {classifier.intercept_[0]}')\n    \n    logit = np.dot(X_new, classifier.coef_[0]) + classifier.intercept_[0]\n    print(f'logit: {logit}')\n    print(f'after sigmoid: {sigmoid(logit)}')\n    return sigmoid(logit)","c7c140e4":"get_proba(classifier, bow_vectorizer, 'worrying sinkhole')","f582d6bb":"### Drop rows with missing values","b675384b":"### How Logistic Regression Works","fdddf8b9":"Now that we looked into our data, we need to clean it up next.","aec0bbe9":"### Closer look at the \"text\" column by target value","87cd3289":"## Data Investigation","08e62ac4":"### Location influence","615bdbdf":"### Examplary Samples","f1973fe2":"### Logistic Regression Model","32b464bc":"### Number of Target Values by Keyword","ca76e140":"### Comparing distributions for target","b072e84c":"## Data Cleaning","508511f0":"### Some first quick insights","bf86bddb":"**Question 4**\n\nDo you think that we should use the `country` column as a feature for our model?\n1. Yes\n2. No\n\nThis might be controversial. However looking at the high cardinality of `3341` unique entries of total `7613` samples, the `country` column is probably going to generalize very poorly. One however could think of replacing the rare values with a `rare` placeholder and besides that only take the unique values which appear more often than a certain threshold (e.g 20).\n\nAnother idea might be to extract the country or the region from the `location` column. Like this, we might get down to fewer unique values.\n","f2a8478d":"### Bag Of Words","53b0d606":"## Prerequisities","32931d8a":"**Question 2 - Which column has the most absolute number of missing values?**\n1. `keyword`\n2. `location`\n3. `text`\n4. `target`","c6b10fbf":"Let's split the data into train and val.","a2693595":"### Remove duplicate whitespace characters","eb867a73":"### Idea","568c210d":"### Remove non alphanumeric entries from Keyword","a7b8f292":"# Disaster Tweets","23b235b0":"**Open Question 2**\n\nWhat are the possible downsides of a bag of words model?\nPlease unmute yourself and share your thoughts.\n\n- high dimensional, sparse sentence vectors\n- No contextual information considered, just the notion of the presence of a word without any sense of place inside the sentence. Take the following two examples which would result in the same output vectors: \"I like cats and hate dogs\" and \"I hate cats and like dogs\". Clearly they are quite the opposite.","d783c6c0":"**Question 3**\nDoes above result directly indicate that `derailment` is certain to be a good hint for a disaster?\n1. Yes\n2. No\n\nNo, `derailment` is not certain to be a good hint for a disaster since we are only looking at absolute number counts. One should also take into account the fraction that the keyword `derailment` was of a `disaster` target.","2385d2b5":"### Keyword count by target","3775f27b":"### Model Explainability","68744b9f":"### Distribution of our target","89003572":"**Question 1 - What is the overall shape of our dataset (number rows, number columns)?**\n1. (7552 x 3) \n2. (7552 x 4)\n3. (5080 x 3)\n4. (7613 x 4)","3eeb5c4c":"### Look again at Pandas Profiling Output","96d85025":"**Open Question 1 - Why is it important that the target is equally distributed?**\n\n- for unbalanced datasets, the prediction of the model will most likely be biased towards the majority class. This is due to the fact that the model tries to minimize the training error on the entire training dataset. Each sample is equally important here. If the training data is unbalanced however, the majority class will come into play much more often.\n- one easily can be fooled looking at the accuracy metric only when evaluating the performance of our model: Think of a dataset about a very rare disease (0.001%), meaning we have 1 sample for the \"disease\" case, and 100000 - 1 samples for the \"non-disease\" case. Given we have a naive model that always predicts a \"non-disease\" target, this model will have a accuracy of 99.999%. Nonetheless, the model fails to correctly classify the \"disease\" cases (which to be fair is what our main intention ist). Therefore, we need to make sure to use other metrics like recall, precision, f1 when evaluating the performance on unbalanced datasets.\n\nRead more at https:\/\/medium.com\/strands-tech-corner\/unbalanced-datasets-what-to-do-144e0552d9cd and https:\/\/twitter.com\/svpino\/status\/1357302018428256258?lang=en for more information.","62c670b8":"### Meta Information","8f940431":"### Pairplot","cefa5273":"Since we are using a bag of words model, let's simply add the keyword to the text.","387d28d4":"### Pandas Profiling","c8c03608":"**Interactive Coding 2**\n\nTask: Create a table with unique entries in the `keyword` column and their number counts.","49eb4def":"> Twitter has become an important communication channel in times of emergency.\n> The ubiquitousness of smartphones enables people to announce an emergency they\u2019re observing in real-time. Because of this, more agencies are interested in programatically monitoring Twitter (i.e. disaster relief organizations and news agencies).\n> \n> But, it\u2019s not always clear whether a person\u2019s words are actually announcing a disaster.","e194e5bf":"### Word Clouds","8be4bb5d":"## Feature Engineering","bf759ec5":"**Question 6**\n\n![grafik.png](attachment:c0f793b4-0199-48f6-b014-d946bd10234f.png)\n\nFor the `text_len` distribution, which first preporcessing step would make the most sense?\n1. Normalize the data using a Gaussian Distribution --> `StandardScaler`\n2. Rescale the values to be between 0 and 1 using min and max values --> `MinMaxScaler`\n3. Apply a log transformation --> `PowerTransformer`\n4. Remove outliers cutting off everything above and below 3 standard deviations\n\nAnswer 3 or generally 4 is correct. Applying a `StandardScaler` as well as a `MinMaxScaler` works badly for skewed distributions. Additionally we do not have any severe outliers in this concrete example. Overall one should be aware of treating outliers as a first step before any normalization though.\n","8b4318db":"**Question 5**\n\nWhat piece of code correctly computes the number of words for each entrie in the `text` column?\n1. `df['text'].str.len`\n2. `df['text'].count()`\n3. `df['text'].apply(lambda s: len(s.split()))`\n4. `df['text'].apply(lambda s: len(s)`","b25a5295":"We want to start with getting an overview about our dataset:\n* Shape\n* Datatypes\n* Missing Values\n* Distributions\n* Cardinality\n* \"Dirty\" Data","38f5c5b5":"Pandas Profiling helps you a lot with data exploration:","d83d2c79":"**Question 6**\n\nWhat will be the result of the following cell?\n1. `['people', 'be', 'whitnesse', 'wildfire', 'see', 'wwwevacuation', 'and']`\n2. `['whitnesse', 'wildfire', 'wwwevacuation']`\n3. `['people', 'whitnessing', 'wildfires', 'wwwevacuation']`\n4. `['people', 'whitnesse', 'wildfire', 'evacuation']`","97cf0bf3":"### Drop the location column","6ea7e7e8":"### Target mean by Keyword","8ce60226":"**Interactive Coding 1 - Numeric and Categorical Columns**\n\nTask: Get column names of numerical and categorical columns."}}