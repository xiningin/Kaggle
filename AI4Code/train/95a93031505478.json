{"cell_type":{"d9fb73ff":"code","1fa67281":"code","a277f357":"code","78208644":"code","e54900ea":"code","2fff9142":"code","b42b387b":"code","7c766cab":"code","117d77ab":"code","af61f1eb":"code","d3df2c44":"code","660c697b":"code","7d5675ec":"code","cf33475d":"code","e4550450":"code","8d00afd6":"code","0c104331":"code","8aea2873":"code","c6094e20":"code","35b7c5b3":"code","f6fc7966":"code","fc2ed9c3":"code","7c49a801":"code","49f3b0b8":"code","48cf6d3b":"code","329c2d16":"code","7bdb07dd":"code","053f16ca":"code","c4841eb8":"code","8f488f76":"code","9b360efd":"code","6bfe07ed":"code","bc05c440":"code","75a83786":"code","993577bf":"code","9c3dffea":"code","2f7a7234":"code","dd07fbe6":"code","29e84f23":"code","34c12390":"code","1d8df3ba":"code","32298949":"code","d9e6474c":"code","971293e6":"code","e19155c2":"code","8250578b":"code","b3040c72":"code","df60d7ee":"code","2901cae3":"code","ab3b9415":"code","4c0fb23c":"code","2247fed4":"code","11e3a580":"code","998db198":"code","ce04afc4":"code","61c3500b":"code","b410bad5":"code","ffb6f244":"code","7e9af5d9":"code","8f5281ef":"code","0d31ddc1":"code","18f2979b":"code","a7f27b1d":"code","1ba9c5d5":"code","3dfc97d1":"code","3a1c184d":"code","0dd5404f":"code","95035d09":"markdown","81d98dc9":"markdown","464fc30d":"markdown","977c7b7b":"markdown","5c92b468":"markdown","a2d044f5":"markdown","b7ef25f0":"markdown","1f4a6e6d":"markdown","563d171c":"markdown","aa43f5cf":"markdown","39bf5880":"markdown","460cd031":"markdown","755dce9e":"markdown","35c638dc":"markdown","adc5288a":"markdown","04f3278e":"markdown","69e33f03":"markdown","e825f011":"markdown","073c4cab":"markdown","30849b42":"markdown","bb50523a":"markdown","6ce92ee4":"markdown","fdb25c5d":"markdown","191d5e89":"markdown","dd3c8543":"markdown","96bebb6b":"markdown","6d4c6887":"markdown","26197ab0":"markdown","c90a62b1":"markdown","fca3201c":"markdown","29f0993a":"markdown","7219206d":"markdown","a26792d3":"markdown","1970556c":"markdown","7fb47a94":"markdown","56a03da6":"markdown","346d5174":"markdown","a6df96f9":"markdown","9bc1a278":"markdown","ef9b37a0":"markdown","d72a4528":"markdown","8268517b":"markdown","813009a6":"markdown","065e5f6e":"markdown","c6ef3b11":"markdown","869c820a":"markdown","d3d891a3":"markdown","517bd7fe":"markdown","ef6323bb":"markdown","452c6631":"markdown","db7fe3fb":"markdown","efef8f9d":"markdown","10d11ff0":"markdown","7d892c19":"markdown","df5e0b45":"markdown","a4de10e5":"markdown","54fab1bb":"markdown","c860b62a":"markdown","e42e755e":"markdown","cb359ef5":"markdown","9686eee8":"markdown","d60b4392":"markdown","179e4197":"markdown","b4b9be4f":"markdown","1aabf2e4":"markdown"},"source":{"d9fb73ff":"def splitfeatures(X):\n  X_num=X.describe().columns\n  X_cat=X.columns.difference(X_num)\n  X_num=X_num.tolist()\n  X_cat=X_cat.tolist()\n  print(\"Numeric features:      \",X_num)\n  print(\"Categorical features:  \",X_cat)\n\ndef numfeatures(X):\n    return(X.describe().columns.tolist())\n\ndef catfeatures(X):\n    X_num=X.describe().columns\n    return(X.columns.difference(X_num).tolist())\n\ndef plot_distribution( df , var , target , **kwargs ):\n    row = kwargs.get( 'row' , None )\n    col = kwargs.get( 'col' , None )\n    facet = sns.FacetGrid( df , hue=target , aspect=4 , row = row , col = col )\n    facet.map( sns.kdeplot , var , shade= True )\n    facet.set( xlim=( 0 , df[ var ].max() ) )\n    facet.add_legend()\n    \ndef plot_categories( df , cat , target , **kwargs ):\n    row = kwargs.get( 'row' , None )\n    col = kwargs.get( 'col' , None )\n    facet = sns.FacetGrid( df , row = row , col = col )\n    facet.map( sns.barplot , cat , target )\n    facet.add_legend()\n    \ndef missing_percentage(df):\n    total = df.isnull().sum().sort_values(ascending = False)\n    percent = round(df.isnull().sum().sort_values(ascending = False)\/len(df)*100,2)\n    return pd.concat([total, percent], axis=1, keys=['Total','Percent'])\n\ndef percent_value_counts(df, feature):\n    percent = pd.DataFrame(round(df.loc[:,feature].value_counts(dropna=False, normalize=True)*100,2))\n    total = pd.DataFrame(df.loc[:,feature].value_counts(dropna=False))\n    total.columns = [\"Total\"]\n    percent.columns = ['Percent']\n    return pd.concat([total, percent], axis = 1)\n\ndef unique_values_in_column(data,feature):\n    unique_val=pd.Series(data.loc[:,feature].unique())\n    return pd.concat([unique_val],axis=1,keys=['Unique Values'])","1fa67281":"#Data Processing\nimport numpy as np \nimport pandas as pd\n\n#Visualization\nimport matplotlib.pyplot as plt\nfrom matplotlib import pyplot\nimport seaborn as sns\nsns.set(style=\"whitegrid\")\n%matplotlib inline\n        \n#Warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","a277f357":"train=pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest=pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntrain.sample(5)","78208644":"splitfeatures(train)\nprint(\"=\"*100)\nsplitfeatures(test)","e54900ea":"train.info()\nprint(\"=\"*100)\nprint(\"=\"*100)\ntest.info()","2fff9142":"train.describe(include=\"all\")","b42b387b":"missing_percentage(train)","7c766cab":"test.describe(include=\"all\")","117d77ab":"missing_percentage(test)","af61f1eb":"print(\"Numeric features: \",numfeatures(train))","d3df2c44":"df_num=train[numfeatures(train)]\ncorr=df_num.corr()\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\nf, ax = plt.subplots(figsize=(12,6))\ncmap = sns.color_palette(\"PRGn\",10)\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=1,vmin=-1, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5},annot=True)","660c697b":"train_num=train[numfeatures(train)]\nsns.set(style=\"white\")\nsns.pairplot(train_num,corner=True)","7d5675ec":"train.sample(5)","cf33475d":"sns.set(style=\"whitegrid\")\nfig, ax = pyplot.subplots(figsize=(12,6))\ncolors=[\"#80CEE1\",\"#FFB6C1\"]\ncustomPalette=sns.set_palette(sns.color_palette(colors))\nprint(train[[\"Sex\",\"Survived\"]].groupby([\"Sex\"],as_index=False).mean())\nsns.barplot(x=\"Sex\",y=\"Survived\",data=train,ax=ax,palette=customPalette)\nax.spines[\"top\"].set_visible(False)\nax.spines[\"right\"].set_visible(False)","e4550450":"fig, ax = pyplot.subplots(figsize=(12,6))\nprint(train[[\"Pclass\",\"Survived\"]].groupby([\"Pclass\"],as_index=False).mean())\nsns.barplot(x=\"Pclass\",y=\"Survived\",data=train,hue=\"Sex\",ax=ax,palette=customPalette)\ncolors=[\"#80CEE1\",\"#FFB6C1\"]\ncustomPalette=sns.set_palette(sns.color_palette(colors))\nax.spines[\"top\"].set_visible(False)\nax.spines[\"right\"].set_visible(False)","8d00afd6":"percent_value_counts(train,\"Name\")","0c104331":"import re\ndef get_title(name):\n    title_search = re.search(' ([A-Za-z]+)\\.', name)\n    if title_search:\n        return title_search.group(1)\n    return \"\"\n\n# Apply get_title function\ntrain['Title'] = train['Name'].apply(get_title)\ntest['Title'] = test['Name'].apply(get_title)\n\npercent_value_counts(train,\"Title\")","8aea2873":"train['Title'] = train['Title'].replace(['Lady', 'Countess','Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\ntrain['Title'] = train['Title'].replace('Mlle', 'Miss')\ntrain['Title'] = train['Title'].replace('Ms', 'Miss')\ntrain['Title'] = train['Title'].replace('Mme', 'Mrs')\n\ntest['Title'] = test['Title'].replace(['Lady', 'Countess','Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\ntest['Title'] = test['Title'].replace('Mlle', 'Miss')\ntest['Title'] = test['Title'].replace('Ms', 'Miss')\ntest['Title'] = test['Title'].replace('Mme', 'Mrs')\n\nfig, ax = pyplot.subplots(figsize=(12,6))\nprint(train[['Title','Survived']].groupby(['Title'], as_index=False).mean())\nsns.barplot(x=\"Title\", y=\"Survived\", data=train,hue=\"Sex\",ax=ax)\ncolors=[\"#80CEE1\",\"#FFB6C1\"]\ncustomPalette=sns.set_palette(sns.color_palette(colors))\nax.spines[\"top\"].set_visible(False)\nax.spines[\"right\"].set_visible(False)","c6094e20":"train.drop(['Name'], axis=1, inplace=True)\ntest.drop(['Name'], axis=1, inplace=True)\ntrain.sample(5)","35b7c5b3":"print(train[[\"Age\",\"Survived\"]].groupby([\"Age\"],as_index=False).mean())\nfig, ax = pyplot.subplots(figsize=(12,6))\nsns.distplot(train.Age,ax=ax,color=\"blue\")\nax.spines[\"top\"].set_visible(False)\nax.spines[\"right\"].set_visible(False)","f6fc7966":"train[\"Age\"] = train[\"Age\"].fillna(-0.5)\nbins = [-1, 0, 5, 12, 18, 24, 35, 60, np.inf]\nlabels = ['Unknown', 'Baby', 'Child', 'Teenager', 'Student', 'Young Adult', 'Adult', 'Senior']\ntrain['AgeGroup'] = pd.cut(train[\"Age\"], bins, labels = labels)\n\ntest[\"Age\"] = test[\"Age\"].fillna(-0.5)\ntest['AgeGroup'] = pd.cut(test[\"Age\"], bins, labels = labels)\n\nfig, ax = pyplot.subplots(figsize=(12,6))\nsns.barplot(x=\"AgeGroup\", y=\"Survived\", data=train,hue=\"Sex\",ax=ax)\n\ncolors=[\"#80CEE1\",\"#FFB6C1\"]\ncustomPalette=sns.set_palette(sns.color_palette(colors))\nax.spines[\"top\"].set_visible(False)\nax.spines[\"right\"].set_visible(False)","fc2ed9c3":"train.drop(['Age'], axis=1, inplace=True)\ntest.drop(['Age'], axis=1, inplace=True)\ntrain.sample(5)","7c49a801":"fig, ax = pyplot.subplots(figsize=(12,6))\nprint(train[[\"SibSp\",\"Survived\"]].groupby([\"SibSp\"],as_index=False).mean())\nsns.barplot(x=\"SibSp\",y=\"Survived\",data=train,hue=\"Sex\",ax=ax)\nax.spines[\"top\"].set_visible(False)\nax.spines[\"right\"].set_visible(False)","49f3b0b8":"fig, ax = pyplot.subplots(figsize=(12,6))\nprint(train[[\"Parch\",\"Survived\"]].groupby([\"Parch\"],as_index=False).mean())\nsns.barplot(x=\"Parch\",y=\"Survived\",data=train,hue=\"Sex\",ax=ax)\n\nax.spines[\"top\"].set_visible(False)\nax.spines[\"right\"].set_visible(False)","48cf6d3b":"train['FamilySize'] = train['SibSp'] + train['Parch'] + 1\ntest['FamilySize'] = test['SibSp'] + test['Parch'] + 1\nfig, ax = pyplot.subplots(figsize=(12,6))\nprint(train[[\"FamilySize\",\"Survived\"]].groupby([\"FamilySize\"],as_index=False).mean())\nsns.barplot(x=\"FamilySize\",y=\"Survived\",data=train,ax=ax,hue=\"Sex\")\n\nax.spines[\"top\"].set_visible(False)\nax.spines[\"right\"].set_visible(False)","329c2d16":"train[\"IsAlone\"]=0\ntrain.loc[train[\"FamilySize\"] == 1,\"IsAlone\"]= 1\n\ntest[\"IsAlone\"]=0\ntest.loc[test[\"FamilySize\"] == 1,\"IsAlone\"]= 1\n\nprint(train[[\"IsAlone\",\"Survived\"]].groupby([\"IsAlone\"],as_index=False).mean())\nfig, ax = pyplot.subplots(figsize=(12,6))\nsns.barplot(x=\"IsAlone\",y=\"Survived\",data=train,ax=ax,hue=\"Sex\")\n\nax.spines[\"top\"].set_visible(False)\nax.spines[\"right\"].set_visible(False)","7bdb07dd":"train.drop(['Ticket'], axis=1, inplace=True)\ntest.drop(['Ticket'], axis=1, inplace=True)\ntrain.sample(5)","053f16ca":"print(train[[\"Fare\",\"Survived\"]].groupby([\"Fare\"],as_index=False).mean())\nfig, ax = pyplot.subplots(figsize=(12,6))\nsns.distplot(train.Fare,ax=ax,color=\"Blue\")\n\nax.spines[\"top\"].set_visible(False)\nax.spines[\"right\"].set_visible(False)","c4841eb8":"train['FareGroup'] = pd.qcut(train['Fare'], 4, labels=['A', 'B', 'C', 'D'])\ntest['FareGroup'] = pd.qcut(test['Fare'], 4, labels=['A', 'B', 'C', 'D'])\nprint(train[['FareGroup','Survived']].groupby(['FareGroup'], as_index=False).mean())\nprint(percent_value_counts(train,\"FareGroup\"))\nprint(percent_value_counts(test,\"FareGroup\"))\n\nfig, ax = pyplot.subplots(figsize=(12,6))\nsns.barplot(x=\"FareGroup\",y=\"Survived\",data=train,ax=ax,color=\"salmon\")","8f488f76":"train.drop(['Fare'], axis=1, inplace=True)\ntest.drop(['Fare'], axis=1, inplace=True)\ntrain.sample(5)","9b360efd":"test[\"FareGroup\"].fillna((\"B\"),inplace=True)","6bfe07ed":"missing_percentage(test)","bc05c440":"percent_value_counts(train,\"Cabin\")","75a83786":"pd.unique(train['Cabin'])","993577bf":"train[\"Cabin_Data\"] = train[\"Cabin\"].isnull().apply(lambda x: not x)\ntest[\"Cabin_Data\"] = test[\"Cabin\"].isnull().apply(lambda x: not x)\n\ntrain[\"Deck\"] = train[\"Cabin\"].str.slice(0,1)\ntrain[\"Room\"] = train[\"Cabin\"].str.slice(1,5).str.extract(\"([0-9]+)\", expand=False).astype(\"float\")\ntrain[\"Deck\"] = train[\"Deck\"].fillna(\"N\")\ntrain[\"Room\"] = round(train[\"Room\"].fillna(train[\"Room\"].mean()),0).astype(\"int\")\n\ntest[\"Deck\"] = test[\"Cabin\"].str.slice(0,1)\ntest[\"Room\"] = test[\"Cabin\"].str.slice(1,5).str.extract(\"([0-9]+)\", expand=False).astype(\"float\")\ntest[\"Deck\"] = test[\"Deck\"].fillna(\"N\")\ntest[\"Room\"] = round(test[\"Room\"].fillna(test[\"Room\"].mean()),0).astype(\"int\")\n\ntrain.sample(5)","9c3dffea":"percent_value_counts(train,\"Deck\")","2f7a7234":"fig, ax = pyplot.subplots(figsize=(12,6))\nsns.barplot(x=\"Deck\", y=\"Survived\", data=train,ax=ax,color=\"Salmon\")\n\nax.spines[\"top\"].set_visible(False)\nax.spines[\"right\"].set_visible(False)","dd07fbe6":"train['Room'].describe()","29e84f23":"bins = [0, 50, 75, 100,np.inf]\nlabels = [\"g1\",\"g2\",\"g3\",\"g4\"]\ntrain[\"RoomGroup\"] = pd.cut(train[\"Room\"], bins, labels = labels)\ntest[\"RoomGroup\"] = pd.cut(test[\"Room\"], bins, labels = labels)\ntrain.sample(5)","34c12390":"fig, ax = pyplot.subplots(figsize=(12,6))\nsns.barplot(x=\"RoomGroup\", y=\"Survived\", data=train,color=\"Salmon\")\n\nax.spines[\"top\"].set_visible(False)\nax.spines[\"right\"].set_visible(False)","1d8df3ba":"train.drop([\"Cabin\", \"Cabin_Data\", \"Room\"], axis=1, inplace=True)\ntest.drop([\"Cabin\", \"Cabin_Data\", \"Room\"], axis=1, inplace=True)\ntrain.sample(5)","32298949":"print(percent_value_counts(train,\"Embarked\"))\nprint(train[[\"Embarked\",\"Survived\"]].groupby([\"Embarked\"],as_index=False).mean())\n\nfig, ax = pyplot.subplots(figsize=(12,6))\nsns.barplot(x=\"Embarked\",y=\"Survived\",data=train,ax=ax,hue=\"Sex\")\n\nax.spines[\"top\"].set_visible(False)\nax.spines[\"right\"].set_visible(False)","d9e6474c":"train.Embarked.fillna(\"S\", inplace=True)\nmissing_percentage(train)","971293e6":"missing_percentage(test)","e19155c2":"train_backup=train\ntest_backup=test\n\ntrain.drop(\"SibSp\",axis=1,inplace=True)\ntest.drop(\"SibSp\",axis=1,inplace=True)\ntrain.drop(\"Parch\",axis=1,inplace=True)\ntest.drop(\"Parch\",axis=1,inplace=True)\n\nids=test[\"PassengerId\"]\n\ntrain.drop(\"PassengerId\",axis=1,inplace=True)\ntest.drop(\"PassengerId\",axis=1,inplace=True)\n\nX_train = train.drop(\"Survived\", axis=1)\ny_train = train[\"Survived\"]\nX_test  = test\nX_train.shape, y_train.shape, X_test.shape","8250578b":"import pickle\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n\n\nX_train=pd.get_dummies(X_train)\nX_test=pd.get_dummies(X_test)","b3040c72":"print(X_train.columns)\nprint(\"=\"*100)\nprint(X_test.columns)","df60d7ee":"X_train.drop(\"Deck_T\",axis=1,inplace=True)","2901cae3":"print(X_train.columns)\nprint(\"=\"*100)\nprint(X_test.columns)","ab3b9415":"from sklearn.preprocessing import StandardScaler\nscale = StandardScaler()\nX_train=scale.fit_transform(X_train)\nX_test=scale.fit_transform(X_test)\n\nnp.savez_compressed(\"np_savez_comp\", X=X_train, y=y_train)\ndata = np.load(\"np_savez_comp.npz\")\n\nX = data[\"X\"]\ny = data[\"y\"]","4c0fb23c":"from sklearn.model_selection import train_test_split\nX_train, X_valid, y_train, y_valid = train_test_split(X_train,y_train, test_size=0.2, random_state=0)","2247fed4":"from sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import confusion_matrix\n\ngnb = GaussianNB()\ngnb.fit(X_train, y_train)\n\nwith open('Naive Bayes.pickle', mode='wb') as fp:\n    pickle.dump(gnb, fp)\n    \nscore = gnb.score(X_valid, y_valid)\nprint('NB score: {}' .format(score))\n\npredicted=gnb.predict(X_valid)\nmatrix = confusion_matrix(y_valid, predicted)\nsns.heatmap(matrix, annot=True, fmt=\"d\", cmap='Blues', square=True)\nplt.xlabel(\"predicted\")\nplt.ylabel(\"actual\")\nplt","11e3a580":"from sklearn.linear_model import LogisticRegression\nlogistic_regression = LogisticRegression(random_state=0)\nlogistic_regression.fit(X_train, y_train)\n\nwith open('Logistic Regression.pickle', mode='wb') as fp:\n    pickle.dump(logistic_regression, fp)\n\nscore = logistic_regression.score(X_valid, y_valid)\nprint('LR score: {}' .format(score))\n\npredicted=logistic_regression.predict(X_valid)\nmatrix = confusion_matrix(y_valid, predicted)\nsns.heatmap(matrix, annot=True, fmt=\"d\", cmap='Blues', square=True)\nplt.xlabel(\"predicted\")\nplt.ylabel(\"actual\")\nplt","998db198":"from sklearn.svm import SVC\nsvm = SVC(kernel='linear', C=1.0, random_state=0)\nsvm.fit(X_train, y_train)\n\nwith open('Support Vector Machine.pickle', mode='wb') as fp:\n    pickle.dump(svm, fp)\n    \nscore = svm.score(X_valid, y_valid)\nprint('SVC linear score: {}' .format(score))\n\n\nsvm2 = SVC(kernel='rbf', C=1.0, random_state=0)\nsvm2.fit(X_train, y_train)\nscore = svm2.score(X_valid, y_valid)\nprint('SVC rbf score: {}' .format(score))\n\npredicted=svm.predict(X_valid)\nmatrix = confusion_matrix(y_valid, predicted)\nsns.heatmap(matrix, annot=True, fmt=\"d\", cmap='Blues', square=True)\nplt.xlabel(\"predicted\")\nplt.ylabel(\"actual\")\nplt","ce04afc4":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=5, p=2, metric='minkowski')\nknn.fit(X_train, y_train)\n\nwith open('Nearest Neighbors.pickle', mode='wb') as fp:\n    pickle.dump(knn, fp)\n    \nscore = knn.score(X_valid, y_valid)\nprint('KNN score: {}' .format(score))\n\npredicted=knn.predict(X_valid)\nmatrix = confusion_matrix(y_valid, predicted)\nsns.heatmap(matrix, annot=True, fmt=\"d\", cmap='Blues', square=True)\nplt.xlabel(\"predicted\")\nplt.ylabel(\"actual\")\nplt","61c3500b":"from sklearn.tree import DecisionTreeClassifier\ndecision_tree = DecisionTreeClassifier(criterion='entropy',random_state=0)\ndecision_tree.fit(X_train, y_train)\n\nwith open('Decision Tree.pickle', mode='wb') as fp:\n    pickle.dump(decision_tree, fp)\n    \nscore = decision_tree.score(X_valid, y_valid)\nprint('DT score: {}' .format(score))\n\npredicted=decision_tree.predict(X_valid)\nmatrix = confusion_matrix(y_valid, predicted)\nsns.heatmap(matrix, annot=True, fmt=\"d\", cmap='Blues', square=True)\nplt.xlabel(\"predicted\")\nplt.ylabel(\"actual\")\nplt","b410bad5":"from sklearn.ensemble import RandomForestClassifier\n\nrandom_forest = RandomForestClassifier(random_state=0)\nrandom_forest.fit(X_train, y_train)\n\nwith open('Random Forest.pickle', mode='wb') as fp:\n    pickle.dump(random_forest, fp)\n    \nscore = random_forest.score(X_valid, y_valid)\nprint('RF score: {}' .format(score))\n\npredicted=random_forest.predict(X_valid)\nmatrix = confusion_matrix(y_valid, predicted)\nsns.heatmap(matrix, annot=True, fmt=\"d\", cmap='Blues', square=True)\nplt.xlabel(\"predicted\")\nplt.ylabel(\"actual\")\nplt","ffb6f244":"from xgboost import XGBClassifier\n\nxgb = XGBClassifier(max_depth=3, learning_rate=0.1, n_estimators=100)\nxgb.fit(X_train, y_train)\n\nwith open('Gradient Boosting.pickle', mode='wb') as fp:\n    pickle.dump(xgb, fp)\n    \nscore = xgb.score(X_valid, y_valid)\nprint('score: {}' .format(score))\n\npredicted=xgb.predict(X_valid)\nmatrix = confusion_matrix(y_valid, predicted)\nsns.heatmap(matrix, annot=True, fmt=\"d\", cmap='Blues', square=True)\nplt.xlabel(\"predicted\")\nplt.ylabel(\"actual\")\nplt","7e9af5d9":"names = [\"Support Vector Machine\", \"Logistic Regression\", \"Nearest Neighbors\",\n         \"Decision Tree\",\"Random Forest\", \"Naive Bayes\",\"Gradient Boosting\"]\n\nresult = []\nprint(\"For guessing the Survived feature we used the following models:\")\nprint(\" \")\nfor name in names:\n    with open(name + '.pickle', 'rb') as fp:\n        clf = pickle.load(fp)\n    \n    clf.fit(X_train, y_train)\n    score1 = clf.score(X_train, y_train)\n    score2 = clf.score(X_valid, y_valid)\n    result.append([score1, score2])\n    \n    print(name)\n\ndf_result = pd.DataFrame(result, columns=['Training', 'Validation'], index = names)\ndf_result.sort_values(\"Validation\",ascending=False)","8f5281ef":"from sklearn.model_selection import GridSearchCV\nparams = {'max_iter' : [2000],\n              'penalty' : ['l1', 'l2'],\n              'C' : np.logspace(-4, 4, 20),\n              'solver' : ['liblinear']}\n\nclf=GridSearchCV(logistic_regression, params, cv=10, return_train_score=True)\nbest_clf = clf.fit(X_train,y_train)\nbest_clf\n\nmeans=clf.cv_results_[\"mean_test_score\"]\nstds=clf.cv_results_[\"std_test_score\"]\nparams=clf.cv_results_[\"params\"]\n\nfor m,s,p in zip(means,stds,params):\n    print(\"%0.3f (+\/-%0.3f) for %r\"%(m,2*s,p))\n    \nprint(\"=\"*100)\nprint(\"=\"*100)\n\nprint('best score: {:0.3f}'.format(clf.score(X, y)))\nprint('best params: {}'.format(clf.best_params_))\nprint('best val score:  {:0.3f}'.format(clf.best_score_))","0d31ddc1":"params = {\n    'n_estimators': [100],\n    'max_depth': [2,3,5,10,None],\n    'gamma':[0,.01,.1,1,10,100],\n    'min_child_weight':[0,.01,0.1,1,10,100],\n    'sampling_method': ['uniform', 'gradient_based']\n}\n\nclf = GridSearchCV(xgb, params, cv=5, return_train_score=True)\nbest_clf = clf.fit(X_train,y_train)\nbest_clf\n\nmeans=clf.cv_results_[\"mean_test_score\"]\nstds=clf.cv_results_[\"std_test_score\"]\nparams=clf.cv_results_[\"params\"]\n\nfor m,s,p in zip(means,stds,params):\n    print(\"%0.3f (+\/-%0.3f) for %r\"%(m,2*s,p))\n    \nprint(\"=\"*100)\nprint(\"=\"*100)\n\nprint('best score: {:0.3f}'.format(clf.score(X, y)))\nprint('best params: {}'.format(clf.best_params_))\nprint('best val score:  {:0.3f}'.format(clf.best_score_))","18f2979b":"temp=pd.DataFrame(X_train,columns=['Pclass', 'FamilySize', 'IsAlone', 'Sex_female',\n       'Sex_male', 'Embarked_C', 'Embarked_Q', 'Embarked_S', 'Title_Master',\n       'Title_Miss', 'Title_Mr', 'Title_Mrs', 'Title_Rare', 'AgeGroup_Unknown',\n       'AgeGroup_Baby', 'AgeGroup_Child', 'AgeGroup_Teenager',\n       'AgeGroup_Student', 'AgeGroup_Young Adult', 'AgeGroup_Adult',\n       'AgeGroup_Senior', 'FareGroup_A', 'FareGroup_B', 'FareGroup_C',\n       'FareGroup_D', 'Deck_A', 'Deck_B', 'Deck_C', 'Deck_D', 'Deck_E',\n       'Deck_F', 'Deck_G', 'Deck_N', 'RoomGroup_g1', 'RoomGroup_g2',\n       'RoomGroup_g3', 'RoomGroup_g4'])\nbest_rf = best_clf.best_estimator_.fit(X_train,y_train)\nfeat_importances = pd.Series(best_rf.feature_importances_,index=temp.columns)\nfeat_importances.nlargest(20).plot(kind='barh',color=\"salmon\")","a7f27b1d":"params = [{'kernel': ['rbf'], 'gamma': [.1,.5,1,2,5,10],\n                                  'C': [.1, 1, 10, 100]},\n                                 {'kernel': ['linear'], 'C': [.1, 1, 10, 100]}]\n\nclf = GridSearchCV(svm, params, cv=10, return_train_score=True)\nbest_clf = clf.fit(X_train,y_train)\nbest_clf\n\nmeans=clf.cv_results_[\"mean_test_score\"]\nstds=clf.cv_results_[\"std_test_score\"]\nparams=clf.cv_results_[\"params\"]\n\nfor m,s,p in zip(means,stds,params):\n    print(\"%0.3f (+\/-%0.3f) for %r\"%(m,2*s,p))\n    \nprint(\"=\"*100)\nprint(\"=\"*100)\n\nprint('best score: {:0.3f}'.format(clf.score(X, y)))\nprint('best params: {}'.format(clf.best_params_))\nprint('best val score:  {:0.3f}'.format(clf.best_score_))","1ba9c5d5":"params = {'n_estimators': [100],\n               'criterion':['gini','entropy'],\n                                  'bootstrap': [True],\n                                  'max_depth': [15, 20, 25],\n                                  'max_features': ['auto','sqrt', 10],\n                                  'min_samples_leaf': [2,3],\n                                  'min_samples_split': [2,3]}\n\nclf = GridSearchCV(random_forest, params, cv=10, return_train_score=True)\nbest_clf = clf.fit(X_train,y_train)\nbest_clf\n\nmeans=clf.cv_results_[\"mean_test_score\"]\nstds=clf.cv_results_[\"std_test_score\"]\nparams=clf.cv_results_[\"params\"]\n\nfor m,s,p in zip(means,stds,params):\n    print(\"%0.3f (+\/-%0.3f) for %r\"%(m,2*s,p))\n    \nprint(\"=\"*100)\nprint(\"=\"*100)\n\nprint('best score: {:0.3f}'.format(clf.score(X, y)))\nprint('best params: {}'.format(clf.best_params_))\nprint('best val score:  {:0.3f}'.format(clf.best_score_))","3dfc97d1":"temp=pd.DataFrame(X_train,columns=['Pclass', 'FamilySize', 'IsAlone', 'Sex_female',\n       'Sex_male', 'Embarked_C', 'Embarked_Q', 'Embarked_S', 'Title_Master',\n       'Title_Miss', 'Title_Mr', 'Title_Mrs', 'Title_Rare', 'AgeGroup_Unknown',\n       'AgeGroup_Baby', 'AgeGroup_Child', 'AgeGroup_Teenager',\n       'AgeGroup_Student', 'AgeGroup_Young Adult', 'AgeGroup_Adult',\n       'AgeGroup_Senior', 'FareGroup_A', 'FareGroup_B', 'FareGroup_C',\n       'FareGroup_D', 'Deck_A', 'Deck_B', 'Deck_C', 'Deck_D', 'Deck_E',\n       'Deck_F', 'Deck_G', 'Deck_N', 'RoomGroup_g1', 'RoomGroup_g2',\n       'RoomGroup_g3', 'RoomGroup_g4'])\nbest_rf = best_clf.best_estimator_.fit(X_train,y_train)\nfeat_importances = pd.Series(best_rf.feature_importances_,index=temp.columns)\nfeat_importances.nlargest(20).plot(kind='barh',color=\"salmon\")","3a1c184d":"best_model=RandomForestClassifier(n_estimators=100,bootstrap=True,criterion=\"gini\",max_depth=15,max_features=\"auto\",min_samples_leaf=2,min_samples_split=2)\nbest_model.fit(X_train,y_train)\npreds=best_model.predict(X_test)","0dd5404f":"output = pd.DataFrame({ \"PassengerId\" : ids, \"Survived\": preds })\noutput.to_csv(\"submission.csv\", index=False)","95035d09":"We can see how having a big family aboard, decreases the survival rate, but having one or two family members aboard increases the survival rate for male passengers","81d98dc9":"As this feature is alphanumerical and it depends on the embarking port, prices and more features we will drop this feature","464fc30d":"This feature is like the \"Name\" feature. We can extract the letter from the cabin as it indicates the area of the ship where the cabin is. Also we will extract the number of the cabin\n\nFor the missing values we will use \"N\"","977c7b7b":"# 1.-Reproducible functions","5c92b468":"# 5.-Modeling and Best Model Choice","a2d044f5":"# 3.5.-Age Feature","b7ef25f0":"The Dataset is divided by the following features:\n\nNumeric features:\n   *     PassengerID: Identifier for each passenger. This feature won\u00b4t have value for our model  (Continuous)\n   *     Survived: Survival status of the passenger. 1=Survived 0=Deceased. This is the feature that we need to predict in the test set (Discrete)\n   *     Pclass: Ticket class. 1=First Class  2=Second Class  3=Third Class  (Discrete)\n   *     Age: Age of the passenger  (Continuous)\n   *     SibSp: Number of siblings and spouses aboard  (Discrete)\n   *     Parch: Number of parents and children aboard  (Discrete)\n   *     Fare: Ticket Fare  (Continuous)\n        \nCategorical features:\n   * Cabin: Cabin number. (Alphanumeric)\n   * Embarked: Embarkation port. C=Cherbourg  Q=Queenstown  S=Southampton\n   * Name: Title and name of the passenger\n   * Sex: Male or Female\n   * Ticket: Ticket number (Alphanumeric)","1f4a6e6d":"# 4.-Preparing the Dataset for modeling","563d171c":"# 3.1.-PassengerID Feature","aa43f5cf":"# 3.7.-Parch Feature","39bf5880":"## Support Vector Classifier","460cd031":"This is the end of my first data analysis in Kaggle. Remember you can give recomendations and tips in the comments! Hope you enjoyed it!\n\n### Source:\n   * [Titanic Project Example](https:\/\/www.kaggle.com\/kenjee\/titanic-project-example)\n   * [Titanic Survival Predictions (Beginner)](https:\/\/www.kaggle.com\/nadintamer\/titanic-survival-predictions-beginner)    \n   * [Various machine learning for beginners](https:\/\/www.kaggle.com\/yanaitti\/various-machine-learning-for-beginners)    ","755dce9e":"As we suposed, babies survival rate is high and surprisingly it\u00b4s almost the same for female and male babies\n\nWith this new feature we can drop the original one","35c638dc":"## Best Model","adc5288a":"As this feature has no value for our model, we will try to get the title name for each passenger and drop the original feature","04f3278e":"# 3.8.-Ticket Feature","69e33f03":"## K Nearest Neighbor","e825f011":"## Searching the best params for Logistic Regression","073c4cab":"### We take the best model for predicting","30849b42":"# 3.6.-SibSp Feature","bb50523a":"## Searching the best params for Random Forest","6ce92ee4":"## Searching the best params for XGBoosting","fdb25c5d":"## Random Forest","191d5e89":"As we can see the best models that fits our data are random forest, gradiend boosting, logistic regression and SVC so we will search the best params for them to se what model performs better with good params to make the prediction","dd3c8543":"We can appreciate that a more expensive ticket fare increases the survival rate of the passenger that buy it.\n\nNow we can drop the original feature.","96bebb6b":"Now we can drop the original cabin feature, the temporal one and the numeric cabin feature","6d4c6887":"As we predicted, the survival rate for higher socio-economic status passengers was higher than for lower ones. **~0.63%** for first class, **~0.47%** for second class and **~0.24%** for third class.\n\nWe can highlight that the great difference between male\u00b4s survival according to their socio-economic class is between the first and second class while for female passengers it\u00b4s between the second and third class since the survival rate of females for the first and second class is very similar","26197ab0":"# 3.10.-Cabin Feature","c90a62b1":"# 3.3.-Pclass Feature","fca3201c":"As with the \"SibSp\" feature, we can see how having one or two children or parents increases the male survival rate\n\nWith the \"SibSp\" and \"Parch\" feature we can create new features as \"FamilySize\" or \"IsAlone\"","29f0993a":"Our prediction for this feature was that higher socio-economic status passengers tend to survive more than lower socio-economic status passengers.","7219206d":"# 3.2.-Sex Feature","a26792d3":"Our prediction for this feature was that female passengers tend to survive more than male passengers.","1970556c":"As predicted, baby\u00b4s survival rate is higher maybe because they had preference in evacuating the ship\n\nFor making this feature usefull for our predictions we can create a new feature with age ranges.But first of all, as we have missing values in this feature, we will treat them like \"unknown\"","7fb47a94":"## Extreme Gradient Boosting","56a03da6":"# **Titanic's Shipwreck Survival Prediction (Beginner)**\nIn this notebook we will try to predict if someone survived to the disaster. This is my first notebook, so if anyone has any recommendations or tips let me know them in the comments. Enjoy it!\n\n## ** Best results : 0.839 with 10k-fold crossvalidation **\n## ** Top: 59% **","346d5174":"Since we are not interested in this variable for our model, we are going to drop it from both train and test set just before modeling","a6df96f9":"## Decision Tree","9bc1a278":"# 3.11.-Embarked Feature","ef9b37a0":"We need to divide the room feature by groups","d72a4528":"# Table of contents\n1. [Reproducible Functions](https:\/\/www.kaggle.com\/arcticai\/titanic-shipwreck-survival-prediction#1.-Reproducible-functions)\n1. [Import libraries and Reading the Dataset](https:\/\/www.kaggle.com\/arcticai\/titanic-shipwreck-survival-prediction#2.-Import-libraries-and-Reading-the-Dataset)\n1. [Exploratory Data Analysis (EDA) with Data Visualization and Feature Engineering](https:\/\/www.kaggle.com\/arcticai\/titanic-shipwreck-survival-prediction)\n1. [Preparing the dataset for modeling](https:\/\/www.kaggle.com\/arcticai\/titanic-shipwreck-survival-prediction#4.-Preparing-the-Dataset-for-modeling)\n1. [Modeling and best model choice](https:\/\/www.kaggle.com\/arcticai\/titanic-shipwreck-survival-prediction#5.-Modeling-and-Best-Model-Choice)\n1. [Creating the Submission File](https:\/\/www.kaggle.com\/arcticai\/titanic-shipwreck-survival-prediction#6.-Creating-the-Submission-File)","8268517b":"## Logistic Regression","813009a6":"## Comparing models with 10k-fold Crossvalidation\nLogistic Regression: 0.826\\\nXGBoosting: 0.83\\\nSVC: 0.82\\\nRandom Forest: 0.839\n\nAs the score of the random forest and the xgboosting is similar, we are going to use the random forest to make the prediction since less computational cost is required.","065e5f6e":"We will inpute the missing value with the mode.","c6ef3b11":"# 2.-Import libraries and Reading the Dataset","869c820a":"We can replace the two missing values with the mode that is Southampton and check if there are more missing values","d3d891a3":"# Distribution of Numeric Features","517bd7fe":"### Train set","ef6323bb":"# 3.-Exploratory Data Analysis (EDA) with Data Visualization and Feature Engineering","452c6631":"# 3.4.-Name Feature","db7fe3fb":"# 3.9.-Fare Feature","efef8f9d":"## Searching the best params for SVC","10d11ff0":"### Reading the Dataset","7d892c19":"## Assumptions\nLooking at the features we can have a few assumptions before the data analysis:\n\n* Female passengers survived more than male passengers\n* Children passengers tend to survive more\n* Higher socio-economic status passengers survived more than lower ones\n* Passengers who paid a higher fare tend to survive more","df5e0b45":"# 6.-Creating the Submission File","a4de10e5":"Now that we know the numeric and categorical features, we need to know how are they distributed and as we can see above, there are some features with missing values so we will look at that too","54fab1bb":"We still have so many titles so they cant be usefull for our model. Let\u00b4s try to reduce the categories.","c860b62a":"### Test set","e42e755e":"With this feature happens the same that with \"Age\" feature. We need to create fare ranges for creating a new feature.","cb359ef5":"In this part we scale the numeric features and convert the categorical features into numeric features","9686eee8":"As we predicted,the survival rate for female passengers is **~0.74%** while survival rate for male passengers is **~0.19%**. This is an important consideration for our models and visualizations.","d60b4392":"Now we can appreciate better the survival rate for each name title where the female passengers and higher status passengers still has the higher survival rate.\n\nOnce we have this new feature we can drop the original one","179e4197":"## Naive Bayes","b4b9be4f":"# Correlation between the Numeric Features","1aabf2e4":"We see we have an extra column in the training set after doing the dummy features from the categorical features so we will drop it"}}