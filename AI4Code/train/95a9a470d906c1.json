{"cell_type":{"c3ad048b":"code","abb7feff":"code","2e5e8025":"code","83a0f829":"code","18a9ad87":"code","c6e37802":"code","070c5b03":"code","4b04bbf5":"code","a4d5ec7b":"code","198b4c66":"code","7698335f":"code","2cf618d8":"code","a4b0c550":"code","63b4d11c":"code","e2eecea8":"code","80249f6d":"code","fc2bdef1":"code","e552ca6e":"code","fa204f9d":"markdown","392bb04a":"markdown","4c2cc388":"markdown","b7e4b4a3":"markdown","a24765f7":"markdown","9475a5f1":"markdown","2845367d":"markdown","d0b0618a":"markdown","aa37fc15":"markdown","11578503":"markdown"},"source":{"c3ad048b":"#@title Some utils\nfrom random import randint\nfrom matplotlib import pyplot as plt\nfrom tensorflow.keras.callbacks import EarlyStopping\n\nimport numpy as np\n\ndef generate_sequence(length, n_unique):\n    \"\"\"\n    Generate a sequence of random integers.\n    \n    :length: Total length of the generated sequence\n    :n_unique: Maximum number allowed\n    \"\"\"\n    return [randint(1, n_unique-1) for _ in range(length)]\n\ndef one_hot_encode(sequence, n_unique):\n    \"\"\"\n    Transform a sequence of integers into a one-hot-encoding vector\n    \n    :sequence: The sequence we want to transform\n    :n_unique: Maximum number allowed (length of the one-hot-encoded vector)\n    \"\"\"\n    encoding = list()\n    for value in sequence:\n        vector = [0 for _ in range(n_unique)]\n        vector[value] = 1\n        encoding.append(vector)\n    return np.array(encoding)\n\ndef one_hot_decode(encoded_seq):\n    \"\"\"\n    Transorm a one-hot-encoded vector into a list of integers\n    \n    :encoded_seq: One hot encoded sequence to be transformed\n    \"\"\"\n    return [np.argmax(vector) for vector in encoded_seq]\n\n\ndef get_reversed_pairs(time_steps,vocabulary_size):\n    \"\"\"\n    Generate a pair X, y where y is the 'reversed' version of X.\n    \n    :time_steps: Sequence length\n    :vocabulary_size: Maximum number allowed\n    \"\"\"\n    # generate random sequence and reverse it\n    sequence_in = generate_sequence(time_steps, vocabulary_size)\n    sequence_out = sequence_in[::-1]\n\n    # one hot encode both sequences\n    X = one_hot_encode(sequence_in, vocabulary_size)\n    y = one_hot_encode(sequence_out, vocabulary_size)\n    \n    # reshape as 3D so it can be inputed to the LSTM\n    X = X.reshape((1, X.shape[0], X.shape[1]))\n    y = y.reshape((1, y.shape[0], y.shape[1]))\n    return X,y\n\n\ndef create_dataset(train_size, test_size, time_steps,vocabulary_size):\n    \"\"\"\n    Generates a datset of reversed pairs X, y.\n    \n    :train_size: Number of train pairs\n    :test_size: Number of test pairs\n    :time_steps: Sequence length\n    :vocabulary_size: Maximum number allowed\n    \"\"\"\n    \n    # Generate reversed pairs for training\n    pairs = [get_reversed_pairs(time_steps,vocabulary_size) for _ in range(train_size)]\n    pairs= np.array(pairs).squeeze()\n    X_train = pairs[:,0]\n    y_train = pairs[:,1]\n    \n    # Generate reversed pairs for test\n    pairs = [get_reversed_pairs(time_steps,vocabulary_size) for _ in range(test_size)]\n    pairs= np.array(pairs).squeeze()\n    X_test = pairs[:,0]\n    y_test = pairs[:,1]\t\n\n    return X_train, y_train, X_test, y_test\n\n\ndef train_test(model, X_train, y_train , X_test, y_test, epochs=500, batch_size=32, patience=5):\n    \"\"\"\n    It trains a model and evaluates the result on the test dataset\n    \n    :model: Model to be fit\n    :X_train, y_train: Train samples and labels \n    :X_test y_test: Test samples and labels \n    :epochs: Maximum number of iterations that the model will perform\n    :batch_size: Samples per batch\n    :patience: Number of rounds without improvement that the model can perform. If there is no improvement on the loss, it will stop the trainning process.\n    \"\"\"\n    \n    # Train the model\n    history=model.fit(X_train, y_train, \n                      validation_split= 0.1, \n                      epochs=epochs,\n                      batch_size=batch_size, \n                      callbacks=[EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True)])\n    \n    _, train_acc = model.evaluate(X_train, y_train, batch_size=batch_size)\n    _, test_acc = model.evaluate(X_test, y_test, batch_size=batch_size)\n    \n    print('\\nPREDICTION ACCURACY (%):')\n    print('Train: %.3f, Test: %.3f' % (train_acc*100, test_acc*100))\n    \n    fig, axs = plt.subplots(1,2, figsize=(12,5))\n    # summarize history for loss\n    axs[0].plot(history.history['loss'])\n    axs[0].plot(history.history['val_loss'])\n    axs[0].set_title(model.name+' loss')\n    axs[0].set_ylabel('loss')\n    axs[0].set_xlabel('epoch')\n    axs[0].legend(['train', 'val'], loc='upper left')\n    \n    # summarize history for accuracy\n    axs[1].plot(history.history['accuracy'])\n    axs[1].plot(history.history['val_accuracy'])\n    axs[1].set_title(model.name+' accuracy')\n    axs[1].set_ylabel('accuracy')\n    axs[1].set_xlabel('epoch')\n    axs[1].legend(['train', 'val'], loc='upper left')\n    plt.show()\n    \n# deprecated method\ndef predict(model, n_timesteps_in, n_features, x, y_real=None, batch_size=1):\n    pred=model.predict(x.reshape(1,n_timesteps_in,n_features), batch_size)\n    print('input', one_hot_decode(x))    \n    print('predicted', one_hot_decode(pred[0]))\n    if y_real is not None:\n        print('expected', one_hot_decode(y_real))","abb7feff":"import tensorflow as tf\nimport numpy as np\n\nfrom tensorflow.keras.layers import Input, Dense, LSTM, Lambda\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras import backend as K\n\ntf.keras.backend.set_floatx('float64')\n\n#attention model\ndef build_attention_model(attention, batch_size, rnn_units):\n    \n    # ENCODER STEP\n    # ------------\n    # Same encoder as before with one and only difference. Now we need all the lstm states so we\n    # set return_sequences=True and return_state=True.\n    encoder_inputs = Input(shape=(n_timesteps_in, n_features), name='encoder_inputs')\n    encoder_lstm = LSTM(rnn_units, return_sequences=True, return_state=True, name='encoder_lstm')\n    encoder_outputs, encoder_state_h, encoder_state_c = encoder_lstm(encoder_inputs)\n    \n    states = [encoder_state_h, encoder_state_c] # h -> hidden? : c -> compressed?\n    \n    # DECODER STEP\n    # ------------\n    # Set up the decoder layers\n    # input shape: (1, n_features + rnn_units)\n    decoder_lstm = LSTM(rnn_units, return_state=True, name='decoder_lstm')\n    decoder_dense = Dense(n_features, activation='softmax', name='decoder_dense')\n    \n    # As before, we use as first input the 0-sequence\n    all_outputs = []\n    inputs = np.zeros((batch_size, 1, n_features))\n    \n    # Decoder_outputs is the last hidden state of the encoder. Encoder_outputs are all the states\n    decoder_outputs = encoder_state_h\n    \n    # Decoder will only process one time step at a time.\n    for _ in range(n_timesteps_in):\n\n        # Pay attention!\n        # decoder_outputs (last hidden state) + encoder_outputs (all hidden states)\n        context_vector, attention_weights = attention(decoder_outputs, encoder_outputs)\n        context_vector = tf.expand_dims(context_vector, 1)\n\n        # create the context vector by applying attention to \n        # Concatenate the input + context vectors to find the next decoder's input\n        inputs = tf.concat([context_vector, inputs], axis=-1)\n\n        # Passing the concatenated vector to the LSTM\n        # Run the decoder on one timestep with attended input and previous states\n        decoder_outputs, state_h, state_c = decoder_lstm(inputs, initial_state=states)        \n        outputs = decoder_dense(decoder_outputs)\n        \n        # Use the last hidden state for prediction the output\n        # save the current prediction\n        # we will concatenate all predictions later\n        outputs = tf.expand_dims(outputs, 1)\n        all_outputs.append(outputs)\n        \n        # Reinject the output (prediction) as inputs for the next loop iteration\n        # as well as update the states\n        inputs = outputs\n        states = [state_h, state_c]\n        \n    decoder_outputs = Lambda(lambda x: K.concatenate(x, axis=1))(all_outputs)\n    model = Model(encoder_inputs, decoder_outputs, name='model_encoder_decoder')\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    \n    return model\n","2e5e8025":"class LuongDotAttention(tf.keras.layers.Layer):\n    def __init__(self):\n        super(LuongDotAttention, self).__init__()\n\n    def call(self, query, values):\n        query_with_time_axis = tf.expand_dims(query, 1)\n        values_transposed = tf.transpose(values, perm=[0, 2, 1])\n\n        # LUONGH Dot-product\n        score = tf.transpose(tf.matmul(query_with_time_axis, \n                                       values_transposed), perm=[0, 2, 1])\n\n        # attention_weights shape == (batch_size, max_length, 1)\n        attention_weights = tf.nn.softmax(score, axis=1)\n        \n        # context_vector shape after sum == (batch_size, hidden_size)\n        context_vector = attention_weights * values\n        context_vector = tf.reduce_sum(context_vector, axis=1)\n\n        return context_vector, attention_weights\n","83a0f829":"# dataset \nn_timesteps_in = 100  # Sequence length\nn_features = 20  # Maximum number allowed-1 (length of the one-hot-encoded vector)\ntrain_size = 2000\ntest_size = 200\nX_train, y_train, X_test, y_test = create_dataset(train_size, test_size, n_timesteps_in,n_features )\n\n# training parameters\nbatch_size = 10 # 10 is the best batch_size\n\n# model parameters\nrnn_units = 50 # best rnn_units","18a9ad87":"# attention model\nattention = LuongDotAttention()\nmodel_attention = build_attention_model(attention, batch_size, rnn_units)","c6e37802":"# model_attention.summary() # uncomment to see the attention model's structure","070c5b03":"#training\ntrain_test(model_attention, X_train, y_train , X_test,\n           y_test, batch_size=batch_size, epochs=50, patience=3) # todo try on different epochs","4b04bbf5":"def predict_mod(model, n_timesteps_in, n_features, x, y_real=None, batch_size=1):\n    #pred=model.predict(x.reshape(1,n_timesteps_in,n_features), batch_size=1)\n    pred=model.predict(x.reshape(x.shape[0],n_timesteps_in,n_features), batch_size)\n    print('Input -> ', one_hot_decode(x[0]), '\\n')    \n    print('Predicted -> ', one_hot_decode(pred[0]), '\\n')\n    if y_real is not None:\n        print('Expected -> ', one_hot_decode(y_real[0]))","a4d5ec7b":"predict_mod(model_attention, n_timesteps_in, n_features, X_train, y_real=y_train, batch_size=batch_size)","198b4c66":"class BahdanauAttention(tf.keras.layers.Layer):\n    \n    def __init__(self, units):\n        super(BahdanauAttention, self).__init__()\n        self.w1 = tf.keras.layers.Dense(units)\n        self.w2 = tf.keras.layers.Dense(units)\n        self.v = tf.keras.layers.Dense(1)\n\n    def call(self, query, values):\n        query_with_time_axis = tf.expand_dims(query, 1)\n        \n        # measuring score\n        score = self.v(tf.nn.tanh(self.w1(query_with_time_axis) + self.w2(values)))\n        \n        # getting weights\n        attention_weights = tf.nn.softmax(score, axis=1)\n        \n        # getting the context vector (z)\n        context_vector = attention_weights * values\n        context_vector = tf.reduce_sum(context_vector, axis=1)\n        \n        return context_vector, attention_weights","7698335f":"class LuongGeneralAttention(tf.keras.layers.Layer):\n    def __init__(self, units):\n        super(LuongGeneralAttention, self).__init__()\n        self.w1 = tf.keras.layers.Dense(units)\n        self.w2 = tf.keras.layers.Dense(units)\n        self.v = tf.keras.layers.Dense(1)\n\n    def call(self, query, values):\n        query_with_time_axis = tf.expand_dims(query, 1)\n        values_transposed = tf.transpose(values, perm=[0, 2, 1])\n        \n        # measuring score\n        score = tf.transpose(tf.matmul(query_with_time_axis, values_transposed) , perm=[0, 2, 1])\n        \n        # getting weights\n        attention_weights = tf.nn.softmax(score, axis=1)\n        \n        # getting the context vector (z)\n        context_vector = attention_weights * values\n        context_vector = tf.reduce_sum(context_vector, axis=1)\n        \n        return context_vector, attention_weights","2cf618d8":"# attention model for Bahdanau\nattentionBahdanau = BahdanauAttention(rnn_units)\nmodel_attentionBahdanau = build_attention_model(attentionBahdanau, batch_size, rnn_units)","a4b0c550":"# model_attentionBahdanau.summary() # uncomment to see the attention model's structure","63b4d11c":"# train and measure the accuracy\ntrain_test(model_attentionBahdanau, X_train, y_train , X_test,\n           y_test, batch_size=batch_size, epochs=50, patience=3)","e2eecea8":"# attention model for Luong General\nattentionLuong = LuongGeneralAttention(rnn_units)\nmodel_attentionLuong = build_attention_model(attentionLuong, batch_size, rnn_units)","80249f6d":"# model_attentionLuong.summary() # uncomment to see the attention model's structure","fc2bdef1":"# train and measure the accuracy\ntrain_test(model_attentionLuong, X_train, y_train , X_test,\n           y_test, batch_size=batch_size, epochs=50, patience=3)","e552ca6e":"# function for plotting the attention weights\ndef plot_attention(attention, sequence, predicted_sequence):\n    fig = plt.figure(figsize=(8,8))\n    ax = fig.add_subplot(1, 1, 1)\n    ax.matshow(attention, cmap='viridis')\n\n    fontdict = {'fontsize': 14}\n\n    ax.set_xticklabels([''] + sequence, fontdict=fontdict, rotation=90)\n    ax.set_yticklabels([''] + predicted_sequence, fontdict=fontdict)\n\n    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n\n    plt.show()","fa204f9d":"# Visualization of the weights (NOT DONE!)","392bb04a":"# Check out my [blog entry](https:\/\/hackmd.io\/n9AGgNAASmqoxP9tSGgJoA?view)!\n\n","4c2cc388":"Below are the results obtained through many experiments in **number of epochs (#Epochs)**, and **accuracy** in both train and test sets, just by manipulating ``batch_size`` and ``rnn_units``:\n\n``batch_size=5`` ``rnn_units=60``\n\n| **Variable names** | **_LuongDotAtenttion_** | **_BahdanauAttention_** | **_LuongGeneralAttention_** |\n| :---: | :---: | :---: | :---: |\n| #Epochs | 11 | 16 | 17 |\n| Prediction accuracy on Train set (%) | 90.493 | 95.520 | 81.398 |\n| Prediction accuracy on Test set (%) | 90.260 | 94.825 | 80.565 |\n\n``batch_size=10`` ``rnn_units=60``\n\n| **Variable names** | **_LuongDotAtenttion_** | **_BahdanauAttention_** | **_LuongGeneralAttention_** |\n| :---: | :---: | :---: | :---: |\n| #Epochs | 25 | 18 | 19 |\n| Prediction accuracy on Train set (%) | 82.761 | 92.139 | 81.406 |\n| Prediction accuracy on Test set (%) | 80.875 | 92.335 | 78.515 |\n\n``batch_size=10`` ``rnn_units=80``\n\n| **Variable names** | **_LuongDotAtenttion_** | **_BahdanauAttention_** | **_LuongGeneralAttention_** |\n| :---: | :---: | :---: | :---: |\n| #Epochs | 11 | 10 | 9 |\n| Prediction accuracy on Train set (%) | 54.385 | 56.819 | 46.722 |\n| Prediction accuracy on Test set (%) | 53.105 | 56.750 | 46.035 |\n\n\n``batch_size=5`` ``rnn_units=50``\n\n| **Variable names** | **_LuongDotAtenttion_** | **_BahdanauAttention_** | **_LuongGeneralAttention_** |\n| :---: | :---: | :---: | :---: |\n| #Epochs | 12 | 20 | 12 |\n| Prediction accuracy on Train set (%) | 44.490 | 95.912 | 70.029 |\n| Prediction accuracy on Test set (%) | 45.170 | 96.075 | 69.555 |\n\n``batch_size=10`` ``rnn_units=50``\n\n| **Variable names** | **_LuongDotAtenttion_** | **_BahdanauAttention_** | **_LuongGeneralAttention_** |\n| :---: | :---: | :---: | :---: |\n| #Epochs | 19 | 35 | 26 |\n| Prediction accuracy on Train set (%) | 64.253 | 99.972 | 87.707 |\n| Prediction accuracy on Test set (%) | 61.905 | 99.950 | 86.465 |\n\n``batch_size=10`` ``rnn_units=50`` **(2nd round - BEST OVERALL RESULT!)**\n\n| **Variable names** | **_LuongDotAtenttion_** | **_BahdanauAttention_** | **_LuongGeneralAttention_** |\n| :---: | :---: | :---: | :---: |\n| #Epochs | 20 | 24 | 24 |\n| Prediction accuracy on Train set (%) | 92.109 | 99.290 | 85.073 |\n| Prediction accuracy on Test set (%) | 92.660 | 99.325 | 84.570 |\n\n``batch_size=10`` ``rnn_units=50`` (3rd round)\n\n| **Variable names** | **_LuongDotAtenttion_** | **_BahdanauAttention_** | **_LuongGeneralAttention_** |\n| :---: | :---: | :---: | :---: |\n| #Epochs | 26 | 24 | 19 |\n| Prediction accuracy on Train set (%) | 81.831 | 98.981 | 96.047 |\n| Prediction accuracy on Test set (%) | 78.305 | 98.920 | 96.415 |\n\n``batch_size=20`` ``rnn_units=50``\n\n| **Variable names** | **_LuongDotAtenttion_** | **_BahdanauAttention_** | **_LuongGeneralAttention_** |\n| :---: | :---: | :---: | :---: |\n| #Epochs | 37 | 17 | 35 |\n| Prediction accuracy on Train set (%) | 85.979 | 38.931 | 75.177 |\n| Prediction accuracy on Test set (%) | 84.565 | 38.730 | 72.460 |\n\n``batch_size=20`` ``rnn_units=30``\n\n| **Variable names** | **_LuongDotAtenttion_** | **_BahdanauAttention_** | **_LuongGeneralAttention_** |\n| :---: | :---: | :---: | :---: |\n| #Epochs | 50 | 50 | 50 |\n| Prediction accuracy on Train set (%) | 58.387 | 94.270 | 58.319 |\n| Prediction accuracy on Test set (%) | 57.020 | 94.220 | 57.305 |\n\n``batch_size=20`` ``rnn_units=60``\n\n| **Variable names** | **_LuongDotAtenttion_** | **_BahdanauAttention_** | **_LuongGeneralAttention_** |\n| :---: | :---: | :---: | :---: |\n| #Epochs | 28 | 21 | 32 |\n| Prediction accuracy on Train set (%) | 74.963 | 70.128 | 91.514 |\n| Prediction accuracy on Test set (%) | 74.055 | 69.890 | 89.380 |\n\n``batch_size=10`` ``rnn_units=40``\n\n| **Variable names** | **_LuongDotAtenttion_** | **_BahdanauAttention_** | **_LuongGeneralAttention_** |\n| :---: | :---: | :---: | :---: |\n| #Epochs | 36 | 23 | 35 |\n| Prediction accuracy on Train set (%) | 90.111 | 78.399 | 86.289 |\n| Prediction accuracy on Test set (%) | 88.205 | 78.245 | 83.835 |","b7e4b4a3":"### Prediction results on the first model","a24765f7":"### _Luong Dot Attention_","9475a5f1":"### Bahdanau and Luong General Attention implementation","2845367d":"# Code","d0b0618a":"# Report\n\n+ Bahdanau Attention and Luong General Attention implementation.\n+ Comparative.\n+ Weight visualization. (NOT DONE!)\n+ Blog site.\n\n","aa37fc15":"# Assignment 3: Attention\n\nThe objectives of this assignment are:\n\n+ To implement Bahdanau Attention and Luong General Attention classes.\n+ To do a comparative (# of steps to converge, test error) of the three methods we have seen. Use these values for the comparative (the training datset size and `rnn_units` and `batch_size` values are up to you): \n    + `n_timesteps_in = 100`\n    + `n_features = 20`.   \n+ To implement a function to visualize the attention weights for one example. You can visualize them as in this figure (that corresponds to a machine translation task):\n\n<div>\n<center>\n<img src=\"https:\/\/jalammar.github.io\/images\/attention_sentence.png\" width=\"200\">\n<\/center>\n<\/div>\n\n+ To write a blog entry explaining in your words how does attention work. You can do it in your favourite blog site. If you do not have a favourite blog site, you can start one here: https:\/\/hackmd.io\/\n\nYou have to report all your work at the end of this notebook.\n\n","11578503":"# Comparing attention models"}}