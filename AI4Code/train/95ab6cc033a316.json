{"cell_type":{"5bb4dc1e":"code","8ea13c66":"code","cfae8c3e":"code","ba969e5a":"code","cb1754bb":"code","54a48ace":"code","02195a45":"code","95ed92bf":"code","b6eb7a03":"code","b7a4b346":"code","f59fd877":"code","a8e9ff89":"code","0217d5f4":"code","f052b2c3":"code","73d9af0a":"code","f2cae925":"code","11d94b11":"code","8de5427d":"code","8c966621":"code","12c9f179":"code","f7fcde77":"code","5fc02a5b":"code","fa12a35d":"code","dc138d64":"code","0ceae7fc":"code","9b022d47":"code","ea2a206d":"code","92a913ad":"code","43ee2fb0":"code","5bcc5267":"code","d7006362":"code","a6a4e6e5":"code","c5ab71dc":"code","9cac1397":"markdown","4852d512":"markdown","e0894089":"markdown","fe46659e":"markdown","6cf082ab":"markdown","7f361c55":"markdown","8fffe11c":"markdown"},"source":{"5bb4dc1e":"import pandas as pd\nimport numpy as np\nimport io\nimport math\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor\nimport xgboost as xgb\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error,r2_score,make_scorer, mean_absolute_error\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import RandomizedSearchCV ,GridSearchCV\nfrom sklearn.svm import SVR","8ea13c66":"df= pd.read_csv('..\/input\/insurance-premium-prediction\/insurance.csv')","cfae8c3e":"df.head()","ba969e5a":"df.info()","cb1754bb":"df.describe().T","54a48ace":"#check for outliers using IQR method\ndef outlier(col):\n    m= df[col].mean()\n    m1= df[col].min()\n    m2=df[col].max()\n    l= len(df)\n    q1= df[col].quantile(0.25)\n    q3=df[col].quantile(0.75)\n    IQR= q3-q1\n    lower= q1-(IQR*1.5)\n    upper= q3+(IQR*1.5)\n    n= len(df.loc[np.where((df[col] > upper) | (df[col] < lower))])\n    perc= (n\/l)*100\n    print(f'{col}\\n percentage= {perc}\\n number={n}\\n mean= {m}\\n min={m1}\\n max={m2}')","02195a45":"for i in df.select_dtypes(exclude='object').columns:\n  outlier(i)","95ed92bf":"#using box plot to understand the outliers visually and to understand the relation between the smokers and other features on expenses\nsns.set(font_scale=1.2)\nplt.figure(figsize=(25,25))\n\nfor i, column in enumerate(['sex','region','children'], 1):\n    plt.subplot(2,2, i)\n    g = sns.boxplot(x=f\"{column}\", y='expenses',hue='smoker',data=df)\n    g.set_xticklabels(g.get_xticklabels())\n    plt.ylabel('expenses')\n    plt.xlabel(f'{column}')","b6eb7a03":"(df[df['region']=='southeast']).sort_values(by='expenses',ascending=False)","b7a4b346":"(df[df['region']=='southwest']).sort_values(by='expenses',ascending=False)","f59fd877":"#Analysis of Variance\n(df.var()).plot(kind='bar',ylim= (0,2))","a8e9ff89":"sns.pairplot(df,hue='sex')","0217d5f4":"plt.figure(figsize=(15,15))\nsns.scatterplot(x='age',y='expenses',hue='smoker',data=df)","f052b2c3":"#manully encoding the smoker feature to 0 and 1\ndf[\"smoker\"] = df[\"smoker\"].replace({\"yes\":1,\"no\":0})","73d9af0a":"#one-hot encoding the categorical features \ndf_dummy= pd.get_dummies(df)","f2cae925":"df_dummy.head()","11d94b11":"#checking the correlation among the features\nplt.figure(figsize=(15,10))\nsns.heatmap(df_dummy.corr(),annot=True)","8de5427d":"from sklearn.preprocessing import StandardScaler\nfrom scipy.stats import norm","8c966621":"#divided the dataset to dependent and independent variables\nx= df_dummy.drop(columns='expenses')\ny=df_dummy[['expenses']]","12c9f179":"scale= StandardScaler()","f7fcde77":"#standard scaling the age and bmi feature as they are in different units\nx[['age','bmi']]= scale.fit_transform(x[['age','bmi']])","5fc02a5b":"#splitting the data into test and train\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=101)","fa12a35d":"#polynomial features created to introduce the linear relation between the dependent and independent variables\npoly= PolynomialFeatures(degree=2)\nx_poly= poly.fit_transform(x)\nX_train1, X_test1, y_train1, y_tes1 = train_test_split(x_poly, y, test_size=0.3, random_state=101)","dc138d64":"LR= LinearRegression(fit_intercept=True)\nLR.fit(X_train1,y_train1)\ny_pred_poly= LR.predict(X_train1)","0ceae7fc":"print(f'MAE:{mean_absolute_error(y_train1,y_pred_poly)}')\nprint(f'RMSE: {np.sqrt(mean_squared_error(y_train1,y_pred_poly))}')","9b022d47":"y_pred_poly_t= LR.predict(X_test1)\nprint(f'MAE:{mean_absolute_error(y_tes1,y_pred_poly_t)}')\nprint(f'RMSE: {np.sqrt(mean_squared_error(y_tes1,y_pred_poly_t))}')","ea2a206d":"RFR= RandomForestRegressor()\nparams_RFR= {'n_estimators':[100,500,1000],\n                'max_features':['log2', 'sqrt'],\n                'max_depth':[2, 3, 5],\n                'min_samples_split':[2,5,10,15,20,25],\n                'min_samples_leaf':[1,2,4,6,8,10]}\n\nKNNR= KNeighborsRegressor()\nparams_KNNR= {'n_neighbors':list(range(2,20,2)),\n              'weights':['uniform', 'distance'],\n              'leaf_size':list(range(1,5)),\n              'p':[1,2]}\n\nDTR=DecisionTreeRegressor()\nparams_DTR= {  'max_depth':[2, 3, 5],\n                'min_samples_split':[2,5,10,15,20,25]}\n\nGBR=GradientBoostingRegressor()\nparams_GBR= {'n_estimators':[10, 50, 100, 500],\n             'max_depth':[2, 3, 5,7],\n              'min_samples_split':[2,5,10,15,20,25],\n              'learning_rate':[0.0001, 0.001, 0.01, 0.1, 1.0],\n               'min_samples_leaf':[1,2,4,6,8,10],\n              'max_features':['log2', 'sqrt']}\n\nSvr= SVR()\nparams_SVR= {'kernel':['linear', 'poly', 'rbf', 'sigmoid'],\n             'degree':[1,2,3,4,5,6,7],\n             'gamma':['scale', 'auto'],\n             'C':[1, 10, 100, 1000, 10000]}","92a913ad":"for i,j,k in zip([Svr,DTR,RFR,KNNR,GBR],[params_SVR,params_DTR,params_RFR,params_KNNR,params_GBR],['SVR','DTR','RFR','KNNR','GBR']):\n  grid= GridSearchCV(estimator=i, param_grid=j,cv=5,n_jobs=-1,scoring=make_scorer(r2_score))\n  grid.fit(X_train,y_train)\n  print(f'{k}= ')\n  print('-'.center(20,'-'))\n  print(f'r2 score:{grid.best_score_}')\n  print(f'Best Parameter=')\n  print(grid.best_params_)\n  print('xxx'.center(100,'-'))\n  print('xxx'.center(100,'-'))","43ee2fb0":"reg= GradientBoostingRegressor(learning_rate= 0.1, max_depth=3, max_features= 'sqrt',min_samples_leaf= 4,min_samples_split=15, n_estimators=100)","5bcc5267":"reg.fit(X_train,y_train)","d7006362":"y_pred= reg.predict(X_train)","a6a4e6e5":"print(f'RMSE: {np.sqrt(mean_squared_error(y_train,y_pred))}')\nprint(f'MAE: {(mean_absolute_error(y_train,y_pred))}')","c5ab71dc":"y_pred1= reg.predict(X_test)\nprint(f'RMSE: {np.sqrt(mean_squared_error(y_test,y_pred1))}')\nprint(f'MAE: {(mean_absolute_error(y_test,y_pred1))}')","9cac1397":"#HyperParameter Tuning of Non-Linear Models using R2_score","4852d512":"Observation:\nSmokers overall have more expense than non-smokers and also it is has a linear relationship with age i.e. as age inrease expense increase","e0894089":"Onservation:\n- no probable multicolineraity\n- smoker feature has maximum correlation to the target","fe46659e":"Conclusion:\nIt was found that the GradientBoosting Regressor is giving a better model as compare to other models for this dataset using the RMSE and MAE scoring metrices.","6cf082ab":"Observation:\n\nThe above exhibits that the region a person is located is playing a role on the expense one bears along with the smoking habit","7f361c55":"#Polynomial Regression","8fffe11c":"Observation:\nOverfitting observed at a degree of more than 2 thus optimum degree choosen as 2"}}