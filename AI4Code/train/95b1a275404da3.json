{"cell_type":{"f1026715":"code","6c643ec2":"code","e7429faa":"code","993459eb":"code","d2a73c23":"code","7ff30d1b":"code","46d70dcd":"code","1ac9b533":"code","5f31269a":"code","7bc6655a":"code","d785ae74":"code","8622dd32":"code","3551bc8e":"code","affd53cb":"code","5835e07c":"code","34b35d2e":"code","eb9639a7":"markdown","eee3ba39":"markdown","7ae9a226":"markdown","e5084dd2":"markdown","0c30886f":"markdown","775a1bce":"markdown","ba2cfb1e":"markdown","d3859758":"markdown","256f5406":"markdown","695780a8":"markdown","70aebde3":"markdown","d1ef9c08":"markdown","a310f122":"markdown","7cf6d8d7":"markdown","a91b3cce":"markdown","c3efbcac":"markdown","8b01b9b2":"markdown","61a75714":"markdown"},"source":{"f1026715":"import nltk\n\nnltk.download(\"gutenberg\")","6c643ec2":"hamlet_raw = nltk.corpus.gutenberg.raw('shakespeare-hamlet.txt')\nprint(hamlet_raw[:1000])","e7429faa":"from nltk.tokenize import sent_tokenize\n\nsentences = sent_tokenize(hamlet_raw)\n\nprint(sentences[:10])\n","993459eb":"from nltk.tokenize import word_tokenize\n\nwords = word_tokenize(sentences[0])\n\nprint(words)","d2a73c23":"from nltk.corpus import stopwords\n\nstopwords_list = stopwords.words('english')\n\nprint(stopwords_list)\n\n#stopwords_list = stopwords.words('portuguese')\n\n#print(stopwords_list)","7ff30d1b":"non_stopwords = [w for w in words if not w.lower() in stopwords_list]\nprint(non_stopwords)","46d70dcd":"import string\npunctuation = string.punctuation\nprint(punctuation)","1ac9b533":"non_punctuation = [w for w in non_stopwords if not w in punctuation]\n\nprint(non_punctuation)","5f31269a":"from nltk import pos_tag\n\npos_tags = pos_tag(words)\n\nprint(pos_tags)","7bc6655a":"from nltk.stem.snowball import SnowballStemmer\n\nstemmer = SnowballStemmer('english')\n\nsample_sentence = \"He has already gone\"\nsample_words = word_tokenize(sample_sentence)\n\nstems = [stemmer.stem(w) for w in sample_words]\n\nprint(stems)","d785ae74":"nltk.download('wordnet')","8622dd32":"from nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet\n\nlemmatizer = WordNetLemmatizer()\n\npos_tags = nltk.pos_tag(sample_words)\n\nlemmas = []\nfor w in pos_tags:\n    if w[1].startswith('J'):\n        pos_tag = wordnet.ADJ\n    elif w[1].startswith('V'):\n        pos_tag = wordnet.VERB\n    elif w[1].startswith('N'):\n        pos_tag = wordnet.NOUN\n    elif w[1].startswith('R'):\n        pos_tag = wordnet.ADV\n    else:\n        pos_tag = wordnet.NOUN\n        \n    lemmas.append(lemmatizer.lemmatize(w[0], pos_tag))\n    \nprint(lemmas)","3551bc8e":"from nltk import word_tokenize\n\nfrase = 'o cachorro correu atr\u00e1s do gato'\n\n\nngrams = [\"%s %s %s\" % (nltk.word_tokenize(frase)[i], \\\n                      nltk.word_tokenize(frase)[i+1], \\\n                      nltk.word_tokenize(frase)[i+2]) \\\n          for i in range(len(nltk.word_tokenize(frase))-2)]\n\nprint(ngrams)\n","affd53cb":"non_punctuation = [w for w in words if not w.lower() in punctuation]\n\nn_grams_3 = [\"%s %s %s\"%(non_punctuation[i], non_punctuation[i+1], non_punctuation[i+2]) for i in range(0, len(non_punctuation)-2)]\n\nprint(n_grams_3)","5835e07c":"from sklearn.feature_extraction.text import CountVectorizer\n\ncount_vect = CountVectorizer(ngram_range=(2,3))\n\nimport numpy as np\n\nsent2 = ['o cachorro correu atr\u00e1s do gato que correu atr\u00e1s do rato',\n        'o gato correu atr\u00e1s do rato',\n        'o rato comeu a ra\u00e7\u00e3o',\n        'o cachorro comeu o rato que comeu a ra\u00e7\u00e3o',\n        'o gato comeu o rato']\n\narr = np.array(sent2)\n\nprint(arr)\n\nn_gram_counts = count_vect.fit_transform(arr)\n\nprint(n_gram_counts.toarray())\n\nprint(count_vect.vocabulary_)","34b35d2e":"arr = np.array(sentences)\n\nn_gram_counts = count_vect.fit_transform(arr)\n\nprint(n_gram_counts.toarray()[:20])\n\nprint([k for k in count_vect.vocabulary_.keys()][:20])","eb9639a7":"<b>7. N-gramas<\/b>","eee3ba39":"<b>4. Removendo stopwords e pontua\u00e7\u00e3o<\/b>","7ae9a226":"Al\u00e9m da t\u00e9cnica de <i>Bag-of-Words<\/i>, outra op\u00e7\u00e3o \u00e9 utilizar n-gramas (onde \"n\" pode variar)","e5084dd2":"<b>3. Segmenta\u00e7\u00e3o de senten\u00e7as e tokeniza\u00e7\u00e3o de palavras<\/b>","0c30886f":"<b>6. Stemming e Lemmatization<\/b>","775a1bce":"<b>5. Part of Speech (POS) Tags <\/b>","ba2cfb1e":"Agora, vamos contar os n-grams (no nosso caso, trigramas) de todas as senten\u00e7as do texto:","d3859758":"As tags indicam a classifica\u00e7\u00e3o sint\u00e1tica de cada palavra no texto. Ver <a href=\"https:\/\/www.ling.upenn.edu\/courses\/Fall_2003\/ling001\/penn_treebank_pos.html\" target=\"blank\">https:\/\/www.ling.upenn.edu\/courses\/Fall_2003\/ling001\/penn_treebank_pos.html<\/a> para uma lista completa","256f5406":"<p><b>Exerc\u00edcio 2:<\/b>Exiba 10 lemmas mais frequentes do corpus Reuters, ignorando pontua\u00e7\u00f5es e stopwords.<\/p>\n\n","695780a8":"J\u00e1 lemmatization vai al\u00e9m de somente remover sufixos, obtendo a raiz lingu\u00edstica da palavra. Vamos usar as tags POS obtidas anteriormente para otimizar o lemmatizer.","70aebde3":"Stemming permite obter a \"raiz\" da palavra, removendo sufixos, por exemplo.","d1ef9c08":"<h2> T\u00e9cnicas para Pr\u00e9-Processamento <\/h2>","a310f122":"Consiste em dividir o texto em senten\u00e7as. Por\u00e9m, n\u00e3o \u00e9 uma tarefa t\u00e3o trivial quanto usar uma fun\u00e7\u00e3o split(\".\") no Python, por exemplo. Isso porque o uso do ponto (.) nem sempre \u00e9 empregado como ponto final em uma senten\u00e7a. Por exemplo:\n\n\"Dr. Rodolfo S. Silva, um renomado cardiologista, adquiriu um novo consult\u00f3rio na Av. Edson A. Nascimento pela quantia de R$ 4.5 milh\u00f5es, que atender\u00e1 pacientes do S.U.S. e do plano Sa\u00fade Mais Inc.\"\n\nUma das t\u00e9cnicas mais conhecidas para resolver esse problema \u00e9 o algoritmo Punkt. Trata-se de um modelo que consiste em identificar abrevia\u00e7\u00f5es, retic\u00eancias, iniciais e n\u00fameros ordinais para, a\u00ed ent\u00e3o, identificar os pontos finais em senten\u00e7as.\n\n![Arquitetura Punkt](https:\/\/www.researchgate.net\/profile\/Jan_Strunk\/publication\/220355311\/figure\/fig1\/AS:277323623485449@1443130512700\/Architecture-of-the-Punkt-System.png)","7cf6d8d7":"Tamb\u00e9m podemos usar a classe <b>CountVectorizer<\/b>, do scikit-learn:","a91b3cce":"<b>1. Baixando o corpus Gutenberg<\/b>","c3efbcac":"<h1 align=\"center\"> Aplica\u00e7\u00f5es em Processamento de Linguagem Natural <\/h1>\n<h2 align=\"center\"> Aula 02 - T\u00e9cnicas de Pr\u00e9-Processamento de Texto<\/h2>\n<h3 align=\"center\"> Prof. Fernando Vieira da Silva MSc.<\/h3>","8b01b9b2":"<p>Vamos avaliar as t\u00e9cnicas mais comuns para prepararmos o texto para usar com algoritmos de aprendizado de m\u00e1quina logo mais.<\/p>\n<p>Como estudo de caso, vamos usar o texto de <i>Hamlet<\/i>, encontrado no corpus <i>Gutenberg<\/i> do pacote <b>NLTK<\/b><\/p>","61a75714":"<b>2. Exibindo o texto \"Hamlet\"<\/b>"}}