{"cell_type":{"2d566b1a":"code","5fa4c540":"code","ef581574":"code","d59173ee":"code","8fd9e980":"code","11a8efe1":"code","be80035c":"code","38a6fa15":"code","2bc484b3":"code","f2f3fb11":"code","0aa454fb":"code","7e339eb2":"code","c7237264":"code","dcc3034a":"code","4ddfce7f":"code","81338265":"code","4c24cd22":"code","56f50b18":"code","b3e096bc":"code","e3eebd84":"code","3dc0e102":"code","43bceebb":"code","44e744a1":"code","1c43fd8c":"code","2d22025b":"code","e5a6cea7":"code","3e1fa426":"markdown","f52ce4e4":"markdown","860ef006":"markdown","4c9c7957":"markdown","45df36fb":"markdown","f5004c85":"markdown","5b318860":"markdown","afecd658":"markdown","12bf48a6":"markdown","f405c6af":"markdown","5900954b":"markdown","6de3a342":"markdown","2b6f35af":"markdown","25769204":"markdown"},"source":{"2d566b1a":"\"\"\"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport os\nimport json\nimport numpy as np\nimport pandas as pd\nfrom pandas.io.json import json_normalize\n\nfrom sklearn import model_selection, preprocessing, metrics\n\ndef load_df(csv_path='..\/input\/train.csv', nrows=None):\n    JSON_COLUMNS = ['device', 'geoNetwork', 'totals', 'trafficSource']\n    \n    df = pd.read_csv(csv_path, \n                     converters={column: json.loads for column in JSON_COLUMNS}, \n                     dtype={'fullVisitorId': 'str'}, # Important!!\n                     nrows=nrows)\n    \n    for column in JSON_COLUMNS:\n        column_as_df = json_normalize(df[column])\n        column_as_df.columns = [f\"{column}.{subcolumn}\" for subcolumn in column_as_df.columns]\n        df = df.drop(column, axis=1).merge(column_as_df, right_index=True, left_index=True)\n    print(f\"Loaded {os.path.basename(csv_path)}. Shape: {df.shape}\")\n    return df\n\n\ntrain_df = load_df()\ntest_df = load_df(\"..\/input\/test.csv\")\n\ncat_cols = [\"channelGrouping\", \"device.browser\", \n            \"device.deviceCategory\", \"device.operatingSystem\", \n            \"geoNetwork.city\", \"geoNetwork.continent\", \n            \"geoNetwork.country\", \"geoNetwork.metro\",\n            \"geoNetwork.networkDomain\", \"geoNetwork.region\", \n            \"geoNetwork.subContinent\", \"trafficSource.adContent\", \n            \"trafficSource.adwordsClickInfo.adNetworkType\", \n            \"trafficSource.adwordsClickInfo.gclId\", \n            \"trafficSource.adwordsClickInfo.page\", \n            \"trafficSource.adwordsClickInfo.slot\", \"trafficSource.campaign\",\n            \"trafficSource.keyword\", \"trafficSource.medium\", \n            \"trafficSource.referralPath\", \"trafficSource.source\",\n            'trafficSource.adwordsClickInfo.isVideoAd', 'trafficSource.isTrueDirect']\n\n# Impute 0 for missing target values\ntrain_df[\"totals.transactionRevenue\"].fillna(0, inplace=True)\ntrain_y = train_df[\"totals.transactionRevenue\"].values\ntrain_id = train_df[\"fullVisitorId\"].values\ntest_id = test_df[\"fullVisitorId\"].values\n\n\n# label encode the categorical variables and convert the numerical variables to float\n\nfor col in cat_cols:\n    print(col)\n    lbl = preprocessing.LabelEncoder()\n    lbl.fit(list(train_df[col].values.astype('str')) + list(test_df[col].values.astype('str')))\n    train_df[col] = lbl.transform(list(train_df[col].values.astype('str')))\n    test_df[col] = lbl.transform(list(test_df[col].values.astype('str')))\n\ncat_cols_1 = ['__' + col for col in cat_cols]\n\nby_user_train = train_df.groupby('fullVisitorId')\nby_user_test = test_df.groupby('fullVisitorId')\n\ntrn_cat = (by_user_train[cat_cols]).agg(lambda x:x.value_counts().index[0])\ntest_cat = (by_user_test[cat_cols]).agg(lambda x:x.value_counts().index[0])\n\ntrn_cat = trn_cat.fillna(0)\ntest_cat = test_cat.fillna(0)\n\ntrn_cat = trn_cat.astype('category')\ntest_cat = test_cat.astype('category')\ntrn_cat.columns = cat_cols_1\ntest_cat.columns = cat_cols_1\ntrn_cat.to_csv('train_cat_mode.csv')\ntest_cat.to_csv('test_cat_mode.csv')\"\"\"\npass","5fa4c540":"import os\nprint(os.listdir(\"..\/input\"))","ef581574":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import mean_squared_error\nimport gc\nimport time\nfrom pandas.core.common import SettingWithCopyWarning\nimport warnings\nimport lightgbm as lgb\nfrom sklearn.model_selection import GroupKFold\n\n# I don't like SettingWithCopyWarnings ...\nwarnings.simplefilter('error', SettingWithCopyWarning)\ngc.enable()\n%matplotlib inline","d59173ee":"train = pd.read_csv('..\/input\/create-extracted-json-fields-dataset\/extracted_fields_train.gz', \n                    dtype={'date': str, 'fullVisitorId': str, 'sessionId':str}, nrows=None)\ntest = pd.read_csv('..\/input\/create-extracted-json-fields-dataset\/extracted_fields_test.gz', \n                   dtype={'date': str, 'fullVisitorId': str, 'sessionId':str}, nrows=None)\ntrain.shape, test.shape","8fd9e980":"def get_folds(df=None, n_splits=5):\n    \"\"\"Returns dataframe indices corresponding to Visitors Group KFold\"\"\"\n    # Get sorted unique visitors\n    unique_vis = np.array(sorted(df['fullVisitorId'].unique()))\n\n    # Get folds\n    folds = GroupKFold(n_splits=n_splits)\n    fold_ids = []\n    ids = np.arange(df.shape[0])\n    for trn_vis, val_vis in folds.split(X=unique_vis, y=unique_vis, groups=unique_vis):\n        fold_ids.append(\n            [\n                ids[df['fullVisitorId'].isin(unique_vis[trn_vis])],\n                ids[df['fullVisitorId'].isin(unique_vis[val_vis])]\n            ]\n        )\n\n    return fold_ids","11a8efe1":"y_reg = train['totals.transactionRevenue'].fillna(0)\ndel train['totals.transactionRevenue']\n\nif 'totals.transactionRevenue' in test.columns:\n    del test['totals.transactionRevenue']","be80035c":"train.columns","38a6fa15":"train['target'] = y_reg\nfor df in [train, test]:\n    df['vis_date'] = pd.to_datetime(df['visitStartTime'], unit='s')\n    df['sess_date_dow'] = df['vis_date'].dt.dayofweek\n    df['sess_date_hours'] = df['vis_date'].dt.hour\n    df['sess_date_dom'] = df['vis_date'].dt.day\n    df.sort_values(['fullVisitorId', 'vis_date'], ascending=True, inplace=True)\n    df['next_session_1'] = (\n        df['vis_date'] - df[['fullVisitorId', 'vis_date']].groupby('fullVisitorId')['vis_date'].shift(1)\n    ).astype(np.int64) \/\/ 1e9 \/\/ 60 \/\/ 60\n    df['next_session_2'] = (\n        df['vis_date'] - df[['fullVisitorId', 'vis_date']].groupby('fullVisitorId')['vis_date'].shift(-1)\n    ).astype(np.int64) \/\/ 1e9 \/\/ 60 \/\/ 60\n    \n#     df['max_visits'] = df['fullVisitorId'].map(\n#         df[['fullVisitorId', 'visitNumber']].groupby('fullVisitorId')['visitNumber'].max()\n#     )\n    \n    df['nb_pageviews'] = df['date'].map(\n        df[['date', 'totals.pageviews']].groupby('date')['totals.pageviews'].sum()\n    )\n    \n    df['ratio_pageviews'] = df['totals.pageviews'] \/ df['nb_pageviews']\n    \n#     df['nb_sessions'] = df['date'].map(\n#         df[['date']].groupby('date').size()\n#     )\n    \n#     df['nb_sessions_28_ma'] = df['date'].map(\n#         df[['date']].groupby('date').size().rolling(28, min_periods=7).mean()\n#     )\n\n#     df['nb_sessions_28_ma'] = df['nb_sessions'] \/ df['nb_sessions_28_ma']\n\n#     df['nb_sessions_per_day'] = df['date'].map(\n#         df[['date']].groupby('date').size()\n#     )\n    \n#     df['nb_visitors_per_day'] = df['date'].map(\n#         df[['date','fullVisitorId']].groupby('date')['fullVisitorId'].nunique()\n#     )\n\ny_reg = train['target']\ndel train['target']","2bc484b3":"excluded_features = [\n    'date', 'fullVisitorId', 'sessionId', 'totals.transactionRevenue', \n    'visitId', 'visitStartTime', 'vis_date', 'nb_sessions', 'max_visits'\n]\n\ncategorical_features = [\n    _f for _f in train.columns\n    if (_f not in excluded_features) & (train[_f].dtype == 'object')\n]","f2f3fb11":"for f in categorical_features:\n    train[f], indexer = pd.factorize(train[f])\n    test[f] = indexer.get_indexer(test[f])","0aa454fb":"folds = get_folds(df=train, n_splits=5)\n\ntrain_features = [_f for _f in train.columns if _f not in excluded_features]\nprint(train_features)\n\nimportances = pd.DataFrame()\noof_reg_preds = np.zeros(train.shape[0])\nsub_reg_preds = np.zeros(test.shape[0])\nfor fold_, (trn_, val_) in enumerate(folds):\n    trn_x, trn_y = train[train_features].iloc[trn_], y_reg.iloc[trn_]\n    val_x, val_y = train[train_features].iloc[val_], y_reg.iloc[val_]\n    \n    reg = lgb.LGBMRegressor(\n        num_leaves=31,\n        learning_rate=0.03,\n        n_estimators=1000,\n        subsample=.9,\n        colsample_bytree=.9,\n        random_state=1\n    )\n    reg.fit(\n        trn_x, np.log1p(trn_y),\n        eval_set=[(val_x, np.log1p(val_y))],\n        early_stopping_rounds=50,\n        verbose=100,\n        eval_metric='rmse'\n    )\n    imp_df = pd.DataFrame()\n    imp_df['feature'] = train_features\n    imp_df['gain'] = reg.booster_.feature_importance(importance_type='gain')\n    \n    imp_df['fold'] = fold_ + 1\n    importances = pd.concat([importances, imp_df], axis=0, sort=False)\n    \n    oof_reg_preds[val_] = reg.predict(val_x, num_iteration=reg.best_iteration_)\n    oof_reg_preds[oof_reg_preds < 0] = 0\n    _preds = reg.predict(test[train_features], num_iteration=reg.best_iteration_)\n    _preds[_preds < 0] = 0\n    sub_reg_preds += np.expm1(_preds) \/ len(folds)\n    \nmean_squared_error(np.log1p(y_reg), oof_reg_preds) ** .5","7e339eb2":"import warnings\nwarnings.simplefilter('ignore', FutureWarning)\n\nimportances['gain_log'] = np.log1p(importances['gain'])\nmean_gain = importances[['gain', 'feature']].groupby('feature').mean()\nimportances['mean_gain'] = importances['feature'].map(mean_gain['gain'])\n\nplt.figure(figsize=(8, 12))\nsns.barplot(x='gain_log', y='feature', data=importances.sort_values('mean_gain', ascending=False))","c7237264":"train['predictions'] = np.expm1(oof_reg_preds)\ntest['predictions'] = sub_reg_preds","dcc3034a":"# Aggregate data at User level\ntrn_data = train[train_features + ['fullVisitorId']].groupby('fullVisitorId').mean()\ntmp = train[train_features + ['fullVisitorId']].groupby('fullVisitorId').std()\ntmp.columns = ['_' + name for name in tmp.columns]\ntrn_data = pd.concat((trn_data,tmp), axis=1, sort=False)","4ddfce7f":"%%time\n# Create a list of predictions for each Visitor\ntrn_pred_list = train[['fullVisitorId', 'predictions']].groupby('fullVisitorId')\\\n    .apply(lambda df: list(df.predictions))\\\n    .apply(lambda x: {'pred_'+str(i): pred for i, pred in enumerate(x)})","81338265":"# Create a DataFrame with VisitorId as index\n# trn_pred_list contains dict \n# so creating a dataframe from it will expand dict values into columns\ntrn_all_predictions = pd.DataFrame(list(trn_pred_list.values), index=trn_data.index)\ntrn_feats = trn_all_predictions.columns\ntrn_all_predictions['t_mean'] = np.log1p(trn_all_predictions[trn_feats].mean(axis=1))\ntrn_all_predictions['t_max'] = np.log1p(trn_all_predictions[trn_feats].max(axis=1))\ntrn_all_predictions['t_min'] = np.log1p(trn_all_predictions[trn_feats].min(axis=1))\ntrn_all_predictions['t_std'] = np.log1p(trn_all_predictions[trn_feats].std(axis=1))\ntrn_all_predictions['t_median'] = np.log1p(trn_all_predictions[trn_feats].median(axis=1))\ntrn_all_predictions['t_sum_log'] = np.log1p(trn_all_predictions[trn_feats]).sum(axis=1)\ntrn_all_predictions['t_sum_act'] = np.log1p(trn_all_predictions[trn_feats].fillna(0).sum(axis=1))\ntrn_all_predictions['t_nb_sess'] = trn_all_predictions[trn_feats].isnull().sum(axis=1)\nfull_data = pd.concat([trn_data, trn_all_predictions], axis=1)\ndel trn_data, trn_all_predictions\ngc.collect()\nfull_data.shape","4c24cd22":"%%time\nsub_pred_list = test[['fullVisitorId', 'predictions']].groupby('fullVisitorId')\\\n    .apply(lambda df: list(df.predictions))\\\n    .apply(lambda x: {'pred_'+str(i): pred for i, pred in enumerate(x)})","56f50b18":"sub_data = test[train_features + ['fullVisitorId']].groupby('fullVisitorId').mean()\n\ntmp = test[train_features + ['fullVisitorId']].groupby('fullVisitorId').std()\ntmp.columns = ['_' + name for name in tmp.columns]\nsub_data = pd.concat((sub_data,tmp), axis=1, sort=False)\n\nsub_all_predictions = pd.DataFrame(list(sub_pred_list.values), index=sub_data.index)\nfor f in trn_feats:\n    if f not in sub_all_predictions.columns:\n        sub_all_predictions[f] = np.nan\nsub_all_predictions['t_mean'] = np.log1p(sub_all_predictions[trn_feats].mean(axis=1))\nsub_all_predictions['t_median'] = np.log1p(sub_all_predictions[trn_feats].median(axis=1))\nsub_all_predictions['t_sum_log'] = np.log1p(sub_all_predictions[trn_feats]).sum(axis=1)\nsub_all_predictions['t_min'] = np.log1p(sub_all_predictions[trn_feats].min(axis=1))\nsub_all_predictions['t_max'] = np.log1p(sub_all_predictions[trn_feats].max(axis=1))\nsub_all_predictions['t_std'] = np.log1p(sub_all_predictions[trn_feats].std(axis=1))\nsub_all_predictions['t_sum_act'] = np.log1p(sub_all_predictions[trn_feats].fillna(0).sum(axis=1))\nsub_all_predictions['t_nb_sess'] = sub_all_predictions[trn_feats].isnull().sum(axis=1)\nsub_full_data = pd.concat([sub_data, sub_all_predictions], axis=1)\ndel sub_data, sub_all_predictions\ngc.collect()\nsub_full_data.shape","b3e096bc":"trn_cat = pd.read_csv('..\/input\/mode-of-ga\/train_cat_mode.csv', dtype={'fullVisitorId': 'str'}, index_col='fullVisitorId')\ntest_cat = pd.read_csv('..\/input\/mode-of-ga\/test_cat_mode.csv', dtype={'fullVisitorId': 'str'}, index_col='fullVisitorId')","e3eebd84":"trn_cat = trn_cat.astype('category')\ntest_cat = test_cat.astype('category')","3dc0e102":"full_data = pd.concat((full_data, trn_cat), axis=1, sort=False)","43bceebb":"sub_full_data = pd.concat((sub_full_data, test_cat), axis=1, sort=False)","44e744a1":"train['target'] = y_reg\ntrn_user_target = train[['fullVisitorId', 'target']].groupby('fullVisitorId').sum()","1c43fd8c":"folds = get_folds(df=full_data[['totals.pageviews']].reset_index(), n_splits=5)\n\noof_preds = np.zeros(full_data.shape[0])\nsub_preds = np.zeros(sub_full_data.shape[0])\nvis_importances = pd.DataFrame()\n\nfor fold_, (trn_, val_) in enumerate(folds):\n    trn_x, trn_y = full_data.iloc[trn_], trn_user_target['target'].iloc[trn_]\n    val_x, val_y = full_data.iloc[val_], trn_user_target['target'].iloc[val_]\n    \n    reg = lgb.LGBMRegressor(\n        num_leaves=31,\n        learning_rate=0.03,\n        n_estimators=1000,\n        subsample=.9,\n        colsample_bytree=.9,\n        random_state=1\n    )\n    reg.fit(\n        trn_x, np.log1p(trn_y),\n        eval_set=[(trn_x, np.log1p(trn_y)), (val_x, np.log1p(val_y))],\n        eval_names=['TRAIN', 'VALID'],\n        early_stopping_rounds=50,\n        eval_metric='rmse',\n        verbose=100\n    )\n    \n    imp_df = pd.DataFrame()\n    imp_df['feature'] = trn_x.columns\n    imp_df['gain'] = reg.booster_.feature_importance(importance_type='gain')\n    \n    imp_df['fold'] = fold_ + 1\n    vis_importances = pd.concat([vis_importances, imp_df], axis=0, sort=False)\n    \n    oof_preds[val_] = reg.predict(val_x, num_iteration=reg.best_iteration_)\n    oof_preds[oof_preds < 0] = 0\n    \n    # Make sure features are in the same order\n    _preds = reg.predict(sub_full_data[full_data.columns], num_iteration=reg.best_iteration_)\n    _preds[_preds < 0] = 0\n    sub_preds += _preds \/ len(folds)\n    \nmean_squared_error(np.log1p(trn_user_target['target']), oof_preds) ** .5","2d22025b":"vis_importances['gain_log'] = np.log1p(vis_importances['gain'])\nmean_gain = vis_importances[['gain', 'feature']].groupby('feature').mean()\nvis_importances['mean_gain'] = vis_importances['feature'].map(mean_gain['gain'])\n\nplt.figure(figsize=(8, 25))\nsns.barplot(x='gain_log', y='feature', data=vis_importances.sort_values('mean_gain', ascending=False).iloc[:300])","e5a6cea7":"sub_full_data['PredictedLogRevenue'] = sub_preds\nsub_full_data[['PredictedLogRevenue']].to_csv('test_with_cat_mode.csv', index=True)","3e1fa426":"### Define folding strategy","f52ce4e4":"### Add date features\n\nOnly add the one I think can ganeralize","860ef006":"### Predict revenues at session level","4c9c7957":"### Display feature importances","45df36fb":"### Factorize categoricals","f5004c85":"### Create features list","5b318860":"### Get session target","afecd658":"### Create target at Visitor level","12bf48a6":"### Display feature importances","f405c6af":"### Create user level predictions","5900954b":"### Get the extracted data","6de3a342":"Modification I_have_seen_the_future by olivier for learning purpose. \nAdd additional aggregation to core. \nAdd mode calculation on user-level. \nCode below - script to calculate mode, script still running on kaggle virtual machine, so i added eternal data at this point.T\n\n","2b6f35af":"### Train a model at Visitor level","25769204":"### Save predictions"}}