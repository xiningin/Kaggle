{"cell_type":{"cbed51ca":"code","3e09ee39":"code","09e71ebf":"code","39483cdb":"code","e0a7711c":"code","840564a6":"code","af35e8ce":"code","0169cdde":"code","480e12d8":"code","02b256b0":"code","54af9bd6":"code","9cafe42f":"code","3d4b2b10":"code","b1ac124c":"code","cb35dd18":"code","fae15b2a":"code","fce93dc5":"code","63fb1ecc":"code","6009b0cf":"code","44ed1031":"code","a8a68784":"code","639f7a46":"code","27db5f04":"code","b5c3518d":"code","b27adfb2":"code","8a706f6f":"code","5cc41b5f":"code","c73aa27f":"code","72fac96b":"code","0ecf68da":"code","feffc251":"code","71248883":"code","6bb9fd72":"code","908412b9":"code","975c6d84":"code","234fb364":"code","6cc8290c":"code","a8e83bfb":"code","12c21c27":"code","c6e3c94e":"code","793fd173":"code","2eb08d8c":"code","6cfb9a3d":"code","0139a531":"code","a6a47823":"code","736a85c5":"markdown","dfd1c009":"markdown","b7320171":"markdown","55727cb5":"markdown","3f5d7bfb":"markdown","a1b45896":"markdown","249d18af":"markdown","b90629f3":"markdown","7931ed3a":"markdown","fdd46c2d":"markdown","e576fc93":"markdown","a3273a41":"markdown","f9de10a8":"markdown","0b250b85":"markdown","e2348903":"markdown","dd9081e8":"markdown","23948125":"markdown","de539ece":"markdown"},"source":{"cbed51ca":"import re\nimport nltk\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import f1_score\nfrom collections import Counter\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords, wordnet\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","3e09ee39":"os.listdir(\"..\/input\/nlp-getting-started\/\")\ndata_dir = \"..\/input\/\"","09e71ebf":"data = pd.read_csv(data_dir + \"nlp-getting-started\/train.csv\")\ntest_data = pd.read_csv(data_dir + \"nlp-getting-started\/test.csv\")\n\ndata.head()","39483cdb":"print(\"There are {} tweets\".format(len(data)))","e0a7711c":"print(\"Null values for each column (% of total amount of data):\\n\")\n(data.isnull().sum() \/ len(data)) * 100","840564a6":"#train_data = train_data.drop(\"location\", 1) # remove loaction column\ndata = data.dropna(axis=0) # remove missing keyword entries","af35e8ce":"data.target.value_counts() # Sample from majority class accordingly","0169cdde":"num_neg = data.target.value_counts()[0]\nnum_pos = data.target.value_counts()[1]\n\nfrac = num_pos \/ num_neg\n\nneg_data = data[data.target == 0].sample(frac=frac)\npos_data = data[data.target == 1]\n\nnew_data = pd.concat([pos_data, neg_data])\n\n#Shuffle the dataframe\nnew_data = new_data.sample(frac=1)","480e12d8":"new_data.sample(frac=1)","02b256b0":"new_data.target.value_counts() # Balance issues fixed!","54af9bd6":"print(\"Number of tweets w\/ #: \", len([x for x in new_data.text.values if '#' in x])) # tweets with #\nprint(\"Number of tweets w\/ @: \", len([x for x in new_data.text.values if '@' in x])) # tweets with @","9cafe42f":"# Keywords\nnew_data[['keyword', 'target']].keyword.value_counts()[:25]","3d4b2b10":"kwords = {}\ndisaster_label = 1\n\nfor word in new_data.keyword.unique():\n    kword = new_data[new_data.keyword == word]\n    total = kword.target.value_counts().sum()\n    \n    try:\n        frac = kword.target.value_counts()[disaster_label] \/ total\n    except:\n        frac = 0.0 # not in any disaster-related tweets  \n        \n    kwords[word] = frac\n\nsorted_kwords = {k: v for k, v in sorted(kwords.items(), key=lambda item: item[1], reverse=True)}","b1ac124c":"# Filter dict to get all those with majority association to disasters\nnew_dict = dict(filter(lambda x: x[1] > 0.75, sorted_kwords.items()))","cb35dd18":"items = new_dict\n\nplt.figure(figsize=(30,10))\n\n#x = list(range(0, len(sorted_kwords.keys())))\nx = list(items.keys())\ny = list(items.values())\n\nplt.xticks(rotation = 90, fontsize=25)\nplt.plot(x, y)\nplt.show()","fae15b2a":"def clean_text(text):\n    \n    new_text = text.lower() # lowercase the text\n    new_text = re.sub(r\"\\w+\\:\\\/\\\/([a-z]+)\\.co\\\/\\w+(\\n)?\", \"\", new_text) #remove urls\n    new_text = re.sub(r\"@[a-zA-Z0-9]+(?:;)*\", \"\", new_text) # remove @s\n    new_text = re.sub(r\"#\", \"\", new_text) # remove #s\n    new_text = re.sub(r\"[^a-z0-9A-Z]\", \" \", new_text) # remove non alphanumerics\n    new_text = re.sub(r\"[0-9]+[^\\w+]\", \"\", new_text) # remove words made wholy of digits\n    new_text = re.sub(r\"\\b\\w{1,2}\\b\", \"\", new_text) # remove words w\/ 1 char\n    new_text = re.sub(\" +\", \" \", new_text) # remove multiple consecutive spaces\n    \n    new_text = new_text.strip() # remove leading\/trailing whitespaces\n    \n    return new_text","fce93dc5":"new_data.text.values","63fb1ecc":"#Test cleaning on given tweet\ni = 6\nfor tweet in new_data.text.values[i:]:\n    print(\"Original: \", tweet)\n    tweet = clean_text(tweet)\n    print(\"Cleaned: \", tweet)\n    break","6009b0cf":"tweets = {}\nfor i, tweet in enumerate(new_data.text):\n    tweets[i] = clean_text(tweet)\n    \nlabels = {}\nfor i, label in enumerate(new_data.target):\n    labels[i] = label","44ed1031":"import random\n\nfor i in range(len(tweets)):\n    temp, label = tweets[i], labels[i]\n    j = random.randint(0, len(temp.split())-1)\n    word = temp.split()[j]\n    temp = temp.replace(word, \"\")\n    temp = re.sub(\" +\", \" \", temp) # remove multiple consecutive spaces\n    temp = temp.strip() # remove leading\/trailing whitespaces\n\n    tweets[len(tweets)] = temp\n    labels[len(labels)] = label","a8a68784":"lm = nltk.stem.WordNetLemmatizer()\nall_tokens = [item for _, value in tweets.items() for item in word_tokenize(value)]\n#all_tokens_lm = [lm.lemmatize(t) for t in all_tokens]\nall_tokens_lm = [lm.lemmatize(t) for t in all_tokens if t not in stopwords.words('english')]","639f7a46":"N = len(all_tokens_lm)\nV = len(set(all_tokens_lm))\n         \nprint(f\"There are {N} tokens after processing\")\nprint(f\"There are {V} unique tokens after processing\")","27db5f04":"def filter_dict(tweets, sentiments, sent):\n    \"\"\"\n    Gets a dictionary with tweets of a certain sentiment\n    \n    Inputs:\n        tweets: dict, contains the tweets (key = ID, value = tweet)\n        sentiments: dict, contains the sentiments (key = ID, value = 0 or 1)\n        sent: string, the sentiment (1 for \"disaster\", 0 for \"non-disaster\")\n    \n    Note: tweets & sentiments need to have the same ID\n    \"\"\"\n    new_dict = {}\n    for key, value in tweets.items():\n        if sentiments[key] == sent:\n            new_dict[key] = value\n            \n    return new_dict\n\ndef count_occurences(w, counts):\n    try:\n        return counts[w]\n    except:\n        return 0","b5c3518d":"### Testing Fucntionality ### \ntest = filter_dict(tweets, labels, 0)\nlist(test.items())[:10]","b27adfb2":"#60-20-20 train-dev-test split\ncutoff = int(0.8*len(tweets))\ntrain_cutoff = int(0.6*len(tweets))\n\ntrain_set = dict(list(tweets.items())[:train_cutoff])\ntrain_labels = dict(list(labels.items())[:train_cutoff])\n\nvalidation_set = dict(list(tweets.items())[train_cutoff:cutoff])\nvalidation_labels = dict(list(labels.items())[train_cutoff:cutoff])\n\ntest_set = dict(list(tweets.items())[cutoff:])\ntest_labels = dict(list(labels.items())[cutoff:])","8a706f6f":"import random\nfrom sklearn.utils import shuffle\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.callbacks import EarlyStopping \nfrom tensorflow.keras.layers import Embedding, Flatten, Dense, LSTM, Dropout, MaxPooling1D, SpatialDropout1D, Conv1D\nfrom tensorflow.keras.layers import Bidirectional, BatchNormalization, Conv1D, MaxPooling1D, GlobalMaxPooling1D\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","5cc41b5f":"max_seq_len = 0\nfor val in list(train_set.values()):\n    length = len(val.split())\n    max_seq_len = max(max_seq_len, length)\nprint(\"Maximum tweet length: {}\".format(max_seq_len))","c73aa27f":"from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n\n#Vectorize only the top 5000 tokens of the training data, then fetch vocab.\nvectorizer = TextVectorization(max_tokens=5000, output_sequence_length=max_seq_len, standardize=None)\nvectorizer.adapt(list(train_set.values()) + list(validation_set.values()))\n\nvocab = vectorizer.get_vocabulary()","72fac96b":"X_train = vectorizer(np.array([[s] for s in list(train_set.values())])).numpy()\ny_train = np.array(list(train_labels.values()))\n\nX_valid = vectorizer(np.array([[s] for s in list(validation_set.values())])).numpy()\ny_valid = np.array(list(validation_labels.values()))\n\nprint(f\"Shape of training data: {X_train.shape}\")\nprint(f\"Shape of validation data: {X_valid.shape}\")","0ecf68da":"fname = data_dir + \"glove-global-vectors-for-word-representation\/glove.6B.100d.txt\"\ndef build_embedding_matrix(fname, vocab, embedding_dim=100):\n    \n    #First two elems of matrix are for padding, <UNK> tokens\n    matrix, embedding_map = np.zeros((len(vocab)+2, embedding_dim)), {}\n    with open(fname, \"r\", encoding='utf8') as f:\n        for line in f.readlines():\n            \n            elems = line.split()\n            #First element is word, rest are vector entries\n            token = elems[0]\n            vec = np.array([float(x) for x in elems[1:]])\n            #Get the index of the word in our word tokens list\n            #Then place the vector at position given by index\n            embedding_map[token] = vec\n    \n    suc, fail = 0, 0\n    wordmap = dict(zip(vocab, range(len(vocab))))\n    for word, i in wordmap.items():\n        #Only words in vocab that have embedding can be used\n        vec = embedding_map.get(word)\n        if vec is not None:\n            matrix[i] = vec\n            suc += 1\n        else:\n            fail += 1\n    \n    print(f\"Number of words successfully embedded: {suc}, Unsuccessful attempts: {fail}\")\n    return matrix","feffc251":"#Takes a second...\nembedding_matrix = build_embedding_matrix(fname, vocab)","71248883":"print(f\"Shape of the embedding matrix: {embedding_matrix.shape}\")","6bb9fd72":"num_tokens = embedding_matrix.shape[0]\nembedding_dim = embedding_matrix.shape[1]\nmax_seq_len = X_train.shape[1]\n\nembedding_layer = Embedding(\n    num_tokens,\n    embedding_dim,\n    embeddings_initializer = keras.initializers.Constant(embedding_matrix),\n    trainable=False,\n    input_length = max_seq_len\n)","908412b9":"class custom_attention(keras.layers.Layer):\n    def __init__(self, return_sequences=True):\n        self.return_sequences=return_sequences\n        super(custom_attention, self).__init__()\n        \n    def get_config(self):\n        config = super().get_config().copy()\n        config.update({'return_sequences': self.return_sequences})\n        return config\n    \n    def build(self, input_shape):\n        ##last index of the input_shape is the number of dimensions of the prev\n        ##RNN layer. last but 1 index is the num of timesteps\n        self.w=self.add_weight(shape=(input_shape[-1], 1), initializer=\"normal\", name='weight')\n        self.b=self.add_weight(shape=(input_shape[1], 1), initializer=\"zeros\", name='bias')\n        \n        super(custom_attention, self).build(input_shape)\n        \n    def call(self, x):\n        ##x is the input tensor..each word that needs to be attended to\n        ##Below is the main processing done during training\n        ##K is the Keras Backend import\n        e = K.tanh(K.dot(x,self.w)+self.b)\n        a = K.softmax(e, axis=1)\n        output = x*a\n        \n        if self.return_sequences:\n            return output\n        \n\n        return K.sum(output, axis=1)","975c6d84":"model = Sequential()\nmodel.add(embedding_layer)\nmodel.add(SpatialDropout1D(0.50))\nmodel.add(Bidirectional(LSTM(max_seq_len, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)))\nmodel.add(Dropout(0.5))\nmodel.add(custom_attention(return_sequences=False))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.summary()","234fb364":"batch_size = 128\nepochs = 100\nlr = 0.001","6cc8290c":"#Enable early stopping if the validation loss hasn't been decreasing for 20 epochs\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20)","a8e83bfb":"optimizer = keras.optimizers.Adam(learning_rate=lr)\nmodel.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer=optimizer, metrics=['accuracy'])\n\nhistory = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, \n                     callbacks=[es], validation_data=(X_valid, y_valid))","12c21c27":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss=history.history['loss']\nval_loss=history.history['val_loss']\n\nepochs_range = range(epochs)\n\nplt.figure(figsize=(10, 4))\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\n\nplt.show()","c6e3c94e":"test = vectorizer(np.array([[s] for s in list(test_set.values())])).numpy()\ntest = pad_sequences(test, maxlen=X_train.shape[1])","793fd173":"predictions = model.predict(test)\npredictions[predictions > 0.5] = 1\npredictions[predictions < 0.5] = 0","2eb08d8c":"score_lstm = f1_score(list(test_labels.values()), predictions)\nprint(\"BiLSTM F1 score: {}\".format(score_lstm))","6cfb9a3d":"test_tweets = {}\nfor i, tweet in enumerate(test_data.text):\n    test_tweets[i] = clean_text(tweet)\n    \nXtest = vectorizer(np.array([[s] for s in list(test_tweets.values())])).numpy()\nXtest = pad_sequences(Xtest, maxlen=X_train.shape[1])\n\ny_pred = model.predict(Xtest)\ny_pred[y_pred > 0.5] = 1\ny_pred[y_pred < 0.5] = 0","0139a531":"sm = pd.read_csv(\"..\/input\/nlp-getting-started\/sample_submission.csv\")\nsm['target'] = y_pred.astype('int')\nsm","a6a47823":"sm.to_csv(\"submission.csv\", index=False)","736a85c5":"### Train the model!","dfd1c009":"## Data augmentation: delete random word from each sentence, add back into data","b7320171":"### Get number of tokens and vocab size","55727cb5":"# Model: Bidirectional LSTM w\/ Attention","3f5d7bfb":"### May be able to use statistics above to improve classification, although, the number of tweets that these keywords are appearing in is a very small fraction of the total number of tweets. Will neglect the keyword information for now and just focus on the text and the target","a1b45896":"## For each keyword, find percent of tweets that are disaster related","249d18af":"### Get tokens and lemmatize tweets (removing stopwords)","b90629f3":"### Is data balanced?","7931ed3a":"### Define attention layer","fdd46c2d":"### Define the model","e576fc93":"# Get final predictions","a3273a41":"### Get the maximum tweet length","f9de10a8":"### No! Balance the data","0b250b85":"### Learning curves","e2348903":"### 30% of location are NaN. Location may indicate likelihood of disaster tweet, but still not always indicative. Will leave out for now.\n\n### Also, there is a very small percentage with missing keywords, so I will remove these entries for now too.","dd9081e8":"### Preprocess tweets","23948125":"# Output to file","de539ece":"## Evaluate on the test set"}}