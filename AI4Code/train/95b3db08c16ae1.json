{"cell_type":{"0e57b504":"code","ace2a07d":"code","8a5ad82c":"code","fe97f6d1":"code","d257efb3":"code","05408971":"code","51a81f9e":"code","94b620ab":"code","cf9f074e":"code","9df8fd2f":"code","78ec0331":"code","65a32bc2":"code","10388e03":"code","2afacfeb":"code","3c467e0b":"code","36d333bd":"code","118bb158":"code","1eb4a7e8":"code","b8767d64":"code","a0b5b7f2":"code","84b5cbf5":"code","afa6a7be":"code","a22424c9":"code","e6e76af0":"code","4872a48b":"code","02aa4ea7":"code","24292643":"markdown","81c4ada3":"markdown","6c21087a":"markdown","97178350":"markdown","f4128bc4":"markdown","a1638cc9":"markdown","4b38163a":"markdown","12863a05":"markdown","12382a21":"markdown","24bc17d1":"markdown","05d303b9":"markdown","aa6dac04":"markdown","ddd64446":"markdown","eb641b30":"markdown","403ecf0a":"markdown","aba1ff20":"markdown","979644ae":"markdown","7d2ed0d9":"markdown","dbac38db":"markdown","950d5e82":"markdown","bb9b4597":"markdown","552b7719":"markdown","15ccaad7":"markdown","102192e0":"markdown","9e26b09c":"markdown","07149b24":"markdown"},"source":{"0e57b504":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/nab\/realKnownCause\/realKnownCause'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ace2a07d":"# Extra Libs\nimport seaborn as sns\nimport matplotlib\nimport matplotlib.dates as mdates\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport holoviews as hv\nfrom holoviews import opts\nhv.extension('bokeh')\nfrom bokeh.models import HoverTool\nfrom IPython.display import HTML, display\n\n# Default visual settings\n\nplt.rcParams[\"font.family\"] = \"serif\"\nplt.rcParams['figure.dpi'] = 150\n\n# Ignore warnings\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom sklearn.ensemble import IsolationForest","8a5ad82c":"df = pd.read_csv('\/kaggle\/input\/nab\/realKnownCause\/realKnownCause\/nyc_taxi.csv')\ndf.head(5).style.set_caption('New York City Taxi Demand').format({'value':\"{:,.0f}\"})","fe97f6d1":"print(df.isnull().sum(),'\\n')\nprint(df.dtypes)","d257efb3":"# Convert timestamp in to datetime format\ndf['timestamp'] = pd.to_datetime(df['timestamp'])","05408971":"# Range we're dealing with\nprint('Start: ',df['timestamp'].min())\nprint('End: ',df['timestamp'].max())\nprint('Days: ',(df['timestamp'].max() - df['timestamp'].min()))","51a81f9e":"Hourly = hv.Curve(df.set_index('timestamp').resample('H').mean()).opts(\n    opts.Curve(title=\"New York City Taxi Demand Hourly\", xlabel=\"\", ylabel=\"Demand\",\n               width=700, height=300,tools=['hover'],show_grid=True))\n\nDaily = hv.Curve(df.set_index('timestamp').resample('D').mean()).opts(\n    opts.Curve(title=\"New York City Taxi Demand Daily\", xlabel=\"\", ylabel=\"Demand\",\n               width=700, height=300,tools=['hover'],show_grid=True))\n\nWeekly = hv.Curve(df.set_index('timestamp').resample('W').mean()).opts(\n    opts.Curve(title=\"New York City Taxi Demand Weekly\", xlabel=\"Date\", ylabel=\"Demand\",\n               width=700, height=300,tools=['hover'],show_grid=True))\n\n\n(Hourly + Daily + Weekly).opts(shared_axes=False).cols(1)","94b620ab":"# A variety of resamples which I may or may not use\ndf_hourly = df.set_index('timestamp').resample('H').mean().reset_index()\ndf_daily = df.set_index('timestamp').resample('D').mean().reset_index()\ndf_weekly = df.set_index('timestamp').resample('W').mean().reset_index()","cf9f074e":"# New features \n# Loop to cycle through both DataFrames\n\nfor DataFrame in [df_hourly, df_daily]:\n    \n    DataFrame['Weekday'] = pd.Categorical(DataFrame['timestamp'].dt.strftime('%A'), categories=['Monday', 'Tuesday', 'Wednesday', 'Thursday','Friday', 'Saturday', 'Sunday'])\n    DataFrame['Hour'] = DataFrame['timestamp'].dt.hour\n    DataFrame['Day'] = DataFrame['timestamp'].dt.weekday\n    DataFrame['Month'] = DataFrame['timestamp'].dt.month\n    DataFrame['Year'] = DataFrame['timestamp'].dt.year\n    DataFrame['Month_day'] = DataFrame['timestamp'].dt.day\n\n\n    DataFrame['Lag'] = DataFrame['value'].shift(1)\n    DataFrame['Rolling_Mean'] = DataFrame['value'].rolling(7).mean()\n    \n\n#from statsmodels.graphics.tsaplots import plot_acf\n#plot_acf(df_hourly['value'], lags=10)\n#plot_pacf(df_hourly['value'], lags=10)    ","9df8fd2f":"hv.Distribution(df['value']).opts(opts.Distribution(title=\"Overall Value Distribution\", xlabel=\"Value\", ylabel=\"Density\", width=700, height=300,tools=['hover'],show_grid=True))","78ec0331":"by_weekday = df_hourly.groupby(['Hour','Weekday']).mean()['value'].unstack()\nplot = hv.Distribution(by_weekday['Monday'], label='Monday') * hv.Distribution(by_weekday['Tuesday'], label='Tuesday') * hv.Distribution(by_weekday['Wednesday'], label='Wednesday') * hv.Distribution(by_weekday['Thursday'], label='Thursday') * hv.Distribution(by_weekday['Friday'], label='Friday') * hv.Distribution(by_weekday['Saturday'], label='Saturday') *hv.Distribution(by_weekday['Sunday'], label='Sunday').opts(opts.Distribution(title=\"Demand Density by Day & Hour\"))\nplot.opts(opts.Distribution(width=800, height=300,tools=['hover'],show_grid=True, ylabel=\"Demand\", xlabel=\"Demand\"))\n","65a32bc2":"#df_hourly[['value','Weekday']].groupby('Weekday').mean().plot()\n\nhv.Bars(df_hourly[['value','Weekday']].groupby('Weekday').mean()).opts(\n    opts.Bars(title=\"New York City Taxi Demand by Day\", xlabel=\"\", ylabel=\"Demand\",\n               width=700, height=300,tools=['hover'],show_grid=True))","10388e03":"hv.Curve(df_hourly[['value','Hour']].groupby('Hour').mean()).opts(\n    opts.Curve(title=\"New York City Taxi Demand Hourly\", xlabel=\"Hour\", ylabel=\"Demand\",\n               width=700, height=300,tools=['hover'],show_grid=True))","2afacfeb":"by_weekday = df_hourly.groupby(['Hour','Weekday']).mean()['value'].unstack()\nplot = hv.Curve(by_weekday['Monday'], label='Monday') * hv.Curve(by_weekday['Tuesday'], label='Tuesday') * hv.Curve(by_weekday['Wednesday'], label='Wednesday') * hv.Curve(by_weekday['Thursday'], label='Thursday') * hv.Curve(by_weekday['Friday'], label='Friday') * hv.Curve(by_weekday['Saturday'], label='Saturday') *hv.Curve(by_weekday['Sunday'], label='Sunday').opts(opts.Curve(title=\"Average Demand by Day & Hour\"))\nplot.opts(opts.Curve(width=800, height=300,tools=['hover'],show_grid=True, ylabel=\"Demand\"))\n\n# in Matplotlib\n# #df_hourly.groupby(['Hour','Weekday']).mean()['value'].unstack().plot()","3c467e0b":"df_hourly = df_hourly.join(df_hourly.groupby(['Hour','Weekday'])['value'].mean(),\non = ['Hour', 'Weekday'], rsuffix='_Average')\ndf_hourly.head()","36d333bd":"df_daily = df_daily.join(df_daily.groupby(['Hour','Weekday'])['value'].mean(),\non = ['Hour', 'Weekday'], rsuffix='_Average')","118bb158":"sat_max = df_hourly.query(\"Day == 5\").set_index('timestamp').loc['2015-01-31':'2015-01-31'].reset_index()['value']\navg_sat = df_hourly.groupby(['Weekday','Hour'])['value'].mean().unstack().T['Saturday']\nplot2 = hv.Curve(avg_sat, label='Average Saturday') * hv.Curve(sat_max, label='Busiest Saturday').opts(opts.Curve(title=\"Average Saturday vs Busiest Saturday\"))\nplot2.opts(opts.Curve(width=800, height=300,tools=['hover'],show_grid=True, ylabel=\"Demand\", show_legend=False))\n","1eb4a7e8":"df_daily = df_daily.dropna()\ndf_hourly = df_hourly.dropna()\ndf_hourly.head()","b8767d64":"# Daily\ndf_daily_model_data = df_daily[['value', 'Hour', 'Day',  'Month','Month_day','Rolling_Mean']].dropna()\n\n# Hourly\nmodel_data = df_hourly[['value', 'Hour', 'Day', 'Month_day', 'Month','Rolling_Mean','Lag']]\nmodel_data.head()","a0b5b7f2":"IF = IsolationForest(random_state=0, contamination=0.005, n_estimators=200, max_samples=0.7)\nIF.fit(model_data)\n\n# New Outliers Column\n\ndf_hourly['Outliers'] = pd.Series(IF.predict(model_data)).apply(lambda x: 1 if x == -1 else 0)\n\n# Get Anomaly Score\n\nscore = IF.decision_function(model_data)\n\n# New Anomaly Score column\n\ndf_hourly['Score'] = score\ndf_hourly.head()","84b5cbf5":"def outliers(thresh):\n    print(f'Number of Outliers below Anomaly Score Threshold {thresh}:')\n    print(len(df_hourly.query(f\"Outliers == 1 & Score <= {thresh}\")))","afa6a7be":"tooltips = [\n    ('Weekday', '@Weekday'),\n    ('Day', '@Month_day'),\n    ('Month', '@Month'),\n    ('Value', '@value'),\n    ('Average Vale', '@value_Average'),\n    ('Outliers', '@Outliers')\n]\nhover = HoverTool(tooltips=tooltips)\n\n\n\nhv.Points(df_hourly.query(\"Outliers == 1\")).opts(size=10, color='#ff0000') * hv.Curve(df_hourly).opts(opts.Curve(title=\"New York City Taxi Demand Anomalies\", xlabel=\"\", ylabel=\"Demand\" , height=300, responsive=True,tools=[hover,'box_select', 'lasso_select', 'tap'],show_grid=True))","a22424c9":"len(df_hourly.query(\"Outliers == 1\"))","e6e76af0":"frequencies, edges = np.histogram(score, 50)\nprint('Values: %s, Edges: %s' % (frequencies.shape[0], edges.shape[0]))\nhv.Histogram((edges, frequencies)).opts(width=800, height=300,tools=['hover'], xlabel='Score')","4872a48b":"# Function to view number of outliers at a given threshold\noutliers(0.05)\n\n#for num in (np.arange(-0.08, 0.2, 0.02)):\n #   print(len(df_hourly.query(f\"Outliers == 1 & Score <= {num}\")))\n  #  num_outliers = len(df_hourly.query(f\"Outliers == 1 & Score <= {num}\"))","02aa4ea7":"tooltips = [\n    ('Weekday', '@Weekday'),\n    ('Day', '@Month_day'),\n    ('Month', '@Month'),\n    ('Value', '@value'),\n    ('Average Vale', '@value_Average'),\n    ('Outliers', '@Outliers')\n]\nhover = HoverTool(tooltips=tooltips)\n\n\n\nhv.Points(df_hourly.query(\"Outliers == 1 & Score <= 0.05\")).opts(size=10, color='#ff0000') * hv.Curve(df_hourly).opts(opts.Curve(title=\"New York City Taxi Demand\", xlabel=\"\", ylabel=\"Demand\" , height=300, responsive=True,tools=[hover,'box_select', 'lasso_select', 'tap'],show_grid=True))","24292643":"**Housekeeping**\n\nChecking for blank values, checking Data Types etc.","81c4ada3":"# **Viewing the Anomalies**","6c21087a":"By changing the threshold for anomalies, we are effectively determining the sensitivity of our model.","97178350":"# **Visual Overview**","f4128bc4":"Through the plots above we learn a few interesting things.\n\nLet's now turn to average hourly demand.","a1638cc9":"By plotting the anomalies in our data we can begin to assess how our model performs.","4b38163a":"**Choose Features for model**","12863a05":"# **Conclusion**\n\nThe aim of this notebook was to add anomaly detection to my portfolio, and to utilise a new data visulisation package (Bokeh\/HoloVoews). I have acheived both of those aims and am happy with the outcome.\n\nAnomaly detection is an area I'll likely be exploring more soon! There really is a lot of value in this field.\n\nAs for the data visualisation, I enjoyed using Bokeh\/HoloViews. The interactivity is a nice feature. \n\nHowever, my preferred libraries are still Maptlotlib & Seaborn due to the amount of elements & customisation a user can enjoy.\n\nMaybe next I'll try Plotly!?","12382a21":"# **Models**\n\nBelow is the DataFrame with the new Feautres created earlier in the notebook.\n\nI'll drop the Null values created by the new features","24bc17d1":"We can now see the anomaly scores for each data point. The lower, the more abnormal. Negative scores represent outliers, positive scores represent inliers.\n\nThis offers us some flexibility in determining our cutoff points for anomalies","05d303b9":"# **Anomaly Detection in Time Series Data**\n\nThis will be a **short notebook exploring Anomaly Detection**. I will, initially, use just one algorithm (**Isolation Forest**), but with the view to expand this notebook over time.\n\nThe Isolation Forest \u2018isolates\u2019 observations by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature.\n\nSince recursive partitioning can be represented by a tree structure, the number of splittings required to isolate a sample is equivalent to the path length from the root node to the terminating node.\n\nThis path length, averaged over a forest of such random trees, is a measure of normality and our decision function.\n\nRandom partitioning produces noticeably shorter paths for anomalies. Hence, when a forest of random trees collectively produce shorter path lengths for particular samples, they are highly likely to be anomalies.","aa6dac04":"# **Feature Engineering**","ddd64446":"Let's see how an average Saturday compares to the Saturday with the highest demand in our dataset","eb641b30":"When displayed Hourly, the dataset is hard to fully understand. I will resample this from hourly to daily to weekly, and see if we can pick out any interesting features.\n\n**A Quick note on the visuals**\n\nMy previous notebooks all have a strong focus on data visualisation, using primarily Matplotlib & Seaborn. \n\nToday though, I will use **Holoviews & Bokeh**. I want to expand my Data Visualisation toolkit and this library is, to me at least, more visually pleasing than Plotly (although that too is a tool I want to begin practising with as I have seen some fantastic Plotly-based notebooks).","403ecf0a":"# **The Data**\n\nThe dataset I will use here is the New York City Taxi Demand dataset.\n\nThe raw data is from the NYC Taxi and Limousine Commission.\nThe data file included here consists of aggregating the total number of\ntaxi passengers into 30 minute buckets.\n\n\n**Some Inspiration & References for this Project**\n\n\nhttps:\/\/www.kaggle.com\/victorambonati\/unsupervised-anomaly-detection\n\nhttps:\/\/www.youtube.com\/watch?v=XCF-kqCB_vA&ab_channel=AIEngineering\n\nhttps:\/\/www.kaggle.com\/koheimuramatsu\/industrial-machine-anomaly-detection\/comments\n\nhttps:\/\/holoviews.org\/\n\nhttp:\/\/holoviews.org\/user_guide\/Plotting_with_Bokeh.html\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.IsolationForest.html","aba1ff20":"We can also see how this varies by day. \n\nThe legend acts as a filter here, so one can select\/deselect certain days.","979644ae":"Seeing the data plotted in different units is helpful to see the underlying trends of the data.\n\nHourly data may contain a lot of information, but it is difficult to spot anomoalies at a glance. \nIn contrast, Daily & Weekly plotting is much easier to understand. We also spot clearly times of year when demand is boosted and when it lags.","7d2ed0d9":"**Fit Model & View Outliers**","dbac38db":"# **Choices**\n\nDetermining the cut-off point for anomaly scores is a subjective decision. \n\nIt will likely depends on the business, and exactly what the anomalies represent. As with many Machine Learning tasks (especially classification or anomaly detection), the balance is often between being over-cautious and highlighting too many potential anomalies, and being under-cautious and risk missing genuine anomalies.\n\n**Next Steps**\n\nI didn't tune the Isolation Forest model at all, so this is an obvious first step.\n\nIn addition, there are many other anomaly detection techniques that could be employed. Perhaps I'll add them to this notebook over time.\n\nOther methods might include:\n\n* Clustering\n* Gaussian Probability\n* One-Class SVM\n* Markov processes\n","950d5e82":"# **More Feature Engineering**","bb9b4597":"I'll now plot only those Outliers with an anomaly score of less than 0.05 - reflecting the function above","552b7719":"# **Assessing Outliers**","15ccaad7":"As I said above, we can now see the anomaly scores for our dataset. The lower, the more abnormal. Negative scores represent outliers, positive scores represent inliers.\n\nThis offers us some flexibility in determining our cutoff points for anomalies.","102192e0":"We are trying to detect anomales in Taxi Demand. This is the 'value' column","9e26b09c":"# **More Visual Exploration**","07149b24":"The dataset has just two columns. \n\nIt will be good to do some **Feauture Engineering** later to extract as much information as we can from these existing features."}}