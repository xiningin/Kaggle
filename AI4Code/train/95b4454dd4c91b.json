{"cell_type":{"ade20fab":"code","bd9670bd":"code","3dd91af5":"code","b4d11645":"code","5b5fbfc1":"code","608685e9":"code","efe84e9d":"code","ec57556e":"code","9a4c3ef9":"code","5ccc8f4f":"code","192ab3f2":"code","b126be0c":"code","debf3dee":"code","1d33176f":"code","5763a98d":"code","d93169c3":"code","b0d976d9":"code","d65bea5f":"code","a1eababa":"code","5749d0d0":"code","737005a8":"code","2e155464":"code","4f58bb20":"code","cf5da3d2":"code","00bcac2a":"code","337baa30":"code","6c9e1bc5":"markdown","55f18ed4":"markdown","5f95a1f0":"markdown","a13bc9df":"markdown","1ee4845d":"markdown","36d7cc12":"markdown","91c83917":"markdown"},"source":{"ade20fab":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\nimport re\n","bd9670bd":"pip install stanza","3dd91af5":"import stanza","b4d11645":"# Importing the Classic Reuters-21578 Data Set\nfrom nltk.corpus import reuters","5b5fbfc1":"#https:\/\/www.kaggle.com\/alvations\/testing-1000-files-datasets-from-nltk\nfrom nltk.corpus import (LazyCorpusLoader, CategorizedPlaintextCorpusReader)\nreuters = LazyCorpusLoader('reuters', CategorizedPlaintextCorpusReader, \n                           '(training|test).*', cat_file='cats.txt', encoding='ISO-8859-2',\n                          nltk_data_subdir='\/kaggle\/input\/reuters\/reuters\/reuters\/')","608685e9":"# List of documents\ndocuments = reuters.fileids()\nprint(str(len(documents)) + \" documents\");\n \ntrain_docs = list(filter(lambda doc: doc.startswith(\"train\"),\ndocuments));\nprint(str(len(train_docs)) + \" total train documents\");\n\ntrain_documents, train_categories = zip(*[(reuters.raw(i), reuters.categories(i)) for i in reuters.fileids() if i.startswith('training\/')])","efe84e9d":"# List of categories\ncategories = reuters.categories();\nprint(str(len(categories)) + \" categories\");\nprint(categories)","ec57556e":"print(train_documents[0])","9a4c3ef9":"print(train_categories[0])","5ccc8f4f":"stanza.download('en')\nnlp = stanza.Pipeline('en')","192ab3f2":"doc = nlp(train_documents[0])","b126be0c":"print(\"Numbers of sentences, tokens, words, and entities :\")\nprint(len(doc.sentences))\nprint(doc.num_tokens)\nprint(doc.num_words)\nprint(len(doc.entities))","debf3dee":"for i, sent in enumerate(doc.sentences):\n    print(\"[Sentence {}]\".format(i+1))\n    for word in sent.words:\n        print(\"{:12s}\\t{:12s}\\t{:6s}\".format(\\\n              word.text, word.lemma, word.pos))\n    print(\"\")","1d33176f":"from collections import Counter\n\ndef cleanText(text):\n    text = text.lower()\n    # replace line breaks with one space\n    text = re.sub(r\"\\n\", \"\", text)\n    # replace multiple spacess with one space\n    text = re.sub('\\s+', ' ', text)  \n    # replace '\\' by nothing because there are a lot in the text for some reason\n    text.replace(\"\\\\\",\"\")\n    return text","5763a98d":"text = cleanText(train_documents[0])\nwords = text.split(\" \")\n\ntext","d93169c3":"Counter(text).most_common(20)","b0d976d9":"train_splittedTexts = []\nfor document in train_documents:\n    text = cleanText(document)\n    train_splittedTexts.append(text.split())","d65bea5f":"indexes = [word for splittedText in train_splittedTexts for word in splittedText]  \nprint(f\"There are {len(indexes)} words in the data corpus\")","a1eababa":"Counter(indexes).most_common(200)","5749d0d0":"# Some of the most common nouns\nwordsList = [\"u.s.\", \"loss\", \"company\", \"bank\", \"profit\", \"trade\", \"oil\", \"foreign\", \"international\", \"japan\", \"agreement\", \"money\", \"growth\", \"spokesman\", \"officials\", \"national\", \"investment\", \"cash\", \"banks\"]","737005a8":"# Getting a string of the full corpus:\nfullText = \"\"\nfor document in train_documents: \n    text = cleanText(document)\n    fullText = fullText + text\nfullText[:1000]","2e155464":"len(fullText)","4f58bb20":"fullText[92150:92250]","cf5da3d2":"# Setting up Stanza\n# Turn on the gpu\nfullText = fullText[:92150] + \"a\"*100 + fullText[92250:] #because problem\nfullDoc = nlp(fullText[:100000])\n\n# ~1min per 1M characters, so around 40min for the full text","00bcac2a":"print(\"Sentence 100: \")\nprint(fullDoc.sentences[100].text)\nprint(\"\")\nprint (\"Sentiment: \")\nprint(fullDoc.sentences[100].sentiment)","337baa30":"# For each words, finding the corresponding \"entity\" created by Stanza\nentitiesList = [ent for ent in fullDoc.entities if ent.text in wordsList]\n\n#Attributing a score for each word\nnWords = len(wordsList)\nwordsScore = [1]*len(wordsList)\n\nfor i in range(0, nWords):\n    nOccurence = 0\n    for ent in entitiesList:\n        if ent.text == wordsList[i]:\n            #Getting the sentiment of the parent sentence and adding it to the score\n            wordsScore[i] += ent._sent.sentiment\n            nOccurence+=1\n    # Calculating the mean score\n    if (nOccurence>0):\n        wordsScore[i] = wordsScore[i]\/nOccurence\n    \n# Displaying the scores\nprint(\"{:13s}\\t{:13s}\".format(\"       Word\", \"Sentiment (<1 is negative, >1 is positive)\"))\nprint(\"\")\nfor i in range(0, nWords):\n    print(f\"{wordsList[i]:>13}, {wordsScore[i]-1:>13}\")","6c9e1bc5":"### Counting the words","55f18ed4":"Here is a selection of the most used words in the corpus: u.s., loss, company, bank, profit, trade, oil, foreign, international, japan, agreement, money, growth, spokesman, officials, national, investment, cash, banks.","5f95a1f0":"Sentiment of 0,1, and 2 means respectively negative, neutral, and positive.","a13bc9df":"Let's use Stanza's built-in sentiment analyzer for each of the words above.\n\nReference: https:\/\/stanfordnlp.github.io\/stanza\/sentiment.html","1ee4845d":"### Sentiment analysis","36d7cc12":"Getting the sentiment of the first sentence:","91c83917":"Counting the most common words:"}}