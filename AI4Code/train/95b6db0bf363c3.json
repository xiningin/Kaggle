{"cell_type":{"636112c3":"code","88c2fcd8":"code","08f8518b":"code","58d1ac4a":"code","3ab84b9a":"code","b6ea0305":"code","0248c442":"code","6eb4a4e2":"code","843f933b":"code","9f8dc95e":"code","9b110a43":"code","c7e5b206":"code","c72a72ea":"code","488e9860":"code","b4538305":"code","37d5d420":"code","3b368de0":"code","809c5670":"code","71fea49e":"code","618557b8":"code","c567b2c7":"markdown","dee8ed8a":"markdown","1c54a976":"markdown","110a022f":"markdown","816e2c2b":"markdown","ba67bc43":"markdown","8ea84030":"markdown","b94c31a8":"markdown","53259d05":"markdown","0cb1d223":"markdown"},"source":{"636112c3":"import os\nimport numpy as np\nimport pandas as pd\nimport sys\nimport gc\nimport re\nimport time\nfrom tqdm.auto import tqdm\nfrom transformers import AutoModel, AutoTokenizer, logging\n# Ignore model init warning\nlogging.set_verbosity_error()\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nfrom IPython.display import display\nfrom copy import deepcopy\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n# Ignore tokenizers warning\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"","88c2fcd8":"valid_df = pd.read_csv(\"..\/input\/jigsaw-toxic-severity-rating\/validation_data.csv\")\nvalid_df.head()","08f8518b":"type(valid_df)","58d1ac4a":"def clean(data):\n\n    # Clean some punctutations\n    data = re.sub('\\n', ' ', data)\n    # Remove ip address\n    data = re.sub(r'(([0-9]+\\.){2,}[0-9]+)',' ', data)\n    \n    data = re.sub(r'([a-zA-Z]+)([\/!?.])([a-zA-Z]+)',r'\\1 \\2 \\3', data)\n    # Replace repeating characters more than 3 times to length of 3\n    data = re.sub(r'([*!?\\'])\\1\\1{2,}',r'\\1\\1\\1', data)\n    # patterns with repeating characters \n    data = re.sub(r'([a-zA-Z])\\1{2,}\\b',r'\\1\\1', data)\n    data = re.sub(r'([a-zA-Z])\\1\\1{2,}\\B',r'\\1\\1\\1', data)\n\n    # Add space around repeating characters\n    data = re.sub(' +', ' ', data)\n    \n    # Ex) I didn ' t -> I didn't\n    data = re.sub(\" ' \", \"'\", data)\n    \n    return data","3ab84b9a":"valid_df[\"less_toxic\"] = valid_df[\"less_toxic\"].apply(clean)\nvalid_df[\"more_toxic\"] = valid_df[\"more_toxic\"].apply(clean)\nvalid_df.head()","b6ea0305":"comments_df = pd.read_csv(\"..\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv\")\nsubmission_df = pd.read_csv(\"..\/input\/jigsaw-toxic-severity-rating\/sample_submission.csv\")\ndisplay(comments_df.head())\ndisplay(submission_df.head())","0248c442":"jigsaw_train_df = pd.read_csv(\"..\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv\")\njigsaw_train_df.head()","6eb4a4e2":"features = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n\nfor feature in features:\n    print(\"*\"*20, feature.upper(), \"*\"*20)\n    display(jigsaw_train_df[jigsaw_train_df[feature]==1][[\"comment_text\", feature]].sample(5))","843f933b":"jigsaw_label = deepcopy(jigsaw_train_df)\n\nFEATURE_WTS = {\n    'severe_toxic': 3, 'identity_hate': 1.5, 'threat': 1.5, \n    'insult': 0.64, 'toxic': 2, 'obscene': 0.16, \n}\n\nFEATURES = list(FEATURE_WTS.keys())\n\njigsaw_label['label'] = 0\nfor feat, wt in FEATURE_WTS.items(): \n    jigsaw_label.label += wt*jigsaw_label[feat]\njigsaw_label.label = jigsaw_label.label\/jigsaw_label.label.max()\n    \npos = jigsaw_label[jigsaw_label.label>0]\nneg = jigsaw_label[jigsaw_label.label==0].sample(len(pos), random_state=201)\njigsaw_label = pd.concat([pos, neg])\njigsaw_label = jigsaw_label[[\"comment_text\", \"label\"]]\njigsaw_label","9f8dc95e":"jigsaw_label[\"comment_text\"] = jigsaw_label[\"comment_text\"].apply(clean)\njigsaw_label","9b110a43":"# Define Train Dataset Class\nclass JigsawDataset(Dataset):\n    def __init__(self, df, tokenizer, max_length, text_col=\"comment_text\", is_test=False):\n        self.df = df\n        self.max_len = max_length\n        self.tokenizer = tokenizer\n        self.is_test = is_test\n        if not is_test:\n            self.labels = [torch.tensor(label, dtype=torch.float) for label in tqdm(df['label'].values)]\n        self.texts = [\n            self.tokenizer.encode_plus(\n                                text,\n                                truncation=True,\n                                add_special_tokens=True,\n                                max_length=self.max_len,\n                                padding='max_length'\n                            )\n            for text in tqdm(df[text_col].values)]\n        \n        self.input_ids = [torch.tensor(text[\"input_ids\"], dtype=torch.long) for text in tqdm(self.texts)]\n        self.attention_masks = [torch.tensor(text[\"attention_mask\"], dtype=torch.long) for text in tqdm(self.texts)]\n\n        \n    def __len__(self):\n        return len(self.df)\n    \n    \n    def __getitem__(self, index):\n        if self.is_test:\n            return {\n                \"text_ids\": self.input_ids[index],\n                \"text_mask\": self.attention_masks[index]\n            }\n        \n        label = self.labels[index]\n        return {\n            \"text_ids\": self.input_ids[index],\n            \"text_mask\": self.attention_masks[index],\n            \"label\": label\n        }\n","c7e5b206":"model_name = \"..\/input\/roberta-base\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)","c72a72ea":"train_dataset = JigsawDataset(\n    df=jigsaw_label,\n    tokenizer=tokenizer,\n    max_length=tokenizer.max_model_input_sizes[model_name.split(\"\/\")[-1]]\n)\n\nvalid_less_dataset = JigsawDataset(\n    df=valid_df,\n    tokenizer=tokenizer,\n    max_length=tokenizer.max_model_input_sizes[model_name.split(\"\/\")[-1]],\n    text_col=\"less_toxic\",\n    is_test=True\n)\n\nvalid_more_dataset = JigsawDataset(\n    df=valid_df,\n    tokenizer=tokenizer,\n    max_length=tokenizer.max_model_input_sizes[model_name.split(\"\/\")[-1]],\n    text_col=\"more_toxic\",\n    is_test=True\n)\n\ntest_dataset = JigsawDataset(\n    df=comments_df,\n    tokenizer=tokenizer,\n    max_length=tokenizer.max_model_input_sizes[model_name.split(\"\/\")[-1]],\n    text_col=\"text\",\n    is_test=True\n)","488e9860":"train_dataloader = DataLoader(\n    train_dataset,\n    batch_size=16,\n    num_workers=2,\n    pin_memory=True,\n    shuffle=True\n)\n\nvalid_less_dataloader = DataLoader(\n    valid_less_dataset,\n    batch_size=16,\n    num_workers=2,\n    pin_memory=True,\n    shuffle=False\n)\n\nvalid_more_dataloader = DataLoader(\n    valid_more_dataset,\n    batch_size=16,\n    num_workers=2,\n    pin_memory=True,\n    shuffle=False\n)\n\ntest_dataloader = DataLoader(\n    test_dataset,\n    batch_size=16,\n    num_workers=2,\n    pin_memory=True,\n    shuffle=False\n)","b4538305":"class JigsawModel(nn.Module):\n    def __init__(self, model_name):\n        super(JigsawModel, self).__init__()\n        self.model = AutoModel.from_pretrained(model_name)\n        self.drop = nn.Dropout(p=0.2)\n        self.fc = nn.Linear(768, 1)\n        \n    def forward(self, ids, mask):        \n        out = self.model(input_ids=ids,\n                         attention_mask=mask,\n                         output_hidden_states=False)\n        out = self.drop(out[1])\n        outputs = self.fc(out)\n        return outputs","37d5d420":"learning_rate = 1e-5\nepochs = 2\n\nmodel = JigsawModel(model_name).to(device)\ncriterion = nn.L1Loss()\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer,\n                                                max_lr=learning_rate, \n                                                steps_per_epoch=len(train_dataloader),\n                                                epochs=epochs,\n                                                pct_start=0.05\n                                               )","3b368de0":"def train(model, optimizer, scheduler, dataloader, device):\n    \n    model.train()\n    dataset_size = 0\n    running_loss = 0.0\n\n    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n    for step, data in bar:\n        text_ids = data['text_ids'].to(device, dtype = torch.long)\n        text_mask = data['text_mask'].to(device, dtype = torch.long)\n        targets = data['label'].to(device, dtype=torch.float)\n\n        batch_size = text_ids.size(0)\n\n        outputs = model(text_ids, text_mask)\n\n        loss = criterion(outputs, targets)\n\n        loss.backward()\n\n        optimizer.step()\n        optimizer.zero_grad()\n        \n        if scheduler is not None:\n            scheduler.step()\n\n        running_loss += (loss.item() * batch_size)\n        dataset_size += batch_size\n\n        epoch_loss = running_loss \/ dataset_size\n\n        bar.set_postfix(Epoch=epoch, Train_Loss=epoch_loss,\n                        LR=optimizer.param_groups[0]['lr'])\n        \n    return epoch_loss\n            \n        ","809c5670":"@torch.no_grad()\ndef valid(model, loader, device):\n    \n    all_labels = []\n    model.eval()\n    for data in tqdm(loader):\n        \n        text_ids = data['text_ids'].to(device, dtype = torch.long)\n        text_mask = data['text_mask'].to(device, dtype = torch.long)\n\n        batch_size = text_ids.size(0)\n\n        outputs = model(text_ids, text_mask).view(-1)\n        all_labels.extend(list(outputs.detach().cpu().numpy()))\n        \n    return np.array(all_labels)","71fea49e":"# Training Part\nbest_acc = 0\nall_preds = []\nfor epoch in tqdm(range(epochs)):\n    \n    epoch_loss = train(\n        model = model,\n        optimizer = optimizer,\n        scheduler = scheduler,\n        dataloader = train_dataloader,\n        device = device\n    )\n    \n    less_labels = valid(model, valid_less_dataloader, device)\n    more_labels = valid(model, valid_more_dataloader, device)\n    preds = more_labels - less_labels\n    accuracy = len(preds[preds > 0])\/len(preds)\n\n    if accuracy > best_acc:\n        print(f\"Best Accuracy Updated: {accuracy:.4f}\")\n        print(f\"Outdated Best Acc.: {best_acc:.4f}\")\n        best_acc = accuracy\n        best_submission = valid(model, test_dataloader, device)\n        if os.path.exists(\"checkpoint.pth\"):\n            os.remove(\"checkpoint.pth\")\n        torch.save(model, \"checkpoint.pth\")","618557b8":"submission_df['score'] = best_submission\nsubmission_df.to_csv(\"submission.csv\", index=False)","c567b2c7":"## Define Model Class","dee8ed8a":"## Datasets & DataLoaders","1c54a976":"## Set-up for Training","110a022f":"# BERT only end-to-end Baseline for Jigsaw Toxic Classification\n\nHello, I'm a new Kaggler and this is the very first competition I participate in Kaggle  \nSaw many of great Kagglers utilize sklearn based architecture like TF-IDF, Ridge Regression which are really really awesome and well-performing!!  \nAnd I wrote just a simple baseline notebook for the freshman like me! (BERT architecture only ver.)  \nAppreciate any comment or advice except for TOXIC one LOL\ud83e\udd17  \n\nReferences :\n- https:\/\/www.kaggle.com\/debarshichanda\/pytorch-w-b-jigsaw-starter\n- https:\/\/www.kaggle.com\/chryzal\/jigsaw-ensemble-0-864\n- https:\/\/www.kaggle.com\/readoc\/toxic-linear-model-pseudo-labelling-lb-0-864  \n\nPlease Upvote for the above links which are much greater than mine :)\n","816e2c2b":"# Jigsaw Toxic Comment Classification Data","ba67bc43":"## Submit Your Best Validated Result","8ea84030":"### Approach Summary\n1. Train: BERT model with pseudo-labeled \"jigsaw-toxic-comment-classification-challenge\" data\n2. Valid: test accuracy with our \"jigsaw-toxic-severity-rating\" data after every one-epoch-training done\n3. Test: make submission with the model which got best accuracy in validation part","b94c31a8":"## Functions for Training","53259d05":"# Original Dataset","0cb1d223":"## Main Training Part"}}