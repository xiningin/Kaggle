{"cell_type":{"7d480ad1":"code","e634437f":"code","b38a202d":"code","653cd965":"code","2dcff792":"code","dd22e1d5":"code","415469d4":"code","2c4c64cb":"code","76436ebd":"code","94a7d489":"code","38b006aa":"code","baaeb0a0":"code","5027b58d":"code","1db90b0b":"code","386d77d9":"code","e1e95f80":"code","08fb7944":"code","d6a35db4":"code","30396155":"code","1abb3bed":"code","9b4af4fe":"code","53db2f7f":"code","a53f2205":"markdown","3552f104":"markdown","df8be454":"markdown","39ee6e7a":"markdown"},"source":{"7d480ad1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","e634437f":"header = pd.read_csv('..\/input\/header.csv')\ntrain = pd.read_csv('..\/input\/train.csv',names=header.columns)\ntest = pd.read_csv('..\/input\/test.csv', names=header.columns[:-1])\nsample = pd.read_csv('..\/input\/sample_submission.csv')","b38a202d":"test_key = test['key']\ntest.drop(columns=['key'], inplace=True)\ntrain.drop(columns=['key'], inplace=True)","653cd965":"train.head()","2dcff792":"# Function to calculate missing values by column# Funct \ndef missing_values_table(df):\n        # Total missing values\n        mis_val = df.isnull().sum()\n        \n        # Percentage of missing values\n        mis_val_percent = 100 * df.isnull().sum() \/ len(df)\n        \n        # Make a table with the results\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        \n        # Rename the columns\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        \n        # Sort the table by percentage of missing descending\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        \n        # Print some summary information\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns that have missing values.\")\n        \n        # Return the dataframe with missing information\n        return mis_val_table_ren_columns","dd22e1d5":"print(missing_values_table(train), '\\n')\nmissing_values_table(test)","415469d4":"def numRel(feature):\n    exp = train[[feature, 'label']]\n    exp=exp.groupby(feature).label.agg(['count', 'sum']).reset_index()\n    exp['label']=exp['sum']\/exp['count']\n    exp.plot(x=feature, y='label', marker='.')","2c4c64cb":"def kdeRel(feature):\n    plt.figure(figsize = (5, 4))\n\n    # KDE plot for area_assesed_Building removed\n    sns.kdeplot(train.loc[train['label'] == 0, feature], label = 'label == 0')\n    # KDE plot for area_assesed_Building removed\n    sns.kdeplot(train.loc[train['label'] == 1, feature], label = 'label == 1')\n\n    # Labeling of plot\n    plt.xlabel(feature); plt.ylabel('label'); plt.title('Matrix');","76436ebd":"numRel('V1')","94a7d489":"kdeRel('V3')","38b006aa":"exp = train[['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'label']]","baaeb0a0":"train.corr()['label'].sort_values(ascending=False).head(10)","5027b58d":"train.corr()['label'].sort_values(ascending=True).head(10)","1db90b0b":"train['v10v26'] = train['V10'] * train['V26']\ntest['v10v26'] = test['V10'] * test['V26']\n\ntrain['v1v11'] = train['V1'] * train['V11']\ntest['v1v11'] = test['V1'] * test['V11']\n\ntrain['v1v14'] = train['V1'] * train['V14']\ntest['v1v14'] = test['V1'] * test['V14']\n\ntrain['v6v11'] = train['V6'] * train['V11']\ntest['v6v11'] = test['V6'] * test['V11']\n\ntrain['v10v11'] = train['V10'] * train['V11']\ntest['v10v11'] = test['V10'] * test['V11']","386d77d9":"from sklearn.preprocessing import MinMaxScaler\nfrom sklearn.impute import SimpleImputer\n\ntrain_labels = train['label'].astype(float)\n# Drop the target from the training data\ntrain.drop(columns = ['label'], inplace=True)\n    \n# Feature names\nfeatures = list(train.columns)\n\n# Scale each feature to 0-1\nscaler = MinMaxScaler(feature_range = (0, 1))\n\n# with the scaler\nscaler.fit(train)\nstrain = scaler.transform(train)\nstest = scaler.transform(test)\ntrain['label'] = train_labels\nprint('Training data shape: ', strain.shape)\nprint('Testing data shape: ', stest.shape)","e1e95f80":"# from sklearn.linear_model import LogisticRegression\n# from sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBRegressor\n\n# Make the random forest classifier\n# clf = RandomForestClassifier(n_estimators = 100, random_state = 50, verbose = 1, n_jobs = -1)\n\n# Make the model with the specified regularization parameter\n# clf = LogisticRegression(C = 0.0001, n_jobs=-1)\n\n#Use XGBooster\nclf = XGBRegressor(n_estimators=100, learning_rate=0.08, gamma=0, subsample=0.75,\n                           colsample_bytree=1, max_depth=7)","08fb7944":"from sklearn.metrics import roc_auc_score\nfrom sklearn import model_selection\n\nX_train, X_test, y_train, y_test = model_selection.train_test_split(\n    strain, train_labels, test_size=0.30)\n# Train on the training data\nclf.fit(X_train, y_train, early_stopping_rounds=5, \n             eval_set=[(X_test, y_test)], verbose=True)\n# clf.fit(X_train, y_train)\n\n# roc_auc_score(y_test, clf.predict(X_test), average='weighted')\n# clf.score(X_test, y_test)","d6a35db4":"def plot_feature_importances(df):\n    \"\"\"\n    Plot importances returned by a model. This can work with any measure of\n    feature importance provided that higher importance is better. \n    \n    Args:\n        df (dataframe): feature importances. Must have the features in a column\n        called `features` and the importances in a column called `importance\n        \n    Returns:\n        shows a plot of the 15 most importance features\n        \n        df (dataframe): feature importances sorted by importance (highest to lowest) \n        with a column for normalized importance\n        \"\"\"\n    \n    # Sort features according to importance\n    df = df.sort_values('importance', ascending = False).reset_index()\n    \n    # Normalize the feature importances to add up to one\n    df['importance_normalized'] = df['importance'] \/ df['importance'].sum()\n\n    # Make a horizontal bar chart of feature importances\n    plt.figure(figsize = (10, 6))\n    ax = plt.subplot()\n    \n    # Need to reverse the index to plot most important on top\n    ax.barh(list(reversed(list(df.index[:15]))), \n            df['importance_normalized'].head(15), \n            align = 'center', edgecolor = 'k')\n    \n    # Set the yticks and labels\n    ax.set_yticks(list(reversed(list(df.index[:15]))))\n    ax.set_yticklabels(df['feature'].head(15))\n    \n    # Plot labeling\n    plt.xlabel('Normalized Importance'); plt.title('Feature Importances')\n    plt.show()\n    \n    return df","30396155":"# Extract feature importances\nfeature_importance_values = clf.feature_importances_\nfeature_importances = pd.DataFrame({'feature': features, 'importance': feature_importance_values})\n# Show the feature importances for the default features\nfeature_importances_sorted = plot_feature_importances(feature_importances)","1abb3bed":"#Prediction with classifier\ny=clf.predict(stest)\nprediction=pd.DataFrame({'key': test_key, 'score':y})\nprediction.to_csv('submission.csv', index=False)","9b4af4fe":"score = [round(i) for i in sample['score'].tolist()]\npred_score = [round(i) for i in y[:20]]\nprint(\"Score: \", roc_auc_score(score, pred_score, average='weighted'))\nprediction.head(20)","53db2f7f":"sample","a53f2205":"**PREDICTION**","3552f104":"**MISSING VALUES & ANAMOLY**","df8be454":"**FEATURE ENGINEERING**","39ee6e7a":"**GRAPHICAL ANALYSIS**"}}