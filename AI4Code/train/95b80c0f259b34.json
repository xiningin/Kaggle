{"cell_type":{"a37962b7":"code","a019dcd4":"code","a06d3f48":"code","3497cc74":"code","8db43337":"code","c7e34ad8":"code","702d92bf":"code","61826632":"code","11372b59":"code","14e9cca5":"code","6ff793b3":"code","9fd4e8ef":"code","5f7f6835":"code","60490754":"code","1ce7bb13":"code","7d365aeb":"code","7baa4706":"code","58069a3f":"code","627f42c4":"code","4ff9cf3b":"code","7441c3ff":"markdown","9a3b083e":"markdown","dc36fdae":"markdown","227755e3":"markdown","bb0e816c":"markdown","19d856fa":"markdown","3159fe45":"markdown","505c2839":"markdown","0f73909f":"markdown"},"source":{"a37962b7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a019dcd4":"train1 = pd.read_csv('\/kaggle\/input\/ranchpal-assignment-data\/E1_train.csv')\ntrain2 = pd.read_csv('\/kaggle\/input\/ranchpal-assignment-data\/D2_train.csv')\ntrain3 = pd.read_csv('\/kaggle\/input\/ranchpal-assignment-data\/W3_train.csv')\ntrain4 = pd.read_csv('\/kaggle\/input\/ranchpal-assignment-data\/S4_train.csv')\ntrain5 = pd.read_csv('\/kaggle\/input\/ranchpal-assignment-data\/L5_train.csv')\ntrain6 = pd.read_csv('\/kaggle\/input\/ranchpal-assignment-data\/RS6_train.csv')\ntrain7 = pd.read_csv('\/kaggle\/input\/ranchpal-assignment-data\/RL7_train.csv')\ntrain8 = pd.read_csv('\/kaggle\/input\/ranchpal-assignment-data\/G8_train.csv')\ntrain9 = pd.read_csv('\/kaggle\/input\/ranchpal-assignment-data\/I9_train.csv')","a06d3f48":"all_data = pd.concat([train1,train2,train3,train4,train5,train6,train7,train8,train9],ignore_index=True)\nall_data","3497cc74":"import seaborn as sns\nimport matplotlib.pyplot as plt","8db43337":"#checking null\nall_data.isnull().sum()","c7e34ad8":"sns.heatmap(all_data.isnull(),cbar=False,yticklabels=False)\n#we can visulaize there in no null data","702d92bf":"plt.figure(figsize=(11,7))\nsns.heatmap(all_data.corr(),annot=True)","61826632":"features = ['acc_x','acc_y','acc_z','gyr_x','gyr_y','gyr_z','mag_x','mag_y','mag_z']\nfor a in [train1,train2,train3,train4,train5,train6,train7,train8,train9]:\n    fig = plt.figure(figsize=(20,4))\n    for i,j in enumerate(features):\n        fig.add_subplot(1,9,i+1)\n        sns.boxplot(x=j,data=a)\n    \n    plt.show()","11372b59":"all_data.describe()","14e9cca5":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split,StratifiedKFold","6ff793b3":"X = all_data.drop('label',axis=1)\nY = all_data['label']\n","9fd4e8ef":"train_x,test_x,train_y,test_y = train_test_split(X,Y,test_size=0.2,shuffle=True,stratify=Y,random_state=30)","5f7f6835":"%time\nmodel = RandomForestClassifier(criterion='entropy',n_jobs=-1,max_depth=10)\nmodel.fit(train_x,train_y)","60490754":"from sklearn.metrics import f1_score , confusion_matrix\nval_pred = model.predict(test_x)\n","1ce7bb13":"confusion_matrix(test_y,val_pred,labels=[1,2,3,4,5,6,7,8,9])\n","7d365aeb":"f1_score(test_y,val_pred,average='micro')","7baa4706":"#there is noo need for tuning\n#import optuna\n#from xgboost import XGBClassifier\n#from sklearn.model_selection import StratifiedKFold","58069a3f":"test_y.value_counts()","627f42c4":"pred = pd.Series(val_pred)\npred.value_counts()","4ff9cf3b":"plt.figure(figsize = (12,7))\nx = [i for i in range(test_x.shape[0])]\nplt.plot(x[:100], test_y[:100], color = 'r', label = \"True output\")\nplt.plot(x[:100], val_pred[:100], color = 'b', label = \"Predicted output\")\nplt.ylabel(\"labels\", fontsize = 12)\nplt.title(\"First 100 True & Predicted outputs\", fontsize = 14)\nplt.legend()\nplt.show()","7441c3ff":"Most of the independent variables are weekly co-related. so there is no problem for training them","9a3b083e":"# Importing Datasets","dc36fdae":"## visulaizing our prediction","227755e3":"## Modeling","bb0e816c":"we got above 80 in f1 score , good accuracy","19d856fa":"### Exploratory Data Analysis","3159fe45":"we can hyperparameter tune with optuna. we will defintely get the above 95 % F1 score","505c2839":"**From the analyis there are outliers in the datas from the accuracy and gyro features.. but it is not neccesory for this model . this wont affect the model because , the data are well processed and structured.**","0f73909f":"As from the analysis we got the points that, the data is perfectly distributed ."}}