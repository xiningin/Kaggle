{"cell_type":{"31d3fdb3":"code","d9a14fe0":"code","87cba535":"code","3a93fe41":"code","625b0530":"code","76f06153":"code","ec15be26":"code","8798f9c7":"code","94070c74":"code","dc7815cd":"code","8fbd5d4c":"code","0258d05b":"code","89b934a3":"code","ddcea966":"code","943ddd17":"code","6abec49a":"code","b7069591":"code","e28b15b0":"code","244c4607":"code","b9c5f4b4":"code","d20cbdde":"code","e417132d":"code","fb54e3b6":"code","bfad44b3":"code","1595037f":"code","d6e99e49":"code","e1dc3891":"markdown","95c9cb5c":"markdown","89c107f6":"markdown","2781dadb":"markdown","01fd3df3":"markdown","88c05e59":"markdown","dd064620":"markdown","96e5988c":"markdown","6742796e":"markdown","c6332af3":"markdown"},"source":{"31d3fdb3":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport keras\nimport cv2\nfrom keras.models import Sequential\nfrom keras.models import load_model\nfrom keras.preprocessing.image import img_to_array\nfrom keras.layers import Dense,Flatten,Conv2D,MaxPool2D,Dropout\nfrom tensorflow.keras.layers import BatchNormalization\nimport pickle\nimport os\nimport warnings\nwarnings.filterwarnings(\"ignore\")       \nplt.style.use('fivethirtyeight') \n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","d9a14fe0":"train = pd.read_csv('\/kaggle\/input\/sign-language-recognition-alphabets\/TrainData.csv')\ntest = pd.read_csv('\/kaggle\/input\/sign-language-recognition-alphabets\/TestData.csv')\ntrain.head()","87cba535":"train.shape","3a93fe41":"train_label=train['label']\ntrain_label.head()\ntrainset=train.drop(['label'],axis=1)\ntrainset.head()","625b0530":"X_train = trainset.values\nX_train = trainset.values.reshape(-1,28,28,1)\nprint(X_train.shape)","76f06153":"test_label=test['label']\nX_test=test.drop(['label'],axis=1)\nprint(X_test.shape)\nX_test.head()","ec15be26":"from sklearn.preprocessing import LabelBinarizer\nlb=LabelBinarizer()\ny_train=lb.fit_transform(train_label)\ny_test=lb.fit_transform(test_label)","8798f9c7":"print(y_train)\nprint(\"-------------------\")\nprint(y_test)","94070c74":"X_test=X_test.values.reshape(-1,28,28,1)\nprint(X_train.shape,y_train.shape,X_test.shape,y_test.shape)","dc7815cd":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\ntrain_datagen = ImageDataGenerator(rescale = 1\/255.,\n                                  rotation_range = 0,\n                                  height_shift_range=0.2,\n                                  width_shift_range=0.2,\n                                  shear_range=0,\n                                  zoom_range=0.2,\n                                  horizontal_flip=True,\n                                  fill_mode='nearest')\n\nX_test=X_test\/255","8fbd5d4c":"base={0:'0',1:'A',2:'B',3:'C',4:'D',5:'E',6:'F',7:'G',8:'H',9:'I',10:'J',11:'K',12:'L',13:'M',14:'N',\n      15:'O',16:'P',17:'Q',18:'R',19:'S',20:'T',21:'U',22:'V',23:'W',24:'X',25:'Y',26:'Z'}\n","0258d05b":"label = np.argmax(y_train,axis=1)\nlabel","89b934a3":"f, mx= plt.subplots(4,4,figsize=(12,12))\nk=800\nfor i in range(4):\n    for j in range(4):\n        mx[i,j].set_title(\"Label: {} \".format(base[label[k]]))\n        mx[i,j].imshow(X_train[k].reshape(28,28),cmap='pink')\n        k+=300\n        \n    plt.tight_layout()","ddcea966":"model=Sequential()\nmodel.add(Conv2D(128,kernel_size=(5,5),\n                 strides=1,padding='same',activation='relu',input_shape=(28,28,1)))\nmodel.add(MaxPool2D(pool_size=(3,3),strides=2,padding='same'))\n\nmodel.add(Conv2D(32,kernel_size=(2,2),\n                strides=1,activation='relu',padding='same'))\nmodel.add(MaxPool2D((2,2),2,padding='same'))\n\nmodel.add(Conv2D(16,kernel_size=(2,2),\n                strides=1,activation='relu',padding='same'))\nmodel.add(MaxPool2D((2,2),2,padding='same'))\n          \nmodel.add(Flatten())","943ddd17":"model.add(Dense(units=512,activation='relu'))\nmodel.add(Dropout(rate=0.25))\nmodel.add(Dense(units=27,activation='softmax'))\nmodel.summary()","6abec49a":"model.compile(optimizer='adam',loss='categorical_crossentropy',\n              metrics=['accuracy'])","b7069591":"X_test.shape,y_test.shape,X_train.shape,y_train.shape","e28b15b0":"model.fit(train_datagen.flow(X_train,y_train,batch_size=200),\n         epochs = 135,\n          validation_data=(X_test,y_test),\n          shuffle=1)","244c4607":"(ls,acc)=model.evaluate(x=X_test,y=y_test)","b9c5f4b4":"print('MODEL ACCURACY = {}%'.format(acc*100))","d20cbdde":"pd=model.predict(X_test) \nclasses_x=np.argmax(pd,axis=1)","e417132d":"pd[0],classes_x[0]","fb54e3b6":"len(classes_x),len(pd)","bfad44b3":"print(\"Predicted: {}\".format(classes_x[800]))\nplt.imshow(X_test[800].reshape(28,28),cmap='pink')","1595037f":"actual = np.argmax(y_test,axis=1)\nactual","d6e99e49":"f, mx= plt.subplots(4,4,figsize=(16,16))\nk=300\nfor i in range(4):\n    for j in range(4):\n        mx[i,j].set_title(\"Predicted: {} Actual: {} \".format(base[classes_x[k]],base[actual[k]]))\n        mx[i,j].imshow(X_test[k].reshape(28,28),cmap='pink')\n        k+=100\n        \n    plt.tight_layout()","e1dc3891":"<h2 id='3.5'> 5. Prediction <\/h2>","95c9cb5c":"<h1 id='3'> Model Building <\/h1>","89c107f6":"<h1 id='2'> Sample Data<h1>\n<h3> Checking for some sample images <\/h3>","2781dadb":"<h2 id='3.1'> 1. Defining Model <\/h2>","01fd3df3":"<h2 id='3.2'> 2. Model Compiling <\/h2>","88c05e59":"<h2 id='3.3'> 3. Model Fitting <\/h2>","dd064620":"<h4> Adding droupout to prevent overfitting <\/h4>","96e5988c":"<h2 id='3.4'> 4. Model Evaluation <\/h2>","6742796e":"<h1 id=1> Importing Libraries and dataset <\/h1> ","c6332af3":"<h1> Sign Language Recognition <\/h1>\n\n![](https:\/\/t4.ftcdn.net\/jpg\/00\/70\/03\/79\/240_F_70037979_VKSXsDN9LFLAlyyOet9ByGpi9KQsWhrr.jpg)\n<hr \/>\n<h2> Details <\/h2>\n<h4 style=\"font-family: Noto Sans; padding: 10px;\"> \nIn this sign language recognition notebook, we create a sign detector, which detects alphabets from A to Z that can very easily be extended to cover a vast multitude of other signs and hand gestures including the alphabets.\n \n   <\/h4>\n   <h4 style=\"font-family: Noto Sans; padding: 10px;\">\nThe dataset format is patterned to match closely with the the orignal MNIST. Each training and test case represents a label (0-26) for each alphabetic letter A-Z and a blank. \n   The training data (12,844 cases) and test data (4267 cases) with a header row of label, pixel1,pixel2\u2026.pixel784 which represent a single 28x28 pixel image with grayscale values between 0-255. The original hand gesture image data represented multiple users repeating the gesture against different backgrounds. \n   <\/h4>\n   <h4 style=\"font-family: Noto Sans; padding: 10px;\">\n    Hand gesture is one of the method used in sign language for non-verbal communication. It is most commonly used by deaf & dumb people who have hearing or speech problems to communicate among themselves or with normal people. Various sign language systems has been developed by many makers around the world but they are neither flexible nor cost-effective for the end users. Hence in this paper introduced software which presents a system prototype that is able to automatically recognize sign language to help deaf and dumb people to communicate more effectively with each other or normal people. Pattern recognition and Gesture recognition are the developing fields of research. Being a significant part in nonverbal communication hand gestures are playing key role in our daily life. Hand Gesture recognition system provides us an innovative, natural, user friendly way of communication with the computer which is more familiar to the human beings. By considering in mind the similarities of human hand shape with four fingers and one thumb, the software aims to present a real time system for recognition of hand gesture on basis of detection of some shape based features like orientation, Centre of mass centroid, fingers status, thumb in positions of raised or folded fingers of hand (https:\/\/ieeexplore.ieee.org\/document\/7916786)\n   <\/h4> \n<hr \/>\n    \n<h2>Work Flow<\/h2>\n \n<ul>\n    <li><a href=\"#1\"><h3>Importing Libraries and Dataset<\/h3><\/a>\n    <\/li>\n    <li><a href=\"#2\"><h3>Sample Data<\/h3><\/a><\/li>\n    <li> <a href=\"#3\"><h3>Modeling<\/h3><\/a>\n        <ul>\n            <li><a href=\"#3.1\">Defining Model <\/li>\n            <li><a href=\"#3.2\">Model Compilation<\/li>\n            <li><a href=\"#3.3\">Model Fitting<\/li>\n            <li><a href=\"#3.4\">Model Evaluation<\/li>\n            <li><a href=\"#3.5\">Prediction<\/li>\n        <\/ul>\n    <\/li>\n<\/ul>"}}