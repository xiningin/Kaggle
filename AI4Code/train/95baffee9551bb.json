{"cell_type":{"fc5b2f59":"code","13db2a87":"code","eefddbed":"code","52f90b79":"code","9a718f94":"code","180dacd1":"code","01095b2d":"code","879b2504":"code","e5018109":"code","c6c49dad":"code","d13785d3":"code","d382514f":"code","1a1be8b4":"code","7dfc1f2a":"code","c9a99527":"code","e3664f9e":"code","9ecfa970":"code","319f1a5d":"code","3625d523":"code","3650ea41":"code","67bba448":"code","96653c95":"code","d3b007a4":"code","160803c5":"code","c7e4f963":"code","049606f8":"code","3dc886a4":"code","f0e38579":"code","8a909bda":"code","7e67e617":"code","1724e5b3":"code","26b30386":"code","592d6b5e":"code","c4179227":"code","8ababc24":"code","829a4af6":"code","cfc1d095":"code","14494826":"code","c254a4d0":"code","f8aee183":"code","b1c79cc7":"markdown","073df31e":"markdown","d3c80e70":"markdown","4c2f9e5e":"markdown","88da335f":"markdown","c8394529":"markdown","5e853660":"markdown","b28698ee":"markdown","36679727":"markdown","7d6f02f1":"markdown","b52fa057":"markdown","e20d4f86":"markdown","d11e0c93":"markdown","78f66f45":"markdown","1b7be2a2":"markdown","d1390d39":"markdown","a2aa0ad6":"markdown","12050ad0":"markdown","dd631779":"markdown","14636821":"markdown","b15fe377":"markdown","37a6ee72":"markdown","8b1e20f9":"markdown","38db19d4":"markdown","ff2a8218":"markdown","4753c132":"markdown","4b4ab492":"markdown","8d8908e7":"markdown","e7f66f57":"markdown","7e88c2ea":"markdown","cd981401":"markdown","5d36fd0a":"markdown","b6824c66":"markdown","d7f1bb5b":"markdown","1ab06f6a":"markdown","b44ab26c":"markdown","92c769d0":"markdown","35872aed":"markdown","6eef09d9":"markdown","45bb345b":"markdown","046bb2d6":"markdown","9c167fb4":"markdown","7ab854df":"markdown","cc392136":"markdown","ff71a5c2":"markdown"},"source":{"fc5b2f59":"# Data Visualisation and Manipulation\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline \nimport seaborn as sns\n\n#import the necessary modelling algos.\n#regression\nfrom sklearn.ensemble import RandomForestRegressor,AdaBoostRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\n\n#model selection\nfrom sklearn.model_selection import train_test_split,cross_val_score,KFold,GridSearchCV\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler\n\n#evaluation metrics\nfrom sklearn.metrics import mean_squared_log_error\n\n# Ignore  the warnings\nimport warnings\nwarnings.filterwarnings('ignore')","13db2a87":"df_train=pd.read_csv('..\/input\/bike-sharing-demand\/train.csv',parse_dates = ['datetime'])\ndf_test=pd.read_csv('..\/input\/bike-sharing-demand\/test.csv',parse_dates = ['datetime'])","eefddbed":"# checking the first five rows of train data\ndf_train.head()","52f90b79":"# checking Shape of the data\ndf_train.shape","9a718f94":"# Summary Statistics \ndf_train.describe()","180dacd1":"# checking missing values in training dataset\ndf_train.isnull().sum()","01095b2d":"# checking datatypes\ndf_train.dtypes","879b2504":"# Create new columns \"Year\",\"Day,\"hour\",\"weekDay\",\"month\" from \"datetime\" column.\ndf_train['Year'] = df_train.datetime.dt.year\ndf_train['Month'] = df_train.datetime.dt.month\ndf_train['Day'] = df_train.datetime.dt.day\ndf_train['Hour'] = df_train.datetime.dt.hour\ndf_train['Wday'] = df_train.datetime.dt.weekday","e5018109":"# Coerce the datatype of \"season\" and 'weather' to category.\n# 1 = spring, 2 = summer, 3 = fall, 4 = winter \ndf_train[\"season\"] = df_train.season.map({1: \"Spring\", 2 : \"Summer\", 3 : \"Fall\", 4 :\"Winter\" })\n\n# Weather\n# 1: Clear, Few clouds, Partly cloudy, Partly cloudy\n# 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n# 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n# 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog \ndf_train[\"weather\"] = df_train.weather.map({1: \" Clear + Few clouds + Partly cloudy + Partly cloudy\",\\\n                                        2 : \" Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist \", \\\n                                        3 : \" Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\", \\\n                                        4 :\" Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog \" }) \n\ncategoricalVariableList = [\"season\",\"weather\",'Year']\nfor var in categoricalVariableList:\n    df_train[var] = df_train[var].astype(\"category\")","c6c49dad":"plt.figure(figsize=(15,10))\nsns.heatmap(df_train.corr(),annot=True,cmap='plasma')\nplt.show()","d13785d3":"# Lets Explore our target variable\nsns.boxplot(data=df_train,y=\"count\")\nplt.show()","d382514f":"Q1 = df_train['count'].quantile(0.25)\nQ3 = df_train['count'].quantile(0.75)\nIQR = Q3 - Q1\ndf_train = df_train[~((df_train['count'] < (Q1 - 1.5 * IQR)) |(df_train['count'] > (Q3 + 1.5 * IQR)))]\ndf_train.shape","1a1be8b4":"# lets check the distribution of count variable\nplt.figure(figsize=(8,6))\ndf_train['count'].plot(kind='hist',bins=50,ec='black')\nplt.show()","7dfc1f2a":"plt.figure(figsize=(10,6))\nsns.boxplot(data=df_train,y=\"count\",x=\"season\",orient=\"v\")\nplt.show()","c9a99527":"plt.figure(figsize=(12,6))\nsns.boxplot(data=df_train,y=\"count\",x=\"Hour\",orient=\"v\")\nplt.show()","e3664f9e":"sns.boxplot(data=df_train,y=\"count\",x=\"workingday\",orient=\"v\")\nplt.show()","9ecfa970":"plt.figure(figsize=(12,6))\nsortOrder = [\"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\"July\",\"August\",\"September\",\"October\",\"November\",\"December\"]\ng = sns.barplot(data=df_train,x=\"Month\",y=\"count\")\ng.set_xticklabels(sortOrder)\nplt.show()","319f1a5d":"plt.figure(figsize=(15,6))\nl = sns.pointplot(x=df_train[\"Hour\"], y=df_train[\"count\"],hue=df_train[\"season\"], data=df_train, join=True)\nl.set(xlabel='Hour Of The Day', ylabel='Users Count',title=\"Average Users Count By Hour Of The Day Across Season\",label='big')\nplt.show()","3625d523":"plt.figure(figsize=(15,6))\nhueOrder = [\"Sunday\",\"Monday\",\"Tuesday\",\"Wednesday\",\"Thursday\",\"Friday\",\"Saturday\"]\nm = sns.pointplot(x=df_train[\"Hour\"], y=df_train[\"count\"],hue=df_train[\"Wday\"], data=df_train, join=True)\nm.set(xlabel='Hour Of The Day', ylabel='Users Count',title=\"Average Users Count By Hour Of The Day Across Weekdays\",label='big')\nplt.show()","3650ea41":"#holiday\nsns.factorplot(x='holiday',data=df_train,kind='count',size=5,aspect=1)\nplt.show()","67bba448":"plt.figure(figsize=(18,8))\nsns.boxplot(data=df_train[['temp',\n       'atemp', 'humidity', 'windspeed', 'casual', 'registered', 'count']])\nfig=plt.gcf()\nplt.show()","96653c95":"dummy_train = pd.get_dummies(df_train[['season','weather','Year']],drop_first=True)\ndf_train = pd.concat([df_train,dummy_train],axis=1)","d3b007a4":"# now drop unneccesary columns.\ndf_train.drop(['season','weather','datetime','atemp','casual','registered','Year'],inplace=True,axis=1)","160803c5":"df_train.shape","c7e4f963":"# Create new columns \"Year\",\"Day,\"hour\",\"weekDay\",\"month\" from \"datetime\" column.\ndates = df_test['datetime']\ndf_test['Year'] = df_test.datetime.dt.year\ndf_test['Month'] = df_test.datetime.dt.month\ndf_test['Day'] = df_test.datetime.dt.day\ndf_test['Hour'] = df_test.datetime.dt.hour\ndf_test['Wday'] = df_test.datetime.dt.weekday \n\n# Coerce the datatype of \"season\" and 'weather' to category.\ndf_test[\"season\"] = df_test.season.map({1: \"Spring\", 2 : \"Summer\", 3 : \"Fall\", 4 :\"Winter\" })\ndf_test[\"weather\"] = df_test.weather.map({1: \" Clear + Few clouds + Partly cloudy + Partly cloudy\",\\\n                                        2 : \" Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist \", \\\n                                        3 : \" Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\", \\\n                                        4 :\" Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog \" }) \n\ncategoricalVariableListTest = [\"season\",\"weather\",'Year']\nfor var in categoricalVariableListTest:\n    df_test[var] = df_test[var].astype(\"category\")\n\ndummy_test = pd.get_dummies(df_test[['season','weather','Year']],drop_first=True)\ndf_test = pd.concat([df_test,dummy_test],axis=1) \n\n# now drop unneccesary columns.\ndf_test.drop(['season','weather','datetime','atemp','Year'],inplace=True,axis=1)\nprint(df_test.shape)","049606f8":"num_features = ['temp','humidity','windspeed']","3dc886a4":"X = df_train.drop('count',axis=1)\ny = df_train['count']","f0e38579":"sc = MinMaxScaler()\n\nX[num_features] = sc.fit_transform(X[num_features])\ndf_test[num_features] = sc.transform(df_test[num_features])","8a909bda":"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2,random_state = 42)","7e67e617":"# model fitting\nmodel_rf = RandomForestRegressor(random_state=42)\nmodel_rf.fit( X_train , y_train )","1724e5b3":"# Prediction\ny_pred = model_rf.predict(X_test)","26b30386":"# checking the rmsle\nprint(np.sqrt(mean_squared_log_error(y_test,y_pred)))","592d6b5e":"##########################Feature Importance########################\nfor name, importance in zip(X.columns, model_rf.feature_importances_):\n    print(name, \"=\", importance)","c4179227":"model_abr = AdaBoostRegressor(random_state=42)\nmodel_abr.fit( X_train , y_train )\ny_pred_abr = model_abr.predict(X_test)\nprint(np.sqrt(mean_squared_log_error(y_test,y_pred_abr)))","8ababc24":"model_knn = KNeighborsRegressor()\nmodel_knn.fit( X_train , y_train )\ny_pred_knn = model_knn.predict(X_test)\nprint(np.sqrt(mean_squared_log_error(y_test,y_pred_knn)))","829a4af6":"# Hyperparameter tuning using GridSearchCV\nfrom sklearn.model_selection import KFold\nkfold = KFold(n_splits=5, random_state=42,shuffle=True)\n\nparams_dict = {'n_estimators':[200,300,400],'n_jobs':[-1],'max_features':[\"auto\",'sqrt','log2']}\nreg_rf = GridSearchCV(estimator= model_rf,param_grid = params_dict,scoring = 'neg_mean_squared_log_error',cv = kfold)\nreg_rf.fit(X,y)","cfc1d095":"# checking the best parameter\nreg_rf.best_params_","14494826":"# Final Model \nfinal_rf = RandomForestRegressor(max_features = 'auto', n_estimators= 400, n_jobs= -1,random_state=42)\nfinal_rf.fit(X,y)","c254a4d0":"y_pred_final = final_rf.predict(df_test)","f8aee183":"data = {'datetime':dates,'count':y_pred_final}\nfinal_sub = pd.DataFrame(data)\nfinal_sub.to_csv('submission.csv',index=False)","b1c79cc7":"# **Bike Sharing Demand**","073df31e":"## **Feature Engineering**","d3c80e70":"**A SHORT DESCRIPTION OF THE DATA FEATURES-**<br>\n\ndatetime -> hourly date + timestamp\n\nseason -> 1 = spring, 2 = summer, 3 = fall, 4 = winter\n\nholiday -> whether the day is considered a holiday\n\nworkingday -> whether the day is neither a weekend nor holiday\n\nweather ->\n\n1: Clear, Few clouds, Partly cloudy, Partly cloudy\n\n2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n\n3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n\n4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog\n\ntemp -> temperature in Celsius\n\natemp -> \"feels like\" temperature in Celsius\n\nhumidity -> relative humidity\n\nwindspeed -> wind speed\n\ncasual -> number of non-registered user rentals initiated\n\nregistered -> number of registered user rentals initiated\n\ncount -> number of total rentals\n","4c2f9e5e":"### Boxplot of WorkingDay","88da335f":"### **Random Forest**","c8394529":"### Boxplot of Season","5e853660":"**Insights-**\n\n* temp and humidity feature has got positive and negative correlation with count respectively.\n* windspeed is not gonna be really useful numerical feature and it is visible from it correlation value with \"count\".\n* humidity is inversely related to count as expected as the weather is humid, people will not like to travel on a bike.\n* registered\/casual and count are highly correlated which indicates that most of the bikes that are rented are registered.\n* \"atemp\" is variable is not taken into account since \"atemp\" and \"temp\" has got very strong correlation with each other. During model building any one of the variable has to be dropped since they will exhibit multicollinearity in the data.\n* \"Casual\" and \"Registered\" are also not taken into account since they are leakage variables in nature and need to dropped during model building.","b28698ee":"**Note That there are other modeling algos like linear regression ,ridge regression,lasso regression,Xgboost etc but the problem is that most of the models are predicting negative values for the target which is not possible thats why I am just limited to above models and try to tune the best model.**","36679727":"### Distribution of Holiday","7d6f02f1":"## **Feature Engineering and Data Cleaning in Test Data**","b52fa057":"**It is quiet obvious that people tend to rent bike during summer season since it is really conducive to ride bike at that season.Therefore June, July and August has got relatively higher demand for bicycle.**","e20d4f86":"**NOW WE CAN EXPLORE OUR FEATURES. FIRST LETS EXPLORE THE DISTRIBUTION OF VARIOUS DISCRETE FEATURES LIKE weather , season etc...**","d11e0c93":"### **Lets Tune a bit More for Random Forest Regressor**","78f66f45":"### Boxplot of Hour","1b7be2a2":"### Average Count by Month","d1390d39":"**No Missing Value, hence no imputation needed.**","a2aa0ad6":"**Most of the outlier points are mainly contributed from \"Working Day\" than \"Non Working Day\".**","12050ad0":"## **Load the Data**","dd631779":"### Average Users Count By Hour Of The Day Across Weekdays","14636821":"**Majority of data is for non holiday days.**","b15fe377":"### Encoding Categorical features with dummy encoding","37a6ee72":"#### Separate Dependent and Independent variables From Train Data","8b1e20f9":"**Boxplot Shows that \"count\" variable contains lot of outlier data points.**","38db19d4":"**NOW WE CAN DO SOME MORE FEATURE ENGINEERING AND GET SOME NEW FEATURES AND DROP SOME USELESS OR LESS RELEVANT FEATURES.**","ff2a8218":"**Spring season has got relatively lower count.The dip in median value in boxplot gives evidence for it.**","4753c132":"**RANDOM FORETS REGRESSOR GIVES THE LEAST RMSLE. HENCE WE USE IT TO MAKE PREDICTIONS.**<br>","4b4ab492":"### **Submission**","8d8908e7":"### Lets try Different Models","e7f66f57":"**More people tend to rent bicycle around 7AM-8AM and 5PM-6PM.**","7e88c2ea":"As we can see from the above results, the columns \"season\",\"holiday\",\"workingday\" and \"weather\" should be of \"categorical\" data type.But the current data type is \"int\" for those columns. Lets transform the dataset in the following ways so that we can get started up with our EDA.\n\n* **Create new columns \"Year\",\"Day,\"hour\",\"weekDay\",\"month\" from \"datetime\" column.**\n* **Coerce the datatype of \"season\" and 'weather' to category.**\n* **Drop the datetime column as we already extracted useful features from it.**","cd981401":"### Average Users Count By Hour Of The Day Across Season","5d36fd0a":"## **Exploratory Data Analysis(EDA)**","b6824c66":"**The boxplot of \"Hour\" is quiet interesting.The median value are relatively higher at 7AM - 8AM and 5PM - 6PM. It can be attributed to regular school and office users at that time.**","d7f1bb5b":"### Correlation Analysis","1ab06f6a":"## **Model Building**","b44ab26c":"### Distribution of Continuous Variables","92c769d0":"### **Model Prediction**","35872aed":"## **Import Required Libraries**","6eef09d9":"* Lets Remove Outliers In The Count Column","45bb345b":"**HERE ALL THE VARIABLES  ARE NUMERIC AND THE TARGET VARIABLE THAT WE HAVE TO PREDICT IS THE 'count' VARIABLE. HENCE THIS IS A TYPICAL EXAMPLE OF A REGRESSION PROBLEM AS THE 'count' VARIABLE IS CONTINUOUS HERE.**","046bb2d6":"### **AdaBoost**","9c167fb4":"### Data Scaling ","7ab854df":"### **KNN**","cc392136":"**As we mentioned earlier On weekdays more people tend to rent bicycle around 7AM-8AM and 5PM-6PM. this can be attributed to regular school and office users and on weekends \"Saturday(5)\" and \"Sunday(6)\",More people tend to rent bicycle between 10AM and 4PM.**","ff71a5c2":"#### Lets split the data into train and test set "}}