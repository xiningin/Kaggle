{"cell_type":{"752881ce":"code","b589ea75":"code","2e4150e7":"code","3363f718":"code","4b46705f":"code","7a56cd98":"code","c31c214d":"code","30e0d9a8":"code","3650a845":"code","758f170e":"code","d0ade042":"markdown","c5032266":"markdown","d3d27f62":"markdown","0635361f":"markdown","15e06296":"markdown","5ee91718":"markdown","c4478ca2":"markdown","9c528f9f":"markdown","bda8c76d":"markdown","cc11aa31":"markdown"},"source":{"752881ce":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt   #Data visualisation libraries \nimport seaborn as sns\n%matplotlib inline\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","b589ea75":"#Create a dataframe with the entire dataset; we will later split this data later during the regression step\nhouse_data = pd.read_csv(\"..\/input\/train.csv\")\nhouse_data.head()","2e4150e7":"#Create dataframe with numeric features\nhouse_data_numeric_fields = house_data[['LotFrontage','LotArea','OverallQual','OverallCond',\n                            'YearBuilt','YearRemodAdd','MasVnrArea',\n                             'YrSold','MoSold','BsmtFinSF1','BsmtUnfSF','TotalBsmtSF','TotRmsAbvGrd',\n                             'BedroomAbvGr','FullBath','Fireplaces',\n                             'GarageYrBlt','GarageCars','GarageArea','GrLivArea','SalePrice']]\n\n#Display sample rows from above dataframe\nhouse_data_numeric_fields.head()","3363f718":"#Create correlation dataframe\ncorr = house_data_numeric_fields.corr()\n\n#Configure figure size\nfig, ax = plt.subplots(figsize=(10, 10))\n\n#Produce colour map\ncolormap = sns.diverging_palette(220, 10, as_cmap=True)\n\n#Generate Heat Map, allow annotations and place floats in map\nsns.heatmap(corr, cmap=colormap, annot=True, fmt=\".2f\")\n\n#Configure x ticks\nplt.xticks(range(len(corr.columns)), corr.columns);\n\n#Configure y ticks\nplt.yticks(range(len(corr.columns)), corr.columns)\n\n#show plot\nplt.show()","4b46705f":"\n# Create a dataframe with numeric attributes with most correlation with the sale price\nhouse_data_numeric_high_corr = house_data_numeric_fields[['GrLivArea', 'GarageArea','GarageCars','GarageYrBlt','FullBath',\n             'TotRmsAbvGrd', 'TotalBsmtSF','YearRemodAdd','YearBuilt','OverallQual','Fireplaces','SalePrice']]","7a56cd98":"# Plot a scatter matrix\n\npd.plotting.scatter_matrix(house_data_numeric_high_corr,alpha=0.2, figsize=(20, 20))\nplt.show()","c31c214d":"# identify all columsn which  have null values\nhouse_data_numeric_high_corr_nulls = house_data_numeric_high_corr.loc[:, house_data_numeric_high_corr.isna().any()]\n\n# The year when the garage was built had nulls; it was also observed that the mode of the feature was year was 2005, hence the \n# nulls were replaced by the statistical mode, i.e.year 2005\ndf_temp = house_data_numeric_high_corr.fillna({'GarageYrBlt':2005})\n\n# All the other features with nulls, except for the garage year built, which was fixed in the above step, were dropped\nhouse_data_numeric_high_corr_cleaned = df_temp.dropna(axis=0,how='any')\n\n# Display sample rows from the cleaned up dataset with no null values\nhouse_data_numeric_high_corr_cleaned.head()\n","30e0d9a8":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import metrics\n\n# X_true : captures the features in the data set, to which LR will be applied, i.e this the training set\n# y_true : captures the sale price in the data set, to which LR will be applied, i.e this the training set\n\n# X_predict : captures the features in the data set, which can be used to verify the model generated using X_true\n# y_predict : captures the sale price in the data set, which can be used to verify the model using y_true\n\nlinreg = LinearRegression()\n\n# Divide the data into a 30% to 70% split, and use one to train the model, and use the other to predict; hence, training dataset \n# has the sale price; however, sale price has been removed from the dataset to be used for prediction\n\nX_true, X_predict, y_true, y_predict = train_test_split(\n                                house_data_numeric_high_corr_cleaned.loc[:, house_data_numeric_high_corr_cleaned.columns != 'SalePrice'],\n                                house_data_numeric_high_corr_cleaned['SalePrice'],\n                                train_size=.30, test_size=.70)\n\n# Perform curve fitting\n\nlinreg = LinearRegression().fit(X_true, y_true)\nprint('Curve Fitting Complete!')\n\n# Output relevant results from the above step\n\nprint('House Prices Dataset')\nprint('linear model intercept: {}'\n     .format(linreg.intercept_))\nprint('linear model coeff:\\n{}'\n     .format(linreg.coef_))\nprint('R-squared score (training): {:.3f}'\n     .format(linreg.score(X_true, y_true)))\nprint('R-squared score (test): {:.3f}'\n     .format(linreg.score(X_predict, y_predict)))\n","3650a845":"predictions = linreg.predict(X_predict)\nprint('Prediction using the test set based on the curve fitting of the training set is complete!')","758f170e":"plt.scatter(y_predict,predictions)\n","d0ade042":"*Based on the above graph, we are able to deduce that the following attributes contribute the most to the price fluctuations (not in any particular order)*\n\n1. Ground living Area\n2. Garage Area\n3. Number of cars which can be parked in the garage\n4. The year garage was built\n5. Fireplace\n6. Number of full bathrooms\n7. Total number of rooms\n8. Overall quality of the house\n9. The year when the house was built\n10. Year when last remodelling was done\n11. Total rooms above ground\n12. Total basement size in square feet","c5032266":"**Predict the test set, based on the curve fitting performed in the above test**","d3d27f62":"**----Problem Statement:----**\n\nA dataset which contains attributes about homes (including sale price) in Ames, Iowa is provided as source. Create a model which could predict the sale price of homes for a \"fresh\" dataset (with no sale price available).\n\n\n**----Approach:----**\n\nBased on the dataset, it was observed that a multi-variable Linear Regression model would be a good starting point\n\n**----Solution Steps:----**\n\nSTEP-1 **[Feature selection]**: Out of the available list of 80 features, choose the most impactful list of features in terms of sale price of the house; in order to simplify feature engineering, all categorical attributes, such as \"Neighbourhood\", were\nnot included in the final dataset (this is not realistic in a production environment)\n\nSTEP-2 **[Correlation with Sale Price]**: Create a correlogram displaying the relationship among the attributes from the earlier step. The most useful correlation, however, would be the correlation of house attributes with the sale price-any feature with correlation less then 0.50 (with the sale price) will be dropped.\n\nSTEP-3 **[Build Final Feature List]**: Build a dataset with only features which have a high correlation with the sale price\n\nSTEP-4 **[Data Cleanup]**: Drop the rows with null values or replace them with statistical mean, mode, etc. of the data fields (if meaningful)\n\nSTEP-5 **[Apply Linear Regression]**: Divide the data into test and training sets, so that the model effectiveness can be verified independently. Apply Linear Regression to the cleaned up dataset from step-4 and output the model coefficients and R-squared values for both the test and the training sets \n\n*High R-squared values are usually better, since the model would be able to better explain the variances*\n\nSTEP-6 **[Visualize Predictions]**: Create a graph of the predictions \n\nEach of the above step is executed below in sequential order\n\n**----Solution Analysis----**\n\nBy splitting the data into training set for training the model, and a test set for validating the model, the following R-squared values were encountered during **ONE** of the runs.\n\nR-squared score (training): 0.829\n\nR-squared score (test): 0.735\n\nWhile R-squared alone should not be used for model evaluation, it's a reasonable staring point before employing more sophisticated techniques such as confusion matrices; Furthermore, with the inclusion of categorical attributes such as \"Neighborhood\" and \"House Style\", much more accurate model outputs can be achieved\n","0635361f":"**STEP-4: Get rid of the rows with null values, or use statistical mean, mode, etc. from the attributes as fillers**\n","15e06296":"**STEP-5:Divide the data into test and training sets, so that the model effectiveness can be verified independantly. Apply Linear Regression to the cleaned up dataset from step-4 and output the model coefficeints and R-squared values.**","5ee91718":"**STEP-6:Graph the predictions**","c4478ca2":"Visualize the final set of features (from above) to be used to train the LR model, as a scatter matrix. Some interesting observations from the scatter matrix below are as follows:\n\n**Above ground living area and basement square feet has the most linear relationship to price*\n\n** Most homes are less than the half a million mark*\n\n** Most of the cheaper houses are old*\n\n** Most basements are less than 2500 SF*\n\n** Most of the expensive homes were built after 1975*","9c528f9f":"**STEP-1: Choose the features from the available list**\n\n*All though not advisable in production scenarios, categorical data columns, such as \"Neighborhood\" have been removed for ease of computation*","bda8c76d":"**STEP-3: Build a dataset with only highly correlated values identified in step-2**","cc11aa31":"**STEP-2: Create a correlogram of all the attributes in the numeric data fields of the housing data**\n\n*This would help in identifying the features which contribute most to the increase or decrease in sale price.\ni.e any correlation value above .5 is statistically significant*"}}