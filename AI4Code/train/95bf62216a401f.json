{"cell_type":{"a70e8eb0":"code","7e09c9ff":"code","28c510fc":"code","bf8fa675":"code","9e4ac5da":"code","d55115a2":"code","a94421fe":"code","257471b7":"code","b167bfd8":"code","7ad684c3":"code","4189a679":"code","37d34069":"code","09875db5":"code","113660d9":"code","3d37e5e5":"code","64266f2d":"code","3a9e9357":"code","2e9bc472":"code","2d45b42c":"code","b4aa650e":"code","93566644":"code","7441e3f3":"code","b7e2a571":"code","21d5ef28":"code","a824bb3f":"code","a9feb359":"code","0b40fa68":"code","aa628eb7":"code","b80a4739":"code","ac3cd180":"code","2db030e7":"code","6ae4c4db":"code","eba6e0cc":"code","dffeea73":"code","e2385860":"code","7487dba6":"code","4c1b798a":"code","cc0fd4c1":"code","3221482b":"code","fbd5576c":"code","f1fe54db":"code","f7f1c988":"code","5c9e693d":"code","33e03dda":"code","df10f4f5":"code","65d9dd75":"code","a9589736":"code","5194174a":"code","9889bf31":"code","636cc673":"code","35cd238a":"code","1e950fa2":"markdown","49a283ad":"markdown","5a08cdc1":"markdown","4cd29057":"markdown","48eb322a":"markdown","9690beb7":"markdown","000d7d4d":"markdown","7cc73fd2":"markdown","a49103eb":"markdown","76d89fe3":"markdown","d00cbad3":"markdown","fc1e2e74":"markdown","f5c9b80b":"markdown","c01eb7b7":"markdown","ef2e5436":"markdown","fbb35503":"markdown","4910c02b":"markdown","48fe5739":"markdown"},"source":{"a70e8eb0":"## Convinience function for reading a file\ndef read_file(filepath):\n    \n    with open(filepath) as f:\n        str_text = f.read()\n    \n    return str_text","7e09c9ff":"# read_file(\"..\/input\/the-great-gatsby\/The Great Gatsby.txt\")","28c510fc":"len(read_file(\"..\/input\/the-great-gatsby\/The Great Gatsby.txt\"))","bf8fa675":"import spacy\n\nnlp = spacy.load('en', disable=['parser', 'tagger', 'ner']) ## since I only want to use `tokenizer`, I can disable the other ones","9e4ac5da":"nlp.max_length = 1198623 # Increasing SpaCys max-nlp limit to avoid errors like mentioned below","d55115a2":"## Separating the Punctuations from the text since we don't want our NN to train on those informations\ndef separate_punc(doc_text):\n    return [token.text.lower() for token in nlp(doc_text) if token.text not in '\\n\\n \\n\\n\\n!\"-#$%&()--.*+,-\/:;<=>?@[\\\\]^_`{|}~\\t\\n ']","a94421fe":"doc = read_file(\"..\/input\/the-great-gatsby\/The Great Gatsby.txt\")\ntokens = separate_punc(doc)","257471b7":"# tokens","b167bfd8":"len(tokens)","7ad684c3":"# organize into sequences of tokens\ntrain_len = 25+1 # 50 training words , then one target word\n\n# Empty list of sequences\ntext_sequences = []\n\nfor i in range(train_len, len(tokens)):\n    \n    # Grab train_len# amount of characters\n    seq = tokens[i-train_len:i]\n    \n    # Add to list of sequences\n    text_sequences.append(seq)","4189a679":"type(text_sequences)","37d34069":"text_sequences[1]","09875db5":"\" \".join(text_sequences[1])","113660d9":"\" \".join(text_sequences[2])","3d37e5e5":"len(text_sequences)","64266f2d":"from keras.preprocessing.text import Tokenizer","3a9e9357":"# integer encode sequences of words\n## We're replacing the above shown sequence of words into numbers\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(text_sequences)\nsequences = tokenizer.texts_to_sequences(text_sequences)","2e9bc472":"sequences[1] ## Each of this number is an ID for a particular word","2d45b42c":"## To check the relationship mapping between each word\/sequence_number\n# for i in sequences[0]:\n#     print(f'{i} : {tokenizer.index_word[i]}')\n\n# tokenizer.index_word","b4aa650e":"## Let's count how many times a word shows up\n# tokenizer.word_counts","93566644":"vocabulary_size = len(tokenizer.word_counts)","7441e3f3":"vocabulary_size ## size of the vocabulary","b7e2a571":"import numpy as np","21d5ef28":"sequences = np.array(sequences)","a824bb3f":"type(sequences)","a9feb359":"sequences","0b40fa68":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense,LSTM,Embedding","aa628eb7":"def create_model(vocabulary_size, seq_len):\n    model = Sequential()\n    model.add(Embedding(vocabulary_size, 25, input_length=seq_len))\n    model.add(LSTM(150, return_sequences=True))\n    model.add(LSTM(150))\n    model.add(Dense(150, activation='relu'))\n\n    model.add(Dense(vocabulary_size, activation='softmax'))\n    \n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n   \n    model.summary()\n    \n    return model","b80a4739":"from keras.utils import to_categorical","ac3cd180":"sequences","2db030e7":"# First 49 words\nsequences[:,:-1]","6ae4c4db":"X = sequences[:,:-1]","eba6e0cc":"y = sequences[:,-1]","dffeea73":"y = to_categorical(y, num_classes=vocabulary_size+1)","e2385860":"seq_len = X.shape[1]","7487dba6":"seq_len","4c1b798a":"# define model\nmodel = create_model(vocabulary_size+1, seq_len)","cc0fd4c1":"from pickle import dump,load","3221482b":"# fit model\nmodel.fit(X, y, batch_size=128, epochs=400,verbose=1)","fbd5576c":"# save the model to file\nmodel.save('epochBIG.h5')\n# save the tokenizer\ndump(tokenizer, open('epochBIG', 'wb'))","f1fe54db":"from random import randint\nfrom pickle import load\nfrom keras.models import load_model\nfrom keras.preprocessing.sequence import pad_sequences","f7f1c988":"def generate_text(model, tokenizer, seq_len, seed_text, num_gen_words):\n    '''\n    INPUTS:\n    model : model that was trained on text data\n    tokenizer : tokenizer that was fit on text data\n    seq_len : length of training sequence\n    seed_text : raw string text to serve as the seed\n    num_gen_words : number of words to be generated by model\n    '''\n    \n    # Final Output\n    output_text = []\n    \n    # Intial Seed Sequence\n    input_text = seed_text\n    \n    # Create num_gen_words\n    for i in range(num_gen_words):\n        \n        # Take the input text string and encode it to a sequence\n        encoded_text = tokenizer.texts_to_sequences([input_text])[0]\n        \n        # Pad sequences to our trained rate (50 words in the video)\n        pad_encoded = pad_sequences([encoded_text], maxlen=seq_len, truncating='pre')\n        \n        # Predict Class Probabilities for each word\n        pred_word_ind = model.predict_classes(pad_encoded, verbose=0)[0]\n        \n        # Grab word\n        pred_word = tokenizer.index_word[pred_word_ind] \n        \n        # Update the sequence of input text (shifting one over with the new word)\n        input_text += ' ' + pred_word\n        \n        output_text.append(pred_word)\n        \n    # Make it look like a sentence.\n    return ' '.join(output_text)","5c9e693d":"text_sequences[0]","33e03dda":"import random\nrandom.seed(101)\nrandom_pick = random.randint(0,len(text_sequences))","df10f4f5":"random_seed_text = text_sequences[random_pick]","65d9dd75":"random_seed_text","a9589736":"seed_text = ' '.join(random_seed_text)","5194174a":"seed_text","9889bf31":"generate_text(model,tokenizer,seq_len,seed_text=seed_text,num_gen_words=50)","636cc673":"full_text = read_file('..\/input\/the-great-gatsby\/The Great Gatsby.txt')","35cd238a":"for i,word in enumerate(full_text.split()):\n    if word == 'Great':\n        print(' '.join(full_text.split()[i-20:i+20]))\n        print('\\n')","1e950fa2":"`# Format the \"type(sequences) -----> list\" into a Numpy Matrix (ndarray)`","49a283ad":"<a id=\"chap3\"><\/a>\n<h3 style=\"background-color:gold;font-family:newtimeroman;font-size:200%;text-align:center\">LSTM CELL<\/h3>","5a08cdc1":"<a id=\"chap1\"><\/a>\n<h3 style=\"background-color:gold;font-family:newtimeroman;font-size:200%;text-align:center\">Text Processing<\/h3>","4cd29057":"`# Tokenizing and cleaning text`","48eb322a":"<a id=\"chap7\"><\/a>\n<h3 style=\"background-color:gold;font-family:newtimeroman;font-size:200%;text-align:center\">Exploring more!<\/h3>","9690beb7":"<a id=\"1.1\"><\/a>\n<h3 style=\"background-color:gold;font-family:newtimeroman;font-size:200%;text-align:center\">Text generation and Recurrent Neural Networks<\/h3>","000d7d4d":"`## Let's create something so that when we pass in the first #25 words, it should automatically predict the next word in the sequence, i.e. #26`","7cc73fd2":"<a id=\"1.1\"><\/a>\n<h3 style=\"background-color:gold;font-family:newtimeroman;font-size:200%;text-align:center\">What are Recurrent Neural Networks?<\/h3>","a49103eb":"`# Grab a random seed sequence`","76d89fe3":"<p><span style='font-family: \"Trebuchet MS\", Times, serif; font-size: 18px;'>RNN is a type of artificial neural network designed to work with data that has sequences. Let's consider an example:\nAssume we've a sequence of data: [\"1st day sale\", \"2nd day sale\", \"3rd day sale\", \"4th day sale\"].<\/span><\/p>\n\n<p><span style='font-family: \"Trebuchet MS\", Times, serif; font-size: 18px;'>To predict what's the sale price would likely to be on the 5th day, RNN comes into use, i.e; Predicting the shift of data with one day ahead in sequence. Most common uses of RNN are in:<\/span><\/p>\n<p>\n    <span style='font-family: \"Trebuchet MS\", Times, serif; font-size: 18px;'>\n        <li>Time Series<\/li>\n        <li>Automobile Trajectories (left,right,back,forward)<\/li>\n        <li>Sound\/Speech (sequence of sounds)<\/li>\n        <li>Music<\/li>\n    <\/span>\n<\/p>\n\nMore Information here: [Technical definition and knowledge anout RNN](https:\/\/en.wikipedia.org\/wiki\/Recurrent_neural_network)","d00cbad3":"**<font size=\"3\"><a href=\"#chap1\">1. Text Processing<\/a><\/font>**\n**<br><font size=\"3\"><a href=\"#chap2\">2. Keras Tokenization<\/a><\/font>**\n**<br><font size=\"3\"><a href=\"#chap3\">3. LSTM<\/a><\/font>**\n**<br><font size=\"3\"><a href=\"#chap4\">4. Split the Data into Training and Test<\/a><\/font>**\n**<br><font size=\"3\"><a href=\"#chap5\">5. Training the model<\/a><\/font>**\n**<br><font size=\"3\"><a href=\"#chap6\">6. New text generation<\/a><\/font>**\n**<br><font size=\"3\"><a href=\"#chap7\">7. Exploring<\/a><\/font>**","fc1e2e74":"<a id=\"chap5\"><\/a>\n<h3 style=\"background-color:gold;font-family:newtimeroman;font-size:200%;text-align:center\">Model Training<\/h3>","f5c9b80b":"<a id=\"chap4\"><\/a>\n<h3 style=\"background-color:gold;font-family:newtimeroman;font-size:200%;text-align:center\">Splitting the dataset<\/h3>","c01eb7b7":"<a id=\"chap2\"><\/a>\n<h3 style=\"background-color:gold;font-family:newtimeroman;font-size:200%;text-align:center\">Tokenization using Keras<\/h3>","ef2e5436":"> [E088] Text of length 1029371 exceeds maximum of 1000000. The v2.x parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.","fbb35503":"<a id=\"1.2\"><\/a>\n<h3 style=\"background-color:gold;font-family:newtimeroman;font-size:200%;text-align:center\">Working of a neuron in Feed Forward Network<\/h3>","4910c02b":"<a id=\"chap6\"><\/a>\n<h3 style=\"background-color:gold;font-family:newtimeroman;font-size:200%;text-align:center\">Generating new text<\/h3>","48fe5739":"<p><span style='font-family: \"Trebuchet MS\", Times, serif; font-size: 18px;'>To solve the above type of problem using RNN, let's understand how a simple neuron works in a Feed Forward Network!<\/span><\/p>\n\n![Working of a simple neuron](https:\/\/learnopencv.com\/wp-content\/uploads\/2017\/10\/neuron-diagram.jpg)\n\n<p><span style='font-family: \"Trebuchet MS\", Times, serif; font-size: 18px;'>A single neuron takes in some inputs, aggregates them, and passes them through an activation function(like 'relu', 'sigmoid', 'tanh', etc.) and then generates an output.<\/span><\/p>\n\n![RNN](https:\/\/wiki.tum.de\/download\/attachments\/22578349\/RNN0.JPG?version=1&modificationDate=1485263911757&api=v2)\n\n<p><span style='font-family: \"Trebuchet MS\", Times, serif; font-size: 18px;'>In a RNN, the input generated after passing through the Activation function is sent back to itself, into the input of the same neuron!<\/span><\/p>\n\n<p><span style='font-family: \"Trebuchet MS\", Times, serif; font-size: 18px;'>Here a neuron recieves input from a previous timestamp, as well as current time stamp. Hence, they are also known as memory cells<\/span><\/p>\n\n<p><span style='font-family: \"Trebuchet MS\", Times, serif; font-size: 18px;'>RNNs are very comfortable with I\/p and O\/p for both sequences and single vector values. It's very easy to create layer of an RNN<\/span><\/p>\n\nMore Information here: [Technical definition and knowledge anout RNN](https:\/\/en.wikipedia.org\/wiki\/Recurrent_neural_network)"}}