{"cell_type":{"c0cd3530":"code","92dcfe70":"code","3ab70ba5":"code","290d9fcd":"code","54babaaa":"code","d2616758":"code","28e89a9b":"code","ef655d86":"code","9bbabff0":"code","4af54375":"code","3122663a":"code","66a117cf":"code","9f2f6655":"code","c40828f2":"code","86a8d9ee":"code","ebab0c19":"code","6995ec4e":"code","adcacdc5":"markdown","feb916dd":"markdown","81b82eaf":"markdown","1fed3ce8":"markdown","01f465f4":"markdown","c9f1ea12":"markdown","52c81b6a":"markdown","ceb0a526":"markdown","b68f6d03":"markdown","077b1edf":"markdown","c6b8e36f":"markdown","75f092a8":"markdown","2858b919":"markdown","04801e07":"markdown","aaa6d41e":"markdown","4c615c6a":"markdown","cc16ac1e":"markdown","291d7295":"markdown","9840db0e":"markdown","944b778a":"markdown","2e762ea6":"markdown","d02ae0d2":"markdown","ec3e0868":"markdown","c42e7d6e":"markdown","24041d02":"markdown","f2b83f24":"markdown","098ffb09":"markdown","07f640b7":"markdown","c9754bf5":"markdown","6a073a53":"markdown","5f3e5ba6":"markdown","ef2aa1f9":"markdown","1b383e77":"markdown","54b6cf5a":"markdown","31874198":"markdown","a1f8fd0e":"markdown","1540c9fb":"markdown","d55de338":"markdown","be544d76":"markdown","0ee994fa":"markdown","92cee509":"markdown","19050c14":"markdown","0f2e4f06":"markdown"},"source":{"c0cd3530":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt \nimport seaborn as sns \nimport plotly.offline as py\nfrom plotly.offline import init_notebook_mode, iplot\nimport plotly.graph_objs as go\nfrom skimage.transform import resize\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","92dcfe70":"data = pd.read_csv(\"\/kaggle\/input\/heart-disease-uci\/heart.csv\")","3ab70ba5":"data.isnull().sum()","290d9fcd":"data.info()\ndata.describe()","54babaaa":"data.head(5)\n","d2616758":"data.tail(5)","28e89a9b":"y = data.target.values\nx_data = data.drop([\"target\"], axis=1)","ef655d86":"from sklearn import preprocessing\nmin_max_scaler = preprocessing.MinMaxScaler()\nx = min_max_scaler.fit_transform(x_data)\nprint(x)","9bbabff0":"f,ax = plt.subplots(figsize=(12,12))\nsns.heatmap(data.corr(), annot=False, linewidths=.5, fmt =\".1f=\", ax=ax)\nplt.show()","4af54375":"import warnings\nwarnings.filterwarnings('ignore')\n\nplt.style.use('fivethirtyeight')\nplt.rcParams['figure.figsize'] = (10, 3)\nsns.distplot(data['age'], color = 'blue')\nplt.title('Range of Age', fontsize = 17)\nplt.show()","3122663a":"\n\nsize = data['sex'].value_counts()\ncolors = ['blue', 'green']\nlabels = \"Male\", \"Female\"\nexplode = [0, 0]\n\nmy_circle = plt.Circle((0, 0), 0.4, color = 'white')\n\nplt.rcParams['figure.figsize'] = (9, 9)\nplt.pie(size, colors = colors, labels = labels, shadow = True, explode = explode, autopct = '%.2f%%')\nplt.title('Gender', fontsize = 17)\np = plt.gcf()\np.gca().add_artist(my_circle)\nplt.legend()\nplt.show()","66a117cf":"\nx_thalach = data.thalach.values.reshape(-1,1)\ny = y.reshape(-1,1)\n\nfrom sklearn.linear_model import LinearRegression\nlinreg = LinearRegression()\nlinreg.fit(x_thalach,y)\n\ny_headlinreg = linreg.predict(x_thalach)\n\nplt.scatter(x_thalach,y)\nplt.xlabel(\"thalach\")\nplt.ylabel(\"target\")\n\nplt.plot(x_thalach,y_headlinreg, color= \"green\")\nplt.show()\n\nfrom sklearn.metrics import r2_score\n\nprint(\"r_square score: \", r2_score(y,y_headlinreg))\n\n\n\n","9f2f6655":"from sklearn.preprocessing import PolynomialFeatures\npolynomial_regression = PolynomialFeatures(degree = 4)\n\nx_polynomial = polynomial_regression.fit_transform(x)\n\nlinear_regression2 = LinearRegression()\nlinear_regression2.fit(x_polynomial,y)\ny_head2 = linear_regression2.predict(x_polynomial)\n\nprint(\"r_square score: \", r2_score(y,y_head2))\n","c40828f2":"y= y.ravel() #the returned array will have the same type as the input array\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.2, random_state= 42)\nprint(x_train)\nx_train = x_train.T\nx_test = x_test.T\ny_train = y_train.T\ny_test = y_test.T\nprint(x_train)\n\ndef initialize_weights_and_bias (dimension):\n    w= np.full((dimension,1),0.01)\n    b = 0.0 \n    return w,b\n\n\ndef sigmoid(z) :\n    y_head = 1\/(1+np.exp(-z))\n    return y_head\n\ndef forward_backward_propagation (w,b,x_train,y_train):\n    z = np.dot(w.T,x_train)+b\n    y_head = sigmoid(z)\n    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n    cost = (np.sum(loss))\/x_train.shape[1]\n    \n    \n    #backward propogation \n    derivative_weight = np.dot(x_train,((y_head-y_train).T))\/x_train.shape[1]\n    derivative_bias = np.sum(y_head-y_train)\/x_train.shape[1]\n    gradients = {\"derivative_weight\": derivative_weight, \"derivative_bias\": derivative_bias}\n    \n    return cost,gradients\n\n#updating learning parameters\ndef update(w,b,x_train,y_train,learning_rate,number_of_iterations):\n    cost_list=[]\n    cost_list2=[]\n    index=[]\n    \n#updating learning parameters for number of iteration times\n    for i in range (number_of_iterations):\n        cost,gradients = forward_backward_propagation(w,b,x_train,y_train)\n        cost_list.append(cost)\n    \n        w= w-learning_rate*gradients[derivative_weight]\n        b= b-learning_rate*gradients[derivative_bias]\n        if i %10 ==0 :\n            cost_list2.append(cost)\n            index.append(i)\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n        \n    \n        parameters = {\"weights\" : w , \"bias\" : b}\n        plt.plot(index,cost_list2)\n        plt.xticks(index,rotation=\"vertical\")\n        plt.xlabel(\"Number of iteration\")\n        plt.ylabel(\"cost\")\n        plt.show()\n        return parameters, gradients, cost_list\n    \n#predict\ndef predict(w,b,x_test):\n    z = sigmoid(np.dot(w.T,x_test)+b)\n    Y_prediction = np.zeros((1,x_test.shape[1]))\n    \n    for i in range(z.shape[1]):\n        if z[0,i]<= 0.5:\n            Y_prediction[0,i] = 0\n        else:\n            Y_prediction[0,i] = 1\n                \n            return Y_prediction\n    \ndef logistic_regression(x_train, y_train, x_test, y_test, learning_rate ,  number_of_iterations):\n    # initialize\n    dimension =  x_train.shape[0]  # that is 30\n    w,b = initialize_weights_and_bias(dimension)\n    # do not change learning rate\n    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate,number_of_iterations)\n    \n    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n    y_prediction_train = predict(parameters[\"weight\"],parameters[\"bias\"],x_train)\n\n    # Print test Errors\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n    \n    logistic_regression(x_train, y_train, x_test, y_test,learning_rate = 0.01, num_iterations = 150)","86a8d9ee":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(x_train.T,y_train.T)\nprint(\"test accuracy {}\".format(lr.score(x_test.T,y_test.T)))","ebab0c19":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.3,random_state=1)\n\n# knn model\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 3) # n_neighbors = k\nknn.fit(x_train,y_train)\nprediction = knn.predict(x_test)\nprint(\" {} nn score: {} \".format(3,knn.score(x_test,y_test)))","6995ec4e":"from sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(x_train,y_train)\n \n# %% test\nprint(\"print accuracy of naive bayes algo: \",nb.score(x_test,y_test))","adcacdc5":"***THE CODE***\n* thalach = maximum heart rate achieved | We look for the thalach relation with the target.\n* Since our data.shape is (303,) we can not use sklearn method. Therefore, we transform the data shape to (303,1) by using reshape.(-1,1).","feb916dd":"<a id=\"3\"><\/a> <br>\n### 2.2 MULTIPLE LINEAR REGRESSION","81b82eaf":"<a id=\"3\"><\/a> <br>\n### 2.1 LINEAR REGRESSION\n\n* Linear Regression =  The simple approach to model which has linear relationships with data values.","1fed3ce8":"> ***THE CODE***","01f465f4":"<a id=\"4\"><\/a> <br>\n### 2.4 LOGISTIC REGRESSION\n\n* In statistics, the logistic model  is used to model the probability of a certain class or event existing such as pass\/fail, win\/lose, alive\/dead or healthy\/sick. This can be extended to model several classes of events such as determining whether an image contains a cat, dog, lion, etc. Each object being detected in the image would be assigned a probability between 0 and 1 and the sum adding to one. \n* It is the best model for binary classification.\n* In addition, Logistic Regression is very basic form of deep learning.\n\n![](https:\/\/miro.medium.com\/max\/4000\/0*qDhdxS4TjN_TUJHV.jpg)","c9f1ea12":"> ","52c81b6a":"***THE CODE***\n* (degree = 4) means polynomical degree, it can be changed according to evaluation.","ceb0a526":"> ***Logistic Regression with sklearn***","b68f6d03":"> **y = b0+b1*x**\n\n* This is the mathematical background of linear regression \n\n![](http:\/\/www.sthda.com\/english\/sthda-upload\/images\/machine-learning-essentials\/linear-regression.png)\n\n> *In this graph*; \n\n1.   b0 = bias (constant) \/ the intercept of the regression line (that is the predicted value when x = 0)\n1.   b1 = coefficient \/ the slope of the regression line\n         *  slope = \u0394\ud835\udc66\/\u0394\ud835\udc65 | sometimes slope is signed by m = (y2-y1)\/(x2-x1)\n         *  when y = mx+b m ---> = slope\n   *  *Example* --->  y = (9x+6)\/2 --> y = 9\/2(x)+3 so m = 9\/2\n   \n3.    Error term (e) =  Since the data points do not fit exactly on the regression line, some of the points   are above the curve and some are below. So error occurs. When we subtract the original value from the predicted one and square the result value (to not encounter the negative value), we find Mean Squared Error(MSE). \n  ","077b1edf":"> ![](http:\/\/www.datascribble.com\/wp-content\/uploads\/2017\/09\/euclidean1.png) \n\n*  The euclidean distance is the math behind KNN algorithm.\n*  More clearly euclidean distance;\n\n![](https:\/\/hlab.stanford.edu\/brian\/making7.gif)\n\n* More clearly KNN;\n\n![](https:\/\/blogs.sas.com\/content\/iml\/files\/2016\/09\/nearestnbr1.png)\n","c6b8e36f":"* *One should check before using data whether there is nan-values or not in the data*\n* So, isnull().sum shows us how many nan-values in the columns of the data in total. ","75f092a8":" **INFORMATION ABOUT MACHINE LEARNING**\n* ***1)SUPERVISED LEARNING***\n\"Supervised learning is the machine learning task of learning a function that maps an input to an output based on example input-output pairs. It infers a function from labeled training data consisting of a set of training examples.\"\n* ***2)UNSUPERVISED LEARNING***\n\"Unsupervised learning is a machine learning technique, where you do not need to supervise the model. Unsupervised machine learning helps you to finds all kind of unknown patterns in data. Clustering and Association are two types of Unsupervised learning.\"\n* ***3)R^2 SCORE EVALUATION***\n\"After you have fit a linear model using regression analysis, you need to determine how well the model fits the data. To help you out, there are evaluations models more. However; in this post, we\u2019ll explore the R-squared (R2 ) statistic, some of its limitations, and uncover some surprises along the way. For instance, low R-squared values are not always bad and high R-squared values are not always good!\"\n","2858b919":"> ***Forward Propagation***","04801e07":"> ***THE CODE***\n\n  from sklearn.linear_model import LinearRegression\n  \n  data = pd.read_csv(\"mpg_dataset.csv\")\n\n  hp = data.horsepower.values.reshape(-1,1)\n  \n  weight = data.weight.values.reshape(-1,1)\n\n  x = pd.concat([hp,weight], keys = [\"horsepower\",\"weight\"]) #data compounding\n  \n  y = data.mpg.values.reshape(-1,1)\n\n  multiple_linear_regression = LinearRegression()\n  \n  multiple_linear_regression.fit(x,y)\n  \n  multiple_linear_regression.predict(np.array([[2200,310]))\n  \n  predict1 = weight = 2750, hp = 310\n","aaa6d41e":"> ***CONTENT***\n\n\n1. [Information about the data](#1)\n    1. Inform the data and get knowledge\n    1. Scaling data\n    1. Explatory data analysis\n    1. Transforming data frames   \n1. [Machine Learning and Evaluation](#2)\n          1. Information about Machine Learning\n          1. R^2 Score Evaluation \n   \n      2.1.   [Linear Regression(Simple)](#3)\n               1. Information about Linear Regression\n                * Mathematical Explanation\n                * Graphic\n                * The Code\n                * Last Info\n      2.2.   [Multiple Linear Regression](#3)\n                * Mathematical Explanation\n                * Graphic\n                * The usage with an example\n      2.3.   [Polynomial Regression](#3)\n               1. Information about Polynomial Regression\n                * Mathematical Explanation\n                * Graphic\n                * The Code\n                * Last Info\n\n      2.4.   [Logistic Regression](#4)\n               1. Information about Logistic Regression\n      2.5.   [K-Nearest Neighbor](#6)\n               1. Information about K-Nearest Neighbor\n      2.6.   [Naive Bayes ](#7)\n               1. Information about Naive Bayes\n\n1. [Conclusion](#8)\n   \n    ","4c615c6a":"> LAST INFO","cc16ac1e":"> In this kernel, I tried to explain basic machine learning algorithms with maths and to evaluate those algorithms by using R^2 Score technique.\n\n> Some algorithms gave us bad scores while others gave avarage or good.\n\n> \"Linear Regression, Polynomial Regression, Logistic Regression,K-Nearest Neighbors,Naive Bayes\" These algorithms are treated. But there are other Machine Learning algorithms too.\n\n> See you in the next issue!\n\n> This is written by Hasan Do\u011fukan \u0130nce and I'm open to criticise \"hdogukanince@gmail.com\".","291d7295":"3. Then we put z into sigmoid function that returns y_head(probability).\n4. After that , we calculate loss function with Forward Propagation. If loss function values is big, we should rearrange weight and bias by using derivative.\n  \n   Forward propagation steps:\n   \n   find z = w*x+b\n   \n   y_head = sigmoid(z)\n   \n   loss(error) = loss(y,y_head)\n   \n   cost = sum(loss)\n   ","9840db0e":"**SCALING**\n* *The reason for scaling our data to prevent slowing down the calculations and to prevent confusion during the calculation and ftiing process *","944b778a":"<a id=\"7\"><\/a> <br>\n### 2.7 NAIVE BAYES CLASSIFIER\n\n*  Naive Bayes classifier is a probabilistic machine learning model that\u2019s used for classification task. The crux of the classifier is based on the Bayes theorem.\n\n","2e762ea6":"* We have learnt Naive Bayes and the score is not bad.","d02ae0d2":"> LAST INFO\n\n* We have knowlegde about KNN algorithm and the score is avarage.","ec3e0868":"![](https:\/\/helloacm.com\/wp-content\/uploads\/2016\/03\/logistic-regression-example.jpg)\n\n* Before I get started, I want you to know those mathematical terms given below.\n> ***z = wx + b***\n> *** z = b + px1w1 + px2w2 + ... + pxn*wn***\n1. z = result\n1. w = weight\n1. b = bias\n\n> ***Sigmoid Function = 1\/(1+e^-z)***\n\n> ***y_predicted = sigmoid(z)***\n\n> ***Log Loss(error) Fuction =*** ![](https:\/\/image.ibb.co\/eC0JCK\/duzeltme.jpg)\n\n> ***Cost = Summation of errors (Log Loss)***\n\n\n","c42e7d6e":"> y = b0+b1*x1+b2*x^2+...+bn*x^n\n\n> b2+x^2 is a prabolic value | Now, we have a curve.\n\n* This is the  mathematical formula behind polynomial regression.\n\n![](https:\/\/static.javatpoint.com\/tutorial\/machine-learning\/images\/machine-learning-polynomial-regression.png)\n\n* Given graph tells us ; In non-linear data types, we should use polynomial regression models.\n\n\n\n","24041d02":"> LAST INFO\n\n* We have learnt something about Logistic Regression. \n\n  The score is not bad.","f2b83f24":"5. Now, we Know our cost(summation of errors). So, as I said , we should optimize our weights and bias with derrivation. The given graph below, explains better;\n* ![](https:\/\/cdn-images-1.medium.com\/max\/1000\/1*P8WFY4D7Mm1FwzA2I85yBw.png) \n\n*  With the help of derivation, we try to get nearest value to zero. In other saying , we try to decrease the slope.\n![](http:\/\/image.ibb.co\/hYTTJH\/8.jpg)\n\n* There is a cost function(takes weight and bias). Take derivative of cost function according to weight and bias. Then multiply it with \u03b1 learning rate. Then update weight. ( These all steps will be applied for bias)\n*  Learning Rate = The learning rate is a hyperparameter that controls how much to change the model in response to the estimated error each time the model weights are updated.\n\n  * 1\/m*x(y_head-y)^T\n  \n  \n     \n     ","098ffb09":"* So the question is \"What the fuck those math terms serve for us ?\"\n* The answer is \"It serves a lot!\"\n\n  Let's go step by step:\n 1. Firstly, we should divide our models for training and testing by usin train_test_split.\n 2. Secondly, we should initialize the parameters in \"z = wx+b\". I mean weight and bias. We choose weight values as 0.01 and initial bias as 0. Then, we had better define the sigmoid function.\n\n","07f640b7":"> ***THE CODE***","c9754bf5":"<a id=\"8\"><\/a> <br>\n### CONCLUSION","6a073a53":"<a id=\"4\"><\/a> <br>\n### 2.3 POLYNOMIAL  REGRESSION\n\n* Polynomial regression = The simple approach to model non-linear relationships. It adds polynomial terms to  regression model.","5f3e5ba6":"![](https:\/\/miro.medium.com\/max\/510\/1*tjcmj9cDQ-rHXAtxCu5bRQ.png)\n\n* Using Bayes theorem, we can find the probability of A happening, given that B has occurred.","ef2aa1f9":"* KNN algorithms works like;\n\n1. Initialize K to your chosen number of neighbors\n2. For each example in the data\n   2.1 Calculate the distance between the query example and the current example from the data.\n   2.2 Add the distance and the index of the example to an ordered collection\n3. Sort the ordered collection of distances and indices from smallest to largest (in ascending order) by the distances\n4. Pick the first K entries from the sorted collection\n5. Get the labels of the selected K entries\n6. If regression, return the mean of the K labels\n7. If classification, return the mode of the K labels\n\n* To select the K that\u2019s right for your data, we run the KNN algorithm several times with different values of K and choose the K that reduces the number of errors.","1b383e77":"> ***THE CODE***","54b6cf5a":"10. After we update our parameters, we are ready for prediction.\n11. Finally we combine what we wrote as codes step by step and make prediction; with the accuracy value.","31874198":"<a id=\"2\"><\/a> <br>\n# 2. MACHINE LEARNING AND EVALUATION (ML&R^2)","a1f8fd0e":"** **THE WHOLE REDE OF ML- HEART DISEASE**\n\nThis karnel aims to compose an estimative algorithm for hearth disease by using Heart Disease UCI database with detailed explanation of those algoritms which are Linear Regression,Polynomial Regression,Logistic Regression, K-Nearest Neighbor and Naive Bayes.\n","1540c9fb":"> LAST INFO\n\n* As you see, our model fit very well. \n* ATTENTION!  ---> DO NOT TOTALLY TRUST \"R SQUARE SCORE\"","d55de338":"> LAST INFO\n\n* Linear Regression is not appropriate technique for this database, as you see. On the other hand; Sigmoid Function will be the best for this data.(We will examine in Logistic Regression part)\n* Linear Regression is generally used for the data types linearly increasing or decreasing. As an example, Imagine you are working in a department of HR in Dogo Company and you are in charge for determining the salaries. The rule for determining salary is basic that the more experienced, get higher salary. So, there is a linear increasing. You can use linear regression to determine new arrivals' salary.","be544d76":"> y = b0+b1*x1+b2*x2+...+bn*xn\n\n* This is formula is the mathematic behind multiple linear regression.\n* Given graphs below, shows us our data does not depend on one column. So we had better to use Multiple Linear Regression. \n\n> y = b0 + b1*weight + b2*horsepower\n\n![](https:\/\/miro.medium.com\/max\/1120\/0*AqzOn7p--nveVULA.png)\n\n* MPG = Miles Per Gallon ---> A measure of how far a car can travel if you put just one gallon of petrol or diesel in its tank.\n* As an example, You are responsible for fuel consumption of the cars with in Dogo Company. You are asked to use two data values(\"horsepower and weight\") or more. In addition, Both of the data values has a linear impact on Fuel Consumption(MPG). So, you can use multiple linear regression.\n\n\n\n","0ee994fa":"**COMPOSING DATA**\n* *Since we want to compose our model to estimate two probability(binary), we should divide the data. Target is what the machine guess, on the other hand the other features are essential to train our model. That's the reason ahy we have divided our data to two. *","92cee509":"![](https:\/\/nicolovaligi.com\/tf_iris.png)\n\n* Naive Bayes works the same logic as probability. \n* The high school probability.\n\n* As an example, you work for Dogo Company. There are hardworking and lazy employees. You are in charge finding who is hardworking.  \n\n  Hardworking person: 11\n  \n  Lazy:9\n  \n  Total: 20\n  \n* (11\/20)*(4\/11) \/ (4\/20)\n\n>  Determined data (4), so 4\/20\n\n\n  ","19050c14":"> ***Optimization***","0f2e4f06":"<a id=\"4\"><\/a> <br>\n### 2.6 K - NEAREST NEIGHBOR CLASSIFIER\n\n* The k-nearest neighbors (KNN) algorithm is a simple, supervised machine learning algorithm that can be used to solve both classification and regression problems."}}