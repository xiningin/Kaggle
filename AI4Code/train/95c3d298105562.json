{"cell_type":{"fcb90ec6":"code","22e98c91":"code","7d7c1f0f":"code","ceea503e":"code","2acdbb04":"code","427d2c62":"code","227a39a0":"code","b3fc4de8":"code","b5bd2f11":"code","52139c05":"code","8f46959d":"code","c72f5d51":"code","a1de0957":"code","0ba8c5be":"code","dca138f2":"code","1b98cd30":"code","fadacffd":"code","9dbe37e5":"code","b9f0860d":"code","f01a2f1f":"code","1edfb657":"code","60930199":"code","9236cf27":"code","ae29b1d8":"code","a80f63d6":"code","ae17e884":"code","9285782a":"code","c6f3b17c":"code","b4467d45":"code","7b6638b8":"code","0771f9df":"code","e212e229":"code","5155ba88":"code","4b6eaf2d":"code","4cf71b41":"code","fbe4e7d8":"code","aaa670f9":"code","1de536e0":"code","93e35b63":"code","6f2ee64b":"code","20fad566":"code","fa83366e":"code","bb9a0e92":"code","2c933587":"code","20e8fb18":"code","c039504c":"code","32e9fdba":"code","d5e7aac7":"code","901aa6cf":"code","b598673e":"code","c01c0c87":"code","4be2b753":"code","3d19cccc":"code","1ca7afa3":"code","987c5428":"code","bfb4dfe6":"code","0b2c676d":"markdown","6f867ab8":"markdown","4fc45044":"markdown","610a931e":"markdown","5f638fd6":"markdown","dc0b884a":"markdown","f7f7fdd9":"markdown","9e88ba77":"markdown","5ef46b01":"markdown","80a53e97":"markdown","5c058787":"markdown","3110ac08":"markdown","f856a583":"markdown","73d829b3":"markdown","52cc9aa5":"markdown","08da5c78":"markdown"},"source":{"fcb90ec6":"# Importing libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn import metrics","22e98c91":"# Importing the dataset\ndata = pd.read_csv(\"..\/input\/WA_Fn-UseC_-HR-Employee-Attrition.csv\")","7d7c1f0f":"# Printing the 1st 5 columns\ndata.head()","ceea503e":"# Printing the dimenions of data\ndata.shape","2acdbb04":"# Viewing the column heading\ndata.columns","427d2c62":"# Inspecting the target variable\ndata.Attrition.value_counts()","227a39a0":"data.dtypes","b3fc4de8":"# Identifying the unique number of values in the dataset\ndata.nunique()","b5bd2f11":"# Checking if any NULL values are present in the dataset\ndata.isnull().sum()","52139c05":"# See rows with missing values\ndata[data.isnull().any(axis=1)]","8f46959d":"# Viewing the data statistics\ndata.describe()","c72f5d51":"# Here the value for columns, Over18, StandardHours and EmployeeCount are same for all rows, we can eliminate these columns\ndata.drop(['EmployeeCount','StandardHours','Over18','EmployeeNumber'],axis=1, inplace=True)","a1de0957":"# Plotting a boxplot to study the distribution of features\nfig,ax = plt.subplots(1,3, figsize=(20,5))               \nplt.suptitle(\"Distribution of various factors\", fontsize=20)\nsns.boxplot(data['DailyRate'], ax = ax[0]) \nsns.boxplot(data['MonthlyIncome'], ax = ax[1]) \nsns.boxplot(data['DistanceFromHome'], ax = ax[2])  \nplt.show()","0ba8c5be":"# Finding out the correlation between the features\ncorr = data.corr()\ncorr.shape","dca138f2":"# Plotting the heatmap of correlation between features\nplt.figure(figsize=(20,20))\nsns.heatmap(corr, cbar=True, square= True, fmt='.1f', annot=True, annot_kws={'size':15}, cmap='Greens')","1b98cd30":"# Check for multicollinearity using correlation plot\nf,ax = plt.subplots(figsize=(10,10))\nsns.heatmap(data[['DailyRate','HourlyRate','MonthlyIncome','MonthlyRate']].corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)\nplt.show()","fadacffd":"# Plotting countplots for the categorical variables\nfig,ax = plt.subplots(2,3, figsize=(20,20))            \nplt.suptitle(\"Distribution of various factors\", fontsize=20)\nsns.countplot(data['Attrition'], ax = ax[0,0]) \nsns.countplot(data['BusinessTravel'], ax = ax[0,1]) \nsns.countplot(data['Department'], ax = ax[0,2]) \nsns.countplot(data['EducationField'], ax = ax[1,0])\nsns.countplot(data['Gender'], ax = ax[1,1])  \nsns.countplot(data['OverTime'], ax = ax[1,2]) \nplt.xticks(rotation=20)\nplt.subplots_adjust(bottom=0.4)\nplt.show()","9dbe37e5":"# Combine levels in a categorical variable by seeing their distribution\nJobRoleCrossTab = pd.crosstab(data['JobRole'], data['Attrition'], margins=True)\nJobRoleCrossTab","b9f0860d":"JobRoleCrossTab.div(JobRoleCrossTab[\"All\"], axis=0)","f01a2f1f":"# Combining job roles with high similarities together\ndata['JobRole'].replace(['Human Resources','Laboratory Technician'],value= 'HR-LT',inplace = True)\ndata['JobRole'].replace(['Research Scientist','Sales Executive'],value= 'RS-SE',inplace = True)\ndata['JobRole'].replace(['Healthcare Representative','Manufacturing Director'],value= 'HE-MD',inplace = True)","1edfb657":"# Encoding Yes \/ No values in Attrition column to 1 \/ 0\ndata.Attrition.replace([\"Yes\",\"No\"],[1,0],inplace=True)\ndata.head()","60930199":"# One hot encoding for categorical variables\nfinal_data = pd.get_dummies(data)\nfinal_data.head().T","9236cf27":"final_data.shape","ae29b1d8":"# Spliting target variable and independent variables\nX = final_data.drop(['Attrition'], axis = 1)\ny = final_data['Attrition']","a80f63d6":"# Splitting the data into training set and testset\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.25, random_state = 0, stratify=y)","ae17e884":"y_train.value_counts()","9285782a":"# Checking distribtution of Target varaible in training set\ny_train.value_counts()[1]\/(y_train.value_counts()[0]+y_train.value_counts()[1])*100","c6f3b17c":"y_test.value_counts()","b4467d45":"# Checking distribtution of Target varaible in test set\ny_test.value_counts()[1]\/(y_test.value_counts()[0]+y_test.value_counts()[1])*100","7b6638b8":"# Logistic Regression\n\n# Import library for LogisticRegression\nfrom sklearn.linear_model import LogisticRegression\n\n# Create a Logistic regression classifier\nlogreg = LogisticRegression()\n\n# Train the model using the training sets \nlogreg.fit(X_train, y_train)","0771f9df":"# Prediction on test data\ny_pred = logreg.predict(X_test)","e212e229":"# Calculating the accuracy, precision and the recall\nacc_logreg = round( metrics.accuracy_score(y_test, y_pred) * 100, 2 )\nprint( 'Total Accuracy : ', acc_logreg )\nprint( 'Precision : ', round( metrics.precision_score(y_test, y_pred) * 100, 2 ) )\nprint( 'Recall : ', round( metrics.recall_score(y_test, y_pred) * 100, 2 ) )","5155ba88":"# Create confusion matrix function to find out sensitivity and specificity\nfrom sklearn.metrics import auc,confusion_matrix\ndef draw_cm(actual, predicted):\n    cm = confusion_matrix( actual, predicted, [1,0]).T\n    sns.heatmap(cm, annot=True,  fmt='.2f', xticklabels = [\"Yes\",\"No\"] , yticklabels = [\"Yes\",\"No\"] )\n    plt.ylabel('Predicted')\n    plt.xlabel('Actual')\n    plt.show()","4b6eaf2d":"# Confusion matrix \ndraw_cm(y_test, y_pred)","4cf71b41":"# Gaussian Naive Bayes\n\n# Import library of Gaussian Naive Bayes model\nfrom sklearn.naive_bayes import GaussianNB\n\n# Create a Gaussian Classifier\nmodel = GaussianNB()\n\n# Train the model using the training sets \nmodel.fit(X_train,y_train)","fbe4e7d8":"# Prediction on test set\ny_pred = model.predict(X_test)","aaa670f9":"# Calculating the accuracy, precision and the recall\nacc_nb = round( metrics.accuracy_score(y_test, y_pred) * 100, 2 )\nprint( 'Total Accuracy : ', acc_nb )\nprint( 'Precision : ', round( metrics.precision_score(y_test, y_pred) * 100, 2 ) )\nprint( 'Recall : ', round( metrics.recall_score(y_test, y_pred) * 100, 2 ) )","1de536e0":"# Confusion matrix \ndraw_cm(y_test, y_pred)","93e35b63":"# Decision Tree Classifier\n\n# Import Decision tree classifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Create a Decision tree classifier model\nclf = DecisionTreeClassifier(criterion = \"gini\" , min_samples_split = 100, min_samples_leaf = 10, max_depth = 50)\n\n# Train the model using the training sets \nclf.fit(X_train, y_train)","6f2ee64b":"# Model prediction on train data\ny_pred = clf.predict(X_train)","20fad566":"# Finding the variable with more importance\nfeature_importance = pd.DataFrame([X_train.columns, clf.tree_.compute_feature_importances()])\nfeature_importance = feature_importance.T.sort_values(by = 1, ascending=False)[1:10]","fa83366e":"sns.barplot(x=feature_importance[1], y=feature_importance[0])\n# Add labels to the graph\nplt.xlabel('Feature Importance Score')\nplt.ylabel('Features')\nplt.title(\"Visualizing Important Features\")\nplt.legend()\nplt.show()","bb9a0e92":"# Prediction on test set\ny_pred = clf.predict(X_test)","2c933587":"# Confusion matrix\ndraw_cm(y_test, y_pred)","20e8fb18":"# Calculating the accuracy, precision and the recall\nacc_dt = round( metrics.accuracy_score(y_test, y_pred) * 100, 2 )\nprint( 'Total Accuracy : ', acc_dt )\nprint( 'Precision : ', round( metrics.precision_score(y_test, y_pred) * 100, 2 ) )\nprint( 'Recall : ', round( metrics.recall_score(y_test, y_pred) * 100, 2 ) )","c039504c":"# Random Forest Classifier\n\n# Import library of RandomForestClassifier model\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Create a Random Forest Classifier\nrf = RandomForestClassifier()\n\n# Train the model using the training sets \nrf.fit(X_train,y_train)","32e9fdba":"# Finding the variable with more importance\nfeature_imp = pd.Series(rf.feature_importances_,index= X_train.columns).sort_values(ascending=False)\n# Creating a bar plot\nfeature_imp=feature_imp[0:10,]\nsns.barplot(x=feature_imp, y=feature_imp.index)\n# Add labels to the graph\nplt.xlabel('Feature Importance Score')\nplt.ylabel('Features')\nplt.title(\"Visualizing Important Features\")\nplt.legend()\nplt.show()","d5e7aac7":"# Prediction on test data\ny_pred = rf.predict(X_test)","901aa6cf":"# Confusion metrix\ndraw_cm(y_test, y_pred)","b598673e":"# Calculating the accuracy, precision and the recall\nacc_rf = round( metrics.accuracy_score(y_test, y_pred) * 100 , 2 )\nprint( 'Total Accuracy : ', acc_rf )\nprint( 'Precision : ', round( metrics.precision_score(y_test, y_pred) * 100 , 2 ) )\nprint( 'Recall : ', round( metrics.recall_score(y_test, y_pred) * 100, 2 ) )","c01c0c87":"# SVM Classifier\n\n# Creating scaled set to be used in model to improve the results\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","4be2b753":"# Import Library of Support Vector Machine model\nfrom sklearn import svm\n\n# Create a Support Vector Classifier\nsvc = svm.SVC()\n\n# Train the model using the training sets \nsvc.fit(X_train,y_train)","3d19cccc":"# Prediction on test data\ny_pred = svc.predict(X_test)","1ca7afa3":"# Confusion Matrix\ndraw_cm(y_test, y_pred)","987c5428":"# Calculating the accuracy, precision and the recall\nacc_svm = round( metrics.accuracy_score(y_test, y_pred) * 100, 2 )\nprint( 'Total Accuracy : ', acc_svm )\nprint( 'Precision : ', round( metrics.precision_score(y_test, y_pred) * 100, 2 ) )\nprint( 'Recall : ', round( metrics.recall_score(y_test, y_pred) * 100, 2 ) )","bfb4dfe6":"models = pd.DataFrame({\n    'Model': ['Logistic Regression', 'Naive Bayes', 'Decision Tree', 'Random Forest', 'Support Vector Machines'],\n    'Score': [acc_logreg, acc_nb, acc_dt, acc_rf, acc_svm]})\nmodels.sort_values(by='Score', ascending=False)","0b2c676d":"## Evaluation and comparision of all the models","6f867ab8":"### Random Forest Classifier","4fc45044":"### Support Vector Machine Classifier","610a931e":"#### Once the data is cleaned, we split the data into training set and test set to prepare it for our machine learning model in a suitable proportion.","5f638fd6":"## Dataset: [IBM HR Analytics Attrition Dataset](https:\/\/www.kaggle.com\/pavansubhasht\/ibm-hr-analytics-attrition-dataset)","dc0b884a":"# IBM HR Analytics","f7f7fdd9":"## Hence we can see that the Logistic Regression works the best for this dataset. ","9e88ba77":"### Read the data from the dataset using the read_csv() function from the pandas library.","5ef46b01":"**pandas** : An open source library used for data manipulation, cleaning, analysis and visualization. <br>\n**numpy** : A library used to manipulate multi-dimensional data in the form of numpy arrays with useful in-built functions. <br>\n**matplotlib** : A library used for plotting and visualization of data. <br>\n**seaborn** : A library based on matplotlib which is used for plotting of data. <br>\n**sklearn.metrics** : A library used to calculate the accuracy, precision and recall. <br>\n**sklearn.preprocessing** : A library used to encode and onehotencode categorical variables. <br>","80a53e97":"### Inspecting and cleaning the data","5c058787":"### Import all the necessary header files as follows:","3110ac08":"### Please upvote if you found this kernel useful! :) <br>\n### Any sort of feedback is appreciated!","f856a583":"### Logistic Regression","73d829b3":"### Data Visualization","52cc9aa5":"### Decision Tree Classifier","08da5c78":"### Gaussian Naive Bayes"}}