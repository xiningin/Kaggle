{"cell_type":{"12dd2953":"code","3efbfaf1":"code","82419810":"code","de460164":"code","bcfb862a":"code","526e9c37":"code","e1f58348":"code","7ab58a02":"code","d2cc4d4c":"code","2b32338a":"code","623f1f81":"code","7231dd68":"code","d456fd7f":"code","59367602":"code","f19b04f4":"code","a40e4e9b":"code","0680741b":"code","0f1af212":"code","3f48afe7":"code","1b691651":"markdown","f2604cec":"markdown","0b00b7a8":"markdown","3f5a91be":"markdown","2aaf7c3b":"markdown","93e72eed":"markdown","f6892bd9":"markdown","4a5934d5":"markdown","880c95f3":"markdown","a93cb3fa":"markdown","cf2d79f8":"markdown"},"source":{"12dd2953":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3efbfaf1":"# Directive pour afficher les graphiques dans Jupyter\n%matplotlib inline\n\n# Pandas : librairie de manipulation de donn\u00e9es\n# NumPy : librairie de calcul scientifique\n# MatPlotLib : librairie de visualisation et graphiques\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nfrom sklearn import model_selection\n\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score,auc, accuracy_score\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn import datasets\n\nfrom keras.datasets import mnist\n\nfrom keras import losses, regularizers\n\nfrom keras.models import Sequential, load_model, Model\n\nfrom keras.layers import Activation,Dense, Dropout, Flatten, BatchNormalization\n\nfrom keras.layers.convolutional import Conv2D, MaxPooling2D\n\nfrom keras.constraints import maxnorm\n\nfrom keras.utils.np_utils import to_categorical\nimport cv2\nimport os\nimport glob\nimport sys","82419810":"classes=[]\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/stanford-car-dataset-by-classes-folder\/car_data\/car_data\/train\/'):\n    folderName = os.path.basename(dirname)\n    if folderName!=\"\" :\n        classes.append(folderName)\nclasses=sorted(classes)","de460164":"X_train=[]\ny_train=[]\nX_test=[]\ny_test=[]\nfor i in range(len(classes)):\n    train_dir='\/kaggle\/input\/stanford-car-dataset-by-classes-folder\/car_data\/car_data\/train\/'+classes[i]\n    data_train_path=os.path.join(train_dir,'*g')\n    train_files=glob.glob(data_train_path)\n    for f1 in train_files:\n        X_train.append(np.array(cv2.resize(cv2.imread(f1),(80,80))))\n        y_train.append(i)\n    test_dir='\/kaggle\/input\/stanford-car-dataset-by-classes-folder\/car_data\/car_data\/test\/'+classes[i]\n    data_test_path=os.path.join(test_dir,'*g')\n    test_files=glob.glob(data_test_path)\n    for f1 in test_files:\n        X_test.append(np.array(cv2.resize(cv2.imread(f1),(80,80))))\n        y_test.append(i)\nprint(len(X_train))\nprint(len(X_test))","bcfb862a":"plt.imshow(X_train[0])\nplt.title(classes[y_train[0]])","526e9c37":"np.array(X_train).shape","e1f58348":"X_train=np.array(X_train)\nX_test=np.array(X_test)","7ab58a02":"X_train=X_train\/255\nX_test=X_test\/255","d2cc4d4c":"model = Sequential([\n    Flatten(input_shape=(80, 80,3)),\n    Dense(1024, activation='relu'),\n    Dense(256, activation='relu'),\n    Dense(len(classes))\n])\n\nmodel.compile(optimizer='adam',\n              loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])","2b32338a":"#train = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=200, verbose=1)","623f1f81":"model = Sequential()\nmodel.add(Conv2D(32, kernel_size = (3, 3), activation='relu', input_shape=(80, 80, 3)))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(64, kernel_size=(3,3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(64, kernel_size=(3,3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(96, kernel_size=(3,3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(32, kernel_size=(3,3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(BatchNormalization())\nmodel.add(Flatten())\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(len(classes), activation = 'softmax'))\nmodel.compile(optimizer='adam',\n              loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n","7231dd68":"#train = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=40, batch_size=200, verbose=1)","d456fd7f":"def plot_scores(train) :\n    accuracy = train.history['accuracy']\n    val_accuracy = train.history['val_accuracy']\n    epochs = range(len(accuracy))\n    plt.plot(epochs, accuracy, 'b', label='Score apprentissage')\n    plt.plot(epochs, val_accuracy, 'r', label='Score validation')\n    plt.title('Scores')\n    plt.legend()\n    plt.show()\n#plot_scores(train)","59367602":"#train2 = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=40, batch_size=200, verbose=1)","f19b04f4":"#plot_scores(train2)","a40e4e9b":"y_train1 = to_categorical(y_train)\ny_test1 = to_categorical(y_test)","0680741b":"model = Sequential()\nmodel.add(Conv2D(32, (3, 3), input_shape=(80, 80, 3), activation='relu'))\nmodel.add(Conv2D(32, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.2))\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.2))\nmodel.add(Conv2D(20, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.2))\nmodel.add(Flatten())\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dense(len(classes), activation='softmax'))\n\n# Compilation du mod\u00e8le\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])","0f1af212":"train = model.fit(X_train, y_train1, validation_data=(X_test, y_test1), epochs=30, batch_size=200, verbose=1)","3f48afe7":"plot_scores(train)","1b691651":"Ensuite, on va pouvoir r\u00e9cup\u00e9rer les images dans un X et y associer une classe en y","f2604cec":"200 x 200 pixels, x3 pour le RGB.","0b00b7a8":"Notre score d'apprentissage augmente de mani\u00e8re plut\u00f4t lin\u00e9aire, mais le score de validation reste m\u00e9diocre. On a un gros probl\u00e8me d'overfitting.","3f5a91be":"On a une pr\u00e9cision qui commence \u00e0 augmenter, et qui continuerait s\u00fbrement de s'am\u00e9liorer en continuant l'entra\u00eenement. C'est ce qu'on va faire.","2aaf7c3b":"On transforme nos X en array","93e72eed":"On va maintenant pouvoir attaquer le probl\u00e8me","f6892bd9":"D'abord, on r\u00e9cup\u00e8re tous les noms de folder. Ca sera nos noms de classe.","4a5934d5":"On regarde la dimension des images","880c95f3":"Ce r\u00e9seau est rapidement tr\u00e8s bon pour les cas d'entra\u00eenement, mais m\u00eame probl\u00e8me d'overfitting.","a93cb3fa":"Puis on normalise entre 0 et 1","cf2d79f8":"Ce n'est pas bon du tout, on est \u00e0 peine au dessus de la pr\u00e9cision que l'on aurait en r\u00e9pondant totalement au hasard (on aurait environ 0.5%)."}}