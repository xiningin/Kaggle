{"cell_type":{"623eb1ba":"code","6a8f2280":"code","dd7f60e9":"code","0a2677bb":"code","34e353f7":"code","eea92822":"code","b99ec4c9":"code","314dd752":"code","8ac44a65":"code","485320a0":"code","352a747e":"code","71e6b5a6":"code","7c06d791":"code","6d302e77":"code","d7a3ffb6":"code","8119bd0e":"code","53612b6a":"code","64a4c890":"code","04f399ce":"code","9136348f":"code","59b116b8":"code","5f88f9ae":"code","707db671":"code","aebcd515":"code","7bd15058":"code","ef7f2fa5":"code","664d4ca4":"code","f18569a6":"code","7eec24fa":"code","b32ad4aa":"code","fffc6c90":"code","6c6b58e3":"code","eacd5df5":"code","6bd0b7f8":"code","dd64afa9":"code","6b7589b0":"code","6049d310":"code","75acb762":"markdown","a24d6db7":"markdown","0d99b091":"markdown","824a555a":"markdown","5a886140":"markdown","866dd05d":"markdown","07c1ccb7":"markdown","14afe6dd":"markdown","9538c5f5":"markdown","723b1558":"markdown","a19a7352":"markdown","c3aef016":"markdown","cd5f3625":"markdown","afeb538c":"markdown","da20a19a":"markdown","ffa7e81f":"markdown","89f461f9":"markdown","dfd8155f":"markdown","5d5faf31":"markdown","820a246d":"markdown","9eb74ec2":"markdown","f06fe31f":"markdown","94913226":"markdown","80bff478":"markdown","dba1f87b":"markdown","6917c895":"markdown","55a4db37":"markdown","29ec2371":"markdown","afd2d1b9":"markdown","0edafe4a":"markdown","d25aa84c":"markdown","61080b91":"markdown","98d111cb":"markdown","c79817b6":"markdown","4502ffc7":"markdown","1daa95da":"markdown","57852b01":"markdown","305e4bb8":"markdown","53c5e84c":"markdown","d1592451":"markdown","01fa5e9f":"markdown","eea64b2a":"markdown","11719d40":"markdown","02ac265f":"markdown","649964cd":"markdown","76b82973":"markdown","d4094f0a":"markdown","1917c2a4":"markdown","63ba1586":"markdown","d16fb69b":"markdown","ff3fe66f":"markdown","9c28e19c":"markdown","65f3aff2":"markdown"},"source":{"623eb1ba":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport math\nimport csv\nimport sys\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n","6a8f2280":"df = pd.read_csv(\"..\/input\/raw-benford-numbers-edited\/Raw Benford Numbers.csv\", index_col = 'Unnamed: 0')","dd7f60e9":"#an example of some of our data\ndf.tail()","0a2677bb":"## Remove all \"nan\" values for blank cells\nall = []\nfor x in df:\n    all.append(df[x].values)\n\ncData = []\nfor i in range(0,3):\n    cleanedList = [x for x in all[i] if str(x) != 'nan']\n    cData.append(cleanedList)\nfinalData = []\nfor z in range(len(cData)):\n    for y in cData[z]:\n        finalData.append(y)\n#finalData is the cleaned data without nan values, we still have to clear the trailing decimal points and zeroes\n","34e353f7":"#this for and if loop evaluates if the value ends with '.0' and if it does, the last two digits are removed\nfor val in range(len(finalData)):\n    if str(finalData[val])[-2:] == '.0':\n        finalData[val] = str(finalData[val])[:-2]","eea92822":"print(finalData)","b99ec4c9":"def firstDigit(input_list):\n    myList = [0,0,0,0,0,0,0,0,0]\n    for num in input_list:\n        output = str(num)[:1]\n        myList[int(output)-1] += 1\n    return myList\n\ndef secondDigit(input_list):\n    myList = [0,0,0,0,0,0,0,0,0,0]\n    for num in input_list:\n        output = str(num)[1:2]\n        myList[int(output)] += 1\n    return myList\n\ndef thirdDigit(input_list):\n    myList = [0,0,0,0,0,0,0,0,0,0]\n    for num in input_list:\n        output = str(num)[2:3]\n        myList[int(output)] += 1\n    return myList\n","314dd752":"second_Digit = secondDigit(finalData)\nthird_Digit = thirdDigit(finalData)\nfirst_Digit = firstDigit(finalData)","8ac44a65":"Benford_percentiles = pd.DataFrame({\n    'First Digit Expected': [0, .301, .176, .125, .097, .079, .067, .058, .051, .046],\n    'Second Digit Expected': [.12, .114, .109, .104, .100, .097, .093, .090, .088, .085],\n    'Third Digit Expected': [.102, .101, .101, .101, .100, .100, .099, .099, .099, .098]\n                                    })","485320a0":"Benford_percentiles","352a747e":"#This is just a quick script to seperate out the expected first digit from the rest of the Data Frame as these are on a\n#scale of 1-9 instead of 0-9 like the second and third digits\nFirst_digit_benfords = []\nfor x in Benford_percentiles['First Digit Expected']:\n    if x > 0:\n        First_digit_benfords.append(x)","71e6b5a6":"first_digit_percentile = [(x \/ sum(first_Digit)) for x in first_Digit]\nfig = plt.figure()\nax = fig.add_axes([0,0,1,1])\nindex = ['1', '2', '3', '4', '5', '6', '7', '8', '9']\nx = np.arange(len(index))\nplt.xticks(x, index)\nax.plot(x, First_digit_benfords, label= 'Actual')\nax.plot(x, first_digit_percentile, label= 'Expected')\nplt.title('First Digit; Expected values are in blue, actual are in orange')\nplt.show","7c06d791":"second_Digit_percentile = [(x \/ sum(second_Digit)) for x in second_Digit]\nfig = plt.figure()\nax = fig.add_axes([0,0,1,1])\nindex = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\nx = np.arange(len(index))\nplt.xticks(x, index)\nax.plot(x, Benford_percentiles['Second Digit Expected'])\nax.plot(x, second_Digit_percentile)\nplt.title('Second Digit; Expected values are in blue, actual are in orange')\nplt.show()","6d302e77":"third_Digit_percentile = [(x \/ sum(third_Digit)) for x in third_Digit]\nfig = plt.figure()\nax = fig.add_axes([0,0,1,1])\nindex = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\nx = np.arange(len(index))\nplt.xticks(x, index)\nax.plot(x, Benford_percentiles['Third Digit Expected'])\nax.plot(x, third_Digit_percentile)\nplt.title('Third Digit; Expected values are in blue, actual are in orange')\nplt.show()","d7a3ffb6":"#reinitialize these variables before I alter them just to keep the code clean\nfirst_digit_percentile = [(x \/ sum(first_Digit)) for x in first_Digit]\nfirst_digit_percentile.insert(0, 0)\nindex = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n\nfinal_series = {'First Digit Test': first_digit_percentile,\n                'Second Digit Test': second_Digit_percentile,\n                'Third Digit Test': third_Digit_percentile}\ndigit_df = pd.DataFrame(data=final_series, index=index)\nfinal_df = pd.merge(digit_df, Benford_percentiles, on=digit_df.index, how='outer')","8119bd0e":"final_df","53612b6a":"final_df.plot(kind='box', rot=-30)","64a4c890":"print('Skew\\n', final_df.skew(), '\\nKurtosis:\\n', final_df.kurt())","04f399ce":"final_df.std()","9136348f":"def myDataFrame(sample, digit_test, expected):\n    difference = []\n    for x in range(len(digit_test)):\n        difference.append(digit_test[x]-expected[x])\n    if len(sample) < 10:\n        sample.insert(0, 0)\n    output = pd.DataFrame({\n        'Sample Count': sample, \n        'Digit Test (%)': digit_test, \n        'Expected Values (%)': expected, \n        'Difference (%)': difference\n                        })\n    return output","59b116b8":"#This runs the function we created before on each of my different tests\nfirst_digit_df = myDataFrame(first_Digit, final_df['First Digit Test'], final_df['First Digit Expected'])\nsecond_digit_df = myDataFrame(second_Digit, final_df['Second Digit Test'], final_df['Second Digit Expected'])\nthird_digit_df = myDataFrame(third_Digit, final_df['Third Digit Test'], final_df['Third Digit Expected'])\n\n#now I convert the numbers into percentage values to make my data tables more readable\nfor x in [first_digit_df, second_digit_df, third_digit_df]:\n    for y in ['Digit Test (%)', 'Expected Values (%)', 'Difference (%)']:\n        x[y] = round(x[y].apply(lambda i: i*100), 2)\n\n#Below is an example of the completed data table\nfirst_digit_df","5f88f9ae":"def last_two_digit_test(input_list):\n    '''accepts a dataframe as an input and returns a list of the count of each digit out of 100 digits'''\n    #create an list of 100 integers, each with a zero value\n    myList = []\n    for _ in range(100):\n        myList.append(0)\n    for num in input_list:\n        num = str(num)\n        output = str(num)[-2:]\n        myList[(int(output))-1] += 1\n    return myList","707db671":"last_two = last_two_digit_test(finalData)","aebcd515":"index = []\nfor x in range(100):\n    index.append(x)\n\n#Plotting\nfig = plt.figure()\nax = fig.add_axes([0,0,1,1])\nwidth = .35\nx = np.arange(len(index))\nplt.xticks(x, index)\nax.bar(x - width\/2, last_two, width= width)\nplt.title('Last Two Digit Count')\nplt.show()","7bd15058":"last_two_df = pd.DataFrame(last_two, index=index)\nprint([ j for (i,j) in zip(last_two, index) if i >= 4 ])","ef7f2fa5":"#We create new variables called 'significant numbers' which we will alter in order to deliver us just the significant digits\n\nfirst_digit_significant_numbers = first_digit_df.copy()\nsecond_digit_significant_numbers = second_digit_df.copy()\nthird_digit_significant_numbers = third_digit_df.copy()\n\n#This for loop adds the significant digit to a list called 'temp' when the difference of the number is greater than 5%\ntemp = []\nfor x in [first_digit_significant_numbers, second_digit_significant_numbers, third_digit_significant_numbers]:\n    temp.append(x.index.where(x['Difference (%)'] > 5).dropna())\n\n#Now we convert the list into a cleaner version which will be easier to work with and put it in a dictionary\nsignificant_numbers = {\n    'First Digit Significant Values' : temp[0].astype(int).values,\n    'Second Digit Significant Values' : temp[1].astype(int).values,\n    'Third Digit Significant Values' : temp[2].astype(int).values\n}\n#here is an example of what those numbers are\nsignificant_numbers","664d4ca4":"#This cell changes all of my integers into strings which are easier to slice in order to locate the sections of the 990 that\n#might contain fraudulent data\nfinalData = [str(x) for x in finalData]","f18569a6":"#these lines below actually are finding the numbers in our entire dataset where the digit at some given position\n#matches the number and the position that we are looking for\nfirst_numbers = [x for x in finalData if x[:1] in significant_numbers['First Digit Significant Values'].astype(str)]\nsecond_numbers = [x for x in finalData if x[1:2] in significant_numbers['Second Digit Significant Values'].astype(str)]\nthird_numbers = [x for x in finalData if x[2:3] in significant_numbers['Third Digit Significant Values'].astype(str)]","7eec24fa":"print(first_numbers, second_numbers, third_numbers)","b32ad4aa":"def intersection(lst1, lst2): \n    lst3 = [value for value in lst1 if value in lst2] \n    return lst3 ","fffc6c90":"inter_one_two = intersection(first_numbers, second_numbers)\ninter_one_three = intersection(first_numbers, third_numbers)\ninter_two_three = intersection(second_numbers, third_numbers)\ninterfinal = intersection(inter_one_two, third_numbers)\nprint(inter_one_two)\nprint(inter_one_three)\nprint(inter_two_three)\nprint(interfinal)","6c6b58e3":"in_first = set(inter_one_two)\nin_second = set(inter_two_three)\n\nin_second_but_not_in_first = in_second - in_first\n\nresult = inter_one_two + list(in_second_but_not_in_first)","eacd5df5":"print(result)","6bd0b7f8":"#delete all values that arent on our list 'result'\nall_to_investigate = df[df.isin(result)]\n\n#remove every column that is entirely 'NaN' values\nall_to_investigate = all_to_investigate.dropna(axis='columns', how='all')\n\n#remove every row that is entirely 'NaN' values\nall_to_investigate = all_to_investigate.dropna(axis='index', how='all')","dd64afa9":"all_to_investigate","6b7589b0":"to_drop = ['revenue less expenses', 'Total expenses']\nfinal_list = all_to_investigate.drop(to_drop)\nfinal_list","6049d310":"final_list","75acb762":"**Especially insteresting is the cell below which shows the actual standard deviations and the expected standard deviations which differ vastly on the second and third digit test**","a24d6db7":"## Benford Analysis\n   Benford's Law is based around the idea that numbers aren't evenly distributed in nature. They are distributed abnormally. When examining the first digit of every number from a set, a distinct distribution emerges that appears in all sorts of data sets (see link below). A Benford Analysis leverages this information by comparing numbers on SEC filings, population sizes, street names to the expected value those numbers should be based on Benford's Law and looks for descrepencies between the two.\n    \n   Often Descrepencies could mean fraud is involved. For example, Enron's financial statements do not follow Benford's Law while the line count of the *Rails* 3.0.9 source code does (https:\/\/testingbenfordslaw.com\/rails-core-line-count). \n    \n   On this project we will use the American University of Paris's 2017 990 tax returns as our reference material. Our goal is to use python to examine these tax returns and to determine if they follow Benford's law, and if they do not, what numbers could possibly be fraudulent because of this. At the end we will examine what accounts contain numbers in positions that are not distributed as they should be, then make a recommendation based off of this information as to what accounts should be audited in search of fraud.","0d99b091":"## The Data is now cleaned and ready for analysis. \n\n#### Below is how the data looks now.\n\nYou may have noticed some of the data has quotation marks around them. That is because they are strings. In other words your computer views the string as a word, instead of a number that can be used in math functions. This wont matter in the future as we are going to convert everything to type string later.","824a555a":"## Conclusion:\n","5a886140":"## First Conclusions:\n* **First Digit Test**\n    * The first digit test didnt tell us a lot of information. The data seems to conform to benford's law with some deviation possible with the digit 3. We will examine this data again later to see if three is significantly different than its expected value.\n  \n* **Second Digit Test**\n    * The Second digit test was more telling with nearly *no* zeroes, fours and nines. At least according to the graph. Three, five and eight also are suspiciously distributed.\n    \n* **Third Digit Test**\n    * The third digit test seems to reveal numbers close to their expected values, with most digits staying with 2% except for four and five. Since we arent testing directly for deficiencies of numbers, we will probably examine just number 5.\n","866dd05d":"# Introduction and Glossary","07c1ccb7":"### Now we will clean the data","14afe6dd":"### Second Digit Test","9538c5f5":"# Last-Two Test","723b1558":"## Graphing\n\nWe graph the functions here to visually analyze the data first. This is useful for two reasons.\n\n - First it is good for error catching, in case we made a mistake with our earlier code\n - Second, we can see if there are any numbers that clearly stick our more than others. This will be useful to compare to our final answer in case the numbers differ","a19a7352":"***Analysis Section***\n\nThis is a long one so bear with me. In this section we define functions to parse through our data list and break out the first, second and third digit so we may examine and count them. Then we call the functions and graph the results. \n\nExcuse the prehistoric looking grpahs at this point... I am working on creating them using a different engine, but you get what you get.\n\nWhen we have our core data, we do some data manipulation to examine the standard deviations and skew of our results against their expected values, which yields some quite interesting results. \n\nLastly we create data structures which contain our list of results along with their corresponding expxected values, and the difference between them. We do this in case we later want to add more statistics to each section, it will also make the final data analysis easier.","c3aef016":"**Imports**","cd5f3625":"# Analysis","afeb538c":"...and we also create a data frame of each of the percentiles expected according to Benford's law.\nAs you can see, the dataframe makes the values quite easy to read.","da20a19a":"**The two numbers that 'interfinal' produces are the two numbers that fit in our suspicious lists for all of the first, second and third digits. These are prime suspects.**\n\nIt turns out in our case that there is only one digit, '355273'\n\n\n**Below we find our 'result' which is a combination of every number which appeared in multiple lists**\n\nWhat this means is that every value in the 'result' list is a suspicious value selected for its lack of correlation with their expected result according to Benford's law.","ffa7e81f":"# Final Analysis:","89f461f9":"# Conclusion","dfd8155f":"**Now we are creating a DataFrame which just makes our data easier to manipulate**","5d5faf31":"### Define Functions\n\nFirst we will define a set of functions that are all the different tests for the first second and third digit, as shown on the next page.","820a246d":"We need to read and clean our data to put it in a state that is easy for our computer to handle. \n\n* First we are going to go to each column and add each value to a list, even blank values, which show up as 'NaN'\n* Then we create a list called cleanedList which removes all values names 'nan'\n* Assign the values to a list finalData\n\nThe Next slide is how the data currently looks.\n\nWe need to convert that into a list of just numbers, which is easy to evaluate","9eb74ec2":"**For clarity lets print our list at this point to make sure the numbers make sense**","f06fe31f":"#### Results\nAs this graph shows, there are a few values with more than two occurences.\n\nThere might be something interesting with whatever value shows up four times, but *due to the small sample size, it is difficult finding strong evidence for rounding in the last two digits* \n\nThat being said, lets figure out which number appears four times just to see if there is significance to it.","94913226":"**Next we find which values reappear in multiple tests**\n\nMeaning... which values have *multiple* suspicious digits in them.\n\nFirst we are going to define a function called 'intersection' which will take two arguments, two lists, and then return another list which contains all the values that are found in both lists","80bff478":"### Now lets define a function to count the iterations of the last two digits of every number","dba1f87b":"**We need to clean the trailing 0 and decimal place.**","6917c895":"### First Digit Test","55a4db37":"**Now we have our answer.**\n\nThe digit *3* in the first test\n\nThe digits *5* and *8* in our second test\n\nAnd the digits *4* and *5* in our third digit test.\n\n***Next Steps***\n\nNow we will see where these numbers appear in our original CSV.","29ec2371":"### Third Digit Test","afd2d1b9":"Now we have a list of every suspicious account. The next course of action is to recommend an audit for all of these accounts. They are all likely targets of fraud due to the fact that they all likely represent a large number of transactions. \n\nHowever, it is likely that no fraud exists as our sample size is so low. With only 85 samples we cannot be too sure, but it is still very interesting that things like the first digit test worked so well but the second and third digit tests saw massive deviations from their expected values.","0edafe4a":"This gives us three lists of numbers which contain one or more of the suspicious digits as shown below","d25aa84c":"### Plot the Data\n\n* The function is now defined and we have run our dataset through the function\n\n* 'last_two' contains the frequency of every number between 01-100. \n\nNext lets plot the data in order to visualize if there is any oddly repetitive values","61080b91":"**Now you can Navigate to the right to begin**","98d111cb":"## Before Moving On\nBefore doing our final analysis, locating of the accounts and our conclusion, we will explore the last-two test. This test checks for rounding of numbers by examining the last two digits in our entire datasets, and looking for highly used numbers and abnormally groupped numbers.","c79817b6":"The first thing we will do is import our CSV. It is formatted with account names on the Y axis and headers on the X.\n\nThe goal of this program is to be useable for a large number of different CSV's so we can replace this file with another and it will produce meaningful results.","4502ffc7":"Next, we run the functions on our cleaned Data","1daa95da":"***Number Selection***\n\nWe have all of our Data in one long list with every number. Now is a good time to discuss what numbers were chosen from the 990 and what numbers were ignored...\n\nI intentionally only chose significant, unique numbers that were not entirely dependent on other numbers already included in the data set. I tried to use as few 'totals' and 'subtotals' that were calculated off the sheet as I could.\n\nWhat this means is that almost every number included here was directly input into the 990 with little supporting documentation available to me. For an example: The account titled, \"Contributions, gifts and grants\" is a 'totaled' account... but we cannot view the documents and receipts that comprise this total. Therefore there is an opportunity for mistakes or fraud in the number reported.","57852b01":"## Data Cleaning","305e4bb8":"**Lets we combine the data to view it all together and manipulate it**","53c5e84c":"# Benford Analysis\n\n### Charles Beach\n\n### IBA 4020 - Computational Finance","d1592451":"**Let us view the box plot for our data**","01fa5e9f":"So the two number that appears four times is 80.\n\nDue to our small sample size, we should disregard the significance of this number at this point, but if it reappears in our final analysis we should re-examine.\n\nNow back to the heart of our analysis.","eea64b2a":"**Now we need to find out where we are finding these abnormal digit counts.**\n\n-In order to do that we will seperate each digit test into its own data sheet and then find their differences\n\n-Once we have found the differences, we will establish a threshold for error. In our case I chose 5%. \n\n-So if a number is more than 5% away from the expected value based on Benford's law, then this program should select it.","11719d40":"**Before we finish analyzing our earlier data, lets examine the last two digits to determine whether or not rounding was used**\n\nFirst lets define our expectations for the last two digits. \n\n* Not including items in the cents place we should expecct to see an even distribution across all numbers between 00-99\n* Something that may corrupt our dataset is that we dont even have 100 different numbers (around 95 unique numbers)\n* However, if we see a significant repitition or grouping of some numbers over others then there may be a case of rounding to be made","02ac265f":"# Locating the Suspicious Data","649964cd":"### Finally\n\nThe last thing we should do is drop every account that is dependent on other values to bring us to our final list of the auditing targets.","76b82973":"Normally one would expect strong similarities between a digit test and its expected result in terms of its box plot statistics... although our small sample size may have hindered our results","d4094f0a":"# Sources\n* Association of Certified Fraud Examiners; \"Using Benford's Law to Detect Fraud.\" https:\/\/www.acfe.com\/uploadedFiles\/Shared_Content\/Products\/Self-Study_CPE\/UsingBenfordsLaw_2018_final_extract.pdf\n\n* Theodore P. Hill, \"The Significant-Digit Phenomenon\", The American Mathematical Monthly, Vol. 102, No. 4, (Apr., 1995), pp. 322\u2013327. https:\/\/digitalcommons.calpoly.edu\/cgi\/viewcontent.cgi?referer=&httpsredir=1&article=1041&context=rgp_rsr\n\n* Formann, A. K. (2010). Morris, Richard James (ed.). \"The Newcomb\u2013Benford Law in Its Relation to Some Common Distributions\". PLoS ONE. 5 (5): e10541. https:\/\/journals.plos.org\/plosone\/article?id=10.1371\/journal.pone.0010541\n\n* Nigrini, M. J. (2012) Benford's Law. Hoboken, NJ: John Wiley & Sons.\n\n* Goodman, William (2016) \"The promises and pitfalls of Benford's law\" The Royal Statistical Society. https:\/\/rss.onlinelibrary.wiley.com\/doi\/full\/10.1111\/j.1740-9713.2016.00919.x\n\n* Ward's Wonderful Computational Finance Slides","1917c2a4":"There are quite a few suspicious numers in our list. 8 out of a sample size of 85. \n\nOn to the last step: Our final account location analysis","63ba1586":"### Now we have a data frame (shown below) of every suspicious account name and its associated value\n\n-Next we should go through this list and rule out values that are dependent on other values\n\n-For example, 'Total expenses' is unlikely to be a suspicious account by itself, as it is merely a sum of other values. However, accounts like Contributions, gifts and grants or other fees could entirely contain fraudulent numbers.","d16fb69b":"### Glossary\n* Importing and cleaning the Data\n* Basic Data Analysis\n* Last-Two Digit Test\n* Final Data Analysis\n* Locating the Suspicious Data\n* Conclusion","ff3fe66f":"### Analyze Suspicious Account Names\nWe are on the last steps...\n\nWe just need to find the suspicious values and there corresponding account names, from that point we can come to our final conclusion.\n\nRead the comments in the code below to see how we filter the list down to just our targeted values.\n","9c28e19c":"# Import and Data Cleaning","65f3aff2":"In this section we go through the original csv file and examine which numbers follow suspicious patterns.\n\nOur next section of code finds statistically significant differences between our tested count values and expected values. We them store those in a dictionary called 'significant_numbers.'\n\nThe output of our function is below the code."}}