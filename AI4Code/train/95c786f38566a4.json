{"cell_type":{"107c6fef":"code","fddace5e":"code","12084b4c":"code","065e3913":"code","fffb724f":"code","f19f4b3d":"code","df6ef114":"code","077d1324":"code","8a242fea":"code","52d80e74":"code","a6d77991":"code","5bc3fcd6":"code","06d2e786":"code","bd466aa3":"code","3f582cbe":"code","208d7443":"code","7ffbfb05":"code","2833e9c4":"code","b2d9199f":"code","838141b7":"code","50828359":"code","0c1ee221":"code","b6bc2c95":"code","df649a2e":"markdown","176705ea":"markdown","7bae366e":"markdown","2377ac84":"markdown","f3e50166":"markdown","389393e5":"markdown","0b31f11a":"markdown","fbcb98e8":"markdown","e4561e3c":"markdown","4e55abb7":"markdown","592d8640":"markdown","78fb6780":"markdown"},"source":{"107c6fef":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","fddace5e":"# Numpy and Pandas for statistics and DataSet\nimport numpy as np\nimport pandas as pd\n\n# Seaborn and matplotlib for Graphs and Plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport plotly.express as px\nimport matplotlib\n\n# To divide data into train and test set\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\n\n# Libraries for Machine Learning Algorithms\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import cross_val_score\n\n# Warning Library to avoid warnings\nimport warnings\nwarnings.filterwarnings('ignore')","12084b4c":"train = pd.read_csv(\"\/kaggle\/input\/mobile-price-classification\/train.csv\")\ntrain.head()","065e3913":"test = pd.read_csv(\"\/kaggle\/input\/mobile-price-classification\/test.csv\")\ntest.head()","fffb724f":"train.info()","f19f4b3d":"train.shape, test.shape","df6ef114":"train.describe()","077d1324":"# Columns unique values\nfor col in train.columns:\n    print(\"{} have {} unique values: \".format(col, train[col].nunique()))\nprint(\"*\" * 35)\nfor col in train.columns:\n    if train[col].nunique() <= 16:\n        print(\"{}: {}\".format(col, train[col].unique()))","8a242fea":"sns.set_theme(style=\"darkgrid\")\ncolors = sns.color_palette('pastel')[0:5]\nplt.figure(figsize=(6,4))\nsns.countplot(data=train, x='wifi')\nplt.title('Mobile Price Range', fontsize=15, fontweight='bold')\nplt.show()","52d80e74":"plt.figure(figsize=[16, 6])\nsns.lineplot(x=\"price_range\", y=\"ram\", data=train, hue='battery_power', marker = '*', ms=13, ci=None)\nplt.show()","a6d77991":"plt.figure(figsize=[16, 6])\nsns.lineplot(x=\"price_range\", y=\"ram\", data=train, marker = '*', ms=13, ci=None)\nplt.show()","5bc3fcd6":"corr = train.corr()\ng = sns.heatmap(corr, vmax=.3, center=0,\nsquare=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True, fmt='.2f', cmap='coolwarm')\nsns.despine()\ng.figure.set_size_inches(25,9)\nplt.show()","06d2e786":"a = ((train['price_range'] == 0).sum() \/ train['price_range'].count() * 100), ((train['price_range'] == 1).sum() \/ train['price_range'].count() * 100), ((train['price_range'] == 2).sum() \/ train['price_range'].count() * 100), ((train['price_range'] == 3).sum() \/ train['price_range'].count() * 100)\nfont1 = {'family':'serif','color':'black','size':20}\n# Creating plot\nfig = plt.figure(figsize =(10, 7))\nplt.pie(a, labels = train['price_range'].unique(),explode = [0, 0, 0, 0], autopct='%1.0f%%')\nplt.title(\"Distribution of Mobile Price Range\", fontdict = font1)\nplt.show()","bd466aa3":"X = train.drop(['price_range'], axis=1) # Features\ny = train['price_range'] # Label\nX_test = test.drop(['id'], axis=1) # Test Features\nX.shape, y.shape, X_test.shape","3f582cbe":"def dt_classifer(X, y):\n    list = []\n    # Split data in train_test_split\n    print(\"First we split data with train_test_split and kfolds: \")\n    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=11)\n    # Find best parameters for decision tree\n    print(\"Lets findout best parameters for decision tree: \")\n    dt_classifier = DecisionTreeClassifier()\n    param_dist = {\"max_depth\": [3, 10],\n              \"random_state\": [3, 11],\n              \"max_features\": range(1, 11),\n            \"splitter\": [\"best\", \"random\"], \n              \"min_samples_split\": range(2, 11),\n              \"criterion\": [\"gini\", \"entropy\"]}\n    random_search = RandomizedSearchCV(dt_classifier, param_distributions=param_dist, n_iter=10, cv=5)\n    random_search.fit(X_train, y_train)\n    print(\"best parameters: \", random_search.best_params_)\n    print(\"best parameters score: \", random_search.best_score_ * 100)\n    # Save the dict in list\n    for v in random_search.best_params_.values():\n        list.append(v)\n    \n    # Give parameters to Decision tree with train_test_split\n    print(\"Now we use this parameters in our model: \")\n    dt_classifier = DecisionTreeClassifier(splitter= list[0], random_state=list[1], min_samples_split=list[2], max_features=list[3], max_depth=list[4], criterion=list[5])\n    dt_classifier.fit(X_train, y_train)\n    y_pred_dt = dt_classifier.predict(X_test)\n    dt_test_accuracy = accuracy_score(y_test, y_pred_dt)\n    dt_train_acc = cross_val_score(dt_classifier, X_train, y_train)\n    print(\"Model Accuracy with train_test_split: \", dt_test_accuracy * 100)\n    \n    # Give same parameters to kfold splitting\n    kfolds = KFold(n_splits=10, random_state=7, shuffle=True)\n    model = DecisionTreeClassifier(splitter= list[0], random_state=list[1], min_samples_split=list[2], max_features=list[3], max_depth=list[4], criterion=list[5])\n    score = cross_val_score(model, X, y, cv=kfolds)\n    print(\"Accuracy of model with Kfolds: \",(score.mean()*100))\n\n\ndt_classifer(X, y)","208d7443":"# Splitting Data into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=11)","7ffbfb05":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=10, metric='manhattan')\n\n# training a knn model\nknn.fit(X_train,y_train)\na = knn.predict(X_test)\nprint(\"KNN Score: \", knn.score(X_train, y_train) * 100)\nprint(\"KNN Accuracy: \", accuracy_score(y_test, a) * 100)","2833e9c4":"xgb_classifier = XGBClassifier(random_state=11, verbosity = 0)\nxgb_classifier.fit(X_train, y_train)\ny_pred_xgb = xgb_classifier.predict(X_test)\nxgb_accuracy = accuracy_score(y_test, y_pred_xgb)\nxgb_train_acc = cross_val_score(xgb_classifier, X_train, y_train)\nprint(\"XGBoost Accuracy: \", xgb_accuracy * 100)\nprint(xgb_train_acc * 100)","b2d9199f":"random_forest = RandomForestClassifier(max_depth=7)\nrandom_forest.fit(X_train, y_train)\nrandom_pred = random_forest.predict(X_test)\nrandom_test_score = accuracy_score(y_test, random_pred)\nacc_random = cross_val_score(random_forest, X_train, y_train, cv=5)\nprint(random_test_score * 100)\nprint(acc_random * 100)","838141b7":"clf = RandomForestClassifier(100)\nclf.fit(X_train, y_train)\nfeature_importance = pd.DataFrame({'importance': clf.feature_importances_}, index= X.columns).sort_values('importance')\nfeature_importance","50828359":"feature_importance.plot.barh()","0c1ee221":"X1 = train.drop(['price_range', 'three_g', 'wifi', 'four_g', 'blue', 'dual_sim', 'touch_screen'], axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X1, y, test_size=0.3, random_state=11)\nrandom_forest = RandomForestClassifier(max_depth=7)\nrandom_forest.fit(X_train, y_train)\nrandom_pred = random_forest.predict(X_test)\nrandom_test_score = accuracy_score(y_test, random_pred)\nacc_random = cross_val_score(random_forest, X_train, y_train, cv=5)\nprint(random_test_score * 100)\nprint(acc_random * 100)","b6bc2c95":"predictions = random_forest.predict(X_test)\npredictions","df649a2e":"**Random Forest**","176705ea":"**Best Algorithm for this dataset is KNN. who give 93% Accuracy.**\n**I will Update my algorithms in next notebok.**\n**If you like my work, please upvote and comment**","7bae366e":"# EDA","2377ac84":"**KNN**","f3e50166":"**XGBoost**","389393e5":"**Decision Tree**","0b31f11a":"# Load DataSet and Analysis it","fbcb98e8":"**Lets Findout Importance Feature**","e4561e3c":"**Price Range is depended on Ram. Increasing of Ram also increase Price**","4e55abb7":"**We have two CSV files Train and Test**\n* Train have 2000 rows and 21 columns\n* Do not have any null values\n* Target Columns is \"price_range\" in train file\n* Train data have 2 \"float64\" dtype and 19 \"int64\" dtypes\n* Test have 1000 rows and have 21 columns, but test data have \"id\" column which is not in train data\n* We have to predict price_range in test data","592d8640":"# Import Helpful Libraries","78fb6780":"# Giving Data to Machine Learning algorithms"}}