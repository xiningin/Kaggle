{"cell_type":{"7fbe9703":"code","64aa664e":"code","822fa0b4":"code","5c08f40d":"code","9cbb14ee":"code","1ab438ee":"code","1a526fd8":"code","f23e9d2b":"code","2f0167f0":"code","52c9c56c":"code","bb72adab":"code","bd572626":"code","5bdc0d98":"code","1053b513":"code","a60182e4":"code","235fb97f":"code","f11a3b07":"code","a0ec06cf":"code","0016297e":"code","7dc78772":"code","f8cca187":"code","a4cbd7ce":"code","a765e363":"code","e9e82fde":"code","86c058bb":"code","0f9b3375":"code","2f41eac3":"code","4c9d04de":"markdown","daeeef7a":"markdown","d98d9c63":"markdown","49ac5414":"markdown","f5dc9802":"markdown","edd035a2":"markdown","df021544":"markdown","375acb9a":"markdown","36f1a12f":"markdown","1c233e0e":"markdown","8b94bff1":"markdown","5c66b72a":"markdown","2cf80b4e":"markdown","4453d456":"markdown","ce3d1e2d":"markdown","7ddca4ce":"markdown","b681cfe6":"markdown","27d73815":"markdown","3719ee23":"markdown","cd5a0a8e":"markdown"},"source":{"7fbe9703":"import numpy as np\nimport pandas as pd\nimport math\npd.plotting.register_matplotlib_converters()\n\n# Classification\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier , GradientBoostingClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis , QuadraticDiscriminantAnalysis\n\n# Regression\nfrom sklearn.linear_model import LinearRegression,Ridge,Lasso,RidgeCV, ElasticNet\nfrom sklearn.ensemble import RandomForestRegressor,BaggingRegressor,GradientBoostingRegressor,AdaBoostRegressor \nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.neural_network import MLPRegressor\n\n# Modelling Helpers\nfrom sklearn.preprocessing import Normalizer, scale\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.model_selection import GridSearchCV , KFold , cross_val_score\n\n#preprocessing\nfrom sklearn.preprocessing import MinMaxScaler , StandardScaler, LabelEncoder\n\n# Regression\nfrom sklearn.metrics import mean_squared_log_error,mean_squared_error, r2_score,mean_absolute_error \n\n# Classification\nfrom sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score  \n\nfrom numpy import loadtxt\nfrom xgboost import XGBClassifier\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\n%matplotlib inline\n\nimport seaborn as sns\nimport missingno as msno\n\nmpl.style.use( 'ggplot' )\nplt.style.use('fivethirtyeight')\nsns.set(context=\"notebook\", palette=\"dark\", style = 'whitegrid' , color_codes=True)\nparams = { \n    'axes.labelsize': \"large\",\n    'xtick.labelsize': 'x-large',\n    'legend.fontsize': 20,\n    'figure.dpi': 150,\n    'figure.figsize': [25, 7]\n}\nplt.rcParams.update(params)","64aa664e":"import os\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n# Any results you write to the current directory are saved as output.\n\n# Path of the file to read\nign_filepath = \"..\/input\/diamonds\/diamonds.csv\"\n\n# Fill in the line below to read the file into a variable ign_data\n# index_col means what the dataframe indexcolm is.\ndata = pd.read_csv(ign_filepath, index_col=\"Unnamed: 0\")\n\nprint(\"setup completed\")","822fa0b4":"data.head()","5c08f40d":"# let's see the shape\ndata.shape","9cbb14ee":"# check some info about this dataset\ndata.info()","1ab438ee":"# check if there are any NaN variables.\ndata.isnull().sum()","1a526fd8":"# just to visualize. no missing values.\nmsno.matrix(data)","f23e9d2b":"# let's see some things about it\ndata.describe()","2f0167f0":"# data.loc[(data['x']==0) | (data['y']==0) | (data['z']==0)]\n# data[(data['x']==0) | (data['y']==0) | (data['z']==0)]\ndata.loc()\n","52c9c56c":"len(data[(data['x']==0) | (data['y']==0) | (data['z']==0)])","bb72adab":"data = data[(data[['x','y','z']] != 0).all(axis=1)]\n\n# Just to Confirm\ndata.loc[(data['x']==0) | (data['y']==0) | (data['z']==0)]","bd572626":"# Correlation Map\ncorr = data.corr()\nsns.heatmap(data=corr, square=True , annot=True, cbar=True)","5bdc0d98":"sns.kdeplot(data['carat'], shade=True , color='b')","1053b513":"sns.jointplot(x='carat' , y='price' , data=data , height=5)","a60182e4":"sns.catplot(x='cut', data=data , kind='count',aspect=2.5 )","235fb97f":"sns.factorplot(x='cut', y='price', data=data, kind='box', aspect=2.5)\n\n# Understanding Box Plot :\n\n# The bottom line indicates the min value of Age.\n# The upper line indicates the max value.\n# The middle line of the box is the median or the 50% percentile.\n# The side lines of the box are the 25 and 75 percentiles respectively.","f11a3b07":"sns.catplot(x='color', y='price' , data=data , kind='violin', aspect=2.5)","a0ec06cf":"sns.boxplot(x='clarity', y='price', data=data)","0016297e":"data['volume'] = data['x'] * data['y'] * data['z']\ndata.head()","7dc78772":"sns.jointplot(x='volume', y='price', data=data, height=5)","f8cca187":"data = data[(data[\"volume\"]<800)]\nsns.jointplot(x='volume', y='price', data=data, height=5)","a4cbd7ce":"data.drop(['x','y','z'], axis=1, inplace= True)","a765e363":"# Label to numbers\nlabel_cut = LabelEncoder()\nlabel_color = LabelEncoder()\nlabel_clarity = LabelEncoder()\n\n\ndata['cut'] = label_cut.fit_transform(data['cut'])\ndata['color'] = label_color.fit_transform(data['color'])\ndata['clarity'] = label_clarity.fit_transform(data['clarity'])\ndata.head()","e9e82fde":"# Split the data into train and test.\n\nX = data.drop(['price'], axis=1)\ny = data['price']\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=66)\nX_train.head()\n# y_train.head()","86c058bb":"sc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","0f9b3375":"clf_rf = RandomForestRegressor()\nclf_rf.fit(X_train , y_train)\naccuracies = cross_val_score(estimator = clf_rf, X = X_train, y = y_train, cv = 5,verbose = 1)\ny_pred = clf_rf.predict(X_test)\nprint('')\nprint('###### Random Forest ######')\nprint('Score : %.4f' % clf_rf.score(X_test, y_test))\nprint(accuracies)\n\nmse = mean_squared_error(y_test, y_pred)\nmae = mean_absolute_error(y_test, y_pred)\nrmse = mean_squared_error(y_test, y_pred)**0.5\nr2 = r2_score(y_test, y_pred)\n\nprint('')\nprint('MSE    : %0.2f ' % mse)\nprint('MAE    : %0.2f ' % mae)\nprint('RMSE   : %0.2f ' % rmse)\nprint('R2     : %0.2f ' % r2)","2f41eac3":"# fit model no training data\nmodel = XGBClassifier()\nmodel.fit(X_train, y_train)","4c9d04de":"> let's see what we have by running the head() function or the tail() function to see the first or last 5 rows of the dataframe","daeeef7a":"# Cut","d98d9c63":"# **Select a dataset**\n\n# And\n# **Load the data**\n> As you learned in the previous tutorial, we load the dataset using the `\"pd.read_csv\"` command.","49ac5414":"# Color vs Price","f5dc9802":"## So there are 20 rows with a dimation of zero.\n* I will just remove those instad of filling them with some Mean.","edd035a2":"So there are no no NaN values","df021544":"# Scaling","375acb9a":"## Correlation Between Features","36f1a12f":"It seems like it's a exponentional growht","1c233e0e":"## Do you see the min of x, y and z!\n## That can be good and posible.\nlet's zoom in","8b94bff1":"## Features\n* Carat : Carat weight of the Diamond.\n* Cut : Describe cut quality of the diamond.\n> Quality in increasing order Fair, Good, Very Good, Premium, Ideal .\n* Color : Color of the Diamond.\n> With D being the best and J the worst.\n* Clarity : Diamond Clarity refers to the absence of the Inclusions and Blemishes.\n> (In order from Best to Worst, FL = flawless, I3= level 3 inclusions) FL, IF, VVS1, VVS2, VS1, VS2, SI1, SI2, I1, I2, I3\n* Depth : The Height of a Diamond, measured from the Culet to the table, divided by its average Girdle Diameter.\n* Table : The Width of the Diamond's Table expressed as a Percentage of its Average Diameter.\n* Price : the Price of the Diamond.\n* X : Length of the Diamond in mm.\n* Y : Width of the Diamond in mm.\n* Z : Height of the Diamond in mm.\n\nQualitative Features (Categorical) : Cut, Color, Clarity.\n\nQuantitative Features (Numerical) : Carat, Depth , Table , Price , X , Y, Z.\n\n### Price is the Target Variable.\n![image.png](attachment:image.png)\n","5c66b72a":"# Clarity vs Price","2cf80b4e":"Remove X, Y and Z axais","4453d456":"# Volume","ce3d1e2d":"# Cut vs Price","7ddca4ce":"### Carat vs Price","b681cfe6":"# Conclution\nSo what can you see?\n1. The Price of the Diamond is highly correlated to Carat, and its Dimensions.\n2. The Length(x) , Width(y) and Height(z) seems to be higly related to Price and even each other.\n3. Depth is inversely related to Price.\n> * This is because if a Diamond's Depth percentage is too large or small the Diamond will become 'Dark' in appearance because it will no longer return an Attractive amount of light.\n4. The Weight (Carat) of a diamond has the most significant impact on its Price.","27d73815":"Here you can see that higher carat is rearer than lower carat.","3719ee23":"# **Set up the notebook**","cd5a0a8e":"It seems that Carat varies with Price Exponentially.\u00b6"}}