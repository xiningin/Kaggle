{"cell_type":{"392dd2a0":"code","2a85f3f7":"code","f11e8908":"code","73ff897f":"code","a1db7c4e":"code","c66af796":"code","f31a07ca":"code","522df2b5":"code","8813bd67":"code","f9d9be3b":"code","878fe0ac":"code","84942a61":"code","8aad5249":"code","c7db3a63":"code","f59eb5e9":"code","e46700d1":"code","4c17ef36":"code","0014aa55":"code","ad163f04":"code","976373c6":"code","6dcbbf36":"code","869a39d3":"code","ee9f5d06":"code","2201e212":"code","9b46635c":"code","199115d0":"code","78e6cd2f":"code","384c84fe":"code","6f7b33ed":"code","12d049dd":"code","2eabaefc":"code","3e5cb0c8":"code","bff65cfc":"code","25175e63":"code","9e7f3b32":"code","7ae53108":"code","501c0feb":"code","b85aa654":"code","73e3d213":"markdown","9a196ed4":"markdown","590b25c9":"markdown","763073c0":"markdown","f086df5b":"markdown","86cb9f7e":"markdown","4f1f3446":"markdown","2e46c7fa":"markdown","2a75e863":"markdown","69c4cd9e":"markdown","9d2a3e33":"markdown","cd555d30":"markdown","9a748ed0":"markdown","d4c105bd":"markdown","d5a8ec8e":"markdown","86b2cae8":"markdown","0ef0bf43":"markdown","c1ac5bed":"markdown","e77b3d56":"markdown","19dc7ca2":"markdown","48c25d06":"markdown","7465a36b":"markdown","fca08bef":"markdown","278fdaad":"markdown","9728c660":"markdown","b709b617":"markdown","f42f3841":"markdown","e6cadf80":"markdown","4f65c8c8":"markdown","381c9451":"markdown","01f74c56":"markdown","0dd7930c":"markdown","511cf86e":"markdown","f9acf67b":"markdown","32aae4d4":"markdown","6fcad552":"markdown","b2a14cf2":"markdown"},"source":{"392dd2a0":"import numpy as np\nimport pandas as pd\nimport warnings\n\nwarnings.simplefilter('ignore')\n\ntrain = pd.read_csv('..\/input\/ieee-fraud-detection\/train_transaction.csv')\ntest = pd.read_csv('..\/input\/ieee-fraud-detection\/test_transaction.csv')","2a85f3f7":"card_features = ['card1', 'card2', 'card3', 'card4', 'card5', 'card6']","f11e8908":"train[card_features].head()","73ff897f":"pd.concat([train[card_features].isna().sum(), test[card_features].isna().sum()], axis=1).rename(columns={0: 'train_NaNs', 1: 'test_NaNs'})","a1db7c4e":"pd.concat([train[card_features].isna().sum() \/ train.shape[0], test[card_features].isna().sum() \/ test.shape[0]], axis=1).rename(columns={0: 'train_NaNs_%', 1: 'test_NaNs_%'})","c66af796":"#Some usefull functions\n\ndef count_uniques(train, test, pair):\n    unique_train = []\n    unique_test = []\n\n    for value in train[pair[0]].unique():\n        unique_train.append(train[pair[1]][train[pair[0]] == value].value_counts().shape[0])\n\n    for value in test[pair[0]].unique():\n        unique_test.append(test[pair[1]][test[pair[0]] == value].value_counts().shape[0])\n\n    pair_values_train = pd.Series(data=unique_train, index=train[pair[0]].unique())\n    pair_values_test = pd.Series(data=unique_test, index=test[pair[0]].unique())\n    \n    return pair_values_train, pair_values_test\n\ndef fill_card_nans(train, test, pair_values_train, pair_values_test, pair):\n    print(f'In train{[pair[1]]} there are {train[pair[1]].isna().sum()} NaNs' )\n    print(f'In test{[pair[1]]} there are {test[pair[1]].isna().sum()} NaNs' )\n\n    print('Filling train...')\n    \n    for value in pair_values_train[pair_values_train == 1].index:\n        train[pair[1]][train[pair[0]] == value] = train[pair[1]][train[pair[0]] == value].value_counts().index[0]\n        \n    print('Filling test...')\n\n    for value in pair_values_test[pair_values_test == 1].index:\n        test[pair[1]][test[pair[0]] == value] = test[pair[1]][test[pair[0]] == value].value_counts().index[0]\n        \n    print(f'In train{[pair[1]]} there are {train[pair[1]].isna().sum()} NaNs' )\n    print(f'In test{[pair[1]]} there are {test[pair[1]].isna().sum()} NaNs' )\n    \n    return train, test\n\ndef nans_distribution(train, test, unique_train, unique_test, pair):\n    train_nans_per_category = []\n    test_nans_per_category = []\n\n    for value in unique_train.unique():\n        train_nans_per_category.append(train[train[pair[0]].isin(list(unique_train[unique_train == value].index))][pair[1]].isna().sum())\n\n    for value in unique_test.unique():\n        test_nans_per_category.append(test[test[pair[0]].isin(list(unique_test[unique_test == value].index))][pair[1]].isna().sum())\n\n    pair_values_train = pd.Series(data=train_nans_per_category, index=unique_train.unique())\n    pair_values_test = pd.Series(data=test_nans_per_category, index=unique_test.unique())\n    \n    return pair_values_train, pair_values_test","f31a07ca":"train[train['card1'] == 13926][['card1', 'card2']]","522df2b5":"unique_values_train, unique_values_test = count_uniques(train, test, ('card1', 'card2'))\npd.concat([unique_values_train.value_counts(), unique_values_test.value_counts()], axis=1).rename(columns={0: 'train', 1: 'test'})","8813bd67":"train_nan_dist, test_nan_dist = nans_distribution(train, test, unique_values_train, unique_values_test, ('card1', 'card2'))\npd.concat([train_nan_dist, test_nan_dist], axis=1).rename(columns={0: 'train', 1: 'test'})","f9d9be3b":"train, test = fill_card_nans(train, test, unique_values_train, unique_values_test, ('card1', 'card2'))","878fe0ac":"train[train['card1'] == 13926][['card1', 'card3']]","84942a61":"unique_values_train, unique_values_test = count_uniques(train, test, ('card1', 'card3'))\npd.concat([unique_values_train.value_counts(), unique_values_test.value_counts()], axis=1).rename(columns={0: 'train', 1: 'test'})","8aad5249":"train_nan_dist, test_nan_dist = nans_distribution(train, test, unique_values_train, unique_values_test, ('card1', 'card3'))\npd.concat([train_nan_dist, test_nan_dist], axis=1).rename(columns={0: 'train', 1: 'test'})","c7db3a63":"train, test = fill_card_nans(train, test, unique_values_train, unique_values_test, ('card1', 'card3'))","f59eb5e9":"train[train['card1'] == 13926][['card1', 'card4']]","e46700d1":"unique_values_train, unique_values_test = count_uniques(train, test, ('card1', 'card4'))\npd.concat([unique_values_train.value_counts(), unique_values_test.value_counts()], axis=1).rename(columns={0: 'train', 1: 'test'})","4c17ef36":"train_nan_dist, test_nan_dist = nans_distribution(train, test, unique_values_train, unique_values_test, ('card1', 'card4'))\npd.concat([train_nan_dist, test_nan_dist], axis=1).rename(columns={0: 'train', 1: 'test'})","0014aa55":"train, test = fill_card_nans(train, test, unique_values_train, unique_values_test, ('card1', 'card4'))","ad163f04":"train[train['card1'] == 13926][['card1', 'card5']]","976373c6":"unique_values_train, unique_values_test = count_uniques(train, test, ('card1', 'card5'))\npd.concat([unique_values_train.value_counts(), unique_values_test.value_counts()], axis=1).rename(columns={0: 'train', 1: 'test'})","6dcbbf36":"train_nan_dist, test_nan_dist = nans_distribution(train, test, unique_values_train, unique_values_test, ('card1', 'card5'))\npd.concat([train_nan_dist, test_nan_dist], axis=1).rename(columns={0: 'train', 1: 'test'})","869a39d3":"train, test = fill_card_nans(train, test, unique_values_train, unique_values_test, ('card1', 'card5'))","ee9f5d06":"train[train['card1'] == 13926][['card1', 'card6']]","2201e212":"unique_values_train, unique_values_test = count_uniques(train, test, ('card1', 'card6'))\npd.concat([unique_values_train.value_counts(), unique_values_test.value_counts()], axis=1).rename(columns={0: 'train', 1: 'test'})","9b46635c":"train_nan_dist, test_nan_dist = nans_distribution(train, test, unique_values_train, unique_values_test, ('card1', 'card6'))\npd.concat([train_nan_dist, test_nan_dist], axis=1).rename(columns={0: 'train', 1: 'test'})","199115d0":"train, test = fill_card_nans(train, test, unique_values_train, unique_values_test, ('card1', 'card6'))","78e6cd2f":"pd.concat([train[card_features].isna().sum(), test[card_features].isna().sum()], axis=1).rename(columns={0: 'train_NaNs', 1: 'test_NaNs'})","384c84fe":"train[card_features].head()","6f7b33ed":"print('Card3 == 150: ', train[train['card3'] == 150]['card2'].nunique())\nprint('Card4 == mastercard: ', train[train['card4'] == 'mastercard']['card2'].nunique())\nprint('Card5 == 102: ', train[train['card5'] == 102]['card2'].nunique())\nprint('Card6 == credit: ', train[train['card6'] == 'credit']['card2'].nunique())","12d049dd":"print('Card2 == 327: ', train[train['card2'] == 327]['card5'].nunique())\nprint('Card3 == 150: ', train[train['card3'] == 150]['card5'].nunique())\nprint('Card4 == mastercard: ', train[train['card4'] == 'mastercard']['card5'].nunique())\nprint('Card6 == credit: ', train[train['card6'] == 'credit']['card5'].nunique())","2eabaefc":"train[train['card1'] == 13926][['card1', 'addr2']]","3e5cb0c8":"unique_values_train, unique_values_test = count_uniques(train, test, ('card1', 'addr2'))\npd.concat([unique_values_train.value_counts(), unique_values_test.value_counts()], axis=1).rename(columns={0: 'train', 1: 'test'})","bff65cfc":"train_nan_dist, test_nan_dist = nans_distribution(train, test, unique_values_train, unique_values_test, ('card1', 'addr2'))\npd.concat([train_nan_dist, test_nan_dist], axis=1).rename(columns={0: 'train', 1: 'test'})","25175e63":"train, test = fill_card_nans(train, test, unique_values_train, unique_values_test, ('card1', 'addr2'))","9e7f3b32":"train[train['card1'] == 13926]['addr2'].value_counts().shape[0] == 1","7ae53108":"depend_features = []\n\nfor col in train.columns:\n    if train[train['card1'] == 13926][col].value_counts().shape[0] == 1:\n        depend_features.append(col)\n\nprint(depend_features)","501c0feb":"def fill_pairs(train, test, pairs):\n    for pair in pairs:\n\n        unique_train = []\n        unique_test = []\n\n        print(f'Pair: {pair}')\n        print(f'In train{[pair[1]]} there are {train[pair[1]].isna().sum()} NaNs' )\n        print(f'In test{[pair[1]]} there are {test[pair[1]].isna().sum()} NaNs' )\n\n        for value in train[pair[0]].unique():\n            unique_train.append(train[pair[1]][train[pair[0]] == value].value_counts().shape[0])\n\n        for value in test[pair[0]].unique():\n            unique_test.append(test[pair[1]][test[pair[0]] == value].value_counts().shape[0])\n\n        pair_values_train = pd.Series(data=unique_train, index=train[pair[0]].unique())\n        pair_values_test = pd.Series(data=unique_test, index=test[pair[0]].unique())\n        \n        print('Filling train...')\n\n        for value in pair_values_train[pair_values_train == 1].index:\n            train.loc[train[pair[0]] == value, pair[1]] = train.loc[train[pair[0]] == value, pair[1]].value_counts().index[0]\n\n        print('Filling test...')\n\n        for value in pair_values_test[pair_values_test == 1].index:\n            test.loc[test[pair[0]] == value, pair[1]] = test.loc[test[pair[0]] == value, pair[1]].value_counts().index[0]\n\n        print(f'In train{[pair[1]]} there are {train[pair[1]].isna().sum()} NaNs' )\n        print(f'In test{[pair[1]]} there are {test[pair[1]].isna().sum()} NaNs' )\n        \n    return train, test","b85aa654":"pairs = [('card1', 'card2'), ('card1', 'card3')]\n\ntrain, test = fill_pairs(train, test, pairs)","73e3d213":"Not very high ratios though.","9a196ed4":"# Card1 and Card5","590b25c9":"Still there are a lot of NaNs in the card2 and card5. Let's try some other fill combinations.","763073c0":"Same for card5.","f086df5b":"In this kernel I'm trying to fill some NaNs values using one intresting observation.","86cb9f7e":"In the dataset we can found a lot of cases like that. Where most of the values are the same, but there are some missing values. So we can assume that in NaN rows should be that only value which occurs in that card1 category. ","4f1f3446":"## If you want to apply my kernel you can use that function:","2e46c7fa":"Ok, here is the same dependency.","2a75e863":"There are a lot of columns that we can suspect in dependency. And some of them we can fill like above.","69c4cd9e":"We can see that there are too many unique values to implement this approch to fill remaining NaNs in card2.","9d2a3e33":"So, we can fill to many values using this approach. But we cannot be 100% sure that missing values in 1-amount category is most frequent category.","cd555d30":"But right now we will focus only on 1-amount category and fill NaNs with most frequent value in card1 category.","9a748ed0":"Let's count unique values for each card1 category.","d4c105bd":"# Card1 and Card3","d5a8ec8e":"# Card1 and Card2","86b2cae8":"# Card1 and Card4","0ef0bf43":"Let's find another dependent feature for card2.","c1ac5bed":"We can see that most of the card1 category have only one unique value. ","e77b3d56":"If you find this kernel helpful please upvote!","19dc7ca2":"Now let's count amount of the missing values for amount of the unique values.","48c25d06":"Let's find another dependent feature for card5.","7465a36b":"There are really a lot of missing values in addr2, especially for 1-amount category.","fca08bef":"I've noticed that there are cases in card columns that depends on other card columns. So using that approach we can fill some NaNs in data. Let's look at the data!","278fdaad":"So we filled almost all NaNs in card3.","9728c660":"Let's try some other features.","b709b617":"Let's count all NaNs in every card columns","f42f3841":"Here we have the same problem. And that approach can solve it too.","e6cadf80":"*just random pic idk*\n![](https:\/\/sun9-31.userapi.com\/c857628\/v857628861\/4c59e\/JiPqE9xmzjs.jpg)","4f65c8c8":"There is dependency between \u0441ard2 and card1 values.  ","381c9451":"# Card1 and Card6","01f74c56":"Let's do all the same but for card3 category.","0dd7930c":"Let's look ratio of missing values to the total number of rows.","511cf86e":"We can see that card2 is the most NaN card feature. What is more, card3, card4 and car6 in test have 2 times more NaNs values.","f9acf67b":"Hm. There are a lot of missing values for categorys where there is no values(only NaNs) and where only one value.\nSo we can do that:\n\n* Fill NaNs in 1-amount category with most frequent value\n* Treat 0-amount category NaNs as only one category. We can just encode it somehow.","32aae4d4":"### Ok. Let's look at number on NaNs now.","6fcad552":"# Let's find all the features that depends on card1","b2a14cf2":"# Card1 and Addr2"}}