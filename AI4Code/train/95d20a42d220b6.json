{"cell_type":{"7e77325f":"code","b20d9c22":"code","e104026f":"code","2f91a4a8":"code","946ffdea":"code","3b8e709f":"code","c86edcf2":"code","72a54568":"code","90f0bdbf":"code","7dd3719f":"code","5ecca050":"code","2f07cf6f":"code","d2bb7908":"code","2cac23ea":"code","39be5939":"code","ac752784":"code","beaacf44":"code","3f25a2d7":"code","692d2683":"code","9023fe13":"code","ec07e34d":"code","0352d058":"code","0abe274f":"code","3c9316b3":"code","bb31ac88":"code","f7df0c92":"code","66df83b3":"code","b90771ce":"code","0d69c743":"code","b278429e":"code","cdd05680":"code","0b2653ce":"code","7e3381a9":"code","d21b511f":"code","b55b6143":"code","2d955fd8":"markdown","59ab08f9":"markdown","f4af6e46":"markdown","d597d7f2":"markdown","611b3b39":"markdown","5a4eb83a":"markdown","8679a31b":"markdown","1ab79926":"markdown","6c6c506d":"markdown","c9b3ada4":"markdown","bb82f104":"markdown","f743afe6":"markdown","eeaf04ac":"markdown","01391c16":"markdown"},"source":{"7e77325f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b20d9c22":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import random_split,DataLoader,TensorDataset\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split","e104026f":"train=pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\")\ntest=pd.read_csv(\"..\/input\/digit-recognizer\/test.csv\")","2f91a4a8":"test.head()","946ffdea":"train.head()","3b8e709f":"len(train)","c86edcf2":"len(test)","72a54568":"train.shape","90f0bdbf":"test.shape","7dd3719f":"# Target variable\nlabels=train.label\n","5ecca050":"# Independent variables\ntrain_ds=train.drop('label',axis=1)","2f07cf6f":"train_ds.head()","d2bb7908":"plt.imshow(train_ds[0:1].values.reshape(28,28))\nplt.axis(\"off\")\nprint(labels[0])","2cac23ea":"train_ds=train_ds.values\ntest_ds=test.values\nlabels=labels.to_numpy()             #labels was actually a pandas series\n","39be5939":"train_ds=torch.tensor(train_ds)\ntest_ds =torch.tensor(test_ds)\nlabels = torch.tensor(labels)","ac752784":"traiin_ds=TensorDataset(train_ds,labels)","beaacf44":"traiin_ds[0:2]","3f25a2d7":"train_ds , val_ds = random_split(traiin_ds,(32000,10000))","692d2683":"# Define hyperparameters\nbatch_size = 150\nlearning_rate = 0.001","9023fe13":"train_loader=DataLoader(train_ds , batch_size, shuffle = True)\nval_loader=DataLoader(val_ds , batch_size, shuffle = False)\n","ec07e34d":"# for each image\ninput_size = 784\nnum_class = 10\n\n","0352d058":"class MnistModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = nn.Sequential(nn.Linear(28*28 , 512),\n                                    nn.ReLU(),\n                                    nn.Linear(512 , 100),\n                                    nn.ReLU(),\n                                    nn.Linear(100 , 10))\n        \n        \n    def forward(self,x):\n        out = self.linear(x)\n        return out\n\nmodel = MnistModel()","0abe274f":"opt      = torch.optim.Adam\nopt = opt(model.parameters(),lr=learning_rate)","3c9316b3":"def fit(epochs,model,data):\n    \n    loss_fun = F.cross_entropy\n    hist = []\n    for Epoch in range(epochs):\n        for img,label in data:\n            out = model(img\/255)\n            loss = loss_fun(out,label)\n            loss.backward()\n            opt.step()\n            opt.zero_grad()\n            hist.append(loss)\n        \n        if (Epoch+1)%10 ==0:\n            print(f\"Epoch:[{Epoch+1}\/{epochs}] ; Loss : {loss}\")\n    return hist\n      ","bb31ac88":"hist=fit(90,model,train_loader)","f7df0c92":"def acc(data):\n    accuracy=[]\n    for img , label in data:\n        out = model(img\/255)\n        _,pred_index =torch.max(out,dim=1)\n        x=torch.sum(pred_index==label)\n        x=x\/len(label)\n        x=x*100\n        accuracy.append(x)\n    return np.mean(accuracy)\n        \n        ","66df83b3":"acc(val_loader)","b90771ce":"# plt.plot(hist, range(32100))","0d69c743":"submission= pd.read_csv(\"..\/input\/digit-recognizer\/sample_submission.csv\")\nsubmission.head()","b278429e":"len(submission)\n","cdd05680":"def prediction(data):\n    LABEL=[]\n    \n    out = model(data\/255)\n    out = F.softmax(out)     \n    _,pred_index =torch.max(out,dim=1)\n    LABEL.append(pred_index)   \n    return LABEL","0b2653ce":"x=prediction(test_ds)\nlen(x[0])","7e3381a9":"x=x[0].numpy()","d21b511f":"submission[\"Label\"]=x","b55b6143":"submission.to_csv(\"submission.csv\",index=False)","2d955fd8":"## Importing required libraries","59ab08f9":"## Training the model using logistic regression","f4af6e46":"## Let's visualise the image and take a look at the label ","d597d7f2":"## Inspecting the datasets","611b3b39":"## Converting all kinds of data to Tensors","5a4eb83a":"## DO UPVOTE !!","8679a31b":"## Defining predictors and target variable","1ab79926":"## Constructing the model","6c6c506d":"## Define optimizer","c9b3ada4":"## Splitting the Tensordataset into train and validation data","bb82f104":"## Create batches","f743afe6":"## Reading the data you are provided with","eeaf04ac":"## Converting all kinds of data to numpy arrays","01391c16":"# Organising data into Tensordataset\n ***Each sample will be retrieved by indexing tensors along the first dimension.***"}}