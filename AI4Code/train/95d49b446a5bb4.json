{"cell_type":{"aea328df":"code","a2a9d343":"code","a249ee3e":"code","ff739ae4":"code","ed8da9b0":"code","1b6e818f":"code","df7479d3":"code","f89bc551":"code","8b6175dc":"code","18c567f5":"code","0f2e5e27":"code","31924397":"code","6e04a0e1":"code","559c4f38":"code","404ba576":"markdown"},"source":{"aea328df":"import random\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom keras import backend as K\nfrom keras import initializers, regularizers, constraints\nfrom keras.models import *\nfrom keras.layers import *\nfrom keras.optimizers import *\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau","a2a9d343":"data = np.load('..\/input\/ustc-geo-ai20\/data.npy')\nlabel = np.load('..\/input\/ustc-geo-ai20\/label.npy')\ntest_data = np.load('..\/input\/ustc-geo-ai20\/new_test_data.npy')","a249ee3e":"index = random.randint(0, len(data))\n\nplt.figure(figsize = (8, 16))\nplt.subplot(1, 2, 1)\nplt.imshow(data[index], cmap = plt.cm.gray)\nplt.subplot(1, 2, 2)\nplt.imshow(label[index], cmap = plt.cm.gray)","ff739ae4":"index = random.randint(0, len(test_data))\n\nplt.figure(figsize = (8, 20))\nplt.imshow(test_data[index], cmap = plt.cm.gray)","ed8da9b0":"# data augmentation(DA)\n\nfor i in range(len(data)):\n    mu = np.average(data[i, :])\n    sigma = np.std(data[i, :])\n    data[i, :] = (data[i, :] - mu) \/ sigma\n\ndata_h = np.flip(data, axis = 1)\ndata_v = np.flip(data, axis = 2)\n\nlabel_h = np.flip(label, axis = 1)\nlabel_v = np.flip(label, axis = 2)\n\ndata = np.concatenate((data, data_h, data_v))\nlabel = np.concatenate((label, label_h, label_v))\n\nnp.random.seed(1)\nnp.random.shuffle(data)\nnp.random.seed(1)\nnp.random.shuffle(label)\n    \ntest_data_h = np.flip(test_data, axis = 1)\ntest_data_v = np.flip(test_data, axis = 2)\n\ndata_len = len(data) # data_len == 6000\n\ndata = np.reshape(data, (data_len, 128, 128, 1))\nlabel = np.reshape(label, (data_len, 128, 128, 1))\ntest_data = np.reshape(test_data, (100, 256, 640, 1))\ntest_data_h = np.reshape(test_data_h, (100, 256, 640, 1))\ntest_data_v = np.reshape(test_data_v, (100, 256, 640, 1))\n\ntrain_data = data[0 : int(data_len * 0.8)]\nval_data = data[int(data_len * 0.8) : data_len]\ntrain_label = label[0 : int(data_len * 0.8)]\nval_label = label[int(data_len * 0.8) : data_len]","1b6e818f":"# dice loss\n\n# https:\/\/www.kaggle.com\/meaninglesslives\/nested-unet-with-efficientnet-encoder\n\nfrom keras.losses import binary_crossentropy\n\ndef dice_coef(y_true, y_pred):\n    y_true_f = K.flatten(y_true)\n    y_pred = K.cast(y_pred, 'float32')\n    y_pred_f = K.cast(K.greater(K.flatten(y_pred), 0.5), 'float32')\n    intersection = y_true_f * y_pred_f\n    score = 2. * K.sum(intersection) \/ (K.sum(y_true_f) + K.sum(y_pred_f))\n    return score\n\ndef dice_loss(y_true, y_pred):\n    smooth = 1.\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = y_true_f * y_pred_f\n    score = (2. * K.sum(intersection) + smooth) \/ (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n    return 1. - score\n\ndef bce_dice_loss(y_true, y_pred):\n    return binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)","df7479d3":"# resnet block\n# instance normalization(IN)\n# nnu-net\n# SEnet\n\n# https:\/\/github.com\/zhao62\/Deep-Residual-Shrinkage-Networks\n# https:\/\/ieeexplore.ieee.org\/document\/8850096\n# https:\/\/www.kaggle.com\/shaojiaxin\/u-net-with-simple-resnet-blocks-v2-new-loss\n# https:\/\/arxiv.org\/pdf\/1809.10486.pdf\n\nfrom IN import InstanceNormalization\n\ndef InstanceActivate(x):\n    x = InstanceNormalization()(x)\n    x = LeakyReLU(0.01)(x)\n    return x\n\ndef convolution_block(x, filters, size, strides=(1,1), padding='same', activation=True):\n    x = Conv2D(filters, size, strides=strides, padding=padding)(x)\n    if activation == True:\n        x = InstanceActivate(x)\n    return x\n\ndef squeeze_excitation_block(blockInput, num_filters):\n    scale = GlobalAveragePooling2D()(blockInput)\n    scale = Dense(num_filters \/\/ 16, activation = 'relu')(scale)\n    scale = Dense(num_filters, activation = 'sigmoid')(scale)\n    scale = Reshape((1, 1, num_filters))(scale)\n    x = Multiply()([blockInput, scale])\n    return x\n    \ndef residual_block(blockInput, num_filters=16, instance_activate=False):\n    x = InstanceActivate(blockInput)\n    x = convolution_block(x, num_filters, (3,3))\n    x = convolution_block(x, num_filters, (3,3), activation = False)\n    x = squeeze_excitation_block(x, num_filters)\n    x = Add()([x, blockInput])\n    if instance_activate:\n        x = InstanceActivate(x)\n    return x","f89bc551":"def encoder_block(blockInput, num_filters):   \n    x = Conv2D(num_filters, (3, 3), activation = None, padding = 'same')(blockInput)\n    x = Conv2D(num_filters, (3, 3), activation = None, padding = 'same')(x)\n    x = residual_block(x, num_filters)  \n    x = residual_block(x, num_filters, True)\n    y = MaxPooling2D(pool_size = (2, 2))(x)\n    # y = Dropout(0.5)(y)\n    \n    output = {'before_pool' : x, 'after_pool' : y}\n    return output\n    \ndef decoder_block(blockInput, num_filters, *info):\n    x = Conv2DTranspose(num_filters, (3, 3), strides = (2, 2), padding = \"same\")(blockInput)\n    x = Concatenate()([x] + list(info))\n    # x = Dropout(0.5)(x)\n    x = Conv2D(num_filters, (3, 3), activation = None, padding = 'same')(x)\n    x = Conv2D(num_filters, (3, 3), activation = None, padding = 'same')(x)\n    x = residual_block(x, num_filters)\n    x = residual_block(x, num_filters, True)\n    \n    return x","8b6175dc":"def unet(input_size = (None, None, 1)):\n\n    input = Input(input_size, name = 'input')\n\n    output_0 = encoder_block(input, 16)\n    output_1 = encoder_block(output_0['after_pool'], 32)\n    output_2 = encoder_block(output_1['after_pool'], 64)\n    output_3 = encoder_block(output_2['after_pool'], 128)\n\n    output_m = encoder_block(output_3['after_pool'], 256)\n\n    output_4 = decoder_block(output_m['before_pool'], 128, output_3['before_pool'])\n    output_5 = decoder_block(output_4, 64, output_2['before_pool'])\n    output_6 = decoder_block(output_5, 32, output_1['before_pool'])\n    output_7 = decoder_block(output_6, 16, output_0['before_pool'])\n    \n    output = Conv2D(1, (1, 1), activation = 'sigmoid')(output_7) \n\n    model = Model(inputs = [input], outputs = [output])\n    model.compile(optimizer = Adam(lr = 3e-4), loss = bce_dice_loss, metrics = ['accuracy'])\n\n    # model.summary()\n    return model","18c567f5":"model = unet()\n\nfilepath = '.\/model\/model.hdf5'\nautosave = ModelCheckpoint(filepath, monitor = 'val_loss', verbose = 1, save_best_only = 'true', mode = 'min')\n# reduce = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.5, patience = 2, verbose = 1, mode = 'auto', min_lr = 1e-6)\ncallbacks_list = [autosave]","0f2e5e27":"history = model.fit(\n    train_data, train_label, \n    batch_size = 16,\n    validation_data = (val_data, val_label),\n    epochs = 40,\n    verbose = 1,\n    shuffle = True,\n    callbacks = callbacks_list\n)","31924397":"plt.figure(figsize = (12, 4))\n\nplt.subplot(1, 2, 1)\nplt.plot(history.history['acc'], label = 'training set')\nplt.plot(history.history['val_acc'], label = 'validation set')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(history.history['loss'], label = 'training set')\nplt.plot(history.history['val_loss'], label = 'validation set')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend()\n\nplt.show()","6e04a0e1":"# test time augmentation(TTA)\n\nmodel = load_model(\"model\/model.hdf5\", custom_objects = {\n      'InstanceNormalization' : InstanceNormalization,\n      'bce_dice_loss' : bce_dice_loss\n})\n\nresult_o = model.predict(test_data)\nresult_h = model.predict(test_data_h)\nresult_v = model.predict(test_data_v)\n\nresult_h = np.flip(result_h, axis = 1)\nresult_v = np.flip(result_v, axis = 2)","559c4f38":"result = (result_o + result_v + result_h) \/ 3\n\nthreshold = 0.35\nresult[result > threshold] = 1\nresult[result < threshold] = 0\nresult_o[result_o > threshold] = 1\nresult_o[result_o < threshold] = 0\n\nindex = random.randint(0, len(test_data))\n\nplt.figure(figsize = (20, 60))\nplt.subplot(1, 3, 1)\nplt.imshow(np.squeeze(test_data[index]), cmap = plt.cm.gray)\nplt.subplot(1, 3, 2)\nplt.imshow(np.squeeze(result_o[index]), cmap = plt.cm.gray)\nplt.subplot(1, 3, 3)\nplt.imshow(np.squeeze(result[index]), cmap = plt.cm.gray)","404ba576":"# What I use to improve the baseline\n* data augamentation\n* test time augmentation\n* nnU-Net\n* ResNet block\n* instance normalization\n* dice loss\n* SENet"}}