{"cell_type":{"5f0ade51":"code","1848bd83":"code","e722e1ca":"code","55b34915":"code","62f0d893":"code","b25a659d":"code","ac01413f":"code","f6813a25":"code","fcd10461":"code","4a516710":"code","16da7474":"code","056733de":"code","a1610eb7":"code","ad9a9e0e":"code","38b1be0b":"code","a1e0946f":"code","3e31ef30":"code","c3e197f7":"code","816034a4":"code","2141084b":"code","eb60f61a":"code","1e168fdf":"code","6ee96d66":"code","f119c83f":"code","b5c6a000":"code","e00f53ea":"code","8d3fdf3e":"code","8c6557b5":"code","502a7d7d":"code","64e52abd":"code","0ff6c8c2":"markdown"},"source":{"5f0ade51":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow as tf\nfrom matplotlib import pyplot as plt\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nfrom sklearn.metrics import confusion_matrix\nimport itertools\nimport seaborn as sns\nfrom subprocess import check_output\nimport csv\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","1848bd83":"df = pd.read_csv('\/kaggle\/input\/equipfails\/equip_failures_training_set.csv')","e722e1ca":"print(df)","55b34915":"df = df.apply(pd.to_numeric, errors = 'coerce')","62f0d893":"df = df.fillna(df.mean())\ndf","b25a659d":"z_train = Counter(df['target'])\nz_train","ac01413f":"sns.countplot(df['target'])","f6813a25":"test = pd.read_csv('\/kaggle\/input\/equipfails\/equip_failures_test_set.csv')\ntest = test.apply(pd.to_numeric, errors = 'coerce')\ntest = test.fillna(test.mean())\n\nprint(test.shape)\ntest.head()","fcd10461":"x_train = (df.iloc[:,1:].values).astype('float32')\ny_train = (df.iloc[:,1].values).astype('int32')\nx_test = test.values.astype('float32')\ny_test = test.values.astype('int32')","4a516710":"x_test.shape","16da7474":"#%matplotlib inline\n#plt.figure(figsize=(12,10))\n#x, y = 10,4\n#for i in range(40):\n #   plt.subplot(y, x, i+1)\n  #  plt.imshow(x_train[i].reshape((1000,1000)),interpolation='nearest')\n#plt.show()","056733de":"from sklearn.preprocessing import MinMaxScaler","a1610eb7":"#normalise the data\nscaler = MinMaxScaler()\nscaler.fit_transform(x_train)\nx_train = scaler.transform(x_train)\nx_test = scaler.transform(x_test)\nx_train.shape","ad9a9e0e":"y_train = y_train.reshape(60000,1)\ny_train.shape","38b1be0b":"print('x_train shape:', x_train.shape)\nprint(x_train.shape[0], 'train samples')\nprint(x_test.shape[0], 'test samples')","a1e0946f":"#X_train = x_train.reshape(x_train.shape[0], 57, 3, -1)\n#X_test = x_test.reshape(x_test.shape[0], 57, 3,-1) \n","3e31ef30":"from keras.datasets import mnist\nfrom keras.preprocessing.image import load_img, array_to_img, ImageDataGenerator\nfrom keras.utils.np_utils import to_categorical\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, Input\nfrom keras.callbacks import ReduceLROnPlateau\nfrom sklearn.model_selection import train_test_split\n\nimport matplotlib.pyplot as plt\n%matplotlib inline","c3e197f7":"#weights\n#Biasis\n#Input Layer > Weight > Hidden Layer > Output Layer\n#cybenko's theorem says you only need one hidden layer for a good answer\n#Use Relu\n#Three Common Optimizers: SGD, RMSprop, Adam\n#Loss function: mean_squared error, categorical_crossentropy, binary_crossentropy\n","816034a4":"model = Sequential()","2141084b":"model.add(Dense(214, activation='sigmoid', input_shape = (171,)))\nmodel.add(Dense(214, activation='sigmoid'))\nmodel.add(Dense(214, activation='sigmoid'))\nmodel.add(Dense(1, activation='sigmoid'))","eb60f61a":"model.compile(optimizer='adam',loss='binary_crossentropy', metrics=['accuracy'])","1e168fdf":"model.summary()","6ee96d66":"history = model.fit(x_train, y_train, epochs=20)","f119c83f":"plt.plot(history.history['accuracy'])\n#plt.plot(history.history['validation_accuracy'])\nplt.plot(history.history['loss'])","b5c6a000":"plt.plot(history.history['loss'])","e00f53ea":"train_df = pd.read_csv('\/kaggle\/input\/equipfails\/equip_failures_training_set.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/equipfails\/equip_failures_test_set.csv')","8d3fdf3e":"prediction = model.predict(y_test)","8c6557b5":"round_off_val = np.round_(prediction)\nround_off_val.sum()","502a7d7d":"#from sklearn.ensemble import RandomForestClassifier\n\n#rf_model = RandomForestClassifier()\n#rf_model.fit(x_train, y_train)\n#rf_model.score(x_train, y_train)\n#print(rf_model.predict(x_test)[35])\n#i  = 0\n#while i < 16001:\n #   if(rf_model.predict(x_test)[i] == 1):\n  #      rf_model.predict(x_test)[i] = 0\n   # if(rf_model.predict(x_test)[i] == 0):\n    #    rf_model.predict(x_test)[i] = 1\n    #i+=1\n\n#print(sum(rf_model.predict(x_test)))\n            ","64e52abd":"#prediction['id'] = test['id']\n#prediction.to_csv(\"submission.csv\")\npredictions = pd.DataFrame(round_off_val, columns=['target']).to_csv('submission.csv')","0ff6c8c2":"1. What is accuracy?\n    **The accuracy of our model is going to be much higher than that of our evaluation dataset**\n2. What is loss?\n3. What is y_test?\n    **I don't know! It's hidden we need to predict it!**\n4. What is validation_accuracy?\n    **Basically we predict the y.test values using our newly found x.test and model.predict**\n"}}