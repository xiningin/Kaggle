{"cell_type":{"986e9339":"code","b7948618":"code","ea22cdd2":"code","8f85676b":"code","261f4689":"code","a4d98f5f":"code","eaed2a3e":"code","93f7c81c":"code","dd7bb6b5":"code","54edf300":"code","aea95c79":"code","aa894394":"code","8d49f644":"code","dad0795a":"code","15c4bc9d":"code","34a4b8c4":"code","c35cc5e2":"code","75e04a9f":"code","6bedb95d":"code","d11d1847":"markdown","755fca94":"markdown","673a7222":"markdown","7a9735af":"markdown"},"source":{"986e9339":"import os\nimport numpy as np\nimport pandas as pd\nimport string\nfrom tqdm import tqdm\nimport time\nimport gc\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","b7948618":"def clean_data(text):\n    regular_punct = list(string.punctuation)\n    for punc in regular_punct:\n        text = text.replace(punc, f\" {punc} \")\n    return text\n\ndef drop_empty_rows(df):\n    nan_value = float(\"NaN\")\n    df.replace(\"\", nan_value, inplace=True)\n\ndef data_process(path):\n    train_df = pd.read_csv(os.path.join(path, \"train.csv\"))\n    test_df = pd.read_csv(os.path.join(path, \"test.csv\"))\n\n    \"\"\"\n        Train: textID, text, selected_text, sentiment\n        Test: textID, text, sentiment\n    \"\"\"\n    ## Train\n    train_df[\"text\"] = train_df[\"text\"].astype(str).str.lower()\n    train_df[\"selected_text\"] = train_df[\"selected_text\"].astype(str).str.lower()\n    train_df[\"sentiment\"] = train_df[\"sentiment\"].astype(str).str.lower()\n    drop_empty_rows(train_df)\n\n    ## Test\n    test_df[\"text\"] = test_df[\"text\"].astype(str).str.lower()\n    test_df[\"sentiment\"] = test_df[\"sentiment\"].astype(str).str.lower()\n    drop_empty_rows(test_df)\n\n    return train_df, test_df","ea22cdd2":"\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\n\ndef jaccard(str1, str2):\n    a = set(str1.lower().split())\n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))\n\ndef jaccard_distance(y_true, y_pred, smooth=100):\n    y_true = tf.cast(y_true, tf.float32)\n    y_pred = tf.cast(y_pred, tf.float32)\n    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n    sum_ = K.sum(K.abs(y_true) + K.abs(y_pred), axis=-1)\n    jac = (intersection + smooth) \/ (sum_ - intersection + smooth)\n    return (1 - jac) * smooth","8f85676b":"import os\nimport string\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense, Embedding, GRU, Bidirectional, Concatenate\nfrom tensorflow.keras.layers import GlobalMaxPooling1D, GlobalAveragePooling1D, Reshape, SpatialDropout1D\nfrom tensorflow.keras.preprocessing import text, sequence\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping","261f4689":"def build_model(embedding_matrix, params):\n    inputs = Input(shape=(params[\"input_size\"],))\n    x = Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1], weights=[embedding_matrix], trainable=False)(inputs)\n    x = SpatialDropout1D(0.2)(x)\n    x = Bidirectional(GRU(params[\"gru_units\"], return_sequences=True))(x)\n    x = Concatenate()([\n        GlobalMaxPooling1D()(x),\n        GlobalAveragePooling1D()(x)\n    ])\n    x = Dense(params[\"gru_units\"], activation=\"relu\")(x)\n    y1 = Dense(params[\"input_size\"], activation=\"softmax\", name=\"y1\")(x)\n    y2 = Dense(params[\"input_size\"], activation=\"softmax\", name=\"y2\")(x)\n    model = Model(inputs=inputs, outputs=[y1, y2])\n    return model","a4d98f5f":"params = {}\nparams[\"batch_size\"] = 128\nparams[\"gru_units\"] = 256\nparams[\"epochs\"] = 100\nparams[\"input_size\"] = 32\nparams[\"embed_dim\"] = 300\nparams[\"num_words\"] = 70000","eaed2a3e":"!ls ..\/input\/embeddings","93f7c81c":"def load_glove(word_index):\n    EMBEDDING_FILE = '..\/input\/embeddings\/glove-840B-300d.txt'\n    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n\n    all_embs = np.stack(embeddings_index.values())\n    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n    embed_size = all_embs.shape[1]\n\n    # word_index = tokenizer.word_index\n    nb_words = max_features= len(word_index)+1\n    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n    for word, i in word_index.items():\n        if i >= max_features: continue\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n            \n    return embedding_matrix \n    \ndef load_fasttext(word_index):    \n    EMBEDDING_FILE = '..\/input\/embeddings\/wiki-news-1M-300d.vec'\n    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if len(o)>100)\n\n    all_embs = np.stack(embeddings_index.values())\n    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n    embed_size = all_embs.shape[1]\n\n    # word_index = tokenizer.word_index\n    nb_words = max_features= len(word_index)+1\n    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n    for word, i in word_index.items():\n        if i >= max_features: continue\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n\n    return embedding_matrix","dd7bb6b5":"dataset_path = \"..\/input\/tweet-sentiment-extraction\/\"\ntrain_df, test_df = data_process(dataset_path)","54edf300":"## Data from train\ntrain_text = train_df[\"text\"].apply(lambda x: clean_data(x)).values\ntrain_selected_text = train_df[\"selected_text\"].apply(lambda x: clean_data(x)).values\ntrain_sentiment = train_df[\"sentiment\"].values","aea95c79":"## Data from test\ntest_text = test_df[\"text\"].apply(lambda x: clean_data(x)).values\ntest_sentiment = test_df[\"sentiment\"].values","aa894394":"##\ntrain_size = len(train_text)\ntest_size = len(test_text)\nprint(f\"Train data: {train_size} - Test data: {test_size}\")","8d49f644":"tokenizer = text.Tokenizer(num_words=params[\"num_words\"], filters=\"\")\ntotal_text = list(train_text) + list(test_text)\ntokenizer.fit_on_texts(total_text)","dad0795a":"total_train_len = len(train_text)\nx_train = []\ny1_train = []\ny2_train = []\n\nfor i in range(total_train_len):\n    text1 = train_text[i].strip()\n    text2 = train_selected_text[i].strip()\n\n    idx1 = text1.find(text2)\n    idx2 = idx1 + len(text2) - 1\n\n    x = tokenizer.texts_to_sequences([text1])\n    y1 = np.zeros((len(text1)))\n    y2 = np.zeros((len(text1)))\n\n    y1[idx1] = 1\n    y2[idx2] = 1\n    #y[idx1:idx2] = 1.0\n\n    # text3 = text1[idx1:idx2]\n\n    x = sequence.pad_sequences(x, maxlen=params[\"input_size\"])[0]\n    y1 = sequence.pad_sequences([y1], maxlen=params[\"input_size\"])[0]\n    y2 = sequence.pad_sequences([y2], maxlen=params[\"input_size\"])[0]\n\n    x_train.append(x)\n    y1_train.append(y1)\n    y2_train.append(y2)\n\n    # print(text2)\n    # print(text3)","15c4bc9d":"import time\nstart_time = time.time()\nword_index = tokenizer.word_index\nembedding_matrix_1 = load_glove(word_index)\nembedding_matrix_2 = load_fasttext(word_index)\n\ntotal_time = (time.time() - start_time)\/60.0\nprint(\"Took {0} minutes\".format(total_time))","34a4b8c4":"embedding_mean = np.mean([embedding_matrix_1, embedding_matrix_2], axis = 0)\ndel embedding_matrix_1\ndel embedding_matrix_2\n\nprint(np.shape(embedding_mean))","c35cc5e2":"model = build_model(embedding_mean, params)\nmodel.compile(loss=jaccard_distance, optimizer=\"adam\")\nmodel.summary()\n\nx_train = np.array(x_train)\ny1_train = np.array(y1_train)\ny2_train = np.array(y2_train)\n\ncallbacks = [\n    ReduceLROnPlateau(patience=5, monitor=\"val_loss\", factor=0.1),\n    EarlyStopping(patience=10, monitor=\"val_loss\")\n]\n\nmodel.fit(\n    x_train,\n    [y1_train, y2_train],\n    batch_size=params[\"batch_size\"],\n    epochs=params[\"epochs\"],\n    callbacks=callbacks\n)","75e04a9f":"test_x = tokenizer.texts_to_sequences(test_text)\ntest_x = sequence.pad_sequences(test_x, maxlen=params[\"input_size\"])\nprint(len(test_x))\ntest_y1, test_y2 = model.predict(test_x)\nprint(len(test_y1), len(test_y2))\n\nanswers = []\nfor i in range(len(test_text)):\n    start = np.argmax(test_y1[i])\n    end = np.argmax(test_y2[i])\n    ans = test_text[i][start:end]\n\n    regular_punct = list(string.punctuation)\n    for punc in regular_punct:\n        ans = ans.replace(f\" {punc} \", f\"{punc}\")\n    answers.append(ans)\n\nsubmission = pd.read_csv(f\"{dataset_path}\/sample_submission.csv\")\nsubmission['selected_text'] = answers\nsubmission.to_csv('submission.csv', index=False)","6bedb95d":"submission","d11d1847":"# DATA.PY","755fca94":"# EMBEDDINGS","673a7222":"# SIMPLE_GRU.PY","7a9735af":"# METRICS.PY"}}