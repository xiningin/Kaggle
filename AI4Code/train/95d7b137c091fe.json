{"cell_type":{"611efc11":"code","4d675a10":"code","71961688":"code","2e065318":"code","0e1514f1":"code","0c55917a":"code","81d187ff":"code","600168f2":"code","1f554bf8":"code","35239db4":"code","9b805475":"code","d9f0aea4":"code","1e7a8d77":"code","9efe9ba8":"code","79781f20":"code","66f5d1c4":"code","c11d341b":"code","1accb999":"code","38de1ee5":"markdown","b6737fd4":"markdown","24c2912b":"markdown","81e00629":"markdown","816344d1":"markdown","04f5fee9":"markdown","3a4ecd10":"markdown","6a9b718f":"markdown","b4d20fd1":"markdown","29313c0a":"markdown","6f5714a2":"markdown","f58d2f3d":"markdown","f14d4553":"markdown","0b320301":"markdown","692ebb8e":"markdown","dceeeabd":"markdown","eb23a143":"markdown","a1a2ea46":"markdown","4a5d3765":"markdown"},"source":{"611efc11":"# import librairies\nimport numpy as np\nimport pandas as pd\nimport time \nimport requests\nimport json\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\nplt.rcParams['figure.figsize'] = [16,13]","4d675a10":"root2017=\"\/kaggle\/input\/toronto-bikeshare-data\/bikeshare-ridership-2017\/\"\nroot2018=\"\/kaggle\/input\/toronto-bikeshare-data\/bikeshare2018\/\"\nfiles2017=[root2017+\"2017 Data\/Bikeshare Ridership (2017 Q1).csv\",\nroot2017+\"2017 Data\/Bikeshare Ridership (2017 Q2).csv\",\nroot2017+\"2017 Data\/Bikeshare Ridership (2017 Q3).csv\",\nroot2017+\"2017 Data\/Bikeshare Ridership (2017 Q4).csv\"]\nfiles2018=[root2018+\"bikeshare2018\/Bike Share Toronto Ridership_Q1 2018.csv\",\nroot2018+\"bikeshare2018\/Bike Share Toronto Ridership_Q2 2018.csv\",\nroot2018+\"bikeshare2018\/Bike Share Toronto Ridership_Q3 2018.csv\",\nroot2018+\"bikeshare2018\/Bike Share Toronto Ridership_Q4 2018.csv\"]","71961688":"# get the stations information from https:\/\/tor.publicbikesystem.net\nreq = requests.get('https:\/\/tor.publicbikesystem.net\/ube\/gbfs\/v1\/en\/station_information')\nstations = json.loads(req.content)['data']['stations']\nstations = pd.DataFrame(stations)[['station_id', 'name', 'lat', 'lon']].astype({\n    'station_id': 'float64',\n})\n\nstations.head()","2e065318":"# load the data\n\nfirst=True\n\nfor f in files2018:\n    start_time = time.time()\n    data = pd.read_csv(f,parse_dates = ['trip_start_time'],dayfirst=True)\n    if first:\n        df=data\n        first=False\n    else:\n        df=df.append(data,sort=True)\n    print(\"file {} -- {} seconds --\".format(f,time.time() - start_time))        \ndel data , first\n","0e1514f1":"# Hour of the day\ndf['hour'] = df['trip_start_time'].dt.hour\n# Day of the week\ndf['dayofweek'] = df['trip_start_time'].dt.dayofweek\n# Date\ndf['date'] = df['trip_start_time'].dt.floor('d')\n# 2 stations in the dataset are not in the list of available stations, so map to the closest\ndf.loc[df['to_station_id']==7068,'to_station_id']=7399\ndf.loc[df['to_station_id']==7219,'to_station_id']=7148\ndf.loc[df['from_station_id']==7068,'from_station_id']=7399\ndf.loc[df['from_station_id']==7219,'from_station_id']=7148","0c55917a":"df.head()","81d187ff":"from sklearn.cluster import KMeans\nX = stations[['lon', 'lat']].values\nn_clusters_ = 20\nkmeans = KMeans(n_clusters = n_clusters_, init ='k-means++')\nkmeans.fit(X) # Compute k-means clustering.\nlabels = kmeans.fit_predict(X)","600168f2":"# Coordinates of cluster centers.\ncenters = kmeans.cluster_centers_ \n\n# map each station with its region\nstations['clusters']=labels\nstations.index=stations['station_id']\ncluster_map = stations['clusters'].to_dict()\ndf['from_station_c']=df['from_station_id'].map(cluster_map)\ndf['to_station_c']=df['to_station_id'].map(cluster_map)","1f554bf8":"import matplotlib.image as mpimg\nunique_labels = set(labels)\ncolors = [plt.cm.Spectral(each)\n          for each in np.linspace(0, 1, len(unique_labels))]\nBBox = (-79.4900,-79.2700,43.6100,43.7200)\n# map_tor = mpimg.imread('map.png')\nfig, ax = plt.subplots()\nfor k, col in zip(unique_labels, colors):\n    if k == -1:\n        # Black used for noise.\n        col = [0, 0, 0, 1]\n\n    class_member_mask = (labels == k)\n\n    xy = X[class_member_mask]\n    ax.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n             markeredgecolor='k', markersize=5)\n    ax.plot(centers[k, 0], centers[k, 1], '^',markerfacecolor=tuple(col), markersize=18)\n    ax.text(centers[k, 0], centers[k, 1], k,fontsize=12)\n# ax.set_xlim(BBox[0],BBox[1])\n# ax.set_ylim(BBox[2],BBox[3])\n# ax.imshow(map_tor, zorder=0, extent = BBox, aspect= 'equal')\nplt.show()","35239db4":"##############################\n# Count edges\n##############################\n\n\ndf['region_count']=(df['from_station_c']<df['to_station_c']).astype(int)-(df['from_station_c']>df['to_station_c']).astype(int)\ndf['region_key'] = df[['from_station_c','to_station_c']].min(axis=1).astype('str') +'_'+ df[['from_station_c','to_station_c']].max(axis=1).astype('str')\n\ndf['station_count']=(df['from_station_id']<df['to_station_id']).astype(int)-(df['from_station_id']>df['to_station_id']).astype(int)\ndf['station_key'] = df[['from_station_id','to_station_id']].min(axis=1).astype('str') +'_'+ df[['from_station_id','to_station_id']].max(axis=1).astype('str')\n\ndf.describe(include=['O'])\ndf.columns\n\n\n","9b805475":"##############################\n# Extract flows and create graph\n##############################\n\ndef getFlows(region=True,start_date=pd.Timestamp('2018-01-01'),end_date=pd.Timestamp('2019-01-01'),days = [0,1,2,3,4,5,6]):\n    # flows\n    if region:\n        flows = df.loc[(df['from_station_c']!=df['to_station_c'])&(df['dayofweek'].isin(days)) & (df['date']>=start_date) & (df['date']<end_date) ].groupby(['region_key'])['region_count'].sum()\n    else:\n        flows = df.loc[(df['from_station_id']!=df['to_station_id'])&(df['dayofweek'].isin(days)) & (df['date']>=start_date) & (df['date']<end_date) ].groupby(['station_key'])['station_count'].sum()\n    # graph\n    G = nx.DiGraph()\n    for index, value in flows.items():\n        G.add_weighted_edges_from([(int(index.split('_')[0]), int(index.split('_')[1]), value)])\n    # degrees of each nodes\n    nodes=[]\n    degrees=[]\n    for e in G:\n        nodes.append(e)\n        degrees.append(G.in_degree(e, weight='weight')-G.out_degree(e, weight='weight'))\n    deg = pd.Series(data=degrees,index=nodes).sort_values()\n    return flows,G,deg\n\nf_stations_w, g_stations_w , d_stations_w = getFlows(region=False,start_date=pd.Timestamp('2018-01-01'),end_date=pd.Timestamp('2019-01-01'),days = [0,1,2,3,4])\nf_stations_we, g_stations_we , d_stations_we = getFlows(region=False,start_date=pd.Timestamp('2018-01-01'),end_date=pd.Timestamp('2019-01-01'),days = [5,6])\nf_regions_w, g_regions_w , d_regions_w = getFlows(region=True,start_date=pd.Timestamp('2018-01-01'),end_date=pd.Timestamp('2019-01-01'),days = [0,1,2,3,4])\nf_regions_we, g_regions_we , d_regions_we = getFlows(region=True,start_date=pd.Timestamp('2018-01-01'),end_date=pd.Timestamp('2019-01-01'),days = [5,6])\n","d9f0aea4":"top10 = pd.DataFrame(np.vstack([d_stations_w.iloc[:10].index.values,d_stations_w.iloc[-10:].index.values,d_stations_we.iloc[:10].index.values,d_stations_we.iloc[-10:].index.values]).transpose()\n                                     ,columns=[\"Negative_Week\",\"Positive_Week\",\"Negative_Weekend\",\"Positive_Weekend\"])\ntop10[\"Negative_Week\"] = top10[\"Negative_Week\"].map(stations['name'])\ntop10[\"Positive_Week\"] = top10[\"Positive_Week\"].map(stations['name'])\ntop10[\"Negative_Weekend\"] = top10[\"Negative_Weekend\"].map(stations['name'])\ntop10[\"Positive_Weekend\"] = top10[\"Positive_Weekend\"].map(stations['name'])\n\ntop10.head(10)","1e7a8d77":"############################################################\n# Plot the graphs\n############################################################\ndef plot_G_stations(G,degrees,ptitle=\"\"):\n    colors=[]\n    pos={}\n    node_sizes=[]\n    max_deg = degrees.abs().max()\n    labels={}\n    for e in G:\n        d=degrees[e]\n        node_sizes.append(100 + 500*(abs(d)\/max_deg))\n        pos[int(e)]=(stations.loc[e,'lon'],stations.loc[e,'lat'])\n        labels[int(e)]=(int(d))\n        if d==0:\n            colors.append('#00aedb')\n        elif d < 0:\n            colors.append('#f37735')\n        else: \n            colors.append('#00b159')\n    nx.draw_networkx_nodes(G,pos, node_size=node_sizes,node_color=colors)\n    nx.draw_networkx_labels(G, pos,labels= labels, font_size=7)\n    plt.title(ptitle)\n    plt.show()\n\n\n\ndef plot_G_regions(G,flows,degrees,ptitle=\"\"):\n    colors=[]\n    pos={}\n    max_deg = degrees.abs().max()\n    node_sizes=[]\n    labels={}\n    for e in G:\n        d=degrees[e]\n        node_sizes.append(100 + 600*(abs(d)\/max_deg))\n        pos[int(e)]=(centers[e,0],centers[e,1])\n        labels[int(e)]=(int(d))\n        if d==0:\n            colors.append('#00aedb')\n        elif d < 0:\n            colors.append('#f37735')\n        else: \n            colors.append('#00b159')\n\n    color_cut_pos = flows[flows>0].median()\n    color_cut_neg = flows[flows<0].median()\n\n    elargepos = [(u, v) for (u, v, d) in G.edges(data=True) if d['weight'] >color_cut_pos]\n    elargeneg = [(u, v) for (u, v, d) in G.edges(data=True) if d['weight'] < color_cut_neg]\n    eother = [(u, v) for (u, v, d) in G.edges(data=True) if d['weight'] >= color_cut_neg and d['weight'] <= color_cut_pos]\n\n    # nodes\n    nx.draw_networkx_nodes(G, pos, node_size=node_sizes,node_color=colors,alpha=0.9)\n\n    # edges\n    nx.draw_networkx_edges(G, pos, edgelist=elargepos,edge_color='#00b159', width=4,alpha=0.7)\n    nx.draw_networkx_edges(G, pos, edgelist=elargeneg,edge_color='#f37735', width=4,alpha=0.7)\n    nx.draw_networkx_edges(G, pos, edgelist=eother,edge_color='#00aedb', width=1,alpha=0.5)\n\n    # labels\n    nx.draw_networkx_labels(G, pos, labels= labels,font_size=14)\n    plt.title(ptitle)\n    plt.show()","9efe9ba8":"plot_G_regions(g_regions_w,f_regions_w,d_regions_w)","79781f20":"plot_G_regions(g_regions_we,f_regions_we,d_regions_we)","66f5d1c4":"plot_G_stations(g_stations_w,  d_stations_w)","c11d341b":"plot_G_stations(g_stations_we,  d_stations_we)","1accb999":"def getAverageFlowsByDays(region=True,start_date=pd.Timestamp('2018-01-01'),end_date=pd.Timestamp('2019-01-01')):\n    \n    f_list=[]\n    d_list=[]\n    g_list=[]\n    # flows\n    if region:\n        flows = df.loc[(df['from_station_c']!=df['to_station_c'])& (df['date']>=start_date) & (df['date']<end_date) ].groupby(['region_key','date'],as_index=False)['region_count'].sum()\n        flows['dayofweek']=flows['date'].dt.dayofweek\n        flows = flows.groupby(['region_key','dayofweek'],as_index=False)['region_count'].mean()\n    else:\n        flows = df.loc[(df['from_station_id']!=df['to_station_id'])& (df['date']>=start_date) & (df['date']<end_date) ].groupby(['station_key','date'],as_index=False)['station_count'].sum()\n        flows['dayofweek']=flows['date'].dt.dayofweek\n        flows = flows.groupby(['station_key','dayofweek'],as_index=False)['station_count'].mean()\n    \n    for day in range(0,7):\n        f=flows.loc[flows['dayofweek']==day]       \n        if region:\n            f.index = f['region_key']\n            f=f['region_count']\n        else:\n            f.index = f['station_key']\n            f=f['station_count']\n        f_list.append(f)\n        # graph\n        G = nx.DiGraph()\n        for index, value in f.items():\n            G.add_weighted_edges_from([(int(index.split('_')[0]), int(index.split('_')[1]), value)])\n        g_list.append(G)\n        # degrees of each nodes\n        nodes=[]\n        degrees=[]\n        for e in G:\n            nodes.append(e)\n            degrees.append(G.in_degree(e, weight='weight')-G.out_degree(e, weight='weight'))\n        deg = pd.Series(data=degrees,index=nodes).sort_values()\n        d_list.append(deg)\n    return f_list,g_list,d_list\n\nf_stations, g_stations , d_stations = getAverageFlowsByDays(region=False,start_date=pd.Timestamp('2018-01-01'),end_date=pd.Timestamp('2019-01-01'))\nf_regions, g_regions , d_regions = getAverageFlowsByDays(region=True,start_date=pd.Timestamp('2018-01-01'),end_date=pd.Timestamp('2019-01-01'))\n\n\nweekday={0:'Monday',1:'Tuesday',2:'Wednesday',3:'Thursday',4:'Friday',5:'Saturday',6:'Sunday'}\n\nfor i in range(0,7):\n    plot_G_regions(g_regions[i], f_regions[i] , d_regions[i],\"Region flows - {}\".format(weekday.get(i)))\n    plot_G_stations(g_stations[i],  d_stations[i],\"Station balance - {}\".format(weekday.get(i)))\n\n","38de1ee5":"# Load the data\n> <span>&#171;<\/span>This can be a little bit long, because of the date parsing of the column 'trip_start_time'<span>&#187;<\/span>","b6737fd4":"## For the year 2018, flows between regions for the weekdays","24c2912b":"<div class=\"alert alert-block alert-info\"><b><span>&#171;<\/span> to be continue ... <span>&#187;<\/span><\/b><\/div>\nTo do : give more explanation, try to explore graph with algorithms (find components ...)\n<div class=\"alert alert-block alert-info\"><b><span>&#171;<\/span> please don't forget to upvote, that will keep me motivated <span>&#187;<\/span><\/b><\/div> \n","81e00629":"### Map stations with its cluster","816344d1":"# Top 10 stations in negative \/ positive over the year 2018","04f5fee9":"# Count edges","3a4ecd10":"# Average flows by day","6a9b718f":"![map2.png](attachment:map2.png)","b4d20fd1":"# Clustering of the stations in 20 regions\n\n> <span>&#171;<\/span>There are 463 stations, so the idea is to group them in order to have 20 regions <br\/> this will be easier to see the flow of bikes between these 20 regions <span>&#187;<\/span>\n","29313c0a":"# Plot over a map\n<div class=\"alert alert-block alert-warning\">\nQuestion : how can I load the image 'map.png' as input in order to execute this code ?\n<\/div>\n\nTo do this, the following line have to be uncomment in the code above :\n```\n# map_tor = mpimg.imread('map.png')\n...\n# ax.set_xlim(BBox[0],BBox[1])\n# ax.set_ylim(BBox[2],BBox[3])\n# ax.imshow(map_tor, zorder=0, extent = BBox, aspect= 'equal')\n```\n","6f5714a2":"## For the year 2018, negative\/positive stations during the weekdays","f58d2f3d":"### See the clusters","f14d4553":"# Flows for the year 2018","0b320301":"## Cleaning and columns mapping","692ebb8e":"# How to see the flows of bikes in the Bike Share Toronto Service\n\n**Does bike share go round trip or one way?**\n\n>The purpose of this notebook is to analyse the flow of bikes. Are bikes returned to its initial location? Or are there flows of bike from one region to another?\n\nThe sum of all the travels for each station for one year are too noisy, so \n* Data can be split between weekend days or not\n* Stations can be grouped by proximity in order to define regions\n* Data can be summarized by period of time ( Day, Week , Month, Year)\n* Focus on 2018, that should be enough representative\n\nGraph representation :\n* A node is a station or a region\n* A edge is the sum of all travel between 2 nodes during a period of time\n* For each node we consider the balance between outgoing and incoming flows\n* To simplify : a travel from one station to the same station is not taken in account. (balance is zero)\n* To simplify : the inverse edges are grouped as follow, (10 travels from 1 to 2 and 12 travels from 2 to 1, gives  -2 travels from 1 to 2) (5 travels from 1 to 2 and 4 travels from 2 to 1, gives  +1 travels from 1 to 2) \n\nThe plan\n\n* [Get the stations information](#station_info)\n* [Load the data](#load_data)\n* [Clustering of the stations in 20 regions](#clustering_region)\n","dceeeabd":"## For the year 2018, negative\/positive stations during the weekends","eb23a143":"<a id=\"station_info\"><\/a>\n# Get the stations information (gps coordinates) from https:\/\/tor.publicbikesystem.net","a1a2ea46":"## For the year 2018, flows between regions during the weekends","4a5d3765":"### KMeans on gps coordinates (nbr_cluster = 20)"}}