{"cell_type":{"6a54054b":"code","7bb9826c":"code","6275ead1":"code","80a39344":"code","82a90dfd":"code","4e3f6463":"code","2b4005f5":"code","bbc6757b":"code","db3e65b2":"code","a966435b":"code","c827a36b":"markdown","360c5948":"markdown","bad96fba":"markdown","82904a39":"markdown","f4c7cd2d":"markdown","f1149a12":"markdown","2b1b803d":"markdown","9543ace5":"markdown"},"source":{"6a54054b":"!pip install trimap --quiet | cat","7bb9826c":"from typing import List\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objects as go\n\nimport trimap\n\n# Read the data\ntrain = pd.read_csv('\/kaggle\/input\/jane-street-market-prediction\/train.csv')\nprint(f\">> Done loading data. Shape: {train.shape}\")\n\n# Subset the data for processing speed\ntrain = train.fillna(0).sample(frac=0.1, random_state=2020)\n\n# Select all features, except feature_0\nFEATURES = [c for c in train.columns if 'feature' in c and c != 'feature_0']\n\n# Scale the features in the data\nmeans = np.nanmean(train[FEATURES], axis=0)\nstds = np.nanstd(train[FEATURES], axis=0)\ntrain[FEATURES] = (train[FEATURES] - means) \/ stds\nprint(\">> Done scaling data\")\n\n# Split up the data\nX = train[FEATURES]\ny = train['feature_0']","6275ead1":"# Create TriMap embeddings\ntrimap_embedding = trimap.TRIMAP(\n  n_iters=1000,\n  apply_pca=True,\n  weight_adj=50\n).fit_transform(X.values)\nprint(\">> Done with TriMap embeddings\")\n\n# Put embeddings into dataframe\nembedding_df = pd.DataFrame({\n    'Component 1': trimap_embedding[:, 0], \n    'Component 2': trimap_embedding[:, 1],\n    'Feature 0': train.feature_0.astype(str),\n})\n\n# Visualize the embeddings in 2D plot\nfig = px.scatter(\n    embedding_df, \n    x='Component 1', y='Component 2', \n    color='Feature 0',\n    opacity=0.8\n)\nfig.update_traces(\n    marker=dict(size=5, line=dict(width=1, color='DarkSlateGrey')),\n    selector=dict(mode='markers')\n)    \nfig.update_layout(title={\n    'text': f'TriMAP Embedding on Features 1-{len(FEATURES)+1}',\n    'x':0.5,\n    'xanchor': 'center',\n    'yanchor': 'top'}\n)\nfig.show()","80a39344":"import lightgbm as lgb\nfrom sklearn.model_selection import cross_val_score\n\n# Reset X & y, just in case I'm running this cell multiple times\n# Only run on small subset for processing speed (I checked with \n# larger sample, and results are the same)\nsubset = train.sample(2000, random_state=2020)\nX = subset[FEATURES]\ny = subset['feature_0']\n\n# Iteratively remove most important feature & measure AUC score\ni, current_auc = 0, 1.0\nhistory = []\nwhile current_auc > 0.5 and len(X.columns) > 1:\n    \n    # Calculate CV scores\n    model = lgb.LGBMRegressor(n_estimators=100, n_jobs=1)\n    scores = cross_val_score(model, X, y, scoring='roc_auc', n_jobs=-1)\n    current_auc = scores.mean()\n    \n    # Get the most important feature for the model\n    model = lgb.LGBMRegressor(n_estimators=100)\n    model.fit(X, y)\n\n    # Remove the best column\n    best_column = X.columns[np.argmax(model.feature_importances_)]\n    history.append((f'Iteration {i} - {best_column}', scores.mean()))\n    X = X.drop([best_column], axis=1)\n    \n    #print(f\">> Current AUC: {current_auc:.2f} -> Removing: {best_column}. X.shape: {X.shape}\")\n    i += 1\n    \n# Put history into a dataframe\nresult = pd.DataFrame(history, columns=['Removed Feature', 'Resulting AUC'])\n\n# Create a plot \nfig = px.bar(result, x='Removed Feature', y='Resulting AUC', title='AUC as a function of removed best feature')\n\n# Rotate tick labels & set y-axis\nfig.update_layout(xaxis=dict(tickangle=-90))   \nfig.update_yaxes(range=[0.45, 1])","82a90dfd":"import lightgbm as lgb\nfrom sklearn.model_selection import cross_val_score\n\n# Reset X & y, just in case I'm running this cell multiple times\n# Only run on small subset for processing speed (I checked with \n# larger sample, and results are the same)\nsubset = train.sample(5000, random_state=2020)\ny = subset['feature_0']\n\n# Go through each feature & fit a model for feature_0\nhistory = []\nfor i, feature in enumerate(FEATURES):\n    \n    # Define X as the single feature \n    X = subset[[feature]]\n    \n    # Calculate CV scores\n    model = lgb.LGBMRegressor(n_estimators=100, n_jobs=1)\n    scores = cross_val_score(model, X, y, scoring='roc_auc', n_jobs=-1)\n    history.append((i+1, scores.mean()))\n    \n# Put history into a dataframe\nresult = pd.DataFrame(history, columns=['Feature ID', 'ROC AUC'])\n\n# Create a plot \nfig = px.bar(\n    result, \n    x='Feature ID', \n    y='ROC AUC', \n    title='Predicting feature_0 using solo other features'\n)\n\n# Rotate tick labels & set y-axis\nfig.update_layout(xaxis=dict(tickangle=-90))   \nfig.update_yaxes(range=[0.45, 1])","4e3f6463":"# Get all features with an individual score < 0.6\nbad_features = [f'feature_{i}' for i, auc in history if auc < 0.6]\n\n# Get the subset of data with these \"poor\" features\nX = subset[bad_features]\n    \n# Calculate a CV score\nmodel = lgb.LGBMRegressor(n_estimators=100, n_jobs=1)\nscore = cross_val_score(model, X, y, scoring='roc_auc', n_jobs=-1)\nprint(f\">> AUC score of poor features combined: {score.mean():.2f}\")","2b4005f5":"def plotFeatureSplits(df: pd.DataFrame, feature_list: List[int]) -> None:\n    for i in feature_list:\n\n        # Create a plot with original timeseries, and split by feature 0\n        _, axes = plt.subplots(1, 2, figsize=(20, 5))\n\n        # Original timeseries\n        df[f'feature_{i}'].plot(ax=axes[0])\n        axes[0].set_title(f'Feature {i}')\n        axes[0].set_ylabel(f'Feature {i}')\n        axes[0].set_xlabel(f'Trade ID')\n\n        # Plot by feature 0 split\n        df.groupby('feature_0')[f'feature_{i}'].plot(ax=axes[1])\n        axes[1].set_title(f'Feature {i}, split by feature_0')\n        axes[1].set_ylabel(f'Feature {i}')\n        axes[1].set_xlabel(f'Trade ID')\n\n        # Show figure with legend\n        plt.legend()    \n        plt.show()\n        \n# Get the first 10k rows, which have not be\nordered_subset = pd.read_csv('\/kaggle\/input\/jane-street-market-prediction\/train.csv', nrows=10000)","bbc6757b":"# Show pattern 1, features 17-40\nplotFeatureSplits(ordered_subset, np.arange(17, 41))","db3e65b2":"# Show pattern 2, features 65-68\nplotFeatureSplits(ordered_subset, np.arange(65, 69))","a966435b":"# Show pattern 3\nplotFeatureSplits(ordered_subset, np.arange(72,107))","c827a36b":"**My Take:** As expected the features 1-130 are split into two different distributions, and indeed these match perfectly with `feature_0`. Whether that indicates bid\/ask, long\/short, or something else I don't know enough about finance to say, but it's clear that the value of `feature_0` influences the values of some if not all other features in the dataset.\n\n## Experiment 2: Features related with Feature 0\nClearly `feature_0` splits up the rest of the features in two data distributions, but the plot only tells so much. To investigate a little further what effect `feature_0` has on the other features, I here try to solve the reverse problem of predicting `feature_0` based on features 1-130. I'll then remove the single most important feature (based off feature importance), and again check how well the remaining features can predict `feature_0`. By iteratively removing the most important feature, we can get a sense of how many features are required for predicting `feature_0`, and thereby how many features are 'related' to `feature_0`.\n\nThe reason for this approach rather than just looking at correlations between `feature_0` and `feature_x` is that we also want to consider possible non-linear effects, i.e. `feature_i` through `feature_j` together might in conjunction with each other be able to predict `feature_0`, but not by themselves.","360c5948":"This also makes sense, it's clear that some features such as 73 split the data into plus\/minus almost perfectly, while others are a lot more subtle.","bad96fba":"**My take:** We can remove *a lot* of the most important features, and still be able to perfectly classify `feature_0` - it is not just a distribution of a single feature that is dependent on the value of `feature_0`; rather it is the distribution of a lot of the features that is linked to the value of `feature_0`.\n\n## Experiment 3: Relation with each other feature\nIn experiment 2 I iteratively removed the most predictive feature for `feature_0` from the other features to get an idea about the minimal set of features that together are related to `feature_0`. In this experiment I'll do it the other way around and see which features by themself are enough to predict the value of `feature_0`.","82904a39":"**My take:** This looks very interesting - following patterns occur:\n\n* Pattern 1: Features *17 through 40* are extremely good at separating `feature_0`. Looking at the tags file, this does not seem to match clearly any of the given tags :\/ \n\n* Pattern 2: Features 65-68 seem to have some relation to `feature_0`. None of these feature have any tag.\n\n* Pattern 3: Features 72-106 seemt to have a repeating pattern of relation to `feature_0` which matches the the frequency (6) of a similar pattern seen in 6 tags for the same feature (tags 0-5), however in the tags file the pattern extends all the way to feature 118, so it's not a complete match. \n\n* Pattern 4: Even though the figure of model scores based on individual features indicate that some features by themselves are not related to `feature_0`, when all these \"poor\" features are combined, they are able to classify `feature_0` very well, indicating that the value of `feature_0` truly influences the distribution of a lot of the other features.\n\n## Experiment 4: Inspecting feature patterns\nWith the different patterns identified above, let's look at the actual values of the features as split by `feature_0`, to see if we can actually observe the differences in distribution.\n\n### Pattern 1 - Good Predictors","f4c7cd2d":"# Feature-0 Exploration\nAs [mentioned previously](https:\/\/www.kaggle.com\/c\/jane-street-market-prediction\/discussion\/199462), feature 0 seems to be somewhat different from the other features, in that it's a binary indicator of something. At the same time, [another notebook](https:\/\/www.kaggle.com\/hkailee\/eda-lung-shape-umap-clusters-comparison) of the data showed a lung-shaped UMAP of all the features, indicating two major \"lungs\"\/clusters, with potentially some internal structure. \n\n## Experiment 1: TriMap of features 1-130\nWhat I wanted to check out below is whether if we run dimensionality reduction on all features **except feature 0**, in order to find the two major clusters as from the lung UMAP, will those two clusters correspond to the values of feature 0. I'll use TriMAP instead of UMAP for my investigation, but I expect we'll see similar clusters in the TriMAP space.","f1149a12":"Cool - makes sense that these features split the data perfectly. Seems like an opportunity for some feature engineering here as well!\n\n### Pattern 2 - Weak Predictors","2b1b803d":"For good measure, let's see what the AUC is of a model where we use all features with a score < 0.6 when used by themselves.","9543ace5":"Again this makes sense, seems like `feature_0 = 1` is generally a bit higher than the signal for `feature_0 = -1`.\n\n### Pattern 3 - Various strength predictors"}}