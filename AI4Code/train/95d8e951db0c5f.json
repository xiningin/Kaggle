{"cell_type":{"4e8fd0de":"code","848e2c35":"code","029ca653":"code","b532ffa2":"code","e8892355":"code","29e5688a":"code","f41c5ab5":"code","329d50f1":"code","f9e1e7db":"code","a9f919b0":"code","797f72db":"code","fedfdfca":"code","ffb77910":"code","075bb609":"code","b9229e98":"code","49dbca4a":"code","4c0c3d0f":"code","fb0482f1":"code","a3631a74":"code","2d01d21b":"code","d11b853c":"code","0335276a":"code","258a892a":"code","54b91e7a":"code","fec147f2":"code","4155a0ab":"code","80e4267f":"code","23dc14f1":"code","1d3dc1e2":"code","f23e330a":"code","bc815efb":"code","d786ab5c":"code","3ab46096":"code","8716f9ea":"code","292e44ea":"code","5c3f13ba":"code","50f4041d":"code","6f203881":"code","49ad9d09":"code","f01e8db7":"code","51c97212":"code","a203f3d6":"code","70acb7aa":"code","f33ced85":"code","032f3103":"code","f143c476":"code","55daf84b":"code","bca8bba1":"code","b2ad57c1":"code","48d269ab":"code","bd3d2897":"code","114f96d0":"code","dbeace11":"code","e7b20dea":"code","d0dc3396":"code","649c7c47":"code","4472d0f1":"code","012baba7":"code","6aae4a91":"code","6775b2e0":"code","bfd7af5d":"code","d72e6299":"code","c7452091":"code","2eb75cca":"code","1e6d868e":"code","c384ccd5":"code","0983f01a":"code","35da7777":"code","a305d05f":"code","5e7fa1df":"code","79840566":"code","3f2f246b":"code","8aace625":"code","395cc784":"code","995c4009":"code","a8bc5f4b":"code","5476fcef":"code","942ec1b3":"code","ae51ae89":"code","9ff266fd":"markdown","558602e4":"markdown","266c8fa2":"markdown","f7796f28":"markdown","bb5a4972":"markdown","5674038a":"markdown","7a619da2":"markdown","d30eba42":"markdown","083725c1":"markdown","6aede2ef":"markdown","55b0f7ad":"markdown","9e614c05":"markdown","fba3eba7":"markdown","5727decb":"markdown","b98034e7":"markdown","6f92976c":"markdown","ea1868d2":"markdown","0195cb40":"markdown","280a456c":"markdown","658e2e75":"markdown","64ddf79e":"markdown","027f37c5":"markdown"},"source":{"4e8fd0de":"from IPython.display import Image\nImage(filename=\"..\/input\/architectures\/Ynet.png\")","848e2c35":"Image(filename=\"..\/input\/architectures\/Wnet.png\")","029ca653":"import os\nimport numpy as np\nimport pandas as pd\nimport PIL\nfrom PIL import Image\nimport keras\nfrom keras import layers\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\nimport warnings\n#warnings.filterwarnings(action=\"ignore\")","b532ffa2":"img = Image.open(\"..\/input\/lgg-mri-segmentation\/kaggle_3m\/TCGA_CS_5396_20010302\/TCGA_CS_5396_20010302_14.tif\")\nmask = Image.open(\"..\/input\/lgg-mri-segmentation\/kaggle_3m\/TCGA_CS_5396_20010302\/TCGA_CS_5396_20010302_14_mask.tif\")\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10,5))\naxes[0].imshow(img)\naxes[1].imshow(mask,cmap=\"binary_r\")\nplt.show()","e8892355":"df = pd.read_csv(\"..\/input\/lgg-mri-segmentation\/kaggle_3m\/data.csv\")","29e5688a":"df.info()","f41c5ab5":"df.describe()","329d50f1":"df.head()","f9e1e7db":"df.isna().sum()","a9f919b0":"df.shape","797f72db":"fig, axes = plt.subplots(nrows=17, ncols=1, figsize=(20,150))\nfor i in range(1,18):\n    col = df.columns[i]\n    count = sns.countplot(x=col, data=df, color=\"mediumspringgreen\", ax=axes[i-1])\n    axes[i-1].set_title(col, fontsize=20)\n    axes[i-1].set_xlabel(None)\nplt.show()","fedfdfca":"df.drop(['tumor_tissue_site'], axis=1, inplace=True)","ffb77910":"for i in range(df.shape[0]):\n    if pd.isnull(df[\"RNASeqCluster\"][i]):\n        impval = df[\"RNASeqCluster\"].sample(1).iloc[0]\n        while(pd.isnull(impval)):\n            impval = df[\"RNASeqCluster\"].sample(1).iloc[0]\n        df[\"RNASeqCluster\"][i] = impval\n    if pd.isnull(df[\"RPPACluster\"][i]):\n        impval = df[\"RPPACluster\"].sample(1).iloc[0]\n        while(pd.isnull(impval)):\n            impval = df[\"RPPACluster\"].sample(1).iloc[0]\n        df[\"RPPACluster\"][i] = impval","075bb609":"filler = {}\nfor col in df.columns[1:]:\n    filler[col] = df[col].mode().iloc[0]","b9229e98":"filler","49dbca4a":"df.fillna(filler, inplace=True)","4c0c3d0f":"df.isnull().sum()","fb0482f1":"ohcols = [\"gender\", \"ethnicity\", \"race\", \"death01\"]","a3631a74":"for col in ohcols:\n    df[col] = pd.get_dummies(df.loc[:,col], drop_first=True)","2d01d21b":"df.head()","d11b853c":"from sklearn.preprocessing import RobustScaler","0335276a":"rs = RobustScaler()","258a892a":"arraydata = rs.fit_transform(df.drop(\"Patient\", axis=1, inplace=False))","54b91e7a":"arraydata.shape","fec147f2":"path = \"..\/input\/lgg-mri-segmentation\/kaggle_3m\"","4155a0ab":"folderlist = os.listdir(\"..\/input\/lgg-mri-segmentation\/kaggle_3m\")","80e4267f":"folderlist.remove('data.csv')\nfolderlist.remove(\"README.md\")","23dc14f1":"len(folderlist)","1d3dc1e2":"number_of_images = 0\nfor folder in folderlist:\n    number_of_images += len(os.listdir(os.path.join(path, folder)))\nprint(number_of_images)","f23e330a":"missing_masks = 0\nfor folder in folderlist:\n    folderpath = os.path.join(path, folder)\n    filelist = os.listdir(folderpath)\n    for file in filelist:\n        if file[:len(file)-4][-1].isnumeric():\n            if file[:len(file)-4]+'_mask.tif' not in filelist:\n                missing_masks += 1\n                print(f\"Image without mask: {os.path.join(folderpath, file)}\")\nprint(f\"Total number of missing masks: {missing_masks}\")","bc815efb":"df1 = pd.DataFrame([[1, 2], [3, 4]], columns=list('AB'))\ndf2 = pd.DataFrame([[5, 6], [7, 8]], columns=list('AB'))\ndf1.append(df2, ignore_index=True)","d786ab5c":"class DataGenerator(keras.utils.Sequence):\n    \n    def __init__(self, path, image_paths_list, df, y, processing = None, batch_size = 64, input_size = (256,256), tabular=False):\n        self.path = path\n        self.image_paths_list = image_paths_list\n        self.df=df\n        self.y = y\n        self.processing = processing\n        self.batch_size = batch_size\n        self.index = 0\n        self.input_size = input_size\n        self.n = len(self.image_paths_list)\n        self.tabular=tabular\n        \n    def __len__(self):\n        return self.n\/\/self.batch_size + 1\n    \n    def __getitem__(self, index):\n        \"\"\"print(\"\\n\")\n        print(f\"self.index: {self.index}\")\n        print(f\"self.n: {self.n}\")\n        print(\"*****\")\"\"\"\n        if self.index == self.n:\n            self.index=0\n        index = self.index\n        lines = pd.DataFrame()\n        if self.n - 1 >= index + self.batch_size:\n            indices = range(index, index + self.batch_size)\n            images = np.empty(shape = (self.batch_size, self.input_size[0], self.input_size[1], 3), dtype=int)\n            masks = np.empty(shape = (self.batch_size, self.input_size[0], self.input_size[1]), dtype=float)\n        else:\n            indices = range(index, self.n)\n            images = np.empty(shape = (self.batch_size, self.input_size[0], self.input_size[1], 3), dtype=int)\n            masks = np.empty(shape = (self.batch_size, self.input_size[0], self.input_size[1]), dtype=float)\n        batchindex = 0\n        for i in indices:\n            patient = self.image_paths_list[i][:12]\n            line = self.df[self.df[self.y]==patient]\n            name = os.path.join(path, self.image_paths_list[i][:21], self.image_paths_list[i])\n            img = np.array(Image.open(name).resize(self.input_size))\n            mask = np.array(Image.open(name[:len(name)-4]+\"_mask.tif\").resize(self.input_size))\n            if self.processing != None:\n                img = self.processing(img)\n            images[batchindex, :] = img\n            masks[batchindex, :] = mask\n            lines=lines.append(line, ignore_index=True)\n            \"\"\"fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(20,35))\n            axes[0,0].imshow(img)\n            axes[0,1].imshow(mask)\n            axes[1,0].imshow(images[batchindex,:])\n            axes[1,1].imshow(masks[batchindex, :])\"\"\"\n            batchindex += 1\n        for bi in range(batchindex, self.batch_size):\n            smolname=random.sample(self.image_paths_list,k=1)[0]\n            ind=self.image_paths_list.index(smolname)\n            name=os.path.join(path, self.image_paths_list[ind][:21], smolname)\n            patient = self.image_paths_list[ind][:12]\n            line = self.df[self.df[self.y]==patient]\n            img = np.array(Image.open(name).resize(self.input_size))\n            mask = np.array(Image.open(name[:len(name)-4]+\"_mask.tif\").resize(self.input_size))\n            if self.processing != None:\n                img = self.processing(img)\n            images[bi, :] = img\n            masks[bi, :] = mask\n            lines=lines.append(line, ignore_index=True)\n        self.index += batchindex\n        lines.drop(\"Patient\",axis=1,inplace=True)\n        \"\"\"fig, axes = plt.subplots(nrows=6, ncols=2, figsize=(20,120))\n        for i in range(6):\n            axes[i,0] = images[i,:]\n            axes[i,1] = masks[i,:]\"\"\"\n        \"\"\"print(\"*****\")\n        print(f\"index= {index}\")\n        print(f\"indices: {len(indices)}\")\n        print(f\"images: {images.shape}\")\n        print(f\"masks: {masks.shape}\")\n        print(f'self.n: {self.n}')\n        print(\"*****\")\"\"\"\n        if self.tabular==True:\n            return [images\/255, lines.to_numpy()], masks\/255\n        else:\n            return images\/255, masks\/255\n    \n    def on_epoch_end(self):\n        pass","3ab46096":"df.columns","8716f9ea":"df[df[\"Patient\"]==\"TCGA_CS_4942\"]","292e44ea":"class DataSplitter():\n    def __init__(self, df, path, y, n_val_imgs = 100, tabular=False):\n        self.df = df\n        self.path = path\n        self.n = n_val_imgs\n        self.y = y\n        self.tabular=tabular\n        self.image_paths = []\n        folderlist = os.listdir(path)\n        folderlist.remove(\"README.md\")\n        folderlist.remove(\"data.csv\")\n        for folder in folderlist:\n            filelist = os.listdir(os.path.join(path, folder))\n            for file in filelist:\n                if file[:len(file)-4][-1].isnumeric():\n                    self.image_paths.append(file)\n        self.val_images = random.sample(self.image_paths, k = n_val_imgs)\n        self.train_images = []\n        for im in self.image_paths:\n            if im not in self.val_images:\n                self.train_images.append(im)\n        random.shuffle(self.train_images)\n    def TrainFlow(self, processing, batch_size, input_size):\n        return DataGenerator(path = self.path, df=self.df, y=self.y, image_paths_list = self.train_images, processing=processing, batch_size=batch_size, input_size=input_size, tabular=self.tabular)\n    def TestFlow(self, processing, batch_size, input_size):\n        return DataGenerator(path = self.path, df=self.df, y=self.y, image_paths_list = self.val_images, processing=processing, batch_size=batch_size, input_size=input_size, tabular=self.tabular)","5c3f13ba":"unet_splitter = DataSplitter(df = df, path = path, y=\"Patient\")\nynet_splitter = DataSplitter(df = df, path = path, y=\"Patient\", tabular=True)","50f4041d":"unet_train_gen = unet_splitter.TrainFlow(processing=None, batch_size=32, input_size=(128,128))\nynet_train_gen = ynet_splitter.TrainFlow(processing=None, batch_size=32, input_size=(128,128))","6f203881":"unet_val_gen = unet_splitter.TestFlow(processing=None, batch_size=32, input_size=(128,128))\nynet_val_gen = ynet_splitter.TestFlow(processing=None, batch_size=32, input_size=(128,128))","49ad9d09":"def NormalEncoderBlock(input_tensor, convolution, nfilters, pool = (2,2), dropout_rate = 0.3, kernel = (3,3)):\n    c = convolution(nfilters, kernel, activation = \"relu\", padding=\"same\")(input_tensor)\n    c = convolution(nfilters, kernel, activation = \"relu\", padding=\"same\")(c)\n    p = layers.MaxPooling2D(pool)(c)\n    p = layers.Dropout(dropout_rate)(p)\n    return c, p","f01e8db7":"def AlternativeEncoderBlock(input_tensor, convolution, nfilters, pool = (2,2), dropout_rate = 0.3, kernel = (3,3)):\n    c = convolution(nfilters, kernel, activation = \"relu\", padding=\"same\")(input_tensor)\n    c = layers.Dropout(dropout_rate)(c)\n    c = layers.BatchNormalization()(c)\n    c = convolution(nfilters, kernel, activation = \"relu\", padding=\"same\")(c)\n    p = layers.MaxPooling2D(pool)(c)\n    p = layers.Dropout(dropout_rate)(p)\n    p = layers.BatchNormalization()(p)\n    return c, p","51c97212":"def Encoder(input_tensor, convolution, kind):\n    if kind==\"normal\":\n        encoderblock = NormalEncoderBlock\n    elif kind==\"alternative\":\n        encoderblock = AlternativeEncoderBlock\n    else:\n        encoderblock=kind\n    c1, p1 = encoderblock(input_tensor, convolution, nfilters = 64)\n    c2, p2 = encoderblock(p1, convolution, nfilters = 128)\n    c3, p3 = encoderblock(p2, convolution, nfilters = 256)\n    c4, p4 = encoderblock(p3, convolution, nfilters = 256)\n    c5, p5 = encoderblock(p4, convolution, nfilters = 512)\n    return p5, (c1, c2, c3, c4, c5)","a203f3d6":"def NormalBottleneck(input_tensor, convolution, dropout_rate=0.3):\n    x = convolution(1024, (3,3), activation = \"relu\", padding=\"same\")(input_tensor)\n    x = convolution(1024, (3,3), activation = \"relu\", padding=\"same\")(x)\n    return x","70acb7aa":"def AlternativeBottleneck(input_tensor, convolution, dropout_rate=0.3):\n    x = convolution(1024, (3,3), activation = \"relu\", padding=\"same\")(input_tensor)\n    x = layers.Dropout(dropout_rate)(x)\n    x = layers.BatchNormalization()(x)\n    x = convolution(1024, (3,3), activation = \"relu\", padding=\"same\")(x)\n    x = layers.Dropout(dropout_rate)(x)\n    x = layers.BatchNormalization()(x)\n    return x","f33ced85":"def YNET_Bottleneck(input_tensor, convolution, line, dropout_rate=0.3):\n    x = layers.Dropout(dropout_rate)(input_tensor)\n    x = layers.BatchNormalization()(x)\n    x = convolution(1024, (3,3), activation = \"relu\", padding=\"same\")(x)\n    x = layers.concatenate([x,line])\n    x = layers.Dropout(dropout_rate)(x)\n    x = layers.BatchNormalization()(x)\n    x = convolution(1024, (3,3), activation = \"relu\", padding=\"same\")(x)\n    x = layers.Dropout(dropout_rate)(x)\n    x = layers.BatchNormalization()(x)\n    return x","032f3103":"def NormalDecoderBlock(input_tensor, convolution, encoder_output, nfilters, strides, kernel = (3,3), dropout_rate=0.3):\n    x = layers.Conv2DTranspose(nfilters, kernel, strides = strides, padding = \"same\")(input_tensor)\n    x = layers.concatenate([x, encoder_output])\n    x = layers.Dropout(dropout_rate)(x)\n    x = convolution(nfilters, kernel, activation=\"relu\", padding=\"same\")(x)\n    x = convolution(nfilters, kernel, activation=\"relu\", padding=\"same\")(x)\n    return x","f143c476":"def AlternativeDecoderBlock(input_tensor, convolution, encoder_output, nfilters, strides, kernel = (3,3), dropout_rate=0.3):\n    x = layers.Conv2DTranspose(nfilters, kernel, strides = strides, padding = \"same\")(input_tensor)\n    x = layers.concatenate([x, encoder_output])\n    x = layers.Dropout(dropout_rate)(x)\n    x = layers.BatchNormalization()(x)\n    x = convolution(nfilters, kernel, activation=\"relu\", padding=\"same\")(x)\n    x = layers.Dropout(dropout_rate)(x)\n    x = layers.BatchNormalization()(x)\n    x = convolution(nfilters, kernel, activation=\"relu\", padding=\"same\")(x)\n    x = layers.Dropout(dropout_rate)(x)\n    x = layers.BatchNormalization()(x)\n    return x","55daf84b":"def Decoder(input_tensor, convolution, convs, kind):\n    if kind==\"normal\":\n        decoderblock = NormalDecoderBlock\n    elif kind==\"alternative\":\n        decoderblock = AlternativeDecoderBlock\n    else:\n        decoderblock=kind\n    c1, c2, c3, c4, c5 = convs\n    x = decoderblock(input_tensor, convolution, c5, nfilters=512, kernel=(3,3), strides=(2,2))\n    x = decoderblock(x, convolution, c4, nfilters=256, kernel=(3,3), strides=(2,2))\n    x = decoderblock(x, convolution, c3, nfilters=256, kernel=(3,3), strides=(2,2))\n    x = decoderblock(x, convolution, c2, nfilters=128, kernel=(3,3), strides=(2,2))\n    x = decoderblock(x, convolution, c1, nfilters=64, kernel=(3,3), strides=(2,2))\n    outputs = layers.Conv2D(1, (1,1), activation='sigmoid')(x)\n    return outputs","bca8bba1":"def WNET_Center(input_tensor, convolution, convs):\n    c1, c2, c3, c4, c5 = convs\n    d5 = AlternativeDecoderBlock(input_tensor, convolution, c5, nfilters=512, kernel=(3,3), strides=(2,2))\n    d4 = AlternativeDecoderBlock(d5, convolution, c4, nfilters=256, kernel=(3,3), strides=(2,2))\n    d3 = AlternativeDecoderBlock(d4, convolution, c3, nfilters=256, kernel=(3,3), strides=(2,2))\n    d2 = AlternativeDecoderBlock(d3, convolution, c2, nfilters=128, kernel=(3,3), strides=(2,2))\n    d1 = AlternativeDecoderBlock(d2, convolution, c1, nfilters=64, kernel=(3,3), strides=(2,2))\n    return d1, d2, d3, d4, d5","b2ad57c1":"def Trunk(line, dropout=0.3):\n    x=layers.Dense(32, activation=\"relu\")(line)\n    x=layers.Dropout(dropout)(x)\n    x=layers.BatchNormalization()(x)\n    x=layers.Dense(64, activation=\"relu\")(x)\n    x=layers.Dropout(dropout)(x)\n    x=layers.BatchNormalization()(x)\n    x=layers.Dense(32, activation=\"relu\")(x)\n    x=layers.Dropout(dropout)(x)\n    x=layers.BatchNormalization()(x)\n    x=layers.Dense(16, activation=\"relu\")(x)\n    x=layers.Dropout(dropout)(x)\n    x=layers.BatchNormalization()(x)\n    return x","48d269ab":"batchsize=32\nNormal_Conv = layers.Conv2D\nDepthwise_Separable_Conv = layers.SeparableConv2D","bd3d2897":"def UNET(convolution, kind):\n    if kind==\"normal\":\n        bottleneck = NormalBottleneck\n        name=\"NormalUNET\"\n    elif kind==\"alternative\":\n        bottleneck = AlternativeBottleneck\n        name=\"AlternativeUNET\"\n    input_tensor=layers.Input(shape=(128,128,3))\n    encoder_outputs, convs = Encoder(input_tensor, convolution, kind)\n    bottleneck_output = bottleneck(encoder_outputs, convolution)\n    decoder_output = Decoder(bottleneck_output, convolution, convs, kind)\n    model = keras.Model(inputs=input_tensor, outputs=decoder_output, name=name)\n    return model","114f96d0":"def YNET(convolution):\n    input_tensor=layers.Input(shape=(128,128,3))\n    input_line=layers.Input(shape=(16))\n    encoder_outputs, convs = Encoder(input_tensor, convolution, kind=\"alternative\")\n    trunk_output = tf.reshape(Trunk(input_line), shape=(batchsize, 4, 4, 1))\n    bottleneck = YNET_Bottleneck(encoder_outputs, convolution, trunk_output)\n    decoder_output = Decoder(bottleneck, convolution, convs, kind=\"alternative\")\n    model = keras.Model(inputs=[input_tensor, input_line], outputs=decoder_output, name=\"YNET\")\n    return model","dbeace11":"def WNET(convolution):\n    input_tensor=layers.Input(shape=(128,128,3))\n    encoder_outputs, convs = Encoder(input_tensor, convolution, kind=\"alternative\")\n    first_bottleneck = AlternativeBottleneck(encoder_outputs, convolution)\n    wnet_center_output = WNET_Center(first_bottleneck, convolution, convs)\n    second_bottleneck = AlternativeBottleneck(first_bottleneck, convolution)\n    decoder_output = Decoder(second_bottleneck, convolution, wnet_center_output, kind=\"alternative\")\n    model = keras.Model(inputs=input_tensor, outputs=decoder_output, name=\"WNET\")\n    return model","e7b20dea":"unet = UNET(Normal_Conv, kind=\"normal\")\nsep_unet = UNET(Depthwise_Separable_Conv, kind=\"alternative\")\nynet = YNET(Normal_Conv)\nsep_ynet = YNET(Depthwise_Separable_Conv)\nwnet = WNET(Depthwise_Separable_Conv)","d0dc3396":"print(unet.summary())\nprint(sep_unet.summary())\nprint(ynet.summary())\nprint(sep_ynet.summary())\nprint(wnet.summary())","649c7c47":"from keras.utils.vis_utils import plot_model","4472d0f1":"plot_model(sep_unet, to_file='.\/sep_unet.png', show_shapes=False, show_layer_names=False)","012baba7":"plot_model(ynet, to_file='.\/ynet.png', show_shapes=False, show_layer_names=False)","6aae4a91":"plot_model(wnet, to_file='.\/wnet.png', show_shapes=False, show_layer_names=False)","6775b2e0":"import keras.backend as K","bfd7af5d":"def iou(ytrue, ypred):\n    smoothing_factor=0.1\n    #y_true_f=K.flatten(y_true)\n    #y_pred_f=K.flatten(y_pred)\n    intersection = K.sum(ytrue*ypred)\n    combined_area = K.sum(ytrue+ypred)\n    union_area = combined_area - intersection\n    iou = (intersection+smoothing_factor)\/(union_area+smoothing_factor)\n    return iou","d72e6299":"def dice_score(ytrue, ypred):\n    smoothing_factor=0.1\n    ytrue_f = K.flatten(ytrue)\n    ypred_f = K.flatten(ypred)\n    intersection = K.sum(ytrue*ypred)\n    ytrue_area = K.sum(ytrue)\n    ypred_area = K.sum(ypred)\n    combined_area = ytrue_area + ypred_area\n    dice = 2*((intersection+smoothing_factor)\/(combined_area+smoothing_factor))\n    return dice","c7452091":"def iou_loss(ytrue, ypred):\n    return -iou(ytrue, ypred)","2eb75cca":"unet.compile(optimizer=\"adam\", loss=iou_loss,\n              metrics=[iou,dice_score])\nsep_unet.compile(optimizer=\"adam\", loss=iou_loss,\n              metrics=[iou,dice_score])\nynet.compile(optimizer=\"adam\", loss=iou_loss,\n              metrics=[iou,dice_score])\nsep_ynet.compile(optimizer=\"adam\", loss=iou_loss,\n              metrics=[iou,dice_score])\nwnet.compile(optimizer=\"adam\", loss=iou_loss,\n              metrics=[iou,dice_score])","1e6d868e":"from tensorflow.keras.callbacks import ModelCheckpoint","c384ccd5":"unetpath=\"unet.hdf5\"\nunet_checkpoint = ModelCheckpoint(filepath=unetpath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\nunet_history = unet.fit(unet_train_gen, epochs=60,\n                        validation_data=unet_val_gen,\n                        callbacks=[unet_checkpoint])","0983f01a":"sep_unetpath=\"sep_unet.hdf5\"\nsep_unet_checkpoint = ModelCheckpoint(filepath=sep_unetpath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\nsep_unet_history = sep_unet.fit(unet_train_gen, epochs=60,\n                        validation_data=unet_val_gen,\n                        callbacks=[sep_unet_checkpoint])","35da7777":"ynetpath=\"ynet.hdf5\"\nynet_checkpoint = ModelCheckpoint(filepath=ynetpath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\nynet_history = ynet.fit(ynet_train_gen, epochs=60,\n                        validation_data=ynet_val_gen,\n                        callbacks=[ynet_checkpoint])","a305d05f":"sep_ynetpath=\"sep_ynet.hdf5\"\nsep_ynet_checkpoint = ModelCheckpoint(filepath=sep_ynetpath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\nsep_ynet_history = sep_ynet.fit(ynet_train_gen, epochs=60,\n                        validation_data=ynet_val_gen,\n                        callbacks=[sep_ynet_checkpoint])","5e7fa1df":"wnetpath=\"wnet.hdf5\"\nwnet_checkpoint = ModelCheckpoint(filepath=wnetpath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\nwnet_history = wnet.fit(unet_train_gen, epochs=60,\n                        validation_data=unet_val_gen,\n                        callbacks=[wnet_checkpoint])","79840566":"from matplotlib import style\nstyle.use(\"seaborn-darkgrid\")","3f2f246b":"fig, axes = plt.subplots(nrows=4, ncols=1, figsize=(20,65))\ncolors=[\"lightskyblue\", \"mediumblue\", \"darkviolet\", \"forestgreen\", \"crimson\"]\nmetrics = [\"iou\", \"val_iou\", \"dice_score\", \"val_dice_score\"]\ntitles = [\"Training IOU\", \"Testing IOU\", \"Training DICE Score\", \"Testing DICE Score\"]\nmodelnames=[\"UNET\", \"SEP_UNET\", \"YNET\", \"SEP_YNET\", \"WNET\"]\nhistories=[unet_history, sep_unet_history, ynet_history, sep_ynet_history, wnet_history]\nfor i in range(len(histories)):\n    model_his = histories[i]\n    for j in range(len(metrics)):\n        metric=metrics[j]\n        sns.lineplot(y=model_his.history[metric], x=list(range(60)), color=colors[i], ax=axes[j])\n        axes[j].set_title(titles[j], fontsize=30)\n        axes[j].legend(modelnames,fontsize=20)\n        axes[j].set_xlabel(\"Epochs\", fontsize=15)","8aace625":"fig, axes = plt.subplots(nrows=4, ncols=1, figsize=(20,35))\ncolors=[\"lightskyblue\", \"mediumblue\", \"darkviolet\", \"forestgreen\", \"crimson\"]\nmetrics = [\"iou\", \"val_iou\", \"dice_score\", \"val_dice_score\"]\ntitles = [\"Training IOU\", \"Testing IOU\", \"Training DICE Score\", \"Testing DICE Score\"]\nhistories=[unet_history, sep_unet_history, ynet_history, sep_ynet_history, wnet_history]\nfor j in range(len(metrics)):\n    metric=metrics[j]\n    best_scores=[]\n    for i in range(len(histories)):\n        best_scores.append(max(histories[i].history[metric]))\n    bsdf = pd.DataFrame(best_scores, index=modelnames, columns=[\"BestScores\"])\n    sns.barplot(x=bsdf.index, y=bsdf.BestScores, ax=axes[j], palette=colors)\n    axes[j].set_title(f\"Maximum {titles[j]}\", fontsize=30)","395cc784":"results = pd.DataFrame(np.empty((5,6)), index=modelnames, columns=titles+[\"n\u00b0 of Weights\", \"Approximate Time per Epoch\"])\nfor i in range(len(modelnames)):\n    his = histories[i]\n    for j in range(len(titles)):\n        metric = metrics[j]\n        results.loc[modelnames[i], titles[j]] = max(his.history[metric])\nweights=[\"38 Million\",\"10 Million\", \"38 Million\", \"10 Million\", \"20 Million\"]\ntime=[\"51s\",\"72s\",\"61s\",\"76s\",\"110s\"]\nfor i in range(5):\n    results.iloc[i,4]=weights[i]\n    results.iloc[i,5]=time[i]\nresults","995c4009":"plt.figure(figsize=(20,10))\nsns.heatmap(results.iloc[:,:4], vmin=0, vmax=1, cmap=\"Blues\", annot=True, annot_kws={\"fontsize\":15}, fmt='.2%')\nplt.show()","a8bc5f4b":"unet.load_weights(\".\/unet.hdf5\")\nsep_unet.load_weights(\".\/sep_unet.hdf5\")\nynet.load_weights(\".\/ynet.hdf5\")\nsep_ynet.load_weights(\".\/sep_ynet.hdf5\")\nwnet.load_weights(\".\/wnet.hdf5\")","5476fcef":"style.use(\"bmh\")","942ec1b3":"images, masks = unet_val_gen.__getitem__(random.randint(4,30))\nfig, axes = plt.subplots(nrows=32, ncols=5, figsize=(20,150))\nfor i in range(32):\n    axes[i,0].imshow(images[i,:])\n    axes[i,0].set_title(\"Original image\", fontsize=12)\n    axes[i,1].imshow(masks[i,:], cmap=\"binary_r\")\n    axes[i,1].set_title(\"Original mask\", fontsize=12)\n    img=images[i,:].reshape(1,128,128,3)\n    axes[i,2].imshow(unet.predict(img).reshape(128,128,1)*255, cmap=\"binary_r\")\n    axes[i,2].set_title(\"unet mask\", fontsize=12)\n    axes[i,3].imshow(sep_unet.predict(img).reshape(128,128,1)*255, cmap=\"binary_r\")\n    axes[i,3].set_title(\"sep_unet mask\", fontsize=12)\n    axes[i,4].imshow(wnet.predict(img).reshape(128,128,1)*255, cmap=\"binary_r\")\n    axes[i,4].set_title(\"wnet mask\", fontsize=12)","ae51ae89":"inputs, masks = ynet_val_gen.__getitem__(random.randint(3,15))\nimages, lines = inputs[0], inputs[1]\nynet_preds=ynet.predict(inputs)\nsep_ynet_preds=sep_ynet.predict(inputs)\nfig, axes = plt.subplots(nrows=32, ncols=4, figsize=(20,150))\nfor i in range(32):\n    axes[i,0].imshow(images[i,:])\n    axes[i,0].set_title(\"Original image\", fontsize=12)\n    axes[i,1].imshow(masks[i,:], cmap=\"binary_r\")\n    axes[i,1].set_title(\"Original mask\", fontsize=12)\n    img=images[i,:].reshape(1,128,128,3)\n    line=lines[i,:].reshape(1,16)\n    axes[i,2].imshow(ynet_preds[i,:].reshape(128,128,1)*255, cmap=\"binary_r\")\n    axes[i,2].set_title(\"ynet mask\", fontsize=12)\n    axes[i,3].imshow(sep_ynet_preds[i,:].reshape(128,128,1)*255, cmap=\"binary_r\")\n    axes[i,3].set_title(\"sep_ynet mask\", fontsize=12)","9ff266fd":"#### Getting rid of the useless variable:","558602e4":"#### Notes: \n* tumor_tissue_site contains only one value so it should be discarded.\n* There is serious imbalance in some variables.\n* For many variables, most samples are located far from the center of the distribution.  \n  These variables should be imputed with their respective modes instead of means and\/or medians.  \n* We can impute most variables with their modes, \"RNASeqCluster\" and \"RPPACluster\" being the exceptions due to them having   not so few missing values while being somewhat balanced.","266c8fa2":"# \u0628\u0633\u0645 \u0627\u0644\u0644\u0647","f7796f28":"In other words, each missing value is imputed with an existing (non-nan) value randomly chosen from that column.","bb5a4972":"#### The following is Wnet.\n#### Basically a Unet with an extra decoder in the center.\n#### I named it Wnet because it looks like a \"W\" instead of a \"U\"","5674038a":"#### The Sep_Unet model has the same architecture as Unet\n#### Ynet and Sep_Ynet have the same architecture\n#### Wnet has a unique architecture\n#### Thus I will only display the Ynet and Wnet architectures.\n#### Note: Sep_Unet and Sep_Ynet use Depthwise Separable Convolutions instead of normal Convolutions (hence the name).","7a619da2":"### Scaling tabular data","d30eba42":"#### The following is the Ynet architecture.\n#### Basically Unet, but with an additional 'Trunk' that deals with tabular data.\n#### I call it Ynet because it looks like \"Y\" instead of \"U\"","083725c1":"# Exploration and Processing\nTabular data needs to be processed in order to be fed into the Ynet architectures.","6aede2ef":"#### Random Imputation on \"RNASeqCluster\" and \"RPPACluster\":","55b0f7ad":"# Creating a Custom Data Generator with Processing\/Augmentation","9e614c05":"#### Encoding:\nAs age is numerical it will not be encoded.  \nThe remaining features are categorical.  \nBinary features will be One-Hot encoded. They already are binary but I just prefer 0s & 1s over 2s & 3s xD   \nMulti-class features will be left as they are.  \nThe thing is that a multi-class feature is usually label\/index encoded when its values follow an inherit order.  \nIf that's the case with our multi-class features (which would be true if they were discrete representations of continuous quantities) then I'd be certain about my decision.  \nHowever, I am no doctor.  \nBut i'll assume that cause otherwise One-Hot encoding will result in sparse high-dimensional data, which isn't usually pleasant.  \nTarget Encoding would be a good alternative in such a scenario, hadn't the outputs been masks.","fba3eba7":"# Training","5727decb":"We can see that WNet outputs considerable noise in cases when there is no tumor.  \nSep_Unet does output noise as well but it's much less noticeable.","b98034e7":"As we can see, we lose in terms of training time, but we gain a lot in all other aspects.  \nConsidering the facts that:  \n* All but Unet use Dropout and BatchNorm before every convolution.  \n* Sep_Unet, Sep_Ynet and Wnet use separable convolutions, whereas Ynet doesn't.  \n* Wnet is the only one with an extra decoder in the center.  \n\nWe can conclude that:  \n\n* Batchnorm and Dropout between convolutions allowed the models to increase performance beyond Unet's 0 scores, and to reach impressive scores at around 80%.  \n* Depthwise Separable Convolutions increase training time but reduce size considerably.  \n* The extra central decoder doesn't do much besides increasing size and training time.","6f92976c":"### \u0627\u0644\u062d\u0645\u062f \u0644\u0644\u0647 \u0627\u0644\u0630\u064a \u0628\u0646\u0639\u0645\u062a\u0647 \u062a\u062a\u0645 \u0627\u0644\u0635\u0627\u0644\u062d\u0627\u062a","ea1868d2":"### Checking number of folders and number of images","0195cb40":"# Brain MRI Segmentation: Repetitive BatchNormalization for improving performance from 0 to 80%  \n  \n### This is a project in which I built a couple of models that segment Brain MRI scans by localizing tumors in case there is any.\n### I built 5 models including the classic UNet.\n### The other 4 models are variations of it.\n### In all models (other than the classic UNet), 30% Dropout is applied between every two Conv layers, and BatchNorm is applied right before every Conv layer.\n### What I personally concluded (criticism is very welcomed):\n### * Adding dropout and batchnorm layers before every convolutional layer improves performance significantly.\n### * Depthwise Separable Convolutions reduce model size considerably, albeit increasing training time.\n### * Tabular Data (or at least the one in this dataset) about the patient doesn't increase segmentation performance (once batchnorm and dropout have been used) if fed into the bottleneck after passing through a Multi-Layer Perceptron. One possible hypothesis is that convolutions in the bottleneck and the decoder aren't suitable for tabular data even in tensor form.\n### * Adding a central decoder component to the UNet architecture doesn't imrove performance either (once batchnorm and dropout have been used).","280a456c":"# Building the Architectures","658e2e75":"### Making sure no masks are missing","64ddf79e":"#### Imputing the remaining variables:","027f37c5":"# Evaluation Metrics: IOU & DICE Score"}}