{"cell_type":{"09a441b6":"code","bf809d94":"code","3d73ab24":"code","50800e52":"code","60821318":"code","4dcf2225":"code","c0334572":"code","92219b86":"code","6942e3f4":"code","3d5cd480":"code","75412320":"code","5d3f0300":"code","eab749b2":"code","195079b3":"code","973a844b":"code","b6862fa7":"code","a3f2b187":"code","3b990f1b":"code","71097508":"code","e507dcd4":"code","030b6385":"code","1ee3252d":"code","cebee5ee":"code","b1dd9d56":"code","c3709ce1":"code","84a9a653":"code","3c449779":"code","dcad0a0c":"code","0d8158ca":"code","10a8eb17":"code","b0e479d1":"code","29be2476":"code","54f7a69b":"code","78924a96":"code","4894dd62":"code","12f843f0":"code","62f6024a":"code","38e53ad4":"code","427774a1":"code","e64c1d26":"code","76438975":"code","156730c1":"code","71630557":"code","2e6a7988":"code","3dc4ba46":"code","6be73bb4":"code","18d66796":"code","d8e38c60":"code","03dd526a":"code","9147789e":"code","348764ce":"code","8d00e0d4":"code","35983246":"code","7dfd0f50":"code","637bdc66":"code","c828997c":"code","268a7943":"code","6479729b":"code","2ff0e40f":"code","2270bd72":"code","932f835c":"code","96b4e098":"code","8828ce56":"code","fa03a837":"code","68fe95f4":"code","e3e1009c":"markdown","0da4ce66":"markdown","427d7498":"markdown","2cbdc20f":"markdown","ea01e185":"markdown","364ac8aa":"markdown","0c37869d":"markdown","59e62c4d":"markdown","0d0280e6":"markdown","7d2b07ed":"markdown","84fb67ca":"markdown","e1b0e655":"markdown","a7a7e4d0":"markdown","46ed8001":"markdown","d4cd0a04":"markdown","1cb91dfa":"markdown","b1ee6246":"markdown","ec2e7613":"markdown","d532d782":"markdown","0bcf4d13":"markdown","0533f2e5":"markdown","8d2526e7":"markdown","37a640e2":"markdown","6d84bf03":"markdown","70a23e44":"markdown"},"source":{"09a441b6":"# Install required packages\n\n!pip install -q transformers==3.1.0\n!pip install -qU hazm\n!pip install -qU clean-text[gpl]","bf809d94":"# Import required packages\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import f1_score\nfrom sklearn.utils import shuffle\n\nimport hazm\nfrom cleantext import clean\n\nimport plotly.express as px\nimport plotly.graph_objects as go\n\nfrom tqdm.notebook import tqdm\n\nimport os\nimport re\nimport json\nimport copy\nimport collections","3d73ab24":"data = pd.read_csv('\/kaggle\/input\/taaghche\/taghche.csv', encoding='utf-8')\ndata = data[['comment', 'rate']]\n\ndata.head()","50800e52":"# print data information\nprint('data information')\nprint(data.info(), '\\n')\n\n# print missing values information\nprint('missing values stats')\nprint(data.isnull().sum(), '\\n')\n\n# print some missing values\nprint('some missing values')\nprint(data[data['rate'].isnull()].iloc[:5], '\\n')","60821318":"# handle some conflicts with the dataset structure\n# you can find a reliable solution, for the sake of the simplicity\n# I just remove these bad combinations!\ndata['rate'] = data['rate'].apply(lambda r: r if r < 6 else None)\n\ndata = data.dropna(subset=['rate'])\ndata = data.dropna(subset=['comment'])\ndata = data.drop_duplicates(subset=['comment'], keep='first')\ndata = data.reset_index(drop=True)\n\n\n# previous information after solving the conflicts\n\n# print data information\nprint('data information')\nprint(data.info(), '\\n')\n\n# print missing values information\nprint('missing values stats')\nprint(data.isnull().sum(), '\\n')\n\n# print some missing values\nprint('some missing values')\nprint(data[data['rate'].isnull()].iloc[:5], '\\n')","4dcf2225":"# calculate the length of comments based on their words\ndata['comment_len_by_words'] = data['comment'].apply(lambda t: len(hazm.word_tokenize(t)))","c0334572":"min_max_len = data[\"comment_len_by_words\"].min(), data[\"comment_len_by_words\"].max()\nprint(f'Min: {min_max_len[0]} \\tMax: {min_max_len[1]}')","92219b86":"def data_gl_than(data, less_than=100.0, greater_than=0.0, col='comment_len_by_words'):\n    data_length = data[col].values\n\n    data_glt = sum([1 for length in data_length if greater_than < length <= less_than])\n\n    data_glt_rate = (data_glt \/ len(data_length)) * 100\n\n    print(f'Texts with word length of greater than {greater_than} and less than {less_than} includes {data_glt_rate:.2f}% of the whole!')","6942e3f4":"data_gl_than(data, 256, 3)","3d5cd480":"minlim, maxlim = 3, 256","75412320":"# remove comments with the length of fewer than three words\ndata['comment_len_by_words'] = data['comment_len_by_words'].apply(lambda len_t: len_t if minlim < len_t <= maxlim else None)\ndata = data.dropna(subset=['comment_len_by_words'])\ndata = data.reset_index(drop=True)","5d3f0300":"fig = go.Figure()\n\nfig.add_trace(go.Histogram(\n    x=data['comment_len_by_words']\n))\n\nfig.update_layout(\n    title_text='Distribution of word counts within comments',\n    xaxis_title_text='Word Count',\n    yaxis_title_text='Frequency',\n    bargap=0.2,\n    bargroupgap=0.2)\n\nfig.show()","eab749b2":"unique_rates = list(sorted(data['rate'].unique()))\nprint(f'We have #{len(unique_rates)}: {unique_rates}')","195079b3":"fig = go.Figure()\n\ngroupby_rate = data.groupby('rate')['rate'].count()\n\nfig.add_trace(go.Bar(\n    x=list(sorted(groupby_rate.index)),\n    y=groupby_rate.tolist(),\n    text=groupby_rate.tolist(),\n    textposition='auto'\n))\n\nfig.update_layout(\n    title_text='Distribution of rate within comments',\n    xaxis_title_text='Rate',\n    yaxis_title_text='Frequency',\n    bargap=0.2,\n    bargroupgap=0.2)\n\nfig.show()","973a844b":"def rate_to_label(rate, threshold=3.0):\n    if rate <= threshold:\n        return 'negative'\n    else:\n        return 'positive'\n\n\ndata['label'] = data['rate'].apply(lambda t: rate_to_label(t, 3.0))\nlabels = list(sorted(data['label'].unique()))\ndata.head()","b6862fa7":"def cleanhtml(raw_html):\n    cleanr = re.compile('<.*?>')\n    cleantext = re.sub(cleanr, '', raw_html)\n    return cleantext\n\n\ndef cleaning(text):\n    text = text.strip()\n    \n    # regular cleaning\n    text = clean(text,\n        fix_unicode=True,\n        to_ascii=False,\n        lower=True,\n        no_line_breaks=True,\n        no_urls=True,\n        no_emails=True,\n        no_phone_numbers=True,\n        no_numbers=False,\n        no_digits=False,\n        no_currency_symbols=True,\n        no_punct=False,\n        replace_with_url=\"\",\n        replace_with_email=\"\",\n        replace_with_phone_number=\"\",\n        replace_with_number=\"\",\n        replace_with_digit=\"0\",\n        replace_with_currency_symbol=\"\",\n    )\n\n    # cleaning htmls\n    text = cleanhtml(text)\n    \n    # normalizing\n    normalizer = hazm.Normalizer()\n    text = normalizer.normalize(text)\n    \n    # removing wierd patterns\n    wierd_pattern = re.compile(\"[\"\n        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n        u\"\\U00002702-\\U000027B0\"\n        u\"\\U000024C2-\\U0001F251\"\n        u\"\\U0001f926-\\U0001f937\"\n        u'\\U00010000-\\U0010ffff'\n        u\"\\u200d\"\n        u\"\\u2640-\\u2642\"\n        u\"\\u2600-\\u2B55\"\n        u\"\\u23cf\"\n        u\"\\u23e9\"\n        u\"\\u231a\"\n        u\"\\u3030\"\n        u\"\\ufe0f\"\n        u\"\\u2069\"\n        u\"\\u2066\"\n        # u\"\\u200c\"\n        u\"\\u2068\"\n        u\"\\u2067\"\n        \"]+\", flags=re.UNICODE)\n    \n    text = wierd_pattern.sub(r'', text)\n    \n    # removing extra spaces, hashtags\n    text = re.sub(\"#\", \"\", text)\n    text = re.sub(\"\\s+\", \" \", text)\n    \n    return text","a3f2b187":"# cleaning comments\ndata['cleaned_comment'] = data['comment'].apply(cleaning)\n\n\n# calculate the length of comments based on their words\ndata['cleaned_comment_len_by_words'] = data['cleaned_comment'].apply(lambda t: len(hazm.word_tokenize(t)))\n\n# remove comments with the length of fewer than three words\ndata['cleaned_comment_len_by_words'] = data['cleaned_comment_len_by_words'].apply(lambda len_t: len_t if minlim < len_t <= maxlim else len_t)\ndata = data.dropna(subset=['cleaned_comment_len_by_words'])\ndata = data.reset_index(drop=True)\n\ndata.head()","3b990f1b":"data = data[['cleaned_comment', 'label']]\ndata.columns = ['comment', 'label']\ndata.head()","71097508":"print(f'We have #{len(labels)} labels: {labels}')","e507dcd4":"fig = go.Figure()\n\ngroupby_label = data.groupby('label')['label'].count()\n\nfig.add_trace(go.Bar(\n    x=list(sorted(groupby_label.index)),\n    y=groupby_label.tolist(),\n    text=groupby_label.tolist(),\n    textposition='auto'\n))\n\nfig.update_layout(\n    title_text='Distribution of label within comments [DATA]',\n    xaxis_title_text='Label',\n    yaxis_title_text='Frequency',\n    bargap=0.2,\n    bargroupgap=0.2)\n\nfig.show()","030b6385":"negative_data = data[data['label'] == 'negative']\npositive_data = data[data['label'] == 'positive']\n\ncutting_point = min(len(negative_data), len(positive_data))\n\nif cutting_point <= len(negative_data):\n    negative_data = negative_data.sample(n=cutting_point).reset_index(drop=True)\n\nif cutting_point <= len(positive_data):\n    positive_data = positive_data.sample(n=cutting_point).reset_index(drop=True)\n\nnew_data = pd.concat([negative_data, positive_data])\nnew_data = new_data.sample(frac=1).reset_index(drop=True)\nnew_data.info()","1ee3252d":"fig = go.Figure()\n\ngroupby_label = new_data.groupby('label')['label'].count()\n\nfig.add_trace(go.Bar(\n    x=list(sorted(groupby_label.index)),\n    y=groupby_label.tolist(),\n    text=groupby_label.tolist(),\n    textposition='auto'\n))\n\nfig.update_layout(\n    title_text='Distribution of label within comments [NEW DATA]',\n    xaxis_title_text='Label',\n    yaxis_title_text='Frequency',\n    bargap=0.2,\n    bargroupgap=0.2)\n\nfig.show()","cebee5ee":"new_data.head()","b1dd9d56":"new_data['label_id'] = new_data['label'].apply(lambda t: labels.index(t))\n\ntrain, test = train_test_split(new_data, test_size=0.1, random_state=1, stratify=new_data['label'])\ntrain, valid = train_test_split(train, test_size=0.1, random_state=1, stratify=train['label'])\n\ntrain = train.reset_index(drop=True)\nvalid = valid.reset_index(drop=True)\ntest = test.reset_index(drop=True)\n\nx_train, y_train = train['comment'].values.tolist(), train['label_id'].values.tolist()\nx_valid, y_valid = valid['comment'].values.tolist(), valid['label_id'].values.tolist()\nx_test, y_test = test['comment'].values.tolist(), test['label_id'].values.tolist()\n\nprint(train.shape)\nprint(valid.shape)\nprint(test.shape)","c3709ce1":"print('Train sample:', x_train[0], y_train[0])","84a9a653":"from transformers import BertConfig, BertTokenizer\nfrom transformers import BertModel\n\nfrom transformers import AdamW\nfrom transformers import get_linear_schedule_with_warmup\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F","3c449779":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(f'device: {device}')\n\ntrain_on_gpu = torch.cuda.is_available()\n\nif not train_on_gpu:\n    print('CUDA is not available.  Training on CPU ...')\nelse:\n    print('CUDA is available!  Training on GPU ...')","dcad0a0c":"# general config\nMAX_LEN = 128\nTRAIN_BATCH_SIZE = 16\nVALID_BATCH_SIZE = 16\nTEST_BATCH_SIZE = 16\n\nEPOCHS = 3\nEEVERY_EPOCH = 1000\nLEARNING_RATE = 2e-5\nCLIP = 0.0\n\nMODEL_NAME_OR_PATH = 'HooshvareLab\/bert-fa-base-uncased'\nOUTPUT_PATH = '.\/bert-fa-base-uncased-sentiment-taaghche-v1'\nOUTPUT_MODEL_PATH = f'{OUTPUT_PATH}\/pytorch_model.bin'\n\nprint('OUTPUT_MODEL_PATH', OUTPUT_MODEL_PATH)\nos.makedirs(OUTPUT_PATH, exist_ok=True)","0d8158ca":"# create a key finder based on label 2 id and id to label\n\nlabel2id = {label: i for i, label in enumerate(labels)}\nid2label = {v: k for k, v in label2id.items()}\n\nprint(f'label2id: {label2id}')\nprint(f'id2label: {id2label}')","10a8eb17":"# setup the tokenizer and configuration\n\ntokenizer = BertTokenizer.from_pretrained(MODEL_NAME_OR_PATH)\nconfig = BertConfig.from_pretrained(\n    MODEL_NAME_OR_PATH, **{\n        'label2id': label2id,\n        'id2label': id2label,\n    })\n\nprint(config.to_json_string())","b0e479d1":"idx = np.random.randint(0, len(train))\nsample_comment = train.iloc[idx]['comment']\nsample_label = train.iloc[idx]['label']\n\nprint(f'Sample: \\n{sample_comment}\\n{sample_label}')","29be2476":"tokens = tokenizer.tokenize(sample_comment)\ntoken_ids = tokenizer.convert_tokens_to_ids(tokens)\n\nprint(f'  Comment: {sample_comment}')\nprint(f'   Tokens: {tokenizer.convert_tokens_to_string(tokens)}')\nprint(f'Token IDs: {token_ids}')","54f7a69b":"encoding = tokenizer.encode_plus(\n    sample_comment,\n    max_length=32,\n    truncation=True,\n    add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n    return_token_type_ids=True,\n    return_attention_mask=True,\n    padding='max_length',\n    return_tensors='pt',  # Return PyTorch tensors\n)\n\nprint(f'Keys: {encoding.keys()}\\n')\nfor k in encoding.keys():\n    print(f'{k}:\\n{encoding[k]}')","78924a96":"class TaaghcheDataset(torch.utils.data.Dataset):\n    \"\"\" Create a PyTorch dataset for Taaghche. \"\"\"\n\n    def __init__(self, tokenizer, comments, targets=None, label_list=None, max_len=128):\n        self.comments = comments\n        self.targets = targets\n        self.has_target = isinstance(targets, list) or isinstance(targets, np.ndarray)\n\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n        \n        self.label_map = {label: i for i, label in enumerate(label_list)} if isinstance(label_list, list) else {}\n    \n    def __len__(self):\n        return len(self.comments)\n\n    def __getitem__(self, item):\n        comment = str(self.comments[item])\n\n        if self.has_target:\n            target = self.label_map.get(str(self.targets[item]), str(self.targets[item]))\n\n        encoding = self.tokenizer.encode_plus(\n            comment,\n            add_special_tokens=True,\n            truncation=True,\n            max_length=self.max_len,\n            return_token_type_ids=True,\n            padding='max_length',\n            return_attention_mask=True,\n            return_tensors='pt')\n        \n        inputs = {\n            'comment': comment,\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'token_type_ids': encoding['token_type_ids'].flatten(),\n        }\n\n        if self.has_target:\n            inputs['targets'] = torch.tensor(target, dtype=torch.long)\n        \n        return inputs\n\n\ndef create_data_loader(x, y, tokenizer, max_len, batch_size, label_list):\n    dataset = TaaghcheDataset(\n        comments=x,\n        targets=y,\n        tokenizer=tokenizer,\n        max_len=max_len, \n        label_list=label_list)\n    \n    return torch.utils.data.DataLoader(dataset, batch_size=batch_size)","4894dd62":"label_list = ['negative', 'positive']\ntrain_data_loader = create_data_loader(train['comment'].to_numpy(), train['label'].to_numpy(), tokenizer, MAX_LEN, TRAIN_BATCH_SIZE, label_list)\nvalid_data_loader = create_data_loader(valid['comment'].to_numpy(), valid['label'].to_numpy(), tokenizer, MAX_LEN, VALID_BATCH_SIZE, label_list)\ntest_data_loader = create_data_loader(test['comment'].to_numpy(), None, tokenizer, MAX_LEN, TEST_BATCH_SIZE, label_list)","12f843f0":"sample_data = next(iter(train_data_loader))\n\nprint(sample_data.keys())\n\nprint(sample_data['comment'])\nprint(sample_data['input_ids'].shape)\nprint(sample_data['input_ids'][0, :])\nprint(sample_data['attention_mask'].shape)\nprint(sample_data['attention_mask'][0, :])\nprint(sample_data['token_type_ids'].shape)\nprint(sample_data['token_type_ids'][0, :])\nprint(sample_data['targets'].shape)\nprint(sample_data['targets'][0])","62f6024a":"sample_test = next(iter(test_data_loader))\nprint(sample_test.keys())","38e53ad4":"class SentimentModel(nn.Module):\n\n    def __init__(self, config):\n        super(SentimentModel, self).__init__()\n\n        self.bert = BertModel.from_pretrained(MODEL_NAME_OR_PATH)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n    \n    def forward(self, input_ids, attention_mask, token_type_ids):\n        _, pooled_output = self.bert(\n            input_ids=input_ids, \n            attention_mask=attention_mask, \n            token_type_ids=token_type_ids)\n        \n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output)\n        return logits ","427774a1":"# import torch, gc\n\n# gc.collect()\n# torch.cuda.empty_cache()\n# pt_model = None\n\n# !nvidia-smi","e64c1d26":"pt_model = SentimentModel(config=config)\npt_model = pt_model.to(device)\n\nprint('pt_model', type(pt_model))","76438975":"def simple_accuracy(y_true, y_pred):\n    return (y_true == y_pred).mean()\n\ndef acc_and_f1(y_true, y_pred, average='weighted'):\n    acc = simple_accuracy(y_true, y_pred)\n    f1 = f1_score(y_true=y_true, y_pred=y_pred, average=average)\n    return {\n        \"acc\": acc,\n        \"f1\": f1,\n    }\n\ndef y_loss(y_true, y_pred, losses):\n    y_true = torch.stack(y_true).cpu().detach().numpy()\n    y_pred = torch.stack(y_pred).cpu().detach().numpy()\n    y = [y_true, y_pred]\n    loss = np.mean(losses)\n\n    return y, loss\n\n\ndef eval_op(model, data_loader, loss_fn):\n    model.eval()\n\n    losses = []\n    y_pred = []\n    y_true = []\n\n    with torch.no_grad():\n        for dl in tqdm(data_loader, total=len(data_loader), desc=\"Evaluation... \"):\n            \n            input_ids = dl['input_ids']\n            attention_mask = dl['attention_mask']\n            token_type_ids = dl['token_type_ids']\n            targets = dl['targets']\n\n            # move tensors to GPU if CUDA is available\n            input_ids = input_ids.to(device)\n            attention_mask = attention_mask.to(device)\n            token_type_ids = token_type_ids.to(device)\n            targets = targets.to(device)\n\n            # compute predicted outputs by passing inputs to the model\n            outputs = model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                token_type_ids=token_type_ids)\n            \n            # convert output probabilities to predicted class\n            _, preds = torch.max(outputs, dim=1)\n\n            # calculate the batch loss\n            loss = loss_fn(outputs, targets)\n\n            # accumulate all the losses\n            losses.append(loss.item())\n\n            y_pred.extend(preds)\n            y_true.extend(targets)\n    \n    eval_y, eval_loss = y_loss(y_true, y_pred, losses)\n    return eval_y, eval_loss\n\n\ndef train_op(model, \n             data_loader, \n             loss_fn, \n             optimizer, \n             scheduler, \n             step=0, \n             print_every_step=100, \n             eval=False,\n             eval_cb=None,\n             eval_loss_min=np.Inf,\n             eval_data_loader=None, \n             clip=0.0):\n    \n    model.train()\n\n    losses = []\n    y_pred = []\n    y_true = []\n\n    for dl in tqdm(data_loader, total=len(data_loader), desc=\"Training... \"):\n        step += 1\n\n        input_ids = dl['input_ids']\n        attention_mask = dl['attention_mask']\n        token_type_ids = dl['token_type_ids']\n        targets = dl['targets']\n\n        # move tensors to GPU if CUDA is available\n        input_ids = input_ids.to(device)\n        attention_mask = attention_mask.to(device)\n        token_type_ids = token_type_ids.to(device)\n        targets = targets.to(device)\n\n        # clear the gradients of all optimized variables\n        optimizer.zero_grad()\n\n        # compute predicted outputs by passing inputs to the model\n        outputs = model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids)\n        \n        # convert output probabilities to predicted class\n        _, preds = torch.max(outputs, dim=1)\n\n        # calculate the batch loss\n        loss = loss_fn(outputs, targets)\n\n        # accumulate all the losses\n        losses.append(loss.item())\n\n        # compute gradient of the loss with respect to model parameters\n        loss.backward()\n\n        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs \/ LSTMs.\n        if clip > 0.0:\n            nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip)\n\n        # perform optimization step\n        optimizer.step()\n\n        # perform scheduler step\n        scheduler.step()\n\n        y_pred.extend(preds)\n        y_true.extend(targets)\n\n        if eval:\n            train_y, train_loss = y_loss(y_true, y_pred, losses)\n            train_score = acc_and_f1(train_y[0], train_y[1], average='weighted')\n\n            if step % print_every_step == 0:\n                eval_y, eval_loss = eval_op(model, eval_data_loader, loss_fn)\n                eval_score = acc_and_f1(eval_y[0], eval_y[1], average='weighted')\n\n                if hasattr(eval_cb, '__call__'):\n                    eval_loss_min = eval_cb(model, step, train_score, train_loss, eval_score, eval_loss, eval_loss_min)\n\n    train_y, train_loss = y_loss(y_true, y_pred, losses)\n\n    return train_y, train_loss, step, eval_loss_min","156730c1":"optimizer = AdamW(pt_model.parameters(), lr=LEARNING_RATE, correct_bias=False)\ntotal_steps = len(train_data_loader) * EPOCHS\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=0,\n    num_training_steps=total_steps\n)\n\nloss_fn = nn.CrossEntropyLoss()\n\nstep = 0\neval_loss_min = np.Inf\nhistory = collections.defaultdict(list)\n\n\ndef eval_callback(epoch, epochs, output_path):\n    def eval_cb(model, step, train_score, train_loss, eval_score, eval_loss, eval_loss_min):\n        statement = ''\n        statement += 'Epoch: {}\/{}...'.format(epoch, epochs)\n        statement += 'Step: {}...'.format(step)\n        \n        statement += 'Train Loss: {:.6f}...'.format(train_loss)\n        statement += 'Train Acc: {:.3f}...'.format(train_score['acc'])\n\n        statement += 'Valid Loss: {:.6f}...'.format(eval_loss)\n        statement += 'Valid Acc: {:.3f}...'.format(eval_score['acc'])\n\n        print(statement)\n\n        if eval_loss <= eval_loss_min:\n            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n                eval_loss_min,\n                eval_loss))\n            \n            torch.save(model.state_dict(), output_path)\n            eval_loss_min = eval_loss\n        \n        return eval_loss_min\n\n\n    return eval_cb\n\n\nfor epoch in tqdm(range(1, EPOCHS + 1), desc=\"Epochs... \"):\n    train_y, train_loss, step, eval_loss_min = train_op(\n        model=pt_model, \n        data_loader=train_data_loader, \n        loss_fn=loss_fn, \n        optimizer=optimizer, \n        scheduler=scheduler, \n        step=step, \n        print_every_step=EEVERY_EPOCH, \n        eval=True,\n        eval_cb=eval_callback(epoch, EPOCHS, OUTPUT_MODEL_PATH),\n        eval_loss_min=eval_loss_min,\n        eval_data_loader=valid_data_loader, \n        clip=CLIP)\n    \n    train_score = acc_and_f1(train_y[0], train_y[1], average='weighted')\n    \n    eval_y, eval_loss = eval_op(\n        model=pt_model, \n        data_loader=valid_data_loader, \n        loss_fn=loss_fn)\n    \n    eval_score = acc_and_f1(eval_y[0], eval_y[1], average='weighted')\n    \n    history['train_acc'].append(train_score['acc'])\n    history['train_loss'].append(train_loss)\n    history['val_acc'].append(eval_score['acc'])\n    history['val_loss'].append(eval_loss)","71630557":"!ls {OUTPUT_PATH}","2e6a7988":"def predict(model, comments, tokenizer, max_len=128, batch_size=32):\n    data_loader = create_data_loader(comments, None, tokenizer, max_len, batch_size, None)\n    \n    predictions = []\n    prediction_probs = []\n\n    \n    model.eval()\n    with torch.no_grad():\n        for dl in tqdm(data_loader, position=0):\n            input_ids = dl['input_ids']\n            attention_mask = dl['attention_mask']\n            token_type_ids = dl['token_type_ids']\n\n            # move tensors to GPU if CUDA is available\n            input_ids = input_ids.to(device)\n            attention_mask = attention_mask.to(device)\n            token_type_ids = token_type_ids.to(device)\n            \n            # compute predicted outputs by passing inputs to the model\n            outputs = model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                token_type_ids=token_type_ids)\n            \n            # convert output probabilities to predicted class\n            _, preds = torch.max(outputs, dim=1)\n\n            predictions.extend(preds)\n            prediction_probs.extend(F.softmax(outputs, dim=1))\n\n    predictions = torch.stack(predictions).cpu().detach().numpy()\n    prediction_probs = torch.stack(prediction_probs).cpu().detach().numpy()\n\n    return predictions, prediction_probs","3dc4ba46":"test_comments = test['comment'].to_numpy()\npreds, probs = predict(pt_model, test_comments, tokenizer, max_len=128)\n\nprint(preds.shape, probs.shape)","6be73bb4":"y_test, y_pred = [label_list.index(label) for label in test['label'].values], preds\n\nprint(f'F1: {f1_score(y_test, y_pred, average=\"weighted\")}')\nprint()\nprint(classification_report(y_test, y_pred, target_names=label_list))","18d66796":"from transformers import BertConfig, BertTokenizer\nfrom transformers import TFBertModel, TFBertForSequenceClassification\nfrom transformers import glue_convert_examples_to_features\n\nimport tensorflow as tf","d8e38c60":"# general config\nMAX_LEN = 128\nTRAIN_BATCH_SIZE = 16\nVALID_BATCH_SIZE = 16\nTEST_BATCH_SIZE = 16\n\nEPOCHS = 3\nEEVERY_EPOCH = 1000\nLEARNING_RATE = 2e-5\nCLIP = 0.0\n\nMODEL_NAME_OR_PATH = 'HooshvareLab\/bert-fa-base-uncased'\nOUTPUT_PATH = '.\/bert-fa-base-uncased-sentiment-taaghche-v1'\n\nos.makedirs(OUTPUT_PATH, exist_ok=True)","03dd526a":"label2id = {label: i for i, label in enumerate(labels)}\nid2label = {v: k for k, v in label2id.items()}\n\nprint(f'label2id: {label2id}')\nprint(f'id2label: {id2label}')","9147789e":"tokenizer = BertTokenizer.from_pretrained(MODEL_NAME_OR_PATH)\nconfig = BertConfig.from_pretrained(\n    MODEL_NAME_OR_PATH, **{\n        'label2id': label2id,\n        'id2label': id2label,\n    })\n\nprint(config.to_json_string())","348764ce":"class InputExample:\n    \"\"\" A single example for simple sequence classification. \"\"\"\n\n    def __init__(self, guid, text_a, text_b=None, label=None):\n        \"\"\" Constructs a InputExample. \"\"\"\n        self.guid = guid\n        self.text_a = text_a\n        self.text_b = text_b\n        self.label = label\n\n\ndef make_examples(tokenizer, x, y=None, maxlen=128, output_mode=\"classification\", is_tf_dataset=True):\n    examples = []\n    y = y if isinstance(y, list) or isinstance(y, np.ndarray) else [None] * len(x)\n\n    for i, (_x, _y) in tqdm(enumerate(zip(x, y)), position=0, total=len(x)):\n        guid = \"%s\" % i\n        label = int(_y)\n        \n        if isinstance(_x, str):\n            text_a = _x\n            text_b = None\n        else:\n            assert len(_x) == 2\n            text_a = _x[0]\n            text_b = _x[1]\n        \n        examples.append(InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n    \n    features = glue_convert_examples_to_features(\n        examples, \n        tokenizer, \n        maxlen, \n        output_mode=output_mode, \n        label_list=list(np.unique(y)))\n\n    all_input_ids = []\n    all_attention_masks = []\n    all_token_type_ids = []\n    all_labels = []\n\n    for f in tqdm(features, total=len(examples)):\n        if is_tf_dataset:\n            all_input_ids.append(tf.constant(f.input_ids))\n            all_attention_masks.append(tf.constant(f.attention_mask))\n            all_token_type_ids.append(tf.constant(f.token_type_ids))\n            all_labels.append(tf.constant(f.label))\n        else:\n            all_input_ids.append(f.input_ids)\n            all_attention_masks.append(f.attention_mask)\n            all_token_type_ids.append(f.token_type_ids)\n            all_labels.append(f.label)\n\n    if is_tf_dataset:\n        dataset = tf.data.Dataset.from_tensor_slices(({\n            'input_ids': all_input_ids,\n            'attention_mask': all_attention_masks,\n            'token_type_ids': all_token_type_ids\n        }, all_labels))\n\n        return dataset, features\n    \n    xdata = [np.array(all_input_ids), np.array(all_attention_masks), np.array(all_token_type_ids)]\n    ydata = all_labels\n\n    return [xdata, ydata], features","8d00e0d4":"train_dataset_base, train_examples = make_examples(tokenizer, x_train, y_train, maxlen=128)\nvalid_dataset_base, valid_examples = make_examples(tokenizer, x_valid, y_valid, maxlen=128)\n\ntest_dataset_base, test_examples = make_examples(tokenizer, x_test, y_test, maxlen=128)\n[xtest, ytest], test_examples = make_examples(tokenizer, x_test, y_test, maxlen=128, is_tf_dataset=False)","35983246":"for value in train_dataset_base.take(1):\n    print(f'     input_ids: {value[0][\"input_ids\"]}')\n    print(f'attention_mask: {value[0][\"attention_mask\"]}')\n    print(f'token_type_ids: {value[0][\"token_type_ids\"]}')\n    print(f'        target: {value[1]}')","7dfd0f50":"def get_training_dataset(dataset, batch_size):\n    dataset = dataset.repeat()\n    dataset = dataset.shuffle(2048)\n    dataset = dataset.batch(batch_size)\n\n    return dataset\n\ndef get_validation_dataset(dataset, batch_size):\n    dataset = dataset.batch(batch_size)\n\n    return dataset","637bdc66":"train_dataset = get_training_dataset(train_dataset_base, TRAIN_BATCH_SIZE)\nvalid_dataset = get_training_dataset(valid_dataset_base, VALID_BATCH_SIZE)\n\ntrain_steps = len(train_examples) \/\/ TRAIN_BATCH_SIZE\nvalid_steps = len(valid_examples) \/\/ VALID_BATCH_SIZE\n\ntrain_steps, valid_steps","c828997c":"def build_model(model_name, config, learning_rate=3e-5):\n    model = TFBertForSequenceClassification.from_pretrained(model_name, config=config)\n\n    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n    metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n    model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n\n    return model","268a7943":"model = build_model(MODEL_NAME_OR_PATH, config, learning_rate=LEARNING_RATE)\n# model.summary()","6479729b":"%%time\n\nr = model.fit(\n    train_dataset,\n    validation_data=valid_dataset,\n    steps_per_epoch=train_steps,\n    validation_steps=valid_steps,\n    epochs=EPOCHS)\n\nfinal_accuracy = r.history['val_accuracy']\nprint('FINAL ACCURACY MEAN: ', np.mean(final_accuracy))","2ff0e40f":"# save the model\n\nmodel.save_pretrained(OUTPUT_PATH)","2270bd72":"!ls {OUTPUT_PATH}","932f835c":"ev = model.evaluate(test_dataset_base.batch(TEST_BATCH_SIZE), verbose=0)\nprint()\nprint(f'Evaluation: {ev}')\nprint()\n\npredictions = model.predict(xtest)\nypred = predictions[0].argmax(axis=-1).tolist()\n\nprint()\nprint(classification_report(ytest, ypred, target_names=labels))\nprint()\n\nprint(f'F1: {f1_score(ytest, ypred, average=\"weighted\")}')","96b4e098":"!rm -rf .\/taaghche\n!mkdir -p .\/taaghche\n\nntrain = train[['comment', 'label']]\nntrain.columns = ['text', 'label']\n\nndev = valid[['comment', 'label']]\nndev.columns = ['text', 'label']\n\nntest = test[['comment', 'label']]\nntest.columns = ['text', 'label']\n\n\nntrain.to_csv('.\/taaghche\/' + 'train.tsv', sep='\\t', index=False)\nndev.to_csv('.\/taaghche\/' + 'dev.tsv', sep='\\t', index=False)\nntest.to_csv('.\/taaghche\/' + 'test.tsv', sep='\\t', index=False)","8828ce56":"!git clone https:\/\/github.com\/hooshvare\/parsbert.git parsbert\n!ls","fa03a837":"!python .\/parsbert\/scripts\/run_clf_finetuning.py","68fe95f4":"!rm -rf .\/bert-fa-base-uncased-sentiment-taaghceh-v2\/\n!mkdir -p .\/bert-fa-base-uncased-sentiment-taaghceh-v2\/\n\n\n!python .\/parsbert\/scripts\/run_clf_finetuning.py \\\n    --labels positive,negative \\\n    --output_mode classification \\\n    --task_name taaghche \\\n    --model_name_or_path HooshvareLab\/bert-fa-base-uncased \\\n    --data_dir .\/taaghche\/ \\\n    --output_dir .\/bert-fa-base-uncased-sentiment-taaghceh-v2\/ \\\n    --max_seq_length 128 \\\n    --per_device_train_batch_size 8 \\\n    --per_device_eval_batch_size 16 \\\n    --learning_rate 2e-5 \\\n    --num_train_epochs 3.0 \\\n    --logging_steps 500 \\\n    --save_steps 2000 \\\n    --do_train \\\n    --do_eval \\\n    --do_predict \\\n    --overwrite_output_dir \\\n    --logging_first_step","e3e1009c":"### Training","0da4ce66":"### Model\n\nDuring the implementation of the model, sometime, you may be faced with this kind of error. It said you used all the Cuda-Memory for solving. There are many ways for the simple one is to clear the Cuda cache memory!\n\n![Cuda-Error](https:\/\/res.cloudinary.com\/m3hrdadfi\/image\/upload\/v1599979552\/kaggle\/cuda-error_iyqh4o.png)\n\n\n**Simple Solution**\n```python\nimport torch, gc\n\ngc.collect()\ntorch.cuda.empty_cache()\n\n!nvidia-smi\n```","427d7498":"## Train,Validation,Test split\n\nTo achieve a globalized model, we need to split the cleaned dataset into train, valid, test sets due to size of the data. In this tutorial, I have considered a rate of **0.1** for both *valid*, *test* sets. For splitting, I use `train_test_split` provided by Sklearn package with stratifying on the label for preserving the distribution balance.","2cbdc20f":"### Input Embeddings \/ Dataset","ea01e185":"### Training","364ac8aa":"For simplicity, I transformed the rate in a range of 0.0 to 5.0 to a binary form of negative (0) or positive (1) with a threshold. If the rate is less than 3.0, it labeled as negative otherwise specified as positive.","0c37869d":"### Input Embeddings","59e62c4d":"### Evaluation \/ Prediction","0d0280e6":"### Configuration","7d2b07ed":"### Prediction\/Evaluation","84fb67ca":"### Load the data using Pandas","e1b0e655":"## PyTorch","a7a7e4d0":"### Normalization \/ Preprocessing\n\nThe comments have different lengths based on words! Detecting the most normal range could help us find the maximum length of the sequences for the preprocessing step. On the other hand, we suppose that the minimum word combination for having a meaningful phrase for our learning process is 3.","46ed8001":"### Dataset","d4cd0a04":"## Dataset\n\nFor this tutorial, I'm going to use one of the sentiment analysis datasets accessible from Kaggle ([Taaghche](https:\/\/www.kaggle.com\/saeedtqp\/taaghche)). You can use whatever dataset you have (Please let me know your results on different datasets you used). \n\nLet's look at the Taaghche dataset and obtain some intuitions about the data, distribution, and any further operation regarding this particular case.","1cb91dfa":"# Sentiment Analysis with ParsBERT\n\n\n## BERT Overview\n\nBERT stands for Bi-directional Encoder Representation from Transformers is designed to pre-train deep bidirectional representations from unlabeled texts by jointly conditioning on both left and right context in all layers. The pretrained BERT model can be fine-tuned with just one additional output layer (in many cases) to create state-of-the-art models. This model can use for a wide range of NLP tasks, such as question answering and language inference, and so on without substantial task-specific architecture modification.\n\n\nNatural Language Processing (NLP) tasks include sentence-level tasks or token-level tasks:\n\n- **Sentence-Level:** Tasks such as Natural Language Inference (NLI) aim to predict the relationships between sentences by holistically analyzing them.\n- **Token-Level:** Tasks such as Named Entity Recognition (NER), Question Answering (QA), the model makes predictions on a word-by-word basis.\n\nIn the pre-trained language representation, there are two primary strategies for applying to down-stream NLP tasks:\n\n- Feature-based: They use task-specific architectures that include pre-training representation as additional features like Word2vec, ELMo, ...\n- Fine-tunning: Introduce minimal task-specific parameters, and are trained on the down-stream tasks by merely tuning the pre-training parameters like GPT.\n\nBefore going more further into code, let me introduce **ParsBERT**.\n\n## ParsBERT\n\nIs a monolingual language model based on Google's BERT architecture. This model is pre-trained on large Persian corpora with various writing styles from numerous subjects (e.g., scientific, novels, news, ...) with more than **3.9M** documents, **73M** sentences, and **1.3B** words. For more information about ParsBERT, please check out our article: [arXiv:2005.12515](https:\/\/arxiv.org\/abs\/2005.12515).\n\n[ParsBERT Repository](https:\/\/github.com\/hooshvare\/parsbert)\n\nSo, now you have a little understanding of BERT in total, we need to know how to use ParsBERT in our project. In the following tutorial, I would like to implement a fine-tuned model on the Sentiment Analysis task for TensorFlow and PyTorch. Stay tuned!","b1ee6246":"Now, my favorite part comes. First of all, I follow the model using *PyTorch*, and next, do the same processes using *TensorFlow* and finally use the ParsBERT script to do all the things once in a desired form for HuggignFace.\n\n![BERT INPUTS](https:\/\/res.cloudinary.com\/m3hrdadfi\/image\/upload\/v1595158991\/kaggle\/bert_inputs_w8rith.png)\n\nAs you may know, the BERT model input is a combination of 3 embeddings.\n- Token embeddings: WordPiece token vocabulary (WordPiece is another word segmentation algorithm, similar to BPE)\n- Segment embeddings: for pair sentences [A-B] marked as $E_A$ or $E_B$ mean that it belongs to the first sentence or the second one.\n- Position embeddings: specify the position of words in a sentence","ec2e7613":"### Model","d532d782":"### Configuration","0bcf4d13":"### Fixing Conflicts\n\nAs you might be realized, the dataset has some structural problems, as shown below.\n\n![alt text](https:\/\/res.cloudinary.com\/m3hrdadfi\/image\/upload\/v1596398432\/kaggle\/data-structure-flaws_xoqprk.png)\n\nFor simplicity, I fix this problem by removing rows with the `rate` value of `None`. Furthermore, the dataset contains duplicated rows and missing values in the comment section.","0533f2e5":"## Script","8d2526e7":"Again, for making things simple. We cut the dataset randomly based on the fewer label, the negative class.","37a640e2":"## TensorFlow","6d84bf03":"Cleaning is the final step in this section. Our cleaned method includes these steps:\n\n- fixing unicodes\n- removing specials like a phone number, email, url, new lines, ...\n- cleaning HTMLs\n- normalizing\n- removing emojis","70a23e44":"### Handling Unbalanced Data"}}