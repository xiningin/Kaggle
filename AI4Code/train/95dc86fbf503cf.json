{"cell_type":{"574b697d":"code","13a0e920":"code","9593d47a":"code","cb05f326":"code","a8c9dab6":"code","dd235ca7":"code","df1ba579":"code","31de3ae0":"code","1ccb120d":"code","4ebeeccc":"code","33f24a6a":"code","5ece9828":"code","62d12e28":"code","33ae200d":"code","c05614c1":"code","44308c28":"markdown"},"source":{"574b697d":"# Import Libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport sklearn\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split \nfrom sklearn.linear_model import LogisticRegression ","13a0e920":"# Import Data\ndata = pd.read_csv('..\/input\/iris\/Iris.csv').drop(columns = ['Id'])\ndata","9593d47a":"# Data Types\ndata.info()","cb05f326":"# Summary Statistics\ndata.describe()","a8c9dab6":"# Any Null Values ?\ndata.isna().sum()","dd235ca7":"# Target Variable Boxplot\nsns.set_style('ticks')\nsns.set_palette('Set1')\nsns.countplot(data=data, x=\"Species\", palette='rainbow')\nplt.xlabel('Species')\nplt.ylabel('Frequency')\nplt.ylim([0, 80])\nplt.show()","df1ba579":"# Independent and Dependent Variable Subsets\nX = data.drop(columns = ['Species'])\ny = data['Species']","31de3ae0":"# Lets Take a Look At X And y\nX, y","1ccb120d":"# Correlation between Input Features \nX.corr()","4ebeeccc":"# Boxplots for each dependent variable with respect to inputs.\nsns.set(style=\"whitegrid\") \nplt.figure(figsize = (10 , 40))\nfor i in range(4):\n    plt.subplot(10, 2 , i + 1)\n    sns.boxplot(x = data['Species'], y =data[list(X)[i]],  palette='Set1' )\nplt.show()","33f24a6a":"# Split the datset into training and testing\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, shuffle = True)","5ece9828":"# Lets take a look at data structures\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","62d12e28":"# Feature Scaling\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","33ae200d":"# Logistic Regression\nclf_lr = LogisticRegression(random_state=0).fit(X_train, y_train)\ny_pred = clf_lr.predict(X_test)\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\n\nscore = accuracy_score(y_test, y_pred)\nprint(\"\\n Logistic Regression Model Accuracy: \", score.round(2))\n\nreport = classification_report(y_test, y_pred)\nprint('\\n ', report)","c05614c1":"# Lets deploy 10 fold cross validation\naccuracies = cross_val_score(estimator = clf_lr, X = X_train, y = y_train, cv = 10)\nscore = accuracies.mean()\nprint(\"Accuracy: {:.3f} %\".format(score.mean()*100))\nprint(\"Standard Deviation: {:.3f} %\".format(accuracies.std()*100))","44308c28":"This was a simple analysis on the iconic Iris dataset. The relationship between the independent and dependent variables is linearly sperable. The model achieved 100% predictive accuracy on the default logistic regression model. Moreover, average accuracy on 10-fold cross validation turned out to be 95%. At this point I will conclude this notebook. It would be worth carrying out some intricate bivariate\/multivatiate analysis through visualizations some day. "}}