{"cell_type":{"e815e58d":"code","351f8b77":"code","9b63037f":"code","7adeb641":"code","039f842a":"code","d38fea91":"code","7b201fbd":"code","d94c2c3f":"code","cba999de":"code","5f933a3d":"code","c84c67f6":"code","a892f3e9":"code","24f1658c":"code","034c0277":"code","6679616c":"code","fd9dc3cf":"code","d539d84d":"code","14693c06":"code","23d4e0c0":"code","6ad4d1ae":"code","614eaf89":"code","c6ba7667":"code","95f3e6d8":"code","1f3a689c":"code","cbbe2876":"code","6092e04d":"code","7fd8e44c":"code","7df4c14f":"code","fc75dfb7":"code","e9a343da":"code","df515e91":"code","bc225843":"code","8f5b3377":"code","a484fe34":"code","8d72df33":"code","ba9b7938":"code","0a19d4dd":"code","955732be":"code","df2e0247":"code","706071fd":"code","dcb731d2":"code","7ff0e309":"code","dd6e3222":"code","25bf39b6":"code","aebe3e0a":"code","5f136754":"code","231a09ae":"code","b70fae58":"code","9a25726e":"code","d699adf8":"code","6d0019d4":"code","5b7ccc81":"code","b3e0e947":"code","44fd3a52":"code","9fa2a929":"code","1bde7e71":"code","ce89937b":"code","370d093b":"code","634a9849":"code","4c53191f":"code","59dc685b":"code","82f02080":"code","6607873f":"code","ea8019b5":"code","d1ebf9e6":"code","234420a9":"code","62c986bd":"code","c05df74d":"code","7ff0fe6f":"code","b99c97dd":"markdown","61225dfe":"markdown","6d30346d":"markdown","ea23ccc2":"markdown","99099119":"markdown","0ffd862a":"markdown","6503b279":"markdown","60c79c93":"markdown","d9b90889":"markdown","fd66b7f0":"markdown","e7632a92":"markdown","4d8d79ff":"markdown","37a09850":"markdown","638af83a":"markdown"},"source":{"e815e58d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","351f8b77":"df = pd.read_csv('..\/input\/telecom_churn_data.csv')\ndf.head()","9b63037f":"print(df.circle_id.value_counts())\ndf.drop('circle_id', axis =1, inplace = True)","7adeb641":"#Saving in a series for Feature extraction\nnumbers = df.mobile_number\ndf.drop('mobile_number', axis = 1 , inplace=True)","039f842a":"df.drop(df.loc[df.isnull().sum(axis=1)>150].index, axis = 0, inplace = True)","d38fea91":"df.reset_index(inplace=True)\ndf.info()","7b201fbd":"df.isnull().sum()","d94c2c3f":"df['Churn'] = np.where((df['total_ic_mou_9']==0) & (df['total_og_mou_9']==0) & (df['vol_2g_mb_9']==0 ) & (df['vol_3g_mb_9']==0),1,0)","cba999de":"df[(df['total_ic_mou_9']==0) & (df['total_og_mou_9']==0) & (df['vol_2g_mb_9']==0 ) & (df['vol_3g_mb_9']==0)][['total_ic_mou_9','total_og_mou_9','vol_2g_mb_9','vol_3g_mb_9','Churn']].head()","5f933a3d":"col_9 = [i for i in df.columns if i.endswith('_9')]\ncol_9","c84c67f6":"df = df.drop(col_9, axis =1)","a892f3e9":"\ndf.info()","24f1658c":"df[['av_rech_amt_data_7','total_rech_num_7', 'total_rech_amt_7','total_rech_data_7']].info()","034c0277":"\ncol_7 = ['av_rech_amt_data_7','total_rech_num_7', 'total_rech_amt_7','total_rech_data_7']\ndf[col_7].head()","6679616c":"col_6 = ['av_rech_amt_data_6','total_rech_num_6', 'total_rech_amt_6','total_rech_data_6']\ndf[col_6].head()","fd9dc3cf":"print(df[col_7].info())\nprint(df[col_6].info())","d539d84d":"df[col_7]=df[col_7].fillna(0)\ndf[col_6]=df[col_6].fillna(0)","14693c06":"print(df[col_7].info())\nprint(df[col_6].info())","23d4e0c0":"df['total_amount_7']=df['total_rech_data_7']+(df['total_rech_data_7']*df['av_rech_amt_data_7'])\ndf['total_amount_6']=df['total_rech_data_6']+(df['total_rech_data_6']*df['av_rech_amt_data_6'])","6ad4d1ae":"df['avg_amt_7_6']=(df['total_amount_7']+df['total_amount_6'])\/2.0","614eaf89":"perc_70 = (df['avg_amt_7_6'].quantile(0.7))","c6ba7667":"df=df[df['avg_amt_7_6']>=perc_70]\ndf.info()","95f3e6d8":"high_null = df.columns[df.isnull().sum()>=9000]\nhigh_null","1f3a689c":"df[high_null]=df[high_null].fillna(0)","cbbe2876":"high_null= df.columns[df.isnull().sum()>=5000]\ndf[high_null]=df[high_null].fillna(0)","6092e04d":"col = df.columns[df.isnull().sum()>1500]\ndf[col] = df[col].fillna(0)","7fd8e44c":"sum(df.isnull().sum(axis = 1)>50)","7df4c14f":"df.info()","fc75dfb7":"\ndf.reset_index(inplace = True,drop = True)\ndf.head()\ndf.drop('index', inplace = True, axis =1)","e9a343da":"df.info()\n","df515e91":"df[df.isnull().sum(axis =1)>60].head()","bc225843":"df.fillna(0, inplace=True)\ndf.head()","8f5b3377":"df.info()","a484fe34":"sum(df.isnull().sum()>0)","8d72df33":"obj_col = df.select_dtypes(include = 'object').columns\nobj_col","ba9b7938":"df.drop(obj_col,axis = 1 ,inplace = True)\ndf.head()","0a19d4dd":"X=df.drop('Churn', axis =1)\ny=df.Churn\ny.value_counts()","955732be":"import matplotlib.pyplot as plt\nimport seaborn as sns\nf,ax = plt.subplots(1,2,figsize=(16,8))\n\ncolors = [\"#FA5858\", \"#64FE2E\"]\nlabels = 'Non-Churned','Churned'\nplt.suptitle('Information on Churn vs NON-Churn labels')\ny.value_counts().plot.pie(explode=[0,0.05], autopct='%1.2f%%', ax =ax[0], shadow = True,\n                                    colors = colors, labels = labels, fontsize =12, startangle=25)\nax[0].set_ylabel('% of ADR vs Non-ADR')\n\npalette = [\"#64FE2E\", \"#FA5858\"]\nsns.countplot(x='Churn', data=df)\nax[1].set_xticklabels(['Non-Churn','Churn'],rotation=0, rotation_mode=\"anchor\")\nplt.show()","df2e0247":"df.head()","706071fd":"\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,train_size = 0.7, random_state = 42)","dcb731d2":"X_train.head()","7ff0e309":"X_train.shape","dd6e3222":"#Improting the PCA module\nfrom sklearn.decomposition import PCA\npca = PCA(svd_solver='randomized', random_state=42)","25bf39b6":"#Doing the PCA on the train data\npca.fit(X_train)","aebe3e0a":"len(pca.components_)","5f136754":"colnames = list(X_train.columns)\npcs_df = pd.DataFrame({'PC1':pca.components_[0],'PC2':pca.components_[1], 'Feature':colnames})\npcs_df.head()","231a09ae":"#Making the screeplot - plotting the cumulative variance against the number of components\n%matplotlib inline\nfig = plt.figure(figsize = (12,8))\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance')\nplt.axvline(x=20,c ='red')\nplt.axhline(y=0.955, c = 'red')\nplt.annotate(s ='optimal Number of Components(20 for 95.5% variance)' ,xy=[20,0.955], xytext = (21, 0.930))\nplt.show()","b70fae58":"#Using incremental PCA for efficiency - as it saves a lot of time on larger datasets\nfrom sklearn.decomposition import IncrementalPCA\npca_final = IncrementalPCA(n_components=20)","9a25726e":"pca_train = pca_final.fit_transform(X_train)\npca_train.shape","d699adf8":"#Applying selected components to the test data - 16 components\npca_test = pca_final.transform(X_test)\npca_test.shape","6d0019d4":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import accuracy_score, recall_score\n\n\ndef get_score(model,X_train,X_test):\n    model.fit(X_train,y_train)\n    y_pred=model.predict(X_test)\n    print(confusion_matrix(y_test,y_pred))\n    print(classification_report(y_test,y_pred))\n    print(\"The accuracy for Model is:\",accuracy_score(y_test,y_pred))\n    print('roc = ',roc_auc_score(y_test,model.predict_proba(X_test)[:,1]))\n    print('recall = ',recall_score(y_test, y_pred))","5b7ccc81":"from sklearn.linear_model import LogisticRegression\nlog_clf = LogisticRegression(random_state=42, class_weight='balanced')\n","b3e0e947":"get_score(log_clf,X_train,X_test)","44fd3a52":"get_score(log_clf,pca_train,pca_test)","9fa2a929":"from sklearn.ensemble import RandomForestClassifier\nrf_clf = RandomForestClassifier(random_state = 42, n_estimators= 100,n_jobs =-1, class_weight = {0:1,1:10})\nget_score(rf_clf,X_train,X_test)","1bde7e71":"from sklearn.model_selection import GridSearchCV\nparams = { 'n_estimators':[150],\n          'max_features': ['auto','log2','sqrt'] ,\n          'class_weight':['balanced','balanced_subsample'],\n          'max_depth' : [2,4,8] , \n          'min_samples_leaf':[20,22,24],\n          'criterion' : ['gini']\n            }\ng_search = GridSearchCV(estimator = RandomForestClassifier(random_state = 42,n_jobs = -1 ), scoring = 'recall', cv =3,\n                                                           param_grid = params)\ng_search.fit(X_train,y_train)\n","ce89937b":"b=g_search.best_params_\n##print(a)\nb","370d093b":"g_search.best_score_\n","634a9849":"# get_score(RandomForestClassifier(random_state = 42,n_jobs = -1 ,**a))","4c53191f":"get_score(RandomForestClassifier(random_state = 42,n_jobs = -1 ,**b),X_train,y_train)","59dc685b":"from sklearn.model_selection import GridSearchCV\nfrom xgboost import XGBClassifier\n\nparam_test = {\n  'min_child_weight':[5,6],\n  'max_depth': range(3,5),\n#   'n_estimators':[150,200,300,400],\n#   'scale_pos_weight':[1,2,3,4],\n#   'colsample_bytree':[0.7,0.8], \n#   'subsample':[0.7,0.8],\n  'gamma':[0,0.2,0.4]\n    \n}\ngsearch = GridSearchCV(estimator = XGBClassifier(), param_grid = param_test, scoring='recall',n_jobs=4,iid=False, cv=3)\ngsearch.fit(X_train, y_train)\n#print(gsearch.grid_scores_)\nprint(\"gsearch.best_params_\",gsearch.best_params_)\nprint(\"gsearch.best_score_\",gsearch.best_score_)\n","82f02080":"from xgboost import XGBClassifier\nmodelXg = XGBClassifier(learning_rate=0.1, n_estimators=150, max_depth=4, min_child_weight=7, \n                      gamma=0.4,nthread=4, subsample=0.8, colsample_bytree=0.8,\n                      objective= 'binary:logistic',scale_pos_weight=3,seed=29)\n\n\nmodelXg.fit(X_train, y_train)\ny_xg = modelXg.predict(X_test)","6607873f":"## Determine whether your model is overfitting or not , with the help of ROC.\n\nfrom sklearn.metrics import precision_score, recall_score,f1_score\n                                                  \nprint(confusion_matrix(y_test, y_xg))\npredictions = [value for value in y_xg]\naccuracy = accuracy_score(y_test, predictions)\nprecision = precision_score(y_test, predictions)\nrecall = recall_score(y_test, predictions)\nf1 = f1_score(y_test, predictions)\nprint(\"Accuracy_score: %.2f%% on test dataset\" % (accuracy * 100.0))\nprint(\"precision_score: %.2f%% on test dataset\" % (precision * 100.0))\nprint(\"recall_score: %.2f%% on test dataset\" % (recall * 100.0))\nprint(\"f1_score: %.2f%% on test dataset\" % (f1 * 100.0))\nprint(\"roc_auc test set\", roc_auc_score(y_test, modelXg.predict_proba(X_test)[:,1]))\nprint(\"roc_auc training set\", roc_auc_score(y_train, modelXg.predict_proba(X_train)[:,1]))","ea8019b5":"from xgboost import XGBClassifier\nfrom xgboost import plot_importance\nimport matplotlib.pyplot as plt","d1ebf9e6":"fig, ax = plt.subplots(figsize=(12,18))\nplot_importance(modelXg,importance_type='gain', max_num_features=50, height=0.8, ax=ax)\nplt.show()","234420a9":"# columns sorted according to their importance\nsorted_idx = np.argsort(modelXg.feature_importances_)[::-1]\nX_train.columns[sorted_idx[:50]]","62c986bd":"# columns and Values in sorted normalized format\nfor index in sorted_idx:\n    print([X_train.columns[index], modelXg.feature_importances_[index]]) ","c05df74d":"# use feature importance for feature selection\nfrom numpy import loadtxt\nfrom numpy import sort\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.feature_selection import SelectFromModel\n\n# select features using threshold\nselection = SelectFromModel(modelXg, threshold=0.06, prefit=True)\nselect_X_train = selection.transform(X_train)\n# train model\nselection_model = XGBClassifier()\nselection_model.fit(select_X_train, y_train)\n# eval model\nselect_X_test = selection.transform(X_test)\ny_pred = selection_model.predict(select_X_test)\naccuracy_score(y_test,y_pred)\n\n","7ff0fe6f":"# accuracy = accuracy_score(y_test, predictions)\n# print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n# # Fit model using each importance as a threshold\n# thresholds = sort(modelXg.feature_importances_)\n# for thresh in thresholds:\n# # select features using threshold\n# \tselection = SelectFromModel(modelXg, threshold=thresh, prefit=True)\n# \tselect_X_train = selection.transform(X_train)\n# \t# train model\n# \tselection_model = XGBClassifier()\n# \tselection_model.fit(select_X_train, y_train)\n# \t# eval model\n# \tselect_X_test = selection.transform(X_test)\n# \ty_pred = selection_model.predict(select_X_test)\n# \tpredictions = [round(value) for value in y_pred]\n# \taccuracy = accuracy_score(y_test, predictions)\n# \tprint(\"Thresh=%.3f, n=%d, Accuracy: %.2f%%\" % (thresh, select_X_train.shape[1], accuracy*100.0))","b99c97dd":"#### Looking at the screeplot to assess the number of needed principal components","61225dfe":"## Feature Selection with XGBoostFeature Importance Scores\n### Feature importance scores can be also used for feature selection in scikit-learn.\n\nThis is done using the SelectFromModel class that takes a model and can transform a dataset into a subset with selected features.\n\nThis class can take a pre-trained model, such as one trained on the entire training dataset. It can then use a threshold to decide which features to select. This threshold is used when you call the transform() method on the SelectFromModel instance to consistently select the same features on the training dataset and the test dataset.\n\nIn the example below we first train and then evaluate an XGBoost model on the entire training dataset and test datasets respectively.\n\nUsing the feature importances calculated from the training dataset, we then wrap the model in a SelectFromModel instance. We use this to select features on the training dataset, train a model from the selected subset of features, then evaluate the model on the testset, subject to the same feature selection scheme****","6d30346d":"# Feature Importance Using XGBOOST","ea23ccc2":"## Modeling process","99099119":"## Cols needed to calculate the HVC's","0ffd862a":"## Dropping the columns with prefix as _9","6503b279":"#### Looks like 20 components are enough to describe 95% of the variance in the dataset\n> - We'll choose 16 components for our modeling","60c79c93":"# As you can see almost similar results with only 20 PCA components\n## Going forward we can replace X_train with PCA comps","d9b90889":"\n\n#### \u2018Gain\u2019 is the improvement in accuracy brought by a feature to the branches it is on. The idea is that before adding a new split on a feature X to the branch there was some wrongly classified elements, after adding the split on this feature, there are two new branches, and each of these branch is more accurate (one branch saying if your observation is on this branch then it should be classified as 1, and the other branch saying the exact opposite)\n### The Gain is the most relevant attribute to interpret the relative importance of each feature.**","fd66b7f0":"# PCA","e7632a92":"# Filling all NA's with Zero for Now","4d8d79ff":"## Gain for feature Importance\n**The Gain implies the relative contribution of the corresponding feature to the model calculated by taking each feature\u2019s contribution for each tree in the model. A higher value of this metric when compared to another feature implies it is more important for generating a prediction******","37a09850":"## Assigning Labels","638af83a":"# Preparing Data for Modeling purpose"}}