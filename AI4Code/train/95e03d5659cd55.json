{"cell_type":{"9bb3df06":"code","bf435ddf":"code","0adc5632":"code","bbead2e3":"code","3d5995fb":"code","bf74b189":"code","2a7e4f81":"code","18025524":"code","07e569af":"code","710495ac":"code","1bf06e47":"code","e7740f23":"code","e901cc1c":"code","72047e27":"code","b6b09685":"code","68503b0e":"code","5c46c39b":"code","c04ffc15":"code","1f818813":"code","c1afc126":"code","35fadc1b":"code","34bcc052":"markdown","f32fd18f":"markdown","564020ef":"markdown","acf37d66":"markdown","a8231e76":"markdown","43d80ca2":"markdown","071cb379":"markdown","9825bfd1":"markdown","be52cfb1":"markdown","442799b9":"markdown","87f28a94":"markdown","f6904a31":"markdown","84ab6781":"markdown","ed3b06d3":"markdown","125569e2":"markdown","891b0daa":"markdown","9349de65":"markdown","124fd250":"markdown"},"source":{"9bb3df06":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","bf435ddf":"# Let's start importing the first two libraries\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt","0adc5632":"# Now I'm setting the configuration to display the dataframes' info\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\npd.set_option('display.width', None)\npd.set_option('display.max_colwidth', None)","bbead2e3":"# I will use the following two variables to have better visibility when printing the info I want to display by adding separators. (print(divider_length * divider_shape))\ndivider_length = 80\ndivider_shape = '='","3d5995fb":"# reading the dataset\ndataset = pd.read_csv('..\/input\/fetal-health-classification\/fetal_health.csv')","bf74b189":"print(divider_length * divider_shape)\nprint('ANALYSING Dataset')\nprint('Size: ' + str(dataset.shape))\nprint(divider_length * divider_shape)\nprint('Dataset columns: ')\nprint(dataset.columns)\nprint(divider_length * divider_shape)\nprint('Dataset head: ')\nprint(dataset.head(10))\nprint(divider_length * divider_shape)\nprint('Dataset info: ')\ndataset.info()","2a7e4f81":"# double checking that there are not null values in the dataset\nprint(divider_length * divider_shape)\nprint('Number missing values per column in train set:')\ntrain_cols_with_nulls = dataset.isnull().sum()\nprint(train_cols_with_nulls[train_cols_with_nulls > 0])\n","18025524":"print(divider_length * divider_shape)\nprint('Checking how imbalance is the dataset:')\nprint(dataset['fetal_health'].value_counts())","07e569af":"print(divider_length * divider_shape)\nprint('Check number of duplicate rows:')\nprint(dataset.duplicated().sum())","710495ac":"print(divider_length * divider_shape)\nprint('Labels counts duplicate: ')\nprint(dataset.loc[dataset.duplicated(), 'fetal_health'].value_counts())\n","1bf06e47":"#%% Removing duplicates\ndataset.drop_duplicates(inplace=True)\n# Given the low number of duplicates for label 1 the dataset is still imbalance\nprint(divider_length * divider_shape)\nprint('Checking how imbalance is the dataset after removing duplicates:')\nprint(dataset['fetal_health'].value_counts())\n","e7740f23":"# Dividing the data into features (X) and labels (y)\nX = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, -1].values\n","e901cc1c":"# Splitting the dataset into the Training set and Test set. By using the same random_state there is guarantee that both _no_stra and _stra sets will be split in the same way\nfrom sklearn.model_selection import train_test_split\nX_train_no_stra, X_validation_no_stra, y_train_no_stra, y_validation_no_stra = train_test_split(X, y, test_size=0.3, random_state=0)\nX_train_stra, X_validation_stra, y_train_stra, y_validation_stra = train_test_split(X, y, test_size=0.3, stratify=y, random_state=0)","72047e27":"# Let's save the dataset classes for future use\nclasses = np.unique(y)\nprint(divider_length * divider_shape)\nprint(classes)","b6b09685":"# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train_no_stra = sc.fit_transform(X_train_no_stra)\nX_validation_no_stra = sc.transform(X_validation_no_stra)\n\nX_train_stra = sc.fit_transform(X_train_stra)\nX_validation_stra = sc.transform(X_validation_stra)\n","68503b0e":"# Let's import the required libraries\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom catboost import CatBoostClassifier","5c46c39b":"all_models = [\n    {'model': LogisticRegression(random_state=0),\n      'grid_params': {'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n                     },\n    },\n    {'model': KNeighborsClassifier(),\n     'grid_params': {'n_neighbors': [5, 10, 15],\n                      'metric': ['euclidean', 'manhattan', 'chebyshev', 'minkowski'],\n                   }\n    },\n    {'model': SVC(random_state = 0, probability=True),\n      'grid_params': [{'C': [0.25, 0.5, 0.75, 1],\n                      'kernel': ['linear'],\n                      },\n                      {'C': [0.25, 0.5, 0.75, 1],\n                      'kernel': ['poly'],\n                      'gamma': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n                      'degree': [2, 3, 4, 5],\n                      },\n                      {'C': [0.25, 0.5, 0.75, 1],\n                      'kernel': ['rbf'],\n                      'gamma': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n                      },\n                      {'C': [0.25, 0.5, 0.75, 1],\n                      'kernel': ['sigmoid'],\n                      'gamma': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n                      },\n                    ]\n    },\n    {'model': DecisionTreeClassifier(random_state = 0),\n      'grid_params': {'criterion': ['gini', 'entropy'],\n                      'splitter': ['best', 'random'],\n                      'max_depth': [5, 10, 20, None],\n                      'max_features': ['auto', 'sqrt', 'log2', None],\n                    }\n    },\n    {'model': RandomForestClassifier(random_state = 0),\n      'grid_params': {'n_estimators': [100, 250, 500, 1000],\n                      'criterion': ['gini', 'entropy'],\n                      'max_depth': [5, 10, 20, None],\n                      'max_features': ['auto', 'sqrt', 'log2', None],\n                    }\n    },\n    {'model': CatBoostClassifier(early_stopping_rounds=100, verbose=False, random_state=0, loss_function='MultiClass'),\n     'grid_params': {'learning_rate': [0.001, 0.01, 0.1, 0.25, 0.5, 0.75, 1],\n                     'iterations': [100, 500, 1000],\n                    }\n    }\n]","c04ffc15":"#%% create grid search and fit the models\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold\nfrom mlxtend.plotting import plot_confusion_matrix\n\naverage = 'macro'                # Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.\nmulti_class = 'ovo'              # Computes the average AUC of all possible pairwise combinations of classes. Insensitive to class imbalance when average == 'macro'\nscoring = {'f1_macro', 'roc_auc_ovo'}  # The two scores that I will use according to the exercise's notes\n\nkfolds = KFold(n_splits=10)      # 10 plots\nn_jobs = -1                      # use all available cores\n\nfinal_scores = []\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score, f1_score, classification_report, roc_auc_score, ConfusionMatrixDisplay\n\nfor model in all_models:        # traverse the model list (for each model)\n    for stratify_data in [False, True]:   # choose whether to use the stratify or no stratify dataset\n        if stratify_data == False:\n            X_train, X_validation, y_train, y_validation = X_train_no_stra, X_validation_no_stra, y_train_no_stra, y_validation_no_stra\n        else:\n            X_train, X_validation, y_train, y_validation = X_train_stra, X_validation_stra, y_train_stra, y_validation_stra\n\n        model_name = type(model['model']).__name__\n        print(divider_length * divider_shape)\n        print('Finding parameters for ' + model_name + ' with stratify_data = ', str(stratify_data))\n        grid_search = GridSearchCV(estimator = model['model'],\n                                   param_grid = model['grid_params'],\n                                   scoring = scoring,\n                                   refit='f1_macro',       # F1 will be the main score to refit the estimator\n                                   cv = kfolds,\n                                   n_jobs = n_jobs,\n                                  )\n        grid_search.fit(X_train, y_train)                   # fit the grid search\n        best_classifier = grid_search.best_estimator_       # get the best estimator\n        best_score = grid_search.best_score_                # get the best score (f1)\n        best_parameters = grid_search.best_params_          # get the best parameters\n        \n        # Metrics on train data. Just for fun.\n        y_pred_train = best_classifier.predict(X_train)\n        y_pred_proba_train = best_classifier.predict_proba(X_train)\n\n        # calculate the training accuracy\n        train_accuracy = accuracy_score(y_true = y_train, y_pred = y_pred_train)\n\n        # calculate the training f-score\n        train_f1 = f1_score(y_true = y_train, y_pred = y_pred_train, average=average)\n\n        # calculate the training roc_auc\n        train_roc_auc = roc_auc_score(y_true = y_train, y_score = y_pred_proba_train, average=average, multi_class=multi_class)\n\n        # calculate the confusion matrix for training data\n        train_cm = confusion_matrix(y_true = y_train, y_pred = y_pred_train)\n\n    \n        \n        # Metrics on validation data. The results are reported on this metrics\n        y_pred_validation = best_classifier.predict(X_validation)\n        y_pred_proba_validation = best_classifier.predict_proba(X_validation)\n\n        # calculate the validation accuracy\n        validation_accuracy = accuracy_score(y_true = y_validation, y_pred = y_pred_validation)\n\n        # calculate the validation f-score\n        validation_f1 = f1_score(y_true = y_validation, y_pred = y_pred_validation, average=average)\n\n        # calculate the validation roc_auc\n        validation_roc_auc = roc_auc_score(y_true = y_validation, y_score = y_pred_proba_validation, average=average, multi_class=multi_class)\n\n        # calculate the confusion matrix for validation data\n        validation_cm = confusion_matrix(y_true = y_validation, y_pred = y_pred_validation)\n\n        # consolidated data in classification_report\n        validation_report = classification_report(y_true = y_validation, y_pred = y_pred_validation)\n    \n        print(divider_length * divider_shape)\n        print('Saving results for the best ' + model_name + ' with stratify_data = ' + str(stratify_data))\n    \n        # the best model is stored as a dictionary within the final_scores list\n        final_scores.append({\n                             'model': best_classifier,\n                             'model_name': model_name,\n                             'stratify_data':stratify_data,\n                             'model_parameters': best_parameters,\n                             'gs_cv_score': best_score,\n                             'train_accuracy': train_accuracy,\n                             'train_f1': train_f1,\n                             'train_roc_auc': train_roc_auc,\n                             'training_conf_mat': train_cm,\n                             'validation_accuracy': validation_accuracy,\n                             'validation_f1': validation_f1,\n                             'validation_roc_auc': validation_roc_auc,\n                             'validation_conf_mat': validation_cm,\n                             'validation_report': validation_report,\n                             }\n                            )\n        # printing the results I care the most: model's hyperparameters, F1, ROC_AUC, confusion matrix and classification report\n        print('Best parameters: ' + str(best_parameters))\n        print('F1: ' + str(round(validation_f1, 2)))\n        print('ROC_AUC: ' + str(round(validation_roc_auc, 2)))\n        fig, ax = plot_confusion_matrix(conf_mat=validation_cm, class_names=classes, figsize=(3, 3), cmap=plt.cm.Blues)\n        plt.xlabel('Predictions', fontsize=12)\n        plt.ylabel('Actuals', fontsize=12)\n        plt.title('CM for ' + model_name + ' stratify=' + str(stratify_data), fontsize=14)\n        plt.show()\n        print(validation_report)\n    \n        print('\\a')\n","1f818813":"# save results in a dataframe and print scores per model\nprint(divider_length * divider_shape)\nprint('Best scores per model')\ndf_final_scores = pd.DataFrame(final_scores)\nprint_columns = ['model_name', 'stratify_data', 'model_parameters', 'gs_cv_score', \n                 'train_accuracy', 'train_f1', 'train_roc_auc', \n                 'validation_accuracy', 'validation_f1', 'validation_roc_auc']\nprint(df_final_scores[print_columns])\n","c1afc126":"print(divider_length * divider_shape)\nprint_columns = ['model_name', 'stratify_data', 'validation_f1', 'validation_roc_auc']\nprint(df_final_scores[print_columns])","35fadc1b":"print(divider_length * divider_shape)\nprint('Best model')\nprint_columns = ['model_name', 'stratify_data', 'model_parameters', \n                 'validation_f1', 'validation_roc_auc']\nwinner_model_index = df_final_scores['validation_f1'].idxmax()\nprint(df_final_scores.loc[winner_model_index, print_columns])\nfig, ax = plot_confusion_matrix(conf_mat=df_final_scores.loc[winner_model_index, 'validation_conf_mat'], class_names=classes, figsize=(3, 3), cmap=plt.cm.Blues)\nplt.xlabel('Predictions', fontsize=12)\nplt.ylabel('Actuals', fontsize=12)\nplt.title('Winner Confusion Matrix', fontsize=14)\nplt.show()\nprint('Validation classification report')\nprint(df_final_scores.loc[winner_model_index, 'validation_report'])\n","34bcc052":"## 2. Data split and scale\n\nThe exercise's notes suggests to use 30% of the data for testing. It also mentions to stratify the data given the imbalance. I will work with two datasets, one with the stratify split and another one without stratify.","f32fd18f":"## 3. Create the ML models with their grid search parameters\n\nI will use the following models: Logistic Regression, K-Nearest-Neighbors, SupportVectorMachines, Decision Trees, Random Forest and Catboost\n","564020ef":"Now we can save the results saved in the final_scores list into a dataframe and print the columns we are interested in.","acf37d66":"The dataset is pretty imbalace towards label 1.\n\nNow let's check whether there are duplicate rows:","a8231e76":"## 1. Exploratory data analysis\n### 1.1. Reading the data","43d80ca2":"## 5. Choose the winner model\n\nNow we could determine the winner model. Let's take a closer look to both f1 and roc_auc metrics to reach out to a conclusion","071cb379":"# Fetal Health Classification\n\nThis kernel shows the process that I designed to create a model to classify the outcome of Cardiotocogram (CTG) exam, which represents the well being of the fetus. Given that, according to the exercise's notes, the dataset is highly imbalance so **accuracy is not a good metric to messure the model's performance**. The exercise's notes also mention to stratify the data when spliting.\n\nThe recommended metrics to measure the performance are:\n* Area under the ROC curve\n* F1 score\n* Area under the Precision-Recall curve\n\nI will use F1 and Area under the ROC curve\n\nMoreover, the author suggest to stratify data when spliting the datasets into training and testing given the imbalance in the dataset. I will execute the experiment on both, stratify and no stratified data to compare the results.\n\nThis notebook is divided as follows:\n\n1. Exploratory data analysis\n    1. Reading the data\n    2. Exploring the data\n2. Data split and scale\n3. Create the ML models with their grid search parameters\n4. Tune and Fit the models, with grid search and predict on validation set\n5. Choose the winner model","9825bfd1":"## 4. Tune and Fit the models, with grid search and predict on validation set\n\nNow is time to traverse our previous model list and execute the grid search for all of them. I will save the grid search best result in the final_scores list.","be52cfb1":"Let's remove the duplicates. Given the few amount of duplicates per label, it is expected that the dataset without dups will still be imbalance","442799b9":"There are 13 duplicate rows. Let's now check duplicates per label","87f28a94":"Let's use sklearn standar scaler to scale our features","f6904a31":"Now the data is ready to be injected into the ML models","84ab6781":"Now, let's create the models with the grid params that will be used to tune each model. I'm saving all of the models in a list that I will traverse in next steps.","ed3b06d3":"As expected, the dataset without dups is still imbalanced.\n\nThere is no need to impute data since there are not null values in the dataset. Let's proceed to the next step","125569e2":"ROC_AUC metric determines how good is the model by making a relationship between true positive rate and false positive rate. In an ideal dataset, and AUC of area 1 will define a perfect model. However this value doesn't get affected by imbalance datasets. In the other hand, F1 is a good metric to determine the estimators' perfomance for imbalance datasets given it is calculated by the weighted average of the precision and recall. That said, we could determine the best model as the one with highest F1 and if there is a tie, use ROC_AUC to break the draw.\n\nAccording to the previous table, CatBoost with no stratify dataset will be the winner. Let's determine the winner with few lines of code","891b0daa":"Let's check how imbalance is the dataset","9349de65":"### 1.2. Exploring the data\n\nThe goal of this section is to understand the info we read in previous section and determine whether we have to execute additional operations on the dataset such as cleaning, imputation, etc","124fd250":"- We are dealing with a dataset of 2126 rows and 22 columns\n- The fetal_health column (last column in the dataset) holds the dataset labels. The rest of the columns will be used as the features\n- All of the columns' data types are float64 (numeric)\n- There are not null values in the dataset! (which is very unusual! but good news for a data scientist)"}}