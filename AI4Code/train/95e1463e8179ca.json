{"cell_type":{"0cd3b7c1":"code","3498656f":"code","ee58ad3c":"code","a079d47d":"code","52294882":"code","90840458":"code","ac38a08d":"code","b25e5f4c":"code","14da8760":"code","2e9f8729":"code","ea7925eb":"code","b21bf936":"code","8d54ef4b":"code","35998d3f":"code","7707d9ff":"code","53dcb55e":"code","1ac8d3b2":"code","576befa7":"code","6de13efa":"code","f63d0fe7":"code","32e2965f":"code","3d471266":"code","33aef2d7":"code","656fdf70":"code","5f6c3897":"code","7436e814":"code","69b3f336":"code","68756072":"code","f5a5c6fe":"code","2ecfe5ad":"code","3a1afb6a":"code","23a4d599":"code","c1dc5ce9":"code","19406f32":"markdown"},"source":{"0cd3b7c1":"# DataFrame\nimport pandas as pd\n\n# Matplot\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Scikit-learn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\nfrom sklearn.manifold import TSNE\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.utils import shuffle\n\n# Keras\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential, Model\nfrom keras.layers import Reshape, Input, Activation, Dense, Dropout, Embedding, Flatten, Conv1D, MaxPool1D, LSTM, Bidirectional\nfrom keras import utils\nfrom keras.callbacks import ReduceLROnPlateau, EarlyStopping\nfrom tensorflow.keras.regularizers import l2\n\n# nltk\nimport nltk\nfrom nltk.corpus import stopwords\nfrom  nltk.stem import SnowballStemmer\n\n# Word2vec\nimport gensim\n\n# Utility\nimport re\nimport numpy as np\nimport os\nfrom collections import Counter\nimport logging\nimport time\nimport pickle\nimport itertools\n\n# Set log\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)","3498656f":"nltk.download('stopwords')","ee58ad3c":"# DATASET\nDATASET_COLUMNS = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\nDATASET_ENCODING = \"ISO-8859-1\"\nTRAIN_SIZE = 0.8\n\n# TEXT CLENAING\nTEXT_CLEANING_RE = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n\n# WORD2VEC \nW2V_SIZE = 300\nW2V_WINDOW = 7\nW2V_EPOCH = 32\nW2V_MIN_COUNT = 10\n\n# KERAS\nSEQUENCE_LENGTH = 300\nEPOCHS = 8\nBATCH_SIZE = 1024\n\n# SENTIMENT\nPOSITIVE = \"POSITIVE\"\nNEGATIVE = \"NEGATIVE\"\nNEUTRAL = \"NEUTRAL\"\nSENTIMENT_THRESHOLDS = (0.4, 0.7)\n\n# EXPORT\nKERAS_MODEL = \"model.h5\"\nWORD2VEC_MODEL = \"model.w2v\"\nTOKENIZER_MODEL = \"tokenizer.pkl\"\nENCODER_MODEL = \"encoder.pkl\"","a079d47d":"df = pd.read_csv('..\/input\/sentiment140\/training.1600000.processed.noemoticon.csv', encoding = DATASET_ENCODING , names=DATASET_COLUMNS)\ndf.head()","52294882":"decode_map = {0: \"NEGATIVE\", 2: \"NEUTRAL\", 4: \"POSITIVE\"}\ndef decode_sentiment(label):\n    return decode_map[int(label)]","90840458":"df.target = df.target.apply(lambda x: decode_sentiment(x))","ac38a08d":"stop_words = stopwords.words(\"english\")\nstemmer = SnowballStemmer(\"english\")","b25e5f4c":"def preprocess(text, stem=False):\n    # Remove link,user and special characters\n    text = re.sub(TEXT_CLEANING_RE, ' ', str(text).lower()).strip()\n    tokens = []\n    for token in text.split():\n        if token not in stop_words:\n            if stem:\n                tokens.append(stemmer.stem(token))\n            else:\n                tokens.append(token)\n    return \" \".join(tokens)","14da8760":"df.text = df.text.apply(lambda x: preprocess(x))","2e9f8729":"x_train, x_test, y_train, y_test = train_test_split(df.text, df.target, test_size=0.2, random_state=42)\nprint(\"TRAIN size:\", x_train.shape, y_train.shape)\nprint(\"TEST size:\", x_test.shape, y_test.shape)","ea7925eb":"encoder = LabelEncoder()\nencoder.fit(y_train)\n\ny_train = encoder.transform(y_train)\ny_test = encoder.transform(y_test)\n\ny_train = y_train.reshape(-1,1)\ny_test = y_test.reshape(-1,1)\n\nprint(\"y_train\",y_train.shape)\nprint(\"y_test\",y_test.shape)","b21bf936":"documents = [_text.split() for _text in x_train] ","8d54ef4b":"w2v_model = gensim.models.word2vec.Word2Vec(size=W2V_SIZE, \n                                            window=W2V_WINDOW, \n                                            min_count=W2V_MIN_COUNT, \n                                            workers=8)","35998d3f":"w2v_model.build_vocab(documents)","7707d9ff":"words = w2v_model.wv.vocab.keys()\nvocab_size = len(words)\nprint(\"Vocab size:\", vocab_size)","53dcb55e":"w2v_model.train(documents, total_examples=len(documents), epochs=W2V_EPOCH)","1ac8d3b2":"w2v_model.most_similar(\"love\")","576befa7":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(x_train)\nvocab_size = len(tokenizer.word_index) + 1\nprint(\"Total words\", vocab_size)","6de13efa":"x_train = pad_sequences(tokenizer.texts_to_sequences(x_train), maxlen=300)\nx_test = pad_sequences(tokenizer.texts_to_sequences(x_test), maxlen=300)","f63d0fe7":"embedding_matrix = np.zeros((vocab_size, W2V_SIZE))\nfor word, i in tokenizer.word_index.items():\n    if word in w2v_model.wv:\n        embedding_matrix[i] = w2v_model.wv[word]\nprint(embedding_matrix.shape)","32e2965f":"from keras.layers import Layer\nimport keras.backend as K\n\nclass attention(Layer):\n    def __init__(self,**kwargs):\n        super(attention,self).__init__(**kwargs)\n\n    def build(self,input_shape):\n        self.W=self.add_weight(name=\"att_weight\",shape=(input_shape[-1],1),initializer=\"normal\")\n        self.b=self.add_weight(name=\"att_bias\",shape=(input_shape[1],1),initializer=\"zeros\")        \n        super(attention, self).build(input_shape)\n\n    def call(self,x):\n        et=K.squeeze(K.tanh(K.dot(x,self.W)+self.b),axis=-1)\n        at=K.softmax(et)\n        at=K.expand_dims(at,axis=-1)\n        output=x*at\n        return K.sum(output,axis=1)\n\n    def compute_output_shape(self,input_shape):\n        return (input_shape[0],input_shape[-1])\n\n    def get_config(self):\n        return super(attention,self).get_config()","3d471266":"embedding_layer = Embedding(vocab_size, W2V_SIZE, weights=[embedding_matrix], input_length=SEQUENCE_LENGTH, trainable=False)","33aef2d7":"def build_bilstm(nclasses, dropout=0.5, hidden_layer = 3, lstm_node = 100, l2_value = 0.01):\n    # Initialize a sequebtial model\n    input = Input(shape = SEQUENCE_LENGTH,)\n    # Add embedding layer\n    embedding_layer = Embedding(vocab_size, W2V_SIZE, weights=[embedding_matrix], input_length=SEQUENCE_LENGTH, trainable=False)(input)\n    # Add Convolution and Pooling layers\n    conv = Conv1D(128, 3, padding='SAME', activation='relu')(embedding_layer)\n    pool = MaxPool1D(pool_size=2)(conv)\n    # Add hidden layers \n    bilstm1 = Bidirectional(LSTM(lstm_node, return_sequences=True, recurrent_dropout=0, recurrent_activation = \"sigmoid\"))(pool)\n    dropout1 = Dropout(dropout)(bilstm1)\n    bilstm2 = Bidirectional(LSTM(lstm_node, return_sequences=True, recurrent_dropout=0, recurrent_activation = \"sigmoid\"))(dropout1)\n    dropout2 = Dropout(dropout)(bilstm2)\n    bilstm3 = Bidirectional(LSTM(lstm_node, return_sequences=True, recurrent_dropout=0, recurrent_activation = \"sigmoid\"))(dropout2)\n    dropout3 = Dropout(dropout)(bilstm3)\n    bilstm4 = Bidirectional(LSTM(lstm_node, return_sequences=True, recurrent_dropout=0, recurrent_activation = \"sigmoid\"))(dropout3)\n    dropout4 = Dropout(dropout)(bilstm4)\n    att_out=attention()(dropout4)\n    # Add the fully connected layer with 256 neurons and relu activation\n    dense = Dense(128, activation='relu', kernel_regularizer=l2(l2_value))(att_out)\n    # Add the output layer with softmax activation since we have 12 classes\n    output = Dense(nclasses, activation='softmax')(dense)\n    model = Model([input],[output])\n    model.summary()\n    # Compile the model using sparse_categorical_crossentropy\n    model.compile(loss='sparse_categorical_crossentropy',\n                  optimizer='adam',\n                  metrics=['accuracy'])\n    return model","656fdf70":"model = build_bilstm(2)","5f6c3897":"# from keras.callbacks import ModelCheckpoint\n# #es = EarlyStopping(monitor='val_loss')\n# history = model.fit(x_train, y_train,\n#                     validation_split=0.1,\n#                     epochs=10,\n#                     batch_size=128,\n#                     callbacks=[ModelCheckpoint(filepath=\"weights.best.hdf5\",\n#                                               save_weights_only=True,\n#                                               monitor='val_accuracy',\n#                                               mode='max',\n#                                               save_best_only=True)],\n#                     verbose=1)","7436e814":"model.load_weights(\"..\/input\/cz4042-bilstm-cnn-attention\/weights.best.hdf5\")","69b3f336":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n \nepochs = range(len(acc))\n \nplt.plot(epochs, acc, 'b', label='Training acc')\nplt.plot(epochs, val_acc, 'r', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\n \nplt.figure()\n \nplt.plot(epochs, loss, 'b', label='Training loss')\nplt.plot(epochs, val_loss, 'r', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n \nplt.show()","68756072":"decode_map = {0: \"NEGATIVE\", 2: \"NEUTRAL\", 1: \"POSITIVE\"}\ndef decode_sentiment(label):\n    return decode_map[int(label)]","f5a5c6fe":"%%time\ny_pred_1d = []\ny_test_1d = list(y_test)\nscores = model.predict(x_test, verbose=1)","2ecfe5ad":"y_pred_1d = [decode_sentiment(np.argmax(score)) for score in scores]\ny_test_1d = [decode_sentiment(score) for score in y_test_1d]","3a1afb6a":"def plot_confusion_matrix(cm, classes,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n\n    cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title, fontsize=30)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=90, fontsize=22)\n    plt.yticks(tick_marks, classes, fontsize=22)\n\n    fmt = '.2f'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label', fontsize=25)\n    plt.xlabel('Predicted label', fontsize=25)","23a4d599":"%%time\n\ncnf_matrix = confusion_matrix(y_test_1d, y_pred_1d)\nplt.figure(figsize=(12,12))\nplot_confusion_matrix(cnf_matrix, classes=['POSITIVE','NEGATIVE'], title=\"Confusion matrix\")\nplt.show()","c1dc5ce9":"print(classification_report(y_test_1d, y_pred_1d))","19406f32":"# Word2Vec"}}