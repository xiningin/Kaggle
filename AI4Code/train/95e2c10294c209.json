{"cell_type":{"0fc6f3e0":"code","6ea2dc5f":"code","dd0c275f":"code","21f87b0a":"code","785b6aeb":"code","eba23335":"code","0a1e5001":"code","d4193e52":"code","95d62c30":"code","0259ed04":"code","b8ecf0bc":"code","f07f9da1":"code","7babe8f7":"code","e5d46f14":"code","d9d67645":"code","53f45110":"code","453feee1":"code","1d778c28":"code","da0d1537":"code","ef9f73b3":"code","6b48f717":"code","ab2ee138":"code","47912a67":"code","2bae0b7e":"code","3a11ce97":"markdown","585c363a":"markdown","6f94ec03":"markdown","a5d69227":"markdown","941917b3":"markdown","91137c4c":"markdown"},"source":{"0fc6f3e0":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport os\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn import svm\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV\n\n%matplotlib inline","6ea2dc5f":"#main_df = pd.read_csv('..\/input\/titanicdataset-traincsv\/train.csv')\nmain_df = pd.read_csv('..\/input\/titanic\/train.csv')\nunmodified_df = main_df\ntest_df = pd.read_csv('..\/input\/titanic\/test.csv')\nmain_df.sample(5)","dd0c275f":"#FORMATTING TRAINING SET\nmain_df.columns=['Passengerid', 'survived', 'pclass', 'name', 'sex', 'age', 'sibsp',\n       'parch', 'ticket', 'fare', 'cabin', 'embarked']\n\n#FORMATTING TESTING SET\ntest_df.columns=['Passengerid','pclass', 'name', 'sex', 'age', 'sibsp',\n       'parch', 'ticket', 'fare', 'cabin', 'embarked']\n\nmain_df.info()","21f87b0a":"for dataframe in [main_df, test_df]:\n    label_status = LabelEncoder()\n\n    dataframe.loc[:,'contains_mr'] = dataframe.loc[:,'name'].str.lower().str.contains('|'.join(['mr','mister']))\n    dataframe.loc[:,'contains_mrs'] = dataframe.loc[:,'name'].str.lower().str.contains('|'.join(['mrs']))\n    dataframe.loc[:,'contains_ms'] = dataframe.loc[:,'name'].str.lower().str.contains('|'.join(['ms','miss','mlle','mme']))\n    dataframe.loc[:,'contains_master'] = dataframe.loc[:,'name'].str.lower().str.contains('|'.join(['master']))\n    dataframe.loc[:,'contains_sir'] = dataframe.loc[:,'name'].str.lower().str.contains('|'.join(['sir','jonkheer','col','major','don']))\n    dataframe.loc[:,'contains_rev'] = dataframe.loc[:,'name'].str.lower().str.contains('|'.join(['rev','reverend']))\n    dataframe.loc[:,'contains_lady'] = dataframe.loc[:,'name'].str.lower().str.contains('|'.join(['lady','dona','the countess']))\n    dataframe.loc[:,'contains_dr'] = dataframe.loc[:,'name'].str.lower().str.contains('|'.join(['dr','doctor']))\n    dataframe.loc[:,'contains_col'] = dataframe.loc[:,'name'].str.lower().str.contains('|'.join(['col']))\n\n\n    dataframe['contains_mr'] = label_status.fit_transform(dataframe['contains_mr'])\n    dataframe['contains_mrs'] = label_status.fit_transform(dataframe['contains_mrs'])\n    dataframe['contains_ms'] = label_status.fit_transform(dataframe['contains_ms'])\n    dataframe['contains_master'] = label_status.fit_transform(dataframe['contains_master'])\n    dataframe['contains_sir'] = label_status.fit_transform(dataframe['contains_sir'])\n    dataframe['contains_rev'] = label_status.fit_transform(dataframe['contains_rev'])\n    dataframe['contains_lady'] = label_status.fit_transform(dataframe['contains_lady'])\n    dataframe['contains_dr'] = label_status.fit_transform(dataframe['contains_dr'])\n    dataframe['contains_col'] = label_status.fit_transform(dataframe['contains_col'])","785b6aeb":"main_df['cabin']","eba23335":"main_df.info()","0a1e5001":"for dataset in [main_df, test_df]:\n\n    # Feature Engineering\n\n    # Joining all family members together\n    dataset['family'] = dataset.loc[:,'sibsp'] + dataset.loc[:,'parch'] + 1\n\n    # Filling missing fare values and assigning var to missing fare\n    dataset['age'] = dataset.loc[:,'age'].fillna(dataset['age'].median()+.01)\n    dataset['missing_age'] = dataset.loc[:,'age']==28.01\n    dataset['fare'] = dataset.loc[:,'fare'].fillna(dataset['fare'].median())\n    dataset['cabin_pp'] = dataset['cabin'].fillna('x')\n    dataset['cabin_pp'] = dataset.loc[:,'cabin_pp'].apply(lambda x: x[0])\n    dataset['missing_cabin'] = dataset.loc[:,'cabin_pp']=='x'\n    dataset['embarked'] = dataset['embarked'].fillna('S')\n    dataset['age'] = dataset['age'].fillna(dataset['age'].median())\n    dataset['ticket_cn'] = np.where(dataset.ticket != '1601',0,1)\n    \n    dataset.loc[:,'no_fam'] = dataset.loc[:,'family'].astype(int)==1\n    dataset.loc[:,'fam_less_than_4'] = ((dataset.loc[:,'family'].astype(int)>1) & (dataset.loc[:,'family'].astype(int)<4))\n    dataset.loc[:,'fam_greater_than_4'] = dataset.loc[:,'family'].astype(int)>4\n    \n    # converts range into simple encoded 0,1 scalar\n    label_status = LabelEncoder()\n    dataset['sex'] = label_status.fit_transform(dataset['sex'])\n    dataset['embarked'] = label_status.fit_transform(dataset['embarked'])\n    dataset['cabin_pp'] = label_status.fit_transform(dataset['cabin_pp'])\n    dataset['missing_age'] = label_status.fit_transform(dataset['missing_age'])\n    dataset['missing_cabin'] = label_status.fit_transform(dataset['missing_cabin'])\n    dataset['ticket_cn'] = label_status.fit_transform(dataset['ticket_cn'])\n\n    dataset['no_fam'] = label_status.fit_transform(dataset['no_fam'])\n    dataset['fam_less_than_4'] = label_status.fit_transform(dataset['fam_less_than_4'])\n    dataset['fam_greater_than_4'] = label_status.fit_transform(dataset['fam_greater_than_4'])\n    \nmain_df.sample(5)","d4193e52":"corr = main_df.corr()\ncorr.style.background_gradient(cmap='coolwarm')","95d62c30":"test_df.sample(5)","0259ed04":"for dataset in [main_df, test_df]:\n    dataset = dataset.select_dtypes('number').dropna() \n\ny = main_df['survived']\nX = main_df.drop(['survived','sibsp','parch','name','cabin','ticket',], axis=1)\ntest_df_sample = test_df.drop(['name','sibsp','parch','cabin','ticket',], axis=1)","b8ecf0bc":"#split data into test and train\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=(2**32 - 1))","f07f9da1":"#weights of int\/floats important\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","7babe8f7":"dtc = DecisionTreeClassifier(criterion='entropy',max_depth=3,)\ndtc.fit(X_train,y_train)\npred_dtc = dtc.predict(X_test)\n#Lets see how they preformed!\nprint(classification_report(y_test, pred_dtc))\nprint(confusion_matrix(y_test, pred_dtc))\naccuracy = dtc.score(X_test, y_test)\nprint(f'Decision Tree Classifier Accuracy: {accuracy}')","e5d46f14":"rfc = RandomForestClassifier(n_estimators=200, criterion='entropy') #best for medium-sized datasets # max_depth=10\nrfc.fit(X_train, y_train)\n\n# TEST THE TRAINING DATA\npred_rfc_train = rfc.predict(X_train)\npred_rfc = rfc.predict(X_test)\npred_rfc_final = rfc.predict(test_df_sample)\n#Lets see how they preformed!\nprint(classification_report(y_test, pred_rfc))\nprint(confusion_matrix(y_test, pred_rfc))\naccuracy = rfc.score(X_test, y_test)\nprint(f'Random Forest Classifier Accuracy: {accuracy}')","d9d67645":"pred_rfc_final = rfc.predict(test_df_sample)\nsubmission = pd.DataFrame({'PassengerId':test_df['Passengerid'],'Survived':pred_rfc_final})\nsubmission.to_csv('.\/horrigan_submission_rfc_v2.csv')","53f45110":"submission","453feee1":"lrc = LogisticRegression()\nlrc.fit(X_train,y_train)\npred_lrc = lrc.predict(X_test)\npred_lrc_final = lrc.predict(test_df_sample)\n#Lets see how they preformed!\nprint(classification_report(y_test, pred_lrc))\nprint(confusion_matrix(y_test, pred_lrc))\naccuracy = lrc.score(X_test, y_test)\nprint(f'Logistic Regression Classifier Accuracy: {accuracy}')\n\nsubmission = pd.DataFrame({'PassengerId':test_df['Passengerid'],'Survived':pred_lrc_final})\nsubmission.to_csv('.\/horrigan_submission_lrc.csv')","1d778c28":"svm = svm.SVC() # best on smaller numbers\nsvm.fit(X_train, y_train)\npred_svm=svm.predict(X_test)\n\ntest_df_sample = sc.transform(test_df_sample)\npred_svm_final = svm.predict(test_df_sample)   \n     \nsubmission = pd.DataFrame({'PassengerId':test_df['Passengerid'],'Survived':pred_svm_final})\nsubmission.to_csv('.\/horrigan_submission_svm.csv')\n\nprint(classification_report(y_test, pred_svm))\nprint(confusion_matrix(y_test, pred_svm))\naccuracy = svm.score(X_test, y_test)\nprint(f'Support Vector Machine Classifier Accuracy: {accuracy}')","da0d1537":"mlpc = MLPClassifier(hidden_layer_sizes=(5,5,5),max_iter=500)\nmlpc.fit(X_train, y_train)\npred_mlpc=mlpc.predict(X_test)\nprint(classification_report(y_test, pred_mlpc))\nprint(confusion_matrix(y_test, pred_mlpc))\naccuracy = mlpc.score(X_test, y_test)\nprint(f'Neural Network Classifier Accuracy: {accuracy}')","ef9f73b3":"from sklearn.metrics import accuracy_score\nsvm_score = accuracy_score(y_test, pred_svm)\nrfc_score = accuracy_score(y_test, pred_rfc)\nmlpc_score = accuracy_score(y_test, pred_mlpc)\ndtc_score = accuracy_score(y_test,pred_dtc)\n\nprint(f'Support Vector Machine Classifier: {svm_score}')\nprint(f'Random Forest Classifier: {rfc_score}')\nprint(f'Neural Network Classifier: {mlpc_score}')\nprint(f'Decision Tree Classifier: {dtc_score}')","6b48f717":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nmodels = [\n    ('Logistic Regression Classifier:', LogisticRegression()),\n    #('Naive Bayes Gaussian NB:', GaussianNB()),\n    ('Support Vector Machine Classifier:', SVC()),\n    ('KNeighbors Classifier:', KNeighborsClassifier()),\n    ('Decision Tree Classifier:', DecisionTreeClassifier()),\n    ('Neural Network Classifier',MLPClassifier(hidden_layer_sizes=(10,10,10),max_iter=600)),\n    ('Random Forest Classifier',RandomForestClassifier(n_estimators=200, criterion='entropy',)),]\n\nfor dataset_name, dataset, label in [('UNMODIFIED',unmodified_df,'survived'),('FEATURE ENGINEERED SET',main_df,'survived')]:\n    dataset=dataset.select_dtypes('number').dropna()\n    y = np.array(dataset[label])\n    X = np.array(dataset.drop(label, axis=1))\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=(2**32 - 1))\n    sc = StandardScaler()\n    X_train = sc.fit_transform(X_train)\n    X_test = sc.transform(X_test)\n    print(dataset_name)\n    for name, model in models:\n        clf = model\n        clf.fit(X_train, y_train)\n        accuracy = clf.score(X_test, y_test)\n        print(name, accuracy)\n    \n    print('------ BREAK -------')","ab2ee138":"#model = RandomForestClassifier(n_estimators=100,)\n\n#n_estim = range(100,1000,100)\n#criterion = ['entropy','gini']\n\n#param_grid = {\"n_estimators\" :n_estim,'criterion':criterion}\n#model_rfc = GridSearchCV(model,param_grid = param_grid, cv=5, scoring=\"accuracy\", n_jobs=4, verbose = 1)\n#model_rfc.fit(X_train,y_train)\n\n# Best score\n#print(model_rfc.best_score_)\n\n#best estimator\n#model_rfc.best_params_","47912a67":"#model_rfc.best_params_\ndf = pd.read_csv('.\/horrigan_submission_svm.csv')\ndf","2bae0b7e":"# filled NA = extra column telling it is filled\n# actual OHE\n# tree-based feature engineering can cause more noise","3a11ce97":"# Random Forest Classifier","585c363a":"# Logistic Regression","6f94ec03":"# TRAIN TEST SPLIT","a5d69227":"# DecisionTreeClassifier","941917b3":"# Neural Networks","91137c4c":"# SVM Classifier"}}