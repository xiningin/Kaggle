{"cell_type":{"1f77bd74":"code","29611f98":"code","2856bd31":"code","a8d38250":"code","93dee913":"code","1273cbf7":"code","e7b7bc6b":"code","e4ec4853":"code","8899c25d":"code","afc7ecf2":"code","bca09a46":"code","624d5d8c":"code","0ffaa782":"code","aa4327db":"code","d70381b6":"code","1a9d8dfd":"code","6022c503":"code","6a8c50fd":"code","f23f2dfb":"code","506bff1d":"code","71ad8765":"code","069c56c1":"code","3d5b9f94":"code","d611036c":"code","520dac51":"code","b70de5ad":"code","e857b40c":"code","8abe36c1":"code","3002e238":"code","1733160d":"code","c7445ecc":"code","c45c88bf":"markdown","1e600d68":"markdown","ed461c43":"markdown","ca54a185":"markdown","a8473503":"markdown","55d391d3":"markdown","29ec3d22":"markdown","01d43836":"markdown"},"source":{"1f77bd74":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport h5py\nimport scipy\nfrom PIL import Image\nfrom scipy import ndimage\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, auc, roc_auc_score, precision_recall_curve\nfrom sklearn.metrics import confusion_matrix","29611f98":"df_train = pd.read_csv('..\/input\/digit-recognizer\/train.csv')\ndf_test = pd.read_csv('..\/input\/digit-recognizer\/test.csv')\n\n\n#Training data\nprint('Training data shape: ', df_train.shape)\nprint('Test data shape: ', df_test.shape)","2856bd31":"df_train.head()","a8d38250":"# Lets make it a binary classification problem. Our aim is to determine whether the digit is 1 or not. \n\ndf_train['label'] = np.where(df_train['label'] == 1, 1, 0) ","93dee913":"# Sigmoid function\ndef sigmoid(x):\n    s = (1 + np.exp(-x))**-1\n    return s","1273cbf7":"X = df_train.iloc[:,1:]\ny = df_train.loc[:,['label']]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)","e7b7bc6b":"X_train_arr = X_train.T.values\nX_test_arr = X_test.T.values","e4ec4853":"index = 9\nplt.imshow(X_train_arr[:,index].reshape((28,28)))","8899c25d":"X_train_st = X_train_arr\/255.\nX_test_st = X_test_arr\/255.","afc7ecf2":"X_train_st.shape","bca09a46":"# Dimensions of arrays\nm_train = X_train_st.shape[1]\nm_test = X_test_st.shape[1]\nn = X_train_st.shape[0]\n\nprint(m_train)\nprint(m_test)\nprint(n)","624d5d8c":"print(X_train_st.shape)\nprint(X_test_st.shape)\nprint(y_train.shape)\nprint(y_test.shape)","0ffaa782":"y_train = np.ravel(y_train).reshape(1,m_train)\ny_test = np.ravel(y_test).reshape(1,m_test)","aa4327db":"# Initialize the weights\n#def initialize(X_train_st):\nw = np.zeros(n).reshape(n,1)\nb = 0\n#    return w, b","d70381b6":"w.shape","1a9d8dfd":"# Get the linear combinaiton of input data & weights\nZ = np.dot(w.T, X_train_st)\nZ.shape","6022c503":"A = sigmoid(Z)\nA.shape","6a8c50fd":"y_train.shape","f23f2dfb":"J = -(1\/m_train) * ( np.dot(y_train, A.T)  +   np.dot( (1-y_train), np.log(1- A.T)) )\nJ","506bff1d":"def eval_metrics(y_val,y_pred):\n  print('Accuracy: {:.2f}'.format(accuracy_score(y_val, y_pred)))\n  print('Precision: {:.2f}'.format(precision_score(y_val, y_pred)))\n  print('Recall: {:.2f}'.format(recall_score(y_val, y_pred)))\n  print('F1: {:.2f}'.format(f1_score(y_val, y_pred)))\n  print('AUC: {:.2f}'.format(roc_auc_score(y_val, y_pred)))","71ad8765":"def initialize(n):\n    w = np.zeros(n).reshape(n,1)\n    b = 0\n    return w, b","069c56c1":"def forward_prop(X_train_st, y_train, w,b, m_train):\n    # Get the linear combinaiton of input data & weights\n    Z = (np.dot(w.T, X_train_st) + b)\n    A = sigmoid(Z)\n    J = -(1\/m_train) * ( np.dot(y_train, A.T)  +   np.dot( (1-y_train), np.log(1- A.T)) )\n    return Z, A, J","3d5b9f94":"def backward_prop(X_train_st,y_train, w, b, A, m_train, learning_rate):\n    dw = (1\/m_train) * (np.dot(X_train_st, (A-y_train).T ))\n    db = (1\/m_train) * (np.sum(A-y_train))\n    w = w - (learning_rate * dw)\n    b = b - (learning_rate * db)\n    return w, b","d611036c":"def optimize(X_train_st, y_train, w, b, m_train, num_iter, learning_rate):\n    for i in np.arange(num_iter):\n        print(\"This is iteration number: \", i)\n        Z, A, J = forward_prop(X_train_st, y_train, w, b, m_train)\n        print(\"Loss is :\", J)\n        w, b = backward_prop(X_train_st,y_train, w, b, A, m_train, learning_rate)\n    return w, b   ","520dac51":"def train_model(X_train_st, y_train, m_train, n, learning_rate, num_iter):\n    w, b = initialize(n)\n    w, b = optimize(X_train_st, y_train, w, b, m_train, num_iter, learning_rate)\n    return w, b","b70de5ad":"learning_rate = 0.01\nnum_iter = 200\nw, b = train_model(X_train_st, y_train, m_train, n, learning_rate, num_iter)","e857b40c":"def predict(X_test_st,w,b):\n    y_pred = sigmoid(np.dot(w.T, X_test_st))\n    y_pred = np.where(y_pred > 0.5, 1, 0)\n    return y_pred","8abe36c1":"y_pred = predict(X_train_st,w,b)","3002e238":"eval_metrics(y_test.T, y_pred.T)","1733160d":"confusion_matrix(y_train.T, y_pred.T)","c7445ecc":"confusion_matrix(y_test.T, y_pred.T)","c45c88bf":"[Link to Problem statement](https:\/\/www.kaggle.com\/c\/digit-recognizer)","1e600d68":"In this notebook, we implement a logistic regression as described by the Andrew Ng in his Neural Networks and Deep Learning Course on Coursera. We take the digits dataset and convert it into a binary classification problem so that we can practice the concepts explained in chapter 2 of the course. ","ed461c43":"Standardize the dataset.","ca54a185":"Ensure that the shape of the arrays is as per theory taught in the lectures.","a8473503":"Split the data into train and test sets.","55d391d3":"We observe that the shape of labels is of the type (m, 1) where as it should be (1, m).","29ec3d22":"# Logistic Regression as a Neural Network: Based on Andrew Ng's Lecture","01d43836":"Here our images are 28x28 pixels. This means that each image has 784 pixels and we have one value associated with each pixel. So, we have 784 features for each image. Therefore, we will have 784 weights and a bias term. Our weights will be a vector of shape (784, 1), Lets initialize our weights vector with zeros and the bais to zero as well."}}