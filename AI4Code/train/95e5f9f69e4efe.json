{"cell_type":{"fab609e6":"code","09861187":"code","bfb9093b":"code","69e4966e":"code","533d8098":"code","49d12ee4":"code","334dfe1f":"code","4926a8b4":"code","275016d9":"code","330fab28":"code","0b3d6b89":"code","687a2f61":"code","6409a418":"code","557ae981":"code","4c3ec69d":"code","31891ee4":"code","ee1ca85c":"code","a01ab206":"code","9686369d":"code","e9f0c863":"code","7c0c897a":"code","04f3858f":"code","26835284":"code","91a38026":"code","cb6da5fa":"code","44f676da":"code","c66a36e4":"code","c87b46f5":"code","5c0c8567":"code","d8149799":"code","51211a4b":"code","716e65bf":"code","d1354a3f":"code","1f17ebea":"code","6c0825a8":"code","c63b827e":"code","2edf6699":"code","511a0923":"code","eaa18e56":"code","46bbd69a":"code","dcea3aac":"code","a2926bbe":"code","99f2cced":"code","ebf26e70":"code","c8aa5e01":"code","a93d2f47":"code","c6efe0ef":"code","c1ffefd8":"markdown","a585618f":"markdown","2f9afd2e":"markdown","8bb75bbb":"markdown","5225e9c0":"markdown","81654b87":"markdown","49391f55":"markdown","39fd9846":"markdown","218cb7bf":"markdown","1050d0b5":"markdown","256862a0":"markdown","031861b3":"markdown","416f08f3":"markdown","fc23ae85":"markdown","1d089d98":"markdown","d0684dff":"markdown","965e8c80":"markdown","c4c8595d":"markdown","7fbd93f6":"markdown","e1bf344a":"markdown","3748c30e":"markdown","0f5093bb":"markdown","26be6025":"markdown","6c9e0f8c":"markdown"},"source":{"fab609e6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # plotting\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","09861187":"# read csv, use error_bad_lines=False because there are some errors\ndf = pd.read_csv('\/kaggle\/input\/goodreadsbooks\/books.csv', error_bad_lines=False, index_col='bookID')","bfb9093b":"# print head of data\ndf.head()","69e4966e":"df.info()","533d8098":"df.describe()","49d12ee4":"df.columns","334dfe1f":"df.where(df == 102).sum()","4926a8b4":"# get all unique values for \"language_code\"\nlist_of_lang = df['language_code'].unique()\nprint(list_of_lang)","275016d9":"# how many in each\nlangs = df['language_code'].value_counts()\nprint(langs)","330fab28":"import seaborn as sns\n\n\n# Set the width and height of the figure\nplt.figure(figsize=(14,6))\n\n# Bar chart showing average arrival delay for Spirit Airlines flights by month\n#sns.barplot(x=flight_data.index, y=flight_data['NK'])\nsns.barplot(x=langs.index, y=langs)","0b3d6b89":"# replace all en-XX with eng\ndf_better = df.copy()\ndf_better.replace(to_replace=('en-US', 'en-GB', 'en-CA'), value='eng',inplace=True)\ndf_better['language_code'].value_counts()","687a2f61":"df_better.where(df_better == 102).sum()","6409a418":"# rename the oddly named column\ndf_better.rename(columns = {'  num_pages':'num_pages'}, inplace = True) \nprint(df_better.columns)","557ae981":"df.where(df == 102).sum()","4c3ec69d":"# using seaborn\nfig, ax = plt.subplots(figsize=(16, 4))\nsns.histplot(df_better['num_pages'],  bins=60, kde=True)\nplt.show()","31891ee4":"# visualize skewedness\nfrom scipy import stats\nstats.probplot(df_better['num_pages'], plot=plt)","ee1ca85c":"# complete duplicates\ndf_better.duplicated().sum()","a01ab206":"# only title duplicates\ndf_better.duplicated(subset = 'title').sum()","9686369d":"# show ranking of title duplicates\ndf_better['title'].value_counts()[:10]","e9f0c863":"# only authors duplicates\ndf_better.duplicated(subset = 'authors').sum()","7c0c897a":"# show ranking of author duplicates\ndf_better['authors'].value_counts()[:10]","04f3858f":"# only isbn duplicates\nduple_isbn = df_better.duplicated(subset = 'isbn13')\nduple_isbn.sum()","26835284":"df_better.info()","91a38026":"#only use sensible columns for correlation\ndf_better_corr = df_better.select_dtypes(exclude=['object']).copy()\ndf_better_corr.drop(['isbn13'], axis=1, inplace=True)","cb6da5fa":"df_better_corr.info()","44f676da":"# get correlations and show heatmap\ncorr=df_better_corr.corr(method='pearson')\nsns.heatmap(data=corr, annot=True)","c66a36e4":"# mega scatterplot\nsns.pairplot(df_better_corr, height = 2)","c87b46f5":"#other graphic\nsns.jointplot(x=\"average_rating\", y=\"num_pages\", data = df_better, kind='reg')","5c0c8567":"# same without outliers\nsns.jointplot(x=\"average_rating\", y=\"num_pages\", data = df_better[df_better.num_pages < 1000], color = 'darkcyan', kind='reg')","d8149799":"# define variables\nm = 100\n\n# calculate C (assuming this has to be done before dropping rows according to m)\nC = df_better['average_rating'].mean()\nprint('mean vote across report', C)","51211a4b":"# define weighted rating function\ndef WR(R, v, m, C):\n    WR = (v\/(v+m))*R+(m\/(v+m))*C\n    return WR","716e65bf":"# find out how many ratings below 3000\nm_out = df_better['ratings_count'].where(df_better['ratings_count'] > m).isna()\nm_out_sum = m_out.sum()\nprint('books below m ratins:', m_out_sum)","d1354a3f":"# drop rows with ratings<m\n# This will make all nan that is below m\ndf_better_WR = df_better.copy()\ndf_better_WR['ratings_count'].where(df_better['ratings_count'] > m, inplace=True)\n# drops rows with any nan\ndf_better_WR.dropna(axis=0, how='any', inplace=True)\ndf_better_WR.shape","1f17ebea":"# add column with WR\n\n# get a list with all WR\nWR_list = WR(df_better['average_rating'], df_better['ratings_count'], m, C)\n# add list as column\ndf_better_WR['WR'] = WR_list","6c0825a8":"# confirm new column\ndf_better_WR.head()","c63b827e":"# list top three according to WR:\ndf_better_WR.nlargest(3, 'WR', keep='all')","2edf6699":"# check for nan\ndf_better_WR.isnull().values.any()","511a0923":"from sklearn.model_selection import train_test_split\n\n# Create X and y\ny = df_better_WR['average_rating'].copy()\nX = df_better_WR[['num_pages', 'ratings_count', 'text_reviews_count']].copy()\n\n# Break off validation set from training data\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)","eaa18e56":"from sklearn.metrics import mean_absolute_error\nfrom xgboost import XGBRegressor\n\n\n# Define the model\nmy_model = XGBRegressor(random_state=0, n_estimators=500, learning_rate=0.1) \n\n# Fit the model\nmy_model.fit(X_train, y_train, \n             early_stopping_rounds=5, \n             eval_set=[(X_valid, y_valid)], \n             verbose=False)\n\n\n# Get predictions\npredictions = my_model.predict(X_valid)\n\n# Calculate MAE\nmae = mean_absolute_error(predictions, y_valid)  \n\n# Uncomment to print MAE\nprint(\"Mean Absolute Error:\" , mae)","46bbd69a":"## create dictionary from author's number of books\nauthor_dict = df_better_WR['authors'].value_counts().to_dict()","dcea3aac":"# add column to DF according to dict\ndf_better_WR['no_books_author'] = df_better_WR['authors'].map(author_dict)\n# add this column to X\nX2 = X.copy()\nX2['no_books_author'] = df_better_WR['no_books_author'].copy()\ny2 = y.copy()","a2926bbe":"X2.head()","99f2cced":"# new training\n\n# Break off validation set from training data\nX_train2, X_valid2, y_train2, y_valid2 = train_test_split(X2, y2, train_size=0.8, test_size=0.2, random_state=0)\n\n# Define the model\nmy_model2 = XGBRegressor(random_state=0, n_estimators=500, learning_rate=0.1)\n\n# Fit the model\nmy_model2.fit(X_train2, y_train2, \n             early_stopping_rounds=5, \n             eval_set=[(X_valid2, y_valid2)], \n             verbose=False)\n\n\n# Get predictions\npredictions2 = my_model2.predict(X_valid2)\n\n# Calculate MAE\nmae2 = mean_absolute_error(predictions2, y_valid2) \n\n# Uncomment to print MAE\nprint(\"Mean Absolute Error:\" , mae2)","ebf26e70":"df_better_WR['publication_date']","c8aa5e01":"# convert last 4 strings of publication date to int for new feature 'publication year'\ndf_better_WR['publication_year'] = df_better_WR['publication_date'].map(lambda x: x[-4:])\ndf_better_WR['publication_year'] = df_better_WR['publication_year'].astype('int32') \n  \n# create new X\nX3 = X2.copy()\nX3['publication_year'] = df_better_WR['publication_year']\ny3 = y.copy()","a93d2f47":"# do the machine learning stuff\n\nX_train3, X_valid3, y_train3, y_valid3 = train_test_split(X3, y3, train_size=0.8, test_size=0.2, random_state=0)\nmy_model3 = XGBRegressor(random_state=0, n_estimators=500, learning_rate=0.1)\nmy_model3.fit(X_train3, y_train3, \n             early_stopping_rounds=5, \n             eval_set=[(X_valid3, y_valid3)], \n             verbose=False)\npredictions3 = my_model3.predict(X_valid3)\n\n# Calculate MAE\nmae3 = mean_absolute_error(predictions3, y_valid3) \n\n# print MAE\nprint(\"Mean Absolute Error:\" , mae3)","c6efe0ef":"print(mae, mae2, mae3)","c1ffefd8":"It seems unnecessary that there so many 'different' english languages, so we change all of them to just eng","a585618f":"# 3. Convert en-US and en-GB in eng","2f9afd2e":"### 9.2.2 Include year","8bb75bbb":"# 5. Explore number of pages in Histogram","5225e9c0":"# 9. Rating predictions","81654b87":"For some reason, there are 2 spaces in 'num_pages'. We want to get rid of them.","49391f55":"### 9.2.1 Number of books per author","39fd9846":"## 9.1 First approach","218cb7bf":"# 7. Correlations??","1050d0b5":"# 1. Start","256862a0":"## 9.2 Feature Engineering","031861b3":"First idea is to create a new feature from the author data, like number of books per author","416f08f3":"even better!","fc23ae85":"Improved only slightly","1d089d98":"# Data Exploration and more for the goodreads data","d0684dff":"The data indeed is skewed.","965e8c80":"# 8. add a better rating system (WR)","c4c8595d":"data seems skewed, let's visualize it better","7fbd93f6":"# 6. are there any duplicates??","e1bf344a":"# 2. What types of languages are there?","3748c30e":"The MAE got better with every added feature.","0f5093bb":"We cannot use WR here because it depends on the number of ratings","26be6025":"# 4. change '  num_pages' to 'num_pages'","6c9e0f8c":"This rating system takes into account the number of ratings. So a book with one single 5.0 rating won't be the best:\n\nWeighted rating (WR) = (v \u00f7 (v+m)) \u00d7 R + (m \u00f7 (v+m)) \u00d7 C , where:\n\n* R = average for the movie (mean) = (Rating)\n* v = number of votes for the movie = (votes)\n* m = minimum votes required to be listed in the Top 250 (currently 3000)\n* C = the mean vote across the whole report (currently 6.9)\n\nfrom: https:\/\/stats.stackexchange.com\/questions\/6418\/rating-system-taking-account-of-number-of-votes\n"}}