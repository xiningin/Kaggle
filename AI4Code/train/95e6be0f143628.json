{"cell_type":{"768e7f5b":"code","388783ff":"code","00dcdde4":"code","421d80e7":"code","71bafa61":"code","e772432a":"code","8c8f4fcf":"code","b8140636":"code","c449a3d9":"code","9428d404":"code","acd94d1b":"code","8e80f7cd":"markdown","f67daac1":"markdown"},"source":{"768e7f5b":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","388783ff":"sample_submission = pd.read_csv(\"..\/input\/sample_submission.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")\ntrain = pd.read_csv(\"..\/input\/train.csv\")","00dcdde4":"print(train.info())\ndisplay(train)","421d80e7":"print(test.info())\ndisplay(test)","71bafa61":"train.describe()","e772432a":"test.describe()","8c8f4fcf":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.heatmap(train.corr(), annot=True, linewidth=0.2)\nfig=plt.gcf()\nfig.set_size_inches(20,20)\nplt.show()","b8140636":"Y = train['DEFCON_Level']\n\nX = train.drop(['DEFCON_Level', 'ID'], axis=1)\nX['Closest_Threat_log'] = np.log(X['Closest_Threat_Distance(km)'])\nX['Troops_Mobilized_log'] = np.log(X['Troops_Mobilized(thousands)'])\n\ntest['Closest_Threat_log'] = np.log(test['Closest_Threat_Distance(km)'])\ntest['Troops_Mobilized_log'] = np.log(test['Troops_Mobilized(thousands)'])\n\ncombined = pd.concat([X, test.drop(['ID'], axis=1)], axis=0)\ndisplay(combined)","c449a3d9":"from tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input, Multiply\nfrom tensorflow.keras.optimizers import Adam, SGD, Adagrad\nfrom tensorflow.keras.regularizers import l1_l2\nfrom tensorflow.keras.initializers import he_normal\nfrom sklearn.model_selection import train_test_split as tts\nfrom keras import backend as K\n\ndef xavier(shape, dtype=None):\n    return np.random.rand(*shape)*np.sqrt(1\/(17))\ndef output_w(shape, dtype=None):\n    return np.ones(shape)*np.array([0, 0.5, 0.22, 0.22, 0.05, 0.01])\n\ndef c_model(shape, lr=0.001):\n    i = Input(shape)\n    x = Dense(100, activation='relu', kernel_initializer=xavier)(i)\n    x = Dense(100, activation='relu', kernel_initializer=xavier)(x)\n    x = Dense(100, activation='relu', kernel_initializer=xavier)(x)\n    o = Dense(6, activation='softmax', kernel_initializer=xavier)(x)\n    \n    opt = Adam(lr=lr, amsgrad=True)\n    #opt = SGD(lr=lr, momentum=0.25, nesterov=True)\n    #opt = Adagrad(lr=lr)\n    x = Model(inputs=i, outputs=o)\n    x.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n    return x","9428d404":"from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\nlrr = ReduceLROnPlateau(monitor = 'val_accuracy',\n                         patience = 7,\n                         verbose = 1,\n                         factor = 0.5,\n                         min_lr = 1e-5)\n\nes = EarlyStopping(monitor='val_loss',\n                   mode='min',\n                   verbose=1,\n                   patience=50,\n                   restore_best_weights=True)\n\nepochs = 2000\nbatch_size = 128\n\nfrom sklearn.preprocessing import StandardScaler as ss\nscale = ss()\nscale.fit(combined)\n\ntx, vx, ty, vy = tts(scale.transform(X), Y, test_size=0.25, random_state=121)\nmodel = c_model(tx.shape[1:], 0.003)\n\nhistory = model.fit(tx, ty, validation_data=(vx, vy),\n                    epochs=epochs, batch_size=batch_size,\n                    verbose=2, callbacks=[lrr, es])","acd94d1b":"model.evaluate(scale.transform(X), Y)\npred = np.argmax(model.predict(scale.transform(test[X.columns])), axis=1)\nn = len(Y)\/100\nprint([(x, sum(Y==x), sum(Y==x)\/n) for x in range(1, 6)])\nn = len(pred)\/100\nprint([(x, sum(pred==x), sum(pred==x)\/n) for x in range(1, 6)])\n\npredictions = pd.DataFrame()\npredictions['ID'] = test['ID']\npredictions['DEFCON_Level'] = pred\npredictions.to_csv(\"submission.csv\", index=False)","8e80f7cd":"# Peek","f67daac1":"#These are the percentage of the data per category after trying to peek:\n- 1 - 64.44800%\n- 2 - 21.15930%\n- 3 - 23.09266%\n- 4 - 2.870800%\n- 5 - 0.060880%"}}