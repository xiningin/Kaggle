{"cell_type":{"b0ddea8c":"code","a4600a42":"code","b4747e0e":"code","30a0d334":"code","5ec6e0ab":"code","f0d38948":"code","9d73555e":"code","c457424d":"code","6723e7e7":"code","1b09f1c5":"code","66b0e2c4":"code","96abf5eb":"code","dd062963":"code","93c7cf58":"code","743b1e28":"code","4e251a78":"code","4c422f6a":"code","6b6499bb":"code","7631cb50":"code","fa18b78b":"code","0255c940":"code","3e993c2c":"code","e72977ef":"code","6ab49b51":"code","9e6ccd05":"code","41538c40":"code","58f2d8ee":"code","03f5d342":"code","6b938569":"markdown","f05e3284":"markdown","3a685ad9":"markdown","9e3ab364":"markdown","2f4863be":"markdown","32764fad":"markdown","34e1ef47":"markdown","398bf295":"markdown","c62b2b2a":"markdown","b05fdc4e":"markdown","74dd7e37":"markdown","fd6c7233":"markdown","7a7ed4b1":"markdown","47234ef7":"markdown","ce115c57":"markdown","fcc645cf":"markdown","5f617f30":"markdown"},"source":{"b0ddea8c":"import os\nos.chdir(\"..\/input\")\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndata = pd.read_csv(\"creditcard.csv\")\ndata.head()","a4600a42":"data.shape","b4747e0e":"f_variables = data.iloc[:,0:30]\nf_variables.dtypes","30a0d334":"data['Class'].dtypes","5ec6e0ab":"#f_variables.isnull().any\nf_variables.describe()\n#by observing count values, all the variables have same count which means no missing values","f0d38948":"#f_variables.isnull().values.any() gives if there any missing values(true\/false)\n#f_variables.isnull().sum() gives number of missing values by each column","9d73555e":"### There are no missing values in the data ","c457424d":"from numpy import percentile\nQ1 = f_variables.quantile(0.25)\nQ3 = f_variables.quantile(0.75)\nIQR = Q3-Q1\n#print(\"quartiles are \",Q3,Q1)\nprint(IQR)","6723e7e7":"import matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nplt.figure(figsize = (15,100))\ngs = gridspec.GridSpec(10,3)\nfor i,cn in enumerate(f_variables.columns):\n    ax = plt.subplot(gs[i])\n    sns.boxplot(f_variables[cn])\n    ax.set_xlabel('')\n    ax.set_title('feature: ' + str(cn))\nplt.show() ","1b09f1c5":"lower_band = IQR - 1.5*Q1\nupper_band = IQR +1.5*Q3\nf_variables.clip(lower = lower_band, upper = upper_band, axis = 1)\nf_variables.shape","66b0e2c4":"#9.1\nf_variables.hist(figsize = (20,20))\nplt.show()","96abf5eb":"data['Class'].value_counts()","dd062963":"sns.countplot(x = 'Class', data= data)","93c7cf58":"##10.1)\ncorr = data.corr()\ncorr","743b1e28":"plt.figure(figsize = (30,10))\nsns.heatmap(corr,xticklabels = corr.columns, yticklabels = corr.columns)","4e251a78":"plt.figure(figsize = (15,100))\ngs = gridspec.GridSpec(10,3)\nfor i, c in enumerate(f_variables.columns):\n    plt.subplot(gs[i])\n    plt.scatter(f_variables['V1'], f_variables[c])\n    plt.xlabel(\"V1\")\n    plt.ylabel(\"feature Variable:\"+str(c))","4c422f6a":"import numpy as np\ncolors = np.where(data['Class']==0,'red','green')","6b6499bb":"plt.figure(figsize = (15,100))\ngs = gridspec.GridSpec(10,3)\nfor i, c in enumerate(f_variables.columns):\n    plt.subplot(gs[i])\n    plt.scatter(f_variables['V1'], f_variables[c], c = colors)\n    plt.xlabel(\"V1\")\n    plt.ylabel(\"feature Variable:\" +str(c))\nplt.show()","7631cb50":"f_variables['Class'] = data['Class']\nf_variables.head()","fa18b78b":"split1 = int(0.8*len(f_variables))\nsplit2 = int(0.9*len(f_variables))\ntrain = f_variables[:split1]\nvalidation = data[split1:split2]\ntest = data[split2:]","0255c940":"x_train = train.drop('Class', axis = 1)\ny_train = train['Class']\nx_validation = validation.drop('Class', axis =1)\ny_validation = validation['Class']\nx_test = test.drop('Class', axis = 1)\ny_test = test['Class']","3e993c2c":"from sklearn.ensemble import RandomForestRegressor\nrf = RandomForestRegressor(n_estimators = 40, random_state = 10)\nrf.fit(x_train, y_train)","e72977ef":"pred = rf.predict(x_validation)\npred","6ab49b51":"from sklearn.metrics import accuracy_score\nscore = accuracy_score(y_validation, pred.round())\nscore","9e6ccd05":"from sklearn.tree import DecisionTreeClassifier\nclf = DecisionTreeClassifier()\nclf.fit(x_train, y_train)\n","41538c40":"c_pred = clf.predict(x_validation)\nscore = accuracy_score(y_validation, c_pred)\nscore","58f2d8ee":"from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\nlogreg.fit(x_train, y_train)","03f5d342":"log_pred = logreg.predict(x_validation)\nscore = accuracy_score(y_validation, log_pred)\nscore","6b938569":"### Q8) Treat outliers. What is your strategy?","f05e3284":"### Q9) Pick each one of the feature variables and perform univariate analysis (be as creative as possible in your analysis)\n* ### Q9.1) Visualize the shape of the distribution of data.Is every feature variable normally distributed? Why is normal distribution important for data?\n* ### Q9.2) Is the data distribution skewed? If highly skewed,do you still find outliers which you did not treat?\n* ### Q9.3) Draw box and whiskers plot of each of the feature variables\n* ### Q9.4) How do the distributions look in terms of variation? Which features are widely spread and which are kind of concentrated towards the mean?","3a685ad9":"### Q1) Read the dataset into the notebook","9e3ab364":"### Q5) Check for null values in the feature variables","2f4863be":"### Q10) Pick the feature variables and perform bi-variate analysis (be as creative as possible)\n* ### Q10.1) Try creating correlation matrices. See if there are variables which are strongly or weakly related\n* ### Q10.2) Try build joint distribution charts\n* ### Q10.3) If there are variables showing high correlation, what corrective action is needed? Why is this a matter of concern? What if we do not treat the variables showing high degree of correlation?","32764fad":"### Q11.1) What is the type of machine learning problem at hand? (Supervised or Unsupervised?) Why?\n### Q11.2) What is the category of the machine learning problem at hand? (Classification or Regression?) Why?","34e1ef47":"### Q6) Treat the null variables. What is your strategy? Why did you use that? What other strategies could be taken? Explain","398bf295":"### Q7) Check for outliers in the feature variables","c62b2b2a":"### Q3) List out the feature variables and their data-types","b05fdc4e":"### Q4) List out response variable and its data type","74dd7e37":"11.1) It is a Supervised learning problem, because in supervised learning for a given input \nwe have an output values, here we are predicting the class outcome by giving input fields.\n\n11.2) It is a Classification problem, because the response variable is in terms of yes or no i.e. 1 or 0, \nwhere as in regression type of problems, we will predict the continuous variable.","fd6c7233":" ### Q13.1) List down all the algorithms known to you which you think might be applicable in this case?\nDecision Trees\nRandom Forest\nKNN\nLogistic Regression\nSVM","7a7ed4b1":"# ** What do we plan to do here? **\n* Read the dataset\n* Analyze the data for missing values and outliers\n* Perform uni-variate analysis\n* Perform bi-variate analysis\n* We will not do any feature engineering in this particular problem\n* We will create a lot of visualizations to do a thorough analysis of the problem at hand\n* Pick a list of algorithms which we can choose to apply in this case\n* Pick the best algorithm\n* Score the algorithm based on the evaluation criteria\n* Fine tune algorithms to achieve the best possible value of the evaluation metric","47234ef7":"### checking the class distribution\n","ce115c57":"### Q2) Print the shape of the data","fcc645cf":"### Q14) Pick each of the algorithm and perform the below steps : \n### Q14.1) Split your data between test, train and validation steps. Why 3 and not just test and train? \n### Q14.2) Build your model\n### Q14.3) List down the evaluation metrics you would use to evaluate the performance of the model?\n### Q14.4) Evaluate the model on training data\n### Q14.5) Predict the response variables for the validation test data\n### Q14.6) Evaluate the model on test data\n### Q14.7) How are the two scores? Are they significantly different? Are they the same? Is the test score better than training score?","5f617f30":"### Q12.1) Draw univariate plots for each of the feature variables, color each plotted point as red if the class value = 0 else green.\n### Q12.2) Which feature segregates the data the cleanest way? How would you calculate the misclassification rate?\n### Q12.3) Now take two features at a time, again color each plotted point as mentioned in 12.1. Calculate and comment on the misclassification rate?"}}