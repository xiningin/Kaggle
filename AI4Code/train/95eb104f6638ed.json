{"cell_type":{"e1614016":"code","5308dfc0":"code","96006cd5":"code","35de444d":"code","00fa7463":"code","37424cca":"code","a4726c81":"code","bfe3824e":"code","dba3e020":"code","a10e6deb":"code","52734358":"code","49e78fb8":"code","ab07c7fd":"markdown","bc06440b":"markdown","d680c11b":"markdown","20a88453":"markdown","d6b6a217":"markdown","84a1af1e":"markdown","b232fc09":"markdown","f0348f61":"markdown","ec71e371":"markdown","a18a3921":"markdown","dfac84d0":"markdown"},"source":{"e1614016":"! pip -q install transformers","5308dfc0":"from transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n\nmodel_size = \"small\" \n\ntokenizer = AutoTokenizer.from_pretrained(f\"microsoft\/DialoGPT-{model_size}\")\nmodel = AutoModelForCausalLM.from_pretrained(f\"microsoft\/DialoGPT-{model_size}\")","96006cd5":"def chat(model, tokenizer, trained=False):\n    print(\"type \\\"q\\\" to quit. Automatically quits after 5 messages\")\n\n    for step in range(5):\n        message = input(\"MESSAGE: \")\n\n        if message in [\"\", \"q\"]:  # if the user doesn't wanna talk\n            break\n\n        # encode the new user input, add the eos_token and return a tensor in Pytorch\n        new_user_input_ids = tokenizer.encode(message + tokenizer.eos_token, return_tensors='pt')\n\n        # append the new user input tokens to the chat history\n        bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n\n        # generated a response while limiting the total chat history to 1000 tokens, \n        if (trained):\n            chat_history_ids = model.generate(\n                bot_input_ids, \n                max_length=1000,\n                pad_token_id=tokenizer.eos_token_id,  \n                no_repeat_ngram_size=3,       \n                do_sample=True, \n                top_k=100, \n                top_p=0.7,\n                temperature = 0.8, \n            )\n        else:\n            chat_history_ids = model.generate(\n                bot_input_ids, \n                max_length=1000, \n                pad_token_id=tokenizer.eos_token_id,\n                no_repeat_ngram_size=3\n            )\n\n        # pretty print last ouput tokens from bot\n        print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))\n\nchat(model, tokenizer)","35de444d":"import glob, logging, os, pickle, random, re, torch, pandas as pd, numpy as np\nfrom typing import Dict, List, Tuple\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\nfrom tqdm.notebook import tqdm, trange\nfrom pathlib import Path\nfrom transformers import (\n    AdamW,\n    AutoConfig,\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    PreTrainedModel,\n    PreTrainedTokenizer,\n    get_linear_schedule_with_warmup,\n)\n\ntry:\n    from torch.utils.tensorboard import SummaryWriter\nexcept ImportError:\n    from tensorboardX import SummaryWriter\n\nlogger = logging.getLogger(__name__)\n\n# Args to allow for easy convertion of python script to notebook\nclass Args():\n    def __init__(self):\n        self.output_dir = f'output-{model_size}'\n        self.model_type = 'gpt2'\n        self.model_name_or_path = f'microsoft\/DialoGPT-{model_size}'\n        self.config_name = f'microsoft\/DialoGPT-{model_size}'\n        self.tokenizer_name = f'microsoft\/DialoGPT-{model_size}'\n        self.cache_dir = 'cached'\n        self.block_size = 512\n        self.per_gpu_train_batch_size = 4\n        self.gradient_accumulation_steps = 1\n        self.learning_rate = 5e-5\n        self.weight_decay = 0.0\n        self.adam_epsilon = 1e-8\n        self.max_grad_norm = 1.0\n        self.num_train_epochs = 20  # 3\n        self.max_steps = -1\n        self.warmup_steps = 0\n        self.logging_steps = 1000\n        self.save_total_limit = None\n        self.seed = 42\n        self.local_rank = -1\n\nargs = Args()","00fa7463":"df = pd.read_csv(\"..\/input\/harry-potter-final-data\/final_data.csv\")\n\n'''contexted = []\nn = 7\n\nfor i in range(n, len(data['line'])):\n  row = []\n  prev = i - 1 - n\n  for j in range(i, prev, -1):\n    row.append(data['line'][j])\n  contexted.append(row) \n\ncolumns = ['response'] + ['context '+str(i+1) for i in range(n)]\ndf = pd.DataFrame.from_records(contexted, columns=columns)'''\ndf.head(5)","37424cca":"def construct_conv(row, tokenizer, eos = True):\n    flatten = lambda l: [item for sublist in l for item in sublist]\n    conv = list(reversed([tokenizer.encode(x) + [tokenizer.eos_token_id] for x in row]))\n    conv = flatten(conv)\n    return conv\n\ndef load_and_cache_examples(args, tokenizer, df_trn):\n    return ConversationDataset(tokenizer, args, df_trn)\n\ndef set_seed(args):\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n\nclass ConversationDataset(Dataset):\n    def __init__(self, tokenizer: PreTrainedTokenizer, args, df, block_size=512):\n\n        block_size = block_size - (tokenizer.model_max_length - tokenizer.max_len_single_sentence)\n        directory = args.cache_dir\n        cached_features_file = os.path.join(directory, args.model_type + \"_cached_lm_\" + str(block_size))\n\n        logger.info(\"Creating features from dataset file at %s\", directory)\n        self.examples = []\n        for _, row in df.iterrows():\n            conv = construct_conv(row, tokenizer)\n            self.examples.append(conv)\n\n        logger.info(\"Saving features into cached file %s\", cached_features_file)\n        with open(cached_features_file, \"wb\") as handle:\n            pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\n    def __len__(self):\n        return len(self.examples)\n\n    def __getitem__(self, item):\n        return torch.tensor(self.examples[item], dtype=torch.long)","a4726c81":"def train(args, train_dataset, model: PreTrainedModel, tokenizer: PreTrainedTokenizer) -> Tuple[int, float]:\n    if args.local_rank in [-1, 0]:\n        tb_writer = SummaryWriter()\n\n    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n\n    def collate(examples: List[torch.Tensor]):\n        if tokenizer._pad_token is None:\n            return pad_sequence(examples, batch_first=True)\n        return pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id)\n\n    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n    train_dataloader = DataLoader(\n        train_dataset, sampler=train_sampler, batch_size=args.train_batch_size, collate_fn=collate, drop_last = True\n    )\n\n    t_total = len(train_dataloader) \/\/ args.gradient_accumulation_steps * args.num_train_epochs\n    model = model.module if hasattr(model, \"module\") else model  # Take care of distributed\/parallel training\n    model.resize_token_embeddings(len(tokenizer))\n\n    # Prepare optimizer and schedule (linear warmup and decay)\n    no_decay = [\"bias\", \"LayerNorm.weight\"]\n    optimizer_grouped_parameters = [\n        {\n            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n            \"weight_decay\": args.weight_decay,\n        },\n        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n    ]\n    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n    )\n\n    logger.info(\"*** Running trainng, Num examples = %d, Num Epochs = %d ***\", len(train_dataset), args.num_train_epochs)\n\n    global_step, epochs_trained = 0, 0\n    tr_loss, logging_loss = 0.0, 0.0\n\n    model.zero_grad()\n    train_iterator = trange(\n        epochs_trained, int(args.num_train_epochs), desc=\"Epoch\", disable=args.local_rank not in [-1, 0]\n    )\n    set_seed(args)  # Added here for reproducibility\n    for _ in train_iterator:\n        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=args.local_rank not in [-1, 0])\n        for step, batch in enumerate(epoch_iterator):\n            \n            inputs, labels = (batch, batch)\n            if inputs.shape[1] > 1024: continue\n            inputs = inputs.to(args.device)\n            labels = labels.to(args.device)\n            model.train()\n            outputs = model(inputs, labels=labels)\n            loss = outputs[0]  # model outputs are always tuple in transformers (see doc)\n\n            if args.gradient_accumulation_steps > 1:\n                loss = loss \/ args.gradient_accumulation_steps\n\n            loss.backward()\n\n            tr_loss += loss.item()\n            if (step + 1) % args.gradient_accumulation_steps == 0:\n                torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n                optimizer.step()\n                scheduler.step()  # Update learning rate schedule\n                model.zero_grad()\n                global_step += 1\n\n                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n                    # Log metrics\n                    tb_writer.add_scalar(\"lr\", scheduler.get_lr()[0], global_step)\n                    tb_writer.add_scalar(\"loss\", (tr_loss - logging_loss) \/ args.logging_steps, global_step)\n                    logging_loss = tr_loss\n\n    tb_writer.close()\n\n    return global_step, tr_loss \/ global_step","bfe3824e":"def main(df_trn):\n    args = Args()\n    \n    # Setup CUDA, GPU & distributed training\n    device = torch.device(\"cuda\")\n    args.n_gpu = torch.cuda.device_count()\n    args.device = device\n\n    # Setup logging\n    logging.basicConfig(\n        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n        datefmt=\"%m\/%d\/%Y %H:%M:%S\",\n        level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN,\n    )\n    logger.warning(\"Process rank: %s, device: %s, n_gpu: %s\", args.local_rank, device, args.n_gpu)\n\n    set_seed(args) # Set seed\n\n    config = AutoConfig.from_pretrained(args.config_name, cache_dir=args.cache_dir)\n    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, cache_dir=args.cache_dir)\n    model = AutoModelForCausalLM.from_pretrained(args.model_name_or_path, from_tf=False, config=config, cache_dir=args.cache_dir)\n    model.to(args.device)\n    \n    # Training\n    train_dataset = load_and_cache_examples(args, tokenizer, df_trn)\n    global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n    logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n\n    # Saving best-practices: if you use save_pretrained for the model and tokenizer, you can reload them using from_pretrained()\n    os.makedirs(args.output_dir, exist_ok=True)\n\n    logger.info(\"Saving model checkpoint to %s\", args.output_dir)\n    model_to_save = (model.module if hasattr(model, \"module\") else model)  # Take care of distributed\/parallel training\n    model_to_save.save_pretrained(args.output_dir)\n    tokenizer.save_pretrained(args.output_dir)\n\n    # Good practice: save your training arguments together with the trained model\n    torch.save(args, os.path.join(args.output_dir, \"training_args.bin\"))\n\n    # Load a trained model and vocabulary that you have fine-tuned\n    model = AutoModelForCausalLM.from_pretrained(args.output_dir)\n    tokenizer = AutoTokenizer.from_pretrained(args.output_dir)\n    model.to(args.device)","dba3e020":"df=pd.read_csv(\"..\/input\/harry-potter-final-data\/final_data.csv\")","a10e6deb":"main(df)","52734358":"tokenizer = AutoTokenizer.from_pretrained(f'microsoft\/DialoGPT-{model_size}')\nmodel = AutoModelForCausalLM.from_pretrained(f'output-{model_size}')\nchat(model, tokenizer, trained=True)","49e78fb8":"model.save_pretrained(\"Saved_model_1\")","ab07c7fd":"# Training\n\nNow, this is quite a hefty chunk of code but don't worry you don't need to understant it yet, we can cover this in later tutorials","bc06440b":"# Lets Run it!\nThis should take around 5 minutes so you might as well go grab a cup of coffee \u2615\ufe0f","d680c11b":"# Chatting with the trained bot","20a88453":"### Formatting the data and defining some helper functions\nWe need to construct the data in the right format so the bot can interpret it properly. To do this we're adding special characters like the 'end of string' charater\n","d6b6a217":"\nWe want the model to be aware of previous messages from the dialogue to help it decide what to say next. Here we modify the dataset to include context from 7 previous messages.","84a1af1e":"## Configuring the model","b232fc09":"### Install the Huggingface transformers module","f0348f61":"## Import DialoGPT\nDialoGPT is a chatbot model made by microsoft. This will be the base for our RickBot.","ec71e371":"## Chat with the untrained model","a18a3921":"# Main Runner\n\nHere we're simply setting up the logger and starting the training!","dfac84d0":"# Gather the training data\n\nWe're using some rick and morty scripts from this [article](https:\/\/www.kaggle.com\/andradaolteanu\/sentiment-analysis-rick-and-morty-scripts\/#1.-Data-%F0%9F%93%81) by [Andrada Olteanu](https:\/\/www.kaggle.com\/andradaolteanu)  \\(the data can be found [here](https:\/\/www.kaggle.com\/andradaolteanu\/rickmorty-scripts)\\)"}}