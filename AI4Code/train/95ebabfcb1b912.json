{"cell_type":{"78ab916d":"code","d76d6209":"code","892721f6":"code","db579792":"code","cfd9ce44":"code","12cb9cfc":"code","c868cda1":"code","027d738f":"code","50019eaf":"code","de69cce8":"code","a63e2809":"code","2005927e":"code","548a2fe0":"code","e4938dd7":"code","7ba18b3d":"code","b8e7259f":"code","8d2558c4":"code","017698bc":"code","72a13bbd":"code","2e83a3ea":"code","5b02f1a9":"code","5bf3c487":"code","3b88ad2f":"code","b2663a34":"code","af72a536":"code","43951ed0":"code","d918b7a1":"code","d59cd238":"code","f21dcfee":"code","ca500d46":"code","0b2bb066":"code","6ee7d70a":"code","83dfbf92":"code","489e8e32":"code","c20d0028":"code","5eb46012":"code","8025d863":"code","c3d139bc":"code","d946d591":"code","0bb452de":"code","b2a68ece":"code","0945c3da":"markdown","b49351a2":"markdown","9e9b98b5":"markdown","ea98719e":"markdown","99c2baf7":"markdown","7322952b":"markdown","385f37a3":"markdown","d5ce88d6":"markdown","05516735":"markdown","5d5ecc18":"markdown","fb9b29b3":"markdown","20cd35b1":"markdown","f90af004":"markdown","a65254b3":"markdown","28bd1d98":"markdown","7be39b7e":"markdown","45fe5376":"markdown","46bba81c":"markdown","ba096f36":"markdown","d6495998":"markdown","4067d720":"markdown","2e1e6fc1":"markdown","4753b43f":"markdown","98942676":"markdown","e80d990e":"markdown","2d7e4e58":"markdown","5530b80a":"markdown","ab70ac68":"markdown","7348e070":"markdown","40f4af92":"markdown","966e53ca":"markdown","467f5988":"markdown","7860c831":"markdown","f120cde6":"markdown","c24e83e5":"markdown"},"source":{"78ab916d":"# import the library\nimport gc\nimport numpy as np\nimport pandas as pd\nimport os\nimport time\nprint(f'numpy version: {np.__version__}')\nprint(f'pandas version: {pd.__version__}')","d76d6209":"%%time\npath = \"\/kaggle\/input\/nfl-big-data-bowl-2021\/\"\n# I am using a function to avoid any kind of additional unnecassary variable - helps in RAM saving\n# As this creates a new scope for the intermediate variables \n# and removes them automatically when the interpreter exits the function.\n# Note: I am using 50e5 rows because of memory limitation on Kaggle platform\ndef load_data():\n    weekly_data = [csv for csv in os.listdir(path) if csv[:5] == \"week1\"]\n    # selecting data for all the weeks having \"1\" in week name and using 20e5 rows due to the memory limitation of Kaggle notebook.\n    # As only 16 gigs is allowed to use.\n    dataframe = pd.DataFrame()\n    for files in weekly_data:\n        df = pd.read_csv(filepath_or_buffer = \"\/kaggle\/input\/nfl-big-data-bowl-2021\/%s\"%files, nrows=3000000)\n        dataframe = pd.concat([dataframe,df])\n    return dataframe[:]\n\ndataframe = load_data()","892721f6":"%%time\n# Saving data in CSV Format \ndataframe.to_csv(\"\/kaggle\/working\/csv_data.csv\",index = False) ","db579792":"%%time\n# Reading data in CSV Format \ncsv_data = pd.read_csv(\"\/kaggle\/working\/csv_data.csv\")","cfd9ce44":"del dataframe #we won't use this dataframe so i am deleting those to free up some space.\ndel csv_data\ngc.collect() # collect garbage value from the memory","12cb9cfc":"%%time\n# let's define a function to avoid creating unnecessary variable\ndef load_data_chunk(filepath, chunksize =500000, iterator = True, engine = 'c', memory_map = True):\n    chunk_df = [x for x in pd.read_csv(filepath_or_buffer = filepath,nrows=3000000, chunksize=chunksize, iterator=iterator, engine=engine, memory_map= memory_map)]\n    chunk_data = pd.concat(chunk_df,sort=False)\n    return chunk_data\n\nchunk_data = load_data_chunk(filepath= \"\/kaggle\/working\/csv_data.csv\")","c868cda1":"del chunk_data # we won't use this dataframe so i am deleting those to free up some space.\ngc.collect() # collect garbage value from the memory","027d738f":"# !pip install dask[complete] --> for complete installation\n!pip install dask # installing dask\n# import dask dataframe\nimport dask.dataframe as dd","50019eaf":"%%time\n# Reading data in CSV Format using Dask Dataframe \ndask_csv_data = dd.read_csv(urlpath= \"\/kaggle\/working\/csv_data.csv\")\ndask_csv_data.head(3)","de69cce8":"dask_csv_data.columns","a63e2809":"dask_csv_data.shape","2005927e":"del dask_csv_data\ngc.collect()","548a2fe0":"%%time\n# Reading data in CSV Format using Dask Dataframe \ndask_csv_data = dd.read_csv(urlpath= \"\/kaggle\/working\/csv_data.csv\", blocksize= 64e6).compute()  # 64MB chunks \ndask_csv_data.head(3)","e4938dd7":"%%time\n# Storing data in CSV Format - using dask\ndask_csv_data.to_csv(\"\/kaggle\/working\/dask_data.csv\",index = False)","7ba18b3d":"del dask_csv_data # free up some space.\ngc.collect() # collect garbage value from the memory","b8e7259f":"!pip install datatable # install datatable \nimport datatable as dt # import datatable\nprint(f'Version: {dt.__version__}')","8d2558c4":"%%time\nframe = dt.fread(\"csv_data.csv\")","017698bc":"frame.head(2)","72a13bbd":"# %%time\n# pandas_df = frame.to_pandas()\n# pandas_df = dt.fread(\"csv_data.csv\").to_pandas() # we can combine both the above steps\n# pandas_df.head(2)","2e83a3ea":"%%time\n# save data to .jay format\nframe.to_jay(\"data_table.jay\")","5b02f1a9":"%%time\n#read the .jay data foramt\ndt.open(\"data_table.jay\").head(2) # we would get Frame\n# -- we can easily covert frames to pandas dataframe directly\n# jay_data = dt.open(\"data_table.jay\").to_pandas()","5bf3c487":"#del pandas_df# free up some space.\ndel frame\ngc.collect() # collect garbage value from the memory","3b88ad2f":"!pip install modin[dask] # Install Modin dependencies and Dask to run on Dask\n\n# Initialize all cores- MODIN-DASK Engine\n# import os - Uncomment this if in use.Already imported in top cell.\nos.environ[\"MODIN_ENGINE\"] = \"dask\"\nfrom distributed import Client\nclient = Client()\nimport modin.pandas as mpd","b2663a34":"%%time\n# using default pandas method to prepare data\ndataframe = load_data() # function load_data defined above","af72a536":"%%time\n# changing the default pandas(pd) method by modin.pandas(mpd)\npath = \"\/kaggle\/input\/nfl-big-data-bowl-2021\/\"\n# I am using a function to avoid any kind of additional unnecassary variable - helps in RAM saving\n# As this creates a new scope for the intermediate variables \n#  and removes them automatically when the interpreter exits the function.\ndef load_data_modin():\n    weekly_data = [csv for csv in os.listdir(path) if csv[:5] == \"week1\"]\n    # selecting data for all the weeks having \"1\" in week name due to the memory limitation of Kaggle notebook.\n    # As only 16 gigs is allowed to use.\n    dataframe = mpd.DataFrame()\n    for files in weekly_data:\n        df = mpd.read_csv(filepath_or_buffer = \"\/kaggle\/input\/nfl-big-data-bowl-2021\/%s\"%files)\n        dataframe = mpd.concat([dataframe,df])\n    return dataframe\n\ndataframe_mpd = load_data_modin()","43951ed0":"%%time\n# Storing merged data in CSV Format \ndataframe_mpd.to_csv(\"\/kaggle\/working\/modin_csv_data.csv\",index = False)","d918b7a1":"%%time\n# Reading data in CSV Format \ncsv_data = mpd.read_csv(\"\/kaggle\/working\/csv_data.csv\")","d59cd238":"del csv_data\ngc.collect()","f21dcfee":"%%time\n# We would be using pandas library\nsave = {} # creating an dict obj to store the data saving time\nread = {} # creating an dict obj to save the data reading time\n\n# load data\n# dataframe_pandas  = load_data() and change to csv\n# pandas \"dataframe\" is already defined above so we would be using this here.\n\n# save the data\nt1 = time.time() #initial time\ndataframe.to_csv(\"df_csv.csv\", index = False) # save df in .csv format\nsave[\"csv_pd\"] = time.time()-t1 # append the data to \"save\" dict\n\n#load the saved data\nt2 = time.time()\ndf_csv = pd.read_csv(\"df_csv.csv\")\nread[\"csv_pd\"] = time.time()-t2 # append the data to \"read\" dict\n\ndel dataframe","ca500d46":"%%time\n# We would be using modin.pandas library\n# load data\n# dataframe_pandas  = load_data_modin() and change to csv\n# pandas \"dataframe_mpd\" is already defined above so we would be using that here.\n\n#save the data\nt1 = time.time() #initial time\ndataframe_mpd.to_csv(\"df_csv.csv\", index = False) # save dataframe in .csv format\nsave[\"csv_mpd\"] = time.time()-t1 # append the data to \"save\" dict\n\n#load the saved data\nt2 = time.time()\ndf_csv_mpd = mpd.read_csv(\"df_csv.csv\")\nread[\"csv_mpd\"] = time.time()-t2 # append the data to \"read\" dict\n\ndel dataframe_mpd\ndel df_csv_mpd # delete the csv data to free memory space\ngc.collect() # collect garbage value (if any) from the memory","0b2bb066":"%%time\n\n#save the data\nt1 = time.time() #initial time\ndf_csv.to_feather(\"df_csv\") # save df in feather format\nsave[\"feather_pd\"] = time.time()-t1 # append the data to \"save\" dict\n\n#load the saved data\nt2 = time.time()\ndf_feather = pd.read_feather(\"df_csv\", use_threads = True)\nread[\"feather_pd\"] = time.time()-t2 # append the data to \"read\" dict\n\ndel df_feather # delete the csv data to free memory space\ngc.collect() # collect garbage value (if any) from the memory","6ee7d70a":"%%time\n\n#save the data\nt1 = time.time() #initial time\ndf_csv.to_hdf(\"df_csv.h5\", key='hdf', mode='w') # save df in hdf format\nsave[\"hdf_pd\"] = time.time()-t1 # append the data to \"save\" dict\n\n#load the saved data\nt2 = time.time()\ndf_hdf = pd.read_hdf(\"df_csv.h5\", key = 'hdf')\nread[\"hdf_pd\"] = time.time()-t2 # append the data to \"read\" dict\n\ndel df_hdf # delete the csv data to free memory space\ngc.collect() # collect garbage value (if any) from the memory","83dfbf92":"%%time\n\n#save the data\nt1 = time.time() #initial time\ndf_csv.to_parquet(\"df_csv.parquet.gzip\", compression='gzip') # save df in parquet format\nsave[\"parquet_pd\"] = time.time()-t1 # append the data to \"save\" dict\n\n#load the saved data\nt2 = time.time()\ndf_parquet = pd.read_parquet(\"df_csv.parquet.gzip\")\nread[\"parquet_pd\"] = time.time()-t2 # append the data to \"read\" dict\n\ndel df_parquet # delete the csv data to free memory space\ngc.collect() # collect garbage value (if any) from the memory","489e8e32":"%%time\n\n#save the data\nt1 = time.time() #initial time\ndf_csv.to_pickle(\"df_csv.pkl\") # save df in pkl format\nsave[\"pickle_pd\"] = time.time()-t1 # append the data to \"save\" dict\n\n#load the saved data\nt2 = time.time()\ndf_pickle = pd.read_pickle(\"df_csv.pkl\")\nread[\"pickle_pd\"] = time.time()-t2 # append the data to \"read\" dict\n\ndel df_pickle # delete the csv data to free memory space\ngc.collect() # collect garbage value (if any) from the memory","c20d0028":"# Note: pandas-msgpack is not working in kaggle notebook -- facing some installation error\n# but for me it does work in local machine.\n# For the time being, i am leaving the code as it is.\n!pip install pandas-msgpack -U\n# to avoid error we need specific pandas version\n!pip install pandas-compat\n# !pip uninstall --yes pandas\n!pip install --upgrade pandas\nfrom pandas_msgpack import to_msgpack, read_msgpack","5eb46012":"%%time\n\n#save the data\nt1 = time.time() #initial time\nto_msgpack('msgpack_dataframe.msg', df_csv)\nsave[\"msgpack\"] = time.time()-t1 # append the data to \"save\" dict\n\n#load the saved data\nt2 = time.time()\ndf_msgpack = read_msgpack('msgpack_dataframe.msg')\nread[\"msgpack\"] = time.time()-t2 # append the data to \"read\" dict\n\ndel df_msgpack # delete the csv data to free memory space\ngc.collect() # collect garbage value (if any) from the memory","8025d863":"%%time\nprint(f'Version: {dt.__version__}')  #datatable is already installed above\n\n# save to .jay format\nt1 = time.time() #initial time\nframe = dt.fread(\"df_csv.csv\")       #store data in datatable frame \nframe.to_jay(\"data_table.jay\")       # change to .jay format\nsave[\"jay\"] = time.time()-t1         # append the data to \"save\" dict\n\nt2 = time.time()\njay_data = dt.open(\"data_table.jay\")\nread[\"jay\"] = time.time()-t2          # append the data to \"save\" dict\n# to covert to pandas dataframe directly\n# jay_data = dt.open(\"data_table.jay\").to_pandas() # because of limited memory - Commenting.\n\n\ndel frame\ndel jay_data       # delete the csv data to free memory space\ngc.collect()       # collect garbage value (if any) from the memory","c3d139bc":"#select dataframe having numerical dtype (int\/float)\nnumerical_df = df_csv.select_dtypes(include=['int64', 'float64'])\nprint(f\"numerical_df memory info: {numerical_df.memory_usage().sum()\/1024**2} MBs.\")","d946d591":"%%time\n# save numerical data in .npy format\n# np.save('data.npy', dataframe)\nwith open(\"data.npy\", \"wb\") as file:\n    np.save(file, numerical_df)","0bb452de":"%%time\n# read numerical data in .npy format\n# data_array = np.load('data.npy')\n# data = pd.DataFrame(data_array)\nwith open(\"data.npy\", \"rb\") as file:\n    data = np.load(file)\ndf = pd.DataFrame(data)","b2a68ece":"result_df = pd.DataFrame({'SaveTime(in sec)':pd.Series(save),'ReadTime(in sec)':pd.Series(read)})\nresult_df","0945c3da":"We can see from the above results that NPY File format is blazingly fast for reading the numrical data.","b49351a2":"# **C) Using Dask DataFrame**\n> In Layman's term, [dask](https:\/\/docs.dask.org\/en\/latest\/why.html) is basically a parallel computing library where we can analyze csv data without loading the whole csv into the memory and we can perform faster computation by on single machines by leveraging their multi-core CPUs (idle cpu cores) and streaming data efficiently from disk.\n\nA Dask DataFrame is composed of many smaller Pandas DataFrames, split along the index. These Pandas DataFrames may live on disk for larger-than-memory computing on a single machine, or on many different machines in a cluster. \n> The best part about dask or dask dataframe is that we can use this library just like we do use pandas library as the dusk API is built on top of pandas API.\n\nWe can use Dask Library in the following cases:\n* Manipulating large datasets, even when those datasets don\u2019t fit in memory\n* Accelerating long computations by using many cores\n* Distributed computing on large datasets with standard Pandas operations like groupby, join, and time series computations\n\nNote: **Data transformation** can not utilize the perk of parallel computation.","9e9b98b5":"> Note: From now onwards we would use Dask Library instead of Pandas library for the demo.\n\nAs i already mentioned the Dask APIs is a subset of Pandas API, so we can use the same methods like we do use in pandas. Now we will replicate the data saving and loading\/reading (like we did use Pandas above) using Dask library.\n> **Prepare Data:** Instead of \"pd\" we would use \"dd\". Other than this everything would be same below.","ea98719e":"* **Reading the CSV Data**","99c2baf7":"> We can also save the Frame into a **.jay** (binary) format on disk, then open it later instantly, regardless of the data size.","7322952b":"# E) MODIN-DASK\/RAY Library\n[Modin Library](https:\/\/modin.readthedocs.io\/en\/latest\/) is my personal favourite which i do use mostly along with datatable in jupyter notebook. During the process execution(either data manipulation or any kind of operatin) the system generally uses a single core\/cpu and others are in idel state. Why not utilize the idle cores?\n\n> The main idea is that it does utilize all the Idle CPU Cores which helps in computing faster.\n\n* Modin is an early stage DataFrame library that wraps pandas and transparently distributes the data and computation, accelerating your pandas workflows with one line of code change. \n* The user does not need to know how many cores their system has, nor do they need to specify how to distribute the data. \n* In fact, users can continue using their previous pandas notebooks while experiencing a considerable speedup from Modin, even on a single machine. Only a modification of the import statement is needed. Once you\u2019ve changed your import statement, you\u2019re ready to use Modin just like you would pandas, since the API is identical to pandas.\n\n> Modin uses **Ray or Dask** to provide an effortless way to speed up your pandas notebooks, scripts, and libraries.\n* **[Ray](https:\/\/docs.ray.io\/en\/latest\/)** is a fast, simple framework for building and running distributed applications. Ray leverages Apache Arrow for efficient data handling and provides task and actor abstractions for distributed computing.\n* **[Dask](https:\/\/docs.dask.org\/en\/latest\/)** provides advanced parallelism for analytics, enabling performance at scale. Dask focuses more on the data science world, providing higher-level APIs that in turn provide partial replacements for Pandas, NumPy, and scikit-learn, in addition to a low-level scheduling and cluster management framework.\n\nThe main difference between Dask and Ray is *Dask uses a centralized scheduler to share work across multiple cores, while Ray uses distributed bottom-up scheduling.\n\nYou can find this [link](https:\/\/www.datarevenue.com\/en-blog\/pandas-vs-dask-vs-vaex-vs-modin-vs-rapids-vs-ray#:~:text=Dask%20vs.-,Ray,parallel%20across%20clusters%20of%20machines.&text=Dask%20uses%20a%20centralized%20scheduler,uses%20distributed%20bottom%2Dup%20scheduling.) useful for more differences.\n\nNote: Ray does not support Windows, so it will not be possible to install modin[ray] or modin[all].\n* pip install modin[all] # Install all of the above\n* pip install modin[ray] # Install Modin dependencies and Ray to run on Ray","385f37a3":"![image.png](attachment:image.png)\n\nWe can see the results now for the above operation i.e using modin.pandas(mpd). Please note the CPU utilization and time taken. Modin uses all the CPU cores for faster results. You can check the local CPU core performance in task manager just to cross check whether it's working or not.\n\n**Conclusion:** From the above results i.e the time taken (approx 1\/4 of the general approach) and CPU utilization, we can conclude that modin library works blazingly fast.\n\n**Saving and Reading CSV data using modin.pandas**","d5ce88d6":"# B) Using pandas chunksize, engine, iterator and memory_map parameter.\nThe following parameters proved helpful in enhancing the preformance. You can play with them.\n* **chunksize:** Sometimes, while loading\/reading data, because of the system's memory limitation(Limited RAM) we may encounter *MemoryError*.\nWell the good news is we can can read the larger data files in smaller chunks in pandas. pandas **read_csv** has an inbuilt paramenter \"**chunksize**\", which does read the data in chunks. Meaning,the number of rows to be read into a dataframe at any single time in order to fit into the local memory. Using *chunksize* would result in **TextFileReader object** for iteration. We can perform operation on each chunk and concatenate each of them to form a dataframe as shown below to fit into the local memory.\n* **engine:** 'c' and 'python' engines. It's an optional parameter. The names indicate the language in which the parsers are written. Pandas uses the C parser (specified as engine='c') whereever possible, but may fall back to Python if C-unsupported options are specified.\n> *The main intent is to use \"C\" engine whenever we can use because C engine is faster.*\nMore Info: https:\/\/stackoverflow.com\/questions\/52774459\/engines-in-python-pandas-read-csv\n* **iterator:** Produces a series of values over time (TextfileReader objects), rather than computing them at once. Saves memory.\n* **memory_map:** Maps the file object directly onto memory and access the data directly from there.","05516735":"![image.png](attachment:image.png)\nAs you can see results for the above default pd method. Note the CPU utilization and time taken. Below we would be executing the same function but using MODIN.PANDAS.","5d5ecc18":"As we can see above that data saving and data loading really took a lot of time.\nLet's try improve it.","fb9b29b3":"# G2) Feather Format\n[Feather](https:\/\/arrow.apache.org\/docs\/python\/feather.html#:~:text=Feather%20is%20a%20portable%20file,Python%20(pandas)%20and%20R.) is a fast, lightweight, and easy-to-use binary file format for storing data frames.\u201cFeather\u201d provides binary columnar serialization for data frames designed to make efficient reading and writing of data frames. It uses Apache Arrow columnar memory specification to represent binary data on disk. It has a few specific design goals:\n\n* Lightweight, minimal API: make pushing data frames in and out of memory as simple as possible\n* Language agnostic: Feather files are the same whether written by Python or R code. Other languages can read and write Feather files, too. \n* High read and write performance. Whenever possible, Feather operations should be bound by local disk performance.\n\nUseful [Link](https:\/\/blog.rstudio.com\/2016\/03\/29\/feather\/).\n","20cd35b1":"Hello Everyone,\n\nI would like to share my learning on various quick and efficient ways of dealing with the problem of data loading for large datasets.\n\nHope you would like it!\n\nWhile working with data, I hope everyone usually started with Jupyter notebooks running either on local system or cloud platform and the exceptional pandas library to read data files and transform them into various summaries of interest. We all are familiar with the awesome functionality and tools that it brings to data processing. My usual process would start with a text file with data in a CSV format. I would read data into a pandas DataFrame and run various transformations of interest. It is a very straightforward process for small datasets which you can store as plain-text files without too much overhead.\nHowever, when the number of observations increases, the process of saving and loading data back into the memory becomes slower, and now each kernel\u2019s restart steals your time and forces you to wait until the data reloads. So eventually, the CSV files or any other plain-text formats lose their attractiveness. But there are various ways where we can avoid this delay and utilize the computation power efficiently.\n\n> **Quick Summary**\n\nWe would start discussing from general data loading and saving approach to the new data loading\/saving approaches where we would fully utilize the multiprocessing concept(make use of all the IDLE CPU cores), which actually helps in faster computation. The below mentioned approcahes has it pros and cons and i would like you to explore all the approaches and choose whichever suits you requirements.\n\n* A) Pandas Dataframe\n* B) Pandas Dataframe + Using pandas chunksize, engine, iterator and memory_map parameter (saves memory)\n* C) Dask Dataframe\n* D) Datatable Library\n* E) Modin-Dask\/Ray Library\n* F) Other Parallel Processing Libraries including swifter, pandaral-lel, dispy, multiprocessing, joblib and many more.\n* G) Save And Load Data Using Various Data Formats\n\nBesides it,there are plenty of binary formats to store the data which consumes less memory space and helps in faster task execution, which we would see in section G. These are:\n* csv (default)\n* feather\n* hdf\n* msgpack\n* parquet\n* pickle\n* jay\n* numpy array(.npy format) - for numerical data","f90af004":"*So instead of pandas pd we just have to use mpd i.e modin.pandas* and other format\/methods would be same like pandas. Unlike in dask dataframe (refer section C above) we have some other methods (like .compute() and others) which we need for various operations or for data manupulation but here, in modin we don't need to care about any method\/format, just use it like pandas library as Modin APIs are identical to Pandas APIs.","a65254b3":"# G7) Jay Format\nAs we already discussed about the datatable above(refer section D above for more info). Datatable is both powerful and extremely fast. We can use .jay format for faster data manipulation.","28bd1d98":"Did you see the runtime?.. Hurrah! Blazigly Fast :) We got our result in no time.","7be39b7e":"# Conclusion\nIf your pc has several CPUs, only one is fully dedicated to your calculation. Why don't we use the Idle CPUs?\nFrom the above experiments, one thing is clear that we can make use of the libraries that helps in parallel processing. They definitely boosts the performance and saves time. Beside it we can play with other approaches\/libraries\/methods as single approach\/library\/method would never fit for all scenarios. Every approach has it's own drawbacks and advantages. You can try with different datasets and see if the above methods proves benficial for you or not. There are plenty of binary formats to store the data which consumes less memory space and helps in faster task execution, hence we must utilize the perk of various data formats.\n\nI hope the above methods would help you like they does help me.\n\nPlease feel free to share other methods. Feedbacks are welcome.\n\nThanks!","45fe5376":"# G6) Msgpack Format\n[MessagePack](https:\/\/msgpack.org\/index.html) is an efficient binary serialization format. It lets you exchange data among multiple languages like JSON. But it's faster and smaller. Small integers are encoded into a single byte, and typical short strings require only one extra byte in addition to the strings themselves. This package provides CPython bindings (for python) for reading and writing MessagePack data.\n\nWe would be using **pandas-msgpack** library.\n> The *pandas_msgpack* module provides an interface from [pandas](https:\/\/pandas.pydata.org) to the msgpack library. This is a lightweight portable binary format, similar to binary JSON, that is highly space efficient, and provides good performance both on the writing (serialization), and reading (deserialization).\n","46bba81c":"I would be working on [*nfl_big_data_bowl_2021*](https:\/\/www.kaggle.com\/c\/nfl-big-data-bowl-2021\/data) dataset for this demo notebook. (~2.2 Gb in size). Lets load the data.","ba096f36":"From the above results, we can conclude that datatable is really fast. Within seconds we can get the desired result.","d6495998":"For the above method we can conclude that data saving took time but data reading is comparatively faster. You can play around with different dataset and check the results.","4067d720":"# G7) Numpy Format- For numerical dataset\nWhenever dealing with numerical data, it's always a good practice to use [Numpy](https:\/\/numpy.org\/devdocs\/reference\/generated\/numpy.lib.format.html) File format as they are fast and easy to work with. It contains an array saved in the NumPy (NPY) file format. NPY files store all the information required to reconstruct an array on any computer, which includes dtype and shape information.\nUseful [Link](https:\/\/www.kdnuggets.com\/2018\/04\/start-using-npy-files-more-often.html)","2e1e6fc1":"Wow!, as we can see that above method using Dask Dataframe is really really fast. We can apply some of the quick data manipulation methods as shown in below cells:","4753b43f":"# F) Other Parallel Processing Libraries\nYou can play around with few others parallel processing libraries and see if they are helpful for you.\n* [SWIFTER](https:\/\/github.com\/jmcarpenter2\/swifter\/blob\/master\/docs\/documentation.md): Swifter is a package that tries to efficiently apply any function to a Pandas Data Frame or Series object in the quickest available method. It is integrated with the Pandas object so that we would use this package only with a Pandas object such as Data Frame or Series.\nUseful [Link](https:\/\/towardsdatascience.com\/speed-up-your-pandas-processing-with-swifter-6aa314600a13)\n* [Pandaral-lel](https:\/\/github.com\/nalepae\/pandarallel): The idea of Pandaral\u00b7lel is to distribute your pandas calculation over all available CPUs on your computer to get a significant speed increase  **Limitation of Pandaral-lel**: Pandaral-lel only works with pandas and for windows pc, Pandaral\u00b7lel would works only if the Python session (python, ipython, jupyter notebook, jupyter lab) is executed from [Windows Subsystem for Linux](https:\/\/docs.microsoft.com\/en-us\/windows\/wsl\/) (WSL). \nUseful [Link](https:\/\/towardsdatascience.com\/pandaral-lel-a-simple-and-efficient-tool-to-parallelize-your-pandas-operations-on-all-your-cpus-bb5ff2a409ae).\n* [Ipyparallel](http:\/\/people.duke.edu\/~ccc14\/sta-663-2016\/19C_IPyParallel.html): This another multiprocessing and task-distribution system, specifically for parallelizing the execution of Jupyter notebook code across a cluster. We can use prefix %px in any Python statement to automatically parallelize it. Useful [Link](https:\/\/ipyparallel.readthedocs.io\/en\/latest\/).\n* [Dispy](http:\/\/dispy.sourceforge.net\/): dispy is a generic, comprehensive and easy to use framework for creating and using compute clusters to execute computations in parallel across multiple processors in a single machine (SMP), among many machines in a cluster, grid or cloud. It lets you distribute whole Python programs or just individual functions across a cluster of machines for parallel execution. It uses platform-native mechanisms for network communication to keep things fast and efficient, so Linux, MacOS, and Windows machines work equally well.\n* [Multiprocessing](https:\/\/docs.python.org\/3\/library\/multiprocessing.html): We can use multi-processing package to process the input file in parallel to speed up processing.\n* [Joblib](https:\/\/joblib.readthedocs.io\/en\/latest\/generated\/joblib.Parallel.html): Joblib provides a simple helper class to write parallel for loops using multiprocessing. The core idea is to write the code to be executed as a generator expression, and convert it to parallel computing. Joblib basically uses the \"Parallel\" class to execute parallel tasks on CPU.\n\nI would draft a seperate notebook with working examples having all the above methods to fasten\/speed-up the computation tasks for data manipulation and analysis if you would like this notebook.","98942676":"As we can see above we are unable to get the total rows in a dataframe. The Dask delayed function decorates your functions so that they operate lazily. Rather than executing your function immediately, it will defer execution, placing the function and its arguments into a task graph.\n\nSo we can turn any dask collection into a concrete value by calling the **.compute()** method or **dask.compute(...)** function. This function will block until the computation is finished, going straight from a lazy dask collection to a concrete value in local memory.","e80d990e":"# G3) hdf Format\n[Hierarchical Data Format (HDF)](https:\/\/www.hdfgroup.org\/solutions\/hdf5\/) is a set of file formats (HDF4, HDF5) designed to store and organize large amounts of data. It supports an unlimited variety of datatypes, and is designed for flexible and efficient I\/O and for high volume and complex data. It allows access time and storage space optimizations.\nThere is a shortcoming which i noticed that while reading the data, it consumes a lot of memory initially and then reset to normal state, which can cause MemoryError or can break the kernel if we won't have sufficient memory. ","2d7e4e58":"# G4) Parquet Format\n[Parquet](https:\/\/parquet.apache.org\/documentation\/latest\/) is an open source file format available to any project in the Hadoop ecosystem. Apache Parquet is designed for efficient as well as performant flat columnar storage format of data compared to row based files like CSV or TSV files.\n\nParquet uses the record shredding and assembly algorithm which is superior to simple flattening of nested namespaces. Parquet is optimized to work with complex data in bulk and features different ways for efficient data compression and encoding types.  This approach is best especially for those queries that need to read certain columns from a large table. Parquet can only read the needed columns therefore greatly minimizing the IO.\n\nUseful [Link](https:\/\/www.upsolver.com\/blog\/apache-parquet-why-use)","5530b80a":"# D) Datatable Library\n[datatable](http:\/\/https:\/\/datatable.readthedocs.io\/en\/latest\/manual\/index-manual.html) provides fast and convenient parsing of text (csv) files. I recently came to know about this library and it proves helpful for me for data analysis. Thanks! to the [author](https:\/\/www.kaggle.com\/rohanrao\/riiid-with-blazing-fast-rid).\n\n> Datatable is inspired by R's data.table which enhances the accessibility of data. It is faster than pandas.\n\nThe datatable parser\n* Automatically detects separators, headers, column types, quoting rules, etc.\n* Reads from file, URL, shell, raw text, archives, glob\n* Provides multi-threaded file reading for maximum speed\n* Includes a progress indicator when reading large files","ab70ac68":"This way *we can eliminate the MemoryError* but it comes with a price .i.e sometimes execution time can be a little bit more as compare to the general approach. But it proves faster than the default pandas.read_csv approach.","7348e070":"We would create a frame using **fread() function** which is both powerful and extremely fast. It can automatically detect parse parameters for the majority of text files, load data from .zip archives or URLs, read Excel files, and much more.","40f4af92":"**B. Save And Load CSV Using Modin.Pandas**","966e53ca":"That was incredibly fast!\nWe can easily convert an existing Frame into a numpy array, a pandas DataFrame using **to_numpy** and  **to_pandas()** method respectively and perform the data manipulation like we do in pandas. \n\nPretty fast and easy right?","467f5988":"# G1) csv format\nA CSV is a comma-separated values file, which allows data to be saved in a tabular format. This is the default standard text file format that we are using from the beginning and mostly data is available in this format.\n\n**A. Save And Load CSV using PANDAS**","7860c831":"# G) Save And Load Data Using Various Data Formats\nThe above methods talks about reading\/loading data in csv format to enhace the data manipulation\/computation and eliminates the waiting time until the data loads into the memory upto a great extent. Besides it, it's always the best practice to save the dataset into various data formats which helps in faster data manipulation and consumes less memory. There are plenty of binary formats to store the data on disk, and many of them pandas supports. Useful [Link](https:\/\/towardsdatascience.com\/the-best-format-to-save-pandas-data-414dca023e0d).\n\nWe can convert our data into the following data formats:\n* csv (default)\n* feather\n* hdf\n* msgpack\n* parquet\n* pickle\n* jay\n* numpy array(.npy format) - for numerical data\n\nLet's start......","f120cde6":"\n# A) General Approach To Save And Load Data - Using Pandas CSV\n* **Saving into the CSV Format**","c24e83e5":"# G5) Pickle Format\nWe can save our data files in pickle format which works on serializing and de-serializing objects. The process to converts any kind of python objects (list, dict, etc.) into byte streams (0s and 1s) is called pickling or serialization or flattening or marshalling. We can converts the byte stream (generated through pickling) back into objects by a process called as unpickling."}}