{"cell_type":{"74088ae6":"code","dbbd02e2":"code","67436550":"code","e66c9de3":"code","87c9515e":"code","f770d5fe":"code","7642ee70":"code","fe969973":"code","1285de6c":"code","9867671d":"code","0db889a4":"code","0849b57b":"code","450fd5b4":"code","9f9a08bd":"code","34dd1ba5":"code","8a1c6e14":"code","74b61ae8":"code","ad19e504":"code","4e3d3f1d":"code","b101f19c":"code","7831bf8f":"code","e7824f10":"code","7080e977":"code","d53b2a51":"code","21b5f210":"code","0f291ae5":"code","8f4d5b5a":"code","af6b0de1":"code","cdf7ce6d":"code","2be326b5":"code","4ed27da8":"code","5b3ee123":"code","1bc48a72":"code","c3ea1e3c":"code","466ab723":"code","df6089e9":"code","6a7a78b5":"code","5419ae3c":"code","61a45e87":"code","61f21986":"code","ffe8a7ad":"code","8f1cf375":"code","e1d8422e":"code","718856e8":"code","f3ed901b":"code","65075918":"code","b010d565":"code","c371b6ea":"code","d460aa91":"code","95d71706":"code","359e04b7":"code","008fab76":"code","063d5728":"code","84471ab2":"code","efe13823":"code","6b81c03a":"code","991c5148":"code","b875828b":"code","83a1b32f":"code","9d4141a2":"code","29e67ae7":"code","e561c064":"code","f2637dba":"code","aaf77cbe":"code","1dfd38fa":"code","c1488f07":"code","6552d61f":"code","29495739":"code","22b7521a":"code","19ab676d":"code","5e6503d2":"code","eb884a37":"code","83ce08f1":"code","86f109e9":"code","7b18c82b":"code","3bb086db":"code","f5f6095d":"code","8facc1a6":"code","d9c52dba":"code","3b2c7773":"code","bf65d585":"code","fd7022e5":"code","36610ee2":"code","5e27fea8":"code","f920f4b7":"code","66a3061e":"code","4de91951":"code","6bdba140":"code","8868fc55":"code","004fb4a6":"code","58c561be":"code","7730eeb5":"code","4fb984e8":"code","f5337e7a":"code","b2ce2337":"markdown","8f463978":"markdown","c4acbbaa":"markdown","01d471fc":"markdown","57d55f8b":"markdown","d99a7360":"markdown","02c0d519":"markdown","1610bfcc":"markdown","78682b1d":"markdown","b65a2027":"markdown","7a7e70fc":"markdown","8f7fc9de":"markdown","27cbc285":"markdown","32276fa7":"markdown","cf4c2706":"markdown","7bdb2604":"markdown","4d75b36d":"markdown","b6eb938c":"markdown","9ba04361":"markdown"},"source":{"74088ae6":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\n\nimport re\nfrom fuzzywuzzy import fuzz\nfrom tqdm import tqdm\n\nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize \nfrom nltk.stem.snowball import SnowballStemmer\n\nimport xgboost as xgb\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import SVC","dbbd02e2":"pd.set_option(\"display.max_rows\", None) \n  ","67436550":"data = pd.read_csv(\"\/kaggle\/input\/60k-stack-overflow-questions-with-quality-rate\/train.csv\")\ndata.head()","e66c9de3":"def replace_punct(text):\n    text = text.replace(\"><\",\",\")\n    text = text.replace(\"<\",\"\")\n    text = text.replace(\">\",\"\")\n    \n    return text\n\ndata['Tags_cleaned'] = data['Tags'].apply(replace_punct)\n    \n    ","87c9515e":"data.head()","f770d5fe":"vector = CountVectorizer(tokenizer=lambda x:x.split(\",\"))\ntag_trans = vector.fit_transform(data['Tags_cleaned'])","7642ee70":"print(\"Number of tags are {}\".format(tag_trans.shape[1]))","fe969973":"tags = vector.get_feature_names()\nprint(\"some of tags are {}\".format(tags[40:100]))","1285de6c":"freq = tag_trans.sum(axis=0).A1\nfreq_dict=dict(zip(tags,freq))","9867671d":"freq_dict['.net']","0db889a4":"tag_freq_df = pd.DataFrame.from_dict(freq_dict, orient='index', columns=['Count']).reset_index(drop=False)\ntag_df_sorted = tag_freq_df.sort_values(['Count'], ascending=False).reset_index(drop=True)\ntag_df_sorted.head()","0849b57b":"tag_df_sorted.tail(10)","450fd5b4":"tag_counts = tag_df_sorted['Count'].values\nplt.plot(tag_counts[0:150])\nplt.title(\"Distribution of frequency of Tags Appeared\")\nplt.grid()\nplt.ylabel(\"Number of times tag appeared\")\nplt.xlabel(\"Tag Number\")\nplt.show()","9f9a08bd":"tags_final = tag_df_sorted[tag_df_sorted.Count>3]","34dd1ba5":"tags_final.shape","8a1c6e14":"final_tags = list(tags_final['index'].values)","74b61ae8":"final_tags[:10]","ad19e504":"def tag_remove(text):\n    text_list = text.split(\",\")\n    text_list = \",\".join(list(set(text_list) & set(final_tags)))\n    return text_list","4e3d3f1d":"data['Tags_final'] = data['Tags_cleaned'].apply(tag_remove)","b101f19c":"data['Tags_cleaned'].apply(lambda x:len(x.split(\",\"))).equals(data['Tags_final'].apply(lambda x:len(x.split(\",\"))))","7831bf8f":"data = data.drop(['Tags', 'CreationDate','Tags_cleaned'], axis=1)\ndata['Y'] = data['Y'].map({'LQ_CLOSE':0, 'LQ_EDIT': 1, 'HQ':2})\ndata.head()\n","e7824f10":"data.loc[10,'Body']","7080e977":"def striphtml(data):\n    cleanr = re.compile('<.*?>')\n    cleantext = re.sub(cleanr, ' ', str(data))\n    return cleantext\n\nstop_words = set(stopwords.words('english'))\nstemmer = SnowballStemmer(\"english\")","d53b2a51":"data['code'] = data['Body'].apply(lambda x: re.findall(r'<code>(.*?)<\/code>', x, flags=re.DOTALL))","21b5f210":"data.head(11)","0f291ae5":"data.loc[3, 'Body']","8f4d5b5a":"data['question'] = data['Body'].apply(lambda x:re.sub('<code>(.*?)<\/code>', '', x, flags=re.MULTILINE|re.DOTALL))\ndata['question'] = data['question'].apply(lambda x: striphtml(x))","af6b0de1":"data.loc[10,'code'][1]","cdf7ce6d":"def clean_text(text):\n    text = text.lower()\n    text = re.sub(r'[^(a-zA-Z)\\s]','', text)\n    return text","2be326b5":"#data['Title'] = data['Title'].apply(lambda x:re.findall(r'b(.*?)', x, flags=re.DOTALL))\n#data['Title'] = data['Title'].apply(lambda x:x.encode('utf-8'))","4ed27da8":"data['question'] = data['Title'].astype(str) + data['question'].astype(str)\ndata['question'] = data['question'].apply(clean_text)\n","5b3ee123":"data.loc[31,'Body']","1bc48a72":"data.loc[31,'question']","c3ea1e3c":"data.loc[31,'Title']","466ab723":"stop_words = set(stopwords.words('english')) \n\ndef remove_stopword(words):\n    list_clean = [w for w in words.split(' ') if not w in stop_words]\n    \n    return ' '.join(list_clean)\n\ndef remove_next_line(words):\n    words = words.split('\\n')\n    \n    return \" \".join(words)\n\ndef remove_r_char(words):\n    words = words.split('\\r')\n    \n    return \"\".join(words)","df6089e9":"data['question'] = data['question'].apply(remove_stopword)\ndata['question'] = data['question'].apply(remove_next_line)\ndata['question'] = data['question'].apply(remove_r_char)","6a7a78b5":"distribution = data.groupby('Y')['Body'].count().reset_index()","5419ae3c":"distribution","61a45e87":"data['Num_words_body'] = data['Body'].apply(lambda x:len(str(x).split())) #Number Of words in Selected Text\ndata['Num_words_title'] = data['Title'].apply(lambda x:len(str(x).split())) #Number Of words in main text\ndata['difference_in_words'] = abs(data['Num_words_body'] - data['Num_words_title']) #Difference in Number of words text and Selected Text","61f21986":"data['Num_char_body'] = data['Body'].apply(lambda x:len(\"\".join(set(str(x).replace(\" \",\"\"))))) \ndata['Num_char_title'] = data['Title'].apply(lambda x:len(\"\".join(set(str(x).replace(\" \",\"\")))))","ffe8a7ad":"data['len_common_words'] = data.apply(lambda x:len(set(str(x['Title']).split()).intersection(set(str(x['Body']).split()))),axis=1)","8f1cf375":"data.head(3)","e1d8422e":"data['fuzz_qratio'] = data.apply(lambda x:fuzz.QRatio(str(x['Title']),str(x['Body'])), axis=1)\ndata['fuzz_Wratio'] = data.apply(lambda x:fuzz.WRatio(str(x['Title']),str(x['Body'])), axis=1)\ndata['fuzz_partial_ratio'] = data.apply(lambda x:fuzz.partial_ratio(str(x['Title']),str(x['Body'])), axis=1)\ndata['fuzz_partial_token_set_ratio'] = data.apply(lambda x:fuzz.partial_token_set_ratio(str(x['Title']),str(x['Body'])), axis=1)\ndata['fuzz_partial_token_sort_ratio'] = data.apply(lambda x:fuzz.partial_token_sort_ratio(str(x['Title']),str(x['Body'])), axis=1)\ndata['fuzz_token_set_ratio'] = data.apply(lambda x:fuzz.token_set_ratio(str(x['Title']),str(x['Body'])), axis=1)\ndata['fuzz_token_sort_ratio'] = data.apply(lambda x:fuzz.token_sort_ratio(str(x['Title']),str(x['Body'])), axis=1)","718856e8":"data.head(3)","f3ed901b":"data['Body_with_title'] = data['Title'] + \" \" + data['Body']","65075918":"xtrain, xvalid, ytrain, yvalid = train_test_split(data.drop(['Id','Title','Body','Y'],axis=1).values, data['Y'].values, \n                                                  stratify=data['Y'].values, \n                                                  random_state=42, \n                                                  test_size=0.2, shuffle=True)","b010d565":"len(ytrain)","c371b6ea":"len(yvalid)","d460aa91":"yvalid","95d71706":"def get_accuracy(clf, predictions, yvalid):\n    return np.mean(predictions == yvalid)","359e04b7":"tfv = TfidfVectorizer(min_df=3,  max_features=None, \n            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n            ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,\n            stop_words = 'english')","008fab76":"tfv.fit(list(xtrain[:,-1]))\nxtrain_tfv =  tfv.transform(xtrain[:,-1]) \nxvalid_tfv = tfv.transform(xvalid[:,-1])","063d5728":"ctv = CountVectorizer(analyzer='word',token_pattern=r'\\w{1,}',\n            ngram_range=(1, 3), stop_words = 'english')\n\n# Fitting Count Vectorizer to both training and test sets (semi-supervised learning)\nctv.fit(xtrain[:,-1])\nxtrain_ctv =  ctv.transform(xtrain[:,-1]) \nxvalid_ctv = ctv.transform(xvalid[:,-1])\n","84471ab2":"# Fitting a simple Logistic Regression on TFIDF\nclf = LogisticRegression()\nclf.fit(xtrain_tfv, ytrain)\npredictions = clf.predict_proba(xvalid_tfv)","efe13823":"clf_ctv = LogisticRegression()\nclf_ctv.fit(xtrain_ctv, ytrain)\npredictions_ctv = clf_ctv.predict_proba(xvalid_ctv)","6b81c03a":"predictions","991c5148":"from sklearn.metrics import multilabel_confusion_matrix\nmultilabel_confusion_matrix(yvalid, predictions)\n","b875828b":"multilabel_confusion_matrix(yvalid, predictions_ctv)","83a1b32f":"get_accuracy(clf, predictions, yvalid)","9d4141a2":"get_accuracy(clf_ctv, predictions_ctv, yvalid)","29e67ae7":"clf = xgb.XGBClassifier(max_depth=10, n_estimators=200, colsample_bytree=0.8, \n                        subsample=0.8, nthread=10, learning_rate=0.1, silent=False)\nclf.fit(xtrain_tfv, ytrain)\npredictions = clf.predict(xvalid_tfv)","e561c064":"get_accuracy(clf, predictions, yvalid)","f2637dba":"multilabel_confusion_matrix(yvalid, predictions","aaf77cbe":"clf_ctv = xgb.XGBClassifier(max_depth=10, n_estimators=200, colsample_bytree=0.8, \n                        subsample=0.8, nthread=10, learning_rate=0.1)\nclf_ctv.fit(xtrain_ctv, ytrain)\npredictions_ctv = clf_ctv.predict(xvalid_ctv)","1dfd38fa":"get_accuracy(clf_ctv, predictions_ctv, yvalid)","c1488f07":"clf = MultinomialNB()\nclf.fit(xtrain_tfv, ytrain)\npredictions = clf.predict(xvalid_tfv)\n","6552d61f":"get_accuracy(clf, predictions, yvalid)","29495739":"clf_ctv = MultinomialNB()\nclf_ctv.fit(xtrain_ctv, ytrain)\npredictions_ctv = clf_ctv.predict(xvalid_ctv)","22b7521a":"get_accuracy(clf_ctv, predictions_ctv, yvalid)","19ab676d":"# Apply SVD, I chose 120 components. 120-200 components are good enough for SVM model.\nsvd = decomposition.TruncatedSVD(n_components=180)\nsvd.fit(xtrain_tfv)\nxtrain_svd = svd.transform(xtrain_tfv)\nxvalid_svd = svd.transform(xvalid_tfv)\n\n# Scale the data obtained from SVD. Renaming variable to reuse without scaling.\nscl = preprocessing.StandardScaler()\nscl.fit(xtrain_svd)\nxtrain_svd_scl = scl.transform(xtrain_svd)\nxvalid_svd_scl = scl.transform(xvalid_svd)","5e6503d2":"clf = SVC(C=1.0) # since we need probabilities\nclf.fit(xtrain_svd_scl, ytrain)","eb884a37":"predictions = clf.predict(xvalid_svd_scl)","83ce08f1":"get_accuracy(clf, predictions, yvalid)","86f109e9":"clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n                        subsample=0.8, nthread=10, learning_rate=0.1)\nclf.fit(xtrain_svd, ytrain)\npredictions = clf.predict(xvalid_svd)","7b18c82b":"get_accuracy(clf, predictions, yvalid)","3bb086db":"mll_scorer = metrics.make_scorer(get_accuracy, greater_is_better=True, needs_proba=False)","f5f6095d":"svd = TruncatedSVD()\n    \n# Initialize the standard scaler \nscl = preprocessing.StandardScaler()\n\n# We will use logistic regression here..\nxg_model = xgb.XGBClassifier()\n\n# Create the pipeline \nclf = pipeline.Pipeline([('svd', svd),\n                         ('scl', scl),\n                         ('xg', xg_model)])","8facc1a6":"param_grid = {'svd__n_components' : [120, 150, 180],\n              'xg__max_depth':[5,7,10],\n              'xg__learning_rate':[0.1,0.01,0.5]}\n","d9c52dba":"def read_glove_vecs(glove_file):\n    #input: file\n    #output: word to 200d vector mapping output\n    with open(glove_file, 'r') as f:\n        words = set()\n        word_to_vec_map = {}\n        for line in f:\n            line = line.strip().split()\n            curr_word = line[0]\n            words.add(curr_word)\n            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n    return word_to_vec_map\n#word_to_vec_map = read_glove_vecs('..\/input\/glove-global-vectors-for-word-representation\/glove.6B.100d.txt')\nword_to_vec_map = read_glove_vecs('..\/input\/glovetwitter27b100dtxt\/glove.twitter.27B.200d.txt')","3b2c7773":"def prepare_sequence(ds, word_to_vec_map):\n    #input: Series, and word_to_vec_map of size(vocab_size,200)\n    #output: returns shape of (len(ds), 200)\n    traintest_X = []\n    for sentence in tqdm(ds):\n        sequence_words = np.zeros((word_to_vec_map['cucumber'].shape))\n        for word in sentence.split():\n            if word in word_to_vec_map.keys():\n                temp_X = word_to_vec_map[word]\n            else:\n                temp_X = word_to_vec_map['#']\n            #print(temp_X)\n            sequence_words+=(temp_X)\/len(sentence)\n            #print(sequence_words)\n        traintest_X.append(sequence_words)\n    return np.array(traintest_X)\n","bf65d585":"prepare_sequence(xtrain[:,-1][0], word_to_vec_map)","fd7022e5":"#concatenate all sequences for training and testing set\ntrain_w2v = prepare_sequence(xtrain[:,-1], word_to_vec_map)\nvalid_w2v = prepare_sequence(xvalid[:,-1], word_to_vec_map)","36610ee2":"clf = LogisticRegression()\nclf.fit(train_w2v, ytrain)\npredictions = clf.predict(valid_w2v)","5e27fea8":"get_accuracy(clf, predictions, yvalid)","f920f4b7":"clf = xgb.XGBClassifier(max_depth=15, n_estimators=200, colsample_bytree=0.8, \n                        subsample=0.8, nthread=10, learning_rate=0.1, silent=False)\nclf.fit(train_w2v, ytrain)\npredictions = clf.predict(valid_w2v)\n","66a3061e":"get_accuracy(clf, predictions, yvalid)","4de91951":"clf2 = xgb.XGBClassifier(max_depth=10, n_estimators=150, colsample_bytree=0.8, \n                        subsample=0.8, nthread=10, learning_rate=0.1, silent=False)\nclf2.fit(train_w2v, ytrain)\n","6bdba140":"predictions = clf2.predict(valid_w2v)","8868fc55":"get_accuracy(clf2, predictions, yvalid)","004fb4a6":"final_xtrain = np.concatenate((xtrain[:,:-1],train_w2v), axis=1)\nfinal_xvalid = np.concatenate((xvalid[:,:-1],valid_w2v),axis=1)","58c561be":"clf = LogisticRegression()\nclf.fit(final_xtrain, ytrain)\npredictions = clf.predict(final_xvalid)","7730eeb5":"get_accuracy(clf, predictions, yvalid)","4fb984e8":"clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n                        subsample=0.8, nthread=10, learning_rate=0.1, silent=False)\nclf.fit(final_xtrain, ytrain)\npredictions = clf.predict(final_xvalid)","f5337e7a":"get_accuracy(clf, predictions, yvalid)","b2ce2337":"# **Make Fuzzy features**","8f463978":"## Analysis Of Tags","c4acbbaa":"# Fit a simple Logistic regression Model on tf-idf","01d471fc":"# Preprocessing data","57d55f8b":"# **Exploratory Data analysis**","d99a7360":"# Make Word vector features(Still improving)","02c0d519":"# Make Tf-idf features","1610bfcc":"# Grid Search","78682b1d":"# Split train test data","b65a2027":"# Fit an Xgboost on tf-idf features","7a7e70fc":"# Making basic Features\n","8f7fc9de":"**Number of times tag appears**","27cbc285":"model = GridSearchCV(estimator=clf, param_grid=param_grid, scoring=mll_scorer,\n                                 verbose=10, n_jobs=-1, iid=True, refit=True, cv=2)\n\nmodel.fit(xtrain_tfv, ytrain)  # we can use the full data here but im only using xtrain\nprint(\"Best score: %0.3f\" % model.best_score_)\nprint(\"Best parameters set:\")\nbest_parameters = model.best_estimator_.get_params()\nfor param_name in sorted(param_grid.keys()):\n    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))","32276fa7":"# Fit an SVD on tf-idf features only","cf4c2706":"# Fit a Naive bayes Model on tf-idf only","7bdb2604":"# Fitting Xgboost on tf-idf-SVD feature","4d75b36d":"Total number of unique tags","b6eb938c":"# Import all libraries","9ba04361":"# Count vectorizer Model for Comparison with TF-IDF"}}