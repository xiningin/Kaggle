{"cell_type":{"f1f4bee8":"code","c299b9dd":"code","e1c56ff9":"code","8dfb7891":"code","faffd2ec":"code","cdfa0a16":"code","c4b984fa":"code","1fc4aa73":"code","dbbdedef":"code","da522958":"code","86fa6d8c":"code","08a43aaa":"code","80b0ae7a":"code","cb802cf4":"code","01370228":"code","c96d22bf":"code","59a09ccc":"code","1acbfb2d":"markdown"},"source":{"f1f4bee8":"import os\n\nimport cv2\nimport numpy as np\nimport pandas as pd\n\nimport torch\nfrom torch.utils.data import Dataset\nfrom torchvision import datasets\nfrom torchvision import transforms\nfrom torchvision.transforms import ToTensor\nfrom torch.utils.data import DataLoader\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import optim\n\n!pip install torchsummary\nfrom torchsummary import summary\n\nimport matplotlib.pyplot as plt\nimport time","c299b9dd":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","e1c56ff9":"# Data Loading\nDATA_PATH = \"\/kaggle\/input\/chest-xray-pneumonia\/chest_xray\/\"\nTRAIN_PATH = DATA_PATH + \"train\/\"\nTEST_PATH = DATA_PATH + \"test\/\"\nVAL_PATH = DATA_PATH + \"val\/\"\nLABELS = [\"NORMAL\", \"PNEUMONIA\"]\nTRAIN_BATCH_SIZE = 10\nIMG_DIM_NORMALIZED = 224  # Square center crops of images for training and val\nCHECKPOINT_PATH = \"\/kaggle\/working\/checkpoint.pth.tar\"\n\n# Transforms somewhat explained here:\n# https:\/\/stackoverflow.com\/questions\/50002543\/what-are-transforms-in-pytorch-used-for\n# Instructions on this transform composition found here:\n# https:\/\/www.kaggle.com\/c\/detecting-pneumonia-using-cnn-in-pytorch\/data\n# Training on ImageNet example code here:\n# https:\/\/github.com\/pytorch\/examples\/blob\/master\/imagenet\/main.py\ntrain_dataset = datasets.ImageFolder(\n    TRAIN_PATH,\n    transforms.Compose([\n        transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),  # resize to more manageable sizes\n        transforms.RandomRotation(degrees=15),  # prevent model from overfitting to positional features \n        transforms.ColorJitter(),  # randomly changes the brightness, contrast, saturation, and hue of an image\n        transforms.RandomHorizontalFlip(),  # similar to rotation above\n        transforms.CenterCrop(size=IMG_DIM_NORMALIZED),  # crops the center of the image to be a 224x224 square\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet chosen values   \n    ])\n)\ntrain_loader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True)\n\nval_dataset = datasets.ImageFolder(\n    VAL_PATH,\n    transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(IMG_DIM_NORMALIZED),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ])\n)\nval_loader = DataLoader(val_dataset, shuffle=True)\n\ntest_dataset = datasets.ImageFolder(\n    TEST_PATH,\n    transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(IMG_DIM_NORMALIZED),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ])\n)\ntest_loader = DataLoader(test_dataset, shuffle=True)","8dfb7891":"train_dataset.root","faffd2ec":"# Create the CNN Model\nclass CNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # 3 RGB input channels, 6 kernels for output channels, each 5x5, 1 stride, 2 pixel padding around edges\n        # TODO: non-overlapping strides (5)\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=4, stride=4)\n        self.pool = nn.MaxPool2d(kernel_size=2)  # Stride by default is kernel_size\n        self.conv2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=4, stride=4)\n        # For in size calculation, see: https:\/\/www.kaggle.com\/c\/detecting-pneumonia-using-cnn-in-pytorch\/data\n        # conv1: (224 - 4 + 2*0) \/ 4 + 1 = 56\n        # pool: (56 - 2) \/ 2 + 1 = 28\n        # conv2: (28 - 4 + 2*0) \/ 4 + 1 = 7\n        # pool: (7 - 1) \/ 2 + 1 = 4\n        # 56 x 56 after all conv and maxpool layers, 16 channels. 56*56*16 = 50176 (see model summary below)\n        # Want 1 output label\n        # TODO: Consider dropout?\n        self.fc1 = nn.Linear(288, 288)\n        self.fc2 = nn.Linear(288, 1)\n        self.bn = nn.BatchNorm2d(32)\n    \n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn(x)\n        x = self.pool(torch.tanh(x))\n        x = self.conv2(x)\n        x = self.bn(x)\n        x = self.pool(torch.tanh(x))\n        # TODO: What does this mean? probably flatten it to ready it for fully connected layer, but idk batch??\n        x = torch.flatten(x, 1) # flatten all dimensions except batch\n        return torch.sigmoid(self.fc2(self.fc1(x)))\n        \ncnn = CNN()\ncnn.to(device)","cdfa0a16":"summary(cnn, (3, 224, 224))","c4b984fa":"# Loss function and optimizer\n# TODO: Figure out exactly what cross entropy loss is\n# Binary Cross Entropy loss is for just binary classification\ncriterion = nn.BCELoss()\noptimizer = optim.Adam(cnn.parameters(), lr=0.0001)","1fc4aa73":"class AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self, name, fmt=':f'):\n        self.name = name\n        self.fmt = fmt\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum \/ self.count\n\n    def __str__(self):\n        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n        return fmtstr.format(**self.__dict__)\n\n\nclass ProgressMeter(object):\n    def __init__(self, num_batches, meters, prefix=\"\"):\n        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n        self.meters = meters\n        self.prefix = prefix\n\n    def display(self, batch):\n        print(batch)\n        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n        entries += [str(meter) for meter in self.meters]\n        print('\\t'.join(entries))\n\n    def _get_batch_fmtstr(self, num_batches):\n        num_digits = len(str(num_batches \/\/ 1))\n        fmt = '{:' + str(num_digits) + 'd}'\n        return '[' + fmt + '\/' + fmt.format(num_batches) + ']'","dbbdedef":"def accuracy(output, target):\n    \"\"\"\n    Computes simple binary classification accuracy.\n    Assumes values in output lie in [0, 1], and target contains only 0 or 1 elements\n    \"\"\"\n    batch_size = target.size(0)\n    pred = output >= 0.5\n    truth = target >= 0.5\n    return pred.eq(truth).sum() \/ batch_size\n    \ndef train(train_loader, model, criterion):\n    # switch to train mode\n    model.train()\n    \n    running_loss = 0\n    for i, data in enumerate(train_loader):\n        imgs, labels = data[0].to(device), data[1].to(device)\n        optimizer.zero_grad()\n        \n        outputs = model(imgs)\n        loss = criterion(outputs, labels.unsqueeze(1).float())\n        loss.backward()\n        optimizer.step()\n        \n        # print statistics\n        running_loss += loss.item()\n        if i % 100 == 99:    # print every 100 mini-batches\n            print('[%d, %5d] loss: %.3f' %\n                  (epoch + 1, i + 1, running_loss \/ 100))\n            running_loss = 0.0\n    \ndef validate(val_loader, model, criterion):\n    batch_time = AverageMeter('Time', ':6.3f')\n    losses = AverageMeter('Loss', ':.4e')\n    top1 = AverageMeter('Acc@1', ':6.2f')\n    progress = ProgressMeter(\n        len(val_loader),\n        [batch_time, losses, top1],\n        prefix='Test: '\n    )\n    \n    model.eval()\n    \n    with torch.no_grad():\n        end = time.time()\n        for i, (images, target) in enumerate(val_loader):\n            images = images.to(device)\n            target = target.unsqueeze(1).float().to(device)\n\n            output = model(images)\n            loss = criterion(output, target)\n\n            # measure accuracy and record loss\n            acc = accuracy(output, target)\n            losses.update(loss.item(), images.size(0))\n            top1.update(acc.item(), images.size(0))\n\n            # measure elapsed time\n            batch_time.update(time.time() - end)\n            end = time.time()\n\n            progress.display(i)\n\n        # TODO: this should also be done with the ProgressMeter\n        print(' * Acc@1 {top1.avg:.3f}'.format(top1=top1))\n    \n    return top1.avg","da522958":"# Train the CNN\nEPOCHS = 40\nbest_acc1 = 0\n\nfor epoch in range(EPOCHS):\n    start = time.time()\n    \n    # Train all mini-batches for 1 epoch\n    train(train_loader, cnn, criterion)\n    \n    # evaluate on validation set\n    acc1 = validate(val_loader, cnn, criterion)\n\n    # remember best acc@1 and save checkpoint\n    is_best = acc1 > best_acc1\n    best_acc1 = max(acc1, best_acc1)\n                \n    end = time.time()\n    print(f\"Epoch {epoch} took {end - start} s.\")\n        \nprint(\"Finished Training!\")","86fa6d8c":"best_acc1","08a43aaa":"torch.save(\n    {\n        'epoch': epoch + 1,\n        'arch': \"cnn\",\n        'state_dict': cnn.state_dict(),\n        'best_acc1': best_acc1,\n        'optimizer' : optimizer.state_dict(),\n    },\n    CHECKPOINT_PATH\n)","80b0ae7a":"CHECKPOINT_PATH = \"\/kaggle\/input\/modelcheckpoint\/checkpoint.pth.tar\"\ncheckpoint = torch.load(CHECKPOINT_PATH)","cb802cf4":"checkpoint.keys()","01370228":"cnn.load_state_dict(checkpoint['state_dict'])\ncnn.eval()\ncnn","c96d22bf":"def test(test_loader, model, criterion):\n    batch_time = AverageMeter('Time', ':6.3f')\n    losses = AverageMeter('Loss', ':.4e')\n    accuracy_metric = AverageMeter('Acc@1', ':6.2f')\n    progress = ProgressMeter(\n        len(test_loader),\n        [batch_time, losses, accuracy_metric],\n        prefix='Test: '\n    )\n    \n    # Testing loop\n    correct, total = 0, 0\n    with torch.no_grad():\n        end = time.time()\n        for i, (images, target) in enumerate(val_loader):\n            images = images.to(device)\n            target = target.unsqueeze(1).float().to(device)\n\n            output = model(images)\n            loss = criterion(output, target)\n\n            # measure accuracy and record loss\n            acc = accuracy(output, target)\n            losses.update(loss.item(), images.size(0))\n            accuracy_metric.update(acc.item(), images.size(0))\n\n            # measure elapsed time\n            batch_time.update(time.time() - end)\n            end = time.time()\n#             progress.display(i)\n\n        # TODO: this should also be done with the ProgressMeter\n        print(' * Accuracy {accuracy_metric.avg:.3f}'.format(accuracy_metric=accuracy_metric))\n        \n    return accuracy_metric\n\ntest_acc = test(test_loader, cnn, criterion)\ntest_acc","59a09ccc":"dir(test_acc)","1acbfb2d":"# Things I Learned in this CNN from Scratch Journey\n\n### Loading data in Pytorch\n- Use datasets.ImageFolder with the necessary data preprocessing transforms to store data\n    - Alternatively, can define my own dataset class that defines `__getitem__()`\n- Use a dataloader that can be iterated through later during training etc\n- Useful links to understand the transforms (comments below also explain what's going on):\n    - <https:\/\/stackoverflow.com\/questions\/50002543\/what-are-transforms-in-pytorch-used-for>\n    - <https:\/\/www.kaggle.com\/c\/detecting-pneumonia-using-cnn-in-pytorch\/data>\n    - <https:\/\/github.com\/pytorch\/examples\/blob\/master\/imagenet\/main.py>\n\n### CNN Arch\n- Use torchsummary to get a better understanding of the parameters of your model\n- in channels is 3 if rgb images and 1 if greyscale\n- Simpler to have stride match the kernel size in a conv2d layer\n- Math for calculating the output feature map sizes given conv and pool layers:\n    - <https:\/\/www.kaggle.com\/c\/detecting-pneumonia-using-cnn-in-pytorch\/data>\n- More output channels -> more features, more complex model\n- Adjust kernel size based on what you think the identifiable features in the image would be like (small visual features -> 4x4, larger features -> 9x9 or bigger)\n- Batch normalization to avoid internal covariate shift: \n    - <https:\/\/machinelearning.wtf\/terms\/internal-covariate-shift\/>\n    - <https:\/\/arxiv.org\/abs\/1502.03167>\n    - Run these before nonlinear activations\n- In standard n-way classification, canonical output unit size is n with a softmax activation applied at the end, then use Cross Entropy loss\n    - But for us, in binary classification, we can use a single output unit with values in [0, 1] and use sigmoid activation with Binary Cross Entropy loss\n    - Can initialize loss function with a bias weight against certain labels to penalize them more (potentially helpful in our skewed dataset case)\n\n### CNN Training\/Testing\n- Make sure accuracy function is correct! Account for potential FN penalties that could be more costly than say a FP (eg cancer)\n- Adam or SGD optimizer not too important, learning rate should be around 0.0001 for best results in practice\n- Magic incantation for training every batch:\n    - ```\n    optimizer.zero_grad()\n\n    outputs = model(imgs)\n    loss = criterion(outputs, labels.unsqueeze(1).float())\n    loss.backward()\n    optimizer.step()\n    ```\n    (Ignore the unsqueeze and float part, that was just to get the labels to match the preds for binary CE loss)\n- Validation should have a `with torch.no_grad()` context to avoid extra memory overhead when just evaluating model performance\n- Have a running loss and plot training accuracy for debugging purposes. If it looks good but there are issues, then likely the validation code is the problem. Similarly, have avg train loss \/ batch available too for visualization\/sanity checking.\n\n### Saving\/Loading Model\n- ```\ntorch.save(\n    {\n        'epoch': epoch + 1,\n        'arch': \"cnn\",\n        'state_dict': cnn.state_dict(),\n        'best_acc1': best_acc1,\n        'optimizer' : optimizer.state_dict(),\n    },\n    CHECKPOINT_PATH\n)\n```\n- ```\n# Initialize cnn model with default weights, then:\ncnn.load_state_dict(checkpoint['state_dict'])\ncnn.eval()  # Use this if just planning on testing using the loaded model, otherwise cnn.train() to continue from epoch\n```"}}