{"cell_type":{"10261fa5":"code","35a69d70":"code","79a35112":"code","6af9c172":"code","7cc4a4f0":"code","2b2c95d2":"code","d9d82d8d":"code","f914da5b":"code","51db7eae":"code","9be73acb":"code","d1c669d8":"code","31cbed09":"code","a1757ecb":"code","0c1d5296":"code","89acd695":"code","cf2a75b4":"code","2d8dff7e":"code","e5e56b49":"code","97bc6950":"code","d98a945c":"code","88eebf7c":"code","f01997a1":"code","2f43a72d":"code","bcb68bb3":"code","fe113309":"code","7e6f2bb8":"code","d056fbcb":"code","a502ae88":"code","ce902ee1":"code","c8d07ca0":"code","bd8d9591":"code","2ff5674c":"code","4cb521bd":"code","de1de220":"code","9b22a73f":"code","9209889c":"markdown","61c96ea1":"markdown","fbe4b9ab":"markdown","5ac30d65":"markdown","6641e458":"markdown","a47814ab":"markdown","035bc155":"markdown","69e4b71a":"markdown","383bd6b2":"markdown"},"source":{"10261fa5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\naudioFilePaths = []\nrecordingInfoFilePaths = []\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/respiratory-sound-database\/Respiratory_Sound_Database\/Respiratory_Sound_Database\/audio_and_txt_files'):\n    for filename in filenames:\n        fullPath = os.path.join(dirname, filename)\n        if filename.endswith(\"wav\"):\n            audioFilePaths.append(fullPath)\n        elif filename.endswith(\"txt\"):\n            recordingInfoFilePaths.append(fullPath) \n        #print(os.path.join(dirname, filename))\n\nprint(len(audioFilePaths))\nprint(len(recordingInfoFilePaths))","35a69d70":"import csv\n\ndef extractFilenameWithoutExtension(fullPath):\n    fileName = os.path.basename(fullPath)\n    fileNameWithoutPath = os.path.splitext(fileName)[0]\n    return fileNameWithoutPath\n\n#(fullPaths:string[]) => dict<filename:string, (start, end, hasCrackles, hasWheezes)[]>\ndef getFileInfoDict(pathList):\n    fileInfoDict = {}\n    for path in pathList:\n        fileNameWithoutPath = extractFilenameWithoutExtension(path) \n        infoList = []\n        with open(path, 'r') as hFile:\n            reader = csv.reader(hFile, delimiter='\\t')\n            for row in reader:\n                startTime = float(row[0])\n                endTime = float(row[1])\n                hasCrackles = True if int(row[2]) == 1 else False\n                hasWheezes = True if int(row[3]) == 1 else False\n                infoList.append((startTime, endTime, hasCrackles, hasWheezes))\n                \n        fileInfoDict[fileNameWithoutPath] = infoList\n    return fileInfoDict\n \naudioFilePathsWithoutExtension = [extractFilenameWithoutExtension(path) for path in audioFilePaths]\nfileInfoDict = getFileInfoDict(recordingInfoFilePaths)\n#List order is aligned with the order in which the audio clips are extracted\nfileInfoList = [fileInfoDict[audioPath] for audioPath in audioFilePathsWithoutExtension] ","79a35112":"import librosa\n\ngSampleRate = 7000\n\ndef loadFiles(fileList):\n    outputBuffers = []\n    for filename in fileList:\n        audioBuffer, nativeSampleRate = librosa.load(filename, dtype=np.float32, mono=True, sr=None)\n        if nativeSampleRate == gSampleRate:\n            outputBuffers.append(audioBuffer)\n        else:\n        #Linear resampling using numpy is significantly faster than Librosa's default technique\n            duration = len(audioBuffer) \/ nativeSampleRate\n            nTargetSamples = int(duration * gSampleRate)\n            timeXSource = np.linspace(0, duration, len(audioBuffer), dtype=np.float32)\n            timeX = np.linspace(0, duration, nTargetSamples, dtype=np.float32)\n            resampledBuffer = np.interp(timeX, timeXSource, audioBuffer)\n            outputBuffers.append(resampledBuffer)\n            \n    return outputBuffers\n\naudioBuffers = loadFiles(audioFilePaths)","6af9c172":"from scipy import signal\nimport matplotlib.pyplot as plt\n\nupperCutoffFreq = 3000\ncutoffFrequencies = [80, upperCutoffFreq]\n\n#FIR coefficients for a bandpass filter with a window of 80-3000 Hz\nhighPassCoeffs = signal.firwin(401, cutoffFrequencies, fs=gSampleRate, pass_zero=\"bandpass\")\n\ndef applyHighpass(npArr):\n    return signal.lfilter(highPassCoeffs, [1.0], npArr)\n\n#Higher gamma results in more aggressive compression\ndef applyLogCompressor(signal, gamma):\n    sign = np.sign(signal)\n    absSignal = 1 + np.abs(signal) * gamma\n    logged = np.log(absSignal)\n    scaled = logged * (1 \/ np.log(1.0 + gamma)) #Divide by the maximum possible value from compression\n    return sign * scaled\n\n#Scales all samples to ensure the peak signal is 1\/-1\ndef normalizeVolume(npArr):\n    minAmp, maxAmp = (np.amin(npArr), np.amax(npArr))\n    maxEnv = max(abs(minAmp), abs(maxAmp))\n    scale = 1.0 \/ maxEnv\n    #in place multiply\n    npArr *= scale\n    return npArr\n\n#Removing the low-freq noise, re-normalizing volume then apply compressor\nnoiseRemoved = [normalizeVolume(applyHighpass(buffer)) for buffer in audioBuffers]\nnoiseRemoved = [applyLogCompressor(sig, 30) for sig in noiseRemoved]","7cc4a4f0":"import IPython.display as ipd\n\nfig, axs = plt.subplots(1,2, figsize=(16,5))\n\nselectedSampleIdx = 19\n\nfig.suptitle('Before\/After Bandpass filtering + Log Compression', fontsize=18)\n\naxs[0].plot(audioBuffers[selectedSampleIdx])\naxs[0].set_title(\"Before\")\n\naxs[1].plot(noiseRemoved[selectedSampleIdx])\naxs[1].set_title(\"After\")\n\nfor ax in axs.flat:\n    ax.set(ylabel='Amplitude', xlabel='Sample Index')    \n\nplt.tight_layout() \nplt.show()\n\nprint(\"Before Filtering\")\nipd.display(ipd.Audio(audioBuffers[selectedSampleIdx], rate=gSampleRate))\nprint(\"Post Filtering\")\nipd.display(ipd.Audio(noiseRemoved[selectedSampleIdx], rate=gSampleRate))","2b2c95d2":"windowSizeSeconds = 0.05 \nwindowSampleSize = int(gSampleRate * windowSizeSeconds)\n\ndef plotSpectrogram(specData):\n    plt.figure(figsize=(16,5))\n    #Gamma scaling factor of 0.1 needed to make spectrogram more readable\n    plt.pcolormesh(specData[1], specData[0], np.power(specData[2],0.1) , shading='gouraud')\n    plt.ylim(0, upperCutoffFreq)\n    plt.ylabel('Frequency [Hz]')\n    plt.xlabel('Time [sec]')\n    plt.show()\n    \n#(audioBuffers:float[][]) => (frequencies:float[], time(seconds):float[], amplitudes:float[][]))[]\ndef getSpectrograms(audioBuffers):\n    spectrograms = []\n    for buffer in audioBuffers:\n        freqTable, times, powerSpectrum = signal.spectrogram(buffer, gSampleRate, nperseg=windowSampleSize)\n        spectrograms.append((freqTable, times, powerSpectrum))\n    return spectrograms\n\nspectrograms = getSpectrograms(noiseRemoved)\nplotSpectrogram(spectrograms[selectedSampleIdx])","d9d82d8d":"#(spectrogram:float[][], cutoffFreq(hz):float, plot:bool) => (times:float, amplitudes:float[])\ndef getPowerEnvelop(spectrogram, cutoff, plot=False):\n    frequencies = spectrogram[0]\n    timeSlices = spectrogram[1]\n    spectrum = spectrogram[2]\n    \n    maxInd = np.sum(frequencies <= cutoff)\n    truncFreq = frequencies[:maxInd]\n    \n    powerEnvelop = []\n    for idx, _ in enumerate(timeSlices):\n        freqAmplitudes = spectrum[:maxInd,idx]\n        \n        powerBins = freqAmplitudes * np.square(truncFreq)\n        powerEnvelop.append(sum(powerBins))\n    if (plot): \n        plt.figure(figsize=(16,5))\n        plt.title(\"Intensity vs time\")\n        plt.plot(timeSlices, powerEnvelop)\n        plt.xlabel(\"Time(s)\")\n        plt.ylabel(\"Power\")\n        plt.show()\n        \n    return (timeSlices, powerEnvelop)\n\ntime, amp = getPowerEnvelop(spectrograms[selectedSampleIdx], upperCutoffFreq, True)","f914da5b":"from scipy.signal import find_peaks\nfrom scipy.ndimage import gaussian_filter1d\nfrom scipy.signal import peak_widths\nimport math\n\n#(amplitudes:float[], time(seconds):float[], sampleInterval(seconds):float, minPeakDuration(seconds):float, gaussainSmoothingSigma:float, peakRelHeight(0-1):float, plot:bool) =>\n#(smoothed:float[], peakTiming(seconds):float[], leftRightBoundaries:(left(seconds):float, right(seconds):float)[])\ndef findPeaksAndWidthsFromSmoothedCurve(amplitudes,time, sampleInterval, minPeakDuration=0.4, gaussianSmoothingSigma = 3, peakRelHeight=0.8, plot=False):\n    smoothed = gaussian_filter1d(amplitudes, gaussianSmoothingSigma)\n    minPeakDurationSamples = int(math.ceil(minPeakDuration \/ sampleInterval))\n    peakIndices, _ = find_peaks(smoothed, width=minPeakDurationSamples) \n    peakWidthResult = peak_widths(smoothed, peakIndices, peakRelHeight)\n    \n    leftPeakTimes = time[np.rint(peakWidthResult[2]).astype(int)]\n    rightPeakTimes = time[np.rint(peakWidthResult[3]).astype(int)]\n    leftRightBoundaries = list(zip(leftPeakTimes, rightPeakTimes))\n    \n    peakTiming = time[peakIndices]\n    if plot:\n        plt.figure(figsize=(16,5))\n        plt.plot(time, amplitudes, color=\"tab:gray\", label=\"Original Signal\") \n        plt.plot(time, smoothed, color=\"tab:orange\", label=\"Smoothed\")\n        plt.plot(peakTiming, smoothed[peakIndices], \"v\", color=\"red\", markersize=10)\n        plt.hlines(peakWidthResult[1], leftPeakTimes , rightPeakTimes , color=\"red\")\n        plt.xlabel(\"Time(s)\")\n        plt.ylabel(\"Intensity\")\n        plt.title(\"Peak Locations and Width (Red Markers)\")\n        plt.legend()\n        plt.show()\n        \n    return (smoothed, peakTiming, leftRightBoundaries)\n    \n\n_  = findPeaksAndWidthsFromSmoothedCurve(amp, time, windowSizeSeconds, plot=True)","51db7eae":"#Calulate power envelop from sepctrogram\n#(time(s):float[], amp:float[]) \npowerEnvelops = [getPowerEnvelop(spectrogram, upperCutoffFreq, False) for spectrogram in spectrograms]","9be73acb":"from sklearn.model_selection import train_test_split\n\ntrainIndices, testIndices = train_test_split(range(len(recordingInfoFilePaths)), test_size=0.2, random_state=0xDEADBEEF)\n\ndef multiIndexList(sourceList, indices):\n    return [obj for idx, obj in enumerate(sourceList) if idx in indices]\n\ntrainFileNames = multiIndexList(audioFilePathsWithoutExtension, trainIndices)\ntestFileNames = multiIndexList(audioFilePathsWithoutExtension, testIndices)","d1c669d8":"#Peak identification from power envelop\n#(smoothedSignal:float[], peakTiming:seconds[], peakLeftRightBoundaries:(left:float,right:float)[])[]\npeakData = [findPeaksAndWidthsFromSmoothedCurve(amp, time, windowSizeSeconds) for time, amp in powerEnvelops]\ntrainPeakData =  multiIndexList(peakData, trainIndices)\ntestPeakData =  multiIndexList(peakData, testIndices)","31cbed09":"#Compare the difference between the middle of the detected cycle and the middle of the closest hand-annotated cycle\n#Repeat for the left\/right boundaries as well\n#This will be used to fudge the boundaries to tune the left\/right padding values.\nfrom scipy.sparse.csgraph import min_weight_full_bipartite_matching\nfrom scipy.sparse import csr_matrix\n\n#(handAnnotatedCentres: float(seconds)[], autoExtractedCentes:float(seconds)[]) => (handAnnotatedIndices:int[], autoExtractedIndices:int[])\n#matches the auto-extracted centres with the hand-annotated centres. Each point can only be matched once \n#(i.e. one hand-annotated centre will not be matched with more than 1 auto-extracted centre)\ndef bipartiteMatchWithClosestCentres(handAnnotatedCentres, autoExtractedCentes): \n    nRows = len(autoExtractedCentes) \n    nCols = len(handAnnotatedCentres)   \n    weights = np.zeros((nRows, nCols))\n    \n    handAnnotatedArr = np.array(handAnnotatedCentres)\n    #compute euclidean distance (1-D)\n    for row, extractedCentre in enumerate(autoExtractedCentes):\n        #Add 1 since the bipartite match requires non-zero weights\n        distVec = np.abs(np.subtract(handAnnotatedArr, extractedCentre)) + 1 \n        weights[row, :] = distVec\n    \n    weightsCSR = csr_matrix(weights)\n    \n    matchedRows, matchedCols = min_weight_full_bipartite_matching(weightsCSR)\n    #(handAnnotatedIndices:int[], autoExtractedIndices:int[])\n    return (matchedCols, matchedRows)","a1757ecb":"def calculateBoundaryError(audioFileName, autoDetectedPeaks, paddingTime):\n    leftBoundaryDiff = []\n    rightBoundaryDiff = []\n    centreDiffList = []\n    \n    for filename, autoPeakInfo in zip(audioFileName, autoDetectedPeaks):\n        #Get boundaries from hand-annotated cycles\n        handAnnotatedInfo = fileInfoDict[filename]\n        handAnnLeft = np.array([info[0] for info in handAnnotatedInfo])\n        handAnnRight = np.array([info[1] for info in handAnnotatedInfo])\n        handAnnCentre = np.array([(info[0] + info[1]) * 0.5 for info in handAnnotatedInfo])\n        \n        #Get centres form automatically extracted cycles\n        leftRightBoundaries = autoPeakInfo[2]\n        autoLeft = np.array([left - paddingTime[0] for left, _ in leftRightBoundaries]) \n        autoRight = np.array([right + paddingTime[1] for _, right in leftRightBoundaries])\n        autoCentre = np.array([0.5 * (left + right) for left, right in zip(autoLeft, autoRight)])\n        \n        #Min. Bipartite Match the centres to pair the most appropriate auto-extracted centre to a hand-annotated centre\n        handAnnIndices, autoExtractedIndices = bipartiteMatchWithClosestCentres(handAnnCentre, autoCentre)\n        \n        #Now compute the diff. for each cycle extracted from peak-detection.\n        #diff from ground truth (<ground-truth> - <peak-detection-method>), \n        #-ve values mean the peak detection method detects later than the ground truth (appear right on the graphs)\n        leftDiff = np.subtract(handAnnLeft[handAnnIndices], autoLeft[autoExtractedIndices])\n        rightDiff = np.subtract(handAnnRight[handAnnIndices], autoRight[autoExtractedIndices])\n        centreDiff = np.subtract(handAnnCentre[handAnnIndices], autoCentre[autoExtractedIndices])\n        \n        leftBoundaryDiff += leftDiff.tolist()\n        rightBoundaryDiff += rightDiff.tolist()\n        centreDiffList += centreDiff.tolist()\n        \n    return (leftBoundaryDiff, rightBoundaryDiff, centreDiffList)\n\n#Values to be iteratively fine-tuned using the hand-annotated data via the method shown below\npaddingTimeInit = (0.305, 1.391) #Note, this has already been refined by the optimizer\n\nboundaryDiff = calculateBoundaryError(trainFileNames, trainPeakData, paddingTimeInit)","0c1d5296":"def plotBoundaryErrorSummary(overallCaption, leftBoundaryDiff, rightBoundaryDiff, centreDiffList):\n    nBins = 50\n    fig, axs = plt.subplots(3, figsize=(6,10))\n    fig.suptitle(\n        overallCaption + \n        '\\nError Values between ground truth and detected boundaries\\n(-ve values indicate detected boundaries are late)\\n'\n        , fontsize=18\n    )\n    \n    axs[0].hist(leftBoundaryDiff, nBins)\n    axs[0].set_title(\"Left boundary error (Ground Truth - Detected)\")\n    axs[0].set_xlim([-5, 5])\n    \n    axs[1].hist(rightBoundaryDiff, nBins)\n    axs[1].set_title(\"Right boundary error (Ground Truth - Detected)\")\n    axs[1].set_xlim([-5, 5])\n    \n    axs[2].hist(centreDiffList, nBins)\n    axs[2].set_title(\"Cycle Centre error (Ground Truth - Detected)\")\n    axs[2].set_xlim([-5, 5])\n    \n    for ax in axs.flat:\n        ax.set(xlabel='Error(seconds)', ylabel='No. of instances')    \n        \n    plt.tight_layout() \n    plt.show()\n    \n    print(\"-Mean Errors-\\nLeft Boundary:{0}\\nRight Boundary:{1}\\nCentre:{2}\"\n          .format(np.mean(leftBoundaryDiff), np.mean(rightBoundaryDiff),np.mean(centreDiffList)))\n    \n#plotBoundaryErrorSummary(\"[Training Set]\", *boundaryDiff)","89acd695":"from scipy.optimize import minimize\n\ndef boundaryErrorObj(leftRight):\n    leftDiff, rightDiff, _ = calculateBoundaryError(trainFileNames, trainPeakData, leftRight)\n    return np.abs(np.sum(leftDiff)) + np.abs(np.sum(rightDiff)) #i.e. make the means as close to 0 as possible\n\n#optimalPaddingRes = minimize(boundaryErrorObj, paddingTimeInit, method=\u2019BFGS\u2019,  options={'disp':True, 'maxiter':1})\n#optimalPadding = optimalPaddingRes.x\noptimalPadding = paddingTimeInit #Value already optimized, commented lines above to save time.\nprint(optimalPadding)","cf2a75b4":"boundaryDiffOptimal = calculateBoundaryError(testFileNames, testPeakData, optimalPadding)\nplotBoundaryErrorSummary(\"[Training Set (Optimal Padding)]\", *boundaryDiffOptimal)","2d8dff7e":"boundaryDiffOptimalValidation = calculateBoundaryError(trainFileNames, trainPeakData, optimalPadding)\nplotBoundaryErrorSummary(\"[Validation Set (Optimal Padding)]\", *boundaryDiffOptimalValidation)","e5e56b49":"#Comparison between distribution of detected cycle times.\nhandAnnotatedWidths = []\nfor _, cycleInfoList in fileInfoDict.items():\n    for cycleInfo in cycleInfoList:\n        handAnnotatedWidths.append(cycleInfo[1] - cycleInfo[0])\n\npeakDetectionWidths = []\nfor peaks in peakData:\n    paddingOffset = sum(optimalPadding) #The times are before the left\/right pads are added to the sample, re-add pad to get actual extracted length\n    for leftRightBoundary in peaks[2]:\n        peakDetectionWidths.append(leftRightBoundary[1] - leftRightBoundary[0] + paddingOffset)\n        \nprint(\"No. of cycles from Hand Annotation:{0}\".format(len(handAnnotatedWidths)))\nprint(\"No. of cycles from Peak Detection:{0}\".format(len(peakDetectionWidths)))\nprint(\"95th Percentile of Peak Detection Cycle Lengths:{:.2f}s\".format(np.percentile(peakDetectionWidths, 95)))\n\nnBins = 50\nplt.figure(figsize=(16,5))\nplt.hist(handAnnotatedWidths, nBins, alpha=0.5, label='Hand Annotated')\nplt.hist(peakDetectionWidths, nBins, alpha=0.5, label='Peak Detection Method (Inclusive of padding)')\nplt.title(\"Cycle Widths(s) from Manual Annotation v.s. Peak Detection\")\nplt.ylabel(\"No. of cycles\")\nplt.xlabel(\"Cycle Length(s)\")\nplt.legend(loc='upper right')\nplt.show()","97bc6950":"def clamp(val, lowerInclusive, upperInclusive):\n    if val > upperInclusive:\n        return upperInclusive\n    elif val < lowerInclusive:\n        return lowerInclusive\n    else:\n        return val\n    \n#(timeSegments:(left_seconds:float, right_seconds:float), audioSamples:float[], sampleRate:int) => float[][]\ndef sliceAudio(timeSegments, audioSamples, sampleRate):\n    maxSampleIdx = len(audioSamples) - 1\n    segments = []\n    for boundaries in timeSegments:\n        left = clamp(int(boundaries[0] * sampleRate), 0, maxSampleIdx)\n        right = clamp(int(boundaries[1] * sampleRate), 0, maxSampleIdx)\n        segments.append(audioSamples[left:right])\n    return segments\n\n#(rawAudio:float[], \n# peakInfo:(smoothedSignal:float[], peakTiming:seconds[], peakLeftRightBoundaries:(left:float,right:float)[]), \n# padding:(left:float, right:float), \n# sampleRate:int)\n#   => (audioClips:float[][], peakTimes:float[], leftRightBoundaries:(float,float)[])\ndef extractAudioWithPeakInfo(rawSignal, peakInfo, padding, sampleRate):\n    maxSampleIdx = len(rawSignal) - 1\n    maxTime = maxSampleIdx * sampleRate\n    leftPadding, rightPadding = padding\n    leftRightBoundaries = [(clamp(left - leftPadding, 0, maxTime), clamp(right + rightPadding, 0, maxTime)) for left, right in peakInfo[2]]\n    audioSegments = sliceAudio(leftRightBoundaries, rawSignal, sampleRate)\n    return (audioSegments, peakInfo[1], leftRightBoundaries)\n\n#Get sample slices\n#(audioClips:float[][], peakTimes:float[], leftRightBoundaries:(float,float)[])[]\nextractedAudioClips = [extractAudioWithPeakInfo(signal, peakInfo, optimalPadding,gSampleRate) for signal, peakInfo in zip(noiseRemoved, peakData)]","d98a945c":"def plotAllSamples(audioSegments):\n    nPlots = len(audioSegments)\n    cols = int(math.ceil(math.sqrt(nPlots)))\n    rows = math.ceil(nPlots \/ cols)\n    fig, axs = plt.subplots(rows, cols, figsize=(16,9))\n    for idx, sample in enumerate(audioSegments):\n        row = idx \/\/ cols\n        col = idx % cols\n        ax = axs[row, col]\n        ax.plot(sample)\n        ax.set_title(\"Segment #{0}\".format(idx + 1))\n        \n    for ax in axs.flat:\n        ax.set(xlabel='Sample Index', ylabel='Amplitude')\n\n    for ax in axs.flat:\n        ax.label_outer()\n        \n    plt.tight_layout()   \n    plt.show()\n    \n#plotAllSamples(onlyPeaks)\nidx = 19\nselectedClip = extractedAudioClips[idx][0]\nplotAllSamples(selectedClip)\ntime, amp = getPowerEnvelop(spectrograms[idx], upperCutoffFreq, False)\n_  = findPeaksAndWidthsFromSmoothedCurve(amp, time, windowSizeSeconds, plot=True)\nprint(\"Original\")\nipd.display(ipd.Audio(audioBuffers[idx], rate=gSampleRate))\nfor i in range(len(selectedClip)):\n    print(\"Segment #{0}\".format(i + 1))\n    ipd.display(ipd.Audio(selectedClip[i], rate=gSampleRate))","88eebf7c":"#(containingRegion:(left:float,right:float), targetRegion:(left:float,right:float)) => overlapFrac(0-1):float\ndef getOverlapFrac(containingRegion, targetRegion):\n    overlap = max(0, min(containingRegion[1], targetRegion[1]) - max(containingRegion[0], targetRegion[0]))\n    overlapFrac = overlap \/ (targetRegion[1] - targetRegion[0])\n    return overlapFrac\n\ndef fracOfSampleCovered(groundTruth, autoDetectedWindow):\n    overlap = max(0, min(groundTruth[1], autoDetectedWindow[1]) - max(groundTruth[0], autoDetectedWindow[0]))\n    fracOfSampleCovered = overlap \/ (groundTruth[1] - groundTruth[0])\n    return fracOfSampleCovered\n\n#Applying labels to auto-extracted data\n#(hand-annotated-labels:(start, end, hasCrackles, hasWheezes)[], auto-extracted-segments:(left,right)[], minOverlapFrac(0-1):float) => \n#transfered-labels:(left,right, hasCrackles, hasWheezes)[]\n#Any segment where the overlap ratio with a hand annotated segment is more than <minOverlapFrac> will have the crackles\/wheezes label applied\n#minOverlapFrac is defined as the intersection between the hand-annotated segment and auto-detected segment, divided by the size of the auto-detected segment\ndef transferLabelToExtractedSegment(handAnnotation, automaticSegmentTimes, minOverlapFrac):\n    transferredLabels = []\n    handAnnotatedSegments = [(left, right) for left, right, _ , _ in handAnnotation]\n    handAnnotatedLabels = [(hasCrackles, hasWheezes) for _, _, hasCrackles, hasWheezes in handAnnotation]\n    for autoSegment in automaticSegmentTimes:\n        overlapFractions = [getOverlapFrac(groundTruth, autoSegment) for groundTruth in handAnnotatedSegments]\n        fracOfAnnSampleCovered = [fracOfSampleCovered(groundTruth, autoSegment) for groundTruth in handAnnotatedSegments]\n        allSelectedLabels = [labels if (oFrac > minOverlapFrac or sFrac > minOverlapFrac) else (False, False) for oFrac, sFrac, labels in zip(overlapFractions,fracOfAnnSampleCovered, handAnnotatedLabels)]\n        hasCrackles = any([bCrackles for bCrackles, _ in allSelectedLabels])\n        hasWheezes = any([bWheezes for _, bWheezes in allSelectedLabels])\n        transferredLabels.append((*autoSegment, hasCrackles, hasWheezes))\n    return transferredLabels\n\n#illustrative purposes\ntestIdx = 0\nhandAnnotations = fileInfoList[testIdx]\nautoExtractedSegments = extractedAudioClips[testIdx][2]\nprint(handAnnotations)\n#print(autoExtractedSegments)\nprint(transferLabelToExtractedSegment(handAnnotations, autoExtractedSegments, 0.5))","f01997a1":"#Apply annotation transfer to all cycles.\n\n#(handAnnotationList:(start, end, hasCrackles, hasWheezes)[],\n# autoSegmentList:(audioClips:float[][], peakTimes:float[], leftRightBoundaries:(float,float)[])[], \n# minOverlapFrac:float) =>\n#    autoSegmentWithLabels(audioClips:float[][], peakTimes:float[], cycleInfo:(float,float, hasCrackles, hasWheezes)[])[]\ndef transferLabels(handAnnotationList, autoSegmentList, minOverlapFrac):\n    autoSegmentWithTransferLabels = []\n    for handAnn, autoSeg in zip(handAnnotationList,autoSegmentList):\n        xferredLabels = transferLabelToExtractedSegment(handAnn, autoSeg[2], minOverlapFrac)\n        autoSegmentWithTransferLabels.append((autoSeg[0], autoSeg[1], xferredLabels))\n    return autoSegmentWithTransferLabels\n                                             \nminOverlapFrac = 0.5\ntransferredLabels = transferLabels(fileInfoList, extractedAudioClips, minOverlapFrac)","2f43a72d":"nAudioClips = len(fileInfoList)\nindices = range(0, nAudioClips)\n\ntrainIndices, testIndices = train_test_split(indices, test_size=0.2, random_state=0xDEADBEEF)\n\n#Normalizes all spectrograms on a per spectrogram basis\n#(segmentsWithLabels:(audioClips:float[][], peakTimes:float[], cycleInfo:(float,float, hasCrackles, hasWheezes)[])[],\n# windowSize(seconds):float, sampleRate:int) =>\n#    (spectrogram:float[][][], labels(hasCrackles, hasWheezes)[])[]\ndef getCorrespondingSpectroGrams(segmentsWithLabels, windowSize, sampleRate, fftSampleWindowSize):\n    extractedSpectrogramList = []\n    maxSamples = int(windowSize * sampleRate)\n    for segmentInfo in segmentsWithLabels:\n        clipInfo = []\n        for idx, cycleInfo in enumerate(segmentInfo[2]):\n            sampleBuffer = np.zeros(maxSamples)\n            audioClip = segmentInfo[0][idx]\n            #If the extracted clip can fit within the window\n            #Simply copy it to the front of the buffer\n            if len(audioClip) <= maxSamples:\n                sampleBuffer[:len(audioClip)] = audioClip\n            #Otherwise, fit the centre of the peak into the buffer\n            else:\n                nClipSamples = len(audioClip)\n                peakTiming = segmentInfo[1][idx]\n                left, right = cycleInfo[:2]\n                peakIdx = int((peakTiming - left) * sampleRate)\n                rightIdx = int(min(peakIdx + (maxSamples \/\/ 2), nClipSamples))\n                leftIdx = int(max(0, rightIdx - maxSamples))\n                sampleBuffer[:(rightIdx - leftIdx)] = audioClip[leftIdx: rightIdx]\n            labels = cycleInfo[2:]\n            \n            melSpec = librosa.feature.melspectrogram(sampleBuffer, sr=sampleRate,  n_fft=fftSampleWindowSize)\n            melMin = np.min(melSpec)\n            melMax = np.max(melSpec)\n            melNorm = melSpec \/ (melMax - melMin)\n            clipInfo.append((np.power(melNorm, 0.2), labels))\n        extractedSpectrogramList.append(clipInfo)\n    return extractedSpectrogramList\n\nmodelWindowSize = 4.0 #(seconds) to fit 95th percentile of sample widths\nfftWindowSizeSeconds = 0.025\nfftWindowSizeSamples = int(fftWindowSizeSeconds * gSampleRate)\n\ntrainingSet = multiIndexList(transferredLabels, trainIndices)\ntestSet = multiIndexList(transferredLabels, testIndices)\n\ntrainingLabelledSpectrograms = getCorrespondingSpectroGrams(trainingSet, modelWindowSize, gSampleRate, fftWindowSizeSamples)\ntestLabelledSpectrograms = getCorrespondingSpectroGrams(testSet, modelWindowSize, gSampleRate, fftWindowSizeSamples)","bcb68bb3":"plt.pcolormesh(trainingLabelledSpectrograms[0][0][0], shading='gouraud')","fe113309":"#(spectrogram:float[][], labels(hasCrackles, hasWheezes)[])[] -> (flattened_spectrogram[], hasCrackles[], hasWheezes[])\ndef flattenLabelledData(labelledData):\n    flattenedSpectrograms = []\n    crackleList = []\n    wheezesList = []\n    for specLabelPairList in labelledData:\n        for idx, specLabelPair in enumerate(specLabelPairList):\n            spec, label = specLabelPair\n            #flattenedSpectrograms.append(spec.reshape(-1))\n            #flattenedSpectrograms.append(spec.flatten('F')) \n            flattenedSpectrograms.append(spec) \n            crackleList.append(label[0])\n            wheezesList.append(label[1])\n    #return (flattenedSpectrograms, crackleList, wheezesList)\n    return (flattenedSpectrograms, crackleList, wheezesList)\n\ntrainSpec1D, trainCrackleLabels, trainWheezeLabels = flattenLabelledData(trainingLabelledSpectrograms)\ntestSpec1D, testCrackleLabels, testWheezeLabels = flattenLabelledData(testLabelledSpectrograms)","7e6f2bb8":"inputRows, inputCols = trainSpec1D[0].shape\n\ndef getLabels(crackleList, wheezeList):\n    oneHotLabels = {\n        (False, False): [1,0],\n        (True, False): [0,1],\n        (False, True): [0,1],\n        (True, True): [0,1]\n    }\n    oneHot = []\n    for crackleWheezePair in zip(crackleList, wheezeList):\n        oneHot.append(oneHotLabels[crackleWheezePair])\n    return np.array(oneHot)\n\ntrainLabels = getLabels(trainCrackleLabels, trainWheezeLabels)\ntrainingDataNp = np.reshape(np.array(trainSpec1D), (-1,inputRows, inputCols,1))\n\ntestLabels = getLabels(testCrackleLabels, testWheezeLabels)\ntestDataNp = np.reshape(np.array(testSpec1D), (-1,inputRows, inputCols,1))","d056fbcb":"trainTotalPerClass = np.sum(trainLabels, axis=0)\ntrainClassWeights = np.sum(trainTotalPerClass) \/ trainTotalPerClass\ntrainClassWeightsDict = dict(enumerate(trainClassWeights))\nprint(trainTotalPerClass)\nprint(trainClassWeightsDict)","a502ae88":"import random \n\ndef spectorgramTemporalDistortion(spectrogram, col1, col2, offset1, offset2):\n    nRows, nCols = spectrogram.shape\n    colIndices = np.arange(nCols)\n    remappedCols = np.array([0, col1 + offset1, col2 + offset2, nCols - 1])\n    originalCols = np.array([0, col1, col2, nCols - 1]) \n    remappedIndices = np.interp(colIndices, originalCols, remappedCols)\n    \n    distorted = np.zeros((nRows, nCols))\n    for row in range(nRows):\n        distorted[row,:] = np.interp(remappedIndices, colIndices, spectrogram[row,:])\n    return distorted\n\ndef blotOutRow(spectrogram, row, width):\n    nRows, _ = spectrogram.shape\n    rowIndices = np.arange(nRows)\n    dipBegin = clamp(row - width \/2 , 0, nRows - 1)\n    dipEnd = clamp(row + width \/2, 0, nRows - 1)\n    amp = np.array([1,1,0,1,1])\n    indices = np.array([0, dipBegin, row, dipEnd, nRows - 1])\n    scalingFactor = np.interp(rowIndices, indices, amp)\n    blotted = scalingFactor[:,None] * spectrogram\n    return blotted\n    \ndef blotOutCol(spectrogram, col, width):\n    _ , nCols = spectrogram.shape\n    colIndices = np.arange(nCols)\n    dipBegin = clamp(col - width \/2 , 0, nCols - 1)\n    dipEnd = clamp(col + width \/2, 0, nCols - 1)\n    amp = np.array([1,1,0,1,1])\n    indices = np.array([0, dipBegin, col, dipEnd, nCols - 1])\n    scalingFactor = np.interp(colIndices, indices, amp)\n    blotted = scalingFactor[None,:] * spectrogram\n    return blotted\n\ndef offsetSpectrogram(spectrogram, xOffset):\n    nRows, nCols = spectrogram.shape\n    offsetCopy = np.zeros((nRows, nCols))\n    nColsToTake = nCols - xOffset\n    offsetCopy[:,xOffset:] = spectrogram[:,:nColsToTake]\n    return offsetCopy\n\ndef randomlyApplyDistortions(spectrogram, temporalProb, rowBlotProb, colBlotProb, offsetProb):\n    acc = spectrogram\n    nRows , nCols = spectrogram.shape\n    \n    if random.random() < offsetProb:\n        offset = int(random.uniform(0, nCols * 0.3))\n        acc = offsetSpectrogram(acc, offset)\n        \n    if random.random() < temporalProb:\n        col1Centre = nCols * 0.25\n        col2Centre = nCols * 0.7\n        col1 = col1Centre + random.uniform(-nCols * 0.1, nCols * 0.1)\n        col2 = col2Centre + random.uniform(-nCols * 0.1, nCols * 0.1)\n        offset1 = random.uniform(-nCols * 0.075, nCols * 0.075)\n        offset2 = random.uniform(-nCols * 0.075, nCols * 0.075)\n        acc = spectorgramTemporalDistortion(acc, col1, col2, offset1, offset2)\n        \n    if random.random() < rowBlotProb:\n        row = random.uniform(0, nRows - 1)\n        width = random.uniform(1, nRows * 0.1)\n        acc = blotOutRow(acc, row, width)\n    \n    if random.random() < colBlotProb:\n        col = random.uniform(0, nCols - 1)\n        width = random.uniform(1, nCols * 0.1)\n        acc = blotOutCol(acc, col, width)\n\n    return acc","ce902ee1":"fig, axs = plt.subplots(1,2, figsize=(14,6))\n\naxs[0].set_title(\"Original\")\naxs[0].pcolormesh(trainingLabelledSpectrograms[0][0][0], shading='gouraud')\n\naugmented = randomlyApplyDistortions(trainingLabelledSpectrograms[0][0][0], 1, 1, 1, 1)\naxs[1].set_title(\"Augmented\\n(Time\/Freq Masking, Delayed Start & Temporal Distortion)\")\naxs[1].pcolormesh(augmented, shading='gouraud')\n \nfor ax in axs.flat:\n    ax.set(xlabel='Time steps', ylabel='Freq bins')    \n        \nplt.tight_layout() \nplt.show()","c8d07ca0":"from tensorflow.keras.utils import Sequence\n\nclass SpectrogramSequence(Sequence):\n\n    def __init__(self, x_set, y_set, batch_size):\n        self.x, self.y = x_set, y_set\n        self.batch_size = batch_size\n        self.shuffleIndices()\n    \n    def shuffleIndices(self):\n        nSamples = len(self.x)\n        indices = np.arange(nSamples)\n        np.random.shuffle(indices)\n        self.indices = indices\n    \n    def on_epoch_end(self):\n        self.shuffleIndices()\n        \n    def __len__(self):\n        return math.ceil(len(self.x) \/ self.batch_size)\n\n    def __getitem__(self, idx):\n        startingIdx = idx * self.batch_size\n        endingIdx = (idx + 1) * self.batch_size\n        selecedIndices = self.indices[startingIdx:endingIdx]\n        \n        rawSpectrogramBatch = np.reshape(self.x[selecedIndices], (-1,inputRows, inputCols))\n        augmentedSpectrograms = np.array([randomlyApplyDistortions(spectrogram, 0.5, 0.25, 0.25, 0.75) for spectrogram in rawSpectrogramBatch])\n        \n        batch_x = np.reshape(augmentedSpectrograms, (-1,inputRows, inputCols, 1))\n        batch_y = self.y[selecedIndices]\n\n        return (batch_x, batch_y)","bd8d9591":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nkeras.backend.clear_session()\n\nmodel = keras.Sequential(\n    [\n        layers.InputLayer(input_shape=(inputRows, inputCols, 1)),\n        \n        layers.Conv2D(96, 7),\n        layers.LeakyReLU(alpha=0.1),\n        layers.MaxPooling2D(2),\n        \n        layers.Conv2D(128, 5),\n        layers.LeakyReLU(alpha=0.1),\n        layers.MaxPooling2D(2),\n        \n        layers.Conv2D(128, 3),\n        layers.LeakyReLU(alpha=0.1),\n        layers.MaxPooling2D(2),\n        layers.SpatialDropout2D(0.1),\n        \n        layers.Conv2D(256, 3),\n        layers.LeakyReLU(alpha=0.1),\n        layers.MaxPooling2D(2),\n        layers.SpatialDropout2D(0.1),\n        \n        layers.Flatten(),\n        \n        layers.Dense(4096),\n        layers.LeakyReLU(alpha=0.1),\n        layers.Dropout(0.5),\n        \n        layers.Dense(256),\n        layers.LeakyReLU(alpha=0.1),\n        \n        layers.Dense(2, activation='Softmax')\n    ]\n)\n\nopt = keras.optimizers.Adam(learning_rate=0.00005)\nmodel.compile(optimizer=opt, loss = 'categorical_crossentropy', metrics = ['acc'])\nmodel.summary()","2ff5674c":"batch_size=128\ngenerator = SpectrogramSequence(trainingDataNp, trainLabels, batch_size)\nhistory = model.fit(\n    x=generator,\n    epochs=100, #class_weight=trainClassWeightsDict,\n    validation_data=(testDataNp, testLabels))","4cb521bd":"plt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['training', 'validation'])\nplt.show()","de1de220":"predictions = model.predict(testDataNp)","9b22a73f":"from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\ngroundTruthClasses = np.argmax(testLabels, axis = 1)\npredictedClasses = np.argmax(predictions, axis = 1)\n\nprint(classification_report(groundTruthClasses, \n                            predictedClasses, \n                            target_names = ['none','crackles, wheezes or both']))\nprint(confusion_matrix(groundTruthClasses, predictedClasses))\nprint(\"Overall Accuracy: {0}\".format(accuracy_score(groundTruthClasses, predictedClasses)))","9209889c":"# Results\nHave a listen to some of the automatically extracted clips. This method works reasonably well when the audio source isn't flooded with noise, but tends to generate overlapping clips for noiser sources.","61c96ea1":"# Peak detection from Spectrograms\n\nSpectrograms are generated using a FFT window size of 50ms. The resultant bins in the spectrogram are multiplied by the square of their frequency, as the power of a sound wave is given by the following relation:\n\n$$ P \\propto Amplitude^{2} \\times Frequency^{2}$$\n\nThe amplitude square relation is ignored in this case since omitting it results in cleaner peaks.\n\nTo filter out transients (i.e. knocks), a Guassian filter is used to smooth out the curve. Peak detection is then performed on the smoothed curve. The timings corresponding to the left\/ right bases (relative to 80% of peak height) of each peak are then recorded.","fbe4b9ab":"# Validation of the offset values\n\nThe Left\/Right boundary errors are still mainly centred around 0s, with the mean errors also being fairly small (on the order of $10^{-4}$ seconds), indicating that fitting a simple constant to the training set yields results that generalize well.","5ac30d65":"# Transfer of labels\n\nDataset labels for the presence of crackles\/wheezes can be transferred using an intersection over union approach(IoU).\n\nIf the extracted sample entirely covers a hand-annotated label with a positive crackle\/wheeze label, it will assume the label of the hand-annotated cycle. Otherwise, positive labels will only be copied if the IoU between the estimated cycle and hand annotated label is greater than 50%.","6641e458":"# Basic Pre-processing\n\nAll audio files are resampled to 7000Hz. The resultant Nyquist frequency is covers the range for lung sounds (25-1500Hz <a href=\"https:\/\/telehealthtechnology.org\/toolkit\/electronic-stethoscopes-technology-overview\/\">[1]<\/a>).\n\nDue to the prevalance of handling noise (bumps\/knocks), which manifest as Low-Frequency noise, a brick-wall high pass filter of 80Hz is applied.\n\nLog Compression is then used to reduce the differences in dynamic range within the audio clips.","a47814ab":"# Obtaining the left\/right offsets for each breathing cycle\n\nThe left\/right peak bases will be used as the reference for start\/end of a breathing cycle. A constant offset for the left and right base will be applied to estimate the starting\/ending times of the breathing cycle.\n\n$$ \\text{Breathing Cycle}_{\\text{start}} = \\text{Peak Base}_{\\text{left}} - \\text{Offset}_{\\text{left}}$$\n$$ \\text{Breathing Cycle}_{\\text{end}} = \\text{Peak Base}_{\\text{right}} + \\text{Offset}_{\\text{right}}$$\n\nThe values for $\\{\\text{Offset}_{\\text{left}}, \\text{Offset}_{\\text{right}}\\}$ will be found by finding the value that minimizes error between the estimated start\/end of each breathing cycle with the hand-annotated dataset. A multivariate optimization funciton is used to search for the offset values, with the objective function function being the sum of the mean errors of the start\/end time between the estimated and hand-annotated cyles.\n\nAs there are more detected peaks than hand-annotated cycles, only the peaks that are closest to the hand-annotated cycles will be used for comparison. Unpaired peaks are not considered in the objective funciton. The metric for closeness to a hand-annoated cycle will be the time difference between the peak and the middle of the hand-annotated cycle. A min-weight bipartite search is used for the pairing process.\n\nTo prove that this method generalizes well to the entire dataset, a train-test-split (test=20%) methodology was adopted, where the offset values found using the training dataset are then evaluated by comparing the mean left\/right boundary errors within the test dataset.","035bc155":"**Note:** The multivariate optimizer has already been run for a multiple cycles to produce <i>\"paddingTimeInit\"<\/i>. Given that each evaluation of the objective function requires multiple bipartite matches across the entire dataset, it takes quite a while to run even for a few iterations. It has been commented out to save time.","69e4b71a":"# Introduction\n\nDeployment is the end goal of any Machine-Learning based project. Whilst this dataset provides conveniently labelled timings of each breathing cycle to train ML based models, extracting similar timings from new audio clips for a trained model remains an unresolved issue.\n\nThis notebook strives to produce an automated method of extracting breathing cycle timings from audio clips for ML model training, as well as to generate the input data for a deployed ML model.\n\nThe crux of the method revolves around peak detection of the sound intensity envelop. Estimates of the start and end of each breathing cycle are then fine-tuned using data from the hand-annotated breathing cycles. The following is a rough breakdown of the steps:\n\n**Peak Extraction**\n\n1. Noise Removal(Bandpass Filtering) + Log Compression\n2. Computation of Intensity envelop from spectrogram\n3. Peak detection of Guassian-smoothed peak intensity envelop (Left\/Right Base of peaks also recorded)\n\n**Breathing Cycle Start\/End Estimate**\n\n4. Represent the start\/end of each cycle as the left\/right base of each extracted peak with an outward offset\n5. Among the detected peaks, use a min-weight Bipartite match to match peaks with the closest hand-annotated breathing cycle (Each cycle may only be paired with 1 peak, unpaired peaks are ignored)\n6. Perform multivariate optimization to find the appropriate offset values to minimize the *mean* error of each start\/end cycle with the paired hand-annotated breathing cycle timings.\n\nAfter performing these steps, peaks may be extracted by finding a peak and the timing of the left & right bases. An audio clip is extracted from the left \/ right base with the offset found above added to get an audio clip of a breathing cycle.\n\nFor training data, labels can be transferred to the automatically detected cycle timings via Intersection over Union metrics.","383bd6b2":"# Extra: Training of a CNN using the extracted samples\n\nWhile not the focus of this notebook, the automatically extracted samples will be used to train a CNN to differentiate between healthy breathing sound clips, and those which contain wheezes and\/or crackles (binary classification).\n\nBasic details are as follows:\n* Input Features: Mel-Spectograms\n    * Classes (2)\n       1. Healthy\n       2. Wheezes, Crackles or Both\n* Data Augmentation\n    * Frequency & Time masking\n    * Temporal distortion (The spectrogram is stretched\/squeezed at 2 different points)\n    * Variable spectrogram starting delay (i.e. the entire spectrogram is shifted right by a random amount)"}}