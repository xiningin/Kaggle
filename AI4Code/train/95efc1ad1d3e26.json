{"cell_type":{"c771e258":"code","b02cc88b":"code","a505cc9c":"code","325be5d4":"code","cce1fb06":"code","4454f717":"code","760f23f0":"code","3b0d4196":"code","033e1cab":"code","c5d20364":"code","49ff1bec":"code","96c7bfc3":"code","25166f28":"code","099c0da1":"code","515b9ba8":"code","79b5c6d6":"code","df9b0094":"code","e911af8f":"code","e61f50b5":"code","9987a2fa":"code","067fbfbc":"code","30793c99":"code","222637b4":"code","807521e2":"code","f28d3fc1":"code","ed949b56":"code","935dec17":"code","48a30c54":"code","769786d2":"code","be6e24e7":"code","cdd7c0af":"code","4c584559":"code","03e49949":"code","a09afea3":"code","b5076ac7":"code","d1093738":"code","ce500c7b":"code","6bc399f6":"code","180f3830":"code","f2b8b748":"code","839e268d":"code","ea5f3e08":"markdown","478f733b":"markdown","fd7ad4d8":"markdown","3f2cbec4":"markdown","ae519ac9":"markdown","a3a1a2fa":"markdown","023a913d":"markdown","59f3bd70":"markdown","445a991f":"markdown","a423d7e8":"markdown","51425ae6":"markdown"},"source":{"c771e258":"pip install torchsummary","b02cc88b":"import os\nimport numpy as np\nimport pandas as pd\nimport skimage.io\nimport skimage.transform\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, Subset, DataLoader\nimport torchvision.transforms as tfs\nfrom typing import Tuple, List, Dict\nfrom sklearn.model_selection import train_test_split\nfrom tqdm.notebook import tqdm\nfrom IPython.display import clear_output\nfrom torchsummary import summary","a505cc9c":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","325be5d4":"ARCHIVE = '..\/input\/labeled-faces-in-the-wild\/lfw-deepfunneled.tgz'\nATTRIBUTES = '..\/input\/labeled-faces-in-the-wild\/lfw_attributes.txt'\nWORKDIR = '..\/working\/lfw-deepfunneled\/'\nDEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'","cce1fb06":"# extract if not exists\nif not os.path.exists(WORKDIR):\n    print('extracting images...', end='')\n    os.system(f'tar xzf {ARCHIVE}')\n    print('done')\n    assert os.path.exists(WORKDIR)\n\n# read attributes\ndf_attrs = pd.read_csv(ATTRIBUTES, sep='\\t', skiprows=1,)\ndf_attrs = pd.DataFrame(df_attrs.iloc[:,:-1].values, columns=df_attrs.columns[1:])\n\n# read photos\nphotos = []\nfor dirname, _, filenames in os.walk(WORKDIR):\n    for filename in filenames:\n        if filename.lower().endswith('.jpg'):\n            filepath = os.path.join(dirname, filename)\n            photo_id = filename[:-4].split('_')\n            person = ' '.join(photo_id[:-1])\n            imagenum = int(photo_id[-1])\n            photos.append({'person': person, 'imagenum': imagenum, 'imagepath': filepath})\nphotos = pd.DataFrame(photos)\n\ndf = pd.merge(df_attrs, photos, on=('person', 'imagenum'))\nassert len(df) == len(df_attrs)\ndf.sample(5)","4454f717":"CROPX = 80\nCROPY = 80\nIMAGEX = 64\nIMAGEY = 64\nCHANNELS = 3\n\ntqdm.pandas(desc=\"Load\")\nphotos_all = df['imagepath'].progress_apply(skimage.io.imread)\ntqdm.pandas(desc=\"Convert\")\nphotos_all = photos_all.progress_apply(skimage.util.img_as_float32)\ntqdm.pandas(desc=\"Crop\")\nphotos_all = photos_all.progress_apply(lambda img: img[CROPY:-CROPY, CROPX:-CROPX])\ntqdm.pandas(desc=\"Resize\")\nphotos_all = photos_all.progress_apply(lambda img: skimage.transform.resize(img, (IMAGEY, IMAGEX)))\ntqdm.pandas(desc=\"Transpose\")\nphotos_all = photos_all.progress_apply(lambda img: img.transpose(2, 0, 1))\n\nphotos_all = np.stack(photos_all.values)\nattrs_all = df.drop(['person', 'imagenum', 'imagepath'], axis=1)","760f23f0":"# CROPX = 80\n# CROPY = 80\n# IMAGEX = 64\n# IMAGEY = 64\n\n# class CustomDataSet(Dataset):\n#     def __init__(self, imagepaths: np.ndarray):\n#         super().__init__()\n#         self.samples = imagepaths\n\n#     def __len__(self):\n#         return len(self.samples)\n\n#     def __getitem__(self, idx):\n#         imagepath = self.samples[idx]\n#         img = skimage.io.imread(imagepath)\n#         img = img[CROPY:-CROPY, CROPX:-CROPX]\n#         img = skimage.transform.resize(img, (IMAGEY, IMAGEX))\n#         tensor_img = img.transpose(2, 0, 1)\n#         tensor_img = torch.FloatTensor(tensor_img)\n#         return tensor_img\n\n# dataset = CustomDataSet(df['imagepath'].values)\n\n# idx = np.random.choice(len(dataset), len(dataset), replace=False)\n# idx_train, idx_valid = np.split(idx, [int(len(dataset) * 0.9)])\n\n# dataset_train = Subset(dataset, idx_train)\n# dataset_valid = Subset(dataset, idx_valid)\n\n# dataloader = {\n#     'train': DataLoader(dataset_train, batch_size=32),\n#     'valid': DataLoader(dataset_valid, batch_size=32),\n# }","3b0d4196":"photos_train, photos_valid, attrs_train, attrs_valid = train_test_split(\n    photos_all, attrs_all, train_size=0.9, shuffle=False\n)\n\ndataloader = {\n    'train': DataLoader(photos_train, batch_size=32, shuffle=True),\n    'valid': DataLoader(photos_valid, batch_size=32),\n}","033e1cab":"def show_faces(faces: np.ndarray, size: Tuple[int, int] = None):\n    if size is None:\n        rows = (len(faces) + 7) \/\/ 8\n        cols = len(faces) if len(faces) < 8 else 8\n    else:\n        rows, cols = size\n    fig, axes = plt.subplots(rows, cols, figsize=(cols * 2, rows * 2))\n    for ax, img in zip(axes.flatten(), faces):\n        ax.axis('off')\n        ax.imshow(img.numpy().transpose(1, 2, 0))","c5d20364":"batch = next(iter(dataloader['train']))\nshow_faces(batch)","49ff1bec":"class AutoencoderFC(torch.nn.Module):\n    def __init__(self, dim_code: int):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.encoder = nn.Sequential(\n            nn.Linear(in_features=(CHANNELS * IMAGEY * IMAGEX), out_features=256),\n            nn.BatchNorm1d(256),\n            nn.ReLU(),\n            nn.Dropout(0.25),\n            nn.Linear(in_features=256, out_features=512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(),\n            nn.Linear(in_features=512, out_features=dim_code),\n        )\n        self.decoder = nn.Sequential(\n            nn.Linear(in_features=dim_code, out_features=512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(),\n            nn.Dropout(0.25),\n            nn.Linear(in_features=512, out_features=256),\n            nn.BatchNorm1d(256),\n            nn.ReLU(),\n            nn.Linear(in_features=256, out_features=(CHANNELS * IMAGEY * IMAGEX)),\n        )\n        \n        \n    def forward(self, sample):\n        sample = self.flatten(sample).float()\n        latent_code = self.encoder(sample)\n        reconstructed = self.decoder(latent_code)\n        return reconstructed, latent_code","96c7bfc3":"DIM_CODE = 32\n\nautoencoder = AutoencoderFC(dim_code=DIM_CODE).to(DEVICE)\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(autoencoder.parameters())\nsheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)","25166f28":"def train_model(\n    model: nn.Module,\n    dataloader: Dict[str, DataLoader],\n    criterion: nn.Module,\n    optimizer: nn.Module,\n    epochs: int = 20\n) -> Dict[str, List[float]]:\n    \"\"\"Training the model. Returns dictionary of train and validation losses.\n    Args:\n        model (torch.nn.Module): Neural network\n        dataloader (Dict[str, DataLoader]):\n            Dictionary with 'train' and 'valid' dataloaders\n        criterion (torch.nn.Module): Cost function\n        optimizer (torch.nn.Module): Optimization algorithm\n        epochs (int): Number of training iterations. Default: 20\n    \"\"\"\n\n    losses = {'train': [], 'valid': []}\n    progress = tqdm(range(epochs), desc='Epoch')\n    \n    for epoch in progress:\n        for phase in ['train', 'valid']:\n            loss_accum = []\n            model.train(mode=(phase == 'train'))\n            \n            #for inputs in tqdm(dataloader[phase], desc=f'Phase {phase}'):\n            for inputs in dataloader[phase]:\n                inputs = inputs.to(DEVICE)\n                # print(inputs.shape)\n                \n                if phase == 'train':\n                    optimizer.zero_grad()\n                    outp, _ = model(inputs)\n                    outp = outp.view(-1, CHANNELS, IMAGEY, IMAGEX)\n                    # print(outp.min())\n                    loss = criterion(outp, inputs.float())\n                    loss.backward()\n                    optimizer.step()\n                else:\n                    with torch.no_grad():\n                        outp, _ = model(inputs)\n                        outp = outp.view(-1, CHANNELS, IMAGEY, IMAGEX)\n                        loss = criterion(outp, inputs.float())\n                \n                loss_accum.append(loss.item())\n\n            phase_loss = np.mean(loss_accum)\n            losses[phase].append(phase_loss)\n            progress.set_description('loss: {:.4f}'.format(phase_loss))\n            \n            #clear_output(wait=True)\n            #show_dermoscopic_imgs(X_val, Y_hat, threshold=0.1)\n            #plt.title('%d \/ %d - loss: %f' % (epoch+1, epochs, avg_loss))\n            #plt.show()\n    return losses","099c0da1":"EPOCH = 100\nlosses = train_model(autoencoder, dataloader, criterion, optimizer, epochs=EPOCH)","515b9ba8":"def show_metrics(metrics: Dict[str, List[float]]) -> None:\n    plt.figure(figsize=(12, 8))\n    plt.plot(metrics['train'], label='train')\n    plt.plot(metrics['valid'], label='validation')\n    plt.grid()\n    plt.legend()\n    plt.show()\n\n# look at loss function...\nshow_metrics(losses)\nprint('min train loss {}, min valid loss {}'.format(\n    min(losses['train']), min(losses['valid'])))","79b5c6d6":"def get_reconstruction(model, batch):\n    model.eval()\n    with torch.no_grad():\n        reconstruction, _ = model(batch.to(DEVICE))\n    reconstruction = reconstruction.view(-1, CHANNELS, IMAGEY, IMAGEX)\n    reconstruction = reconstruction.cpu().detach()\n    return reconstruction","df9b0094":"batch = next(iter(dataloader['valid']))\nreconstruction = get_reconstruction(autoencoder, batch)\npairs = torch.cat((batch[:8], reconstruction[:8]))\nshow_faces(pairs, size=(2, 8))","e911af8f":"DIM_CODES = [64, 128, 256, 384, 512]\nEPOCH = 100\nfor dim_code in DIM_CODES:\n    print(f'HIDDEN DIMENSION = {dim_code}')\n    autoencoder = AutoencoderFC(dim_code=dim_code).to(DEVICE)\n    criterion = nn.MSELoss()\n    optimizer = torch.optim.Adam(autoencoder.parameters())\n    losses = train_model(autoencoder, dataloader, criterion, optimizer, epochs=EPOCH)\n    print('Dimension: {}, min train loss {}, min valid loss {}'.format(\n        dim_code, min(losses['train']), min(losses['valid'])))\n    show_metrics(losses)\n    reconstruction = get_reconstruction(autoencoder, batch)\n    pairs = torch.cat((pairs, reconstruction[:8]))\n\nshow_faces(pairs, size=(len(DIM_CODES) + 2, 8))              ","e61f50b5":"DROPOUT_RATE = 0.5\n\nclass AutoencoderConv(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.encoder = nn.Sequential(\n            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=(3, 3), padding=1),\n            nn.ReLU(),\n#            nn.BatchNorm2d(32),\n            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(3, 3), stride=2, padding=1),\n            \n            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(3, 3), padding=1),\n            nn.ReLU(),\n#            nn.BatchNorm2d(64),\n            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=(3, 3), stride=2, padding=1),\n\n            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(3, 3), padding=1),\n            nn.ReLU(),\n#            nn.BatchNorm2d(128),\n            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(3, 3), stride=2, padding=1),\n\n            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=(3, 3), padding=1),\n            nn.ReLU(),\n #           nn.BatchNorm2d(128),\n            nn.Dropout(DROPOUT_RATE),\n            nn.Conv2d(in_channels=256, out_channels=128, kernel_size=(3, 3), stride=2, padding=1),\n\n            nn.Conv2d(in_channels=128, out_channels=16, kernel_size=(3, 3), padding=1),\n            nn.ReLU(),\n            \n            nn.Flatten(),\n        )\n        # 16 x 4 x 4\n        self.decoder = nn.Sequential(\n            nn.Unflatten(-1, (16, 4, 4)),\n            \n            nn.Conv2d(in_channels=16, out_channels=128, kernel_size=(3, 3), padding=1),\n            nn.ReLU(),\n#            nn.ConvTranspose2d(in_channels=128, out_channels=128, kernel_size=(2, 2), stride=2),\n            nn.Upsample(scale_factor=2),\n            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(3, 3), padding=1),\n            nn.ReLU(),\n#            nn.ConvTranspose2d(in_channels=128, out_channels=128, kernel_size=(2, 2), stride=2),\n            nn.Upsample(scale_factor=2),\n            nn.Conv2d(in_channels=128, out_channels=64, kernel_size=(3, 3), padding=1),\n            nn.ReLU(),\n#            nn.ConvTranspose2d(in_channels=64, out_channels=64, kernel_size=(2, 2), stride=2),\n            nn.Upsample(scale_factor=2),\n            nn.Conv2d(in_channels=64, out_channels=32, kernel_size=(3, 3), padding=1),\n            nn.ReLU(),\n#            nn.ConvTranspose2d(in_channels=32, out_channels=32, kernel_size=(2, 2), stride=2),\n            nn.Upsample(scale_factor=2),\n            nn.Conv2d(in_channels=32, out_channels=3, kernel_size=(3, 3), padding=1),\n            nn.Sigmoid(),\n\n            \n            #nn.ConvTranspose2d(dim_code, 32, kernel_size=7, padding=3),\n            #nn.ReLU(),\n            #nn.Dropout(0.2),\n            #nn.ConvTranspose2d(32, 32, kernel_size=7, padding=3),\n            #nn.ReLU(),\n            #nn.ConvTranspose2d(32, 3, kernel_size=7, padding=3)\n        )\n        \n    def forward(self, sample):\n        latent = self.encoder(sample)\n        reconstructed = self.decoder(latent)\n        return reconstructed, latent\n    \n    def latent(self, sample):\n        latent = self.encoder(sample)\n        return latent\n    \n    def sample(self, z: torch.Tensor) -> torch.Tensor:\n        generated = self.decoder(z)\n        return generated","9987a2fa":"CHANNELS = 3\nautoencoder_conv = AutoencoderConv().to(DEVICE)\nsummary(autoencoder_conv, input_size=(CHANNELS, IMAGEY, IMAGEX))","067fbfbc":"EPOCH = 100\n\nbatch = next(iter(dataloader['valid']))\npairs = batch[:8]\n\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(autoencoder_conv.parameters())\nlosses = train_model(\n    autoencoder_conv, dataloader, criterion, optimizer, epochs=EPOCH\n)","30793c99":"print('Dimension: {}, min train loss {}, min valid loss {}'.format(\n    '8 x 4 x 4', min(losses['train']), min(losses['valid'])))\nshow_metrics(losses)\nreconstruction = get_reconstruction(autoencoder_conv, batch)\npairs = torch.cat((pairs, reconstruction[:8]))\nshow_faces(pairs, size=(2, 8))","222637b4":"latents = torch.Tensor()\nfor inputs in tqdm(dataloader['train']):\n    inputs = inputs.to(DEVICE)\n    with torch.no_grad():\n        latent = autoencoder_conv.latent(inputs).cpu()\n        latents = torch.cat((latents, latent))\nlatents.shape","807521e2":"lat_mean = latents.mean(dim=0)\nlat_std = latents.std(dim=0)\nlat_mean.shape, lat_std.shape","f28d3fc1":"def get_samples(model, batch):\n    model.eval()\n    with torch.no_grad():\n        sample = model.sample(batch.to(DEVICE))\n    sample = sample.view(-1, CHANNELS, IMAGEY, IMAGEX)\n    sample = sample.cpu().detach()\n    return sample\n\nz = torch.FloatTensor(np.random.randn(25, 256))\nz = z * lat_std\/2 + lat_mean\nsamples = get_samples(autoencoder_conv, z)\nshow_faces(samples, size=(5, 5))","ed949b56":"smile = [171, 308, 1433, 1984, 2593, 2816, 3722, 3843, 4623, 4810, 5046, 5071, 5448, 5497, 5619, 6220, 6468, 6842]\nnon_smile = [170, 307, 1432, 1983, 2594, 2815, 3721, 3844, 4622, 4809, 5047, 5072, 5447, 5496, 5620, 6221, 6467, 6843]\ninput_smile = torch.Tensor(photos_all[smile])\ninput_non_smile = torch.Tensor(photos_all[non_smile])\nprint('Smile')\nshow_faces(input_smile, size=(2, 9))\nprint('Non smile')\nshow_faces(input_non_smile, size=(2, 9))","935dec17":"input_smile = input_smile.to(DEVICE)\ninput_non_smile = input_non_smile.to(DEVICE)\nautoencoder_conv.eval()\nwith torch.no_grad():\n    latent_smile = autoencoder_conv.latent(input_smile).cpu()\n    latent_non_smile = autoencoder_conv.latent(input_non_smile).cpu()\nsmile = latent_smile.mean(dim=0) - latent_non_smile.mean(dim=0)\nsmile.shape","48a30c54":"sad_faces = [0, 2, 12, 16, 21, 2112, 2120, 2116]\nsad_faces = torch.Tensor(photos_all[sad_faces])\nsad_faces = sad_faces.to(DEVICE)\nautoencoder_conv.eval()\nwith torch.no_grad():\n    latent_sad = autoencoder_conv.latent(sad_faces).cpu()\n    latent_sad_to_smile = (latent_sad + smile).to(DEVICE)\n    smile_faces = autoencoder_conv.sample(latent_sad_to_smile)\npairs = torch.cat((sad_faces.cpu(), smile_faces.cpu()))\nshow_faces(pairs, size=(2, 8))","769786d2":"FEATURES = 16\nCHANNELS = 3\n\nclass LinearVAE(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.encoder = nn.Sequential(\n            nn.Linear(in_features=(CHANNELS * IMAGEY * IMAGEX), out_features=512),\n            nn.ReLU(),\n            nn.Linear(in_features=512, out_features=(FEATURES * 2)),\n        )\n        self.decoder = nn.Sequential(\n            nn.Linear(in_features=FEATURES, out_features=512),\n            nn.ReLU(),\n            nn.Linear(in_features=512, out_features=(CHANNELS * IMAGEY * IMAGEX)),\n        )\n        #self.unflatten = nn.Unflatten(-1, (CHANNELS, IMAGEY, IMAGEX))\n        \n    def get_distribution_params(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        x = self.flatten(x).float()\n        x = self.encoder(x).view(-1, 2, FEATURES)\n        mu = x[:, 0, :]\n        log_var = x[:, 1, :]\n        return mu, log_var\n\n    def reparameterize(self, mu: torch.Tensor, log_var: torch.Tensor) -> torch.Tensor:\n        std = torch.exp(0.5 * log_var)\n        eps = torch.randn_like(std)\n        sample = mu + (eps * std)\n        return sample\n\n    def sample(self, z: torch.Tensor) -> torch.Tensor:\n        generated = self.decoder(z)\n        generated = torch.sigmoid(generated)\n        generated = generated.view(-1, CHANNELS, IMAGEY, IMAGEX)\n        return generated\n    \n    def forward(\n        self, x: torch.Tensor\n        ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n\n        mu, log_var = self.get_distribution_params(x)\n        z = self.reparameterize(mu, log_var)\n        reconstruction = self.sample(z)\n        #x = self.decoder(z)\n        #x = torch.sigmoid(x)\n        #reconstruction = x.view(-1, CHANNELS, IMAGEY, IMAGEX)\n        return reconstruction, mu, log_var\n\n    def get_latent_vector(self, x: torch.Tensor):\n        mu, log_var = self.get_distribution_params(x)\n        z = self.reparameterize(mu, log_var)\n        return z","be6e24e7":"class KLDivergenceLoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, mu: torch.Tensor, log_var: torch.Tensor):\n        return -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())","cdd7c0af":"class VAELoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.kl_divergence = KLDivergenceLoss()\n        self.log_likelihood = nn.BCELoss(reduction='sum')\n\n    def forward(self,\n        mu: torch.Tensor,\n        log_var: torch.Tensor,\n        outputs: torch.Tensor,\n        labels: torch.Tensor\n    ) -> torch.Tensor:\n        #print(self.kl_divergence(mu, log_var), self.log_likelihood(outputs, labels))\n        return self.kl_divergence(mu, log_var) + self.log_likelihood(outputs, labels)","4c584559":"autoencoder_vae = LinearVAE().to(DEVICE)\ncriterion = VAELoss()\noptimizer = torch.optim.Adam(autoencoder_vae.parameters())","03e49949":"def train_model(\n    model: nn.Module,\n    dataloader: Dict[str, DataLoader],\n    criterion: nn.Module,\n    optimizer: nn.Module,\n    epochs: int = 25\n) -> Dict[str, List[float]]:\n    \"\"\"Training the model. Returns dictionary of train and validation losses.\n    Args:\n        model (torch.nn.Module): Neural network\n        dataloader (Dict[str, DataLoader]):\n            Dictionary with 'train' and 'valid' dataloaders\n        criterion (torch.nn.Module): Cost function\n        optimizer (torch.nn.Module): Optimization algorithm\n        epochs (int): Number of training iterations. Default: 50\n    \"\"\"\n\n    losses = {'train': [], 'valid': []}\n    progress = tqdm(range(epochs), desc='Epoch')\n    \n    for epoch in progress:\n        for phase in ['train', 'valid']:\n            loss_accum = []\n            model.train(mode=(phase == 'train'))\n            \n            #for inputs in tqdm(dataloader[phase], desc=f'Phase {phase}'):\n            for inputs in dataloader[phase]:\n                inputs = inputs.to(DEVICE)\n                \n                if phase == 'train':\n                    optimizer.zero_grad()\n                    outp, mu, log_var = model(inputs)\n                    #outp = outp.view(-1, IMAGEX, IMAGEY, CHANNELS)\n                    loss = criterion(mu, log_var, outp, inputs.float())\n                    loss.backward()\n                    optimizer.step()\n                else:\n                    with torch.no_grad():\n                        outp, mu, log_var = model(inputs)\n                        #outp = outp.view(-1, IMAGEX, IMAGEY, CHANNELS)\n                        loss = criterion(mu, log_var, outp, inputs.float())\n                \n                loss_accum.append(loss.item())\n\n#                 fig,ax = plt.subplots(5,5)\n#                 for ax, img in zip(ax.flatten(), outp.cpu().detach().numpy()):\n#                     ax.imshow(img)\n#                 plt.show()\n\n            phase_loss = np.mean(loss_accum)\n            losses[phase].append(phase_loss)\n            progress.set_description('loss: {:.4f}'.format(phase_loss))\n\n    return losses","a09afea3":"losses = train_model(autoencoder_vae, dataloader, criterion, optimizer, epochs=50)","b5076ac7":"show_metrics(losses)","d1093738":"batch = next(iter(dataloader['valid']))\nautoencoder_vae.eval()\nwith torch.no_grad():\n    reconstruction, mu, log_var = autoencoder_vae(batch.to(DEVICE))\npairs = torch.cat((batch[:8].cpu(), reconstruction[:8].cpu()))\nshow_faces(pairs, size=(2, 8))","ce500c7b":"z = np.array([np.random.normal(0, 1, 16) for i in range(16)])\noutput = autoencoder_vae.sample(torch.FloatTensor(z).to(DEVICE))\nshow_faces(output.cpu().detach(), size=(2, 8))","6bc399f6":"gt_0 = batch[0][None,:,:,:].to(DEVICE)\ngt_1 = batch[2][None,:,:,:].to(DEVICE)\nfirst_latent_vector = autoencoder_vae.get_latent_vector(gt_0)\nsecond_latent_vector = autoencoder_vae.get_latent_vector(gt_1)","180f3830":"plt.imshow(autoencoder_vae.sample(first_latent_vector)[0].cpu().detach().numpy().transpose(1, 2, 0))","f2b8b748":"plt.imshow(autoencoder_vae.sample(second_latent_vector)[0].cpu().detach().numpy().transpose(1, 2, 0))","839e268d":"plt.figure(figsize=(18, 5))\nfor i, alpha in enumerate(np.linspace(0., 1., 7)):\n    plt.subplot(1, 7, i + 1)\n    latent = (1 - alpha) * first_latent_vector + alpha * second_latent_vector\n    img = autoencoder_vae.sample(latent)[0].cpu().detach().numpy().transpose(1, 2, 0)\n    plt.axis('off')\n    plt.imshow(img)","ea5f3e08":"## Time to make fun!","478f733b":"Show results for different hidden dimensions (original + hidden_dim=32, 64, 128, 256, 384, 512)","fd7ad4d8":"## Loss\n\n$$\\mathcal{L} = -D_{KL}(q_{\\phi}(z|x)||p(z)) + \\log p_{\\theta}(x|z)$$\n$$D_{KL} = -\\frac{1}{2}\\sum_{i=1}^{dimZ}(1+log(\\sigma_i^2)-\\mu_i^2-\\sigma_i^2)$$","3f2cbec4":"# 1. Vanilla Autoencoder\n\n## 1.1. Data preparation","ae519ac9":"# 2. Variational Autoencoder","a3a1a2fa":"## 1.4. Sampling","023a913d":"## 1.3. Train","59f3bd70":"## Sampling","445a991f":"## Latent vectors\n\n$$\\alpha l_1 + (1 - \\alpha)l_2$$\n$$where l_1, l_2 \u2013 latent vectors, \\alpha \\in [0, 1]$$","a423d7e8":"## 1.2. Model FC","51425ae6":"## Model (Conv)"}}