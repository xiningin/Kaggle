{"cell_type":{"ac4d4c50":"code","73664626":"code","23f8c207":"code","f42ac0af":"code","403fff72":"code","4fefee2b":"code","7af237ad":"code","27462f07":"code","50598b70":"code","30219655":"code","cb48b4c0":"code","b07d895d":"code","0a9a1280":"code","52360338":"code","2ef165ca":"code","c4d122ab":"code","bfa606f6":"code","476943eb":"markdown","244a4cf8":"markdown","8ec0d4f6":"markdown","2d098a3d":"markdown","c9e4ebb1":"markdown","ca0d7310":"markdown","8f3e11a0":"markdown","b0eb8204":"markdown","b8bd3171":"markdown","47041c5c":"markdown","aa21ecb9":"markdown","dd590c7a":"markdown","51211b6b":"markdown","ebe7ed8c":"markdown","578daf32":"markdown","1881e0e7":"markdown","6dcbf8e2":"markdown","fb40110a":"markdown"},"source":{"ac4d4c50":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd# data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","73664626":"dataset = pd.read_csv('..\/input\/glass\/glass.csv')","23f8c207":"dataset.info()","f42ac0af":"dataset.head()","403fff72":"dataset.isnull().sum()","4fefee2b":"dataset['Type'].value_counts()","7af237ad":"x = dataset.iloc[:,:-1].values\ny = dataset.iloc[:,-1].values\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.2,random_state = 177013)","27462f07":"for i in range(x_train.shape[0]):\n    print('x_train {0} == y train {1}'.format(x_train[i][0],y_train[i]))","50598b70":"from sklearn.tree import DecisionTreeClassifier\nDT = DecisionTreeClassifier(criterion = 'entropy',splitter = 'best')\nDT.fit(x_train,y_train)","30219655":"from sklearn.ensemble import RandomForestClassifier\nRT = RandomForestClassifier(n_estimators = 500,random_state = 177013)\nRT.fit(x_train,y_train)","cb48b4c0":"from sklearn.svm import SVC\nSV = SVC(kernel = 'rbf')\nSV.fit(x_train,y_train)","b07d895d":"from xgboost import XGBClassifier\nxboost = XGBClassifier(n_estimators = 700, learning_rate = 0.1)\nxboost.fit(x_train,y_train)","0a9a1280":"from sklearn.metrics import confusion_matrix,accuracy_score","52360338":"y_pred_X =xboost.predict(x_test)\nprint(y_pred_X)\ncm_X = confusion_matrix(y_test,y_pred_X)\nprint('Accuracy == {0}'.format(accuracy_score(y_test,y_pred_X)))\nprint(cm_X)","2ef165ca":"y_pred_S = SV.predict(x_test)\nprint(y_pred_S)\ncm_s = confusion_matrix(y_test, y_pred_S)\nprint('Accuracy == {0}'.format(accuracy_score(y_test,y_pred_S)))\nprint(cm_s)","c4d122ab":"y_pred_R = RT.predict(x_test)\nprint(y_pred_R)\ncm_r = confusion_matrix(y_test,y_pred_R)\nprint(\"accuracy == {0}\".format(accuracy_score(y_test,y_pred_R)))\nprint(cm_r)","bfa606f6":"y_pred = DT.predict(x_test)\nprint(y_pred)\ncm = confusion_matrix(y_test,y_pred)\nprint(\"accuracy == {0}\".format(accuracy_score(y_test,y_pred)))\nprint(cm)","476943eb":"#### not compatible for given problem.but WTH, lets do it","244a4cf8":"##### as described in dataset description no 4th type of glass","8ec0d4f6":"##### ding ,ding ,ding! we have a winner!","2d098a3d":"### testing accuracy for each model ","c9e4ebb1":"### decision tree classifier","ca0d7310":"### XG-BOOST","8f3e11a0":"### DATA PREPARATION","b0eb8204":"### assigning X and Y","b8bd3171":"#### the \"gini\" criterion gives your 60% accuracy","47041c5c":"#### checking for null values","aa21ecb9":"#### checking for all unique values in labels","dd590c7a":"### INPUTS","51211b6b":"### random forest classifier","ebe7ed8c":"##### this is kinda expected of support vector as it relies on outliers","578daf32":"#### optimal n_estimators is around 400-500","1881e0e7":"### Model selection for classification","6dcbf8e2":"#### had high hopes, maybe tweaking it more can squeeze more accuracy?","fb40110a":"### Support vector classifier"}}