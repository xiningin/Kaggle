{"cell_type":{"35330187":"code","ac42bcc2":"code","81eada41":"code","17f6aca4":"code","62758040":"code","b298b1ee":"code","b8f76ab7":"code","80bed00d":"code","f4238d9b":"code","b0c91437":"code","e96b7be8":"markdown","6abd3f2b":"markdown","32272aba":"markdown","ca448a31":"markdown","33ff405b":"markdown","e520bbb5":"markdown","48e3a085":"markdown","07e34a32":"markdown","902d0ea4":"markdown","f22c8b7a":"markdown","839495df":"markdown","cce8517d":"markdown","af2ed824":"markdown","8936a573":"markdown","e479ca82":"markdown","291b5059":"markdown"},"source":{"35330187":"#This is a usual set of Visualizations we can use while working with Text data.\n#The idea was to build a repository for future correspondence\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud,STOPWORDS\n","ac42bcc2":"import nltk","81eada41":"# import the dataset\n\nfrom nltk.corpus import inaugural\n# extract the datataset in raw format, you can also extract it in other formats as well\ntext = inaugural.raw()\nwordcloud = WordCloud(max_font_size=60).generate(text)\nplt.figure(figsize=(16,12))\n# plot wordcloud in matplotlib\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","17f6aca4":"from  nltk.book import text4 as inaugural_speeches\nplt.figure(figsize=(16,5))\ntopics = ['citizens', 'democracy', 'freedom', 'duties', 'America','principle','people', 'Government']\ninaugural_speeches.dispersion_plot(topics)","62758040":"from nltk.corpus import brown\nstop_words = set(STOPWORDS)\ntopics = ['government', 'news', 'religion','adventure','hobbies']\nfor topic in topics:\n    # filter out stopwords and punctuation mark and only create array of words\n    words = [word for word in brown.words(categories=topic)\n            if word.lower() not in stop_words and word.isalpha() ]\n    freqdist = nltk.FreqDist(words)\n    # print 5 most frequent words\n    print(topic,'more :', ' , '.join([ word.lower() for word, count in freqdist.most_common(5)]))\n    # print 5 least frequent words\n    print(topic,'less :', ' , '.join([ word.lower() for word, count in freqdist.most_common()[-5:]]))\n","b298b1ee":"# get all words for government corpus\ncorpus_genre = 'government'\nwords = [word for word in brown.words(categories=corpus_genre) if word.lower() not in stop_words and word.isalpha() ]\nfreqdist = nltk.FreqDist(words)\nplt.figure(figsize=(16,5))\nfreqdist.plot(50)","b8f76ab7":"def lexical_diversity(text):\n    return round(len(set(text)) \/ len(text),2) #Measure of uniqueness\n\ndef get_brown_corpus_words(category, include_stop_words=False):\n    '''helper method to get word array for a particular category\n     of brown corpus which may\/may not include the stopwords that can be toggled\n     with the include_stop_words flag in the function parameter'''\n    if include_stop_words:\n        words = [word.lower() for word in brown.words(categories=category) if word.isalpha() ]\n    else:\n        words = [word.lower() for word in brown.words(categories=category)\n                 if word.lower() not in stop_words and word.isalpha() ]\n    return words\n\n# calculate and print lexical diversity for each genre of the brown corpus\nfor genre in brown.categories():\n    lex_div_with_stop = lexical_diversity(get_brown_corpus_words(genre, True))\n    lex_div = lexical_diversity(get_brown_corpus_words(genre, False))\n    print(genre ,lex_div , lex_div_with_stop)","80bed00d":"#Function to sort the words of a given corpus and category lexicographically\ndef Lexo_sort(corpus,category):\n    words1 = sorted([wrd for wrd in list(set(corpus.words(categories=category))) if wrd.isalpha()])\n    return (words1) ","f4238d9b":"cfd = nltk.ConditionalFreqDist(\n           (genre, len(word))\n           for genre in brown.categories()\n           for word in get_brown_corpus_words(genre))\n\nplt.figure(figsize=(16,8))\ncfd.plot()","b0c91437":"from nltk.util import ngrams\nplt.figure(figsize=(16,8))\nfor genre in brown.categories():\n    sol = []\n    for i in range(1,6):\n        count = 0\n        fdist = nltk.FreqDist(ngrams(get_brown_corpus_words(genre), i))\n        sol.append(len([cnt for ng,cnt in fdist.most_common() if cnt > 1]))\n    plt.plot(np.arange(1,6), sol, label=genre)\nplt.legend()\nplt.show()","e96b7be8":"* Lexical diversity lets us what is the percentage of the unique words in the text corpus \n* For example if there are 100 words in the corpus and there are only 20 unique words then lexical diversity is 20\/100=0.2. \n* The formula for calculating lexical diversity is as below :","6abd3f2b":"## 5: Word length distribution plot:","32272aba":"* It will be surprising to see that in least frequent table words belonging to a category of text corpus \n* are more informative compared to the words found in the most frequent table \n* which is the core idea behind TF-IDF algorithm. \n* Most frequent words convey little information about text compared to less frequent words.","ca448a31":"* This plot tries to communicate the frequency of the vocabulary in the text. \n* Frequency distribution plot is word vs the frequency of the word. \n* The frequency of the word can help us understand the topic of the corpus. \n* Different genre of text can have a different set of frequent words, for example, \n* If we have news corpus then sports news may have a different set of frequent words as compared to news related to politics, \n* nltk has FreqDist class that helps to create a frequency distribution of the text corpus. ","33ff405b":"* It would also be interesting to see how to lexical diversity changes in the corpus. \n* To visualise this we can divide a text corpus into small chunks and calculate the diversity for that chuck and plot it. \n* Corpus can be divided by sentence or we can consider each paragraph as chunks but for sake of simplicity \n* We can consider a batch of 1000 words as a chunk and plot its lexical diversity. ","e520bbb5":"# Visual Text Analytics","48e3a085":"## 1: WordCloud","07e34a32":"* Text dataset used is the US presidential inaugural addresses which are part of nltk.corpus package.\n* WordClouds help in detecting the words that occur frequently.","902d0ea4":"## 3: Frequency distribution plot:","f22c8b7a":"## 4: Lexical diversity dispersion plot","839495df":"* This plot is word length on x-axis vs number of words of that length on the y-axis. \n* This plot helps to visualise the composition of different word length in the text corpus. ","cce8517d":"### The code below will plot frequency distribution for a government text, you can change the genre to see distribution for a different genre like try humor, new, etc.","af2ed824":"* This is the plot of a word vs the offset of the word in the text corpus.\n* The y-axis represents the word. \n* Each word has a strip representing entire text in terms of offset, \n* and a mark on the strip indicates the occurrence of the word at that offset, a strip is an x-axis. \n* The positional information can indicate the focus of discussion in the text. \n* So if you observe the plot below the words America, \n* Democracy and freedom occur more often at the end of the speeches and words like duties and some words have somewhat uniform distribution in the middle. ","8936a573":"## 2: Lexical dispersion plot","e479ca82":"## 6: N-gram frequency distribution plot","291b5059":"* n-grams is the continuous sequences of n words that occur very often \n* for example for n=2 we are looking for 2 words that occur very often together \n* like New York, Butter milk, etc. such pair of words are also called bigram, for n=3 its called trigram and so on. \n* N-gram distribution plot tries to visualise distribution n-grams for different value of n, \n* for this example, we consider n from 1 to 5. In the plot, x-axis has the different value of n \n* and y-axis has the number of time n-gram sequence has occurred."}}