{"cell_type":{"fbcc94ff":"code","e657d3d3":"code","c2b9edec":"code","66b1c781":"code","7af8f063":"code","cfe55521":"code","fba31272":"code","9041417f":"code","0e7c7d10":"code","534638b5":"code","cb62f956":"code","268861ac":"code","2f94749e":"code","404de809":"code","2be130e2":"code","3fe88f1e":"code","c20d8ca2":"code","4961e215":"code","ec6d0753":"code","65651aca":"code","ffa6b59c":"code","46de8d1a":"code","8d692f11":"code","cf7a1390":"code","150f4000":"code","15eaf80f":"code","c13a466f":"code","83c8ee0a":"code","7c633c13":"code","0a6fff13":"code","869e0903":"code","1d79f4b8":"code","5f934930":"code","645a584a":"code","9dcbcc5d":"code","a74f3995":"code","ff4430b6":"markdown","7a077458":"markdown","ae414c37":"markdown","0476b905":"markdown","609d2fea":"markdown","6a9d734d":"markdown","5b6cfacd":"markdown","a7127b4d":"markdown","3a7a9183":"markdown","2b01248d":"markdown","4a5fde2d":"markdown","a5d7715b":"markdown","604120b9":"markdown","14cd3c8f":"markdown","773e8f03":"markdown","7da19157":"markdown","7748151c":"markdown","84c47585":"markdown","95ef6928":"markdown","72dcfb1d":"markdown","9847075e":"markdown","7401bed7":"markdown","59fdf526":"markdown","b2844b04":"markdown","8132a810":"markdown","3c16a093":"markdown","1e0a9a70":"markdown","b8bdb09a":"markdown","ab19d2b8":"markdown","4b1ce231":"markdown","e8c8ba03":"markdown","81f3e3d4":"markdown","a3344194":"markdown","b7ee112a":"markdown","11e88a91":"markdown","c1128c49":"markdown","3b1dec81":"markdown","4f372f51":"markdown","58f52f21":"markdown","2e271a10":"markdown","85262897":"markdown","0c182b5c":"markdown","70c68af6":"markdown","85ade739":"markdown","96ce2eb6":"markdown","ca52d095":"markdown"},"source":{"fbcc94ff":"#Import libraries for analysis\n\n#Analysis \nimport pandas as pd\nfrom pandas import Series,DataFrame\nimport numpy as np\nimport sklearn\nimport warnings\nwarnings.filterwarnings('ignore')\n\n#Visualisations\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('whitegrid')\n%matplotlib inline\n\n#Machine Learning tree based ensemble methods\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom xgboost import XGBClassifier\n#Logistic Regression Classifier\nfrom sklearn.linear_model import LogisticRegression\n#Split model data and Accuracy Score \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score","e657d3d3":"#Import the train and test datasets for analysis\ntrain = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\")\n\n#Combine the datasets to scub and explore data. We will split it before modelling\ncombined_df=train.append(test)\nprint(combined_df.info())","c2b9edec":"# Get a the view of the combined data\ncombined_df.head()","66b1c781":"#Get a set of descriptive statistics for the data and understand the data types\n\ncombined_df.describe()","7af8f063":"#Extract out the Title strings using regex\n\ncombined_df['Title']=combined_df['Name'].str.extract(' ([A-Za-z]+)\\.')\n\n#Plot Title against gender\nprint(pd.crosstab(combined_df['Title'], combined_df['Sex']))\n\nprint (combined_df['Title'].count())\n","cfe55521":"combined_df['Title'] = combined_df['Title'].replace({\n     'Mlle':'Miss',\n     'Mme':'Miss',\n     'Ms':'Miss',\n     'Countess':'Other',\n     'Lady':'Other',\n     'Capt': 'Other',\n     'Col': 'Other',  \n     'Don': 'Other',\n     'Dr': 'Other',\n     'Jonkheer': 'Other',\n     'Major': 'Other',\n     'Rev': 'Other',\n     'Sir':'Other',\n     'Dona':'Other'\n})\n   \n#Plot Title against gender\nprint(pd.crosstab(combined_df['Title'], combined_df['Sex']))\nprint (combined_df['Title'].count())\n","fba31272":"print(combined_df['Age'].isnull().sum())\n\nprint(combined_df.groupby(['Title','Sex'])['Age'].median())","9041417f":"combined_df.drop('Name', inplace=True, axis=1)","0e7c7d10":"print(combined_df['Age'].isnull().sum())","534638b5":"#Get dummies for the dataset\ncombined_df['Title_Int'] = combined_df['Title'].map({'Master': 1, 'Miss': 2, 'Mr': 3, 'Mrs': 4,'Other': 5})\n\ncombined_df.head()","cb62f956":"#Keep columns that could be relevant to imputing age and create a new dataframe\n\ntrain_imputer=combined_df[['PassengerId','Survived','Pclass','Age', 'SibSp', 'Parch', 'Title_Int']]\n\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n# define imputer\nimputer = IterativeImputer()\n# fit on the dataset\nimputer.fit(train_imputer)\n# transform the dataset\nXtrans_training = imputer.transform(train_imputer)\n# convert the array back into a dataframe to join imputed Age back to the training dataset\nXtrans_DF=pd.DataFrame(Xtrans_training, columns= ['PassengerId','Survived','Pclass','Age_Imp', 'SibSp', 'Parch', 'Title_Int'])\nXtrans_DF=Xtrans_DF[['PassengerId','Age_Imp']]\nprint(Xtrans_DF.head())\n\n#join up the two dataframes with the imputed Age column\ndf_imputed=combined_df.merge(Xtrans_DF, on='PassengerId',how='inner')\nprint(df_imputed.head())\n#Check that there are no null values within Imputed Age:\nprint ('Number of null values for Age_Imp:', df_imputed['Age_Imp'].isnull().sum())","268861ac":"print(df_imputed.groupby(['Title'])['Age','Age_Imp'].median())\nprint(df_imputed[['Age','Age_Imp']].head(10)) \n#Looks sensible, so drop the Age column\ndf_imputed.drop(['Age'], axis=1, inplace=True)\n#Also, lets drop the imputed Title_Int column, as we will use hot encoding before we model on all categorical data  \ndf_imputed.drop(['Title_Int'], axis=1, inplace=True)\n","2f94749e":"df_imputed.drop(['Cabin','Ticket'], axis=1, inplace=True)\ndf_imputed.head()","404de809":"print(df_imputed['Embarked'].isnull().sum())","2be130e2":"print(pd.crosstab(df_imputed['Embarked'], df_imputed['Pclass']))","3fe88f1e":"df_imputed['Embarked'].fillna('S', inplace = True)","c20d8ca2":"print(df_imputed['Fare'].isnull().sum())","4961e215":"df_imputed['Fare'].fillna(df_imputed['Fare'].mean(), inplace = True)","ec6d0753":"print(df_imputed.info())","65651aca":"#Plot Sex against Survival\n\nsns.barplot(x='Sex',y='Survived',data=df_imputed)","ffa6b59c":"#Plot Passenger Class against Survival\n\nsns.barplot(x='Pclass', y= 'Survived',data=df_imputed)\n","46de8d1a":"#Plot Age against Survival\n\nsns.violinplot(y='Age_Imp', x='Survived', data=df_imputed)","8d692f11":"#Check fare against fare, passenger class and survival\n\nprint(df_imputed.groupby(['Survived','Pclass'])['Fare','Age_Imp'].mean())","cf7a1390":"#Check Title against Survival\n\nprint(pd.crosstab(df_imputed['Title'], df_imputed['Survived']))","150f4000":"#Plot Sex against Survival\n\nsns.barplot(x='Embarked',y='Survived',data=df_imputed)","15eaf80f":"print(df_imputed.groupby(['Embarked','Pclass'])['Pclass'].count())","c13a466f":"#Create a new column, containing the siblings\/parents and add 1 for each row to relate to the passenger\ndf_imputed['Family_Size']=df_imputed['Parch']+df_imputed['SibSp']+1\n#Plot Family Size\nsns.barplot(x='Family_Size',y='Survived',data=df_imputed)","83c8ee0a":"encoded_df = pd.get_dummies(df_imputed)","7c633c13":"encoded_df","0a6fff13":"#Remember PassengerId starts at 1, but iloc Index starts at 0\ntrain = encoded_df.iloc[:891,:]\ntest = encoded_df.iloc[891:,:]","869e0903":"#Splitting out the X,y on the train dataset \nX, y = train.drop(['PassengerId','Survived'],axis=1), train['Survived']\nprint(X.shape, y.shape)\n\n# split into train test sets. Note that this \"test\" is within the train dataset, to evaluate model fit on unseen data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n\n","1d79f4b8":"\n# Fit the Logistic Regression Classifier model\nLR_model = LogisticRegression()\nLR_model.fit(X_train, y_train)\n# make predictions\nLR_yhat = LR_model.predict(X_test)\n# evaluate predictions\nLR_acc = accuracy_score(y_test, LR_yhat)\nprint('Accuracy of Logistic Regression Classifier: %.3f' % LR_acc)","5f934930":"# Fit the Random Forest Classifier model\nRF_model = RandomForestClassifier(random_state=1)\nRF_model.fit(X_train, y_train)\n# make predictions\nRF_yhat = RF_model.predict(X_test)\n# evaluate predictions\nRF_acc = accuracy_score(y_test, RF_yhat)\nprint('Accuracy of Random Forest Classifier: %.3f' % RF_acc)","645a584a":"# Fit the AdaBoost Classifier model AdaBoostClassifier()\nAdaB_model = AdaBoostClassifier()\nAdaB_model.fit(X_train, y_train)\n# make predictions\nAdaB_yhat = AdaB_model.predict(X_test)\n# evaluate predictions\nAdaB_acc = accuracy_score(y_test, AdaB_yhat)\nprint('Accuracy of AdaBoost Classifier: %.3f' % AdaB_acc)","9dcbcc5d":"# Fit the XGBoost Classifier model AdaBoostClassifier()\nXGB_model = XGBClassifier()\nXGB_model.fit(X_train, y_train)\n# make predictions\nXGB_yhat = XGB_model.predict(X_test)\n# evaluate predictions\nXGB_acc = accuracy_score(y_test, XGB_yhat)\nprint('Accuracy of XGBoost Classifier: %.3f' % XGB_acc)","a74f3995":"#Splitting out the X,y on the test dataset \nX_test_unseen = test.drop(['PassengerId','Survived'],axis=1)\n\n#AdaBoost Submission, Model Accuracy = 0.76555\nyhat_unseen_AdaB = AdaB_model.predict(X_test_unseen)\n\n#Create a dataframe and a csv for submission\nAdB_submission = pd.DataFrame({\n        \"PassengerId\": test[\"PassengerId\"],\n        \"Survived\": yhat_unseen_AdaB\n    })\nSubmission_Final_AdB=AdB_submission.astype(int)\nSubmission_Final_AdB.to_csv('titanic.csv', index=False)\n","ff4430b6":"Importing of data analysis and visualisation libraries","7a077458":"Based on the above, Cherbourg has as a comparison has the greatest number of Pclass=1 passengers in relation to the other ports of embarkation, as well as the lowest number of Pclass=3. ","ae414c37":"In general, it seems like the AdaBoost model has the greatest model accuracy, followed by the Logistic Regession model.\nMy expectation would have been XGBoost as having the greatest model accuracy based on the ML community and competition consensus. \n\nI will submit both AdaBoost for competition results here.  ","0476b905":"There are three additional fields that we need to address here which are text, to convert into numeric types for modelling or drop if unnecessary (Identified as objects in the Dtype):\n\n1. Name\n2. Sex\n3. Ticket\n\nThere is missing data under the fields of: \n\n4. Age \n5. Cabin \n6. Embarked  \n7. Fare\n\nWe will address these all below as it makes sense, so not necessarily in order.\n\n","609d2fea":"We will do a median impute to fill the one null gap for Fare","6a9d734d":"## 1) Import Libraries","5b6cfacd":"We need to split the training and test datasets again before applying our models. The test dataset doesn't have any \"Survived\" values and is part of the submission. We will use this test data once the model has been trained.","a7127b4d":"Before we move onto modelling the data, we will convert any categorical variables to dummy variables using one hot encoding. ","3a7a9183":"It would appear that there's a relationship between survival, passenger class and median fare-higher the class, higher the fare (as expected) and greater the survival. In addition, average age trends lower with survival (1) across all classes, in line with the violinplot above. ","2b01248d":"### Name and Age","4a5fde2d":"What do survival rates look like across the various fields","a5d7715b":"This step is done to provide a sufficiently clean dataset for data exploration, where we will determine which features to model.","604120b9":"It would appear that titles with \"Mr\" have a low survival rate. ","14cd3c8f":"The Titanic data resides on Kaggle, and is sourced through the API in CSV format.","773e8f03":"Lets do a sanity check between the original age and imputed age before deleting the Age column","7da19157":"It appears that Females have a higher survival rate than Males\n","7748151c":"#Apply the model on the test dataset and submit to csv","84c47585":"Lets check if there's any pattern with survival and embarked","95ef6928":"## 6) Interpret Data","72dcfb1d":"This script uses the [*mice*](http:\/\/www.kaggle.com\/mrisdal\/exploring-survival-on-the-titanic) package, as done in R: https:\/\/ for multiple imputation using chained equations. \n\nSklearn has a similar (experimental) package to *mice* that can be found [here](http:\/\/scikit-learn.org\/stable\/modules\/impute.html). I think I'm complicating it more than I should going this route as a start, but I'm going to give it a go.  \n\nI found [this](http:\/\/machinelearningmastery.com\/iterative-imputation-for-missing-values-in-machine-learning\/) tutorial useful to get a high level understanding of more advanced imputation methods\n","9847075e":"To impute however, we need to convert Titles into integers if we are going to use this in our imputation method. We could use some sort of encoding method here as well, but I will map integers as a new column instead. \n\nIn addition, I will drop the Name column as we have extracted the Titles from this already.","7401bed7":"For Fare, let's check the number of null values:","59fdf526":"### Ticket and Cabin","b2844b04":"## 5) Model Data","8132a810":"The data dictionary can be found in the original post, at https:\/\/www.kaggle.com\/c\/titanic\/data","3c16a093":"### Embarked and Fare","1e0a9a70":"It looks like the dataset is sufficiently complete for exploration in the following section. ","b8bdb09a":"Lastly, let's deal with the parch and sibsp values, to understand family size and effects of survival","ab19d2b8":"# Titanic Exploratory Data Analysis\n\nThis is my first attempt at a ML script here at Kaggle, and the goal is primarily a learning one. \n\nI've sourced some of the thinking from other Kaggle scripts e.g. https:\/\/www.kaggle.com\/mrisdal\/exploring-survival-on-the-titanic done in in R, so thanks for this starting point.  \n\nI also found [this](https:\/\/towardsdatascience.com\/5-steps-of-a-data-science-project-lifecycle-26c50372b492) post around the OSEMN framework to be helpful in structuring my thinking. \n\nThe steps I am going to take here are: \n\n1. Import Libraries\n2. Obtain Data (O)\n3. Scrub Data (S)\n4. Explore Data (E)\n5. Model Data (M)\n6. Interpret Data (N)\n\n\nBefore we get started,the question we want to answer here is: Which sort of people were more likely to survive in the sinking of the Titanic?\n\nAs such, this is a classification ML problem.\n\n","4b1ce231":"Check how many entries are null on the Embarked field","e8c8ba03":"## 4) Explore Data","81f3e3d4":"Lets take out the titles from the names, and clean these up. This could be useful to impute the missing values in Age as well, so we will work with Age at the same time.","a3344194":"The train data will inself be split into train and test data (on the train data) to 1) first fit the model and 2) second to test the model accuracy on a split of the data (where test_size = 33%) where survival is known. Otherwise we're overfitting the model if we test and train on the exact same data. ","b7ee112a":"Based on the plot, it appears that there is a greater spread of survival at lower ages ","11e88a91":"For simplicity, we will assign all low frequency Titles to \"Other\" e.g Dr. Any that are identifiable will be replaced accordingly e.g.Ms -> Miss","c1128c49":"It appears that Passenger Class has a relationship with survival rates, with higher classes having a greater survival rate","3b1dec81":"## 2) Obtain Data","4f372f51":"**1. Age**: We need to replace the nulls that exist under Age with some sort of imputation method.","58f52f21":"## 3) Scrub Data","2e271a10":"Based on the large numbers of missing entries for the Cabin, it seems likely that the best route would be to drop the column. \n\nThere could be some meaning to the Ticket numbers e.g. the text prefix to the numbers could be extracted, but in this exercise it's easier to drop this column as well. ","85262897":"It appears that being single (1) or part of a large family unit (5+) is less likely to result in survival. Families between 2 and 4 members appear to have the greatest chance of survival. We could check the breakdown for e.g. mother\/child and build features off that as well, but maybe for another time.  ","0c182b5c":"There are too many values to just drop these rows, and will need to impute for missing values. One way could be to check against Name title (e.g. Mrs being older than Miss) and impute using the median age within these Name title categories. ","70c68af6":"It seems like port of embarkation of C (Cherbourg) has a higher survival rate, which on it's own doesn't make sense. It may be that Cherbourg has more passengers in First Class-let's check.","85ade739":"Let's deal with missing values in Age, where we will use the Titles to impute a value for each age. The assumption here is that a title like \"Master\" will have a lower age than \"Mr\", which appears to hold true: ","96ce2eb6":"I'll use the kaggle tutorial [here](https:\/\/www.kaggle.com\/dansbecker\/using-categorical-data-with-one-hot-encoding) as a guide to work through this as well as this [link](https:\/\/machinelearningmastery.com\/train-test-split-for-evaluating-machine-learning-algorithms\/) to apply my machine learning model.\n\nIn general, I'm going to be using a set of tree based ensemble methods for my models, some resources that I used can be found [here](https:\/\/towardsdatascience.com\/https-medium-com-vishalmorde-xgboost-algorithm-long-she-may-rein-edd9f99be63d) and [here](https:\/\/machinelearningmastery.com\/gentle-introduction-xgboost-applied-machine-learning\/), but will also try a Logistic Regression model","ca52d095":"For simplicity, as there are few missing entries, lets fill it in with the majority Embarked field"}}