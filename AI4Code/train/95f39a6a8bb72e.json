{"cell_type":{"1a0e407c":"code","a0a58b27":"code","3563d754":"code","1f68b14d":"code","212fa5fd":"code","fd918af2":"code","cb89f589":"code","95294d1b":"code","24c8b41d":"code","577b4f04":"code","ae7a65c3":"code","d4467873":"code","4b3794b2":"code","afd8a0e6":"code","af80f13a":"code","554edd0b":"code","e3a03fc9":"code","f300d0c1":"code","7df00b32":"code","aba25cf0":"code","fa6e8b21":"code","d077a398":"code","264ff8e6":"code","515d5289":"code","203f66e8":"code","79b18e4f":"code","d08e928c":"code","77ef4a8e":"code","46560306":"code","6daf9e75":"code","290a2f74":"code","335cbd70":"code","7985dda9":"code","1cf92af2":"code","4cfa3a4c":"code","2087931b":"code","34dc4532":"code","52003a8e":"code","80210a30":"code","3073253e":"code","405b8943":"code","c6eb8ec5":"code","591d0a1f":"code","6b856cd5":"code","2846088c":"code","95dd847d":"code","95d6f995":"code","115011e1":"code","1bc5da88":"code","2e3e4bac":"code","0492efcc":"code","7f9444c5":"code","529cb4c0":"code","b0ba819c":"code","9876fc04":"code","f7000e83":"code","6c109830":"code","8509b8f3":"code","cb1bf046":"code","37f252cf":"code","4fc10229":"code","708c7bc0":"code","40dbf822":"code","f6053f96":"code","53bb3b59":"code","8cdef643":"code","03a969c0":"code","8be3c3a5":"code","72104ff6":"code","796ee016":"code","96abac82":"code","d1f2dee5":"code","2fcf10ac":"code","1cc8ec4f":"code","5829ac82":"code","388b949e":"code","3d68cb43":"code","5b64e1e6":"code","4ba85a30":"code","5bc8cea0":"code","12d30b49":"code","0702118c":"code","ba3e1763":"code","290bca24":"code","44b9ef7e":"code","2f9cf501":"code","6ad493ba":"code","034b8cdc":"code","22f4cf5a":"code","94086442":"code","b7bcdb56":"code","41f9320a":"code","bff095f2":"code","076ea01b":"code","ef4a1a04":"code","505fd05c":"code","70c9282c":"code","f69b525d":"code","26395b2c":"markdown","a98c5a30":"markdown","a0597464":"markdown","fe3597dd":"markdown","c5d00a01":"markdown","7d5fd953":"markdown","5b99f355":"markdown","3698f117":"markdown","8f03fc63":"markdown","1a258f6f":"markdown","c95ea077":"markdown","9b7dc9c8":"markdown","b06b7671":"markdown","24a25c4f":"markdown"},"source":{"1a0e407c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a0a58b27":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nfrom PIL import Image\n\nprint('OK')","3563d754":"tw_merkel = pd.read_excel('\/kaggle\/input\/presidency-in-the-middle-of-the-pandemic\/tw_merkel.xlsx',sheet_name='Sheet1')\ntw_merkel.head()","1f68b14d":"tw_merkel.tail()","212fa5fd":"tw_merkel.info()","fd918af2":"tw_merkel.describe()","cb89f589":"#remove cloumn=id\ntw_data = tw_merkel.drop(columns=['id'])\ntw_data","95294d1b":"tw_data[\"tweet\"]","24c8b41d":"#null values visualization on heatmap\nsns.heatmap(tw_data.isnull(), yticklabels = False, cbar = False, cmap=\"Oranges\")","577b4f04":"#fulling null values\ntw_data.fillna(value = 'No Reason')","ae7a65c3":"#making changes permanent\ntw_data.fillna(value = 'No Reason', inplace = True)","d4467873":"tw_data.head()","4b3794b2":"#add a new column for the length\ntw_data['length'] = tw_data['tweet'].apply(len)\ntw_data.head()","afd8a0e6":"#visualing numeric data with frequency\ntw_data['length'].plot(bins=100, kind='hist') ","af80f13a":"#count how many tweets in which language\ntw_data['language'].value_counts()","554edd0b":"#describe for the longest and shortest tweet\ntw_data.describe()","e3a03fc9":"#max tweet \ntw_data[tw_data['length'] == 459]['tweet'].iloc[0]","f300d0c1":"#min tweet\ntw_data[tw_data['length'] == 7]['tweet'].iloc[0]","7df00b32":"positive = tw_data[tw_data['label']==0]\npositive.head()","aba25cf0":"negative = tw_data[tw_data['label']==1]\nnegative.head()","fa6e8b21":"#collecting negative tweets together for wodcloud\nnegative_list = negative['tweet'].tolist()\nnegative_sentences_as_one_string = ''.join(negative_list)\nnegative_sentences_as_one_string","d077a398":"from stop_words import get_stop_words","264ff8e6":"stopwords_eng = get_stop_words('english')\nstopwords_eng","515d5289":"stopwords = get_stop_words('german')\nstopwords","203f66e8":"len(stopwords)","79b18e4f":"from wordcloud import WordCloud, STOPWORDS","d08e928c":"#Uniting german and english stopwords and adding words 'Merkel','Angela','Frau'\nstopwords = get_stop_words('german')\nstopwords = set(stopwords)\nstopwords.update([\"Frau\",\"Angela\",\"Merkel\",\"a\",\"about\",\"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\",\"aren't\",\n\"as\",\"at\",\"be\",\"because\",\"been\",\"before\",\"being\",\"below\",\"between\",\"both\",\"but\",\"by\",\"can't\",\"cannot\",\"could\",\"couldn't\", \"did\",\n\"didn't\",\"do\",\"does\",\"doesn't\",\"doing\",\"don't\",\"down\",\"during\",\"each\",\"few\",\"for\",\"from\",\"further\",\"had\",\"hadn't\",\"has\",\"hasn't\",\n\"have\",\"haven't\",\"having\",\"he\",\"he'd\",\"he'll\",\"he's\",\"her\",\"here\",\"here's\",\"hers\",\"herself\",\"him\",\"himself\",\"his\",\"how\",\"how's\",\n\"i\",\"i'd\",\"i'll\",\"i'm\",\"i've\",\"if\",\"in\",\"into\",\"is\",\"isn't\",\"it\",\"it's\",\"its\",\"itself\",\"let's\",\"me\",\"more\",\"most\",\"mustn't\",\"my\",\n\"myself\",\"no\",\"nor\",\"not\",\"of\",\"off\",\"on\",\"once\",\"only\",\"or\",\"other\",\"ought\",\"our\",\"ours\",\"ourselves\",\"out\",\"over\",\"own\",\"same\",\n\"shan't\",\"she\",\"she'd\",\"she'll\",\"she's\",\"should\",\"shouldn't\",\"so\",\"some\",\"such\",\"than\",\"that\",\"that's\",\"the\",\"their\",\"theirs\",\"them\",\n\"themselves\",\"then\",\"there\",\"there's\",\"these\",\"they\",\"they'd\",\"they'll\",\"they're\",\"they've\",\"this\",\"those\",\"through\",\"to\",\"too\",\"under\",\"until\",\"up\",\"very\",\"was\",\"wasn't\",\"we\",\"we'd\",\"we'll\",\"we're\",\"we've\",\"were\",\"weren't\",\"what\",\"what's\",\"when\",\"when's\",\"where\",\"where's\",\"which\",\"while\",\"who\",\"who's\",\"whom\",\"why\",\"why's\",\"with\",\n\"won't\",\"would\",\"wouldn't\",\"you\",\"you'd\",\"you'll\",\"you're\",\"you've\",\"your\",\"yours\",\"yourself\",\"yourselves\"])","77ef4a8e":"len(stopwords)","46560306":"#wordCloud negative\n\nwc = WordCloud(stopwords=stopwords, width=3000, height=2000).generate(negative_sentences_as_one_string)\nplt.figure(figsize=(15,10))\nplt.imshow(wc)","6daf9e75":"#To see all the words in WordCloud\nWordCloud().process_text(negative_sentences_as_one_string)","290a2f74":"#key words ind WordCloud \nprint(WordCloud().process_text(negative_sentences_as_one_string)['Adolf'])\nprint(WordCloud().process_text(negative_sentences_as_one_string)['Hitler'])\nprint(WordCloud().process_text(negative_sentences_as_one_string)['Nazi'])","335cbd70":"positive_list = positive['tweet'].tolist()\npositive_sentences_as_one_string = ''.join(positive_list)\npositive_sentences_as_one_string","7985dda9":"#for positive WordCloud with image_mask \n#First: chech the values of np.array \nmerkel_mask = np.array(Image.open(\"..\/input\/merkel-png\/merkel.png\"))\nmerkel_mask","1cf92af2":"def transform_mask(val):\n    if val.any() == 0:\n        return 255\n    else:\n        return val.any()  ","4cfa3a4c":"transformed_merkel_mask = np.ndarray((merkel_mask.shape[0],merkel_mask.shape[1]), np.int32)\n\nfor i in range(len(merkel_mask)):\n    transformed_merkel_mask[i] = list(map(transform_mask, merkel_mask[i]))","2087931b":"transformed_merkel_mask","34dc4532":"wc = WordCloud(stopwords=stopwords, width=3000, height=2000, background_color=\"black\", max_words=1000, mask=transformed_merkel_mask,\n               contour_width=3, contour_color='firebrick').generate(positive_sentences_as_one_string)\n\nplt.figure(figsize=(15,10))\nplt.imshow(wc)\n","52003a8e":"# negative tweets in a group by pandas \nlanguage_group_negative = negative.groupby('language')\nlanguage_group_negative","80210a30":"language_group_negative.sum()","3073253e":"#which language has been tweeted longer?\nlanguage_group_negative.mean()","405b8943":"#visualize average messages by language\nnegative.set_index('language', inplace=False)\nlanguage_group_negative.mean().plot(legend=True)","c6eb8ec5":"#negative tweets waer for languages\nfig, ax = plt.subplots(1,1, figsize=(15, 6))\ndata_q1 = negative['language'].value_counts().sort_index()\nax.bar(data_q1.index, data_q1, width=0.55, \n       edgecolor='blue', color='#d4dddd',\n       linewidth=0.7)\n\nfor i in data_q1.index:\n    ax.annotate(f\"{data_q1[i]}\", \n                   xy=(i, data_q1[i] + 100),\n                   va = 'center', ha='center',fontweight='light', fontfamily='serif',\n                   color='#4a4a4a')\n\n\nfor s in ['top', 'left', 'right']:\n    ax.spines[s].set_visible(False)\n\nax.set_ylim(0, 700)    \nax.set_xticklabels(data_q1.index, fontfamily='serif')\nax.set_yticklabels(np.arange(0, 701, 100),fontfamily='serif')\nfig.text(0.1, 0.95, 'Negative Tweets water for Languages', fontsize=15, fontweight='bold', fontfamily='serif')    \nax.grid(axis='y', linestyle='-', alpha=0.4)    \nplt.show()","591d0a1f":"# plot the top 5 languages\nsns.barplot(y=negative['language'].value_counts()[:5].index, x=negative['language'].value_counts()[:5],palette='summer')\nplt.title('Top 5 Language');","6b856cd5":"#positive tweets water for languages\nfig, ax = plt.subplots(1,1, figsize=(15, 6))\ndata_q1 = positive['language'].value_counts().sort_index()\nax.bar(data_q1.index, data_q1, width=0.55, \n       edgecolor='blue', color='#d4dddd',\n       linewidth=0.7)\n\nfor i in data_q1.index:\n    ax.annotate(f\"{data_q1[i]}\", \n                   xy=(i, data_q1[i] + 100),\n                   va = 'center', ha='center',fontweight='light', fontfamily='serif',\n                   color='#4a4a4a')\n\n\nfor s in ['top', 'left', 'right']:\n    ax.spines[s].set_visible(False)\n\nax.set_ylim(0, 700)    \nax.set_xticklabels(data_q1.index, fontfamily='serif')\nax.set_yticklabels(np.arange(0, 701, 100),fontfamily='serif')\nfig.text(0.1, 0.95, 'Positive Tweets water for Languages', fontsize=15, fontweight='bold', fontfamily='serif')    \nax.grid(axis='y', linestyle='-', alpha=0.4)    \nplt.show()","2846088c":"# plot the top 5 languages\nsns.barplot(y=positive['language'].value_counts()[:5].index, x=positive['language'].value_counts()[:5],palette='summer')\nplt.title('Top 5 Language');","95dd847d":"#Issue generel distrubition\nplt.figure(figsize=(12,6))\nsns.countplot(x='issue',data=tw_data)","95d6f995":"issue_negative = tw_data[tw_data['issue']!='No Reason']\nissue_negative","115011e1":"issue_negative.mean()","1bc5da88":"issue_positive = tw_data[tw_data['issue']=='No Reason']\nissue_positive","2e3e4bac":"issue_positive.mean()","0492efcc":"issue_group_negative = negative.groupby('issue')\nissue_group_negative","7f9444c5":"issue_group_negative.sum()","529cb4c0":"issue_group_negative.size()","b0ba819c":"language_order = negative['language'].value_counts()[:6].index\nlanguage_issue = negative[['issue','language']].groupby('language')['issue'].value_counts().unstack().loc[language_order]\nlanguage_issue['sum'] = language_issue.sum(axis=1)\nlanguage_ratio = (language_issue.T \/ language_issue['sum']).T[['conspiracy theory', 'insult', 'political criticism']]","9876fc04":"fig, ax = plt.subplots(1,1,figsize=(12, 6),)\n\nax.barh(language_ratio.index, language_ratio['conspiracy theory'], \n        color='#004c70', alpha=0.7, label='conspiracy theory')\nax.barh(language_ratio.index, language_ratio['insult'], left=language_ratio['conspiracy theory'], \n        color='#990000', alpha=0.7, label='insult')\nax.barh(language_ratio.index, language_ratio['political criticism'], left=language_ratio['conspiracy theory']+language_ratio['insult'], \n        color='#edbc1d', alpha=0.7, label='political criticism')\n\nax.set_xlim(0, 1)\nax.set_xticks([])\nax.set_yticklabels(language_ratio.index, fontfamily='serif', fontsize=11)\n\n# male percentage\nfor i in language_ratio.index:\n    ax.annotate(f\"{language_ratio['conspiracy theory'][i]*100:.3}%\", \n                   xy=(language_ratio['conspiracy theory'][i]\/2, i),\n                   va = 'center', ha='center',fontsize=10, fontweight='light', fontfamily='serif',\n                   color='white')\nfor i in language_ratio.index:\n    ax.annotate(f\"{language_ratio['insult'][i]*100:.3}%\", \n                   xy=(language_ratio['conspiracy theory'][i]+language_ratio['insult'][i]\/2, i),\n                   va = 'center', ha='center',fontsize=10, fontweight='light', fontfamily='serif',\n                   color='white')\nfor i in language_ratio.index:\n    ax.annotate(f\"{language_ratio['political criticism'][i]*100:.3}%\", \n                   xy=(language_ratio['conspiracy theory'][i]+language_ratio['insult'][i]+language_ratio['political criticism'][i]\/2, i),\n                   va = 'center', ha='center',fontsize=10, fontweight='light', fontfamily='serif',\n                   color='black')    \n#+language_ratio['insult'][i]\nfig.text(0.13, 0.95, 'Top-6 Language : Negative Comment Distribution', fontsize=15, fontweight='bold', fontfamily='serif')   \nfig.text(0.131, 0.91, 'Percent Stacked Bar Chart', fontsize=12,fontfamily='serif')   \n\nfor s in ['top', 'left', 'right', 'bottom']:\n    ax.spines[s].set_visible(False)\n    \nax.legend(loc='lower center', ncol=3, bbox_to_anchor=(0.5, -0.06))\nplt.show()","f7000e83":"language_order2 = positive['language'].value_counts()[:6].index\nlanguage_issue2 = positive[['issue','language']].groupby('language')['issue'].value_counts().unstack().loc[language_order2]\nlanguage_issue2['sum'] = language_issue2.sum(axis=1)","6c109830":"language_issue2.style.background_gradient(cmap='Greens')","8509b8f3":"from palettable.colorbrewer.qualitative import Pastel1_7\nplt.figure(figsize=(16,10))\nmy_circle=plt.Circle((0,0), 0.7, color='white')\nplt.rcParams['text.color'] = 'black'\nplt.pie(language_issue2['sum'],labels=['german','english','spanish','turkish','italian','polish'], colors=Pastel1_7.hex_colors)\np=plt.gcf()\np.gca().add_artist(my_circle)\nplt.title('DoNut Plot Of Positive Comments')\nplt.show()","cb1bf046":"# NLP \/ Text specific imports\nimport nltk\nimport spacy\nimport en_core_web_sm\nnlp = en_core_web_sm.load()\n\nnlp = spacy.load('en_core_web_sm',disable=['parser','tagger','ner'])\n\n# NLP - Text Cleaning\nfrom nltk.tokenize import word_tokenize\nfrom nltk import sent_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\n\n# text vectorization\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# regular expression\nimport re\n\n# string operations\nimport string\n\n# ML Models\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom xgboost import XGBClassifier\n\n# cross validation\nfrom sklearn.model_selection import cross_val_score,cross_val_predict,KFold,RepeatedStratifiedKFold\n\n# Model Fine Tuning\nfrom sklearn.model_selection import GridSearchCV\n\n# Pipeline\nfrom sklearn.pipeline import Pipeline\n\n# keras\nfrom keras.utils import to_categorical\n\n# Model evaluation metrics\nfrom sklearn.metrics import classification_report,f1_score,accuracy_score,precision_score,recall_score\n\n\n\nprint('All imported!')","37f252cf":"tweet_punct_removed = []\nfor char in negative_sentences_as_one_string:\n    if char not in string.punctuation:\n        tweet_punct_removed.append(char)\n        \n        \ntweet_punct_removed_join = ''.join(tweet_punct_removed)\ntweet_punct_removed_join","4fc10229":"#See the full tweets randomly\n# Print out the full tweet of a few rows\nimport random\ntweets = tw_data.drop(columns=['label']) \nfor i in range(100):\n    row = random.randint(0,len(tweets))\n    print('Example-{}:'.format(i+1), )\n    print(f'Tweet at Row {row}')\n    print(tweets['tweet'].loc[row])\n    print('\\n')","708c7bc0":"list(tweets)","40dbf822":"#check data balance\ntw_data['label'].value_counts()","f6053f96":"#Remove the urls and weblinks\ndef rem_url(text):\n    '''This function removes the urls from the text. Alternatively use lambda functions'''\n    new_text = re.sub(r'http\\S+','',str(text)) \n    new_text = re.sub(r'@\\S+','',new_text) # get rid of all the name referrals as in @stemgrk\n    return new_text","53bb3b59":"#Split into sentences\ndef sent_text(text):\n    '''This function will return the sentences from the tweets'''\n    text = str(text)\n    sentences = sent_tokenize(text)\n    return sentences","8cdef643":"#Split into words\ndef to_lower(text):\n    ''' Convert the string to lower case'''\n    ## first split on spaces \n    text_split = text.split()\n    word_list = []\n    \n    for word in text_split:        \n        if type(word) == str:\n            word_ = word.lower()\n            word_list.append(word_)\n        new_text = ' '.join(word_list)\n    return new_text     ","03a969c0":"#Filter out Punctuation\ndef split_words(text):\n    '''This function splits the sentences into words and filters out the punctuations'''\n    tokens = word_tokenize(text)\n    # filter out the punctuations\n    words = [word for word in tokens if word.isalpha()]\n    return words","8be3c3a5":"#Filter out Stop Words\ndef stop_word(text):\n    '''This function will filter out the stop words from the word tokens'''\n    stop_words = stopwords\n    words = [word for word in text if not word in stop_words]\n    return words","72104ff6":"#Stem Words\ndef stem_word(text):\n    '''This function performs stemming -- Returns the root form of the word'''\n    porter = PorterStemmer()\n    stemmed = [porter.stem(word) for word in text]\n    return stemmed","796ee016":"#Lemmatization\ndef lemma_word(text):\n    '''This function performs lemmatization'''\n    lemmatizer = WordNetLemmatizer()\n    word_lemma = [lemmatizer.lemmatize(word) for word in text]\n    return word_lemma","96abac82":"test_for_functions = \"I Am 57 years old and my @purpose is #greatAmerica's Idea = ___http:\/\/www.test.com & say just No!!!!!WOWWWWWWWW\"","d1f2dee5":"rem_url(test_for_functions)","2fcf10ac":"to_lower(test_for_functions)","1cc8ec4f":"def join_list(text):\n    '''This function will perform the join operation'''\n    join_text = ' '.join(text)\n    return join_text","5829ac82":"train = tw_data","388b949e":"test = pd.read_excel('..\/input\/presidency-in-the-middle-of-the-pandemic\/tw_merkel_test.xlsx',sheet_name='Sheet1')\ntest.head()","3d68cb43":"test = test.drop(columns=['id'])\ntest","5b64e1e6":"stopwords = get_stop_words('german')","4ba85a30":"clean_test_list =  [to_lower,rem_url,split_words,stop_word,lemma_word,join_list]\nfor function in clean_test_list:\n    train['tweet'] = train['tweet'].apply(function)\n    test['tweet'] = test['tweet'].apply(function)\n    \n# print a sample of cleaned text \nprint(train['tweet'][random.randint(0,len(train))])\nprint(test['tweet'][random.randint(0,len(test))])","5bc8cea0":"X = train['tweet']\ny = train['label']\n\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.20,random_state=42)","12d30b49":"X_train.shape, y_train.shape, X_test.shape, y_test.shape","0702118c":"tfidf = TfidfVectorizer(min_df=2,max_df=0.95,ngram_range=(1,2))","ba3e1763":"X_train = tfidf.fit_transform(X_train)\n#y_train = to_categorical(y_train)","290bca24":"X_test = tfidf.transform(X_test)\n#y_test = to_categorical(y_test)","44b9ef7e":"import warnings\nwarnings.filterwarnings(action='ignore',message='')","2f9cf501":"pip install spacy --upgrade","6ad493ba":" def cross_validate(X=X_train,y=y_train):\n    '''This function will test various models using cross validation and \n       prints out the mean and standard deviation. The best performing model will be \n       used for further fine tuning using Grid search. Due to time constraint, ensemble techniques have been\n       excluded.'''\n    \n    #warnings.filterwarnings(action='ignore',message='')\n    \n    #create a list of models to be cross validated\n    models = []\n    models.append(('Naive Bayes',MultinomialNB()))\n    models.append(('Logistic Regression',LogisticRegression()))\n    #models.append(('Random Forest',RandomForestClassifier()))\n    #models.append(('Gradient Boosting',GradientBoostingClassifier()))\n    #models.append(('XG Boost',XGBClassifier()))\n    models.append(('Linear SVC',LinearSVC()))\n    models.append(('Decision Tree',DecisionTreeClassifier()))\n    \n    results = []\n    names = []\n    scoring = 'f1'\n    \n    # Cross validate all the models in the list and print the mean and std deviation of the f1 score. \n    for name,model in models:\n        kfold = RepeatedStratifiedKFold(random_state=42,n_repeats=10,n_splits=5)\n        cv_results = cross_val_score(model,X,y,cv=kfold,scoring=scoring)\n        results.append(cv_results)\n        names.append(name)\n        print (f'Model:{name},Mean:{cv_results.mean()},Std Dev: {cv_results.std()}') \n        # Cross validate all the models in the list and print the mean and std deviation of the f1 score. \n    for name,model in models:\n        kfold = RepeatedStratifiedKFold(random_state=42,n_repeats=10,n_splits=5)\n        cv_results = cross_val_score(model,X,y,cv=kfold,scoring=scoring)\n        results.append(cv_results)\n        names.append(name)\n        print (f'Model:{name},Mean:{cv_results.mean()},Std Dev: {cv_results.std()}')    ","034b8cdc":"cross_validate(X_train,y_train)","22f4cf5a":"nb_model = MultinomialNB()\nnb_model.fit(X_train,y_train)","94086442":"y_test_predict = nb_model.predict(X_test)","b7bcdb56":"print(classification_report(y_test,y_test_predict))","41f9320a":"text_nb_clf = Pipeline([('tfidf',TfidfVectorizer(max_df=0.95,min_df=2,ngram_range=(1,2))),\n                       ('Naive Bayes',MultinomialNB())])","bff095f2":"text_nb_clf.fit(X,y)","076ea01b":"test_vector = test['tweet']\ny_pred = text_nb_clf.predict(test_vector)","ef4a1a04":"submission =  test.copy()\nsubmission['label'] = y_pred","505fd05c":"submission.to_csv('submission.csv',index=False)","70c9282c":"result = pd.read_csv('submission.csv')\nresult","f69b525d":"result['label'].value_counts()","26395b2c":"## NLP Processing","a98c5a30":"#### Downloading the test data \n","a0597464":"__Dividing train and vaidation data set:__ \n\nIn this step, it is better to split the training dataset into training and validation set and test various models before applying text vectorization. The best model will be chosen to predict the test data set.","fe3597dd":"#### Positive Comments by Language","c5d00a01":"#### Join the list to form the cleaned text","7d5fd953":"__Text Vectorization__","5b99f355":"__Cross Validation__","3698f117":"### Create pipeline for vectorizer and model","8f03fc63":"__Prediction on the validation set__","1a258f6f":"### Comparative model analysis","c95ea077":"#### Negative Comments by Language","9b7dc9c8":"__Prediction on the test dataset__","b06b7671":"###  Analysis of Issue  & Percentage rate creation ","24a25c4f":"### Text Cleaning:\n    -Remove the urls and weblinks\n    -Split into sentences\n    -Split into words\n    -Filter out Punctuation\n    -Filter out Stop Words\n    -Stem Words\n    -Lemmatization"}}