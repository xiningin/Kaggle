{"cell_type":{"df54ce07":"code","4e2d613c":"code","c3da3c11":"code","ea3fd9a8":"code","52177a32":"code","c4757b0c":"code","f151457d":"code","3e9f3eb4":"code","99d02cf6":"code","5009d35d":"code","27534e1a":"code","cf31779b":"code","02071a79":"code","5766f717":"code","b90f9030":"code","509a586b":"code","a8bb2fe4":"code","ebd9d24a":"code","028c1ee2":"code","b8856305":"code","7e43a246":"code","ba171bc1":"code","fb1c82b1":"code","8ec677d9":"code","af773b61":"code","bea8e7bc":"code","e43e3f7e":"code","e0352728":"code","02c0bd6c":"markdown","d1ea8cc8":"markdown","d3657765":"markdown","72a16563":"markdown","ee55c0d3":"markdown","8d6902f6":"markdown","dc18af48":"markdown","bca7ba07":"markdown","9a5a306a":"markdown","3a8c74a8":"markdown","76e6c57f":"markdown","3cdedfcc":"markdown","b292a411":"markdown","8dd9ba54":"markdown","680ded87":"markdown","18869a6f":"markdown","b8801646":"markdown","1a171d0c":"markdown","aeacfc1a":"markdown","b77a70bf":"markdown","fd6b1e94":"markdown","9de8a54f":"markdown","0e12d089":"markdown","3eae5ae9":"markdown","149ef99b":"markdown","728b7766":"markdown","a4a336d9":"markdown","b0374084":"markdown","344e2558":"markdown","fa859298":"markdown","9ec6c959":"markdown","11bca0a7":"markdown"},"source":{"df54ce07":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\nimport warnings\nwarnings.filterwarnings('ignore')\nimport os\nprint(os.listdir(\"..\/input\"))\n\npd.set_option('display.float_format', lambda x: '{:.3f}'.format(x)) #Limiting floats output to 3 decimal points\n%matplotlib inline","4e2d613c":"# Read files\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\n\n#Save the 'Id' column\ntrain_ID = train['Id']\ntest_ID = test['Id']\n\n#Now drop the  'Id' colum since it's unnecessary for  the prediction process.\ntrain.drop(\"Id\", axis = 1, inplace = True)\ntest.drop(\"Id\", axis = 1, inplace = True)\n\n# For future projects \ny_train = train.SalePrice.values\nall_data = pd.concat((train, test)).reset_index(drop=True)\n\nprint (train.columns)\nprint(test.columns)\nprint(train.shape,test.shape)","c3da3c11":"train.describe()\n","ea3fd9a8":"train.head(7)\n","52177a32":"test.head(7)\n","c4757b0c":"train['SalePrice'].describe()\nsns.distplot(train['SalePrice']);\n#skewness and kurtosis\nprint(\"Skewness: %f\" % train['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % train['SalePrice'].kurt())","f151457d":"from scipy import stats\nfrom scipy.stats import norm, skew #for some statistics\n\n# Plot histogram and probability\nfig = plt.figure(figsize=(15,5))\nplt.subplot(1,2,1)\nsns.distplot(train['SalePrice'] , fit=norm);\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\nplt.subplot(1,2,2)\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.suptitle('Before transformation')\n\n# Apply transformation\ntrain.SalePrice = np.log1p(train.SalePrice )\n# New prediction\ny_train = train.SalePrice.values\n\n\n# Plot histogram and probability after transformation\nfig = plt.figure(figsize=(15,5))\nplt.subplot(1,2,1)\nsns.distplot(train['SalePrice'] , fit=norm);\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\nplt.subplot(1,2,2)\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.suptitle('After transformation')","3e9f3eb4":"# Missing data in train\ntrain_nas = train.isnull().sum()\ntrain_nas = train_nas[train_nas>0]\ntrain_nas.sort_values(ascending=False)","99d02cf6":"# Missing data in test\ntest_nas = test.isnull().sum()\ntest_nas = test_nas[test_nas>0]\ntest_nas.sort_values(ascending = False)","5009d35d":"#missing data percent plot\ntotal = train.isnull().sum().sort_values(ascending=False)\npercent = (train.isnull().sum()\/train.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)","27534e1a":"\n# Handle missing values for features where median\/mean or most common value doesn't make sense\n# Alley : data description says NA means \"no alley access\"\ntrain.loc[:, \"Alley\"] = train.loc[:, \"Alley\"].fillna(\"None\")\n# BedroomAbvGr : NA most likely means 0\ntrain.loc[:, \"BedroomAbvGr\"] = train.loc[:, \"BedroomAbvGr\"].fillna(0)\n# BsmtQual etc : data description says NA for basement features is \"no basement\"\ntrain.loc[:, \"BsmtQual\"] = train.loc[:, \"BsmtQual\"].fillna(\"No\")\ntrain.loc[:, \"BsmtCond\"] = train.loc[:, \"BsmtCond\"].fillna(\"No\")\ntrain.loc[:, \"BsmtExposure\"] = train.loc[:, \"BsmtExposure\"].fillna(\"No\")\ntrain.loc[:, \"BsmtFinType1\"] = train.loc[:, \"BsmtFinType1\"].fillna(\"No\")\ntrain.loc[:, \"BsmtFinType2\"] = train.loc[:, \"BsmtFinType2\"].fillna(\"No\")\ntrain.loc[:, \"BsmtFullBath\"] = train.loc[:, \"BsmtFullBath\"].fillna(0)\ntrain.loc[:, \"BsmtHalfBath\"] = train.loc[:, \"BsmtHalfBath\"].fillna(0)\ntrain.loc[:, \"BsmtUnfSF\"] = train.loc[:, \"BsmtUnfSF\"].fillna(0)\n# CentralAir : NA most likely means No\ntrain.loc[:, \"CentralAir\"] = train.loc[:, \"CentralAir\"].fillna(\"N\")\n# Condition : NA most likely means Normal\ntrain.loc[:, \"Condition1\"] = train.loc[:, \"Condition1\"].fillna(\"Norm\")\ntrain.loc[:, \"Condition2\"] = train.loc[:, \"Condition2\"].fillna(\"Norm\")\n# EnclosedPorch : NA most likely means no enclosed porch\ntrain.loc[:, \"EnclosedPorch\"] = train.loc[:, \"EnclosedPorch\"].fillna(0)\n# External stuff : NA most likely means average\ntrain.loc[:, \"ExterCond\"] = train.loc[:, \"ExterCond\"].fillna(\"TA\")\ntrain.loc[:, \"ExterQual\"] = train.loc[:, \"ExterQual\"].fillna(\"TA\")\n# Fence : data description says NA means \"no fence\"\ntrain.loc[:, \"Fence\"] = train.loc[:, \"Fence\"].fillna(\"No\")\n# FireplaceQu : data description says NA means \"no fireplace\"\ntrain.loc[:, \"FireplaceQu\"] = train.loc[:, \"FireplaceQu\"].fillna(\"No\")\ntrain.loc[:, \"Fireplaces\"] = train.loc[:, \"Fireplaces\"].fillna(0)\n# Functional : data description says NA means typical\ntrain.loc[:, \"Functional\"] = train.loc[:, \"Functional\"].fillna(\"Typ\")\n# GarageType etc : data description says NA for garage features is \"no garage\"\ntrain.loc[:, \"GarageType\"] = train.loc[:, \"GarageType\"].fillna(\"No\")\ntrain.loc[:, \"GarageFinish\"] = train.loc[:, \"GarageFinish\"].fillna(\"No\")\ntrain.loc[:, \"GarageQual\"] = train.loc[:, \"GarageQual\"].fillna(\"No\")\ntrain.loc[:, \"GarageCond\"] = train.loc[:, \"GarageCond\"].fillna(\"No\")\ntrain.loc[:, \"GarageArea\"] = train.loc[:, \"GarageArea\"].fillna(0)\ntrain.loc[:, \"GarageYrBlt\"] = train.loc[:, \"GarageYrBlt\"].fillna(0)\n\ntrain.loc[:, \"GarageCars\"] = train.loc[:, \"GarageCars\"].fillna(0)\n# HalfBath : NA most likely means no half baths above grade\ntrain.loc[:, \"HalfBath\"] = train.loc[:, \"HalfBath\"].fillna(0)\n# HeatingQC : NA most likely means typical\ntrain.loc[:, \"HeatingQC\"] = train.loc[:, \"HeatingQC\"].fillna(\"TA\")\n# KitchenAbvGr : NA most likely means 0\ntrain.loc[:, \"KitchenAbvGr\"] = train.loc[:, \"KitchenAbvGr\"].fillna(0)\n# KitchenQual : NA most likely means typical\ntrain.loc[:, \"KitchenQual\"] = train.loc[:, \"KitchenQual\"].fillna(\"TA\")\n# LotFrontage : NA most likely means no lot frontage\ntrain.loc[:, \"LotFrontage\"] = train.loc[:, \"LotFrontage\"].fillna(0)\n# LotShape : NA most likely means regular\ntrain.loc[:, \"LotShape\"] = train.loc[:, \"LotShape\"].fillna(\"Reg\")\n# MasVnrType : NA most likely means no veneer\ntrain.loc[:, \"MasVnrType\"] = train.loc[:, \"MasVnrType\"].fillna(\"None\")\ntrain.loc[:, \"MasVnrArea\"] = train.loc[:, \"MasVnrArea\"].fillna(0)\n# MiscFeature : data description says NA means \"no misc feature\"\ntrain.loc[:, \"MiscFeature\"] = train.loc[:, \"MiscFeature\"].fillna(\"No\")\ntrain.loc[:, \"MiscVal\"] = train.loc[:, \"MiscVal\"].fillna(0)\n# OpenPorchSF : NA most likely means no open porch\ntrain.loc[:, \"OpenPorchSF\"] = train.loc[:, \"OpenPorchSF\"].fillna(0)\n# PavedDrive : NA most likely means not paved\ntrain.loc[:, \"PavedDrive\"] = train.loc[:, \"PavedDrive\"].fillna(\"N\")\n# PoolQC : data description says NA means \"no pool\"\ntrain.loc[:, \"PoolQC\"] = train.loc[:, \"PoolQC\"].fillna(\"No\")\ntrain.loc[:, \"PoolArea\"] = train.loc[:, \"PoolArea\"].fillna(0)\n# SaleCondition : NA most likely means normal sale\ntrain.loc[:, \"SaleCondition\"] = train.loc[:, \"SaleCondition\"].fillna(\"Normal\")\n# ScreenPorch : NA most likely means no screen porch\ntrain.loc[:, \"ScreenPorch\"] = train.loc[:, \"ScreenPorch\"].fillna(0)\n# TotRmsAbvGrd : NA most likely means 0\ntrain.loc[:, \"TotRmsAbvGrd\"] = train.loc[:, \"TotRmsAbvGrd\"].fillna(0)\n# Utilities : NA most likely means all public utilities\ntrain.loc[:, \"Utilities\"] = train.loc[:, \"Utilities\"].fillna(\"AllPub\")\n# WoodDeckSF : NA most likely means no wood deck\ntrain.loc[:, \"WoodDeckSF\"] = train.loc[:, \"WoodDeckSF\"].fillna(0)\n\n\ntrain.head(7)\n\n","cf31779b":"#missing data\ntotal = train.isnull().sum().sort_values(ascending=False)\npercent = (train.isnull().sum()\/train.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(10)","02071a79":"#dealing with the last NAN\n# train = train.drop((missing_data[missing_data['Total'] > 1]).index,1)\ntrain = train.drop(train.loc[train['Electrical'].isnull()].index)\ntrain.isnull().sum().max() #just checking that there's no missing data missing...","5766f717":"# Differentiate numerical features (minus the target) and categorical features\ncategorical_features = train.select_dtypes(include=['object']).columns\nprint(categorical_features)\nnumerical_features = train.select_dtypes(exclude = [\"object\"]).columns\nprint(numerical_features)\n\nnumerical_features = numerical_features.drop(\"SalePrice\")\nprint(\"Numerical features : \" + str(len(numerical_features)))\nprint(\"Categorical features : \" + str(len(categorical_features)))\ntrain_num = train[numerical_features]\ntrain_cat = train[categorical_features]","b90f9030":"train_num.head(5)\n","509a586b":"train_cat.head(5)","a8bb2fe4":"# Plot skew value for each numerical value\nfrom scipy.stats import skew \nskewness = train_num.apply(lambda x: skew(x))\nskewness.sort_values(ascending=False)","ebd9d24a":"skewness = skewness[abs(skewness) > 0.75]\nprint(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n\nfrom scipy.special import boxcox1p\nskewed_features = skewness.index\nlam = 0.0001\nfor feat in skewed_features:\n    train[feat] = boxcox1p(train[feat], lam)\n    \n    \nfrom scipy.stats import skew \ntrain_num = train[numerical_features]  \ntrain[numerical_features] = train_num\n\n\ntrain_num.head(5)\n","028c1ee2":"skewness = train_num.apply(lambda x: skew(x))\nskewness.sort_values(ascending=False)","b8856305":"train.head(10)","7e43a246":"vars = train.columns\n# vars = numerical_features\nfigures_per_time = 4\ncount = 0 \nfor var in vars:\n    x = train[var]\n    y = train['SalePrice']\n    plt.figure(count\/\/figures_per_time,figsize=(25,5))\n    plt.subplot(1,figures_per_time,np.mod(count,4)+1)\n    plt.scatter(x, y);\n    plt.title('f model: T= {}'.format(var))\n    count+=1\n        \n","ba171bc1":"\n# vars_box = ['OverallQual','YearBuilt','BedroomAbvGr']\nvars_box = categorical_features\nfor var in vars_box:\n    data = pd.concat([train['SalePrice'], train[var]], axis=1)\n    f, ax = plt.subplots(figsize=(8, 6))\n    fig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\n","fb1c82b1":"# Complete numerical correlation matrix\ncorrmat = train.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=.8, square=True);","8ec677d9":"# saleprice correlation matrix\ncorr_num = 15 #number of variables for heatmap\ncols_corr = corrmat.nlargest(corr_num, 'SalePrice')['SalePrice'].index\ncorr_mat_sales = np.corrcoef(train[cols_corr].values.T)\nsns.set(font_scale=1.25)\nf, ax = plt.subplots(figsize=(12, 9))\nhm = sns.heatmap(corr_mat_sales, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 7}, yticklabels=cols_corr.values, xticklabels=cols_corr.values)\nplt.show()","af773b61":"# pair plots for variables with largest correlation\nvar_num = 10\nvars = cols_corr[0:var_num]\nsns.set()\n# cols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\nsns.pairplot(train[vars], size = 2.5)\nplt.show();\n\n","bea8e7bc":"# 2* Combinations of existing features\n# Overall quality of the house\ntrain[\"OverallGrade\"] = train[\"OverallQual\"] * train[\"OverallCond\"]\n# Overall fireplace score\ntrain[\"FireplaceScore\"] = train[\"Fireplaces\"] * train[\"FireplaceQu\"]\n# Overall garage score\ntrain[\"GarageScore\"] = train[\"GarageArea\"] * train[\"GarageQual\"]\n\n\n# Total number of bathrooms\ntrain[\"TotalBath\"] = train[\"BsmtFullBath\"] + (0.5 * train[\"BsmtHalfBath\"]) + \\\ntrain[\"FullBath\"] + (0.5 * train[\"HalfBath\"])\n# Total SF for house (incl. basement)\ntrain[\"AllSF\"] = train[\"GrLivArea\"] + train[\"TotalBsmtSF\"]\n# Total SF for 1st + 2nd floors\ntrain[\"AllFlrsSF\"] = train[\"1stFlrSF\"] + train[\"2ndFlrSF\"]\n# Total SF for porch\ntrain[\"AllPorchSF\"] = train[\"OpenPorchSF\"] + train[\"EnclosedPorch\"] + \\\ntrain[\"3SsnPorch\"] + train[\"ScreenPorch\"]\n# Has masonry veneer or not\n\n\n\ntrain[\"OverallQual-s2\"] = train[\"OverallQual\"] ** 2\ntrain[\"OverallQual-s3\"] = train[\"OverallQual\"] ** 3\ntrain[\"OverallQual-Sq\"] = np.sqrt(train[\"OverallQual\"])\ntrain[\"AllSF-2\"] = train[\"AllSF\"] ** 2\ntrain[\"AllSF-3\"] = train[\"AllSF\"] ** 3\ntrain[\"AllSF-Sq\"] = np.sqrt(train[\"AllSF\"])\ntrain[\"AllFlrsSF-2\"] = train[\"AllFlrsSF\"] ** 2\ntrain[\"AllFlrsSF-3\"] = train[\"AllFlrsSF\"] ** 3\ntrain[\"AllFlrsSF-Sq\"] = np.sqrt(train[\"AllFlrsSF\"])\ntrain[\"GrLivArea-2\"] = train[\"GrLivArea\"] ** 2\ntrain[\"GrLivArea-3\"] = train[\"GrLivArea\"] ** 3\ntrain[\"GrLivArea-Sq\"] = np.sqrt(train[\"GrLivArea\"])\ntrain[\"GarageCars-2\"] = train[\"GarageCars\"] ** 2\ntrain[\"GarageCars-3\"] = train[\"GarageCars\"] ** 3\ntrain[\"GarageCars-Sq\"] = np.sqrt(train[\"GarageCars\"])\ntrain[\"TotalBath-2\"] = train[\"TotalBath\"] ** 2\ntrain[\"TotalBath-3\"] = train[\"TotalBath\"] ** 3\ntrain[\"TotalBath-Sq\"] = np.sqrt(train[\"TotalBath\"])\n\n","e43e3f7e":"# Complete numerical correlation matrix\ncorrmat = train.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=.8, square=True);","e0352728":"# saleprice correlation matrix\ncorr_num = 15*3 #number of variables for heatmap\ncols_corr = corrmat.nlargest(corr_num, 'SalePrice')['SalePrice'].index\ncorr_mat_sales = np.corrcoef(train[cols_corr].values.T)\nsns.set(font_scale=1.25  )\nf, ax = plt.subplots(figsize=(12 * 3, 9 * 3))\nhm = sns.heatmap(corr_mat_sales, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 12}, yticklabels=cols_corr.values, xticklabels=cols_corr.values)\nplt.show()","02c0bd6c":"### 7.1 Creating new features","d1ea8cc8":"### 6.1 Visually comparing data to sale prices\nOne can observe the behaviour of the variables, locate outlier and more.","d3657765":"## 1. Importing libraries","72a16563":"## 4. Missing data","ee55c0d3":"### 4.2 Replacing the missing data\n\nCorrecting for the format, mostly filling NaN with \"No\" or \"0\"","8d6902f6":"## 5. Numerical and Categorial features","dc18af48":"## 3. The predicted variable - Sales price Skew & kurtosis analysis\nThe predicted variable is probably the most important variable, therefore it should be inspected throughly. \n<br> It turns out models work better with symmetric gaussian distributions, therefore we want to get rid of the skewness by using log transformation. More on log transformation later\n<br> <br> Skew: \n\\begin{equation} \nskew \\left( X  \\right) = E[ \\frac{X-\\mu}{\\sigma} ]^3\n\\end{equation} https:\/\/en.wikipedia.org\/wiki\/Skewness\n![](https:\/\/www.managedfuturesinvesting.com\/images\/default-source\/default-album\/measure-of-skewness.jpg?sfvrsn=0)\n \n<br> Kurtosis: $$kurtosis(X) = E[ (\\frac{X-\\mu}{\\sigma})^4  ]$$\nhttps:\/\/en.wikipedia.org\/wiki\/Kurtosis\n![](https:\/\/siemensplm.i.lithium.com\/t5\/image\/serverpage\/image-id\/38460iB0F0D63C4F9B568A\/image-size\/large?v=1.0&px=999)","bca7ba07":"### 3.1 Observing Sale price histogram\n","9a5a306a":"### 3.2 Tansforming: \\begin{equation*} Y = log(1 + X)) \\end{equation*}\nShould correct for skew.\n<br> A random example of a different log transformation\n![](http:\/\/www.biostathandbook.com\/pix\/transformfig1.gif)","3a8c74a8":"## Introduction\n\nHi\n\nThis is my first attempt at DL\/ML of something other than images. Therefore, I wanted to start with well explored dataset that has a lot of good kernels, relatively small amount of data and this competiotion seems to suit it. This kernel is mostly about getting farmiliar with the data through visualization, common feature tricks as log transform, missing data handling and generally how to approach various features of the data. Hopefully someone will find this useful.\n<br> Upvotes, notes and remarks are very appriciated!\n<brr>\nI heavily used those kernels: \n<br>\nSource1 : https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python\n<br>\nSource2 : https:\/\/www.kaggle.com\/bsivavenu\/house-price-calculation-methods-for-beginners\n<br> \nSource3 : https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard\n\n\n### Table of interest:\n> ### 1. Importing libraries\n> ### 2. Importing and inquiring data\n> > #### 2.1 Importing data\n> > #### 2.2 Quiring data\n> ### 3. The predicted variable - Sales price Skew & kurtosis analysis\n> > #### 3.1 Observing histogram\n> > #### 3.2 Tansforming log or box cox: \n> ### 4. Missing data\n> > #### 4.1 Presenting and locating missing data\n> > #### 4.2 Replacing the missing data\n> ### 5. Numerical and Categorial features\n> > #### 5.1 Splitting the data into categorial and numerical features\n> > #### 5.2 Box cox transform for skewd numerical data\n> ### 6.Plotting the data\n> > #### 6.1 Visually comparing data to sale prices\n> > #### 6.2 Comparing data to sale price through correlation matrix\n> > #### 6.3 Pairplot for the most intresting parameters\n> ### 7 Feature Engineering \n> > #### 7.1 Creating new features\n> > #### 7.2 Evaluation created features\n\n\n\n","76e6c57f":"## 7 Feature Engineering ","3cdedfcc":"### 4.1 Locating missing data\n","b292a411":"### 2.1 Importing data","8dd9ba54":"Sales price corr with new features","680ded87":"## Replacing format","18869a6f":"Numerical values correlation matrix, to locate dependencies between different variables. ","b8801646":"## 6.Plotting the data","1a171d0c":"Encode categorial features: can and should be replaced.","aeacfc1a":"Checking that we got rid of all the NaN's","b77a70bf":"Corr with new features","fd6b1e94":"### 5.2 Box cox transform for skewd numerical data\nAnother transformation to reduce skew. \n<br> Equation:\n![](https:\/\/www.statisticshowto.datasciencecentral.com\/wp-content\/uploads\/2015\/07\/boxcox-formula-1.png)\n<br> Transformation example:\n![](https:\/\/www.itl.nist.gov\/div898\/handbook\/eda\/section3\/gif\/boxcox.gif)","9de8a54f":"Importing and dropping ID.","0e12d089":"## 2. Import And quiring data","3eae5ae9":"### 7.2 Evaluation created features","149ef99b":"Check all numerical\/categorial","728b7766":"### 5.1 Splitting the data into categorial and numerical features","a4a336d9":"### 6.2 Comparing data to sale price through correlation matrix","b0374084":"Observe the correction. \nWe can see that a lot of parameters remained skewd. I suspect that's for variables that have a lot of 0. ","344e2558":"### 6.3 Pairplot for the most intresting parameters","fa859298":"### Optional: Box plot\n\nBox plot is heavy, one can manualy choose the intresting parameters","9ec6c959":"### 2.2 Quiring the data\nJust watching what's out there","11bca0a7":"#### Largest correlation with Sale Price\nIts important to remmber that this are 2D correlations, between sale price and another variable. When stacking all of the parameters the dependencies the picture gets more complex."}}