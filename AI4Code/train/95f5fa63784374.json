{"cell_type":{"852a2e93":"code","f40d854d":"code","39c866bf":"code","067cd44d":"code","f074cc3b":"code","fc571e0f":"code","f0f9b790":"code","7441257c":"code","de7562ee":"code","1d14572a":"code","b7b8a178":"code","1049da1f":"code","756c85a3":"code","c297e026":"markdown","8b26b586":"markdown","e062f846":"markdown","6114774f":"markdown","8c283e49":"markdown","fb5a2f6a":"markdown","ec493d9e":"markdown","f19ecce6":"markdown"},"source":{"852a2e93":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.decomposition import PCA,KernelPCA\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import KFold, train_test_split\nfrom sklearn import metrics\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","f40d854d":"complete_features = pd.read_csv(\"..\/input\/cached-features\/complete_features.csv\", parse_dates=[\"first_active_month\"])\ncomplete_features_test = pd.read_csv(\"..\/input\/cached-features\/complete_features_test.csv\", parse_dates=[\"first_active_month\"])","39c866bf":"target = complete_features['target']\ndrops = ['card_id', 'first_active_month', 'target']\n# to_remove = [c for c in complete_features if 'new' in c]\nuse_cols = [c for c in complete_features.columns if c not in drops]\nfeatures = list(complete_features[use_cols].columns)","067cd44d":"complete_features[features] = complete_features[features].fillna(0)\ncomplete_features_test[features] = complete_features_test[features].fillna(0)\npca = PCA(n_components=2)\nreduced_data = pca.fit_transform(complete_features[features])\nprint(reduced_data.shape)\nprint(pca.explained_variance_ratio_)\nplt.plot(reduced_data[:,0],reduced_data[:,1],'.b')\nplt.show()\nreduced_df = pd.DataFrame({'x' : reduced_data[:,0], 'y' : reduced_data[:,1], 'c': complete_features.target.apply(lambda x : x<-20).astype(int)})\nplt.scatter(x=reduced_df['x'], y=reduced_df['y'],c=reduced_df['c'], s=1)","f074cc3b":"kpca = KernelPCA(n_components=2, kernel='rbf')\nnp.random.seed(3)\ns = pd.concat([complete_features[complete_features['target']>-20].sample(2207),complete_features[complete_features['target']<-20]]).reset_index(drop='index')\n\nreduced_data_d = kpca.fit_transform(s[features])\nprint(reduced_data_d.shape)\nplt.plot(reduced_data_d[:,0],reduced_data_d[:,1],'.b')","fc571e0f":"reduced_df_d = pd.DataFrame({'x' : reduced_data_d[:,0], 'y' : reduced_data_d[:,1], 'c': s.target.apply(lambda x : x<-20).astype(int)})\nplt.scatter(x=reduced_df_d['x'], y=reduced_df_d['y'],c=reduced_df_d['c'], s=1)","f0f9b790":"X_train, X_test, y_train, y_test = train_test_split(\n    reduced_df_d[['x','y']].values, s.target.apply(lambda x : -1 if x<-20 else 1).values, test_size=0.2, random_state=42)","7441257c":"svm = SVC(C=1,gamma=1.3, tol=0.0001, kernel='rbf', degree=5)\nsvm.fit(X_train, y_train)\nrf = RandomForestClassifier(n_estimators=100,max_depth=5)\nrf.fit(X_train, y_train)","de7562ee":"\ndef make_meshgrid(x, y, h=.002):\n    \"\"\"Create a mesh of points to plot in\n\n    Parameters\n    ----------\n    x: data to base x-axis meshgrid on\n    y: data to base y-axis meshgrid on\n    h: stepsize for meshgrid, optional\n\n    Returns\n    -------\n    xx, yy : ndarray\n    \"\"\"\n    x_min, x_max = x.min() - 0.1, x.max() + 0.1\n    y_min, y_max = y.min() - 0.1, y.max() + 0.1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n    return xx, yy\n\n\ndef plot_contours(ax, clf, xx, yy, **params):\n    \"\"\"Plot the decision boundaries for a classifier.\n\n    Parameters\n    ----------\n    ax: matplotlib axes object\n    clf: a classifier\n    xx: meshgrid ndarray\n    yy: meshgrid ndarray\n    params: dictionary of params to pass to contourf, optional\n    \"\"\"\n    if not isinstance(clf, RandomForestClassifier):\n        Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n        Z = Z.reshape(xx.shape)\n        out = ax.contourf(xx, yy, Z, **params)\n        return out\n    else:\n        for tree in clf.estimators_:\n            Z = tree.predict(np.c_[xx.ravel(), yy.ravel()])\n            Z = Z.reshape(xx.shape)\n            plt.contourf(xx, yy, Z, alpha=1. \/len(clf.estimators_), cmap=plt.cm.coolwarm)\n\n    \n","1d14572a":"fig, ax = plt.subplots(figsize=(12,10))\nX0, X1 = X_train[:,0],X_train[:,1]\nxx, yy = make_meshgrid(X0, X1)\n\nplot_contours(ax, svm, xx, yy,\n                  cmap=plt.cm.coolwarm, alpha=0.8)\nax.scatter(X0, X1, c=y_train, cmap=plt.cm.coolwarm, s=25, edgecolors='k')","b7b8a178":"print(metrics.classification_report(y_train, svm.predict(X_train)))","1049da1f":"fig, ax = plt.subplots(figsize=(12,10))\n\nplot_contours(ax, rf, xx, yy,\n                  cmap=plt.cm.coolwarm, alpha=1.0 \/ len(rf.estimators_))\nax.scatter(X0, X1, c=y_train, cmap=plt.cm.coolwarm, s=25, edgecolors='k')","756c85a3":"print(metrics.classification_report(y_train, rf.predict(X_train)))","c297e026":"Most of the features are obtained from [You're Going to Want More Categories](https:\/\/www.kaggle.com\/peterhurford\/you-re-going-to-want-more-categories-lb-3-737) which I believe is a greate starter kernel for anyone wanting to join in the fun! I have also added a few features of my own from the merchants dataset which seemed largely unexplored but to no avail!~","8b26b586":"So this visualisation here needs a bit of thinking. We first see that data points are spread almost uniformly across this 2D space. Most importantly that our outliers are not easily distinguishable as they are spread across the whole space and not concentrated anywhere. I make the following observations :\n* We are potentially lacking signals that the top competitors have identified and are able to produce a better distinction between the outliers and non-outliers as the outliers are essentially what produce most of the error\n* We have added too much noise with our features and we need to eliminate a number of them\n\n**BUT** a drawback of PCA is that it performs LINEAR projections so let's try using something differen't called Kernel PCA ([Kernel Principal Components Analysis](https:\/\/en.wikipedia.org\/wiki\/Kernel_principal_component_analysis) which takes advantages of the kernel trick to produce non-linear projections. However, as computing the Kernel Matrix is expensive we can only do this for a sample of our data. ","e062f846":"RandomForest are much better, still not as high as we would like but is still good enough. We can now use this classifier by first transforming the rest of our data using KPCA and adding an extra feature called 'outlier_proba'. I will be extending this kernel later on to do this.","6114774f":"Now this seems a bit better but it is still noisy. Let's try and build some **classifiers** to see how well we can classifiy outliers. I used SVC and RandomForests which to get a sense of what is going on","8c283e49":"This is a kernel to explore the outliers for the [Elo Merchant Category Recommendation Competition](https:\/\/www.kaggle.com\/c\/elo-merchant-category-recommendation) . It can also serve as a reall good introduction to **dimensionality reduction** for visualisation and model training purposes.","fb5a2f6a":"Let's plot the boundaries for SVMs","ec493d9e":"So the process here will be pretty simple I am using PCA ([Principal Components Analysis ](https:\/\/en.wikipedia.org\/wiki\/Principal_component_analysis)) to reduce the dimensionality to the first two Principal Components. A good way to think about this process is that it reduces our features into two uncorrelated features which are ideal for visualisations. These two features (=projections) carry the most variance of our dataset. Going a bit meta, PCA finds linear projections in our feature space to which the variance is maximized.","f19ecce6":"Hmmm, SVC is not doing a pretty good job 0 precision and 0 recall for the outlier class"}}