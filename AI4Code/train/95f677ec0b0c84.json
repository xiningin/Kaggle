{"cell_type":{"6f52ebf3":"code","4788823e":"code","4152debd":"code","ac1ec443":"code","aa745418":"code","2bccc017":"code","67c3a992":"code","d2db8555":"code","58502dd5":"code","93286f2a":"code","2f1982ab":"code","157074e9":"code","84aef017":"code","1f93d269":"code","047a47b8":"code","05f85d88":"code","52cfe738":"code","8c1a1bc1":"code","74cac7ac":"code","5e967c47":"code","fc258669":"code","4aed3183":"code","8259f9d7":"code","c42687ac":"code","f8e92d0f":"code","35b5c299":"code","84ee89f5":"code","73046c6f":"code","e69325af":"code","2d00c52e":"code","e9ce9847":"code","f6c182ed":"code","c5ca9ba2":"code","b5c3a5c2":"code","a3d268cc":"code","6c325a35":"code","2e238085":"code","71437f1a":"code","a59aabb7":"code","155b25b0":"code","a85bea9e":"code","55660f53":"code","4dbd9a00":"markdown","3bd243d7":"markdown"},"source":{"6f52ebf3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4788823e":"!pip install py7zr","4152debd":"!python -m py7zr x ..\/input\/cifar-10\/train.7z \/kaggle\/working\/\n!python -m py7zr x ..\/input\/cifar-10\/test.7z \/kaggle\/working\/","ac1ec443":"!ls ..\/working\n","aa745418":"train_images_path = \"\/kaggle\/working\/train\"\ntest_images_path = \"\/kaggle\/working\/test\"","2bccc017":"!pip install tensorflow-gpu\n\n!pip install glob2\n!pip install opencv\n\n!pip install numpy\n!pip install pandas\n\n!pip install scikit-learn \n!pip install pydot\n!pip install matplotlib","67c3a992":"import glob\nimport os\nimport cv2\nimport numpy as np \n        \n# Since the folders only contain Images, the size of the datasets is the number of files in it's folder\nnum_of_train_images = len(glob.glob(train_images_path+\"\/*\"))\nnum_of_test_images = len(glob.glob(test_images_path+\"\/*\"))\n\n# Let's create a dataset from the images\ntrain_images = [[]]*num_of_train_images\nfor dir_name, _, filenames in os.walk('\/kaggle\/working\/train'):\n    for filename in filenames:\n        image_index = int(filename.split(\".\")[0])-1\n        img = cv2.imread(os.path.join(dir_name,filename))\n        # Add the image to the dataset\n        train_images[image_index] = img\n        \ntest_images = [[]]*num_of_test_images\nfor dir_name, _, filenames in os.walk('\/kaggle\/working\/test'):\n    for filename in filenames:\n        image_index = int(filename.split(\".\")[0])-1\n        img = cv2.imread(os.path.join(dir_name,filename))\n        # Add the image to the dataset\n        test_images[image_index] = img\n        \n        \n# The RGB values are between 0 and 255, let's divide them so the values will be between 0 and 1\ntrain_images = np.asarray(train_images, dtype=float)\/255\ntest_images = np.asarray(test_images, dtype=float)\/255\n\n\n\n# # If you want, you can save the dataset and load it, instead of running this block each time\nnp.save(\"..\/working\/train_images.npy\", train_images)\nnp.save(\"..\/working\/test_images.npy\", test_images)","d2db8555":"train_images = np.load(\"..\/working\/train_images.npy\")\ntest_images = np.load(\"..\/working\/test_images.npy\")","58502dd5":"test_images.shape","93286f2a":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd","2f1982ab":"train_images.shape\n","157074e9":"train_labels = pd.read_csv('\/kaggle\/input\/cifar-10\/trainLabels.csv')\n\nprint(\"Number of train images: \", train_images.shape[0])\nprint(\"Number of test images: \", test_images.shape[0])","84aef017":"fig=plt.figure(figsize=(8, 8))\ncolumns = 4\nrows = 2\nfor i in range(1, columns*rows +1):\n    img = train_images[i]\n    fig.add_subplot(rows, columns, i, title=[train_labels.label[i]])\n    plt.imshow(img)\nplt.show()","1f93d269":"import numpy as np \nimport pandas as pd \nfrom tensorflow import keras\n\n# Model architecture\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras import Sequential, datasets,models,layers\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D\nfrom tensorflow.keras.layers import MaxPooling2D, DepthwiseConv2D, BatchNormalization\nfrom tensorflow.keras.models import load_model\n\n# Data processing\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.utils import to_categorical","047a47b8":"from tensorflow.python.client import device_lib\nprint(device_lib.list_local_devices())\n# Make sure you are running on a GPU!","05f85d88":"train_labels.head(5)","52cfe738":"classes = list(set(list(train_labels.label)))\nclasses","8c1a1bc1":"num_classes=len(classes)","74cac7ac":"labels_dict =  {0:'airplane', 1:'automobile', 2:'bird', 3:'cat', 4:'deer', 5:'dog', 6:'frog', 7:'horse', 8:'ship', 9:'truck'}\nlabels_dict_reversed = {'airplane':0, 'automobile':1, 'bird':2, 'cat':3, 'deer':4, 'dog':5, 'frog':6, 'horse':7, 'ship':8, 'truck':9}","5e967c47":"train_labels['Category'] = train_labels.label.map(labels_dict_reversed)","fc258669":"# The categories are represented by numbers from 0 to 9.\n# Our network needs another representation- a vector of 0's and 1's.\n# This function converts the categories to a One-hot vector.\n# For example, if the label is 3, then the function will return [0,0,0,1,0,0,0,0,0,0]\ntrain_labels_categories = keras.utils.to_categorical(train_labels.Category, num_classes)#.astype('uint8')\n\n# Splitting the training data into train set and validation set\nx_train, x_val, y_train, y_val = train_test_split(train_images, train_labels_categories, random_state=0, test_size=0.05)\n\n# for Keras dataset only\n# x_test = test_images.astype('float32')\/255\n# y_test = keras.utils.to_categorical(test_labels, num_classes).astype('uint8')","4aed3183":"train_labels_categories","8259f9d7":"x_train.shape","c42687ac":"x_val.shape","f8e92d0f":"# Data augumetation\ndatagen = ImageDataGenerator(\n        rotation_range=0.3,  \n        zoom_range = 0.1,  \n        width_shift_range=0.1, \n        height_shift_range=0.1,\n        horizontal_flip=True)\ndatagen.fit(x_train)","35b5c299":"# # resnet layer\n# def resnet_layer(inputs,\n#                  num_filters=16,\n#                  kernel_size=3,\n#                  strides=1,\n#                  activation='relu',\n#                  batch_normalization=True,\n#                  conv_first=True):\n\n#     conv = Conv2D(num_filters,\n#                   kernel_size=kernel_size,\n#                   strides=strides,\n#                   padding='same',\n#                   kernel_initializer='he_normal',\n#                   kernel_regularizer=l2(1e-4))\n\n#     x = inputs\n#     if conv_first:\n#         x = conv(x)\n#         if batch_normalization:\n#             x = BatchNormalization()(x)\n#         if activation is not None:\n#             x = Activation(activation)(x)\n#     else:\n#         if batch_normalization:\n#             x = BatchNormalization()(x)\n#         if activation is not None:\n#             x = Activation(activation)(x)\n#         x = conv(x)\n#     return x\n\n\n# def resnet_v1(input_shape, depth, num_classes=10):\n#     # ResNet Version 1 Model builder [a]\n#     if (depth - 2) % 6 != 0:\n#         raise ValueError('depth should be 6n+2 (eg 20, 32, 44 in [a])')\n#     # Start model definition.\n#     num_filters = 16\n#     num_res_blocks = int((depth - 2) \/ 6)\n\n#     inputs = Input(shape=input_shape)\n#     x = resnet_layer(inputs=inputs)\n#     # Instantiate the stack of residual units\n#     for stack in range(3):\n#         for res_block in range(num_res_blocks):\n#             strides = 1\n#             if stack > 0 and res_block == 0:  # first layer but not first stack\n#                 strides = 2  # downsample\n#             y = resnet_layer(inputs=x,\n#                              num_filters=num_filters,\n#                              strides=strides)\n#             y = resnet_layer(inputs=y,\n#                              num_filters=num_filters,\n#                              activation=None)\n#             if stack > 0 and res_block == 0:  # first layer but not first stack\n#                 # linear projection residual shortcut connection to match\n#                 # changed dims\n#                 x = resnet_layer(inputs=x,\n#                                  num_filters=num_filters,\n#                                  kernel_size=1,\n#                                  strides=strides,\n#                                  activation=None,\n#                                  batch_normalization=False)\n#             x = keras.layers.add([x, y])\n#             x = Activation('relu')(x)\n#         num_filters *= 2\n\n#     # Add classifier on top.\n#     # v1 does not use BN after last shortcut connection-ReLU\n#     x = AveragePooling2D(pool_size=8)(x)\n#     y = Flatten()(x)\n#     outputs = Dense(num_classes,\n#                     activation='softmax',\n#                     kernel_initializer='he_normal')(y)\n\n#     # Instantiate model.\n#     model = Model(inputs=inputs, outputs=outputs)\n#     return model","84ee89f5":"depth = n * 6 + 2\ninput_shape = x_train.shape[1:]","73046c6f":"def lr_schedule(epoch):\n    lr = 1e-3\n    if epoch > 180:\n        lr *= 0.5e-3\n    elif epoch > 160:\n        lr *= 1e-3\n    elif epoch > 120:\n        lr *= 1e-2\n    elif epoch > 80:\n        lr *= 1e-1\n    print('Learning rate: ', lr)\n    return lr","e69325af":"# model = resnet_v1(input_shape=input_shape, depth=depth, num_classes=num_classes)\n# model.compile(loss='categorical_crossentropy',\n#               optimizer=Adam(lr=lr_schedule(0)),\n#               metrics=['accuracy'])\n\n# model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n#                     steps_per_epoch=len(x_train) \/\/ batch_size,\n#                     validation_data=(x_test, y_test),\n#                     epochs=epochs, verbose=1, workers=4,\n#                     callbacks=callbacks)","2d00c52e":"model_layers = [\n    Conv2D(32, (3, 3), activation='relu', strides=(1,1), padding='same', input_shape=(32, 32, 3)),\n    BatchNormalization(),\n    DepthwiseConv2D(kernel_size=(3,3), strides=(1, 1), padding='same', activation=keras.activations.relu, depth_multiplier=3),\n#     MaxPooling2D(2, 2),\n    Dropout(rate =0.1),\n    \n    \n    Conv2D(64, (3, 3), activation='relu', strides=(2, 2), padding='same'),\n    BatchNormalization(),\n    DepthwiseConv2D(kernel_size=(3,3), strides=(1, 1), padding='same', activation=keras.activations.relu),\n#     MaxPooling2D(2, 2),\n    Dropout(rate = 0.1),\n    \n    Conv2D(128, (3, 3), activation='relu', strides=(1, 1), padding='same'),\n    BatchNormalization(),\n    DepthwiseConv2D(kernel_size=(3,3), strides=(1, 1), padding='same', activation=keras.activations.relu),\n#     MaxPooling2D(2, 2),\n    Dropout(rate = 0.4),\n    \n    Conv2D(128, (3, 3), activation='relu', strides=(1, 1), padding='same'),\n    BatchNormalization(),\n    DepthwiseConv2D(kernel_size=(1,1), strides=(1, 1), padding='same', activation=keras.activations.relu),\n    \n    \n    Conv2D(256, (3, 3), activation='relu', strides=(2, 2), padding='same'),\n    BatchNormalization(),\n    DepthwiseConv2D(kernel_size=(3,3), strides=(1, 1), padding='same', activation=keras.activations.relu),\n    \n    \n    \n    Conv2D(512, (1, 1), activation='relu', strides=(2, 2), padding='same'),\n    BatchNormalization(),\n    DepthwiseConv2D(kernel_size=(1,1), strides=(1, 1), padding='same', activation=keras.activations.relu),\n    \n#     MaxPooling2D(2, 2),\n    Dropout(rate = 0.4),\n    \n    Flatten(),\n    Dropout(rate = 0.3),\n    Dense(2048, activation='relu'),\n    Dropout(rate = 0.3),\n    Dense(512, activation='relu'),\n    Dropout(rate = 0.4),\n    Dense(10, activation='softmax')\n] \nmodel = Sequential(model_layers)","e9ce9847":"# A summary of the model. \n# We can see how many parameters are in each layer and the total number of parameters.\nmodel.summary()","f6c182ed":"from keras.optimizers import Adam\nmodel.compile(optimizer=Adam(lr=lr_schedule(0)), loss='categorical_crossentropy', metrics=['accuracy'])","c5ca9ba2":"# Visualization of the model\nkeras.utils.plot_model(model)","b5c3a5c2":"mcp_save = ModelCheckpoint('..\/working\/best_model', save_best_only=True, monitor='val_accuracy', mode='max')","a3d268cc":"# Training the model on the augmented train set\n# We reached a score of ~90% after 100 epochs. 5 epochs won't get a score near that, but on Kaggle each epcoh takes a long time.\n# Choose the number of epochs that fits your machine. Same for batch_size, which we set to 512 on my machine.\nnum_of_epochs = 5\nbatch_size = 64 \n# The number of iteration in one epoch is ceil((size of training data)\/(batch size)). ceil(47,500\/64)=ceil(742.1875)=743\nmodel.fit(datagen.flow(x_train, y_train, batch_size=batch_size), validation_data=(x_val, y_val), callbacks=[mcp_save], epochs=num_of_epochs)","6c325a35":"model_ = load_model('..\/working\/best_model', compile=False)","2e238085":"test_predictions = model.predict_classes(test_images)\ntest_predictions","71437f1a":"test_predictions_df = pd.DataFrame(test_predictions)","a59aabb7":"# It's easy to use the sample submission file to create our own submission file\nsamples = pd.read_csv(\"..\/working\/cifar-10\/sampleSubmission.csv\", index_col=False)\n\n# Replace the sample labels with those our model predicted\nsamples.label= test_predictions\n\n# Our model predicts the number of the classes of each image.\n# Kaggle is expecting a string, for example, if our model predicted \"2\" we need to translate it to \"bird\"\nsamples.replace({\"label\":labels_dict}, inplace=True)","155b25b0":"# Let's see how our model predicted some images\nfig=plt.figure(figsize=(8, 8))\ncolumns = 4\nrows = 2\nfor i in range(1, columns*rows +1):\n    img = test_images[i]\n    fig.add_subplot(rows, columns, i, title=samples.label[i])\n    plt.imshow(img)\nplt.show()","a85bea9e":"samples.to_csv(\"..\/working\/submission.csv\", index=False)","55660f53":"!kaggle competitions submit -c cifar-10 -f ..\/working\/submission.csv -m \"Submitted using the notebook\"","4dbd9a00":"Because the decompression is too time-consuming, the code has been tested, but this notebook is only used to help understand the process.","3bd243d7":"# **First try**"}}