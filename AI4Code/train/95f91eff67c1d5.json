{"cell_type":{"2500f89f":"code","23bec20d":"code","15515dd8":"code","e5f3bcc1":"markdown","02704f4f":"markdown"},"source":{"2500f89f":"from nltk.tokenize import word_tokenize\nsentence = '''\nHello I am Aayush living in canada\nbut mentally I am wandering in the streets of India\neating american food\nliving indian life\ndrinking candian water'''\n\n#tokenisation\ntokens = word_tokenize(sentence)\nprint(tokens)\n\n#lowercase\nlowercase_word = []\nfor word in tokens:\n    word = word.lower()\n#     print(word)\n    lowercase_word.append(word)\n\nlowercase_word\n\n#PorterStemmer\nfrom nltk.stem.porter import PorterStemmer\nps = PorterStemmer()\nfor word in lowercase_word:\n    tokens = ps.stem(word)\n    print(word, \":\",tokens)\n\nimport nltk\nnltk.download('wordnet')#Need to have this for Stemming\n\n# Lemmatizer\nfrom nltk.stem import WordNetLemmatizer\nwml = WordNetLemmatizer()\nlemma = []\nfor word in lowercase_word:\n    tokens = wml.lemmatize(word)\n    lemma.append(tokens)\n    print(word,\":\",tokens)\n\n#Good words\nfrom nltk.corpus import stopwords\nfilter_words = []\nStopwords = set(stopwords.words('english'))\nfor word in lemma:\n    if word not in Stopwords:\n        filter_words.append(word)\n\nfilter_words","23bec20d":"import pandas as pd\n\n# Loading the spam data\n# ham is the label for non-spam messages\nspam = pd.read_csv('..\/input\/nlp-course\/spam.csv')\nspam.head(10)","15515dd8":"spam['text'].tolist()[:20]","e5f3bcc1":"# Text Classification with SpaCy\n\nA common task in NLP is **text classification**. This is \"classification\" in the conventional machine learning sense, and it is applied to text. Examples include spam detection, sentiment analysis, and tagging customer queries. \n\nIn this tutorial, you'll learn text classification with spaCy. The classifier will detect spam messages, a common functionality in most email clients. Here is an overview of the data you'll use:","02704f4f":"# Repeat the same for the spam data(filter by label \"ham\")\n# try cleaning the data, find patterns which are evident. like cleaning www., numbers, words which are non english"}}