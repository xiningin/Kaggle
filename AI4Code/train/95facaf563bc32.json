{"cell_type":{"e03d884f":"code","5325f11d":"code","02492446":"code","8a54baa7":"code","06468168":"code","73f651df":"code","9ab9a96f":"code","c240684b":"code","92ed9e68":"code","999d2ca2":"code","929c85cc":"markdown","d0c0981f":"markdown","7627b0bf":"markdown","c2eb413c":"markdown","4bb5addf":"markdown","706f3501":"markdown","930596f3":"markdown","80457841":"markdown","fde1a51a":"markdown","59f48921":"markdown"},"source":{"e03d884f":"import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import OneHotEncoder\nimport textblob\n\nprint(\"Loading data...\")\ntrain = pd.read_csv(\"..\/input\/movie-review-sentiment-analysis-kernels-only\/train.tsv\", sep=\"\\t\")\nprint(\"Train shape:\", train.shape)\ntest = pd.read_csv(\"..\/input\/movie-review-sentiment-analysis-kernels-only\/test.tsv\", sep=\"\\t\")\nprint(\"Test shape:\", test.shape)\n\nenc = OneHotEncoder(sparse=False)\nenc.fit(train[\"Sentiment\"].values.reshape(-1, 1))\nprint(\"Number of classes:\", enc.n_values_[0])\n\nprint(\"Class distribution:\\n{}\".format(train[\"Sentiment\"].value_counts()\/train.shape[0]))","5325f11d":"print(\"Ratio of test set examples which occur in the train set: {0:.2f}\".format(len(set(train[\"Phrase\"]).intersection(set(test[\"Phrase\"])))\/test.shape[0]))\ntest = pd.merge(test, train[[\"Phrase\", \"Sentiment\"]], on=\"Phrase\", how=\"left\")","02492446":"from sklearn.feature_extraction.text import CountVectorizer\n\ncv1 = CountVectorizer()\ncv1.fit(train[\"Phrase\"])\n\ncv2 = CountVectorizer()\ncv2.fit(test[\"Phrase\"])\n\nprint(\"Train Set Vocabulary Size:\", len(cv1.vocabulary_))\nprint(\"Test Set Vocabulary Size:\", len(cv2.vocabulary_))\nprint(\"Number of Words that occur in both:\", len(set(cv1.vocabulary_.keys()).intersection(set(cv2.vocabulary_.keys()))))","8a54baa7":"def transform(df):\n    df[\"phrase_count\"] = df.groupby(\"SentenceId\")[\"Phrase\"].transform(\"count\")\n    df[\"word_count\"] = df[\"Phrase\"].apply(lambda x: len(x.split()))\n    df[\"has_upper\"] = df[\"Phrase\"].apply(lambda x: x.lower() != x)\n    df[\"sentence_end\"] = df[\"Phrase\"].apply(lambda x: x.endswith(\".\"))\n    df[\"after_comma\"] = df[\"Phrase\"].apply(lambda x: x.startswith(\",\"))\n    df[\"sentence_start\"] = df[\"Phrase\"].apply(lambda x: \"A\" <= x[0] <= \"Z\")\n    df[\"Phrase\"] = df[\"Phrase\"].apply(lambda x: x.lower())\n    return df\n\ntrain = transform(train)\ntest = transform(test)\n\ndense_features = [\"phrase_count\", \"word_count\", \"has_upper\", \"after_comma\", \"sentence_start\", \"sentence_end\"]\n\ntrain.groupby(\"Sentiment\")[dense_features].mean()","06468168":"NUM_FOLDS = 5\n\ntrain[\"fold_id\"] = train[\"SentenceId\"].apply(lambda x: x%NUM_FOLDS)","73f651df":"EMBEDDING_FILE = \"..\/input\/glove-global-vectors-for-word-representation\/glove.6B.100d.txt\"\nEMBEDDING_DIM = 100\n\nall_words = set(cv1.vocabulary_.keys()).union(set(cv2.vocabulary_.keys()))\n\ndef get_embedding():\n    embeddings_index = {}\n    f = open(EMBEDDING_FILE)\n    for line in f:\n        values = line.split()\n        word = values[0]\n        if len(values) == EMBEDDING_DIM + 1 and word in all_words:\n            coefs = np.asarray(values[1:], dtype=\"float32\")\n            embeddings_index[word] = coefs\n    f.close()\n    return embeddings_index\n\nembeddings_index = get_embedding()\nprint(\"Number of words that don't exist in GLOVE:\", len(all_words - set(embeddings_index)))","9ab9a96f":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nMAX_SEQUENCE_LENGTH = 60\n\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(np.append(train[\"Phrase\"].values, test[\"Phrase\"].values))\nword_index = tokenizer.word_index\n\nnb_words = len(word_index) + 1\nembedding_matrix = np.random.rand(nb_words, EMBEDDING_DIM + 2)\n\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    sent = textblob.TextBlob(word).sentiment\n    if embedding_vector is not None:\n        embedding_matrix[i] = np.append(embedding_vector, [sent.polarity, sent.subjectivity])\n    else:\n        embedding_matrix[i, -2:] = [sent.polarity, sent.subjectivity]\n        \nseq = pad_sequences(tokenizer.texts_to_sequences(train[\"Phrase\"]), maxlen=MAX_SEQUENCE_LENGTH)\ntest_seq = pad_sequences(tokenizer.texts_to_sequences(test[\"Phrase\"]), maxlen=MAX_SEQUENCE_LENGTH)","c240684b":"from keras.layers import *\nfrom keras.models import Model\nfrom keras.callbacks import EarlyStopping\n\ndef build_model():\n    embedding_layer = Embedding(nb_words,\n                                EMBEDDING_DIM + 2,\n                                weights=[embedding_matrix],\n                                input_length=MAX_SEQUENCE_LENGTH,\n                                trainable=True)\n    dropout = SpatialDropout1D(0.2)\n    mask_layer = Masking()\n    lstm_layer = LSTM(50)\n    \n    seq_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=\"int32\")\n    dense_input = Input(shape=(len(dense_features),))\n    \n    dense_vector = BatchNormalization()(dense_input)\n    \n    phrase_vector = lstm_layer(mask_layer(dropout(embedding_layer(seq_input))))\n    \n    feature_vector = concatenate([phrase_vector, dense_vector])\n    feature_vector = Dense(50, activation=\"relu\")(feature_vector)\n    feature_vector = Dense(20, activation=\"relu\")(feature_vector)\n    \n    output = Dense(5, activation=\"softmax\")(feature_vector)\n    \n    model = Model(inputs=[seq_input, dense_input], outputs=output)\n    return model","92ed9e68":"test_preds = np.zeros((test.shape[0], 5))\n\nfor i in range(NUM_FOLDS):\n    print(\"FOLD\", i+1)\n    \n    print(\"Splitting the data into train and validation...\")\n    train_seq, val_seq = seq[train[\"fold_id\"] != i], seq[train[\"fold_id\"] == i]\n    train_dense, val_dense = train[train[\"fold_id\"] != i][dense_features], train[train[\"fold_id\"] == i][dense_features]\n    y_train = enc.transform(train[train[\"fold_id\"] != i][\"Sentiment\"].values.reshape(-1, 1))\n    y_val = enc.transform(train[train[\"fold_id\"] == i][\"Sentiment\"].values.reshape(-1, 1))\n    \n    print(\"Building the model...\")\n    model = build_model()\n    model.compile(loss=\"categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"acc\"])\n    \n    early_stopping = EarlyStopping(monitor=\"val_acc\", patience=2, verbose=1)\n    \n    print(\"Training the model...\")\n    model.fit([train_seq, train_dense], y_train, validation_data=([val_seq, val_dense], y_val),\n              epochs=15, batch_size=1024, shuffle=True, callbacks=[early_stopping], verbose=1)\n    \n    print(\"Predicting...\")\n    test_preds += model.predict([test_seq, test[dense_features]], batch_size=1024, verbose=1)\n    print()\n    \ntest_preds \/= NUM_FOLDS","999d2ca2":"print(\"Select the class with the highest probability as prediction...\")\ntest[\"pred\"] = test_preds.argmax(axis=1)\n\nprint(\"Use these predictions for the phrases which don't exist in train set...\")\ntest.loc[test[\"Sentiment\"].isnull(), \"Sentiment\"] = test.loc[test[\"Sentiment\"].isnull(), \"pred\"]\n\nprint(\"Make the submission ready...\")\ntest[\"Sentiment\"] = test[\"Sentiment\"].astype(int)\ntest[[\"PhraseId\", \"Sentiment\"]].to_csv(\"submission.csv\", index=False)","929c85cc":"Let's see if all the words in the test set occurs in the train set:","d0c0981f":"**Fast and Basic Solution to Movie Review Sentiment Analysis using LSTM\n**\n\nI have used some of my previous code from Quora Duplicate Question Competition. https:\/\/github.com\/aerdem4\/kaggle-quora-dup","7627b0bf":"**Numerical Feature Extraction**","c2eb413c":"**Splitting Data into folds**\n\nIf we split the data totally random, we may bias our validation set because the phrases in the same sentence may be distributed to train and validation sets. We need to guarantee that all phrases of one sentence is in one fold. We can assume that SentenceId%NUM_FOLDS preserves this while splitting the data randomly.","4bb5addf":"**Train the Model:**","706f3501":"**Transfer Learning Using GLOVE Embeddings**","930596f3":"**Making submission...**","80457841":"For the examples which occur in both sets, we can directly use the labels from train set as our prediction.","fde1a51a":"**Prepare the sequences for LSTM**","59f48921":"**Define the Model**"}}