{"cell_type":{"9dca5fe0":"code","21d6ac75":"code","65aa3379":"code","4801cad7":"code","64c82209":"code","ad39f3ee":"code","b0609bfa":"code","95ce6d0a":"code","c8f786a5":"code","7bef1cbc":"code","fc96b860":"code","18e4bd64":"code","c61c1f1f":"code","986d847a":"code","0ac06ef1":"code","d8511014":"code","6846f807":"code","bb666727":"code","33c48dfe":"code","bca331da":"code","17cdd6c8":"code","b012cdf5":"code","2f387b15":"code","b032207e":"code","68169cdc":"code","bfde8333":"code","d77b7490":"code","0849ca61":"code","7f69da0c":"code","319f6aaf":"code","23d6bf8c":"code","2b434900":"code","515a15e2":"code","6d2ecbe8":"code","51b532e7":"code","5215ed06":"code","44924b75":"code","e2242413":"code","b2c1a69a":"code","c4177135":"code","70edb3a6":"code","39e7259f":"code","301dad78":"code","0828aa8c":"code","b3efcf81":"code","cea07448":"code","dcef53e2":"code","a0687fbf":"code","14738c8b":"code","c8bafd42":"code","729cb5ab":"code","6a18d55c":"code","116269ef":"code","f5731a56":"code","cc13c2a0":"code","2d499106":"code","2f02a86c":"code","8dcac021":"code","5b81db82":"code","d466b8f5":"code","59fbe6b9":"code","7881d2da":"code","98c2b7e7":"code","cea59401":"code","7a2d8fc1":"code","c22ebc87":"code","096ed083":"code","fac7b9e5":"code","e20b70a9":"code","b0077b21":"code","cfdc7804":"code","559cc7b1":"code","631c0c82":"code","8700b5b1":"code","7d4d6eed":"code","c796c29c":"code","de0ba04f":"code","67ce8e27":"code","ee80e6ec":"code","c15a8e33":"code","bf84fadc":"code","636e9f68":"code","507b74d0":"code","bd7bba1d":"code","8c76a0a3":"code","b40ffee9":"code","ebcf7415":"code","bf12526b":"code","8cce9688":"code","3bc51b84":"code","bf2b6f46":"code","46c63d43":"markdown","29269c99":"markdown","1340d2a6":"markdown","af87bda2":"markdown","87e1bd5e":"markdown","056d2e6d":"markdown","0bfc1ba1":"markdown","0f18418a":"markdown","345fd498":"markdown","52527d97":"markdown","16c8d051":"markdown","e3b1458c":"markdown","6a14aff6":"markdown","b7db738d":"markdown","f34151c5":"markdown","15a5bf6b":"markdown","81fd7771":"markdown","5b3fca76":"markdown","4133a4dd":"markdown","a2a71057":"markdown","b6b54c0f":"markdown","bb456834":"markdown","0df2cb6e":"markdown","0e395073":"markdown","c0753ae4":"markdown","747bd5a1":"markdown","e9453792":"markdown","ac7f641c":"markdown","69aefa78":"markdown","d72ab786":"markdown","d1ba9c16":"markdown","92e32006":"markdown","fb107bf9":"markdown","83f579af":"markdown","feb10f50":"markdown","b5aa3b3c":"markdown","9eb6cf06":"markdown","5f0e3f1e":"markdown","b67aa636":"markdown","4aa21383":"markdown","438fb566":"markdown","083660c1":"markdown","7fd4d698":"markdown","43baf418":"markdown","e86bc5ff":"markdown","561e1825":"markdown","948db1c2":"markdown","c8cf7db5":"markdown","1682dbbe":"markdown","450f0cc7":"markdown","849532ff":"markdown","443aee9e":"markdown","05bb0ad7":"markdown","b0051ff9":"markdown","6866764c":"markdown","749bfb29":"markdown","9ef7b56f":"markdown","9c2a5ad7":"markdown","fabca417":"markdown","6092d5fb":"markdown","f83c0be6":"markdown","9d0375c6":"markdown","d538b787":"markdown","fb287d52":"markdown","fdf8ce86":"markdown","2f734428":"markdown","cbfda3d1":"markdown","7925725f":"markdown","669416a7":"markdown"},"source":{"9dca5fe0":"import numpy as np\nimport pandas as pd\npd.set_option('max_columns', 105)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\n%matplotlib inline\nsns.set()\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n#warnings.filterwarnings(\"ignore\")\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))","21d6ac75":"# setting the number of cross validations used in the Model part \nnr_cv = 5\n\n# switch for using log values for SalePrice and features     \nuse_logvals = 1    \n# target used for correlation \ntarget = 'SalePrice_Log'\n    \n# only columns with correlation above this threshold value  \n# are used for the ML Regressors in Part 3\nmin_val_corr = 0.4    \n    \n# switch for dropping columns that are similar to others already used and show a high correlation to these     \ndrop_similar = 1\n","65aa3379":"def get_best_score(grid):\n    \n    best_score = np.sqrt(-grid.best_score_)\n    print(best_score)    \n    print(grid.best_params_)\n    print(grid.best_estimator_)\n    \n    return best_score","4801cad7":"def print_cols_large_corr(df, nr_c, targ) :\n    corr = df.corr()\n    corr_abs = corr.abs()\n    print (corr_abs.nlargest(nr_c, targ)[targ])","64c82209":"def plot_corr_matrix(df, nr_c, targ) :\n    \n    corr = df.corr()\n    corr_abs = corr.abs()\n    cols = corr_abs.nlargest(nr_c, targ)[targ].index\n    cm = np.corrcoef(df[cols].values.T)\n\n    plt.figure(figsize=(nr_c\/1.5, nr_c\/1.5))\n    sns.set(font_scale=1.25)\n    sns.heatmap(cm, linewidths=1.5, annot=True, square=True, \n                fmt='.2f', annot_kws={'size': 10}, \n                yticklabels=cols.values, xticklabels=cols.values\n               )\n    plt.show()","ad39f3ee":"df_train = pd.read_csv(\"..\/input\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/test.csv\")","b0609bfa":"print(df_train.shape)\nprint(\"*\"*50)\nprint(df_test.shape)","95ce6d0a":"print(df_train.info())\nprint(\"*\"*50)\nprint(df_test.info())","c8f786a5":"df_train.head()","7bef1cbc":"df_train.describe()","fc96b860":"df_test.head()","18e4bd64":"df_test.describe()","c61c1f1f":"sns.distplot(df_train['SalePrice']);\n#skewness and kurtosis\nprint(\"Skewness: %f\" % df_train['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % df_train['SalePrice'].kurt())","986d847a":"df_train['SalePrice_Log'] = np.log(df_train['SalePrice'])\n\nsns.distplot(df_train['SalePrice_Log']);\n# skewness and kurtosis\nprint(\"Skewness: %f\" % df_train['SalePrice_Log'].skew())\nprint(\"Kurtosis: %f\" % df_train['SalePrice_Log'].kurt())\n# dropping old column\ndf_train.drop('SalePrice', axis= 1, inplace=True)","0ac06ef1":"numerical_feats = df_train.dtypes[df_train.dtypes != \"object\"].index\nprint(\"Number of Numerical features: \", len(numerical_feats))\n\ncategorical_feats = df_train.dtypes[df_train.dtypes == \"object\"].index\nprint(\"Number of Categorical features: \", len(categorical_feats))","d8511014":"print(df_train[numerical_feats].columns)\nprint(\"*\"*100)\nprint(df_train[categorical_feats].columns)","6846f807":"df_train[numerical_feats].head()","bb666727":"df_train[categorical_feats].head()","33c48dfe":"total = df_train.isnull().sum().sort_values(ascending=False)\npercent = (df_train.isnull().sum()\/df_train.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)","bca331da":"# columns where NaN values have meaning e.g. no pool etc.\ncols_fillna = ['PoolQC','MiscFeature','Alley','Fence','MasVnrType','FireplaceQu',\n               'GarageQual','GarageCond','GarageFinish','GarageType', 'Electrical',\n               'KitchenQual', 'SaleType', 'Functional', 'Exterior2nd', 'Exterior1st',\n               'BsmtExposure','BsmtCond','BsmtQual','BsmtFinType1','BsmtFinType2',\n               'MSZoning', 'Utilities']\n\n# replace 'NaN' with 'None' in these columns\nfor col in cols_fillna:\n    df_train[col].fillna('None',inplace=True)\n    df_test[col].fillna('None',inplace=True)","17cdd6c8":"total = df_train.isnull().sum().sort_values(ascending=False)\npercent = (df_train.isnull().sum()\/df_train.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(5)","b012cdf5":"# fillna with mean for the remaining columns: LotFrontage, GarageYrBlt, MasVnrArea\ndf_train.fillna(df_train.mean(), inplace=True)\ndf_test.fillna(df_test.mean(), inplace=True)","2f387b15":"total = df_train.isnull().sum().sort_values(ascending=False)\npercent = (df_train.isnull().sum()\/df_train.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(5)","b032207e":"df_train.isnull().sum().sum()","68169cdc":"df_test.isnull().sum().sum()","bfde8333":"for col in numerical_feats:\n    print('{:15}'.format(col), \n          'Skewness: {:05.2f}'.format(df_train[col].skew()) , \n          '   ' ,\n          'Kurtosis: {:06.2f}'.format(df_train[col].kurt())  \n         )","d77b7490":"sns.distplot(df_train['GrLivArea']);\n#skewness and kurtosis\nprint(\"Skewness: %f\" % df_train['GrLivArea'].skew())\nprint(\"Kurtosis: %f\" % df_train['GrLivArea'].kurt())","0849ca61":"sns.distplot(df_train['LotArea']);\n#skewness and kurtosis\nprint(\"Skewness: %f\" % df_train['LotArea'].skew())\nprint(\"Kurtosis: %f\" % df_train['LotArea'].kurt())","7f69da0c":"for df in [df_train, df_test]:\n    df['GrLivArea_Log'] = np.log(df['GrLivArea'])\n    df.drop('GrLivArea', inplace= True, axis = 1)\n    df['LotArea_Log'] = np.log(df['LotArea'])\n    df.drop('LotArea', inplace= True, axis = 1)\n    \n    \n    \nnumerical_feats = df_train.dtypes[df_train.dtypes != \"object\"].index\n   ","319f6aaf":"sns.distplot(df_train['GrLivArea_Log']);\n#skewness and kurtosis\nprint(\"Skewness: %f\" % df_train['GrLivArea_Log'].skew())\nprint(\"Kurtosis: %f\" % df_train['GrLivArea_Log'].kurt())","23d6bf8c":"sns.distplot(df_train['LotArea_Log']);\n#skewness and kurtosis\nprint(\"Skewness: %f\" % df_train['LotArea_Log'].skew())\nprint(\"Kurtosis: %f\" % df_train['LotArea_Log'].kurt())","2b434900":"nr_rows = 12\nnr_cols = 3\n\nfig, axs = plt.subplots(nr_rows, nr_cols, figsize=(nr_cols*3.5,nr_rows*3))\n\nli_num_feats = list(numerical_feats)\nli_not_plot = ['Id', 'SalePrice', 'SalePrice_Log']\nli_plot_num_feats = [c for c in list(numerical_feats) if c not in li_not_plot]\n\n\nfor r in range(0,nr_rows):\n    for c in range(0,nr_cols):  \n        i = r*nr_cols+c\n        if i < len(li_plot_num_feats):\n            sns.regplot(df_train[li_plot_num_feats[i]], df_train[target], ax = axs[r][c])\n            stp = stats.pearsonr(df_train[li_plot_num_feats[i]], df_train[target])\n            #axs[r][c].text(0.4,0.9,\"title\",fontsize=7)\n            str_title = \"r = \" + \"{0:.2f}\".format(stp[0]) + \"      \" \"p = \" + \"{0:.2f}\".format(stp[1])\n            axs[r][c].set_title(str_title,fontsize=11)\n            \nplt.tight_layout()    \nplt.show()   ","515a15e2":"df_train = df_train.drop(\n    df_train[(df_train['OverallQual']==10) & (df_train['SalePrice_Log']<12.3)].index)","6d2ecbe8":"df_train = df_train.drop(\n    df_train[(df_train['GrLivArea_Log']>8.3) & (df_train['SalePrice_Log']<12.5)].index)","51b532e7":"corr = df_train.corr()\ncorr_abs = corr.abs()\n\nnr_num_cols = len(numerical_feats)\nser_corr = corr_abs.nlargest(nr_num_cols, target)[target]\n\ncols_abv_corr_limit = list(ser_corr[ser_corr.values > min_val_corr].index)\ncols_bel_corr_limit = list(ser_corr[ser_corr.values <= min_val_corr].index)","5215ed06":"print(ser_corr)\nprint(\"*\"*30)\nprint(\"List of numerical features with r above min_val_corr :\")\nprint(cols_abv_corr_limit)\nprint(\"*\"*30)\nprint(\"List of numerical features with r below min_val_corr :\")\nprint(cols_bel_corr_limit)\n","44924b75":"for catg in list(categorical_feats) :\n    print(df_train[catg].value_counts())\n    print('#'*50)","e2242413":"li_cat_feats = list(categorical_feats)\nnr_rows = 15\nnr_cols = 3\n\nfig, axs = plt.subplots(nr_rows, nr_cols, figsize=(nr_cols*4,nr_rows*3))\n\nfor r in range(0,nr_rows):\n    for c in range(0,nr_cols):  \n        i = r*nr_cols+c\n        if i < len(li_cat_feats):\n            sns.boxplot(x=li_cat_feats[i], y=target, data=df_train, ax = axs[r][c])\n    \nplt.tight_layout()    \nplt.show()   ","b2c1a69a":"catg_strong_corr = [ 'MSZoning', 'Neighborhood', 'Condition2', 'MasVnrType', 'ExterQual', \n                     'BsmtQual','CentralAir', 'Electrical', 'KitchenQual', 'SaleType']\n\ncatg_weak_corr = ['Street', 'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', \n                  'LandSlope', 'Condition1',  'BldgType', 'HouseStyle', 'RoofStyle', \n                  'RoofMatl', 'Exterior1st', 'Exterior2nd', 'ExterCond', 'Foundation', \n                  'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'Heating', \n                  'HeatingQC', 'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish', \n                  'GarageQual', 'GarageCond', 'PavedDrive', 'PoolQC', 'Fence', 'MiscFeature', \n                  'SaleCondition' ]\n      ","c4177135":"nr_feats = len(cols_abv_corr_limit)","70edb3a6":"plot_corr_matrix(df_train, nr_feats, target)","39e7259f":"id_test = df_test['Id']\n\nto_drop_num  = cols_bel_corr_limit\nto_drop_catg = catg_weak_corr\n\ncols_to_drop = ['Id'] + to_drop_num + to_drop_catg \n\nfor df in [df_train, df_test]:\n    df.drop(cols_to_drop, inplace= True, axis = 1)\n","301dad78":"catg_list = catg_strong_corr.copy()\ncatg_list.remove('Neighborhood')\n\nfor catg in catg_list :\n    #sns.catplot(x=catg, y=target, data=df_train, kind='boxen')\n    sns.violinplot(x=catg, y=target, data=df_train)\n    plt.show()\n    #sns.boxenplot(x=catg, y=target, data=df_train)\n    #bp = df_train.boxplot(column=[target], by=catg)","0828aa8c":"fig, ax = plt.subplots()\nfig.set_size_inches(16, 5)\nsns.violinplot(x='Neighborhood', y=target, data=df_train, ax=ax)\nplt.xticks(rotation=45)\nplt.show()","b3efcf81":"for catg in catg_list :\n    g = df_train.groupby(catg)[target].mean()\n    print(g)","cea07448":"# 'MSZoning'\nmsz_catg2 = ['RM', 'RH']\nmsz_catg3 = ['RL', 'FV'] \n\n\n# Neighborhood\nnbhd_catg2 = ['Blmngtn', 'ClearCr', 'CollgCr', 'Crawfor', 'Gilbert', 'NWAmes', 'Somerst', 'Timber', 'Veenker']\nnbhd_catg3 = ['NoRidge', 'NridgHt', 'StoneBr']\n\n# Condition2\ncond2_catg2 = ['Norm', 'RRAe']\ncond2_catg3 = ['PosA', 'PosN'] \n\n# SaleType\nSlTy_catg1 = ['Oth']\nSlTy_catg3 = ['CWD']\nSlTy_catg4 = ['New', 'Con']\n\n\n#[]","dcef53e2":"for df in [df_train, df_test]:\n    \n    df['MSZ_num'] = 1  \n    df.loc[(df['MSZoning'].isin(msz_catg2) ), 'MSZ_num'] = 2    \n    df.loc[(df['MSZoning'].isin(msz_catg3) ), 'MSZ_num'] = 3        \n    \n    df['NbHd_num'] = 1       \n    df.loc[(df['Neighborhood'].isin(nbhd_catg2) ), 'NbHd_num'] = 2    \n    df.loc[(df['Neighborhood'].isin(nbhd_catg3) ), 'NbHd_num'] = 3    \n\n    df['Cond2_num'] = 1       \n    df.loc[(df['Condition2'].isin(cond2_catg2) ), 'Cond2_num'] = 2    \n    df.loc[(df['Condition2'].isin(cond2_catg3) ), 'Cond2_num'] = 3    \n    \n    df['Mas_num'] = 1       \n    df.loc[(df['MasVnrType'] == 'Stone' ), 'Mas_num'] = 2 \n    \n    df['ExtQ_num'] = 1       \n    df.loc[(df['ExterQual'] == 'TA' ), 'ExtQ_num'] = 2     \n    df.loc[(df['ExterQual'] == 'Gd' ), 'ExtQ_num'] = 3     \n    df.loc[(df['ExterQual'] == 'Ex' ), 'ExtQ_num'] = 4     \n   \n    df['BsQ_num'] = 1          \n    df.loc[(df['BsmtQual'] == 'Gd' ), 'BsQ_num'] = 2     \n    df.loc[(df['BsmtQual'] == 'Ex' ), 'BsQ_num'] = 3     \n \n    df['CA_num'] = 0          \n    df.loc[(df['CentralAir'] == 'Y' ), 'CA_num'] = 1    \n\n    df['Elc_num'] = 1       \n    df.loc[(df['Electrical'] == 'SBrkr' ), 'Elc_num'] = 2 \n\n\n    df['KiQ_num'] = 1       \n    df.loc[(df['KitchenQual'] == 'TA' ), 'KiQ_num'] = 2     \n    df.loc[(df['KitchenQual'] == 'Gd' ), 'KiQ_num'] = 3     \n    df.loc[(df['KitchenQual'] == 'Ex' ), 'KiQ_num'] = 4      \n    \n    df['SlTy_num'] = 2       \n    df.loc[(df['SaleType'].isin(SlTy_catg1) ), 'SlTy_num'] = 1  \n    df.loc[(df['SaleType'].isin(SlTy_catg3) ), 'SlTy_num'] = 3  \n    df.loc[(df['SaleType'].isin(SlTy_catg4) ), 'SlTy_num'] = 4  \n  ","a0687fbf":"new_col_num = ['MSZ_num', 'NbHd_num', 'Cond2_num', 'Mas_num', 'ExtQ_num', 'BsQ_num', 'CA_num', 'Elc_num', 'KiQ_num', 'SlTy_num']\n\nnr_rows = 4\nnr_cols = 3\n\nfig, axs = plt.subplots(nr_rows, nr_cols, figsize=(nr_cols*3.5,nr_rows*3))\n\nfor r in range(0,nr_rows):\n    for c in range(0,nr_cols):  \n        i = r*nr_cols+c\n        if i < len(new_col_num):\n            sns.regplot(df_train[new_col_num[i]], df_train[target], ax = axs[r][c])\n            stp = stats.pearsonr(df_train[new_col_num[i]], df_train[target])\n            str_title = \"r = \" + \"{0:.2f}\".format(stp[0]) + \"      \" \"p = \" + \"{0:.2f}\".format(stp[1])\n            axs[r][c].set_title(str_title,fontsize=11)\n            \nplt.tight_layout()    \nplt.show()   ","14738c8b":"catg_cols_to_drop = ['Neighborhood' , 'Condition2', 'MasVnrType', 'ExterQual', 'BsmtQual','CentralAir', 'Electrical', 'KitchenQual', 'SaleType']\n\ncorr1 = df_train.corr()\ncorr_abs_1 = corr1.abs()\n\nnr_all_cols = len(df_train)\nser_corr_1 = corr_abs_1.nlargest(nr_all_cols, target)[target]\n\nprint(ser_corr_1)\ncols_bel_corr_limit_1 = list(ser_corr_1[ser_corr_1.values <= min_val_corr].index)\n\n\nfor df in [df_train, df_test] :\n    df.drop(catg_cols_to_drop, inplace= True, axis = 1)\n    df.drop(cols_bel_corr_limit_1, inplace= True, axis = 1)    ","c8bafd42":"corr2 = df_train.corr()\ncorr_abs_2 = corr2.abs()\n\nnr_all_cols = len(df_train)\nser_corr_2 = corr_abs_2.nlargest(nr_all_cols, target)[target]\n\nprint(ser_corr_2)","729cb5ab":"df_train.head()","6a18d55c":"df_test.head()","116269ef":"corr = df_train.corr()\ncorr_abs = corr.abs()\n\nnr_all_cols = len(df_train)\nprint (corr_abs.nlargest(nr_all_cols, target)[target])","f5731a56":"nr_feats=len(df_train.columns)\nplot_corr_matrix(df_train, nr_feats, target)","cc13c2a0":"cols = corr_abs.nlargest(nr_all_cols, target)[target].index\ncols = list(cols)\n\nif drop_similar == 1 :\n    for col in ['GarageArea','1stFlrSF','TotRmsAbvGrd','GarageYrBlt'] :\n        if col in cols: \n            cols.remove(col)","2d499106":"cols = list(cols)\nprint(cols)","2f02a86c":"feats = cols.copy()\nfeats.remove('SalePrice_Log')\n\nprint(feats)","8dcac021":"df_train_ml = df_train[feats].copy()\ndf_test_ml  = df_test[feats].copy()\n\ny = df_train[target]","5b81db82":"\"\"\"\nall_data = pd.concat((df_train[feats], df_test[feats]))\n\nli_get_dummies = ['OverallQual', 'NbHd_num', 'GarageCars','ExtQ_num', 'KiQ_num',\n                  'BsQ_num', 'FullBath', 'Fireplaces', 'MSZ_num']\nall_data = pd.get_dummies(all_data, columns=li_get_dummies, drop_first=True)\n\ndf_train_ml = all_data[:df_train.shape[0]]\ndf_test_ml  = all_data[df_train.shape[0]:]\n\"\"\"","d466b8f5":"from sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\ndf_train_ml_sc = sc.fit_transform(df_train_ml)\ndf_test_ml_sc = sc.transform(df_test_ml)","59fbe6b9":"df_train_ml_sc = pd.DataFrame(df_train_ml_sc)\ndf_train_ml_sc.head()","7881d2da":"X = df_train_ml.copy()\ny = df_train[target]\nX_test = df_test_ml.copy()\n\nX_sc = df_train_ml_sc.copy()\ny_sc = df_train[target]\nX_test_sc = df_test_ml_sc.copy()\n\nX.info()\nX_test.info()","98c2b7e7":"X.head()","cea59401":"X_sc.head()","7a2d8fc1":"X_test.head()","c22ebc87":"from sklearn.model_selection import GridSearchCV\nscore_calc = 'neg_mean_squared_error'","096ed083":"from sklearn.linear_model import LinearRegression\n\nlinreg = LinearRegression()\nparameters = {'fit_intercept':[True,False], 'normalize':[True,False], 'copy_X':[True, False]}\ngrid_linear = GridSearchCV(linreg, parameters, cv=nr_cv, verbose=1 , scoring = score_calc)\ngrid_linear.fit(X, y)\n\nsc_linear = get_best_score(grid_linear)","fac7b9e5":"linreg_sc = LinearRegression()\nparameters = {'fit_intercept':[True,False], 'normalize':[True,False], 'copy_X':[True, False]}\ngrid_linear_sc = GridSearchCV(linreg_sc, parameters, cv=nr_cv, verbose=1 , scoring = score_calc)\ngrid_linear_sc.fit(X_sc, y)\n\nsc_linear_sc = get_best_score(grid_linear_sc)","e20b70a9":"linregr_all = LinearRegression()\n#linregr_all.fit(X_train_all, y_train_all)\nlinregr_all.fit(X, y)\npred_linreg_all = linregr_all.predict(X_test)\npred_linreg_all[pred_linreg_all < 0] = pred_linreg_all.mean()","b0077b21":"sub_linreg = pd.DataFrame()\nsub_linreg['Id'] = id_test\nsub_linreg['SalePrice'] = pred_linreg_all\n#sub_linreg.to_csv('linreg.csv',index=False)","cfdc7804":"from sklearn.linear_model import Ridge\n\nridge = Ridge()\nparameters = {'alpha':[0.001,0.005,0.01,0.1,0.5,1], 'normalize':[True,False], 'tol':[1e-06,5e-06,1e-05,5e-05]}\ngrid_ridge = GridSearchCV(ridge, parameters, cv=nr_cv, verbose=1, scoring = score_calc)\ngrid_ridge.fit(X, y)\n\nsc_ridge = get_best_score(grid_ridge)","559cc7b1":"ridge_sc = Ridge()\nparameters = {'alpha':[0.001,0.005,0.01,0.1,0.5,1], 'normalize':[True,False], 'tol':[1e-06,5e-06,1e-05,5e-05]}\ngrid_ridge_sc = GridSearchCV(ridge_sc, parameters, cv=nr_cv, verbose=1, scoring = score_calc)\ngrid_ridge_sc.fit(X_sc, y)\n\nsc_ridge_sc = get_best_score(grid_ridge_sc)","631c0c82":"pred_ridge_all = grid_ridge.predict(X_test)","8700b5b1":"from sklearn.linear_model import Lasso\n\nlasso = Lasso()\nparameters = {'alpha':[1e-03,0.01,0.1,0.5,0.8,1], 'normalize':[True,False], 'tol':[1e-06,1e-05,5e-05,1e-04,5e-04,1e-03]}\ngrid_lasso = GridSearchCV(lasso, parameters, cv=nr_cv, verbose=1, scoring = score_calc)\ngrid_lasso.fit(X, y)\n\nsc_lasso = get_best_score(grid_lasso)\n\npred_lasso = grid_lasso.predict(X_test)","7d4d6eed":"from sklearn.linear_model import ElasticNet\n\nenet = ElasticNet()\nparameters = {'alpha' :[0.1,1.0,10], 'max_iter' :[1000000], 'l1_ratio':[0.04,0.05], \n              'fit_intercept' : [False,True], 'normalize':[True,False], 'tol':[1e-02,1e-03,1e-04]}\ngrid_enet = GridSearchCV(enet, parameters, cv=nr_cv, verbose=1, scoring = score_calc)\ngrid_enet.fit(X_sc, y_sc)\n\nsc_enet = get_best_score(grid_enet)\n\npred_enet = grid_enet.predict(X_test_sc)","c796c29c":"from sklearn.linear_model import SGDRegressor\n\nsgd = SGDRegressor()\nparameters = {'max_iter' :[10000], 'alpha':[1e-05], 'epsilon':[1e-02], 'fit_intercept' : [True]  }\ngrid_sgd = GridSearchCV(sgd, parameters, cv=nr_cv, verbose=1, scoring = score_calc)\ngrid_sgd.fit(X_sc, y_sc)\n\nsc_sgd = get_best_score(grid_sgd)\n\npred_sgd = grid_sgd.predict(X_test_sc)","de0ba04f":"from sklearn.tree import DecisionTreeRegressor\n\nparam_grid = { 'max_depth' : [7,8,9,10] , 'max_features' : [11,12,13,14] ,\n               'max_leaf_nodes' : [None, 12,15,18,20] ,'min_samples_split' : [20,25,30],\n                'presort': [False,True] , 'random_state': [5] }\n            \ngrid_dtree = GridSearchCV(DecisionTreeRegressor(), param_grid, cv=nr_cv, refit=True, verbose=1, scoring = score_calc)\ngrid_dtree.fit(X, y)\n\nsc_dtree = get_best_score(grid_dtree)\n\npred_dtree = grid_dtree.predict(X_test)","67ce8e27":"dtree_pred = grid_dtree.predict(X_test)\nsub_dtree = pd.DataFrame()\nsub_dtree['Id'] = id_test\nsub_dtree['SalePrice'] = dtree_pred\n#sub_dtree.to_csv('dtreeregr.csv',index=False)","ee80e6ec":"from sklearn.ensemble import RandomForestRegressor\n\nparam_grid = {'min_samples_split' : [3,4,6,10], 'n_estimators' : [70,100], 'random_state': [5] }\ngrid_rf = GridSearchCV(RandomForestRegressor(), param_grid, cv=nr_cv, refit=True, verbose=1, scoring = score_calc)\ngrid_rf.fit(X, y)\n\nsc_rf = get_best_score(grid_rf)","c15a8e33":"pred_rf = grid_rf.predict(X_test)\n\nsub_rf = pd.DataFrame()\nsub_rf['Id'] = id_test\nsub_rf['SalePrice'] = pred_rf \n\nif use_logvals == 1:\n    sub_rf['SalePrice'] = np.exp(sub_rf['SalePrice']) \n\nsub_rf.to_csv('rf.csv',index=False)","bf84fadc":"sub_rf.head(10)","636e9f68":"from sklearn.neighbors import KNeighborsRegressor\n\nparam_grid = {'n_neighbors' : [3,4,5,6,7,10,15] ,    \n              'weights' : ['uniform','distance'] ,\n              'algorithm' : ['ball_tree', 'kd_tree', 'brute']}\n\ngrid_knn = GridSearchCV(KNeighborsRegressor(), param_grid, cv=nr_cv, refit=True, verbose=1, scoring = score_calc)\ngrid_knn.fit(X_sc, y_sc)\n\nsc_knn = get_best_score(grid_knn)","507b74d0":"pred_knn = grid_knn.predict(X_test_sc)\n\nsub_knn = pd.DataFrame()\nsub_knn['Id'] = id_test\nsub_knn['SalePrice'] = pred_knn\n\nif use_logvals == 1:\n    sub_knn['SalePrice'] = np.exp(sub_knn['SalePrice']) \n\nsub_knn.to_csv('knn.csv',index=False)","bd7bba1d":"sub_knn.head(10)","8c76a0a3":"from sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import DotProduct, ConstantKernel\n\ngpr = GaussianProcessRegressor(random_state=5, alpha=5e-9, \n                                n_restarts_optimizer=0, \n                               optimizer='fmin_l_bfgs_b', \n                               copy_X_train=True)\n\nparam_grid = {'normalize_y' : [True,False],\n              'kernel' : [DotProduct(), ConstantKernel(1.0, (1e-3, 1e3))] }\n\ngrid_gpr = GridSearchCV(gpr, param_grid, cv=nr_cv, verbose=1, scoring = score_calc)\ngrid_gpr.fit(X_sc, y_sc)\n\nsc_gpr = get_best_score(grid_gpr)","b40ffee9":"pred_gpr = grid_gpr.predict(X_test_sc)\n\nsub_gpr = pd.DataFrame()\nsub_gpr['Id'] = id_test\nsub_gpr['SalePrice'] = pred_gpr\n\nif use_logvals == 1:\n    sub_gpr['SalePrice'] = np.exp(sub_gpr['SalePrice']) \n\nsub_gpr.to_csv('gpr.csv',index=False)","ebcf7415":"list_scores = [sc_linear, sc_ridge, sc_lasso, sc_enet,\n               sc_sgd, sc_dtree, sc_rf, sc_knn, sc_gpr]\nlist_regressors = ['Linear','Ridge','Lasso','ElaNet','SGD','DTr','RF','KNN','GPR']","bf12526b":"fig, ax = plt.subplots()\nfig.set_size_inches(10,7)\nsns.barplot(x=list_regressors, y=list_scores, ax=ax)\nplt.ylabel('RMSE')\nplt.show()","8cce9688":"predictions = {'Linear': pred_linreg_all, 'Ridge': pred_ridge_all, 'Lasso': pred_lasso,\n               'ElaNet': pred_enet, 'SGD': pred_sgd, 'DTr': pred_dtree, 'RF': pred_rf,\n               'KNN': pred_knn, 'GPR': pred_gpr}\ndf_predictions = pd.DataFrame(data=predictions) \ndf_predictions.corr()","3bc51b84":"plt.figure(figsize=(7, 7))\nsns.set(font_scale=1.25)\nsns.heatmap(df_predictions.corr(), linewidths=1.5, annot=True, square=True, \n                fmt='.2f', annot_kws={'size': 10}, \n                yticklabels=df_predictions.columns , xticklabels=df_predictions.columns\n            )\nplt.show()","bf2b6f46":"sub_mean = pd.DataFrame()\nsub_mean['Id'] = id_test\nsub_mean['SalePrice'] = np.round( (pred_lasso + pred_enet + pred_rf + pred_sgd) \/ 4.0 ) \nsub_mean['SalePrice'] = sub_mean['SalePrice'].astype(float)\nsub_mean.to_csv('mean.csv',index=False)","46c63d43":"### Correlation of model results","29269c99":"### StandardScaler","1340d2a6":"### Checking correlation to SalePrice for the new numerical columns","af87bda2":"**Combine train and test data**  \nfor one hot encoding (use pandas get dummies) of all categorical features  \nuncommenting the following cell increases the number of features  \nup to now, all models in Part 3 are optimized for not applying one hot encoder  \nwhen applied, GridSearchCV needs to be rerun","87e1bd5e":"### List of categorical features and their unique values","056d2e6d":"### KNN Regressor","0bfc1ba1":"TODO","0f18418a":"**mean of best models**","345fd498":"**new dataframes**","52527d97":"### log transform\nLike the target variable, also some of the feature values are not normally distributed and it is therefore better to use log values in df_train and df_test. Checking for skewness and kurtosis:","16c8d051":"## **House Prices: EDA to ML (Beginner)**  \n\n**This is my first Kaggle for the House Prices competition.**  \n**It includes the following approaches and techniques:**\n\n* EDA with Pandas and Seaborn\n* Find features with strong correlation to target\n* Data Wrangling, convert categorical to numerical\n* apply the basic Regression models of sklearn \n* use gridsearchCV to find the best parameters for each model\n* compare the performance of the Regressors and choose best one","e3b1458c":"### The target variable : Distribution of SalePrice","6a14aff6":"Numerical columns : drop similar and low correlation\n\nCategorical columns : Transform  to numerical","b7db738d":"## 1.2 Relation of features to target (SalePrice_log)","f34151c5":"**Find columns with strong correlation to target**  \nOnly those with r > min_val_corr are used in the ML Regressors in Part 3  \nThe value for min_val_corr can be chosen in global settings","15a5bf6b":"**Missing values in test data ?**","81fd7771":"### Numerical and Categorical features","5b3fca76":"### shape, info, head and describe","4133a4dd":"There are few columns with quite large correlation to SalePrice (NbHd_num, ExtQ_num, BsQ_num, KiQ_num).  \nThese will probably be useful for optimal performance of the Regressors in part 3.","a2a71057":"For the first five models, the predictions show a very high correlation to each other (very close to 1.00).  \nOnly for Random Forest and Decision Tree, the results  are less correlated with the other Regressors. ","b6b54c0f":"### List of features with missing values","bb456834":"**Conclusion from EDA on numerical columns:**\n\nWe see that for some features like 'OverallQual' there is a strong linear correlation (0.79) to the target.  \nFor other features like 'MSSubClass' the correlation is very weak.  \nFor this kernel I decided to use only those features for prediction that have a correlation larger than a threshold value to SalePrice.  \nThis threshold value can be choosen in the global settings : min_val_corr  \n\nWith the default threshold for min_val_corr = 0.4, these features are dropped in Part 2, Data Wrangling:  \n'Id', 'MSSubClass', 'LotArea', 'OverallCond', 'BsmtFinSF2', 'BsmtUnfSF',  'LowQualFinSF',  'BsmtFullBath', 'BsmtHalfBath', 'HalfBath',   \n'BedroomAbvGr', 'KitchenAbvGr', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal', 'MoSold', 'YrSold'\n\nWe also see that the entries for some of the numerical columns are in fact categorical values.  \nFor example, the numbers for 'OverallQual' and 'MSSubClass' represent a certain group for that feature ( see data description txt)","0df2cb6e":"# Part 2: Data wrangling\n\n**Drop all columns with only small correlation to SalePrice**  \n**Transform Categorical to numerical **  \n**Handling columns with missing data**  \n**Log values**  \n**Drop all columns with strong correlation to similar features**  ","0e395073":"**columns and correlation before dropping**","c0753ae4":"**Load data**","747bd5a1":"**Outliers**","e9453792":"### Plots of relation to target for all numerical features","ac7f641c":"# Part 1: Exploratory Data Analysis","69aefa78":"### RandomForestRegressor","d72ab786":"**For some more advanced approaches on this task inluding Feature Engineering, Pipelines and methods like Stacking, Boosting and Voting have a look at [my second House Prices kernel](https:\/\/www.kaggle.com\/dejavu23\/house-prices-plotly-pipelines-and-ensembles)**","d1ba9c16":"# Part 3: Scikit-learn basic regression models and comparison of results\n\n**Test simple sklearn models and compare by metrics**\n\n**We test the following Regressors from scikit-learn:**  \nLinearRegression  \nRidge  \nLasso  \nElastic Net  \nStochastic Gradient Descent  \nDecisionTreeRegressor  \nRandomForestRegressor  \nSVR ","92e32006":"**List of all features with strong correlation to SalePrice_Log**  \nafter dropping all coumns with weak correlation","fb107bf9":"### GaussianProcessRegressor","83f579af":"**Filling missing values**  \nFor a few columns there is lots of NaN entries.  \nHowever, reading the data description we find this is not missing data:  \nFor PoolQC, NaN is not missing data but means no pool, likewise for Fence, FireplaceQu etc.  ","feb10f50":"**List of features used for the Regressors in Part 3**","b5aa3b3c":"**Missing values in train data ?**","9eb6cf06":"### Comparison plot: RMSE of all models","5f0e3f1e":"**Dropping the converted categorical columns and the new numerical columns with weak correlation**","b67aa636":"As we see, the target variable SalePrice is not normally distributed.  \nThis can reduce the performance of the ML regression models because some assume normal distribution,   \nsee [sklearn info on preprocessing](http:\/\/scikit-learn.org\/stable\/modules\/preprocessing.html)  \nTherfore we make a log transformation, the resulting distribution looks much better.  ","4aa21383":"**columns and correlation after dropping**","438fb566":"### Relation to SalePrice for all categorical features","083660c1":"**When you finished studying this beginner level kernel, continue with [my second House Prices kernel.](https:\/\/www.kaggle.com\/dejavu23\/house-prices-plotly-pipelines-and-ensembles)**  \n**There I explore some more advanced approaches on this task inluding Feature Engineering, Pipelines and methods like Stacking, Boosting and Voting**","7fd4d698":"**Model tuning and selection with GridSearchCV**","43baf418":"### Convert categorical columns to numerical  \nFor those categorcial features where the EDA with boxplots seem to show a strong dependence of the SalePrice on the category, we transform the columns to numerical.\nTo investigate the relation of the categories to SalePrice in more detail, we make violinplots for these features \nAlso, we look at the mean of SalePrice as function of category.","e86bc5ff":"### Ridge","561e1825":"### Correlation matrix 1\n**Features with largest correlation to SalePrice_Log**  \nall numerical features with correlation coefficient above threshold ","948db1c2":"**Of those features with the largest correlation to SalePrice, some also are correlated strongly to each other.**\n\n\n**To avoid failures of the ML regression models due to multicollinearity, these are dropped in part 2.**\n\n\n**This is optional and controlled by the switch drop_similar (global settings)**","c8cf7db5":"### SGDRegressor  \nLinear model fitted by minimizing a regularized empirical loss with SGD. SGD stands for Stochastic Gradient Descent: the gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing strength schedule (aka learning rate). The regularizer is a penalty added to the loss function that shrinks model parameters towards the zero vector using either the squared euclidean norm L2 or the absolute norm L1 or a combination of both (Elastic Net). ","1682dbbe":"## 1.1 Overview of features and relation to target\n\nLet's get a first overview of the train and test dataset  \nHow many rows and columns are there?  \nWhat are the names of the features (columns)?  \nWhich features are numerical, which are categorical?  \nHow many values are missing?  \nThe **shape** and **info** methods answer these questions  \n**head** displays some rows of the dataset  \n**describe** gives a summary of the statistics (only for numerical columns)","450f0cc7":"df train has 81 columns (79 features + id and target SalePrice) and 1460 entries (number of rows or house sales)  \ndf test has 80 columns (79 features + id) and 1459 entries  \nThere is lots of info that is probably related to the SalePrice like the area, the neighborhood, the condition and quality.   \nMaybe other features are not so important for predicting the target, also there might be a strong correlation for some of the features (like GarageCars and GarageArea).\nFor some columns many values are missing: only 7 values for Pool QC in df train and 3 in df test","849532ff":"**Conclusion from EDA on categorical columns:**\n\nFor many of the categorical there is no strong relation to the target.  \nHowever, for some fetaures it is easy to find a strong relation.  \nFrom the figures above these are : 'MSZoning', 'Neighborhood', 'Condition2', 'MasVnrType', 'ExterQual', 'BsmtQual','CentralAir', 'Electrical', 'KitchenQual', 'SaleType'\nAlso for the categorical features, I use only those that show a strong relation to SalePrice. \nSo the other columns are dropped when creating the ML dataframes in Part 2 :  \n 'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope', 'Condition1',  'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl',\n'Exterior1st', 'Exterior2nd', 'ExterCond', 'Foundation', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'Heating', 'HeatingQC', \n'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'PavedDrive', 'PoolQC', 'Fence', 'MiscFeature', 'SaleCondition' \n ","443aee9e":"**References**  \n\n* **[Kaggle: Comprehensive data exploration with Python](https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python)**\n* **[Udemy: Python for Data Science and Machine Learning Bootcamp](https:\/\/www.udemy.com\/python-for-data-science-and-machine-learning-bootcamp\/)**\n* **[Data School: Machine learning in Python with scikit-learn](https:\/\/www.youtube.com\/playlist?list=PL5-da3qGB5ICeMbQuqbbCOQWcS6OYBr5A)**\n\n\nTODO\n\n* Conclusions\n* complete documentation","05bb0ad7":"**The notebook is organized as follows:**\n\n* **[Part 0: Imports, Settings and switches, Global functions](#Part-0-:-Imports,-Settings,-Functions)**  \nimport libraries  \nsettings for number of cross validations  \ndefine functions that are used often\n\n* **[Part 1: Exploratory Data Analysis](#Part-1:-Exploratory-Data-Analysis)**  \n1.1 Get an overview of the features (numerical and categorical) and first look on the target variable SalePrice  \n[shape, info, head and describe](#shape,-info,-head-and-describe)  \n[Distribution of the target variable SalePrice](#The-target-variable-:-Distribution-of-SalePrice)  \n[Numerical and Categorical features](#Numerical-and-Categorical-features)  \n[List of features with missing values](#List-of-features-with-missing-values) and Filling missing values  \n[log transform](#log-transform)  \n1.2 Relation of all features to target SalePrice  \n[Seaborn regression plots for numerical features](#Plots-of-relation-to-target-for-all-numerical-features)  \n[List of numerical features and their correlation coefficient to target](#List-of-numerical-features-and-their-correlation-coefficient-to-target)  \n[Seaborn boxplots for categorical features](#Relation-to-SalePrice-for-all-categorical-features)  \n[List of categorical features and their unique values](#List-of-categorical-features-and-their-unique-values)  \n1.3 Determine the columns that show strong correlation to target  \n[Correlation matrix 1](#Correlation-matrix-1) : all numerical features  \nDetermine features with largest correlation to SalePrice_Log\n\n\n* **[Part 2: Data wrangling](#Part-2:-Data-wrangling)**  \n[Dropping all columns with weak correlation to SalePrice](#Dropping-all-columns-with-weak-correlation-to-SalePrice)  \n[Convert categorical columns to numerical](#Convert-categorical-columns-to-numerical)  \n[Checking correlation to SalePrice for the new numerical columns](#Checking-correlation-to-SalePrice-for-the-new-numerical-columns)  \nuse only features with strong correlation to target  \n[Correlation Matrix 2 (including converted categorical columns)](#Correlation-Matrix-2-:-All-features-with-strong-correlation-to-SalePrice)  \ncreate datasets for ML algorithms  \nOne Hot Encoder  \n[StandardScaler](#StandardScaler)\n\n* **[Part 3: Scikit-learn basic regression models and comparison of results](#Part-3:-Scikit-learn-basic-regression-models-and-comparison-of-results)**  \nimplement GridsearchCV with RMSE metric for Hyperparameter tuning  \nfor these models from sklearn:  \n[Linear Regression](#Linear-Regression)  \n[Ridge](#Ridge)  \n[Lasso](#Lasso)  \n[Elastic Net](#Elastic-Net)  \n[Stochastic Gradient Descent](#SGDRegressor)  \n[DecisionTreeRegressor](#DecisionTreeRegressor)  \n[Random Forest Regressor](#RandomForestRegressor)  \n[KNN Regressor](#KNN-Regressor)  \nbaed on RMSE metric, compare performance of the regressors with their optimized parameters,  \nthen explore correlation of the predictions and make submission with mean of best models  \nComparison plot: [RMSE of all models](#Comparison-plot:-RMSE-of-all-models)  \n[Correlation of model results](#Correlation-of-model-results)  \nMean of best models\n\n\nNote on scores:  \nSubmissions are evaluated on Root-Mean-Squared-Error (RMSE) between the logarithm of the predicted value and the logarithm of the observed sales price. (Taking logs means that errors in predicting expensive houses and cheap houses will affect the result equally.)","b0051ff9":"### Dropping all columns with weak correlation to SalePrice","6866764c":"![](https:\/\/www.reno.gov\/Home\/ShowImage?id=7739&t=635620964226970000)\n\n**Competition Description from Kaggle**  \nWith 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.\n\n**Data description**  \nThis is a detailed description of the 79 features and their entries, quite important for this competition.  \nYou can download the txt file here: [**download**](https:\/\/www.kaggle.com\/c\/5407\/download\/data_description.txt)","749bfb29":"### List of numerical features and their correlation coefficient to target","9ef7b56f":"### Lasso","9c2a5ad7":"**Conclusions**","fabca417":"**Creating Datasets for ML algorithms**","6092d5fb":"**Imports**","f83c0be6":"The performance of all applied Regressors is very similar, except for Decision Tree which has larger RMSE than the other models.","9d0375c6":"**Check for Multicollinearity**\n\nStrong correlation of these features to other, similar features:\n\n'GrLivArea_Log' and 'TotRmsAbvGrd'\n\n'GarageCars' and 'GarageArea'\n\n'TotalBsmtSF' and '1stFlrSF'\n\n'YearBuilt' and 'GarageYrBlt'\n\n**Of those features we drop the one that has smaller correlation coeffiecient to Target.**","d538b787":"# Part 0 : Imports, Settings, Functions","fb287d52":"### Elastic Net","fdf8ce86":"### Linear Regression","2f734428":"### DecisionTreeRegressor","cbfda3d1":"### Correlation Matrix 2 : All features with strong correlation to SalePrice","7925725f":"**Some useful functions**","669416a7":"**Settings and switches**\n\n**Here one can choose settings for optimal performance and runtime.**  \n**For example, nr_cv sets the number of cross validations used in GridsearchCV, and**  \n**min_val_corr is the minimum value for the correlation coefficient to the target (only features with larger correlation will be used).** "}}