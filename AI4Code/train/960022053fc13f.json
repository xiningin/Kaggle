{"cell_type":{"2ae95756":"code","e6445bee":"code","2dbf39d0":"code","2198691e":"code","9df0128d":"code","41f8a4fb":"code","06a44555":"code","d6645940":"code","5e0b7f92":"code","22e1b32e":"code","40dcf646":"code","31384773":"code","5be843bb":"code","056cbed1":"code","e0903666":"code","5e04d1c2":"code","34ea285c":"code","5e4d86ce":"code","d49eb4fe":"code","32bf005c":"code","0a90c584":"code","3dfa4a35":"code","e5081f5f":"code","0c4ac1ef":"code","8378474a":"code","aebf75d3":"code","c6c50d0e":"code","625ffc78":"code","aeeb0ab7":"code","9b381e81":"code","69a0a721":"code","840214f9":"code","7407f95d":"code","f87cb985":"code","88659f44":"code","cb47cb65":"code","a0cec649":"code","51b01de1":"code","bc4e0390":"code","a153f88b":"code","0267e9ca":"code","bced8f13":"code","c0ad6771":"code","7dacd2ce":"code","5e9c14bf":"code","8d51d1a5":"code","4d3f9178":"code","a3d52a6c":"code","59985d24":"code","fe94fbaf":"code","098a7de9":"code","5d85db3c":"code","6b08f0ec":"code","75dfb94e":"code","c2877cdb":"code","22c3a1d5":"code","d10b1957":"code","cb6c4b06":"code","66ad1d4a":"code","070c8d91":"code","26a7ece8":"code","c9c2ae92":"code","b995cf66":"code","e191be61":"code","01a0b87c":"code","fb8537fa":"code","15375873":"code","b756c03c":"code","5b0b166b":"code","cdc56be1":"code","378433a6":"code","bf81a2bc":"code","1510df07":"code","2f304580":"code","ae97f0e7":"code","f33073c7":"code","03ae8cca":"code","80ee59f2":"code","8ff88820":"code","eb5e8e12":"code","1e3a493b":"code","3970cc37":"code","10c0f00a":"code","32cec7af":"code","04903da1":"code","339ffcbd":"code","3087eb79":"code","16e943de":"code","c30e9b63":"code","df2a2f78":"code","eae4a677":"code","9b6b8fc7":"code","460b8f07":"code","4fa1cb22":"code","5bc12e80":"code","31fa77ed":"code","63869825":"code","72a41c2b":"code","6d05aaae":"code","2feb3259":"code","a57ad35a":"code","c32190b3":"code","3b453669":"code","aef9f5ce":"code","2e022942":"code","734a5494":"code","a082adc9":"code","0b7eabe6":"code","dd3e0ccc":"code","f5fdec4b":"code","99f206df":"code","3896f897":"code","5e34df06":"code","2f9d3294":"code","82a88845":"code","c4717f23":"code","baeaa654":"code","74f717dd":"code","7a3d4699":"code","7e6e33f7":"code","a5c43dec":"code","e5ad1ed2":"code","b374bba2":"code","52ec58e4":"code","e7779557":"code","2cdcb57a":"code","90653be6":"code","a2d3329d":"code","8afb6c50":"code","72c2956c":"code","f05a1ab6":"code","be8e0324":"code","77229024":"code","2a08f654":"code","acb38ed4":"code","67b965c8":"code","f2e4a4f1":"code","d26c8d08":"markdown","3276fd36":"markdown","2e422cc6":"markdown","23dd0c98":"markdown","4ae5cc18":"markdown","08eef0e2":"markdown","d20223aa":"markdown","ab80fa95":"markdown","7da33629":"markdown","5938babf":"markdown","59e45c8b":"markdown","2310412b":"markdown","54e88a4f":"markdown","f4be4a98":"markdown","1231f004":"markdown","704530ba":"markdown","5ab67186":"markdown","c352930e":"markdown","40daa274":"markdown","e772bb51":"markdown","4ad47457":"markdown"},"source":{"2ae95756":"# imports\nimport gc\nimport glob\nimport os\nimport json\nimport matplotlib.pyplot as plt\nimport pprint\nimport re\n\nimport numpy as np\nimport pandas as pd\n\nfrom joblib import Parallel, delayed\nfrom tqdm import tqdm_notebook as tqdm\nfrom PIL import Image\nimport cv2\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import SparsePCA, TruncatedSVD, LatentDirichletAllocation, NMF\nfrom sklearn.model_selection import StratifiedKFold,KFold,GroupKFold\nfrom sklearn.metrics import cohen_kappa_score, mean_squared_error\nfrom sklearn.metrics import confusion_matrix as sk_cmatrix\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\n\nimport scipy as sp\n\nfrom collections import Counter\nfrom functools import partial\nfrom math import sqrt\n\nimport lightgbm as lgb\nimport xgboost as xgb\nimport time\n\nfrom gensim.sklearn_api.ldamodel import LdaTransformer\nfrom gensim.models import LdaMulticore\nfrom gensim import corpora\nfrom sklearn.linear_model import Ridge\nfrom sklearn.manifold import TSNE\n\nimport itertools\n\n%matplotlib inline","e6445bee":"from nltk.tokenize import TweetTokenizer\nimport nltk\nisascii = lambda s: len(s) == len(s.encode())\ntknzr = TweetTokenizer()\nimport jieba\nfrom nltk.stem import PorterStemmer\nps = PorterStemmer()\nfrom nltk.stem.lancaster import LancasterStemmer\nlc = LancasterStemmer()\nfrom nltk.stem import SnowballStemmer\nsb = SnowballStemmer(\"english\")","2dbf39d0":"# torch imports\nfrom torch import nn\nimport torch\nfrom torch.nn import functional as F\nfrom torchvision.models import resnet50, resnet34, densenet201, densenet121\nfrom torch.optim.lr_scheduler import StepLR\nfrom torch.autograd import Variable\nfrom torch.utils.data import TensorDataset","2198691e":"# util funcs\ndef confusion_matrix(rater_a, rater_b, min_rating=None, max_rating=None):\n    \"\"\"\n    Returns the confusion matrix between rater's ratings\n    \"\"\"\n    assert(len(rater_a) == len(rater_b))\n    if min_rating is None:\n        min_rating = min(rater_a + rater_b)\n    if max_rating is None:\n        max_rating = max(rater_a + rater_b)\n    num_ratings = int(max_rating - min_rating + 1)\n    conf_mat = [[0 for i in range(num_ratings)]\n                for j in range(num_ratings)]\n    for a, b in zip(rater_a, rater_b):\n        conf_mat[a - min_rating][b - min_rating] += 1\n    return conf_mat\n\ndef histogram(ratings, min_rating=None, max_rating=None):\n    \"\"\"\n    Returns the counts of each type of rating that a rater made\n    \"\"\"\n    if min_rating is None:\n        min_rating = min(ratings)\n    if max_rating is None:\n        max_rating = max(ratings)\n    num_ratings = int(max_rating - min_rating + 1)\n    hist_ratings = [0 for x in range(num_ratings)]\n    for r in ratings:\n        hist_ratings[r - min_rating] += 1\n    return hist_ratings\n\ndef quadratic_weighted_kappa(y, y_pred):\n    min_rating, max_rating =None, None\n    rater_a, rater_b = np.array(y, dtype=int), np.array(y_pred, dtype=int)\n    assert(len(rater_a) == len(rater_b))\n    if min_rating is None:\n        min_rating = min(min(rater_a), min(rater_b))\n    if max_rating is None:\n        max_rating = max(max(rater_a), max(rater_b))\n    conf_mat = confusion_matrix(rater_a, rater_b, min_rating, max_rating)\n    num_ratings = len(conf_mat)\n    num_scored_items = float(len(rater_a))\n\n    hist_rater_a = histogram(rater_a, min_rating, max_rating)\n    hist_rater_b = histogram(rater_b, min_rating, max_rating)\n\n    numerator, denominator = 0.0, 0.0\n\n    for i in range(num_ratings):\n        for j in range(num_ratings):\n            expected_count = (hist_rater_a[i] * hist_rater_b[j] \/ num_scored_items)\n            d = np.square(i - j) \/ np.square(num_ratings - 1)\n            numerator += d * conf_mat[i][j] \/ num_scored_items\n            denominator += d * expected_count \/ num_scored_items\n\n    return (1.0 - numerator \/ denominator)\n\ndef val_kappa(preds, train_data):\n    labels = train_data.get_label()\n    preds = np.argmax(preds.reshape((-1,5)), axis=1)\n    \n    return 'qwk', quadratic_weighted_kappa(labels, preds), True\n\ndef val_kappa_reg(preds, train_data, cdf):\n    labels = train_data.get_label()\n    preds = getTestScore2(preds, cdf)\n    return 'qwk', quadratic_weighted_kappa(labels, preds), True\n\ndef get_cdf(hist):\n    return np.cumsum(hist\/np.sum(hist))\n\ndef getScore(pred, cdf, valid=False):\n    num = pred.shape[0]\n    output = np.asarray([4]*num, dtype=int)\n    rank = pred.argsort()\n    output[rank[:int(num*cdf[0]-1)]] = 0\n    output[rank[int(num*cdf[0]):int(num*cdf[1]-1)]] = 1\n    output[rank[int(num*cdf[1]):int(num*cdf[2]-1)]] = 2\n    output[rank[int(num*cdf[2]):int(num*cdf[3]-1)]] = 3\n    if valid:\n        cutoff = [ pred[rank[int(num*cdf[i]-1)]] for i in range(4) ]\n        return output, cutoff\n    return output\n\ndef getTestScore(pred, cutoff):\n    num = pred.shape[0]\n    output = np.asarray([4]*num, dtype=int)\n    for i in range(num):\n        if pred[i] <= cutoff[0]:\n            output[i] = 0\n        elif pred[i] <= cutoff[1]:\n            output[i] = 1\n        elif pred[i] <= cutoff[2]:\n            output[i] = 2\n        elif pred[i] <= cutoff[3]:\n            output[i] = 3\n    return output\n\ndef getTestScore2(pred, cdf):\n    num = pred.shape[0]\n    rank = pred.argsort()\n    output = np.asarray([4]*num, dtype=int)\n    output[rank[:int(num*cdf[0]-1)]] = 0\n    output[rank[int(num*cdf[0]):int(num*cdf[1]-1)]] = 1\n    output[rank[int(num*cdf[1]):int(num*cdf[2]-1)]] = 2\n    output[rank[int(num*cdf[2]):int(num*cdf[3]-1)]] = 3\n    return output\n\ndef rmse(actual, predicted):\n    return sqrt(mean_squared_error(actual, predicted))\n\nisascii = lambda s: len(s) == len(s.encode())\n\ndef custom_tokenizer(text):\n    init_doc = tknzr.tokenize(text)\n    retval = []\n    for t in init_doc:\n        if isascii(t): \n            retval.append(t)\n        else:\n            for w in t:\n                retval.append(w)\n    return retval\n\ndef build_emb_matrix(word_dict, emb_dict):\n    embed_size = 300\n    nb_words = len(word_dict)+1000\n    nb_oov = 0\n    embedding_matrix = np.zeros((nb_words, embed_size), dtype=np.float32)\n    unknown_vector = np.zeros((embed_size,), dtype=np.float32) - 1.\n    for key in tqdm(word_dict):\n        word = key\n        embedding_vector = emb_dict.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[word_dict[key]] = embedding_vector\n            continue\n        word = key.lower()\n        embedding_vector = emb_dict.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[word_dict[key]] = embedding_vector\n            continue\n        word = key.upper()\n        embedding_vector = emb_dict.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[word_dict[key]] = embedding_vector\n            continue\n        word = key.capitalize()\n        embedding_vector = emb_dict.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[word_dict[key]] = embedding_vector\n            continue\n        word = ps.stem(key)\n        embedding_vector = emb_dict.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[word_dict[key]] = embedding_vector\n            continue\n        word = lc.stem(key)\n        embedding_vector = emb_dict.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[word_dict[key]] = embedding_vector\n            continue\n        word = sb.stem(key)\n        embedding_vector = emb_dict.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[word_dict[key]] = embedding_vector\n            continue\n        nb_oov+=1\n        embedding_matrix[word_dict[key]] = unknown_vector                    \n    return embedding_matrix, nb_words, nb_oov\n\ndef _init_esim_weights(module):\n    if isinstance(module, nn.Linear):\n        nn.init.kaiming_normal_(module.weight.data)\n        nn.init.constant_(module.bias.data, 0.0)\n\n    elif isinstance(module, nn.LSTM) or isinstance(module, nn.GRU):\n        if isinstance(module, nn.LSTM):\n            hidden_size = module.bias_hh_l0.data.shape[0] \/\/ 4\n        else:\n            hidden_size = module.bias_hh_l0.data.shape[0] \/\/ 3\n        for name, param in module.named_parameters():\n            if 'bias' in name:\n                nn.init.constant_(param, 0.0)\n            elif '_ih_' in name:\n                nn.init.xavier_normal_(param)\n            elif '_hh_' in name:\n                nn.init.orthogonal_(param)\n                param.data[hidden_size:(2 * hidden_size)] = 1.0","9df0128d":"class OptimizedRounder(object):\n    def __init__(self):\n        self.coef_ = 0\n    \n    def _kappa_loss(self, coef, X, y):\n        preds = pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3, 4])\n        return -cohen_kappa_score(y, preds, weights='quadratic')\n    \n    def fit(self, X, y):\n        loss_partial = partial(self._kappa_loss, X = X, y = y)\n        initial_coef = [0.5, 1.5, 2.5, 3.5]\n        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')\n    \n    def predict(self, X, coef):\n        preds = pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3, 4])\n        return preds\n    \n    def coefficients(self):\n        return self.coef_['x']","41f8a4fb":"class ResnetModel(nn.Module):\n    def __init__(self, resnet_fun = resnet50, freeze_basenet = True):\n        super(ResnetModel, self).__init__()\n        self.resnet = resnet_fun(pretrained=False)\n        if freeze_basenet:\n            for p in self.resnet.parameters():\n                p.requires_grad = False\n       \n    def init_resnet(self, path):\n        state = torch.load(path)\n        self.resnet.load_state_dict(state)\n        \n    def forward(self, x):\n        batch_size = x.shape[0]\n        x = x\/255.0\n        mean = [0.485, 0.456, 0.406]\n        std = [0.229, 0.224, 0.225]\n        x = torch.cat([\n            (x[:, [0]] - mean[0]) \/ std[0],\n            (x[:, [1]] - mean[1]) \/ std[1],\n            (x[:, [2]] - mean[2]) \/ std[2],\n        ], 1)\n        x = self.resnet.conv1(x)\n        x = self.resnet.bn1(x)\n        x = self.resnet.relu(x)\n        x = self.resnet.maxpool(x)\n        x = self.resnet.layer1(x)\n        x = self.resnet.layer2(x)\n        x = self.resnet.layer3(x)\n        x = self.resnet.layer4(x)\n        x = F.adaptive_avg_pool2d(x, output_size=1).view(batch_size, -1)\n        return x\n    \nclass DenseModel(nn.Module):\n    def __init__(self, dense_func = densenet201, freeze_basenet = True):\n        super(DenseModel, self).__init__()\n        self.densenet = dense_func(pretrained=False)\n#         model_name = 'se_resnet50'\n#         self.resnet = pretrainedmodels.__dict__[model_name](num_classes=1000, pretrained=None)\n        if freeze_basenet:\n            for p in self.densenet.parameters():\n                p.requires_grad = False\n        \n    def init_densenet(self, path):\n        state = torch.load(path)\n        self.densenet.load_state_dict(state)\n        \n    def forward(self, x):\n        batch_size = x.shape[0]\n        x = x\/255.0\n        mean = [0.485, 0.456, 0.406]\n        std = [0.229, 0.224, 0.225]\n        x = torch.cat([\n            (x[:, [0]] - mean[0]) \/ std[0],\n            (x[:, [1]] - mean[1]) \/ std[1],\n            (x[:, [2]] - mean[2]) \/ std[2],\n        ], 1)\n        x = self.densenet.features(x)\n        x = F.adaptive_avg_pool2d(F.relu(x,inplace=True), output_size=1).view(batch_size, -1)\n        return x\n","06a44555":"from torch.utils.data import Dataset, DataLoader\n\ndef img_to_torch(image):\n    return torch.from_numpy(np.transpose(image, (2, 0, 1)))\n\ndef pad_to_square(image):\n    h, w = image.shape[0:2]\n    new_size = max(h, w)\n    delta_top = (new_size-h)\/\/2\n    delta_bottom = new_size-h-delta_top\n    delta_left = (new_size-w)\/\/2\n    delta_right = new_size-delta_left-w\n    new_im = cv2.copyMakeBorder(image, delta_top, delta_bottom, delta_left, delta_right, \n                                cv2.BORDER_CONSTANT,  value=[0,0,0])\n    return new_im\n\nclass PetDataset(Dataset):\n    def __init__(self, df):\n        self.df = df\n        self.path = df['path'].tolist()\n        self.cache = {}\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        if index not in range(0, len(self.df)):\n            return self.__getitem__(np.random.randint(0, self.__len__()))\n        \n        # only take on channel\n#         if index not in self.cache:\n        image = cv2.imread(self.path[index])\n        image = pad_to_square(image)\n        image = cv2.resize(image, (IMG_SIZE, IMG_SIZE))\n#             self.cache[index] = img_to_torch(image)\n\n        return img_to_torch(image)","d6645940":"os.listdir('..\/input\/')","5e0b7f92":"labels_state = pd.read_csv('..\/input\/petfinder-adoption-prediction\/color_labels.csv')\nlabels_color = pd.read_csv('..\/input\/petfinder-adoption-prediction\/state_labels.csv')","22e1b32e":"# read data\ntrain = pd.read_csv('..\/input\/petfinder-adoption-prediction\/train\/train.csv')\ntest = pd.read_csv('..\/input\/petfinder-adoption-prediction\/test\/test.csv')\nsample_submission = pd.read_csv('..\/input\/petfinder-adoption-prediction\/test\/sample_submission.csv')\ntrain_len = len(train)\ndata_df = pd.concat([train, test], sort=False).reset_index(drop=True)\ndata_df['Breed_full'] = data_df['Breed1'].astype(str)+'_'+data_df['Breed2'].astype(str)\ndata_df['Color_full'] = data_df['Color1'].astype(str)+'_'+data_df['Color2'].astype(str)+'_'+data_df['Color3'].astype(str)\n\ndata_df['Breed_full'],_ = pd.factorize(data_df['Breed_full'])\ndata_df['Color_full'],_ = pd.factorize(data_df['Color_full'])\ndata_df['State'],_ = pd.factorize(data_df['State'])\n\ndata_df['hard_interaction'] = data_df['Type'].astype(str)+data_df['Gender'].astype(str)+ \\\n                              data_df['Vaccinated'].astype(str)+'_'+ \\\n                              data_df['Dewormed'].astype(str)+'_'+data_df['Sterilized'].astype(str)\ndata_df['hard_interaction'],_ = pd.factorize(data_df['hard_interaction'])\n\ndata_df['MaturitySize'] = data_df['MaturitySize'].replace(0, np.nan)\ndata_df['FurLength'] = data_df['FurLength'].replace(0, np.nan)\n\ndata_df['Vaccinated'] = data_df['Vaccinated'].replace(3, np.nan)\ndata_df['Vaccinated'] = data_df['Vaccinated'].replace(2, 0)\n\ndata_df['Dewormed'] = data_df['Dewormed'].replace(3, np.nan)\ndata_df['Dewormed'] = data_df['Dewormed'].replace(2, 0)\n\ndata_df['Sterilized'] = data_df['Sterilized'].replace(3, np.nan)\ndata_df['Sterilized'] = data_df['Sterilized'].replace(2, 0)\n\n\ndata_df['Health'] = data_df['Health'].replace(0, np.nan)\ndata_df['age_in_year'] = (data_df['Age']\/\/12).astype(np.int8)\ndata_df['avg_fee'] = data_df['Fee']\/data_df['Quantity']\ndata_df['avg_photo'] = data_df['PhotoAmt']\/data_df['Quantity']\n\n# name feature\npattern = re.compile(r\"[0-9\\.:!]\")\ndata_df['empty_name'] = data_df['Name'].isnull().astype(np.int8)\ndata_df['Name'] =data_df['Name'].fillna('')\ndata_df['name_len'] = data_df['Name'].apply(lambda x: len(x))\ndata_df['strange_name'] = data_df['Name'].apply(lambda x: len(pattern.findall(x))>0).astype(np.int8)","40dcf646":"data_df['color_num'] = 1\ndata_df['color_num'] += data_df['Color2'].apply(lambda x: 1 if x!=0 else 0)\ndata_df['color_num'] += data_df['Color3'].apply(lambda x: 1 if x!=0 else 0)","31384773":"# breed feature\nlabels_breed = pd.read_csv('..\/input\/petfinder-adoption-prediction\/breed_labels.csv')\nlabels_breed.rename(index=str, columns={'BreedName':'Breed1Name'},inplace=True)\nlabels_breed['Breed2Name'] = labels_breed['Breed1Name'].values","5be843bb":"data_df = data_df.merge(labels_breed[['BreedID','Breed1Name']], left_on='Breed1', right_on='BreedID', how='left')\ndata_df.drop('BreedID',axis=1,inplace=True)\ndata_df = data_df.merge(labels_breed[['BreedID','Breed2Name']], left_on='Breed2', right_on='BreedID', how='left')\ndata_df.drop('BreedID',axis=1,inplace=True)\ndata_df['Breed2Name'].fillna('',inplace=True)\ndata_df['BreedName_full'] = data_df['Breed1Name']+' '+data_df['Breed2Name']","056cbed1":"data_df['breed_noname'] = data_df['BreedName_full'].isnull().astype(np.int8)\ndata_df['BreedName_full'].fillna('',inplace=True)\ndata_df['BreedName_full'] = data_df['BreedName_full'].str.lower()","e0903666":"data_df['breed_num'] = 1\ndata_df['breed_num'] += data_df['Breed2'].apply(lambda x: 1 if x!=0 else 0)\ndata_df['breed_mixed'] = data_df['BreedName_full'].apply(lambda x: x.find('mixed')>=0).astype(np.int8)\ndata_df['breed_Domestic'] = data_df['BreedName_full'].apply(lambda x: x.find('domestic')>=0).astype(np.int8)\ndata_df['pure_breed'] = ((data_df['breed_num']==1)&(data_df['breed_mixed']==0)).astype(np.int8)","5e04d1c2":"# \n# data_df['breed_American'] = data_df['BreedName_full'].apply(lambda x: x.find('american')>=0).astype(np.int8)\n# data_df['breed_Australian'] = data_df['BreedName_full'].apply(lambda x: x.find('australian')>=0).astype(np.int8)\n# data_df['breed_Belgian'] = data_df['BreedName_full'].apply(lambda x: x.find('belgian')>=0).astype(np.int8)\n# data_df['breed_English'] = data_df['BreedName_full'].apply(lambda x: x.find('english')>=0).astype(np.int8)\n# data_df['breed_German'] = data_df['BreedName_full'].apply(lambda x: x.find('german')>=0).astype(np.int8)\n# data_df['breed_irish'] = data_df['BreedName_full'].apply(lambda x: x.find('irish')>=0).astype(np.int8)\n# \n# data_df['breed_Oriental'] = data_df['BreedName_full'].apply(lambda x: x.find('Oriental')>=0).astype(np.int8)","34ea285c":"import json\n# add features from ratings \nwith open('..\/input\/cat-and-dog-breeds-parameters\/rating.json', 'r') as f:\n    ratings = json.load(f)","5e4d86ce":"cat_ratings = ratings['cat_breeds']\ndog_ratings = ratings['dog_breeds']\ncatdog_ratings = {**cat_ratings, **dog_ratings} ","d49eb4fe":"parameters_df=pd.DataFrame()\ni = 0\nfor breed in catdog_ratings.keys():\n    for key in catdog_ratings[breed].keys():\n        parameters_df.at[i,'breed'] = breed\n        parameters_df.at[i,key] = catdog_ratings[breed][key] \n    i = i+1","32bf005c":"parameters_df.rename(index=str, columns={'breed':'Breed1Name'},inplace=True)","0a90c584":"data_df = data_df.merge(parameters_df[['Breed1Name','Affectionate with Family','Amount of Shedding',\n                                      'Easy to Groom','General Health','Intelligence','Kid Friendly',\n                                      'Pet Friendly','Potential for Playfulness']], on='Breed1Name', how='left')","3dfa4a35":"data_df['Description'] = data_df['Description'].fillna(' ')","e5081f5f":"english_desc, chinese_desc = [], []\ntokens = set()\nword_dict = {}\npos_count, word_count = 1, 1 # starts from 1, 0 for padding token\npos_dict = {}\neng_sequences = []\npos_sequences = []\nfor i in range(len(data_df)):\n    e_d, c_d, eng_seq, pos_seq = [], [], [], []\n    doc = custom_tokenizer(data_df['Description'].iloc[i])\n    for token in doc:\n        if not isascii(token):\n            c_d.append(token)\n        else:\n            e_d.append(token)\n            if token not in word_dict:\n                word_dict[token] = word_count\n                word_count +=1\n    english_desc.append(' '.join(e_d))\n    chinese_desc.append(' '.join(c_d))\n    pos_seq = nltk.pos_tag(e_d)\n    for t in pos_seq:\n        if t[1] not in pos_dict:\n            pos_dict[t[1]] = pos_count\n            pos_count += 1\n    pos_seq = [pos_dict[t[1]] for t in pos_seq]\n    eng_seq = [word_dict[t] for t in e_d]\n    if len(eng_seq)==0:\n        eng_seq.append(0)\n        pos_seq.append(0)\n    eng_sequences.append(eng_seq)\n    pos_sequences.append(pos_seq)","0c4ac1ef":"data_df['English_desc'] = english_desc\ndata_df['Chinese_desc'] = chinese_desc\n\ndata_df['e_description_len'] = data_df['English_desc'].apply(lambda x:len(x))\ndata_df['e_description_word_len'] = data_df['English_desc'].apply(lambda x: len(x.split(' ')))\ndata_df['e_description_word_unique'] = data_df['English_desc'].apply(lambda x: len(set(x.split(' '))))\n\ndata_df['c_description_len'] = data_df['Chinese_desc'].apply(lambda x:len(x))\ndata_df['c_description_word_len'] = data_df['Chinese_desc'].apply(lambda x:len(x.split(' ')))\ndata_df['c_description_word_unique'] = data_df['Chinese_desc'].apply(lambda x: len(set(x)))\n\ndata_df['description_len'] = data_df['Description'].apply(lambda x:len(x))\ndata_df['description_word_len'] = data_df['Description'].apply(lambda x: len(x.split(' ')))","8378474a":"print(len(eng_sequences))\nprint(len(pos_sequences))","aebf75d3":"nb_pos = len(pos_dict)\nprint(nb_pos)","c6c50d0e":"# build embedding\ndef load_glove():\n    EMBEDDING_FILE = '..\/input\/glove840b300dtxt\/glove.840B.300d.txt'\n\n    def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n\n    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in (open(EMBEDDING_FILE)))\n    return embeddings_index\n\nglove_emb = load_glove()\n\nembedding_matrix, nb_words, nb_oov = build_emb_matrix(word_dict, glove_emb)\nprint(nb_words, nb_oov)\ndel glove_emb\ngc.collect()","625ffc78":"# np.save('mini_embedding.npy',embedding_matrix)","aeeb0ab7":"# embedding_matrix = np.load('mini_embedding.npy')\n# nb_words = 33947","9b381e81":"len(set(train.index.tolist()))","69a0a721":"n_splits = 5\n# kfold = GroupKFold(n_splits=n_splits)\nsplit_index = []\n# for train_idx, valid_idx in kfold.split(train, train['AdoptionSpeed'], train['RescuerID']):\n#     split_index.append((train_idx, valid_idx))\n\nkfold = StratifiedKFold(n_splits=n_splits, random_state=1991)\nfor train_idx, valid_idx in kfold.split(train, train['AdoptionSpeed']):\n    split_index.append((train_idx, valid_idx))","840214f9":"train_image_files = sorted(glob.glob('..\/input\/petfinder-adoption-prediction\/train_images\/*.jpg'))\ntrain_metadata_files = sorted(glob.glob('..\/input\/petfinder-adoption-prediction\/train_metadata\/*.json'))\ntrain_sentiment_files = sorted(glob.glob('..\/input\/petfinder-adoption-prediction\/train_sentiment\/*.json'))\n\nprint('num of train images files: {}'.format(len(train_image_files)))\nprint('num of train metadata files: {}'.format(len(train_metadata_files)))\nprint('num of train sentiment files: {}'.format(len(train_sentiment_files)))\n\ntest_image_files = sorted(glob.glob('..\/input\/petfinder-adoption-prediction\/test_images\/*.jpg'))\ntest_metadata_files = sorted(glob.glob('..\/input\/petfinder-adoption-prediction\/test_metadata\/*.json'))\ntest_sentiment_files = sorted(glob.glob('..\/input\/petfinder-adoption-prediction\/test_sentiment\/*.json'))\n\nprint('num of test images files: {}'.format(len(test_image_files)))\nprint('num of test metadata files: {}'.format(len(test_metadata_files)))\nprint('num of test sentiment files: {}'.format(len(test_sentiment_files)))\n\nimage_files = train_image_files+test_image_files\nmetadata_files = train_metadata_files+test_metadata_files\nsentiment_files = train_sentiment_files+test_sentiment_files","7407f95d":"IMG_SIZE = 256\nBATCH_SIZE = 256\n\ndef get_petid(path):\n    basename = os.path.basename(path)\n    return basename.split('-')[0]\ndef get_picid(path):\n    basename = os.path.splitext(os.path.basename(path))[0]\n    return basename.split('-')[1]\n\nimage_df = pd.DataFrame(image_files, columns=['path'])\nimage_df['PetID'] = image_df['path'].apply(get_petid)\nimage_df['PicID'] = image_df['path'].apply(get_picid)\n\ngc.collect()","f87cb985":"image_df[image_df['PicID']=='1'].shape","88659f44":"pet_image_dataset = PetDataset(image_df)\npet_image_loader = DataLoader(pet_image_dataset, batch_size=BATCH_SIZE, shuffle=False, \n                       num_workers=1, pin_memory=True)","cb47cb65":"# cat and dog model\nresnet34_feature = []\nmodel = ResnetModel(resnet_fun=resnet34)\nmodel.init_resnet('..\/input\/pretrained-pytorch-dog-and-cat-models\/resnet34.pth')\nmodel.cuda()\nmodel.eval()\nwith torch.no_grad():\n    for img_batch in tqdm(pet_image_loader):\n        img_batch = img_batch.float().cuda()\n        y_pred = model(img_batch)\n        resnet34_feature.append(y_pred.cpu().numpy()) \nresnet34_feature = np.vstack(resnet34_feature)\ndel model\ngc.collect()\ntorch.cuda.empty_cache()\n\ndensenet121_feature = []\nmodel = DenseModel(dense_func=densenet121)\nmodel.init_densenet('..\/input\/pretrained-pytorch-dog-and-cat-models\/densenet121.pth')\nmodel.cuda()\nmodel.eval()\nwith torch.no_grad():\n    for img_batch in tqdm(pet_image_loader):\n        img_batch = img_batch.float().cuda()\n        y_pred = model(img_batch)\n        densenet121_feature.append(y_pred.cpu().numpy()) \ndensenet121_feature = np.vstack(densenet121_feature)\ndel model\n# del pet_image_loader\n# del pet_image_dataset\ngc.collect()\ntorch.cuda.empty_cache()","a0cec649":"densenet_feature = []\nmodel = DenseModel()\nmodel.init_densenet('..\/input\/pytorch-pretrained-image-models\/densenet201.pth')\nmodel.cuda()\nmodel.eval()\nwith torch.no_grad():\n    for img_batch in tqdm(pet_image_loader):\n        img_batch = img_batch.float().cuda()\n        y_pred = model(img_batch)\n        densenet_feature.append(y_pred.cpu().numpy()) \ndensenet_feature = np.vstack(densenet_feature)\n\ndel model\ngc.collect()\ntorch.cuda.empty_cache()\n\nresnet50_feature = []\nmodel = ResnetModel()\nmodel.init_resnet('..\/input\/pytorch-pretrained-image-models\/resnet50.pth')\nmodel.cuda()\nmodel.eval()\nwith torch.no_grad():\n    for img_batch in tqdm(pet_image_loader):\n        img_batch = img_batch.float().cuda()\n        y_pred = model(img_batch)\n        resnet50_feature.append(y_pred.cpu().numpy()) \nresnet50_feature = np.vstack(resnet50_feature)","51b01de1":"del img_batch","bc4e0390":"del model\ndel pet_image_loader\ndel pet_image_dataset\ngc.collect()\ntorch.cuda.empty_cache()","a153f88b":"RES50_IMG_FEATURE_DIM = resnet50_feature.shape[1]\nDENSE_IMG_FEATURE_DIM = densenet_feature.shape[1]\nRES34_IMG_FEATURE_DIM = resnet34_feature.shape[1]\nDENSE121_IMG_FEATURE_DIM = densenet121_feature.shape[1]\nprint(RES50_IMG_FEATURE_DIM)\nprint(DENSE_IMG_FEATURE_DIM)\nprint(RES34_IMG_FEATURE_DIM)\nprint(DENSE121_IMG_FEATURE_DIM)","0267e9ca":"resnet50_feature_df = pd.DataFrame(resnet50_feature, dtype=np.float32,\n                                   columns=['resnet50_%d'%i for i in range(RES50_IMG_FEATURE_DIM)])\nresnet50_feature_df['PetID'] = image_df['PetID'].values\nresnet50_feature_df['PicID'] = image_df['PicID'].values\nresnet50_feature_df_avg = resnet50_feature_df.drop('PicID', axis=1).groupby('PetID').agg('mean').reset_index()\nresnet50_feature_df_avg.columns = ['PetID']+['resnet50_mean_%d'%i for i in range(RES50_IMG_FEATURE_DIM)]\nresnet50_feature_df_1 = resnet50_feature_df[resnet50_feature_df['PicID']=='1'].drop('PicID', axis=1)\nresnet50_feature_df = resnet50_feature_df_1.merge(resnet50_feature_df_avg, on='PetID', how='outer')\nresnet50_feature_df = data_df[['PetID','RescuerID','AdoptionSpeed']].merge(resnet50_feature_df, on='PetID', how='left')\nfor c in resnet50_feature_df.columns:\n    if c in ['PetID','RescuerID','AdoptionSpeed']:continue\n    resnet50_feature_df[c] = resnet50_feature_df[c].fillna(-1)\ndel resnet50_feature_df_avg\ndel resnet50_feature_df_1\ngc.collect()","bced8f13":"dense_feature_df = pd.DataFrame(densenet_feature, dtype=np.float32,\n                                   columns=['densenet_%d'%i for i in range(DENSE_IMG_FEATURE_DIM)])\ndense_feature_df['PetID'] = image_df['PetID'].values\ndense_feature_df['PicID'] = image_df['PicID'].values\ndense_feature_df_avg = dense_feature_df.drop('PicID', axis=1).groupby('PetID').agg('mean').reset_index()\ndense_feature_df_avg.columns = ['PetID']+['densenet_mean_%d'%i for i in range(DENSE_IMG_FEATURE_DIM)]\ndense_feature_df_1 = dense_feature_df[dense_feature_df['PicID']=='1'].drop('PicID', axis=1)\ndense_feature_df = dense_feature_df_1.merge(dense_feature_df_avg, on='PetID', how='left')\ndense_feature_df = data_df[['PetID','RescuerID','AdoptionSpeed']].merge(dense_feature_df, on='PetID', how='left')\nfor c in dense_feature_df.columns:\n    if c in ['PetID','RescuerID','AdoptionSpeed']:continue\n    dense_feature_df[c] = dense_feature_df[c].fillna(-1)\ndel dense_feature_df_1\ndel dense_feature_df_avg\ngc.collect()","c0ad6771":"# res34 on pet and dogs\nres34_feature_df = pd.DataFrame(resnet34_feature, dtype=np.float32,\n                                   columns=['densenet_%d'%i for i in range(RES34_IMG_FEATURE_DIM)])\nres34_feature_df['PetID'] = image_df['PetID'].values\nres34_feature_df['PicID'] = image_df['PicID'].values\nres34_feature_df_avg = res34_feature_df.drop('PicID', axis=1).groupby('PetID').agg('mean').reset_index()\nres34_feature_df_avg.columns = ['PetID']+['res34_mean_%d'%i for i in range(RES34_IMG_FEATURE_DIM)]\nres34_feature_df_1 = res34_feature_df[res34_feature_df['PicID']=='1'].drop('PicID', axis=1)\nres34_feature_df = res34_feature_df_1.merge(res34_feature_df_avg, on='PetID', how='left')\nres34_feature_df = data_df[['PetID','RescuerID','AdoptionSpeed']].merge(res34_feature_df, on='PetID', how='left')\nfor c in res34_feature_df.columns:\n    if c in ['PetID','RescuerID','AdoptionSpeed']:continue\n    res34_feature_df[c] = res34_feature_df[c].fillna(-1)\ndel res34_feature_df_1\ndel res34_feature_df_avg\ngc.collect()","7dacd2ce":"# densenet121 on pet and dogs\ndense121_feature_df = pd.DataFrame(densenet121_feature, dtype=np.float32,\n                                   columns=['densenet_%d'%i for i in range(DENSE121_IMG_FEATURE_DIM)])\ndense121_feature_df['PetID'] = image_df['PetID'].values\ndense121_feature_df['PicID'] = image_df['PicID'].values\ndense121_feature_df_avg = dense121_feature_df.drop('PicID', axis=1).groupby('PetID').agg('mean').reset_index()\ndense121_feature_df_avg.columns = ['PetID']+['dense121_mean_%d'%i for i in range(DENSE121_IMG_FEATURE_DIM)]\ndense121_feature_df_1 = dense121_feature_df[dense121_feature_df['PicID']=='1'].drop('PicID', axis=1)\ndense121_feature_df = dense121_feature_df_1.merge(dense121_feature_df_avg, on='PetID', how='left')\ndense121_feature_df = data_df[['PetID','RescuerID','AdoptionSpeed']].merge(dense121_feature_df, on='PetID', how='left')\nfor c in dense121_feature_df.columns:\n    if c in ['PetID','RescuerID','AdoptionSpeed']:continue\n    dense121_feature_df[c] = dense121_feature_df[c].fillna(-1)\ndel dense121_feature_df_1\ndel dense121_feature_df_avg\ngc.collect()","5e9c14bf":"class ImgExtractor(nn.Module):\n    def __init__(self, raw_feature_dim = 4096, feature_dim=64):\n        super(ImgExtractor, self).__init__()\n        self.extractor = nn.Sequential(\n            nn.BatchNorm1d(raw_feature_dim),\n            nn.Dropout(0.5),\n            nn.Linear(raw_feature_dim, raw_feature_dim\/\/4),\n            nn.ELU(inplace=True),\n#             nn.BatchNorm1d(raw_feature_dim\/\/4),\n            nn.Dropout(0.1),\n            nn.Linear(raw_feature_dim\/\/4, raw_feature_dim\/\/8),\n            nn.ELU(inplace=True),\n#             nn.Dropout(0.1),\n#             nn.BatchNorm1d(raw_feature_dim\/\/8),\n            nn.Linear(raw_feature_dim\/\/8, feature_dim),\n            nn.ELU(inplace=True)\n        )\n        self.logit = nn.Sequential(\n            nn.Linear(feature_dim, 1)\n        )\n        \n#         self.apply(_init_esim_weights)\n        \n    def forward(self, x):\n        batch_size = x.shape[0]\n        feat = self.extractor(x)\n        out = self.logit(feat)\n        return out, feat","8d51d1a5":"IMG_FEATURE_DIM_NN = 128\nIMG_FEATURE_DIM_NN2 = 64\n\ndef get_image_feature(img_train_data, img_test_data,img_feature_dim):\n    loss_fn = torch.nn.MSELoss().cuda()\n\n    oof_train_img = np.zeros((img_train_data.shape[0], img_feature_dim+1))\n    oof_test_img = []\n\n    # X_test = raw_img_features_test.drop(['PetID','RescuerID','AdoptionSpeed'], axis=1)\n    test_dataset = TensorDataset(torch.tensor(img_test_data))\n    test_loader = DataLoader(test_dataset, batch_size=1024, shuffle=False, num_workers=1, pin_memory=True)\n\n    qwks = []\n    rmses = []\n\n    for n_fold, (train_idx, valid_idx) in enumerate(split_index): \n        print('fold:',n_fold)\n        X_tr = img_train_data[train_idx]\n        X_val = img_train_data[valid_idx]\n\n        y_tr = train.iloc[train_idx]['AdoptionSpeed'].values    \n        y_val = train.iloc[valid_idx]['AdoptionSpeed'].values\n\n        hist = histogram(y_tr.astype(int), \n                         int(np.min(train['AdoptionSpeed'])), \n                         int(np.max(train['AdoptionSpeed'])))\n        tr_cdf = get_cdf(hist)\n\n        tra_dataset = TensorDataset(torch.tensor(X_tr), torch.tensor(y_tr))\n        val_dataset = TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n\n        training_loader = DataLoader(tra_dataset, batch_size=1024, shuffle=True, num_workers=1, pin_memory=True)\n        validation_loader = DataLoader(val_dataset, batch_size=1024, shuffle=False, num_workers=1, pin_memory=True)\n\n        model = ImgExtractor(raw_feature_dim=img_train_data.shape[1], feature_dim=img_feature_dim)\n        model.cuda()\n\n        optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.002)\n        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=7, eta_min=0.0003)\n        iteration = 0\n        min_val_loss = 100\n        since = time.time()\n        test_feats = None\n        for epoch in range(15):\n            scheduler.step()\n            model.train()\n            for x, y in training_loader:\n                iteration += 1\n                x = x.cuda()\n                y = y.type(torch.FloatTensor).cuda().view(-1, 1)\n\n                pred, feat = model(x)\n                loss = loss_fn(pred, y)\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n\n            model.eval()\n            val_predicts = []\n            val_feats = []\n            with torch.no_grad():\n                for x, y in validation_loader:\n                    x = x.cuda()\n                    y = y.type(torch.FloatTensor).cuda()#.view(-1, 1)\n                    v_pred, feat = model(x)\n                    val_predicts.append(v_pred.cpu().numpy())\n                    val_feats.append(feat.cpu().numpy())\n\n            val_predicts = np.concatenate(val_predicts) \n            val_feats = np.vstack(val_feats)\n            pred_test_y_k = getTestScore2(val_predicts.flatten(), tr_cdf)\n            qwk = quadratic_weighted_kappa(y_val, pred_test_y_k)\n            val_loss = rmse(y_val,val_predicts)\n\n            if val_loss<min_val_loss:\n                min_val_loss = val_loss\n                oof_train_img[valid_idx,:] = np.hstack([val_feats,val_predicts])\n                test_feats = []\n                test_preds = []\n                with torch.no_grad():\n                    for x, in test_loader:\n                        x = x.cuda()\n                        v_pred, feat = model(x)\n                        test_preds.append(v_pred.cpu().numpy())\n                        test_feats.append(feat.cpu().numpy())\n                test_feats = np.hstack([np.vstack(test_feats), np.concatenate(test_preds)])\n                print(epoch, \"best loss! val loss:\", val_loss, 'qwk:', qwk, \"elapsed time:\", time.time()-since)\n        oof_test_img.append(test_feats)\n        rmses.append(min_val_loss)\n        qwks.append(qwk)\n        del model\n        del x\n        del y\n        del tra_dataset\n        del val_dataset\n        del training_loader\n        del validation_loader\n        gc.collect()\n        torch.cuda.empty_cache()\n    print('overall rmse: %.5f'%rmse(oof_train_img[:,-1], train['AdoptionSpeed']))\n    print('mean rmse =', np.mean(rmses), 'rmse std =', np.std(rmses))\n    print('mean QWK =', np.mean(qwks), 'std QWK =', np.std(qwks))\n    del test_loader\n    del test_dataset\n    gc.collect()\n    return oof_train_img, np.mean(oof_test_img, axis=0)","4d3f9178":"res34_array = res34_feature_df.drop(['PetID','RescuerID','AdoptionSpeed'], axis=1).values\n# raw_img_array  = stdscaler.fit_transform(raw_img_array)\nres34_tra = res34_array[0:train_len]\nres34_test = res34_array[train_len:]\noof_train_res34, oof_test_res34 = get_image_feature(res34_tra, res34_test, IMG_FEATURE_DIM_NN2)\ndel res34_tra\ndel res34_test\ngc.collect()\ndnn_resnet34_features = np.vstack([oof_train_res34, oof_test_res34])\ndnn_resnet34_features = pd.DataFrame(dnn_resnet34_features, columns=['res34_%d'%c for c in range(dnn_resnet34_features.shape[1])])\ndnn_resnet34_features['PetID'] = data_df['PetID'].values\ndel oof_train_res34\ndel oof_test_res34\ngc.collect()","a3d52a6c":"stdscaler = RobustScaler()\nres50_array = resnet50_feature_df.drop(['PetID','RescuerID','AdoptionSpeed'], axis=1).values\n# raw_img_array  = stdscaler.fit_transform(raw_img_array)\nres50_tra = res50_array[0:train_len]\nres50_test = res50_array[train_len:]\noof_train_res, oof_test_res = get_image_feature(res50_tra, res50_test, IMG_FEATURE_DIM_NN)\ndel res50_tra\ndel res50_test\ngc.collect()\ndnn_resnet50_features = np.vstack([oof_train_res, oof_test_res])\ndnn_resnet50_features = pd.DataFrame(dnn_resnet50_features, columns=['resnet_%d'%c for c in range(dnn_resnet50_features.shape[1])])\ndnn_resnet50_features['PetID'] = data_df['PetID'].values\ndel oof_train_res\ndel oof_test_res\ngc.collect()","59985d24":"dense121_array = dense121_feature_df.drop(['PetID','RescuerID','AdoptionSpeed'], axis=1).values\ndense121_tra = dense121_array[0:train_len]\ndense121_test = dense121_array[train_len:]\noof_train_dense121, oof_test_dense121 = get_image_feature(dense121_tra, dense121_test, IMG_FEATURE_DIM_NN)\ndel dense121_tra\ndel dense121_test\ngc.collect()\ndnn_dense121_features = np.vstack([oof_train_dense121, oof_test_dense121])\ndnn_dense121_features = pd.DataFrame(dnn_dense121_features, columns=['dense121_%d'%c for c in range(dnn_dense121_features.shape[1])])\ndnn_dense121_features['PetID'] = data_df['PetID'].values\ndel oof_train_dense121\ndel oof_test_dense121\ngc.collect()","fe94fbaf":"dense_array = dense_feature_df.drop(['PetID','RescuerID','AdoptionSpeed'], axis=1).values\ndense_tra = dense_array[0:train_len]\ndense_test = dense_array[train_len:]\noof_train_dense, oof_test_dense = get_image_feature(dense_tra, dense_test, IMG_FEATURE_DIM_NN)\ndel dense_tra\ndel dense_test\ngc.collect()\ndnn_dense_features = np.vstack([oof_train_dense, oof_test_dense])\ndnn_dense_features = pd.DataFrame(dnn_dense_features, columns=['dense_%d'%c for c in range(dnn_dense_features.shape[1])])\ndnn_dense_features['PetID'] = data_df['PetID'].values\ndel oof_train_dense\ndel oof_test_dense\ngc.collect()","098a7de9":"n_components = 64\n\n\n# features = raw_img_features.drop(['PetID','RescuerID','AdoptionSpeed'], axis=1).values\nsvd_ = TruncatedSVD(n_components=32, random_state=1337)\nsvd_resnet50_features = svd_.fit_transform(res50_array)\nsvd_resnet50_features = pd.DataFrame(svd_resnet50_features)\nsvd_resnet50_features = svd_resnet50_features.add_prefix('resnet50_SVD_')\nsvd_resnet50_features['PetID'] = resnet50_feature_df['PetID'].values","5d85db3c":"svd_ = TruncatedSVD(n_components=32, random_state=1337)\nsvd_resnet34_features = svd_.fit_transform(res34_array)\nsvd_resnet34_features = pd.DataFrame(svd_resnet34_features)\nsvd_resnet34_features = svd_resnet34_features.add_prefix('resnet34_SVD_')\nsvd_resnet34_features['PetID'] = resnet50_feature_df['PetID'].values","6b08f0ec":"svd_ = TruncatedSVD(n_components=32, random_state=1337)\nsvd_dense121_features = svd_.fit_transform(dense121_array)\nsvd_dense121_features = pd.DataFrame(svd_dense121_features)\nsvd_dense121_features = svd_dense121_features.add_prefix('dense121_SVD_')\nsvd_dense121_features['PetID'] = resnet50_feature_df['PetID'].values","75dfb94e":"svd_ = TruncatedSVD(n_components=32, random_state=1337)\nsvd_dense_features = svd_.fit_transform(dense_array)\nsvd_dense_features = pd.DataFrame(svd_dense_features)\nsvd_dense_features = svd_dense_features.add_prefix('dense_SVD_')\nsvd_dense_features['PetID'] = dense_feature_df['PetID'].values","c2877cdb":"svd_ = TruncatedSVD(n_components=64, random_state=1337)\nsvd_img_features = svd_.fit_transform(np.hstack([res50_array, dense_array, dense121_array, res34_array]))\nsvd_img_features = pd.DataFrame(svd_img_features)\nsvd_img_features = svd_img_features.add_prefix('IMG_SVD_')\nsvd_img_features['PetID'] = resnet50_feature_df['PetID'].values","22c3a1d5":"#img clustering\nfrom sklearn.cluster import DBSCAN, FeatureAgglomeration, KMeans\n\nfeatures = svd_img_features.drop(['PetID'], axis=1).values\n\ncluster = KMeans(n_clusters=32)\ncluster_label = cluster.fit_predict(features)\n\ncluster_img_features = pd.DataFrame(cluster_label)\ncluster_img_features = cluster_img_features.add_prefix('img_CLUSTER_')\ncluster_img_features['PetID'] = dense_feature_df['PetID'].values\n\n# features = svd_dense_features.drop(['PetID'], axis=1).values\n\n# cluster = KMeans(n_clusters=32)\n# cluster_label = cluster.fit_predict(features)\n\n# cluster_dense_features = pd.DataFrame(cluster_label)\n# cluster_dense_features = cluster_dense_features.add_prefix('dense_CLUSTER_')\n# cluster_dense_features['PetID'] = dense_feature_df['PetID'].values","d10b1957":"svd_img_features.head()","cb6c4b06":"# svd_resnet50_features.to_pickle('image_features_svd.pkl')\n# dnn_resnet50_features.to_pickle('image_features_nn.pkl')\n# cluster_resnet50_features.to_pickle('image_features_cluster.pkl')","66ad1d4a":"class PetFinderParser(object):\n    def __init__(self, debug=False):\n        self.debug = debug\n        self.sentence_sep = '; '\n        \n        # Does not have to be extracted because main DF already contains description\n        self.extract_sentiment_text = False\n        \n    def open_metadata_file(self, filename):\n        \"\"\"\n        Load metadata file.\n        \"\"\"\n        with open(filename, 'r') as f:\n            metadata_file = json.load(f)\n        return metadata_file\n            \n    def open_sentiment_file(self, filename):\n        \"\"\"\n        Load sentiment file.\n        \"\"\"\n        with open(filename, 'r') as f:\n            sentiment_file = json.load(f)\n        return sentiment_file\n            \n    def open_image_file(self, filename):\n        \"\"\"\n        Load image file.\n        \"\"\"\n        image = np.asarray(Image.open(filename))\n        return image\n        \n    def parse_sentiment_file(self, file):\n        \"\"\"\n        Parse sentiment file. Output DF with sentiment features.\n        \"\"\"\n        # documentSentiment\n        ret_val = {}\n        ret_val['doc_mag'] = file['documentSentiment']['magnitude']\n        ret_val['doc_score']= file['documentSentiment']['score']\n        ret_val['doc_language'] = file['language']\n        ret_val['doc_stcs_len'] = len(file['sentences'])\n        if ret_val['doc_stcs_len']>0:\n            ret_val['doc_first_score'] = file['sentences'][0]['sentiment']['score']\n            ret_val['doc_first_mag'] = file['sentences'][0]['sentiment']['magnitude']\n            ret_val['doc_last_score'] = file['sentences'][-1]['sentiment']['score']\n            ret_val['doc_last_mag'] = file['sentences'][-1]['sentiment']['magnitude']\n        else:\n            ret_val['doc_first_score'] = np.nan\n            ret_val['doc_first_mag'] = np.nan\n            ret_val['doc_last_score'] = np.nan\n            ret_val['doc_last_mag'] = np.nan\n        ret_val['doc_ent_num'] = len(file['entities'])\n        \n        # sentence score\n        mags, scores = [], []\n        for s in file['sentences']:\n            mags.append(s['sentiment']['magnitude'])\n            scores.append(s['sentiment']['score'])\n        \n        if len(scores)==0:\n            ret_val['doc_score_sum'] = np.nan\n            ret_val['doc_mag_sum'] = np.nan\n            ret_val['doc_score_mena'] = np.nan\n            ret_val['doc_mag_mean'] = np.nan\n            ret_val['doc_score_max'] = np.nan\n            ret_val['doc_mag_max'] = np.nan\n            ret_val['doc_score_min'] = np.nan\n            ret_val['doc_mag_min'] = np.nan\n            ret_val['doc_score_std'] = np.nan\n            ret_val['doc_mag_std'] = np.nan\n        else:\n            ret_val['doc_score_sum'] = np.sum(scores)\n            ret_val['doc_mag_sum'] = np.sum(mags)\n            ret_val['doc_score_mena'] = np.mean(scores)\n            ret_val['doc_mag_mean'] = np.mean(mags)\n            ret_val['doc_score_max'] = np.max(scores)\n            ret_val['doc_mag_max'] = np.max(mags)\n            ret_val['doc_score_min'] = np.min(scores)\n            ret_val['doc_mag_min'] = np.min(mags)\n            ret_val['doc_score_std'] = np.std(scores)\n            ret_val['doc_mag_std'] = np.std(mags)\n\n        # entity type\n        ret_val['sentiment_entities'] = []\n        ret_val['doc_ent_person_count'] = 0\n        ret_val['doc_ent_location_count'] = 0\n        ret_val['doc_ent_org_count'] = 0\n        ret_val['doc_ent_event_count'] = 0\n        ret_val['doc_ent_woa_count'] = 0\n        ret_val['doc_ent_good_count'] = 0\n        ret_val['doc_ent_other_count'] = 0\n        key_mapper = {\n            'PERSON':'doc_ent_person_count',\n            'LOCATION':'doc_ent_location_count',\n            'ORGANIZATION':'doc_ent_org_count',\n            'EVENT':'doc_ent_event_count',\n            'WORK_OF_ART':'doc_ent_woa_count',\n            'CONSUMER_GOOD':'doc_ent_good_count',\n            'OTHER':'doc_ent_other_count'\n        }\n        for e in file['entities']:\n            ret_val['sentiment_entities'].append(e['name'])\n            if e['type'] in key_mapper:\n                ret_val[key_mapper[e['type']]]+=1\n        \n        ret_val['sentiment_entities'] = ' '.join(ret_val['sentiment_entities'])\n        return ret_val\n    \n    def parse_metadata_file(self, file, img):\n        \"\"\"\n        Parse metadata file. Output DF with metadata features.\n        \"\"\"\n        file_keys = list(file.keys())\n        if 'textAnnotations' in file_keys:\n#             textanno = 1\n            textblock_num = len(file['textAnnotations'])\n            textlen = np.sum([len(text['description']) for text in file['textAnnotations']])\n        else:\n#             textanno = 0\n            textblock_num = 0\n            textlen = 0\n        if 'faceAnnotations' in file_keys:\n            faceanno = 1\n        else:\n            faceanno = 0\n        if 'labelAnnotations' in file_keys:\n            file_annots = file['labelAnnotations']#[:len(file['labelAnnotations'])]\n            if len(file_annots)==0:\n                file_label_score_mean = np.nan\n                file_label_score_max = np.nan\n                file_label_score_min = np.nan\n            else:\n                temp = np.asarray([x['score'] for x in file_annots])\n                file_label_score_mean = temp.mean()\n                file_label_score_max = temp.max()\n                file_label_score_min = temp.min()\n            file_top_desc = [x['description'] for x in file_annots]\n        else:\n            file_label_score_mean = np.nan\n            file_label_score_max = np.nan\n            file_label_score_min = np.nan\n            file_top_desc = ['']\n        \n        file_colors = file['imagePropertiesAnnotation']['dominantColors']['colors']\n        file_crops = file['cropHintsAnnotation']['cropHints']\n        if len(file_colors)==0:\n            file_color_score = np.nan\n            file_color_pixelfrac = np.nan\n            color_red_mean = np.nan\n            color_green_mean = np.nan\n            color_blue_mean = np.nan\n            color_red_std = np.nan\n            color_green_std = np.nan\n            color_blue_std = np.nan\n        else:\n            file_color_score = np.asarray([x['score'] for x in file_colors]).mean()\n            file_color_pixelfrac = np.asarray([x['pixelFraction'] for x in file_colors]).mean()\n            file_color_red = np.asarray([x['color']['red'] if 'red' in x['color'] else 0 for x in file_colors])\n            file_color_green = np.asarray([x['color']['green'] if 'green' in x['color'] else 0for x in file_colors])\n            file_color_blue = np.asarray([x['color']['blue'] if 'blue' in x['color'] else 0 for x in file_colors])\n            color_red_mean = file_color_red.mean()\n            color_green_mean = file_color_green.mean()\n            color_blue_mean = file_color_blue.mean()\n            color_red_std = file_color_red.std()\n            color_green_std = file_color_green.std()\n            color_blue_std = file_color_blue.std()\n        \n        if len(file_crops)==0:\n            file_crop_conf=np.nan\n            file_crop_importance = np.nan\n            file_crop_fraction_mean = np.nan\n            file_crop_fraction_sum = np.nan\n            file_crop_fraction_std = np.nan\n            file_crop_num = 0\n        else:\n            file_crop_conf = np.asarray([x['confidence'] for x in file_crops]).mean()\n            file_crop_num = len(file_crops)\n            if 'importanceFraction' in file_crops[0].keys():\n                file_crop_importance = np.asarray([x['importanceFraction'] for x in file_crops]).mean()\n            else:\n                file_crop_importance = np.nan\n            crop_areas = []\n            image_area = img.shape[0]*img.shape[1]\n            for crophint in file_crops:\n                v_x, v_y = [], []\n                for vertices in crophint['boundingPoly']['vertices']:\n                    if 'x' not in vertices:\n                        v_x.append(0)\n                    else:\n                        v_x.append(vertices['x'])\n                    if 'y' not in vertices:\n                        v_y.append(0)\n                    else:\n                        v_y.append(vertices['y'])\n                crop_areas.append((max(v_x)-min(v_x))*(max(v_y)-min(v_y))\/image_area)\n            file_crop_fraction_mean = np.mean(crop_areas)\n            file_crop_fraction_sum = np.sum(crop_areas)\n            file_crop_fraction_std = np.std(crop_areas)\n\n        df_metadata = {\n            'label_score_mean': file_label_score_mean,\n            'label_score_max': file_label_score_max,\n            'label_score_min': file_label_score_min,\n            'color_score': file_color_score,\n            'color_pixelfrac': file_color_pixelfrac,\n            'crop_conf': file_crop_conf,\n            'crop_importance': file_crop_importance,\n            'color_red_mean':color_red_mean,\n            'color_green_mean':color_green_mean,\n            'color_blue_mean':color_blue_mean,\n            'color_red_std':color_red_std,\n            'color_green_std':color_green_std,\n            'color_blue_std':color_blue_std,\n#             'crop_area_mean':file_crop_fraction_mean,\n            'crop_area_sum':file_crop_fraction_sum,\n#             'crop_area_std':file_crop_fraction_std,\n            'annots_top_desc': self.sentence_sep.join(file_top_desc),\n            'img_aratio':img.shape[0]\/img.shape[1],\n#             'text_annotation':textanno,\n            'text_len':textlen,\n            'textblock_num':textblock_num,\n            'face_annotation':faceanno\n        }\n        \n        return df_metadata\n    \n# Helper function for parallel data processing:\ndef extract_additional_features(pet_id, mode='train'):\n    sentiment_filename = '..\/input\/petfinder-adoption-prediction\/{}_sentiment\/{}.json'.format(mode, pet_id)\n    try:\n        sentiment_file = pet_parser.open_sentiment_file(sentiment_filename)\n        df_sentiment = pet_parser.parse_sentiment_file(sentiment_file)\n        df_sentiment['PetID'] = pet_id\n    except FileNotFoundError:\n        df_sentiment = None\n\n    dfs_metadata = []\n    for ind in range(1,200):\n        metadata_filename = '..\/input\/petfinder-adoption-prediction\/{}_metadata\/{}-{}.json'.format(mode, pet_id, ind)\n        image_filename = '..\/input\/petfinder-adoption-prediction\/{}_images\/{}-{}.jpg'.format(mode, pet_id, ind)\n        try:\n            image = cv2.imread(image_filename)\n            metadata_file = pet_parser.open_metadata_file(metadata_filename)\n            df_metadata = pet_parser.parse_metadata_file(metadata_file, image)\n            df_metadata['PetID'] = pet_id\n            dfs_metadata.append(df_metadata)\n        except FileNotFoundError:\n            break\n    return [df_sentiment, dfs_metadata]\n    \npet_parser = PetFinderParser()","070c8d91":"# Unique IDs from train and test:\ntrain_pet_ids = train.PetID.unique()\ntest_pet_ids = test.PetID.unique()\nn_jobs = 8\n\n# Train set:\n# Parallel processing of data:\ndfs_train = Parallel(n_jobs=n_jobs, verbose=1)(\n    delayed(extract_additional_features)(i, mode='train') for i in train_pet_ids)\n\n# Extract processed data and format them as DFs:\ntrain_dicts_sentiment = [x[0] for x in dfs_train if x[0] is not None]\ntrain_dfs_metadata = [x[1] for x in dfs_train if len([x[1]])>0]\n\ntrain_dfs_sentiment = pd.DataFrame(train_dicts_sentiment)\ntrain_dfs_metadata = list(itertools.chain.from_iterable(train_dfs_metadata))\ntrain_dfs_metadata = pd.DataFrame(train_dfs_metadata)\n\nprint(train_dfs_sentiment.shape, train_dfs_metadata.shape)","26a7ece8":"# Test set:\n# Parallel processing of data:\ndfs_test = Parallel(n_jobs=n_jobs, verbose=1)(\n    delayed(extract_additional_features)(i, mode='test') for i in test_pet_ids)\n\n# Extract processed data and format them as DFs:\ntest_dicts_sentiment = [x[0] for x in dfs_test if x[0] is not None]\ntest_dfs_metadata = [x[1] for x in dfs_test if len(x[1])>0]\n\ntest_dfs_sentiment = pd.DataFrame(test_dicts_sentiment)\ntest_dfs_metadata = list(itertools.chain.from_iterable(test_dfs_metadata))\ntest_dfs_metadata = pd.DataFrame(test_dfs_metadata)\n\nprint(test_dfs_sentiment.shape, test_dfs_metadata.shape)","c9c2ae92":"meta_df = pd.concat([train_dfs_metadata, test_dfs_metadata], sort=False).reset_index(drop=True)\nsenti_df = pd.concat([train_dfs_sentiment, test_dfs_sentiment], sort=False).reset_index(drop=True)","b995cf66":"# meta_df.to_pickle('.\/meta_df.pkl')\n# senti_df.to_pickle('.\/senti_df.pkl')","e191be61":"# meta_df = pd.read_pickle('.\/meta_df.pkl')\n# senti_df = pd.read_pickle('.\/senti_df.pkl')","01a0b87c":"metadata_desc = meta_df.groupby(['PetID'])['annots_top_desc'].unique().reset_index()\nmetadata_desc['meta_annots_top_desc'] = metadata_desc['annots_top_desc'].apply(lambda x: '; '.join(x))\nmetadata_desc.drop('annots_top_desc', axis=1, inplace=True)\n\npossible_annots = set()\nfor i in range(len(meta_df)):\n    possible_annots = possible_annots.union(set(meta_df['annots_top_desc'].iloc[i].split('; ')))\nannot_mapper = {}\nfor idx, a in enumerate(possible_annots):\n    annot_mapper[a] = str(idx)\nmetadata_desc['meta_desc'] = metadata_desc['meta_annots_top_desc'].apply(lambda x: ' '.join(annot_mapper[i] for i in x.split('; ')))","fb8537fa":"# sentiment feature\nsenti_df['sentiment_entities'].fillna('', inplace=True)\nsenti_df['sentiment_entities'] = senti_df['sentiment_entities'].str.lower()\nsenti_df['sentiment_len'] = senti_df['sentiment_entities'].apply(lambda x:len(x))\nsenti_df['sentiment_word_len'] = senti_df['sentiment_entities'].apply(lambda x: len(x.replace(';',' ').split(' ')))\nsenti_df['sentiment_word_unique'] = senti_df['sentiment_entities'].apply(lambda x: len(set(x.replace(';',' ').split(' '))))\n\nsenti_df['doc_language'] = pd.factorize(senti_df['doc_language'])[0]","15375873":"# meta \u9700\u8981agg\naggregates = {\n    'color_blue_mean':['mean','std'],\n    'color_blue_std':['mean'],\n    'color_green_mean':['mean','std'], \n    'color_green_std':['mean'],\n    'color_pixelfrac':['mean','std'],\n    'color_red_mean':['mean','std'],\n    'color_red_std':['mean'],\n    'color_score':['mean','max'], \n#     'crop_area_mean':['mean','std','max'],\n#     'crop_area_std':['mean'], \n    'crop_area_sum':['mean','std','min'], \n    'crop_conf':['mean','std','max'],\n    'crop_importance':['mean','std'],\n    'label_score_max':['mean','std','max'],\n    'label_score_mean':['mean','max','std'],\n    'label_score_min':['mean','max','std'],\n    'img_aratio':['nunique','std','max','min'],\n    'textblock_num':['mean','max'],\n#     'text_len':['mean','max'],\n    'face_annotation':['mean','nunique']\n}\n\n# Train\nmetadata_gr = meta_df.drop(['annots_top_desc'], axis=1)\nfor i in metadata_gr.columns:\n    if 'PetID' not in i:\n        metadata_gr[i] = metadata_gr[i].astype(float)\nmetadata_gr = metadata_gr.groupby(['PetID']).agg(aggregates)\nmetadata_gr.columns = pd.Index(['{}_{}_{}'.format('meta', c[0], c[1].upper()) for c in metadata_gr.columns.tolist()])\nmetadata_gr = metadata_gr.reset_index()","b756c03c":"meta_df = metadata_desc.merge(metadata_gr, on='PetID', how='left')","5b0b166b":"# annotation feature\nmeta_df['meta_annots_top_desc'].fillna(' ', inplace=True)","cdc56be1":"meta_df[meta_df['meta_textblock_num_MEAN']>0.8].head()","378433a6":"feat_df = data_df[['PetID','Color1','Breed1','State','RescuerID','Name','Breed_full','Color_full','hard_interaction']]","bf81a2bc":"# color feature\nagg = {\n    'Fee':['mean','std','max'],\n    'avg_fee':['mean','std','max'],\n    'Breed1':['nunique'],\n    #'Gender':['nunique'],\n    'Age':['mean','std','max'], #,'min'\n    'Quantity':['std'],#'mean',,'min','max'\n    'PetID':['nunique']\n}\nfeat = data_df.groupby('Color1').agg(agg)\nfeat.columns = pd.Index(['COLOR_' + e[0] + \"_\" + e[1].upper() for e in feat.columns.tolist()])\nfeat.reset_index(inplace=True)\nfeat_df = feat_df.merge(feat, on='Color1', how='left')\n\nagg = {\n    'Fee':['mean','std','max'],\n    'avg_fee':['mean','std','max'],\n    'Breed_full':['nunique'],\n    'Quantity':['sum'],\n}\nfeat = data_df.groupby('Color_full').agg(agg)\nfeat.columns = pd.Index(['COLORfull_' + e[0] + \"_\" + e[1].upper() for e in feat.columns.tolist()])\nfeat.reset_index(inplace=True)\nfeat_df = feat_df.merge(feat, on='Color_full', how='left')","1510df07":"# Breed feature\nagg = {\n    'Color_full':['nunique'],\n    'Breed2':['nunique'],\n    'FurLength':['nunique'],\n    'Fee':['mean','max'],#,'min'\n    'avg_fee':['mean','std','max'],\n    'Age':['mean','std','min','max'],\n    'Quantity':['mean','std','max','sum'],#'min'\n    'PetID':['nunique'],\n    'FurLength':['mean'],\n    'Health':['mean'],\n    'MaturitySize':['mean','std','min','max'],\n    'Vaccinated':['mean'],\n    'Dewormed':['mean'],\n    'Sterilized':['mean']\n}\nfeat = data_df.groupby('Breed1').agg(agg)\nfeat.columns = pd.Index(['BREED1_' + e[0] + \"_\" + e[1].upper() for e in feat.columns.tolist()])\nfeat.reset_index(inplace=True)\nfeat_df = feat_df.merge(feat, on='Breed1', how='left')\n\n# Breed feature\nagg = {\n    'Color_full':['nunique'],\n    'Fee':['mean','min','max'],\n    'avg_fee':['mean','std','max'],\n    'Quantity':['sum'],\n    'PetID':['nunique']\n}\nfeat = data_df.groupby('Breed_full').agg(agg)\nfeat.columns = pd.Index(['BREEDfull_' + e[0] + \"_\" + e[1].upper() for e in feat.columns.tolist()])\nfeat.reset_index(inplace=True)\nfeat_df = feat_df.merge(feat, on='Breed_full', how='left')","2f304580":"# State feature\nagg = {\n    'Color_full':['nunique'],\n    'Breed_full':['nunique'],\n    'PetID':['nunique'],\n    'RescuerID':['nunique'],\n    'Fee':['mean','max'],\n    'avg_fee':['mean','std','max'],\n    'Age':['mean','std','max'],\n    'Quantity':['mean','std','max'],#,'min','sum'\n    'FurLength':['mean','std'],\n    'Health':['mean'],\n    'MaturitySize':['mean','std'],\n    'Vaccinated':['mean'],\n    'Dewormed':['mean'],\n    'Sterilized':['mean'],\n    'VideoAmt':['mean','std'],\n    'PhotoAmt':['mean','std'],\n    'avg_photo':['mean','std']\n}\nfeat = data_df.groupby('State').agg(agg)\nfeat.columns = pd.Index(['STATE_' + e[0] + \"_\" + e[1].upper() for e in feat.columns.tolist()])\nfeat.reset_index(inplace=True)\nfeat_df = feat_df.merge(feat, on='State', how='left')","ae97f0e7":"# multiple agg\nagg = {\n    'Fee':['mean','min','max'],\n    'avg_fee':['mean','min','max'],\n    'Age':['mean','std','min','max'],\n    'Quantity':['mean','std','sum'],\n    'PetID':['nunique'],\n    'FurLength':['mean'],\n    'Health':['mean'],\n    'MaturitySize':['mean','std'],\n    'Vaccinated':['mean'],\n    'Dewormed':['mean'],\n    'Sterilized':['mean']\n}\nfeat = data_df.groupby(['State','Breed1','Color1']).agg(agg)\nfeat.columns = pd.Index(['MULTI_' + e[0] + \"_\" + e[1].upper() for e in feat.columns.tolist()])\nfeat.reset_index(inplace=True)\nfeat_df = feat_df.merge(feat, on=['State','Breed1','Color1'], how='left')\n\nagg = {\n    'Fee':['mean','min','max'],\n    'avg_fee':['mean','min','max'],\n    'Age':['mean','std','min','max'],\n    'Quantity':['mean','std','sum'],\n    'PetID':['nunique'],\n}\nfeat = data_df.groupby(['State','Breed_full','Color_full']).agg(agg)\nfeat.columns = pd.Index(['MULTI2_' + e[0] + \"_\" + e[1].upper() for e in feat.columns.tolist()])\nfeat.reset_index(inplace=True)\nfeat_df = feat_df.merge(feat, on=['State','Breed_full','Color_full'], how='left')","f33073c7":"# name feature\nfeat = data_df.groupby('Name')['PetID'].agg({'name_count':'nunique'}).reset_index()\nfeat_df = feat_df.merge(feat, on='Name', how='left')","03ae8cca":"# Count RescuerID occurrences:\nagg = {\n#     'avg_fee':['mean','std'], # hurt\n#     'Age':['mean','std'], #,'min','max' hurt\n#     'Quantity':['mean','std','sum'],\n    'PetID':['nunique'],\n    'Breed_full':['nunique'],\n    'VideoAmt':['mean','std'],\n    'PhotoAmt':['mean','std'],\n    'avg_photo':['mean','std'],\n    'Sterilized':['mean'],\n    'Dewormed':['mean'],\n    'Vaccinated':['mean']\n#     'description_word_len':['mean','std'] # hurt\n}\nrescuer_count = data_df.groupby(['RescuerID']).agg(agg)\nrescuer_count.columns = pd.Index(['RESCUER_' + e[0] + \"_\" + e[1].upper() for e in rescuer_count.columns.tolist()])\nrescuer_count.reset_index(inplace=True)\nfeat_df = feat_df.merge(rescuer_count, how='left', on='RescuerID')","80ee59f2":"# State feature\nagg = {\n    'Fee':['mean','min','max'],\n    'avg_fee':['mean','std','max']\n}\nfeat = data_df.groupby('hard_interaction').agg(agg)\nfeat.columns = pd.Index(['INTERACTION_' + e[0] + \"_\" + e[1].upper() for e in feat.columns.tolist()])\nfeat.reset_index(inplace=True)\nfeat_df = feat_df.merge(feat, on='hard_interaction', how='left')","8ff88820":"feat_df.drop(['Color1','Breed1','State','RescuerID','Name','Breed_full','Color_full','hard_interaction'], axis=1, inplace=True)","eb5e8e12":"X_text = data_df[['PetID','Description']].merge(data_df[['PetID','Chinese_desc']], on='PetID', how='left')\nX_text = X_text.merge(senti_df[['PetID','sentiment_entities']], on='PetID', how='left')\nX_text = X_text.merge(metadata_desc[['PetID','meta_annots_top_desc','meta_desc']], on='PetID', how='left')\ntext_columns = ['Description','Chinese_desc','sentiment_entities','meta_annots_top_desc','meta_desc']\nprint(text_columns)\nX_text['meta_annots_top_desc'].fillna(' ',inplace=True)\nX_text['meta_desc'].fillna(' ',inplace=True)\nX_text['sentiment_entities'].fillna(' ',inplace=True)","1e3a493b":"text_features = []\nngram_ranges = [(1,3),(1,3),(1,1),(1,3),(1,1)]\nn_components = [80, 24, 10, 32,16]\n\n# Generate text features:\nfor idx, i in enumerate(text_columns):\n    # Initialize decomposition methods:\n    print('generating features from: {}'.format(i))\n    svd_ = TruncatedSVD(\n        n_components=n_components[idx], random_state=1337)\n    \n    tfidf_col = TfidfVectorizer(ngram_range = ngram_ranges[idx], stop_words = 'english', #lowercase=False,\n                                tokenizer=custom_tokenizer,\n                               strip_accents='unicode').fit_transform(X_text.loc[:, i].values)\n    svd_col = svd_.fit_transform(tfidf_col)\n    svd_col = pd.DataFrame(svd_col)\n    svd_col = svd_col.add_prefix('SVD_{}_'.format(i))\n    \n    text_features.append(svd_col)\n\n# Combine all extracted features:\n# text_features = pd.concat(text_features, axis=1)","3970cc37":"docs = [custom_tokenizer(x) for x in data_df['Description'].values]\ndictionary = corpora.Dictionary(docs)\nname = 'Description'\ncorpus = [dictionary.doc2bow(text) for text in docs]\nlda = LdaMulticore(corpus, id2word=dictionary, num_topics=20, random_state = 999)\ndocres = [dict(lda[doc_bow]) for doc_bow in corpus]\ndoc_df = pd.DataFrame(docres,dtype=np.float16).fillna(0.001)\ndoc_df.columns = ['%s_lda_%d'%(name,x) for x in range(20)]","10c0f00a":"text_features.append(doc_df)","32cec7af":"X_text_char = data_df[['PetID','Name','BreedName_full']].merge(metadata_desc[['PetID','meta_annots_top_desc']], on='PetID', how='left')\nX_text_char['meta_annots_top_desc'].fillna(' ',inplace=True)\nfor c in ['Name','BreedName_full','meta_annots_top_desc']:\n    svd_ = TruncatedSVD(\n        n_components=16, random_state=1337)\n\n    tfidf_col = TfidfVectorizer(ngram_range = (1,5), analyzer='char',#lowercase=False,\n                               strip_accents='unicode').fit_transform(X_text_char[c])\n    svd_col = svd_.fit_transform(tfidf_col)\n    svd_col = pd.DataFrame(svd_col)\n    svd_col = svd_col.add_prefix('SVD_CHAR_{}_'.format(c))\n\n    text_features.append(svd_col)\ndel X_text_char\ngc.collect()","04903da1":"text_features = pd.concat(text_features, axis=1)","339ffcbd":"text_features['PetID'] = X_text['PetID'].values","3087eb79":"def get_lda_feature(data, target, source, name, n_topic = 10, random_state = 999):\n    retval = pd.DataFrame(data[[target, source]])\n    x = retval.groupby(target, as_index=False)[source].agg({'list':(lambda x: list(x))})\n    x['sentence'] = x['list'].apply(lambda x: list(map(str,x)))\n    docs = x['sentence'].tolist() #.apply(lambda x:x.split()).tolist()\n    dictionary = corpora.Dictionary(docs)\n    corpus = [dictionary.doc2bow(text) for text in docs]\n    lda = LdaMulticore(corpus, id2word=dictionary, num_topics=n_topic, random_state = random_state)\n    docres = [dict(lda[doc_bow]) for doc_bow in corpus]\n    doc_df = pd.DataFrame(docres,dtype=np.float16).fillna(0.001)\n    doc_df.columns = ['%s_lda_%d'%(name,x) for x in range(n_topic)]\n    doc_df[target] = x[target]\n    return doc_df","16e943de":"breed_color_lda = get_lda_feature(data_df, 'Breed1', 'Color_full', 'breed', n_topic=5)\nbreed_breed_lda = get_lda_feature(data_df, 'Breed1', 'Breed2', 'breed_breed', n_topic=5)\nstate_breed_lda = get_lda_feature(data_df, 'State', 'Breed_full', 'State_breed', n_topic=5)\nstate_color_lda = get_lda_feature(data_df, 'State', 'Color_full', 'State_color',n_topic=5)\n# rescuer_breed_lda = get_lda_feature(data_df, 'RescuerID', 'Breed_full', 'rescuer_breed',n_topic=30)\n# rescuer_color_lda = get_lda_feature(data_df, 'RescuerID', 'Color_full', 'rescuer_color',n_topic=10)","c30e9b63":"# Train merges:\ndata_df_proc = data_df.copy()\ndata_df_proc = data_df_proc.merge(senti_df, how='left', on='PetID')\ndata_df_proc = data_df_proc.merge(meta_df, how='left', on='PetID')\ndata_df_proc = data_df_proc.merge(feat_df, how='left', on='PetID')\n\ndata_df_proc = data_df_proc.merge(dnn_resnet50_features[['PetID','resnet_%d'%IMG_FEATURE_DIM_NN]], how='left', on='PetID')#\ndata_df_proc = data_df_proc.merge(dnn_dense_features[['PetID','dense_%d'%IMG_FEATURE_DIM_NN]], how='left', on='PetID')#\ndata_df_proc = data_df_proc.merge(dnn_resnet34_features[['PetID','res34_%d'%IMG_FEATURE_DIM_NN2]], how='left', on='PetID')#\ndata_df_proc = data_df_proc.merge(dnn_dense121_features[['PetID','dense121_%d'%IMG_FEATURE_DIM_NN]], how='left', on='PetID')#\n\ndata_df_proc = data_df_proc.merge(svd_resnet34_features, how='left', on='PetID')\ndata_df_proc = data_df_proc.merge(svd_resnet50_features, how='left', on='PetID')\ndata_df_proc = data_df_proc.merge(svd_dense_features, how='left', on='PetID')\ndata_df_proc = data_df_proc.merge(svd_dense121_features, how='left', on='PetID')\n\ndata_df_proc = data_df_proc.merge(cluster_img_features, how='left', on='PetID')\n\n# lda features\ndata_df_proc = data_df_proc.merge(breed_color_lda, how='left', on='Breed1')\ndata_df_proc = data_df_proc.merge(breed_breed_lda, how='left', on='Breed1')\ndata_df_proc = data_df_proc.merge(state_breed_lda, how='left', on='State')\ndata_df_proc = data_df_proc.merge(state_color_lda, how='left', on='State')\n\n# Concatenate with main DF:\ndata_df_proc = data_df_proc.merge(text_features, how='left', on='PetID')\n\n\nprint(data_df_proc.shape)\nassert data_df_proc.shape[0] == data_df.shape[0]","df2a2f78":"# Split into train and test again:\nX_train = data_df_proc.iloc[0:train_len]\nX_test = data_df_proc.iloc[train_len:]\n# X_train_dummy = data_df_proc_dummy.iloc[0:train_len]\n# X_test_dummy = data_df_proc_dummy.iloc[train_len:]\n\n# Remove missing target column from test:\nX_test = X_test.drop(['AdoptionSpeed'], axis=1)\n\nprint('X_train shape: {}'.format(X_train.shape))\nprint('X_test shape: {}'.format(X_test.shape))\n\nassert X_train.shape[0] == train.shape[0]\nassert X_test.shape[0] == test.shape[0]\n\n# Check if columns between the two DFs are the same:\ntrain_cols = X_train.columns.tolist()\ntrain_cols.remove('AdoptionSpeed')\n\ntest_cols = X_test.columns.tolist()\n\nassert np.all(train_cols == test_cols)","eae4a677":"# Additional parameters:\nearly_stop = 300\nverbose_eval = 100\nnum_rounds = 10000\n\nto_drop_columns = ['PetID', 'Name', 'RescuerID', 'AdoptionSpeed', 'target2', \n                   'main_breed_Type', 'main_breed_BreedName', 'second_breed_Type', 'second_breed_BreedName',\n                   'Description', 'sentiment_entities', 'meta_annots_top_desc','meta_desc',\n                   'Chinese_desc', 'English_desc','BreedName_full','Breed1Name','Breed2Name']","9b6b8fc7":"torch.manual_seed(1991)\ntorch.cuda.manual_seed(1991)\ntorch.backends.cudnn.deterministic = True","460b8f07":"fm_cols = ['Type','age_in_year','Breed1','Breed2','Gender','Color1','Color2','Color3','MaturitySize',\n           'FurLength','Vaccinated','Dewormed','Sterilized','Health','State','Breed_full',\n           'Color_full', 'hard_interaction','img_CLUSTER_0']\nfm_data = data_df_proc[fm_cols]\nfm_values = []\nfor c in fm_cols:\n    fm_data.loc[:,c] = fm_data[c].fillna(0)\n    fm_data.loc[:,c] = c+'_'+fm_data[c].astype(str)\n    fm_values+=fm_data[c].unique().tolist()","4fa1cb22":"from sklearn.preprocessing import LabelEncoder\nlbe = LabelEncoder()\nlbe.fit(fm_values)\nfor c in fm_cols:\n    fm_data.loc[:,c] = lbe.transform(fm_data[c])","5bc12e80":"numerical_cols = [x for x in X_train.columns if x not in to_drop_columns+fm_cols+svd_dense_features.columns.tolist()]","31fa77ed":"len(numerical_cols)","63869825":"numerical_feats = []\nfor c in numerical_cols:\n    numerical_feats.append(data_df_proc[c].fillna(0))\n    \n# for c in range(1920):\n#     numerical_feats.append(raw_img_features['resnet50_%d'%c].fillna(0))\n\nnumerical_feats = np.vstack(numerical_feats).T\n# numerical_feats = stdscaler.fit_transform(numerical_feats)","72a41c2b":"numerical_feats.shape","6d05aaae":"MAX_LEN = 400\nclass PetDesDataset(Dataset):\n    def __init__(self, sentences, pos, fm_data, numerical_feat,\n                 mode='train', target=None):\n        super(PetDesDataset, self).__init__()\n        self.data = sentences\n        self.pos = pos\n        self.target = target\n        self.mode = mode\n        self.fm_data = fm_data\n        self.fm_dim = fm_data.shape[1]\n        self.numerical_feat = numerical_feat\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, index):\n        if index not in range(0, self.__len__()):\n            return self.__getitem__(np.random.randint(0, self.__len__()))\n        sentence_len = min(MAX_LEN, len(self.data[index]))\n        sentence = torch.tensor(self.data[index][:sentence_len])\n        fm_data = self.fm_data[index,:]\n        pos = torch.tensor(self.pos[index][:sentence_len])\n\n        if self.mode != 'test':  # , pos, tag\n            return sentence, pos, sentence_len, fm_data, self.numerical_feat[index], self.target[index]  # , clf_label\n        else:\n            return sentence, pos, sentence_len, fm_data, self.numerical_feat[index]\n        \ndef nn_collate(batch):\n    has_label = len(batch[0]) == 6\n    if has_label:\n        sentences, poses, lengths, fm_data, numerical_feats, label = zip(*batch)\n        sentences = nn.utils.rnn.pad_sequence(sentences, batch_first=True).type(torch.LongTensor)\n        poses = nn.utils.rnn.pad_sequence(poses, batch_first=True).type(torch.LongTensor)\n        lengths = torch.LongTensor(lengths)\n        fm_data = torch.LongTensor(fm_data)\n        numerical_feats = torch.FloatTensor(numerical_feats)\n        label = torch.FloatTensor(label)\n        return sentences, poses, lengths, fm_data, numerical_feats, label\n    else:\n        sentences, poses, lengths, fm_data, numerical_feats = zip(*batch)\n        sentences = nn.utils.rnn.pad_sequence(sentences, batch_first=True).type(torch.LongTensor)\n        poses = nn.utils.rnn.pad_sequence(poses, batch_first=True).type(torch.LongTensor)\n        lengths = torch.LongTensor(lengths)\n        fm_data = torch.LongTensor(fm_data)\n        numerical_feats = torch.FloatTensor(numerical_feats)\n        return sentences, poses, lengths, fm_data, numerical_feats","2feb3259":"def get_mask(sequences_batch, sequences_lengths, cpu=False):\n    batch_size = sequences_batch.size()[0]\n    max_length = torch.max(sequences_lengths)\n    mask = torch.ones(batch_size, max_length, dtype=torch.float)\n    mask[sequences_batch[:, :max_length] == 0] = 0.0\n    if cpu:\n        return mask\n    else:\n        return mask.cuda()\nclass Attention(nn.Module):\n    def __init__(self, feature_dim, bias=True, head_num=1, **kwargs):\n        super(Attention, self).__init__(**kwargs)\n        self.supports_masking = True\n        self.bias = bias\n        self.feature_dim = feature_dim\n        self.head_num = head_num\n        weight = torch.zeros(feature_dim, self.head_num)\n        bias = torch.zeros((1, 1, self.head_num))\n        nn.init.xavier_uniform_(weight)\n        self.weight = nn.Parameter(weight)\n        self.b = nn.Parameter(bias)\n\n    def forward(self, x, mask=None):\n        batch_size, step_dim, feature_dim = x.size()\n        eij = torch.mm(\n            x.contiguous().view(-1, feature_dim),  # B*L*H\n            self.weight  # B*H*1\n        ).view(-1, step_dim, self.head_num)  # B*L*head\n        if self.bias:\n            eij = eij + self.b\n        eij = torch.tanh(eij)\n        if mask is not None:\n            eij = eij * mask - 99999.9 * (1 - mask)\n        a = torch.softmax(eij, dim=1)\n\n        weighted_input = torch.bmm(x.permute((0,2,1)),\n                                   a).view(batch_size, -1)\n        return weighted_input","a57ad35a":"embed_size = 300\nclass FM(nn.Module):\n\n    def __init__(self, max_features, feat_len, embed_size):\n        super(FM, self).__init__()\n        self.bias_emb = nn.Embedding(max_features, 1)\n        self.fm_emb = nn.Embedding(max_features, embed_size)\n        self.feat_len = feat_len\n        self.embed_size = embed_size\n\n    def forward(self, x):\n        bias = self.bias_emb(x)\n        bias = torch.sum(bias,1) # N * 1\n\n        # second order term\n        # square of sum\n        emb = self.fm_emb(x)\n        sum_feature_emb = torch.sum(emb, 1) # N * k\n        square_sum_feature_emb = sum_feature_emb*sum_feature_emb\n\n        # sum of square\n        square_feature_emb = emb * emb\n        sum_square_feature_emb = torch.sum(square_feature_emb, 1) # N * k\n\n        second_order = 0.5*(square_sum_feature_emb-sum_square_feature_emb) # N *k\n        return bias+second_order, emb.view(-1, self.feat_len*self.embed_size)\n","c32190b3":"class FmNlpModel(nn.Module):\n    def turn_on_embedding(self):\n        self.embedding.weight.requires_grad = True\n\n    def __init__(self, hidden_size=64, init_embedding=None, head_num=3,\n                 fm_embed_size=8, fm_feat_len=10, fm_max_feature = 300, numerical_dim = 300,\n                 nb_word = 40000, nb_pos = 200, pos_emb_size = 10):\n        super(FmNlpModel, self).__init__()\n        self.hidden_size = hidden_size\n        self.embedding = nn.Embedding(nb_word, 300, padding_idx=0)\n        self.pos_embedding = nn.Embedding(nb_pos+100, pos_emb_size, padding_idx=0)\n        \n        if init_embedding is not None:\n            self.embedding.weight.data.copy_(torch.from_numpy(init_embedding))\n        self.embedding.weight.requires_grad = False\n        \n        self.fm = FM(fm_max_feature, fm_feat_len, fm_embed_size)\n\n        self.dropout = nn.Dropout(0.1)\n        self.attention_gru = Attention(feature_dim=self.hidden_size * 2, head_num=head_num)\n        self.gru = nn.GRU(embed_size+pos_emb_size, hidden_size, bidirectional=True, batch_first=True) #\n        self.gru2 = nn.GRU(hidden_size * 2, hidden_size, bidirectional=True, batch_first=True)\n        \n        self.dnn = nn.Sequential(\n            nn.BatchNorm1d(numerical_dim),\n            nn.Dropout(0.1),\n            nn.Linear(numerical_dim, 256),\n            nn.ELU(inplace=True),\n            nn.Dropout(0.1),\n            nn.Linear(256, 128),\n            nn.ELU(inplace=True),\n        )\n        \n        self.rnn_dnn = nn.Sequential(\n            nn.BatchNorm1d(fm_embed_size+fm_feat_len*fm_embed_size +2*head_num * hidden_size+128), #\n            nn.Dropout(0.3),\n            nn.Linear(fm_embed_size+fm_feat_len*fm_embed_size+2*head_num * hidden_size+128, 32),\n            nn.ELU(inplace=True),\n        )\n        self.logit = nn.Sequential(\n            nn.Linear(32,1)\n        )\n#         self.apply(_init_esim_weights)\n\n    def forward(self, x, pos_x, len_x, fm_x, numerical_x):\n        \n        fm_result, fm_embed = self.fm(fm_x)\n        \n        sentence_mask = get_mask(x, len_x)\n        x = x * sentence_mask.long()\n        sentence_mask = torch.unsqueeze(sentence_mask, -1)\n\n        h_embedding = self.embedding(x)\n        h_pos_embedding = self.pos_embedding(pos_x)\n        h_embedding = torch.cat([h_embedding, h_pos_embedding],2)\n        \n        h_embedding = self.dropout(h_embedding)\n        \n        sorted_seq_lengths, indices = torch.sort(len_x, descending=True)\n        # \u6392\u5e8f\u524d\u7684\u987a\u5e8f\u662f\u5bf9\u4e0b\u6807\u518d\u6392\u4e00\u6b21\u5e8f\n        _, desorted_indices = torch.sort(indices, descending=False)\n        h_embedding = h_embedding[indices]\n        packed_inputs = nn.utils.rnn.pack_padded_sequence(h_embedding, sorted_seq_lengths, batch_first=True)\n        \n        h_gru, _ = self.gru(packed_inputs)\n        h_gru2, _ = self.gru2(h_gru)  # sentence_mask.expand_as(h_lstm)\n        \n        h_gru2, _ = nn.utils.rnn.pad_packed_sequence(h_gru2, batch_first=True)\n        h_gru2 = h_gru2[desorted_indices]\n        att_pool_gru = self.attention_gru(h_gru2, sentence_mask)\n        \n        numerical_x = self.dnn(numerical_x)\n\n        x = torch.cat([att_pool_gru,fm_result,fm_embed,numerical_x],1) # \n        feat = self.rnn_dnn(x)\n        out = self.logit(feat)\n\n        return out, feat","3b453669":"X_train_numerical = numerical_feats[0:len(train)]\nX_test_numerical = numerical_feats[len(train):]\n\nX_train_seq = pd.Series(eng_sequences[0:len(train)])\nX_test_seq = pd.Series(eng_sequences[len(train):])\n\nX_train_pos_seq = pd.Series(pos_sequences[0:len(train)])\nX_test_pos_seq = pd.Series(pos_sequences[len(train):])\n\nX_train_fm = fm_data.iloc[0:len(train)].values\nX_test_fm = fm_data.iloc[len(train):].values\n\nY_train = data_df.iloc[0:len(train)]['AdoptionSpeed'].values","aef9f5ce":"train_epochs = 6\nloss_fn = torch.nn.MSELoss().cuda()\noof_train_nlp = np.zeros((X_train.shape[0], 32+1))\noof_test_nlp = []\n\ntest_set = PetDesDataset(X_test_seq.tolist(), X_test_pos_seq.tolist(), X_test_fm, X_test_numerical, mode='test')\ntest_loader = DataLoader(test_set, batch_size=512, shuffle=False, num_workers=1, pin_memory=True,\n                                collate_fn=nn_collate)\nqwks = []\nrmses = []\n\nfor n_fold, (train_idx, valid_idx) in enumerate(split_index): \n        \n    print('fold:', n_fold)\n    hist = histogram(Y_train[train_idx].astype(int), \n                     int(np.min(X_train['AdoptionSpeed'])), \n                     int(np.max(X_train['AdoptionSpeed'])))\n    tr_cdf = get_cdf(hist)\n    \n    training_set = PetDesDataset(X_train_seq[train_idx].tolist(), \n                                 X_train_pos_seq[train_idx].tolist(),\n                                 X_train_fm[train_idx], \n                                 X_train_numerical[train_idx], target = Y_train[train_idx])\n    \n    validation_set = PetDesDataset(X_train_seq[valid_idx].tolist(), \n                                   X_train_pos_seq[valid_idx].tolist(),\n                                   X_train_fm[valid_idx], \n                                  X_train_numerical[valid_idx],target = Y_train[valid_idx])\n    \n    training_loader = DataLoader(training_set, batch_size=512, shuffle=True, num_workers=1,\n                                collate_fn=nn_collate)\n    validation_loader = DataLoader(validation_set, batch_size=512, shuffle=False, num_workers=1,\n                                collate_fn=nn_collate)\n    \n    model = FmNlpModel(hidden_size=48, init_embedding=embedding_matrix, head_num=10, \n                      fm_embed_size=10, fm_feat_len=X_train_fm.shape[1], fm_max_feature=len(fm_values),\n                      numerical_dim=X_train_numerical.shape[1],\n                      nb_word=nb_words, nb_pos=nb_pos, pos_emb_size=10)\n    model.cuda()\n    \n    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.01)\n#     scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=4, gamma=0.5)\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=4, eta_min=0.0001)\n    \n    iteration = 0\n    min_val_loss = 100\n    since = time.time()\n    \n    for epoch in range(train_epochs):       \n        scheduler.step()\n        model.train()\n        for sentences, poses, lengths, x_fm, x_numerical, labels in training_loader:\n            iteration += 1\n            sentences = sentences.cuda()\n            poses = poses.cuda()\n            lengths = lengths.cuda()\n            x_fm = x_fm.cuda()\n            x_numerical = x_numerical.cuda()\n            labels = labels.type(torch.FloatTensor).cuda().view(-1, 1)\n\n            pred,_ = model(sentences, poses, lengths, x_fm, x_numerical)\n            loss = loss_fn(pred, labels)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n        model.eval()\n        val_predicts = []\n        val_feats = []\n        with torch.no_grad():\n            for sentences, poses, lengths, x_fm, x_numerical, labels in validation_loader:\n                sentences = sentences.cuda()\n                poses = poses.cuda()\n                lengths = lengths.cuda()\n                x_fm = x_fm.cuda()\n                x_numerical = x_numerical.cuda()\n                labels = labels.type(torch.FloatTensor).cuda()#.view(-1, 1)\n                v_pred, v_feat = model(sentences, poses, lengths, x_fm, x_numerical)\n                val_predicts.append(v_pred.cpu().numpy())\n                val_feats.append(v_feat.cpu().numpy())\n\n        val_predicts = np.concatenate(val_predicts)\n        val_feats = np.vstack(val_feats)\n        val_loss = rmse(Y_train[valid_idx], val_predicts)\n        if val_loss<min_val_loss:\n            min_val_loss = val_loss\n            oof_train_nlp[valid_idx,:] = np.hstack([val_feats, val_predicts])\n            test_feats = []\n            test_preds = []\n            with torch.no_grad():\n                for sentences, poses, lengths, x_fm, x_numerical in test_loader:\n                    sentences = sentences.cuda()\n                    poses = poses.cuda()\n                    lengths = lengths.cuda()\n                    x_fm = x_fm.cuda()\n                    x_numerical = x_numerical.cuda()\n                    v_pred, feat = model(sentences, poses, lengths, x_fm, x_numerical)\n                    test_preds.append(v_pred.cpu().numpy())\n                    test_feats.append(feat.cpu().numpy())\n            test_feats = np.hstack([np.vstack(test_feats), np.concatenate(test_preds)])\n            pred_test_y_k = getTestScore2(val_predicts.flatten(), tr_cdf)\n            qwk = quadratic_weighted_kappa(Y_train[valid_idx], pred_test_y_k)\n            print(epoch, \"val loss:\", val_loss, \"val QWK_2 = \", qwk, \"elapsed time:\", time.time()-since)\n    oof_test_nlp.append(test_feats)\n    del model\n    del training_set\n    del validation_set \n    del sentences\n    del lengths\n    del x_fm\n    del x_numerical\n    del poses\n    gc.collect()\n    torch.cuda.empty_cache()\n    \n    qwks.append(qwk)\n    rmses.append(min_val_loss)","2e022942":"print('overall rmse: %.5f'%rmse(oof_train_nlp[:,-1], X_train['AdoptionSpeed']))\nprint('mean rmse =', np.mean(rmses), 'rmse std =', np.std(rmses))\nprint('mean QWK =', np.mean(qwks), 'std QWK =', np.std(qwks))","734a5494":"# del model\n# del training_set\n# del validation_set \n# del sentences\n# del lengths\n# del x_fm\n# del x_numerical\n# gc.collect()\n# torch.cuda.empty_cache()","a082adc9":"oof_test_nlp = np.mean(oof_test_nlp, axis=0)","0b7eabe6":"features = [x for x in X_train.columns if x not in to_drop_columns+svd_resnet50_features.columns.tolist()]","dd3e0ccc":"print(len(features))","f5fdec4b":"def run_lgb(X_train, X_test, features, split_index):\n    params = {'application': 'regression',\n          'boosting': 'gbdt',\n          'metric': 'rmse',\n          'num_leaves': 70,\n          'max_depth': 9,\n          'learning_rate': 0.02,\n          'subsample': 0.85,\n          'feature_fraction': 0.7,\n          'lambda_l1':0.01,\n          'verbosity': -1,\n         }\n    oof_train_lgb = np.zeros((X_train.shape[0]))\n    oof_test_lgb = []\n    qwks = []\n    rmses = []\n    \n    for n_fold, (train_idx, valid_idx) in enumerate(split_index):\n        since = time.time()\n        X_tr = X_train.iloc[train_idx]\n        X_val = X_train.iloc[valid_idx]\n\n        y_tr = X_tr['AdoptionSpeed'].values    \n        y_val = X_val['AdoptionSpeed'].values\n\n        d_train = lgb.Dataset(X_tr[features], label=y_tr,\n    #                          categorical_feature=['Breed1','Color1','Breed2','State','Breed_full','Color_full']\n                             )\n        d_valid = lgb.Dataset(X_val[features], label=y_val, reference=d_train)\n        watchlist = [d_valid]\n\n        print('training LGB:')\n        lgb_model = lgb.train(params,\n                          train_set=d_train,\n                          num_boost_round=num_rounds,\n                          valid_sets=watchlist,\n                          verbose_eval=500,\n                          early_stopping_rounds=100,\n                         )\n\n        val_pred = lgb_model.predict(X_val[features])\n        test_pred = lgb_model.predict(X_test[features])\n        train_pred = lgb_model.predict(X_tr[features])\n\n        oof_train_lgb[valid_idx] = val_pred\n        oof_test_lgb.append(test_pred)\n\n        hist = histogram(X_tr['AdoptionSpeed'].astype(int), \n                         int(np.min(X_train['AdoptionSpeed'])), \n                         int(np.max(X_train['AdoptionSpeed'])))\n        tr_cdf = get_cdf(hist)\n        _, cutoff = getScore(train_pred, tr_cdf, True)\n\n        pred_test_y_k = getTestScore2(val_pred, tr_cdf)\n        qwk = quadratic_weighted_kappa(X_val['AdoptionSpeed'].values, pred_test_y_k)\n        qwks.append(qwk)\n        rmses.append(rmse(X_val['AdoptionSpeed'].values, val_pred))\n        print(\"QWK_2 = \", qwk,'elapsed time:',time.time()-since)\n    \n    oof_test_lgb = np.mean(oof_test_lgb, axis=0)\n    print('overall rmse: %.5f'%rmse(oof_train_lgb, X_train['AdoptionSpeed']))\n    print('mean rmse =', np.mean(rmses), 'rmse std =', np.std(rmses))\n    print('mean QWK =', np.mean(qwks), 'std QWK =', np.std(qwks))\n    return oof_train_lgb, oof_test_lgb","99f206df":"base_cols = [x for x in senti_df.columns.tolist()+meta_df.columns.tolist()+feat_df.columns.tolist() \\\n             +cluster_img_features.columns.tolist() \\\n             +breed_color_lda.columns.tolist()+breed_breed_lda.columns.tolist() \\\n             +state_breed_lda.columns.tolist() \\\n             +state_color_lda.columns.tolist()+text_features.columns.tolist() if x not in to_drop_columns]","3896f897":"cols34 = base_cols+['res34_%d'%IMG_FEATURE_DIM_NN2]+['resnet34_SVD_%d'%i for i in range(32)]\nlgb34_oof, lgb34_test = run_lgb(X_train, X_test, cols34, split_index)","5e34df06":"cols50 = base_cols+['resnet_%d'%IMG_FEATURE_DIM_NN]+['resnet50_SVD_%d'%i for i in range(32)]\nlgb50_oof, lgb50_test = run_lgb(X_train, X_test, cols50, split_index)","2f9d3294":"cols121 = base_cols+['dense121_%d'%IMG_FEATURE_DIM_NN]+['dense121_SVD_%d'%i for i in range(32)]\nlgb121_oof, lgb121_test = run_lgb(X_train, X_test, cols121, split_index)","82a88845":"cols201 = base_cols+['dense_%d'%IMG_FEATURE_DIM_NN]+['dense_SVD_%d'%i for i in range(32)]\nlgb201_oof, lgb201_test = run_lgb(X_train, X_test, cols201, split_index)","c4717f23":"# from catboost import CatBoostRegressor, Pool","baeaa654":"# features = [x for x in X_train.columns if x not in to_drop_columns+svd_dense121_features.columns.tolist()]","74f717dd":"# cat_index = []\n# for idx, c in enumerate(features):\n#     if c in ['Type','Breed1','Breed2','Gender','Color1','Color2','Color3','State','Breed_full',\n#            'Color_full', 'hard_interaction','img_CLUSTER_0']:\n#         cat_index.append(idx)","7a3d4699":"# oof_train_cat = np.zeros((X_train.shape[0]))\n# oof_test_cat = []\n# qwks = []\n# rmses = []\n\n# for n_fold, (train_idx, valid_idx) in enumerate(split_index):\n#     since = time.time()\n#     X_tr = X_train.iloc[train_idx]\n#     X_val = X_train.iloc[valid_idx]\n\n#     y_tr = X_tr['AdoptionSpeed'].values    #apply(target_transform).\n#     y_val = X_val['AdoptionSpeed'].values #apply(target_transform)\n        \n    \n#     eval_dataset = Pool(X_val[features].values,\n#                     y_val,\n#                    cat_index)\n#     print('training Catboost:')\n#     model = CatBoostRegressor(learning_rate=0.01,  depth=8, task_type = \"GPU\", l2_leaf_reg=1)\n#     model.fit(X_tr[features].values,\n#               y_tr,\n#               eval_set=eval_dataset,\n#               cat_features= cat_index,\n#               verbose=False)\n    \n#     val_pred = model.predict(eval_dataset)\n#     test_pred = model.predict(X_test[features])\n    \n#     oof_train_cat[valid_idx] = val_pred\n#     oof_test_cat.append(test_pred)\n               \n#     hist = histogram(X_tr['AdoptionSpeed'].astype(int), \n#                      int(np.min(X_train['AdoptionSpeed'])), \n#                      int(np.max(X_train['AdoptionSpeed'])))\n#     tr_cdf = get_cdf(hist)\n    \n#     pred_test_y_k = getTestScore2(val_pred, tr_cdf)\n#     qwk = quadratic_weighted_kappa(X_val['AdoptionSpeed'].values, pred_test_y_k)\n#     qwks.append(qwk)\n#     rmses.append(rmse(X_val['AdoptionSpeed'].values, val_pred))\n#     print('rmse=',rmses[-1],\"QWK_2 = \", qwk,'elapsed time:',time.time()-since)","7e6e33f7":"# print('overall rmse: %.5f'%rmse(oof_train_cat, X_train['AdoptionSpeed']))\n# print('mean rmse =', np.mean(rmses), 'rmse std =', np.std(rmses))\n# print('mean QWK =', np.mean(qwks), 'std QWK =', np.std(qwks))","a5c43dec":"def mean_encode(train_data, test_data, columns, target_col, reg_method=None,\n                alpha=0, add_random=False, rmean=0, rstd=0.1, folds=1):\n    '''Returns a DataFrame with encoded columns'''\n    encoded_cols = []\n    target_mean_global = train_data[target_col].mean()\n    for col in columns:\n        # Getting means for test data\n        nrows_cat = train_data.groupby(col)[target_col].count()\n        target_means_cats = train_data.groupby(col)[target_col].mean()\n        target_means_cats_adj = (target_means_cats*nrows_cat + \n                                 target_mean_global*alpha)\/(nrows_cat+alpha)\n        # Mapping means to test data\n        encoded_col_test = test_data[col].map(target_means_cats_adj)\n        # Getting a train encodings\n        if reg_method == 'expanding_mean':\n            train_data_shuffled = train_data.sample(frac=1, random_state=1)\n            cumsum = train_data_shuffled.groupby(col)[target_col].cumsum() - train_data_shuffled[target_col]\n            cumcnt = train_data_shuffled.groupby(col).cumcount()\n            encoded_col_train = cumsum\/(cumcnt)\n            encoded_col_train.fillna(target_mean_global, inplace=True)\n            if add_random:\n                encoded_col_train = encoded_col_train + normal(loc=rmean, scale=rstd, \n                                                               size=(encoded_col_train.shape[0]))\n        elif (reg_method == 'k_fold') and (folds > 1):\n            kf = StratifiedKFold(folds, shuffle=True, random_state=1)\n            kfold = kf.split(train_data, train_data[target_col].values) \n            parts = []\n            for tr_in, val_ind in kfold:\n                # divide data\n                df_for_estimation, df_estimated = train_data.iloc[tr_in], train_data.iloc[val_ind]\n                # getting means on data for estimation (all folds except estimated)\n                nrows_cat = df_for_estimation.groupby(col)[target_col].count()\n                target_means_cats = df_for_estimation.groupby(col)[target_col].mean()\n                target_means_cats_adj = (target_means_cats*nrows_cat + \n                                         target_mean_global*alpha)\/(nrows_cat+alpha)\n                # Mapping means to estimated fold\n                encoded_col_train_part = df_estimated[col].map(target_means_cats_adj)\n                if add_random:\n                    encoded_col_train_part = encoded_col_train_part + normal(loc=rmean, scale=rstd, \n                                                                             size=(encoded_col_train_part.shape[0]))\n                # Saving estimated encodings for a fold\n                parts.append(encoded_col_train_part)\n            encoded_col_train = pd.concat(parts, axis=0)\n            encoded_col_train.fillna(target_mean_global, inplace=True)\n        else:\n            encoded_col_train = train_data[col].map(target_means_cats_adj)\n            if add_random:\n                encoded_col_train = encoded_col_train + normal(loc=rmean, scale=rstd, \n                                                               size=(encoded_col_train.shape[0]))\n\n        # Saving the column with means\n        encoded_col = pd.concat([encoded_col_train, encoded_col_test], axis=0)\n        encoded_col[encoded_col.isnull()] = target_mean_global\n        encoded_cols.append(pd.DataFrame({'mean_'+target_col+'_'+col:encoded_col}))\n    all_encoded = pd.concat(encoded_cols, axis=1)\n    return (all_encoded[:train_data.shape[0]], \n            all_encoded[train_data.shape[0]:])","e5ad1ed2":"features = [x for x in X_train.columns if x not in to_drop_columns+svd_dense_features.columns.tolist()]","b374bba2":"xgb_features = features","52ec58e4":"len(xgb_features)","e7779557":"params = {\n        'objective': 'reg:linear', #huber\n        'eval_metric':'rmse',\n        'eta': 0.01,\n        'tree_method':'gpu_hist',\n        'max_depth': 9,  \n        'subsample': 0.85,  \n        'colsample_bytree': 0.7,     \n        'alpha': 0.01,  \n    } \n\noof_train_xgb = np.zeros((X_train.shape[0]))\noof_test_xgb = []\nqwks = []\n\ni = 0\ntest_set = xgb.DMatrix(X_test[xgb_features])\n\nfor n_fold, (train_idx, valid_idx) in enumerate(split_index):  \n    X_tr = X_train.iloc[train_idx]\n    X_val = X_train.iloc[valid_idx]\n    \n    y_tr = X_tr['AdoptionSpeed'].values    \n    y_val = X_val['AdoptionSpeed'].values\n        \n    d_train = xgb.DMatrix(X_tr[xgb_features], y_tr)\n    d_valid = xgb.DMatrix(X_val[xgb_features], y_val)\n    watchlist = [d_valid]\n    since = time.time()\n    print('training XGB:')\n    model = xgb.train(params, d_train, num_boost_round = 10000, evals=[(d_valid,'val')],\n                     early_stopping_rounds=100, #feval=xgb_eval_kappa,\n                     verbose_eval=500)\n    \n    val_pred = model.predict(d_valid)\n    test_pred = model.predict(test_set)\n    \n    oof_train_xgb[valid_idx] = val_pred\n    oof_test_xgb.append(test_pred)\n    \n#     hist = histogram(X_tr['AdoptionSpeed'].astype(int), \n#                      int(np.min(X_train['AdoptionSpeed'])), \n#                      int(np.max(X_train['AdoptionSpeed'])))\n    tr_cdf = get_cdf(hist)\n    \n    pred_test_y_k = getTestScore2(val_pred, tr_cdf)\n    qwk = quadratic_weighted_kappa(X_val['AdoptionSpeed'].values, pred_test_y_k)\n    qwks.append(qwk)\n    print(\"QWK_2 = \", qwk,'elapsed time:',time.time()-since)","2cdcb57a":"print('overall rmse: %.5f'%rmse(oof_train_xgb, X_train['AdoptionSpeed']))\nprint('mean QWK =', np.mean(qwks), 'std QWK =', np.std(qwks))","90653be6":"hist = histogram(X_train['AdoptionSpeed'].astype(int), \n                 int(np.min(X_train['AdoptionSpeed'])), \n                 int(np.max(X_train['AdoptionSpeed'])))\ntr_cdf = get_cdf(hist)","a2d3329d":"# valid\nfinal_pred = pd.DataFrame()\nfinal_pred['lgb34'] = getTestScore2(lgb34_oof, tr_cdf) \nfinal_pred['lgb50'] = getTestScore2(lgb50_oof, tr_cdf)\nfinal_pred['lgb121'] = getTestScore2(lgb121_oof, tr_cdf)\nfinal_pred['lgb201'] = getTestScore2(lgb201_oof, tr_cdf)\nfinal_pred['xgb'] = getTestScore2(oof_train_xgb, tr_cdf)\nfinal_pred['nlp_pred'] = getTestScore2(oof_train_nlp[:,-1], tr_cdf)","8afb6c50":"final_pred['pred'] = final_pred[['lgb34','lgb50','lgb121','lgb201','xgb','nlp_pred']].mode(axis=1)[0]","72c2956c":"qwk = quadratic_weighted_kappa(X_train['AdoptionSpeed'].values, final_pred['pred'])\nprint(qwk)","f05a1ab6":"final_pred = pd.DataFrame()\nfinal_pred['lgb34'] = getTestScore2(lgb34_test, tr_cdf) \nfinal_pred['lgb50'] = getTestScore2(lgb50_test, tr_cdf)\nfinal_pred['lgb121'] = getTestScore2(lgb121_test, tr_cdf)\nfinal_pred['lgb201'] = getTestScore2(lgb201_test, tr_cdf)\nfinal_pred['xgb'] = getTestScore2(np.mean(oof_test_xgb,axis=0), tr_cdf)\nfinal_pred['nlp_pred'] = getTestScore2(oof_test_nlp[:,-1], tr_cdf)\nfinal_pred['pred'] = final_pred[['lgb34','lgb50','lgb121','lgb201','xgb','nlp_pred']].mode(axis=1)[0]","be8e0324":"np.corrcoef([lgb34_test, \n             lgb50_test,\n             lgb121_test,\n             lgb201_test,\n             np.mean(oof_test_xgb,axis=0),\n             oof_test_nlp[:,-1]])","77229024":"X_train_stacking = np.vstack([lgb34_oof, \n                              lgb50_oof,\n                              lgb121_oof, \n                              lgb201_oof, \n                              oof_train_xgb,\n                              oof_train_nlp[:,-1]]).T\nX_test_stacking = np.vstack([lgb34_test,\n                             lgb50_test,\n                             lgb121_test,\n                             lgb201_test,\n                             np.mean(oof_test_xgb,axis=0),\n                             oof_test_nlp[:,-1]]).T\n\nstacking_train = np.zeros((X_train.shape[0]))\nstacking_test = []\nrmses, qwks = [], []\n\nfor n_fold, (train_idx, valid_idx) in enumerate(split_index):\n    \n    X_tr = X_train_stacking[train_idx]\n    X_val = X_train_stacking[valid_idx]\n    \n    y_tr = X_train.iloc[train_idx]['AdoptionSpeed'].values    \n    y_val = X_train.iloc[valid_idx]['AdoptionSpeed'].values\n        \n    since = time.time()\n    \n    print('training Ridge:')\n    model = Ridge(alpha=1)\n    model.fit(X_tr, y_tr)\n    print(model.coef_)\n    \n    val_pred = model.predict(X_val)\n    test_pred = model.predict(X_test_stacking)\n    \n    stacking_train[valid_idx] = val_pred\n    stacking_test.append(test_pred)\n    loss = rmse(Y_train[valid_idx], val_pred)\n    hist = histogram(y_tr.astype(int), \n                     int(np.min(X_train['AdoptionSpeed'])), \n                     int(np.max(X_train['AdoptionSpeed'])))\n    tr_cdf = get_cdf(hist)\n    \n    pred_test_y_k = getTestScore2(val_pred, tr_cdf)\n    qwk = quadratic_weighted_kappa(y_val, pred_test_y_k)\n    qwks.append(qwk)\n    rmses.append(loss)\n    print(\"RMSE=\",loss, \"QWK_2 = \", qwk,'elapsed time:',time.time()-since)\nstacking_test = np.mean(stacking_test, axis=0)\nprint('mean rmse:',np.mean(rmses), 'rmse std:', np.std(rmses))\nprint('mean qwk:', np.mean(qwks), 'qwk std:', np.std(qwks))","2a08f654":"# Compute QWK based on OOF train predictions:\nhist = histogram(X_train['AdoptionSpeed'].astype(int), \n                 int(np.min(X_train['AdoptionSpeed'])), \n                 int(np.max(X_train['AdoptionSpeed'])))\ntr_cdf = get_cdf(hist)\ntrain_predictions = getTestScore2(stacking_train, tr_cdf)\ntest_predictions = getTestScore2(stacking_test, tr_cdf)\nqwk = quadratic_weighted_kappa(X_train['AdoptionSpeed'].values, train_predictions)\nprint(\"QWK = \", qwk)","acb38ed4":"# Distribution inspection of original target and predicted train and test:\n\n# print(\"True Distribution:\")\n# print(pd.value_counts(X_train['AdoptionSpeed'], normalize=True).sort_index())\n# print(\"\\nTrain Predicted Distribution:\")\n# print(pd.value_counts(train_predictions, normalize=True).sort_index())\n# print(\"\\nTest Predicted Distribution:\")\n# print(pd.value_counts(test_predictions, normalize=True).sort_index())","67b965c8":"# Generate submission:\n\nsubmission = pd.DataFrame({'PetID': test['PetID'].values, 'AdoptionSpeed': stacking_test.astype(np.int32)})\n# submission.head()\nsubmission.to_csv('submission.csv', index=False)","f2e4a4f1":"# result = pd.read_csv('..\/input\/test_solution.csv')\n\n# predictions = pd.DataFrame({'PetID': test['PetID'].values, \n#                             'vote': final_pred['pred'].values.astype(np.int32),\n#                             'stacking': stacking_test.astype(np.int32)})\n\n# pb = result[result['Usage']=='Public']\n# pb = pb.merge(predictions, on='PetID')\n\n# pb.head()\n\n# qwk = quadratic_weighted_kappa(pb['AdoptionSpeed'], pb['vote'])\n# print(\"QWK = \", qwk)\n\n# qwk = quadratic_weighted_kappa(pb['AdoptionSpeed'], pb['stacking'])\n# print(\"QWK = \", qwk)","d26c8d08":"## Catboost","3276fd36":"## imports and functions","2e422cc6":"### load mapping dictionaries:","23dd0c98":"## voting","4ae5cc18":"### model training:","08eef0e2":"## NN model","d20223aa":"### train\/test split:","ab80fa95":"### group extracted features by PetID:","7da33629":"## image feature","5938babf":"## merge all features","59e45c8b":"### split data","2310412b":"## meta & senti feature\n\nAfter taking a look at the data, we know its structure and can use it to extract additional features and concatenate them with basic train\/test DFs.","54e88a4f":"## xgb model","f4be4a98":"## read data","1231f004":"## LGB Model","704530ba":"## stacking","5ab67186":"## LDA feature","c352930e":"## text features","40daa274":"## feature engineering:","e772bb51":"### description","4ad47457":"### external data"}}