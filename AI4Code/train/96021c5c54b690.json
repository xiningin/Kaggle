{"cell_type":{"575b761b":"code","b8df7560":"code","6daf6bc0":"code","70980a9c":"code","b3ad5a1d":"code","5f252151":"code","f1ad69fe":"code","889ef35b":"code","2a7419b8":"code","f6398fc1":"code","034dab60":"code","b7fe0b43":"code","69fca04f":"code","d0ab2ad1":"code","461ba0da":"code","097b2c09":"code","2e50c576":"code","ee5dcad5":"code","82ecf3db":"code","4946c326":"code","0bf76885":"code","bae9ec34":"code","01a4cf66":"code","85cefd12":"code","17b15d1a":"code","51ca104d":"code","e3d3477b":"code","ea0098af":"code","14e22c43":"code","c58bd24f":"code","46083d31":"code","3be5d68b":"code","f97708cd":"code","1da73069":"code","48f37af7":"code","9ab730ca":"code","a770d4c8":"code","a9245294":"code","5b3f929e":"code","5cf193e0":"code","dc675f04":"code","113f0126":"code","3868ba83":"code","90f3bad3":"code","76dfbec1":"code","6cff64bb":"code","f3445449":"code","78518b14":"code","81f73818":"code","639efa74":"code","5e17682f":"code","d392e837":"code","2f9e8a13":"code","2fa0e619":"code","d9ad7dad":"code","37c1d295":"code","5980e67b":"code","fa2e71a9":"code","5295cc30":"code","317cd5f3":"code","3c6d98f6":"code","728c194a":"markdown","1ec61a96":"markdown","b9fd888d":"markdown","619353f5":"markdown","7939bb3c":"markdown","343fea0c":"markdown","72b32574":"markdown","2199ca67":"markdown","72e3edb3":"markdown","c850d351":"markdown","cc4bf0e0":"markdown","31702fbc":"markdown","13ed1a88":"markdown","00507968":"markdown","901f9b7b":"markdown","8b5d9791":"markdown","b592a90b":"markdown","d23b0804":"markdown","64a674e6":"markdown","fe9e6a4e":"markdown","5f542225":"markdown","9287dbb1":"markdown","83df1372":"markdown","336d9e72":"markdown","2cf2ed03":"markdown","d77aa0d8":"markdown","2ea83b39":"markdown","53ee17f7":"markdown","3f879bae":"markdown","533a3616":"markdown","7b68afa5":"markdown","92bb5710":"markdown","4aba84bd":"markdown","584d5383":"markdown","30df3db2":"markdown","4e052191":"markdown","f5476163":"markdown","6d76d986":"markdown","94189cd5":"markdown","c9b1d16f":"markdown","90132b31":"markdown","4f70f027":"markdown","2e2aea42":"markdown","6f8810fa":"markdown","ba61299c":"markdown","8684010f":"markdown","8989e26c":"markdown","fce71a46":"markdown","dc79b432":"markdown","f1d636f4":"markdown","fa63bd2c":"markdown","2620ce8b":"markdown","63c8ae99":"markdown","bc15baa7":"markdown","50e9c11d":"markdown","206300b4":"markdown","56a5a4f1":"markdown","e78223be":"markdown","643e286f":"markdown","a61697ba":"markdown","a5d01eb1":"markdown","9b54ebc1":"markdown","08eec662":"markdown","bca68286":"markdown","468d8d88":"markdown","0ac9af34":"markdown","a9b9ccaa":"markdown","c88131d0":"markdown","8fcc8388":"markdown","71bc81c5":"markdown","1da84264":"markdown","3e60bbca":"markdown","3c98cfb4":"markdown","87e52cc2":"markdown","699ad37a":"markdown","26881de3":"markdown","28fa4e48":"markdown","9a522138":"markdown","5d20510d":"markdown","d3092f9f":"markdown","93524f80":"markdown","c32cada5":"markdown","9172c67f":"markdown","d91e7aa8":"markdown","b24b1e9a":"markdown","6e48eafe":"markdown","b8318e9d":"markdown","a8601dea":"markdown","ba60d56e":"markdown","dcda4452":"markdown","0651f1cd":"markdown","7663e99b":"markdown","31b19a6e":"markdown","bd28ac88":"markdown","b747102b":"markdown","e7aa76af":"markdown","728758bb":"markdown","8e8a1c7c":"markdown","ebe037bf":"markdown","82a8f3ce":"markdown","359338a7":"markdown","8ede833d":"markdown","84124352":"markdown","b683ad84":"markdown","91a52bb1":"markdown","c096cfe1":"markdown","e466a7ff":"markdown","95816a9c":"markdown","fa931ff9":"markdown","be702fc7":"markdown","30a60719":"markdown","f580eab0":"markdown","ea82d366":"markdown","d2ddbef9":"markdown","d45eaa3f":"markdown","f272b741":"markdown","6b73f5f9":"markdown","9c1401d5":"markdown","98648497":"markdown","1ff5c84f":"markdown","f11cd70f":"markdown","8df4bcb9":"markdown","db984d23":"markdown","37d78280":"markdown","6c0cd17e":"markdown","cd848637":"markdown","d26222c3":"markdown","f6221d4d":"markdown","dc5b96af":"markdown","fd2adc07":"markdown","6dc8c34b":"markdown","302839ed":"markdown","cac71d89":"markdown","c51584c8":"markdown","aa33a2a9":"markdown","d45c8153":"markdown","935de818":"markdown","d889865d":"markdown","2871f20d":"markdown","4cf9ba23":"markdown","91174b10":"markdown","5edc5163":"markdown","d7104289":"markdown","b781cee6":"markdown","ebbecaa4":"markdown","3e66afb6":"markdown","9bcbc374":"markdown","dc71c35e":"markdown","e874bd82":"markdown","c7d833ae":"markdown","faf422fe":"markdown","6be6a253":"markdown","ce73a695":"markdown","6a378144":"markdown","bb25b2cd":"markdown","538f027c":"markdown","a28a3275":"markdown","cd845d9f":"markdown","e4508c25":"markdown","fcd23f18":"markdown","6497079f":"markdown","e642b2df":"markdown","08d793e6":"markdown","460aeb8b":"markdown","30b3856a":"markdown","dc263c74":"markdown"},"source":{"575b761b":"!python -m pip install matplotlib\n!python -m pip install nltk\n!python -m pip install numpy\n!python -m pip install pandas\n!python -m pip install seaborn\n!python -m pip install sklearn\n!python -m pip install tensorflow\n!python -m pip install transformers","b8df7560":"import re\n\nimport matplotlib.pyplot as plt\nimport nltk\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport tensorflow as tf\nfrom sklearn import feature_extraction\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom sklearn.metrics import (accuracy_score, confusion_matrix, precision_score,\n                             recall_score)\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom transformers import TFTrainer, TFTrainingArguments","6daf6bc0":"nltk.download(\"punkt\")\nnltk.download(\"stopwords\")\nnltk.download(\"wordnet\")","70980a9c":"df = pd.read_csv(\n    \"..\/input\/sms-spam-collection-dataset\/spam.csv\", encoding=\"latin-1\"\n)\ndf.head(n=10)","b3ad5a1d":"df.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], inplace=True, axis=1)\ndf.rename({\"v1\": \"is_spam\", \"v2\": \"content\"}, axis=1, inplace=True)\ndf[\"is_spam\"].replace({\"ham\": 0, \"spam\": 1}, inplace=True)\ndf.head(n=10)","5f252151":"df.shape","f1ad69fe":"df[\"nwords\"] = df[\"content\"].apply(lambda s: len(re.findall(r\"\\w+\", s)))\ndf[\"message_len\"] = df[\"content\"].apply(len)\ndf[\"nupperchars\"] = df[\"content\"].apply(\n    lambda s: sum(1 for c in s if c.isupper())\n)\ndf[\"nupperwords\"] = df[\"content\"].apply(\n    lambda s: len(re.findall(r\"\\b[A-Z][A-Z]+\\b\", s))\n)\ndf[\"is_free_or_win\"] = df[\"content\"].apply(\n    lambda s: int(\"free\" in s.lower() or \"win\" in s.lower())\n)\ndf[\"is_url\"] = df[\"content\"].apply(\n    lambda s: 1\n    if re.search(\n        r\"http[s]?:\/\/(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\",\n        s,\n    )\n    else 0\n)\ndf.head(n=25)","889ef35b":"n_sms = pd.value_counts(df[\"is_spam\"], sort=True)\nn_sms.plot(kind=\"pie\", labels=[\"ham\", \"spam\"], autopct=\"%1.0f%%\")\n\nplt.title(\"SMS Distribution\")\nplt.ylabel(\"\")\nplt.show()","2a7419b8":"from collections import Counter\n\ndf1 = pd.DataFrame.from_dict(\n    Counter(\" \".join(df[df['is_spam'] == 0][\"content\"]).split()).most_common(20)\n)\ndf1 = df1.rename(columns={0: \"word_in_ham\", 1 : \"frequency\"})\n                 \ndf2 = pd.DataFrame.from_dict(\n    Counter(\" \".join(df[df['is_spam'] == 1][\"content\"]).split()).most_common(20)\n)\ndf2 = df2.rename(columns={0: \"word_in_spam\", 1 : \"frequency\"})","f6398fc1":"df1.plot.bar(legend=False)\nplt.xticks(np.arange(len(df1[\"word_in_ham\"])), df1[\"word_in_ham\"])\nplt.title(\"Word Frequency in Ham SMS.\")\nplt.xlabel(\"Words\")\nplt.ylabel(\"Frequency\")\nplt.show()","034dab60":"df2.plot.bar(legend=False, color=\"orange\")\nplt.xticks(np.arange(len(df2[\"word_in_spam\"])), df2[\"word_in_spam\"])\nplt.title(\"Word Frequency in Spam SMS.\")\nplt.xlabel(\"Word\")\nplt.ylabel(\"Frequency\")\nplt.show()","b7fe0b43":"_, ax = plt.subplots(figsize=(10, 4))\nsns.kdeplot(\n    df.loc[df.is_spam == 0, \"message_len\"],\n    shade=True,\n    label=\"Ham\",\n    clip=(-50, 250),\n)\nsns.kdeplot(df.loc[df.is_spam == 1, \"message_len\"], shade=True, label=\"Spam\")\nax.set(\n    xlabel=\"Length\",\n    ylabel=\"Density\",\n    title=\"Length of SMS.\",\n)\nax.legend(loc=\"upper right\")\nplt.show()","69fca04f":"_, ax = plt.subplots(figsize=(10, 4))\nsns.kdeplot(\n    df.loc[df.is_spam == 0, \"nwords\"],\n    shade=True,\n    label=\"Ham\",\n    clip=(-10, 50),\n)\nsns.kdeplot(df.loc[df.is_spam == 1, \"nwords\"], shade=True, label=\"Spam\")\nax.set(\n    xlabel=\"Words\",\n    ylabel=\"Density\",\n    title=\"Number of Words in SMS.\",\n)\nax.legend(loc=\"upper right\")","d0ab2ad1":"_, ax = plt.subplots(figsize=(10, 4))\nsns.kdeplot(\n    df.loc[df.is_spam == 0, \"nupperwords\"],\n    shade=True,\n    label=\"Ham\",\n    clip=(0, 35),\n)\nsns.kdeplot(df.loc[df.is_spam == 1, \"nupperwords\"], shade=True, label=\"Spam\")\nax.set(\n    xlabel=\"Uppercased Words\",\n    ylabel=\"Density\",\n    title=\"Number of Uppercased Words.\",\n)\nax.legend(loc=\"upper right\")","461ba0da":"_, ax = plt.subplots(figsize=(10, 5))\nax = sns.scatterplot(x=\"message_len\", y=\"nupperchars\", hue=\"is_spam\", data=df)\nax.set(\n    xlabel=\"Characters\",\n    ylabel=\"Uppercase Characters\",\n    title=\"Number of Uppercased Characters in SMS.\",\n)\nax.legend(loc=\"upper right\")\nplt.show()","097b2c09":"_, ax = plt.subplots(figsize=(10, 4))\ngrouped_data = (\n    df.groupby(\"is_spam\")[\"is_free_or_win\"]\n    .value_counts(normalize=True)\n    .rename(\"Percentage of Group\")\n    .reset_index()\n)\nprint(grouped_data)\n\nax.set(\n    title=\"Distribution of FREE\/WIN Words Between Spam and Ham\"\n)\n\nsns.barplot(\n    x=\"is_spam\",\n    y=\"Percentage of Group\",\n    hue=\"is_free_or_win\",\n    data=grouped_data,\n)\nplt.show()","2e50c576":"_, ax = plt.subplots(figsize=(10, 4))\ngrouped_data = (\n    df.groupby(\"is_spam\")[\"is_url\"]\n    .value_counts(normalize=True)\n    .rename(\"Percentage of Group\")\n    .reset_index()\n)\nprint(grouped_data)\n\nax.set(\n    title=\"Distribution of URL Between Spam and Ham\"\n)\n\nsns.barplot(\n    x=\"is_spam\",\n    y=\"Percentage of Group\",\n    hue=\"is_url\",\n    data=grouped_data,\n)\nplt.show()","ee5dcad5":"from nltk import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\ndf[\"content\"] = df[\"content\"].apply(\n    lambda row: re.sub(r\"[^a-zA-Z]+\", \" \", row)  \n)\ndf[\"content\"] = df[\"content\"].apply(lambda row: word_tokenize(row))\ndf[\"content\"] = df[\"content\"].apply(\n    lambda row: [\n        token for token in row if token not in set(stopwords.words(\"english\"))\n    ]\n)\ndf[\"content\"] = df[\"content\"].apply(\n    lambda row: \" \".join([WordNetLemmatizer().lemmatize(word) for word in row])\n)\ndf.head(n=25)","82ecf3db":"X_train, X_test, y_train, y_test = train_test_split(\n    df[\"content\"], df[\"is_spam\"], stratify=df[\"is_spam\"],test_size=0.2\n)","4946c326":"print(f\"Training data: {len(X_train)} (80%)\")\nprint(f\" Testing data: {len(X_test)} (20%)\")","0bf76885":"from transformers import BertTokenizerFast\n\ntokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\ntokenizer","bae9ec34":"max_len = 0\nfor row in X_train:\n    max_len = max(max_len, len(tokenizer.encode(row)))\nprint(f\"Max sentence length (train): {max_len}\")\n\nmax_len = 0\nfor row in X_test:\n    max_len = max(max_len, len(tokenizer.encode(row)))\nprint(f\"Max sentence length (test): {max_len}\")","01a4cf66":"train_encodings = tokenizer(\n    X_train.tolist(),\n    max_length=96,\n    padding=\"max_length\",\n    truncation=True,\n)\ntest_encodings = tokenizer(\n    X_test.tolist(),\n    max_length=96,\n    padding=\"max_length\",\n    truncation=True,\n)","85cefd12":"train_dataset = tf.data.Dataset.from_tensor_slices(\n    (dict(train_encodings), y_train)\n)\ntest_dataset = tf.data.Dataset.from_tensor_slices(\n    (dict(test_encodings), y_test)\n)","17b15d1a":"training_args = TFTrainingArguments(\n    output_dir=\"\/kaggle\/working\/sms\/results\/bert\",\n    num_train_epochs=8,\n    per_device_train_batch_size=16,\n    warmup_steps=500,\n    weight_decay=0.01,\n    logging_dir=\"\/kaggle\/working\/sms\/logs\/bert\",\n    logging_steps=10,\n)","51ca104d":"from transformers import TFBertForSequenceClassification\n\nwith training_args.strategy.scope():\n    model = TFBertForSequenceClassification.from_pretrained(\n        \"bert-base-uncased\"\n    )\n\ntrainer = TFTrainer(\n    model=model, args=training_args, train_dataset=train_dataset\n)\ntrainer.train()","e3d3477b":"trainer.save_model(\"\/kaggle\/working\/sms\/models\/bert\")\ntokenizer.save_pretrained(training_args.output_dir)","ea0098af":"preds, label_ids, metrics = trainer.predict(test_dataset)\npreds[:5]","14e22c43":"print(f\"Test dataset size: {len(y_test)}\")\nprint(f\" Predictions size: {len(preds)}\")","c58bd24f":"preds = preds[: len(y_test)]\nlen(preds)","46083d31":"preds = np.argmax(preds, axis=1)\npreds","3be5d68b":"plt.figure(figsize=(10, 4))\n\nheatmap = sns.heatmap(\n    data=pd.DataFrame(confusion_matrix(y_test, preds)),\n    annot=True,\n    fmt=\"d\",\n    cmap=sns.color_palette(\"Blues\", 50),\n)\nheatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), fontsize=14)\nheatmap.yaxis.set_ticklabels(\n    heatmap.yaxis.get_ticklabels(), rotation=0, fontsize=14\n)\n\nplt.title(\"Confusion Matrix\")\nplt.ylabel(\"Ground Truth\")\nplt.xlabel(\"Prediction\")","f97708cd":"print(f\"Precision: {precision_score(y_test, preds) * 100:.3f}%\")\nprint(f\"   Recall: {recall_score(y_test, preds) * 100 :.3f}%\")\nprint(f\" Accuracy: {accuracy_score(y_test, preds) * 100:.3f}%\")","1da73069":"from transformers import DistilBertTokenizerFast\n\ntokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\ntokenizer","48f37af7":"train_encodings = tokenizer(\n    X_train.tolist(),\n    max_length=96,\n    padding=\"max_length\",\n    truncation=True,\n)\ntest_encodings = tokenizer(\n    X_test.tolist(),\n    max_length=96,\n    padding=\"max_length\",\n    truncation=True,\n)","9ab730ca":"train_dataset = tf.data.Dataset.from_tensor_slices(\n    (dict(train_encodings), y_train)\n)\ntest_dataset = tf.data.Dataset.from_tensor_slices(\n    (dict(test_encodings), y_test)\n)","a770d4c8":"training_args = TFTrainingArguments(\n    output_dir=\"\/kaggle\/working\/sms\/results\/distilbert\",\n    num_train_epochs=8,\n    per_device_train_batch_size=16,\n    warmup_steps=500,\n    weight_decay=0.01,\n    logging_dir=\"\/kaggle\/working\/sms\/logs\/distilbert\",\n    logging_steps=10,\n)","a9245294":"from transformers import TFDistilBertForSequenceClassification\n\nwith training_args.strategy.scope():\n    model = TFDistilBertForSequenceClassification.from_pretrained(\n        \"distilbert-base-uncased\"\n    )\n\ntrainer = TFTrainer(\n    model=model, args=training_args, train_dataset=train_dataset\n)\ntrainer.train()","5b3f929e":"trainer.save_model(\"\/kaggle\/working\/sms\/models\/distilbert\")\ntokenizer.save_pretrained(training_args.output_dir)","5cf193e0":"preds, label_ids, metrics = trainer.predict(test_dataset)\npreds[:5]","dc675f04":"print(f\"Test dataset size: {len(y_test)}\")\nprint(f\" Predictions size: {len(preds)}\")","113f0126":"preds = preds[: len(y_test)]\nlen(preds)","3868ba83":"preds = np.argmax(preds, axis=1)","90f3bad3":"plt.figure(figsize=(10, 4))\n\nheatmap = sns.heatmap(\n    data=pd.DataFrame(confusion_matrix(y_test, preds)),\n    annot=True,\n    fmt=\"d\",\n    cmap=sns.color_palette(\"Blues\", 50),\n)\nheatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), fontsize=14)\nheatmap.yaxis.set_ticklabels(\n    heatmap.yaxis.get_ticklabels(), rotation=0, fontsize=14\n)\n\nplt.title(\"Confusion Matrix\")\nplt.ylabel(\"Ground Truth\")\nplt.xlabel(\"Prediction\")","76dfbec1":"print(f\"Precision: {precision_score(y_test, preds) * 100:.3f}%\")\nprint(f\"   Recall: {recall_score(y_test, preds) * 100 :.3f}%\")\nprint(f\" Accuracy: {accuracy_score(y_test, preds) * 100:.3f}%\")","6cff64bb":"from sklearn.neighbors import KNeighborsClassifier\n\nknn = GridSearchCV(\n    Pipeline(\n        [\n            (\"bow\", CountVectorizer()),\n            (\"tfidf\", TfidfTransformer()),\n            (\"clf\", KNeighborsClassifier()),\n        ]\n    ),\n    {\n        \"clf__n_neighbors\": (8, 15, 20, 25, 40, 55),\n    }\n)\nknn.fit(X=X_train, y=y_train)","f3445449":"preds = knn.predict(X_test)\npreds","78518b14":"plt.figure(figsize=(10, 4))\n\nheatmap = sns.heatmap(\n    data=pd.DataFrame(confusion_matrix(y_test, preds)),\n    annot=True,\n    fmt=\"d\",\n    cmap=sns.color_palette(\"Blues\", 50),\n)\nheatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), fontsize=14)\nheatmap.yaxis.set_ticklabels(\n    heatmap.yaxis.get_ticklabels(), rotation=0, fontsize=14\n)\n\nplt.title(\"Confusion Matrix\")\nplt.ylabel(\"Ground Truth\")\nplt.xlabel(\"Prediction\")","81f73818":"print(f\"Precision: {precision_score(y_test, preds) * 100:.3f}%\")\nprint(f\"   Recall: {recall_score(y_test, preds) * 100 :.3f}%\")\nprint(f\" Accuracy: {accuracy_score(y_test, preds) * 100:.3f}%\")","639efa74":"from sklearn.naive_bayes import MultinomialNB\n\nmnbayes = GridSearchCV(\n    Pipeline(\n        [\n            (\"bow\", CountVectorizer()),\n            (\"tfidf\", TfidfTransformer()),\n            (\"clf\", MultinomialNB()),\n        ]\n    ),\n    {\n        \"tfidf__use_idf\": (True, False),\n        \"clf__alpha\": (0.1, 1e-2, 1e-3),\n        \"clf__fit_prior\": (True, False),\n    },\n)\nmnbayes.fit(X=X_train, y=y_train)","5e17682f":"mnbayes.best_params_","d392e837":"print(f\"{mnbayes.best_score_ * 100:.3f}%\") ","2f9e8a13":"preds = mnbayes.predict(X_test)\npreds","2fa0e619":"plt.figure(figsize=(10, 4))\n\nheatmap = sns.heatmap(\n    data=pd.DataFrame(confusion_matrix(y_test, preds)),\n    annot=True,\n    fmt=\"d\",\n    cmap=sns.color_palette(\"Blues\", 50),\n)\nheatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), fontsize=14)\nheatmap.yaxis.set_ticklabels(\n    heatmap.yaxis.get_ticklabels(), rotation=0, fontsize=14\n)\n\nplt.title(\"Confusion Matrix\")\nplt.ylabel(\"Ground Truth\")\nplt.xlabel(\"Prediction\")","d9ad7dad":"print(f\"Precision: {precision_score(y_test, preds) * 100:.3f}%\")\nprint(f\"   Recall: {recall_score(y_test, preds) * 100 :.3f}%\")\nprint(f\" Accuracy: {accuracy_score(y_test, preds) * 100:.3f}%\")","37c1d295":"from sklearn.svm import SVC\n\nsvc = GridSearchCV(\n    Pipeline(\n        [\n            (\"bow\", CountVectorizer()),\n            (\"tfidf\", TfidfTransformer()),\n            (\"clf\", SVC(gamma=\"auto\", C=1000)),\n        ]\n    ),\n    dict(tfidf=[None, TfidfTransformer()], clf__C=[500, 1000, 1500]),\n)\nsvc.fit(X=X_train, y=y_train)","5980e67b":"svc.best_params_","fa2e71a9":"print(f\"{svc.best_score_ * 100:.3f}%\") ","5295cc30":"preds = svc.predict(X_test)\npreds","317cd5f3":"plt.figure(figsize=(10, 4))\n\nheatmap = sns.heatmap(\n    data=pd.DataFrame(confusion_matrix(y_test, preds)),\n    annot=True,\n    fmt=\"d\",\n    cmap=sns.color_palette(\"Blues\", 50),\n)\nheatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), fontsize=14)\nheatmap.yaxis.set_ticklabels(\n    heatmap.yaxis.get_ticklabels(), rotation=0, fontsize=14\n)\n\nplt.title(\"Confusion Matrix\")\nplt.ylabel(\"Ground Truth\")\nplt.xlabel(\"Prediction\")","3c6d98f6":"print(f\"Precision: {precision_score(y_test, preds) * 100:.3f}%\")\nprint(f\"   Recall: {recall_score(y_test, preds) * 100 :.3f}%\")\nprint(f\" Accuracy: {accuracy_score(y_test, preds) * 100:.3f}%\")","728c194a":"As we said before, let's use grid search techniques using cross-validation to determine the optimal value of the $\\alpha$ hyper-parameter of our model and train this model on this hyper-parameter:","1ec61a96":"Here it is the case, we have 13 additional embeddings. Let's make sure to delete them:","b9fd888d":"# DistilBERT\n","619353f5":"Let's see if the number of words has an influence on SMS spam or ham:","7939bb3c":"We can see that BERT and DistilBERT are the ML classification algorithms that provide the best results. However, based on the scores, we can see that there is no significant difference in precision and accuracy between these algorithms, it is only a few percent!","343fea0c":"We can see that the dataset has become much clearer.","72b32574":"It should be noted above that we have what are called **logits** (i.e. \t$\\exists n, n \\in ]-\\infty, \\infty[$).\n\nAnother thing we have to be careful of, is that no additional embeddings have been generated after the predictions:","2199ca67":"This model being trained, let's save it, as well as its configuration to be able to load it directly when needed.","72e3edb3":"Let's see if we also have additional embeddings:","c850d351":"For our training dataset, $\\alpha$ must be equal to $10^{-2}$.","cc4bf0e0":"### Confusion Matrix","31702fbc":"## Loading","13ed1a88":"## Fine-Tuning and Training","00507968":"Here, we already have the final predictions given by the logit probabilities.","901f9b7b":"### Scores","8b5d9791":"## Measurement of Predictions","b592a90b":"The mean cross-validated score is therefore 98.587%","d23b0804":"Our dataset initially composed of 5572 lines is now split into two smaller datasets according to the following proportions:","64a674e6":"Tokenization will allow us to feed batches of sequences into the model at the same time, only if these two conditions are met:\n\n1.   **the SMS are padded to the same length**;\n2.   **the SMS are truncated to be not longer model's maximum input length**.\n\nTo do the tokenization of our datasets, we also need to choose a pre-trained model. \nFor this dataset, the basic model (`bert-base-uncased`) will be sufficient:\n","fe9e6a4e":"Let's also save our DistilBERT model and its configuration in a persistent way:","5f542225":"Our Multinomial Naive Bayes model being trained, we can now use it to predict if the SMS in our test dataset are spams or not:","9287dbb1":"## Tokenization","83df1372":"With this plot we can notice two things:\n1.   **Spam** messages are clustered together based on their length. But we can also see that some **spam** messages have more uppercased characters that others.\n2.   There is a linear pattern for **ham** messages that contains more uppercased character than others. \n\n","336d9e72":"Based on this pre-trained model, the encodings for our training and testing  datasets are generated as follows:","2cf2ed03":"## Contains \"free\" or \"win\"","d77aa0d8":"## Number of Words","2ea83b39":"# Creation of Training and Testing Datasets","53ee17f7":"# Conclusion","3f879bae":"We are now ready to fine-tuning and training our DistilBERT model!","533a3616":"## Tokenization","7b68afa5":"Similar to what we saw before, let's associate these codings to a `TensorSliceDataset` object in order to Fine-Tuning and train our model.","92bb5710":"Since this dataset has a lot of abbreviations, we will not apply stemming, but only lemmatization.\n\nAs a quick reminder:\n\n*   **Stemming**: NLP algorithm that **cuts the end or the beginning of a word** based on a list of common prefixes that can be found in an inflected word (e.g., `Stemming[change, changing, changes]` \u27a1\ufe0f chang).\n*   **Lemmatization**: NLP algorithm that **looks at the morphological analysis of words** based on detailed dictionaries, in order to relate the shape of a word to its lemma (e.g., `Lemmatization[change, changing, changes]` \u27a1\ufe0f change).\n\nAs a second pre-processing of these data, let's remove the stop words, punctuation and digits from each SMS, without forgetting to apply lemmatization to them:","4aba84bd":"### Predictions","584d5383":"Following Fine-Tuning and our datasets, the training of the BERT model can be done as follows:","30df3db2":"We can see that the columns corresponding to our features have been created.","4e052191":"Sketch the confusion matrix will allow us to measures the quality of the classification system:","f5476163":"Using the confusion matrix, measures of the quality of the classification system are given:","6d76d986":"  | ML Algo                  |                                                                  Accuracy |  Precision |      Recall |\n  | -----------------------: | ------------------------------------------------------------------------: | ---------: | ----------: |\n  | BERT                     |                                                                  **99.910** |**100.000** |  **99.329** |\n  | DistilBERT               |                                                                    99.641 |     98.658 |      98.658 |\n  | KNN                      |                                                                    95.695 |     98.095 |      69.128 |\n  | Multinomial Naive Bayes  |                                                                    98.117 |     95.070 |      90.604 |\n  | SVM                      |                                                                    98.655 |     98.551 |      91.275 |","94189cd5":"After sketching, we can see that stop words are the most frequent words in both spam and ham SMS.","c9b1d16f":"Using the confusion matrix, measures of the quality of the classification system are given:","90132b31":"Let's start by creating two DataFrames: \n\n1.   `df1`: will contain the words and their frequency in the SMS ham.\n2.   `df2`: will contain the words and their frequency in the SMS spam.\n","4f70f027":"The fine-tuning for DistilBERT is identical to BERT:","2e2aea42":"With this plot, we can notice two things:\n1.   There is **36.94% of spam** SMS that contains the words **\"free\"** or **\"win\"**.\n2.   There is only **2.69% of ham** SMS that contains the words **\"free\"** or **\"win\"**.\n\n","6f8810fa":"# Dataset","ba61299c":"### Normalization","8684010f":"Using the `pip` Python package manager, let's install all the necessary packages for this Notebook:","8989e26c":"### Confusion Matrix","fce71a46":"Using the confusion matrix, measures of the quality of the classification system are given:","dc79b432":"Following Fine-Tuning and our datasets, the training of the BERT model can be done as follows:","f1d636f4":"Our SVM model being trained, we can now use it to predict if the SMS in our test dataset are spams or not:","fa63bd2c":"## Fine-Tuning and Training","2620ce8b":"## Contains URL","63c8ae99":"Let's look at the score obtained by the predictions:","bc15baa7":"### Confusion Matrix","50e9c11d":"# Installing and Importing Packages\n","206300b4":"# Feature Engineering","56a5a4f1":"We are now ready to Fine-Tuning and training our BERT model!","e78223be":"With this plot, we notice two things:\n1.   In general, **spam** messages are **longer** than **ham** messages (that is normal due to the number of words).\n2.   **Spam** messages have around **150 characters**.\n\n","643e286f":"## SMS Distribution","a61697ba":"Above, each row of the confusion matrix corresponds to a real class and each column corresponds to an estimated class.\n\nThrough the matrix of confusion, we have:\n\n*  **964 SMS being ham were well predicted**: True Negative (TN);\n*  **2 ham SMS have been detected as spam** False Positive (FP);\n*  **2 spam SMS have been detected as ham** False Negative (FN);\n*  **147 spam SMS have been detected as spam** True Positive (TP).","a5d01eb1":"Here it is the case, we have 13 additional embeddings. Let's make sure to delete them:","9b54ebc1":"By Feature Engineering, we **refer to the creation of the features according to the raw data**. It is on the basis of these features that the training of the classification ML models will be done.\n\nAmong these features, we will create these:\n\n*   `nwords`: feature that will **contain the number of words** in an SMS.\n*   `message_len`: feature that will **contain the number of characters** in an SMS message.\n*   `nupperchars`: feature that will **contain the number of uppercase characters** in an SMS.\n*   `nupperwords`: feature that will **contain the number of uppercase words** in an SMS.\n*   `is_free_o_win`: feature that will **contain 1 if the SMS contains the words \"free\" and \"win\"; 0 otherwise**.\n*   `is_url`: feature that will **contain 1 if the SMS contains a URL; 0 otherwise**.\n\nThis translates as follows:","08eec662":"K-Nearest Neighbors (KNN) is an approach to data classification that estimates how likely a data point is to be a member of one group or the other depending on what group the data points nearest to it are in.","bca68286":"# Multinomial Naive Bayes Classifier","468d8d88":"With this plot, we can notice that there is a small pattern with the number of **uppercased words**. The **density is lower** which is normal due to the fact that there is **less spam** messages than **ham** messages.\n\nWe can also notice that the number of **uppercased words** is around **zero** for the **ham** messages.","0ac9af34":"With this plot, we can notice that **spam** SMS have more words than **ham** SMS.\n\n**Spam** SMS seem to have around **30 words**, where **ham** SMS seem to have around **10 words** to **25 words** and more.","a9b9ccaa":"Through the confusion matrix, we have:\n\n*   **964 SMS being ham were well predicted**: True Negative (TN);\n*   **2 ham SMS have been detected as spam**: False Positive (FP);\n*   **46 spam SMS have been detected as ham**: False Negative (FN);\n*   **103 spam SMS have been detected as spam**: True Positive (TP).","c88131d0":"As seen previously, let's measure SMS predictions as spam or ham.","8fcc8388":"### Normalization","71bc81c5":"**NOTE:** in a multi-class classification problem, these logits would be normalized with a `softmax` function.","1da84264":"### Confusion Matrix","3e60bbca":"Our KNN model being trained, we can now use it to predict if the SMS in our test dataset are spams or not:","3c98cfb4":"# Results","87e52cc2":"As we said before, let's use grid search techniques using cross-validation to determine the hyper-parameters of our model and train this model on them:\n\nTo tune the hyper-parameters of the KNN, it is recommended to use grid search techniques using cross-validation (**SEE:** [scikit-learn's documentation](https:\/\/scikit-learn.org\/stable\/modules\/cross_validation.html#cross-validation)) to evaluate the performance of the model on the data at each value.\n\nLet's use this technique to train our model according to the optimal value of the neighbors hyper-parameter:","699ad37a":"BERT is a bidirectional transformer pretrained using a combination of Masked Language Modeling (MLM) objective and Next Sentence Prediction (NSP) on a large corpus comprising the Book Corpus and Wikipedia.","26881de3":"**NOTE:** you can ignore the warnings.","28fa4e48":"As seen previously, let's measure SMS predictions as spam or ham.","9a522138":"Through the confusion matrix, we have:\n\n*   **964 SMS being ham were well predicted**: True Negative (TN);\n*   **2 ham SMS have been detected as spam**: False Positive (FP);\n*   **13 spam SMS have been detected as ham**: False Negative (FN);\n*   **136 spam SMS have been detected as spam**: True Positive (TP).","5d20510d":"In order to have a better ease of use, a first pre-processing of these data would be to apply the modifications mentioned above:","d3092f9f":"Now that we know a bit more about the organization of the dataset, it is good to know the percentage of spam SMS and ham SMS:","93524f80":"Here, we already have the final predictions given by the logit probabilities.","c32cada5":"Before we can Fine-Tuning and training our model, we must batched these encodings to a `TensorSliceDataset` object, so that each key in the batch encoding corresponds to a hyper-parameters named according to the model we are going to train:","9172c67f":"### Scores","d91e7aa8":"Let's look at the score obtained by the predictions:","b24b1e9a":"Let's see if the \"free\" and \"win\" words has an influence on SMS spam or ham:","6e48eafe":"## Word Frequency","b8318e9d":"In addition, we can get the mean cross-validated score of the estimator that was chosen by the search:","a8601dea":"### Predictions","ba60d56e":"From this Notebook, we started by loading a dataset of Spam SMS and created our features on the raw data using Feature Engineering. \n\nOnce our features were created, we analyzed the data made available on the basis of these features, before being able to do data preprocessing which consisted in removing the presence of stop words, punctuation, digits and lemmatize the words.\n\nIn addition, we learned how to fine-tuning different Machine Learning classification algorithms. To do this, it was useful for fine-tuning some of these algorithms to use search grid techniques using cross-validation to evaluate the performance of the model.\n\nBERT and DistilBERT are to be preferred when we would like to push performance to its maximum and to optimize the avoidance of True Positive misclassification (given by the recall score).\n\nHowever, even these algorithms are the best according to the scores, we can still apply Okhalm's razor principle. Indeed, if these few percent more can be neglected, classical classification algorithms such as Multinomial Naive Bayes and SVM can still be preferred because of their simplicity of understanding and implementation.","dcda4452":"Finally, to get a better idea on the amount of data made available, we can look at the shape of the DataFrame that defines the dataset:","0651f1cd":"**NOTE:** you can ignore the warnings.","7663e99b":"Using the confusion matrix, measures of the quality of the classification system are given: ","31b19a6e":"## Fine-Tuning and Training","bd28ac88":"### Predictions","b747102b":"# KNN","e7aa76af":"Let's see if a URL has an influence on SMS spam or ham:","728758bb":"## Number of Uppercased Words","8e8a1c7c":"Let's see if the length has an influence on SMS spam or ham:","ebe037bf":"## Number of Uppercased Characters","82a8f3ce":"Let's look at the score obtained by the predictions:","359338a7":"After loading this dataset, you can directly see some modifications to be made:\n\n*   **three** \"Unnamed\" **columns can be deleted**;\n*   **the spam column** (`v1`) a**nd the SMS content column** (`v2`) **can be renamed** to be more explicit;\n*  **the content of the spam column** (`v1`) **can be binarized** for better processing ease for ML algorithms.\n\n","8ede833d":"Out of curiosity, let us look at which were the hyper-parameters to be privileged for the training of the model with respect to our training dataset:","84124352":"The mean cross-validated score is therefore 98.519%","b683ad84":"## Measurement of Predictions","91a52bb1":"Through the confusion matrix, we have:\n\n*   **959 SMS being ham were well predicted**: True Negative (TN);\n*   **7 ham SMS have been detected as spam**: False Positive (FP);\n*   **14 spam SMS have been detected as ham**: False Negative (FN);\n*   **135 spam SMS have been detected as spam**: True Positive (TP).","c096cfe1":"[BERT Fine-Tuning Tutorial with PyTorch](http:\/\/mccormickml.com\/2019\/07\/22\/BERT-fine-tuning\/)\n\n[Naive Bayes & SVM Spam Filtering](https:\/\/www.kaggle.com\/pablovargas\/naive-bayes-svm-spam-filtering)\n\n[Starter: Neural Net w\/ 0.97 ROC-AUC - 99% accuracy](https:\/\/www.kaggle.com\/mrlucasfischer\/starter-neural-net-w-0-97-roc-auc-99-accuracy)","e466a7ff":"As for BERT, let's tokenize our dataset so that we can feed batches of sequences into the model at the same time.\n\nFor this dataset, the basic model (`distilbert-base-uncased`) will be sufficient:","95816a9c":"## Transformation of Labels and Encodings","fa931ff9":"## Transformation of Labels and Encodings","be702fc7":"It would still be possible to speculate on more pre-processing to be done (e.g., finding the original words based on abbreviations), but since a SMS is not a formal message, it may be wise to keep capital letters and abbreviations.","30a60719":"Here, we already have the final predictions given by the logit probabilities.","f580eab0":"With the NLTK's data downloader, we will install the following corpora and trained models:\n*   `punkt`: Punkt Tokenizer Models.\n*   `stopwords`: Stopwords Corpus.\n*   `wordnet`: WordNet-InfoContent.","ea82d366":"Before we can encode our datasets with BERT, **it is important to decide on a maximum sentence length for padding\/truncating to**. This will allow us to have a better speed for training and evaluation.\n\nTo do this, we will perform one tokenization pass of the datasets in order to measure the maximum sentence length:","d2ddbef9":"# BERT","d45eaa3f":"Let's start by loading our dataset and looking at the columns available to us:","f272b741":"Above, **87% of these SMS are ham and 13% of them are spam**.","6b73f5f9":"Let's see if the number of uppercased words has an influence on SMS spam or ham:","9c1401d5":"## Preprocessing","98648497":"## Measurement of Predictions","1ff5c84f":"Our DistilBERT model being trained, we can now use it to predict if the SMS in our test dataset are spams or not:","f11cd70f":"DistilBERT is a distilled version of BERT, which is smaller, faster, cheaper and lighter. This variant should have performance close to BERT.","8df4bcb9":"The necessary packages being installed, let's already import most of the packages for this Notebook:","db984d23":"With DistilBERT, we also have logits.","37d78280":"Now that the DataFrames have been created, let's sketch their corresponding graphs in order to look at their respective word frequencies:","6c0cd17e":"# Introduction\n\nThis Notebook is **mainly for educational purposes**. According to a dataset to classify Short Message Service (SMS) as spam or not, the goal will be to evaluate Bidirectional Encoder Representations from Transformers (BERT) with other Machine Learning (ML) classification algorithms.\n\nAmong these algorithms, four ML classification algorithms will be compared:\n\n1.   **DistilBERT**;\n2.   **K-Nearest Neighbors (KNN)**;\n3.   **Multinomial Naive Bayes**;\n4.   **Support Vector Model (SVM)**.\n\nWe had the opportunity to test those algorithms with [another dataset](https:\/\/www.kaggle.com\/rememberyou\/comparison-of-bert-and-other-ml-classification-ii) containing SMS samples as well.","cd848637":"## Measurement of Predictions","d26222c3":"The reason for these extra embeddings after training is due to a `huggingface\/transformers` bug, which should be fixed in the next releases.","f6221d4d":"Let's look at the score obtained by the predictions:","dc5b96af":"### Predictions","fd2adc07":"Above all, it is **important to analyze and understand the data** made available. Indeed, once a better understanding of the dataset is achieved, **we will be able to create the necessary features** for the dataset.\n\nThe dataset being loaded, we will analyze the following seven aspects:\n\n1.   **the SMS distribution**;\n2.   **the word frequency in spam and ham SMS**;\n3.   **the length of spam SMS compared to ham SMS**;\n4.   **the number of words in spam SMS compared to ham SMS**;\n5.   **the number of uppercase words in spam SMS compared to ham SMS**;\n6.   **the number of uppercase characters in spam SMS compared to ham SMS**;\n7.   **the content of the words \"free\" or \"win\" in the SMS**;\n8.   **the content of a URL in the SMS**.","6dc8c34b":"So we have a **dataset that contains 5572 rows and 2 columns**.","302839ed":"Before being able to train our model, it is necessary to split our dataset into a training and testing dataset:","cac71d89":"Let's convert these logits into probabilities and the latter into final predictions by taking the label for which the probability is highest:","c51584c8":"Since we have that the maximum length sentence is 93 for the training dataset and 93 for the testing dataset, **we will take a maximum length of 96 characters for both datasets**.","aa33a2a9":"### Scores","d45c8153":"The measurement of SMS predictions present in our test dataset as spam or ham, will allow us to make sure that the model is well trained.","935de818":"### Scores","d889865d":"# SVM","2871f20d":"# Objectives\n\nTo compare these algorithms, we will:\n\n*   **Do Feature Engineering**: create the features according to the raw data.\n*   **Analyze and understand the data** made available.\n*   **Pre-process these data according to the algorithm**: for instance, some of these algorithms only work with numerical values.\n*   **Do Fine-Tuning**: optimize the training parameters of the ML algorithm.\n*   **Compare the results** obtained.\n*   **Apply the Ockham's razor principle**: take the best and\/or simplest algorithm if there is no significant difference.","4cf9ba23":"### Predictions","91174b10":"Following Fine-Tuning and our datasets, the training of the DistilBERT model can be done as follows:","5edc5163":"# Analyze and Understanding Data","d7104289":"Let's look at the score obtained by the predictions.\n\nAs a quick reminder:\n\n1.   **Precision:** is the ratio between the True Positives and all the Positives.\n2.   **Recall:** is the measure of our model correctly identifying True Positives.\n3.   **Accuracy:** is the ratio of the total number of correct predictions and the total number of predictions.\n\nWhich gives us:","b781cee6":"With this plot, we can notice two things:\n1.   there is **2.55% of spam** that contains a URL;\n2.   there is only **97.45% of spam** that doesn't contains a URL.","ebbecaa4":"To get rid of these logits, the vector of raw (non-normalized) predictions generated by the classification model should be passed to a normalization function to convert logits to probabilities. As we use a binary classification, we should use the `sigmoid` function and then the conversion of the probabilities into final predictions is done by taking the label for which the probability is highest.\n\nWith the help of the `argmax` function from `numpy`, we can make a two-shot stone:","3e66afb6":"As with BERT, we will take a maximum length of 96 characters for both datasets to have a better speed for training and evaluation. \n\nBased on this pre-trained model, the encodings for our training and testing datasets are generated as follows:","9bcbc374":"## Length","dc71c35e":"### Scores","e874bd82":"## Fine-Tuning and Training","c7d833ae":"**Fine-tuning consists of generating embeddings specific to a task**. Since we would like to create embeddings specifically for a classification task, we will have to train our data only for this task. However, for a pre-trained BERT model that is best suited for multiple tasks, fine-tuning will not be possible. It will therefore be necessary to generate the BERT embeddings as features and pass them through an independent classifier (e.g., RandomForest).\n\nUsing the `TFTrainingArguments` class present in the `huggingface\/transformers` module, the Fine-Tuning can be done this way:","faf422fe":"As we said before, let's use grid search techniques using cross-validation to determine the hyper-parameters of our model and train this model on them:","6be6a253":"# References","ce73a695":"In ML, preprocessing data is a process of preparing raw data to make them suitable to a ML model. \n\nFrom a semantic point of view, our dataset has some drawbacks for a ML model:\n\n*   **presence of stop words** (e.g., so, is, a);\n*   **presence of punctuations and digits**;\n*   **words are not lemmatized.**","6a378144":"For this Notebook, the [SMS Spam Collection Dataset](https:\/\/www.kaggle.com\/uciml\/sms-spam-collection-dataset) dataset has been chosen. The main reasons for such a choice is that it contains a lot of data, and its columns are suitable for the comparison of the ML algorithms we want to make.\n\nIn this section we will load the dataset and apply minor preprocessing to the columns and values to make it easier to use.","bb25b2cd":"Above, each row of the confusion matrix corresponds to a real class and each column corresponds to an estimated class.\n\nThrough the matrix of confusion, we have:\n\n*  **966 SMS being ham were well predicted**: True Negative (TN);\n*  **0 ham SMS have been detected as spam** False Positive (FP);\n*  **1 spam SMS have been detected as ham** False Negative (FN);\n*  **148 spam SMS have been detected as spam** True Positive (TP).","538f027c":"As the features of our dataset have discrete frequency counts, we will use the Multinomial type of Naive Bayes Model.\n\nTo detect if a SMS is consider as spam or not, the Multinomial Naive Bayes classifier will use word counts in the content of the SMS with the help of the Bag-of-Words (BoW) method. This method, will elaborate a matrix of rows according to words, where each intersection corresponds to the frequency of occurrence of these words.","a28a3275":"## Fine-Tuning and Training","cd845d9f":"For our training dataset, $C$ must be equal to 1000 and we shouldn't transform the count matrix to a normalized term-frequency (tf) representation or for a term-frequency times inverse document-frequency (tf-idf) representation.","e4508c25":"Out of curiosity, let us look at which were the hyper-parameters to be privileged for the training of the model with respect to our training dataset:","fcd23f18":"## Measurement of Predictions","6497079f":"Let's see if the number of uppercased characters has an influence on SMS spam or ham:","e642b2df":"# Preprocessing Data","08d793e6":"### Confusion Matrix","460aeb8b":"Our BERT model being trained, we can now use it to predict if the SMS in our test dataset are spams or not:","30b3856a":"**NOTE:** in some use-cases, it can be interesting to split again the training dataset in order to create a validation dataset.  The validation dataset could be useful when we want to stop training a model when a certain precision is reached, to avoid overlearning. In our case, it may be preferable to use the training data set to train the model and achieve better accuracy.\n","dc263c74":"If we summarize the results obtained, here is what we get:"}}