{"cell_type":{"9c5e167f":"code","51445e8d":"code","b2342982":"code","bc379bf4":"code","df476386":"code","a38fc291":"code","8965efa4":"code","e592fe66":"code","3d8019b5":"code","0a9df731":"code","00752212":"code","480fcc4e":"code","4314d0b5":"code","338a8703":"code","de924a11":"code","29726fb3":"code","bc7bb96e":"code","79469783":"code","aa4cb05e":"code","010d2f3c":"code","79b30a07":"code","e345fc96":"code","a9a4efd5":"code","353408b3":"code","e8900f3b":"code","08d701c3":"code","6cf545b1":"code","3510d8bb":"code","86501bc8":"code","d61c7edd":"code","dd54a83c":"code","adaebd03":"code","1f32f2c9":"code","fd37f153":"code","1d5ea754":"code","6b2a8c60":"code","ff0dfcac":"code","84542e4b":"code","9da69e35":"markdown","9a5d3e59":"markdown","f616b119":"markdown","75e3efa4":"markdown","fd2c34fd":"markdown","57c1cb6d":"markdown","5e444656":"markdown","355ebd05":"markdown","c8e19fa5":"markdown","dd9f56d8":"markdown","9b4b611f":"markdown","4c073286":"markdown","f5ceabf6":"markdown"},"source":{"9c5e167f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","51445e8d":"import pandas as pd\nimport numpy as np\nimport random\nimport os\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n\nimport lightgbm as lgb\nimport catboost as ctb\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz\n\nimport graphviz\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.simplefilter('ignore')","b2342982":"TARGET = 'Survived'\n\nN_ESTIMATORS = 1000\nN_SPLITS = 10\nSEED = 2021\nEARLY_STOPPING_ROUNDS = 100\nVERBOSE = 100","bc379bf4":"#\ub79c\ub364 \uc2dc\ub4dc \uc0dd\uc131\ndef set_seed(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    \nset_seed(SEED)","df476386":"train_df = pd.read_csv('..\/input\/tabular-playground-series-apr-2021\/train.csv')\ntest_df = pd.read_csv('..\/input\/tabular-playground-series-apr-2021\/test.csv')\nsubmission = pd.read_csv('..\/input\/tabular-playground-series-apr-2021\/sample_submission.csv')\n#test_df['Survived'] = pd.read_csv(\"..\/input\/submission-merged3\/submission_merged3.csv\")['Survived']\n\nall_df = pd.concat([train_df, test_df]).reset_index(drop=True)\n#reset_index : \uc778\ub371\uc2a4\ub97c \uc138\ud305\ud55c\ub2e4. drop=True\ub97c \ud558\uba74 \uc778\ub371\uc2a4\ub97c \uc138\ud305\ud55c\uac78 \uc0ad\uc81c\ud568. \n","a38fc291":"print('Rows and Columns in train dataset:', train_df.shape)\nprint('Rows and Columns in test dataset:', test_df.shape)","8965efa4":"print('Missing values per columns in train dataset')\nfor col in train_df.columns:\n    temp_col = train_df[col].isnull().sum()\n    print(f'{col}: {temp_col}')\nprint()\nprint('Missing values per columns in test dataset')\nfor col in test_df.columns:\n    temp_col = test_df[col].isnull().sum()\n    print(f'{col}: {temp_col}')","e592fe66":"#\ub098\uc774\ub294 \ub098\uc774\uc758 \ud3c9\uade0\uce58\ub85c \ucc44\uc6b4\ub2e4.\nall_df['Age'] = all_df['Age'].fillna(all_df['Age'].mean())\n\n#cabin\uc740 \ubb38\uc790\uc5f4\uc744 \ubd84\ud560\ud558\uace0, \uc81c\uc77c \uccab\ubc88\uc9f8 \uae00\uc790\ub97c \ub530\uc640\uc11c \ub123\ub294\ub2e4. \uacb0\uce21\uce58\uc5d4 X\ub97c \ub123\ub294\ub2e4.\n#strip() : \uc591\ucabd \uacf5\ubc31\uc744 \uc9c0\uc6b4\ub2e4. \uc5ec\uae30\uc11c\ub290 x[0]\uc678\uc5d4 \ub2e4 \uc9c0\uc6b0\ub294\ub4ef. \nall_df['Cabin'] = all_df['Cabin'].fillna('X').map(lambda x: x[0].strip())\n\n\n#print(all_df['Ticket'].head(10))\n#Ticket, fillna with 'X', split string and take first split \n#split() : \ubb38\uc790\uc5f4 \ub098\ub204\uae30. \ub514\ud3f4\ud2b8\ub294 ' '\uc774\uace0, \ubb38\uc790\ub97c \uac00\uc9c4 \ub370\uc774\ud130\ub4e4\uc774 \uc804\ubd80 \ub744\uc6cc\uc4f0\uae30\ub85c \uad6c\ubd84\ub418\uc5b4\uc788\uae30\ub54c\ubb38\uc5d0 \uac00\ub2a5. \nall_df['Ticket'] = all_df['Ticket'].fillna('X').map(lambda x:str(x).split()[0] if len(str(x).split()) > 1 else 'X')\n\n#pclass\uc5d0 \ub530\ub978 Fare\uc758 \ud3c9\uade0\uc744 \uad6c\ud574\uc11c dictionary\ud615\ud0dc\ub85c \ub9cc\ub4e0\ub2e4. \nfare_map = all_df[['Fare', 'Pclass']].dropna().groupby('Pclass').median().to_dict()\n#fare\uc758 \uacb0\uce21\uce58\uc5d0 \ubcf8\uc778 \ud589\uc758 pclass \uac12\uc744 \ub123\uace0, \uadf8 \uac12\uc744 fare \ud3c9\uade0\uc5d0 \ub9f5\ud551\uc2dc\ud0a8\ub2e4.  \nall_df['Fare'] = all_df['Fare'].fillna(all_df['Pclass'].map(fare_map['Fare']))\n#\uc720\ub3c5 \ub192\uc740 \uac00\uaca9\uc774\ub098 \ub0ae\uc740 \uac00\uaca9\uc774 \uc788\uae30\ub54c\ubb38\uc5d0, \uc774\uc0c1\uce58\uc758 \uc601\ud5a5\uc744 \uc904\uc774\uae30 \uc704\ud574\uc11c Fare\uc5d0 log\ub97c \ucde8\ud574\uc900\ub2e4.\nall_df['Fare'] = np.log1p(all_df['Fare'])\n\n\n#\ud56d\uad6c\uc758 \uacb0\uce21\uce58\ub97c X\ub85c \ucc44\uc6b4\ub2e4. \nall_df['Embarked'] = all_df['Embarked'].fillna('X')\n\n#\uc774\ub984\uc740 \uc131\ub9cc \uc0ac\uc6a9\ud55c\ub2e4.\nall_df['Name'] = all_df['Name'].map(lambda x: x.split(',')[0])\n","3d8019b5":"data_1=all_df.loc[all_df['Pclass']==1].groupby('Ticket')['Ticket'].count().sort_values(ascending=False)\nprint(data_1)\nprint()\ndata_2=all_df.loc[all_df['Pclass']==2].groupby('Ticket')['Ticket'].count().sort_values(ascending=False)\nprint(data_2)\nprint()\ndata_3=all_df.loc[all_df['Pclass']==3].groupby('Ticket')['Ticket'].count().sort_values(ascending=False)\nprint(data_3)\nprint()","0a9df731":"label_cols = ['Name', 'Ticket', 'Sex','Pclass','Embarked']\nonehot_cols = [ 'Cabin',]\nnumerical_cols = [ 'Age', 'SibSp', 'Parch', 'Fare']","00752212":"#\ub77c\ubca8 \uc778\ucf54\ub529 \ud568\uc218. c\ub77c\ub294 \ub9e4\uac1c\ubcc0\uc218\ub97c \ubc1b\uc544\uc11c \ub9de\uac8c \ud2b8\ub80c\uc2a4\ud3fc \ud574\uc900\ub2e4. \ndef label_encoder(c):\n    le = LabelEncoder()\n    return le.fit_transform(c)","480fcc4e":"\n#StandardScaler(): \ud3c9\uade0\uc744 \uc81c\uac70\ud558\uace0 \ub370\uc774\ud130\ub97c \ub2e8\uc704 \ubd84\uc0b0\uc73c\ub85c \uc870\uc815\ud55c\ub2e4. \n#\uadf8\ub7ec\ub098 \uc774\uc0c1\uce58\uac00 \uc788\ub2e4\uba74 \ud3c9\uade0\uacfc \ud45c\uc900\ud3b8\ucc28\uc5d0 \uc601\ud5a5\uc744 \ubbf8\uccd0 \ubcc0\ud658\ub41c \ub370\uc774\ud130\uc758 \ud655\uc0b0\uc740 \ub9e4\uc6b0 \ub2ec\ub77c\uc9c0\uac8c \ub418\ub294 \ud568\uc218\nscaler = StandardScaler()\n\nonehot_encoded_df = pd.get_dummies(all_df[onehot_cols])\nlabel_encoded_df = all_df[label_cols].apply(label_encoder)\nnumerical_df = pd.DataFrame(scaler.fit_transform(all_df[numerical_cols]), columns=numerical_cols)\ntarget_df = all_df[TARGET]\n\nall_df = pd.concat([numerical_df, label_encoded_df,onehot_encoded_df, target_df], axis=1)\n#all_df = pd.concat([numerical_df, label_encoded_df, target_df], axis=1)","4314d0b5":"drop_list=['Survived','Parch']","338a8703":"train = all_df.iloc[:100000, :]#0\uac1c~100000\uac1c\ntest = all_df.iloc[100000:, :] #100000\uac1c~ \n#iloc\uc740 \uc815\uc218\ud615 \uc778\ub371\uc2f1\ntest = test.drop('Survived', axis=1) #test\uc5d0\uc11c \uc885\uc18d\ubcc0\uc218\ub97c \ub4dc\ub78d\ud55c\ub2e4. \nmodel_results = pd.DataFrame()\nfolds = 5","de924a11":"y= train.loc[:,'Survived']\nX= train.drop(drop_list,axis=1)","29726fb3":"# y=all_df.loc[:,'Survived']\n# X=all_df.drop('Survived',axis=1)","bc7bb96e":"X_train, X_valid, y_train, y_valid = train_test_split(X,y,test_size=0.25, random_state=21)\n","79469783":"from sklearn import metrics  \nfrom sklearn.metrics import accuracy_score\nimport numpy as np","aa4cb05e":"params = {\n    'metric': 'binary_logloss',\n    'n_estimators': N_ESTIMATORS,\n    'objective': 'binary',\n    'random_state': SEED,\n    'learning_rate': 0.01,\n    'min_child_samples': 150,\n    'reg_alpha': 3e-5,\n    'reg_lambda': 9e-2,\n    'num_leaves': 20,\n    'max_depth': 16,\n    'colsample_bytree': 0.8,\n    'subsample': 0.8,\n    'subsample_freq': 2,\n    'max_bin': 240,\n}","010d2f3c":"lgbm_model=lgb.LGBMClassifier(**params)\nlgbm_model.fit(X_train,y_train)\nlgbm_pred=lgbm_model.predict(X_valid)\n\nlgbm_R2=metrics.accuracy_score(y_valid,lgbm_pred)\n#lgbm_rmse = np.sqrt(mean_squared_error(lgbm_pred,y_valid))\nprint('R2 : ',lgbm_R2)\n#print(\"RMSE : \", lgbm_rmse)","79b30a07":"print(len(X_train.columns))\nprint(X_train.columns)","e345fc96":"def cal_adjust_r2(r2):\n    n=80000\n    k= len(X_train.columns)\n    temp=(1-r2)*(n-1)\n    temp2=n-k-1\n    ad_r2=1-(temp\/temp2)\n    return ad_r2","a9a4efd5":"ad_r2_lgbm=cal_adjust_r2(lgbm_R2)\nprint(ad_r2_lgbm)","353408b3":"#NOT Pseudo\ntrain_kf_feature=train.drop(drop_list,axis=1)\ntrain_kf_label=train.loc[:,'Survived']","e8900f3b":"#Pseudo\n# train_kf_feature=all_df.drop(drop_list,axis=1)\n# train_kf_label=all_df.loc[:,'Survived']","08d701c3":"n_iter=0\nkfold=StratifiedKFold(n_splits=5)\ncv_accuracy=[]\nfeature_importances = pd.DataFrame()\n\nfor train_idx, test_idx in kfold.split(train_kf_feature,train_kf_label):\n\n    X_train=train_kf_feature.iloc[train_idx]\n    X_test=train_kf_feature.iloc[test_idx]\n    y_train,y_test=train_kf_label.iloc[train_idx],train_kf_label.iloc[test_idx]\n    #\ud559\uc2b5 \uc9c4\ud589\n    lgbm_model.fit(X_train,y_train)\n    #\uc608\uce21\n    fold_pred=lgbm_model.predict(X_test)\n    \n    #\uc815\ud655\ub3c4\n    n_iter+=1\n    fold_accuracy=metrics.accuracy_score(y_test,fold_pred)\n    print(\"\\n {}\ubc88\uc9f8  \uad50\ucc28 \uac80\uc99d \uc815\ud655\ub3c4 : {} , \ud559\uc2b5 \ub370\uc774\ud130 \ud06c\uae30:{}, \uac80\uc99d \ub370\uc774\ud130 \ud06c\uae30 :{} \".\n          format(n_iter,fold_accuracy,X_train.shape[0],X_test.shape[0]))\n    cv_accuracy.append(fold_accuracy)\n    \n    #\uc911\uc694\ub3c4 \n    fi_tmp = pd.DataFrame()\n    fi_tmp[\"feature\"] = lgbm_model.feature_name_\n    fi_tmp[\"importance\"] = lgbm_model.feature_importances_\n    feature_importances = feature_importances.append(fi_tmp)\n\nprint('\\n \ud3c9\uade0 \uac80\uc99d \uc815\ud655\ub3c4 : ',np.mean(cv_accuracy))\n    ","6cf545b1":"order = list(feature_importances.groupby(\"feature\").\n             mean().sort_values(\"importance\", ascending=False).index)\nplt.figure(figsize=(10, 10))\nsns.barplot(x=\"importance\", y=\"feature\", data=feature_importances, order=order)\nplt.title(\"{} importance\".format(\"LGBMRegressor\"))\nplt.tight_layout()","3510d8bb":"params_cat = {\n    'bootstrap_type': 'Poisson',\n    'loss_function': 'Logloss',\n    'eval_metric': 'Logloss',\n    'random_seed': SEED,\n    'task_type': 'GPU',\n    'max_depth': 8,\n    'learning_rate': 0.01,\n    'n_estimators': N_ESTIMATORS,\n    'max_bin': 280,\n    'min_data_in_leaf': 64,\n    'l2_leaf_reg': 0.01,\n    'subsample': 0.8\n}","86501bc8":"#\uc0c8\ub85c\uc6b4 \ud2b8\ub808\uc778 valid \uc14b\nX_train, X_valid, y_train, y_valid = train_test_split(X,y,test_size=0.25, random_state=21)\n","d61c7edd":"\ncat_model=ctb.CatBoostClassifier(**params_cat)\ncat_model.fit(X_train, y_train,verbose=300)\ncat_pred=cat_model.predict(X_valid)\nprint(\"\\n\uc815\ud655\ub3c4: \", metrics.accuracy_score(y_valid, cat_pred))\ncat_R2=metrics.accuracy_score(y_valid,cat_pred)\n#lgbm_rmse = np.sqrt(mean_squared_error(lgbm_pred,y_valid))\nprint('R2 : ',cat_R2)","dd54a83c":"cv_accuracy=[]\nfeature_importances = pd.DataFrame()\n\nfor train_idx, test_idx in kfold.split(train_kf_feature,train_kf_label):\n\n    X_train=train_kf_feature.iloc[train_idx]\n    X_test=train_kf_feature.iloc[test_idx]\n    y_train,y_test=train_kf_label.iloc[train_idx],train_kf_label.iloc[test_idx]\n    #\ud559\uc2b5 \uc9c4\ud589\n    cat_model.fit(X_train,y_train,verbose=500)\n    #\uc608\uce21\n    fold_pred=cat_model.predict(X_test)\n    \n    #\uc815\ud655\ub3c4\n    n_iter+=1\n    fold_accuracy=metrics.accuracy_score(y_test,fold_pred)\n    print(\"\\n {}\ubc88\uc9f8  \uad50\ucc28 \uac80\uc99d \uc815\ud655\ub3c4 : {} , \ud559\uc2b5 \ub370\uc774\ud130 \ud06c\uae30:{}, \uac80\uc99d \ub370\uc774\ud130 \ud06c\uae30 :{} \".\n          format(n_iter,fold_accuracy,X_train.shape[0],X_test.shape[0]))\n    cv_accuracy.append(fold_accuracy)\n    \n    #\uc911\uc694\ub3c4 . lgbm\uc774\ub791 \uba85\ub839\uc5b4\uac00 \ub2e4\ub974\ub2e4.\n    fi_tmp = pd.DataFrame()\n    fi_tmp[\"feature\"] = X_test.columns.to_list()\n    fi_tmp[\"importance\"] = cat_model.get_feature_importance()\n    feature_importances = feature_importances.append(fi_tmp)\n\nprint('\\n \ud3c9\uade0 \uac80\uc99d \uc815\ud655\ub3c4 : ',np.mean(cv_accuracy))","adaebd03":"# just to get ideas to improve\norder = list(feature_importances.groupby(\"feature\").mean().sort_values(\"importance\", ascending=False).index)\nplt.figure(figsize=(10, 10))\nsns.barplot(x=\"importance\", y=\"feature\", data=feature_importances, order=order)\nplt.title(\"{} importance\".format(\"CatBoostClassifier\"))\nplt.tight_layout()","1f32f2c9":"def create_submission(model, test, test_passenger_id, model_name):\n    y_pred_test = model.predict_proba(test)[:, 1]\n    submission = pd.DataFrame(\n        {\n            'PassengerId': test_passenger_id, \n            'Survived': (y_pred_test >= 0.5).astype(int),\n        }\n    )\n    submission.to_csv(f\"submission_{model_name}.csv\", index=False)\n    \n    return y_pred_test","fd37f153":"test_df.head()","1d5ea754":"#X_test=test.drop('Pclass',axis=1)\ntest = all_df.iloc[100000:, :] #100000\uac1c~ \nX_test=test.drop(drop_list,axis=1)\nX_test.head()","6b2a8c60":"test_pred_lightgbm = create_submission(\n    lgbm_model, X_test, test_df[\"PassengerId\"], \"lightgbm\"\n)\ntest_pred_catboost = create_submission(\n    cat_model, X_test, test_df[\"PassengerId\"], \"catboost\"\n)","ff0dfcac":"test_pred_merged = (\n\n    test_pred_lightgbm + \n    test_pred_catboost \n)\ntest_pred_merged = np.round(test_pred_merged \/ 2)","84542e4b":"submission = pd.DataFrame(\n    {\n        'PassengerId': test_df[\"PassengerId\"], \n        'Survived': test_pred_merged.astype(int),\n    }\n)\nsubmission.to_csv(f\"submission_merged3.csv\", index=False)","9da69e35":"### Filling missing values","9a5d3e59":"# KAGGLE \uc2a4\ud130\ub514 ","f616b119":"## not pseudo","75e3efa4":"https:\/\/www.kaggle.com\/udbhavpangotra\/tps-apr21-eda-model\n\n\nhttps:\/\/www.kaggle.com\/hiro5299834\/tps-apr-2021-voting-pseudo-labeling","fd2c34fd":"## Submission\n","57c1cb6d":"## pseudo","5e444656":"### lode data","355ebd05":"## \ub370\uc774\ud130 \uc804\ucc98\ub9ac","c8e19fa5":"### \uacb0\uce21\uce58 \uac2f\uc218 \ucd9c\ub825","dd9f56d8":"## \uc778\ucf54\ub529 ","9b4b611f":"\ubcc0\uc218\ubcc4\ub85c \uc778\ucf54\ub529\uc744 \ub2e4\ub974\uac8c \ud574\uc900\ub2e4. ","4c073286":"### CATBoost\n","f5ceabf6":"## \ubaa8\ub378\ub9c1"}}