{"cell_type":{"848c0abc":"code","828f4974":"code","eecf750a":"code","2a56a8ed":"code","ee0e2ae8":"code","666b2127":"code","b037de49":"code","d073ddb6":"code","b72250ce":"code","55e716bf":"code","207c53f9":"code","e40b9f5f":"code","86a6946c":"code","1ea9d261":"code","612d829e":"code","a07bb6ec":"code","964770e4":"code","33c6c01e":"code","a9ddcb61":"code","3929d39a":"code","9db9f493":"code","ff486190":"code","819adde6":"code","2626e9ed":"code","a1670c60":"code","839059a0":"code","63784aeb":"code","6234f109":"code","fa1a93f1":"code","c6f7bb78":"code","4419f579":"code","f43be131":"code","c3116661":"code","5184c439":"code","d33996df":"code","25c48dd7":"code","b9d86376":"code","9a6a4664":"code","6d3345a3":"code","8745dc2c":"code","8e9b24a8":"code","999c485b":"code","2fd7302e":"code","cbdba325":"code","4940a1e0":"code","93f1f3ed":"code","3ccff7f6":"code","bcf1efdb":"code","3fb035de":"code","870d9773":"code","55373339":"code","620f89aa":"code","377aa1ed":"code","c8397a36":"code","f1129213":"code","fe3112f5":"code","40767d73":"code","4de55dd4":"code","24e98bf4":"code","6448ea0b":"code","1010e0c1":"code","6ec23781":"code","3ae77256":"code","4f7c6dee":"code","a512102d":"code","d730aa85":"code","4a4c4fab":"code","5fdfd2b0":"code","203d2832":"code","95a47f92":"code","4be1ed38":"code","d719af04":"code","3e1c5644":"code","aed9b7ba":"code","73feab93":"code","db6a10cd":"code","7c58940b":"code","7d78de21":"code","b4b1cfed":"code","a3ec2e12":"code","ba19aef6":"code","2ec9dd02":"code","bde5ea3b":"code","7a4489ac":"code","0f7bc6e3":"code","1df96aab":"code","ee7125f1":"code","1673f6be":"code","078f9259":"code","ad7d0338":"code","f528fa40":"code","44e91c31":"code","54804094":"code","7db9d3b3":"code","2e8704f6":"code","5a1f41eb":"code","d8518fd4":"code","5955ad44":"code","fd981593":"code","d9e4a6d2":"code","2b02ee8b":"code","bb9b5d7b":"code","f5bcc38b":"code","1ad37335":"code","d20169f7":"code","11577adf":"code","273ed7a5":"code","5d081493":"code","27afc3c3":"code","83758e6e":"code","6624b3d1":"code","5f68c8a9":"code","7fcced72":"code","9e8f2bbb":"code","a3d2d3fa":"code","88c118eb":"code","82701194":"code","0924d1bd":"markdown","e75d424a":"markdown","579de967":"markdown","2e0c391a":"markdown","b9c2904f":"markdown","eed13d6b":"markdown","a392b843":"markdown","c960e872":"markdown","21b5c3bf":"markdown","e327c95c":"markdown","41322cbc":"markdown","0e836854":"markdown","cbed0edf":"markdown","e61c35f2":"markdown","4918cef9":"markdown","524f9d17":"markdown","0b521c8b":"markdown","732996ca":"markdown","f8b15bb9":"markdown","f7db40ad":"markdown","79df0790":"markdown","1adcd692":"markdown","89bf8d79":"markdown","2495c2be":"markdown","0d877c04":"markdown","2e705630":"markdown","fc069d08":"markdown","14400215":"markdown","dde98ede":"markdown","7f4f18ce":"markdown","b9b2cab3":"markdown","98fb9285":"markdown","e54a429d":"markdown","52d4b0c7":"markdown","74e73d58":"markdown","44b4e8a5":"markdown","6ca67481":"markdown","b6235ffe":"markdown","529b96b0":"markdown","bcc02e9a":"markdown","9ef6da7e":"markdown","3a795306":"markdown","b25dd130":"markdown","c8016d17":"markdown","a540f630":"markdown","cb941f83":"markdown","2e4f8f4f":"markdown","4dc1d4ea":"markdown"},"source":{"848c0abc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport json\nimport os\nimport re\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","828f4974":"df = pd.read_excel('\/kaggle\/input\/crunchbasse-companies-details-dataset\/Data2.xlsx')\ndf = df.replace(to_replace =\"\u2014\", \n                 value = np.nan) \ndf =  df.replace(to_replace =\"\u2014\\r\\n\", \n                 value = np.nan, regex=True)\n\ndf.head()","eecf750a":"df.isnull().sum()","2a56a8ed":"feature_with_nan = [features for features in df.columns if df[features].isnull().sum()>1 and df[features].dtype == 'O']\nlen(feature_with_nan)\n\nfor features in feature_with_nan:\n    print('{}: {}% missing value'.format(features, np.around(df[features].isnull().mean()*100,4)))","ee0e2ae8":"#numericals features\nnumerical_feature = [feature for feature in df.columns if df[feature].dtype != 'O']  \nprint('Number of numerical variables:', len(numerical_feature))  ","666b2127":"#categorical feature\ncategorical_feature = [feature for feature in df.columns if df[feature].dtype == 'O']  \nprint('Number of categorical variables:', len(categorical_feature))  \ncategorical_feature","b037de49":"#number of categories in each feature\nfor feature in categorical_feature:\n    print('The feature is {} and number of categories are {}'.format(feature, len(df[feature].unique())))\n","d073ddb6":"plt.figure(figsize=(10, 6))\nstatus_count = df['Status'].value_counts()\nbar_sector = sns.barplot(x = status_count.index, y = status_count.values ,palette=\"deep\")\nbar_sector.axes.set_title(\"Distribution of Company Status\",fontsize=15)\nbar_sector.set_xlabel(\"Status\",fontsize=15)\nbar_sector.set_ylabel(\"Count\",fontsize=15)\nbar_sector.tick_params(labelsize=15)\nlocs, labels = plt.xticks()\nplt.setp(labels, rotation=0)\nfor i,j in zip(bar_sector.patches,status_count.values):\n  height = i.get_height()\n  bar_sector.text(i.get_x() + i.get_width()\/2, height + 5, j, ha='center', va='bottom',fontsize=12)\n\nplt.show()","b72250ce":"#drop unwanted columns\n\ndf = df.drop(['Name','Full Name','Number of Lead Investment', 'Investment Stage',\n              'Number of Investment','Status','Purpose','Primary Organization'], axis=1)","55e716bf":"df_sectors = ','.join(df['Type'])\nsectors = df_sectors.split(',')\nsectors = pd.DataFrame(df_sectors.split(','),columns=['Type'])\nsector_cat_count = sectors['Type'].unique()\nlen(sector_cat_count)","207c53f9":"#consider only upto 7 sectors for a company\n\ntypes = []\nfor type in df['Type']:\n  sep = ','\n  rest = \"\"\n  if len(type.split(sep)) > 7:\n    rest = type.split(sep)[1] + ',' + type.split(sep)[2] + ',' + type.split(sep)[3] + ',' + type.split(sep)[4] + ',' + type.split(sep)[5] + ',' + type.split(sep)[6] + ',' + type.split(sep)[7]\n  elif len(type.split(sep)) > 6:\n    rest = type.split(sep)[1] + ',' + type.split(sep)[2] + ',' + type.split(sep)[3] + ',' + type.split(sep)[4] + ',' + type.split(sep)[5] + ',' + type.split(sep)[6]\n  elif len(type.split(sep)) > 5:\n    rest = type.split(sep)[1] + ',' + type.split(sep)[2] + ',' + type.split(sep)[3] + ',' + type.split(sep)[4] + ',' + type.split(sep)[5]\n  elif len(type.split(sep)) > 4:\n    rest = type.split(sep)[1] + ',' + type.split(sep)[2] + ',' + type.split(sep)[3] + ',' + type.split(sep)[4]\n  elif len(type.split(sep)) > 3:\n    rest = type.split(sep)[1] + ',' + type.split(sep)[2] + ',' + type.split(sep)[3]\n  elif len(type.split(sep)) > 2:\n    rest = type.split(sep)[1] + ',' + type.split(sep)[2] \n  elif len(type.split(sep)) > 1:\n    rest = type.split(sep)[1]\n  else:\n    rest = type.split(sep)[0]\n  types.append(rest)\n\ndf['Type'] = types\n\n#find top 25 sectors overall\ndf_sectors = ','.join(df['Type'])\nsectors = df_sectors.split(',')\nsectors = pd.DataFrame(df_sectors.split(','),columns=['Type'])\nsector_cat_count = sectors['Type'].value_counts()\nsector_cat_count = sector_cat_count.sort_values(ascending=False)\ntop_sectors = sector_cat_count[0:25]\n\n#replace entire dataframe with top 25 sectors\ntype_df = []\nfor type in df['Type']:\n  types = type.split(',')\n  intersect = [x for x in types if x in top_sectors.index]\n  strg = ','.join([str(elem) for elem in intersect])\n  if not strg:\n    strg = \"Other\"\n  type_df.append(strg)\n  \n\ndf['Type'] = type_df","e40b9f5f":"# #convert vales of different currency to USD \n# !pip install currencyconverter\n# from currency_converter import CurrencyConverter\n# def get_symbol(price):\n#         import re\n#         pattern =  r'(\\D*)\\d*\\.?\\d*(\\D*)'\n#         g = re.match(pattern,price).groups()\n#         return g[0] \n \n\n# c = CurrencyConverter()\n# converted_amount = []\n# for price in df['Funding Amount']:\n#     if isinstance(price, float):\n#         converted_amount.append(int(0))\n#         continue\n#     symbol = get_symbol(price)\n#     prc = price.replace(symbol, \"\").replace(\",\", \"\")\n#     prc = int(prc)\n#     if symbol == '\u00a3':\n#         prc = c.convert(prc, 'GBP','USD')\n#     elif symbol == '\u20ac':\n#         prc = c.convert(prc, 'EUR','USD')\n#     elif symbol == 'IDR':\n#         prc = c.convert(prc, 'IDR','USD')\n#     elif symbol == 'NOK':\n#         prc = c.convert(prc, 'NOK','USD')\n#     elif symbol == 'PLN':\n#         prc = c.convert(prc, 'PLN','USD')\n#     elif symbol == 'CN\u00a5':\n#         prc = c.convert(prc, 'CNY','USD')\n#     elif symbol == 'MYR':\n#         prc = c.convert(prc, 'MYR','USD')\n#     elif symbol == 'PHP':\n#         prc = c.convert(prc, 'PHP','USD')\n#     elif symbol == 'CA$':\n#         prc = c.convert(prc, 'CAD','USD')\n#     elif symbol == 'RUB':\n#         prc = c.convert(prc, 'RUB','USD')\n#     elif symbol == 'SEK':\n#         prc = c.convert(prc, 'SEK','USD')\n#     elif symbol == 'CHF':\n#         prc = c.convert(prc, 'CHF','USD')\n#     elif symbol == 'ISK':\n#         prc = c.convert(prc, 'ISK','USD')\n#     elif symbol == 'TRY':\n#         prc = c.convert(prc, 'TRY','USD')\n#     elif symbol == 'SGD':\n#         prc = c.convert(prc, 'SGD','USD')\n#     elif symbol == 'ZAR':\n#         prc = c.convert(prc, 'ZAR','USD')\n#     elif symbol == 'DKK':\n#         prc = c.convert(prc, 'DKK','USD')\n#     elif symbol == 'A$':\n#         prc = c.convert(prc, 'AUD','USD')\n#     elif symbol == '\u20b9':\n#         prc = c.convert(prc, 'INR','USD')\n#     elif symbol == 'R$':\n#         prc = c.convert(prc, 'BRL','USD')\n#     elif symbol == '\u00a5':\n#         prc = c.convert(prc, 'JPY','USD')\n#     elif symbol == '\u20a9':\n#         prc = c.convert(prc, 'KRW','USD')\n#     elif symbol == 'MX$':\n#         prc = c.convert(prc, 'MXN','USD')\n#     elif symbol == 'NZ$':\n#         prc = c.convert(prc, 'NZD','USD')\n#     elif symbol == '\u20aa':\n#         prc = c.convert(prc, 'ILS','USD')\n#     elif symbol != '$':\n#         df = df[df['Funding Amount'] != price]\n#         continue\n#     else:\n#         converted_amount.append(prc)\n#         continue\n#     prc = round(prc)\n#     converted_amount.append(prc)","86a6946c":"# #Categorize Funding Amount\n# cc_amount = pd.cut(converted_amount,bins=[-1,1000000,10000000,100000000,30079814466],\n#        labels=['Less than $1M','$1M to $10M','$10M to 100M','100M+'])\n# df['Funding Amount'] = cc_amount","1ea9d261":"df['Acquisition Status'].value_counts()","612d829e":"# Merge Acquisition Status\n\ndf['Acquisition Status'] = df['Acquisition Status'].astype(str)\ndf['Acquisition Status'] = df['Acquisition Status'].map(lambda x: re.sub(r'\\W+', '', x))\ndf['Acquisition Status'] = df['Acquisition Status'].replace(to_replace =\"MadeAcquisitionsWasAcquired\", value = \"Merger\")\ndf['Acquisition Status'] = df['Acquisition Status'].replace(to_replace =\"WasAcquired\", value = \"Merger\")\ndf['Acquisition Status'] = df['Acquisition Status'].replace(to_replace =\"MadeAcquisitions\", value = \"Acquisition\")\ndf['Acquisition Status'] = df['Acquisition Status'].replace(to_replace ='nan',value = 'No Participation')","a07bb6ec":"# Categorize Job Titles into Most common 6 Titles\n\ndf['Job Title'].fillna(\"Other\", inplace = True) \ndf['Job Title'] =  df['Job Title'].str.upper()\ndf['Job Title'] =  df['Job Title'].apply(lambda x: x.strip())","964770e4":"\n\n# Categorize common Bizzare words into particular job title\ndf['Job Title'] = df['Job Title'].replace({'|'.join(['CHIEF EXECUTIVE OFFICER','CHIEF EXECUTIVE','C.E.O','CEO (CEO)','CEO (EMEA)',\n                                                      'CEO INTERNATIONAL','CEO CO-FOUNDER','CEO (CEO)','FOUNDER (CEO)','CEO CROSSBORDER',\n                                                     'CEO - HYPERGIANT INDUSTRIES','CEO - HYPERGIANT INDUSTRIES','FOUNDING CEO','CEO OF BLIZZARD ENTERTAINMENT',\n                                                     'CEO (FMR. CTO)','CEO OF 3D NANO BATTERIES LLC','MD - CEO','CEO OF 3D NANO BATTERIES LLC','CO-CEO',\n                                                     'CEO OF STITCH','CEO OFFCER','CO - CEO','FOUNDER - CEO','CEO TECHHUB','FOUNDER I CEO','CEO IN RESIDENCE'\n                                                     ]):'CEO'},regex=True)\n\n\ndf['Job Title'] = df['Job Title'].replace({'|'.join(['CO FOUNDER','COFOUNDER','CO - FOUNDER','CO- FOUNDER','CO-FOUNDER','CO -FOUNDER',\n                                                               'CO-FOUNDER - MYPROTEIN GROUP','CO-FOUNDER','MEMBER OF FOUNDING TEAM','TITLECO-FOUNDER',\n                                                               'CO-FOUNDER - STARTUPBOOTCAMP AFRICA','FOUNDER']):'CO-FOUNDER'},regex=True)\n\ndf['Job Title'] = df['Job Title'].replace({'|'.join(['CHIEF TECHNOLOGY OFFICER','CHIEF TECHNICAL OFFICER','CTO SOFTWARE','CHIEF TECHNOLOGY','CTO'\n                                                     ]):'CTO'},regex=True)\ndf['Job Title'] = df['Job Title'].replace({'|'.join(['CHIEF OPERATING OFFICER']):'COO'},regex=True)\ndf['Job Title'] = df['Job Title'].replace({'|'.join(['CHIEF PRODUCT OFFICER','CPO ITALY']):'CPO'},regex=True)\n\ndf['Job Title'] = df['Job Title'].replace({'|'.join(['VICE PRESIDENT','VICE PRESIDENT OF ENGINEERING','VP OF RESEARCH','VP OF STRATEGIC BUSINESS DEVELOPMENT',\n                                                      'VP IMAGING SYSTEMS','VP BUSINESS DEVELOPMENT','CORPORATE VICE PRESIDENT','VP OF ENGINEERING','VP OF PRODUCT',\n                                                      'VP OF LEARNING AT WORK','VP BUSINESS DEVELOPMENT','VP PRODUCT','VP ENGINEERING','VP MARKETING','VP PRODUCT MANAGEMENT',\n                                                      'VP MACHINE LEARNING','EXECUTIVE VICE-PRESIDENT','VICE-PRESIDENT','VP OPERATIONS','VP R','VP MANAGEMENT','VP - COMMERCIAL',\n                                                      'VP OF GROWTH','VP OF TECHNOLOGY','VP OF OPS','VP CONTENT','VP MANAGEMENT','VP OF PRODUCT','VP OF TECHNOLOGY',\n                                                      'VP OF INVESTMENTS','VP TECHNOLOGY','VP OF PROFESSIONAL SERVICES','VP DATA','VP OF INNOVATION','VP OF GROWTH'\n                                                      'VP OF STRATEGIC BUSINESS DEVELOPMENT','VP OF ALGORITHMS','VP - ENTERPRISE DEVELOPMENT','VP OF SPECIAL PROJECTS (POSTMATES X)',\n                                                      'V.P. MATERIALS DEVELOPMENT','VP OF STRATEGIC INITIATIVES','VP STRATEGY','CORPORATE VP',\n                                                     'VP OF INVESTOR RELATIONS']):'VP'},regex=True)\n\ndf['Job Title'] = df['Job Title'].replace({'|'.join(['CHIEF MARKETING OFFICER']):'CMO'},regex=True)\ndf['Job Title'] = df['Job Title'].replace({'|'.join(['CHIEF FINANCIAL OFFICER']):'CFO'},regex=True)\ndf['Job Title'] = df['Job Title'].replace({'|'.join(['CHIEF COMMERCIAL OFFICER']):'CCO'},regex=True)\n\ndf['Job Title'] = df['Job Title'].replace({'|'.join(['MANAGING DIRECTOR','DIRECTOR','MANAGING DIRECTOR SOUTHEAST ASIA','MANAGING DIRECTOR OPERATIONS',\n                                                     'GLOBAL MD','MD MIDDLE EAST INTERNET GROUP','BUSINESS DEVELOPMENT MD','MEDICAL MD',\n                                                     'NON-EXECUTIVE MD','CREATIVE MD OF ADVERTISING','EXECUTIVE MD OF THE HYPERLEDGER PROJECT',\n                                                     'MD OF ENGINEERING','MD OF MARKETING','FINANCE MD','CREATIVE MD','MD OF FINANCE','GENERAL MD',\n                                                     'MD HONG KONG','MD OF R','MD OF THE STANFORD AI LAB','MD OF GLOBAL ACQUIRING','REGIONAL MD'\n                                                     'GROUP MD','PROGRAM MD','MD OF PURCHASING','MD OF GLOBAL FINANCIAL SOLUTIONS','MD OF SPECIAL PROJECTS',\n                                                     'TECHNICAL MD','MD OF BD','SR. MD','PRESIDENT MD','SENIOR MD OF INVESTMENTS','MD OF TECHNOLOGY',\n                                                     'MD OPERATIONS','MD OF PRODUCT','REGIONAL MD','FOUNDING MD']):'MD'},regex=True)\n\ndf['Job Title'] = df['Job Title'].replace({'|'.join(['BOARD MEMBER','CHAIRMAN OF THE BOARD','BOARD OF DIRECTORS','MEMBER OF THE BOARD OF DIRECTORS',\n                                                               'CHAIRMAN OF THE BOARD OF DIRECTORS','BOARD DIRECTOR','MEMBER OF THE BOARD','BOARD MEMBERS',\n                                                               'BOD OF DIRECTORS','BOARD CHAIRMAN','MEMBER OF BOARD','BOARD OF DIRECTER','BOD OF MDS',\n                                                                'BOARD OF MDS','BOARD OF MD','ADVISORY BOD','CHAIRMAN OF BOD','BOARD MD','BOARD OF OVERSEERS'\n                                                     ]):'BOD'},regex=True)\n\ndf['Job Title'] = df['Job Title'].replace({'|'.join(['SENIOR VP','SENIOR VP OF MANUFACTURING','SVP OF SALES','VP OF ENTREPRENEURSHIP','SPV','SVP DIGITAL PRODUCT',\n                                                     'SPV OF PRODUCT','SVP OF R','SPV OF MARKETING','SR. VP','SVP OF MANUFACTURING','SVP INNOVATION']):'SVP'},regex=True)\n\n","33c6c01e":"# Top 6 titles\ndf_title = ' & '.join(df['Job Title'])\ndf_title = df_title.replace('AND','&').replace(',','&').replace('\/','&').replace('|','&').replace('+','&')\ntitles = pd.DataFrame(df_title.split('&'),columns=['Job Title'])\ntitles['Job Title'] = titles['Job Title'].apply(lambda x: x.strip())\n\ntitle_count = titles['Job Title'].value_counts()\ntitle_count = title_count.sort_values(ascending=False)\ntop_titles = title_count[0:6]\n\n#replace entire dataframe with top 6 Job titles\ntitle_df = []\nfor type in df['Job Title']:\n  type = type.replace('AND','&').replace(',','&').replace('\/','&').replace('|','&').replace('+','&')\n  types = type.split('&')\n\n  intersect = [x.strip() for x in types if x.strip() in top_titles.index]\n  strg = '&'.join([str(elem) for elem in intersect])\n  if not strg:\n    strg = \"OTHER\"\n  title_df.append(strg)\n  \ndf['Job Title'] = title_df","a9ddcb61":"#Continent wise data cleaning\n\ndf_h_list = []\ncountries = []\nstates = []\ncities = []\nfor i in df['Headquaters']:\n  df_h = i.split(',')\n  df_h_list.append(df_h)\nfor i in df_h_list:\n  if(len(i)==3):\n    i[2] = i[2].strip()\n  \n    countries.append(i[2])\n    states.append(i[1])\n    cities.append(i[0])\n\ncountries_df = pd.DataFrame(countries,columns = ['Countries'])","3929d39a":"import pandas as pd\ndf1 = pd.read_csv('\/kaggle\/input\/country-continent\/country_cont.csv')\nasia1 = []\nafrica1 = []\nsouth_america1 = []\nnorth_america1 = []\neurope1 = []\noceania1 = []\nasia = df1['Continent_Name']=='Asia'\nfor i in df1[asia]['Country_Name']:\n  i = i.split(',')\n  i[0] = i[0].strip()\n  asia1.append(i[0])\n\n\nafrica = df1['Continent_Name']=='Africa'\nfor i in df1[africa]['Country_Name']:\n  i = i.split(',')\n  i[0] = i[0].strip()\n  africa1.append(i[0])\n\n\nna = df1['Continent_Name']=='North America'\nfor i in df1[na]['Country_Name']:\n  i = i.split(',')\n  i[0] = i[0].strip()\n  # if(i[0]=='United States of America'):\n  #   i[0] = i[0].replace('United States of America','United States') \n  north_america1.append(i[0])\n\n\nsa = df1['Continent_Name']=='South America'\nfor i in df1[sa]['Country_Name']:\n  \n  i = i.split(',')\n  i[0] = i[0].strip()\n  south_america1.append(i[0])\n\n\neurope = df1['Continent_Name']=='Europe'\nfor i in df1[europe]['Country_Name']:\n  \n  i = i.split(',')\n  i[0] = i[0].strip()\n  europe1.append(i[0])\n\noceania = df1['Continent_Name']=='Oceania'\nfor i in df1[oceania]['Country_Name']:\n  i = i.split(',')\n  i[0] = i[0].strip()\n  oceania1.append(i[0])\n\n\n","9db9f493":"for i in countries_df['Countries']:\n  if(i == 'South Korea'):\n    countries_df['Countries'].replace(i,'Asia',inplace = True)\n\n  if(i in asia1):\n    countries_df['Countries'].replace(i,'Asia',inplace = True)\n\nfor i in countries_df['Countries']:\n  if(i == 'United States'):\n    countries_df['Countries'].replace(i,'North America',inplace = True)\n  if(i in north_america1):\n    countries_df['Countries'].replace(i,'North America',inplace = True)\n    \n\n    \n  # if(i in north_america1):\n  #   print(i)\n  #   countries_df['Countries'].replace(i,'North America',inplace = True)\n\nfor i in countries_df['Countries']:\n  if(i == \"C\u00f4te d'Ivoire\"):\n    countries_df['Countries'].replace(i,'Africa',inplace = True)\n\n  if(i in africa1):\n    countries_df['Countries'].replace(i,'Africa',inplace = True)\n\nfor i in countries_df['Countries']:\n  if(i in oceania1):\n    countries_df['Countries'].replace(i,'Australia',inplace = True)\n\nfor i in countries_df['Countries']:\n  if(i in south_america1):\n    countries_df['Countries'].replace(i,'South America',inplace = True)\n\nfor i in countries_df['Countries']:\n  if(i=='United Kingdom'):\n    countries_df['Countries'].replace(i,'Europe',inplace = True)\n  if(i=='The Netherlands'):\n    countries_df['Countries'].replace(i,'Europe',inplace = True)\n\n  if(i in europe1):\n    countries_df['Countries'].replace(i,'Europe',inplace = True)\n\ndf['Headquaters'] = countries_df['Countries']\ncountries_df['Countries'].unique()\n","ff486190":"#Categorize number of Employees\n\ndf['Number of employees'].replace(np.nan,'11-50',inplace = True)\ndf['Number of employees'].replace('1-10','<10',inplace = True)\ndf['Number of employees'].replace('11-50','11-50',inplace = True)\ndf['Number of employees'].replace('51-100','50-100',inplace = True)\n\ndf['Number of employees'].replace('101-250','100-500',inplace = True)\ndf['Number of employees'].replace('251-500','100-500',inplace = True)\n\ndf['Number of employees'].replace('501-1000','500-5000',inplace = True)\ndf['Number of employees'].replace('1001-5000','500-5000',inplace = True)\n\ndf['Number of employees'].replace('5001-10000','>5000',inplace = True)\ndf['Number of employees'].replace('10001+','>5000',inplace = True)\n","819adde6":"#Estimated Revenue Categorization\n\n\ndf['Estimated Revenue'].replace('Less than $1M','Less than $1M',inplace = True)\n\ndf['Estimated Revenue'].replace('$1M to $10M','$1M - $10M',inplace = True)\n\ndf['Estimated Revenue'].replace('$10M to $50M','$10M - $100M',inplace = True)\ndf['Estimated Revenue'].replace('$50M to $100M','$10M - $100M',inplace = True)\n\ndf['Estimated Revenue'].replace('$100M to $500M','$100M-$1B+',inplace = True)\ndf['Estimated Revenue'].replace('$500M to $1B','$100M-$1B+',inplace = True)\ndf['Estimated Revenue'].replace('$1B to $10B','$100M-$1B+',inplace = True)\ndf['Estimated Revenue'].replace('$10B+','$100M-$1B+',inplace = True)\n\ndf['Estimated Revenue'].replace(np.nan,'$1M to $10M',inplace = True) #mode\n\ndf['Estimated Revenue'].unique()","2626e9ed":"# Categorize Founders \n\ndf['Founders']= df['Founders'].astype(float) \nprint(df['Founders'].median())\n\ndf['Founders'] = df['Founders'].fillna('2')\ndf = df.loc[df['Founders'].astype(float) <= 7]\n\ndf['Founders']= df['Founders'].astype(float) \n\ndf['Founders']=pd.cut(df['Founders'], bins=[0,1,2,7], labels = [\"1\",\"2\",\"3-7\"])","a1670c60":"df['Acquisitions'].value_counts()\ndf['Acquisitions'].isnull().sum()\n","839059a0":"# # Categorize Number of Acquisitions\n# df['Acquisitions']= df['Acquisitions'].astype(float) \n# #print(data['Acquisitions'].median())\n\n# df['Acquisitions'] = df['Acquisitions'].fillna('2')\n# df = df.loc[df['Acquisitions'].astype(float) <= 10]\n# df['Acquisitions']= df['Acquisitions'].astype(float) \n# df['Acquisitions']=pd.cut(df['Acquisitions'], bins=[0,1,5,10], labels = [\"1\",\"2-5\",\"6-10\"])","63784aeb":"#binning funding rounds and removed outliers\ndf['Funding Rounds']= df['Funding Rounds'].astype(float) \nprint(df['Funding Rounds'].median())\ndf['Funding Rounds'] = df['Funding Rounds'].fillna('3')\ndf = df.loc[df['Funding Rounds'].astype(float) <= 13]\ndf['Funding Rounds']= df['Funding Rounds'].astype(float)\ndf['Funding Rounds']=pd.cut(df['Funding Rounds'], bins=[0,1,4,13], labels = [\"1\",\"2-4\",\"5-13\"])","6234f109":"import plotly.express as px\nfig = px.box(df, y='Active Products', hover_data= [\"Active Products\"], width=400, height=400)\nfig.update_layout(\n    yaxis = dict(range=[0, 500]),\n    yaxis_title=\"Active Products Distribution\",\n    font=dict(\n         size=12\n    )\n)\nfig.show()","fa1a93f1":"def normalize_outliers(data):\n    threshold = 1.5\n    for col in data.columns:\n      normalized_data = []\n      mean = np.mean(data[col])\n      std = np.std(data[col])\n      for y in data[col]:\n          z_score= (y - mean)\/std\n          if np.abs(z_score) > threshold:\n            normalized_data.append(mean)\n          else:\n            normalized_data.append(y)\n      data[col] = normalized_data\n    return data","c6f7bb78":"\nz = pd.DataFrame(df['Active Products'].astype(float))\nz['Active Products'] = z.fillna(z['Active Products'].astype(float).mean())\nprint(df['Active Products'].astype(float).mean())\nprint(df['Active Products'].astype(float).median())\nprint(df['Active Products'].astype(float).std())\nz = normalize_outliers(z)\n","4419f579":"import plotly.express as px\nfig = px.box(z, y='Active Products', hover_data= [\"Active Products\"], width=400, height=400)\nfig.update_layout(\n    yaxis = dict(range=[0, 150]),\n    yaxis_title=\"Active Product Distribution\",\n    font=dict(\n         size=12\n    )\n)\nfig.show()","f43be131":"#binning Active products\ndf['Active Products'] = z\ndf['Active Products']= df['Active Products'].astype(float) \ndf['Active Products']=pd.cut(df['Active Products'], bins=[-1,10.0,22.0,30.0,60.0], labels = [\"1-10\",\"11-22\",\"23-30\",\"31-60\"])\n","c3116661":"#Nunmber of Lead Investors\ndf['Nunmber of Lead Investors']= df['Nunmber of Lead Investors'].astype(float) \nprint(df['Nunmber of Lead Investors'].median())\ndf['Nunmber of Lead Investors'] = df['Nunmber of Lead Investors'].fillna('2')\n\ndf['Nunmber of Lead Investors'] = df['Nunmber of Lead Investors'].astype(float)\ndf['Nunmber of Lead Investors']=pd.cut(df['Nunmber of Lead Investors'], bins=[0,1,3,15], labels = [\"1\",\"2-3\",\"4-15\"])","5184c439":"#Bining Number of Investors\n\ndf['Nmber of Investors']= df['Nmber of Investors'].astype(float) \nprint(df['Nmber of Investors'].median())\n\ndf['Nmber of Investors'] = df['Nmber of Investors'].fillna('5')\n\ndf['Nmber of Investors'] = df['Nmber of Investors'].astype(float)\ndf['Nmber of Investors']=pd.cut(df['Nmber of Investors'], bins=[0,1,4,9,104], labels = [\"1\",\"2-3\",\"4-20\", \"21-104\"])","d33996df":"#Binning Founded organizations\ndf['Founded Organization']= df['Founded Organization'].astype(float) \nprint(df['Founded Organization'].median())\ndf['Founded Organization'] = df['Founded Organization'].fillna('2')\n\ndf['Founded Organization']  = df['Founded Organization'] .astype(float)\ndf['Founded Organization'] =pd.cut(df['Founded Organization'] , bins=[0,1,3,23], labels = [\"1\",\"2-3\",\"4-23\"])","25c48dd7":"df['Founded Organization'].isna().sum()","b9d86376":"#binning Portfolio companies\n\ndf['Portfolio Companies']= df['Portfolio Companies'].astype(float) \nprint(df['Portfolio Companies'].median())\ndf['Portfolio Companies'] = df['Portfolio Companies'].fillna('3')\n\ndf['Portfolio Companies']  = df['Portfolio Companies'].astype(float)\ndf['Portfolio Companies'] =pd.cut(df['Portfolio Companies'], bins=[0,2,3,208], labels = [\"1-2\",\"3\",\"4-208\"])","9a6a4664":"# funding status\n\nval = df['Funding Status'].mode()\nprint(val)\ndf['Funding Status'] = df['Funding Status'].fillna('Early Stage Venture')\n","6d3345a3":"df.head()","8745dc2c":"#categorical feature\ncategorical_feature = [feature for feature in df.columns if df[feature].dtype == 'O']  \nprint('Number of categorical variables:', len(categorical_feature))  \n\n#number of categories in each feature\nfor feature in categorical_feature:\n    print('The feature is {} and number of categories are {}'.format(feature, len(df[feature].unique())))\n","8e9b24a8":"countries_cat_count = countries_df['Countries'].value_counts()\ncountries_cat_count = countries_cat_count.sort_values(ascending = False)\nprint('There are a total of ',len(countries_cat_count),'different continents in our dataset')\n\ncountries_cat_count = countries_cat_count.iloc[:30]\nprint(countries_cat_count)\n\nplt.figure(figsize=(14,8))\nbar_countries = sns.barplot(x =countries_cat_count.index, y = countries_cat_count.values,palette=\"YlOrRd\")\nsns.set_style(\"white\")\nplt.title('Distribution of different continents')\nplt.xlabel('Continent')\nplt.ylabel('Counts of each continent')\nlocs, labels = plt.xticks()\nplt.setp(labels, rotation=0)\nfor i,j in zip(bar_countries.patches,countries_cat_count.values):\n  height = i.get_height()\n  bar_countries.text(i.get_x() + i.get_width()\/2, height + 5, j, ha='center', va='bottom')\nplt.savefig('Headquaters.png',bbox_inches='tight')\n","999c485b":"df_sectors = ','.join(df['Type'])\nsectors = pd.DataFrame(df_sectors.split(','),columns=['Type'])\nsector_cat_count = sectors['Type'].value_counts()\n\n\nprint('There are',len(sector_cat_count),'different sectors of Businesses in our Dataset')\n\nplt.figure(figsize=(14,8))\nbar_sector = sns.barplot(x =sector_cat_count.index, y = sector_cat_count.values,palette=\"deep\")\nsns.set_style(\"white\")\nplt.title('Distribution of different sectors of business')\nplt.xlabel('categories of sectors')\nplt.ylabel('Counts of sector appearing in companies')\nlocs, labels = plt.xticks()\nplt.setp(labels, rotation=90)\nfor j,i in enumerate(bar_sector.patches):\n  height = i.get_height()\n  bar_sector.text(i.get_x() + i.get_width()\/2, height + 5, sector_cat_count[j], ha='center', va='bottom')\nplt.savefig('Sectors.png',bbox_inches='tight')","2fd7302e":"df_title = '&'.join(df['Job Title'])\ntitles = pd.DataFrame(df_title.split('&'),columns=['Job Title'])\ntitle_count = titles['Job Title'].value_counts()\ntitle_count = title_count.sort_values(ascending=False)\n\nprint('There are',len(title_count),'different Job titles  in our Dataset')\n\nplt.figure(figsize=(14,8))\nbar_sector = sns.barplot(x =title_count.index, y = title_count.values,palette=\"rocket\")\nsns.set_style(\"white\")\nplt.title('Top Job Titles of Investors')\nplt.xlabel('Job Titles')\nplt.ylabel('Count')\nlocs, labels = plt.xticks()\nplt.setp(labels, rotation=0)\nfor j,i in enumerate(bar_sector.patches):\n  height = i.get_height()\n  bar_sector.text(i.get_x() + i.get_width()\/2, height + 5, title_count[j], ha='center', va='bottom')\nplt.savefig('Job Titles.png',bbox_inches='tight')","cbdba325":"\nfounded_y = []\nfounded1 =[]\nleny = []\nfor i in df['Founded Year']:\n  if isinstance(i, float):\n    i= ' '\n  i = i.split(' ')\n  founded_y.append(i)\n\nfor i in founded_y:\n  if(len(i)==3):\n    founded1.append(i[2])\n  elif(len(i)==2):\n    founded1.append(i[1])\n  elif(len(i)==1):\n    founded1.append(i[0])\n\n\nyear_counts = np.unique(founded1,return_counts=True)\n\n\nyear_counts1 = year_counts[0][:-1]\nyear_counts1_c = year_counts[1][:-1]\n\nyear_counts1 = year_counts1[::-1]\nyear_counts1_c = [int(i) for i in year_counts1_c]\nyear_counts1_c = year_counts1_c[::-1]\n\nyear_counts1 = year_counts1[0:30]\nyear_counts1_c = year_counts1_c[0:30]\n\n\nplt.figure(figsize=(14,8))\n\nbar_year = sns.barplot(x =year_counts1, y = year_counts1_c,palette=\"PuBu\")\nplt.title('Distribution of different  founding years')\nplt.xlabel('Years')\nplt.ylabel('Counts of each year')\nlocs, labels = plt.xticks()\nplt.setp(labels, rotation=90)\n\n\nfor i,j in zip(bar_year.patches,year_counts1_c[::-1]):\n  height = i.get_height()\n  bar_year.text(i.get_x() + i.get_width()\/2, height + 5, j, ha='center', va='bottom')\nplt.savefig(\"Founding_Years\",bbox_inches='tight')\ndf['Founded Year'] = founded1\n","4940a1e0":"#count of all companies with the given 3 acquisition statuses\n\nplt.figure(figsize=(14,8))\nsns.set(font_scale=1.4)\nacqstat = df['Acquisition Status'].value_counts()\n\nbar_plot = sns.barplot(x =acqstat.index, y = acqstat.values,palette=\"GnBu_d\")\nsns.set_style(\"white\")\nplt.xlabel(\"M&A Status\", labelpad=14)\nplt.ylabel(\"Count of companies\", labelpad=14)\nplt.title(\"Distribution of M&A Status\", y=1.02);\n\nlocs, labels = plt.xticks()\nplt.setp(labels, rotation=0)\nfor i,j in zip(bar_plot.patches,acqstat.values):\n  height = i.get_height()\n  bar_plot.text(i.get_x() + i.get_width()\/2, height + 5, j, ha='center', va='bottom')\nplt.savefig('acq_status.png',bbox_inches='tight')","93f1f3ed":"#count of all companies with the given funding statuses\n\nplt.figure(figsize=(14,8))\nsns.set(font_scale=1.4)\nacqstat = df['Funding Status'].value_counts()\n\nbar_plot = sns.barplot(x =acqstat.index, y = acqstat.values,palette=\"RdPu\")\nsns.set_style(\"white\")\nplt.xlabel(\"Funding Status\", labelpad=14)\nplt.ylabel(\"Count of companies\", labelpad=14)\nplt.title(\"Count of companies with these funding status\", y=1.02);\n\nlocs, labels = plt.xticks()\nplt.setp(labels, rotation=90)\nfor i,j in zip(bar_plot.patches,acqstat.values):\n  height = i.get_height()\n  bar_plot.text(i.get_x() + i.get_width()\/2, height + 5, j, ha='center', va='bottom')\nplt.show()\n","3ccff7f6":"import plotly.express as px\nfig = px.box(df, x=\"IPO status\", y=\"Funding Amount\", hover_data= [\"IPO status\"])\nfig.show()","bcf1efdb":"# plt.figure(figsize=(14,8))\n# sns.set(font_scale=1.4)\n# acqstat = df['Funding Amount'].value_counts()\n# bar_plot = sns.barplot(x =acqstat.index, y = acqstat.values,palette=\"OrRd\")\n# plt.title('Distribution of Funding Amount')\n# plt.xlabel('Funding Amount')\n# plt.ylabel('Company Count')\n# for i, p in enumerate(bar_plot.patches):\n#     height = p.get_height()\n#     bar_plot.text(p.get_x()+p.get_width()\/2., height + 0.1, acqstat[i],ha=\"center\")\n# plt.savefig(\"Funding_Amount\",bbox_inches='tight')\n\n","3fb035de":"\nfounded_y = []\nfounded1 =[]\nleny = []\ndf['Exit Date'].fillna(\" \", inplace = True)\nfor i in df['Exit Date']:\n  i = i.split(' ')\n  founded_y.append(i)\n\nfor i in founded_y:\n  if(len(i)==3):\n    founded1.append(i[2])\n  elif(len(i)==2):\n    founded1.append(i[1])\n  elif(len(i)==1):\n    founded1.append(i[0])\n\n\nyear_counts = np.unique(founded1,return_counts=True)\n\n\nyear_counts1 = year_counts[0][:-1]\nyear_counts1_c = year_counts[1][:-1]\n\nyear_counts1 = year_counts1[::-1]\nyear_counts1_c = [int(i) for i in year_counts1_c]\nyear_counts1_c = year_counts1_c[::-1]\n\nyear_counts1 = year_counts1[0:30]\nyear_counts1_c = year_counts1_c[0:30]\n\n\nplt.figure(figsize=(14,8))\nbar_year = sns.barplot(x =year_counts1, y = year_counts1_c,palette=\"YlOrRd\")\nsns.set_style(\"white\")\nplt.title('Distribution of different  exit date')\nplt.xlabel('Years')\nplt.ylabel('Counts of each year')\nlocs, labels = plt.xticks()\nplt.setp(labels, rotation=90)\n\n\nfor i,j in zip(bar_year.patches,year_counts1_c[::-1]):\n  height = i.get_height()\n  bar_year.text(i.get_x() + i.get_width()\/2, height + 5, j, ha='center', va='bottom')\nplt.show()\ndf['Exit Date'] = founded1","870d9773":"countries_cat_count = df['Number of employees'].value_counts()\ncountries_cat_count = countries_cat_count.sort_values(ascending = False)\nprint('There are a total of ',len(countries_cat_count),'different employee categories in our dataset')\n\ncountries_cat_count = countries_cat_count.iloc[:30]\nprint(countries_cat_count)\n\nplt.figure(figsize=(14,8))\nbar_countries = sns.barplot(x =countries_cat_count.index, y = countries_cat_count.values,palette=\"YlOrRd\")\nsns.set_style(\"white\")\nplt.title('Distribution of different employees')\nplt.xlabel('Employees')\nplt.ylabel('Counts of each Employee category')\nlocs, labels = plt.xticks()\nplt.setp(labels, rotation=90)\nfor i,j in zip(bar_countries.patches,countries_cat_count.values):\n  height = i.get_height()\n  bar_countries.text(i.get_x() + i.get_width()\/2, height + 5, j, ha='center', va='bottom')\nplt.show()\n","55373339":"countries_cat_count = df['Estimated Revenue'].value_counts()\ncountries_cat_count = countries_cat_count.sort_values(ascending = False)\nprint('There are a total of ',len(countries_cat_count),'different revenue categories in our dataset')\n\ncountries_cat_count = countries_cat_count.iloc[:30]\nprint(countries_cat_count)\n\nplt.figure(figsize=(14,8))\nbar_countries = sns.barplot(x =countries_cat_count.index, y = countries_cat_count.values,palette=\"YlOrRd\")\nsns.set_style(\"white\")\nplt.title('Distribution of different revenue')\nplt.xlabel('revenue')\nplt.ylabel('Counts of each revenue category')\nlocs, labels = plt.xticks()\nplt.setp(labels, rotation=90)\nfor i,j in zip(bar_countries.patches,countries_cat_count.values):\n  height = i.get_height()\n  bar_countries.text(i.get_x() + i.get_width()\/2, height + 5, j, ha='center', va='bottom')\nplt.show()","620f89aa":"plt.figure(figsize=(14,8))\nsns.set(font_scale=1.4)\nacqstat = df['Founders'].value_counts()\nacqstat\nbar_plot = sns.barplot(x =acqstat.index, y = acqstat.values,palette=\"RdPu\")\nsns.set_style(\"white\")\nplt.xlabel(\"Founders\", labelpad=14)\nplt.ylabel(\"Number of companies\", labelpad=14)\nplt.title(\"Count of companies with number of Founders\", y=1.02);\nfor i, p in enumerate(bar_plot.patches):\n    height = p.get_height()\n    bar_plot.text(p.get_x()+p.get_width()\/2., height + 0.1, acqstat[i],ha=\"center\")\nplt.savefig('Founders',bbox_inches='tight')","377aa1ed":"# plt.figure(figsize=(14,8))\n# sns.set(font_scale=1.4)\n# acqstat = df['Acquisitions'].value_counts()\n# acqstat\n# bar_plot = sns.barplot(x =acqstat.index, y = acqstat.values,palette=\"RdPu\")\n# sns.set_style(\"white\")\n# plt.xlabel(\"Acquisitions\", labelpad=14)\n# plt.ylabel(\"Count of companies\", labelpad=14)\n# plt.title(\"Count of companies with number of Acquisitions\", y=1.02);\n# for i, p in enumerate(bar_plot.patches):\n#     height = p.get_height()\n#     bar_plot.text(p.get_x()+p.get_width()\/2., height + 0.1, acqstat[i],ha=\"center\")\n# plt.show()","c8397a36":"plt.figure(figsize=(14,8))\nsns.set(font_scale=1.4)\nacqstat = df['Funding Rounds'].value_counts()\nacqstat\nbar_plot = sns.barplot(x =acqstat.index, y = acqstat.values,palette=\"RdPu\")\nsns.set_style(\"white\")\nplt.xlabel(\"Funding Rounds\", labelpad=14)\nplt.ylabel(\"Count of companies\", labelpad=14)\nplt.title(\"Count of companies with number of Funding Rounds\", y=1.02);\nfor i, p in enumerate(bar_plot.patches):\n    height = p.get_height()\n    bar_plot.text(p.get_x()+p.get_width()\/2., height + 0.1, acqstat[i],ha=\"center\")\nplt.show()","f1129213":"plt.figure(figsize=(14,8))\nsns.set(font_scale=1.4)\nacqstat = df['Active Products'].value_counts()\nacqstat\nbar_plot = sns.barplot(x =acqstat.index, y = acqstat.values,palette=\"RdPu\")\nsns.set_style(\"white\")\nplt.xlabel(\"Active Products\", labelpad=14)\nplt.ylabel(\"Count of companies\", labelpad=14)\nplt.title(\"Count of companies with number of Active Products\", y=1.02);\nfor i, p in enumerate(bar_plot.patches):\n    height = p.get_height()\n    bar_plot.text(p.get_x()+p.get_width()\/2., height + 0.1, acqstat[i],ha=\"center\")\nplt.savefig(\"active_products\", bbox_inches='tight')","fe3112f5":"plt.figure(figsize=(14,8))\nsns.set(font_scale=1.4)\nacqstat = df['Nunmber of Lead Investors'] .value_counts()\nacqstat\nbar_plot = sns.barplot(x =acqstat.index, y = acqstat.values,palette=\"RdPu\")\nsns.set_style(\"white\")\nplt.xlabel(\"Number of Lead Investors\", labelpad=14)\nplt.ylabel(\"Count of companies\", labelpad=14)\nplt.title(\"Count of companies with Number of Lead Investors\", y=1.02);\nfor i, p in enumerate(bar_plot.patches):\n    height = p.get_height()\n    bar_plot.text(p.get_x()+p.get_width()\/2., height + 0.1, acqstat[i],ha=\"center\")\nplt.show()\n","40767d73":"plt.figure(figsize=(14,8))\nsns.set(font_scale=1.4)\nacqstat = df['Nmber of Investors'].value_counts()\nacqstat\nbar_plot = sns.barplot(x =acqstat.index, y = acqstat.values,palette=\"RdPu\")\nsns.set_style(\"white\")\nplt.xlabel(\"Number of Lead Investors\", labelpad=14)\nplt.ylabel(\"Count of companies\", labelpad=14)\nplt.title(\"Count of companies with Number of Lead Investors\", y=1.02);\nfor i, p in enumerate(bar_plot.patches):\n    height = p.get_height()\n    bar_plot.text(p.get_x()+p.get_width()\/2., height + 0.1, acqstat[i],ha=\"center\")\nplt.show()","4de55dd4":"plt.figure(figsize=(14,8))\nsns.set(font_scale=1.4)\nacqstat = df['Founded Organization'].value_counts()\nacqstat\nbar_plot = sns.barplot(x =acqstat.index, y = acqstat.values,palette=\"RdPu\")\nsns.set_style(\"white\")\nplt.xlabel(\"Founded Organization\", labelpad=14)\nplt.ylabel(\"Count of companies\", labelpad=14)\nplt.title(\"Count of companies with number of Founded Organization\", y=1.02);\nfor i, p in enumerate(bar_plot.patches):\n    height = p.get_height()\n    bar_plot.text(p.get_x()+p.get_width()\/2., height + 0.1, acqstat[i],ha=\"center\")\nplt.show()","24e98bf4":"plt.figure(figsize=(14,8))\nsns.set(font_scale=1.4)\nacqstat = df['Portfolio Companies'].value_counts()\nacqstat\nbar_plot = sns.barplot(x =acqstat.index, y = acqstat.values,palette=\"RdPu\")\nsns.set_style(\"white\")\nplt.xlabel(\"Portfolio Companies\", labelpad=14)\nplt.ylabel(\"Count of companies\", labelpad=14)\nplt.title(\"Count of companies with number of Portfolio Companies\", y=1.02);\nfor i, p in enumerate(bar_plot.patches):\n    height = p.get_height()\n    bar_plot.text(p.get_x()+p.get_width()\/2., height + 0.1, acqstat[i],ha=\"center\")\nplt.show()","6448ea0b":"df.to_excel(\"data_Model.xlsx\")","1010e0c1":"df.isnull().sum()","6ec23781":"df = pd.read_excel('\/kaggle\/input\/datamodel\/data_mod.xlsx')\ndata_model_df = df\ndf1 = data_model_df['Type'].str.get_dummies(sep=',')\ndf11 = pd.DataFrame(df1)\ndf2 = data_model_df['Job Title'].str.get_dummies(sep='&')\ndf22 = pd.DataFrame(df2)","3ae77256":"data_model_df = data_model_df.join(df11)\ndata_model_df = data_model_df.join(df22)\n\n# df.join(df2)","4f7c6dee":"data_model_df = pd.get_dummies(data_model_df, columns=['Headquaters','Estimated Revenue','Founders',\n                                   'Number of employees','Funding Rounds','Funding Status','Active Products','Funding Amount',\n                                   'Nunmber of Lead Investors','Nmber of Investors','IPO status','Founded Organization',\n                                    'Portfolio Companies','Founded Year','Exit Date'])\ndata_model_df.head()\n","a512102d":"data_model_df.head()","d730aa85":"test_data = data_model_df\nsample_df = data_model_df[data_model_df['Acquisition Status']=='WasAcquired'].sample(n=1100, random_state=1)\nsample_df = sample_df.append(data_model_df[data_model_df['Acquisition Status']=='MadeAcquisitions']).sample(n=1000, random_state=1)\nsample_df = sample_df.append(data_model_df[data_model_df['Acquisition Status']=='Did Not Participate'].sample(n=1500, random_state=1))\n#np.unique(sample_df['Acquisition Status'],return_counts = True)\ndata_model_df = sample_df\n","4a4c4fab":"# test_data = data_model_df\n# sample_df = data_model_df[data_model_df['Acquisition Status']=='Did Not Participate'].sample(n=1750, random_state=1,replace=True)\n# sample_df = sample_df.append(data_model_df[data_model_df['Acquisition Status']=='WasAcquired'].sample(n=1850, random_state=1,replace=True))\n# sample_df = sample_df.append(data_model_df[data_model_df['Acquisition Status']=='MadeAcquisitions'].sample(n=1650, random_state=1,replace=True))\n# #np.unique(sample_df['Acquisition Status'],return_counts = True)\n# data_model_df = sample_df","5fdfd2b0":"data_model_df.shape","203d2832":"test_data.shape","95a47f92":"# test_data = test_data.drop(sample_df.index)\n# data_model_df = test_data ","4be1ed38":"Acquired_Status = data_model_df['Acquisition Status']\nAcquired_price = data_model_df['Acquired Price']\nAcquired_By = data_model_df['Acquired By']","d719af04":"\ndata_model_df = data_model_df.drop(['Acquired Price'],axis = 1)\ndata_model_df = data_model_df.drop(['Acquired By'],axis = 1)\ndata_model_df = data_model_df.drop(['Acquisition Status'],axis = 1)\ndata_model_df = data_model_df.drop(['Acquisitions'],axis = 1)\ndata_model_df = data_model_df.drop(['Type'],axis = 1)\ndata_model_df = data_model_df.drop(['Job Title'],axis = 1)\ndata_model_df= data_model_df.drop(['Unnamed: 0'],axis=1)\ndata_model_df= data_model_df.drop(['Unnamed: 0.1'],axis=1)","3e1c5644":"from sklearn.svm import LinearSVC\nfrom sklearn.naive_bayes import GaussianNB as NB\nfrom sklearn.ensemble import RandomForestClassifier as RF\nfrom sklearn.neighbors import KNeighborsClassifier as KNN\nfrom sklearn.ensemble import GradientBoostingClassifier as GB\nfrom sklearn.metrics import average_precision_score\nfrom sklearn.ensemble import AdaBoostClassifier as ab\nfrom sklearn.tree import DecisionTreeClassifier as dt\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import validation_curve\nfrom sklearn.linear_model import LogisticRegression as LR\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import learning_curve\nfrom xgboost import XGBClassifier\n\nfrom sklearn.linear_model import SGDClassifier as sg\nimport matplotlib.pyplot as plt\nimport numpy as np","aed9b7ba":"X_train, X_test, y_train, y_test = train_test_split(data_model_df, Acquired_Status, test_size=0.33, random_state=42)","73feab93":"y_train.value_counts()","db6a10cd":"y_test.value_counts()","7c58940b":"model_name=['naive_bayes','RandomForestClassifier','KNeighborsClassifier','GradientBoostingClassifier','AdaBoostClassifier','DecisionTreeClassifier','LogisticRegression']\nmodels_list= [NB(),RF(),KNN(),GB(),ab(),dt(),LR(max_iter=600)]\nfor i, j in zip(model_name, models_list):\n    scores = cross_val_score(j, X_train, y_train, cv=5)\n    \n    print(i+\"--\"+ \"Accuracy: %0.2f (+\/- %0.2f)\" % (scores.mean(), scores.std() * 2))","7d78de21":"train_sizes, train_scores, test_scores = learning_curve(LR(max_iter = 1000), \n                                                        X_test, \n                                                        y_test,\n                                                        cv = 5,\n                                                       \n                                                        scoring='accuracy',\n                                                        \n                                                        train_sizes=np.linspace(0.01, 1.0, 50))\n\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\n\n\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\n\n\nplt.plot(train_sizes, train_mean, '--', color=\"#111111\",  label=\"Training score\")\nplt.plot(train_sizes, test_mean, color=\"#111111\", label=\"Cross-validation score\")\n\n\nplt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"#DDDDDD\")\nplt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color=\"#DDDDDD\")\n\nplt.title(\"Learning Curve for Logistic Regression\")\nplt.xlabel(\"Training Set Size\"), plt.ylabel(\"Accuracy Score\"), plt.legend(loc=\"best\")\nplt.tight_layout()\nplt.savefig('logistic_reg.png', bbox_inches='tight')\n","b4b1cfed":"train_sizes, train_scores, test_scores = learning_curve(NB(), \n                                                        X_test, \n                                                        y_test,\n                                                        cv = 5,\n                                                       \n                                                        scoring='accuracy',\n                                                        \n                                                        train_sizes=np.linspace(0.01, 1.0, 50))\n\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\n\n\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\n\n\nplt.plot(train_sizes, train_mean, '--', color=\"#111111\",  label=\"Training score\")\nplt.plot(train_sizes, test_mean, color=\"#111111\", label=\"Cross-validation score\")\n\n\nplt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"#DDDDDD\")\nplt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color=\"#DDDDDD\")\n\nplt.title(\"Learning Curve for Naive Bayes\")\nplt.xlabel(\"Training Set Size\"), plt.ylabel(\"Accuracy Score\"), plt.legend(loc=\"best\")\nplt.tight_layout()\nplt.savefig(\"NB.png\",bbox_inches='tight')","a3ec2e12":"train_sizes, train_scores, test_scores = learning_curve(dt(criterion = 'entropy' ,max_depth = 10), \n                                                        X_test, \n                                                        y_test,\n                                                        cv = 5,\n                                                       \n                                                        scoring='accuracy',\n                                                        \n                                                        train_sizes=np.linspace(0.01, 1.0, 50))\n\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\n\n\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\n\n\nplt.plot(train_sizes, train_mean, '--', color=\"#111111\",  label=\"Training score\")\nplt.plot(train_sizes, test_mean, color=\"#111111\", label=\"Cross-validation score\")\n\n\nplt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"#DDDDDD\")\nplt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color=\"#DDDDDD\")\n\nplt.title(\"Learning Curve for Decision Tree\")\nplt.xlabel(\"Training Set Size\"), plt.ylabel(\"Accuracy Score\"), plt.legend(loc=\"best\")\nplt.tight_layout()\nplt.savefig(\"DT.png\",bbox_inches='tight')\n","ba19aef6":"train_sizes, train_scores, test_scores = learning_curve(ab(), \n                                                        X_test, \n                                                        y_test,\n                                                        cv = 5,\n                                                       \n                                                        scoring='accuracy',\n                                                        \n                                                        train_sizes=np.linspace(0.01, 1.0, 50))\n\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\n\n\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\n\n\nplt.plot(train_sizes, train_mean, '--', color=\"#111111\",  label=\"Training score\")\nplt.plot(train_sizes, test_mean, color=\"#111111\", label=\"Cross-validation score\")\n\n\nplt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"#DDDDDD\")\nplt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color=\"#DDDDDD\")\n\nplt.title(\"Learning Curve for Ada Boost\")\nplt.xlabel(\"Training Set Size\"), plt.ylabel(\"Accuracy Score\"), plt.legend(loc=\"best\")\nplt.tight_layout()\nplt.savefig('AdaBoost.png', bbox_inches='tight')","2ec9dd02":"X = data_model_df\ny = Acquired_Status","bde5ea3b":"from itertools import cycle\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom scipy import interp\nfrom sklearn.metrics import roc_auc_score\n\n\n\ny_1 = label_binarize(Acquired_Status, classes=['Did Not Participate', 'MadeAcquisitions', 'WasAcquired'])\nn_classes = y_1.shape[1]\nprint(n_classes)\n\nrandom_state = np.random.RandomState(0)\nn_samples, n_features = X.shape\n# print(df.shape)\nX_train_roc, X_test_roc, y_train_roc, y_test_roc = train_test_split(X, y_1, test_size=0.33, random_state=42)\n\nclassifier = OneVsRestClassifier(dt(criterion = 'entropy' ,max_depth = 15))\ny_score = classifier.fit(X_train_roc,y_train_roc).predict(X_test_roc)\n\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\nyu = np.unique(Acquired_Status)\nfor i,v in enumerate(yu):\n    fpr[i], tpr[i], _ = roc_curve(y_test_roc[:, i], y_score[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n# First aggregate all false positive rates\nall_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\nfpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test_roc.ravel(), y_score.ravel())\nroc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n# Then interpolate all ROC curves at this points\nmean_tpr = np.zeros_like(all_fpr)\nfor i in range(n_classes):\n    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n\n# Finally average it and compute AUC\nmean_tpr \/= n_classes\n\nfpr[\"macro\"] = all_fpr\ntpr[\"macro\"] = mean_tpr\nroc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n\n# Plot all ROC curves\nplt.figure()\nplt.figure(figsize=(14,8))\nlw = 2\nplt.plot(fpr[\"micro\"], tpr[\"micro\"],\n         label='micro-average ROC curve (area = {0:0.2f})'\n               ''.format(roc_auc[\"micro\"]),\n         color='deeppink', linestyle=':', linewidth=4)\n\nplt.plot(fpr[\"macro\"], tpr[\"macro\"],\n         label='macro-average ROC curve (area = {0:0.2f})'\n               ''.format(roc_auc[\"macro\"]),\n         color='navy', linestyle=':', linewidth=4)\n\ncolors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\nfor i, color in zip(range(n_classes), colors):\n    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n             label='ROC curve of class {0} (area = {1:0.2f})'\n             ''.format(i, roc_auc[i]))\n\n#plt.plot([0, 1], [0, 1], 'k--', lw=lw)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate',fontsize = 25)\nplt.ylabel('True Positive Rate',fontsize = 25)\nplt.title('ROC CURVE DECISION TREE',fontsize = 30)\nplt.legend(loc=\"lower right\",prop={'size': 23})\nplt.savefig(\"DT_ROC.png\",bbox_inches='tight')","7a4489ac":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree\nmodel = tree.DecisionTreeClassifier()\nmodel.fit(X_train, y_train)","0f7bc6e3":"from sklearn import tree\nclf = tree.DecisionTreeClassifier(criterion = 'entropy' , random_state=42,max_depth = 25)\n#scores = cross_val_score(X_train, y_train, cv=5)\nclf = clf.fit(X_train,y_train)\ny_pred = clf.predict(X_test)\nscore = clf.score(X_test,y_test)\n\n# plt.figure(figsize=(20,18))\n# tree.plot_tree(clf)","1df96aab":"# y_pred = clf.predict(data_model_df)\n# y_test = Acquired_Status\n# score = clf.score(data_model_df,y_test)","ee7125f1":"import seaborn as sns\ndef print_ConfusionMatrix(actual, pred,score,algo):\n  cm = confusion_matrix(actual, pred)\n  sns.set(font_scale=2)\n  plt.figure(figsize=(9,9))\n  sns.heatmap(cm, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'YlOrRd');\n  plt.ylabel('Actual label');\n  plt.xlabel('Predicted label');\n  all_sample_title = 'Accuracy Score: {0}'.format(round(score,4))\n  plt.title(all_sample_title)\n  plt.savefig(algo, bbox_inches='tight')\n \nprint_ConfusionMatrix(y_test, y_pred,score,'decision_tree.png')","1673f6be":"#plt.figure(figsize=(20,18))\n#tree.plot_tree(clf)\n","078f9259":"clf = LR(random_state=0,max_iter = 1000).fit(X_train, y_train)\ny_pred = clf.predict(X_test)\nscore = clf.score(X_test, y_test)\nprint_ConfusionMatrix(y_test, y_pred,score,\"LR_CM.png\")","ad7d0338":"#Random Forest confusion matrix\nclf = RF(random_state=0).fit(X_train, y_train)\ny_pred = clf.predict(X_test)\nscore = clf.score(X_test, y_test)\nprint_ConfusionMatrix(y_test, y_pred,score,\"RF.png\")","f528fa40":"# y_pred = clf.predict(data_model_df)\n# y_test = Acquired_Status\n# score = clf.score(data_model_df,y_test)\n# print_ConfusionMatrix(y_test, y_pred,score,\"RF.png\")","44e91c31":"from sklearn.preprocessing import label_binarize\n\ny = Acquired_Status\ny = label_binarize(Acquired_Status, classes=['Did Not Participate', 'MadeAcquisitions', 'WasAcquired'])\nn_classes = y.shape[1]\n\nrandom_state = np.random.RandomState(0)\nn_samples, n_features = X.shape\n# print(df.shape)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\n\nclassifier = OneVsRestClassifier(dt(criterion = 'entropy' ,max_depth = 15))\ny_score = classifier.fit(X_train,y_train).predict(X_test)\n\nfrom sklearn.multiclass import OneVsRestClassifier\n# Run classifier\nclassifier = OneVsRestClassifier(dt(criterion = 'entropy' ,max_depth = 15))\ny_score = classifier.fit(X_train,y_train).predict(X_test)\n\nfrom sklearn.metrics import precision_recall_curve\n\n\n# For each class\nprecision = dict()\nrecall = dict()\naverage_precision = dict()\nfor i in range(n_classes):\n    precision[i], recall[i], _ = precision_recall_curve(y_test[:,i], y_score[:,i])\n    average_precision[i] = average_precision_score(y_test[:, i], y_score[:, i])\n\n# A \"micro-average\": quantifying score on all classes jointly\nprecision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(y_test.ravel(),\n    y_score.ravel())\naverage_precision[\"micro\"] = average_precision_score(y_test, y_score,\n                                                     average=\"micro\")\nprint('Average precision score, micro-averaged over all classes: {0:0.2f}'\n      .format(average_precision[\"micro\"]))\n","54804094":"plt.figure()\nplt.figure(figsize=(14,8))\nplt.step(recall['micro'], precision['micro'], where='post')\n\nplt.xlabel('Recall',fontsize = 25)\nplt.ylabel('Precision',fontsize = 25)\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\nplt.title(\n    'Average precision score, micro-averaged over all classes: AP={0:0.2f}'\n    .format(average_precision[\"micro\"]),fontsize=30)","7db9d3b3":"from itertools import cycle\n# setup plot details\ncolors = cycle(['navy', 'turquoise', 'darkorange', 'cornflowerblue', 'teal'])\n\nplt.figure(figsize=(14,8))\nf_scores = np.linspace(0.2, 0.8, num=4)\nlines = []\nlabels = []\nfor f_score in f_scores:\n    x = np.linspace(0.01, 1)\n    y = f_score * x \/ (2 * x - f_score)\n    l, = plt.plot(x[y >= 0], y[y >= 0], color='gray', alpha=0.2)\n    plt.annotate('f1={0:0.1f}'.format(f_score), xy=(0.9, y[45] + 0.02))\n    lines.append(l)\nlabels.append('iso-f1 curves')\nl, = plt.plot(recall[\"micro\"], precision[\"micro\"], color='gold', lw=2)\nlines.append(l)\nlabels.append('micro-average Precision-recall (area = {0:0.2f})'\n              ''.format(average_precision[\"micro\"]))\n\nfor i, color in zip(range(n_classes), colors):\n    l, = plt.plot(recall[i], precision[i], color=color, lw=2)\n    lines.append(l)\n    labels.append('Precision-recall for class {0} (area = {1:0.2f})'\n                  ''.format(i, average_precision[i]))\n\nfig = plt.gcf()\nfig.subplots_adjust(bottom=0.1)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('Recall',fontsize=25)\nplt.ylabel('Precision',fontsize = 25)\nplt.title('Precision-Recall Class',fontsize=30)\nplt.legend(lines, labels, loc=(0, -.38), prop=dict(size=14))\nplt.show()","2e8704f6":"# from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n# # Interaction terms\n# poly = PolynomialFeatures(degree=2)\n# X_train = pd.DataFrame(poly.fit_transform(X_train), columns=poly.get_feature_names(X.columns))\n# X_test = pd.DataFrame(poly.transform(X_test), columns=poly.get_feature_names(X.columns))\n# # Standardize data\n# scaler = StandardScaler()\n# X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n# X_test = pd.DataFrame(scaler.transform(X_test), columns=X_train.columns)","5a1f41eb":"# # Import your necessary dependencies\n# from sklearn.feature_selection import RFE\n# from sklearn.linear_model import LogisticRegression\n# # Feature extraction\n# model = LogisticRegression(max_iter = 1000)\n# rfe = RFE(model, 14)\n# fit = rfe.fit(X_train, y_train)","d8518fd4":"# from sklearn.feature_selection import RFECV\n \n# # RFE\n# from sklearn.svm import SVC\n# clf = LR(random_state=0,max_iter = 1000)\n\n# rfe = RFECV(estimator=clf, cv=4, scoring='accuracy')\n# rfe = rfe.fit(X_train, y_train)\n \n# # Select variables and calulate test accuracy\n# cols = X_train.columns[rfe.support_]\n# acc = accuracy_score(y_test, rfe.estimator_.predict(X_test[cols]))\n# print('Number of features selected: {}'.format(rfe.n_features_))\n# print('Test Accuracy {}'.format(acc))\n \n# # Plot number of features vs CV scores\n# plt.figure()\n# plt.xlabel('k')\n# plt.ylabel('CV accuracy')\n# plt.plot(np.arange(1, rfe.grid_scores_.size+1), rfe.grid_scores_)\n# plt.show()","5955ad44":"# print(X.columns[fit.n_features_])\n# idx = np.where(fit.support_ == 1)\n# print(X.columns[idx])\n\n","fd981593":"# import matplotlib.pyplot as plt\n# from sklearn.svm import SVC\n# from sklearn.model_selection import StratifiedKFold\n# from sklearn.feature_selection import RFECV\n# from sklearn.datasets import make_classification\n\n\n\n# # Create the RFE object and compute a cross-validated score.\n# svc = SVC(kernel=\"linear\")\n# # The \"accuracy\" scoring is proportional to the number of correct\n# # classifications\n# rfecv = RFECV(estimator=svc, step=1, cv=StratifiedKFold(2),\n#               scoring='accuracy')\n# rfecv.fit(X, y)\n\n# print(\"Optimal number of features : %d\" % rfecv.n_features_)\n\n# # Plot number of features VS. cross-validation scores\n# plt.figure()\n# plt.xlabel(\"Number of features selected\")\n# plt.ylabel(\"Cross validation score (nb of correct classifications)\")\n# plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\n# plt.show()","d9e4a6d2":"# select = rfecv.get_support()\n# idx = np.where(select)\n# print(X.columns[idx])","2b02ee8b":"# data_model_best_subset_df = data_model_df[X.columns[idx]]\n# data_model_best_subset_df.head()\n# X_train, X_test, y_train, y_test = train_test_split(data_model_best_subset_df, Acquired_Status, test_size=0.33, random_state=42)","bb9b5d7b":"# model_name=['naive_bayes','RandomForestClassifier','KNeighborsClassifier','GradientBoostingClassifier','AdaBoostClassifier','DecisionTreeClassifier','LogisticRegression']\n# models_list= [NB(),RF(),KNN(),GB(),ab(),dt(),LR(max_iter=600)]\n# for i, j in zip(model_name, models_list):\n#     scores = cross_val_score(j, X_train, y_train, cv=5)\n    \n#     print(i+\"--\"+ \"Accuracy: %0.2f (+\/- %0.2f)\" % (scores.mean(), scores.std() * 2))","f5bcc38b":"dfold = pd.read_excel('\/kaggle\/input\/modelknn\/Data_knn.xlsx')\ndf = pd.read_excel('\/kaggle\/input\/datamodel\/data_mod.xlsx')\n\nsample_df = df[df['Acquisition Status']=='WasAcquired']\nsample_df = sample_df.append(df[df['Acquisition Status']=='MadeAcquisitions'])\nsample_df = sample_df.append(df[df['Acquisition Status']=='Did Not Participate'].sample(n=1500, random_state=1))\n#np.unique(sample_df['Acquisition Status'],return_counts = True)\ndf = sample_df\n\nextra = df\nextra.shape","1ad37335":"df= df.drop(['Unnamed: 0'],axis=1)\ndf = df.drop(['Unnamed: 0.1'],axis = 1)\ndf = df.drop(['Job Title'],axis = 1)\ndf = df.drop(['Acquired Price'],axis = 1)\n","d20169f7":"df = pd.get_dummies(df, columns=['Headquaters','Estimated Revenue','Founders',\n                                   'Number of employees','Funding Rounds','Funding Status','Active Products','Funding Amount',\n                                   'Nunmber of Lead Investors','Nmber of Investors','IPO status','Founded Organization','Acquired By','Acquisition Status','Acquisitions',\n                                    'Portfolio Companies','Founded Year','Exit Date'])","11577adf":"df1 = df['Type'].str.get_dummies(sep=',')\ndf11 = pd.DataFrame(df1)","273ed7a5":"df = df.drop(['Type'],axis = 1)","5d081493":"df = df.join(df11)\n","27afc3c3":"extra.dtypes\nextra= extra.drop(['Unnamed: 0'],axis=1)\nextra = extra.drop(['Unnamed: 0.1'],axis = 1)\nextra = extra.drop(['Founded Year'],axis = 1)\nextra = extra.drop(['Exit Date'],axis = 1)\nextra = extra.drop(['Job Title'],axis = 1)\nextra = extra.drop(['Acquired Price'],axis = 1)\nextra = extra.drop(['Acquired By'],axis = 1)\nextra = extra.drop(['Acquisitions'],axis = 1)\nextra.isnull().sum()","83758e6e":"extra['Headquaters'].mode()","6624b3d1":"extra['Headquaters'] = extra['Headquaters'].fillna('North America')","5f68c8a9":"#kmodes\nfrom kmodes.kmodes import KModes\n\nkm = KModes(n_clusters=8, init='Huang', n_init=8, verbose=1)\nclusters = km.fit_predict(extra)\n\n\n# Print the cluster centroids\nprint(km.cluster_centroids_)\n\n\n#ploting\ncost = pd.DataFrame(km.epoch_costs_, columns=['cost'])\ncc= pd.DataFrame(range(len(km.epoch_costs_)), columns=['index'])\ncost1 = cc.join(cost)\nprint(cost1)","7fcced72":"cost.plot.line()","9e8f2bbb":"from scipy.sparse import csr_matrix\n\nmovie_features_df_matrix = csr_matrix(df.values)\n# movie_features_df_matrix.shape\nfrom sklearn.neighbors import NearestNeighbors\n\n\nmodel_knn = NearestNeighbors(metric = 'cosine', algorithm = 'brute',radius = 1.5)\nmodel_knn.fit(movie_features_df_matrix)","a3d2d3fa":"sectors=dfold.iloc[:,1:3]\nquery_index = np.random.choice(df.shape[0])\nprint(sectors.loc[query_index]['Name'])\nprint(sectors.loc[query_index]['Type'])\n","88c118eb":"distances, indices = model_knn.kneighbors(df.iloc[query_index,:].values.reshape(1, -1), n_neighbors = 4)\ndistances = distances[0][1:6]\nindices = indices[0][1:6]\nprint(distances,indices)","82701194":"print('Prediction for : ',sectors.loc[query_index]['Name'])\nprint('Sector :',sectors.loc[query_index]['Type'])\nprint('---------------------------------------------')\nprint('---------------------------------------------')\nfor i in indices:\n  print('Company it could participate with is : ')\n  print('{0} :'.format(sectors.iloc[i]['Name']))\n  print('Sector :{0}'.format(sectors.iloc[i]['Type']))\n  print('---------------------------------------------')","0924d1bd":"# Relationship between Funding Amount and IPO status","e75d424a":"# Funding Rounds","579de967":"### Unseen Data","2e0c391a":"### Down Sample 1500 samples of \"Did Not Participate\" due to imbalanced Data","b9c2904f":"### Importing libraries for Naive Bayes, Decision Tree and other boosting algorithms","eed13d6b":"# Number of Lead Investors","a392b843":"# Number of Acquisitions","c960e872":"# Founded organization","21b5c3bf":"## Confusion Matrix","e327c95c":"### Calculating the cost for number of clusters and plotting it","41322cbc":"## Graph of Logistic Regression","0e836854":"### Acquisitions contain 7k null values consider dropping the column","cbed0edf":"### QUES 1 (Milestone 1)","e61c35f2":"## Fitting  and plotting the Decision Tree","4918cef9":"# Funding Amount","524f9d17":"### Logistic Regression","0b521c8b":"# Predicting whether a company will take part in Merger or Acquisition Process","732996ca":"# Data Cleaning","f8b15bb9":"# Exit Year","f7db40ad":"# Clustering","79df0790":"# Portfolio Companies","1adcd692":"# Estimated Revenue","89bf8d79":"### Down Sampling","2495c2be":"## Naive Bayes, KNN, Gradient Boosting , Ada Boost, Decision Tree","0d877c04":"# Active Products","2e705630":"### Random Forest","fc069d08":"# Founders Data","14400215":"# Top 30 Sectors","dde98ede":"## RFE","7f4f18ce":"### QUES 2 (Milestone 2 )","b9b2cab3":"# Predicting the best companies, a company should merge with for a profitable Merger ","98fb9285":" ## Training Curve for Ada Boost","e54a429d":"### Best Subset Selection","52d4b0c7":"### Mixed Sampling","74e73d58":"### Normalize Active Products","44b4e8a5":"### Precision Recall DT","6ca67481":"# Continent wise distribution","b6235ffe":"# Distribution of companies by founding year","529b96b0":"# Funding Status","bcc02e9a":"# Decision Tree Learning Curve","9ef6da7e":"## ROC Curve Decision Tree","3a795306":"# Acquisition Statuses","b25dd130":"### Graph for Naive Bayes","c8016d17":"# Model Building","a540f630":"### One hot Encoding","cb941f83":"# Number of Investors","2e4f8f4f":"# Number of Employees","4dc1d4ea":"# Top 6 Job Titles"}}