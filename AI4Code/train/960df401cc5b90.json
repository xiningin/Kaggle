{"cell_type":{"97ad8f3c":"code","35ed3d03":"code","e4e2a440":"code","725cce73":"code","73bae7a4":"code","09fe01d2":"code","8b483b1e":"code","2d950834":"code","e053815e":"code","a865672f":"code","f930644e":"code","482c348f":"code","01d78f39":"code","ed1be553":"code","ac855c34":"markdown","a385d903":"markdown","2e5a2b6d":"markdown","c0dd4bf7":"markdown","12b1822a":"markdown","0ee71fcf":"markdown","878f9bce":"markdown","2fc4e41a":"markdown","3c5a30a9":"markdown","8dd113e5":"markdown","9445a3d8":"markdown","6f78fbc2":"markdown","b82c6e68":"markdown","ffb86124":"markdown","d36e4714":"markdown","e5215487":"markdown","91568bec":"markdown","e38c2720":"markdown","dfce2ee1":"markdown"},"source":{"97ad8f3c":"import numpy as np \nimport pandas as pd\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","35ed3d03":"# import BERT tokenization\n\n!wget --quiet https:\/\/raw.githubusercontent.com\/tensorflow\/models\/master\/official\/nlp\/bert\/tokenization.py","e4e2a440":"import tokenization\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom keras.utils import to_categorical\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split","725cce73":"train_data = pd.read_csv('\/kaggle\/input\/covid-19-nlp-text-classification\/Corona_NLP_train.csv', encoding='latin-1')\ntest_data = pd.read_csv('\/kaggle\/input\/covid-19-nlp-text-classification\/Corona_NLP_test.csv', encoding='latin-1')","73bae7a4":"train_data.head()","09fe01d2":"test_data.head()","8b483b1e":"label = preprocessing.LabelEncoder()\ny = label.fit_transform(train_data['Sentiment'])\ny = to_categorical(y)\nprint(y[:5])","2d950834":"m_url = 'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-12_H-768_A-12\/2'\nbert_layer = hub.KerasLayer(m_url, trainable=True)","e053815e":"vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\n\ndef bert_encode(texts, tokenizer, max_len=512):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n        text = tokenizer.tokenize(text)\n        \n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len-len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence) + [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n        \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)","a865672f":"def build_model(bert_layer, max_len=512):\n    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    segment_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n    \n    pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n    \n    clf_output = sequence_output[:, 0, :]\n    \n    lay = tf.keras.layers.Dense(64, activation='relu')(clf_output)\n    lay = tf.keras.layers.Dropout(0.2)(lay)\n    lay = tf.keras.layers.Dense(32, activation='relu')(lay)\n    lay = tf.keras.layers.Dropout(0.2)(lay)\n    out = tf.keras.layers.Dense(5, activation='softmax')(lay)\n    \n    model = tf.keras.models.Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n    model.compile(tf.keras.optimizers.Adam(lr=2e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n    \n    return model","f930644e":"max_len = 250\ntrain_input = bert_encode(train_data.OriginalTweet.values, tokenizer, max_len=max_len)\ntest_input = bert_encode(test_data.OriginalTweet.values, tokenizer, max_len=max_len)\ntrain_labels = y","482c348f":"labels = label.classes_\nprint(labels)","01d78f39":"model = build_model(bert_layer, max_len=max_len)\nmodel.summary()","ed1be553":"checkpoint = tf.keras.callbacks.ModelCheckpoint('model.h5', monitor='val_accuracy', save_best_only=True, verbose=1)\nearlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, verbose=1)\n\ntrain_sh = model.fit(\n    train_input, train_labels,\n    validation_split=0.2,\n    epochs=3,\n    callbacks=[checkpoint, earlystopping],\n    batch_size=32,\n    verbose=1\n)","ac855c34":"**Label encoding of labels**","a385d903":"**Build The Model**","2e5a2b6d":"BERT relies on a Transformer (the attention mechanism that learns contextual relationships between words in a text). A basic Transformer consists of an encoder to read the text input and a decoder to produce a prediction for the task. Since BERT\u2019s goal is to generate a language representation model, it only needs the encoder part. The input to the encoder for BERT is a sequence of tokens, which are first converted into vectors and then processed in the neural network. But before processing can start, BERT needs the input to be massaged and decorated with some extra metadata:\n\n1. Token embeddings: A [CLS] token is added to the input word tokens at the beginning of the first sentence and a [SEP] token is inserted at the end of each sentence.\n2. Segment embeddings: A marker indicating Sentence A or Sentence B is added to each token. This allows the encoder to distinguish between sentences.\n3. Positional embeddings: A positional embedding is added to each token to indicate its position in the sentence.","c0dd4bf7":"**BERT is a deep learning model that has given state-of-the-art results on a wide variety of natural language processing tasks. It stands for Bidirectional Encoder Representations for Transformers. It has been pre-trained on Wikipedia and BooksCorpus and requires task-specific fine-tuning**","12b1822a":"![](data:image\/png;base64,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)","0ee71fcf":"**3. How does it work?**","878f9bce":"**2. Why BERT?**","2fc4e41a":"**BERT can be used for a wide variety of language tasks, while only adding a small layer to the core model**\n\n1. Classification tasks such as sentiment analysis are done similarly to Next Sentence classification, by adding a classification layer on top of the Transformer output for the [CLS] token.\n\n2. In Question Answering tasks (e.g. SQuAD v1.1), the software receives a question regarding a text sequence and is required to mark the answer in the sequence. Using BERT, a Q&A model can be trained by learning two extra vectors that mark the beginning and the end of the answer.\n\n3. In Named Entity Recognition (NER), the software receives a text sequence and is required to mark the various types of entities (Person, Organization, Date, etc) that appear in the text. Using BERT, a NER model can be trained by feeding the output vector of each token into a classification layer that predicts the NER label.","3c5a30a9":"**Encoding the text**","8dd113e5":"we create a BERT vocab_file in the form a numpy array. We then set the text to lowercase and finally we pass our vocab_file and do_lower_case variables to the Tokenizer object.","9445a3d8":"**Import Libraries and Data** ","6f78fbc2":"Now we are all set to create our model. To do so, we will create a function named build_model that having tf.keras.models.Model class. Inside the function we will define our model layers. Our model will consist of three **Dense** neural network layers and also dropout layer. We have chosen a learning rate to 1e-5.\n\n**RELU function** :- \nWith default values, this returns max(x, 0), the element-wise maximum of 0 and the input tensor.\nModifying default parameters allows you to use non-zero thresholds, change the max value of the activation, and to use a non-zero multiple of the input for values below the threshold.\n\n\n**Softmax function** :-\nSoftmax converts a real vector to a vector of categorical probabilities.\nThe elements of the output vector are in range (0, 1) and sum to 1.\nEach vector is handled independently. The axis argument sets which axis of the input the function is applied along.\nSoftmax is often used as the activation for the last layer of a classification network because the result could be interpreted as a probability distribution.\nThe softmax of each vector x is computed as exp(x) \/ tf.reduce_sum(exp(x)).\n\n**Binary corssentropy**:-\nComputes the cross-entropy loss between true labels and predicted labels.\nWe can use this cross-entropy loss when there are only two label classes (assumed to be 0 and 1). For each example, there should be a single floating-point value per prediction.","b82c6e68":"we create a BERT embedding layer by importing the BERT model from hub.KerasLayer","ffb86124":"* BERT was built upon recent work and clever ideas in pre-training contextual representations including Semi-supervised Sequence Learning, Generative Pre-Training, ELMo, the OpenAI Transformer, ULMFit and the Transformer. Although these models are all unidirectional or shallowly bidirectional, BERT is fully bidirectional.\n* BERT gives it incredible accuracy and performance on smaller data sets which solves a huge problem in natural language processing.","d36e4714":"**1. What is BERT?**","e5215487":"**Build a BERT layer**","91568bec":"**4. How to use BERT?**","e38c2720":"**Run the model**","dfce2ee1":"Here We check only the first 250 characters of each text, and also we set train-test input and train labels"}}