{"cell_type":{"572d150c":"code","f8802824":"code","404f6f85":"code","d2548e6f":"code","5cf5e06b":"code","6bd1a5d5":"code","6c62de0e":"code","09d7c537":"code","51f3ae02":"code","a0d2e1f8":"code","0b867b6a":"code","309aec65":"code","1becf833":"code","91e617ff":"code","9100c2aa":"code","be014a8b":"code","1756c5ac":"code","3d858441":"code","d5b590a0":"code","03f8e750":"code","19111b31":"code","f75fc0bf":"code","0cf607bb":"code","f07f347c":"code","28d4bcbd":"code","319cb5e7":"code","61fa4809":"code","3b69c4b4":"code","1d0741f7":"code","23950df0":"code","f29f3b48":"code","35d875ac":"code","ae0f82ec":"code","599f2d57":"code","e132a56b":"code","7499fc6b":"code","db5acdb4":"code","daef094e":"code","52bd25be":"code","797bd358":"code","5c1dec7d":"code","bc1e7be5":"code","da9d316c":"code","c5c28413":"code","700f2297":"code","cbf41e87":"code","7baa9b1f":"code","b96eee90":"code","c6fb328c":"markdown","70769ba0":"markdown","33d5a2a5":"markdown","123d66a5":"markdown","f30e87bb":"markdown","8ba28a71":"markdown","ff5531d0":"markdown","7b41777a":"markdown","5abb2006":"markdown","ea17d104":"markdown","8a2514d1":"markdown","02d675c8":"markdown","d11cc90e":"markdown","078b52ed":"markdown","ea94f60b":"markdown","bd4cb599":"markdown","944fa4aa":"markdown","6562f23c":"markdown","2e1b155f":"markdown"},"source":{"572d150c":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report, recall_score, precision_score, f1_score, roc_auc_score, roc_curve\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense","f8802824":"df = pd.read_csv(\"..\/input\/heart-attack-analysis-prediction-dataset\/heart.csv\")","404f6f85":"df = df.sample(frac = 1, random_state = 56).reset_index(drop = True)\n\ndisplay(df.head(3))\n\ndf.shape","d2548e6f":"df.isnull().sum()","5cf5e06b":"df.info()","6bd1a5d5":"df.describe().T","6c62de0e":"df.hist(bins = 30, figsize = (18, 25));","09d7c537":"features_num = [\"age\", \"trtbps\",\"chol\",\"thalachh\",\"oldpeak\"]\n\nfeatures_cat = ['sex','exng','caa','cp','fbs','restecg','slp','thall']\n\nscaler = StandardScaler()\nohe = OneHotEncoder(sparse = False)\n\nscaled_columns = scaler.fit_transform(df[features_num]) \nencoded_columns = ohe.fit_transform(df[features_cat])\n\nX = np.concatenate([scaled_columns, encoded_columns], axis = 1)\ny = df['output']","51f3ae02":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 4)","a0d2e1f8":"nb = GaussianNB()","0b867b6a":"nb.fit(X_train, y_train)\n\ny_pred_nb = nb.predict(X_test)","309aec65":"y_pred_nb = nb.predict(X_test)\n\naccuracy_nb = accuracy_score(y_test, y_pred_nb)\nrecall_nb = recall_score(y_test, y_pred_nb)\nprecision_nb = precision_score(y_test, y_pred_nb)\nf1_nb = f1_score(y_test, y_pred_nb)\nroc_nb = roc_auc_score(y_test, y_pred_nb)","1becf833":"print(classification_report(y_test, y_pred_nb))","91e617ff":"sns.heatmap(confusion_matrix(y_test, y_pred_nb),annot = True, fmt = 'd')\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\n\nplt.show()","9100c2aa":"dt = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)","be014a8b":"dt.fit(X_train, y_train)","1756c5ac":"y_pred_dt = dt.predict(X_test)\n\naccuracy_dt = accuracy_score(y_test, y_pred_dt)\nrecall_dt = recall_score(y_test, y_pred_dt)\nprecision_dt = precision_score(y_test, y_pred_dt)\nf1_dt = f1_score(y_test, y_pred_dt)\nroc_dt = roc_auc_score(y_test, y_pred_dt)","3d858441":"print(classification_report(y_test, y_pred_dt))","d5b590a0":"sns.heatmap(confusion_matrix(y_test, y_pred_dt),annot = True, fmt = 'd')\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\n\nplt.show()","03f8e750":"rf = RandomForestClassifier(n_estimators = 250, criterion= 'entropy', random_state = 0)","19111b31":"rf.fit(X_train, y_train)","f75fc0bf":"y_pred_rf = rf.predict(X_test)\n\naccuracy_rf = accuracy_score(y_test, y_pred_rf)\nrecall_rf = recall_score(y_test, y_pred_rf)\nprecision_rf = precision_score(y_test, y_pred_rf)\nf1_rf = f1_score(y_test, y_pred_rf)\nroc_rf = roc_auc_score(y_test, y_pred_rf)","0cf607bb":"print(classification_report(y_test, y_pred_rf))","f07f347c":"sns.heatmap(confusion_matrix(y_test, y_pred_rf),annot = True, fmt = 'd')\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\n\nplt.show()","28d4bcbd":"knn = KNeighborsClassifier(n_neighbors = 5)","319cb5e7":"knn.fit(X_train, y_train)","61fa4809":"y_pred_knn = knn.predict(X_test)\n\naccuracy_knn = accuracy_score(y_test, y_pred_knn)\nrecall_knn = recall_score(y_test, y_pred_knn)\nprecision_knn = precision_score(y_test, y_pred_knn)\nf1_knn = f1_score(y_test, y_pred_knn)\nroc_knn = roc_auc_score(y_test, y_pred_knn)","3b69c4b4":"print(classification_report(y_test, y_pred_knn))","1d0741f7":"sns.heatmap(confusion_matrix(y_test, y_pred_dt),annot = True, fmt = 'd')\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\n\nplt.show()","23950df0":"rl = LogisticRegression(solver = 'liblinear', max_iter = 250, random_state = 0)","f29f3b48":"rl.fit(X_train, y_train)","35d875ac":"y_pred_rl = rl.predict(X_test)\n\naccuracy_rl = accuracy_score(y_test, y_pred_rl)\nrecall_rl = recall_score(y_test, y_pred_rl)\nprecision_rl = precision_score(y_test, y_pred_rl)\nf1_rl = f1_score(y_test, y_pred_rl)\nroc_rl = roc_auc_score(y_test, y_pred_rl)","ae0f82ec":"print(classification_report(y_test, y_pred_rl))","599f2d57":"sns.heatmap(confusion_matrix(y_test, y_pred_rl),annot = True, fmt = 'd')\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\n\nplt.show()","e132a56b":"svm = SVC(kernel = 'rbf', degree = 3, gamma = 'auto', random_state = 0)","7499fc6b":"svm.fit(X_train, y_train)","db5acdb4":"y_pred_svm = svm.predict(X_test)\n\naccuracy_svm = accuracy_score(y_test, y_pred_svm)\nrecall_svm = recall_score(y_test, y_pred_svm)\nprecision_svm = precision_score(y_test, y_pred_svm)\nf1_svm = f1_score(y_test, y_pred_svm)\nroc_svm = roc_auc_score(y_test, y_pred_svm)","daef094e":"print(classification_report(y_test, y_pred_svm))","52bd25be":"sns.heatmap(confusion_matrix(y_test, y_pred_svm),annot = True, fmt = 'd')\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\n\nplt.show()","797bd358":"rn = MLPClassifier(verbose = True, max_iter= 350, tol = 0.00010)","5c1dec7d":"rn.fit(X_train, y_train)","bc1e7be5":"y_pred_rn = rn.predict(X_test)\n\naccuracy_rn = accuracy_score(y_test, y_pred_rn)\nrecall_rn = recall_score(y_test, y_pred_rn)\nprecision_rn = precision_score(y_test, y_pred_rn)\nf1_rn = f1_score(y_test, y_pred_rn)\nroc_rn = roc_auc_score(y_test, y_pred_rn)","da9d316c":"print(classification_report(y_test, y_pred_rn))","c5c28413":"sns.heatmap(confusion_matrix(y_test, y_pred_rn),annot = True, fmt = 'd')\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\n\nplt.show()","700f2297":"models = [('Naive Bayes', accuracy_nb, recall_nb, precision_nb, f1_nb, roc_nb),\n          ('Decision Tree', accuracy_dt, recall_dt, precision_dt, f1_dt, roc_dt),\n          ('Random Forest', accuracy_rf, recall_rf, precision_rf, f1_rf, roc_rf),\n          ('kNN', accuracy_knn, recall_knn, precision_knn, f1_knn, roc_knn),\n          ('Logistic Regression', accuracy_rl, recall_rl, precision_rl, f1_rl, roc_rl),\n          ('SVM', accuracy_svm, recall_svm, precision_svm, f1_svm, roc_svm),\n          ('Neural Networks', accuracy_rn, recall_rn, precision_rn, f1_rn, roc_rn)]\n\ndf_all_models = pd.DataFrame(models, columns = ['Model', 'Accuracy (%)', 'Recall (%)', 'Precision (%)', 'F1 (%)', 'AUC'])\n\ndf_all_models","cbf41e87":"plt.style.use(\"dark_background\")\n\nplt.subplots(figsize=(12, 10))\nsns.barplot(y = df_all_models['Model'], x = df_all_models['Accuracy (%)'], palette = 'icefire')\nplt.xlabel(\"Models\")\nplt.title('Accuracy')\nplt.show()","7baa9b1f":"r_probs = [0 for _ in range(len(y_test))]\nr_auc = roc_auc_score(y_test, r_probs)\nr_fpr, r_tpr, _ = roc_curve(y_test, r_probs)\n\nfpr_nb, tpr_nb, _ = roc_curve(y_test, y_pred_nb)\nfpr_dt, tpr_dt, _ = roc_curve(y_test, y_pred_dt)\nfpr_rf, tpr_rf, _ = roc_curve(y_test, y_pred_rf)\nfpr_knn, tpr_knn, _ = roc_curve(y_test, y_pred_knn)\nfpr_rl, tpr_rl, _ = roc_curve(y_test, y_pred_rl)\nfpr_svm, tpr_svm, _ = roc_curve(y_test, y_pred_svm)\nfpr_rn, tpr_rn, _ = roc_curve(y_test, y_pred_rn)","b96eee90":"sns.set_style('darkgrid')\n\nplt.plot(r_fpr, r_tpr, linestyle='--', label='Random prediction (AUROC = %0.3f)' % r_auc)\n\nplt.plot(fpr_nb, tpr_nb, marker='.', label='Naive Bayes (AUROC = %0.3f)' % roc_nb)\nplt.plot(fpr_dt, tpr_dt, marker='.', label='Decision Tree (AUROC = %0.3f)' % roc_dt)\nplt.plot(fpr_rf, tpr_rf, marker='.', label='Random Forest (AUROC = %0.3f)' % roc_rf)\nplt.plot(fpr_knn, tpr_knn, marker='.', label='kNN (AUROC = %0.3f)' % roc_knn)\nplt.plot(fpr_rl, tpr_rl, marker='.', label='Logistic Regression (AUROC = %0.3f)' % roc_rl)\nplt.plot(fpr_svm, tpr_svm, marker='.', label='SVM (AUROC = %0.3f)' % roc_svm)\nplt.plot(fpr_rn, tpr_rn, marker='.', label='Neural Networks (AUROC = %0.3f)' % roc_rn)\n\nplt.title('ROC Plot')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend() \nplt.show()","c6fb328c":"## 4. Creation of models","70769ba0":"As expected, we have no missing values :D","33d5a2a5":"###### 1. Naive Bayes\nThe Naive Bayes algorithm is a simple classification algorithm that uses historical data to predict the classification of new data. It works by calculating the probability of an event occurring given that another event has already occurred.","123d66a5":"###### 3. Random Forest\nThe Random Forest algorithm creates a forest in a random way, creating several decision trees and combining them, each tree tries to estimate a ranking and this is called as \u201cvote\u201d, thus to obtain a more accurate and more stable prediction.","f30e87bb":"## 2. Starting...","8ba28a71":"###### Defining the features of our model\nFor this, we will create the variable X that will receive the characteristic variables of our model, and the variable y that will receive the target variable of our model.","ff5531d0":"###### 7. Neural networks\nThe purpose of the Neural Networks algorithm is to imitate the nervous system of humans in the learning process, it is inspired by biological neural networks","7b41777a":"###### 6. SVM (Support Vector Machines)\nThe SVM algorithm separates data points using a line. This line is chosen in such a way that it will be the most important of the closest data points in 2 categories.","5abb2006":"## 5. Viewing the results of all models","ea17d104":"###### 2. Decision Tree\nThe Decision Tree algorithm are statistical models that use supervised training for data classification and prediction. These models use the divide-and-conquer strategy: a complex problem is decomposed into simpler sub-problems and recursively this technique is applied to each sub. -problem","8a2514d1":"## 3. Dividing into training and testing sets\nNow we need to convert our data into training and testing sets. We will use 75% as our training data and test our model on the remaining 25% with Scikit-learn's train_test_split function.","02d675c8":"The ``describe()`` function generates a lot of information about numeric variables that can also be useful:","d11cc90e":"###### Finding Missing Data (NaN)\nOur dataset does not have missing values as it says on the Kaggle, but just in case:","078b52ed":"###### 4. kNN\nThe KNN or k-nearest neighbor algorithm is a very simple machine learning algorithm. It uses some sort of similarity measure to tell which class the new data falls into, in which case we'll use 5 nearest neighbors.","ea94f60b":"###### 5. Logistic Regression\nLogistic regression algorithm is used where a discrete output is expected, (eg Predict whether a user is a good or bad payer). Typically, logistic regression uses some function to squeeze values into a given range.","bd4cb599":"We have 303 rows and 14 columns present in the dataset provided, 13 of which are characteristic variables (input data) and one of them is a target variable (which we want our model to be able to predict).\n\nThe characteristic variables are:\n\n    age - Age of the patient\n    sex - Sex of the patient\n    cp - Chest Pain type chest pain type\n    trtbps - resting blood pressure (in mm Hg)\n    chol - cholestoral in mg\/dl fetched via BMI sensor\n    fbs - (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false)\n    rest_ecg  resting electrocardiographic results\n    thalach - maximum heart rate achieved\n    exang - exercise induced angina (1 = yes; 0 = no)\n    oldpeak - Previous peak\n    slp - Slope\n    caa -  number of major vessels (0-3)\n    thall - Thal rate\n\nThe target variable is:\n\n     output - a *binary* type that indicates the user's less or more chance of heart attack:\n             0= less chance of heart attack \n             1= more chance of heart attack","944fa4aa":"## 1. Imports from libraries","6562f23c":"We can see that in the charts we have different types of columns, being categorical and numerical:\n\nCategorical columns: sex, exng, caa, cp, fbs, restecg, slp and thall.\n\nNumeric columns: age, trtbps, chol, thalachh and oldpeak.","2e1b155f":"## Heart Attack Analysis & Prediction Dataset\n![hearthattack.png](attachment:670c4a68-c402-4e77-8dc3-fcafe088ec53.png)\n\n###### Dataset information:\n\n- The data was collected to train a model to distinguish between people less likely to have heart attacks and people more likely to have heart attacks, so the whole problem is a binary classification.\n\nThe dataset can be found on the `` Kaggle`` platform at the link below:\n\nhttps:\/\/www.kaggle.com\/rashikrahmanpritom\/heart-attack-analysis-prediction-dataset"}}