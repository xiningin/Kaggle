{"cell_type":{"ef6f9d9b":"code","18bf42c8":"code","ac5422d4":"code","28af7da4":"code","4b0181ea":"code","5652caf7":"code","903d8d10":"code","61f96d6c":"code","52b7c5f3":"code","2d0371e0":"code","1ed80642":"code","ae83ee52":"code","5642ee98":"code","ef64dea9":"code","a49b9da6":"code","d01a00e5":"markdown","6c32e178":"markdown"},"source":{"ef6f9d9b":"import numpy as np\nimport tensorflow as tf\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.sequence import TimeseriesGenerator","18bf42c8":"dataset = pd.read_csv(\"..\/input\/praktikum-rnn-if4074\/train.csv\")\ndataset","ac5422d4":"from sklearn.preprocessing import MinMaxScaler\n\ndataset = dataset.drop(columns=['Id', 'Name', 'Symbol', 'Date', 'High', 'Low', 'Open', 'Volume', 'Marketcap'])","28af7da4":"dataset","4b0181ea":"train, test = train_test_split(dataset,test_size=0.2)","5652caf7":"tstrain = TimeseriesGenerator(np.array(train), np.array(train), length=10, sampling_rate=1)\ntstest = TimeseriesGenerator(np.array(test), np.array(test), length=10, sampling_rate=1)","903d8d10":"from keras.models import Sequential\nfrom keras.layers import Dense, GRU\nfrom keras.layers import LSTM, Bidirectional\nfrom keras.layers import Dropout\nfrom keras.layers import BatchNormalization\n\nregressor = Sequential()\n\nregressor.add(Bidirectional(LSTM(512), input_shape=(10,1)))\n\nregressor.add(Dense(units = 1))","61f96d6c":"regressor.summary()","52b7c5f3":"regressor.compile(optimizer = 'adam', loss = 'mean_squared_error')\nregressor.fit(tstrain, epochs = 30, batch_size = 32, validation_data=tstest)","2d0371e0":"x = np.array(dataset.iloc[-10:]).astype('float32').flatten().tolist()\n# predict_multi(x, regressor, 10, 10)\nfor i in range(200):\n    prediction = regressor.predict(np.reshape(x[i:i+10], (1, 10, 1)).astype('float32'))\n    x.append(prediction[0][0])\nlen(x)","1ed80642":"sub = \"Id,Close\\n\"\nfor i,j in zip(range(2119,2319),x[10:]):\n    sub+=str(i) + ',' + str(j) + '\\n'\nwith open(\".\/submission.csv\",'w') as f:\n    f.write(sub)","ae83ee52":"from keras.models import Sequential\nfrom keras.layers import Dense, GRU\nfrom keras.layers import LSTM, Bidirectional\nfrom keras.layers import Dropout\nfrom keras.layers import BatchNormalization\n\nmodel_usulan_1 = Sequential()\n\n\nmodel_usulan_1.add(Bidirectional(LSTM(256, return_sequences=True, input_shape=(10,1))))\nmodel_usulan_1.add(GRU(512))\nmodel_usulan_1.add(Dense(units = 1))","5642ee98":"model_usulan_1.compile(optimizer = 'adam', loss = 'mean_squared_error')\nmodel_usulan_1.fit(tstrain, epochs = 5, batch_size = 32, validation_data=tstest)","ef64dea9":"model3 = Sequential()\n\nmodel3.add(LSTM(units = 50, return_sequences=True, input_shape = (14, 6)))\nmodel3.add(Dropout(0.2))\n\nmodel3.add(LSTM(units = 256))\nmodel3.add(Dropout(0.2))\n\nmodel3.add(Dense(units = 6))","a49b9da6":"model3.compile(optimizer = 'adam', loss = 'mean_squared_error')\nmodel3.fit(tstrain, epochs = 5, batch_size = 32, validation_data=tstest)","d01a00e5":"## Usulan Arsitektur , By : Aqil Abdul Aziz\nUntuk usulan arsitektur saya, saya merasa sangat banyak yang bisa diimprove dari model yang sudah kami buat, ini dikarenakan underperformance dari model kami yang sangat kentara sehingga kami merasa sangat mungkin dan banyak cara untuk diimprove. <br>\nUsulan arsitektur baru yang saya sarankan adalah :\n1. Bi LST, 256\n2. GRU, 512\n3. Dense, 1 \n\n## Penjelasan\nArsitektur ini banyak memanfaatkan GRU, ini dilakukan karena GRU lebih cost efficient dari LSTM, serta karena berdasarkan riset yang saya coba, GRU berperforma lebih baik dengan data yang kurang dan inadequate. Bi LSTM dipakai untuk pemanfaatan informasi yang lebih banyak untuk network. Dense di akhir adalah untuk flattening sesuai jumlah variable yang diperlukan untuk output.","6c32e178":"# Usulan arsitektur by Valentinus Devin Setiadi\n\nUntuk usulan arsitektur saya, menurut saya masih banyak yang dapat diimprove dari model yang sudah kami buat karena adanya underperformance dari model kami. Saya mencoba membuat arsitektur baru dengan mengurangi jumlah layer dan tetap menggunakan LSTM dengan jumlah neuron yang lebih banyak. Model yang kompleks dengan banyak layer membuat kinerja model kurang baik sehingga saya mengusahakan membuat layer yang lebih sedikit dengan jumlah neuron yang lebih banyak dan tetap menggunakan layer dropout dengan harapan peningkatan akurasi yang lebih baik."}}