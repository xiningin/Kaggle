{"cell_type":{"97e47aed":"code","82a05961":"code","4e4f037e":"code","d019e325":"code","d31ca5ed":"code","fcb0a6d9":"code","8904a7cb":"code","5186d7e8":"code","d37cc172":"code","49c3420a":"code","800b4b6f":"code","04cb802e":"code","19ea7dfa":"code","7860bd27":"code","4896caef":"code","04ebcb0f":"code","00ce3af2":"code","82f94539":"code","924fc08c":"code","1c600c50":"code","f75f4500":"code","52e78326":"markdown","6970f6ca":"markdown","fa65ff1d":"markdown","5fad952a":"markdown","c5f30603":"markdown","8f515c9c":"markdown","3f1ebfb1":"markdown","d9b0311a":"markdown","470367de":"markdown","57330499":"markdown","698c694b":"markdown","bc1fbcde":"markdown","749f9e77":"markdown","4db4b00d":"markdown","27a6eb95":"markdown"},"source":{"97e47aed":"# Importing libraries:\n\nimport re\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport wordcloud\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\nnltk.download('stopwords')\nnltk.download('punkt')\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.svm import SVC\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import confusion_matrix, plot_confusion_matrix\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nimport warnings\nwarnings.filterwarnings('ignore')\nwarnings.simplefilter('ignore')","82a05961":"train = pd.read_csv('..\/input\/twitter-sentiment-analysis-analytics-vidya\/train_E6oV3lV.csv')\ntest = pd.read_csv('..\/input\/twitter-sentiment-analysis-analytics-vidya\/test_tweets_anuFYb8.csv')\ntrain.head()","4e4f037e":"# Checking Shape of Train and Test sets:\nprint(\"Shape of Train set:\", train.shape)\nprint(\"Shape of Test set:\", test.shape)","d019e325":"# Removing duplicates of Train set. There are few duplicates in Test set as well,\n# however, duplicates of Test set can'b be removed because the final test with label has to be uploaded as a submission\n\ntrain.drop_duplicates(subset=['tweet'], keep='last', inplace=True)\ntrain.reset_index(inplace=True)\nprint(\"Shape of Train set after removing duplicates:\", train.shape)","d31ca5ed":"train[train['tweet'].map(lambda x: x.isascii())]\ntest[test['tweet'].map(lambda x: x.isascii())]\n\n# Cleaning tweets\ndef clean_tweets(text):\n    text = re.sub(r'@[A-Za-z0-9_]+','',text)    # Removing @mentions\n    text = re.sub(r'#','',text)                 # Removing #tag symbol\n    text = re.sub(r'RT[\\s]+',' ',text)          # Remvoing RT\n    text = re.sub(r'\\n','',text) \n    text = re.sub(r',','',text) \n    text = re.sub(r'.[.]+','',text) \n    text = re.sub(r'\\w+:\\\/\\\/\\S+','',text) \n    text = re.sub(r'https?:\\\/\\\/\\S+','',text)    # Removing hyperlinks\n    text = re.sub(r'\/',' ',text)\n    text = re.sub(r'-',' ',text)\n    text = re.sub(r'_',' ',text)\n    text = re.sub(r'!','',text)\n    text = re.sub(r':',' ',text)\n    text = re.sub(r'$','',text)\n    text = re.sub(r'%','',text)\n    text = re.sub(r'^','',text)\n    text = re.sub(r'&','',text)\n    text = re.sub(r'=',' ',text)\n    text = re.sub(r' +',' ',text)               # Removing extra whitespaces\n\n    return text\n\n# Removing Emojis\ndef clean_emoji(inputString):\n    return inputString.encode('ascii', 'ignore').decode('ascii')\n\ntrain['tweet'] = train['tweet'].apply(clean_tweets)    # Applying function to clean tweets\ntrain['tweet'] = train['tweet'].apply(clean_emoji)     # Applying function to remove emojis\ntrain['tweet'] = train.tweet.str.lower()               # Making all texts to lower case\ntrain['tweet'] = train['tweet'].str.strip()            # Removing leading and trailing whitespaces\n\ntest['tweet'] = test['tweet'].apply(clean_tweets)      # Applying function to clean tweets\ntest['tweet'] = test['tweet'].apply(clean_emoji)       # Applying function to remove emojis\ntest['tweet'] = test.tweet.str.lower()                 # Making all texts to lower case\ntest['tweet'] = test['tweet'].str.strip()              # Removing leading and trailing whitespaces\n#pd.set_option('display.max_rows', None)\n#pd.set_option('display.max_colwidth', -1)","fcb0a6d9":"train.head()","8904a7cb":"test.head()","5186d7e8":"df = train.copy()\ndf.drop(['index'],axis=1, inplace=True)\n#pd.set_option('display.max_rows', None)\npd.set_option('display.max_colwidth', None) # To display full length text of column.\ndf","d37cc172":"print(\"Dataset shape: \", df.shape)\ndf['label'].value_counts()","49c3420a":"import string\nstring.punctuation","800b4b6f":"df['tweet'] = df['tweet'].astype(str)\npunctuations_list = string.punctuation\ndef cleaning_punctuations(text):\n    translator = str.maketrans('', '', punctuations_list)\n    return text.translate(translator)\n\ndf['tweet'] = df['tweet'].apply(lambda x: cleaning_punctuations(x))","04cb802e":"sw = stopwords.words('english')\ndf['tweet'] = df['tweet'].apply(lambda x: ' '.join([word for word in x.split() if word not in (sw)]))","19ea7dfa":"def cleaning_numbers(text):\n    return re.sub('[0-9]+', '', text)\n\ndf['tweet'] = df['tweet'].apply(lambda text: cleaning_numbers(text))","7860bd27":"# Stemming requires tokens, hence convertting tweets into Tokens\ntokens = (word_tokenize(i) for i in df.tweet)\ndf['tweet'] = df['tweet'].apply(nltk.word_tokenize)\ndf.head()","4896caef":"stemm = SnowballStemmer('english')\ndf['tweet'] = df['tweet'].apply(lambda x: [stemm.stem(y) for y in x])","04ebcb0f":"X = df['tweet'].astype(str)  # Converting to string, because vectorizer does'nt accept list.\ny = df['label'].astype(str)  # Converting to string, because vectorizer does'nt accept list.\nX_train, X_test, y_train, y_test =  train_test_split(X, y, test_size = 0.2, random_state = 3) # random_state=3 because 3 is my favourite number :D","00ce3af2":"common_words=''\nfor i in df.tweet:\n    i = str(i)\n    tokens = i.split()\n    common_words += \" \".join(tokens)+\" \"\nwordcloud = wordcloud.WordCloud().generate(common_words)\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","82f94539":"# Extracting features using TF-IDF (1,2) - unigrams and bigrams\nvectoriser = TfidfVectorizer(ngram_range=(1,2), max_features=500000)\nvectoriser.fit(X_train)\nprint('No. of feature_words: ', len(vectoriser.get_feature_names()))","924fc08c":"X_train = vectoriser.transform(X_train)\nX_test  = vectoriser.transform(X_test)","1c600c50":"# making a dictionary with four models with some parameters:\nmodels = {\n    \n    'SVC' :{\n        'model' : SVC(),\n        'parameters' : {\n            'C': [0.1, 1, 10, 100, 1000], 'gamma': [1, 0.1, 0.01, 0.001], 'kernel': ['rbf','linear','poly','sigmoid']\n        }\n    },\n\n    'MultinomialNB' :{\n        'model' : MultinomialNB(),\n        'parameters' : {\n            'alpha' : np.linspace(0.5, 1.5, 6), 'fit_prior' : [True, False]\n        }\n    },\n    \n    'logistics_regression' :{\n        'model' : LogisticRegression(solver = 'lbfgs', multi_class = 'auto'),\n        'parameters' : {\n            'C' : [0.1, 1, 10, 50, 60, 90, 100], 'solver' : ['lbfgs', 'liblinear']\n        }\n    },\n    \n    'random_forest' :{\n        'model' : RandomForestClassifier(),\n        'parameters' : {\n            'n_estimators' : [80,85,90,95,100], \n            'max_depth':[20,30,None], 'criterion':['gini','entropy']\n        }\n    }\n}","f75f4500":"# implemented GridSearchCV for four models using a loop and a previously created dictionary\n# in the created variable 'scores', results are stored for each model such as: model, best_score and best_params.\n\nscore = []\n\nfor model_name, mp in models.items():\n    clf = GridSearchCV(mp['model'], mp['parameters'], cv=5, n_jobs=-1) # Using Cross Validation of 5 and n_jobs=-1 for fast training by using all the processors\n    print(mp['model'])\n    print('\\nFitting...')\n    best_model = clf.fit(X_train, y_train)                      # Training the model\n    clf_pred = best_model.predict(X_test)                       # Predicting the results\n    print(confusion_matrix(y_test,clf_pred))                    # Printing Confusion Matrix\n    print(metrics.classification_report(y_test, clf_pred))      # Printing Classification Report\n    score.append({                                              # Appending results to 'scores' list\n        'model' : model_name,\n        'best_accuracy' : best_model.score(X_test, y_test),\n        'best_parameters' : clf.best_params_\n    })\n    print('\\nThe score is appended to the list...\\n')\n    \n# Creating DataFrame with model, best_accuracy and best_parameters:\nres = pd.DataFrame(score, columns=['model', 'best_accuracy', 'best_parameters'])\nres","52e78326":"### Plotting Word Cloud:","6970f6ca":"### Stemming:","fa65ff1d":"### Fitting the Count Vectorizer","5fad952a":"# Conclusion:\nThis is an unbalanced dataset, so we have to focus more on precision, recall, and F1-score, rather than just accuracy. From the above result, it can be concluded that **SVC has better precision, recall, and F1-score for the 'Toxic' label**. So, I'm going to train the **SVC model** with 100% train data, so that it gets better training and will use the test set to predict & label them and will use that file for submission.\n\n**Note:** I have also experimented **'Count Vectorizer -> ngram_range=(1,1): unigrams and ngram_range=(1,2): unigram - bigrams'**, that gave very close result to **'TF-IDF Vectorizer -> ngram_range=(1,1): unigrams and ngram_range=(1,2): unigram - bigrams'**, however, **'TF-IDF with ngram_range=(1,2): unigram - bigrams'** turns out to be little bit better than **'Count Vectorizer -> ngram_range=(1,1): unigrams and ngram_range=(1,2): unigram - bigrams'** and **'TF-IDF ngram_range=(1,1): unigram'**.\n\n**Imp:** This will take around 5 hrs to train all the models.                                                                                           \nI was able to score max **76.87% accuracy** on the unlabeled test set. There were **17272 registered users** and I was on **70th rank out of 1280 users** who were on the **leaderboard** at the time of adding files on Kaggle and GitHub.\n**Leaderboard:** https:\/\/datahack.analyticsvidhya.com\/contest\/practice-problem-twitter-sentiment-analysis\/#LeaderBoard \n\n![image.png](attachment:9c6cf00d-c0a5-4d58-a2df-9d998d918507.png)\n","c5f30603":"# Labels are as follows:\nlabel '1' ---> racist\/sexist tweet           \nlabel '0' ---> not racist\/sexist tweet","8f515c9c":"### Cleaning and removing Punctuations:","3f1ebfb1":"### Tokenizing Tweets:","d9b0311a":"# Extracting features using CountVectorizer - unigrams\nvectoriser = CountVectorizer(ngram_range=(1,2))\nvectoriser.fit(X_train)\nprint('No. of feature_words: ', len(vectoriser.get_feature_names()))  ","470367de":"### Transforming the data using TF-IDF Vectorizer","57330499":"### Removing Numeric numbers:","698c694b":"### Fitting the TF-IDF Vectorizer","bc1fbcde":"### Splitting data into Train and Test sets","749f9e77":"# Cleaning data:","4db4b00d":"# Transforming Dataset using TF-IDF Vectorizer","27a6eb95":"### Removing Stopwords:"}}