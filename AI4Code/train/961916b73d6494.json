{"cell_type":{"583842a7":"code","66743da0":"code","af486766":"code","62b852ce":"code","d6468c12":"code","37077344":"code","23ef7e05":"code","ad433c79":"code","2c9a427a":"code","294cb3c9":"code","072fb250":"code","7de2cadc":"code","451216da":"code","1c95964d":"code","c0de514d":"code","ee6c79d0":"code","e40e511a":"code","f4eccb97":"code","1df1a883":"code","9bcaad59":"code","40bf1905":"code","61b68238":"code","aeb1846f":"code","29d06c99":"code","d4e351c3":"code","03432616":"code","54fbaf8f":"code","26f9e950":"code","dc3c116c":"code","0458d807":"code","8f209937":"code","1e24f16b":"code","0bcd482f":"code","d2c17eff":"code","635c26eb":"code","ffd1f451":"code","d45f6907":"code","79d0913a":"code","c2103137":"code","41102a09":"code","bdf570a4":"code","aa917f7d":"code","2cb284a2":"code","84f7bbe3":"code","61f78687":"code","1341a9d8":"code","a28c789e":"code","585c447e":"code","6287cf5a":"code","2497ffab":"code","d2de232b":"code","24a783a1":"code","72d4454f":"code","fb3947c4":"code","06be6cca":"code","e06e231a":"code","4b103594":"code","c2b0e7fd":"code","822baff1":"code","e36ff806":"code","87696679":"code","6a2933f5":"code","de95a471":"code","43a3a19c":"code","fb55b2a8":"code","5de354ce":"code","41debd40":"code","ec50981c":"code","12891fae":"code","abed33c6":"code","528a21ca":"code","d3987cf0":"code","42c11321":"code","8662172b":"markdown"},"source":{"583842a7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","66743da0":"# In this project we have separate csv for train and test dataset\n# So we perform all operations on train dataset first , and then to test data set.\n# and we apply linear regression","af486766":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score, mean_squared_error\n\n%matplotlib inline","62b852ce":"# Reading the dataset\ntrain = pd.read_csv('..\/input\/bigmart-dataset\/Train.csv', delimiter = ',')\ntrain.head()","d6468c12":"train.shape","37077344":"# Checking whether null values are present\ntrain.isnull().sum() ","23ef7e05":"train['Item_Fat_Content'].unique()","ad433c79":"train['Outlet_Size'].unique()","2c9a427a":"train['Outlet_Size'].mode()[0]","294cb3c9":"train['Outlet_Identifier'].unique()","072fb250":"train.groupby(['Outlet_Identifier','Outlet_Size']).nunique('Outlet_Size')\n\n## Out of 10 Unique Outlets, only for 7 Outlet size ia available'\n## Need to find the outlet_size for OUT010, OUT017, OUT045\n","7de2cadc":"#train[train['Outlet_Size'] == np.nan].head()\nprint(train[train['Outlet_Identifier']=='OUT010']['Outlet_Size'].size)\ntrain[train['Outlet_Identifier']=='OUT010']['Outlet_Size'].isna().sum()","451216da":"train[train['Outlet_Identifier']=='OUT010'].head(2)","1c95964d":"print(train[train['Outlet_Identifier']=='OUT017']['Outlet_Size'].size)\ntrain[train['Outlet_Identifier']=='OUT017']['Outlet_Size'].isna().sum()","c0de514d":"train[train['Outlet_Identifier']=='OUT017'].head(2)","ee6c79d0":"print(train[train['Outlet_Identifier']=='OUT045']['Outlet_Size'].size)\ntrain[train['Outlet_Identifier']=='OUT045']['Outlet_Size'].isna().sum()","e40e511a":"train[train['Outlet_Identifier']=='OUT045'].head(2)","f4eccb97":"train.groupby(['Outlet_Size','Outlet_Location_Type','Outlet_Type']).nunique()","1df1a883":"# Filling Nans in Outlet_Size with the identified value above\ntrain['Outlet_Size']=train['Outlet_Size'].fillna('Small')","9bcaad59":"train.isnull().sum()","40bf1905":"train.groupby(['Item_Type'])['Item_Weight'].mean('Item_Weight')","61b68238":"train.groupby(['Item_Type'])['Item_Weight'].mean('Item_Weight')\ntrain[train['Item_Type']=='Frozen Foods']['Item_Weight'].mean()","aeb1846f":"print(train[train['Item_Type']=='Baking Goods']['Item_Weight'].size)\ntrain[train['Item_Type']=='Baking Goods']['Item_Weight'].isna().sum()","29d06c99":"print(train[train['Item_Type']=='Breads']['Item_Weight'].size)\ntrain[train['Item_Type']=='Breads']['Item_Weight'].isna().sum()\n\n# Insight -- With the above, most of the Item types have null values","d4e351c3":"#Filling Nans in Item_Weight by mean value of the particular Item_Type\n\ntrain['Item_Weight'] = train['Item_Weight'].fillna(train.groupby('Item_Type')['Item_Weight'].transform('mean'))","03432616":"train.isnull().sum()","54fbaf8f":"train.head()","26f9e950":"train['Item_Visibility'].hist(bins=20)","dc3c116c":"#Treating Outliers in Item_Visibility Column\nq1 = train['Item_Visibility'].quantile(0.25)\nq2 = train['Item_Visibility'].quantile(0.50)\nq3 = train['Item_Visibility'].quantile(0.75)\niqr = q3 - q1\nmin_val = q1 - 1.5 * iqr\nmax_val = q3 + 1.5 * iqr","0458d807":"print('q1=',q1,'q2=',q2,'q3=',q3)\nprint(min_val,max_val,iqr)\ntrain['Item_Visibility'].value_counts()","8f209937":"train['Item_Visibility'] = np.where(train['Item_Visibility']  > max_val, q2, train['Item_Visibility'] )\ntrain['Item_Visibility'] = np.where(train['Item_Visibility']  < min_val, q2, train['Item_Visibility'] ) ","1e24f16b":"train['Item_Fat_Content'].value_counts()","0bcd482f":"# Treating Typos of Item_Fat_Content\ntrain['Item_Fat_Content'] =  train['Item_Fat_Content'].replace(['low fat','LF'],'Low Fat')\ntrain['Item_Fat_Content'] =  train['Item_Fat_Content'].replace(['reg'],'Regular')","d2c17eff":"train.head(2)","635c26eb":"le = LabelEncoder()\n#Transforming the categorical values","ffd1f451":"train['Item_Fat_Content'] = le.fit_transform(train['Item_Fat_Content'])","d45f6907":"train['Outlet_Size'] = le.fit_transform(train['Outlet_Size'])","79d0913a":"train['Outlet_Location_Type'] = le.fit_transform(train['Outlet_Location_Type'])","c2103137":"#Creating dummies for Outlet_Types\ndummy = pd.get_dummies(train['Outlet_Type'])\ndummy.head()","41102a09":"train = pd.concat([train,dummy],axis=1)","bdf570a4":"train.head()","aa917f7d":"train.dtypes","2cb284a2":"train = train.drop(['Item_Identifier','Item_Type','Outlet_Identifier','Outlet_Type','Outlet_Establishment_Year'],axis=1)","84f7bbe3":"train.head()","61f78687":"x = train.drop('Item_Outlet_Sales',axis=1)\ny = train['Item_Outlet_Sales']","1341a9d8":"test = pd.read_csv('..\/input\/bigmart-dataset\/Test.csv', delimiter = ',')","a28c789e":"test['Outlet_Identifier'].unique()","585c447e":"test['Outlet_Size']=test['Outlet_Size'].fillna('Small')","6287cf5a":"# Filling Nans in Item_Weight by mean value of the particular Item_Type\n\ntest['Item_Weight'] = test['Item_Weight'].fillna(test.groupby('Item_Type')['Item_Weight'].transform('mean'))","2497ffab":"# Treating Outliers in Item_Visibility Column\nq1 = test['Item_Visibility'].quantile(0.25)\nq2 = test['Item_Visibility'].quantile(0.50)\nq3 = test['Item_Visibility'].quantile(0.75)\niqr = q3 - q1\nmin_val = q1 - 1.5 * iqr\nmax_val = q3 + 1.5 * iqr\ntest['Item_Visibility'] = np.where(test['Item_Visibility']  > max_val, q2, test['Item_Visibility'] )\ntest['Item_Visibility'] = np.where(test['Item_Visibility']  < min_val, q2, test['Item_Visibility'] ) ","d2de232b":"# Treating Typos of Item_Fat_Content\ntest['Item_Fat_Content'] =  test['Item_Fat_Content'].replace(['low fat','LF'],'Low Fat')\ntest['Item_Fat_Content'] =  test['Item_Fat_Content'].replace(['reg'],'Regular')","24a783a1":"test['Item_Fat_Content'] = le.fit_transform(test['Item_Fat_Content'])\ntest['Outlet_Size'] = le.fit_transform(test['Outlet_Size'])\ntest['Outlet_Location_Type'] = le.fit_transform(test['Outlet_Location_Type'])\n# Creating dummies for Outlet_Types\ndmy = pd.get_dummies(test['Outlet_Type'])\ntest = pd.concat([test,dmy],axis=1)\ntest.head()","72d4454f":"test.dtypes","fb3947c4":"xt = test.drop(['Item_Identifier','Item_Type','Outlet_Identifier','Outlet_Type','Outlet_Establishment_Year'],axis=1)","06be6cca":"x.columns, xt.columns","e06e231a":"from sklearn import model_selection\nxtrain,xtest,ytrain,ytest=model_selection.train_test_split(x,y,test_size=0.3,random_state=42)","4b103594":"##  Applying Liner Regression ##\n##-----------------------------\nlin = LinearRegression()","c2b0e7fd":"lin.fit(xtrain,ytrain)\nprint(lin.coef_)\nlin.intercept_","822baff1":"predictions = lin.predict(xtest)","e36ff806":"print(\"RMSE:\",np.sqrt(mean_squared_error(ytest,predictions)))\nprint('R2 sqr:',r2_score(ytest, predictions))\nprint(xtrain.shape)\nprint(ytrain.shape)\nprint(xtest.shape)\nprint(ytest.shape)\nprint(predictions.shape)","87696679":"## Applying KNN for Regression ##\n##----------------------------------\nfrom sklearn import neighbors ","6a2933f5":"knnR = neighbors.KNeighborsRegressor(n_neighbors = 10)","de95a471":"knnR.fit(xtrain, ytrain)  #fit the model","43a3a19c":"pred=knnR.predict(xtest) #make prediction on test set","fb55b2a8":"from math import sqrt\nerror = sqrt(mean_squared_error(ytest,pred)) #calculate rmse   \nprint('RMSE:',error)\nprint('R2 sqr:',r2_score(ytest, pred))","5de354ce":"## Applying Ridge for Regression ##\n##----------------------------------\nfrom sklearn.linear_model import Ridge","41debd40":"ridge = Ridge(alpha=1.0)","ec50981c":"ridge.fit(xtrain, ytrain)  #fit the model","12891fae":"pred_train=ridge.predict(xtrain) #make prediction on train set\npred_test=ridge.predict(xtest) #make prediction on test set","abed33c6":"error = sqrt(mean_squared_error(ytrain,pred_train)) #calculate rmse   \nprint('RMSE:',error)\nprint('R2 sqr:',r2_score(ytrain, pred_train))\n\nerror = sqrt(mean_squared_error(ytest,pred_test)) #calculate rmse   \nprint('RMSE:',error)\nprint('R2 sqr:',r2_score(ytest, pred_test))","528a21ca":"## Applying Lasso for Regression ##\n##----------------------------------\nfrom sklearn.linear_model import Lasso\nlasso = Lasso(alpha=1.0)\n\nlasso.fit(xtrain, ytrain) \npred_train= lasso.predict(xtrain)\nprint('RMSE:',np.sqrt(mean_squared_error(ytrain,pred_train)))\nprint('R2 sqr:',r2_score(ytrain, pred_train))\n\npred_test= lasso.predict(xtest)\nprint('RMSE:',np.sqrt(mean_squared_error(ytest,pred_test))) \nprint('R2 sqr:',r2_score(ytest, pred_test))","d3987cf0":"## Applying Elasticnet for Regression ##\n##----------------------------------\nfrom sklearn.linear_model import ElasticNet\nenet = ElasticNet(alpha = 0.01)\nenet.fit(xtrain, ytrain) \n\npred_train= enet.predict(xtrain)\nprint('RMSE:',np.sqrt(mean_squared_error(ytrain,pred_train)))\nprint('R2 SQR:',r2_score(ytrain, pred_train))\n\npred_test= enet.predict(xtest)\nprint('RMSE:',np.sqrt(mean_squared_error(ytest,pred_test)))\nprint('R2 SQR:', r2_score(ytest, pred_test))","42c11321":"# Insights \n#---------\n# Linear -> \tR2 sqr: 0.5684678630829685\n# KNN Reg -> \tR2 sqr: 0.42021059506319336\n# Ridge -> \tR2 sqr: 0.5684964063069835\n# Lasso -> \tR2 sqr: 0.5688779527143972\n# Elastic Net -> \tR2 SQR: 0.5680487875386936","8662172b":"with the above values we can able to fill size for OUT010, OUT017, OUT045 and it will be 'Small'\n!-- Outlet_Size Outlet_Location_Type Outlet_Type\nHigh Tier 3 Supermarket Type1 Medium Tier 1 Supermarket Type1 Tier 3 Supermarket Type2 Supermarket Type3 Small Tier 1 Grocery Store\nSupermarket Type1 Tier 2 Supermarket Type1 --!\n\nOUT010 Small Tier 1 Grocery Store OUT017 Small Tier 2 Supermarket Type1 OUT045 Small Tier 2 Supermarket Type1"}}