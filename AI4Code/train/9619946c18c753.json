{"cell_type":{"e13460f4":"code","ef47381d":"code","92ecf419":"code","53f3dead":"code","c2389426":"code","366955de":"code","cf4d7f04":"code","83071c40":"code","6de01d65":"code","620f08c6":"code","fbbff858":"code","625750ab":"code","287c44ca":"code","192032c1":"code","83dd2fbb":"code","bbfe604d":"code","8bfe3b74":"code","fb24bd96":"code","60b8db17":"code","cc4223a3":"code","4e3c6214":"code","4549e736":"code","22934a9e":"code","763dc0d1":"code","0105b6aa":"code","90987ef1":"code","f3592d0b":"code","827fb418":"code","19186263":"code","401b9c96":"code","a9c2a923":"code","95e9d514":"code","3b8ddf31":"code","fcca7f60":"code","1db76f14":"code","961de99b":"code","d2bf9a71":"markdown","e5b1a034":"markdown","13fa6d01":"markdown","1d9027e7":"markdown","9a5272b7":"markdown","1222c671":"markdown","414991b2":"markdown","dcf7510f":"markdown","95ea64a7":"markdown","9ba32f75":"markdown","41f9a323":"markdown","34bf66e5":"markdown","3ddb9c81":"markdown","13d75745":"markdown","b2ee86c4":"markdown","212f411c":"markdown","10f1fe5f":"markdown","98dfdf6d":"markdown","61533a45":"markdown","938ecf9e":"markdown","28954869":"markdown","1b81e96f":"markdown","f301eaab":"markdown","e5085b24":"markdown","0ac36de8":"markdown","15b4720a":"markdown","74107f52":"markdown","b67dbdc4":"markdown","81099f9f":"markdown"},"source":{"e13460f4":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline","ef47381d":"train_df = pd.read_csv(\"..\/input\/mengary-revenue-prediction\/train.csv\");\ntest_df = pd.read_csv(\"..\/input\/mengary-revenue-prediction\/test.csv\");\n\ntotal_df = pd.concat((train_df, test_df))\n\ndfs = [train_df, test_df]\n\nprint(f\"Train: \\n{train_df.sample(5)}\")\nprint(f\"\\n\\nTest: \\n{test_df.sample(5)}\")","92ecf419":"train_df.info()","53f3dead":"test_df.info()","c2389426":"total_df[\"departure city\"].value_counts()","366955de":"total_df[\"departure state\"].value_counts()","cf4d7f04":"plt.hist(total_df[\"discount\"])","83071c40":"sns.pairplot(total_df[[\"price\", \"discount\", \"no of items\"]])","6de01d65":"sns.heatmap(total_df[[\"price\", \"discount\", \"no of items\"]].corr())","620f08c6":"plt.hist(total_df[\"price\"])","fbbff858":"plt.hist(train_df[\"price\"])","625750ab":"plt.hist(test_df[\"price\"])","287c44ca":"plt.hist(total_df[\"no of items\"])","192032c1":"total_df[\"sub-class\"].value_counts()","83dd2fbb":"total_df[\"segment\"].value_counts()","bbfe604d":"total_df[\"class\"].value_counts()","8bfe3b74":"total_df[\"location\"].value_counts()","fb24bd96":"total_df[\"delivery type\"].value_counts()","60b8db17":"plt.hist(( pd.to_datetime(total_df[\"delivery date\"]) -  pd.to_datetime(total_df[\"placement date\"]))\/np.timedelta64(1, 'D'))","cc4223a3":"top_cities = total_df[\"departure city\"].value_counts()[:14].index.values.tolist()\ntop_states = total_df[\"departure state\"].value_counts()[:14].index.values.tolist()\n\nfor df in dfs:\n    df[\"delivery date\"] = pd.to_datetime(df[\"delivery date\"])\n    df[\"placement date\"] = pd.to_datetime(df[\"placement date\"])\n    df[\"departure city filtered\"] = df[\"departure city\"].map(lambda x: x if x in top_cities else \"other city\")\n    df[\"departure state filtered\"] = df[\"departure state\"].map(lambda x: x if x in top_states else \"other state\")\n    df[\"date diff\"] = (df[\"delivery date\"] - df[\"placement date\"])\/np.timedelta64(1, 'D')\n    df[\"delivery date day\"] = df[\"delivery date\"].dt.day\n    df[\"delivery date month\"] = df[\"delivery date\"].dt.month\n    df[\"delivery date year\"] = df[\"delivery date\"].dt.year\n    df[\"delivery date weekday\"] = df[\"delivery date\"].dt.dayofweek\n\ntrain_df[\"profit fraction\"] = train_df[\"profit\"]\/train_df[\"price\"]\ntotal_df = pd.concat([train_df, test_df])","4e3c6214":"train_df.head()","4549e736":"test_df.head()","22934a9e":"from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n\ncat_columns = [\"class\", \"sub-class\", \"segment\", \"location\"]\n#                \"departure city filtered\", \n#     \"departure state filtered\"]\nfor column in cat_columns:\n    le = LabelEncoder()\n    ohe = OneHotEncoder()\n    ohe.fit(le.fit_transform(total_df[column]).reshape(-1, 1))\n    labels = list(le.classes_)\n    \n    for i, df in enumerate(dfs):\n        df[f\"{column} label\"] = le.transform(df[column])\n        features = ohe.transform(df[[f\"{column} label\"]]).toarray()\n        features_df = pd.DataFrame(features, columns=labels)\n        dfs[i] = pd.concat([df, features_df], axis = 1)","763dc0d1":"train_df = dfs[0]\ntest_df = dfs[1]","0105b6aa":"for df in dfs:\n    df[\"delivery type label\"] = df[\"delivery type\"].map({\n        \"Standard Class\": 0,\n        \"Second Class\": 1,\n        \"First Class\": 2,\n        \"Same Day\": 3\n    })","90987ef1":"train_df_features = dfs[0].drop([\n    \"id\", \n    \"departure city\", \n    \"location\", \n    \"class\", \n    \"segment\", \n    \"sub-class\", \n    \"delivery type\", \n    \"RID\",\n    \"delivery date\",\n    \"address code\",\n    \"departure state\",\n    \"placement date\",\n    \"departure city filtered\",\n    \"departure state filtered\",\n    \"class label\",\n    \"segment label\",\n    \"sub-class label\",\n    \"location label\"\n#     \"departure state filtered label\"\n#     \"departure city filtered label\"\n], axis=1)","f3592d0b":"train_df_features[\"delivery type label\"] = train_df_features[\"delivery type label\"].fillna(0)","827fb418":"# from sklearn.model_selection import cross_val_score, KFold\n# from sklearn.ensemble import RandomForestRegressor\n# from sklearn.preprocessing import MinMaxScaler\n\n# scaler = MinMaxScaler()\n# X = scaler.fit_transform(train_df_features.drop([\"profit fraction\", \"profit\"], axis=1))\n# # X = train_df_features.drop([\"profit fraction\", \"profit\"], axis=1)\n# y = np.array(train_df_features[\"profit fraction\"])\n# y_actual = np.array(train_df_features[\"profit\"])\n\n# kf = KFold(n_splits=5, shuffle=True, random_state=42)\n# scores_percent = []\n# scores_actual = []\n# for train_index, test_index in kf.split(X):\n#     reg = RandomForestRegressor(n_estimators=150, max_depth=10, max_leaf_nodes=34)\n#     reg.fit(X[train_index], y[train_index])\n#     preds = reg.predict(X[test_index])\n#     actual_preds = np.array(train_df_features[\"price\"])[test_index]*preds\n#     scores_percent.append(r2_score(y[test_index], preds))\n#     scores_actual.append(r2_score(y_actual[test_index], actual_preds))","19186263":"from sklearn.model_selection import cross_val_score, KFold\nfrom sklearn.ensemble import GradientBoostingRegressor, BaggingRegressor, ExtraTreesRegressor, AdaBoostRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.preprocessing import MinMaxScaler\n\nfrom sklearn.feature_selection import SelectKBest, mutual_info_regression\n\nscaler = MinMaxScaler()\n# X = scaler.fit_transform(train_df_features.drop([\"profit fraction\", \"profit\"], axis=1))\nX = train_df_features.drop([\"profit fraction\", \"profit\"], axis=1)\ny = train_df_features[\"profit fraction\"]\n\nselector = SelectKBest(mutual_info_regression, k=30)\nX = selector.fit_transform(X, y)\n\n# reg = ExtraTreesRegressor(max_depth=10, max_leaf_nodes=40, n_estimators=80)\nreg = BaggingRegressor(GradientBoostingRegressor(max_depth=10, max_leaf_nodes=25, n_estimators=100), n_estimators=200)\nscores = cross_val_score(reg, X, y, cv=5, scoring=\"r2\")","401b9c96":"scores","a9c2a923":"np.average(scores)","95e9d514":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\n\nreg = BaggingRegressor(GradientBoostingRegressor(n_estimators=100, max_depth=10, max_leaf_nodes=25), n_estimators=200)\n\nX = train_df_features.drop([\"profit fraction\", \"profit\"], axis=1)\ny = train_df_features[\"profit fraction\"]\ny_actual = train_df_features[\"profit\"]\nX_train, X_test, y_train, y_test, y_actual_train, y_actual_test = train_test_split(X, y, y_actual, test_size=0.2, random_state=42, shuffle=True)\nreg.fit(X_train, y_train)\npreds = reg.predict(X_test)\nprint(r2_score(y_test, preds))\n\n# calculating R2 score for `profit` instead of `profit fraction`\nactual_preds = preds*X_test[\"price\"]\nprint(r2_score(y_actual_test, actual_preds))","3b8ddf31":"test_df_features = dfs[1].drop([\n    \"id\", \n    \"departure city\", \n    \"location\", \n    \"class\", \n    \"segment\", \n    \"sub-class\", \n    \"delivery type\", \n    \"RID\",\n    \"delivery date\",\n    \"address code\",\n    \"departure state\",\n    \"placement date\",\n    \"departure city filtered\",\n    \"departure state filtered\",\n    \"class label\",\n    \"segment label\",\n    \"sub-class label\",\n    \"location label\"\n#     \"departure state filtered label\",\n#     \"departure city filtered label\"\n], axis=1)","fcca7f60":"reg = BaggingRegressor(GradientBoostingRegressor(n_estimators=100, max_depth=10, max_leaf_nodes=25), n_estimators=200)\nX = train_df_features.drop([\"profit fraction\", \"profit\"], axis=1)\ny = train_df_features[\"profit fraction\"]\nreg.fit(X, y)\n\nX_test = test_df_features\npreds = reg.predict(X_test)\n\n# calculate actual profits\nactual_preds = test_df_features[\"price\"]*preds\n\nfinal_df = pd.DataFrame(columns = [\"id\", \"profit\"])\nfinal_df[\"id\"] = dfs[1][\"id\"]\nfinal_df[\"profit\"] = actual_preds","1db76f14":"final_df","961de99b":"final_df.to_csv(\"submission.csv\", index=False)","d2bf9a71":"`class` shall be one-hot vectorized","e5b1a034":"Ordinal values in the order that seems obvious ","13fa6d01":"## Exploring The Data","1d9027e7":"The difference between placement and delivery date is not much most of the times. So only the `delivery date` is considered (`placment date` may be considered instead, does not make a difference). \n\nThe one month late deliveries may have some effect on prices. So, the difference of the dates shall be taken as a feature. ","9a5272b7":"The profit shall not be predicted directly from the model. The profit fraction (profit\/price) is a better candidate for prediction as it will have less variance. Predicting profit shall also be tried just for sanity check.","1222c671":"## Training on entire dataset and predicting on test dataset","414991b2":"`segment` shall be one-hot vctorized","dcf7510f":"## Model training and testing\n\n* Five fold cross validation is performed\n* Trial for both Min-Max scaled data and unscaled data shall be tried\n* Try to predict Profit directly instead of profit fraction\n* Linear regression, SVM regression, Decision Tree regression, Neural Network regression to be performed and hyperparameter tuning shall be done on the best among these","95ea64a7":"Not too many classes and distibution is not too uneven. Shall be one-hot vectorized. This feature shall be kept and tried as weell as dropped and tried as `class` already contains some information and `subclass` may cause it to overfit.","9ba32f75":"One hot encoding the required features","41f9a323":"Not much correlation between the continuous variables, so keeping all of them","34bf66e5":"Nothing to conclude from this","3ddb9c81":"**No. of items**","13d75745":"Nothing much to conclude, as this kind of distribution is to be expected.","b2ee86c4":"There is one null value on `delivery type` to be filled with the mode `Standard Class` (0)","212f411c":"## Summary from EDA for feature engineering\n\n* `departure city` and `departure state`: categorical value but only top 14 categories are taken and rest are classified as other. May be dropped.\n* `delivery date` and `placement date`: take either of these as features and separate into `year`, `month`, `date` and `weekday`. The difference of the dates shall be taken as a feature`  \n* `sub-class` and `class`: shall be one hot vectorized, one of these features may be dropped and tested.\n* `delivery type`: ordinal values\n* `segment`:  one-hot vector\n* `location`: one-hot vector\n* `price`, `discount` and `no.of items`: nothing to change. Oversampling may be done on `price` as it seems to be an important feature but has an uneven distribution.\n* remaining features to be dropped","10f1fe5f":"**Checking correlation between continuous variables**","98dfdf6d":"Drop remaining unnecessary features","61533a45":"Checking distribution in test data","938ecf9e":"Both the `departure state` and `departure city` have too many possible values and uneven distribution. But since it is categorical, the top 14 values are taken in separate categories and remaining shall be put in `other` category. These shall be one-hot vectorized as there is no sense of order.","28954869":"`location` shall be one-hot vectorized","1b81e96f":"There is only one null value for `delivery type` which shall be filled by the `mode`","f301eaab":"Trying to force an even distribution in Price by oversampling may be tried as the distribution is too skewed. Not trying this now as it won't affect the performance much since the test data has a similar distribution as the train data.","e5085b24":"### Sanity Check by doing usual train-dev splitting and also getting R2-score for actual profits","0ac36de8":"Checking distribution in train data","15b4720a":"## Summary of model selection\n\n* Decision tree performed the best with default parameters. Linear Regression and Neural Networks respectively underfit and overfit by a lot so no point in Hyperparameter tuning them. SVM performs roughly 1% less than Decision tree which means SVM may also be hyperparameter-tuned. But since SVMs take a long time to train, only decision trees are tried.\n* Predicting price directly gave very bad results (negative $r^2$ in some cases).\n* Decision trees best parameters are obtained at max_depth=8-10, max_leaf_nodes=25-35 and remaining parameters are best at default.\n* Ensemble methods tried while varying the decision tree hyperparameters in the above range.\n* Bagging, RF, GBDT, Bagging + GBDT, AdaBoost, Bagging + AdaBoost, ExtraTreesRegressor were tried. \n* Bagging + GBDT: GBDT => max_depth=10, max_leaf_nodes=25, n_estimators=100, Bagging=>n_estimators=200 gave the best results. (~95.47%)\n* Next, `delivery city`, `delivery state` were dropped  and performance increased by 0.5%. Keeping only either of these did not have much effect on performance.\n* Dropping `class` or `sub-class` decreased the performance of the model.\n* Selecting 20 best features (SelectKBest with mutual_info_regression) decreased the performance but selecting 30 best features increased the performance.\n* **Final features and model**: drop `delivery city`, `delivery state`, select best 30 features, do not perform feature-scaling, and use the above model discussed in the previous point.","74107f52":"## Feature engineering\n\nPerforming the changes discussed in the summary","b67dbdc4":"`delivery type` follows some order, so this shall be given ordinal values instead of one-hot vectorizing.","81099f9f":"**Price Histogram**"}}