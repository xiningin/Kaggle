{"cell_type":{"20a515a1":"code","4ed4de18":"code","a10e93be":"code","7cdf09c6":"code","17fd3176":"code","1c34809c":"code","bf8dd1f3":"code","df1cb6e4":"code","111883f9":"code","81a5c52f":"code","3266234c":"code","61738a92":"code","6cb0fec2":"code","b7d6f464":"code","94cd9e76":"code","52dc9a71":"code","3b151c42":"code","1d936531":"code","2dad01a2":"code","fe1b739d":"code","d1a701e1":"code","e15d6f1a":"code","25e4f141":"code","ab528438":"code","f57b68d2":"code","57d4bf74":"code","2f143e1b":"code","f016b7fa":"code","ef1d953d":"code","46f426aa":"code","08aec35a":"code","84a8a1f2":"code","32784d42":"code","8d0cf191":"markdown","57c4d9af":"markdown","73e386ff":"markdown","069143be":"markdown","95118c77":"markdown","7a52a4df":"markdown","ec93ea59":"markdown","79e155cb":"markdown","e4a6ebaa":"markdown","1c1457df":"markdown","bcd7d75c":"markdown","ef1b4818":"markdown","4b58bce4":"markdown","ee9c3689":"markdown","fd7add59":"markdown","ead741c0":"markdown","18928489":"markdown","88020fdb":"markdown","3588da46":"markdown","48d761cc":"markdown","df9a64de":"markdown","635f53f8":"markdown","a5f2d01e":"markdown"},"source":{"20a515a1":"import pandas as pd\nimport matplotlib.pyplot       as plt\nimport seaborn                 as sns\nimport plotly.express          as ex\nimport plotly.graph_objs       as go\nimport plotly.offline          as py\nimport plotly.express as px\ndata=pd.read_csv(r'..\/input\/rice-type-classification\/riceClassification.csv')\ndata","4ed4de18":"# Id cells are not useful\ndata=data.drop([\"id\"],axis=1)\n","a10e93be":"#Checking for Null Values\ndata.isna().sum()","7cdf09c6":"d = data.copy()\nd[\"Class\"] =  d[\"Class\"].map({1:'Class : 1',0:'Class : 0'})\nex.pie(d,names='Class',title='Rice Classes')","17fd3176":"data[\"Class\"].value_counts().plot(kind=\"bar\")","1c34809c":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition  import PCA\nN = 5 \npca_pipeline = Pipeline(steps = [\n    ('scale',StandardScaler()),\n    ('PCA',PCA(N))\n])\n\ntf_data = pca_pipeline.fit_transform(data.iloc[:,:9])\ntf_data = pd.DataFrame({'PC1':tf_data[:,0],'PC2':tf_data[:,1],'PC3':tf_data[:,2],'PC4':tf_data[:,3],'PC5':tf_data[:,4],\n                        'label':data.iloc[:,-1].map({0:'Class 0',1:'Class 1 '})})\nex.scatter_3d(tf_data,x='PC1',y='PC2',z='PC3',color='label',color_discrete_sequence=['blue','red'])","bf8dd1f3":"def var_distribution2(dataframe):\n    import matplotlib.pyplot as plt\n    numbers = pd.Series(dataframe.columns)\n    dataframe[numbers].hist(figsize=(14,14))\n    plt.show();\n    return dataframe.var()\nvar_distribution2(data)","df1cb6e4":"def Outliers(dataframe,cols):\n    import plotly.graph_objects as go\n    from plotly.subplots import make_subplots\n    numeric_col2=[]\n    for x in cols:\n        numeric_col2.append(x)\n\n    fig=make_subplots(rows=1, cols=len(cols))\n\n    for i,col in enumerate(numeric_col2):\n        fig.add_trace(go.Box(y=dataframe[col].values, name=dataframe[col].name), row=1, col=i+1)\n\n    return fig.show()\n\ncols=data.columns.values.tolist()\nOutliers(data,cols)","111883f9":"data.groupby(\"Class\").mean()","81a5c52f":"def corr(dataframe,target_variable):\n    import seaborn as sns\n    import matplotlib.pyplot as plt\n    fig, ax = plt.subplots(figsize=(10,10))\n    correlation_matrix = dataframe.corr().round(2)\n    sns.heatmap(data=correlation_matrix, annot=True)\n    \n    correlation = data.corr()[target_variable].abs().sort_values(ascending = False)\n    return correlation\ncorr(data,\"Class\")","3266234c":"fig1=px.scatter(data_frame=data,x='Class', y='MinorAxisLength', color='Class', hover_name='MinorAxisLength')\nfig1.update_layout(title=dict(text='MinorAxisLength vs Class', xanchor='center', yanchor='top', x=0.5))\nfig1.show()\n","61738a92":"fig1=px.scatter(data_frame=data,x='Class', y='AspectRation', color='Class', hover_name='AspectRation')\nfig1.update_layout(title=dict(text='AspectRation vs Class', xanchor='center', yanchor='top', x=0.5))\nfig1.show()\n","6cb0fec2":"fig1=px.scatter(data_frame=data,x='Class', y='Roundness', color='Class', hover_name='Roundness')\nfig1.update_layout(title=dict(text='Roundness vs Class', xanchor='center', yanchor='top', x=0.5))\nfig1.show()\n","b7d6f464":"fig1=px.scatter(data_frame=data,x='Class', y='Area', color='Class', hover_name='Area')\nfig1.update_layout(title=dict(text='Area vs Class', xanchor='center', yanchor='top', x=0.5))\nfig1.show()\n","94cd9e76":"fig1=px.scatter(data_frame=data,x='Class', y='ConvexArea', color='Class', hover_name='ConvexArea')\nfig1.update_layout(title=dict(text='ConvexArea vs Class', xanchor='center', yanchor='top', x=0.5))\nfig1.show()","52dc9a71":"fig1=px.scatter(data_frame=data,x='Class', y=\"EquivDiameter\", color='Class', hover_name=\"EquivDiameter\")\nfig1.update_layout(title=dict(text=\"EquivDiameter vs Class\", xanchor='center', yanchor='top', x=0.5))\nfig1.show()","3b151c42":"fig1=px.scatter(data_frame=data,x='Class', y='Eccentricity', color='Class', hover_name='Eccentricity')\nfig1.update_layout(title=dict(text='Eccentricity vs Class', xanchor='center', yanchor='top', x=0.5))\nfig1.show()","1d936531":"sns.FacetGrid(data, hue=\"Class\", height=6) \\\n   .map(sns.histplot, \"MinorAxisLength\") \\\n   .add_legend();\nplt.suptitle(\"MinorAxisLength\",size=28)\nplt.show();","2dad01a2":"sns.FacetGrid(data, hue=\"Class\", height=6) \\\n   .map(sns.histplot, \"AspectRation\") \\\n   .add_legend();\nplt.suptitle(\"AspectRation\",size=28)\nplt.show();","fe1b739d":"sns.FacetGrid(data, hue=\"Class\", height=6) \\\n   .map(sns.histplot, \"Roundness\") \\\n   .add_legend();\nplt.suptitle(\"Roundness\",size=28)\nplt.show();","d1a701e1":"sns.FacetGrid(data, hue=\"Class\", height=6) \\\n   .map(sns.histplot, \"Area\") \\\n   .add_legend();\nplt.suptitle(\"Area\",size=28)\nplt.show();","e15d6f1a":"sns.FacetGrid(data, hue=\"Class\", height=6) \\\n   .map(sns.histplot, \"ConvexArea\") \\\n   .add_legend();\nplt.suptitle(\"ConvexArea\",size=28)\nplt.show();","25e4f141":"sns.FacetGrid(data, hue=\"Class\", height=6) \\\n   .map(sns.histplot, \"EquivDiameter\") \\\n   .add_legend();\nplt.suptitle(\"EquivDiameter\",size=28)\nplt.show();","ab528438":"sns.FacetGrid(data, hue=\"Class\", height=6) \\\n   .map(sns.histplot, \"Eccentricity\") \\\n   .add_legend();\nplt.suptitle(\"Eccentricity\",size=28)\nplt.show();","f57b68d2":"data=data[~(data[\"MajorAxisLength\"]>=183.2144)]\ndata=data[~(data[\"MajorAxisLength\"]<=77.41707)]\ndata=data[~(data[\"Eccentricity\"]<=0.6798581)]\ndata=data[~(data[\"EquivDiameter\"]<=58.25104)]\ndata=data[~(data[\"Perimeter\"]<=200.587)]\ndata=data[~(data[\"Perimeter\"]>=476.522)]\ndata=data[~(data[\"Roundness\"]<=0.2992976)]","57d4bf74":"def feature_selector(dataframe,feature_number,target_variable):\n    from sklearn import datasets\n    from sklearn.feature_selection import RFE\n    from sklearn.linear_model import LogisticRegression\n    \n    dataframe2=dataframe[target_variable]\n    dataframe=dataframe.drop([target_variable],axis=1)\n     \n    n=feature_number\n    lr = LogisticRegression(solver=\"liblinear\")\n    rfe=RFE(lr,n)\n    rfe=rfe.fit(dataframe,dataframe2)\n    cols=[]\n    for x in dataframe.columns.values.tolist():\n        cols.append(x)\n    ranking=[]\n    for x in rfe.ranking_:\n        ranking.append(x)\n    n=0\n    for x in rfe.support_:\n        print(f\"{ranking[n]}: {x}----> {cols[n]}\")\n        n+=1\n    selected=[]\n    n=0\n    z=zip(dataframe.columns.values.tolist(),rfe.support_)\n    z=list(z)\n    for x in range(len(z)+1):\n    \n        try:\n            if str(z[n][1])==\"True\":\n                selected.append(z[n])\n            else:\n                pass\n        except IndexError:\n            pass\n        n+=1\n    cols_selected=[]   \n    \n    for x,y in selected:\n        cols_selected.append(x)\n    \n    if len(cols_selected)==feature_number:\n        return  cols_selected\n    else:\n        print(\"ERROR!. Cols_Selected does not meet feature_number requirements\")\n\nfeature_selector(data,8,\"Class\")","2f143e1b":"def VIF(dataframe,chosen_cols):\n    from statsmodels.stats.outliers_influence import variance_inflation_factor as VIF\n    from statsmodels.tools.tools import add_constant\n    X=dataframe[chosen_cols]\n    X=add_constant(X)\n    vif_data=pd.DataFrame()\n    vif_data[\"feature\"]=X.columns\n    vif_data[\"VIF\"]=[VIF(X.values, i) for i in range(len(X.columns))]\n    return vif_data\nchosen_cols=['Area',\n 'MajorAxisLength',\n 'MinorAxisLength',\n 'ConvexArea',\n 'EquivDiameter',\n 'Perimeter',\n 'Roundness',\n 'AspectRation']\nVIF(data,chosen_cols)","f016b7fa":"new_chosen_cols=[\"MajorAxisLength\",\"Roundness\",\"Eccentricity\",\"Extent\"]\nVIF(data,new_chosen_cols)","ef1d953d":"from sklearn.model_selection import cross_val_score\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LogisticRegression\nlogit_model= linear_model.LogisticRegression(max_iter=1000)\nX=data[[\"MajorAxisLength\",\"Roundness\",\"Eccentricity\",\"Extent\"]]\nY=data[\"Class\"]\nlogit_model.fit(X,Y)\nprint(f\"R2 is {logit_model.score(X,Y)}\")\nscores= cross_val_score(linear_model.LogisticRegression(max_iter=1000),X,Y,scoring=\"accuracy\",cv=10)\nprint(f\"\\nThe score mean with cross validation is {scores.mean()*100}%\")","46f426aa":"\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import linear_model\nimport pandas as pd\nfrom sklearn import metrics\nimport numpy as np\nX=data[[\"MajorAxisLength\",\"Roundness\",\"Eccentricity\",\"Extent\"]]\nY=data[\"Class\"]\nX_train,X_test,Y_train,Y_test= train_test_split(X,Y,test_size=0.2,random_state=0)\nlm=linear_model.LogisticRegression(max_iter=1000)\nlm.fit(X_train,Y_train)\npredict=lm.predict(X_test)\nprobs=lm.predict_proba(X_test)\nprob=probs[:,1]\nprob_df=pd.DataFrame(prob)\n    \n\ntreshold = 0.25\n\n    \nprint(f\"Prediction Accuracy: {metrics.accuracy_score(Y_test,predict)}\")\nacc_lr=metrics.accuracy_score(Y_test,predict)\nprint()\nprob_df[\"prediction\"]= np.where(prob_df[0]> treshold,1,0)\nprint(prob_df)\nprint(f\"\\nTreshold: {treshold}\")\nprint()\ncon_tab=pd.crosstab(prob_df[\"prediction\"],columns=\"Count\")\nprint(f\"Number of Positive Cases: {con_tab.values[1]\/len(prob_df)*100}%\")\ncon_tab\n    ","08aec35a":"from ggplot import *\nfrom sklearn import metrics\n  \n    \nespecifities,sensibilities,_=metrics.roc_curve(Y_test,prob)\n    \ndf=pd.DataFrame({\n    \"x\":especifities,\n    \"y\":sensibilities\n})\n    \nauc=metrics.auc(especifities,sensibilities)\n    \nprint(f\"The AUC is: {auc}\")\n    \nggplot(df,aes(x=\"x\",y=\"y\"))+geom_line()+geom_abline(linetype=\"dashed\")+xlim(-0.01,1.01)+ylim(-0.01,1.01)    \n    \n","84a8a1f2":"def sklearn_decision_tree_clasiffier(dataframe,chosen_cols,target_variable,max_depth):\n    from sklearn.tree import DecisionTreeClassifier\n    import numpy as np\n    import pandas as pd\n    from sklearn.model_selection import cross_val_score\n    from sklearn.model_selection import KFold\n    global predictor,X,Y,acc_dtc\n    predictor=chosen_cols\n    target= target_variable\n    dataframe[\"is_train\"] = (np.random.uniform(0,1,len(dataframe)) <= 0.75).astype(int)\n    train,test=dataframe[dataframe[\"is_train\"]==True],dataframe[dataframe[\"is_train\"]==False]\n    tree=DecisionTreeClassifier(criterion=\"entropy\",min_samples_split=int((len(dataframe)\/16)))\n    tree.fit(train[predictor],train[target])\n    preds=tree.predict(test[predictor])\n    print(f\"The R2 is {tree.score(train[predictor],train[target])}\")\n    print()\n    X= dataframe[predictor]\n    Y=dataframe[target]\n    tree.fit(X,Y)\n    cv= KFold(n_splits=100,shuffle=True)\n    score=np.mean(cross_val_score(tree,X,Y,scoring=\"accuracy\",cv=cv,n_jobs=1))\n    print(f\"The score for Cross Validation is : {score}\")\n    print(\"-----------------------------------------------------------------\")\n    \n    \n    return pd.crosstab(test[target],preds,rownames=[\"Actual\"],colnames=[\"Predictions\"])\n    \n    \n    \n    \n    \nchosen_cols=[\"MajorAxisLength\",\"Roundness\",\"Eccentricity\",\"Extent\"]\nsklearn_decision_tree_clasiffier(data,chosen_cols,\"Class\",5)","32784d42":"from sklearn.tree import DecisionTreeClassifier\nfrom ggplot import *\nX=data[[\"MajorAxisLength\",\"Roundness\",\"Eccentricity\",\"Extent\"]]\nY=data[\"Class\"]\nX_train,X_test,Y_train,Y_test= train_test_split(X,Y,test_size=0.2,random_state=0)\ntree=DecisionTreeClassifier(criterion=\"entropy\",min_samples_split=int((len(data)\/16)))\n\ntree.fit(X_train,Y_train)\npredict=tree.predict(X_test)\nprobs=tree.predict_proba(X_test)\nprob=probs[:,1]\nprob_df=pd.DataFrame(prob)\n    \n\ntreshold = 0.25\nfrom sklearn import metrics\n  \n    \nespecifities,sensibilities,_=metrics.roc_curve(Y_test,prob)\n    \ndf=pd.DataFrame({\n    \"x\":especifities,\n    \"y\":sensibilities\n})\n    \nauc=metrics.auc(especifities,sensibilities)\n    \nprint(f\"The AUC is: {auc}\")\n    \nggplot(df,aes(x=\"x\",y=\"y\"))+geom_line()+geom_abline(linetype=\"dashed\")+xlim(-0.01,1.01)+ylim(-0.01,1.01)    \n    \n","8d0cf191":"# Data Preprocessing","57c4d9af":"**We got an R2 of 98% and a 97% of cross val, wich is pretty good. But now we will divide the dataset in train and test**","73e386ff":"**We got a prediction accuracy of 98%**","069143be":"**Since the Dataset is pretty well we will continue with the EDA**","95118c77":"# Logistic Regression","7a52a4df":"**Now we will remove those outliers we said about**","ec93ea59":"**We can see that ,MinorAxisLength,AspectRation,Roundness,Area,ConvexArea,EquivDiameter and Eccentricity have a very high correlation with the target variable**","79e155cb":"# EDA","e4a6ebaa":"**We got a 96% of acc. Not as good as the logistic**","1c1457df":"**We will use a func to select the best variables for the model**","bcd7d75c":"# Rice Clasiffication With Logistic and DT Regression","ef1b4818":"**Now we will see its ROC curve**","4b58bce4":"****We can see  outliers in some features, mostly is because the features have many values so we will take care of the farest outliers later****","ee9c3689":"**We can see that the distribution in most variables tends to gaussian**","fd7add59":"**In this vif func, values above 10 must leave the model, here we have super high values. So we will do some cleaning**","ead741c0":"#  Now we will use the scond model, Decision Tree Classifier","18928489":"**We will check the vif of the features**","88020fdb":"#  Thanks for watching. This is barely my second prediction kaggle and my first EDA. So i would love some feedback!.","3588da46":"**We see there is 54.9% of the data on class 1 and the class 0 45.1%. The data is balanced too.**","48d761cc":"**Now let see the ROC curve**","df9a64de":"**We can see that with most of the features there is a negative relation with class 1**","635f53f8":"**We cleaned all the features with the highest VIF and added some whom werent chose by the feature_selector**","a5f2d01e":"# ***Some Data Cleaning***"}}