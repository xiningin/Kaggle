{"cell_type":{"8408e3ac":"code","ac410d1b":"code","b44b25f7":"code","7a710583":"code","780711ad":"code","a22803f9":"code","250fc90b":"code","d06c7fbe":"code","86bde660":"code","5a3882b6":"code","f5e8d68f":"code","bb1950e3":"code","62c50c3b":"code","fa8c1dd2":"code","6865597d":"markdown","1d497d99":"markdown","d10fbf8e":"markdown","83b1bf19":"markdown","c59fbb60":"markdown","0cb900d1":"markdown","e280f698":"markdown","caee928f":"markdown","77732e1c":"markdown","4336ed8a":"markdown","5d140e2f":"markdown"},"source":{"8408e3ac":"!rm -r \/opt\/conda\/lib\/python3.6\/site-packages\/lightgbm\n!git clone --recursive https:\/\/github.com\/Microsoft\/LightGBM","ac410d1b":"!apt-get install -y -qq libboost-all-dev","b44b25f7":"%%bash\ncd LightGBM\nrm -r build\nmkdir build\ncd build\ncmake -DUSE_GPU=1 -DOpenCL_LIBRARY=\/usr\/local\/cuda\/lib64\/libOpenCL.so -DOpenCL_INCLUDE_DIR=\/usr\/local\/cuda\/include\/ ..\nmake -j$(nproc)","7a710583":"!cd LightGBM\/python-package\/;python3 setup.py install --precompile","780711ad":"!mkdir -p \/etc\/OpenCL\/vendors && echo \"libnvidia-opencl.so.1\" > \/etc\/OpenCL\/vendors\/nvidia.icd\n!rm -r LightGBM","a22803f9":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nimport lightgbm as lgb\nfrom sklearn import metrics\nimport gc\n\npd.set_option('display.max_columns', 200)","250fc90b":"train_df = pd.read_csv('..\/input\/train.csv')\ntest_df = pd.read_csv('..\/input\/test.csv')\n\n#extracting a subset for quick testing\n#train_df = train_df[1:1000]","d06c7fbe":"param = {\n        'num_leaves': 10,\n        'max_bin': 127,\n        'min_data_in_leaf': 11,\n        'learning_rate': 0.02,\n        'min_sum_hessian_in_leaf': 0.00245,\n        'bagging_fraction': 1.0, \n        'bagging_freq': 5, \n        'feature_fraction': 0.05,\n        'lambda_l1': 4.972,\n        'lambda_l2': 2.276,\n        'min_gain_to_split': 0.65,\n        'max_depth': 14,\n        'save_binary': True,\n        'seed': 1337,\n        'feature_fraction_seed': 1337,\n        'bagging_seed': 1337,\n        'drop_seed': 1337,\n        'data_random_seed': 1337,\n        'objective': 'binary',\n        'boosting_type': 'gbdt',\n        'verbose': 1,\n        'metric': 'auc',\n        'is_unbalance': True,\n        'boost_from_average': False,\n    }","86bde660":"%%time\nnfold = 2\n\ntarget = 'target'\npredictors = train_df.columns.values.tolist()[2:]\n\nskf = StratifiedKFold(n_splits=nfold, shuffle=True, random_state=2019)\n\noof = np.zeros(len(train_df))\npredictions = np.zeros(len(test_df))\n\ni = 1\nfor train_index, valid_index in skf.split(train_df, train_df.target.values):\n    print(\"\\nfold {}\".format(i))\n    xg_train = lgb.Dataset(train_df.iloc[train_index][predictors].values,\n                           label=train_df.iloc[train_index][target].values,\n                           feature_name=predictors,\n                           free_raw_data = False\n                           )\n    xg_valid = lgb.Dataset(train_df.iloc[valid_index][predictors].values,\n                           label=train_df.iloc[valid_index][target].values,\n                           feature_name=predictors,\n                           free_raw_data = False\n                           )   \n\n    \n    clf = lgb.train(param, xg_train, 5000, valid_sets = [xg_valid], verbose_eval=50, early_stopping_rounds = 50)\n    oof[valid_index] = clf.predict(train_df.iloc[valid_index][predictors].values, num_iteration=clf.best_iteration) \n    \n    predictions += clf.predict(test_df[predictors], num_iteration=clf.best_iteration) \/ nfold\n    i = i + 1\n\nprint(\"\\n\\nCV AUC: {:<0.2f}\".format(metrics.roc_auc_score(train_df.target.values, oof)))","5a3882b6":"!nvidia-smi","f5e8d68f":"param = {\n        'num_leaves': 10,\n        'max_bin': 127,\n        'min_data_in_leaf': 11,\n        'learning_rate': 0.02,\n        'min_sum_hessian_in_leaf': 0.00245,\n        'bagging_fraction': 1.0, \n        'bagging_freq': 5, \n        'feature_fraction': 0.05,\n        'lambda_l1': 4.972,\n        'lambda_l2': 2.276,\n        'min_gain_to_split': 0.65,\n        'max_depth': 14,\n        'save_binary': True,\n        'seed': 1337,\n        'feature_fraction_seed': 1337,\n        'bagging_seed': 1337,\n        'drop_seed': 1337,\n        'data_random_seed': 1337,\n        'objective': 'binary',\n        'boosting_type': 'gbdt',\n        'verbose': 1,\n        'metric': 'auc',\n        'is_unbalance': True,\n        'boost_from_average': False,\n        'device': 'gpu',\n        'gpu_platform_id': 0,\n        'gpu_device_id': 0\n    }","bb1950e3":"%%time\nnfold = 2\n\ntarget = 'target'\npredictors = train_df.columns.values.tolist()[2:]\n\nskf = StratifiedKFold(n_splits=nfold, shuffle=True, random_state=2019)\n\noof = np.zeros(len(train_df))\npredictions = np.zeros(len(test_df))\n\ni = 1\nfor train_index, valid_index in skf.split(train_df, train_df.target.values):\n    print(\"\\nfold {}\".format(i))\n    xg_train = lgb.Dataset(train_df.iloc[train_index][predictors].values,\n                           label=train_df.iloc[train_index][target].values,\n                           feature_name=predictors,\n                           free_raw_data = False\n                           )\n    xg_valid = lgb.Dataset(train_df.iloc[valid_index][predictors].values,\n                           label=train_df.iloc[valid_index][target].values,\n                           feature_name=predictors,\n                           free_raw_data = False\n                           )   \n\n    \n    clf = lgb.train(param, xg_train, 5000, valid_sets = [xg_valid], verbose_eval=50, early_stopping_rounds = 50)\n    oof[valid_index] = clf.predict(train_df.iloc[valid_index][predictors].values, num_iteration=clf.best_iteration) \n    \n    predictions += clf.predict(test_df[predictors], num_iteration=clf.best_iteration) \/ nfold\n    i = i + 1\n\nprint(\"\\n\\nCV AUC: {:<0.2f}\".format(metrics.roc_auc_score(train_df.target.values, oof)))","62c50c3b":"sub_df = pd.DataFrame({\"ID_code\": test_df.ID_code.values})\nsub_df[\"target\"] = predictions\nsub_df[:10]","fa8c1dd2":"sub_df.to_csv(\"lightgbm_gpu.csv\", index=False)","6865597d":"# GPU-accelerated LightGBM\n\nThis kernel explores a GPU-accelerated LGBM model to predict customer transaction.\n\n## Notebook  Content\n1. [Re-compile LGBM with GPU support](#1)\n1. [Loading the data](#2)\n1. [Training the model on CPU](#3)\n1. [Training the model on GPU](#4)\n1. [Submission](#5)","1d497d99":"<a id=\"4\"><\/a>\n## 4. Train model on GPU","d10fbf8e":"<a id=\"3\"><\/a>\n## 3. Training the model on CPU","83b1bf19":"<a id=\"2\"><\/a> \n## 2. Loading the data","c59fbb60":"In order to leverage the GPU, we need to set the following parameters: \n\n        'device': 'gpu',\n        'gpu_platform_id': 0,\n        'gpu_device_id': 0\n        \n        ","0cb900d1":"<a id=\"1\"><\/a> \n## 1. Re-compile LGBM with GPU support\nIn Kaggle notebook setting, set the `Internet` option to `Internet connected`, and `GPU` to `GPU on`. \n\nWe first remove the existing CPU-only lightGBM library and clone the latest github repo.","e280f698":"Last, carry out some post processing tricks for OpenCL to work properly, and clean up.","caee928f":"Next, the Boost development library must be installed.","77732e1c":"First, check the GPU availability.","4336ed8a":"The next step is to build and re-install lightGBM with GPU support.","5d140e2f":"<a id=\"5\"><\/a>\n## 5. Submission"}}