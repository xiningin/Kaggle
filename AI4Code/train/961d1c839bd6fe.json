{"cell_type":{"84528424":"code","bd709841":"code","d32223a6":"code","b54ad841":"code","7aa5ea24":"code","c53588a2":"code","2a89e928":"code","57a89e69":"code","898bda9a":"code","6d66f848":"code","3dd41fd0":"code","76aafeff":"code","db810439":"code","a7bd3037":"code","142d5ea5":"code","09542a38":"code","bc11fb4d":"code","e016119c":"code","ed30fae8":"code","41c4d179":"code","241051f3":"code","b8f76860":"code","5f130df2":"code","e4af3c51":"code","39c98426":"code","195f62d1":"code","572f8581":"code","66d52eab":"code","453eccf3":"markdown","c55d7705":"markdown","f24af468":"markdown"},"source":{"84528424":"!pip install ..\/input\/python-datatable\/datatable-0.11.0-cp37-cp37m-manylinux2010_x86_64.whl > \/dev\/null 2>&1","bd709841":"import numpy as np\nimport pandas as pd\nfrom collections import defaultdict\nimport datatable as dt\nimport lightgbm as lgb\nfrom matplotlib import pyplot as plt\nimport riiideducation\nimport torch\n\n\n# Error handling, ignore all\nnp.seterr(divide = 'ignore', invalid = 'ignore')","d32223a6":"data_types_dict = {\n    'user_id': 'int32', \n    'timestamp': 'int64',\n    'content_id': 'int16', \n    'answered_correctly': 'int8', \n    'prior_question_elapsed_time': 'float32', \n    'prior_question_had_explanation': 'bool'\n}\ntarget = 'answered_correctly'","b54ad841":"train_df = dt.fread('..\/input\/riiid-test-answer-prediction\/train.csv', columns = set(data_types_dict.keys())).to_pandas()\n","7aa5ea24":"#train_df = train_df.groupby('user_id').tail(24).reset_index(drop = True)","c53588a2":"print('Training dataset detailed information')\nprint('*' * 50)\nprint('Columns:', train_df.columns)\nprint('*' * 50)\nprint('Shape:', train_df.shape)\nprint('*' * 50)\nprint('NA values in each column:', sum(train_df.isna().sum()))\nprint('*' * 50)","2a89e928":"train_df = train_df[train_df[target] != -1].reset_index(drop = True, inplace = False)#\u83b7\u53d6target\u975e-1\u7684\u6837\u672c\n\ntrain_df['prior_question_had_explanation'].fillna(False, inplace = True)#\u7528False\u586b\u5145nan\n\ntrain_df = train_df.astype(data_types_dict)\n","57a89e69":"prior_question_elapsed_time_mean = train_df.prior_question_elapsed_time.dropna().values.mean()\ntrain_df['prior_question_elapsed_time_mean'] = train_df.prior_question_elapsed_time.fillna(prior_question_elapsed_time_mean)","898bda9a":"train_df.shape","6d66f848":"train_df['lag'] = train_df.groupby('user_id')[target].shift()\n\ncum = train_df.groupby('user_id')['lag'].agg(['cumsum', 'cumcount']) # \u5217\u65b9\u5411\u4e0a\u6c42\u7d2f\u79ef\u548c \u548c\u7d2f\u8ba1\u4e2a\u6570\n#  \u5b66\u4e60\u8fdb\u6b65\u7684\u589e\u957f\u7387\ntrain_df['user_correctness'] = cum['cumsum'] \/ cum['cumcount']\n# \ntrain_df.drop(columns = ['lag'], inplace = True)\n\n","3dd41fd0":"# Overall correctness of users \u7528\u6237\u56de\u7b54\u95ee\u9898\u6b63\u786e\u7684\u6bd4\u4f8b\uff0c\u6570\u76ee\u548c\u6b21\u6570 sum\u662f\u56de\u7b54\u6b63\u786e\u7684\u6b21\u6570\uff0ccount\u662f\u56de\u7b54\u7684xx\u9898\u76ee\u7684\u603b\u6b21\u6570\nuser_agg = train_df.groupby('user_id')[target].agg(['sum', 'count'])\n# Overall difficulty of questions\u6bcf\u4e2acontent\u51fa\u73b0\u7684\u6b21\u6570\u548c\u88ab\u56de\u7b54\u6b63\u786e\u7684\u6bd4\u4f8b\ncontent_agg = train_df.groupby('content_id')[target].agg(['sum', 'count'])","76aafeff":"#train_df['Accuracy'] = train_df['user_id'].map(user_agg['sum']\/user_agg['count'])#\u6bcf\u4e2a\u7528\u6237\u56de\u7b54\u95ee\u9898\u7684\u51c6\u786e\u7387\ntrain_df['Accuracy_sum'] = train_df['user_id'].map(user_agg['sum'])#\u6bcf\u4e2a\u7528\u6237\u56de\u7b54\u95ee\u9898\u5bf9\u7684\u603b\u6570\ntrain_df['Questions_num'] = train_df['user_id'].map(user_agg['count'])#\u6bcf\u4e2a\u7528\u6237\u56de\u7b54\u95ee\u9898\u7684\u603b\u6570","db810439":"# Take only 24 last observations of each user\ntrain_df = train_df.groupby('user_id').tail(500).reset_index(drop = True)","a7bd3037":"#train = train_df.groupby('user_id').tail(24).reset_index(drop = True)\n#valid_df = valid_df.groupby('user_id').tail(24).reset_index(drop = True)","142d5ea5":"questions_df = pd.read_csv(\n    '..\/input\/riiid-test-answer-prediction\/questions.csv', \n    usecols = [0, 3],\n    dtype = {'question_id': 'int16', 'part': 'int8'}\n)\ntrain_df = pd.merge(train_df, questions_df, left_on = 'content_id', right_on = 'question_id', how = 'left')\n\ntrain_df.drop(columns = ['question_id'], inplace = True)\n","09542a38":"train_df['content_count'] = train_df['content_id'].map(content_agg['count']).astype('int32')#\u67d0\u8bb2\u5ea7\u88ab\u56de\u7b54\u7684\u6b21\u6570\n\ntrain_df['content_id'] = train_df['content_id'].map(content_agg['sum'] \/ content_agg['count'])#\u67d0\u8bb2\u5ea7\u88ab\u56de\u7b54\u6b63\u786e\u7684\u6bd4\u4f8b","bc11fb4d":"train_df.prior_question_had_explanation=train_df.prior_question_had_explanation.astype('int8')\n\ntrain_df['lag'] = train_df.groupby('user_id')['prior_question_had_explanation'].shift()#\u7528\u6237\u662f\u5426\n","e016119c":"#train_df['lag'] = train_df.groupby('user_id')['prior_question_had_explanation'].shift()#\u7528\u6237\u662f\u5426\u770b\u5230\u4e0a\u4e00\u4e2a\u95ee\u9898\u7684\u7b54\u6848\uff0c\u7b2c\u4e00\u4e2a\u9898\u76ee\u4e3anull\u3002\u901a\u5e38\u524d\u51e0\u4e2a\u90fd\u4e3afalse\uff0c\u56e0\u4e3a\u90a3\u662f\u6d4b\u8bd5\u3002\ncum = train_df.groupby('user_id')['lag'].agg(['cumsum', 'cumcount'])#\u770b\u4e0a\u4e00\u9898\u89e3\u91ca\u7684\u603b\u6570\u548c\u5217\u6570\ntrain_df['explanation_mean'] = cum['cumsum'] \/ cum['cumcount']#\u89e3\u91ca\u7684\u5e73\u5747\ntrain_df['explanation_cumsum'] = cum['cumsum'] \n\ntrain_df.drop(columns=['lag'], inplace=True)\n\ntrain_df['explanation_mean'].fillna(0, inplace=True)\ntrain_df['explanation_cumsum'].fillna(0, inplace=True)\ntrain_df.explanation_mean=train_df.explanation_mean.astype('float16')\ntrain_df.explanation_cumsum=train_df.explanation_cumsum.astype('int16')\n\n","ed30fae8":"explanation_agg = train_df.groupby('user_id')['prior_question_had_explanation'].agg(['sum', 'count'])#\u4e0e\u4e0a\u9762cusum\u548ccucount\u7684\u533a\u522b\nexplanation_agg = explanation_agg.astype('int16')\n","41c4d179":"max_timestamp_u = train_df[['user_id','timestamp']].groupby(['user_id']).agg(['max']).reset_index()#\u53d6\u51fatimestamp\u7684\u6700\u5927\u503c\nmax_timestamp_u.columns = ['user_id', 'max_time_stamp']#\u91cd\u65b0\u8bbe\u7f6ecolumns\n\ntrain_df['lagtime'] = train_df.groupby('user_id')['timestamp'].shift()\ntrain_df['lagtime']=train_df['timestamp']-train_df['lagtime']#\u6b64\u7528\u6237\u4ea4\u4e92\u4e0e\u8be5\u7528\u6237\u5b8c\u6210\u7b2c\u4e00\u4e2a\u4e8b\u4ef6\u4e4b\u95f4\u7684\u65f6\u95f4\uff08\u6beb\u79d2\uff09\u3002\ntrain_df['lagtime'].fillna(0, inplace=True)#\u75280\u586b\u5145\u7a7a\u503c\ntrain_df.lagtime=train_df.lagtime.astype('int32')#\u6570\u636e\u683c\u5f0f\u8f6c\u6362\n\nlagtime_agg = train_df.groupby('user_id')['lagtime'].agg(['mean'])#\u5b8c\u6210\u6bcf\u4e00\u9898\u7684\u5e73\u5747\u65f6\u95f4\ntrain_df['lagtime_mean'] = train_df['user_id'].map(lagtime_agg['mean'])#map\u6620\u5c04\ntrain_df.lagtime_mean=train_df.lagtime_mean.astype('int32')#\u8f6c\u6362\u6570\u636e\u683c\u5f0f\n\n\ntrain_df['timestamp']=train_df['timestamp']\/(1000*3600)#\u65f6\u95f4\u8f6c\u6362\u4e3a\u5c0f\u65f6\ntrain_df.timestamp=train_df.timestamp.astype('int16')\n","241051f3":"# Ratio is 6\/24 = 25%\nvalid_df = train_df.groupby('user_id').tail(125)\ntrain_df.drop(valid_df.index, inplace = True)","b8f76860":"train_df.shape,valid_df.shape","5f130df2":"\nfeatures = [ 'timestamp','lagtime','lagtime_mean','Accuracy_sum','Questions_num','prior_question_elapsed_time', \n            'prior_question_had_explanation', 'user_correctness', 'prior_question_elapsed_time_mean',\n            'part', 'content_count','content_id','explanation_mean','explanation_cumsum']\n'''\nfeatures = [ 'timestamp','lagtime','lagtime_mean',\n    'content_id', 'prior_question_elapsed_time', \n            'prior_question_had_explanation', 'user_correctness', \n            'part', 'content_count']\n'''\n\n\nparams = {\n    'loss_function': 'Logloss',\n    'eval_metric': 'AUC',\n    'task_type': 'GPU' if torch.cuda.is_available() else 'CPU',\n    'grow_policy': 'Lossguide',\n    'iterations': 5000,\n    'learning_rate': 4e-2,\n    'random_seed': 0,\n    'l2_leaf_reg': 1e-1,\n    'depth': 15,\n    'max_leaves': 10,\n    'border_count': 128,\n    'verbose': 50,\n}","e4af3c51":"from catboost import CatBoostClassifier, Pool\n\n# Training and validating data\ntrain_set = Pool(train_df[features], label = train_df[target])\nval_set = Pool(valid_df[features], label = valid_df[target])","39c98426":"# Model definition\nmodel = CatBoostClassifier(**params)\n\n# Fitting\nmodel.fit(train_set, eval_set = val_set, use_best_model = True)","195f62d1":"user_sum_dict = user_agg['sum'].astype('int16').to_dict(defaultdict(int))\nuser_count_dict = user_agg['count'].astype('int16').to_dict(defaultdict(int))\ncontent_sum_dict = content_agg['sum'].astype('int32').to_dict(defaultdict(int))\ncontent_count_dict = content_agg['count'].astype('int32').to_dict(defaultdict(int))\n\nexplanation_sum_dict = explanation_agg['sum'].astype('int16').to_dict(defaultdict(int))\nexplanation_count_dict = explanation_agg['count'].astype('int16').to_dict(defaultdict(int))\n\n\nlagtime_mean_dict = lagtime_agg['mean'].astype('int32').to_dict(defaultdict(int))\nmax_timestamp_u_dict = max_timestamp_u.set_index('user_id').to_dict()","572f8581":"try:\n    env = riiideducation.make_env()\nexcept:\n    pass\niter_test = env.iter_test()\nprior_test_df = None","66d52eab":"for (test_df, sample_prediction_df) in iter_test:\n    if prior_test_df is not None:\n        prior_test_df[target] = eval(test_df['prior_group_answers_correct'].iloc[0])\n        prior_test_df = prior_test_df[prior_test_df[target] != -1].reset_index(drop = True)\n        \n        user_ids = prior_test_df['user_id'].values\n        content_ids = prior_test_df['content_id'].values\n        targets = prior_test_df[target].values\n        \n        for user_id, content_id, answered_correctly in zip(user_ids, content_ids, targets):\n            user_sum_dict[user_id] += answered_correctly\n            user_count_dict[user_id] += 1\n            content_sum_dict[content_id] += answered_correctly\n            content_count_dict[content_id] += 1\n\n    prior_test_df = test_df.copy()\n    \n    test_df = test_df[test_df['content_type_id'] == 0].reset_index(drop = True)#\u6d4b\u8bd5\u6570\u636e\n    test_df = pd.merge(test_df, questions_df, left_on = 'content_id', right_on = 'question_id', how = 'left')\n    test_df['prior_question_had_explanation'] = test_df['prior_question_had_explanation'].fillna(False).astype('bool')  \n    #prior_question_elapsed_time_mean = test_df.prior_question_elapsed_time.dropna().values.mean()\n    test_df['prior_question_elapsed_time_mean'] = test_df.prior_question_elapsed_time.fillna(prior_question_elapsed_time_mean)\n    \n    user_sum = np.zeros(len(test_df), dtype = np.int16)\n    user_count = np.zeros(len(test_df), dtype = np.int16)\n    content_sum = np.zeros(len(test_df), dtype = np.int32)\n    content_count = np.zeros(len(test_df), dtype = np.int32)\n    explanation_sum = np.zeros(len(test_df), dtype=np.int32)\n    explanation_count = np.zeros(len(test_df), dtype=np.int32)\n    \n    lagtime = np.zeros(len(test_df), dtype=np.int32)\n    lagtime_mean = np.zeros(len(test_df), dtype=np.int32)\n    \n    for i, (user_id, content_id,timestamp) in enumerate(zip(test_df['user_id'].values, test_df['content_id'].values,test_df['timestamp'].values)):\n        user_sum[i] = user_sum_dict[user_id]\n        user_count[i] = user_count_dict[user_id]\n        content_sum[i] = content_sum_dict[content_id]\n        content_count[i] = content_count_dict[content_id]\n        explanation_sum[i] = explanation_sum_dict[user_id]\n        explanation_count[i] = explanation_count_dict[user_id]\n        \n        if user_id in max_timestamp_u_dict['max_time_stamp'].keys():\n            lagtime[i] = timestamp-max_timestamp_u_dict['max_time_stamp'][user_id]\n            max_timestamp_u_dict['max_time_stamp'][user_id]=timestamp\n            lagtime_mean[i] = (lagtime_mean_dict[user_id]+lagtime[i])\/2           \n        else:\n            lagtime[i]=0\n            max_timestamp_u_dict['max_time_stamp'].update({user_id:timestamp})\n            lagtime_mean_dict.update({user_id:timestamp})\n            lagtime_mean[i]=(lagtime_mean_dict[user_id]+lagtime[i])\/2\n\n    #test_df['Accuracy'] = user_sum \/ user_count#\u6bcf\u4e2a\u7528\u6237\u56de\u7b54\u95ee\u9898\u7684\u51c6\u786e\u7387\n    test_df['user_correctness'] = user_sum \/ user_count\n    test_df['Accuracy_sum'] = user_sum\n    test_df['Questions_num'] = user_count\n    test_df['content_count'] = content_count\n    test_df['content_id'] = content_sum \/ content_count\n    test_df['explanation_mean'] = explanation_sum \/ explanation_count\n    test_df['explanation_cumsum'] = explanation_sum \n    test_df[\"lagtime\"] = lagtime\n    test_df[\"lagtime_mean\"] = lagtime_mean\n    test_df['timestamp']=test_df['timestamp']\/(1000*3600)#\u65f6\u95f4\u8f6c\u6362\u4e3a\u5c0f\u65f6\n    test_df.timestamp=test_df.timestamp.astype('int16')\n        \n\n       \n    test_df[target] = model.predict_proba(test_df[features])[:,1]\n    env.predict(test_df[['row_id', target]])","453eccf3":"\u63d0\u53d6\u9a8c\u8bc1\u96c6","c55d7705":"\u63a5\u53e3\uff0c\u8fd9\u91cc\u662f\u76f8\u6bd4\u4e8e\u56fd\u5185\u7684\u6bd4\u8d5b\u6709\u5f88\u5927\u7684\u4e0d\u540c\u3002","f24af468":"\u8bad\u7ec3"}}