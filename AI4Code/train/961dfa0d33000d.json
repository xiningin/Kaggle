{"cell_type":{"76f7293e":"code","bdc6beb1":"code","c7f0fee7":"code","27240108":"code","dfc07283":"code","5236ddf6":"code","d276baf1":"code","3b30dacf":"code","8a973c5b":"code","7794d338":"code","bf6b6f6a":"code","0968076d":"code","1be05af1":"code","c1db17e6":"code","c42ca8bf":"code","a45114e7":"markdown","d6842192":"markdown","baf222b6":"markdown","34f1a03d":"markdown","62a2ea9c":"markdown","8eeb0a21":"markdown","3b27bf31":"markdown","6f0833a2":"markdown","ac6fc526":"markdown","39aa303a":"markdown","e2d8fdef":"markdown"},"source":{"76f7293e":"from bq_helper import BigQueryHelper\n\nbq_assistant = BigQueryHelper(\"bigquery-public-data\", \"github_repos\")","bdc6beb1":"def get_condition(values):\n    condition = [f'sample_path LIKE \"%.{value}\"' for value in values]\n    return \" OR \".join(condition)\n\n\ndef get_query(languages, limit=5000):\n    query_template = \"\"\"\n        (SELECT sample_path, content\n        FROM `bigquery-public-data.github_repos.sample_contents`\n        WHERE (binary = False AND ({query}))\n        LIMIT {limit})\n        \"\"\"\n    query_list = [query_template.format(query=get_condition(lang), limit=limit) for lang in languages.values()]\n    return \"\\nUNION ALL\\n\".join(query_list)","c7f0fee7":"import json\n\nwith open(\"..\/input\/lang-table\/languages.json\", 'r') as f:\n    lang_table = json.load(f)\n\nQUERY = get_query(lang_table, limit=5000)","27240108":"bq_assistant.estimate_query_size(QUERY)","dfc07283":"df = bq_assistant.query_to_pandas_safe(QUERY, max_gb_scanned=25)","5236ddf6":"print(f'Size of dataframe: {df.memory_usage(index=True, deep=True).sum() \/ (1024**3):.2f} GiB')","d276baf1":"# Table size\ndf.shape","3b30dacf":"df.head()","8a973c5b":"df.drop_duplicates(inplace=True)\ndf.dropna(inplace=True)\ndf[\"extension\"] = df[\"sample_path\"].apply(lambda x: x.split('.')[-1])\ndf[\"language\"] = df[\"extension\"].replace({ext: lang for lang, exts in lang_table.items() for ext in exts})\ndf.columns","7794d338":"# Deleting the columns we won't need for our prediction\ndf.drop(labels=['extension', 'sample_path'], axis=\"columns\", inplace=True)","bf6b6f6a":"df.columns","0968076d":"len(df[\"language\"].value_counts())","1be05af1":"import matplotlib.pyplot as plt\nplt.style.use(\"ggplot\")\n\ndf[\"language\"].value_counts().plot(kind='bar',figsize = (21,10), fontsize=20, rot=75)\nplt.title(\"Number of code snippets per language\", fontsize=20)","c1db17e6":"df.to_csv(\"data.csv\")","c42ca8bf":"from IPython.display import FileLink\nFileLink(r'data.csv')","a45114e7":"### Performing the Query\n\nBefore getting to the execution part, it's safer to estimate the query size since, on Kaggle, you only have a maximum of 5TB\/month for free. To do so, we simply call the `estimate_query_size` method which will return the size of the query in GB.","d6842192":"## DataSet Creation\nIn this NoteBook we will be creating our custom DataSet out of **Github Repos DataSet** [GitHub Repos](https:\/\/www.kaggle.com\/github\/github-repos). We'll be using Sohier's [BigQuery helper module](https:\/\/github.com\/SohierDane\/BigQuery_Helper\/blob\/master\/bq_helper.py) as suggested by this [kernel](https:\/\/www.kaggle.com\/mrisdal\/safely-analyzing-github-projects-popular-licenses).\n\nYou'll find below the following steps:\n\n1. Creating a `BigQueryHelper` object\n2. Creating a Search Query\n4. Performing the Query\n5. Exploring the DataSet and Visualizing results\n6. Exporting the DataSet to a CSV file","baf222b6":"### Exporting the DataSet to a CSV file\nTo export your data set under csv format, use one of these two methods:","34f1a03d":"### Exploring the DataSet and Visualizing results","62a2ea9c":"### Creating a BigQueryHelper object\nThe first step is to create a `BigQueryHelper` object. All you need is your dataset name ( here \"github_repos\" ) and your project name ( here \"bigquery-public-data\" ).","8eeb0a21":"To fetch the data, we can use `query_to_pandas_safe` method which returns a `DataFrame` if it doesn't exceed the given max size ( which is at least 23.75 GB here )","3b27bf31":"### Method 2:\nIf you choose to use this method, you will get directly a link to download you CSV file","6f0833a2":"Now it's time to decide which Data we need to include to our Dataset and in which proportion. To do so we're going to compose a new query using SQL.\n\n### Creating a Search Query\nThe pieces of information we're going to be needing for our model are the code snippets and the language. Those pieces correspond to the **content** and **sample_path** columns in the DataSet, or almost. In fact the **sample_path** column features the file name with the corresponding file extension (ex: code.*py* for python script file). To get the language out of the file extension, we built a correspondance table to link each language with its different possible file extensions and we stored it in the json file \"**languages.json**\". You can see an exemple below:\n```\n{\"Groovy\": [\"groovy\", \"gvy\", \"gy\", \"gsh\"]}\n```\nNow we are able to group file names corresponding to the same language. All we have left to do is to build the query:","ac6fc526":"### Method 1:\nThe first method is the regular pandas export method to_csv(). The file will then be stored in the \"output\/kaggle\/working\/\" repository. You can use the three vertical dots icon to download it locally. ","39aa303a":"# Now we're all set to get to the fun part : Our prediction model!","e2d8fdef":"The resulting Query looks something like this : \n\n\n\n        (SELECT sample_path, content\n        FROM `bigquery-public-data.github_repos.sample_contents`\n        WHERE (binary = False AND (sample_path LIKE \"%.bat\" OR sample_path LIKE \"%.cmd\" OR sample_path LIKE \"%.btm\"))\n        LIMIT 5000)\n        \n        UNION ALL\n        \n        (SELECT sample_path, content\n        FROM `bigquery-public-data.github_repos.sample_contents`\n        WHERE (binary = False AND (sample_path LIKE \"%.c\"))\n        LIMIT 5000)\n        \n        UNION ALL\n        \n        ...\n                     "}}