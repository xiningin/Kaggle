{"cell_type":{"eed707f5":"code","396e3168":"code","a539bde1":"code","0f00916a":"code","51a92bf8":"code","40b3323f":"code","ca05c4bc":"code","3ccd2e3d":"code","59dc7c1e":"code","d0d1f5a4":"code","adfab263":"code","512f265d":"code","6c660ec4":"code","f5312c63":"code","9a0014ca":"code","04aeb1b8":"code","961c3f1a":"code","eedde2be":"code","12fb2420":"code","12579298":"code","b0137916":"code","3e389b72":"code","e414fe03":"code","05952545":"code","cffc2d01":"code","1fd5a20c":"code","28ecd484":"code","6054842b":"code","a05aba47":"code","fa02f2b2":"code","68192793":"code","698d6ed5":"code","5224a853":"code","8621dc69":"code","176d5d1b":"code","14256e5e":"code","f6d303e4":"code","3935dfe9":"code","9e0b842e":"code","f42e2cb1":"code","3c52eab2":"code","66ec1345":"code","a77b4de7":"code","5eb83b8c":"code","cd183e55":"code","d644265a":"code","56f26a92":"code","f7eb799d":"code","6aa42f60":"code","e36855e1":"code","3c9be6fa":"code","8cf5b06a":"code","747751a3":"code","62193982":"code","1cd61843":"code","464b17d6":"code","eb1a8b6e":"code","aab2f7bf":"code","7345d150":"code","e8cc5127":"code","870b739e":"code","6781ab8d":"code","2670a3c9":"code","c276e35e":"code","cb3f7919":"code","e7fd185a":"code","3aca8303":"code","0dbf6182":"code","eb55d42e":"code","687e8375":"markdown","279a852a":"markdown","719316cb":"markdown","49445d7b":"markdown","28ee8423":"markdown","898c95c0":"markdown","857b7d00":"markdown","c0d95705":"markdown","907b5483":"markdown","fe172cb2":"markdown","42763bed":"markdown","1b4480c5":"markdown","bdbbb623":"markdown","0506dc35":"markdown","46e52bb1":"markdown","55f6ee1f":"markdown","b9e5addc":"markdown","056f2927":"markdown","e634b406":"markdown","6e6644f1":"markdown","8b709ef5":"markdown","f935e2ed":"markdown","3396af38":"markdown","9817e04c":"markdown","3578117a":"markdown","cab25b03":"markdown","0767ed50":"markdown","ab2cc108":"markdown","621e56b9":"markdown","7ab38a9c":"markdown","4abe52cd":"markdown","1b41e432":"markdown","8c743ac8":"markdown","1b2ba5fc":"markdown","b7ba8778":"markdown","0cb3cdd0":"markdown","9ac865a4":"markdown","2837ec79":"markdown","a11fc473":"markdown","00ef5c01":"markdown","e472d2d7":"markdown","8932d5e6":"markdown","782719fa":"markdown","6d758e09":"markdown","8a3a57b5":"markdown","da179f47":"markdown","e82dca17":"markdown","541c6fee":"markdown","e9a48f3b":"markdown","a5d301f0":"markdown","af6eea68":"markdown","da3f937f":"markdown","1655fc32":"markdown","bdd081c3":"markdown","334ef57a":"markdown","a80aaec4":"markdown","42ba780e":"markdown","4bba8063":"markdown","b3c8130d":"markdown"},"source":{"eed707f5":"# Visualisation\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport seaborn as sns\nfrom wordcloud import WordCloud\nimport mapclassify\nimport geoplot\nimport matplotlib.image as mpimg\n\n\n# File and matrices \nimport numpy as np\nimport pandas as pd\nimport geopandas as gpd\nfrom scipy.stats.stats import pearsonr\npd.set_option('display.max_rows', None) # Display all rows in a Jupyter Notebook cell when needed\npd.set_option('display.max_columns', None) # Display all columns \n\n\n# Statistics\nfrom scipy import stats\nfrom scipy.stats import skew, norm\nfrom scipy import stats\n\n\n# sklearn\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import make_column_selector as selector\nfrom sklearn.linear_model import Lasso\nfrom sklearn.model_selection import GridSearchCV\n\n\n# Warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n# Directory check\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","396e3168":"# Check total and percent of missing values for a dataframe's columns\ndef check_missing_values(df):\n    pct_missing = round(df.isnull().sum() * 100 \/ len(df), 1)\n    missing_df = pd.DataFrame({'column_name': df.columns,\n                               'total_missing': df.isnull().sum().values,\n                               'percent_missing (%)': pct_missing}).set_index('column_name')\n    missing_df.sort_values('total_missing', ascending=False, inplace=True)\n    print(missing_df.query('total_missing > 0'))\n    \n    \n# Get the median property price and number of properties sold by a feature\ndef agg_price_and_num_by_feature(df, feature):\n    df_temp = df[[feature, 'Price']].groupby(feature).agg({'Price': ['median', 'count']})\n    df_temp.columns = df_temp.columns.map('_'.join)\n    return df_temp.reset_index()\n\n\n# Groupby a feature and get the median, sum, and count of Price\ndef agg_price_sum_num_by_feature(df, feature):\n    df_temp = df[[feature, 'Price']].groupby(feature).agg({'Price': ['median', 'sum', 'count']})\n    df_temp.columns = df_temp.columns.map('_'.join)\n    return df_temp.reset_index()\n    \n\n# Plot two bar charts for median Price and total number of properties sold\ndef bar_plots(df, feature):\n    sns.set(font_scale=1.5)\n    plt.subplots(figsize=(15, 5))\n\n    plt.subplot(1, 2, 1)\n    g = sns.barplot(x=feature, y='Price_median', data=df)\n    plt.ylabel('Median property price (AUD)')\n\n    plt.subplot(1, 2, 2)\n    g = sns.barplot(x=feature, y='Price_count', data=df)\n    g.yaxis.set_major_formatter(mpl.ticker.StrMethodFormatter('{x:,.0f}'))\n    plt.ylabel('Total number of properties sold')\n\n    plt.tight_layout()\n    \n    \n# Plot two line charts for median price and total number of properties sold\ndef line_plots(df, feature):\n    sns.set(font_scale=1.5)\n    plt.subplots(figsize=(30, 10))\n\n    plt.subplot(1, 2, 1)\n    g = sns.lineplot(x=feature, y='Price_median', data=df)\n    plt.ylabel('Median property price (AUD)')\n    plt.xticks(rotation=90)\n\n    plt.subplot(1, 2, 2)\n    g = sns.lineplot(x=feature, y='Price_count', data=df)\n    g.yaxis.set_major_formatter(mpl.ticker.StrMethodFormatter('{x:,.0f}'))\n    plt.ylabel('Total number of properties sold')\n    plt.xticks(rotation=90)\n    \n    plt.tight_layout()\n    \n\n# Plot two scatter plots with linear regression fits and pearson correlation coefficient between Price and a feature\ndef reg_plots(df, feature):\n    plt.subplots(figsize=(12, 6))\n    g=sns.regplot(x=feature, y='Price', data=df, fit_reg=True, label = \"corr: %.3f\"%(pearsonr(df[feature], df['Price'])[0]))\n    g.xaxis.set_major_formatter(mpl.ticker.StrMethodFormatter('{x:,.0f}'))\n    plt.legend(loc=\"best\")\n    plt.ylabel('Price (AUD)')\n    plt.tight_layout()\n    plt.show()\n\n\n# Select numeric features that have a skewness larger than a threshold and apply log transform in place\ndef log_transform_features(df, skewness_threshold):\n    skewness_all = df.apply(lambda x: skew(x))\n    skewness_to_transform = skewness_all[abs(skewness_all) > skewness_threshold]\n    print(str(skewness_to_transform.shape[0]) + \" skewed numerical features to log transform as below:\")\n    print(skewness_to_transform)\n    skewed_features = skewness_to_transform.index\n    df.loc[:, skewed_features] = np.log1p(df[skewed_features])","a539bde1":"data_raw = pd.read_csv('..\/input\/melbourne-housing-market\/Melbourne_housing_FULL.csv')\nsuburb_layer = gpd.read_file('..\/input\/state-suburbs-asgs-ed-2016\/SSC_2016_AUST.shp') # Suburb spatial layer in ESRI Shapefile format in Australia, from https:\/\/www.abs.gov.au\/AUSSTATS\/abs@.nsf\/DetailsPage\/1270.0.55.003July%202016?OpenDocument","0f00916a":"data_raw.describe(include='all')","51a92bf8":"sns.set_style('darkgrid')\n\nplt.subplots(figsize=(20,10))\nsns.distplot(data_raw.Price, kde=True, color=\"b\")\nplt.show()","40b3323f":"# Select all numeric features and do a scatter plot for quick visual check\ndata_num_raw = data_raw.select_dtypes(include=np.number)","ca05c4bc":"g = sns.pairplot(data_num_raw, corner=True)\ng.fig.savefig(\"Numeric features pairplot before any data treatment.png\")","3ccd2e3d":"data_raw.loc[((data_raw.Landsize > 200000) | (data_raw.BuildingArea > 40000) | (data_raw.YearBuilt <1700)), ['Landsize', 'BuildingArea', 'YearBuilt']]","59dc7c1e":"data_raw = data_raw.loc[~((data_raw.Landsize > 200000) | (data_raw.BuildingArea > 40000) | (data_raw.YearBuilt <1700)), :]\ndata_raw.shape # After removing 3 outliers, we still have 34854 records in the dataset","d0d1f5a4":"# Select all categorical features and do a scatter plot for quick visual check\ndata_cat_raw = data_raw.select_dtypes(exclude=np.number)\ndata_cat_raw.describe(include='all')","adfab263":"print(\"Total number of unique date: {}\".format(data_raw.Date.nunique()))","512f265d":"data_raw.Date = pd.to_datetime(data_raw.Date, infer_datetime_format=True, errors='raise') # Invalid parsing will raise an exception\ndata_raw.Date.describe(datetime_is_numeric=True)","6c660ec4":"data_raw['YearSold'] = data_raw.Date.dt.year\ndata_raw['MonthSold'] = data_raw.Date.dt.month","f5312c63":"check_missing_values(data_raw)","9a0014ca":"# Check other features of the record missing Distance\ndata_raw.loc[data_raw.Distance.isnull(), :]","04aeb1b8":"# Drop this record\ndata_raw = data_raw.loc[~data_raw.Suburb.str.contains('Fawkner Lot'), :]\n\n# Check the number of properties in Fawkner\ndata_raw.loc[data_raw.Suburb=='Fawkner', :].shape","961c3f1a":"data_raw.loc[data_raw.Suburb=='Fawkner', :].sample(5)","eedde2be":"data_raw.drop(columns=['Postcode', 'Lattitude', 'Longtitude'], inplace=True)","12fb2420":"for feature in ['Propertycount', 'CouncilArea', 'Regionname']:\n    data_raw[feature] = data_raw.groupby('Suburb')[feature].transform(lambda x: x.fillna(x.bfill()))","12579298":"# Firstly, let's check the distribution of number of properties by Type\nplt.style.use('seaborn')\n\nax = data_raw.Type.map({'h': 'house', 'u': 'unit\/duplex', 't': 'townhouse'}).value_counts().plot(kind='bar', title='Number of properties in the current dataset by Type', figsize=(9, 4))\nax.yaxis.set_major_formatter(mpl.ticker.StrMethodFormatter('{x:,.0f}')) # Set the y axis tick values with thousands separator\nax.set_ylabel(\"Count\")\nplt.xticks(rotation=0)\nplt.show()","b0137916":"for feature in ['Bedroom2', 'Bathroom', 'Car', 'Landsize', 'YearBuilt', 'BuildingArea']:\n    data_raw[feature] = data_raw.groupby(['Rooms', 'Type', 'Suburb'])[feature].transform(lambda x: x.fillna(x.median()))\ncheck_missing_values(data_raw)    ","3e389b72":"data_raw.loc[(data_raw.Suburb=='Coburg North') & (data_raw.Type =='u'), :]","e414fe03":"for feature in ['Bedroom2', 'Bathroom', 'Car', 'Landsize', 'BuildingArea']:\n    data_raw[feature] = data_raw.groupby(['Rooms', 'Type'])[feature].transform(lambda x: x.fillna(x.median()))\ncheck_missing_values(data_raw)        ","05952545":"# Impute again using the median of corresponding values by Type\nfor feature in ['Bedroom2', 'Bathroom', 'Car', 'Landsize', 'BuildingArea']:\n    data_raw[feature] = data_raw.groupby(['Type'])[feature].transform(lambda x: x.fillna(x.median()))\ncheck_missing_values(data_raw) ","cffc2d01":"# Impute YearBuilt with Suburb\ndata_raw['YearBuilt'] = data_raw.groupby(['Suburb'])['YearBuilt'].transform(lambda x: x.fillna(x.median()))\ncheck_missing_values(data_raw)","1fd5a20c":"# Impute YearBuilt with CouncilArea\ndata_raw['YearBuilt'] = data_raw.groupby(['CouncilArea'])['YearBuilt'].transform(lambda x: x.fillna(x.median()))\ncheck_missing_values(data_raw)","28ecd484":"# Extract street name and number using regular expression\ndf_street = data_raw['Address'].str.extract('(?P<Number>\\d*\/*\\d*[a-zA-Z]*)(?P<StreetName>.*)', expand=True)\ndf_street.tail()","6054842b":"df_street.isnull().sum() # No NULL values after the splitting","a05aba47":"data_raw = data_raw.join(df_street)\ndata_raw.tail()","fa02f2b2":"data_raw['StreetSuburb'] = data_raw['StreetName'] + ', ' + data_raw['Suburb']\ndata_raw.tail()","68192793":"# Inspired by Arun Godwin Patel, https:\/\/www.kaggle.com\/agodwinp\/stacking-house-prices-walkthrough-to-top-5\nplt.subplots(figsize =(30, 8))\n\nplt.subplot(1, 3, 1)\nsns.boxplot(x=\"Regionname\", y=\"Price\", data=data_raw)\nplt.xticks(rotation=-90)\n\nplt.subplot(1, 3, 2)\nsns.stripplot(x=\"Regionname\", y=\"Price\", data=data_raw, size = 5, jitter = True)\nplt.xticks(rotation=-90)\n\nplt.subplot(1, 3, 3)\nsns.barplot(x=\"Regionname\", y=\"Price\", data=data_raw)\nplt.xticks(rotation=-90)","698d6ed5":"num_properties_by_council = data_raw['CouncilArea'].value_counts(normalize=True).rename_axis('CouncilArea').reset_index(name='Frequency')\nnum_properties_by_council.head()","5224a853":"council_area_dict = dict(zip(num_properties_by_council['CouncilArea'].tolist(), num_properties_by_council['Frequency'].tolist()))\nwc = WordCloud(width=800, height=400, max_words=200).generate_from_frequencies(council_area_dict)\nplt.figure(figsize=(30, 20))\nplt.imshow(wc, interpolation='bilinear')\nplt.axis('off')\nplt.show()","8621dc69":"# Select suburbs in Victoria only for mapping\nsuburb_vic = suburb_layer.query(\"STE_NAME16 == 'Victoria'\")\n\n# Noticing the bottom two records are not valid suburbs, which will be excluded when left joining with our data_raw\n# In addition, some suburb name in this spatial layer includes (Vic.) to differentiate with the suburb with the same name in another state. We need to exclude (Vic.)\nsuburb_vic.SSC_NAME16.replace(r'\\s*\\(.*\\)\\s*', '', regex=True, inplace=True)","176d5d1b":"# Join the polygon geometry with Suburb in our dataset for mapping\nsuburb_joined = pd.merge(data_raw, suburb_vic[['SSC_NAME16', 'geometry']], left_on='Suburb', right_on='SSC_NAME16', how='left').drop('SSC_NAME16', axis=1)","14256e5e":"# These 3 suburbs do not have a match with the standard suburb names, so we need to fix their suburb names before mapping and also for our modelling\nsuburb_joined.loc[suburb_joined.geometry.isnull(), :].Suburb.unique()","f6d303e4":"# Fix suburb names\ndata_raw.Suburb.replace({'MacLeod': 'Macleod', 'croydon': 'Croydon', 'viewbank': 'Viewbank'}, inplace=True)\n\n# Join and check NULL again\nsuburb_joined = pd.merge(data_raw, suburb_vic[['SSC_NAME16', 'geometry']], left_on='Suburb', right_on='SSC_NAME16', how='left').drop('SSC_NAME16', axis=1)\nsuburb_joined.loc[suburb_joined.geometry.isnull(), :].Suburb.unique()","3935dfe9":"# Get the median property price and number of properties sold by suburb\nsuburb_df = agg_price_and_num_by_feature(suburb_joined, 'Suburb')\n\n# Get each suburb's geometry object\nsuburb_joined_gdf = gpd.GeoDataFrame(pd.merge(suburb_df, suburb_vic[['SSC_NAME16', 'geometry']], left_on='Suburb', right_on='SSC_NAME16', how='left').drop('SSC_NAME16', axis=1))","9e0b842e":"scheme = mapclassify.Quantiles(suburb_joined_gdf['Price_median'], k=5)\ngeoplot.choropleth(\n    suburb_joined_gdf.geometry, hue=suburb_joined_gdf['Price_median'], scheme=scheme,\n    cmap='Greens', figsize=(20, 20)\n)\nplt.show()","f42e2cb1":"# Save to a shape file and use QGIS software to do a detailed visualisation and check\nsuburb_joined_gdf.to_file(\"Median property price and total number of properties sold by suburb.shp\")","3c52eab2":"# Read images\nimg_price = mpimg.imread('..\/input\/median-price-by-suburb\/Median price.png')\nimg_count = mpimg.imread('..\/input\/properties-sold-by-suburb\/Properties sold.png')\n\n# Display images\nfig, ax = plt.subplots(1,2, figsize=(40, 20))\nax[0].imshow(img_price);\nax[1].imshow(img_count);","66ec1345":"# Create a dataframe without nan in Price for visualisation\ndata_raw_vis = data_raw.dropna(subset=['Price'], axis=0)","a77b4de7":"reg_plots(data_raw_vis, 'Distance')","5eb83b8c":"agg_by_yearsold = agg_price_and_num_by_feature(data_raw, 'YearSold')\nbar_plots(agg_by_yearsold, 'YearSold')","cd183e55":"agg_by_monthsold = agg_price_and_num_by_feature(data_raw, 'MonthSold')\nbar_plots(agg_by_monthsold, 'MonthSold')","d644265a":"agg_by_date = agg_price_and_num_by_feature(data_raw, 'Date')\nline_plots(agg_by_date, 'Date')","56f26a92":"agg_by_yearbuilt = agg_price_and_num_by_feature(data_raw, 'YearBuilt')\nline_plots(agg_by_yearbuilt, 'YearBuilt')","f7eb799d":"space_features = ['Landsize', 'BuildingArea', 'Rooms', 'Bedroom2', 'Bathroom']\nsns.set(font_scale=1.5)\nfor idx, feature in enumerate(space_features):\n    reg_plots(data_raw_vis, feature)","6aa42f60":"agg_by_type = agg_price_and_num_by_feature(data_raw, 'Type')\nbar_plots(agg_by_type, 'Type')","e36855e1":"agg_by_method = agg_price_and_num_by_feature(data_raw, 'Method')\nbar_plots(agg_by_method, 'Method')","3c9be6fa":"agg_by_agent = agg_price_sum_num_by_feature(data_raw, 'SellerG')\nprint('-------- order by properties sold for agents --------')\nprint(agg_by_agent.sort_values(by=['Price_count'], ascending=False).head())\nprint('-------- order by total price sold for agents --------')\nprint(agg_by_agent.sort_values(by=['Price_sum'], ascending=False).head())\nprint('-------- order by median property price sold for agents --------')\nprint(agg_by_agent.sort_values(by=['Price_median'], ascending=False).head())","8cf5b06a":"reg_plots(data_raw_vis, 'Car')","747751a3":"reg_plots(data_raw_vis, 'Propertycount')","62193982":"data_raw.drop(columns=['Address', 'Date',  'Number', 'StreetName'], inplace=True)","1cd61843":"# For now, we choose the following features. I will do a better feature selection in the future\nnumeric_features = ['Car', 'Bathroom', 'Propertycount', 'Rooms', 'YearSold', 'YearBuilt', 'Landsize', 'BuildingArea', 'Bedroom2'] # 'Distance', 'MonthSold',\ncategorical_features = ['Type', 'Method', 'SellerG', 'Suburb', 'Regionname'] # 'StreetSuburb', 'CouncilArea', \n\ndata_train_raw = data_raw.loc[~data_raw.Price.isnull(), :]\ndata_test_raw = data_raw.loc[data_raw.Price.isnull(), :]\n\nprint('Training samples = {}\\nTesting samples = {}\\nTrain-test ratio = {}'.format(len(data_train_raw), len(data_test_raw), round(len(data_train_raw)\/len(data_test_raw), 1)))","464b17d6":"y = data_train_raw.Price\n\npt_y = PowerTransformer(method='box-cox')\npt_y.fit(y.to_frame()) # PowerTransformer requires a matrix\/DataFrame\ny_pt = pt_y.transform(y.to_frame())","eb1a8b6e":"fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n\nplt.subplot(1, 2, 1)\naxes[0] = sns.distplot(y, fit=norm, label = \"Skewness : %.2f\"%(y.skew()))\naxes[0] = axes[0].legend(loc=\"best\")\naxes[0].set(title='Before transformation')\n\n\nplt.subplot(1, 2, 2)\naxes[1] = sns.distplot(y_pt, fit=norm, label = \"Skewness : %.2f\"%(skew(y_pt)))\naxes[1] = axes[1].legend(loc=\"best\")\nplt.xlabel('Price (transformed)')\naxes[1].set(title='After transformation')\n\nplt.subplots_adjust(wspace=0.3)","aab2f7bf":"data_train_num = data_train_raw.drop(columns='Price').select_dtypes(include=[np.number])\ndata_test_num = data_test_raw.select_dtypes(include=[np.number])","7345d150":"log_transform_features(data_train_num, 0.5) # As a general rule of thumb, a skewness with an absolute value > 0.5 is considered at least moderately skewed\nlog_transform_features(data_test_num, 0.5)","e8cc5127":"X = data_train_raw.drop(columns='Price')\nX_predict = data_test_raw.drop(columns='Price')","870b739e":"# Try the following scalers \nscalers = {'StandardScaler': StandardScaler(),\n           'RobustScaler': RobustScaler()\n          }\n\nbest_score_gridsearch = {}\ngrid_search = {}\n\nfor key, scaler in scalers.items():\n    numeric_transformer = Pipeline(steps=[('scalar', scaler)])\n    categorical_transformer = Pipeline(steps=[('encoder', OneHotEncoder(handle_unknown='ignore'))])\n\n    preprocessor = ColumnTransformer(\n        transformers=[('num', numeric_transformer, numeric_features), \n                      ('cat', categorical_transformer, categorical_features)])\n\n    # Append regressor to preprocessing pipeline\n    pipe = Pipeline(steps=[('preprocessor', preprocessor),\n                          ('estimator', Lasso(random_state=0))])\n    \n    param_grid = {'estimator__alpha': [0.0001, 0.01, 0.1]} # a quick template for myself and not for model tuning here\n    grid_search[key] = GridSearchCV(pipe, param_grid, cv=5, scoring='neg_root_mean_squared_error')   \n    grid_search[key].fit(X,y_pt)\n    best_score_gridsearch[key] = grid_search[key].best_score_   \n    print('Best score using {} = {}'.format(key, round(best_score_gridsearch[key], 4)))","6781ab8d":"scores_df = pd.DataFrame.from_dict(best_score_gridsearch, orient='index', columns=['neg_root_mean_squared_error']).\\\n            reset_index().rename(columns={'index': 'Scaler'}).sort_values(by='neg_root_mean_squared_error', ascending=False)\nscores_df","2670a3c9":"plt.subplots(figsize=(20, 6))\nsns.barplot(data=scores_df, x='Scaler', y='neg_root_mean_squared_error')\nplt.xticks(rotation=45)\nplt.show()","c276e35e":"# Select the best estimator with the best score\nbest_estimator = grid_search[scores_df.iloc[0, 0]].best_estimator_","cb3f7919":"best_estimator","e7fd185a":"y_preds = best_estimator.predict(X_predict)\ny_preds_dollar = pt_y.inverse_transform(y_preds.reshape(-1, 1)) # Apply the inverse power transformation using the fitted lambdas\nprint('Training set median property price = {}\\nPredicting set median property price = {}'.format(y.median(), np.median(y_preds_dollar)))","3aca8303":"print(stats.describe(y_preds_dollar))\nprint(stats.describe(y))","0dbf6182":"plt.subplots(figsize=(15, 10))\n\nplt.subplot(1, 2, 1)\nsns.boxplot(y)\nplt.title('Training')\n\nplt.subplot(1, 2, 2)\nsns.boxplot(y_preds_dollar)\nplt.title('Predicting')\n\nplt.tight_layout()","eb55d42e":"plt.subplots(figsize=(15, 10))\n\nsns.distplot(y)\nsns.distplot(y_preds_dollar)\nplt.legend(labels=['Training', 'Predicting'])\n\nplt.tight_layout()","687e8375":"1. *YearSold*","279a852a":"From the distribution check above, I do not have too much confidence in the predicted Price. However, this is just a start of the journey towards a better model. \n\nThank you very much for any suggestions and feedback.","719316cb":"<a id=\"subsec-3.4\"><\/a>\n## Step 3.4: Type of properties","49445d7b":"<a id=\"subsec-3.1\"><\/a>\n## Step 3.1: Location","28ee8423":"<a id=\"subsec-5.1\"><\/a>\n## Step 5.1: Transform target variable *y*","898c95c0":"2.2 *CouncilArea*\n- THe WordCloud plot below is mainly for my learning purpose :)","857b7d00":"We can have many sensible findings from the maps above, e.g.,\n- Median property price is relatively higher in Southern and Eastern Melbourne metropolitons\n- In addition, there are relatively more properties sold in these areas as well\n- On the other hand, the western properties are more affordable\n- In general properties further to the CBD is cheaper, but not quite significant\n- ...","c0d95705":"4) Bedroom2, Bathroom, Car, Landsize, YearBuilt, and BuildingArea:\n- These are approximated using the median of properties with the same Type and in the same Suburb\n- We could get the street name for this imputation but I am not going to do this in this project","907b5483":"<a id=\"sec-3\"><\/a>\n# Step 3: Exploratory data analysis and feature engineering\n1. Explore the relationship among features and the target variable\n2. Create new features if desired\n4. Drop features that are highly correlated to others or redundant before further analysis","fe172cb2":"<a id=\"subsec-3.9\"><\/a>\n## Step 3.9: Drop features\n- Simply drop these features, 'Address', 'Date', 'Number', 'StreetName'","42763bed":"<a id=\"subsec-2.2\"><\/a>\n## Step 2.2: Outliers of numeric features","1b4480c5":"<a id=\"sec-4\"><\/a>\n# Step 4: Split data into train and test\n- This \"test\" dataset are those with missing *Price* value, which I will do a prediction after building a model using records with a valid *Price*","bdbbb623":"<a id=\"subsec-3.3\"><\/a>\n## Step 3.3: Space","0506dc35":"Set *YearBuilt* to be the same as the nearest property according to Lattitude and Longtitude","46e52bb1":"- House is the most expensive type, followed by Townhouse\n- Normally units are the cheapest option\n- Melbourne\/Victorian properties soldd in this dataset are mainly houses\n- Townhouse sold occupies the least percentage of all sales","55f6ee1f":"- Plots above indicate the general trend that there had been an increase number of properties sold over time, but the price has dropped. This intuitively reflects supply and demand in a housing market :)\n- There are so many properties sold in 2017\n- Some months have seen a lot more properties sold, e.g., March vs December (holiday month in Australia)\n- Newly built properties are less expensive. This may be caused by many newly built properties are appartment units\n- ...","b9e5addc":"<a id=\"subsec-3.8\"><\/a>\n## Step 3.8: Number of properties in a suburb","056f2927":"4. *YearBuilt*","e634b406":"We can find the following outliers, which may or may not from the same property. Remove outliers for this analysis\n* Landsize > 200,000\n* BuildingArea > 40,000\n* YearBuilt < 1700","6e6644f1":"- Nelson sold the most properties in this dataset, 2,735\n- Jellis is the champion regarding the total price of properties sold, A\\$3.4 billion\n- For is the agent with the highest median price sold, but he\/she only sold 1 property","8b709ef5":"<a id=\"subsec-2.1\"><\/a>\n## Step 2.1: Outliers of the target variable","f935e2ed":"1) Distance:\n- The only record missing *Distance* feature misses most other features.\n- Its Suburb is likely wrong\/non-existing and may be Fawkner. Fawkner Lot does not exist in Melbourne and the Lot should actually be part of the *Address*, i.e., Lot 1\/3 Brian St.\n- [Domain website](https:\/\/www.domain.com.au\/property-profile\/1-3-brian-street-fawkner-vic-3060) shows it was sold in Dec 2017 at the same price as this record, which has a different date with the dataset.\n- Considering we have 211 properties' records in Fawkner, I decided to drop this record","3396af38":"- Most properties were sold by \"property sold\" method\n- Properties sold by \"vendor bid\" had the highest median price, which is not surprising","9817e04c":"1. Interestingly, there are 7610 properties that do not have a Price. Therefore, they can be ideal records for me to predict the Price :) \n2. One by one treatment of other features with missing values.\n    - *Distance* : Approximate it the median *Distance* of the *Suburb* it is in\n    - *Postcode*, *Lattitude*, and *Longtitude*: Drop this feature, because it is highly correlated to *Suburb*. [This is not desirable for us](https:\/\/datascience.stackexchange.com\/a\/24453)\n    - *Propertycount*, *CouncilArea*, *Regionname*: check whether they can be found out by other properties in the same suburb, council, or region\n    - *Bedroom2*, *Bathroom*, *Car*, *Landsize*, *YearBuilt*, and *BuildingArea*: Approximate it using the median numbers of properties of the same *Rooms*, *Type* and in the same *Suburb*\n3. After these treatments, there will still be some missing values and another round of imputing will be executed. Details are documented as following.","3578117a":"5) Further refinement:\n- There are still records missing some features, because they do not have the same Type of property in the same Suburb, e.g., all units in Coburg North suburb do not have any BuildingArea.\n- Impute with median values of *Car*, *Bathroom*, *Bedroom2*, *Landsize*, and *BuildingArea* for properties with the same *Rooms* and *Type*\n- Impute with median values of *YearBuilt* for properties with the same *Suburb* (then *CouncilArea*). Another approach may be using the same *YearBuilt* as the nearest property according to *Lattitude* and *Longtitude*, but I leave this for another day","cab25b03":"## Methods","0767ed50":"## Methods","ab2cc108":"<a id=\"subsec-3.6\"><\/a>\n## Step 3.6: Real Estate Agent","621e56b9":"<a id=\"sec-6\"><\/a>\n# Step 6: Model building with Pipeline\n1. Following [scikit-learn algorithm cheat-sheet](https:\/\/scikit-learn.org\/stable\/tutorial\/machine_learning_map\/index.html**), I choose Lasso for simplicity\n2. Apply grid search for a quick tuning","7ab38a9c":"2.1 *Regionname*","4abe52cd":"2.3 *Suburb*\n1. [Choropleth map](https:\/\/www.arcgis.com\/apps\/MapJournal\/index.html?appid=75eff041036d40cf8e70df99641004ca) of median property price\n2. Choropleth map of total properties sold\n\n\nThese maps give us a better visualisation of property price geospatially and can reveal some patterns that are not easy to spot in other charts or tables.","1b41e432":"<a id=\"sec-5\"><\/a>\n# Step 5: Skewness check and transformation (target and numeric features)\n- Inspired by [Arun Godwin Patel](https:\/\/www.kaggle.com\/agodwinp\/stacking-house-prices-walkthrough-to-top-5) (thanks). *Machine Learning algorithms work well with features that are normally distributed, a distribution that is symmetric and has a characteristic bell shape*\n- Here we apply scikit-learn PowerTransformer upon the target variable, which makes the data more Gaussian-like in order to stabilize variance and minimize skewness. Two supported methods are Yeo-Johnson (default) and Box-Cox (only be applied to strictly positive data). Refer to [Section 6.3.2.2 in this scikit-learn documentation](https:\/\/scikit-learn.org\/stable\/modules\/preprocessing.html) for math formula\n- PowerTransformer has a [known RuntimeWarning issue of dividing by zero](https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/14959), so applied a log transform","8c743ac8":"3. *Date*","1b2ba5fc":"<a id=\"subsec-2.4\"><\/a>\n## Step 2.4: Outliers of datatime feature\n- There are 78 unique dates in this datasets\n- Check whether there are dates that seem to be outliers, e.g., 0\/1\/2050 :)\n- Transform *Date* column to two new features, *YearSold* and *MonthSold*. This belongs to feature engineering, but I put it forward here","b7ba8778":"Choropleth maps\n- geoplot package provides the choropleth method for a quick view in python\n- Two suburbs, Bellfield and Hillside, are in the far west and far east of Victoria\n- I save the GeoDataFrame into a shape file and use QGIS to create two maps","0cb3cdd0":"- *Rooms* have the highest correlation with *Price*\n- These features, except *Landsize*, are correlated to each other","9ac865a4":"<a id=\"sec-0\"><\/a>\n# Step 0: Imports and methods","2837ec79":"2) Postcode, Lattitude, and Longtitude:\n- Drop these features as discussed earlier","a11fc473":"<a id=\"subsec-3.5\"><\/a>\n## Step 3.5: Sold method","00ef5c01":"1. Location is one of the most important feature for a property as we know around the world, so I will extract *StreetName* from *Address* feature and then concatenate with Suburb to create a new feature called *StreetSuburb*. I am not sure whether this will eventually help with the model.\n    - The street number may be too much detail for the predicting model, which may not be helpful at all\n   ","e472d2d7":"- Number of properties in a suburb may not matter much for this model\n- However, there is still a small negative correlation with Price\n    - This could make sense when a suburb has many appartment buildings, which tend to be cheaper than houses\n    - Conversely, some CBD or near CBD suburbs' appartment prices are still very high. They do tend to be smaller in terms of suburb land size??","8932d5e6":"2. Visualise the relationship between *Price* and location features","782719fa":"2. *MonthSold*","6d758e09":"Above, we can see some information, e.g., Jellis, a Real Estate Agent, sold the most properties (3,359). However, I do not know how to find outliers for categorical features. Please leave me a comment if you have any suggestions. Thanks","8a3a57b5":"# Table of Contents\n\n* [Preamble](#sec-preamble)\n* [Step 0 - Imports and methods](#sec-0)\n* [Step 1 - Read data](#sec-1)\n* [Step 2 - Outliers and missing values](#sec-2)\n    * [Step 2.1 - Outliers of the target variable](#subsec-2.1)\n    * [Step 2.2 - Outliers of numeric features](#subsec-2.2)\n    * [Step 2.3 - Outliers of categorical features](#subsec-2.3)\n    * [Step 2.4 - Outliers of datatime feature](#subsec-2.4)\n    * [Step 2.5 - Missing values](#subsec-2.5)\n* [Step 3 - Exploratory data analysis and feature engineering](#sec-3)\n    * [Step 3.1 - Location](#subsec-3.1)\n    * [Step 3.2 - Year and month](#subsec-3.2)\n    * [Step 3.3 - Space](#subsec-3.3)\n    * [Step 3.4 - Type of properties](#subsec-3.4)\n    * [Step 3.5 - Sold method](#subsec-3.5)\n    * [Step 3.6 - Real Estate Agent](#subsec-3.6)\n    * [Step 3.7 - Number of carspots](#subsec-3.7)\n    * [Step 3.8 - Number of properties in a suburb](#subsec-3.8)\n    * [Step 3.9 - Drop features](#subsec-3.9)\n* [Step 4 - Split data into train and test](#sec-4)\n* [Step 5 - Skewness check and transformation (target and numeric features)](#sec-5)\n    * [Step 5.1 - Transform target variable y](#subsec-5.1)\n    * [Step 5.2 - Transform numeric features](#subsec-5.2)\n* [Step 6 - Model building with Pipeline](#sec-6)\n* [Step 7 - Prediction](#sec-7)","da179f47":"We can see Price is right skewed but unable to see those extremely high price records. Therefore, we keep checking using a scatter plot. However, keep in mind that we need to transform Price and other potential significantly skewed features before modelling for a better performance.","e82dca17":"<a id=\"subsec-2.5\"><\/a>\n## Step 2.5: Missing values\n1. Quick and naive treatment\n    - If a feature has too many missing values, drop that feature\n    - If a feature has a reasonable amount of missing values, impute it with median (numeric feature) or mode (categorical feature), i.e., the most commonly occuring value\n2. I will \n    - Impute the missing values of each feature individually considering their geospatial location information if available    \n3. Detailed imputation methods in scikit-learn is [here](https:\/\/scikit-learn.org\/stable\/modules\/impute.html)","541c6fee":"3) Propertycount, CouncilArea, Regionname:\n- These should be the same as other properties in the same Suburb","e9a48f3b":"<a id=\"sec-2\"><\/a>\n# Step 2: Outliers and missing values\n1. Remove outliers if desirable.\n    - Target variable\n    - Numeric features\n    - Categorical features. Note: I accidentally found a few typos in *Suburb*, but not sure how this could be done in a general way\n    - Datetime feature\n2. Impute missing values of features.","a5d301f0":"- The possitive correlation is as expected between number of carpots and price\n- There is some property having 18 carpots, which might cause some problems to our model in the future steps","af6eea68":"2.4 *Distance*","da3f937f":"<a id=\"sec-7\"><\/a>\n# Step 7: Prediction","1655fc32":"<a id=\"subsec-3.2\"><\/a>\n## Step 3.2: Year and month","bdd081c3":"<a id=\"sec-1\"><\/a>\n# Step 1: Read data","334ef57a":"<a id=\"subsec-2.3\"><\/a>\n## Step 2.3: Outliers of categorical features","a80aaec4":"<a id=\"subsec-3.7\"><\/a>\n## Step 3.7: Number of carspots","42ba780e":"<a id=\"subsec-5.2\"><\/a>\n## Step 5.2: Transform numeric features\n- Log transform of the skewed numerical features to lessen impact of outliers. Thanks to [juliencs](https:\/\/www.kaggle.com\/juliencs\/a-study-on-regression-applied-to-the-ames-dataset) and [Alexandru Papiu](https:\/\/www.kaggle.com\/apapiu\/house-prices-advanced-regression-techniques\/regularized-linear-models)","4bba8063":"<a id=\"sec-preamble\"><\/a>\n# **Preamble**\n\nThis is my first public kernel as a beginner of machine learning. I tried to document what I learnt from various resources and people in this kernel. Thanks to all, in particular, [Arun Godwin Patel](https:\/\/www.kaggle.com\/agodwinp\/stacking-house-prices-walkthrough-to-top-5), [juliencs](https:\/\/www.kaggle.com\/juliencs\/a-study-on-regression-applied-to-the-ames-dataset), and [Alexandru Papiu](https:\/\/www.kaggle.com\/apapiu\/house-prices-advanced-regression-techniques\/regularized-linear-models)\n\n\nI applied a simple Pipeline including preprocessing for numeric and categorical features with Lasso regression. There are 7610 properties that do not have Price value, so I just give a prediction of their Price in the end. \n\n\nI also quickly produced two maps in QGIS for visualising the median Price and total properties sold by suburbs in a map, which may be interesting to some readers. \n\n\nI will update this kernel to include more feature engineering, feature selection, model selection, hyperparameter tuning, and result interpretation in the future.","b3c8130d":"* By far, Southern Metropolitan region has the most most expensive properties and is the most expensive region on average\n* Regional Victoria, i.e., Eastern Victoria, Northern Victoria, and Western Victoria, is the most affordable regions in Victoria. This is understandable because they are regional areas\n* Buyers can find cheaper properties in Western Metropolitan region due to some reason, which is out of the scope of this study"}}