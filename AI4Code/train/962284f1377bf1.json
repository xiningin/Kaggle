{"cell_type":{"87c574a5":"code","8371d957":"code","5c16f3ee":"code","41e31238":"code","4c0296fa":"code","2d7d688d":"code","076e4187":"code","7691340d":"code","79cd2aef":"code","9db44c21":"code","853435f8":"code","2a156fee":"code","34bd3f61":"code","faa7a4ee":"code","55580b82":"code","ee94735d":"code","ea71b308":"code","4fa7c3bd":"code","46e58cb0":"code","0a2077fe":"code","a9cbb15b":"code","01920e92":"code","1da9c93e":"code","fff8cbc8":"code","f65b2611":"code","3b54101e":"code","982725ff":"code","212d7c2f":"code","39be9efb":"code","b65bcab5":"code","46fcab5e":"code","1789bd88":"code","2ed36fef":"code","95acf7d9":"code","7a3caecc":"code","5e5a62cd":"code","6db8c3e0":"code","9aaf924b":"code","90fb03a4":"code","6b28b40e":"code","2418a3a3":"code","707d2f17":"code","f6a7cad7":"code","d83dea9a":"code","09d8b73c":"code","a167acea":"code","4f3223c9":"code","200be5db":"code","95f5bcf5":"code","ddcf5c5d":"code","c6f335cb":"code","e0c6db32":"code","65a5ccbb":"code","c62ec24f":"code","f0724ee0":"code","28489551":"code","462d94ba":"code","5612cf8e":"code","025813c0":"code","a410f0b7":"code","8b083f5d":"code","4e9be477":"code","94c927b4":"code","9d5debf7":"code","158c2ee9":"code","e2d85218":"code","8521a998":"code","31b892fe":"code","1f07b83b":"code","d0d00f4b":"code","0398e546":"code","20bee621":"code","94d5a9e3":"code","ad34a437":"code","1e6404ef":"code","21185ccb":"code","a0f73d9c":"code","785bd96f":"code","9895cf1b":"code","894c584d":"code","e09a9489":"code","9c7e6036":"code","90535d6e":"markdown","b4a4ae96":"markdown","0873001d":"markdown","f476eed7":"markdown","f92941bd":"markdown","f5030d02":"markdown","a1f2ee1b":"markdown","e763fc30":"markdown","7d4f35ca":"markdown","f96060be":"markdown","536ab6d4":"markdown","09bef6b3":"markdown","cf1938c8":"markdown","c535f6d0":"markdown","97ca536d":"markdown","1024e0d0":"markdown","e6af0337":"markdown","0ed6b4cb":"markdown","d4ab824b":"markdown","d4c9a66c":"markdown","64ea24bc":"markdown","7f99f73e":"markdown","19335a1b":"markdown","3926a57e":"markdown","8fc5dc1d":"markdown","7d35c933":"markdown","5d6f5520":"markdown","d9197180":"markdown","81889ff1":"markdown","3c193b68":"markdown","d2f89599":"markdown","9ea5ea27":"markdown","54080b65":"markdown","ffe353de":"markdown","f10070ab":"markdown","6a2169b2":"markdown","f849da9e":"markdown","4c719b83":"markdown","11cd5daf":"markdown","27806636":"markdown","89df12c1":"markdown","36ebba45":"markdown","facf0e39":"markdown"},"source":{"87c574a5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8371d957":"df = pd.read_csv('\/kaggle\/input\/widsdatathon2021\/TrainingWiDS2021.csv')","5c16f3ee":"pd.set_option('display.max_columns', None)\ndf.head()","41e31238":"#filling median in numerical data\ndf1 = df.fillna(df.median())","4c0296fa":"percent_missing = df1.isnull().sum() * 100 \/ len(df1)\nmissing_value_df = pd.DataFrame({'column_name': df1.columns,\n                                 'percent_missing': percent_missing})\npd.set_option('display.max_rows', None)\nmissing_value_df","2d7d688d":"df1 = df1.drop('hospital_admit_source',axis=1)","076e4187":"df1.shape","7691340d":"## Print the categorical columns\nprint([c for c in df1.columns if (1<df1[c].nunique()) & (df1[c].dtype != np.number)& (df1[c].dtype != int) ])","79cd2aef":"#i am taking 40 percent of actual data\ndf_40 = df1.sample(frac = 0.40) ","9db44c21":"df_40.shape","853435f8":"percent_missing = df_40.isnull().sum() * 100 \/ len(df_40)\nmissing_value_df_40 = pd.DataFrame({'column_name': df_40.columns,\n                                 'percent_missing': percent_missing})\npd.set_option('display.max_rows', None)\nmissing_value_df_40","2a156fee":"\ndf_40.head()","34bd3f61":"df_male = df_40[df_40['gender']=='M']","faa7a4ee":"df_male['height'].mean()","55580b82":"df_F = df_40[df_40['gender']=='F']\ndf_F['height'].mean()","ee94735d":"def impute_gender(cols):\n    height = cols[1]\n    gender = cols[0]\n    if pd.isnull(gender):\n        if (height<161.834):\n            return 'F'\n        else:\n            return 'M'\n    else:\n        return gender","ea71b308":"df_40['gender'] = df_40[['gender','height']].apply(impute_gender,axis = 1)","4fa7c3bd":"df_40['gender'].isnull().sum()","46e58cb0":"df_40['ethnicity'].value_counts()","0a2077fe":"df_caus = df_40[df_40['ethnicity']=='Caucasian']\ndf_caus['height'].mean()","a9cbb15b":"df_caus = df_40[df_40['ethnicity']=='African American']\ndf_caus['height'].mean()","01920e92":"df_caus = df_40[df_40['ethnicity']=='Other\/Unknown']\ndf_caus['height'].mean()","1da9c93e":"df_caus = df_40[df_40['ethnicity']=='Hispanic']\ndf_caus['height'].mean()","fff8cbc8":"df_caus = df_40[df_40['ethnicity']=='Asian']\ndf_caus['height'].mean()","f65b2611":"df_caus = df_40[df_40['ethnicity']=='Native American']\ndf_caus['height'].mean()","3b54101e":"def impute_ethnicity(cols):\n    height = cols[1]\n    ethnicity = cols[0]\n    if pd.isnull(ethnicity):\n        if (height>170.361):\n            return 'African American'\n        elif(height<162.740):\n            return 'Asian'\n        elif (height>167 and height<170):\n            return 'Caucasian'\n        elif (height>162.740 and height<166):\n            return 'Hispanic'\n        elif (height>166. and height<168):\n                return 'Other\/Unknown'\n        else:\n            return 'Native American'\n                \n    else:\n        return ethnicity","982725ff":"df_40['ethnicity'] = df_40[['ethnicity','height']].apply(impute_ethnicity,axis = 1)","212d7c2f":"df_40['ethnicity'].isnull().sum()","39be9efb":"df_40.head(300)","b65bcab5":"df_40 = df_40.drop(['icu_admit_source', 'icu_stay_type', 'icu_type'],axis=1)","46fcab5e":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ndf_40['gender'] = le.fit_transform(df_40['gender'])\ndf_40['ethnicity'] = le.fit_transform(df_40['ethnicity'])","1789bd88":"x = df_40.drop('diabetes_mellitus', axis = 1)\ny = df_40['diabetes_mellitus']","2ed36fef":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.1,stratify=y)","95acf7d9":"%%time\nfrom sklearn.linear_model import LogisticRegression\nlog = LogisticRegression()\nlog.fit(x_train,y_train)","7a3caecc":"log.score(x_test,y_test)","5e5a62cd":"print('Original shape',df_40.shape)","6db8c3e0":"from sklearn.feature_selection import VarianceThreshold","9aaf924b":"constant_filter = VarianceThreshold(threshold=0) #100 percent constant features will be removed\nconstant_filter.fit(x_train)   ","90fb03a4":"VarianceThreshold(threshold=0)","6b28b40e":"constant_filter.get_support().sum()","2418a3a3":"constant_list = [not temp for temp in constant_filter.get_support()]\nx.columns[constant_list]","707d2f17":"#let us transform training and testing data into these\nx_train_filter = constant_filter.transform(x_train)\nx_test_filter = constant_filter.transform(x_test)","f6a7cad7":"print('original size of train data',x_train.shape)\nprint('After Constant feature removal',x_train_filter.shape)","d83dea9a":"quasi_constant_filter = VarianceThreshold(threshold=0.01) #looking for 99 percent similarity\nquasi_constant_filter.fit(x_train_filter)\nVarianceThreshold(threshold=0.01)","09d8b73c":"quasi_constant_filter.get_support().sum()","a167acea":"x_train_quasi = quasi_constant_filter.transform(x_train_filter)\nx_test_quasi = quasi_constant_filter.transform(x_test_filter)","4f3223c9":"print('original size of train data',x_train.shape)\nprint('After Constant feature removal',x_train_filter.shape)\nprint('After removing quasi constant features',x_test_quasi.shape)","200be5db":"x_train_T = x_train_quasi.T\nx_test_T = x_test_quasi.T\ntype(x_train_T)","95f5bcf5":"x_train_T = pd.DataFrame(x_train_T)\nx_test_T = pd.DataFrame(x_test_T)","ddcf5c5d":"x_train_T.duplicated().sum()","c6f335cb":"duplicated_features = x_train_T.duplicated()\nfeatures_to_keep = [not index for index in duplicated_features]","e0c6db32":"x_train_unique = x_train_T[features_to_keep].T\nx_test_unique = x_test_T[features_to_keep].T","65a5ccbb":"print('original size of train data',x_train.shape)\nprint('After Constant feature removal',x_train_filter.shape)\nprint('After removing quasi constant features',x_train_quasi.shape)\nprint('Unique Features',x_train_unique.shape)","c62ec24f":"corrmat= x_train_unique.corr()","f0724ee0":"def get_correlation(data, threshold):\n    corr_col = set()\n    corrmat = data.corr()\n    for i in range(len(corrmat.columns)):\n        for j in range(i):\n            if abs(corrmat.iloc[i, j])> threshold:\n                colname = corrmat.columns[i]\n                corr_col.add(colname)\n    return corr_col","28489551":"corr_features = get_correlation(x_train_unique, 0.85)","462d94ba":"len(corr_features)","5612cf8e":"\nx_train_uncorr = x_train_unique.drop(labels=corr_features, axis = 1)\nx_test_uncorr = x_test_unique.drop(labels = corr_features, axis = 1)\nprint('original size of train data',x_train.shape)\nprint('After Constant feature removal',x_train_filter.shape)\nprint('After removing quasi constant features',x_train_quasi.shape)\nprint('Unique Features',x_train_unique.shape)\nprint('After removing co related features',x_train_uncorr.shape)","025813c0":"log.fit(x_train_uncorr,y_train)\nlog.score(x_test_uncorr,y_test)","a410f0b7":"import matplotlib.pyplot as plt\nimportance = log.coef_[0]\nfor i,v in enumerate(importance):\n\tprint('Feature: %0d, Score: %.5f' % (i,v))\n# plot feature importance\nplt.bar([x for x in range(len(importance))], importance)\nplt.xlabel('Feature Number')\nplt.ylabel('Importance')\n","8b083f5d":"# RandomForest Classifier","4e9be477":"from sklearn.ensemble import RandomForestClassifier\nforest = RandomForestClassifier()\nforest.fit(x_train_uncorr,y_train)","94c927b4":"forest.score(x_test_uncorr,y_test)","9d5debf7":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\ncf = confusion_matrix(y_test,forest.predict(x_test_uncorr))","158c2ee9":"sns.heatmap(cf\/np.sum(cf), annot=True, \n            fmt='.2%', cmap='Blues')","e2d85218":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_selection import SelectFromModel","8521a998":"sel = SelectFromModel(RandomForestClassifier(n_estimators=100, random_state=0, n_jobs=-1))\nsel.fit(x_train_uncorr, y_train)","31b892fe":"sel.get_support()","1f07b83b":"features = x_train_uncorr.columns[sel.get_support()]","d0d00f4b":"len(features)","0398e546":"x_train_forest = sel.transform(x_train_uncorr)\nx_test_forest = sel.transform(x_test_uncorr)","20bee621":"forest.fit(x_train_forest,y_train)","94d5a9e3":"forest.score(x_test_forest,y_test)","ad34a437":"\ncf = confusion_matrix(y_test,forest.predict(x_test_forest))\nsns.heatmap(cf\/np.sum(cf), annot=True, \n            fmt='.2%', cmap='Blues')","1e6404ef":"forest_opt = RandomForestClassifier(max_depth=8,min_samples_split=850,max_leaf_nodes=50,min_samples_leaf=550,n_estimators=200,max_samples=0.25,max_features=6)","21185ccb":"forest_opt.fit(x_train_forest,y_train)","a0f73d9c":"forest.score(x_test_forest,y_test)","785bd96f":"import xgboost as xgb","9895cf1b":"D_train = xgb.DMatrix(x_train_forest, label=y_train)\nD_test = xgb.DMatrix(x_test_forest, label=y_test)","894c584d":"param = {\n    'eta': 0.3, \n    'max_depth': 3,  \n    'objective': 'multi:softprob',  \n    'num_class': 3} \n\nsteps = 20  # The number of training iterations","e09a9489":"model = xgb.train(param, D_train, steps)","9c7e6036":"import numpy as np\nfrom sklearn.metrics import precision_score, recall_score, accuracy_score\n\npreds = model.predict(D_test)\nbest_preds = np.asarray([np.argmax(line) for line in preds])\n\nprint(\"Precision = {}\".format(precision_score(y_test, best_preds, average='macro')))\nprint(\"Recall = {}\".format(recall_score(y_test, best_preds, average='macro')))\nprint(\"Accuracy = {}\".format(accuracy_score(y_test, best_preds)))","90535d6e":"There is no significant increase in performance","b4a4ae96":"# TO BE CONTINUED..","0873001d":"i am removing feature which still has more tht 10 percent empty data ","f476eed7":"we succesfully removed 12 features that were not necessary","f92941bd":"I am not using GridSearch or RandomSearch CV , i am randomyly putting some parameters which i read from an article\n\nhttps:\/\/www.analyticsvidhya.com\/blog\/2020\/03\/beginners-guide-random-forest-hyperparameter-tuning\/\n\nYou can read article ","f5030d02":"we would apply same principal as previous and we would see height of all different ethnicities and then fill the average height with that","a1f2ee1b":"Before attacking actual data , let us attack a sub part of data which would help us in seeing the trends","e763fc30":"2 features have been removed","7d4f35ca":"# SELECTING TRAIN AND TEST DATA","f96060be":"REMOVING DUPLICATE FEATURES","536ab6d4":"Deciding Paramters","09bef6b3":"So There are 49 features that are important and actually contributing to the model and its efficiency","cf1938c8":"Converting our training and testing features to these 49 features","c535f6d0":"Let us see number of features after removing quasi constant filters","97ca536d":"Filling categorical missing data with most frequent data entry","1024e0d0":"# REMOVING CO RELATED FEATURES","e6af0337":"# IMPUTING GENDER","0ed6b4cb":"# Recursive Feature Elimination (RFE) by Using Tree Based and Gradient Based Estimators","d4ab824b":"let us see number of features after removal of constant features","d4c9a66c":"These icu type do not affect how diabetes in the body would be so removing these columns","64ea24bc":"If two features are exactly same those are called as duplicate features that means these features doesn\u2019t provide any new information and makes our model complex.","7f99f73e":"## Feature selection by feature importance of random forest classifier ","19335a1b":"Now I will drop those rows where there is some missing categorical data","3926a57e":"Let us see the constant features that have been removed","8fc5dc1d":"# XGBOOST CLASSIFIER","7d35c933":"Those features which contain constant values (i.e. only one value for all the outputs or target values) in the dataset are known as Constant Features.","5d6f5520":"In this, first we need to create a variance threshold and then fit the model with training set of the data.","d9197180":"CONSTANT FEATURE REMOVAL","81889ff1":"so we have three duplicated features","3c193b68":"So, our strategy to fill missing gender data with one strategy\n1. We would check avg height for all the male and female\n\n2.We would see avg height for the missing gender data and then according to average height , we would fill the gender","d2f89599":"To pass data into xgboost classifier it has tp be in a different format","9ea5ea27":"Average height for male is 176.217 cm","54080b65":"REMOVING CO RELATED FEATURES","ffe353de":"# FILLING NUMERICAL MISSING DATA WITH MEDIAN","f10070ab":"# RANDOM FOREST : PARAMETER OPTIMIZATION","6a2169b2":"    from sklearn.ensemble import R","f849da9e":"Logistic Regression","4c719b83":"# UNIVARIATE FILTERING METHODS","11cd5daf":"QUASI CONSTANT FEATURE REMOVAL : These are the filters that are almost constant or quasi constant in other words these features have same values for large subset of outputs and such features are not very useful for making predictions.","27806636":"we sucessfully filled our empty male and female places in our dataframe","89df12c1":"# ETHNICITY","36ebba45":"So basically True is telling we have important features and False are the features that can be ignored","facf0e39":"Average height for female is 161.834 cm"}}