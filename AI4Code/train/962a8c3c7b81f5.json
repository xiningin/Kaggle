{"cell_type":{"57f27dec":"code","4a8e86b3":"code","8be04d23":"code","1efa0b74":"code","54a1c1e3":"code","4899f8d5":"code","fbeb9c14":"code","8b55d2f9":"code","d4419454":"code","d93112ac":"code","a13ac642":"code","4a88fbb5":"code","3bb33195":"code","775f12be":"code","d6ac9041":"code","43c3d6b0":"code","b4b3a4d8":"code","5da9b068":"code","cfae45e2":"code","01a136d2":"code","3a563b24":"markdown","4a0915c5":"markdown","06dfaa53":"markdown","392f7f1d":"markdown","f07436ce":"markdown","8953dbc3":"markdown","9e8bdb31":"markdown","d37ffbd4":"markdown","973a1ee6":"markdown"},"source":{"57f27dec":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4a8e86b3":"import lightgbm as lgb\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import roc_auc_score, confusion_matrix\nimport warnings","8be04d23":"!pip install kaggler","1efa0b74":"import kaggler\nfrom kaggler.model import AutoLGB\nfrom kaggler.preprocessing import DAE, TargetEncoder, LabelEncoder\n\nprint(f'Kaggler: {kaggler.__version__}')","54a1c1e3":"warnings.simplefilter('ignore')\npd.set_option('max_columns', 100)","4899f8d5":"feature_name = 'dae_te'\nalgo_name = 'lgb'\nmodel_name = f'{algo_name}_{feature_name}'\n\ndata_dir = Path('\/kaggle\/input\/tabular-playground-series-apr-2021\/')\ntrn_file = data_dir \/ 'train.csv'\ntst_file = data_dir \/ 'test.csv'\nsample_file = data_dir \/ 'sample_submission.csv'\npseudo_label_file = '..\/input\/tps-apr-2021-pseudo-label-dae\/REMEK-TPS04-FINAL005.csv'\n\nfeature_file = f'{feature_name}.csv'\npredict_val_file = f'{model_name}.val.txt'\npredict_tst_file = f'{model_name}.tst.txt'\nsubmission_file = f'{model_name}.sub.csv'\n\ntarget_col = 'Survived'\nid_col = 'PassengerId'","fbeb9c14":"n_fold = 5\nseed = 42\nencoding_dim = 64","8b55d2f9":"trn = pd.read_csv(trn_file, index_col=id_col)\ntst = pd.read_csv(tst_file, index_col=id_col)\nsub = pd.read_csv(sample_file, index_col=id_col)\npseudo_label = pd.read_csv(pseudo_label_file, index_col=id_col)\nprint(trn.shape, tst.shape, sub.shape, pseudo_label.shape)","d4419454":"tst[target_col] = pseudo_label[target_col]\nn_trn = trn.shape[0]\ndf = pd.concat([trn, tst], axis=0)\ndf.head()","d93112ac":"# Feature engineering code from https:\/\/www.kaggle.com\/udbhavpangotra\/tps-apr21-eda-model\n\ndf['Embarked'] = df['Embarked'].fillna('No')\ndf['Cabin'] = df['Cabin'].fillna('_')\ndf['CabinType'] = df['Cabin'].apply(lambda x:x[0])\ndf.Ticket = df.Ticket.map(lambda x:str(x).split()[0] if len(str(x).split()) > 1 else 'X')\n\ndf['Age'].fillna(round(df['Age'].median()), inplace=True,)\ndf['Age'] = df['Age'].apply(round).astype(int)\n\n# Fare, fillna with mean value\nfare_map = df[['Fare', 'Pclass']].dropna().groupby('Pclass').median().to_dict()\ndf['Fare'] = df['Fare'].fillna(df['Pclass'].map(fare_map['Fare']))\n\ndf['FirstName'] = df['Name'].str.split(', ').str[0]\ndf['SecondName'] = df['Name'].str.split(', ').str[1]\n\ndf['n'] = 1\n\ngb = df.groupby('FirstName')\ndf_names = gb['n'].sum()\ndf['SameFirstName'] = df['FirstName'].apply(lambda x:df_names[x]).fillna(1)\n\ngb = df.groupby('SecondName')\ndf_names = gb['n'].sum()\ndf['SameSecondName'] = df['SecondName'].apply(lambda x:df_names[x]).fillna(1)\n\ndf['Sex'] = (df['Sex'] == 'male').astype(int)\n\ndf['FamilySize'] = df.SibSp + df.Parch + 1\n\nfeature_cols = ['Pclass', 'Age','Embarked','Parch','SibSp','Fare','CabinType','Ticket','SameFirstName', 'SameSecondName', 'Sex',\n                'FamilySize', 'FirstName', 'SecondName']\ncat_cols = ['Pclass','Embarked','CabinType','Ticket', 'FirstName', 'SecondName']\nnum_cols = [x for x in feature_cols if x not in cat_cols]\nprint(len(feature_cols), len(cat_cols), len(num_cols))","a13ac642":"for col in ['SameFirstName', 'SameSecondName', 'Fare', 'FamilySize', 'Parch', 'SibSp']:\n    df[col] = np.log2(1 + df[col])\n    \nscaler = StandardScaler()\ndf[num_cols] = scaler.fit_transform(df[num_cols])","4a88fbb5":"lbe = LabelEncoder(min_obs=50)\ndf[cat_cols] = lbe.fit_transform(df[cat_cols]).astype(int)","3bb33195":"cv = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=seed)\nte = TargetEncoder(cv=cv)\ndf_te = te.fit_transform(df[cat_cols], df[target_col])\ndf_te.columns = [f'te_{col}' for col in cat_cols]\ndf_te.head()","775f12be":"dae = DAE(cat_cols=cat_cols, num_cols=num_cols, encoding_dim=encoding_dim)\nX = dae.fit_transform(df[feature_cols])","d6ac9041":"df_dae = pd.DataFrame(X, columns=[f'dae_{i}' for i in range(encoding_dim)])\nprint(df_dae.shape)","43c3d6b0":"X = pd.concat([df[feature_cols], df_te, df_dae], axis=1)\ny = df[target_col]\nX_tst = X.iloc[n_trn:]\n\np = np.zeros_like(y, dtype=float)\np_tst = np.zeros((tst.shape[0],))\nprint(f'Training a stacking ensemble LightGBM model:')\nfor i, (i_trn, i_val) in enumerate(cv.split(X, y)):\n    if i == 0:\n        clf = AutoLGB(objective='binary', metric='auc', sample_size=len(i_trn), random_state=seed)\n        clf.tune(X.iloc[i_trn], y[i_trn])\n        features = clf.features\n        params = clf.params\n        n_best = clf.n_best\n        print(f'{n_best}')\n        print(f'{params}')\n        print(f'{features}')\n    \n    trn_data = lgb.Dataset(X.iloc[i_trn], y[i_trn])\n    val_data = lgb.Dataset(X.iloc[i_val], y[i_val])\n    clf = lgb.train(params, trn_data, n_best, val_data, verbose_eval=100)\n    p[i_val] = clf.predict(X.iloc[i_val])\n    p_tst += clf.predict(X_tst) \/ n_fold\n    print(f'CV #{i + 1} AUC: {roc_auc_score(y[i_val], p[i_val]):.6f}')","b4b3a4d8":"np.savetxt(predict_val_file, p, fmt='%.6f')\nnp.savetxt(predict_tst_file, p_tst, fmt='%.6f')","5da9b068":"print(f'  CV AUC: {roc_auc_score(y, p):.6f}')\nprint(f'Test AUC: {roc_auc_score(pseudo_label[target_col], p_tst)}')","cfae45e2":"n_pos = int(0.34911 * tst.shape[0])\nth = sorted(p_tst, reverse=True)[n_pos]\nprint(th)\nconfusion_matrix(pseudo_label[target_col], (p_tst > th).astype(int))","01a136d2":"sub[target_col] = (p_tst > th).astype(int)\nsub.to_csv(submission_file)","3a563b24":"## **UPDATE on 6\/12\/2021**\n\nToday's `Kaggler` v.9.13 release includes transfer learning between `DAE`\/`SDAE`. Now you can initialize `DAE`\/`SDAE` with a pretrained model as follows:\n\n```python\n# train supervised DAE only with trianing data\nsdae = SDAE(cat_cols=cat_cols, num_cols=num_cols, encoding_dim=encoding_dim, random_state=RANDOM_SEED)\n_ = sdae.fit_transform(trn[feature_cols], trn[TARGET_COL])\n\n# initialize unsupervied DAE and train it with both training and test data\ndae = DAE(cat_cols=cat_cols, num_cols=num_cols, encoding_dim=encoding_dim, random_state=RANDOM_SEED,\n          pretrained_model=sdae, freeze_embedding=True)\n_ = dae.fit_transform(df[feature_cols])\n\n# initialize another supervised DAE and train it with training data\nsdae2 = SDAE(cat_cols=cat_cols, num_cols=num_cols, encoding_dim=encoding_dim, random_state=RANDOM_SEED,\n             pretrained_model=dae, freeze_embedding=False)\n_ = sdae2.fit_transform(trn[feature_cols], trn[TARGET_COL])\n```\n\nYou can check the example of using transfer learning between DAE and SDAE in the notebook below:\n* [TPS 6 Supervised DAE + Keras (GPU)](https:\/\/www.kaggle.com\/jeongyoonlee\/tps-6-supervised-dae-keras-gpu)\n\nEnjoy~!\n\n## **UPDATE on 6\/8\/2021**\n\nToday's `Kaggler` v.9.10 release includes further improvements in `DAE`\/`SDAE` as follows:\n\n* add an option, `n_encoder` to add more than 1 encoder in `DAELayer`\n* add an option, `validation_data` to add validation data in `DAE`\/`SDAE`\n* make label-encoding optional in `DAE`\/`SDAE`\n\nEspecially, the first one is useful. previous winning solutions using DAE usually combines multiple (3 ~ 5) encodings together to improve feature representation.\n\nThis is different from `n_layer` in `DAE`\/`SDAE`, which determines the number of the encoder\/decoder pairs to be stacked. e.g. `DAE` with `n_layer=3` and `n_encoder=2` will have three encoder\/decoder pairs, and in each pair, there will be two encoders (dense layers).\n\nHope you find it useful.\n\n\n## **UPDATE on 6\/3\/2021**\n\nI added the supervised version of DAE, `SDAE` to `Kaggler` in today's v0.9.8 release. At Kaggle, DAE is mostly used as a unsupervised feature extraction method. However, it's possible to train DAE in a supervised manner with a target variable.\n\nTo transform features with `SDAE`, you can do as follows:\n\n```python\nsdae = SDAE(cat_cols=feature_cols, encoding_dim=encoding_dim, n_layer=1, noise_std=.001, random_state=seed)\nsdae.fit(trn[feature_cols], y)\nX = sdae.transform(df[feature_cols])\n```\n\nYou can find an example from this notebook, [TPS 6 Supervised DAE + Keras (GPU)](https:\/\/www.kaggle.com\/jeongyoonlee\/tps-6-supervised-dae-keras-gpu).\n\nHope it helps.\n\n## **UPDATE on 5\/1\/2021**\n\nToday, [`Kaggler`](https:\/\/github.com\/jeongyoonlee\/Kaggler) v0.9.4 is released with additional features for DAE as follows:\n* In addition to the swap noise (`swap_prob`), the Gaussian noise (`noise_std`) and zero masking (`mask_prob`) have been added to DAE to overcome overfitting.\n* Stacked DAE is available through the `n_layer` input argument (see Figure 3. in [Vincent et al. (2010), \"Stacked Denoising Autoencoders\"](https:\/\/www.jmlr.org\/papers\/volume11\/vincent10a\/vincent10a.pdf) for reference).\n\nFor example, to build a stacked DAE with 3 pairs of encoder\/decoder and all three types of noises, you can do:\n```python\nfrom kaggler.preprocessing import DAE\n\ndae = DAE(cat_cols=cat_cols, num_cols=num_cols, n_layer=3, noise_std=.05, swap_prob=.2, masking_prob=.1)\nX = dae.fit_transform(pd.concat([trn, tst], axis=0))\n```\n\nIf you're using previous versions, please upgrade `Kaggler` using `pip install -U kaggler`.\n\n---\n\nToday I released a new version (v0.9.0) of the `Kaggler` package with Denoising AutoEncoder (DAE) with the swap noise. \n\nNow you can train a DAE with only 2 lines of code as follows:\n\n```python\ndae = DAE(cat_cols=cat_cols, num_cols=num_cols, encoding_dim=encoding_dim)\nX = dae.fit_transform(df[feature_cols])\n```\n\nIn addition to the new DAE feature encoder, `Kaggler` supports many of feature transformations used in Kaggle including:\n* `TargetEncoder`: with smoothing and cross-validation to avoid overfitting\n* `FrequencyEncoder`\n* `LabelEncoder`: that imputes missing values and groups rare categories\n* `OneHotEncoder`: that imputes missing values and groups rare categories\n* `EmbeddingEncoder`: that transforms categorical features into embeddings\n* `QuantileEncoder`: that transforms numerical features into quantiles\n\nIn the notebook below, I will show how to use `Kaggler`'s `LabelEncoder`, `TargetEncoder`, and `DAE` for feature engineering, then use `Kaggler`'s `AutoLGB` to do feature selection and hyperparameter optimization.","4a0915c5":"### Label encoding with rare category grouping and missing value imputation","06dfaa53":"### Submission","392f7f1d":"If you find it useful, please upvote the notebook and leave your feedback. It will be greatly appreciated!\n\nAlso please check my previous notebooks as well:\n* [AutoEncoder + Pseudo Label + AutoLGB](https:\/\/www.kaggle.com\/jeongyoonlee\/autoencoder-pseudo-label-autolgb): shows how to build a basic AutoEncoder using Keras, and perform automated feature selection and hyperparameter optimization using Kaggler's AutoLGB.\n* [Supervised Emphasized Denoising AutoEncoder](https:\/\/www.kaggle.com\/jeongyoonlee\/supervised-emphasized-denoising-autoencoder): shows how to build a more sophiscated version of AutoEncoder, called supervised emphasized Denoising AutoEncoder (DAE), which trains DAE and a classifier simultaneously.\n* [Stacking Ensemble](https:\/\/www.kaggle.com\/jeongyoonlee\/stacking-ensemble): shows how to perform stacking ensemble.","f07436ce":"### AutoLGB for Feature Selection and Hyperparameter Optimization","8953dbc3":"# Part 1: Data Loading & Feature Engineering","9e8bdb31":"# Part 2: Model Training","d37ffbd4":"### DAE","973a1ee6":"### Target encoding with smoothing and 5-fold cross-validation"}}