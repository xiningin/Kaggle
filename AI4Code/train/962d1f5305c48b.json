{"cell_type":{"cf119cd7":"code","3b819f40":"code","d22a2d81":"code","f5945328":"code","e6d835ac":"code","2134e48f":"code","a07700ec":"code","4d4ef139":"code","467efe6a":"code","be9df97c":"code","20eb93b3":"code","2ff09f36":"code","abd20dc0":"code","bea9d315":"code","671cba37":"code","d9274e46":"code","9ebb07bb":"code","25b6c395":"code","839a2b6f":"code","18f6538f":"code","b6c07353":"code","bb883d3a":"markdown","cffb26d6":"markdown","fe9ead14":"markdown","c096c58d":"markdown","e0937935":"markdown","5494329f":"markdown","9507a390":"markdown","d6c26bd2":"markdown","60facf1c":"markdown"},"source":{"cf119cd7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3b819f40":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.linear_model import Ridge, LinearRegression\nfrom sklearn.metrics import mean_squared_error","d22a2d81":"\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer,TfidfVectorizer\nfrom sklearn.multioutput import MultiOutputClassifier\nfrom sklearn.metrics import classification_report, accuracy_score,make_scorer,confusion_matrix\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import precision_recall_fscore_support","f5945328":"import re\nimport nltk\nnltk.download(['punkt','wordnet','stopwords'])\nfrom nltk.tokenize import word_tokenize,sent_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer","e6d835ac":"import string","2134e48f":"np.random.seed(0)","a07700ec":"full_data = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/train.csv\")","4d4ef139":"full_data.head()","467efe6a":"excerpts = full_data[\"excerpt\"]\ntarget = full_data[\"target\"]","be9df97c":"def tokenize(text):\n    table = text.maketrans(dict.fromkeys(string.punctuation))\n    words = word_tokenize(text.lower().strip().translate(table))\n    words = [word for word in words if word not in stopwords.words('english')]\n    lemmed = [WordNetLemmatizer().lemmatize(word) for word in words]\n    lemmed = [WordNetLemmatizer().lemmatize(word, pos='v') for word in lemmed]\n    stemmed = [PorterStemmer().stem(word) for word in lemmed]\n    \n    return stemmed","20eb93b3":"excerpts[5]","2ff09f36":"tokenize(excerpts[5])","abd20dc0":"pipeline = Pipeline([\n    ('vect', CountVectorizer(tokenizer=tokenize)),\n    ('tfidf', TfidfTransformer()),\n    ('clf', RandomForestRegressor(random_state=42))\n])","bea9d315":"X_train, X_val, y_train, y_val = train_test_split(excerpts, target, random_state=42)","671cba37":"import time\ntime_begin = time.time()\npipeline.fit(X_train, y_train)\nprint(time.time() - time_begin)","d9274e46":"y_pred = pipeline.predict(X_val)","9ebb07bb":"mean_absolute_error(y_val,y_pred)","25b6c395":"test_data = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/test.csv\")","839a2b6f":"test_excerpts = test_data[\"excerpt\"]","18f6538f":"y_sub=pipeline.predict(test_excerpts)","b6c07353":"x_sub = test_data[[\"id\"]].copy()\nx_sub[\"target\"] = y_sub\nx_sub.to_csv('submission.csv', index = False)\nx_sub","bb883d3a":"## Set seed to get the same results each time","cffb26d6":"splitting the dataset for making validation set","fe9ead14":"These tokenization are basics. They may or maynot be good for perfromance for this particular competition.","c096c58d":"## Load the training data","e0937935":"submission ready","5494329f":"## Separate the text from the target- readability score","9507a390":"## Loading required packages","d6c26bd2":"Whole pipeline from text processing(lemmetaization,stemming) to Modelling.","60facf1c":"Prediction on Test Data."}}