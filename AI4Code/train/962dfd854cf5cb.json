{"cell_type":{"74504c2a":"code","79f84b69":"code","4f439e0d":"code","085ad1d6":"code","ac075991":"code","cf6d51e9":"code","8c005819":"code","e279d1b6":"code","7ff5fefd":"code","39372219":"code","3ba74dc4":"code","43dfc99d":"code","70098142":"code","638b589f":"code","ff8fbd7a":"code","a1caa5c8":"code","134315a7":"code","4176e3c6":"code","a02c0c0c":"code","46332d42":"code","1cd201d7":"code","d4422456":"code","4f517e95":"code","9c52e1ae":"code","32948ba2":"code","e034554e":"code","49879cec":"code","feae2073":"code","a383d7ca":"code","009daad8":"code","2d4c0a4d":"code","cb964579":"code","61dac9bf":"code","b7c8792b":"code","e4f3f7b5":"code","1c64540e":"code","2702e261":"code","37faec64":"code","9cd11492":"code","f441305b":"code","d2e8fb02":"code","8bd72a6a":"code","84e9880b":"code","7b161b74":"code","208c3220":"code","88c8becf":"code","7c07f784":"code","63e20848":"code","dc7a72e0":"code","6309c02f":"code","2dc86f99":"code","296b61f7":"code","a0483f2c":"code","14b4d98d":"code","85b4adcb":"code","ae3acbfa":"code","70f3ddad":"code","139e166d":"code","5b027b87":"code","1a361f5d":"code","1cbb4907":"code","4541c943":"code","9a15afd8":"code","8b394fdd":"code","df17f876":"code","6879e83d":"code","cc8dbf1f":"code","c3f862d6":"code","a583e48f":"code","4a34a73c":"code","459386ab":"code","07f375f5":"code","41df7d58":"code","55d34ec3":"code","c473f6bb":"code","ff5aebd1":"code","31d098a2":"code","c0eb56ba":"code","e98e2a8b":"code","ebc6b85b":"code","7b5d91ed":"markdown","57b2544b":"markdown","efeefc92":"markdown","06a22c97":"markdown","7c639f1c":"markdown","1cbc7ff7":"markdown","e2e11eae":"markdown","26cccea6":"markdown","c2ab2b9f":"markdown","d50cf68b":"markdown","bb32f99c":"markdown","cb4424cf":"markdown","7ca48458":"markdown","4a58483e":"markdown","1ee801d9":"markdown","6d0789c9":"markdown","ff516060":"markdown","6afefeba":"markdown","31935752":"markdown","7531ddee":"markdown","6aa59cff":"markdown","4e6e118a":"markdown","51f1e9e2":"markdown","88f641ca":"markdown","55e947eb":"markdown","e2c8055e":"markdown","519f50ee":"markdown","30a102ed":"markdown","083a19b2":"markdown","25e21fbd":"markdown","7c88fefc":"markdown","533b4c25":"markdown","50cde5af":"markdown"},"source":{"74504c2a":"import os\r\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\r\n    for filename in filenames:\r\n        print(os.path.join(dirname, filename))","79f84b69":"!pip install openpyxl\r\n!pip install plotly\r\n!pip install xlrd\r\n!pip install seaborn\r\nimport pandas as pd\r\nimport numpy as np \r\nimport matplotlib.pyplot as plt\r\nimport plotly.express as px\r\nimport seaborn as sns\r\nimport warnings\r\nwarnings.filterwarnings(\"ignore\")","4f439e0d":"df = pd.read_csv(\"..\/input\/openintro-possum\/possum.csv\")","085ad1d6":"df.shape","ac075991":"df.ndim","cf6d51e9":"df.describe()","8c005819":"df.isnull().sum()","e279d1b6":"df.duplicated().sum()","7ff5fefd":"# Dropping null rows \r\ndf.dropna(axis=0,inplace=True)","39372219":"print(\"Data shape after dropping null values: {}\".format(df.shape))\r\nprint(\"Data dimension: {}\".format(df.ndim))\r\nprint(\"Checking for null values: \\n{}\".format(df.isnull().sum()))","3ba74dc4":"df.keys()","43dfc99d":"# Categorical columns\r\nunique = [\"sex\", 'Pop', 'site']\r\n\r\n# Accessing the key and values\r\nfor x in unique:\r\n    # Accessing the unique labels in each  column\r\n    print(f\"In column {x}: {df[x].unique()}\")\r\n    # Total number of unique labels in each column\r\n    print(f\"Total number of unique values: {df[x].nunique()}\\n\")","70098142":"df.head(10)","638b589f":"df.sample(10)","ff8fbd7a":"df['site'] = df['site'].apply(lambda x:str(x))\r\ndf.info() #change site into string","a1caa5c8":"# Categorical columns\r# \nunique = [\"sex\", 'Pop', 'site']\r\n\r\n\r\nfig, ax = plt.subplots(3, figsize=(10,10))\r\nax = ax.ravel()\r\n\r\nfor index, value in enumerate(unique):\r\n    sns.boxplot(x='age', y=value, data=df, ax=ax[index])","134315a7":"df.info()","4176e3c6":"# Looking into the correlation\r\nplt.subplots(figsize=(15,15))\r\nsns.heatmap(df.corr(),annot=True)\r\nplt.show()","a02c0c0c":"body = [\r\n'hdlngth',\r\n'skullw',\r\n'totlngth',\r\n'taill',\r\n'footlgth',\r\n'earconch',\r\n'eye',\r\n'chest',\r\n'belly']\r\n\r\nfig, ax = plt.subplots(3,3, figsize=(10,10), constrained_layout=True)\r\nax = ax.ravel()\r\n\r\n\r\n# row, column - (1,1),-> (1,'hdlngth'), (1,2) -> (1 ,'skullw')\r\n# It's accessing the list\r\nfor index, value in enumerate(body):\r\n    sns.histplot(x=value, data=df, ax=ax[index], kde=True)\r\n    ax[index].set_title(f\"Skewness:{np.around(df[value].skew(axis=0),2)}\")\r\n","46332d42":"fig, ax = plt.subplots(3,3, figsize=(10,10), constrained_layout=True)\r\nax = ax.ravel()\r\n\r\n\r\nfor index, value in enumerate(body):\r\n    log = (f'{value}_log')\r\n    df[log] = df[value].apply(lambda x: np.log(x+1))\r\n    sns.histplot(x=log, data=df, ax=ax[index], kde=True)\r\n    ax[index].set_title(f\"Skewness in log10: {np.around(df[log].skew(axis=0),2)}\")","1cd201d7":"sns.histplot(x='age', data=df, kde=True)\r\nplt.title(f\"Skewness:{np.around(df['age'].skew(axis=0),2)}\")","d4422456":"# log transformation\r\ndf[\"age_log\"] = df['age'].apply(lambda x: np.log10(x+1))\r\nsns.histplot(x='age_log', data=df, kde=True)\r\nplt.title('Skewness: {}'.format(np.around(df['age_log'].skew(axis=0),2)))\r\nplt.show()","4f517e95":"df.head()","9c52e1ae":"df.info()","32948ba2":"# Different column variations\r\n\r\nX_log = ['site', 'Pop', 'sex','hdlngth_log','skullw_log', 'totlngth_log', 'taill_log', 'footlgth_log',\r\n         'earconch_log', 'eye_log', 'chest_log', 'belly_log']\r\n\r\nX_regular = ['site', 'Pop', 'sex', 'hdlngth', 'skullw', 'totlngth', 'taill',\r\n       'footlgth', 'earconch', 'eye', 'chest', 'belly']\r\n\r\nX_mix = ['skullw_log', 'taill_log', 'earconch_log', 'eye_log', 'chest_log','totlngth', 'taill',\r\n       'footlgth','site', 'Pop', 'sex', 'belly']\r\n\r\ny_regular = 'age'\r\ny_log = 'age_log'","e034554e":"from sklearn.linear_model import  LinearRegression,Ridge,Lasso\r\nfrom sklearn.model_selection import train_test_split, GridSearchCV, KFold\r\nfrom sklearn.metrics import  mean_squared_error","49879cec":"X_reg = df[X_regular]\r\ny_reg = df[y_regular]","feae2073":"X_reg = pd.get_dummies(X_reg)\r\nX_reg","a383d7ca":"X_train, X_test, y_train, y_test = train_test_split(X_reg, y_reg, test_size=.30, random_state=42)","009daad8":"lr =LinearRegression()\r\nlr.fit(X_train, y_train)\r\ny_pred_test = lr.predict(X_test)\r\nprint(f'RMSE value: {np.around(np.sqrt(mean_squared_error(y_test, y_pred_test)),3)}')\r\nprint(f'Standard Deviation of Age: {np.around(df.age.std(),3)}')","2d4c0a4d":"values_ridge = [0.1,1,10]\r\nparams_grid_ridge = {\"alpha\":values_ridge, \"max_iter\":[1000]}\r\nprint(f'Ridge grid {params_grid_ridge}')","cb964579":"grid_ridge = GridSearchCV(Ridge(),param_grid=params_grid_ridge,\r\ncv=KFold(n_splits=5))","61dac9bf":"grid_ridge.fit(X_train, y_train)","b7c8792b":"print(\"Grid- SearchCV with accuracy\")\r\nprint(\"Best params: \", grid_ridge.best_params_)\r\nprint(\"Best score: {:.3f} \".format( grid_ridge.best_score_))\r\nprint(\"RMSE: {:.3f}\".format(np.sqrt(mean_squared_error(y_test,grid_ridge.predict(X_test)))))","e4f3f7b5":"values_lasso = [1,0.1,0.001]\r\nparams_grid_lasso = {\"alpha\":values_lasso, \"max_iter\":[1000]}\r\nprint(f'Lasso grid {params_grid_lasso}')\r\ngrid_lasso = GridSearchCV(Ridge(),param_grid=params_grid_lasso,\r\ncv=KFold(n_splits=5))\r\ngrid_lasso.fit(X_train, y_train)","1c64540e":"print(\"Grid- SearchCV with accuracy\" )\r\nprint(\"Best params: \", grid_lasso.best_params_)\r\nprint(\"Best score: {:.3f} \".format( grid_lasso.best_score_))\r\nprint(\"RMSE: {:.3f}\".format(np.sqrt(mean_squared_error(y_test,grid_lasso.predict(X_test)))))","2702e261":"## The credit for this EstimatorSelectionHelper class is given to David S. Batista \r\n## Link - https:\/\/github.com\/davidsbatista\/machine-learning-notebooks\/blob\/master\/hyperparameter-across-models.ipynb\r\n\r\nimport numpy as np\r\nimport pandas as pd\r\n\r\nfrom sklearn.model_selection import GridSearchCV\r\n\r\nclass EstimatorSelectionHelper:\r\n    \r\n    def __init__(self, models, params):\r\n        self.models = models\r\n        self.params = params\r\n        self.keys = models.keys()\r\n        self.grid_searches = {}\r\n    \r\n    def fit(self, X, y, **grid_kwargs):\r\n        for key in self.keys:\r\n            print('Running GridSearchCV for %s.' % key)\r\n            model = self.models[key]\r\n            params = self.params[key]\r\n            grid_search = GridSearchCV(model, params, **grid_kwargs)\r\n            grid_search.fit(X, y)\r\n            self.grid_searches[key] = grid_search\r\n        print('Done.')\r\n    \r\n    def score_summary(self, sort_by='mean_test_score'):\r\n        frames = []\r\n        for name, grid_search in self.grid_searches.items():\r\n            frame = pd.DataFrame(grid_search.cv_results_)\r\n            frame = frame.filter(regex='^(?!.*param_).*$')\r\n            frame['estimator'] = len(frame)*[name]\r\n            frames.append(frame)\r\n        df = pd.concat(frames)\r\n        \r\n        df = df.sort_values([sort_by], ascending=False)\r\n        df = df.reset_index()\r\n        # The below line of code was commented out to show\r\n        # the rank of each model\r\n        # df = df.drop(['rank_test_score', 'index'], 1)\r\n        \r\n        columns = df.columns.tolist()\r\n        columns.remove('estimator')\r\n        columns = ['estimator']+columns\r\n        df = df[columns]\r\n        return df","37faec64":"values_ridge = [0.1,1,10]\r\nvalues_lasso = [1,0.1,0.001]\r\nmodels = {\r\n    'Ridge': Ridge(),\r\n    \"Lasso\": Lasso()\r\n}\r\n\r\nparams = {\r\n    'Ridge': {\"alpha\":values_ridge, \"max_iter\":[1000]},\r\n    \"Lasso\": {\"alpha\":values_lasso, \"max_iter\":[1000]}\r\n}","9cd11492":"helper1 = EstimatorSelectionHelper(models, params)\r\nhelper1.fit(X_train, y_train,cv=KFold(n_splits=5))","f441305b":"helper1.score_summary().T","d2e8fb02":"X = df[X_log]\r\ny = df[y_log]\r\n\r\nX = pd.get_dummies(X)\r\n\r\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.30, random_state=42)","8bd72a6a":"lr =LinearRegression()\r\nlr.fit(X_train, y_train)\r\n\r\ny_pred_test_log = lr.predict(X_test)\r\n\r\n# Removing log \r\ny_test = np.exp(y_test)-1\r\ny_pred_test = np.exp(y_pred_test_log)-1\r\n\r\n\r\nprint(f'RMSE value: {np.around(np.sqrt(mean_squared_error(y_test, y_pred_test)),3)}')\r\nprint(f'Standard Deviation of Age: {np.around(df.age.std(),3)}')","84e9880b":"# Making use of the EstimatorSelectionHelper class through a function\r\n# Using a function that takes in your X_train and y_train values\r\ndef EstimatorSelectionHelper_execute(X_train, y_train):\r\n    values_ridge = [0.1,1,10]\r\n    values_lasso = [1,0.1,0.001]\r\n    models = {\r\n        'Ridge': Ridge(),\r\n        \"Lasso\": Lasso()\r\n    }\r\n\r\n    params = {\r\n        'Ridge': {\"alpha\":values_ridge, \"max_iter\":[1000]},\r\n        \"Lasso\": {\"alpha\":values_lasso, \"max_iter\":[1000]}\r\n    }\r\n\r\n\r\n    helper1 = EstimatorSelectionHelper(models, params)\r\n    helper1.fit(X_train, y_train,cv=KFold(n_splits=5))\r\n\r\n    return helper1.score_summary().T\r\n","7b161b74":"EstimatorSelectionHelper_execute(X_train, y_train)","208c3220":"# Ridge regression for log inputs\r\ndef ridge_log(X_train, X_test, y_train, y_test,alpha):\r\n    ridge =Ridge(alpha=alpha, max_iter=1000)\r\n    ridge.fit(X_train, y_train)\r\n    y_pred_test_log = ridge.predict(X_test)\r\n\r\n    # Removing log \r\n    y_test = np.exp(y_test)-1\r\n    y_pred_test = np.exp(y_pred_test_log)-1\r\n\r\n\r\n    print(f'RMSE test: {np.around(np.sqrt(mean_squared_error(y_test, y_pred_test)),3)}')\r\n    print(f'Standard Deviation of Age: {np.around(df.age.std(),3)}')\r\n\r\nridge_log(X_train, X_test, y_train, y_test,0.1)","88c8becf":"# Lasso regression for log inputs\r\ndef lassoregression_log(X_train, X_test, y_train, y_test,alpha):\r\n    lasso =Lasso(alpha=alpha, max_iter=1000)\r\n    lasso.fit(X_train, y_train)\r\n    y_pred_test_log = lasso.predict(X_test)\r\n\r\n    # Removing log \r\n    y_test = np.exp(y_test)-1\r\n    y_pred_test = np.exp(y_pred_test_log)-1\r\n\r\n\r\n    print(f'RMSE test: {np.around(np.sqrt(mean_squared_error(y_test, y_pred_test)),3)}')\r\n    print(f'Standard Deviation of Age: {np.around(df.age.std(),3)}')\r\n\r\nlassoregression_log(X_train, X_test, y_train, y_test,1)","7c07f784":"X = df[X_mix].copy()\r\ny = df[y_log].copy()\r\n\r\nX = pd.get_dummies(X)\r\n\r\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.30, random_state=42)","63e20848":"def linearregression_log(X_train, X_test, y_train, y_test):\r\n    lr =LinearRegression()\r\n    lr.fit(X_train, y_train)\r\n    y_pred_test_log = lr.predict(X_test)\r\n\r\n    # Removing log \r\n    y_test = np.exp(y_test)-1\r\n    y_pred_test = np.exp(y_pred_test_log)-1\r\n\r\n\r\n    print(f'RMSE test: {np.around(np.sqrt(mean_squared_error(y_test, y_pred_test)),3)}')\r\n    print(f'Standard Deviation of Age: {np.around(df.age.std(),3)}')\r\n\r\nlinearregression_log(X_train, X_test, y_train, y_test)","dc7a72e0":"EstimatorSelectionHelper_execute(X_train, y_train)","6309c02f":"ridge_log(X_train, X_test, y_train, y_test,10)","2dc86f99":"lassoregression_log(X_train, X_test, y_train, y_test,0.1)","296b61f7":"X = df[X_mix].copy()\r\ny = df[y_regular].copy()\r\n\r\nX = pd.get_dummies(X)\r\n\r\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.30, random_state=42)","a0483f2c":"def linearregression(X_train, X_test, y_train, y_test):\r\n    lr =LinearRegression()\r\n    lr.fit(X_train, y_train)\r\n    y_pred_test = lr.predict(X_test)\r\n\r\n    print(f'RMSE test: {np.around(np.sqrt(mean_squared_error(y_test, y_pred_test)),3)}')\r\n    print(f'Standard Deviation of Age: {np.around(df.age.std(),3)}')\r\n\r\nlinearregression(X_train, X_test, y_train, y_test)","14b4d98d":"def ridge(X_train, X_test, y_train, y_test,alpha):\r\n    ridge =Ridge(alpha=alpha, max_iter=1000)\r\n    ridge.fit(X_train, y_train)\r\n    y_pred_test = ridge.predict(X_test)\r\n\r\n    \r\n    print(f'RMSE test: {np.around(np.sqrt(mean_squared_error(y_test, y_pred_test)),3)}')\r\n    print(f'Standard Deviation of Age: {np.around(df.age.std(),3)}')","85b4adcb":"def lassoregression(X_train, X_test, y_train, y_test,alpha):\r\n    lasso =Lasso(alpha=alpha, max_iter=1000)\r\n    lasso.fit(X_train, y_train)\r\n    y_pred_test = lasso.predict(X_test)\r\n\r\n    print(f'RMSE test: {np.around(np.sqrt(mean_squared_error(y_test, y_pred_test)),3)}')\r\n    print(f'Standard Deviation of Age: {np.around(df.age.std(),3)}')","ae3acbfa":"EstimatorSelectionHelper_execute(X_train, y_train)","70f3ddad":"ridge(X_train, X_test, y_train, y_test,10)","139e166d":"lassoregression(X_train, X_test, y_train, y_test,1)","5b027b87":"body","1a361f5d":"X = df[X_regular].copy()\r\ny = df[y_regular].copy()","1cbb4907":"bins_2 = ['footlgth', 'earconch']\r\n\r\nfor col in bins_2:\r\n    bins = np.linspace(X[col].min(),X[col].max(),2)\r\n    bin_s = np.digitize(X[col],bins=bins)\r\n    X[f\"{col}_binned\"] = bin_s\r\n    X[f\"{col}_binned\"] = X[f\"{col}_binned\"].apply(lambda x:str(x))","4541c943":"X = pd.get_dummies(X)\r\nX","9a15afd8":"X.info()","8b394fdd":"X.drop(bins_2,axis=1, inplace=True)\r\nX_train, X_test, y_train, y_test = train_test_split(X, y, \r\ntest_size=0.30, random_state=42)\r\nlinearregression(X_train, X_test, y_train, y_test)","df17f876":"EstimatorSelectionHelper_execute(X_train,y_train)","6879e83d":"ridge(X_train, X_test, y_train, y_test,10)","cc8dbf1f":"lassoregression(X_train, X_test, y_train, y_test,1)","c3f862d6":"bin_num = [2,3,4,5,6,7,8,9,10,11,12,13,14,15]\r\n\r\nbins_2 = ['footlgth', 'earconch']\r\n\r\ndef linearregression_binning(bin_num):\r\n    for num in bin_num:\r\n        X = df[X_regular].copy()\r\n        y = df[y_regular].copy()\r\n\r\n        for col in bins_2:\r\n            bins = np.linspace(X[col].min(),X[col].max(),num)\r\n            bin_s = np.digitize(X[col],bins=bins)\r\n            X[f\"{col}_binned\"] = bin_s\r\n            X[f\"{col}_binned\"] = X[f\"{col}_binned\"].apply(lambda x:str(x))\r\n\r\n        X.drop(bins_2,axis=1, inplace=True)\r\n\r\n        X = pd.get_dummies(X)\r\n        X_train, X_test, y_train, y_test = train_test_split(X, y, \r\n        test_size=0.30, random_state=42)\r\n        lr = LinearRegression()\r\n        lr.fit(X_train,y_train)\r\n        y_pred = lr.predict(X_test)\r\n        \r\n        print(f\"At bin = {num}, RMSE:{np.around(np.sqrt(mean_squared_error(y_test,y_pred)),3)} \")\r\n        print(f\"Standard Deviation of Age: {np.around(df.age.std(),3)}\\n\")\r\nlinearregression_binning(bin_num) ","a583e48f":"bin_num = [2,3,4,5,6,7,8,9,10,11,12,13,14,15]\r\n\r\nbins_2 = ['footlgth', 'earconch']\r\n\r\ndef ridge_binning(bin_num):\r\n    for num in bin_num:\r\n        X = df[X_regular].copy()\r\n        y = df[y_regular].copy()\r\n\r\n        for col in bins_2:\r\n            bins = np.linspace(X[col].min(),X[col].max(),num)\r\n            bin_s = np.digitize(X[col],bins=bins)\r\n            X[f\"{col}_binned\"] = bin_s\r\n            X[f\"{col}_binned\"] = X[f\"{col}_binned\"].apply(lambda x:str(x))\r\n\r\n        X.drop(bins_2,axis=1, inplace=True)\r\n        X = pd.get_dummies(X)\r\n        X_train, X_test, y_train, y_test = train_test_split(X, y, \r\n        test_size=0.30, random_state=42)\r\n        ridge =Ridge(max_iter=1000)\r\n        ridge.fit(X_train, y_train)\r\n        y_pred_test = ridge.predict(X_test)\r\n\r\n        \r\n        print(f\"At bin = {num}\")\r\n        print(f'RMSE test: {np.around(np.sqrt(mean_squared_error(y_test, y_pred_test)),3)}')\r\n        print(f'Standard Deviation of Age: {np.around(df.age.std(),3)}')\r\n\r\nridge_binning(bin_num) ","4a34a73c":"bin_num = [2,3,4,5,6,7,8,9,10,11,12,13,14,15]\r\n\r\nbins_2 = ['footlgth', 'earconch']\r\n\r\ndef lasso_binning(bin_num):\r\n    for num in bin_num:\r\n        X = df[X_regular].copy()\r\n        y = df[y_regular].copy()\r\n\r\n        for col in bins_2:\r\n            bins = np.linspace(X[col].min(),X[col].max(),num)\r\n            bin_s = np.digitize(X[col],bins=bins)\r\n            X[f\"{col}_binned\"] = bin_s\r\n            X[f\"{col}_binned\"] = X[f\"{col}_binned\"].apply(lambda x:str(x))\r\n\r\n        X.drop(bins_2,axis=1, inplace=True)\r\n        X = pd.get_dummies(X)\r\n        X_train, X_test, y_train, y_test = train_test_split(X, y, \r\n        test_size=0.30, random_state=42)\r\n        lasso =Lasso(max_iter=1000)\r\n        lasso.fit(X_train, y_train)\r\n        y_pred_test = lasso.predict(X_test)\r\n\r\n        \r\n        print(f\"At bin = {num}\")\r\n        print(f'RMSE test: {np.around(np.sqrt(mean_squared_error(y_test, y_pred_test)),3)}')\r\n        print(f'Standard Deviation of Age: {np.around(df.age.std(),3)}')\r\n    \r\n\r\n\r\nlasso_binning(bin_num)","459386ab":"# adding back \r\n\r\nbin_num_7 = 7\r\n\r\nbins_2 = ['footlgth', 'earconch']\r\n\r\ndef linearregression_bin_7(bin_num_7):\r\n    X = df[X_regular].copy()\r\n    y = df[y_regular].copy()\r\n\r\n    for col in bins_2:\r\n        bins = np.linspace(X[col].min(),X[col].max(),bin_num_7)\r\n        bin_s = np.digitize(X[col],bins=bins)\r\n        X[f\"{col}_binned\"] = bin_s\r\n        X[f\"{col}_binned\"] = X[f\"{col}_binned\"].apply(lambda x:str(x))\r\n\r\n    # No dropping \r\n    X = pd.get_dummies(X)\r\n    X_train, X_test, y_train, y_test = train_test_split(X, y, \r\n    test_size=0.30, random_state=42)\r\n    lr = LinearRegression()\r\n    lr.fit(X_train,y_train)\r\n    y_pred = lr.predict(X_test)\r\n    \r\n    print(f\"At bin = {bin_num_7}, RMSE:{np.around(np.sqrt(mean_squared_error(y_test,y_pred)),3)} \")\r\n    print(f\"Standard Deviation of Age: {np.around(df.age.std(),3)}\\n\")\r\n\r\n\r\nlinearregression_bin_7(bin_num_7) # bin 7\r\n","07f375f5":"bin_num_7 = 7\r\n\r\nbins_2 = ['footlgth', 'earconch']\r\n\r\ndef ridge_bin_7(bin_num_7):\r\n\r\n    X = df[X_regular].copy()\r\n    y = df[y_regular].copy()\r\n\r\n    for col in bins_2:\r\n        bins = np.linspace(X[col].min(),X[col].max(),bin_num_7)\r\n        bin_s = np.digitize(X[col],bins=bins)\r\n        X[f\"{col}_binned\"] = bin_s\r\n        X[f\"{col}_binned\"] = X[f\"{col}_binned\"].apply(lambda x:str(x))\r\n\r\n    # No dropping \r\n    X = pd.get_dummies(X)\r\n    X_train, X_test, y_train, y_test = train_test_split(X, y, \r\n    test_size=0.30, random_state=42)\r\n    ridge =Ridge(max_iter=1000)\r\n    ridge.fit(X_train, y_train)\r\n    y_pred_test = ridge.predict(X_test)\r\n\r\n    \r\n    print(f\"At bin = {bin_num_7}\")\r\n    print(f'RMSE test: {np.around(np.sqrt(mean_squared_error(y_test, y_pred_test)),3)}')\r\n    print(f'Standard Deviation of Age: {np.around(df.age.std(),3)}')\r\n\r\nridge_bin_7(bin_num_7) ","41df7d58":"bin_num_7 = 7\r\n\r\nbins_2 = ['footlgth', 'earconch']\r\n\r\ndef lasso_bin_7(bin_num_7):\r\n\r\n    X = df[X_regular].copy()\r\n    y = df[y_regular].copy()\r\n\r\n    for col in bins_2:\r\n        bins = np.linspace(X[col].min(),X[col].max(),bin_num_7)\r\n        bin_s = np.digitize(X[col],bins=bins)\r\n        X[f\"{col}_binned\"] = bin_s\r\n        X[f\"{col}_binned\"] = X[f\"{col}_binned\"].apply(lambda x:str(x))\r\n\r\n    # No dropping \r\n    X = pd.get_dummies(X)\r\n    X_train, X_test, y_train, y_test = train_test_split(X, y, \r\n    test_size=0.30, random_state=42)\r\n    lasso = Lasso(max_iter=1000)\r\n    lasso.fit(X_train, y_train)\r\n    y_pred_test = lasso.predict(X_test)\r\n\r\n    \r\n    print(f\"At bin = {bin_num_7}\")\r\n    print(f'RMSE test: {np.around(np.sqrt(mean_squared_error(y_test, y_pred_test)),3)}')\r\n    print(f'Standard Deviation of Age: {np.around(df.age.std(),3)}')\r\n\r\nlasso_bin_7(bin_num_7)","55d34ec3":"# adding back \r\n\r\nbin_num_7 = 7\r\n\r\nbins_2 = ['footlgth', 'earconch']\r\n\r\ndef linearregression_bin_7_product(bin_num_7):\r\n    X = df[X_regular].copy()\r\n    y = df[y_regular].copy()\r\n\r\n    for col in bins_2:\r\n        bins = np.linspace(X[col].min(),X[col].max(),bin_num_7)\r\n        bin_s = np.digitize(X[col],bins=bins)\r\n        X[f\"{col}_binned\"] = bin_s\r\n        X[f\"{col}_binned\"] = X[f\"{col}_binned\"].apply(lambda x:str(x))\r\n\r\n\r\n\r\n    X = pd.get_dummies(X)\r\n    X[\r\n        ['footlgth_binned_1', 'footlgth_binned_2', 'footlgth_binned_3',\r\n            'footlgth_binned_4', 'earconch_binned_1', 'earconch_binned_2',\r\n            'earconch_binned_3', 'earconch_binned_4']\r\n    ]\r\n\r\n    dummy = [\r\n        ['footlgth_binned_1', 'footlgth_binned_2', 'footlgth_binned_3',\r\n                        'footlgth_binned_4'], \r\n            ['earconch_binned_1', 'earconch_binned_2',\r\n                        'earconch_binned_3', 'earconch_binned_4']\r\n                        ]\r\n    \r\n    original = ['footlgth', 'earconch']\r\n   \r\n    for o in range(0, len(original)):\r\n        for d in range(0,len(dummy[o])):\r\n            col_name = f\"{original[o]} * {dummy[o][d]}\"\r\n            X[col_name] = (X[original[o]] * X[dummy[o][d]])\r\n    X.drop(bins_2,axis=1, inplace=True)\r\n    X_train, X_test, y_train, y_test = train_test_split(X, y, \r\n    test_size=0.30, random_state=42)\r\n    lr = LinearRegression()\r\n    lr.fit(X_train,y_train)\r\n    y_pred = lr.predict(X_test)\r\n    \r\n    print(f\"At bin = {bin_num_7}, RMSE:{np.around(np.sqrt(mean_squared_error(y_test,y_pred)),3)} \")\r\n    print(f\"Standard Deviation of Age: {np.around(df.age.std(),3)}\\n\")\r\n\r\n\r\nlinearregression_bin_7_product(bin_num_7) # bin 7\r\n","c473f6bb":"# adding back \r\n\r\nbin_num_7 = 7\r\n\r\nbins_2 = ['footlgth', 'earconch']\r\n\r\ndef ridge_bin_7_product(bin_num_7):\r\n    X = df[X_regular].copy()\r\n    y = df[y_regular].copy()\r\n\r\n    for col in bins_2:\r\n        bins = np.linspace(X[col].min(),X[col].max(),bin_num_7)\r\n        bin_s = np.digitize(X[col],bins=bins)\r\n        X[f\"{col}_binned\"] = bin_s\r\n        X[f\"{col}_binned\"] = X[f\"{col}_binned\"].apply(lambda x:str(x))\r\n\r\n\r\n\r\n    X = pd.get_dummies(X)\r\n    X[\r\n        ['footlgth_binned_1', 'footlgth_binned_2', 'footlgth_binned_3',\r\n            'footlgth_binned_4', 'earconch_binned_1', 'earconch_binned_2',\r\n            'earconch_binned_3', 'earconch_binned_4']\r\n    ]\r\n\r\n    dummy = [\r\n        ['footlgth_binned_1', 'footlgth_binned_2', 'footlgth_binned_3',\r\n                        'footlgth_binned_4'], \r\n            ['earconch_binned_1', 'earconch_binned_2',\r\n                        'earconch_binned_3', 'earconch_binned_4']\r\n                        ]\r\n    \r\n    original = ['footlgth', 'earconch']\r\n    \r\n    for o in range(0, len(original)):\r\n        for d in range(0,len(dummy[o])):\r\n            col_name = f\"{original[o]} * {dummy[o][d]}\"\r\n            X[col_name] = (X[original[o]] * X[dummy[o][d]])\r\n    X.drop(bins_2,axis=1, inplace=True)\r\n    X_train, X_test, y_train, y_test = train_test_split(X, y, \r\n    test_size=0.30, random_state=42)\r\n    ridge = Ridge()\r\n    ridge.fit(X_train,y_train)\r\n    y_pred = ridge.predict(X_test)\r\n    \r\n    print(f\"At bin = {bin_num_7}, RMSE:{np.around(np.sqrt(mean_squared_error(y_test,y_pred)),3)} \")\r\n    print(f\"Standard Deviation of Age: {np.around(df.age.std(),3)}\\n\")\r\n\r\n\r\nridge_bin_7_product(bin_num_7) ","ff5aebd1":"bin_num_7 = 7\r\n\r\nbins_2 = ['footlgth', 'earconch']\r\n\r\ndef lasso_bin_7_product(bin_num_7):\r\n    X = df[X_regular].copy()\r\n    y = df[y_regular].copy()\r\n\r\n    for col in bins_2:\r\n        bins = np.linspace(X[col].min(),X[col].max(),bin_num_7)\r\n        bin_s = np.digitize(X[col],bins=bins)\r\n        X[f\"{col}_binned\"] = bin_s\r\n        X[f\"{col}_binned\"] = X[f\"{col}_binned\"].apply(lambda x:str(x))\r\n\r\n\r\n\r\n    X = pd.get_dummies(X)\r\n    X[\r\n        ['footlgth_binned_1', 'footlgth_binned_2', 'footlgth_binned_3',\r\n            'footlgth_binned_4', 'earconch_binned_1', 'earconch_binned_2',\r\n            'earconch_binned_3', 'earconch_binned_4']\r\n    ]\r\n\r\n    dummy = [\r\n        ['footlgth_binned_1', 'footlgth_binned_2', 'footlgth_binned_3',\r\n                        'footlgth_binned_4'], \r\n            ['earconch_binned_1', 'earconch_binned_2',\r\n                        'earconch_binned_3', 'earconch_binned_4']\r\n                        ]\r\n    \r\n    original = ['footlgth', 'earconch']\r\n    \r\n    \r\n    for o in range(0, len(original)):\r\n        for d in range(0,len(dummy[o])):\r\n            col_name = f\"{original[o]} * {dummy[o][d]}\"\r\n            X[col_name] = (X[original[o]] * X[dummy[o][d]])\r\n    X.drop(bins_2,axis=1, inplace=True)\r\n    X_train, X_test, y_train, y_test = train_test_split(X, y, \r\n    test_size=0.30, random_state=42)\r\n    lasso = Lasso(max_iter=1000)\r\n    lasso.fit(X_train, y_train)\r\n    y_pred_test = lasso.predict(X_test)\r\n\r\n    \r\n    print(f\"At bin = {bin_num_7}\")\r\n    print(f'RMSE test: {np.around(np.sqrt(mean_squared_error(y_test, y_pred_test)),3)}')\r\n    print(f'Standard Deviation of Age: {np.around(df.age.std(),3)}')\r\n    \r\n\r\nlasso_bin_7_product(bin_num_7) # bin 7","31d098a2":"X","c0eb56ba":"from sklearn.preprocessing import  PolynomialFeatures\r\ndegrees = [2,3,4,5,6,7,8,9,10]\r\n\r\nbin_num_7 = 7\r\n\r\nbins_2 = ['footlgth', 'earconch']\r\n\r\nto_transform = ['hdlngth',\r\n                'skullw',\r\n                'totlngth',\r\n                'taill',\r\n                'eye',\r\n                'chest',\r\n                'belly']\r\n\r\nadd_back = ['site','Pop','sex','footlgth_binned','earconch_binned']\r\n\r\ndef linearregression_poly(degrees,to_transform,bin_num_7):\r\n    X = df[X_regular].copy()\r\n    y = df[y_regular].copy()\r\n\r\n    for col in bins_2:\r\n        bins = np.linspace(X[col].min(),X[col].max(),bin_num_7)\r\n        bin_s = np.digitize(X[col],bins=bins)\r\n        X[f\"{col}_binned\"] = bin_s\r\n        X[f\"{col}_binned\"] = X[f\"{col}_binned\"].apply(lambda x:str(x))\r\n\r\n       \r\n    # polynomial interactions\r\n    X_preprocessed = X.copy()\r\n    for d in degrees:\r\n        poly = PolynomialFeatures(degree=d, include_bias=False)\r\n        poly.fit(X_preprocessed[to_transform])\r\n\r\n        X_poly = pd.DataFrame(poly.transform(X_preprocessed[to_transform])\r\n        , columns=poly.get_feature_names(X_preprocessed[to_transform].columns))\r\n\r\n        X_poly = pd.get_dummies(X_poly)\r\n        X_train, X_test, y_train, y_test = train_test_split(X_poly, y, \r\n        test_size=0.30, random_state=42)\r\n        lr = LinearRegression()\r\n        lr.fit(X_train,y_train)\r\n        y_pred = lr.predict(X_test)\r\n        \r\n        print(f\"At bin = {bin_num_7}, degree {d}, \\nRMSE: {np.around(np.sqrt(mean_squared_error(y_test,y_pred)),3)} \")\r\n        print(f\"Standard Deviation of Age: {np.around(df.age.std(),3)}\\n\")\r\n\r\nlinearregression_poly(degrees,to_transform,bin_num_7)\r\n","e98e2a8b":"from sklearn.preprocessing import  PolynomialFeatures\r\ndegrees = [2,3,4,5,6,7,8,9,10]\r\n\r\nbin_num_7 = 7\r\n\r\nbins_2 = ['footlgth', 'earconch']\r\n\r\nto_transform = ['hdlngth',\r\n                'skullw',\r\n                'totlngth',\r\n                'taill',\r\n                'eye',\r\n                'chest',\r\n                'belly']\r\n\r\nadd_back = ['site','Pop','sex','footlgth_binned','earconch_binned']\r\n\r\ndef ridge_poly(degrees,to_transform,bin_num_7):\r\n    X = df[X_regular].copy()\r\n    y = df[y_regular].copy()\r\n\r\n    for col in bins_2:\r\n        bins = np.linspace(X[col].min(),X[col].max(),bin_num_7)\r\n        bin_s = np.digitize(X[col],bins=bins)\r\n        X[f\"{col}_binned\"] = bin_s\r\n        X[f\"{col}_binned\"] = X[f\"{col}_binned\"].apply(lambda x:str(x))\r\n\r\n       \r\n    \r\n    # polynomial interactions\r\n    X_preprocessed = X.copy()\r\n    for d in degrees:\r\n        poly = PolynomialFeatures(degree=d, include_bias=False)\r\n        poly.fit(X_preprocessed[to_transform])\r\n\r\n        X_poly = pd.DataFrame(poly.transform(X_preprocessed[to_transform])\r\n        , columns=poly.get_feature_names(X_preprocessed[to_transform].columns))\r\n\r\n        X_poly = pd.get_dummies(X_poly)\r\n        X_train, X_test, y_train, y_test = train_test_split(X_poly, y, \r\n        test_size=0.30, random_state=42)\r\n        ridge = Ridge()\r\n        ridge.fit(X_train,y_train)\r\n        y_pred = ridge.predict(X_test)\r\n        \r\n        print(f\"At bin = {bin_num_7}, degree {d}, \\nRMSE: {np.around(np.sqrt(mean_squared_error(y_test,y_pred)),3)} \")\r\n        print(f\"Standard Deviation of Age: {np.around(df.age.std(),3)}\\n\")\r\n\r\nridge_poly(degrees,to_transform,bin_num_7)\r\n","ebc6b85b":"from sklearn.preprocessing import  PolynomialFeatures\r\ndegrees = [2,3,4,5,6,7,8,9,10]\r\n\r\nbin_num_7 = 7\r\n\r\nbins_2 = ['footlgth', 'earconch']\r\n\r\nto_transform = ['hdlngth',\r\n                'skullw',\r\n                'totlngth',\r\n                'taill',\r\n                'eye',\r\n                'chest',\r\n                'belly']\r\n\r\nadd_back = ['site','Pop','sex','footlgth_binned','earconch_binned']\r\n\r\ndef lasso_poly(degrees,to_transform,bin_num_7):\r\n    X = df[X_regular].copy()\r\n    y = df[y_regular].copy()\r\n\r\n    for col in bins_2:\r\n        bins = np.linspace(X[col].min(),X[col].max(),bin_num_7)\r\n        bin_s = np.digitize(X[col],bins=bins)\r\n        X[f\"{col}_binned\"] = bin_s\r\n        X[f\"{col}_binned\"] = X[f\"{col}_binned\"].apply(lambda x:str(x))\r\n   \r\n    \r\n    # polynomial interactions\r\n    X_preprocessed = X.copy()\r\n    for d in degrees:\r\n        poly = PolynomialFeatures(degree=d, include_bias=False)\r\n        poly.fit(X_preprocessed[to_transform])\r\n\r\n        X_poly = pd.DataFrame(poly.transform(X_preprocessed[to_transform])\r\n        , columns=poly.get_feature_names(X_preprocessed[to_transform].columns))\r\n\r\n        X_poly = pd.get_dummies(X_poly)\r\n        X_train, X_test, y_train, y_test = train_test_split(X_poly, y, \r\n        test_size=0.30, random_state=42)\r\n        lasso = Lasso()\r\n        lasso.fit(X_train,y_train)\r\n        y_pred = lasso.predict(X_test)\r\n        \r\n        print(f\"At bin = {bin_num_7}, degree {d}, \\nRMSE: {np.around(np.sqrt(mean_squared_error(y_test,y_pred)),3)} \")\r\n        print(f\"Standard Deviation of Age: {np.around(df.age.std(),3)}\\n\")\r\n\r\nlasso_poly(degrees,to_transform,bin_num_7)\r\n","7b5d91ed":"### Observations\r\n1. Lasso regression's RMSE value remains constant; 1.837 \r\n2. Linear regression's performance is extremely poor, with an RMSE value of 2.122\r\n3. Ridge regression performs worsens with an RMSE value of 1.894","57b2544b":"There are two null values in age and one in footlgth","efeefc92":"### Observation\r\n1. Lasso regression at degree 3, bin 7, has the best RMSE value; 1.765","06a22c97":"## Binning and interactions \r\n\r\nIn the above, we dropped the original data when we binned their columns\r\n\r\nHowever, we can include the original data back in after getting dummies\r\n\r\nWithout adding back, we predict a value for each bin. However, we may also want to capture the slope for each bin\r\n\r\nBesides adding back, we can compute a product of the bin dummies and the original data so we can capture a unique slope for each of the bin","7c639f1c":"Linear Regression has a RMSE values of 1.909 while standard deviation has a value of 1.915. Not much improvement in performance","1cbc7ff7":"## Polynomial interactions","e2e11eae":"## Observations\r\n1. Based on sex, females have one outlier and its median is more than that of males.\r\n2. Vic has more variations in age.\r\n3. Sites 2,4,6,7 all have outliers, furthermore site 6 has two outliers. Moreover, sites 1 and 5 have no outliers.\r\n4. Site 5 has the highest median ","26cccea6":"### Observations\r\nLinear Regression has the best perfomance when using a log configuration with an RMSE value of 0.331","c2ab2b9f":"age is the target variable and it has a reasonable good correlation with hdlngth, chest and belly, with values 0.33, 0.34 and 0.36 respectively","d50cf68b":"Now we explore other versions of interactions log, and mixture","bb32f99c":"## Applying Regression\r\n\r\nWe shall be using three methods based on the book Introduction to Machine Learning Using Python\r\n1. Using log, no log, mixture of log and no log\r\n2. Binning and discretization\r\n3. Polynomial features\r\n\r\nAlso, we shall be making use of Linear Regression, Ridge and Lasso","cb4424cf":"### Use of EstimatorSelectionHelper Class ","7ca48458":"### Observation \r\nRidge regression has a reasonable performance when using regular configuration of the dataset with an RMSE value of 1.826","4a58483e":"## Data Mixture 1","1ee801d9":"No duplicates detected.","6d0789c9":"## Product of bin dummies with original data","ff516060":"RMSE value is 1.274","6afefeba":"1. hdlngth, totlngth and eye has negative values for skewness with values -0.05,0.25 and 0.06 respectively.\r\n2. skullw has the highest level of skewness","31935752":"### Log-transform findings\r\n\r\n\r\n'hdlngth', -0.05 to -0.22 \r\n'skullw', 1.03 to 0.78 improves\r\n'totlngth', -0.25 to -0.38\r\n'taill', 0.12 to -0.06 improves\r\n'footlgth', 0.12 to 0.05 improves\r\n'earconch', 0.22 to 0.15 improves\r\n'eye', 0.38 to 0.2 improves \r\n'chest', -0.06 to -0.25\r\n'belly' 0.11 to 0.17","7531ddee":"RMSE value considerably low with a value of 0.331","6aa59cff":"### Observations\r\n1. Lasso regression's RMSE value remains relatively constant; 1.832\r\n2. Linear regression's performance is still extremely poor, with an RMSE value of 2.114\r\n3. Ridge regression's performance improves with an RMSE value of 1.794","4e6e118a":"RMSE value is 1.259 for ridge regression","51f1e9e2":"## Performing EDA","88f641ca":"### Observation\r\nLasso regression has the best performance amongst other regression  algortihms when using data mixture 1 configuration","55e947eb":"### Use of Binning \r\n\r\nWe explore the use of footlgth and earconch for binning due to the fact that both distributions has 2 bell-like curves joined together.","e2c8055e":"### Use of log data","519f50ee":"## Checking Skewness\r\nLevels of skewness\r\n1. (-0.5,0.5) = lowly skewed\r\n2. (-1,0-0.5) U (0.5,1) = Moderately skewed\r\n3. (-1 & beyond ) U (1 & beyond) = Highly skewed","30a102ed":"### Observation\r\n1. Data mixture 1 is a better choice for this dataset \r\n2. Performance for all regression algorithms in data mixture 2 are terrible compared to that of data mixture 1","083a19b2":"Ridge regression shows minor improvements in RMSE value than that of linear regression","25e21fbd":"## Conclusion\r\nIn this analysis, different methods of evaluation were used with on this dataset. \r\n1. Standard deviation is 1.915.\r\n2. For the regular dataset, Ridge regression showed a reasonable result, having a RMSE value of 1.826.\r\n3. For the log dataset, Linear regression showed the best result having a a RMSE value of 0.331.\r\n4. For data mixture 1, all regression algorithms showed great improvements, but lasso regression was the best out of them.\r\n5. For data mixture 2, the regression algorithms performed poorly compared to that of data mixture 1.\r\n6. Use of binning gave minor improvements , with ridge regression having a RMSE value of 1.729 at bin 7.\r\n7. Use of binning with interactions worsen all algorithms except lasso regression that remained constant.\r\n8. The product of bin dummy set and original data, did not show major improvements, but also worsened ridge regression with a RMSE value of 1.794.\r\n9. Polynomials interactions at degree, bin 7 with lasso regression showed a RMSE value of 1.765.\r\n\r\nInjecting some form of non-linearity is required, due to the fact that regression assumes a linear relationship between predictors and the response. The use of a non-linear transformer in turn improves the performance of linear regression. ","7c88fefc":"## Data Mixture 2","533b4c25":"### Observation\r\n1. Ridge regression's RMSE value is 1.729 at bin 7\r\n2. Lasso regression showed constant RMSE from bins 2-15, with an RMSE value of 1.837","50cde5af":"## The Possum Regression Dataset"}}