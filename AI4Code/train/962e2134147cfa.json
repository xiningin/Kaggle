{"cell_type":{"81c7c140":"code","fc4fb710":"code","26affa8a":"code","2ef28ad8":"code","cb0a945e":"code","ed8b8fd6":"code","e3c0df48":"code","fb603615":"code","a0493d13":"code","9470a641":"code","df399ee2":"code","f3c6ef6e":"code","eedc313b":"code","2316656b":"code","52865e1e":"code","71e197c7":"code","e6804e0b":"code","752ad4ca":"code","cb3e914e":"code","966f6c4f":"code","c68a5285":"code","328859da":"code","23510f6c":"code","e646bfc9":"code","02fc9392":"code","16d7e6e4":"code","d7738f61":"code","2c300fa9":"code","b59d2c4e":"code","a55c9a28":"markdown","037d36f4":"markdown","1d2e4fcb":"markdown","0f410b62":"markdown","7f8e6a96":"markdown","f803c3fd":"markdown","4b30440f":"markdown","f7b1ff75":"markdown","38185843":"markdown","1452339c":"markdown","57199b97":"markdown","8bc1cfd3":"markdown","e85f92d3":"markdown","6e81767d":"markdown","8681a95d":"markdown","96e54173":"markdown","e063a8fb":"markdown","8a8c9400":"markdown"},"source":{"81c7c140":"import numpy as np # linear algebra\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport sklearn.linear_model as skl_lm\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.model_selection import train_test_split, LeaveOneOut, KFold, cross_val_score\nfrom sklearn.preprocessing import PolynomialFeatures\n\nfrom sklearn.preprocessing import scale\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\n%matplotlib inline\nplt.style.use('seaborn-white')","fc4fb710":"cav = pd.read_csv(\"..\/input\/CAvideos.csv\")\ndev = pd.read_csv(\"..\/input\/DEvideos.csv\")\nfrv = pd.read_csv(\"..\/input\/FRvideos.csv\")\ngbv = pd.read_csv(\"..\/input\/GBvideos.csv\")\nusv = pd.read_csv(\"..\/input\/USvideos.csv\")","26affa8a":"cav.head()","2ef28ad8":"frv.info()","cb0a945e":"usv.head()\n","ed8b8fd6":"usv.info()","e3c0df48":"usv.describe()","fb603615":"esVLD = smf.ols('views ~ likes + dislikes', usv).fit()\nesVLD.summary()","a0493d13":"\nsns.jointplot(x='views', y='likes', \n              data=usv, color ='red', kind ='reg', \n              size = 8.0)\nplt.show()","9470a641":"import json\n\n\nusv['category_id'] = usv['category_id'].astype(str)\n# usv_cat_name['category_id'] = usv['category_id'].astype(str)\n\ncategory_id = {}\n\nwith open('..\/input\/US_category_id.json', 'r') as f:\n    data = json.load(f)\n    for category in data['items']:\n        category_id[category['id']] = category['snippet']['title']\n\nusv.insert(4, 'category', usv['category_id'].map(category_id))\n# usv_cat_name.insert(4, 'category', usv_cat_name['category_id'].map(category_id))\ncategory_list = usv['category'].unique()\ncategory_list","df399ee2":"# labels = usv.groupby(['category_id']).count().index\nlabels = category_list\ntrends  = usv.groupby(['category_id']).count()['title']\nexplode = (0, 0, 0.2, 0, 0, 0, 0, 0, 0, 0, 0,0, 0, 0, 0 ,0)  # only \"explode\" the 2nd slice (i.e. 'Hogs')\n\nfig, ax = plt.subplots(figsize=(10,10))\nax.pie(trends, labels=labels, autopct='%1.1f%%',explode = explode,\n        shadow=False, startangle=180)\nax.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\ntrends\n","f3c6ef6e":"plt.style.use('ggplot')\nplt.figure(figsize=(20,10))\n\nphour=usv.groupby(\"category\").count()[\"comment_count\"].plot.bar()\nphour.set_xticklabels(phour.get_xticklabels(),rotation=45)\nplt.title(\"Comment count vs category of Videos\")\nsns.set_context()","eedc313b":"import glob\nfiles = [file for file in glob.glob('..\/input\/*.{}'.format('csv'))]\nsorted(files)\nycd_initial = list()\nfor csv in files:\n    ycd_partial = pd.read_csv(csv)\n    ycd_partial['country'] = csv[9:11] #Adding the new column as \"country\"\n    ycd_initial.append(ycd_partial)\n\nycd = pd.concat(ycd_initial)\nycd.info()","2316656b":"ycd.head()","52865e1e":"ycd.apply(lambda x: sum(x.isnull()))","71e197c7":"column_list=[] \n# Exclude Description column of description because many YouTubers don't include anything in description. This is important to not accidentally delete those values. This for loop will display existing columns in given dataset.\nfor column in ycd.columns:\n    if column not in [\"description\"]:\n        column_list.append(column)\nprint(column_list)","e6804e0b":"ycd.dropna(subset=column_list, inplace=True) \n# Drop NA values","752ad4ca":"ycd.head()","cb3e914e":"# Feature engineering\n\n#Adjusting Date and Time format in right way\nycd[\"trending_date\"]=pd.to_datetime(ycd[\"trending_date\"],errors='coerce',format=\"%y.%d.%m\")\nycd[\"publish_time\"]=pd.to_datetime(ycd[\"publish_time\"],errors='coerce')\n#Create some New columns which will help us to dig more into this data.\nycd[\"T_Year\"]=ycd[\"trending_date\"].apply(lambda time:time.year).astype(int)\nycd[\"T_Month\"]=ycd[\"trending_date\"].apply(lambda time:time.month).astype(int)\nycd[\"T_Day\"]=ycd[\"trending_date\"].apply(lambda time:time.day).astype(int)\nycd[\"T_Day_in_week\"]=ycd[\"trending_date\"].apply(lambda time:time.dayofweek).astype(int)\nycd[\"P_Year\"]=ycd[\"publish_time\"].apply(lambda time:time.year).astype(int)\nycd[\"P_Month\"]=ycd[\"publish_time\"].apply(lambda time:time.month).astype(int)\nycd[\"P_Day\"]=ycd[\"publish_time\"].apply(lambda time:time.day).astype(int)\nycd[\"P_Day_in_Week\"]=ycd[\"publish_time\"].apply(lambda time:time.dayofweek).astype(int)\nycd[\"P_Hour\"]=ycd[\"publish_time\"].apply(lambda time:time.hour).astype(int)\n","966f6c4f":"plt.figure(figsize = (15,10))\nycd.describe()\nsns.heatmap(ycd[[\"views\", \"likes\",\"dislikes\",\"comment_count\"]].corr(), annot=True)\nplt.show()","c68a5285":"category_from_json={}\nwith open(\"..\/input\/US_category_id.json\",\"r\") as file:\n    data=json.load(file)\n    for category in data[\"items\"]:\n        category_from_json[category[\"id\"]]=category[\"snippet\"][\"title\"]\n        \n        \nlist1=[\"views likes dislikes comment_count\".split()] \nfor column in list1:\n    ycd[column]=ycd[column].astype(int)\n#Similarly Convert The Category_id into String,because later we're going to map it with data extracted from json file    \nlist2=[\"category_id\"] \nfor column in list2:\n    ycd[column]=ycd[column].astype(str)\n","328859da":"\nfrom collections import OrderedDict\n\nycd[\"Category\"]=ycd[\"category_id\"].map(category_from_json)\n\nycd.groupby([\"Category\",\"country\"]).count()[\"video_id\"].unstack().plot.barh(figsize=(20,10), stacked=True, cmap = \"inferno\")\nplt.yticks(rotation=0, fontsize=20) \nplt.xticks(rotation=0, fontsize=20) \nplt.title(\"Category analysis with respect to countries\", fontsize=20)\nplt.legend(handlelength=5, fontsize  = 10)\nplt.show()","23510f6c":"def trend_plot(country):\n    ycd[ycd[\"country\"] == country][[\"video_id\", \"trending_date\"]].groupby('video_id').count().sort_values\\\n    (by=\"trending_date\",ascending=False).plot.kde(figsize=(15,10), cmap = \"rainbow\")\n    plt.yticks(fontsize=18) \n    plt.xticks(fontsize=15) \n    plt.title(\"\\nYouTube trend in \"+ country +\"\\n\", fontsize=25)\n    plt.legend(handlelength=2, fontsize  = 20)\n    plt.show()\n#country_list = df.groupby(['country']).count().index\ncountry_list = [\"FR\", \"CA\", \"GB\",\"US\",\"DE\"]\nfor country in country_list:\n    trend_plot(country)","e646bfc9":"from wordcloud import WordCloud\nimport nltk\n#nltk.download(\"all\")\nfrom nltk.sentiment import SentimentIntensityAnalyzer\nfrom nltk.corpus import stopwords\nfrom nltk import sent_tokenize, word_tokenize\nfrom wordcloud import WordCloud, STOPWORDS\nfrom collections import Counter\nfrom nltk.tokenize import RegexpTokenizer\nimport re","02fc9392":"def get_cleaned_data(tag_words):\n    #Removes punctuation,numbers and returns list of words\n    cleaned_data_set=[]\n    cleaned_tag_words = re.sub('[^A-Za-z]+', ' ', tag_words)\n    word_tokens = word_tokenize(cleaned_tag_words)\n    filtered_sentence = [w for w in word_tokens if not w in en_stopwords]\n    without_single_chr = [word for word in filtered_sentence if len(word) > 2]\n    cleaned_data_set = [word for word in without_single_chr if not word.isdigit()]  \n    return cleaned_data_set\nMAX_N = 1000\n#Collect all the related stopwords.\nen_stopwords = nltk.corpus.stopwords.words('english')\nde_stopwords = nltk.corpus.stopwords.words('german')\nfr_stopwords = nltk.corpus.stopwords.words('french')   \nen_stopwords.extend(de_stopwords)\nen_stopwords.extend(fr_stopwords)\n","16d7e6e4":"def word_cloud(category):\n    tag_words = ycd[ycd['Category']== category]['tags'].str.lower().str.cat(sep=' ')\n    temp_cleaned_data_set = get_cleaned_data(tag_words) #get_cleaned_data() defined above.\n    \n    #Lets plot the word cloud.\n    plt.figure(figsize = (20,15))\n    cloud = WordCloud(background_color = \"white\", max_words = 200,  max_font_size = 30)\n    cloud.generate(' '.join(temp_cleaned_data_set))\n    plt.imshow(cloud)\n    plt.axis('off')\n    plt.title(\"\\nWord cloud for \" + category + \"\\n\", fontsize=40)","d7738f61":"category_list = [\"Music\", \"Entertainment\",\"News & Politics\"]\nfor category in category_list:\n    word_cloud(category)","2c300fa9":"def best_publish_time(list, title):\n    plt.style.use('ggplot')\n    plt.figure(figsize=(16,8))\n    #list3=df1.groupby(\"Publish_Hour\").count()[\"Category\"].plot.bar()\n    list_temp = list.plot.bar()\n    #list3.set_xticklabels(list3.get_xticklabels(),rotation=30, fontsize=15)\n    list_temp.set_xticklabels(list_temp.get_xticklabels(),rotation=0, fontsize=15)\n    plt.title(title, fontsize=25)\n    plt.xlabel(s=\"Best Publishing hour\", fontsize=20)\n    sns.set_context(font_scale=1)","b59d2c4e":"list = ycd[ycd['country'] == 'US'].groupby(\"P_Hour\").count()[\"Category\"]\ntitle = \"\\nBest Publish Time for USA\\n\"\nbest_publish_time(list, title)","a55c9a28":"It Looks like from 2 PM to 6 PM seems to be popular time for uploads.","037d36f4":"# EDA on Youtube data\n**The place where cats are celebrieties** Let's check out human psyche through trends in 5 countries. I will focus on US trends\n![](https:\/\/cdn.wccftech.com\/wp-content\/uploads\/2017\/08\/Screen-Shot-2017-08-30-at-12.37.20-AM.png)","1d2e4fcb":"We can see that entertainment and music receives more comments compared to any other category. This means these categories are important enough to people that they invest their time. It also means people have high sense of opinion towards these aspects.","0f410b62":"Summary statistics are useful in every model design in machine learning. Here t-statistics shows departure of standard value from hypothetical value. P-value from given table is equal to zero so we can reject the null hypothesis.\nPositive skewness shows that the distribution is right skewed.\n![![image.png]](attachment:image.png)","7f8e6a96":"**Please upvote if this Kernel was helpful. Also check out some of my other projects. Have a nice Kaggleing ! : ) ** \n\n","f803c3fd":"### Generating wordcloud","4b30440f":"\nsns.boxplot('views','likes', data=usv)\nplt.show()","f7b1ff75":"### We will find out the relationship between views with respect to comments and likes.\nAssuming comments and likes have equal effect, I will create a combined summary with respct to views","38185843":"**Feature Engineering for keeping clean values and removing null data**","1452339c":"### Category analysis with respect to countries. \nHere we will figure out which countries watch which categories.\n","57199b97":"### What can we deduce ?\nAs far as US population is concerned, entertainment and education are most watched categories. Auto and vehicles consist of 10 % of trends. Music and Pets have almost equal distribution (8% approx). Frankly I expected more traffic towards comedy views. But we also have to keep in mind that these cataegories are classified using tags. So it is entirely possible that some comdey videos are classified incorrectly. The sports videos in general have less traffic but this can change when there's NBA or superbowl season.","8bc1cfd3":"### Heatmap showing correlation between numerical variables.\nData scientists should always keep in mind that correlation alone is not perfect metric. We should also check for causality and independence between given attributes. Generally manipulating dependent variables without knowing it's repercussions can cause massive shift while building up predictive models. Hence as a thumb rule we check summary statistics.","e85f92d3":"## Combined Analysis of 5 Countries","6e81767d":"### Checking which catergory has more video comments","8681a95d":"mostv_ent = usv[usv['category_id']=='Entertainment'][['title','category_id','views']].sort_values(by='views', ascending=False)\n\nmostv_ent = mostv_ent.groupby('title')['views'].mean().sort_values(ascending=False).head(3)\n","96e54173":"\nplt.figure()\nsns.distplot(usv[\"comment_count\"], hist=False, rug=True);","e063a8fb":"**These wordclouds show the important words for these categories. Late Show hosts dominates entertainment searches while words like \"official\", \"Hip hop\" even \"punjabi songs\" are very popular in music industry**\nThe big words shows how frequent these searches are. According tom my prediction this data set was published weeks after black panther movie because even though it's not top trending in word cloud but it's still visible there.","8a8c9400":"### Best time to Publish videos in America"}}