{"cell_type":{"25de7301":"code","c0ea0c84":"code","12cfc001":"code","12444d24":"code","d81495fc":"code","667516d3":"code","75135cbb":"code","21d99b14":"code","6115a29a":"code","28fcfd17":"code","e9c3d755":"code","75a2c0d3":"code","419ec2d1":"code","b9834e52":"code","7ded4b9f":"code","2e0153d5":"code","4e22d7e7":"code","bd54c5bd":"code","e79f3771":"code","736ae990":"code","59ac4eb8":"code","875d90c1":"code","fa8c4f2e":"code","3c12dc0c":"code","236b4de2":"code","dafcbb63":"code","389ac5c4":"code","1b3df4c1":"code","b3dfb35c":"code","539af0a4":"code","be96f195":"code","275e4e99":"code","a2ffc85d":"code","9406b3ad":"code","67bc5bc9":"code","fb0bfad7":"code","8c7825fd":"code","999e3b8c":"markdown","a40da2bf":"markdown","5f3d809a":"markdown","bde936e8":"markdown","bda8f8a0":"markdown","714313de":"markdown","70174f91":"markdown","3b1cfe5e":"markdown","1c962bc9":"markdown","c62072d1":"markdown","d2b0bb79":"markdown","67529803":"markdown","24d03129":"markdown","a526b07b":"markdown","a6d9e775":"markdown","d1f8cd5c":"markdown","41652a2f":"markdown","f8dc8ada":"markdown","cf6f9388":"markdown","abb2986f":"markdown","c0c5119c":"markdown","7e7b7402":"markdown","c283290c":"markdown","d1d376d0":"markdown","0e1ddf4a":"markdown","db2b7ebc":"markdown","0ce452b8":"markdown","c68dfd14":"markdown","c828c032":"markdown","50326357":"markdown","518ecafd":"markdown","0aca15ca":"markdown"},"source":{"25de7301":"import pandas as pd\nimport numpy as np\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline \nimport plotly.express as px\nimport plotly.graph_objects as go\nplt.rcParams['figure.figsize'] = 20, 16\nplt.style.use(\"fivethirtyeight\")\npd.options.plotting.backend = \"plotly\"\n\n\n#Modelos\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\nfrom sklearn.metrics import accuracy_score,plot_confusion_matrix,classification_report","c0ea0c84":"data=pd.read_csv(\"..\/input\/breast-cancer-prediction-dataset\/Breast_cancer_data.csv\")","12cfc001":"data.head()","12444d24":"data.info()","d81495fc":"#Vamos a observar alguna caracter\u00edstica de los datos\ndata.describe()","667516d3":"for columna in data.drop([\"diagnosis\"],axis=1).columns.values.tolist():\n    g= sns.FacetGrid(data, col=\"diagnosis\")\n    g.map(plt.hist, columna ,bins=20)","75135cbb":"fig = px.scatter_matrix(data[[\"mean_area\",\"mean_perimeter\",\"mean_radius\"]])\nfig.update_layout(title=\"Matriz de dispersi\u00f3n (Scatter Matrix) para las columnas\")\nfig.show()","21d99b14":"data2=data.drop([\"mean_smoothness\",\"mean_texture\",\"diagnosis\"],axis=1)","6115a29a":"plt.figure(figsize=(10,5))\nsns.heatmap(data2.corr(), annot=True, cmap=\"coolwarm\")","28fcfd17":"data.head()","e9c3d755":"data3=data.drop([\"mean_perimeter\",\"mean_area\"],axis=1)\ndata3.head()","75a2c0d3":"#La variable radios la partimos en 5 y ya ser\u00eda suficiente\ndata3['radios'] = pd.cut(data3['mean_radius'], 5)\n#La variable texture quedan los rangos mejor definidos si la partimos en 7\ndata3['texture'] = pd.cut(data3['mean_texture'], 7)\n# Smoothness tambi\u00e9n la cortamos en 5 partes\ndata3['smooth'] = pd.cut(data3['mean_smoothness'], 5)\n","419ec2d1":"data3[[\"radios\",\"diagnosis\"]].groupby(['radios'], as_index=False).mean().sort_values(by='radios', ascending=True)","b9834e52":"data3.loc[ data3['mean_radius'] <= 11.207, 'mean_radius'] = 0\ndata3.loc[(data3['mean_radius'] > 11.027) & (data3['mean_radius'] <= 15.433), 'mean_radius'] = 1\ndata3.loc[(data3['mean_radius'] > 15.433) & (data3['mean_radius'] <= 19.658), 'mean_radius'] = 2\ndata3.loc[(data3['mean_radius'] > 19.658) & (data3['mean_radius'] <= 23.884), 'mean_radius'] = 3\ndata3.loc[ data3['mean_radius'] > 23.884, 'mean_radius']=4","7ded4b9f":"#Eliminamos ya la columna radios\ndata3=data3.drop([\"radios\"],axis=1)\ndata3.head()","2e0153d5":"data3[[\"texture\",\"diagnosis\"]].groupby(['texture'], as_index=False).mean().sort_values(by='texture', ascending=True)","4e22d7e7":"data3.loc[ data3['mean_texture'] <= 13.934, 'mean_texture'] = 0\ndata3.loc[(data3['mean_texture'] > 13.934) & (data3['mean_texture'] <= 18.159), 'mean_texture'] = 1\ndata3.loc[(data3['mean_texture'] > 18.159) & (data3['mean_texture'] <= 22.383), 'mean_texture'] = 2\ndata3.loc[(data3['mean_texture'] > 22.383) & (data3['mean_texture'] <= 26.607), 'mean_texture'] = 3\ndata3.loc[(data3['mean_texture'] >26.607) & (data3['mean_texture'] <= 30.831), 'mean_texture'] = 4\ndata3.loc[(data3['mean_texture'] > 30.831) & (data3['mean_texture'] <= 35.056), 'mean_texture'] = 5\ndata3.loc[(data3['mean_texture'] > 35.056), 'mean_radius'] = 6","bd54c5bd":"#Eliminamos ya la columna texture\ndata3=data3.drop([\"texture\"],axis=1)\ndata3.head()","e79f3771":"data3[[\"smooth\",\"diagnosis\"]].groupby(['smooth'], as_index=False).mean().sort_values(by='smooth', ascending=True)","736ae990":"data3.loc[ data3['mean_smoothness'] <= 0.0748, 'mean_smoothness'] = 0\ndata3.loc[(data3['mean_smoothness'] > 0.0748) & (data3['mean_smoothness'] <= 0.0969), 'mean_smoothness'] = 1\ndata3.loc[(data3['mean_smoothness'] > 0.0969) & (data3['mean_smoothness'] <= 0.119), 'mean_smoothness'] = 2\ndata3.loc[(data3['mean_smoothness'] > 0.119) & (data3['mean_smoothness'] <= 0.141), 'mean_smoothness'] = 3\ndata3.loc[ data3['mean_smoothness'] > 0.141, 'mean_smoothness']=4","59ac4eb8":"#Eliminamos ya la columna smooth\ndata3=data3.drop([\"smooth\"],axis=1)\ndata3.head()","875d90c1":"#En primer lugar vamos a separar entre entrenamiento y test.\nX_train, X_test, y_train, y_test = train_test_split(data3.drop([\"diagnosis\"],axis=1),data3[\"diagnosis\"] , \n                                                    test_size = 0.2, random_state=20)\n","fa8c4f2e":"X_train.head()","3c12dc0c":"model = LogisticRegression()\nmodel.fit(X_train,y_train)\nprediction_logistic = model.predict(X_test)\nacc_log = round(accuracy_score(prediction_logistic,y_test) * 100, 2)\nprint('La precisi\u00f3n (accuracy) de la regresi\u00f3n logistica es {}%'.format(acc_log))","236b4de2":"model = DecisionTreeClassifier()\nmodel.fit(X_train, y_train)\nprediction_tree = model.predict(X_test)\nacc_tree = round(accuracy_score(prediction_tree,y_test) * 100, 2)\nprint('La precisi\u00f3n (accuracy) del decision tree clasiffier es {}%'.format(acc_tree))","dafcbb63":"model=SVC()\nmodel.fit(X_train,y_train)\nprediction_svc=model.predict(X_test)\nacc_svc=round(accuracy_score(prediction_svc,y_test) * 100, 2)\nprint('La precisi\u00f3n (accuracy) del SVC es {}%'.format(acc_svc))","389ac5c4":"#Para obtener el n\u00famero de vecinos, debemos ejecutar un peque\u00f1o c\u00f3digo\nk_range = range(1, 20)\nscores = []\nfor k in k_range:\n    knn = KNeighborsClassifier(n_neighbors = k)\n    knn.fit(X_train, y_train)\n    scores.append(knn.score(X_test, y_test))\nplt.figure(figsize=(10,5))\nplt.xlabel('k')\nplt.ylabel('accuracy')\nplt.scatter(k_range, scores)\nplt.xticks([0,5,10,15,20])","1b3df4c1":"model=KNeighborsClassifier(n_neighbors=8)\nmodel.fit(X_train,y_train)\nprediction_knn=model.predict(X_test)\nacc_knn = round(accuracy_score(prediction_knn,y_test) * 100, 2)\nprint('La precisi\u00f3n (accuracy) del decision knn es {}%'.format(acc_knn))","b3dfb35c":"model=RandomForestClassifier(n_estimators=100)\nmodel.fit(X_train,y_train)\nprediction_rf=model.predict(X_test)\nacc_rf = round(accuracy_score(prediction_rf,y_test)*100,2)\nprint('La precisi\u00f3n (accuracy) del Random Forest es {}%'.format(acc_rf))","539af0a4":"model=GaussianNB()\nmodel.fit(X_train,y_train)\nprediction_gnb=model.predict(X_test)\nacc_gnb=round(accuracy_score(prediction_gnb,y_test) * 100, 2)\nprint('La precisi\u00f3n (accuracy) de Gaussian Naive Bayes es {}%'.format(acc_gnb))","be96f195":"models = pd.DataFrame({\n    'Model': ['Logistic Regression', 'SVC', 'Decision Trees',\"Random Forest\",\"GaussianNB\",\"KNN\"],\n    'Score': [acc_log, acc_svc, acc_tree,acc_rf,acc_gnb,acc_knn]})\nmodels.sort_values(by='Score', ascending=False)","275e4e99":"model=RandomForestClassifier(n_estimators=100)\nmodel.fit(X_train,y_train)\nprediction_rf=model.predict(X_test)\nacc_rf = round(accuracy_score(prediction_rf,y_test)*100,2)\nprint('La precisi\u00f3n (accuracy) del Random Forest es {}%'.format(acc_rf))","a2ffc85d":"plt.rcParams['figure.figsize'] = 8, 5\nplot_confusion_matrix(model, X_test,y_test)\n\nplt.title('Random Forest Confusion Matrix')\nplt.show()","9406b3ad":"# Comprobaci\u00f3n (f1-score - accuracy)\nreport = classification_report(y_test, prediction_rf)\nprint(report)","67bc5bc9":"modelo=KNeighborsClassifier(n_neighbors=8)#Seleccionamos 8, porque ya vimos antes cual coger\nmodelo.fit(X_train,y_train)\nprediction_knn=modelo.predict(X_test)\nacc_knn = round(accuracy_score(prediction_knn,y_test) * 100, 2)\nprint('La precisi\u00f3n (accuracy) del decision knn es {}%'.format(acc_knn))","fb0bfad7":"plt.rcParams['figure.figsize'] = 8, 5\nplot_confusion_matrix(modelo, X_test,y_test)\n\nplt.title('KNN Confusion Matrix')\nplt.show()","8c7825fd":"report = classification_report(y_test, prediction_knn)\nprint(report)","999e3b8c":"#### Logistic Regression","a40da2bf":"### KNN(Evaluaci\u00f3n)","5f3d809a":"No obstante, hay una cosa que me llama la atenci\u00f3n, ya que como estamos trabajando con los n\u00facleos de las c\u00e9lulas y a\u00fan no sabiendo mucho acerca de biolog\u00eda pueden ser bastante circulares. Lo que quiero decir con esto es que seguramente, el radio est\u00e9 relacionado con el \u00e1rea y el per\u00edmetro, para ello vamos a realizar unas representaciones.","bde936e8":"#### Gaussian Naive Bayes","bda8f8a0":"Para ver ahora las 7 particiones, vamos a realizar un groupby","714313de":"#### Variable radio","70174f91":"Con una primera visualizaci\u00f3n, podemos ver que realmente todas nuestras columnas parecen tener relevancia. \n","3b1cfe5e":"## Define el problema y sus objetivos","1c962bc9":"# Breast Cancer Wisconsin","c62072d1":"## Modelo a escoger","d2b0bb79":"#### Variable smoothness","67529803":"### Variables num\u00e9ricas a variables categ\u00f3ricas, mediante la funci\u00f3n .cut y la funci\u00f3n .map","24d03129":"Vemos que hay una relaci\u00f3n lineal bastante definida pero para asegurarnos a\u00fan m\u00e1s vamos a realizar una matriz de correlaci\u00f3n para averiguar el r^2","a526b07b":"#### Decision Tree Classifier","a6d9e775":"Observamos que afortunadamente tenemos unos datos completos y todos n\u00famericos.","d1f8cd5c":"## Carga y explicaci\u00f3n del problema ","41652a2f":"El objetivo de nuestro estudio va a ser realizar un m\u00f3delo que seg\u00fan una serie de caracter\u00edsticas que han sido tomadas del nucleo celular sea capaz de predecir el diagn\u00f3stico simplemente analizando estas caracter\u00edsticas.","f8dc8ada":"Al comprobar la matriz de confusi\u00f3n y el reporte de clasificaci\u00f3n vemos que aciertan lo mismo y fallan en los mismos casos, luego ambos modelos son igual de precisos y cualquiera de ellos nos servir\u00eda ya que cuentan con una precisi\u00f3n del 92%","cf6f9388":"Vamos ahora a comprobar la cantidad de datos nulos o NAN que tenemos:","abb2986f":"Como observamos en la matriz de confusi\u00f3n, se han marcado solo 8 errores, 6 se maracaron como menignos que eran malignos y 2 como malignos que eran malignos. Si observamos el reporte de clasificaci\u00f3n pues vemos que tenemos para ambos casos un gran f1. Luego visto todo esto podemos decir que el random forest es un gran algoritmo de clasificaci\u00f3n para este problema.","c0c5119c":"## Exploraci\u00f3n de los datos","7e7b7402":"Exceptuando la columna de diagnosis, podemos ver que son todo variables numer\u00edcas cont\u00ednuas. Dado el caso, vamos a optar por correlacionar caracter\u00edsticas num\u00e9ricas mediante la visualizaci\u00f3n, para ver si acabamos haciendo categ\u00f3ricas algunas variables.","c283290c":"Como sospech\u00e1bamos se ajustan extremadamente, por lo que a la hora de realizar el modelo usaremos una sola columna de estas, ya que al ser tan dependientes  utilizar las 3 solo har\u00eda que hagamos trabajar m\u00e1s al ordenador de manera innecesaria.","d1d376d0":"#### Random Forest ","0e1ddf4a":"#### k Neighbors Classifier","db2b7ebc":"Tenemos el mismo accuracy con tres modelos, luego vamos a estudiar random forest y Knn, ya que decision tree es el caso simple de random forest.","0ce452b8":"Vamos a definir las columnas, cabe mencionar que estamos trabajando con los datos medios de las observaciones:\n* radius= radio observado\n* texture = desviaci\u00f3n est\u00e1ndar de la escala de grises\n* permiter = el per\u00edmetro que ocupa el n\u00facleo de la c\u00e9lula\n* area = \u00e1rea\n* smoothness= Podr\u00edamos traducirlo como la suavidad o textura\n* diagnosis = Es el diagn\u00f3stico que ser\u00e1 una variable catag\u00f3rico, tomando 0 como maligno y  como menigno 1","c68dfd14":"Como sabemos el resultado de nuestro an\u00e1lisis sabemos que se trata de aprendizaje supervisado y como el resultado solo puede tomar 2 valores 0 o 1 pues sabemos que se trata de clasificaci\u00f3n y tambi\u00e9n de regresi\u00f3n. Dicho esto, vamos a probar unos cuantos modelos de manera directa y cuando seleccionemos los mejores  observaremos las matrices de confusi\u00f3n.","c828c032":"Una vez ya hemos probado unos cuantos modelos, vamos a ordenarlos para quedarnos con los dos mejores.","50326357":"#### Variable texture","518ecafd":"#### Support Vector Machine","0aca15ca":"### Random Forest(Evaluaci\u00f3n del modelo)"}}