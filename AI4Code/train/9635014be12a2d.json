{"cell_type":{"1174a414":"code","7fb17d97":"code","b4a47d40":"code","584bb887":"code","2669bc75":"code","91500978":"code","db5c2b0c":"code","1fdb0ca4":"code","c792fc35":"code","27c124d7":"code","0ffc76a2":"code","c971e951":"code","5b4aade7":"code","f9eec7cf":"code","6e80268c":"code","e7e90675":"code","82b44541":"code","b23478e7":"code","b6bb9af7":"code","76b76791":"code","9918bd6c":"code","684b19d2":"code","2df9082a":"code","e4a82b53":"code","a874d3ae":"code","175b07cb":"code","6049e090":"code","ed07d43e":"code","5d7b380a":"code","032e603c":"code","7ba79b41":"code","7b244c65":"code","c951a842":"code","4cc1d238":"code","5991a7d7":"code","4085b2e1":"code","78190814":"code","d2429b1b":"markdown","2c3848eb":"markdown","77efb8fd":"markdown","d1afd79c":"markdown","b4f697b8":"markdown","08852c74":"markdown","865395ca":"markdown","497cae14":"markdown","8d32aa63":"markdown","dc252930":"markdown","8e7b86a0":"markdown","6e95eb55":"markdown","14435403":"markdown","def41666":"markdown","0e98d30d":"markdown","c8a9055f":"markdown","6ad9057c":"markdown","0e81c6d2":"markdown"},"source":{"1174a414":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7fb17d97":"!pip install impyute","b4a47d40":"from impyute.imputation.cs import mice\nfrom sklearn.preprocessing import OrdinalEncoder","584bb887":"df = pd.read_csv('..\/input\/ckdisease\/kidney_disease.csv')","2669bc75":"cols_names={\"bp\":\"blood_pressure\",\n          \"sg\":\"specific_gravity\",\n          \"al\":\"albumin\",\n          \"su\":\"sugar\",\n          \"rbc\":\"red_blood_cells\",\n          \"pc\":\"pus_cell\",\n          \"pcc\":\"pus_cell_clumps\",\n          \"ba\":\"bacteria\",\n          \"bgr\":\"blood_glucose_random\",\n          \"bu\":\"blood_urea\",\n          \"sc\":\"serum_creatinine\",\n          \"sod\":\"sodium\",\n          \"pot\":\"potassium\",\n          \"hemo\":\"haemoglobin\",\n          \"pcv\":\"packed_cell_volume\",\n          \"wc\":\"white_blood_cell_count\",\n          \"rc\":\"red_blood_cell_count\",\n          \"htn\":\"hypertension\",\n          \"dm\":\"diabetes_mellitus\",\n          \"cad\":\"coronary_artery_disease\",\n          \"appet\":\"appetite\",\n          \"pe\":\"pedal_edema\",\n          \"ane\":\"anemia\"}\n\ndf.rename(columns=cols_names, inplace=True)","91500978":"df['red_blood_cell_count'] = pd.to_numeric(df['red_blood_cell_count'], errors='coerce')\ndf['packed_cell_volume'] = pd.to_numeric(df['packed_cell_volume'], errors='coerce')\ndf['white_blood_cell_count'] = pd.to_numeric(df['white_blood_cell_count'], errors='coerce')","db5c2b0c":"df.drop([\"id\"],axis=1,inplace=True)","1fdb0ca4":"numerical_features = []\ncategorical_features = []\n\nfor i in df.drop('classification', axis=1).columns:\n    if df[i].nunique()>7:\n        numerical_features.append(i)\n    else:\n        categorical_features.append(i)","c792fc35":"#Replace incorrect values\ndf['diabetes_mellitus'] = df['diabetes_mellitus'].replace(to_replace = {'\\tno':'no','\\tyes':'yes',' yes':'yes'})\ndf['coronary_artery_disease'] = df['coronary_artery_disease'].replace(to_replace = '\\tno', value='no')\ndf['classification'] = df['classification'].replace(to_replace = 'ckd\\t', value = 'ckd')","27c124d7":"df.loc[:,categorical_features].isnull().sum().sort_values(ascending=False)","0ffc76a2":"df.loc[:,numerical_features].isnull().sum().sort_values(ascending=False)","c971e951":"to_encode = [feat for feat in categorical_features if df[feat].dtype=='object']","5b4aade7":"to_encode","f9eec7cf":"ode = OrdinalEncoder(dtype = int)","6e80268c":"def encode(data):\n    '''function to encode non-nan data and replace it in the original data'''\n    #retains only non-null values\n    nonulls = np.array(data.dropna())\n    #reshapes the data for encoding\n    impute_reshape = nonulls.reshape(-1,1)\n    #encode date\n    impute_ordinal = ode.fit_transform(impute_reshape)\n    #Assign back encoded values to non-null values\n    data.loc[data.notnull()] = np.squeeze(impute_ordinal)\n    return data\n\n#create a for loop to iterate through each column in the data\nfor columns in to_encode:\n    encode(df[columns])","e7e90675":"df.loc[:, categorical_features].head(10)","82b44541":"X = df.drop('classification', axis=1)","b23478e7":"X_train = X.loc[:300,]\nX_test = X.loc[300:,]","b6bb9af7":"# MICE requires float values\nX_train_numerical = X_train.loc[:,numerical_features].astype('float64')","76b76791":"# Passing the numpy arrays to mice\nX_train_numerical_imputed = mice(X_train_numerical.values)","9918bd6c":"X_train.loc[:,numerical_features].isna().sum().sort_values(ascending=False)","684b19d2":"X_train.loc[:,numerical_features] = X_train_numerical_imputed","2df9082a":"X_train.loc[:,numerical_features].isna().sum().sort_values(ascending=False)","e4a82b53":"from fancyimpute import KNN","a874d3ae":"imputer = KNN()","175b07cb":"X_train_imputed = pd.DataFrame(np.round(imputer.fit_transform(X_train)),columns = X_train.columns)","6049e090":"X_train_imputed.isnull().sum()","ed07d43e":"X_train_imputed.describe().T","5d7b380a":"from sklearn.preprocessing import MinMaxScaler","032e603c":"scaler = MinMaxScaler()\nscaler.fit(X_train_imputed)\nX_train_scaled = scaler.transform(X_train_imputed)","7ba79b41":"X_train_scaled = pd.DataFrame(data=X_train_scaled, columns = X_train.columns)","7b244c65":"X_train_scaled.describe()","c951a842":"# MICE requires float values\nX_test_numerical = X_test.loc[:,numerical_features].astype('float64')","4cc1d238":"X_test_numerical_imputed = mice(X_test_numerical.values)\nX_test.loc[:,numerical_features] = X_test_numerical_imputed","5991a7d7":"X_test_imputed = pd.DataFrame(np.round(imputer.fit_transform(X_test)),columns = X_test.columns)","4085b2e1":"scaler.fit(X_test_imputed)\nX_test_scaled = scaler.transform(X_test_imputed)","78190814":"X_test_scaled = pd.DataFrame(data=X_test_scaled, columns = X_test.columns)","d2429b1b":"With the tensorflow backend, the process is quick and results will be printed as it iterates through every 100 rows. We need to round the values because KNN will produce floats. This means that our categorical columns will be rounded as well, so be sure to leave any features you do not want rounded left out of the data.","2c3848eb":"Here I'll be using the KNN function from FancyImpute for the task. Note that KNN outputs float values, so I'll round them to intergers to preserve categorical nature","77efb8fd":"# Please upvote if you liked my work :-)","d1afd79c":"#          Pre Processing of Chronic Kidney Disease","b4f697b8":"Now, train and test data both are ready. In my next notebook, I will trying different models and also doing hyperparameter tuning using Hyperopt. Thank you!","08852c74":"## Imputing numerical features using MICE","865395ca":"Let's scale the data now, as the distributions are highly varying for a few features. Here I'll use MinMaxScaler as I don't want to change the under lying distribution and the outliers.","497cae14":"## Scaling Data","8d32aa63":"Now, all the numerical features for training data are imputed. Let's take a look at the categorical features now.","dc252930":"## Here I will be using MICE (Multi-Variate Imputation by Chained Equations)\n\n**Impyute** and **FancyImpute** are libraries specially designed for smart imputations. Two of their best techniques are: KNN and MICE.\n\nKNN (K-Nearest Neighbors) finds the similar values of the nearest neighbors and imputes its average.\n\nMICE (Multivariate Imputation by Chained Equations): What a heavy name!\n\nSimply put, MICE considers the feature with missing values as a dependent variable, and the remaining features as the predictors.\n\nFrom these multiple fitted models, MICE picks up the best ones and imputes using them.\n\nI will be using MICE for numerical data and KNN for categorical data\n\nResults? Way better than mean imputations.","8e7b86a0":"## Encoding categorical features with Object type","6e95eb55":"Now, the data is on similar scales, and good enough to be modeled. The same steps shall also be applied on the test set.","14435403":"## Test Data","def41666":"Now, the data is imputed","0e98d30d":"## * This is my 2nd notebook in extension to the EDA one.\n## * The first notebook can be found here for your reference: https:\/\/www.kaggle.com\/chayan8\/chronic-kidney-disease-explored","c8a9055f":"Chronic kidney disease (CKD), also known as chronic renal disease. Chronic kidney disease involves conditions that damage your kidneys and decrease their ability to keep you healthy.","6ad9057c":"So, they're Label encoded now.","0e81c6d2":"## Imputing Categorical features"}}