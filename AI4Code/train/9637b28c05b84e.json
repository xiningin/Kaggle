{"cell_type":{"207c0a05":"code","5684d8dc":"code","ed9c2ee2":"code","725a5c7d":"code","34ee310c":"code","576fa347":"code","7ca0f0c8":"code","d421e9aa":"code","290373d1":"code","de41c320":"code","34af0b9b":"code","90b93926":"code","7f4a2f0b":"code","64c4bd08":"code","5334bbd9":"code","86107e58":"code","18ed12c2":"code","46a35247":"code","d5764f9b":"code","b07551be":"code","164c340e":"code","2ef20493":"code","97c18659":"code","db05ab31":"code","503388f2":"code","07c09beb":"code","80a78aa8":"code","3c2f7ea6":"code","8a91ea95":"code","ecec707f":"code","fc2e1493":"code","c03dc36c":"code","98e6fa8e":"code","63eaeb60":"code","543ef49c":"code","7b9f6f1f":"code","30701d0b":"code","3aba21a6":"markdown","d4f86df4":"markdown","bd41dff1":"markdown","a0f0b170":"markdown","58d4c1da":"markdown","bc46cdd6":"markdown","04d12bd8":"markdown","4a034572":"markdown","06ce6bbe":"markdown","0fc89bc1":"markdown","43817fba":"markdown","46eeafb0":"markdown","23c3d210":"markdown","2ec02415":"markdown","470f8b6b":"markdown","fe8a7f04":"markdown","c63c3463":"markdown","e25c0ede":"markdown","10407e31":"markdown","6cd47ce4":"markdown","56ce3c46":"markdown","348b3936":"markdown"},"source":{"207c0a05":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.datasets import load_iris\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.linear_model import SGDClassifier #\u6bd4\u8f03\u7528\n\nfrom sklearn import metrics\n\nfrom matplotlib.colors import ListedColormap\nimport matplotlib.patches as mpatches\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport os # reading the input files we have access to\nprint(os.listdir('..\/input'))","5684d8dc":"#read sample file \ndata = np.loadtxt('..\/input\/log-sample\/log_sample.csv', delimiter=',', skiprows=1)","ed9c2ee2":"data.shape","725a5c7d":"x_columns = ['browse_experiences', 'hours_of_study', 'exam_results']\ndata_frame = pd.DataFrame(data, columns=x_columns)\ndata_frame","34ee310c":"#\u7279\u5fb4\u91cfX\nX = data[:, 0:2]\n#\u76ee\u7684\u5909\u6570y\ny = data[:, 2]","576fa347":"#\u6b63\u898f\u5316\ndef normalize_x_features(X):\n    X_norm = np.zeros((X.shape[0], X.shape[1]))\n    mean = np.zeros((1, X.shape[1]))\n    std = np.zeros((1, X.shape[1]))\n    for i in range(X.shape[1]):\n        mean[:, i] = np.mean(X[:, i])\n        std[:, i] = np.std(X[:, i])\n        X_norm[:, i] = (X[:, i] - float(mean[:, i])) \/ float(std[:, i])\n    return X_norm, mean, std","7ca0f0c8":"X_norm, mean, std = normalize_x_features(X)","d421e9aa":"X_norm[0:5, :]","290373d1":"print(X_norm.mean())\nprint(X_norm.std())","de41c320":"fig, axes = plt.subplots(1,2,figsize=(12,6))\nleft = sns.countplot(x='browse_experiences',\n                  hue='exam_results',\n                  data=data_frame,\n                  ax=axes[0])\n\nright = sns.barplot(x='browse_experiences',\n                  y='exam_results',\n                  data=data_frame,\n                  ax=axes[1]).set_ylabel('Pass Rate')\n\naxes[0].set_title('Count of Pass and Fail')\naxes[1].set_title('Pass Rate')","34af0b9b":"plt.figure(figsize=(8, 6))\nax = sns.countplot(x=\"hours_of_study\", hue=\"exam_results\", data=data_frame)\nplt.show()","90b93926":"fig, axes = plt.subplots(1,2,figsize=(12,6))\n\nleft_fail = sns.kdeplot(data_frame['browse_experiences'][data_frame['exam_results'] == 0],\n                        shade=True, color=\"r\", label='fail', ax=axes[0])\nleft_pass = sns.kdeplot(data_frame['browse_experiences'][data_frame['exam_results'] == 1],\n                        shade=True, color=\"b\", label='pass', ax=axes[0])\n\nright_fail = sns.kdeplot(data_frame['hours_of_study'][data_frame['exam_results'] == 0],\n                         shade=True, color=\"r\", label='fail', ax=axes[1])\nright_pass = sns.kdeplot(data_frame['hours_of_study'][data_frame['exam_results'] == 1],\n                         shade=True, color=\"b\", label='pass', ax=axes[1])\n\n\naxes[0].set_title('Density of Pass and Fail')\naxes[1].set_title('Density of Pass and Fail')","7f4a2f0b":"class ScratchLogisticRegression():\n    \"\"\"\n    \u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u306e\u30b9\u30af\u30e9\u30c3\u30c1\u5b9f\u88c5\n\n    Parameters\n    ----------\n    num_iter : int\n      \u30a4\u30c6\u30ec\u30fc\u30b7\u30e7\u30f3\u6570\n    lr : float\n      \u5b66\u7fd2\u7387\n    bias : bool\n      \u30d0\u30a4\u30a2\u30b9\u9805\u3092\u5165\u308c\u308b\u5834\u5408\u306fTrue\n    verbose : bool\n      \u5b66\u7fd2\u904e\u7a0b\u3092\u51fa\u529b\u3059\u308b\u5834\u5408\u306fTrue\n    lambda_ : float\n    \u3000L2\u6b63\u5247\u5316\u30d1\u30e9\u30e1\u30fc\u30bf\n    to_pickle_ : bool\n    \u3000\u5b66\u7fd2\u3057\u305f\u91cd\u307f\u306e\u4fdd\u5b58\u6709\u7121\n\n    Attributes\n    ----------\n    self.coef_ : \u6b21\u306e\u5f62\u306endarray, shape (n_features,)\n      \u30d1\u30e9\u30e1\u30fc\u30bf\n    self.loss : \u6b21\u306e\u5f62\u306endarray, shape (self.iter,)\n      \u8a13\u7df4\u30c7\u30fc\u30bf\u306b\u5bfe\u3059\u308b\u640d\u5931\u306e\u8a18\u9332\n    self.val_loss : \u6b21\u306e\u5f62\u306endarray, shape (self.iter,)\n      \u691c\u8a3c\u30c7\u30fc\u30bf\u306b\u5bfe\u3059\u308b\u640d\u5931\u306e\u8a18\u9332\n\n    \"\"\"\n    def __init__(self, num_iter, lr, bias, verbose, lambda_=1.0, to_pickle_=False):\n        # \u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u5c5e\u6027\u3068\u3057\u3066\u8a18\u9332\n        self.iter = num_iter\n        self.lr = lr\n        self.lambda_ = lambda_\n        self.bias = bias\n        self.verbose = verbose\n        self.to_pickle_ = to_pickle_\n        # \u640d\u5931\u3092\u8a18\u9332\u3059\u308b\u914d\u5217\u3092\u7528\u610f\n        self.loss = np.zeros(self.iter)\n        self.val_loss = np.zeros(self.iter)\n        \n    def __add_x0(self, X):\n        \"\"\"\n        \u30d0\u30a4\u30a2\u30b9\u9805\u3092\u5165\u308c\u308b\u5834\u5408\u3001\n        \u50241\u3092\u7279\u5fb4\u91cfX\u306e1\u5217\u76ee\u306b\u7d50\u5408\n        \"\"\"\n        x0 = np.ones((X.shape[0], 1))\n        return np.concatenate((x0, X), axis=1)\n    \n    def __sigmoid_function(self, X):\n        \"\"\"\n        \u30b7\u30b0\u30e2\u30a4\u30c9\u95a2\u6570\u306b\u3066\u4eee\u5b9a\u95a2\u6570\u3092\u8a08\u7b97\n        \"\"\"\n        return 1 \/ (1 + np.exp(-np.dot(X, self.theta)))               \n       \n    def __gradient_descent(self, X, y, h):\n        \"\"\"\n        \u30d1\u30e9\u30e1\u30fc\u30bftheta\u66f4\u65b0\u7528\u306e\u30e1\u30bd\u30c3\u30c9\n\n        Parameters\n        ----------\n        X : \u6b21\u306e\u5f62\u306endarray, shape (n_samples, n_features)\n          \u8a13\u7df4\u30c7\u30fc\u30bf\n        y : \u6b21\u306e\u5f62\u306endarray, shape (n_samples, )\n          \u8a13\u7df4\u30c7\u30fc\u30bf\u306e\u6b63\u89e3\u5024\n        h : \u6b21\u306e\u5f62\u306endarray, shape (n_samples, )\n        \u3000\u30b7\u30b0\u30e2\u30a4\u30c9\u95a2\u6570\u306e\u51fa\u529b\u7d50\u679c\n\n        Returns\n        -------\n        self.theta\uff1a\u56de\u5e30\u4fc2\u6570\u30d9\u30af\u30c8\u30eb shape (n_features, )\n\n        \"\"\"\n        m = len(y)\n        gradient = np.dot(X.T, (h - y)) \/ m\n        if self.bias:\n            l2norm = (self.lambda_ \/ m) * self.theta\n            l2norm[0] = 0 #\u5207\u7247\u306e\u5217\u306f0\n            self.theta -= self.lr * gradient + l2norm\n        else:\n            l2norm = (self.lambda_ \/ m) * self.theta\n            self.theta -= self.lr * gradient + l2norm\n            \n        return self.theta    \n\n    def __loss_function(self, h, y):\n        \"\"\"\n        \u640d\u5931\u95a2\u6570\u3092\u8a08\u7b97\n        \"\"\"\n        m = len(y)\n        loss = np.sum((-y * np.log(h) - (1 - y) * np.log(1 - h)).mean() +\n                      (self.lambda_ * np.square(self.theta)) \/ (2 * m))\n\n        return loss\n\n    def fit(self, X, y, X_val=None, y_val=None):\n        \"\"\"\n        \u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u3092\u5b66\u7fd2\u3059\u308b\u3002\u691c\u8a3c\u30c7\u30fc\u30bf\u304c\u5165\u529b\u3055\u308c\u305f\u5834\u5408\u306f\u305d\u308c\u306b\u5bfe\u3059\u308b\u640d\u5931\u3068\u7cbe\u5ea6\u3082\u30a4\u30c6\u30ec\u30fc\u30b7\u30e7\u30f3\u3054\u3068\u306b\u8a08\u7b97\u3059\u308b\u3002\n\n        Parameters\n        ----------\n        X : \u6b21\u306e\u5f62\u306endarray, shape (n_samples, n_features)\n            \u8a13\u7df4\u30c7\u30fc\u30bf\u306e\u7279\u5fb4\u91cf\n        y : \u6b21\u306e\u5f62\u306endarray, shape (n_samples, )\n            \u8a13\u7df4\u30c7\u30fc\u30bf\u306e\u6b63\u89e3\u5024\n        X_val : \u6b21\u306e\u5f62\u306endarray, shape (n_samples, n_features)\n            \u691c\u8a3c\u30c7\u30fc\u30bf\u306e\u7279\u5fb4\u91cf\n        y_val : \u6b21\u306e\u5f62\u306endarray, shape (n_samples, )\n            \u691c\u8a3c\u30c7\u30fc\u30bf\u306e\u6b63\u89e3\u5024\n        \"\"\"\n        if self.bias:\n            X = self.__add_x0(X)\n            if not(X_val is None) and not(y_val is None):\n                X_val = self.__add_x0(X_val)\n\n        self.theta = np.random.rand(X.shape[1])\n\n        for i in range(self.iter):\n            h = self.__sigmoid_function(X)\n            self.theta = self.__gradient_descent(X, y, h)\n            self.loss[i] = self.__loss_function(h, y)\n\n            if not(X_val is None) and not(y_val is None):\n                h_val = self.__sigmoid_function(X_val)\n                self.val_loss[i] = self.__loss_function(h_val, y_val)\n\n            if self.verbose:\n                #verbose\u3092True\u306b\u3057\u305f\u5834\u5408\u306f\u5b66\u7fd2\u904e\u7a0b\u3092\u51fa\u529b\n                print(\"Training Data\" + \"*\"*40)\n                print(\"theta:{}\".format(self.theta))\n                print(\"loss:{}\".format(self.loss[i]))\n                if not(X_val is None) and not(y_val is None):\n                    print(\"Validation Data\" + \"*\"*40)\n                    print(\"theta:{}\".format(self.theta))\n                    print(\"loss in cost function:{}\".format(self.val_loss[i])) \n        \n        if self.to_pickle_:\n            pickle_obj = self.theta\n            pd.to_pickle(pickle_obj, 'pickle_obj.pkl')\n\n    def predict_proba(self, X):\n        \"\"\"\n        \u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u3092\u4f7f\u3044\u78ba\u7387\u3092\u63a8\u5b9a\u3059\u308b\u3002\n\n        Parameters\n        ----------\n        X : \u6b21\u306e\u5f62\u306endarray, shape (n_samples, n_features)\n            \u30b5\u30f3\u30d7\u30eb\n\n        Returns\n        -------\n            \u6b21\u306e\u5f62\u306endarray, shape (n_samples, 1)\n            \u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u306b\u3088\u308b\u63a8\u5b9a\u7d50\u679c\n        \"\"\"\n        if self.bias:\n            X = self.__add_x0(X)\n    \n        return self.__sigmoid_function(X)\n    \n    def predict(self, X):\n        \"\"\"\n        \u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u3092\u4f7f\u3044\u30e9\u30d9\u30eb\u3092\u63a8\u5b9a\u3059\u308b\u3002\n\n        Parameters\n        ----------\n        X : \u6b21\u306e\u5f62\u306endarray, shape (n_samples, n_features)\n            \u30b5\u30f3\u30d7\u30eb\n\n        Returns\n        -------\n            \u6b21\u306e\u5f62\u306endarray, shape (n_samples, 1)\n            \u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u306b\u3088\u308b\u63a8\u5b9a\u7d50\u679c\n        \"\"\"\n        return self.predict_proba(X).round()","64c4bd08":"model = ScratchLogisticRegression(num_iter=100, lr=0.1, bias=True, verbose=True)\nmodel.fit(X_norm, y)","5334bbd9":"y_pred = model.predict(X_norm)\ny_pred_proba = model.predict_proba(X_norm)","86107e58":"y_pred","18ed12c2":"y_pred_proba","46a35247":"def evaluate(y_test, y_pred):\n    \"\"\"\n    2\u5024\u5206\u985e\u306e\u8a55\u4fa1\u6307\u6a19\u3092\u8a08\u7b97\u30fb\u63cf\u753b\u3059\u308b\u3002\n    \"\"\"\n    acc = metrics.accuracy_score(y_test, y_pred)\n    precision = metrics.precision_score(y_test, y_pred)\n    recall = metrics.recall_score(y_test, y_pred)\n    f1 = metrics.f1_score(y_test, y_pred)\n    \n    plt.figure(figsize=(1.6, 1.2))\n    cm_def = metrics.confusion_matrix(y_test, y_pred)\n    sns.heatmap(cm_def, annot=True, cmap='Blues')\n    plt.title('Confusion Matrix')\n    plt.show()\n    return acc, precision, recall, f1","d5764f9b":"print('Accuracy : {:.2f}\\n'\n      'Precision : {:.2f}\\n'\n      'Recall : {:.2f}\\n'\n      'F1 : {:.2f}\\n'\n      .format(*evaluate(y, y_pred)))","b07551be":"iris_dataset = load_iris()","164c340e":"#\u30aa\u30ea\u30b8\u30ca\u30eb\u30c7\u30fc\u30bf\u306e\u4f5c\u6210\nx_origin_columns = ['sepal_length', 'petal_length']\nX_origin = pd.DataFrame(iris_dataset.data[:,[0,2]], columns=x_origin_columns)\ny_origin = pd.DataFrame(iris_dataset.target, columns=['Species'])\ndf_origin = pd.concat([X_origin, y_origin], axis=1)","2ef20493":"rows_to_drop = df_origin.index[df_origin['Species'] == 0 ] #'setosa'\u306e\u884c\u3092\u524a\u9664\ndf = df_origin.drop(rows_to_drop, axis=0).reset_index(drop=True)\niris_mapping = {1: 0, 2: 1} #'versicolor'\u30920,  'virginica'\u30921\u306b\u30de\u30c3\u30d4\u30f3\u30b0\ndf['Species'] = df['Species'].map(iris_mapping)\nX = df.iloc[:, :-1].to_numpy()\ny = df.loc[:, 'Species'].to_numpy()","97c18659":"X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.80)","db05ab31":"scaler = StandardScaler()","503388f2":"scaler.fit(X_train)","07c09beb":"X_train_scaled = scaler.transform(X_train)","80a78aa8":"X_test_scaled = scaler.transform(X_test)","3c2f7ea6":"scikit_model = SGDClassifier(loss=\"log\")\nscikit_model.fit(X_train_scaled, y_train.ravel())\nscikit_pred = scikit_model.predict(X_test_scaled)\nscikit_pred_proba = scikit_model.predict_proba(X_test_scaled)","8a91ea95":"print('Accuracy : {:.2f}\\n'\n      'Precision : {:.2f}\\n'\n      'Recall : {:.2f}\\n'\n      'F1 : {:.2f}\\n'\n      .format(*evaluate(y_test, scikit_pred)))","ecec707f":"scratch_model = ScratchLogisticRegression(num_iter=100, lr=0.1, bias=True, verbose=True)\nscratch_model.fit(X_train_scaled, y_train.ravel(), X_test_scaled, y_test.ravel())\nscratch_pred = scratch_model.predict(X_test_scaled)\nscratch_pred_proba = scratch_model.predict_proba(X_test_scaled)","fc2e1493":"print('Accuracy : {:.2f}\\n'\n      'Precision : {:.2f}\\n'\n      'Recall : {:.2f}\\n'\n      'F1 : {:.2f}\\n'\n      .format(*evaluate(y_test, scratch_pred)))","c03dc36c":"plt.figure()\nplt.title(\"model loss\")\nplt.xlabel(\"iter\")\nplt.ylabel(\"loss\")\n\nx_plot = range(0, scratch_model.iter)\nplt.plot(x_plot, scratch_model.loss, '-', color=\"b\", label=\"train loss\")\nplt.plot(x_plot, scratch_model.val_loss, '-', color=\"r\", label=\"val loss\")\n\nplt.legend(loc=\"best\")\n\nplt.show()","98e6fa8e":"def decision_region(X, y, model, step=0.01, title='decision region',\n                    xlabel='sepal_length', ylabel='petal_length', target_names=['versicolor', 'virginica']):\n    \"\"\"\n    2\u5024\u5206\u985e\u30922\u6b21\u5143\u306e\u7279\u5fb4\u91cf\u3067\u5b66\u7fd2\u3057\u305f\u30e2\u30c7\u30eb\u306e\u6c7a\u5b9a\u9818\u57df\u3092\u63cf\u304f\u3002\n    \u80cc\u666f\u306e\u8272\u304c\u5b66\u7fd2\u3057\u305f\u30e2\u30c7\u30eb\u306b\u3088\u308b\u63a8\u5b9a\u5024\u304b\u3089\u63cf\u753b\u3055\u308c\u308b\u3002\n    \u6563\u5e03\u56f3\u306e\u70b9\u306f\u8a13\u7df4\u30c7\u30fc\u30bf\u307e\u305f\u306f\u691c\u8a3c\u30c7\u30fc\u30bf\u3067\u3042\u308b\u3002\n\n    Parameters\n    ----------------\n    X : ndarray, shape(n_samples, 2)\n        \u7279\u5fb4\u91cf\n    y : ndarray, shape(n_samples,)\n        \u30e9\u30d9\u30eb\n    model : object\n        \u5b66\u7fd2\u3057\u305f\u30e2\u30c7\u30eb\u306e\u30a4\u30f3\u30b9\u30f3\u30bf\u30b9\u3092\u5165\u308c\u308b\n    step : float, (default : 0.1)\n        \u63a8\u5b9a\u5024\u3092\u8a08\u7b97\u3059\u308b\u9593\u9694\u3092\u8a2d\u5b9a\u3059\u308b\n    title : str\n        \u30b0\u30e9\u30d5\u306e\u30bf\u30a4\u30c8\u30eb\u306e\u6587\u7ae0\u3092\u4e0e\u3048\u308b\n    xlabel, ylabel : str\n        \u8ef8\u30e9\u30d9\u30eb\u306e\u6587\u7ae0\u3092\u4e0e\u3048\u308b\n    target_names= : list of str\n        \u51e1\u4f8b\u306e\u4e00\u89a7\u3092\u4e0e\u3048\u308b\n    \"\"\"\n    # setting\n    scatter_color = ['red', 'blue']\n    contourf_color = ['pink', 'skyblue']\n    n_class = 2\n    # pred\n    mesh_f0, mesh_f1  = np.meshgrid(np.arange(np.min(X[:,0])-0.5,\n                                              np.max(X[:,0])+0.5, step),\n                                    np.arange(np.min(X[:,1])-0.5,\n                                              np.max(X[:,1])+0.5, step))\n\n    mesh = np.c_[np.ravel(mesh_f0),np.ravel(mesh_f1)]\n    y_pred = model.predict(mesh).reshape(mesh_f0.shape)\n    # plot\n    plt.title(title)\n    plt.xlabel(xlabel)\n    plt.ylabel(ylabel)\n    plt.contourf(mesh_f0, mesh_f1, y_pred, n_class-1, cmap=ListedColormap(contourf_color))\n    plt.contour(mesh_f0, mesh_f1, y_pred, n_class-1, colors='y', linewidths=3, alpha=0.5)\n    for i, target in enumerate(set(y)):\n        plt.scatter(X[y==target][:, 0], X[y==target][:, 1], s=80,\n                    color=scatter_color[i], label=target_names[i], marker='o')\n    patches = [mpatches.Patch(color=scatter_color[i], label=target_names[i]) for i in range(n_class)]\n    plt.legend(handles=patches)\n    plt.legend()\n    plt.show()","63eaeb60":"scikit_pred, scratch_pred","543ef49c":"decision_region(X_test_scaled, scratch_pred, scratch_model)","7b9f6f1f":"scratch_model = ScratchLogisticRegression(num_iter=100, lr=0.1,\n                                          bias=True, verbose=False,\n                                          to_pickle_=True)\nscratch_model.fit(X_train_scaled, y_train.ravel())","30701d0b":"pickle_obj = pd.read_pickle('pickle_obj.pkl')\nprint(pickle_obj)","3aba21a6":"## \u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u5206\u6790","d4f86df4":"[\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u4f5c\u6210\u306b\u3042\u305f\u3063\u3066\u53c2\u8003\u306b\u3057\u305f\u30b5\u30a4\u30c8](https:\/\/bellcurve.jp\/statistics\/blog\/8607.html)","bd41dff1":"### \u3010\u554f\u984c6\u3011\u5b66\u7fd2\u66f2\u7dda\u306e\u30d7\u30ed\u30c3\u30c8\n\u5b66\u7fd2\u66f2\u7dda\u3092\u898b\u3066\u640d\u5931\u304c\u9069\u5207\u306b\u4e0b\u304c\u3063\u3066\u3044\u308b\u304b\u3069\u3046\u304b\u78ba\u8a8d\u3057\u3066\u304f\u3060\u3055\u3044\u3002","a0f0b170":"#### iris\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8","58d4c1da":"### \u3010\u554f\u984c2\u3011\u6700\u6025\u964d\u4e0b\u6cd5\n\u6700\u6025\u964d\u4e0b\u6cd5\u306b\u3088\u308a\u5b66\u7fd2\u3055\u305b\u308b\u5b9f\u88c5\u3092\u884c\u306a\u3063\u3066\u304f\u3060\u3055\u3044\u3002\u4ee5\u4e0b\u306e\u5f0f\u3067\u8868\u3055\u308c\u308b\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u66f4\u65b0\u5f0f\u306e\u30e1\u30bd\u30c3\u30c9_gradient_descent\u3092\u8ffd\u52a0\u3057\u3001fit\n\u30e1\u30bd\u30c3\u30c9\u304b\u3089\u547c\u3073\u51fa\u3059\u3088\u3046\u306b\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n\n$$\n\\theta_{j}:=\\theta_{j}-\u03b1\\frac{\\delta J(\\theta)}{\\delta\\theta_{j}}\n$$\n\n$$\n\\frac{\\delta J(\\theta)}{\\delta\\theta_{0}}=\\theta_{j}-\\frac{1}{m}\\displaystyle \\sum_{i=1}^m\n(h_{\\theta}(x^{(i)})-y^{(i)})x_{j}^{(i)},j = 0\n$$\n\n$$\n\\frac{\\delta J(\\theta)}{\\delta\\theta_{0}}=(\\theta_{j}-\\frac{1}{m}\\displaystyle \\sum_{i=1}^m\n(h_{\\theta}(x^{(i)})-y^{(i)})x_{j}^{(i)})+\\frac{\\lambda}{m}\\theta_{j},j \\geq 1\n$$\n\n\n$\\alpha$ : \u5b66\u7fd2\u7387\n\n\n$i$ : \u30b5\u30f3\u30d7\u30eb\u306e\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\n\n\n$j$ : \u7279\u5fb4\u91cf\u306e\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\n\n\n$m$ : \u5165\u529b\u3055\u308c\u308b\u30c7\u30fc\u30bf\u306e\u6570\n\n\n$h_\\theta()$ : \u4eee\u5b9a\u95a2\u6570\n\n\n$x$ : \u7279\u5fb4\u91cf\u30d9\u30af\u30c8\u30eb\n\n\n$\\theta$ : \u30d1\u30e9\u30e1\u30fc\u30bf\uff08\u91cd\u307f\uff09\u30d9\u30af\u30c8\u30eb\n\n\n$x^{(i)}$ : i\u756a\u76ee\u306e\u30b5\u30f3\u30d7\u30eb\u306e\u7279\u5fb4\u91cf\u30d9\u30af\u30c8\u30eb\n\n\n$y^{(i)}$ : i\u756a\u76ee\u306e\u30b5\u30f3\u30d7\u30eb\u306e\u6b63\u89e3\u30e9\u30d9\u30eb\n\n\n$\\theta_j$ : j\u756a\u76ee\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\uff08\u91cd\u307f\uff09\n\n\n$\u03bb$ : \u6b63\u5247\u5316\u30d1\u30e9\u30e1\u30fc\u30bf","bc46cdd6":"### \u3010\u554f\u984c4\u3011\u76ee\u7684\u95a2\u6570\n\u4ee5\u4e0b\u306e\u6570\u5f0f\u3067\u8868\u3055\u308c\u308b\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u306e \u76ee\u7684\u95a2\u6570\uff08\u640d\u5931\u95a2\u6570\uff09 \u3092\u5b9f\u88c5\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u305d\u3057\u3066\u3001\u3053\u308c\u3092self.loss, self.val_loss\u306b\u8a18\u9332\u3059\u308b\u3088\u3046\u306b\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n\n\n\u306a\u304a\u3001\u3053\u306e\u6570\u5f0f\u306b\u306f\u6b63\u5247\u5316\u9805\u304c\u542b\u307e\u308c\u3066\u3044\u307e\u3059\u3002\n\n\n\uff0a\u6570\u5f0f\u304c\u898b\u5207\u308c\u308b\u5834\u5408\u3001DIVER\u3092\u5168\u753b\u9762\u306b\u3057\u3066\u5fa1\u89a7\u304f\u3060\u3055\u3044\u3002\n\n$$\nJ(\\theta)=\\frac{1}{m}\\displaystyle \\sum_{i=1}^m\n[-y^{(i)}\\log(h_{\\theta}(x^{(i)})) -(1 - y^{(i)}) \\log(1 - h_{\\theta}(x^{(i)}))] +\n\\frac{\\lambda}{2m}\\displaystyle \\sum_{j=1}^n \\theta_{j}^2.\n$$\n\n\n$m$ : \u5165\u529b\u3055\u308c\u308b\u30c7\u30fc\u30bf\u306e\u6570\n\n\n$h_\\theta()$ : \u4eee\u5b9a\u95a2\u6570\n\n\n$x$ : \u7279\u5fb4\u91cf\u30d9\u30af\u30c8\u30eb\n\n\n$\\theta$ : \u30d1\u30e9\u30e1\u30fc\u30bf\uff08\u91cd\u307f\uff09\u30d9\u30af\u30c8\u30eb\n\n\n$x^{(i)}$ : i\u756a\u76ee\u306e\u30b5\u30f3\u30d7\u30eb\u306e\u7279\u5fb4\u91cf\u30d9\u30af\u30c8\u30eb\n\n\n$y^{(i)}$ : i\u756a\u76ee\u306e\u30b5\u30f3\u30d7\u30eb\u306e\u6b63\u89e3\u30e9\u30d9\u30eb\n\n\n$\\theta_j$ : j\u756a\u76ee\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\uff08\u91cd\u307f\uff09\n\n\n$n$ : \u7279\u5fb4\u91cf\u306e\u6570\n\n\n$\u03bb$ : \u6b63\u5247\u5316\u30d1\u30e9\u30e1\u30fc\u30bf","04d12bd8":"\uff1c\u7279\u5fb4\u91cf\u306e\u8a73\u7d30\uff1e\n- \u8a66\u9a13\u52c9\u5f37\u306b\u3042\u305f\u308a\u3001\u6559\u79d1\u66f8\u3084\u554f\u984c\u96c6\u306b\u3066\u52c9\u5f37\u6642\u9593\u3092\u5272\u304f\u3068\u3059\u308b\u3002\n\n\n- \u8a66\u9a13\u5bfe\u7b56\u306b\u3042\u305f\u308a\u3001\u30a4\u30f3\u30bf\u30fc\u30cd\u30c3\u30c8\u3084\u60c5\u5831\u5546\u6750\u306b\u3066\u50be\u5411\u3084\u5bfe\u7b56\u3092\u5f97\u305f\u304b\u3092\u95b2\u89a7\u7d4c\u9a13\u3068\u3059\u308b\u3002","4a034572":"### \u30af\u30e9\u30b9\u5b9f\u88c5","06ce6bbe":"### \u3010\u554f\u984c5\u3011\u5b66\u7fd2\u3068\u63a8\u5b9a\n\u6a5f\u68b0\u5b66\u7fd2\u30b9\u30af\u30e9\u30c3\u30c1\u5165\u9580\u306eSprint\u3067\u7528\u610f\u3057\u305firis\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306evirgicolor\u3068virginica\u306e2\u5024\u5206\u985e\u306b\u5bfe\u3057\u3066\u30b9\u30af\u30e9\u30c3\u30c1\u5b9f\u88c5\u306e\u5b66\u7fd2\u3068\u63a8\u5b9a\u3092\u884c\u306a\u3063\u3066\u304f\u3060\u3055\u3044\u3002\n\n\nscikit-learn\u306b\u3088\u308b\u5b9f\u88c5\u3068\u6bd4\u3079\u3001\u6b63\u3057\u304f\u52d5\u3044\u3066\u3044\u308b\u304b\u3092\u78ba\u8a8d\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n\n\nAccuracy\u3084Precision\u3001Recall\u306a\u3069\u306e\u6307\u6a19\u5024\u306fscikit-learn\u3092\u4f7f\u7528\u3057\u3066\u304f\u3060\u3055\u3044\u3002","0fc89bc1":"- \u524d\u51e6\u7406\u30fb\u6a19\u6e96\u5316","43817fba":"### \u3010\u554f\u984c1\u3011\u4eee\u5b9a\u95a2\u6570\n\n\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u306e\u4eee\u5b9a\u95a2\u6570\u306e\u30e1\u30bd\u30c3\u30c9\u3092ScratchLogisticRegression\u30af\u30e9\u30b9\u306b\u5b9f\u88c5\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n\n\n\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u306e\u4eee\u5b9a\u95a2\u6570\u306f\u3001\u7dda\u5f62\u56de\u5e30\u306e\u4eee\u5b9a\u95a2\u6570\u3092 \u30b7\u30b0\u30e2\u30a4\u30c9\u95a2\u6570 \u306b\u901a\u3057\u305f\u3082\u306e\u3067\u3059\u3002\u30b7\u30b0\u30e2\u30a4\u30c9\u95a2\u6570\u306f\u4ee5\u4e0b\u306e\u5f0f\u3067\u8868\u3055\u308c\u307e\u3059\u3002\n\n$$\ng(z)=\\frac{1}{1 + e^{-z}}\n$$\n\n\n\u7dda\u5f62\u56de\u5e30\u306e\u4eee\u5b9a\u95a2\u6570\u306f\u6b21\u306e\u5f0f\u3067\u3057\u305f\u3002\n\n$$\nh_{\\theta(x)}=\\theta^T.x.\n$$\n\n\n\u307e\u3068\u3081\u3066\u66f8\u304f\u3068\u3001\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u306e\u4eee\u5b9a\u95a2\u6570\u306f\u6b21\u306e\u3088\u3046\u306b\u306a\u308a\u307e\u3059\u3002\n\n$$\nh_{\\theta(x)}=\\frac{1}{1 + e^{-\\theta^T.x.}}\n$$\n\n\n$x$ : \u7279\u5fb4\u91cf\u30d9\u30af\u30c8\u30eb\n\n\n$\\theta$ : \u30d1\u30e9\u30e1\u30fc\u30bf\uff08\u91cd\u307f\uff09\u30d9\u30af\u30c8\u30eb","46eeafb0":"### \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8","23c3d210":"\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u524d\u63d0\n\n- \u8a66\u9a13\u60c5\u5831\u306e\u95b2\u89a7\u7d4c\u9a13\u304c\u3042\u308b\u8005\u306e\u65b9\u304c\u3001\u306a\u3044\u8005\u3068\u6bd4\u8f03\u3057\u3066\u3001\u8a66\u9a13\u306b\u5408\u683c\u3059\u308b\u53ef\u80fd\u6027\u304c\u9ad8\u3044\u3002\n\n\n- \u8a66\u9a13\u52c9\u5f37\u306b\u6642\u9593\u3092\u8cbb\u3084\u3057\u305f\u8005\u306e\u65b9\u304c\u3001\u306a\u3044\u8005\u3068\u6bd4\u8f03\u3057\u3066\u3001\u8a66\u9a13\u306b\u5408\u683c\u3059\u308b\u53ef\u80fd\u6027\u304c\u9ad8\u3044\u3002","2ec02415":"### \u3010\u554f\u984c3\u3011\u63a8\u5b9a\n\u63a8\u5b9a\u3059\u308b\u4ed5\u7d44\u307f\u3092\u5b9f\u88c5\u3057\u3066\u304f\u3060\u3055\u3044\u3002ScratchLogisticRegression\u30af\u30e9\u30b9\u306e\u96db\u5f62\u306b\u542b\u307e\u308c\u308bpredict\u30e1\u30bd\u30c3\u30c9\u3068predict_proba\u30e1\u30bd\u30c3\u30c9\u306b\u66f8\u304d\u52a0\u3048\u3066\u304f\u3060\u3055\u3044\u3002\n\n\n\u4eee\u5b9a\u95a2\u6570 $h_\\theta(x)$ \u306e\u51fa\u529b\u304cpredict_proba\u306e\u8fd4\u308a\u5024\u3001\u3055\u3089\u306b\u305d\u306e\u5024\u306b\u95be\u5024\u3092\u8a2d\u3051\u30661\u30680\u306e\u30e9\u30d9\u30eb\u3068\u3057\u305f\u3082\u306e\u304cpredict\u306e\u8fd4\u308a\u5024\u3068\u306a\u308a\u307e\u3059\u3002","470f8b6b":"### \u7279\u5fb4\u91cf\u30a8\u30f3\u30b8\u30cb\u30a2\u30ea\u30f3\u30b0(EDA)","fe8a7f04":"- \u524d\u51e6\u7406\u30fb\u8a13\u7df4\u30c7\u30fc\u30bf\u3068\u691c\u8a3c\u30c7\u30fc\u30bf\u306e\u5206\u5272","c63c3463":"### \u3010\u554f\u984c7\u3011\u6c7a\u5b9a\u9818\u57df\u306e\u53ef\u8996\u5316\n\u6c7a\u5b9a\u9818\u57df\u3092\u53ef\u8996\u5316\u3057\u3066\u304f\u3060\u3055\u3044\u3002","e25c0ede":"1. scikit-learn \u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30","10407e31":"\uff1c\u7279\u5fb4\u91cfX\uff1e\\\n\u95b2\u89a7\u7d4c\u9a13: \u8a66\u9a13\u306b\u95a2\u4fc2\u3059\u308b\u60c5\u5831\u3092\u95b2\u89a7\u3057\u305f\u304b\u5426\u304b(0: \u95b2\u89a7\u7d4c\u9a13\u306a\u3057\u30011: \u95b2\u89a7\u7d4c\u9a13\u3042\u308a)\\\n\u52c9\u5f37\u6642\u9593: \u8a66\u9a13\u306b\u95a2\u4fc2\u3059\u308b\u52c9\u5f37\u6642\u9593\n\n\uff1c\u76ee\u7684\u5909\u6570y\uff1e\\\n\u8a66\u9a13\u7d50\u679c: \u8a66\u9a13\u306e\u7d50\u679c(0: \u8a66\u9a13\u4e0d\u5408\u683c\u30011: \u8a66\u9a13\u5408\u683c)","6cd47ce4":"\u8a66\u9a13\u60c5\u5831\u306e\u95b2\u89a7\u7d4c\u9a13\u304a\u3088\u3073\u8a66\u9a13\u52c9\u5f37\u306e\u6642\u9593\u306b\u3088\u308a\u8a66\u9a13\u7d50\u679c\u3092\u4e88\u6e2c\u3059\u308b\u3002\n\n|\u3000| \u95b2\u89a7\u7d4c\u9a13 | \u52c9\u5f37\u6642\u9593 | \u8a66\u9a13\u7d50\u679c |\n:---:|:---:|:---:|:---:|\n|**0**| 1.0 | 24.0 | 1.0 |\n|**1**| 1.0 | 18.0 | 1.0 |\n|**2**| 0.0 | 15.0 | 1.0 |\n|**3**| 1.0 | 16.0 | 1.0 |\n|\uff1a|\uff1a|\uff1a|\uff1a|","56ce3c46":"### \u3010\u554f\u984c8\u3011\uff08\u30a2\u30c9\u30d0\u30f3\u30b9\u8ab2\u984c\uff09\u91cd\u307f\u306e\u4fdd\u5b58\n\u691c\u8a3c\u304c\u5bb9\u6613\u306b\u306a\u308b\u3088\u3046\u306b\u3001\u5b66\u7fd2\u3057\u305f\u91cd\u307f\u3092\u4fdd\u5b58\u304a\u3088\u3073\u8aad\u307f\u8fbc\u307f\u304c\u3067\u304d\u308b\u3088\u3046\u306b\u3057\u307e\u3057\u3087\u3046\u3002pickle\u30e2\u30b8\u30e5\u30fc\u30eb\u3084NumPy\u306enp.savez\u3092\u5229\u7528\u3057\u307e\u3059\u3002\n\n\n[pickle \u2014 Python \u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u306e\u76f4\u5217\u5316 \u2014 Python 3.7.4 \u30c9\u30ad\u30e5\u30e1\u30f3\u30c8](https:\/\/docs.python.org\/ja\/3\/library\/pickle.html)\n\n\n[numpy.savez \u2014 NumPy v1.17 Manual](https:\/\/numpy.org\/doc\/stable\/reference\/generated\/numpy.savez.html)","348b3936":"2. scratch \u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30"}}