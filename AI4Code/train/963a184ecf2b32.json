{"cell_type":{"5e49c282":"code","723aaadb":"code","4275771b":"code","b105c1e5":"code","0c9a85e8":"code","9fa0a1ca":"code","ca42a6ce":"code","2d3b99c5":"code","a86bf858":"code","eff71ce8":"code","a585ec45":"code","8d4b5ae2":"code","b66d04f7":"code","8b7dbba2":"code","999c908f":"code","07af33f0":"code","92824a62":"code","bf14752a":"code","fc1b43b2":"code","751b3f06":"code","65098e63":"code","803d8749":"code","3e637e14":"code","046d7b03":"code","9da418b3":"code","63d5a616":"code","33ca519d":"code","f1bd69ad":"code","bdea416f":"code","562cb01f":"code","58f078d1":"code","e774698d":"code","01a657bd":"code","4b802979":"code","0fa1f9af":"code","a9ccf0af":"code","091e0eb6":"code","cc2ca5f8":"code","8a119874":"code","f9f6d4e0":"code","483e35c5":"code","1465cc84":"code","69b46bf4":"code","a76b8e3f":"code","eac2ab8a":"code","ad700588":"markdown","979ae1ef":"markdown","2b84e4e3":"markdown","6f65d23f":"markdown","361008a5":"markdown","8ce1638e":"markdown","9fe8ecad":"markdown","6e04c764":"markdown","6deb664d":"markdown","1b1aea93":"markdown","70b958da":"markdown","e7d0db75":"markdown","69eac9d0":"markdown","b2c09c5a":"markdown","f22df9d5":"markdown","b0a6eeb1":"markdown","a092ff1f":"markdown","5fe7399c":"markdown","eeebb6c2":"markdown","ead837fb":"markdown","e689fafb":"markdown","db2df197":"markdown","d6afeb38":"markdown","afd747ff":"markdown","d4562db0":"markdown","01d6af84":"markdown","5aa1a920":"markdown","595a4d97":"markdown","d653f0d3":"markdown","456676a4":"markdown","171e947e":"markdown"},"source":{"5e49c282":"import re\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nfrom wordcloud import STOPWORDS\nimport seaborn as sb\nfrom sklearn.model_selection import StratifiedKFold\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport torch\n","723aaadb":"!pip install nltk==3.4 \nimport nltk","4275771b":"# Load d\u1eef li\u1ec7u hu\u1ea5n luy\u1ec7n v\u00e0 d\u1eef li\u1ec7u test b\u1eb1ng h\u00e0m read_csv t\u1eeb th\u01b0 vi\u1ec7n pandas\ntrain_df = pd.read_csv('..\/input\/quora-insincere-questions-classification\/train.csv')\ntest_df = pd.read_csv('..\/input\/quora-insincere-questions-classification\/test.csv')\n\nprint('S\u1ed1 l\u01b0\u1ee3ng \u0111i\u1ec3m d\u1eef li\u1ec7u \u1edf trong t\u1eadp hu\u1ea5n luy\u1ec7n:', train_df.shape[0])\nprint('S\u1ed1 l\u01b0\u1ee3ng \u0111i\u1ec3m d\u1eef li\u1ec7u \u1edf trong t\u1eadp test:', test_df.shape[0])\n","b105c1e5":"# Xem d\u1eef li\u1ec7u hu\u1ea5n luy\u1ec7n\ntrain_df.head(10)","0c9a85e8":"# ki\u1ec3m tra c\u00e1c \u0111i\u1ec3m d\u1eef li\u1ec7u trong d\u1eef li\u1ec7u hu\u1ea5n luy\u1ec7n c\u00f3 gi\u00e1 tr\u1ecb null n\u00e0o kh\u00f4ng\nprint('S\u1ed1 gi\u00e1 tr\u1ecb null \u1edf c\u1ed9t qid, question_text, target:', train_df['qid'].isnull().sum(),',',\n                                                            train_df['question_text'].isnull().sum(),',',\n                                                            train_df['target'].isnull().sum())\n","9fa0a1ca":"# Xem c\u00e1c sincere question \u1edf trong d\u1eef li\u1ec7u hu\u1ea5n luy\u1ec7n\ntrain_df[train_df['target']==0].head(5)","ca42a6ce":"#Xem c\u00e1c insincere question \u1edf trong d\u1eef li\u1ec7u hu\u1ea5n luy\u1ec7n\ntrain_df[train_df['target']==1].head(5)","2d3b99c5":"#Xem d\u1eef li\u1ec7u test\ntest_df.head(10)","a86bf858":"train_df['target'].value_counts() # V\u1edbi 0 l\u00e0 sincere question, 1 l\u00e0 insincere question","eff71ce8":"# Bi\u1ec3u \u0111\u1ed3 s\u1ed1 l\u01b0\u1ee3ng c\u00e1c c\u00e2u h\u1ecfi\nplt.subplot(1, 2, 1)\ntrain_df.groupby('target')['qid'].count().plot.bar()\nplt.grid(True)\nplt.title('Question Count')\nplt.subplots_adjust(right=1.9)\n\n# Bi\u1ec3u \u0111\u1ed3 ph\u00e2n ph\u1ed1i c\u00e1c c\u00e2u h\u1ecfi \nplt.subplot(1, 2, 2)\nvalues = [train_df[train_df['target']==0].shape[0], train_df[train_df['target']==1].shape[0]]\nlabels = ['Sincere questions', 'Insincere questions']\n\nplt.pie(values, labels=labels, autopct='%1.1f%%', shadow=True)\nplt.title('Question Distribution')\nplt.tight_layout()\nplt.subplots_adjust(right=1.9)\n\nplt.show()","a585ec45":"def pltTopWord(data, title, bar_color, numberOfWordsInTop):\n    \n    top_words = Counter(data).most_common(numberOfWordsInTop) # 25 t\u1eeb xu\u1ea5t hi\u1ec7n nhi\u1ec1u nh\u1ea5t\n\n    df_top = pd.DataFrame(top_words, columns=['word', 'count']).sort_values('count')\n\n    plt.barh(df_top['word'].values, df_top['count'].values, orientation='horizontal', color=bar_color)\n    plt.title(f'Top words in {title}')","8d4b5ae2":"def get_unigrams(data):\n    unigrams = []\n    for sent in data:\n        unigrams.extend([w for w in sent.lower().split() if w not in STOPWORDS]) # kh\u00f4ng l\u1ea5y c\u00e1c t\u1eeb stopword\n    return unigrams\n\ndef get_bigrams(data):\n    bigrams = []\n    for question in data:\n        question = [w for w in question.lower().split() if w not in STOPWORDS] # kh\u00f4ng l\u1ea5y c\u00e1c t\u1eeb stopword\n        if not question: \n            continue # tr\u00e1nh vi\u1ec7c m\u1ea3ng question r\u1ed7ng g\u00e2y ra l\u1ed7i khi d\u00f9ng nltk.bigrmas\n        bi = [b for b in nltk.bigrams(question)]\n        bi = [' '.join(w) for w in bi]\n        bigrams.extend(bi)\n    return bigrams","b66d04f7":"unigrams_sincere  = get_unigrams(train_df[train_df['target']==0]['question_text'])\nunigrams_insincere = get_unigrams(train_df[train_df['target']==1]['question_text'])\n\nplt.subplot(1, 2, 1)\npltTopWord(unigrams_sincere, 'Sincere questions: Unigrams', 'blue',25)\n\n\nplt.subplot(1, 2, 2)\npltTopWord(unigrams_insincere, 'Insincere questions: Unigrams', 'red',25)\n\nplt.subplots_adjust(right=3.0)\nplt.subplots_adjust(top=2.0)\nplt.show()\n","8b7dbba2":"bigrams_sincere = get_bigrams(train_df[train_df['target']==0]['question_text'])\nbigrams_insincere = get_bigrams(train_df[train_df['target']==1]['question_text'])\n\nplt.subplot(1, 2, 1)\npltTopWord(bigrams_sincere, 'Sincere questions: Bigrams', 'blue',25)\n\nplt.subplot(1, 2, 2)\npltTopWord(bigrams_insincere, 'Insincere questions: Bigrams', 'red',25)\n\nplt.subplots_adjust(right=3.0)\nplt.subplots_adjust(top=2.0)\nplt.show()","999c908f":"special_character_list = []\ndef count_special_character_in(text):\n    count = 0\n    for i in range(0, len(text)):\n        if((text[i].isalpha())  or text[i] ==' '):\n            continue\n        elif(text[i].isdigit()):\n            continue   \n        else:\n            special_character_list.extend(text[i])\n            count+=1\n    return count\n\n\n\n# \u0110\u1ed9 d\u00e0i c\u1ee7a c\u00e2u\ntrain_df['question_length'] = train_df['question_text'].apply(lambda ques: len(str(ques)))\n\n# S\u1ed1 l\u01b0\u1ee3ng t\u1eeb c\u1ee7a c\u00e2u\ntrain_df['number_of_words'] = train_df['question_text'].apply(lambda ques: len(str(ques).split()))\nmax_word = train_df['number_of_words'].max()\nprint(max_word)\n\n#S\u1ed1 l\u01b0\u1ee3ng t\u1eeb kh\u00f4ng tr\u00f9ng nhau c\u1ee7a c\u00e2u\ntrain_df['number_of_unique_words'] = train_df['question_text'].apply(lambda ques: len(set(str(ques).split())))\n\n#S\u1ed1 l\u01b0\u1ee3ng c\u00e1c stopwords\ntrain_df['number_of_stopwords'] = train_df['question_text'].apply(lambda ques: len([w for w in str(ques).lower().split() if w in STOPWORDS]))\n\n#S\u1ed1 l\u01b0\u1ee3ng c\u00e1c k\u00fd t\u1ef1 \u0111\u1eb7c bi\u1ec7t\ntrain_df['number_special'] = train_df['question_text'].apply(lambda ques: count_special_character_in(ques))\n\nspecial_character_list = set(special_character_list)\nprint(list(set(special_character_list)))\n\ntrain_df.head(5)\n","07af33f0":"def boxPlot(x,y,data,title):\n    sb.boxplot(x=x,y=y,data=data)\n    plt.grid(True)\n    plt.title(title)\n\n# boxplot: \u0110\u1ed9 d\u00e0i c\u1ee7a c\u00e2u\nplt.subplot(2,3,1)\nboxPlot('target', 'question_length', train_df, 'question_length of each question class')\n\n# boxplot: S\u1ed1 l\u01b0\u1ee3ng t\u1eeb\nplt.subplot(2,3,2)\nboxPlot('target', 'number_of_words', train_df, 'number_of_words of each question class')\n\n# boxplot: S\u1ed1 l\u01b0\u1ee3ng t\u1eeb kh\u00f4ng b\u1ecb tr\u00f9ng\nplt.subplot(2,3,3)\nboxPlot('target', 'number_of_unique_words', train_df, 'number_of_unique_words of each question class')\n\n# boxplot: S\u1ed1 l\u01b0\u1ee3ng stopwords\nplt.subplot(2,3,4)\nboxPlot('target', 'number_of_stopwords', train_df, 'number_of_stopwords of each question class')\n\n# boxplot: S\u1ed1 l\u01b0\u1ee3ng k\u00fd t\u1ef1 \u0111\u1eb7c bi\u1ec7t\nplt.subplot(2,3,5)\nboxPlot('target', 'number_special', train_df, 'number_special of each question class')\n\nplt.subplots_adjust(right=3.0)\nplt.subplots_adjust(top=2.0)\nplt.show()\n\n\n\n","92824a62":"train_df_insincere = pd.DataFrame(train_df[train_df['target']==1], columns =['question_text','target','question_length'])\ntrain_df_insincere.head()","bf14752a":"# Sort t\u1eadp training \u1edf l\u1edbp insincere t\u1eeb l\u1edbn \u0111\u1ebfn nh\u1ecf \ntrain_df_insincere.sort_values('question_length', inplace = True, ascending = False)\ntrain_df_insincere.head(10)","fc1b43b2":"def topQuestionIn(data,noOfQues):\n    listTop = data.tolist()\n    for i in range(0,noOfQues):\n        print('Question',i)\n        print(listTop[i]+'\\n')","751b3f06":"topQuestionIn(train_df_insincere['question_text'],10)  ","65098e63":"# L\u00e0m t\u01b0\u01a1ng t\u1ef1 v\u1edbi l\u1edbp sincere\ntrain_df_sincere = pd.DataFrame(train_df[train_df['target']==0], columns =['question_text','target','question_length'])\ntrain_df_sincere.sort_values('question_length', inplace = True, ascending = False)\n\ntopQuestionIn(train_df_sincere['question_text'],10)  \n\n","803d8749":"# Lo\u1ea1i b\u1ecf c\u00e1c k\u00fd t\u1ef1 \u0111\u1eb7c bi\u1ec7t\ndef remove_special_character(text):\n    text = str(text)\n    for c in special_character_list: # special_character_list khi ch\u1ea1y boxplot k\u00fd t\u1ef1 \u0111\u1eb7c bi\u1ec7t\n        if c in text:\n            text = text.replace(c,'')\n    return text","3e637e14":"#X\u1eed l\u00fd c\u00e1c misspell \nmispell_dict = {'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor', 'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ', 'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What', 'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can', 'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doI': 'do I', 'theBest': 'the best', 'howdoes': 'how does', 'mastrubation': 'masturbation', 'mastrubate': 'masturbate', \"mastrubating\": 'masturbating', 'pennis': 'penis', 'Etherium': 'bitcoin', 'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend', 'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization', 'demonitization': 'demonetization', 'demonetisation': 'demonetization', \n                'electroneum':'bitcoin','nanodegree':'degree','hotstar':'star','dream11':'dream','ftre':'fire','tensorflow':'framework','unocoin':'bitcoin',\n                'lnmiit':'limit','unacademy':'academy','altcoin':'bitcoin','altcoins':'bitcoin','litecoin':'bitcoin','coinbase':'bitcoin','cryptocurency':'cryptocurrency',\n                'simpliv':'simple','quoras':'quora','schizoids':'psychopath','remainers':'remainder','twinflame':'soulmate','quorans':'quora','brexit':'demonetized',\n                'iiest':'institute','dceu':'comics','pessat':'exam','uceed':'college','bhakts':'devotee','boruto':'anime',\n                'cryptocoin':'bitcoin','blockchains':'blockchain','fiancee':'fiance','redmi':'smartphone','oneplus':'smartphone','qoura':'quora','deepmind':'framework','ryzen':'cpu','whattsapp':'whatsapp',\n                'undertale':'adventure','zenfone':'smartphone','cryptocurencies':'cryptocurrencies','koinex':'bitcoin','zebpay':'bitcoin','binance':'bitcoin','whtsapp':'whatsapp',\n                'reactjs':'framework','bittrex':'bitcoin','bitconnect':'bitcoin','bitfinex':'bitcoin','yourquote':'your quote','whyis':'why is','jiophone':'smartphone',\n                'dogecoin':'bitcoin','onecoin':'bitcoin','poloniex':'bitcoin','7700k':'cpu','angular2':'framework','segwit2x':'bitcoin','hashflare':'bitcoin','940mx':'gpu',\n                'openai':'framework','hashflare':'bitcoin','1050ti':'gpu','nearbuy':'near buy','freebitco':'bitcoin','antminer':'bitcoin','filecoin':'bitcoin','whatapp':'whatsapp',\n                'empowr':'empower','1080ti':'gpu','crytocurrency':'cryptocurrency','8700k':'cpu','whatsaap':'whatsapp','g4560':'cpu','payymoney':'pay money',\n                'fuckboys':'fuck boys','intenship':'internship','zcash':'bitcoin','demonatisation':'demonetization','narcicist':'narcissist','mastuburation':'masturbation',\n                'trignometric':'trigonometric','cryptocurreny':'cryptocurrency','howdid':'how did','crytocurrencies':'cryptocurrencies','phycopath':'psychopath',\n                'bytecoin':'bitcoin','possesiveness':'possessiveness','scollege':'college','humanties':'humanities','altacoin':'bitcoin','demonitised':'demonetized',\n                'bras\u00edlia':'brazilia','accolite':'accolyte','econimics':'economics','varrier':'warrier','quroa':'quora','statergy':'strategy','langague':'language',\n                'splatoon':'game','7600k':'cpu','gate2018':'gate 2018','in2018':'in 2018','narcassist':'narcissist','jiocoin':'bitcoin','hnlu':'hulu','7300hq':'cpu',\n                'weatern':'western','interledger':'blockchain','deplation':'deflation', 'cryptocurrencies':'cryptocurrency', 'bitcoin':'blockchain cryptocurrency',}\n\ndef correct_mispell(x):\n    words = x.split()\n    for i in range(0, len(words)):\n        if mispell_dict.get(words[i]) is not None:\n            words[i] = mispell_dict.get(words[i])\n        elif mispell_dict.get(words[i].lower()) is not None:\n            words[i] = mispell_dict.get(words[i].lower())\n        \n    words = \" \".join(words)\n    return words","046d7b03":"#Lo\u1ea1i b\u1ecf c\u00e1c stopwords\ndef remove_stopwords(text):\n    text = [word for word in text.split() if word not in STOPWORDS]\n    text = ' '.join(text)\n    return text","9da418b3":"#X\u1eed l\u00fd c\u00e1c contraction\ncontraction_map = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }\ndef clean_contractions(text):\n    specials = [\"\u2019\", \"\u2018\", \"\u00b4\", \"`\"]\n    for s in specials:\n        text = text.replace(s, \"'\")\n    \n    text = ' '.join([contraction_map[t] if t in contraction_map else t for t in text.split(\" \")])\n    return text\n    ","63d5a616":"# lo\u1ea1i b\u1ecf s\u1ed1\ndef clean_numbers(text):\n    text = re.sub('[0-9]{5,}', '#####', text)\n    text = re.sub('[0-9]{4}', '####', text)\n    text = re.sub('[0-9]{3}', '###', text)\n    text = re.sub('[0-9]{2}', '##', text)\n    return text","33ca519d":"#Lemamatizing\n\"\"\"\nword_lemmatizer = WordNetLemmatizer()\ndef lemma_word(text):\n    text = text.split()\n    text =[word_lemmatizer.lemmatize(w) for w in text]\n    return ' '.join(text)\n\"\"\"","f1bd69ad":"# X\u00f3a b\u1ecf c\u00e1c tag to\u00e1n h\u1ecdc, \u0111\u01b0\u1eddng d\u1eabn \ndef clean_tag(x):\n    if '[math]' in x:\n        x = re.sub('\\[math\\].*?math\\]', 'MATH EQUATION', x) #replacing with [MATH EQUATION]\n    \n    if 'http' in x or 'www' in x:\n        x = re.sub('(?:(?:https?|ftp):\\\/\\\/)?[\\w\/\\-?=%.]+\\.[\\w\/\\-?=%.]+', 'URL', x) #replacing with [url]\n    return x","bdea416f":"def clean_data(text):\n    text = clean_tag(text)\n    text = clean_contractions(text)\n    text = correct_mispell(text)\n    text = clean_numbers(text)\n    text = remove_special_character(text)\n    return text","562cb01f":"train_df['preprocessed_question_text'] = train_df['question_text'].apply(lambda x: clean_data(x))\ntest_df['preprocessed_question_text'] = test_df['question_text'].apply(lambda x: clean_data(x))","58f078d1":"train_df.head(20)","e774698d":"test_df.head(20)","01a657bd":"embed_size = 300\nmax_features = 150000\nmaxlen = 70\nn_epochs = 10\nn_splits = 5\n\nbatch_size = 512\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","4b802979":"train_X = train_df['preprocessed_question_text'].values\ntest_X = test_df['preprocessed_question_text'].values\ntokenizer = Tokenizer(num_words = max_features)\n\n#tokenizer t\u1eeb trong t\u1eadp train v\u00e0 t\u1eadp hu\u1ea5n luy\u1ec7n\ntokenizer.fit_on_texts(list(train_X)+list(test_X))\n\ntrain_X = tokenizer.texts_to_sequences(train_X)\ntest_X = tokenizer.texts_to_sequences(test_X)\n\n# t\u1ea1o m\u1ed9t dict \u0111\u1ec3 mapping t\u1eeb v\u00e0 s\u1ed1 th\u1ee9 t\u1ef1\nword_index = tokenizer.word_index # type: dict\n\n# reize l\u1ea1i m\u1ea3ng \u0111\u1ec3 c\u00f3 \u0111\u1ed9 d\u00e0i b\u1eb1ng nhau\ntrain_X = pad_sequences(train_X,maxlen = maxlen)\ntest_X = pad_sequences(test_X, maxlen=maxlen)\n\ntrain_Y = train_df['target'].values","0fa1f9af":"!unzip ..\/input\/quora-insincere-questions-classification\/embeddings.zip","a9ccf0af":"def load_glove(word_index):\n    EMBEDDING_FILE = '.\/glove.840B.300d\/glove.840B.300d.txt'\n    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')[:300]\n    \n    f = open(EMBEDDING_FILE,encoding=\"utf-8\")\n    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in f)\n    \n    all_embs = np.stack(embeddings_index.values())\n    emb_mean,emb_std = -0.005838499,0.48782197\n    embed_size = all_embs.shape[1]\n\n    # word_index = tokenizer.word_index\n    nb_words = min(max_features, len(word_index))\n    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n    for word, i in word_index.items():\n        if i >= max_features: continue\n        embedding_vector = embeddings_index.get(word)\n        #ALLmight\n        if embedding_vector is not None: \n            embedding_matrix[i] = embedding_vector\n        else:\n            embedding_vector = embeddings_index.get(word.capitalize())\n            if embedding_vector is not None: \n                embedding_matrix[i] = embedding_vector\n    return embedding_matrix ","091e0eb6":"glove_embedding = load_glove(tokenizer.word_index)\n#fasttext_embedding = load_wiki(tokenizer.word_index)\n#para_embedding = load_para(tokenizer.word_index)\n#embedding_matrix = np.mean([glove_embedding, fasttext_embedding, para_embedding], axis = 0)\nembedding_matrix = glove_embedding","cc2ca5f8":"print(embedding_matrix.shape)","8a119874":"splits = list(StratifiedKFold(n_splits=5, shuffle=True, random_state=2000).split(train_X, train_Y))","f9f6d4e0":"device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') \nprint(device)","483e35c5":"class LSTM_Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n        self.embedding = nn.Embedding(150000, 300)\n        self.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))\n        self.embedding.weight.requires_grad = False\n        self.lstm = nn.LSTM(300, 64, bidirectional=True, batch_first=True)\n        self.fc1 = nn.Linear(64*2 , 64)\n        self.fc2 = nn.Linear(64,1)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(0.1)\n    \n    def forward(self, x):\n        out = self.embedding(x)\n        out, (h,c) = self.lstm(out)\n        cat = torch.cat((h[-2,:,:],h[-1,:,:]),dim=1) # \u0111\u1ea7u ra c\u1ee7a hai h\u01b0\u1edbng LSTM s\u1ebd \u0111\u01b0\u1ee3c k\u1ebft h\u1ee3p \u1edf tr\u1ea1ng th\u00e1i cu\u1ed1i \n        out = self.relu(cat)\n        out = self.dropout(self.fc1(out))\n        out = self.fc2(out)\n        return out\n","1465cc84":"# \u1ede ph\u1ea7n th\u1ef1c hi\u1ec7n training em tham kh\u1ea3o \u0111o\u1ea1n code: https:\/\/www.kaggle.com\/oysiyl\/107-place-solution-using-public-kernel","69b46bf4":"train_epochs = 6\nx_test = torch.tensor(test_X, dtype=torch.long).to(device)\ntest = torch.utils.data.TensorDataset(x_test)\ntest_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\n\n\n\ntrain_preds= np.zeros((len(train_X)))\ntest_preds = np.zeros((len(test_X)))\nfor i_fold, (train_idx, valid_idx) in enumerate(splits):\n\n    x_train_fold = torch.tensor(train_X[train_idx], dtype=torch.long).to(device)\n    y_train_fold = torch.tensor(train_Y[train_idx, np.newaxis], dtype=torch.float32).to(device)\n    x_val_fold = torch.tensor(train_X[valid_idx], dtype=torch.long).to(device)\n    y_val_fold = torch.tensor(train_Y[valid_idx, np.newaxis], dtype=torch.float32).to(device)\n    \n    train = torch.utils.data.TensorDataset(x_train_fold, y_train_fold)\n    valid = torch.utils.data.TensorDataset(x_val_fold, y_val_fold)\n    \n    #Load data \n    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n    valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n    \n    # m\u00f4 h\u00ecnh \n    model = LSTM_Model().to(device)\n    # h\u00e0m loss v\u00e0 adam optimizer\n    criterion = torch.nn.BCEWithLogitsLoss(reduction='sum')\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n    print(f'Fold {i_fold + 1}')\n    \n    avg_losses_f = []\n    avg_val_losses_f = []\n\n    for epoch in range(train_epochs):\n        model.train()\n        avg_loss = 0.\n        for batch_i,(x_batch, y_batch) in enumerate(train_loader):\n            # lan truy\u1ec1n t\u1edbi \n            y_pred = model(x_batch)\n            # t\u00ednh h\u00e0m m\u1ea5t m\u00e1t\n            loss = criterion(y_pred, y_batch)\n\n            optimizer.zero_grad()\n            # lan truy\u1ec1n ng\u01b0\u1ee3c \n            loss.backward()\n            # c\u1eadp nh\u1eadt l\u1ea1i tr\u1ecdng s\u1ed1 m\u00f4 h\u00ecnh\n            optimizer.step()\n            \n            avg_loss += loss.item() \/ len(train_loader)\n            \n        # Ki\u1ec3m tra m\u00f4 h\u00ecnh v\u1edbi t\u1eadp validation \n        model.eval()\n        valid_preds_fold = np.zeros(x_val_fold.size(0))\n        test_preds_fold = np.zeros(len(test_X))\n        \n        avg_val_loss = 0.\n        for batch_i, (x_batch, y_batch) in enumerate(valid_loader):\n            y_pred = model(x_batch).detach()\n            avg_val_loss += criterion(y_pred, y_batch).item() \/ len(valid_loader)\n            valid_preds_fold[batch_i * batch_size:(batch_i+1) * batch_size] = torch.sigmoid(y_pred.cpu())[:, 0]\n\n        print('Epoch {}\/{} \\t loss={:.2f} \\t val_loss={:.2f}'.format(epoch + 1, train_epochs, avg_loss, avg_val_loss))\n        avg_losses_f.append(avg_loss)\n        avg_val_losses_f.append(avg_val_loss)\n    # Bi\u1ec3u \u0111\u1ed3 sau m\u1ed7i fold\n    plt.figure(figsize=(10,5))\n    plt.title(f'Loss During Training in Fold {i_fold + 1}')\n    plt.plot(avg_losses_f,label=\"training\")\n    plt.plot(avg_val_losses_f,label=\"valid\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.show()\n\n    # D\u1ef1 \u0111o\u00e1n t\u1eadp test sau m\u1ed7i fold \n    for batch_i, (x_batch,) in enumerate(test_loader):\n        y_pred = model(x_batch).detach()\n        test_preds_fold[batch_i * batch_size:(batch_i+1) * batch_size] = torch.sigmoid(y_pred.cpu())[:, 0]\n\n    train_preds[valid_idx] = valid_preds_fold\n    test_preds += test_preds_fold \/ len(splits)\n","a76b8e3f":"from sklearn.metrics import f1_score\ndef bestThresshold(y_train,train_preds_):\n    thres = 0.0\n    max_f1 = 0.0 \n    for t in np.arange(0.1, 0.5, 0.01):\n        t = np.round(t,2)\n        f1_score_ = f1_score(y_train,np.array(train_preds_)>t)\n        if f1_score_ > max_f1:\n            thres = t\n            max_f1 = f1_score_\n    print(thres,max_f1)\n    return thres \n\nbestThres = bestThresshold(train_Y,train_preds)","eac2ab8a":"submission = test_df[['qid']].copy()\nsubmission['prediction'] = (test_preds > bestThres).astype(int)\nsubmission.to_csv('submission.csv', index=False)","ad700588":"## 2.2. Tr\u1ef1c quan h\u00f3a d\u1eef li\u1ec7u","979ae1ef":"\nTa s\u1eed d\u1ee5ng ph\u01b0\u01a1ng ph\u00e1p N-grams \u0111\u1ec3 t\u00ecm c\u00e1c t\u1eeb\/ c\u1ee5m t\u1eeb c\u00f3 t\u1ea7n su\u1ea5t \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng nhi\u1ec1u nh\u1ea5t trong t\u1eebng lo\u1ea1i c\u00e2u h\u1ecfi. \n\n\u1ede \u0111\u00e2y, ta s\u1eed d\u1ee5ng 1-grams (unigrams) v\u00e0 2-grams (bigrams). \nV\u00ed d\u1ee5: \"h\u00f4m nay tr\u1eddi \u0111\u1eb9p\". \n* unigrams: \"h\u00f4m\",\"nay\",\"tr\u1eddi\",\"\u0111\u1eb9p\"\n* bigrams: \"h\u00f4m nay\", \"nay tr\u1eddi\", \"tr\u1eddi \u0111\u1eb9p\"","2b84e4e3":"# 5. K\u1ebft lu\u1eadn\n* M\u00f4 h\u00ecnh BiLSTM cho ra s\u1ed1 \u0111i\u1ec3m t\u1ed1t h\u01a1n nhi\u1ec1u so v\u1edbi m\u00f4 h\u00ecnh h\u1ed3i quy logistc\n* C\u00f3 th\u1ec3 k\u1ebft h\u1ee3p nhi\u1ec1u pre trained embedding \u0111\u1ec3 c\u1ea3i thi\u1ec7n ch\u1ea5t l\u01b0\u1ee3ng trong qu\u00e1 tr\u00ecnh preprocessing","6f65d23f":"Ta s\u1ebd s\u1eed d\u1ee5ng b\u1ed9 pre trained embedding c\u1ee7a glove \u0111\u00e3 \u0111\u01b0\u1ee3c cho s\u1eb5n b\u1edfi cu\u1ed9c thi ","361008a5":"#### Tr\u1ef1c quan h\u00f3a s\u1ed1 l\u01b0\u1ee3ng c\u00e1c c\u00e2u h\u1ecfi th\u00f4ng qua bar chart v\u00e0 pie chart","8ce1638e":"# 3. Ti\u1ec1n x\u1eed l\u00fd v\u00e0 clean d\u1eef li\u1ec7u ","9fe8ecad":"Ta s\u1eed d\u1ee5ng box plot \u0111\u1ec3 ph\u00e2n t\u00edch c\u00e1c \u0111\u1eb7c tr\u01b0ng. Box plot l\u00e0 m\u1ed9t d\u1ea1ng bi\u1ec3u \u0111\u1ed3 th\u1ec3 hi\u1ec7n ph\u00e2n ph\u1ed1i d\u1eef li\u1ec7u c\u00e1c thu\u1ed9c t\u00ednh th\u00f4ng qua c\u00e1c t\u1ee9 ph\u00e2n v\u1ecb. Box plot th\u1ec3 hi\u1ec7n c\u00e1c ph\u00e2n ph\u1ed1i d\u1eef li\u1ec7u, gi\u00fap ta bi\u1ebft \u0111\u1ed9 ph\u00e2n b\u1ed1 c\u1ee7a c\u00e1c \u0111i\u1ec3m d\u1eef li\u1ec7u nh\u01b0 th\u1ebf n\u00e0o, d\u1eef li\u1ec7u c\u00f3 ph\u00e2n b\u1ed1 r\u1ed9ng hay h\u1eb9p, gi\u00e1 tr\u1ecb l\u1edbn nh\u1ea5t v\u00e0 c\u00e1c \u0111i\u1ec3m ngo\u1ea1i l\u1ec7.\n\n<img src=\"https:\/\/miro.medium.com\/max\/1400\/1*2c21SkzJMf3frPXPAR_gZA.png\">\n\nBi\u1ec3u \u0111\u1ed3 Boxplot th\u1ec3 hi\u1ec7n 6 th\u00f4ng s\u1ed1:\n* Median: Trung v\u1ecb c\u1ee7a t\u1eadp d\u1eef li\u1ec7u, t\u1ee9c l\u00e0 gi\u00e1 tr\u1ecb \u1edf ph\u1ea7n t\u1eed gi\u1eefa\n* First quartile (Q1): Trung v\u1ecb gi\u1eefa Median v\u00e0 ph\u1ea7n t\u1eed nh\u1ecf nh\u1ea5t trong t\u1eadp d\u1eef li\u1ec7u. C\u00f2n g\u1ecdi l\u00e0 25th Percentile.\n* Third quartile (Q3): Trung v\u1ecb gi\u1eefa Median v\u00e0 ph\u1ea7n t\u1eed l\u1edbn nh\u1ea5t trong t\u1eadp d\u1eef li\u1ec7u. C\u00f2n g\u1ecdi l\u00e0 75th Percentile.\n* Minimum: Ph\u1ea7n t\u1eed nh\u1ecf nh\u1ea5t kh\u00f4ng ph\u1ea3i ngo\u1ea1i l\u1ec7.\n* Maximum: Ph\u1ea7n t\u1eed l\u1edbn nh\u1ea5t kh\u00f4ng ph\u1ea3i l\u00e0 ngo\u1ea1i l\u1ec7.\n* Outliers: C\u00e1c ngo\u1ea1i l\u1ec7, l\u00e0 c\u00e1c \u0111i\u1ec3m d\u1eef li\u1ec7u kh\u00e1c bi\u1ec7t \u0111\u00e1ng k\u1ec3 so v\u1edbi c\u00e1c \u0111i\u1ec3m d\u1eef li\u1ec7u c\u00f2n l\u1ea1i\n\n","6e04c764":"Thu\u1eadt to\u00e1n hu\u1ea5n luy\u1ec7n:\n* L\u1ea5y m\u1ed9t fold trong t\u1eadp fold \u0111\u00e3 \u0111\u01b0\u1ee3c chia:\n    * L\u1eb7p epoch = 1,2,...\n        * m\u00f4 h\u00ecnh th\u1ef1c hi\u1ec7n forward\n        * T\u00ednh \u0111\u1ea1o h\u00e0m v\u00e0 c\u1eadp nh\u1eadt tr\u1ecdng s\u1ed1 c\u1ee7a m\u00f4 h\u00ecnh\n        ","6deb664d":"### 2.2.1.1. C\u00e1c t\u1eeb c\u00f3 t\u1ea7n su\u1ea5t xu\u1ea5t hi\u1ec7n nhi\u1ec1u nh\u1ea5t trong sincere v\u00e0 insincere questions","1b1aea93":"**Nh\u1eadn x\u00e9t:** \n* Ta th\u1ea5y c\u00e2u c\u00f3 \u0111\u1ed9 d\u00e0i l\u1edbn nh\u1ea5t c\u1ee7a insincere l\u00e0 \u0111\u01b0\u1ee3c vi\u1ebft b\u1eb1ng c\u00f4ng th\u1ee9c to\u00e1n h\u1ecdc Latex\n* C\u00e2u h\u1ecfi c\u00f3 \u0111\u1ed9 d\u00e0i 800 t\u1eeb c\u1ee7a sincere l\u00e0 m\u1ed9t c\u00e2u h\u1ecfi li\u00ean quan \u0111\u1ebfn phim \u1ea3nh, c\u00f2n c\u00e2u h\u1ecfi c\u00f3 \u0111\u1ed9 d\u00e0i th\u1ee9 nh\u00ec l\u00e0 m\u1ed9t c\u00e2u h\u1ecfi to\u00e1n h\u1ecdc Latex \n* Ta chuy\u1ec3n \u0111\u1ed5i 3 c\u00e2u h\u1ecfi to\u00e1n h\u1ecdc \u1edf trong top c\u00e1c t\u1eeb d\u00e0i nh\u1ea5t \u1edf 2 l\u1edbp sang d\u1ea1ng latex view: \n    * 2 h\u00ecnh \u1ea3nh \u1edf l\u1edbp insincere:\n    \n    ![Screen Shot 2022-01-03 at 18.06.34.png](attachment:e8894300-0bca-4f46-ba33-11049bd35dc8.png)\n    ![Screen Shot 2022-01-03 at 18.07.52.png](attachment:19d43e51-be48-4424-a0c5-ac7d235a6ccd.png)\n    \n    * h\u00ecnh \u1ea3nh \u1edf l\u1edbp sincere:\n    \n    ![Screen Shot 2022-01-03 at 18.14.41.png](attachment:8cb76091-f684-4226-8438-0df9a900bf0d.png)\n    \n* Ta th\u1ea5y c\u00e1c c\u00e2u h\u1ecfi to\u00e1n h\u1ecdc \u0111\u01b0\u1ee3c \u0111\u00e1nh d\u1ea5u \u1edf hai l\u1edbp \u1edf tr\u00ean kh\u00f4ng c\u00f3 v\u1ea5n \u0111\u1ec1 g\u00ec \u0111\u00e1ng ng\u1edd. Tuy nhi\u00ean c\u00f3 kh\u1ea3 n\u0103ng v\u00ec nhi\u1ec1u k\u00fd t\u1ef1 \u0111\u1eb7c bi\u1ec7t n\u00ean ch\u00fang b\u1ecb \u0111\u00e1nh d\u1ea5u insincere\n    \n    \n    \n","70b958da":"### 2.2.1.2 M\u1ed9t v\u00e0i \u0111\u1eb7c tr\u01b0ng c\u01a1 b\u1ea3n ","e7d0db75":"**Nh\u1eadn x\u00e9t**:\n* C\u00e1c insincere question c\u00f3 \u0111\u1ed9 d\u00e0i c\u00e2u h\u1ecfi, s\u1ed1 l\u01b0\u1ee3ng t\u1eeb, s\u1ed1 l\u01b0\u1ee3ng k\u00fd t\u1ef1 \u0111\u1eb7c bi\u1ec7t nh\u00ecn chung l\u1edbn h\u01a1n so v\u1edbi c\u00e1c sincere question (ngo\u1ea1i tr\u1eeb m\u1ed9t v\u00e0i outliers c\u1ee7a sincere).\n    * insincere c\u00f3 th\u1ec3 l\u00e0 c\u00e1c c\u00e2u h\u1ecfi spam, qu\u1ea3ng c\u00e1o,...\n    * D\u1ef1a v\u00e0o s\u1ed1 l\u01b0\u1ee3ng k\u00fd hi\u1ec7u \u0111\u1eb7c bi\u1ec7t, insincere c\u00f3 th\u1ec3 l\u00e0 c\u00e1c c\u00f4ng th\u1ee9c to\u00e1n h\u1ecdc latex, c\u00e1c c\u00e2u h\u1ecfi ch\u1ee9a c\u00e1c icon,... \n* Ta nh\u00ecn th\u1ea5y outliers \u1edf boxplot question_length c\u1ee7a sincere x\u1ea5p s\u1ec9 800. Ta s\u1ebd xem \u0111\u00e2y l\u00e0 c\u00e2u h\u1ecfi nh\u01b0 n\u00e0o. \u0110\u1ed3ng th\u1eddi ta s\u1ebd xem c\u00e2u h\u1ecfi c\u00f3 \u0111\u1ed9 d\u00e0i l\u1edbn nh\u1ea5t l\u00e0 1000 c\u1ee7a insincere l\u00e0 c\u00e2u h\u1ecfi nh\u01b0 n\u00e0o.\n","69eac9d0":"# 1. M\u00f4 t\u1ea3 b\u00e0i to\u00e1n \nQuora l\u00e0 m\u1ed9t n\u1ec1n t\u1ea3ng cho ph\u00e9p m\u1ecdi ng\u01b0\u1eddi h\u1ecdc h\u1ecfi l\u1eabn nhau. Tr\u00ean Quora, m\u1ecdi ng\u01b0\u1eddi c\u00f3 th\u1ec3 \u0111\u1eb7t c\u00e2u h\u1ecfi \u0111\u1ec3 nh\u1eefng ng\u01b0\u1eddi kh\u00e1c t\u01b0\u01a1ng t\u00e1c, \u0111\u01b0a ra nh\u1eefng c\u00e2u tr\u1ea3 l\u1eddi ch\u1ea5t l\u01b0\u1ee3ng. B\u00e0i to\u00e1n \u0111\u1eb7t ra l\u00e0 l\u00e0m th\u1ebf n\u00e0o \u0111\u1ec3 lo\u1ea1i b\u1ecf nh\u1eefng c\u00e2u h\u1ecfi thi\u1ebfu th\u00e0nh th\u1eadt (insincere question) - nh\u1eefng c\u00e2u h\u1ecfi d\u1ef1a tr\u00ean nh\u1eefng \u0111\u1ecbnh ki\u1ebfn sai l\u1ea7m ho\u1eb7c c\u00f3 \u00fd \u0111\u1ecbnh \u0111\u01b0a ra m\u1ed9t tuy\u00ean b\u1ed1 h\u01a1n l\u00e0 t\u00ecm ki\u1ebfm nh\u1eefng c\u00e2u tr\u1ea3 l\u1eddi h\u1eefu \u00edch","b2c09c5a":"Ta s\u1eed d\u1ee5ng ph\u01b0\u01a1ng ph\u00e1p K-fold, \u0111\u1ec3 chia t\u1eadp training. Ph\u1ea7n d\u1eef li\u1ec7u training th\u00ec s\u1ebd \u0111\u01b0\u1ee3c chia th\u00e0nh K ph\u1ea7n (K \u1edf \u0111\u00e2y ta cho b\u1eb1ng 5). Sau \u0111\u00f3 train model K l\u1ea7n, m\u1ed7i l\u1ea7n train s\u1ebd ch\u1ecdn 1 ph\u1ea7n l\u00e0m d\u1eef li\u1ec7u validation v\u00e0 K-1 ph\u1ea7n con l\u1ea1i l\u00e0m training set. K\u1ebft qu\u1ea3 cu\u1ed1i c\u00f9ng s\u1ebd l\u00e0 trung b\u00ecnh c\u1ed9ng k\u1ebft qu\u1ea3 \u0111\u00e1nh gi\u00e1 c\u1ee7a K l\u1ea7n train\n\n\n![](https:\/\/web888.vn\/wp-content\/uploads\/2021\/09\/image-108.png)\n","f22df9d5":"Sau khi lo\u1ea1i b\u1ecf c\u00e1c stopwords, v\u1edbi c\u00e1c sincere question, c\u00e1c t\u1eeb xu\u1ea5t hi\u1ec7n nhi\u1ec1u nh\u1ea5t th\u01b0\u1eddng l\u00e0 nh\u1eefng t\u1eeb mang ngh\u0129a t\u00edch c\u1ef1c nh\u01b0: best, possible, people, etc. V\u1edbi c\u00e1c insincere question, c\u00e1c t\u1eeb xu\u1ea5t hi\u1ec7n nhi\u1ec1u nh\u1ea5t th\u01b0\u1eddng l\u00e0: donald trump, black, indian.","b0a6eeb1":"# 2. Ph\u00e2n t\u00edch d\u1eef li\u1ec7u\n## 2.1 T\u1ed5ng quan v\u1ec1 d\u1eef li\u1ec7u","a092ff1f":"B\u1ed9 d\u1eef li\u1ec7u hu\u1ea5n luy\u1ec7n g\u1ed3m 3 c\u1ed9t: qid, question_text (n\u1ed9i dung c\u00e2u h\u1ecfi) v\u00e0 target (l\u00e0 gi\u00e1 tr\u1ecb nh\u1ecb ph\u00e2n 0,1, v\u1edbi 0 l\u00e0 sincere question, 1 l\u00e0 insincere question)","5fe7399c":"Ta in ra 10 c\u00e2u h\u1ecfi d\u00e0i nh\u1ea5t \u1edf l\u1edbp insincere","eeebb6c2":"C\u00e1c c\u00e2u h\u1ecfi insincere s\u1ebd \u0111\u01b0\u1ee3c \u0111\u00e1nh d\u1ea5u l\u00e0 1, v\u00e0 c\u00e1c c\u00e2u h\u1ecfi c\u00f2n l\u1ea1i \u0111\u01b0\u1ee3c \u0111\u00e1nh d\u1ea5u l\u00e0 0","ead837fb":"#### Ph\u00e2n t\u00edch c\u00e1c \u0111\u1eb7c tr\u01b0ng","e689fafb":"Metrics \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng \u0111\u1ec3 \u0111\u00e1nh g\u00eda m\u00f4 h\u00ecnh \u1edf \u0111\u00e2y cho c\u00e1c file submisson l\u00e0 \u0111i\u1ec3m F1","db2df197":"Sau khi ti\u1ec1n x\u1eed l\u00fd, l\u00e0m s\u1ea1ch d\u1eef li\u1ec7u. Ta s\u1ebd ti\u1ebfn h\u00e0nh c\u00e1c b\u01b0\u1edbc nh\u01b0 sau\n\n![](https:\/\/miro.medium.com\/max\/1218\/1*zsIXWoN0_CE9PXzmY3tIjQ.png)\n\n\n","d6afeb38":"# 4. M\u00f4 h\u00ecnh s\u1eed d\u1ee5ng\n## 4.1 M\u00f4 h\u00ecnh h\u1ed3i quy Logistic\n* \u0110\u00e3 \u0111\u01b0\u1ee3c gi\u1ea3ng vi\u00ean d\u1ea1y: https:\/\/excessive-source-1c9.notion.site\/16-09-2021-H-i-quy-Logistics-cdcc911147e5458ba9203b58e6bd0099\n* Notebook cho x\u1eed l\u00fd b\u00e0i to\u00e1n b\u1eb1ng m\u00f4 h\u00ecnh h\u1ed3i quy logistic: https:\/\/www.kaggle.com\/dinhvietahn19021217\/toxic-quora-question\/notebook\n* \u0110i\u1ec3m s\u1ed1 \u0111\u1ea1t \u0111\u01b0\u1ee3c: private score: 0.36198, public score: 0.36785\n\n\n## 4.2 M\u00f4 h\u00ecnh Bidirectional LSTM\n#### RNN (Recuurent Neural Networks)\n* RNN l\u00e0 m\u1ed9t m\u1ea1ng neural networks cho ph\u00e9p c\u00e1c \u0111\u1ea7u ra tr\u01b0\u1edbc \u0111\u00f3 \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng l\u00e0m \u0111\u1ea7u v\u00e0o khi c\u00f3 c\u00e1c tr\u1ea1ng th\u00e1i \u1ea9n. \n* Trong l\u00fd thuy\u1ebft ng\u00f4n ng\u1eef, ng\u1eef ngh\u0129a c\u1ee7a c\u00e2u \u0111\u01b0\u1ee3c t\u1ea1o th\u00e0nh t\u1eeb m\u1ed9t chu\u1ed7i c\u00e1c t\u1eeb trong c\u00e2u theo m\u1ed9t c\u1ea5u tr\u00fac ng\u1eef ph\u00e1p. Do \u0111\u00f3 c\u1ea7n ph\u1ea3i c\u00f3 m\u1ed9t ki\u1ebfn tr\u00fac m\u1ea1ng \u0111\u1eb7c bi\u1ec7t cho neural netwwork bi\u1ec3u di\u1ec5n chu\u1ed7i t\u1eeb n\u00e0y nh\u1eb1m m\u1ee5c \u0111\u00edch li\u00ean k\u1ebft c\u00e1c t\u1eeb li\u1ec1n tr\u01b0\u1edbc v\u1edbi c\u00e1c t\u1eeb hi\u1ec7n t\u1ea1i \u0111\u1ec3 t\u1ea1o ra m\u1ed1i li\u00ean h\u1ec7 x\u00e2u chu\u1ed7i. RNN \u0111\u00e3 \u0111\u01b0\u1ee3c thi\u1ebft k\u1ebf \u0111\u1ec3 gi\u1ea3i quy\u1ebft y\u00eau c\u1ea7u n\u00e0y\n\n##### M\u00f4 h\u00ecnh\n![Screen Shot 2022-01-08 at 12.09.51.png](attachment:6ebdd3b4-d466-4293-a079-8d60b910f7cd.png)\n\n* M\u00f4 h\u00ecnh g\u1ed3m t input, c\u00e1c input \u0111\u01b0\u1ee3c \u0111\u01b0a v\u00e0o m\u00f4 h\u00ecnh \u0111\u00fang v\u1edbi th\u1ee9 t\u1ef1 t\u1eeb trong c\u00e2u\n* M\u1ed7i h\u00ecnh vu\u00f4ng \u0111\u01b0\u1ee3c g\u1ecdi l\u00e0 1 state, \u0111\u1ea7u v\u00e0o m\u1ed7i state l\u00e0 $ x_t $, $h_{t-1}$ v\u1edbi $h_t = f(W*x_t + U*h_{t-1})$. (W l\u00e0 tr\u1ecdng s\u1ed1 c\u1ee7a \u0111\u1ea7u v\u00e0o, U l\u00e0 tr\u1ecdng s\u1ed1 c\u1ee7a tr\u1ea1ng th\u00e1i \u1ea9n), $f$ l\u00e0 activation value nh\u01b0: sigmoid, tanh, ReLU,....\n* C\u00f3 th\u1ec3 th\u1ea5y $h_t$ mang c\u1ea3 th\u00f4ng tin t\u1eeb hidden state tr\u01b0\u1edbc \n* $h_0$ \u0111\u01b0\u1ee3c th\u00eam v\u00e0o \u0111\u1ec3 cho chu\u1ea9n c\u00f4ng th\u1ee9c n\u00ean th\u01b0\u1eddng \u0111\u01b0\u1ee3c g\u00e1n b\u1eb1ng 0 ho\u1eb7c gi\u00e1 tr\u1ecb ng\u1eabu nhi\u00ean\n* $y_t = g(V*h_t)$. V l\u00e0 tr\u1ecdng s\u1ed1 c\u1ee7a tr\u1ea1ng th\u00e1i \u1ea9n sau khi t\u00ednh \u0111\u1ea7u ra\n\n#### LSTM (Long short term memory)\n* \u0110i\u1ec3m y\u1ebfu c\u1ee7a RNN l\u00e0 kh\u00f4ng h\u1ecdc \u0111\u01b0\u1ee3c c\u00e1c th\u00f4ng tin tr\u01b0\u1edbc \u0111\u00f3 \u1edf xa do b\u1ecb vanishing graident. M\u1ea1ng LSTM ra \u0111\u1eddi \u0111\u1ec3 kh\u1eafc ph\u1ee5c \u0111i\u1ec3m y\u1ebfu n\u00e0y \n\n![Screen Shot 2022-01-08 at 12.15.57.png](attachment:f71fd52b-c21e-4b88-b2fc-f61b418dc7de.png)\n\n* \u1ede state th\u1ee9 t c\u1ee7a m\u00f4 h\u00ecnh\n    * output: $c_t$  l\u00e0 cell state, $h_t$ l\u00e0 hidden state     \n    * input: $c_{t-1},h_{t-1}$. \u1ede \u0111\u00e2y $c$ l\u00e0 \u0111i\u1ec3m m\u1edbi so v\u1edbi RNN\n* T\u00ednh to\u00e1n trong \u00f4 LSTM:\n    * C\u1ed5ng qu\u00ean (forget gate): $\\mathbf f_t = \\sigma(\\mathbf W_{f} \\mathbf x_t + \\mathbf U_{f}\\mathbf h_{t-1})$\n    * C\u1ed5ng \u0111\u1ea7u v\u00e0o (input gate): $\\mathbf i_t = \\sigma(\\mathbf W_{i} \\mathbf x_t + \\mathbf U_{i}\\mathbf h_{t-1})$\n    * C\u1ed5ng \u0111\u1ea7u ra (output gate): $\\mathbf o_t = \\sigma(\\mathbf W_{o} \\mathbf x_t + \\mathbf U_{o}\\mathbf h_{t-1})$\n    * $\\tilde{\\mathbf c}_t = \\mathrm{tanh}(\\mathbf W_{c} \\mathbf x_t + \\mathbf U_{c}\\mathbf h_{t-1})$\n    * C\u1ed5ng tr\u1ea1ng th\u00e1i \u00f4 (cell state): $\\mathbf c_{t} = \\mathbf f_t \\times \\mathbf c_{t-1} + \\mathbf i_t \\times \\tilde{\\mathbf c}_t$. Forget gate quy\u1ebft \u0111\u1ecbnh xem l\u1ea5y bao nhi\u00eau t\u1eeb cell state tr\u01b0\u1edbc v\u00e0 input gate s\u1ebd quy\u1ebft \u0111\u1ecbnh l\u1ea5y bao nhi\u00eau t\u1eeb input c\u1ee7a state v\u00e0 hidden state c\u1ee7a state tr\u01b0\u1edbc\n    * $ \\mathbf h_t = \\mathrm{tanh}(c_t) \\times \\mathbf o_t $ , $\\mathbf y_t = \\phi_y(\\mathbf W_y \\mathbf h_t)$\n* Ta th\u1ea5y LSTM l\u00e0 RNN \u0111\u01b0\u1ee3c hi\u1ec7u ch\u1ec9nh b\u1edfi $c_t$, th\u00f4ng tin n\u00e0o c\u1ea7n quan tr\u1ecdng v\u00e0 d\u00f9ng \u1edf sau s\u1ebd \u0111\u01b0\u1ee3c g\u1eedi v\u00e0o v\u00e0 d\u00f9ng khi c\u1ea7n => c\u00f3 th\u1ec3 mang th\u00f4ng tin \u0111i xa\n\n#### Bidirectional LSTM \n\n![](https:\/\/production-media.paperswithcode.com\/methods\/Screen_Shot_2020-05-25_at_8.54.27_PM.png)\n\n* Bidirectional LSTM l\u00e0 m\u00f4 h\u00ecnh g\u1ed3m hai LSTM: LSTM th\u1ee9 nh\u1ea5t nh\u1eadn \u0111\u1ea7u v\u00e0o l\u00e0 chu\u1ed7i c\u00e1c t\u1eeb theo th\u1ee9 t\u1ef1 t\u1eeb tr\u00e1i sang ph\u1ea3i, LSTM c\u00f2n l\u1ea1i nh\u1eadn \u0111\u1ea7u v\u00e0o l\u00e0 chu\u1ed7i c\u00e1c t\u1eeb theo th\u1ee9 t\u1ef1 t\u1eeb ph\u1ea3i sang tr\u00e1i. \u0110i\u1ec1u n\u00e0y l\u00e0m t\u0103ng hi\u1ec7u qu\u1ea3 l\u01b0\u1ee3ng th\u00f4ng tin c\u00f3 s\u1eb5n, \u0111\u1ed3ng th\u1eddi c\u1ea3i thi\u1ec7n ng\u1eef c\u1ea3nh c\u00f3 s\u1eb5n cho thu\u1eadt to\u00e1n.\n\n\n#### Ki\u1ebfn tr\u00fac m\u00f4 h\u00ecnh s\u1eed d\u1ee5ng\n* L\u1edbp Embedding layer s\u1eed d\u1ee5ng embbeding c\u1ee7a glove\n* BiLSTM\n* 2 l\u1edbp linear, v\u1edbi h\u00e0m k\u00edch ho\u1ea1t relu\n* dropout \n\n","afd747ff":"#### S\u1ed1 l\u01b0\u1ee3ng c\u00e1c c\u00e2u h\u1ecfi \u1edf m\u1ed7i lo\u1ea1i trong t\u1eadp training","d4562db0":"## 2.2.1 Tr\u1ef1c quan h\u00f3a d\u1eef li\u1ec7u trong t\u1eadp hu\u1ea5n luy\u1ec7n","01d6af84":"#### B\u1ed5 sung c\u00e1c \u0111\u1eb7c tr\u01b0ng ","5aa1a920":"### C\u00e1c th\u01b0 vi\u1ec7n \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng","595a4d97":"#### Ph\u00e2n t\u00edch \u0111\u1eb7c tr\u01b0ng \u0111\u1ed9 d\u00e0i","d653f0d3":"Ta s\u1ebd th\u00eam m\u1ed9t v\u00e0i \u0111\u1eb7c tr\u01b0ng c\u01a1 b\u1ea3n c\u1ee7a c\u00e2u h\u1ecfi v\u00e0o trong t\u1eadp training:\n* \u0110\u1ed9 d\u00e0i c\u1ee7a c\u00e2u\n* S\u1ed1 l\u01b0\u1ee3ng t\u1eeb \n* S\u1ed1 l\u01b0\u1ee3ng t\u1eeb kh\u00f4ng tr\u00f9ng nhau\n* S\u1ed1 l\u01b0\u1ee3ng c\u00e1c stopwords\n* S\u1ed1 l\u01b0\u1ee3ng k\u00fd t\u1ef1 \u0111\u1eb7c bi\u1ec7t (\u1edf \u0111\u00e2y, c\u00e1c k\u00fd t\u1ef1 ngo\u00e0i ch\u1eef,s\u1ed1, d\u1ea5u space, ta coi l\u00e0 c\u00e1c k\u00fd t\u1ef1 \u0111\u1eb7c bi\u1ec7t)","456676a4":"Qu\u00e1 tr\u00ecnh ti\u1ec1n x\u1eed l\u00fd d\u1eef li\u1ec7u g\u1ed3m:\n* Lo\u1ea1i b\u1ecf c\u00e1c k\u00fd t\u1ef1 \u0111\u1eb7c bi\u1ec7t c\u00f3 trong c\u00e2u h\u1ecfi\n* X\u1eed l\u00fd c\u00e1c misspell words (c\u00e1c t\u1eeb vi\u1ebft\/ph\u00e1t \u00e2m kh\u00f4ng ch\u00ednh x\u00e1c)\n* X\u1eed l\u00fd c\u00e1c Contractions (nh\u1eefng t\u1eeb c\u00f3 d\u1ea5u nh\u00e1y \u0111\u01a1n ['])\n* Lo\u1ea1i b\u1ecf s\u1ed1 \n* Lo\u1ea1i b\u1ecf c\u00e1c tag to\u00e1n h\u1ecdc, \u0111\u01b0\u1eddng d\u1eabn","171e947e":"B\u1ed9 d\u1eef li\u1ec7u hu\u1ea5n luy\u1ec7n c\u00f3 s\u1ef1 m\u1ea5t c\u00e2n b\u1eb1ng khi 93,8% () l\u00e0 c\u00e1c c\u00e2u h\u1ecfi sincere, 6,2% c\u00f2n l\u1ea1i l\u00e0 insincere question. Ta c\u1ea7n c\u00f3 gi\u1ea3i ph\u00e1p c\u00e2n b\u1eb1ng l\u1ea1i d\u1eef li\u1ec7u trong t\u1eadp training"}}