{"cell_type":{"4e882346":"code","8230444a":"code","7387024c":"code","5809eeae":"code","6706a01f":"code","614af2b9":"code","525603d0":"code","70240e95":"code","c5974424":"code","507024ba":"code","a293d541":"code","a5c91216":"code","6e26619a":"code","e8fef245":"code","de85deac":"code","8e5e0f47":"code","e2112295":"code","8516ff4d":"code","cd8b17f4":"code","0bfef22a":"markdown"},"source":{"4e882346":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport string\nimport os\n","8230444a":"#Here we are preparing our directories\nworking_directory = os.getcwd()\ndata_directory = \"..\/input\/duygu-analizi-icin-urun-yorumlari\/magaza_yorumlari_duygu_analizi.csv\"","7387024c":"#The commented code belove throws and error meassage about unicode. From the notebooks under this dataset I see that changing encoding to\n#\"utf-16\" solves the issue\n#data = pd.read_csv(data_directory)\ndata = pd.read_csv(data_directory,encoding=\"utf-16\")","5809eeae":"data.head()","6706a01f":"data.info()","614af2b9":"#There are some float valued inputs that reads nan.\nprint(data[\"G\u00f6r\u00fc\u015f\"].iloc[2603].index)\n","525603d0":"#THere are three float valued inputs and we remove them from the dataframe\nlist_to_drop =  []\nfor i in range(len(data[\"G\u00f6r\u00fc\u015f\"])):\n    if type(data[\"G\u00f6r\u00fc\u015f\"].iloc[i]) is not str:\n        list_to_drop.append(i)\n\ndata.drop(labels=list_to_drop,axis=0,inplace=True)\n        ","70240e95":"def tokenize_the_data_from_pandas(dataframe,column_name): # This function returns tokens dictionary that includes every unique word as keys and unique integers for each key.\n    #Here we clean the string out of punctuations.\n    import string\n    for sent in range(len(data[column_name])):\n        example_sentence = data[column_name].iloc[sent]\n        new_sentence = example_sentence.translate(str.maketrans(\"\",\"\",string.punctuation))\n        data[column_name].iloc[sent] = new_sentence\n    #Here we create a dictionary that will encode each word into an integer to have a representation of the word in the deep neural networks processes\n    tokens = {}\n    for sent in range(len(data[column_name])):\n        example_sentence = data[column_name].iloc[sent]\n        values = example_sentence.split(\" \")\n        for word in values:\n            tokens[word] = 0\n         \n    names = list(tokens.keys())\n    for num in range(len(names)):\n        tokens[names[num]] = num+1\n        \n    return tokens","c5974424":"tokens = tokenize_the_data_from_pandas(data,column_name=\"G\u00f6r\u00fc\u015f\")\n","507024ba":"len(tokens.keys())","a293d541":"#Here we look at each text to decide about the proper length of data.\nmax_length = 0\nfor sentence in data[\"G\u00f6r\u00fc\u015f\"]:\n    new_sentence = sentence.translate(str.maketrans(\"\",\"\",string.punctuation))\n    #print(new_sentence)\n    length = len(new_sentence.split(\" \"))\n    if length > max_length:\n        max_length = length","a5c91216":"#Creating the data vectors.\n\nX = []\ny = []\nfor sent in range(len(data[\"G\u00f6r\u00fc\u015f\"])):\n    example_sentence = data[\"G\u00f6r\u00fc\u015f\"].iloc[sent]\n    values = example_sentence.split(\" \")\n    for word in range(len(values)):\n        values[word] = tokens[values[word]]\n    X.append(values)\n    y.append(data[\"Durum\"].iloc[sent])\n    \n#After this function we have two lists one is training data one is labels.\n\n#This code will pad the sequences with zeros to make every input the same length.\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nmax_length = max_length\npadded_docs = pad_sequences(X, maxlen=max_length, padding='post')\n\n#This list is the types of Durum which we wil need to one hot encode the labels.\nt\u00fcrler = list(data[\"Durum\"].unique())\n#This code below is going to onehot encode the labels in to format [0,0,0] by changing one of the zeros into 1 -> [0,0,1]\nnew_y = []\nfor t\u00fcr in range(len(data[\"Durum\"])):\n    st_liste = [0,0,0]\n    itham = data[\"Durum\"].iloc[t\u00fcr]\n    st_liste[t\u00fcrler.index(itham)] += 1\n    new_y.append(st_liste)\n#the code below changes the final data lists into numpy arrays so that we can train our neural network with.    \nX_ar = np.array(padded_docs)\ny_ar = np.array(new_y)\n#Here we split our data into train and test sets.\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X_ar,y_ar,test_size=0.1,random_state = 5)","6e26619a":"X_train.shape","e8fef245":"#This is a 1D convolutional model for the task\n#This model is yet overfitting. I am working on developing it and regularizing it. I am open to any suggestions you make :)\nmodel = tf.keras.Sequential()\nmodel.add(tf.keras.layers.Embedding(45400,400,input_length=max_length))#This embedding layer reduces the data dimension from 290 to 10 by creating relations and using floating numbers to represent the words.\nmodel.add(tf.keras.layers.Conv1D(16,10,activation=\"relu\"))\nmodel.add(tf.keras.layers.MaxPooling1D(5))\nmodel.add(tf.keras.layers.Dropout(0.4))\nmodel.add(tf.keras.layers.Conv1D(32,7,activation=\"relu\"))\nmodel.add(tf.keras.layers.MaxPooling1D(5))\nmodel.add(tf.keras.layers.Dropout(0.4))\nmodel.add(tf.keras.layers.Conv1D(64,5,activation=\"relu\"))\nmodel.add(tf.keras.layers.MaxPooling1D(2))\nmodel.add(tf.keras.layers.Dropout(0.2))\n\n\n\n\n\n#model.add(tf.keras.layers.GlobalMaxPooling1D())\nmodel.add(tf.keras.layers.Flatten())\nmodel.add(tf.keras.layers.Dense(16, activation='relu'))\nmodel.add(tf.keras.layers.Dropout(0.2))\nmodel.add(tf.keras.layers.Dense(3, activation='softmax'))\n\nmodel.compile(optimizer = \"adam\",loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])\nmodel.fit(X_train,y_train,epochs=5,validation_data=(X_test,y_test))","de85deac":"#Predicting the test set to investiagte further.\ny_pred=model.predict(X_test[:])","8e5e0f47":"for out in range(len(y_pred)):\n    maximum = np.max(y_pred[out])\n    y_pred[out][y_pred[out] == maximum] = 1\n    y_pred[out][y_pred[out] < 1]=0","e2112295":"for i in range(len(y_pred)):\n    print(y_pred[i],y_test[i])","8516ff4d":"Truth = 0\nFalset = 0\nfor pik in range(len(y_pred)):\n    \n    result = y_pred[pik] == y_test[pik]\n    if False in result:\n        Falset += 1 \n    else:\n        Truth += 1\n    \n    #print(y_pred[pik] == y_test[pik])","cd8b17f4":"print(Truth,Falset)","0bfef22a":"# In this notebook I am using one dimensional convolutional network to classify customers comments in to positive negative and neutral.\n#  This is a developing notebook therefore I am open to any suggestions and comments you would make :)"}}