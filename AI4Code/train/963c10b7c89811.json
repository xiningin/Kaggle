{"cell_type":{"bbd861ef":"code","dc662d9e":"code","7b8ebbc9":"code","4d63704b":"code","f8028a97":"code","8983a366":"code","b8b5986c":"code","3c03ae71":"code","92ef9d89":"code","3cb859f2":"code","c98fc4f6":"markdown","37c429d0":"markdown","0d9ff1c1":"markdown","15fc218a":"markdown","52edae02":"markdown","c48746c9":"markdown","0ea9095e":"markdown","9d0dadf4":"markdown","043909f6":"markdown","9183d4ca":"markdown","e8ac2175":"markdown","585de521":"markdown","9578b99b":"markdown","42614066":"markdown","21ab4535":"markdown","0b0f9fa4":"markdown","9e89fc37":"markdown","bbb33af5":"markdown","5dd88cad":"markdown"},"source":{"bbd861ef":"!pip install --upgrade pip\n!pip install pymap3d==2.1.0\n!pip install -U l5kit\n","dc662d9e":"import l5kit, os\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom l5kit.data import ChunkedDataset, LocalDataManager\nfrom l5kit.dataset import EgoDataset, AgentDataset\nfrom l5kit.rasterization import build_rasterizer\nfrom l5kit.configs import load_config_data\nfrom l5kit.visualization import draw_trajectory, TARGET_POINTS_COLOR\nfrom l5kit.geometry import transform_points\nfrom tqdm import tqdm\nfrom collections import Counter\nfrom l5kit.data import PERCEPTION_LABELS\nfrom prettytable import PrettyTable\nfrom matplotlib import animation, rc\nfrom IPython.display import HTML\n","7b8ebbc9":"os.environ[\"L5KIT_DATA_FOLDER\"] = \"..\/input\/lyft-motion-prediction-autonomous-vehicles\"\ncfg = load_config_data(\"..\/input\/lyft-config-files\/visualisation_config.yaml\")","4d63704b":"dm = LocalDataManager()\ndataset_path = dm.require(cfg[\"val_data_loader\"][\"key\"])\nzarr_dataset = ChunkedDataset(dataset_path)\nzarr_dataset.open()\nprint(zarr_dataset)","f8028a97":"data = dataset[50]\n\nim = data[\"image\"].transpose(1, 2, 0)\nim = dataset.rasterizer.to_rgb(im)\ntarget_positions_pixels = transform_points(data[\"target_positions\"] + data[\"centroid\"][:2], data[\"world_to_image\"])\ndraw_trajectory(im, target_positions_pixels, data[\"target_yaws\"], TARGET_POINTS_COLOR)\n\nplt.imshow(im[::-1])\nplt.show()","8983a366":"cfg[\"raster_params\"][\"map_type\"] = \"py_semantic\"\nrast = build_rasterizer(cfg, dm)\ndataset = EgoDataset(cfg, zarr_dataset, rast)\ndata = dataset[50]\n\nim = data[\"image\"].transpose(1, 2, 0)\nim = dataset.rasterizer.to_rgb(im)\ntarget_positions_pixels = transform_points(data[\"target_positions\"] + data[\"centroid\"][:2], data[\"world_to_image\"])\ndraw_trajectory(im, target_positions_pixels, data[\"target_yaws\"], TARGET_POINTS_COLOR)\n\nplt.imshow(im[::-1])\nplt.show()","b8b5986c":"cfg[\"raster_params\"][\"map_type\"] = \"py_satellite\"\nrast = build_rasterizer(cfg, dm)\ndataset = EgoDataset(cfg, zarr_dataset, rast)\ndata = dataset[50]\n\nim = data[\"image\"].transpose(1, 2, 0)\nim = dataset.rasterizer.to_rgb(im)\ntarget_positions_pixels = transform_points(data[\"target_positions\"] + data[\"centroid\"][:2], data[\"world_to_image\"])\ndraw_trajectory(im, target_positions_pixels, data[\"target_yaws\"], TARGET_POINTS_COLOR)\n\nplt.imshow(im[::-1])\nplt.show()","3c03ae71":"from IPython.display import display, clear_output\nimport PIL\n \ncfg[\"raster_params\"][\"map_type\"] = \"py_satellite\"\nrast = build_rasterizer(cfg, dm)\ndataset = EgoDataset(cfg, zarr_dataset, rast)\nscene_idx = 2\nindexes = dataset.get_scene_indices(scene_idx)\nimages = []\n\nfor idx in indexes:\n    \n    data = dataset[idx]\n    im = data[\"image\"].transpose(1, 2, 0)\n    im = dataset.rasterizer.to_rgb(im)\n    target_positions_pixels = transform_points(data[\"target_positions\"] + data[\"centroid\"][:2], data[\"world_to_image\"])\n    center_in_pixels = np.asarray(cfg[\"raster_params\"][\"ego_center\"]) * cfg[\"raster_params\"][\"raster_size\"]\n    draw_trajectory(im, target_positions_pixels, data[\"target_yaws\"], TARGET_POINTS_COLOR)\n    clear_output(wait=True)\n    display(PIL.Image.fromarray(im[::-1]))","92ef9d89":"import numpy as np\nfrom IPython.display import display, clear_output\nimport PIL\n \ncfg[\"raster_params\"][\"map_type\"] = \"py_semantic\"\nrast = build_rasterizer(cfg, dm)\ndataset = EgoDataset(cfg, zarr_dataset, rast)\nscene_idx = 2\nindexes = dataset.get_scene_indices(scene_idx)\nimages = []\n\nfor idx in indexes:\n    \n    data = dataset[idx]\n    im = data[\"image\"].transpose(1, 2, 0)\n    im = dataset.rasterizer.to_rgb(im)\n    target_positions_pixels = transform_points(data[\"target_positions\"] + data[\"centroid\"][:2], data[\"world_to_image\"])\n    center_in_pixels = np.asarray(cfg[\"raster_params\"][\"ego_center\"]) * cfg[\"raster_params\"][\"raster_size\"]\n    draw_trajectory(im, target_positions_pixels, data[\"target_yaws\"], TARGET_POINTS_COLOR)\n    clear_output(wait=True)\n    display(PIL.Image.fromarray(im[::-1]))","3cb859f2":"from IPython.display import display, clear_output\nfrom IPython.display import HTML\n\nimport PIL\nimport matplotlib.pyplot as plt\nfrom matplotlib import animation, rc\ndef animate_solution(images):\n\n    def animate(i):\n        im.set_data(images[i])\n \n    fig, ax = plt.subplots()\n    im = ax.imshow(images[0])\n    \n    return animation.FuncAnimation(fig, animate, frames=len(images), interval=60)\ncfg[\"raster_params\"][\"map_type\"] = \"py_satellite\"\nrast = build_rasterizer(cfg, dm)\ndataset = EgoDataset(cfg, zarr_dataset, rast)\nscene_idx = 34\nindexes = dataset.get_scene_indices(scene_idx)\nimages = []\n\nfor idx in indexes:\n    \n    data = dataset[idx]\n    im = data[\"image\"].transpose(1, 2, 0)\n    im = dataset.rasterizer.to_rgb(im)\n    target_positions_pixels = transform_points(data[\"target_positions\"] + data[\"centroid\"][:2], data[\"world_to_image\"])\n    center_in_pixels = np.asarray(cfg[\"raster_params\"][\"ego_center\"]) * cfg[\"raster_params\"][\"raster_size\"]\n    draw_trajectory(im, target_positions_pixels, data[\"target_yaws\"], TARGET_POINTS_COLOR)\n    clear_output(wait=True)\n    images.append(PIL.Image.fromarray(im[::-1]))\nanim = animate_solution(images)\nHTML(anim.to_jshtml())","c98fc4f6":"The green blob represents the AV's motion, and we would require to predict the movement of the AV in these traffic conditions as a sample.","37c429d0":"# These HD maps need to represent the world at an unprecedented centimeter resolution, which is one to two orders of magnitude greater than the roughly meter level resolution that web map services offer today.","0d9ff1c1":"Analysis:\nIntersection of 4 road would be seen . \nThe yellow Lines are the various Lanes possible .\nMaybe the red Path indicates the various Trajectories possible for our AV to take.\n","15fc218a":"# Semantic View\nTHis is the Semantic view of the scene","52edae02":"![1_novXPga1nTb5aI1g9_-ReQ.png](attachment:1_novXPga1nTb5aI1g9_-ReQ.png)","c48746c9":"# # Layering In HD Maps:\n","0ea9095e":"Rasterisation is the task of taking an image described in a vector graphics format and converting it into a raster image.\n\n* BoxRasterizer: this object renders agents (e.g. vehicles or pedestrians) as oriented 2D boxes\n* SatelliteRasterizer: this object renders an oriented crop from a satellite map","9d0dadf4":"# DATA LOADING :\n\nLocalDataManager object resolves**** relative paths from the config using the L5KIT_DATA_FOLDER env variable we have just set.","043909f6":"# Setup Environment Variable and Configs:\n\nIn this yaml file we have the necessary configs\n1. Model Params\n2. Input Image Raster Params\n3. The DataLoader params\n","9183d4ca":"# HD MAP Principles:\n1.**Mapping as pre-computation** :  Perception and localization of static objects in the world such as roads, intersections, street signs, etc. can be solved offline and in a highly accurate manner .This are some of the things that could be precomputed before the AV starts driving.\n\n2.**Mapping to improve safety**: Level 5 HD maps are designed not only to contain speed limit information for each lane segment, but also speed profiles derived from actual human drivers on the Lyft network that meet our high bar for safety.\n\n3.**Map as a unique sensor**: Viewing the map as yet another sensor allows us to design efficient map access patterns and integrate map data more naturally into the autonomy stack (e.g. sensor-fusion components).\n\n4.**Map as global shared state:** The map then becomes a shared data structure that lives both in the cloud and also docked in each of the AVs. AVs use the map to both read and write to this social memory.\n","e8ac2175":"# Four noteworthy HD layers are: \n1. The geometric map : The geometric map layer contains 3D information of the world. This information is organized in very high detail to support precise calculations. The ground map is key for aligning the subsequent layers of the map, such as the semantic map.\n\n2. The semantic map : Semantic objects include various traffic 2D and 3D objects such as lane boundaries, intersections, crosswalks, parking spots, stop signs, traffic lights, etc. that are used for driving safely. These objects contain rich metadata associated with them such as speed limits and turn restrictions for lanes. \n\n3. Map priors :The map priors layer contains derived information about dynamic elements and also human driving behavior. Information here can pertain to both semantic and geometric parts of the map\n\n4. Real-time knowledge:The real-time layer is the top most layer in the map and is designed to be read\/write capable. This is the only layer in the map designed to be updated while the map is in use by the AV serving a ride\n\nSource  : https:\/\/medium.com\/lyftlevel5\/https-medium-com-lyftlevel5-rethinking-maps-for-self-driving-a147c24758d6\n\n\n","585de521":"Analysis:\nThe Green Blob is our Autonomous vechicle\nI think that the Blue Blobs are the Path to be taken by our vehicle . As we are able to generate the path for our movement this will help to tackle obstacle in a better way","9578b99b":"Additional Sources and Info :\nhttps:\/\/medium.com\/lyftlevel5\/continued-momentum-through-simulation-8b9a8df79f3b\n\nhttps:\/\/github.com\/lyft\/l5kit\/\n\nhttps:\/\/medium.com\/lyftlevel5\/lyfts-approach-to-autonomous-vehicle-safety-fc771ed25786\n\nhttps:\/\/github.com\/lyft\/l5kit\/tree\/master\/examples\/visualisation\n\nhttps:\/\/arxiv.org\/pdf\/1912.11676.pdf\n\nThankyou For reading !!","42614066":"This shows the way to visualize a sample from our dataset\n\nWe use Rasterizer to get an rgb format and then plot .\n\nThe target positions can be changed with the pixel co-ordinates necssary for knowing the ground truth.\n","21ab4535":"# Satellite View\nThis is the Satellite View of the Scene .  Where the Pink Trajectory seems to be the line of motion ie the expected trajectory of the AV","0b0f9fa4":"This notebook shows some of the visulisation utility of Lyft l5 toolkit.\n\nThe core packages for visualisation are:\n\n* 1.rasterization\n* 2.visualization\n\nAlong with the information regarding Maps that Lyft uses for Motion planning\n","9e89fc37":"# The Exploration of individual Images would help us to gain more insights into the data\n1. Trajectory Of AV\n2. Satellite View\n3. Semantic Veiw\n\n","bbb33af5":"Analysis:\nThis Shows the real time ( actual path planning done by Autonomous vehicle ) . With Blue being the possible Paths to take.","5dd88cad":"# Entire Motion \nEntire Motion of the Av wrt other vehicles and its trajectory planning would be seen from this."}}