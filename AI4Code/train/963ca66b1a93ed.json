{"cell_type":{"2c1e48c7":"code","e5bc1cdc":"code","0557c2be":"code","e68456b9":"code","a0011f5e":"code","1ee97bb4":"code","57de0bce":"code","396d5441":"code","8fca896a":"markdown"},"source":{"2c1e48c7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.\n#Import Libraries\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nimport lightgbm as lgb\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nfrom tqdm import tqdm_notebook as tqdm\nfrom subprocess import check_output\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set_style('whitegrid')\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.filterwarnings('ignore')\nrandom_state = 2019\nnp.random.seed(random_state)\n\n#Read the csv files train and test\ndf_train = pd.read_csv('..\/input\/train.csv')\ndf_test = pd.read_csv('..\/input\/test.csv')\n#Find the number of columns\nnumcols = [col for col in df_train.columns if col.startswith('var_')] \t","e5bc1cdc":"test = df_test.copy()\ntest.drop(['ID_code'], axis=1, inplace=True)\ntest = test.values\n\n#Find the number of unique samples\nunique_samples = []\nunique_count = np.zeros_like(test)\nfor feature in tqdm(range(test.shape[1])):\n    _, index_, count_ = np.unique(test[:, feature], return_counts=True, return_index=True)\n    unique_count[index_[count_ == 1], feature] += 1\n\n# Samples which have unique values are real the others are fake\nreal_samples_indexes = np.argwhere(np.sum(unique_count, axis=1) > 0)[:, 0]\nsynthetic_samples_indexes = np.argwhere(np.sum(unique_count, axis=1) == 0)[:, 0]\n\n#Drop the fake columns\ndf_test = df_test.drop(synthetic_samples_indexes)\n\n#Added both the test and train datafram together\nfull = pd.concat([df_train, df_test])\nfor col in numcols:\n    full['count'+col] = full[col].map(full[col].value_counts())\n\n#See sample dataframe\nfull.head()\n","0557c2be":"plt.figure(figsize=(12,5))\nplt.subplot(121)\nsns.distplot(full[full['target']==1]['countvar_12'], label='target = 1')\nsns.distplot(full[full['target']==0]['countvar_12'], label='target = 0')\nplt.legend()\nplt.subplot(122)\nsns.distplot(full[full['target']==1]['var_12'], label='target = 1')\nsns.distplot(full[full['target']==0]['var_12'], label='target = 0')\nplt.legend()\n\n#Plot the count and histogram to see the variables\nplt.figure(figsize=(12,5))\nplt.subplot(121)\nsns.distplot(full[full['target']==1]['countvar_61'], label='target = 1')\nsns.distplot(full[full['target']==0]['countvar_61'], label='target = 0')\nplt.legend()\nplt.subplot(122)\nsns.distplot(full[full['target']==1]['var_61'], label='target = 1')\nsns.distplot(full[full['target']==0]['var_61'], label='target = 0')\nplt.legend()","e68456b9":"codecols = ['countvar_61','countvar_45','countvar_136','countvar_187',\n            'countvar_74','countvar_160', 'countvar_199','countvar_120',\n            'countvar_158','countvar_96']\n\n#Calculate the ratio variable by dividing the column value with frequency\t\t\t\nfor col in numcols:\n    full['ratio'+col] = full[col] \/ full['count'+col]\n\n\n#Plot the distribution of the ratio variables\nsns.distplot(full['ratiovar_81'])\n#sns.distplot(full['ratiovar_61'])\n#sns.distplot(full['ratiovar_146'])\n","a0011f5e":"# Define the functions for target encoding:\n# This is required for categorical variables\ndef add_noise(series, noise_level):\n    return series * (1 + noise_level * np.random.randn(len(series)))\n\ndef target_encode(trn_series=None,\n                  tst_series=None,\n                  target=None,\n                  min_samples_leaf=1,\n                  smoothing=1,\n                  noise_level=0):\n\n    assert len(trn_series) == len(target)\n    assert trn_series.name == tst_series.name\n    temp = pd.concat([trn_series, target], axis=1)\n    # Compute target mean\n    averages = temp.groupby(by=trn_series.name)[target.name].agg([\"mean\", \"count\"])\n    # Compute smoothing\n    smoothing = 1 \/ (1 + np.exp(-(averages[\"count\"] - min_samples_leaf) \/ smoothing))\n    # Apply average function to all target data\n    prior = target.mean()\n    # The bigger the count the less full_avg is taken into account\n    averages[target.name] = prior * (1 - smoothing) + averages[\"mean\"] * smoothing\n    averages.drop([\"mean\", \"count\"], axis=1, inplace=True)\n    # Apply averages to trn and tst series\n    ft_trn_series = pd.merge(\n        trn_series.to_frame(trn_series.name),\n        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n        on=trn_series.name,\n        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n    # pd.merge does not keep the index so restore it\n    ft_trn_series.index = trn_series.index\n  \n\n    ft_tst_series = pd.merge(\n        tst_series.to_frame(tst_series.name),\n        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n        on=tst_series.name,\n        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n    # pd.merge does not keep the index so restore it\n    ft_tst_series.index = tst_series.index\n    return add_noise(ft_trn_series, noise_level), add_noise(ft_tst_series, noise_level)\n","1ee97bb4":"ncols = [col for col in full if col not in ['target', 'ID_code']]\ndf_train = full[~full['target'].isna()]\ndf_test = full[full['target'].isna()]\ndf_test.drop('target', axis=1, inplace=True)","57de0bce":"lgb_params = {\n    \"objective\" : \"binary\",\n    \"metric\" : \"auc\",\n    \"boosting\": 'gbdt',\n    \"max_depth\" : -1,\n    \"num_leaves\" : 2,\n    \"learning_rate\" : 0.02, # Lower it for actual submission\n    \"bagging_freq\": 5,\n    \"bagging_fraction\" : 0.4,\n    \"feature_fraction\" : 0.05,\n    \"min_data_in_leaf\": 100,\n    \"min_sum_hessian_in_leaf\": 0.3,\n    \"lambda_l1\":0.556,\n    \"lambda_l2\":4.772,\n    \"tree_learner\": \"serial\",\n    \"boost_from_average\": \"false\",\n    \"bagging_seed\" : random_state,\n    \"verbosity\" : 1,\n    \"seed\": random_state}\n\nskf = StratifiedKFold(n_splits=3, shuffle=True, random_state=random_state)\noof = df_train[['ID_code', 'target']]\noof['predict'] = 0\npredictions = df_test[['ID_code']]\nval_aucs = []\nfeature_importance_df = pd.DataFrame()\n\n\nfeatures = [col for col in df_train.columns if col not in ['target', 'ID_code']]\nfor fold, (trn_idx, val_idx) in enumerate(skf.split(df_train, df_train['target'])):\n    d_train = df_train.iloc[trn_idx]\n    d_val = df_train.iloc[val_idx]\n    d_test = df_test.copy()\n\n    d_val['type'] = 'val'\n    d_test['type'] = 'test'\n    d_valtest = pd.concat([d_val, d_test])\n\n    for var in codecols:\n        d_train['encoded' + var], d_valtest['encoded' + var] = target_encode(d_train[var],\n                                                                          d_valtest[var],\n                                                                          target=d_train.target,\n                                                                          min_samples_leaf=100,\n                                                                          smoothing=10,\n                                                                          noise_level=0.01)\n\n    real_test = d_valtest[d_valtest['type']=='test'].drop('type', axis=1)\n    real_val = d_valtest[d_valtest['type']=='val'].drop('type', axis=1)\n\n    features = [col for col in d_train.columns if col not in ['target', 'ID_code']]\n    X_test = real_test[features].values\n    X_train, y_train = d_train[features], d_train['target']\n    X_valid, y_valid = real_val[features], real_val['target']\n\n    p_valid, yp = 0, 0\n    X_t, y_t = X_train.values, y_train.values\n    X_t = pd.DataFrame(X_t)\n    X_t = X_t.add_prefix('var_')\n\n    trn_data = lgb.Dataset(X_t, label=y_t)\n    val_data = lgb.Dataset(X_valid, label=y_valid)\n    evals_result = {}\n    lgb_clf = lgb.train(lgb_params,\n                        trn_data,\n                        100000,  # Submission with: 100000\n                        valid_sets=[trn_data, val_data],\n                        early_stopping_rounds=3000, # Submission with: 3000\n                        verbose_eval=1000,\n                        evals_result=evals_result\n                        )\np_valid += lgb_clf.predict(X_valid)\nyp += lgb_clf.predict(X_test)\nfold_importance_df = pd.DataFrame()\nfold_importance_df[\"feature\"] = features\nfold_importance_df[\"importance\"] = lgb_clf.feature_importance()\nfold_importance_df[\"fold\"] = fold + 1\nfeature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\noof['predict'][val_idx] = p_valid\nval_score = roc_auc_score(y_valid, p_valid)\nval_aucs.append(val_score)\n\npredictions['fold{}'.format(fold + 1)] = yp\n\n\nmean_auc = np.mean(val_aucs)\nstd_auc = np.std(val_aucs)\nall_auc = roc_auc_score(oof['target'], oof['predict'])\nprint(\"Mean auc: %.9f, std: %.9f. \" % (mean_auc, std_auc))","396d5441":"predictions['target'] = np.mean(predictions[[col for col in predictions.columns if col not in ['ID_code', 'target']]].values, axis=1)\nsub_df = pd.DataFrame({\"ID_code\":df_test[\"ID_code\"].values})\nsub_df[\"target\"] = predictions['target'].values\n\n\ndf_test = pd.read_csv('..\/input\/test.csv')\nfinalsub = df_test[['ID_code']]\nfinalsub = finalsub.merge(sub_df, how='left', on='ID_code')\nfinalsub = finalsub.fillna(0)\nfinalsub.head()\nfinalsub.to_csv('Final.csv')\nprint(type(yp))\n\n\nfrom IPython.display import HTML\nimport pandas as pd\nimport numpy as np\nimport base64\n\ndef create_download_link(df, title = \"Download CSV file\", filename = \"Final_4.csv\"):  \n    csv = df.to_csv()\n    b64 = base64.b64encode(csv.encode())\n    payload = b64.decode()\n    html = '<a download=\"{filename}\" href=\"data:text\/csv;base64,{payload}\" target=\"_blank\">{title}<\/a>'\n    html = html.format(payload=payload,title=title,filename=filename)\n    return HTML(html)\n\n# create a random sample dataframe\n#df = pd.DataFrame(np.random.randn(50, 4), columns=list('ABCD'))\n\n# create a link to download the dataframe\nfinalsub=finalsub[150000:200000]\ncreate_download_link(finalsub[0:50000],\"File1\",\"File_1.csv\")\nfinalsub.to_csv('..\/input\/final.csv')\n#print(finalsub[\"target\"])\n\n","8fca896a":"* "}}