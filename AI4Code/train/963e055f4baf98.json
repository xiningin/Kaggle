{"cell_type":{"81a64fd7":"code","1730595f":"code","276397dc":"code","543a190e":"code","ca917594":"code","35ebb71c":"code","693763ec":"code","50be0a10":"code","a7fb4253":"code","6b2a2f50":"code","113ccb17":"code","d47844ed":"code","bc77ad0b":"code","83cfb5bf":"code","2d2170f1":"code","f82b3d83":"code","7db31320":"code","1e5a9d01":"code","66d47617":"code","69b7ef04":"code","5d3bc130":"code","9c237677":"code","ac7cae72":"code","11e6ee33":"code","976be660":"code","0f012dfb":"code","0ce99034":"code","11887c77":"code","1c4c90f1":"code","b5b8220d":"code","f7c6b7e4":"code","4b8fade1":"code","b2768415":"code","337b6145":"code","d6ac82ab":"code","6e8b58e0":"code","cf450d98":"code","4273965d":"code","c2a3c8f4":"code","22c50677":"code","d0d41396":"code","31bfa6b4":"code","69b63c12":"code","197d8540":"code","29fa665d":"code","9ef3267c":"code","df9a003b":"code","9160e4c1":"code","4bbe8703":"code","1a334d4b":"code","99c95a7d":"code","ef35fce4":"code","3f2dd3eb":"code","5e902270":"code","1f474f1e":"code","d1e25b82":"code","73c08bee":"code","b22666e0":"code","d4211e24":"code","b25692dd":"code","54ac38b4":"markdown","85c49c92":"markdown","65940ffa":"markdown","47f318fc":"markdown","0cd18670":"markdown","319d341c":"markdown","c57cafda":"markdown","1f25f7d0":"markdown"},"source":{"81a64fd7":"import matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import classification_report","1730595f":"\ndf = pd.read_csv('..\/input\/Consumer_Complaints.csv')\ndf.head()","276397dc":"df.info()","543a190e":"df.shape","ca917594":"#Running the exercise on the first 5000 rows\ndf = df.head(100_000)","35ebb71c":"df.info()","693763ec":"#TO DO: Identify the target and the feature columns","50be0a10":"#TO DO: Clean the columns (removing missing values)\n","a7fb4253":"from io import StringIO\ncol = ['Product', 'Consumer complaint narrative']\ndf = df[col]\ndf = df[pd.notnull(df['Consumer complaint narrative'])]\ndf.columns = ['Product', 'Consumer_complaint_narrative']\ndf['category_id'] = df['Product'].factorize()[0]\ncategory_id_df = df[['Product', 'category_id']].drop_duplicates().sort_values('category_id')\ncategory_to_id = dict(category_id_df.values)\nid_to_category = dict(category_id_df[['category_id', 'Product']].values)\ndf.head()","6b2a2f50":"df.loc[:, ['category_id', 'Product']].drop_duplicates().reset_index(drop=True)","113ccb17":"#cats_complaints.merge(df.loc[:, ['category_id', 'Product']], how = 'left')","d47844ed":"#pd.merge(left=cats_complaints, right=df.loc[:, ['category_id', 'Product']], right_on = 'category_id', left_on = 'category_id', how='left')","bc77ad0b":"df.loc[:, ['Product', 'category_id']].head(10)","83cfb5bf":"df['Consumer_complaint_narrative'].values[3]","2d2170f1":"df_cats = df.groupby('Product').Consumer_complaint_narrative.count().reset_index()","f82b3d83":"df_cats = df_cats.rename(columns = {'Consumer_complaint_narrative': 'n'})","7db31320":"plt.barh(df_cats.sort_values('n')['Product'], df_cats.sort_values('n')['n'])\nNone","1e5a9d01":"df.shape, df.isnull().sum().sum()","66d47617":"from sklearn.model_selection import train_test_split\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom datetime import datetime","69b7ef04":"!pip install bert-tensorflow","5d3bc130":"import bert\nfrom bert import run_classifier\nfrom bert import optimization\nfrom bert import tokenization","9c237677":"OUTPUT_DIR = '.\/'#@param {type:\"string\"}\n#@markdown Whether or not to clear\/delete the directory and create a new one\nDO_DELETE = False #@param {type:\"boolean\"}\n#@markdown Set USE_BUCKET and BUCKET if you want to (optionally) store model output on GCP bucket.\nUSE_BUCKET = False #@param {type:\"boolean\"}\nBUCKET = None #@param {type:\"string\"}","ac7cae72":"from tensorflow import keras\nimport os\nimport re","11e6ee33":"df.head()","976be660":"df['category_id'].unique()","0f012dfb":"DATA_COLUMN = 'Consumer_complaint_narrative'\nLABEL_COLUMN = 'category_id'\n# label_list is the list of labels, i.e. True, False or 0, 1 or 'dog', 'cat'\nlabel_list = [0, 1, 2, 3, 4, 5, 6, 7, 8]","0ce99034":"from sklearn.model_selection import train_test_split\n\ntrain = df.sample(5000)\ntest = df.sample(5000)","11887c77":"train_InputExamples = train.apply(lambda x: bert.run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n                                                                   text_a = x[DATA_COLUMN], \n                                                                   text_b = None, \n                                                                   label = x[LABEL_COLUMN]), axis = 1)\n\ntest_InputExamples = test.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n                                                                   text_a = x[DATA_COLUMN], \n                                                                   text_b = None, \n                                                                   label = x[LABEL_COLUMN]), axis = 1)","1c4c90f1":"BERT_MODEL_HUB = \"https:\/\/tfhub.dev\/google\/bert_uncased_L-12_H-768_A-12\/1\"","b5b8220d":"print(tf.__version__)","f7c6b7e4":"def create_tokenizer_from_hub_module():\n    \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n    with tf.Graph().as_default():\n        bert_module = hub.Module(BERT_MODEL_HUB)\n        tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n        with tf.Session() as sess:\n            vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],tokenization_info[\"do_lower_case\"]])\n      \n    return bert.tokenization.FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)","4b8fade1":"print(os.listdir(\"..\/..\"))","b2768415":"tokenizer = create_tokenizer_from_hub_module()","337b6145":"MAX_SEQ_LENGTH = 128\n# Convert our train and test features to InputFeatures that BERT understands.\ntrain_features = bert.run_classifier.convert_examples_to_features(train_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\ntest_features = bert.run_classifier.convert_examples_to_features(test_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)","d6ac82ab":"def create_model(is_predicting, input_ids, input_mask, segment_ids, labels,\n                 num_labels):\n    \"\"\"Creates a classification model.\"\"\"\n\n    bert_module = hub.Module(\n      BERT_MODEL_HUB,\n      trainable=True)\n    bert_inputs = dict(\n      input_ids=input_ids,\n      input_mask=input_mask,\n      segment_ids=segment_ids)\n    bert_outputs = bert_module(\n      inputs=bert_inputs,\n      signature=\"tokens\",\n      as_dict=True)\n\n  # Use \"pooled_output\" for classification tasks on an entire sentence.\n  # Use \"sequence_outputs\" for token-level output.\n    output_layer = bert_outputs[\"pooled_output\"]\n\n    hidden_size = output_layer.shape[-1].value\n\n  # Create our own layer to tune for politeness data.\n    output_weights = tf.get_variable(\n      \"output_weights\", [num_labels, hidden_size],\n      initializer=tf.truncated_normal_initializer(stddev=0.02))\n\n    output_bias = tf.get_variable(\n      \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n\n    with tf.variable_scope(\"loss\"):\n\n    # Dropout helps prevent overfitting\n        output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n    \n        logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n        logits = tf.nn.bias_add(logits, output_bias)\n        log_probs = tf.nn.log_softmax(logits, axis=-1)\n    \n        # Convert labels into one-hot encoding\n        one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n    \n        predicted_labels = tf.squeeze(tf.argmax(log_probs, axis=-1, output_type=tf.int32))\n        # If we're predicting, we want predicted labels and the probabiltiies.\n        if is_predicting:\n            return (predicted_labels, log_probs)\n\n    # If we're train\/eval, compute loss between predicted and actual label\n    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n    loss = tf.reduce_mean(per_example_loss)\n    return (loss, predicted_labels, log_probs)","6e8b58e0":"# model_fn_builder actually creates our model function\n# using the passed parameters for num_labels, learning_rate, etc.\ndef model_fn_builder(num_labels, learning_rate, num_train_steps,\n                     num_warmup_steps):\n  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n\n    input_ids = features[\"input_ids\"]\n    input_mask = features[\"input_mask\"]\n    segment_ids = features[\"segment_ids\"]\n    label_ids = features[\"label_ids\"]\n\n    is_predicting = (mode == tf.estimator.ModeKeys.PREDICT)\n    \n    # TRAIN and EVAL\n    if not is_predicting:\n\n      (loss, predicted_labels, log_probs) = create_model(\n        is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n\n      train_op = bert.optimization.create_optimizer(\n          loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu=False)\n\n      # Calculate evaluation metrics. \n      def metric_fn(label_ids, predicted_labels):\n        accuracy = tf.metrics.accuracy(label_ids, predicted_labels)\n        f1_score = tf.contrib.metrics.f1_score(\n            label_ids,\n            predicted_labels)\n        auc = tf.metrics.auc(\n            label_ids,\n            predicted_labels)\n        recall = tf.metrics.recall(\n            label_ids,\n            predicted_labels)\n        precision = tf.metrics.precision(\n            label_ids,\n            predicted_labels) \n        true_pos = tf.metrics.true_positives(\n            label_ids,\n            predicted_labels)\n        true_neg = tf.metrics.true_negatives(\n            label_ids,\n            predicted_labels)   \n        false_pos = tf.metrics.false_positives(\n            label_ids,\n            predicted_labels)  \n        false_neg = tf.metrics.false_negatives(\n            label_ids,\n            predicted_labels)\n        return {\n            \"eval_accuracy\": accuracy,\n            \"f1_score\": f1_score,\n            \"auc\": auc,\n            \"precision\": precision,\n            \"recall\": recall,\n            \"true_positives\": true_pos,\n            \"true_negatives\": true_neg,\n            \"false_positives\": false_pos,\n            \"false_negatives\": false_neg\n        }\n\n      eval_metrics = metric_fn(label_ids, predicted_labels)\n\n      if mode == tf.estimator.ModeKeys.TRAIN:\n        return tf.estimator.EstimatorSpec(mode=mode,\n          loss=loss,\n          train_op=train_op)\n      else:\n          return tf.estimator.EstimatorSpec(mode=mode,\n            loss=loss,\n            eval_metric_ops=eval_metrics)\n    else:\n      (predicted_labels, log_probs) = create_model(\n        is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n\n      predictions = {\n          'probabilities': log_probs,\n          'labels': predicted_labels\n      }\n      return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n\n  # Return the actual model function in the closure\n  return model_fn","cf450d98":"# Compute train and warmup steps from batch size\n# These hyperparameters are copied from this colab notebook (https:\/\/colab.sandbox.google.com\/github\/tensorflow\/tpu\/blob\/master\/tools\/colab\/bert_finetuning_with_cloud_tpus.ipynb)\nBATCH_SIZE = 32\nLEARNING_RATE = 2e-5\nNUM_TRAIN_EPOCHS = 3.0\n# Warmup is a period of time where hte learning rate \n# is small and gradually increases--usually helps training.\nWARMUP_PROPORTION = 0.1\n# Model configs\nSAVE_CHECKPOINTS_STEPS = 500\nSAVE_SUMMARY_STEPS = 100","4273965d":"# Compute # train and warmup steps from batch size\nnum_train_steps = int(len(train_features) \/ BATCH_SIZE * NUM_TRAIN_EPOCHS)\nnum_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)","c2a3c8f4":"\n# Specify outpit directory and number of checkpoint steps to save\nrun_config = tf.estimator.RunConfig(\n    model_dir=OUTPUT_DIR,\n    save_summary_steps=SAVE_SUMMARY_STEPS,\n    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)","22c50677":"model_fn = model_fn_builder(\n  num_labels=len(label_list),\n  learning_rate=LEARNING_RATE,\n  num_train_steps=num_train_steps,\n  num_warmup_steps=num_warmup_steps)\n\nestimator = tf.estimator.Estimator(\n  model_fn=model_fn,\n  config=run_config,\n  params={\"batch_size\": BATCH_SIZE})","d0d41396":"# Create an input function for training. drop_remainder = True for using TPUs.\ntrain_input_fn = bert.run_classifier.input_fn_builder(\n    features=train_features,\n    seq_length=MAX_SEQ_LENGTH,\n    is_training=True,\n    drop_remainder=False)","31bfa6b4":"print(f'Beginning Training!')\ncurrent_time = datetime.now()\nestimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\nprint(\"Training took time \", datetime.now() - current_time)\n","69b63c12":"test_input_fn = run_classifier.input_fn_builder(\n    features=test_features,\n    seq_length=MAX_SEQ_LENGTH,\n    is_training=False,\n    drop_remainder=False)","197d8540":"estimator.evaluate(input_fn=test_input_fn, steps=None)","29fa665d":"df.loc[:, ['category_id', 'Product']].drop_duplicates().reset_index(drop=True)['Product'].values","9ef3267c":"def getPrediction(in_sentences):\n    labels = df.loc[:, ['category_id', 'Product']].drop_duplicates().reset_index(drop=True)['Product'].values\n    input_examples = [run_classifier.InputExample(guid=\"\", text_a = x, text_b = None, label = 0) for x in in_sentences] # here, \"\" is just a dummy label\n    input_features = run_classifier.convert_examples_to_features(input_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n    predict_input_fn = run_classifier.input_fn_builder(features=input_features, seq_length=MAX_SEQ_LENGTH, is_training=False, drop_remainder=False)\n    predictions = estimator.predict(predict_input_fn)\n    return [(sentence, prediction['probabilities'], labels[prediction['labels']]) for sentence, prediction in zip(in_sentences, predictions)]","df9a003b":"def getPrediction_cat(in_sentences):\n    labels = df.loc[:, ['category_id', 'Product']].drop_duplicates().reset_index(drop=True)['Product'].values\n    input_examples = [run_classifier.InputExample(guid=\"\", text_a = x, text_b = None, label = 0) for x in in_sentences] # here, \"\" is just a dummy label\n    input_features = run_classifier.convert_examples_to_features(input_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n    predict_input_fn = run_classifier.input_fn_builder(features=input_features, seq_length=MAX_SEQ_LENGTH, is_training=False, drop_remainder=False)\n    predictions = estimator.predict(predict_input_fn)\n    return [labels[prediction['labels']] for sentence, prediction in zip(in_sentences, predictions)]","9160e4c1":"\npred_sentences = [\n  \"I am disputing this debt for XXXX XXXX that is currently in collections. I have no knowledge of this account and I have not received any information regarding this account from XXXX or the assigned collection agency.\",\n  df['Consumer_complaint_narrative'].values[3],\n  \"The film was creative and surprising\",\n  \"Absolutely fantastic!\"\n]","4bbe8703":"df['Consumer_complaint_narrative'].values[3]","1a334d4b":"train.shape, test.shape","99c95a7d":"#test['Consumer_complaint_narrative'].values[:100]","ef35fce4":"y_pred = getPrediction_cat(test['Consumer_complaint_narrative'].values)","3f2dd3eb":"#test['Product'].values","5e902270":"print(classification_report(test['Product'].values, y_pred, target_names=df['Product'].unique()))","1f474f1e":"\npredictions = getPrediction(pred_sentences)","d1e25b82":"predictions","73c08bee":"    input_examples = [run_classifier.InputExample(guid=\"\", text_a = x, text_b = None, label = 0) for x in pred_sentences] # here, \"\" is just a dummy label\n    ","b22666e0":"pred_sentences","d4211e24":"input_examples","b25692dd":"    input_features = run_classifier.convert_examples_to_features(input_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n    predict_input_fn = run_classifier.input_fn_builder(features=input_features, seq_length=MAX_SEQ_LENGTH, is_training=False, drop_remainder=False)","54ac38b4":"# Creating a model","85c49c92":"#### Checking for class imbalances","65940ffa":"## Data Preprocessing","47f318fc":"## BERT","0cd18670":"# Multi-class text classification Problem","319d341c":"GOAL: Use Supervised Machine Learning Methods and NLP to build a model that classifies new incoming \"user complains\" into one of the product categories.\n      Target variable: Product\n      Feature: \"Consumer complaint narrative\"\n\nhttps:\/\/towardsdatascience.com\/multi-class-text-classification-with-scikit-learn-12f1e60e0a9f\n\nhttps:\/\/github.com\/google-research\/bert\/blob\/master\/predicting_movie_reviews_with_bert_on_tf_hub.ipynb","c57cafda":"### DATA EXPLORATION","1f25f7d0":"### DATA CLEANING"}}