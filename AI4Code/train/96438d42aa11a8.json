{"cell_type":{"5b27daac":"code","415d5c6d":"code","a8628167":"code","ab277de6":"code","fbe05abf":"code","df9119ba":"code","2e9f9979":"code","79aa9261":"code","45a56a0e":"code","3d28c716":"code","959ba7e3":"code","bcc530bc":"code","0a33e8aa":"code","073fc1a4":"code","a9470b20":"code","648528c7":"code","8b8793ba":"code","c92808f8":"code","638cd13a":"code","87f98eef":"code","96e0fb97":"code","f4e7ef07":"code","a1b7a172":"code","ad1b3f5f":"code","e6ce89cf":"code","c1c74e60":"code","6b0bc18f":"code","716dabf0":"code","253e48b6":"code","15752527":"code","97171a80":"code","4abce48d":"code","b3633ce2":"code","2ef3cd83":"code","369b1800":"markdown","3ead56f8":"markdown","d6c7be26":"markdown","d9f9a690":"markdown","658dd7d7":"markdown","83b66c31":"markdown","c6725026":"markdown","116cec6a":"markdown","1335cdbd":"markdown","345047d8":"markdown","114cc64e":"markdown","9960526a":"markdown","e2b00fd8":"markdown","f852e23d":"markdown","a53e71e7":"markdown","35c5b5ab":"markdown"},"source":{"5b27daac":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","415d5c6d":"## Initialize the Libraries\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nfrom scipy.stats import zscore\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix\nimport statsmodels.api as sm\nfrom statsmodels.sandbox.regression.predstd import wls_prediction_std\nfrom sklearn import model_selection\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier","a8628167":"## Import data\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","ab277de6":"## creating dataframe\n\ntitanic_train= pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntitanic_test= pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ntitanic_submission= pd.read_csv('\/kaggle\/input\/titanic\/gender_submission.csv')","fbe05abf":"titanic_test.head()","df9119ba":"\n\n## rearranging column in titanic train\n\n\ntitanic_train_arrange=titanic_train[['PassengerId','Name','Age','Sex','SibSp','Parch','Ticket','Fare','Cabin','Embarked','Pclass','Survived']]\n\n\n## rearranging column in titanic test\n\ntitanic_test_arrange=titanic_test[['PassengerId','Name','Age','Sex','SibSp','Parch','Ticket','Fare','Cabin','Embarked','Pclass']]\n\n## Looking into train & test rearrange dataset\n\nprint(titanic_train_arrange.head())\n\nprint('\\n')\nprint(titanic_test_arrange.head())\n\n","2e9f9979":"## info about the train dataset\n\ntitanic_train_arrange.info()","79aa9261":"## summary statistics\n\ntitanic_train_arrange.describe()","45a56a0e":"## Missing values\n\nprint(titanic_train_arrange.isnull().sum())\n\n\nprint('\\n')\n\nprint(titanic_test_arrange.isnull().sum())","3d28c716":"\n\n# titanic_train_arrange.describe()","959ba7e3":"# ## age\n\n# titanic_train_arrange['Age']= titanic_train_arrange.fillna(titanic_train_arrange['Age'].median())\n# titanic_test_arrange['Age']= titanic_test_arrange.fillna(titanic_test_arrange['Age'].median())\n\n## replacing nan values of age with ffill method\n\ntitanic_train_arrange[\"Age\"].fillna( method ='ffill',inplace=True)\ntitanic_test_arrange[\"Age\"].fillna( method ='ffill',inplace=True)\n\n# ## Fare\n\n# titanic_train_arrange['Fare']= titanic_train_arrange.fillna(titanic_train_arrange['Fare'].median())\n# titanic_test_arrange['Fare']= titanic_test_arrange.fillna(titanic_test_arrange['Fare'].median())\n\n## replacing nan values of fare with ffill method\n\ntitanic_train_arrange[\"Fare\"].fillna( method ='ffill',inplace=True)\n\ntitanic_test_arrange[\"Fare\"].fillna( method ='ffill',inplace=True)\n\n# ##Cabin\n\ntitanic_train_arrange['Cabin']= titanic_train_arrange.fillna(titanic_train_arrange['Cabin'].mode())\ntitanic_test_arrange['Cabin']= titanic_test_arrange.fillna(titanic_test_arrange['Cabin'].mode())\n\n## Embarked\ntitanic_train_arrange['Embarked']= titanic_train_arrange.fillna(titanic_train_arrange['Embarked'].mode())\n\n","bcc530bc":"print(titanic_train_arrange.info())\n\nprint(titanic_test_arrange.info())","0a33e8aa":"# # ## checking for datatypes\n\nprint(titanic_train_arrange.describe())\n\nprint(titanic_test_arrange.describe())","073fc1a4":"# ### coverting fractional age valuees to 1\n\ntitanic_train_arrange.loc[titanic_train_arrange.Age <1,'Age']=1\ntitanic_test_arrange.loc[titanic_test_arrange.Age <1,'Age']=1\ntitanic_train_arrange.info()\ntitanic_train_arrange.describe()\ntitanic_train_arrange.shape\n\n","a9470b20":"\n\ntitanic_test_arrange.info()\nprint('\\n')\ntitanic_test_arrange.describe()","648528c7":"# # 1. Age round off\n\ntitanic_train_arrange['Age']= round(titanic_train_arrange['Age'])\ntitanic_test_arrange['Age']= round(titanic_test_arrange['Age'])\n","8b8793ba":"# # 2. Fare round off upto 2 decimal points\n\ntitanic_train_arrange['Fare']= round(titanic_train_arrange['Fare'],2)\ntitanic_test_arrange['Fare']= round(titanic_test_arrange['Fare'],2)\n\n\n","c92808f8":"print(titanic_train_arrange.describe())\n\nprint('\\n')\n\nprint(titanic_test_arrange.describe())","638cd13a":"# # 2. Dummy variable for sex column \n\ntitanic_train_dummy= pd.get_dummies(titanic_train_arrange,columns=['Sex','Embarked','Pclass'],drop_first=True)\ntitanic_test_dummy= pd.get_dummies(titanic_test_arrange,columns=['Sex','Embarked','Pclass'],drop_first=True)\n\n","87f98eef":"titanic_train_dummy.head()","96e0fb97":"titanic_test_dummy.head()","f4e7ef07":"##removal of unnecessary columns\n\ntitanic_train_ml= titanic_train_dummy.drop(['PassengerId','Name','Ticket','Cabin'],axis=1)\ntitanic_test_ml= titanic_train_dummy.drop(['PassengerId','Name','Ticket','Cabin'],axis=1)","a1b7a172":"## train test split\n\n# create target and independent vaiable\n\nX=titanic_train_ml.drop(['Survived'],axis=1)\ny=titanic_train_ml.filter(['Survived'])\n\n","ad1b3f5f":"\nX_train,X_test,y_train,y_test= train_test_split(X,y,test_size= 0.2,random_state=1)","e6ce89cf":"\n# create model\nLinear_model= LinearRegression()\n# fit the model\nLinear_model= Linear_model.fit(X_train,y_train)\n# Predict the model\nLinear_y_pred= Linear_model.predict(X_test)\n\nprint(Linear_y_pred)\n","c1c74e60":"#Model Fitted summary\n\nLinearmodel_ols=sm.OLS(y_train,X_train)\n\nresult=Linearmodel_ols.fit()\n\nprint(result.summary())","6b0bc18f":"\nprint(\"mean_absolute_error : \",metrics.mean_absolute_error(y_test,Linear_y_pred))\nprint(\"mean_squared_error : \",metrics.mean_squared_error(y_test,Linear_y_pred))\nprint(\"mean_squared_root_error :\",np.sqrt(metrics.mean_squared_error(y_test,Linear_y_pred)))\n\n","716dabf0":"# create model\nLogistic_model= LogisticRegression()\n# fit the model\nLogistic_model= Logistic_model.fit(X_train,y_train)\n# Predict the model\nLogistic_y_pred= Logistic_model.predict(X_test)\n\nprint(Logistic_y_pred)","253e48b6":"print(\"accuracy metrics :\",metrics.accuracy_score(y_test,Logistic_y_pred))\nprint('\\n')\nprint(\"Confusion metrics: \",metrics.confusion_matrix(y_test,Logistic_y_pred))\nprint('\\n')\nprint('\\n')\n\nprint('classification report: ',metrics.classification_report(y_test,Logistic_y_pred))","15752527":"# create model\ndecision_tree_model= DecisionTreeClassifier()\n# fit the model\ndecision_tree_model= decision_tree_model.fit(X_train,y_train)\n# Predict the model\ndecision_tree_y_pred= decision_tree_model.predict(X_test)\n\nprint(decision_tree_y_pred)","97171a80":"print(\"accuracy metrics :\",metrics.accuracy_score(y_test,decision_tree_y_pred))\nprint('\\n')\nprint(\"Confusion metrics: \",metrics.confusion_matrix(y_test,decision_tree_y_pred))\nprint('\\n')\nprint('\\n')\n\nprint('classification report: ',metrics.classification_report(y_test,decision_tree_y_pred))","4abce48d":"kfold= model_selection.KFold(n_splits=10,random_state=7)\ny=np.array(y)\ncart= DecisionTreeClassifier()\nnum_trees=100\nmodel= BaggingClassifier(base_estimator=cart,n_estimators=num_trees,random_state=7)\nresults= model_selection.cross_val_score(model,X,y,cv=kfold)\nprint(results.mean())","b3633ce2":"print(results)","2ef3cd83":"kfold= model_selection.KFold(n_splits=10,random_state=10)\ncart= DecisionTreeClassifier()\nnum_trees=100\nmodel= AdaBoostClassifier(base_estimator=cart,n_estimators=num_trees,random_state=10)\nresults= model_selection.cross_val_score(model,X,y,cv=kfold)\nprint(results.mean())","369b1800":"### Feature engineering","3ead56f8":"# Machine learning","d6c7be26":"# Overview\n\nThe data has been split into two groups:\n\ntraining set (train.csv)\ntest set (test.csv)\nThe training set should be used to build your machine learning models. For the training set, we provide the outcome (also known as the \u201cground truth\u201d) for each passenger. Your model will be based on \u201cfeatures\u201d like passengers\u2019 gender and class. You can also use feature engineering to create new features.\n\nThe test set should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.\n\nWe also include gender_submission.csv, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.\n\n\nData Dictionary\n## Variable\t  ## Definition\t   ## Key\n        survival\t    Survival\t        0 = No, 1 = Yes\n        pclass\t       Ticket class\t        1 = 1st, 2 = 2nd, 3 = 3rd\n        sex\t            Sex\t\n        Age\t            Age in years\t\n        sibsp\t  # of siblings \/ spouses aboard the Titanic\t\n        parch\t  # of parents \/ children aboard the Titanic\t\n        ticket\t       Ticket number\t\n        fare\t       Passenger fare\t\n        cabin\t       Cabin number\t\n        embarked\tPort of Embarkation\t    C = Cherbourg, Q = Queenstown, S = Southampton\n        \n\n### Variable Notes\n\n#### pclass: A proxy for socio-economic status (SES)\n* 1st = Upper\n* 2nd = Middle\n* 3rd = Lower\n\n\n### age: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n\n######################################################################\n\n### sibsp: The dataset defines family relations in this way...\n\n* Sibling = brother, sister, stepbrother, stepsister\n* Spouse = husband, wife (mistresses and fianc\u00e9s were ignored)\n\n### parch: The dataset defines family relations in this way...\n\n* Parent = mother, father\n* Child = daughter, son, stepdaughter, stepson\n* Some children travelled only with a nanny, therefore parch=0 for them.","d9f9a690":"## Trying out Ensemble models","658dd7d7":"## replacing missing vales","83b66c31":"## 1. Linear regresssion","c6725026":"# calculate metrics","116cec6a":"## Logistic regression","1335cdbd":"# Create multiple models\n\n-----------------------------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------------------------","345047d8":"## calculate metrics","114cc64e":"## 1.Trying out bagging classifier","9960526a":"# Correlation analysis","e2b00fd8":"## EDA","f852e23d":"# Decision tree","a53e71e7":"# Accuracy metrics","35c5b5ab":"## 2.Trying out Adaboost classifier"}}