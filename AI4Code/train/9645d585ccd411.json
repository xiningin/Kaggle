{"cell_type":{"d5052c6e":"code","d03030aa":"code","e0f8e9ba":"code","96f7dcff":"code","101a653e":"code","41436813":"code","dce282fd":"code","0d2ed6ff":"markdown","143b3167":"markdown","06c76abf":"markdown","500b29ac":"markdown","ff70956a":"markdown","34941f20":"markdown","89d22751":"markdown"},"source":{"d5052c6e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d03030aa":"Data = pd.read_csv('\/kaggle\/input\/heart-failure-prediction\/heart.csv')\nprint(Data.head())\nprint('--------------------')\nprint(Data.describe())\nprint('--------------------')\nprint(Data.dtypes)\nprint('--------------------')\nsns.pairplot(Data, hue = 'HeartDisease')","e0f8e9ba":"ModelData = pd.get_dummies(Data,columns = ['Sex','ChestPainType','RestingECG','ExerciseAngina','ST_Slope'])","96f7dcff":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier as KNN\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.svm import SVC\n\nSEED = 42\n\nx_train,x_test,y_train,y_test = train_test_split(ModelData.drop(columns = 'HeartDisease'),ModelData['HeartDisease'],\n                                                 train_size = 0.7,stratify = ModelData['HeartDisease'],\n                                                random_state = SEED)\n\nScaler = StandardScaler()\nx_train = Scaler.fit_transform(x_train)\nx_test = Scaler.fit_transform(x_test)","101a653e":"lr = LogisticRegression(random_state = SEED)\nknn = KNN()\ndt = DecisionTreeClassifier(random_state = SEED)\nsvc = SVC()\n\nclassifiers = [('Log_Reg',lr),\n              ('KNN',knn),\n              ('Decision_Tree_Class',dt),\n              ('Support_Vector_Classifier',svc)]","41436813":"PARAMS = {'Log_Reg__C': [0.01,0.1,1],\n         'Log_Reg__solver': ['liblinear'],\n         'Log_Reg__penalty': ['l1', 'l2'],\n         'Support_Vector_Classifier__C': [1,2,4],\n         'KNN__n_neighbors': [3,5,7,9],\n         'Decision_Tree_Class__max_depth': [3,5,7,9],\n         'Decision_Tree_Class__min_samples_leaf': [1,2,8,0.01]\n         }","dce282fd":"for Model, clf in classifiers:\n    clf.fit(x_train,y_train)\n    y_pred = clf.predict(x_test)\n    print('{:s} : {:.3f}'.format(Model,accuracy_score(y_test,y_pred)))\nprint('---------------')\n\nVC = VotingClassifier(estimators = classifiers)\nVC.fit(x_train,y_train)\ny_pred = VC.predict(x_test)\nprint('Voting Classifier : {:.3f}'.format(accuracy_score(y_test,y_pred)))\n\n\ngrid_VC = GridSearchCV(VC, param_grid=PARAMS, cv=5,n_jobs = -1)\ngrid_VC.fit(x_train,y_train)\nprint(grid_VC.best_params_)\ny_pred = grid_VC.predict(x_test)\nprint('Voting Classifier with hyper-parameter tuning: {:.3f}'.format(accuracy_score(y_test,y_pred)))","0d2ed6ff":"Now that the hyper parameter grid is established, we can look at the accuracy of predictions for each model seperatly, an un-tuned voting class model and the accuracy of a tuned voting class ensemble model.","143b3167":"Ensemble model of 4 classifiers leads to a much better accuracy than each model alone.  Standardizing the data also shows a marked improvment over raw data values.  Teh default values for hyper parameters do a good enough job.  More advanced hyper parameter search methods may yeild better results but could also lead to overfitting... most likely the issue here. ","06c76abf":"Create paramater array for Gridsearch Cross-validation hyperparameter tuning for each model","500b29ac":"Load in the ML packages, split the data into training and tests sets and also standarize the numerical data.  Standardization occures after the train\/test split to ensure that there is no data leakage between the training and test set.  If standardization occurs prior to the split, then some distribution information is present in the test set that would not be otherwise.","ff70956a":"Create a sparse matrix of all of the categorical columns using the \"get_dummies\" method. AKA one-hot encoding","34941f20":"Load the data into the environment and look at correlations between data points as well as the Taget Flag.  Also figure out what sort of preprocessing will need to take place.","89d22751":"Initalize the ML methods to be used"}}