{"cell_type":{"71a7dc84":"code","6e66fa6b":"code","eaf715ad":"code","34b8333f":"code","290534e6":"code","9cabf01e":"code","8d4ee17b":"code","6cb65483":"code","605c6ef9":"code","e2351a69":"code","640e4b3b":"code","7b88146b":"code","07b51eef":"markdown"},"source":{"71a7dc84":"%matplotlib inline \n\nimport numpy as np \nimport matplotlib.pyplot as plt \n\nfrom keras.datasets import fashion_mnist \nfrom keras.layers import Dense, Flatten, Reshape \nfrom keras.layers.advanced_activations import LeakyReLU\nfrom keras.optimizers import Adam\nfrom keras.models import Sequential ","6e66fa6b":"rows = 28\ncols = 28\nchannels = 1 #fashion_mnist grayscale \nnoise = 100","eaf715ad":"def generator(noise, rows, cols, channels):\n    shape = (rows, cols, channels)\n    model = Sequential()\n    model.add(Dense(128, input_dim = noise))\n    model.add(LeakyReLU(alpha = 0.01))\n    model.add(Dense((rows*cols*channels), activation = 'tanh'))\n    model.add(Reshape(shape))\n    return model","34b8333f":"def discriminator(rows, cols, channels):\n    shape = (rows, cols, channels)\n    model = Sequential()\n    model.add(Flatten(input_shape = shape))\n    model.add(Dense(128))\n    model.add(LeakyReLU(alpha = 0.01))\n    model.add(Dense(1, activation = 'sigmoid'))\n    return model","290534e6":"def gan(generator, discriminator):\n    model = Sequential()\n    model.add(generator)\n    model.add(discriminator)\n    return model","9cabf01e":"generator = generator(noise, rows, cols, channels)\ndiscriminator = discriminator(rows, cols, channels)\ndiscriminator.compile(loss = 'binary_crossentropy', \n                     optimizer = Adam(),\n                     metrics = ['accuracy'])\ndiscriminator.trainable = False\ngan = gan(generator, discriminator)\ngan.compile(loss = 'binary_crossentropy', optimizer = Adam())","8d4ee17b":"check_points = []\nloss = []\naccuracy = []","6cb65483":"def train(iterations, sample_interval, batch_size):\n    \n    (X_train, _), (_, _) = fashion_mnist.load_data()\n    X_train = X_train \/ 127.5 - 1.0\n    X_train = np.expand_dims(X_train, axis=3)\n\n    true = np.ones((batch_size, 1))\n    fake = np.zeros((batch_size, 1))\n\n    for i in range(iterations):\n\n        idt = np.random.randint(0, X_train.shape[0], batch_size)\n        true_images = X_train[idt]\n\n        idf = np.random.normal(0, 1, (batch_size, 100))\n        fake_images = generator.predict(idf)\n\n        disc_loss_true = discriminator.train_on_batch(true_images, true)\n        disc_loss_fake = discriminator.train_on_batch(fake_images, fake)\n        disc_loss, tot_accuracy = 0.5 * np.add(disc_loss_true, disc_loss_fake)\n\n        idf = np.random.normal(0, 1, (batch_size, 100))\n        fake_images = generator.predict(idf)\n\n        gen_loss = gan.train_on_batch(idf, true)\n\n        if (i + 1) % sample_interval == 0:\n\n            loss.append((disc_loss, gen_loss))\n            accuracy.append(100.0 * tot_accuracy)\n            check_points.append(i + 1)\n\n            print(\"%d [Discriminator loss: %f, accuracy.: %.2f%%] [Generator loss: %f]\" %\n                  (i + 1, disc_loss, 100.0 * tot_accuracy, gen_loss))\n            \n            display_images(generator, noise)\n    ","605c6ef9":"def display_images(generator, noise, grid_rows=4, grid_cols=4):\n\n    n = np.random.normal(0, 1, (grid_rows * grid_cols, noise))\n    images = generator.predict(n)\n    images = 0.5 * images + 0.5\n\n    fig, axs = plt.subplots(grid_rows,\n                            grid_cols,\n                            figsize=(4, 4),\n                            sharey=True,\n                            sharex=True)\n\n    count = 0\n    for i in range(grid_rows):\n        for j in range(grid_cols):\n            axs[i, j].imshow(images[count, :, :, 0], cmap='gray')\n            axs[i, j].axis('off')\n            count += 1","e2351a69":"iterations = 20000\nsample_interval = 1000\nbatch_size = 128\n\ntrain(iterations, sample_interval, batch_size)","640e4b3b":"loss = np.array(loss)\n\nplt.figure(figsize=(15, 5))\nplt.plot(check_points, loss.T[0], label=\"Discriminator Loss\")\nplt.plot(check_points, loss.T[1], label=\"Generator Loss\")\nplt.xticks(check_points, rotation=90)\nplt.title(\"Training Loss\")\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Loss\")\nplt.legend()","7b88146b":"accuracy = np.array(accuracy)\n\n# Plot Discriminator accuracy\nplt.figure(figsize=(15, 5))\nplt.plot(check_points, accuracy, label=\"Discriminator accuracy\")\nplt.xticks(check_points, rotation=90)\nplt.yticks(range(0, 100, 5))\nplt.title(\"Discriminator Accuracy\")\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Accuracy (%)\")\nplt.legend()","07b51eef":"# For supported blog write up check this link out!\nhttps:\/\/medium.com\/delvify\/introduction-to-generative-adversarial-networks-gans-ecb1ac0b0d83"}}