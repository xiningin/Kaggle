{"cell_type":{"b5c8b2b7":"code","ade5dd66":"code","add9dcd9":"code","d4f8dfa6":"code","b04f2d38":"code","03433685":"code","21792706":"code","e6fe6c1a":"code","1770cd2f":"code","d6929065":"code","2e817aa9":"code","dddc5032":"code","c2cfbd5d":"code","c005a8c8":"code","f09d1c8e":"code","bf140e76":"code","4bd65f73":"code","edcf72d4":"code","7be19342":"code","c83c7d8b":"code","5730efc3":"code","40103eca":"code","c6f71442":"code","13e81d80":"code","468d38fb":"code","2ae11189":"code","603d3e25":"code","4be34419":"code","10dd43de":"code","a9348c39":"code","9e812de0":"code","a0422d17":"code","175c3048":"code","2978d50b":"code","ac8d8876":"code","54490d29":"code","384f311e":"code","74469b4b":"code","c44c3610":"code","05d5c6c5":"code","827f1cb4":"code","7bf09b17":"code","77898eec":"code","7b729a29":"code","555a30ee":"code","fb007d3e":"code","23ac15ce":"code","e21acba2":"code","d98fbe85":"code","f723495e":"code","f366e833":"code","b281f6f6":"code","e8f13681":"code","31e4358d":"code","e49ebd76":"code","fc0fcba7":"code","89da1eed":"code","346d2930":"code","a2cb8891":"code","28259f81":"code","abeadb60":"code","462f0080":"code","2d2ec057":"code","7df2193b":"code","5799db2a":"code","723086e6":"code","2b907c5f":"code","66dc9bff":"code","57bed4e4":"code","8b5387a6":"code","76a1d02f":"code","4d55e8b8":"code","f4a6dee3":"code","85e10a62":"code","5c02284a":"code","d9f8b036":"code","601ba4d5":"code","0f868e0d":"code","fbf51c89":"code","9b0e9f63":"code","8abe0277":"code","9c6dcccf":"code","0196fd58":"code","29a03c0b":"code","9931299c":"code","205f844f":"code","c8eaa699":"code","e033b520":"code","781d14d3":"code","3aab9935":"code","4b156bd3":"code","e911f031":"code","acc4bffb":"code","5bd55c5f":"code","e3959502":"code","a4d341dd":"code","e1ea26af":"code","988b2df8":"code","58cfcedf":"code","1b8fa9bc":"code","0eade648":"code","acfdb4fb":"code","7e228a4a":"code","4852fbd0":"code","d1b0f364":"code","08f03a5d":"code","1ffdcd71":"code","b3d73bae":"code","5dda2d60":"code","fdb6ab2f":"code","019a4aa7":"code","b8331c6f":"code","a19c40ed":"code","e50b5e46":"code","d84458cb":"code","fcb39ded":"code","011dc835":"code","6cc65363":"code","e631d96d":"code","067e4d64":"code","c22803a7":"code","5f6ab9a6":"code","e62b81f1":"code","6debae47":"code","2d46e3e0":"code","05ee3f17":"code","9607c582":"code","9c8a637c":"code","b22a9931":"code","ee7b9115":"code","04ce8d21":"code","92699aed":"code","65bdeda4":"code","7417727a":"code","1d2d2c24":"code","89407e0d":"code","82bd9c87":"code","9140b420":"code","6c9defad":"code","89077953":"code","6de7498a":"code","c4fa6ce9":"code","d5cec6a7":"code","ca674da5":"code","d237e24f":"code","9ed5aef4":"code","eadbb5fa":"code","781fdd6b":"code","f07d3061":"code","0c4da2aa":"code","1b59285f":"code","a4287c08":"code","313b15e7":"code","4601dfd7":"code","2c7250d9":"code","1df7ec20":"code","45b66c86":"code","71550e87":"code","cdd99054":"code","aa5566d2":"code","d5ef648f":"code","92612b5d":"code","d95f082a":"code","369e3da9":"code","d3715363":"code","4d605da4":"code","54963782":"code","e8617ef4":"code","cc83f2f5":"code","fb945001":"code","9e45383e":"code","2e134bab":"code","3b68cbed":"code","2c582510":"code","6279cc4f":"code","5b92698b":"code","bd6376ad":"code","b40aaa8a":"code","ec8b8e9d":"code","c83f5b31":"code","56cf9bd7":"code","d2d43819":"code","47e1eb00":"code","ec9a2e0f":"code","e026c6d3":"code","19f5be05":"code","26510bd5":"code","a3b9f806":"code","0f524806":"code","6620f881":"code","05e6f4a0":"code","e18c327f":"code","525b57c7":"code","9c6c5acb":"code","656c3063":"code","ef9a3585":"code","141ceed4":"code","9e87cb9d":"code","08e15dcb":"code","6be3d6a1":"code","b0a08788":"code","e3c9df4e":"code","8fa9d604":"code","c914edee":"code","f5e331f5":"code","fd89e0fc":"code","925d5fdb":"code","44448188":"code","d6a2f5ad":"code","8093bfd3":"code","ac1c2446":"code","50c4def2":"code","2548b907":"code","bc87e758":"code","1c36ec8b":"code","8bdff170":"code","77e13ac9":"code","c815298d":"code","74b492b5":"code","6265e8b4":"code","bbb0ef9c":"code","6c001864":"code","fa5f0fab":"code","69ea7f21":"code","f5284d32":"code","dfe51652":"code","6d7714b9":"code","b1f13939":"code","d91c9f10":"code","c25acd9b":"code","21e2355d":"code","69c9e6ff":"code","0baf7961":"code","b4c6bcf2":"code","c91d907c":"code","31e57e81":"code","5d7832b6":"code","b40743db":"code","7381c3e9":"code","bc105fed":"code","a0f3d814":"code","3f86769f":"code","5424a1ea":"code","cae01524":"code","51bd42bb":"code","89c2d506":"code","026c29a4":"code","ba8284b0":"code","d1b37998":"code","eed97017":"code","3e5e1d73":"code","2f86e9a7":"code","48451588":"code","3b75b4ac":"code","850e60ca":"code","3503c859":"code","9d5947e3":"code","1526fed9":"code","96ec8ad1":"code","5c65e12d":"code","b7d41f55":"code","f353a50a":"code","60f3c01e":"code","bcece506":"code","97fb6bae":"code","531c3b88":"markdown","147aefca":"markdown","fbb1fe9f":"markdown","1378c494":"markdown","0af804dd":"markdown","bfdde79e":"markdown","4fbf737e":"markdown","dbb13a3a":"markdown","40f3df4c":"markdown","1f1bd053":"markdown","52ddded8":"markdown","7ab2aa05":"markdown","46809717":"markdown","a1cdd0f0":"markdown","6519148a":"markdown","c9ca658f":"markdown","3eb98d29":"markdown","0973e494":"markdown","f0a5e505":"markdown","154f5129":"markdown","f73a6faf":"markdown","e80b2261":"markdown","887cec59":"markdown","f883f2e4":"markdown","5d3d05b5":"markdown","37770299":"markdown","cdfdeb15":"markdown","5f38ccd1":"markdown","84d27888":"markdown","0c99e89e":"markdown","9a66d685":"markdown","07fc5bf8":"markdown","7c76807c":"markdown","f8020b9b":"markdown","c15a197d":"markdown","349b4d26":"markdown","25d3ee86":"markdown","8b5600dc":"markdown","1f591af3":"markdown","94b65cf2":"markdown","b304d4e2":"markdown","5d0e4275":"markdown","f50d4706":"markdown","b74c65d9":"markdown","adaedb69":"markdown","05a9977a":"markdown","b3ecd9de":"markdown","3772ef71":"markdown","5f808fe7":"markdown","9df3506d":"markdown","63eb9411":"markdown","1dff2d50":"markdown","ef2dd859":"markdown","795ce653":"markdown","9679fd9f":"markdown","d8ee0431":"markdown","fdd1c9e8":"markdown","12de9aad":"markdown","03130a3e":"markdown","df80e49c":"markdown","4416b081":"markdown","7fb91a38":"markdown","3950b8bc":"markdown","b826f736":"markdown","c309e263":"markdown","00ed0305":"markdown","391ef3f8":"markdown","ce7ff5bb":"markdown","65d63f8e":"markdown","9c88294c":"markdown","9fee66a4":"markdown","d46ae834":"markdown","032cbc9d":"markdown","d577f009":"markdown","b1a91e5d":"markdown","a2e3c27a":"markdown","3e85d597":"markdown","235127c2":"markdown","0ff0e504":"markdown","97f7af7c":"markdown","73a2f388":"markdown","b92cdab0":"markdown","1a923187":"markdown","b81ad954":"markdown","d09ca290":"markdown","af3c1c81":"markdown","96bba26d":"markdown","af044c4a":"markdown","dff0a90d":"markdown","24eb437e":"markdown","bec5b499":"markdown","1f85e060":"markdown","9743d880":"markdown","2bfcbd99":"markdown","ce3004ab":"markdown","96febcd0":"markdown","742724c2":"markdown","39194f3a":"markdown","7fb693b8":"markdown","7066755e":"markdown","0f10f9c1":"markdown","a54b9870":"markdown","b62cc712":"markdown","3a153fd9":"markdown","31425244":"markdown","ee2ae64e":"markdown","2e9aded6":"markdown","36945bb1":"markdown","8395dc7e":"markdown","5876080c":"markdown","7a69b4e8":"markdown","ac62f5c7":"markdown","bc01362b":"markdown","5165a3b5":"markdown","3fc4acaa":"markdown","e2287c55":"markdown","d9bedc00":"markdown","600f294f":"markdown","4efe2d27":"markdown","ff67d1ba":"markdown","27cc4b9e":"markdown","edce06c2":"markdown","e204f451":"markdown","049bd5b6":"markdown","1c34ee4a":"markdown","cc090c88":"markdown","16ccf04e":"markdown","68d4c03b":"markdown","dc1dff44":"markdown","87185364":"markdown","09725b3c":"markdown","214b082a":"markdown","84135e6e":"markdown","72bc34d7":"markdown","22749254":"markdown","301094d9":"markdown","1485c64c":"markdown","a7ab6178":"markdown","982fae7e":"markdown","df5779f8":"markdown","5212c841":"markdown","61df46ea":"markdown","efe67d70":"markdown","00177c2c":"markdown","09d8f83d":"markdown","f238665a":"markdown","6f841e37":"markdown","24b4793a":"markdown","0d4c64fa":"markdown","18adf8d2":"markdown","77dcbad1":"markdown","062333d3":"markdown","1e06925d":"markdown","73a7da71":"markdown","ad168e83":"markdown","0c81a7c2":"markdown","4a92d5bc":"markdown","66ca9d54":"markdown","0ae35511":"markdown","43276e35":"markdown","b0cce10d":"markdown","fd6a072a":"markdown","43733bee":"markdown","c516721f":"markdown","e7e15585":"markdown","94208b16":"markdown","283e75e2":"markdown","c0d99eb3":"markdown","b8c4cb23":"markdown","0e1a3830":"markdown","57d7d7d1":"markdown","2f1d4479":"markdown","793b74c9":"markdown","06d4039f":"markdown","75b3b18e":"markdown","8c697837":"markdown","3e23a0a2":"markdown","f16d335d":"markdown","dc22a667":"markdown","46d64baa":"markdown","12f7669d":"markdown","220aeffc":"markdown","b06f9f8e":"markdown","45e64cda":"markdown","33b07919":"markdown","f9377368":"markdown","0a7d7d95":"markdown","56382f4f":"markdown","21323e1b":"markdown","b322e94a":"markdown","5514d59c":"markdown","e1806f0a":"markdown","9cfca1f6":"markdown","59fc10c7":"markdown","c48adb53":"markdown","780b64f4":"markdown","6c41f10b":"markdown","7bc783a1":"markdown","751aa745":"markdown","d5eeed68":"markdown","ce126050":"markdown","7bbcc2a0":"markdown","650d64d5":"markdown","1d98a2c0":"markdown","2fc493d0":"markdown","de0e3804":"markdown","c7826c74":"markdown","abb53749":"markdown","bd4664bf":"markdown","19882a32":"markdown","e6db84a6":"markdown","1dd34916":"markdown","8788f3a7":"markdown","83430c82":"markdown","96c1b94c":"markdown","4e64fe65":"markdown","69698eeb":"markdown","a2812593":"markdown"},"source":{"b5c8b2b7":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport sklearn.metrics as metrics\nimport pandas_profiling as pdp\nimport warnings\nimport adj_helper as helper\nimport pickle as pk\n\n\nfrom math import sqrt\nfrom sklearn import datasets\nfrom sklearn.metrics import mean_squared_error \nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import r2_score\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom sklearn.linear_model import LinearRegression\nfrom yellowbrick.regressor import ResidualsPlot\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.ensemble import ExtraTreesClassifier\n\n%matplotlib inline\n\nwarnings.filterwarnings('ignore')","ade5dd66":"with pd.ExcelFile('..\/input\/regression-genome-assembly\/genome.xlsx') as xlsx:\n    plasmid = pd.read_excel(xlsx, 'NBB4 Plasmid')\n    hamburgensis = pd.read_excel(xlsx, 'Hamburgensis X14')\n    vibrio = pd.read_excel(xlsx, 'Vibrio Cholerae')\n    pab1 = pd.read_excel(xlsx, 'PAb1')","add9dcd9":"plasmid.describe()","d4f8dfa6":"plasmid.info()","b04f2d38":"plasmid.drop(\"Sr.No\",axis=1, inplace = True)\nplasmid = plasmid.replace(['-'],'NaN')\nplasmid['MARAGAP'] = pd.to_numeric(plasmid['MARAGAP'], downcast='float')\nplasmid['Mira'] = plasmid['Mira'].astype(float)\nplasmid['Mira2'] = plasmid['Mira2'].astype(float)\nplasmid['Maq'] = plasmid['Maq'].astype(float)\nplasmid = plasmid.fillna('0')","03433685":"plasmid = plasmid.set_index('Assembly_Metrics').T.astype(float)\nplasmid","21792706":"plasmid.info()","e6fe6c1a":"X = plasmid.iloc[:,0:13]\nY = plasmid[['Order']]","1770cd2f":"X = MinMaxScaler().fit_transform(X)\nX = pd.DataFrame({'Number_of_Contigs': X[:, 0], 'Length_of_Largest_Contig': X[:, 1],'N50': X[:, 2],'N75': X[:, 3], 'N90': X[:, 4], 'NG50': X[:,5], 'NG75': X[:, 6], 'Contigs_greater_and_equal_N50': X[:, 7],'Contigs_greater_and_equal_200': X[:, 8],'Mean': X[:, 9], 'Median': X[:, 10],'Sum_of_the_Contig_Lengths': X[:, 11], 'Coverage':X[:, 12]})\nX","d6929065":"data = {'Assembly_Metrics':['Velvet', 'VCAKE', 'SSAKE', 'QSRA','SHARCGS','IDBA','Mira','Mira2','Maq','MARAGAP']} \ndata = pd.DataFrame(data)\nX['Assembly_Metrics'] = data\nX = X.set_index('Assembly_Metrics')\nX","2e817aa9":"plasmid = pd.concat([X, Y], axis=1)\nplasmid","dddc5032":"a = sns.pairplot(plasmid, x_vars=['Number_of_Contigs','Length_of_Largest_Contig','N50'], y_vars='Order', height=5, aspect=0.9)\nb = sns.pairplot(plasmid, x_vars=['N75','Contigs_greater_and_equal_N50','Contigs_greater_and_equal_200'], y_vars='Order', height=5, aspect=0.9)\nc = sns.pairplot(plasmid, x_vars=['Mean','Median','Sum_of_the_Contig_Lengths','Coverage'], y_vars='Order', height=5, aspect=0.9)","c2cfbd5d":"correlations = plasmid.corr()\ncorrelations","c005a8c8":"mask = np.triu(np.ones_like(correlations, dtype=np.bool))\n\nplt.figure(figsize=(11,8))\nsns.heatmap(correlations*100,mask=mask,annot=True, fmt='.0f' )","f09d1c8e":"X_ = plasmid.drop(['Contigs_greater_and_equal_N50','Contigs_greater_and_equal_200','Median','Sum_of_the_Contig_Lengths','Coverage'],axis=1)\nY_ = plasmid[['Order']]\nprint(X_.shape)\nprint(Y_.shape)","bf140e76":"X1 = X_.corr()","4bd65f73":"mask = np.triu(np.ones_like(X1, dtype=np.bool))\n\nplt.figure(figsize=(11,8))\nsns.heatmap(X1*100,mask=mask,annot=True, fmt='.0f' )","edcf72d4":"bestfeatures = SelectKBest(score_func=chi2, k=9)\nfit = bestfeatures.fit(X_,Y_)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X_.columns)\n#concat two dataframes for better visualization \nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['Specs','Score']  #naming the dataframe columns\nprint(featureScores.nlargest(10,'Score'))  #print 10 best features","7be19342":"feature_imp = ExtraTreesClassifier()\nfeature_imp.fit(X_,Y_)\nprint(feature_imp.feature_importances_)","c83c7d8b":"feature_importance = pd.Series(feature_imp.feature_importances_, index=X_.columns)\nplt.figure(figsize=(12,8))\nfeature_importance.nlargest(13).plot(kind='barh')\nplt.show()\n","5730efc3":"X__ = X_.drop(['Order','Number_of_Contigs'],axis=1)\npredict_plasmid = X_.drop(['Order','Number_of_Contigs'],axis=1)\nX__","40103eca":"#profile = pdp.ProfileReport(X__)\n#profile","c6f71442":"model = LinearRegression()","13e81d80":"model = model.fit(X__, Y_)","468d38fb":"coefficients = model.coef_\nco = pd.DataFrame(coefficients).astype(float)\nco","2ae11189":"print(\"Intercept: \", model.intercept_)","603d3e25":"pred = model.predict(X__)\npred.astype(int)","4be34419":"r2_regression = model.score(X__, Y_)\nprint('R^2: {0}'.format(r2_regression))","10dd43de":"train_r2_1=r2_score(Y_,model.predict(X__))\ntest_r2_1=r2_score(Y_,pred)","a9348c39":"Xb = X__['N75']\nplt.plot(Xb, Y_,'o')\nm,b = np.polyfit(Xb,Y_,1)\nplt.plot(Xb,m*Xb+b)","9e812de0":"Xd = X__['N90']\nplt.plot(Xd, Y_,'o')\nm,b = np.polyfit(Xd,Y_,1)\nplt.plot(Xd,m*Xd+b)\n","a0422d17":"Xf = X__['NG75']\nplt.plot(Xf, Y_,'o')\nm,b = np.polyfit(Xf,Y_,1)\nplt.plot(Xf,m*Xf+b)\n","175c3048":"r_2_1 = []\nfor i in range(1, (X__.shape[-1])+1):\n    m1=LinearRegression()\n    m1.fit(X__.values[:,:i],Y_)\n    prd1=m1.predict(X__.values[:,:i])\n    r_2_1.append(r2_score(Y_,prd1))\n    \nplt.figure(figsize=(15,5))\nplt.plot(r_2_1);\nplt.xlabel('Features')\nplt.ylabel('R_2 Score')","2978d50b":"plt.figure(figsize=(15,6)) \nvisualizer = ResidualsPlot(model,hist=True)\nvisualizer.fit(X__.values, Y_.values)  \nvisualizer.score(X__.values, Y_.values)  \nvisualizer.poof()    ","ac8d8876":"mse_regression = mean_squared_error(Y_,pred)\nrmse_regression = sqrt(mse_regression)\nmae_regression = metrics.mean_absolute_error(Y_,pred)\nvrs_regression = metrics.explained_variance_score(Y_,pred)\nr_square_model = model.score(X__, Y_)\n\nprint('MEAN SQUARE ERROR : {0:.4f}'.format(mse_regression))\nprint('ROOT MEAN SQUARE ERROR : {0:.4f}' .format(rmse_regression))\nprint('MEAN ABSOLUTE ERROR : {0:.4f}'.format(mae_regression))\nprint('VARIANCE REGRESSION SCORE :{0:.4f}'.format(vrs_regression))\nprint('Model R Square :{0:.4f}'.format(r_square_model))","54490d29":"pk.dump(model, open(\"model_linear.pkl\",\"wb\"))","384f311e":"model_linear=pk.load(open(\"model_linear.pkl\",'rb'))\npredict_linear=model_linear.predict(predict_plasmid)\npredict_linear +=1\npred_linear=predict_linear.astype(int).flatten()\nOrder_reg = pd.DataFrame(pred_linear,columns=['Predicted_Order'])\nOrder_reg['Assemblers'] = data\nOrder_reg.sort_values(by='Predicted_Order')","74469b4b":"Y_=np.ravel(Y_)","c44c3610":"randomregressor = RandomForestRegressor(max_depth=3, random_state=0, max_features=7)\nrandom_regressor = randomregressor.fit(X__,Y_)\nrandom_model=random_regressor.predict(X__)","05d5c6c5":"random_model.astype(int)","827f1cb4":"r2_forest = randomregressor.score(X__, Y_)\nprint('R^2: {0:.4f}'.format(r2_forest))","7bf09b17":"mse_random = mean_squared_error(Y_,random_model)\nrmse_random = sqrt(mse_random)\nmae_random = metrics.mean_absolute_error(Y_,random_model)\nvrs_random = metrics.explained_variance_score(Y_,random_model)\nprint('MEAN SQUARE ERROR : {0:.4f}'.format(mse_random))\nprint('ROOT MEAN SQUARE ERROR : {0:.4f}'.format(rmse_random)) \nprint('MEAN ABSOLUTE ERROR : {0:.4f}'.format(mae_random))\nprint('VARIANCE REGRESSION SCORE :{0:.4f}'.format(vrs_random))\nprint('R Square : {0:.4f}'.format(r2_forest))","77898eec":"pk.dump(randomregressor, open(\"model_random.pkl\",\"wb\"))","7b729a29":"model_random=pk.load(open(\"model_random.pkl\",'rb'))\npred_random=model_random.predict(predict_plasmid)\npred_random=pred_random.astype(int)\nOrder_random = pd.DataFrame(pred_random,columns=['Predicted_Order'])\nOrder_random['Assemblers'] = data\nOrder_random.sort_values(by='Predicted_Order')","555a30ee":"ridge = Ridge()\nparameters = {'alpha':[0.000001,0.00001, 0.001, 0.01, 0.1, 0.3, 0.5]}\nridge_model_params = GridSearchCV(ridge, parameters,scoring='r2',cv=7)\nridge_model_params.fit(X__,Y_)","fb007d3e":"best_ridge_params = ridge_model_params.best_params_\nprint(\"Best Ridge Params : \", best_ridge_params)","23ac15ce":"ridge_model=Ridge(alpha=0.000001).fit(X__, Y_)\nridge_coefficients= ridge_model.coef_\npd.DataFrame(ridge_coefficients).astype(float)","e21acba2":"print('Model Intercept : {0:.4f}'.format(ridge_model.intercept_))","d98fbe85":"ridge_model_predict=ridge_model.predict(X__)","f723495e":"r_square_ridge = ridge_model.score(X__, Y_)\nmse_ridge = mean_squared_error(Y_,ridge_model_predict)\nrmse_ridge = sqrt(mse_ridge)\nmae_ridge = metrics.mean_absolute_error(Y_,ridge_model_predict)\nvrs_ridge = metrics.explained_variance_score(Y_,ridge_model_predict)\n\nprint('MEAN SQUARE ERROR : {0:.4f}'.format(mse_ridge))\nprint('ROOT MEAN SQUARE ERROR : {0:.4f}'.format(rmse_ridge)) \nprint('MEAN ABSOLUTE ERROR : {0:.4f}'.format(mae_ridge))\nprint('VARIANCE REGRESSION SCORE : {0:.4f}'.format(vrs_ridge))\nprint('Model R Square: {0:.4f}'.format(r_square_ridge))","f366e833":"pk.dump(ridge_model, open(\"model_ridge.pkl\",\"wb\"))","b281f6f6":"model_ridge=pk.load(open(\"model_ridge.pkl\",'rb'))\npred_ridge=model_ridge.predict(predict_plasmid)\npred_ridge+=1\npred_ridge = pred_ridge.astype(int)\nOrder_ridge = pd.DataFrame(pred_ridge,columns=['Predicted_Order'])\nOrder_ridge['Assemblers'] = data\nOrder_ridge.sort_values(by='Predicted_Order')","e8f13681":"lasso = Lasso()\nparameters = {'alpha':[0.000001,0.00001, 0.001, 0.01, 0.1, 0.3, 0.5]}\nlasso_model = GridSearchCV(lasso, parameters,scoring='r2',cv=7)\nlasso_model = lasso_model.fit(X__,Y_)","31e4358d":"best_lasso_params = lasso_model.best_params_\nprint(\"Best lasso Params : \", best_lasso_params)","e49ebd76":"lasso = Lasso()\nlasso_model=Lasso(alpha=0.000001).fit(X__, Y_)","fc0fcba7":"lasso_model.coef_\nlasso_coefficients= lasso_model.coef_\npd.DataFrame(lasso_coefficients).astype(float)","89da1eed":"print('Model Intercept : {0:.4f}'.format(lasso_model.intercept_))","346d2930":"lasso_model_predict=lasso_model.predict(X__)","a2cb8891":"r_square_lasso = lasso_model.score(X__, Y_)\nmse_lasso = mean_squared_error(Y_,lasso_model_predict)\nrmse_lasso = sqrt(mse_lasso)\nmae_lasso = metrics.mean_absolute_error(Y_,lasso_model_predict)\nvrs_lasso = metrics.explained_variance_score(Y_,lasso_model_predict)\n\nprint('MEAN SQUARE ERROR : {0:.4f}'.format(mse_lasso))\nprint('ROOT MEAN SQUARE ERROR : {0:.4f}'.format(rmse_lasso)) \nprint('MEAN ABSOLUTE ERROR : {0:.4f}'.format(mae_lasso))\nprint('VARIANCE REGRESSION SCORE : {0:.4f}'.format(vrs_lasso))\nprint('Model R Square: {0:.4f}'.format(r_square_lasso))","28259f81":"pk.dump(lasso_model, open(\"model_lasso.pkl\",\"wb\"))","abeadb60":"model_lasso=pk.load(open(\"model_lasso.pkl\",'rb'))\npred_lasso=model_lasso.predict(predict_plasmid)\npred_lasso+=1\npred_lasso = pred_lasso.astype(int)\nOrder_lasso = pd.DataFrame(pred_lasso,columns=['Predicted_Order'])\nOrder_lasso['Assemblers'] = data\nOrder_lasso.sort_values(by='Predicted_Order')","462f0080":"alphas = [0.000001,0.00001, 0.001, 0.01, 0.1, 0.3, 0.5]\nfor a in alphas:\n    model = ElasticNet(alpha=a).fit(X__,Y_)   \n    score = model.score(X__, Y_)\n    pred_y = model.predict(X__)\n    mse_elastic = mean_squared_error(Y_, pred_y)   \n    print(\"Alpha:{0:.6f}, R2:{1:.2f}, MSE:{2:.2f}, RMSE:{3:.2f}\".format(a, score, mse_elastic, np.sqrt(mse_elastic)))","2d2ec057":"elastic_model=ElasticNet(alpha=0.000001).fit(X__, Y_)\nelastic_model_predict = elastic_model.predict(X__)\nscore = elastic_model.score(X__,Y_)\nmse = mean_squared_error(Y_, elastic_model_predict)\nprint(\"R2:{0:.2f}, MSE:{1:.2f}, RMSE:{2:.2f}\"\n      .format(score, mse, np.sqrt(mse)))","7df2193b":"elastic_model.coef_","5799db2a":"r_square_elastic = elastic_model.score(X__, Y_)\nmse_elastic = mean_squared_error(Y_,elastic_model_predict)\nrmse_elastic = sqrt(mse_elastic)\nmae_elastic = metrics.mean_absolute_error(Y_,elastic_model_predict)\nvrs_elastic = metrics.explained_variance_score(Y_,elastic_model_predict)\n\n\nprint('MEAN SQUARE ERROR : {0:.4f}'.format(mse_elastic))\nprint('ROOT MEAN SQUARE ERROR : {0:.4f}'.format(rmse_elastic)) \nprint('MEAN ABSOLUTE ERROR : {0:.4f}'.format(mae_elastic))\nprint('VARIANCE REGRESSION SCORE : {0:.4f}'.format(vrs_elastic))\nprint('Model R Square: {0:.4f}'.format(r_square_elastic))\n","723086e6":"pk.dump(elastic_model, open(\"model_elastic.pkl\",\"wb\"))","2b907c5f":"model_elastic=pk.load(open(\"model_elastic.pkl\",'rb'))\npred_elastic=model_elastic.predict(predict_plasmid)\npred_elastic+=1\npred_elastic = pred_elastic.astype(int)\nOrder_elastic = pd.DataFrame(pred_elastic,columns=['Predicted_Order'])\nOrder_elastic['Assemblers'] = data\nOrder_elastic.sort_values(by='Predicted_Order')","66dc9bff":"comparison={\"Models\":[\"Simple Regression\", \"Random Forest\",\"Ridge Regression\",\"Lasso Regression\",\"ElasticNET Regression\" ],\n            \"R_Square\/Accuracy\": [r2_regression, r2_forest,r_square_ridge,r_square_lasso,r_square_elastic],\n            \"Mean Square Error\/Accuracy\": [mse_regression, mse_random, mse_ridge,mse_lasso,mse_elastic],\n            \"Root Mean Sqaure Error\":[rmse_regression,rmse_random,rmse_ridge,rmse_lasso,rmse_elastic],\n            \"Mean Absoulate Error\" :[mae_regression,mae_random,mae_ridge,mae_lasso,mae_elastic],\n            \"Variance Regression Score\" :[vrs_regression,vrs_random,vrs_ridge,vrs_lasso,vrs_elastic]\n                   }\ncomparison = pd.DataFrame(comparison)\ncomparison","57bed4e4":"sort=comparison.sort_values('R_Square\/Accuracy',ascending=False)\nsort","8b5387a6":"hamburgensis.info()","76a1d02f":"hamburgensis.drop(\"Sr.No\",axis=1, inplace = True)\nhamburgensis = hamburgensis.replace(['-'],'NaN')\nhamburgensis['Maq'] = hamburgensis['Maq'].astype(float)\nhamburgensis = hamburgensis.fillna(0)\nhamburgensis = hamburgensis.set_index('Assembly_Metrics').T\nhamburgensis","4d55e8b8":"X2 = hamburgensis.drop(['Order'],axis=1)\nY2 = hamburgensis[['Order']]\nprint(X2.shape)\nprint(Y2.shape)","f4a6dee3":"X2 = MinMaxScaler().fit_transform(X2)\nX2 = pd.DataFrame({'Number_of_Contigs': X2[:, 0], 'Length_of_Largest_Contig': X2[:, 1],'N50': X2[:, 2],'N75': X2[:, 3], 'N90': X2[:, 4], 'NG50': X2[:,5], 'NG75': X2[:, 6], 'Contigs_greater_and_equal_N50': X2[:, 7],'Contigs_greater_and_equal_200': X2[:, 8],'Mean': X2[:, 9], 'Median': X2[:, 10],'Sum_of_the_Contig_Lengths': X2[:, 11], 'Coverage':X2[:, 12]})\nX2","85e10a62":"data2 = {'Assembly_Metrics':['Velvet', 'VCAKE', 'SSAKE', 'QSRA','SHARCGS','IDBA','Mira','Mira2','Maq','MARAGAP']} \ndata2 = pd.DataFrame(data2)\nX2['Assembly_Metrics'] = data2\nX2 = X2.set_index('Assembly_Metrics')\nX2","5c02284a":"hamburgensis = pd.concat([X2, Y2], axis=1)\nhamburgensis","d9f8b036":"a2 = sns.pairplot(hamburgensis, x_vars=['Number_of_Contigs','Length_of_Largest_Contig','N50'], y_vars='Order', height=5, aspect=0.9)\nb2 = sns.pairplot(hamburgensis, x_vars=['N75','Contigs_greater_and_equal_N50','Contigs_greater_and_equal_200'], y_vars='Order', height=5, aspect=0.9)\nc2 = sns.pairplot(hamburgensis, x_vars=['Mean','Median','Sum_of_the_Contig_Lengths','Coverage'], y_vars='Order', height=5, aspect=0.9)","601ba4d5":"correlations2 = hamburgensis.corr()\ncorrelations2","0f868e0d":"mask = np.triu(np.ones_like(correlations2, dtype=np.bool))\nplt.figure(figsize=(11,8))\nsns.heatmap(correlations2*100,mask=mask,annot=True, fmt='.0f' )","fbf51c89":"X_2 = hamburgensis.drop(['Contigs_greater_and_equal_200','Sum_of_the_Contig_Lengths','Coverage'],axis=1)\n\nY_2 = hamburgensis[['Order']]\nprint(X_2.shape)\nprint(Y_2.shape)","9b0e9f63":"bestfeatures2 = SelectKBest(score_func=chi2, k=11)\nfit2 = bestfeatures2.fit(X_2,Y_2)\ndfscores2 = pd.DataFrame(fit2.scores_)\ndfcolumns2 = pd.DataFrame(X_2.columns)\n#concat two dataframes for better visualization \nfeatureScores2 = pd.concat([dfcolumns2,dfscores2],axis=1)\nfeatureScores2.columns = ['Specs','Score']  #naming the dataframe columns\nprint(featureScores2.nlargest(8,'Score'))  #print 9 best features","8abe0277":"feature_imp_ham = ExtraTreesClassifier()\nfeature_imp_ham.fit(X_2,Y_2)\nprint(feature_imp_ham.feature_importances_)","9c6dcccf":"feature_importance_ham = pd.Series(feature_imp_ham.feature_importances_, index=X_2.columns)\nplt.figure(figsize=(12,8))\nfeature_importance_ham.nlargest(13).plot(kind='barh')\nplt.show()\n","0196fd58":"X_Final = X_2.drop(['Order','Length_of_Largest_Contig'],axis=1)\npredict_hamburgensis = X_2.drop(['Order','Length_of_Largest_Contig'],axis=1)\nX_Final","29a03c0b":"X_Final.describe()","9931299c":"profile_hamurgensis = pdp.ProfileReport(X_Final)\nprofile_hamurgensis","205f844f":"model2 = LinearRegression()","c8eaa699":"model2 = model2.fit(X_Final, Y_2)","e033b520":"pred2 = model2.predict(X_Final)","781d14d3":"r2_regression2 = model2.score(X_Final, Y_2)\nprint('R^2: {0}'.format(r2_regression2))","3aab9935":"model2.coef_","4b156bd3":"model2.intercept_","e911f031":"test_r2_2=r2_score(Y_2,pred2)","acc4bffb":"train_r2_2=r2_score(Y_2,model2.predict(X_Final))","5bd55c5f":"print('R2 score for testing:',test_r2_2 )\nprint('R2 score for training:',train_r2_2 )","e3959502":"r_2_2 = []\nfor i in range(1, (X_Final.shape[-1])+1):\n    m2=LinearRegression()\n    m2.fit(X_Final.values[:,:i],Y_2)\n    prd2=m2.predict(X_Final.values[:,:i])\n    r_2_2.append(r2_score(Y_2,prd2))","a4d341dd":"plt.figure(figsize=(15,5))\nplt.plot(r_2_2);\nplt.xlabel('Features')\nplt.ylabel('R_2 Score')","e1ea26af":"\nhelper.adjR2(X_Final,Y_2,test_r2_2)","988b2df8":"from yellowbrick.regressor import ResidualsPlot\n\nplt.figure(figsize=(15,6)) \nvisualizer2 = ResidualsPlot(model2,hist=True)\nvisualizer2.fit(X_Final.values, Y_2.values)  \nvisualizer2.score(X_Final.values, Y_2.values)  \nvisualizer2.poof()    ","58cfcedf":"mse2_regression = mean_squared_error(Y_2,pred2)\nrmse2_regression = sqrt(mse2_regression)\nmae2_regression = metrics.mean_absolute_error(Y_2,pred2)\nvrs2_regression = metrics.explained_variance_score(Y_2,pred2)\n\nprint(\"MEAN SQUARE ERROR : \", mse2_regression)\nprint(\"ROOT MEAN SQUARE ERROR : \", rmse2_regression)\nprint(\"MEAN ABSOLUTE ERROR : \", mae2_regression)\nprint(\"VARIANCE REGRESSION SCORE : \",vrs2_regression)","1b8fa9bc":"pk.dump(model2, open(\"model2_linear.pkl\",\"wb\"))","0eade648":"model2_linear=pk.load(open(\"model2_linear.pkl\",'rb'))\npred_linear2=model2_linear.predict(predict_hamburgensis)\npred_linear2 = pred_linear2.astype(int)\nOrder_linear2 = pd.DataFrame(pred_linear2,columns=['Predicted_Order'])\nOrder_linear2['Assemblers'] = data2\nOrder_linear2.sort_values(by='Predicted_Order')","acfdb4fb":"Y_2=np.ravel(Y_2)","7e228a4a":"randomregressor2 = RandomForestRegressor(max_depth=3, random_state=0, max_features=7)\nrandom_regressor2 = randomregressor2.fit(X_Final,Y_2)\nrandom_model2=random_regressor2.predict(X_Final)","4852fbd0":"random_model2.astype(int)","d1b0f364":"mse2_random = mean_squared_error(Y_2,random_model2)\nrmse2_random = sqrt(mse2_random)\nmae2_random = metrics.mean_absolute_error(Y_2,random_model2)\nvrs2_random = metrics.explained_variance_score(Y_2,random_model2)\nprint(\"MEAN SQUARE ERROR : \",mse2_random)\nprint(\"ROOT MEAN SQUARE ERROR : \",rmse2_random) \nprint(\"MEAN ABSOLUTE ERROR : \", mae2_random)\nprint(\"VARIANCE REGRESSION SCORE : \",vrs2_random)","08f03a5d":"r2_forest2 = randomregressor2.score(X_Final, Y_2)\nprint('R^2: {0}'.format(r2_forest2))","1ffdcd71":"pk.dump(randomregressor2, open(\"model2_random.pkl\",\"wb\"))","b3d73bae":"model2_random=pk.load(open(\"model2_random.pkl\",'rb'))\npred2_random=model2_random.predict(predict_hamburgensis)\npred2_random=pred2_random.astype(int)\nOrder2_random = pd.DataFrame(pred2_random,columns=['Predicted_Order'])\nOrder2_random['Assemblers'] = data2\nOrder2_random.sort_values(by='Predicted_Order')","5dda2d60":"ridge2 = Ridge()\nparameters2 = {'alpha':[0.000001,0.00001, 0.001, 0.01, 0.1, 0.3, 0.5]}\nridge2_model_params = GridSearchCV(ridge2, parameters2,scoring='r2',cv=7)\nridge2_model_params.fit(X_Final,Y_2)","fdb6ab2f":"best_ridge2_params = ridge2_model_params.best_params_\n\nprint(\"Best Ridge Params : \", best_ridge2_params)","019a4aa7":"ridge2_model=Ridge(alpha=0.000001).fit(X_Final, Y_2)","b8331c6f":"ridge2_coefficients= ridge2_model.coef_\npd.DataFrame(ridge2_coefficients).astype(float)","a19c40ed":"r2_ridge2 = ridge2_model.score(X_Final, Y_2)\nprint('R^2: {0}'.format(r2_ridge2))","e50b5e46":"ridge2_model.intercept_","d84458cb":"ridge2_model_predict=ridge2_model.predict(X_Final)","fcb39ded":"r_square_ridge2 = ridge2_model.score(X_Final, Y_2)\nmse2_ridge = mean_squared_error(Y_2,ridge2_model_predict)\nrmse2_ridge = sqrt(mse2_ridge)\nmae2_ridge = metrics.mean_absolute_error(Y_2,ridge2_model_predict)\nvrs2_ridge = metrics.explained_variance_score(Y_2,ridge2_model_predict)\n\nprint(\"MEAN SQUARE ERROR : \",mse2_ridge)\nprint(\"ROOT MEAN SQUARE ERROR : \",rmse2_ridge) \nprint(\"MEAN ABSOLUTE ERROR : \", mae2_ridge)\nprint(\"VARIANCE REGRESSION SCORE : \",vrs2_ridge)\n\nprint(\"Model R Square: \",r_square_ridge2)","011dc835":"pk.dump(ridge2_model, open(\"model2_ridge.pkl\",\"wb\"))","6cc65363":"model2_ridge=pk.load(open(\"model2_ridge.pkl\",'rb'))\npred2_ridge=model2_ridge.predict(predict_hamburgensis)\npred2_ridge = pred2_ridge.astype(int)\nOrder2_ridge = pd.DataFrame(pred2_ridge,columns=['Predicted_Order'])\nOrder2_ridge['Assemblers'] = data2\nOrder2_ridge.sort_values(by='Predicted_Order')","e631d96d":"lasso2 = Lasso()\nparameters_2 = {'alpha':[0.000001,0.00001, 0.001, 0.01, 0.1, 0.3, 0.5]}\nlasso2_model = GridSearchCV(lasso2, parameters_2,scoring='r2',cv=7)\nlasso2_model = lasso2_model.fit(X_Final,Y_2)","067e4d64":"best_lasso2_params = lasso2_model.best_params_\nprint(\"Best lasso Params : \", best_lasso2_params)","c22803a7":"lasso2 = Lasso()\nlasso2_model=Lasso(alpha=0.000001).fit(X_Final, Y_2)","5f6ab9a6":"lasso2_model.coef_\nlasso2_coefficients= lasso2_model.coef_\npd.DataFrame(lasso2_coefficients).astype(float)","e62b81f1":"lasso2_model.intercept_","6debae47":"lasso2_model_predict=lasso2_model.predict(X_Final)","2d46e3e0":"r_square_lasso2 = lasso2_model.score(X_Final, Y_2)\n\nmse2_lasso = mean_squared_error(Y_2,lasso2_model_predict)\n\nrmse2_lasso = sqrt(mse2_lasso)\n\nmae2_lasso = metrics.mean_absolute_error(Y_2,lasso2_model_predict)\n\nvrs2_lasso = metrics.explained_variance_score(Y_2,lasso2_model_predict)\n\nprint('MEAN SQUARE ERROR :{0:.4f} '.format(mse2_lasso))\n\nprint('ROOT MEAN SQUARE ERROR :{0:.4f} '.format(rmse2_lasso)) \n\nprint('MEAN ABSOLUTE ERROR :{0:.4f}'.format(mae2_lasso))\n\nprint('VARIANCE REGRESSION SCORE : {0:.4f}'.format(vrs2_lasso))\n\nprint('Model R Square : {0:.4f} '.format(r_square_lasso2))\n","05ee3f17":"pk.dump(lasso2_model, open(\"model2_lasso.pkl\",\"wb\"))","9607c582":"model2_lasso=pk.load(open(\"model2_lasso.pkl\",'rb'))\npred2_lasso=model2_lasso.predict(predict_hamburgensis)\npred2_lasso = pred2_lasso.astype(int)\nOrder2_lasso = pd.DataFrame(pred2_lasso,columns=['Predicted_Order'])\nOrder2_lasso['Assemblers'] = data2\nOrder2_lasso.sort_values(by='Predicted_Order')","9c8a637c":"alphas = [0.000001,0.00001, 0.001, 0.01, 0.1, 0.3, 0.5]\nfor a in alphas:\n    model2 = ElasticNet(alpha=a).fit(X_Final,Y_2)   \n    score2 = model2.score(X_Final, Y_2)\n    pred_y2 = model2.predict(X_Final)\n    mse2_elastic = mean_squared_error(Y_2, pred_y2)   \n    print(\"Alpha:{0:.6f}, R2:{1:.2f}, MSE:{2:.2f}, RMSE:{3:.2f}\".format(a, score2, mse2_elastic, np.sqrt(mse2_elastic)))","b22a9931":"elastic2_model=ElasticNet(alpha=0.00001).fit(X_Final, Y_2)\nelastic2_model_predict = elastic2_model.predict(X_Final)\nscore2 = elastic2_model.score(X_Final,Y_2)\nmse2 = mean_squared_error(Y_2, elastic2_model_predict)\nprint(\"R2:{0:.2f}, MSE:{1:.2f}, RMSE:{2:.2f}\"\n      .format(score2, mse2, np.sqrt(mse2)))","ee7b9115":"elastic2_model.coef_","04ce8d21":"r_square_elastic2 = elastic2_model.score(X_Final, Y_2)\n\nmse2_elastic = mean_squared_error(Y_2,elastic2_model_predict)\n\nrmse2_elastic = sqrt(mse2_elastic)\n\nmae2_elastic = metrics.mean_absolute_error(Y_2,elastic2_model_predict)\n\nvrs2_elastic = metrics.explained_variance_score(Y_2,elastic2_model_predict)\n\nprint(\"MEAN SQUARE ERROR : \",mse2_elastic)\n\nprint(\"ROOT MEAN SQUARE ERROR : \",rmse2_elastic) \nprint(\"MEAN ABSOLUTE ERROR : \", mae2_elastic)\nprint(\"VARIANCE REGRESSION SCORE : \",vrs2_elastic)\nprint(\"Model R Square: \",r_square_elastic2)\n","92699aed":"pk.dump(elastic2_model, open(\"model2_elastic.pkl\",\"wb\"))","65bdeda4":"model2_elastic=pk.load(open(\"model2_elastic.pkl\",'rb'))\npred2_elastic=model2_elastic.predict(predict_hamburgensis)\npred2_elastic = pred2_elastic.astype(int)\nOrder2_elastic = pd.DataFrame(pred2_elastic,columns=['Predicted_Order'])\nOrder2_elastic['Assemblers'] = data2\nOrder2_elastic.sort_values(by='Predicted_Order')","7417727a":"comparison2={\"Models\":[\"Simple Regression\", \"Random Forest\",\"Ridge Regression\",\"Lasso Regression\",\"ElasticNET Regression\" ],\n            \"R_Square\/Accuracy\": [r2_regression2, r2_forest2, r_square_ridge2,r_square_lasso2,r_square_elastic2],\n            \"Mean Square Error\/Accuracy\": [mse2_regression, mse2_random, mse2_ridge,mse2_lasso,mse2_elastic],\n            \"Root Mean Sqaure Error\":[rmse2_regression,rmse2_random,rmse2_ridge,rmse2_lasso,rmse2_elastic],\n            \"Mean Absoulate Error\" :[mae2_regression,mae2_random,mae2_ridge,mae2_lasso,mae2_elastic],\n            \"Variance Regression Score\" :[vrs2_regression,vrs2_random,vrs2_ridge,vrs2_lasso,vrs2_elastic]\n                   }\ncomparison2 = pd.DataFrame(comparison2)\ncomparison2","1d2d2c24":"sort2=comparison2.sort_values('R_Square\/Accuracy',ascending=False)\nsort2","89407e0d":"vibrio.info()","82bd9c87":"vibrio.drop(\"Sr.No\",axis=1, inplace = True)","9140b420":"vibrio.isnull().any().sum()","6c9defad":"vibrio = vibrio.set_index('Assembly_Metrics').T","89077953":"X3 = vibrio.iloc[:,0:13]\nY3 = vibrio[['Order']]\nprint(X3.shape)\nprint(Y3.shape)","6de7498a":"X3 = MinMaxScaler().fit_transform(X3)\nX3= pd.DataFrame({'Number_of_Contigs': X3[:, 0], 'Length_of_Largest_Contig': X3[:, 1],'N50': X3[:, 2],'N75': X3[:, 3], 'N90': X3[:, 4], 'NG50': X3[:,5], 'NG75': X3[:, 6], 'Contigs_greater_and_equal_N50': X3[:, 7],'Contigs_greater_and_equal_200': X3[:, 8],'Mean': X3[:, 9], 'Median': X3[:, 10],'Sum_of_the_Contig_Lengths': X3[:, 11], 'Coverage':X3[:, 12]})\nX3","c4fa6ce9":"data3 = {'Assembly_Metrics':['Velvet', 'VCAKE', 'SSAKE', 'QSRA','SHARCGS','IDBA','Mira','Mira2','Maq','MARAGAP']} \ndata3 = pd.DataFrame(data3)\nX3['Assembly_Metrics'] = data3\nX3 = X3.set_index('Assembly_Metrics')\nX3","d5cec6a7":"vibrio = pd.concat([X3, Y3], axis=1)","ca674da5":"a3 = sns.pairplot(vibrio, x_vars=['Number_of_Contigs','Length_of_Largest_Contig','N50'], y_vars='Order', height=5, aspect=0.9)\nb3 = sns.pairplot(vibrio, x_vars=['N75','Contigs_greater_and_equal_N50','Contigs_greater_and_equal_200'], y_vars='Order', height=5, aspect=0.9)\nc3 = sns.pairplot(vibrio, x_vars=['Mean','Median','Sum_of_the_Contig_Lengths','Coverage'], y_vars='Order', height=5, aspect=0.9)","d237e24f":"correlations3 = vibrio.corr()\ncorrelations3","9ed5aef4":"mask = np.triu(np.ones_like(correlations3, dtype=np.bool))\nplt.figure(figsize=(11,8))\nsns.heatmap(correlations3*100,mask=mask,annot=True, fmt='.0f' )","eadbb5fa":"X_3 = vibrio.drop(['Contigs_greater_and_equal_200','Sum_of_the_Contig_Lengths'],axis=1)\nY_3 = vibrio[['Order']]\nprint(X_3.shape)\nprint(Y_3.shape)","781fdd6b":"bestfeatures3 = SelectKBest(score_func=chi2, k=12)\nfit3 = bestfeatures3.fit(X_3,Y_3)\ndfscores3 = pd.DataFrame(fit3.scores_)\ndfcolumns3 = pd.DataFrame(X_3.columns)\n#concat two dataframes for better visualization \nfeatureScores3 = pd.concat([dfcolumns3,dfscores3],axis=1)\nfeatureScores3.columns = ['Specs','Score']  #naming the dataframe columns\nprint(featureScores3.nlargest(12,'Score'))","f07d3061":"feature_imp_vib = ExtraTreesClassifier()\nfeature_imp_vib.fit(X_3,Y_3)\nprint(feature_imp_vib.feature_importances_)","0c4da2aa":"feature_importance_vibrio = pd.Series(feature_imp_vib.feature_importances_, index=X_3.columns)\nplt.figure(figsize=(12,8))\nfeature_importance_vibrio.nlargest(13).plot(kind='barh')\nplt.show()\n","1b59285f":"X_Final3 = X_3.drop(['Order','Number_of_Contigs','Contigs_greater_and_equal_N50'],axis=1)\npredict_vibrio = X_3.drop(['Order','Number_of_Contigs','Contigs_greater_and_equal_N50'],axis=1)\npredict_vibrio.info()","a4287c08":"X_Final3.describe()","313b15e7":"profile_vibrio = pdp.ProfileReport(X_Final3)\nprofile_vibrio","4601dfd7":"model3 = LinearRegression()","2c7250d9":"model3=model3.fit(X_Final3, Y3)","1df7ec20":"pred3 = model3.predict(X_Final3)","45b66c86":"model3.coef_","71550e87":"model3.intercept_","cdd99054":"r2_regression3 = model3.score(X_Final3, Y_3)\nprint('R^2: {0}'.format(r2_regression3))","aa5566d2":"test_r2_3=r2_score(Y3,pred3)\ntrain_r2_3=r2_score(Y3,model3.predict(X_Final3))","d5ef648f":"print('R2 score for testing:',test_r2_3)\nprint('R2 score for training:',train_r2_3)","92612b5d":"r_2_3 = []\nfor i in range(1, (X3.shape[-1])+1):\n    m3=LinearRegression()\n    m3.fit(X3.values[:,:i],Y3)\n    prd3=m3.predict(X3.values[:,:i])\n    r_2_3.append(r2_score(Y3,prd3))","d95f082a":"plt.figure(figsize=(15,5))\nplt.plot(r_2_3);\nplt.xlabel('Features')\nplt.ylabel('R_2 Score')","369e3da9":"r_square_regression3 = model3.score(X_Final3, Y_3)\nmse3_regression = mean_squared_error(Y_3,pred3)\nrmse3_regression = sqrt(mse3_regression)\nmae3_regression = metrics.mean_absolute_error(Y_3,pred3)\nvrs3_regression = metrics.explained_variance_score(Y_3,pred3)\n\nprint(\"MEAN SQUARE ERROR : \", mse3_regression)\nprint(\"ROOT MEAN SQUARE ERROR : \", rmse3_regression)\nprint(\"MEAN ABSOLUTE ERROR : \", mae3_regression)\nprint(\"VARIANCE REGRESSION SCORE : \",vrs3_regression)\nprint(\"R^2 :\",r_square_regression3)","d3715363":"pk.dump(model3, open(\"model3_linear.pkl\",\"wb\"))","4d605da4":"model3_linear=pk.load(open(\"model3_linear.pkl\",'rb'))\npred_linear3=model3_linear.predict(predict_vibrio)\npred_linear3 = pred_linear3.astype(int)\nOrder_linear3 = pd.DataFrame(pred_linear3,columns=['Predicted_Order'])\nOrder_linear3['Assemblers'] = data3\nOrder_linear3.sort_values(by='Predicted_Order')","54963782":"Y_3=np.ravel(Y_3)","e8617ef4":"randomregressor3 = RandomForestRegressor(max_depth=5, random_state=0, max_features=9)\nrandom_regressor3 = randomregressor3.fit(X_Final3,Y_3)\nrandom_model3=random_regressor3.predict(X_Final3)","cc83f2f5":"random_model3.astype(int)","fb945001":"mse3_random = mean_squared_error(Y_3,random_model3)\nrmse3_random = sqrt(mse3_random)\nmae3_random = metrics.mean_absolute_error(Y_3,random_model3)\nvrs3_random = metrics.explained_variance_score(Y_3,random_model3)\nprint('MEAN SQUARE ERROR : {0:.4f} '.format(mse3_random))\nprint('ROOT MEAN SQUARE ERROR : {0:.4f}'.format(rmse3_random)) \nprint('MEAN ABSOLUTE ERROR : {0:.4f}'.format(mae3_random))\nprint('VARIANCE REGRESSION SCORE : {0:.4f}'.format(vrs3_random))","9e45383e":"r2_forest3 = randomregressor3.score(X_Final3, Y_3)\nprint('R^2: {0:4f}'.format(r2_forest3))","2e134bab":"pk.dump(randomregressor3, open(\"model3_random.pkl\",\"wb\"))","3b68cbed":"model3_random=pk.load(open(\"model3_random.pkl\",'rb'))\npred3_random=model3_random.predict(predict_vibrio)\npred3_random=pred3_random.astype(int)\nOrder3_random = pd.DataFrame(pred3_random,columns=['Predicted_Order'])\nOrder3_random['Assemblers'] = data3\nOrder3_random.sort_values(by='Predicted_Order')","2c582510":"ridge3 = Ridge()\nparameters3 = {'alpha':[0.000001,0.00001, 0.001, 0.01, 0.1, 0.3, 0.5]}\nridge3_model_params = GridSearchCV(ridge3, parameters3,scoring='r2',cv=7)\nridge3_model_params.fit(X_Final3,Y_3)","6279cc4f":"best_ridge3_params = ridge3_model_params.best_params_\nprint(\"Best Ridge Params : \", best_ridge3_params)","5b92698b":"ridge3_model=Ridge(alpha=0.000001).fit(X_Final3, Y_3)","bd6376ad":"ridge3_coefficients= ridge3_model.coef_\npd.DataFrame(ridge3_coefficients).astype(float)","b40aaa8a":"ridge3_model.intercept_","ec8b8e9d":"ridge3_model_predict=ridge3_model.predict(X_Final3)","c83f5b31":"r_square_ridge3 = ridge3_model.score(X_Final3, Y_3)\nmse3_ridge = mean_squared_error(Y_3,ridge3_model_predict)\nrmse3_ridge = sqrt(mse3_ridge)\nmae3_ridge = metrics.mean_absolute_error(Y_3,ridge3_model_predict)\nvrs3_ridge = metrics.explained_variance_score(Y_3,ridge3_model_predict)\n\nprint('MEAN SQUARE ERROR : {0:.4f}'.format(mse3_ridge))\nprint('ROOT MEAN SQUARE ERROR : {0:.4f}'.format(rmse3_ridge))\nprint('MEAN ABSOLUTE ERROR : {0:.4f}'.format(mae3_ridge))\nprint('VARIANCE REGRESSION SCORE : {0:.4f}'.format(vrs3_ridge))\nprint('Model R Square: {0:.4f}'.format(r_square_ridge3))","56cf9bd7":"pk.dump(ridge3_model, open(\"model3_ridge.pkl\",\"wb\"))","d2d43819":"model3_ridge=pk.load(open(\"model3_ridge.pkl\",'rb'))\npred3_ridge=model3_ridge.predict(predict_vibrio)\npred3_ridge = pred3_ridge.astype(int)\nOrder3_ridge = pd.DataFrame(pred3_ridge,columns=['Predicted_Order'])\nOrder3_ridge['Assemblers'] = data3\nOrder3_ridge.sort_values(by='Predicted_Order')","47e1eb00":"lasso3 = Lasso()\nparameters_3 = {'alpha':[0.000001,0.00001, 0.001, 0.01, 0.1, 0.3, 0.5]}\nlasso3_model = GridSearchCV(lasso3, parameters_3,scoring='r2',cv=7)\nlasso3_model = lasso3_model.fit(X_Final3,Y_3)","ec9a2e0f":"best_lasso3_params = lasso3_model.best_params_\nprint(\"Best lasso Params : \", best_lasso3_params)","e026c6d3":"lasso3 = Lasso()\nlasso3_model=Lasso(alpha=0.000001).fit(X_Final3, Y_3)","19f5be05":"lasso3_model.coef_\nlasso3_coefficients= lasso3_model.coef_\npd.DataFrame(lasso3_coefficients).astype(float)","26510bd5":"lasso3_model.intercept_","a3b9f806":"lasso3_model_predict=lasso3_model.predict(X_Final3)","0f524806":"r_square_lasso3 = lasso3_model.score(X_Final3, Y_3)\nmse3_lasso = mean_squared_error(Y_3,lasso3_model_predict)\nrmse3_lasso = sqrt(mse3_lasso)\nmae3_lasso = metrics.mean_absolute_error(Y_3,lasso3_model_predict)\nvrs3_lasso = metrics.explained_variance_score(Y_3,lasso3_model_predict)\n\nprint('MEAN SQUARE ERROR : {0:.4f}'.format(mse3_lasso))\nprint('ROOT MEAN SQUARE ERROR : {0:.4f}'.format(rmse3_lasso))\nprint('MEAN ABSOLUTE ERROR : {0:.4f}'.format(mae3_lasso))\nprint('VARIANCE REGRESSION SCORE : {0:.4f}'.format(vrs3_lasso))\nprint('Model R Square: {0:.4f}'.format(r_square_lasso3))\n","6620f881":"pk.dump(lasso3_model, open(\"model3_lasso.pkl\",\"wb\"))","05e6f4a0":"model3_lasso=pk.load(open(\"model3_lasso.pkl\",'rb'))\npred3_lasso=model3_lasso.predict(predict_vibrio)\npred3_lasso = pred3_lasso.astype(int)\nOrder3_lasso = pd.DataFrame(pred3_lasso,columns=['Predicted_Order'])\nOrder3_lasso['Assemblers'] = data3\nOrder3_lasso.sort_values(by='Predicted_Order')","e18c327f":"alphas = [0.00001, 0.001, 0.01, 0.1, 0.3, 0.5]\nfor a in alphas:\n    model3 = ElasticNet(alpha=a).fit(X_Final3,Y_3)   \n    score3 = model3.score(X_Final3, Y_3)\n    pred_y3 = model3.predict(X_Final3)\n    mse3_elastic = mean_squared_error(Y_3, pred_y3)   \n    print(\"Alpha:{0:.6f}, R2:{1:.3f}, MSE:{2:.3f}, RMSE:{3:.3f}\".format(a, score3, mse3_elastic, np.sqrt(mse3_elastic)))","525b57c7":"elastic3_model=ElasticNet(alpha=0.00001).fit(X_Final3, Y_3)\nelastic3_model_predict = elastic3_model.predict(X_Final3)\nscore3 = elastic3_model.score(X_Final3,Y_3)\nmse3 = mean_squared_error(Y_3, elastic3_model_predict)\nprint(\"R2:{0:.2f}, MSE:{1:.2f}, RMSE:{2:.2f}\"\n      .format(score3, mse3, np.sqrt(mse3)))","9c6c5acb":"elastic3_model.coef_","656c3063":"r_square_elastic3 = elastic3_model.score(X_Final3, Y_3)\n\nmse3_elastic = mean_squared_error(Y_3,elastic3_model_predict)\n\nrmse3_elastic = sqrt(mse3_elastic)\n\nmae3_elastic = metrics.mean_absolute_error(Y_3,elastic3_model_predict)\n\nvrs3_elastic = metrics.explained_variance_score(Y_3,elastic3_model_predict)\n\nprint('MEAN SQUARE ERROR : {0:.4f}'.format(mse3_elastic))\nprint('ROOT MEAN SQUARE ERROR : {0:.4f}'.format(rmse3_elastic))\nprint('MEAN ABSOLUTE ERROR : {0:.4f}' .format(mae3_elastic))\nprint('VARIANCE REGRESSION SCORE : {0:.4f}'.format(vrs3_elastic))\nprint('Model R Square: {0:.4f}'.format(r_square_elastic3))\n","ef9a3585":"pk.dump(elastic3_model, open(\"model3_elastic.pkl\",\"wb\"))","141ceed4":"model3_elastic=pk.load(open(\"model3_elastic.pkl\",'rb'))\npred3_elastic=model3_elastic.predict(predict_vibrio)\npred3_elastic = pred3_elastic.astype(int)\nOrder3_elastic = pd.DataFrame(pred3_elastic,columns=['Predicted_Order'])\nOrder3_elastic['Assemblers'] = data3\nOrder3_elastic.sort_values(by='Predicted_Order')","9e87cb9d":"comparison3={\"Models\":[\"Simple Regression\", \"Random Forest\",\"Ridge Regression\",\"Lasso Regression\",\"ElasticNET Regression\" ],\n            \"R_Square\/Accuracy\": [r2_regression3, r2_forest3, r_square_ridge3,r_square_lasso3,r_square_elastic3],\n            \"Mean Square Error\/Accuracy\": [mse3_regression, mse3_random, mse3_ridge,mse3_lasso,mse3_elastic],\n            \"Root Mean Sqaure Error\":[rmse3_regression,rmse3_random,rmse3_ridge,rmse3_lasso,rmse3_elastic],\n            \"Mean Absoulate Error\" :[mae3_regression,mae3_random,mae3_ridge,mae3_lasso,mae3_elastic],\n            \"Variance Regression Score\" :[vrs3_regression,vrs3_random,vrs3_ridge,vrs3_lasso,vrs3_elastic]\n                   }\ncomparison3 = pd.DataFrame(comparison3)\ncomparison3","08e15dcb":"sort=comparison3.sort_values('R_Square\/Accuracy',ascending=False)\nsort","6be3d6a1":"pab1.info()","b0a08788":"pab1.drop(\"Sr.No\",axis=1, inplace = True)","e3c9df4e":"pab1 = pab1.set_index('Assembly_Metrics').T.astype(float)","8fa9d604":"pab1['Order'] = pab1['Order'].astype(int)\nX4 = pab1.iloc[:,0:13]\nY4 = pab1[['Order']]\nprint(X4.shape)\nprint(Y4.shape)","c914edee":"X4 = MinMaxScaler().fit_transform(X4)\nX4 = pd.DataFrame({'Number_of_Contigs': X4[:, 0], 'Length_of_Largest_Contig': X4[:, 1],'N50': X4[:, 2],'N75': X4[:, 3], 'N90': X4[:, 4], 'NG50': X4[:,5], 'NG75': X4[:, 6], 'Contigs_greater_and_equal_N50': X4[:, 7],'Contigs_greater_and_equal_200': X4[:, 8],'Mean': X4[:, 9], 'Median': X4[:, 10],'Sum_of_the_Contig_Lengths': X4[:, 11], 'Coverage':X4[:, 12]})\nX4","f5e331f5":"data4 = {'Assembly_Metrics':['VCAKE', 'QSRA','IDBA','MARAGAP']} \ndata4 = pd.DataFrame(data4)\nX4['Assembly_Metrics'] = data4\nX4 = X4.set_index('Assembly_Metrics')\nX4","fd89e0fc":"pab1 = pd.concat([X4, Y4], axis=1)\npab1","925d5fdb":"a4 = sns.pairplot(pab1, x_vars=['Number_of_Contigs','Length_of_Largest_Contig','N50'], y_vars='Order', height=5, aspect=0.9)\nb4 = sns.pairplot(pab1, x_vars=['N75','Contigs_greater_and_equal_N50','Contigs_greater_and_equal_200'], y_vars='Order', height=5, aspect=0.9)\nc4 = sns.pairplot(pab1, x_vars=['Mean','Median','Sum_of_the_Contig_Lengths','Coverage'], y_vars='Order', height=5, aspect=0.9)","44448188":"correlations4 = pab1.corr()\ncorrelations4","d6a2f5ad":"mask = np.triu(np.ones_like(correlations4, dtype=np.bool))\n\nplt.figure(figsize=(11,8))\nsns.heatmap(correlations4*100,mask=mask,annot=True, fmt='.0f' )","8093bfd3":"X_4 = pab1.iloc[:,0:13]\nY_4 = pab1[['Order']]\nprint(X_4.shape)\nprint(Y_4.shape)","ac1c2446":"bestfeatures4 = SelectKBest(score_func=chi2, k=12)\nfit4 = bestfeatures4.fit(X_4,Y_4)\ndfscores4 = pd.DataFrame(fit4.scores_)\ndfcolumns4 = pd.DataFrame(X_4.columns)\n#concat two dataframes for better visualization \nfeatureScores4 = pd.concat([dfcolumns4,dfscores4],axis=1)\nfeatureScores4.columns = ['Specs','Score']  #naming the dataframe columns\nprint(featureScores4.nlargest(12,'Score'))  #print 10 best features","50c4def2":"feature_imp_pab = ExtraTreesClassifier()\nfeature_imp_pab.fit(X_4,Y_4)\nprint(feature_imp_pab.feature_importances_)","2548b907":"feature_importance_pab1 = pd.Series(feature_imp_pab.feature_importances_, index=X_4.columns)\nplt.figure(figsize=(12,8))\nfeature_importance_pab1.nlargest(13).plot(kind='barh')\nplt.show()\n","bc87e758":"X_Final4 = X_4.drop(['Number_of_Contigs','Contigs_greater_and_equal_N50','Coverage'],axis=1)\npredict_pab1 = X_4.drop(['Number_of_Contigs','Contigs_greater_and_equal_N50','Coverage'],axis=1)\nX_Final4\n","1c36ec8b":"X_Final4.describe()","8bdff170":"profile_pab1 = pdp.ProfileReport(X_Final4)\nprofile_pab1","77e13ac9":"model4 = LinearRegression()","c815298d":"model4.fit(X_Final4, Y_4)","74b492b5":"pred4 = model4.predict(X_Final4)","6265e8b4":"coefficients4 = model4.coef_\nco4 = pd.DataFrame(coefficients4).astype(float)\nco4","bbb0ef9c":"model4.intercept_","6c001864":"pred4 = model4.predict(X_Final4)\npred4.astype(int)","fa5f0fab":"test_r2_4=r2_score(Y_4,pred4)","69ea7f21":"train_r2_4=r2_score(Y_4,model4.predict(X_Final4))","f5284d32":"print('R2 score for testing:',test_r2_4 )\nprint('R2 score for training:',train_r2_4 )","dfe51652":"r_2_4 = []\nfor i in range(1, (X_Final4.shape[-1])+1):\n    m4=LinearRegression()\n    m4.fit(X_Final4.values[:,:i],Y_4)\n    prd4=m4.predict(X_Final4.values[:,:i])\n    r_2_4.append(r2_score(Y_4,prd4))","6d7714b9":"plt.figure(figsize=(15,5))\nplt.plot(r_2_4);\nplt.xlabel('Features')\nplt.ylabel('R_2 Score')","b1f13939":"helper.adjR2(X_Final4,Y_4,test_r2_4)","d91c9f10":"r2_regression4 = model4.score(X_Final4, Y_4)\nprint('R^2: {0}'.format(r2_regression4))","c25acd9b":"plt.figure(figsize=(15,6)) \nvisualizer4 = ResidualsPlot(model4,hist=True)\nvisualizer4.fit(X_Final4.values, Y_4.values)  \nvisualizer4.score(X_Final4.values, Y_4.values)  \nvisualizer4.poof()    ","21e2355d":"mse4_regression = mean_squared_error(Y_4,pred4)\nrmse4_regression = sqrt(mse4_regression)\nmae4_regression = metrics.mean_absolute_error(Y_4,pred4)\nvrs4_regression = metrics.explained_variance_score(Y_4,pred4)\n\nprint(\"MEAN SQUARE ERROR : \", mse4_regression)\n\nprint(\"ROOT MEAN SQUARE ERROR : \", rmse4_regression)\n\nprint(\"MEAN ABSOLUTE ERROR : \", mae4_regression)\n\nprint(\"VARIANCE REGRESSION SCORE : \",vrs4_regression)","69c9e6ff":"pk.dump(model4, open(\"model4_linear.pkl\",\"wb\"))","0baf7961":"model4_linear=pk.load(open(\"model4_linear.pkl\",'rb'))\npredict4_linear=model4_linear.predict(predict_pab1)\npred4_linear=predict4_linear.astype(int).flatten()\nOrder4_reg = pd.DataFrame(pred4_linear,columns=['Predicted_Order'])\nOrder4_reg['Assemblers'] = data4\nOrder4_reg.sort_values(by='Predicted_Order')","b4c6bcf2":"Y_4=np.ravel(Y_4)","c91d907c":"randomregressor4 = RandomForestRegressor(max_depth=5, random_state=0, max_features=7)\nrandom_regressor4 = randomregressor4.fit(X_Final4,Y_4)\nrandom_model4 = random_regressor4.predict(X_Final4)","31e57e81":"random_model4.astype(int)","5d7832b6":"r2_forest4 = randomregressor4.score(X_Final4, Y_4)\nmse4_random = mean_squared_error(Y_4,random_model4)\nrmse4_random = sqrt(mse4_random)\nmae4_random = metrics.mean_absolute_error(Y_4,random_model4)\nvrs4_random = metrics.explained_variance_score(Y_4,random_model4)\nprint(\"MEAN SQUARE ERROR : \",mse4_random)\nprint(\"ROOT MEAN SQUARE ERROR : \",rmse4_random) \nprint(\"MEAN ABSOLUTE ERROR : \", mae4_random)\nprint(\"VARIANCE REGRESSION SCORE: \",vrs4_random)\nprint('R^2: {0:.2f}'.format(r2_forest4))","b40743db":"pk.dump(randomregressor4, open(\"model4_random.pkl\",\"wb\"))","7381c3e9":"model4_random=pk.load(open(\"model4_random.pkl\",'rb'))\npred4_random=model4_random.predict(predict_pab1)\npred4_random=pred4_random.astype(int)\nOrder4_random = pd.DataFrame(pred4_random,columns=['Predicted_Order'])\nOrder4_random['Assemblers'] = data4\nOrder4_random.sort_values(by='Predicted_Order')","bc105fed":"ridge4 = Ridge()\nparameters4 = {'alpha':[0.000001,0.00001, 0.001, 0.01, 0.1, 0.3, 0.5]}\nridge4_model_params = GridSearchCV(ridge4, parameters4,scoring='r2',cv=4)\nridge4_model_params.fit(X_Final4,Y_4)","a0f3d814":"best_ridge4_params = ridge4_model_params.best_params_\n\nprint(\"Best Ridge Params : \", best_ridge4_params)","3f86769f":"ridge4_model=Ridge(alpha=0.000001).fit(X_Final4, Y_4)","5424a1ea":"ridge4_coefficients= ridge4_model.coef_\npd.DataFrame(ridge4_coefficients).astype(float)","cae01524":"ridge4_model.intercept_","51bd42bb":"ridge4_model_predict=ridge4_model.predict(X_Final4)","89c2d506":"r_square_ridge4 = ridge4_model.score(X_Final4, Y_4)\nmse4_ridge = mean_squared_error(Y_4,ridge4_model_predict)\nrmse4_ridge = sqrt(mse4_ridge)\nmae4_ridge = metrics.mean_absolute_error(Y_4,ridge4_model_predict)\nvrs4_ridge = metrics.explained_variance_score(Y_4,ridge4_model_predict)\n\nprint('MEAN SQUARE ERROR : {0:.3f}'.format(mse4_ridge))\nprint(\"ROOT MEAN SQUARE ERROR : \",rmse4_ridge) \nprint(\"MEAN ABSOLUTE ERROR : \", mae4_ridge)\nprint('VARIANCE REGRESSION SCORE :{0:.3f}' .format(vrs4_ridge))\nprint('Model R Square: {0:.3f}'.format(r_square_ridge4))","026c29a4":"pk.dump(ridge4_model, open(\"model4_ridge.pkl\",\"wb\"))","ba8284b0":"model4_ridge=pk.load(open(\"model4_ridge.pkl\",'rb'))\npred4_ridge=model4_ridge.predict(predict_pab1)\npred4_ridge = pred4_ridge.astype(int)\nOrder4_ridge = pd.DataFrame(pred4_ridge,columns=['Predicted_Order'])\nOrder4_ridge['Assemblers'] = data4\nOrder4_ridge.sort_values(by='Predicted_Order')","d1b37998":"lasso4 = Lasso()\nparameters_4 = {'alpha':[0.000001,0.00001, 0.001, 0.01, 0.1, 0.3, 0.5]}\nlasso4_model = GridSearchCV(lasso4, parameters_4,scoring='r2',cv=4)\nlasso4_model = lasso4_model.fit(X_Final4,Y_4)","eed97017":"best_lasso4_params = lasso4_model.best_params_\nprint(\"Best lasso Params : \", best_lasso4_params)","3e5e1d73":"lasso4 = Lasso()\nlasso4_model=Lasso(alpha=0.000001).fit(X_Final4, Y_4)","2f86e9a7":"lasso4_model.coef_\nlasso4_coefficients= lasso4_model.coef_\npd.DataFrame(lasso4_coefficients).astype(float)","48451588":"lasso4_model.intercept_","3b75b4ac":"lasso4_model_predict=lasso4_model.predict(X_Final4)","850e60ca":"r_square_lasso4 = lasso4_model.score(X_Final4, Y_4)\nmse4_lasso = mean_squared_error(Y_4,lasso4_model_predict)\nrmse4_lasso = sqrt(mse4_lasso)\nmae4_lasso = metrics.mean_absolute_error(Y_4,lasso4_model_predict)\nvrs4_lasso = metrics.explained_variance_score(Y_4,lasso4_model_predict)\nprint(\"MEAN SQUARE ERROR : \",mse4_lasso)\nprint(\"ROOT MEAN SQUARE ERROR : \",rmse4_lasso) \nprint(\"MEAN ABSOLUTE ERROR : \", mae4_lasso)\nprint(\"VARIANCE REGRESSION SCORE : \",vrs4_lasso)\nprint(\"Model R Square: \",r_square_lasso4)","3503c859":"pk.dump(lasso4_model, open(\"model4_lasso.pkl\",\"wb\"))","9d5947e3":"model4_lasso=pk.load(open(\"model4_lasso.pkl\",'rb'))\npred4_lasso=model4_lasso.predict(predict_pab1)\npred4_lasso = pred4_lasso.astype(int)\nOrder4_lasso = pd.DataFrame(pred4_lasso,columns=['Predicted_Order'])\nOrder4_lasso['Assemblers'] = data4\nOrder4_lasso.sort_values(by='Predicted_Order')","1526fed9":"alphas = [0.00001, 0.001, 0.01, 0.1, 0.3, 0.5]\nfor a in alphas:\n    model4 = ElasticNet(alpha=a).fit(X_Final4,Y_4)   \n    score4 = model4.score(X_Final4, Y_4)\n    pred_y4 = model4.predict(X_Final4)\n    mse4_elastic = mean_squared_error(Y_4, pred_y4)   \n    print(\"Alpha:{0:.6f}, R2:{1:.3f}, MSE:{2:.3f}, RMSE:{3:.3f}\".format(a, score4, mse4_elastic, np.sqrt(mse4_elastic)))","96ec8ad1":"elastic4_model=ElasticNet(alpha=0.001).fit(X_Final4, Y_4)\nelastic4_model_predict = elastic4_model.predict(X_Final4)\nscore4 = elastic4_model.score(X_Final4,Y_4)\nmse4 = mean_squared_error(Y_4, elastic4_model_predict)\nprint(\"R2:{0:.2f}, MSE:{1:.2f}, RMSE:{2:.2f}\"\n      .format(score4, mse4, np.sqrt(mse4)))","5c65e12d":"elastic4_model.coef_","b7d41f55":"r_square_elastic4 = elastic4_model.score(X_Final4, Y_4)\n\nmse4_elastic = mean_squared_error(Y_4,elastic4_model_predict)\n\nrmse4_elastic = sqrt(mse4_elastic)\n\nmae4_elastic = metrics.mean_absolute_error(Y_4,elastic4_model_predict)\n\nvrs4_elastic = metrics.explained_variance_score(Y_4,elastic4_model_predict)\n\nprint(\"MEAN SQUARE ERROR : \",mse4_elastic)\n\nprint(\"ROOT MEAN SQUARE ERROR : \",rmse4_elastic) \nprint(\"MEAN ABSOLUTE ERROR : \", mae4_elastic)\nprint(\"VARIANCE REGRESSION SCORE : \",vrs4_elastic)\nprint(\"Model R Square: \",r_square_elastic4)\n","f353a50a":"pk.dump(elastic4_model, open(\"model4_elastic.pkl\",\"wb\"))","60f3c01e":"model4_elastic=pk.load(open(\"model4_elastic.pkl\",'rb'))\npred4_elastic=model4_elastic.predict(predict_pab1)\npred4_elastic = pred4_elastic.astype(int)\nOrder4_elastic = pd.DataFrame(pred4_elastic,columns=['Predicted_Order'])\nOrder4_elastic['Assemblers'] = data4\nOrder4_elastic.sort_values(by='Predicted_Order')","bcece506":"comparison4={\"Models\":[\"Simple Regression\", \"Random Forest\",\"Ridge Regression\",\"Lasso Regression\",\"ElasticNET Regression\" ],\n            \"R_Square\/Accuracy\": [r2_regression4, r2_forest4, r_square_ridge4,r_square_lasso4,r_square_elastic4],\n            \"Mean Square Error\/Accuracy\": [mse4_regression, mse4_random, mse4_ridge,mse4_lasso,mse4_elastic],\n            \"Root Mean Sqaure Error\":[rmse4_regression,rmse4_random,rmse4_ridge,rmse4_lasso,rmse4_elastic],\n            \"Mean Absoulate Error\" :[mae4_regression,mae4_random,mae4_ridge,mae4_lasso,mae4_elastic],\n            \"Variance Regression Score\" :[vrs4_regression,vrs4_random,vrs4_ridge,vrs4_lasso,vrs4_elastic]\n                   }\ncomparison4 = pd.DataFrame(comparison4)\ncomparison4","97fb6bae":"sort4=comparison4.sort_values('R_Square\/Accuracy',ascending=False)\nsort4","531c3b88":"### Model Coefficients###","147aefca":"### Identify Best Ridge Alpha Parameter","fbb1fe9f":"- We can see that the data types of the independent variables. Mira, Mira2 and Maqa are object type.\n- We need to explore these variables and convert its datatypes into numeric data type.","1378c494":"### Select the Best Features ###\n* Using Feature Selection Above we select the best features and reduce one more variable with least k score.","0af804dd":"### Filtering Variables ###\nFrom the above correlations map, we decide to drop the independent variables which are less correlated with dependent variable","bfdde79e":"### Sorted Models on Basis of R Square","4fbf737e":"### Model Intercept","dbb13a3a":"### Lasso Regression ###\n\n* Lasso (Least Absolute Shrinkage and Selection Operator) also penalizes the absolute size of the regression coefficients.\n* It is capable of reducing the variability and improving the accuracy of linear regression models.","40f3df4c":"### Select the Best Features###\n- Using Feature Selection Above we select the best features and reduce one more variable with least k score.","1f1bd053":"### VIBRIO ###\n* We can see that the data types of the independent variables. Mira, Mira2 and Maqa are object type.\n* We need to explore these variables and convert its datatypes into numeric data type.","52ddded8":"### Preprocess the data ###\n* Values in the dataset are rightly skewed, so we need to scale them.\n* In order to acheive standard normal distribution.\n* MinMaxScaler() of sklearn estimator scales and translates each feature individually.\n* Such that it is in the given range on the training set, e.g. between zero and one.\n* We have standardize values with mean 0 and standard deviation of 1 against each assembler metrics","7ab2aa05":"### Regression Evaluation Metrics###\n- The Mean Squared Error (MSE) or Mean Squared Deviation (MSD) of an estimator measures the average of error squares i.e. the average squared difference between the estimated values and true value. \n- Mean absolute error is a measure of errors between paired observations expressing the same phenomenon.\n- RMSE is a measure of how spread out these residuals are. In other words, it tells you how concentrated the data is around the line of best fit.\n- Explained variance regression score function . Best possible score is 1.0, lower values are worse.","46809717":"### Simple Regression###\n- Call the Linear regression model and save it in the variable name model\n  ","a1cdd0f0":"### Model Coefficinets###","6519148a":"### Using PandaProfiling Library for Exploratory Data Analysis","c9ca658f":"### Model R Square","3eb98d29":"### ElasticNet Regressor ###\n* ElasticNet is hybrid of Lasso and Ridge Regression techniques.\n* Elastic-net is useful when there are multiple features which are correlated.","0973e494":"### Feature Selection ###\n* Now after selecting best features using coorelation we used sklearn feature selection class and use SelectKbest to Select * features according to the k highest scores.\n* With that we use chi-squared stats between each non-negative feature and class.\n* This score can be used to select the n_features features with the highest values for the test chi-squared statistic from X\n* Which must contain only non-negative features such as booleans or frequencies","f0a5e505":"### Model Prediction###","154f5129":"### Model Coefficient ###","f73a6faf":"### Organize Dataset for Further Processing###\n- Make a array of Assemblers Name\n- Convert the array into Data Frame\n- Set the Assembly_Metrics as the index of the data","e80b2261":"### Ridge Regression ###\n* Ridge Regression is a technique used when the data suffers from multicollinearity.\n* By adding a degree of bias to the regression estimates, ridge regression reduces the standard errors.\n* Exhaustive search over specified parameter values for an estimator.\n* The parameters of the estimator used to apply these methods are optimized by cross-validated grid-search over a parameter grid.\n","887cec59":"### Model Intercept","f883f2e4":"### Prediction Using Trained Model###","5d3d05b5":"### Model Serialize","37770299":"### Model Prediction","cdfdeb15":"### Feature Selection\n* Now after selecting best features using coorelation we used sklearn feature selection class and use SelectKbest to Select * features according to the k highest scores.\n* With that we use chi-squared stats between each non-negative feature and class.\n* This score can be used to select the n_features features with the highest values for the test chi-squared statistic from X\n* Which must contain only non-negative features such as booleans or frequencies","5f38ccd1":"### R Square Curve","84d27888":"### R Square Curve","0c99e89e":"### RANDOM FOREST\n* A random forest regressor is a meta estimator that fits a number of classifying decision trees on various sub-samples of the dataset\n* And uses averaging to improve the predictive accuracy and control over-fitting.\n* np.ravel returns contiguous flattened array(1D array with all the input-array elements and with the same type as it).","9a66d685":"### Correlation HeatMap\nFor better visulization of the correlations the heat map of correaltions is drawn","07fc5bf8":"### Prediction Using Trained Model###","7c76807c":"### Prediction Using Trained Data###","f8020b9b":"### Model Evalution Metrics\n* MSE\n* RMSE\n* MAE\n* VRS\n* R Square","c15a197d":"### Sorted Models on Basis of R Square ###","349b4d26":"### Model Prediction ###","25d3ee86":"### Concatinate the X and Y ###\nHere concat command of pandas Dataframe will concatnate the Independent and dependent variable.","8b5600dc":"### Sterializing Model\n* Pickle is the standard way of serializing objects in Python.\n* Use the pickle operation to serialize the model and save the serialized format to a file\n* load the pickle file to deserialize your model and use it to make new predictions.","1f591af3":"### Identify Best Ridge Alpha Parameter###","94b65cf2":"### Model R Square ###","b304d4e2":"### Models Comparison###","5d0e4275":"### Split the Dataset\n* iloc is Purely integer-location based indexing for selection by position.\n* We select the columns from position 0 to 13 and placed them in X variable as indepnednt variables and the order column is placed in Y variable as dependent variable","f50d4706":"### Prediction using Trained Model###","b74c65d9":"### Using PandaProfiling Library for Exploratory Data Analysis ###","adaedb69":"### Model R Square","05a9977a":"### Preprocess the data ### \n* Values in the dataset are rightly skewed, so we need to scale them.\n* In order to acheive standard normal distribution.\n* MinMaxScaler() of sklearn estimator scales and translates each feature individually.\n* Such that it is in the given range on the training set, e.g. between zero and one.\n* We have standardize values with mean 0 and standard deviation of 1 against each assembler metrics","b3ecd9de":"### LASSO Regressor###\n- Lasso (Least Absolute Shrinkage and Selection Operator) also penalizes the absolute size of the regression coefficients.\n- It is capable of reducing the variability and improving the accuracy of linear regression models.","3772ef71":"### Model coefficients ###","5f808fe7":"### PLASMID###","9df3506d":"### Model Intercept","63eb9411":"### Model Training###\n- Fit the model on the Final dataset after data exploration, Data visulization and Preprocessing.","1dff2d50":"### SIMPLE REGRESSION ###\nCall the Linear regression model and save it in the variable name model","ef2dd859":"### Models Comparison","795ce653":"### Sterializing Model###\n- Pickle is the standard way of serializing objects in Python.\n- Use the pickle operation to serialize the  model and save the serialized format to a file\n- load the pickle file to deserialize your model and use it to make new predictions. ","9679fd9f":"### R Square Curve###","d8ee0431":"### Random Forest Regressor Fit and Predict\nmax_depth is 3, means depth of tree","fdd1c9e8":"### Model Evaluation Metrics###","12de9aad":"- Values in the dataset are rightly skewed, so we need to scale them.\n- In order to acheive standard normal distribution.\n- MinMaxScaler() of sklearn estimator scales and translates each feature individually.\n- Such that it is in the given range on the training set, e.g. between zero and one.\n- We have standardize values with mean 0 and standard deviation of 1 against each assembler metrics","03130a3e":"### Model Intercept","df80e49c":"### ElasticNet Regression ###\n\n* ElasticNet is hybrid of Lasso and Ridge Regression techniques.\n* Elastic-net is useful when there are multiple features which are correlated.","4416b081":"### Sorted Models on Basis of R Square","7fb91a38":"### Model Fit##","3950b8bc":"### Transpose of Dataset###","b826f736":"### Prediction Using Trained Model###","c309e263":"### Model Prediction ###","00ed0305":"### Regression Evaluation Metrics ###\n* The Mean Squared Error (MSE) or Mean Squared Deviation (MSD) of an estimator measures the average of error squares i.e. the  average squared difference between the estimated values and true value.\n* Mean absolute error is a measure of errors between paired observations expressing the same phenomenon.\n* RMSE is a measure of how spread out these residuals are. In other words, it tells you how concentrated the data is around the line of best fit.\n* Explained variance regression score function . Best possible score is 1.0, lower values are worse.","391ef3f8":"### Model R Square ###","ce7ff5bb":"### Identify Coefficients of Ridge Regression\nAlpha Value used, identify by above code 1e-06 = 0.000001","65d63f8e":"### Verifying Important Features Using Extra Tree Classifier ###\n* An extra-trees classifier.This class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset.\n* And uses averaging to improve the predictive accuracy.","9c88294c":"### Model Deserialize and Prediction Using Train Model ###","9fee66a4":"### Concatinate the X and Y\nHere concat command of pandas Dataframe will concatnate the Independent and dependent variable.","d46ae834":"### Prediction Using Trained Data###","032cbc9d":"### Model Evaluation","d577f009":"### Models Comparison ####","b1a91e5d":"### Model Coefficient###","a2e3c27a":"### Identify Coefficients of Ridge Regression### \n- Alpha Value used, identify by above code 1e-06 = 0.000001","3e85d597":"### Verifying Important Features Using Extra Tree Classifier\n* An extra-trees classifier.This class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset.\n* And uses averaging to improve the predictive accuracy.","235127c2":"### Best LASSO Alpha Parameter ###","0ff0e504":"### Concatinate the X and Y###\n- Here concat command of pandas Dataframe will concatnate the Independent and dependent variable.","97f7af7c":"### Identify Coefficients of Ridge Regression\nAlpha Value used, identify by above code 1e-06 = 0.000001","73a2f388":"### Data Preparation and Filtering###\n- Drop the \"Sr.No\" column, beacuse we didnot need this in model.\n- Variables which shows data type as object contain hyphens \"-\". We replace hyphens with NaN and convert them into float data type. Becuase these columns contians float values.\n- Then mssing values is imputed with Zero. Because this is biological data of genomes which is collected using different Assemblers.","b92cdab0":"### Ridge Regression###\n\n* Ridge Regression is a technique used when the data suffers from multicollinearity.\n* By adding a degree of bias to the regression estimates, ridge regression reduces the standard errors.\n* Exhaustive search over specified parameter values for an estimator.\n* The parameters of the estimator used to apply these methods are optimized by cross-validated grid-search over a parameter grid.","1a923187":"### Feature Selection ###\n* Now after selecting best features using coorelation we used sklearn feature selection class and use SelectKbest to Select features according to the k highest scores.\n* With that we use chi-squared stats between each non-negative feature and class.\n* This score can be used to select the n_features features with the highest values for the test chi-squared statistic from X\n* Which must contain only non-negative features such as booleans or frequencies","b81ad954":"### Model Serialize###","d09ca290":"### Organize Dataset for Further Processing ###\n* Make a array of Assemblers Name\n* Convert the array into Data Frame\n* Set the Assembly_Metrics as the index of the data","af3c1c81":"### Model Coefficients","96bba26d":"### Distribution of Independent variables along Dependent Variable ###\n* We also need to check the distributions of the independent variables with dependent variable.\n* For that we use seaborn feature pairplot.","af044c4a":"### Ridge Regression\n* Ridge Regression is a technique used when the data suffers from multicollinearity.\n* By adding a degree of bias to the regression estimates, ridge regression reduces the standard errors.\n* Exhaustive search over specified parameter values for an estimator.\n* The parameters of the estimator used to apply these methods are optimized by cross-validated grid-search over a parameter grid.","dff0a90d":"### R Square Curve ###","24eb437e":"### Model Training ###\n* Fit the model on the Final dataset after data exploration, Data visulization and Preprocessing.","bec5b499":"### Select the Best Features\nUsing Feature Selection Above we select the best features and reduce one more variable with least k score.","1f85e060":"### Prediction Using Trained Model###","9743d880":"- An extra-trees classifier.This class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset.\n- And uses averaging to improve the predictive accuracy.","2bfcbd99":"- We take a Trasnpose of the data in order to take independent variables in columns and order as dependent variable\n- Set the \"Assembly Metrics\" as the index of the dataset","ce3004ab":"### Correlation analysis###\n- Using correlation we figure out which variables are highly correlated with dependent variable.\n- For, that we choose the standard cut-off point of 0.7 or 70.\n- Variables whose correlation are below 70 with dependent variable are droped from the model.","96febcd0":"### Model Training ###\n* Fit the model on the Final dataset after data exploration, Data visulization and Preprocessing.","742724c2":"### Verifying Important Features Using Extra Tree Classifier ###\n* An extra-trees classifier.This class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset.\n* And uses averaging to improve the predictive accuracy.","39194f3a":"### Correlation analysis ###\n* Using correlation we figure out which variables are highly correlated with dependent variable.\n* For, that we choose the standard cut-off point of 0.7 or 70.\n* Variables whose correlation are below 70 with dependent variable are droped from the model.","7fb693b8":"### Regression Evaluation Metrics\n* The Mean Squared Error (MSE) or Mean Squared Deviation (MSD) of an estimator measures the average of error squares i.e. the average squared difference between the estimated values and true value.\n* Mean absolute error is a measure of errors between paired observations expressing the same phenomenon.\n* RMSE is a measure of how spread out these residuals are. In other words, it tells you how concentrated the data is around the line of best fit.\n* Explained variance regression score function . Best possible score is 1.0, lower values are worse.","7066755e":"### Model Serialize","0f10f9c1":"### Importing Required Packages###","a54b9870":"### Transpose of Dataset\n* We take a Trasnpose of the data in order to take independent variables in columns and order as dependent variable\n* Set the \"Assembly Metrics\" as the index of the dataset","b62cc712":"### SIMPLE REGRESSION###\n\n* Call the Linear regression model and save it in the variable name model","3a153fd9":"### Model Evaluation ###","31425244":"### Model Training\nFit the model on the Final dataset after data exploration, Data visulization and Preprocessing.","ee2ae64e":"### Model Prediction","2e9aded6":"### Model Serialize","36945bb1":"### Using PandaProfiling Library for Exploratory Data Analysis ###","8395dc7e":"### Model Coefficient","5876080c":"### Filtering Variables\nFrom the above correlations map, we decide to drop the independent variables which are less correlated with dependent variable","7a69b4e8":"### Random Forest Regressor ###\n\n* A random forest regressor is a meta estimator that fits a number of classifying decision trees on various sub-samples of the dataset\n* And uses averaging to improve the predictive accuracy and control over-fitting.\n* np.ravel returns contiguous flattened array(1D array with all the input-array elements and with the same type as it).","ac62f5c7":"### Model Train & Test R Sqaure","bc01362b":"### Concatinate the X and Y ###\nHere concat command of pandas Dataframe will concatnate the Independent and dependent variable.","5165a3b5":"### Predicition Using Regression###","3fc4acaa":"### Model Fit","e2287c55":"### Model Evaluation","d9bedc00":"### Split the Dataset\n* iloc is Purely integer-location based indexing for selection by position.\n* We select the columns from position 0 to 13 and placed them in X variable as indepnednt variables and the order column is placed in Y variable as dependent variable","600f294f":"### Model Evaluation","4efe2d27":"### Model Training ###\nFit the model on the Final dataset after data exploration, Data visulization and Preprocessing.","ff67d1ba":"### PAb1 ###\n* We can see that the data types of the independent variables. Mira, Mira2 and Maqa are object type.\n* We need to explore these variables and convert its datatypes into numeric data type.","27cc4b9e":"### Random Forest Regressor Fit and Predict###\n- max_depth is 3, means depth of tree","edce06c2":"### Model with Best Alpha###","e204f451":"### Best LASSO Alpha Parameter","049bd5b6":"### Regression Evaluation Metrics\n* The Mean Squared Error (MSE) or Mean Squared Deviation (MSD) of an estimator measures the average of error squares i.e. the average squared difference between the estimated values and true value.\n* Mean absolute error is a measure of errors between paired observations expressing the same phenomenon.\n* RMSE is a measure of how spread out these residuals are. In other words, it tells you how concentrated the data is around the line of best fit.\n* Explained variance regression score function . Best possible score is 1.0, lower values are worse.","1c34ee4a":"### Transpose of Dataset ### \n* We take a Trasnpose of the data in order to take independent variables in columns and order as dependent variable\n* Set the \"Assembly Metrics\" as the index of the dataset","cc090c88":"### Model Evalution Metrics###\n- MSE\n- RMSE\n- MAE\n- VRS\n- R Square\n","16ccf04e":"### Random Forest Regressor Fit and Predict ###\nmax_depth is 3, means depth of tree","68d4c03b":"### Data Preparation and Filtering\n* Drop the \"Sr.No\" column, beacuse we didnot need this in model.\n* Variables which shows data type as object contain hyphens \"-\". We replace hyphens with NaN and convert them into float data type. Becuase these columns contians float values.\n* Then mssing values is imputed with Zero. Because this is biological data of genomes which is collected using different Assemblers.","dc1dff44":"### Preprocess the data###","87185364":"### Model with Best Alpha","09725b3c":"####  Prediciton usingTraining Model###","214b082a":"* We can see that the data types of the independent variables. Mira, Mira2 and Maqa are object type.\n* We need to explore these variables and convert its datatypes into numeric data type.","84135e6e":"### Sterializing Model ###\n* Pickle is the standard way of serializing objects in Python.\n* Use the pickle operation to serialize the model and save the serialized format to a file\n* load the pickle file to deserialize your model and use it to make new predictions.","72bc34d7":"### Model Intercept###","22749254":"### Organize Dataset for Further Processing\n* Make a array of Assemblers Name\n* Convert the array into Data Frame\n* Set the Assembly_Metrics as the index of the data","301094d9":"### Best LASSO Alpha Parameter###","1485c64c":"### Model Intercept###","a7ab6178":"### Model Evaluation###","982fae7e":"### Random Forest Regressor Fit and Predict\nmax_depth is 3, means depth of tree","df5779f8":"### Model Coefficient","5212c841":"### Model Evaluation###","61df46ea":"### Residual Plot###","efe67d70":"### Lasso Regressor###\n\n* Lasso (Least Absolute Shrinkage and Selection Operator) also penalizes the absolute size of the regression coefficients.\n* It is capable of reducing the variability and improving the accuracy of linear regression models.","00177c2c":"### Model R Square###","09d8f83d":"### Sterializing Model\n* Pickle is the standard way of serializing objects in Python.\n* Use the pickle operation to serialize the model and save the serialized format to a file\n* load the pickle file to deserialize your model and use it to make new predictions.","f238665a":"### Identify Best Ridge Alpha Parameter","6f841e37":"### Split the Dataset###\n- iloc is Purely integer-location based indexing for selection by position. \n- We select the columns from position 0 to 13 and placed them in X variable as indepnednt variables and the order column is placed in Y variable as dependent variable","24b4793a":"#### Model Evaluation ####","0d4c64fa":"### LASSO Regression ###\n* Lasso (Least Absolute Shrinkage and Selection Operator) also penalizes the absolute size of the regression coefficients.\n* It is capable of reducing the variability and improving the accuracy of linear regression models.","18adf8d2":"### Residual Plot ###","77dcbad1":"### Data Preparation and Filtering ###\n* Drop the \"Sr.No\" column, beacuse we didnot need this in model.\n* Variables which shows data type as object contain hyphens \"-\". We replace hyphens with NaN and convert them into float data type. Becuase these columns contians float values.\n* Then mssing values is imputed with Zero. Because this is biological data of genomes which is collected using different Assemblers.","062333d3":"### Verifying Important Features Using Extra Tree Classifier###","1e06925d":"### Importing Dataset###","73a7da71":"### Prediction Using Training Data###","ad168e83":"### Model R Square###","0c81a7c2":"### Model with Best Alpha ###","4a92d5bc":"### ElasticNetCV Regression###\n- ElasticNet is hybrid of Lasso and Ridge Regression techniques.\n- Elastic-net is useful when there are multiple features which are correlated. ","66ca9d54":"### Ridge Regression###","0ae35511":"### Prediction Using Trained Model###","43276e35":"<h1>HUMBURGENSIS<h1>","b0cce10d":"### Correlation analysis\n* Using correlation we figure out which variables are highly correlated with dependent variable.\n* For, that we choose the standard cut-off point of 0.7 or 70.\n* Variables whose correlation are below 70 with dependent variable are droped from the model.","fd6a072a":"### Select the Best Features ### \nUsing Feature Selection Above we select the best features and reduce one more variable with least k score.","43733bee":"### Preprocess the data\n* Values in the dataset are rightly skewed, so we need to scale them.\n* In order to acheive standard normal distribution.\n* MinMaxScaler() of sklearn estimator scales and translates each feature individually.\n* Such that it is in the given range on the training set, e.g. between zero and one.\n* We have standardize values with mean 0 and standard deviation of 1 against each assembler metrics","c516721f":"### Identify Coefficients of Ridge Regression ###\nAlpha Value used, identify by above code 1e-06 = 0.000001","e7e15585":"### Model Evalution Metrics\n* MSE\n* RMSE\n* MAE\n* VRS\n* R Square","94208b16":"## Using PandaProfiling Library for Exploratory Data Analysis","283e75e2":"### Prediction Using Trained Model###","c0d99eb3":"### Model coefficients","b8c4cb23":"### Cororelation and Corelation Heat Map - Again ###\n* After droping the independent variables which are less than the threshold of 0.7. We again chech the correlations.\n* It comes out that the independent variables are also corelated, which leads to multicollinearity.","0e1a3830":"### Best LASSO Alpha Parameter","57d7d7d1":"### Organize Dataset for Further Processing ###\n* Make a array of Assemblers Name\n* Convert the array into Data Frame\n* Set the Assembly_Metrics as the index of the data","2f1d4479":"### Model Serialize ###","793b74c9":"### Model Intercept ###","06d4039f":"### Distribution of Independent variables along Dependent Variable ###\n* We also need to check the distributions of the independent variables with dependent variable.\n* For that we use seaborn feature pairplot.","75b3b18e":"### Predicition Using Regression###","8c697837":"### Residual Plot","3e23a0a2":"### Model Deserialize and Prediction Using Train Model###","f16d335d":"### Model with Best Alpha","dc22a667":"### Random Forest Regressor ###\n* A random forest regressor is a meta estimator that fits a number of classifying decision trees on various sub-samples of the dataset\n* And uses averaging to improve the predictive accuracy and control over-fitting.\n* np.ravel returns contiguous flattened array(1D array with all the input-array elements and with the same type as it).","46d64baa":"### Filtering Variables ###\nFrom the above correlations map, we decide to drop the independent variables which are less correlated with dependent variable","12f7669d":"### Correlation HeatMap###\n- For better visulization of the correlations the heat map of correaltions is drawn","220aeffc":"### Correlation HeatMap ###\nFor better visulization of the correlations the heat map of correaltions is drawn","b06f9f8e":"### Simple Regression###\n* Call the Linear regression model and save it in the variable name model\n","45e64cda":"### Model Prediction###","33b07919":"### Model Evalution Metrics ###\n\n* MSE\n* RMSE\n* MAE\n* VRS\n* R Square","f9377368":"### Sorted Models on Basis of R Square###","0a7d7d95":"### Model Train & Test R Sqaure###\n","56382f4f":"### Model Train & Test R Sqaure ","21323e1b":"#### Model Intercept ###","b322e94a":"### Prediction Using Trained Model###","5514d59c":"- Ridge Regression is a technique used when the data suffers from multicollinearity.\n- By adding a degree of bias to the regression estimates, ridge regression reduces the standard errors.\n- Exhaustive search over specified parameter values for an estimator.\n- The parameters of the estimator used to apply these methods are optimized by cross-validated grid-search over a parameter grid.","e1806f0a":"### Model Coefficients","9cfca1f6":"### Model Fit","59fc10c7":"### Model Serialize###","c48adb53":"### Model Fit ###","780b64f4":"### Prediction Using Trained Model  of Hamburgensis Data###","6c41f10b":"### RANDOM FOREST###\n- A random forest regressor is a meta estimator that fits a number of classifying decision trees on various sub-samples of the dataset\n- And uses averaging to improve the predictive accuracy and control over-fitting.\n- np.ravel returns contiguous flattened array(1D array with all the input-array elements and with the same type as it).","7bc783a1":"### Model Serialize","751aa745":"### Feature Selection###\n- Now after selecting best features using coorelation we used sklearn feature selection class and use SelectKbest to Select features according to the k highest scores.\n- With that we use chi-squared stats between each non-negative feature and class.\n- This score can be used to select the n_features features with the highest values for the test chi-squared statistic from X\n- Which must contain only non-negative features such as booleans or frequencies","d5eeed68":"### <center>Regression Line Plot<center>\n### <center>Y = mx + b<center>\n### <center>variable(Y) = slope(m) * variable(x) + intercept(b)<center>","ce126050":"### Identify Best Ridge Alpha Parameter ###","7bbcc2a0":"### Distribution of Independent variables along Dependent Variable\n* We also need to check the distributions of the independent variables with dependent variable.\n* For that we use seaborn feature pairplot.","650d64d5":"### Prediction Using Trained Model","1d98a2c0":"### Model Intercept###","2fc493d0":"### Prediciton Using Trained Model###","de0e3804":"#### Model Evaluation ###","c7826c74":"### Model Coefficients ###","abb53749":"### Model Prediction","bd4664bf":"<h1><b><center>Genome Assembly - Regression <\/center><\/b><\/h1>\n<h2><b><center>Machine Learning Project<\/center><\/b><\/h2>\n<h3><b><center> Muhammad Afan<\/center><\/b><\/h3>\n","19882a32":"### Data Preparation and Filtering ###\n* Drop the \"Sr.No\" column, beacuse we didnot need this in model.\n* Variables which shows data type as object contain hyphens \"-\". We replace hyphens with NaN and convert them into float data type. Becuase these columns contians float values.\n* Then mssing values is imputed with Zero. Because this is biological data of genomes which is collected using different Assemblers.","e6db84a6":"### Filtering Variables###\n- From the above correlations map, we decide to drop the independent variables which are less correlated with dependent variable","1dd34916":"### ElasticNet Regressor ###\n* ElasticNet is hybrid of Lasso and Ridge Regression techniques.\n* Elastic-net is useful when there are multiple features which are correlated.","8788f3a7":"### Model coefficients###","83430c82":"### Distribution of Independent variables along Dependent Variable###\n- We also need to check the distributions of the independent variables with dependent variable.\n- For that we use seaborn feature pairplot.","96c1b94c":"### Cororelation and Corelation Heat Map - Again###\n- After droping the independent variables which are less than the threshold of 0.7. We again chech the correlations.\n- It comes out that the independent variables are also corelated, which leads to multicollinearity.","4e64fe65":"### Model Evaluation","69698eeb":"### Models Comparison","a2812593":"### Prediction Using Trained Model"}}