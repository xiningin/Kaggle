{"cell_type":{"777d446d":"code","35e02f94":"code","f420310c":"code","f1c6058b":"code","fd2f4f7a":"code","f7f1d54a":"code","ee09877c":"code","cfa05001":"code","0b56b6a0":"code","c710e750":"code","58354c8c":"code","f7611d7c":"code","122e9b22":"code","bcf57f2f":"code","670c43f5":"code","946617fc":"code","53d95647":"code","a384b5ca":"code","08f2cfbb":"code","44bde318":"code","eceab4c0":"code","b121bfd7":"code","ced03e30":"code","5b4792ba":"code","f34301ea":"code","ee81e61f":"code","62c1b1da":"code","5cb14880":"code","6d5b7dc6":"code","0ee89fb8":"code","c63a20b4":"code","a0d320cc":"code","1d1c2008":"code","2549994b":"code","0610dd40":"code","077314a6":"code","c3e8b302":"code","3e42f471":"code","993ce9f0":"code","ef10982e":"code","a361991c":"code","4f2bc1e7":"code","44a3b43d":"code","e17a28f3":"code","f12cae80":"code","242d59e3":"code","aabeeb37":"code","b74f885c":"code","7cb49362":"code","be6705d2":"code","0b38ac9e":"code","58171fd6":"code","83d81d7e":"code","d1c6dafb":"markdown","7a1a445d":"markdown","61149754":"markdown"},"source":{"777d446d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","35e02f94":"#Import train and test dataset\nhouse_train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\nhouse_test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\nsample_sub = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')","f420310c":"#print first 5 rows of train data set\nhouse_train.head()","f1c6058b":"#Summary of train.csv\nhouse_train.describe()","fd2f4f7a":"#Summary of test.csv\nhouse_test.describe()","f7f1d54a":"#print first 5 rows of test data set\nhouse_test.head()","ee09877c":"#Shape of the given dataset\nhouse_train.shape, house_test.shape","cfa05001":"#Check for duplicated row in train.csv and test.csv\nhouse_train.duplicated().values.any(),house_test.duplicated().values.any()","0b56b6a0":"#Drop unecessary features\nhouse_train.drop(['Id'], axis=1, inplace=True)\nhouse_test.drop(['Id'], axis=1, inplace=True)","c710e750":"#checking outliers\nimport seaborn as sns\nsns.scatterplot(data=house_train, x='GrLivArea', y='SalePrice')\nplt.grid()\nprint(house_train.shape)","58354c8c":"#removing outliers and check again\nhouse_train = house_train.drop(house_train[house_train['GrLivArea']>5000].index)\n\nsns.scatterplot(data=house_train, x='GrLivArea', y='SalePrice')\nplt.grid()\nprint(\"2 points removed from data\")\nprint(house_train.shape)","f7611d7c":"#Separate Categorical and Numerical Features (numerical_features) (Train)\ntrain_numerical = house_train[house_train.dtypes[house_train.dtypes != \"object\"].index]\ntrain_numerical.head()","122e9b22":"#Separate Categorical and Numerical Features (categorical_features) (Train)\ntrain_categorical = house_train[house_train.dtypes[house_train.dtypes == \"object\"].index]\ntrain_categorical.head()","bcf57f2f":"#Separate Categorical and Numerical Features (categorical_features) (Test)\ntest_categorical = house_test[house_test.dtypes[house_test.dtypes == \"object\"].index]\ntest_categorical.head()","670c43f5":"#Separate Categorical and Numerical Features (numerical_features) (Test)\ntest_numerical = house_test[house_test.dtypes[house_test.dtypes != \"object\"].index]\ntest_numerical.head()","946617fc":"#Define Function to count every column with missing value\ndef get_missing_value(df):\n    mis_val = df.isnull().sum()\n    tot_val = df.shape[0]\n    mis_percentage = round((100 * mis_val \/ tot_val),2)\n    miss_tab = pd.concat([mis_val, mis_percentage], axis=1)\n    miss_tab_col_name = miss_tab.rename(columns = {0 : 'Row With Missing Value', 1 : 'Percentage (%)'})\n    miss_tab_col_name = miss_tab_col_name.sort_values('Percentage (%)',ascending=False)\n    return miss_tab_col_name","53d95647":"#Columns that has missing values in train data\nmiss_train = get_missing_value(house_train)\nmiss_train","a384b5ca":"#missing values in numerical features (train)\nmiss_train_numerical = get_missing_value(train_numerical)\nmiss_train_numerical","08f2cfbb":"#Replacing missing values in numerical features (train)\ntrain_numerical['GarageYrBlt'].fillna(train_numerical['YearBuilt'],inplace=True)\ntrain_numerical['MasVnrArea'].fillna(0,inplace=True)\ntrain_numerical['LotFrontage'].fillna(train_numerical['LotFrontage'].mean(),inplace=True)\nnew_missing_value_train = train_numerical.isna().values.sum()\nprint(\"total missing values for numerical features in train.csv is \",new_missing_value_train)","44bde318":"#missing values in categorical features (train)\nmiss_train_categorical = get_missing_value(train_categorical)\nmiss_train_categorical","eceab4c0":"#Replacing missing values in categorical features (train)\ntrain_categorical['PoolQC'].fillna('No',inplace=True)\ntrain_categorical['Alley'].fillna('No',inplace=True)\ntrain_categorical['MiscFeature'].fillna('No',inplace=True)\ntrain_categorical['Fence'].fillna('No',inplace=True)\ntrain_categorical['FireplaceQu'].fillna('No',inplace=True)\ntrain_categorical['GarageType'].fillna('No',inplace=True)\ntrain_categorical['GarageCond'].fillna('No',inplace=True)\ntrain_categorical['GarageQual'].fillna('No',inplace=True)\ntrain_categorical['GarageFinish'].fillna('No',inplace=True)\ntrain_categorical['BsmtFinType2'].fillna('No',inplace=True)\ntrain_categorical['BsmtExposure'].fillna('No',inplace=True)\ntrain_categorical['BsmtFinType1'].fillna('No',inplace=True)\ntrain_categorical['BsmtQual'].fillna('No',inplace=True)\ntrain_categorical['BsmtCond'].fillna('No',inplace=True)\ntrain_categorical['MasVnrType'].fillna('No',inplace=True)\ntrain_categorical['Electrical'].fillna('FuseF',inplace=True)\nnew_missing_value_train_cat = train_categorical.isna().values.sum()\nprint(\"total missing values for categorical features in train.csv is \",new_missing_value_train_cat)","b121bfd7":"#Columns that has missing values in test data\nmiss_test = get_missing_value(house_test)\nmiss_test","ced03e30":"#Replacing missing values in numerical features (test)\ntest_numerical['LotFrontage'].fillna(house_test['LotFrontage'].mean(),inplace=True)\ntest_numerical['GarageYrBlt'].fillna(house_test['YearBuilt'],inplace=True)\ntest_numerical['MasVnrArea'].fillna(0,inplace=True)\ntest_numerical['BsmtHalfBath'].fillna(0,inplace=True)\ntest_numerical['BsmtFullBath'].fillna(0,inplace=True)\ntest_numerical['BsmtFinSF1'].fillna(0,inplace=True)\ntest_numerical['GarageCars'].fillna(0,inplace=True)\ntest_numerical['BsmtUnfSF'].fillna(0,inplace=True)\ntest_numerical['TotalBsmtSF'].fillna(0,inplace=True)\ntest_numerical['GarageArea'].fillna(0,inplace=True)\ntest_numerical['BsmtFinSF2'].fillna(0,inplace=True)\nnew_missing_value_test_num = test_numerical.isna().values.sum()\nprint(\"total missing values for numerical features in test.csv is \",new_missing_value_test_num)","5b4792ba":"#Replacing missing values in categorical features (test)\ntest_categorical['PoolQC'].fillna('No',inplace=True)\ntest_categorical['MiscFeature'].fillna('No',inplace=True)\ntest_categorical['Alley'].fillna('No',inplace=True)\ntest_categorical['Fence'].fillna('No',inplace=True)\ntest_categorical['FireplaceQu'].fillna('No',inplace=True)\ntest_categorical['GarageQual'].fillna('No',inplace=True)\ntest_categorical['GarageCond'].fillna('No',inplace=True)\ntest_categorical['GarageType'].fillna('No',inplace=True)\ntest_categorical['GarageFinish'].fillna('No',inplace=True)\ntest_categorical['BsmtCond'].fillna('No',inplace=True)\ntest_categorical['BsmtQual'].fillna('No',inplace=True)\ntest_categorical['BsmtExposure'].fillna('No',inplace=True)\ntest_categorical['BsmtFinType2'].fillna('No',inplace=True)\ntest_categorical['BsmtFinType1'].fillna('No',inplace=True)\ntest_categorical['MasVnrType'].fillna('No',inplace=True)\ntest_categorical['MSZoning'].fillna('No',inplace=True)\ntest_categorical['Functional'].fillna('No',inplace=True)\ntest_categorical['Utilities'].fillna('No',inplace=True)\ntest_categorical['KitchenQual'].fillna('No',inplace=True)\ntest_categorical['SaleType'].fillna('No',inplace=True)\ntest_categorical['Exterior1st'].fillna('No',inplace=True)\ntest_categorical['Exterior2nd'].fillna('No',inplace=True)\nnew_missing_value_test_cat = test_categorical.isna().values.sum()\nprint(\"total missing values for categorical features in test.csv is \",new_missing_value_test_cat)","f34301ea":"#correlation between column in train.csv\nimport matplotlib.pyplot as plt\ncorrelation_significant = train_numerical.corr().index[abs(train_numerical.corr()[\"SalePrice\"])>0.5]\nf, ax = plt.subplots(figsize=(20, 9))\nsns.heatmap(train_numerical[correlation_significant].corr(), annot=True)\nplt.show()","ee81e61f":"#check the distribution of SalePrice\nsns.histplot(train_numerical['SalePrice'],kde=True)\nplt.title('Sale Price Distribution')\nplt.grid()","62c1b1da":"#log transformation for SalePrice\ntrain_numerical['SalePrice']=np.log(train_numerical['SalePrice'])\n#check the distribution of SalePrice\nsns.histplot(train_numerical['SalePrice'],kde=True)\nplt.title('Sale Price Distribution')\nplt.grid()","5cb14880":"#Correlation any features to SalePrice\ncorr_0 = train_numerical.corr()\ncorr_0.sort_values([\"SalePrice\"], ascending = False, inplace = True)\ncorr_0.SalePrice[abs(corr_0.SalePrice)>0.5]","6d5b7dc6":"#Correlation between SalePrice and GrLivArea\nimport matplotlib.pyplot as plt\nplt.scatter(train_numerical['SalePrice'],train_numerical['GrLivArea'])\nplt.grid()\nplt.show()","0ee89fb8":"#Correlation between SalePrice and GarageArea\nimport matplotlib.pyplot as plt\nplt.scatter(train_numerical['SalePrice'],train_numerical['GarageArea'])\nplt.grid()\nplt.show()","c63a20b4":"#Correlation between SalePrice and YearBuilt\nimport matplotlib.pyplot as plt\nplt.scatter(train_numerical['SalePrice'],train_numerical['YearBuilt'])\nplt.grid()\nplt.show()","a0d320cc":"#Correlation between SalePrice and TotalBsmtSF\nimport matplotlib.pyplot as plt\nplt.scatter(train_numerical['SalePrice'],train_numerical['TotalBsmtSF'])\nplt.grid()\nplt.show()","1d1c2008":"#Unique data in every columns of categorical features (train)\nunique_data = [len(train_categorical[col].unique()) for col in train_categorical.columns]\nunique_train_cat = list(zip(train_categorical,unique_data))\nunique_train_cat_tab= pd.DataFrame(unique_train_cat, columns=['Features','Number of distinct values'])\nunique_train_cat_tab.sort_values('Number of distinct values',ascending=False)\nunique_train_cat_tab","2549994b":"#Label Encoding for categorical features (train)\nfrom sklearn.preprocessing import LabelEncoder\nfor i in train_categorical.columns:\n    le = LabelEncoder() \n    le.fit(list(train_categorical[i].values)) \n    train_categorical[i] = le.transform(list(train_categorical[i].values))\ntrain_categorical","0610dd40":"#Label Encoding for categorical features (train)\nfrom sklearn.preprocessing import LabelEncoder\nfor i in test_categorical.columns:\n    le = LabelEncoder() \n    le.fit(list(test_categorical[i].values)) \n    test_categorical[i] = le.transform(list(test_categorical[i].values))\ntest_categorical","077314a6":"#combine categorical and numerical features into one datafarame (test)\ntest_fix = test_numerical.join(test_categorical)\ntest_fix = test_fix.dropna()\ntest_fix.isna().sum()","c3e8b302":"#combine categorical and numerical features into one datafarame (train)\ntrain_fix = train_numerical.join(train_categorical)\ntrain_fix = train_fix.dropna()\ntrain_fix.isna().sum()","3e42f471":"#Checking train_fix\ntrain_fix.head(10)","993ce9f0":"#define train_X,train_y,test_X and test_y\ntrain_X = train_fix.drop('SalePrice',axis=1)\ntrain_y = train_fix['SalePrice'].values","ef10982e":"train_fix.shape,test_fix.shape","a361991c":"#Import necessary tools\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.model_selection import GridSearchCV","4f2bc1e7":"#LinearRegression\n# Creating the hyperparameter grid\nlr_copyX = [True,False]\nlr_fit_intercept = [True,False]\nparam_grid = {'copy_X': lr_copyX,          \n             'fit_intercept' : lr_fit_intercept,\n             }\n\n# Instantiating knn classifier\nlr = LinearRegression()\n\n# Instantiating the GridSearchCV object\nlr_cv = GridSearchCV(lr, param_grid, cv = 5)\nlr_cv.fit(train_X,train_y)\n\n# Print the tuned parameters and score\nprint(\"Tuned lr Parameters: {}\".format(lr_cv.best_params_)) \nprint(\"Best score is {}\".format(lr_cv.best_score_))","44a3b43d":"#Lasso\n# Creating the hyperparameter grid\nls_alpha = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]\nls_copy_X = [True,False]\nls_positive = [True,False]\nls_normalize = [True,False]\nls_fit_intercept = [True,False]\nls_warm_start = [True,False]\nparam_grid = {'alpha' : ls_alpha,\n             'copy_X': ls_copy_X,\n             'positive' : ls_positive,\n             'normalize' : ls_normalize,\n             'fit_intercept' : ls_fit_intercept,\n             'warm_start' : ls_warm_start\n             }\n\n# Instantiating knn classifier\nls = Lasso()\n\n# Instantiating the GridSearchCV object\nls_cv = GridSearchCV(ls, param_grid, cv = 5)\nls_cv.fit(train_X,train_y)\n\n# Print the tuned parameters and score\nprint(\"Tuned ls Parameters: {}\".format(ls_cv.best_params_)) \nprint(\"Best score is {}\".format(ls_cv.best_score_))","e17a28f3":"#Ridge\n# Creating the hyperparameter grid\nrg_alpha = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]\nrg_copy_X = [True,False]\nrg_positive = [True,False]\nrg_normalize = [True,False]\nrg_fit_intercept = [True,False]\nrg_warm_start = [True,False]\nrg_solver = ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']\nparam_grid = {'alpha' : rg_alpha,\n             'copy_X': rg_copy_X,\n             'fit_intercept' : rg_fit_intercept,\n             'solver' : rg_solver\n             }\n\n# Instantiating knn classifier\nrg = Ridge()\n\n# Instantiating the GridSearchCV object\nrg_cv = GridSearchCV(rg, param_grid, cv = 5)\nrg_cv.fit(train_X,train_y)\n\n# Print the tuned parameters and score\nprint(\"Tuned rg Parameters: {}\".format(rg_cv.best_params_)) \nprint(\"Best score is {}\".format(rg_cv.best_score_))","f12cae80":"#ElasticNet\n# Creating the hyperparameter grid\nen_alpha = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]\nen_l1_ratio = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]\nen_copy_X = [True,False]\nen_positive = [True,False]\nen_normalize = [True,False]\nen_fit_intercept = [True,False]\nen_warm_start = [True,False]\nen_selection = ['cyclic','random']\nparam_grid = {'alpha' : en_alpha,\n              'l1_ratio' : en_l1_ratio,\n              'copy_X': en_copy_X,\n               'positive' : en_positive,\n               'normalize' : en_normalize,\n              'fit_intercept' : rg_fit_intercept,\n             }\n\n# Instantiating ridge classifier\nen = ElasticNet()\n\n# Instantiating the GridSearchCV object\nen_cv = GridSearchCV(en, param_grid, cv = 5)\nen_cv.fit(train_X,train_y)\n\n# Print the tuned parameters and score\nprint(\"Tuned en Parameters: {}\".format(en_cv.best_params_)) \nprint(\"Best score is {}\".format(en_cv.best_score_))","242d59e3":"#RandomForestRegressor\n# Creating the hyperparameter grid\nrfr_n_estimators = [10,20,30,40,50,60,70,80,90,100]\nrfr_criterion = ['squared_error','absolute_error','poisson']\nrfr_max_depth = [2,3,4,5,6,7,8,9,10]\nrfr_max_features = ['auto','sqrt','log2']\nparam_grid = {'n_estimators': rfr_n_estimators,\n#              'criterion' : rfr_criterion,\n             'max_depth' : rfr_max_depth,\n             'max_features' : rfr_max_features,\n             }\n\n# Instantiating ridge classifier\nrfr = RandomForestRegressor()\n\n# Instantiating the GridSearchCV object\nrfr_cv = GridSearchCV(rfr, param_grid, cv = 5)\nrfr_cv.fit(train_X,train_y)\n\n# Print the tuned parameters and score\nprint(\"Tuned en Parameters: {}\".format(rfr_cv.best_params_)) \nprint(\"Best score is {}\".format(rfr_cv.))","aabeeb37":"#KNeighborsRegressor\n# Creating the hyperparameter grid\nknn_neighbors = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\nknn_weight = ['uniform', 'distance']\nknn_algorithm = ['auto', 'ball_tree', 'kd_tree', 'brute']\nknn_leaf_size = [5,10,15,20,25,30,35,40,45,50]\nparam_grid = {'n_neighbors': knn_neighbors,\n             'weights' : knn_weight,\n             'algorithm' : knn_algorithm,\n             'leaf_size' : knn_leaf_size,\n             }\n\n# Instantiating knn classifier\nknn = KNeighborsRegressor()\n\n# Instantiating the GridSearchCV object\nknn_cv = GridSearchCV(knn, param_grid, cv = 5)\nknn_cv.fit(train_X,train_y)\n\n# Print the tuned parameters and score\nprint(\"Tuned KNN Classifier Parameters: {}\".format(knn_cv.best_params_)) \nprint(\"Best score is {}\".format(knn_cv.best_score_))","b74f885c":"#DecisionTreeRegressor\n# Creating the hyperparameter grid\n# dtr_criterion = ['gini','entropy']\ndtr_splitter = ['best','random']\ndtr_max_depth = [1,2,3,4,5,6,7,8,9,10]\ndtr_max_features = ['auto','sqrt','log2']\nparam_grid = {\n             'splitter' : dtr_splitter,\n             'max_depth' : dtr_max_depth,\n             'max_features' : dtr_max_features,\n             }\n\n# Instantiating ridge classifier\ndtr = DecisionTreeRegressor()\n\n# Instantiating the GridSearchCV object\ndtr_cv = GridSearchCV(dtr, param_grid, cv = 5)\ndtr_cv.fit(train_X,train_y)\n\n# Print the tuned parameters and score\nprint(\"Tuned en Parameters: {}\".format(dtr_cv.best_params_)) \nprint(\"Best score is {}\".format(dtr_cv.best_score_))","7cb49362":"a = ['LinearRegression','Lasso','Ridge','ElasticNet','RandomForestRegressor','KNeighborRegressor','DecisionTreeRegressor']\nb = [lr_cv.best_score_,ls_cv.best_score_,rg_cv.best_score_,en_cv.best_score_,rfr_cv.best_score_,knn_cv.best_score_,dtr_cv.best_score_]\nplt.plot(a,b)\nplt.ylabel('best score')\nplt.xticks(rotation=90)\nplt.grid()\nplt.show()","be6705d2":"# c = ['LinearRegression','Lasso','Ridge','ElasticNet','RandomForestRegressor','KNeighborRegressor','DecisionTreeRegressor']\n# d = [lin_rmse,lasso_rmse,ridge_rmse,elasticnet_rmse,RFR_rmse,knn_rmse,dtr_rmse]\n# plt.plot(c,d)\n# plt.ylabel('Root Mean Square Error')\n# plt.xticks(rotation=90)\n# plt.grid()\n# plt.show()","0b38ac9e":"#prediction\nprediction = lr_cv.predict(test_fix)\nprediction","58171fd6":"submission = pd.DataFrame({'Id':sample_sub.Id, 'SalePrice': np.exp(prediction)})\nsubmission['Id']=submission['Id'].astype('int32')\nsubmission","83d81d7e":"submission.to_csv('my_submission.csv',index=False)","d1c6dafb":"**Model Building**","7a1a445d":"**IMPORTING DATA**","61149754":"**Predicting Value**"}}