{"cell_type":{"dc2352b8":"code","da6db531":"code","51f29227":"code","3c7b4909":"code","b86ab134":"code","d25e985f":"code","62da1012":"code","dc683ea4":"code","840182a3":"code","cdf44fff":"code","71d96664":"code","b51ff9bd":"code","b07e2cb9":"code","9abb933f":"code","3e1e30c5":"code","7ce9a901":"code","f69dab91":"code","61d05e58":"code","35db1186":"code","8d9955b0":"code","99f6b159":"code","b8cc163d":"code","8c1d861e":"code","350915d6":"code","17e7dc2d":"code","0731dec4":"code","8ddf6665":"code","ea07f696":"code","3c2b4e13":"code","3618f37e":"code","99ba09b8":"code","c054f1cf":"markdown","f9453447":"markdown","a91af7e6":"markdown","cd4c7964":"markdown","e520a87b":"markdown","13f81daa":"markdown","61beabde":"markdown","8df69f99":"markdown","743e4800":"markdown","f1a7535d":"markdown","0ed68bcb":"markdown","9ef9c9ea":"markdown","04cbc363":"markdown"},"source":{"dc2352b8":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(context='notebook', style='white', palette='colorblind')\n\nimport os\nfrom pathlib import Path","da6db531":"df = pd.read_csv('..\/input\/uci-auto-mpg-dataset\/mpg.csv')","51f29227":"df.sample(2)","3c7b4909":"df.info()","b86ab134":"df.isna().sum().plot(kind='bar')\nplt.title('Columns with missing values')","d25e985f":"df.corr()['horsepower'].sort_values()","62da1012":"df[df.horsepower.isna()].sort_values(by='model_year')","dc683ea4":"df.groupby(['origin','model_year','fuel_type']).median().loc['usa', 71, 'diesel']","840182a3":"# from above\ndf.loc[(df.model_year==71) & (df.name=='ford pinto'), 'horsepower'] = 153","cdf44fff":"# similarly, for other 5 values\ndf.loc[(df.model_year==74) & (df.name=='ford maverick'), 'horsepower'] = 100\ndf.loc[(df.model_year==80) & (df.name=='renault lecar deluxe'), 'horsepower'] = 67\ndf.loc[(df.model_year==80) & (df.name=='ford mustang cobra'), 'horsepower'] = 90\ndf.loc[(df.model_year==81) & (df.name=='renault 18i'), 'horsepower'] = 81\ndf.loc[(df.model_year==82) & (df.name=='amc concord dl'), 'horsepower'] = 85.5","71d96664":"# numerical subset\ndfnum=df.select_dtypes(include=np.number)\ndfnum.drop(columns=['cylinders', 'model_year'], inplace=True) # these two will be later treated as categorical features\n\n# categorical subset\ndfcat = pd.concat([df.select_dtypes(exclude=np.number), df[['cylinders', 'model_year']]], axis=1)","b51ff9bd":"dfnum.sample(2)","b07e2cb9":"fig, axs = plt.subplots(nrows=1, ncols=5, figsize=(15, 4), sharey=False)\ni = 0\n\nfor col in axs:\n    col.title.set_text(dfnum.columns[i])\n    sns.boxplot(data=dfnum.iloc[:,i], ax=col)\n    i+=1","9abb933f":"from sklearn.preprocessing import RobustScaler\n\nrs = RobustScaler()\ndfnum_scaled = pd.DataFrame(rs.fit_transform(dfnum), columns=dfnum.columns)","3e1e30c5":"fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(14, 4))\n\nfor col in dfnum_scaled.columns:\n    sns.kdeplot(dfnum_scaled[col], ax=ax2)\n    \nfor col in dfnum.columns:\n    sns.kdeplot(dfnum[col], ax=ax1)\n\nax2.title.set_text('After Standardization')\nax1.title.set_text('Before Standardization')","7ce9a901":"del dfcat['origin'] # deleting target feature","f69dab91":"dfcat.sample(2)","61d05e58":"dfcat.info()","35db1186":"dfcat['cylinder_cat'] = pd.Categorical(dfcat.cylinders.values, categories=list(dfcat.cylinders.unique()), ordered=False)\ndfcat['model_year_cat'] = pd.Categorical(dfcat.model_year.values, categories=list(dfcat.model_year.unique()), ordered=True)","8d9955b0":"del dfcat['cylinders']\ndel dfcat['model_year']\ndel dfcat['name']","99f6b159":"dfcat.info()","b8cc163d":"dfcat_dummy = pd.get_dummies(dfcat, drop_first=True)","8c1d861e":"dfcat_dummy.sample(2)","350915d6":"df_total = pd.concat([dfnum_scaled, dfcat_dummy], axis=1)","17e7dc2d":"X, y = df_total, df.origin.map({'usa':1, 'japan':0, 'europe':0})","0731dec4":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=0, stratify=y)","8ddf6665":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC\nimport xgboost as xgb","ea07f696":"xg_cl = xgb.XGBClassifier(objective='binary:logistic', n_estimators=10, seed=0)\ngb_cl = GradientBoostingClassifier(random_state=0)","3c2b4e13":"classifiers = [('LogisticRegression', LogisticRegression()), \n               ('RandomForestClassifier', RandomForestClassifier()), \n               ('LinearSVC', LinearSVC()), ('SVC', SVC()), \n               ('KNeighborsClassifier', KNeighborsClassifier()), \n               ('XGBoost', xg_cl), ('GradientBoost', gb_cl)]\n\n#vc = VotingClassifier(estimators=classifiers)\n#classifiers.append(('VotingClassifier', VotingClassifier(estimators=classifiers)))","3618f37e":"from sklearn.metrics import accuracy_score\n\ndict={}\n\nfor clf in classifiers:\n    clf[1].fit(X_train, y_train)\n    dict[clf[0]] = accuracy_score(y_test, clf[1].predict(X_test))\n\ndf_accuracy = pd.DataFrame(dict, index=['Accuracy']).T.sort_values(by='Accuracy', ascending=False)\nprint(df_accuracy.reset_index())\n\ndf_accuracy.plot(kind='bar')\nplt.yticks(df_accuracy.values, rotation=0)\nplt.ylim(0.77, 0.93)\nplt.plot()","99ba09b8":"importances = pd.Series(data=clf[1].feature_importances_, index= X_train.columns) \n\nimportances_sorted = importances.sort_values() \nimportances_sorted.plot(kind='barh', color='lightgreen') \n\nplt.title('Random Forest: Features Importances') \nplt.show() ","c054f1cf":"### Data Wrangling - Categorical Dataset","f9453447":"**6 rows with missing values in 'horsepower'**","a91af7e6":"**Grouping by 'origin', 'model_year' and 'fuel_type' and calculating median values to fill NaN**","cd4c7964":"**Random Forest**","e520a87b":"# Numerical & Categorical Subsets","13f81daa":"### Dealing with missing values in 'horsepower'","61beabde":"**Using Swarm Plots**\n\n* univariate distributions\n\n* to avoid bining bias in histograms\n* different number of bins in histograms can create completely different looking histograms. Hence, introducing bias\n\n* generally easier to understand than strip plots because they spread out the points to avoid overlap","8df69f99":"# Before\/After Robust Standardizing","743e4800":"# Training ","f1a7535d":"### Using RobustScaler to standardize columns because of the presence of outliers","0ed68bcb":"### Data Wrangling - Numerical Dataset","9ef9c9ea":"### Outliers in the numerical subset ","04cbc363":"# Predicting cars manufactured in USA"}}