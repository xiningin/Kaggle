{"cell_type":{"1facfb10":"code","19602606":"code","a935d33a":"code","10e9f23a":"code","00662c69":"code","9fad95fd":"code","e06a482e":"code","6468ccd7":"code","697ef28a":"code","4b631310":"code","746c2604":"code","bca9c889":"code","8ac63b71":"code","b53647c5":"code","6e9a2000":"code","ba1ccdc2":"code","a87933b2":"code","5368218a":"code","599e3b83":"code","16797e79":"code","94f5060a":"code","e7dc71e7":"code","4fa360f5":"code","eefb392b":"code","d7a0a5fc":"code","0053f72f":"code","68f80714":"code","3c576370":"code","4b1c448d":"code","f98117a7":"code","88eb8a53":"code","019d279c":"code","2e5edd0a":"code","484bf3c6":"code","6c06434b":"code","b28eddd0":"code","4cb4f3dc":"code","640d62c2":"code","f811ea7b":"code","97d7a8b0":"code","b88ddcbc":"code","43d3eda3":"code","eff7f84b":"code","89d55a20":"code","fb598155":"code","e6c2ad59":"code","93bc2990":"code","4649102f":"code","f19fca0a":"code","0fd64ff7":"code","49afb656":"markdown","f8abaf6c":"markdown","946d16fe":"markdown","25b9d2c3":"markdown","aa3d3a9e":"markdown","620ccf0f":"markdown","f1cf2a82":"markdown","c716aa41":"markdown","928da6d5":"markdown","922804ec":"markdown","70ce305a":"markdown","a5b3044d":"markdown","77a1cab6":"markdown","a0f060ac":"markdown","58cb1e27":"markdown","33f9adab":"markdown","675bb52e":"markdown","32a9a1d5":"markdown","29cf8539":"markdown","3252aa5c":"markdown","17fa5c96":"markdown","bb04a50b":"markdown","c1aa8a11":"markdown","4d2c2a5d":"markdown","4ef91556":"markdown","4c2b4cf8":"markdown","66c3b0b5":"markdown","7b6e863d":"markdown","f4f92070":"markdown","5ab017b3":"markdown","cf41d573":"markdown","36757235":"markdown","f50c36ff":"markdown","900ed284":"markdown","5f998d00":"markdown","42b4efd0":"markdown","9d493d88":"markdown","14632ec4":"markdown","fbbe799e":"markdown","d3041add":"markdown","3584b300":"markdown","b18636f4":"markdown","3dba7b68":"markdown","62ef5604":"markdown","b5d0293d":"markdown","0c7e1739":"markdown","b5d43084":"markdown","d57c880b":"markdown","871467d2":"markdown","2bb05647":"markdown","fee02d83":"markdown","cef7b3e4":"markdown","2706a5b9":"markdown","3c498c56":"markdown","544f4601":"markdown","d28dfe2f":"markdown","fdafd5a7":"markdown"},"source":{"1facfb10":"# common modules\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn\n\n# preprocessing, clustering, PCA\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, MaxAbsScaler, RobustScaler\nimport plotly.express as px\nfrom sklearn.cluster import MiniBatchKMeans, KMeans, DBSCAN\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.decomposition import PCA\n# from sklearn_extra.cluster import KMedoids","19602606":"data = pd.read_csv('https:\/\/raw.githubusercontent.com\/daniel-bss\/CreditCardCustomerSegmentation\/main\/CC_GENERAL.csv', index_col = 'CUST_ID') # Set the customer's ID as index\nprint(data.shape)\ndata.head()","a935d33a":"data.isna().sum()","10e9f23a":"data.dropna(subset = ['CREDIT_LIMIT'], inplace = True)","00662c69":"data[['PAYMENTS', 'MINIMUM_PAYMENTS']][data['MINIMUM_PAYMENTS'].isna()]","9fad95fd":"# Looking at PAYMENTS that's above MINIMUM_PAYMENTS\n\nprint('Shape:', data[['PAYMENTS', 'MINIMUM_PAYMENTS']][(data['MINIMUM_PAYMENTS'].notna()) & (data['PAYMENTS'] > data['MINIMUM_PAYMENTS'])].shape)\ndata[['PAYMENTS', 'MINIMUM_PAYMENTS']][(data['MINIMUM_PAYMENTS'].notna()) & (data['PAYMENTS'] > data['MINIMUM_PAYMENTS'])].head()","e06a482e":"# Looking at PAYMENTS that's below MINIMUM_PAYMENTS\n\nprint('Shape:', data[['PAYMENTS', 'MINIMUM_PAYMENTS']][(data['MINIMUM_PAYMENTS'].notna()) & (data['PAYMENTS'] < data['MINIMUM_PAYMENTS'])].shape)\ndata[['PAYMENTS', 'MINIMUM_PAYMENTS']][(data['MINIMUM_PAYMENTS'].notna()) & (data['PAYMENTS'] < data['MINIMUM_PAYMENTS'])].head()","6468ccd7":"minpay = data['MINIMUM_PAYMENTS'].copy() # make a copy of MINIMUM_PAYMENTS\npayments_mean = np.mean(data['PAYMENTS']) # take the mean value of PAYMENTS\n\ni = 0\nfor payments, minpayments in zip(data['PAYMENTS'], data['MINIMUM_PAYMENTS'].isna()):\n    if (payments == 0) and (minpayments == True):\n        minpay[i] = 0\n    elif (0 < payments < payments_mean) and (minpayments == True): \n        minpay[i] = payments\n    elif minpayments == True: # sisanya diisi oleh mean\n        minpay[i] = payments_mean\n    i += 1\n    \ndata['MINIMUM_PAYMENTS'] = minpay.copy()","697ef28a":"# Final check of missing values presence:\n\ndata.isna().sum()","4b631310":"fig, axs = plt.subplots(nrows=5, ncols=4, figsize = (20,20))\n\nz = 0\nfor i in range(5):\n    for j in range(4):\n        if z > 16:\n            axs[i][j].axis(False)\n        else:\n            axs[i][j].set_title(data.iloc[:,z].name)\n            axs[i][j].hist(data.iloc[:,z])\n        z+=1","746c2604":"logscaled_data = np.log2(data + 0.01)\nlogscaled_data.head()","bca9c889":"minMaxScaler = MinMaxScaler().fit_transform(logscaled_data)","8ac63b71":"standardScaler = StandardScaler().fit_transform(logscaled_data)","b53647c5":"robustScaler = RobustScaler().fit_transform(logscaled_data)","6e9a2000":"maxAbsScaler = MaxAbsScaler().fit_transform(logscaled_data)","ba1ccdc2":"pca = PCA(n_components = 17)\npca.fit(minMaxScaler)\npca_data = pca.transform(minMaxScaler)\n\ndef scree_plot(pca, scaler):\n    per_var = np.round(pca.explained_variance_ratio_* 100, decimals=1)\n    labels = ['PC' + str(x) for x in range(1, len(per_var)+1)]\n\n    plt.figure(figsize = (12,7))\n    plt.bar(x = range(1,len(per_var)+1), height = per_var, tick_label = labels)\n    plt.plot(range(1,len(per_var)+1), per_var, c = 'red')\n    plt.ylabel('Percentage of Explained Variance', size = 13)\n    plt.xlabel('Principal Components', size = 13)\n    plt.title('Scree Plot (%s)' %(scaler), size = 15)\n    plt.grid()\n    plt.show()\n    print('%s%% variance of the dataset lies on the first 3 Principal Components'  %(np.sum(per_var[:3])))\n\nscree_plot(pca, 'MinMaxScaler')","a87933b2":"def scatter_plot3D(pca_data):\n    x, y, z = pca_data[:,:3].T\n    fig = px.scatter_3d(x = x, \n                        y = y, \n                        z = z)\n    fig.show()\n    \nscatter_plot3D(pca_data)","5368218a":"pca = PCA(n_components = 17)\npca.fit(standardScaler)\npca_data = pca.transform(standardScaler)\n\nscree_plot(pca, 'StandardScaler')","599e3b83":"scatter_plot3D(pca_data)","16797e79":"pca = PCA(n_components = 17)\npca.fit(robustScaler)\npca_data = pca.transform(robustScaler)\n\nscree_plot(pca, 'RobustScaler')","94f5060a":"scatter_plot3D(pca_data)","e7dc71e7":"pca = PCA(n_components = 17)\npca.fit(maxAbsScaler)\npca_data = pca.transform(maxAbsScaler)\n\nscree_plot(pca, 'MaxAbsScaler')","4fa360f5":"scatter_plot3D(pca_data)","eefb392b":"# Final decision to use MinMAxScaler to transform data fed into PCA:\n\npca = PCA(n_components = 17)\npca.fit(minMaxScaler)\npca_data = pca.transform(minMaxScaler)\ntop3_pc = pca_data[:,:3]","d7a0a5fc":"inertia = []\nsilh = []\n\nfor i in range(2,10):\n    kmeans = KMeans(n_clusters = i) \n    kmeans.fit(top3_pc)\n    inertia.append(kmeans.inertia_)\n    silh.append(silhouette_score(top3_pc, kmeans.labels_))","0053f72f":"plt.figure(figsize = (8,4))\nplt.plot(range(2,10), inertia)\nplt.title('Inertia Score')\nplt.xticks(range(1,10))\nplt.grid()\nplt.show()","68f80714":"plt.figure(figsize = (8,4))\nplt.plot(range(2,10), silh)\nplt.title('Silhouette Score')\nplt.xticks(range(1,10))\nplt.grid()\nplt.show()","3c576370":"# Choosing cluster = 7\n\nkmeans = KMeans(n_clusters=7)\nkmeans.fit(top3_pc)\nkmeans_labels = kmeans.labels_","4b1c448d":"x, y, z = top3_pc.T\n\nfig = px.scatter_3d(x = x, \n                    y = y, \n                    z = z,\n                    color = kmeans_labels)\nfig.show()","f98117a7":"# inertia = []\n# silh = []\n\n# for i in range(2,10):\n#     kmedoids = KMedoids(n_clusters = i) \n#     kmedoids.fit(top3_pc)\n#     inertia.append(kmedoids.inertia_)\n#     silh.append(silhouette_score(top3_pc, kmedoids.labels_))","88eb8a53":"# plt.figure(figsize = (8,4))\n# plt.plot(range(2,10), inertia)\n# plt.title('Inertia Score')\n# plt.xticks(range(1,10))\n# plt.grid()\n# plt.show()","019d279c":"# plt.figure(figsize = (8,4))\n# plt.plot(range(2,10), silh)\n# plt.title('Silhouette Score')\n# plt.xticks(range(1,10))\n# plt.grid()\n# plt.show()","2e5edd0a":"# # Final decision on numbers of cluster chosen: 5\n\n# kmedoids = KMedoids(n_clusters = 5)\n# kmedoids.fit(top3_pc)\n# kmedoids_labels = kmedoids.labels_","484bf3c6":"# x, y, z = top3_pc.T\n\n# fig = px.scatter_3d(x = x, \n#                     y = y, \n#                     z = z,\n#                     color = kmedoids_labels)\n# fig.show()","6c06434b":"dbscan = DBSCAN(eps = 0.2) # eps = 0.2 is achieved iteratively to get the most reasonable number of clusters.\ndbscan.fit(top3_pc)\ndbscan_labels = dbscan.labels_ # label\/cluster setiap instance","b28eddd0":"x, y, z = top3_pc.T\n\nfig = px.scatter_3d(x = x, \n                    y = y, \n                    z = z,\n                    color = dbscan_labels)\nfig.show()","4cb4f3dc":"data = pd.read_csv(\"https:\/\/raw.githubusercontent.com\/daniel-bss\/CreditCardCustomerSegmentation\/main\/CC_GENERAL.csv\", index_col = 'CUST_ID') # Set the customer's ID as index\nprint(data.shape)\ndata.head()","640d62c2":"def imputeMissingValues(df):\n    # Missing Values of CREDIT_LIMIT\n    df.dropna(subset = ['CREDIT_LIMIT'], inplace = True)\n    \n    # Missing Values of MINIMUM_PAYMENTS\n    minpay = df['MINIMUM_PAYMENTS'].copy() # make a copy of MINIMUM_PAYMENTS\n    payments_mean = np.mean(df['PAYMENTS']) # take the mean value of PAYMENTS\n\n    i = 0\n    for payments, minpayments in zip(df['PAYMENTS'], df['MINIMUM_PAYMENTS'].isna()):\n        if (payments == 0) and (minpayments == True):\n            minpay[i] = 0\n        elif (0 < payments < payments_mean) and (minpayments == True): \n            minpay[i] = payments\n        elif minpayments == True: # sisanya diisi oleh mean\n            minpay[i] = payments_mean\n        i += 1\n\n    df['MINIMUM_PAYMENTS'] = minpay.copy()\n    return df","f811ea7b":"cleaned_data = imputeMissingValues(data)","97d7a8b0":"cleaned_data.isna().sum()","b88ddcbc":"logscaled_data = np.log2(cleaned_data + 0.01)\nlogscaled_data.head()","43d3eda3":"minMaxScaler = MinMaxScaler().fit_transform(logscaled_data)","eff7f84b":"# PCA\n\npca = PCA(n_components = 17)\npca.fit(minMaxScaler)\npca_data = pca.transform(minMaxScaler)\n\ntop3_pc = pca_data[:,:3] # Take the first 3 Principal Components (PC1, PC2, and PC3)\n\ndef scree_plot(pca):\n    per_var = np.round(pca.explained_variance_ratio_* 100, decimals=1)\n    labels = ['PC' + str(x) for x in range(1, len(per_var)+1)]\n\n    plt.figure(figsize = (12,7))\n    plt.bar(x = range(1,len(per_var)+1), height = per_var, tick_label = labels)\n    plt.plot(range(1,len(per_var)+1), per_var, c = 'red')\n    \n    plt.ylabel('Percentage of Explained Variance', size = 13)\n    plt.xlabel('Principal Components', size = 13)\n    plt.title('Scree Plot', size = 15)\n    plt.grid()\n    plt.show()\n    print('%s%% variance of the dataset lies on the first 3 Principal Components'  %(np.sum(per_var[:3])))\n\nscree_plot(pca)","89d55a20":"# DBSCAN\n\ndbscan = DBSCAN(eps=0.2)\ndbscan.fit(top3_pc)\ndbscan_labels = dbscan.labels_","fb598155":"x, y, z = top3_pc.T\n\nfig = px.scatter_3d(x = x, \n                    y = y, \n                    z = z,\n                    color = dbscan_labels)\nfig.show()","e6c2ad59":"labels, counts = np.unique(dbscan_labels, return_counts = True)\n\nplt.figure(figsize = (8,6))\nplt.title('Quantity of Each Cluster', size = 16)\nplt.bar(labels, counts)\nplt.xlabel('Cluster', size = 14)\nplt.ylabel('Customers', size = 14)\nplt.xticks(labels)\nplt.show()\n\nfor i in zip(labels, counts):\n    print('Cluster %s: %s' %(i[0],i[1]))","93bc2990":"cleaned_data.shape","4649102f":"final_data = cleaned_data.copy()\nfinal_data['Label'] = dbscan_labels\nfinal_data.drop(final_data[final_data['Label'] == -1].index, inplace = True) # Drop the 12 Anomaly Customers\nfinal_data.to_csv('final_data.csv') # save final dataset for next step, interpretation, conclusion, and recommendation\nfinal_data","f19fca0a":"for i in range(7):\n    print('Cluster:', i)\n    display(final_data[final_data['Label'] == i].describe())\n    print('\\n')","0fd64ff7":"fig, axs = plt.subplots(nrows = 17, ncols = 1, figsize = (16,70))\nfor i in range(17):\n    q1, q3 = np.quantile(final_data.iloc[:,i], [0.25, 0.75])\n    sns.violinplot(data = final_data[['Label', final_data.columns[i]]].loc[final_data[final_data.columns[i]] <= (q3 + 1.5*(q3-q1))], x = 'Label', y = final_data.columns[i], ax = axs[i])\n\nplt.show()","49afb656":"### 3. MinMaxScaler the dataset","f8abaf6c":"### CREDIT_LIMIT","946d16fe":"## Plotting Violing Plots\nVioling Plots helps to visualize each cluster against each feature\/variable. And is used to interpret each clusters behavior and to decide how certain treatments or promotion should be done for each cluster (explanation in next chapter, Conclusion)","25b9d2c3":"## 3. DBSCAN\nDBSCAN stands for density-based spatial clustering of applications with noise. It is able to find arbitrary shaped clusters and clusters with noise (i.e. outliers).\nThe main idea behind DBSCAN is that a point belongs to a cluster if it is close to many points from that cluster.","aa3d3a9e":"<html>\n<body>\n<img src=\"https:\/\/raw.githubusercontent.com\/daniel-bss\/CreditCardCustomerSegmentation\/main\/images\/bg.png\" style=\"width: 100%; height: 45%\">\n<h1 style=\"text-align: center\">Credit Card Customer Segmentation<\/h1>\n<hr style=\"border: 2px solid black;\">\n<h1>Business Understanding<\/h1>\n<p>It is essential for any businesses to understand their customers behaviour. Through that understanding, a business is able to effectively provide solutions for customers as products\/services. Many businesses leverage this knowledge to gain an edge over the competition and try to come up with better offers to customers.<\/p>\n\n<p>Markets are now saturated with the sheer amount of companies that provide similar products & services. Competition is is tighter than it was in the past because of the reach that technology has provided companies. This exasperates the importance of customer understanding.<\/p>\n\n<p>The banking industry collects insurmountable amounts of customer purchasing data every single day. Often times it is very overwhelming to try and understand large sums of data. However, as mentioned above, it is crucial for any modern business (especially banks) to leverage the data to maximize the decision making process. This project will attempt to understand the given data by finding patterns within the dataset using Clustering methods from the art of Machine Learning. The impact of this analysis will give the user clarity about their current customers and help with the automation of classifying the potential of future incoming customers. This understanding will then translate into specified marketing strategies that could maximize the banks revenue generation.<\/p>\n<\/body>\n<\/html>","620ccf0f":"### RobustScaler\nRobustScaler behaves similar to MinMaxScaler, but instead of using its Minimum and Maximum values, RobustScaler uses the 1sr and 3rd Quartile. With this way, data won't be affected by the presence of Outliers (huge number)","f1cf2a82":"It is pretty obvious that customers who haven't done any payments wouldn't have minimum payments also\n<br>\nHence, **if PAYMENTS = 0, MINIMUM_PAYMENTS = 0**","c716aa41":"# Import Libraries","928da6d5":"Data Understanding\nThis case requires to develop a customer segmentation to define marketing strategy. The sample Dataset summarizes the usage behavior of about 9000 active credit card holders during the last 6 months. The file is at a customer level with 18 behavioral variables. This study also aims to see different Machine Learning Methods on the same data set.\n\nGeneral Information of the Data\nCUST_ID : Identification of Credit Card holder (Categorical)\n\nBALANCE : Balance amount left in their account to make purchases\n\nBALANCE_FREQUENCY : How frequently the Balance is updated, score between 0 and 1 (1 = frequently updated, 0 = not frequently updated)\n\nPURCHASES : Amount of purchases made from account\n\nONEOFF_PURCHASES : Maximum purchase amount done in one-go\n\nINSTALLMENTS_PURCHASES : Amount of purchase done in installment\n\nCASH_ADVANCE : Cash in advance given by the user\n\nPURCHASES_FREQUENCY : How frequently the Purchases are being made, score between 0 and 1 (1 = frequently purchased, 0 = not frequently purchased)\n\nONEOFFPURCHASESFREQUENCY : How frequently Purchases are happening in one-go (1 = frequently purchased, 0 = not frequently purchased)\n\nPURCHASESINSTALLMENTSFREQUENCY : How frequently purchases in installments are being done (1 = frequently done, 0 = not frequently done)\n\nCASHADVANCEFREQUENCY : How frequently the cash in advance being paid\n\nCASHADVANCETRX : Number of Transactions made with \"Cash in Advanced\"\n\nPURCHASES_TRX : Numbe of purchase transactions made\n\nCREDIT_LIMIT : Limit of Credit Card for user\n\nPAYMENTS : Amount of Payment done by user\n\nMINIMUM_PAYMENTS : Minimum amount of payments made by user\n\nPRCFULLPAYMENT : Percent of full payment paid by user\n\nTENURE : Tenure of credit card service for user\n\nThe credit card data has 17 attributes for each customer which include the balance (credit owed by the customer), cash advance (when a customer withdraws cash using the credit card), the customer\u2019s credit limit, minimum payment, percentage of full payments and tenure.","922804ec":"### MaxAbsScaler\nMaxAbsScaler scales the data by the Absolute and Maximum number. So the presence of massive negative numbers will become 1","70ce305a":"MINIMUM_PAYMENTS has 313 missing values, that should be treated specifically","a5b3044d":"It's shown that CREDIT_LIMIT only has 1 missing value, which can simply be removed from the dataset","77a1cab6":"Turns out every feature has outliers. But this doesn't mean that we will delete the outliers, because it's possible that all of these values do happen on the field. <br> \nHence, the treatment that will be applied is to do **logarithmic scale** for each features. <br>\n\nLogarithmic scale is used because, mathematically huge numbers will be kept much smaller without losing the meaning of the data.","a0f060ac":"# Clustering","58cb1e27":"# Evaluation\nIn this evaluation chapter, we would like to withdraw conclusions and also give Recommendation which might helps the marketing team to optimize Credit Card utilization by different segment of customers.\n\n### Recommendation\nBased on the Violin Plot that is on Chapter 4, we can see the different type of customers within the dataset. Below we will propose potential strategies for each customer type.\n<ul>\n    <li><b>Cluster 0 : Using Installment Purchase Only<\/b> - They make expensive purchases using installments and their credit's balance is low. We can give them promotions for cheap item purchases using their Credit Card (e.g: Buy 1 Get 1 Starbucks Promo). The idea of this promotion is to encourage more frequent purchases within their behavior, we hope that through the discounts we can improve this clusters retention and improve the consistency of revenue.<\/li>\n    <li><b>Cluster 1 : Using Cash Advances Only<\/b> - This is the largest group among the 6 clusters. The marketing strategy for this cluster is to increase their credit limit for Cash Advance & to reduce the cash advances interest. Through increasing the limit & reducing interest we will put ourselves in a position over our competitors since we have our users best interest. Again we hope to improve retention and frequency with this strategy. Since this is the largest group amongst the customer base, we must prioritize our strategies here.<\/li>\n    <li><b>Cluster 2 : Using One Off Purchase Only<\/b> - They tend to make cheap purchases using One Off Purchase. We can give them higher One Off credit limit to keep them using One Off Purchases.<\/li>\n    <li><b>Cluster 3 : Less Spenders but Not Using Installment Purchases<\/b> - Marketing strategy for this cluster is to increase the limit of cash advance and one off purchases. Every other purchase could be awarded with 0% interest for installment purchases. This would keep customers purchasing and chase the 0% interest reward.<\/li>\n    <li><b>Cluster 4 : Big Spenders but Not Using Cash Advance<\/b> - There are no treatments for this Cluster because this group is using credit card very well and likely to continue to do so. It is better to focus on users that uses cash advances so that we can gain interest.<\/li>\n    <li><b>Cluster 5 : Low Financial<\/b> - This appears to be a small group of customers that using installment and cash advance from credit card. It is best to not focus strategies to this cluster as well, mainly due to its small group size.<\/li>\n    <li><b>Cluster 6 : Highest Average Credit Limit with All Types of Purchases<\/b> - The best marketing strategy for this cluster is to give credit points for every time they make transaction using credit card. These points would incentivies the customers to be loyal to the bank and reduce churn rates.<\/li>\n<\/ul>\n\n### Conclusion\nWithin the domain of Unsupervised Learning, especially in the context of Customer Segmentation, normalizing and standardizing the data will determine how the data will be plotted. Therefore it will determine how the clusters get formed. Along with the scaling methods, the modeling algorithm will affect clustering as well. Unsupervised learning finds patterns within unlabelled data. Due to the vague nature of unsupervised learning, the patters that are formed could make sense mathematically and still be misaligned with the the goal of said case study (in this case Credit Card Data). To solve this issue, the best way is to brute force it and analyze the results within every situation. Fortunately, the variables that we have to consider are realistic and manageable.<br><br>\nTo better understand the flow of our experiments we can summarize our process into a pipeline<br>\n<ol>\n    <li>Log the data<\/li>\n    <li>Scale the data<\/li>\n    <ul>\n        <li>MinMax Scaler<\/li>\n        <li>Robust Scaler<\/li>\n        <li>Standard Scaler<\/li>\n        <li>MaxAbs Scaler<\/li>\n    <\/ul>\n    <li>PCA<\/li>\n    <li>Model the data<\/li>\n    <ul>\n        <li>K-Means<\/li>\n        <li>DBSCAN<\/li>\n        <li>K-Medoids<\/li>\n    <\/ul>\n<\/ol>\n\nWithin steps 2 & 3 (Before Clustering) we aim to normalize & scale the data In a way where when we reduce its dimensionality the data\u2019s variance is kept. What we will look at is the variance score of each scaling method of the top 3 PC from the PCA.<br>\nVariance scores :\n<ol>\n    <li>RobustScaler = 91.3%<\/li>\n    <li>MinMaxScaler = 81.5%<\/li>\n    <li>StandardScaler = 67.5%<\/li>\n    <li>MaxAbsScaler = 87.5%<\/li>\n<\/ol>\n\nIt is shown that the Robust scaler has the best variance score compared to the rest, looking at the scores we should use the robust scaler over the other three. The next stage of the experiment is to pair each scaler with the clustering algorithms (K-Means, DBSCAN, and K-Medoids). Despite the Robust Scalers variance score, during the final stage of clustering we found out that the combination between MinMaxScaler and DBSCAN resulted in clusters that makes sense within our use case.<br><br>\nTo tie things up, it is more important to understand the business results that could be gained from clustering the credit card data. Understanding your customers is at the heart of a successful marketing strategy, through clustering the business can now identify the distinct customer types that they have thus validating and aligning their strategy accordingly. Marketing often times is labelled as the backbone of companies within the 21st century, responsible for the business\u2019s high revenues. Deploying this clustering model, will help the marketing department increase Business Revenues as well as overall customer satisfaction.","33f9adab":"### 4. PCA with MaxAbsScaler","675bb52e":"## Trying different scalers:\n- MinMaxScaler\n- StandardScaler\n- RobustScaler\n- MaxAbsScaler","32a9a1d5":"## Taking a glance of data distributions","29cf8539":"### Conclusion:\nMinMaxScaler did the best for resulting in good shape of clusters, and also have big value of 3 Principal Components which is 81.5%<br> Hence, MinMaxScaler is used to transform the data before PCA Transformation","3252aa5c":"Data scatters is clustered very clearly, which can be seen as 7 clusters","17fa5c96":"After Exploratory Data Analysis that has been done in previous chapter, we have made a pipeline for the raw data transformation, which are:\n1. Imputing Missing Values as planned before.\n2. Logarithmic Scale the data as a treatment for Outliers.\n3. MinMaxScaler to scale the dataset equally from 0 to 1.\n4. Apply clustering method by using DBSCAN, because the PCA-transformed data are clustered neatly.","bb04a50b":"#### Conclusion:\n5 Clusters is chosen, because the elbow is positioned at 5th cluster, and the highest value in Silhouetter Score is also positioned at 5th cluster","c1aa8a11":"#### Conclusion:\n7 Clusters is chosen, because the elbow is positioned at 7th cluster, and the highest value in Silhouetter Score is also positioned at 7th cluster","4d2c2a5d":"### Label the dataset by each cluster (Customer Segementation) ","4ef91556":"Logically, payments should be done if PAYMENTS above MINIMUM_PAYMENTS. Which is true for **6272** customers.\n<br> But on the other hand, we got **2364** customers who did the PAYMENTS below MINIMUM_PAYMENTS which lead to invalid values. But we would like to leave it that way.\n<br> Hence, we **fill the missing values by the mean of PAYMENTS**\n<br> **if PAYMENTS is less than MINIMUM_PAYMENTS, the missing values will be filled by the correspond PAYMENTS**","4c2b4cf8":"<img src = \"https:\/\/raw.githubusercontent.com\/daniel-bss\/CreditCardCustomerSegmentation\/main\/images\/dbscan_anomaly.png\">","66c3b0b5":"### MinMaxScaler\n\nMinMax Scaling is a scaler that is highly influenced by the maximum and minimum values in our data so if our data contains outliers it is going to be biased.\nMinMaxScaler rescales the data set in such a way that all features values are in the range [0, 1]. This is done by feature-wise in an independent way.","7b6e863d":"#### Looking at the quantity of each Cluster","f4f92070":"### 2. Logarithmic Scale the data as a treatment for Outliers.","5ab017b3":"### 4. PCA + DBSCAN Clustering ","cf41d573":"# EDA","36757235":"## Conclusion\nIn K-Means Clustering, few datas is miss-clustered. Because in K-Means algorithm itself, it uses Euclidean Distance which in short, it measure how likely a data is clustered to its nearest cluster. Whereas in DBSCAN, it uses what so-called Epsilon, which is the radius parameter. Meaning that datas that nearer with each other will be clustered in one.\n\nHence, **MinMaxScaler and DBSCAN will be used for Data Processing in the future**","f50c36ff":"<!-- Preview:<br>\n<img src=\"https:\/\/raw.githubusercontent.com\/daniel-bss\/CreditCardCustomerSegmentation\/main\/images\/pca_1.png\"> -->","900ed284":"### MINIMUM_PAYMENTS","5f998d00":"### 3. PCA with RobustScaler","42b4efd0":"<!-- Preview:<br>\n<img src=\"https:\/\/raw.githubusercontent.com\/daniel-bss\/CreditCardCustomerSegmentation\/main\/images\/pca_4.png\"><br>\nThis result behave similar to MinMaxScaler, but around each clusters are surrounded by some unclustered data, which likely to be an anomoly that doesn't cluster well -->","9d493d88":"### 2. PCA with StandardScaler","14632ec4":"<!-- Preview:<br>\n<img src=\"https:\/\/raw.githubusercontent.com\/daniel-bss\/CreditCardCustomerSegmentation\/main\/images\/pca_3.png\"><br>\nData is obviously not clustered very well. Looks like some data is heavily skewed -->","fbbe799e":"### 1. K-Means Clustering\nK-means clustering is a type of unsupervised learning, which is used with unlabeled dataset. The goal of this algorithm is to find K groups in the data. The algorithm works iteratively to assign each data point to one of K groups based on the features that are provided.\n<br>\n\nChoosing K\nIf the true label is not known in advance, then K-Means clustering can be evaluated using Elbow Criterion , Silhouette Coefficient , cross-validation, information criteria, the information theoretic jump method, and the G-means algorithm. ","d3041add":"## 2. K-Medoids\nIn contrast to the K-Means algorithm, K-Medoids chooses actual data points as centers and attempts to minimize dissimilarities between points data","3584b300":"### StandardScaler\nStandardScaler is a scaler that move the whole dataset into the center\/origin point and scaling it, which results in 0 Mean and 1 Standard Deviation ","b18636f4":"### 1. Imputing Missing Values","3dba7b68":"It's shown that there are 12 Customers which are considered as Anomaly (-1), that can be deleted considering that there are only 12 customers.\n<br>\n\nThose customers are considered as Anomaly, because their data don't behave differently with other cluster, which make them act as a **bridge which lay between clusters**","62ef5604":"<!-- Preview:<br>\n<img src=\"https:\/\/raw.githubusercontent.com\/daniel-bss\/CreditCardCustomerSegmentation\/main\/images\/eda_dbscan.png\"> -->","b5d0293d":"Clustering method will be done with K-Means Clustering, K-Medoids, and DBSCAN","0c7e1739":"### Take a glance of Descriptive Statistics for each cluster","b5d43084":"---","d57c880b":"<!-- Preview:<br>\n<img src=\"https:\/\/raw.githubusercontent.com\/daniel-bss\/CreditCardCustomerSegmentation\/main\/images\/pca_2.png\"><br>\nData is not clustered clear enough, and results in much skewed to the negative values -->","871467d2":"<!-- Preview:<br>\n<img src=\"https:\/\/raw.githubusercontent.com\/daniel-bss\/CreditCardCustomerSegmentation\/main\/images\/eda_Kmedoids.png\"> -->","2bb05647":"# Principal Component Analysis\nPrincipal component analysis is a technique for feature extraction \u2014 so it combines our input variables in a specific way, then we can drop the \u201cleast important\u201d variables while still retaining the most valuable parts of all of the variables\nThis is a benefit because the assumptions of a linear model require our independent variables to be independent of one another.\n\nSo after having 4 different scaled data, we will use it against PCA and choose which one results in good cluster in 3D Scatter Plot","fee02d83":"<!-- Preview:<br>\n<img src=\"https:\/\/raw.githubusercontent.com\/daniel-bss\/CreditCardCustomerSegmentation\/main\/images\/eda_Kmedoids.png\"><br>\nUsing K-Medoids as a cluster method results in worse cluster. There are 3 Dark Blue Clusters that are different, and also 2 Yellow Clusters that are different -->","cef7b3e4":"# Data Understanding","2706a5b9":"Elbow Criterion Method:\nThe idea behind elbow method is to run k-means clustering on a given dataset for a range of values of k (e.g k=1 to 10), for each value of k, calculate sum of squared errors (SSE).","3c498c56":"### 1. PCA with MinMaxScaler ","544f4601":"<!-- Preview:<br>\n<img src=\"https:\/\/raw.githubusercontent.com\/daniel-bss\/CreditCardCustomerSegmentation\/main\/images\/eda_kmeans.png\"> -->","d28dfe2f":"Silhouette Coefficient Method:\n<br> A higher Silhouette Coefficient score relates to a model with better-defined clusters. The Silhouette Coefficient is defined for each sample and is composed of two scores:\n- The mean distance between a sample and all other points in the same class.\n- The mean distance between a sample and all other points in the next nearest cluster.","fdafd5a7":"# Data Preparation and Modeling"}}