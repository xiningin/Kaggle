{"cell_type":{"8b21304a":"code","4ed5e685":"code","9f0a57da":"code","0ae71160":"markdown","4c6276b0":"markdown","92707072":"markdown","f9fd7d5e":"markdown","f9f8030d":"markdown","09096b6d":"markdown","a964c4ae":"markdown"},"source":{"8b21304a":"import numpy as np\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom IPython.core.display import HTML\ndef css_styling():\n    styles = open(\"..\/input\/titlestyle\/style2.css\", \"r\").read()\n    return HTML(styles)\ncss_styling()","4ed5e685":"train_df = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/train.csv\")\nprint('Sample with minimum target value ({})'.format(train_df['target'].min()))\nprint(train_df[train_df['target'] == train_df['target'].min()]['excerpt'].values)\nprint('_'*100)\nprint('Sample with maximum target value ({})'.format(train_df['target'].max()))\nprint(train_df[train_df['target'] == train_df['target'].max()]['excerpt'].values)","9f0a57da":"train_df.head()","0ae71160":"<div class=\"heading\">\n   <h1><span style=\"color: white\">Transformers<\/span><\/h1>\n<\/div>\n<div class='content'>\n    \nA <b>transformer<\/b> is a deep learning model that adopts the mechanism of attention, differentially weighing the significance of each part of the input data and It is used primarily in the field of natural language processing (NLP) The most popular transformer-based model is called <b>BERT (Bidirectional Encoder Representations from Transformers)<\/b>.\n<br><br>\n\ud83d\udd25<b>Comprehensive notebook related to BERT and Transformer models<\/b> with different ways of utilizing layers and outputs, finetuning stability, LIT (Language Interpretability Tool), speeding up transformers, etc. This notebook also provides many content-related references. <a href=\"https:\/\/www.kaggle.com\/rhtsingh\/utilizing-transformer-representations-efficiently\">https:\/\/www.kaggle.com\/rhtsingh\/utilizing-transformer-representations-efficiently<\/a>\n<br><br>\n\ud83d\udd25<b>BERT, theoretical explanation from scratch<\/b> <a href=\"https:\/\/www.kaggle.com\/mdfahimreshm\/bert-in-depth-understanding\">https:\/\/www.kaggle.com\/mdfahimreshm\/bert-in-depth-understanding<\/a>\n<br><br>\n\ud83d\udd25<b>Ideas to improve BERT model<\/b> <a href=\"https:\/\/www.kaggle.com\/c\/commonlitreadabilityprize\/discussion\/241029\">https:\/\/www.kaggle.com\/c\/commonlitreadabilityprize\/discussion\/241029<\/a>\n<br><br>\nSimplier theoretical explanation of BERT model with example using Keras <a href=\"https:\/\/www.kaggle.com\/krishna1997gopal\/understand-bert-in-depth-theory-implementation\">https:\/\/www.kaggle.com\/krishna1997gopal\/understand-bert-in-depth-theory-implementation<\/a>\n<br><br>\nOne modification of BERT model is <b>RoBERTa<\/b>. It builds on BERT and modifies key hyperparameters, removing the next-sentence pretraining objective and training with much larger mini-batches and learning rates. RoBERTa base solution in PyTorch <a href=\"https:\/\/www.kaggle.com\/andretugan\/lightweight-roberta-solution-in-pytorch\">https:\/\/www.kaggle.com\/andretugan\/lightweight-roberta-solution-in-pytorch<\/a>\n<br><br>\n<b>RoBERTa with TF<\/b> implementation can be found here <a href=\"https:\/\/www.kaggle.com\/dimitreoliveira\/commonlit-readability-eda-roberta-tf-baseline\">https:\/\/www.kaggle.com\/dimitreoliveira\/commonlit-readability-eda-roberta-tf-baseline<\/a>\n<br><br>\nSimilarly, user friendly implementation of <b>ensemble with BERT, RoBERTa and distilBERT<\/b> <a href=\"https:\/\/www.kaggle.com\/eneszvo\/bert-roberta-distilbert-ensemble-5-fold-cv\">https:\/\/www.kaggle.com\/eneszvo\/bert-roberta-distilbert-ensemble-5-fold-cv<\/a>\n<br><br>\n<b>Pytorch BERT<\/b> step by step for begginers <a href=\"https:\/\/www.kaggle.com\/chumajin\/pytorch-bert-beginner-s-room\">https:\/\/www.kaggle.com\/chumajin\/pytorch-bert-beginner-s-room<\/a>\n<br><br>\nUser friendly <b>BERT model with Keras<\/b> as well as many other models <a href=\"https:\/\/www.kaggle.com\/donmarch14\/commonlit-detailed-guide-to-learn-nlp\">https:\/\/www.kaggle.com\/donmarch14\/commonlit-detailed-guide-to-learn-nlp<\/a>\n<br><br>\nOne interesting method of word embedding used to be popular before transformers and BERT is <b>GloVe<\/b>. It focuses on words co-occurrences over the whole corpus. Its embeddings relate to the probability that two words appear together. <b>GloVe with LSTM<\/b> are explained here <a href=\"https:\/\/www.kaggle.com\/andreshg\/commonlit-a-complete-analysis\">https:\/\/www.kaggle.com\/andreshg\/commonlit-a-complete-analysis<\/a>\n<\/div>","4c6276b0":"<div style = \"font-family: Arial;font-size:1.6em;color: #0a6121;background: #ace6bc;padding:5px;border-style: solid;border-color:#0a6121;\">\n<b>Summa summarum: Transformers, ensemble methods and meta-labeling<\/b> \n<\/div>","92707072":"<div class=\"heading\">\n   <h1><span style=\"color: white\">Intro<\/span><\/h1>\n<\/div>\n<div class=\"content\">\n\n<u>\ud83d\udcd4 Public notebooks - 942+<\/u><br>\n\n<u>\ud83e\udd47 Gold medals - 56+<\/u><br>\n\n<u>\ud83e\udd48 Silver medals - 64+<\/u><br>\n\n<u>\ud83e\udd49 Bronze medals - 235+<\/u><br>\n    \n<u>1\ufe0f place solution - https:\/\/www.kaggle.com\/c\/commonlitreadabilityprize\/discussion\/257844<\/u><br>\n    \n<u>2\ufe0f place solution - https:\/\/www.kaggle.com\/c\/commonlitreadabilityprize\/discussion\/258328<\/u><br>\n    \n<u>3\ufe0f place solution - https:\/\/www.kaggle.com\/c\/commonlitreadabilityprize\/discussion\/258095<\/u><br>\n\n\nWordcloud made of notebook titles:\n<img src=\"https:\/\/i.imgur.com\/FDr7zla.png\" alt=\"img1\"\/>\n<\/div>","f9fd7d5e":"<div class=\"heading\">\n   <h1><span style=\"color: white\">Other useful notebooks<\/span><\/h1>\n<\/div>\n<div class='content'>\n    \nNot directly related to this competition but interesting, step by step implementation of <b>NGram<\/b> approach, where the idea is to get the next word given the sequence of words <a href=\"https:\/\/www.kaggle.com\/alincijov\/nlp-starter-logsoftmax-nlloss-cross-entropy\">https:\/\/www.kaggle.com\/alincijov\/nlp-starter-logsoftmax-nlloss-cross-entropy<\/a>\n<br><br>\n<b>PyTorch data samplers<\/b> and how to contol batches <a href=\"https:\/\/www.kaggle.com\/shahules\/guide-pytorch-data-samplers-sequence-bucketing\">https:\/\/www.kaggle.com\/shahules\/guide-pytorch-data-samplers-sequence-bucketing<\/a>\n<br><br>\nComprehensive notebook with <b>data cleaning<\/b> techniques <a href=\"https:\/\/www.kaggle.com\/mpwolke\/dataprep-clean-literature\">https:\/\/www.kaggle.com\/mpwolke\/dataprep-clean-literature<\/a>\n<\/div>","f9f8030d":"<div class=\"heading\">\n   <h1><span style=\"color: white\">Non neural network approach<\/span><\/h1>\n<\/div>\n<div class='content'>\n\n<b>Features engineering<\/b> is a process by which we extract features from raw data for use in our machine learning model. We need these features because they help us better understand the relationships between variables in our dataset. Using the `readability` library we can extract 24 powerful traditional features such as <b>words per sentence, Flesch, or Kincaid readability score.<\/b> In addition, the way of creating more than <b>300 diverse features from text<\/b> using `spacy` library, as well as a Ridge regression model, is presented here <a href=\"https:\/\/www.kaggle.com\/ravishah1\/readability-feature-engineering-non-nn-baseline\">https:\/\/www.kaggle.com\/ravishah1\/readability-feature-engineering-non-nn-baseline<\/a>\n<br><br>\nXGBRFRegressor with features importance can be found here <a href=\"https:\/\/www.kaggle.com\/andradaolteanu\/i-commonlit-explore-xgbrf-repeatedfold-model\">https:\/\/www.kaggle.com\/andradaolteanu\/i-commonlit-explore-xgbrf-repeatedfold-model<\/a>\n<br><br>\nOne way of converting the collection of text documents to a matrix of tokens is using <b>CountVectorizer<\/b>. It simply counts the appearance of each word in the sentence using one corpus vector.\n<img src=\"https:\/\/i.imgur.com\/sFToffN.png\" alt=\"img1\"\/>\nSlightly more complex sentence representation would be TF-IDF (term frequency-inverse document frequency)\n<img src=\"https:\/\/i.imgur.com\/72ZeW1X.png\" alt=\"img1\"\/>\nBoth approaches are explained here <a href=\"https:\/\/www.kaggle.com\/andreshg\/commonlit-a-complete-analysis\">https:\/\/www.kaggle.com\/andreshg\/commonlit-a-complete-analysis<\/a>\n\nVery simple <b>Ridge<\/b> model with spacy features <a href=\"https:\/\/www.kaggle.com\/konradb\/linear-baseline-with-cv\">https:\/\/www.kaggle.com\/konradb\/linear-baseline-with-cv<\/a>\n<\/div>","09096b6d":"<div class=\"heading\">\n   <h1><span style=\"color: white\">Text preprocessing<\/span><\/h1>\n<\/div>\n<div class='content'>\n    \nText preprocessing is the first step in Natural Language Processing (NLP). The process of NLP text preprocessing includes removing stop words, punctuation, and numbers in the text. The primary goal of NLP text preprocessing is to remove content that does not carry any semantic meaning. This will help reduce the time needed for processing and make the rest of the process more accurate.\n<br><br>\n<b>Stopword removal<\/b> removes a list of common words (e.g., \"a\", \"an\", \"the\") that are not useful for building any meaningful relationships between different input sentences or phrases.\n<b>Lemmatizing<\/b> is the process of reducing words to their roots or base forms, which are more easily processed by computers. For example, the words \"writing\", \"written\" and \"writes\" might all be assigned the lexeme \"write.\"\nIn order to analyze textuall content, we can observe most common <b>unigrams<\/b> (single word), <b>bigrams<\/b> (group of two words), <b>trigrams<\/b> (group of two words) etc.\n<br><br>\nText <b>part-of-speech tagging<\/b> is the process by which a machine assigns parts of speech to words according to linguistic rules and patterns. The most common POS tags are Noun, Verb, Adjective, Adverb, Preposition, Conjunction, Interjection.\n<br><br>\nEDA, stopword removal, lemmatizing and part-of-speech tagging are presented here <a href=\"https:\/\/www.kaggle.com\/ruchi798\/commonlit-readability-prize-eda-baseline\">https:\/\/www.kaggle.com\/ruchi798\/commonlit-readability-prize-eda-baseline<\/a>\n<\/div>","a964c4ae":"<div class=\"heading\">\n   <h1><span style=\"color: white\">Goal<\/span><\/h1>\n<\/div>\n<div class='content'>\n    <h3><b>\ud83d\udcd6 Build algorithms to rate the complexity of reading passages for grade 3-12 classroom use.<\/b><\/h3>\n    <h3><b>\ud83d\udcd6 The rate complexity is a number between -3.67 and 1.71 that is the result of a Bradley-Terry analysis of more than 111,000 pairwise comparisons. This task is very similar to simple sentiment prediction with a difference in different target range.<\/b><\/h3>\n<\/div>"}}