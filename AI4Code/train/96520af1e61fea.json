{"cell_type":{"a1baa9d8":"code","75744cca":"code","c6d0d24d":"code","39186f98":"code","a9bacc38":"code","8e4c183e":"code","264baa64":"code","bdf5075f":"code","3aa87403":"code","76c0f35b":"code","f85a9da4":"code","90662239":"code","5560f83f":"code","1e112f2e":"code","d7d7f6c0":"code","db26aeb9":"code","e220df6c":"code","8c9a7c41":"code","09e06295":"code","d5d8bfdf":"code","2c35dea6":"code","a625ccce":"code","2c347bc3":"code","bdcad18b":"code","84879862":"code","4de3566b":"code","69ccff58":"code","80de9959":"code","8e8332f4":"code","c4f5b2e6":"code","039f1378":"code","4574f83a":"code","a9728ef9":"code","855faa79":"code","28cd1185":"code","42de8653":"code","7edeea56":"code","1c34a614":"code","aa6f6fba":"code","dcab28f7":"code","9f0dd17d":"code","71d1c6ef":"code","e95ddb45":"code","676d2ed7":"code","a0bab5c7":"code","278cad9c":"code","b0298da6":"code","40334f8f":"code","5d05247c":"code","5952e9db":"code","b15d65ec":"code","c7379c02":"code","6e89cdcf":"code","0ed11f15":"code","204fbc5b":"code","6b20abe8":"code","a7e53817":"code","aa82e2a7":"code","a6daefc8":"code","724f0a0d":"code","b687b0de":"code","5c0b9572":"code","b7c721b6":"code","0642834f":"code","5d3c42a4":"code","e99de604":"code","79234655":"code","0559d999":"code","bd8ee8ff":"code","3685d820":"code","2da8aa7f":"code","e90ea19a":"code","b054dfd2":"code","59fe6a65":"code","4767b1f3":"code","cfeed55e":"code","d337fbfd":"code","13b758af":"code","be7a5793":"code","27281b46":"markdown","105e6536":"markdown","385ae3cc":"markdown","ba870055":"markdown","36e44a0c":"markdown","dccfeac4":"markdown","8bf341f4":"markdown"},"source":{"a1baa9d8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","75744cca":"pd.set_option(\"display.max_columns\", None)\n\ntrain_df = pd.read_csv(\"..\/input\/tabular-playground-series-dec-2021\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/tabular-playground-series-dec-2021\/test.csv\")\nsub_df = pd.read_csv(\"..\/input\/tabular-playground-series-dec-2021\/sample_submission.csv\")\n\ntrain_df.head()","c6d0d24d":"train_df.groupby(\"Cover_Type\")['Id'].nunique()","39186f98":"temp_df=train_df[train_df.Cover_Type==5]","a9bacc38":"temp_df","8e4c183e":"temp_df.shape","264baa64":"train_df=train_df.append(temp_df)","bdf5075f":"train_df.shape","3aa87403":"cols = [\"Soil_Type7\", \"Soil_Type15\"]\n\ntrain_df.drop(cols, axis=1, inplace=True)\ntest_df.drop(cols, axis=1, inplace=True)","76c0f35b":"new_names = {\n    \"Horizontal_Distance_To_Hydrology\": \"x_dist_hydrlgy\",\n    \"Vertical_Distance_To_Hydrology\": \"y_dist_hydrlgy\",\n    \"Horizontal_Distance_To_Roadways\": \"x_dist_rdwys\",\n    \"Horizontal_Distance_To_Fire_Points\": \"x_dist_firepts\"\n}\n\ntrain_df.rename(new_names, axis=1, inplace=True)\ntest_df.rename(new_names, axis=1, inplace=True)","f85a9da4":"train_df[\"Cover_Type\"]=train_df[\"Cover_Type\"]-1","90662239":"train_df.groupby(\"Cover_Type\")['Id'].nunique()","5560f83f":"train_df[\"Aspect\"][train_df[\"Aspect\"] < 0] += 360\ntrain_df[\"Aspect\"][train_df[\"Aspect\"] > 359] -= 360\n\ntest_df[\"Aspect\"][test_df[\"Aspect\"] < 0] += 360\ntest_df[\"Aspect\"][test_df[\"Aspect\"] > 359] -= 360","1e112f2e":"train_df[\"mnhttn_dist_hydrlgy\"] = np.abs(train_df[\"x_dist_hydrlgy\"]) + np.abs(train_df[\"y_dist_hydrlgy\"])\ntest_df[\"mnhttn_dist_hydrlgy\"] = np.abs(test_df[\"x_dist_hydrlgy\"]) + np.abs(test_df[\"y_dist_hydrlgy\"])\n\n# Euclidean distance to Hydrology\ntrain_df[\"ecldn_dist_hydrlgy\"] = (train_df[\"x_dist_hydrlgy\"]**2 + train_df[\"y_dist_hydrlgy\"]**2)**0.5\ntest_df[\"ecldn_dist_hydrlgy\"] = (test_df[\"x_dist_hydrlgy\"]**2 + test_df[\"y_dist_hydrlgy\"]**2)**0.5","d7d7f6c0":"soil_features = [x for x in train_df.columns if x.startswith(\"Soil_Type\")]\ntrain_df[\"soil_type_count\"] = train_df[soil_features].sum(axis=1)\ntest_df[\"soil_type_count\"] = test_df[soil_features].sum(axis=1)\n\nwilderness_features = [x for x in train_df.columns if x.startswith(\"Wilderness_Area\")]\ntrain_df[\"wilderness_area_count\"] = train_df[wilderness_features].sum(axis=1)\ntest_df[\"wilderness_area_count\"] = test_df[wilderness_features].sum(axis=1)","db26aeb9":"train_df.loc[train_df[\"Hillshade_9am\"] < 0, \"Hillshade_9am\"] = 0\ntest_df.loc[test_df[\"Hillshade_9am\"] < 0, \"Hillshade_9am\"] = 0\n\ntrain_df.loc[train_df[\"Hillshade_Noon\"] < 0, \"Hillshade_Noon\"] = 0\ntest_df.loc[test_df[\"Hillshade_Noon\"] < 0, \"Hillshade_Noon\"] = 0\n\ntrain_df.loc[train_df[\"Hillshade_3pm\"] < 0, \"Hillshade_3pm\"] = 0\ntest_df.loc[test_df[\"Hillshade_3pm\"] < 0, \"Hillshade_3pm\"] = 0\n\ntrain_df.loc[train_df[\"Hillshade_9am\"] > 255, \"Hillshade_9am\"] = 255\ntest_df.loc[test_df[\"Hillshade_9am\"] > 255, \"Hillshade_9am\"] = 255\n\ntrain_df.loc[train_df[\"Hillshade_Noon\"] > 255, \"Hillshade_Noon\"] = 255\ntest_df.loc[test_df[\"Hillshade_Noon\"] > 255, \"Hillshade_Noon\"] = 255\n\ntrain_df.loc[train_df[\"Hillshade_3pm\"] > 255, \"Hillshade_3pm\"] = 255\ntest_df.loc[test_df[\"Hillshade_3pm\"] > 255, \"Hillshade_3pm\"] = 255","e220df6c":"from sklearn.preprocessing import RobustScaler\n\n\ncols = [\n    \"Elevation\",\n    \"Aspect\",\n    \"mnhttn_dist_hydrlgy\",\n    \"ecldn_dist_hydrlgy\",\n    \"soil_type_count\",\n    \"wilderness_area_count\",\n    \"Slope\",\n    \"x_dist_hydrlgy\",\n    \"y_dist_hydrlgy\",\n    \"x_dist_rdwys\",\n    \"Hillshade_9am\",\n    \"Hillshade_Noon\",\n    \"Hillshade_3pm\",\n    \"x_dist_firepts\",\n    \"soil_type_count\",\n    \"wilderness_area_count\"\n]\n\nscaler = RobustScaler()\ntrain_df[cols] = scaler.fit_transform(train_df[cols])\ntest_df[cols] = scaler.transform(test_df[cols])","8c9a7c41":"df=train_df","09e06295":"df.groupby(\"Cover_Type\")['Id'].nunique()","d5d8bfdf":"df.rename({'Cover_Type':'target'}, axis=1, inplace=True)","2c35dea6":"df.groupby(\"target\")['Id'].nunique()","a625ccce":"from sklearn import model_selection\ndf[\"kfold\"] = -1\n\ndf = df.sample(frac=1).reset_index(drop=True)\n\nkf = model_selection.StratifiedKFold(n_splits=2, shuffle=False)\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X=df, y=df.target.values)):\n    print(len(train_idx), len(val_idx))\n    df.loc[val_idx, 'kfold'] = fold","2c347bc3":"df.groupby(\"target\")['Id'].nunique()","bdcad18b":"df.to_csv(\"tpsDecemberTrainingDataPreprocessed_new.csv\",index=False)\ntest_df.to_csv(\"tpsDecemberTestingDataPreprocessed_new.csv\",index=False)","84879862":"n_folds=2","4de3566b":"import joblib\nimport xgboost as xgb","69ccff58":"df_folds=pd.read_csv(\"..\/input\/preprocessedtrainingandtestingdata-new\/prprocessedTrainingAndTestingData\/tpsDecemberTrainingDataPreprocessed_new.csv\")","80de9959":"def run_folds(df,fold):\n    df_train=df[df.kfold!=fold]\n    df_test=df[df.kfold==fold]\n    print(\"Entered\")\n    param={'tree_method':'gpu_hist'}\n    \n    clf= xgb.XGBClassifier(**param)\n    clf.fit(df_train.drop(['target','Id','kfold'],axis=1).values,df_train.target.values)\n    \n    y_pred = clf.predict_proba(df_test.drop(['target','Id','kfold'],axis=1).values)\n    \n    File_name = 'model_xgbclf' + str(fold)\n    joblib.dump(\n    clf,File_name)\n    print(y_pred.shape)\n    df_test.loc[:,\"xgbclf_pred1\"]=y_pred[:,0]\n    df_test.loc[:,\"xgbclf_pred2\"]=y_pred[:,1]\n    df_test.loc[:,\"xgbclf_pred3\"]=y_pred[:,2]\n    df_test.loc[:,\"xgbclf_pred4\"]=y_pred[:,3]\n    df_test.loc[:,\"xgbclf_pred5\"]=y_pred[:,4]\n    df_test.loc[:,\"xgbclf_pred6\"]=y_pred[:,5]\n    df_test.loc[:,\"xgbclf_pred7\"]=y_pred[:,6]\n    \n    return df_test[['Id','target','kfold','xgbclf_pred1','xgbclf_pred2','xgbclf_pred3','xgbclf_pred4','xgbclf_pred5','xgbclf_pred6','xgbclf_pred7']]\n\n\ndfs=[]\n    \nfor i in range(n_folds):\n    temp_df=run_folds(df_folds,i)\n    dfs.append(temp_df)\ndf_pred=pd.concat(dfs)\n\nprint(df_pred.shape)","8e8332f4":"df_pred.to_csv(\"XGBOOST_PREDICTIONS_PREPROCESSED_NEW.csv\",index=False)","c4f5b2e6":"n_folds=2","039f1378":"import joblib\nimport lightgbm as lgb","4574f83a":"df_folds=pd.read_csv(\".\/tpsDecemberTrainingDataPreprocessed_new.csv\")","a9728ef9":"df_folds.groupby('target')['Id'].nunique()","855faa79":"def run_folds(df,fold):\n    print(\"ENTER\")\n    df_train=df[df.kfold!=fold]\n    df_test=df[df.kfold==fold]\n    \n    clf= lgb.LGBMClassifier()\n    clf.fit(df_train.drop(['target','Id','kfold'],axis=1).values,df_train.target.values)\n    \n    y_pred = clf.predict_proba(df_test.drop(['target','Id','kfold'],axis=1).values)#[:,1]\n\n    \n    File_name = 'model_lgbmclf' + str(fold)\n    joblib.dump(\n    clf,File_name)\n    print(y_pred.shape)\n    df_test.loc[:,\"lgbmclf_pred1\"]=y_pred[:,0]\n    df_test.loc[:,\"lgbmclf_pred2\"]=y_pred[:,1]\n    df_test.loc[:,\"lgbmclf_pred3\"]=y_pred[:,2]\n    df_test.loc[:,\"lgbmclf_pred4\"]=y_pred[:,3]\n    df_test.loc[:,\"lgbmclf_pred5\"]=y_pred[:,4]\n    df_test.loc[:,\"lgbmclf_pred6\"]=y_pred[:,5]\n    df_test.loc[:,\"lgbmclf_pred7\"]=y_pred[:,6]\n    \n    return df_test[['Id','target','kfold','lgbmclf_pred1','lgbmclf_pred2','lgbmclf_pred3','lgbmclf_pred4','lgbmclf_pred5','lgbmclf_pred6','lgbmclf_pred7']]\n\n\ndfs=[]\n    \nfor i in range(n_folds):\n    temp_df=run_folds(df_folds,i)\n    dfs.append(temp_df)\ndf_pred=pd.concat(dfs)\n\nprint(df_pred.shape)","28cd1185":"df_pred.head()","42de8653":"df_pred.groupby('target')['Id'].nunique()","7edeea56":"df_pred.to_csv(\"LGBM_PRED_PREPROCESSED_NEW.csv\",index=False)","1c34a614":"df_lgbm=pd.read_csv(\".\/LGBM_PRED_PREPROCESSED_NEW.csv\")","aa6f6fba":"df_lgbm.head()","dcab28f7":"arr=np.array(df_lgbm.iloc[:,3:])","9f0dd17d":"df_lgbm[\"pred_class\"]=arr.argmax(axis=1)+1","71d1c6ef":"df_lgbm.head()","e95ddb45":"from sklearn.metrics import accuracy_score\nprint(\"Accuracy of LGBM: \",end=\"\")\nprint(accuracy_score(df_lgbm['target']+1,df_lgbm[\"pred_class\"]))","676d2ed7":"df_xgb=pd.read_csv(\".\/XGBOOST_PREDICTIONS_PREPROCESSED_NEW.csv\")\ndf_xgb.head()","a0bab5c7":"arr=np.array(df_xgb.iloc[:,3:])\ndf_xgb[\"pred_class\"]=arr.argmax(axis=1)+1\nfrom sklearn.metrics import accuracy_score\nprint(\"Accuracy of XGB: \",end=\"\")\nprint(accuracy_score(df_xgb['target']+1,df_xgb[\"pred_class\"]))","278cad9c":"df_folds=pd.read_csv(\"..\/input\/preprocessedtrainingandtestingdata-new\/prprocessedTrainingAndTestingData\/tpsDecemberTrainingDataPreprocessed_new.csv\")","b0298da6":"df_xgb=pd.read_csv(\"..\/input\/xgboostpreprocesseddecember-new\/xgboostPreprocessedDecember_NEW\/XGBOOST_PREDICTIONS_PREPROCESSED_NEW.csv\")\ndf_lgbm=pd.read_csv(\"..\/input\/lgbmpreprocesseddecembertpsnew\/lgbmPreprocessedNew\/LGBM_PRED_PREPROCESSED_NEW.csv\")","40334f8f":"df_lgbm.head()","5d05247c":"df_xgb.head()","5952e9db":"df_lgbm2=df_lgbm.iloc[:,3:]","b15d65ec":"df_lgbm2.loc[:,'Id']=df_lgbm.loc[:,'Id']","c7379c02":"df_lgbm2","6e89cdcf":"df_xgb2=pd.merge(df_xgb,df_lgbm2,on='Id')","0ed11f15":"df_xgb2.shape,df_xgb.shape,df_lgbm.shape,df_lgbm2.shape","204fbc5b":"df_xgb2.head()","6b20abe8":"df_xgb3=df_xgb2.iloc[:,3:]\ndf_xgb3.loc[:,\"Id\"]=df_xgb2.loc[:,\"Id\"]\ndf_xgb3.head()","a7e53817":"df_folds1=pd.merge(df_folds,df_xgb3,on='Id')\ndf_folds1.shape,df_folds.shape,df_xgb3.shape","aa82e2a7":"df_folds1.to_csv(\"CB_FOLDS.csv\",index=False)","a6daefc8":"n_folds=2","724f0a0d":"import joblib\nimport catboost as cb","b687b0de":"df_folds1.columns","5c0b9572":"def run_folds(df,fold):\n    print(\"ENTER\")\n    df_train=df[df.kfold!=fold]\n    df_test=df[df.kfold==fold]\n    \n    clf= cb.CatBoostClassifier()\n    clf.fit(df_train.drop(['target','Id','kfold'],axis=1).values,df_train.target.values)\n    \n    y_pred = clf.predict_proba(df_test.drop(['target','Id','kfold'],axis=1).values)#[:,1]\n\n    \n    File_name = 'model_cbclf' + str(fold)\n    joblib.dump(\n    clf,File_name)\n    print(y_pred.shape)\n    df_test.loc[:,\"cbclf_pred1\"]=y_pred[:,0]\n    df_test.loc[:,\"cbclf_pred2\"]=y_pred[:,1]\n    df_test.loc[:,\"cbclf_pred3\"]=y_pred[:,2]\n    df_test.loc[:,\"cbclf_pred4\"]=y_pred[:,3]\n    df_test.loc[:,\"cbclf_pred5\"]=y_pred[:,4]#-y_pred[:,4]\n    df_test.loc[:,\"cbclf_pred6\"]=y_pred[:,5]\n    df_test.loc[:,\"cbclf_pred7\"]=y_pred[:,6]\n    \n    return df_test[['Id','target','kfold','cbclf_pred1','cbclf_pred2','cbclf_pred3','cbclf_pred4','cbclf_pred5','cbclf_pred6','cbclf_pred7']]\n\n\ndfs=[]\n    \nfor i in range(n_folds):\n    temp_df=run_folds(df_folds1,i)\n    dfs.append(temp_df)\ndf_pred=pd.concat(dfs)\n\nprint(df_pred.shape)","b7c721b6":"df_pred.to_csv(\"CB_PRED_NEW.csv\",index=False)","0642834f":"df_xgb2.to_csv(\"CB_PART_OF_TRAINING_DATA_NEW.csv\",index=False)","5d3c42a4":"df_test=pd.read_csv(\"..\/input\/preprocessedtrainingandtestingdata-new\/prprocessedTrainingAndTestingData\/tpsDecemberTestingDataPreprocessed_new.csv\")","e99de604":"import joblib\nxgb0=joblib.load(\"..\/input\/xgboostpreprocesseddecember-new\/xgboostPreprocessedDecember_NEW\/model_xgbclf0 (3)\")\nxgb1=joblib.load(\"..\/input\/xgboostpreprocesseddecember-new\/xgboostPreprocessedDecember_NEW\/model_xgbclf1 (4)\")\ny_pred0=xgb0.predict_proba(df_test.drop(columns=['Id']))\ny_pred1=xgb1.predict_proba(df_test.drop(columns=['Id']))\ny_pred_xgb=(y_pred0+y_pred1)\/2","79234655":"import joblib\nlgbm0=joblib.load(\"..\/input\/lgbmpreprocesseddecembertpsnew\/lgbmPreprocessedNew\/model_lgbmclf0 (3)\")\nlgbm1=joblib.load(\"..\/input\/lgbmpreprocesseddecembertpsnew\/lgbmPreprocessedNew\/model_lgbmclf1 (3)\")\ny_pred0=lgbm0.predict_proba(df_test.drop(columns=['Id']))\ny_pred1=lgbm1.predict_proba(df_test.drop(columns=['Id']))\ny_pred=(y_pred0+y_pred1)\/2","0559d999":"df_test.loc[:,\"xgbclf_pred1\"]=y_pred_xgb[:,0]\ndf_test.loc[:,\"xgbclf_pred2\"]=y_pred_xgb[:,1]\ndf_test.loc[:,\"xgbclf_pred3\"]=y_pred_xgb[:,2]\ndf_test.loc[:,\"xgbclf_pred4\"]=y_pred_xgb[:,3]\ndf_test.loc[:,\"xgbclf_pred5\"]=y_pred_xgb[:,4]#-y_pred_xgb[:,4]\ndf_test.loc[:,\"xgbclf_pred6\"]=y_pred_xgb[:,5]\ndf_test.loc[:,\"xgbclf_pred7\"]=y_pred_xgb[:,6]","bd8ee8ff":"df_test.loc[:,\"lgbmclf_pred1\"]=y_pred[:,0]\ndf_test.loc[:,\"lgbmclf_pred2\"]=y_pred[:,1]\ndf_test.loc[:,\"lgbmclf_pred3\"]=y_pred[:,2]\ndf_test.loc[:,\"lgbmclf_pred4\"]=y_pred[:,3]\ndf_test.loc[:,\"lgbmclf_pred5\"]=y_pred[:,4]#-y_pred[:,4]\ndf_test.loc[:,\"lgbmclf_pred6\"]=y_pred[:,5]\ndf_test.loc[:,\"lgbmclf_pred7\"]=y_pred[:,6]","3685d820":"df_test.head()","2da8aa7f":"df_test.columns","e90ea19a":"df_test.to_csv(\"pass_through_catboost_preprocessed_NEW.csv\",index=False)","b054dfd2":"import joblib\ncb0=joblib.load(\"..\/input\/catboost-model-december-new\/cbmodelDecemberNew\/model_cbclf0\")\ncb1=joblib.load(\"..\/input\/catboost-model-december-new\/cbmodelDecemberNew\/model_cbclf1\")\ny_pred0=cb0.predict_proba(df_test.drop(columns=['Id']))\ny_pred1=cb1.predict_proba(df_test.drop(columns=['Id']))\ny_pred_cb=(y_pred0+y_pred1)\/2","59fe6a65":"clas=y_pred_cb.argmax(axis=1)+1","4767b1f3":"sub=pd.read_csv(\"..\/input\/tabular-playground-series-dec-2021\/sample_submission.csv\")","cfeed55e":"sub['Cover_Type']=clas","d337fbfd":"sub.to_csv(\"STACKED_SUBMISSION_NEW.csv\",index=False)","13b758af":"y_PRED=(y_pred_cb+y_pred_xgb+y_pred)\/3\nclas=y_PRED.argmax(axis=1)+1\nsub['Cover_Type']=clas\nsub.to_csv(\"STACKED_AND_ENSEMBLED3_SUBMISSION_NEW.csv\",index=False)","be7a5793":"y_PRED=(y_pred_cb+y_pred_xgb)\/2\nclas=y_PRED.argmax(axis=1)+1\nsub['Cover_Type']=clas\nsub.to_csv(\"STACKED_AND_ENSEMBLED2_SUBMISSION_NEW.csv\",index=False)  #This is best score ","27281b46":"### FINALLY MAKING PREDICTIONS","105e6536":"CHECKING ACCURACY OF LGBM","385ae3cc":"### TRAINGING CATBOOST BY STACKING LGBM AND XGBOOST","ba870055":"PREPROCESSNG BROUGHT FROM BIBHABASU MOHAPATRA https:\/\/www.kaggle.com\/bibhabasumohapatra\/multiclass-using-predict-proba-ensemble-part-2","36e44a0c":"TRAINING XGBOOST","dccfeac4":"LGBM TRAINING","8bf341f4":"CHECKIN ACCURACY OF XGBOOST"}}