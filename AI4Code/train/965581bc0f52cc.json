{"cell_type":{"4a15ddf2":"code","533a329c":"code","67f9f18a":"code","6d59ec70":"code","95ac589a":"code","7cefaefd":"code","b02749fa":"code","cdd6872e":"code","acfb9fa7":"code","6f312fe1":"code","2e0b5909":"code","1f33fe92":"code","7a18c111":"code","c313e21c":"code","1fb41351":"code","7f474adc":"code","030c05c2":"code","88a1dcdd":"code","3df1197e":"code","6877d3fc":"code","eb019727":"code","d73f7b62":"code","a9d83523":"code","62ad9c2f":"code","95008760":"code","1b841d03":"code","cace752f":"code","526a6b38":"code","83238186":"code","0357988d":"code","b333e4da":"code","dc62d2e4":"code","59cfaa22":"code","80020fb2":"code","7882b0c1":"code","0e8391f9":"code","df570118":"code","cee3f1e1":"code","098ccfe8":"code","bfc7203b":"code","69033999":"code","f5b086ee":"code","e3c35222":"code","00e12e52":"code","54ba77c2":"code","a5ab2584":"code","1c90e937":"code","d0ae19aa":"code","b5286b3b":"code","83bde240":"markdown","28359a66":"markdown","d77d538f":"markdown","46c7f8b7":"markdown","1e463bd0":"markdown","3d788b33":"markdown","12dc84bc":"markdown","59350adc":"markdown","4b813a44":"markdown","60e3ac08":"markdown","2cd829e6":"markdown"},"source":{"4a15ddf2":"# Import Library \nimport nltk","533a329c":"# Now download necessary text files \n# First press d then 'all'\n# After completing download press 'q' to exit \n\n# nltk.download()","67f9f18a":"# Now import all books what we have downloaded \nfrom nltk.book import * ","6d59ec70":"# Now check all books \ntext1\n\n# It will only shows the book author and published year ","95ac589a":"# What we neeed\n\n# urllib\n# urlopen()\n# read()\n\nfrom urllib.request import urlopen\nurl = \"https:\/\/www.thedailystar.net\/business\/news\/can-bangladesh-challenge-vietnam-japans-ict-market-1977097\"\nraw = urlopen(url).read()","7cefaefd":"# Return xml text \nraw","b02749fa":"type(raw)","cdd6872e":"len(raw)","acfb9fa7":"raw_data = raw.decode('utf-8')","6f312fe1":"len(raw_data)","2e0b5909":"type(raw)","1f33fe92":"tokens = nltk.word_tokenize(data)","7a18c111":"# Concord Functions Test \ntext2.concordance('love')\n\n# It will shows all context which has word 'love'","c313e21c":"## Similer Test \ntext2.similar('daughter')\n## Shows all similer word to 'daughter'","1fb41351":"## common_context text\ntext2.common_contexts([\"father\", \"mother\"])","7f474adc":"# Okay now  read a text file called news \n# A text file downloaded into .txt files \ndata = open('..\/input\/newsbd\/news.txt', encoding='latin2').read()","030c05c2":"data","88a1dcdd":"type(data)","3df1197e":"# now create tokenization \ntokens = nltk.word_tokenize(data)","6877d3fc":"type(tokens)","eb019727":"# After applying it every words even space were seperated with a cotetation. \ntokens","d73f7b62":"# Here some of the words has upper and lower case. For better understanding lets make all upper case word into lower case word \nwords = [w.lower() for w in tokens]","a9d83523":"# All word are now lower case \nwords","62ad9c2f":"# Okay fine, now sorting the text \n# But there could be duplicated words. So, at first remove thoese duplicated words\n\nprint('Without removing duplicated words total length = ', len(words))\n\nvocabs = sorted(set(words))\nprint('After removing duplicated words total length = ', len(vocabs))","95008760":"import re ","1b841d03":"# Corpus is a collection of text \n# Create a word list fro corpus data \nwordlist = [w for w in nltk.corpus.words.words('en') if w.islower()]\n\n# Search data in the list end with a\n[w for w in wordlist if re.search('a$', w)]","cace752f":"# PorterStammer() : Remove extra words \n# LancasterStammer(): Correct spelling ","526a6b38":"row_data = open('..\/input\/newsbd\/news.txt', encoding='latin2').read()","83238186":"row_data","0357988d":"tokens = nltk.word_tokenize(row_data)","b333e4da":"# Now Creating stammer \np = nltk.PorterStemmer() \nl = nltk.LancasterStemmer()","dc62d2e4":"[p.stem(t) for t in tokens]","59cfaa22":"[l.stem(t) for t in tokens]","80020fb2":"# It will not removing any fefx (last extension of the words) and each words has a meaning\nle = nltk.WordNetLemmatizer()\n[le.lemmatize(t) for t in tokens]","7882b0c1":"# load segmenter \nsen_segmenter = nltk.data.load('tokenizers\/punkt\/english.pickle')","0e8391f9":"# load data from corpus \ntext = nltk.corpus.gutenberg.raw('chesterton-thursday.txt')","df570118":"# Find sentences \nfrom nltk.tokenize import sent_tokenize\nsentences = sent_tokenize(text)","cee3f1e1":"sentences[171:181]","098ccfe8":"def segment(text, segs):\n    words = []\n    last = 0\n    \n    for i in range(len(segs)):\n        if segs[i] == '1':\n            words.append(text[last: i+1])\n            last = i+1\n    words.append(text[last:])\n    return words","bfc7203b":"# Create words and their segmenter \ntext = 'HelloThere.IamHappytoseeyou.'\nseg1 = '0000000000100000000000000001'\nseg2 = '000010000011010000101001001'","69033999":"# apply functions\nsegment(text, seg1)","f5b086ee":"segment(text, seg2)","e3c35222":"text = \"\"\" The most important source of texts is undoubtedly the Web.\"\"\"","00e12e52":"sents = sent_tokenize(text)","54ba77c2":"sents","a5ab2584":"# word tokenization \nsents = [nltk.word_tokenize(sents) for sents in sents]","1c90e937":"sents","d0ae19aa":"sents = [nltk.pos_tag(sents) for sents in sents]","b5286b3b":"sents","83bde240":"# Text accessing from web (TokeNization)","28359a66":"# Lemmatisation \nIt is a process to groping togeher the different infected words in a forms of word so that it can be analyzed as a single word.","d77d538f":"# Segmentation ","46c7f8b7":"# What we need \n1. NLTK module\n2. Numpy module\n3. Collections : Books, Corpora etc","1e463bd0":"# Information Extraction ","3d788b33":"# Regular Expressions","12dc84bc":"### **HTML -> ASCII -> TEXT -> VOCAB(Dictionary)**\n\nThat means at first we download a web page from URL then we do tokenization(seperate all words) and finally createing dictionary. ","59350adc":"# NLP Pipeline","4b813a44":"# word segmentation","60e3ac08":"# Stamming\nIt is a process to keep only base word from ful word. Like if 'fighting' is a word it will remove ing from it and inly keep 'Fight'","2cd829e6":"# NLTK Features\n## **concordance:**It show every occurrence of a given word together with some context. \n## **similar:** What other words appear in a similer range of contexts?\n## **common_contexs:** Examine the words that are shared by two or more words. "}}