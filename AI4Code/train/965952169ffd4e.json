{"cell_type":{"cffa55c6":"code","b6154bce":"code","1e525703":"code","f7e852b9":"code","9b209ce3":"code","17c5a55d":"code","b20cdaf6":"markdown","2e67a46c":"markdown","25abe0f0":"markdown"},"source":{"cffa55c6":"import pandas as pd\nimport numpy as np\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt","b6154bce":"def get_log_pred(data):\n    # Need more features!!! Note that if we use\n    features = ['f190486d6', '58e2e02e6', 'eeb9cd3aa', '9fd594eec', '6eef030c1', '15ace8c9f', 'fb0f5dbfe', '58e056e12', '20aa07010', '024c577b9', 'd6bb78916', 'b43a7cfd5', '58232a6fb']\n    d1 = data[features[:-2]].apply(tuple, axis=1).to_frame().rename(columns={0: 'key'})\n    d2 = data[features[2:]].apply(tuple, axis=1).to_frame().rename(columns={0: 'key'})\n    d2['pred'] = data[features[0]]\n    d2 = d2[d2['pred'] != 0] # Keep?\n    d3 = d2[~d2.duplicated(['key'], keep='first')] # Need more features!\n    d = d1.merge(d3, how='left', on='key')\n    return np.log1p(d.pred).fillna(0)","1e525703":"train = pd.read_csv('..\/input\/train.csv')","f7e852b9":"log_pred = get_log_pred(train)\nhave_data = log_pred != 0\nprint(f'Score = {sqrt(mean_squared_error(np.log1p(train.target[have_data]), log_pred[have_data]))} on {have_data.sum()} out of {train.shape[0]} training samples')","9b209ce3":"test = pd.read_csv('..\/input\/test.csv')","17c5a55d":"log_pred = get_log_pred(test)\nhave_data = log_pred != 0\nprint(f'Have predictions for {have_data.sum()} out of {test.shape[0]} test samples')","b20cdaf6":"This code uses only Giba's original features.  Note that the more features we have - the better the result.","2e67a46c":"The goal of this kernel is to show simple code to exploit Giba's features.  This is not a complete solution.  Nor have I checked the results.","25abe0f0":"But are the predictions any good for the private test set? :)"}}