{"cell_type":{"a8c555ba":"code","81f160d4":"code","53a4f89c":"code","4dec9f90":"code","58c706b6":"code","9149c9db":"code","970bccea":"markdown","d2d05901":"markdown","9b1e8232":"markdown"},"source":{"a8c555ba":"# import standard libraries\nimport pandas as pd\nimport numpy as np\n\n# import spacy for NLP and re for regular expressions\nimport spacy\nfrom spacy.lang.en.stop_words import STOP_WORDS\nimport re\n\n# import sklearn transformers, models and pipelines\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import cross_val_score, GridSearchCV\n\n# Load the small language model from spacy\nnlp = spacy.load('en_core_web_sm')\n\n# set pandas text output to 400\npd.options.display.max_colwidth = 400","81f160d4":"# load data\ntrain = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\nsample_submission = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/sample_submission.csv')\n\n# print shape of datasets\nprint('Train shape: {}'.format(train.shape))\nprint('Test shape: {}'.format(test.shape))\nprint('Sample submission shape: {}'.format(sample_submission.shape))\n\n# inspect train set\ntrain.head()","53a4f89c":"# Load the en_core_web_lg model\nnlp = spacy.load('en_core_web_lg', disable=[\"tagger\", \"parser\", \"ner\"])\n\n# create train set by getting the document vector\ndocs_train = [nlp(doc).vector for doc in train.text]\nX_train = np.vstack(docs_train)\nprint('Shape of train set: {}'.format(X_train.shape))\n\n# create test set likewise\ndocs_test = [nlp(doc).vector for doc in test.text]\nX_test = np.vstack(docs_test)\nprint('Shape of test set: {}'.format(X_test.shape))\n\n# create target\ny_train = train.target.copy()","4dec9f90":"# create machine learning pipeline\nword2vec_pipe = Pipeline([('estimator', LogisticRegression())])\n\n# cross validate\nprint('F1 score: {:.3f}'.format(np.mean(cross_val_score(word2vec_pipe, X_train, y_train, scoring = 'f1'))))\n\n# fit pipeline\nword2vec_pipe.fit(X_train, y_train)\n\n# predict on test set\npred = word2vec_pipe.predict(X_test)\n\n# submit prediction\nsample_submission.target = pred\nsample_submission.to_csv('word2vec_baseline.csv', index = False)","58c706b6":"# create a parameter grid\nparam_grid = [{'estimator' : [LogisticRegression()], \n               'estimator__C' : np.logspace(-3, 3, 7)},\n              {'estimator' : [SVC()], \n               'estimator__C' : np.logspace(-1, 1, 3), \n               'estimator__gamma' : np.logspace(-2, 2, 5) \/ X_train.shape[0]}]\n\n# create a RandomizedSearchCV object\nword2vec_grid_search = GridSearchCV(\n    estimator = word2vec_pipe,\n    param_grid = param_grid,\n    scoring = 'f1',\n    n_jobs = -1,\n    refit = True,\n    verbose = 1,\n    return_train_score = True\n)\n\n# fit RandomizedSearchCV object\nword2vec_grid_search.fit(X_train, y_train)\n\n# print grid search results\ncols = ['param_estimator',\n        'param_estimator__C',\n        'param_estimator__gamma',\n        'mean_test_score',\n        'mean_train_score']\n\npd.options.display.max_colwidth = 50\n\nword2vec_grid_search_results = pd.DataFrame(word2vec_grid_search.cv_results_).sort_values(by = 'mean_test_score', \n                                                                                          ascending = False)\nword2vec_grid_search_results[cols].head(10)","9149c9db":"# predict on test set with the best model from the randomized search\npred = word2vec_grid_search.predict(X_test)\n\n# submit prediction\nsample_submission.target = pred\nsample_submission.to_csv('word2vec_tuned.csv', index = False)","970bccea":"## Load and Prepare Data\nThere are duplicate rows. Some have the same text and target, while others only have the same text but different target. Here, I do not remove them because after removing them model performance gets worse. This behavior is unintuitive, especially since there are tweets with the same text but different labels. \n\nAfter that, I create a word embedding for each document using Word2Vec. Word2Vec creates a dense representation for each word, such that words appearing in similar contexts have similar vectors. To get an embedding for the entire tweet, the mean of all vectors for the words in the tweet are taken. The assumption now is that similar tweets have similar vectors. ","d2d05901":"## Hyperparameter Tuning\nAfter creating the baseline, I now want to test if a more complex model works better than the logistic regression. I chose a kernel SVM in this case, as SVM models are one of the classical machine learning models commonly used for text classification. \n\nI tune the regularization parameter C for both the logistic regression and SVM and the gamma parameter for the SVM. The hyperparameters influence the model complexity, with more complex models having a higher chance of overfitting. In case of the SVM, a more complex model can even find decision boundaries which are considered non-linear in the original feature space. ","9b1e8232":"## Create Machine Learning Pipeline\nThe first step is creating a machine learning pipeline using the `Pipeline` class from scikit-learn. Creating a pipeline is important to have a robust workflow. For example, it ensures that all preprocessing steps that are learned on data are done within the cross-validation, to ensure that no data is leaked to the model.\n\nIn this case, it doesn't add much value since the only step in the pipeline is an estimator (here a logistic regression). However, since it's useful for pipelines with data preprocessing steps that are learned on data, such standard scaling, I even do it when it's not required. \n\nOne advantage even when just using an estimator is that I can treat the estimator like a hyperparameter in the grid search. "}}