{"cell_type":{"4a355c8e":"code","b71f0c17":"code","06ecd0ae":"code","bc403643":"code","7588e938":"code","c79db937":"code","122aed49":"code","25cb2132":"code","21707f86":"code","014a8caf":"code","6b496532":"code","2e2fb7f7":"code","a8497b41":"code","930331d9":"code","99785634":"code","179ce768":"code","a79b9e39":"code","cb46396f":"code","32b53a05":"code","d9e33c34":"code","3e6a0785":"code","43eaa6f2":"code","7f71cbb7":"code","546080b2":"code","c24cb72d":"code","e55d1640":"code","267e04a6":"code","497f4b3b":"code","95b094fb":"code","aa2ca5e4":"code","73367586":"code","458a8d31":"code","8d9cbfe7":"code","378d3843":"code","79f62af3":"code","b8e6bfa8":"code","ba7c750e":"code","268bedc1":"code","23c69a57":"code","9dd0d9f5":"code","f643e648":"code","d3566eeb":"code","b897070e":"code","8a02cd0c":"code","ebbdc44f":"code","1b128629":"code","1e1c98d7":"markdown","e7c0914d":"markdown","2443f5eb":"markdown","85156658":"markdown","7f773036":"markdown","78aaa92d":"markdown","7b12ff6d":"markdown","961ed9a5":"markdown","a4013b69":"markdown","dd08c40e":"markdown","b6c448a5":"markdown","bd07bc99":"markdown","1cf771bf":"markdown","f43de612":"markdown","f4252b56":"markdown","197f6446":"markdown","0932c59e":"markdown","9d79973b":"markdown","d8f978a6":"markdown","1e4b79e1":"markdown","b3d7d91e":"markdown","5570947f":"markdown","3265143e":"markdown","20c8434b":"markdown","ac729f40":"markdown","411f323d":"markdown","77fbe549":"markdown"},"source":{"4a355c8e":"# Import necessary libraries\nimport numpy as np\nimport pandas as pd\nimport datatable\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\nfrom sklearn import tree\nimport graphviz\nimport shap\nimport gc\nimport warnings\n\nwarnings.filterwarnings('ignore')\nsns.set_style('whitegrid')\nSEED = 2222\nnp.random.seed(SEED)","b71f0c17":"train_path = '..\/input\/jane-street-market-prediction\/train.csv'\n\n# use datatable to load big data file\ntrain_file = datatable.fread(train_path).to_pandas()\ntrain_file.info()","06ecd0ae":"# It is found from info() that there are only two datatypes - float64 and int32\n# Reduce memeory usage by adopting suitable datatypes\nfor c in train_file.columns:\n    min_val, max_val = train_file[c].min(), train_file[c].max()\n    if train_file[c].dtype == 'float64':\n        if min_val>np.finfo(np.float16).min and max_val<np.finfo(np.float16).max:\n            train_file[c] = train_file[c].astype(np.float16)\n        elif min_val>np.finfo(np.float32).min and max_val<np.finfo(np.float32).max:\n            train_file[c] = train_file[c].astype(np.float32)\n    elif train_file[c].dtype == 'int32':\n        if min_val>np.iinfo(np.int8).min and max_val<np.iinfo(np.int8).max:\n            train_file[c] = train_file[c].astype(np.int8)\n        elif min_val>np.iinfo(np.int16).min and max_val<np.iinfo(np.int16).max:\n            train_file[c] = train_file[c].astype(np.int16)\ntrain_file.info()","bc403643":"print('There are %s NAN values in the train data'%train_file.isnull().sum().sum())","7588e938":"features = [f'feature_{i}' for i in range(130)]\nval_range = train_file[features].max()-train_file[features].min()\nfiller = pd.Series(train_file[features].min()-0.01*val_range, index=features)\n# This filler value will be used as a constant replacement of missing values \n\n\n# A function to fill all missing values with negative outliers as discussed in the referred notebook\n# https:\/\/www.kaggle.com\/rajkumarl\/jane-tf-keras-lstm\ndef fill_missing(df):\n    df[features] = df[features].fillna(filler)\n    return df  \n\ntrain = fill_missing(train_file)\n\ntrain.info()","c79db937":"filler.plot(figsize=(20,5),kind='bar',rot=90, color='green')\nplt.show()","122aed49":"print(\"Now we have %d missing values in our data\" %train.isnull().sum().sum())","25cb2132":"# Let's define our target\n# Take mean of resp, resp_1, resp_2, resp_3, resp_4\n# Create target named 'action' which takes value of 1 if the above mean is higher than 0.5, else 0 \ny = train[[c for c in train.columns if 'resp' in c]]\ny = (y>0)*1\ntrain['action'] = (y.mean(axis=1)>0.5).astype(np.int8)","21707f86":"# save memory by deleting useless variables\ndel val_range\ndel y\ngc.collect()","014a8caf":"day_242 = train.query('date==242')","6b496532":"# Generate Features using Linear shifting, Natural Logarithm and Square Root\nfor f in [f'feature_{i}' for i in range(1,130)]: # linear shifting to value above 1.0\n    day_242['pos_'+str(f)] = (day_242[f]+abs(train[f].min())+1).astype(np.float16)\nfor f in [f'feature_{i}' for i in range(1,130)]: # Natural log of all the values\n    day_242['log_'+str(f)] = np.log(day_242['pos_'+str(f)]).astype(np.float16)\nfor f in [f'feature_{i}' for i in range(1,130)]: # Square root of all the values\n    day_242['sqrt_'+str(f)] = np.sqrt(day_242['pos_'+str(f)]).astype(np.float16)\nday_242.info()","2e2fb7f7":"# Linearly shifted values are used for log and sqrt transformations\n# However they are useless since we have our original values which are 100% correlated\n# Let's drop them from our data\nday_242.drop([f'pos_feature_{i}' for i in range(1,130)], inplace=True, axis=1)\nday_242.info()","a8497b41":"# Find Correlation among features and remove highly correlated features\ncorr = day_242.corr(method='pearson').abs().unstack().sort_values(kind='quicksort', ascending=False).reset_index()\ncorr.rename(columns={'level_0':'feature_A', 'level_1':'feature_B', 0:'Corr_Coeff'}, inplace=True)\ncorr = corr[corr['Corr_Coeff']<=0.8]\ncorr.dropna(inplace=True)","930331d9":"# Let's have a look at correlation table\ncorr.head()","99785634":"# Which features correlate more with the target?\ncorr[corr['feature_A']=='action'].head(10)","179ce768":"# Visualize correlation coefficients of features with respect to our target\ncorr[corr['feature_A']=='action'].iloc[5:40].plot(x='feature_B', y='Corr_Coeff', rot=90, figsize=(20,5), kind='bar', colormap='viridis')\nplt.show()","a79b9e39":"gen_features = [f for f in day_242.columns if 'feature' in f]\nlen(gen_features)","cb46396f":"train_X, train_y = day_242[gen_features], day_242['action']\nmodel = RandomForestClassifier(max_features='auto', random_state=SEED).fit(train_X, train_y)","32b53a05":"# How does the classification take place in regard of the first tree in Random Forest?\ngraph = tree.export_graphviz(model.estimators_[0], \n                             out_file=None, \n                             feature_names=gen_features, \n                             rounded=True, \n                             filled=True, \n                             precision=2)\ngraphviz.Source(graph)","d9e33c34":"# How does the classification take place in regard of the second tree in Random Forest?\ngraph = tree.export_graphviz(model.estimators_[1], \n                             out_file=None, \n                             feature_names=gen_features, \n                             rounded=True, \n                             filled=True, \n                             precision=2)\ngraphviz.Source(graph)","3e6a0785":"# SHAP values are popular in finding hidden patterns among features\n# define an shap value explainer\nexplainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(train_X)\n# Initialize Javascript Visualization\nprint('Initializing JavaScript visualization')\nshap.initjs() ","43eaa6f2":"for i in range(1,20):\n    shap.dependence_plot(f'feature_{i}', shap_values[1], train_X)","7f71cbb7":"for i in range(20,40):\n    shap.dependence_plot(f'feature_{i}', shap_values[1], train_X)","546080b2":"for i in range(40,60):\n    shap.dependence_plot(f'feature_{i}', shap_values[1], train_X)","c24cb72d":"for i in range(60,80):\n    shap.dependence_plot(f'feature_{i}', shap_values[1], train_X)","e55d1640":"for i in range(80,100):\n    shap.dependence_plot(f'feature_{i}', shap_values[1], train_X)","267e04a6":"for i in range(100,120):\n    shap.dependence_plot(f'feature_{i}', shap_values[1], train_X)","497f4b3b":"for i in range(120,130):\n    shap.dependence_plot(f'feature_{i}', shap_values[1], train_X)","95b094fb":"# Have a copy of dataframe to experiment\nsam = day_242.copy()","aa2ca5e4":"print(sam.feature_0.value_counts())\n# feature_0 is a categorical variable\n# Make it OneHot\nsam['feature_0']=((sam['feature_0']+1)\/2).astype(np.int8)\nprint(\"\\nAfter One Hot...\\n\", sam.feature_0.value_counts())","73367586":"# From the Shap Dependence plots above, the following features seem to have cubic relationship with target\ncubic = [37, 39, 67, 68, 89, 98, 99, 118, 119, 121, 124, 125, 127]\nfor i in cubic:\n    f = f'feature_{i}'\n    threes = np.full((len(sam[f])), 3)\n    sam['cub_'+f] =np.power(sam[f], threes) ","458a8d31":"# From the Shap Dependence plots above, the following features seem to have quadratic relationship with target\nquad = [6, 37, 39, 40, 53, 60, 61, 62, 63, 64, 67, 68, 89, 98, 99, 101, 113, 116, 118, 119, 121, 123, 124, 125, 127]\nfor i in quad:\n    f = f'feature_{i}'\n    sam['quad_'+f] =np.square(sam[f]) ","8d9cbfe7":"# Test the correlation of newly generated features with respect to target \nnew = sam.corr(method='pearson').abs().unstack().sort_values(kind='quicksort', ascending=False).reset_index()\nnew.rename(columns={0:'coeff'}, inplace=True)\nnew = new[new['coeff']<0.8]\nnew.dropna(inplace=True)","378d3843":"# Test cubic features\nnew[new['level_0'].str.contains('action')][new['level_1'].str.contains('cub')].head()","79f62af3":"# Test a random feature\nnew[new['level_0'].str.contains('action')][new['level_1'].str.contains('67')].head(6)","b8e6bfa8":"# features that can be added together or subtracted\nadd_pairs = [(3,6), (15,26), (19,26), (30,37), (34,33), (35,39),(94,65), (101,4)]\nfor i,j in add_pairs:\n    sam[f'add_{i}_{j}'] = sam[f'feature_{i}']+sam[f'feature_{j}']\n    sam[f'sub_{i}_{j}'] = sam[f'feature_{i}']-sam[f'feature_{j}']\n\nadd_log_pairs = [(9,20), (22,37), (28,39), (29,25), (65,91), (74,103),(99,126), (109,7), (111,87), (112,97), (118,112)]\nfor i,j in add_log_pairs:\n    sam[f'add_{i}_log{j}'] = sam[f'feature_{i}']+sam[f'log_feature_{j}']\n    sam[f'sub_{i}_log{j}'] = sam[f'feature_{i}']-sam[f'log_feature_{j}']","ba7c750e":"# features that can be multiplied together\nmul_pairs = [(5,42), (12,66), (37,45), (39,95), (122,35)]\nfor i,j in mul_pairs:\n    sam[f'mul_{i}_{j}'] = sam[f'feature_{i}']*sam[f'feature_{j}']\n\nmul_log_pairs = [(5,42), (6,42), (11,99), (21,42), (81,66), (98,20), (122,35)]\nfor i,j in mul_log_pairs:\n    sam[f'mul_{i}_log{j}'] = sam[f'feature_{i}']*sam[f'log_feature_{j}']","268bedc1":"# Test the correlation of newly generated features with respect to target \nn = sam.corr(method='pearson').abs().unstack().sort_values(kind='quicksort', ascending=False).reset_index()\nn.rename(columns={0:'coeff'}, inplace=True)\nn = n[n['coeff']<0.9]\nn.dropna(inplace=True)","23c69a57":"# Test latest features by plotting their correlation\ncorr_action = n[n['level_0']=='action'].iloc[5:40, :].reset_index()\nax = corr_action.plot(y='coeff', kind='bar', xticks=np.arange(35), rot=90, figsize=(16,5))\nax.set_xticklabels(corr_action['level_1'])\nplt.show()","9dd0d9f5":"print('Feature Generation completed')","f643e648":"def feature_transforms(df):\n    # Generate Features using Linear shifting, Natural Logarithm and Square Root\n    for f in [f'feature_{i}' for i in range(1,130)]: \n        # linear shifting to value above 1.0\n        df['pos_'+str(f)] = (df[f]+abs(train[f].min())+1).astype(np.float16)\n    for f in [f'feature_{i}' for i in range(1,130)]: \n        # Natural log of all the values\n        df['log_'+str(f)] = np.log(df['pos_'+str(f)]).astype(np.float16)\n    for f in [f'feature_{i}' for i in range(1,130)]: \n        # Square root of all the values\n        df['sqrt_'+str(f)] = np.sqrt(df['pos_'+str(f)]).astype(np.float16)\n    \n    # Linearly shifted values are used for log and sqrt transformations\n    # However they are useless since we have our original values which are 100% correlated\n    # Let's drop them from our data\n    df.drop([f'pos_feature_{i}' for i in range(1,130)], inplace=True, axis=1)\n    \n    # From the Shap Dependence plots, the following features seem to have cubic relationship with target\n    cubic = [37, 39, 67, 68, 89, 98, 99, 118, 119, 121, 124, 125, 127]\n    for i in cubic:\n        f = f'feature_{i}'\n        threes = np.full((len(df[f])), 3)\n        df['cub_'+f] =np.power(df[f], threes) \n        \n    # From the Shap Dependence plots, the following features seem to have quadratic relationship with target\n    quad = [6, 37, 39, 40, 53, 60, 61, 62, 63, 64, 67, 68, 89, 98, 99, 101, 113, 116, 118, 119, 121, 123, 124, 125, 127]\n    for i in quad:\n        f = f'feature_{i}'\n        df['quad_'+f] =np.square(df[f]) \n    \n    return df","d3566eeb":"def manipulate_pairs(df):\n    # features that can be added together or subtracted\n    add_pairs = [(3,6), (15,26), (19,26), (30,37), (34,33), (35,39),(94,65), (101,4)]\n    for i,j in add_pairs:\n        df[f'add_{i}_{j}'] = df[f'feature_{i}']+df[f'feature_{j}']\n        df[f'sub_{i}_{j}'] = df[f'feature_{i}']-df[f'feature_{j}']\n\n    add_log_pairs = [(9,20), (22,37), (28,39), (29,25), (65,91), (74,103),(99,126), (109,7), (111,87), (112,97), (118,112)]\n    for i,j in add_log_pairs:\n        df[f'add_{i}_log{j}'] = df[f'feature_{i}']+df[f'log_feature_{j}']\n        df[f'sub_{i}_log{j}'] = df[f'feature_{i}']-df[f'log_feature_{j}']\n    # features that can be multiplied together\n    mul_pairs = [(5,42), (12,66), (37,45), (39,95), (122,35)]\n    for i,j in mul_pairs:\n        df[f'mul_{i}_{j}'] = df[f'feature_{i}']*df[f'feature_{j}']\n\n    mul_log_pairs = [(5,42), (6,42), (11,99), (21,42), (81,66), (98,20), (122,35)]\n    for i,j in mul_log_pairs:\n        df[f'mul_{i}_log{j}'] = df[f'feature_{i}']*df[f'log_feature_{j}']\n    return df","b897070e":"# Obtain data from day 201 to 300\ndf = train.query('date>200')\ndf = df.query('date<=300')","8a02cd0c":"# Perform feature generation\ndf = feature_transforms(df)\ndf = manipulate_pairs(df)","ebbdc44f":"# Number of features we have now\nlatest_features = df.columns.drop(['action','resp','resp_1','resp_2','resp_3','resp_4'])\nlen(latest_features)","1b128629":"# Feature selection\n# Select 100 features (100 is random, can be varied)\nselector = SelectKBest(f_classif, k=100)\ntemp = selector.fit_transform(df[latest_features], df['action'])\ndf_new = pd.DataFrame(selector.inverse_transform(temp), index=df.index, columns=latest_features)\nselected_features = df_new.columns[df_new.var() != 0]\nprint(selected_features)","1e1c98d7":"## Create features by manipulating two different features by closely observing patterns in SHAP plots","e7c0914d":"### We cannot handle the whole data during exploration. Rather let's use a small representative part. \n### [This notebook](https:\/\/www.kaggle.com\/rajkumarl\/jane-tf-keras-lstm) suggests that Day_242 is one among the days which have most probable number of opportunities per day. ","2443f5eb":"### Insights: features 20 to 39\n#### Features 21, 25, 26, 27, 28, 37, 38 and 39 have good effects on predicting target\n#### Features and Interacting features can be (21, log 42), (22, log 37), (28, log 39), (37, 45), (39, 95)","85156658":"### Insights: features 40 to 59\n#### Features 40, 44, 45, 53, 54, 55, 57, and 58 have good effects on predicting target\n#### Features do not seem to have remarkable interactions among them","7f773036":"### It can be visualized that a lot many generated features show great correlation with target than their original counterparts\n\n### For instance, feature `sub_3_6` is found to be more correlated to target than individual features `feature_3` or `feature_6`","78aaa92d":"# 4. FEATURE ENGINEERING","7b12ff6d":"# 3. HANDLING MISSING VALUES","961ed9a5":"# 5. ANALYZING INTERACTION OF FEATURES","a4013b69":"## Transforming possible features to higher order forms","dd08c40e":"### Thank you for your time!","b6c448a5":"### Insights: features 60 to 79\n#### Features 60, 61, 62, 63, 64, 65, 66,67, 68, 69, and 71 have good effects on predicting target\n#### Features and Interacting features can be (65, log 91), (66, 0), (74, log 103)","bd07bc99":"# 1. CREATE ENVIRONMENT","1cf771bf":"### Visualize Features and their interaction effects with SHAP values","f43de612":"# 2. LOAD DATA AND OPTIMIZE MEMORY","f4252b56":"# 6. FEATURE SELECTION","197f6446":"### Insights: features 80 to 99\n#### Features 80, 81, 82, 84, 86, 87, 89, 90, 94, 98, and 99 have good effects on predicting target\n#### Features and Interacting features can be (81, log 66), (88, sqrt 29), (92, sqrt 95), (94, 65), (96, log 67), (98, log 20), (99, log 126)","0932c59e":"### Develop a Random Forest Classifer to analyze features further","9d79973b":"### That's a great reduction in memory usage (around 74% reduction)! It will help us go further efficiently!","d8f978a6":"### Insights: features 100 to 119\n#### Features 101, 103, 104, 107, 110, 113, 114, 115, 116, 118, and 119 have good effects on predicting target\n#### Features and Interacting features can be (101, 4), (109, log 7), (111, log 87), (112, log 97), (114, 40), (118, log 112)","1e4b79e1":"### Insights: features 120 to 129\n#### Features 121, 123, 124, 125, 127, and 129 have good effects on predicting target\n#### Features and Interacting features can be (122, 35)","b3d7d91e":"### Generate Features using Natural Logarithm and Square Root. Each feature has negative values, hence shift all values linearly to make them all non_zero positive, because logarithm and square root cannot be applied on negative numbers (yields NAN)","5570947f":"### We generated a lot of features. Since our dataset is in Gigabytes, it is mandatory that we have to cut useless features down. At the same time, we have to use a little larger portion of dataset to make feature selection.\n### Let's choose data from day_201 to day_300 for feature selection. At first, all the feature generation steps must be applied to those data.","3265143e":"### Insights: features 1 to 19\n#### Features 3, 4, 5, 6, 7, 17 and 19 have good effects on predicting target\n#### Features and Corresponding interacting features can be (3, 6), (5, log 42), (6, log 42), (9, log 20), (11, log 99), (12, 66), (15, 26), (19, 26)","20c8434b":"### Conclusion Note: These `selected_features` can be applied to any ML\/DL model to check for performance. If these features outperform or underperform original features with your model, kindly comment here! I develop a model with these features on my own. It will be published soon. ","ac729f40":"### This notebook has the following Notebook as reference on detailed analysis and reasonable way of handling the missing values. Apart from that there are few other good notebooks from where this notebook got some value addition pieces of ideas! I wish to thank my fellow kagglers who compel me to learn and grow!\n\n### [Kaggle Notebook] [Jane TF Keras LSTM](https:\/\/www.kaggle.com\/rajkumarl\/jane-tf-keras-lstm) (to fill missing values)\n","411f323d":"### Great Success. `feature_67` (randomly selected) has negligible correlation with target. But its cubic and quadratic orders exhibit excellent correlation! ","77fbe549":"### We are about to generate new and powerful features by properly interacting above found, correlated features"}}