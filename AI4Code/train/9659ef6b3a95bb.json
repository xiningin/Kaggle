{"cell_type":{"1a482098":"code","08d31841":"code","2ea0ef7e":"code","dc320cc4":"code","3222f1d9":"code","bfbf5748":"code","ca6c8601":"code","80c0fbf6":"code","3ed7d911":"code","5ef7c018":"code","294bfbdd":"code","e83ef549":"code","07461010":"code","5c5efb81":"code","c7dc1f3d":"markdown","40dad5fd":"markdown","e5ae5944":"markdown","43d6bd56":"markdown","85854155":"markdown","a09873e3":"markdown","baa0313b":"markdown","097f73fd":"markdown","6d96b88f":"markdown","31fb1e15":"markdown","58c948d7":"markdown","537b8db3":"markdown","09e06622":"markdown","f19eb716":"markdown"},"source":{"1a482098":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import  RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import cross_val_score\n\nplt.rcParams[\"figure.figsize\"] = (12, 6)","08d31841":"adult_train = pd.read_csv(\"..\/input\/adult-pmr3508\/train_data.csv\", na_values='?', index_col= 0)\nadult_test = pd.read_csv(\"..\/input\/adult-pmr3508\/test_data.csv\", na_values='?', index_col= 0)","2ea0ef7e":"# Explorando a base de dados de treino\nadult_train","dc320cc4":"# Explorando a base de dados de teste\nadult_test","3222f1d9":"adult_train = adult_train.drop_duplicates(keep='first')","bfbf5748":"adult_train = adult_train.dropna()","ca6c8601":"# Quantificando os valores das colunas 'income', 'sex' e 'race'\nadult_train['income'] = LabelEncoder().fit_transform(adult_train['income'])\nadult_train['sex'] = LabelEncoder().fit_transform(adult_train['sex'])\nadult_train['race'] = LabelEncoder().fit_transform(adult_train['race'])\nadult_test['sex'] = LabelEncoder().fit_transform(adult_test['sex'])\nadult_test['race'] = LabelEncoder().fit_transform(adult_test['race'])\n\n# Analisando a correla\u00e7\u00e3o das vari\u00e1veis para determinar quais par\u00e2metros ser\u00e3o usados para treino\nsns.heatmap(adult_train.corr(), annot = True, cmap = 'Reds')\nplt.show()","80c0fbf6":"X_train = adult_train[[\"age\",\"education.num\", \"sex\", \"capital.gain\", \"capital.loss\", \"hours.per.week\"]]\nY_train = adult_train.income","3ed7d911":"naive = GaussianNB()\nbestscores = cross_val_score(naive, X_train, Y_train)\n\nbestscores.mean()","5ef7c018":"random = RandomForestClassifier()\nbestscores = cross_val_score(random, X_train, Y_train)\n\nbestscores.mean()","294bfbdd":"decision = DecisionTreeClassifier()\nbestscores = cross_val_score(decision, X_train, Y_train)\n\nbestscores.mean()","e83ef549":"neural = MLPClassifier(max_iter=100, early_stopping=True)\nbestscores = cross_val_score(neural, X_train, Y_train)\n\nbestscores.mean()","07461010":"boost = XGBClassifier(objective='binary:logistic', eval_metric='error', use_label_encoder=False)\nbestscores = cross_val_score(boost, X_train, Y_train)\n\nbestscores.mean()","5c5efb81":"# Como o XGBoost foi o melhor classificador com aproximadamente 0.85 de acur\u00e1cia m\u00e9dia, o resultado ser\u00e1 submetido a partir dele\nboost.fit(X_train,Y_train)\n\nX_test = adult_test[[\"age\",\"education.num\", \"sex\", \"capital.gain\", \"capital.loss\", \"hours.per.week\"]]\nY_test = boost.predict(X_test)\n\nprint(\"Resultados:\", Y_test)\n\n# Exportando resultados para um arquivo .csv\nresults = pd.DataFrame(data=Y_test).replace(to_replace=[0,1], value=['<=50K','>50K'])\nresults.columns = ['income']\nresults.to_csv(\"resultados.csv\", index = True, index_label = 'Id')","c7dc1f3d":"# 1. Importando todas as bibliotecas que ser\u00e3o usadas","40dad5fd":"# 2. Prepara\u00e7\u00e3o de dados","e5ae5944":"# 4. Submiss\u00e3o de resultados","43d6bd56":"## 2.3 An\u00e1lise de correla\u00e7\u00e3o dos dados","85854155":"## 3.2 Random Forest","a09873e3":"## 3.4 Neural Network","baa0313b":"## 3.1 Naive Bayes","097f73fd":"## 3.5 XGBoost","6d96b88f":"Ser\u00e3o comparados 5 classificadores, o que obter maior m\u00e9dia de acur\u00e1cia ser\u00e1 o escolhido","31fb1e15":"## 2.2 Remo\u00e7\u00e3o de dados faltantes","58c948d7":"## 2.1 Remo\u00e7\u00e3o de linhas duplicadas","537b8db3":"# 3. Classificadores","09e06622":"## 3.3 Decision Tree","f19eb716":"## 2.4 Tratamento de dados"}}