{"cell_type":{"9417d7c9":"code","44268cff":"code","6805b7fb":"code","7f485ee6":"code","414186b0":"code","5d6855a8":"code","84109569":"code","564fb617":"code","bd42c348":"code","fafaeaeb":"code","8d099edc":"code","ec8c11f1":"code","c0e74542":"code","eecf70e4":"code","07f3e6e2":"code","ff52d92e":"code","264e3f42":"code","d8460ebd":"code","257bccd0":"code","25dff372":"code","f8611ab9":"code","be9bb634":"markdown","41182e8f":"markdown","6d7a221a":"markdown","7cfcaad5":"markdown","8f7364b0":"markdown","308a73d8":"markdown","5c386c32":"markdown"},"source":{"9417d7c9":"import numpy as np \nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn import preprocessing\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_auc_score\nimport lightgbm as lgb\nimport gc\nfrom sklearn.compose import ColumnTransformer\nfrom optuna.integration.lightgbm import LightGBMTunerCV\nfrom sklearn.model_selection import KFold","44268cff":"def imputation(df):\n    for col in df:\n        if(df[col].dtype=='object'):\n            df[col].replace(np.nan,df[col].mode().iloc[0],inplace=True)\n        else:\n            df[col].replace(np.nan,df[col].mean(),inplace=True)\n    return df\n\ndef label_encoder(df): #based on https:\/\/www.geeksforgeeks.org\/ml-one-hot-encoding-of-datasets-in-python\/?ref=rp\n    le = LabelEncoder()\n    le_count = 0\n    for col in df: \n        if df[col].dtype == 'object':\n            #if len(list(df[col].unique())) <= 2:   #uncomment this line to encode columns with total unique values less than 3\n            le.fit(df[col])\n            df[col] = le.transform(df[col])\n            le_count += 1\n    print('%d columns were label encoded.' % le_count)\n    return df\n\ndef one_hot_encoder(df,nan_as_category, lst):  #based on https:\/\/www.kaggle.com\/jsaguiar\/lightgbm-with-simple-features\n    original_columns = list(df.columns)\n    categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n    for i in lst:\n        categorical_columns.remove(i)\n    print(\"One Hot Enocding:\",len(categorical_columns),\"columns\")\n    print(categorical_columns)\n    df = pd.get_dummies(df, columns = categorical_columns, dummy_na = nan_as_category)\n    new_columns = [c for c in df.columns if c not in original_columns] \n    print(len(categorical_columns),\"columns one hot encoded\")\n    return df,new_columns","6805b7fb":"train_file = pd.read_csv(\"..\/input\/malware-prediction-tm\/train.csv\", sep = \",\", low_memory = False)\ntest_file = pd.read_csv(\"..\/input\/malware-prediction-tm\/test.csv\", sep = \",\", low_memory = False)","7f485ee6":"cat_cols = []\nint_cols = []\n\nno_unique = 0 #number of unique values\npercentage = 90 #max percentage the no_unique can take\n\nfor col in train_file:\n    k = (train_file[col].value_counts()\/train_file[col].count())*100\n    if sum(list(k.head(no_unique)))<percentage:\n        if train_file[col].dtypes == \"object\":   #converting dtypes to string for label econding\n            train_file[col] = train_file[col].astype(str)\n            cat_cols.append(col)\n        else: int_cols.append(col)\ncat_cols.remove(\"MachineIdentifier\")\n\n#removing all features execpt the ones that have passed the above test\ndf = train_file.drop(train_file.columns.difference(cat_cols+int_cols),1)\n\n#print(\"cat columns:\",len(cat_cols),cat_cols)\n#print(\"Int columns:\", len(int_cols),int_cols)","414186b0":"#Pushing HasDetections feature to the first column\ncol_name = \"HasDetections\"\nfirst_col = df.pop(col_name)\n\ndf.insert(0,col_name,first_col)","5d6855a8":"df_label = label_encoder(df)\n\n#df_label","84109569":"correlations = df_label.corr()\n\n#print('Most Positive Correlations:\\n', correlations.tail(20))\n#print('\\nMost Negative Correlations:\\n', correlations.head(20))","564fb617":"#Remove features that have very weak correlations with HasDetections\ncorr_value = 0.001\ncorrelations_list = correlations.drop(correlations[((correlations['HasDetections'] < corr_value)&(correlations['HasDetections'] > -corr_value))].index)\n#Drop all columns except HasDetections and sort \ncorrelations_list = correlations_list.drop(correlations_list.columns.difference([\"HasDetections\"]),1).sort_values([\"HasDetections\"])\n\nprint(len(correlations_list))\n#print('Most Positive Correlations:\\n', correlations_list.tail(40))\n#print('\\nMost Negative Correlations:\\n', correlations_list.head(20))","bd42c348":"#Remove features that are dependent. Features that have correlation value greater than max_corr with other features\n\nindex = list(correlations_list.index)\nindex.remove(\"HasDetections\")\nindex.remove(\"Census_IsWIMBootEnabled\")\n#print(index)\nmax_corr = 0.99\n\ndata = df_label[index]\ndata_corr = data.corr()\n\ndf_temp = data_corr\ndf_temp.values[np.triu_indices_from(df_temp,0)] = 0\ndf_temp\n\ndf_temp2 = df_temp\nfor col in df_temp2:\n    df_temp2 = df_temp2.drop(df_temp2[(df_temp2[col] > max_corr)|(df_temp2[col] < -max_corr)].index)\n\nvar_list = list(df_temp2.index)\n#print(len(var_list),var_list)","fafaeaeb":"#Create new Datasets with the only the features obtained\nDF = train_file.drop(train_file.columns.difference([\"HasDetections\"]+var_list),1)\ncol_name = \"HasDetections\"\nfirst_col = DF.pop(col_name)\n\nDF.insert(0,col_name,first_col)\n\nDF_test = test_file.drop(test_file.columns.difference(var_list),1)","8d099edc":"#Perform Label Encoding and Imputation\nln = len(DF)\nDF_1 = DF.append(DF_test)\n\nfor col in DF_1:\n    if DF_1[col].dtypes == 'object':\n        DF_1[col] = DF_1[col].astype(str)\nlabel_encoder(DF_1)\n\nimputation(DF_1)\n#DF_1","ec8c11f1":"#Split data into training data, validaion data and test data\nsample = 150000\nln1 = ln-sample\n\nX_train = DF_1.iloc[:ln,1:]\ny_train = DF_1.iloc[:ln,:1]\n\nX_test = DF_1.iloc[ln1:ln1+sample,1:]\ny_test = DF_1.iloc[ln1:ln1+sample,:1]\n\nx_test_file = DF_1.iloc[ln:,1:]\n","c0e74542":"#Create Lightgbm Dataset\nlgb_train_file = lgb.Dataset(X_train, label=y_train,free_raw_data=False)\nlgb_test_file = lgb.Dataset(X_test, label = y_test,free_raw_data=False)\ngc.collect()","eecf70e4":"#Parameter Tuning\n\n\"\"\"\"params = {\n        \"objective\": \"binary\",\n        \"metric\": \"binary_logloss\",\n        \"verbosity\": -1,\n        \"boosting_type\": \"gbdt\",\n    }\n\ntuner = LightGBMTunerCV(\n        params, lgb_train_file, verbose_eval=100, early_stopping_rounds=100, folds=KFold(n_splits=5)\n    )\n\ntuner.run()\n    \nprint(\"Best score:\", tuner.best_score)\nbest_params = tuner.best_params\nprint(\"Best params:\", best_params)\nprint(\"  Params: \")\nfor key, value in best_params.items():\n    print(\"    {}: {}\".format(key, value))\"\"\"","07f3e6e2":"#Parameters obtained after parameter tuning\nparams = {'objective': 'binary',\n          'metric': 'binary_logloss',\n          'verbosity': -1,\n          'boosting_type': 'gbdt',\n          'feature_pre_filter': False,\n          'lambda_l1': 9.578050569274525,\n          'lambda_l2': 4.483433289177043e-05,\n          'num_leaves': 100,\n          'feature_fraction': 0.4,\n          'bagging_fraction': 1.0,\n          'bagging_freq': 0,\n          'min_child_samples': 20}\nparams['learning_rate'] = 0.001\n\n#Train the LightGBM Model\nbst = lgb.train(params,lgb_train_file,50000,\n                valid_sets = [lgb_test_file],\n                verbose_eval=100, early_stopping_rounds = 1000)","ff52d92e":"#Predict \ntrain_pred = bst.predict(X_train)\ntest_pred = bst.predict(X_test)\nprint(\"Train Accuracy:\",roc_auc_score(y_train,train_pred)*100)\nprint(\"Test Accuracy:\",roc_auc_score(y_test,test_pred)*100)\ngc.collect()","264e3f42":"#69.84\n#70.4\n#71.20645\n#71.43952 200 trees\n#71.82003\n#72.09997767157274 Params optimized and 0.01 lr 0.375766\n#Train Accuracy: 80.77362699612381\n#Test Accuracy: 79.52696084452377 [50000]\tvalid_0's binary_logloss: 0.349694","d8460ebd":"#Predict on the Test file\ntest_file_pred = bst.predict(x_test_file)\n#len(test_file_pred)","257bccd0":"#Creating a submission file\nID = test_file.drop(test_file.columns.difference([\"MachineIdentifier\"]),1)\n\nsubmission = pd.DataFrame({'MachineIdentifier': ID['MachineIdentifier'], 'HasDetections' : test_file_pred})\nsubmission.to_csv('submission.csv',index=False)","25dff372":"test_file.shape","f8611ab9":"submission.describe()","be9bb634":"Import Libraries","41182e8f":"Training with the features obtained ","6d7a221a":"Import Data","7cfcaad5":"Label Encoding","8f7364b0":"Remove features where some unique values are in majaority","308a73d8":"Correlations","5c386c32":"All Functions"}}