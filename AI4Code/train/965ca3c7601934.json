{"cell_type":{"88639b82":"code","649af6b3":"code","c905e31c":"code","c2496baf":"code","852adfe0":"code","0f67e548":"code","f659680e":"code","4748bb5c":"code","0fc9d787":"code","33cc863c":"code","15a6a25c":"code","4c00a05c":"code","d05ba51d":"code","26166c7d":"code","9dd58b1b":"code","a562338f":"code","49417111":"code","42cbabcf":"code","797373b2":"code","a62ff440":"code","2a131f77":"code","aacc6961":"code","52fd55e8":"markdown","8c9d2a9b":"markdown","45c04ceb":"markdown","b5d980e1":"markdown","3bd3098e":"markdown","97a5f17e":"markdown","a6c9a984":"markdown","0786e3bc":"markdown","d28e79ab":"markdown","9bb06a77":"markdown","0411ae9f":"markdown","4820d133":"markdown","530127dc":"markdown","7561268a":"markdown","f4b21b2a":"markdown","17a22e67":"markdown","51ccf85c":"markdown"},"source":{"88639b82":"import numpy as np\nimport pandas as pd\nimport os\nimport matplotlib\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm_notebook\n%matplotlib inline \nimport cv2 as cv\nfrom matplotlib.patches import Rectangle","649af6b3":"DATA_FOLDER = '..\/input\/deepfake-detection-challenge'\nTRAIN_SAMPLE_FOLDER = 'train_sample_videos'\nTEST_FOLDER = 'test_videos'\n\nprint(f\"Train samples: {len(os.listdir(os.path.join(DATA_FOLDER, TRAIN_SAMPLE_FOLDER)))}\")\nprint(f\"Test samples: {len(os.listdir(os.path.join(DATA_FOLDER, TEST_FOLDER)))}\")","c905e31c":"train_list = list(os.listdir(os.path.join(DATA_FOLDER, TRAIN_SAMPLE_FOLDER)))\next_dict = []\nfor file in train_list:\n    file_ext = file.split('.')[1]\n    if (file_ext not in ext_dict):\n        ext_dict.append(file_ext)\nprint(f\"Extensions: {ext_dict}\")     \n\n\nfor file_ext in ext_dict:\n    print(f\"Files with extension `{file_ext}`: {len([file for file in train_list if  file.endswith(file_ext)])}\")","c2496baf":"json_file = [file for file in train_list if  file.endswith('json')][0]\nprint(f\"JSON file: {json_file}\")\n\ndef get_meta_from_json(path):\n    df = pd.read_json(os.path.join(DATA_FOLDER, path, json_file))\n    df = df.T\n    return df\n\nmeta_train_df = get_meta_from_json(TRAIN_SAMPLE_FOLDER)\nmeta_train_df.head()","852adfe0":"meta_train_df","0f67e548":"def unique_values(data):\n    total = data.count()\n    tt = pd.DataFrame(total)\n    tt.columns = ['Total']\n    uniques = []\n    for col in data.columns:\n        unique = data[col].nunique()\n        uniques.append(unique)\n    tt['Uniques'] = uniques\n    return(np.transpose(tt))\n\n\nunique_values(meta_train_df)","f659680e":"def plot_count(feature, title, df, size=1):\n    '''\n    Plot count of classes \/ feature\n    param: feature - the feature to analyze\n    param: title - title to add to the graph\n    param: df - dataframe from which we plot feature's classes distribution \n    param: size - default 1.\n    '''\n    f, ax = plt.subplots(1,1, figsize=(4*size,4))\n    total = float(len(df))\n    g = sns.countplot(df[feature], order = df[feature].value_counts().index[:20], palette='Set3')\n    g.set_title(\"Number and percentage of {}\".format(title))\n    if(size > 2):\n        plt.xticks(rotation=90, size=8)\n    for p in ax.patches:\n        height = p.get_height()\n        ax.text(p.get_x()+p.get_width()\/2.,\n                height + 3,\n                '{:1.2f}%'.format(100*height\/total),\n                ha=\"center\") \n    plt.show()    \n\nplot_count('label', 'label (train)', meta_train_df)","4748bb5c":"def most_frequent_values(data):\n    total = data.count()\n    tt = pd.DataFrame(total)\n    tt.columns = ['Total']\n    items = []\n    vals = []\n    for col in data.columns:\n        itm = data[col].value_counts().index[0]\n        val = data[col].value_counts().values[0]\n        items.append(itm)\n        vals.append(val)\n    tt['Most frequent item'] = items\n    tt['Frequence'] = vals\n    tt['Percent from total'] = np.round(vals \/ total * 100, 3)\n    return(np.transpose(tt))\n\nmost_frequent_values(meta_train_df)","0fc9d787":"from IPython.display import HTML\nfrom base64 import b64encode\n\ndef play_video(video_file, subset=TRAIN_SAMPLE_FOLDER):\n    '''\n    Display video\n    param: video_file - the name of the video file to display\n    param: subset - the folder where the video file is located (can be TRAIN_SAMPLE_FOLDER or TEST_Folder)\n    '''\n    video_url = open(os.path.join(DATA_FOLDER, subset,video_file),'rb').read()\n    data_url = \"data:video\/mp4;base64,\" + b64encode(video_url).decode()\n    return HTML(\"\"\"<video width=500 controls><source src=\"%s\" type=\"video\/mp4\"><\/video>\"\"\" % data_url)\n","33cc863c":"from tensorflow.python.keras.layers import Conv2D, Input, ZeroPadding2D, Dense, Lambda\nfrom tensorflow.python.keras.models import Model\nfrom tensorflow.python.keras.applications.mobilenet_v2 import MobileNetV2\nimport tensorflow as tf\ntf.compat.v1.disable_eager_execution()\nimport math\nimport numpy as np\nimport cv2","15a6a25c":"def load_mobilenetv2_224_075_detector(path):\n    input_tensor = Input(shape=(224, 224, 3))\n    output_tensor = MobileNetV2(weights=None, include_top=False, input_tensor=input_tensor, alpha=0.75).output\n    output_tensor = ZeroPadding2D()(output_tensor)\n    output_tensor = Conv2D(kernel_size=(3, 3), filters=5)(output_tensor)\n\n    model = Model(inputs=input_tensor, outputs=output_tensor)\n    model.load_weights(path)\n    \n    return model","4c00a05c":"mobilenetv2 = load_mobilenetv2_224_075_detector(\"..\/input\/facedetection-mobilenetv2\/facedetection-mobilenetv2-size224-alpha0.75.h5\")\nmobilenetv2.summary()","d05ba51d":"# Converts A:B aspect rate to B:A\ndef transpose_shots(shots):\n    return [(shot[1], shot[0], shot[3], shot[2], shot[4]) for shot in shots]\n\n#That constant describe pieces for 16:9 images\nSHOTS = {\n    # fast less accurate\n    '2-16\/9' : {\n        'aspect_ratio' : 16\/9,\n        'shots' : [\n             (0, 0, 9\/16, 1, 1),\n             (7\/16, 0, 9\/16, 1, 1)\n        ]\n    },\n    # slower more accurate\n    '10-16\/9' : {\n        'aspect_ratio' : 16\/9,\n        'shots' : [\n             (0, 0, 9\/16, 1, 1),\n             (7\/16, 0, 9\/16, 1, 1),\n             (0, 0, 5\/16, 5\/9, 0.5),\n             (0, 4\/9, 5\/16, 5\/9, 0.5),\n             (11\/48, 0, 5\/16, 5\/9, 0.5),\n             (11\/48, 4\/9, 5\/16, 5\/9, 0.5),\n             (22\/48, 0, 5\/16, 5\/9, 0.5),\n             (22\/48, 4\/9, 5\/16, 5\/9, 0.5),\n             (11\/16, 0, 5\/16, 5\/9, 0.5),\n             (11\/16, 4\/9, 5\/16, 5\/9, 0.5),\n        ]\n    }\n}\n\n# 9:16 respectively\nSHOTS_T = {\n    '2-9\/16' : {\n        'aspect_ratio' : 9\/16,\n        'shots' : transpose_shots(SHOTS['2-16\/9']['shots'])\n    },\n    '10-9\/16' : {\n        'aspect_ratio' : 9\/16,\n        'shots' : transpose_shots(SHOTS['10-16\/9']['shots'])\n    }\n}\n\ndef r(x):\n    return int(round(x))\n\ndef sigmoid(x):\n    return 1 \/ (np.exp(-x) + 1)\n\ndef non_max_suppression(boxes, p, iou_threshold):\n\n    if len(boxes) == 0:\n        return np.array([])\n\n    x1 = boxes[:, 0]\n    y1 = boxes[:, 1]\n    x2 = boxes[:, 2]\n    y2 = boxes[:, 3]\n\n    indexes = np.argsort(p)\n    true_boxes_indexes = []\n\n    while len(indexes) > 0:\n        true_boxes_indexes.append(indexes[-1])\n\n        intersection = np.maximum(np.minimum(x2[indexes[:-1]], x2[indexes[-1]]) - np.maximum(x1[indexes[:-1]], x1[indexes[-1]]), 0) * np.maximum(np.minimum(y2[indexes[:-1]], y2[indexes[-1]]) - np.maximum(y1[indexes[:-1]], y1[indexes[-1]]), 0)\n        iou = intersection \/ ((x2[indexes[:-1]] - x1[indexes[:-1]]) * (y2[indexes[:-1]] - y1[indexes[:-1]]) + (x2[indexes[-1]] - x1[indexes[-1]]) * (y2[indexes[-1]] - y1[indexes[-1]]) - intersection)\n\n        indexes = np.delete(indexes, -1)\n        indexes = np.delete(indexes, np.where(iou >= iou_threshold)[0])\n\n    return boxes[true_boxes_indexes]\n\ndef union_suppression(boxes, threshold):\n    if len(boxes) == 0:\n        return np.array([])\n\n    x1 = boxes[:, 0]\n    y1 = boxes[:, 1]\n    x2 = boxes[:, 2]\n    y2 = boxes[:, 3]\n\n    indexes = np.argsort((x2 - x1) * (y2 - y1))\n    result_boxes = []\n\n    while len(indexes) > 0:\n        intersection = np.maximum(np.minimum(x2[indexes[:-1]], x2[indexes[-1]]) - np.maximum(x1[indexes[:-1]], x1[indexes[-1]]), 0) * np.maximum(np.minimum(y2[indexes[:-1]], y2[indexes[-1]]) - np.maximum(y1[indexes[:-1]], y1[indexes[-1]]), 0)\n        min_s = np.minimum((x2[indexes[:-1]] - x1[indexes[:-1]]) * (y2[indexes[:-1]] - y1[indexes[:-1]]), (x2[indexes[-1]] - x1[indexes[-1]]) * (y2[indexes[-1]] - y1[indexes[-1]]))\n        ioms = intersection \/ (min_s + 1e-9)\n        neighbours = np.where(ioms >= threshold)[0]\n        if len(neighbours) > 0:\n            result_boxes.append([min(np.min(x1[indexes[neighbours]]), x1[indexes[-1]]), min(np.min(y1[indexes[neighbours]]), y1[indexes[-1]]), max(np.max(x2[indexes[neighbours]]), x2[indexes[-1]]), max(np.max(y2[indexes[neighbours]]), y2[indexes[-1]])])\n        else:\n            result_boxes.append([x1[indexes[-1]], y1[indexes[-1]], x2[indexes[-1]], y2[indexes[-1]]])\n\n        indexes = np.delete(indexes, -1)\n        indexes = np.delete(indexes, neighbours)\n\n    return result_boxes\n\nclass FaceDetector():\n    \"\"\"\n    That's API you can easily use to detect faces\n    \n    __init__ parameters:\n    -------------------------------\n    model - model to infer\n    shots - list of aspect ratios that images could be (described earlier)\n    image_size - model's input size (hardcoded for mobilenetv2)\n    grids - model's output size (hardcoded for mobilenetv2)\n    union_threshold - threshold for union of predicted boxes within multiple shots\n    iou_threshold - IOU threshold for non maximum suppression used to merge YOLO detected boxes for one shot,\n                    you do need to change this because there are one face per image as I can see from the samples\n    prob_threshold - probability threshold for YOLO algorithm, you can balance beetween precision and recall using this threshold\n    \n    detect parameters:\n    -------------------------------\n    frame - (1920, 1080, 3) or (1080, 1920, 3) RGB Image\n    returns: list of 4 element tuples (left corner x, left corner y, right corner x, right corner y) of detected boxes within [0, 1] range (see box draw code below)\n    \"\"\"\n    def __init__(self, model=mobilenetv2, shots=[SHOTS['10-16\/9'], SHOTS_T['10-9\/16']], image_size=224, grids=7, iou_threshold=0.1, union_threshold=0.1, prob_threshold=0.65):\n        self.model = model\n        self.shots = shots\n        self.image_size = image_size\n        self.grids = grids\n        self.iou_threshold = iou_threshold\n        self.union_threshold = union_threshold\n        self.prob_threshold = prob_threshold\n        \n    \n    def detect(self, frame):\n        original_frame_shape = frame.shape\n\n        aspect_ratio = None\n        for shot in self.shots:\n            if abs(frame.shape[1] \/ frame.shape[0] - shot[\"aspect_ratio\"]) < 1e-9:\n                aspect_ratio = shot[\"aspect_ratio\"]\n                shots = shot\n        \n        assert aspect_ratio is not None\n        \n        c = min(frame.shape[0], frame.shape[1] \/ aspect_ratio)\n        slice_h_shift = r((frame.shape[0] - c) \/ 2)\n        slice_w_shift = r((frame.shape[1] - c * aspect_ratio) \/ 2)\n        if slice_w_shift != 0 and slice_h_shift == 0:\n            frame = frame[:, slice_w_shift:-slice_w_shift]\n        elif slice_w_shift == 0 and slice_h_shift != 0:\n            frame = frame[slice_h_shift:-slice_h_shift, :]\n\n        frames = []\n        for s in shots[\"shots\"]:\n            frames.append(cv2.resize(frame[r(s[1] * frame.shape[0]):r((s[1] + s[3]) * frame.shape[0]), r(s[0] * frame.shape[1]):r((s[0] + s[2]) * frame.shape[1])], (self.image_size, self.image_size), interpolation=cv2.INTER_NEAREST))\n        frames = np.array(frames)\n\n        predictions = self.model.predict(frames, batch_size=len(frames), verbose=0)\n\n        boxes = []\n        prob = []\n        shots = shots['shots']\n        for i in range(len(shots)):\n            slice_boxes = []\n            slice_prob = []\n            for j in range(predictions.shape[1]):\n                for k in range(predictions.shape[2]):\n                    p = sigmoid(predictions[i][j][k][4])\n                    if not(p is None) and p > self.prob_threshold:\n                        px = sigmoid(predictions[i][j][k][0])\n                        py = sigmoid(predictions[i][j][k][1])\n                        pw = min(math.exp(predictions[i][j][k][2] \/ self.grids), self.grids)\n                        ph = min(math.exp(predictions[i][j][k][3] \/ self.grids), self.grids)\n                        if not(px is None) and not(py is None) and not(pw is None) and not(ph is None) and pw > 1e-9 and ph > 1e-9:\n                            cx = (px + j) \/ self.grids\n                            cy = (py + k) \/ self.grids\n                            wx = pw \/ self.grids\n                            wy = ph \/ self.grids\n                            if wx <= shots[i][4] and wy <= shots[i][4]:\n                                lx = min(max(cx - wx \/ 2, 0), 1)\n                                ly = min(max(cy - wy \/ 2, 0), 1)\n                                rx = min(max(cx + wx \/ 2, 0), 1)\n                                ry = min(max(cy + wy \/ 2, 0), 1)\n\n                                lx *= shots[i][2]\n                                ly *= shots[i][3]\n                                rx *= shots[i][2]\n                                ry *= shots[i][3]\n\n                                lx += shots[i][0]\n                                ly += shots[i][1]\n                                rx += shots[i][0]\n                                ry += shots[i][1]\n\n                                slice_boxes.append([lx, ly, rx, ry])\n                                slice_prob.append(p)\n\n            slice_boxes = np.array(slice_boxes)\n            slice_prob = np.array(slice_prob)\n\n            slice_boxes = non_max_suppression(slice_boxes, slice_prob, self.iou_threshold)\n\n            for sb in slice_boxes:\n                boxes.append(sb)\n\n\n        boxes = np.array(boxes)\n        boxes = union_suppression(boxes, self.union_threshold)\n\n        for i in range(len(boxes)):\n            boxes[i][0] \/= original_frame_shape[1] \/ frame.shape[1]\n            boxes[i][1] \/= original_frame_shape[0] \/ frame.shape[0]\n            boxes[i][2] \/= original_frame_shape[1] \/ frame.shape[1]\n            boxes[i][3] \/= original_frame_shape[0] \/ frame.shape[0]\n\n            boxes[i][0] += slice_w_shift \/ original_frame_shape[1]\n            boxes[i][1] += slice_h_shift \/ original_frame_shape[0]\n            boxes[i][2] += slice_w_shift \/ original_frame_shape[1]\n            boxes[i][3] += slice_h_shift \/ original_frame_shape[0]\n\n        return list(boxes)","26166c7d":"detector = FaceDetector()","9dd58b1b":"DATA_FOLDER = '..\/input\/deepfake-detection-challenge'\nTRAIN_SAMPLE_FOLDER = 'train_sample_videos'\nTEST_FOLDER = 'test_videos'\n\ntrain_list = list(os.listdir(os.path.join(DATA_FOLDER, TRAIN_SAMPLE_FOLDER)))\njson_file = [file for file in train_list if  file.endswith('json')][0]\n\ndef get_meta_from_json(path):\n    df = pd.read_json(os.path.join(DATA_FOLDER, path, json_file))\n    df = df.T\n    return df\n\nmeta_train_df = get_meta_from_json(TRAIN_SAMPLE_FOLDER)","a562338f":"def display_image_from_video(video_path):\n    '''\n    input: video_path - path for video\n    process:\n    1. perform a video capture from the video\n    2. read the image\n    3. display the image\n    '''\n    capture_image = cv.VideoCapture(video_path) \n    ret, frame = capture_image.read()\n    fig = plt.figure(figsize=(10,10))\n    ax = fig.add_subplot(111)\n    frame = cv.cvtColor(frame, cv.COLOR_BGR2RGB)\n    ax.imshow(frame)\n    boxes = detector.detect(frame)\n    # lets's draw boxes, just multiply each predicted [0, 1] relative coordinate to image side in pixels respectively\n    for box in boxes:\n        lx = int(round(box[0] * frame.shape[1]))\n        ly = int(round(box[1] * frame.shape[0]))\n        rx = int(round(box[2] * frame.shape[1]))\n        ry = int(round(box[3] * frame.shape[0]))\n        # x, y, w, h here\n        ax.add_patch(Rectangle((lx,ly),rx - lx,ry - ly,linewidth=2,edgecolor='r',facecolor='none'))","49417111":"fake_train_sample_video = list(meta_train_df.loc[meta_train_df.label=='FAKE'].sample(30).index)\nprint(meta_train_df.loc[meta_train_df.label=='FAKE'].sample(20).index)","42cbabcf":"for video_file in fake_train_sample_video:\n    display_image_from_video(os.path.join(DATA_FOLDER, TRAIN_SAMPLE_FOLDER, video_file))","797373b2":"real_train_sample_video = list(meta_train_df.loc[meta_train_df.label=='REAL'].sample(30).index)\n\nprint(meta_train_df.loc[meta_train_df.label=='REAL'].sample(20).index)","a62ff440":"play_video(\"ehtdtkmmli.mp4\")","2a131f77":"play_video(\"eukvucdetx.mp4\")","aacc6961":"for video_file in real_train_sample_video:\n    display_image_from_video(os.path.join(DATA_FOLDER, TRAIN_SAMPLE_FOLDER, video_file))","52fd55e8":"\u6a23\u672c\u4e2d\u986f\u793a\uff1a\n\u672c\u6b21\u6bd4\u8cfd\u4e2d\u6709 1920x1080 (16:9) \u548c 1080x1920 (9:16) \u5716\u50cf\n\uff08\u5982\u679c\u6709\u767c\u73fe\u5176\u4ed6\u5716\u50cf\uff0c\u53ef\u4ee5\u8f15\u9b06\u5730\u5c07\u5b83\u5011\u5206\u5225\u6dfb\u52a0\u5230 SHOTS \u548c SHOTS_T \u5411\u91cf\u4e2d\uff09\n\n\u6a21\u578b\u662f\u5728 1:1 \u7e31\u6a6b\u6bd4\u5716\u50cf\u4e0a\u8a13\u7df4\u3002\n\u6240\u4ee5\u5982\u679c\u6211\u5011\u60f3\u4f7f\u7528 16:9 \u548c 9:16 \u5716\u50cf\uff0c\u6211\u5011\u9700\u8981\u5c07\u5b83\u5011\u5206\u6210 2 \u90e8\u5206\uff0c\u6211\u5011\u4e5f\u53ef\u4ee5\u5c07\u5b83\u5011\u5206\u6210\u66f4\u5c0f\u7684\uff08\u4f8b\u5982 10\uff09\u76f8\u4ea4\u90e8\u5206\u4ee5\u7372\u5f97\u66f4\u591a\u5c0d\u8f03\u5c0f\u81c9\u90e8\u7684\u6e96\u78ba\u9810\u6e2c\u3002","8c9d2a9b":"\u9a57\u8b49\u8a13\u7df4\u6a94\u6848\u4e2d\uff0cJSON \u6a94\u7684\u5167\u5bb9\uff1a","45c04ceb":"#### \u78ba\u8a8d\u4e00\u4e0b\u8a13\u7df4\u6a23\u672c\u53ca\u6e2c\u8a66\u6a23\u7248\u4e2d\u7684\u526f\u6a94\u540d\u3002","b5d980e1":"\u7531\u5c08\u6848\u4fee\u6539\uff1a\nhttps:\/\/www.kaggle.com\/drjerk\/detect-faces-using-yolo\n\n\u6b64\u5c08\u6848\u570b\u9632\u5927\u5b78\u6a5f\u5668\u5b78\u7fd2\u7531\u8521\u5b97\u61b2\u8001\u5e2b\u6307\u5c0e\u7684\u671f\u4e2d\u5831\u544a-DeepFakes\u63db\u81c9\u8fa8\u8b58\u6311\u6230(\u4f7f\u7528YOLOv2)","3bd3098e":"# \u8f09\u5165\u5df2\u8a13\u7df4\u904e\u7684\u6700\u4f73\u6a21\u578b(Optimal Model)\n\u4f7f\u7528\u5df2\u53d7\u8a13\u7df4\u7684\u6a23\u672c\u7279\u5fb5\u8cc7\u6599\u96c6\uff1afacedetection-mobilenetv2-size224-alpha0.75.h5","97a5f17e":"# \u5148\u8a2d\u5b9a\u8f09\u5165\u5f71\u7247\u6642\u7684\u756b\u9762\u9577\u5bec\u8207\u4e00\u4e9b\u5f71\u50cf\u7b49\u539f\u59cb\u8a2d\u5b9a\u3002","a6c9a984":"\u6700\u983b\u7e41\u7684\u6a19\u7c64\u662f FAKE\uff0880.75%\uff09\uff0catvmxvwyns.mp4 \u662f\u6700\u983b\u7e41\u7684\u539f\u59cb\u6a19\u7c64\uff086 \u500b\u6a23\u672c\uff09\u3002","0786e3bc":"\u6211\u5011\u89c0\u5bdf\u5230\u539f\u59cb\u6a19\u7c64\u5c0d\u65bc\u552f\u4e00\u503c\u5177\u6709\u76f8\u540c\u7684\u6a21\u5f0f\u3002 \n\u6211\u5011\u77e5\u9053\u6211\u5011\u6709 77 \u500b\u7f3a\u5931\u6578\u64da\uff08\u9019\u5c31\u662f\u70ba\u4ec0\u9ebc\u7e3d\u6578\u53ea\u6709 323 \u500b\uff09\u4e26\u4e14\u6211\u5011\u89c0\u5bdf\u5230\u6211\u5011\u78ba\u5be6\u6709 209 \u500b\u7368\u7279\u7684\u4f8b\u5b50\u3002","d28e79ab":"# 1. Load packages","9bb06a77":"# \u6bd4\u8f03\u771f\u5be6 \/ \u507d\u9020\u5f71\u7247","0411ae9f":"* \u8a13\u7df4\u7d50\u679c\uff1a\u53ef\u4ee5\u900f\u904eREAL\u5c6c\u6027\u53caFAKE\u7cbe\u78ba\u53d6\u5f97\u4eba\u81c9\u8fa8\u8b58\u7d50\u679c\u3002\n\n","4820d133":"* \u6aa2\u6e2c\u5668\u7a0b\u5f0f\u78bc\u7684\u7d50\u5c3e\n\n\u5728\u6a21\u578b\u8a13\u7df4\u5b8c\u6210\u5f8c\uff0c\u6700\u7d42\u9700\u8981\u53ef\u4ee5\u5373\u6642\u5075\u6e2c\u5230\u4eba\u81c9\u5f8c\u8fa8\u8b58\uff1a","530127dc":"# 2.Load data\n","7561268a":"# \u7528\u65bc\u6aa2\u6e2c\u4eba\u81c9\u7684 API\uff1aFaceDetector():\n\n__init__ \u53c3\u6578\u8aaa\u660e\uff1a\n\nmodel - \u8981\u63a8\u65b7\u7684\u6a21\u578b\n\nshots - \u5716\u50cf\u53ef\u80fd\u7684\u7e31\u6a6b\u6bd4\u5217\u8868\uff08\u4e4b\u524d\u63cf\u8ff0\u904e\uff09\n\nimage_size - \u6a21\u578b\u7684\u8f38\u5165\u5927\u5c0f\uff08\u70ba mobilenetv2 \u786c\u7de8\u78bc\uff09\n\ngrids - \u6a21\u578b\u7684\u8f38\u51fa\u5927\u5c0f\uff08\u70ba mobilenetv2 \u786c\u7de8\u78bc\uff09\n\nunion_threshold - \u591a\u500b\u93e1\u982d\u4e2d\u9810\u6e2c\u6846\u4e26\u96c6\u7684\u95be\u503c\n\niou_threshold - \u975e\u6700\u5927\u6291\u5236\u7684 IOU \u95be\u503c\uff0c\u7528\u65bc\u5c07 YOLO \u6aa2\u6e2c\u5230\u7684\u6846\u5408\u4f75\u70ba\u4e00\u500b\u93e1\u982d\uff0c\n                \u60a8\u78ba\u5be6\u9700\u8981\u66f4\u6539\u6b64\u8a2d\u7f6e\uff0c\u9810\u671f\u6bcf\u5f35\u5716\u50cf\u90fd\u6709\u81f3\u5c11\u6709\u4e00\u5f35\u81c9\n                \nprob_threshold - YOLO \u7b97\u6cd5\u7684\u6982\u7387\u95be\u503c\uff0c\u60a8\u53ef\u4ee5\u4f7f\u7528\u6b64\u95be\u503c\u5e73\u8861\u7cbe\u5ea6\u548c\u53ec\u56de\u7387\n\n    \n\u6aa2\u6e2c\u53c3\u6578\uff1a\n-------------------------------\nframe - (1920, 1080, 3) or (1080, 1920, 3) RGB Image\nreturns: list of 4 element tuples (left corner x, left corner y, right corner x, right corner y) of detected boxes within [0, 1] range (see box draw code below)\n","f4b21b2a":"# \u507d\u9020\u81c9\u90e8\u6e05\u55ae\u6e2c\u8a66\nfake_train_sample_video = list(meta_train_df.loc[meta_train_df.label=='FAKE'].sample(300).index)\nfake_train_sample_video\n\n\u6216\u7528\u5f71\u7247\u4f86\u78ba\u8a8d\uff1a\n\nplay_video(\"eudeqjhdfd.mp4\")\n\nplay_video(\"eukvucdetx.mp4\")","17a22e67":"# \u771f\u5be6\u5f71\u7247\u6e05\u55ae","51ccf85c":"# \u8b80\u53d6\u5f71\u50cf\u524d\u7684\u9810\u8655\u7406\n\u5c07\u5f71\u50cf\u8655\u7406\u70ba\u6613\u65bc\u8b58\u5225\u7684\u683c\u5f0f\uff0c\u8b58\u5225\u9032\u884c\u4eba\u81c9\u5075\u6e2c\u7684\u5716\u7247\uff0c\u5982\u679c\u6210\u529f\u5075\u6e2c\u5230\u4eba\u81c9\u5c31\u6846\u9078\u51fa\u4eba\u81c9\u3002"}}