{"cell_type":{"9a94a0bc":"code","f1d45a16":"code","6af66a83":"code","d18cc9b2":"code","4894d5e1":"code","f774b579":"code","17906e22":"code","22ac57c6":"code","5c286efc":"code","b6b22720":"code","2a9232da":"code","e2e2b721":"code","7907a3f8":"code","45f10741":"code","4991979d":"code","a1360884":"code","eb263626":"code","f9c4fcf8":"code","f54fdd89":"code","a75b101d":"code","07f4ec54":"code","4fb83ee1":"code","0cfbaf5c":"code","83cbadf1":"code","4b60a379":"code","4e45b5e0":"code","474e7b57":"code","4366f9b6":"code","1ed05f47":"code","cec4919d":"code","13ee3ee1":"code","5708d8a2":"code","a96ed173":"code","77c9b34e":"code","54c12ad8":"code","ffcdf7b4":"code","9be880c7":"code","1d6ce55b":"code","f272db8d":"code","576bdae8":"code","8aa02fd8":"code","2e8edde3":"code","7475a440":"code","b8ef45cd":"markdown","d6fde122":"markdown","2d888f4a":"markdown","02ab50d7":"markdown","ef53424e":"markdown","a8120f88":"markdown","9b614864":"markdown","e51552b9":"markdown","62af77e8":"markdown","458dc32e":"markdown","979aa493":"markdown","cc3ee5f2":"markdown","eb32c8bc":"markdown","9ade607f":"markdown","f5272ad4":"markdown","99f8d77d":"markdown","dc0966c8":"markdown","eeaba3de":"markdown","78fb1991":"markdown","faf0101b":"markdown","81fe35d3":"markdown","3b26d8b7":"markdown","0385e831":"markdown","4490d7b1":"markdown","61ebb0b9":"markdown","d347e675":"markdown","1e02dde6":"markdown","199a957d":"markdown","94bedf57":"markdown","9818cf55":"markdown","a1de7162":"markdown","0c0e13d3":"markdown","1813aca1":"markdown","73890833":"markdown","cbac42ca":"markdown","cd6556b3":"markdown","e9ed6f64":"markdown","7d0c4882":"markdown","a060bbfe":"markdown","b54067f6":"markdown","bf79536c":"markdown","c6bce2ee":"markdown","f515d9c1":"markdown","b058e662":"markdown","245e939d":"markdown","391649b2":"markdown"},"source":{"9a94a0bc":"import os\nimport sys\n\nimport pandas as pd\nimport numpy as np\nimport datetime\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.metrics import mean_absolute_error, r2_score\nfrom lightgbm import LGBMRegressor\n\nimport plotly.plotly as py\nimport plotly.graph_objs as go\nfrom plotly.offline import  init_notebook_mode, iplot\ninit_notebook_mode()\n\nimport shap\n\nDATA_DIR = '..\/input'\nRANDOM_STATE = 212\n\npd.options.display.max_columns = 60\npd.options.display.float_format = '{0:.2f}'.format\n\nsns.set_style('darkgrid') \n\n%config InlineBackend.figure_format = 'retina'\n%matplotlib inline","f1d45a16":"def shape(df):\n    return '{:,} rows - {:,} columns'.format(df.shape[0], df.shape[1])","6af66a83":"data = pd.read_csv('{}\/{}'.format(DATA_DIR, 'train.csv'))\ndata.columns = data.columns.str.lower()\nshape(data)","d18cc9b2":"data.head()","4894d5e1":"def plot_hist(x, title):\n    \n    fig, ax = plt.subplots(figsize=(13,7))\n    formatter = plt.FuncFormatter(lambda x, y: '{:,.2f}'.format(x))\n    \n    ax.yaxis.set_major_formatter(formatter=formatter)\n    ax.xaxis.set_major_formatter(formatter=formatter)\n\n    ax.set_title(title)\n    sns.distplot(x, bins=50, kde=False, ax=ax);","f774b579":"print('The average winning percentile is {:.3f}, the median is {:.3f}'.format(data.winplaceperc.mean(), data.winplaceperc.median()))","17906e22":"plot_hist(data.winplaceperc, title='Histogram of winning percentiles')","22ac57c6":"data = data.assign(match_mean=data.groupby('matchid').winplaceperc.transform('mean'))\ndata = data.assign(match_median = data.groupby('matchid').winplaceperc.transform('median'))","5c286efc":"print('The average match winning percentile is {:.2f}, the median is {:,.2f}'.format(data.winplaceperc.mean(), data.winplaceperc.median()))","b6b22720":"plot_hist(data.match_mean, title='Histogram of average match winning percentiles');\nplot_hist(data.match_median, title='Histogram of median match winning percentiles');","2a9232da":"data = data.assign(team_size=data.groupby('groupid').groupid.transform('count'))\ndata = data.assign(max_team_size=data.groupby('matchid').team_size.transform('max'))\ndata= data.assign(match_size=data.groupby('matchid').id.transform('nunique'))","e2e2b721":"plot_hist(data.match_size, title='Distribution of players per game')","7907a3f8":"print('The largest team has {} team members'.format(data.max_team_size.max()))","45f10741":"plot_hist(data.team_size, title='Histogram of team sizes')\nplt.xlim(0,12);","4991979d":"data[data.max_team_size == 2].team_size.value_counts()","a1360884":"plot_hist(data.max_team_size, title='Distribution of maximum team size')\nplt.xlim(0,20);","eb263626":"data =  data.assign(team_indicator = data.team_size.apply(lambda x: 5 if x>= 5 else x))","f9c4fcf8":"data = pd.get_dummies(data, columns=['team_indicator'])\ndummy_cols = ['team_indicator_{}'.format(i) for i in np.arange(1,6)]\ndata[dummy_cols] = data.groupby('matchid')[dummy_cols].transform('mean')\ndata.head()","f54fdd89":"plot_hist(data[data.max_team_size==2].team_indicator_1, title='Distribution of solo teams density where maximum team size is 2')","a75b101d":"data.loc[data.team_indicator_1 >= 0.7, 'game_mode'] = 'solo'\ndata.loc[data.team_indicator_2 >= 0.6, 'game_mode'] = 'duo'\ndata.loc[(data.team_indicator_3 + data.team_indicator_4) >= 0.5, 'game_mode'] = 'squad'\n\ndata.game_mode = np.where((data.team_indicator_5 >= 0.2), 'custom', data.game_mode)","07f4ec54":"data.game_mode = data.game_mode.fillna('custom')","4fb83ee1":"data[dummy_cols+['game_mode']].sample(15)","0cfbaf5c":"print('The average winning percentile for regular games is {:.3f}, the median is {:.3f}'.format(data[data.game_mode!='custom'].winplaceperc.mean(), data[data.game_mode!='custom'].winplaceperc.median()))\nprint('The average winning percentile for custom games is {:.3f}, the median is {:.3f}'.format(data[data.game_mode=='custom'].winplaceperc.mean(), data[data.game_mode=='custom'].winplaceperc.median()))\n\nplot_hist(data[data.game_mode != 'custom'].winplaceperc, title = 'Histogram of winning percentiles scores for regular games')\nplot_hist(data[data.game_mode == 'custom'].winplaceperc, title = 'Histogram of winning percentiles scores for custom games')","83cbadf1":"data['max_possible_kills'] = data.match_size - data.team_size\ndata['total_distance'] = data.ridedistance + data.swimdistance + data.walkdistance\ndata['total_items_acquired'] = data.boosts + data.heals + data.weaponsacquired\ndata['items_per_distance'] =  data.total_items_acquired\/data.total_distance\ndata['items_per_distance'] =  data.total_items_acquired\/data.total_distance\ndata['kills_per_distance'] = data.kills\/data.total_distance\ndata['knocked_per_distance'] = data.dbnos\/data.total_distance\ndata['damage_per_distance'] = data.damagedealt\/data.total_distance\ndata['headshot_kill_rate'] = data.headshotkills\/data.kills\ndata['max_kills_by_team'] = data.groupby('groupid').kills.transform('max')\ndata['total_team_damage'] = data.groupby('groupid').damagedealt.transform('sum')\ndata['total_team_kills'] =  data.groupby('groupid').kills.transform('sum')\ndata['total_team_items'] = data.groupby('groupid').total_items_acquired.transform('sum')\ndata['pct_killed'] = data.kills\/data.max_possible_kills\ndata['pct_knocked'] = data.dbnos\/data.max_possible_kills\ndata['pct_team_killed'] = data.total_team_kills\/data.max_possible_kills\ndata['team_kill_points'] = data.groupby('groupid').killpoints.transform('sum')\ndata['team_kill_rank'] = data.groupby('groupid').killplace.transform('mean')\ndata['max_kills_match'] = data.groupby('matchid').kills.transform('max')\ndata['total_kills_match'] = data.groupby('matchid').kills.transform('sum')\ndata['total_distance_match'] = data.groupby('matchid').total_distance.sum()\ndata['map_has_sea'] =  data.groupby('matchid').swimdistance.transform('sum').apply(lambda x: 1 if x>0 else 0)\ndata.fillna(0, inplace=True)","4b60a379":"def plot_interactions(df, feature_list, hue_labels=None, sample_size=10000):\n    \n    '''\n    Target to decile should be first\n    '''\n    sample_df = df.sample(sample_size)\n    sample_df.team_size = sample_df.team_size.apply(lambda x: 5 if x>= 5 else x)\n    \n    colors = pd.qcut(sample_df[feature_list[0]], q=10, labels=np.arange(1,11)).astype(int)\n    colorscale = 'RdBu'\n    \n    trace = [go.Parcoords(\n        line = dict(color=colors, colorscale = colorscale),\n        dimensions = list([dict(range = [np.round(sample_df[i].quantile(0.01)*0.9, decimals=1),\n                                         np.round(sample_df[i].quantile(0.99)*1.1, decimals=1)],\n                                label = str(i),\n                                values = sample_df[i]) for i in feature_list]))]\n\n    fig = go.Figure(data=trace)\n    iplot(fig)","4e45b5e0":"features = ['winplaceperc', 'walkdistance', 'damagedealt', 'boosts', 'total_items_acquired', 'revives']\nplot_interactions(df=data, feature_list=features)","474e7b57":"EXCLUDE_COLS = ['id', 'match_mean', 'match_median', 'team_indicator_5', 'game_mode']\nCATEGORICAL_COLS = ['matchid', 'groupid']\nTARGET = 'winplaceperc'\nTRAIN_SIZE = 0.9\nEARLY_STOP_ROUNDS = 10","4366f9b6":"df = data[data.game_mode != 'custom'].drop(EXCLUDE_COLS, axis=1)\ndf[CATEGORICAL_COLS] =  df[CATEGORICAL_COLS].astype('category')\nshape(df)","1ed05f47":"def train_validation(df, train_size=TRAIN_SIZE):\n    \n    unique_games = df.matchid.unique()\n    train_index = round(int(unique_games.shape[0]*train_size))\n    \n    np.random.shuffle(unique_games)\n    \n    train_id = unique_games[:train_index]\n    validation_id = unique_games[train_index:]\n    \n    train = df[df.matchid.isin(train_id)]\n    validation = df[df.matchid.isin(validation_id)]\n    \n    return train, validation\n    \ntrain, validation = train_validation(df)","cec4919d":"train_weights = (1\/train.team_size)\nvalidation_weights = (1\/validation.team_size)","13ee3ee1":"X_train = train.drop(TARGET,axis=1)\nX_test = validation.drop(TARGET, axis=1)\n\ny_train = train[TARGET]\ny_test = validation[TARGET]\n\nshape(X_train), shape(X_test)","5708d8a2":"time_0 = datetime.datetime.now()\n\nlgbm = LGBMRegressor(objective='mae', n_estimators=250,  \n                     learning_rate=0.3, num_leaves=200, \n                     n_jobs=-1,  random_state=RANDOM_STATE, verbose=0)\n\nlgbm.fit(X_train, y_train, sample_weight=train_weights,\n         eval_set=[(X_test, y_test)], eval_sample_weight=[validation_weights], \n         eval_metric='mae', early_stopping_rounds=EARLY_STOP_ROUNDS, \n         verbose=0)\n\ntime_1  = datetime.datetime.now()\n\nprint('Training took {} seconds. Best iteration is {}'.format((time_1 - time_0).seconds, lgbm.best_iteration_))","a96ed173":"print('Mean Absolute Error is {:.5f}'.format(mean_absolute_error(y_test, lgbm.predict(X_test, num_iteration=lgbm.best_iteration_), sample_weight=validation_weights)))\nprint('R2 score is {:.2%}'.format(r2_score(y_test, lgbm.predict(X_test, num_iteration=lgbm.best_iteration_), sample_weight=validation_weights)))","77c9b34e":"def plot_training(lgbm):\n    \n    fig, ax = plt.subplots(figsize=(13,7))\n    losses = lgbm.evals_result_['valid_0']['l1']\n    ax.set_ylim(np.max(losses), 0)\n    ax.set_xlim(0,100)\n    ax.set_xlabel('n_estimators')\n    ax.set_ylabel('Mean Asbolute Error')\n    ax.set_title('Evolution of MAE over training iterations')\n    ax.plot(losses, color='grey');\n    \nplot_training(lgbm)","54c12ad8":"results = validation.copy()\nresults = results.assign(predicted_player_rank=lgbm.predict(X_test, num_iteration=lgbm.best_iteration_))\nprint('The minimum predicted ranking is {}, the maximum is {}'.format(results.predicted_player_rank.min(), results.predicted_player_rank.max()))","ffcdf7b4":"results.predicted_player_rank = results.predicted_player_rank.clip(0, 1)\nprint('The minimum predicted ranking is {}, the maximum is {}'.format(results.predicted_player_rank.min(), results.predicted_player_rank.max()))","9be880c7":"print('R2 score is {:.2%}'.format(r2_score(y_test, results.predicted_player_rank, sample_weight=validation_weights)))\nprint('Mean Absolute Error is {:.5f}'.format(mean_absolute_error(y_test, results.predicted_player_rank, sample_weight=validation_weights)))","1d6ce55b":"results = results.assign(predicted_team_rank_max=results.groupby('groupid').predicted_player_rank.transform('max'))\nresults = results.assign(predicted_team_rank_mean=results.groupby('groupid').predicted_player_rank.transform('mean'))\n\nprint('Using team maximum predicted ranking:')\nprint('R2 score is {:.2%}'.format(r2_score(y_test, results.predicted_team_rank_max.clip(0, 1), sample_weight=validation_weights)))\nprint('Mean Absolute Error is {:.5f}'.format(mean_absolute_error(y_test, results.predicted_team_rank_max, sample_weight=validation_weights)))\n\nprint('\\nUsing team average predicted ranking:')\nprint('R2 score is {:.2%}'.format(r2_score(y_test, results.predicted_team_rank_mean.clip(0, 1), sample_weight=validation_weights)))\nprint('Mean Absolute Error is {:.5f}'.format(mean_absolute_error(y_test, results.predicted_team_rank_mean, sample_weight=validation_weights)))\n","f272db8d":"sns.jointplot(y_test, results.predicted_team_rank_mean,\n              kind='reg', height=12,\n              xlim=(-0.1, 1.1), ylim=(-0.1, 1.1),\n              color='darkred', scatter_kws={'edgecolor':'w'}, line_kws={'color':'black'});\nplt.title('Actual Ranking vs Predicted Ranking');","576bdae8":"shap.initjs()\n\nSAMPLE_SIZE = 10000\nSAMPLE_INDEX = np.random.randint(0, X_test.shape[0], SAMPLE_SIZE)\n\nX = X_test.iloc[SAMPLE_INDEX]\n\nexplainer = shap.TreeExplainer(lgbm)\nshap_values = explainer.shap_values(X)","8aa02fd8":"shap.summary_plot(shap_values, X, plot_type='bar', color='lightblue')","2e8edde3":"shap.summary_plot(shap_values, X)","7475a440":"interactions = ['assists', 'boosts', 'damagedealt', 'heals', 'longestkill', 'walkdistance', 'revives']\nfeatures = ['team_kill_rank'] * len(interactions)\n\nfor i, j in zip(features, interactions):\n    shap.dependence_plot(i, shap_values, X, interaction_index=j);\n","b8ef45cd":"**Fixing our predictions:**\n\nAs mentioned earlier, the algorithm used optimizes for MAE and won't know the upper and lower bound inherent to percentiles. We most likely will have predictions below 0 and above 1, which we must correct (if we were to increase training time, this would more strongly mitigated). Furthermore each players in a team will most likely get different rankings while it should be identical.","d6fde122":"The aim of this notebook differs slightly from your traditional kaggle kernel, where submissions tend to be about more computational power, latest NN implementations, and fishing for data leaks. \n\nHere we approach the matter at hand with a business\/data science methodology, trying to understand what the problem is and how we should model it.\n\nNo more youtube hack videos, hail the power of data and become your best player!","2d888f4a":"The above graph seems to corroborate the fact that max_team_size is not very indicative of game mode. A recorded max_team_size of 2 consists mostly of solo games!\n\nConsequently, we probably want to use the team size densities in order to define our game modes. The following filters are not statistically derived but should be reasonnable. Anything that falls outside the filter's scope is considered custom.\n\n**Note:** Ideally we would like to use unsupervised learning in this case trying to derive clusters of game modes. Improving how you well you define regular games will improve your model's accuracy. Would love to hear some ideas!","02ab50d7":"The sample results classification looks reasonnable.\n\nLet's isolate the regular games and compare the winning percentile distribution against custom games!","ef53424e":"**Looking at team and match sizes:**\n\nTeam size and game sizes are not given metrics but they can be easily derived from the groupid and matchid.","a8120f88":"**Context:**\n\n*You are given over 65,000 games' worth of anonymized player data, split into training and testing sets, and asked to predict final placement from final in-game stats and initial player ratings.*\n\n*What's the best strategy to win in PUBG? Should you sit in one spot and hide your way into victory, or do you need to be the top shot? Let's let the data do the talking!*","9b614864":"# Testing our Assumptions\n[Return to top](#TABLE-OF-CONTENT)\n\n![assumptions](https:\/\/i.ytimg.com\/vi\/gdvxapI9b_0\/maxresdefault.jpg)","e51552b9":"#### TABLE OF CONTENT\n\n* [Introduction](#Introduction:)\n* [Back to School](#Back-to-School)\n* [Testing our Assumptions](#Testing-our-Assumptions)\n* [Transforming our Dataset](#Transforming-our-Dataset)\n* [Feature Engineering](#Feature-Engineering)\n* [Machine Learning - Training Grounds](#Machine-Learning---Training-Grounds)\n* [Interpretation](#Interpretation)\n* [Next Steps](#Next-Steps)\n\n\n","62af77e8":"My first approach was to define the game mode using maximum team size:\n    - solo if max_team_size == 1 \n    - duo if max_team_size == 2\n    - squad if max_team_size == 4\n    - custom if max_team_size > 4\n    \nHowever this approach was somewhat inconclusive as some games would have only a single occurrence of a larger team distorting the labels.","458dc32e":"**Data Dictionary:**\n- **DBNOs** - Number of enemy players knocked.\n- **assists** - Number of enemy players this player damaged that were killed by teammates.\n- **boosts** - Number of boost items used.\n- **damageDealt** - Total damage dealt. Note: Self inflicted damage is subtracted.\n- **headshotKills** - Number of enemy players killed with headshots.\n- **heal** - Number of healing items used.\n- **killPlace** - Ranking in match of number of enemy players killed.\n- **killPoints** - Kills-based external ranking of player. (Think of this as an Elo ranking where only kills matter.)\n- **killStreaks** - Max number of enemy players killed in a short amount of time.\n- **kills** - Number of enemy players killed.\n- **longestKill** - Longest distance between player and player killed at time of death. This may be misleading, as downing a player and driving away may lead to a large longestKill stat.\n- **matchId** - Integer ID to identify match. There are no matches that are in both the training and testing set.\n- **revives** - Number of times this player revived teammates.\n- **rideDistance** - Total distance traveled in vehicles measured in meters.\n- **roadKills** - Number of kills while in a vehicle.\n- **swimDistance** - Total distance traveled by swimming measured in meters.\n- **teamKills** - Number of times this player killed a teammate.\n- **vehicleDestroys** - Number of vehicles destroyed.\n- **walkDistance** - Total distance traveled on foot measured in meters.\n- **weaponsAcquired** - Number of weapons picked up.\n- **winPoints** - Win-based external ranking of player. (Think of this as an Elo ranking where only winning matters.)\n- **groupId** - Integer ID to identify a group within a match. If the same group of players plays in different matches, they will have a different groupId each time.\n- **numGroups** - Number of groups we have data for in the match.\n- **maxPlace** - Worst placement we have data for in the match. This may not match with numGroups, as sometimes the data skips over placements.\n- **winPlacePerc** - The target of prediction. This is a percentile winning placement, where 1 corresponds to 1st place, and 0 corresponds to last place in the match. It is calculated off of maxPlace, not numGroups, so it is possible to have missing chunks in a match.","979aa493":"![pubg](http:\/\/media.comicbook.com\/2018\/01\/pubg-4-1070817.jpeg)","cc3ee5f2":"**Feature Importance:**","eb32c8bc":"The goal of this machine learning exercise is to identify what features make or break player ranks. We need to focus our attention on games that are consistent across time and mitigate irregularities.\n\nIn due order, we must first find out if we are in a regular game or a custom game.","9ade607f":"From the above, we can see that our percentile distribution isn't exactly as uniform as we expected.There seems to be more cases occuring at low percentiles. This indicates that overall, we have more losers than winners, why is that? \n\nThe higher counts at the percentile extremes is to be expected. There is **always** some someone getting the percentiles scores of zero and 1, the rest of the scores varying across different matches.\n\nLet's consider the average winning percentile per match and look at the distribution. As mentioned earlier, we should expect a normal distirbution with mean 0.5","f5272ad4":"# Back to School\n[Return to top](#TABLE-OF-CONTENT)\n\n![school](https:\/\/zilliongamer.com\/uploads\/pubg-mobile\/map\/all-maps\/school-and-apartment.PNG)","99f8d77d":"Interrestingly, the mean distribution is somewhat right-tailed. Why is it that certain games have lower average percentiles, is there anything odd about these games?","dc0966c8":"The first question we must ask ourselves is what are we trying to optimize and what is the math?\nLet's look at our problem statement and our target\n\n> **Problem Statement:** *You must create a model which predicts **players' finishing placement** based on their final stats, on a scale from 1 (first place) to 0 (last place)*\n\n> **Target variable:** *winPlacePerc - This is a **percentile winning placement**, where 1 corresponds to 1st place, and 0 corresponds to last place in the match. It is calculated off of maxPlace, not numGroups, so it is possible to have missing chunks in a match.*\n\n\nSo our goal is to **rank** players and our target variable is a **percentile**. \n\nThese two bits are critical pieces of information: the goal tells us what kind of **ML technique** we want to use while the target definition tells us about its **distribution**.","eeaba3de":"We generate match statistics per game, returning the percentage split of team sizes\n\nThe new fields represent the team size densities per game.\n","78fb1991":"Luckily this graph turns out as expected. We see the frequency is highest when approaching 100 players which is our maximum size. As such, we wouldn't expect the earlier graphs to be due to a lack of players in a game. \n\nIt is probably due to team imbalance.","faf0101b":"**Configuration:**","81fe35d3":"# Transforming our Dataset\n[Return to top](#TABLE-OF-CONTENT)","3b26d8b7":"**Challenge:**\n\n*In a PUBG game, up to 100 players start in each match (matchId). Players can be on teams (groupId) which get ranked at the end of the game (winPlacePerc) based on how many other teams are still alive when they are eliminated. In game, players can pick up different munitions, revive downed-but-not-out (knocked) teammates, drive vehicles, swim, run, shoot, and experience all of the consequences -- such as falling too far or running themselves over and eliminating themselves.*\n\n*You are provided with a large number of anonymized PUBG game stats, formatted so that each row contains one player's post-game stats. The data comes from matches of all types: solos, duos, squads, and custom; there is no guarantee of there being 100 players per match, nor at most 4 player per group.*\n\n*You must create a model which predicts players' finishing placement based on their final stats, on a scale from 1 (first place) to 0 (last place).*\n","0385e831":"**How do I get the dub?**\n\nThe below chart is interactive, feel free to try to play around and identify trends! Maybe you can find the key to winning!\n\nFeel free to fork the notebook and try different features!","4490d7b1":"**Reading Data:**","61ebb0b9":"We can see that higher distance, more boosts, lower kill rank, lower team kill rank, and lower kills per distance travelled lead to higher ranking! Nothing surprising here\n\nMore interesting fields include number of kills, number of kills as a percentage of game size and whether the map has a sea or not.\n- Individual kills don't seem to be particularily correlated with higher rankings, there is more higher ranked players\/teams with lower kill counts\n- The lower your number of kills as a percentage of the match size, the higher ranked you can get.\n- If the doesn't have a sea you are more likely to be ranked lower. You shall not swim away!\n\n","d347e675":"**Defining and fitting our model:**","1e02dde6":"**Defining our sample weights:**\n\nThis is quite important and I would love to hear suggestions and ideas.\n\nEach player in a team will share the same percentile ranking, which we must account for. Having a team of 4 and a team of 3 in one game only accounts for 2 unique scores but 7 observations. Using the team size as a weight should help with that.\n\nAnother thing to consider is incorporating the fact that there are only so many teams in one game. I was thinking predictions should also be weighed by the number of teams or match size but haven't come up with a definite answer, would love some feedback.","199a957d":"My **big assumption** here is that you can have regular games where a player disconnects while still in the lobby, getting replaced by a second player who will inherit the groupid under a different user id\n\nMy second approach was to look at the prevalence of team sizes across a singular game. The logic here would be that in a for each mode, the majority of participants will be concentraded in team sizes of 1, 2, 3-4, and 5+ as follows:\n\n    - solo if majority of the game's team_size is 1\n    - duo if majority of the game's team_size is 2\n    - squad if majority of the game's team_size is 3 or 4\n    - custom if the majority of the game's team_size is more than 4\n    \n    \nAnother approach worth considering would be looking at number of teams per game.","94bedf57":"**Looking at target distribution:**","9818cf55":"# Introduction\n[Return to top](#TABLE-OF-CONTENT)","a1de7162":"# ML Workflow - Data Science Approach","0c0e13d3":"**Conclusion: The PUBG Commandments**\n\n- You shall assist your teammates\n- You shall boost yourself\n- You shall deal a lot of damage\n- You shall heal yourself\n- You shall kill from afar\n- You shall walk a lot\n- You shall revive the fallen ones\n\nand victory shall be yours!","1813aca1":"**Generating train and validation sets:**","73890833":"We're almost ready to start modelling, we'll create a few additional features to help LGBM find interactions more easily.","cbac42ca":"# Interpretation\n[Return to top](#TABLE-OF-CONTENT)","cd6556b3":"Interestingly we can see that there are teams bigger than what your traditional squad can contain. It is possible that due to this team imbalance, the percentiles scores get distorted. \n\nFor example, if a team of 50 ends up first or last (assuming the other teams are smaller), the average percentile ranking will be twisted towards the extremes. As such our original assumptions won't apply.\n\nReading through some of the kernels and discussions, it's been established that a good proportion of the games are custom games where team imbalance is more pronounced.\n\n**Here is the hypothesis:** the regular games will be much more statistically driven than the custom games. If we can isolate regular games from custom ones, the data should lend itself to modelling better and the feature importances should be much more meaningful in ranking your day-to-day player!","e9ed6f64":"The results are quite strongs and our predicted distribution matches the actual distribution quite well.\n\nInterestingly, the model is missing out on a lot of winners. The good news is: some players in the above the 60th percentile share the same characteristics as players in the top percentile - you might just be the chosen one!","7d0c4882":"Let's homogenize our team ratings","a060bbfe":" Distance, end game kill rank and team kill rank are the features that have the largest mean decrease in impurity. This seems very reasonnable.\n \n Let's look at feature values and how they correlate to the shap values. This will gives us an idea of what type of values contributes to a higher\/lower ranking","b54067f6":"**Importing Libraries:**","bf79536c":"**ML Technique:** What is the metric?\n\n\nIdeally, we would like to use a ranking algorithm like lambdarank\/lambdamart.\n\nLearning to Rank (LTR) is a class of techniques that apply supervised machine learning to solve ranking problems. Essentially, the ranking is transformed into a pairwise regression problem. The algorithm compares pairs of items and comes up with the optimal ordering for that pair, iterating through the different pairs to extrapolate with the final ranking of all items.\n\nThe business metric is the rank, not the MAE. Unfortunately, I've found the LGBM lambdarank implementation quite confusing and was unsuccessful at using it. As such we will optimize our algorithm using an MAE objective.\n\nLimitations using MAE:\n- The predictions are not bound, as such we can have some predictions falling above\/below our percentile range\n- The predictions are not going to be unique, you could have several players\/teams assigned same scores\n\n\n**Target variable:** Percentile\n\nIn a  game of PUBG, each team gets assigned a percentile value so there there should approximatly be the same cout of 0s, 0.5s, 1s etc. (there might be irregularities in the distribution due imbalance in team sizes or number of teams). We should expect to see a uniform distribution for all percentile scores value, and a gaussian distribution with mean 0.5 for the average percentile score per match. We want our target to mimic a uniform distribution as much as possible.\n\n","c6bce2ee":"# Next Steps\n[Return to top](#TABLE-OF-CONTENT)\n\n- Unsupervised learning for game mode classification. This approach would be stronger than the rule-based one defined here\n- Stronger EDA analysis for a more powerful outlier treatment (i.e. look at trends across teams, identifying individuals farming xp, cheaters, zombies?)\n- Additional feature engineering. Team statistics and match statistics are so important! The algorithm won't necessarily identify the relationship between the teams and the overall game if you don't help it.\n- Build a cross-validator that ensures that no match is split across different folds. This will prevent temporal leaks and inflated accuracy metrics.\n- Try the lambdarank\/lambdamart implementations in XGBoost, try quantile regression.\n- Second model built for the custom game mode. I'm thinking fitting a separate model for the custom games, or using some form of stacking including outputs from the \"regular games\" model.\n\nThank you and looking forward to some discussion!\n","f515d9c1":"Training is quite fast and the results are quite strong. While the decrease in impurity considerably slows down after a few iterations iterations, I've noticed that the results do continuously improve as you grow more estimators. If you are looking for higher accuracy (but longer computation), try reducing the learning rate and the number of leaves (beware of over-fitting!)","b058e662":"While we never got to the hoped-for 50th percentile average, we can clearly see that statistical difference between the regular and custom games. We were successful in segmenting the game modes, obtaining a more uniform distribution. Our earlier assumptions about regular games seem to hold quite well!","245e939d":"# Machine Learning - Training Grounds\n[Return to top](#TABLE-OF-CONTENT)\n\n![training_grounds](https:\/\/i.ytimg.com\/vi\/UZXM_JRwKpE\/maxresdefault.jpg)\n\nTime to train our model! \n\nIn the interest of speed and given the rather large nature of our dataset, no cross-validation or hyper-parameter tuning is in this notebook (although I have iterated through a few parameters with early stopping). \n\nIdeally we would want to use hyper-parameter optimization using a custom stratified cross-validator (one that would make sure teams don't get split-up!).\n","391649b2":"# Feature Engineering\n[Return to top](#TABLE-OF-CONTENT)"}}