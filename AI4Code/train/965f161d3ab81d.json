{"cell_type":{"580042a4":"code","c392c167":"code","45731cd2":"code","a435857d":"code","fd34b371":"code","d33d88cb":"code","0d8bd19e":"code","85117fa8":"code","60b520b6":"code","4b5d0685":"code","d685c776":"code","aaf7112e":"code","d65f0a5e":"code","36c47916":"code","76b2f1ea":"code","12e0042b":"code","666fee6b":"code","7387f2ad":"code","a7f0e7d3":"code","7bb0e2c8":"code","b63d74ae":"markdown","ae6221f5":"markdown","b66806b4":"markdown","59b984d5":"markdown","8fc43d96":"markdown","268b4c65":"markdown","aa84933a":"markdown","1806b7cc":"markdown","d0e02a84":"markdown","8bd69749":"markdown","9fd8300b":"markdown","4360f8b0":"markdown","48177eae":"markdown","9d26dc04":"markdown"},"source":{"580042a4":"!pip install ensemble-boxes","c392c167":"%matplotlib inline\n\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib import rcParams\nsns.set(rc={\"font.size\":9,\"axes.titlesize\":15,\"axes.labelsize\":9,\n            \"axes.titlepad\":11, \"axes.labelpad\":9, \"legend.fontsize\":7,\n            \"legend.title_fontsize\":7, 'axes.grid' : False})\nimport cv2\nimport json\nimport pandas as pd\nimport glob\nimport os.path as osp\nfrom path import Path\nimport datetime\nimport numpy as np\nfrom tqdm.auto import tqdm\nimport random\nimport shutil\nfrom sklearn.model_selection import train_test_split\n\nfrom ensemble_boxes import *\nimport warnings\nfrom collections import Counter\nfrom tqdm import tqdm\n\nimport pydicom\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut","45731cd2":"def read_xray(path, voi_lut = True, fix_monochrome = True, downscale_factor = 3):\n    dicom = pydicom.read_file(path)\n\n    # VOI LUT (if available by DICOM device) is used to transform raw DICOM data to \"human-friendly\" view\n    if voi_lut:\n        data = apply_voi_lut(dicom.pixel_array, dicom)\n    else:\n        data = dicom.pixel_array\n\n    # depending on this value, X-ray may look inverted - fix that:\n    if fix_monochrome and dicom.PhotometricInterpretation == \"MONOCHROME1\":\n        data = np.amax(data) - data\n\n    data = data - np.min(data)\n    data = data \/ np.max(data)\n    data = (data * 255.0).astype(np.uint8)\n    new_shape = tuple([int(x \/ downscale_factor) for x in data.shape])\n    data = cv2.resize(data, (new_shape[1], new_shape[0]))\n\n    return data\n\nftrain = os.listdir('..\/input\/vinbigdata-chest-xray-abnormalities-detection\/train')\nftest = os.listdir('..\/input\/vinbigdata-chest-xray-abnormalities-detection\/test')\n\n! mkdir .\/train\n! mkdir .\/test\n\nfor i in tqdm(range(len(ftrain))):\n    img = read_xray('..\/input\/vinbigdata-chest-xray-abnormalities-detection\/train\/'+ftrain[i])\n    cv2.imwrite('.\/train\/'+ftrain[i].replace('.dicom','.jpg'), img)\n\nfor i in tqdm(range(len(ftest))):\n    img = read_xray('..\/input\/vinbigdata-chest-xray-abnormalities-detection\/test\/'+ftest[i])\n    cv2.imwrite('.\/test\/'+ftest[i].replace('.dicom','.jpg'), img)\n    \n#! cp ..\/input\/vinbigdata-chest-xray-abnormalities-detection\/train.csv .\/","a435857d":"train_annotations = pd.read_csv(\".\/train.csv\")\ntrain_annotations.head(5)","fd34b371":"train_annotations = train_annotations[train_annotations.class_id!=14]\ntrain_annotations['image_path'] = train_annotations['image_id'].map(lambda x:os.path.join('.\/train', str(x)+'.jpg'))\ntrain_annotations.head(5)","d33d88cb":"imagepaths = train_annotations['image_path'].unique()\nprint(\"Number of Images with abnormalities:\",len(imagepaths))\nanno_count = train_annotations.shape[0]\nprint(\"Number of Annotations with abnormalities:\", anno_count)","0d8bd19e":"def plot_img(img, size=(18, 18), is_rgb=True, title=\"\", cmap='gray'):\n    plt.figure(figsize=size)\n    plt.imshow(img, cmap=cmap)\n    plt.suptitle(title)\n    plt.show()\n\ndef plot_imgs(imgs, cols=2, size=10, is_rgb=True, title=\"\", cmap='gray', img_size=None):\n    rows = len(imgs)\/\/cols + 1\n    fig = plt.figure(figsize=(cols*size, rows*size))\n    for i, img in enumerate(imgs):\n        if img_size is not None:\n            img = cv2.resize(img, img_size)\n        fig.add_subplot(rows, cols, i+1)\n        plt.imshow(img, cmap=cmap)\n    plt.suptitle(title)\n    \ndef draw_bbox(image, box, label, color):   \n    alpha = 0.1\n    alpha_box = 0.4\n    overlay_bbox = image.copy()\n    overlay_text = image.copy()\n    output = image.copy()\n\n    text_width, text_height = cv2.getTextSize(label.upper(), cv2.FONT_HERSHEY_SIMPLEX, 0.6, 1)[0]\n    cv2.rectangle(overlay_bbox, (box[0], box[1]), (box[2], box[3]),\n                color, -1)\n    cv2.addWeighted(overlay_bbox, alpha, output, 1 - alpha, 0, output)\n    cv2.rectangle(overlay_text, (box[0], box[1]-7-text_height), (box[0]+text_width+2, box[1]),\n                (0, 0, 0), -1)\n    cv2.addWeighted(overlay_text, alpha_box, output, 1 - alpha_box, 0, output)\n    cv2.rectangle(output, (box[0], box[1]), (box[2], box[3]),\n                    color, thickness)\n    cv2.putText(output, label.upper(), (box[0], box[1]-5),\n            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1, cv2.LINE_AA)\n    return output","85117fa8":"labels =  [\n            \"__ignore__\",\n            \"Aortic_enlargement\",\n            \"Atelectasis\",\n            \"Calcification\",\n            \"Cardiomegaly\",\n            \"Consolidation\",\n            \"ILD\",\n            \"Infiltration\",\n            \"Lung_Opacity\",\n            \"Nodule\/Mass\",\n            \"Other_lesion\",\n            \"Pleural_effusion\",\n            \"Pleural_thickening\",\n            \"Pneumothorax\",\n            \"Pulmonary_fibrosis\"\n            ]\nviz_labels = labels[1:]","60b520b6":"# map label_id to specify color\n#label2color = [[random.randint(0,255) for i in range(3)] for class_id in viz_labels]\nlabel2color = [[59, 238, 119], [222, 21, 229], [94, 49, 164], [206, 221, 133], [117, 75, 3],\n                 [210, 224, 119], [211, 176, 166], [63, 7, 197], [102, 65, 77], [194, 134, 175],\n                 [209, 219, 50], [255, 44, 47], [89, 125, 149], [110, 27, 100]]\n\nthickness = 3\nimgs = []\n\nfor img_id, path in zip(train_annotations['image_id'][:6], train_annotations['image_path'][:6]):\n\n    boxes = train_annotations.loc[train_annotations['image_id'] == img_id,\n                                  ['x_min', 'y_min', 'x_max', 'y_max']].values\n    img_labels = train_annotations.loc[train_annotations['image_id'] == img_id, ['class_id']].values.squeeze()\n    \n    img = cv2.imread(path)\n    \n    for label_id, box in zip(img_labels, boxes):\n        color = label2color[label_id]\n        img = draw_bbox(img, list(np.int_(box)), viz_labels[label_id], color)\n    imgs.append(img)\n\nplot_imgs(imgs, size=9, cmap=None)\nplt.show()","4b5d0685":"iou_thr = 0.5\nskip_box_thr = 0.0001\nviz_images = []\nsigma = 0.1\n\nfor i, path in tqdm(enumerate(imagepaths[5:8])):\n    img_array  = cv2.imread(path)\n    image_basename = Path(path).stem\n    print(f\"(\\'{image_basename}\\', \\'{path}\\')\")\n    img_annotations = train_annotations[train_annotations.image_id==image_basename]\n\n    boxes_viz = img_annotations[['x_min', 'y_min', 'x_max', 'y_max']].to_numpy().tolist()\n    labels_viz = img_annotations['class_id'].to_numpy().tolist()\n    \n    print(\"Bboxes before WBF:\\n\", boxes_viz)\n    print(\"Labels before WBF:\\n\", labels_viz)\n    \n    ## Visualize Original Bboxes\n    img_before = img_array.copy()\n    for box, label in zip(boxes_viz, labels_viz):\n        x_min, y_min, x_max, y_max = (box[0], box[1], box[2], box[3])\n        color = label2color[int(label)]\n        img_before = draw_bbox(img_before, list(np.int_(box)), viz_labels[label], color)\n    viz_images.append(img_before)\n    \n    boxes_list = []\n    scores_list = []\n    labels_list = []\n    weights = []\n    \n    boxes_single = []\n    labels_single = []\n    \n    cls_ids = img_annotations['class_id'].unique().tolist()\n    count_dict = Counter(img_annotations['class_id'].tolist())\n    print(count_dict)\n\n    for cid in cls_ids:       \n        ## Performing Fusing operation only for multiple bboxes with the same label\n        if count_dict[cid]==1:\n            labels_single.append(cid)\n            boxes_single.append(img_annotations[img_annotations.class_id==cid][['x_min', 'y_min', 'x_max', 'y_max']].to_numpy().squeeze().tolist())\n\n        else:\n            cls_list =img_annotations[img_annotations.class_id==cid]['class_id'].tolist()\n            labels_list.append(cls_list)\n            bbox = img_annotations[img_annotations.class_id==cid][['x_min', 'y_min', 'x_max', 'y_max']].to_numpy()\n            ## Normalizing Bbox by Image Width and Height\n            bbox = bbox\/(img_array.shape[0], img_array.shape[1], img_array.shape[0], img_array.shape[1])\n            bbox = np.clip(bbox, 0, 1)\n            boxes_list.append(bbox.tolist())\n            scores_list.append(np.ones(len(cls_list)).tolist())\n\n            weights.append(1)\n            \n\n    # Perform WBF\n    boxes, scores, box_labels= weighted_boxes_fusion(boxes_list, scores_list, labels_list, weights=weights,\n                                                     iou_thr=iou_thr, skip_box_thr=skip_box_thr)\n    \n    \n    boxes = boxes*(img_array.shape[0], img_array.shape[1], img_array.shape[0], img_array.shape[1])\n    boxes = boxes.round(1).tolist()\n    box_labels = box_labels.astype(int).tolist()\n\n    boxes.extend(boxes_single)\n    box_labels.extend(labels_single)\n    \n    print(\"Bboxes after WBF:\\n\", boxes)\n    print(\"Labels after WBF:\\n\", box_labels)\n    \n    ## Visualize Bboxes after operation\n    img_after = img_array.copy()\n    for box, label in zip(boxes, box_labels):\n        color = label2color[int(label)]\n        img_after = draw_bbox(img_after, list(np.int_(box)), viz_labels[label], color)\n    viz_images.append(img_after)\n    print()\n        \nplot_imgs(viz_images, cmap=None)\nplt.figtext(0.3, 0.9,\"Original Bboxes\", va=\"top\", ha=\"center\", size=25)\nplt.figtext(0.73, 0.9,\"WBF\", va=\"top\", ha=\"center\", size=25)\nplt.savefig('wbf.png', bbox_inches='tight')\nplt.show()","d685c776":"random.seed(42)\n## 42 -  The Answer to the Ultimate Question of Life\nrandom.shuffle(imagepaths)\ntrain_len = round(0.75*len(imagepaths))\ntrain_paths = imagepaths[:train_len]\nval_paths = imagepaths[train_len:]\n\nprint(\"Split Counts\\nTrain Images:\\t\\t{0}\\nVal Images:\\t\\t{1}\"\n      .format(len(train_paths), len(val_paths)))","aaf7112e":"now = datetime.datetime.now()\n\ndata = dict(\n    info=dict(\n        description=None,\n        url=None,\n        version=None,\n        year=now.year,\n        contributor=None,\n        date_created=now.strftime('%Y-%m-%d %H:%M:%S.%f'),\n    ),\n    licenses=[dict(\n        url=None,\n        id=0,\n        name=None,\n    )],\n    images=[\n        # license, url, file_name, height, width, date_captured, id\n    ],\n    type='instances',\n    annotations=[\n        # segmentation, area, iscrowd, image_id, bbox, category_id, id\n    ],\n    categories=[\n        # supercategory, id, name\n    ],\n)","d65f0a5e":"class_name_to_id = {}\nfor i, each_label in enumerate(labels):\n    class_id = i - 1  # starts with -1\n    class_name = each_label\n    if class_id == -1:\n        assert class_name == '__ignore__'\n        continue\n    class_name_to_id[class_name] = class_id\n    data['categories'].append(dict(\n        supercategory=None,\n        id=class_id,\n        name=class_name,\n    ))","36c47916":"train_output_dir = \".\/vinbigdata_coco_chest_xray\/train_images\"\nval_output_dir = \".\/vinbigdata_coco_chest_xray\/val_images\"\n\nif not osp.exists(train_output_dir):\n    os.makedirs(train_output_dir)\n    print('Coco Train Image Directory:', train_output_dir)\n    \nif not osp.exists(val_output_dir):\n    os.makedirs(val_output_dir)\n    print('Coco Val Image Directory:', val_output_dir)","76b2f1ea":"warnings.filterwarnings(\"ignore\", category=UserWarning)","12e0042b":"## Setting the output annotations json file path\ntrain_out_file = '.\/vinbigdata_coco_chest_xray\/train_annotations.json'\n\ndata_train = data.copy()\ndata_train['images'] = []\ndata_train['annotations'] = []","666fee6b":"iou_thr = 0.5\nskip_box_thr = 0.0001\nviz_images = []\n\nfor i, path in tqdm(enumerate(train_paths)):\n    img_array  = cv2.imread(path)\n    image_basename = Path(path).stem\n#     print(f\"(\\'{image_basename}\\', \\'{path}\\')\")\n    \n    ## Copy Image \n    shutil.copy2(path, train_output_dir)\n    \n    ## Add Images to annotation\n    data_train['images'].append(dict(\n        license=0,\n        url=None,\n        file_name=os.path.join('train_images', image_basename+'.jpg'),\n        height=img_array.shape[0],\n        width=img_array.shape[1],\n        date_captured=None,\n        id=i\n    ))\n    \n    img_annotations = train_annotations[train_annotations.image_id==image_basename]\n    boxes_viz = img_annotations[['x_min', 'y_min', 'x_max', 'y_max']].to_numpy().tolist()\n    labels_viz = img_annotations['class_id'].to_numpy().tolist()\n    \n    ## Visualize Original Bboxes every 500th\n    if (i%500==0):\n        img_before = img_array.copy()\n        for box, label in zip(boxes_viz, labels_viz):\n            x_min, y_min, x_max, y_max = (box[0], box[1], box[2], box[3])\n            color = label2color[int(label)]\n            img_before = draw_bbox(img_before, list(np.int_(box)), viz_labels[label], color)\n        viz_images.append(img_before)\n    \n    boxes_list = []\n    scores_list = []\n    labels_list = []\n    weights = []\n    \n    boxes_single = []\n    labels_single = []\n\n    cls_ids = img_annotations['class_id'].unique().tolist()\n    \n    count_dict = Counter(img_annotations['class_id'].tolist())\n\n    for cid in cls_ids:\n        ## Performing Fusing operation only for multiple bboxes with the same label\n        if count_dict[cid]==1:\n            labels_single.append(cid)\n            boxes_single.append(img_annotations[img_annotations.class_id==cid][['x_min', 'y_min', 'x_max', 'y_max']].to_numpy().squeeze().tolist())\n\n        else:\n            cls_list =img_annotations[img_annotations.class_id==cid]['class_id'].tolist()\n            labels_list.append(cls_list)\n            bbox = img_annotations[img_annotations.class_id==cid][['x_min', 'y_min', 'x_max', 'y_max']].to_numpy()\n            \n            ## Normalizing Bbox by Image Width and Height\n            bbox = bbox\/(img_array.shape[0], img_array.shape[1], img_array.shape[0], img_array.shape[1])\n            bbox = np.clip(bbox, 0, 1)\n            boxes_list.append(bbox.tolist())\n            scores_list.append(np.ones(len(cls_list)).tolist())\n            weights.append(1)\n    \n    ## Perform WBF\n    boxes, scores, box_labels = weighted_boxes_fusion(boxes_list=boxes_list, scores_list=scores_list,\n                                                  labels_list=labels_list, weights=weights,\n                                                  iou_thr=iou_thr, skip_box_thr=skip_box_thr)\n    \n    boxes = boxes*(img_array.shape[0], img_array.shape[1], img_array.shape[0], img_array.shape[1])\n    boxes = boxes.round(1).tolist()\n    box_labels = box_labels.astype(int).tolist()\n    boxes.extend(boxes_single)\n    box_labels.extend(labels_single)\n    \n    img_after = img_array.copy()\n    for box, label in zip(boxes, box_labels):\n        x_min, y_min, x_max, y_max = (box[0], box[1], box[2], box[3])\n        area = round((x_max-x_min)*(y_max-y_min),1)\n        bbox =[\n                round(x_min, 1),\n                round(y_min, 1),\n                round((x_max-x_min), 1),\n                round((y_max-y_min), 1)\n                ]\n        \n        data_train['annotations'].append(dict( id=len(data_train['annotations']), image_id=i,\n                                            category_id=int(label), area=area, bbox=bbox,\n                                            iscrowd=0))\n        \n    ## Visualize Bboxes after operation every 500th\n    if (i%500==0):\n        img_after = img_array.copy()\n        for box, label in zip(boxes, box_labels):\n            color = label2color[int(label)]\n            img_after = draw_bbox(img_after, list(np.int_(box)), viz_labels[label], color)\n        viz_images.append(img_after)\n\nplot_imgs(viz_images, cmap=None)\nplt.figtext(0.3, 0.9,\"Original Bboxes\", va=\"top\", ha=\"center\", size=25)\nplt.figtext(0.73, 0.9,\"WBF\", va=\"top\", ha=\"center\", size=25)\nplt.show()\n               \nwith open(train_out_file, 'w') as f:\n    json.dump(data_train, f, indent=4)","7387f2ad":"print(\"Number of Images in the Train Annotations File:\", len(data_train['images']))\nprint(\"Number of Bboxes in the Train Annotations File:\", len(data_train['annotations']))\n\nprint(\"Number of Images in the Val Annotations File:\", len(data_val['images']))\nprint(\"Number of Bboxes in the Val Annotations File:\", len(data_val['annotations']))\n\n# Should output\n# Number of Images in the Train Annotations File: 3296\n# Number of Bboxes in the Train Annotations File: 17815\n# Number of Images in the Val Annotations File: 1098\n# Number of Bboxes in the Val Annotations File: 5880","a7f0e7d3":"!find .\/vinbigdata_coco_chest_xray\/val_images -type f | wc -l\n# 1098","7bb0e2c8":"!find .\/vinbigdata_coco_chest_xray\/train_images -type f | wc -l\n# 3296","b63d74ae":"# Dicom to JPG\nThe downscale factor is a hyperparameter (note: this cell is too slow so try to run the cell only once then upload the data to google drive or keep on kaggle working directory)","ae6221f5":"# Visualize Original Boxes","b66806b4":"### Verify Annotations","59b984d5":"### Defining Structure","8fc43d96":"### Doing the COCO Conversion","268b4c65":"# Create COCO Dataset and Consolidate the Similar Annotations for Different Radiologists w\/ Weighted Boxes Fusion (WBF)\n\nNote: here we consolidate the different annotations by different radiologists for the same images, another option is to not consolidate but to replicate the entries once for each different annotation","aa84933a":"# Read the Dataset into a Dataframe","1806b7cc":"## Building COCO DATASET","d0e02a84":"# Credits\n* The code for converting from dicom to jpg is courtsey of [raddar](https:\/\/www.kaggle.com\/raddar\/vinbigdata-competition-jpg-data-3x-downsampled)\n* The code for visualization, different radiologists bbox consolidation, and coco conversion is courtsey of [sreevishnudamodaran](https:\/\/www.kaggle.com\/sreevishnudamodaran\/vinbigdata-fusing-bboxes-coco-dataset) (note: you can choose a different technoque to combine overlapping bboxes from different radiologists, I just chose weighted boxes fusion because it gave better results based on his visualizations) ","8bd69749":"## Visualize WBF","9fd8300b":"### Train and Val Split","4360f8b0":"# Helper Functions","48177eae":"# Class Definitions","9d26dc04":"> ### Creating Output Directories"}}