{"cell_type":{"41f884f4":"code","106793f6":"code","94f964ad":"code","b7957ea3":"code","2b38faa4":"code","a8f59421":"code","fbe23473":"code","6724be6b":"code","8b394095":"code","87e0f45b":"code","74583cd2":"code","a3aad6ed":"code","79b905d8":"code","e738c5f6":"code","be9d0434":"code","19f53d38":"code","d098fdb1":"code","bf7bda2a":"code","6244e1eb":"code","2aaf7c15":"code","3265ffce":"code","acf7dca6":"code","608b0ee3":"markdown","0b4a3463":"markdown","9554a292":"markdown","ef594eeb":"markdown","0663b18e":"markdown","fe200e8d":"markdown","f30216f7":"markdown","e4139538":"markdown","1eca2790":"markdown","91293b19":"markdown","66611b82":"markdown","0d6c67b8":"markdown","98d73b73":"markdown","2e195b89":"markdown"},"source":{"41f884f4":"import numpy as np\nimport pandas as pd\nfrom pathlib import Path\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, f1_score\n\nfrom matplotlib import pyplot as plt\n%config InlineBackend.figure_format = 'retina'","106793f6":"PATH_TO_DATA = Path('..\/input\/hierarchical-text-classification\/')","94f964ad":"train_df = pd.read_csv(PATH_TO_DATA \/ 'train_40k.csv').fillna(' ')\nvalid_df = pd.read_csv(PATH_TO_DATA \/ 'val_10k.csv').fillna(' ')","b7957ea3":"train_df.head()","2b38faa4":"train_df.info()","a8f59421":"train_df.loc[0, 'Text']","fbe23473":"train_df.loc[0, 'Cat1'], train_df.loc[0, 'Cat2'], train_df.loc[0, 'Cat3']","6724be6b":"train_df['Cat1'].value_counts()","8b394095":"train_df['Cat1_Cat2_Cat3'] = train_df['Cat1'] + '\/' + train_df['Cat2'] + '\/' + train_df['Cat3']\nvalid_df['Cat1_Cat2_Cat3'] = valid_df['Cat1'] + '\/' + valid_df['Cat2'] + '\/' + train_df['Cat3']","87e0f45b":"train_df['Cat1_Cat2_Cat3'].nunique()","74583cd2":"train_df['Cat1_Cat2_Cat3'].value_counts().head()","a3aad6ed":"# put a limit on maximal number of features and minimal word frequency\ntf_idf = TfidfVectorizer(max_features=50000, min_df=2)\n# multinomial logistic regression a.k.a softmax classifier\nlogit = LogisticRegression(C=1e2, n_jobs=4, solver='lbfgs', \n                           random_state=17, verbose=0, \n                           multi_class='multinomial',\n                           fit_intercept=True)\n# sklearn's pipeline\ntfidf_logit_pipeline = Pipeline([('tf_idf', tf_idf), \n                                 ('logit', logit)])","79b905d8":"%%time\ntfidf_logit_pipeline.fit(train_df['Title'], train_df['Cat1_Cat2_Cat3'])","e738c5f6":"%%time\nvalid_pred_level_3 = tfidf_logit_pipeline.predict(valid_df['Title'])","be9d0434":"valid_pred_level_1 = [el.split('\/')[0] for el in valid_pred_level_3]\nvalid_pred_level_2 = [el.split('\/')[1] for el in valid_pred_level_3]","19f53d38":"print(\"Level 1:\\n\\tF1 micro (=accuracy): {}\\n\\tF1 weighted:\\t      {}\".format(\n    f1_score(y_true=valid_df['Cat1'], y_pred=valid_pred_level_1, average='micro').round(3),\n    f1_score(y_true=valid_df['Cat1'], y_pred=valid_pred_level_1, average='weighted').round(3)\n    )\n)","d098fdb1":"print(\"Level 2:\\n\\tF1 micro (=accuracy): {}\\n\\tF1 weighted:\\t      {}\".format(\n    f1_score(y_true=valid_df['Cat2'], y_pred=valid_pred_level_2, average='micro').round(3),\n    f1_score(y_true=valid_df['Cat2'], y_pred=valid_pred_level_2, average='weighted').round(3)\n    )\n)","bf7bda2a":"print(\"Level 3:\\n\\tF1 micro (=accuracy): {}\\n\\tF1 weighted:\\t      {}\".format(\n    f1_score(y_true=valid_df['Cat1_Cat2_Cat3'], y_pred=valid_pred_level_3, average='micro').round(3),\n    f1_score(y_true=valid_df['Cat1_Cat2_Cat3'], y_pred=valid_pred_level_3, average='weighted').round(3)\n    )\n)","6244e1eb":"def plot_confusion_matrix(y_true, y_pred, classes,\n                          normalize=False,\n                          title='Confusion matrix', figsize=(7,7),\n                          cmap=plt.cm.Blues, path_to_save_fig=None):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    import itertools\n    cm = confusion_matrix(y_true, y_pred).T\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n    \n    plt.figure(figsize=figsize)\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=90)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('Predicted label')\n    plt.xlabel('True label')\n    \n    if path_to_save_fig:\n        plt.savefig(path_to_save_fig, dpi=300, bbox_inches='tight')","2aaf7c15":"plot_confusion_matrix(\n    y_true=valid_df['Cat1'],\n    y_pred=valid_pred_level_1, \n    classes=sorted(train_df['Cat1'].unique()),\n    figsize=(8, 8)\n)","3265ffce":"%%capture\nimport eli5","acf7dca6":"eli5.show_weights(\n    estimator=tfidf_logit_pipeline.named_steps['logit'],\n    vec=tfidf_logit_pipeline.named_steps['tf_idf'])","608b0ee3":"# <center> Classifying amazon product reviews with logistic regression\n## <center> Three levels of structured classes\n    \nWe are faced to a simple NLP problem \u2013 Amazon product reviews classification. But classes are structured, like in this picture. \n\n<img src=\"https:\/\/habrastorage.org\/webt\/nf\/en\/j7\/nfenj7gktep6dtbrtzgijcsdzwy.png\" width=40%\/>\n\nThat poses a question, what's the best way to approach this hierarchical text classification problem. \n\nHere we present a basic tf-idf + logreg baseline.\n\n**Idea**\n\nEach review has 3 labels which are elements of a taxonomy, eg. \n\n> 'The description and photo on this product needs to be changed to indicate this product is the BuffalOs version of this beef jerky.'\n\n> Category 1: `grocery gourmet food` \n\n> Category 2: `meat poultry`\n\n> Category 3: `jerky`\n\nFirst, we concatenate classes for each sample, eg. `grocery gourmet food\/meat poultry\/jerky`. Then we train the model and measure F1 score for Category 3.\n\nThen we split the prediction string and thus get predictions for Category 1 and Category 2:\n\n-  Category 3 prediction is `grocery gourmet food\/meat poultry\/jerky` --> Category 1 prediction is `grocery gourmet food`\n-  Category 3 prediction is `grocery gourmet food\/meat poultry\/jerky` --> Category 2 prediction is `meat poultry` \n\nAfter that we measure F1 scores for Category 1 and Category 2\n\n**Results:**\n\nF1 micro (=accuracy):\n- Category 1: **0.946**\n- Category 2: **0.886**\n- Category 3: **0.008**\n\nFail with Category 3, no enough data for that obviously, but the performance is decent for Category 1 and Category 2.\n\nPS. using \"level\" and \"category\" interchangeably here.","0b4a3463":"## Training the model\n\nWe are training our model only with review titles, a ciuple of experiments show that it works better than with review text. ","9554a292":"For evaluation, let's take a look at F1 score (micro and weigthed) at Level 1 and Level 2 separately. Note that in a multiclass setting F1 score with micro averaging is the same as accuracy.","ef594eeb":"Now we have 555 classes","0663b18e":"Most popular ones (at level 3) are:","fe200e8d":"Fields:\n\n* productId \u2013 the review is given about this product\n* Title - title of a review as given by the author\n* user - Iduser ID of the author of the review\n* Helpfulness - whether the review is found helpful by other users\n* Score - score of a review as rated by other users\n* Time - timestamp of the review\n* Text - text of a review","f30216f7":"That was a level 2 model. Now to predict level 1 as well we simple take the first part of level1\/level2 prediction. Eg. if 'health personal care\/health care' is predicted, then the level 1 prediction is 'health personal care'","e4139538":"We concatenate level 1, level 2 and level 3 classes, the model will be trained with these targets. It's very important that the model satisfies the class taxonomy. This way it never predicts contradicting, level 1 and level 2 classes (eg. 'pet supplies' as L1 and 'meat poultry' as L2 when actually 'meat poultry' is a sub-level of 'grocery gourmet food'), same for level 3 and level 2,  level 3 and level 1. ","1eca2790":"We can explore words\/ngrams, which a most indicative of different classes. With 555 classes it's overwhelming though.","91293b19":"## Reading and analyzing the data","66611b82":"## Explaining model predictions","0d6c67b8":"Distribution of level 1 classes","98d73b73":"Confusion matrix is quite balanced.","2e195b89":"Example of a review"}}