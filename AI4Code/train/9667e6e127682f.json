{"cell_type":{"91a547ea":"code","ddc39e0e":"code","997a7dc6":"code","a21c82ce":"code","a720ceb2":"code","16a6ccb1":"code","0b7b3e11":"code","d0c48c4e":"code","d80887ae":"code","2986d1ee":"code","c0631aeb":"code","69c162b3":"code","b62c9f4b":"code","6e672e36":"code","3cb1a3ed":"code","5ef20fe8":"code","11b2524b":"code","7796890e":"code","507ba934":"code","8c9b5ba9":"code","f77fd2e3":"markdown","7b22b6b4":"markdown","a3513e51":"markdown","f8402663":"markdown","7d31bf9a":"markdown","3a1f9b85":"markdown"},"source":{"91a547ea":"# [This part takes a while]\n\n# uninstall torchaudio and torchtext, which were causing a versioning issue\n%pip uninstall torchaudio torchtext -y\n\n# Install PyTorch\n%pip install -U \"torch==1.5.0+cu101\" \"torchvision==0.6.0+cu101\" -f \"https:\/\/download.pytorch.org\/whl\/torch_stable.html\"\n\n# Install MMCV (altered from tutorial)\n%pip install \"mmcv-full\" -f https:\/\/download.openmmlab.com\/mmcv\/dist\/cu101\/torch1.5.0\/index.html","ddc39e0e":"# clone the mmsegmentation repository, which provides the backbone upon which we will work\n# note that this installs mmsegmentation in \"dev mode\"\n!rm -rf .\/content\/mmsegmentation\n!git clone https:\/\/github.com\/open-mmlab\/mmsegmentation.git .\/content\/mmsegmentation\n%pip install -e \".\/content\/mmsegmentation\"","997a7dc6":"# install gdown to install the dataset\n!pip install gdown","a21c82ce":"# install the dataset from the tutorial as a zip file\n!gdown -O .\/content\/dataset.zip \"https:\/\/drive.google.com\/u\/0\/uc?id=1SlAcoPk_L-lhcy-g1_qPGIhuR1RXPUID&export=download\"","a720ceb2":"# unzip the downloaded dataset.  The root of the zip file only contains \n# a single folder, so we can just unzip into the \/content directory\n!unzip -o .\/content\/dataset.zip -d .\/content","16a6ccb1":"# remove the zip file\n!rm -rf .\/content\/dataset.zip","0b7b3e11":"# install tree for viewing directories\n!apt-get update && apt-get install tree","d0c48c4e":"# verify that everything is downloaded correctly\n# \/content\/ should have a folder \/content\/iccv09Data\/\n# with images and labels\n!tree --filelimit 5 --noreport .\/content\/iccv09Data\/","d80887ae":"import os.path as osp\nimport numpy as np\nfrom PIL import Image\nfrom mmcv import mkdir_or_exist, scandir\n\n# Set up paths\ndata_root = '.\/content\/iccv09Data'\nimg_dir = 'images'\nann_dir = 'labels'\n# [from article] define class and palette for better visualization\nclasses = ('sky', 'tree', 'road', 'grass', 'water', 'bldg', 'mntn', 'fg obj')\npalette = [\n  [128, 128, 128], \n  [129, 127, 38], \n  [120, 69, 125], \n  [53, 125, 34], \n  [0, 11, 123], \n  [118, 20, 12], \n  [122, 81, 25], \n  [241, 134, 51]]","2986d1ee":"# split the data into training and validation sets\n# this is done by writing the names of the images to text files\n# called \"train.txt\" and \"val.txt\", which will be placed at\n# \/content\/iccv09Data\/splits\nmkdir_or_exist(osp.join(data_root, 'splits'))\nfilename_list = [osp.splitext(filename)[0] for filename in scandir(\n    osp.join(data_root, ann_dir), suffix='.png')]\n\n# use 80% of the images for training and 20% for validation\ntrain_length = int(len(filename_list)*4\/5)\n\n# set images to be used for training\nwith open(osp.join(data_root, 'splits', 'train.txt'), 'w') as f:\n  f.writelines(line + '\\n' for line in filename_list[:train_length])\n\n# set images to be used for validation\nwith open(osp.join(data_root, 'splits', 'val.txt'), 'w') as f:\n  f.writelines(line + '\\n' for line in filename_list[train_length:])","c0631aeb":"# create a custom dataset abstraction, which can be referenced in the config file.\n#  See https:\/\/mmdetection.readthedocs.io\/en\/v2.18.1\/tutorials\/customize_dataset.html\n\n# needed to add this install here again. not sure why, but it works\n!pip install \"git+https:\/\/github.com\/open-mmlab\/mmsegmentation.git\"\n\nfrom mmseg.datasets.builder import DATASETS\nfrom mmseg.datasets.custom import CustomDataset\n\n@DATASETS.register_module()\nclass StanfordBackgroundDataset(CustomDataset):\n  CLASSES = classes\n  PALETTE = palette\n\n  def __init__(self, split, **kwargs):\n    super().__init__(\n      img_suffix='.jpg', \n      seg_map_suffix='.png', \n      split=split, \n      **kwargs)\n    \n    # img_dir and split are required fields\n    assert osp.exists(self.img_dir) and self.split is not None","69c162b3":"from mmcv import Config\n\n# create the config based on pspnet, since that's the pre-trained model we will be using\nconfig = Config.fromfile(\".\/content\/mmsegmentation\/configs\/pspnet\/pspnet_r50-d8_512x1024_40k_cityscapes.py\")","b62c9f4b":"# install the PSPNet checkpoint file\n# obtained from https:\/\/colab.research.google.com\/github\/open-mmlab\/mmsegmentation\/blob\/master\/demo\/MMSegmentation_Tutorial.ipynb#scrollTo=2hd41IGaiNet&line=2&uniqifier=1\n!mkdir \".\/content\/checkpoints\"\n!wget \"https:\/\/download.openmmlab.com\/mmsegmentation\/v0.5\/pspnet\/pspnet_r50-d8_512x1024_40k_cityscapes\/pspnet_r50-d8_512x1024_40k_cityscapes_20200605_003338-2966598c.pth\" -P \".\/content\/checkpoints\"","6e672e36":"# dynamically create a config file so that we can use local variables\nfrom mmseg.apis import set_random_seed\n\n# since we use ony one GPU, BN is used instead of SyncBN\n# Note that BN refers to batch normalization.  The difference between BN and SyncBN is \n# explained here: https:\/\/paperswithcode.com\/method\/syncbn\nconfig.norm_cfg = dict(type='BN', requires_grad=True)\nconfig.model.backbone.norm_cfg = config.norm_cfg\nconfig.model.decode_head.norm_cfg = config.norm_cfg\nconfig.model.auxiliary_head.norm_cfg = config.norm_cfg\n# modify num classes of the model in decode\/auxiliary head\nconfig.model.decode_head.num_classes = 8\nconfig.model.auxiliary_head.num_classes = 8\n\n# [article] Modify dataset type and path\nconfig.dataset_type = 'StanfordBackgroundDataset'\nconfig.data_root = data_root\n\nconfig.data.samples_per_gpu = 8\nconfig.data.workers_per_gpu = 8\n\n# normalization information per channel (RGB)\n# I guess these were computed for the dataset originally?\nconfig.img_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\nconfig.crop_size = (256, 256)\n\n# define the operations to perform on each image in the training set\nconfig.train_pipeline = [\n  dict(type='LoadImageFromFile'),\n  dict(type='LoadAnnotations'),\n  dict(type='Resize', img_scale=(320, 240), ratio_range=(0.5, 2.0)),\n  dict(type='RandomCrop', crop_size=config.crop_size, cat_max_ratio=0.75),\n  dict(type='RandomFlip', flip_ratio=0.5),\n  dict(type='PhotoMetricDistortion'),\n  dict(type='Normalize', **config.img_norm_cfg),\n  dict(type='Pad', size=config.crop_size, pad_val=0, seg_pad_val=255),\n  dict(type='DefaultFormatBundle'),\n  dict(type='Collect', keys=['img', 'gt_semantic_seg']),\n]\n\n# define the operations to perform on each image in the testing set\nconfig.test_pipeline = [\n  dict(type='LoadImageFromFile'),\n  dict(\n    type='MultiScaleFlipAug',\n    img_scale=(320, 240),\n    # [article] img_ratios=[0.5, 0.75, 1.0, 1.25, 1.5, 1.75],\n    flip=False,\n    transforms=[\n      dict(type='Resize', keep_ratio=True),\n      dict(type='RandomFlip'),\n      dict(type='Normalize', **config.img_norm_cfg),\n      dict(type='ImageToTensor', keys=['img']),\n      dict(type='Collect', keys=['img']),\n  ])\n]\n\n\n# training config\nconfig.data.train.type = config.dataset_type\nconfig.data.train.data_root = config.data_root\nconfig.data.train.img_dir = img_dir\nconfig.data.train.ann_dir = ann_dir\nconfig.data.train.pipeline = config.train_pipeline\nconfig.data.train.split = 'splits\/train.txt'\n\n# validation config\nconfig.data.val.type = config.dataset_type\nconfig.data.val.data_root = config.data_root\nconfig.data.val.img_dir = img_dir\nconfig.data.val.ann_dir = ann_dir\nconfig.data.val.pipeline = config.test_pipeline\nconfig.data.val.split = 'splits\/val.txt'\n\n# testing config (identical to validation lol)\nconfig.data.test.type = config.dataset_type\nconfig.data.test.data_root = config.data_root\nconfig.data.test.img_dir = img_dir\nconfig.data.test.ann_dir = ann_dir\nconfig.data.test.pipeline = config.test_pipeline\nconfig.data.test.split = 'splits\/val.txt'\n\n# use pspnet as our pre-trained model\nconfig.load_from = '.\/content\/checkpoints\/pspnet_r50-d8_512x1024_40k_cityscapes_20200605_003338-2966598c.pth'\n\n# Set up working dir to save files and logs.\nconfig.work_dir = '.\/content\/tutorial'\n\nconfig.total_iters = 200\nconfig.log_config.interval = 10\nconfig.evaluation.interval = 200\nconfig.checkpoint_config.interval = 200\n\n# Set rng seed to facitate reproducing the result\nconfig.seed = 0\nset_random_seed(0, deterministic=False)\nconfig.gpu_ids = range(1)\n\n# bug fixing?  See https:\/\/github.com\/open-mmlab\/mmsegmentation\/issues\/490#issuecomment-825602414\nconfig.lr_config = dict(policy='poly', power=0.9, min_lr=0.0001, by_epoch=False)\n\n# reduce the number of iterations so I don't get cancelled again\nconfig.runner = dict(type='IterBasedRunner', max_iters=1000)\n\n# Let's have a look at the final config used for training\nprint(f'Config:\\n{config.pretty_text}')","3cb1a3ed":"# import some functions\n\nfrom mmseg.apis import (\n    inference_segmentor, # (model, images): applies the model to segment the images\n    init_segmentor, # (config, checkpoint?, device?): creates a model from a config file\n    train_segmentor, # (model, dataset, config): trains the model\n    show_result_pyplot # (model, image, result, ...): show the segments of the image\n)\n\nfrom mmseg.datasets import build_dataset # (config): build the dataset\nfrom mmseg.models import build_segmentor # (config): build the segmentor model\n\nfrom mmseg.core.evaluation import get_palette # (dataset): gets the \"class palette\" (?) of the dataset","5ef20fe8":"# build the training dataset\ndatasets = [build_dataset(config.data.train)]\n\n# build the segmentor\nmodel = build_segmentor(\n    config.model,\n    #train_cfg=config.train_cfg,\n    #test_cfg=config.test_cfg\n)\n\n# create the working directory (which is already an absolute path)\nmkdir_or_exist(config.work_dir)\n\n# train the model\ntrain_segmentor(model, datasets, config, validate=True, meta=dict())","11b2524b":"!ls .\/content\/tutorial\/","7796890e":"from mmcv import imread\n\n# read from an image\nimg = imread(f'{data_root}\/images\/6000124.jpg')\n\nmodel.cfg = config\n\n# Add an attribute for visualization convenience\nmodel.CLASSES = datasets[0].CLASSES\n\nresult = inference_segmentor(model, img)\n\n# show result in window\nmodel.show_result(img, result, palette=palette)\n# download result to file.  Can't do both at once >:(\nmodel.show_result(img, result, palette=palette, show=False, out_file=\".\/content\/tutorial\/output.png\")\nshow_result_pyplot(model, img, result, palette=palette)","507ba934":"!identify .\/content\/tutorial\/output.png","8c9b5ba9":"# see https:\/\/www.kaggle.com\/general\/62279\n# from IPython.display import FileLink\n# import os\n\n# (\n# # download the output file\n# FileLink(\".\/content\/tutorial\/output.png\"),\n\n# # download the checkpoints\n# FileLink(\".\/content\/tutorial\/latest.pth\")\n# )","f77fd2e3":"# Install Libraries\n\nIn order to train and run the segmentation, we need [pytorch with CUDA](https:\/\/pytorch.org\/get-started\/previous-versions\/#v150), mmcv (specifically [mmcv-full](https:\/\/github.com\/open-mmlab\/mmcv#installation)), and [mmsegmentation](https:\/\/github.com\/open-mmlab\/mmsegmentation\/blob\/master\/docs\/en\/get_started.md#installation).","7b22b6b4":"# Training and Evaluation\n\nThis step is the culmination of all of the other steps.  It combines the dataset we have prepared with the model for which we have created the configuration in order to actually train the model and perform an image segmentation.\n\n*Question: in theory, I could use a pretrained model without doing any additional training in order to get a preliminary result, right?  If there was a pretrained model for something like SETR, then, would I just be able to use that and be done?*","a3513e51":"# About\n\nThis notebook is a follow-along of the guide [Semantic Segmentation using mmsegmentation](https:\/\/mducducd33.medium.com\/sematic-segmentation-using-mmsegmentation-bcf58fb22e42), which gives a basic walkthrough of using mmsegmentation to create a segmentation network.\n\nI am learning how to use mmsegmentation because they have collated a variety of different segmentation algorithms and I would like to be able to use them interchangeably as part of my research project (which isn't nearly as easy as it sounds).","f8402663":"# Create Config File\n\nThe config file is a file which specified the actual structure of a network.  An explanation of what they are, how to use them, and some examples with comments can be found at the [MMDetection documentation](https:\/\/mmdetection.readthedocs.io\/en\/latest\/tutorials\/config.html).\n\nConfig files are typically created by extending a base.  This example uses PSPnet with `max_iteration = 40000` as a base, but other config files (as well as checkpoint files!) to use as bases can be found in the [MMSegmentation repository](https:\/\/github.com\/open-mmlab\/mmsegmentation\/tree\/master\/configs) (this list includes SETR!).\n\nA significant part of the config files is the data pipelines, which are operations performed on the data before it is used for training or testing, which include things like introducing photometric distortion with `PhotoMetricDistortion`, cropping the images with `RandomCrop`, and resizing with `Resize`.  All of these operations are defined on the object `mmseg.datasets.pipelines` (or in the [`transforms.py` file in the mmsegmentation GitHub repo](https:\/\/github.com\/open-mmlab\/mmsegmentation\/blob\/master\/mmseg\/datasets\/pipelines\/transforms.py)), but it is possible to create custom ones, like what the article did with `StanfordBackgroundDataset`.","7d31bf9a":"# Install and Set Up Dataset\n\nThe dataset that the article used is the [Stanford Background Dataset](http:\/\/dags.stanford.edu\/projects\/scenedataset.html), which has a bunch of small, mostly-outdoors images taken from various other public datasets.  The dataset will be installed in the `\/content\/dataset` folder, since that seems like a reasonable place for it.  The tutorial didn't specify how to actually install the data...","3a1f9b85":"# Note to put in Notion\n\nPer the source code for [`train_segmentor()`](https:\/\/mmsegmentation.readthedocs.io\/en\/latest\/_modules\/mmseg\/apis\/train.html#train_segmentor), it always expects to be able to use CUDA.  This would explain why it wasn't working in the docker container.  The docker containers never had access to a gpu, so naturally wouldn't be compatible with anything from open-mmlab (since they expect to have CUDA and a GPU available). "}}