{"cell_type":{"d27b69f4":"code","66acd015":"code","41eadd23":"code","dac21b22":"code","74030028":"code","1e7c1a02":"code","071d9269":"code","54c32887":"code","b30d44c0":"code","2969d376":"code","1f30f376":"code","840b7762":"code","a8d3a92d":"code","71a48522":"code","75d2f822":"code","e37eb211":"code","8d50b8be":"code","3ed775ff":"code","8af61ae7":"code","0f18021c":"code","58c1861b":"code","7e569788":"code","9c2a1173":"code","3c999f9a":"code","e7663856":"code","3f55c8f5":"code","a26412c2":"code","5513d14d":"markdown","e63bc349":"markdown","80fb3553":"markdown","fba50554":"markdown","b440516f":"markdown","e57cd171":"markdown","fe3dc21a":"markdown","8cb8f405":"markdown","9298a9de":"markdown"},"source":{"d27b69f4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('darkgrid')\nimport warnings\nwarnings.filterwarnings('ignore')\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","66acd015":"df= pd.read_csv('\/kaggle\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv')\ndf.head()","41eadd23":"df.info()","dac21b22":"df.describe()","74030028":"sns.countplot(df['quality'])","1e7c1a02":"i=1\nplt.figure(figsize=(15,20))\nfor col in df.columns:\n    plt.subplot(6,2,i)\n    sns.distplot(df[col])\n    i+=1\n    ","071d9269":"i=1\nplt.figure(figsize=(15,20))\nfor col in df.columns:\n    plt.subplot(6,2,i)\n    sns.barplot(x=df['quality'], y= df[col])\n    i+=1","54c32887":"sns.pairplot(df, hue='quality')","b30d44c0":"df['Quality']=0\ndf.loc[df['quality']>6, 'Quality']=1\ndf.head()","2969d376":"sns.countplot(df['Quality'])","1f30f376":"df['Quality'].value_counts()\ndf.drop('quality', axis=1, inplace=True)","840b7762":"from sklearn.utils import resample, shuffle\n\nzero= df[df['Quality']==0]\nones= df[df['Quality']==1]\n\nupsampled= resample(ones, replace=True, n_samples=zero.shape[0])\n\ndf_new= pd.concat([zero, upsampled])\ndf_new= shuffle(df_new)","a8d3a92d":"sns.countplot(df_new['Quality'])","71a48522":"sns.pairplot(df_new, hue='Quality')","75d2f822":"from sklearn.model_selection import train_test_split\nX= df_new.drop('Quality', axis=1)\ny= df_new['Quality']\n\nX_train, X_test, y_train,y_test= train_test_split(X,y, test_size=0.2)","e37eb211":"from sklearn.preprocessing import StandardScaler\n\nss=StandardScaler()\n\nX_train= ss.fit_transform(X_train.values)\nX_test= ss.transform(X_test.values)\nX_train= pd.DataFrame(X_train, columns= X.columns)\nX_test=pd.DataFrame(X_test, columns=X.columns)\nX_train.head()","8d50b8be":"X_test.head()","3ed775ff":"plt.figure(figsize=(12,12))\nsns.heatmap(df.corr(), annot=True, cmap='coolwarm')","8af61ae7":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.svm import SVC","0f18021c":"key= ['KNeighborsClassifier', 'LogisticRegression', 'RandomForestClassifier', 'GaussianNB', 'DecisionTreeClassifier', 'XGBClassifier', 'SVC']\nvalue= [KNeighborsClassifier(), LogisticRegression(), RandomForestClassifier(), GaussianNB(), DecisionTreeClassifier(), XGBClassifier(), SVC()]\n\nmodels= dict(zip(key,value))","58c1861b":"training_scores= []\ntesting_scores=[]\n\nfor key, value in models.items():\n    value.fit(X_train, y_train)\n    train_score= value.score(X_train,  y_train)\n    test_score= value.score(X_test, y_test)\n    training_scores.append(train_score)\n    testing_scores.append(test_score)\n    \n    print(f\"{key}\\n\")\n    print(f\"Training Score: {train_score}\" )\n    print(f\"Testing Score: {test_score} \\n\")","7e569788":"from sklearn.model_selection import cross_val_score\ncv_scores= []\n\nfor key, value in models.items():\n    cvs=cross_val_score(value, X,y, cv=5)\n    \n    cv_scores.append(cvs.mean())\n    print(f\"{key}\\n\")\n    print(f\"CV Score: {cvs.mean()} \\n\" )","9c2a1173":"from sklearn.metrics import confusion_matrix, classification_report\n\nrfc=RandomForestClassifier(random_state=42)\nrfc.fit(X_train,y_train)\ny_pred= rfc.predict(X_test)\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test,y_pred))","3c999f9a":"from sklearn.model_selection import GridSearchCV\n\nparams= {'n_estimators':[10,100,200],\n        'max_depth':[10,50,100,150],\n        'min_weight_fraction_leaf':[0, 0.1,0.01],\n        'ccp_alpha':[0, 0.01,0.1]}\n\ngrid=GridSearchCV(rfc, param_grid=params, cv=5, verbose=1)","e7663856":"grid.fit(X,y)","3f55c8f5":"grid.best_params_","a26412c2":"best= grid.best_estimator_\nbest.fit(X_train, y_train)\ny_pred=best.predict(X_test)\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test,y_pred))","5513d14d":"# Scaling ","e63bc349":"# Oversampling\nThe data is still skewed so we use Sklearn's resample option to generate more data from existing data","80fb3553":"# Hyperparameter Tuning\nRandomForestClassifier seems to be the best model for this data","fba50554":"# Dividing the Dataset\n* Since the total amount of data we have is very low, we will divide our wines as good, or bad.\n* All wines with quality 7 or above are good, rest are bad","b440516f":"# Final Results","e57cd171":"# Upvote and Comment if you liked my notebook :)","fe3dc21a":"# Load Data","8cb8f405":"# Exploratory Data Analysis","9298a9de":"# Training our Models"}}