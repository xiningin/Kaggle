{"cell_type":{"fff4d917":"code","8c4f6c5c":"code","31b6d26c":"code","a267d43e":"code","96a096c1":"code","030766a4":"code","d71dc0da":"code","93937c6e":"code","e654f166":"code","dcee2005":"code","69a24539":"code","9028c6ce":"code","3f757cbe":"code","b02c9a97":"code","3420837f":"code","e90111d0":"code","b7fab7a5":"code","7fcd9f0c":"code","79e113fb":"code","09c68ab0":"code","58c652ab":"code","e4f3b600":"code","a635b060":"code","db9acc53":"code","9f078e8c":"code","2ffdb2cc":"code","5632fdf4":"code","c325ffc9":"code","21d8b182":"code","da701195":"code","19970695":"code","948bdc22":"code","9c630867":"code","ba1cea09":"code","771ec228":"code","339c2edb":"code","51ed3c89":"code","5fee83bf":"code","b19d7035":"code","53e0446f":"code","ee274623":"code","05942ed4":"code","bed834cf":"code","a8600465":"code","2ae32e1c":"code","abc1b765":"code","50437338":"code","bced9957":"code","f685a1a1":"code","5d7af55a":"code","c738e826":"code","c4450db9":"code","38b3500b":"code","dc565fce":"code","64124e60":"code","d286f4af":"code","b66ed313":"code","0a14b582":"code","278f87cd":"code","6d85c5b0":"code","876b72a0":"code","e016b9df":"markdown","33749026":"markdown","f0eb69d2":"markdown","eaae32a9":"markdown","e3526a0d":"markdown","68cc56b5":"markdown","3d57e770":"markdown","3be0f2a3":"markdown","ca15f9db":"markdown","7f50d08c":"markdown","a1df6f9e":"markdown","d2f25f5f":"markdown","059c9843":"markdown","6eee5abb":"markdown","70dd3b84":"markdown","b41e28d7":"markdown","40bac533":"markdown","6e9dc525":"markdown","3ee79f6c":"markdown","1e89c480":"markdown","677a0480":"markdown","4079abbd":"markdown","274a05f8":"markdown","4d1a0a18":"markdown","c5f51d06":"markdown","fe4d4450":"markdown","42873310":"markdown","d7e3c9bc":"markdown"},"source":{"fff4d917":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8c4f6c5c":"abalone_data = pd.read_csv('\/kaggle\/input\/abalone-age-classification\/abalone.csv')\nabalone_data.head()","31b6d26c":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm","a267d43e":"for col in abalone_data.columns:\n    print(\"column under review is:\",col)\n    print(abalone_data[col].describe())\n    if col =='sex':\n        print(abalone_data[col].value_counts())\n        continue\n    plt.figure()\n    sns.distplot(abalone_data[col],fit = norm,kde = False)\n    print(\"number of missing values are:\",abalone_data[col].isna().sum())","96a096c1":"sns.pairplot(abalone_data,diag_kind = 'kde')","030766a4":"abalone_data['remaining_weight'] = abalone_data['whole weight'] - \\\n                                   (abalone_data['shucked weight']+abalone_data['viscera weight']+\\\n                                    abalone_data['shell weight'])\nabalone_data['sex_M'] = abalone_data['sex'].apply(lambda x: (x=='M')*1.0)\nabalone_data['sex_F'] = abalone_data['sex'].apply(lambda x: (x=='F')*1.0)\nabalone_data['sex_I'] = abalone_data['sex'].apply(lambda x: (x=='I')*1.0)","d71dc0da":"abalone_data.head()","93937c6e":"from sklearn.model_selection import train_test_split as tts\nY = abalone_data['rings']\nX = abalone_data.drop('rings',axis = 1)\nX_train,X_test,Y_train,Y_test = tts(X,Y,test_size = 0.2)","e654f166":"X_train = X_train.drop('sex',axis = 1)\nX_test = X_test.drop('sex',axis = 1)","dcee2005":"from sklearn.linear_model import LinearRegression as Linreg\nfrom sklearn.metrics import r2_score as rsc\nregressor = Linreg()\nregressor.fit(X_train,Y_train)\npred_train = regressor.predict(X_train)\nprint(rsc(Y_train,pred_train))\npred_test = regressor.predict(X_test)\nprint(rsc(Y_test,pred_test))","69a24539":"plt.figure()\nsns.distplot(pred_train,fit = norm,kde = False)\nplt.figure()\nsns.distplot(Y_train,fit = norm,kde = False)","9028c6ce":"from sklearn.metrics import mean_squared_error as mse\nprint('train rmse is:',mse(Y_train,pred_train,squared = False))\nprint(\"test rmse is:\",mse(Y_test,pred_test,squared = False))","3f757cbe":"small_df = pd.DataFrame()\nsmall_df['actual_rings'] = Y_train\nsmall_df['predicted_rings'] = pred_train\nsns.scatterplot(data = small_df,x='actual_rings',y = 'predicted_rings')","b02c9a97":"abalone_data.head()\nprep_data = abalone_data.drop('sex',axis = 1)\nprep_data.head()","3420837f":"col_dict = {'whole weight':'whole_weight',\n            'shucked weight':'shucked_weight',\n            'viscera weight':'viscera_weight',\n            'shell weight': 'shell_weight'}\nprep_data = prep_data.rename(columns = col_dict)","e90111d0":"prep_data.head()","b7fab7a5":"import statsmodels.formula.api as smf\nfit = smf.ols('rings ~ length+diameter+whole_weight+shucked_weight+shell_weight+remaining_weight+sex_M+sex_F', \n              data=prep_data).fit()\nprint(fit.summary())\nfrom statsmodels.compat import lzip\nimport statsmodels.stats.api as sms\n\n#perform Bresuch-Pagan test\nnames = ['Lagrange multiplier statistic', 'p-value',\n        'f-value', 'f p-value']\ntest = sms.het_breuschpagan(fit.resid, fit.model.exog)\n\nprint(lzip(names, test))\n\n#ideal result type:\n#[('Lagrange multiplier statistic', 6.003951995818433),\n# ('p-value', 0.11141811013399583),\n# ('f-value', 3.004944880309618),\n# ('f p-value', 0.11663863538255281)]","7fcd9f0c":"fit = smf.ols('rings ~ diameter+shucked_weight+shell_weight+remaining_weight+sex_M+sex_F+sex_I', \n              data=prep_data).fit()\nprint(fit.summary())","79e113fb":"prep_data['area'] = (3.14\/4)*prep_data['diameter']**2\nprep_data['volume'] = (3.14\/6)*prep_data['diameter']**3 ","09c68ab0":"fit = smf.ols('rings ~ diameter+shucked_weight+shell_weight+remaining_weight+sex_M+sex_F+sex_I+area+volume', \n              data=prep_data).fit()\nprint(fit.summary())","58c652ab":"fit = smf.ols('rings ~ diameter+shucked_weight+shell_weight+remaining_weight+sex_I+area', \n              data=prep_data).fit()\nprint(fit.summary())","e4f3b600":"pred_train = fit.predict(prep_data)","a635b060":"print(mse(prep_data['rings'],pred_train,squared = False))","db9acc53":"more_data = prep_data\nmore_data['log_rings'] = np.log(more_data['rings']+1)\nmore_data = more_data.drop('rings',axis = 1)","9f078e8c":"fit = smf.ols('log_rings ~ diameter+shucked_weight+shell_weight+remaining_weight+sex_I+area', \n              data=more_data).fit()\nprint(fit.summary())","2ffdb2cc":"small_df = pd.DataFrame()\nsmall_df['log_rings'] = more_data['log_rings']\nsmall_df['predicted_log_rings'] = fit.predict(more_data)\nsns.scatterplot(data = small_df,x='log_rings',y = 'predicted_log_rings')","5632fdf4":"names = ['Lagrange multiplier statistic', 'p-value',\n        'f-value', 'f p-value']\ntest = sms.het_breuschpagan(fit.resid, fit.model.exog)\n\nprint(lzip(names, test))","c325ffc9":"from scipy.stats import norm\nsns.distplot(more_data['log_rings'],fit = norm)","21d8b182":"fit = smf.ols('log_rings ~ diameter+shucked_weight+shell_weight+remaining_weight+sex_I+area', \n              data=more_data[more_data['log_rings']<2]).fit()\nprint(fit.summary())","da701195":"names = ['Lagrange multiplier statistic', 'p-value',\n        'f-value', 'f p-value']\ntest = sms.het_breuschpagan(fit.resid, fit.model.exog)\n\nprint(lzip(names, test))","19970695":"fit = smf.ols('log_rings ~ diameter+shucked_weight+shell_weight+remaining_weight+sex_I+area', \n              data=more_data[more_data['log_rings']>=1.5]).fit()\nprint(fit.summary())","948bdc22":"names = ['Lagrange multiplier statistic', 'p-value',\n        'f-value', 'f p-value']\ntest = sms.het_breuschpagan(fit.resid, fit.model.exog)\n\nprint(lzip(names, test))","9c630867":"small_df = pd.DataFrame()\nsmall_df['log_rings'] = more_data[more_data['log_rings']>=1.5]['log_rings']\nsmall_df['predicted_log_rings'] = fit.predict(more_data[more_data['log_rings']>=1.5])\nsns.scatterplot(data = small_df,x='log_rings',y = 'predicted_log_rings')","ba1cea09":"fit = smf.ols('log_rings ~ diameter+shucked_weight+shell_weight+remaining_weight+sex_I+area', \n              data=more_data[more_data['log_rings']>=2.75]).fit()\nprint(fit.summary())","771ec228":"small_df = pd.DataFrame()\nsmall_df['log_rings'] = more_data[more_data['log_rings']>=2.75]['log_rings']\nsmall_df['predicted_log_rings'] = fit.predict(more_data[more_data['log_rings']>=2.75])\nsns.scatterplot(data = small_df,x='log_rings',y = 'predicted_log_rings')","339c2edb":"import statsmodels.api as sm\nglm = smf.glm('log_rings ~ diameter+shucked_weight+shell_weight+remaining_weight+sex_I+area',\n              data=more_data, family=sm.families.Poisson())\nres_f = glm.fit()\nprint(res_f.summary())","51ed3c89":"small_df = pd.DataFrame()\nsmall_df['log_rings'] = more_data['log_rings']\nsmall_df['predicted_log_rings'] = res_f.predict(more_data)\nprint(mse(small_df['log_rings'],small_df['predicted_log_rings'],squared = False))\nsns.scatterplot(data = small_df,x='log_rings',y = 'predicted_log_rings')","5fee83bf":"names = ['Lagrange multiplier statistic', 'p-value',\n        'f-value', 'f p-value']\ntest = sms.het_breuschpagan(res_f.resid_deviance, res_f.model.exog)\n\nprint(lzip(names, test))","b19d7035":"import statsmodels.api as sm\nglm = smf.glm('log_rings ~ diameter+shucked_weight+shell_weight+remaining_weight+sex_I+area',\n              data=more_data, family=sm.families.NegativeBinomial())\nres_f = glm.fit()\nprint(res_f.summary())","53e0446f":"small_df = pd.DataFrame()\nsmall_df['log_rings'] = more_data['log_rings']\nsmall_df['predicted_log_rings'] = res_f.predict(more_data)\nprint(mse(small_df['log_rings'],small_df['predicted_log_rings'],squared = False))\nsns.scatterplot(data = small_df,x='log_rings',y = 'predicted_log_rings')","ee274623":"names = ['Lagrange multiplier statistic', 'p-value',\n        'f-value', 'f p-value']\ntest = sms.het_breuschpagan(res_f.resid_deviance, res_f.model.exog)\n\nprint(lzip(names, test))","05942ed4":"import statsmodels.api as sm\nexog = more_data.drop('log_rings',axis = 1)\nexog['constant'] = 1\nendog = more_data['log_rings']\nglm = sm.RLM(endog,exog,M=sm.robust.norms.HuberT())\nres_f = glm.fit()\nprint(res_f.summary())","bed834cf":"names = ['Lagrange multiplier statistic', 'p-value',\n        'f-value', 'f p-value']\ntest = sms.het_breuschpagan(res_f.resid, res_f.model.exog)\n\nprint(lzip(names, test))","a8600465":"small_df = pd.DataFrame()\nsmall_df['log_rings'] = more_data['log_rings']\nsmall_df['predicted_log_rings'] = res_f.predict(more_data)\nprint(mse(small_df['log_rings'],small_df['predicted_log_rings'],squared = False))\nsns.scatterplot(data = small_df,x='log_rings',y = 'predicted_log_rings')","2ae32e1c":"import statsmodels.api as sm\nexog = more_data[(more_data['log_rings']<=2.5) &\n                 (more_data['log_rings']>=1.5)].drop('log_rings',axis = 1)\nexog['constant'] = 1\nendog = more_data[(more_data['log_rings']<=2.5) &\n                  (more_data['log_rings']>=1.5)]['log_rings']\nglm = sm.RLM(endog,exog,M=sm.robust.norms.HuberT())\nres_f = glm.fit()\nprint(res_f.summary())","abc1b765":"small_df = pd.DataFrame()\nsmall_df['log_rings'] = more_data['log_rings']\nsmall_df['predicted_log_rings'] = res_f.predict(more_data)\nprint(mse(small_df['log_rings'],small_df['predicted_log_rings'],squared = False))\nsns.scatterplot(data = small_df,x='log_rings',y = 'predicted_log_rings')","50437338":"names = ['Lagrange multiplier statistic', 'p-value',\n        'f-value', 'f p-value']\ntest = sms.het_breuschpagan(res_f.resid, res_f.model.exog)\nprint(lzip(names, test))","bced9957":"better_data = more_data\nbetter_data['logger_rings'] = np.log(better_data['log_rings']+1)\nbetter_data = better_data.drop('log_rings',axis = 1)","f685a1a1":"import statsmodels.api as sm\nexog = better_data.drop('logger_rings',axis = 1)\nexog['constant'] = 1\nendog = better_data['logger_rings']\nglm = sm.RLM(endog,exog,M=sm.robust.norms.HuberT())\nres_f = glm.fit()\nprint(res_f.summary())","5d7af55a":"small_df = pd.DataFrame()\nsmall_df['logger_rings'] = better_data['logger_rings']\nsmall_df['predicted_logger_rings'] = res_f.predict(better_data)\nprint(mse(small_df['logger_rings'],small_df['predicted_logger_rings'],squared = False))\nsns.scatterplot(data = small_df,x='logger_rings',y = 'predicted_logger_rings')","c738e826":"names = ['Lagrange multiplier statistic', 'p-value',\n        'f-value', 'f p-value']\ntest = sms.het_breuschpagan(res_f.resid, res_f.model.exog)\nprint(lzip(names, test))","c4450db9":"from sklearn.model_selection import KFold\nkf = KFold(n_splits=5,random_state=4)\nlin_test = [] \nlin_prediction = []\nfor train_index,test_index in kf.split(more_data):    \n    X_train,X_test = more_data.iloc[train_index,:],more_data.iloc[test_index,:]\n    glm = smf.glm('log_rings ~ diameter+shucked_weight+shell_weight+remaining_weight+sex_I+area',\n              data=more_data, family=sm.families.Poisson())\n    fit = glm.fit()\n    print(fit.summary())    \n    pred_test = list(fit.predict(X_test))\n    y_test = X_test['log_rings'].tolist()\n    lin_prediction += [pred_test[x] for x in range(len(pred_test))]    \n    lin_test += [y_test[x] for x in range(len(y_test))] \nlin_test = [np.exp(y-1) for y in lin_test]\nlin_prediction = [np.exp(y-1) for y in lin_prediction]\nerror = mse(lin_test,lin_prediction) \nprint(error) # mean squared error \nprint(error**0.5) ","38b3500b":"## check the performance in classification\nimport math\ndef tune(x):\n    if x<0: return 0\n    floor = math.floor(x)\n    if x-floor>0.5:\n        return floor+1\n    return floor","dc565fce":"from sklearn.model_selection import KFold\nkf = KFold(n_splits=5,random_state=4)\ntest = [] \nprediction = []\nfor train_index,test_index in kf.split(more_data):    \n    X_train,X_test = more_data.iloc[train_index,:],more_data.iloc[test_index,:]\n    glm = smf.glm('log_rings ~ diameter+shucked_weight+shell_weight+remaining_weight+sex_I+area',\n              data=more_data, family=sm.families.Poisson())\n    fit = glm.fit()    \n    pred_test = list(fit.predict(X_test))\n    y_test = X_test['log_rings'].tolist()\n    prediction += [pred_test[x] for x in range(len(pred_test))]    \n    test += [y_test[x] for x in range(len(y_test))] \ntest = [np.exp(y-1) for y in test]\nprediction = [np.exp(y-1) for y in prediction]    \nprediction = [int(tune(x)) for x in prediction]\ntest = [int(tune(x)) for x in test]\nfrom sklearn.metrics import classification_report as clr\nprint(clr(prediction,test))","64124e60":"import tensorflow as tf\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers.experimental import preprocessing\n\nprint(tf.__version__)","d286f4af":"train_cols = list(prep_data.columns)\ntrain_cols.remove('rings')","b66ed313":"normalizer = preprocessing.Normalization()\nnormalizer.adapt(np.array(prep_data[train_cols]))\nabalone_model = tf.keras.Sequential([normalizer,layers.Dense(units=100),layers.Dense(units = 1)])\nabalone_model.summary()","0a14b582":"abalone_model.compile(\n    optimizer=tf.optimizers.Adam(learning_rate=0.01),\n    loss='mean_absolute_error')","278f87cd":"history = abalone_model.fit(prep_data[train_cols],\n                            prep_data['rings'],\n                            epochs = 100,\n                            validation_split = 0.2)\nhist = pd.DataFrame(history.history)\nhist['epoch'] = history.epoch\nhist.tail()","6d85c5b0":"def plot_loss(history):\n    plt.plot(history.history['loss'], label='loss')\n    plt.plot(history.history['val_loss'], label='val_loss')\n    plt.ylim([0,0.5])\n    plt.xlabel('Epoch')\n    plt.ylabel('Error [MPG]')\n    plt.legend()\n    plt.grid(True)","876b72a0":"plot_loss(history)","e016b9df":"clearly, the 2.75 to 3.25 portion is having a separate type of distribution. So let's try and fit a separate linear regression in this part.","33749026":"So the p-value is much less than 0.05. As the null hypothesis gets rejected on low p-values; and the null hypothesis of breusch-pagan is that homoscedascity is present; so there is no homoscedascity is present in this data. So now we have to treat the model for heteroscedascity.<br\/>\nOne other important observation is that,length and whole_weight has high p-value, meaning these two feature are statistically insignificant. So we will remove them and model once more. Then we will start heteroscedascity treatment.<br\/>","f0eb69d2":"So area is indeed meaningful new features, as they increase the R-square by 0.013. It seems that gender doesn't matter,as well volume also has high p-value. But being infant has a good relation with age, which is an obvious thing from common sense. So let's drop the features sex_M,sex_F, and volume, and check the performance.","eaae32a9":"## Abalone Age classification:\n<img src = 'https:\/\/alteredreality.me\/wp-content\/uploads\/2020\/04\/abalone-shell.jpg'><b>what does abalone mean?<\/b><\/img>\n<img src = 'https:\/\/i.imgur.com\/JlKwIau.png'><br\/>\nThis problem has been researched in the [following paper recently](https:\/\/www.researchgate.net\/publication\/337146276_Machine_Learning_Project_-_Predict_the_Age_of_Abalone?channel=doi&linkId=5dc702364585151435fb41d4&showFulltext=true). We will try and achieve the accuracy reached in this paper. We also actually have reached further details of the model, detected that homoscedascity is not present in the data. Therefore we tried treatments for heteroscedascity; by fitting generalized linear models with poisson and negative binomial distributions; and achieved a much less rmse with poisson glm model. We also tried fitting robust linear model, which is often used to defend against heteroscedascity. Sadly enough, the heteroscedascity was still present with our data even with glm model. I will try to add further researches in future to get rid of that. Till then, check out the rest of the notebook and feel free to comment your suggestions or show your appreciation.\n<br\/>\n### Content:\n(1) [Basic feature exploration](#feature)<br\/>\n(2) [feature engineering](#feng)<br\/>\n(3) [Linear regression](#linreg)<br\/>\n(4) [homoscedascity check using Breusch-pagan test](#homo)<br\/>\n(5) [Further experiment with statsmodels](#stats)<br\/>\n(6) [Fighting heteroscedascity with transformation](#hetero)<br\/>\n(7) [glm poisson and negative binomial distribution](#glm)<br\/>\n(8) [deep linear regression with statsmodels](#dnn)<br\/>\n### References:\n(1) [seaborn scatterplot](https:\/\/seaborn.pydata.org\/generated\/seaborn.scatterplot.html)<br\/>\n(2) [seaborn distplot](https:\/\/seaborn.pydata.org\/generated\/seaborn.distplot.html)<br\/>\n(3) [seaborn new plot figure](https:\/\/stackoverflow.com\/questions\/36018681\/stop-seaborn-plotting-multiple-figures-on-top-of-one-another)<br\/>\n(4) [what is homoscedascity](https:\/\/en.wikipedia.org\/wiki\/Homoscedasticity)<br\/>\n(5) [how to do Breusch-pagan test in python](https:\/\/www.statology.org\/breusch-pagan-test-python\/)<br\/>\n(6) [gam model](https:\/\/www.statsmodels.org\/stable\/gam.html)<br\/>\n(7) [glm models](https:\/\/www.statsmodels.org\/stable\/examples\/notebooks\/generated\/glm_formula.html)<br\/>\n(8) [RLM models](https:\/\/www.statsmodels.org\/stable\/generated\/statsmodels.robust.robust_linear_model.RLM.html)<br\/>\n(9) [stackoverflow discussion-1](https:\/\/stats.stackexchange.com\/questions\/147119\/best-way-to-deal-with-heteroscedasticity)<br\/>\n(10)[stackoverflow discussion-2](https:\/\/stackoverflow.com\/questions\/4385436\/regression-with-heteroskedasticity-corrected-standard-errors)","e3526a0d":"## <a id = 'homo'>homoscedascity check using Breusch-pagan test<\/a>\nSo the graph of predicted vs actual looks homoscedastic to me; as in the variance in the data keeps increasing with the value of the target variable; and the plot looks like a funnel. To be sure whether the data is homoscedastic actually, we will perform [Breusch-Pagan test](https:\/\/en.wikipedia.org\/wiki\/Breusch%E2%80%93Pagan_test).<br\/>\nFor doing this test, we will use the statsmodels library.<br\/>","68cc56b5":"## <a id = 'linreg'>Linear Regression<\/a>","3d57e770":"Now, it makes more sense to try out relation with higher powers of diameter as volumes and area may have more correlation with age.","3be0f2a3":"## <a id = 'feng'>feature engineering<\/a>","ca15f9db":"## <a id ='stats'>Further experiment with statsmodels<\/a>","7f50d08c":"Nope, the log(log(ring)) didn't help also. So let's not go into this much details. So in gist, the best linear model we trained was the poisson regression glm model. Although we couldn't correct the heteroscedascity issue; this model achieved lowest rmse as well as highest p-value for homoscedascity too. ","a1df6f9e":"So as it seems, there is no handling of this situation. Let's try to apply log once more.","d2f25f5f":"## <a id ='dnn'>deep linear regression with tensorflow<\/a>\n[Neural networks](https:\/\/www.tensorflow.org\/tutorials\/keras\/regression)","059c9843":"## <a id = 'hetero'>Fighting the heteroscedascity<\/a>\nWe will take the transformation method here. Redefining is not a choice here; as ","6eee5abb":"### <a id = 'glm'>Fitting a glm model<\/a>:\nwe will also try our luck with fitting a poisson glm model.<br\/>\n[some resource](https:\/\/www.bauer.uh.edu\/rsusmel\/phd\/ec1-22.pdf)<br\/>\n[GLM model documentation](https:\/\/www.statsmodels.org\/stable\/examples\/notebooks\/generated\/glm_formula.html)","70dd3b84":"Clearly, the p-value is still much less than 0.05. Let's try negative binomial regression model.","b41e28d7":"## Final rmse reporting with poisson glm model","40bac533":"So as we have seen in the original paper also, when turned from the regression to classification setting, the problem reduces drastically in accuracy. I would actually suggest to therefore stick with regression as the age is a continuous variable and therefore our mse is better performance there.","6e9dc525":"## Target task:\nFor an abalone, the number of rings represent how old the abalone is. For this reason, we are going to predict the number of rings from the other measurable items. It is important to be able to predict it from other features, as otherwise you have to cut open the abalone to count the ring number. So let's dive in to save the abalones!","3ee79f6c":"Clearly, robust linear modeling didn't work either. There is still significant amount of heteroscedascity present in this data as well. Let's try the segment wise modeling once with this data. We will take log_rings within 1.5 and 3 for modeling.","1e89c480":"This is also not better. So let's try something else now.","677a0480":"The DNN regression didn't get trained well. So that's it for regression today! to summarize, we reached a better RMSE result(2.25) than the original paper(2.34); while our classification accuracy is not better(23%) than what was achieved in the paper(27%). We conclude the exploration suggesting that one should treat this classical problem of abalone age classification as regression problem itself as that ends up making the problem better. If you liked the work, try showing your appreciation ;)","4079abbd":"so this didn't work.","274a05f8":"So as we can see, there is this portion of long tail on left side in the log_rings.Let's try to fit a separate linear model for this part.","4d1a0a18":"## <a id = 'feature' >Basic feature exploration<\/a>","c5f51d06":"So, sadly enough the homoscedascity is still not achieved. We will have to try some other method then. We can think of redefining the dependent variable. First, let's plot the dependent variable once and see.","fe4d4450":"### Heteroscedascity checks:\n[Resources](https:\/\/www.statology.org\/heteroscedasticity-regression\/)<br\/>\nThere are 3 ways to defend against heteroscedascity:<br\/>\n(1) transforming the dependent variable in some way<br\/>\n(2) Redefining the dependent variable in some way<br\/>\n(3) using weighted regression<br\/>","42873310":"Now we will use the code for cross validation rmse checking from the original paper and check the values. We will try the rmse for poisson regression.","d7e3c9bc":"Clearly, the funneling has decreased a lot. Let's run that homoscedascity check once again."}}