{"cell_type":{"7f17ea9b":"code","b8c10011":"code","308e54c4":"code","e734f93c":"code","2e468681":"code","1999242f":"code","f640eb28":"code","71489cae":"code","d016b311":"code","23f349d4":"code","343b5274":"code","e69015b1":"code","c5afbe14":"code","a5ab3be4":"code","27d96417":"code","cfc3a724":"code","cda5c222":"code","eae77b2d":"code","d7e6c139":"code","f0cb5191":"code","d573974f":"code","769ee50e":"code","083e70c3":"code","bd8638ea":"code","d4a351b2":"code","9d9ae9c7":"code","f06d4bd8":"code","69b3d89f":"code","8e03fe67":"code","e96b103f":"code","9a0f6122":"code","b238f766":"code","abea65fc":"code","9f9d5bf5":"code","244a862b":"code","390a9aac":"code","2749b1b2":"code","3a5ddac7":"code","752eb319":"code","42ab0ad5":"code","58629a75":"code","1233bc17":"code","72be4542":"code","7c4b5802":"code","6d5b854f":"code","9fc1788b":"code","dc59d4be":"code","76ca81da":"code","75cb497d":"code","9eef4a48":"code","1ad53500":"code","991b0232":"code","4bc22b4d":"code","5efadeb0":"code","74e3719d":"code","32ebddf2":"code","4fb5ed6b":"code","40123572":"code","0c273a5f":"code","4383767a":"code","f6f3129d":"code","b11b1205":"code","f41e3f0b":"code","17f909a2":"code","8e6439d5":"code","0d0b5632":"code","ee8cfd0a":"code","42866847":"code","f6e5b9e3":"code","ade7c563":"code","443fa580":"code","6bfafd1a":"code","b4c207a6":"code","50d71127":"code","aca64367":"code","d3dddd69":"markdown","b80dfa73":"markdown","c05e770b":"markdown","4a340ecf":"markdown","fa730ffa":"markdown","3aed198c":"markdown","ed27a198":"markdown","ccc2f397":"markdown","29e1cd19":"markdown","4b0e245d":"markdown","09455c90":"markdown","ba0c4a37":"markdown","4aea232b":"markdown","8ef27079":"markdown","f5a03c80":"markdown","f864576a":"markdown","a831036b":"markdown","7b7abf55":"markdown","8ebbc6e6":"markdown","d772044b":"markdown","5e7dcd3d":"markdown","55782cf0":"markdown","66da228f":"markdown","f4df7deb":"markdown","930e8773":"markdown","3d8fd51a":"markdown","f2397d6c":"markdown","bef2f275":"markdown","4a9ca470":"markdown","039b4e32":"markdown","941330cd":"markdown","cdfad839":"markdown","11a4e35a":"markdown","038946f7":"markdown","97e6b415":"markdown","d5a02e52":"markdown","aab33be0":"markdown","56d3ba95":"markdown","cb5ff64d":"markdown","20d1a592":"markdown","850147d2":"markdown","77f800ac":"markdown","677dadd5":"markdown","efe781d0":"markdown","2e9ac070":"markdown","627dfbf7":"markdown","e787b0d8":"markdown","2315e1eb":"markdown","01437b75":"markdown","585d0973":"markdown","90c78daf":"markdown","f7d6aa9d":"markdown","e9b037c9":"markdown","e6cbbbe4":"markdown","d27d656e":"markdown","a8ab14d9":"markdown","2550aea9":"markdown","25d3de4b":"markdown","0338654e":"markdown","cbdea632":"markdown","1a48c505":"markdown","ea5c8259":"markdown","cd4efcc8":"markdown","e7324c7c":"markdown","47cc31d2":"markdown","8df1c4c8":"markdown","2b9ca6d7":"markdown","c6f1c8d3":"markdown","6d2b49a8":"markdown","2e36ce6b":"markdown","8b1c711e":"markdown","2cf9475d":"markdown","63b89c5d":"markdown","d73da135":"markdown","675f6601":"markdown","f4805f58":"markdown","0bf87e49":"markdown","e52beffc":"markdown","bbdb1007":"markdown","b1100094":"markdown","a58781f0":"markdown","c450c42b":"markdown","1e7ea9fa":"markdown","d1a66c19":"markdown","832b0f16":"markdown","3e8ec4cb":"markdown","6b188e9d":"markdown","360233d2":"markdown","55aba5f3":"markdown","93a071d3":"markdown","c3a3acd1":"markdown","31723d28":"markdown","3266ba0f":"markdown","ed08a867":"markdown"},"source":{"7f17ea9b":"# Standard libraries\nimport pandas as pd\nimport numpy as np\npd.set_option('display.max_columns', 500)\nfrom math import ceil\nfrom datetime import datetime\n\n# Viz libraries\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# Utils (homemade)\nfrom viz_utils import *\nfrom prep_utils import *\nfrom ml_utils import *\n\n# Ml libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier, BaggingClassifier, AdaBoostClassifier, \\\n    GradientBoostingClassifier\nimport lightgbm as lgb\n\n# Deep Learning frameworks\nimport tensorflow as tf","b8c10011":"# Reading the data\npath = '..\/input\/bank-marketing-dataset\/bank.csv'\ndf_ori = pd.read_csv(path, sep=',')\ndf_ori.columns = [col.lower().strip().replace('.', '_') for col in df_ori.columns]\n\nprint(f'Data shape: {df_ori.shape}')\ndf_ori.head()","308e54c4":"# Creating the class object\nprep = DataPrep()\n\n# Transforming the dataset target\ndf = df_ori.copy()\ndf['target'] = (df['deposit'] == 'yes') * 1\ndf.drop('deposit', axis=1, inplace=True)\n\n# Returing an overview from the data\ntarget = 'target'\ndf_overview = prep.data_overview(df, label_name=target)\ndf_overview","e734f93c":"# Ploting a donut chart with the target variable\nlabel_names = ['No', 'Yes']\ncolor_list = ['salmon', 'darkslateblue']\n\nfig, ax = plt.subplots(figsize=(8, 8))\ntitle = 'Donut Chart for Target Variable'\ndonut_plot(df, target, label_names, ax=ax, text=f'Total: {len(df)}', colors=color_list, title=title)\nplt.show()","2e468681":"# Visualizing the categorical features\ncat_features = [col for col, dtype in df.dtypes.items() if dtype == 'object']\ncatplot_analysis(df, cat_features, fig_cols=3, hue='target', palette=['salmon', 'darkslateblue'], figsize=(16, 16))","1999242f":"# Parameters\nnum_features = ['age', 'balance', 'duration', 'campaign']\ncolor_list = ['salmon', 'darkslateblue']","f640eb28":"# Analyzing numerical features\ndistplot(df, num_features, fig_cols=3, hue='target', color=color_list, figsize=(16, 12))","71489cae":"# Stripplot\nstripplot(df, num_features, fig_cols=3, hue='target', palette=color_list)","d016b311":"# Stripplot\nboxenplot(df, num_features, fig_cols=3, hue='target', palette=color_list)","23f349d4":"# Analisando top vari\u00e1veis com maior correla\u00e7\u00e3o POSITIVA\ntop_pos_corr_cols = target_correlation_matrix(df, label_name='target', corr='positive')","343b5274":"# Columns to be dropped\nto_drop = ['duration']\ndf_drop = df.drop(to_drop, axis=1)\n\n# Verifyng\nprint(f'Shape before the drop: {df.shape}')\nprint(f'Shape after the drop: {df_drop.shape}')\ndf_drop.head()","e69015b1":"# Splitting the data\nX = df_drop.drop('target', axis=1)\ny = df_drop['target'].values\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.20, random_state=42)\nprint(f'Shape of X_train: {X_train.shape}')\nprint(f'Shape of X_test: {X_test.shape}')","c5afbe14":"# Returing features by dtype\nnum_features = [col for col, dtype in X_train.dtypes.items() if dtype != 'object']\ncat_features = [col for col, dtype in X_train.dtypes.items() if dtype == 'object']\nprint(f'Total of numerical features: {len(num_features)}')\nprint(f'Total of categorical features: {len(cat_features)}')\n\n# Splitting data by dtype\nX_train_num = X_train[num_features]\nX_train_cat = X_train[cat_features]\nprint(f'\\nShape of numerical training data: {X_train_num.shape}')\nprint(f'Shape of categorical training data: {X_train_cat.shape}')","a5ab3be4":"# Class for splitting the data by dtype\nclass SplitDataDtype(BaseEstimator, TransformerMixin):\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        # Returing features by dtype\n        self.num_features = [col for col, dtype in X.dtypes.items() if dtype != 'object']\n        self.cat_features = [col for col, dtype in X.dtypes.items() if dtype == 'object']\n        \n        # Indexing data\n        X_num = X[self.num_features]\n        X_cat = X[self.cat_features]\n        \n        return X_num, X_cat","27d96417":"# Creating object and calling the fit_transform method\ndtype_splitter = SplitDataDtype()\nX_train_num, X_train_cat = dtype_splitter.fit_transform(X_train)\n\nprint(f'Shape of numerical training data: {X_train_num.shape}')\nprint(f'Shape of categorical training data: {X_train_cat.shape}')","cfc3a724":"# Class por encoding the data\nclass DummiesEncoding(BaseEstimator, TransformerMixin):\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        # Collecting variables\n        self.cat_features_ori = [col for col, dtype in X.dtypes.items() if dtype == 'object']\n        \n        # Applying encoding with get_dummies()\n        X_cat_dum = pd.get_dummies(X)\n        \n        # Merging the datasets and eliminating old columns\n        X_dum = X.join(X_cat_dum)\n        X_dum = X_dum.drop(self.cat_features_ori, axis=1)\n        self.features_after_encoding = list(X_dum.columns)\n        \n        return X_dum","cda5c222":"# Applying encoding on categorical data\nencoder = DummiesEncoding()\nX_train_encoded = encoder.fit_transform(X_train_cat)\n\nprint(f'Shape of X_train_encoded: {X_train_encoded.shape}')\nX_train_encoded.head()","eae77b2d":"# Scaling with StandardScaler() class\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train_num)\n\n# Looking at the first line\nX_train_scaled[0]","d7e6c139":"# Initial block code for splitting the data\ndtype_spliter = SplitDataDtype()\nX_num, X_cat = dtype_spliter.fit_transform(X_train)\nnum_features = dtype_spliter.num_features\ncat_features = dtype_spliter.cat_features\n\n# Numerical pipeline\nnum_pipeline = Pipeline([\n    ('scaler', StandardScaler())\n])\n\n# Categorical pipeline\ncat_pipeline = Pipeline([\n    ('encoder', DummiesEncoding())\n])\n\n# Full pipeline\nfull_pipeline = ColumnTransformer([\n    ('num', num_pipeline, num_features),\n    ('cat', cat_pipeline, cat_features)\n])\n\n# Applying the complete pipeline on the training set\nX_train_prep = full_pipeline.fit_transform(X_train)\n\n# Returing features\ncat_features_encoded = full_pipeline.named_transformers_['cat']['encoder'].features_after_encoding\nmodel_features = num_features + cat_features_encoded","f0cb5191":"# Result\nprint(f'Shape of X_train: {X_train.shape}')\nprint(f'Shape of X_train_prep: {X_train_prep.shape}')\nprint(f'Total features: {len(model_features)}')\nprint(f'\\nFirst line of X_train_prep: \\n\\n{X_train_prep[0]}')","d573974f":"# Applying the same pipeline for the test set\nX_test_prep = full_pipeline.fit_transform(X_test)\n\nprint(f'Shape of X_test_prep: {X_test_prep.shape}')","769ee50e":"# Saving everything on a prepared set to feed some homemade classes\nset_prep = {\n    'X_train_prep': X_train_prep,\n    'X_test_prep': X_test_prep,\n    'y_train': y_train,\n    'y_test': y_test\n}","083e70c3":"# Creating the model and a class object\nlogreg_clf = LogisticRegression()\nlogreg_tool = BinaryBaselineClassifier(logreg_clf, set_prep, model_features)","bd8638ea":"# Defining hyperparmeters\nlogreg_param_grid = {\n    'C': np.linspace(0.1, 10, 20),\n    'penalty': ['l1', 'l2'],\n    'class_weight': ['balanced', None],\n    'random_state': [42],\n    'solver': ['liblinear']\n}\n\n# Training the model and optimizing AUC score\nlogreg_tool.fit(rnd_search=True, param_grid=logreg_param_grid, scoring='roc_auc')","d4a351b2":"# Model performance\nlogreg_train_performance = logreg_tool.evaluate_performance()\nlogreg_train_performance","9d9ae9c7":"# Plotting confusion matrix\ntitle = 'Logistic Regression\\nConfusion Matrix'\nclasses = ['No', 'Yes']\n\n# Creating figure and calling the method\nplt.figure(figsize=(5, 5))\nlogreg_tool.plot_confusion_matrix(classes, title=title)\nplt.show()","f06d4bd8":"plt.figure(figsize=(12, 7))\nlogreg_tool.plot_roc_curve()\nplt.annotate('\u00c1rea under the curve (ROC) with 50%\\n(Random model)', xy=(0.5, 0.5), xytext=(0.6, 0.4),\n             arrowprops=dict(facecolor='#6E726D', shrink=0.05))\nplt.show()","69b3d89f":"# Plotting the learning curve\nlogreg_tool.plot_learning_curve()","8e03fe67":"# Full performance with Logistic Regression\nlogreg_test_performance = logreg_tool.evaluate_performance(test=True)\nlogreg_performance = logreg_train_performance.append(logreg_test_performance)\nlogreg_performance","e96b103f":"# Creating objects\ntree_model = DecisionTreeClassifier()\ntree_tool = BinaryBaselineClassifier(tree_model, set_prep, model_features)","9a0f6122":"# Defining hyperparameters\ntree_param_grid = {\n    'criterion': ['entropy', 'gini'],\n    'max_depth': [3, 5, 10, 20],\n    'max_features': np.arange(1, X_train_prep.shape[1]),\n    'class_weight': ['balanced', None],\n    'random_state': [42]\n}\n\ntree_tool.fit(rnd_search=True, scoring='roc_auc', param_grid=tree_param_grid)","b238f766":"# Performance\ntree_train_performance = tree_tool.evaluate_performance()\ntree_train_performance","abea65fc":"# Variables to plotting\nlogreg_title = 'LogisticRegression\\nConfusion Matrix'\ntree_title = 'DecisionTree Classifier\\nConfusion Matrix'\nclasses = ['No', 'Yes']\n\n# Creating figure and calling the method\nplt.figure(figsize=(10, 5))\n\n# Logistic Regression\nplt.subplot(1, 2, 1)\nlogreg_tool.plot_confusion_matrix(classes, title=logreg_title)\n\n# Decision Trees\nplt.subplot(1, 2, 2)\ntree_tool.plot_confusion_matrix(classes, title=tree_title, cmap=plt.cm.Greens)\nplt.tight_layout()\nplt.show()","9f9d5bf5":"# Creating a figure and calling the method for each estimator\nplt.figure(figsize=(12, 7))\n\nlogreg_tool.plot_roc_curve()\ntree_tool.plot_roc_curve()\nplt.annotate('\u00c1rea under the curve (ROC) with 50%\\n(Random model)', xy=(0.5, 0.5), xytext=(0.6, 0.4),\n             arrowprops=dict(facecolor='#6E726D', shrink=0.05))\nplt.show()","244a862b":"# Plotting the learning curve\ntree_tool.plot_learning_curve()","390a9aac":"# Evaluating feature importance\nfeat_imp = tree_tool.feature_importance_analysis()\nfeat_imp.head(30)","2749b1b2":"# Complete performance\ntree_test_performance = tree_tool.evaluate_performance(test=True)\ntree_performance = tree_train_performance.reset_index().append(tree_test_performance.reset_index())\n\nall_performances = logreg_performance.reset_index().append(tree_performance).reset_index(drop=True)\ncm = sns.light_palette('cornflowerblue', as_cmap=True)\nall_performances.style.background_gradient(cmap=cm)","3a5ddac7":"# Creating objects\nforest_model = RandomForestClassifier()\nforest_tool = BinaryBaselineClassifier(forest_model, set_prep, model_features)","752eb319":"# Defining hyperparameters\nforest_param_grid = {\n    'bootstrap': [True, False],\n    'max_depth': [3, 5, 10, 20, 50],\n    'n_estimators': [50, 100, 200, 500],\n    'random_state': [42],\n    'max_features': ['auto', 'sqrt'],\n    'class_weight': ['balanced', None]\n}\n\nforest_tool.fit(rnd_search=True, scoring='roc_auc', param_grid=forest_param_grid)","42ab0ad5":"# Model performance\nforest_train_performance = forest_tool.evaluate_performance()\nforest_train_performance","58629a75":"# Variables for plotting\nlogreg_title = 'LogisticRegression\\nConfusion Matrix'\ntree_title = 'DecisionTree Classifier\\nConfusion Matrix'\nforest_title = 'RandomForest Classifier\\nConfusion Matrix'\nclasses = ['No', 'Yes']\n\n# Creating figure and calling the method\nplt.figure(figsize=(15, 5))\n\n# Logistic Regression\nplt.subplot(1, 3, 1)\nlogreg_tool.plot_confusion_matrix(classes, title=logreg_title)\n\n# Decision Trees\nplt.subplot(1, 3, 2)\ntree_tool.plot_confusion_matrix(classes, title=tree_title, cmap=plt.cm.Greens)\n\n# Random Forest\nplt.subplot(1, 3, 3)\ntree_tool.plot_confusion_matrix(classes, title=forest_title, cmap=plt.cm.Reds)\n\nplt.tight_layout()\nplt.show()","1233bc17":"# Creating figure and calling the method for each estimator\nplt.figure(figsize=(12, 7))\n\n# ROC Curve for the models\nlogreg_tool.plot_roc_curve()\ntree_tool.plot_roc_curve()\nforest_tool.plot_roc_curve()\n\n# Annotation\nplt.annotate('\u00c1rea under the curve (ROC) with 50%\\n(Random model)', xy=(0.5, 0.5), xytext=(0.6, 0.4),\n             arrowprops=dict(facecolor='#6E726D', shrink=0.05))\nplt.show()","72be4542":"# Plotting the learning curve\nforest_tool.plot_learning_curve()","7c4b5802":"# Complete performance\nforest_test_performance = forest_tool.evaluate_performance(test=True)\nforest_performance = forest_train_performance.reset_index().append(forest_test_performance.reset_index())\n\nall_performances = all_performances.append(forest_performance).reset_index(drop=True)\ncm = sns.light_palette('cornflowerblue', as_cmap=True)\nall_performances.style.background_gradient(cmap=cm)","6d5b854f":"# Creating objects\nvoting_model = VotingClassifier(\n    estimators=[('logreg', logreg_tool.trained_model), ('forest', forest_tool.trained_model)],\n    voting='soft'\n)\n\n# Training the model\nvoting_tool = BinaryBaselineClassifier(voting_model, set_prep, model_features)\nvoting_tool.fit(rnd_search=False)","9fc1788b":"# Model performance\nvoting_train_performance = voting_tool.evaluate_performance()\nvoting_train_performance","dc59d4be":"# Plotting the matrices\nlogreg_title = 'LogisticRegression\\nConfusion Matrix'\ntree_title = 'DecisionTree Classifier\\nConfusion Matrix'\nforest_title = 'RandomForest Classifier\\nConfusion Matrix'\nvoting_title = 'Voting Classifier\\nConfusion Matrix'\nclasses = ['No', 'Yes']\n\n# Creating figure e calling the model\nplt.figure(figsize=(15, 10))\n\n# Logistic Regression\nplt.subplot(2, 3, 1)\nlogreg_tool.plot_confusion_matrix(classes, title=logreg_title)\n\n# Decision Trees\nplt.subplot(2, 3, 2)\ntree_tool.plot_confusion_matrix(classes, title=tree_title, cmap=plt.cm.Greens)\n\n# Random Forest\nplt.subplot(2, 3, 3)\ntree_tool.plot_confusion_matrix(classes, title=forest_title, cmap=plt.cm.Reds)\n\n# Voting Classifier\nplt.subplot(2, 3, 4)\nvoting_tool.plot_confusion_matrix(classes, title=voting_title, cmap=plt.cm.Greys)\n\nplt.tight_layout()\nplt.show()","76ca81da":"# Creating figure and calling the method for each estimator\nplt.figure(figsize=(12, 7))\n\n# ROC Curve for the models\nlogreg_tool.plot_roc_curve()\ntree_tool.plot_roc_curve()\nforest_tool.plot_roc_curve()\nvoting_tool.plot_roc_curve()\n\n# Annotation\nplt.annotate('\u00c1rea under the curve (ROC) with 50%\\n(Random model)', xy=(0.5, 0.5), xytext=(0.6, 0.4),\n             arrowprops=dict(facecolor='#6E726D', shrink=0.05))\nplt.show()","75cb497d":"# Plotting the learning curve\nvoting_tool.plot_learning_curve()","9eef4a48":"# Complete performance\nvoting_test_performance = voting_tool.evaluate_performance(test=True)\nvoting_performance = voting_train_performance.reset_index().append(voting_test_performance.reset_index())\n\nall_performances = all_performances.append(voting_performance).reset_index(drop=True)\ncm = sns.light_palette('cornflowerblue', as_cmap=True)\nall_performances.style.background_gradient(cmap=cm)","1ad53500":"# Creating the bagging model based on the Random Forest Classifier\nbagging_model = BaggingClassifier(\n    RandomForestClassifier(bootstrap=False, criterion='gini', max_depth=10, max_features='sqrt', \n                           n_estimators=50, random_state=42), \n    n_estimators=20,\n    max_samples=100,\n    bootstrap=True,\n    n_jobs=-1,\n    oob_score=True\n)\n\n# Training model\nbagging_tool = BinaryBaselineClassifier(bagging_model, set_prep, model_features)\nbagging_tool.fit(rnd_search=False)","991b0232":"# Verificando performance\nbagging_train_performance = bagging_tool.evaluate_performance()\nbagging_train_performance","4bc22b4d":"# Plotting Confusion Matrix\nlogreg_title = 'LogisticRegression\\nConfusion Matrix'\ntree_title = 'DecisionTree Classifier\\nConfusion Matrix'\nforest_title = 'RandomForest Classifier\\nConfusion Matrix'\nvoting_title = 'Voting Classifier\\nConfusion Matrix'\nbagging_title = 'Bootstrap Aggregating\\nConfusion Matrix'\nclasses = ['No', 'Yes']\n\n# Creating figure and calling the method\nplt.figure(figsize=(15, 10))\n\n# Regress\u00e3o Log\u00edstica\nplt.subplot(2, 3, 1)\nlogreg_tool.plot_confusion_matrix(classes, title=logreg_title)\n\n# Decision Trees\nplt.subplot(2, 3, 2)\ntree_tool.plot_confusion_matrix(classes, title=tree_title, cmap=plt.cm.Greens)\n\n# Random Forest\nplt.subplot(2, 3, 3)\ntree_tool.plot_confusion_matrix(classes, title=forest_title, cmap=plt.cm.Reds)\n\n# Voting Classifier\nplt.subplot(2, 3, 4)\nvoting_tool.plot_confusion_matrix(classes, title=voting_title, cmap=plt.cm.Greys)\n\n# Bagging Classifier\nplt.subplot(2, 3, 5)\nbagging_tool.plot_confusion_matrix(classes, title=bagging_title, cmap=plt.cm.Oranges)\n\nplt.tight_layout()\nplt.show()","5efadeb0":"# Creating figure and calling the method\nplt.figure(figsize=(12, 7))\n\n# Plotting the ROC Curve\nlogreg_tool.plot_roc_curve()\ntree_tool.plot_roc_curve()\nforest_tool.plot_roc_curve()\nvoting_tool.plot_roc_curve()\nbagging_tool.plot_roc_curve()\n\n# Anota\u00e7\u00e3o\nplt.annotate('\u00c1rea under the curve (ROC) with 50%\\n(Random model)', xy=(0.5, 0.5), xytext=(0.6, 0.4),\n             arrowprops=dict(facecolor='#6E726D', shrink=0.05))\nplt.show()","74e3719d":"# Plotting the learning curve\nbagging_tool.plot_learning_curve()","32ebddf2":"# Complete Performance\nbagging_test_performance = bagging_tool.evaluate_performance(test=True)\nbagging_performance = bagging_train_performance.reset_index().append(bagging_test_performance.reset_index())\n\nall_performances = all_performances.append(bagging_performance).reset_index(drop=True)\ncm = sns.light_palette('cornflowerblue', as_cmap=True)\nall_performances.style.background_gradient(cmap=cm)","4fb5ed6b":"# Training the model\nadaboost_model = AdaBoostClassifier(\n    RandomForestClassifier(bootstrap=False, criterion='gini', max_depth=10, max_features='sqrt', \n                           n_estimators=50, random_state=42), \n    n_estimators=20,\n    learning_rate=0.5,\n    random_state=42\n)\n\nadaboost_tool = BinaryBaselineClassifier(adaboost_model, set_prep, model_features)\nadaboost_tool.fit(rnd_search=False)","40123572":"# Model performance\nadaboost_train_performance = adaboost_tool.evaluate_performance()\nadaboost_train_performance","0c273a5f":"# PLotting the matrices\nlogreg_title = 'LogisticRegression\\nConfusion Matrix'\ntree_title = 'DecisionTree Classifier\\nConfusion Matrix'\nforest_title = 'RandomForest Classifier\\nConfusion Matrix'\nvoting_title = 'Voting Classifier\\nConfusion Matrix'\nbagging_title = 'Bootstrap Aggregating\\nConfusion Matrix'\nadaboost_title = 'Adaptative Boosting\\nConfusion Matrix'\nclasses = ['No', 'Yes']\n\n# Creating figure and calling the methods\nplt.figure(figsize=(15, 10))\n\n# Logistic Regression\nplt.subplot(2, 3, 1)\nlogreg_tool.plot_confusion_matrix(classes, title=logreg_title)\n\n# Decision Trees\nplt.subplot(2, 3, 2)\ntree_tool.plot_confusion_matrix(classes, title=tree_title, cmap=plt.cm.Greens)\n\n# Random Forest\nplt.subplot(2, 3, 3)\ntree_tool.plot_confusion_matrix(classes, title=forest_title, cmap=plt.cm.Reds)\n\n# Voting Classifier\nplt.subplot(2, 3, 4)\nvoting_tool.plot_confusion_matrix(classes, title=voting_title, cmap=plt.cm.Greys)\n\n# Bagging Classifier\nplt.subplot(2, 3, 5)\nbagging_tool.plot_confusion_matrix(classes, title=bagging_title, cmap=plt.cm.Oranges)\n\n# Adaboost Classifier\nplt.subplot(2, 3, 6)\nadaboost_tool.plot_confusion_matrix(classes, title=adaboost_title, cmap=plt.cm.Purples)\n\nplt.tight_layout()\nplt.show()","4383767a":"# Creating figure\nplt.figure(figsize=(12, 7))\n\n# Plotting the ROC Curve for each estimator\nlogreg_tool.plot_roc_curve()\ntree_tool.plot_roc_curve()\nforest_tool.plot_roc_curve()\nvoting_tool.plot_roc_curve()\nbagging_tool.plot_roc_curve()\nadaboost_tool.plot_roc_curve()\n\n# Annotation\nplt.annotate('\u00c1rea under the curve (ROC) with 50%\\n(Random model)', xy=(0.5, 0.5), xytext=(0.6, 0.4),\n             arrowprops=dict(facecolor='#6E726D', shrink=0.05))\nplt.show()","f6f3129d":"# Learning Curve\nadaboost_tool.plot_learning_curve()","b11b1205":"# Complete Performance\nadaboost_test_performance = adaboost_tool.evaluate_performance(test=True)\nadaboost_performance = adaboost_train_performance.reset_index().append(adaboost_test_performance.reset_index())\n\nall_performances = all_performances.append(adaboost_performance).reset_index(drop=True)\ncm = sns.light_palette('cornflowerblue', as_cmap=True)\nall_performances.style.background_gradient(cmap=cm)","f41e3f0b":"# Creating the object\ngboost_model = GradientBoostingClassifier(\n    n_estimators=20,\n    learning_rate=1.0,\n    max_depth=5, \n    max_features=15,\n    random_state=42\n)\n\n# Training the model\ngboost_tool = BinaryBaselineClassifier(gboost_model, set_prep, model_features)\ngboost_tool.fit(rnd_search=False)","17f909a2":"# Model Performance\ngboost_train_performance = gboost_tool.evaluate_performance()\ngboost_train_performance","8e6439d5":"# Plotting matrices\nlogreg_title = 'LogisticRegression\\nConfusion Matrix'\ntree_title = 'DecisionTree Classifier\\nConfusion Matrix'\nforest_title = 'RandomForest Classifier\\nConfusion Matrix'\nvoting_title = 'Voting Classifier\\nConfusion Matrix'\nbagging_title = 'Bootstrap Aggregating\\nConfusion Matrix'\nadaboost_title = 'Adaptative Boosting\\nConfusion Matrix'\ngboost_title = 'Gradient Boosting\\nConfusion Matrix'\nclasses = ['No', 'Yes']\n\n# Creating figure and calling the methods\nplt.figure(figsize=(15, 15))\n\n# Logistic Regression\nplt.subplot(3, 3, 1)\nlogreg_tool.plot_confusion_matrix(classes, title=logreg_title)\n\n# Decision Trees\nplt.subplot(3, 3, 2)\ntree_tool.plot_confusion_matrix(classes, title=tree_title, cmap=plt.cm.Greens)\n\n# Random Forest\nplt.subplot(3, 3, 3)\ntree_tool.plot_confusion_matrix(classes, title=forest_title, cmap=plt.cm.Reds)\n\n# Voting Classifier\nplt.subplot(3, 3, 4)\nvoting_tool.plot_confusion_matrix(classes, title=voting_title, cmap=plt.cm.Greys)\n\n# Bagging Classifier\nplt.subplot(3, 3, 5)\nbagging_tool.plot_confusion_matrix(classes, title=bagging_title, cmap=plt.cm.Oranges)\n\n# Adaboost Classifier\nplt.subplot(3, 3, 6)\nadaboost_tool.plot_confusion_matrix(classes, title=adaboost_title, cmap=plt.cm.Purples)\n\n# Gradient Boosting\nplt.subplot(3, 3, 7)\ngboost_tool.plot_confusion_matrix(classes, title=gboost_title, cmap=plt.cm.cool)\n\nplt.tight_layout()\nplt.show()","0d0b5632":"# Creating figure\nplt.figure(figsize=(12, 7))\n\n# Plotando curva para os modelos\nlogreg_tool.plot_roc_curve()\ntree_tool.plot_roc_curve()\nforest_tool.plot_roc_curve()\nvoting_tool.plot_roc_curve()\nbagging_tool.plot_roc_curve()\nadaboost_tool.plot_roc_curve()\ngboost_tool.plot_roc_curve()\n\n# Annotation\nplt.annotate('\u00c1rea under the curve (ROC) with 50%\\n(Random model)', xy=(0.5, 0.5), xytext=(0.6, 0.4),\n             arrowprops=dict(facecolor='#6E726D', shrink=0.05))\nplt.show()","ee8cfd0a":"# Learning Curve\ngboost_tool.plot_learning_curve()","42866847":"# Complete performance\ngboost_test_performance = gboost_tool.evaluate_performance(test=True)\ngboost_performance = gboost_train_performance.reset_index().append(gboost_test_performance.reset_index())\n\nall_performances = all_performances.append(gboost_performance).reset_index(drop=True)\ncm = sns.light_palette('cornflowerblue', as_cmap=True)\nall_performances.style.background_gradient(cmap=cm)","f6e5b9e3":"# Setting up a LightGBM model\ntrain_data = lgb.Dataset(X_train_prep, label=y_train)\ntest_data = lgb.Dataset(X_test_prep, label=y_test)\n\n# Parameters\nlgbm_params = {\n    'application': 'binary',\n    'objective': 'binary',\n    'metric': 'auc',\n    'is_unbalance': 'true',\n    'boosting_type': 'gbdt',\n    'num_leaves': 31,\n    'feature_fraction': 0.5,\n    'bagging_fraction': 0.5,\n    'bagging_freq': 20,\n    'learning_rate': 0.05,\n    'verbose': 0\n}\nlgbm_model = lgb.LGBMClassifier(**lgbm_params)","ade7c563":"# Training the model\nlgbm_tool = BinaryBaselineClassifier(lgbm_model, set_prep, model_features)\nlgbm_tool.fit(rnd_search=False)","443fa580":"# Model Performance\nlgbm_train_performance = lgbm_tool.evaluate_performance()\nlgbm_train_performance","6bfafd1a":"# Plotting matrices\nlogreg_title = 'LogisticRegression\\nConfusion Matrix'\ntree_title = 'DecisionTree Classifier\\nConfusion Matrix'\nforest_title = 'RandomForest Classifier\\nConfusion Matrix'\nvoting_title = 'Voting Classifier\\nConfusion Matrix'\nbagging_title = 'Bootstrap Aggregating\\nConfusion Matrix'\nadaboost_title = 'Adaptative Boosting\\nConfusion Matrix'\ngboost_title = 'Gradient Boosting\\nConfusion Matrix'\nlgbm_title = 'LightGBM\\nConfusion Matrix'\nclasses = ['No', 'Yes']\n\n# Creating figure\nplt.figure(figsize=(15, 15))\n\n# Logistic Regression\nplt.subplot(3, 3, 1)\nlogreg_tool.plot_confusion_matrix(classes, title=logreg_title)\n\n# Decision Trees\nplt.subplot(3, 3, 2)\ntree_tool.plot_confusion_matrix(classes, title=tree_title, cmap=plt.cm.Greens)\n\n# Random Forest\nplt.subplot(3, 3, 3)\ntree_tool.plot_confusion_matrix(classes, title=forest_title, cmap=plt.cm.Reds)\n\n# Voting Classifier\nplt.subplot(3, 3, 4)\nvoting_tool.plot_confusion_matrix(classes, title=voting_title, cmap=plt.cm.Greys)\n\n# Bagging Classifier\nplt.subplot(3, 3, 5)\nbagging_tool.plot_confusion_matrix(classes, title=bagging_title, cmap=plt.cm.Oranges)\n\n# Adaboost Classifier\nplt.subplot(3, 3, 6)\nadaboost_tool.plot_confusion_matrix(classes, title=adaboost_title, cmap=plt.cm.Purples)\n\n# Gradient Boosting\nplt.subplot(3, 3, 7)\ngboost_tool.plot_confusion_matrix(classes, title=gboost_title, cmap=plt.cm.cool)\n\n# LightGBM\nplt.subplot(3, 3, 8)\nlgbm_tool.plot_confusion_matrix(classes, title=lgbm_title, cmap=plt.cm.winter)\n\nplt.tight_layout()\nplt.show()","b4c207a6":"# Creating figure and calling the method\nplt.figure(figsize=(12, 7))\n\n# Plotting curves\nlogreg_tool.plot_roc_curve()\ntree_tool.plot_roc_curve()\nforest_tool.plot_roc_curve()\nvoting_tool.plot_roc_curve()\nbagging_tool.plot_roc_curve()\nadaboost_tool.plot_roc_curve()\ngboost_tool.plot_roc_curve()\nlgbm_tool.plot_roc_curve()\n\n# Annotation\nplt.annotate('\u00c1rea under the curve (ROC) with 50%\\n(Random model)', xy=(0.5, 0.5), xytext=(0.6, 0.4),\n             arrowprops=dict(facecolor='#6E726D', shrink=0.05))\nplt.show()","50d71127":"# Learning curve\nlgbm_tool.plot_learning_curve()","aca64367":"# Complete performance\nlgbm_test_performance = lgbm_tool.evaluate_performance(test=True)\nlgbm_performance = lgbm_train_performance.reset_index().append(lgbm_test_performance.reset_index())\n\nall_performances = all_performances.append(lgbm_performance).reset_index(drop=True)\ncm = sns.light_palette('cornflowerblue', as_cmap=True)\nall_performances.style.background_gradient(cmap=cm)","d3dddd69":"The results show us that joining the Logistic Regression and the Random Florest classifiers is not very effective in therms of improvement. Of course, for a good performance, this voting approach demands a high number of classifiers and, if they are diverse i.e. makes mistakes at different points, then the group performance could be improved.","b80dfa73":"As we could see on the dataset documentation, we have a group of 16 features and one target (this one represented by the column `deposit`). By looking for the data shape, we can see that the data has a little bit more than 11k rows.\n\nAt this point, it's important to take a look at each feature's meaning individually. With this we can be prepared to take more confident decisions.","c05e770b":"**Evaluating metrics**","4a340ecf":"**Confusion Matrix**","fa730ffa":"**What are the categories of the categorical features?**","3aed198c":"**Evaluating on test set**","ed27a198":"From this moment, we will put aside the test set and we will work marjoritary with the training set. We go back with test set just to evaluate the model already trained.\n\nLet's make another split: a categorical and a numerical set.","ccc2f397":"The concept behind Adaptatve Bosting algorithm (or simply `AdaBoost`) is to train a bunch of classifiers capable of correcting the antecessor's errors.","29e1cd19":"**Model Metrics**","4b0e245d":"The first task is related with filtering the columns that won't be used on a preditive model. As long as we don't have any key-columns that don't make sense to put on a Machine Learning algorithm, the only column to be dropped is `duration`. Besides this variable is the most correlated one, this drop must be done because the `duration` here represents the call duration with the customer during the product offer, so its values is known only after the contact is finished.\n\nThinking of a preditive perspective, we can build a model that is capable to predict the chance of a customer subscripe a product **BEFORE** the contact is made. That's why we must drop `duration`.","09455c90":"**How is the distribution of numerical features?**","ba0c4a37":"_Searching for different insights from the stripplot_","4aea232b":"On the Bootstrap Aggregating ensemble, we used another ensemble: Random Forest. In fact, it was used 100 estimators for the final model, but the performance was not so good. Besides the lower performance, the time spent on evaluating metrics was much higher.","8ef27079":"**Evaluating on the test set**","f5a03c80":"## Numerical Pipeline","f864576a":"## LightGBM","a831036b":"## Decision Trees","7b7abf55":"This is really an important session on the analysis. Here we will apply some data visualization and data exploration techniques in order to gain insights from data. We will propose some questions to be answeared with coding steps.","8ebbc6e6":"The numbers says that the DecisionTrees model are not so good. By the way it performed worse than our baseline (Logistic Regression). Comparing both metric by metric, the tree model have only a precision higher.","d772044b":"**ROC Curve**","5e7dcd3d":"**Confusion Matrix**","55782cf0":"Let's apply something known as `Voting Classifier`. This approach represents a group of models making predictions and, after all, the final model consider the prediction of the marjority.","66da228f":"**ROC Curve**","f4df7deb":"**Learning Curve**","930e8773":"In order to make life easier, we will use the `BinaryBaselineClassifier()`, a homemade implementation with some useful methods for training and evaluating Machine Learning models.","3d8fd51a":"**Confusion Matrix**","f2397d6c":"## Random Forest","bef2f275":"**Learning Curve**","4a9ca470":"A fundamental point on the training Pipeline of Machine Learning models is, with no doubts, the dataset split in training and testing sets. This allows us to optimize the model with one set and validate with another unseen set.","039b4e32":"**Learning Curve**","941330cd":"**Model Metrics**","cdfad839":"## Logistic Regression","11a4e35a":"**Evaluating on test set**","038946f7":"**Is this a kind of imbalanced dataset?**","97e6b415":"# Data Prep","d5a02e52":"**ROC Curve**","aab33be0":"**Evaluating on test set**","56d3ba95":"**Learning Curve**","cb5ff64d":"**Learning Curve**","20d1a592":"**Evaluating on test set**","850147d2":"**Confusion Matrix**","77f800ac":"After wenting trough the data exploration step, we can start the preparation step.","677dadd5":"**Evaluating on test set**","efe781d0":"## Train and Test","2e9ac070":"After splitting the data on numerical and categorical sets, let's apply the encoding processing on categorical data. This is important for the Machine Learning model to train the data in a correct way.","627dfbf7":"**ROC Curve**","e787b0d8":"With all the pipeline steps already defined, let's put everything in a complete Pipeline.","2315e1eb":"**Confusion Matrix**","01437b75":"**Feature Importance**","585d0973":"Let's go ahead with our trials. On this second approach, we will use a Decision Trees algorithm.","90c78daf":"## Categorical Pipeline","f7d6aa9d":"**Confusion Matrix**","e9b037c9":"**Evaluating Performance**","e6cbbbe4":"Thinking somehow on building a pre-processing pipeline, let's create a class for doing this splitting by dtype automatically.","d27d656e":"## Gradient Boosting","a8ab14d9":"**What are the features with most positive correlation with the target?**","2550aea9":"Looking at the results gotten from Logistic Regression model, it's possible to conclude that this approach allowed a good performance. At least something expected from a baseline model. Meanwhile, it's not wrong to say that the model maybe suffer from a high bias.\n\nFor this we can:\n\n    - Collect more features;\n    - Collect more data;\n    - Train a more complex model ","25d3de4b":"**Evaluating on test set**","0338654e":"**ROC Curve**","cbdea632":"# Project Libraries","1a48c505":"**Learning Curve**","ea5c8259":"## Adaptative Boosting","cd4efcc8":"<h1>Table of Contents<span class=\"tocSkip\"><\/span><\/h1>\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Project-Libraries\" data-toc-modified-id=\"Project-Libraries-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;<\/span>Project Libraries<\/a><\/span><\/li><li><span><a href=\"#The-First-Contact-with-Data\" data-toc-modified-id=\"The-First-Contact-with-Data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;<\/span>The First Contact with Data<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Data-Overview\" data-toc-modified-id=\"Data-Overview-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;<\/span>Data Overview<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#EDA\" data-toc-modified-id=\"EDA-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;<\/span>EDA<\/a><\/span><\/li><li><span><a href=\"#Data-Prep\" data-toc-modified-id=\"Data-Prep-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;<\/span>Data Prep<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Feature-Selection\" data-toc-modified-id=\"Feature-Selection-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;<\/span>Feature Selection<\/a><\/span><\/li><li><span><a href=\"#Train-and-Test\" data-toc-modified-id=\"Train-and-Test-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;<\/span>Train and Test<\/a><\/span><\/li><li><span><a href=\"#Categorical-Pipeline\" data-toc-modified-id=\"Categorical-Pipeline-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;<\/span>Categorical Pipeline<\/a><\/span><\/li><li><span><a href=\"#Numerical-Pipeline\" data-toc-modified-id=\"Numerical-Pipeline-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;<\/span>Numerical Pipeline<\/a><\/span><\/li><li><span><a href=\"#Full-Pipeline\" data-toc-modified-id=\"Full-Pipeline-4.5\"><span class=\"toc-item-num\">4.5&nbsp;&nbsp;<\/span>Full Pipeline<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Predictive-Model\" data-toc-modified-id=\"Predictive-Model-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;<\/span>Predictive Model<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Logistic-Regression\" data-toc-modified-id=\"Logistic-Regression-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;<\/span>Logistic Regression<\/a><\/span><\/li><li><span><a href=\"#Decision-Trees\" data-toc-modified-id=\"Decision-Trees-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;<\/span>Decision Trees<\/a><\/span><\/li><li><span><a href=\"#Random-Forest\" data-toc-modified-id=\"Random-Forest-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;<\/span>Random Forest<\/a><\/span><\/li><li><span><a href=\"#Voting-Classifier\" data-toc-modified-id=\"Voting-Classifier-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;<\/span>Voting Classifier<\/a><\/span><\/li><li><span><a href=\"#Bootstrap-Agregating\" data-toc-modified-id=\"Bootstrap-Agregating-5.5\"><span class=\"toc-item-num\">5.5&nbsp;&nbsp;<\/span>Bootstrap Agregating<\/a><\/span><\/li><li><span><a href=\"#Adaptative-Boosting\" data-toc-modified-id=\"Adaptative-Boosting-5.6\"><span class=\"toc-item-num\">5.6&nbsp;&nbsp;<\/span>Adaptative Boosting<\/a><\/span><\/li><li><span><a href=\"#Gradient-Boosting\" data-toc-modified-id=\"Gradient-Boosting-5.7\"><span class=\"toc-item-num\">5.7&nbsp;&nbsp;<\/span>Gradient Boosting<\/a><\/span><\/li><li><span><a href=\"#LightGBM\" data-toc-modified-id=\"LightGBM-5.8\"><span class=\"toc-item-num\">5.8&nbsp;&nbsp;<\/span>LightGBM<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Deep-Neural-Network\" data-toc-modified-id=\"Deep-Neural-Network-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;<\/span>Deep Neural Network<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Treinando-Rede-Neural-Profunda\" data-toc-modified-id=\"Treinando-Rede-Neural-Profunda-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;<\/span>Treinando Rede Neural Profunda<\/a><\/span><\/li><\/ul><\/li><\/ul><\/div>","e7324c7c":"**Learning Curve**","47cc31d2":"While the `Voting Classifier` makes predictions based on individual predictions of a set of classifiers, the `Bootstrap Aggregating`(or bagging) uses the **same** classifier trained repeatedly.\n\nOnce trained, the ensemble make predictions based on the aggregation of all estimatores individually.","8df1c4c8":"**Learning Curve**","2b9ca6d7":"Well, the graph below shows us that this is not a imbalanced dataset. Both classes has similar proportion.","c6f1c8d3":"Let's try something more complex: a `RandomForestClassifier`. The basic idea of this model is to use several DecisionTrees models to have a more robust decision.","6d2b49a8":"Well, the goal of this notebook is to present you a efficient way to gather insights from data. We will also train a Machine Learning model to predict the chance of a customer to subscribe the bank produto, given the features selected from data. _I really hope you enjoy and if you do so, don't forget to **upvote this kernel**_!","2e36ce6b":"**ROC Curve**","8b1c711e":"# Predictive Model","2cf9475d":"**Evaluating metrics**","63b89c5d":"**Evaluating Metrics**","d73da135":"# EDA","675f6601":"**Confusion Matrix**","f4805f58":"Here I take freedom to use some homemade implementations built to make some general Data Science steps easier. The next cell calls the `data_overview()` method from my `DataPrep()` class to consolidate some useful insights from the dataset.","0bf87e49":"By looking at the result above, it's possible to point:\n\n* There is no null data on this dataset;\n* The feature `duration` is the one with the highest correlation with our target (there is an observation of this on the dataset's documentation).","e52beffc":"## Voting Classifier","bbdb1007":"On the numerical features case, it's possible to apply a scaling proccess. For some model like Logistic Regression, this step is really important in order to give to the algorithm a fast chance to reach the optimal cost. But for another ones, like Decision Trees, the scaling is not a request.\n\nFrom a didactic perspective, let's apply the scaling on your data.","b1100094":"**Evaluating on test set**","a58781f0":"_Searching for different insights from the boxenplot_","c450c42b":"Hi everyone and welcome to my notebook! I've prepared a helpful (I hope) analysis on the [Bank Marketing Data Set](https:\/\/archive.ics.uci.edu\/ml\/datasets\/Bank+Marketing), a dataset build from a 2014 study based on marketing campaing of a portuguese bank. By the end, it was possible to flag the data if the customer subsribed or not the produt (`yes` or `no`).","1e7ea9fa":"# First Contact with Data","d1a66c19":"## Full Pipeline","832b0f16":"## Data Overview","3e8ec4cb":"## Bootstrap Agregating","6b188e9d":"**Curva ROC**","360233d2":"**Confusion Matrix**","55aba5f3":"**Evaluating metrics**","93a071d3":"After concluding the Data Prep, we can finally start our journey of finding the best Machine Learning model capable to predict the product subscribing chance for a given customer. Let's start with a baseline: Logistic Regression.","c3a3acd1":"With a better performance than the other models, the Random Forest is certainly a good candidate to be the best model for this task. Meanwhile, looking at the learning curve, it's reasonable to say that this model maybe is suffering for a high variance (common on this type of algorithm). Let's keep moving by now.","31723d28":"## Feature Selection","3266ba0f":"**Evaluating Metrics**","ed08a867":"**ROC Curve**"}}