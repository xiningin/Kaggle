{"cell_type":{"f473ea30":"code","40463358":"code","1ce03a39":"code","0df4356b":"code","36dd9827":"code","f6c8d835":"code","780f6c4c":"code","4ffdaa64":"code","ea628f56":"code","bdc78dfc":"code","5588b314":"code","abe225ad":"code","4367d2fe":"code","be480de9":"code","0b3697a2":"code","20f329ab":"code","4a668154":"code","f93db414":"code","7d7a4fb0":"code","f6214e18":"code","e48885fd":"code","783bcd5b":"code","87fe9b6d":"code","e05dd2a0":"code","e42d6eba":"code","32b75a88":"code","33990c08":"code","3ef43601":"code","f0999efa":"code","781796bb":"code","4728412a":"code","5c43aad5":"code","d60a5874":"code","8eda5b5a":"code","327316e2":"code","23883776":"code","4eb4674e":"code","52ea630f":"code","d641d036":"code","246005c3":"code","bced8c48":"code","77b26802":"code","0af56924":"code","8f23e520":"code","9221e43a":"code","c2053415":"code","e04dbac2":"code","5e067241":"code","26d646c5":"code","9edc7774":"code","d2ca6746":"code","3c04f9c8":"code","d6b931ed":"code","aeba3910":"code","ba91ccd9":"code","9282f710":"code","bc4b3b4f":"markdown","22df06e6":"markdown","7205152f":"markdown","c1371a3d":"markdown","58513cef":"markdown","e60de90e":"markdown","f32524b6":"markdown","e48554fc":"markdown","24349ae7":"markdown","4a752998":"markdown","63f6c2fe":"markdown","8434ac75":"markdown","c9e4b4a6":"markdown","ee6d9c24":"markdown","6fa8a066":"markdown","4f4973fb":"markdown","b7516323":"markdown","8ac73a43":"markdown"},"source":{"f473ea30":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport random\nimport seaborn as sns","40463358":"dataset_train = pd.read_csv('..\/input\/titanic\/train.csv')\ndataset_test = pd.read_csv('..\/input\/titanic\/test.csv')","1ce03a39":"train = dataset_train.copy()\ntest = dataset_test.copy()","0df4356b":"combine = [train,test]","36dd9827":"train.head()","f6c8d835":"test.head()","780f6c4c":"train.describe()","4ffdaa64":"train.info()","ea628f56":"col_list, lst_str = [], ['object','int64','float64']# different \ni=0\nfor string in lst_str:\n    lst = train.select_dtypes(include=[string]).columns\n    print(lst_str[i],'\\n',lst)\n    col_list.append(lst)\n    i+=1","bdc78dfc":"train.pop('Ticket') # this is not necessary because we have PClass and Fare \ntrain.pop('Cabin') # similar reasoning to ticket\ntest.pop('Ticket')\ntest.pop('Cabin')\nprint('Unecessary Columns Deleted')","5588b314":"for dataset in combine:\n    dataset['Title'] = dataset['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)","abe225ad":"# show the titles and distribution of titles and gender\npd.crosstab(combine[0]['Title'], combine[0]['Sex'])","4367d2fe":"neon = ['#df0772','#fe546f','#ff9e7d','#ffd080','#01cbcf','#0188a5','#3e3264']\nneon_text = ['#352a55','#01cbcf','#fffdff','#dfc907']","be480de9":"train['Sex']","0b3697a2":"plt.rcParams['text.color'] = neon_text[2]\nplt.rcParams['xtick.color'] = neon_text[2]\nplt.rcParams['ytick.color'] = neon_text[2]\nplt.rcParams['axes.edgecolor'] = neon_text[2]\nplt.rcParams['axes.facecolor'] = neon_text[0]\nwidth = 0.3","20f329ab":"perished = train[train['Survived']==0].groupby('Sex')['Survived'].count()\nsurvived = train[train['Survived']==1].groupby('Sex')['Survived'].count()\n\nfig = plt.figure(figsize=(12,4), dpi=80, facecolor=neon_text[0],edgecolor=neon_text[2])\n\nax1 = fig.add_subplot(121)\nplt.title('Perished')\nperished.plot(kind='bar', ax=ax1, legend=False,color=[neon[1],neon[5]])\nax1.xaxis.set_label_text('')\nplt.xticks(rotation=0)\n\nax2 = fig.add_subplot(122)\nplt.title('Survived')\nsurvived.plot(kind='bar', ax=ax2, legend=False,color=[neon[1],neon[5]])\nax2.xaxis.set_label_text('')\nplt.xticks(rotation=0)","4a668154":"perished_a = train[train['Survived']==0][['Age','Sex','Survived']].groupby('Sex')['Age']\nsurvived_a = train[train['Survived']==1][['Age','Sex','Survived']].groupby('Sex')['Age']\n\nfig = plt.figure(figsize=(12,4), dpi=80, facecolor=neon_text[0],edgecolor=neon_text[2])\n\nax1 = fig.add_subplot(121)\nplt.title('Perished')\nax1.set_ylim((0, 60))\nperished_a.plot.hist(ax=ax1,color=neon[1],alpha=0.5,bins=20)\nax1.yaxis.set_label_text('')\n\nax2 = fig.add_subplot(122)\nplt.title('Survived')\nax2.set_ylim((0, 60))\nsurvived_a.plot.hist(ax=ax2,color=neon[5],alpha=0.5,bins=20)\nax2.yaxis.set_label_text('')","f93db414":"perished_fsp = train[(train['Sex']=='female')&(train['Survived']==0)].groupby('Parch')['Parch'].count()\nsurvived_fsp = train[(train['Sex']=='female')&(train['Survived']==1)].groupby('Parch')['Parch'].count()\nperished_msp = train[(train['Sex']=='male')&(train['Survived']==0)].groupby('Parch')['Parch'].count()\nsurvived_msp = train[(train['Sex']=='male')&(train['Survived']==1)].groupby('Parch')['Parch'].count()\n\nfig = plt.figure(figsize=(12,6), dpi=80, facecolor=neon_text[0],edgecolor=neon_text[2])\nax1 = fig.add_subplot(221)\nplt.title('Perished Females by # of Parch')\nperished_fsp.plot(kind='bar', ax=ax1, legend=False,color=neon[1])\nax1.xaxis.set_label_text('')\nplt.xticks(rotation=0)\n\nax2 = fig.add_subplot(222)\nplt.title('Survived Females by # of Parch')\nsurvived_fsp.plot(kind='bar', ax=ax2, legend=False,color=neon[1])\nax2.xaxis.set_label_text('')\nplt.xticks(rotation=0)\n\nax3 = fig.add_subplot(223)\nplt.title('Perished Males by # of Parch')\nperished_msp.plot(kind='bar', ax=ax3, legend=False,color=neon[5])\nax3.xaxis.set_label_text('')\nplt.xticks(rotation=0)\n\nax4 = fig.add_subplot(224)\nplt.title('Survived Males by # of Parch')\nsurvived_msp.plot(kind='bar', ax=ax4, legend=False,color=neon[5])\nax4.xaxis.set_label_text('')\nplt.xticks(rotation=0)","7d7a4fb0":"for dataset in combine:\n    dataset['Title'] = dataset['Title'].apply(lambda x: 'Sir' if (x == 'Sir') | (x == 'Col') | (x == 'Jonkheer') | (x == 'Capt') | (x == 'Don') | (x == 'Major') else x)\n    dataset['Title'] = dataset['Title'].apply(lambda x: 'Miss' if (x == 'Miss') | (x == 'Ms') | (x == 'Mlle') else x)\n    dataset['Title'] = dataset['Title'].apply(lambda x: 'Mrs' if (x == 'Mrs') | (x == 'Mme') else x)\n    dataset['Title'] = dataset['Title'].apply(lambda x: 'Lady' if (x == 'Lady') | (x == 'Countess') | (x == 'Dona') else x)\ncombine[0]['Title'].unique()","f6214e18":"title_survival = pd.DataFrame()\nfor title in combine[0]['Title'].unique():\n    count = combine[0][combine[0]['Title'] == title]['Survived'].sum()\n    survived = round( count \/ combine[0][combine[0]['Title'] == title]['Survived'].shape[0],2)\n    perished = 1 - survived\n    title_survival.at[0,title] = survived\n    title_survival.at[1,title] = perished","e48885fd":"title_survival","783bcd5b":"fig = plt.figure(figsize=(12,4), dpi=80, facecolor=neon_text[0],edgecolor=neon_text[2])\n\nax1 = fig.add_subplot(121)\nplt.title('Perished')\ntitle_survival.loc[1].plot(kind='bar', ax=ax1, legend=False,color=neon[1])\nax1.xaxis.set_label_text('')\nplt.xticks(rotation=0)\n\nax2 = fig.add_subplot(122)\nplt.title('Survived')\ntitle_survival.loc[0].plot(kind='bar', ax=ax2, legend=False,color=neon[5])\nax2.xaxis.set_label_text('')\nplt.xticks(rotation=0)","87fe9b6d":"embarked = train[['Embarked','Survived']]\nperished_e = embarked[embarked['Survived']==0].groupby('Embarked').count()\nsurvived_e = embarked[embarked['Survived']==1].groupby('Embarked').count()\nfor p,s in zip(perished_e['Survived'],survived_e['Survived']):\n    p_ = round(p \/ (p + s)*100,2)\n    perished_e[perished_e['Survived']==p] = p_ \n    s_ = round(s \/ (s + p)*100,2)\n    survived_e[survived_e['Survived']==s] = s_\n    \nfig = plt.figure(figsize=(12,4), dpi=80, facecolor=neon_text[0],edgecolor=neon_text[2])\n\nax1 = fig.add_subplot(121)\nplt.title('% Perished')\nperished_e.plot(kind='bar', ax=ax1, legend=False,color=neon[1])\nax1.yaxis.set_label_text('')\nax1.set_ylim((0, 100))\nplt.xticks(rotation=0)\n\nax2 = fig.add_subplot(122)\nplt.title('% Survived')\nsurvived_e.plot(kind='bar', ax=ax2, legend=False,color=neon[5])\nax2.yaxis.set_label_text('')\nax2.set_ylim((0, 100))\nplt.xticks(rotation=0)","e05dd2a0":"for dataset in combine:\n    dataset['Embarked'] = dataset['Embarked'].fillna(0)\n    dataset['Embarked'] = dataset['Embarked'].apply(lambda x: 1 if x == 'S' else 2 if x == 'Q' else 3)","e42d6eba":"for dataset in combine:\n    dataset['Sex'] = dataset['Sex'].apply(lambda x: 1 if x == 'female' else 0)","32b75a88":"for dataset in combine:\n    dataset['FareGroups'] = pd.qcut(dataset['Fare'],5,labels=False) # sort into Quantile buckets for encoding\n    dataset['Fare'] = dataset['FareGroups']\n    dataset['Fare'] = dataset['Fare'].fillna(0).astype(int)\ncombine[0]['Fare'].unique()","33990c08":"for dataset in combine:\n    # 0% survived of the following\n    dataset['Title'] = dataset['Title'].apply(lambda x: 0 if x == 'Rev' else x)\n    # 16 % survived of the following\n    dataset['Title'] = dataset['Title'].apply(lambda x: 1 if x == 'Mr' else x)\n    # 38% survived of the following\n    dataset['Title'] = dataset['Title'].apply(lambda x: 2 if x == 'Sir' else x)\n    # 43% survived of the following\n    dataset['Title'] = dataset['Title'].apply(lambda x: 3 if x == 'Dr' else x)\n    # 57 % of the following survived\n    dataset['Title'] = dataset['Title'].apply(lambda x: 4 if x == 'Master' else x)\n    # 70% of the following survived\n    dataset['Title'] = dataset['Title'].apply(lambda x: 5 if x == 'Miss' else x)\n    # 79% survived of the following\n    dataset['Title'] = dataset['Title'].apply(lambda x: 6 if x == 'Mrs' else x)\n    # 100% survived of the following\n    dataset['Title'] = dataset['Title'].apply(lambda x: 7 if x == 'Lady' else x)\ncombine[0]['Title'].unique()\n","3ef43601":"for dataset in combine:\n    age_guess = []\n    for sex,p,t in zip(dataset['Sex'],dataset['Pclass'],dataset['Title']):\n        #print(sex,p)\n        age_group = dataset[(dataset['Sex']==sex) & (dataset['Pclass']==p) & (dataset['Title']==t)]['Age'].dropna()\n        median_age = int(age_group.median())\n        age_guess.append(median_age)\n    dataset['AgeGuess'] = age_guess\n    dataset['Age'] = dataset['Age'].fillna(dataset['AgeGuess'])","f0999efa":"for dataset in combine:\n    dataset['AgeBand'] = pd.qcut(dataset['Age'],5,labels=False) # sort into Quantile buckets for encoding","781796bb":"for dataset in combine:\n    dataset['Alone'] = dataset['SibSp'] + dataset['Parch']\n    dataset['Alone'] = dataset['Alone'].apply(lambda x: 1 if x > 0 else 0)","4728412a":"for dataset in combine:\n    dataset['Age'] = dataset['AgeBand'].astype(int)\n    dataset.pop('AgeGuess')\n    dataset.pop('AgeBand')\n    dataset.pop('FareGroups')\n    #dataset.pop('PassengerId')\n    dataset.pop('Name')\n    dataset['AgeClass'] = dataset['Pclass'] + dataset['Age']\ncombine[0].head()","5c43aad5":"combine[0].describe()","d60a5874":"combine[0].corr()","8eda5b5a":"#importing libraries from sklearn\nfrom sklearn import tree\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n\n# import algorithm modules\nfrom sklearn import svm\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_recall_fscore_support as score","327316e2":"train.info()","23883776":"test.info()","4eb4674e":"X_train = train.copy()\nX_train.pop('PassengerId')\ny_train = X_train.pop('Survived')\nX_test = test.copy()","52ea630f":"X_train.head()","d641d036":"# PassengerId is not required for testing\nX_test.pop('PassengerId') # popping this column is a simpler method than dropping it\n# you could save it into a variable if you wanted to --- passenger_id = X_test.pop('PassengerId')\nX_test.head()","246005c3":"results_array = [] # array to store all the scores for the many models that will be created\nimport time\ndef train_rf(n_est, depth,m_leaf):\n    rf = RandomForestClassifier(n_estimators=n_est,\n                                max_depth=depth,\n                                min_samples_leaf = m_leaf,\n                                n_jobs=-1)\n    \n    start = time.time()\n    rf_model = rf.fit(X_train,y_train)\n    end = time.time()\n    fit_time = round((end - start),3)\n    start = time.time()\n    y_pred = rf_model.predict(X_test)\n    end = time.time()\n    pred_time = round((end - start),3)\n    rf_model.score(X_train, y_train)\n    rfs = round(rf_model.score(X_train, y_train) * 100, 2)\n    #precision, recall, fscore, support = score(y_test, y_pred,average='macro') # pass the y labels to the score, the predicted, positive label; what we are predicting\n    results_array.append([rfs,n_est,m_leaf,depth,fit_time,pred_time])","bced8c48":"for n_est in [5,10,15,20,25,30,35,40,50,60,70,80]:\n    for depth in [5,10,15,20,25,30,35,50,75,100,None]:\n        for m_leaf in [0.25,0.5,1,2]:\n            train_rf(n_est,depth,m_leaf)","77b26802":"final = pd.DataFrame(results_array,columns=['score','estimators','min samples','depth','fit time','pred time'])","0af56924":"# sorting by score, using ascending False to place the top scores at the top of the DataFrame\nfinal = final.sort_values(by=['score'],ascending=False)\nfinal.head()","8f23e520":"# let's now grab the top 10 results, sort those by the time it takes for the model to make the prediciton\n# reset the index and take the top result for our model\nfinal = final.head(10)\nfinal = final.sort_values(by=['pred time'])\nfinal = final.reset_index()\nfinal.pop('index')\nfinal","9221e43a":"# this section gets the parameters from the top performing model based on criteria above\nestimators,leaf,depth = final.at[0,'estimators'],final.at[0,'min samples'],final.at[0,'depth']\nif np.isnan(depth):\n    depth = None\nif leaf >= 1:\n    leaf = int(leaf)\nprint('Final Model Parameters:\\n Estimators:',estimators,'\\n Samples:',leaf,'\\nDepth:',depth)","c2053415":"# best score to date 0.76076 with the following\nestimators,leaf,depth = 70,1,25","e04dbac2":"#Random Forest Regression\ndef forestRegression_1(x,y):\n    model = RandomForestClassifier(n_estimators = estimators,\n                                   min_samples_leaf = leaf,\n                                   max_depth = depth,\n                                   n_jobs=-1)\n    model.fit(x, y)\n    return model\n\nrf = forestRegression_1(X_train, y_train)","5e067241":"print('Random Forest',round(rf.score(X_train, y_train)*100,2))","26d646c5":"def predict_outcome(model):\n    \n    def probability_of_survival(num,model):\n        columns = X_train.columns # get the columns from the training set so we drop what we don't need from the test set\n        p = test[test['PassengerId'] == num][columns].values[0] # this way we can get PassengerId without using it\n        e = model.predict_proba([p]).flatten()\n        return e\n    \n    def predict_survival(num,model):\n        columns = X_train.columns\n        p = test[test['PassengerId'] == num][columns].values[0]\n        e = model.predict([p]).flatten()\n        return e[0]\n    \n    inference = []\n    passengers = test['PassengerId']\n    for i in passengers:\n        prob = probability_of_survival(i,model)\n        pred = predict_survival(i,model)\n        inference.append([i,pred,prob])\n    dz = pd.DataFrame(inference)\n    return dz","9edc7774":"r1 = predict_outcome(rf)","d2ca6746":"submission = pd.DataFrame()\nsubmission['PassengerId'] = r1[0]\nsubmission['Survived'] = r1[1]","3c04f9c8":"submission.describe()","d6b931ed":"submission.info()","aeba3910":"print('Test Set Prediction:\\nPerished:',submission[submission['Survived']==0]['Survived'].count(),'\\nSurvived:',submission[submission['Survived']==1]['Survived'].count())","ba91ccd9":"submission.head()","9282f710":"submission.to_csv('gender_submission.csv', index=False)","bc4b3b4f":"### Dropping Columns that don't heavily Influence the Outcome","22df06e6":"# Random Forest Classification","7205152f":"### Separate the Title of the Individual\nextract the titles that give an indication of the individual's class standing and age","c1371a3d":"# Submit Results","58513cef":"### Datasets","e60de90e":"We are going to get an age estimate for ","f32524b6":"### Combining the Train and Test Datasets\n\nThis will allow us to iterate over both datasets and make similar changes to both.","e48554fc":"Assigning 1 to females and 0 to males.\nThis will give a higher importance to females.","24349ae7":"# Data Wrangling & Feature Engineering\n\nAltering the data within both datasets.\nOne Hot Encoding to convert categorical variables into a format that can be fed into the Random Forest algorithm.\n\nmore information found [here](https:\/\/en.wikipedia.org\/wiki\/One-hot)","4a752998":"# Data Visualization\n\nPulling in some varied HEX colours in order to provide some unique visualizations.","63f6c2fe":"accurate_submission","8434ac75":"# Final Prediction\n\nCreating a few functions to cycle through the test data and predict an outcome for each passenger in the list.\n\n* We load in the original test data using PassengerId\n* get the probability of the individual's outcome\n* depending on which probability we get the individual's survival outcome\n* 0 = Perished\n* 1 = Survived","c9e4b4a6":"# Final Model\n\nThis is the final model using the parameters above.","ee6d9c24":"# Exploratory Data Analysis\n### Data Analysis","6fa8a066":"### Libraries","4f4973fb":"# The Titanic Sank - Hopefully This Notebook Doesn't\nSearching for Titanic Survivors using Random Forest Classification.\n\nThis notebook is for the beginner who is new to Machine Learning and Python. I hope that you have some sort of knowledge with Python.","b7516323":"## For Loops to Test the Various Parameters\n\nThe results will be appended to an array (results_array) which will be then loaded into a Pandas DataFrame.\n\nWe will then sort the various variables in the DataFrame to get the best result which is then loaded into the final model.","8ac73a43":"# Testing Random Forest Parameters\n\nYou COULD manually try a bunch of variations, but that brute force method wouldn't be very elegant would it?\n\nBelow we define a function that will be used in a for loop to test many different parameters we have provided in arrays to get the optimal Model.\n\nI have cut this list down, but it could be way more EXTENSIVE than this. And should be in a professional setting."}}