{"cell_type":{"3a574df6":"code","a89fb6a0":"code","59b986ca":"code","40e33ead":"code","32d28673":"code","eda88024":"code","e8ed4607":"code","76ffc0f2":"code","88b9653a":"code","1f4c2664":"code","835e7a38":"code","e40c2a68":"code","8d4d7eae":"code","eba5cbb2":"code","0c0e040f":"code","d81b532e":"code","656d2a9c":"code","a240a343":"code","5defa103":"code","937cd663":"code","73f47623":"code","2cd86772":"code","7cfc9ad2":"code","8afe840b":"code","5cfa916f":"code","9d87fbe2":"code","176a1579":"code","c4673f71":"code","a32979e4":"code","8e816a86":"code","d41340aa":"code","d8b7c03a":"code","3291a1c0":"code","bce0cc0f":"code","6ec1ae33":"code","7e3d8032":"code","f8fa2f1b":"code","a35358b3":"code","977cce45":"code","aa985793":"code","1d9368a0":"code","9349eab2":"code","aaaf73e9":"code","89b22957":"code","db677178":"code","d1dd97b2":"code","8f929d70":"code","ef86bb0c":"code","8b54332d":"code","db4d98ec":"code","f1d71b87":"code","561ad7ff":"code","cc303d8f":"code","3c027cd0":"code","b0076006":"code","88b942d9":"code","8d805d49":"markdown","f18b0cf1":"markdown","c3fa7c9a":"markdown","1ebe7d01":"markdown","63085a70":"markdown","7af46faa":"markdown","192802bd":"markdown","953006cb":"markdown","f5807ff7":"markdown","12083ba6":"markdown","79759e5d":"markdown","d106700b":"markdown","54a697b5":"markdown","5bfc4534":"markdown","3bf65142":"markdown","584224a5":"markdown","adfcc22c":"markdown","63b95034":"markdown","581732a2":"markdown","5ab756ae":"markdown","1a4844f3":"markdown","39550fa4":"markdown","b0e07487":"markdown","9b97c078":"markdown","8da71b42":"markdown","25e31b2d":"markdown","d57acbd1":"markdown","c974bd47":"markdown","e4119c32":"markdown","7cff55ca":"markdown","5cef24fd":"markdown","5037d35e":"markdown","d93719f9":"markdown","dfa3aa95":"markdown","5f5c17bb":"markdown","e92baaea":"markdown","61e85064":"markdown","5c950156":"markdown","e66d80cc":"markdown","558a57ad":"markdown","848130d4":"markdown","238e7699":"markdown","aa1e8ae4":"markdown","23e55c92":"markdown","a4eb3fcf":"markdown","50c82831":"markdown","5adc0766":"markdown","8dbcc063":"markdown","668c18c0":"markdown","edcd09d3":"markdown","d6e55774":"markdown","653660bd":"markdown","ebb6db02":"markdown","cbdf80f6":"markdown"},"source":{"3a574df6":"import numpy as np\nimport pandas as pd\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 100)\n\nfrom itertools import product\nfrom sklearn.preprocessing import LabelEncoder\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport time\nimport sys\nimport gc\nimport pickle\nsys.version_info","a89fb6a0":"# load data\nitems = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/items.csv')\nshops = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/shops.csv')\ncats = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/item_categories.csv')\ntrain = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/sales_train.csv')\n# set index to ID to avoid droping it later\ntest  = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/test.csv').set_index('ID')","59b986ca":"train.head()","40e33ead":"print('train size, item in train, shop in train', train.shape[0], train.item_id.nunique(), train.shop_id.nunique())\nprint('train size, item in train, shop in train', test.shape[0], test.item_id.nunique(),test.shop_id.nunique())\nprint('new items:', len(list(set(test.item_id) - set(test.item_id).intersection(set(train.item_id)))), len(list(set(test.item_id))), len(test))","32d28673":"train.isnull().sum()","eda88024":"sale_by_month = train.groupby('date_block_num')['item_cnt_day'].sum()\nsale_by_month.plot()","e8ed4607":"block_item_shop_sale = train.groupby(['date_block_num','item_id','shop_id'])['item_cnt_day'].sum()\nblock_item_shop_sale.clip(0,20).plot.hist(bins=20)","76ffc0f2":"plt.figure(figsize=(10,4))\nplt.xlim(-100, 3000)\nsns.boxplot(x=train.item_cnt_day)\n\nplt.figure(figsize=(10,4))\nplt.xlim(train.item_price.min(), train.item_price.max()*1.1)\nsns.boxplot(x=train.item_price)\n\ntrain = train[train.item_price<100000]\ntrain = train[train.item_cnt_day<1001]","88b9653a":"median = train[(train.shop_id==32)&(train.item_id==2973)&(train.date_block_num==4)&(train.item_price>0)].item_price.median()\ntrain.loc[train.item_price<0, 'item_price'] = median","1f4c2664":"# \u042f\u043a\u0443\u0442\u0441\u043a \u041e\u0440\u0434\u0436\u043e\u043d\u0438\u043a\u0438\u0434\u0437\u0435, 56\ntrain.loc[train.shop_id == 0, 'shop_id'] = 57\ntest.loc[test.shop_id == 0, 'shop_id'] = 57\n# \u042f\u043a\u0443\u0442\u0441\u043a \u0422\u0426 \"\u0426\u0435\u043d\u0442\u0440\u0430\u043b\u044c\u043d\u044b\u0439\"\ntrain.loc[train.shop_id == 1, 'shop_id'] = 58\ntest.loc[test.shop_id == 1, 'shop_id'] = 58\n# \u0416\u0443\u043a\u043e\u0432\u0441\u043a\u0438\u0439 \u0443\u043b. \u0427\u043a\u0430\u043b\u043e\u0432\u0430 39\u043c\u00b2\ntrain.loc[train.shop_id == 10, 'shop_id'] = 11\ntest.loc[test.shop_id == 10, 'shop_id'] = 11","835e7a38":"shops.loc[shops.shop_name == '\u0421\u0435\u0440\u0433\u0438\u0435\u0432 \u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"', 'shop_name'] = '\u0421\u0435\u0440\u0433\u0438\u0435\u0432\u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"'\nshops['city'] = shops['shop_name'].str.split(' ').map(lambda x: x[0])\nshops.loc[shops.city == '!\u042f\u043a\u0443\u0442\u0441\u043a', 'city'] = '\u042f\u043a\u0443\u0442\u0441\u043a'\nshops['city_code'] = LabelEncoder().fit_transform(shops['city'])\nshops = shops[['shop_id','city_code']]\n\ncats['split'] = cats['item_category_name'].str.split('-')\ncats['type'] = cats['split'].map(lambda x: x[0].strip())\ncats['type_code'] = LabelEncoder().fit_transform(cats['type'])\n# if subtype is nan then type\ncats['subtype'] = cats['split'].map(lambda x: x[1].strip() if len(x) > 1 else x[0].strip())\ncats['subtype_code'] = LabelEncoder().fit_transform(cats['subtype'])\ncats = cats[['item_category_id','type_code', 'subtype_code']]\n\nitems.drop(['item_name'], axis=1, inplace=True)","e40c2a68":"ts = time.time()\nmatrix = []\ncols = ['date_block_num','shop_id','item_id']\nfor i in range(34):\n    sales = train[train.date_block_num==i]\n    matrix.append(np.array(list(product([i], sales.shop_id.unique(), sales.item_id.unique())), dtype='int16'))\n    \nmatrix = pd.DataFrame(np.vstack(matrix), columns=cols)\nmatrix['date_block_num'] = matrix['date_block_num'].astype(np.int8)\nmatrix['shop_id'] = matrix['shop_id'].astype(np.int8)\nmatrix['item_id'] = matrix['item_id'].astype(np.int16)\nmatrix.sort_values(cols,inplace=True)\ntime.time() - ts","8d4d7eae":"train['revenue'] = train['item_price'] *  train['item_cnt_day']","eba5cbb2":"\ngroup = train.groupby(['date_block_num','shop_id','item_id']).agg({'item_cnt_day': ['sum']})\ngroup.columns = ['item_cnt_month']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=cols, how='left')\nmatrix['item_cnt_month'] = (matrix['item_cnt_month']\n                                .fillna(0)\n                                .clip(0,20) # NB clip target here\n                                .astype(np.float32))\n","0c0e040f":"test['date_block_num'] = 34\ntest['date_block_num'] = test['date_block_num'].astype(np.int8)\ntest['shop_id'] = test['shop_id'].astype(np.int8)\ntest['item_id'] = test['item_id'].astype(np.int16)","d81b532e":"\nmatrix = pd.concat([matrix, test], ignore_index=True, sort=False, keys=cols)\nmatrix.fillna(0, inplace=True) # 34 month\n","656d2a9c":"\nmatrix = pd.merge(matrix, shops, on=['shop_id'], how='left')\nmatrix = pd.merge(matrix, items, on=['item_id'], how='left')\nmatrix = pd.merge(matrix, cats, on=['item_category_id'], how='left')\nmatrix['city_code'] = matrix['city_code'].astype(np.int8)\nmatrix['item_category_id'] = matrix['item_category_id'].astype(np.int8)\nmatrix['type_code'] = matrix['type_code'].astype(np.int8)\nmatrix['subtype_code'] = matrix['subtype_code'].astype(np.int8)\n","a240a343":"def lag_feature(df, lags, col):\n    tmp = df[['date_block_num','shop_id','item_id',col]]\n    for i in lags:\n        shifted = tmp.copy()\n        shifted.columns = ['date_block_num','shop_id','item_id', col+'_lag_'+str(i)]\n        shifted['date_block_num'] += i\n        df = pd.merge(df, shifted, on=['date_block_num','shop_id','item_id'], how='left')\n    return df","5defa103":"\nmatrix = lag_feature(matrix, [1,2,3,6,12], 'item_cnt_month')","937cd663":"def add_group_stats(matrix_, groupby_feats, target, enc_feat, last_periods):\n    if not 'date_block_num' in groupby_feats:\n        print ('date_block_num must in groupby_feats')\n        return matrix_\n    \n    group = matrix_.groupby(groupby_feats)[target].sum().reset_index()\n    max_lags = np.max(last_periods)\n    for i in range(1,max_lags+1):\n        shifted = group[groupby_feats+[target]].copy(deep=True)\n        shifted['date_block_num'] += i\n        shifted.rename({target:target+'_lag_'+str(i)},axis=1,inplace=True)\n        group = group.merge(shifted, on=groupby_feats, how='left')\n    group.fillna(0,inplace=True)\n    for period in last_periods:\n        lag_feats = [target+'_lag_'+str(lag) for lag in np.arange(1,period+1)]\n        # we do not use mean and svd directly because we want to include months with sales = 0\n        mean = group[lag_feats].sum(axis=1)\/float(period)\n        mean2 = (group[lag_feats]**2).sum(axis=1)\/float(period)\n        group[enc_feat+'_avg_sale_last_'+str(period)] = mean\n        group[enc_feat+'_std_sale_last_'+str(period)] = (mean2 - mean**2).apply(np.sqrt)\n        group[enc_feat+'_std_sale_last_'+str(period)].replace(np.inf,0,inplace=True)\n        # divide by mean, this scales the features for NN\n        group[enc_feat+'_avg_sale_last_'+str(period)] \/= group[enc_feat+'_avg_sale_last_'+str(period)].mean()\n        group[enc_feat+'_std_sale_last_'+str(period)] \/= group[enc_feat+'_std_sale_last_'+str(period)].mean()\n    cols = groupby_feats + [f_ for f_ in group.columns.values if f_.find('_sale_last_')>=0]\n    matrix = matrix_.merge(group[cols], on=groupby_feats, how='left')\n    return matrix","73f47623":"ts = time.time()\nmatrix = add_group_stats(matrix, ['date_block_num', 'item_id'], 'item_cnt_month', 'item', [6,12])\nmatrix = add_group_stats(matrix, ['date_block_num', 'shop_id'], 'item_cnt_month', 'shop', [6,12])\nmatrix = add_group_stats(matrix, ['date_block_num', 'item_category_id'], 'item_cnt_month', 'category', [12])\nmatrix = add_group_stats(matrix, ['date_block_num', 'city_code'], 'item_cnt_month', 'city', [12])\nmatrix = add_group_stats(matrix, ['date_block_num', 'type_code'], 'item_cnt_month', 'type', [12])\nmatrix = add_group_stats(matrix, ['date_block_num', 'subtype_code'], 'item_cnt_month', 'subtype', [12])\ntime.time() - ts","2cd86772":"#first use target encoding each group, then shift month to creat lag features\ndef target_encoding(matrix_, groupby_feats, target, enc_feat, lags):\n    print ('target encoding for',groupby_feats)\n    group = matrix_.groupby(groupby_feats).agg({target:'mean'})\n    group.columns = [enc_feat]\n    group.reset_index(inplace=True)\n    matrix = matrix_.merge(group, on=groupby_feats, how='left')\n    matrix[enc_feat] = matrix[enc_feat].astype(np.float16)\n    matrix = lag_feature(matrix, lags, enc_feat)\n    matrix.drop(enc_feat, axis=1, inplace=True)\n    return matrix","7cfc9ad2":"ts = time.time()\nmatrix = target_encoding(matrix, ['date_block_num'], 'item_cnt_month', 'date_avg_item_cnt', [1])\nmatrix = target_encoding(matrix, ['date_block_num', 'item_id'], 'item_cnt_month', 'date_item_avg_item_cnt', [1,2,3,6,12])\nmatrix = target_encoding(matrix, ['date_block_num', 'shop_id'], 'item_cnt_month', 'date_shop_avg_item_cnt', [1,2,3,6,12])\nmatrix = target_encoding(matrix, ['date_block_num', 'item_category_id'], 'item_cnt_month', 'date_cat_avg_item_cnt', [1])\nmatrix = target_encoding(matrix, ['date_block_num', 'shop_id', 'item_category_id'], 'item_cnt_month', 'date_shop_cat_avg_item_cnt', [1])\nmatrix = target_encoding(matrix, ['date_block_num', 'city_code'], 'item_cnt_month', 'date_city_avg_item_cnt', [1])\nmatrix = target_encoding(matrix, ['date_block_num', 'item_id', 'city_code'], 'item_cnt_month', 'date_item_city_avg_item_cnt', [1])\ntime.time() - ts","8afe840b":"ts = time.time()\ngroup = train.groupby(['item_id']).agg({'item_price': ['mean']})\ngroup.columns = ['item_avg_item_price']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['item_id'], how='left')\nmatrix['item_avg_item_price'] = matrix['item_avg_item_price'].astype(np.float16)\n\ngroup = train.groupby(['date_block_num','item_id']).agg({'item_price': ['mean']})\ngroup.columns = ['date_item_avg_item_price']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','item_id'], how='left')\nmatrix['date_item_avg_item_price'] = matrix['date_item_avg_item_price'].astype(np.float16)\n\nlags = [1,2,3,4,5,6]\nmatrix = lag_feature(matrix, lags, 'date_item_avg_item_price')\n\nfor i in lags:\n    matrix['delta_price_lag_'+str(i)] = \\\n        (matrix['date_item_avg_item_price_lag_'+str(i)] - matrix['item_avg_item_price']) \/ matrix['item_avg_item_price']\n\ndef select_trend(row):\n    for i in lags:\n        if row['delta_price_lag_'+str(i)]:\n            return row['delta_price_lag_'+str(i)]\n    return 0\n    \nmatrix['delta_price_lag'] = matrix.apply(select_trend, axis=1)\nmatrix['delta_price_lag'] = matrix['delta_price_lag'].astype(np.float16)\nmatrix['delta_price_lag'].fillna(0, inplace=True)\n\n# https:\/\/stackoverflow.com\/questions\/31828240\/first-non-null-value-per-row-from-a-list-of-pandas-columns\/31828559\n# matrix['price_trend'] = matrix[['delta_price_lag_1','delta_price_lag_2','delta_price_lag_3']].bfill(axis=1).iloc[:, 0]\n# Invalid dtype for backfill_2d [float16]\n\nfetures_to_drop = ['item_avg_item_price', 'date_item_avg_item_price']\nfor i in lags:\n    fetures_to_drop += ['date_item_avg_item_price_lag_'+str(i)]\n    fetures_to_drop += ['delta_price_lag_'+str(i)]\n\nmatrix.drop(fetures_to_drop, axis=1, inplace=True)\n\ntime.time() - ts","5cfa916f":"ts = time.time()\ngroup = train.groupby(['date_block_num','shop_id']).agg({'revenue': ['sum']})\ngroup.columns = ['date_shop_revenue']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','shop_id'], how='left')\nmatrix['date_shop_revenue'] = matrix['date_shop_revenue'].astype(np.float32)\n\ngroup = group.groupby(['shop_id']).agg({'date_shop_revenue': ['mean']})\ngroup.columns = ['shop_avg_revenue']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['shop_id'], how='left')\nmatrix['shop_avg_revenue'] = matrix['shop_avg_revenue'].astype(np.float32)\n\nmatrix['delta_revenue'] = (matrix['date_shop_revenue'] - matrix['shop_avg_revenue']) \/ matrix['shop_avg_revenue']\nmatrix['delta_revenue'] = matrix['delta_revenue'].astype(np.float16)\n\nmatrix = lag_feature(matrix, [1], 'delta_revenue')\n\nmatrix.drop(['date_shop_revenue','shop_avg_revenue','delta_revenue'], axis=1, inplace=True)\ntime.time() - ts","9d87fbe2":"matrix['month'] = matrix['date_block_num'] % 12\nmatrix['year'] = (matrix['date_block_num'] \/ 12).astype(np.int8)","176a1579":"#Month since last sale for each shop\/item pair.\nts = time.time()\nlast_sale = pd.DataFrame()\nfor month in range(1,35):    \n    last_month = matrix.loc[(matrix['date_block_num']<month)&(matrix['item_cnt_month']>0)].groupby(['item_id','shop_id'])['date_block_num'].max()\n    df = pd.DataFrame({'date_block_num':np.ones([last_month.shape[0],])*month,\n                       'item_id': last_month.index.get_level_values(0).values,\n                       'shop_id': last_month.index.get_level_values(1).values,\n                       'item_shop_last_sale': last_month.values})\n    last_sale = last_sale.append(df)\nlast_sale['date_block_num'] = last_sale['date_block_num'].astype(np.int8)\n\nmatrix = matrix.merge(last_sale, on=['date_block_num','item_id','shop_id'], how='left')\ntime.time() - ts","c4673f71":"#Month since last sale for each item.\nts = time.time()\nlast_sale = pd.DataFrame()\nfor month in range(1,35):    \n    last_month = matrix.loc[(matrix['date_block_num']<month)&(matrix['item_cnt_month']>0)].groupby('item_id')['date_block_num'].max()\n    df = pd.DataFrame({'date_block_num':np.ones([last_month.shape[0],])*month,\n                       'item_id': last_month.index.values,\n                       'item_last_sale': last_month.values})\n    last_sale = last_sale.append(df)\nlast_sale['date_block_num'] = last_sale['date_block_num'].astype(np.int8)\n\nmatrix = matrix.merge(last_sale, on=['date_block_num','item_id'], how='left')\ntime.time() - ts","a32979e4":"# Months since the first sale for each shop\/item pair and for item only.\nts = time.time()\nmatrix['item_shop_first_sale'] = matrix['date_block_num'] - matrix.groupby(['item_id','shop_id'])['date_block_num'].transform('min')\nmatrix['item_first_sale'] = matrix['date_block_num'] - matrix.groupby('item_id')['date_block_num'].transform('min')\ntime.time() - ts","8e816a86":"matrix = matrix[matrix.date_block_num > 11]\nmatrix.columns","d41340aa":"matrix.to_pickle('data.pkl')\ndel matrix\ndel group\ndel items\ndel shops\ndel cats\ndel train\n# leave test for submission\ngc.collect();","d8b7c03a":"import numpy as np\nimport pandas as pd\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 100)\n\nfrom itertools import product\nfrom sklearn.preprocessing import LabelEncoder\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport time\nimport sys\nimport gc\nimport pickle\nsys.version_info","3291a1c0":"data = pd.read_pickle('..\/input\/predict-future-sales-ensemble\/data.pkl')\ndata.head()","bce0cc0f":"data = data[[\n    'date_block_num',\n    'shop_id',\n    #'item_id',\n    'item_cnt_month',\n    'city_code',\n    'item_category_id',\n    'type_code','subtype_code',\n    'item_cnt_month_lag_1','item_cnt_month_lag_2','item_cnt_month_lag_3','item_cnt_month_lag_6','item_cnt_month_lag_12',\n    'item_avg_sale_last_6', 'item_std_sale_last_6',\n    'item_avg_sale_last_12', 'item_std_sale_last_12',\n    'shop_avg_sale_last_6', 'shop_std_sale_last_6',\n    'shop_avg_sale_last_12', 'shop_std_sale_last_12',\n    'category_avg_sale_last_12', 'category_std_sale_last_12',\n    'city_avg_sale_last_12', 'city_std_sale_last_12',\n    'type_avg_sale_last_12', 'type_std_sale_last_12',\n    'subtype_avg_sale_last_12', 'subtype_std_sale_last_12',\n    'date_avg_item_cnt_lag_1',\n    'date_item_avg_item_cnt_lag_1','date_item_avg_item_cnt_lag_2','date_item_avg_item_cnt_lag_3','date_item_avg_item_cnt_lag_6','date_item_avg_item_cnt_lag_12',\n    'date_shop_avg_item_cnt_lag_1','date_shop_avg_item_cnt_lag_2','date_shop_avg_item_cnt_lag_3','date_shop_avg_item_cnt_lag_6','date_shop_avg_item_cnt_lag_12',\n    'date_cat_avg_item_cnt_lag_1',\n    'date_shop_cat_avg_item_cnt_lag_1',\n    'date_city_avg_item_cnt_lag_1',\n    'date_item_city_avg_item_cnt_lag_1',\n    'delta_price_lag',\n    'month','year',\n    'item_shop_last_sale','item_last_sale',\n    'item_shop_first_sale','item_first_sale',\n]]\n\ncat_feats = ['shop_id','city_code','item_category_id','type_code','subtype_code']","6ec1ae33":"X_train = data[data.date_block_num < 33].drop(['item_cnt_month'], axis=1)\ny_train = data[data.date_block_num < 33]['item_cnt_month']\nX_valid = data[data.date_block_num == 33].drop(['item_cnt_month'], axis=1)\ny_valid = data[data.date_block_num == 33]['item_cnt_month']\nX_test = data[data.date_block_num == 34].drop(['item_cnt_month'], axis=1)\n\ndel data\ngc.collect();\n","7e3d8032":"%%time\nfrom catboost import CatBoostRegressor\nfrom sklearn.metrics import mean_squared_error\n\nmodel_catboost = CatBoostRegressor(verbose=False, random_seed=566)\n\n\nmodel_catboost.fit(X_train,y_train);\nprediction_catboost = model_catboost.predict(X_valid)\nprint(mean_squared_error(y_valid, prediction_catboost, squared=False))\n\nnp.save('prediction_catboost.npy', prediction_catboost)\npickle.dump(model_catboost, open('model_catboost.pkl', 'wb'))\n","f8fa2f1b":"# prediction = model_catboost.predict(X_test)\n# prediction_catboost = prediction\n\n# submission = pd.DataFrame({\n#     \"ID\": np.arange(prediction.shape[0]), \n#     \"item_cnt_month\": prediction.clip(0, 20)\n# })\n# submission.to_csv('submission_catboost.csv', index=False)","a35358b3":"importance = pd.DataFrame({'feature':X_train.columns, 'importance': model_catboost.feature_importances_})\nimportance.sort_values('importance', ascending=False).set_index('feature').plot(kind='barh', figsize=(10,20))\nplt.show()","977cce45":"importance.sort_values('importance', ascending=False).feature.values","aa985793":"%%time\nfrom catboost import CatBoostRegressor\nfrom sklearn.metrics import mean_squared_error\n\nmain_features = ['item_cnt_month_lag_1', 'item_first_sale',\n       'date_item_avg_item_cnt_lag_1', 'item_category_id']\n\n\n\nmodel_catboost = CatBoostRegressor(verbose=False, random_seed=566)\n\n\nmodel_catboost.fit(X_train[main_features],y_train);\nprediction_catboost = model_catboost.predict(X_valid[main_features])\nprint(mean_squared_error(y_valid, prediction_catboost, squared=False))\n\n","1d9368a0":"%%time\n\nfrom lightgbm import LGBMRegressor\nfrom sklearn.metrics import mean_squared_error\n\n\n\nmodel_lgbm = LGBMRegressor(\n    max_depth = 8,\n    n_estimators = 500,\n    colsample_bytree=0.7,\n    min_child_weight = 300,\n    reg_alpha = 0.1,\n    reg_lambda = 1,\n    random_state = 42,\n)\n\nmodel_lgbm.fit(\n    X_train, \n    y_train, \n    eval_metric=\"rmse\", \n    eval_set=[(X_train, y_train), (X_valid, y_valid)], \n    verbose=10, \n    early_stopping_rounds = 40,\n    categorical_feature = cat_feats) \n\n\nprediction_lgbm = model_lgbm.predict(X_valid)\nprint(mean_squared_error(y_valid, prediction_lgbm, squared=False))\n\nnp.save('prediction_lgbm.npy', prediction_lgbm)\npickle.dump(model_lgbm, open('model_lgbm.pkl', 'wb'))","9349eab2":"prediction_lgbm = model_lgbm.predict(X_valid)\nprint(mean_squared_error(y_valid, prediction_lgbm, squared=False))","aaaf73e9":"prediction = model_lgbm.predict(X_test)\nprediction_lgbm = prediction\n\nsubmission = pd.DataFrame({\n    \"ID\": np.arange(prediction.shape[0]), \n    \"item_cnt_month\": prediction.clip(0, 20)\n})\nsubmission.to_csv('submission_lgbm.csv', index=False)","89b22957":"import numpy as np\nimport pandas as pd\nfrom sklearn.metrics import mean_squared_error\nfrom keras.models import Model\nfrom keras.layers import Input, Dense, Dropout\nfrom keras.regularizers import l2, l1\nfrom keras.optimizers import RMSprop, Adam\n#from tensorflow import set_random_seed\nimport tensorflow as tf    \n\nnp.random.seed(566)\n\n# define model\ndef Sales_prediction_model(input_shape):\n    in_layer = Input(input_shape)\n    x = Dense(16,kernel_initializer='RandomUniform', kernel_regularizer=l2(0.02), activation = \"relu\")(in_layer)\n    x = Dense(8, kernel_initializer='RandomUniform', kernel_regularizer=l2(0.02), activation = \"relu\")(x)\n    x = Dense(1, kernel_initializer='RandomUniform', kernel_regularizer=l2(0.02), activation = \"relu\")(x)\n    \n    model = Model(inputs = in_layer, outputs = x, name='Sales_prediction_model')\n    return model\n\n# NN cannot take missing values, fill NaN with 0.\nX_train.fillna(0,inplace=True)\nX_valid.fillna(0,inplace=True)\nX_test.fillna(0,inplace=True)\n\n# We do no feature scaling here. \n# Some features like 'item_avg_sale_last_6' are already scaled in feature engineering part.\n\ninput_shape = [X_train.shape[1]]\nmodel_nn = Sales_prediction_model(input_shape)\nmodel_nn.compile(optimizer = Adam(lr=0.0005) , loss = [\"mse\"], metrics=['mse'])\nmodel_nn.fit(X_train, y_train, validation_data = (X_valid, y_valid), batch_size = 10000, epochs=5)\n\n\nprediction_nn = model_nn.predict(X_valid)\nprint(mean_squared_error(y_valid, prediction_nn, squared=False))\n\nnp.save('prediction_nn.npy', prediction_nn)\n# pickle.dump(model_nn, open('model_nn.pkl', 'wb'))","db677178":"prediction.clip(0, 20).flatten()","d1dd97b2":"# prediction = model_nn.predict(X_test)\n# prediction_nn = prediction\n\n# submission = pd.DataFrame({\n#     \"ID\": np.arange(prediction.shape[0]), \n#     \"item_cnt_month\": prediction.clip(0, 20).flatten()\n# })\n# submission.to_csv('submission_nn.csv', index=False)","8f929d70":"%%time\nfrom xgboost import XGBRegressor\n\nmodel_xgb = XGBRegressor(\n    max_depth=7,\n    n_estimators=1000,\n    min_child_weight=300,   \n    colsample_bytree=0.8, \n    subsample=0.8, \n    gamma = 0.005,\n    eta=0.1,    \n    seed=42)\n\nmodel_xgb.fit(\n    X_train, \n    y_train, \n    eval_metric=\"rmse\", \n    eval_set=[(X_train, y_train), (X_valid, y_valid)], \n    verbose=10, \n    early_stopping_rounds = 40,\n    )\n\n\nprediction_xgb = model_xgb.predict(X_valid)\nprint(mean_squared_error(y_valid, prediction_xgb, squared=False))\n\nnp.save('prediction_xgb.npy', prediction_xgb)\npickle.dump(model_xgb, open('model_xgb.pkl', 'wb'))","ef86bb0c":"# prediction = model_xgb.predict(X_test)\n# prediction_xgb = prediction\n\n# submission = pd.DataFrame({\n#     \"ID\": np.arange(prediction.shape[0]), \n#     \"item_cnt_month\": prediction.clip(0, 20)\n# })\n# submission.to_csv('submission_xgb.csv', index=False)","8b54332d":"# Skip this part if the models haven't been saved before or correct paths in your enviroment\nmodel_catboost = pickle.load(open('..\/input\/predict-future-sales-models\/model_catboost.pkl','rb'))\nmodel_lgbm = pickle.load(open('..\/input\/predict-future-sales-models\/model_lgbm.pkl','rb'))\nmodel_xgb = pickle.load(open('..\/input\/model-xgb-new\/model_xgb.pkl','rb'))","db4d98ec":"%%time\nmodels = [model_lgbm, model_catboost, model_nn, model_xgb] # public score 0.90556\n\n# models = [model_lgbm, model_catboost] # public score 0.90728\n\nmeta_X_train = [model.predict(X_valid) for model in models]\n\nmeta_X_train[2] = meta_X_train[2].flatten()\n\nmeta_X_train = np.array(meta_X_train).T\n\nmeta_X_test = [model.predict(X_test) for model in models]\n\nmeta_X_test[2] = meta_X_test[2].flatten()\n\nmeta_X_test = np.array(meta_X_test).T\n\nmeta_y_train = y_valid","f1d71b87":"from sklearn.metrics import mean_squared_error\n\n# choose models \n# model_index = [0,1]  # lgbm and catboost, public score 0.90617\n# model_index = [0,2]  # lgbm and nn, public score \nmodel_index = [0,-1]  # lgbm and xgb, public score 0.90462\n\nalphas_to_try = np.linspace(0, 1, 1001)\nrmse_scores = np.empty_like(alphas_to_try)\n\nfor ind, alpha in enumerate(alphas_to_try):\n    simple_mix_pred = alpha * meta_X_train[:,model_index[0]] + (1 - alpha) * meta_X_train[:,model_index[1]]\n    rmse_scores[ind] = mean_squared_error(meta_y_train, simple_mix_pred, squared=False)\n\n\nbest_alpha = alphas_to_try[np.argmin(rmse_scores)]\nrmse_train_simple_mix = np.min(rmse_scores) \n\nprint('Best alpha: %f; Corresponding rmse score on train: %f' % (best_alpha, rmse_train_simple_mix))","561ad7ff":"\n# # remember that meta_X_test is a prediction for a model for X_test\n# meta_prediction = best_alpha * meta_X_test[:,model_index[0]] + (1 - best_alpha) * meta_X_test[:,model_index[1]]\n# submission = pd.DataFrame({\n#     \"ID\": np.arange(meta_prediction.shape[0]), \n#     \"item_cnt_month\": meta_prediction.clip(0, 20).flatten()\n# })\n# submission.to_csv('submission_meta_simple_mix_lgbm_xgb.csv', index=False)","cc303d8f":"%%time\nfrom sklearn.metrics import mean_squared_error\n\n\nalphas_to_try = np.linspace(0, 1, 51)\nbetas_to_try = np.linspace(0, 1, 51)\ngammas_to_try = np.linspace(0, 1, 51)\nrmse_scores = np.zeros((len(alphas_to_try), len(betas_to_try),len(gammas_to_try)))\n\nfor ind1, alpha in enumerate(alphas_to_try):\n    for ind2, beta in enumerate(betas_to_try):  \n        for ind3, gamma in enumerate(gammas_to_try):  \n            simple_mix_pred = alpha * meta_X_train[:,0] + beta * meta_X_train[:,1] +  gamma * meta_X_train[:,3]\n            rmse_scores[ind1, ind2, ind3] = mean_squared_error(meta_y_train, simple_mix_pred, squared=False)\n\n\nind1, ind2, ind3 = np.unravel_index(rmse_scores.argmin(), rmse_scores.shape)\nbest_alpha, best_beta, best_gamma = alphas_to_try[ind1], betas_to_try[ind2], gammas_to_try[ind3]\nrmse_train_simple_mix = np.min(rmse_scores) \n\nprint('Best alpha: %f; best beta: %f; best gamma: %f; Corresponding rmse score on train: %f' % (best_alpha, best_beta, best_gamma, rmse_train_simple_mix))","3c027cd0":"# # remember that meta_X_test is a prediction for a model for X_test\n# meta_prediction = best_alpha * meta_X_test[:,0] + best_beta * meta_X_test[:,1] + best_gamma * meta_X_test[:,3]\n# submission = pd.DataFrame({\n#     \"ID\": np.arange(meta_prediction.shape[0]), \n#     \"item_cnt_month\": meta_prediction.clip(0, 20).flatten()\n# })\n# submission.to_csv('submission_meta_simple_mix_3_models.csv', index=False)","b0076006":"from sklearn.linear_model import LinearRegression\nfrom lightgbm import LGBMRegressor\n\nmeta_model = LinearRegression()\n\n# meta_model =LGBMRegressor(\n#     max_depth = 2,\n#     n_estimators = 500,\n#     colsample_bytree=0.7,\n#     min_child_weight = 300,\n#     reg_alpha = 0.1,\n#     reg_lambda = 1,\n#     random_state = 42,\n# )\n\n\nmeta_model.fit(meta_X_train, meta_y_train)\n\nmeta_prediction = meta_model.predict(meta_X_test)\nprint(mean_squared_error(meta_y_train, meta_model.predict(meta_X_train), squared=False))\n\n# np.save('meta_prediction_lr_2.npy', meta_prediction)","88b942d9":"# submission = pd.DataFrame({\n#     \"ID\": np.arange(meta_prediction.shape[0]), \n#     \"item_cnt_month\": meta_prediction.clip(0, 20).flatten()\n# })\n# submission.to_csv('submission_meta_lr_4_models.csv', index=False)","8d805d49":"# Exploratory Data Analysis","f18b0cf1":"There is no missing value in the training set.\n\nPlot the total sale of each month, we see a clear trend and seasonality. The overall sale is decreasing with time, and there are peaks in November.","c3fa7c9a":"# Read data from file","1ebe7d01":"\n# Feature engineering and data cleaning","63085a70":"## Public score: meta_model LR 0.90443","7af46faa":"# Models","192802bd":"## Shops\/Items\/Cats features","953006cb":"## Final preparations\nBecause of the using 12 as lag value drop first 12 months. Also drop all the columns with this month calculated values (other words which can not be calcucated for the test set).\nLightgbm and XGBboost can deal with missing values, so we will leave the NaNs as it is. Later for neural network, we will fill na with 0.","f5807ff7":"## Trend features","12083ba6":"## Three models","79759e5d":"## Meta features","d106700b":"Several shops are duplicates of each other (according to its name). Fix train and test set.","54a697b5":"# Predict Future Sales: feature engineering and ensemble approach","5bfc4534":"### Train using the most important features","3bf65142":"## Instructions:\n\n* Please note that it may take a lot of time to run the whole notebook.\n\n* If you run it for the first time, you need first to generate the data for models. Run the notebook up to [this point](#stop_point) first.\n\n* Next you train specific models that you want. XGBoost is the most time consuming.\n\n* In order to get the best submission, you need to run only Light GBM, which is quite fast.\n\n* If you want to get a submission file for a specific model, you need to uncomment the corresponding cell.\n\n* If you want to run it many times then you, then you can save your time by using saved data and models. You might need to correct paths to saved files in your enviroment. ","584224a5":"## Simple convex mix","adfcc22c":"## Public score: 0.91555","63b95034":"#### Shops\/Cats\/Items preprocessing\nObservations:\n* Each shop_name starts with the city name.\n* Each category contains type and subtype in its name.","581732a2":"## Public score: 0.90462","5ab756ae":"### Use the best $\\alpha$ to make predictions","1a4844f3":"\n\n#### Exploratary Data Analysis\n* load data\n* trend of sales\n* distribution of target\n\n#### Data Cleaning & Feature Engineering\n* heal data and remove outliers\n* work with shops\/items\/cats objects and features\n* expand training set to include all item-shop pairs\n* clip item_cnt_month by (0,20)\n* append test to the matrix, fill 34 month nans with zeros\n* merge shops\/items\/cats dataframe to training set.\n* add group sale stats in recent months\n* add lag features\n* add trend features\n* add month and year\n* add months since last sale\/months since first sale features\n* cut first year and drop columns which can not be calculated for the test set","39550fa4":"The distribution of sale grouped by month, item and shop, we see most item-shop pairs have small monthly sale.","b0e07487":"### Conclusion: feature selection doesn't improve the error","9b97c078":"### Use the best $\\alpha$, $\\beta$ and $\\gamma$ to make predictions","8da71b42":"### Public score: 0.94059","25e31b2d":"Check for missing values.","d57acbd1":"## Public score: 0.90682","c974bd47":"## Add month since the last and first sale\nThe code has been simplified to reduce run time, though still may not be optimal -- ideally we don't need to compute max for each month.","e4119c32":"# **Set up validation strategy**\n\nValidation strategy is 34 month for the test set, 33 month for the validation set and 13-32 months for the train.","7cff55ca":"## Monthly sales\nMost of the items in the test set target value should be zero, while train set contains only pairs which were sold or returned in the past. So we expand the train set to include those item-shop pairs with zero monthly sales. This way train data will be similar to test data.","5cef24fd":"## <a id=\"stop_point\"><\/a> Run the notebook up to this point first.\n## Then chose the model(s) you want to train.","5037d35e":"## Neural Network","d93719f9":"# Ensemble based on different models","dfa3aa95":"Aggregate train set by shop\/item pairs to calculate target aggreagates, then <b>clip(0,20)<\/b> target value. This way train target will be similar to the test predictions.\n\nDowncast item_cnt_month to float32 -- float16 was too small to perform sum operation.","5f5c17bb":"## Add month and year","e92baaea":"Last month shop revenue trend","61e85064":"There is one item with price below zero. Fill it with median.","5c950156":"## LightGBM","e66d80cc":"### Public score: 0.97956","558a57ad":"**The goal of this notebook was to apply concepts and methods learned in the Coursera course \"How to Win a Data Science Competition: Learn from Top Kagglers**\". \n\nThe first part includes EDA and feature engineering similar\u00a0to what\u00a0you find in many other notebooks here.\u00a0\n\nIn the second part I tried different ensemble approaches including simple convex mix and stacking applied for four different models: **CatBoost, Light GBM, Neural Network and XGBoost**. I tried also to slecet the best features. You will find the public scores that I received\u00a0for most of my attempts in the\u00a0notebook. I hope this can help you and save your time in your experiments.\u00a0\n\nSurprisingly, the best score that I got was for the Light\u00a0GBM alone, although some scores from the ensemble approaches were very close to it.\u00a0 My conclusion is that apparently\u00a0 Light\u00a0GBM was fine tuned in the best way and one needs to improve\u00a0the hyperparameter turning\u00a0for other models in order to get an advantage from the ensemble\u00a0approach.\n\n\n**Please upvote, if you find\u00a0this notebook useful!**\n**Any feedback\u00a0is very welcome!**\n","848130d4":"Test set is a product of some shops and some items within 34 month. There are 5100 items * 42 shops = 214200 pairs. 363 items are new compared to the train. ","238e7699":"Price trend for the last six months.","aa1e8ae4":"## Test set\nTo use time tricks append test pairs to the matrix.","23e55c92":"#### remove outliers\nRemove outliers with very large item_cnt_day and item_price.","a4eb3fcf":"## CatBoost","50c82831":"## Group sale stats in recent\ncreate stats (mean\/var) of sales of certain groups during the past 12 months","5adc0766":"#### Load models, if they were saved before","8dbcc063":"### Public score: 0.90429","668c18c0":"### Identifying the most important features","edcd09d3":"## Stacking","d6e55774":"## Lag features","653660bd":"## Two models","ebb6db02":"# XGBoost","cbdf80f6":"## Traget lags"}}