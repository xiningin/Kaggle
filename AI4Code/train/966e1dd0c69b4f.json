{"cell_type":{"47c05e74":"code","920c3a0d":"code","aefe7daa":"code","c5f283be":"code","ac9eaec9":"code","a4ec3aba":"code","95573ab1":"code","5f1bf6d5":"code","c7bbd04a":"code","fdc757b6":"code","88d41b24":"code","abe03eb3":"code","505aabca":"code","ca948ad8":"code","272ca1b1":"code","0ea337b4":"code","7838de41":"code","5ed3e7c0":"code","5967a49f":"code","f5f41ccb":"code","4eab06a0":"code","0dbe43f2":"code","44c6bf81":"code","6653a188":"code","941bdb40":"code","5d4f6d15":"code","5831654a":"code","cdc8073a":"code","967301f5":"code","0b054d56":"code","63cd16ce":"code","1c623375":"code","94b6b990":"code","f5ca6302":"code","8f6ffe4b":"code","c273756f":"code","0761a9c3":"code","430f5337":"code","e6d12cd6":"code","e33d2311":"code","34aa1214":"code","1cc5e190":"code","e0a1b2ed":"code","458b36c6":"code","32177842":"code","414ab29b":"code","a831d0da":"code","cec688d0":"code","a8a42899":"code","b6809d8a":"code","23a3fd7f":"code","a45347e7":"code","cbfdce6c":"code","c9a36388":"code","fffddefd":"markdown","299940e1":"markdown","8b6ab2fc":"markdown","6ff31290":"markdown","7092b62d":"markdown","3a33aaeb":"markdown","1d0d0a37":"markdown","26f56f3d":"markdown","767eb516":"markdown","78d710b0":"markdown","4afed09a":"markdown","80b6d016":"markdown","02f8cfca":"markdown","676e3c5a":"markdown","a7841a8a":"markdown","e10e96b3":"markdown","addf24dd":"markdown","c57aafd5":"markdown","2f2340a0":"markdown","337b980b":"markdown","9795227a":"markdown","88b24e8f":"markdown","07cdc6c6":"markdown","c8a73a12":"markdown","bd6ec6b9":"markdown","32d0da04":"markdown","9d746e0c":"markdown","f9bcf9bd":"markdown","995e9546":"markdown","e2c3c225":"markdown","14559d89":"markdown","031614d8":"markdown","70161719":"markdown","ed8bfd55":"markdown","d2526f43":"markdown","50f518c8":"markdown","aff410d2":"markdown"},"source":{"47c05e74":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","920c3a0d":"def mape(actual, pred): \n    actual, pred = np.array(actual), np.array(pred)\n    return np.mean(np.abs((actual - pred) \/ actual)) * 100","aefe7daa":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\n\n\nfrom math import sqrt\n\nplt.style.use(\"seaborn-whitegrid\")\nplt.rc(\"figure\", autolayout=True)\nplt.rc(\"axes\", labelweight=\"bold\", labelsize=\"large\", titleweight=\"bold\", titlesize=14, titlepad=10)\n","c5f283be":"df=pd.read_csv('\/kaggle\/input\/time-series-datasets\/Electric_Production.csv', index_col='DATE', parse_dates=True)","ac9eaec9":"df.head()","a4ec3aba":"df.columns = ['value']","95573ab1":"df.tail()","5f1bf6d5":"df.info()","c7bbd04a":"df.plot()","fdc757b6":"prediction_window=12","88d41b24":"n_input=12 #Use 12 months data to predict 13 th month data\nn_features=1 # we are dealing with an univariate time series, so n_features should be set to 1. \n#In case of a multivariate time series, n_features should be set to a proper value higher than 1.","abe03eb3":"train = df.copy()","505aabca":"scaler = MinMaxScaler()\nscaler.fit(train)\nscaled_train = scaler.transform(train)","ca948ad8":"def sliding_windows(data, n_input):\n    X_train=[]\n    y_train=[]\n    for i in range(n_input,len(data)):\n        X_train.append(data[i-n_input:i])\n        y_train.append(data[i])\n    return np.array(X_train), np.array(y_train)","272ca1b1":"x, y = sliding_windows(scaled_train, prediction_window)","0ea337b4":"print(f'Given the Array: \\n {x[0].flatten()}')\nprint(f'Predict this value: \\n {y[0]}')","7838de41":"train_size = int(len(train) - prediction_window*3)\nval_size = len(train) - train_size","5ed3e7c0":"dataX = Variable(torch.Tensor(np.array(x)))\ndataY = Variable(torch.Tensor(np.array(y)))\n\nX_train = Variable(torch.Tensor(np.array(x[:train_size])))\ny_train = Variable(torch.Tensor(np.array(y[:train_size])))\n\nX_valid = Variable(torch.Tensor(np.array(x[train_size:len(x)])))\ny_valid = Variable(torch.Tensor(np.array(y[train_size:len(y)])))","5967a49f":"class LSTMNet(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers):\n        super(LSTMNet, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        \n        self.lstm = nn.LSTM(\n            input_size=input_size, hidden_size=hidden_size,\n            num_layers=num_layers, batch_first=True\n        )\n        \n        self.fc1 = nn.Linear(hidden_size,40)\n        self.fc2 = nn.Linear(40,1)\n        self.relu = nn.ReLU()\n        \n    def forward(self,x):\n        h0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size))\n        c0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size))\n        _, (h_out,_) = self.lstm(x, (h0,c0))\n        \n        h_out = h_out.view(-1, self.hidden_size)\n        \n        out = self.fc2(self.relu(self.fc1(h_out)))\n        \n        return out","f5f41ccb":"EPOCHS = 2000\nLEARNING_RATE = 0.008\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n\nINPUT_SIZE = n_features\nHIDDEN_SIZE = 100\nNUM_LAYERS = 1","4eab06a0":"model = LSTMNet(INPUT_SIZE, HIDDEN_SIZE, NUM_LAYERS)\nmodel.to(DEVICE)\nprint(model)","0dbe43f2":"criterion = torch.nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)","44c6bf81":"early_stopping_patience = 150\nearly_stopping_counter = 0\n\nvalid_loss_min=np.inf\n\nfor epoch in range(EPOCHS):\n    optimizer.zero_grad()\n    model.train()\n    output = model(X_train)\n    \n    train_loss = criterion(output, y_train)\n    \n    train_loss.backward()\n    optimizer.step()\n    \n    \n    with torch.no_grad():\n        model.eval()\n        output_val = model(X_valid)\n        valid_loss = criterion(output_val, y_valid)\n        \n        if valid_loss <= valid_loss_min:\n            torch.save(model.state_dict(), '.\/state_dict.pt')\n            print(f'Epoch {epoch + 0:01}: Validation loss decreased ({valid_loss_min:.6f} --> {valid_loss:.6f}).  Saving model ...')\n            valid_loss_min = valid_loss\n            early_stopping_counter=0 #reset counter if validation loss decreases\n        else:\n            print(f'Epoch {epoch + 0:01}: Validation loss did not decrease')\n            early_stopping_counter+=1\n\n        if early_stopping_counter > early_stopping_patience:\n            print('Early stopped at epoch :', epoch)\n            break\n\n        print(f'\\t Train_Loss: {train_loss:.4f} Val_Loss: {valid_loss:.4f}  BEST VAL Loss: {valid_loss_min:.4f}\\n')","6653a188":"# Loading the best model\nmodel.load_state_dict(torch.load('.\/state_dict.pt'))","941bdb40":"valid_predict = model(X_valid)\ny_pred_scaled = valid_predict.data.numpy()\ny_pred = scaler.inverse_transform(y_pred_scaled)","5d4f6d15":"df_pred=train.iloc[-24:]\ndf_pred['prediction']=y_pred","5831654a":"plt.figure(figsize=(14,5))\nplt.plot(df_pred.index,df_pred[\"value\"], label=\"actual\", marker=\"o\")\nplt.plot(df_pred.index,df_pred[\"prediction\"], label=\"predicted\", marker=\"o\")\nplt.title(\"Electric production IP prediction by LSTM RNN\", fontsize=25)\nplt.ylabel(\"ylabel\")\nplt.legend(title_fontsize=14, fontsize=13, fancybox=True, shadow=True, frameon=True)\nplt.show()","cdc8073a":"mape_lstm = mape(df_pred[\"value\"], df_pred[\"prediction\"])\nprint(f\"MAPE OF LSTM MODEL : {mape_lstm:.2f} %\")","967301f5":"rmse_lstm = mean_squared_error(df_pred[\"value\"], df_pred[\"prediction\"], squared=False)\nprint(f\"RMSE OF LSTM MODEL : {rmse_lstm:.2f}\")","0b054d56":"test_predictions = []\n\nfirst_eval_batch = Variable(torch.Tensor(scaled_train[-n_input:])) # use the previous 12 samples to predict the 13th\ncurrent_batch=first_eval_batch.reshape((1,n_input,n_features)) # reshape the data into (1,12,1)\nfor i in range(len(scaled_train[-n_input:])):\n    #get the prediction value for the first batch\n    current_pred = model(current_batch)\n    #append the prediction into the array\n    test_predictions.append(current_pred)\n    \n    #use the prediction to update the batch and remove the first value\n    current_batch = torch.cat((current_batch[:,1:,:],current_pred.reshape(1,1,1)),1)","63cd16ce":"forec_vals = [val.flatten().item() for val in test_predictions]\nforec_vals = np.array(forec_vals).reshape(-1,1)\nforec_vals = scaler.inverse_transform(forec_vals)","1c623375":"date_offset=12\nforecast_dates =  (train.index + pd.DateOffset(months=date_offset))[-date_offset:]\nforecast_dates","94b6b990":"df_forecast=pd.DataFrame({'date': forecast_dates})\ndf_forecast.set_index('date', inplace=True)\ndf_forecast['prediction'] = forec_vals\ndf_forecast.head(12)","f5ca6302":"df_full=df_pred.append(df_forecast)","8f6ffe4b":"plt.figure(figsize=(14,5))\nplt.plot(df_full.index,df_full[\"value\"], label=\"Actual\", marker=\"o\")\nplt.plot(df_full.index,df_full[\"prediction\"], label=\"Predicted\", marker=\"o\", color=\"purple\")\nplt.axvline(df_pred.index[-1], color=\"red\", linestyle=\"--\")\nplt.title(\"Electric production IP forecasting by LSTM RNN\", fontsize=25)\nplt.ylabel('Industrial Production (IP) index', fontsize=15)\nplt.legend(title_fontsize=14, fontsize=13, fancybox=True, shadow=True, frameon=True)\nplt.show()","c273756f":"result = seasonal_decompose(df)\n\nfig = plt.figure()  \nfig = result.plot()","0761a9c3":"result=adfuller(df.value.dropna())\nprint(f'ADF Statistics:{result[0]}')\nprint(f'p-value:{result[1]}')","430f5337":"result=adfuller(df.value.diff().dropna())\nprint(f'ADF Statistics:{result[0]}')\nprint(f'p-value:{result[1]}')","e6d12cd6":"fig, (ax1, ax2)=plt.subplots(2,1,figsize=(8,8))\n\nplot_acf(df,lags=14, zero=False, ax=ax1)\nplot_pacf(df,lags=14, zero=False, ax=ax2)\nplt.show()","e33d2311":"!pip install pmdarima\nimport pmdarima as pm","34aa1214":"results=pm.auto_arima(df, d=1, start_p=1, start_q=1, max_p=3, max_q=3,\n                    seasonal=True, m=6, D=1, start_P=1, start_Q=1, max_P=2, max_Q=2, information_criterion='aic', trace=True, error_action='ignore', stepwise=True)","1cc5e190":"model=SARIMAX(df,order=(2,1,2),  seasonal_order=(1, 1, 2, 6))\nresults=model.fit()","e0a1b2ed":"results.summary()","458b36c6":"# Create the 4 diagostics plots\nresults.plot_diagnostics(figsize=(8,8))\nplt.show()","32177842":"prediction = results.get_prediction(start=-24)\nmean_prediction=prediction.predicted_mean\nmean_prediction = mean_prediction.rename(\"prediction\")","414ab29b":"confi_int_p=prediction.conf_int()\nlower_limits_p=confi_int_p.iloc[:,0]\nupper_limits_p=confi_int_p.iloc[:,1]","a831d0da":"plt.figure(figsize=(14,5))\nplt.title(\"Electric production IP prediction by SARIMA\", fontsize=25)\n\nplt.plot(df[-24:].index,df[-24:].values, label='Actual values', color=\"blue\", marker=\"o\")\n\nplt.plot(mean_prediction[-24:].index, mean_prediction[-24:].values,label='Prediction', color=\"green\", marker=\"o\")\nplt.fill_between(mean_prediction[-24:].index, lower_limits_p, upper_limits_p, alpha=0.1, color=\"green\")\n\nplt.legend(fontsize=12, fancybox=True, shadow=True, frameon=True)\nplt.ylabel('Industrial Production (IP) index', fontsize=15)\nplt.show()","cec688d0":"mape_sarima = mape(df.iloc[-24:,0],mean_prediction)\nprint(f\"MAPE OF LSTM MODEL : {mape_sarima:.2f} %\")","a8a42899":"rmse_sarima = sqrt(mean_squared_error(df[-24:].values,mean_prediction.values))\nprint(f\"RMSE OF LSTM MODEL : {rmse_sarima:.2f}\")","b6809d8a":"# Make ARIMA forecast of next 10 values\nforecast = results.get_forecast(steps=12)\nmean_forecast=forecast.predicted_mean\nmean_forecast = mean_forecast.rename(\"prediction\")","23a3fd7f":"confi_int_f=forecast.conf_int()\nlower_limits_f=confi_int_f.iloc[:,0]\nupper_limits_f=confi_int_f.iloc[:,1]","a45347e7":"plt.figure(figsize=(14,5))\nplt.title(\"Electric production IP forecasting by SARIMA\", fontsize=25)\n\nplt.plot(df[-24:].index,df[-24:].values, label='Actual values', color=\"blue\", marker=\"o\")\n\nplt.plot(mean_prediction[-24:].index, mean_prediction[-24:].values,label='Prediction', color=\"green\", marker=\"o\")\nplt.fill_between(mean_prediction[-24:].index, lower_limits_p, upper_limits_p, alpha=0.1, color=\"green\")\n\nplt.plot(mean_forecast[-24:].index,mean_forecast[-24:].values, label='Forecast', color=\"orange\", marker=\"o\")\nplt.fill_between(mean_forecast[-24:].index, lower_limits_f, upper_limits_f, alpha=0.1, color=\"orange\")\n\nplt.axvline(df_pred.index[-1], color=\"red\", linestyle=\"--\")\nplt.legend(fontsize=12, fancybox=True, shadow=True, frameon=True)\nplt.ylabel('Industrial Production (IP) index', fontsize=15)\nplt.show()","cbfdce6c":"df_sarimax = mean_prediction.append(mean_forecast)","c9a36388":"plt.figure(figsize=(14,5))\nplt.title(\"Electric production IP forecasting by LSTM RNN and SARIMA models\", fontsize=20)\nplt.plot(df[-24:].index,df[-24:].values, color='darkblue', label='Real')\nplt.plot(df_sarimax.index, df_sarimax.values, color='darkorange', label='SARIMA', marker='o')\nplt.plot(df_full.index, df_full['prediction'].values, color='green',label='LSTM', marker='o')\nplt.axvline(df_pred.index[-1], color=\"red\", linestyle=\"--\")\nplt.legend(bbox_to_anchor=(1, 1.03),fontsize=16,fancybox=True, shadow=True, frameon=True)\ntext = 'LSTM RMSE : {:.3f}\\n\\nSARIMA RMSE : {:.3f}\\n\\nLSTM MAPE : {:.2f} %\\n\\nSARIMA MAPE : {:.2f} %'.format(rmse_lstm,rmse_sarima,mape_lstm,mape_sarima)\nplt.text(mean_forecast.index[-1] + pd.DateOffset(days=80),90,text, fontsize=16)\nplt.ylabel('Industrial Production (IP) index', fontsize=15)\nplt.show()","fffddefd":"Then we create a copy named \"train\" of the original dataframe \"df\" which will be used in the following.","299940e1":"Now, we will check the values predicted by the LSTM RNN on the validation data","8b6ab2fc":"# PyTorch LSTM RNN ","6ff31290":"Since we will use a RNN, it is recommended to rescale the data.","7092b62d":"# LSTM Prediction","3a33aaeb":"# LSTM Modeling","1d0d0a37":"Now we can formally create the train set. As stated before, the train set will have a \"sliding window\" shape. This means that we have to shape the data in such a way that the RNN will predict the 13th sample starting from the previous from 12 samples.","26f56f3d":"Then we convert the data to the tensor format required by PyTorch.","767eb516":"In the following,the time series will be modeled by as a SARIMA model.","78d710b0":"**The results look indeed very similar both in terms of rmse and mape.**","4afed09a":"**We can see that SARIMA and LSTM obtained very similar results visually and in both terms of MAPE and RMSE.**","80b6d016":"## ACF and PACF","02f8cfca":"# SARIMA Forecast","676e3c5a":"<img src=\"https:\/\/i.imgur.com\/3x3yTR5.png\">","a7841a8a":"Moreover, we also get the confidence intervals from the sarima prediction","e10e96b3":"# SARIMA Prediction","addf24dd":"Now the series looks stationary with a 1-order difference","c57aafd5":"The data can be interpreted as a time series with a clear positive trend and seasonality.<br>\nIn the following, prediction models by LSTM RNN and SARIMAX will be developed to forecast the time series.","2f2340a0":"## Custom Functions definition","337b980b":"## Auto ARIMA","9795227a":"**Thanks for reading my notebook ! Write a comment if you have any questions or if you want me to check out your works :)**","88b24e8f":"# SARIMA modeling","07cdc6c6":"# Results Sumamry","c8a73a12":"All the 4 plots indicates a good fit of the SARIMA model on the given time serie.","bd6ec6b9":"# Electric Production Forecast by PyTorch LSTM and SARIMA models","32d0da04":"The first thing we must do is to properly shape the input data. When modeling a time series by LSTM RNN, it is crucial to to properly shape the input data in a sliding windows format. In this application, the data is given as monthly data. So, for example, we can use a 12 steps prediction window. This means that we use 12 samples of data (data of an entire year) to predict the 13th sample.","9d746e0c":"The p-value is higher than 0.05. This means that the time serie is non stationary with a confidence of 95%. We check if with a one step differentiation, the time serie become stationary (in terms of a trendless time series).","f9bcf9bd":"Then we set some parameters for the training:","995e9546":"# LSTM Training Loop","e2c3c225":"# Prediction and Forecast comparison:","14559d89":"The series looks indeed non stationary","031614d8":"We can use some useful tools such as 'seasonal decompose' to better visualize the time series.","70161719":"This project aims to model the Electric Production IP index in US using data from 1985 to 2017 (UCI Dataset).<br>\nThe notebook is organized as follows:<br>\n- PyTorch LSTM RNN :\n    - Data preparation for LSTM training : scaling, reshaping\n    - LSTM modeling\n    - LSTM Prediction\n    - LSTM Forecast\n <br>\n- SARIMA:<br>\n    - Data preparation for SARIMA: stationarity analysis, PACF, ACF\n    - SARIMA modeling\n    - SARIMA Prediction\n    - SARIMA Forecast","ed8bfd55":"# LSTM Forecasting","d2526f43":"We can also check the stationarity of the time serie by performing the Augmented Dickey Fuller (ADF) test.","50f518c8":"Now we will create a vector that will host the predictions","aff410d2":"There are no missing values!"}}