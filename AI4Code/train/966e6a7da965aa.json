{"cell_type":{"7e16ac9d":"code","3546022e":"code","b8ce6033":"code","ce81ba2f":"code","63cf963f":"code","e87800df":"code","8bd1f5be":"code","afa49239":"code","fb6936ae":"code","04f21c52":"code","a982221b":"code","cd86f316":"code","61cafabb":"code","fc84efd3":"code","ed71ffb2":"code","84d4a0a2":"code","592b140d":"code","a1bc36a6":"code","182c817d":"code","cf5f705f":"code","3d813e43":"code","e34b627d":"code","a2f224a2":"code","290094ee":"code","f4132ec1":"code","bcb54dcb":"code","d17febbb":"code","20e384ed":"code","6f12a0ea":"code","0a1bb10f":"code","f471d1ef":"code","0d66704d":"code","ae561445":"code","f6aac6cb":"code","8685ff46":"code","abbd3155":"code","87266caa":"code","b62bafd6":"code","39876356":"code","2f4f7ebc":"code","43c16cd4":"code","d989946c":"code","e260c66c":"code","d28422ec":"code","e43e2bbc":"code","ccc9813d":"code","86158937":"code","0f76fd5a":"code","5939c3f3":"code","02508b05":"code","7414843a":"code","5ec9e42d":"code","98fd1cf1":"code","c863a55a":"code","037fe405":"code","68c96674":"code","d112a804":"code","61849a5d":"markdown","7b1eeb0c":"markdown","9b6b0816":"markdown","647729b2":"markdown","602ff560":"markdown","8c250f3d":"markdown","2bf9ff76":"markdown","7a9096dd":"markdown","67903090":"markdown","0d7e9cfa":"markdown","7e3f6d2d":"markdown","1d3ccc99":"markdown","fa26531a":"markdown","745cf3e8":"markdown","8afbe311":"markdown","b5e3d661":"markdown","6677aa19":"markdown","00e48b7c":"markdown","098c65f8":"markdown","f60b51db":"markdown","d40824e8":"markdown","9e0abee8":"markdown","1ae2e683":"markdown","cd922f4d":"markdown","238fca1f":"markdown","7112389c":"markdown","79c4183d":"markdown","54c999e5":"markdown","109915e1":"markdown","80f96f57":"markdown","20783e54":"markdown","89a9ef34":"markdown","176b2e2a":"markdown"},"source":{"7e16ac9d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\n\n# Data management\nimport numpy as np \nimport pandas as pd \nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Visualization\nimport matplotlib.pyplot as plt\nplt.style.use('fivethirtyeight') #style for time series visualization\n%matplotlib inline\nfrom pylab import rcParams\nfrom plotly import tools\nimport plotly.plotly as py\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\n\n#Statistics \nimport statsmodels.api as sm\nfrom numpy.random import normal, seed\nfrom scipy.stats import norm\nfrom statsmodels.tsa.ar_model import AR\nfrom statsmodels.tsa.arima_model import ARMA\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom statsmodels.tsa.arima_process import ArmaProcess\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\n\n\n\n# Any results you write to the current directory are saved as output.","3546022e":"candy = pd.read_csv(\"..\/input\/candy_production.csv\")","b8ce6033":"candy.tail()","ce81ba2f":"candy.info()","63cf963f":"# Create time series from pandas\nrng = pd.date_range(start = '1\/1\/1972', end = '8\/1\/2017', freq = 'MS')","e87800df":"cdy = pd.Series(list(candy['IPG3113N']), index = rng)","8bd1f5be":"rcParams['figure.figsize'] = 20, 5\ncdy.plot(linewidth = 1, ls = 'solid')\nplt.title('Monthly US Candy Production (1972 - 2017)')\nplt.show()","afa49239":"seconds_of_year = 365*24*3600\nfrac = [0]*len(rng)\nfor i in range(1,len(rng)):\n    frac[i] = round((rng[i] - rng[0]).total_seconds()\/seconds_of_year, 3)\nfrac = np.array(frac).reshape(-1,1)","fb6936ae":"reg = LinearRegression().fit(frac, cdy)","04f21c52":"reg.score(frac, cdy)","a982221b":"lr_detrended = cdy - reg.predict(frac)","cd86f316":"lr_detrended.plot(lw = 1)\nplt.title('Linear Regression Detrended')\nplt.show()","61cafabb":"frac_sq = frac**2\nX = np.concatenate([frac_sq, frac], axis = 1)","fc84efd3":"X.shape","ed71ffb2":"reg_2 = LinearRegression().fit(X, cdy)","84d4a0a2":"reg_2.score(X, cdy)","592b140d":"lr_detrended_2 = cdy - reg_2.predict(X)","a1bc36a6":"lr_detrended_2.plot(lw = 1)\nplt.title('Linear Regression Detrended (Quadratic Equation)')\nplt.show()","182c817d":"mean_year = cdy.resample('Y').mean()","cf5f705f":"mean_year.head()","3d813e43":"dup_mean_year = pd.Series(np.repeat(mean_year.values,12,axis=0)[:548], index = rng)","e34b627d":"dup_mean_year.head()","a2f224a2":"nonlinear_transformed = cdy - dup_mean_year","290094ee":"nonlinear_transformed.plot(lw = 1)\nplt.title(\"Non-linear transformation\")\nplt.show()","f4132ec1":"rcParams['figure.figsize'] = 20, 5\ndiff_1 = cdy.diff()\ndiff_2 = diff_1.diff()\nax1 = plt.subplot(211)\ndiff_1.plot(lw = 1, axes = ax1)\nax2 = plt.subplot(212, sharex=ax1)\ndiff_2.plot(lw = 1, axes = ax2)\nax1.set_title(\"Differentiation (1st order)\")\nax2.set_title(\"Differentiation (2nd order)\")\nplt.show()","bcb54dcb":"candy['lr_residual'] = lr_detrended.values","d17febbb":"candy['month'] = candy['observation_date'].apply(lambda x: x[5:7])","20e384ed":"candy.head()","6f12a0ea":"month_average = candy.groupby('month')['lr_residual'].mean().reset_index().rename(columns = {'lr_residual':'month_average'})\n","0a1bb10f":"m = list(month_average['month_average'].values)\ndup_mean_month = pd.Series((m*(2017-1971))[:548], index = rng)\n\n","f471d1ef":"month_average","0d66704d":"subtract_month = lr_detrended - dup_mean_month","ae561445":"rcParams['figure.figsize'] = 20, 5\ndup_mean_month.plot(lw = 1)\nplt.title(\"Seasonal Pattern\")","f6aac6cb":"rcParams['figure.figsize'] = 20,9\nax1 = plt.subplot(411)\ncdy.plot(lw = 1, axes = ax1)\nax1.set_title(\"Observed\")\n\nax2 = plt.subplot(412, sharex=ax1)\ntrend = pd.Series(reg.predict(frac), index = rng)\ntrend.plot(lw = 1, axes=ax2)\nax2.set_title(\"Trend\")\n\nax3 = plt.subplot(413, sharex=ax1)\ndup_mean_month.plot(lw = 1, axes=ax3)\nax3.set_title(\"Seasonal Pattern\")\n\n\nax4 = plt.subplot(414, sharex=ax1)\nsubtract_month.plot(lw = 1, axes=ax4)\nax4.set_title('Seasonal pattern extracted by monthly average (Residual)')\nplt.show()","8685ff46":"rcParams['figure.figsize'] = 20, 5\n# diff_monthly = cdy.diff(periods = 12)\ndiff_monthly=diff_1.diff(periods=12)\nax1 = plt.subplot(211)\ndiff_1.plot(lw = 1, axes = ax1)\n# ax2 = plt.subplot(312, sharex=ax1)\n# diff_2.plot(lw = 1, axes = ax2)\nax3 = plt.subplot(212, sharex=ax1)\ndiff_monthly.plot(lw = 1, axes = ax3)\nax1.set_title(\"Differentiation (1st order)\")\nax2.set_title(\"Differentiation (2nd order)\")\nax3.set_title(\"Seasonal differentiation (1st order)\")\nplt.show()","abbd3155":"moving_average = diff_1.rolling(3, center = True).mean()","87266caa":"rcParams['figure.figsize'] = 20, 5\ndiff_1.plot(lw = 1, alpha = 0.5, label = '1st order diff')\nmoving_average.plot(lw = 1, color = 'red', label = 'moving average')\nplt.title(\"Removing seasonal patterns by moving average (window size = 3)\")\nplt.legend()\nplt.show()","b62bafd6":"# Now, for decomposition...\nrcParams['figure.figsize'] = 20, 9\ndecomposed = sm.tsa.seasonal_decompose(cdy,freq=30) # The frequncy is monthly\nfigure = decomposed.plot()\nplt.show()","39876356":"rcParams['figure.figsize'] = 20, 15\nplt.subplots_adjust(hspace=0.5)\n\nax1 = plt.subplot(611)\nplot_acf(cdy, ax = ax1, marker = '.', lags=200)\nax1.set_title(\"ACF of original series\")\n\nax2 = plt.subplot(612)\nplot_acf(lr_detrended, ax = ax2, marker = '.', lags=200)\nax2.set_title(\"ACF of linear regression detrended\")\n\n\nax3 = plt.subplot(613)\nplot_acf(diff_1.dropna(), ax = ax3, marker = '.', lags = 200)\nax3.set_title(\"ACF of 1st order differentiation\")\n\nax4 = plt.subplot(614)\nplot_acf(moving_average.dropna(), ax=ax4, marker='.', lags=200)\nax4.set_title(\"ACF of moving average\")\n\nax5 = plt.subplot(615)\nplot_acf(decomposed.resid.dropna(), ax=ax5, marker='.', lags=200)\nax5.set_title(\"ACF of built-in decomposed residual\")\n\nax6 = plt.subplot(616)\nplot_acf(diff_monthly.dropna(), ax=ax6, marker='.', lags=200)\nax6.set_title(\"ACF of monthly differentiation\")\n\nplt.show()","2f4f7ebc":"white_noise = pd.Series(normal(size=500))","43c16cd4":"rcParams['figure.figsize'] = 20, 8\nplt.subplots_adjust(hspace=0.5)\n\nax1 = plt.subplot(211)\nwhite_noise.plot(lw=1)\nax1.set_title(\"White noise series\")\n\nax2 = plt.subplot(212)\nplot_acf(white_noise, marker = '.', lags = 200,ax=ax2)\nax2.set_title(\"ACF of white noise\")\n\nplt.show()","d989946c":"# stationary_cdy = decomposed.resid.dropna()\nstationary_cdy = diff_monthly.dropna()","e260c66c":"# stationary_cdy has data from 1973-04-01 to 2016-05-01 (518 data points)\n# split 80-20 for train and test\nind_80 = int(len(stationary_cdy)*0.8)\ntrain, test = stationary_cdy[:ind_80], stationary_cdy[ind_80:]\nprint(len(train), len(test))","d28422ec":"plot_pacf(stationary_cdy, title = 'PACF of the stationary candy series', alpha = 0.05, lags = 100)\nplt.ylabel('PACF')\nplt.xlabel('Lags')\nplt.show()","e43e2bbc":"# Order is determined by BIC\nar_aic = AR(stationary_cdy).fit(ic = 'bic')\nprint('Lag by BIC: %s' % ar_aic.k_ar)","ccc9813d":"def rolling_forecast_evaluation(order):\n    preds = []\n    for i in range(ind_80,len(stationary_cdy)):\n        cdy_train = stationary_cdy[:i]\n        ar_model = AR(cdy_train).fit(maxlag = order)\n        one_ahead_predict = ar_model.predict(start = len(cdy_train), end = len(cdy_train), dynamic = False)\n        preds.append(one_ahead_predict[0])\n\n    print(\"MSE of order = %d: %.3f\" % (order,mean_squared_error(test, preds)))","86158937":"print(\">>> Evaluated by rolling forecast\")\nrolling_forecast_evaluation(18)","0f76fd5a":"# model_13 = AR(stationary_cdy).fit(maxlag = 13)\nmodel_18 = AR(train).fit(maxlag=18)\n# preds_13 = model_13.predict(start=len(train), end=len(train) + len(test) - 1, dynamic=False)\npreds_18 = model_18.predict(start=len(train), end=len(train) + len(test) - 1, dynamic=False)\nprint(\">>> Evaluated by normal cross validation\")\n# print(\"MSE of order = 13: %.3f\" % (mean_squared_error(test, preds_13)))\nprint(\"MSE of order = %d: %.3f\" % (model_18.k_ar,mean_squared_error(test, preds_18)))","5939c3f3":"rcParams['figure.figsize'] = 20, 8\nall_preds=AR(stationary_cdy).fit(maxlag=18).predict(start = 18, end = len(stationary_cdy)-1, dynamic = False)\n# First, plot both series on the same plot\nax1 = plt.subplot(311)\nstationary_cdy.plot(lw=1, label = 'stationary')\nall_preds.plot(lw=1, label = 'AR(18)')\nax1.legend()\n\nresidual = (stationary_cdy - all_preds)\nax2 = plt.subplot(312)\nresidual.plot(lw=1, label = 'residual')\nax2.legend()\n\nax3 = plt.subplot(313)\nplot_pacf(residual, title = None, alpha = 0.05, lags = 100, ax=ax3, label = 'Residual')\nax3.set_ylabel('PACF')\nax3.legend()\n\nplt.show()","02508b05":"rcParams['figure.figsize'] = 20, 8\ncdy_model=AR(cdy).fit()\nall_preds=cdy_model.predict(start = cdy_model.k_ar, end = len(cdy)-1, dynamic = False)\n# First, plot both series on the same plot\nax1 = plt.subplot(311)\ncdy.plot(lw=1, label = 'original')\nall_preds.plot(lw=1, label = 'AR')\nax1.legend()\n\nresidual = (cdy - all_preds)\nax2 = plt.subplot(312)\nresidual.plot(lw=1, label = 'residual')\nax2.legend()\n\nax3 = plt.subplot(313)\nplot_pacf(residual, title = None, alpha = 0.05, lags = 100, ax=ax3, label = 'Residual')\nax3.set_ylabel('PACF')\nax3.legend()\n\nplt.show()","7414843a":"np.random.seed(42)\nrandom_walk = list()\nrandom_walk.append(-1 if np.random.random() < 0.5 else 1)\nfor i in range(1, 1000):\n    movement = -1 if np.random.random() < 0.5 else 1\n    value = random_walk[i-1] + movement\n    random_walk.append(value)\n    \nrcParams['figure.figsize'] = 20, 4\nplt.plot(random_walk, lw = 1)\nplt.title(\"Simulation of random walk model\")\nplt.show()","5ec9e42d":"rcParams['figure.figsize'] = 20, 6\n\nplt.subplots_adjust(hspace = 0.5)\nrw_diff = pd.Series(random_walk).diff(periods=1)\nax1 = plt.subplot(211)\nrw_diff.plot(lw=1, ax=ax1)\nax1.set_ylim(1.5,-1.5)\nax1.set_title('1st order differencing of random walk model')\n\nax2 = plt.subplot(212)\nplot_acf(rw_diff.dropna(), ax=ax2, lags=200, marker = '.', title = 'ACF of 1st order differencing of random walk model')\n\n\nplt.show()","98fd1cf1":"plot_acf(stationary_cdy, lags = 50, title = 'Candy production ACF')\nplt.show()\nplot_pacf(stationary_cdy, lags = 50, title = 'Candy production Partial ACF')\nplt.show()","c863a55a":"p,q = 12,2\n#ARMA model doesn't have a nice information criterion built-in hyperparameters tuning\narma_model = ARMA(stationary_cdy, order = (p,q)).fit(disp=0) \nfig, ax = plt.subplots()\nax = stationary_cdy.plot(ax=ax, label='Candy')\narma_model.plot_predict('2015','2019',ax=ax, plot_insample=False)\nplt.show()","037fe405":"rcParams['figure.figsize'] = 20, 6\nplot_acf(stationary_cdy, lags = 50, title = 'Candy production ACF')\nplt.show()\nplot_pacf(stationary_cdy, lags = 50, title = 'Candy production Partial ACF')\nplt.show()","68c96674":"model = ARIMA(stationary_cdy, order=[12, 1, 2])\nmodel_fit = model.fit(disp=0)","d112a804":"prediction = model_fit.predict()\nfig, ax = plt.subplots()\nstationary_cdy.plot(ax=ax, label='Candy production')\n# prediction.plot(ax=ax, label='ARIMA Predicted')\nmodel_fit.plot_predict('2015','2019',ax=ax, plot_insample=False)\nax.legend()","61849a5d":"## <a id='2.6'>2.6 ARIMA model <\/a>\nARIMA model is similar to ARMA model but with a differencing degree. This differencing degree is used to make the series stationary. The order of ARIMA model should be determined by <a href=\"https:\/\/en.wikipedia.org\/wiki\/Box%E2%80%93Jenkins_method#Autocorrelation_and_partial_autocorrelation_plots\">Box-Jenkins method.<\/a>","7b1eeb0c":"What if we fit the AR model to our original, non-stationary series?","9b6b0816":"# <a id='1'>1. Preliminary analysis<\/a>","647729b2":"From the PACF plot, it looks like a model of order 18 would be sufficient. Next, we will use the information criterion to determine the model order.","602ff560":"# <a id='2'>2. Models<\/a>\n## <a id='2.1'>2.1 White noise model<\/a>\n$\\{W_t\\}_t$ uncorrelated, mean zero, same variance $\\sigma_W^2$ (often i.i.d Gaussian)\n<h3><center> $\\mu_t = \\mathbb{E}[W_t] = 0 \\quad \\forall t$, $cov(W_s, W_t) = 0 \\quad s\\neq t$ <\/center><\/h3>\n* autocovariance:\n\n<h3><center> \n    \n$$ \\gamma_W(s,t) =\n  \\begin{cases}\n    \\sigma_W^2       & \\quad \\text{if } s=t\\\\\n    0 & \\quad \\text{otherwise}\n  \\end{cases}\n$$\n<\/center><\/h3>\ndepends only  on $|s-t|$\n\n* Stationary, capture no dependencies overtime\n* **Checking for white noise**: $\\hat{\\rho}(h)$ is approximately $\\mathcal{N}(0, \\frac{1}{n})$ under mild conditions\n\nWe will simulate a white noise model (Gaussian with mean 0 and std 1) and experiment some of the series' properties.\n","8c250f3d":"Choosing the model of order 18, we can test if the residual is white noise.","2bf9ff76":"Using the first order differencing time series, we can applying smoothing technique to reduce the effects of seasonal patterns.","7a9096dd":"Non linear transformation $Y_t  = \\sqrt{X_t - \\mu_t}$, where $\\mu_t$ is the mean of $X_t$. \n\nTo get the mean, we need to do sampling. There are two types of sampling:\n* **Upsampling**: Time series is resampled from low frequency to high frequency (monthly to daily frequency). It involves filling or interpolating missing data. \n* **Downsampling**: Time series is resampled from high frequency to low frequency (weekly to monthly frequency). It involves aggregation of existing data. This is what we can do in this case to estimate $\\mu_t$.","67903090":"We can also compare the behavior of these two models based on normal cross validation. Here, we only fit the model up to the train dataset. If we fit the model with all the data, prediction with dynamic=False sample the data from the series (in-sample) to be used as lags. Otherwise, if dynamic=True, use the previously predicted datapoint as lag for the next datapoint.","0d7e9cfa":"## <a id='2.3'>2.3 Random walk model (with drift)<\/a>\nRandom walk is a special case of AR(p) model ($p=1$):\n<center>$X_t = X_{t-1} + W_t = X_0 + \\sum_{h=1}^t W_h \\quad \\text{(fixed } X_0 \\text{)}$<\/center> \nRandom walk with drift adds a constant term:\n<center>$X_t = X_{t-1} + W_t + \\delta = X_0 + \\sum_{h=1}^t W_h + t\\delta$ <\/center>\nSome properties:\n* Mean: $\\mathbb{E}[X_t] = t\\delta + X_0$\n* Variance: $var(X_t) = t\\sigma_W^2$ (without drift)\n* Autocovariance: $\\gamma_X(s,t) = min\\{s,t\\} \\sigma_W^2$\n* Not stationary. However, first order differentiation $\\nabla X$ is stationary (because it is just white noise)\n\nAfter fitting AR model, we are confident that our dataset cannot be generated from a random walk model. Below is just a simulation of random walk model using pseudo-random function to generate numbers in [0.0, 1.0) interval.","7e3f6d2d":"We will confirm if differencing the random walk model returns white noise model.","1d3ccc99":"# <a id='0'>0. Time series overview. Stationary. Fitting a time series overview.<\/a>\n## 1. Time Series\n$\\{X_t\\}_t: X_0, X_1, ..., X_t$  is collection of random variables indexed in time\nDefine:\n* Marginal mean per time step: $\\mu_X(t) = \\mathbb{E}[X_t]$\n* Marginal variance per time step: $var_X(t) = \\mathbb{E}[(X_t - \\mu_X(t))^2] = \\gamma(t,t)$\n* Autocovariance: $\\gamma_X(s,t) = cov(X_s, X_t) = \\mathbb{E}[(X_s - \\mu_X(s))(X_t - \\mu_X(t))]$\n\n## 2. Stationarity\n### **2.1 Weak stationarity**\n* Mean and variance are the same for all $X_t$'s: $\\mu_X(t) = \\mu_X$ & $var_X(t) = var_X$\n* Covariance is only a function of gap: $cov(X_s, X_t) = \\gamma_X(s-t)$\n\n### **2.2 Strong stationarity**: Distribution of $X_t, ..., X_{t+n}$ is the same as distribution of $X_{t+h},..., X_{t+h+n} \\forall t, n, h$\n**Stationarity is important because it allows prediction. Many prediction models on time series rely on stationarity of the series.**\n## 3. Fitting a time series overview\n### 3.1 Transformation to make it stationary\n* non-linearly transform (i.e log-transform)\n* remove trends\/ seasonality\n* differentiate successively (differencing)\n\n### 3.2 Check for white noise (ACF)\n### 3.3 If stationary, plot autocorrelation (ACF) to find order. If finite lag, fit MA model. \n###               Otherwise, fit AR model.\n                \n\n\n","fa26531a":"The above ACF plots show that there are still some level of seasonal patterns after all the possible transformations. Overall, the monthly differentiation (after 1st order differentiation) seems to be very similar to white noise model.","745cf3e8":"In summary, the result of the above decomposition is as following:","8afbe311":"Next, we can try removing the seasonal pattern by periodic differentiation (in this case, monthly differentiation) on the 1st-order differentiated time series.","b5e3d661":"## <a id='1.1'>1.1 Detrend<\/a>\nIn general, there are three factors that account for the time series:\n$X_t = T_t + S_t + Y_t$ \n\nIn which:\n* $T_t$ is (linear) trend, can be found by linear regression\n* $S_t$ is seasonal component\n* $Y_t$ is remainder (hopefully stationary, mean zero)\n\nThere are many methods to detrend. Here, I cover three[](http:\/\/) most popular methods:\n* Fit a linear regression model to identify the trend, then subtract it (cons: can only remove linear trend)\n* Non-linear transformation\n* Differentiation","6677aa19":"## <a id='2.4'>2.4 Moving average (MA) model <\/a>\n### $MA(p)$ model\n\n<center> $X_t = W_t + \\theta_1W_{t-1} + ... + \\theta_pW_{t-p} \\quad \\text{for } W_t \\text{ is white noise}$ <\/center>\n* Mean, $\\mathbb{E}[X_t] = 0$\n* Autocovarianc $\\gamma$ only depends on $|s-t|$ \n* Always stationary! (Proof comes later)\n* ACF reflects order: $\\gamma(s,t) = 0 \\iff |s-t| > p$\n* ACF distinguishes MA and AR models\n\nIn practice, this model is not used very often because it requires the series to be strictly stationary and can be decomposed into white noise models. We will consider a more practical model, Autoregressive Moving Average (ARMA).","00e48b7c":"## <a id='1.3'>1.3 Autocorrelation (ACF) - Measuring correlation in time<\/a>\n<h3><center> $\\rho_X(s,t) = \\frac{\\gamma_X(s,t)}{\\sqrt{\\gamma_X(s,s)\\gamma_X(t,t)}}$ <\/center><\/h3>\n\nProperties of ACF:\n* Symmetric $\\gamma_X(s,t) = \\gamma_X(t,s)$\n* Measures linear dependence of $X_t$, $X_s$\n* Relates to smoothness\n* For **weak stationarity** $\\gamma_X(t, t+h) = \\gamma_X(h),  \\forall t$\n\n### **Sample estimates for stationary series**\n* $\\hat{\\mu}_X = \\bar{X} = \\frac{1}{n} \\sum_{t=1}^n X_t \\qquad var(\\bar{X}) = \\frac{1}{n} \\sum_{h=-n}^n (1-\\frac{|h|}{n} \\gamma(h) )$ \n* sample autocovariance:\n<h3><center> $\\hat{\\gamma}_X(h) = \\frac{1}{n} \\sum_{t=1}^{n - |h|} (X_t - \\mu_X) (X_{t + |h|} - \\mu_X)$ <\/center><\/h3>\n* sample autocorrelation:\n<h3><center> $\\hat{\\rho}_X(h) = \\frac{\\hat{\\gamma}_X(h)}{\\hat{\\gamma}_X(0)}$ <\/center><\/h3>","098c65f8":"Autocorrelation of white noise vanishs quickly after $h=0$. ","f60b51db":"It looks like the model order at $p=13$ makes the most sense but BIC criterion favors $p=18$. We will then evaluate these models using cross-validation.<br>\nThere are three most common ways to conduct cross validation:\n* In limited AR conditions (i.e limited time correlation), using normal cross validation (train on the past and predict on the future)\n* [Rolling forecast](https:\/\/www.google.com\/search?q=rolling+forecast&oq=rolling+forecast&aqs=chrome..69i57j0l5.2856j0j7&sourceid=chrome&ie=UTF-8). In the \"modified\" version of rolling forecast, to predict, say '2017-02-01', we can train a $AR(15)$ model using all the data up to '2017-01-01'. Next, to predict '2017-03-01', we train a new model using all the data up to '2017-02-01'.\n\n<img src=\"https:\/\/i.stack.imgur.com\/fXZ6k.png\" alt=\"drawing\" width=\"400\"\/>\n<center>[Reference](https:\/\/stats.stackexchange.com\/questions\/14099\/using-k-fold-cross-validation-for-time-series-model-selection)<\/center>\n\n* Others (out-of-sample, [h-hv block](https:\/\/pdfs.semanticscholar.org\/fec8\/709c61374672305f5f07d1af884254a01c24.pdf))\n<img src=\"http:\/\/www.jneurosci.org\/content\/jneuro\/36\/47\/11987\/F2.large.jpg?width=800&height=600&carousel=1\" alt=\"drawing\" width=\"200\"\/>\n<center>[Illustration of hv-block cross-validation (reference: The Journal of Neuroscience)](http:\/\/www.jneurosci.org\/content\/jneuro\/36\/47\/11987\/F2.large.jpg?width=800&height=600&carousel=1)<\/center>\n\n\n\n**In this example, we will evaluate using rolling forecast.**\n","d40824e8":"This suggests that:\n* There might be a trend in candy production (increasing trend)\n* There might be a seasonal pattern \n\nTherefore, it is important to search for stationarity over this dataset.","9e0abee8":"Till now, for consistency, we will use the stationary decomposed residual for model fitting. Below, we will try to fit an AR model. Before using a package with information theory to find the best order, we will explore the dataset by plotting the partial autocorrelation.","1ae2e683":"Subtract monthly averages. It looks like there will be monthly pattern in candy production (in some months there might be more production than the others). To illustrate this method, I will use the linear regression detrended series and remove the seasonal patterns from there.","cd922f4d":"## <a id='2.5'>2.5 Autoregressive Moving average (ARMA) model <\/a>\n### $ARMA(p,q)$ model can be thought as AR(p) + MA(q)\n\n<center> $X_t = \\phi_1X_{t-1} + ... + \\phi_pX_{t-p} + W_t + \\theta_1W_{t-1} + ... + \\theta_qW_{t-q} \\quad \\text{for } W_t \\text{ is white noise}$ <\/center>\n\n* Both ACF and PACF decay (never converge to zeros)\n\nWe will explore fitting an ARMA to the original candy series.\n\n<i> **Note: R has nicer packages for these types of models. However, for consistency, let's bear with Python! <i> ","238fca1f":"## <a id='1.2'>1.2 Seasonal pattern<\/a>\n* Fit periodic regression model & subtract\n* Subtract monthly averages\n* Differentiation\n* Fourier analysis\n* Smoothing by moving average $Y_t = \\frac{1}{2k+1} \\sum_{h = -k}^k a_h X_{t +h}$","7112389c":"Visualization of candy production (as % of 2012 production) over time.","79c4183d":"## <a id='2.2'>2.2 Autoregressive (AR) models. Partial Autocorrelation (PACF)<\/a>\n### $AR(p)$ model\n\n<center> $X_t = \\sum_{h=1}^p \\phi_h X_{t-h} + W_t \\quad \\text{for } W_t \\text{ is white noise}$ <\/center>\n* Autocorrelation decays exponentially over time (but never zero)\n* Only stationary under certain conditions (later)\n\n### Partial Autocorrelation\nAutocorrelation of $AR(p)$, $\\gamma(h) \\neq 0$ even for $h>p$. However, once $X_{t-1}, ..., X_{t-p}$ is known, knowing $X_{t-p-1}$ gives no further information. In other words, *conditional autocorrelation* should be 0. <br>\nDefine $\\phi_{hh}$ the last coefficient of a *fitted* $AR(h)$ model (*to the dataset*): $\\hat{X}_t = \\hat{\\phi}_1 X_{t-1} + ... + \\hat{\\phi}_h X_{t-h}$ where $\\phi_{hh} = \\hat{\\phi}_h$. (*Notice that it is a fitted model, so there is no white noise term. $\\hat{X}_t$ is an estimate of $X_t$)*<br>\nFormally, \n<center>$\\phi_{11} = corr(X_t, X_{t-1})$ <\/center>\n<center> $\\phi_{hh} = corr(X_t - \\hat{X}_t, X_{t-h} - \\hat{X}_{t-h})$ <\/center>\n(both $\\hat{X}_t, \\hat{X}_{t-h}$ are uncorrelated with $X_{t-1}, ..., X_{t-h}$)<br>\n\nThen, for an $AR(p)$ model, $\\phi_{hh} = 0 \\quad \\forall h > p$. \n\n\n### Fitting to a $AR(p)$ model\n1. Compute PACF (of the residual) to get order. Alternatively, find model with k parameters to minimize [Akaike Information Criterion (AIC)](https:\/\/en.wikipedia.org\/wiki\/Akaike_information_criterion). In short, AIC is a information-theory-based criterion used in model selection. <br>\n*\"When a statistical model is used to represent the process that generated the data, the representation will almost never be exact; so some information will be lost by using the model to represent the process. AIC estimates the relative amount of information lost by a given model: the less information a model loses, the higher the quality of that model.\"* <br>\nThe usage of AIC in model selection comes in handy in *statsmodels* package. There are many more similar criteria covered by this package (i.e Bayes Information Criterion, t-stat).\n2. Estimate *k* coefficients and noise variance $\\sigma^2_w$ via [Yule-Walker equations](http:\/\/www-stat.wharton.upenn.edu\/~steele\/Courses\/956\/Resource\/YWSourceFiles\/YW-Eshel.pdf) (also in *statsmodels* package).\n3. Compute residuals, test for white noise. Alternative, compute mean square error (MSE) by cross-validation.","54c999e5":"Compare this to the built-in method (which also uses moving average to remove seasonal patterns)","109915e1":"To detect non-linear trend, we can fit other non-linear model (such as quadratic model) and subtract to get the remainder.","80f96f57":"That was pretty good. Next, we will try the last method - differentiation. \n\nDifferentiation is taking the differences between the observations by some **lag** value (difference between consecutive observations is lag-1 difference). The lag difference can be adjusted to suit the specific temporal structure. For time series with a seasonal component, the lag may be expected to be the period (width) of the seasonality.\n\nTemporal structure may still exist after performing a differencing operation, such as in the case of a nonlinear trend. As such, the process of differencing can be repeated more than once until all temporal dependence has been removed. The number of times that differencing is performed is called the **difference order**.\n\nThese following equations summarize the above:\n\n$Y_t = \\nabla X_t = X_t - X_{t-1}$                                                                                     (First order)\n\n$Y_t = \\nabla^2 X_t = \\nabla X_t - \\nabla X_{t-1} = X_t -  2 X_{t-1} + X_{t-2}$                 (Second order)\n","20783e54":"**Linear regression**\n\nIn order to do linear regression, need to convert date range into fraction of year since the start date (1\/1\/1972). \nRegression the candy production on the above time fraction.","89a9ef34":"In this notebook, I will apply all the time series techniques I have learnt. My goal is:\n* Understand the trend of candy production\n* Review some basic time series models\n* Review some basic processes that explain the latency of some of the models\n\nTable of content:\n- <a href='#0'>0. Time series overview. Stationary. Fitting a time series overview.<\/a>\n- <a href='#1'>1. Preliminary analysis - Time series decomposition<\/a>\n    - <a href='#1.1'>1.1 Detrend<\/a>\n    - <a href='#1.2'>1.2 Seasonal pattern<\/a>\n    - <a href='#1.3'>1.3 Autocorrelation (ACF) - Measuring correlation in time<\/a>\n    \n- <a href='#2'>2. Models<\/a>\n    - <a href='#2.1'>2.1 White noise model <\/a>\n    - <a href='#2.2'>2.2 Autoregressive (AR) models. Partial Autocorrelation (PACF)<\/a>\n    - <a href='#2.3'>2.3 Random walk model (with drift) <\/a>\n    - <a href='#2.4'>2.4 Moving average (MA) models<\/a>\n    - <a href='#2.5'>2.5 ARMA models<\/a>\n    - <a href='#2.6'>2.6 ARIMA models<\/a>","176b2e2a":"The residual looks like white noise! AR model works very well with both the original candy data and the stationary candy data. "}}