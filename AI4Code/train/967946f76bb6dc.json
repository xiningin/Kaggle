{"cell_type":{"6ecd6040":"code","2c3d68b5":"code","76bd7354":"code","259efe11":"code","423b50ff":"code","5cc6da07":"code","b5c17b16":"code","e68a1848":"code","3b1af313":"code","6671e6e4":"code","ddc4ca9b":"code","824d1d41":"code","4898fa05":"code","9dbbaf80":"code","4c5a4ca4":"code","722b730e":"code","d0529907":"code","5ebf6b07":"code","062245f0":"code","87e0e7a6":"code","3a332168":"code","d24e647c":"code","c7f15cc0":"code","4c98e8f3":"code","080799a0":"code","fd709493":"code","265feff1":"code","787908ae":"code","3f9f44ad":"code","9d022625":"code","25a67626":"code","d7c948be":"code","2ece5e04":"code","948f4edd":"code","5ec7789d":"code","9227e2a2":"code","0c29b38a":"code","31d5fd83":"code","103beb47":"code","b48958b0":"code","f0b18269":"code","f5e117fc":"markdown","7502b440":"markdown","00dab0bf":"markdown","f14e28ef":"markdown","3b902653":"markdown","1a1fb91f":"markdown","caadef21":"markdown","9409f8e2":"markdown","332a51e1":"markdown","cd15f6e7":"markdown","573c3c01":"markdown","2ef145e1":"markdown","a6eeb2c0":"markdown","18397c50":"markdown","98decf20":"markdown","5db93b2b":"markdown","70c19657":"markdown","52da27a4":"markdown","d4b2c639":"markdown","909f05ef":"markdown","52a1dadf":"markdown","b2ff9037":"markdown","3ebbc302":"markdown","2ba1438d":"markdown","5bfef920":"markdown","5395a940":"markdown","6c438789":"markdown","1c78a33e":"markdown","9eaf3b92":"markdown","bb528368":"markdown"},"source":{"6ecd6040":"%%time\n\nimport requests\nfrom bs4 import BeautifulSoup\n\nGETURL = \"https:\/\/www.ncbi.nlm.nih.gov\/pubmed\/?term=connectome\"\nPOSTURL = \"https:\/\/www.ncbi.nlm.nih.gov\/pubmed\/\"\n\n\n# Solution problem with pagination - https:\/\/stackoverflow.com\/questions\/51100224\/cant-go-on-to-the-next-page-using-post-request\n# as an alternative, we can use selenium.\n\ns = requests.session()\ns.headers[\"User-Agent\"] = \"Mozilla\/5.0\"\n\nsoup = BeautifulSoup(s.get(GETURL).text,\"lxml\")\ninputs = {i['name']: i.get('value', '') for i in soup.select('form#EntrezForm input[name]')}\n\nresults = int(inputs['EntrezSystem2.PEntrez.PubMed.Pubmed_ResultsPanel.Pubmed_ResultsController.ResultCount'])\nitems_per_page = 100\npages = results \/\/ items_per_page + int(bool(results % items_per_page))\n\ninputs['EntrezSystem2.PEntrez.PubMed.Pubmed_ResultsPanel.Pubmed_DisplayBar.PageSize'] = items_per_page\ninputs['EntrezSystem2.PEntrez.PubMed.Pubmed_ResultsPanel.Pubmed_DisplayBar.PrevPageSize'] = items_per_page\ninputs['EntrezSystem2.PEntrez.DbConnector.Cmd'] = 'PageChanged'\n\n\n\ndef parse_text_from_paper(pmid):\n    '''\n    pmid: article id,\n    return: body data and title.\n    '''\n    html = requests.get(f\"{POSTURL}{pmid}\")\n    soup = BeautifulSoup(html.text, 'lxml')\n    title = [text.find('h1').text for text in soup.find_all(\"div\", class_=\"rprt_all\")]\n    data = [text.find('p').text for text in soup.find_all(\"div\", class_=\"abstr\")]\n    return data, title[0]\n\ndef get_data(session, num_pages, inputs):\n    \n    \"\"\"\n    Iteration over the n\u0332u\u0332m\u0332_\u0332p\u0332a\u0332g\u0332e\u0332s\u0332 in a s\u0332e\u0332s\u0332s\u0332i\u0332o\u0332n\u0332\n    return: MAP where keys - link on article, values - data.\n    \"\"\"\n    \n    MAP = {}\n    for page in range(num_pages):\n        print(f\"Getting data from page \u2116{page}\")\n        inputs['EntrezSystem2.PEntrez.PubMed.Pubmed_ResultsPanel.Pubmed_Pager.CurrPage'] = page + 1\n        inputs['EntrezSystem2.PEntrez.PubMed.Pubmed_ResultsPanel.Pubmed_Pager.cPage'] = page\n        res = session.post(POSTURL, inputs)\n        soup = BeautifulSoup(res.text, \"lxml\")\n\n        items = [pmid.text for pmid in soup('dd')]\n\n        for pmid in items:\n            \n            try:\n                MAP[POSTURL+pmid] = parse_text_from_paper(pmid)\n            except:\n                MAP[POSTURL+pmid] = None\n                \n    print(f\"Done!\\nMap length: {len(MAP)}\")\n    return MAP\n\npubmed_MAP = get_data(s, pages, inputs)","2c3d68b5":"# Solution problem with Event Loop in Jupyter kernel https:\/\/github.com\/jupyter\/notebook\/issues\/3397#issuecomment-376803076\n\n!pip install nest_asyncio\n\nimport nest_asyncio\nnest_asyncio.apply()","76bd7354":"!pip install aiohttp -qq","259efe11":"%%time\n\nimport asyncio\nimport aiohttp\nimport socket\n\n#import requests\nfrom bs4 import BeautifulSoup\n\n\nGETURL = \"https:\/\/www.ncbi.nlm.nih.gov\/pubmed\/?term=connectome\"\nPOSTURL = \"https:\/\/www.ncbi.nlm.nih.gov\/pubmed\/\"\n\n\nPMID = []\nNUM_PAGES = 46\nITEMS_PER_PAGE = 100\n\n\npubmed_MAP = {}\nSEMA = asyncio.BoundedSemaphore(300)\n\n####################### Get all PMID #########################\n\nasync def get_session_and_inputs(page, items_per_page):\n\n    headers = {\"User-Agent\" : \"Mozilla\/5.0\", \"Connection\": \"close\"}\n    async with aiohttp.ClientSession(headers=headers) as session:\n        async with session.get(GETURL) as response:\n            data = await response.text()\n            soup = BeautifulSoup(data, \"lxml\")\n\n            inputs = {i['name']: i.get('value', '') for i\n                      in soup.select('form#EntrezForm input[name]')}\n            inputs['EntrezSystem2.PEntrez.PubMed.Pubmed_ResultsPanel.Pubmed_DisplayBar.PageSize'] = ITEMS_PER_PAGE\n            inputs['EntrezSystem2.PEntrez.PubMed.Pubmed_ResultsPanel.Pubmed_DisplayBar.PrevPageSize'] = ITEMS_PER_PAGE\n            inputs['EntrezSystem2.PEntrez.DbConnector.Cmd'] = 'PageChanged'\n\n            inputs['EntrezSystem2.PEntrez.PubMed.Pubmed_ResultsPanel.Pubmed_Pager.CurrPage'] = page\n            inputs['EntrezSystem2.PEntrez.PubMed.Pubmed_ResultsPanel.Pubmed_Pager.cPage'] = page\n\n        async with session.post(POSTURL, data=inputs) as response2:\n            data = await response2.text()\n            soup = BeautifulSoup(data, \"lxml\")\n            PMID.append([pmid.text for pmid in soup('dd')])\n\n\nasync def get_all_pmid():\n    tasks = []\n    for page in range(NUM_PAGES):\n        task = asyncio.ensure_future(get_session_and_inputs(page=page,\n                                                            items_per_page=NUM_PAGES))\n        tasks.append(task)\n\n    await asyncio.gather(*tasks)\n\n############# Get all data in MAP from each uniq PMID articles ###################\n\nasync def parse_text_from_paper(pmid):\n    '''\n    From link to science paper (pmid) - get html,\n    then get document's body (data) from one page.\n    '''\n    \n    conn = aiohttp.TCPConnector(family=socket.AF_INET)\n    headers={\"User-Agent\" : \"Mozilla\/5.0\",\n             \"Connection\": \"close\"}\n    \n    async with aiohttp.ClientSession(headers=headers, connector=conn) as session:\n        async with SEMA, session.get(f\"https:\/\/www.ncbi.nlm.nih.gov\/pubmed\/{pmid}\") as response:\n            data = await response.text()\n            soup = BeautifulSoup(data, \"lxml\")\n\n            title = [text.find('h1').text for text in soup.find_all(\"div\", class_=\"rprt_all\")]\n            data = [text.find('p').text for text in soup.find_all(\"div\", class_=\"abstr\")]\n            pubmed_MAP[POSTURL+pmid] = data, title[0]\n                \nasync def get_all_data_p(PMID):\n\n    tasks = []\n    pmid_items = [pmid_i for lst in PMID for pmid_i in lst]\n\n    for pmid in pmid_items:\n\n        task = asyncio.ensure_future(parse_text_from_paper(pmid))\n        tasks.append(task)\n\n    await asyncio.gather(*tasks)\n\n\nloop = asyncio.get_event_loop()\nloop.run_until_complete(get_all_pmid())\nloop = asyncio.get_event_loop()\nloop.run_until_complete(get_all_data_p(PMID))\n    \nprint(f\"Done!\\nMap length: {len(pubmed_MAP)}\")","423b50ff":"print(len(pubmed_MAP))\nfiltered = {k: v for k, v in pubmed_MAP.items() if v is not None}\npubmed_MAP.clear()\npubmed_MAP.update(filtered)\nprint(len(pubmed_MAP))","5cc6da07":"%%time\n\nimport asyncio\nimport aiohttp\n\nimport requests\nfrom bs4 import BeautifulSoup\n\nGETURL = \"https:\/\/www.biorxiv.org\/search\/connectome%20numresults%3A75%20sort%3Arelevance-rank?page=\"\nMAIN_URL = \"https:\/\/www.biorxiv.org\"\nPAPER_LINKS = []\nNUM_PAGES = 32\nbiorxiv_MAP = {}\n\nasync def fetch_links_from_page(page, session, geturl=GETURL):\n    \"\"\"\n    corutine parse links on paper from page\n    and added there into list.\n    \"\"\"\n\n    async with session.get(f\"{geturl}{page}\") as response:\n        data = await response.text()\n        soup = BeautifulSoup(data, \"lxml\")\n\n        PAPER_LINKS.append([MAIN_URL + i.get(\"href\")for i in \n                         soup.find_all(name='a',class_='highwire-cite-linked-title')])\n\n\n\nasync def get_paper_links():\n    tasks = []\n    async with aiohttp.ClientSession() as session:\n        for page in range(NUM_PAGES):\n            task = asyncio.ensure_future(fetch_links_from_page(page, session))\n            tasks.append(task)\n        \n        await asyncio.gather(*tasks)\n\n\nasync def fetch_content(url, session):\n    async with session.get(url) as response:\n        data = await response.text()\n        soup = BeautifulSoup(data, \"lxml\")\n        try:\n            title = soup.find(\"h1\", {\"id\": \"page-title\"}).text\n            biorxiv_MAP[url] = [p.text for p in \n                            soup.find(\"div\", {\"id\": \"abstract-1\"}).find_all('p')], title\n        except:\n            print(f\"\\n Problem with this link {url}\")           \n            biorxiv_MAP[url] = None\n\n\n\n\nasync def get_all_data_b():\n    tasks = []\n\n    async with aiohttp.ClientSession() as session:\n        URLS = [link for lst in PAPER_LINKS for link in lst]\n        for url in URLS:\n            task = asyncio.ensure_future(fetch_content(url, session))\n            tasks.append(task)\n        \n        await asyncio.gather(*tasks)\n\n\n\n\nloop = asyncio.get_event_loop()\nloop.run_until_complete(get_paper_links())\n\nloop.run_until_complete(get_all_data_b())","b5c17b16":"print(len(biorxiv_MAP))\nfiltered = {k: v for k, v in biorxiv_MAP.items() if v is not None}\nbiorxiv_MAP.clear()\nbiorxiv_MAP.update(filtered)\nprint(len(biorxiv_MAP))","e68a1848":"! wget -O brain.jpg http:\/\/scienews.com\/images\/2017\/02\/bc9db0958f3f8847bd4337960c0ec609.jpg","3b1af313":"from PIL import Image\nimport numpy as np\nimport copy\nimport matplotlib.pyplot as plt\n\n\nfrom wordcloud import WordCloud, STOPWORDS\nfrom collections import Counter\nfrom nltk.corpus import stopwords\nimport nltk\n\nnltk.download('stopwords')","6671e6e4":"brain = Image.open('brain.jpg').convert(\"L\")\nbrain = np.array(brain)\nprint(\"Number unique values in original image:\")\nprint(len(np.unique(brain)), brain.shape)\n\nmask = copy.deepcopy(brain)\n\nprint(len(np.unique(mask)))\nmask[mask > 5] = 255\nmask[mask == 255] = 0\nmask[mask > 0] = 255\n\nfig, ax = plt.subplots(nrows=1, ncols=2, sharey=True, figsize=(25,25))\nax[0].set_title(\"Original\", \n                fontsize=20)\nax[0].imshow(brain)\nax[0].axis(\"off\")  \n\nax[1].set_title(\"Mask\", \n                fontsize=20) \nax[1].imshow(mask, cmap=\"binary\");\nax[1].axis(\"off\")   \n\nplt.subplots_adjust(wspace=.05, hspace=.05)\nplt.show()","ddc4ca9b":"def makeWordCloud(numWords, data, img_name, mask=None, color=None):\n    topic_words = [ z.lower() for y in\n                        [ x[0] for x in data.values()]\n                        for z in y]\n    word_count_dict = dict(Counter(topic_words))\n    popular_words = sorted(word_count_dict, key = word_count_dict.get, reverse = True)\n    popular_words_nonstop = [w for w in popular_words if w not in stopwords.words(\"english\")]\n    \n    word_string=str(popular_words_nonstop)\n    wordcloud = WordCloud(stopwords=STOPWORDS,\n                          background_color='black',\n                          max_words=numWords,\n                          width=1600, height=1080,\n                          mask=mask,\n                          contour_width=3, \n                          contour_color=color,\n                         ).generate(word_string)\n\n    wordcloud.to_file(f\"{img_name}.png\")\n\n    return wordcloud","824d1d41":"pubmed = makeWordCloud(10000000, \n                       pubmed_MAP,\n                       \"pubmed_worldcloud\")\n\npubmed_with_mask = makeWordCloud(10000000,\n                                 pubmed_MAP,\n                                 \"pubmed_worldcloud_with_mask\",\n                                 mask=mask)\nbiorxiv = makeWordCloud(10000000, \n                        biorxiv_MAP,\n                        \"biorxiv_worldcloud\")\n\nbiorxiv_with_mask = makeWordCloud(10000000,\n                                  biorxiv_MAP,\n                                  \"biorxiv_worldcloud_with_mask\",\n                                  mask=mask)\n\nfig, ax = plt.subplots(nrows=2, ncols=2, sharey=True, figsize=(20, 15))\n\nax[0, 0].imshow(pubmed);\nax[0, 0].axis(\"off\") \nax[0, 0].set_title(\"PubMed\", fontsize=20)\n\nax[0, 1].imshow(pubmed_with_mask);\nax[0, 1].axis(\"off\")   \nax[0, 1].set_title(\"PubMed\", fontsize=20)\n\nax[1, 0].imshow(biorxiv);\nax[1, 0].axis(\"off\")\nax[1, 0].set_title(\"bioRxiv\", fontsize=20)\n\nax[1, 1].imshow(biorxiv_with_mask);\nax[1, 1].axis(\"off\")\nax[1, 1].set_title(\"bioRxiv\", fontsize=20)\n\nplt.tight_layout(pad=.5, w_pad=.5, h_pad=.5);\nplt.show()","4898fa05":"#data has format:\nfor idx, i in enumerate([x for x in pubmed_MAP.items()]):\n    print(i[0])\n    print(i[1][1])\n    print(i[1][0][0])\n    break","9dbbaf80":"import nltk\n\nnltk.download('punkt')\nnltk.download('stopwords')\n\nimport re\nimport codecs\nimport re\nimport codecs\nimport multiprocessing\nimport gensim\nfrom gensim.models import Word2Vec","4c5a4ca4":"# Data preparation \n\ndef preprocess_text(text):\n    text = re.sub('[^a-zA-Z\u0430-\u044f\u0410-\u042f1-9]+', ' ', text)\n    text = re.sub(' +', ' ', text)\n    return text\n\ndef get_sentences(data, site):\n    if site == \"pubmed\":\n        text = [i for i in [x[0] for x in data.values()]]\n    else:\n        try:\n            text = [i for i in [x[0][0] for x in data.values()]]\n        except:\n            text = [i for i in [x[0] for x in data.values()]]\n\n    stop_words = stopwords.words('english')\n    print(\"Num articles:\", len(text))\n    text = \"\".join(str(text))\n    sentences = []\n    for sentence in nltk.sent_tokenize(text, \"english\"): \n        sentence = preprocess_text(sentence).lower()\n        clear_text = [word for word in sentence.split() if word not in stop_words]\n\n        clear_text = \" \".join(clear_text).strip()\n        sentences.append(clear_text.lower().split())\n\n    print(\"Num senteces:\", len(sentences))\n    return sentences\n\npubmed_sentences = get_sentences(pubmed_MAP, \"pubmed\")\nbiorxiv_sentences = get_sentences(biorxiv_MAP, \"bioarchiv\")\n","722b730e":"# Train\n\nbiorxiv_model = Word2Vec(biorxiv_sentences,\n                   size=200,\n                   window=5,\n                   min_count=3, \n                   workers=multiprocessing.cpu_count())\npubmed_model = Word2Vec(pubmed_sentences,\n                   size=200,\n                   window=5,\n                   min_count=3, \n                   workers=multiprocessing.cpu_count())\n\n\ndef get_embedds_and_words(model):\n    embeddings = []\n    words = []\n\n    for word in list(model.wv.vocab):\n        embeddings.append(model.wv[word])\n        words.append(word)\n        \n    return embeddings, words\n\n\npubmed_sentences = get_sentences(pubmed_MAP, \"pubmed\")\nbiorxiv_sentences = get_sentences(biorxiv_MAP, \"bioarchiv\")\nbiorxiv_model = Word2Vec(biorxiv_sentences,\n                   size=200,\n                   window=5,\n                   min_count=3, \n                   workers=multiprocessing.cpu_count())\npubmed_model = Word2Vec(pubmed_sentences,\n                   size=200,\n                   window=5,\n                   min_count=3, \n                   workers=multiprocessing.cpu_count())\n\nbiorxiv_embeddings, biorxiv_words = get_embedds_and_words(biorxiv_model)\npubmed_embeddings, pubmed_words = get_embedds_and_words(pubmed_model)","d0529907":"%%time\n# Train\nfrom sklearn.manifold import TSNE\nimport numpy as np\n\ndef get_tsna_2d(embeddings, p=30, n_iter=3500):\n    tsne = TSNE(perplexity=p,\n                      n_components=2,\n                      init='pca', \n                      n_iter=n_iter,\n                      random_state=36)\n    embeddings_2d = tsne.fit_transform(embeddings)\n    return embeddings_2d\n\ndef get_tsna_3d(embeddings, p=30, n_iter=3500):\n    tsne = TSNE(perplexity=p,\n                      n_components=3,\n                      init='pca', \n                      n_iter=n_iter,\n                      random_state=36)\n    embeddings_3d = tsne.fit_transform(embeddings)\n    return embeddings_3d\n\n\nbiorxiv_embeddings_2d = get_tsna_2d(biorxiv_embeddings)\nbiorxiv_embeddings_3d = get_tsna_3d(biorxiv_embeddings)\n\npubmed_embeddings_2d = get_tsna_2d(pubmed_embeddings)\npubmed_embeddings_3d = get_tsna_3d(pubmed_embeddings)","5ebf6b07":"from PIL import Image\nimport numpy as np\nimport random\n\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nimport seaborn as sns\n\n# set theme\nsns.set_context(\"poster\", font_scale=0.8, rc={\"lines.markersize\": 6.})\n\ndef get_color():\n    color = random.choice([\"mediumspringgreen\",\n                           \"cyan\",\n                           \"darkcyan\",\n                           \"mediumvioletred\",\n                           \"seagreen\",\n                           \"purple\",\n                           \"darkmagenta\"])\n    return color\n\n\ndef tsne_plot_2d(filename, label, embeddings, words=[], a=1, color=get_color()):\n    plt.figure(figsize=(16, 9))\n    colors = cm.rainbow(np.linspace(0, 1, 1))\n    x = embeddings[:,0]\n    y = embeddings[:,1]\n    plt.scatter(x, y, c=color, alpha=a, label=label)\n    for i, word in enumerate(words):\n        plt.annotate(word, alpha=0.3, xy=(x[i], y[i]), xytext=(5, 2), \n                     textcoords='offset points', ha='right', va='bottom', size=10)\n    plt.legend(loc=\"best\")\n    plt.grid(True)\n    plt.savefig(filename, format='png', dpi=150, bbox_inches='tight')\n    plt.axis(\"off\")","062245f0":"tsne_plot_2d(filename=\"biorxiv_tsne_2d_dots.png\",\n             label='Word2Vec vocabulary \\ntrained by papers on the topic \"Connectome\"',\n             embeddings=biorxiv_embeddings_2d,\n             a=0.1)","87e0e7a6":"tsne_plot_2d(filename=\"biorxiv_tsne_2d_words.png\",\n             label='Word2Vec vocabulary \\ntrained by papers on the topic \"Connectome\"',\n             embeddings=biorxiv_embeddings_2d,\n             words=biorxiv_words,\n             a=0.1)\n","3a332168":"tsne_plot_2d(filename=\"pubmed_tsne_2d_dots.png\",\n             label='Word2Vec vocabulary \\ntrained by papers on the topic \"Connectome\"',\n             embeddings=pubmed_embeddings_2d,\n             a=0.3)","d24e647c":"tsne_plot_2d(filename=\"pubmed_tsne_2d_words.png\",\n             label='Papers from the \"pubmed.gov\" on the topic -\"Conectome\"',\n             embeddings=pubmed_embeddings_2d,\n             words=pubmed_words,\n             a=0.1)","c7f15cc0":"from mpl_toolkits.mplot3d import Axes3D\nimport matplotlib.patches as mpatches\n\nsns.set_context(\"poster\", font_scale=0.5, rc={\"lines.markersize\": 6.})\n\nimport os\nimport tempfile\nimport imageio\nimport shutil\n\n\ndef tsne_plot_3d_gif(title,\n                     label,\n                     embeddings,\n                     c=\"darkslategray\",\n                     filename=\"3d.gif\",\n                     a=0.4):\n    \n    fig = plt.figure(figsize=(15,10))\n    ax = Axes3D(fig)\n\n    a = plt.scatter(embeddings[:, 0], embeddings[:, 1], embeddings[:, 2], c=c, alpha=a)\n\n    plt.title(title, fontsize=18)\n\n    red_patch = mpatches.Patch(color=c, label=label)\n    plt.legend(handles=[red_patch], loc=3, fontsize=15)\n\n    dirpath = tempfile.mkdtemp()\n    images = []\n    for angle in range(0, 360, 5):\n        ax.view_init(30, angle)\n        fname = os.path.join(dirpath, str(angle) + '.png')\n        plt.savefig(fname, dpi=120, format='png', bbox_inches='tight')\n        images.append(imageio.imread(fname))\n    imageio.mimsave(filename, images)\n    shutil.rmtree(dirpath)","4c98e8f3":"tsne_plot_3d_gif(title='Visualizing Word Embeddings using t-SNE',\n                 label='Papers from the \"bioarchive.org\" on the topic -\"Conectome\"',\n                 embeddings=biorxiv_embeddings_3d, \n                 c=get_color(),\n                 filename='biorxiv_3d.gif')\n","080799a0":"tsne_plot_3d_gif(title='Visualizing Word Embeddings using t-SNE',\n                 label='Papers from the \"pubmed.gov\" on the topic -\"Conectome\"',\n                 embeddings=pubmed_embeddings_3d, \n                 c=get_color(),\n                 filename='pubmed_3d.gif')","fd709493":"import random\nimport matplotlib.colors as mcolors\n\ndef get_2d_clusters(model,\n                    k_words,\n                    n_top_words,\n                    p=15,\n                    n_iter=3200):\n\n    keys = random.choices(list(model.wv.vocab.keys()), k=k_words)\n\n    embedding_clusters = []\n    word_clusters = []\n    for word in keys:\n        embeddings = []\n        words = []\n        for similar_word, _ in model.most_similar(word, topn=n_top_words):\n            words.append(similar_word)\n            embeddings.append(model[similar_word])\n        embedding_clusters.append(embeddings)\n        word_clusters.append(words)\n\n    # Find tsne coordinats for 2 dimensions\n    embedding_clusters = np.array(embedding_clusters)\n    n, m, k = embedding_clusters.shape\n    tsne_model_2d = TSNE(perplexity=p,\n                         n_components=2,\n                         init='pca',\n                         n_iter=n_iter,\n                         random_state=36)\n    \n    embeddings_2d = np.array(tsne_model_2d.fit_transform(embedding_clusters.reshape(n * m, k))).reshape(n, m, 2)\n\n    return keys, embeddings_2d, word_clusters\n\nb_keys, b_embeddings_2d, b_word_clusters =  get_2d_clusters(biorxiv_model, k_words=30, n_top_words=30)\np_keys, p_embeddings_2d, p_word_clusters =  get_2d_clusters(pubmed_model, k_words=30, n_top_words=30)","265feff1":"import matplotlib.colors as mcolors\nimport random\n\nsns.set_context(\"poster\", font_scale=0.6, rc={\"lines.linewidth\": 0.7})\n\ndef tsne_plot_similar_words(title,\n                            labels,\n                            embedding_clusters,\n                            word_clusters,\n                            a,\n                            filename):\n    plt.figure(figsize=(20, 15))\n    colors = random.choices(list(mcolors.CSS4_COLORS.keys()),k=len(labels))\n    idx_color = 0\n    for label, embeddings, words, color in zip(labels, embedding_clusters, word_clusters, colors):\n\n        x = embeddings[:, 0]\n        y = embeddings[:, 1]\n\n        plt.scatter(x, y, c=colors[idx_color], alpha=a, label=label)\n        for i, word in enumerate(words):\n            plt.annotate(word, alpha=0.5, xy=(x[i], y[i]), xytext=(5, 2),\n                         textcoords='offset points', ha='right', va='bottom', size=8)\n        idx_color += 1\n\n    plt.legend(loc=4, fontsize=12)\n    plt.title(title, fontsize=18)\n    plt.grid(True)\n    plt.axis(\"off\")\n    if filename:\n        plt.savefig(filename, format='png', dpi=150, bbox_inches='tight')","787908ae":"tsne_plot_similar_words(title='Similar words from Biorchiv on the topic \"Connectome\"',\n                        labels=b_keys,\n                        embedding_clusters=b_embeddings_2d,\n                        word_clusters=b_word_clusters,\n                        a=0.7,\n                        filename='biorxiv_similar_words.png')","3f9f44ad":"tsne_plot_similar_words(title='Similar words from PubMed on the topic \"Connectome\"',\n                        labels=p_keys,\n                        embedding_clusters=p_embeddings_2d,\n                        word_clusters=p_word_clusters,\n                        a=0.7,\n                        filename='pubmed_similar_words.png')","9d022625":"! wget -O promo.jpg https:\/\/i.pinimg.com\/originals\/34\/1a\/4e\/341a4edbf5ff6997b455e5bf1d524358.jpg","25a67626":"!pip install fpdf -qq","d7c948be":"from fpdf import FPDF\n\ndef encode_decode(data):\n    udata = data.encode()\n    return udata.decode('latin-1')\n\ndef make_pdf(site,\n             MAP,\n             pdf_name,\n             wc_name,\n             wc_name_mask,\n             name1_2d,\n             name2_2d,\n             sim_name,\n             gif_name,\n             theme,\n             img_promo=\"promo.jpg\"):\n    \n    PROLOG = f'''Web Scraping science papers from site:\n    {site} on the topic - \"{theme}\".'''\n    EPILOG = \"Top Words Cloud\"\n    EPILOG2 = \"t-SNE\"\n    empty = []\n\n    # Description\n    pdf = FPDF()\n    pdf.add_page()\n    pdf.set_font(\"Arial\", size=15, style=\"B\")\n\n    #pdf.set_creator(\"Bonart\")\n    pdf.multi_cell(0, 8, txt=PROLOG, align=\"C\")\n    pdf.image(img_promo, x=15, y=50, w=180, link=\"8.8.8.8\",)#\n    pdf.add_page()\n\n    # Create pdf-pages with data from dictionary\n    for idx, i in enumerate([x for x in MAP.items()]):\n        try:\n            url = i[0]\n            title = encode_decode(i[1][1])\n            data = encode_decode(i[1][0][0])\n            pdf.ln(10)\n            pdf.set_font(\"Arial\", size=12, style=\"B\")\n            pdf.multi_cell(0, 8, txt=title, align=\"L\")\n            pdf.set_text_color(180, 5, 100)\n            pdf.cell(0, 10, txt=url, ln=1, align=\"L\")\n            pdf.set_text_color(0, 0, 0)\n            pdf.set_font(\"Arial\", size=12)\n            pdf.multi_cell(0, 8, txt=data)\n        except: \n            empty.append(idx)\n            continue\n    print(\"Amount of empty data:\", len(empty))\n\n    # Epilog\n    pdf.add_page()\n    pdf.set_font(\"Arial\", size=15, style=\"B\")\n    pdf.multi_cell(0, 8, txt=EPILOG, align=\"C\")\n    pdf.image(f'{wc_name}.png', x=15, y=30, w=180, link=\"8.8.8.8\")\n    pdf.add_page()\n    pdf.image(f'{wc_name_mask}.png', x=15, y=10, w=180, link=\"8.8.8.8\")\n    pdf.add_page()\n    pdf.image(f'{name1_2d}.png', x=15, y=10, w=180, link=\"8.8.8.8\")\n    pdf.add_page()\n    pdf.image(f'{name2_2d}.png', x=15, y=10, w=180, link=\"8.8.8.8\")\n    pdf.add_page()\n    pdf.image(f'{sim_name}.png', x=15, y=10, w=180, link=\"8.8.8.8\")\n    pdf.add_page()\n    pdf.image(gif_name, x=15, y=10, w=180, link=\"8.8.8.8\")\n    pdf.output(f\"{pdf_name}.pdf\")","2ece5e04":"!ls -l","948f4edd":" make_pdf(site=\"bioRxiv\",\n          MAP=biorxiv_MAP,\n          pdf_name=\"bioRxiv\",\n          wc_name=\"biorxiv_worldcloud\",\n          wc_name_mask=\"biorxiv_worldcloud_with_mask\",\n          name1_2d=\"biorxiv_tsne_2d_dots\",\n          name2_2d=\"biorxiv_tsne_2d_words\",\n          sim_name=\"biorxiv_similar_words\",\n          gif_name=\"biorxiv_3d.gif\",\n          theme=\"Connectome\")\n\n    \nmake_pdf(site=\"PubMed\",\n         MAP=pubmed_MAP,\n         pdf_name=\"PubMed\",\n         wc_name=\"pubmed_worldcloud\",\n         wc_name_mask=\"pubmed_worldcloud_with_mask\",\n         name1_2d=\"pubmed_tsne_2d_dots\",\n         name2_2d=\"pubmed_tsne_2d_words\",\n         sim_name=\"pubmed_similar_words\",\n         gif_name=\"pubmed_3d.gif\",\n         theme=\"Connectome\")","5ec7789d":"from IPython.display import FileLink, FileLinks\nFileLinks('.')","9227e2a2":"! git clone https:\/\/github.com\/OldBonhart\/EDA-NLP-PubMed-and-bioRxiv.git","0c29b38a":"import sys\nsys.path.append('EDA-NLP-PubMed-and-bioRxiv')\nfrom source.main import main","31d5fd83":"from PIL import Image\nimport numpy as np\n\ndef get_mask(img_path):\n    img = Image.open(img_path).convert(\"L\")\n    return np.array(img)","103beb47":"# \"Coronavirus 2019-nCoV\" PubMed\nGETURL = \"https:\/\/www.ncbi.nlm.nih.gov\/pubmed\/?term=Coronavirus+2019-nCoV\"\nPOSTURL = \"https:\/\/www.ncbi.nlm.nih.gov\/pubmed\/\"\n\nPMID = []\nNUM_PAGES = 1\nITEMS_PER_PAGE = 100\npubmed_MAP = {}\n\nloop = asyncio.get_event_loop()\nloop.run_until_complete(get_all_pmid())\nloop = asyncio.get_event_loop()\nloop.run_until_complete(get_all_data_p(PMID))\n    \n# \"Coronavirus 2019-nCoV\" bioRxiv\n\nGETURL = \"https:\/\/www.biorxiv.org\/search\/coronavirus%252B2019-nCoV%20numresults%3A75%20sort%3Arelevance-rank?page=\"\nMAIN_URL = \"https:\/\/www.biorxiv.org\"\nPAPER_LINKS = []\nNUM_PAGES = 1\nbiorxiv_MAP = {}\n\nloop = asyncio.get_event_loop()\nloop.run_until_complete(get_paper_links())\n\nloop.run_until_complete(get_all_data_b())\n\n\nprint(f\"Done!\\nMap length: {len(biorxiv_MAP)}\")    \nprint(f\"Done!\\nMap length: {len(pubmed_MAP)}\")","b48958b0":"def remove_none(data):\n    print(len(data))\n    filtered = {k: v for k, v in data.items() if v is not None}\n    data.clear()\n    data.update(filtered)\n    print(len(data))\n    return data\n\npubmed_MAP = remove_none(pubmed_MAP)\nbiorxiv_MAP = remove_none(biorxiv_MAP)","f0b18269":"%%time\n\nMASK = get_mask(\"\/kaggle\/working\/EDA-NLP-PubMed-and-bioRxiv\/coronovirus\/mask.png\")\nPROMO = \"\/kaggle\/working\/EDA-NLP-PubMed-and-bioRxiv\/coronovirus\/promo.jpg\"\n\nmain(pubmed_MAP,biorxiv_MAP, \n     p_gif_name=\"3d_pubmded.gif\",b_gif_name=\"3d_bioxriv.gif\",\n     p_wc_name=\"pubmed_wordcloud\",\n     b_wc_name_mask=\"bioxriv_wordcloud_mask\",\n     b_wc_name=\"bioxriv_wordcloud\",\n     p_wc_name_mask=\"pubmed_wordcloud_mask\",\n     p_pdf_name=\"coronavirus_pubmed\",\n     b_pdf_name=\"coronavirus_bioxriv\",\n     p_site=\"PubMed\",\n     b_site=\"bioRxiv\", \n     b_2d_name1=\"biorxiv_tsne_2d_dots.png\",\n     b_2d_name2=\"biorxiv_tsne_2d_words.png\",\n     p_2d_name1=\"pubmed_tsne_2d_dots.png\",\n     p_2d_name2=\"pubmed_tsne_2d_words.png\", \n     b_sim_name=\"bioxriv_similar_words.png\", \n     p_sim_name=\"pubmed_similar_words.png\",\n     theme=\"Coronavirus 2019-nCoV\",\n     mask=MASK,\n     img_promo=PROMO)","f5e117fc":"#### For demonstration, I chose the theme - \u201c[Connectome](https:\/\/en.wikipedia.org\/wiki\/Connectome)\u201d in the end we will have connectome of connectomes :)\n#### In total, we need to parse all words from all articles of both sites and make the connection connect from the most commonly used words.\n\n<img style=\"display: block; margin-left: auto; margin-right: auto; width: 70%;\" border=\"0\" class=\"center\" alt=\"Connectome\" src=\"https:\/\/www.extremetech.com\/wp-content\/uploads\/2016\/01\/connectome.jpg?style=centerme\">\n","7502b440":"**Let's make GIF**","00dab0bf":"**PubMed**","f14e28ef":"# Coronavirus 2019-nCoV\n### Now we will do the same for \"Coronavirus 2019-nCoV\"","3b902653":"### Now let's make wordcloud image from our scraped data. \n**Firstly upload a representative image of a figure on the subject of our data and create its binary mask.**\n\n**We can also use sefexa or any other image segmentation tool.**","1a1fb91f":"# Biorxiv - https:\/\/www.biorxiv.org\/\n## Asynchronous scraping","caadef21":"### Data preparation.","9409f8e2":"# PubMed - https:\/\/www.ncbi.nlm.nih.gov\/\n## Synchronous scraping","332a51e1":"**bioRxiv**","cd15f6e7":"**In our case, we have an RGB image, so now we have to make zero values around our figure.**","573c3c01":"**bioRxiv - https:\/\/www.biorxiv.org\/search\/Connectome**\n\n**4568 Results.**\n\n**PubMed - https:\/\/www.ncbi.nlm.nih.gov\/pubmed\/?term=connectome**\n\n**2,372 Results.**","2ef145e1":"**Download representative image connectome brain from wiki**","a6eeb2c0":"# PubMed - https:\/\/www.ncbi.nlm.nih.gov\/\n## Asynchronous scraping","18397c50":"# Word2Vec\n\n**Now let's train word2vec on our data.**","98decf20":"**PubMed**","5db93b2b":"**bioRxiv**","70c19657":"## Coronavirus 2019-nCoV\n**bioRxiv - https:\/\/www.biorxiv.org\/search\/Coronavirus%252B2019-nCoV**\n\n**42 Results.**\n\n**PubMed - https:\/\/www.ncbi.nlm.nih.gov\/pubmed\/?term=Coronavirus+2019-nCoV**\n\n**79 Results.**","52da27a4":"# Rapid analysis of scientific papers from bioRxiv and PubMed","d4b2c639":"**Remove NoneType from data dict**","909f05ef":"# WordCloud","52a1dadf":"[](http:\/\/)Remove NoneType from data dict","b2ff9037":"**Remove NoneType from data dicts**","3ebbc302":"**Visualizing vector representations of words in 2d and 3d**","2ba1438d":"and let's look at clusters of similar words","5bfef920":"### This notebook contains the source code for asynchronously scraping science papers from the [**bioRxiv**](https:\/\/www.biorxiv.org\/) and [**PubMed**](https:\/\/www.ncbi.nlm.nih.gov\/pubmed\/), for subsequent conversion of this data into word embeddings Word2Vec for semantic and syntactic similarity words from this data, relations with other words, etc., followed by visualization of hight-dimensional Word2Vec word embeddings using t-SNE.","5395a940":"**PubMed**","6c438789":"**For comparison, compare synchronous and asynchronous web-scraping.**\n\n**47min 26s vs 3min 47s**","1c78a33e":"# Create pdf\n\n**Finally, let's combine everything into one file**","9eaf3b92":"**bioRxiv**","bb528368":"# t-SNE\n### Now we visualize vector representations of words using t-SNE"}}