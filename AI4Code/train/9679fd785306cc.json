{"cell_type":{"5ab5cd13":"code","93380fa2":"code","475eb7d4":"code","bd18da1e":"code","92987b59":"code","9598bd76":"code","65788dc5":"code","539fa35f":"code","c6b49154":"code","540a44a3":"code","d5c9f6c3":"code","6a6e4e0c":"code","3fc6883e":"code","7b2b3e5a":"code","0bff61c5":"code","24539664":"code","e2620a90":"code","46b6dfed":"code","b6a83bde":"code","b743fc2c":"markdown","4821c0e2":"markdown","8ee7659f":"markdown","cd4bb4a5":"markdown","90dd2f9f":"markdown","3186a3e7":"markdown","47c8dbda":"markdown","46db0125":"markdown","580de79b":"markdown","afb6d6c6":"markdown","2034b27f":"markdown","7f740001":"markdown","b78f65c7":"markdown","0ff38225":"markdown"},"source":{"5ab5cd13":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","93380fa2":"test = pd.read_csv('\/kaggle\/input\/tabular-playground-series-aug-2021\/test.csv')\ntrain = pd.read_csv('\/kaggle\/input\/tabular-playground-series-aug-2021\/train.csv')","475eb7d4":"train.shape, test.shape","bd18da1e":"train[:5]","92987b59":"test[:5]","9598bd76":"y = train[\"loss\"]\nXtrn = train.drop([\"loss\", \"id\"], axis = 1)","65788dc5":"Xtst = test.drop(\"id\", axis = 1)","539fa35f":"#standardisation\nnormed_Xtst = (Xtst - Xtrn.mean()) \/ Xtrn.std()\nnormed_Xtrn = (Xtrn - Xtrn.mean()) \/ Xtrn.std()","c6b49154":"#PCA\nfrom sklearn.decomposition import PCA\n\npcas = PCA(n_components=100)\npcas.fit(normed_Xtrn) #training PCA\nprojected = pcas.transform(normed_Xtrn) #projecting the data onto Principal components\nprint(Xtrn.shape)\nprint(projected.shape)\nplt.plot(pcas.explained_variance_); plt.grid();\nplt.xlabel('Explained Variance')\nplt.figure()\ncumvar = np.cumsum(pcas.explained_variance_ratio_)\ndimmin = np.where(cumvar > 0.95)[0][0]\nprint(dimmin)\nplt.plot(np.arange(len(pcas.explained_variance_ratio_))+1, cumvar,'o-') #plot the scree graph\nplt.axis([1,len(pcas.explained_variance_ratio_),0,1])\nplt.hlines(0.95, 1, 100, colors=\"orange\")\nplt.vlines(dimmin, 0, 0.95, colors = \"red\")\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance');\nplt.title('Scree Graph')\nplt.grid()\nplt.show()","540a44a3":"pca = PCA(n_components=94)\npca.fit(Xtrn) #training PCA\nprojected_Xtrn = pca.transform(Xtrn)\nprojected_Xtst = pca.transform(Xtst)","d5c9f6c3":"#Random forest\nfrom sklearn.ensemble import RandomForestRegressor\nRFclf = RandomForestRegressor(n_estimators=10)\nRFmodel = RFclf.fit(Xtrn, y)","6a6e4e0c":"#%store RFmodel\n%store -r RFmodel","3fc6883e":"feature_importances = pd.DataFrame([Xtrn.columns, RFmodel.feature_importances_]).T\nfeature_importances.columns = ['features', 'importances']\nplt.figure(figsize=(20,10))\nplt.title('Importances')\nplt.rcParams['font.size']=5\nplt.xticks(rotation=90)\nsns.barplot(y=feature_importances['importances'] , x=feature_importances['features'], palette='viridis')","7b2b3e5a":"from sklearn.linear_model import LinearRegression\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\n\n\nlr = LinearRegression()\nsvr =  SVR()\nrf = RandomForestRegressor(n_estimators=10)\nknn = KNeighborsRegressor(n_neighbors=5)","0bff61c5":"train_data = [Xtrn, normed_Xtrn, projected_Xtrn]\nmodels = [lr]","24539664":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import cross_validate\n\nstratifiedkfold = StratifiedKFold(n_splits=3)\n\ndef score(X, model):\n    scores = cross_validate(model, X, y, scoring=\"neg_root_mean_squared_error\", cv=stratifiedkfold)\n    return -np.mean(scores[\"test_score\"]), np.std(scores[\"test_score\"])","e2620a90":"for i in range(len(models)): \n    for j in range(len(train_data)):\n        if j==0:\n            x = \"raw\"\n        elif j==1:\n            x = \"normed\"\n        else:\n            x = \"pca\"\n        print(models[i], x, score(train_data[j],models[i]))","46b6dfed":"bestmodel = lr\nbestmodel.fit(Xtrn, y)\n\nprediction = bestmodel.predict(Xtst)","b6a83bde":"output = pd.DataFrame({'id': pd.read_csv('\/kaggle\/input\/tabular-playground-series-aug-2021\/test.csv').id,\n                       'loss': prediction, \n                       })\n\noutput.to_csv('submission.csv', index = False)\noutput","b743fc2c":"## Producing the results","4821c0e2":"## Progress so far:\n* 2\/8 - Implementing Feature importance and closs validation\n* 2\/8 - Implementing standardisation and PCA (Public score:7.94023)\n* 1\/8 - The first commitment, with a linear regression (Public score:7.93942)\n\n## What's next?\nHyperparameters tuning, SVR and random forest","8ee7659f":"## Closs Validation","cd4bb4a5":"SVR and Random forest take ages...","90dd2f9f":"---------","3186a3e7":"## Preprocessing and EDA","47c8dbda":"I don't know why usual RMSE is not available but negative one lol, any ideas?\nhttps:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html#scoring-parameter","46db0125":"How we shold use %store command on python?","580de79b":"## Analysis","afb6d6c6":"Also, it wasn't possible to set n_splits=5....","2034b27f":"> -------","7f740001":"There aren't much differences in importance between variables.","b78f65c7":"## Data Loading","0ff38225":"* ## This notebook intends to write down the pipeline of how Kaggle competition works, more than ML technical aspects (literally a \"note\"book !) \n* ## How to load the data, carry out analysis (even if it's so basic), and produce csv file.\n\n* ## Any comments, feedback, or advice are appreciated :)"}}