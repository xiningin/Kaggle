{"cell_type":{"14682c2b":"code","22af6b1e":"code","c4ccaa83":"code","aa7e754f":"code","7c560be5":"code","434a81c7":"code","c58959e6":"code","9a833a37":"code","15fce513":"code","f9c3ac42":"code","344248cb":"code","bfe83907":"code","fc476004":"code","ae37f00f":"code","cedb38e7":"code","5ead4f69":"code","6a524ae2":"code","5ab89468":"code","ab103410":"code","73945121":"code","c01b74f2":"code","7116cc8b":"code","d1bd00c7":"code","820beba2":"code","0c0fa5ac":"code","5a9cc6cf":"code","6263676b":"code","ba52fae7":"code","f0419303":"code","ea60b298":"code","5ecfa932":"code","84cb0592":"code","400f7b1f":"code","f861608a":"code","6315f029":"code","8edfdeb3":"code","51d82c60":"code","7da71abf":"code","10f16dd0":"code","9cdd6f3f":"code","075d5399":"code","7e64916c":"code","db303a45":"code","26137248":"code","8b7faac1":"markdown","06f77e2a":"markdown","1f67ec14":"markdown","02638f04":"markdown","28da00f4":"markdown","906041dc":"markdown","73506a6e":"markdown","01f0c7fe":"markdown","11484ea4":"markdown","26ee8165":"markdown","1232e239":"markdown","8728b9a4":"markdown","ff12b4de":"markdown","00e3074a":"markdown","06b34a49":"markdown","ad6af170":"markdown","9b1d3e3e":"markdown","109e8ec2":"markdown","0971407a":"markdown","ecef2d0d":"markdown","21f005a1":"markdown","d2b5682f":"markdown","0c07d29c":"markdown","26b17496":"markdown","94871d5b":"markdown","6d0e9caf":"markdown","1694f99c":"markdown","58d928a3":"markdown","60475eb8":"markdown","17c5df1e":"markdown","dfa46a05":"markdown","18acfd01":"markdown","b6d88709":"markdown"},"source":{"14682c2b":"import datetime\nimport calendar\nimport requests\nimport pandas as pd\nimport json\nimport os.path\nimport time\n","22af6b1e":"if (os.path.isfile(\"finex.csv\")): #if the file already exists start from the latest date\n    starttime = datetime.datetime.fromtimestamp(int(str(int(pd.read_csv('finex.csv', header=None).iloc[-1][0]))[:-3])) #read the last timestamp for csv file. Bitstamp takes and returs date date with 3 extra zeros. So that\nelse:\n    starttime = datetime.datetime.strptime('01\/04\/2013', '%d\/%m\/%Y') #Start collecting from April 1, 2013\n\nstart_unixtime = calendar.timegm(starttime.utctimetuple())\n\nlatest_time = int(time.time() - 60 * 60 * 24) #The real ending time. Collect data from starttime to current time - 24 hours\n\ntrack_time = time.time() #because bitstamp only allows 10 requests per minute. Take rest if we are faster than that\ncount = 0\n\nwhile (start_unixtime < latest_time):\n    end_unixtime = start_unixtime + 60*60*24*30 #30 days at a time\n    \n    if (end_unixtime > latest_time):\n        end_unixtime = latest_time #if the time is in future.\n\n    url = 'https:\/\/api.bitfinex.com\/v2\/candles\/trade:1h:tBTCUSD\/hist?start={}&end={}&limit=1000'.format(str(start_unixtime) + \"000\", str(end_unixtime) + \"000\") #1 hour can be changed to any timeframe\n    response = requests.get(url)\n    data = response.json()\n\n    df = pd.DataFrame(data).set_index(0).sort_index() #set the date column as index and sort all data\n\n    df.to_csv('finex.csv',header=None,mode='a') #append the data\n    \n    print('Saved till {}'.format(datetime.datetime.fromtimestamp(int(end_unixtime)).strftime('%Y-%m-%d %H:%M:%S')))\n    \n    start_unixtime = end_unixtime + 60 * 60 #to prevent duplicates\n    count = count + 1\n    \n    if (count == 10): #if 10 requests are made\n        count = 0 #reset it\n        \n        diff = time.time() - track_time\n        \n        if (diff <= 60):\n            print('Sleeping for {} seconds'.format(str(60 - diff)))\n            time.sleep(60 - diff) #sleep\n            \n        \n        track_time = time.time()\n    #bitstamp limits to 10 requests per minute\n\t\n\n#add the header\ndf = pd.read_csv('finex.csv', header=None, index_col=None)\n\ndf.columns = ['Time', 'Open', 'Close', 'High', 'Low', 'Volume']\n    \ndf.set_index('Time')\ndf.to_csv('final.csv', index=False) ","c4ccaa83":"import numpy as np\nimport pandas as pd\nimport pandas_datareader.data as web\nimport warnings\nfrom matplotlib import pyplot\nfrom pandas.plotting import scatter_matrix\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom pandas.plotting import scatter_matrix\nfrom statsmodels.graphics.tsaplots import plot_acf","aa7e754f":"#Function and modules for time series models\nfrom statsmodels.tsa.arima_model import ARIMA\nimport statsmodels.api as sm\nfrom statsmodels.tsa.stattools import acf, pacf","7c560be5":"#Function and modules for the supervised regression models\n\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.neural_network import MLPRegressor\n\n\n# Function and modules for data analysis and model evaluation\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2, f_regression\n\n\n# Function and modules for deep learning models\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import SGD\nfrom keras.layers import LSTM\nfrom keras.wrappers.scikit_learn import KerasRegressor","434a81c7":"df1=pd.read_csv('\/kaggle\/input\/bitstamp\/final.csv')","c58959e6":"df1","9a833a37":"from datetime import datetime\ne=[]\nfor i in df1['Time']\/1000:\n    \n    d=datetime.fromtimestamp(i)\n    e.append(d.strftime('%Y\/%m\/%d\/%H\/%M\/%S'))\n  \n\n    \n\n\n","15fce513":"f=pd.to_datetime(e, format='%Y\/%m\/%d\/%H\/%M\/%S')","f9c3ac42":"df1['Date']=f","344248cb":"df1['Date']=pd.to_datetime(df1['Date'], format='%Y\/%m\/%d\/%H\/%M\/%S')","bfe83907":"\ndf2=df1.resample('D', on='Date').mean()","fc476004":"ccy_tickers = ['DEXJPUS', 'DEXUSUK']\nidx_tickers = ['SP500', 'DJIA', 'VIXCLS']\n\nccy_data = web.DataReader(ccy_tickers, 'fred')\nidx_data = web.DataReader(idx_tickers, 'fred')","ae37f00f":"df4=pd.concat([df2,idx_data,ccy_data], join='outer', axis=1).ffill(axis=0).drop(['Time', 'Open','High','Low'], axis=1)","cedb38e7":"df4.describe()","5ead4f69":"return_period = 1\nY = np.log(df4.loc[:, ('Close')]).diff(return_period).\\\nshift(-return_period)\nY.name = Y.name+'_pred'\n","6a524ae2":"return_period = 1\nX = np.log(df4.iloc[:,1:]).diff(return_period).shift(-return_period)\n\n\n","5ab89468":"df4","ab103410":"X4 = pd.concat([np.log(df4.loc[:, ('Close')]).diff(i) for i in [return_period*32, return_period*4,\\\nreturn_period*8, return_period*16]], axis=1).dropna()\nX4.columns = ['Close_4', 'Close_8', 'Close_16', 'Close_32']\nX0 = pd.concat([X, X4], axis=1)\ndataset = pd.concat([Y, X0], axis=1).dropna()\nY = dataset.loc[:, Y.name]\nX = dataset.loc[:, X0.columns]","73945121":"res = sm.tsa.seasonal_decompose(Y,freq=52)\nfig = res.plot()\nfig.set_figheight(8)\nfig.set_figwidth(15)\npyplot.show()","c01b74f2":"correlation = dataset.corr()\npyplot.figure(figsize=(15,15))\npyplot.title('Correlation Matrix')\nsns.heatmap(correlation, vmax=1, square=True,annot=True,cmap=\"YlGnBu\")","7116cc8b":"pyplot.figure(figsize=(15,15))\nscatter_matrix(dataset,figsize=(12,12))\npyplot.show()","d1bd00c7":"l=int(len(X))","820beba2":"X_train, X_test = X.iloc[0:int(0.6*l)], X.iloc[int(0.6*l):l]\nY_train, Y_test  = Y.iloc[0:int(0.6*l)], Y.iloc[int(0.6*l):l]","0c0fa5ac":"\nmodels = []\nmodels.append(('LR', LinearRegression()))\nmodels.append(('LASSO', Lasso()))\nmodels.append(('EN', ElasticNet()))\nmodels.append(('KNN', KNeighborsRegressor()))\nmodels.append(('CART', DecisionTreeRegressor()))\nmodels.append(('SVR', SVR()))","5a9cc6cf":"models.append(('MLP', MLPRegressor()))","6263676b":"models.append(('ABR',AdaBoostRegressor()))\nmodels.append(('GBR',GradientBoostingRegressor()))","ba52fae7":"models.append(('RFR',RandomForestRegressor()))\nmodels.append(('ETR',ExtraTreesRegressor()))\n","f0419303":"num_folds = 10\nscoring = 'neg_mean_squared_error'\nseed = 7","ea60b298":"names = []\nkfold_results = []\ntest_results = []\ntrain_results = []\nfor name, model in models:\n    names.append(name)\n## k-fold analysis:\n    kfold = KFold(n_splits=num_folds, random_state=seed, shuffle=True)\n#converted mean squared error to positive. The lower the better\n    cv_results = -1* cross_val_score(model, X_train, Y_train, cv=kfold,  scoring=scoring)\n   \n    kfold_results.append(cv_results)\n# Full Training period\n    res = model.fit(X_train, Y_train)\n    train_result = mean_squared_error(res.predict(X_train), Y_train)\n    train_results.append(train_result)\n# Test results\n    test_result = mean_squared_error(res.predict(X_test), Y_test)\n    test_results.append(test_result)","5ecfa932":"test_results","84cb0592":"fig = pyplot.figure()\nfig.suptitle('Algorithm Comparison: Kfold results')\nax = fig.add_subplot(111)\npyplot.boxplot(kfold_results)\nax.set_xticklabels(names)\nfig.set_size_inches(15,8)\npyplot.show()","400f7b1f":"fig = pyplot.figure()\nind = np.arange(len(names)) # the x locations for the groups\nwidth = 0.35 # the width of the bars\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\npyplot.bar(ind - width\/2, train_results, width=width, label='Train Error')\npyplot.bar(ind + width\/2, test_results, width=width, label='Test Error')\nfig.set_size_inches(15,8)\npyplot.legend()\nax.set_xticks(ind)\nax.set_xticklabels(names)\npyplot.show()","f861608a":"X_train_ARIMA=X_train.iloc[:,:6]\nX_test_ARIMA=X_test.iloc[:,:6]\ntr_len = len(X_train_ARIMA)\nte_len = len(X_test_ARIMA)\nto_len = len (X)","6315f029":"X_test_ARIMA","8edfdeb3":"modelARIMA=ARIMA(endog=Y_train,exog=X_train_ARIMA,order=[1,0,0])\nmodel_fit = modelARIMA.fit()","51d82c60":"error_Training_ARIMA = mean_squared_error(Y_train, model_fit.fittedvalues)\npredicted = model_fit.predict(start = tr_len -1 ,end = to_len -1, exog = X_test_ARIMA)[1:]\n\nerror_Test_ARIMA = mean_squared_error(Y_test,predicted)\nerror_Test_ARIMA","7da71abf":"test_results.append(error_Test_ARIMA)\n#test_results.append(error_Test_LSTM)\ntrain_results.append(error_Training_ARIMA)\n#train_results.append(error_Training_LSTM)\nnames.append(\"ARIMA\")\n#names.append(\"LSTM\")","10f16dd0":"fig = pyplot.figure()\nind = np.arange(len(names)) # the x locations for the groups\nwidth = 0.35 # the width of the bars\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\npyplot.bar(ind - width\/2, train_results, width=width, label='Train Error')\npyplot.bar(ind + width\/2, test_results, width=width, label='Test Error')\nfig.set_size_inches(15,8)\npyplot.legend()\nax.set_xticks(ind)\nax.set_xticklabels(names)\npyplot.show()","9cdd6f3f":"def evaluate_arima_model(arima_order):\n#predicted = list()\n    modelARIMA=ARIMA(endog=Y_train,exog=X_train_ARIMA,order=arima_order)\n    model_fit = modelARIMA.fit()\n    error = mean_squared_error(Y_train, model_fit.fittedvalues)\n    return error","075d5399":"def evaluate_models(p_values, d_values, q_values):\n    best_score, best_cfg = float(\"inf\"), None\n    for p in p_values:\n        for d in d_values:\n            for q in q_values:\n                order = (p,d,q)\n                try:\n                    mse = evaluate_arima_model(order)\n                    if mse < best_score:\n                        best_score, best_cfg = mse, order\n                        print('ARIMA%s MSE=%.7f' % (order,mse))\n                except:\n                    continue\n    print('Best ARIMA%s MSE=%.7f' % (best_cfg, best_score))","7e64916c":"# evaluate parameters\np_values = [0, 1, 2]\nd_values = range(0, 2)\nq_values = range(0, 2)\nwarnings.filterwarnings(\"ignore\")\nevaluate_models(p_values, d_values, q_values)","db303a45":"# prepare model\nmodelARIMA_tuned=ARIMA(endog=Y_train,exog=X_train_ARIMA,order=[2,0,1])\nmodel_fit_tuned = modelARIMA_tuned.fit()\n# estimate accuracy on validation set\npredicted_tuned = model_fit.predict(start = tr_len -1 ,\\\nend = to_len -1, exog = X_test_ARIMA)[1:]\nprint(mean_squared_error(Y_test,predicted_tuned))","26137248":"# plotting the actual data versus predicted data\npredicted_tuned.index = Y_test.index\nY_test.iat[0]=np.log(df4['Close'].loc['2019-05-21'])\npyplot.plot(np.exp(Y_test).cumprod(), 'r', label='actual',)\n# plotting t, a separately\npredicted_tuned.iat[0]=np.log(df4['Close'].loc['2019-05-21'])\npyplot.plot(np.exp(predicted_tuned).cumprod(), 'b--', label='predicted')\npyplot.legend()\npyplot.rcParams[\"figure.figsize\"] = (20,10)\npyplot.show()","8b7faac1":"### **Feature generation**\n\nSometimes future values of the time series are correlated with its preceeding values known as autocorrelation. So, these values can be used to predict the future values ofthe target variable. Here, four new variables are geenrated from the target variable Y with differencing of period 4, 8, 16, 32 and concated with the dataset as input variable. \n","06f77e2a":"[[](http:\/\/)](http:\/\/)","1f67ec14":"### **Scatter plot**\n\nA somewhat linear relation of value being predicted with the  lagged values. No particular relation with indices and currency rates.","02638f04":"**Ensemble models**","28da00f4":"Now split the dataset into train and test sets. 60% of data comprise train and rest is under test set.","906041dc":"### **Decomposition of time series**\n\nSeries can be decompopsed as,\n\nyt = St + Tt + Rt    \nwhere yt is the target variable, St is the seasonal component, Tt is the trend-cycle component and Rt is the remainder at some period t. \nAbive is an example of additive decomposition.\n\nMultiplicative decomposotion can also be done (appropriate when the seasonal or trend component varies with time series) as follows,\n\nyt=St * Tt * Rt\n\nwhich is equivalent to:\n\nlog(yt) = log(St) + log(Tt) + log(Rt)     This is employed in this notebook.\n\nMore on decomposition of time series can be found on the [link](http:\/\/otexts.com\/fpp2\/components.html)\n","73506a6e":"#### **The Input variable**\n\nThe rest of dataset columns (except) close column have been taken as input variables. Same logarithmic function and difference of period 1 is performed on the input variables  as done for the target variable above.","01f0c7fe":"**2. Bagging methods**","11484ea4":"### **Time series model - ARIMA**\n\nNow ARIMA model is used to analyse the problem. The lagged variables are not needed so the dataset for ARIMA would not include the lagged values for close column. Following is the train and test split of dataset required for ARIMA.","26ee8165":"####  **Comparing algorithms**\n\nCART is overfitting giving the highest test error and lowest train error. Linear regression models gave similar test and train error.\n","1232e239":"### Optimized parameters\n\nARIMA(2,0,1) is the best model for fitting and predicting the desired variable. Following is the prediction and the error associated with it.","8728b9a4":"**Error for different models**","ff12b4de":"## **Conclusion**\n\nThe model captures the ternd of the series and it is less volatile. The purpose of the model is to predict the Close variable for the next few days dpening upon the dataset. It is reasonable that the predicted values will get deviated from the actual as we get away from the test date points as only few dates after test dates are modelled to be predicted.\n\nThis model bring forth the simplicity of the linear models along with the time series model ARIMA to pedict the closing price of Bitcoin. This approach helps\nus deal with overfitting and underfitting, which are some of the key challenges in predicting\nproblems in prediction of share price and crypto price. \n\nModel can be improved to predict more accurately by incorporating more variables like news data, tweet data, P\/E ratio, price momentum etc. ","00e3074a":"## **Dataset**\n\nThe indices and currency columns are merged into the main dataset. Time, open, high and low columns are dropped as only volume and close parameters are used from the original dataset from Bitstamp.","06b34a49":"### **Mean Squared error of models**\n\nBelw is the fig for mean square error for various models. As can be seen, the linear models are giving the best results. EN and LASSO slightly better than LR.","ad6af170":"**Notice that data is obtained every one hour during a day so we will avaerage over the values per day for our prediction model.**","9b1d3e3e":"![image.png](attachment:image.png)","109e8ec2":"**Train and test set for ARIMA**","0971407a":"Since data for the correlated assets are available only for the dates after 2016\/06\/06, the data for the dates before this date have been dropped in the original dataset.","ecef2d0d":"### Model tuning and grid search\n\nThe ARIMA model needs three parameters p, q and d, where p is the order of the autoregressive part, d is the degree of first differencing involved, and q is the order of the moving average part. For intial model ARIMA(p, q, d) is set as ARIMA(1, 0, 0).  So we perform a grid search with different p, d, and q combinations in the ARIMA model\u2019s order and select the combination that minimizes the fitting error.","21f005a1":"### **Models**\n\nTen fold cross validation (CV) have been used to optimise the various parameters of models. The model performance is decided by measuring the mean square error.\nThe following models are implemented using sklearn package.","d2b5682f":"**Notice that the datetime format is in the form of UNIX Timestamp, below it is converted**","0c07d29c":"## **Correlation Matrix**\n\nNegative correlation between the close value (to be predicted) and VIX index, JPY\/USD. Rest of independent variables have positive correlation with variable to be predicted. Note that we are taking the log of dataset and comparing the correlation between different columns. Also the vlue to b predicted is taken as log of close value differnced at period 1. Finally the exponential will be taken and difference effect will be removed ro get the desired outcome.","26b17496":"#### **Obtaining data from Bitfinex**\nThe data is obtained from Bitfinex from 01\/01\/2013 till latest. The code to obtain the data is taken from this [link](https:\/\/github.com\/zengqiang041\/Historical-Hourly-Bitstamp-Python\/blob\/master\/script.py)","94871d5b":" # **Bitcoin Price Prediction**","6d0e9caf":"#### **Loading required modules, libraries and models**","1694f99c":"#### **Obtained dataset for the various parameters of bitcoin trading**","58d928a3":"**Error for ARIMA model**\n\nFollowing the error associated with the ARIMA model is calculated as mean squared error and it comes out to be reasonable and comparable to the linear models discussed above.","60475eb8":"**Neural network algorithms**","17c5df1e":"##### **Regression and tree regression algorithms**","dfa46a05":"## **Correlated Assets**\n\nFor this study potentially correlated assets are used for the prediction. For currency the USD\/JPY and GBP\/USD are used and for indices Dow jones, VIX and S&P 500 index are used. Correlated assests are those that effects each other behavior either positiely or negatively. Here indices such as S&P 500, Dow Jones, VIX are used that may represent potential correlated asset.\n\nAccording to [investopedia](http:\/\/www.investopedia.com\/terms\/i\/index.asp),\"An index is a method to track the performance of a group of assets in a standardized way. Indexes typically measure the performance of a basket of securities intended to replicate a certain area of the market. These could be a broad-based index that captures the entire market, such as the Standard & Poor's 500 Index or Dow Jones Industrial Average (DJIA), or more specialized such as indexes that track a particular industry or segment.\"\n\nThe other asset used is USD\/JPY and GBP\/USD currency exchange rate between US Dollar- Japanese Yen and British Pound - US Dollar.","18acfd01":"**1. Boosting methods**","b6d88709":"#### **The Target variable**\n\nThe close column is used as the target or output variable. The logarithmic of 'close' variable is taken and then the adjacent terms have been subtracte to smooth out the data and to make the series as much stationary as possible, which is obviously easier to predict. More about stationarity, log function of a series and differencing can be found on the [link](http:\/\/otexts.com\/fpp2\/stationarity.html). Mainly differencing is done to get rid of varying mean in the series."}}