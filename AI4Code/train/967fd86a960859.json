{"cell_type":{"41fc98cf":"code","d9bc3a64":"code","0fc3f4da":"code","2e7b2489":"code","ed70c894":"code","b3284ec7":"code","34886c27":"code","1cb312ed":"code","c67ac145":"code","6af9e292":"code","94103728":"code","089a4f65":"code","ae21f1eb":"code","03368b20":"code","83d8fad4":"code","db7894fa":"code","0f80ddf5":"code","fee0c6d7":"code","2c154faf":"code","3eda8cbb":"markdown","0b8e83e5":"markdown"},"source":{"41fc98cf":"# Load Libraries \n\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Flatten\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error \nfrom matplotlib import pyplot as plt\nimport seaborn as sb\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport warnings \nwarnings.filterwarnings('ignore')\nwarnings.filterwarnings('ignore', category=DeprecationWarning)\nfrom xgboost import XGBRegressor","d9bc3a64":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","0fc3f4da":"root_path = '\/kaggle\/input\/house-prices-advanced-regression-techniques\/'\n\ndef get_data():\n    #get train data\n    train_data_path ='train.csv'\n    train = pd.read_csv(root_path + train_data_path)\n    \n    #get test data\n    test_data_path ='test.csv'\n    test = pd.read_csv(root_path + test_data_path)\n    \n    return train , test\n\ndef get_combined_data():\n  #reading train data\n  train , test = get_data()\n\n  target = train.SalePrice\n  train.drop(['SalePrice'],axis = 1 , inplace = True)\n\n  combined = train.append(test)\n  combined.reset_index(inplace=True)\n  combined.drop(['index', 'Id'], inplace=True, axis=1)\n  return combined, target\n\n#Load train and test data into pandas DataFrames\ntrain_data, test_data = get_data()\n\n#Combine train and test data to process them together\n# Q - why is data being combined?  \n# A - One Hot Encoding\n\ncombined, target = get_combined_data()","2e7b2489":"combined.describe()","ed70c894":"def get_cols_with_no_nans(df,col_type):\n    '''\n    Arguments :\n    df : The dataframe to process\n    col_type : \n          num : to only get numerical columns with no nans\n          no_num : to only get nun-numerical columns with no nans\n          all : to get any columns with no nans    \n    '''\n    if (col_type == 'num'):\n        predictors = df.select_dtypes(exclude=['object'])\n    elif (col_type == 'no_num'):\n        predictors = df.select_dtypes(include=['object'])\n    elif (col_type == 'all'):\n        predictors = df\n    else :\n        print('Error : choose a type (num, no_num, all)')\n        return 0\n    cols_with_no_nans = []\n    for col in predictors.columns:\n        if not df[col].isnull().any():\n            cols_with_no_nans.append(col)\n    return cols_with_no_nans","b3284ec7":"num_cols = get_cols_with_no_nans(combined , 'num')\ncat_cols = get_cols_with_no_nans(combined , 'no_num')\n\nprint ('Number of numerical columns with no nan values :',len(num_cols))\nprint ('Number of nun-numerical columns with no nan values :',len(cat_cols))","34886c27":"combined = combined[num_cols + cat_cols]\ncombined.hist(figsize = (12,10))\nplt.show()","1cb312ed":"train_data = train_data[num_cols + cat_cols]\ntrain_data['Target'] = target\n\nC_mat = train_data.corr()\nfig = plt.figure(figsize = (15,15))\n\nsb.heatmap(C_mat, vmax = .8, square = True)\nplt.show()","c67ac145":"def oneHotEncode(df,colNames):\n    for col in colNames:\n        if( df[col].dtype == np.dtype('object')):\n            dummies = pd.get_dummies(df[col],prefix=col)\n            df = pd.concat([df,dummies],axis=1)\n\n            #drop the encoded column\n            df.drop([col],axis = 1 , inplace=True)\n    return df\n    \n\nprint('There were {} columns before encoding categorical features'.format(combined.shape[1]))\ncombined = oneHotEncode(combined, cat_cols)\nprint('There are {} columns after encoding categorical features'.format(combined.shape[1]))\n\n# TODO - why do we have to one-hot encode these categories?  Look at book to see if it says.","6af9e292":"def split_combined():\n    global combined\n    train = combined[:1460]\n    test = combined[1460:]\n\n    return train , test \n  \ntrain, test = split_combined()","94103728":"#TODO - bone up on param calculation.  I believe it's something like:\n#   # of parameters (columns) * # of neurons in layer + # of bias values?\nNN_model = Sequential()\n\n# The Input Layer :\nNN_model.add(Dense(128, kernel_initializer='normal',input_dim = train.shape[1], activation='relu'))\n\n# The Hidden Layers :\nNN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\nNN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\nNN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n\n# The Output Layer :\nNN_model.add(Dense(1, kernel_initializer='normal',activation='linear'))\n\n# Compile the network :\nNN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\nNN_model.summary()","089a4f65":"# Very important, as we start to bounce around min loss the further in we get.\ncheckpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \ncheckpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\ncallbacks_list = [checkpoint]","ae21f1eb":"# Original\nNN_model.fit(train, target, epochs=500, batch_size=32, validation_split = 0.2, callbacks=callbacks_list)\n","03368b20":"# PLOT losses\nstyles=[':','-.','--','-',':','-.','--','-',':','-.','--','-']\ndef plot_history(nets, history, ylim):\n    plt.figure(figsize=(15,5))\n    for i in range(nets):\n        plt.plot(history[i].history['val_loss'],linestyle=styles[i])\n    plt.title('model accuracy')\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend(names, loc='upper left')\n    axes = plt.gca()\n    axes.set_ylim(ylim)\n    plt.show()","83d8fad4":"# BUILD CONVOLUTIONAL NEURAL NETWORKS\nnets = 3\nmodel = [0] *nets\n\nfor j in range(nets):\n   \n    model[j] = Sequential()\n\n    # The Input Layer :\n    model[j].add(Dense(128, kernel_initializer='normal',input_dim = train.shape[1], activation='relu'))\n\n    # The Hidden Layers :\n    model[j].add(Dense(256, kernel_initializer='normal',activation='relu'))\n    if j>0:\n        model[j].add(Dense(256, kernel_initializer='normal',activation='relu'))\n    if j>1:\n        model[j].add(Dense(256, kernel_initializer='normal',activation='relu'))\n\n    # The Output Layer :\n    model[j].add(Dense(1, kernel_initializer='normal',activation='linear'))\n\n    # Compile the network :\n    model[j].compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n","db7894fa":"# TRAIN NETWORKS\nhistory = [0] * nets\nnames = [\"1L\", \"2L\", \"3L\"]\nepochs = 500\nfor j in range(nets):\n\n    checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}--' + names[j] + '.hdf5' \n    checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 0, save_best_only = True, mode ='auto')\n    callbacks_list = [checkpoint]\n    \n    history[j] = model[j].fit(train, \n                              target, \n                              epochs=epochs, \n                              batch_size=32, \n                              validation_split = 0.2, \n                              callbacks=callbacks_list, \n                              verbose=0)\n\n    print(\"CNN {0}: Epochs={1:d}, Train loss={2:.5f}, Validation loss={3:.5f}\".format(\n        names[j],epochs,max(history[j].history['loss']),max(history[j].history['val_loss']) ))","0f80ddf5":"plot_history(nets, history, [15000,60000])","fee0c6d7":"# TODO - is there a more deterministic way of grabbing best checkpoint?\n# Load weights file of the best model :\nweights_file = 'Weights-327--18856.07227.hdf5' # choose the best checkpoint \nNN_model.load_weights(weights_file) # load it\nNN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n","2c154faf":"def make_submission(prediction, sub_name):\n  my_submission = pd.DataFrame({'Id':pd.read_csv(root_path + 'test.csv').Id,'SalePrice':prediction})\n  my_submission.to_csv('{}.csv'.format(sub_name),index=False)\n  print('A submission file has been made')\n\npredictions = NN_model.predict(test)\nmake_submission(predictions[:,0],'submission(NN).csv')","3eda8cbb":"From:\nhttps:\/\/towardsdatascience.com\/deep-neural-networks-for-regression-problems-81321897ca33","0b8e83e5":"# Summary #\n\n3 layers seemed to perform better!  Executing again with 500 epochs, and restricting graph"}}