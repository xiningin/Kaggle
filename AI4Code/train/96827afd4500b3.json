{"cell_type":{"200d8c44":"code","30bd01c1":"code","0be982e0":"code","385ec24f":"code","21b8e722":"code","b0890383":"code","e7514bd8":"code","b4a7554c":"code","ce4a9d82":"code","d5493cc7":"code","f1d3c714":"code","f6e82997":"code","6ac4dc7f":"code","64e28a21":"code","a7a56dce":"code","6b10e12e":"markdown","2809f9b3":"markdown","02ac2130":"markdown","f50a373b":"markdown","7dd15406":"markdown","4c4f73e4":"markdown","92b58ce2":"markdown","d62566ba":"markdown","71d884f4":"markdown","d6941666":"markdown"},"source":{"200d8c44":"import os\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nimport pandas_profiling as pp\n\nfrom sklearn import cluster, mixture\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans, DBSCAN, OPTICS\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.neighbors import kneighbors_graph\nfrom itertools import cycle, islice\n\nimport plotly as py\nimport plotly.graph_objs as go\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n\nimport warnings\nwarnings.simplefilter('ignore')\n\npd.set_option('max_columns', 200)","30bd01c1":"data = pd.read_csv(\"..\/input\/heart-disease-uci\/heart.csv\")\ndata = data.drop_duplicates().reset_index(drop=True)","0be982e0":"data.head(3)","385ec24f":"data.describe()","21b8e722":"# Data format optimization\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int8', 'int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df\n\ndata = reduce_mem_usage(data)","b0890383":"data.head(3)","e7514bd8":"data.info()","b4a7554c":"# Thanks to https:\/\/www.kaggle.com\/vbmokin\/titanic-top-3-cluster-analysis\n\ndef generate_clustering_algorithms(Z, n_clusters, m):\n    # Generate clustering algorithms:\n    # m = 'MeanShift', 'KMeans', 'MiniBatchKMeans', 'AgglomerativeClustering_ward',\n    # 'SpectralClustering', 'DBSCAN', 'OPTICS', 'AffinityPropagation',\n    # 'AgglomerativeClustering_average', 'Birch', 'GaussianMixture'\n    \n    # The minimal percentage of similarity of the clustered feature with \"Survived\" for inclusion in the final dataset\n    limit_opt = 0.7\n    \n    # Thanks to: https:\/\/scikit-learn.org\/stable\/auto_examples\/cluster\/plot_cluster_comparison.html#sphx-glr-auto-examples-cluster-plot-cluster-comparison-py\n    params = {'quantile': .2,\n              'eps': .3,\n              'damping': .9,\n              'preference': -200,\n              'n_neighbors': 10,\n              'n_clusters': n_clusters,\n              'min_samples': 3,\n              'xi': 0.05,\n              'min_cluster_size': 0.05}\n    \n    # estimate bandwidth for mean shift\n    bandwidth = cluster.estimate_bandwidth(Z, quantile=params['quantile'])\n\n    # connectivity matrix for structured Ward\n    connectivity = kneighbors_graph(\n        Z, n_neighbors=params['n_neighbors'], include_self=False)\n    \n    # make connectivity symmetric\n    connectivity = 0.5 * (connectivity + connectivity.T)\n\n    # ============\n    # Create cluster objects\n    # ============\n    if m == 'MeanShift':\n        cl = cluster.MeanShift(bandwidth=bandwidth, bin_seeding=True)\n    elif m == 'KMeans':\n        cl = cluster.KMeans(n_clusters=n_clusters, random_state = 1000)\n    elif m == 'MiniBatchKMeans':\n        cl = cluster.MiniBatchKMeans(n_clusters=n_clusters)\n    elif m == 'AgglomerativeClustering_ward':\n        cl = cluster.AgglomerativeClustering(n_clusters=n_clusters, linkage='ward',\n                                    connectivity=connectivity)\n    elif m == 'SpectralClustering':\n        cl = cluster.SpectralClustering(n_clusters=n_clusters, eigen_solver='arpack',\n                                        affinity=\"nearest_neighbors\")\n    elif m == 'DBSCAN':\n        cl = cluster.DBSCAN(eps=params['eps'])\n    elif m == 'OPTICS':\n        cl = cluster.OPTICS(min_samples=params['min_samples'],\n                            xi=params['xi'],\n                            min_cluster_size=params['min_cluster_size'])\n    elif m == 'AffinityPropagation':\n        cl = cluster.AffinityPropagation(damping=params['damping'])\n    elif m == 'AgglomerativeClustering_average':\n        cl = cluster.AgglomerativeClustering(linkage=\"average\", affinity=\"cityblock\",\n                    n_clusters=params['n_clusters'], connectivity=connectivity)\n    elif m == 'AgglomerativeClustering_complete':\n        cl = cluster.AgglomerativeClustering(linkage=\"complete\", affinity=\"cityblock\",\n                    n_clusters=params['n_clusters'], connectivity=connectivity)        \n    elif m == 'Birch':\n        cl = cluster.Birch(n_clusters=params['n_clusters'])\n    elif m == 'GaussianMixture':\n        cl = mixture.GaussianMixture(n_components=n_clusters, covariance_type='full')\n        \n    return cl","ce4a9d82":"def clustering_df(X, n, m, output_hist):\n    \n    # Standardization\n    X_columns = X.columns\n    scaler = StandardScaler()\n    scaler.fit(X)\n    X = pd.DataFrame(scaler.transform(X), columns = X_columns)\n    cl = generate_clustering_algorithms(X, n, m)\n    cl.fit(X)\n    if hasattr(cl, 'labels_'):\n        labels = cl.labels_.astype(np.int)\n    else:\n        labels = cl.predict(X) \n    clusters=pd.concat([X, pd.DataFrame({'cluster':labels})], axis=1)\n    \n    # Inverse Standardization\n    X_inv = pd.DataFrame(scaler.inverse_transform(X), columns = X_columns)    \n    clusters_inv=pd.concat([X_inv, pd.DataFrame({'cluster':labels})], axis=1)\n    \n    # Number of points in clusters\n    print(\"Number of points in clusters:\\n\", clusters['cluster'].value_counts())\n    \n    # Data in clusters - thanks to https:\/\/www.kaggle.com\/sabanasimbutt\/clustering-visualization-of-clusters-using-pca    \n    if output_hist:\n        for c in clusters:\n            grid = sns.FacetGrid(clusters_inv, col='cluster')\n            grid.map(plt.hist, c)\n        \n    return clusters, clusters_inv","d5493cc7":"# All 12 methods\nmethods_all = ['KMeans', 'MiniBatchKMeans', 'MeanShift', \n               'DBSCAN', 'OPTICS', \n               'AffinityPropagation',               \n               'AgglomerativeClustering_ward',\n               'AgglomerativeClustering_average',\n               'AgglomerativeClustering_complete',\n               'Birch', \n               'GaussianMixture',\n               'SpectralClustering'\n              ]","f1d3c714":"# The number of default clusters in methods where such a parameter is required\nn_default = 6","f6e82997":"def plot_draw(X, title, m):\n    # Drawing a plot with clusters on the plane (using PCA transformation)\n    # Thanks to https:\/\/www.kaggle.com\/sabanasimbutt\/clustering-visualization-of-clusters-using-pca\n    \n    dist = 1 - cosine_similarity(X)\n    \n    # PCA transform\n    pca = PCA(2)\n    pca.fit(dist)\n    X_PCA = pca.transform(dist)\n    \n    # Generate point numbers and colors for clusters\n    hsv = plt.get_cmap('hsv')\n    n_clusters = max(X['cluster'].value_counts().index)-min(X['cluster'].value_counts().index)+2\n    colors = list(hsv(np.linspace(0, 1, n_clusters)))\n    colors_num = list(np.linspace(min(X['cluster'].value_counts().index), max(X['cluster'].value_counts().index), n_clusters))\n    colors_num = [int(x) for x in colors_num]\n    colors_str = [str(x) for x in colors_num]\n    names_dict = dict(zip(colors_num, colors_str))\n    colors_dict = dict(zip(colors_num, colors))\n    \n    # Visualization\n    x, y = X_PCA[:, 0], X_PCA[:, 1]\n\n    df = pd.DataFrame({'x': x, 'y':y, 'label':X['cluster'].tolist()}) \n    groups = df.groupby('label')\n\n    fig, ax = plt.subplots(figsize=(12, 8)) \n\n    for name, group in groups:\n        ax.plot(group.x, group.y, marker='o', linestyle='', ms=10,\n                color=colors_dict[name],\n                label=names_dict[name], \n                mec='none')\n        ax.set_aspect('auto')\n        ax.tick_params(axis='x',which='both',bottom='off',top='off',labelbottom='off')\n        ax.tick_params(axis= 'y',which='both',left='off',top='off',labelleft='off')\n\n    ax.legend(loc='upper right')\n    ax.set_title(f\"{title} by method {m}\")\n    plt.show()","6ac4dc7f":"res = dict(zip(methods_all, [False]*len(methods_all)))\nn_clust = dict(zip(methods_all, [1]*len(methods_all)))\nfor method in methods_all:\n    print(f\"Method - {method}\")\n    Y, Y_inv = clustering_df(data.copy(), n_default, method, True)\n    \n    # If the number of clusters is less than 2, then the clustering is not successful\n    n_cl = len(Y['cluster'].value_counts())\n    if n_cl > 1:\n        res[method] = True\n        n_clust[method] = n_cl\n        plot_draw(Y, \"Data clustering\", method)\n    else:\n        print('Clustering is not successful because all data is in one cluster!\\n')","64e28a21":"# Results: optimal clustering methods\nmethods_bad = []\nprint('Optimal clustering methods:\\n')\nfor (k, v) in res.items():\n    if v:\n        print(f\"- {k} with number of clusters = {n_clust[k]}\")\n    else: \n        methods_bad.append(k)","a7a56dce":"# Results: methods in which all data are in one cluster\nif len(methods_bad) > 0:\n    print('Methods in which all data are in one cluster:\\n')\n    for method in methods_bad:\n        print(f'- {method}')","6b10e12e":"I hope you find this notebook useful and enjoyable.\n\nYour comments and feedback are most welcome.\n\n[Go to Top](#0)","2809f9b3":"## 1. Import libraries <a class=\"anchor\" id=\"1\"><\/a>\n\n[Back to Table of Contents](#0.1)","02ac2130":"## 2. Download datasets <a class=\"anchor\" id=\"2\"><\/a>\n\n[Back to Table of Contents](#0.1)","f50a373b":"## 5. Conclusion <a class=\"anchor\" id=\"5\"><\/a>\n\n[Back to Table of Contents](#0.1)","7dd15406":"<a class=\"anchor\" id=\"0\"><\/a>\n\n# The Multiple Clustering by 12 methods for data from the dataset [Heart Disease UCI data](https:\/\/www.kaggle.com\/ronitf\/heart-disease-uci):\n### Methods with automatic determination of the number of clusters:\n* MeanShift\n* DBSCAN\n* OPTICS\n* AffinityPropagation\n\n### Methods that require the number of clusters as an input parameter:\n* KMeans\n* MiniBatchKMeans\n* AgglomerativeClustering_ward\n* AgglomerativeClustering_average\n* AgglomerativeClustering_complete\n* Birch\n* GaussianMixture\n* SpectralClustering","4c4f73e4":"## 3. EDA & FE <a class=\"anchor\" id=\"3\"><\/a>\n\n[Back to Table of Contents](#0.1)","92b58ce2":"## Acknowledgements\n* [Heart Disease - Automatic AdvEDA & FE & 20 models](https:\/\/www.kaggle.com\/vbmokin\/heart-disease-automatic-adveda-fe-20-models)\n* [Titanic Top 3% : cluster analysis](https:\/\/www.kaggle.com\/vbmokin\/titanic-top-3-cluster-analysis)\n* [Clustering & Visualization of Clusters using PCA](https:\/\/www.kaggle.com\/sabanasimbutt\/clustering-visualization-of-clusters-using-pca)\n* https:\/\/scikit-learn.org\/stable\/auto_examples\/cluster\/plot_cluster_comparison.html#sphx-glr-auto-examples-cluster-plot-cluster-comparison-py","d62566ba":"## 4. Clustering <a class=\"anchor\" id=\"4\"><\/a>\n\n[Back to Table of Contents](#0.1)","71d884f4":"## Methods with automatic determination of the number of clusters:\n* MeanShift\n* DBSCAN\n* OPTICS\n* AffinityPropagation\n\n## Methods that require the number of clusters as an input parameter:\n* KMeans\n* MiniBatchKMeans\n* AgglomerativeClustering_ward\n* AgglomerativeClustering_average\n* AgglomerativeClustering_complete\n* Birch\n* GaussianMixture\n* SpectralClustering","d6941666":"<a class=\"anchor\" id=\"0.1\"><\/a>\n\n## Table of Contents\n\n1. [Import libraries](#1)\n1. [Download datasets](#2)\n1. [EDA & FE](#3)\n1. [Clustering](#4)\n1. [Conclusion](#5)"}}