{"cell_type":{"cf139e66":"code","f0776f4b":"code","8a8cf512":"code","a697c3f7":"code","6c30fcb0":"code","86f00be2":"code","c6b56299":"code","1727d0df":"code","f0e77e27":"code","957aa565":"code","d80bcbc2":"code","3f95f6ad":"code","2af715bf":"code","470b6374":"code","c32b62f0":"code","8311e2b2":"code","30700fec":"code","0aec8267":"code","063fddd7":"code","37e160a9":"code","eeebe36a":"code","398816be":"code","a3d40f43":"code","ef515b93":"code","b3e0fa07":"code","83714e6a":"code","28f23b91":"code","7f62eace":"code","37993595":"code","a8c89482":"code","014a191b":"code","e836a560":"markdown","b5a0fafe":"markdown","fe11f9c4":"markdown","079fa988":"markdown","92ea9c12":"markdown"},"source":{"cf139e66":"import numpy as np\nimport pandas as pd\n\nfile_path_train = \"..\/input\/home-data-for-ml-course\/train.csv\"\nfile_path_test = \"..\/input\/home-data-for-ml-course\/test.csv\"\n\ntrain_data = pd.read_csv(file_path_train, index_col = \"Id\")\ntest_data = pd.read_csv(file_path_test, index_col = \"Id\")\n\nprint(train_data.shape)\ntrain_data.head()\n","f0776f4b":"train_data.isnull().sum().sort_values(ascending = False).head(11)\n\ntrain_data.isnull().sum().sort_values(ascending = False).head(11).index","8a8cf512":"drop_cols = [\"PoolQC\",\"MiscFeature\",\"Alley\",\"Fence\",\"FireplaceQu\",\"LotFrontage\",\"GarageYrBlt\",\n             \"GarageCond\",\"GarageType\",\"GarageFinish\",\"GarageQual\"]","a697c3f7":"train_data= train_data.drop(drop_cols,axis= 1)\n\ntrain_data.isnull().sum().sort_values(ascending = False).head(11)","6c30fcb0":"train_data= train_data.dropna(axis=0, how='any')\n\ntrain_data.isnull().sum().sort_values(ascending = False).head(11)","86f00be2":"X_full = train_data.dropna(axis=0, subset= [\"SalePrice\"])\nprint(X_full.shape)\n\ny = X_full.SalePrice\nX_full = X_full.drop([\"SalePrice\"], axis = 1)\n\nX_full","c6b56299":"X_full = train_data.dropna(axis=0, subset= [\"SalePrice\"])\nprint(X_full.shape)\n\nX_full_pca = X_full.copy()\n\nnumerical_cols = [cname for cname in X_full_pca.columns\n                 if X_full_pca[cname].dtype in [\"int64\",\"float64\"]]\n\nX_scaled = X_full_pca[numerical_cols]\n\ny = X_scaled.SalePrice\n\nX_scaled = (X_scaled - X_scaled.mean(axis=0)) \/ X_scaled.std(axis=0)\n\nX_scaled.head()","1727d0df":"from sklearn.decomposition import PCA\n\n# Create principal components\npca = PCA()\nX_pca = pca.fit_transform(X_scaled)\n\n# Convert to dataframe\ncomponent_names = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\nX_pca = pd.DataFrame(X_pca, columns=component_names)\n\nX_pca.head()","f0e77e27":"loadings = pd.DataFrame(\n    pca.components_.T,  # transpose the matrix of loadings\n    columns=component_names,  # so the columns are the principal components\n    index=X_scaled.columns,  # and the rows are the original features\n)\nloadings","957aa565":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom IPython.display import display\nfrom sklearn.feature_selection import mutual_info_regression\n\n\nplt.style.use(\"seaborn-whitegrid\")\nplt.rc(\"figure\", autolayout=True)\nplt.rc(\n    \"axes\",\n    labelweight=\"bold\",\n    labelsize=\"large\",\n    titleweight=\"bold\",\n    titlesize=14,\n    titlepad=10,\n)\n\n\ndef plot_variance(pca, width=8, dpi=100):\n    # Create figure\n    fig, axs = plt.subplots(1, 2)\n    n = pca.n_components_\n    grid = np.arange(1, n + 1)\n    # Explained variance\n    evr = pca.explained_variance_ratio_\n    axs[0].bar(grid, evr)\n    axs[0].set(\n        xlabel=\"Component\", title=\"% Explained Variance\", ylim=(0.0, 1.0)\n    )\n    # Cumulative Variance\n    cv = np.cumsum(evr)\n    axs[1].plot(np.r_[0, grid], np.r_[0, cv], \"o-\")\n    axs[1].set(\n        xlabel=\"Component\", title=\"% Cumulative Variance\", ylim=(0.0, 1.0)\n    )\n    # Set up figure\n    fig.set(figwidth=8, dpi=100)\n    return axs\n\ndef make_mi_scores(X, y, discrete_features):\n    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores","d80bcbc2":"# Look at explained variance\nplot_variance(pca);","3f95f6ad":"mi_scores = make_mi_scores(X_pca, y, discrete_features=False)\nmi_scores","2af715bf":"X_full = train_data.dropna(axis=0, subset= [\"SalePrice\"])\nprint(X_full.shape)\n\ny = X_full.SalePrice\nX_full = X_full.drop([\"SalePrice\"], axis = 1)\n\nX_full","470b6374":"from sklearn.feature_selection import mutual_info_regression\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\n# Utility functions from Tutorial\ndef make_mi_scores(X, y):\n    X = X.copy()\n    for colname in X.select_dtypes([\"object\", \"category\"]):\n        X[colname], _ = X[colname].factorize()\n    # All discrete features should now have integer dtypes\n    discrete_features = [pd.api.types.is_integer_dtype(t) for t in X.dtypes]\n    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features, random_state=0)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores\n\n\ndef plot_mi_scores(scores):\n    scores = scores.sort_values(ascending=True)\n    width = np.arange(len(scores))\n    ticks = list(scores.index)\n    plt.barh(width, scores)\n    plt.yticks(width, ticks)\n    plt.title(\"Mutual Information Scores\")","c32b62f0":"mi_scores = make_mi_scores(X_full, y)\n\nprint(mi_scores.head(20))","8311e2b2":"plot_mi_scores(mi_scores.head(20))","30700fec":"mi_scores.head(20).index","0aec8267":"df = X_full.copy()\ndf[\"feature_1\"] =  df.KitchenQual + df.ExterQual + df.BsmtQual\ndf[\"feature_2\"] =  df.GrLivArea*df.OverallQual \/ df.TotRmsAbvGrd\ndf[\"feature_3\"] =  df.OverallQual * df.MoSold\ndf[\"feature_4\"] =  df.Street + df.Neighborhood","063fddd7":"df.info()","37e160a9":"test_data[\"feature_1\"] =  test_data.KitchenQual + test_data.ExterQual + test_data.BsmtQual\ntest_data[\"feature_2\"] =  test_data.GrLivArea*test_data.OverallQual \/ test_data.TotRmsAbvGrd\ntest_data[\"feature_3\"] =  test_data.OverallQual * test_data.MoSold\ntest_data[\"feature_4\"] =  test_data.Street + test_data.Neighborhood","eeebe36a":"mi_scores = make_mi_scores(df, y)\n\nprint(mi_scores.head(60))\n\n#mi_scores","398816be":"mi_scores.head(20).index","a3d40f43":"numerical_cols = [cname for cname in df.columns\n                 if df[cname].dtype in [\"int64\",\"float64\"]]\n#numerical_cols\nprint(\"Length of numerical column : \",len(numerical_cols))\n\ncategorical_cols = [cname for cname in df.columns \n                    if df[cname].nunique() <= 36 and\n                    df[cname].dtypes==\"object\"\n                   ]\ncategorical_cols\nprint(\"Length of categorical column : \",len(categorical_cols))\n\nmy_col = numerical_cols + categorical_cols\nX = df[my_col]\n\nprint(\"X : \", X.shape)\n\nX_test = test_data[my_col]\nprint(\"X_test : \", X.shape)","ef515b93":"# Scaling\n\n# from sklearn.preprocessing import MinMaxScaler\n# scale = MinMaxScaler()\n# X_scaled = X.copy()\n# X_scaled[numerical_cols]= scale.fit_transform(X_scaled[numerical_cols])\n# X_scaled\n\n# X_test_scaled = X_test.copy()\n# X_test_scaled[numerical_cols]= scale.fit_transform(X_test_scaled[numerical_cols])","b3e0fa07":"from sklearn.model_selection import train_test_split\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size= 0.2, random_state = 0)","83714e6a":"from sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error","28f23b91":"numerical_transformer = SimpleImputer(strategy = \"constant\")\n\ncategorical_transformer = Pipeline(steps=[\n    (\"impute\", SimpleImputer (strategy = \"constant\")),\n    (\"onehot\",OneHotEncoder(handle_unknown = \"ignore\"))\n])\n\n\npreprocessor = ColumnTransformer(transformers=[\n    (\"num\", numerical_transformer, numerical_cols),\n    (\"cat\", categorical_transformer, categorical_cols)\n])","7f62eace":"best_model = RandomForestRegressor(random_state=0)\n\nmy_pipeline = Pipeline(steps=[\n        (\"preprocessor\",preprocessor),\n        (\"model\",best_model)\n    ])","37993595":"my_pipeline.fit(X_train,y_train)\n    \ny_pred = my_pipeline.predict(X_valid)\n\nscore = mean_absolute_error(y_valid, y_pred)\n\nprint(score)","a8c89482":"my_pipeline.fit(X,y)\n\ny_pred_final = my_pipeline.predict(X_test)","014a191b":"# Save test predictions to file\noutput = pd.DataFrame({'Id': X_test.index,\n                       'SalePrice': y_pred_final})\noutput.to_csv('submission.csv', index=False)","e836a560":"# Creating New features","b5a0fafe":"# Dealing with NAN and Droping","fe11f9c4":"# Model Building and check the MAE Score","079fa988":"# Principal Component Analysis","92ea9c12":"# MI_Score"}}