{"cell_type":{"c54e650e":"code","628d0885":"code","171ca70a":"code","8bb99b5f":"code","9b1880dc":"code","18333052":"code","a4e2710f":"code","cb7c67e9":"code","59276b46":"code","64b82172":"code","6ab1be85":"code","03f8240d":"code","7ac41b41":"code","c5937e47":"code","75bd09d2":"code","ae3cc5db":"code","5c001d04":"code","34a506b0":"code","874e57f2":"code","716c577a":"code","7b778c2a":"code","95094f4b":"code","93b89c08":"code","9266212a":"code","e908f977":"code","e3f83cef":"code","3e6fdecc":"code","148fa5e7":"code","4627f193":"code","4f0799cf":"code","9dcdf679":"code","a3158117":"code","792b74a2":"code","4ebe6891":"code","a9981cb4":"code","3f24392a":"code","e67899de":"code","add9f010":"code","7e7d9a0c":"code","f5daad22":"code","26a9461a":"markdown","c56cd53b":"markdown","075e5bd3":"markdown","863d6ef9":"markdown","2d77028b":"markdown","5a0a017b":"markdown","1fbe3b6a":"markdown","dc0401c5":"markdown","eaaca653":"markdown","a0b02719":"markdown","a2b9d462":"markdown","75b6fd1d":"markdown","55914351":"markdown","770c1668":"markdown","5657b2ad":"markdown","57bbfc4f":"markdown","6acc6861":"markdown"},"source":{"c54e650e":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\nfrom sklearn.linear_model import Ridge\n\nfrom sklearn import preprocessing\nimport warnings\nimport datetime\nwarnings.filterwarnings(\"ignore\")\nimport gc\n\nfrom scipy.stats import describe\n%matplotlib inline\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\n# Any results you write to the current directory are saved as output.","628d0885":"import os\nprint(os.listdir(\"..\/input\"))","171ca70a":"%%time\n#Loading Train and Test Data\ntrain = pd.read_csv(\"..\/input\/train.csv\", parse_dates=[\"first_active_month\"])\ntest = pd.read_csv(\"..\/input\/test.csv\", parse_dates=[\"first_active_month\"])\nprint(\"{} observations and {} features in train set.\".format(train.shape[0],train.shape[1]))\nprint(\"{} observations and {} features in test set.\".format(test.shape[0],test.shape[1]))\ngc.collect()","8bb99b5f":"%time train.head()","9b1880dc":"%time test.head()","18333052":"%time train.target.describe()","a4e2710f":"%%time\nplt.figure(figsize=(12, 5))\nplt.hist(train.target.values, bins=200)\nplt.title('Histogram target counts')\nplt.xlabel('Count')\nplt.ylabel('Target')\nplt.show()\ngc.collect()","cb7c67e9":"%%time\nsns.set_style(\"whitegrid\")\nax = sns.violinplot(x=train.target.values)\nplt.show()\ngc.collect()","59276b46":"%%time\nplt.figure(figsize=(12, 5))\nplt.hist(train.feature_1.values, bins=50, color = 'orange')\nplt.title('Histogram feature_1 counts')\nplt.xlabel('Count')\nplt.ylabel('Target')\nplt.show()\ngc.collect()","64b82172":"plt.figure(figsize=(12, 5))\nplt.hist(train.feature_2.values, bins=50, color=\"green\")\nplt.title('Histogram feature_2 counts')\nplt.xlabel('Count')\nplt.ylabel('Target')\nplt.show()","6ab1be85":"plt.figure(figsize=(12, 5))\nplt.hist(train.feature_3.values, bins=50, color = \"red\")\nplt.title('Histogram feature_3 counts')\nplt.xlabel('Count')\nplt.ylabel('Target')\nplt.show()","03f8240d":"%%time\ntrain[\"month\"] = train[\"first_active_month\"].dt.month\ntest[\"month\"] = test[\"first_active_month\"].dt.month\ntrain[\"year\"] = train[\"first_active_month\"].dt.year\ntest[\"year\"] = test[\"first_active_month\"].dt.year\ntrain['elapsed_time'] = (datetime.date(2018, 2, 1) - train['first_active_month'].dt.date).dt.days\ntest['elapsed_time'] = (datetime.date(2018, 2, 1) - test['first_active_month'].dt.date).dt.days\ntrain.head()\ngc.collect()","7ac41b41":"%%time\ntrain = pd.get_dummies(train, columns=['feature_1', 'feature_2'])\ntest = pd.get_dummies(test, columns=['feature_1', 'feature_2'])\ntrain.head()\ngc.collect()","c5937e47":"%%time\nhist_trans = pd.read_csv(\"..\/input\/historical_transactions.csv\")\nhist_trans.head()\ngc.collect()","75bd09d2":"%%time\nhist_trans = pd.get_dummies(hist_trans, columns=['category_2', 'category_3'])\nhist_trans['authorized_flag'] = hist_trans['authorized_flag'].map({'Y': 1, 'N': 0})\nhist_trans['category_1'] = hist_trans['category_1'].map({'Y': 1, 'N': 0})\nhist_trans.head()\ngc.collect()","ae3cc5db":"%%time\ndef aggregate_transactions(trans, prefix):  \n    trans.loc[:, 'purchase_date'] = pd.DatetimeIndex(trans['purchase_date']).\\\n                                      astype(np.int64) * 1e-9\n    \n    agg_func = {\n        'authorized_flag': ['sum', 'mean'],\n        'category_1': ['mean'],\n        'category_2_1.0': ['mean'],\n        'category_2_2.0': ['mean'],\n        'category_2_3.0': ['mean'],\n        'category_2_4.0': ['mean'],\n        'category_2_5.0': ['mean'],\n        'category_3_A': ['mean'],\n        'category_3_B': ['mean'],\n        'category_3_C': ['mean'],\n        'merchant_id': ['nunique'],\n        'purchase_amount': ['sum', 'mean', 'max', 'min', 'std'],\n        'installments': ['sum', 'mean', 'max', 'min', 'std'],\n        'purchase_date': [np.ptp],\n        'month_lag': ['min', 'max']\n    }\n    agg_trans = trans.groupby(['card_id']).agg(agg_func)\n    agg_trans.columns = [prefix + '_'.join(col).strip() \n                           for col in agg_trans.columns.values]\n    agg_trans.reset_index(inplace=True)\n    \n    df = (trans.groupby('card_id')\n          .size()\n          .reset_index(name='{}transactions_count'.format(prefix)))\n    \n    agg_trans = pd.merge(df, agg_trans, on='card_id', how='left')\n    \n    return agg_trans\ngc.collect()","5c001d04":"%%time\nimport gc\nmerch_hist = aggregate_transactions(hist_trans, prefix='hist_')\ndel hist_trans\ngc.collect()\ntrain = pd.merge(train, merch_hist, on='card_id',how='left')\ntest = pd.merge(test, merch_hist, on='card_id',how='left')\ndel merch_hist\ngc.collect()\ntrain.head()","34a506b0":"%%time\nnew_trans = pd.read_csv(\"..\/input\/new_merchant_transactions.csv\")\nnew_trans.head()\ngc.collect()","874e57f2":"%%time\nnew_trans = pd.get_dummies(new_trans, columns=['category_2', 'category_3'])\nnew_trans['authorized_flag'] = new_trans['authorized_flag'].map({'Y': 1, 'N': 0})\nnew_trans['category_1'] = new_trans['category_1'].map({'Y': 1, 'N': 0})\nnew_trans.head()\ngc.collect()","716c577a":"%%time\nmerch_new = aggregate_transactions(new_trans, prefix='new_')\ndel new_trans\ngc.collect()\ntrain = pd.merge(train, merch_new, on='card_id',how='left')\ntest = pd.merge(test, merch_new, on='card_id',how='left')\ndel merch_new\ngc.collect()\ntrain.head()","7b778c2a":"%%time\ntarget = train['target']\ndrops = ['card_id', 'first_active_month', 'target']\nuse_cols = [c for c in train.columns if c not in drops]\nfeatures = list(train[use_cols].columns)\ntrain[features].head()\ngc.collect()","95094f4b":"print(train[features].shape)\nprint(test[features].shape)\ngc.collect()","93b89c08":"%%time\ntrain[features+['target']].to_csv('new_train.csv', index=False)\ntest[features].to_csv('new_test.csv', index=False)\ngc.collect()","9266212a":"%%time\nfolds = KFold(n_splits=7, shuffle=True, random_state=15)\noof_ridge = np.zeros(train.shape[0])\npredictions_ridge = np.zeros(test.shape[0])\n\ntst_data = test.copy()\ntst_data.fillna((tst_data.mean()), inplace=True)\n\ntst_data = tst_data[features].values\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train, target)):\n    print(\"fold n\u00b0{}\".format(fold_+1))\n    trn_data, trn_y = train.iloc[trn_idx][features], target.iloc[trn_idx].values\n    val_data, val_y = train.iloc[val_idx][features], target.iloc[val_idx].values\n    \n    trn_data.fillna((trn_data.mean()), inplace=True)\n    val_data.fillna((val_data.mean()), inplace=True)\n    \n    trn_data = trn_data.values\n    val_data = val_data.values\n\n    clf = Ridge(alpha=100)\n    clf.fit(trn_data, trn_y)\n    \n    oof_ridge[val_idx] = clf.predict(val_data)\n    predictions_ridge += clf.predict(tst_data) \/ folds.n_splits\n\nnp.save('oof_ridge', oof_ridge)\nnp.save('predictions_ridge', predictions_ridge)\nnp.sqrt(mean_squared_error(target.values, oof_ridge))\ngc.collect()","e908f977":"%%time\nparam = {'num_leaves': 50,\n         'min_data_in_leaf': 30, \n         'objective':'regression',\n         'max_depth': -1,\n         'learning_rate': 0.005,\n         \"boosting\": \"gbdt\",\n         \"feature_fraction\": 0.9,\n         \"bagging_freq\": 1,\n         \"bagging_fraction\": 0.9,\n         \"bagging_seed\": 11,\n         \"metric\": 'rmse',\n         \"lambda_l1\": 0.1,\n         \"verbosity\": -1}\n\nfolds = KFold(n_splits=5, shuffle=True, random_state=15)\noof_lgb = np.zeros(len(train))\npredictions_lgb = np.zeros(len(test))\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train.values, target.values)):\n    print('-')\n    print(\"Fold {}\".format(fold_ + 1))\n    trn_data = lgb.Dataset(train.iloc[trn_idx][features], label=target.iloc[trn_idx])\n    val_data = lgb.Dataset(train.iloc[val_idx][features], label=target.iloc[val_idx])\n\n    num_round = 10000\n    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=100, early_stopping_rounds=100)\n    oof_lgb[val_idx] = clf.predict(train.iloc[val_idx][features], num_iteration=clf.best_iteration)\n    predictions_lgb += clf.predict(test[features], num_iteration=clf.best_iteration) \/ folds.n_splits\n    \nnp.save('oof_lgb', oof_lgb)\nnp.save('predictions_lgb', predictions_lgb)\nnp.sqrt(mean_squared_error(target.values, oof_lgb))\ngc.collect()","e3f83cef":"%%time\nxgb_params = {'eta': 0.005, 'max_depth': 10, 'subsample': 0.8, 'colsample_bytree': 0.8, \n          'objective': 'reg:linear', 'eval_metric': 'rmse', 'silent': True}\n\nfolds = KFold(n_splits=5, shuffle=True, random_state=15)\noof_xgb = np.zeros(len(train))\npredictions_xgb = np.zeros(len(test))\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train.values, target.values)):\n    print('-')\n    print(\"Fold {}\".format(fold_ + 1))\n    trn_data = xgb.DMatrix(data=train.iloc[trn_idx][features], label=target.iloc[trn_idx])\n    val_data = xgb.DMatrix(data=train.iloc[val_idx][features], label=target.iloc[val_idx])\n    watchlist = [(trn_data, 'train'), (val_data, 'valid')]\n    print(\"xgb \" + str(fold_) + \"-\" * 50)\n    num_round = 10000\n    xgb_model = xgb.train(xgb_params, trn_data, num_round, watchlist, early_stopping_rounds=50, verbose_eval=1000)\n    oof_xgb[val_idx] = xgb_model.predict(xgb.DMatrix(train.iloc[val_idx][features]), ntree_limit=xgb_model.best_ntree_limit+50)\n\n    predictions_xgb += xgb_model.predict(xgb.DMatrix(test[features]), ntree_limit=xgb_model.best_ntree_limit+50) \/ folds.n_splits\n    \nnp.save('oof_xgb', oof_xgb)\nnp.save('predictions_xgb', predictions_xgb)\nnp.sqrt(mean_squared_error(target.values, oof_xgb))\ngc.collect()","3e6fdecc":"%%time\ndel train, test\ngc.collect()","148fa5e7":"new_transactions = pd.read_csv('..\/input\/new_merchant_transactions.csv', parse_dates=['purchase_date'])\nhistorical_transactions = pd.read_csv('..\/input\/historical_transactions.csv', parse_dates=['purchase_date'])\n\ndef binarize(df):\n    for col in ['authorized_flag', 'category_1']:\n        df[col] = df[col].map({'Y':1, 'N':0})\n    return df\n\nhistorical_transactions = binarize(historical_transactions)\nnew_transactions = binarize(new_transactions)","4627f193":"def read_data(input_file):\n    df = pd.read_csv(input_file)\n    df['first_active_month'] = pd.to_datetime(df['first_active_month'])\n    df['elapsed_time'] = (datetime.date(2018, 2, 1) - df['first_active_month'].dt.date).dt.days\n    return df\n#_________________________________________\ntrain = read_data('..\/input\/train.csv')\ntest = read_data('..\/input\/test.csv')\n\ntarget = train['target']\ndel train['target']","4f0799cf":"historical_transactions = pd.get_dummies(historical_transactions, columns=['category_2', 'category_3'])\nnew_transactions = pd.get_dummies(new_transactions, columns=['category_2', 'category_3'])\n\nhistorical_transactions = reduce_mem_usage(historical_transactions)\nnew_transactions = reduce_mem_usage(new_transactions)","9dcdf679":"historical_transactions['purchase_month'] = historical_transactions['purchase_date'].dt.month\nnew_transactions['purchase_month'] = new_transactions['purchase_date'].dt.month\n\ndef aggregate_transactions(history):\n    \n    history.loc[:, 'purchase_date'] = pd.DatetimeIndex(history['purchase_date']).\\\n                                      astype(np.int64) * 1e-9\n    \n    agg_func = {\n        'authorized_flag': ['sum', 'mean'],\n        'category_1': ['sum', 'mean'],\n        'category_2_1.0': ['mean'],\n        'category_2_2.0': ['mean'],\n        'category_2_3.0': ['mean'],\n        'category_2_4.0': ['mean'],\n        'category_2_5.0': ['mean'],\n        'category_3_A': ['mean'],\n        'category_3_B': ['mean'],\n        'category_3_C': ['mean'],\n        'merchant_id': ['nunique'],\n        'merchant_category_id': ['nunique'],\n        'state_id': ['nunique'],\n        'city_id': ['nunique'],\n        'subsector_id': ['nunique'],\n        'purchase_amount': ['sum', 'mean', 'max', 'min', 'std'],\n        'installments': ['sum', 'mean', 'max', 'min', 'std'],\n        'purchase_month': ['mean', 'max', 'min', 'std'],\n        'purchase_date': [np.ptp],\n        'month_lag': ['min', 'max']\n        }\n    \n    agg_history = history.groupby(['card_id']).agg(agg_func)\n    agg_history.columns = ['_'.join(col).strip() for col in agg_history.columns.values]\n    agg_history.reset_index(inplace=True)\n    \n    df = (history.groupby('card_id')\n          .size()\n          .reset_index(name='transactions_count'))\n    \n    agg_history = pd.merge(df, agg_history, on='card_id', how='left')\n    \n    return agg_history\n","a3158117":"history = aggregate_transactions(historical_transactions)\nhistory.columns = ['hist_' + c if c != 'card_id' else c for c in history.columns]\nhistory[:5]","792b74a2":"new = aggregate_transactions(new_transactions)\nnew.columns = ['new_' + c if c != 'card_id' else c for c in new.columns]\nnew[:5]","4ebe6891":"def aggregate_per_month(history):\n    grouped = history.groupby(['card_id', 'month_lag'])\n\n    agg_func = {\n            'purchase_amount': ['count', 'sum', 'mean', 'min', 'max', 'std'],\n            'installments': ['count', 'sum', 'mean', 'min', 'max', 'std'],\n            }\n\n    intermediate_group = grouped.agg(agg_func)\n    intermediate_group.columns = ['_'.join(col).strip() for col in intermediate_group.columns.values]\n    intermediate_group.reset_index(inplace=True)\n\n    final_group = intermediate_group.groupby('card_id').agg(['mean', 'std'])\n    final_group.columns = ['_'.join(col).strip() for col in final_group.columns.values]\n    final_group.reset_index(inplace=True)\n    \n    return final_group\n#___________________________________________________________\nfinal_group =  aggregate_per_month(historical_transactions) \nfinal_group[:10]","a9981cb4":"train = pd.merge(train, history, on='card_id', how='left')\ntest = pd.merge(test, history, on='card_id', how='left')\n\ntrain = pd.merge(train, new, on='card_id', how='left')\ntest = pd.merge(test, new, on='card_id', how='left')\n\ntrain = pd.merge(train, final_group, on='card_id')\ntest = pd.merge(test, final_group, on='card_id')\n\nfeatures = [c for c in train.columns if c not in ['card_id', 'first_active_month']]\ncategorical_feats = [c for c in features if 'feature_' in c]","3f24392a":"train[features+['target']].to_csv('new_train_2.csv', index=False)\ntest[features].to_csv('new_test_2.csv', index=False)","e67899de":"param = {'num_leaves': 50,\n         'min_data_in_leaf': 30, \n         'objective':'regression',\n         'max_depth': -1,\n         'learning_rate': 0.005,\n         \"min_child_samples\": 20,\n         \"boosting\": \"gbdt\",\n         \"feature_fraction\": 0.9,\n         \"bagging_freq\": 1,\n         \"bagging_fraction\": 0.9 ,\n         \"bagging_seed\": 11,\n         \"metric\": 'rmse',\n         \"lambda_l1\": 0.1,\n         \"verbosity\": -1}\n\nfolds = KFold(n_splits=5, shuffle=True, random_state=15)\noof_lgb_2 = np.zeros(len(train))\npredictions_lgb_2 = np.zeros(len(test))\nstart = time.time()\nfeature_importance_df = pd.DataFrame()\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train.values, target.values)):\n    print(\"fold n\u00b0{}\".format(fold_))\n    trn_data = lgb.Dataset(train.iloc[trn_idx][features], label=target.iloc[trn_idx], categorical_feature=categorical_feats)\n    val_data = lgb.Dataset(train.iloc[val_idx][features], label=target.iloc[val_idx], categorical_feature=categorical_feats)\n\n    num_round = 10000\n    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=100, early_stopping_rounds = 200)\n    oof_lgb_2[val_idx] = clf.predict(train.iloc[val_idx][features], num_iteration=clf.best_iteration)\n    \n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"feature\"] = features\n    fold_importance_df[\"importance\"] = clf.feature_importance()\n    fold_importance_df[\"fold\"] = fold_ + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    \n    predictions_lgb_2 += clf.predict(test[features], num_iteration=clf.best_iteration) \/ folds.n_splits\n\nnp.save('oof_lgb_2', oof_lgb_2)\nnp.save('predictions_lgb_2', predictions_lgb_2)\nprint(\"CV score: {:<8.5f}\".format(mean_squared_error(oof_lgb_2, target)**0.5))","add9f010":"'''xgb_params = {'eta': 0.005, 'max_depth': 10, 'subsample': 0.8, 'colsample_bytree': 0.8, \n          'objective': 'reg:linear', 'eval_metric': 'rmse', 'silent': True}\n\nfolds = KFold(n_splits=5, shuffle=True, random_state=15)\noof_xgb_2 = np.zeros(len(train))\npredictions_xgb_2 = np.zeros(len(test))\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train.values, target.values)):\n    print('-')\n    print(\"Fold {}\".format(fold_ + 1))\n    trn_data = xgb.DMatrix(data=train.iloc[trn_idx][features], label=target.iloc[trn_idx])\n    val_data = xgb.DMatrix(data=train.iloc[val_idx][features], label=target.iloc[val_idx])\n    watchlist = [(trn_data, 'train'), (val_data, 'valid')]\n    print(\"xgb \" + str(fold_) + \"-\" * 50)\n    num_round = 10000\n    xgb_model = xgb.train(xgb_params, trn_data, num_round, watchlist, early_stopping_rounds=50, verbose_eval=1000)\n    oof_xgb_2[val_idx] = xgb_model.predict(xgb.DMatrix(train.iloc[val_idx][features]), ntree_limit=xgb_model.best_ntree_limit+50)\n\n    predictions_xgb_2 += xgb_model.predict(xgb.DMatrix(test[features]), ntree_limit=xgb_model.best_ntree_limit+50) \/ folds.n_splits\n    \nnp.save('oof_xgb_2', oof_xgb_2)\nnp.save('predictions_xgb_2', predictions_xgb_2)\nnp.sqrt(mean_squared_error(target.values, oof_xgb_2))'''","7e7d9a0c":"train_stack = np.vstack([oof_ridge, oof_lgb, oof_xgb, oof_lgb_2]).transpose()\ntest_stack = np.vstack([predictions_ridge, predictions_lgb, predictions_xgb, \n                        predictions_lgb_2]).transpose()\n\nfolds = KFold(n_splits=5, shuffle=True, random_state=15)\noof = np.zeros(train_stack.shape[0])\npredictions = np.zeros(test_stack.shape[0])\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train_stack, target)):\n    print(\"fold n\u00b0{}\".format(fold_))\n    trn_data, trn_y = train_stack[trn_idx], target.iloc[trn_idx].values\n    val_data, val_y = train_stack[val_idx], target.iloc[val_idx].values\n\n    clf = Ridge(alpha=1)\n    clf.fit(trn_data, trn_y)\n    \n    oof[val_idx] = clf.predict(val_data)\n    predictions += clf.predict(test_stack) \/ folds.n_splits\n\n\nnp.sqrt(mean_squared_error(target.values, oof))","f5daad22":"sample_submission = pd.read_csv('..\/input\/sample_submission.csv')\nsample_submission['target'] = predictions\nsample_submission.to_csv('stacker_2.csv', index=False)","26a9461a":"Here is a gratuitous embedding of YouTube video of 'The Girl From Ipanema'. For no good reason.","c56cd53b":"Finally, we'll stack them all together:","075e5bd3":"Yup, there is that little bump on the far left again.","863d6ef9":"Seems like a pretty nice normal-looking distribution, except for the few anomalous elements at teh far left. They will have to be dealt with separately.\n\nLet's look at the \"violin\" version of the same plot. ","2d77028b":"Let's now look at the distributions of various \"features\"","5a0a017b":"# Feature Engineering","1fbe3b6a":"Seems farily straightforward - just ID, first active months, three anonimous features, and target firld for train set.\n\nLet's take a look at the target variable:\n","dc0401c5":"A great thing about stackign is that you can not only use different set of models, but also create the same models with a different set of features. Here we'll use features from the wonderful [Elo world kernel](https:\/\/www.kaggle.com\/fabiendaniel\/elo-world) by Fabien Daniel:","eaaca653":"## Overview\n\nThe purpose of this kernel is to take a look at the data, come up with some insights, and attempt to create a predictive model or two. This notebook is still **very** raw. I will work on it as my very limited time permits, and hope to expend it in the upcoming days and weeks.\n\nNB: Most of the feature engineering and some of the modeling is based on [Peter Hurford's excellent kernel.](https:\/\/www.kaggle.com\/peterhurford\/you-re-going-to-want-more-categories-lb-3-737\/notebook) \n\nInspired From : https:\/\/www.kaggle.com\/tunguz\/eloda-with-feature-engineering-and-stacking\n\n## Packages\n\nFirst, let's load a few useful Python packages. This section will keep growing in subsequent versions of this EDA.","a0b02719":"Seems like a very wide range of values, relatively spaking. Let's take a look at the graph of the distribution:","a2b9d462":"Now soem XGBoost:","75b6fd1d":"3.83 CV is not bad, but it's far from what the best models can do in this competition. Let's take a look at a few non-linear models. We'll start with LightGBM.","55914351":"# Modeling\n\nNow let's do some of what everyone is here for - modeling. We'll start with a simple Ridge regression model. ","770c1668":"Let's see what files we have in the input directory:","5657b2ad":"For now I am not including plots for the test set, as they at first approsimation look very similar.\n\nA couple of things that stand out are:\n\n1. There are only a handful of values for each of the three features.\n2. They are discrete\n3. They are relatively eavenly distributed.\n\nAll of this suggests that these features are categorical and have been label-encoded. ","57bbfc4f":"We see that in addition to the usual,`train`, `test` and `sample_submission` files, we also have `merchants`, `historical_transactions`, `new_merchant_transactions`, and even one (**HORROR!!!**) excel file - `Data_Ditionary`. The names of the files are pretty self-explanatory, but we'll take a look at them and explore them. First, let's look at the `train` and `test` files.","6acc6861":"# Second Set of Features and Models"}}