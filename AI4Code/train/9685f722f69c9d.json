{"cell_type":{"ec17198f":"code","e2797719":"code","f182396f":"code","73b234e0":"code","661f3a49":"code","606821ff":"code","717472d5":"code","2dbf6fbe":"code","3c0474a7":"code","8dd14319":"code","8128fafb":"code","be19e591":"code","e98e2321":"code","4ad09f0b":"code","b79a483f":"code","80ed70e2":"code","07e7335d":"code","e4801d8c":"code","42986de6":"code","381f1ad6":"code","1ac8c16e":"code","3ae93aa3":"code","22952782":"code","b9b26c98":"code","6e9cda5a":"code","d21861d1":"code","3e85f745":"code","67466ab6":"code","bfb14eaf":"code","6c6187f4":"code","b159d077":"code","579ef033":"code","8b724e92":"code","e19494cb":"code","008a73c2":"code","73fa6db1":"code","7224f508":"code","b0f0a4b3":"code","fb779a6e":"code","c82fdadb":"code","267527c1":"code","220a8f14":"code","3873e997":"code","cc88b267":"code","e2dd890b":"code","1d3f272b":"code","4855c845":"code","b79253ae":"code","fe280265":"code","5ec64864":"code","7cfb669a":"markdown","0a5c48f7":"markdown","5070be49":"markdown","8c592a4f":"markdown","e9ca0d94":"markdown","32af247a":"markdown","c7c7f153":"markdown","6076c42c":"markdown","b675e4c3":"markdown","2338a221":"markdown","857ad1ba":"markdown","658692fe":"markdown","b3d1d083":"markdown","e06e824d":"markdown","ecf424ae":"markdown","adfa2f64":"markdown","985a69e7":"markdown","c3cfd70c":"markdown","21cc4fa9":"markdown"},"source":{"ec17198f":"# Ignore Warnings\nimport warnings\nwarnings.filterwarnings(action=\"ignore\", module=\"scipy\", message=\"^internal gelsd\")","e2797719":"# Clear memory -  https:\/\/ipython.readthedocs.io\/en\/stable\/interactive\/magics.html#magic-reset\n# Resets the namespace by removing all names defined by the user\n# -f : force reset without asking for confirmation.\n%reset -f","f182396f":"# Call data manipulation libraries\nimport pandas as pd\nimport numpy as np","73b234e0":"# Plotting libraries to plot feature importance\nimport matplotlib.pyplot as plt\nimport seaborn as sns","661f3a49":"# Read the train Data File 5 rows \u00d7 142 columns\ntrain = pd.read_csv(\"..\/input\/train.csv\")\ntrain.head()","606821ff":"# Read the test Data File  5 rows \u00d7 142 columns\ntest = pd.read_csv(\"..\/input\/test.csv\")\ntest.head()","717472d5":"train.info()","2dbf6fbe":"test.info()","3c0474a7":"#https:\/\/www.kaggle.com\/willkoehrsen\/a-complete-introduction-and-walkthrough\ntrain.select_dtypes(np.int64).nunique().value_counts().sort_index().plot.bar(color = 'orange', \n                                                                             figsize = (8, 6),\n                                                                            edgecolor = 'k', linewidth = 2);\nplt.xlabel('Number of Unique Values'); plt.ylabel('Count');\nplt.title('Count of Unique Values in Integer Columns');","8dd14319":"from collections import OrderedDict\n\nplt.figure(figsize = (20, 16))\nplt.style.use('fivethirtyeight')\n\n# Color mapping\ncolors = OrderedDict({1: 'red', 2: 'orange', 3: 'blue', 4: 'green'})\npoverty_mapping = OrderedDict({1: 'extreme', 2: 'moderate', 3: 'vulnerable', 4: 'non vulnerable'})\n\n# Iterate through the float columns\nfor i, col in enumerate(train.select_dtypes('float')):\n    ax = plt.subplot(4, 2, i + 1)\n    # Iterate through the poverty levels\n    for poverty_level, color in colors.items():\n        # Plot each poverty level as a separate line\n        sns.kdeplot(train.loc[train['Target'] == poverty_level, col].dropna(), \n                    ax = ax, color = color, label = poverty_mapping[poverty_level])\n        \n    plt.title(f'{col.capitalize()} Distribution'); plt.xlabel(f'{col}'); plt.ylabel('Density')\n\nplt.subplots_adjust(top = 2)","8128fafb":"train.select_dtypes('object').head()","be19e591":"mapping = {\"yes\": 1, \"no\": 0}\n\n# Apply same operation to both train and test\nfor df in [train, test]:\n    # Fill in the values with the correct mapping\n    df['dependency'] = df['dependency'].replace(mapping).astype(np.float64)\n    df['edjefa'] = df['edjefa'].replace(mapping).astype(np.float64)\n    df['edjefe'] = df['edjefe'].replace(mapping).astype(np.float64)\n\ntrain[['dependency', 'edjefa', 'edjefe']].describe()","e98e2321":"target_values = train['Target'].value_counts()\ntarget_values = pd.DataFrame(target_values)\ntarget_values['Household_type'] = target_values.index\nmappy = {4: \"NonVulnerable\", 3: \"Moderate Poverty\", 2: \"Vulnerable\", 1: \"Extereme Poverty\"}\ntarget_values['Household_type'] = target_values.Household_type.map(mappy)\ntarget_values","4ad09f0b":"sns.set(style = 'whitegrid', font_scale=1.4)\nfig = plt.subplots(figsize=(15, 8))\nax = sns.barplot(x = 'Household_type', y = 'Target', data = target_values, palette='Accent', ci = None).set_title('Distribution of Poverty in Households')\n","b79a483f":"train['Target'].value_counts().plot(kind='pie',  autopct='%1.1f%%')","80ed70e2":"#sns.countplot(x=\"v2a1\",data=train)\nsns.set(style = 'whitegrid', font_scale=1.4)\nfig = plt.subplots(figsize=(15, 8))\n\nsns.countplot(x=\"rooms\", hue= \"Target\", data=train, palette=\"Accent\").set_title('# of Rooms in Households for Diff Proverty Class')","07e7335d":"sns.set(style = 'whitegrid', font_scale=1.4)\nfig = plt.subplots(figsize=(15, 8))\n\nsns.countplot(x=\"r4h3\", hue= \"Target\", data=train, palette=\"Accent\").set_title('# of Males in Households for Diff Proverty Class')","e4801d8c":"sns.set(style = 'whitegrid', font_scale=1.4)\nfig = plt.subplots(figsize=(15, 8))\n\nsns.countplot(x=\"refrig\", hue= \"Target\", data=train, palette=\"Accent\").set_title('# of Refrigrator in Households for Diff Proverty Class')","42986de6":"# Number of missing in each column\nmissing = pd.DataFrame(train.isnull().sum()).rename(columns = {0: 'total'})\n\n# Create a percentage missing\nmissing['percent'] = missing['total'] \/ len(train)\n\nmissing.sort_values('percent', ascending = False).head(10)","381f1ad6":"# Number of missing in each column - Test Data\nmissing = pd.DataFrame(test.isnull().sum()).rename(columns = {0: 'total'})\n\n# Create a percentage missing\nmissing['percent'] = missing['total'] \/ len(test)\n\nmissing.sort_values('percent', ascending = False).head(10)","1ac8c16e":"train[['meaneduc', 'SQBmeaned']].describe()","3ae93aa3":"#train\ntrain['meaneduc'].fillna(train['meaneduc'].mean(), inplace = True)\ntrain['SQBmeaned'].fillna(train['SQBmeaned'].mean(), inplace = True)\n#the same for test\ntest['meaneduc'].fillna(test['meaneduc'].mean(), inplace = True)\ntest['SQBmeaned'].fillna(test['SQBmeaned'].mean(), inplace = True)\ntrain['rez_esc'].fillna(0, inplace = True)\ntrain['v18q1'].fillna(0, inplace = True)\ntrain['v2a1'].fillna(0, inplace = True)","22952782":"def extract_features(df):\n    df['bedrooms_to_rooms'] = df['bedrooms']\/df['rooms']\n    df['rent_to_rooms'] = df['v2a1']\/df['rooms']\n    df['rent_to_bedrooms'] = df['v2a1']\/df['bedrooms']\n    df['tamhog_to_rooms'] = df['tamhog']\/df['rooms'] # tamhog - size of the household\n    df['tamhog_to_bedrooms'] = df['tamhog']\/df['bedrooms']\n    df['r4t3_to_tamhog'] = df['r4t3']\/df['tamhog'] # r4t3 - Total persons in the household\n    df['r4t3_to_rooms'] = df['r4t3']\/df['rooms'] # r4t3 - Total persons in the household\n    df['r4t3_to_bedrooms'] = df['r4t3']\/df['bedrooms']\n    df['rent_to_r4t3'] = df['v2a1']\/df['r4t3']\n    df['v2a1_to_r4t3'] = df['v2a1']\/(df['r4t3'] - df['r4t1'])\n    df['hhsize_to_rooms'] = df['hhsize']\/df['rooms']\n    df['hhsize_to_bedrooms'] = df['hhsize']\/df['bedrooms']\n    df['rent_to_hhsize'] = df['v2a1']\/df['hhsize']\n    df['qmobilephone_to_r4t3'] = df['qmobilephone']\/df['r4t3']\n    df['qmobilephone_to_v18q1'] = df['qmobilephone']\/df['v18q1']\n    \n\nextract_features(train)\nextract_features(test)","b9b26c98":"train.shape,test.shape","6e9cda5a":"# Number of missing in each column\nmissing = pd.DataFrame(train.isnull().sum()).rename(columns = {0: 'total'})\n\n# Create a percentage missing\nmissing['percent'] = missing['total'] \/ len(train)\n\nmissing.sort_values('percent', ascending = False).head(20)","d21861d1":"train['qmobilephone_to_v18q1'].fillna(0, inplace = True)\n\n\ntest['qmobilephone_to_v18q1'].fillna(0, inplace = True)\n","3e85f745":"# Splitting data into dependent and independent variable\n# X is the independent variables matrix\nX = train.drop('Target', axis = 1)\n\n# y is the dependent variable vector\ny = train.Target","67466ab6":"X.drop(['Id','idhogar'], inplace = True, axis=1)\nX.drop(['qmobilephone_to_v18q1'], inplace = True, axis=1)\nX.shape","bfb14eaf":"X.describe()","6c6187f4":"# Scaling Features\nfrom sklearn.preprocessing import StandardScaler\nss = StandardScaler()\n\nX_ss = ss.fit_transform(X)","b159d077":"from sklearn.decomposition import PCA\npca = PCA(n_components=0.95)\nX_PCA = pca.fit_transform(X)","579ef033":"# split into train\/test and resample the data\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nX_train, X_test, y_train, y_test = train_test_split(X_PCA, y, random_state=1)","8b724e92":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=42)\nrf = rf.fit(X_train, y_train)\n","e19494cb":"y_pred = rf.predict(X_test)\ny_pred","008a73c2":"print('    Accuracy Report: Random Forest Model\\n', classification_report(y_test, y_pred))\n","73fa6db1":"from sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier(max_depth=3, random_state=42)\ndt = dt.fit(X_train, y_train)\n","7224f508":"y_pred1 = dt.predict(X_test)\ny_pred1","b0f0a4b3":"print('    Accuracy Report: Decision Tree Model\\n', classification_report(y_test, y_pred1))","fb779a6e":"from sklearn.ensemble import GradientBoostingClassifier as gbm\n\ngbc = gbm()\ngbc = gbc.fit(X_train, y_train)\n","c82fdadb":"y_pred2 = gbc.predict(X_test)\ny_pred2","267527c1":"print('    Accuracy Report: Gradient Boost Model\\n', classification_report(y_test, y_pred2))\n","220a8f14":"from sklearn.neighbors import KNeighborsClassifier\nkn = KNeighborsClassifier(n_neighbors=4)\nkn = kn.fit(X_train, y_train)","3873e997":"y_pred3 = kn.predict(X_test)\ny_pred3","cc88b267":"print(' Accuracy Report: K Neighbors Model\\n', classification_report(y_test, y_pred3))","e2dd890b":"from xgboost.sklearn import XGBClassifier as XGB\nxgb = XGB()\nxgb = xgb.fit(X_train, y_train)","1d3f272b":"y_pred4 = xgb.predict(X_test)\ny_pred4","4855c845":"print('Accuracy Report: XGB Model\\n', classification_report(y_test, y_pred4))","b79253ae":"import lightgbm as lgb\nlightgbm = lgb.LGBMClassifier()\nlightgbm = lightgbm.fit(X_train, y_train)","fe280265":"y_pred5 = lightgbm.predict(X_test)\ny_pred5","5ec64864":"print('Accuracy Report: Light GBM Model\\n', classification_report(y_test, y_pred5))","7cfb669a":"Feature Engine","0a5c48f7":"  XGBoost\n ","5070be49":"Target - the target is an ordinal variable indicating groups of income levels.\n1 = extreme poverty\n2 = moderate poverty\n3 = vulnerable households\n4 = non vulnerable households ","8c592a4f":"Gradient Boost Classifier","e9ca0d94":"The point is that if we observe outliers in data, we should fill in NAs with median, otherwise it's ok to fill in with mean values. In the table, 50% is the median value, mean is mean :) Here it's fine to use mean values\n\n\nOther 3 columns we fill in with 0's temporarily\n","32af247a":"# Costa Rican Household Poverty Level Prediction : Data Visualization, Feature Engineering, PCA, Estimators & Model Building\n\nThe Inter-American Development Bank is asking community for help with income qualification for some of the world's poorest families.\n\nHere's the backstory: Many social programs have a hard time making sure the right people are given enough aid. It\u2019s especially tricky when a program focuses on the poorest segment of the population. The world\u2019s poorest typically can\u2019t provide the necessary income and expense records to prove that they qualify.\n\nIn Latin America, one popular method uses an algorithm to verify income qualification. It\u2019s called the Proxy Means Test (or PMT). With PMT, agencies use a model that considers a family\u2019s observable household attributes like the material of their walls and ceiling, or the assets found in the home to classify them and predict their level of need.\n\nTo improve on PMT, the IDB (the largest source of development financing for Latin America and the Caribbean) has turned to the Kaggle community. They believe that new methods beyond traditional econometrics, based on a dataset of Costa Rican household characteristics, might help improve PMT\u2019s performance.\n\nPseudo Code :\na. Explore data and perform data visualization\n\nb. Fill in missing values (NULL values) either using mean or median (if the attribute is numeric) or most-frequently occurring value if the attribute is 'object' or categorical.\n\nb. Perform feature engineering, may be using some selected features and only from numeric features.\n\nc. Scale numeric features, AND IF REQUIRED, perform One HOT Encoding of categorical features\n\nd. IF number of features is very large, please do not forget to do PCA.\n\ne. Select some estimators for your work. May be select some (or all) of these:\n\n        \n        RandomForestClassifier\n        GradientBoostingClassifier\n        KNeighborsClassifier\n        XGBoost\n        LightGBM\n\nFirst perform modeling with default parameter values and get accuracy.\n\nf. Then perform tuning using Bayesian Optimization. ","c7c7f153":"\nId - a unique identifier for each row. \n\nidhogar - this is a unique identifier for each household. This can be used to create household-wide features, etc. All rows in a given household will have a matching value for this identifier.\n\ndependency - Dependency rate, calculated = (number of members of the household younger than 19 or older than 64)\/(number of member of household between 19 and 64)\n\nedjefe - years of education of male head of household, based on the interaction of escolari (years of education), head of household and gender, yes=1 and no=0\n\nedjefa - years of education of female head of household, based on the interaction of escolari (years of education), head of household and gender, yes=1 and no=0\n\nid & idhogar are seems to be identifier column. For remaining 3 object columns, we can map it to 'no' to 0 & 'yes' to 1","6076c42c":"Fill in missing values (NULL values) either using mean or median (if the attribute is numeric) or most-frequently occurring value if the attribute is 'object' or categorical.","b675e4c3":"Random Forest Classifier","2338a221":"Split in Train and Test","857ad1ba":"KNeighborsClassifier","658692fe":"Decision Tree Classifier","b3d1d083":"Object Column\n","e06e824d":"PCA","ecf424ae":"It seems that there are 130 integer columns, 8 float columns, and 5 object columns.","adfa2f64":" LightGBM","985a69e7":"There are 8 columns with Float Datatypes. \n\nFollowing graphs shows the distributions of the float columns colored by the value of the Target. With these plots, we can see if there is a significant difference in the variable distribution depending on the household poverty level.","c3cfd70c":"Integer Columns\n\nLet's look at the distribution of unique values in the integer columns. For each column, we'll count the number of unique values and show the result in a bar plot.\n\nFor near to 100 columns, it seems that 2 unique values ( Boolean 0 or 1)","21cc4fa9":"same columns with missing values are observed in test!"}}