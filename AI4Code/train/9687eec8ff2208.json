{"cell_type":{"8a69cfcd":"code","a9ea9e94":"code","b4cfe419":"code","9da4ea7c":"code","5c702215":"code","18c13875":"code","7ba8477a":"code","5158c599":"code","d5b9ca9a":"code","2ce5efa4":"code","4b03adc9":"code","62c06849":"code","c6639519":"code","a5b088b4":"code","423fcad1":"code","4ab4ab69":"code","ebab660a":"code","3451c41c":"code","7cbce695":"markdown","a2233726":"markdown","9be97b24":"markdown","183016fa":"markdown","51f681de":"markdown","2fc5f077":"markdown","035ce2cf":"markdown","1c7875eb":"markdown","d29ab787":"markdown"},"source":{"8a69cfcd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a9ea9e94":"import matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\nfrom sklearn.metrics import f1_score, confusion_matrix, precision_recall_curve, roc_curve\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression","b4cfe419":"data = pd.read_csv('..\/input\/pima-indians-diabetes-database\/diabetes.csv')\nprint(data['Outcome'].value_counts())\ndata.head(3)","9da4ea7c":"# feature type \ud655\uc778\ndata.info()","5c702215":"# \uacb0\uce21\uce58 \ud655\uc778\ndata.isna().sum()","18c13875":"# train_test_split\nX = data.iloc[:, :-1]\ny = data.iloc[:, -1]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y,\n                                                    test_size=.2,\n                                                    random_state=156,\n                                                    stratify=y)","7ba8477a":"# get_clf_eval function \uc0dd\uc131\n# confusion, accuracy, precision, recall, f1, roc_auc \ub098\ud0c0\ub0c4\n\ndef get_clf_eval(y_test, pred=None, pred_proba=None):\n    confusion = confusion_matrix(y_test, pred)\n    accuracy = accuracy_score(y_test, pred)\n    precision = precision_score(y_test, pred)\n    recall = recall_score(y_test, pred)\n    f1 = f1_score(y_test, pred)\n    \n    roc_auc = roc_auc_score(y_test, pred_proba)\n    print('\uc624\ucc28 \ud589\ub82c confusion matrix')\n    print(confusion)\n    print('\uc815\ud655\ub3c4 accuracy: {0:.4f}, \uc815\ubc00\ub3c4 precision: {1:.4f}, \uc7ac\ud604\uc728 recall: {2:.4f}, \\F1: {3:.4f}, AUC: {4:.4f}'.format(accuracy, precision, recall, f1, roc_auc))","5158c599":"# logistic regression\nlr = LogisticRegression()\nlr.fit(X_train, y_train)\npred = lr.predict(X_test)\npred_proba = lr.predict_proba(X_test)[:, 1]\n\nget_clf_eval(y_test, pred, pred_proba)","d5b9ca9a":"# precision_recall_curve_plot function\n# \uc784\uacc4\uac12 \uc124\uc815\nthresholds = [.4, .45, .5, .55, .6]\n\ndef precision_recall_curve_plot(y_test, pred_proba_c1):\n    # threshold ndarray\uc640 \uc774 threshold\uc5d0 \ub530\ub978 \uc815\ubc00\ub3c4, \uc7ac\ud604\uc728 ndarray \ucd94\ucd9c\ud558\uae30\n    precisions, recalls, thresholds = precision_recall_curve(y_test, pred_proba_c1)\n    \n    # x\ucd95\uc744 threshold\uac12\uc73c\ub85c, y\ucd95\uc740 \uc815\ubc00\ub3c4, \uc7ac\ud604\uc728 \uac12\uc73c\ub85c \uac01\uac01 plot \uc218\ud589. \uc815\ubc00\ub3c4\ub294 \uc810\uc120\uc73c\ub85c \ud45c\uc2dc\n    plt.figure(figsize=(8, 6))\n    threshold_boundary = thresholds.shape[0]\n    plt.plot(thresholds, precisions[0:threshold_boundary], linestyle='--', label='precision')\n    plt.plot(thresholds, recalls[0:threshold_boundary], label='recall')\n    \n    # threshold \uac12 x\ucd95\uc758 scale\uc744 0, 1 \ub2e8\uc704\ub85c \ubcc0\uacbd\n    start, end = plt.xlim()\n    plt.xticks(np.round(np.arange(start, end, 0.1), 2))\n    \n    # x, y\ucd95 label\uacfc legend, grid \uc124\uc815\n    plt.xlabel('Threshold value'); plt.ylabel('Precision and Recall value')\n    plt.legend(); plt.grid()\n    plt.show()","2ce5efa4":"pred_proba_c1 = lr.predict_proba(X_test)[:, 1]\nprecision_recall_curve_plot(y_test, pred_proba_c1)","4b03adc9":"data.describe()","62c06849":"# describe\ub97c \ubcf4\uba74 `Glocose` \uac12\uc758 min\uc774 0\uc73c\ub85c \ub41c \ud53c\ucc98\uac00 \uc0c1\ub2f9\uc218 \uc874\uc7ac\ud558\ub294 \uac83\uc73c\ub85c \ubcf4\uc785\ub2c8\ub2e4. Glocuse\ub294 \ud3ec\ub3c4\ub2f9 \uc218\uce58\uc774\ubbc0\ub85c 0\uc774 \ub9d0\uc774 \uc548\ub429\ub2c8\ub2e4.\n# \ud788\uc2a4\ud1a0\uadf8\ub7a8\uc73c\ub85c \ud655\uc778\ud574\ubcf4\uba74 \ub2e4\uc74c\uacfc \uac19\uc2b5\ub2c8\ub2e4\n\nplt.hist(data['Glucose'], bins=10)","c6639519":"# min\uac12\uc774 0\uc73c\ub85c \ub418\uc5b4 \uc788\ub294 \ud53c\ucc98\uc5d0 \ub300\ud574 0 \uac12\uc758 \uac74\uc218\uc640 \uc804\uccb4 \ub370\uc774\ud130 \uac74\uc218 \ub300\ube44 \uba87 \ud37c\uc13c\ud2b8 \ube44\uc728\ub85c \uc874\uc7ac\ud558\ub294 \uc9c0 \ud655\uc778\ud569\ub2c8\ub2e4\n# 0\uac12 \uccb4\ud06c\ud560 feature list\nzero_features = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']\n\n# \uc804\uccb4 \ub370\uc774\ud130 \uac74\uc218\ntotal_count = data['Glucose'].count()\n\n# feature \ubcc4\ub85c \ubc18\ubcf5\ud558\uba74\uc11c \ub370\uc774\ud130 \uac12\uc774 0\uc778 \ub370\uc774\ud130 \uac74\uc218\uc640 \ud37c\uc13c\ud2b8 \uacc4\uc0b0\nfor feature in zero_features:\n    zero_count = data[data[feature] == 0][feature].count()\n    print('{0} 0 \uac74\uc218\ub294 {1}, \ud37c\uc13c\ud2b8\ub294 {2:.2f} %'.format(feature, zero_count,\n                                                100 * (zero_count \/ total_count)))","a5b088b4":"# \uc704 feature\uc758 0\uac12\ub4e4\uc744 \ud3c9\uade0\uac12\uc73c\ub85c \ub300\uccb4\ud569\ub2c8\ub2e4\nmean_zero_features = data[zero_features].mean()\ndata[zero_features] = data[zero_features].replace(0, mean_zero_features)","423fcad1":"X = data.iloc[:, :-1]\ny = data.iloc[:, -1]\n\n# StandardScaler \ud074\ub798\uc2a4\ub97c \uc774\uc6a9\ud574 \ud53c\ucc98 \ub370\uc774\ud130 \uc138\ud2b8\uc5d0 \uc77c\uad04\uc801\uc73c\ub85c \uc2a4\ucf00\uc77c\ub9c1 \uc801\uc6a9\nscaler = StandardScaler( )\nX_scaled = scaler.fit_transform(X)\n\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size = 0.2, random_state = 156, stratify=y)\n\n# \ub85c\uc9c0\uc2a4\ud2f1 \ud68c\uadc0\ub85c \ud559\uc2b5, \uc608\uce21 \ubc0f \ud3c9\uac00 \uc218\ud589. \nlr_clf = LogisticRegression()\nlr_clf.fit(X_train, y_train)\npred = lr_clf.predict(X_test)\npred_proba = lr_clf.predict_proba(X_test)[:, 1]\n\nget_clf_eval(y_test, pred, pred_proba)","4ab4ab69":"from sklearn.preprocessing import Binarizer\n\ndef get_eval_by_threshold(y_test , pred_proba_c1, thresholds):\n    # thresholds \ub9ac\uc2a4\ud2b8 \uac1d\uccb4\ub0b4\uc758 \uac12\uc744 \ucc28\ub840\ub85c iteration\ud558\uba74\uc11c Evaluation \uc218\ud589.\n    for custom_threshold in thresholds:\n        binarizer = Binarizer(threshold=custom_threshold).fit(pred_proba_c1) \n        custom_predict = binarizer.transform(pred_proba_c1)\n        print('\uc784\uacd7\uac12:', custom_threshold)\n        get_clf_eval(y_test , custom_predict, pred_proba_c1)","ebab660a":"thresholds = [.3, .33, .36, .39, .42, .45, .48, .5]\nget_eval_by_threshold(y_test, pred_proba.reshape(-1, 1), thresholds)","3451c41c":"# \uc55e\uc120 \ub85c\uc9c0\uc2a4\ud2f1 \ud68c\uadc0 \ubaa8\ub378\uc744 \uc774\uc6a9\ud574 \uc784\uacc4\uac12\uc744 0.48\ub85c \ub0ae\ucd98 \uc0c1\ud0dc\uc5d0\uc11c \ub2e4\uc2dc \uc608\uce21\nbinarizer = Binarizer(threshold = 0.48)\n\npred_th_048 = binarizer.fit_transform(pred_proba.reshape(-1, 1))\nget_clf_eval(y_test, pred_th_048, pred_proba)","7cbce695":"\uc608\uce21 \uc815\ud655\ub3c4\uac00 \uc57d 77%, \uc7ac\ud604\uc728\uc774 57%\ub85c \uce21\uc815\ub429\ub2c8\ub2e4. \uc804\uccb4 \ub370\uc774\ud130\uc758 65%\uac00 negative\uc774\uae30 \ub54c\ubb38\uc5d0 \uc815\ud655\ub3c4\ubcf4\ub2e4\ub294 \uc7ac\ud604\uc728\uc5d0 \uc870\uae08 \ub354 \ucd08\uc810\uc744 \ub9de\ucdb0 \ubd05\ub2c8\ub2e4.\n\n\uba3c\uc800 \uc815\ubc00\ub3c4 \uc7ac\ud604\uc728 \uace1\uc120\uc744 \ubcf4\uace0 \uc784\uacd7\uac12\ubcc4 \uc815\ubc00\ub3c4\uc640 \uc7ac\ud604\uc728 \uac12\uc758 \ubcc0\ud654\ub97c \ud655\uc778\ud558\uaca0\uc2b5\ub2c8\ub2e4.","a2233726":"output \uc815\ub9ac\ud558\uc5ec \ubcfc \ub54c,\n\n- \uc7ac\ud604\uc728\uc744 \ub192\uc774\ub294 \ub370 \uac00\uc7a5 \uc88b\uc740 \uc784\uacc4\uac12\uc740 0.33 (\ud558\uc9c0\ub9cc \ub9e4\uc6b0 \uadf9\ub2e8\uc801\uc778 \uc120\ud0dd)\n\n- \uc804\uccb4\uc801\uc778 \uc131\ub2a5 \ud3c9\uac00 \uc9c0\ud45c \uc720\uc9c0 + \uc7ac\ud604\uc728 \uc870\uae08 \uc0c1\uc2b9\uc740 0.42\n","9be97b24":"\uacb0\uce21\uce58\ub294 \uc5c6\uc73c\uba70 \ud53c\ucc98 \ud0c0\uc785\uc740 \ubaa8\ub450 \uc22b\uc790\ud615\uc785\ub2c8\ub2e4. \ubcc4\ub3c4\uc758 \ud53c\ucc98 \uc778\ucf54\ub529\uc740 \ud544\uc694\uc5c6\uc5b4 \ubcf4\uc785\ub2c8\ub2e4.\n\n<br>\n\ub85c\uc9c0\uc2a4\ud2f1 \ud68c\uadc0\ub97c \uc704\ud574 \uc608\uce21 \ubaa8\ub378\uc744 \uc0dd\uc131\ud569\ub2c8\ub2e4.\n\ntrain_test_split\uc744 \uc774\uc6a9\ud558\uc5ec \ubd84\ub9ac\ud558\uace0, \uc131\ub2a5\uc9c0\ud45c\ub97c \ucd9c\ub825\ud558\uc5ec \uc7ac\ud604\uc728 \uace1\uc120\uc744 \uc2dc\uac01\ud654 \ud569\ub2c8\ub2e4.","183016fa":"---","51f681de":"\uc815\ubc00\ub3c4\uc640 \uc7ac\ud604\uc728\uc774 \uc77c\uc815 \uac1c\uc120\ub41c \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n<br>\n\ucd94\uac00\uc801\uc73c\ub85c \uc7ac\ud604\uc728 \uc218\uce58 \uac1c\uc120\uc744 \uc704\ud574 \uc784\uacc4\uac12 \ubcc0\ud654 \ud568\uc218\ub97c \uc774\uc6a9\ud569\ub2c8\ub2e4.","2fc5f077":"## [Tutorial] Pima Indians Diabetes\n### \ucc45 <\ud30c\uc774\uc36c \uba38\uc2e0\ub7ec\ub2dd \uc644\ubcbd \uac00\uc774\ub4dc> \ucf54\ub4dc\ub97c \ucc38\uc870\ud588\uc2b5\ub2c8\ub2e4.","035ce2cf":"\uc784\uacc4\uac12\uc744 0.42 \uc815\ub3c4\ub85c \ub0ae\ucd94\uba74 \uc815\ubc00\ub3c4\uc640 \uc7ac\ud604\uc728\uc774 \uc5b4\ub290 \uc815\ub3c4 \uade0\ud615\uc744 \ub9de\ucda5\ub2c8\ub2e4. \ud558\uc9c0\ub9cc \ub450 \uc9c0\ud45c \ubaa8\ub450 0.7\uc774 \uc548\ub418\ub294 \uc218\uce58\ub85c \ubcf4\uc785\ub2c8\ub2e4.\n\n\uc784\uacc4\uac12\uc744 \uc778\uc704\uc801\uc73c\ub85c \uc870\uc791\ud558\uae30 \uc804\uc5d0 \ub2e4\uc2dc \ub370\uc774\ud130 \uac12\uc744 \uc810\uac80\ud569\ub2c8\ub2e4.","1c7875eb":"**\ud53c\ub9c8 \uc778\ub514\uc5b8 \ub2f9\ub1e8\ubcd1 feature information**\n\n- `Pregnancies` : \uc784\uc2e0 \ud69f\uc218\n\n- `Glucose` : \ud3ec\ub3c4\ub2f9 \ubd80\ud558 \uac80\uc0ac \uc218\uce58\n\n- `BloodPressure` : \ud608\uc555 (mm Hg)\n\n- `SkinThickness` : \ud314 \uc0bc\ub450\uadfc \ub4a4\ucabd\uc758 \ud53c\ud558\uc9c0\ubc29 \uce21\uc815\uac12(mm)\n\n- `Insulin` : \ud608\uccad \uc778\uc290\ub9b0(mu U\/ml)\n\n- `BMI` : \uccb4\uc9c8\ub7c9 \uc9c0\uc218\n\n- `DiabetesPedigreeFunction` : \ub2f9\ub1e8 \ub0b4\ub825 \uac00\uc911\uce58 \uac12\n\n- `Age` : \ub098\uc774\n\n- `Outcome` : \ud074\ub798\uc2a4 \uacb0\uc815 \uac12(0 \ub610\ub294 1)","d29ab787":"\uc804\uccb4 768\uac1c\uc758 \ub370\uc774\ud130 \uc911\uc5d0\uc11c negative \uac12 0\uc774 500\uac1c, positive \uac12 1\uc774 268\ub85c \uc0c1\ub300\uc801\uc73c\ub85c negative\uac00 \ub9ce\uc2b5\ub2c8\ub2e4."}}