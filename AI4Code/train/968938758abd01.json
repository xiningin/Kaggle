{"cell_type":{"1e24d7de":"code","b4b7c953":"code","d21076e0":"code","58a6b418":"code","5ca3d3d7":"code","69eaecde":"code","71b307a9":"code","694ea36f":"code","472be307":"code","fd03c126":"code","3f0075ee":"code","c396f007":"code","eb94c641":"code","950ee03a":"code","54ea279c":"code","e3ae28f4":"code","963d6766":"code","8aa96a96":"code","c87fe3cd":"code","149ed34c":"markdown","3cec9e3d":"markdown","9a553848":"markdown"},"source":{"1e24d7de":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport os, time, random, cv2, glob, pickle, librosa\nfrom pathlib import Path\nfrom PIL import Image\nimport imgaug as ia\nfrom imgaug import augmenters as iaa\nfrom tqdm import tqdm\n\nfrom keras.models import Model\nfrom keras.layers import (Convolution1D, Input, Dense, Flatten, Dropout, GlobalAveragePooling1D, concatenate,\n                          Activation, MaxPool1D, GlobalMaxPool1D, BatchNormalization, Concatenate, ReLU, LeakyReLU)\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping, LearningRateScheduler\nfrom keras.optimizers import Adam, SGD, RMSprop\nfrom keras.losses import sparse_categorical_crossentropy\nfrom keras.utils.np_utils import to_categorical\nfrom sklearn.model_selection import train_test_split\nprint(os.listdir(\"..\/input\"))","b4b7c953":"t_start = time.time()\n\n# Keras reproduce score (then init all model seed)\nseed_nb=14\nimport numpy as np \nnp.random.seed(seed_nb)\nimport tensorflow as tf\ntf.set_random_seed(seed_nb)","d21076e0":"input_length = 5000\n\nbatch_size = 64\n\ndef audio_norm(data):\n\n    max_data = np.max(data)\n    min_data = np.min(data)\n    data = (data-min_data)\/(max_data-min_data+0.0001)\n    return data-0.5\n\n\ndef load_audio_file(file_path, input_length=input_length):\n    data = librosa.core.load(file_path, sr=16000)[0] #, sr=16000\n    if len(data)>input_length:\n        max_offset = len(data)-input_length\n        offset = np.random.randint(max_offset)\n        data = data[offset:(input_length+offset)]\n        \n    else:\n        if input_length > len(data):\n            max_offset = input_length - len(data)\n            offset = np.random.randint(max_offset)\n        else:\n            offset = 0\n            \n        data = np.pad(data, (offset, input_length - len(data) - offset), \"constant\")\n        \n    data = audio_norm(data)\n    return data","58a6b418":"train_files = glob.glob(\"..\/input\/train_curated\/*.wav\")\ntrain_labels = pd.read_csv(\"..\/input\/train_curated.csv\")\ntrain_labels['labels'] = train_labels['labels'].apply(lambda x: x.split(',')[0]) # only keep first label for now","5ca3d3d7":"file_to_label = {\"..\/input\/train_curated\/\"+k:v for k,v in zip(train_labels.fname.values, train_labels.labels.values)}","69eaecde":"list_labels = sorted(list(set(train_labels.labels.values)))\nlabel_to_int = {k:v for v,k in enumerate(list_labels)}\nint_to_label = {v:k for k,v in label_to_int.items()}\nfile_to_int = {k:label_to_int[v] for k,v in file_to_label.items()}","71b307a9":"def get_model():\n    nclass = len(list_labels)\n    inp = Input(shape=(input_length, 1))\n    img_1 = Convolution1D(16, kernel_size=9, activation=\"relu\", padding=\"valid\")(inp)\n    img_1 = Convolution1D(16, kernel_size=9, activation=\"relu\", padding=\"valid\")(img_1)\n    img_1 = MaxPool1D(pool_size=16)(img_1)\n    img_1 = Convolution1D(64, kernel_size=3, activation=\"relu\", padding=\"valid\")(img_1)\n    img_1 = Convolution1D(64, kernel_size=3, activation=\"relu\", padding=\"valid\")(img_1)\n    img_1 = MaxPool1D(pool_size=4)(img_1)\n    img_1 = Convolution1D(64, kernel_size=3, activation=\"relu\", padding=\"valid\")(img_1)\n    img_1 = Convolution1D(64, kernel_size=3, activation=\"relu\", padding=\"valid\")(img_1)\n    img_1 = MaxPool1D(pool_size=4)(img_1)\n    img_1 = Convolution1D(256, kernel_size=3, activation=\"relu\", padding=\"valid\")(img_1)\n    img_1 = Convolution1D(256, kernel_size=3, activation=\"relu\", padding=\"valid\")(img_1)\n    img_1 = GlobalMaxPool1D()(img_1)\n    \n    dense_1 = Dense(512, activation=\"relu\")(img_1)\n    dense_1 = Dense(256, activation=\"relu\")(dense_1)\n    dense_1 = Dense(nclass, activation=\"softmax\")(dense_1)\n\n    model = Model(inputs=inp, outputs=dense_1)\n\n    model.compile(optimizer=Adam(0.001), loss=sparse_categorical_crossentropy, metrics=['acc'])\n    model.summary()\n    return model","694ea36f":"def chunker(seq, size):\n    return (seq[pos:pos + size] for pos in range(0, len(seq), size))","472be307":"def train_generator(list_files, batch_size=batch_size):\n    while True:\n        random.shuffle(list_files)\n        for batch_files in chunker(list_files, size=batch_size):\n            batch_data = [load_audio_file(fpath) for fpath in batch_files]\n            batch_data = np.array(batch_data)[:,:,np.newaxis]\n            batch_labels = [file_to_int[fpath] for fpath in batch_files]\n            batch_labels = np.array(batch_labels)\n            \n            yield batch_data, batch_labels","fd03c126":"tr_files, val_files = train_test_split(train_files, test_size=0.05)","3f0075ee":"model = get_model()","c396f007":"model.fit_generator(train_generator(tr_files), \n                    steps_per_epoch=len(tr_files)\/\/batch_size, \n                    validation_data=train_generator(val_files),\n                    validation_steps=len(val_files)\/\/batch_size,\n                    epochs=2)","eb94c641":"list_preds = []\nbatch_size = 128\ntest_files = glob.glob(\"..\/input\/test\/*.wav\")\ntest_files.sort()","950ee03a":"for batch_files in tqdm(chunker(test_files, size=batch_size), total=len(test_files)\/\/batch_size ):\n    batch_data = [load_audio_file(fpath) for fpath in batch_files]\n    batch_data = np.array(batch_data)[:,:,np.newaxis]\n    preds = model.predict(batch_data).tolist()\n    list_preds += preds","54ea279c":"array_preds = np.array(list_preds)","e3ae28f4":"df = pd.read_csv('..\/input\/sample_submission.csv')\nfor i, v in enumerate(list_labels):\n    df[v] = array_preds[:, i]","963d6766":"df['fname'] = df.fname.apply(lambda x: x.split(\"\/\")[-1])","8aa96a96":"df.to_csv(\"submission.csv\", index=False)\ndf.head()","c87fe3cd":"t_finish = time.time()\nprint(f\"Kernel run time = {(t_finish-t_start)\/3600} hours\")","149ed34c":"# || Configuration","3cec9e3d":"# || Data Preparation","9a553848":"based on : \n* https:\/\/www.kaggle.com\/CVxTz\/keras-cnn-starter\n* https:\/\/www.kaggle.com\/jmourad100\/keras-eda-and-cnn-starter\n\n# || Loading Packages"}}