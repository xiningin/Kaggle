{"cell_type":{"6ddfe78b":"code","bc31b70b":"code","616b0904":"code","f1c71178":"code","38b9f9f5":"code","7a15b16b":"code","c4246006":"code","e51fc6eb":"code","94791cad":"code","9a9a1efc":"code","47515c7a":"code","bc45ab8f":"code","8a0d62dd":"code","14c60fe0":"code","f4ba544b":"code","e7b795c3":"markdown","44640b7c":"markdown","a312c061":"markdown"},"source":{"6ddfe78b":"import numpy as np\nimport pandas as pd\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nimport matplotlib.colors\nfrom pylab import rcParams\nimport seaborn as sns\nfrom pathlib import Path\nimport os\nimport folium\nfrom folium.plugins import MarkerCluster\nimport shapely\nfrom shapely.geometry import Point\n%matplotlib inline","bc31b70b":"# understanding column names\npd.read_csv('..\/input\/crime-data-in-brazil\/RDO_column_description.csv')","616b0904":"# list of csv files that will be used as input data\ninput_dir = '..\/input\/crime-data-in-brazil\/'\ncsv_list = sorted(list(os.listdir(input_dir)))\ndel csv_list[-5::]\nfor file in csv_list:\n    print(file)","f1c71178":"# columns to drop\ncols = ['NUM_BO', 'ANO_BO', 'ID_DELEGACIA', 'NOME_DEPARTAMENTO',\n        'NOME_SECCIONAL', 'DELEGACIA', 'DESDOBRAMENTO', 'CIDADE',\n        'NOME_DEPARTAMENTO_CIRC', 'NOME_SECCIONAL_CIRC','NOME_DELEGACIA_CIRC',\n        'MES', 'ANO', 'CONT_PESSOA', 'FLAG_STATUS', 'Unnamed: 30',\n        'Unnamed: 31', 'Unnamed: 32', 'Unnamed: 33', 'Unnamed: 34',\n        'Unnamed: 35', 'Unnamed: 36', 'Unnamed: 37', 'Unnamed: 38',\n        'Unnamed: 39', 'Unnamed: 40', 'Unnamed: 41', 'Unnamed: 42',\n        'Unnamed: 43', 'Unnamed: 44', 'Unnamed: 45', 'Unnamed: 46',\n        'Unnamed: 47', 'Unnamed: 48', 'Unnamed: 49', 'Unnamed: 50',\n        'Unnamed: 51', 'Unnamed: 52']\n\n# some lat\/long values are redacted and replaced with this str\ndrop_term = ['Informa\u00e7\u00e3o restrita']\n\n# data loading functions\ndef csv_to_df(csv):\n    \"\"\" takes csv file and returns df \"\"\"\n    df = pd.DataFrame(pd.read_csv(input_dir + csv))\n    return df\n\ndef drop_cols(df):\n    \"\"\" deletes columns in cols dict from input df \"\"\"\n    for col in cols:\n        if col in df.columns:\n           del df[col]\n    return df\n\n# data cleaning\/processing functions\ndef drop_redacted(df):\n    df = df[~df['LATITUDE'].str.contains('|'.join(drop_term), na=False)]\n    return df\n\ndef coord_floats(df):\n    \"\"\" convert str long, lat values to floats \"\"\"    \n    df['LONGITUDE'] = df.apply(lambda row: float(row['LONGITUDE']), axis=1)\n    df['LATITUDE'] = df.apply(lambda row: float(row['LATITUDE']), axis=1)\n    return df","38b9f9f5":"# load csv files into dict as dataframes, drop unnecessary columns from dataframes\ndf_dict = []\n\nfor file in csv_list:\n    df = csv_to_df(file)\n    df = drop_cols(df)\n    df_dict.append(df)","7a15b16b":"# assign names to dataframes\ndf_2007 = df_dict[0].append(df_dict[1], ignore_index=True)\ndf_2008 = df_dict[2].append(df_dict[3], ignore_index=True)\ndf_2009 = df_dict[4].append(df_dict[5], ignore_index=True)\ndf_2010 = df_dict[6].append(df_dict[7], ignore_index=True)\ndf_2011 = df_dict[8].append(df_dict[9], ignore_index=True)\ndf_2012 = df_dict[10].append(df_dict[11], ignore_index=True)\ndf_2013 = df_dict[12].append(df_dict[13], ignore_index=True)\ndf_2014 = df_dict[14].append(df_dict[15], ignore_index=True)\ndf_2015 = df_dict[16]\ndf_2016 = df_dict[17]\n\ndataframes = [df_2007, df_2008, df_2009, df_2010, df_2011, df_2012, df_2013, df_2014, df_2015, df_2016]","c4246006":"# checking for NaN coordinate values\ndef percent_nan(list):\n    year = 2007\n    for df in list:\n        nan_lat = \"{:.0%}\".format((sum(df['LATITUDE'].isnull())\/len(df)))\n        nan_long = \"{:.0%}\".format((sum(df['LONGITUDE'].isnull())\/len(df)))\n        print(year, ':', nan_lat, 'NaN latitude values, ', nan_long, 'NaN longitude values')\n        year += 1\n        \npercent_nan(dataframes)","e51fc6eb":"# cleaning coordinate data for 2011 dataframe\ndf_2011 = drop_redacted(df_2011)\ndf_2011 = df_2011.dropna(subset=['LATITUDE', 'LONGITUDE'])\ndf_2011 = coord_floats(df_2011)\ndf_2011 = df_2011.reset_index(drop=True)","94791cad":"# check for NaN coordinate values in 2011 dataframe\nprint('NaN values left =', sum(df_2011['LATITUDE'].isnull()))","9a9a1efc":"# create base map\nsp_map = folium.Map(location=[-23.5505, -46.6333], zoom_start=10, tiles='CartoDB dark_matter')","47515c7a":"# import Sao Paulo municipality boundary polygons from geojson file\nsp_geo = gpd.read_file('..\/input\/geojson\/saopaulo.json')\n\n# select after first 5, rows 1-4 are not municipalities\nsp_geo = sp_geo.iloc[5:]\nsp_geo = sp_geo.reset_index(drop=True)\n\n# check quantity of municipalities in sao paulo, should be 645\nprint(len(sp_geo))","bc45ab8f":"# functions to assign municipality to each row in df based on coordinate values\ndef check_polygon(point):\n    \"\"\" \n    checks if input point is in each of polygons in sp_geo geodataframe \n    returns municipality name corresponding to polygon containg point\n    \"\"\"\n    for index, row in sp_geo.iterrows():\n        polygon = sp_geo.loc[index]['geometry']\n        if polygon.contains(point) == False:\n            continue\n        else:\n            if polygon.contains(point) == True:\n                muni = sp_geo.loc[index]['NOMEMUNICP']\n                return muni\n\ndef get_muni(long_series, lat_series):\n    \"\"\" \n    takes series of long\/lat values and series of geojson polygons\n    returns list of corresponding municipalities where point is in polygon\n    \"\"\"\n    points = [Point(xy) for xy in zip(long_series.values, lat_series.values)]\n    muni_list = [check_polygon(x) for x in points]\n    return muni_list","8a0d62dd":"# randomly sample n = 35000 random rows from 2011 dataframe\n# df_2011 = df_2011.reset_index()\ndf_2011_sample = df_2011.sample(1)","14c60fe0":"# assign municipality for each row in sampled df_2011 sample to ['MUNI'] column\nnew_cols = df_2011_sample.columns.tolist() + ['MUNI']\ndf_2011_sample = df_2011_sample.reindex(columns = new_cols)\n\nmuni_list = get_muni(df_2011_sample['LONGITUDE'], df_2011_sample['LATITUDE'])\nmuni_series= pd.Series(muni_list)\ndf_2011_sample['MUNI'] = muni_series\ndf_2011_sample.head()","f4ba544b":"# create chloropleth map of crime coordinate density in Sao Paulo municipalities\n# sp_map.chloropleth(geo_data=sp_geo,\n#                    data= ,\n#                    columns= ,\n#                    key_on= ,\n#                    fill_color= ,\n#                    fill_opacity=0.7,\n#                    line_opacity=0.2,\n#                    legend_name='Crime Event Density')","e7b795c3":"#### Findings:\n* For years 2007 to 2010, latitude and longitude data is mostly missing\n* Latitude and longitude data is only really available for years 2011 to 2016\n* Possibly due to crime coordinate data not being routinely recorded prior to 2011","44640b7c":"* Ideally 2007 - 2016 csv files would have name of municipality, but that column is not included in those files","a312c061":"#### Features of interest:\n* [DATA_OCORRENCIA_BO]: Date of occurence\n* [HORA_OCORRENCIA_BO]: Hour of occurence\n* [RUBRICA]: Nature of crime\n* [IDADE_PESSOA]: Age of person\n* [SEXO_PESSOA]: Sex of person\n* [LOGRADOURO]: Street of crime occurence\n* [NOME_MUNICIPIO_CIRC]: Municipality of occurence\t\n* [LATITUDE]\n* [LONGITUDE]\n* [NUMERO_LOGRADOURO]: Street number of crime occurence\n* [COR]: Skin color\n* [DESCR_TIPO_PESSOA]: Nature of person involved\n* [DESCR_PROFISSAO]: Profession\n* [DESCR_GRAU_INSTRUCAO]: Degree of education"}}