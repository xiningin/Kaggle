{"cell_type":{"fde1c830":"code","1bd37283":"code","7094737d":"code","f0be13df":"code","4f7d1026":"code","e9735ded":"code","069653e9":"code","ae168731":"code","c54cfb65":"code","0610d58b":"code","49ed2610":"code","d6be132a":"code","c544f14b":"code","0874dc33":"code","f64f9a0b":"code","dedf34ed":"code","97b91438":"code","7bea4da4":"code","8e26767f":"code","de9c02e9":"code","1d5ee2aa":"markdown","e0f7638f":"markdown","bb0e6b40":"markdown","c5de29f1":"markdown","fa5800c2":"markdown","096224df":"markdown","eb23057b":"markdown","d648edf6":"markdown","add56bf6":"markdown","5de51116":"markdown","d90f4b13":"markdown","2ec7f42b":"markdown","8bc2f53e":"markdown","6ba1da9a":"markdown","b4fe1422":"markdown","d768e00e":"markdown","f0269073":"markdown","f98e95b2":"markdown"},"source":{"fde1c830":"import pandas as pd\nimport numpy as np\nimport matplotlib.pylab as plt\nimport seaborn as sns\nfrom fbprophet import Prophet\nfrom itertools import cycle\nimport warnings\nwarnings.filterwarnings('ignore')\nimport dask.dataframe as dd\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_rows', 500)\nimport lightgbm as lgb\nimport dask_xgboost as xgb\nimport dask.dataframe as dd\nfrom sklearn import preprocessing, metrics\nfrom sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold, GroupKFold, GridSearchCV, train_test_split, TimeSeriesSplit\nimport gc\nimport os\nfrom  datetime import datetime, timedelta\nimport gc\nimport lightgbm as lgb\nfrom sklearn.preprocessing import OrdinalEncoder\nplt.style.use('bmh')\ncolor_pal = plt.rcParams['axes.prop_cycle'].by_key()['color']\ncolor_cycle = cycle(plt.rcParams['axes.prop_cycle'].by_key()['color'])\n","1bd37283":"!ls -GFlash --color ..\/input\/m5-forecasting-accuracy\/","7094737d":"# Read in the data\nINPUT_DIR = '..\/input\/m5-forecasting-accuracy'\ncal = pd.read_csv(f'{INPUT_DIR}\/calendar.csv')\nstv = pd.read_csv(f'{INPUT_DIR}\/sales_train_validation.csv')\nss = pd.read_csv(f'{INPUT_DIR}\/sample_submission.csv')\nsellp = pd.read_csv(f'{INPUT_DIR}\/sell_prices.csv')","f0be13df":"ss.head()","4f7d1026":"stv.head()","e9735ded":"d_cols = [c for c in stv.columns if 'd_' in c] # sales data columns\n\n# Below we are chaining the following steps in pandas:\n# 1. Select the item.\n# 2. Set the id as the index, Keep only sales data columns\n# 3. Transform so it's a column\n# 4. Plot the data\nstv.loc[stv['id'] == 'FOODS_3_090_CA_3_validation'] \\\n    .set_index('id')[d_cols] \\\n    .T \\\n    .plot(figsize=(15, 5),\n          title='FOODS_3_090_CA_3 sales by \"d\" number',\n          color=next(color_cycle))\nplt.legend('')\nplt.show()","069653e9":"# Calendar data looks like this (only showing columns we care about for now)\ncal[['d','date','event_name_1','event_name_2',\n     'event_type_1','event_type_2', 'snap_CA']].head()","ae168731":"for i, var in enumerate([\"year\", \"weekday\", \"month\", \"event_name_1\", \"event_name_2\", \n                         \"event_type_1\", \"event_type_2\", \"snap_CA\", \"snap_TX\", \"snap_WI\"]):\n    plt.figure()\n    g = sns.countplot(cal[var])\n    g.set_xticklabels(g.get_xticklabels(), rotation=45)\n    g.set_title(var)","c54cfb65":"cal.head()","0610d58b":"# Merge calendar on our items' data\nexample = stv.loc[stv['id'] == 'FOODS_3_090_CA_3_validation'][d_cols].T\nexample = example.rename(columns={8412:'FOODS_3_090_CA_3'}) # Name it correctly\nexample = example.reset_index().rename(columns={'index': 'd'}) # make the index \"d\"\nexample = example.merge(cal, how='left', validate='1:1')\nexample.set_index('date')['FOODS_3_090_CA_3'] \\\n    .plot(figsize=(15, 5),\n          color=next(color_cycle),\n          title='FOODS_3_090_CA_3 sales by actual sale dates')\nplt.show()\n\n# Select more top selling examples\nexample2 = stv.loc[stv['id'] == 'HOBBIES_1_234_CA_3_validation'][d_cols].T\nexample2 = example2.rename(columns={6324:'HOBBIES_1_234_CA_3'}) # Name it correctly\nexample2 = example2.reset_index().rename(columns={'index': 'd'}) # make the index \"d\"\nexample2 = example2.merge(cal, how='left', validate='1:1')\n\nexample3 = stv.loc[stv['id'] == 'HOUSEHOLD_1_118_CA_3_validation'][d_cols].T\nexample3 = example3.rename(columns={6776:'HOUSEHOLD_1_118_CA_3'}) # Name it correctly\nexample3 = example3.reset_index().rename(columns={'index': 'd'}) # make the index \"d\"\nexample3 = example3.merge(cal, how='left', validate='1:1')\n","49ed2610":"for i, var in enumerate([\"state_id\", \"store_id\", \"cat_id\", \"dept_id\"]):\n    plt.figure()\n    g = sns.countplot(stv[var])\n    g.set_xticklabels(g.get_xticklabels(), rotation=45)\n    g.set_title(var)","d6be132a":"examples = ['FOODS_3_090_CA_3','HOBBIES_1_234_CA_3','HOUSEHOLD_1_118_CA_3']\nexample_df = [example, example2, example3]\nfor i in [0, 1, 2]:\n    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 3))\n    example_df[i].groupby('wday').mean()[examples[i]] \\\n        .plot(kind='line',\n              title='average sale: day of week',\n              lw=5,\n              color=color_pal[0],\n              ax=ax1)\n    example_df[i].groupby('month').mean()[examples[i]] \\\n        .plot(kind='line',\n              title='average sale: month',\n              lw=5,\n              color=color_pal[4],\n\n              ax=ax2)\n    example_df[i].groupby('year').mean()[examples[i]] \\\n        .plot(kind='line',\n              lw=5,\n              title='average sale: year',\n              color=color_pal[2],\n\n              ax=ax3)\n    fig.suptitle(f'Trends for item: {examples[i]}',\n                 size=20,\n                 y=1.1)\n    plt.tight_layout()\n    plt.show()\n","c544f14b":"twenty_examples = stv.sample(20, random_state=529) \\\n        .set_index('id')[d_cols] \\\n    .T \\\n    .merge(cal.set_index('d')['date'],\n           left_index=True,\n           right_index=True,\n            validate='1:1') \\\n    .set_index('date')","0874dc33":"fig, axs = plt.subplots(10, 2, figsize=(15, 20))\naxs = axs.flatten()\nax_idx = 0\nfor item in twenty_examples.columns:\n    twenty_examples[item].plot(title=item,\n                              color=next(color_cycle),\n                              ax=axs[ax_idx])\n    ax_idx += 1\nplt.tight_layout()\nplt.show()\n","f64f9a0b":"stv['cat_id'].unique()\n","dedf34ed":"stv.groupby('cat_id').count()['id'] \\\n    .sort_values() \\\n    .plot(kind='barh', figsize=(15, 5), title='Count of Items by Category')\nplt.show()","97b91438":"past_sales = stv.set_index('id')[d_cols] \\\n    .T \\\n    .merge(cal.set_index('d')['date'],\n           left_index=True,\n           right_index=True,\n            validate='1:1') \\\n    .set_index('date')\n\n\nfor i in stv['cat_id'].unique():\n    items_col = [c for c in past_sales.columns if i in c]\n    past_sales[items_col] \\\n        .sum(axis=1) \\\n        .plot(figsize=(15, 5),\n              alpha=0.8,\n              title='Total Sales by Item Type')\nplt.legend(stv['cat_id'].unique())\nplt.show()","7bea4da4":"past_sales_clipped = past_sales.clip(0, 1)\nfor i in stv['cat_id'].unique():\n    items_col = [c for c in past_sales.columns if i in c]\n    (past_sales_clipped[items_col] \\\n        .mean(axis=1) * 100) \\\n        .plot(figsize=(15, 5),\n              alpha=0.8,\n              title='Inventory Sale Percentage by Date',\n              style='.')\nplt.ylabel('% of Inventory with at least 1 sale')\nplt.legend(stv['cat_id'].unique())\nplt.show()\n","8e26767f":"store_list = sellp['store_id'].unique()\nfor s in store_list:\n    store_items = [c for c in past_sales.columns if s in c]\n    past_sales[store_items] \\\n        .sum(axis=1) \\\n        .rolling(90).mean() \\\n        .plot(figsize=(15, 5),\n              alpha=0.8,\n              title='Rolling 90 Day Average Total Sales (10 stores)')\nplt.legend(store_list)\nplt.show()","de9c02e9":"fig, axes = plt.subplots(5, 2, figsize=(15, 10), sharex=True)\naxes = axes.flatten()\nax_idx = 0\nfor s in store_list:\n    store_items = [c for c in past_sales.columns if s in c]\n    past_sales[store_items] \\\n        .sum(axis=1) \\\n        .rolling(7).mean() \\\n        .plot(alpha=1,\n              ax=axes[ax_idx],\n              title=s,\n              lw=3,\n              color=next(color_cycle))\n    ax_idx += 1\n# plt.legend(store_list)\nplt.suptitle('Weekly Sale Trends by Store ID')\nplt.tight_layout()\nplt.show()","1d5ee2aa":"# Lets look at a lot of different items!\nLets put it all together to plot 20 different items and their sales\nSome observations from these plots:\nIt is common to see an item unavailable for a period of time.\nSome items only sell 1 or less in a day, making it very hard to predict.\nOther items show spikes in their demand (super bowl sunday?) possibly the \"events\" provided to us could help with these","e0f7638f":"# Sales broken down by time variables\nNow that we have our example item lets see how it sells by:\nDay of the week\nMonth\nYear\n","bb0e6b40":"# Rollout of items being sold.\nWe can see the some items come into supply that previously didn't exist. Similarly some items stop being sold completely.\nLets plot the sales, but only count if item is selling or not selling (0 -> not selling, >0 -> selling)\nThis plot shows us that many items are being slowly introduced into inventory, so many of them will not register a sale at the beginning of the provided data.","c5de29f1":"* **future versions coming with facebook prophet. **","fa5800c2":"# Calender Visualization","096224df":"# Merging the data with real dates\nWe are given a calendar with additional information about past and future dates.\nThe calendar data can be merged with our days data\nFrom this we can find weekly and annual trends","eb23057b":"Import required  modules","d648edf6":"Looking at the same data a different way, we can plot a rolling 7 day total demand count by store. Note clearly that some stores have abrupt changes in their demand, it could be that the store expanded or a new competitor was built near by. Either way this is imporant to note when creating predictive models about demand pattern.","add56bf6":"![image.png](attachment:image.png)","5de51116":"# Sales by Store\nWe are provided data for 10 unique stores. What are the total sales by stores?\n\nNote that some stores are more steady than others.\nCA_2 seems to have a big change occur in 2015","d90f4b13":"# What exactly are we trying to predict?\nWe are trying for forecast sales for 28 forecast days. The sample submission has the following format:\n\nThe columns represent 28 forecast days. We will fill these forecast days with our predictions.\nThe rows each represent a specific item. This id tells us the item type, state, and store. We don't know what these items are exactly.","2ec7f42b":"<b>The dataset consists of five .csv files.<\/b>\n<ul>\n<li>calendar.csv - Contains the dates on which products are sold. The dates are in a yyyy\/dd\/mm format.<\/li>\n\n<li>sales_train_validation.csv - Contains the historical daily unit sales data per product and store [d_1 - d_1913].<\/li>\n\n<li>submission.csv - Demonstrates the correct format for submission to the competition.<\/li>\n\n<li>sell_prices.csv - Contains information about the price of the products sold per store and date.<\/li>\n\n<li>sales_train_evaluation.csv - Available one month before the competition deadline. It will include sales for [d_1 - d_1941].<\/li>\n<\/ul>\nIn this competition, we need to forecast the sales for [d_1942 - d_1969]. These rows form the evaluation set. The rows [d_1914 - d_1941] form the validation set, and the remaining rows form the training set. Now, since we understand the dataset and know what to predict, let us visualize the dataset.","8bc2f53e":"# Introduction\n\n","6ba1da9a":"# Please Upvote the Kernel , If you like this...","b4fe1422":"Special Thanks to all Great Kernel notebooks Author from where I got this knowledge and combine into this notebook.","d768e00e":"Thanks..","f0269073":"# Sales Counts","f98e95b2":"# Combined Sales over Time by Type\nWe have several item types:\nHobbies\nHousehold\nFoods\nLets plot the total demand over time for each type"}}