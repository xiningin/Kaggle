{"cell_type":{"3b33f84c":"code","f4f31087":"code","dd1f3d51":"code","4532b540":"code","dd356180":"code","3e675928":"code","8456fa21":"code","060a58a9":"code","3a5709a4":"code","bc1d81d7":"code","7aa7835d":"code","2aa6f335":"code","e39da225":"code","7e9581de":"code","e5133cc5":"code","9e199690":"code","c4661b72":"code","3918b1ab":"code","0f223ba5":"code","32b221c8":"code","11c72550":"code","986412a6":"code","7f11e3a6":"code","4f037a2b":"code","d1edd655":"code","e92564b2":"code","1eac5b5c":"code","040d9b06":"code","98e1ca58":"code","b2af88b4":"code","e7b6d1c5":"code","4bf78dea":"code","ac239c21":"code","bcb5612d":"code","b80e3952":"code","28ce161f":"code","1b8d80a7":"code","7c0187d1":"code","489037eb":"code","02587c85":"code","cff07592":"code","353391c0":"code","9bcfb5c9":"code","c0db54e8":"code","17974ff3":"code","8499809d":"code","fca050c3":"code","521ab88a":"code","289d30df":"code","b677d176":"code","e9160901":"code","9099be3f":"code","8cb8ff3d":"code","c7c7a5ef":"code","e99a34e7":"code","e8cfb3b6":"code","4201fa4b":"code","f576280e":"code","bf6c06fe":"code","25325d8d":"code","9c0cbd41":"code","48fd46a6":"code","5f29fffa":"code","c90731b1":"code","d7be2688":"code","395d2e86":"code","22ea7166":"code","728b4314":"code","d805454d":"code","2753db53":"code","ec81afba":"code","9e8f17d2":"code","9449db1f":"code","8b3c6970":"code","6f54e3ed":"code","72fe6476":"code","d0170c67":"code","df068d71":"code","bb14fc26":"code","b84ec806":"code","04272904":"code","f0c98403":"code","94aae28c":"code","c7ad8127":"code","099e3a5b":"code","3cf99aef":"code","3486c575":"code","13eba31c":"code","28d9cc38":"code","1c79d4f1":"code","f48d94b0":"code","5e276418":"code","187d10e9":"code","2d80b01c":"code","79341e80":"code","c2ec7d19":"code","2abfdcb9":"code","5be34831":"code","f1122df0":"code","bb9e178f":"code","1b823810":"code","b591eae8":"code","f4837407":"code","64ef3801":"code","b730e003":"code","7c9c8599":"code","1821450c":"code","ad876e79":"code","6bacfad7":"code","a0e467f4":"code","4aaaa8a0":"code","0875f35c":"code","6a6c1dcf":"code","508d748c":"code","19708bc3":"code","83f6b847":"code","3f11f923":"code","ef2676ff":"code","c99bec53":"code","2a233ab9":"code","9bf3fb42":"code","d200d6cb":"code","120eb38b":"code","2ba322ab":"code","a9af01ec":"code","3ee9aa99":"code","47d35a23":"code","c561db3c":"code","91dece43":"code","333d33bc":"code","48c73a8b":"code","93deb97c":"code","a2bda1fc":"code","c57318b0":"code","70af26f8":"code","5b05505b":"code","bc39009b":"code","9a19e142":"code","50fc1e75":"code","05e6b3ce":"code","79e60654":"code","bd9f4ab4":"code","e4acdfd4":"code","2992f351":"code","d9ab1cf1":"code","723f5937":"code","9e02ef42":"code","b24d5b00":"code","5fc5b388":"code","6ee5a530":"code","cdcf80eb":"code","57eaa671":"code","3b7b23d0":"code","f5fed45c":"code","48519168":"code","56952907":"code","b9530e79":"code","b04e0bc9":"code","f01236cf":"markdown","ee874b66":"markdown","187a5a8b":"markdown","1f1e4ceb":"markdown","0287b44c":"markdown","bebea26f":"markdown","46adaab8":"markdown","0bbcdf6f":"markdown","d88aef85":"markdown","3e853efd":"markdown","4062ad44":"markdown","24cbc7db":"markdown","3422caf7":"markdown","57d14ffd":"markdown","556a7818":"markdown","57e18e14":"markdown","9e3d7cc7":"markdown","5f9c6cfb":"markdown","9d244873":"markdown","dc3f6fef":"markdown","bae29ced":"markdown","b900bc29":"markdown","fff27f46":"markdown","8cc78777":"markdown"},"source":{"3b33f84c":"import pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt \nimport seaborn as sns \nimport datetime as dt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom sklearn.svm import SVC \nfrom sklearn.multioutput import MultiOutputClassifier \nfrom sklearn.ensemble import GradientBoostingClassifier \nfrom sklearn.tree import DecisionTreeClassifier \nfrom sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier \nfrom sklearn.naive_bayes import GaussianNB \nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn.linear_model import LogisticRegression \nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split, GridSearchCV \nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.cluster import KMeans\nfrom imblearn.over_sampling import SMOTE\nfrom collections import Counter\nimport statsmodels.api as sm\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import RFE\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn import metrics\nfrom sklearn.tree import DecisionTreeClassifier\nfrom IPython.display import Image  \nfrom six import StringIO  \nfrom sklearn.tree import export_graphviz\nfrom sklearn.metrics import confusion_matrix, accuracy_score,f1_score,classification_report,log_loss, precision_score,recall_score,precision_recall_curve","f4f31087":"# Loading the data\ndf = pd.read_csv('..\/input\/titanic\/train.csv')","dd1f3d51":"# Viewing the top 5 rows\ndf.head()","4532b540":"# Viewing the shape of the dataset\ndf.shape","dd356180":"# Viewing the info of dataype and null entries\ndf.info()","3e675928":"# Viewing the number of null entries in each column\ndf.isnull().sum()","8456fa21":"mean_age=df['Age'].mean()\nmean_age=int(round(mean_age,0))\nmean_age","060a58a9":"df['Age'].fillna(mean_age,inplace=True)","3a5709a4":"embark_mode=df['Embarked'].mode()\nembark_mode","bc1d81d7":"df['Embarked'].fillna(embark_mode,inplace=True)","7aa7835d":"# Dropping Name and Ticket Number\ndf=df.drop(columns=['Name','Ticket','Cabin','PassengerId'])","2aa6f335":"# Creating a dummy variable for some of the categorical variables and dropping the first one.\ndummy1 = pd.get_dummies(df[['Sex','Embarked']], drop_first=True)\n\n# Adding the results to the master dataframe\ndf = pd.concat([df, dummy1], axis=1)\ndf=df.drop(columns=['Sex','Embarked'])","e39da225":"df.head()","7e9581de":"df.info()","e5133cc5":"# Calculating correlation matrix\ncorr_matrix = df[df.columns].corr()\n# Create a dataframe for the minimum correlation coefficient values\ncorr_df = pd.DataFrame(corr_matrix.min())\n# Rename the column label of corr_df\ncorr_df.columns = ['MinCorrelationCoeff']\n# Calculate the maximum correlation coefficient value apart less than 1.\ncorr_df['MaxCorrelationCoeff'] = corr_matrix[corr_matrix < 1].max()\ncorr_df","9e199690":"plt.figure(figsize = (40, 30))\nsns.heatmap(corr_matrix, annot = True, linewidths=0.2, fmt=\".2f\", annot_kws={\"fontsize\":20});","c4661b72":"## Splitting Data into Train and Test\nX=df.drop(columns='Survived')\ny=df['Survived']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=24)","3918b1ab":"counter=Counter(y_train)\nprint(\"Before\",counter)\n\n# Oversampling the training dataset using SMOTE\nsmt=SMOTE()\nX_train_sm,y_train_sm=smt.fit_resample(X_train,y_train)\n\ncounter=Counter(y_train_sm)\nprint(\"After\",counter)","0f223ba5":"# Logistic regression model\nlogm1 = sm.GLM(y_train_sm,(sm.add_constant(X_train_sm)), family = sm.families.Binomial())\nlogm1.fit().summary()","32b221c8":"# Choosing only those columns from RFE\ncol = X_train_sm.columns","11c72550":"# Applying GLM\nX_train_sm = sm.add_constant(X_train_sm[col])\nlogm2 = sm.GLM(y_train_sm,X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","986412a6":"# Viewing the VIF\nvif = pd.DataFrame()\nvif['Features'] = X_train_sm[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train_sm[col].values, i) for i in range(X_train_sm[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","7f11e3a6":"# Dropping High VIF column\ncol = col.drop('Pclass', 1)","4f037a2b":"# Applying GLM\nX_train_sm = sm.add_constant(X_train_sm[col])\nlogm2 = sm.GLM(y_train_sm,X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","d1edd655":"# Viewing the VIF\nvif = pd.DataFrame()\nvif['Features'] = X_train_sm[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train_sm[col].values, i) for i in range(X_train_sm[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","e92564b2":"# Getting the predicted values on the train set\ny_train_pred = res.predict(X_train_sm)\ny_train_pred[:10]","1eac5b5c":"# Reshaping the dataset\ny_train_pred = y_train_pred.values.reshape(-1)\ny_train_pred[:10]","040d9b06":"# Renaming the column and viewing the dataset\ny_train_pred_final = pd.DataFrame({'SurvivalActuals':y_train_sm.values, 'Survival_Prob':y_train_pred})\ny_train_pred_final.head()","98e1ca58":"# Creating the probability dataframe\nnumbers=[float(x)\/10 for x in range(10)]\nfor i in numbers:\n    y_train_pred_final[i]=y_train_pred_final.Survival_Prob.map(lambda x:1 if x > i else 0)\ny_train_pred_final.head()","b2af88b4":"# Creating the probability dataframe\ncutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\nnum = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i in num:\n    cm1 = metrics.confusion_matrix(y_train_pred_final.SurvivalActuals, y_train_pred_final[i] )\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])\/total1\n    \n    speci = cm1[0,0]\/(cm1[0,0]+cm1[0,1])\n    sensi = cm1[1,1]\/(cm1[1,0]+cm1[1,1])\n    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\nprint(cutoff_df)","e7b6d1c5":"# Plotting the accuracy-sensitivity-specificity curve\ncutoff_df.plot.line(x='prob',y=['accuracy','sensi','speci'])\nplt.show()","4bf78dea":"# Making predictions on the train set\ny_train_pred_final['final_predicted'] = y_train_pred_final.Survival_Prob.map(lambda x: 1 if x > 0.375 else 0)\n\n# Let's see the head\ny_train_pred_final.head()","ac239c21":"# Accuracy Metric\nmetrics.accuracy_score(y_train_pred_final.SurvivalActuals,y_train_pred_final.final_predicted)","bcb5612d":"# Confusion Matrix\nconfusion=metrics.confusion_matrix(y_train_pred_final.SurvivalActuals,y_train_pred_final.final_predicted)\nconfusion","b80e3952":"TP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives","28ce161f":"# Let's see the sensitivity of our logistic regression model\nTP \/ float(TP+FN)","1b8d80a7":"# Let us calculate specificity\nTN \/ float(TN+FP)","7c0187d1":"# Precision Score\nprecision_score(y_train_pred_final.SurvivalActuals,y_train_pred_final.final_predicted)","489037eb":"# Recall Score\nrecall_score(y_train_pred_final.SurvivalActuals,y_train_pred_final.final_predicted)","02587c85":"p,r,thresholds=precision_recall_curve(y_train_pred_final.SurvivalActuals,y_train_pred_final.Survival_Prob)","cff07592":"# Viewing the precision recall curve\nplt.plot(thresholds,p[:-1],\"g-\")\nplt.plot(thresholds,r[:-1],\"r-\")\nplt.show()","353391c0":"# Transforming test set with standard scaler and predicting\nX_test_logtest=X_test.copy()\nX_test_logtest=X_test_logtest[col]\nX_test_sm = sm.add_constant(X_test_logtest)\ny_test_pred = res.predict(X_test_sm)","9bcfb5c9":"# Converting y_pred to a dataframe which is an array\ny_pred_1 = pd.DataFrame(y_test_pred)\n\n# Converting y_test to dataframe\ny_test_df = pd.DataFrame(y_test)\n\n# Removing index for both dataframes to append them side by side \ny_pred_1.reset_index(drop=True, inplace=True)\ny_test_df.reset_index(drop=True, inplace=True)\n\n# Appending y_test_df and y_pred_1\ny_pred_final = pd.concat([y_test_df, y_pred_1],axis=1)\n\n# Viewing the dataset\ny_pred_final.head()","c0db54e8":"# Renaming the column \ny_pred_final= y_pred_final.rename(columns={ 0 : 'Survival_Prob'})\n\n# Let's see the head of y_pred_final\ny_pred_final.head()","17974ff3":"# Making predictions in the test set\ny_pred_final['final_predicted'] = y_pred_final.Survival_Prob.map(lambda x: 1 if x > 0.375 else 0)\n\n# Viewing the dataset\ny_pred_final.head()","8499809d":"# Let's check the overall accuracy.\nmetrics.accuracy_score(y_pred_final.Survived, y_pred_final.final_predicted)","fca050c3":"# Confusion matrix \nconfusion2 = metrics.confusion_matrix(y_pred_final.Survived, y_pred_final.final_predicted )\nconfusion2","521ab88a":"TP = confusion2[1,1] # true positive \nTN = confusion2[0,0] # true negatives\nFP = confusion2[0,1] # false positives\nFN = confusion2[1,0] # false negatives","289d30df":"# Let's see the sensitivity of our logistic regression model\nTP \/ float(TP+FN)","b677d176":"# Let us calculate specificity\nTN \/ float(TN+FP)","e9160901":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=24)\ncounter=Counter(y_train)\nprint(\"Before\",counter)\n\n# Oversampling the training dataset using SMOTE\nsmt=SMOTE()\nX_train_sm,y_train_sm=smt.fit_resample(X_train,y_train)\n\ncounter=Counter(y_train_sm)\nprint(\"After\",counter)","9099be3f":"# Creating decision tree classifier object and doing a 'fit'\ndt = DecisionTreeClassifier(max_depth=3)\ndt.fit(X_train_sm, y_train_sm)","8cb8ff3d":"# Predicting using the model on train and test data\ny_train_pred = dt.predict(X_train_sm)\ny_test_pred = dt.predict(X_test)","c7c7a5ef":"# Function for Checking accuracy scores for train and test data and printing confusion matrix\ndef evaluate_model(dt_classifier):\n    print(\"Train Accuracy :\", accuracy_score(y_train_sm, dt_classifier.predict(X_train_sm)))\n    print(\"Train Confusion Matrix:\")\n    print(confusion_matrix(y_train_sm, dt_classifier.predict(X_train_sm)))\n    print(\"-\"*50)\n    print(\"Test Accuracy :\", accuracy_score(y_test, dt_classifier.predict(X_test)))\n    print(\"Test Confusion Matrix:\")\n    print(confusion_matrix(y_test, dt_classifier.predict(X_test)))","e99a34e7":"# Checking accuracy scores for train and test data and printing confusion matrix\nevaluate_model(dt)","e8cfb3b6":"# Confusion Matrix for Train\nconfusion_train=metrics.confusion_matrix(y_train_sm,dt.predict(X_train_sm))\nconfusion_train","4201fa4b":"TP = confusion_train[1,1] # true positive \nTN = confusion_train[0,0] # true negatives\nFP = confusion_train[0,1] # false positives\nFN = confusion_train[1,0] # false negatives","f576280e":"# Let's see the sensitivity of Train Set\nTP \/ float(TP+FN)","bf6c06fe":"# Let us calculate specificity of Train Set\nTN \/ float(TN+FP)","25325d8d":"# Confusion Matrix for Test\nconfusion_test=metrics.confusion_matrix(y_test,dt.predict(X_test))\nconfusion_test","9c0cbd41":"TP = confusion_test[1,1] # true positive \nTN = confusion_test[0,0] # true negatives\nFP = confusion_test[0,1] # false positives\nFN = confusion_test[1,0] # false negatives","48fd46a6":"# Let's see the sensitivity of Test Set\nTP \/ float(TP+FN)","5f29fffa":"# Let us calculate specificity of Test Set\nTN \/ float(TN+FP)","c90731b1":"dt.predict_proba(X_test[:5])","d7be2688":"feats = {} # a dict to hold feature_name: feature_importance\nfor feature, importance in zip(X_train.columns, dt.feature_importances_):\n    feats[feature] = importance #add the name\/value pair \n\n#importances = pd.DataFrame.from_dict(feats, orient='index').rename(columns={0: 'Importance'})\n#importances.sort_values(by='Gini-importance').plot(kind='bar', rot=45)\nfeats_df=pd.DataFrame(feats.items(), columns=['Feature', 'Importance'])\nfeats_df.sort_values(by=['Importance'],ascending=False).head(5)\n#feats_df.head(5)","395d2e86":"# Creating decision tree classifier object\ndt = DecisionTreeClassifier(random_state=42)","22ea7166":"# Create the parameter grid based on the results of random search \nparams = {\n    'max_depth': [2, 3, 5, 10, 20],\n    'min_samples_leaf': [5, 10, 20, 50, 100],\n    'criterion': [\"gini\", \"entropy\"]\n}","728b4314":"# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator=dt, \n                           param_grid=params, \n                           cv=4, n_jobs=-1, verbose=1, scoring = \"accuracy\")","d805454d":"%%time\n# Using 'fit' with hyperparameters\ngrid_search.fit(X_train_sm, y_train_sm)","2753db53":"# Checking the scores of the grid_search_cv\nscore_df = pd.DataFrame(grid_search.cv_results_)\nscore_df.head()","ec81afba":"# Checking the mean test score of top 5\nscore_df.nlargest(5,\"mean_test_score\")","9e8f17d2":"# Choosing the best estimator model\ngrid_search.best_estimator_\ndt_best = grid_search.best_estimator_","9449db1f":"# Checking accuracy scores for train and test data and printing confusion matrix\nevaluate_model(dt_best)","8b3c6970":"# Confusion Matrix for Train\nconfusion_train=metrics.confusion_matrix(y_train_sm,dt_best.predict(X_train_sm))\nconfusion_train","6f54e3ed":"TP = confusion_train[1,1] # true positive \nTN = confusion_train[0,0] # true negatives\nFP = confusion_train[0,1] # false positives\nFN = confusion_train[1,0] # false negatives","72fe6476":"# Let's see the sensitivity of Train Set\nTP \/ float(TP+FN)","d0170c67":"# Let us calculate specificity of Train Set\nTN \/ float(TN+FP)","df068d71":"# Confusion Matrix for Test\nconfusion_test=metrics.confusion_matrix(y_test,dt_best.predict(X_test))\nconfusion_test","bb14fc26":"TP = confusion_test[1,1] # true positive \nTN = confusion_test[0,0] # true negatives\nFP = confusion_test[0,1] # false positives\nFN = confusion_test[1,0] # false negatives","b84ec806":"# Let's see the sensitivity of Test Set\nTP \/ float(TP+FN)","04272904":"# Let us calculate specificity of Test Set\nTN \/ float(TN+FP)","f0c98403":"dt_best.predict_proba(X_test[:5])","94aae28c":"feats = {} # a dict to hold feature_name: feature_importance\nfor feature, importance in zip(X_train.columns, dt_best.feature_importances_):\n    feats[feature] = importance #add the name\/value pair \n\n#importances = pd.DataFrame.from_dict(feats, orient='index').rename(columns={0: 'Importance'})\n#importances.sort_values(by='Gini-importance').plot(kind='bar', rot=45)\nfeats_df=pd.DataFrame(feats.items(), columns=['Feature', 'Importance'])\nfeats_df.sort_values(by=['Importance'],ascending=False).head(5)\n#feats_df.head(5)","c7ad8127":"# Creating the random forest object\nrf = RandomForestClassifier(random_state=42, n_estimators=10, max_depth=3)","099e3a5b":"# Performing the 'fit'\nrf.fit(X_train_sm, y_train_sm)","3cf99aef":"# Checking accuracy scores for train and test data and printing confusion matrix\nevaluate_model(rf)","3486c575":"# Confusion Matrix for Train\nconfusion_train=metrics.confusion_matrix(y_train_sm,rf.predict(X_train_sm))\nconfusion_train","13eba31c":"TP = confusion_train[1,1] # true positive \nTN = confusion_train[0,0] # true negatives\nFP = confusion_train[0,1] # false positives\nFN = confusion_train[1,0] # false negatives","28d9cc38":"# Let's see the sensitivity of Train Set\nTP \/ float(TP+FN)","1c79d4f1":"# Let us calculate specificity of Train Set\nTN \/ float(TN+FP)","f48d94b0":"# Confusion Matrix for Test\nconfusion_test=metrics.confusion_matrix(y_test,rf.predict(X_test))\nconfusion_test","5e276418":"TP = confusion_test[1,1] # true positive \nTN = confusion_test[0,0] # true negatives\nFP = confusion_test[0,1] # false positives\nFN = confusion_test[1,0] # false negatives","187d10e9":"# Let's see the sensitivity of Test Set\nTP \/ float(TP+FN)","2d80b01c":"# Let us calculate specificity of Test Set\nTN \/ float(TN+FP)","79341e80":"rf.predict_proba(X_test[:5])","c2ec7d19":"feats = {} # a dict to hold feature_name: feature_importance\nfor feature, importance in zip(X_train.columns, rf.feature_importances_):\n    feats[feature] = importance #add the name\/value pair \n\n#importances = pd.DataFrame.from_dict(feats, orient='index').rename(columns={0: 'Importance'})\n#importances.sort_values(by='Gini-importance').plot(kind='bar', rot=45)\nfeats_df=pd.DataFrame(feats.items(), columns=['Feature', 'Importance'])\nfeats_df.sort_values(by=['Importance'],ascending=False).head(5)\n#feats_df.head(5)","2abfdcb9":"# Creating the Random Forest Object\nclassifier_rf = RandomForestClassifier(random_state=42, n_jobs=-1)","5be34831":"# Create the parameter grid based on the results of random search \nparams = {\n    'max_depth': [5, 10],\n    'min_samples_leaf': [5, 10, 20, 50],\n    'max_features': [2,3,4],\n    'n_estimators': [30, 50, 100]\n}","f1122df0":"# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator=classifier_rf, param_grid=params, \n                          cv=4, n_jobs=-1, verbose=1, scoring = \"accuracy\")","bb9e178f":"%%time\n# Apllying the 'fit'\ngrid_search.fit(X_train_sm,y_train_sm)","1b823810":"# Choosing the best estimator\nrf_best = grid_search.best_estimator_\nrf_best","b591eae8":"# Checking accuracy scores for train and test data and printing confusion matrix\nevaluate_model(rf_best)","f4837407":"# Choosing a sample tree\nsample_tree = rf_best.estimators_[0]","64ef3801":"# Confusion Matrix for Train\nconfusion_train=metrics.confusion_matrix(y_train_sm,rf_best.predict(X_train_sm))\nconfusion_train","b730e003":"TP = confusion_train[1,1] # true positive \nTN = confusion_train[0,0] # true negatives\nFP = confusion_train[0,1] # false positives\nFN = confusion_train[1,0] # false negatives","7c9c8599":"# Let's see the sensitivity of Train Set\nTP \/ float(TP+FN)","1821450c":"# Let us calculate specificity of Train Set\nTN \/ float(TN+FP)","ad876e79":"# Confusion Matrix for Test\nconfusion_test=metrics.confusion_matrix(y_test,rf_best.predict(X_test))\nconfusion_test","6bacfad7":"TP = confusion_test[1,1] # true positive \nTN = confusion_test[0,0] # true negatives\nFP = confusion_test[0,1] # false positives\nFN = confusion_test[1,0] # false negatives","a0e467f4":"# Let's see the sensitivity of Test Set\nTP \/ float(TP+FN)","4aaaa8a0":"# Let us calculate specificity of Test Set\nTN \/ float(TN+FP)","0875f35c":"rf_best.predict_proba(X_test[:5])","6a6c1dcf":"feats = {} # a dict to hold feature_name: feature_importance\nfor feature, importance in zip(X_train.columns, rf_best.feature_importances_):\n    feats[feature] = importance #add the name\/value pair \n\n#importances = pd.DataFrame.from_dict(feats, orient='index').rename(columns={0: 'Importance'})\n#importances.sort_values(by='Gini-importance').plot(kind='bar', rot=45)\nfeats_df=pd.DataFrame(feats.items(), columns=['Feature', 'Importance'])\nfeats_df.sort_values(by=['Importance'],ascending=False).head(5)\n#feats_df.head(5)","508d748c":"# Creating the adaboost object\nadaboost =  AdaBoostClassifier(n_estimators=200, random_state=42)","19708bc3":"# Applying the 'fit'\nadaboost.fit(X_train_sm, y_train_sm)","83f6b847":"# Predicting for the test data set\ny_pred = adaboost.predict(X_test)","3f11f923":"# Checking accuracy scores for train and test data and printing confusion matrix\nevaluate_model(adaboost)","ef2676ff":"# Confusion Matrix for Train\nconfusion_train=metrics.confusion_matrix(y_train_sm,adaboost.predict(X_train_sm))\nconfusion_train","c99bec53":"# Let's see the sensitivity of Train Set\nTP \/ float(TP+FN)","2a233ab9":"# Let us calculate specificity of Train Set\nTN \/ float(TN+FP)","9bf3fb42":"# Confusion Matrix for Test\nconfusion_test=metrics.confusion_matrix(y_test,adaboost.predict(X_test))\nconfusion_test","d200d6cb":"TP = confusion_test[1,1] # true positive \nTN = confusion_test[0,0] # true negatives\nFP = confusion_test[0,1] # false positives\nFN = confusion_test[1,0] # false negatives","120eb38b":"# Let's see the sensitivity of Test Set\nTP \/ float(TP+FN)","2ba322ab":"# Let us calculate specificity of Test Set\nTN \/ float(TN+FP)","a9af01ec":"adaboost.predict_proba(X_test[:5])","3ee9aa99":"feats = {} # a dict to hold feature_name: feature_importance\nfor feature, importance in zip(X_train.columns, adaboost.feature_importances_):\n    feats[feature] = importance #add the name\/value pair \n\n#importances = pd.DataFrame.from_dict(feats, orient='index').rename(columns={0: 'Importance'})\n#importances.sort_values(by='Gini-importance').plot(kind='bar', rot=45)\nfeats_df=pd.DataFrame(feats.items(), columns=['Feature', 'Importance'])\nfeats_df.sort_values(by=['Importance'],ascending=False).head(5)\n#feats_df.head(5)","47d35a23":"# Creating the GBM object\ngb = GradientBoostingClassifier(random_state=42)\ngb.get_params()","c561db3c":"# Fit the model to our train and target\ngb.fit(X_train_sm,y_train_sm)\n# Get our predictions\ngb_predictions = gb.predict(X_test)","91dece43":"# Predicting the probabilities\ngb_predictions_prob = gb.predict_proba(X_test)\ngb_predictions_prob","333d33bc":"# Checking accuracy scores for train and test data and printing confusion matrix\nevaluate_model(gb)","48c73a8b":"# Printing the classification Report\nprint(classification_report(y_test, gb.predict(X_test)))","93deb97c":"# Confusion Matrix for Train\nconfusion_train=metrics.confusion_matrix(y_train_sm,gb.predict(X_train_sm))\nconfusion_train","a2bda1fc":"TP = confusion_train[1,1] # true positive \nTN = confusion_train[0,0] # true negatives\nFP = confusion_train[0,1] # false positives\nFN = confusion_train[1,0] # false negatives","c57318b0":"# Let's see the sensitivity of Train Set\nTP \/ float(TP+FN)","70af26f8":"# Let us calculate specificity of Train Set\nTN \/ float(TN+FP)","5b05505b":"# Confusion Matrix for Test\nconfusion_test=metrics.confusion_matrix(y_test,gb.predict(X_test))\nconfusion_test","bc39009b":"TP = confusion_test[1,1] # true positive \nTN = confusion_test[0,0] # true negatives\nFP = confusion_test[0,1] # false positives\nFN = confusion_test[1,0] # false negatives","9a19e142":"# Let's see the sensitivity of Test Set\nTP \/ float(TP+FN)","50fc1e75":"# Let us calculate specificity of Test Set\nTN \/ float(TN+FP)","05e6b3ce":"gb.predict_proba(X_test[:5])","79e60654":"feats = {} # a dict to hold feature_name: feature_importance\nfor feature, importance in zip(X_train.columns, gb.feature_importances_):\n    feats[feature] = importance #add the name\/value pair \n\n#importances = pd.DataFrame.from_dict(feats, orient='index').rename(columns={0: 'Importance'})\n#importances.sort_values(by='Gini-importance').plot(kind='bar', rot=45)\nfeats_df=pd.DataFrame(feats.items(), columns=['Feature', 'Importance'])\nfeats_df.sort_values(by=['Importance'],ascending=False).head(5)\n#feats_df.head(5)","bd9f4ab4":"models_pca = [['Logistic Regression',79.26,81.96,76.54,80.27,84.03,73.42], ['Decision Trees',84.07,90.12,78.02,81.17,88.89,67.09],['Decision Trees after Hyper-ParameterTuning',89.38,91.85,86.91,81.61,86.11,73.42],['Random Forest',83.46,91.85,75.06,83.41,91.67,68.35],['Random Forest after Hyper-Parameter Tuning',90.12,93.33,86.91,82.96,89.58,70.89],['Adaboost',87.04,89.58,70.89,82.06,84.03,78.48],['GBM',90.62,93.09,88.15,82.06,88.89,69.62]]\nmodels_pca=pd.DataFrame(models_pca, columns=[\"Model\", \"Train Accuracy\",\"Train Specificity\",\"Train Sensitivity\",\"Test Accuracy\",\"Test Specificity\",\"Test Sensitivity\"])\nmodels_pca","e4acdfd4":"test_df=pd.read_csv(\"..\/input\/titanic\/test.csv\")","2992f351":"test_df.shape","d9ab1cf1":"test_df.info()","723f5937":"# Dropping Name and Ticket Number\ntest_df=test_df.drop(columns=['Name','Ticket','Cabin','PassengerId'])","9e02ef42":"# Creating a dummy variable for some of the categorical variables and dropping the first one.\ndummy1 = pd.get_dummies(test_df[['Sex','Embarked']], drop_first=True)\n\n# Adding the results to the master dataframe\ntest_df= pd.concat([test_df, dummy1], axis=1)\ntest_df=test_df.drop(columns=['Sex','Embarked'])","b24d5b00":"test_df.info()","5fc5b388":"mean_age=test_df['Age'].mean()\nmean_age=int(round(mean_age,0))\nmean_age","6ee5a530":"test_df.fillna(mean_age,inplace=True)","cdcf80eb":"result_arr=rf_best.predict_proba(test_df)\nre_dataframe = pd.DataFrame(result_arr) ","57eaa671":"re_dataframe['Survived']=np.where(re_dataframe[1]>re_dataframe[0],1,0)","3b7b23d0":"re_dataframe","f5fed45c":"test_ids=pd.read_csv('..\/input\/titanic\/test.csv')","48519168":"test_ids=test_ids[['PassengerId']]","56952907":"result_df=pd.concat([test_ids,re_dataframe], axis=1)","b9530e79":"result_df=result_df[['PassengerId','Survived']]\nresult_df","b04e0bc9":"result_df","f01236cf":"## <font color=blue>Developed by Adhithia R","ee874b66":"## <font color=green>RANDOM FOREST <a name='no11' \/>","187a5a8b":"## <font color=green> Hyper-Parameter Tuning for Random Forest <a name='no12' \/>","1f1e4ceb":"## <font color=red>4. Data Visualization <a name='no4' \/>","0287b44c":"### 2. Dropping Redundant Columns","bebea26f":"### 3. Creating Dummy Variables","46adaab8":"## <font color=green> Hyper-Parameter Tuning for Decision Trees <a name='no10' \/>","0bbcdf6f":"## <font color=green> DECISION TREE CLASSIFIER <a name='no9' \/>","d88aef85":"## <font color=green>GBM CLASSIFIER<a name='no14' \/>","3e853efd":"# <font color=blue>TITANIC - MACHINE LEARNING FROM DISASTER","4062ad44":"## <font color=red>3. Data Cleaning <a name='no3' \/>","24cbc7db":"## <font color=green>8. Results<a name='no15' \/>","3422caf7":"## <font color=green>Bookmarks to Notebook Sections\n<font color=blue>\n1. Go to <a href=#no1>Improrting necessary libraries<\/a><br>\n2. Go to <a href=#no2>Understanding the dataset<\/a><br>\n3. Go to <a href=#no3>Data Cleaning<\/a><br>\n4. Go to <a href=#no4>Data Visualization<\/a><br>\n5. Go to <a href=#no5>Splitting the Data into Training and Testing Sets<\/a><br>\n6. Go to <a href=#no6>Class Imbalance Treating<\/a><br>\n7. Go to <a href=#no7>Model Building<\/a><br>\n7.1 Go to <a href=#no8>Logistic Regression<\/a><br>\n7.2 Go to <a href=#no9>Decision Tree Classifier<\/a><br>\n7.3 Go to <a href=#no10>Decision Tree Classifier with Hyper Parameter Tuning<\/a><br>\n7.4 Go to <a href=#no11>Random Forest<\/a><br>\n7.5 Go to <a href=#no12>Random Forest with Hyper Paramter Tuning<\/a><br>\n7.6 Go to <a href=#no13>Adaboost<\/a><br>\n7.7 Go to <a href=#no14>GBM<\/a><br>\n8. Go to <a href=#no15>Results<\/a><br>\n9. Go to <a href=#no16>Developing Results for the Test Data Set Provided<\/a><br>","57d14ffd":"## Links to Various Parts of the Notebook","556a7818":"## <font color=red>6. Class Imbalance Treating <a name='no6' \/>","57e18e14":"## <font color=red> 7. Model Building <a name='no7' \/>","9e3d7cc7":"## <font color=red>END","5f9c6cfb":"## <font color=green> 9. Developing Results for the Test Data Set Provided <a name='no16' \/>","9d244873":"## <font color=red>2. Understanding the dataset <a name='no2' \/>","dc3f6fef":"## <font color=green>ADABOOST CLASSIFIER<a name='no13' \/>","bae29ced":"## <font color=red>1. Importing Necessary Libraries <a name='no1' \/>","b900bc29":"### 1. Treating Null Values","fff27f46":"## <font color=green> LOGISTIC REGRESSION <a name='no8' \/>","8cc78777":"## <font color=red>5. Splitting the Data into Training and Testing <a name='no5' \/>"}}