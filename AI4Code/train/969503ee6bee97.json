{"cell_type":{"b4453064":"code","71023900":"code","b94ea039":"code","963036e2":"code","bda5eca4":"code","f118dbf3":"code","8768535d":"code","a50df074":"code","5c70e164":"code","931190ac":"code","f99ac551":"code","b0e40e8a":"code","d81bfa78":"code","81e5c943":"code","3e5696b1":"code","e03a0231":"code","41bd458c":"code","5d71cf6a":"code","ed42803d":"code","f1d48fc1":"code","728d1922":"markdown","12995172":"markdown","af833668":"markdown","8b2c265b":"markdown","b3fe0a92":"markdown","93a89aa2":"markdown","f14dd763":"markdown","96f8ec1d":"markdown","5966356c":"markdown","e75fc5d6":"markdown","fb5770a7":"markdown","001f10cc":"markdown","80f81fd9":"markdown","017986f7":"markdown","0f22c4e3":"markdown","bf925dbc":"markdown","5f4dc8c7":"markdown","f485f1e4":"markdown"},"source":{"b4453064":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","71023900":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport nltk\nimport string\nimport re\nfrom sklearn.model_selection import train_test_split\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import TweetTokenizer\nnltk.download('stopwords')","b94ea039":"data = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')","963036e2":"data.columns","bda5eca4":"data.head()","f118dbf3":"ax = sns.countplot(x=\"target\",data=data)","8768535d":"X_train, X_test, y_train, y_test = train_test_split(data[['text']], data[['target']], test_size=0.40, random_state=42)","a50df074":"X_train = np.array(X_train)","5c70e164":"def process_tweet(tweet):\n    \"\"\"Process tweet function.\n    Input:\n        tweet: a string containing a tweet\n    Output:\n        tweets_clean: a list of words containing the processed tweet\n\n    \"\"\"\n    stemmer = PorterStemmer()\n    stopwords_english = stopwords.words('english')\n    # remove stock market tickers like $GE\n    tweet = re.sub(r'\\$\\w*', '', tweet)\n    # remove old style retweet text \"RT\"\n    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n    # remove hyperlinks\n    tweet = re.sub(r'https?:\\\/\\\/.*[\\r\\n]*', '', tweet)\n    # remove hashtags\n    # only removing the hash # sign from the word\n    tweet = re.sub(r'#', '', tweet)\n    # tokenize tweets\n    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n                               reduce_len=True)\n    tweet_tokens = tokenizer.tokenize(tweet)\n\n    tweets_clean = []\n    for word in tweet_tokens:\n        if (word not in stopwords_english and  # remove stopwords\n                word not in string.punctuation):  # remove punctuation\n            # tweets_clean.append(word)\n            stem_word = stemmer.stem(word)  # stemming word\n            tweets_clean.append(stem_word)\n\n    return tweets_clean","931190ac":"# test the function below\nprint('This is an example of a tweet denoting the disaster: \\n',str(X_train[40]))\nprint('\\nThis is an example of the processed version of the tweet: \\n', process_tweet(str(X_train[40])))","f99ac551":"def build_freqs(tweets, ys):\n    \"\"\"Build frequencies.\n    Input:\n        tweets: a list of tweets\n        ys: an m x 1 array with the sentiment label of each tweet\n            (either 0 or 1)\n    Output:\n        freqs: a dictionary mapping each (word, sentiment) pair to its\n        frequency\n    \"\"\"\n    # Convert np array to list since zip needs an iterable.\n    # The squeeze is necessary or the list ends up with one element.\n    # Also note that this is just a NOP if ys is already a list.\n    yslist = np.squeeze(ys).tolist()\n\n    # Start with an empty dictionary and populate it by looping over all tweets\n    # and over all processed words in each tweet.\n    freqs = {}\n    for y, tweet in zip(yslist, tweets):\n        for word in process_tweet(str(tweet)):\n            pair = (word, y)\n            if pair in freqs:\n                freqs[pair] += 1\n            else:\n                freqs[pair] = 1    \n    return freqs","b0e40e8a":"def extract_features(tweet, freqs):\n    '''\n    Input: \n        tweet: a list of words for one tweet\n        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n    Output: \n        x: a feature vector of dimension (1,3)\n    '''\n    # process_tweet tokenizes, stems, and removes stopwords\n    word_l = process_tweet(tweet)\n    \n    # 3 elements in the form of a 1 x 3 vector\n    x = np.zeros((1, 3)) \n    \n    #bias term is set to 1\n    x[0,0] = 1 \n    \n    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n    # loop through each word in the list of words\n    for word in word_l:\n        \n        # increment the word count for the positive label 1\n        if (word,1.0) in freqs:\n            x[0,1] += freqs[(word,1.0)]\n        \n        \n        # increment the word count for the negative label 0\n        if (word,0) in freqs:\n            x[0,2] += freqs[(word,0)]\n        \n    ### END CODE HERE ###\n    assert(x.shape == (1, 3))\n    return x","d81bfa78":"# create frequency dictionary\nfreqs = build_freqs(X_train, y_train)\n\n# check the output\nprint(\"type(freqs) = \" + str(type(freqs)))\nprint(\"len(freqs) = \" + str(len(freqs.keys())))","81e5c943":"print(freqs)","3e5696b1":"# select some words to appear in the report. we will assume that each word is unique (i.e. no duplicates)\nkeys = ['happi', 'death', 'nice', 'good', 'bad', 'sad', 'mad', 'flood', 'suicid',\n        'damag', ':)', ':(', 'threat', 'outbreak', 'threat', 'love', 'fire',\n        'song', 'idea', 'power', 'play', 'magnific','windstorm','cyclon','panic','bomb','black','nuclear']\n\n# list representing our table of word counts.\n# each element consist of a sublist with this pattern: [<word>, <positive_count>, <negative_count>]\ndata = []\n\n# loop through our selected words\nfor word in keys:\n    \n    # initialize positive and negative counts\n    pos = 0\n    neg = 0\n    \n    # retrieve number of positive counts\n    if (word, 1) in freqs:\n        pos = freqs[(word, 1)]\n        \n    # retrieve number of negative counts\n    if (word, 0) in freqs:\n        neg = freqs[(word, 0)]\n        \n    # append the word counts to the table\n    data.append([word, pos, neg])\n    \ndata","e03a0231":"fig, ax = plt.subplots(figsize = (12, 12))\n\n# convert positive raw counts to logarithmic scale. we add 1 to avoid log(0)\nx = np.log([x[1] + 1 for x in data])  \n\n# do the same for the negative counts\ny = np.log([x[2] + 1 for x in data]) \n\n# Plot a dot for each pair of words\nax.scatter(x, y)  \n\n# assign axis labels\nplt.xlabel(\"Log Positive count(disaster)\")\nplt.ylabel(\"Log Negative count\")\n\n# Add the word as the label at the same position as you added the points just before\nfor i in range(0, len(data)):\n    ax.annotate(data[i][0], (x[i], y[i]), fontsize=12)\n\nax.plot([0, 9], [0, 9], color = 'red') # Plot the red line that divides the 2 areas.\nplt.show()","41bd458c":"# collect the features 'x' and stack them into a matrix 'X'\nX = np.zeros((len(X_train), 3))\nfor i in range(len(X_train)):\n    X[i, :]= extract_features(str(X_train[i]), freqs)\n\n# training labels corresponding to X\nY = y_train\n\nfrom sklearn.linear_model import LogisticRegression\n\n# Apply gradient descent\nclf = LogisticRegression(max_iter=1000).fit(X, Y)\n","5d71cf6a":"from sklearn.metrics import accuracy_score\ny_train_pred = clf.predict(X)\n\ntrain_score = accuracy_score(Y,y_train_pred)\nprint(train_score)","ed42803d":"X_test = np.array(X_test)\n# collect the features 'x' and stack them into a matrix 'X'\nX_t = np.zeros((len(X_test), 3))\nfor i in range(len(X_test)):\n    X_t[i, :]= extract_features(str(X_test[i]), freqs)","f1d48fc1":"y_test_pred = clf.predict(X_t)\n\ntest_score = accuracy_score(y_test,y_test_pred)\nprint(test_score)","728d1922":"## Plotting the word with their freqency:\nHere I sampled some of the words that occured in my frequency dictionary and plotted them.","12995172":"## Let us check our training score","af833668":"Here I create the frequency dictionary with variable name 'freqs' with the help of procedure described above.","8b2c265b":"## How the freq dictionary looks like?","b3fe0a92":"## Function for creating the freqency dictionary:","93a89aa2":"### Import the packages","f14dd763":"## Function for extracting the features given a tweet \n* This function takes in a single tweet.\n* Process the tweet using the `process_tweet()` function and save the list of tweet words.\n* Loop through each word in the list of processed words\n    * For each word, check the `freqs` dictionary for the count when that word has a positive '1' label. (Check for the key (word, 1.0)\n    * Do the same for the count for when the word is associated with the negative label '0'. (Check for the key (word, 0.0).)\n\n","96f8ec1d":"### Perform train_test_split():","5966356c":"As we can see that words like, 'cyclon','outbreak','suicid','bomb','nuclear' etc are more on log positive count which denotes that they are most occuring words for tweets about disasters.\n\nwhereas, words like, 'play','nice','love','song' etc are more about tweets not for disasters.\n\nThe words near the red line denotes that these words are occuring in equal number of times in both types of tweets.","e75fc5d6":"### Read the data","fb5770a7":"We only require to use the column 'text' and 'target'. The column text has the tweets which needs to be preprocessed.","001f10cc":"## Conclusion:\n\nIn this notebook, we saw how can we convert the tweets into features and use it to train our logistic regression model to classify the tweets. Comments and thoughts are invited. Thanks for going through this notebook.","80f81fd9":"## Feature extraction approach for logistic regression:\n\nSo, the first step is to process the tweets i.e. when a tweet is given it is in raw form and there must be done some preprocessing. When preprocessing, you have to perform the following:\n\n* Eliminate handles(a string starting with @) and URLs\n* Tokenize the string into words.\n* Remove stop words like \"and, is, a, on, etc.\"\n* Stemming- or convert every word to its stem. Like dancer, dancing, danced, becomes 'danc'. We can use porter stemmer to take care of this.\n* Convert all your words to lower case.\n\nStep 2: Extracting the features from training data:\nGiven a list of tweets, extract the features and store them in a matrix. You will extract two features.\n\n* The first feature is the number of positive words in a tweet.\n* The second feature is the number of negative words in a tweet.\n\ni.e. Given a corpus with positive and negative tweets as follows: \n\n![](https:\/\/d3c33hcgiwev3.cloudfront.net\/imageAssetProxy.v1\/zhNAjggWTbWTQI4IFu21ZA_32f86a38bc224959be21ede407128c82_Screen-Shot-2020-09-01-at-7.55.08-AM.png?expiry=1609286400000&hmac=8Li5O6pyPrBPgJ_mFBCIdozbonqQOnKRbcCr-qheshY)\n\nYou have to encode each tweet as a vector. \n\nTo do so, you have to create a dictionary to map the word, and the class it appeared in (positive or negative) to the number of times that word appeared in its corresponding class. (we talked about it above).\n\n![](https:\/\/d3c33hcgiwev3.cloudfront.net\/imageAssetProxy.v1\/vhHO7A7dTvuRzuwO3U779Q_5364f83a7bd54782a09279efe06e96f2_Screen-Shot-2020-09-01-at-7.57.30-AM.png?expiry=1609286400000&hmac=H5v8nlRYI8sP2DWxKdDqUD0IaIKp0kjZTJY-kOLFd40)\n\nIn the table above, you can see how words like happy and sad tend to take clear sides, while other words like \"I, am\" tend to be more neutral. Given this dictionary and the tweet, \"I am sad, I am not learning NLP\", you can create a vector corresponding to the feature as follows: \n\n![](https:\/\/d3c33hcgiwev3.cloudfront.net\/imageAssetProxy.v1\/N_PzQkNvSNqz80JDb-ja5A_a44a87942c5e476593cd8e1582cf5b06_Screen-Shot-2020-09-01-at-8.04.10-AM.png?expiry=1609286400000&hmac=tPeIF5TJCs3eMnLx1PFtrbGis9UVpuKHhkU8JXswQQQ)\n\nTo encode the negative feature, you can do the same thing.\n\n![](https:\/\/d3c33hcgiwev3.cloudfront.net\/imageAssetProxy.v1\/dQU0vX1NT3yFNL19TS98gQ_b885bb87a6d644389726ddc740869153_Screen-Shot-2020-09-01-at-8.04.21-AM.png?expiry=1609286400000&hmac=LkS3RuSZR42Conj1YKy46btQLCFHjD53vdCRVOHYL_0)\n\nHence you end up getting the following feature vector [1,8,11]. 1 corresponds to the bias, 8 the positive feature, and 11 the negative feature. \n\nWe do this for all the tweets in training data and make a matrix.\n\n* Then train your logistic regression classifier on these features.\n* Test the classifier on a validation set.\n","017986f7":"## Create the freqency dictionary","0f22c4e3":"## Test score\n\nFor evaluating the test score, first we turn our test data into the features and then predict the class for each tweet.","bf925dbc":"## Plot the count from each class","5f4dc8c7":"### Training Your Model\n\nNow, using the 2 features for each tweet, we can now traing our logistic regression model.\n\nBut, to do it, first we need to extract these features for each tweets and stack them into a matrix 'X'.\n\nBelow codes does the same.","f485f1e4":"## Function for Preprocessing the tweet\nAs we talked the preprocessing in first step, here is the function that returns a preprocessed tweet."}}