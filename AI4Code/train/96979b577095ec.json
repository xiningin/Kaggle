{"cell_type":{"0d744bff":"code","375e0538":"code","8a1cd285":"code","0343ec9b":"code","ee2bea63":"code","809e169b":"code","64dff6d7":"code","e0e01094":"code","374328e9":"code","9a7e3751":"code","383f00e7":"code","d4887c71":"code","57b268bb":"code","f2bc9bf9":"code","94aa8c47":"code","5ef5a2cc":"code","fe1a1b3a":"code","1c149bc4":"code","e22f64b0":"code","123c3f26":"code","5f87081b":"code","543f4452":"code","46729a0c":"code","b3ec6d9d":"code","a087ccda":"code","fad4ccb3":"code","8439358e":"code","870572d4":"code","15289a72":"code","a60cf1ff":"code","72566c3b":"code","68928995":"code","8385e002":"markdown","eda1c944":"markdown","ce395b51":"markdown","8b78ea1d":"markdown","3e3cf4a1":"markdown","d2499f26":"markdown","b0c3f55f":"markdown","a5cd0e2f":"markdown","6db9d5f8":"markdown","8abe46e5":"markdown","034e7c82":"markdown","85ddc115":"markdown","b83a7941":"markdown","e784a5e2":"markdown","440f29d7":"markdown","4212108e":"markdown","deeb0d9b":"markdown","37b445d4":"markdown","8d34001b":"markdown","559d3f45":"markdown","be8d38e8":"markdown","b4825793":"markdown","424fa443":"markdown","3cb3b23e":"markdown","cff31bf7":"markdown","419701b9":"markdown","3e2f7f4e":"markdown","47da1bf1":"markdown","5e0986bb":"markdown","88f65e9d":"markdown","03cb5a0e":"markdown"},"source":{"0d744bff":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n!pip install chart_studio\n!pip install textstat\n\n# Visualisation Library\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = 'all'\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n#visualisation libraries\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport chart_studio.plotly as py\nimport plotly.figure_factory as ff\nfrom plotly.offline import iplot\nimport cufflinks\ncufflinks.go_offline()\ncufflinks.set_config_file(world_readable=True, theme='pearl')\n\n\n#ml\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n#standard libraries\nimport emoji\nimport re\nimport string\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","375e0538":"df_new=pd.read_json('\/kaggle\/input\/indonesiandata\/emotion_id_tweets.json',lines=True)\ndf_new.head()","8a1cd285":"ex_label=[]\nfor i in range(df_new['annotation'].shape[0]):\n    if df_new['annotation'][i]['labels']==[]:\n        ex_label.append('no_emotion')\n    else:\n        ex_label.append(df_new.annotation[i]['labels'][0])","0343ec9b":"df=pd.DataFrame()\ndf['content']=df_new['content']\ndf['emotion']=ex_label\ndf.head()","ee2bea63":"df.describe()","809e169b":"df['len']=df['content'].astype(str).apply(len)\ndf.head()","64dff6d7":"df=df.drop_duplicates(subset='content',keep='first')\ndf.shape","e0e01094":"def missing_value_of_data(data):\n    total=data.isnull().sum().sort_values(ascending=False)\n    percentage=round(total\/data.shape[0]*100,2)\n    return pd.concat([total,percentage],axis=1,keys=['Total','Percentage'])\nmissing_value_of_data(df)\n","374328e9":"def count_values_in_column(data,feature):\n    total=data.loc[:,feature].value_counts(dropna=False)\n    percentage=round(data.loc[:,feature].value_counts(dropna=False,normalize=True)*100,2)\n    return pd.concat([total,percentage],axis=1,keys=['Total','Percentage'])\ncount_values_in_column(df,'emotion')\n","9a7e3751":"name_of_emotion=df.emotion.unique()\nvalue_of_emotion=list(df.emotion.value_counts())\n\nfig=go.Figure([go.Bar(x=list(name_of_emotion), y=value_of_emotion)])\nfig.show()","383f00e7":"!pip install translators","d4887c71":"import translators as ts","57b268bb":"def ngrams_top(corpus,ngram_range,n=None):\n    \"\"\"\n    List the top n words in a vocabulary according to occurrence in a text corpus.\n    \"\"\"\n    vec = CountVectorizer(stop_words = 'english',ngram_range=ngram_range).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    total_list=words_freq[:n]\n    df=pd.DataFrame(total_list,columns=['text','count'])\n    \n    english_ngram=[] #translating indonesian to english\n    for i  in range(10):\n        english_ngram.append(ts.google(df['text'][i], 'auto', 'en'))\n      \n \n    # Plotting Grams and their english conversion   \n    fig = make_subplots(rows=1, cols=2)\n    fig.add_trace(\n        go.Bar(x=df['count'][::-1],\n                    y=df['text'][::-1],\n                   name='Indonesian',\n                    marker_color='rgb(55, 83, 109)',\n                    orientation='h'\n                    ),\n        row=1, col=1\n    )\n    fig.add_trace(\n        go.Bar(x=df['count'][::-1],\n                    y=english_ngram[::-1],\n                   name='English',\n                    marker_color='rgb(123, 67, 199)',\n                    orientation='h'\n                    ),\n        row=1, col=2\n    )\n    fig.update_layout(height=600, width=2000, title_text=str(ngram_range[0])+\" grams for Indonesian\/English\")\n    fig.show()","f2bc9bf9":"ngrams_top(df['content'],(1,1),n=10)\n","94aa8c47":"ngrams_top(df['content'],(2,2),n=10)\n","5ef5a2cc":"ngrams_top(df['content'],(3,3),n=10)","fe1a1b3a":"joy=df[df['emotion']=='joy']\ntrust=df[df['emotion']=='trust']\nfear=df[df['emotion']=='fear']\nanger=df[df['emotion']=='anger']\nanticipation=df[df['emotion']=='anticipation']\ndisgust=df[df['emotion']=='disgust']\nsadness=df[df['emotion']=='sadness']\nsurprise=df[df['emotion']=='surprise']","1c149bc4":"joy['len'].iplot(\n    kind='hist',\n    bins=100,\n    xTitle='text length',\n    linecolor='black',\n    color='red',\n    yTitle='count',\n    title='Joy Text Length Distribution')\ntrust['len'].iplot(\n    kind='hist',\n    bins=100,\n    xTitle='text length',\n    linecolor='black',\n    color='green',\n    yTitle='count',\n    title='Trust Text Length Distribution')\n\nfear['len'].iplot(\n    kind='hist',\n    bins=100,\n    xTitle='text length',\n    linecolor='black',\n    color='pink',\n    yTitle='count',\n    title='Fear Text Length Distribution')\n\nanger['len'].iplot(\n    kind='hist',\n    bins=100,\n    xTitle='text length',\n    linecolor='black',\n    color='orange',\n    yTitle='count',\n    title='Anger Text Length Distribution')\n\nanticipation['len'].iplot(\n    kind='hist',\n    bins=100,\n    xTitle='text length',\n    linecolor='black',\n    yTitle='count',\n    title='Anticipation Text Length Distribution')\ndisgust['len'].iplot(\n    kind='hist',\n    bins=100,\n    xTitle='text length',\n    linecolor='black',\n    \n    yTitle='count',\n    title='Disgust Text Length Distribution')\nsadness['len'].iplot(\n    kind='hist',\n    bins=100,\n    xTitle='text length',\n    linecolor='black',\n    color='gold',\n    yTitle='count',\n    title='Sadness Text Length Distribution')\nsurprise['len'].iplot(\n    kind='hist',\n    bins=100,\n    xTitle='text length',\n    linecolor='black',\n    color='blue',\n    yTitle='count',\n    title='Surprise Text Length Distribution')","e22f64b0":"def ngrams_tops(corpus,ngram_range,emotions,n=None):\n    \"\"\"\n    List the top n words in a vocabulary according to occurrence in a text corpus.\n    \"\"\"\n    vec = CountVectorizer(stop_words = 'english',ngram_range=ngram_range).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    total_list=words_freq[:n]\n    df=pd.DataFrame(total_list,columns=['text','count'])\n    \n    english_ngram=[] #translating indonesian to english\n    for i  in range(10):\n        english_ngram.append(ts.google(df['text'][i], 'auto', 'en'))\n      \n \n    # Plotting Grams and their english conversion   \n    fig = make_subplots(rows=1, cols=2)\n    fig.add_trace(\n        go.Bar(x=df['count'][::-1],\n                    y=df['text'][::-1],\n                   name='Indonesian',\n                    marker_color='rgb(55, 83, 109)',\n                    orientation='h'\n                    ),\n        row=1, col=1\n    )\n    fig.add_trace(\n        go.Bar(x=df['count'][::-1],\n                    y=english_ngram[::-1],\n                   name='English',\n                    marker_color='rgb(123, 67, 199)',\n                    orientation='h'\n                    ),\n        row=1, col=2\n    )\n    fig.update_layout(height=600, width=2000,title_text=\"Most Used words in \"+str(emotions).capitalize())\n    fig.show()\n\n    \ndef most_used(emotions):\n    emotion=df.loc[df['emotion']==str(emotions)]\n\n    _1gram_emotion=ngrams_tops(emotion['content'],(1,1),emotions,n=10)\n    \n    return _1gram_emotion\n   \n        ","123c3f26":"most_used('joy')","5f87081b":"most_used('fear')","543f4452":"most_used('trust')","46729a0c":"most_used('surprise')","b3ec6d9d":"most_used('anger')","a087ccda":"most_used('anticipation')","fad4ccb3":"most_used('disgust')","8439358e":"most_used('sadness')","870572d4":"def find_emoji(text):\n    emo_text=emoji.demojize(text)\n    line=re.findall(r'\\:(.*?)\\:',emo_text)\n    return line\nsentence=\"I love \u26bd very much \ud83d\ude01\"\nfind_emoji(sentence)\n\n# Emoji cheat sheet - https:\/\/www.webfx.com\/tools\/emoji-cheat-sheet\/\n# Uniceode for all emoji : https:\/\/unicode.org\/emoji\/charts\/full-emoji-list.html","15289a72":"def remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\nsentence=\"Its all about \\U0001F600 face\"\nprint(sentence)\nremove_emoji(sentence)","a60cf1ff":"df['content']=df['content'].apply(lambda x: remove_emoji(x))","72566c3b":"def rep(text):\n    grp = text.group(0)\n    if len(grp) > 1:\n        return grp[0:1] # can change the value here on repetition\n    return grp\n   \ndef unique_char(rep,sentence):\n    convert = re.sub(r'(\\w)\\1+', rep, sentence) \n    return convert\n\nsentence=\"heyyy this is loooong textttt\"\nunique_char(rep,sentence)\n","68928995":"df['content']=df['content'].apply(lambda x : unique_char(rep,x))","8385e002":"Removing Repetitve Characters\n\nNote :we will use this function in our code","eda1c944":"# <font color='Blue'>Preprocessing Function<\/font>","ce395b51":"This Helper function could be used somewhere else but will not use it here\n\nBecause our text is in Indonesian and after converting emoji to text we will get **english text** and Hence will spoil our Training data","8b78ea1d":"# <font color='purple'>N Gram Charts<\/font> ","3e3cf4a1":"### <font color='red'>1 Gram<\/font>","d2499f26":"## 2. Fear","b0c3f55f":"* when we observed most used **single** words in each emotion they were mostly :-\n        stopwords like=we,for,me,first,can  etc\n* Observation from Most Used Bigrams in each motion got us some keyowrds:-\n      1. People Were Talking about their personal belongings while expressing Sadness and Fear\n      example= Facebook Account,Account etc.\n      \n      2. Surprise emotion mostly had the words like:-\n      CFC boxing and Chicken pieces after further analysis we can draw some conclusions from data\n      \n      \n        \n* Fun Fact :-\n        Disgust and Anger was always conveyed in Native Indonesian( that even Google Translate couldn't translate)","a5cd0e2f":"## <font color='Green'>Percentage Of Emotions<\/font>","6db9d5f8":"## First Look","8abe46e5":"## 6.Anticipation","034e7c82":"## <font color='purple'>Percentage Of Emotions :) <\/font>","85ddc115":"## <font color='purple'>Repetitive Characters <\/font>","b83a7941":"Convert Emoji To Text","e784a5e2":"# <font color='Orange'>Most used Pair of words in each emotion<\/font>","440f29d7":"# <font color='red'>Importing Necessary Libraries<\/font>","4212108e":"## 1.JOY","deeb0d9b":"## <font color='orange'>5. Anger <\/font>","37b445d4":"## <font color='purple'>MISSING VALUES<\/font>","8d34001b":"# <font color='brown'>Sentence Length Analysis<\/font>","559d3f45":"## <font color='red'>3 Gram<\/font>","be8d38e8":"## <font color='red'>2 Gram<\/font>","b4825793":"**Histogram clearly shows:-\n**\n* Joy , Trust, Fear, anger,Anticipation, Disgust emotions have comment length Mostly less than 600 words.\n* Sadness has Comment Length msotly less than 800 words.\n* Comments having Surprise emotion is Mostly SKewed.","424fa443":"## 3. Trust","3cb3b23e":"Removed Duplicate Content","cff31bf7":"## <font color='purple'>7. Disgust <\/font>","419701b9":"## <font color='purple'>Emoticons <font>","3e2f7f4e":"## <font color='red'>8. Sadness <\/font>","47da1bf1":"No Missing Values In 3 columns","5e0986bb":"## <font color='green'>4. Surprise<\/font>","88f65e9d":"#### Removing Emoji from Text","03cb5a0e":"## Observation from Above "}}