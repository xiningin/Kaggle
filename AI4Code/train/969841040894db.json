{"cell_type":{"fc2829ed":"code","66eaa577":"code","800e3bb1":"code","1c26fdc4":"code","6f170102":"code","7eeadbef":"code","5e0e75dc":"code","e91f7996":"code","2170c5f9":"code","67903651":"code","50b90bb2":"code","66b9b3bc":"code","49fe901c":"code","f455f0dc":"code","10fbd184":"code","9b6c5ccd":"code","0bb6c363":"code","2eb7f58b":"code","ce70dae1":"code","fb52855c":"code","b398943c":"code","35db47a3":"code","8727f1f6":"code","d493ab51":"code","709f5229":"code","fa594a88":"code","f2f1e632":"code","13ba412b":"code","d8a60c4f":"code","a5d61456":"code","bc4ef6b9":"code","3194efec":"code","1263c9cd":"markdown","ffb51670":"markdown","134a5fbf":"markdown","182dfbd5":"markdown","576943e0":"markdown"},"source":{"fc2829ed":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","66eaa577":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom numpy import absolute\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np  # linear algebra\nimport pandas as pd  #\nfrom datetime import datetime\n\nfrom scipy.stats import skew  # for some statistics\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\n\nfrom sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error\n\nfrom mlxtend.regressor import StackingCVRegressor\n\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\n\nimport os\n\n\nprint(os.listdir(\"..\/input\"))","800e3bb1":"# Training Data\n\ntrain = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntrain.head()","1c26fdc4":"# Testing Data\n\ntest = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\ntest.head()","6f170102":"# Saving ID on a new variable and Dropping ID column\n\ntrain_ID = train['Id']\ntest_ID = test['Id']\ntrain.drop(['Id'], axis=1, inplace=True)\ntest.drop(['Id'], axis=1, inplace=True)","7eeadbef":"# Plotting to find outliers \n# for items in train.columns:\n#     if len(train[items].unique()) < 20:\n#         sns.catplot(x=train[items], y=train['SalePrice'], data=train)\n#         plt.show()\n#     else:\n#         sns.scatterplot(x=train[items],y=train['SalePrice'])\n#         plt.show()","5e0e75dc":"# Removing Outlier\ntrain = train[train.GrLivArea < 4500]\ntrain.reset_index(drop=True, inplace=True)\nprint(train.shape)","e91f7996":"# Plotting output data for Visualization\ntrain['SalePrice'].hist(bins = 40)","2170c5f9":"# Adjusting for Skewness\n# train[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\ntrain[\"SalePrice\"] = train[\"SalePrice\"]\ny_train = train['SalePrice'].reset_index(drop=True)\n\ntrain['SalePrice'].hist(bins = 40)","67903651":"# Concat Testing part of both Train and Test and split SalePrice from Train data for Training\ntrain_data = train.drop(['SalePrice'], axis=1)\ntotal_data = pd.concat([train_data, test]).reset_index(drop=True)","50b90bb2":"# Total data is full data to be used for EDA\ntotal_data.shape","66b9b3bc":"print('columns containing missing values =',total_data.isnull().any().sum())","49fe901c":"# Finding missing values to plot a Graph\nmissing_counts = pd.DataFrame(total_data.isnull().sum().sort_values(ascending=False))\nmissing_columns = missing_counts[missing_counts.iloc[:,0]>0]\n# Plotting Missing Values\nplt.figure(figsize=(20,10))\nmissing_columns = missing_counts[missing_counts.iloc[:,0]>0]\nsns.barplot(x=missing_columns.index,y=missing_columns.iloc[:,0])\nplt.xticks(rotation=90)\nprint(missing_columns)\nplt.show()","f455f0dc":"# Some of the non-numeric predictors are stored as numbers; we convert them into strings \ntotal_data['MSSubClass'] = total_data['MSSubClass'].apply(str)\ntotal_data['YrSold'] = total_data['YrSold'].astype(str)\ntotal_data['MoSold'] = total_data['MoSold'].astype(str)\n\n# Fixing columns : filling Null values with suitable values in columns\ntotal_data['Functional'] = total_data['Functional'].fillna('Typ')\ntotal_data['Electrical'] = total_data['Electrical'].fillna(\"SBrkr\")\ntotal_data['KitchenQual'] = total_data['KitchenQual'].fillna(\"TA\")\ntotal_data['Exterior1st'] = total_data['Exterior1st'].fillna(total_data['Exterior1st'].mode()[0])\ntotal_data['Exterior2nd'] = total_data['Exterior2nd'].fillna(total_data['Exterior2nd'].mode()[0])\ntotal_data['SaleType'] = total_data['SaleType'].fillna(total_data['SaleType'].mode()[0])\ntotal_data[\"PoolQC\"] = total_data[\"PoolQC\"].fillna(\"None\")","10fbd184":"# Filling columns with Mode and Median values\ntotal_data[\"LotFrontage\"].fillna(total_data[\"LotFrontage\"].median(),inplace=True)\ntotal_data[\"MSZoning\"].fillna(total_data[\"MSZoning\"].mode(),inplace=True)","9b6c5ccd":"# Filling some columns Nan values with 0's\n\nfor item in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    total_data[item] = total_data[item].fillna(0)\n\n# Filling categorial columns Nan Values with 'None'\n\nfor item in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond','BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2']:\n    total_data[item] = total_data[item].fillna('None')","0bb6c363":"# Filling remaining Object type columns Nan values with None\nobjects = []\nfor i in total_data.columns:\n    if total_data[i].dtype == object:\n        objects.append(i)\ntotal_data.update(total_data[objects].fillna('None'))","2eb7f58b":"# Filling remaining Numerical type columns with 0 values\nnumerical = total_data.select_dtypes(include=np.number).columns.tolist()\nfor i in numerical:\n    total_data.update(total_data[i].fillna(0))","ce70dae1":"# Dropping unwanted columns\ntotal_data = total_data.drop(['Utilities', 'Street', 'PoolQC',], axis=1)","fb52855c":"# Engineered new columns \ntotal_data['YrBltAndRemod']=total_data['YearBuilt']+total_data['YearRemodAdd']\ntotal_data['TotalSF']=total_data['TotalBsmtSF'] + total_data['1stFlrSF'] + total_data['2ndFlrSF']\n\ntotal_data['Total_sqr_footage'] = (total_data['BsmtFinSF1'] + total_data['BsmtFinSF2'] +\n                                 total_data['1stFlrSF'] + total_data['2ndFlrSF'])\n\ntotal_data['Total_Bathrooms'] = (total_data['FullBath'] + (0.5 * total_data['HalfBath']) +\n                               total_data['BsmtFullBath'] + (0.5 * total_data['BsmtHalfBath']))\n\ntotal_data['Total_porch_sf'] = (total_data['OpenPorchSF'] + total_data['3SsnPorch'] +\n                              total_data['EnclosedPorch'] + total_data['ScreenPorch'] +\n                              total_data['WoodDeckSF'])","b398943c":"# Simplifying Features\ntotal_data['haspool'] = total_data['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\ntotal_data['has2ndfloor'] = total_data['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\ntotal_data['hasgarage'] = total_data['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\ntotal_data['hasbsmt'] = total_data['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\ntotal_data['hasfireplace'] = total_data['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)","35db47a3":"# Adding pandas dummy values to encode features\nprint(total_data.shape)\nfinal_data = pd.get_dummies(total_data).reset_index(drop=True)\nprint(final_data.shape)","8727f1f6":"# Splitting Train and test values now with help of variable 'y' \nx_train = final_data.iloc[:len(y_train), :]\nx_test = final_data.iloc[len(x_train):, :]\n\nprint('x_train', x_train.shape)\nprint('y_train', y_train.shape)\nprint('x_test', x_test.shape)","d493ab51":"# Removing Overfitting features\noverfit = []\nfor i in x_train.columns:\n    counts = x_train[i].value_counts()\n    zeros = counts.iloc[0]\n    if zeros \/ len(x_train) * 100 > 99.94:\n        overfit.append(i)\n\noverfit = list(overfit)\n# MSZoning_C have an extra field added in data \noverfit.append('MSZoning_C (all)')\n\nX = x_train.drop(overfit, axis=1).copy()\nX_test = x_test.drop(overfit, axis=1).copy()\ny = y_train.copy()\n\nprint('X_train', X.shape)\nprint('Y_train', y.shape)\nprint('X_test', X_test.shape)","709f5229":"# Sequential Model\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n\nnn = Sequential()\nnn.add(Dense(118, activation='relu', kernel_initializer='normal', input_dim = 331))\nnn.add(Dense(59, activation='relu'))\nnn.add(Dense(1, kernel_initializer='normal'))","fa594a88":"# Complie the model\nfrom keras.optimizers import Adam\nopt = Adam(lr=0.1)\nnn.compile(loss='mean_squared_error', optimizer = opt )","f2f1e632":"# Model summary \nnn.summary()","13ba412b":"# Define XG-Boost Model\nxgb = XGBRegressor(learning_rate=0.01, n_estimators=3460,\n                                     max_depth=3, min_child_weight=0,\n                                     gamma=0, subsample=0.7,\n                                     colsample_bytree=0.7,\n                                     objective='reg:linear', nthread=-1,\n                                     scale_pos_weight=1, seed=27,\n                                     reg_alpha=0.00006, random_state=42)","d8a60c4f":"# Fitting both the models on test set\n\nnn.fit(X,y, epochs = 400, batch_size = 32,verbose = 0)\nxgb.fit(X,y)","a5d61456":"# Ensembling with the weighted average model\n\n# Predicting test data with neural net and XG-boost models\npred1 = nn.predict(X_test)\npred2 = xgb.predict(X_test)\npred2 = pred2.reshape(1459,1)\n\n# Combining both the predictions with average of 0.5 to both model\n# You can change the average weightage of the individual models \nfinalpred=( pred1 * 0.5 + pred2 * 0.5)\nfinalpred","bc4ef6b9":"# Predict submission\nsubmission = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")\nsubmission.iloc[:,1] = np.floor(finalpred)","3194efec":"# Submission\nsubmission.to_csv(\"submission.csv\", index=False)","1263c9cd":"Data Analysis","ffb51670":"Data is skewed, we will use log1p to remove the skewness in data","134a5fbf":"Defining and training models","182dfbd5":"Data Engineering","576943e0":"Visualizing output data from train set"}}