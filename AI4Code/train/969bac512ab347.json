{"cell_type":{"88ab3260":"code","98c51b84":"code","e3b0b8c3":"code","b4ebc8be":"code","8a687ce7":"code","58050dcc":"code","14e929ad":"code","d02a89ab":"code","915bef88":"code","58298f17":"code","bbe59863":"code","b32c2351":"code","2e0f04d7":"code","59f0eb3b":"code","ae99dc3d":"code","69aa9810":"code","2f058649":"code","0f9dab1e":"code","24f5c743":"code","ce098494":"code","b8bc140e":"code","66e0ede1":"code","ea3c39b3":"code","5077f7a3":"code","480a1f47":"code","f91c6097":"code","88ab76b4":"code","ecc6546e":"code","690da10b":"code","be89cbf2":"code","9967fee3":"code","76b8cd25":"markdown","4a477611":"markdown","7a515b3e":"markdown","7b6ac00a":"markdown","a9182c60":"markdown","6e7723ab":"markdown","33426c35":"markdown","239e8660":"markdown","dd887181":"markdown","c459c01f":"markdown","9b9e3884":"markdown","5cc8003a":"markdown","29244456":"markdown"},"source":{"88ab3260":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport pylab as plot\nimport re as re","98c51b84":"# Load in the train and test datasets\ntrain = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')\n\n","e3b0b8c3":"train.head()","b4ebc8be":"test.head()","8a687ce7":"id_train=train['PassengerId']\ntrain.drop('PassengerId',inplace=True,axis=1)\nid_test=test['PassengerId']\ntest.drop('PassengerId',inplace=True,axis=1)\n","58050dcc":"train.info()","14e929ad":"#some feature statistics\ntrain.describe()","d02a89ab":"#Visualizing survival based on the gender.\ntrain['Died'] = 1 - train['Survived']\ntrain.groupby('Sex').agg('sum')[['Survived', 'Died']].plot(kind='bar', figsize=(20, 6),\n                                                          stacked=True, colors=['b', 'R']);\ntrain.drop('Died',inplace=True,axis=1)\n","915bef88":"#Visualizing survival based on the age and sex\nfig = plt.figure(figsize=(20, 5))\nsns.violinplot(x='Sex', y='Age', \n               hue='Survived', data=train, \n               split=True,\n               palette={0: \"r\", 1: \"b\"}\n              );","58298f17":"#Visualizing survival based on the fare ticket\nfigure = plt.figure(figsize=(25, 7))\nplt.hist([train[train['Survived'] == 1]['Fare'], train[train['Survived'] == 0]['Fare']], \n         stacked=True, color = ['b','r'],\n         bins = 50, label = ['Survived','Dead'])\nplt.xlabel('Fare')\nplt.ylabel('Number of passengers')\nplt.legend();","bbe59863":"#Visualizing survival based on the embarkation.\n\nfig = plt.figure(figsize=(20, 5))\nsns.violinplot(x='Embarked', y='Fare', hue='Survived', data=train, split=True, palette={0: \"r\", 1: \"b\"});","b32c2351":"full_data = [train, test]\n","2e0f04d7":"#title of people\ndef get_title(name):\n    title_search = re.search(' ([A-Za-z]+)\\.', name)\n    # If the title exists, extract and return it.\n    if title_search:\n        return title_search.group(1)\n    return \"\"\n\nfor dataset in full_data:\n    dataset['Title'] = dataset['Name'].apply(get_title)","59f0eb3b":"train.head(10)","ae99dc3d":"train.Title.unique()","69aa9810":"train.info()","2f058649":"for dataset in full_data:\n    #replace missing values \n    dataset['Age'] = dataset['Age'].fillna(dataset['Age'].median())\n    dataset['Embarked'] = dataset['Embarked'].replace(np.NAN,'C')\n\n    # Mapping Sex\n    dataset['Sex'] = dataset['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n    \n    # Mapping titles\n    title_mapping = {'Mr': 1, 'Mrs': 2, 'Miss' : 3, 'Master' : 4, 'Don' : 5, 'Rev' : 6, 'Dr' : 7, 'Mme' : 8, 'Ms' : 9,\n       'Major' : 10, 'Lady' : 11 , 'Sir' : 12, 'Mlle' : 13, 'Col' : 14, 'Capt' : 15, 'Countess' : 16,\n       'Jonkheer' : 17}\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n    \n    # Mapping Embarked\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n    \n    \n","0f9dab1e":"for dataset in full_data:\n    dataset['Cabin']=dataset['Cabin'].replace(np.NAN,'Z')   \n    dataset['Cabin']=dataset['Cabin'].astype(str).str[0]\n    dataset['Cabin'] = dataset['Cabin'].map( {'Z': 0, 'C' : 1, 'E': 2, 'G': 3, 'D' : 4, 'A' : 5, 'B': 6, 'F': 7, 'T': 8} ).astype(int)\n\n    ","24f5c743":"drop_elements = ['Name', 'Ticket']\ntrain = train.drop(drop_elements, axis = 1)\ntest = test.drop(drop_elements, axis = 1)\n\n","ce098494":"train.info(),test.info()","b8bc140e":"test['Fare']=test['Fare'].replace(np.NaN,float(test['Fare'].max()))\ntest['Title']=test['Title'].astype(int)\n","66e0ede1":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.metrics import accuracy_score, log_loss\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\n\nclassifiers = [\n    KNeighborsClassifier(3),\n    DecisionTreeClassifier(),\n    GaussianNB(),\n    LinearDiscriminantAnalysis(),\n    QuadraticDiscriminantAnalysis(),\n    LogisticRegression()]\n\nlog_cols = [\"Classifier\", \"Accuracy\"]\nlog  = pd.DataFrame(columns=log_cols)\n\n#croose validation\nsss = StratifiedShuffleSplit(n_splits=10, test_size=0.1, random_state=0)\ntrain = train.values\nX = train[0::, 1::]\ny = train[0::, 0]\n\nacc_dict = {}\n\nfor train_index, test_index in sss.split(X, y):\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n\n    for clf in classifiers:\n        name = clf.__class__.__name__\n        clf.fit(X_train, y_train)\n        train_predictions = clf.predict(X_test)\n        acc = accuracy_score(y_test, train_predictions)\n        if name in acc_dict:\n            acc_dict[name] += acc\n        else:\n            acc_dict[name] = acc\n\nfor clf in acc_dict:\n    acc_dict[clf] = acc_dict[clf] \/ 10.0\n    log_entry = pd.DataFrame([[clf, acc_dict[clf]]], columns=log_cols)\n    log = log.append(log_entry)\n\nplt.xlabel('Accuracy')\nplt.title('Classifier Accuracy')\n\nsns.set_color_codes(\"muted\")\nsns.barplot(x='Accuracy', y='Classifier', data=log, color=\"b\")","ea3c39b3":"sub=pd.read_csv('..\/input\/titanic\/gender_submission.csv')","5077f7a3":"#best parameters for decision_tree \n\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n\nfor train_index, test_index in sss.split(X, y):\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\nparam_grid = { \n    \"criterion\" : [\"gini\", \"entropy\"], \n    \"min_samples_leaf\" : [1, 5, 10, 25, 50, 70], \n    \"min_samples_split\" : [2, 4, 10, 12, 16, 18, 25, 35]}\n\ndt = DecisionTreeClassifier()\n\n#clf = GridSearchCV(rf, param_grid=param_grid, n_jobs=-1)\nclf = RandomizedSearchCV(dt, param_grid, n_jobs=-1)\n \nclf.fit(X_train, y_train)\n \nprint(clf.best_estimator_)","480a1f47":"#make prediction using decsionTree\nmodel = clf.best_estimator_\nmodel.fit(train[0::, 1::], train[0::, 0])\nts  = test.values\nresult = model.predict(ts)\nsub['Survived']=result\nsub['Survived']=sub['Survived'].astype(int)\nsub.to_csv('decisiontree_submission.csv',index=False)","f91c6097":"\n#best parameters random_forest\nfor train_index, test_index in sss.split(X, y):\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\nparam_grid = { \n    \"criterion\" : [\"gini\", \"entropy\"], \n    \"min_samples_leaf\" : [1, 5, 10, 25, 50, 70], \n    \"min_samples_split\" : [2, 4, 10, 12, 16, 18, 25, 35], \n    \"n_estimators\": [100, 400, 700, 1000, 1500]}\n\nrf = RandomForestClassifier(n_estimators=100, max_features='auto', oob_score=True, random_state=1, n_jobs=-1)\n\n#clf = GridSearchCV(rf, param_grid=param_grid, n_jobs=-1)\nclf = RandomizedSearchCV(rf, param_grid, n_jobs=-1)\n \nclf.fit(X_train, y_train)\nprint(clf.best_estimator_)","88ab76b4":"\n#cross validation for random_forest\nfrom sklearn.ensemble import RandomForestClassifier\nrandom_forest =clf.best_estimator_\nrandom_forest.fit(X_train, y_train)\n\nY_pred  = random_forest.predict(X_test)\n\nrandom_forest.score(X_train, y_train)\nacc_random_forest = round(random_forest.score(X_train, y_train)*100, 2)\n\nprint (acc_random_forest)","ecc6546e":"\n#make prediction using randomforest\nrandom_forest.fit(train[0::, 1::], train[0::, 0])\nts  = test.values\nresult = random_forest.predict(ts)\nsub['Survived']=result\nsub['Survived']=sub['Survived'].astype(int)\nsub.to_csv('random_forest_submission.csv',index=False)","690da10b":"\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfor train_index, test_index in sss.split(X, y):\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n\n#best parameters for xgboost\nparams = {\n        'min_child_weight': [1, 5, 10],\n        'gamma': [0.5, 1, 1.5, 2, 5],\n        'subsample': [0.6, 0.8, 1.0],\n        'colsample_bytree': [0.6, 0.8, 1.0],\n        'max_depth': [3, 4, 5]\n        }\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n\nxgb = XGBClassifier(learning_rate=0.02, n_estimators=600,\n                    silent=True, nthread=1)\n\nclf = GridSearchCV(xgb, param_grid=params)\n#clf = RandomizedSearchCV(xgb, param_grid, n_jobs=-1)\n \nclf.fit(X_train, y_train)\n\nprint(clf.best_estimator_)\n","be89cbf2":"\n#cross validation for XGboost\nxgb = clf.best_estimator_\nxgb.fit(X_train, y_train)\n\nY_pred  = xgb.predict(X_test)\n\nxgb.score(X_train, y_train)\nacc_xgb = round(xgb.score(X_train, y_train)*100, 2)\n\nprint (acc_xgb)","9967fee3":"#make prediction using randomforest\nxgb.fit(train[0::, 1::], train[0::, 0])\nts  = test.values\nresult = xgb.predict(ts)\nsub['Survived']=result\nsub['Survived']=sub['Survived'].astype(int)\nsub.to_csv('Xgboost_submission.csv',index=False)","76b8cd25":"* the cabin attribute means that we have a people with the cabin and others without (NaN) and the letter that begins the cabin code makes the difference between cabins, so, we will extract this letter and map the cabin attribute","4a477611":"# Part 1 : data preparation \/ cleaning\nin this part, we will explore the available data, identify possible opportunities for functionality engineering as well as numerically code all categorical functionalities.","7a515b3e":"\n* Younger male tend to survive\n* A large number of passengers between 20 and 40 die\n* The age doesn't seem to have a direct impact on the female survival","7b6ac00a":"*   now let's clean the data and map the features into numerical values.","a9182c60":"# competition goal\nthe goal of the competition is to create a model that predicts which passengers survived the Titanic shipwreck.","6e7723ab":"the best scores is : decisionTree ","33426c35":"# Hard models","239e8660":"* The embarkation C have a wider range of fare tickets and therefore the passengers who pay the highest prices are those who survive.","dd887181":"* Passengers with cheaper ticket fares are more likely to die.","c459c01f":"# Introduction\nthis notebook is especially for beginners in data science.\nin this course, you will find the visualization of titanic dataset data, \nsimple prediction models and also the hard prediction models with score \nimprovement techniques, and also the Ensembling\/Stacking between several models.","9b9e3884":"# Basic models ","5cc8003a":"you can try other hard models such as : LGBM,Adaboost,catboost,NN........\n* thanks to : [Introduction to Ensembling\/Stacking in Python](https:\/\/www.kaggle.com\/arthurtok\/introduction-to-ensembling-stacking-in-python\/output)\n* [Titanic best working Classifier\n](https:\/\/www.kaggle.com\/sinakhorami\/titanic-best-working-classifier)\n* [Data Visualization - Titanic survival](https:\/\/www.kaggle.com\/usharengaraju\/data-visualization-titanic-survival)","29244456":"this graph given us two important information: the number of men is doubled compared to the number of women, and the rate of women surviving is higher than men"}}