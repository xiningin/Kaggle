{"cell_type":{"18cf75aa":"code","5db44d06":"code","bfca3a7a":"code","0f95cca3":"code","3b2249d9":"code","8ec6bf9f":"code","cc86e55f":"code","d4307eea":"code","a112d74b":"code","a9bde3e0":"code","9ce8cb3e":"code","8e16adb7":"code","80e57e86":"code","ac8ed928":"code","4f297d3b":"code","68b0207e":"code","62d22892":"code","7fdcb6ec":"code","776d3b05":"code","9772c043":"code","f4e4caec":"code","69d599d3":"code","5d94af38":"markdown","f051686d":"markdown","bb14eaf6":"markdown","cb2017f8":"markdown","382f265f":"markdown","281b37ff":"markdown","3af28992":"markdown","5ff371c0":"markdown","f8759932":"markdown","6138a3e9":"markdown","7218954e":"markdown","419a48ac":"markdown","ab793b10":"markdown","7222f7d1":"markdown","0244fdfa":"markdown","817599c8":"markdown","424a2fe7":"markdown","c7f9a4d4":"markdown","b0891743":"markdown","cbacc5f7":"markdown","a8956715":"markdown","77d509d6":"markdown","ed3685a8":"markdown","aa16a642":"markdown"},"source":{"18cf75aa":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport random\nimport cv2 # For reading images\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\nprint(os.listdir('..\/input\/flowers\/flowers'))","5db44d06":"# Daisies path from Flower Recognation folder\ndaisy_path = \"..\/input\/flowers\/flowers\/daisy\/\"\n\n\n#  Tulip path from Flower Recognation folder\ntulip_path = \"..\/input\/flowers\/flowers\/tulip\/\" ","bfca3a7a":"trainLabels = [] # For labels. Daisy and tulip\ndata = [] # All image array\n\n# Dimensions of the images are not fixed. They have various sizes and we will fix tham to 128x128\nsize = 128,128\n\ndef readImages(flowerPath, folder):\n    \n    imagePaths = []\n    for file in os.listdir(flowerPath):\n        if file.endswith(\"jpg\"):  # use only .jpg extensions\n            imagePaths.append(flowerPath + file)\n            trainLabels.append(folder)\n            img = cv2.imread((flowerPath + file), 0)\n            im = cv2.resize(img, size)\n            data.append(im)            \n            \n    return imagePaths","0f95cca3":"def showImage(imgPath):\n    img = cv2.imread(imgPath)\n    plt.imshow(img, cmap = 'gray', interpolation = 'bicubic')\n    plt.axis('off')\n    plt.show()","3b2249d9":"daisyPaths = readImages(daisy_path, 'daisy')\ntulipPaths = readImages(tulip_path, 'tulip')","8ec6bf9f":"showImage(daisyPaths[np.random.randint(0,500)])\nshowImage(tulipPaths[np.random.randint(0,500)])","cc86e55f":"rawData = np.array(data)\nrawData.shape","d4307eea":"rawData = rawData.astype('float32') \/ 255.0","a112d74b":"X = rawData\nz = np.zeros(877)\no = np.ones(876)\nY = np.concatenate((z, o), axis = 0).reshape(X.shape[0], 1)\n\nprint(\"X shape: \" , X.shape)\nprint(\"Y shape: \" , Y.shape)","a9bde3e0":"# Let's create train and test data\nfrom sklearn.model_selection import train_test_split\n\nxTrain, xTest, yTrain, yTest = train_test_split(X, Y, test_size = 0.15, random_state = 42)\nnumberOfTrain = xTrain.shape[0]\nnumberOfTest = xTest.shape[0]","9ce8cb3e":"# Transforming data to 2D.\n\nxTrainFlatten = xTrain.reshape(numberOfTrain, xTrain.shape[1] * xTrain.shape[2])\nxTestFlatten = xTest.reshape(numberOfTest, xTest.shape[1] * xTest.shape[2])\n\nprint(\"X train flatten\", xTrainFlatten.shape)\nprint(\"X test flatten\", xTestFlatten.shape)","8e16adb7":"x_train = xTrainFlatten.T\nx_test = xTestFlatten.T\ny_train = yTrain.T\ny_test = yTest.T\nprint(\"x train: \",xTrain.shape)\nprint(\"x test: \",xTest.shape)\nprint(\"y train: \",yTrain.shape)\nprint(\"y test: \",yTest.shape)","80e57e86":"def initializeParametersAndLayerSizesNN(x_train, y_train):\n    \n    parameters = {\"weight1\": np.random.randn(3, x_train.shape[0]) * 0.1,\n                  \"bias1\": np.zeros((3, 1)),\n                  \"weight2\": np.random.randn(y_train.shape[0], 3) * 0.1,\n                  \"bias2\": np.zeros((y_train.shape[0], 1))}\n    \n    return parameters","ac8ed928":"# Method for sigmoid function\n# z = np.dot(w.T, x_train) + b\ndef sigmoid(z):\n    y_head = 1\/(1+np.exp(-z))\n    return y_head","4f297d3b":"def forwardPropagationNN(x_train, parameters):\n\n    Z1 = np.dot(parameters[\"weight1\"], x_train) + parameters[\"bias1\"]\n    A1 = np.tanh(Z1)\n    Z2 = np.dot(parameters[\"weight2\"], A1) + parameters[\"bias2\"]\n    A2 = sigmoid(Z2)\n\n    cache = {\"Z1\": Z1,\n             \"A1\": A1,\n             \"Z2\": Z2,\n             \"A2\": A2}\n    \n    return A2, cache","68b0207e":"# Compute cost\ndef computeCostNN(A2, Y, parameters):\n    \n    logprobs = np.multiply(np.log(A2),Y)\n    cost = -np.sum(logprobs)\/Y.shape[1]\n    \n    return cost","62d22892":"def backwardPropagationNN(parameters, cache, X, Y):\n\n    dZ2 = cache[\"A2\"]-Y\n    dW2 = np.dot(dZ2,cache[\"A1\"].T)\/X.shape[1]\n    db2 = np.sum(dZ2,axis =1,keepdims=True)\/X.shape[1]\n    dZ1 = np.dot(parameters[\"weight2\"].T,dZ2)*(1 - np.power(cache[\"A1\"], 2))\n    dW1 = np.dot(dZ1,X.T)\/X.shape[1]\n    db1 = np.sum(dZ1,axis =1,keepdims=True)\/X.shape[1]\n    grads = {\"dweight1\": dW1,\n             \"dbias1\": db1,\n             \"dweight2\": dW2,\n             \"dbias2\": db2}\n    \n    return grads","7fdcb6ec":"def updateParametersNN(parameters, grads, learning_rate):\n    \n    parameters = {\"weight1\": parameters[\"weight1\"] - learning_rate * grads[\"dweight1\"],\n                  \"bias1\": parameters[\"bias1\"] - learning_rate * grads[\"dbias1\"],\n                  \"weight2\": parameters[\"weight2\"] - learning_rate * grads[\"dweight2\"],\n                  \"bias2\": parameters[\"bias2\"] - learning_rate * grads[\"dbias2\"]}\n    \n    return parameters","776d3b05":"# prediction\n# x_test is the input of forward propagation.\ndef predictNN(parameters, x_test):\n\n    A2, cache = forwardPropagationNN(x_test, parameters)\n    Y_prediction = np.zeros((1,x_test.shape[1]))\n    \n    for i in range(A2.shape[1]):\n        if A2[0, i] <= 0.5:\n            Y_prediction[0, i] = 0\n        else:\n            Y_prediction[0, i] = 1\n\n    return Y_prediction","9772c043":"# 2 - Layer neural network\ndef two_layer_neural_network(x_train, y_train, x_test, y_test, learning_rate, num_iterations):\n    \n    cost_list = []\n    index_list = []\n    \n    # Initialize parameters\n    parameters = initializeParametersAndLayerSizesNN(x_train, y_train)\n\n    for i in range(0, num_iterations):\n        # Forward propagation\n        A2, cache = forwardPropagationNN(x_train, parameters)\n        # Calculation of cost value\n        cost = computeCostNN(A2, y_train, parameters)\n         # Backward propagation\n        grads = backwardPropagationNN(parameters, cache, x_train, y_train)\n         # Updating parameters\n        parameters = updateParametersNN(parameters, grads, learning_rate)\n        \n        if i % 10 == 0:\n            cost_list.append(cost)\n            index_list.append(i)\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n            \n    plt.plot(index_list,cost_list)\n    plt.xticks(index_list,rotation = 'vertical')\n    plt.xlabel(\"Number of Iterarion\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    \n    # Prediction\n    y_prediction_test = predictNN(parameters, x_test)\n    y_prediction_train = predictNN(parameters, x_train)\n\n    # Print results\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n    return parameters\n\nparameters = two_layer_neural_network(x_train, y_train, x_test, y_test, learning_rate = 0.01, num_iterations = 500)","f4e4caec":"# Reshaping for keras\nx_train, x_test, y_train, y_test = x_train.T, x_test.T, y_train.T, y_test.T","69d599d3":"# Evaluating the ANN\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom keras.models import Sequential # initialize neural network library\nfrom keras.layers import Dense # build our layers library\n\ndef build_classifier():\n    classifier = Sequential() # initialize neural network\n    classifier.add(Dense(units = 32, kernel_initializer = 'uniform', activation = 'relu', input_dim = x_train.shape[1]))\n    classifier.add(Dense(units = 16, kernel_initializer = 'uniform', activation = 'relu'))\n    classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n    classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n    return classifier\n\nclassifier = KerasClassifier(build_fn = build_classifier, epochs = 500)\naccuracies = cross_val_score(estimator = classifier, X = x_train, y = y_train, cv = 3)\nmean = accuracies.mean()\nvariance = accuracies.std()\n\nprint(\"Accuracy mean: \"+ str(mean))\nprint(\"Accuracy variance: \"+ str(variance))","5d94af38":"* Load daisies and tulips","f051686d":"<a id=\"4\"><\/a>\n### **Logistic Regression for Deep Learning**\n\n* It is the basis of deep learning. \n* In next steps we will make a binary classification. Then we'll create the neural network model.\n\n* Next step is the train - test split operation","bb14eaf6":"<a id=\"10\"><\/a> \n**Prediction**\n* Definition of prediction method is shown below.","cb2017f8":"* Now we have 1821 samples with 128x128 size. \n* We will normalize data for binary classification.","382f265f":"<a id=\"11\"><\/a> \n**Model Creation**\n* Parameter initialization\n* Forward propagation and cost value calculation\n* Backward propagation\n* Updating parameters\n* The above steps will be repeated until the given iteration count. \n* After iterations end, prediction process will be started.\n* Now we will create 2 - Layer neural network","281b37ff":"Implementing keras library\n* Each classifier is a layer definition (Each Dense)\n    * units -> node count of the layer\n    * kernel_initializer -> initialization form of weights    \n    * activation -> activation function choice\n    * input_dim -> dimension of inputs (128 x 128)\n* In compilation;\n    * optimizer -> adam is the algorithm for training neural networks\n    * loss -> cost function is the same as logistic regression. Sum of cost values.\n    * metrics -> for accuracy\n    * cross_val_score ->  library for calculating accuracy values\n    * epoch -> iteration count","3af28992":"<a id=\"9\"><\/a> \n**Updating Parameters**\n* We need update parameters for complexity of our model. We'll use learning rate and derivatized parameters.","5ff371c0":"* Let's look at some flowers","f8759932":"# **Flower Recognition with ANN Implementation**\n\n\n### **Content**\n* [Introduction](#1)\n* [Data Preparation](#2)\n* [Logistic Regression for Deep Learning](#4)\n    * [Forward Propagation](#5)\n        * [Sigmoid Function](#7)\n        * [Loss and Cost Function](#6)\n    * [Backward Propagation](#8)\n    * [Updating Parameters](#9)\n    * [Prediction](#10)\n    * [Model Creation](#11)    \n* [Artificial Neural Network with Keras](#12)  \n* [Conclusion](#3)","6138a3e9":"<a id=\"2\"><\/a> \n### **Data Preparation**\n* We'll pick two kind of flowers for classification. These are Daisies and tulips.","7218954e":"<a id=\"3\"><\/a> \n## **Conclusion**\n* We used logistic regression for creating artificial neural network model. Then we did the same operation with keras library. \n* *If you have a suggestion, I'd be happy to read it.*","419a48ac":"<a id=\"8\"><\/a> \n**Backward Propagation**\n\n* It means derivative.\n* Function which is shown below derivative of according to the our parameters (weights and bias).","ab793b10":"* Method for forward propagation","7222f7d1":"<a id=\"1\"><\/a> \n## **Introduction**\n\n* The following notebook which I created below is the sample of the my \"deep learning\" learning phase. \n* Deep learning is the part of machine learning. \n* It extracts features from given dataset. \n* It has hidden layers unlike machine learning techniques. Below is the example of how it works.","0244fdfa":"* Method for reading images from data folder. We will use cv2 library for images.\n[For more information about cv2](https:\/\/docs.opencv.org\/3.0-beta\/doc\/py_tutorials\/py_gui\/py_image_display\/py_image_display.html)","817599c8":"* We will create X and Y for our classification. X -> our binary flower date, Y -> label data ","424a2fe7":"<a id=\"6\"><\/a> \n* Method for compute cost.","c7f9a4d4":"<a id=\"7\"><\/a> \n* Method for sigmoid function","b0891743":"<a id=\"12\"><\/a> \n### **Artificial Neural Network with Keras**","cbacc5f7":"<a id=\"5\"><\/a> \n**Forward propagation**\n* Multiplication of weights and features and addition of bias values. \n* Z1 is the result of first process. Then we will use tanh() activation function with Z1 and we get A1\n* After getting A1 value we'll multiply weights and A1 values for Z2. Then we'll use the sigmoid function for getting the A2 value. \n* tanh() function compresses data  [-1, 1] range\n* sigmoid function compresses data [0, 1] range.","a8956715":"Intialize parameters and layer sizes.\n* 3 is the layer size. \n* We have 3 layers for our ANN model.","77d509d6":"![sample.png](http:\/\/i68.tinypic.com\/6oipao.png)","ed3685a8":"* Method for showing sample images","aa16a642":"* Converting images to numpy array for classification"}}