{"cell_type":{"761d7f25":"code","be25b209":"code","2774390d":"code","0733fffd":"code","e2907ed1":"code","eeaf8768":"code","0c9e1161":"code","ef93c1c2":"code","ce87f45c":"code","acf1c8ab":"code","3a004a25":"code","3ecb0bad":"code","4343bbb8":"code","f105e93b":"code","fa513ffd":"code","bf9d9f06":"code","70c995c0":"code","360fa6b6":"markdown","aa4a65e6":"markdown","7b9633db":"markdown","fdf941e2":"markdown","e5828b73":"markdown","45c6c9f2":"markdown","3df1e7f9":"markdown","412f9f6d":"markdown","f1c21494":"markdown","a3df7b31":"markdown","d0426556":"markdown","db86bf8f":"markdown","699f95bb":"markdown"},"source":{"761d7f25":"import os\n\nimport numpy as np \nimport pandas as pd \n\nimport random\n\nfrom sklearn import metrics\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import models, preprocessing, layers, callbacks, optimizers\nimport sklearn\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n","be25b209":"img_dir = \"..\/input\/images\/images\/\"\n\nlabel_df = pd.read_csv(\"..\/input\/artists.csv\")\nlabel_df.head()","2774390d":"classes = os.listdir(img_dir)","0733fffd":"df = label_df\ndf = df.sort_values(by=['name'], ascending=True)\n\nfigsize=(20, 5)\n\nticksize = 14\ntitlesize = ticksize + 8\nlabelsize = ticksize + 5\n\nxlabel = \"Artist\"\nylabel = \"Painting Count\"\n\ntitle = \"Painting Count by Artist\"\n\n\nparams = {'figure.figsize' : figsize,\n          'axes.labelsize' : labelsize,\n          'axes.titlesize' : titlesize,\n          'xtick.labelsize': ticksize,\n          'ytick.labelsize': ticksize}\n\nplt.rcParams.update(params)\n\ncol1 = \"name\"\ncol2 = \"paintings\"\n\nsns.barplot(x=col1, y=col2, data=df)\nplt.title(title)\nplt.xlabel(xlabel)\nplt.ylabel(ylabel)\nplt.xticks(rotation=90)\nplt.plot()","e2907ed1":"def plot_image(file, directory=None, sub=False, aspect=None, title=\"\"):\n    path = directory + \"\/\" + file\n    \n    img = plt.imread(path)\n    \n    plt.imshow(img, aspect=aspect)\n    plt.title(title)\n    plt.xticks([])\n    plt.yticks([])\n    \n    if sub:\n        plt.show()\n        \ndef plot_img_dir(directory=img_dir, count=5, df=None, label_df=label_df):\n    selected_files = random.sample(os.listdir(directory), count)\n    \n    ncols = 5\n    nrows = count\/\/ncols if count%ncols==0 else count\/\/ncols+1\n    \n    figsize=(20, ncols*nrows)\n\n    ticksize = 14\n    titlesize = ticksize + 8\n    labelsize = ticksize + 5\n\n\n    params = {'figure.figsize' : figsize,\n              'axes.labelsize' : labelsize,\n              'axes.titlesize' : titlesize,\n              'xtick.labelsize': ticksize,\n              'ytick.labelsize': ticksize}\n\n    plt.rcParams.update(params)\n    \n    i=0\n    title = \"\"\n    for file in selected_files:        \n        plt.subplot(nrows, ncols, i+1)\n        path = directory + file     \n        plot_image(file, directory, aspect=None, title=title)\n\n        i=i+1\n    \n    plt.tight_layout()\n    plt.show()\n    \n\ndef plot_img_dir_main(directory=img_dir, count=5, label_df=label_df):\n    labels = os.listdir(directory)\n    labels.sort()\n    \n    for label in labels:\n        artist = \" \".join(label.split(\"_\")).strip()\n        print(artist)\n        info_df = label_df[label_df[\"name\"]==artist]\n        if len(info_df) > 0:\n            print(\"Years: \", info_df[\"years\"].values[0], \", Genre: \", info_df[\"genre\"].values[0], \", Nationality: \", info_df[\"nationality\"].values[0], \", Number of Paintings: \", info_df[\"paintings\"].values[0])\n        plot_img_dir(directory=directory+\"\/\"+label+\"\/\", count=count)\n        ","eeaf8768":"plot_img_dir_main(directory=img_dir, count=5, label_df=label_df)","0c9e1161":"rescale = 1.0\/255\nIMG_SIZE = 224\nTARGET_SIZE = (IMG_SIZE, IMG_SIZE)\nCLASSES = os.listdir(img_dir)\nNUM_CLASSES = len(CLASSES)\nBATCH_SIZE = 64\n# BATCH_SIZE = 8 * strategy.num_replicas_in_sync\n\ntrain_batch_size = BATCH_SIZE\nvalidation_batch_size = BATCH_SIZE * 5\ntest_batch_size = BATCH_SIZE * 5\n\n\n\n# Calculate Class Weights\ndef get_weight(y, NUM_CLASSES):\n    class_weights =  sklearn.utils.class_weight.compute_class_weight('balanced', np.unique(y), y)\n    return dict(enumerate(class_weights))\n\n\n    \n\ntrain_datagen = preprocessing.image.ImageDataGenerator(\n    rescale=rescale,\n    rotation_range=45,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    horizontal_flip=True,\n    validation_split=0.05)\n\ntrain_generator = train_datagen.flow_from_directory( \n    img_dir,\n    target_size=TARGET_SIZE, \n    classes=CLASSES,\n    class_mode=\"categorical\", \n    batch_size=train_batch_size, \n    shuffle=True, \n    seed=42,\n    subset='training')\n\nvalidation_generator = train_datagen.flow_from_directory(\n    img_dir,\n    classes=CLASSES,\n    target_size=TARGET_SIZE, \n    class_mode=\"categorical\", \n    batch_size=validation_batch_size, \n    shuffle=False, \n    seed=42,\n    subset='validation')\n\n\n\nclass_weights = get_weight(train_generator.classes, NUM_CLASSES)\n\nsteps_per_epoch = len(train_generator)\nvalidation_steps = len(validation_generator)","ef93c1c2":"figsize=(20, 5)\n\nticksize = 14\ntitlesize = ticksize + 8\nlabelsize = ticksize + 5\n\n\nparams = {'figure.figsize' : figsize,\n          'axes.labelsize' : labelsize,\n          'axes.titlesize' : titlesize,\n          'xtick.labelsize': ticksize,\n          'ytick.labelsize': ticksize}\n\nplt.rcParams.update(params)\n\n\nxlabel = \"Artist No\"\nylabel = \"Painting Count\"\n\ncol1 = \"name\"\ncol2 = \"paintings\"\n\n\ntitle = \"Training Set - Painting Count by Artist\"\n\nsns.countplot(train_generator.labels)\nplt.title(title)\nplt.xlabel(xlabel)\nplt.ylabel(ylabel)\nplt.xticks(rotation=90)\nplt.plot()\nplt.show()\n\n\ntitle = \"Validation Set - Painting Count by Artist\"\n\nsns.countplot(validation_generator.labels)\nplt.title(title)\nplt.xlabel(xlabel)\nplt.ylabel(ylabel)\nplt.xticks(rotation=90)\nplt.plot()\nplt.show()","ce87f45c":"INPUT_SHAPE = (IMG_SIZE, IMG_SIZE, 3)\n\nweights = 'imagenet'\ndense_units = 1024\n\ninputs = layers.Input(INPUT_SHAPE)\n\nbase_model = tf.keras.applications.InceptionResNetV2(\n    include_top=False,\n    weights=weights,\n    input_shape=INPUT_SHAPE,\n)\n            \n\nx = base_model.output\nx = layers.BatchNormalization()(x)\nx = layers.Dropout(0.5)(x)\nx = layers.GlobalAveragePooling2D()(x)\nx = layers.BatchNormalization()(x)\nx = layers.Dropout(0.5)(x)\n\nx = layers.Dense(dense_units)(x)\nx = layers.BatchNormalization()(x)\nx = layers.Activation(activation='relu')(x)\nx = layers.Dropout(0.5)(x)\n\noutputs = layers.Dense(NUM_CLASSES, activation=\"softmax\")(x)\n\n\nmodel = keras.Model(inputs=base_model.input, outputs=outputs)\n\n# model.summary()","acf1c8ab":"OPTIMIZER = optimizers.Adam()\n# OPTIMIZER = optimizers.Adam(learning_rate=0.0001)\n\nEARLY_STOPPING = callbacks.EarlyStopping(\n    monitor='val_loss',\n    patience=5,\n    verbose=1,\n    restore_best_weights=True)\n\n\nREDUCE_LR = callbacks.ReduceLROnPlateau(\n    monitor='val_loss',\n    factor=0.8,\n    patience=1,\n    min_lr=0.000001,\n    verbose=1)\n\nCALLBACKS = [REDUCE_LR, EARLY_STOPPING]","3a004a25":"model.compile(optimizer=OPTIMIZER, loss='categorical_crossentropy', metrics=['accuracy'])\n\n\nVERBOSE = 1\nEPOCHS = 100\n\nprint(\"Trainning Model ...\\n\")\nhistory = model.fit_generator(\n    train_generator,\n    steps_per_epoch=steps_per_epoch,\n    epochs=EPOCHS,\n    verbose=VERBOSE,\n    callbacks=CALLBACKS,\n    validation_data=validation_generator,\n    validation_steps=validation_steps,\n    class_weight=class_weights\n)","3ecb0bad":"def plot_performance(history=None, figure_directory=None):\n    xlabel = 'Epoch'\n    legends = ['Training', 'Validation']\n\n#     ylim_pad = [0.1, 0.005]\n    ylim_pad = [0, 0.5]\n\n\n    plt.figure(figsize=(20, 5))\n\n    # Plot training & validation Accuracy values\n\n    y1 = history.history['acc']\n    y2 = history.history['val_acc']\n\n    min_y = min(min(y1), min(y2))-ylim_pad[0]\n    max_y = max(max(y1), max(y2))+ylim_pad[0]\n    \n    min_y = 0\n    max_y = 1\n\n\n    plt.subplot(121)\n\n    plt.plot(y1)\n    plt.plot(y2)\n\n    plt.title('Model Accuracy\\n', fontsize=17)\n    plt.xlabel(xlabel, fontsize=15)\n    plt.ylabel('Accuracy', fontsize=15)\n    plt.ylim(min_y, max_y)\n    plt.legend(legends, loc='upper left')\n    plt.grid()\n\n\n    # Plot training & validation loss values\n\n    y1 = history.history['loss']\n    y2 = history.history['val_loss']\n\n    min_y = min(min(y1), min(y2))-ylim_pad[1]\n    max_y = max(max(y1), max(y2))+ylim_pad[1]\n\n#     min_y = 0\n#     max_y = 4\n\n    plt.subplot(122)\n\n    plt.plot(y1)\n    plt.plot(y2)\n\n    plt.title('Model Loss\\n', fontsize=17)\n    plt.xlabel(xlabel, fontsize=15)\n    plt.ylabel('Loss', fontsize=15)\n    plt.ylim(min_y, max_y)\n    plt.legend(legends, loc='upper left')\n    plt.grid()\n    plt.show()","4343bbb8":"plot_performance(history=history)","f105e93b":"# validation_generator_test = train_datagen.flow_from_directory(\n#     img_dir,\n#     classes=CLASSES,\n#     target_size=TARGET_SIZE, \n#     class_mode=\"categorical\", \n#     batch_size=validation_batch_size, \n#     shuffle=False, \n#     seed=42,\n#     subset='validation')","fa513ffd":"y_trues = validation_generator.labels\ny_preds = model.predict(validation_generator, verbose=VERBOSE)\ny_preds = y_preds.argmax(axis=1)","bf9d9f06":"matrix = metrics.confusion_matrix(y_trues, y_preds)\n\nplt.figure(figsize = (18,10))\nsns.heatmap(matrix\/np.max(matrix), cmap='Blues')\nplt.show()","70c995c0":"print(metrics.classification_report(y_trues, y_preds, digits=3))","360fa6b6":"![](https:\/\/wisetoast.com\/wp-content\/uploads\/2015\/10\/girl-with-a-pearl-earring-johannes-painting.jpg)","aa4a65e6":"# Artist Prediction from Artworks\n","7b9633db":"# 8. Training Performance Visualization","fdf941e2":"# 6. Configure Callbacks","e5828b73":"# 7. Train Model\n\nTrained model on full tranning dataset for 10 epochs and validated on full validation dataset. Adjusted class weight has been used for trainning. For optimization used Adam optimizer with learning rate of 0.0001. For loss calculation used categorical crossentropy and for model performance evaluation used accuracy metrics.  ","45c6c9f2":"# 2. Read Labels","3df1e7f9":"# 3. Visualization","412f9f6d":"# 5. Create Model","f1c21494":"# 9. Prediction","a3df7b31":"# 10. Model Performance","d0426556":"During Preprocessing, all of the image has been transformed to target size (299, 299) and has been rescaled to unit value. (299, 299) is the input shape for Pretrained model \"InceptionResNetV2\". The target class is treated as categorical and both training and validation image set has been re-shuffled. Some of the images has been horizontally flipped randomly and width and height shift and rotation has been changed randomly to introduce heterogeneity in dataset. A part of training dataset has been used as validation set. ","db86bf8f":"# 1. Import \n","699f95bb":"# 4. Preprocess"}}