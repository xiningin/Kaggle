{"cell_type":{"c9222b21":"code","7cf1b526":"code","3ae7b507":"code","9eb111bf":"code","5cb4562f":"code","e5bf1465":"code","c236cd01":"code","7e71f58c":"code","ce9990b7":"code","dad2af65":"code","8b4c63bc":"code","b6c91349":"code","6d33178b":"code","5726b1c1":"code","0ec113ea":"code","bac39632":"code","5e447db6":"code","6a403f6c":"code","2964f224":"code","7a57b3b1":"code","441844ed":"code","40bee05b":"code","8644f850":"code","95b1a23b":"code","c805e927":"code","53f2bcc8":"code","ba7efc9c":"code","e6d6bd83":"code","d333bfa1":"code","ace0a948":"code","8de13928":"code","b856899b":"code","55768dcb":"code","1dacadcc":"code","80a7bde8":"code","6664b5cf":"markdown","dfb40389":"markdown","e22538c3":"markdown","5e6f56a4":"markdown","ed4fc681":"markdown","b6ebbfab":"markdown","460b2a3e":"markdown","da0c1bcb":"markdown","a79ab897":"markdown","bb26c36a":"markdown","02c70a65":"markdown","71dd6825":"markdown","7ea72af5":"markdown","3b3462c5":"markdown","65808a42":"markdown","9cbcaa5e":"markdown","3ba150a0":"markdown","97e2210b":"markdown","7db1a598":"markdown","e7ab5c3d":"markdown","01a9a38d":"markdown","adb89877":"markdown","5164c2ac":"markdown","1edba538":"markdown","50c3c149":"markdown","7460e5ab":"markdown","209b00fb":"markdown","c17d6dad":"markdown","128adf09":"markdown","27c3a652":"markdown","6d97fba8":"markdown","05e5c225":"markdown","5f3d2169":"markdown","2a5d2727":"markdown","a3a4152b":"markdown","00ce89e9":"markdown","14f7a2ca":"markdown","f512bea1":"markdown","0664ca92":"markdown","bef6d709":"markdown","fa0db207":"markdown","0cc4f592":"markdown","ee56e37a":"markdown","0f2d84f4":"markdown","b232a3c3":"markdown","fd64a790":"markdown","81288d61":"markdown","1e9fd016":"markdown","d4f080ab":"markdown","81c3a27d":"markdown","01d319bb":"markdown","ca18d917":"markdown","6f9c8c83":"markdown"},"source":{"c9222b21":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\n\n#algorithms to use\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree\nfrom sklearn.ensemble import RandomForestClassifier\n\n#Metrics to evaluate the model\nfrom sklearn.metrics import confusion_matrix, classification_report, recall_score\nfrom sklearn import metrics\n\n#for hyperparameter tuning\nfrom sklearn.model_selection import GridSearchCV\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","7cf1b526":"data = pd.read_csv(\"..\/input\/creditrisk\/CreditRisk.csv\")\ndata.head()","3ae7b507":"data.info()","9eb111bf":"data.Loan_ID.nunique()","5cb4562f":"data.drop(columns=['Loan_ID'], inplace=True)","e5bf1465":"#Creating a list of numerical columns\nnum_cols = ['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'Loan_Amount_Term']\n\ndata[num_cols].describe().T","c236cd01":"#Converting the scale of loan term from months to years\ndata['Loan_Amount_Term']=data['Loan_Amount_Term']\/12","7e71f58c":"#Adding the applicant and co-applicant income to get the total income per application\ndata['total_income']=data['ApplicantIncome'] + data['CoapplicantIncome']","ce9990b7":"#Dropping the columns as we created a new column which captures the same information\ndata.drop(columns=['ApplicantIncome', 'CoapplicantIncome'], inplace=True)","dad2af65":"#Creating list of categorical columns\ncat_col= ['Gender', 'Married', 'Dependents', 'Education', 'Self_Employed', 'Credit_History','Property_Area', 'Loan_Status']\n\nfor col in cat_col:\n    print(data[col].value_counts(normalize=True))  #The parameter normalize=True gives the percentage of each category\n    print('*'*40)                                  #Print the * 40 times to separate different variables","8b4c63bc":"#Imputing missing values with mode for the categorical variables \nfor col in ['Gender', 'Married', 'Dependents', 'Self_Employed', 'Loan_Amount_Term', 'Credit_History']:\n    data[col].fillna(value=data[col].mode()[0], inplace=True)","b6c91349":"#Replacing 0's with null values in loan amount \ndata.LoanAmount.replace(0, np.nan, inplace=True)\n\n#Imputing null values in loan amount with the median\ndata.LoanAmount.fillna(value=data.LoanAmount.median(), inplace=True)","6d33178b":"#Separating target variable and other variables\nX=data.drop(columns='Loan_Status')\nY=data['Loan_Status']","5726b1c1":"#Creating dummy variables \n#drop_first=True is used to avoid redundant variables\nX = pd.get_dummies(X, drop_first=True)","0ec113ea":"#Splitting the data into train and test sets\nX_train,X_test,y_train,y_test=train_test_split(X, Y, test_size=0.30, random_state=7)","bac39632":"#function to print classification report and get confusion matrix in a proper format\n\ndef metrics_score(actual, predicted):\n    print(classification_report(actual, predicted))\n    cm = confusion_matrix(actual, predicted)\n    plt.figure(figsize=(8,5))\n    sns.heatmap(cm, annot=True,  fmt='.2f', xticklabels=['Not Eligible', 'Eligible'], yticklabels=['Not Eligible', 'Eligible'])\n    plt.ylabel('Actual')\n    plt.xlabel('Predicted')\n    plt.show()","5e447db6":"#Fitting the decision tree classifier on the training data\nd_tree =   DecisionTreeClassifier(class_weight={0:0.69,1:0.31}, random_state=7)\n\nd_tree.fit(X_train, y_train)","6a403f6c":"#Checking performance on the training data\ny_pred_train1 = d_tree.predict(X_train)\nmetrics_score(y_train,y_pred_train1)","2964f224":"#Checking performance on the testing data\ny_pred_test1 = d_tree.predict(X_test)\nmetrics_score(y_test,y_pred_test1)","7a57b3b1":"# Choose the type of classifier \nd_tree_tuned = DecisionTreeClassifier(random_state=7, class_weight={0:0.7, 1:0.3})\n\n# Grid of parameters to choose from\nparameters = {'max_depth': np.arange(2,10), \n              'criterion': ['gini', 'entropy'],\n              'min_samples_leaf': [5, 10, 20, 25]\n             }\n\n# Type of scoring used to compare parameter combinations - recall score for class 0\nscorer = metrics.make_scorer(recall_score, pos_label=0)\n\n# Run the grid search\ngrid_obj = GridSearchCV(d_tree_tuned, parameters, scoring=scorer, cv=5)\ngrid_obj = grid_obj.fit(X_train, y_train)\n\n# Set the clf to the best combination of parameters\nd_tree_tuned = grid_obj.best_estimator_\n\n# Fit the best algorithm to the data. \nd_tree_tuned.fit(X_train, y_train)","441844ed":"#Checking performance on the training data\ny_pred_train2 = d_tree_tuned.predict(X_train)\nmetrics_score(y_train,y_pred_train2)","40bee05b":"#Checking performance on the testing data\ny_pred_test2 = d_tree_tuned.predict(X_test)\nmetrics_score(y_test,y_pred_test2)","8644f850":"features = list(X.columns)\n\nplt.figure(figsize=(20,20))\n\ntree.plot_tree(d_tree_tuned,feature_names=features,filled=True,fontsize=9,node_ids=True,class_names=True)\nplt.show()","95b1a23b":"# importance of features in the tree building\n\nprint (pd.DataFrame(d_tree_tuned.feature_importances_, columns = [\"Imp\"], index = X_train.columns).sort_values(by = 'Imp', ascending = False))","c805e927":"#Plotting the feature importance\nimportances = d_tree_tuned.feature_importances_\nindices = np.argsort(importances)\n\nplt.figure(figsize=(10,10))\nplt.title('Feature Importances')\nplt.barh(range(len(indices)), importances[indices], color='violet', align='center')\nplt.yticks(range(len(indices)), [features[i] for i in indices])\nplt.xlabel('Relative Importance')\nplt.show()","53f2bcc8":"#Fitting the decision tree classifier on the training data\nrf_estimator = RandomForestClassifier(random_state=7)\n\nrf_estimator.fit(X_train,y_train)","ba7efc9c":"#Checking performance on the training data\ny_pred_train3 = rf_estimator.predict(X_train)\nmetrics_score(y_train, y_pred_train3)","e6d6bd83":"#Checking performance on the testing data\ny_pred_test3 = rf_estimator.predict(X_test)\nmetrics_score(y_test, y_pred_test3)","d333bfa1":"# Choose the type of classifier. \nrf_estimator_tuned = RandomForestClassifier(criterion=\"entropy\", random_state=7)\n\n# Grid of parameters to choose from\nparameters = {\"n_estimators\": [100, 110, 120],\n    \"max_depth\": [5, 6, 7],\n    \"max_features\": [0.8, 0.9, 1]\n             }\n\n# Type of scoring used to compare parameter combinations - recall score for class 0\nscorer = metrics.make_scorer(recall_score, pos_label=0)\n\n# Run the grid search\ngrid_obj = GridSearchCV(rf_estimator_tuned, parameters, scoring=scorer, cv=5)\ngrid_obj = grid_obj.fit(X_train, y_train)\n\n# Set the clf to the best combination of parameters\nrf_estimator_tuned = grid_obj.best_estimator_","ace0a948":"#Fitting the best algorithm to the training data\nrf_estimator_tuned.fit(X_train, y_train)","8de13928":"#Checking performance on the training data\ny_pred_train4 = rf_estimator_tuned.predict(X_train)\nmetrics_score(y_train, y_pred_train4)","b856899b":"# Choose the type of classifier. \nrf_estimator_tuned = RandomForestClassifier(criterion=\"entropy\", random_state=7)\n\n# Grid of parameters to choose from\nparameters = {\"n_estimators\": [110, 120],\n    \"max_depth\": [6, 7],\n    \"min_samples_leaf\": [20, 25],\n    \"max_features\": [0.8, 0.9],\n    \"max_samples\": [0.9, 1],\n    \"class_weight\": [{0:0.7, 1:0.3}, \"balanced\", {0:0.4, 1:0.1}]\n             }\n\n# Type of scoring used to compare parameter combinations - recall score for class 0\nscorer = metrics.make_scorer(recall_score, pos_label=0)\n\n# Run the grid search on the training data using scorer=scorer and cv=5\ngrid_obj =  GridSearchCV(rf_estimator_tuned, parameters, scoring=scorer, cv=5)\ngrid_obj = grid_obj.fit(X_train, y_train)\n\n# Save the best estimator to variable rf_estimator_tuned\nrf_estimator_tuned = grid_obj.best_estimator_\n\n#Fit the best algorithm to the training data\nrf_estimator_tuned.fit(X_train, y_train)","55768dcb":"#Checking performance on the training data\ny_pred_train5 =  rf_estimator_tuned.predict(X_train)\nmetrics_score(y_train, y_pred_train5)","1dacadcc":"#Checking performance on the testing data\ny_pred_test5 = rf_estimator_tuned.predict(X_test)\nmetrics_score(y_test, y_pred_test5)","80a7bde8":"importances = rf_estimator_tuned.feature_importances_\nindices = np.argsort(importances)\nfeature_names = list(X.columns)\n\nplt.figure(figsize=(12,12))\nplt.title('Feature Importances')\nplt.barh(range(len(indices)), importances[indices], color='violet', align='center')\nplt.yticks(range(len(indices)), [feature_names[i] for i in indices])\nplt.xlabel('Relative Importance')\nplt.show()","6664b5cf":"**Note:** We have explored this data earlier in the project for classification. Here, we will simply look at some basic univariate analysis and data preprocessing and move to the model building section.","dfb40389":"## **Checking the info of the data**","e22538c3":"**Let's check the model performance on the testing data**","5e6f56a4":"**Observations:**\n\n- There are **614 observations and 13 columns** in the data.\n- ApplicantIncome, CoapplicantIncome, Loan_Amount_Term, Credit_History, and Loan_Status are numeric data types. **The rest of the variables are of the object data type.**\n- There are several columns with less than 614 non-null entries i.e. **these columns have missing values.**\n- **Loan_ID column is an identifier.** Let's check if each entry of the column is unique.","ed4fc681":"**One of the drawbacks of ensemble models is that we lose the ability to obtain an interpretation of the model. We cannot observe the decision rules for random forests the way we did for decision trees. So, let's just check the feature importances of the model.**","b6ebbfab":"## **Decision Tree - Hyperparameter Tuning**\n\nWe will use the class_weight hyperaparameter with value equal to {0:0.7, 1:0.3} which is approximately the opposite of the imbalance in the original data. \n\n**This would tell the model that 0 is the important class here.**","460b2a3e":"## **Exploratory Data Analysis and Data Preprocessing**","da0c1bcb":"## **Building Classification Models**","a79ab897":"**Let's check the performance of the tuned model:**","bb26c36a":"Now that we are done with the data preprocessing. Let's move onto the model building section.","02c70a65":"## **Decision Tree**\n\nIn this section we will implement a decision tree classifier on the data.\n","71dd6825":"**Observations:_____________**\n- **As observed earlier, credit history is the most important feature** **followed by total income and loan amount** which makes sense.\n- **Property_Area_Semiurban and Married_Yes and also have some importance**.\n- Education_Not Graduate, Property_Area_Urban, Dependant_2, Gneder_Male and Dependant_1 also have smaller impact and the rest of the variables have no impact in this model, while deciding loan eligibility.","7ea72af5":"**Observations:_____________**\n- As compared to the base model, **the performance on the training set is lower but but overall value is better (79%)**\n- The recall for class 0 is lower than the base model but overall is a higher value(~85)\n- The model training data set is giving a lower average precision. A precision of ~0.52 of class 0 suggests that there is a 48% (1 - 0.52) chance that the model will predict that a person is eleigible but the model predict them as not eligible. ","3b3462c5":"- We can see that **all the entries of this column are unique.** Hence, this column would not add any value to our analysis. \n- Let's drop this column.","65808a42":"## **Random Forest Classifier**","9cbcaa5e":"## **Summary Statistics for numerical columns**","3ba150a0":"**Observations:_____________**\n- The Decision Tree works well on the training data but **not well on the test data** as the recall is 0.59 as compared to 1 for training dataset. i.e. The Decision Tree is over fitting on the data.\n- The recall for class 0 is quite low(~59), which implies that this model will not perform well in predicting the applicants who have a high chance of not eligible for the loan, and hence this model might not help identifying non eleigible applicants. This model will predict more non eligible applicants as eligible which will impact highly negatively to the bank.\n- The model is giving a comparatively lower average precision as well. A precision of ~0.55 of class 0 suggests that there is a 45% (1 - 0.55) chance that the model will predict that a person is eleigible but the model predict them as not eligible. \n","97e2210b":"We have tuned the model. Now, **let's fit the tuned model on the training data** and check the model performance on the training and testing data.","7db1a598":"#### Imputing missing values","e7ab5c3d":"# **Project: Decision Trees and Random Forest - Loan Eligibility Prediction**\n\n# Marks: 30\n\n**Credit risk is the risk of financial loss resulting from the failure by a borrower to repay the principal and interest owed to the lender.** The lender uses the interest payments from the loan to compensate for the risk of potential losses. When the borrower defaults on his\/her obligations, it causes an interruption in the cash flow of the lender.\n\nIn the banking sector, this is an important factor to be considered before approving the loan of an applicant in order to cushion the lender from loss of cash flow and reduce the severity of losses. \n\n------------------\n## **Objective:**\n------------------\n\nABC finance company wants to automate this loan eligibility process. They want to harness their past customers' data to **build a model to predict whether the loan should be approved or not.** This would help the company prevent potential losses, save time and focus more on eligible customers.\n\n--------------------------\n## **Data Dictionary:**\n--------------------------\n\nThe data contains the following attributes:\n\n* **Loan_ID**: Unique Loan ID\n* **Gender**: Gender of the applicant - Male\/Female\n* **Married**: Whether the applicant is married or not (Yes\/No)\n* **Dependents**: Number of dependents of the applicant\n* **Education**: Applicant's education (Graduate\/Not Graduate)\n* **Self_Employed**: Whether the applicant is self-employed (Yes\/No)\n* **ApplicantIncome**: The income of the applicant (\\$)\n* **CoapplicantIncome**: The co-applicant's income in case of a joint loan and 0 otherwise (\\$)\n* **LoanAmount**: Loan amount (dollars in thousands) \n* **Loan_Amount_Term**: Term of loan in months\n* **Credit_History**: Whether the applicant's credit history meets required guidelines\n* **Property_Area**: The area the property pertaining to the loan belongs to - Urban\/Semi-Urban\/Rural\n* **Loan_Status**: Loan approved (1 - Yes, 0 - No)","01a9a38d":"**Before training the model, let's choose the appropriate model evaluation criterion as per the problem on hand.**\n\n### Model evaluation criterion:\n\n#### Since this is a binary classification problem, the model can make wrong predictions in one of two ways:\n1. Predicting a customer is not eligible for a loan when the customer actually is eligible - **Loss of opportunity**\n2. Predicting a customer is eligible for a loan when the customer is actually not - **Financial Loss**\n\n#### Which case is more important? \n* **Predicting that the customer is eligible when he is not**, because the person might default or not be able to return the loan, which would result in financial loss to the company.\n\n#### How to reduce this loss i.e need to reduce False Negatives?\n* In classification, the class of interest is considered the positive class. Here, the class of interest is 0 i.e. identifying non-eligible customers. So, the company wants to minimize the number of false negatives, in other words **Recall** is the evaluation criterion to be maximized.","adb89877":"**Let's check the performance on the training data:**","5164c2ac":"#### Preparing data for modeling","1edba538":"## **Feature Engineering**","50c3c149":"**Observations:_____________**\n- - The tuned model is also comparatively overfitting on the training dataset, but **it shows good performance on the test dataset as well(~66).**\n- **The recall for the tuned model has improved from 48% to 70%** with a significat decrease in precision (nearly 38%). \n- **This model is the best-performing one among all the models so far,** and is giving us good precision and recall scores on the test dataset.","7460e5ab":"### Note\n- We have already seen the classification performance earlier using Logistic Regression and the KNN classifier.\n- Here, **we will aim to get better recall for class 0** as the company would want the model to correctly identify as many of the 'non-eligible' customers as possible.\n\nAlso, let's create a function to calculate and print the classification report and confusion matrix so that we don't have to rewrite the same code repeatedly for each model.","209b00fb":"**Let's visualize the tuned decision tree** and observe the decision rules:","c17d6dad":"Let's try **tuning some of the important hyperparameters of the Random Forest Classifier**. \n\nWe will **not** tune the `criterion` hyperparameter as we know from hyperparameter tuning for decision trees that `entropy` is a better splitting criterion for this data.","128adf09":"**Observations:**\n\n- The average income of applicants is about 5.4K dollars. It has a large range of values from 150 to 81,000. It would be interesting to see if low applicant income implies a low loan amount.\n- The average co-applicant income is about 1.6K dollars which is much lower than the applicant's income. The 25th percentile value is 0 and the median value is 1,188 dollars which implies that the number of joint home loans is more than the number of non-joint loans.\n- The two columns `ApplicantIncome` and `CoapplicantIncome` give the same information i.e. income of applicants. It would be better to have that information in one column only. We can add these two columns to get the total income per application.\n- The 25th, 50th, and 75th percentile value of the loan term is 360 months i.e. 30 years. This implies that the majority of home loans in this dataset are for 30 years of term.\n- We can convert the scale of the loan term from months to years to make it easier to read.\n- The average loan amount is 1.41K dollars. It has a large range of values, which is to be expected. \n- The loan amount has zero values, which is absurd since loan amounts are meant to be non-zero. So we can treat 0's as missing values.\n\nBefore imputing the missing values and 0's in `LoanAmount`, let's do some feature engineering and check the distributions, counts, and outliers for different variables.","27c3a652":"**Let's check the performance of the model on the training data:**","6d97fba8":"## **Conclusion and Recommendations**","05e5c225":"## **Loading the data**","5f3d2169":"**Let's confirm this by checking its performance on the testing data:**","2a5d2727":" **Let's see if we can get a better model by tuning the random forest classifier:**","a3a4152b":"**Observations:_____________**\n- The Decision tree is giving a **100% result on the training dataset.**","00ce89e9":"**Observations:_____________**\n\n- The tuned Decision Tree does not work well on test data either and have even lower recall on test data than training data. So we can say tuned model also is overfitting but not stongly overfitting as the base model.\n- Compared to the base model, the tuned model recall also approximately has same value(~61)\n- The recall for class 0 is quite low(~61), which implies that this model will not perform well in predicting the applicants who have a high chance of not eligible for the loan, and hence this model might not help identifying non eleigible applicants. This model will predict more non eligible applicants as eligible which will impact highly negatively to the bank.\n- The model is giving a comparatively lower average precision as well. A precision of ~0.51 of class 0 suggests that there is a 49% (1 - 0.51) chance that the model will predict that a person is eleigible but the model predict them as not eligible. ","14f7a2ca":"## **Checking the percentage of each category for categorical variables**","f512bea1":"**Let's look at the feature importance** of the tuned decision tree model:","0664ca92":"**Let's check the performance of the model on the testing data:**","bef6d709":"## **Importing the necessary libraries and overview of the dataset**","fa0db207":"**Let's check the model performance on the test data:**","0cc4f592":"**Observations:_____________**\n- As compared to the base model, **the performance on the training set has gone down significantly.**\n- The recall for class 0 is low(~79), which implies that compared to the base model this model might not perform well in predicting the applicants who have a high chance of not eligible for the loan. \n- The model trainingdata set is giving a lower average precision as well. A precision of ~0.57 of class 0 suggests that there is a 43% (1 - 0.57) chance that the model will predict that a person is eleigible but the model predict them as not eligible. \n","ee56e37a":"**Let's try hyperparameter tuning using GridSearchCV to find the optimal max_depth** in order to reduce overfitting of the model. We can tune some other hyperparameters as well.","0f2d84f4":"**Tuning the random forest classifier using GridSearchCV and check the performance**","b232a3c3":"## **Random Forest Classifier - Hyperparameter Tuning**","fd64a790":"- **As observed earlier, credit history is the most important feature** **followed by total income and loan amount** which makes sense.\n- **Married_Yes and Property_Area_Semiurban have some importance** and the rest of the variables have no impact in this model, while deciding loan eligibility.\n\nNow let's build another model - **a random forest classifier**","81288d61":"**Observations:_____________**\n- The Random Forest is giving a **100% result on the training dataset.** ","1e9fd016":"**Observations:_____________**\n\n- applicants who has lower credit value has a high possibility of get classified as not eligible.\n- married applicants who have higher credit value has loan amount less than 259.5K have a chance of eligible for the loan \n- Single (Not married) applicants who have higher credit value and have total income greater than 6157.5 have a possibility of not eligible for the loan. This might be because bank might not want to approve loans for rich single people.\n- Moreover, Single (Not married) applicants who have higher credit value and have total income less than 3589 have a possibility of not eligible for the loan. This might be because bank might not want to approve loans for poor single people either.\n- Also, Single (Not married) applicants who have higher credit value and have total income less than 6157.5 but greater than (~3690) have a possibility of eligible for the loan. This might be because bank might approve the loan for single people with average total income. \n- Married applicants live in Semi urban area seems to have high possibly getting approved for loans.\n- Also, married applicants who have higher credit value, who does not live in Semi urban area and who has the loan amount lesser than 127k have a possibility of not eligible.  This might be because bank doesn\u2019t want to approve lower loan amounts even if they have higher credit values and lives in premium (semiurban) area.\n\n","d4f080ab":"**Observations:**\n\n- Percentage of male customers (approx 81%) is more than the percentage of female customers (approx 19%).\n- About 65% of customers are married. This indicates that married people apply more for the home loan as compared to non-married people.\n- The majority of customers have 0 or 1 dependents. Some of these entries are 3+ which means 3 or more dependents. Since we don't know the actual number, we can treat this as a separate category.\n- About 78% of customers are graduate. It looks like the majority of customers are educated.\n- Only ~14% of customers are self-employed. This indicates that salaried people apply more for the home loan.\n- The majority of customers, about 84%, have credit history as per the required guidelines.\n- The count of observations is approximately evenly distributed among Urban, Semi-Urban, and Rural areas. This implies that the company has a presence in all kinds of areas.\n- The number of home loans approved is comparatively higher than the number of those not approved. About 69% of applied loans get approved. Although we have limited data, the reason for this could either be a fairly lenient attitude from the company towards approving loans OR that many of the customers applying for these home loans are actually eligible for them.","81c3a27d":"## **Dropping the Loan_ID column**","01d319bb":"**Conclusions___________:**\n- The best model we have got so far is the tuned random forest model which is giving nearly ~70% recall on the test data .\n- The bank should use this model to know beforehand which applicant's loan applications are not eligible so they can avoid risk of losing money to unreliable applicants. \n- credit history is the most important feature followed by total income and loan amount.\n- Total income, Loan amount, Property_Area_Semiurban, Married_Yes and Education_Not Graduate also have some importance.\n\n**Recommendations____________:**\n- People with good credit history have a high possibility of paying off the loan.\n- Married applicants live in Semi urban area seems to have high possibly getting approved for loans.\n- People based on semi urban areas have higher eligibility rate maybe because most of Married people with families tend to live in these areas and they tend to have good approval rates. But we need further information to analyse this observation.\n- Married people tend to have good approval rates than single people so bank can target married people.\n- Single people with good credit score but have higher incomes have lower approval rates. This might be because bank might not want to approve loans for rich single people.\n- Single people with good credit score but have lower incomes also have lower approval rates. This might be because bank might not want to approve loans for single people with lower income either. \n- - Also, married applicants who have higher credit value, who does not live in Semi urban area and who has the loan amount lesser than 127k have a possibility of not eligible.  This might be because bank doesn\u2019t want to approve lower loan amounts even if they have higher credit values and lives in premium (semiurban) area. \n- Applicants without graduate degrees also has a possibility of not eleigible. This is maybe due to the fact that they might not have better jobs and sufficent income to pay of the loan.\n\n","ca18d917":"**Observations:_____________**\n- The Random Forest works well on the training data but **not well on the test data** as the recall is 0.49 as compared to 1 for training dataset. i.e. The Random Forest is over fitting on the data.\n- The recall for class 0 is quite low(~49), which implies that this model will not perform well in predicting the applicants who have a high chance of not eligible for the loan, and hence this model might not help identifying non eleigible applicants. This model will predict more non eligible applicants as eligible which will impact highly negatively to the bank.\n- The model is giving a somewhat good but comparatively lower than training model average precision as well. A precision of ~0.81 of class 0 suggests that there is a 9% (1 - 0.81) chance that the model will predict that a person is eleigible but the model predict them as not eligible. \n","6f9c8c83":"- We can see that after hyperparameter tuning, the model is performing poorly on the train data as well.\n- We can try adding some other hyperparameters and\/or changing values of some hyperparameters to tune the model and see if we can get a better performance.\n\n**Note:** **GridSearchCV can take a long time to run** depending on the number of hyperparameters and the number of values tried for each hyperparameter. **Therefore, we have reduced the number of values passed to each hyperparameter.** "}}