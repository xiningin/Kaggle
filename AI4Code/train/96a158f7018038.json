{"cell_type":{"e5e744bf":"code","2cbbfdde":"code","fcfe8db6":"code","01e05b99":"code","0c048a00":"code","02a54dfb":"code","c1eee3f4":"code","f4aa5280":"code","4bb47e3f":"code","dd2fd7f4":"code","072f2432":"code","1fc4f8ac":"code","e30e109d":"code","1f6f811e":"code","1b24bcc3":"code","597404aa":"code","7496ebd3":"code","0cb272e9":"code","2cf2b584":"code","898ffd76":"code","c1697afd":"code","7755cf00":"code","23171a23":"code","c2b33333":"code","28ac92da":"code","62d109a4":"code","dc051e45":"code","04e30522":"code","7fe55207":"code","eae4c4ba":"code","8c06074d":"code","46302b3e":"code","09305136":"code","b352ef50":"code","00397833":"code","50846dac":"code","899801e7":"code","7a04eb33":"code","bf2d9eb4":"code","cd5534a9":"code","257c58c6":"code","9fdb09ca":"code","b5397b0f":"code","23bc7b77":"code","2c2ea2cc":"code","558c05a6":"code","696a5d55":"code","908f8aef":"code","5cff3d8d":"code","00a6b1a5":"code","416bf108":"code","7ec7fad4":"code","8ce81ed1":"code","2c7efc1d":"code","f66d6c0f":"code","76e46f28":"code","a17cb00d":"code","25385f82":"code","1a0c6375":"code","06ba9154":"code","c3970dca":"code","c835d753":"code","68142379":"code","1dbb67c9":"code","acc8c925":"code","cb184ff1":"code","55c04cda":"code","a340ddd4":"code","f7fd087c":"code","76a655fe":"code","0f3fd605":"code","21ae4914":"code","7e81fbfe":"code","a43291e3":"code","c11c5739":"code","8836ad24":"code","7232c9cd":"markdown","7211082d":"markdown","b285dd91":"markdown","969e80e6":"markdown","d52ede1c":"markdown","1fcf6729":"markdown","052dcf75":"markdown","1c96d8b9":"markdown","a670f03b":"markdown","9e0286f1":"markdown","52968aae":"markdown","f266f69d":"markdown","bbccab50":"markdown","9b203fff":"markdown","36c2a055":"markdown","3ddd35ae":"markdown","ca70c9b8":"markdown","e11d3455":"markdown","d1fa7596":"markdown","94baa61e":"markdown","84f1477a":"markdown","5f6ed710":"markdown","42756cd9":"markdown","f097f41f":"markdown","91b228d8":"markdown","9729bbc1":"markdown","e3cea896":"markdown","1db9bda8":"markdown","0161b222":"markdown","ca5b705d":"markdown","8d94e07f":"markdown","059592d0":"markdown","5ff7cec2":"markdown","2241dba2":"markdown","6dc20dff":"markdown","7f5274a7":"markdown","b64d2abb":"markdown","e06f90e1":"markdown","b05c7b7f":"markdown","32bcaa8a":"markdown","c3f0d642":"markdown","6a81d68e":"markdown","5ae4cf31":"markdown","dc249b0a":"markdown","95e80435":"markdown","7bbec0e3":"markdown","bba5ea3e":"markdown","15529e99":"markdown","765ee78b":"markdown","ad99201b":"markdown","4e17c42a":"markdown","2ee93ace":"markdown","9fdb3a02":"markdown","6b3b7912":"markdown","f275eacc":"markdown","e605e2b1":"markdown","4a2b2966":"markdown","cd707cf8":"markdown","43308194":"markdown","c543c5a5":"markdown","3ef78967":"markdown","f20ac5e3":"markdown","b1e6394c":"markdown","a3c095e3":"markdown"},"source":{"e5e744bf":"import numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport datetime \nimport warnings\nimport csv\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nfrom lightgbm import LGBMRegressor\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBClassifier\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","2cbbfdde":"items = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/items.csv')\nitem_cat = pd.read_csv ('..\/input\/competitive-data-science-predict-future-sales\/item_categories.csv')\ntrainset1 = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/sales_train.csv')\nshops = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/shops.csv')\ntestset= pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/test.csv')","fcfe8db6":"trainset1.info()","01e05b99":"items.info()","0c048a00":"item_cat.info()","02a54dfb":"shops.info()","c1eee3f4":"trainset1.shape","f4aa5280":"display(trainset1, item_cat, items, shops)","4bb47e3f":"trainset2=pd.merge(items, trainset1)","dd2fd7f4":"trainset3=pd.merge(item_cat, trainset2)","072f2432":"train=pd.merge(shops, trainset3)","1fc4f8ac":"train.info()","e30e109d":"train.shape","1f6f811e":"train.describe()","1b24bcc3":"train.isnull().sum()","597404aa":"train['shop_name'].unique()","7496ebd3":"train['shop_id'].unique()","0cb272e9":"train['item_category_name'].unique()","2cf2b584":"train['item_name'].unique()","898ffd76":"train['item_id'].unique()","c1697afd":"train['shop_name'].nunique()","7755cf00":"train['shop_id'].nunique()","23171a23":"train['item_category_name'].nunique()","c2b33333":"train['item_name'].nunique()","28ac92da":"train['item_id'].nunique()","62d109a4":"train_set=train.copy()","dc051e45":"corrmat=train_set.corr()\ncorrmat","04e30522":"plt.figure(figsize=(15, 15))\nsns.set(font_scale=2)\nsns.heatmap(train_set.corr(), annot=True, cmap=\"Blues\", fmt= \".2g\")","7fe55207":"negative_price=train_set[train_set[\"item_price\"] < 0]\nnegative_price\n","eae4c4ba":"price_2973=train_set[(train_set[\"shop_id\"]== 32) & (train_set[\"item_category_name\"]==\"\u0418\u0433\u0440\u044b - PS3\") & (train_set[\"item_id\"]== 2973 ) & (train_set[\"date_block_num\"]== 4)]\nprice_2973","8c06074d":"price_mean=price_2973[(price_2973.shop_id==32)&(price_2973.item_id==2973)&(price_2973.date_block_num==4)&(price_2973.item_price>0)]\nprice_mean.item_price.mean()","46302b3e":"train_set.at[1644072,'item_price']= 1849","09305136":"negative_item_cnt_day=train_set[train_set[\"item_cnt_day\"] < 0]\nnegative_item_cnt_day\n","b352ef50":"cond = train_set['item_cnt_day'] < 0\ncheck= train_set.loc[cond,'item_cnt_day'] = 0\ncheck\n","00397833":"plt.figure(figsize=(10, 15))\ntrain_set.boxplot(['item_price'])\nplt.title('Outliers in item_price', fontsize = 40)\n","50846dac":"# To find the highest possible price within 3 standard deviation\nprint(\"Maximum possible price\",train_set['item_price'].mean() + 3*train_set['item_price'].std())\nprint(\"Lowest possible price\",train_set['item_price'].mean() - 3*train_set['item_price'].std())\n","899801e7":"# To check the highest and lowest price in our dataset\nhp=max(train_set.item_price)\nprint(\"Highest price :\", hp)\nlp=min(train_set.item_price)\nprint(\"Lowest price: \", lp)","7a04eb33":"# Since our lowest selling price is 0.07 we will check the outliers on the higher side\nhighest_price=train_set[train_set[\"item_price\"] > 6080]\nhighest_price","bf2d9eb4":"# To drop the outlier in item_price\ntrain_set = train_set[train_set['item_price'] < 100000]","cd5534a9":"plt.figure(figsize=(10, 15))\ntrain_set.boxplot(['item_cnt_day'])\nplt.title('Outliers in item_cnt_day', fontsize = 40)\n","257c58c6":"# To find the highest and lowest possible nuber of products sold within 3 standard deviation\nprint(\"Maximum possible number of products sold\",train_set['item_cnt_day'].mean() + 3*train_set['item_cnt_day'].std())\nprint(\"Lowest possible number of products sold\",train_set['item_cnt_day'].mean() - 3*train_set['item_cnt_day'].std())\n","9fdb09ca":"# To check for maximum and lowest number of products sold\nhps=max(train_set.item_cnt_day)\nprint(\"Highest number of products sold :\", hps)\nlps=min(train_set.item_cnt_day)\nprint(\"Lowest number of products sold: \", lps)","b5397b0f":"# Since our lowest sold quantity is zero we will check the outliers on the higher side\nhighest_price=train_set[train_set[\"item_cnt_day\"] > 9]\nhighest_price","23bc7b77":"# To drop the extreme outliers in item_cnt_day\ntrain_set = train_set[train_set['item_cnt_day'] < 1000]","2c2ea2cc":"train_set['Sales'] = (train_set[\"item_price\"] * train_set[\"item_cnt_day\"])","558c05a6":"train_set['date'] = pd.to_datetime(train_set['date'])\ndays = []\nmonths = []\nyears = []\n\nfor day in train_set['date']:\n    days.append(day.day)\nfor month in train_set['date']:\n    months.append(month.month)    \nfor year in train_set['date']:\n    years.append(year.year)","696a5d55":"plt.figure(figsize=(10,10))\nsns.countplot(years)\nplt.title('Sales per Year', fontsize = 40)\nplt.xlabel('Year', fontsize = 25)\nplt.ylabel('Sales', fontsize = 25)\nplt.show()","908f8aef":"plt.figure(figsize=(40, 50))\nx= train_set['item_category_id']\ny= train_set['Sales']\nsns.barplot(x, y)\nplt.title('Highest Selling Product', fontsize = 60)\nplt.xlabel('Item categories', fontsize = 30)\nplt.ylabel('Sales', fontsize = 30)\nplt.show()","5cff3d8d":"highest_selling_itemc= train_set.loc[train_set['item_category_id'] == 12]\nhighest_selling_itemc","00a6b1a5":"plt.figure(figsize=(40, 50))\nx= train_set['shop_id']\ny= train_set['Sales']\nsns.barplot(x, y)\nplt.title('Highest Selling Shop', fontsize = 60)\nplt.xlabel('Shop_id', fontsize = 30)\nplt.ylabel('Sales', fontsize = 30)\nplt.show()","416bf108":"highest_selling_shop= train_set.loc[train_set['shop_id'] == 9]\nhighest_selling_shop","7ec7fad4":"plt.figure(figsize=(10,10))\nsns.countplot(months)\nplt.title('Sales per Year', fontsize = 40)\nplt.xlabel('Year', fontsize = 25)\nplt.ylabel('Sales', fontsize = 25)\nplt.show()","8ce81ed1":"sales_by_month = train_set.groupby(['date_block_num'])[[\"Sales\"]].sum()\nplt.figure(figsize=(30,30))\nsales_by_month.plot()\nplt.title('Month with highest Sales in the dataset', fontsize = 10)\nplt.xlabel('Months', fontsize = 10)\nplt.ylabel('Sales', fontsize = 10)","2c7efc1d":"sales_max = train_set.groupby(['date_block_num'])['Sales'].sum()\nsales_max.max()","f66d6c0f":"print(sales_max==232615420.90999845)","76e46f28":"highest_sales_month = train_set[train_set[\"date_block_num\"]==23]\nhighest_sales_month\n","a17cb00d":"highest_store_sales = train_set.groupby(['date_block_num', 'shop_name'])['Sales'].sum()\n\nhighest_store_sales.max()","25385f82":"highest_store_sales=pd.DataFrame(highest_store_sales)\nhighest_store_sales=highest_store_sales.loc[highest_store_sales['Sales'] == 15730394.000000501]\nhighest_store_sales","1a0c6375":"highest_item_sales = train_set.groupby(['date_block_num', 'item_category_name'])['Sales'].sum()\n\nhighest_item_sales.max()","06ba9154":"highest_item_sales=pd.DataFrame(highest_item_sales)\nhighest_item_sales=highest_item_sales.loc[highest_item_sales['Sales'] == 46487721.9999976]\nhighest_item_sales","c3970dca":"train_set = train_set[[\"date_block_num\", \"shop_id\", \"item_id\", \"item_price\", \"item_cnt_day\", \"shop_name\", \"Sales\", \"item_name\"]].groupby(\n    [\"date_block_num\", \"shop_id\", \"item_id\"]).agg(\n    {\"item_cnt_day\": \"sum\", \"Sales\" : \"sum\"}).reset_index()\ntrain_set.rename(columns={\"item_cnt_day\": \"item_cnt_month\"}, inplace=True)","c835d753":"test1=pd.merge(items, testset)\ntest2=pd.merge(item_cat, test1)\ntest = pd.merge(shops,test2)\ntest","68142379":"test=test.drop(\"ID\",1)","1dbb67c9":"test.shape","acc8c925":"train_set_u = train_set['item_id'].nunique()\ntest_u = test['item_id'].nunique()\ntrain_set_s = train_set['shop_id'].nunique()\ntest_s = test['shop_id'].nunique()\nprint(\"Total unique item_ids in train_set dataset: \", train_set_u)\nprint(\"Total unique item_ids in test dataset: \", test_u )\n\nprint(\"Total unique shop_ids train_set dataset: \", train_set_s)\nprint(\"Total unique shop_ids in test dataset: \", test_s)","cb184ff1":"train_set = train_set[train_set['shop_id'].isin(test['shop_id'].unique())]\ntrain_set = train_set[train_set['item_id'].isin(test['item_id'].unique())]\n ","55c04cda":"train_set_u = train_set['item_id'].nunique()\ntest_u = test['item_id'].nunique()\ntrain_set_s = train_set['shop_id'].nunique()\ntest_s = test['shop_id'].nunique()\nprint(\"Total unique item_ids in train_set dataset: \", train_set_u)\nprint(\"Total unique item_ids in test dataset: \", test_u )\n\nprint(\"Total unique shop_ids train_set dataset: \", train_set_s)\nprint(\"Total unique shop_ids in test dataset: \", test_s)","a340ddd4":"test.shape","f7fd087c":"test[\"item_cnt_month\"]=\" \"","76a655fe":"test.info()\ntrain_set.info()","0f3fd605":"# To drop some insignificant columns\ntrain_set=train_set.drop(\"Sales\", 1)\ntrain_set=train_set.drop(\"date_block_num\", 1)\ntest= test.drop(\"item_category_id\", 1)\ntest= test.drop(\"item_category_name\", 1)\ntest= test.drop(\"item_name\", 1)\ntest= test.drop(\"shop_name\", 1)\n","21ae4914":"train_x = train_set.drop(['item_cnt_month'], axis=1)\ntrain_y = train_set['item_cnt_month']\ntest_x = test.drop(['item_cnt_month'], axis=1)\nmodel_lgb = LGBMRegressor(colsample_bytree=0.9, learning_rate=0.03, max_depth=6,\n              min_child_weight=1, min_split_gain=0.0222415, n_estimators=100,\n              num_leaves=32, reg_alpha=0.04, reg_lambda=0.073,\n              subsample=0.9)\nmodel_lgb.fit(train_x, train_y)\n\n","7e81fbfe":"sales_prediction = model_lgb.predict(test_x)\nsales_prediction","a43291e3":"sales_predict_submission = pd.DataFrame({'ID':test_x.index,'item_cnt_month':sales_prediction})","c11c5739":"sales_predict_submission","8836ad24":" sales_predict_submission.to_csv('.\/submission.csv', index= False, header= 1)","7232c9cd":"##### This shows we have successfully replaced all the -1's with 0's.","7211082d":"### Prediction","b285dd91":"**sales_train.csv    :** The training set. Daily historical data from January 2013 to October 2015.\\\n**test.csv           :** The test set. Forecast the sales for these shops and products for November 2015.\\\n**items.csv          :** Supplemental information about the items\/products.\\\n**item_categories.csv:** Supplemental information about the items categories.\\\n**shops.csv          :** Supplemental information about the shops.","969e80e6":"##### Notice now we have some item_ids not present in the train data set which would make it difficult to predict the sales because we don't have the information for these item_ids in train data set.","d52ede1c":"### To find the item category which recorded highest sale in month from Jan 2013-Oct 2015","1fcf6729":"### To find the month from Jan 2013- Oct 2013 which had the highest sales","052dcf75":"#### Outliers in item_price","1c96d8b9":"### To drop the insignificant column ID because the same information is captured in shopi_id and item_id","a670f03b":"# FUTURE SALES PREDICTION PROJECT","9e0286f1":"### To find the month which contirbutes highest towards the total sales","52968aae":"#### Create a reduced version of test and train to compare with full data set features","f266f69d":"### To merge the data sets; shops, items, item_cat, and trainset1 to get a better understanding of features ","bbccab50":"##### The data set does not contain any missing values.","9b203fff":"### File despcription from Kaggle\n","36c2a055":"##### There is only one record with negative price. Lets find the price of the item sold in the same shop during the same month, so we can imupte the value.\n","3ddd35ae":"##### The number of rows of train and trainset1 are same 2935849, which shows that merge was successful.","ca70c9b8":"##### Lets replace the price of the item with the average of two same items sold in the same month in the same shop.","e11d3455":"##### This shows that each shop_name has an assigned shop_id","d1fa7596":"##### The highest selling item category is 12, lets further explore the item_category_id 12","94baa61e":"### To explore the dataset (EDA)","84f1477a":"### Converting csv to Data frame ","5f6ed710":"##### There are 28701 records with a price of higher than $6080, and it is not suggested to drop all these values, especially considering that the fact that the data pertains to many different products. It is highly possible that the shops have a wide range of products with different prices so we will only eliminate the extreme outliers for item_price.","42756cd9":"## The dataset contains sales information from Jan 2013 to Oct 2015. The task is to forecast the total amount of products sold in every shop for the test set. A warning of change in item number and shop id shoudl be taken into consideration.","f097f41f":" ### LGBMRegressoor Model ","91b228d8":"**ID** an Id that represents a (Shop, Item) tuple within the test set.\\\n**shop_id** unique identifier of a shop.\\\n**item_id** unique identifier of a product.\\\n**item_category_id** unique identifier of item category.\\\n**item_cnt_day** number of products sold. You are predicting a monthly amount of this measure.\\\n**item_price** current price of an item.\\\n**date** date in format dd\/mm\/yyyy.\\\n**date_block_num** a consecutive month number, used for convenience. January 2013 is 0, February 2013 is 1,..., October 2015 is 33.\\\n**item_name** name of item.\\\n**shop_name** name of shop.\\\n**item_category_name** name of item category.","9729bbc1":"### Data fields from Kaggle","e3cea896":"### To explore each data set","1db9bda8":"### To check the dimensions in categorical features","0161b222":"### To understand the basic structre of the tables","ca5b705d":"##### The item selling highest in a given month is \u0418\u0433\u0440\u043e\u0432\u044b\u0435 \u043a\u043e\u043d\u0441\u043e\u043b\u0438 - PS4","8d94e07f":"### To find the store which recorded highest sales in amonth from Jan 2013-Oct2015","059592d0":"##### There are 11936 records on which the items sold is greater than 9. To reduce the impact of outliers and still be able to capture the impact of these sales we will only eliminate the extreme outliers for item_cnt_day.","5ff7cec2":"##### The highest selling item category is \u0418\u0433\u0440\u043e\u0432\u044b\u0435 \u043a\u043e\u043d\u0441\u043e\u043b\u0438 - PS4","2241dba2":"##### There are 7356 rows which have the value of -1 for item_cnt_day, since our task is to predict the sales for the next month our focus should be on sales and not on the return. We will keep these rows in the data set because it can play a siginifacant factor to correctly predict and understand sales but we will replace the -1 with 0.","6dc20dff":"##### There are several in item_price outliers and these values should be explored further.","7f5274a7":"### To check for outliers","b64d2abb":"### To find the year with highest sales","e06f90e1":"### Imputing the negative value for item_cnt_day. ","b05c7b7f":"### To understand the basic structurce of the new train data set","32bcaa8a":"##### There are several outliers in item_cnt_day and should be explored further","c3f0d642":"### To find the highest selling product","6a81d68e":"##### The highest sale was in the 23 date block. Lets explore the date block 23.","5ae4cf31":"### To check the number of records and features in trainset\n","dc249b0a":"### To explore the basic structure of test data set. ","95e80435":"### Submission","7bbec0e3":"### To find the highest shop with highest total sales","bba5ea3e":"### To verify again","15529e99":"#### Outlier in item_cnt_day","765ee78b":"##### The store with the highest sales in a month from Jan 2013-Oct 2015 is \u041c\u043e\u0441\u043a\u0432\u0430 \u0422\u0420\u041a \"\u0410\u0442\u0440\u0438\u0443\u043c\"","ad99201b":"### To merge the test data with other datasets.","4e17c42a":"### Importing the Dataset and libraries","2ee93ace":"##### This shows that each item_name has an assigned item_id","9fdb3a02":"### To remove additional products and shops from the train data set.","6b3b7912":"##### The highest selling shop has the id of 9","f275eacc":"##### This shows 2013 has the highest total sales.","e605e2b1":"### To align train data set with test. It is mentioned that we need to predict the sales of products and shops present in the test data set, therefore we need to check the dimenions of shop_id and item_id between train and test data set.","4a2b2966":"### To aggregate train data set to show monthly data to bring it in line with test data","cd707cf8":"##### The highest selling shop is \u0412\u044b\u0435\u0437\u0434\u043d\u0430\u044f \u0422\u043e\u0440\u0433\u043e\u0432\u043b\u044f\t","43308194":"##### In the statistical summary, notice that an item has a negative price which represents a mistake, also notice that item_cnt_day has a negative value which shows that there wasn't any sale that day. It shows -1 porbably because the products were returned. These issues will be fixed later during exploration.","c543c5a5":"### To check for missing values","3ef78967":"### Imputing the negative item price.","f20ac5e3":"### To copy the train data set","b1e6394c":"### Creating new features and extracting date","a3c095e3":"##### This shows that the highest sales were in the month of December 2014"}}