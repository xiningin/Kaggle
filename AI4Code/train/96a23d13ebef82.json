{"cell_type":{"61972472":"code","ca5505cc":"code","51f23063":"code","ae372440":"code","29760d6e":"code","d724fbef":"code","d343fdbc":"code","c9871af3":"code","a34e38e2":"code","33af7ead":"code","4a417515":"code","21687471":"code","a207ae0a":"code","0f6ef4ab":"code","35fd9033":"code","2bf1a623":"code","fc69a87b":"code","ecca39ca":"code","4a4f4006":"code","6f9bd6c9":"code","4ca85745":"code","f26ed6b0":"code","f951f7b3":"code","5470bbdd":"code","c65170cb":"code","334fc24e":"code","6216d103":"code","595e86c7":"code","0c5df778":"code","30109632":"code","dd11c546":"code","251d89c2":"code","54f047f3":"code","2c6de488":"code","ae8de1a8":"code","5543eaa2":"code","cb6c1c6c":"code","64e33825":"code","663d546a":"code","647057d7":"markdown","f51cddbb":"markdown","ff500c45":"markdown","15c6e3b6":"markdown","7ef6f50b":"markdown","18c2313d":"markdown","ebef9c93":"markdown","19473a4c":"markdown","caf2cbcd":"markdown","e5d4a7c1":"markdown"},"source":{"61972472":"from tensorflow.keras.layers import Dense, Input, Dropout, BatchNormalization, Activation, concatenate\nfrom tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler, Callback\nfrom sklearn.preprocessing import OrdinalEncoder, LabelEncoder, OneHotEncoder\nfrom tensorflow.keras.utils import get_custom_objects\nfrom tensorflow.keras.optimizers import Adam, Nadam\nfrom scipy.stats import rankdata, spearmanr\nfrom tensorflow.keras.models import Model\nfrom sklearn.metrics import roc_auc_score\nimport tensorflow.keras.backend as K\nfrom tqdm.notebook import tqdm\nimport random, os, gc, time\nimport tensorflow as tf\nimport tensorflow.keras\nimport numpy as np\nimport pandas as pd\nimport warnings\n\nwarnings.filterwarnings('ignore')\nwarnings.simplefilter('ignore')\n\n\nSEED = 2020\nbatch_size_train = 2048\nbatch_size_pred  = 8000\nverbose = 0\nverboseB = False\nlow  = 10\nhigh = 100","ca5505cc":"def set_seed(seed=SEED):\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\nset_seed()","51f23063":"P = '..\/input\/cat-in-the-dat-ii\/'\ntrain = pd.read_csv(P+'train.csv')\ntest = pd.read_csv(P+'test.csv')\nsub = pd.read_csv(P+'sample_submission.csv')","ae372440":"useful_features = list(train.iloc[:, 1:24].columns)\n\ny = train.sort_values('id')['target']\nX = train.sort_values('id')[useful_features]\nX_test = test[useful_features]\ndel train, test","29760d6e":"categorical_features = [\n'bin_0', 'bin_1', 'bin_2', 'bin_3', 'bin_4', 'nom_0', 'nom_1', 'nom_2',\n       'nom_3', 'nom_4', 'nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9', 'day', 'month'\n]\n\ncontinuous_features = list(filter(lambda x: x not in categorical_features, X))","d724fbef":"def OrdMapping(df, ordinal):\n    ord_maps = {\n        'ord_0': {val: i for i, val in enumerate([1, 2, 3])},\n        'ord_1': {\n            val: i\n            for i, val in enumerate(\n                ['Novice', 'Contributor', 'Expert', 'Master', 'Grandmaster']\n            )\n        },\n        'ord_2': {\n            val: i\n            for i, val in enumerate(\n                ['Freezing', 'Cold', 'Warm', 'Hot', 'Boiling Hot', 'Lava Hot']\n            )\n        },\n        'ord_3': {val: i for i, val in enumerate(sorted(df['ord_3'].dropna().unique()))},\n        'ord_4': {val: i for i, val in enumerate(sorted(df['ord_4'].dropna().unique()))},\n        'ord_5': {val: i for i, val in enumerate(sorted(df['ord_5'].dropna().unique()))},\n    }\n    ord_cols = pd.concat([df[col].map(ord_map).fillna(max(ord_map.values())\/\/2).astype('float32') for col, ord_map in ord_maps.items()], axis=1)\n    ord_cols \/= ord_cols.max()\n    df[ordinal] = ord_cols\n    return df","d343fdbc":"X['isna_sum'] = X.isna().sum(axis=1)\nX_test['isna_sum'] = X_test.isna().sum(axis=1)\n\nX['isna_sum'] = (X['isna_sum'] - X['isna_sum'].mean())\/X['isna_sum'].std()\nX_test['isna_sum'] = (X_test['isna_sum'] - X_test['isna_sum'].mean())\/X_test['isna_sum'].std()","c9871af3":"X = OrdMapping(X ,continuous_features)\nX_test= OrdMapping(X_test, continuous_features)","a34e38e2":"class ContinuousFeatureConverter:\n    def __init__(self, name, feature, log_transform):\n        self.name = name\n        self.skew = feature.skew()\n        self.log_transform = log_transform\n        \n    def transform(self, feature):\n        if self.skew > 1:\n            feature = self.log_transform(feature)\n        \n        mean = feature.mean()\n        std = feature.std()\n        return (feature - mean)\/(std + 1e-6)        ","33af7ead":"continuous_features +=['isna_sum']\nfeature_converters = {}\ncontinuous_features_processed = []\ncontinuous_features_processed_test = []\n\nfor f in tqdm(continuous_features):\n    feature = X[f]\n    feature_test = X_test[f]\n    log = lambda x: np.log10(x + 1 - min(0, x.min()))\n    converter = ContinuousFeatureConverter(f, feature, log)\n    feature_converters[f] = converter\n    continuous_features_processed.append(converter.transform(feature))\n    continuous_features_processed_test.append(converter.transform(feature_test))\n    \ncontinuous_train = pd.DataFrame({s.name: s for s in continuous_features_processed}).astype(np.float32)\ncontinuous_test = pd.DataFrame({s.name: s for s in continuous_features_processed_test}).astype(np.float32)","4a417515":"isna_columns = []\nfor column in tqdm(continuous_features):\n    isna = continuous_train[column].isna()\n    if isna.mean() > 0.:\n        continuous_train[column + '_isna'] = isna.astype(int)\n        continuous_test[column + '_isna'] = continuous_test[column].isna().astype(int)\n        isna_columns.append(column)\n        \ncontinuous_train = continuous_train.fillna(-1.)\ncontinuous_test = continuous_test.fillna(-1.)","21687471":"def categorical_encode(df_train, df_test, categorical_features, n_values=140):\n    df_train = df_train[categorical_features].astype(str)\n    df_test = df_test[categorical_features].astype(str)\n    \n    categories = []\n    for column in tqdm(categorical_features):\n        categories.append(list(df_train[column].value_counts().iloc[: n_values - 1].index) + ['Other'])\n        values2use = categories[-1]\n        df_train[column] = df_train[column].apply(lambda x: x if x in values2use else 'Other')\n        df_test[column] = df_test[column].apply(lambda x: x if x in values2use else 'Other')\n        \n    \n    ohe = OneHotEncoder(categories=categories)\n    ohe.fit(pd.concat([df_train, df_test]))\n    df_train = pd.DataFrame(ohe.transform(df_train).toarray()).astype(np.float16)\n    df_test = pd.DataFrame(ohe.transform(df_test).toarray()).astype(np.float16)\n    return df_train, df_test","a207ae0a":"train_categorical, test_categorical = categorical_encode(X, X_test, categorical_features)","0f6ef4ab":"num_shape = continuous_train.shape[1]\ncat_shape = train_categorical.shape[1]","35fd9033":"X = pd.concat([continuous_train, train_categorical], axis=1)\ndel [[continuous_train, train_categorical]]\ncontinuous_train = train_categorical = None\nX_test = pd.concat([continuous_test, test_categorical], axis=1)\ndel [[continuous_test, test_categorical]]\ncontinuous_test = test_categorical = None","2bf1a623":"test_rows = X_test.shape[0]\nX = pd.concat([X, X_test], axis = 0)\ndel [[X_test]]\nX_test = None\ngc.collect()","fc69a87b":"class roc_callback(Callback):\n    def __init__(self,training_data,validation_data):\n        self.x = training_data[0]\n        self.y = training_data[1]\n        self.x_val = validation_data[0]\n        self.y_val = validation_data[1]\n\n\n    def on_train_begin(self, logs={}):\n        return\n\n    def on_train_end(self, logs={}):\n        return\n\n    def on_epoch_begin(self, epoch, logs={}):\n        return\n\n    def on_epoch_end(self, epoch, logs={}):\n        y_pred_val = self.model.predict(self.x_val)\n        roc_val = roc_auc_score(self.y_val, y_pred_val)\n        print('\\rroc-auc_val: %s' % (str(round(roc_val,4))),end=100*' '+'\\n')\n        return\n\n    def on_batch_begin(self, batch, logs={}):\n        return\n\n    def on_batch_end(self, batch, logs={}):\n        return\n    \ndef focal_loss(gamma=2., alpha=.25):\n    def focal_loss_fixed(y_true, y_pred):\n        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n        return -K.mean(alpha * K.pow(1. - pt_1, gamma) * K.log(K.epsilon()+pt_1))-K.mean((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0 + K.epsilon()))\n    return focal_loss_fixed\n\ndef custom_gelu(x):\n    return 0.5 * x * (1 + tf.tanh(tf.sqrt(2 \/ np.pi) * (x + 0.044715 * tf.pow(x, 3))))\n\nget_custom_objects().update({'custom_gelu': Activation(custom_gelu)})\nget_custom_objects().update({'focal_loss_fn': focal_loss()})","ecca39ca":"K.clear_session()\nfrom tensorflow.keras.optimizers import Adam\ndef create_model():\n    num_inp = Input(shape=(num_shape,))\n    cat_inp = Input(shape=(cat_shape,))\n    inps = concatenate([num_inp, cat_inp])\n    x = Dense(512, activation=custom_gelu)(inps)\n    x = Dense(256, activation=custom_gelu)(x)\n    x = Dense(512, activation = custom_gelu)(x)\n    x = Dropout(.2)(x)\n    cat_out = Dense(cat_shape, activation = 'linear')(x)\n    num_out = Dense(num_shape, activation = 'linear')(x)\n    model = Model(inputs=[num_inp, cat_inp], outputs=[num_out, cat_out])\n    model.compile(\n        optimizer=Adam(.05, clipnorm = 1, clipvalue = 1),\n        loss=['mse', 'mse']\n    )\n    return model","4a4f4006":"gc.collect()\nmodel_mse = create_model()\nmodel_mse.summary()","6f9bd6c9":"def inputSwapNoise(arr, p):\n    n, m = arr.shape\n    idx = range(n)\n    swap_n = round(n*p)\n    for i in range(m):\n        col_vals = np.random.permutation(arr[:, i])\n        swap_idx = np.random.choice(idx, size= swap_n)\n        arr[swap_idx, i] = np.random.choice(col_vals, size = swap_n) \n    return arr","4ca85745":"def auto_generator(X, swap_rate, batch_size):\n    indexes = np.arange(X.shape[0])\n    while True:\n        np.random.shuffle(indexes)\n        num_X = X[indexes[:batch_size], :num_shape] \n        num_y = inputSwapNoise(num_X, swap_rate)\n        cat_X = X[indexes[:batch_size], num_shape:] \n        cat_y = inputSwapNoise(cat_X, swap_rate)\n        yield [num_y, cat_y], [num_X, cat_X]","f26ed6b0":"class WarmUpLearningRateScheduler(tf.keras.callbacks.Callback):\n    \"\"\"Warmup learning rate scheduler\n    \"\"\"\n\n    def __init__(self, warmup_batches, init_lr, verbose=0):\n        \"\"\"Constructor for warmup learning rate scheduler\n\n        Arguments:\n            warmup_batches {int} -- Number of batch for warmup.\n            init_lr {float} -- Learning rate after warmup.\n\n        Keyword Arguments:\n            verbose {int} -- 0: quiet, 1: update messages. (default: {0})\n        \"\"\"\n\n        super(WarmUpLearningRateScheduler, self).__init__()\n        self.warmup_batches = warmup_batches\n        self.init_lr = init_lr\n        self.verbose = verbose\n        self.batch_count = 0\n        self.learning_rates = []\n\n    def on_batch_end(self, batch, logs=None):\n        self.batch_count = self.batch_count + 1\n        lr = K.get_value(self.model.optimizer.lr)\n        self.learning_rates.append(lr)\n\n    def on_batch_begin(self, batch, logs=None):\n        if self.batch_count <= self.warmup_batches:\n            lr = self.batch_count*self.init_lr\/self.warmup_batches\n            K.set_value(self.model.optimizer.lr, lr)\n            if self.verbose > 0:\n                print('\\nBatch %05d: WarmUpLearningRateScheduler setting learning '\n                      'rate to %s.' % (self.batch_count + 1, lr))\n","f951f7b3":"auto_ckpt = ModelCheckpoint('ae.model', monitor='loss', verbose=verbose, save_best_only=True, save_weights_only=True, mode='min', period=1)\nwarm_up_lr = WarmUpLearningRateScheduler(400, init_lr=0.001)\ntrain_gen = auto_generator(X.values, .15, batch_size_train)\n\nhist = model_mse.fit_generator(train_gen,\n                               steps_per_epoch=len(X)\/\/batch_size_train,\n                               epochs=low,\n                               verbose=verbose,\n                               workers=-1, \n                               use_multiprocessing=True,\n                               callbacks=[auto_ckpt, warm_up_lr])","5470bbdd":"del train_gen\ngc.collect()\nmodel_mse.load_weights('ae.model')","c65170cb":"for layer in model_mse.layers:\n    layer.trainable = False\nmodel_mse.compile(\n    optimizer='adam',\n    loss=['mse', 'mse']\n)\n\nmodel_mse.summary()","334fc24e":"def make_model(loss_fn):\n    x1 = model_mse.layers[3].output\n    x2 = model_mse.layers[4].output\n    x3 = model_mse.layers[5].output\n    x_conc = concatenate([x1,x2,x3])\n    x = Dropout(.5)(x_conc)\n    x = Dense(500, activation='relu')(x)\n    x = Dropout(.5)(x)\n    x = Dense(200, activation='relu')(x)\n    x = Dropout(.5)(x)\n    x = Dense(100, activation='relu')(x)\n    x = Dropout(.5)(x)\n    x = Dense(1, activation = 'sigmoid')(x)\n    model = Model([model_mse.layers[0].input, model_mse.layers[1].input], x)\n    model.compile(\n        optimizer='adam',\n        loss=[loss_fn]\n    )\n    return model","6216d103":"gc.collect()\nbce_model = make_model('binary_crossentropy')\nfcloss_model = make_model('focal_loss_fn')","595e86c7":"X_test = X.iloc[-test_rows:, :]\nX      = X.iloc[:-test_rows, :]\ngc.collect()","0c5df778":"split_ind = int(X.shape[0]*0.8)\n\nX_tr  = X.iloc[:split_ind]\nX_val = X.iloc[split_ind:]\n\ny_tr  = y.iloc[:split_ind]\ny_val = y.iloc[split_ind:]","30109632":"ckpt = ModelCheckpoint('best_bce.model', monitor='val_loss', verbose=verbose, save_best_only=True, save_weights_only=True, mode='min', period=1)\nbce_model.fit([X_tr.iloc[:, :num_shape], X_tr.iloc[:, num_shape:]], y_tr,\n              epochs=high,\n              batch_size=batch_size_train,\n              validation_data = ([X_val.iloc[:, :num_shape], X_val.iloc[:, num_shape:]], y_val),\n              verbose=verbose,\n              workers=-1, \n              use_multiprocessing=True,\n              callbacks=[ckpt])","dd11c546":"valid_preds = bce_model.predict([X_val.iloc[:, :num_shape], X_val.iloc[:, num_shape:]], batch_size = batch_size_pred, verbose = verboseB)\nprint(f'ROC-AUC score {roc_auc_score(y_val, valid_preds)}')","251d89c2":"bce_model.load_weights('best_bce.model')\nvalid_preds = bce_model.predict([X_val.iloc[:, :num_shape], X_val.iloc[:, num_shape:]], batch_size = batch_size_pred, verbose = verboseB)\nprint(f'ROC-AUC score {roc_auc_score(y_val, valid_preds)}')","54f047f3":"ckpt2 = ModelCheckpoint('best_fcloss.model', monitor='val_loss', verbose=verbose, save_best_only=True, save_weights_only=True, mode='min', period=1)\nfcloss_model.fit([X_tr.iloc[:, :num_shape], X_tr.iloc[:, num_shape:]], y_tr,\n                 epochs=high,\n                 batch_size=batch_size_train,\n                 validation_data = ([X_val.iloc[:, :num_shape], X_val.iloc[:, num_shape:]], y_val),\n                 verbose = verbose,\n                 workers=-1, \n                 use_multiprocessing=True,\n                 callbacks=[ckpt2])","2c6de488":"gc.collect()\nbce_model.load_weights('best_bce.model')\nfcloss_model.load_weights('best_fcloss.model')","ae8de1a8":"valid_preds = bce_model.predict([X_val.iloc[:, :num_shape], X_val.iloc[:, num_shape:]], batch_size = batch_size_pred, verbose = verboseB)\nprint(f'ROC-AUC score {roc_auc_score(y_val, valid_preds)}')","5543eaa2":"valid_preds  = bce_model.predict([X_val.iloc[:, :num_shape], X_val.iloc[:, num_shape:]], batch_size = batch_size_pred, verbose = verboseB)\nvalid_preds2 = fcloss_model.predict([X_val.iloc[:, :num_shape], X_val.iloc[:, num_shape:]], batch_size = batch_size_pred, verbose = verboseB)\ngc.collect()\nscore  = roc_auc_score(y_val, valid_preds)\nscore2 = roc_auc_score(y_val, valid_preds2)\nscore_avg = roc_auc_score(y_val, (.5*valid_preds) + (.5*valid_preds2))\nscore_rank_avg = roc_auc_score(y_val, rankdata(valid_preds, method='dense') + rankdata(valid_preds2, method='dense'))\nprint(f'ROC-AUC score of BCE model {score}')\nprint(f'ROC-AUC score of Focal Loss model {score2}')\nprint(f'ROC-AUC score of Average of models {score_avg}')\nprint(f'ROC-AUC score of Rank Average of models {score_rank_avg}')","cb6c1c6c":"bce_model.fit([X.iloc[:, :num_shape], X.iloc[:, num_shape:]], y,\n              epochs=low,\n              batch_size=batch_size_train,\n              verbose = verbose,\n              workers=-1, \n              use_multiprocessing=True)\n\nfcloss_model.fit([X.iloc[:, :num_shape], X.iloc[:, num_shape:]], y,\n                 epochs=low,\n                 batch_size=batch_size_train,\n                 verbose = verbose,\n                 workers=-1, \n                 use_multiprocessing=True)","64e33825":"test_preds  = bce_model.predict([X_test.iloc[:, :num_shape], X_test.iloc[:, num_shape:]], batch_size = batch_size_pred, verbose=verboseB)\ntest_preds2 = fcloss_model.predict([X_test.iloc[:, :num_shape], X_test.iloc[:, num_shape:]], batch_size = batch_size_pred, verbose=verboseB)","663d546a":"sub['target'] = rankdata(test_preds, method='dense') + rankdata(test_preds2, method='dense')\nsub.target = sub.target\/sub.target.max()\nsub.to_csv('submission.csv', index=False)","647057d7":"Now we will train the autoencoder using our generator for several epochs","f51cddbb":"Now we will freeze the layers of the autoencoder","ff500c45":"What we will do here is construct a simple autoencoder that will take in our noised numeric and categorical features, concatenate them and then pass them through several dense layers that will then try to predict our original unnoised numeric and categorical features. What this will do is in essence try to learn the relationships between the features and which features should co-occur. ","15c6e3b6":"We will create a small generator so that we can continuously do this swapping and create new samples for the model to see","7ef6f50b":"Now we need to invent some realistic noise. As Michael Jahrer noted [in his post](https:\/\/www.kaggle.com\/c\/porto-seguro-safe-driver-prediction\/discussion\/44629) he used something he called swap noise. What this is doing is swapping a columns values with other possible values from that column a certain percentage of the time. For example say there is a feature like ord_3. If we used swap noise on that column it would swap 15% of the rows of the ord_3 column with other possible values (like swapping 'a' for 'g', etc.). The model would then see g was swapped in and all of the other features around it and it would try to learn that 'a' was the real original value and try to correct the various errors we have introduced into the input.  ","18c2313d":"**Autoencoder** is an unsupervised neural network that learns how to efficiently compress and encode data and how to reconstruct the data back from the reduced encoded representation to a representation that is as close to the original input as possible.\nAutoencoder, by design, reduces data dimensions by learning how to ignore the noise in the data. It's Components are: \n\n* *Encoder*: In which the model learns how to reduce the input dimensions and compress the input data into an encoded representation.\n* *Bottleneck*: which is the layer that contains the compressed representation of the input data. This is the lowest possible dimensions of the input data.\n* *Decoder*: In which the model learns how to reconstruct the data from the encoded representation to be as close to the original input as possible.\n* *Reconstruction Loss*: This is the method that measures how well the decoder is performing and how close the output is to the original input.\n\n[The Autoencoder reference](https:\/\/www.kaggle.com\/ryches\/keras-nn-autoencoder)  and [Ordinal Mapping reference](https:\/\/www.kaggle.com\/superant\/oh-my-plain-logreg)","ebef9c93":"We will train one with binary crossentropy and another with focal loss just like the previous kernel","19473a4c":"In last part we train the models on all data. Ryches has concatenated validation 4 times to training data but here because of memory issues we just use the original *X* instead of  *X_tr = pd.concat([X_tr, X_val, X_val, X_val, X_val], axis = 0)*\n","caf2cbcd":"Next we will make a new model that branches off the previous one. This will take in non-noisy inputs and pass them through the encoding part of the autoencoder and then concatenated all of the middle layers of the encoder and then we will train our classifier based on the features that concatenated encoder outputs. ","e5d4a7c1":"In the autoencoder we were able to take advantage of being able to train on both the train and test set because the autoencoder was trying to guess inputs rather than our target. No we will split our test set back out because we will be training on those targets for the second phase and we dont have that information for the test set"}}