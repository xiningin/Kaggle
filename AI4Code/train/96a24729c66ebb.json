{"cell_type":{"3bf4df33":"code","d48980ec":"code","9db90291":"code","0efd942e":"code","4f2cee8d":"code","8b62b361":"code","40c7f76c":"code","383e1567":"code","0bb265f1":"code","509a33cf":"code","9cb89e9b":"code","0c6d30bc":"code","fa386bcd":"code","857d9ee4":"code","b520936e":"code","52af7f5d":"code","8975985a":"code","20c8d0c9":"code","af5365a1":"code","3c356483":"code","7fe9cb06":"code","e17ef7ba":"code","65f832d7":"code","582fdbd9":"code","d4572814":"code","aa007b9c":"code","8ca657d5":"code","87653a51":"code","05b00f8b":"code","ad58b7fa":"code","1e945afb":"code","418a9dfe":"code","6dc2c4c6":"code","576c9be7":"code","0548c5e9":"code","2757d881":"code","7a469064":"code","28b005cb":"code","441a4c93":"code","f0f2be7b":"code","0f22aa46":"code","0cedf304":"code","068461b2":"code","58ecf2fb":"code","ad9cf4b0":"code","ad565a31":"code","37e01a9e":"code","fa4eb151":"code","e2e7ddc3":"code","e331a3d9":"code","3c1bac07":"code","321d7468":"code","491037cc":"code","dad5122b":"code","79efd0f9":"code","00334158":"code","be63c4e2":"code","788cc957":"code","28e3c769":"code","566cc929":"code","0668409a":"code","d45f97fe":"code","dce5f618":"code","c7bd231f":"code","0339a1ea":"code","0469f867":"code","2ddb7627":"code","66c3fdcc":"code","293af4ae":"code","5196cb12":"code","391dd2f6":"code","e8226da7":"code","035b4b25":"code","d1d68da3":"code","721e62c5":"code","38626106":"code","70002d7f":"code","cd1feb9a":"code","df0d88fb":"code","a67dd2ad":"code","8548539e":"code","4c2c8ce2":"code","d8440a94":"code","dc8b5037":"code","f59a016a":"code","6de19626":"code","c2698d5f":"code","57eb49b1":"code","7a4c2dae":"code","f0bb91e2":"code","0a7da891":"code","1b9d03e7":"code","b8e7459f":"code","29335add":"code","2fe70a63":"code","3a72cc77":"code","e20d1774":"code","c4a0a487":"code","e589568e":"code","38f41d38":"code","d1a2cb81":"code","52ead2be":"code","9243c008":"code","9fa67b5f":"code","88fdd684":"code","c8041556":"code","e80b3db1":"code","74106f79":"code","dbafb89f":"code","c3fe609c":"code","3f45c454":"code","251db543":"code","620a040e":"code","be19d475":"code","b8b51d8f":"code","193c4ef7":"markdown","e447b62f":"markdown","90bce076":"markdown","ebce32d6":"markdown","4da32c40":"markdown","8e5970ee":"markdown","1a40f7a3":"markdown","80b915fb":"markdown","ad634749":"markdown","af79ada2":"markdown","6ee5ea75":"markdown","252cebc0":"markdown","b31f7ad0":"markdown","38013652":"markdown","67bc9d05":"markdown","85d18316":"markdown","3ebd9cce":"markdown","2296d797":"markdown","e84284f6":"markdown","abcd6a5d":"markdown","cc9fcc78":"markdown","a8d9c705":"markdown","6f80ee5f":"markdown","82b85cea":"markdown","f1256c1c":"markdown"},"source":{"3bf4df33":"#loading necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","d48980ec":"#loading train and test from house-prices dataset\ntrain_set = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest_set = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntest_label = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')","9db90291":"test_set.shape","0efd942e":"test_set.shape","4f2cee8d":"#adding test label\ntest_set[\"SalePrice\"] =test_label['SalePrice']","8b62b361":"#combining the train and test set for cleaning\n#df_final= pd.concat([train_set,test_set])\ndf_final = pd.concat([test_set.assign(ind=\"test_set\"), train_set.assign(ind=\"train_set\")])","40c7f76c":"# correlation between 'SalePrice' and 'GrLivArea' \n# we can spot outliners that we delete later on\nsns.jointplot(data=train_set, x='SalePrice', y='GrLivArea')","383e1567":"#OverallQual is the most corr feature we can see the Linear shape and no outliner\nfigure=figsize = (30,34)\nsns.stripplot(data=train_set, x = 'OverallQual', y='SalePrice')","0bb265f1":"#finding features with the most duplicant value ?","509a33cf":"\ndef missing_percent(train_set):\n    nan_percent = 100*(train_set.isnull().sum()\/len(train_set))\n    nan_percent = nan_percent[nan_percent>0].sort_values(ascending=False).round(1)\n    DataFrame = pd.DataFrame(nan_percent)\n    # Rename the columns\n    mis_percent_table = DataFrame.rename(columns = {0 : '% of Misiing Values'}) \n    # Sort the table by percentage of missing descending\n    mis_percent = mis_percent_table\n    return mis_percent\n","9cb89e9b":"miss = missing_percent(train_set)\nmiss","0c6d30bc":"#Removing the Id that has no value for our prediction\ntrain_set= train_set.drop('Id', axis=1)","fa386bcd":"nan_percent = 100*(train_set.isnull().sum()\/len(train_set))\nnan_percent = nan_percent[nan_percent>0].sort_values()","857d9ee4":"# Every Feature with missing data must be checked!\n# We choose a threshold of 1%. It means, if there is less than 1% of a feature are missing\n\nplt.figure(figsize=(12,6))\nsns.barplot(x=nan_percent.index, y=nan_percent)\nplt.xticks(rotation=90)\n\n#Set 1% threshold:\nplt.ylim(0,1)","b520936e":"train_set.shape","52af7f5d":"#Removing the Id that has no value for our prediction\ntrain_set= train_set.drop('Id', axis=1)","8975985a":"#drop features that have more than 70% missing value\n#credit: https:\/\/www.kaggle.com\/rushikeshdarge\/handle-missing-values-only-notebook-you-need\nthreshold = 70\ndrop_cols = miss[miss['% of Misiing Values'] > threshold].index.tolist()\ndrop_cols\n","20c8d0c9":"train_set= train_set.drop(drop_cols, axis=1)","af5365a1":"nan_percent = 100*(train_set.isnull().sum()\/len(train_set))\nnan_percent = nan_percent[nan_percent>0].sort_values()","3c356483":"#every Feature with missing data must be checked!\n#We choose a threshold of 1%. It means, if there is less than 1% of a feature are missing\n\nplt.figure(figsize=(12,6))\nsns.barplot(x=nan_percent.index, y=nan_percent)\nplt.xticks(rotation=90)\n\n#Set 1% threshold:\nplt.ylim(0,1)","7fe9cb06":"train_set['FireplaceQu']= train_set['FireplaceQu'].fillna('None')","e17ef7ba":"#Filling null values most freq value\n#train_set['KitchenQual']= train_set['KitchenQual'].fillna('TA')","65f832d7":"#df_final['SaleType']= df_final['SaleType'].fillna('Oth')","582fdbd9":"#df_final['Functional']= df_final['Functional'].fillna('Typ')","d4572814":"#df_final['Exterior1st']= df_final['Exterior1st'].fillna('Other')\n#df_final.fillna({'Exterior1st':'Other', 'Exterior2nd':'Other', 'Utilities':'Other'}, inplace=True)","aa007b9c":"#After checking the data documentation,\n#it shows that missing value (two rows) in Basement Features are becouse of there is no basement in these rows\n#Decision: Filling in data based on column: numerical basement & string descriptive:\n\n#Numerical Columns fill with 0:\nbsmt_num_cols= ['BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF' ,'BsmtFullBath', 'BsmtHalfBath']\ntrain_set[bsmt_num_cols]=train_set[bsmt_num_cols].fillna(0)\n\n#String Columns fill with None:\nbsmt_str_cols= ['BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2']\ntrain_set[bsmt_str_cols]= train_set[bsmt_str_cols].fillna('None')","8ca657d5":"train_set[\"MasVnrType\"]= train_set[\"MasVnrType\"].fillna(\"None\")\ntrain_set[\"MasVnrArea\"]= train_set[\"MasVnrArea\"].fillna(0)","87653a51":"train_set[['GarageType', 'GarageYrBlt', 'GarageFinish', 'GarageQual', 'GarageCond']]","05b00f8b":"#now we will extract all the numerical features from the dataset\n#numerical_features= [feature for feature in train_set.columns if train_set[feature].dtypes !='O']\n\n#print('Number of Numerical Features:',len(numerical_features))\n\n#train_set[numerical_features].head(5)","ad58b7fa":"#now we will extract datatime features from the dataset\n#year_feature=[feature for feature in numerical_features if 'Year' in feature or 'Yr' in feature]\n\n#print('Number of Yearly Features:',len(year_feature))\n#train_set[year_feature].head(5)","1e945afb":"#now we will analyze yearly features wrt SalePrice which is our independent feature\n#for feature in year_feature:\n    \n    \n #   train_set.groupby(feature)['SalePrice'].median().plot()\n  #  plt.show()","418a9dfe":"#Filling the missing Value:\nGar_str_cols= ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']\ntrain_set[Gar_str_cols]=train_set[Gar_str_cols].fillna('None')\n\ntrain_set['GarageYrBlt']=train_set['GarageYrBlt'].fillna(0)","6dc2c4c6":"#Impute missing data based on other columns:\n\ntrain_set.groupby('Neighborhood')['LotFrontage']","576c9be7":"train_set.groupby('Neighborhood')['LotFrontage'].mean()","0548c5e9":"#Filling null values mean value\ntrain_set.groupby('Neighborhood')['LotFrontage'].transform(lambda val: val.fillna(val.mean()))","2757d881":"train_set['LotFrontage']=train_set.groupby('Neighborhood')['LotFrontage'].transform(lambda val: val.fillna(val.mean()))","7a469064":"#Filling null values most freq value\n#train_set['MSZoning'].value_counts()[train_set['MSZoning'].value_counts() == df_final['MSZoning'].value_counts().max()].index\n","28b005cb":"#train_set['MSZoning']=train_set['MSZoning'].fillna('RL')","441a4c93":"train_set['LotFrontage']=train_set['LotFrontage'].fillna(0)","f0f2be7b":"train_set[train_set['GarageArea'].isnull()]","0f22aa46":"#Filling null values most freq value\ntrain_set['LotFrontage']=train_set.groupby('Neighborhood')['LotFrontage'].transform(lambda val: val.fillna(val.max()))","0cedf304":"#Filling null values most freq value\n#train_set['Functional'].value_counts()[train_set['Functional'].value_counts() == train_set['Functional'].value_counts().max()].index","068461b2":"nan_percent = 100*(train_set.isnull().sum()\/len(df_final))\nnan_percent = nan_percent[nan_percent>0].sort_values()","58ecf2fb":"#plot the feature with missing indicating the percent of missing data\nplt.figure(figsize=(12,6))\nsns.barplot(x=nan_percent.index, y=nan_percent)\nplt.xticks(rotation=90)","ad9cf4b0":"train_set= train_set.dropna(axis=0, subset=['Electrical', 'GarageArea'])","ad565a31":"#Filling null values most freq value\n#df_final['MSZoning'].value_counts()[df_final['MSZoning'].value_counts() == df_final['MSZoning'].value_counts().max()].index\n","37e01a9e":"#df_final['MSZoning']= df_final['MSZoning'].transform(lambda val: val.fillna(val.max()))","fa4eb151":"df_final.isnull().sum()","e2e7ddc3":"nan_percent= missing_percent(train_set)","e331a3d9":"nan_percent = 100*(train_set.isnull().sum()\/len(df_final))\nnan_percent = nan_percent[nan_percent>0].sort_values()","3c1bac07":"nan_percent","321d7468":"#df = pd.concat([test.assign(ind=\"test\"), train.assign(ind=\"train\")])","491037cc":"train_set.shape","dad5122b":"from sklearn.neighbors import LocalOutlierFactor\n#credit https:\/\/www.kaggle.com\/hrshtporwal5\/houseprice-prediction\ndef detect_outliers(x, y, top=5, plot=True):\n    lof = LocalOutlierFactor(n_neighbors=40, contamination=0.1)\n    x_ =np.array(x).reshape(-1,1)\n    preds = lof.fit_predict(x_)\n    lof_scr = lof.negative_outlier_factor_\n    out_idx = pd.Series(lof_scr).sort_values()[:top].index\n    if plot:\n        f, ax = plt.subplots(figsize=(9, 6))\n        plt.scatter(x=x, y=y, c=np.exp(lof_scr), cmap='RdBu')\n    return out_idx","79efd0f9":"#GrLivArea-SalePrice outlier detection\nouts = detect_outliers(train_set['GrLivArea'], train_set['SalePrice'],top=5) \nouts\nplt.axhline(y=200000, color='r')\nplt.axvline(x=4000, color='r')\n#credit https:\/\/www.kaggle.com\/hrshtporwal5\/houseprice-prediction","00334158":"#train_set[(train_set['OverallQual']>8) &(train_set['SalePrice']<200000)][['SalePrice', 'OverallQual']]","be63c4e2":"#corr = train_set.corr()\n#top_corr_features = corr.index[abs(corr[\"SalePrice\"])>0.5].sort_values(ascending=True)\n","788cc957":"#Remove the outliers:\nindex_drop=train_set[(train_set['GrLivArea']>4000) & (train_set['SalePrice']<400000)].index\ntrain_set=train_set.drop(index_drop, axis=0)","28e3c769":"#Remove the outliers:\nindex_drop=train_set[(train_set['GrLivArea']>4000) & (train_set['SalePrice']>400000)].index\ntrain_set=train_set.drop(index_drop, axis=0)","566cc929":"sns.scatterplot(x='GrLivArea', y='SalePrice', data=train_set)\nplt.axhline(y=200000, color='r')\nplt.axvline(x=4000, color='r')","0668409a":"sns.scatterplot(x='OverallQual', y='SalePrice', data=train_set)\n#no need to remove any data","d45f97fe":"sns.boxplot(x='GarageCars', y='SalePrice', data=train_set)\nplt.axhline(y=680000,color='r')\n","dce5f618":"sns.scatterplot(data=train_set, x='TotRmsAbvGrd', y='SalePrice')\nplt.axhline(y=250000, color='r')\nplt.axvline(x=12.8, color='r')","c7bd231f":"train_set[(train_set['TotRmsAbvGrd']>12.7) & (train_set['SalePrice']<250000)][['SalePrice', 'TotRmsAbvGrd']]","0339a1ea":"#Remove the outliers:\nindex_drop=train_set[(train_set['TotRmsAbvGrd']>12.7) & (train_set['SalePrice']<250000)].index\ntrain_set=train_set.drop(index_drop, axis=0)","0469f867":"sns.scatterplot(data=train_set, x='TotRmsAbvGrd', y='SalePrice')\nsns.scatterplot(data=train_set, x='TotRmsAbvGrd', y='SalePrice')","2ddb7627":"# get correlations of each features in dataset\n# Plotting Heat Map to visualise correlation data better. \n# Drwan for only features having high correlation \n# (>0.5) with Target Variable\ncorr = train_set.corr()\ntop_corr_features = corr.index[abs(corr[\"SalePrice\"])>0.5]\n\nplt.figure(figsize=(10,10))\n#plot heat map\ng=sns.heatmap(train_set[top_corr_features].corr(),annot=True,cmap=\"YlGnBu\")","66c3fdcc":"#this shows that Houses become more expensive with time\ntrain_set.groupby('OverallQual')['SalePrice'].median().plot()\nplt.xlabel('Year Sold')\nplt.ylabel('Median House Price')\nplt.title(\"House Price vs YearSold\")","293af4ae":"top_corr_features","5196cb12":"#Convert to String:\ntrain_set['MSSubClass']= train_set['MSSubClass'].apply(str)","391dd2f6":"train_set.select_dtypes(include='object')","e8226da7":"train_set_num= train_set.select_dtypes(exclude='object')\ntrain_set_obj= train_set.select_dtypes(include='object')","035b4b25":"# Converting:\ntrain_set_obj= pd.get_dummies(train_set_obj, drop_first=True)","d1d68da3":"Final_df= pd.concat([train_set_num, train_set_obj], axis=1)","721e62c5":"#Separate features and target from train_df\nX = Final_df.drop('SalePrice',axis = 1)\ny = Final_df['SalePrice']","38626106":"#X = X.apply(pd.to_numeric, errors='coerce')\n#y = y.apply(pd.to_numeric, errors='coerce')","70002d7f":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression","cd1feb9a":"#Split the Dataset to Train & Test\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=100)","df0d88fb":"#train the model\nfrom sklearn.linear_model import LinearRegression\nmodel=LinearRegression()\nmodel.fit(X_train, y_train)","a67dd2ad":"#predicting test data\ny_pred=model.predict(X_test)","8548539e":"#evaluating the model\nfrom sklearn import metrics\nMAE=metrics.mean_absolute_error(y_test,y_pred)\nMSE=metrics.mean_squared_error(y_test,y_pred)\nRMSE=np.sqrt(MSE)","4c2c8ce2":"#coeficient matrix\npd.DataFrame(model.coef_,X.columns,columns=[\"coeficient\"])","d8440a94":"pd.DataFrame(data=[MAE,MSE,RMSE],index=[\"MAE\",\"MSE\",\"RMSE\"],columns=[\"LinearRegression\"])","dc8b5037":"from sklearn.preprocessing import PolynomialFeatures\n\npolynomial_converter=PolynomialFeatures(degree=2, include_bias=False)","f59a016a":"poly_features=polynomial_converter.fit(X)","6de19626":"poly_features=polynomial_converter.transform(X)","c2698d5f":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(poly_features, y, test_size=0.3, random_state=101)","57eb49b1":"from sklearn.linear_model import LinearRegression\npolymodel=LinearRegression()\npolymodel.fit(X_train, y_train)","7a4c2dae":"y_pred=polymodel.predict(X_test)","f0bb91e2":"pd.DataFrame({'Y_Test': y_test,'Y_Pred':y_pred, 'Residuals':(y_test-y_pred) }).head(5)","0a7da891":"from sklearn import metrics\nMAE_Poly = metrics.mean_absolute_error(y_test,y_pred)\nMSE_Poly = metrics.mean_squared_error(y_test,y_pred)\nRMSE_Poly = np.sqrt(MSE_Poly)\n\npd.DataFrame([MAE_Poly, MSE_Poly, RMSE_Poly], index=['MAE', 'MSE', 'RMSE'], columns=['metrics'])","1b9d03e7":"XS_train, XS_test, ys_train, ys_test = train_test_split(X, y, test_size=0.3, random_state=101)\nsimplemodel=LinearRegression()\nsimplemodel.fit(XS_train, ys_train)\nys_pred=simplemodel.predict(XS_test)\n\nMAE_simple = metrics.mean_absolute_error(ys_test,ys_pred)\nMSE_simple = metrics.mean_squared_error(ys_test,ys_pred)\n","b8e7459f":"RMSE_simple = np.sqrt(MSE_simple)","29335add":"pd.DataFrame({'Poly Metrics': [MAE_Poly, MSE_Poly, RMSE_Poly], 'Simple Metrics':[MAE_simple, MSE_simple, RMSE_simple]}, index=['MAE', 'MSE', 'RMSE'])","2fe70a63":"X = Final_df.drop('SalePrice',axis = 1)\ny = Final_df['SalePrice']","3a72cc77":"from sklearn.preprocessing import PolynomialFeatures\npolynomial_converter= PolynomialFeatures(degree=2, include_bias=False)\npoly_features= polynomial_converter.fit_transform(X)","e20d1774":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(poly_features, y, test_size=0.3, random_state=101)","c4a0a487":"from sklearn.preprocessing import StandardScaler\nscaler= StandardScaler()\nscaler.fit(X_train)","e589568e":"X_train= scaler.transform(X_train)\nX_test= scaler.transform(X_test)","38f41d38":"#Train the Model\nfrom sklearn.linear_model import Ridge\nridge_model= Ridge(alpha=10)","d1a2cb81":"ridge_model.fit(X_train, y_train)","52ead2be":"#predict Test Data\ny_pred= ridge_model.predict(X_test)","9243c008":"#Evaluating the Model\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\n\nMAE= mean_absolute_error(y_test, y_pred)\nMSE= mean_squared_error(y_test, y_pred)\nRMSE= np.sqrt(MSE)","9fa67b5f":"pd.DataFrame([MAE, MSE, RMSE], index=['MAE', 'MSE', 'RMSE'], columns=['metrics'])","88fdd684":"#Train the Model\nfrom sklearn.linear_model import RidgeCV\nridge_cv_model=RidgeCV(alphas=(0.1, 1.0, 10.0), scoring='neg_mean_absolute_error')","c8041556":"ridge_cv_model.fit(X_train, y_train)","e80b3db1":"ridge_cv_model.alpha_","74106f79":"#Predicting Test Data\ny_pred_ridge= ridge_cv_model.predict(X_test)","dbafb89f":"MAE_ridge= mean_absolute_error(y_test, y_pred_ridge)\nMSE_ridge= mean_squared_error(y_test, y_pred_ridge)\nRMSE_ridge= np.sqrt(MSE_ridge)","c3fe609c":"pd.DataFrame([MAE_ridge, MSE_ridge, RMSE_ridge], index=['MAE', 'MSE', 'RMSE'], columns=['Ridge Metrics'])","3f45c454":"from sklearn.linear_model import LassoCV\nlasso_cv_model= LassoCV(eps=0.01, n_alphas=100, cv=5)","251db543":"lasso_cv_model.alpha_","620a040e":"y_pred_lasso= lasso_cv_model.predict(X_test)","be19d475":"MAE_Lasso= mean_absolute_error(y_test, y_pred_lasso)\nMSE_Lasso= mean_squared_error(y_test, y_pred_lasso)\nRMSE_Lasso= np.sqrt(MSE_Lasso)","b8b51d8f":"pd.DataFrame([MAE_Lasso, MSE_Lasso, RMSE_Lasso], index=['MAE', 'MSE', 'RMSE'], columns=['Lasso Metrics'])","193c4ef7":"# <div id=\"intLink2\"> Pre-Processing # \n1. Handling missing value\n2. High\/Low Correletion data\n3. Categorical Data\n4. Numerical Columns to Categorical\n5. Dealing with Outliers\n6. Creating Dummy Variables\none-hot-encodding <\/div>","e447b62f":"#  <div id=\"intLink3\">1.Handling **missing** Values\n* Dropping columns with more than 70% null_value and Id (it might not be the best case for every problem or dataset).\n* Handling null value in the rest of thr features<\/div>","90bce076":"Elastic Net<div id=\"intLink12\">","ebce32d6":"# <div id=\"intLink6\">Polynomial Regression improves our model\n* Polynomial Regression adding more relevant features<\/div>","4da32c40":"Lasso Regression<div id=\"intLink11\">\n","8e5970ee":"Ridge Regression<div id=\"intLink9\">\n","1a40f7a3":"# <div id=\"intLink5\">**Linear Reggression**\n* we start our path with simple Linear Regression and then we try to improve our model<\/div>","80b915fb":"# Handling test_set null value \n* Functional\n* Exterior1st    \n* Exterior2nd    \n* KitchenQual    \n* SaleType       \n* Utilities      \n* Functional     \n* MSZoning","ad634749":"Scaling the Data<div id=\"intLink8\">","af79ada2":"**FireplaceQu: Fireplace quality**\n* acoording to the data this feature has an NA value that means the house has no fire place so we fill the column with 'None'","6ee5ea75":"**Mas Vnr Features:**\n\n* Based on the Dataset Document File, missing values for 'Mas Vnr Type' and 'Mas Vnr Area' means the house doesn't have any mansonry veneer. so, we decide to fill the missing value as below:","252cebc0":"**Garage Columns:**\n* Based on the dataset documentation, NaN in Garage Columns seems to indicate no garage.\n\n* Decision: Fill with 'None' or 0","b31f7ad0":"> ","38013652":"GrLivArea without outliner","67bc9d05":"# <div id=\"intLink\"> EDA <\/div>","85d18316":"**Creating Dummy Variables**","3ebd9cce":"#  Dealing with Categorical Data","2296d797":"# **House Price Prediction using Advance Reggression:**\n* **[EDA](#intLink)**\n* **[Pre-Processing](#intLink2)**\n  * **[Handling Missing Data](#intLink3)**\n  * **[Handling Outliners](#intLink4)**   \n* **[Lineer Reggresion](#intLink5)**\n* **[polynomial Regression](#intLink6)**\n* **[Regularization (Ridge - LASSO - ElasticNet)](#intLink7)**\n","e84284f6":"**Garage & Bacement**\n* by looking at the plot we realize that most features with missing value are from the same catagories.","abcd6a5d":"# Features that have high correlation (higher than 0.5)","cc9fcc78":"# Polymodel Regression vs Linear Regression\n* **RMSE decresed significeantly**","a8d9c705":"# <div id=\"intLink7\">Regularization\n* [Scaling the Data](#intLink8)\n* [Ridge Regression(Cross-Validation)](#intLink9)\n","6f80ee5f":"# <div id=\"intLink4\"> Handling Outliers<\/div>","82b85cea":"**Finally we check if there is more null value in the dataset**","f1256c1c":"# Poly_Features: \n(X1, X2, X3, X1^2, X2^2, X3^2, X1X2, X1X3, X2X3)\n* Split the Data to Train & Test\n* Train the Model"}}