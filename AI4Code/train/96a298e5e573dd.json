{"cell_type":{"d2469e68":"code","fa143111":"code","7fdd7dff":"code","5e7a5c0e":"code","9e7f227d":"code","695ee668":"code","5a425215":"code","2b626523":"code","e007d574":"code","41a731c2":"code","0b0c6982":"code","f7a0f77c":"code","0ea619d0":"code","e9d491c9":"code","ff128237":"code","9e755fcb":"code","3f1504fa":"code","d083beb0":"code","73f258e8":"code","9b93ce69":"code","6042ef59":"code","d7a38ffc":"code","b895e004":"code","8c0469aa":"code","fd541a26":"code","3abec4d2":"code","32ea952a":"code","7bf3c677":"code","d0f704a6":"code","00c4f94e":"code","ba30a525":"code","c09e93fb":"code","7ae8c9e3":"code","7eec90ed":"code","e0aef495":"code","4566673a":"code","37f586f1":"code","9c991955":"code","bdff9257":"code","0c31a94f":"code","3e3ed91f":"code","12ccba0e":"markdown","b47d7dec":"markdown","c7a7872f":"markdown","7a794322":"markdown","2de4e897":"markdown","e395d1ab":"markdown","d6eae551":"markdown","f3664f14":"markdown","12faabcd":"markdown","f2203d8c":"markdown","d12abee8":"markdown","1f75d1e9":"markdown","718e9467":"markdown","35456034":"markdown","5fe17559":"markdown","26078747":"markdown","9423f5a8":"markdown","22380b94":"markdown"},"source":{"d2469e68":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib\n%matplotlib inline\nimport random\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB,CategoricalNB\nimport nltk\nnltk.download('stopwords')\nnltk.download('punkt')\nimport re\nfrom nltk.corpus import stopwords\nimport string\nfrom sklearn import preprocessing\nfrom sklearn.manifold import TSNE\nimport seaborn as sns\nfrom nltk.stem.porter import PorterStemmer\nfrom sklearn.metrics import log_loss\nfrom sklearn.linear_model import LogisticRegression,SGDClassifier\nfrom sklearn import svm\nfrom nltk.tokenize import word_tokenize\nfrom sklearn.metrics import accuracy_score\nfrom time import time\nfrom sklearn.model_selection import StratifiedKFold","fa143111":"#default theme\nsns.set(context='notebook', style='darkgrid', palette='colorblind', font='sans-serif', font_scale=1, rc=None)\nmatplotlib.rcParams['figure.figsize'] =[8,8]\nmatplotlib.rcParams.update({'font.size': 15})\nmatplotlib.rcParams['font.family'] = 'sans-serif'","7fdd7dff":"train = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\nsub = pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')","5e7a5c0e":"print(train.shape,test.shape)","9e7f227d":"train.head()","695ee668":"train.describe(include='all')","5a425215":"missing_values=train.isnull().sum()\npercent_missing = train.isnull().sum()\/train.shape[0]*100\n\nvalue = {\n    'missing_values ':missing_values,\n    'percent_missing %':percent_missing\n}\nframe=pd.DataFrame(value)\nframe","2b626523":"#Remove redundant samples\ntrain=train.drop_duplicates(subset=['text', 'target'], keep='first')\ntrain.shape","e007d574":"train.target.value_counts()","41a731c2":"fig = plt.figure(figsize=(8,6))\ntrain.groupby('target').id.count().plot.pie(explode=[0.1,0.1],autopct='%1.1f%%',shadow=True)\nplt.show()","0b0c6982":"# Numbers of word for each sapmle in train & test data\ntrain['text_length'] = train.text.apply(lambda x: len(x.split()))\ntest['text_length'] = test.text.apply(lambda x: len(x.split()))\n","f7a0f77c":"train['text_length'].describe()","0ea619d0":"test['text_length'].describe()","e9d491c9":"def plot_word_count(df, data_name):\n  sns.distplot(df['text_length'].values)\n  plt.title(f'Sequence char count: {data_name}')\n  plt.grid(True)","ff128237":"#ig = plt.figure(figsize=(16,6))\n#plt.hist(train[\"text_length\"], bins = 30)\n#plt.show()\nplt.subplot(1, 2, 1)\nplot_word_count(train, 'Train')\n\nplt.subplot(1, 2, 2)\nplot_word_count(test, 'Test')\n\nplt.subplots_adjust(right=3.0)\nplt.show()","9e755fcb":"# collecting all words in single list\nlist_= []\nfor i in train.text:\n    list_ += i\nlist_= ''.join(list_)\nallWords=list_.split()\nvocabulary= set(allWords)","3f1504fa":"len(vocabulary)","d083beb0":"def create_corpus(df,target):\n    corpus=[]\n    \n    for x in df[df['target']==target]['text'].str.split():\n        for i in x:\n            corpus.append(i)\n    return corpus","73f258e8":"#most frequent 20 words when label == 0 \nimport collections\nallWords=create_corpus(train,target=0)\nvocabulary= set(allWords)\nvocabulary_list= list(vocabulary)\n\nplt.figure(figsize=(16,5))\ncounter=collections.Counter(allWords)\nmost=counter.most_common()\nx=[]\ny=[]\nfor word,count in most[:20]:\n  x.append(word)\n  y.append(count)\nsns.barplot(x=y,y=x)","9b93ce69":"#most frequent 20 words when label == 1 \nimport collections\nallWords=create_corpus(train,target=1)\nvocabulary= set(allWords)\nvocabulary_list= list(vocabulary)\n\nplt.figure(figsize=(16,5))\ncounter=collections.Counter(allWords)\nmost=counter.most_common()\nx=[]\ny=[]\nfor word,count in most[:20]:\n  x.append(word)\n  y.append(count)\nsns.barplot(x=y,y=x)","6042ef59":"#List of punctuations and we will remove them from our corpus\nstring.punctuation","d7a38ffc":"#for  example\ntext='hi !! whats up bro :) i hope you enjoy with me '\n\"\".join([char for char in text if char not in string.punctuation])","b895e004":"#for example \ntext='hey 4 look 333 at me0 58999632'\nre.sub('[0-9]', '', text)","8c0469aa":"#list of stopwords\nstopwords.words('english')","fd541a26":"#for example\ntext='hey this is me and I am here to help you  '\ntokens = word_tokenize(text)\ntokens=[word for word in tokens if word not in stopwords.words('english')]\n' '.join(tokens)","3abec4d2":"pstem = PorterStemmer()\ndef clean_text(text):\n    text= text.lower()\n    text= re.sub('[0-9]', '', text)\n    text  = \"\".join([char for char in text if char not in string.punctuation])\n    tokens = word_tokenize(text)\n    tokens=[pstem.stem(word) for word in tokens]\n    #tokens=[word for word in tokens if word not in stopwords.words('english')]\n    text = ' '.join(tokens)\n    return text","32ea952a":"clean_text(\"hey I am here # ! looks 4 GOOD can't see you!\")","7bf3c677":"train[\"clean\"]=train[\"text\"].apply(clean_text)\ntest[\"clean\"]=test[\"text\"].apply(clean_text)","d0f704a6":"#Let's see the effect of cleaning\ntrain[[\"text\",\"clean\"]].head()","00c4f94e":"# collecting all words in single list\nlist_= []\nfor i in train.clean:\n    list_ += i\nlist_= ''.join(list_)\nallWords=list_.split()\nvocabulary= set(allWords)\nlen(vocabulary)","ba30a525":"tfidf = TfidfVectorizer(sublinear_tf=True,max_features=60000, min_df=1, norm='l2',  ngram_range=(1,2))\nfeatures = tfidf.fit_transform(train.clean).toarray()\nfeatures.shape","c09e93fb":"features_test = tfidf.transform(test.clean).toarray()","7ae8c9e3":"#split data into 4 parts with same distribution of classes.\nskf = StratifiedKFold(n_splits=4, random_state=48, shuffle=True)\naccuracy=[] # list contains the accuracy for each fold\nn=1\ny=train['target']","7eec90ed":"for trn_idx, test_idx in skf.split(features, y):\n  start_time = time()\n  X_tr,X_val=features[trn_idx],features[test_idx]\n  y_tr,y_val=y.iloc[trn_idx],y.iloc[test_idx]\n  model= LogisticRegression(max_iter=1000,C=3)\n  #model=MultinomialNB(alpha=0.5)\n  #model=svm.SVC(max_iter=1000)\n  model.fit(X_tr,y_tr)\n  s = model.predict(X_val)\n  sub[str(n)]= model.predict(features_test) \n  \n  accuracy.append(accuracy_score(y_val, s))\n  print((time() - start_time)\/60,accuracy[n-1])\n  n+=1","e0aef495":"accuracy","4566673a":"np.mean(accuracy)*100","37f586f1":"from sklearn.metrics import confusion_matrix, classification_report\npred_valid_y = model.predict(X_val)\nprint(classification_report(y_val, pred_valid_y ))","9c991955":"print(confusion_matrix(y_val, pred_valid_y ))","bdff9257":"sub.head(10)","0c31a94f":"df=sub[['1','2','3','4']].mode(axis=1)# select the most frequent predicted class by our model\nsub['target']=df[0]    \nsub=sub[['id','target']]\nsub['target']=sub['target'].apply(lambda x : int(x))","3e3ed91f":"sub.to_csv('submission.csv',index=False)","12ccba0e":"# 3) Data Cleaning","b47d7dec":"### Removing Stopwords","c7a7872f":"labels are not balanced","7a794322":"## What is Sentiment Analysis?\n\nSentiment Analysis is a process of extracting opinions that have different polarities. By polarities, we mean positive, negative or neutral. It is also known as opinion mining and polarity detection. With the help of sentiment analysis, you can find out the nature of opinion that is reflected in documents, websites, social media feed, etc. Sentiment Analysis is a type of classification where the data is classified into different classes. These classes can be binary in nature (positive or negative) or, they can have multiple classes (happy, sad, angry, etc.).\n\n# Please If you find this kernel helpful, upvote it to help others see it \ud83d\ude0a\n![](https:\/\/d1sjtleuqoc1be.cloudfront.net\/wp-content\/uploads\/2019\/04\/25112909\/shutterstock_1073953772.jpg)","2de4e897":"We have 92 redundants sapmles in our dataset","e395d1ab":"we reduced our data from 31480 unique words to 19920","d6eae551":"Max number of words in all data is 31 and min is 1!","f3664f14":"We have 31480 different words in our train data","12faabcd":"### Now let's Build a function that clean our data\n\nI just added lower function in order to lowercase all words and stemming","f2203d8c":"### Removing Punctuations","d12abee8":"# Submission","1f75d1e9":"### Removing Numbers","718e9467":"# 2) load data & analysis","35456034":"## 5) Evaluating Model on Validation Set","5fe17559":"### finding missing values","26078747":"## What files do I need?\n\nYou'll need train.csv, test.csv and sample_submission.csv.\n\n## What should I expect the data format to be?\n\nEach sample in the train and test set has the following information:\n\nThe text of a tweet\nA keyword from that tweet (although this may be blank!)\nThe location the tweet was sent from (may also be blank)\n\n## What am I predicting?\n\nYou are predicting whether a given tweet is about a real disaster or not. If so, predict a 1. If not, predict a 0.\n\n### Files\n* train.csv - the training set\n* test.csv - the test set\n* sample_submission.csv - a sample submission file in the correct format\n\n### Columns\n\n* id - a unique identifier for each tweet\n* text - the text of the tweet\n* location - the location the tweet was sent from (may be blank)\n* keyword - a particular keyword from the tweet (may be blank)\n* target - in train.csv only, this denotes whether a tweet is about a real disaster (1) or not (0)\n\n","9423f5a8":"# 1) import library & packages","22380b94":"## 4) machine leaning algorithm"}}