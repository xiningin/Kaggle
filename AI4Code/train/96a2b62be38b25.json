{"cell_type":{"6f1ab40f":"code","c423c541":"code","0cf4bd85":"code","5b4deb33":"code","fac68537":"code","2afe8771":"code","807ab8a4":"code","3dfde5e1":"code","19974ebf":"code","e46768be":"code","69fc8193":"code","f4b93b1c":"code","e637e670":"code","2c472b87":"code","3e6242a8":"code","60974e20":"code","f4af7046":"code","8467bcc9":"code","63cd5bff":"code","753746f5":"code","b510a567":"code","0c995f0a":"code","0986cce3":"markdown","25e55592":"markdown","9ee07c2b":"markdown","1dd79600":"markdown","63a1a7fa":"markdown","21aa318c":"markdown","f115416b":"markdown","d6bf5a0f":"markdown","3b9e16cc":"markdown"},"source":{"6f1ab40f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nplt.style.use('seaborn-whitegrid')\nsns.set_style(\"white\")\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nfrom keras.layers import Dropout\nfrom keras.layers import LSTM\nfrom keras.layers import TimeDistributed\nfrom keras.layers.convolutional import Conv1D\nfrom keras.layers.convolutional import MaxPooling1D\nfrom keras.layers import ConvLSTM2D\nfrom keras.utils import to_categorical\nfrom keras import backend as K \nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras.models import load_model, model_from_json\nfrom keras.metrics import CategoricalAccuracy, CategoricalCrossentropy\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import confusion_matrix, auc, roc_curve, roc_auc_score, precision_score, recall_score, f1_score, accuracy_score, classification_report\n\nfrom numpy.random import seed\nfrom tensorflow.random import set_seed\n\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c423c541":"activities = {\n    1: 'Walking',\n    2: 'Walking Upstairs',\n    3: 'Walking Downstairs',\n    4: 'Sitting',\n    5: 'Standing',\n    6: 'Laying'\n}","0cf4bd85":"# load a single file as a numpy array\ndef load_file(filepath):\n    df = pd.read_csv(filepath, header=None, delim_whitespace=True)\n    return df.values\n\n# load a list of files into a 3D array of [observations, timesteps, features(x,y,z)]\ndef load_group(files, prefix=''):\n    loaded = list()\n    for f in files:\n        data = load_file(prefix + f)\n        loaded.append(data)\n    # stack group so that features are the 3rd dimension\n    loaded = np.dstack(loaded)\n    return loaded\n\ndef load_dataset_group(group, prefix=''):\n    filepath = prefix + group + '\/Inertial Signals\/'\n    # load all 9 files as a single array\n    files = list()\n    # body acceleration\n    files += ['body_acc_x_'+group+'.txt', 'body_acc_y_'+group+'.txt', 'body_acc_z_'+group+'.txt']\n    # body gyroscope\n    files += ['body_gyro_x_'+group+'.txt', 'body_gyro_y_'+group+'.txt', 'body_gyro_z_'+group+'.txt']\n    # total acceleration\n    files += ['total_acc_x_'+group+'.txt', 'total_acc_y_'+group+'.txt', 'total_acc_z_'+group+'.txt']\n    # load input data\n    X = load_group(files, filepath)\n    # load class output\n    y = load_file(prefix + group + '\/y_'+group+'.txt')\n    return X, y\n\ndef load_dataset(prefix=''):\n    # load all train\n    X_train, y_train = load_dataset_group('train', prefix)\n    # load all test\n    X_test, y_test = load_dataset_group('test', prefix)\n    # zero-offset class values\n    y_train = y_train - 1\n    y_test = y_test - 1\n    y_train = to_categorical(y_train)\n    y_test = to_categorical(y_test)\n    print(f\"\"\"Dataset loaded.\nTraining Set:\nX_train {X_train.shape} y_train {y_train.shape}\nTest Set:\nX_test {X_test.shape} y_test {y_test.shape}\"\"\")\n    return X_train, y_train, X_test, y_test","5b4deb33":"def create_model(model):\n    classifier = KerasClassifier(model, verbose=2)\n    return classifier","fac68537":"# GridSearch\ndef grid(classifier):\n    # define the grid search parameters\n    batch_size = [8, 16, 32, 64]\n    epochs = [10, 15, 20, 25]\n    validation_split=[0.2]\n    param_grid = dict(batch_size=batch_size, epochs=epochs, validation_split=validation_split)\n    grid = GridSearchCV(estimator=classifier, param_grid=param_grid, n_jobs=-1, cv=5, return_train_score=True, verbose=2)\n    grid_result = grid.fit(X_train, y_train)\n    # summarize results\n    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n    means = grid_result.cv_results_['mean_test_score']\n    stds = grid_result.cv_results_['std_test_score']\n    params = grid_result.cv_results_['params']\n    for mean, stdev, param in zip(means, stds, params):\n        print(\"%f (%f) with: %r\" % (mean, stdev, param))\n    return grid_result","2afe8771":"def evaluate_model(X_train, y_train, X_test, y_test, params, model):\n    epochs, batch_size = params[\"epochs\"], params[\"batch_size\"]\n    es =EarlyStopping(monitor='val_loss', patience=5)\n    classifier=model()\n    history = classifier.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=2, validation_split=0.2, callbacks=[es])\n    # evaluate model\n    _, accuracy = classifier.evaluate( X_test, y_test, batch_size=batch_size, verbose=0)\n    return history, accuracy, classifier","807ab8a4":"def run_model(model, grid_result):\n    history, score, classifier = evaluate_model(X_train, y_train, X_test, y_test, grid_result.best_params_, model)\n    score = score * 100.0\n    print('> %.3f' % (score))\n    results=pd.DataFrame(history.history, index=history.epoch)\n    return results, classifier","3dfde5e1":"def plot_loss(results):\n    plt.style.use('seaborn-whitegrid')\n    sns.set(style=\"white\", font_scale = 1.5)\n    fig, axes = plt.subplots(1,2, figsize=(20,10), sharex=True)\n    axes[0].set_title('Loss')\n    axes[0].plot(results['loss'], label='train', )\n    axes[0].plot(results['val_loss'], label='test')\n    axes[0].legend()\n    #plot accuracy during training\n    axes[1].set_title('Accuracy')\n    axes[1].plot(results['accuracy'], label='train')\n    axes[1].plot(results['val_accuracy'], label='test')\n    axes[1].legend()\n    plt.show()\n    return","19974ebf":"def run(model):\n    classifier=create_model(model)\n    grid_result=grid(classifier)\n    results, classifier=run_model(model, grid_result)\n    plot_loss(results)\n    return classifier","e46768be":"X_train, y_train, X_test, y_test = load_dataset(prefix=\"..\/input\/human-activity-recognition\/UCI_HAR_Dataset\/\")\nn_timesteps, n_features, n_outputs = X_train.shape[1], X_train.shape[2], y_train.shape[1]","69fc8193":"def model1():\n    seed(17)\n    set_seed(17)\n    K.clear_session()\n    model = Sequential()\n    model.add(LSTM(64, input_shape=(n_timesteps,n_features)))\n    model.add(Dropout(0.1))\n    model.add(Dense(64, activation='relu'))\n    model.add(Dense(n_outputs, activation='softmax'))\n    model.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n    return model","f4b93b1c":"classifier1=run(model1)","e637e670":"def model2():\n    seed(17)\n    set_seed(17)\n    K.clear_session()\n    model = Sequential()\n    model.add(Conv1D(filters=64, kernel_size=3, activation='relu',input_shape=(n_timesteps, n_features)))\n    model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(MaxPooling1D(pool_size=2))\n    model.add(Flatten())\n    model.add(Dense(100, activation='relu'))\n    model.add(Dense(n_outputs, activation='softmax'))\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model","2c472b87":"classifier2=run(model2)","3e6242a8":"# reshape data into time steps of sub-sequences\nn_steps, n_length = 4, 32\nX_train = X_train.reshape((X_train.shape[0], n_steps, n_length, n_features))\nX_test = X_test.reshape((X_test.shape[0], n_steps, n_length, n_features))","60974e20":"def model3():\n    seed(17)\n    set_seed(17)\n    K.clear_session()\n    model = Sequential()\n    model.add(TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu'), input_shape=(None,n_length,n_features)))\n    model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n    model.add(TimeDistributed(Dropout(0.5)))\n    model.add(TimeDistributed(Conv1D(filters=128, kernel_size=3, activation='relu')))\n    model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n    model.add(TimeDistributed(Dropout(0.5)))\n    model.add(TimeDistributed(Flatten()))\n    model.add(LSTM(100))\n    model.add(Dropout(0.5))\n    model.add(Dense(100, activation='relu'))\n    model.add(Dense(n_outputs, activation='softmax'))\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model","f4af7046":"classifier3=run(model3)","8467bcc9":"classifier3.summary()","63cd5bff":"# Final evaluation of the model\nscores = classifier3.evaluate(X_test, y_test, verbose=0)\nprint(\"Test Loss: %f\" % (scores[0]))\nprint(\"Test Accuracy: %f%%\" % (scores[1]*100))","753746f5":"# Confusion Matrix\ny_pred=classifier3.predict(X_test)\ny_predict=pd.Series([activities[i+1] for i in np.argmax(y_pred, axis=1)])\ny_actual=pd.Series([activities[i+1] for i in np.argmax(y_test, axis=1)])\n\n\n# Code for drawing seaborn heatmaps\nclass_names = list(activities.values())\ndf_heatmap = pd.DataFrame(confusion_matrix(y_actual, y_predict) )\n\nplt.style.use('seaborn-whitegrid')\nsns.set(style=\"white\", font_scale = 1.5)\n\nfig = plt.figure(figsize=(15,12))\nheatmap = sns.heatmap(df_heatmap, annot=True, fmt=\"d\", cmap=\"Blues\")\n\n# Setting tick labels for heatmap\nheatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right')\nheatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=30, ha='right')\nheatmap.set_xticklabels(class_names)\nheatmap.set_yticklabels(class_names)\nplt.ylabel('Actual Activity')\nplt.xlabel('Predicted Activity')\nplt.title(\"Confusion Matrix\\n\")\n\nplt.show()","b510a567":"# Classification Report\ncr=classification_report(y_actual, y_predict, target_names=activities.values(), digits=4)\nprint(cr)","0c995f0a":"# ROC AUC\nn_classes = 6\n\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\nfor i in range(n_classes):\n    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_pred[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n    \n#Compute micro-average ROC curve and ROC area\nfpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_pred.ravel())\nroc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n\n# First aggregate all false positive rates\nall_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n\n# Then interpolate all ROC curves at this points\nmean_tpr = np.zeros_like(all_fpr)\nfor i in range(n_classes):\n    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n\n# Finally average it and compute AUC\nmean_tpr \/= n_classes\n\nfpr[\"macro\"] = all_fpr\ntpr[\"macro\"] = mean_tpr\nroc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n\n# Plot all ROC curves\nplt.style.use('seaborn-whitegrid')\nsns.set(style=\"white\", font_scale = 1.2)\nplt.figure(figsize=(12,12))\nplt.plot(fpr[\"micro\"], tpr[\"micro\"],\n         label='micro-average ROC curve (area = {0:0.2f})'\n               ''.format(roc_auc[\"micro\"]),\n         color='deeppink', linestyle=':', linewidth=4)\n\nplt.plot(fpr[\"macro\"], tpr[\"macro\"],\n         label='macro-average ROC curve (area = {0:0.2f})'\n               ''.format(roc_auc[\"macro\"]),\n         color='navy', linestyle=':', linewidth=4)\n\ncolors = ['aqua', 'darkorange', 'cornflowerblue', \"red\", \"green\", \"black\"]\nfor i, color in zip(range(n_classes), colors):\n    plt.plot(fpr[i], tpr[i], color=color,\n             label='{0} (area = {1:0.2f})'\n             ''.format(activities[i+1], roc_auc[i]))\n\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC curve for each activity')\nplt.legend(loc=\"lower right\")\n\nplt.show()","0986cce3":"### LSTM Model","25e55592":"## Evaluate Model","9ee07c2b":"## Base Model","1dd79600":"## Hyperparameters Tuning","63a1a7fa":"## Data Pipeline","21aa318c":"### CNN-LSTM Network Model","f115416b":"# Neural Network Models","d6bf5a0f":"## Performance of Final Model","3b9e16cc":"### CNN Model"}}