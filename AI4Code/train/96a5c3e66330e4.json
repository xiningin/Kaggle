{"cell_type":{"618bd24e":"code","dc2797a9":"code","d28a7733":"code","a0c37cb2":"code","3764c54d":"code","da5fbb63":"code","c34e9243":"code","e911e1de":"code","5e367093":"code","2233819d":"code","9f654016":"code","cf6cf5c0":"code","f53bc53f":"code","41ca8af0":"code","a546a876":"code","048b7742":"code","90bf4819":"code","7345b000":"code","a0ad3a5c":"code","48e17cb5":"code","0dbd90dc":"code","471277b8":"code","00651cc1":"code","4890352c":"code","e410c2a3":"code","9c2e8d6a":"code","b22ce5ce":"code","308b0e43":"code","da82d888":"code","27b792ed":"code","c18aea9d":"code","529e00e6":"code","e8227e4b":"code","ffdaf9e3":"code","2743b59f":"code","5697f098":"code","efc421b1":"code","bdb2c4fd":"code","0b6084a8":"code","45b7d14c":"code","5a8e76f9":"code","1d5af4f8":"code","110d095e":"code","93f118b4":"code","200a0d2e":"code","7f34d462":"code","f2af9984":"code","b5dc6c44":"code","d9ddae2a":"code","6587b906":"code","9c016f60":"code","5ab8b94c":"code","f4cfb9aa":"code","e4a1f0c0":"code","4ff8b68e":"code","15b333cb":"code","512a9e6a":"code","12d6a686":"code","bb3dc773":"code","ad342113":"code","ede58d52":"code","eccebe53":"code","40e7413b":"code","e033f569":"code","1318db5a":"code","23e0e0b5":"code","b7720a95":"code","aee4f241":"code","86441490":"code","b1f72d63":"code","42b4fcc7":"code","d3141829":"code","3d375575":"code","2d1dd2ae":"code","11b363a7":"code","858b528a":"code","f74c1908":"code","2cd7a8a5":"code","14d14f2d":"code","ce60485a":"code","8c7ea501":"code","209b54c9":"code","a6af3498":"code","92b5057e":"code","570e91cb":"code","76ea252a":"code","fa7b6f60":"code","d5d4b157":"code","49eb1f56":"code","65a61564":"code","aaedbaac":"code","016000d3":"code","d0fb0fb7":"code","49e5dab9":"code","ed399160":"code","840b8d22":"code","1827d381":"code","83baeffb":"code","bdb6734a":"code","4ea72e97":"code","8c4778d0":"code","673321f2":"code","ffdde293":"code","43ef2f05":"code","b0f150a5":"code","89c49e82":"code","81ae6289":"code","070c7627":"code","f8b47f31":"code","d9ee3e7e":"code","eb14666c":"code","9461764e":"code","51ce35c4":"code","2b7d2542":"markdown","38c78185":"markdown","f71fddc9":"markdown","fa9bc101":"markdown","0bf13946":"markdown","d6d5ae3a":"markdown","fdb28b82":"markdown","0c568eb8":"markdown","176a5572":"markdown","7112593f":"markdown","bfeb95d2":"markdown","89a6eaed":"markdown","9a726666":"markdown","c0c32dd0":"markdown","f333226a":"markdown","353218f5":"markdown","cf454005":"markdown","04087d79":"markdown","dd06258e":"markdown","964c36d1":"markdown","6042a85c":"markdown","6dff268a":"markdown","3441b64c":"markdown","085ff176":"markdown","152e83e5":"markdown","646a0712":"markdown","a350237c":"markdown","cb105b38":"markdown","b04701be":"markdown","27052368":"markdown","6958141a":"markdown","f515f0b7":"markdown","01e9e642":"markdown","97c8f362":"markdown","6643897b":"markdown","86334d29":"markdown","c4ba8f5b":"markdown","6e64e14d":"markdown","ba430681":"markdown","3afaac76":"markdown","392617d2":"markdown","e0da1e12":"markdown","5adf000b":"markdown","d45ed986":"markdown","9dc9e3bf":"markdown","45a3714b":"markdown","05261a91":"markdown","673eb897":"markdown","60d530c5":"markdown","cb33773f":"markdown","778b9b2d":"markdown","33a4116d":"markdown","e6698d9c":"markdown","5c56bbff":"markdown","97c6dea0":"markdown","7cf2c7f8":"markdown","91d03e25":"markdown","fb44f7c6":"markdown","7da500f5":"markdown","719c0fad":"markdown","7d49ed1f":"markdown","b899c4df":"markdown","f4b1c5c8":"markdown","884dc7c8":"markdown","4704b031":"markdown","595d2e62":"markdown","2ca7081a":"markdown","1fa9225e":"markdown","f271f581":"markdown","a6a3b81b":"markdown","8f5e963b":"markdown","0c1a2286":"markdown","c3c58a22":"markdown","36c0296d":"markdown","48f850c5":"markdown","7531c8ad":"markdown","522370b4":"markdown","9f36b988":"markdown"},"source":{"618bd24e":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport gc\n\nimport matplotlib.pyplot as plt # matplotlib and seaborn for plotting\nimport matplotlib.patches as patches\nimport seaborn as sns\n\nfrom plotly import tools, subplots\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.express as px\npd.set_option('max_columns', 150)\npy.init_notebook_mode(connected=True)\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport os,random, math, psutil, pickle\n\nfrom time import time\nimport datetime\npd.set_option('display.max_columns',100)\npd.set_option('display.float_format', lambda x: '%.5f' % x)\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split,KFold\nfrom sklearn import metrics\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\n\nroot = '..\/input\/ashrae-energy-prediction'\nprint(os.listdir(root))","dc2797a9":"train = pd.read_csv(root + \"\/train.csv\", parse_dates=['timestamp'])\n\nweather_train = pd.read_csv(root+\"\/weather_train.csv\",parse_dates=['timestamp'])\n\ntest_cols_to_read = ['building_id','meter','timestamp']\ntest = pd.read_csv(root+\"\/test.csv\",parse_dates=['timestamp'],usecols=test_cols_to_read)\n\nweather_test = pd.read_csv(root + \"\/weather_test.csv\", parse_dates=['timestamp'])\n\nbuilding_meta = pd.read_csv(root + \"\/building_metadata.csv\")\n\nsample_submission = pd.read_csv(root + \"\/sample_submission.csv\")","d28a7733":"print('Size of train data', train.shape)\nprint('Size of weather_train data', weather_train.shape)\nprint('Size of weather_test data', weather_test.shape)\nprint('Size of building_meta data', building_meta.shape)","a0c37cb2":"weather = pd.concat([weather_train,weather_test],ignore_index=True)\nweather_key = ['site_id', 'timestamp']\ntemp_skeleton = weather[weather_key + ['air_temperature']].drop_duplicates(subset=weather_key).sort_values(by=weather_key).copy()","3764c54d":"data_to_plot = temp_skeleton.copy()\ndata_to_plot[\"hour\"] = data_to_plot[\"timestamp\"].dt.hour\ncount = 1\nplt.figure(figsize=(25, 15))\nfor site_id, data_by_site in data_to_plot.groupby('site_id'):\n    by_site_by_hour = data_by_site.groupby('hour').mean()\n    ax = plt.subplot(4, 4, count)\n    plt.plot(by_site_by_hour.index,by_site_by_hour['air_temperature'],'xb-')\n    ax.set_title('site: '+str(site_id))\n    count += 1\nplt.tight_layout()\nplt.show()\ndel data_to_plot","da5fbb63":"temp_skeleton['temp_rank'] = temp_skeleton.groupby(['site_id', temp_skeleton.timestamp.dt.date])['air_temperature'].rank('average')\n\ndf_2d = temp_skeleton.groupby(['site_id', temp_skeleton.timestamp.dt.hour])['temp_rank'].mean().unstack(level=1)\n\nsite_ids_offsets = pd.Series(df_2d.values.argmax(axis=1) - 14)\nsite_ids_offsets.index.name = 'site_id'\n\ndef timestamp_align(df):\n    df['offset'] = df.site_id.map(site_ids_offsets)\n    df['timestamp_aligned'] = (df.timestamp - pd.to_timedelta(df.offset, unit='H'))\n    df['timestamp'] = df['timestamp_aligned']\n    del df['timestamp_aligned']\n    return df","c34e9243":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","e911e1de":"building_site_dict = dict(zip(building_meta['building_id'], building_meta['site_id']))\nsite_meter_raw = train[['building_id', 'meter', 'timestamp', 'meter_reading']].copy()\nsite_meter_raw['site_id'] = site_meter_raw.building_id.map(building_site_dict)\ndel site_meter_raw['building_id']\nsite_meter_to_plot = site_meter_raw.copy()\nsite_meter_to_plot[\"hour\"] = site_meter_to_plot[\"timestamp\"].dt.hour\nelec_to_plot = site_meter_to_plot[site_meter_to_plot.meter == 0]","5e367093":"count = 1\nplt.figure(figsize=(25, 50))\nfor site_id, data_by_site in elec_to_plot.groupby('site_id'):\n    by_site_by_hour = data_by_site.groupby('hour').mean()\n    ax = plt.subplot(15, 4, count)\n    plt.plot(by_site_by_hour.index,by_site_by_hour['meter_reading'],'xb-')\n    ax.set_title('site: '+str(site_id))\n    count += 1\nplt.tight_layout()\nplt.show()\ndel elec_to_plot, site_meter_to_plot, building_site_dict, site_meter_raw","2233819d":"train = reduce_mem_usage(train)\ntest = reduce_mem_usage(test)\nweather_train = reduce_mem_usage(weather_train)\nweather_test = reduce_mem_usage(weather_test)\nbuilding_meta = reduce_mem_usage(building_meta)","9f654016":"weather_train = timestamp_align(weather_train)\nweather_test = timestamp_align(weather_test)\ndel weather","cf6cf5c0":"print('train info',train.info())\nprint('-------------------')\nprint('weather_train info', weather_train.info())\nprint('-------------------')\nprint('test info', test.info()) \nprint('-------------------')\nprint('weather_test info', weather_test.info())\nprint('-------------------')\nprint('building info', building_meta.info())","f53bc53f":"train.head()","41ca8af0":"train.describe(include='all')","a546a876":"test.head()","048b7742":"test.describe(include='all')","90bf4819":"missing_train_test = pd.DataFrame(train.isna().sum()\/len(train),columns=[\"Missing_Pct_Train\"])\nmissing_train_test[\"Missing_Pct_Test\"] = test.isna().sum()\/len(test)\nmissing_train_test","7345b000":"building_meta.head()","a0ad3a5c":"building_meta.describe(include='all')","48e17cb5":"building_meta.isna().sum()\/len(building_meta)*100","0dbd90dc":"weather_train.head()","471277b8":"weather_train.describe()","00651cc1":"weather_test.head()","4890352c":"weather_test.describe()","e410c2a3":"missing_weather = pd.DataFrame(weather_train.isna().sum()\/len(weather_train)*100,columns=[\"Weather_Train_Missing_Pct\"]) \nmissing_weather[\"Weather_Test_Missing_Pct\"] = weather_test.isna().sum()\/len(weather_test)*100 \nmissing_weather","9c2e8d6a":"cols = ['air_temperature','cloud_coverage','dew_temperature','precip_depth_1_hr','sea_level_pressure','wind_direction','wind_speed']\nfor col in cols:\n    print (\" Minimum Value of {} column is {}\".format(col,weather_train[col].min()))\n    print (\" Maximum Value of {} column is {}\".format(col,weather_train[col].max()))\n    print (\"----------------------------------------------------------------------\")","b22ce5ce":"weather_train['timestamp'].describe()","308b0e43":"#creating a distplot of columns air_temperature,cloud_coverage,dew_temperature,precip_depth_1_hr,sea_level_pressure,wind_speed\ncols = ['air_temperature','cloud_coverage','dew_temperature','precip_depth_1_hr','sea_level_pressure','wind_speed']\nfor ind,col in enumerate(weather_train[cols]):\n    plt.figure(ind)\n    sns.distplot(weather_train[col].dropna())","da82d888":"weather_test['timestamp'].describe()","27b792ed":"building_meta.drop('floor_count',axis=1,inplace=True)\nbuilding_meta.head()","c18aea9d":"# since NA values only present in building age fillna can be used\nbuilding_meta.fillna(round(building_meta.year_built.mean(),0),\n                inplace=True)","529e00e6":"building_meta.head()\nbuilding_meta.isna().sum()","e8227e4b":"# add month, day of week, day of month and hour \nweather_train['month'] = weather_train['timestamp'].dt.month.astype(np.int8)\nweather_train['day_of_week'] = weather_train['timestamp'].dt.dayofweek.astype(np.int8)\nweather_train['day_of_month']= weather_train['timestamp'].dt.day.astype(np.int8)\nweather_train['hour'] = weather_train['timestamp'].dt.hour\n\n# add is weekend column\nweather_train['is_weekend'] = weather_train.day_of_week.apply(lambda x: 1 if x>=5 else 0)","ffdaf9e3":"def convert_season(month):\n    if (month <= 2) | (month == 12):\n        return 0\n    # as winter\n    elif month <= 5:\n        return 1\n    # as spring\n    elif month <= 8:\n        return 2\n    # as summer\n    elif month <= 11:\n        return 3\n    # as fall","2743b59f":"weather_train['season'] = weather_train.month.apply(convert_season)","5697f098":"#Reset Index for Update for training \nweather_train = weather_train.set_index(\n    ['site_id','day_of_month','month'])","efc421b1":"# create dataframe of daily means per site id \nair_temperature_filler = pd.DataFrame(weather_train\n                                      .groupby(['site_id','day_of_month','month'])\n                                      ['air_temperature'].mean(),\n                                      columns=[\"air_temperature\"])\nair_temperature_filler.isna().sum()","bdb2c4fd":"# create dataframe of air_temperatures to fill\ntemporary_df = pd.DataFrame({'air_temperature' : weather_train.air_temperature})\n\n# update NA air_temperature values\ntemporary_df.update(air_temperature_filler, overwrite=False)\n\n# update in the weather train dataset\nweather_train[\"air_temperature\"] = temporary_df[\"air_temperature\"]\n\ndel temporary_df, air_temperature_filler\ngc.collect()","0b6084a8":"# create dataframe of daily means per site id\ncloud_coverage_filler = pd.DataFrame(weather_train\n                                     .groupby(['site_id','day_of_month','month'])\n                                     ['cloud_coverage'].mean(),\n                                     columns = ['cloud_coverage'])\ncloud_coverage_filler.isna().sum()","45b7d14c":"round(cloud_coverage_filler.cloud_coverage.mean(),0)\ncloud_coverage_filler.fillna(round(cloud_coverage_filler.cloud_coverage.mean(),0), \n                             inplace=True)\n\n# create dataframe of cloud_coverages to fill\ntemporary_df = pd.DataFrame({'cloud_coverage' : weather_train.cloud_coverage})\n\n# update NA cloud_coverage values\ntemporary_df.update(cloud_coverage_filler, overwrite=False)\n\n# update in the weather train dataset\nweather_train[\"cloud_coverage\"] = temporary_df[\"cloud_coverage\"]\n\ndel temporary_df, cloud_coverage_filler\ngc.collect()","5a8e76f9":"# create dataframe of daily means per site id\ndew_temperature_filler = pd.DataFrame(weather_train\n                                      .groupby(['site_id','day_of_month','month'])\n                                      ['dew_temperature'].mean(),\n                                      columns=[\"dew_temperature\"])\ndew_temperature_filler.isna().sum()","1d5af4f8":"# create dataframe of dew_temperatures to fill\ntemporary_df = pd.DataFrame({'dew_temperature' : weather_train.dew_temperature})\n\n# update NA dew_temperature values\ntemporary_df.update(dew_temperature_filler, overwrite=False)\n\n# update in the weather train dataset\nweather_train[\"dew_temperature\"] = temporary_df[\"dew_temperature\"]\n\ndel temporary_df, dew_temperature_filler\ngc.collect()","110d095e":"# create dataframe of daily means per site id\nprecip_depth_filler = pd.DataFrame(weather_train\n                                   .groupby(['site_id','day_of_month','month'])\n                                   ['precip_depth_1_hr'].mean(),\n                                   columns=['precip_depth_1_hr'])\nprecip_depth_filler.isna().sum()","93f118b4":"round(precip_depth_filler['precip_depth_1_hr'].mean(),0)\nprecip_depth_filler.fillna(round(precip_depth_filler['precip_depth_1_hr'].mean(),0)\n                           , inplace=True)\n\n# create dataframe of precip_depth_1_hr to fill\ntemporary_df = pd.DataFrame({'precip_depth_1_hr' : weather_train.precip_depth_1_hr})\n\n# update NA precip_depth_1_hr values\ntemporary_df.update(precip_depth_filler, overwrite=False)\n\n# update in the weather train dataset\nweather_train[\"precip_depth_1_hr\"] = temporary_df[\"precip_depth_1_hr\"]\n\ndel precip_depth_filler, temporary_df\ngc.collect()","200a0d2e":"# create dataframe of daily means per site id\nsea_level_filler = pd.DataFrame(weather_train\n                                .groupby(['site_id','day_of_month','month'])\n                                ['sea_level_pressure'].mean(),\n                                columns=['sea_level_pressure'])\nsea_level_filler.isna().sum()","7f34d462":"mean_sea_level_pressure = round(\n    sea_level_filler\n    ['sea_level_pressure']\n    .astype(float)\n    .mean(),2)","f2af9984":"sea_level_filler.fillna(mean_sea_level_pressure, inplace=True)\n\n# create dataframe of sea_level_pressure to fill\ntemporary_df = pd.DataFrame({'sea_level_pressure' : weather_train.sea_level_pressure})\n\n# update NA sea_level_pressure values\ntemporary_df.update(sea_level_filler, overwrite=False)\n\n# update in the weather train dataset\nweather_train[\"sea_level_pressure\"] = temporary_df[\"sea_level_pressure\"]\n\ndel sea_level_filler, temporary_df\ngc.collect()","b5dc6c44":"# create dataframe of daily means per site id\nwind_direction_filler = pd.DataFrame(weather_train\n                                     .groupby(['site_id','day_of_month','month'])\n                                     ['wind_direction'].mean(),\n                                     columns=['wind_direction'])\nwind_direction_filler.isna().sum()","d9ddae2a":"# create dataframe of wind_direction to fill\ntemporary_df = pd.DataFrame({'wind_direction' : weather_train.wind_direction})\n\n# update NA wind_direction values\ntemporary_df.update(wind_direction_filler, overwrite=False)\n\n# update in the weather train dataset\nweather_train[\"wind_direction\"] = temporary_df[\"wind_direction\"]\n\ndel temporary_df, wind_direction_filler\ngc.collect()","6587b906":"# create dataframe of daily means per site id\nwind_speed_filler = pd.DataFrame(weather_train\n                                 .groupby(['site_id','day_of_month','month'])\n                                 ['wind_speed'].mean(),\n                                 columns=['wind_speed'])\nwind_speed_filler.isna().sum()","9c016f60":"# create dataframe of wind_speed to fill\ntemporary_df = pd.DataFrame({'wind_speed' : weather_train.wind_speed})\n\n# update NA wind_speed values\ntemporary_df.update(wind_speed_filler, overwrite=False)\n\n# update in the weather train dataset\nweather_train[\"wind_speed\"] = temporary_df[\"wind_speed\"]\n\ndel temporary_df, wind_speed_filler\ngc.collect()","5ab8b94c":"# check if NA values left\nweather_train.isna().sum()","f4cfb9aa":"weather_train = weather_train.reset_index()","e4a1f0c0":"weather_train.drop('day_of_month',axis=1,inplace=True)\nweather_train.drop('month',axis=1,inplace=True)\nweather_train.drop('day_of_week',axis=1,inplace=True)\nweather_train.drop('hour',axis=1,inplace=True)\nweather_train.drop('is_weekend',axis=1,inplace=True)\nweather_train.drop('season',axis=1,inplace=True)","4ff8b68e":"weather_train.sample(5)","15b333cb":"# add month, day of week, day of month and hour \nweather_test['month'] = weather_test['timestamp'].dt.month.astype(np.int8)\nweather_test['day_of_week'] = weather_test['timestamp'].dt.dayofweek.astype(np.int8)\nweather_test['day_of_month']= weather_test['timestamp'].dt.day.astype(np.int8)\nweather_test['hour'] = weather_test['timestamp'].dt.hour\n\n# add is weekend column\nweather_test['is_weekend'] = weather_test.day_of_week.apply(lambda x: 1 if x>=5 else 0)\n\nweather_test['season'] = weather_test.month.apply(convert_season)\n\n#Reset Index for Update for training \nweather_test = weather_test.set_index(\n    ['site_id','day_of_month','month'])\n\n#Air temperature\n\n\n# create dataframe of daily means per site id \nair_temperature_filler = pd.DataFrame(weather_test\n                                      .groupby(['site_id','day_of_month','month'])\n                                      ['air_temperature'].mean(),\n                                      columns=[\"air_temperature\"])\nair_temperature_filler.isna().sum()\n\n# create dataframe of air_temperatures to fill\ntemporary_df = pd.DataFrame({'air_temperature' : weather_test.air_temperature})\n\n# update NA air_temperature values\ntemporary_df.update(air_temperature_filler, overwrite=False)\n\n# update in the weather train dataset\nweather_test[\"air_temperature\"] = temporary_df[\"air_temperature\"]\n\ndel temporary_df, air_temperature_filler\ngc.collect()\n\n#Cloud Coverage\n\n# create dataframe of daily means per site id\ncloud_coverage_filler = pd.DataFrame(weather_test\n                                     .groupby(['site_id','day_of_month','month'])\n                                     ['cloud_coverage'].mean(),\n                                     columns = ['cloud_coverage'])\ncloud_coverage_filler.isna().sum()\n\n#Because cloud_coverage takes discrete values and still have some NA value, I will fill it again with rounded mean\n\nround(cloud_coverage_filler.cloud_coverage.mean(),0)\ncloud_coverage_filler.fillna(round(cloud_coverage_filler.cloud_coverage.mean(),0), \n                             inplace=True)\n\n# create dataframe of cloud_coverages to fill\ntemporary_df = pd.DataFrame({'cloud_coverage' : weather_test.cloud_coverage})\n\n# update NA cloud_coverage values\ntemporary_df.update(cloud_coverage_filler, overwrite=False)\n\n# update in the weather train dataset\nweather_test[\"cloud_coverage\"] = temporary_df[\"cloud_coverage\"]\n\ndel temporary_df, cloud_coverage_filler\ngc.collect()\n\n#Dew Temperature\n\n# create dataframe of daily means per site id\ndew_temperature_filler = pd.DataFrame(weather_test\n                                      .groupby(['site_id','day_of_month','month'])\n                                      ['dew_temperature'].mean(),\n                                      columns=[\"dew_temperature\"])\ndew_temperature_filler.isna().sum()\n\n# create dataframe of dew_temperatures to fill\ntemporary_df = pd.DataFrame({'dew_temperature' : weather_test.dew_temperature})\n\n# update NA dew_temperature values\ntemporary_df.update(dew_temperature_filler, overwrite=False)\n\n# update in the weather train dataset\nweather_test[\"dew_temperature\"] = temporary_df[\"dew_temperature\"]\n\ndel temporary_df, dew_temperature_filler\ngc.collect()\n\n#Precip Depth 1 Hour\n\n# create dataframe of daily means per site id\nprecip_depth_filler = pd.DataFrame(weather_test\n                                   .groupby(['site_id','day_of_month','month'])\n                                   ['precip_depth_1_hr'].mean(),\n                                   columns=['precip_depth_1_hr'])\nprecip_depth_filler.isna().sum()\n\n#As cloud_coverage, I fill NA values of the filler with the rounded mean since the discrete values and still got some NA\n\n\nround(precip_depth_filler['precip_depth_1_hr'].mean(),0)\nprecip_depth_filler.fillna(round(precip_depth_filler['precip_depth_1_hr'].mean(),0)\n                           , inplace=True)\n\n# create dataframe of precip_depth_1_hr to fill\ntemporary_df = pd.DataFrame({'precip_depth_1_hr' : weather_test.precip_depth_1_hr})\n\n# update NA precip_depth_1_hr values\ntemporary_df.update(precip_depth_filler, overwrite=False)\n\n# update in the weather train dataset\nweather_test[\"precip_depth_1_hr\"] = temporary_df[\"precip_depth_1_hr\"]\n\ndel precip_depth_filler, temporary_df\ngc.collect()\n\n#Sea Level Pressure\n\n# create dataframe of daily means per site id\nsea_level_filler = pd.DataFrame(weather_test\n                                .groupby(['site_id','day_of_month','month'])\n                                ['sea_level_pressure'].mean(),\n                                columns=['sea_level_pressure'])\nsea_level_filler.isna().sum()\n\n#We did the same as with cloud_coverage:\n\nmean_sea_level_pressure = round(\n    sea_level_filler\n    ['sea_level_pressure']\n    .astype(float)\n    .mean(),2)\n\nsea_level_filler.fillna(mean_sea_level_pressure, inplace=True)\n\n# create dataframe of sea_level_pressure to fill\ntemporary_df = pd.DataFrame({'sea_level_pressure' : weather_test.sea_level_pressure})\n\n# update NA sea_level_pressure values\ntemporary_df.update(sea_level_filler, overwrite=False)\n\n# update in the weather train dataset\nweather_test[\"sea_level_pressure\"] = temporary_df[\"sea_level_pressure\"]\n\ndel sea_level_filler, temporary_df\ngc.collect()\n\n#Wind Direction\n\n# create dataframe of daily means per site id\nwind_direction_filler = pd.DataFrame(weather_test\n                                     .groupby(['site_id','day_of_month','month'])\n                                     ['wind_direction'].mean(),\n                                     columns=['wind_direction'])\nwind_direction_filler.isna().sum()\n\n# create dataframe of wind_direction to fill\ntemporary_df = pd.DataFrame({'wind_direction' : weather_test.wind_direction})\n\n# update NA wind_direction values\ntemporary_df.update(wind_direction_filler, overwrite=False)\n\n# update in the weather train dataset\nweather_test[\"wind_direction\"] = temporary_df[\"wind_direction\"]\n\ndel temporary_df, wind_direction_filler\ngc.collect()\n\n#Wind Speed\n\n\n# create dataframe of daily means per site id\nwind_speed_filler = pd.DataFrame(weather_test\n                                 .groupby(['site_id','day_of_month','month'])\n                                 ['wind_speed'].mean(),\n                                 columns=['wind_speed'])\nwind_speed_filler.isna().sum()\n\n# create dataframe of wind_speed to fill\ntemporary_df = pd.DataFrame({'wind_speed' : weather_test.wind_speed})\n\n# update NA wind_speed values\ntemporary_df.update(wind_speed_filler, overwrite=False)\n\n# update in the weather train dataset\nweather_test[\"wind_speed\"] = temporary_df[\"wind_speed\"]\n\ndel temporary_df, wind_speed_filler\ngc.collect()\n\n# check if NA values left\nweather_test.isna().sum()\n\n#We reset indexes to transfrom weather dataframe to original form:\n\nweather_test = weather_test.reset_index()\n\nweather_test.drop('day_of_month',axis=1,inplace=True)\nweather_test.drop('month',axis=1,inplace=True)\nweather_test.drop('day_of_week',axis=1,inplace=True)\nweather_test.drop('hour',axis=1,inplace=True)\nweather_test.drop('is_weekend',axis=1,inplace=True)\nweather_test.drop('season',axis=1,inplace=True)\n\nweather_test.sample(5)","512a9e6a":"train_total = pd.merge(train,building_meta,how='left',on='building_id')\ntrain_total = pd.merge(train_total,weather_train,how='left',on=[\"site_id\", \"timestamp\"])\ntrain_total.info()","12d6a686":"train_total.sample(5)","bb3dc773":"test_total = pd.merge(test,building_meta,how='left',on='building_id')\ntest_total = pd.merge(test_total,weather_test,how='left',on=[\"site_id\", \"timestamp\"])\n\ntest_total.info()","ad342113":"test_total.head()","ede58d52":"def feat_value_count(df,colname):\n    \"\"\"value count of each feature\n    \n    Args\n    df: data frame.\n    colname: string. Name of to be valued column\n    \n    Returns\n    df_count: data frame.\n    \"\"\"\n    df_count = df[colname].value_counts().to_frame().reset_index()\n    df_count = df_count.rename(columns={'index':colname+'_values',colname:'counts'})\n    return df_count\n\nfeat_value_count(train,'building_id')","eccebe53":"feat_value_count(test,'building_id')","40e7413b":"len(set(train.building_id) & set(test.building_id))","e033f569":"train_total['meter'].replace({0:\"Electricity\",1:\"ChilledWater\",2:\"Steam\",3:\"HotWater\"},inplace=True)\ntest_total['meter'].replace({0:\"Electricity\",1:\"ChilledWater\",2:\"Steam\",3:\"HotWater\"},inplace=True)","1318db5a":"feat_value_count(train_total,'meter')","23e0e0b5":"sns.countplot(train_total['meter'])\nplt.title(\"Distribution of Meter Id Code\")\nplt.xlabel(\"Meter Id Code\")\nplt.ylabel(\"Frequency\")","b7720a95":"feat_value_count(weather_train,'site_id')","aee4f241":"feat_value_count(building_meta,'primary_use')","86441490":"plt.figure(figsize=(8,6))\nbuilding_meta['primary_use'].value_counts().sort_values().plot(kind='bar')\nplt.title(\"Count of Primary_Use Variable in the Metadata table\")\nplt.xlabel(\"Primary Use\")\nplt.ylabel(\"Count\")\nplt.xticks(rotation=90)","b1f72d63":"building_meta['primary_use'].value_counts(normalize=True)","42b4fcc7":"building_meta['primary_use'].replace({\"Healthcare\":\"Other\",\"Parking\":\"Other\",\"Warehouse\/storage\":\"Other\",\"Manufacturing\/industrial\":\"Other\",\n                                \"Retail\":\"Other\",\"Services\":\"Other\",\"Technology\/science\":\"Other\",\"Food sales and service\":\"Other\",\n                                \"Utility\":\"Other\",\"Religious worship\":\"Other\"},inplace=True)","d3141829":"feat_value_count(building_meta,'site_id')","3d375575":"sns.countplot(building_meta['site_id'])\nplt.title(\"Count of Site_id in the Metadata table\")\nplt.xlabel(\"Site_Id\")\nplt.ylabel(\"Count\")","2d1dd2ae":"building_meta['square_feet'].describe()","11b363a7":"sns.distplot(building_meta['square_feet'])\nplt.title(\"Distribution of Square Feet variable of Metadata Table\")\nplt.xlabel(\"Area in Square Feet\")\nplt.ylabel(\"Frequency\")","858b528a":"building_meta['square_feet'] = np.log1p(building_meta['square_feet'])","f74c1908":"building_meta.groupby('primary_use')['square_feet'].agg(['mean','median','count']).sort_values(by='count')","2cd7a8a5":"building_meta['year_built'].value_counts().sort_values().plot(kind='bar',figsize=(15,6))\nplt.xlabel(\"Year Built\")\nplt.ylabel(\"Count\")\nplt.title(\"Distribution of Year Built Variable\")","14d14f2d":"building_meta.groupby('primary_use')['square_feet'].agg(['count','mean','median']).sort_values(by='count')","ce60485a":"cols = ['site_id','primary_use','building_id','year_built']\nfor col in cols:\n    print (\"Number of Unique Values in the {} column are:\".format(col),building_meta[col].nunique())","8c7ea501":"df_one_building = train_total[train_total.building_id == 1258]\ndf_one_building.head()","209b54c9":"sns.lineplot(x='timestamp',y='meter_reading',data=df_one_building[train_total.meter == 'Electricity']).set_title('electricity of building 1258')","a6af3498":"sns.lineplot(x='timestamp',y='meter_reading',data=df_one_building[train_total.meter == 'ChilledWater']).set_title('chilledwater of building 1258')","92b5057e":"sns.lineplot(x='timestamp',y='meter_reading',data=df_one_building[train_total.meter == 'Steam']).set_title('steam of building 1258')","570e91cb":"sns.lineplot(x='timestamp',y='meter_reading',data=df_one_building[train_total.meter == 'HotWater']).set_title('hotwater of building 1258')","76ea252a":"df_lots_building = train_total[train_total['building_id'].isin([1258,1298,1249])]\nmeasures = ['Electricity', 'ChilledWater', 'Steam', 'HotWater']\nfor i in measures:\n    f, ax = plt.subplots(figsize=(15, 6))\n    sns.lineplot(x='timestamp',y='meter_reading', hue = 'building_id',legend='brief',\n             data=df_lots_building[df_lots_building.meter == i]);\n#del df_lots_building\n#gc.collect()","fa7b6f60":"train_total.groupby('meter')['meter_reading'].agg(['min','max','mean','median','count','std'])","d5d4b157":"for df in [train_total, test_total]:\n    df['Month'] = df['timestamp'].dt.month.astype(\"uint8\")\n    df['DayOfMonth'] = df['timestamp'].dt.day.astype(\"uint8\")\n    df['DayOfWeek'] = df['timestamp'].dt.dayofweek.astype(\"uint8\")\n    df['Hour'] = df['timestamp'].dt.hour.astype(\"uint8\")","49eb1f56":"train_total.groupby(['meter','Month'])['meter_reading'].agg(['max','mean','median','count','std'])","65a61564":"train_total.groupby(['meter','DayOfWeek'])['meter_reading'].agg(['max','mean','median','count','std'])","aaedbaac":"train_total['meter_reading'].describe()","016000d3":"sns.distplot(np.log1p(train_total['meter_reading']),kde=False)\nplt.title(\"Distribution of Log of Meter Reading Variable\")","d0fb0fb7":"train_total['meter_reading'] = np.log1p(train_total['meter_reading'])\nsns.distplot(train_total[train_total['meter'] == \"Electricity\"]['meter_reading'],kde=False)\nplt.title(\"Distribution of Meter Reading per MeterID code: Electricity\")","49e5dab9":"sns.distplot(train_total[train_total['meter'] == \"ChilledWater\"]['meter_reading'],kde=False)\nplt.title(\"Distribution of Meter Reading per MeterID code: Chilledwater\")","ed399160":"sns.distplot(train_total[train_total['meter'] == \"Steam\"]['meter_reading'],kde=False)\nplt.title(\"Distribution of Meter Reading per MeterID code: Steam\")","840b8d22":"sns.distplot(train_total[train_total['meter'] == \"HotWater\"]['meter_reading'],kde=False)\nplt.title(\"Distribution of Meter Reading per MeterID code: Hotwater\")","1827d381":"idx_to_drop = list((train_total[(train_total['site_id'] == 0) & (train_total['timestamp'] < \"2016-05-21 00:00:00\")]).index)\ntrain_total.drop(idx_to_drop,axis='rows',inplace=True)","83baeffb":"def label_encoder(df, categorical_columns=None):\n    \"\"\"Encode categorical values as integers (0,1,2,3...) with pandas.factorize. \"\"\"\n    # if categorical_colunms are not given than treat object as categorical features\n    if not categorical_columns:\n        categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n    for col in categorical_columns:\n        df[col], uniques = pd.factorize(df[col])\n    return df, categorical_columns;","bdb6734a":"train_total,colname = label_encoder(train_total, categorical_columns=['primary_use'])\ntest_total,colname = label_encoder(test_total, categorical_columns=['primary_use']);","4ea72e97":"params = {'objective':'regression',\n          'boosting_type':'gbdt',\n          'metric':'rmse',\n          'learning_rate':0.1,\n          'num_leaves': 2**8,\n          'max_depth':-1,\n          'colsample_bytree':0.5,\n          'feature_fraction':0.7,\n          'subsample_freq':1,\n          'subsample':0.7,\n          'verbose':-1,\n          'num_threads':8,\n          'seed': 47,} ;\ncategory_cols = ['building_id', 'site_id', 'primary_use'];","8c4778d0":"#%% create feature: age\ntrain_total['age'] = train_total['year_built'].max() - train_total['year_built'] + 1\ntest_total['age'] = test_total['year_built'].max() - test_total['year_built'] + 1","673321f2":"le = LabelEncoder()\ntrain_total['primary_use'] = train_total['primary_use'].astype(str)\ntrain_total['primary_use'] = le.fit_transform(train_total['primary_use']).astype(np.int8)\n\ntest_total['primary_use'] = test_total['primary_use'].astype(str)\ntest_total['primary_use'] = le.fit_transform(test_total['primary_use']).astype(np.int8)","ffdde293":"#%% divide time into columns of month,weekofyear,dayofyear... and log squarefeet in both training and testing data\n\ntrain_total['month_datetime'] = train_total['timestamp'].dt.month.astype(np.int8)\ntrain_total['weekofyear_datetime'] = train_total['timestamp'].dt.weekofyear.astype(np.int8)\ntrain_total['dayofyear_datetime'] = train_total['timestamp'].dt.dayofyear.astype(np.int16)\n    \ntrain_total['hour_datetime'] =train_total['timestamp'].dt.hour.astype(np.int8)  \ntrain_total['day_week'] = train_total['timestamp'].dt.dayofweek.astype(np.int8)\ntrain_total['day_month_datetime'] = train_total['timestamp'].dt.day.astype(np.int8)\ntrain_total['week_month_datetime'] = train_total['timestamp'].dt.day\/7\ntrain_total['week_month_datetime'] = train_total['week_month_datetime'].apply(lambda x: math.ceil(x)).astype(np.int8)\n    \ntrain_total['year_built'] = train_total['year_built']-1900\ntrain_total['square_feet'] = np.log(train_total['square_feet'])","43ef2f05":"test_total['month_datetime'] = test_total['timestamp'].dt.month.astype(np.int8)\ntest_total['weekofyear_datetime'] = test_total['timestamp'].dt.weekofyear.astype(np.int8)\ntest_total['dayofyear_datetime'] = test_total['timestamp'].dt.dayofyear.astype(np.int16)\n    \ntest_total['hour_datetime'] = test_total['timestamp'].dt.hour.astype(np.int8)\ntest_total['day_week'] = test_total['timestamp'].dt.dayofweek.astype(np.int8)\ntest_total['day_month_datetime'] = test_total['timestamp'].dt.day.astype(np.int8)\ntest_total['week_month_datetime'] = test_total['timestamp'].dt.day\/7\ntest_total['week_month_datetime'] = test_total['week_month_datetime'].apply(lambda x: math.ceil(x)).astype(np.int8)\n    \ntest_total['year_built'] = test_total['year_built']-1900\ntest_total['square_feet'] = np.log(test_total['square_feet'])","b0f150a5":"train_total['meter']= le.fit_transform(train_total['meter']).astype(np.int8)\ntest_total['meter']= le.fit_transform(test_total['meter']).astype(np.int8)","89c49e82":"#%% drop columns\n\nfrom tqdm import tqdm\ndrop_cols = [ \"sea_level_pressure\", \"wind_speed\",\"timestamp\"]\ntarget = np.log1p(train_total[\"meter_reading\"])  \ntrain_df = train_total.drop(drop_cols, axis=1)\ncategoricals = [\"site_id\", \"building_id\", \"primary_use\",  \"meter\",  \"wind_direction\"]\nnumericals = [\"square_feet\", \"year_built\", \"air_temperature\", \"cloud_coverage\",\n              \"dew_temperature\", 'precip_depth_1_hr']\nfeat_cols = categoricals + numericals","81ae6289":"#%% modeling\nfrom sklearn.model_selection import KFold, StratifiedKFold\nparams = {\n            'boosting_type': 'gbdt',\n            'objective': 'regression',\n            'metric': {'rmse'},\n            'subsample_freq': 1,\n            'learning_rate': 0.3,\n            'bagging_freq': 5,\n            'num_leaves': 330,\n            'feature_fraction': 0.9,\n            'lambda_l1': 1,  \n            'lambda_l2': 1\n            }\n\nfolds = 5\nseed = 666\nshuffle = False\nkf = KFold(n_splits=folds, shuffle=shuffle, random_state=seed)\n \nmodels = []\nfor train_index, val_index in kf.split(train_total[feat_cols], train_total['building_id']):\n    train_X = train_total[feat_cols].iloc[train_index]\n    val_X = train_total[feat_cols].iloc[val_index]\n    train_y = target.iloc[train_index]\n    val_y = target.iloc[val_index]\n    lgb_train = lgb.Dataset(train_X, train_y, categorical_feature=categoricals)\n    lgb_eval = lgb.Dataset(val_X, val_y, categorical_feature=categoricals)\n    gbm = lgb.train(params,\n                lgb_train,\n                num_boost_round=500,\n                valid_sets=(lgb_train, lgb_eval),\n                early_stopping_rounds=50,\n                verbose_eval = 50)\n    models.append(gbm)","070c7627":"#%%see which variables are the most relevant\nfeature_imp = pd.DataFrame(sorted(zip(gbm.feature_importance(), gbm.feature_name()),reverse = True), columns=['Value','Feature'])\nplt.figure(figsize=(10, 5))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False))\nplt.title('LightGBM FEATURES')\nplt.tight_layout()\nplt.show();","f8b47f31":"test_total = test_total[feat_cols]\ni=0\nres=[]\nstep_size = 50000\nfor j in tqdm(range(int(np.ceil(test_total.shape[0]\/50000)))):\n    res.append(np.expm1(sum([model.predict(test_total.iloc[i:i+step_size]) for model in models])\/folds))\n    i+=step_size \n\n#%% remove the columns that cannot be calculated in the test data and rerun the last step. DataFrame.dtypes for data must be int, float or bool.\n\nres = np.concatenate(res)","d9ee3e7e":"#%% submission. all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 100000 and the array at index 416 has size 97600\nsample_submission = pd.read_csv(root + \"\/sample_submission.csv\")\nsample_submission['meter_reading'] = res\nsample_submission.loc[sample_submission['meter_reading']<0, 'meter_reading'] = 0\nsample_submission.to_csv('submission.csv', index=False)\n\n#%%\n#submission.shape","eb14666c":"os.chdir(r'\/kaggle\/working')\n#df_name.to_csv(r'df_name.csv')","9461764e":"from IPython.display import FileLink\nFileLink(r'submission.csv')","51ce35c4":"train_total.head()","2b7d2542":"### We fill out the missing values in the weather columns both in train and test\n\nAs we read so many popular tutorials and suggestion, there are few main solutions to fill in the missing values.\n1. Just ignore the data row \n2. Back-fill or forward-fill to propagate next or previous values respectively\n3. Replace with some constant value outside fixed value range-999,-1 etc.\n4. Replace with mean or median value. \n\n> Most of tutorials implement the way 'the daily mean of each site per month'. It had been proven the most logical and proven method so far.<br \/> \n> We will fill with the mean of the filler object created by grouping site_id, day and month if daily means per month \/ site_id is not available.\n--Ref2 and Ref3 <br \/>","38c78185":"Now we take a look of the size of the tables:","f71fddc9":"# Timestamps Adjustments","fa9bc101":"* Lot of 0 values as can be seen from the distribution","0bf13946":"## We merge test data (train, building, weather) into 1 dataframe:","d6d5ae3a":"As cloud_coverage, I fill NA values of the filler with the rounded mean since the discrete values and still got some NA","fdb28b82":"First We deal with wheather_train","0c568eb8":"There are 15 locations according to the weather data","176a5572":"**Before we start filling missing values, we drop floor_count column as it has more than 75% missing values.** \n> It will manipulate the training data into a different direction if we impute it.<br \/> \n> Also,year_built also has a large number of missing columns, but we leave it for now.","7112593f":"### Calculating Min_value and Max_value of the columns in Weather_train:","bfeb95d2":"Because cloud_coverage takes discrete values and still have some NA value, I will fill it again with rounded mean","89a6eaed":"GBM stands for **Gradient Boosting Machine**. It is an ensamble model of decision trees that works on reducing the residual errors. The most time-consuming part is to find the best split points. \n\nLightGBM contains the techniques of **Gradient-based One-Side Sampling** and **Exclusive Feature Bundling** to deal with large number of data instances and large number of features respectively.\n","9a726666":"**Precip Depth 1 Hour**","c0c32dd0":"# Dealing with missing values ","f333226a":"**Cloud Coverage**","353218f5":"* Cloud_Coverage takes distinct values unlike these other variables.\n* Dew Temperature looks like a Negatively skewed distribution.\n* Lot of 0 values in precip_depth_1_hr variable.\n* Distribution of sea_level_pressure looks like a normal distribution.\n* Wind_Speed distribution looks like positively skewed.","cf454005":"*For the building metadata we see:*\n* There are only 16 different primary uses and Education is the most frecuent with 549 apperances. \n* For the square feet the maximum is 875000 and the minimum is 283.\n* For the year built there is a range between 1900 till 2017. \n* For the floor count the average and median coincide in 3 floors; we see that the maximum floor count is 26.\n* And that there are some missing values for the year_built and floor_count columns.","04087d79":"We can see that Steam meter has some values that are very high maximum values, we have to explore further. \nMinimum value for all 4 types of meter is 0.","dd06258e":"We had quite a deal of missing values in the training and testing dataset, except site_id and time_stamp. ","964c36d1":"A total of 1449 buildings are in **test data**. Buildings 1258,1241,1331,1301... have the most records and building 0,666,667,668,669... have the least records. Many buildings have same amout of records. Maybe make sense to check the distribution.","6042a85c":"* Data contains records from 1st Jan to 31st Dec of 2016.\n* Data has information about 1448 buildings.\n* Data has 4 meter types.\n* Some extremely high values in meter reading which can be explored further.","6dff268a":"* Time period in **test data** is 2017 and 2018\n* We see that the test data points are a bit more than the double of the train data points...","3441b64c":"**Wind Direction**","085ff176":"* Out of 1449 buildings and 15 sites, site 3 has most buildings.","152e83e5":"# Looking for unique values","646a0712":"![Captura%20de%20Pantalla%202020-01-20%20a%20la%28s%29%2019.03.53.png](attachment:Captura%20de%20Pantalla%202020-01-20%20a%20la%28s%29%2019.03.53.png)","a350237c":"**Wind Speed**","cb105b38":"* We see that almost all columns have missing values on the weather's datasets.\n* For the columns cloud_coverage and precip_depth_1_hr we see a lot of values = 0.\n\nWe divide the number of missing values by the overall number of values to compare the number of missing values in Weather_Train and Weather_Test:","b04701be":"### Meter_reading is grouped by meter and month. Max, mean, median, count and std are calculated:","27052368":"# Scores results\n[![Captura%20de%20Pantalla%202020-01-20%20a%20la%28s%29%2017.32.21.png](attachment:Captura%20de%20Pantalla%202020-01-20%20a%20la%28s%29%2017.32.21.png)](http:\/\/)","6958141a":"# Lessons learned and outlook\n**IN GENERAL:**\n1. We found \u2018modin\u2019 library can help accelerate pandas even on the laptop and so we tried to install it in windows. We learned that:\n    * Pip install \u201cmodin[dask]\u201d is ran in the command line, therefore, the code should be put in the command in terminal instead of interpreter.\n    * If still get error when using modin to read data, it\u2019s because Windows doesn\u2019t support Ray which is the dependency of modin. To use it, we have to install WSL. But it would easier for laptops using Linux or Mac.\n    * We didn\u2019t use it in the end because we don\u2019t want to install more applications.\n2.\tDataFrame.dtypes for data must be int, float or bool.\n3.\tDifference between statistics modeling (e.g. linear regression) and machine learning:\n    * Statistical modeling is more about finding relationships between variables and the significance of those relationships. Test data is not necessary but we analyze confidential intervals, p value, test value to access the model\u2019s accuracy.\n    * Machine learning is more about prediction results, we don\u2019t care much if the model is interpretable. Test data is normally needed to validate results\u2019 accuracy.\n4.\tEven though in machine learning, we don\u2019t care much about the independency\/collinearity problems, but we still should ensure that the training data is as clean as possible, therefore, we should still drop the useless columns, which also reduce the data size and increase the speed.\n5. The cells took to much time to execute, this was very annoying. It felt like a waste of time.\n6. It was initially difficult to understand the different data sets and the connections between them. It took us some time to get the hang of it.\n7. We encountered many unknown methods, so we had to research what each method does in order to use it properly.\n\n**FOR THE MISSING VALUES:**\n\nAfter tried several different solutions without efficiency, we saw a tutorial indicating a really simple but useful action before start looking for proper ML. This tutorial mentioned that it's better **to calculate a ''common-sense'' baseline**. This baseline is defined in how a person who has knowledge in that field would solve the problem without using any data science tricks. Alternatively, it can be a dummy or simple algorithm, consisting of few lines of code, to use as a baseline metric.\nBaseline metrics can be different in regression and classification problems. For a regression problem, it can be a central tendency measure as the result for all predictions, such as the mean or the median. Since this is a regression problem and competition's results will be evaluated for root mean squared logarithmic error. Baseline metrics are important in a way that, if a ML model cannot beat the simple and intuitive prediction of a person's or an algorithm's guess, the original problem needs reconsideration or training data needs reframing.\ne.g.The baseline guess is a score of 4.38\nBaseline Performance on the valid set: RMSE = 2.1070\nHowever, when we try linear regression, RMSE of the linear regression model is: 1.9895562449483846 without too much difference. Then why we are going to use such ML for prediction? By doing so, it's really easy to do the benchmark even if we don't start with any complicated statistics.\n","f515f0b7":"* There are 15 primary use of buildings. \n* Most buildings are for education, the least is for religious worship. \n* Education, Office, Entertainment\/Public Assembly, Public Services, Lodging\/Residential form the bulk of Primary Use. \n* Education and office occupies 57% of all buildings.","01e9e642":"> There is some discrepancy in the meter_readings for different ste_id's and buildings. It makes sense to delete them. Ref1","97c8f362":"We reduce the memory size. \nFunction to reduce the DF size:","6643897b":"First, we do for the **building part**:","86334d29":"Then we set a parameter:","c4ba8f5b":"We did the same as with cloud_coverage:","6e64e14d":"## Previous Code\n\nfor df in [train, test]:\n    df['air_temperature'] = df['air_temperature'].astype('float16')\n    df['cloud_coverage'] = df['cloud_coverage'].astype(\"float16\")\n    df['dew_temperature'] = df['dew_temperature'].astype('float16')\n    df['precip_depth_1_hr'] = df['precip_depth_1_hr'].astype('float32')\n    df['sea_level_pressure'] = df['sea_level_pressure'].astype('float32')\n    df['wind_direction'] = df['wind_direction'].astype('float32')\n    df['wind_speed'] = df['wind_speed'].astype('float16')\n    df['square_feet'] = df['square_feet'].astype(\"float32\")\n    df['building_id'] = df['building_id'].astype(\"int16\")\n    \ntrain.drop('timestamp',axis=1,inplace=True)\ntest.drop('timestamp',axis=1,inplace=True)\n\ntrain['number_unique_meter_per_building']\ntrain['mean_meter_reading_per_building']\ntrain['median_meter_reading_per_building']\ntrain['std_meter_reading_per_building']\n\ntrain['mean_meter_reading_on_year_built']\ntrain['median_meter_reading_on_year_built']\ntrain['std_meter_reading_on_year_built']\n\ntrain['mean_meter_reading_per_meter']\ntrain['median_meter_reading_per_meter']\ntrain['std_meter_reading_per_meter']\n\ntrain['mean_meter_reading_per_primary_usage']\ntrain['median_meter_reading_per_primary_usage']\ntrain['std_meter_reading_per_primary_usage']\n\ntrain['mean_meter_reading_per_site_id']\ntrain['median_meter_reading_per_site_id']\ntrain['std_meter_reading_per_site_id']\n\nThen again but with the test dataset.\n\nle = LabelEncoder()\n\ntrain['meter']= le.fit_transform(train['meter']).astype(\"uint8\")\ntest['meter']= le.fit_transform(test['meter']).astype(\"uint8\")\ntrain['primary_use']= le.fit_transform(train['primary_use']).astype(\"uint8\")\ntest['primary_use']= le.fit_transform(test['primary_use']).astype(\"uint8\")\n\nCheck the correlation between the variables and eliminate the one's that have high correlation.Threshold for removing correlated variables:\nthreshold = 0.9\n\nAbsolute value correlation matrix\ncorr_matrix = train.corr().abs()\ncorr_matrix.head()\n\nUpper triangle of correlations\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\nupper.head()\n\nSelect columns with correlations above threshold\nto_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n\nprint('There are %d columns to remove.' % (len(to_drop)))\nprint (\"Following columns can be dropped {}\".format(to_drop))\n\ntrain.drop(to_drop,axis=1,inplace=True)\ntest.drop(to_drop,axis=1,inplace=True)\n\ny = train['meter_reading']\ntrain.drop('meter_reading',axis=1,inplace=True)\n\ncategorical_cols = ['building_id','Month','meter','Hour','primary_use','DayOfWeek','DayOfMonth']\n\n%%time\nx_train,x_test,y_train,y_test = train_test_split(train,y,test_size=0.25,random_state=42)\nprint (x_train.shape)\nprint (y_train.shape)\nprint (x_test.shape)\nprint (y_test.shape)\n\nlgb_train = lgb.Dataset(x_train, y_train,categorical_feature=categorical_cols)\nlgb_test = lgb.Dataset(x_test, y_test,categorical_feature=categorical_cols)\ndel x_train, x_test , y_train, y_test\n\nparams = {'feature_fraction': 0.75,\n          'bagging_fraction': 0.75,\n          'objective': 'regression',\n          'max_depth': -1,\n          'learning_rate': 0.15,\n          \"boosting_type\": \"gbdt\",\n          \"bagging_seed\": 11,\n          \"metric\": 'rmse',\n          \"verbosity\": -1,\n          'reg_alpha': 0.5,\n          'reg_lambda': 0.5,\n          'random_state': 47\n         }\n\nreg = lgb.train(params, lgb_train, num_boost_round=3000, valid_sets=[lgb_train, lgb_test], early_stopping_rounds=100, verbose_eval = 100)\n\ndel lgb_train,lgb_test\n\nser = pd.DataFrame(reg.feature_importance(),train.columns,columns=['Importance']).sort_values(by='Importance')\nser['Importance'].plot(kind='bar',figsize=(10,6))\n\ndel train\n\n%%time\npredictions = []\nstep = 50000\nfor i in range(0, len(test), step):\n    predictions.extend(np.expm1(reg.predict(test.iloc[i: min(i+step, len(test)), :], num_iteration=reg.best_iteration)))\n    \n%%time\nSubmission['meter_reading'] = predictions\nSubmission['meter_reading'].clip(lower=0,upper=None,inplace=True)\nSubmission.to_csv(\"Twentysix.csv\",index=None)","ba430681":"# Method Kfold and LightGBM","3afaac76":"We use the function **set()** and the command **&** so that we identify the intersection between the train and the test dataframes. We need to predict all 1449 building meter readings. All buildings that need to be predicted appear in train data.","392617d2":"* Average meter_reading of Steam type of meter is higher as compared to the other meter types.","e0da1e12":"### Converting the dependent variable to logarithmic scale:","5adf000b":"With a Chilled water lineplot we see that **in summer**, more chilled water is used.","d45ed986":"Looks like a normal distribution distribution.","9dc9e3bf":"No Missing values in train\/test datasets","45a3714b":"> This data is from 31st Dec 2015 to 31st Dec 2016, similar to the timestamp of the training data","05261a91":"In summmer there is very low consumption of hot water, but in winter very high consumption.","673eb897":"### Meter_reading is grouped by meter and dayofweek. Max, mean, median, count and std are calculated:","60d530c5":"> The time duration is similar to the test dataset.","cb33773f":"In winter, more steam is used for heating.","778b9b2d":"> Since there are a lot of categories which are a minor percentage of the whole, it makes sense to combine them.\n\nWe therefore combine all categories with minor percentage:","33a4116d":"Then we upload the dataframes. \nWe parse the datetime data and use specific dtypes for the building and weather dataframe. \nAlso for the test dataframe we select to read especific columns.","e6698d9c":"* We calculate ranks of hourly temperatures within date\/site_id chunks.\n* Then create a dataframe of site_ids (0-16) x mean hour rank of temperature within day (0-23).\n* And we subtract the columnID of temperature peak by 14, getting the timestamp alignment gap.\n* Finally we do a function to align the timestamps.","5c56bbff":"### Now we take a single building 1258 to analyze the meter variable (our target):","97c6dea0":"## We merge train data (train, building, weather) into 1 dataframe:","7cf2c7f8":"**We created distplot of Distribution of Square Feet variable of Metadata Table**","91d03e25":"# Team ASYC notebook","fb44f7c6":"We obtain the mean, median, count of grouped by columns primary_use, square_feet:\n* Others (Parking) has the highest average although the count is less.\n* Education has the highest count as can be seen in the countplot above.","7da500f5":"**Air temperature**","719c0fad":" Repeat again for wheather_test","7d49ed1f":"We check for missing values on the test and train dataframes:","b899c4df":"* Not every building has all meter types. Electricity has the most records.","f4b1c5c8":"We divide the number of missing values by the overall number of values to see which columns have missing values:","884dc7c8":"# References\n\nRef1: **As per the discussion in the following thread, https:\/\/www.kaggle.com\/c\/ashrae-energy-prediction\/discussion\/117083, there is some discrepancy in the meter_readings for different ste_id's and buildings. It makes sense to delete them**\n\nRef2: https:\/\/www.kaggle.com\/aitude\/ashrae-kfold-lightgbm-without-leak-1-08\n\nRef3: https:\/\/www.kaggle.com\/cereniyim\/save-the-energy-for-the-future-2-fe-lightgbm\n\nRef4: https:\/\/www.kaggle.com\/nz0722\/aligned-timestamp-lgbm-by-meter-type\n\nOthers:\n\nhttps:\/\/www.kaggle.com\/migglu\/odyssey-towards-a-sustainable-built-environment#Modeling-and-prediction\nRandom forest: https:\/\/www.kaggle.com\/holoong9291\/ashrae-great-energy-predict\nhttps:\/\/www.kaggle.com\/aldrinl\/eda-rf-ashrae-great-energy-predictor\nLightgbm https:\/\/www.kaggle.com\/kaushal2896\/ashrae-eda-fe-lightgbm-1-12\nLightgbm: https:\/\/www.kaggle.com\/isaienkov\/lightgbm-fe-1-19\nKfold lightgbm: https:\/\/www.kaggle.com\/aitude\/ashrae-kfold-lightgbm-without-leak-1-08\nStratified lightgb,: https:\/\/www.kaggle.com\/roydatascience\/ashrae-energy-prediction-using-stratified-kfold\nhttps:\/\/www.kaggle.com\/cereniyim\/save-the-energy-for-the-future-3-predictions\nhttps:\/\/www.kaggle.com\/cereniyim\/save-the-energy-for-the-future-2-fe-lightgbm#-4.-Compare-Several-Machine-Learning-Models-\n\nhttps:\/\/www.kaggle.com\/jaseziv83\/a-deep-dive-eda-into-all-variables\nhttps:\/\/www.kaggle.com\/ishaan45\/pandas-profiling-missing-value-imputation\nhttps:\/\/www.kaggle.com\/caesarlupum\/ashrae-start-here-a-gentle-introduction#14.-Handling-missing-values\nhttps:\/\/www.kaggle.com\/kaushal2896\/ashrae-eda-fe-lightgbm-1-12\nhttps:\/\/www.kaggle.com\/aitude\/ashrae-missing-weather-data-handling\nhttps:\/\/www.kaggle.com\/vikassingh1996\/ashrae-great-energy-insightful-eda-fe-lgbm\nhttps:\/\/www.kaggle.com\/cereniyim\/save-the-energy-for-the-future-2-fe-lightgbm\nhttps:\/\/www.kaggle.com\/drcapa\/ashrae-feature-engineering-merge-data\n\n\nhttps:\/\/www.youtube.com\/watch?v=ErDgauqnTHk - Gopal Prasad Malakar\nhttps:\/\/www.youtube.com\/watch?v=l3OmtvcaTmM&list=PL1UM2yYgxPh-2kKL53wCsBQP5MoCBvH1z&index=5 Ligdi Gonzalez\nhttps:\/\/www.youtube.com\/watch?v=F7xj8H_p288&list=PL1UM2yYgxPh-2kKL53wCsBQP5MoCBvH1z&index=6 - Ligdi Gonzalez\nhttps:\/\/www.microsoft.com\/en-us\/research\/wp-content\/uploads\/2017\/11\/lightgbm.pdf","4704b031":"With a Electricity lineplot we see that **in spring**, less electricity is used.","595d2e62":"A total of 1449 buildings are in **train data**. Building 1298,1249 has the most records and building 403 has the least records.","2ca7081a":"We reset indexes to transfrom weather dataframe to original form:","1fa9225e":"* **precip_depth_1_hr variable** and **cloud_coverage variable** have similar number of missing values in both train and test weather data.\n* site_id and timestamp do not have missing values.\n* Other variables have some missing values.","f271f581":"**Reducing memory:**","a6a3b81b":"*First we installed the necessary libraries:*","8f5e963b":"**Dew Temperature**","0c1a2286":"* We can see that only Steam meter has very high meter_reading values as compared to other types of meters.\n* We can see that the average electricity meter_reading does not vary much across the months.\n* Average Hot Water meter_reading is relatively less from April to October Months.\n* Average Steam meter_reading is way higher from March to June as compared to the other months.","c3c58a22":"### Label encoding: dealing with categorical variables.","36c0296d":"# Exploratory Data Analysis\n## Looking for missing values\n\nWe see the basic statistical measures of the dataframes:","48f850c5":"**We create a distplot Distribution of Log of Meter Reading Variable:**","7531c8ad":"**Sea Level Pressure**","522370b4":"In Multiple buildings the patterns of energy consumption is quite consistent. Building 1258 has the highest consumption.","9f36b988":"We review the dataframes:"}}