{"cell_type":{"21a6a9ed":"code","11abbe18":"code","c62a453c":"code","4a18cd68":"code","1f5b369a":"code","e735c07e":"code","34ef6a87":"code","a961514a":"code","eec676dd":"code","9e537c24":"code","216d1dc7":"code","f59f0951":"code","6bd4428f":"code","913bd5f0":"code","e0b17024":"code","c56802f2":"code","5f283877":"code","cf824672":"code","516a1d0e":"code","613b2619":"code","f603b744":"code","14947638":"code","033c0329":"code","3d0abba9":"code","d9f9daad":"markdown","6a83375c":"markdown","304a6c3c":"markdown","4fbc1e12":"markdown","25e1f570":"markdown","89ec5a7b":"markdown","2324e42e":"markdown","7c1eaf2b":"markdown","25d30129":"markdown","da667bb1":"markdown","7735c0c7":"markdown","3f1e3834":"markdown","d1afebf3":"markdown","bcbbeec4":"markdown","8f35e499":"markdown","f2accee3":"markdown","5719e1e7":"markdown","bfe55853":"markdown","8988892d":"markdown","43f40b5e":"markdown","b520ca60":"markdown","1a4b8f01":"markdown","ac2ed237":"markdown","0e598bfb":"markdown","dea20c6c":"markdown","0a93f01f":"markdown"},"source":{"21a6a9ed":"import re\nimport os\nimport numpy as np \nimport pandas as pd\nimport networkx as nx\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n\nfrom Bio import SeqIO\nfrom io import StringIO\nfrom itertools import product\n\nfrom numpy import linalg as LA\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import train_test_split\n\nfrom tensorflow import keras\n\nfrom tensorflow.keras.utils import Sequence\nfrom tensorflow.keras import backend as K \nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.layers import Input, Activation, Dense,Add,concatenate,LeakyReLU\nfrom tensorflow.keras.layers import Conv2D,Conv2DTranspose, Layer,GlobalAveragePooling2D\nfrom tensorflow.keras.layers import Flatten, Reshape, BatchNormalization,GlobalMaxPooling2D\n\nglobalSeed=768\n\nfrom numpy.random import seed \nseed(globalSeed)\n\ntf.compat.v1.set_random_seed(globalSeed)","11abbe18":"#Wrapper function to load the sequences. \ndef GetSeqs(Dir):\n    \n    cDir=Dir\n    \n    with open(cDir) as file:\n        \n        seqData=file.read()\n        \n    Seq=StringIO(seqData)\n    SeqList=list(SeqIO.parse(Seq,'fasta'))\n    \n    return SeqList\n\ndef SplitString(String,ChunkSize):\n    '''\n    Split a string ChunkSize fragments using a sliding windiow\n\n    Parameters\n    ----------\n    String : string\n        String to be splitted.\n    ChunkSize : int\n        Size of the fragment taken from the string .\n\n    Returns\n    -------\n    Splitted : list\n        Fragments of the string.\n\n    '''\n    try:\n        localString=str(String.seq)\n    except AttributeError:\n        localString=str(String)\n      \n    if ChunkSize==1:\n        Splitted=[val for val in localString]\n    \n    else:\n        nCharacters=len(String)\n        Splitted=[localString[k:k+ChunkSize] for k in range(nCharacters-ChunkSize)]\n        \n    return Splitted\n\ndef UniqueToDictionary(UniqueElements):\n    '''\n    Creates a dictionary that takes a Unique element as key and return its \n    position in the UniqueElements array\n    Parameters\n    ----------\n    UniqueElements : List,array\n        list of unique elements.\n\n    Returns\n    -------\n    localDictionary : dictionary\n        Maps element to location.\n\n    '''\n    \n    localDictionary={}\n    nElements=len(UniqueElements)\n    \n    for k in range(nElements):\n        localDictionary[UniqueElements[k]]=k\n        \n    return localDictionary\n\ndef CountUniqueElements(UniqueElements,String,Processed=False):\n    '''\n    Calculates the frequency of the unique elements in a splited or \n    processed string. Returns a list with the frequency of the \n    unique elements. \n    \n    Parameters\n    ----------\n    UniqueElements : array,list\n        Elements to be analized.\n    String : strting\n        Sequence data.\n    Processed : bool, optional\n        Controls if the sring is already splitted or not. The default is False.\n    Returns\n    -------\n    localCounter : array\n        Normalized frequency of each unique fragment.\n    '''\n    \n    nUnique=len(UniqueElements)\n    localCounter=[0 for k in range(nUnique)]\n    \n    if Processed:\n        ProcessedString=String\n    else:\n        ProcessedString=SplitString(String,len(UniqueElements[0]))\n        \n    nSeq=len(ProcessedString)\n    UniqueDictionary=UniqueToDictionary(UniqueElements)\n    \n    for val in ProcessedString:\n        if val in UniqueElements:\n            localPosition=UniqueDictionary[val]\n            localCounter[localPosition]=localCounter[localPosition]+1      \n            \n    localCounter=[val\/nSeq for val in localCounter]\n    \n    return localCounter\n\ndef MakeSequenceGraph(Sequence,NodeNames,scheme='A',viz=False):\n    '''\n    Creates a graph from a sequence \n    \n    Parameters\n    ----------\n    Sequence : string, sequence object\n        Sequence to create the graph.\n    NodeNames : array\n        Array with the node names or k-mer for each noe.\n    scheme : string, optional default is A\n        Controls the connectivity scheme\n    viz : bool, optional default is False\n        Controls if the graph is multigraph for analysis or \n        a simple graph for visualization\n    Returns\n    -------\n    localGraph : networkx graph object\n        Sequence graph.\n    '''\n    \n    Nodes=np.arange(len(NodeNames))\n    localDict=UniqueToDictionary(NodeNames)\n    \n    if viz:\n        localGraph=nx.Graph()\n    else:    \n        localGraph=nx.MultiGraph()\n        \n    localGraph.add_nodes_from(Nodes)\n    \n    if scheme == 'A':\n        \n        fragmentSize=len(NodeNames[0])\n        processedSequence=SplitString(Sequence,fragmentSize)\n        \n        for k in range(len(processedSequence)-1):\n            \n            if processedSequence[k] in NodeNames and processedSequence[k+1] in NodeNames:\n            \n                current=localDict[processedSequence[k]]\n                forward=localDict[processedSequence[k+1]]\n                localGraph.add_edge(current,forward)\n    \n    elif scheme == 'B':\n        \n        fragmentSize=2*len(NodeNames[0])\n        processedSequence=SplitString(Sequence,fragmentSize)\n        \n        for frag in processedSequence:\n            \n            backFragment = frag[0:int(fragmentSize\/2)] \n            forwardFragment = frag[int(fragmentSize\/2)::]\n            \n            if backFragment in NodeNames and forwardFragment in NodeNames:\n                current = localDict[backFragment]\n                forward = localDict[forwardFragment]\n                localGraph.add_edge(current,forward)\n    \n    return localGraph\n\n#Wrapper function to calculate the normalized adjacency matrix \ndef MakeNormAdjacencyMatrix(graph):\n    \n    matrixShape = (len(graph.nodes),len(graph.nodes))\n    D12 = np.zeros(matrixShape)\n    \n    for (node, val) in graph.degree():\n        D12[node,node] = 1\/np.sqrt(val)\n        \n    A = nx.adjacency_matrix(graph).toarray()    \n    normA = np.dot(D12,A).dot(D12)\n    \n    w,v = LA.eig(normA)\n        \n    return normA\/LA.norm(w)\n\nAlphabet = ['A','C','T','G']\nBlocks = []\n\nmaxSize = 4\nfor k in range(1,maxSize):\n    \n    Blocks.append([''.join(i) for i in product(Alphabet, repeat = k)])\n\n#Wrapper function to format the matrix data \ndef MakeSequenceMatrix(sequence,blocks=Blocks):\n    \n    container = []\n    mat = np.zeros((64,80))\n    \n    for blk in blocks:\n        graphA = MakeSequenceGraph(sequence,blk)\n        graphB = MakeSequenceGraph(sequence,blk,scheme='B')\n    \n        a = MakeNormAdjacencyMatrix(graphA)\n        b = MakeNormAdjacencyMatrix(graphB)\n    \n        c = a-b\n        c = (c-c.min())\/(c.max()-c.min())\n        container.append(c)\n    \n    mat[0:4,0:4] = container[0]\n    mat[4:20,0:16] = container[1]\n    mat[0:64,16:80] = container[2]\n    \n    return mat\n\ndef GetGridShape(TotalNumberOfElements):\n    \"\"\"\n    Parameters\n    ----------\n     TotalNumberOfElements : int\n        Total number of elements in the plot.\n\n    Returns\n    -------\n    nrows : int\n        number of rows in the plot.\n    ncolumns : int\n        number of columns in the plot.\n\n    \"\"\"\n    numberOfUnique=TotalNumberOfElements\n    squaredUnique=int(np.sqrt(numberOfUnique))\n    \n    if squaredUnique*squaredUnique==numberOfUnique:\n        nrows,ncolumns=squaredUnique,squaredUnique\n    elif squaredUnique*(squaredUnique+1)<numberOfUnique:\n        nrows,ncolumns=squaredUnique+1,squaredUnique+1\n    else:\n        nrows,ncolumns=squaredUnique,squaredUnique+1\n    \n    return nrows,ncolumns","c62a453c":"Sequences = GetSeqs('..\/input\/covid19-sequences\/sequences.fasta')","4a18cd68":"Mer01Graph = MakeSequenceGraph(Sequences[0],Blocks[0],viz=True)\npos01 = nx.circular_layout(Mer01Graph)\n\nMer02Graph = MakeSequenceGraph(Sequences[0],Blocks[1],viz=True)\npos02 = nx.circular_layout(Mer02Graph)\n\nMer03Graph = MakeSequenceGraph(Sequences[0],Blocks[2],viz=True)\npos03 = nx.circular_layout(Mer03Graph)\n\nMer01GraphB = MakeSequenceGraph(Sequences[0],Blocks[0],scheme='B',viz=True)\npos01B = nx.circular_layout(Mer01Graph)\n\nMer02GraphB = MakeSequenceGraph(Sequences[0],Blocks[1],scheme='B',viz=True)\npos02B = nx.circular_layout(Mer02Graph)\n\nMer03GraphB = MakeSequenceGraph(Sequences[0],Blocks[2],scheme='B',viz=True)\npos03B = nx.circular_layout(Mer03Graph)\n\nf, axs = plt.subplots(2,3,figsize=(15,10))\n\nnx.draw_networkx(Mer01Graph, pos=pos01, ax=axs[0,0],alpha=0.5)\nnx.draw_networkx(Mer02Graph, pos=pos02, ax=axs[0,1],alpha=0.5)\nnx.draw_networkx(Mer03Graph, pos=pos03, ax=axs[0,2],alpha=0.15)\n\nnx.draw_networkx(Mer01GraphB, pos=pos01B, ax=axs[1,0],alpha=0.5)\nnx.draw_networkx(Mer02GraphB, pos=pos02B, ax=axs[1,1],alpha=0.5)\nnx.draw_networkx(Mer03GraphB, pos=pos03B, ax=axs[1,2],alpha=0.15)","1f5b369a":"plt.figure(figsize=(12,6))\nplt.imshow(MakeSequenceMatrix(Sequences[0]))","e735c07e":"MetaData = pd.read_csv(r'..\/input\/covid19-metadata\/SARSCov2Metadata.csv')\nMetaData.drop(['PCA_A','PCA_B','VAE_A','VAE_B','ConvVAE_A','ConvVAE_B',],axis=1,inplace=True)\nMetaData.fillna(0,inplace=True)\nUSData = MetaData[MetaData['SimplifiedGEO']==\"USA\"]\n\nEastCoastDataA = USData[USData[\"geo_long\"]>-85]\nNorthEastCoast = EastCoastDataA[EastCoastDataA[\"geo_lat\"]>35]\n\nplt.figure(figsize=(12,7))\nplt.hist(NorthEastCoast['outbreaktime'],bins=75,density=True)\nplt.xlabel('Time')","34ef6a87":"class KLDivergenceLayer(Layer):\n    '''\n    Custom KL loss layer\n    '''\n    def __init__(self,*args,**kwargs):\n        self.annealing = tf.Variable(0.,dtype=tf.float32,trainable = False)\n        self.is_placeholder=True\n        super(KLDivergenceLayer,self).__init__(*args,**kwargs)\n        \n    def call(self,inputs):\n        \n        Mu,LogSigma=inputs\n        klbatch=-0.5*self.annealing*K.sum(1+LogSigma-K.square(Mu)-K.exp(LogSigma),axis=-1)\n        self.add_loss(K.mean(klbatch),inputs=inputs)\n        self.add_metric(klbatch,name='kl_loss',aggregation='mean')\n        \n        return inputs\n\nclass Sampling(Layer):\n    '''\n    Custom sampling layer\n    '''\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        \n    def get_config(self):\n        config = {}\n        base_config = super().get_config()\n        return {**base_config, **config}\n    \n    @tf.autograph.experimental.do_not_convert   \n    def call(self,inputs,**kwargs):\n        \n        Mu,LogSigma=inputs\n        batch=tf.shape(Mu)[0]\n        dim=tf.shape(Mu)[1]\n        epsilon=K.random_normal(shape=(batch,dim))\n\n        return Mu+(K.exp(0.5*LogSigma))*epsilon\n\nclass SpatialAttention(Layer):\n    '''\n    Custom Spatial attention layer\n    '''\n    \n    def __init__(self,size, **kwargs):\n        super(SpatialAttention, self).__init__()\n        self.size = size\n        self.kwargs = kwargs\n\n    def build(self, input_shapes):\n        self.conv = Conv2D(filters=1, kernel_size=self.size, strides=1, padding='same')\n\n    def call(self, inputs):\n        pooled_channels = tf.concat(\n            [tf.math.reduce_max(inputs, axis=3, keepdims=True),\n            tf.math.reduce_mean(inputs, axis=3, keepdims=True)],\n            axis=3)\n\n        scale = self.conv(pooled_channels)\n        scale = tf.math.sigmoid(scale)\n\n        return inputs * scale\n\nclass ChannelAttention(Layer):\n    \n    def __init__(self,**kwargs):\n        super(ChannelAttention,self).__init__()\n        self.kwargs = kwargs\n    \n    def get_config(self):\n        config = super(ChannelAttention,self).get_config().copy()\n        config.update({'ratio':self.ratio})\n        return config\n    \n    def build(self,input_shape):\n        channel = input_shape[-1]\n        self.dense0 = Dense(channel)\n        self.dense1 = Dense(channel)\n    \n    def call(self,inputs):\n        \n        channel = inputs.get_shape().as_list()[-1]\n        \n        avgpool = GlobalAveragePooling2D()(inputs)\n        avgpool = Reshape((1,1,channel))(avgpool)\n        avgpool = self.dense0(avgpool)\n        avgpool = self.dense1(avgpool)\n        \n        maxpool = GlobalMaxPooling2D()(inputs)\n        maxpool = Reshape((1,1,channel))(maxpool)\n        maxpool = self.dense0(maxpool)\n        maxpool = self.dense1(maxpool)\n        \n        feature = Add()([avgpool,maxpool])\n        feature = Activation('sigmoid')(feature)\n        \n        return inputs*feature","a961514a":"#Wrapper function, creates a small Functional keras model \n#Bottleneck of the variational autoencoder \ndef MakeVariationalNetwork(Latent):\n    \n    InputFunction=Input(shape=(Latent,))\n    Mu=Dense(Latent)(InputFunction)\n    LogSigma=Dense(Latent)(InputFunction)\n    Mu,LogSigma=KLDivergenceLayer(name='KLDivergence')([Mu,LogSigma])\n    Output=Sampling()([Mu,LogSigma])\n    variationalBottleneck=Model(inputs=InputFunction,outputs=Output)\n    \n    return InputFunction,variationalBottleneck\n\ndef MakeBottleneck(InputShape,Latent,UpSampling=False):\n    '''\n    Parameters\n    ----------\n    InputShape : tuple\n        input shape of the previous convolutional layer.\n    Latent : int\n        Dimentionality of the latent space.\n    UpSampling : bool, optional\n        Controls the sampling behaviour of the network.\n        The default is False.\n\n    Returns\n    -------\n    InputFunction : Keras functional model input\n        input of the network.\n    localCoder : Keras functional model\n        Coder model, transition layer of the bottleneck.\n\n    '''\n    \n    productUnits = np.product(InputShape)\n    Units = [productUnits,productUnits\/\/4,productUnits\/\/16,Latent]\n    \n    if UpSampling:\n        finalUnits = Units[::-1]\n        InputFunction = Input(shape=(Latent,))\n        X = Dense(finalUnits[0],use_bias=False)(InputFunction)\n    \n    else:\n        finalUnits = Units\n        InputFunction = Input(shape=InputShape)\n        X = Flatten()(InputFunction)\n        X = Dense(finalUnits[0],use_bias=False)(X)\n                \n    \n    X = BatchNormalization()(X)\n    X = LeakyReLU()(X)\n    \n    for k in range(1,len(Units)-1):\n        \n        X = Dense(finalUnits[k],use_bias=False)(X)\n        X = BatchNormalization()(X)\n        X = LeakyReLU()(X)\n    \n    X = Dense(finalUnits[-1],use_bias=False)(X)\n    \n    if UpSampling:\n        X=LeakyReLU()(X)\n        Output=Reshape(InputShape)(X)\n    else:\n        Output=LeakyReLU()(X)\n        \n    Bottleneck=Model(inputs=InputFunction,outputs=Output)\n    \n    return InputFunction,Bottleneck\n\ndef MakeConvolutionBlock(X, Convolutions):\n    \n    X = Conv2D(Convolutions, (3, 3), padding='same',use_bias=False)(X)\n    X = BatchNormalization()(X)\n    X = LeakyReLU()(X)\n\n    return X\n\ndef MakeDenseBlock(x, Convolutions,Depth):\n\n    concat_feat= x\n    for i in range(Depth):\n        x = MakeConvolutionBlock(concat_feat,Convolutions)\n        concat_feat=concatenate([concat_feat,x])\n\n    return concat_feat\n\ndef SamplingBlock(X,Units,Depth,UpSampling=False):\n    \n    X = MakeDenseBlock(X,Units,Depth)\n    \n    if UpSampling:\n        X = Conv2DTranspose(Units,(3,3),strides=(2,2),padding='same',use_bias=False)(X)\n    else:    \n        X = Conv2D(Units,(3,3),strides=(2,2),padding='same',use_bias=False)(X)\n    \n    X = ChannelAttention()(X)\n    X = SpatialAttention(3)(X)\n    \n    X = BatchNormalization()(X)\n    X = LeakyReLU()(X)\n    \n    return X \n    \ndef CoderByBlock(InputShape,Units,Depth,UpSampling=False):\n    \n    if UpSampling:\n        Units=Units[::-1]\n    else:\n        Units=Units\n    \n    InputFunction = Input(shape=InputShape)\n    X = SamplingBlock(InputFunction,Units[0],Depth,UpSampling=UpSampling)\n    \n    for k in range(1,len(Units)-1):\n        \n        if Depth-k+1 <= 1:\n            blockSize = 2\n        else:\n            blockSize = Depth-k\n        \n        X = SamplingBlock(X,Units[k],blockSize,UpSampling=UpSampling)\n        \n    if UpSampling:\n        X = Conv2D(1,(3,3),padding='same',use_bias=False)(X)\n        Output = Activation('sigmoid')(X)\n    else:\n        X = Conv2D(Units[-1],(3,3),padding='same',use_bias=False)(X)\n        Output = LeakyReLU()(X)\n        \n    coderModel = Model(inputs=InputFunction,outputs=Output)\n    \n    return InputFunction,coderModel\n\n#Wrapper function joins the Coder function and the bottleneck function \n#to create a simple autoencoder\ndef MakeAutoencoder(CoderFunction,InputShape,Units,BlockSize,**kwargs):\n    \n    InputEncoder,Encoder=CoderFunction(InputShape,Units,BlockSize,**kwargs)\n    EncoderOutputShape=Encoder.layers[-1].output_shape\n    BottleneckInputShape=EncoderOutputShape[1::]\n    InputBottleneck,Bottleneck=MakeBottleneck(BottleneckInputShape,2)\n    ConvEncoderOutput=Bottleneck(Encoder(InputEncoder))\n    \n    ConvEncoder=Model(inputs=InputEncoder,outputs=ConvEncoderOutput)\n    \n    rInputBottleneck,rBottleneck=MakeBottleneck(BottleneckInputShape,2,UpSampling=True)\n    InputDecoder,Decoder=CoderFunction(BottleneckInputShape,Units,BlockSize,UpSampling=True,**kwargs)\n    ConvDecoderOutput=Decoder(rBottleneck(rInputBottleneck))\n    ConvDecoder=Model(inputs=rInputBottleneck,outputs=ConvDecoderOutput)\n    \n    ConvAEoutput=ConvDecoder(ConvEncoder(InputEncoder))\n    ConvAE=Model(inputs=InputEncoder,outputs=ConvAEoutput)\n    \n    return InputEncoder,InputDecoder,ConvEncoder,ConvDecoder,ConvAE\n\n# Wrapper functon, joins the autoencoder function with the custom variational\n#layers to create an autoencoder\ndef MakeVariationalAutoencoder(CoderFunction,InputShape,Units,BlockSize,**kwargs):\n    \n    InputEncoder,InputDecoder,ConvEncoder,ConvDecoder,_=MakeAutoencoder(CoderFunction,InputShape,Units,BlockSize,**kwargs)\n    \n    InputVAE,VAE=MakeVariationalNetwork(2)\n    VAEencoderOutput=VAE(ConvEncoder(InputEncoder))\n    ConvVAEencoder=Model(inputs=InputEncoder,outputs=VAEencoderOutput)\n    \n    VAEOutput=ConvDecoder(ConvVAEencoder(InputEncoder))\n    ConvVAEAE=Model(inputs=InputEncoder,outputs=VAEOutput)\n    \n    return InputEncoder,InputDecoder,ConvVAEencoder,ConvDecoder,ConvVAEAE    ","eec676dd":"class DataSequence(Sequence):\n    \n    def __init__(self, x_set,batch_size):\n        self.x = x_set\n        self.batch_size = batch_size\n        \n    def __len__(self):\n        return int(np.ceil(len(self.x)\/self.batch_size))\n    \n    def __data_generation(self, dirList):\n         \n        X = np.array([np.load(val) for val in dirList])\n        X = X.reshape((-1,64,80,1))\n        y = X\n\n        return X,y\n\n    def __getitem__(self,idx):\n        batch_x = self.x[idx*self.batch_size:(idx+1)*self.batch_size]\n        X, y = self.__data_generation(batch_x)\n        \n        return X,y\n\nclass KLAnnealing(keras.callbacks.Callback):\n\n    def __init__(self,position, weigths):\n        super().__init__()\n        self.position = position\n        self.weigths = tf.Variable(weigths,trainable=False,dtype=tf.float32)\n\n    def on_epoch_end(self, epoch,logs=None):\n        \n        weights = self.model.get_weights()\n        weights[self.position] = self.weigths[epoch]\n        self.model.set_weights(weights)\n\n\ndef MakeAnnealingWeights(epochs,cycles,scale=1):\n    \n    pointspercycle = epochs\/\/cycles\n    AnnealingWeights = 1*(1\/(1+np.exp(-1*np.linspace(-10,10,num=pointspercycle))))\n    \n    for k in range(cycles-1):\n        AnnealingWeights = np.append(AnnealingWeights,1*(1\/(1+np.exp(-1*np.linspace(-10,10,num=pointspercycle+1)))))\n        \n    return scale*AnnealingWeights","9e537c24":"GlobalDirectory=r\"..\/input\/covid19-graph-sequences\"\nmatrixData = GlobalDirectory + '\/featuresdata'\n\nfileNames = os.listdir(matrixData)\nnorthNames = [val+'.1.npy' for val in NorthEastCoast['id']]\nfinalNamesNorth = list(set(northNames).intersection(fileNames))\npathsNorth = np.array([matrixData+'\/'+val for val in finalNamesNorth]) \n\ninput_shape = (64,80,1)\nArch = [12,24,36,48,36]\nDepth = 4\n\nlr = 0.0001\nminlr = 0.000001\nepochs = 60\nbatch_size = 128\ndecay = 2*(lr-minlr)\/epochs\n\nAnnealingWeights = MakeAnnealingWeights(epochs,4,scale=0.00001)\n\n_,_,Encoder,Decoder,AE = MakeVariationalAutoencoder(CoderByBlock,input_shape,Arch,Depth)\nAE.summary()\nKLAposition = [k for k,val in enumerate(AE.get_weights()) if len(val.shape)==0][0]\n\ntrainPaths,testPaths,_,_ = train_test_split(pathsNorth,pathsNorth,test_size=0.1, random_state=42)\n\nFullLoad = DataSequence(pathsNorth,batch_size)\nTrainLoad = DataSequence(trainPaths,batch_size)\nTestLoad = DataSequence(testPaths,batch_size)\n","216d1dc7":"AE.compile(Adam(learning_rate=lr,decay=decay),loss='mse')\nhistory =  AE.fit(TrainLoad,epochs=epochs,\n                  validation_data=TestLoad,workers=8,\n                  callbacks=[KLAnnealing(KLAposition,AnnealingWeights)])\n    \nVariationalRepresentationNorth = Encoder.predict(FullLoad)","f59f0951":"NorthDataSet = NorthEastCoast.set_index('id')\nNorthIds = [val[0:-6] for val in finalNamesNorth]\nNorthDataSet = NorthDataSet.loc[NorthIds]\nNorthDataSet[\"latentA\"] = VariationalRepresentationNorth[:,0]\nNorthDataSet[\"latentB\"] = VariationalRepresentationNorth[:,1]","6bd4428f":"plt.figure(figsize=(16,6))\nplt.scatter(NorthDataSet[\"latentA\"],NorthDataSet[\"latentB\"],c=NorthDataSet['week'],alpha=0.15)","913bd5f0":"ConvVAEkmeans =  DBSCAN(eps=0.025,min_samples=10,algorithm='ball_tree',metric='euclidean',n_jobs=-2).fit(NorthDataSet[['latentA','latentB']])\nclustersLabels,counts = np.unique(ConvVAEkmeans.labels_,return_counts=True)\nclabels = []\n\nfor val,sal in zip(clustersLabels,counts):\n    if sal>1000 and val!=-1:\n        clabels.append(val)#removes outliers and low populated clusters\n\nlocalColors = [plt.cm.seismic(val) for val in np.linspace(0,1,num=len(clabels))]\nNorthDataSet['CLabels'] = [val if val in clabels else -1 for val in ConvVAEkmeans.labels_]","e0b17024":"plt.figure(figsize=(18,10))\n\ngs = plt.GridSpec(2,len(clabels))\nax0 = plt.subplot(gs[0,:])\n\nfor k,col in zip(clabels,localColors):\n    ax0.scatter(NorthDataSet[NorthDataSet['CLabels']==k]['latentA'],NorthDataSet[NorthDataSet['CLabels']==k]['latentB'],c=col,alpha=0.0125)\n\nax0.scatter(NorthDataSet[NorthDataSet['CLabels']==-1]['latentA'],NorthDataSet[NorthDataSet['CLabels']==-1]['latentB'],c='black',alpha=0.0125)\nax0.set_title('Masha',loc='right')\n\nk=0\nfor clust,colr in zip(clabels,localColors):\n    \n    axes = plt.subplot(gs[1,k])\n    axes.hist(NorthDataSet[NorthDataSet['CLabels']==clust]['week'],bins=50,color='blue')\n    k=k+1\n    \nplt.tight_layout()","c56802f2":"KmersData = pd.read_csv('..\/input\/covid19-sequence-kmer-frequencies\/KmerData.csv')\nKmersData['id'] = [val[0:-2] for val in KmersData['id']]\nKmersData = KmersData.set_index('id')\n\nnrows,ncolumns = GetGridShape(len(clabels))\nsubPlotIndexs=[(j,k) for j in range(nrows) for k in range(ncolumns)]","5f283877":"fig,axes=plt.subplots(nrows,ncolumns,figsize=(15,12),sharex=True)\nfor k,kal in enumerate(clabels):\n    \n    clusterData = NorthDataSet[NorthDataSet['CLabels']==kal]\n    low = clusterData[clusterData['week']<0.5].index\n    high = clusterData[clusterData['week']>0.5].index\n    axes[subPlotIndexs[k]].hist(KmersData['A'].loc[high],color='red',bins=100,density=True,label='Cluster = '+str(kal))\n    axes[subPlotIndexs[k]].hist(KmersData['A'].loc[low],color='red',bins=100,alpha=0.5,density=True,label='Cluster = '+str(kal))\n    axes[subPlotIndexs[k]].set_xlim([0.05,0.15])\n    axes[subPlotIndexs[k]].set_xlabel('Nucleotide content')\n    axes[subPlotIndexs[k]].set_title('Cluster = ' + str(kal))\nfig.suptitle('Adenine shift',x=0.9,y=0.9)","cf824672":"fig,axes=plt.subplots(nrows,ncolumns,figsize=(15,12),sharex=True)\nfor k,kal in enumerate(clabels):\n    \n    clusterData = NorthDataSet[NorthDataSet['CLabels']==kal]\n    low = clusterData[clusterData['week']<0.5].index\n    high = clusterData[clusterData['week']>0.5].index\n    axes[subPlotIndexs[k]].hist(KmersData['C'].loc[high],color='blue',bins=100,density=True,label='Cluster = '+str(kal))\n    axes[subPlotIndexs[k]].hist(KmersData['C'].loc[low],color='blue',bins=100,alpha=0.5,density=True,label='Cluster = '+str(kal))\n    axes[subPlotIndexs[k]].set_xlim([0.2,0.35])\n    axes[subPlotIndexs[k]].set_xlabel('Nucleotide content')\n    axes[subPlotIndexs[k]].set_title('Cluster = ' + str(kal))\n    \nfig.suptitle('Cytosine shift',x=0.9,y=0.9)","516a1d0e":"fig,axes=plt.subplots(nrows,ncolumns,figsize=(15,12),sharex=True)\nfor k,kal in enumerate(clabels):\n    \n    clusterData = NorthDataSet[NorthDataSet['CLabels']==kal]\n    low = clusterData[clusterData['week']<0.5].index\n    high = clusterData[clusterData['week']>0.5].index\n    axes[subPlotIndexs[k]].hist(KmersData['G'].loc[high],color='green',bins=100,density=True,label='Cluster = '+str(kal))\n    axes[subPlotIndexs[k]].hist(KmersData['G'].loc[low],color='green',bins=100,alpha=0.5,density=True,label='Cluster = '+str(kal))\n    axes[subPlotIndexs[k]].set_xlim([0.75,0.85])\n    axes[subPlotIndexs[k]].set_xlabel('Nucleotide content')\n    axes[subPlotIndexs[k]].set_title('Cluster = ' + str(kal))\nfig.suptitle('Guanine shift',x=0.9,y=0.9)","613b2619":"fig,axes=plt.subplots(nrows,ncolumns,figsize=(15,12),sharex=True)\nfor k,kal in enumerate(clabels):\n    \n    clusterData = NorthDataSet[NorthDataSet['CLabels']==kal]\n    low = clusterData[clusterData['week']<0.5].index\n    high = clusterData[clusterData['week']>0.5].index\n    axes[subPlotIndexs[k]].hist(KmersData['T'].loc[high],color='black',bins=100,density=True,label='Cluster = '+str(kal))\n    axes[subPlotIndexs[k]].hist(KmersData['T'].loc[low],color='black',bins=100,alpha=0.5,density=True,label='Cluster = '+str(kal))\n    axes[subPlotIndexs[k]].set_xlim([0.7,0.8])\n    axes[subPlotIndexs[k]].set_xlabel('Nucleotide content')\n    axes[subPlotIndexs[k]].set_title('Cluster = ' + str(kal))\nfig.suptitle('Thymine\/Uracil shift',x=0.9,y=0.9)","f603b744":"USAInfectionData = pd.read_csv('..\/input\/us-covid19-dataset-live-hourlydaily-updates\/States.csv')\nUSAInfectionData[\"datets\"] = pd.to_datetime(USAInfectionData['date'],format='%Y-%m-%d')\nUSAInfectionData[\"year\"] = USAInfectionData[\"datets\"].dt.year\nUSAInfectionData[\"week\"] = USAInfectionData[\"datets\"].dt.isocalendar().week\nUSAInfection2021 = USAInfectionData[USAInfectionData[\"year\"]==2021]\n\nGetDataByState = lambda state : np.array(USAInfection2021[USAInfection2021[\"state\"]==state].groupby(\"week\")[\"cases\"].sum())[0:-2]\n\nStates = [\"Maine\",\"Vermont\",\"New Hampshire\",\"Massachusetts\",\"Rhode Island\",\"New York\",\"Connecticut\",\n         \"New Jersey\",\"Pennsylvania\",\"Delaware\"]\n\nStateData = np.array([GetDataByState(st)\/GetDataByState(st).max() for st in States])","14947638":"localData = NorthDataSet[NorthDataSet[\"Geo_Location\"]==\"USA: New York\"]\n\nfig,axs = plt.subplots(3,3,figsize=(20,10))\n\nfor st in StateData:\n    \n    axs[0,0].plot(st,color=\"gray\")\n    axs[0,1].plot(np.diff(st,n=1),color=\"gray\")\n    axs[0,2].plot(np.diff(st,n=2),color=\"gray\")\n\naxs[0,0].plot(StateData[5],color=\"red\")\naxs[0,1].plot(np.diff(StateData[5],n=1),color=\"red\")\naxs[0,2].plot(np.diff(StateData[5],n=2),color=\"red\")\n\naxs[1,0].plot(StateData[5],np.array(localData.groupby(\"week\")[\"latentA\"].mean()),'bo')\naxs[1,1].plot(np.diff(StateData[5],n=1),np.array(localData.groupby(\"week\")[\"latentA\"].mean())[0:-1],'bo')\naxs[1,2].plot(np.diff(StateData[5],n=2),np.array(localData.groupby(\"week\")[\"latentA\"].mean())[0:-2],'bo')\n\naxs[2,0].plot(StateData[5],np.array(localData.groupby(\"week\")[\"latentB\"].mean()),'bo')\naxs[2,1].plot(np.diff(StateData[5],n=1),np.array(localData.groupby(\"week\")[\"latentB\"].mean())[0:-1],'bo')\naxs[2,2].plot(np.diff(StateData[5],n=2),np.array(localData.groupby(\"week\")[\"latentB\"].mean())[0:-2],'bo')","033c0329":"localData = NorthDataSet[NorthDataSet[\"Geo_Location\"]==\"USA: Massachusetts\"]\n\nfig,axs = plt.subplots(3,3,figsize=(20,10))\n\nfor st in StateData:\n    \n    axs[0,0].plot(st,color=\"gray\")\n    axs[0,1].plot(np.diff(st,n=1),color=\"gray\")\n    axs[0,2].plot(np.diff(st,n=2),color=\"gray\")\n\naxs[0,0].plot(StateData[3],color=\"red\")\naxs[0,1].plot(np.diff(StateData[3],n=1),color=\"red\")\naxs[0,2].plot(np.diff(StateData[3],n=2),color=\"red\")\n\naxs[1,0].plot(StateData[3],np.array(localData.groupby(\"week\")[\"latentA\"].mean())[0:-2],'bo')\naxs[1,1].plot(np.diff(StateData[3],n=1),np.array(localData.groupby(\"week\")[\"latentA\"].mean())[0:-3],'bo')\naxs[1,2].plot(np.diff(StateData[3],n=2),np.array(localData.groupby(\"week\")[\"latentA\"].mean())[0:-4],'bo')\n\naxs[2,0].plot(StateData[3],np.array(localData.groupby(\"week\")[\"latentB\"].mean())[0:-2],'bo')\naxs[2,1].plot(np.diff(StateData[3],n=1),np.array(localData.groupby(\"week\")[\"latentB\"].mean())[0:-3],'bo')\naxs[2,2].plot(np.diff(StateData[3],n=2),np.array(localData.groupby(\"week\")[\"latentB\"].mean())[0:-4],'bo')","3d0abba9":"VMData = NorthDataSet[NorthDataSet[\"Geo_Location\"]==\"USA: Pennsylvania\"]\n\nfig,axs = plt.subplots(3,3,figsize=(20,10))\n\nfor st in StateData:\n    \n    axs[0,0].plot(st,color=\"gray\")\n    axs[0,1].plot(np.diff(st,n=1),color=\"gray\")\n    axs[0,2].plot(np.diff(st,n=2),color=\"gray\")\n\naxs[0,0].plot(StateData[8],color=\"red\")\naxs[0,1].plot(np.diff(StateData[8],n=1),color=\"red\")\naxs[0,2].plot(np.diff(StateData[8],n=2),color=\"red\")\n\naxs[1,0].plot(StateData[8],np.array(localData.groupby(\"week\")[\"latentA\"].mean())[0:-2],'bo')\naxs[1,1].plot(np.diff(StateData[8],n=1),np.array(localData.groupby(\"week\")[\"latentA\"].mean())[0:-3],'bo')\naxs[1,2].plot(np.diff(StateData[8],n=2),np.array(localData.groupby(\"week\")[\"latentA\"].mean())[0:-4],'bo')\n\naxs[2,0].plot(StateData[8],np.array(localData.groupby(\"week\")[\"latentB\"].mean())[0:-2],'bo')\naxs[2,1].plot(np.diff(StateData[8],n=1),np.array(localData.groupby(\"week\")[\"latentB\"].mean())[0:-3],'bo')\naxs[2,2].plot(np.diff(StateData[8],n=2),np.array(localData.groupby(\"week\")[\"latentB\"].mean())[0:-4],'bo')","d9f9daad":"# Training","6a83375c":"# Adding the metadata ","304a6c3c":"# Working with biological sequences ","4fbc1e12":"# Network definition ","25e1f570":"## Pennsylvania","89ec5a7b":"One simple analysis of the cluster information will be to measure the quantity of each of the different nucleobases in the sequence and how they change from cluster to cluster. And how that content changes through time within each cluster, dividing the cluster into two periods, the first half of the year and the second half of the year. ","2324e42e":"## New York","7c1eaf2b":"# Adding infection dynamics\n\nAnother kind of metadata to add to the learned representations is the dynamics of the cases in the particular region. Three states will be taken as examples and the total cases per week are plotted against the mean value of each data point in that week from that state. To add the dynamics of the process, the cases rate and acceleration are also compared. This hopefully results in additional information encoded in the learned representation. ","25d30129":"## Packages","da667bb1":"## Cytosine shift","7735c0c7":"As each k relational encoding is defined as a graph, the adjacency matrix will contain the frequency of each connection. This matrix ix normalized with $D^{-\\frac{1}{2}}AD^{-\\frac{1}{2}}$. Where D is the degree matrix and A the adjacency matrix, then this matrix is scaled by dividing it by its Frobenius norm. Then the difference between scheme A and scheme B is used as the final representation for a k size sequence fragment. Arranging the different normalized matrices up to the 3 size fragment results in the following 2D array. \n\nThis array can be used as a single channel image and use one of the many neural network architectures optimized for computer vision. Another advantage for this kind of encoding is that longe range dependencies in the sequence might be closer in the 2D sequence representation.  ","3f1e3834":"## Adenine Shift","d1afebf3":"From the nucleobase content analysis, a clear shift in the content of Cytosine and Thymine\/Uracil is found on every cluster. This particular characteristic might be useful for other interventions. ","bcbbeec4":"Another technique will be to analyze the different clusters in the representation. Clustering by itself might not provide a definitive answer, but it could help us to analyze each cluster individually. ","8f35e499":"Biological sequences can be categorized into three main groups. DNA sequences, RNA sequences, and protein sequences. Each element in a DNA or RNA sequence represents a nucleoside while the elements of a protein sequence represent an amino acid. From DNA or RNA sequences the corresponding protein sequence can be inferred, but the inverse process is not possible. As the amino acid encoding within the DNA\/RNA sequence does not have a one-to-one correspondence. Meaning that single amino acid is encoded in different ways within the DNA\/RNA sequence. \n\nA DNA or RNA sequence consists of 4 different elements, each element represents a nucleoside or nucleobase inside the DNA or RNA chain. Different three elements combinations of nucleobases lead to specific amino acids. While larger combinations usually correspond to regulatory elements inside the sequence. One of the most common methods to encode a text or biological sequence is to use a binary representation of the sequence or one-hot encoding. However, this type of encoding increases the dimensionality of the data for large sequences. \n\nA simple solution to the dimensionality and problem will be to encode the relationship between different elements in the sequence rather than the elements. In a relational encoding scheme, a link is added between two elements if the two elements are consecutive. This simple definition leaves room to process the sequence in different ways creating different encodings of the same sequence. \n\nIn the case of the SARS Cov 2 sequences, two connectivity schemes are proposed. In the first one, each sequence is divided into k size fragments and a link is added to consecutive fragments in the sequence. While in the second one each sequence is divided into 2k size fragments, then the fragment is divided into two k size fragments and a link is added between the two consecutive fragments. Each connectivity scheme can be visualized as graphs showing specific patterns. \n","f2accee3":"# Dataset generation and subsampling. \n\nWith a simple sequence representation in place, a subsample of the graph sequence data set is taken as training data. Particularly the sequences isolated in the northeast coast of the USA. Sequence ids approximately match the file names in the Covid-19 Graph Sequences data set. The main difference is that the file names contain the sequence version as stated in the FASTA record of the sequence. While the metadata contains only the sequence id without version. ","5719e1e7":"## Guanine shift","bfe55853":"### Auxiliary functions","8988892d":"Network construction can be divided into three main parts, the convolutional block, the dense bottleneck, and the variational layers. And those components are combined with three wrapper functions. Two merge the networks to create the variational autoencoder, MakeAutoencoder creates an autoencoder without the variational layers, while MakeVariationalAutoencoder defines the variational autoencoder. While CoderByBlock creates the main body of the convolutional autoencoder, it takes a function that creates a simple convolutional network and the number of filters to apply for each convolutional block. This allows changing the main architecture of the network by modifying only one function for fast experimentation. \n\nThe main convolutional block consists of a dense convolutional network with a LeakyReLU activation function. Downsampling is done by setting strides to (2,2) on the final Conv2D layer within the block and UpSampling by changing the Conv2D layer to Conv2DTranspose with the same strides configuration. The output of this is then connected to a channel and spatial attention module. \n\nTo dynamically weight the KL loss of the autoencoder a TensorFlow variable is added to the KL layer and trainable is set to false. This incorporates the new variable into the network weights. Then the new variable is changed with a callback at the end of each epoch. \n\nFinally, the data is loaded using the sequence class to load and reshape the data. ","43f40b5e":"# SARSCov2 seasonal disentangling with KL annealing\n\nInfection by SARS-Cov-2 results in the respiratory disease COVID-19 and is responsible for the ongoing pandemic. Currently, data science and machine learning approaches to understand and predict the development of the COVID-19 pandemic rely on computer vision or time series analysis. Mainly to identify COVID-19 positive cases from chest X-rays or to predict infection dynamics. However, the use of genetic data for machine learning applications remains scarce. \n\nAlthough there are some examples of machine learning modeling of SARS-Cov-2 genome sequences, the genome consists of around 30k bases making it extremely difficult to analyze using modern NLP neural network architectures. The following shows a simple encoding method to analyze large biological sequences and its use for a representation learning task to cluster genomic data. Also how to combine other data sources to understand the meaning of the embedded dimensions. \n","b520ca60":"From the selected examples looks like there is a particular correlation between either cases rate or acceleration of the cases and the learned representation. However, this correlation is perhaps hindered by the noise in the acceleration approximation. \n\nAlthough the cross-validation is not optimal and the data set presents geographical and temporal imbalances. The ability to find temporal patterns suggest the existence of such patterns but not the ability to generalize at least with the current configuration. \n\nHowever, is worth mentioning that relational encoding offers a viable solution to encode large sequences and find meaningful patterns using convolutional neural networks. The computer vision approach may favor representations where the sequences share some common global similarity and specific patterns in the sequence might be lost. However relational encoding can also be used for Graph neural networks to identify important node clusters that could lead to specific genomic regions. The continuous development of these models could lead to a better understanding of the Covid-19 pandemic and the introduction of new preventive measures. \n\nFurther details at https:\/\/tavoglc.medium.com\/sars-cov-2-classification-with-variational-autoencoders-4842696a43c","1a4b8f01":"### Network definition","ac2ed237":"To understand the encoded meaning of the learned representations we can add new metadata and encode that information. One simple visual technique will be to color code the isolation date of the different SARS Cov 2 sequences. This will visually show any kind of pattern in the data. ","0e598bfb":"## Massachusetts","dea20c6c":"## Thymine\/Uracil shift","0a93f01f":"### Custom layers"}}