{"cell_type":{"cb4264f9":"code","cb2dd84a":"code","58dda2e2":"code","4efa4616":"code","2654368b":"code","47539d76":"code","4a90292c":"code","8aece877":"code","66f94fb7":"code","cecc142d":"code","91b6b0fa":"code","2e3193e9":"code","aa3cbc3d":"code","4b69313d":"code","7cc3570c":"code","18a52f58":"code","ba67fa7e":"code","63e36fe1":"code","28dda251":"code","0eaed767":"code","10967fcd":"code","e9485595":"code","f6000bfc":"code","d59f774f":"code","aa9a4087":"code","4df04582":"code","9b8aab86":"code","c5048821":"code","79a35cdf":"code","37902f06":"code","3a68c3e2":"code","53e032fd":"code","7d0e1be5":"code","05c4aa9e":"code","a96c85a8":"code","696de366":"code","0a2cb075":"code","439b6b26":"code","11220c0a":"code","a2fd0de8":"code","cf7a6fbc":"code","78fbbed0":"code","cadf2aaa":"code","210753dc":"code","229675a5":"code","ffba6e30":"code","9b30f714":"code","bcec8d09":"code","92294412":"code","49a80bb8":"code","7d2fdbac":"code","d64be533":"code","de37fbd7":"code","479b4369":"code","2026ff5b":"code","103d4531":"code","b62419a2":"markdown","61a2ec36":"markdown","8c3fa0bd":"markdown","947d620e":"markdown","bd0665cc":"markdown","2ecfb31f":"markdown","84aa80a9":"markdown","b9dde7f1":"markdown","961a7ebd":"markdown","11aab96f":"markdown","36462a13":"markdown","4556d20e":"markdown","5fccf5ab":"markdown","b9660431":"markdown","4a530f9f":"markdown","44bbd32d":"markdown","528ab892":"markdown","f5499a10":"markdown","b37ab2c5":"markdown","90d3f3d9":"markdown","81efef09":"markdown","2ce7669c":"markdown","c0926353":"markdown","8415708e":"markdown","94286f99":"markdown","fb6e36db":"markdown","31cfa9bc":"markdown","245e3eee":"markdown","ae16de5b":"markdown","fbbce5df":"markdown","96c568fc":"markdown","e610edfc":"markdown","29ad2d85":"markdown","8a7c57ec":"markdown"},"source":{"cb4264f9":"# importing the useful libraries\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nsns.set_style('whitegrid')\nfrom datetime import datetime\nimport statsmodels.api as sm\nfrom statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom sklearn.metrics import mean_squared_error\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.stattools import acf,pacf\nimport itertools\nimport warnings\nwarnings.filterwarnings('ignore')","cb2dd84a":"# loading the train data\ntrain = pd.read_csv('\/kaggle\/input\/time-series\/Train_SU63ISt.csv')\n\n# copying the data to other variable\ntrain_org = train.copy()\n\n# printing first 5 rows\ntrain.head()","58dda2e2":"# loading the test data\ntest = pd.read_csv('\/kaggle\/input\/time-series\/Test_0qrQsBZ.csv')\n\n# copying the data to other variable\ntest_org = test.copy()\n\n# printing first 5 rows\ntest.head()","4efa4616":"# printing the name of columns\ntrain.columns, test.columns","2654368b":"# overall summary of train data\ntrain.info()","47539d76":"# overall summary of test data\ntest.info()","4a90292c":"# checking the # of rows and columns in train and test data\nprint('Number of rows and columns in -')\nprint('Train:',train.shape)\nprint('Test:',test.shape)","8aece877":"# converting the datetime column to datetime as it is stores as object\ntrain['Datetime'] = pd.to_datetime(train['Datetime'], format='%d-%m-%Y %H:%M')\ntrain_org['Datetime'] = pd.to_datetime(train_org['Datetime'], format='%d-%m-%Y %H:%M')\n\n# converting the datetime column to datetime as it is stores as object\ntest['Datetime'] = pd.to_datetime(test['Datetime'], format='%d-%m-%Y %H:%M')\ntest_org['Datetime'] = pd.to_datetime(test_org['Datetime'], format='%d-%m-%Y %H:%M')","66f94fb7":"# checking the min and max datetime period in train data\nprint('Min date in train data is:',train_org['Datetime'].min())\nprint('Max date in train data is:',train_org['Datetime'].max())","cecc142d":"# checking the min and max datetime period in test data\nprint('Min date in test data is:',test_org['Datetime'].min())\nprint('Max date in test data is:',test_org['Datetime'].max())","91b6b0fa":"# extracting year, month, day, hour, day of week from datetime column for some exploratory analysis\n\ntrain['year'] = train['Datetime'].dt.year\ntrain['month'] = train['Datetime'].dt.month_name()\ntrain['month1'] = train['Datetime'].dt.month\ntrain['day'] = train['Datetime'].dt.day\ntrain['hour'] = train['Datetime'].dt.hour\ntrain['day_of_week1'] = train['Datetime'].dt.dayofweek\ntrain['day_of_week'] = train['Datetime'].dt.day_name()\ntrain['weekend'] = train['day_of_week'].apply(lambda x: 1 if (x=='Saturday')|(x=='Sunday') else 0)","2e3193e9":"plt.figure(figsize=(12,5))\nplt.plot(train.Datetime,train.Count)\nplt.xlabel('Time (year-month)')\nplt.ylabel('Passenger count')\nplt.title('Number of passengers count', size=15)\nplt.figtext(x=0.2, y=0.4, s=\"There is a non-decreasing trend\", size=12, color='red')\nplt.show()","aa3cbc3d":"month_index = train.groupby('month1')['Count'].mean().index\nmonth_value = train.groupby('month1')['Count'].mean().values\n\nplt.figure(figsize=(15,5))\nplt.plot(month_index, month_value, color='purple')\nplt.ylabel('Passenger count', size=12)\nplt.xlabel('Month number', size=12)\nplt.title('Mean number of passenger in each month',size=15)\nplt.figtext(x=0.4, y=0.4, s=\"The peak in # of passenger is in July-August\", size=12, color='red')\nplt.show()","4b69313d":"dow_index = train.groupby('day_of_week1')['Count'].mean().index\ndow_value = train.groupby('day_of_week1')['Count'].mean().values\n\nplt.figure(figsize=(15,5))\nplt.plot(dow_index, dow_value, color='purple')\nplt.ylabel('Passenger count', size=12)\nplt.xlabel('Day of week', size=12)\nplt.title('Mean number of passenger in each day of week',size=15)\nplt.figtext(x=0.2, y=0.75, s=\"People travel more in weekdays\", size=12, color='red')\nplt.show()","7cc3570c":"day_index = train.groupby('day')['Count'].mean().index\nday_value = train.groupby('day')['Count'].mean().values\n\nplt.figure(figsize=(15,5))\nplt.plot(day_index, day_value, color='purple')\nplt.ylabel('Passenger count', size=12)\nplt.xlabel('Number of days', size=12)\nplt.xlim(1,31)\nplt.hlines(y=round(train.Count.mean(),2), xmin=1, xmax=31, linestyles='dashed', label='mean number of passenger')\nplt.title('Mean number of passenger in a day', size=15)\nplt.legend(loc='center', fontsize='large')\nplt.show()","18a52f58":"hour_index = train.groupby('hour')['Count'].mean().index\nhour_value = train.groupby('hour')['Count'].mean().values\n\nplt.figure(figsize=(15,5))\nplt.plot(hour_index, hour_value, color='purple')\nplt.ylabel('Passenger count', size=12)\nplt.xlabel('Number of hours', size=12)\nplt.xlim(0,23)\nplt.title('Mean number of passenger in each hour of a day',size=15)\nplt.figtext(x=0.2, y=0.4, s=\"People travel less between 00:AM-10:AM\", size=12, color='red')\nplt.show()","ba67fa7e":"# selecting only required column for time series analysis\ntrain = train.iloc[:,[1,2]]\ntrain.head()","63e36fe1":"# setting the datetime column as an index\ntrain = train.set_index('Datetime')\ntrain.head()","28dda251":"# this command will find the mean closing count of passenger of each hour for each day\nhourly = train.resample('H').mean()\n\n# this command will find the mean closing count of passenger of each day\ndaily = train.resample('D').mean()\n\n# this command will find the mean closing count of passenger of each week\nweekly = train.resample('W').mean()\n\n# this command will find the mean closing count of passenger of each month for a duration of 12 months\nmonthly = train.resample('M').mean()","0eaed767":"plt.subplot(4,1,1)\nhourly.Count.plot(figsize=(10,6),title='HOURLY')\nplt.xlabel('')\nplt.subplot(4,1,2)\ndaily.Count.plot(figsize=(10,6),title='DAILY')\nplt.xlabel('')\nplt.subplot(4,1,3)\nweekly.Count.plot(figsize=(10,6),title='WEEKLY')\nplt.xlabel('')\nplt.subplot(4,1,4)\nmonthly.Count.plot(figsize=(10,6),title='MONTHLY')\nplt.tight_layout()\nplt.show()","10967fcd":"# converting to daily mean\ntrain = train.resample('D').mean()\n\ntest = test.set_index('Datetime')\n# converting to daily mean\ntest = test.resample('D').mean()","e9485595":"train.shape, test.shape","f6000bfc":"Train = train.iloc[:669,:] # 2012-08-25 : 2014-06-24\nvalid = train.iloc[669:,:] # 2014-06-25 : 2014-09-25","d59f774f":"Train.Count.plot(figsize=(15,5), label='Train data', color='black')\nvalid.Count.plot(figsize=(15,5), label='Valid data', color='lightgrey')\nplt.xlabel('Datetime')\nplt.ylabel('Passenger count')\nplt.legend(fontsize='large')\nplt.show()","aa9a4087":"# copying the valid data into y_hat\ny_hat = valid.copy()\n# create a new column 'naive' in y_hat whose value is last value of the train data\ny_hat['naive'] = Train.Count[-1]\n# calculating the RMSE between actual and predicted\nrms = np.sqrt(mean_squared_error(y_hat.Count, y_hat.naive))\nprint('Root Mean Squared Error:',round(rms,2))\n\nplt.figure(figsize=(12,5))\nplt.plot(Train.index, Train['Count'], label='Train', color='black')\nplt.plot(y_hat.index,y_hat['Count'], label='Valid', color='lightgrey')\nplt.plot(y_hat.index,y_hat['naive'], label='Naive Forecast', color='red')\nplt.legend(loc='best', fontsize='large')\nplt.title(\"Naive Forecast\",size=15)\nplt.show()","4df04582":"train_mv = train.copy()\nfor i in range(2,11):\n    train_mv['moving_average'] = train_mv['Count'].rolling(i).mean()\n    train_mv.dropna(inplace=True)\n    rms = np.sqrt(mean_squared_error(train_mv.loc['2014-06-25':'2014-09-25'].Count, \n                                     train_mv.loc['2014-06-25':'2014-09-25'].moving_average))\n    print('RMSE when window is of size {}:'.format(i),round(rms,3))","9b8aab86":"# copying the train data into new variable\ntrain_mv = train.copy()\n# picking the average of last 9 observation\ntrain_mv['moving_average'] = train_mv['Count'].rolling(9).mean()\n# dropping the NAs\ntrain_mv.dropna(inplace=True)\n# calculating the RMSE for valid datetime\nrms = np.sqrt(mean_squared_error(train_mv.loc['2014-06-25':'2014-09-25'].Count, \n                                 train_mv.loc['2014-06-25':'2014-09-25'].moving_average))\nprint('Root Mean Squared Error:',round(rms,3))\n\n# plotting train, valid and Moving Average\nplt.figure(figsize=(15,5))\nplt.plot(Train['Count'], label='Train', color='black')\nplt.plot(valid['Count'], label='Test', color='lightgrey')\nplt.plot(train_mv['moving_average'], label='Moving Average(9)', color='red')\nplt.legend(loc='best', fontsize='large')\nplt.show()","c5048821":"y_hat = valid.copy()\nfor i in np.arange(0,1.1,0.1):\n    fit = SimpleExpSmoothing(np.asarray(Train['Count'])).fit(smoothing_level=i, optimized=False)\n    y_hat['SES'] = fit.forecast(len(valid))\n    rms = np.sqrt(mean_squared_error(y_hat.Count, y_hat.SES))\n    print('RMSE with smoothing level(\u03b1) of {}:'.format(i.round(1)),round(rms,3))","79a35cdf":"y_hat = valid.copy()\nfit1 = SimpleExpSmoothing(np.asarray(Train['Count'])).fit(optimized=True)\ny_hat['SES'] = fit1.forecast(len(valid))\nrms = np.sqrt(mean_squared_error(y_hat.Count, y_hat.SES))\nprint('RMSE with smoothing level(\u03b1) of {}:'.format(fit1.params_formatted.iloc[0,1].round(3)),round(rms,3))\nfit1.summary()","37902f06":"# copying the valid data into other variable\ny_hat = valid.copy()\n# choosing the value of \u03b1 = 0.11 and optimized=False, we fit the SES on train data\nfit2 = SimpleExpSmoothing(np.asarray(Train['Count'])).fit(smoothing_level=0.11, optimized=False)\n# forecast the values\ny_hat['SES'] = fit2.forecast(len(valid))\n# Calculating the RMSE between actual and predicted\nrms = np.sqrt(mean_squared_error(y_hat.Count, y_hat.SES))\nprint('Root Mean Squared Error:', round(rms,3))\n\nplt.figure(figsize=(15,5))\nplt.plot(Train['Count'], label='Train', color='black')\nplt.plot(valid['Count'], label='Valid', color='lightgrey')\nplt.plot(y_hat['SES'], label='SES (\u03b1 = 0.11)', color='red')\nplt.legend(loc='best', fontsize='large')\nplt.show()","3a68c3e2":"y_hat = valid.copy()\nfit1 = Holt(np.asarray(Train['Count'])).fit(optimized=True, initial_level=0.2, initial_slope=0.15)\ny_hat['SES'] = fit1.forecast(len(valid))\nrms = np.sqrt(mean_squared_error(y_hat.Count, y_hat.SES))\nprint('RMSE with \u03b1 = {} & \u03b2 = {}:'.format(fit1.params_formatted.iloc[0,1],fit1.params_formatted.iloc[1,1]),round(rms,3))\nfit1.summary()","53e032fd":"y_hat = valid.copy()\nfit2 = Holt(np.asarray(Train['Count']),exponential=True).fit(optimized=True, initial_level=0.2, initial_slope=0.15)\ny_hat['SES'] = fit2.forecast(len(valid))\nrms = np.sqrt(mean_squared_error(y_hat.Count, y_hat.SES))\nprint('RMSE with \u03b1 = {} & \u03b2 = {}:'.format(fit2.params_formatted.iloc[0,1],fit2.params_formatted.iloc[1,1]),round(rms,3))\nfit2.summary()","7d0e1be5":"# copying the data into other variable\ny_hat = valid.copy()\n# fitting additive Holt with  \u03b1 = 0.063 & \u03b2 = 0.01\nfit3 = Holt(np.asarray(Train.Count)).fit(smoothing_level=0.063, smoothing_slope=0.01)\n# forecast the values\ny_hat['Holt_linear'] = fit3.forecast(len(valid))\n# calculating the RMSE between actual vs predcited\nrms = np.sqrt(mean_squared_error(y_hat.Count, y_hat.Holt_linear))\nprint('Root Mean Squared Error:', round(rms,3))\n\nplt.figure(figsize=(15,5))\nplt.plot(Train.Count,label='Train', color='black')\nplt.plot(valid.Count,label='Valid', color='lightgrey')\nplt.plot(y_hat.Holt_linear,label='Holt Linear (\u03b1 = 0.063 & \u03b2 = 0.01)', color='red')\nplt.legend(fontsize='large')\nplt.title('Additive Trend Holt Linear Model', size=15)\nplt.show()","05c4aa9e":"y_hat = valid.copy()\ntd = ['add','mul']\nsl = ['add','mul']\nplt.figure(figsize=(15,8))\nfor t1,t2 in enumerate(td):\n    for s1,s2 in enumerate(sl):\n        plt.subplot(2,2,t1*2+s1+1)\n        fit = ExponentialSmoothing(np.asarray(Train.Count), seasonal_periods=7, trend=t2, seasonal=s2).fit()\n        y_hat['Holt_Winter'] = fit.forecast(len(valid))\n        rms = np.sqrt(mean_squared_error(y_hat.Count, y_hat.Holt_Winter))\n        plt.plot(Train.Count, label='Train', color='black')\n        plt.plot(valid.Count, label='Valid', color='lightgrey')\n        plt.plot(y_hat.Holt_Winter, label='Holt Winter', color='red')\n        plt.legend(fontsize='large')\n        plt.title('Trend:{}, Seasonal:{}, RMSE:{}'.format(t2,s2,round(rms,3)), size=12)\nplt.suptitle('Exponential Smoothing with different type of trend and seasonality', size=15)\nplt.show()","a96c85a8":"decomposition = seasonal_decompose(Train)\ntrend = decomposition.trend\nseasonal = decomposition.seasonal\nresidual = decomposition.resid\n\nplt.figure(figsize=(10,8))\nplt.subplot(411)\nplt.plot(Train, label='Original')\nplt.legend()\nplt.subplot(412)\nplt.plot(trend, label='Trend')\nplt.legend()\nplt.subplot(413)\nplt.plot(seasonal, label='Seasonal')\nplt.legend()\nplt.subplot(414)\nplt.plot(residual, label='Residual')\nplt.legend()\nplt.tight_layout()\nplt.show()","696de366":"def test_stationarity(timeseries):\n    rolmean = timeseries.rolling(30).mean() # 30 because of months\n    rolstd = timeseries.rolling(30).std()\n    #Plot rolling statistics\n    plt.figure(figsize=(12,5))\n    orig = plt.plot(timeseries, color='black', label='Original')\n    mean = plt.plot(rolmean, color='yellow', label='Rolling Mean')\n    std = plt.plot(rolstd, color='green', label='Rolling Std')\n    plt.legend(loc='best', fontsize='large')\n    plt.title('Rolling Mean & Standard Deviation', size=15)\n    plt.show(block=False)\n    \n    # Perform Augmented Dickey-Fuller test:\n    # H0: It is non-stationary\n    # H1: It is stationary\n    # stationarity can be check using either p-value < 0.05 or test-statistic < critical value\n\n    print('Results of Dickey-Fuller Test:')\n    result = adfuller(timeseries)\n    dfoutput = pd.Series(result[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n    for key,value in result[4].items():\n        dfoutput['Critical Value (%s)'%key] = value\n    print(dfoutput)\n    if dfoutput[1] <= 0.05:\n        print('Strong evidence against null hypothesis. Hence reject the null hypothesis. Data is stationary')\n    else:\n        print('weak evidence against null hypothesis. Hence we cannot reject null hypothesis. Data is non-stationary')","0a2cb075":"test_stationarity(Train['Count'])","439b6b26":"print('Mean # of passenger in train data is:',round(Train.Count.mean()))\nprint('Mean # of passenger in test data is:',round(valid.Count.mean()))","11220c0a":"print('S.D. of passenger in train data is:',round(Train.Count.std()))\nprint('S.D. of passenger in test data is:',round(valid.Count.std()))","a2fd0de8":"# Estimating trend\nTrain_log = np.log(Train.Count)\nvalid_log = np.log(valid.Count)\n# The below transformation is required to make series stationary\nmovingAverage = Train_log.rolling(window=30).mean()\nplt.figure(figsize=(10,5))\nplt.plot(Train_log, color='black', label='Log of passenger count')\nplt.plot(movingAverage, color='yellow', label='Moving Average (30)')\nplt.xlabel('Datetime')\nplt.ylabel('Log # of passenger')\nplt.legend(fontsize='large')\nplt.show()","cf7a6fbc":"# subtracting the log values from moving average\nTrain_log_moving_avg = Train_log - movingAverage\n\n#Remove NAN values\nTrain_log_moving_avg.dropna(inplace=True)\nTrain_log_moving_avg.head(10)","78fbbed0":"test_stationarity(Train_log_moving_avg)","cadf2aaa":"# here we subtract value from its previous value to make data stationary\nTrain_log_diff = Train_log - Train_log.shift(1)\ntest_stationarity(Train_log_diff.dropna())","210753dc":"# decomposing the 'log data' and checking the stationarity of residual\n\ndecomposition = seasonal_decompose(pd.DataFrame(Train_log).Count)\ntrend = decomposition.trend\nseasonal = decomposition.seasonal\nresidual = decomposition.resid\n\nplt.figure(figsize=(10,6))\nplt.subplot(411)\nplt.plot(Train_log, label='Original')\nplt.legend()\nplt.subplot(412)\nplt.plot(trend, label='Trend')\nplt.legend()\nplt.subplot(413)\nplt.plot(seasonal, label='Seasonal')\nplt.legend()\nplt.subplot(414)\nplt.plot(residual, label='Residual')\nplt.legend()\nplt.tight_layout()\nplt.show()","229675a5":"#there can be cases where an observation simply consisted of trend & seasonality. In that case, there won't be \n#any residual component & that would be a null or NaN. Hence, we also remove such cases.\n\ntrain_log_decompose = residual\ntrain_log_decompose.dropna(inplace=True)\ntest_stationarity(train_log_decompose)","ffba6e30":"lag_acf = acf(Train_log_diff.dropna(), nlags=31)\nlag_pacf = pacf(Train_log_diff.dropna(), nlags=31, method='ols')\nplt.figure(figsize=(15,5))\n\n#Plot ACF:\nplt.subplot(121)\nplt.plot(lag_acf)\nplt.axhline(y=0, linestyle='--', color='gray')\nplt.axhline(y=-1.96\/np.sqrt(len(Train_log_diff.dropna())), linestyle='--', color='gray')\nplt.axhline(y=1.96\/np.sqrt(len(Train_log_diff.dropna())), linestyle='--', color='gray')\nplt.title('Autocorrelation Function', size=15)            \n\n#Plot PACF\nplt.subplot(122)\nplt.plot(lag_pacf)\nplt.axhline(y=0, linestyle='--', color='gray')\nplt.axhline(y=-1.96\/np.sqrt(len(Train_log_diff.dropna())), linestyle='--', color='gray')\nplt.axhline(y=1.96\/np.sqrt(len(Train_log_diff.dropna())), linestyle='--', color='gray')\nplt.title('Partial Autocorrelation Function', size=15)\nplt.tight_layout()\nplt.show()","9b30f714":"fig = plt.figure(figsize=(12,8))\nax1 = fig.add_subplot(211)\nfig = sm.graphics.tsa.plot_acf(Train_log_diff.dropna(), lags=30, ax=ax1) \nax2 = fig.add_subplot(212)\nfig = sm.graphics.tsa.plot_pacf(Train_log_diff.dropna(), lags=30, ax=ax2)","bcec8d09":"model = ARIMA(Train_log, order=(7,1,0))\nresults_AR = model.fit(disp=-1, trend='c')\nplt.figure(figsize=(10,5))\nplt.plot(Train_log_diff.dropna(), color='black',label='Original')\nplt.plot(results_AR.fittedvalues, color='red', label='Predictions')\nplt.title('Plotting AR model on train data', size=15)\nplt.legend(fontsize='large')\nplt.show()","92294412":"AR_prediction = results_AR.predict(start='2014-06-25', end='2014-09-25')\nAR_prediction = AR_prediction.cumsum().shift().fillna(0)\nAR_prediction1 = pd.Series(np.ones(valid.shape[0])*np.log(valid['Count'])[0], index=valid.index)\nAR_prediction1 = AR_prediction1.add(AR_prediction,fill_value=0)\nAR_prediction = np.exp(AR_prediction1)\n\nplt.figure(figsize=(10,5))\nplt.plot(valid.Count, label='Valid', color='lightgrey')\nplt.plot(AR_prediction, label='Prediction', color='red')\nplt.title('Actual vs Predicted on test data with RMSE:{}'.\n          format(round(np.sqrt(np.dot(valid.Count,AR_prediction))\/valid.shape[0],2)), size=15)\nplt.legend(fontsize='large')\nplt.show()","49a80bb8":"model = ARIMA(Train_log, order=(0,1,1))\nresults_MA = model.fit(disp=-1, trend='c')\nplt.figure(figsize=(10,5))\nplt.plot(Train_log_diff.dropna(), label='Original', color='black')\nplt.plot(results_MA.fittedvalues, color='red', label='Predictions')\nplt.title('Plotting MA model on train data', size=15)\nplt.legend(fontsize='large')\nplt.show()","7d2fdbac":"MA_prediction = results_MA.predict(start='2014-06-25', end='2014-09-25')\nMA_prediction = MA_prediction.cumsum().shift().fillna(0)\nMA_prediction1 = pd.Series(np.ones(valid.shape[0])*np.log(valid['Count'])[0], index=valid.index)\nMA_prediction1 = MA_prediction1.add(MA_prediction,fill_value=0)\nMA_prediction = np.exp(MA_prediction1)\n\nplt.figure(figsize=(10,5))\nplt.plot(valid.Count, label='Valid', color='lightgrey')\nplt.plot(MA_prediction, label='Prediction', color='red')\nplt.title('Actual vs Predicted on test data with RMSE:{}'.\n          format(round(np.sqrt(np.dot(valid.Count,MA_prediction))\/valid.shape[0],2)), size=15)\nplt.legend(fontsize='large')\nplt.show()","d64be533":"# Running different values of p & q and checking their AIC\n\np = range(1,8)\nq = range(1,5)\nfor i in p:\n    for j in q:\n        try:\n            mod = ARIMA(Train_log, order=(i,1,j))\n            results = mod.fit()\n            print('ARIMA ({},1,{}) - AIC:{}'.format(i, j, round(results.aic,2)))\n        except:\n            continue","de37fbd7":"model = ARIMA(Train_log, order=(6,1,3))\nresults_ARIMA = model.fit(disp=-1, trend='c')\nplt.figure(figsize=(10,5))\nplt.plot(Train_log_diff.dropna(), label='Original', color='black')\nplt.plot(results_ARIMA.fittedvalues, color='red', label='Predictions')\nplt.title('Plotting ARIMA model on train data', size=15)\nplt.legend(fontsize='large')\nplt.show()","479b4369":"results_ARIMA.summary()","2026ff5b":"ARIMA_prediction = results_ARIMA.predict(start='2014-06-25', end='2014-09-25')\nARIMA_prediction = ARIMA_prediction.cumsum().shift().fillna(0)\nARIMA_prediction1 = pd.Series(np.ones(valid.shape[0])*np.log(valid['Count'])[0], index=valid.index)\nARIMA_prediction1 = ARIMA_prediction1.add(ARIMA_prediction,fill_value=0)\nARIMA_prediction = np.exp(ARIMA_prediction1)\n\nplt.figure(figsize=(10,5))\nplt.plot(valid.Count, label='Valid', color='lightgrey')\nplt.plot(ARIMA_prediction, label='Prediction', color='red')\nplt.title('Actual vs Predicted on test data with RMSE:{}'.\n          format(round(np.sqrt(np.dot(valid.Count,ARIMA_prediction))\/valid.shape[0],2)), size=15)\nplt.legend(fontsize='large')\nplt.show()","103d4531":"# Forecast\nfc, se, conf = results_ARIMA.forecast(213, alpha=0.05)  # 95% conf\n\n# Make as pandas series\nfc_series = pd.Series(np.exp(fc))\ntest = test.reset_index()\ntest['Count'] = fc_series\ntest = test.set_index('Datetime')\nlower_series = pd.Series(np.exp(conf[:, 0]))\nupper_series = pd.Series(np.exp(conf[:, 1]))\n\n# Plot\nplt.figure(figsize=(12,5))\nplt.plot(Train.Count, label='Train', color='black')\nplt.plot(valid.Count, label='Valid', color='lightgrey')\nplt.plot(test.Count, label='Forecast', color='red')\nplt.title('Actuals vs Valid vs Forecasted', size=15)\nplt.legend(fontsize='large', loc='upper left')\nplt.show()","b62419a2":"---\n# <span style=\"background-color:blue\"><font color='white'>III. Simple Exponential Smoothing<\/font>\n- `no trend no seasonality`\n- This is the second well known method to produce a smoothed Time Series. Exponential Smoothing assigns exponentially decreasing weights as the observation get older.\n- Exponential smoothing is usually a way of \u201csmoothing\u201d out the data by removing much of the \u201cnoise\u201d (random effect) from the data by giving a better forecast.\n- Forecasts are calculated using weighted averages where the weights decrease exponentially as observations come from further in the past, the smallest weights are associated with the oldest observations\n![image.png](attachment:image.png)\nwhere 0\u2264 \u03b1 \u22641 is the smoothing parameter.\n- If you have a time series that can be described using an additive model with constant level and no seasonality, you can use simple exponential smoothing to make short-term forecast.\n- **If we give entire weight to the last observed value only this method will be similar to the naive approach.**","61a2ec36":"##### Checking the RMSE with different window size","8c3fa0bd":"#### From now onwards -  \n- <font color='black'>**Train data is represented in black line**<\/font>\n- <font color='grey'>**Valid data is presented in grey line**, and<\/font>\n- <font color='red'>**Predictions using any model will be represented in red line.**<\/font>","947d620e":"---\n# <font color='black'>Your Time Series Starts Now!!<\/font>","bd0665cc":"#### Multiplicative Trend","2ecfb31f":"#### ARIMA (6,1,3)\nWe test and try different combination of p & q where AIC is minimum and also all the variables should be significant.","84aa80a9":"---\n# <span style=\"background-color:blue\"><font color='white'>VI. ARIMA (Autoregressive Integrated Moving Average)<\/font>\n1. One of the most common methods used in time series forecasting is known as the ARIMA model, which stands for AutoregRessive Integrated Moving Average. ARIMA is a model that can be fitted to time series data in order to better understand or predict future points in the series.\n2. There are three distinct integers `(p, d, q)` that are used to parametrize ARIMA models. Because of that, ARIMA models are denoted with the notation ARIMA(p, d, q). Together these three parameters account for seasonality, trend, and noise in datasets:\n    - **p is the auto-regressive part of the model**. Autoregressive models operate under the premise that past values have an effect on current values. It allows us to incorporate the effect of past values into our model. Intuitively, this would be similar to stating that it is likely to be warm tomorrow if it has been warm the past 3 days. The order of the AR model corresponds to the number of days incorporated in the formula.\n    - **d is the integrated part of the model**. This includes terms in the model that incorporate the amount of differencing (i.e. the number of past time points to subtract from the current value) to apply to the time series. Intuitively, this would be similar to stating that it is likely to be same temperature tomorrow if the difference in temperature in the last three days has been very small. Differencing subtracts the current value from the previous and can be used to transform a time series into one that\u2019s stationary. For example, first-order differencing addresses linear trends, and employs the transformation zi = yi \u2014 yi-1. Second-order differencing addresses quadratic trends and employs a first-order difference of a first-order difference, namely zi = (yi \u2014 yi-1) \u2014 (yi-1 \u2014 yi-2), and so on.\n    - **q is the moving average part of the model**. This allows us to set the error of our model as a linear combination of the error values observed at previous time points in the past. Assumes the value of the dependent variable on the current day depends on the previous days error terms.","b9dde7f1":"- We have thus tried out 2 different transformation: log & time shift. For simplicity, we will go with the log scale. The reason for doing this is that we can revert back to the original scale during forecasting.\n- Let us now break down the 3 components of the log scale series using a system libary function. Once, we separate our the components, we can simply ignore trend & seasonality and check on the nature of the residual part.","961a7ebd":"#### 4. Building Model","11aab96f":"- Here we can see the acf and pacf both has a recurring pattern every 7 periods. Indicating a weekly pattern exists. \n- Any time you see a regular pattern like that in one of these plots, you should suspect that there is some sort of significant seasonal thing going on. Then we should start to consider SARIMA to take seasonality into accuont","36462a13":"---\n# <span style=\"background-color:blue\"><font color='white'>V. Holt's Winter Model\/Triple Exponential Smoothing<\/font>\n- `Trend is present,Seasonality is present`. Trend and seasonality can be of two types: additive and multiplicative\n- The idea behind triple exponential smoothing(Holt\u2019s Winter) is to apply exponential smoothing to the seasonal components in addition to level and trend.\n- Using Holt\u2019s winter method will be the best option among the rest of the models beacuse of the seasonality factor. The Holt-Winters seasonal method comprises the forecast equation and three smoothing equations \u2014 one for the level \u2113t, one for trend bt and one for the seasonal component denoted by st, with smoothing parameters \u03b1, \u03b2 and \u03b3.\n![image.png](attachment:image.png)\n-  In Halts\u2019 Winter Method there is three smoothing parameters alpha( \u03b1),beta( \u03b2),gamma( \u03b3). The default value of three smoothing parameters are respectively \u03b1=0.2, \u03b2=0.15 and \u03b3 = 0.05.\n- The range of three smoothing parameters are - \u03b1: 0\u2264\u03b1\u22641, for \u03b2 : 0\u2264\u03b2\u22641, for \u03b3: 0\u2264\u03b3\u22641\n- In this method also, we can implement both additive and multiplicative technique.  The additive method is preferred when the seasonal variations are roughly constant through the series, while the multiplicative method is preferred when the seasonal variations are changing proportional to the level of the series","4556d20e":"### <span style=\"background-color:black\"><font color='white'>How SARIMAX works? How it is different from ARIMA?<\/font>\n- SARIMAX is nothing different from ARIMA. It is just an extension with seasonal order. You have to call SARIMAX function and add one more parameter \"seasonal_order\".\n- SARIMAX = Seasonal + ARIMAX, where seasonal order also considered\n- E.g. SARIMAX = sm.tsa.statespace.SARIMAX(endog = Passenger count, order = (p,d,q), seasonal_order = (P,D,Q,s), exog = Weather).fit()","5fccf5ab":"---\n# <span style=\"background-color:blue\"><font color='white'>II. Moving Average<\/font>\n- Basically, a simple moving average is calculated by adding up the last \u2018n\u2019 period\u2019s values and then dividing that number by \u2018n\u2019. So the moving average value is considering as the forecast for next period.\n- We will take the average of the count for last few time periods only. Obviously the thinking here is that only the recent values matter. Such forecasting technique which uses window of time period for calculating the average is called Moving Average technique. Calculation of the moving average involves what is sometimes called a \u201csliding window\u201d of size n.\n- Using a simple moving average model, we forecast the next value(s) in a time series based on the average of a fixed finite number \u2018p\u2019 of the previous values. Thus, for all i > p\n![image.png](attachment:image.png)\n`A moving average can actually be quite effective, especially if you pick the right p for the series.`","b9660431":"#### Automatically finding the value of `\u03b1`\nFor this we set *optimized=True*","4a530f9f":"#### Splitting the Data\n- Now we will divide the data into `train` and `validation`. We will make a model on train data and use validation data for prediction to check for accuracy. `test` is different dataset for which we will make final prediction.\n- We will take last 3 months as validation set and rest as training set.","44bbd32d":"##### B. Shift transformation","528ab892":"#### 3. Finding the value of p & q using ACF and PACF plots \n> ACF (Auto Correlation Function)\n`Auto Correlation function takes into consideration of all the past observations irrespective of its effect on the future or present time period.` It calculates the correlation between the t and (t-k) time period. It includes all the lags or intervals between t and (t-k) time periods. Correlation is always calculated using the Pearson Correlation formula.\n\n> PACF (Partial Correlation Function)\nThe PACF determines the partial correlation between time period t and t-k. It doesn\u2019t take into consideration all the time lags between t and t-k. For e.g. let's assume that today's stock price may be dependent on 3 days prior stock price but it might not take into consideration yesterday's stock price closure. `Hence we consider only the time lags having a direct impact on future time period by neglecting the insignificant time lags in between the two-time slots t and t-k.`","f5499a10":"##### <span style=\"background-color:black\"><font color='white'>1. What is Time series analysis?<\/font>\nTime Series is a series of observations taken at specified time intervals usually equal intervals. Analysis of the series helps us to predict future values based on previous observed values. `In Time series, we have only 2 variables, time & the variable we want to forecast.`\n\n##### <span style=\"background-color:black\"><font color='white'>2. Why & where Time Series is used?<\/font>\nTime series data can be analysed in order to extract meaningful statistics and other charecteristsics. It's used in atleast the 4 scenarios:<br>\n    a) Business Forecasting<br>\n    b) Understand past behavior<br>\n    c) Plan the future<br>\n    d) Evaluate current accomplishment<br>\n\n##### <span style=\"background-color:black\"><font color='white'>3. When shouldn't we use Time Series Analysis?<\/font>\nWe don't need to apply Time series in atleast the following 2 cases:<br>\n    a) The dependant variable(y) (that is supposed to vary with time) is constant. Eq: y=f(x)=4, a line parallel to x-axis(time) will always remain the same.<br>\n    b) The dependant variable(y) represent values that can be denoted as a mathematical function. Eq: sin(x), log(x), Polynomials etc. Thus, we can directly get value at some time using the function itself. No need of forecasting.\n    \n##### <span style=\"background-color:black\"><font color='white'>4. What are the components of Time Series?<\/font>\nThere are 4 components:<br>\na) Trend - Upward & downward movement of the data with time over a large period of time. Eq: Appreciation of Dollar vs rupee.<br>\nb) Seasonality - seasonal variances. Eq: Ice cream sales increases in Summer only<br>\nc) Noise or Irregularity - Spikes & troughs at random intervals<br>\nd) Cyclicity - behavior that repeats itself after large interval of time, like months, years etc.<br>\n**Note**:Many people confuse cyclic behaviour with seasonal behaviour, but they are really quite different. If the fluctuations are not of fixed period then they are cyclic; if the period is unchanging and associated with some aspect of the calendar, then the pattern is seasonal. In general, the average length of cycles is longer than the length of a seasonal pattern, and the magnitude of cycles tends to be more variable than the magnitude of seasonal patterns.\n\n##### <span style=\"background-color:black\"><font color='white'>5. How forecasting is different from prediction?<\/font>\na) A `prediction is a definitive and specific statement` about when and where an earthquake will strike: a major earthquake will hit Tokyo, Japan, on June 28. Whereas a `forecast is a probabilistic statement`, usually over a longer time scale: there is a 60 percent chance of an earthquake in Southern California over the next thirty years.<br>\nb) There is only one difference between these two in time series. `Forecasting pertains to out of sample observations`, whereas `prediction pertains to in sample observations`. Predicted values (and by that I mean OLS predicted values) are calculated for observations in the sample used to estimate the regression. However, forecast is made for the some dates beyond the data used to estimate the regression, so the data on the actual value of the forecasted variable are not in the sample used to estimate the regression.","b37ab2c5":"#### Manually finding the value of `\u03b1` w.r.t min RMSE","90d3f3d9":"#### 5. Forecaste (out-of-sample)","81efef09":"##### Observations:\n- x-axis represents the lag value. x=0 is value itself, x=1 is lag1, x=2 is lag2, and so on.\n- y-axis represents the correlation number. For x=0 the correlation is 1 because a value can be 100% correlated to themselve. For x=1 it represents the correlation between today's and yesterday value, and so on.\n- From the ACF graph, we see that curve touches y=0.0 line at x=3. Thus, from theory, Q = 3 \n- From the PACF graph, we see that curve touches y=0.0 line at x=7. Thus, from theory, P = 7\n- ARIMA is AR + I + MA. Before, we see an ARIMA model, let us check the results of the individual AR & MA model. Note that, these models will give a value of RSS. Lower RSS values indicate a better model.","2ce7669c":"#### AR (7,1,0)","c0926353":"#### Comment:\n- We can see that the time series is becoming more and more stable when you are aggregating it on daily, weekly and monthly basis.\n- But it would be difficult to convert the monthly and weekly predictions to hourly predictions, as first we have to convert monthly predictions to weekly, weekly to daily and daily to hourly predictions which will become very expanded process. `So we will work on daily time series.`","8415708e":"> **The general process for ARIMA models is the following** - \n1. Visualize the time series data\n2. Make the time series data stationary (if required)\n3. Plot the Correlation and Autocorrelation Charts to get the value of p & q\n4. Construct the ARIMA or Seasonal ARIMA model based upon the data\n5. Use the model to make predictions","94286f99":"##### A. Log transformation","fb6e36db":"#### 2. Checking if data is stationary or not\n- Stationarity refers when the statistical values like mean, standard deviation does not change over the time.\n- For this, we write a function that check if the data is stationary or not. If it is stationary, good and we build ARIMA model else first we have to make data stationary (either by log transformation or shifting) and then build model.","31cfa9bc":"# <span style=\"background-color:black\"><font color='white'>What's in this Notebook<\/font>\nThe data in this notebook can be used for time series analysis. Here I perform different time series models with their explanation and proper comment. Topic includes -\n1. *Theory of Time Series*\n2. *Naive Approach*\n3. *Moving Average*\n4. *Simple Exponential Smoothing*\n5. *Holt's Linear Trend Model*\n6. *Holt's Winter Model*\n7. *ARIMA\/ARIMAX\/SARIMAX*\n\n**`If you like this notebook UPVOTE it and for any reviews and doubts please leave a comment.`**","245e3eee":"#### Additive Trend","ae16de5b":"### <span style=\"background-color:black\"><font color='white'>How ARIMAX works? How it is different from ARIMA?<\/font>\n- ARIMAX is nothing different from ARIMA. It is just an extension. You have to call ARIMA function and add one more parameter \"exog\".\n- ARIMAX = ARIMA + X, where X=exogeneous variable\n- E.g. ARIMAX = ARIMA(endog = Passenger count, order = (p,d,q), exog = Weather)","fbbce5df":"##### <span style=\"background-color:black\"><font color='white'>Some important links:<\/font>\n\n- [How to Make Manual Predictions for ARIMA Models with Python](https:\/\/machinelearningmastery.com\/make-manual-predictions-arima-models-python\/)\n\n- [How to Make Out-of-Sample Forecasts with ARIMA in Python](https:\/\/machinelearningmastery.com\/make-sample-forecasts-arima-python\/#:~:text=4.-,One%2DStep%20Out%2Dof%2DSample%20Forecast,used%20to%20fit%20the%20model)\n\n- [Forecasting in statsmodels](https:\/\/www.statsmodels.org\/dev\/examples\/notebooks\/generated\/statespace_forecasting.html#:~:text=Prediction%20vs%20Forecasting,-The%20results%20objects&text=The%20predict%20method%20only%20returns,results%20(similar%20to%20get_forecast%20).)\n\n- [Statsmodels ARIMA - Different results using predict() and forecast()](https:\/\/stackoverflow.com\/questions\/45596492\/statsmodels-arima-different-results-using-predict-and-forecast)\n\n- [ARIMA Model \u2013 Complete Guide to Time Series Forecasting in Python](https:\/\/www.machinelearningplus.com\/time-series\/arima-model-time-series-forecasting-python\/)\n\n- [Time Series Analysis in Python \u2013 A Comprehensive Guide with Examples](https:\/\/www.machinelearningplus.com\/time-series\/time-series-analysis-python\/)\n\n- [Introduction to ARIMA: nonseasonal models](https:\/\/people.duke.edu\/~rnau\/411arim.htm)\n\n- [ARIMA models with regressors](https:\/\/people.duke.edu\/~rnau\/arimreg.htm)\n\n- [What is ARIMAX Forecasting and How is it Used for Enterprise Analysis?](https:\/\/www.slideshare.net\/ElegantJ-BusinessIntelligence\/what-is-arimax-forecasting-and-how-is-it-used-for-enterprise-analysis)\n\n- [Convert 'intercept\/drift' term to 'constant' term in arima\/auto.arima functions, in case of higher orders - R](https:\/\/stats.stackexchange.com\/questions\/252718\/convert-intercept-drift-term-to-constant-term-in-arima-auto-arima-functions)","96c568fc":"---\n# <span style=\"background-color:blue\"><font color='white'>I. Naive Approach<\/font>\n- In this forecasting technique, we assume that the **next expected point is equal to the last observed point**. So we can expect a straight horizontal line as a prediction.\n- It's naive because all we're doing is, we're saying what we sold yesterday, that's how much we're going to sell today. What we sell today is the number we're going to project for tomorrow. It doesn't take a lot. And it's very simple and easy to use.\n- If we sold 20 croissants yesterday, we're going to sell 20 croissants today. If we sell 20 croissants today, we're going to sell 20 croissants tomorrow. The naive forecast works very well in certain situations. And it sometimes works even better than other more complicated methods.\n- `Mathematically: Forecast(t+1) = Demand(t)`","e610edfc":"#### MA (0,1,1)","29ad2d85":"#### 1. Visualize the time series\nHere, we decompose the time series into 4 parts -\n1. Observed: Original time series\n2. Trend: Shows the trend in the time series i.e. increasing or decreasing behaviour of the time series\n3. Seasonal: Which tells us about the seasonality in the time series\n4. Residual: Noise in the data or obtained by removing any trend or seasonality in the time series","8a7c57ec":"---\n# <span style=\"background-color:blue\"><font color='white'>IV. Holt's Linear Trend Model<\/font>\n- `Trend is present, but no seasonality`. Trend can be of two types: additive and multiplicative\n- It is an extension of simple exponential smoothing to allow forecasting of data with a trend (no seasonality).\n- The forecast function in this method is a function of level and trend.\n![image.png](attachment:image.png)\n\u03b1 and \u03b2 are smoothing parameters of HLTM. It is always better to choose default value of \u03b1 and \u03b2 respectively 0.2 and 0.15.\n- The range of \u03b1 should lie between 0 \u2264 \u03b1 \u2264 1 and range of \u03b2 should lie between 0 \u2264 \u03b2 \u2264 1.\n- We will add these equations to generate Forecast equation. We can also generate a multiplicative forecast equation by multiplying trend and level instead of adding it. \n- When the trend increases or decreases linearly, additive equation is used whereas when the trend increases of decreases exponentially, multiplicative equation is used. \n- Practice shows that multiplicative is a more stable predictor, the additive method however is simpler to understand."}}