{"cell_type":{"d8eaf7d0":"code","3aa3f49d":"code","53ef3003":"code","e17a3286":"code","facf9083":"code","f1f5b088":"code","856e5147":"code","e38b4018":"code","f993ef57":"code","c60722a6":"code","d0759702":"code","7c63aa05":"code","66ec27f3":"code","ded49513":"code","455eecf0":"code","80b3431f":"code","96cba6e9":"code","6b853bf1":"code","712f2cfa":"code","64f9c251":"code","b13fd652":"code","5cf87ef2":"code","6e25162f":"code","c91c7828":"code","b2ffce20":"code","28fa12f4":"code","e55373ca":"code","7cf81bba":"code","d3c41a83":"code","58baecc9":"code","9bf2325d":"code","6946c230":"code","0f92915a":"code","53bffa45":"code","f40043f8":"code","d5317893":"code","b36fbef3":"code","2fae523d":"code","0afa8b4c":"code","fcea6ed4":"code","8c38f31f":"code","778e3b05":"code","cc9adec0":"code","3adf8e84":"code","d43333f5":"code","4ac10672":"code","85172fdd":"code","9324481d":"code","17a8c354":"code","9a757021":"code","ed0aff06":"code","6dc9c0a0":"code","55f2249f":"code","b7a00c5b":"code","a78b7167":"code","57f97ed2":"code","2abd6d7f":"code","f7d63569":"code","2ca71cf7":"code","3cf79516":"code","6d37912c":"code","04c2b875":"code","5b138e78":"code","cb2a9a45":"code","55949dca":"code","1f8c8ea2":"code","64b4ebc4":"code","8d6e6c85":"code","40d212cc":"code","0678ae3c":"code","77a10132":"code","f5ca55b0":"code","8f98b91c":"code","6007e6b9":"code","d06ce7f0":"code","04e93b12":"code","3e420152":"markdown","f2be7d44":"markdown","239cf31f":"markdown","2cbc19a6":"markdown","c851d0bd":"markdown","d2d6c7a9":"markdown","45bf2b47":"markdown","a4aa727f":"markdown","eae01bc2":"markdown","c5064f05":"markdown","bcb963b0":"markdown","07b77924":"markdown","0e52f51b":"markdown","fac86d12":"markdown","41a75f42":"markdown","7b687d77":"markdown","e4c03fac":"markdown","97898e45":"markdown","44044fed":"markdown","bc464ec5":"markdown","22a1bdae":"markdown","685e43eb":"markdown","dec8f407":"markdown","f8aae35c":"markdown","0b3a370d":"markdown","db8be83e":"markdown","cabbc186":"markdown","9a9addc2":"markdown","d79ea98d":"markdown","8e88a312":"markdown","022b3d89":"markdown","242d4b08":"markdown","c8d008de":"markdown","5ad3558b":"markdown","6caa4046":"markdown","fe6a0562":"markdown","70358c0c":"markdown","232c1594":"markdown","cc54d9a2":"markdown","0a6a8024":"markdown","14462e85":"markdown","0ed0d64f":"markdown","fa8c99e6":"markdown","0fab6cec":"markdown","a03c69cb":"markdown","735c911f":"markdown","d9331a63":"markdown","5e05c084":"markdown","0ad11eaf":"markdown","9be6474b":"markdown","c7179eaa":"markdown","a97e55ef":"markdown","a7163dca":"markdown","c33c4490":"markdown","ac5f3147":"markdown","a98b652b":"markdown","b89399ba":"markdown"},"source":{"d8eaf7d0":"%matplotlib inline\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport numpy as np\nimport pandas as pd\nimport os\nimport datetime\nfrom sklearn.preprocessing import MinMaxScaler","3aa3f49d":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Input, Dense, GRU, Embedding\nfrom tensorflow.keras.optimizers import RMSprop\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau\nfrom tensorflow.keras.backend import square, mean","53ef3003":"tf.__version__","e17a3286":"tf.keras.__version__","facf9083":"pd.__version__","f1f5b088":"!ls \"..\/input\/ca-data\"","856e5147":"path = '..\/input\/ca-data\/'\nca1_data = pd.read_csv(path+\"CA1_ext.csv\")\nca2_data = pd.read_csv(path+\"CA2_ext.csv\")\nca3_data = pd.read_csv(path+\"CA_3_ext.csv\")\nca4_data = pd.read_csv(path+\"CA4_ext.csv\")\ntx1_data = pd.read_csv(path+\"TX_1_ext.csv\")\ntx2_data = pd.read_csv(path+\"TX_2_ext.csv\")\ntx3_data = pd.read_csv(path+\"TX_3_ext.csv\")\nwi1_data = pd.read_csv(path+\"WI_1_ext.csv\")\nwi2_data = pd.read_csv(path+\"WI_2_ext.csv\")\nwi3_data = pd.read_csv(path+\"WI_3_ext.csv\")\n","e38b4018":"data = {}\ndata[\"CA_1\"] = ca1_data[[\"Hobbie_revenue\",\"House_revenue\",\"Foods_revenue\"]]\ndata[\"CA_2\"] = ca2_data[[\"Hobbie_revenue\",\"House_revenue\",\"Foods_revenue\"]]\ndata[\"CA_3\"] = ca3_data[[\"Hobbie_revenue\",\"House_revenue\",\"Foods_revenue\"]]\ndata[\"CA_4\"] = ca4_data[[\"Hobbie_revenue\",\"House_revenue\",\"Foods_revenue\"]]\ndata[\"TX_1\"] = tx1_data[[\"Hobbie_revenue\",\"House_revenue\",\"Foods_revenue\"]]\ndata[\"TX_2\"] = tx2_data[[\"Hobbie_revenue\",\"House_revenue\",\"Foods_revenue\"]]\ndata[\"TX_3\"] = tx3_data[[\"Hobbie_revenue\",\"House_revenue\",\"Foods_revenue\"]]\ndata[\"WI_1\"] = wi1_data[[\"Hobbie_revenue\",\"House_revenue\",\"Foods_revenue\"]]\ndata[\"WI_2\"] = wi2_data[[\"Hobbie_revenue\",\"House_revenue\",\"Foods_revenue\"]]\ndata[\"WI_3\"] = wi3_data[[\"Hobbie_revenue\",\"House_revenue\",\"Foods_revenue\"]]\ndata","f993ef57":"data[\"CA_1\"].head()","c60722a6":"listofstore = [\"CA_1\",\"CA_2\",\"CA_3\",\"CA_4\",\"TX_1\",\"TX_2\",\"TX_3\",\"WI_1\",\"WI_2\",\"WI_3\"]\nlistofstore","d0759702":"data_temp = data[\"CA_1\"].join(data[\"CA_2\"], lsuffix='_CA_1', rsuffix='_CA_2')\nfor store in listofstore[2:]:\n    data_temp1 = data_temp.join(data[store], lsuffix='', rsuffix=store)\n    data_temp1 = data_temp1.rename(columns={\"Hobbie_revenue\": \"Hobbie_revenue_\"+store,\"House_revenue\": \"House_revenue_\"+store,\"Foods_revenue\": \"Foods_revenue_\"+store})\n    data_temp = data_temp1\n    \n#data_temp = data_temp1.join(data[\"TX_1\"], lsuffix='', rsuffix='_TX_1')\n\ndata_df = data_temp1.copy()","7c63aa05":"data_df.head()","66ec27f3":"data_df.values.shape","ded49513":"import datetime\nnumdays = 1913\nbase = datetime.datetime(2011, 1, 29)\ndate_list = [base + datetime.timedelta(days=x) for x in range(numdays)]","455eecf0":"from datetime import datetime\ndayofyearlist = [i.timetuple().tm_yday for i in date_list]","80b3431f":"data_df[\"Dayofyear\"] = dayofyearlist","96cba6e9":"data_df","6b853bf1":"target_store = 'CA_1'","712f2cfa":"target_names = ['Hobbie_revenue', 'House_revenue', 'Foods_revenue']","64f9c251":"shift_months = 1\nshift_steps = shift_months * 30  # Number of days.","b13fd652":"data_targets = data[target_store][target_names].shift(-shift_steps)","5cf87ef2":"data[target_store][target_names].head(shift_steps + 5)","6e25162f":"data_targets.head(5)","c91c7828":"data_targets.tail()","b2ffce20":"data_df.values","28fa12f4":"x_data = data_df.values[0:-shift_steps]","e55373ca":"print(type(x_data))\nprint(\"Shape:\", x_data.shape)","7cf81bba":"y_data = data_targets.values[:-shift_steps]\ny_data","d3c41a83":"print(type(y_data))\nprint(\"Shape:\", y_data.shape)","58baecc9":"num_data = len(x_data)\nnum_data","9bf2325d":"train_split = 0.9","6946c230":"num_train = int(train_split * num_data)\nnum_train","0f92915a":"num_test = num_data - num_train\nnum_test","53bffa45":"x_train = x_data[0:num_train]\nx_test = x_data[num_train:]\nlen(x_train) + len(x_test)","f40043f8":"y_train = y_data[0:num_train]\ny_test = y_data[num_train:]\nlen(y_train) + len(y_test)","d5317893":"num_x_signals = x_data.shape[1]\nnum_x_signals","b36fbef3":"num_y_signals = y_data.shape[1]\nnum_y_signals","2fae523d":"print(\"Min:\", np.min(x_train))\nprint(\"Max:\", np.max(x_train))","0afa8b4c":"x_scaler = MinMaxScaler()","fcea6ed4":"x_train_scaled = x_scaler.fit_transform(x_train)","8c38f31f":"print(\"Min:\", np.min(x_train_scaled))\nprint(\"Max:\", np.max(x_train_scaled))","778e3b05":"x_test_scaled = x_scaler.transform(x_test)","cc9adec0":"y_scaler = MinMaxScaler()\ny_train_scaled = y_scaler.fit_transform(y_train)\ny_test_scaled = y_scaler.transform(y_test)","3adf8e84":"print(x_train_scaled.shape)\nprint(y_train_scaled.shape)","d43333f5":"def batch_generator(batch_size, sequence_length):\n    \"\"\"\n    Generator function for creating random batches of training-data.\n    \"\"\"\n\n    # Infinite loop.\n    while True:\n        # Allocate a new array for the batch of input-signals.\n        x_shape = (batch_size, sequence_length, num_x_signals)\n        x_batch = np.zeros(shape=x_shape, dtype=np.float16)\n\n        # Allocate a new array for the batch of output-signals.\n        y_shape = (batch_size, sequence_length, num_y_signals)\n        y_batch = np.zeros(shape=y_shape, dtype=np.float16)\n\n        # Fill the batch with random sequences of data.\n        for i in range(batch_size):\n            # Get a random start-index.\n            # This points somewhere into the training-data.\n            idx = np.random.randint(num_train - sequence_length)\n            \n            # Copy the sequences of data starting at this index.\n            x_batch[i] = x_train_scaled[idx:idx+sequence_length]\n            y_batch[i] = y_train_scaled[idx:idx+sequence_length]\n        \n        yield (x_batch, y_batch)","4ac10672":"batch_size = 256","85172fdd":"sequence_length = 30 * 6\nsequence_length","9324481d":"generator = batch_generator(batch_size=batch_size,\n                            sequence_length=sequence_length)","17a8c354":"x_batch, y_batch = next(generator)","9a757021":"print(x_batch.shape)\nprint(y_batch.shape)","ed0aff06":"batch = 0   # First sequence in the batch.\nsignal = 0  # First signal from the 20 input-signals.\nseq = x_batch[batch, :, signal]\nplt.plot(seq)","6dc9c0a0":"seq = y_batch[batch, :, signal]\nplt.plot(seq)","55f2249f":"validation_data = (np.expand_dims(x_test_scaled, axis=0),\n                   np.expand_dims(y_test_scaled, axis=0))","b7a00c5b":"model = Sequential()","a78b7167":"model.add(GRU(units=512,\n              return_sequences=True,\n              input_shape=(None, num_x_signals,)))","57f97ed2":"model.add(Dense(num_y_signals, activation='sigmoid'))","2abd6d7f":"if False:\n    from tensorflow.python.keras.initializers import RandomUniform\n\n    # Maybe use lower init-ranges.\n    init = RandomUniform(minval=-0.05, maxval=0.05)\n\n    model.add(Dense(num_y_signals,\n                    activation='linear',\n                    kernel_initializer=init))","f7d63569":"warmup_steps = 30","2ca71cf7":"def loss_mse_warmup(y_true, y_pred):\n    \"\"\"\n    Calculate the Mean Squared Error between y_true and y_pred,\n    but ignore the beginning \"warmup\" part of the sequences.\n    \n    y_true is the desired output.\n    y_pred is the model's output.\n    \"\"\"\n\n    # The shape of both input tensors are:\n    # [batch_size, sequence_length, num_y_signals].\n\n    # Ignore the \"warmup\" parts of the sequences\n    # by taking slices of the tensors.\n    y_true_slice = y_true[:, warmup_steps:, :]\n    y_pred_slice = y_pred[:, warmup_steps:, :]\n\n    # These sliced tensors both have this shape:\n    # [batch_size, sequence_length - warmup_steps, num_y_signals]\n\n    # Calculat the Mean Squared Error and use it as loss.\n    mse = mean(square(y_true_slice - y_pred_slice))\n    \n    return mse","3cf79516":"optimizer = RMSprop(lr=1e-3)","6d37912c":"model.compile(loss=loss_mse_warmup, optimizer=optimizer)","04c2b875":"model.summary()","5b138e78":"path_checkpoint = '23_checkpoint.keras'\ncallback_checkpoint = ModelCheckpoint(filepath=path_checkpoint,\n                                      monitor='val_loss',\n                                      verbose=1,\n                                      save_weights_only=True,\n                                      save_best_only=True)","cb2a9a45":"callback_early_stopping = EarlyStopping(monitor='val_loss',\n                                        patience=5, verbose=1)","55949dca":"callback_tensorboard = TensorBoard(log_dir='.\/23_logs\/',\n                                   histogram_freq=0,\n                                   write_graph=False)","1f8c8ea2":"callback_reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n                                       factor=0.1,\n                                       min_lr=1e-4,\n                                       patience=0,\n                                       verbose=1)","64b4ebc4":"callbacks = [callback_early_stopping,\n             callback_checkpoint,\n             callback_tensorboard,\n             callback_reduce_lr]","8d6e6c85":"%%time\nmodel.fit(x=generator,\n          epochs=30,\n          steps_per_epoch=100,\n          validation_data=validation_data,\n          callbacks=callbacks)","40d212cc":"try:\n    model.load_weights(path_checkpoint)\nexcept Exception as error:\n    print(\"Error trying to load checkpoint.\")\n    print(error)","0678ae3c":"result = model.evaluate(x=np.expand_dims(x_test_scaled, axis=0),\n                        y=np.expand_dims(y_test_scaled, axis=0))","77a10132":"print(\"loss (test-set):\", result)","f5ca55b0":"# If you have several metrics you can use this instead.\nif False:\n    for res, metric in zip(result, model.metrics_names):\n        print(\"{0}: {1:.3e}\".format(metric, res))","8f98b91c":"def plot_comparison(start_idx, length=100, train=True):\n    \"\"\"\n    Plot the predicted and true output-signals.\n    \n    :param start_idx: Start-index for the time-series.\n    :param length: Sequence-length to process and plot.\n    :param train: Boolean whether to use training- or test-set.\n    \"\"\"\n    \n    if train:\n        # Use training-data.\n        x = x_train_scaled\n        y_true = y_train\n    else:\n        # Use test-data.\n        x = x_test_scaled\n        y_true = y_test\n    \n    # End-index for the sequences.\n    end_idx = start_idx + length\n    \n    # Select the sequences from the given start-index and\n    # of the given length.\n    x = x[start_idx:end_idx]\n    y_true = y_true[start_idx:end_idx]\n    \n    # Input-signals for the model.\n    x = np.expand_dims(x, axis=0)\n\n    # Use the model to predict the output-signals.\n    y_pred = model.predict(x)\n    \n    # The output of the model is between 0 and 1.\n    # Do an inverse map to get it back to the scale\n    # of the original data-set.\n    y_pred_rescaled = y_scaler.inverse_transform(y_pred[0])\n    \n    #print(loss_mse_warmup(y_true, y_pred))\n    \n    # For each output-signal.\n    for signal in range(len(target_names)):\n        # Get the output-signal predicted by the model.\n        signal_pred = y_pred_rescaled[:, signal]\n        \n        # Get the true output-signal from the data-set.\n        signal_true = y_true[:, signal]\n        \n\n        # Make the plotting-canvas bigger.\n        plt.figure(figsize=(15,5))\n        \n        # Plot and compare the two signals.\n        plt.plot(signal_true, label='true')\n        plt.plot(signal_pred, label='pred')\n        \n        # Plot grey box for warmup-period.\n        p = plt.axvspan(0, warmup_steps, facecolor='black', alpha=0.15)\n        \n        # Plot labels etc.\n        plt.ylabel(target_names[signal])\n        plt.legend()\n        plt.show()","6007e6b9":"plot_comparison(start_idx=1000, length=500, train=True)","d06ce7f0":"data[\"CA_4\"]['Hobbie_revenue'][1000:1000+500].plot();","04e93b12":"plot_comparison(start_idx=10, length=500, train=False)","3e420152":"This is the number of output-signals:","f2be7d44":"This callback reduces the learning-rate for the optimizer if the validation-loss has not improved since the last epoch (as indicated by `patience=0`). The learning-rate will be reduced by multiplying it with the given factor. We set a start learning-rate of 1e-3 above, so multiplying it by 0.1 gives a learning-rate of 1e-4. We don't want the learning-rate to go any lower than this.","239cf31f":"This is the number of input-signals:","2cbc19a6":"## This was developed using Python 3.6 (Anaconda) and package versions:","c851d0bd":"### Example from Test-Set\n\nNow consider an example from the test-set. The model has not seen this data during training.\n\nThe temperature is predicted reasonably well, although the peaks are sometimes inaccurate.\n\nThe wind-speed has not been predicted so well. The daily oscillation-frequency seems to match, but the center-level and the peaks are quite inaccurate. A guess would be that the wind-speed is difficult to predict from the given input data, so the model has merely learnt to output sinusoidal oscillations in the daily frequency and approximately at the right center-level.\n\nThe atmospheric pressure is predicted reasonably well, except for a lag and a more noisy signal than the true time-series.","d2d6c7a9":"We can now plot an example of predicted output-signals. \n\nThe prediction is not very accurate for the first 30-50 time-steps because the model has seen very little input-data at this point.\nThe model generates a single time-step of output data for each time-step of the input-data, so when the model has only run for a few time-steps, it knows very little of the history of the input-signals and cannot make an accurate prediction. The model needs to \"warm up\" by processing perhaps 30-50 time-steps before its predicted output-signals can be used.\n\nThat is why we ignore this \"warmup-period\" of 50 time-steps when calculating the mean-squared-error in the loss-function. The \"warmup-period\" is shown as a grey box in these plots.","45bf2b47":"We can now add a Gated Recurrent Unit (GRU) to the network. This will have 512 outputs for each time-step in the sequence.\n\nNote that because this is the first layer in the model, Keras needs to know the shape of its input, which is a batch of sequences of arbitrary length (indicated by `None`), where each observation has a number of input-signals (`num_x_signals`).","a4aa727f":"These are the input-signals for the training- and test-sets:","eae01bc2":"The following is the number of time-steps that we will shift the target-data. Our data-set is sampled to have an observation for each day, so there are 30 observations for a month.\n\nWe want to predict the Revenue 1 month into the future, we shift the data 30 time-steps","c5064f05":"These are the output-signals (or target-signals):","bcb963b0":"These are the top rows of the data-set.","07b77924":"This is the number of observations in the test-set:","0e52f51b":"### NumPy Arrays\n\nWe now convert the Pandas data-frames to NumPy arrays that can be input to the neural network. We also remove the last part of the numpy arrays, because the target-data has `NaN` for the shifted period, and we only want to have valid data and we need the same array-shapes for the input- and output-data.\n\nThese are the input-signals:","fac86d12":"There are 3*10 input-signals in the data-set. There are 1913 rows","41a75f42":"### Compile Model\n\nThis is the optimizer and the beginning learning-rate that we will use.","7b687d77":"## We need to import several things from Keras.","e4c03fac":"We can also plot one of the output-signals that we want the model to learn how to predict given all those 20 input signals.","97898e45":"## Generate Predictions\n\nThis helper-function plots the predicted and true output-signals.","44044fed":"This is a very small model with only two layers. The output shape of `(None, None, 3)` means that the model will output a batch with an arbitrary number of sequences, each of which has an arbitrary number of observations, and each observation has 3 signals. This corresponds to the 3 target signals we want to predict.","bc464ec5":"### Loss Function\n\nWe will use Mean Squared Error (MSE) as the loss-function that will be minimized. This measures how closely the model's output matches the true output signals.\n\nHowever, at the beginning of a sequence, the model has only seen input-signals for a few time-steps, so its generated output may be very inaccurate. Using the loss-value for the early time-steps may cause the model to distort its later output. We therefore give the model a \"warmup-period\" of 30 time-steps where we don't use its accuracy in the loss-function, in hope of improving the accuracy for later time-steps.","22a1bdae":"We will use a large batch-size so as to keep the GPU near 100% work-load. You may have to adjust this number depending on your GPU, its RAM and your choice of `sequence_length` below.","685e43eb":"We then compile the Keras model so it is ready for training.","dec8f407":"A problem with using the Sigmoid activation function, is that we can now only output values in the same range as the training-data.\n\nWe can use a linear activation function on the output instead. This allows for the output to take on arbitrary values. It might work with the standard initialization for a simple network architecture, but for more complicated network architectures e.g. with more layers, it might be necessary to initialize the weights with smaller values to avoid `NaN` values during training. You may need to experiment with this to get it working.","f8aae35c":"The model was able to predict the overall oscillations quite well but the peaks were sometimes inaccurate. ","0b3a370d":"## Create the Recurrent Neural Network\n\nWe are now ready to create the Recurrent Neural Network (RNN). We will use the Keras API for this because of its simplicity.","db8be83e":"## Performance on Test-Set\n\nWe can now evaluate the model's performance on the test-set. This function expects a batch of data, but we will just use one long time-series for the test-set, so we just expand the array-dimensionality to create a batch with that one sequence.","cabbc186":"### Add Data\n\nWe can add some input-signals to the data that may help our model in making predictions.","9a9addc2":"### Scaled Data\n\nThe data-set contains a wide range of values:","d79ea98d":"The GRU outputs a batch of sequences of 512 values. We want to predict 3 output-signals, so we add a fully-connected (or dense) layer which maps 512 values down to only 3 values.\n\nThe output-signals in the data-set have been limited to be between 0 and 1 using a scaler-object. So we also limit the output of the neural network using the Sigmoid activation function, which squashes the output to be between 0 and 1.","8e88a312":"We then detect the range of values from the training-data and scale the training-data.","022b3d89":"## Train the Recurrent Neural Network\n\nWe can now train the neural network.\n","242d4b08":"We will try and predict these signals.","c8d008de":"### Target Data for Prediction","5ad3558b":"## Imports","6caa4046":"## Data Generator\n\nThe data-set has now been prepared as 2-dimensional numpy arrays. The training-data has almost large observations, consisting of 20 input-signals and 3 output-signals.\n\nThese are the array-shapes of the input and output data:","fe6a0562":"### Callback Functions\n\nDuring training we want to save checkpoints and log the progress to TensorBoard so we create the appropriate callbacks for Keras.\n\nThis is the callback for writing checkpoints during training.","70358c0c":"# RNN Prediction","232c1594":"### Validation Set\n\nThe neural network trains quickly so we can easily run many training epochs. But then there is a risk of overfitting the model to the training-set so it does not generalize well to unseen data. We will therefore monitor the model's performance on the test-set after each epoch and only save the model's weights if the performance is improved on the test-set.\n\nThe batch-generator randomly selects a batch of short sequences from the training-data and uses that during training. But for the validation-data we will instead run through the entire sequence from the test-set and measure the prediction accuracy on that entire sequence.","cc54d9a2":"We use the same scaler-object for the input-signals in the test-set.","0a6a8024":"We can plot one of the 20 input-signals as an example.","14462e85":"The target-data comes from the same data-set as the input-signals, because it is the weather-data for one of the cities that is merely time-shifted. But the target-data could be from a different source with different value-ranges, so we create a separate scaler-object for the target-data.","0ed0d64f":"This is the fraction of the data-set that will be used for the training-set:","fa8c99e6":"The neural network works best on values roughly between -1 and 1, so we need to scale the data before it is being input to the neural network. We can use `scikit-learn` for this.\n\nWe first create a scaler-object for the input-signals.","0fab6cec":"This is the number of observations (aka. data-points or samples) in the data-set:","a03c69cb":"List of the cities used in the data-set.","735c911f":"We then create the batch-generator.","d9331a63":"Instead of training the Recurrent Neural Network on the complete sequences of large observations, we will use the following function to create a batch of shorter sub-sequences picked at random from the training-data.","5e05c084":"We can then test the batch-generator to see if it works.","0ad11eaf":"These are the output-signals for the training- and test-sets:","9be6474b":"## Load the pre-processed Data","c7179eaa":"This is the callback for writing the TensorBoard log during training.","a97e55ef":"This gives us a random batch of 256 sequences, each sequence having 180 observations, and each observation having 31 input-signals and 3 output-signals.","a7163dca":"### Load Checkpoint\n\nBecause we use early-stopping when training the model, it is possible that the model's performance has worsened on the test-set for several epochs before training was stopped. We therefore reload the last saved checkpoint, which should have the best performance on the test-set.","c33c4490":"## Conclusion\n\nUsed a Recurrent Neural Network to predict several time-series from a number of input-signals. We used revenue-data for 10 stores to predict next months revenue for one of the stores.","ac5f3147":"Apart from a small rounding-error, the data has been scaled to be between 0 and 1.","a98b652b":"This is the callback for stopping the optimization when performance worsens on the validation-set.","b89399ba":"This is the number of observations in the training-set:"}}