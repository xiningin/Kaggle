{"cell_type":{"4a5f47f4":"code","42e69c21":"code","e506297f":"code","f20af947":"code","3eb41adb":"code","5758a318":"code","f7097e4d":"code","17ddf4fd":"code","79faacb6":"code","81156693":"markdown","85f0ab3c":"markdown","d58d0410":"markdown","95aa062e":"markdown","3369fc37":"markdown","07c5e580":"markdown","9ce23523":"markdown"},"source":{"4a5f47f4":"import os\nimport cv2\nimport keras\nimport numpy as np\nimport pandas as pd\nimport random as rn\nfrom PIL import Image\nfrom tqdm import tqdm\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom IPython.display import SVG\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nimport gc\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\n\nfrom tensorflow.python.keras import backend as K\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.python.keras.models import Sequential\nfrom tensorflow.python.keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.python.keras.utils.vis_utils import model_to_dot\nfrom tensorflow.python.keras.applications.vgg19 import VGG19\nfrom tensorflow.keras.applications.resnet50 import ResNet50,preprocess_input\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3\nfrom tensorflow.keras.applications.xception import Xception\n\nfrom tensorflow.keras.applications.mobilenet import MobileNet\n\nfrom sklearn.model_selection import train_test_split,KFold, cross_val_score, GridSearchCV\nfrom tensorflow.python.keras.preprocessing.image import ImageDataGenerator,load_img, img_to_array, save_img\nfrom tensorflow.python.keras.layers import Dense, Flatten,MaxPooling2D, GlobalAveragePooling2D,BatchNormalization,Dropout,Conv2D,MaxPool2D\nfrom sklearn.utils import shuffle\nfrom sklearn import metrics\nfrom sklearn.metrics import plot_confusion_matrix\nfrom keras.utils.vis_utils import plot_model\n","42e69c21":"train_data_dir='..\/input\/skin-cancer-malignant-vs-benign\/train'\ntest_data_dir='..\/input\/skin-cancer-malignant-vs-benign\/test'\n\nimg_height= 224\nimg_width = 224\nbatch_size= 64\n\n\n\n\ntrain_datagen = ImageDataGenerator(rescale=1.\/255,\n    zoom_range=0.1,\n    validation_split=0.1) # set validation split\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_data_dir,\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    class_mode='binary',\n    subset='training') \n\nvalidation_generator = train_datagen.flow_from_directory(\n    train_data_dir, # same directory as training data\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    class_mode='binary',\n    subset='validation',\n    ) # set as validation data\n\ntest_datagen = ImageDataGenerator(rescale=1. \/ 255)\ntest_generator = test_datagen.flow_from_directory(\n        test_data_dir,  \n        target_size=(img_height,img_width), \n        batch_size=batch_size,  \n        class_mode='binary',\n        shuffle=False)   \n\n","e506297f":"x,y=next(train_generator)\nprint(x[0].shape)\nplt.imshow(x[0,:,:,:])\nplt.show","f20af947":"\nepochs= 50\nmodel = Sequential()\n\npretrained_model= InceptionV3(include_top=False,\n                   input_shape=(224,224,3),\n                   pooling='avg',classes=2,\n                   weights='imagenet')\nprint(len(pretrained_model.layers))\n# if we want to set the first  layers of the network to be non-trainable\n#for layer in pretrained_model.layers[:len(pretrained_model.layers)-2]:\n   # layer.trainable=False\n#for layer in pretrained_model.layers[len(pretrained_model.layers)-2:]:\n    #layer.trainable=True\n#But train Batch Normalization layers\nfor layer in pretrained_model.layers:\n    if(isinstance(layer,tf.keras.layers.BatchNormalization)):\n        layer.trainable=True\n\nmodel.add(pretrained_model)\nmodel.add(Flatten())\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dropout(0.4))\nmodel.add(BatchNormalization())\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dropout(0.4))\nmodel.add(BatchNormalization())\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.summary()\n\nmodel.compile(optimizer=Adam(lr=0.001),loss='binary_crossentropy',metrics=['accuracy'])\nred_lr= ReduceLROnPlateau(monitor='val_loss',patience=3,verbose=1,factor=0.7)\nearly_bird= EarlyStopping(monitor='accuracy', patience=10)\ncheckpoint= ModelCheckpoint('model.hdf5',monitor='val_loss',verbose=1,mode='min',save_weights_only=False,save_best_only=True)\n\nHistory = model.fit_generator(     train_generator,\n    steps_per_epoch = train_generator.samples \/\/ batch_size,\n    validation_data = validation_generator, \n    validation_steps = validation_generator.samples \/\/ batch_size,\n    epochs=epochs,\n    verbose=1,\n    callbacks=[early_bird,checkpoint, red_lr])","3eb41adb":"model.load_weights('model.hdf5')","5758a318":"plt.style.use('grayscale')\nfig1 = plt.gcf()\nplt.plot(History.history['accuracy'])\nplt.plot(History.history['val_accuracy'])\nplt.axis( ymin= 0.4, ymax=1)\nplt.grid()\nplt.title('Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epochs')\nplt.legend(['train', 'validation'])\nplt.show()\n","f7097e4d":"fig1 = plt.gcf()\n\nplt.plot(History.history['loss'])\nplt.plot(History.history['val_loss'])\nplt.grid()\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epochs')\n\nplt.legend(['train', 'validation'])\nplt.show()\nfig1.savefig('loss_plot.jpg')\n","17ddf4fd":"Y_pred = model.predict(test_generator)\ny_pred=[]\nfor i in range(len(Y_pred)):\n    if Y_pred[i][0]>0.5:\n        y_pred.append(1)\n    else:\n        y_pred.append(0)\nprint('Confusion Matrix')\nprint(metrics.confusion_matrix(test_generator.classes, y_pred))\nprint ()\n\nprint('Classification Report')\ntarget_names = ['Non nevus', 'Nevus']\nprint(metrics.classification_report(test_generator.classes, y_pred, target_names=target_names))\n\nprint(test_generator.class_indices)","79faacb6":"plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True, )","81156693":"# Image Data Generators\nLoad batches of images from disk for training and validation purposes.","85f0ab3c":"# Accuracy and Loss figures ","d58d0410":"# Build, Compile and Train the model","95aa062e":"Plot an image from training set","3369fc37":"# Visualize model","07c5e580":"# Confution Matrix and Classification Report\n","9ce23523":"# Load weights from the best model"}}