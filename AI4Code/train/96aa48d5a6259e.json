{"cell_type":{"9d8ca8c4":"code","a0e9b7d1":"code","10e04408":"code","a20762a7":"code","3d15ed43":"code","4389ce90":"code","fd26b59d":"code","e27034a8":"code","8e65d023":"code","676ab580":"code","57ec0e10":"code","910642c0":"code","a767efd0":"code","cd0d5fa8":"code","99bdc07c":"code","23a71d77":"code","291dbcc0":"code","7ed4b313":"code","15ad0b31":"code","8b951dcb":"code","04a2314d":"code","d6c3b941":"code","1819105f":"code","dfb381d2":"code","27fc916c":"code","b57025a7":"code","690cbd8f":"code","859512db":"code","f5b6ff60":"code","46da15e1":"code","b59980ef":"code","6fe02637":"code","aa7832c4":"code","c0ce9238":"code","4003fc1d":"code","75608c26":"code","d17e3c01":"code","4cf56916":"code","41fafe5a":"code","2ba5d51a":"code","26b7e6b0":"code","eb07edbf":"code","6b2fed48":"code","e3737cc6":"code","98eb335b":"code","514c2888":"code","a4d5b561":"code","5bdadf21":"code","7ddba073":"code","3bcb8466":"code","9d3255a3":"code","d89c26c3":"code","a5cbd018":"code","0f39fc6d":"code","1efb8eb9":"code","7d568139":"code","04f1e8d5":"code","262efac9":"code","a6855025":"code","f8c2f36c":"code","82d2156d":"code","042db3ee":"code","3d7ad7e7":"code","fea871e8":"code","41eb8373":"code","c9ada16e":"code","c77190ff":"code","8a95d77a":"code","0175155d":"code","4b95e26f":"code","b8ee95e1":"code","f7f85043":"code","ac097432":"code","a08779ad":"code","55f925f8":"code","4a964f60":"markdown","093637c3":"markdown","669c053c":"markdown","da755b7b":"markdown","096a0ff7":"markdown","6b5616b1":"markdown","5c2b91c0":"markdown","d6728245":"markdown","a585851b":"markdown","ffa92fd4":"markdown","974d5006":"markdown","49b35bd6":"markdown","9602e5ff":"markdown","501547df":"markdown","1adb36b7":"markdown","6c3d0e04":"markdown","ece2d606":"markdown","2bd92c02":"markdown","9c76211c":"markdown","3f86d549":"markdown","aefb2f4e":"markdown","ca134ea5":"markdown","3d712a07":"markdown","772319eb":"markdown","8c097cc9":"markdown","b9406876":"markdown","a2701ef5":"markdown","d5b3425f":"markdown","74f0c3ff":"markdown","15e8c92b":"markdown","963562cc":"markdown","7c866003":"markdown","15c8c774":"markdown","de6cb70f":"markdown","352d9f67":"markdown","5f0e2480":"markdown","777126db":"markdown","abfba15c":"markdown","67b0e95c":"markdown","a552a1db":"markdown","579e47de":"markdown","d7805104":"markdown","42dccbf7":"markdown","3d26fb93":"markdown","0d0cff01":"markdown","c9b5f801":"markdown","1a4189a7":"markdown","88407ef1":"markdown","ed96ea1d":"markdown","89f24b68":"markdown","0f11c0b6":"markdown","1873792b":"markdown","25427bcf":"markdown","0838a50b":"markdown","6295b733":"markdown","9a2d7e5d":"markdown","3a91b30f":"markdown","f9b50f63":"markdown","b519c874":"markdown","02e830cc":"markdown","ed75fd5e":"markdown","8e260be8":"markdown","c6484e71":"markdown","43c30261":"markdown","f1cbe3e5":"markdown","2605b975":"markdown","afb7ccff":"markdown","7b5d017d":"markdown","9f765d2f":"markdown"},"source":{"9d8ca8c4":"import plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go","a0e9b7d1":"fig = go.Figure(data=[go.Table(header=dict(values=['Variables ', 'Descriptions']),\n                 cells=dict(values=[[\"Survival\", \"PassengerId\",\"Pclass\", \"Sex\",\"Age\",\"SibSp\",\"Parch\",\"Ticket\",\"Fare\",\"Cabin\",\"Embarked\"], [\"Survival\",\"Unique Id of a passenger.\",\"Ticket class\",\"Sex\",\"Age in years\",\"# of siblings \/ spouses aboard the Titanic\",\"# of parents \/ children aboard the Titanic\",\"Ticket number\",\"Passenger fare\",\"Cabin number\",\"Port of Embarkation\"]]\n                           ))])\n\nfig.show()","10e04408":"!pip install TPOT","a20762a7":"!pip install pycaret","3d15ed43":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport plotly.express as px\n\nimport cufflinks as cf \n\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n\n%matplotlib inline\nsns.set_style(\"whitegrid\")\nplt.style.use(\"fivethirtyeight\")\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.neighbors import  RadiusNeighborsClassifier\nfrom sklearn.linear_model import PassiveAggressiveClassifier\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.svm import NuSVC\nfrom sklearn.svm import LinearSVC\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nimport xgboost as xgb\nfrom sklearn.linear_model import RidgeClassifier\nfrom catboost import Pool, CatBoostClassifier, cv\nimport lightgbm as lgb\nimport plotly.offline as py\nfrom plotly.offline import init_notebook_mode, iplot\nimport plotly.graph_objs as go\nfrom plotly import tools\ninit_notebook_mode(connected=True)  \nimport plotly.figure_factory as ff\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.decomposition import PCA\nimport random\n","4389ce90":"def random_colors(number_of_colors):\n    color = [\"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)])\n                 for i in range(number_of_colors)]\n    return color","fd26b59d":"train = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ny_test = pd.read_csv(\"\/kaggle\/input\/titanic\/gender_submission.csv\")","e27034a8":"table = ff.create_table(train.head().round(3))\niplot(table,filename='jupyter-table1')","8e65d023":"train.columns\n","676ab580":"train.shape\n","57ec0e10":"iplot(ff.create_table(train.dtypes.to_frame().reset_index().round(3)),filename='jupyter-table2')","910642c0":"iplot(ff.create_table(train.describe().reset_index().round(3)),filename='jupyter-table2')","a767efd0":"train.isnull().sum()","cd0d5fa8":"msno.bar(train, color = 'b', figsize = (10,8))","99bdc07c":"msno.matrix(train[[\"Age\",\"Cabin\",\"Embarked\"]])","23a71d77":"msno.heatmap(train[[\"Age\",\"Cabin\",\"Embarked\"]])","291dbcc0":"data = [go.Bar(\n   x = list(train.Survived.unique()),\n   y = list(train.Survived.value_counts()),\n   marker = dict(color = random_colors(len(train.Survived.unique())),line=dict(color='#000000', width=1.5)))]\n\nlayout = go.Layout(\n   {\n      \"title\":\"Number of Passenger Survived\",\n   }\n)\n\nfig = go.Figure(data=data,layout = layout)\niplot(fig)","7ed4b313":"trace = go.Pie(labels = list(train.Survived.unique()), values = list(train.Survived.value_counts()))\ndata = [trace]\nlayout = go.Layout(\n   {\n      \"title\":\"Number of Passenger Survived\",\n   }\n)\n\nfig = go.Figure(data=data,layout = layout)\niplot(fig)","15ad0b31":"fig = go.Figure(data=[go.Pie(labels=train['Embarked'], pull=[.1, .15, .15, 0])],layout_title_text=' Embarked ',layout_title_font =dict(size= 36),layout_title_x=0.5)\nfig.show()\n\nfig = go.Figure(\n    data=[go.Bar(x = train['Embarked'].value_counts().index,y=train['Embarked'].value_counts())],layout_title_text=' Embarked ',layout_title_font =dict(size= 36),layout_title_x=0.5)\nfig.show()","8b951dcb":"fig = go.Figure(data=[go.Pie(labels=train['Sex'], pull=[.1, .15, .15, 0])],layout_title_text=' Sex ',layout_title_font =dict(size= 36),layout_title_x=0.5)\nfig.show()\n\nfig = go.Figure(\n    data=[go.Bar(x = train['Sex'].value_counts().index,y=train['Sex'].value_counts())],layout_title_text=' Sex ',layout_title_font =dict(size= 36),layout_title_x=0.5)\nfig.show()","04a2314d":"fig = go.Figure(data=[go.Pie(labels=train['SibSp'], pull=[.1, .15, .15, 0])],layout_title_text=' SibSp ',layout_title_font =dict(size= 36),layout_title_x=0.5)\nfig.show()\n\nfig = go.Figure(\n    data=[go.Bar(x = train['SibSp'].value_counts().index,y=train['SibSp'].value_counts())],layout_title_text=' SibSp ',layout_title_font =dict(size= 36),layout_title_x=0.5)\nfig.show()","d6c3b941":"fig = go.Figure(data=[go.Pie(labels=train['Parch'], pull=[.1, .15, .15, 0])],layout_title_text=' Parch ',layout_title_font =dict(size= 36),layout_title_x=0.5)\nfig.show()\n\nfig = go.Figure(\n    data=[go.Bar(x = train['Parch'].value_counts().index,y=train['Parch'].value_counts())],layout_title_text=' Parch ',layout_title_font =dict(size= 36),layout_title_x=0.5)\nfig.show()","1819105f":"fig = go.Figure(data=[go.Pie(labels=train['Pclass'], pull=[.1, .15, .15, 0])],layout_title_text=' Pclass ',layout_title_font =dict(size= 36),layout_title_x=0.5)\nfig.show()\n\nfig = go.Figure(\n    data=[go.Bar(x = train['Pclass'].value_counts().index,y=train['Pclass'].value_counts())],layout_title_text=' Pclass ',layout_title_font =dict(size= 36),layout_title_x=0.5)\nfig.show()","dfb381d2":"fig = go.Figure(\n    data=[go.Histogram(x=train[\"Fare\"])],layout_title_text=\"Fare\",layout_title_font =dict(size= 36),layout_title_x=0.5)\nfig.show()","27fc916c":"fig = go.Figure(\n    data=[go.Histogram(x=train['Age'])],layout_title_text=' Age Distribution ',layout_title_font =dict(size= 36),layout_title_x=0.5)\nfig.show()","b57025a7":"fig = go.Figure(\n    data=[go.Histogram(x=train['PassengerId'])],layout_title_text=' PassengerId Distribution ',layout_title_font =dict(size= 36),layout_title_x=0.5)\nfig.show()","690cbd8f":"survivedvspclass = train[[\"Pclass\",\"Survived\"]].groupby([\"Pclass\"], as_index = False).mean().sort_values(by=\"Survived\",ascending = False)\nfig = px.bar(survivedvspclass, x=\"Pclass\", y=\"Survived\",color=\"Survived\") \nfig.show()","859512db":"survivedvssex = train[[\"Sex\",\"Survived\"]].groupby([\"Sex\"], as_index = False).mean().sort_values(by=\"Survived\",ascending = False)\nfig = px.bar(survivedvssex, x=\"Sex\", y=\"Survived\",color=\"Survived\") \nfig.show()","f5b6ff60":"survivedvsparch = train[[\"Parch\",\"Survived\"]].groupby([\"Parch\"], as_index = False).mean().sort_values(by=\"Survived\",ascending = False)\nfig = px.bar(survivedvsparch, x=\"Parch\", y=\"Survived\",color=\"Survived\") \nfig.show()","46da15e1":"survivedvsSibSp = train[[\"SibSp\",\"Survived\"]].groupby([\"SibSp\"], as_index = False).mean().sort_values(by=\"Survived\",ascending = False)\nfig = px.bar(survivedvsSibSp, x=\"SibSp\", y=\"Survived\",color=\"Survived\") \nfig.show()","b59980ef":"survivedvsEmbarked = train[[\"Embarked\",\"Survived\"]].groupby([\"Embarked\"], as_index = False).mean().sort_values(by=\"Survived\",ascending = False)\nfig = px.bar(survivedvsEmbarked, x=\"Embarked\", y=\"Survived\",color=\"Survived\") \nfig.show()","6fe02637":"fig = px.box(train, x=\"Sex\", y=\"Age\", color=\"Survived\", notched=True)\nfig.show()\n","aa7832c4":"fig = px.box(train, x=\"Pclass\", y=\"Age\", color=\"Survived\", notched=True)\nfig.show()","c0ce9238":"fig = px.box(train, x=\"Parch\", y=\"Age\", color=\"Survived\", notched=True)\nfig.show()\n","4003fc1d":"fig = px.box(train, x=\"Embarked\", y=\"Age\", color=\"Survived\", notched=True)\nfig.show()\n","75608c26":"fig = px.box(train, x=\"Sex\", y=\"Age\", color=\"Survived\", notched=True)\nfig.show()\n","d17e3c01":"fig = px.histogram(train, x=\"Age\", color=\"Survived\")\nfig.show()","4cf56916":"fig = px.histogram(train, x=\"Fare\", color=\"Survived\")\nfig.show()","41fafe5a":"fig = px.density_heatmap(train.corr())\nfig.show()","2ba5d51a":"# creating train test split\nXtrain = train.iloc[:, [2, 4, 5, 6, 7]].values\nytrain = train.iloc[:, 1].values\nXtest = test.iloc[:, [1, 3, 4, 5, 6]].values\nytest = y_test.iloc[:, 1].values\n\n# labeling genders\nle1 = LabelEncoder()\nXtrain[:, 1] = le1.fit_transform(Xtrain[:, 1])\nXtest[:, 1] = le1.fit_transform(Xtest[:, 1])\n\n# dealing with nan data\nimp = SimpleImputer(missing_values=np.nan, strategy='mean')\nXTrainImp = imp.fit_transform(Xtrain)\nXTestImp = imp.fit_transform(Xtest)\n\n# feature scaling\nsc = StandardScaler()\nXTrainImp = sc.fit_transform(XTrainImp)\nXTestImp = sc.transform(XTrainImp)\n# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(XTrainImp, ytrain, test_size = 0.2, random_state = 0)","26b7e6b0":"## Logistic Regression\n\nModel = LogisticRegression()\nModel.fit(X_train, y_train)\ny_pred = Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n\n# Accuracy score\n\nprint('accuracy is',accuracy_score(y_pred,y_test))","eb07edbf":"data = {'y_Actual':    y_test,\n        'y_Predicted': y_pred\n        }\n\ndf = pd.DataFrame(data, columns=['y_Actual','y_Predicted'])\nconfusion_matrix = pd.crosstab(df['y_Actual'], df['y_Predicted'], rownames=['Actual'], colnames=['Predicted'])\n\nsns.heatmap(confusion_matrix, annot=True)\nplt.show()","6b2fed48":"# K-Nearest Neighbours\n\nModel = KNeighborsClassifier(n_neighbors=8)\nModel.fit(X_train, y_train)\n\ny_pred = Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\n\nprint('accuracy is',accuracy_score(y_pred,y_test))","e3737cc6":"data = {'y_Actual':    y_test,\n        'y_Predicted': y_pred\n        }\n\ndf = pd.DataFrame(data, columns=['y_Actual','y_Predicted'])\nconfusion_matrix = pd.crosstab(df['y_Actual'], df['y_Predicted'], rownames=['Actual'], colnames=['Predicted'])\n\nsns.heatmap(confusion_matrix, annot=True)\nplt.show()","98eb335b":"## Decision Tree\n\nModel = DecisionTreeClassifier()\nModel.fit(X_train, y_train)\ny_pred = Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\n\nprint('accuracy is',accuracy_score(y_pred,y_test))","514c2888":"## Naive Bayes\n\nModel = GaussianNB()\nModel.fit(X_train, y_train)\ny_pred = Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\n\nprint('accuracy is',accuracy_score(y_pred,y_test))","a4d5b561":"# Linear Discriminant Analysis\n\nModel = LinearDiscriminantAnalysis()\nModel.fit(X_train, y_train)\n\ny_pred = Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\n\nprint('accuracy is',accuracy_score(y_pred,y_test))","5bdadf21":"## Light GBM\n\nparams = {'objective':'binary', 'metric':'accuracy'}\n  \n# create dataset for lightgbm\nlgb_train = lgb.Dataset(X_train, y_train)\nlgb_test = lgb.Dataset(X_test, y_test, reference=lgb_train)\n\nModel = lgb.train(params, lgb_train, valid_sets=[lgb_train, lgb_test], verbose_eval=10)\ny_pred=Model.predict(X_test)\nprint(classification_report(y_test,y_pred.round()))\n","7ddba073":"\nModel=CatBoostClassifier(eval_metric='Accuracy',use_best_model=True,random_seed=42)\nModel.fit(X_train,y_train,eval_set=(X_test,y_test))","3bcb8466":"## CatBoost\ny_pred=Model.predict(X_test)\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_pred,y_test))\n#Accuracy Score\nprint('accuracy is ',accuracy_score(y_pred,y_test))","9d3255a3":"## XGBoost\n\nModel=xgb.XGBClassifier(max_depth=3, n_estimators=300, learning_rate=0.05)\nModel.fit(X_train,y_train)\ny_pred=Model.predict(X_test)\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_pred,y_test))\n#Accuracy Score\nprint('accuracy is ',accuracy_score(y_pred,y_test))","d89c26c3":"## Ridge Classifier\n\nModel=RidgeClassifier()\nModel.fit(X_train,y_train)\ny_pred=Model.predict(X_test)\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_pred,y_test))\n#Accuracy Score\nprint('accuracy is ',accuracy_score(y_pred,y_test))","a5cbd018":"## Quadratic Discriminant Analysis\n\nModel=QuadraticDiscriminantAnalysis()\nModel.fit(X_train,y_train)\ny_pred=Model.predict(X_test)\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_pred,y_test))\n#Accuracy Score\nprint('accuracy is ',accuracy_score(y_pred,y_test))","0f39fc6d":"## Bagging Classifier\n\nModel=BaggingClassifier()\nModel.fit(X_train,y_train)\ny_pred=Model.predict(X_test)\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_pred,y_test))\n#Accuracy Score\nprint('accuracy is ',accuracy_score(y_pred,y_test))","1efb8eb9":"## MLPClassifier\n\nModel=MLPClassifier()\nModel.fit(X_train,y_train)\ny_pred=Model.predict(X_test)\n# Summary of the predictions\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\n#Accuracy Score\nprint('accuracy is ',accuracy_score(y_pred,y_test))","7d568139":"## Linear Support Vector Classification\n \nModel = LinearSVC()\nModel.fit(X_train, y_train)\n\ny_pred = Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\n\nprint('accuracy is',accuracy_score(y_pred,y_test))","04f1e8d5":"## Nu-Support Vector Classification\n\nModel = NuSVC()\nModel.fit(X_train, y_train)\n\ny_pred = Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\n\nprint('accuracy is',accuracy_score(y_pred,y_test))","262efac9":"## BernoulliNB\n\nModel = BernoulliNB()\nModel.fit(X_train, y_train)\n\ny_pred = Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\nprint('accuracy is',accuracy_score(y_pred,y_test))","a6855025":"## Passive Aggressive Classifier\n\nModel = PassiveAggressiveClassifier()\nModel.fit(X_train, y_train)\n\ny_pred = Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\nprint('accuracy is',accuracy_score(y_pred,y_test))","f8c2f36c":"## Radius Neighbhors Classifier\n\nModel=RadiusNeighborsClassifier(radius=8.0)\nModel.fit(X_train,y_train)\ny_pred=Model.predict(X_test)\n#summary of the predictions made by the classifier\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\n#Accouracy score\nprint('accuracy is ', accuracy_score(y_test,y_pred))","82d2156d":"## Gradient Boosting Machine\nModel = GradientBoostingClassifier(n_estimators=100, random_state=9)\nModel.fit(X_train, y_train)\ny_pred = Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\n\nprint('accuracy is',accuracy_score(y_pred,y_test))\n","042db3ee":"## Adaboost\n\nModel = AdaBoostClassifier(n_estimators=100, random_state=9)\nModel.fit(X_train, y_train)\ny_pred = Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\n\nprint('accuracy is',accuracy_score(y_pred,y_test))\n","3d7ad7e7":"## Extra Trees\n\nModel = ExtraTreesClassifier(n_estimators=100, max_features=3)\nModel.fit(X_train, y_train)\ny_pred = Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\n\nprint('accuracy is',accuracy_score(y_pred,y_test))\n","fea871e8":"\n## Random Forest\n\nModel = RandomForestClassifier(n_estimators=100, max_features=3)\nModel.fit(X_train, y_train)\ny_pred = Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\n\nprint('accuracy is',accuracy_score(y_pred,y_test))","41eb8373":"## Support Vector Machine\n\nModel = SVC()\nModel.fit(X_train, y_train)\ny_pred = Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\n\nprint('accuracy is',accuracy_score(y_pred,y_test))","c9ada16e":"import h2o\nfrom h2o.automl import H2OAutoML\nh2o.init(max_mem_size='16G')","c77190ff":"data = h2o.import_file('\/kaggle\/input\/titanic\/train.csv')\n\nfeatures = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']\n\noutput = 'Survived'\n\naml = H2OAutoML(max_models=20, max_runtime_secs=1500, seed=1)\naml.train(x=features, y=output, training_frame=data)","8a95d77a":"lb = aml.leaderboard\nlb.head()","0175155d":"from tpot import TPOTClassifier\nfrom tpot import TPOTRegressor\n\ntpot = TPOTClassifier(generations=5, verbosity=2)\ntpot.fit(X_train,y_train)\n","4b95e26f":"y_pred=tpot.predict(X_test)\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_pred,y_test))\n#Accuracy Score\nprint('accuracy is ',accuracy_score(y_pred,y_test))","b8ee95e1":"! pip install -U pycaret # Quite large depencies to install !","f7f85043":"from pycaret.classification import *\n\nclf1 = setup(data = train, \n             target = 'Survived',\n             numeric_imputation = 'mean',\n             categorical_features = ['Sex','Embarked'], \n             ignore_features = ['Name','Ticket','Cabin'],\n             silent = True)\n","ac097432":"compare_models()\n","a08779ad":"lgbm  = create_model('lightgbm')      ","55f925f8":"plot_model(estimator = lgbm, plot = 'learning')\n","4a964f60":"[](http:\/\/)<a id=\"3.34\"><\/a>\n<font color=\"green\" size=+2.5><b> Parch <\/b><\/font>\n","093637c3":"<a href=\"#top\" class=\"btn btn-success btn-lg active\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to TOP<\/a>","669c053c":"<a id=\"3.31\"><\/a>\n<font color=\"green\" size=+2.5><b> Embarked <\/b><\/font>\n","da755b7b":"<a id=\"5.16\"><\/a>\n<font color=\"green\" size=+2.5><b> Passive Aggressive Classifier <\/b><\/font>\n\n*Like MultinomialNB, this classifier is suitable for **discrete data**. The difference is that while MultinomialNB works with occurrence counts, BernoulliNB is designed for binary\/boolean features.*\nThe Passive-Aggressive algorithms are a family of Machine learning algorithms that are not very well known by beginners and even intermediate Machine Learning enthusiasts. However, they can be very useful and efficient for certain applications.","096a0ff7":"<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:Green; border:0' role=\"tab\" aria-controls=\"home\"> <center>References and Credits<\/center><\/h3>\n\nThis notebook wouldn't have been possible without these following resources.This kernel includes codes and ideas from kernels below. If this kernel helps you, please upvote their work as well.\n* [Titanic Top 4% with ensemble modeling](https:\/\/www.kaggle.com\/yassineghouzam\/titanic-top-4-with-ensemble-modeling)\n* [Head Start for Data Scientist](https:\/\/www.kaggle.com\/hiteshp\/head-start-for-data-scientist)\n* [Titanic best working Classifier](https:\/\/www.kaggle.com\/sinakhorami\/titanic-best-working-classifier)\n* [A Journey through Titanic](https:\/\/www.kaggle.com\/omarelgabry\/a-journey-through-titanic)\n* [EDA To Prediction(DieTanic)](https:\/\/www.kaggle.com\/ash316\/eda-to-prediction-dietanic)","6b5616b1":"<a id=\"5.1\"><\/a>\n<font color=\"green\" size=+2.5><b> Logistic Regression <\/b><\/font>\n\n*Logistic regression assumes a Gaussian distribution for the numeric input variables and can\nmodel binary classification problems. You can construct a logistic regression model using the\nLogisticRegression class*","5c2b91c0":"<a id=\"3.1\"><\/a>\n<font color=\"green\" size=+2><b> Missing Value Analysis <\/b><\/font>\n","d6728245":"<a id=\"1.2\"><\/a>\n<font color=\"green\" size=+2><b> The Challenge <\/b><\/font>\n<br\/>\n<br\/>\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\nOn April 15, 1912, during her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren\u2019t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\nIn this challenge, we ask you to build a predictive model that answers the question: \u201cwhat sorts of people were more likely to survive?\u201d using passenger data (ie name, age, gender, socio-economic class, etc).","a585851b":"<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:green; border:0' role=\"tab\" aria-controls=\"home\"> <center>Objective  <\/center><\/h3>\n\nGoal of this kernel is following:\n- Basic Exploratory Data Analysis.\n- Beginners guide on Titanic Survival Dataset.\n- Feature Analysis\n- Modelling on 25+ Models","ffa92fd4":"<a id=\"3.33\"><\/a>\n<font color=\"green\" size=+2.5><b> SibSp <\/b><\/font>\n","974d5006":"<a id=\"3.54\"><\/a>\n<font color=\"green\" size=+2.5><b> SibSp VS Survived <\/b><\/font>\n","49b35bd6":"<a id=\"5.22\"><\/a>\n<font color=\"green\" size=+2.5><b> Support Vector Machine <\/b><\/font>\n\n*Support Vector Machines (or SVM) seek a line that best separates two classes. Those data instances that are closest to the line that best separates the classes are called support vectors and influence where the line is placed. SVM has been extended to support multiple classes Of particular importance is the use of different kernel functions via the kernel parameter .A powerful Radial Basis Function is used by default. You can construct an SVM model using the SVC class.*","9602e5ff":"<a id=\"5.25\"><\/a>\n<font color=\"green\" size=+2.5><b> Pycaret <\/b><\/font>\n","501547df":"<a id=\"3.53\"><\/a>\n<font color=\"green\" size=+2.5><b> Parch VS Survived <\/b><\/font>\n","1adb36b7":"<a id=\"5.10\"><\/a>\n<font color=\"green\" size=+2.5><b> Quadratic Discriminant Analysis <\/b><\/font>\n\n\n*A classifier with a quadratic decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule.\nThe model fits a **Gaussian** density to each class.*","6c3d0e04":"<a id=\"5.3\"><\/a>\n<font color=\"green\" size=+2.5><b> Decision Tree (CART) <\/b><\/font>\n\n*Classification and Regression Trees (CART or just decision trees) construct a binary tree from\nthe training data. Split points are chosen greedily by evaluating each attribute and each value\nof each attribute in the training data in order to minimize a cost function (like the Gini index).\nYou can construct a CART model using the DecisionTreeClassifier class*","ece2d606":"<a id=\"3.4\"><\/a>\n<font color=\"green\" size=+2.5><b> Numerical Variables Analysis <\/b><\/font>\n","2bd92c02":"<a id=\"5.23\"><\/a>\n<font color=\"green\" size=+2.5><b> H2O <\/b><\/font>","9c76211c":"<a id=\"5.14\"><\/a>\n<font color=\"green\" size=+2.5><b> Nu-Support Vector Classification <\/b><\/font>\n\n*Similar to SVC but uses a parameter to control the number of support vectors.*","3f86d549":"<a id=\"5.5\"><\/a>\n<font color=\"green\" size=+2.5><b> Linear Discriminant Analysis <\/b><\/font>\n\n*Linear Discriminant Analysis or LDA is a statistical technique for binary and multiclass\nclassification. It too assumes a Gaussian distribution for the numerical input variables. You can\nconstruct an LDA model using the LinearDiscriminantAnalysis class.*","aefb2f4e":"<a id=\"5.2\"><\/a>\n<font color=\"green\" size=+2.5><b> K-Nearest Neighbours <\/b><\/font>\n\n*The k-nearest neighbors (KNN) algorithm is a simple, easy-to-implement supervised machine learning algorithm that can be used to solve both classification and regression problems*","ca134ea5":"<a id=\"3.35\"><\/a>\n<font color=\"green\" size=+2.5><b> Pclass <\/b><\/font>\n","3d712a07":"<a id=\"3.62\"><\/a>\n<font color=\"green\" size=+2.5><b> Correlation <\/b><\/font>\n","772319eb":"<a id=\"5.7\"><\/a>\n<font color=\"green\" size=+2.5><b> CatBoost <\/b><\/font>\n\n*CatBoost is an algorithm for gradient boosting on decision trees. It is developed by Yandex researchers and engineers, and is used for search, recommendation systems, personal assistant, self-driving cars, weather prediction and many other tasks at Yandex and in other companies, including CERN, Cloudflare, Careem taxi. It is in open-source and can be used by anyone.*","8c097cc9":"![image.png](attachment:image.png)","b9406876":"\n\n<a id=\"1\"><\/a>\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:green; border:0' role=\"tab\" aria-controls=\"home\"> <center>  Introduction  <\/center><\/h3>","a2701ef5":"<a id=\"5.15\"><\/a>\n<font color=\"green\" size=+2.5><b> BernoulliNB <\/b><\/font>\n\n*Like MultinomialNB, this classifier is suitable for **discrete data**. The difference is that while MultinomialNB works with occurrence counts, BernoulliNB is designed for binary\/boolean features.*","d5b3425f":"<a id=\"3.57\"><\/a>\n<font color=\"green\" size=+2.5><b> Pclass VS Age <\/b><\/font>\n","74f0c3ff":"<a id=\"3.61\"><\/a>\n<font color=\"green\" size=+2.5><b> Fare VS Survived <\/b><\/font>\n","15e8c92b":"<a id=\"5.24\"><\/a>\n<font color=\"green\" size=+2.5><b> TPOT <\/b><\/font>","963562cc":"<a id=\"3.43\"><\/a>\n<font color=\"green\" size=+2.5><b> Passenger ID <\/b><\/font>\n","7c866003":"<a id=\"3.51\"><\/a>\n<font color=\"green\" size=+2.5><b> Pclass VS Survived <\/b><\/font>\n","15c8c774":"<a id=\"5.4\"><\/a>\n<font color=\"green\" size=+2.5><b> Naive Bayes <\/b><\/font>\n\n*Naive Bayes calculates the probability of each class and the conditional probability of each class\ngiven each input value. These probabilities are estimated for new data and multiplied together,\nassuming that they are all independent (a simple or naive assumption). When working with\nreal-valued data, a Gaussian distribution is assumed to easily estimate the probabilities for\ninput variables using the Gaussian Probability Density Function. You can construct a Naive\nBayes model using the GaussianNB class*","de6cb70f":"<a id=\"3.55\"><\/a>\n<font color=\"green\" size=+2.5><b> Embarked VS Survived <\/b><\/font>\n","352d9f67":"<a id=\"5.11\"><\/a>\n<font color=\"green\" size=+2.5><b> Bagging classifier  <\/b><\/font>\n\n*A Bagging classifier is an ensemble **meta-estimator** that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it.*","5f0e2480":"<a id=\"2.2\"><\/a>\n<font color=\"green\" size=+2><b> Import Dataset <\/b><\/font>\n","777126db":"<a id=\"3.32\"><\/a>\n<font color=\"green\" size=+2.5><b> Sex <\/b><\/font>\n","abfba15c":"<a id=\"5.6\"><\/a>\n<font color=\"green\" size=+2.5><b> LightGBM <\/b><\/font>\n\n*LightGBM is a gradient boosting framework that uses tree based learning algorithms. It is designed to be distributed and efficient with the following advantages:*\n\nFaster training speed and higher efficiency.\nLower memory usage.\nBetter accuracy.\nSupport of parallel and GPU learning.\nCapable of handling large-scale data.","67b0e95c":"<a id=\"4.1\"><\/a>\n<font color=\"green\" size=+2.5><b> Data Handling and Preparation <\/b><\/font>\n","a552a1db":"<a id=\"3.41\"><\/a>\n<font color=\"green\" size=+2.5><b> Fare <\/b><\/font>\n","579e47de":"<a id=\"3\"><\/a>\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:green; border:0' role=\"tab\" aria-controls=\"home\"> <center>  Exploratory Data Analysis <\/center><\/h3>","d7805104":"<a id=\"3.57\"><\/a>\n<font color=\"green\" size=+2.5><b> Parch VS Age <\/b><\/font>\n","42dccbf7":"<a id=\"5.17\"><\/a>\n<font color=\"green\" size=+2.5><b> Radius Neighbors Classifier <\/b><\/font>\n\n*Classifier implementing a **vote** among neighbors within a given **radius**\n\nIn scikit-learn **RadiusNeighborsClassifier** is very similar to **KNeighborsClassifier** with the exception of two parameters. First, in RadiusNeighborsClassifier we need to specify the radius of the fixed area used to determine if an observation is a neighbor using radius. Unless there is some substantive reason for setting radius to some value, it is best to treat it like any other hyperparameter and tune it during model selection. The second useful parameter is outlier_label, which indicates what label to give an observation that has no observations within the radius - which itself can often be a useful tool for identifying outliers.*","3d26fb93":"<a id=\"3.2\"><\/a>\n<font color=\"green\" size=+2><b> Target Variable Analysis <\/b><\/font>\n","0d0cff01":"<a id=\"3.56\"><\/a>\n<font color=\"green\" size=+2.5><b> Sex VS Age <\/b><\/font>\n","c9b5f801":"<a id=\"3.59\"><\/a>\n<font color=\"green\" size=+2.5><b> Sex VS Age <\/b><\/font>\n","1a4189a7":"<a id=\"5.13\"><\/a>\n<font color=\"green\" size=+2.5><b> Linear Support Vector Classification  <\/b><\/font>\n\n*Similar to **SVC** with parameter kernel=\u2019linear\u2019, but implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples.*","88407ef1":"<a id=\"1.1\"><\/a>\n<font color=\"green\" size=+2><b> About Competition<\/b><\/font>\n<br\/>\n<br\/>\nThis is the legendary Titanic ML competition \u2013 the best, first challenge for you to dive into ML competitions and familiarize yourself with how the Kaggle platform works.\nThe competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.","ed96ea1d":"<a id=\"3.52\"><\/a>\n<font color=\"green\" size=+2.5><b> Sex VS Survived <\/b><\/font>\n","89f24b68":"<a id=\"5.8\"><\/a>\n<font color=\"green\" size=+2.5><b> XGBoost <\/b><\/font>\n\n*XGBoost stands for Extreme Gradient Boosting, it is a performant machine learning library based on the paper Greedy Function Approximation: A Gradient Boosting Machine, by Friedman. XGBoost implements a Gradient Boosting algorithm based on decision trees.*","0f11c0b6":"<a id=\"5.12\"><\/a>\n<font color=\"green\" size=+2.5><b> MLPClassifier  <\/b><\/font>\n\n*MLPClassifier stands for Multi-layer Perceptron classifier which in the name itself connects to a Neural Network. Unlike other classification algorithms such as Support Vectors or Naive Bayes Classifier, MLPClassifier relies on an underlying Neural Network to perform the task of classification.*","1873792b":"<a id=\"5.20\"><\/a>\n<font color=\"green\" size=+2.5><b> Extra Trees <\/b><\/font>\n\n*Extra Trees are another modification of bagging where random trees are constructed from samples of the training dataset. You can construct an Extra Trees model for classification using the ExtraTreesClassifier class*","25427bcf":"### If these kernels impress you,give them an <font size=\"+3\" color=\"green\"><b>Upvote<\/b><\/font>.<br>","0838a50b":"<a id=\"5.19\"><\/a>\n<font color=\"green\" size=+2.5><b> \nAdaBoost <\/b><\/font>\n\n*AdaBoost was perhaps the first successful boosting ensemble algorithm. It generally works by weighting instances in the dataset by how easy or difficult they are to classify, allowing the algorithm to pay or less attention to them in the construction of subsequent models. You can construct an AdaBoost model for classification using the AdaBoostClassifier class*","6295b733":"<font size=\"+3\" color=green><b> <center><u>Titanic Survival+ EDA + (25+) Models For Beginners<\/u><\/center><\/b><\/font>\n\n","9a2d7e5d":"<a id=\"top\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:green; border:0' role=\"tab\" aria-controls=\"home\"> <center> Table of content <\/center><\/h3>\n    \n<font color=\"green\" size=+1><b>Introduction<\/b><\/font>\n* [About Competition ](#1.1)\n* [The Challenge ](#1.2)\n* [Data Dictionary ](#1.3)\n* [Data Variable](#1.4)\n    \n<font color=\"green\" size=+1><b> Load and Check Data <\/b><\/font>\n* [Importing Library](#2.1)\n* [Load Dataset](#2.2)\n\n<font color=\"green\" size=+1><b> Exploratory Data Analysis <\/b><\/font>\n* [Missing Value Analysis](#3.1)\n* [Target Variable Analysis](#3.2)    \n* [Categorical Univariate Analysis ](#3.3)\n* [Numerical Univariate Analysis](#3.4)\n* [Bivariate Analysis](#3.5)\n\n<font color=\"green\" size=+1><b> Data Handling and Preparation <\/b><\/font>\n* [Handing Missing Data ](#4.1)\n* [Train Test Split ](#4.2)\n\n<font color=\"green\" size=+1><b> Model Training <\/b><\/font>\n* [Logistic Regression ](#5.1)\n* [K-Nearest Neighbours ](#5.2)    \n* [Decision Tree ](#5.3)\n* [Naive Bayes ](#5.4)    \n* [Linear Discriminant Analysis ](#5.5)\n* [LightLGM ](#5.6)    \n* [CatBoost ](#5.7)\n* [XGBoost ](#5.8)    \n* [Ridge Classifier ](#5.9)\n* [Quadratic Discriminant Analysis ](#5.10)    \n* [Bagging classifier ](#5.11)\n* [MLPClassifier](#5.12)    \n* [Linear Support Vector Classification ](#5.13)\n* [Nu-Support Vector Classification ](#5.14)    \n* [BernoulliNB ](#5.15)\n* [Passive Aggressive Classifier ](#5.16)    \n* [Radius Neighbors Classifier ](#5.17)\n* [Stochastic Gradient Boosting ](#5.18)    \n* [AdaBoost ](#5.19)\n* [Extra Trees ](#5.20)    \n* [Random Forest ](#5.21)\n* [SVC ](#5.22)    \n* [H2O ](#5.23)\n* [TPOT ](#5.24)    \n* [PyCaret ](#5.25)","3a91b30f":"<a id=\"3.60\"><\/a>\n<font color=\"green\" size=+2.5><b> Age VS Survived <\/b><\/font>","f9b50f63":"<a id=\"1.3\"><\/a>\n<font color=\"green\" size=+2><b> Data Dictionary <\/b><\/font>","b519c874":"<a id=\"5.21\"><\/a>\n<font color=\"green\" size=+2.5><b> Random Forest <\/b><\/font>\n\n*Random Forests is an extension of bagged decision trees. Samples of the training dataset are taken with replacement, but the trees are constructed in a way that reduces the correlation between individual classifiers. Specifically, rather than greedily choosing the best split point in the construction of each tree, only a random subset of features are considered for each split. You can construct a Random Forest model for classification using the RandomForestClassifier class.*","02e830cc":"<a id=\"2\"><\/a>\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  style='background:green; border:0'role=\"tab\" aria-controls=\"home\"> <center>  Load and Check Data  <\/center><\/h3>","ed75fd5e":"<font size=\"+1\" color=green ><b>Please Upvote my kernel and keep it in your favourite section if you think it is helpful.<\/b><\/font>","8e260be8":"<a id=\"5.18\"><\/a>\n<font color=\"green\" size=+2.5><b> Stochastic Gradient Boosting <\/b><\/font>\n\n*Stochastic Gradient Boosting (also called Gradient Boosting Machines) are one of the most sophisticated ensemble techniques. It is also a technique that is proving to be perhaps one of the best techniques available for improving performance via ensembles. You can construct a Gradient Boosting model for classification using the GradientBoostingClassifier class*","c6484e71":"<a id=\"5.9\"><\/a>\n<font color=\"green\" size=+2.5><b> Ridge Classifier <\/b><\/font>\n\n*Classifier using Ridge regression. This classifier first converts the target values into {-1, 1} and then treats the problem as a \nregression task (multi-output regression in the multiclass case).*","43c30261":"<a id=\"2.1\"><\/a>\n<font color=\"green\" size=+2><b> Import Libraries <\/b><\/font>","f1cbe3e5":"<a id=\"5\"><\/a>\n<font color=\"green\" size=+2.5><b> Model Training <\/b><\/font>\n","2605b975":"<a id=\"3.58\"><\/a>\n<font color=\"green\" size=+2.5><b> Embarked VS Age <\/b><\/font>\n","afb7ccff":"<a id=\"3.3\"><\/a>\n<font color=\"green\" size=+2.5><b> Categorical Variable Analysis <\/b><\/font>\n","7b5d017d":"<a id=\"3.5\"><\/a>\n<font color=\"green\" size=+2.5><b> Bivariate Data Analysis <\/b><\/font>\n","9f765d2f":"<a id=\"3.42\"><\/a>\n<font color=\"green\" size=+2.5><b> Age <\/b><\/font>\n"}}