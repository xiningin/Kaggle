{"cell_type":{"86de171a":"code","4e62e603":"code","55231cc0":"code","8474e576":"code","0f1d5cd2":"code","c936f490":"code","d4a06b44":"code","56b62a6b":"code","d5277073":"code","f5ca0e98":"code","a9ab458e":"code","c99ee9be":"code","40fa6b4d":"code","e548d470":"code","f9a3a25b":"code","180cf635":"code","a8343bbf":"code","d33d22d3":"code","c92dbbce":"code","739c40fb":"code","3e116fc7":"code","ddbf2f62":"code","60e23ba4":"code","129b504c":"code","f12595f9":"code","f206dc2e":"code","5de69460":"code","30a98cb0":"code","40e0dda4":"code","63216aec":"code","8a94d098":"code","359c5aef":"code","dff5ed3d":"code","265f5976":"markdown","95ff0b0e":"markdown"},"source":{"86de171a":"# Import the necessary libraries\nimport numpy as np\nimport pandas as pd\nimport os\nimport time\nimport warnings\nimport os\nfrom six.moves import urllib\nimport matplotlib\nimport matplotlib.pyplot as plt\nwarnings.filterwarnings('ignore')","4e62e603":"#Add All the Models Libraries\n\n# Scalers\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils import shuffle\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import FeatureUnion\n\n# Models\nfrom sklearn.linear_model import LogisticRegression #logistic regression\nfrom sklearn.svm import SVC # Support Vector Classifier\nfrom sklearn.ensemble import RandomForestClassifier #Random Forest\nfrom sklearn.neighbors import KNeighborsClassifier #KNN\nfrom sklearn.ensemble import ExtraTreesClassifier \nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.tree import DecisionTreeClassifier #Decision Tree\nfrom sklearn.ensemble import BaggingClassifier\n\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import train_test_split #training and testing data split\nfrom sklearn import metrics #accuracy measure\nfrom sklearn.metrics import confusion_matrix #for confusion matrix\nfrom scipy.stats import reciprocal, uniform\n\nfrom sklearn.ensemble import AdaBoostClassifier\n\n\n# Cross-validation\nfrom sklearn.model_selection import KFold #for K-fold cross validation\nfrom sklearn.model_selection import cross_val_score #score evaluation\nfrom sklearn.model_selection import cross_val_predict #prediction\nfrom sklearn.model_selection import cross_validate\n\n# GridSearchCV\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\n\n#Common data processors\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\nfrom sklearn.base import BaseEstimator, TransformerMixin\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.metrics import roc_auc_score, log_loss\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold,KFold\nfrom sklearn.utils import check_array\nfrom scipy import sparse\n\n#Accuracy Score\nfrom sklearn.metrics import accuracy_score\nimport lightgbm as lgb","55231cc0":"# to make this notebook's output stable across runs\nnp.random.seed(123)\n\n# To plot pretty figures\n%matplotlib inline\nplt.rcParams['axes.labelsize'] = 14\nplt.rcParams['xtick.labelsize'] = 12\nplt.rcParams['ytick.labelsize'] = 12","8474e576":"#merge the data for feature engineering and later split it, just before applying Data Pipeline\nTrainFile = pd.read_csv(\"..\/input\/Target-Marketing-Canadians\/train.csv\") #read the data from the csv file.\nTestFile = pd.read_csv(\"..\/input\/Target-Marketing-Canadians\/test.csv\")","0f1d5cd2":"TrainFile.shape","c936f490":"TestFile.shape","d4a06b44":"TrainFile.info()","56b62a6b":"TrainFile.head(5)","d5277073":"TestFile.info()","f5ca0e98":"# Interactions\n\nTrainFile['External_Competitive_1'] = TrainFile[\"ExternalAccount1\"] * TrainFile[\"CompetitiveRate1\"]\nTrainFile['External_Competitive_2'] = TrainFile[\"ExternalAccount2\"] * TrainFile[\"CompetitiveRate2\"]\nTrainFile['External_Competitive_3'] = TrainFile[\"ExternalAccount3\"] * TrainFile[\"CompetitiveRate3\"]\nTrainFile['External_Competitive_4'] = TrainFile[\"ExternalAccount4\"] * TrainFile[\"CompetitiveRate4\"]\nTrainFile['External_Competitive_5'] = TrainFile[\"ExternalAccount5\"] * TrainFile[\"CompetitiveRate5\"]\nTrainFile['External_Competitive_6'] = TrainFile[\"ExternalAccount6\"] * TrainFile[\"CompetitiveRate6\"]\nTrainFile['External_Competitive_7'] = TrainFile[\"ExternalAccount7\"] * TrainFile[\"CompetitiveRate7\"]\nTrainFile['Transactions_Mean'] = TrainFile.iloc[:, 9:17].mean(axis=1)\nTrainFile['Transactions_std'] = TrainFile.iloc[:, 9:17].std(axis=1)\nTrainFile['Transactions_Max'] = TrainFile.iloc[:, 9:17].max(axis=1)\n\n\nTestFile['External_Competitive_1'] = TestFile[\"ExternalAccount1\"] * TestFile[\"CompetitiveRate1\"]\nTestFile['External_Competitive_2'] = TestFile[\"ExternalAccount2\"] * TestFile[\"CompetitiveRate2\"]\nTestFile['External_Competitive_3'] = TestFile[\"ExternalAccount3\"] * TestFile[\"CompetitiveRate3\"]\nTestFile['External_Competitive_4'] = TestFile[\"ExternalAccount4\"] * TestFile[\"CompetitiveRate4\"]\nTestFile['External_Competitive_5'] = TestFile[\"ExternalAccount5\"] * TestFile[\"CompetitiveRate5\"]\nTestFile['External_Competitive_6'] = TestFile[\"ExternalAccount6\"] * TestFile[\"CompetitiveRate6\"]\nTestFile['External_Competitive_7'] = TestFile[\"ExternalAccount7\"] * TestFile[\"CompetitiveRate7\"]\nTestFile['Transactions_Mean'] = TestFile.iloc[:, 9:17].mean(axis=1)\nTestFile['Transactions_std'] = TestFile.iloc[:, 9:17].std(axis=1)\nTestFile['Transactions_Max'] = TestFile.iloc[:, 9:17].max(axis=1)","a9ab458e":"TrainFile.head(2)","c99ee9be":"#Making Balance bins\n\nTrainFile['Balance_Bin'] = pd.qcut(TrainFile['Balance'], 3)\n\nlabel = LabelEncoder()\nTrainFile['Balance_Bin'] = label.fit_transform(TrainFile['Balance_Bin'])\n\n#Making Balance bins\n\nTestFile['Balance_Bin'] = pd.qcut(TestFile['Balance'], 3)\n\nlabel = LabelEncoder()\nTestFile['Balance_Bin'] = label.fit_transform(TestFile['Balance_Bin'])","40fa6b4d":"TrainFile.tail(2)","e548d470":"#drop Customer_id\nTrainFile = TrainFile.drop(['Customer_id','Balance'],axis=1)\nTestFile = TestFile.drop(['Customer_id','Balance'],axis=1)","f9a3a25b":"# Now define x and y.\n\n#the Y Variable\ny_train = TrainFile[\"Target\"].copy()\n\n#the X variables\nX_train = TrainFile.drop(\"Target\", axis=1)","180cf635":"X_train.shape","a8343bbf":"TestFile.shape","d33d22d3":"y_train.shape","c92dbbce":"y = pd.DataFrame(y_train)\ny.column = ['Target']\ny.shape","739c40fb":"Features = X_train.columns.tolist()\nFeatures_Test = TestFile.columns.tolist()","3e116fc7":"# The CategoricalEncoder class will allow us to convert categorical attributes to one-hot vectors.\n\nclass CategoricalEncoder(BaseEstimator, TransformerMixin):\n    def __init__(self, encoding='onehot', categories='auto', dtype=np.float64,\n                 handle_unknown='error'):\n        self.encoding = encoding\n        self.categories = categories\n        self.dtype = dtype\n        self.handle_unknown = handle_unknown\n\n    def fit(self, X, y=None):\n        \"\"\"Fit the CategoricalEncoder to X.\n        Parameters\n        ----------\n        X : array-like, shape [n_samples, n_feature]\n            The data to determine the categories of each feature.\n        Returns\n        -------\n        self\n        \"\"\"\n\n        if self.encoding not in ['onehot', 'onehot-dense', 'ordinal']:\n            template = (\"encoding should be either 'onehot', 'onehot-dense' \"\n                        \"or 'ordinal', got %s\")\n            raise ValueError(template % self.handle_unknown)\n\n        if self.handle_unknown not in ['error', 'ignore']:\n            template = (\"handle_unknown should be either 'error' or \"\n                        \"'ignore', got %s\")\n            raise ValueError(template % self.handle_unknown)\n\n        if self.encoding == 'ordinal' and self.handle_unknown == 'ignore':\n            raise ValueError(\"handle_unknown='ignore' is not supported for\"\n                             \" encoding='ordinal'\")\n\n        X = check_array(X, dtype=np.object, accept_sparse='csc', copy=True)\n        n_samples, n_features = X.shape\n\n        self._label_encoders_ = [LabelEncoder() for _ in range(n_features)]\n\n        for i in range(n_features):\n            le = self._label_encoders_[i]\n            Xi = X[:, i]\n            if self.categories == 'auto':\n                le.fit(Xi)\n            else:\n                valid_mask = np.in1d(Xi, self.categories[i])\n                if not np.all(valid_mask):\n                    if self.handle_unknown == 'error':\n                        diff = np.unique(Xi[~valid_mask])\n                        msg = (\"Found unknown categories {0} in column {1}\"\n                               \" during fit\".format(diff, i))\n                        raise ValueError(msg)\n                le.classes_ = np.array(np.sort(self.categories[i]))\n\n        self.categories_ = [le.classes_ for le in self._label_encoders_]\n\n        return self\n\n    def transform(self, X):\n        \"\"\"Transform X using one-hot encoding.\n        Parameters\n        ----------\n        X : array-like, shape [n_samples, n_features]\n            The data to encode.\n        Returns\n        -------\n        X_out : sparse matrix or a 2-d array\n            Transformed input.\n        \"\"\"\n        X = check_array(X, accept_sparse='csc', dtype=np.object, copy=True)\n        n_samples, n_features = X.shape\n        X_int = np.zeros_like(X, dtype=np.int)\n        X_mask = np.ones_like(X, dtype=np.bool)\n\n        for i in range(n_features):\n            valid_mask = np.in1d(X[:, i], self.categories_[i])\n\n            if not np.all(valid_mask):\n                if self.handle_unknown == 'error':\n                    diff = np.unique(X[~valid_mask, i])\n                    msg = (\"Found unknown categories {0} in column {1}\"\n                           \" during transform\".format(diff, i))\n                    raise ValueError(msg)\n                else:\n                    # Set the problematic rows to an acceptable value and\n                    # continue `The rows are marked `X_mask` and will be\n                    # removed later.\n                    X_mask[:, i] = valid_mask\n                    X[:, i][~valid_mask] = self.categories_[i][0]\n            X_int[:, i] = self._label_encoders_[i].transform(X[:, i])\n\n        if self.encoding == 'ordinal':\n            return X_int.astype(self.dtype, copy=False)\n\n        mask = X_mask.ravel()\n        n_values = [cats.shape[0] for cats in self.categories_]\n        n_values = np.array([0] + n_values)\n        indices = np.cumsum(n_values)\n\n        column_indices = (X_int + indices[:-1]).ravel()[mask]\n        row_indices = np.repeat(np.arange(n_samples, dtype=np.int32),\n                                n_features)[mask]\n        data = np.ones(n_samples * n_features)[mask]\n\n        out = sparse.csc_matrix((data, (row_indices, column_indices)),\n                                shape=(n_samples, indices[-1]),\n                                dtype=self.dtype).tocsr()\n        if self.encoding == 'onehot-dense':\n            return out.toarray()\n        else:\n            return out","ddbf2f62":"class DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attribute_names):\n        self.attribute_names = attribute_names\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        return X[self.attribute_names]","60e23ba4":"Categorical_Features = ['PreviousCampaignResult','Product1','Product2','Product3','Product4','Product5','Product6','ExternalAccount1','ExternalAccount2','ExternalAccount3','ExternalAccount4',\n                        'ExternalAccount5','ExternalAccount6','ExternalAccount7','Balance_Bin']\nContinuous_Features = ['Transaction1','Transaction2','Transaction3','Transaction4','Transaction5','Transaction6','Transaction7','Transaction8','Transaction9','ActivityIndicator','RegularInteractionIndicator',\n                       'CompetitiveRate1','CompetitiveRate2','CompetitiveRate3','CompetitiveRate4','CompetitiveRate5','CompetitiveRate6','CompetitiveRate7','RateBefore','ReferenceRate','External_Competitive_1',\n                       'External_Competitive_2','External_Competitive_3','External_Competitive_4','External_Competitive_5','External_Competitive_6','External_Competitive_7','Transactions_Mean','Transactions_std',\n                       'Transactions_Max']\n\nTotal_Features = Categorical_Features + Continuous_Features\n\ncat_pipeline = Pipeline([\n        (\"selector\", DataFrameSelector(Categorical_Features))\n    ])\n\nnum_pipeline = Pipeline([\n        ('selector', DataFrameSelector(Continuous_Features)),\n        ('std_scaler', StandardScaler()),\n    ])","129b504c":"full_pipeline = FeatureUnion(transformer_list=[\n    (\"cat_pipeline\", cat_pipeline),\n    (\"num_pipeline\", num_pipeline),\n    ])\n\nX_train = full_pipeline.fit_transform(X_train)\nX_test = full_pipeline.transform(TestFile)","f12595f9":"X = pd.DataFrame(X_train)\nX.columns = Total_Features\nX.shape","f206dc2e":"bayesian_tr_idx, bayesian_val_idx = train_test_split(X, test_size = 0.3, random_state = 42, stratify = X['PreviousCampaignResult'])\nbayesian_tr_idx = bayesian_tr_idx.index\nbayesian_val_idx = bayesian_val_idx.index","5de69460":"paramsLGB = {\n    'learning_rate': (0.001,0.005),\n    'num_leaves': (50, 500), \n    'bagging_fraction' : (0.1, 0.9),\n    'feature_fraction' : (0.1, 0.9),\n    'min_child_weight': (0.00001, 0.01),   \n    'min_data_in_leaf': (5,7),\n    'max_depth':(-1,50),\n    'reg_alpha': (1, 2), \n    'reg_lambda': (1, 2),\n}","30a98cb0":"def LGB_bayesian(\n    learning_rate,\n    num_leaves, \n    bagging_fraction,\n    feature_fraction,\n    min_child_weight, \n    min_data_in_leaf,\n    max_depth,\n    reg_alpha,\n    reg_lambda\n     ):\n    \n    # LightGBM expects next three parameters need to be integer. \n    num_leaves = int(num_leaves)\n    min_data_in_leaf = int(min_data_in_leaf)\n    max_depth = int(max_depth)\n\n    assert type(num_leaves) == int\n    assert type(min_data_in_leaf) == int\n    assert type(max_depth) == int\n    \n\n    param = {\n              'num_leaves': num_leaves, \n              'min_data_in_leaf': min_data_in_leaf,\n              'min_child_weight': min_child_weight,\n              'bagging_fraction' : bagging_fraction,\n              'feature_fraction' : feature_fraction,\n              'learning_rate' : learning_rate,\n              'max_depth': max_depth,\n              'reg_alpha': reg_alpha,\n              'reg_lambda': reg_lambda,\n              'objective': 'binary',\n              'save_binary': True,\n              'seed': Random_Seed,\n              'feature_fraction_seed': Random_Seed,\n              'bagging_seed': Random_Seed,\n              'drop_seed': Random_Seed,\n              'data_random_seed': Random_Seed,\n              'boosting_type': 'gbdt',\n              'verbose': -1,\n              'is_unbalance': False,\n              'boost_from_average': True,\n              'metric':'auc'}    \n    \n    oof = np.zeros(len(X))\n    trn_data= lgb.Dataset(X.iloc[bayesian_tr_idx][Total_Features].values, label=y.iloc[bayesian_tr_idx]['Target'].values, params={'verbose': -1}, free_raw_data=False)\n    val_data= lgb.Dataset(X.iloc[bayesian_val_idx][Total_Features].values, label=y.iloc[bayesian_val_idx]['Target'].values, params={'verbose': -1},free_raw_data=False)\n\n    clf = lgb.train(param, trn_data,  num_boost_round=50, valid_sets = [trn_data, val_data], early_stopping_rounds = 50, verbose_eval=False)\n    \n    oof[bayesian_val_idx]  = clf.predict(X.iloc[bayesian_val_idx][Total_Features].values, num_iteration=clf.best_iteration)  \n    \n    score = roc_auc_score(y.iloc[bayesian_val_idx]['Target'].values, oof[bayesian_val_idx])\n\n    return score","40e0dda4":"from bayes_opt import BayesianOptimization\nLGB_BO = BayesianOptimization(LGB_bayesian, paramsLGB, random_state=42)","63216aec":"init_points = 9 #Hyper params\nn_iter = 15\nRandom_Seed = 4230","8a94d098":"print('-' * 130)\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nLGB_BO.maximize(init_points=init_points, n_iter=n_iter, acq='ucb', xi=0.0, alpha=1e-6)","359c5aef":"LGB_BO.max['params']","dff5ed3d":"param = {\n        'min_data_in_leaf': int(LGB_BO.max['params']['min_data_in_leaf']), \n        'num_leaves': int(LGB_BO.max['params']['num_leaves']), \n        'learning_rate': LGB_BO.max['params']['learning_rate'],\n        'min_child_weight': LGB_BO.max['params']['min_child_weight'],\n        'bagging_fraction': LGB_BO.max['params']['bagging_fraction'], \n        'feature_fraction': LGB_BO.max['params']['feature_fraction'],\n        'reg_lambda': LGB_BO.max['params']['reg_lambda'],\n        'reg_alpha': LGB_BO.max['params']['reg_alpha'],\n        'max_depth': int(LGB_BO.max['params']['max_depth']), \n        'objective': 'binary',\n        'save_binary': True,\n        'seed': Random_Seed,\n        'feature_fraction_seed': Random_Seed,\n        'bagging_seed': Random_Seed,\n        'drop_seed': Random_Seed,\n        'data_random_seed': Random_Seed,\n        'boosting_type': 'gbdt',\n        'verbose': -1,\n        'is_unbalance': False,\n        'boost_from_average': True,\n        'metric':'auc'\n    }","265f5976":"Pipeline","95ff0b0e":"Bayesian Optimization"}}