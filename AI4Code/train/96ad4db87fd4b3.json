{"cell_type":{"8995fd09":"code","f33bdcf7":"markdown"},"source":{"8995fd09":"#!\/usr\/bin\/env python3\n\n##### \n##### .\/submission\/kaggle_compile.py .\/src\/main.py\n##### \n##### 2020-05-29 08:50:00+01:00\n##### \n##### archive\tgit@github.com:seshurajup\/kaggle-arc.git (fetch)\n##### archive\tgit@github.com:seshurajup\/kaggle-arc.git (push)\n##### origin\tgit@github.com:JamesMcGuigan\/kaggle-arc.git (fetch)\n##### origin\tgit@github.com:JamesMcGuigan\/kaggle-arc.git (push)\n##### \n##### * master 0abfdfa README.md | writeup and summary of codebase\n##### \n##### 0abfdfa9973c522bab0ee28cb7d269af62c702f2\n##### \n\n#####\n##### START src\/datamodel\/Problem.py\n#####\n\nfrom collections import UserDict\nfrom typing import Any\nfrom typing import Dict\nfrom typing import List\nfrom typing import Union\n\nimport numpy as np\n\n\n# noinspection PyUnresolvedReferences\nclass Problem(UserDict):\n    \"\"\" Problem: An input + output Grid pair \"\"\"\n    dtype = np.int8\n    def __init__(self, problem: Union[Dict[str,np.ndarray],'Problem'], problemset: 'ProblemSet'):\n        super().__init__()\n        self._hash = 0\n        self.problemset: 'ProblemSet'         = problemset\n        self.task:       'Task'               = problemset.task\n        self.raw:        Dict[str,np.ndarray] = problem.raw if isinstance(problem, Problem) else problem\n\n        self.data = {}\n        for key in ['input', 'output']:\n            value = self.cast(problem.get(key, None))\n            self.data[key] = value\n\n    def cast(self, value: Any):\n        if value is None: return None\n        value = np.array(value, dtype=self.dtype)\n        # value = np.ascontiguousarray(value, dtype=self.dtype)  # disable: could potentially mess with hashing\n        value.flags.writeable = False\n        return value\n\n    @property\n    def grids(self) -> List[np.ndarray]:\n        return  [ self.data[label]\n                  for label in ['input','output']\n                  if self.data[label] is not None ]\n\n    @property\n    def filename(self): return self.task.filename\n\n    def __eq__(self, other):\n        if not isinstance(other, (Problem, dict, UserDict)): return False\n        for label in ['input','output']:\n            if label in self  and label not in other: return False\n            if label in other and label not in self:  return False\n            if not np.array_equal(self[label], other[label]):\n                return False\n        return True\n\n    def __hash__(self):\n        if not self._hash:\n            for item in [ self.data['input'], self.data['output'] ]:\n                item = item.tobytes() if isinstance(item, np.ndarray) else item\n                self._hash += hash(item)\n        return self._hash\n\n\n#####\n##### END   src\/datamodel\/Problem.py\n#####\n\n#####\n##### START src\/datamodel\/ProblemSet.py\n#####\n\nfrom collections import UserList\nfrom typing import Dict\nfrom typing import List\nfrom typing import Union\n\nimport numpy as np\n\n# from src.datamodel.Problem import Problem\n\n\n# noinspection PyUnresolvedReferences\nclass ProblemSet(UserList):\n    \"\"\" ProblemSet: An array of either test or training Problems \"\"\"\n    _instance_count = 0\n\n    # def __new__(cls, input_outputs: Union[List[Dict[str, np.ndarray]],ProblemSet], *args, **kwargs):\n    #     if isinstance(input_outputs, ProblemSet): return input_outputs\n    #     else:                                     return super(ProblemSet, cls).__new__(cls, *args, **kwargs)\n\n    def __init__(self,\n                 input_outputs: Union[List[Dict[str, np.ndarray]],List[Problem],'ProblemSet'],\n                 test_or_train: str,\n                 task: 'Task'\n                 ):\n        super().__init__()\n        self.task:          'Task'                       = task\n        self.test_or_train: str                        = test_or_train\n        self.raw:           List[Dict[str,np.ndarray]] = input_outputs.raw if isinstance(input_outputs, ProblemSet) else input_outputs\n        self.data:          List[Problem]              = [ Problem(problem, self) for problem in self.raw ]\n        self._id = self.__class__._instance_count = self.__class__._instance_count + 1\n\n    def __eq__(self, other):\n        if not isinstance(other, ProblemSet): return False\n        return self._id == other._id\n\n    def __hash__(self):\n        return self._id\n\n    def unique(self) -> 'ProblemSet':\n        # unique = list({ hash(problem): problem for problem in self.data }.values())\n        unique = set( problem for problem in self.data )\n        if len(self.data) == len(unique):\n            return self\n        else:\n            unique = [ problem.raw for problem in self.data ]\n            return ProblemSet(unique, test_or_train=self.test_or_train, task=self.task)\n\n    @property\n    def filename(self): return self.task.filename\n\n    @property\n    def inputs(self) -> List[np.ndarray]:\n        return [ problem['input'] for problem in self.data if problem ]\n\n    @property\n    def outputs(self) -> List[np.ndarray]:\n        return [ problem['output'] for problem in self.data if problem ]\n\n    @property\n    def grids(self) -> List[np.ndarray]:\n        return self.inputs + self.outputs\n\n\n#####\n##### END   src\/datamodel\/ProblemSet.py\n#####\n\n#####\n##### START src\/settings.py\n#####\n\n# DOCS: https:\/\/www.kaggle.com\/WinningModelDocumentationGuidelines\nimport os\nimport pathlib\ntry:    root_dir = pathlib.Path(__file__).parent.parent.absolute()\nexcept: root_dir = ''\n\nsettings = {\n    'production': bool( os.environ.get('KAGGLE_KERNEL_RUN_TYPE', False) ) or 'submission' in __file__\n}\nsettings = {\n    **settings,\n    'verbose': True,\n    'debug':   not settings['production'],\n    'caching': settings['production'] or True,\n}\n\nif os.environ.get('KAGGLE_KERNEL_RUN_TYPE'):\n    settings['dir'] = {\n        \"data\":        \"..\/input\/abstraction-and-reasoning-challenge\/\",\n        \"output\":      \".\/\",\n    }\nelse:\n    settings['dir'] = {\n        \"data\":        os.path.join(root_dir, \".\/input\"),\n        \"output\":      os.path.join(root_dir, \".\/submission\"),\n    }\n\n\n####################\nif __name__ == '__main__':\n    for dirname in settings['dir'].values():\n        try:    os.makedirs(dirname, exist_ok=True)  # BUGFIX: read-only filesystem\n        except: pass\n    # for key,value in settings.items():  print(f\"settings['{key}']:\".ljust(30), str(value))\n\n\n#####\n##### END   src\/settings.py\n#####\n\n#####\n##### START src\/datamodel\/Task.py\n#####\n\nimport json\nimport os\nimport re\nfrom collections import UserDict\nfrom typing import Dict\nfrom typing import List\n\nimport numpy as np\nfrom itertools import chain\n\n# from src.datamodel.ProblemSet import ProblemSet\n# from src.settings import settings\n\n\n# noinspection PyUnresolvedReferences\nclass Task(UserDict):\n    \"\"\" Task: The entire contents of a json file, outputs 1-3 lines of CSV \"\"\"\n\n    def __init__(self, filename: str, dataset: 'Dataset' = None):\n        super().__init__()\n\n        self.dataset: 'Dataset' = dataset\n        self.filename: str      = self.format_filename(filename)\n        self.raw  = self.read_file( os.path.join(settings['dir']['data'], self.filename) )\n        self.data = {\n            test_or_train: ProblemSet(input_outputs, test_or_train, self)\n            for test_or_train, input_outputs in self.raw.items()\n        }\n        self.data['solutions']: List[ProblemSet] = [\n            ProblemSet([], test_or_train='solutions', task=self) for task in self.data['test']\n        ]\n\n    def __repr__(self):\n        return f'<{self.__class__.__name__}:{self.filename}>'\n\n    def __hash__(self):\n        return hash(self.filename)\n\n    @classmethod\n    def format_filename(cls, filename):\n        return re.sub(r'^(.*\/)?(\\w+\/\\w+\\.json)$', r'\\2', filename)\n\n    @staticmethod\n    def read_file(filename: str) -> Dict[str,List[Dict[str,np.ndarray]]]:\n        with open(filename, 'r') as file:\n            data = json.load(file)\n        for test_or_train, specs in data.items():\n            for index, spec in enumerate(specs):\n                for input_output, grid in spec.items():\n                    data[test_or_train][index][input_output] = np.array(grid).astype('int8')\n                    data[test_or_train][index][input_output].flags.writeable = False  # make immutable\n        return data\n\n    @property\n    def grids(self) -> List[np.ndarray]:\n        return list(chain(*[ spec.grids for spec in self.data.values() ]))\n\n    @property\n    def test_outputs(self) -> List[np.ndarray]:\n        return self['test'].outputs\n\n    def solve(self) -> 'Task':\n        # TODO: implement\n        print(self.__class__.__name__, 'solve()', NotImplementedError())\n        return self  # for chaining\n\n    def make_solutions_unique(self):\n        self.data['solutions'] = [\n            problemset.unique() for problemset in self.data['solutions']\n        ]\n\n\n    @property\n    def is_solved(self):\n        return all(map(len, self.data['solutions']))\n\n    @property\n    def solutions_count(self):\n        return sum(map(len, self.data['solutions']))\n\n    def score(self) -> int:\n        score = 0\n        # self.make_solutions_unique()  # Is causing exceptions\n        for index, test_problem in enumerate(self.data['test']):\n            for solution in self.data['solutions'][index]:\n                if test_problem == solution:\n                    score += 1\n                    break\n        return min(score, self.max_score())\n\n    def guesses(self) -> int:\n        score = 0\n        for index, test_problem in enumerate(self.data['test']):\n            if len(self.data['solutions'][index]):\n                score += 1\n        return min(score, self.max_score())\n\n    def max_score(self) -> int:\n        return len(self.data['test'])\n\n\n\n#####\n##### END   src\/datamodel\/Task.py\n#####\n\n#####\n##### START src\/datamodel\/CSV.py\n#####\n\nimport os\nimport re\n\nimport numpy as np\n\n# from src.settings import settings\n\n\n# noinspection PyUnresolvedReferences\nclass CSV:\n    @classmethod\n    def write_submission(cls, dataset: 'Dataset', filename='submission.csv'):\n        csv        = CSV.to_csv(dataset)\n        line_count = len(csv.split('\\n'))\n        filename   = os.path.join(settings['dir']['output'], filename)\n        with open(filename, 'w') as file:\n            file.write(csv)\n            print(f\"\\nwrote: {filename} | {line_count} lines\")\n\n    ### No need to extend sample_submission.csv, just sort the CSV\n    # @classmethod\n    # def sample_submission(cls):\n    #     filename = os.path.join(settings['dir']['data'],'sample_submission.csv')\n    #     sample_submission = pd.read_csv(filename)\n    #     return sample_submission\n    #\n    # @classmethod\n    # def write_submission(cls, dataset: 'Dataset', filename='submission.csv'):\n    #     csv        = CSV.to_csv(dataset)\n    #     lines      = csv.split('\\n')\n    #     line_count = len(lines)\n    #     data       = []\n    #\n    #     submission = cls.sample_submission()\n    #     submission = submission.set_index('output_id', drop=False)\n    #     for line in lines[1:]:  # ignore header\n    #         object_id,output = line.split(',',2)\n    #         submission.loc[object_id]['output'] = output\n    #\n    #     submission.to_csv(filename, index=False)\n    #     print(f\"\\nwrote: {filename} | {line_count} lines\")\n\n\n    @classmethod\n    def object_id(cls, filename, index=0) -> str:\n        return re.sub('^.*\/|\\.json$', '', filename) + '_' + str(index)\n\n    @classmethod\n    def to_csv(cls, dataset: 'Dataset'):\n        csv = []\n        for task in dataset:\n            line = CSV.to_csv_line(task)\n            if line: csv.append(line)\n        csv = ['output_id,output'] + sorted(csv) # object_id keys are sorted in sample_submission.csv\n        return \"\\n\".join(csv)\n\n    # noinspection PyUnusedLocal\n    @classmethod\n    def default_csv_line(cls, task: 'Task' = None) -> str:\n        return '|123|456|789|'\n\n    @classmethod\n    def to_csv_line(cls, task: 'Task') -> str:\n        csv = []\n        for index, problemset in enumerate(task['solutions']):\n            solutions = list(set(\n                cls.grid_to_csv_string(problem['output'])\n                for problem in problemset\n            ))\n            solution_str = \" \".join(solutions[:3]) if len(solutions) else cls.default_csv_line(task)\n            line = \",\".join([\n                cls.object_id(task.filename, index),\n                solution_str\n            ])\n            csv.append(line)\n        return \"\\n\".join(csv)\n\n    # Source: https:\/\/www.kaggle.com\/c\/abstraction-and-reasoning-challenge\/overview\/evaluation\n    # noinspection PyTypeChecker\n    @staticmethod\n    def grid_to_csv_string(grid: np.ndarray) -> str:\n        if grid is None: return None\n        grid = np.array(grid).astype('int8').tolist()\n        str_pred = str([ row for row in grid ])\n        str_pred = str_pred.replace(', ', '')\n        str_pred = str_pred.replace('[[', '|')\n        str_pred = str_pred.replace('][', '|')\n        str_pred = str_pred.replace(']]', '|')\n        return str_pred\n\n\n#####\n##### END   src\/datamodel\/CSV.py\n#####\n\n#####\n##### START src\/datamodel\/Dataset.py\n#####\n\nfrom collections import UserList\nfrom typing import Dict\nfrom typing import List\nfrom typing import Union\n\nimport glob2\nimport numpy as np\nimport time\nfrom itertools import chain\n\n# from src.datamodel.CSV import CSV\n# from src.datamodel.Task import Task\n\n\nclass Dataset(UserList):\n    \"\"\" Dataset: An array of all Tasks in the competition \"\"\"\n\n    def __init__(self, directory: str, name: str = ''):\n        super().__init__()\n        self.name       = name\n        self.directory  = directory\n        self.filenames  = glob2.glob( self.directory + '\/**\/*.json' )\n        self.filenames  = sorted([ Task.format_filename(filename) for filename in self.filenames ])\n        assert len(self.filenames), f'invalid directory: {directory}'\n        self.data       = [Task(filename, self) for filename in self.filenames]\n        self.time_taken = 0\n\n    def __repr__(self):\n        return f'<{self.__class__.__name__}:{self.directory}>'\n\n    def __hash__(self):\n        return hash(self.directory)\n\n    def __eq__(self, other):\n        if not isinstance(other, Dataset): return False\n        return self.directory == other.directory\n\n    def apply(self, function):\n        dataset = self\n        dataset.time_start = time.perf_counter()\n        result = function(dataset)\n        dataset.time_taken = time.perf_counter() - dataset.time_start\n        return result\n\n    def solve(self) -> 'Dataset':\n        time_start = time.perf_counter()\n        for task in self:\n            task.solve()\n        self.time_taken = time.perf_counter() - time_start\n        return self  # for chaining\n\n    def score(self) -> Dict[str,Union[int,float]]:\n        score = {}\n        score['correct'] = sum([task.score()   for task in self])\n        score['guesses'] = sum([task.guesses() for task in self])\n        score['total']   = len(self.test_outputs)\n        score['error']   = round(1 - score['correct'] \/ score['total'],4) if score['total'] else 0\n        score['time']    = self.format_clock(self.time_taken)\n        score['name']    = self.name\n        return score\n\n    @classmethod\n    def format_clock(cls, time_taken: float) -> str:\n        hours   = time_taken \/\/ (60 * 60)\n        minutes = time_taken \/\/ 60\n        seconds = time_taken % 60\n        clock   = \"{:02.0f}:{:02.0f}:{:02.0f}\".format(hours,minutes,seconds)\n        return clock\n\n    def to_csv(self):\n        return CSV.to_csv(self)\n\n    def write_submission(self, filename='submission.csv'):\n        return CSV.write_submission(self, filename)\n\n    @property\n    def test_outputs(self) -> List[np.ndarray]:\n        return list(chain(*[task.test_outputs for task in self.data]))\n\n\n#####\n##### END   src\/datamodel\/Dataset.py\n#####\n\n#####\n##### START src\/util\/plot.py\n#####\n\n# Source: https:\/\/www.kaggle.com\/jamesmcguigan\/arc-geometry-solvers\/\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom fastcache._lrucache import clru_cache\nfrom itertools import chain\nfrom matplotlib import colors\n\n# Modified from: https:\/\/www.kaggle.com\/zaharch\/visualizing-all-tasks-updated\n# from src.datamodel.Task import Task\n\n\n@clru_cache()\ndef invert_hexcode(hexcode):\n    hexcode = hexcode.replace('#','0x')\n    number  = (16**len(hexcode)-1) - int(hexcode, 16)\n    return hex(number).replace('0x','#')\n\ndef plot_one(task, ax, i,train_or_test,input_or_output):\n    hexcodes = [\n        '#000000', '#0074D9', '#FF4136', '#2ECC40', '#FFDC00',\n        '#AAAAAA', '#F012BE', '#FF851B', '#7FDBFF', '#870C25',\n    ]\n    # inverted_hexcodes = list(map(invert_hexcode,hexcodes))\n    # icmap = colors.ListedColormap(inverted_hexcodes)\n    cmap  = colors.ListedColormap(hexcodes)\n    norm  = colors.Normalize(vmin=0, vmax=9)\n\n    try:\n        input_matrix  = task[train_or_test][i][input_or_output]\n        font_size     = 50 \/ np.sqrt(input_matrix.shape[0] * input_matrix.shape[1])\n        min_font_size = 6\n\n        ax.imshow(input_matrix, cmap=cmap, norm=norm)\n        # DOC: https:\/\/stackoverflow.com\/questions\/33828780\/matplotlib-display-array-values-with-imshow\n        if font_size >= min_font_size:\n            for (j,i),label in np.ndenumerate(input_matrix):\n                ax.text(i,j,label,ha='center',va='center', fontsize=font_size, color='black')\n        ax.grid(True,which='both',color='lightgrey', linewidth=0.5)\n        ax.set_yticks([x-0.5 for x in range(1+len(input_matrix))])\n        ax.set_xticks([x-0.5 for x in range(1+len(input_matrix[0]))])\n        ax.set_xticklabels([])\n        ax.set_yticklabels([])\n        ax.set_title(train_or_test + ' '+input_or_output)\n    except: pass  # mat throw on tests, as they have not \"output\"\n\ndef plot_task(task: Task, scale=2):\n    \"\"\"\n    Plots the first train and test pairs of a specified task,\n    using same color scheme as the ARC app\n    \"\"\"\n    if isinstance(task, str): task = Task(task)\n    filename = task.filename\n    task_solutions = {\n        \"solutions\": list(chain(*task['solutions']))  # this is a 2D array now\n    }\n    num_train      = len(task['train']) + len(task['test']) + 1\n    if task.solutions_count: num_train += task.solutions_count + 1\n\n    fig, axs = plt.subplots(2, num_train, figsize=(scale*num_train,scale*2))\n    if filename: fig.suptitle(filename)\n\n    i = 0\n    for i in range(len(task['train'])):\n        plot_one(task, axs[0,i],i,'train','input')\n        plot_one(task, axs[1,i],i,'train','output')\n\n    axs[0,i+1].axis('off'); axs[1,i+1].axis('off')\n    j = 0\n    for j in range(len(task['test'])):\n        plot_one(task, axs[0,i+2+j],j,'test','input')\n        plot_one(task, axs[1,i+2+j],j,'test','output')\n\n    if task.solutions_count:\n        axs[0,i+j+3].axis('off'); axs[1,i+j+3].axis('off')\n        for k in range(len(task_solutions)):\n            plot_one(task_solutions, axs[0,i+j+4+k],k,'solutions','input')\n            plot_one(task_solutions, axs[1,i+j+4+k],k,'solutions','output')\n\n    for ax in chain(*axs): ax.axis('off')\n    plt.show()\n\n\n#####\n##### END   src\/util\/plot.py\n#####\n\n#####\n##### START src\/util\/np_cache.py\n#####\n\n# Inspired by: https:\/\/stackoverflow.com\/questions\/52331944\/cache-decorator-for-numpy-arrays\/52332109\nfrom functools import wraps\n\nimport numpy as np\nfrom fastcache._lrucache import clru_cache\n\n### Profiler: 2x speedup\n# from src.settings import settings\n\n__np_cache = {}\ndef np_cache(maxsize=1024, typed=True):\n    \"\"\"\n        Decorator:\n        @np_cache\n        def fn(): return value\n\n        @np_cache(maxsize=128, typed=True)\n        def fn(): return value\n    \"\"\"\n    maxsize_default=None\n\n    def np_cache_generator(function):\n        if not settings['caching']: return function\n        @wraps(function)\n        def wrapper(*args, **kwargs):\n            ### def encode(*args, **kwargs):\n            args = list(args)  # BUGFIX: TypeError: 'tuple' object does not support item assignment\n            for i, arg in enumerate(args):\n                if isinstance(arg, np.ndarray):\n                    hash = arg.tobytes()\n                    if hash not in wrapper.cache:\n                        wrapper.cache[hash] = arg\n                    args[i] = hash\n            for key, arg in kwargs.items():\n                if isinstance(arg, np.ndarray):\n                    hash = arg.tobytes()\n                    if hash not in wrapper.cache:\n                        wrapper.cache[hash] = arg\n                    kwargs[key] = hash\n\n            return cached_wrapper(*args, **kwargs)\n\n        @clru_cache(maxsize=maxsize, typed=typed)\n        def cached_wrapper(*args, **kwargs):\n            ### def decode(*args, **kwargs):\n            args = list(args)  # BUGFIX: TypeError: 'tuple' object does not support item assignment\n            for i, arg in enumerate(args):\n                if isinstance(arg, bytes) and arg in wrapper.cache:\n                    args[i] = wrapper.cache[arg]\n            for key, arg in kwargs.items():\n                if isinstance(arg, bytes) and arg in wrapper.cache:\n                    kwargs[key] = wrapper.cache[arg]\n\n            return function(*args, **kwargs)\n\n        # copy lru_cache attributes over too\n        wrapper.cache       = __np_cache  # use a shared cache between wrappers to save memory\n        wrapper.cache_info  = cached_wrapper.cache_info\n        wrapper.cache_clear = cached_wrapper.cache_clear\n\n        return wrapper\n\n\n    ### def np_cache(maxsize=1024, typed=True):\n    if callable(maxsize):\n        (function, maxsize) = (maxsize, maxsize_default)\n        return np_cache_generator(function)\n    else:\n        return np_cache_generator\n\n#####\n##### END   src\/util\/np_cache.py\n#####\n\n#####\n##### START src\/functions\/queries\/ratio.py\n#####\n\nfrom typing import List\nfrom typing import Tuple\nfrom typing import Union\n\nimport numpy as np\nfrom fastcache._lrucache import clru_cache\nfrom itertools import chain\n\n# from src.datamodel.Task import Task\n# from src.util.np_cache import np_cache\n\n\n@np_cache()\ndef grid_shape_ratio(grid1: np.ndarray, grid2: np.ndarray) -> Tuple[float,float]:\n    try:\n        return ( grid2.shape[0] \/ grid1.shape[0], grid2.shape[1] \/ grid1.shape[1] )\n    except:\n        return (0, 0)  # For tests\n\n@clru_cache(maxsize=None)\ndef task_grids(task) -> List[np.ndarray]:\n    grids = []\n    for test_train in ['test','train']:\n        for spec in task[test_train]:\n            grids += [ spec.get('input',[]), spec.get('output',[]) ]  # tests not guaranteed to have outputs\n    return grids\n\n@clru_cache(maxsize=None)\ndef task_grid_shapes(task) -> List[Tuple[int,int]]:\n    return [ np.array(grid).shape for grid in task_grids(task) ]\n\n@clru_cache(maxsize=None)\ndef task_grid_max_dim(task: Task) -> int:\n    return max(chain(*task_grid_shapes(task)))\n\n@clru_cache(maxsize=None)\ndef is_task_shape_ratio_unchanged(task: Task) -> bool:\n    return task_shape_ratios(task) == [ (1,1) ]\n\n@clru_cache(maxsize=None)\ndef is_task_shape_ratio_consistent(task: Task) -> bool:\n    return len(task_shape_ratios(task)) == 1\n\n@clru_cache(maxsize=None)\ndef is_task_shape_ratio_integer_multiple(task: Task) -> bool:\n    ratios = task_shape_ratios(task)\n    return all([ isinstance(d, int) or d.is_integer() for d in chain(*ratios) ])\n\n@clru_cache(maxsize=None)\ndef task_shape_ratios(task: Task) -> List[Tuple[float,float]]:\n    ratios = list(set([\n        grid_shape_ratio(problem.get('input',[]), problem.get('output',[]))\n        for problem in task['train']\n    ]))\n    # ratios = set([ int(ratio) if ratio.is_integer() else ratio for ratio in chain(*ratios) ])\n    return ratios\n\n@clru_cache(maxsize=None)\ndef task_shape_ratio(task: Task) -> Union[Tuple[float,float],None]:\n    ratios = task_shape_ratios(task)\n    if len(ratios) != 1: return None\n    return ratios[0]\n\n@clru_cache(maxsize=None)\ndef is_task_shape_ratio_integer_multiple(task: Task) -> bool:\n    ratios = task_shape_ratios(task)\n    return all([ isinstance(d, int) or d.is_integer() for d in chain(*ratios) ])\n\n@clru_cache(maxsize=None)\ndef task_output_grid_shapes(task: Task) -> List[Tuple[int,int]]:\n    return list(set(\n        problem['output'].shape\n        for problem in task['train']\n        if problem['output'] is not None\n    ))\n\n# TODO: Replace with OutputGridSizeSolver().predict()\n@clru_cache(maxsize=None)\ndef task_output_grid_shape(task) -> Union[Tuple[int,int], None]:\n    grid_sizes = task_output_grid_shapes(task)\n    return len(grid_sizes) == 1 and grid_sizes[0] or None\n\n@clru_cache(maxsize=None)\ndef is_task_output_grid_shape_constant(task: Task) -> bool:\n    return bool(task_output_grid_shape(task))\n\n\n\n#####\n##### END   src\/functions\/queries\/ratio.py\n#####\n\n#####\n##### START src\/solver_multimodel\/core\/Solver.py\n#####\n\nfrom collections import UserList\nfrom typing import Callable\nfrom typing import List\nfrom typing import Union\n\nimport numpy as np\n\n# from src.datamodel.Dataset import Dataset\n# from src.datamodel.Problem import Problem\n# from src.datamodel.Task import Task\n# from src.util.plot import plot_task\n\n\nclass Solver:\n    verbose = False\n    debug   = False\n    def __init__(self):\n        self.cache = {}\n\n\n    def detect(self, task: Task) -> bool:\n        \"\"\" @override | default heuristic is simply to run the solver\"\"\"\n        return self.test(task)\n\n\n    def fit(self, task: Task):\n        \"\"\" @override | sets: self.cache[task.filename] \"\"\"\n        if task.filename in self.cache: return\n        pass\n\n\n    def solve_grid(self, grid: np.ndarray, *args, task=None, **kwargs):\n        \"\"\" @override | This is the primary method this needs to be defined\"\"\"\n        raise NotImplementedError\n        # return grid\n        # raise NotImplementedError()\n\n\n    def test(self, task: Task) -> bool:\n        \"\"\"test if the given .solve_grid() correctly solves the task\"\"\"\n        if task.filename not in self.cache: self.fit(task)\n        if self.cache.get(task.filename, True) is None: return False\n\n        args = self.cache.get(task.filename, ())\n        return self.is_lambda_valid(task, self.solve_grid, *args, task=task)\n\n\n    def is_lambda_valid(self, _task_: Task, _function_: Callable, *args, **kwargs):  # _task_ = avoid namespace conflicts with kwargs=task\n        for problem in _task_['train']:\n            output = _function_(problem['input'], *args, **kwargs)\n            if not np.array_equal( problem['output'], output):\n                return False\n        return True\n\n\n    def format_args(self, args):\n        if isinstance(args, dict):\n            args = dict(zip(args.keys(), map(self.format_args, list(args.values()))))\n        elif isinstance(args, (list,set,tuple)):\n            args = list(args)\n            for index, arg in enumerate(args):\n                if hasattr(arg, '__name__'):\n                    arg = f\"<{type(arg).__name__}:{arg.__name__}>\"\n                if isinstance(arg, (list,set,tuple,dict)):\n                    arg = self.format_args(arg)\n                args[index] = arg\n            args = tuple(args)\n        return args\n\n\n    def log_solved(self, task: Task, args: Union[list,tuple,set], solutions: List[Problem]):\n        if self.verbose:\n            if 'test' in task.filename:           label = 'test  '\n            elif self.is_solved(task, solutions): label = 'solved'\n            else:                                 label = 'guess '\n\n            args  = self.format_args(args) if len(args) else None\n            print(f'{label}:', task.filename, self.__class__.__name__, args)\n\n\n    def is_solved(self, task: Task, solutions: List[Problem]):\n        for solution in solutions:\n            for problem in task['test']:\n                if solution == problem:\n                    return True\n        return False\n\n\n    def solve(self, task: Task, force=False) -> Union[List[Problem],None]:\n        \"\"\"solve test case and persist\"\"\"\n        if task.filename not in self.cache:             self.fit(task)\n        if self.cache.get(task.filename, True) is None: return None\n        try:\n            if self.detect(task) or force:    # may generate cache\n                if self.test(task) or force:  # may generate cache\n                    args = self.cache.get(task.filename, ())\n                    if args is None: return None\n                    if isinstance(args, dict):\n                        solutions = self.solve_task(task, self.solve_grid, **args, task=task)\n                    else:\n                        solutions = self.solve_task(task, self.solve_grid, *args, task=task)\n\n                    for index, solution in enumerate(solutions):\n                        task['solutions'][index].append(solution)\n                    if len(solutions):\n                        self.log_solved(task, args, solutions)\n                    return solutions\n        except Exception as exception:\n            if self.debug: raise exception\n        return None\n\n\n    def solve_dataset(self, tasks: Union[Dataset, List[Task]], plot=False, solve_detects=False):\n        count = 0\n        for task in tasks:\n            if self.detect(task):\n                solution = self.solve(task, force=solve_detects)\n                if solution or (solve_detects and self.test(task)):\n                    count += 1\n                    if plot:\n                        plot_task(task)\n        return count\n\n\n    def solve_task(self, _task_: Task, _function_: Callable, *args, _inplace_=False, **kwargs) -> List[Problem]:\n        solutions = []\n        for index, problem in enumerate(_task_['test']):\n            solution = self.solve_problem(\n                _problem_  = problem,\n                _task_     = _task_,\n                _function_ = _function_,\n                *args,\n                **kwargs\n            )\n            solutions.append(solution)\n        return solutions\n\n\n    def solve_problem(self, *args, _problem_: Problem, _task_: Task, _function_: Callable, **kwargs) -> Problem:\n        output = _function_(_problem_['input'], *args, **kwargs)\n        solution = Problem({\n            \"input\":  _problem_['input'],\n            \"output\": output,\n        }, problemset=_task_['test'])\n        return solution\n\n\n    def plot(self, tasks: Union[Dataset,List[Task], Task]):\n        if not isinstance(tasks, (list,UserList)): tasks = [ tasks ]\n        return self.solve_dataset(tasks, plot=True, solve_detects=False)\n\n\n    def plot_detects(self, tasks: Union[Dataset,List[Task],Task], unsolved=True):\n        if not isinstance(tasks, (list,UserList)): tasks = [ tasks ]\n        if unsolved:\n            tasks = [ task for task in tasks if not task.solutions_count ]\n        return self.solve_dataset(tasks, plot=True, solve_detects=True)\n\n\n#####\n##### END   src\/solver_multimodel\/core\/Solver.py\n#####\n\n#####\n##### START src\/functions\/transforms\/singlecolor.py\n#####\n\nfrom typing import Tuple\n\nimport numpy as np\n\n# from src.datamodel.Task import Task\n# from src.util.np_cache import np_cache\n\n\ndef identity(input: np.ndarray) -> np.ndarray:\n    return input\n\ndef np_shape(input: np.ndarray) -> Tuple[int,int]:\n    if input is None: return (0,0)\n    return np.array(input).shape\n\ndef np_resize(grid: np.ndarray, shape: Tuple[int,int]) -> np.ndarray:\n    output = np.zeros(shape, dtype=np.int8)\n    w = min(shape[0], grid.shape[0])\n    h = min(shape[1], grid.shape[1])\n    output[:w, :h] = grid[:w, :h]\n    return output\n\ndef np_flatten(input: np.ndarray) -> np.ndarray:\n    return np.array(input).flatten()\n\ndef np_tobytes(input: np.ndarray) -> bytes:\n    return np.array(input).tobytes()\n\ndef np_hash(input: np.ndarray) -> int:\n    return hash(np.array(input).tobytes())\n\n@np_cache()\ndef np_bincount(grid: np.ndarray, minlength=11):\n    return np.bincount(grid.flatten(), minlength=minlength).tolist()  # features requires a fixed length array\n\n@np_cache()\ndef unique_colors_sorted(grid: np.ndarray, minsize=11):\n    bincount = np_bincount(grid)\n    colors   = sorted(np.unique(grid), key=lambda color: bincount[color], reverse=True)\n    output   = np.zeros(minsize, dtype=np.int8)\n    output[:len(colors)] = colors\n    return output\n\n@np_cache()\ndef task_output_unique_sorted_colors(task: Task):\n    grid   = np.concatenate([ problem['output'] for problem in task['train'] ])\n    output = unique_colors_sorted(grid)\n    return output\n\n#####\n##### END   src\/functions\/transforms\/singlecolor.py\n#####\n\n#####\n##### START src\/solver_multimodel\/core\/ProblemSetSolver.py\n#####\n\nfrom collections import Callable\nfrom typing import List\nfrom typing import Union\n\nimport numpy as np\n\n# from src.datamodel.Problem import Problem\n# from src.datamodel.ProblemSet import ProblemSet\n# from src.datamodel.Task import Task\n# from src.solver_multimodel.core.Solver import Solver\n\n\nclass ProblemSetSolver(Solver):\n    def solve_task(self, _task_: Task, _function_: Callable, *args, _inplace_=False, **kwargs) -> List[Problem]:\n        self.fit(_task_)\n        if not _task_.filename in self.cache:   return []\n        if self.cache[_task_.filename] is None: return []\n\n        solutions = self.solve_grid(_task_['test'])\n        if solutions is None: return []\n\n        problemset = self.cast_problemset(solutions, task=_task_)\n        problems   = list(problemset)\n        return problems\n\n\n    def cast_problems(self, solutions: List[np.ndarray], task: Task) -> List[Problem]:\n        problemset = task['test']\n        problems   = []\n        for index, solution in enumerate(solutions):\n            problem = Problem({\n                \"input\":  problemset[index]['input'],\n                \"output\": solution,\n            }, problemset=problemset)\n            problems.append(problem)\n        return problems\n\n\n    def cast_problemset(self, solutions: List[np.ndarray], task: Task) -> ProblemSet:\n        problems = self.cast_problems(solutions, task=task)\n        output   = ProblemSet(problems, task=task, test_or_train='solutions')\n        return output\n\n\n    def predict(self, problemset: Union[ProblemSet,Task], *args, task: Task=None, **kwargs) -> Union[None,List[np.ndarray]]:\n        task       = task or (problemset if isinstance(problemset, Task) else problemset.task)\n        # problemset = (problemset['test'] if isinstance(problemset, Task) else problemset )\n        if task.filename not in self.cache:   self.fit(task)\n        if self.cache[task.filename] is None: return None  # Unsolvable mapping\n        raise NotImplementedError\n\n\n    def test(self, task: Task) -> bool:\n        \"\"\"test if .predict() correctly solves the task\"\"\"\n        self.fit(task)\n        if not task.filename in self.cache:   return False\n        if self.cache[task.filename] is None: return False\n\n        problemset = task['train']\n        training_predictions = self.predict(problemset, task=task)\n        tests_pass = bool( len(training_predictions) == len(problemset) )\n        for index, prediction in enumerate(training_predictions):\n            if not tests_pass: break\n            if not np.array_equal( task['train'][index]['output'], prediction ):\n                tests_pass = False\n        return tests_pass\n\n\n\n#####\n##### END   src\/solver_multimodel\/core\/ProblemSetSolver.py\n#####\n\n#####\n##### START src\/util\/functions.py\n#####\n\nfrom collections import UserList\nfrom typing import Any\nfrom typing import Callable\nfrom typing import List\nfrom typing import Union\n\nimport numpy as np\n\n# from src.util.np_cache import np_cache\n\n\ndef bind(function_or_value: Union[Callable,Any], *args, **kwargs) -> Callable:\n    if callable(function_or_value):\n        def _bind(*runtime_args, **runtime_kwargs):\n            return function_or_value(*args, *runtime_args, **runtime_kwargs, **kwargs)\n        return _bind\n    else:\n        # noinspection PyUnusedLocal\n        def _passthrough(*args, **kwargs):\n            return function_or_value\n        return _passthrough\n\n@np_cache()\ndef invoke(function_or_value, *args, **kwargs) -> List[Any]:\n    if callable(function_or_value):\n        return function_or_value(*args, **kwargs)\n    else:\n        return function_or_value\n\n\ndef append_flat(iterable: List, *args) -> List:\n    if not isinstance(iterable, list):\n        iterable = list(iterable)\n    for arg in args:\n        if isinstance(arg, (list,tuple,set,np.ndarray,UserList)):\n            iterable += list(arg)\n        else:\n            iterable.append(arg)\n    return iterable\n\n\ndef flatten_deep(iterable, types=(list,tuple,set,np.ndarray,UserList)) -> List:\n    output = []\n    for item in iterable:\n        if isinstance(item, types):\n            output += flatten_deep(item)\n        else:\n            output.append(item)\n    return output\n\n#####\n##### END   src\/util\/functions.py\n#####\n\n#####\n##### START src\/solver_multimodel\/core\/ProblemSetEncoder.py\n#####\n\nfrom collections import Callable\nfrom typing import Any\nfrom typing import Dict\nfrom typing import List\nfrom typing import Union\n\nimport numpy as np\nfrom itertools import product\nfrom xgboost import XGBClassifier\n\n# from src.datamodel.Problem import Problem\n# from src.datamodel.ProblemSet import ProblemSet\n# from src.datamodel.Task import Task\n# from src.functions.transforms.singlecolor import identity\n# from src.solver_multimodel.core.ProblemSetSolver import ProblemSetSolver\n# from src.util.functions import flatten_deep\n# from src.util.functions import invoke\n\n\nclass ProblemSetEncoder(ProblemSetSolver):\n    debug    = False,\n    encoders = {\n        np.array: [identity],\n    }\n    features = {\n        np.array:   [],\n        ProblemSet: [],\n        Problem:    [],\n        Task:       []\n    }\n    encoder_defaults = {}\n\n    def __init__(self,\n                 input_encoder:  Callable = None,\n                 output_encoder: Callable = None,\n                 features:       Dict = None,\n                 xgb_args:       Dict = {}\n                 ):\n        super().__init__()\n        self.input_encoder  = input_encoder\n        self.output_encoder = output_encoder\n        self.features       = features if features is not None else self.__class__.features\n        self.encoder_args   = {**self.encoder_defaults, **xgb_args}\n        self._chained_args = { \"task\": None }\n\n\n    def __call__(self, problemset: ProblemSet, task: Task):\n        return self.predict(problemset=problemset, task=task, **self._chained_args)\n\n\n    def create_encoder(self):\n        \"\"\"Return an Encoder that implements .fit() and .predict()\"\"\"\n        raise NotImplementedError\n\n\n    def fit(self, task: Task) -> bool:\n        \"\"\"Find the best input_encoder\/output_encoder for the task \"\"\"\n        if task.filename in self.cache: return True\n\n        problemset      = task['train']\n        input_encoders  = (self.input_encoder, ) if self.input_encoder  else self.encoders[np.array]\n        output_encoders = (self.output_encoder,) if self.output_encoder else self.encoders[np.array]\n        for input_encoder, output_encoder in product(input_encoders, output_encoders):\n            if not callable(input_encoder):  continue\n            if not callable(output_encoder): continue\n            self.input_encoder  = input_encoder   # this is idempotent for a one element list\n            self.output_encoder = output_encoder  # cache the last value in this loop\n\n            inputs  = self.generate_input_array(  problemset )\n            outputs = self.generate_output_array( problemset )\n\n            # See: venv\/lib\/python3.6\/site-packages\/xgboost\/sklearn.py:762\n            encoder = self.create_encoder()\n            encoder.fit(inputs, outputs, verbose=False)\n\n            self.cache[task.filename] = (encoder,)  # Needs to be set for self.test() to read and prevent an infinite loop\n            self._chained_args = { \"task\": task }\n            if self.test(task):\n                return True\n        else:\n            self.cache[task.filename] = None\n            return False\n\n\n    def predict(self,\n                problemset: Union[ProblemSet, Task],\n                xgb:   XGBClassifier = None,\n                task:  Task = None,\n                *args, **kwargs\n                ) -> Any:\n        task       = task or (problemset if isinstance(problemset, Task) else problemset.task)\n        problemset = (problemset['test'] if isinstance(problemset, Task) else problemset )\n        if task.filename not in self.cache:   self.fit(task)\n        if self.cache[task.filename] is None: return None  # Unsolvable mapping\n\n        task: Task          = task or self._chained_args.get('task')\n        xgb:  XGBClassifier = xgb  or self.cache[task.filename][0]\n\n        input  = self.generate_input_array(problemset)\n        output = xgb.predict(input)\n        return output\n\n\n    def test(self, task: Task) -> bool:\n        \"\"\"test if .predict() correctly solves the task\"\"\"\n        if task.filename not in self.cache:             self.fit(task)\n        if self.cache.get(task.filename, True) is None: return False\n\n        train_problemset = task['train']  # we test on the train side, to validate if we can .predict() on test\n        train_expected   = self.generate_output_array(train_problemset)\n        train_actual     = self.predict(train_problemset)\n        train_valid      = np.array_equal(train_expected, train_actual)\n\n        if self.debug:\n            test_problemset = task['test']  # we test on the train side, to validate if we can .predict() on test\n            test_expected = self.generate_output_array(test_problemset)\n            test_actual   = self.predict(test_problemset)\n            test_valid = np.array_equal(test_expected, test_actual)\n            print(\" | \".join([\n                task.filename.ljust(24),\n                f'{str(train_valid).ljust(5)} -> {str(test_valid).ljust(5)}',\n                f'{train_expected} -> {train_actual}',\n                f'{test_expected} -> {test_actual}',\n            ]))\n\n        return train_valid\n\n\n    # def onehotencode(self, input, maxsize=11):\n    #     output = []\n    #     for item in input:\n    #         if isinstance(item, (list, UserList, np.ndarray)):\n    #             item = self.onehotencode(item, maxsize)\n    #         value = int(item)\n    #         encoded = np.zeros(maxsize, dtype=np.int8)\n    #         encoded[value] = 1\n    #         output.append(encoded)\n    #     return np.array(output)\n\n\n    # @np_cache()\n    def generate_output_array(self, problemset: ProblemSet, output_encoder=None):\n        output_encoder = output_encoder or self.output_encoder\n        outputs = []\n        for problem in problemset:\n            if problem['output'] is None: continue\n            # input  = problem['input']\n            output = problem['output']\n            if callable(output_encoder):\n                encoded = output_encoder(output)\n                outputs.append(encoded)\n        return np.array(outputs, dtype=np.int8).flatten()\n\n\n    # @np_cache()\n    def generate_input_array(self, problemset: ProblemSet, input_encoder=None) -> np.ndarray:\n        mappings = self.generate_input_mappings(problemset, input_encoder=input_encoder)\n        for index, mapping in enumerate(mappings):\n            # noinspection PyTypeChecker\n            mappings[index] = flatten_deep(mapping.values())\n        mappings_array = np.array(mappings, dtype=np.int8)  # dtype=np.int8 is broken\n        assert mappings_array.shape[0] == len(problemset)\n        return mappings_array\n\n    # @np_cache()\n    def generate_input_mappings(self, problemset: ProblemSet, input_encoder=None) -> List[Dict[Callable, Any]]:\n        # XGBoost requires a 2D array, one slice for each problem\n        input_encoder = input_encoder or self.input_encoder\n        mappings  = []\n        for problem in problemset:\n            mapping = {}\n            for feature_fn in self.features[Task]:\n                mapping[feature_fn] = invoke(feature_fn, problemset.task)\n            for feature_fn in self.features[ProblemSet]:\n                mapping[feature_fn] = invoke(feature_fn, problemset)\n            for feature_fn in self.features[Problem]:\n                mapping[feature_fn] = invoke(feature_fn, problem)\n            for feature_fn in self.features[np.array]:\n                input               = input_encoder(problem['input']) if callable(input_encoder) else problem['input']\n                mapping[feature_fn] = invoke(feature_fn, input)\n            mappings.append(mapping)\n        return mappings\n\n\n\n\n#####\n##### END   src\/solver_multimodel\/core\/ProblemSetEncoder.py\n#####\n\n#####\n##### START src\/functions\/queries\/grid.py\n#####\n\nimport numpy as np\n\n# from skimage.measure import block_reduce\n# from numpy_lru_cache_decorator import np_cache  # https:\/\/gist.github.com\/Susensio\/61f4fee01150caaac1e10fc5f005eb75\n# from src.util.np_cache import np_cache\n\n\ndef query_true(     grid: np.ndarray, x: int, y: int ):            return True\ndef query_false(    grid: np.ndarray, x: int, y: int ):            return False\ndef query_not_zero( grid: np.ndarray, x: int, y: int ):            return grid[x,y]\ndef query_color(    grid: np.ndarray, x: int, y: int, color: int): return grid[x,y] == color\n\n\n# evaluation\/15696249.json - max(1d.argmax())\n@np_cache()\ndef query_max_color(grid,x,y,exclude_zero=True):\n    return grid[x,y] == max_color(grid, exclude_zero)\n\n@np_cache()\ndef max_color(grid, exclude_zero=True):\n    bincount = np.bincount(grid.flatten())\n    if exclude_zero:\n        bincount[0] = np.min(bincount)  # exclude 0\n    return bincount.argmax()\n\n@np_cache()\ndef query_min_color(grid:np.ndarray, x:int, y:int, exclude_zero=True):\n    return grid[x,y] == min_color(grid, exclude_zero)\n\n@np_cache()\ndef min_color(grid:np.ndarray, exclude_zero=True):\n    bincount = np.bincount(grid.flatten())\n    if exclude_zero:\n        bincount[0] = np.max(bincount)  # exclude 0\n    return bincount.argmin()\n\n@np_cache()\ndef query_max_color_1d(grid:np.ndarray, x:int, y:int, exclude_zero=True):\n    return grid[x,y] == max_color_1d(grid, exclude_zero=exclude_zero)\n\n@np_cache()\ndef max_color_1d(grid: np.ndarray, exclude_zero=True):\n    if grid is None: return 0\n    if len(grid.shape) == 1: return max_color(grid)\n    return max(\n        [ max_color(row,exclude_zero) for row in grid ] +\n        [ max_color(col,exclude_zero) for col in np.swapaxes(grid, 0,1) ]\n    )\n\n@np_cache()\ndef query_min_color_1d(grid: np.ndarray, x: int, y: int):\n    return grid[x,y] == min_color_1d(grid)\n\n@np_cache()\ndef min_color_1d(grid: np.ndarray):\n    return min(\n        [ min_color(row) for row in grid ] +\n        [ min_color(col) for col in np.swapaxes(grid, 0,1) ]\n    )\n\n@np_cache()\ndef query_count_colors(grid: np.ndarray, x: int, y: int):\n    return grid[x,y] >= count_colors(grid)\n\n@np_cache()\ndef query_count_colors_row(grid: np.ndarray, x: int, y: int):\n    return x + grid.shape[0]*y <= count_colors(grid)\n\n@np_cache()\ndef query_count_colors_col(grid: np.ndarray, x: int, y: int):\n    return y + grid.shape[1]*x <= count_colors(grid)\n\n\n@np_cache()\ndef count_colors(grid: np.ndarray):\n    bincount = np.bincount(grid.flatten())\n    return np.count_nonzero(bincount[1:]) # exclude 0\n\n@np_cache()\ndef query_count_squares(grid: np.ndarray, x: int, y: int):\n    return grid[x,y] >= count_squares(grid)\n\n@np_cache()\ndef query_count_squares_row(grid: np.ndarray, x: int, y: int):\n    return x + grid.shape[0]*y <= count_squares(grid)\n\n@np_cache()\ndef query_count_squares_col(grid: np.ndarray, x: int, y: int):\n    return y + grid.shape[1]*x <= count_squares(grid)\n\n@np_cache()\ndef count_squares(grid: np.ndarray):\n    return np.count_nonzero(grid.flatten())\n\n@np_cache()\ndef grid_unique_colors(grid: np.ndarray):\n    return np.unique(grid.flatten())\n\n\n#####\n##### END   src\/functions\/queries\/grid.py\n#####\n\n#####\n##### START src\/solver_multimodel\/core\/XGBEncoder.py\n#####\n\nfrom xgboost import XGBClassifier\n\n# from src.solver_multimodel.core.ProblemSetEncoder import ProblemSetEncoder\n\n\nclass XGBEncoder(ProblemSetEncoder):\n    # DOCS: https:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html\n    # See:  src\/solver_multimodel\/XGBGridSolver.hyperopt.py\n    # Be very conservative here as this is an inheritable class\n    encoder_defaults = {\n        **ProblemSetEncoder.encoder_defaults,\n        'eval_metric': 'error',\n        'n_jobs':      -1,\n        # 'objective':       'reg:squarederror',\n        # 'max_delta_step':   1,\n        # 'max_depth':        1,\n        # 'min_child_weight': 0,\n        # 'num_classes':     11,\n        # 'n_estimators':    1,\n        # 'max_depth':       1,\n    }\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n    def create_encoder(self):\n        encoder = XGBClassifier(**self.encoder_args)\n        return encoder\n\n\n#####\n##### END   src\/solver_multimodel\/core\/XGBEncoder.py\n#####\n\n#####\n##### START src\/datamodel\/Competition.py\n#####\n\nfrom collections import UserDict\nfrom typing import Any\nfrom typing import Dict\n\nimport time\n\n# from src.datamodel.Dataset import Dataset\n# from src.settings import settings\n\n\nclass Competition(UserDict):\n    \"\"\" Competition: The collection of all Dataset in the competition \"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.time_taken  = 0\n        self.directories = {\n            name: f\"{settings['dir']['data']}\/{name}\"\n            for name in ['training', 'evaluation', 'test']\n        }\n        self.data = {\n            name: Dataset(directory, name)\n            for name, directory in self.directories.items()\n        }\n\n    def __str__(self):\n        return \"\\n\".join([ f\"{key:11s}: {value}\" for key, value in self.score().items() ])\n\n    def solve(self) -> 'Competition':\n        time_start = time.perf_counter()\n        for name, dataset in self.data.items():\n            dataset.solve()\n        self.time_taken = time.perf_counter() - time_start\n        return self  # for chaining\n\n    def score(self) -> Dict[str,Any]:\n        score = { name: dataset.score() for name, dataset in self.data.items() }\n        success_ratio = score['evaluation']['correct'] \/ max(1e-10, score['evaluation']['guesses'])\n        score['test']['correct'] = round(score['test']['guesses'] * success_ratio, 1)\n        score['time'] = Dataset.format_clock(self.time_taken)\n        return score\n\n    @classmethod\n    def format_clock(cls, time_taken: float) -> str:\n        return Dataset.format_clock(time_taken)\n\n    def map(self, function):\n        output = []\n        competition = self\n        competition.time_start = time.perf_counter()\n        for name, dataset in competition.items():\n            result = dataset.apply(function)\n            output.append( result )\n        competition.time_taken = time.perf_counter() - competition.time_start\n        return output\n\n\n\n\n#####\n##### END   src\/datamodel\/Competition.py\n#####\n\n#####\n##### START src\/functions\/queries\/colors.py\n#####\n\nfrom fastcache._lrucache import clru_cache\n\n# from src.functions.queries.grid import grid_unique_colors\n# from src.functions.queries.ratio import is_task_shape_ratio_consistent\n\n\n@clru_cache()\ndef task_is_singlecolor(task) -> bool:\n    if not is_task_shape_ratio_consistent(task): return False\n    return all([ len(grid_unique_colors(spec['output'])) == 1 for spec in task['train'] ])\n\n\n\n#####\n##### END   src\/functions\/queries\/colors.py\n#####\n\n#####\n##### START src\/solver_multimodel\/solvers\/XGBSingleColorEncoder.py\n#####\n\n# from src.datamodel.Problem import Problem\n# from src.datamodel.ProblemSet import ProblemSet\n# from src.datamodel.Task import Task\n# from src.functions.queries.grid import *\n# from src.functions.transforms.singlecolor import identity\n# from src.functions.transforms.singlecolor import np_bincount\n# from src.functions.transforms.singlecolor import np_hash\n# from src.functions.transforms.singlecolor import np_shape\n# from src.functions.transforms.singlecolor import unique_colors_sorted\n# from src.solver_multimodel.core.XGBEncoder import XGBEncoder\n\n\nclass SingleColorXGBEncoder(XGBEncoder):\n    dtype    = np.int8,\n    encoders = {\n        np.array: [ identity ],\n    }\n    features = {\n        np.array:   [\n            unique_colors_sorted,\n            max_color,\n            max_color_1d,\n            min_color,\n            min_color_1d,\n            np_bincount,\n            np_hash,\n            np_shape,\n            \n            count_colors,\n            count_squares,\n        ],\n        ProblemSet: [],\n        Problem:    [],\n        Task:       [\n            # task_output_unique_sorted_colors\n        ]\n    }\n    encoder_defaults = {\n        **XGBEncoder.encoder_defaults,\n        'max_delta_step':   np.inf,  # unsure if this has an effect\n        'max_depth':        1,       # possibly required for this problem\n        'n_estimators':     1,       # possibly required for this problem\n        'min_child_weight': 0,       # possibly required for this problem\n        # 'max_delta_step':   1,\n        # 'objective':       'rank:map',\n        # 'objective':       'reg:squarederror',\n        # 'max_delta_step':   1,\n        # 'n_jobs':          -1,\n    }\n    def __init__(self, encoders=None, features=None, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.encoders = encoders if encoders is not None else self.__class__.encoders\n        self.features = features if features is not None else self.__class__.features\n\n\n    ### DEBUG\n    # def predict(self,\n    #             problemset: Union[ProblemSet, Task],\n    #             xgb:   XGBClassifier = None,\n    #             task:  Task = None,\n    #             *args, **kwargs\n    # ) -> Any:\n    #     task       = task or (problemset if isinstance(problemset, Task) else problemset.task)\n    #     problemset = (problemset['test'] if isinstance(problemset, Task) else problemset )\n    #     if task.filename not in self.cache:   self.fit(task)\n    #     # if self.cache[task.filename] is None: return None  # Unsolvable mapping\n    #\n    #     output = [ 8 ] * len(task['test'])\n    #     return output\n\n\n#####\n##### END   src\/solver_multimodel\/solvers\/XGBSingleColorEncoder.py\n#####\n\n#####\n##### START src\/ensemble\/util.py\n#####\n\nimport numpy as np\n\n\ndef Defensive_Copy(A):\n    n = len(A)\n    k = len(A[0])\n    L = np.zeros((n, k), dtype=np.int8)\n    for i in range(n):\n        for j in range(k):\n            L[i, j] = 0 + A[i][j]\n    return L.tolist()\n\n\ndef Create(task, task_id=0):\n    n = len(task['train'])\n    Input  = [Defensive_Copy(task['train'][i]['input'])  for i in range(n)]\n    Output = [Defensive_Copy(task['train'][i]['output']) for i in range(n)]\n    Input.append(Defensive_Copy(task['test'][task_id]['input']))\n    return Input, Output\n\n\n# noinspection PyTypeChecker\ndef flattener(pred):\n    if pred is None: return ''\n    pred = np.array(pred).astype(np.int8).tolist()\n    str_pred = str([row for row in pred])\n    str_pred = str_pred.replace(', ', '')\n    str_pred = str_pred.replace('[[', '|')\n    str_pred = str_pred.replace('][', '|')\n    str_pred = str_pred.replace(']]', '|')\n    return str_pred\n\n\n#####\n##### END   src\/ensemble\/util.py\n#####\n\n#####\n##### START src\/ensemble\/period.py\n#####\n\nimport numpy as np\n\n# from src.ensemble.util import Defensive_Copy\n# from src.util.np_cache import np_cache\n\n\n@np_cache()\ndef get_period_length0(arr):\n    H, W = arr.shape\n    period = 1\n    while True:\n        cycled = np.pad(arr[:period, :], ((0, H - period), (0, 0)), 'wrap')\n        if (cycled == arr).all():\n            return period\n        period += 1\n\n@np_cache()\ndef get_period_length1(arr):\n    H, W = arr.shape\n    period = 1\n    while True:\n        cycled = np.pad(arr[:, :period], ((0, 0), (0, W - period)), 'wrap')\n        if (cycled == arr).all():\n            return period\n        period += 1\n\n\ndef get_period(arr0):\n    if np.sum(arr0) == 0:\n        return -1\n    #     arr_crop=get_bound_image(arr0)\n    #     arr=np.array(arr_crop)\n    arr = np.array(arr0)\n    a, b = get_period_length0(arr), get_period_length1(arr)\n    period = arr[:a, :b]\n    if period.shape == arr.shape:\n        return -1\n    return period.tolist()\n\n\ndef same_ratio(basic_task):\n    # returns -1 if no match is found\n    # returns Transformed_Test_Case  if the mathching rule is found\n    # for this notebook we only look at mosaics\n    Input  = [ Defensive_Copy(x) for x in basic_task[0] ]\n    Output = [ Defensive_Copy(y) for y in basic_task[1] ]\n    same_ratio = True\n    R_x = []\n    R_y = []\n    for x, y in zip(Input[:-1], Output):\n\n        if x == []:\n            same_ratio = False\n            break\n\n        n1 = len(x)\n        n2 = len(y)\n        k1 = len(x[0])\n        k2 = len(y[0])\n\n        R_y.append(n2 \/ n1)\n        R_x.append(k2 \/ k1)\n\n    if same_ratio and min(R_x) == max(R_x) and min(R_y) == max(R_y):\n        r1 = min(R_y)\n        r2 = min(R_x)\n        return r1, r2\n\n    return -1\n\n\n#####\n##### END   src\/ensemble\/period.py\n#####\n\n#####\n##### START src\/functions\/queries\/symmetry.py\n#####\n\nimport numpy as np\n\n# from src.util.np_cache import np_cache\n\n\n@np_cache()\ndef is_grid_symmetry(grid) -> bool:\n    return (\n        is_grid_symmetry_horz(grid)\n     or is_grid_symmetry_vert(grid)\n     or is_grid_symmetry_rot90(grid)\n     or is_grid_symmetry_rot180(grid)\n     or is_grid_symmetry_transpose(grid)\n    )\n\n@np_cache()\ndef is_grid_symmetry_horz(grid) -> bool:\n    return np.array_equal(grid, np.flip(grid, 0))\n\n@np_cache()\ndef is_grid_symmetry_vert(grid) -> bool:\n    return np.array_equal(grid, np.flip(grid, 1))\n\n@np_cache()\ndef is_grid_symmetry_rot90(grid) -> bool:\n    return np.array_equal(grid, np.rot90(grid))\n\n@np_cache()\ndef is_grid_symmetry_rot180(grid) -> bool:\n    return np.array_equal(grid, np.rot90(grid,2))\n\ndef is_grid_symmetry_transpose(grid) -> bool:\n    return np.array_equal(grid, np.transpose(grid))\n\n#####\n##### END   src\/functions\/queries\/symmetry.py\n#####\n\n#####\n##### START src\/functions\/transforms\/grid.py\n#####\n\n\n# BROKEN?\n\nimport numpy as np\n\n# from src.functions.queries.grid import grid_unique_colors\n# from src.util.np_cache import np_cache\n\n\n@np_cache\ndef grid_invert_color(grid: np.ndarray):\n    colors = grid_unique_colors(grid)\n    if len(colors) != 2:\n        output = np.zeros(grid.shape, dtype=np.int8)\n        return output\n    else:\n        color1 = colors[0]\n        color2 = colors[1]\n        mask   = grid[ grid == color1 ]\n        output = np.full(grid.shape, color1, dtype=np.int8)\n        output[mask] = color2\n        return output\n\n\n\n\n#####\n##### END   src\/functions\/transforms\/grid.py\n#####\n\n#####\n##### START src\/functions\/queries\/bincount.py\n#####\n\nimport numpy as np\n\n# from src.functions.transforms.singlecolor import np_bincount\n# from src.functions.transforms.singlecolor import unique_colors_sorted\n# from src.util.np_cache import np_cache\n\n\n@np_cache()\ndef query_bincount(grid: np.ndarray, i:int, j:int, pos: 0) -> bool:\n    bincount = np_bincount(grid)\n    if len(bincount) >= pos: return False\n    result = grid[i,j] == bincount[pos]\n    return result\n\n@np_cache()\ndef query_bincount_sorted(grid: np.ndarray, i:int, j:int, pos: 0) -> bool:\n    bincount = unique_colors_sorted(grid)\n    if len(bincount) >= pos: return False\n    result = grid[i,j] == bincount[pos]\n    return result\n\n\n#####\n##### END   src\/functions\/queries\/bincount.py\n#####\n\n#####\n##### START src\/functions\/queries\/loops.py\n#####\n\nimport numpy as np\n\n# from src.functions.queries.ratio import task_shape_ratios\n# from src.util.np_cache import np_cache\n\n\n@np_cache\ndef loop_ratio(task):\n    ratio = list(task_shape_ratios(task))[0]\n    for i in range(int(ratio[0])):\n        for j in range(int(ratio[1])):\n            yield i,j\n\n# BROKEN?\n@np_cache\ndef rotate_loop(grid, start=0):\n    angle = start\n    while True:\n        yield np.rot90(grid, angle % 4)\n        angle += 1 * np.sign(start)\n\n# BROKEN?\n@np_cache\ndef rotate_loop_rows(grid, start=0):\n    angle = start\n    while True:\n        yield np.rot90(grid, angle % grid.shape[0])\n        angle += 1 * np.sign(start)\n\n# BROKEN?\n@np_cache\ndef rotate_loop_cols(grid, start=0):\n    angle = start\n    while True:\n        yield np.rot90(grid, angle % grid.shape[1])\n        angle += 1 * np.sign(start)\n\n@np_cache\ndef flip_loop(grid, start=0):\n    angle = start\n    while True:\n        if angle % 2: yield np.flip(grid)\n        else:         yield grid\n        angle += 1 * np.sign(start)\n\n# BROKEN?\n@np_cache\ndef flip_loop_rows(grid, start=0):\n    angle = start\n    while True:\n        if angle % grid.shape[0]: yield np.flip(grid)\n        else:                     yield grid\n        angle += 1 * np.sign(start)\n\n# BROKEN?\n@np_cache\ndef flip_loop_cols(grid, start=0):\n    angle = start\n    while True:\n        if angle % grid.shape[1]: yield np.flip(grid)\n        else:                     yield grid\n        angle += 1 * np.sign(start)\n\n\n\n#####\n##### END   src\/functions\/queries\/loops.py\n#####\n\n#####\n##### START src\/functions\/queries\/period.py\n#####\n\nimport numpy as np\n\n# from src.ensemble.period import get_period_length0\n# from src.ensemble.period import get_period_length1\n# from src.util.np_cache import np_cache\n\n\n@np_cache\ndef query_period_length0(grid: np.ndarray, i:int, j:int) -> bool:\n    period = get_period_length0(grid)\n    result = grid[i,j] == period\n    return result\n\n\n@np_cache\ndef query_period_length1(grid: np.ndarray, i:int, j:int) -> bool:\n    period = get_period_length1(grid)\n    result = grid[i,j] == period\n    return result\n\n\n#####\n##### END   src\/functions\/queries\/period.py\n#####\n\n#####\n##### START src\/functions\/transforms\/crop.py\n#####\n\n\n# Source: https:\/\/codereview.stackexchange.com\/questions\/132914\/crop-black-border-of-image-using-numpy\nimport numpy as np\n\n# from src.util.np_cache import np_cache\n\n\n@np_cache\ndef crop_inner(grid,tol=0):\n    mask = grid > tol\n    return grid[np.ix_(mask.any(1),mask.any(0))]\n\n@np_cache\ndef crop_outer(grid,tol=0):\n    mask = grid>tol\n    m,n  = grid.shape\n    mask0,mask1 = mask.any(0),mask.any(1)\n    col_start,col_end = mask0.argmax(),n-mask0[::-1].argmax()\n    row_start,row_end = mask1.argmax(),m-mask1[::-1].argmax()\n    return grid[row_start:row_end,col_start:col_end]\n\n\n\n#####\n##### END   src\/functions\/transforms\/crop.py\n#####\n\n#####\n##### START src\/solver_multimodel\/solvers\/GeometrySolver.py\n#####\n\nimport numpy as np\nfrom itertools import combinations\nfrom itertools import product\n\n# from src.datamodel.Competition import Competition\n# from src.functions.queries.ratio import is_task_shape_ratio_unchanged\n# from src.functions.queries.ratio import task_grid_max_dim\n# from src.functions.transforms.grid import grid_invert_color\n# from src.settings import settings\n# from src.solver_multimodel.core.Solver import Solver\n\n\nclass GeometrySolver(Solver):\n    optimise = True\n    verbose  = True\n    debug    = False\n    actions = {\n        \"flip\":      ( np.flip,      [0,1]    ),\n        \"rot90\":     ( np.rot90,     [1,2,3]  ),\n        \"roll\":      ( np.roll,      product(range(-5,5),[0,1]) ),\n        \"swapaxes\":  ( np.swapaxes,  [(0, 1),(1, 0)] ),\n        \"transpose\": ( np.transpose, []       ),                      # this doesn't find anything\n        \"none\":      ( np.copy,             []        ),\n        \"grid_invert_color\": ( grid_invert_color,   []), # BROKEN\n    }\n\n    def __init__(self):\n        super().__init__()\n        for key, (function, arglist) in self.actions.items():\n            self.actions[key] = (function, [ (args,) if not isinstance(args, tuple) else args for args in arglist ])\n\n    def detect(self, task):\n        return is_task_shape_ratio_unchanged(task)  # grids must remain the exact same size\n\n    def test(self, task):\n        if task.filename in self.cache: return True\n\n        max_roll = (task_grid_max_dim(task) + 1) \/\/ 2\n        for key, (function, arglist) in self.actions.items():\n            if function == np.roll: arglist = product(range(-max_roll,max_roll),[0,1])\n            for args in arglist:\n                if self.is_lambda_valid(task, function, *args):\n                    self.cache[task.filename] = (function, args)\n                    return True\n\n        # this doesn't find anything\n        if self.optimise: return False\n        for ((function1, arglist1),(function2, arglist2)) in combinations( self.actions.values(), 2 ):\n            if function1 == np.roll: arglist1 = product(range(-max_roll,max_roll),[0,1])\n            if function2 == np.roll: arglist2 = product(range(-max_roll,max_roll),[0,1])\n            for args1, args2 in product(arglist1, arglist2):\n                function = lambda grid, args1, args2: function1(function2(grid, *args2), *args1)\n                if self.is_lambda_valid(task, function, *(args1,args2)):\n                    self.cache[task.filename] = (function, (args1,args2))\n                    return True\n        return False\n\n    def solve_grid(self, grid, function=None, args=None, task=None):\n        try:\n            return function(grid, *args)\n        except Exception as exception:\n            if self.debug: print('Exception', self.__class__.__name__, 'solve_grid()', function, args, exception)\n            return grid\n\n\nif __name__ == '__main__' and not settings['production']:\n    solver = GeometrySolver()\n    solver.verbose = True\n    competition = Competition()\n    competition.map(solver.solve_dataset)\n    print(competition)\n\n#####\n##### END   src\/solver_multimodel\/solvers\/GeometrySolver.py\n#####\n\n#####\n##### START src\/solver_multimodel\/solvers\/ZoomSolver.py\n#####\n\nimport cv2\nimport skimage.measure\n\n# from src.functions.queries.ratio import task_shape_ratios\n# from src.solver_multimodel.core.Solver import Solver\n\n\nclass ZoomSolver(Solver):\n    verbose = False\n\n    def detect(self, task):\n        ratios = task_shape_ratios(task)\n        ratio  = list(ratios)[0]\n        detect = (\n                ratios != { (1,1) }   # not no scaling\n                and len(ratios) == 1      # not multiple scalings\n                and ratio[0] == ratio[1]  # single consistent scaling\n        )\n        return detect\n\n    def get_scale(self, task):\n        return task_shape_ratios(task)[0][0]\n\n    def solve_grid(self, grid, task=None, *args):\n        scale = self.get_scale(task)\n        if scale > 1:\n            resize = tuple( int(d*scale) for d in grid.shape )\n            output = cv2.resize(grid, resize, interpolation=cv2.INTER_NEAREST)\n        else:\n            resize = tuple( int(1\/scale) for number in grid.shape )\n            output = skimage.measure.block_reduce(grid, resize)\n        if self.verbose:\n            print('scale', scale, 'grid.shape', grid.shape, 'output.shape', output.shape)\n            print('grid', grid)\n            print('output', output)\n        return output\n\n\n#####\n##### END   src\/solver_multimodel\/solvers\/ZoomSolver.py\n#####\n\n#####\n##### START src\/util\/make_tuple.py\n#####\n\n\ndef make_tuple(args):\n    if isinstance(args, tuple): return args\n    if isinstance(args, list):  return tuple(args)\n    return (args,)\n\n#####\n##### END   src\/util\/make_tuple.py\n#####\n\n#####\n##### START src\/solver_multimodel\/solvers\/BorderSolver.py\n#####\n\n# from src.functions.queries.grid import *\n# from src.functions.queries.ratio import is_task_shape_ratio_consistent\n# from src.functions.queries.ratio import task_shape_ratio\n# from src.solver_multimodel.core.Solver import Solver\n\n\nclass BorderSolver(Solver):\n    verbose = True\n    debug = True\n    cache = {}\n    queries = [\n        *range(0,10),\n        max_color,     # FAIL: evaluation\/fc754716.json\n        max_color_1d,\n        min_color,\n        min_color_1d,\n        count_colors,\n        count_squares,\n        np.count_nonzero,\n    ]\n\n    def task_has_border(self, task):\n        if not is_task_shape_ratio_consistent(task): return False\n        return all([ self.grid_has_border(spec['output']) for spec in task['train'] ])\n\n    def grid_has_border(self, grid):\n        if min(grid.shape) <= 2: return False  # single color problem\n\n        grid_center = grid[1:-1,1:-1]\n        return np.count_nonzero(grid_center) == 0 and all([\n            np.count_nonzero(border) == len(border)\n            for border in [ grid[0,:], grid[-1,:], grid[:,0], grid[:,-1] ]\n        ])\n\n    def detect(self, task):\n        return self.task_has_border(task)\n\n    def test(self, task):\n        if task.filename in self.cache: return True\n        for query in self.queries:\n            args = [ query ]\n            if self.is_lambda_valid(task, self.solve_grid, *args, task=task):\n                self.cache[task.filename] = args\n                return True\n        return False\n\n    def solve_grid(self, grid: np.ndarray, *args, query=None, task=None, **kwargs):\n        color  = query(grid) if callable(query) else query\n        ratio  = task_shape_ratio(task)\n        if color is None: return None\n        if ratio is None: return None\n\n        shape  = ( int(grid.shape[0] * ratio[0]), int(grid.shape[1] * ratio[1]) )\n        output = np.full(shape, color, dtype=np.int8)\n        output[1:-1,1:-1] = 0\n        return output\n\n\n#####\n##### END   src\/solver_multimodel\/solvers\/BorderSolver.py\n#####\n\n#####\n##### START src\/solver_multimodel\/solvers\/DoNothingSolver.py\n#####\n\n# from src.solver_multimodel.core.Solver import Solver\n\n\nclass DoNothingSolver(Solver):\n    def solve_grid(self, grid, task=None, *args):\n        return grid\n\n\n#####\n##### END   src\/solver_multimodel\/solvers\/DoNothingSolver.py\n#####\n\n#####\n##### START src\/solver_multimodel\/solvers\/GlobSolver.py\n#####\n\n# from src.datamodel.Competition import Competition\n# from src.functions.queries.grid import *\n# from src.settings import settings\n# from src.solver_multimodel.core.Solver import Solver\n\n\nclass GlobSolver(Solver):\n    \"\"\" Create a lookup table of all previously seen input\/output pairs \"\"\"\n    verbose = True\n    debug = True\n    solutions = {}\n    cache     = {}\n\n    def __init__(self, tests_only=True):\n        super().__init__()\n        self.tests_only = tests_only\n        self.init_cache()\n\n    def init_cache(self):\n        if len(self.cache): return\n        competition = Competition()\n        for dataset_name, dataset in competition.items():\n            if dataset_name == 'test': continue  # exclude test from the cache\n            for task in dataset:\n                for name, problemset in task.items():\n                    for problem in problemset:\n                        try:\n                            if len(problem) == 0: continue\n                            if problem['input'] is None or problem['output'] is None: continue\n                            hash = problem['input'].tobytes()\n                            self.solutions[hash] = (task.filename, problem['output'])\n                        except Exception as exception:\n                            pass\n\n\n    def detect(self, task):\n        if task.filename in self.cache: return True\n        if self.tests_only and 'test' not in task.filename: return False  # We would get 100% success rate otherwise\n\n        # Loop through the all the inputs, as see if it is in our public database\n        for name, problemset in task.items():\n            inputs = [ problem['input'] for problem in problemset if problem ]\n            for input in inputs:\n                hash = input.tobytes()\n                if hash in self.solutions:\n                    filename, solutions = self.solutions[hash]\n                    self.cache[task.filename] = (filename,)  # for logging purposes\n                    return True\n        return False\n\n\n    def solve_grid(self, grid: np.ndarray, filename:str=None, task=None, *args):\n        \"\"\"If we have seen the input before, then propose the same output\"\"\"\n        hash = grid.tobytes()\n        if hash in self.solutions:\n            filename, solutions = self.solutions[hash]\n            return solutions\n        else:\n            return None\n\n\nif __name__ == '__main__' and not settings['production']:\n    solver = GlobSolver(tests_only=True)\n    solver.verbose = True\n\n    competition = Competition()\n    competition.map(solver.solve_dataset)\n    print(competition)\n\n#####\n##### END   src\/solver_multimodel\/solvers\/GlobSolver.py\n#####\n\n#####\n##### START src\/solver_multimodel\/solvers\/SingleColorSolver.py\n#####\n\n# from src.datamodel.Task import Task\n# from src.functions.queries.colors import task_is_singlecolor\n# from src.functions.queries.grid import *\n# from src.functions.queries.ratio import task_shape_ratio\n# from src.functions.queries.symmetry import is_grid_symmetry\n# from src.settings import settings\n# from src.solver_multimodel.core.Solver import Solver\n\n\nclass SingleColorSolver(Solver):\n    verbose = True\n    debug = True\n    cache = {}\n    queries = [\n        *range(0,10),\n        max_color,     # FAIL: evaluation\/fc754716.json\n        max_color_1d,\n        min_color,\n        min_color_1d,\n        count_colors,\n        count_squares,\n        np.count_nonzero,\n        is_grid_symmetry,\n    ]\n\n    def detect(self, task):\n        return task_is_singlecolor(task)\n\n    def fit(self, task: Task):\n        if task.filename in self.cache: return True\n        for query in self.queries:\n            args = ( query, )\n            if self.is_lambda_valid(task, self.solve_grid, *args, task=task):\n                self.cache[task.filename] = args\n                break\n\n    # noinspection PyMethodOverriding\n    def solve_grid(self, grid: np.ndarray, query=None, *args, task=None, **kwargs):\n        color  = query(grid) if callable(query) else query\n        ratio  = task_shape_ratio(task)\n        if color is None: return None\n        if ratio is None: return None\n\n        output = np.zeros(( int(grid.shape[0] * ratio[0]), int(grid.shape[1] * ratio[1]) ), dtype=np.int8)\n        output[:,:] = color\n        return output\n\n\n\nif __name__ == '__main__' and not settings['production']:\n    solver = SingleColorSolver()\n    solver.verbose = True\n    filenames = [\n        'training\/5582e5ca.json',  # solved\n        'training\/445eab21.json',  # solved\n        'training\/27a28665.json',\n        'training\/44f52bb0.json',\n        'evaluation\/3194b014.json',\n        'test\/3194b014.json',\n    ]\n    for filename in filenames:\n        task = Task(filename)\n        solver.plot_detects([task])\n\n    # competition = Competition()\n    # # competition['test'].apply(solver.solve_dataset)\n    # competition.map(solver.solve_dataset)\n    # print(competition)\n\n#####\n##### END   src\/solver_multimodel\/solvers\/SingleColorSolver.py\n#####\n\n#####\n##### START src\/solver_multimodel\/solvers\/TessellationSolver.py\n#####\n\nimport inspect\n\nfrom itertools import product\n\n# from src.datamodel.Competition import Competition\n# from src.datamodel.Task import Task\n# from src.functions.queries.bincount import query_bincount\n# from src.functions.queries.bincount import query_bincount_sorted\n# from src.functions.queries.grid import *\n# from src.functions.queries.loops import *\n# from src.functions.queries.period import query_period_length0\n# from src.functions.queries.period import query_period_length1\n# from src.functions.queries.ratio import is_task_shape_ratio_integer_multiple\n# from src.functions.queries.ratio import is_task_shape_ratio_unchanged\n# from src.functions.queries.symmetry import is_grid_symmetry\n# from src.functions.transforms.crop import crop_inner\n# from src.functions.transforms.crop import crop_outer\n# from src.functions.transforms.grid import grid_invert_color\n# from src.settings import settings\n# from src.solver_multimodel.solvers.GeometrySolver import GeometrySolver\n# from src.solver_multimodel.solvers.ZoomSolver import ZoomSolver\n# from src.util.make_tuple import make_tuple\n\n\nclass TessellationSolver(GeometrySolver):\n    verbose = True\n    debug   = False\n    options = {\n        \"preprocess\": {\n            \"np.copy\":     (np.copy, []),\n            \"crop_inner\":  (crop_inner, range(0,9)),\n            \"crop_outer\":  (crop_outer, range(0,9)),\n        },\n        \"transform\": {\n            \"none\":              ( np.copy,             []        ),\n            \"flip\":              ( np.flip,             [0,1]     ),\n            \"rot90\":             ( np.rot90,            [1,2,3]   ),\n            \"roll\":              ( np.roll,             product([-1,1],[0,1]) ),\n            \"swapaxes\":          ( np.swapaxes,         [(0, 1)]  ),\n            \"rotate_loop\":       ( rotate_loop,         range(-4,4) ),\n            \"rotate_loop_rows\":  ( rotate_loop_rows,    range(-4,4) ),  # BROKEN ?\n            \"rotate_loop_cols\":  ( rotate_loop_cols,    range(-4,4) ),  # BROKEN ?\n            \"flip_loop\":         ( flip_loop,           range(0,2)  ),  # BROKEN ?\n            \"flip_loop_rows\":    ( flip_loop_rows,      range(0,2)  ),  # BROKEN ?\n            \"flip_loop_cols\":    ( flip_loop_cols,      range(0,2)  ),  # BROKEN ?\n            \"grid_invert_color\": ( grid_invert_color,   []), # BROKEN\n            # TODO: Invert\n        },\n        \"query\": {\n            \"query_true\":              ( query_true,          [] ),\n            \"query_false\":             ( query_false,          [] ),\n            \"query_not_zero\":          ( query_not_zero,      [] ),\n            \"query_max_color\":         ( query_max_color,     [] ),\n            \"query_min_color\":         ( query_min_color,     [] ),\n            \"query_max_color_1d\":      ( query_max_color_1d,  [] ),\n            \"query_min_color_1d\":      ( query_min_color_1d,  [] ),\n            \"query_count_colors\":      ( query_count_colors,      [] ),\n            \"query_count_colors_row\":  ( query_count_colors_row,  [] ),\n            \"query_count_colors_col\":  ( query_count_colors_col,  [] ),\n            \"query_count_squares\":     ( query_count_squares,     [] ),\n            \"query_count_squares_row\": ( query_count_squares_row, [] ),\n            \"query_count_squares_col\": ( query_count_squares_col, [] ),\n            \"query_color\":             ( query_color,            range(0,10) ),\n            \"query_period_length0\":    ( query_period_length0,    []),\n            \"query_period_length1\":    ( query_period_length1,    []),\n            \"query_bincount\":          ( query_bincount,         range(0,10)),\n            \"query_bincount_sorted\":   ( query_bincount_sorted,  range(0,10)),\n            \"is_grid_symmetry\":        ( is_grid_symmetry,       () )\n        }\n    }\n\n\n    def detect(self, task):\n        if is_task_shape_ratio_unchanged(task):            return False  # Use GeometrySolver\n        if not is_task_shape_ratio_integer_multiple(task): return False  # Not a Tesselation problem\n        if not all([ count_colors(spec['input']) == count_colors(spec['output']) for spec in task['train'] ]): return False  # Different colors\n        if ZoomSolver().solve(task):                            return False\n        #if not self.is_task_shape_ratio_consistent(task):       return False  # Some inconsistent grids are tessellations\n        return True\n\n\n\n    def loop_options(self):\n        for (preprocess,p_args) in self.options['preprocess'].values():\n            # print( (preprocess,p_args) )\n            for p_arg in p_args or [()]:\n                p_arg = make_tuple(p_arg)\n                # print( (preprocess,p_args) )\n                for (transform,t_args) in self.options['transform'].values():\n                    for t_arg in t_args or [()]:\n                        t_arg = make_tuple(t_arg)\n                        for (query,q_args) in self.options['query'].values():\n                            for q_arg in q_args or [()]:\n                                q_arg = make_tuple(q_arg)\n                                yield (preprocess, p_arg),(transform,t_arg),(query,q_arg)\n\n\n    # TODO: hierarchical nesting of solves and solutions\/rules array generator\n    def test(self, task):\n        if task.filename in self.cache: return True\n        for (preprocess,p_arg),(transform,t_arg),(query,q_arg) in self.loop_options():\n            kwargs = {\n                \"preprocess\": preprocess,\n                \"p_arg\":      p_arg,\n                \"transform\":  transform,  # TODO: invert every other row | copy pattern from train outputs | extend lines\n                \"t_arg\":      t_arg,\n                \"query\":      query,  # TODO: max_colour limit counter\n                \"q_arg\":      q_arg,\n                }\n            if self.is_lambda_valid(task, self.solve_grid, **kwargs, task=task):\n                self.cache[task.filename] = kwargs\n                return True\n        return False\n\n\n    def solve_grid(self, grid, preprocess=np.copy, p_arg=(), transform=np.copy, t_arg=(), query=query_true, q_arg=(), task=None):\n        if inspect.isgeneratorfunction(transform):\n            generator = transform(grid, *t_arg)\n            transform = lambda grid, *args: next(generator)\n\n        # Some combinations of functions will throw geometry\n        output = None\n        try:\n            grid    = preprocess(grid, *p_arg)\n            output  = self.get_output_grid(grid, task).copy()\n            ratio   = ( int(output.shape[0] \/ grid.shape[0]), int(output.shape[1] \/ grid.shape[1]) )\n            (gx,gy) = grid.shape\n            for x,y in product(range(ratio[0]),range(ratio[1])):\n                copy = np.zeros(grid.shape, dtype=np.int8)\n                # noinspection PyArgumentList\n                if query(grid,x%gx,y%gy, *q_arg):\n                    copy = transform(grid, *t_arg)\n\n                output[x*gx:(x+1)*gx, y*gy:(y+1)*gy] = copy\n        except Exception as exception:\n            if self.debug: print(exception)\n        return output\n\n\n    def get_output_grid(self, grid, task):\n        try:\n            #print('get_output_grid(self, grid, task)', grid, task)\n            for index, spec in enumerate(task['train']):\n                if spec['input'] is grid:\n                    return spec['output']\n            else:\n                # No output for tests\n                ratio = task_shape_ratios(task)[0]\n                ratio = list(map(int, ratio))\n                shape = ( int(grid.shape[0]*ratio[0]), int(grid.shape[1]*ratio[1]) )\n                return np.zeros(shape, dtype=np.int8)\n        except Exception as exception:\n            if self.debug: print(exception)\n            pass\n\n\nif __name__ == '__main__' and not settings['production']:\n    # This is a known test success\n    task   = Task('test\/27f8ce4f.json')\n    solver = TessellationSolver()\n    solver.plot([ task ])\n    print('task.score(): ', task.score())\n\nif __name__ == '__main__' and not settings['production']:\n    solver = GeometrySolver()\n    solver.verbose = True\n    competition = Competition()\n    competition.map(solver.solve_dataset)\n    print(competition)\n\n#####\n##### END   src\/solver_multimodel\/solvers\/TessellationSolver.py\n#####\n\n#####\n##### START src\/solver_multimodel\/solvers\/XGBGridSolver.py\n#####\n\nfrom typing import List\n\nimport pydash\nfrom fastcache._lrucache import clru_cache\nfrom itertools import product\n# from src.ensemble.period import get_period_length0\n# from src.ensemble.period import get_period_length1\nfrom xgboost import XGBClassifier\n\n# from src.datamodel.Competition import Competition\n# from src.datamodel.Task import Task\n# from src.functions.queries.grid import *\n# from src.functions.queries.ratio import is_task_shape_ratio_unchanged\n# from src.functions.queries.symmetry import is_grid_symmetry\n# from src.functions.transforms.singlecolor import np_bincount\n# from src.settings import settings\n# from src.solver_multimodel.core.Solver import Solver\n# from src.util.np_cache import np_cache\n\n\nclass XGBGridSolver(Solver):\n    optimise = True\n    verbose  = True\n    xgb_defaults = {\n        'n_estimators':     32,\n        'max_depth':        10,\n\n        ### Untested\n        # 'tree_method':      'exact',\n        # 'eval_metric':      'error',\n        # 'objective':        'reg:squarederror',\n        # 'min_child_weight': 0,\n        #\n        # 'sampling_method':  'uniform',\n        # 'max_delta_step':   1,\n        # 'min_child_weight': 0,\n    }\n\n    def __init__(self, n_estimators=24, max_depth=10, **kwargs):\n        super().__init__()\n        self.kwargs = {\n            **self.xgb_defaults,\n            'n_estimators': n_estimators,\n            'max_depth':    max_depth,\n            **kwargs,\n        }\n        if self.kwargs.get('booster') == 'gblinear':\n            self.kwargs = pydash.omit(self.kwargs, *['max_depth'])\n\n    def __repr__(self):\n        return f'<{self.__class__.__name__}:self.kwargs>'\n\n    def format_args(self, args):\n        return self.kwargs\n\n    def detect(self, task):\n        if not is_task_shape_ratio_unchanged(task): return False\n        # inputs, outputs, not_valid = self.features(task)\n        # if not_valid: return False\n        return True\n\n    def create_classifier(self, **kwargs):\n        kwargs     = { **self.kwargs, **kwargs }\n        classifier = XGBClassifier(kwargs)\n        return classifier\n\n    def fit(self, task):\n        if task.filename not in self.cache:\n            # inputs  = task['train'].inputs + task['test'].inputs\n            # outputs = task['train'].outputs\n            inputs, outputs, not_valid = self.features(task)\n            if not_valid:\n                self.cache[task.filename] = None\n            else:\n                # BUGFIX: occasionally throws exceptions in jupyter\n                classifier = None\n                try:\n                    classifier = self.create_classifier()\n                    classifier.fit(inputs, outputs, verbose=False)\n                    self.cache[task.filename] = (classifier,)\n                except Exception as exception:\n                    if self.debug:\n                        print(f'{self.__class__.__name__}:fit({task}] | Exception: ')\n                        print(classifier)\n                        print(type(exception), exception)\n                    pass\n\n    def test(self, task: Task) -> bool:\n        \"\"\"test if the given solve_grid correctly solves the task\"\"\"\n        args = self.cache.get(task.filename, ())\n        return self.is_lambda_valid(task, self.solve_grid, *args, task=task)\n\n    def solve_grid(self, grid: np.ndarray, xgb=None, task=None, **kwargs):\n        if task and task.filename not in self.cache: self.fit(task)\n        xgb      = xgb or self.cache[task.filename][0]\n        features = self.make_features(grid, )\n        predict  = xgb.predict(features)\n        output   = predict.reshape(*grid.shape)\n        return output\n\n\n    @classmethod\n    @clru_cache(None)\n    def features(cls, task, mode='train'):\n        num_train_pairs = len(task[mode])\n        feat, target = [], []\n\n        for task_num in range(num_train_pairs):\n            input_color              = np.array(task[mode][task_num]['input'])\n            target_color             = task[mode][task_num]['output']\n            nrows, ncols             = len(task[mode][task_num]['input']),  len(task[mode][task_num]['input'][0])\n            target_rows, target_cols = len(task[mode][task_num]['output']), len(task[mode][task_num]['output'][0])\n\n            # TODO: Reshape all input\/outputs to largest size\n            if (target_rows != nrows) or (target_cols != ncols):\n                return None, None, 1\n\n            feat.extend(cls.make_features(input_color))\n            target.extend(np.array(target_color).reshape(-1, ))\n\n        return np.array(feat), np.array(target), 0\n\n    @classmethod\n    @np_cache()\n    def make_features(cls, grid: np.ndarray):\n        nrows, ncols = grid.shape\n        features = [\n            cls.make_feature(grid, i, j)\n            for i,j in product(range(nrows), range(ncols))\n        ]\n        assert len(set(map(len,features))) == 1\n        return np.array(features, dtype=np.int8)\n\n    @classmethod\n    @np_cache()\n    def make_feature(cls, grid: np.ndarray, i: int, j: int) -> List:\n        nrows, ncols = grid.shape\n        features = [\n            i, j, nrows-i, ncols-j,         # distance from edge\n            i+j, i-j, j-i,                  # abs(i-j) can produce worse results\n            *grid.shape, nrows*ncols,       # grid shape and pixel size\n            grid[i][j],                     # grid[i][j]+1, grid[i][j]-1 = can produce worse results\n\n            *np_bincount(grid),\n            *grid_unique_colors(grid),\n            *cls.get_moore_neighbours(grid, i, j),\n            *cls.get_tl_tr(grid, i, j),\n\n            query_not_zero(grid,i,j),\n            query_max_color(grid,i,j),\n            query_min_color(grid,i,j),\n            query_max_color_1d(grid,i,j),\n            query_min_color_1d(grid,i,j),\n            query_count_colors(grid,i,j),\n            query_count_colors_row(grid,i,j),\n            query_count_colors_col(grid,i,j),\n            query_count_squares(grid,i,j),\n            query_count_squares_row(grid,i,j),\n            query_count_squares_col(grid,i,j),\n            max_color_1d(grid),\n            min_color_1d(grid),\n            get_period_length1(grid),  # has no effect\n            get_period_length0(grid),  # has no effect\n            is_grid_symmetry(grid),\n        ]\n\n        neighbourhoods = [\n            grid,\n            cls.get_neighbourhood(grid,i,j,1),\n            cls.get_neighbourhood(grid,i,j,5),\n            grid[i,:], grid[:i+1,:], grid[i:,:],\n            grid[:,j], grid[:,:j+1], grid[:,j:],\n            grid[i:,j:], grid[:i+1,j:], grid[i:,:j+1], grid[:i+1,:j+1],\n        ]\n        for neighbourhood in neighbourhoods:\n            features += [\n                max_color(neighbourhood)           if len(neighbourhood) else 0,\n                min_color(neighbourhood)           if len(neighbourhood) else 0,\n                count_colors(neighbourhood)        if len(neighbourhood) else 0,\n                count_squares(neighbourhood)       if len(neighbourhood) else 0,\n            ]\n\n        return features\n\n    @classmethod\n    @np_cache()\n    def get_neighbourhood(cls, grid: np.ndarray, i: int, j: int, distance=1):\n        try:\n            output = np.full((2*distance+1, 2*distance+1), 11)  # 11 = outside of grid pixel\n            for x_out, x_grid in enumerate(range(-distance, distance+1)):\n                for y_out, y_grid in enumerate(range(-distance, distance+1)):\n                    if not 0 <= x_out < grid.shape[0]: continue\n                    if not 0 <= y_out < grid.shape[1]: continue\n                    output[x_out,y_out] = grid[i+x_grid,j+y_grid]\n            return output\n        except:\n            return np.full((2*distance+1, 2*distance+1), 11)  # 11 = outside of grid pixel\n\n    @classmethod\n    @np_cache()\n    def get_moore_neighbours(cls, color, cur_row, cur_col):\n        nrows, ncols = color.shape\n        top    = -1 if cur_row <= 0         else color[cur_row - 1][cur_col]\n        bottom = -1 if cur_row >= nrows - 1 else color[cur_row + 1][cur_col]\n        left   = -1 if cur_col <= 0         else color[cur_row][cur_col - 1]\n        right  = -1 if cur_col >= ncols - 1 else color[cur_row][cur_col + 1]\n        return top, bottom, left, right\n\n\n    @classmethod\n    @np_cache()\n    def get_tl_tr(cls, color, cur_row, cur_col):\n        nrows, ncols = color.shape\n        top_left  = -1 if cur_row == 0 or cur_col == 0         else color[cur_row - 1][cur_col - 1]\n        top_right = -1 if cur_row == 0 or cur_col == ncols - 1 else color[cur_row - 1][cur_col + 1]\n        return top_left, top_right\n\n\nclass XGBGridSolverDart(XGBGridSolver):\n    kwargs_defaults = {\n        'booster': 'dart',\n        'eval_metric': 'error',\n        'grow_policy': 'lossguide',\n        'objective': 'reg:squaredlogerror',\n        'sampling_method': 'gradient_based',\n        'tree_method': 'hist'\n    }\n    def __init__(self, **kwargs):\n        self.kwargs = { **self.kwargs_defaults, **kwargs }\n        super().__init__(**self.kwargs)\n\nclass XGBGridSolverGBtree(XGBGridSolver):\n    kwargs_defaults = {\n        'booster': 'gbtree',\n        'eval_metric': 'ndcg',\n        'grow_policy': 'depthwise',\n        'objective': 'reg:squarederror',\n        'sampling_method': 'uniform',\n        'tree_method': 'exact'\n    }\n    def __init__(self, booster='gbtree', **kwargs):\n        self.kwargs = { \"booster\": booster, **self.kwargs_defaults, **kwargs }\n        super().__init__(**self.kwargs)\n\nclass XGBGridSolverGBlinear(XGBGridSolver):\n    def __init__(self, booster='gblinear', **kwargs):\n        self.kwargs = { \"booster\": booster, \"max_depth\": None, **kwargs }\n        super().__init__(**self.kwargs)\n\n\nif __name__ == '__main__' and not settings['production']:\n    solver = XGBGridSolver()\n    solver.verbose = True\n    competition = Competition()\n    competition.map(solver.solve_dataset)\n    print(competition)\n\n\n\n\n### Original\n# training   : {'correct': 18, 'guesses': 49, 'total': 416, 'error': 0.9567, 'time': '00:00:00', 'name': 'training'}\n# evaluation : {'correct': 3, 'guesses': 19, 'total': 419, 'error': 0.9928, 'time': '00:00:00', 'name': 'evaluation'}\n# test       : {'correct': 0, 'guesses': 8, 'total': 104, 'error': 1.0, 'time': '00:00:00', 'name': 'test'}\n\n### Add i+-1, j+-1\n# training   : {'correct': 50, 'total': 416, 'error': 0.8798, 'time': '00:00:00', 'name': 'training'}\n# evaluation : {'correct': 19, 'total': 419, 'error': 0.9547, 'time': '00:00:00', 'name': 'evaluation'}\n# test       : {'correct': 8, 'total': 104, 'error': 0.9231, 'time': '00:00:00', 'name': 'test'}\n# time       : 00:00:30\n\n### Add i+-j\n# training   : {'correct': 54, 'total': 416, 'error': 0.8702, 'time': '00:00:00', 'name': 'training'}\n# evaluation : {'correct': 20, 'total': 419, 'error': 0.9523, 'time': '00:00:00', 'name': 'evaluation'}\n# test       : {'correct': 8, 'total': 104, 'error': 0.9231, 'time': '00:00:00', 'name': 'test'}\n# time       : 00:00:28\n\n### Add abs(i-j) - dangerous\n# training   : {'correct': 58, 'total': 416, 'error': 0.8606, 'time': '00:00:00', 'name': 'training'}\n# evaluation : {'correct': 17, 'total': 419, 'error': 0.9594, 'time': '00:00:00', 'name': 'evaluation'}\n# test       : {'correct': 6, 'total': 104, 'error': 0.9423, 'time': '00:00:00', 'name': 'test'}\n# time       : 00:00:27\n\n### Add color+-1\n# training   : {'correct': 50, 'total': 416, 'error': 0.8798, 'time': '00:00:00', 'name': 'training'}\n# evaluation : {'correct': 19, 'total': 419, 'error': 0.9547, 'time': '00:00:00', 'name': 'evaluation'}\n# test       : {'correct': 8, 'total': 104, 'error': 0.9231, 'time': '00:00:00', 'name': 'test'}\n# time       : 00:00:31\n\n### max_color(grid),; min_color(grid),; max_color_1d(grid),; min_color_1d(grid),; count_colors(grid),; count_squares(grid),\n# training   : {'correct': 88, 'total': 416, 'error': 0.7885, 'time': '00:00:00', 'name': 'training'}\n# evaluation : {'correct': 57, 'total': 419, 'error': 0.864, 'time': '00:00:00', 'name': 'evaluation'}\n# test       : {'correct': 17, 'total': 104, 'error': 0.8365, 'time': '00:00:00', 'name': 'test'}\n# time       : 00:00:45\n\n### max_color(grid),; min_color(grid),; max_color_1d(grid),; min_color_1d(grid),; count_colors(grid),; count_squares(grid),\n### query_not_zero(grid,i,j),; query_max_color(grid,i,j),; query_min_color(grid,i,j),; query_max_color_1d(grid,i,j),; query_min_color_1d(grid,i,j),; query_count_colors(grid,i,j),  # query_count_colors_row(grid,i,j), query_count_colors_col(grid,i,j), query_count_squares(grid,i,j), # query_count_squares_row(grid,i,j), query_count_squares_col(grid,i,j),\n# training   : {'correct': 89, 'total': 416, 'error': 0.7861, 'time': '00:00:00', 'name': 'training'}\n# evaluation : {'correct': 59, 'total': 419, 'error': 0.8592, 'time': '00:00:00', 'name': 'evaluation'}\n# test       : {'correct': 18, 'total': 104, 'error': 0.8269, 'time': '00:00:00', 'name': 'test'}\n# time       : 00:01:05\n\n### *grid.shape\n# training   : {'correct': 95, 'total': 416, 'error': 0.7716, 'time': '00:00:00', 'name': 'training'}\n# evaluation : {'correct': 62, 'total': 419, 'error': 0.852, 'time': '00:00:00', 'name': 'evaluation'}\n# test       : {'correct': 17, 'total': 104, 'error': 0.8365, 'time': '00:00:00', 'name': 'test'}\n# time       : 00:01:18\n\n### *np.bincount(grid.flatten(), minlength=10),; *sorted(np.bincount(grid.flatten(), minlength=10)),\n# training   : {'correct': 99, 'total': 416, 'error': 0.762, 'time': '00:00:00', 'name': 'training'}\n# evaluation : {'correct': 62, 'total': 419, 'error': 0.852, 'time': '00:00:00', 'name': 'evaluation'}\n# test       : {'correct': 17, 'total': 104, 'error': 0.8365, 'time': '00:00:00', 'name': 'test'}\n# time       : 00:01:29\n\n\n### *grid.shape, nrows-i, ncols-j,\n# training   : {'correct': 109, 'total': 416, 'error': 0.738, 'time': '00:00:00', 'name': 'training'}\n# evaluation : {'correct': 70, 'total': 419, 'error': 0.8329, 'time': '00:00:00', 'name': 'evaluation'}\n# test       : {'correct': 18, 'total': 104, 'error': 0.8269, 'time': '00:00:00', 'name': 'test'}\n# time       : 00:01:21\n\n### len(np.bincount(grid.flatten())), *np.bincount(grid.flatten(), minlength=10)\n# training   : {'correct': 107, 'total': 416, 'error': 0.7428, 'time': '00:00:00', 'name': 'training'}\n# evaluation : {'correct': 70, 'total': 419, 'error': 0.8329, 'time': '00:00:00', 'name': 'evaluation'}\n# test       : {'correct': 18, 'total': 104, 'error': 0.8269, 'time': '00:00:00', 'name': 'test'}\n# time       : 00:01:17\n\n\n### neighbourhood\n# training   : {'correct': 111, 'total': 416, 'error': 0.7332, 'time': '00:00:00', 'name': 'training'}\n# evaluation : {'correct': 71, 'total': 419, 'error': 0.8305, 'time': '00:00:00', 'name': 'evaluation'}\n# test       : {'correct': 18, 'total': 104, 'error': 0.8269, 'time': '00:00:00', 'name': 'test'}\n# time       : 00:01:36\n\n# training   : {'correct': 112, 'total': 416, 'error': 0.7308, 'time': '00:00:00', 'name': 'training'}\n# evaluation : {'correct': 72, 'total': 419, 'error': 0.8282, 'time': '00:00:00', 'name': 'evaluation'}\n# test       : {'correct': 19, 'total': 104, 'error': 0.8173, 'time': '00:00:00', 'name': 'test'}\n# time       : 00:02:50\n\n### without features += cls.get_neighbourhood(grid,i,j,local_neighbours).flatten().tolist()\n# training   : {'correct': 112, 'total': 416, 'error': 0.7308, 'time': '00:00:00', 'name': 'training'}\n# evaluation : {'correct': 78, 'total': 419, 'error': 0.8138, 'time': '00:00:00', 'name': 'evaluation'}\n# test       : {'correct': 22, 'total': 104, 'error': 0.7885, 'time': '00:00:00', 'name': 'test'}\n# time       : 00:02:43\n\n### for line in ( grid[i,:], grid[:,j] ):\n# training   : {'correct': 127, 'total': 416, 'error': 0.6947, 'time': '00:00:00', 'name': 'training'}\n# evaluation : {'correct': 87, 'total': 419, 'error': 0.7924, 'time': '00:00:00', 'name': 'evaluation'}\n# test       : {'correct': 22, 'total': 104, 'error': 0.7885, 'time': '00:00:00', 'name': 'test'}\n# time       : 00:02:07\n\n### for line_neighbourhood in [grid[i,:], grid[:i+1,:], grid[i:,:], grid[:,j], grid[:,:j+1], grid[:,j:], cls.get_neighbourhood(grid,i,j,1), cls.get_neighbourhood(grid,i,j,3), cls.get_neighbourhood(grid,i,j,5),]:\n# training   : {'correct': 138, 'total': 416, 'error': 0.6683, 'time': '00:00:00', 'name': 'training'}\n# evaluation : {'correct': 109, 'total': 419, 'error': 0.7399, 'time': '00:00:00', 'name': 'evaluation'}\n# test       : {'correct': 31, 'total': 104, 'error': 0.7019, 'time': '00:00:00', 'name': 'test'}\n# time       : 00:02:28\n\n### for neighbourhood in [ grid[i:,j:], grid[:i+1,j:], grid[i:,:j+1], grid[:i+1,:j+1], ]\n# training   : {'correct': 148, 'total': 416, 'error': 0.6442, 'time': '00:00:00', 'name': 'training'}\n# evaluation : {'correct': 116, 'total': 419, 'error': 0.7232, 'time': '00:00:00', 'name': 'evaluation'}\n# test       : {'correct': 33, 'total': 104, 'error': 0.6827, 'time': '00:00:00', 'name': 'test'}\n# time       : 00:03:10\n\n# XGBGridSolver(n_estimators=10)\n# training   : {'correct': 22, 'guesses': 148, 'total': 416, 'error': 0.9471, 'time': '00:00:00', 'name': 'training'}\n# evaluation : {'correct': 9, 'guesses': 116, 'total': 419, 'error': 0.9785, 'time': '00:00:00', 'name': 'evaluation'}\n# test       : {'correct': 0, 'guesses': 33, 'total': 104, 'error': 1.0, 'time': '00:00:00', 'name': 'test'}\n# time       : 00:00:53\n\n# XGBGridSolver(n_estimators=32)\n# training   : {'correct': 25, 'guesses': 255, 'total': 416, 'error': 0.9399, 'time': '00:00:00', 'name': 'training'}\n# evaluation : {'correct': 10, 'guesses': 257, 'total': 419, 'error': 0.9761, 'time': '00:00:00', 'name': 'evaluation'}\n# test       : {'correct': 0, 'guesses': 64, 'total': 104, 'error': 1.0, 'time': '00:00:00', 'name': 'test'}\n# time       : 00:02:39\n\n### XGBSolverDart(); XGBSolverGBtree(); XGBSolverGBlinear()\n# training   : {'correct': 42, 'guesses': 254, 'total': 416, 'error': 0.899, 'time': '00:03:31', 'name': 'training'}\n# evaluation : {'correct': 14, 'guesses': 242, 'total': 419, 'error': 0.9666, 'time': '00:07:17', 'name': 'evaluation'}\n# test       : {'correct': 3.5, 'guesses': 61, 'total': 104, 'error': 1.0, 'time': '00:01:35', 'name': 'test'}\n# time       : 00:12:23\n\n### max_depth=10\n# training   : {'correct': 43, 'guesses': 266, 'total': 416, 'error': 0.8966, 'time': '00:04:49', 'name': 'training'}\n# evaluation : {'correct': 14, 'guesses': 266, 'total': 419, 'error': 0.9666, 'time': '00:08:23', 'name': 'evaluation'}\n# test       : {'correct': 3.4, 'guesses': 65, 'total': 104, 'error': 1.0, 'time': '00:01:40', 'name': 'test'}\n# time       : 00:14:53\n\n\n#####\n##### END   src\/solver_multimodel\/solvers\/XGBGridSolver.py\n#####\n\n#####\n##### START src\/solver_multimodel\/solvers\/XGBSingleColorSolver.py\n#####\n\nfrom typing import List\nfrom typing import Union\n\n# from src.datamodel.Competition import Competition\n# from src.datamodel.ProblemSet import ProblemSet\n# from src.datamodel.Task import Task\n# from src.functions.queries.colors import task_is_singlecolor\n# from src.functions.queries.grid import *\n# from src.functions.queries.ratio import is_task_output_grid_shape_constant\n# from src.functions.queries.ratio import task_output_grid_shape\n# from src.settings import settings\n# from src.solver_multimodel.core.ProblemSetSolver import ProblemSetSolver\n# from src.solver_multimodel.solvers.XGBSingleColorEncoder import SingleColorXGBEncoder\n# from src.util.plot import plot_task\n\n\n# BUG: XGBoost only works if the output colors have already been seen in the input\nclass XGBSingleColorSolver(ProblemSetSolver):\n    verbose = True\n    debug   = True\n    cache   = {}\n\n    def detect(self, task):\n        return all([\n            task_is_singlecolor(task),\n            is_task_output_grid_shape_constant(task)  # TODO: OutputGridSizeSolver\n        ])\n\n\n    def fit(self, task: Task):\n        if task.filename in self.cache: return True\n        encoder = SingleColorXGBEncoder(output_encoder=max_color)\n        encoder.fit(task)\n        colors  = encoder.predict(task)\n        self.cache[task.filename] = colors\n\n    ### DEBUG\n    # def test(self, task: Task = None) -> Any:\n    #     return True\n\n    # BUGFIX: TypeError: solve_grid() got multiple values for argument 'task'\n    def predict(self, problemset: Union[ProblemSet,Task], *args, task: Task=None, **kwargs) -> Union[None,List[np.ndarray]]:\n        task       = task or (problemset if isinstance(problemset, Task) else problemset.task)\n        # problemset = (problemset['test'] if isinstance(problemset, Task) else problemset )\n        if task.filename not in self.cache:   self.fit(task)\n        if self.cache[task.filename] is None: return None  # Unsolvable mapping\n\n        colors = self.cache[task.filename]\n        output_size = task_output_grid_shape(task) # TODO: Replace with OutputGridSizeSolver().solve_grid() per problem\n        outputs = [\n            np.full(output_size, fill_value=color)\n            for color in colors\n        ]\n        return outputs\n\n\n\nif __name__ == '__main__' and not settings['production']:\n    solver = XGBSingleColorSolver()\n    solver.verbose = True\n    filenames = [\n        'training\/5582e5ca.json',  # solved by SingleColorSolver\n        'training\/445eab21.json',  # solved by SingleColorSolver\n        'training\/27a28665.json',\n        'training\/44f52bb0.json',\n        'evaluation\/3194b014.json',\n        'test\/3194b014.json',\n    ]\n    for filename in filenames:\n        task = Task(filename)\n        plot_task(task)\n        solver.plot(task)\n\n    competition = Competition()\n\n    for name, dataset in competition.items():\n        solver.plot(dataset)\n\n    # competition['test'].apply(solver.solve_dataset)\n    competition.map(solver.solve_dataset)\n    print(competition)\n\n#####\n##### END   src\/solver_multimodel\/solvers\/XGBSingleColorSolver.py\n#####\n\n#####\n##### START src\/solvers.py\n#####\n\nfrom typing import List\n\n# from src.solver_multimodel.core.Solver import Solver\n# from src.solver_multimodel.solvers.BorderSolver import BorderSolver\n# from src.solver_multimodel.solvers.DoNothingSolver import DoNothingSolver\n# from src.solver_multimodel.solvers.GeometrySolver import GeometrySolver\n# from src.solver_multimodel.solvers.GlobSolver import GlobSolver\n# from src.solver_multimodel.solvers.SingleColorSolver import SingleColorSolver\n# from src.solver_multimodel.solvers.TessellationSolver import TessellationSolver\n# from src.solver_multimodel.solvers.XGBGridSolver import XGBGridSolver\n# from src.solver_multimodel.solvers.XGBSingleColorSolver import XGBSingleColorSolver\n# from src.solver_multimodel.solvers.ZoomSolver import ZoomSolver\n\nsolvers: List[Solver] = [\n    # Deterministic (all solved answers are correct)\n    GlobSolver(),\n    DoNothingSolver(),\n    BorderSolver(),\n    GeometrySolver(),\n    SingleColorSolver(),\n    ZoomSolver(),\n    TessellationSolver(),\n\n    # Non-Deterministic (lots of random guesses)\n    XGBSingleColorSolver(),\n    XGBGridSolver(),\n    # XGBGridSolverDart(),     # These don't provide any additional value\n    # XGBGridSolverGBtree(),\n    # XGBGridSolverGBlinear(),\n]\n\n\n#####\n##### END   src\/solvers.py\n#####\n\n#####\n##### START .\/src\/main.py\n#####\n\nfrom operator import itemgetter\n\nimport gc\nimport time\n\n# from src.datamodel.Competition import Competition\n# from src.settings import settings\n# from src.solvers import solvers\n\nif __name__ == '__main__':\n    print('\\n','-'*20,'\\n')\n    print('Abstraction and Reasoning Challenge')\n    print('Team: Mathematicians + Experts')\n    print('https:\/\/www.kaggle.com\/c\/abstraction-and-reasoning-challenge')\n    print('\\n','-'*20,'\\n')\n    for solver in solvers: print(solver.__class__.__name__)\n    print('\\n','-'*20,'\\n')\n\n    plot_results = True # not settings['production']\n    time_start   = time.perf_counter()\n    competition  = Competition()\n    scores       = { name: { solver.__class__.__name__: 0 for solver in solvers } for name in competition.keys() }\n    for dataset_name, dataset in competition.items():\n        time_start_dataset = time.perf_counter()\n        for solver in solvers:\n            print('#######', dataset_name, solver.__class__.__name__)\n            if plot_results:\n                scores[dataset_name][solver.__class__.__name__] += solver.plot(dataset)\n            else:\n                scores[dataset_name][solver.__class__.__name__] += solver.solve_dataset(dataset)\n            # Running on Kaggle uses up nearly all 16GB of RAM\n            solver.cache = {}\n            gc.collect()\n\n        dataset.time_taken = time.perf_counter() - time_start_dataset\n    competition.time_taken = time.perf_counter() - time_start\n\n    competition['test'].write_submission('submission5.csv')\n    competition['test'].write_submission()\n\n    print('-'*20)\n    print('Solver Scores:')\n    for dataset_name in scores.keys():\n        print(f'\\n# {dataset_name}')\n        for solver_name, score in sorted(scores[dataset_name].items(), key=itemgetter(1), reverse=True):\n            if score: print(score, solver_name)\n    print('-'*20)\n    print('Dataset Scores:')\n    print(competition)\n\n\n\n#####\n##### END   .\/src\/main.py\n#####\n\n##### \n##### .\/submission\/kaggle_compile.py .\/src\/main.py\n##### \n##### 2020-05-29 08:50:00+01:00\n##### \n##### archive\tgit@github.com:seshurajup\/kaggle-arc.git (fetch)\n##### archive\tgit@github.com:seshurajup\/kaggle-arc.git (push)\n##### origin\tgit@github.com:JamesMcGuigan\/kaggle-arc.git (fetch)\n##### origin\tgit@github.com:JamesMcGuigan\/kaggle-arc.git (push)\n##### \n##### * master 0abfdfa README.md | writeup and summary of codebase\n##### \n##### 0abfdfa9973c522bab0ee28cb7d269af62c702f2\n##### \n","f33bdcf7":"# Kaggle ARC Abstraction and Reasoning Challenge\n# OO Framework + XGBoost Multimodel Solvers\n\n\n# Visualized Notebook Kernel\n- 0.990 (1\/104) - https:\/\/www.kaggle.com\/jamesmcguigan\/arc-oo-framework-xgboost-multimodel-solvers\n\nThis github codebase contains my entry and ongoing research into the Abstraction and Reasoning Corpus\n- https:\/\/github.com\/JamesMcGuigan\/kaggle-arc\n\n\n# Introduction\n\nIn many ways, I feel I had only just gotten started with this competition by the time the submission deadline hit. This is a very interesting problem that has let me start understanding the machine learning toolbox in a completely new way. I am curious now about what this dataset can show us about the limits of each of the other tools in the machine learning toolbox.\n\nMy first attempt was written inside a Kaggle notebook, attempting to solve geometry and tessellation problems by hand visualizing the results as I went along. I had reached the level \n- https:\/\/www.kaggle.com\/jamesmcguigan\/arc-geometry-solvers\/\n\nThis got me sufficiently noticed to be invited to join my first team: \"Mathematicians + Experts\" who gave me new perspectives on how to approach Kaggle Competitions as well as introducing me to new ideas. Thank you.\n\nI realized at this point that I had reached the complexity limit for code written in a jupyter notebook,  so ported my code to a multi-file IDE github repo and used my previously written `kaggle_compile.py` script to create a single `submission.py` which could easily be copy\/pasted into Kaggle. \n- https:\/\/www.kaggle.com\/jamesmcguigan\/kaggle-compile-py-python-ide-to-kaggle-compiler\/\n\nMy next approach was to write a typed Object Oriented Class Model around the dataset, to allow for static typing (yay for linters!) and ease of navigaton between related data-structures. I also converted all the grids into `numpy.array(dtype=np.int8)`.\n\nAttempting to solve the general problem of `f(g(h(x)))`, I wrote proof of concept code using `inspect.signiture()` figure out all possible permutations of `f(g(h(x)))` and implement an IoC dependency injection solver. Then the deadline started getting close.\n\nI then went back to my original notebook code and properly ported it into the IDE. This resulted in `Solver` and `ProblemSetSorter` and `ProblemEncoder` base classes containing all the common case logic for looping over the dataset and applying the `.detect(task)`, `.fit(task)`, `.predict(task)`, `.test(task)`, `.solve_grid(np.ndarray, *args)` lifecycle. This significantly reduces the effort required to write new solver algorithms, when all that is needed is to implement a subset of the lifecycle methods. The rest of the autowiring is handled by the base class. \n\nCombined with the OO DataModel, and a modified `plot_task()` function (that also shows the numbers and algorithm solutions), this provides a very clean interface for showcasing new Solvers within a Jupiter Notebook.\n\nLooking through the teams collective codebase, I encounter XGBoost for the first time. I ported the existing code into my Solver framework with significant refactoring, and then started playing around with greatly increasing the featuremap and hyperparaeter tuning. I was able to greatly improve the success rate of the algorithm  \n```\n### Original\ntraining   : {'correct': 18, 'guesses': 49, 'total': 416, 'error': 0.9567, 'name': 'training'}\nevaluation : {'correct': 3, 'guesses': 19, 'total': 419, 'error': 0.9928, 'name': 'evaluation'}\ntest       : {'correct': 0, 'guesses': 8, 'total': 104, 'error': 1.0, 'name': 'test'}\ntime       : 00:00:30\n\n\n### XGBGridSolver(n_estimators=32, max_depth=10)\ntraining   : {'correct': 43, 'guesses': 266, 'total': 416, 'error': 0.8966, 'name': 'training'}\nevaluation : {'correct': 14, 'guesses': 266, 'total': 419, 'error': 0.9666, 'name': 'evaluation'}\ntest       : {'correct': 3.4, 'guesses': 65, 'total': 104, 'error': 1.0, 'name': 'test'}\ntime       : 00:14:53\n```\n\nWhat surprised me was how many seemingly unrelated classes of problem XGBoost could autosolve (in the training and evaluation dataset) when given a sufficiently large multi-dimensional (flattened) feature map, including each pixel's relative \"view\" of neighbouring pixels. However, this XGBoost approach also led to a large amount of random guessing, as it could quite often find rules that matched all the `train` examples, but incorrectly matched the `test`. This approach was able to score `0.990` == `1\/104` on the public leaderboard.\n\nThe full list of solved training and evaluation tasks is visualized in the notebook: \n- 0.990 (1\/104) - https:\/\/www.kaggle.com\/jamesmcguigan\/arc-oo-framework-xgboost-multimodel-solvers\n\n\nI did a complete rewrite of the XGBoostSolver, turning it into a fully subclassable ProblemSetSolver and Encoder base classes, that would allow configuration based definition of typed univariate functions that could be used to auto-generate a feature map. However,  it turns out that XGBoost had signifcant trouble when faced with the seemingly simple Single Color Problem. \n\nI also showed how to implement Hyperopt Bayesian Hyperparameter Optimization was also performed on XGBoost.\n\nI also wrote a few silly Solvers, as the `DoNothingSolver()`, and the `GlobSolver()` which did a hashmap lookup on the `training` and `evaluation` datasets incase any of the hidden submission dataset. Alas,  these did not find anything.\n\n\n\n# Install and Execute\n```\n# Source: .\/submission\/kaggle_compile.sh\n\n.\/requirements.sh\nsource venv\/bin\/activate\n\npython3 .\/submission\/kaggle_compile.py src\/main.py | tee .\/submission\/submission.py\npython3 .\/submission\/kaggle_compile.py src\/ensemble\/sample_sub\/sample_sub_combine.py | tee .\/submission\/submission.py\n\ntime -p python3 .\/submission\/submission.py | tee .\/submission\/submission.log\n```\n\n# Gallery\nJupyter Notebook visualization of the solved and unsolved results for each of the Solvers\n- [notebooks_gallery\/solved](https:\/\/github.com\/JamesMcGuigan\/kaggle-arc\/tree\/master\/notebooks_gallery\/solved)\n- [notebooks_gallery\/solved](https:\/\/github.com\/JamesMcGuigan\/kaggle-arc\/tree\/master\/notebooks_gallery\/detects)\n\n\n# Data Model\n- Source: [src\/datamodel\/](https:\/\/github.com\/JamesMcGuigan\/kaggle-arc\/tree\/master\/src\/datamodel\/)\n\nThis is an object oriented data model around the dataset,\nallowing for static typing and ease of use navigating between\nrelated datatypes.\n\n\nConceptual Mapping:\n\n- Competition: The collection of all Dataset in the competition\n- Dataset:     An array of all Tasks in the competition\n- Task:        The entire contents of a json file, outputs 1-3 lines of CSV\n- ProblemSet:  An array of either test or training Problems\n- Problem:     An input + output Grid pair\n- Grid:        An individual grid represented as a numpy array\n- CSV:         Export data model to `submission.csv`\n\n\n# Solver Abstract\n- Source: [src\/solver_abstract](https:\/\/github.com\/JamesMcGuigan\/kaggle-arc\/tree\/master\/src\/solver_abstract)\n\nProof of concept: using `inspect.signiture()` figure out all possible permutations of `f(g(h(x)))`\nand implement an IoC dependency injection solver.\n\n\n# Solver MultiModel\n- Original Notebook: https:\/\/www.kaggle.com\/jamesmcguigan\/arc-geometry-solvers\/\n- Source: [src\/solver_multimodel\/](https:\/\/github.com\/JamesMcGuigan\/kaggle-arc\/tree\/master\/src\/solver_multimodel\/)\n\nThis is the main codebase.\n\n\n## Core\n**Solver** implements an object oriented base to handle common code for looping, testing and generating\nsolutions in the dataset, allowing subclasses to override lifecycle methods such as\n`detect()`, `fit()`, `solve_grid()`\n\n**ProblemSetSolver** is a Solver subclass designed for algorithms requiring data access\nto the Task rather than just the current input Grid.\n\n**ProblemSetEncoder** is a baseclass for autogenerating a feature map from a list of typed univariate functions\n\n- [src\/solver_multimodel\/core\/Solver.py](https:\/\/github.com\/JamesMcGuigan\/kaggle-arc\/tree\/master\/src\/solver_multimodel\/core\/Solver.py)\n- [src\/solver_multimodel\/core\/ProblemSetEncoder.py](https:\/\/github.com\/JamesMcGuigan\/kaggle-arc\/tree\/master\/src\/solver_multimodel\/core\/ProblemSetEncoder.py)\n- [src\/solver_multimodel\/core\/ProblemSetSolver.py](https:\/\/github.com\/JamesMcGuigan\/kaggle-arc\/tree\/master\/src\/solver_multimodel\/core\/ProblemSetSolver.py)\n- [src\/solver_multimodel\/core\/XGBEncoder.py](https:\/\/github.com\/JamesMcGuigan\/kaggle-arc\/tree\/master\/src\/solver_multimodel\/core\/XGBEncoder.py)\n\n\n## Solvers\n\nIn order of complexity:\n\n**DoNothingSolver** just returns the input grid\n- [src\/solver_multimodel\/solvers\/DoNothingSolver.py](https:\/\/github.com\/JamesMcGuigan\/kaggle-arc\/tree\/master\/src\/solver_multimodel\/solvers\/DoNothingSolver.py)\n\n**GlobSolver** indexes the training dataset and returns verbatim any problems seen in the training dataset\n- [src\/solver_multimodel\/solvers\/GlobSolver.py](https:\/\/github.com\/JamesMcGuigan\/kaggle-arc\/tree\/master\/src\/solver_multimodel\/solvers\/GlobSolver.py)\n\n**ZoomSolver** applies `cv2.resize()` and `skimage.measure.block_reduce()` to problems whose input\/output grid sizes are\nan integer multiple of each other\n- [src\/solver_multimodel\/solvers\/ZoomSolver.py](https:\/\/github.com\/JamesMcGuigan\/kaggle-arc\/tree\/master\/src\/solver_multimodel\/solvers\/ZoomSolver.py)\n\n**SingleColorSolver** + **BorderSolver** tests a list of functions to answer single color problems\n- [src\/solver_multimodel\/solvers\/SingleColorSolver.py](https:\/\/github.com\/JamesMcGuigan\/kaggle-arc\/tree\/master\/src\/solver_multimodel\/solvers\/SingleColorSolver.py)\n- [src\/solver_multimodel\/solvers\/BorderSolver.py](https:\/\/github.com\/JamesMcGuigan\/kaggle-arc\/tree\/master\/src\/solver_multimodel\/solvers\/BorderSolver.py)\n- [notebooks_gallery\/solved\/SingleColorSolver.ipynb](https:\/\/github.com\/JamesMcGuigan\/kaggle-arc\/tree\/master\/notebooks_gallery\/solved\/SingleColorSolver.ipynb)\n\n**GeometrySolver** performs a brute force search of numpy array functions\n- [src\/solver_multimodel\/solvers\/GeometrySolver.py](https:\/\/github.com\/JamesMcGuigan\/kaggle-arc\/tree\/master\/src\/solver_multimodel\/solvers\/GeometrySolver.py)\n- [notebooks_gallery\/solved\/GeometrySolver.ipynb](https:\/\/github.com\/JamesMcGuigan\/kaggle-arc\/tree\/master\/notebooks_gallery\/solved\/GeometrySolver.ipynb)\n\n**TessellationSolver** applies nested geometry solutions to tessellation problems\n- [src\/solver_multimodel\/solvers\/TessellationSolver.py](https:\/\/github.com\/JamesMcGuigan\/kaggle-arc\/tree\/master\/src\/solver_multimodel\/solvers\/TessellationSolver.py)\n- [notebooks_gallery\/solved\/TessellationSolver.ipynb](https:\/\/github.com\/JamesMcGuigan\/kaggle-arc\/tree\/master\/notebooks_gallery\/solved\/TessellationSolver.ipynb)\n\n**XGBGridSolver** generates a large multi-dimential featuremap to be solved by XGBoost.\nThe featuremap includes each pixel's \"view\" of neighbouring pixels. This was able to autosolve a\nsuprising number of problem cases, but also produces a large number of incorrect or close guesess that\nmanaged to test correctly against the train side the task.\n\nHyperopt Bayesian Hyperparameter Optimization was also performed on XGBoost.\n- [src\/solver_multimodel\/solvers\/XGBGridSolver.py](https:\/\/github.com\/JamesMcGuigan\/kaggle-arc\/tree\/master\/src\/solver_multimodel\/solvers\/XGBGridSolver.py)\n- [src\/solver_multimodel\/solvers\/XGBGridSolver.hyperopt.py](https:\/\/github.com\/JamesMcGuigan\/kaggle-arc\/tree\/master\/src\/solver_multimodel\/solvers\/XGBGridSolver.hyperopt.py)\n- [notebooks_gallery\/solved\/XGBGridSolver.ipynb](https:\/\/github.com\/JamesMcGuigan\/kaggle-arc\/tree\/master\/notebooks_gallery\/solved\/XGBGridSolver.ipynb)\n\n**XGBSingleColorSolver** solve simple problems using XGBoost in a subclassable way\n- [src\/solver_multimodel\/solvers\/XGBSingleColorEncoder.py](https:\/\/github.com\/JamesMcGuigan\/kaggle-arc\/tree\/master\/src\/solver_multimodel\/solvers\/XGBSingleColorEncoder.py)\n- [src\/solver_multimodel\/solvers\/XGBSingleColorSolver.py](https:\/\/github.com\/JamesMcGuigan\/kaggle-arc\/tree\/master\/src\/solver_multimodel\/solvers\/XGBSingleColorSolver.py)\n\n\n# Utils\n- Source: [src\/utils\/](https:\/\/github.com\/JamesMcGuigan\/kaggle-arc\/tree\/master\/src\/utils\/)\n\nVarious utility functions including `plot_task()` and `@np_cache()`\n\n\n# Functions\n- Source: [src\/functions\/](https:\/\/github.com\/JamesMcGuigan\/kaggle-arc\/tree\/master\/src\/functions\/)\n\nA range of different numpy.array queries and transformations\n\n\n# Kaggle Compile\n- Source: [submission\/kaggle_compile.py](submission\/kaggle_compile.py)\n\nKaggle Compile is a custom python concatenater that resolves local import statements and\nallows an IDE multi-file codebase to be compiled into a single-file Kaggle Kernel Script\n\n\n\n# \"Mathematicians + Experts\" Team\nI would like to say thank you to the _\"Mathematicians + Experts\"_ team who gave me new perspectives on how to approach Kaggle Competitions as well as introducing me to new ideas. \n- https:\/\/www.kaggle.com\/jesucristo\n- https:\/\/www.kaggle.com\/seshurajup\n- https:\/\/www.kaggle.com\/chihantsai\n- https:\/\/www.kaggle.com\/gamingconceited\n- https:\/\/www.kaggle.com\/jamesmcguigan"}}