{"cell_type":{"847b9a18":"code","bb4be735":"code","6eb949f5":"code","959f7b6c":"code","6443b4a8":"code","d0913472":"code","bbfd080c":"code","1b93e4ed":"code","73f41882":"code","cb2c0038":"code","b48c0a8d":"code","26126e9e":"code","6a083c58":"code","63fe2d18":"markdown","a429ed18":"markdown","206284f7":"markdown","16d31db6":"markdown","d37e8dc6":"markdown"},"source":{"847b9a18":"!git clone https:\/\/github.com\/ayulockin\/Explore-NFNet","bb4be735":"import tensorflow as tf\nprint(tf.__version__)\n\nimport tensorflow_datasets as tfds\n\nimport sys\nsys.path.append(\"Explore-NFNet\")\nimport os\nimport gc\nimport cv2\nimport numpy as np\nfrom functools import partial\nimport matplotlib.pyplot as plt\n\n# Imports from the cloned repository\nimport agc\nfrom models.resnet import resnet_v1 \nfrom models.mini_vgg import get_mini_vgg\n\n# Augmentation related imports\nimport albumentations as A\n\n# Seed everything for reproducibility\ndef seed_everything():\n    # Set the random seeds\n    os.environ['TF_CUDNN_DETERMINISTIC'] = '1' \n    np.random.seed(hash(\"improves reproducibility\") % 2**32 - 1)\n    tf.random.set_seed(hash(\"by removing stochasticity\") % 2**32 - 1)\n\nseed_everything()\n\n# Avoid TensorFlow to allocate all the GPU at once. \n# Ref: https:\/\/www.tensorflow.org\/guide\/gpu\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n  try:\n    # Currently, memory growth needs to be the same across GPUs\n    for gpu in gpus:\n      tf.config.experimental.set_memory_growth(gpu, True)\n    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n  except RuntimeError as e:\n    # Memory growth must be set before GPUs have been initialized\n    print(e)","6eb949f5":"import wandb\nfrom wandb.keras import WandbCallback\n\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nwandb_api = user_secrets.get_secret(\"wandb_api\")\n\nwandb.login(key=wandb_api)","959f7b6c":"DATASET_NAME = 'cifar10'\nIMG_HEIGHT = 32\nIMG_WIDTH = 32\nNUM_CLASSES = 10\nSHUFFLE_BUFFER = 1024\nEPOCHS = 100\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE","6443b4a8":"(train_ds, val_ds, test_ds), info = tfds.load(name=DATASET_NAME, \n                          split=[\"train[:85%]\", \"train[85%:]\", \"test\"], \n                          with_info=True,\n                          as_supervised=True)","d0913472":"@tf.function\ndef preprocess(image, label):\n    # preprocess image\n    image = tf.cast(image, tf.float32)\n    image = image\/255.0\n\n    return image, label\n\n# Define the augmentation policies. Note that they are applied sequentially with some probability p.\ntransforms = A.Compose([\n                A.HorizontalFlip(p=0.7),\n                A.Rotate(limit=30, p=0.7)\n        ])\n\n# Apply augmentation policies.\ndef aug_fn(image):\n    data = {\"image\":image}\n    aug_data = transforms(**data)\n    aug_img = aug_data[\"image\"]\n\n    return aug_img\n\n@tf.function\ndef apply_augmentation(image, label):\n    aug_img = tf.numpy_function(func=aug_fn, inp=[image], Tout=tf.float32)\n    aug_img.set_shape((IMG_HEIGHT, IMG_WIDTH, 3))\n    \n    return aug_img, label\n\ntrain_ds = (\n    train_ds\n    .shuffle(SHUFFLE_BUFFER)\n    .map(preprocess, num_parallel_calls=AUTOTUNE)\n    .map(apply_augmentation, num_parallel_calls=AUTOTUNE)\n    .batch(32)\n    .prefetch(AUTOTUNE)\n)","bbfd080c":"def show_batch(image_batch, label_batch):\n  plt.figure(figsize=(10,10))\n  for n in range(25):\n      ax = plt.subplot(5,5,n+1)\n      plt.imshow(image_batch[n])\n      # plt.title(f'{np.argmax(label_batch[n].numpy())}')\n      plt.title(f'{label_batch[n].numpy()}')\n      plt.axis('off')\n  \nimage_batch, label_batch = next(iter(train_ds))\nshow_batch(image_batch, label_batch)\n\nprint(image_batch.shape, label_batch.shape)\n\ndel train_ds\ngc.collect()","1b93e4ed":"class ResNetModel(tf.keras.Model):\n    def __init__(self, resnet):\n        super(ResNetModel, self).__init__()\n        self.resnet = resnet\n    \n    def train_step(self, data):\n        images, labels = data\n\n        with tf.GradientTape() as tape:\n            predictions = self.resnet(images)\n            loss = self.compiled_loss(labels, predictions)\n\n        trainable_params = self.resnet.trainable_variables\n        gradients = tape.gradient(loss, trainable_params)\n\n        self.optimizer.apply_gradients(zip(gradients, trainable_params))\n\n        self.compiled_metrics.update_state(labels, predictions)\n        return {m.name: m.result() for m in self.metrics}\n\n    def test_step(self, data):\n        images, labels = data\n        predictions = self.resnet(images, training=False)\n        loss = self.compiled_loss(labels, predictions)\n        self.compiled_metrics.update_state(labels, predictions)\n        return {m.name: m.result() for m in self.metrics}\n\n    def save_weights(self, filepath):\n        self.resnet.save_weights(filepath=filepath, save_format=\"tf\")\n\n    def call(self, inputs, *args, **kwargs):\n        return self.resnet(inputs)\n\ntf.keras.backend.clear_session()\ntest_model = ResNetModel(resnet_v1((IMG_HEIGHT, IMG_WIDTH, 3), 20, num_classes=NUM_CLASSES, use_bn=True))\ntest_model.build((1, IMG_HEIGHT, IMG_WIDTH, 3))\ntest_model.summary()\nprint(f\"Total learnable parameters: {test_model.count_params()\/1e6} M\")\n\ndel test_model\ngc.collect()","73f41882":"earlystopper = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss', patience=10, verbose=0, mode='auto',\n    restore_best_weights=True\n)\n\nreducelronplateau = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor=\"val_loss\", factor=0.2,\n    patience=3, verbose=1\n)","cb2c0038":"def train():\n    # Initialize W&B run\n    run = wandb.init(job_type='ablation-bn-bs')\n    print(\"config:\", dict(run.config))\n    \n    # Prepare data loader\n    (train_ds, val_ds, test_ds), info = tfds.load(name=DATASET_NAME, \n                              split=[\"train[:85%]\", \"train[85%:]\", \"test\"], \n                              with_info=True,\n                              as_supervised=True)\n    trainloader = (\n        train_ds\n        .shuffle(SHUFFLE_BUFFER)\n        .map(preprocess, num_parallel_calls=AUTOTUNE)\n        .map(apply_augmentation, num_parallel_calls=AUTOTUNE)\n        .batch(run.config.batch_size)\n        .prefetch(AUTOTUNE)\n    )\n    \n    valloader = (\n        val_ds\n        .map(preprocess, num_parallel_calls=AUTOTUNE)\n        .batch(256)\n        .prefetch(AUTOTUNE)\n    )\n\n    testloader = (\n        test_ds\n        .map(preprocess, num_parallel_calls=AUTOTUNE)\n        .batch(256)\n        .prefetch(AUTOTUNE)\n    )\n  \n    # Iniialize model\n    tf.keras.backend.clear_session()\n    resnet_model = resnet_v1((IMG_HEIGHT, IMG_WIDTH, 3), 20, num_classes=NUM_CLASSES, use_bn=True)\n    model = ResNetModel(resnet_model)\n    \n    # Compile the model\n    model.compile('adam', 'sparse_categorical_crossentropy', metrics=['acc'])\n        \n    # Train model\n    _ = model.fit(trainloader,\n                  epochs=EPOCHS,\n                  validation_data=valloader,\n                  callbacks=[WandbCallback(),\n                             reducelronplateau,\n                             earlystopper])  \n\n    # Evaluate model on test set\n    loss, acc = model.evaluate(testloader)\n    wandb.log({'Test Accuracy': round(acc, 3)})\n    \n    del model\n    del train_ds\n    del val_ds\n    del test_ds\n    gc.collect()","b48c0a8d":"sweep_config = {\n  'method': 'grid',\n  'parameters': {\n      'batch_size': {\n          'values': [64, 128, 256, 512, 1024, 2048]\n      }\n  }\n}","26126e9e":"sweep_id = wandb.sweep(sweep_config, entity='ayush-thakur', project='nfnet')","6a083c58":"wandb.agent(sweep_id, function=train)","63fe2d18":"# \u26c4 Download and Prepare Dataset","a429ed18":"# \ud83d\udcf2 Callbacks","206284f7":"# \ud83e\uddf0 Setups, Installations and Imports","16d31db6":"# \ud83d\udc24 Model","d37e8dc6":"# \ud83d\ude8b Albation Study Using W&B Sweep\n"}}