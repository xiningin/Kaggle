{"cell_type":{"87d934f5":"code","4fae0f47":"code","bc3ac4f5":"code","767342a2":"code","4dd148f0":"markdown"},"source":{"87d934f5":"import os\nimport sys\nimport random\nimport warnings\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm_notebook, tnrange, tqdm\nfrom skimage.io import imread, imshow, concatenate_images\nfrom skimage.transform import resize\nfrom skimage.morphology import label\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.image import array_to_img, img_to_array, load_img\nfrom skimage.feature import canny\nfrom skimage.filters import sobel,threshold_otsu, threshold_niblack,threshold_sauvola\nfrom skimage.segmentation import felzenszwalb, slic, quickshift, watershed\nfrom skimage.segmentation import mark_boundaries\nfrom scipy import signal\nimport os\nimport numpy as np\nimport imageio\nimport pandas as pd\nimport torch\nfrom torch.utils import data\n%matplotlib inline\nimport cv2\nfrom PIL import Image\nimport pdb\nimport seaborn as sns\nfrom glob import glob\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nprint(\"Packages Loaded Successfully\")","4fae0f47":"INPUT_PATH = '..\/input'\nDATA_PATH = INPUT_PATH\nTRAIN_DATA = os.path.join(DATA_PATH, \"train\/images\")\nTRAIN_MASKS_DATA = os.path.join(DATA_PATH, \"train\/masks\")\nTEST_DATA = os.path.join(DATA_PATH, \"test\")\ndf = pd.read_csv(DATA_PATH+'\/depths.csv')\npath_train = '..\/input\/train\/'\npath_test = '..\/input\/test\/'\ntrain_ids = next(os.walk(path_train+\"images\"))[2]\ntest_ids = next(os.walk(path_test+\"images\"))[2]","bc3ac4f5":"def get_file_name(image_id, image_type):\n    check_dir = False\n    if \"Train\" == image_type:\n        data_path = TRAIN_DATA\n    elif \"mask\" in image_type:\n        data_path = TRAIN_MASKS_DATA\n    elif \"Test\" in image_type:\n        data_path = TEST_DATA\n    else:\n        raise Exception(\"Image type '%s' is not recognized\" % image_type)\n\n    if check_dir and not os.path.exists(data_path):\n        os.makedirs(data_path)\n\n    return os.path.join(data_path, \"{}\".format(image_id))\n\ndef get_image_data(image_id, image_type, **kwargs):\n    img = _get_image_data_opencv(image_id, image_type, **kwargs)\n    img = img.astype('uint8')\n    return img\n\ndef _get_image_data_opencv(image_id, image_type, **kwargs):\n    fname = get_file_name(image_id, image_type)\n    img = cv2.imread(fname)\n    assert img is not None, \"Failed to read image : %s, %s\" % (image_id, image_type)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    return img","767342a2":"nImg = 32  #no. of images that you want to display\nnp.random.seed(42)\n_train_ids = list(train_ids)\nnp.random.shuffle(_train_ids)\n_train_ids = _train_ids[:nImg]\ntile_size = (256, 256)\nn = 8\nalpha = 0.25\n\nm = int(np.ceil(len(_train_ids) * 1.0 \/ n))\ncomplete_image = np.zeros((m*(tile_size[0]+2), n*(tile_size[1]+2), 3), dtype=np.uint8)\ncomplete_image_masked = np.zeros((m*(tile_size[0]+2), n*(tile_size[1]+2), 3), dtype=np.uint8)\n\ncounter = 0\nfor i in range(m):\n    ys = i*(tile_size[1] + 2)\n    ye = ys + tile_size[1]\n    for j in range(n):\n        xs = j*(tile_size[0] + 2)\n        xe = xs + tile_size[0]\n        if counter == len(_train_ids):\n            break\n        image_id = _train_ids[counter]; counter+=1\n        img = get_image_data(image_id, 'Train')\n        \n        mask = get_image_data(image_id, \"Train_mask\")\n        img_masked =  cv2.addWeighted(img, alpha, mask, 1 - alpha,0)\n#         img_masked = cv2.bitwise_and(img, img, mask=mask)\n\n        img = cv2.resize(img, dsize=tile_size)\n        img_masked = cv2.resize(img_masked, dsize=tile_size)\n        \n        img = cv2.putText(img, image_id, (5,img.shape[0] - 5), cv2.FONT_HERSHEY_PLAIN, 1.5, (0, 255, 0), thickness=2)\n        complete_image[ys:ye, xs:xe, :] = img[:,:,:]\n        \n        img_masked = cv2.putText(img_masked, image_id, (5,img.shape[0] - 5), cv2.FONT_HERSHEY_PLAIN, 1.5, (0, 255, 0), thickness=2)\n        complete_image_masked[ys:ye, xs:xe, :] = img_masked[:,:,:]\n        \n    if counter == len(_train_ids):\n        break    \n        \nm = complete_image.shape[0] \/ (tile_size[0] + 2)\nk = 8\nn = int(np.ceil(m \/ k))\nfor i in range(n):\n    plt.figure(figsize=(20, 20))\n    ys = i*(tile_size[0] + 2)*k\n    ye = min((i+1)*(tile_size[0] + 2)*k, complete_image.shape[0])\n    plt.imshow(complete_image[ys:ye,:,:])\n    plt.title(\"Training dataset\")\n    \nm = complete_image.shape[0] \/ (tile_size[0] + 2)\nk = 8\nn = int(np.ceil(m \/ k))\nfor i in range(n):\n    plt.figure(figsize=(20, 20))\n    ys = i*(tile_size[0] + 2)*k\n    ye = min((i+1)*(tile_size[0] + 2)*k, complete_image.shape[0])\n    plt.imshow(complete_image_masked[ys:ye,:,:])\n    plt.title(\"Training dataset: Lighter Color depicts salt\")","4dd148f0":"**Background Data**\n* Where there is salt, there is oil.\n* But where is the salt?\n* Classifying seismic imaging currently requires human (salt\/not salt)\n* Can we use an algorithm to do this instead? (yes)\n* Seismic data is like an ultra-sound of the subsurface\n* It uses wavelengths around 1m to 100m\n* The Society of Exploration Geophysicists has 10K publications using the keyword 'salt'\n* We can think of Earth as layered.\n* Sand gets deposited on top of existing sand. And in comes the salt.\n* There is an entire research branch dedicated to salt tectonics, that is the movement of salt in the subsurface.\n**Our steps**\n* Install Dependencies\n* View the labeled seismic imaging data\n* Plot the depth distribution in the training data\n* Plot the proportion of salt vs depth in the training data\n* Build a U-Net algorithm to learn the mapping between seismic images and the salt filter mask"}}