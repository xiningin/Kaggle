{"cell_type":{"1c01da71":"code","6144a2cd":"code","b282a446":"code","9c6ad0eb":"code","3635b850":"code","e273d19a":"code","274a8a64":"code","a0074b1d":"code","80e75086":"code","28babfae":"code","3d0de73b":"code","9f432ec0":"code","94c29c54":"code","926ba7ca":"code","9eaf51fe":"code","8ac57a38":"code","75c44eba":"code","7e546836":"code","4ddf3e05":"markdown","75941951":"markdown","9c54e46b":"markdown","9e5dfb7b":"markdown","4c7e9001":"markdown","95b3218c":"markdown","95e2c084":"markdown","59f3e8b2":"markdown","f38824ac":"markdown","3111ad5f":"markdown","a0deaed5":"markdown"},"source":{"1c01da71":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6144a2cd":"df = pd.read_csv(\"\/kaggle\/input\/heart-disease-uci\/heart.csv\")","b282a446":"df.info()","9c6ad0eb":"df.head()","3635b850":"y = df.target.values\nx_data = df.drop([\"target\"],axis =  1)","e273d19a":"x = (x_data - np.min(x_data))\/(np.max(x_data)-np.min(x_data)).values","274a8a64":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.2,random_state=42)\n# by taking transpose of data we switch the location of rows and colums\nx_train = x_train.T\nx_test = x_test.T\ny_train = y_train.T\ny_test = y_test.T\n\nprint(\"x_train: \",x_train.shape)\nprint(\"x_test: \",x_test.shape)\nprint(\"y_train: \",y_train.shape)\nprint(\"y_test: \",y_test.shape)","a0074b1d":"def initialize_weight_and_bias(dimension):\n   \n    w = np.full((dimension,1),0.01)\n    b = 0.0\n    return w,b\n\n# w,b = initialize_weight_and_bias(30)\n\ndef sigmoid(z):\n    y_head = 1\/(1+np.exp(-z))\n    return y_head","80e75086":"def forward_backward_propagation(w,b,x_train,y_train):\n    # forward propagation\n    z = np.dot(w.T,x_train) + b\n    y_head = sigmoid(z)\n    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n    cost = (np.sum(loss))\/x_train.shape[1]      # x_train.shape[1]  is for scaling\n    \n    # backward propagation\n    derivative_weight = (np.dot(x_train,((y_head-y_train).T)))\/x_train.shape[1] # x_train.shape[1]  is for scaling\n    derivative_bias = np.sum(y_head-y_train)\/x_train.shape[1]                 # x_train.shape[1]  is for scaling\n    gradients = {\"derivative_weight\": derivative_weight, \"derivative_bias\": derivative_bias}\n    \n    return cost,gradients","28babfae":"def update(w, b, x_train, y_train, learning_rate,number_of_iterarion):\n    cost_list = []\n    cost_list2 = []\n    index = []\n    \n    # updating(learning) parameters is number_of_iterarion times\n    for i in range(number_of_iterarion):\n        # make forward and backward propagation and find cost and gradients\n        cost,gradients = forward_backward_propagation(w,b,x_train,y_train)\n        cost_list.append(cost)\n        # lets update\n        w = w - learning_rate * gradients[\"derivative_weight\"]\n        b = b - learning_rate * gradients[\"derivative_bias\"]\n        if i % 10 == 0:\n            cost_list2.append(cost)\n            index.append(i)\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n            \n    # we update(learn) parameters weights and bias\n    parameters = {\"weight\": w,\"bias\": b}\n    plt.plot(index,cost_list2)\n    plt.xticks(index,rotation='vertical')\n    plt.xlabel(\"Number of Iterarion\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters, gradients, cost_list","3d0de73b":"def predict(w,b,x_test):\n    # x_test is a input for forward propagation\n    z = sigmoid(np.dot(w.T,x_test)+b)\n    Y_prediction = np.zeros((1,x_test.shape[1]))\n    # if z is bigger than 0.5, our prediction is sign one (y_head=1),\n    # if z is smaller than 0.5, our prediction is sign zero (y_head=0),\n    for i in range(z.shape[1]):\n        if z[0,i]<= 0.5:\n            Y_prediction[0,i] = 0\n        else:\n            Y_prediction[0,i] = 1\n\n    return Y_prediction","9f432ec0":"def logistic_regression(x_train, y_train, x_test, y_test, learning_rate ,  num_iterations):\n    # initialize\n    dimension =  x_train.shape[0]  # that is 30\n    w,b = initialize_weight_and_bias(dimension)\n    # do not change learning rate\n    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate,num_iterations)\n    \n    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n\n    # Print test Errors\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n    \nlogistic_regression(x_train, y_train, x_test, y_test,learning_rate = 1, num_iterations = 15)","94c29c54":"logistic_regression(x_train, y_train, x_test, y_test,learning_rate = 2, num_iterations = 30)","926ba7ca":"logistic_regression(x_train, y_train, x_test, y_test,learning_rate = 3, num_iterations = 50)","9eaf51fe":"logistic_regression(x_train, y_train, x_test, y_test,learning_rate = 3, num_iterations = 100)","8ac57a38":"logistic_regression(x_train, y_train, x_test, y_test,learning_rate = 1, num_iterations = 300)","75c44eba":"logistic_regression(x_train, y_train, x_test, y_test,learning_rate = 3, num_iterations = 300)","7e546836":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(x_train.T,y_train.T)\nprint(\"test accuracy {}\".format(lr.score(x_test.T,y_test.T)))","4ddf3e05":"<a id=\"7\"><\/a> <br>\n# Prediction","75941951":"<a id=\"2\"><\/a> <br>\n# Normalization","9c54e46b":"<a id=\"9\"><\/a> <br>\n# Linear Regression with Sklearn","9e5dfb7b":"<a id=\"4\"><\/a> <br>\n# Parameter Initializing and Sigmoid Function","4c7e9001":"<a id=\"5\"><\/a> <br>\n# Forward and Backward Propogation","95b3218c":"<a id=\"1\"><\/a> <br>\n# Reading Data","95e2c084":"<a id=\"3\"><\/a> <br>\n# Train Test Split","59f3e8b2":"# Data Description\n**age**: The person's age in years\n\n**sex**: The person's sex (1 = male, 0 = female)\n\n**cp**: The chest pain experienced (Value 1: typical angina, Value 2: atypical angina, Value 3: non-anginal pain, Value 4: asymptomatic)\n\n**trestbps**: The person's resting blood pressure (mm Hg on admission to the hospital)\n\n**chol**: The person's cholesterol measurement in mg\/dl\n\n**fbs**: The person's fasting blood sugar (> 120 mg\/dl, 1 = true; 0 = false)\n\n**restecg**: Resting electrocardiographic measurement (0 = normal, 1 = having ST-T wave abnormality, 2 = showing probable or definite left ventricular hypertrophy by Estes' criteria)\n\n**thalach**: The person's maximum heart rate achieved\n\n**exang**: Exercise induced angina (1 = yes; 0 = no)\n\n**oldpeak**: ST depression induced by exercise relative to rest ('ST' relates to positions on the ECG plot. See more here)\n\n**slope**: the slope of the peak exercise ST segment (Value 1: upsloping, Value 2: flat, Value 3: downsloping)\n\n**ca**: The number of major vessels (0-3)\n\n**thal**: A blood disorder called thalassemia (3 = normal; 6 = fixed defect; 7 = reversable defect)\n\n**target**: Heart disease (0 = no, 1 = yes)","f38824ac":"<a id=\"8\"><\/a> <br>\n# Logistic Regression","3111ad5f":"<a id=\"6\"><\/a> <br>\n# Updating Parameters","a0deaed5":"# INTRODUCTION\n* [Reading Data](#1)\n* [Normalization](#2)\n* [Train Test Split](#3)\n* [Parameter Initializing and Sigmoid Function](#4)\n* [Forward and Backward Propogation](#5)\n* [Updating Parameters](#6)\n* [Prediction](#7)\n* [Logistic Regression](#8)\n* [Logistic Regression with Sklearn](#9)"}}