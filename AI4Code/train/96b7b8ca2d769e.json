{"cell_type":{"f64b604a":"code","7b154bd8":"code","9542ec63":"code","ea831446":"code","d223c316":"code","38f22ae0":"code","f60760c5":"code","86d2de19":"code","409ee559":"code","f1bcbdc4":"code","80e20ac6":"code","dc467000":"code","025c10b7":"code","bc8fb5d4":"code","bdc0dcc3":"code","4378e59b":"code","64995063":"code","2c23cd68":"code","11c01df0":"code","5ed5760f":"code","c7a7c107":"code","fe56780a":"code","560df604":"code","cf437908":"code","ecfbc9e4":"code","841c604b":"code","c957352b":"code","7aeef92c":"code","f2093fab":"code","0bf781f7":"code","da8efd07":"code","8a1599b4":"code","e4d6973d":"code","9f0db6d1":"code","ffd569df":"code","a399aa99":"code","5933e8a2":"code","98be68a1":"code","4b6b5c88":"code","178fc339":"code","0ab16154":"code","47b6c263":"code","ebd82f3f":"code","d76b2c62":"code","58c47ce3":"code","446f69a3":"code","b9fe0897":"code","972af69c":"code","6cb46d44":"code","5392da75":"code","f848be59":"code","9869b0ac":"code","31c07cc4":"code","02ad6e84":"code","3a7977b1":"code","654234ef":"code","70a35050":"code","6950ac6f":"code","27108c9e":"code","bbaf8a49":"code","cb9ddf56":"code","75dc95cc":"code","ed0ddf64":"code","9b19f976":"markdown","267949e9":"markdown","c88ced73":"markdown","6391c87c":"markdown","977f20f3":"markdown","b3d68471":"markdown","02106466":"markdown","4b5b8c73":"markdown","8cb1998f":"markdown","3d5a7d54":"markdown","d05a2887":"markdown","a0300edb":"markdown","9313538a":"markdown","4e669763":"markdown","fe14a29e":"markdown","08a2398a":"markdown","8252ccee":"markdown","5853b283":"markdown","29360c19":"markdown","d54b08ec":"markdown","7613e623":"markdown","52c7ab4b":"markdown","985d9aa5":"markdown","d2fcd5a1":"markdown","c748fcc0":"markdown","558d3f4c":"markdown","2bedeaa0":"markdown","8ee009b4":"markdown","9836290b":"markdown","6d28e551":"markdown","6a3f96a8":"markdown","8fe8f3c4":"markdown","ec4cf57f":"markdown","f5af0336":"markdown","ecc38838":"markdown","57a7a30f":"markdown","2dbe8d69":"markdown","f5c7c568":"markdown","193316e6":"markdown","784515c2":"markdown","4e52ecb1":"markdown","d778b8e6":"markdown"},"source":{"f64b604a":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport re\nimport random\n\nfrom textblob import TextBlob\nfrom stop_words import get_stop_words\nfrom nltk.corpus import stopwords\nfrom nltk.stem.snowball import SnowballStemmer\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.metrics import mean_squared_error\n\nfrom wordcloud import WordCloud, STOPWORDS\n\nimport lightgbm as ltb\n\n\nplt.style.use('ggplot')\n\npd.set_option('display.max_columns', None)\npd.set_option('max_colwidth', None)\npd.set_option('max_rows', None)","7b154bd8":"df = pd.read_csv(\"..\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv\")","9542ec63":"df.shape","ea831446":"df.info()","d223c316":"def clean_text(text):\n    text = re.sub(r'<[^<]+?>', '', text)\n    text = text.replace('\\n', ' ')\n    text = re.sub(r'\\s+', ' ', text)\n    text = re.sub(r'<[^<]+?>', '', text) \n    text = text.replace('(\\xa0)', ' ')\n    text = text.replace('(&lt)', '')\n    text = text.replace('(&gt)', '')\n    text = text.replace(\"\\\\\", \"\")\n    \n    return text","38f22ae0":"df['comment_text'] = df['comment_text'].apply(clean_text)","f60760c5":"print(f'{df[df.toxic+df.severe_toxic+df.obscene+df.threat+df.insult+df.identity_hate > 1].shape[0]} comments contain more than one type of toxicity')","86d2de19":"nontoxic_comments = df[df.toxic+df.severe_toxic+df.obscene+df.threat+df.insult+df.identity_hate == 0]\nprint(f'Out of {df.shape[0]} comments, {nontoxic_comments.shape[0]} are non-toxic and {df.shape[0]-nontoxic_comments.shape[0]} are toxic. \\n'\n      f'So we have a percentage of toxic comments of {round((df.shape[0]-nontoxic_comments.shape[0])\/df.shape[0]*100,3)}%')","409ee559":"print(f'There are: \\n'\n      f'- {df[df.toxic == 1].shape[0]} comments classified as toxic \\n'\n      f'- {df[df.severe_toxic == 1].shape[0]} comments classified as severe toxic \\n'\n      f'- {df[df.obscene == 1].shape[0]} comments classified as obscene \\n'\n      f'- {df[df.threat == 1].shape[0]} comments classified as threat \\n'\n      f'- {df[df.insult == 1].shape[0]} comments classified as insult \\n'\n      f'- {df[df.identity_hate == 1].shape[0]} comments classified as identity hate \\n'\n      f'Comments classified as toxic and as severely toxic are {df[df.toxic == 1].shape[0]+df[df.severe_toxic == 1].shape[0]}. \\n'\n      f'There are {df[(df.obscene+df.threat+df.insult+df.identity_hate > 0)&(df.toxic+df.severe_toxic==0)].shape[0]} comments that belong to some class of toxicity but have not been assigned to either the toxic or severe toxic class \\n'\n      f'There are {df[(df.severe_toxic==1)&(df.toxic==1)].shape[0]} comments that belong both to the toxic class and to the severe toxic class, while {df[(df.severe_toxic==1)&(df.toxic==0)].shape[0]} that belong to the severe_toxic class but not to the toxic class \\n')","f1bcbdc4":"df[df.toxic == 1].sample(1, random_state=42)","80e20ac6":"df[df.severe_toxic == 1].sample(1, random_state=42)","dc467000":"df[df.obscene == 1].sample(1, random_state=42)","025c10b7":"df[df.threat == 1].sample(1, random_state=42)","bc8fb5d4":"df[df.insult == 1].sample(1, random_state=42)","bdc0dcc3":"df[df.identity_hate == 1].sample(1, random_state=42)","4378e59b":"def get_top_n_words(corpus, n=None, remove_stop_words=False, n_words=1): # if n_words=1 -> unigrams, if n_words=2 -> bigrams..\n    if remove_stop_words:\n        vec = CountVectorizer(stop_words = 'english', ngram_range=(n_words, n_words)).fit(corpus)\n    else:\n        vec = CountVectorizer(ngram_range=(n_words, n_words)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","64995063":"text_no_toxic = df[df.toxic+df.severe_toxic+df.obscene+df.threat+df.insult+df.identity_hate == 0].comment_text.values\ntext_toxic = df[(df.toxic == 1)&(df.severe_toxic == 0)].comment_text.values\ntext_severe_toxic = df[(df.toxic == 1)&(df.severe_toxic == 1)].comment_text.values\n\n\ncommon_words_non_toxic = get_top_n_words(text_no_toxic, 20, remove_stop_words=True, n_words=1)\ncommon_words_toxic = get_top_n_words(text_toxic, 20, remove_stop_words=True, n_words=1)\ncommon_words_severe_toxic = get_top_n_words(text_severe_toxic, 20, remove_stop_words=True, n_words=1)\n\ndf_tmp_non_toxic = pd.DataFrame(common_words_non_toxic, columns = ['text' , 'count'])\ndf_tmp_toxic = pd.DataFrame(common_words_toxic, columns = ['text' , 'count'])\ndf_tmp_severe_toxic = pd.DataFrame(common_words_severe_toxic, columns = ['text' , 'count'])\n\nfig = plt.figure(figsize=(15,6))\n\nax1 = fig.add_subplot(121)\nax1 = df_tmp_toxic.groupby('text').sum()['count'].sort_values(ascending=False).plot(\n    kind='bar', color = \"#0b8b10\")\nax1.set_title('Toxic Unigram Distribution')\nax1.set_xlabel(\"Unigrams\")\nax1.set_ylabel(\"Frequency\")\n\nax2 = fig.add_subplot(122)\nax2 = df_tmp_severe_toxic.groupby('text').sum()['count'].sort_values(ascending=False).plot(\n   kind='bar', color = \"#0b8b10\")\nax2.set_title('Severe Toxic Unigram Distribution')\nax2.set_xlabel(\"Unigrams\")\nax2.set_ylabel(\"Frequency\")\n\nfig = plt.figure(figsize=(15,6))\n\nax1 = fig.add_subplot(121)\nax1 = df_tmp_non_toxic.groupby('text').sum()['count'].sort_values(ascending=False).plot(\n    kind='bar', color = \"#0b8b10\")\nax1.set_title('Non Toxic Unigram Distribution')\nax1.set_xlabel(\"Unigrams\")\nax1.set_ylabel(\"Frequency\")\n\nplt.show()","2c23cd68":"text_no_toxic = df[df.toxic+df.severe_toxic+df.obscene+df.threat+df.insult+df.identity_hate == 0].comment_text.values\ntext_toxic = df[(df.toxic == 1)&(df.severe_toxic == 0)].comment_text.values\ntext_severe_toxic = df[(df.toxic == 1)&(df.severe_toxic == 1)].comment_text.values\n\n\ncommon_words_non_toxic = get_top_n_words(text_no_toxic, 20, remove_stop_words=True, n_words=2)\ncommon_words_toxic = get_top_n_words(text_toxic, 20, remove_stop_words=True, n_words=2)\ncommon_words_severe_toxic = get_top_n_words(text_severe_toxic, 20, remove_stop_words=True, n_words=2)\n\ndf_tmp_non_toxic = pd.DataFrame(common_words_non_toxic, columns = ['text' , 'count'])\ndf_tmp_toxic = pd.DataFrame(common_words_toxic, columns = ['text' , 'count'])\ndf_tmp_severe_toxic = pd.DataFrame(common_words_severe_toxic, columns = ['text' , 'count'])\n\nfig = plt.figure(figsize=(15,6))\n\nax1 = fig.add_subplot(121)\nax1 = df_tmp_toxic.groupby('text').sum()['count'].sort_values(ascending=False).plot(\n    kind='bar', color = \"#0b8b10\")\nax1.set_title('Toxic Bigram Distribution')\nax1.set_xlabel(\"Bigrams\")\nax1.set_ylabel(\"Frequency\")\n\nax2 = fig.add_subplot(122)\nax2 = df_tmp_severe_toxic.groupby('text').sum()['count'].sort_values(ascending=False).plot(\n   kind='bar', color = \"#0b8b10\")\nax2.set_title('Severe Toxic Bigram Distribution')\nax2.set_xlabel(\"Bigrams\")\nax2.set_ylabel(\"Frequency\")\n\nfig = plt.figure(figsize=(15,6))\n\nax1 = fig.add_subplot(121)\nax1 = df_tmp_non_toxic.groupby('text').sum()['count'].sort_values(ascending=False).plot(\n    kind='bar', color = \"#0b8b10\")\nax1.set_title('Non Toxic Bigram Distribution')\nax1.set_xlabel(\"Bigrams\")\nax1.set_ylabel(\"Frequency\")\n\nplt.show()","11c01df0":"text_no_toxic = df[df.toxic+df.severe_toxic+df.obscene+df.threat+df.insult+df.identity_hate == 0].comment_text.values\ntext_toxic = df[(df.toxic == 1)&(df.severe_toxic == 0)].comment_text.values\ntext_severe_toxic = df[(df.toxic == 1)&(df.severe_toxic == 1)].comment_text.values\n\n\ncommon_words_non_toxic = get_top_n_words(text_no_toxic, 20, remove_stop_words=True, n_words=3)\ncommon_words_toxic = get_top_n_words(text_toxic, 20, remove_stop_words=True, n_words=3)\ncommon_words_severe_toxic = get_top_n_words(text_severe_toxic, 20, remove_stop_words=True, n_words=3)\n\ndf_tmp_non_toxic = pd.DataFrame(common_words_non_toxic, columns = ['text' , 'count'])\ndf_tmp_toxic = pd.DataFrame(common_words_toxic, columns = ['text' , 'count'])\ndf_tmp_severe_toxic = pd.DataFrame(common_words_severe_toxic, columns = ['text' , 'count'])\n\nfig = plt.figure(figsize=(15,6))\n\nax1 = fig.add_subplot(121)\nax1 = df_tmp_toxic.groupby('text').sum()['count'].sort_values(ascending=False).plot(\n    kind='bar', color = \"#0b8b10\")\nax1.set_title('Toxic Trigram Distribution')\nax1.set_xlabel(\"Trigrams\")\nax1.set_ylabel(\"Frequency\")\n\nax2 = fig.add_subplot(122)\nax2 = df_tmp_severe_toxic.groupby('text').sum()['count'].sort_values(ascending=False).plot(\n   kind='bar', color = \"#0b8b10\")\nax2.set_title('Severe Toxic Trigram Distribution')\nax2.set_xlabel(\"Trigrams\")\nax2.set_ylabel(\"Frequency\")\n\nfig = plt.figure(figsize=(15,6))\n\nax1 = fig.add_subplot(121)\nax1 = df_tmp_non_toxic.groupby('text').sum()['count'].sort_values(ascending=False).plot(\n    kind='bar', color = \"#0b8b10\")\nax1.set_title('Non Toxic Trigram Distribution')\nax1.set_xlabel(\"Trigrams\")\nax1.set_ylabel(\"Frequency\")\n\nplt.show()","5ed5760f":"polarity_toxic_not_severe = df[(df.toxic==1)&(df.severe_toxic==0)]['comment_text'].map(lambda text: TextBlob(text).sentiment.polarity)\npolarity_severe_toxic = df[df.severe_toxic==1]['comment_text'].map(lambda text: TextBlob(text).sentiment.polarity)\npolarity_non_toxic = df[(df.toxic+df.severe_toxic+df.obscene+df.threat+df.insult+df.identity_hate == 0)]['comment_text'].map(lambda text: TextBlob(text).sentiment.polarity)","c7a7c107":"fig, axes = plt.subplots(ncols=3, nrows=1, figsize=(25, 6))\nax1, ax2, ax3 = axes.flatten()\n\nax1.hist(polarity_non_toxic, color = \"#0b8b10\", bins=25)\nax1.set_title('Polarity Distribution for Non-Toxic Comments')\nax1.set_xlabel(\"Sentiment\")\nax1.set_ylabel(\"Frequency\")\n\nax2.hist(polarity_toxic_not_severe,  color = \"#0b8b10\", bins=25)\nax2.set_title('Polarity Distribution for Toxic Comments')\nax2.set_xlabel(\"Sentiment\")\nax2.set_ylabel(\"Frequency\")\n\nax3.hist(polarity_severe_toxic, color = \"#0b8b10\", bins=25)\nax3.set_title('Polarity Distribution for Severe Toxic Comments')\nax3.set_xlabel(\"Sentiment\")\nax3.set_ylabel(\"Frequency\")\n\nplt.show()","fe56780a":"def color_func(word, font_size, position, orientation, random_state=None, hsl=[125, 75, 25],\n                    **kwargs):\n    return f\"hsl({hsl[0]}, {random.randint(hsl[1]-10, hsl[1]+10)}%, {random.randint(hsl[2]-10, hsl[1]+10)}%)\"","560df604":"wc_text_no_toxic = WordCloud(background_color=\"#fff\",max_words=1000,stopwords=set(STOPWORDS))\nwc_text_toxic = WordCloud(background_color=\"#fff\",max_words=1000,stopwords=set(STOPWORDS))\nwc_text_severe_toxic = WordCloud(background_color=\"#fff\",max_words=1000,stopwords=set(STOPWORDS))\n\n\nwc_text_no_toxic.generate(\" \".join(text_no_toxic))\nwc_text_toxic.generate(\" \".join(text_toxic))\nwc_text_severe_toxic.generate(\" \".join(text_severe_toxic))\n\n\nfig = plt.figure(figsize=(20,8))\n\nax1 = fig.add_subplot(121)\nax1 = plt.imshow(wc_text_toxic.recolor(color_func=color_func, random_state=42),\n           interpolation=\"bilinear\")\nax1 = plt.title(\"Toxic\", fontsize=20)\n\nax2 = fig.add_subplot(122)\nax2 = plt.imshow(wc_text_severe_toxic.recolor(color_func=color_func, random_state=42),\n           interpolation=\"bilinear\")\nax2 = plt.title(\"Severe Toxic\", fontsize=20)\n\nfig = plt.figure(figsize=(20,8))\n\nax1 = fig.add_subplot(121)\nax1 = plt.imshow(wc_text_no_toxic.recolor(color_func=color_func, random_state=42),\n           interpolation=\"bilinear\")\nax1 = plt.title(\"Non Toxic\", fontsize=20)\n\n\nplt.show()","cf437908":"df['target'] = df.toxic+df.severe_toxic+df.obscene+df.threat+df.insult+df.identity_hate","ecfbc9e4":"df['target'][df.severe_toxic == 1] = 3*df.target","841c604b":"fig = plt.figure(figsize=(10,6))\n\nax1 = df['target'].plot.hist(bins=25, color='#0b8b10')\nax1.set_title('Target Distribution')\nax1.set_ylabel(\"Frequency\")\n\nplt.show()","c957352b":"df0 = df[df.target == 0].sample((round(df[df.target == 0].shape[0]\/4)))","7aeef92c":"df = df[df.target != 0]\ndf = pd.concat([df0,df], axis = 0)","f2093fab":"df.target.value_counts()","0bf781f7":"fig = plt.figure(figsize=(10,6))\n\nax1 = df['target'].plot.hist(bins=25, color='#0b8b10')\nax1.set_title('Target Distribution')\nax1.set_ylabel(\"Frequency\")\n\nplt.show()","da8efd07":"# remove stop words\n\nstop_words = list(get_stop_words('en'))\nnltk_words = list(stopwords.words('english'))\nstop_words.extend(nltk_words)\n\ndf['clean_comment_text_NO_STOPWORDS'] = df['comment_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))","8a1599b4":"df[['comment_text', 'clean_comment_text_NO_STOPWORDS']].head(2)","e4d6973d":"# stemming\nstemmer = SnowballStemmer(\"english\")","9f0db6d1":"df['clean_comment_text_NO_STOPWORDS_stemmed'] = df['clean_comment_text_NO_STOPWORDS'].apply(lambda x: ' '.join([stemmer.stem(word) for word in x.split()]))","ffd569df":"df[['comment_text', 'clean_comment_text_NO_STOPWORDS_stemmed']].head(2)","a399aa99":"X = df['clean_comment_text_NO_STOPWORDS_stemmed']\ny = df['target']","5933e8a2":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","98be68a1":"print(f'Number of comments in train data: {X_train.shape[0]} \\n'\n     f'Number of comments in test data: {X_test.shape[0]}')","4b6b5c88":"vectorizer = TfidfVectorizer(analyzer = 'char_wb', max_df = 0.5, min_df = 3, ngram_range = (3,5), lowercase=False)\nvect_X_train = vectorizer.fit_transform(X_train)\nvect_X_test = vectorizer.transform(X_test)","178fc339":"print(f'Final number of features: {len(vectorizer.get_feature_names())}')","0ab16154":"truncatedSVD = TruncatedSVD(n_components=2000, random_state=42)","47b6c263":"truncatedSVD.fit(vect_X_train)","ebd82f3f":"truncatedSVD.explained_variance_ratio_.sum()","d76b2c62":"X_train_SVD = truncatedSVD.transform(vect_X_train)\nX_test_SVD = truncatedSVD.transform(vect_X_test)","58c47ce3":"# param_grid = {\"learning_rate\": [0.05, 0.1, 0.2, 0.3, 0.5],\n#               \"max_depth\": [20, 22, 24, 30],\n#               \"colsample_bytree\": [0.6, 0.8, 0.9],\n#               \"subsample\": [0.3, 0.5, 0.7, 0.9],\n#               \"reg_alpha\": [0.1, 0.2, 0.6, 0.8],\n#               \"reg_lambda\": [10, 12, 14, 16],\n#               \"n_estimators\": [40, 60, 80]\n#               }\n# reg_tree = ltb.LGBMRegressor()\n# reg = RandomizedSearchCV(reg_tree, param_distributions=param_grid, n_iter=30,\n#                          scoring='neg_mean_absolute_error', verbose=1, cv=4, random_state=42, n_jobs=-1)\n# result = reg.fit(X_train_SVD, y_train)\n# params = result.best_params_\n# params","446f69a3":"params = {'subsample': 0.7,\n 'reg_lambda': 12,\n 'reg_alpha': 0.6,\n 'n_estimators': 80,\n 'max_depth': 22,\n 'learning_rate': 0.5,\n 'colsample_bytree': 0.8,\n 'random_state': 42}","b9fe0897":"reg_tree = ltb.LGBMRegressor(**params)\nreg_tree.fit(X_train_SVD, y_train)","972af69c":"preds = reg_tree.predict(X_test_SVD)","6cb46d44":"mean_squared_error(y_test, preds)","5392da75":"df_sub = pd.read_csv(\"..\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv\")","f848be59":"df_sub.head(2)","9869b0ac":"df_sub['text'] = df_sub['text'].apply(clean_text)","31c07cc4":"df_sub['clean_comment_text_NO_STOPWORDS'] = df_sub['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))","02ad6e84":"df_sub['clean_comment_text_NO_STOPWORDS_stemmed'] = df_sub['clean_comment_text_NO_STOPWORDS'].apply(lambda x: ' '.join([stemmer.stem(word) for word in x.split()]))","3a7977b1":"X = vectorizer.transform(df_sub['clean_comment_text_NO_STOPWORDS_stemmed'])","654234ef":"X_SVD = truncatedSVD.transform(X)","70a35050":"final_preds = reg_tree.predict(X_SVD)","6950ac6f":"df_sub.columns","27108c9e":"final_df = pd.DataFrame(pd.concat([df_sub['comment_id'], pd.Series(final_preds)], axis=1))","bbaf8a49":"final_df.head()","cb9ddf56":"final_df.columns = ['comment_id', 'score']","75dc95cc":"final_df_sorted = final_df.sort_values('score', ascending = False)","ed0ddf64":"final_df_sorted.to_csv(\"submission.csv\", index=False)","9b19f976":"From these graphs we can see that the sentiment for non-toxic comments is predominantly neutral\/positive, for toxic comments neutral and negative, while for severely toxic comments the sentiment is neutral, negative and strongly negative.","267949e9":"For reproducibility the hyperparameters of the selected model were identified by this random search:","c88ced73":"## Train-Test Split","6391c87c":"## General Dataset Information\n### Load Data","977f20f3":"## Predict","b3d68471":"## Clean Data and Preprocessing","02106466":"Bigrams","4b5b8c73":"## SVD","8cb1998f":"<a id='models'><\/a>\n# Models","3d5a7d54":"There are different types of toxicity. Let's see if toxic and severely toxic include all other classes:","d05a2887":"#### In this notebook, an exploratory analysis of a dataset published in an old Jigsaw competition was performed. The model used is trained on this dataset. The proposed model is a simple LightGMB, but future notebooks will use models more suitable for unstructured data.","a0300edb":"<div>The data used for this competition are Wikipedia Talk page comments. The purpose is to rank the severity of comment toxicity from innocuous to outrageous, where the middle matters as much as the extremes.<\/br>\n<b>Important<\/b>:\nThere is no training data for this competition. You can refer to previous Jigsaw competitions for data that might be useful to train models.\n<h4> Competition URL:<\/h4> https:\/\/www.kaggle.com\/c\/jigsaw-toxic-severity-rating\n<h3> Files <\/h3>\n<span style=\"background-color:#e1e6e3;\">comments_to_score.csv<\/span> - collection of comments <\/br>\n<span style=\"background-color:#e1e6e3;\">validation_data.csv<\/span> - pair rankings that can be used to validate models <\/br>\n<span style=\"background-color:#e1e6e3;\">sample_submission.csv<\/span> - a sample submission file in the correct format <\/br>\n<\/br>\n<b style='margin-top:1.5%;margin-left:1%;background-color:#fbffb3'><i>Disclaimer: The dataset for this competition contains text that may be considered profane, vulgar, or offensive.<\/i><\/b><\/div>","9313538a":"<div>\nYou are provided with a large number of Wikipedia comments which have been labeled by human raters for toxic behavior. The types of toxicity are: \n- toxic\n- severe_toxic\n- obscene\n- threat\n- insult\n- identity_hate\n    \n<h4> Competition URL: <\/h4> https:\/\/www.kaggle.com\/c\/jigsaw-toxic-comment-classification-challenge\/overview\n<h3> Files <\/h3>\n<span style=\"background-color:#e1e6e3;\">train.csv<\/span> - the training set, contains comments with their binary labels <\/br>\n<span style=\"background-color:#e1e6e3;\">test.csv<\/span> - the test set, you must predict the toxicity probabilities for these comments. To deter hand labeling, the test set contains some comments which are not included in scoring <\/br>\n<span style=\"background-color:#e1e6e3;\">sample_submission.csv<\/span> - a sample submission file in the correct format <\/br>\n<span style=\"background-color:#e1e6e3;\">test_labels.csv<\/span> - labels for the test data; value of -1 indicates it was not used for scoring; (Note: file added after competition close!) \n<\/div>","4e669763":"## Save Submission File","fe14a29e":"Let's resample the zeros to balance the dataset","08a2398a":"## Clean Data","8252ccee":"<b style='margin-top:1.5%;margin-left:1%;background-color:#f6e51d'>In this notebook I applied a simple machine learning model, in the next notebooks I will use models more recommended for unstructured data, based on neural networks<\/b>","5853b283":"## Distribution of top n-grams for non-toxic, toxic, severe toxic","29360c19":"The selected hyperparameters were:","d54b08ec":"<b style='margin-top:1.5%;margin-left:1%;background-color:#f6e51d'><i>SO ALL COMMENTS CLASSIFIED AS SEVERE TOXIC ARE ALSO CLASSIFIED AS TOXIC<\/i><b>","7613e623":"## Install and Import Libraries","52c7ab4b":"## Word Clouds for non-toxic, toxic, severe toxic","985d9aa5":"## Preprocessing","d2fcd5a1":"Let's use TextBlob to calculate sentiment polarity. The sentiment polarity value lies in the range of [-1, 1] where 1 means positive sentiment and -1 means a negative sentiment:","c748fcc0":"## TF-IDF","558d3f4c":"Let's see some examples for each class","2bedeaa0":"How many comments contain more than one type of toxicity?","8ee009b4":"<a id='data_information'><\/a>\n# Datasets Information\n## Description of the Data Present within this Competition","9836290b":"<a id='eda'><\/a>\n# EDA for Jigsaw Toxic Comment Classification Challenge Dataset\nRegarding the data presented in this competition, I performed an exploratory analysis in the following notebook: https:\/\/www.kaggle.com\/serquet\/jigsaw-full-eda\nTherefore I proceed in an EDA phase only for the external data used.","6d28e551":"How many non-toxic comments?","6a3f96a8":"## LightGBM Model","8fe8f3c4":"### Comments Distribution","ec4cf57f":"### Sentiment Polarity","f5af0336":"Trigrams","ecc38838":"## Description of the Data Present within Jigsaw Toxic Comment Classification Challenge","57a7a30f":"Unigrams","2dbe8d69":"## Inference","f5c7c568":"<div style='color:white;background-color:#0b8b10; height: 100px; border-radius: 25px;'><h1 style='text-align:center;padding: 3%'>Jigsaw Rate Severity of Toxic Comments Competition<\/h1><\/div>","193316e6":"## Table of Contents\n* [Datasets Information](#data_information)\n    - Description of the Data Present within this Competition\n        - Files\n    - Description of the Data Present within Jigsaw Toxic Comment Classification Challenge\n        - Files\n* [EDA for Jigsaw Toxic Comment Classification Challenge Dataset](#eda)\n    - Install and Import Libraries\n    - General Dataset Information\n    - Clean Data\n    - Comments Distribution\n    - Distribution of top n-grams\n    - Sentiment Polarity\n    - Word Clouds\n* [Models](#models)\n    - Assumptions\n    - Preprocessing\n    - Train Test Split\n    - TF-IDF\n    - SVD\n    - LightGBM Model\n    - Inference","784515c2":"<div style='color:white;background-color:#0b8b10; height: 50px; border-radius: 25px;'><h1 style='text-align:center;padding: 1%'>The End<\/h1><\/div>","4e52ecb1":"An assumption we can make is that the severity score for each comment is given by the sum of the scores assigned to the different categories of toxicity: obscene, threat, insult and identity_hate, toxic and severe_toxic. \nFinally we multiply this score by 3 if the comment is considered severe_toxic and leave it unchanged if it is considered only toxic","d778b8e6":"## Assumptions"}}