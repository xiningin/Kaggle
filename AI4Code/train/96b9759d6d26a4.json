{"cell_type":{"03ea75df":"code","44e6f307":"code","b8c6bb65":"code","b3e0e6b2":"code","f4e9e7ea":"code","598a8705":"code","b6e04855":"code","a885d7e7":"markdown","a5a99815":"markdown","3214b358":"markdown","6a72dcfb":"markdown","9a23d8bc":"markdown"},"source":{"03ea75df":"import numpy as np\nimport pandas as pd\nimport gc\nimport time\nimport datetime\nfrom contextlib import contextmanager\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn import metrics, preprocessing\n\nfrom sklearn.model_selection import KFold # StratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.decomposition import PCA # KernelPCA\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\n\nfrom sklearn.preprocessing import LabelEncoder\n\n\npd.set_option('display.max_rows', 999)\npd.set_option('display.max_columns',700)\npd.set_option('display.float_format', lambda x: '%.3f' % x)\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) \nwarnings.filterwarnings(\"ignore\", category=FutureWarning) \nwarnings.filterwarnings(\"ignore\", category=UserWarning) \n","44e6f307":"@contextmanager\ndef timer(title):\n    t0 = time.time()\n    yield\n    print(\"{} - done in {:.0f}s\".format(title, time.time() - t0))\n    \n# Display\/plot feature importance\ndef display_importances(feature_importance_df_):\n    cols = feature_importance_df_[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False)[:100].index\n    best_features = feature_importance_df_.loc[feature_importance_df_.feature.isin(cols)]\n    plt.figure(figsize=(15, 20))\n    sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False))\n    plt.title('LightGBM Features (avg over folds)')\n    plt.tight_layout()\n    plt.savefig('lgbm_importances01.png')\n\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df\n","b8c6bb65":"def loading_data():\n    print('LOADING DATA')\n    train_transaction = pd.read_csv(\"\/kaggle\/input\/ieee-fraud-detection\/train_transaction.csv\")\n    train_identity = pd.read_csv(\"\/kaggle\/input\/ieee-fraud-detection\/train_identity.csv\")\n\n    test_transaction = pd.read_csv(\"\/kaggle\/input\/ieee-fraud-detection\/test_transaction.csv\")\n    test_identity = pd.read_csv(\"\/kaggle\/input\/ieee-fraud-detection\/test_identity.csv\")\n\n    # Fix column name \n    fix_col_name = {testIdCol:trainIdCol for testIdCol, trainIdCol in zip(test_identity.columns, train_identity.columns)}\n    test_identity.rename(columns=fix_col_name, inplace=True)\n\n    ## Reduce memory\n    train_transaction = reduce_mem_usage(train_transaction)\n    train_identity = reduce_mem_usage(train_identity)\n\n    test_transaction = reduce_mem_usage(test_transaction)\n    test_identity = reduce_mem_usage(test_identity)\n\n    # Merge (transaction-identity)\n    train = train_transaction.merge(train_identity, on='TransactionID', how='left')\n    test = test_transaction.merge(test_identity, on='TransactionID', how='left')\n\n    #MERGE (X_train - X_test)\n    train_test = pd.concat([train, test], ignore_index=True)\n\n    print(f'train dataset has {train.shape[0]} rows and {train.shape[1]} columns.')\n    print(f'test dataset has {test.shape[0]} rows and {test.shape[1]} columns.')\n\n    del train_transaction, train_identity, test_transaction, test_identity; x = gc.collect()  \n    return train_test","b3e0e6b2":"def processing_data(train_test):\n    print('PROCESSING DATA')\n    drop_col_list = []\n    \n    # TransactionDT\n    START_DATE = '2015-04-22'\n    startdate = datetime.datetime.strptime(START_DATE, \"%Y-%m-%d\")\n    train_test['NewDate'] = train_test['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds=x)))\n    train_test['NewDate_YMD'] = train_test['NewDate'].dt.year.astype(str) + '-' + train_test['NewDate'].dt.month.astype(str) + '-' + train_test['NewDate'].dt.day.astype(str)\n    train_test['NewDate_YearMonth'] = train_test['NewDate'].dt.year.astype(str) + '-' + train_test['NewDate'].dt.month.astype(str)\n    train_test['NewDate_Weekday'] = train_test['NewDate'].dt.dayofweek\n    train_test['NewDate_Hour'] = train_test['NewDate'].dt.hour\n    train_test['NewDate_Day'] = train_test['NewDate'].dt.day\n    drop_col_list.extend([\"TransactionDT\",\"NewDate\"])  ## !!!\n    \n    # TransactionAMT\n    train_test['New_Cents'] = (train_test['TransactionAmt'] - np.floor(train_test['TransactionAmt'])).astype('float32')\n    train_test['New_TransactionAmt_Bin'] = pd.qcut(train_test['TransactionAmt'],15)\n    \n    #cardX\n    card_cols = [c for c in train_test if c[0:2] == 'ca']\n    for col in ['card2','card3','card4','card5','card6']:\n        train_test[col] = train_test.groupby(['card1'])[col].transform(lambda x: x.mode(dropna=False).iat[0])\n        train_test[col].fillna(train_test[col].mode()[0], inplace=True)\n    \n    \n    # P_email_domain & R_email_domain \n    train_test.loc[train_test['P_emaildomain'].isin(['gmail.com', 'gmail']),'P_emaildomain'] = 'Google'\n    train_test.loc[train_test['P_emaildomain'].isin(['yahoo.com', 'yahoo.com.mx',  'yahoo.co.uk','yahoo.co.jp', 'yahoo.de', 'yahoo.fr','yahoo.es']), 'P_emaildomain'] = 'Yahoo'\n    train_test.loc[train_test['P_emaildomain'].isin(['hotmail.com','outlook.com','msn.com', 'live.com.mx', 'hotmail.es','hotmail.co.uk', 'hotmail.de','outlook.es', 'live.com', 'live.fr','hotmail.fr']), 'P_emaildomain'] = 'Microsoft'\n    train_test.loc[train_test['P_emaildomain'].isin(train_test['P_emaildomain'].value_counts()[train_test['P_emaildomain'].value_counts() <= 500 ].index), 'P_emaildomain'] = \"Others\"\n    train_test['P_emaildomain'].fillna(\"Unknown\", inplace=True)\n\n    train_test.loc[train_test['R_emaildomain'].isin(['gmail.com', 'gmail']),'R_emaildomain'] = 'Google'\n    train_test.loc[train_test['R_emaildomain'].isin(['yahoo.com', 'yahoo.com.mx',  'yahoo.co.uk','yahoo.co.jp', 'yahoo.de', 'yahoo.fr','yahoo.es']), 'R_emaildomain'] = 'Yahoo'\n    train_test.loc[train_test['R_emaildomain'].isin(['hotmail.com','outlook.com','msn.com', 'live.com.mx', 'hotmail.es','hotmail.co.uk', 'hotmail.de','outlook.es', 'live.com', 'live.fr','hotmail.fr']), 'R_emaildomain'] = 'Microsoft'\n    train_test.loc[train_test['R_emaildomain'].isin(train_test['R_emaildomain'].value_counts()[train_test['R_emaildomain'].value_counts() <= 300 ].index), 'R_emaildomain'] = \"Others\"\n    train_test['R_emaildomain'].fillna(\"Unknown\", inplace=True)\n\n    # DeviceInfo\n    train_test['DeviceInfo'] = train_test['DeviceInfo'].fillna('unknown_device').str.lower()\n    train_test['DeviceInfo'] = train_test['DeviceInfo'].str.split('\/', expand=True)[0]\n    \n    train_test.loc[train_test['DeviceInfo'].str.contains('SM', na=False), 'DeviceInfo'] = 'Samsung'\n    train_test.loc[train_test['DeviceInfo'].str.contains('SAMSUNG', na=False), 'DeviceInfo'] = 'Samsung'\n    train_test.loc[train_test['DeviceInfo'].str.contains('GT-', na=False), 'DeviceInfo'] = 'Samsung'\n    train_test.loc[train_test['DeviceInfo'].str.contains('Moto G', na=False), 'DeviceInfo'] = 'Motorola'\n    train_test.loc[train_test['DeviceInfo'].str.contains('Moto', na=False), 'DeviceInfo'] = 'Motorola'\n    train_test.loc[train_test['DeviceInfo'].str.contains('moto', na=False), 'DeviceInfo'] = 'Motorola'\n    train_test.loc[train_test['DeviceInfo'].str.contains('LG-', na=False), 'DeviceInfo'] = 'LG'\n    train_test.loc[train_test['DeviceInfo'].str.contains('rv:', na=False), 'DeviceInfo'] = 'RV'\n    train_test.loc[train_test['DeviceInfo'].str.contains('HUAWEI', na=False), 'DeviceInfo'] = 'Huawei'\n    train_test.loc[train_test['DeviceInfo'].str.contains('ALE-', na=False), 'DeviceInfo'] = 'Huawei'\n    train_test.loc[train_test['DeviceInfo'].str.contains('-L', na=False), 'DeviceInfo'] = 'Huawei'\n    train_test.loc[train_test['DeviceInfo'].str.contains('Blade', na=False), 'DeviceInfo'] = 'ZTE'\n    train_test.loc[train_test['DeviceInfo'].str.contains('BLADE', na=False), 'DeviceInfo'] = 'ZTE'\n    train_test.loc[train_test['DeviceInfo'].str.contains('Linux', na=False), 'DeviceInfo'] = 'Linux'\n    train_test.loc[train_test['DeviceInfo'].str.contains('XT', na=False), 'DeviceInfo'] = 'Sony'\n    train_test.loc[train_test['DeviceInfo'].str.contains('HTC', na=False), 'DeviceInfo'] = 'HTC'\n    train_test.loc[train_test['DeviceInfo'].str.contains('ASUS', na=False), 'DeviceInfo'] = 'Asus'\n\n    train_test.loc[train_test['DeviceInfo'].isin(train_test['DeviceInfo'].value_counts()[train_test['DeviceInfo'].value_counts() < 1000].index), 'DeviceInfo'] = \"Others\"\n\n    # V1 - V339\n    v_cols = [c for c in train_test if c[0] == 'V']\n    v_nan_df = train_test[v_cols].isna()\n    nan_groups={}\n\n    for col in v_cols:\n        cur_group = v_nan_df[col].sum()\n        try:\n            nan_groups[cur_group].append(col)\n        except:\n            nan_groups[cur_group]=[col]\n    del v_nan_df; x=gc.collect()\n\n    for nan_cnt, v_group in nan_groups.items():\n        train_test['New_v_group_'+str(nan_cnt)+'_nulls'] = nan_cnt\n        sc = preprocessing.MinMaxScaler()\n        pca = PCA(n_components=2)\n        v_group_pca = pca.fit_transform(sc.fit_transform(train_test[v_group].fillna(-1)))\n        train_test['New_v_group_'+str(nan_cnt)+'_pca0'] = v_group_pca[:,0]\n        train_test['New_v_group_'+str(nan_cnt)+'_pca1'] = v_group_pca[:,1]\n\n    drop_col_list.extend(v_cols)\n    \n    print('CREATING NEW FEATURES')\n        \n    train_test['New_card1_card2']=train_test['card1'].astype(str)+'_'+train_test['card2'].astype(str)\n    train_test['New_addr1_addr2']=train_test['addr1'].astype(str)+'_'+train_test['addr2'].astype(str)\n    train_test['New_card1_card2_addr1_addr2']=train_test['card1'].astype(str)+'_'+train_test['card2'].astype(str)+'_'+train_test['addr1'].astype(str)+'_'+train_test['addr2'].astype(str)\n\n    train_test['New_P_emaildomain_addr1'] = train_test['P_emaildomain'] + '_' + train_test['addr1'].astype(str)\n    train_test['New_R_emaildomain_addr2'] = train_test['R_emaildomain'] + '_' + train_test['addr2'].astype(str)\n    \n    #Aggregation features\n    train_test['New_TransactionAmt_to_mean_card1'] = train_test['TransactionAmt'] \/ train_test.groupby(['card1'])['TransactionAmt'].transform('mean')\n    train_test['New_TransactionAmt_to_mean_card4'] = train_test['TransactionAmt'] \/ train_test.groupby(['card4'])['TransactionAmt'].transform('mean')\n    train_test['New_TransactionAmt_to_std_card1'] = train_test['TransactionAmt'] \/ train_test.groupby(['card1'])['TransactionAmt'].transform('std')\n    train_test['TransactionAmt_to_std_card4'] = train_test['TransactionAmt'] \/ train_test.groupby(['card4'])['TransactionAmt'].transform('std')\n\n    train_test['New_id_02_to_mean_card1'] = train_test['id_02'] \/ train_test.groupby(['card1'])['id_02'].transform('mean')\n    train_test['New_id_02_to_mean_card4'] = train_test['id_02'] \/ train_test.groupby(['card4'])['id_02'].transform('mean')\n    train_test['New_id_02_to_std_card1'] = train_test['id_02'] \/ train_test.groupby(['card1'])['id_02'].transform('std')\n    train_test['New_id_02_to_std_card4'] = train_test['id_02'] \/ train_test.groupby(['card4'])['id_02'].transform('std')\n\n    train_test['New_D15_to_mean_card1'] = train_test['D15'] \/ train_test.groupby(['card1'])['D15'].transform('mean')\n    train_test['New_D15_to_mean_card4'] = train_test['D15'] \/ train_test.groupby(['card4'])['D15'].transform('mean')\n    train_test['New_D15_to_std_card1'] = train_test['D15'] \/ train_test.groupby(['card1'])['D15'].transform('std')\n    train_test['New_D15_to_std_card4'] = train_test['D15'] \/ train_test.groupby(['card4'])['D15'].transform('std')\n\n    train_test['New_D15_to_mean_addr1'] = train_test['D15'] \/ train_test.groupby(['addr1'])['D15'].transform('mean')\n    train_test['New_D15_to_mean_card4'] = train_test['D15'] \/ train_test.groupby(['card4'])['D15'].transform('mean')\n    train_test['New_D15_to_std_addr1'] = train_test['D15'] \/ train_test.groupby(['addr1'])['D15'].transform('std')\n    train_test['New_D15_to_std_card4'] = train_test['D15'] \/ train_test.groupby(['card4'])['D15'].transform('std')\n    \n    drop_col_list.extend(card_cols)\n    \n    # Frequency Encoding \n    fe_col_list=[\"New_TransactionAmt_Bin\",'card4','card6','P_emaildomain','R_emaildomain','DeviceType','DeviceInfo']+[c for c in train_test if c[0] == 'M']\n    for col in fe_col_list:\n        vc = train_test[col].value_counts(dropna=True, normalize=True).to_dict()\n        vc[-1] = -1\n        nm = 'New_'+col+'_FE'\n        train_test[nm] = train_test[col].map(vc)\n        train_test[nm] = train_test[nm].astype('float32')\n\n    \n    print('DROPING UNNECESSARY FEATURES')\n    train_test=train_test.drop(drop_col_list, axis=1)\n    \n    print('APPLYING LABEL ENCODING TO CATEGORICAL FEATURES')\n    for col in train_test.columns:\n        if train_test[col].dtype == 'object':\n            le = LabelEncoder()\n            le.fit(list(train_test[col].astype(str).values))\n            train_test[col] = le.transform(list(train_test[col].astype(str).values))\n    \n    print('REDUCING MEMORY USAGE')\n    train_test = reduce_mem_usage(train_test)\n    \n    print('DATA IS READY TO MODELLING')\n    \n    return train_test","f4e9e7ea":"def modeling(train_test,target):\n\n    train = train_test[train_test[target].notnull()]\n    test = train_test[train_test[target].isnull()]\n\n    folds = KFold(n_splits = 10, shuffle = True, random_state = 1001)\n\n    oof_preds = np.zeros(train.shape[0])\n    sub_preds = np.zeros(test.shape[0])\n    \n    feature_importance_df = pd.DataFrame()\n\n    features = [f for f in train.columns if f not in [target,'TransactionID','New_TransactionAmt_Bin','NewDate']]\n\n    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train[features], train[target])):\n        \n        start_time = time.time()\n        print('Training on fold {}'.format(n_fold + 1))\n\n        X_train, y_train = train[features].iloc[train_idx], train[target].iloc[train_idx]\n\n        X_valid, y_valid = train[features].iloc[valid_idx], train[target].iloc[valid_idx]\n        \n        params={'learning_rate': 0.01,\n        'objective': 'binary',\n        'metric': 'auc',\n        'num_threads': -1,\n        'num_leaves': 256,\n        'verbose': 1,\n        'random_state': 42,\n        'bagging_fraction': 1,\n        'feature_fraction': 0.85 }\n       \n        clf = LGBMClassifier(**params, n_estimators=1000) #categorical_feature = LGBM_cat_col_list\n\n        clf.fit(X_train, y_train, eval_set = [(X_train, y_train), (X_valid, y_valid)], \n                eval_metric = 'auc', verbose = 200, early_stopping_rounds = 200)\n\n        #y_pred_valid\n        oof_preds[valid_idx] = clf.predict_proba(X_valid, num_iteration=clf.best_iteration_)[:, 1]\n        sub_preds += clf.predict_proba(test[features], num_iteration=clf.best_iteration_)[:, 1] \/ folds.n_splits\n\n        fold_importance_df = pd.DataFrame()\n        fold_importance_df[\"feature\"] = features\n        fold_importance_df[\"importance\"] = clf.feature_importances_\n        fold_importance_df[\"fold\"] = n_fold + 1\n        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n\n\n        print('Fold %2d AUC : %.6f' % (n_fold + 1, roc_auc_score(y_valid, oof_preds[valid_idx]))) \n\n\n    print('Full AUC score %.6f' % roc_auc_score(train[target], oof_preds)) #y_pred_valid   \n\n    test[target] = sub_preds\n    test[['TransactionID', target]].to_csv(\"submission_lightgbm2.csv\", index= False)\n\n    display_importances(feature_importance_df)\n    \n    return feature_importance_df","598a8705":"def main():\n    with timer(\"Loading Data\"):\n        train_test = loading_data()\n    \n    with timer(\"Preprocessing Data\"):\n        train_test = processing_data(train_test)\n        \n    with timer(\"Modeling\"):\n        feat_importance = modeling(train_test ,'isFraud')","b6e04855":"if __name__ == \"__main__\":\n    with timer(\"Full model run\"):\n        main()","a885d7e7":"# IEEE Fraud Detection - MODEL","a5a99815":"I will create a LIGHTGBM model. And, I also have a kernel for data analysis and EDA. You can check it out here : [https:\/\/www.kaggle.com\/mervebdurna\/ieee-fraud-detection-eda?scriptVersionId=35220527](http:\/\/)","3214b358":"## Helper Functions","6a72dcfb":"# Main Functions","9a23d8bc":"## Libraries"}}