{"cell_type":{"ed20e217":"code","aeb9e1a9":"code","8808477b":"code","a2500242":"code","a4f87454":"code","c431ff3f":"code","ec9833d6":"code","e985efdd":"code","58f2ab86":"code","0ce2e609":"code","99346f40":"code","68bdb418":"code","79f821c3":"code","4b14f601":"code","833c2e44":"code","cb74b2ea":"code","8ae066b5":"code","bcebc41b":"code","67de7d5d":"code","7c63744a":"code","6ed0b45d":"code","9eecd206":"code","fcfa7fba":"code","4693f85c":"code","ff50fb33":"code","5cddfc1f":"code","32e1101f":"code","31668167":"code","10fee8e0":"code","91f27893":"code","c87f35c6":"code","ca52f442":"code","b914fd11":"code","30524ed3":"code","205406b6":"code","aa10c23d":"code","3a940317":"code","74515e07":"code","29c93a95":"code","33fc3d2a":"code","53ae9922":"code","40b7db75":"code","0846abb3":"code","56d576b8":"code","c646bf71":"code","c83050d1":"code","af3bdb28":"code","68915a31":"code","01524d44":"code","8b89d415":"code","cd915f7d":"code","5cbf42c1":"code","68022600":"code","df7e5364":"code","996df5c9":"code","e5a8917d":"code","62e9e718":"code","3307d873":"code","21cf2170":"markdown","3d204e6e":"markdown","836ba9d7":"markdown","87e25bba":"markdown","7e25b1e1":"markdown","33a8c820":"markdown","7a25ffa4":"markdown","7b81eeb4":"markdown","a6890033":"markdown","c8f1c5b9":"markdown","707c523e":"markdown","1b2066a0":"markdown","f2c0519b":"markdown","f2c88012":"markdown","4e05d963":"markdown","1c6b645a":"markdown","75c17115":"markdown","c60858c2":"markdown","6ed533db":"markdown","64924ae0":"markdown","b77f1e45":"markdown","c48559f8":"markdown","42781862":"markdown","9380cfde":"markdown","4c69336f":"markdown","ebf5403e":"markdown","60179881":"markdown","b4078bf8":"markdown","15d129b3":"markdown","abb5d40b":"markdown","7c31d91f":"markdown","57b2f946":"markdown","e3a954ac":"markdown","b019bac7":"markdown","aee6f23f":"markdown","72587ad4":"markdown","0ac7061f":"markdown"},"source":{"ed20e217":"import numpy as np\nimport pandas as pd \n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')","aeb9e1a9":"# data will be used for training and validation\n# test will be used for final evaluation\ndata = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ndata.head()","8808477b":"data.describe(include=\"all\")","a2500242":"test.describe(include=\"all\")","a4f87454":"print(\"train dataset has \" + str(len(data)) + \" passengers\")\nprint(\"test dataset has \" + str(len(test)) + \" passengers\")","c431ff3f":"# exploring our feature data types\ndata.dtypes","ec9833d6":"# see how many feature and label data are missing for both train and test\npd.DataFrame({\"train\": data.isna().sum(),\n              \"test\": test.isna().sum()})","e985efdd":"sns.barplot(x=\"Sex\", y=\"Survived\", data=data);","58f2ab86":"print(\"Percentage of females who survived:\", str(round(data[\"Survived\"][data[\"Sex\"] == 'female'].value_counts(normalize = True)[1]*100, 2)),\"%\")\n\nprint(\"Percentage of males who survived:\", str(round(data[\"Survived\"][data[\"Sex\"] == 'male'].value_counts(normalize = True)[1]*100, 2)),\"%\")","0ce2e609":"senior = data[\"Age\"][data[\"Age\"] > 60].count()\nadult = data[\"Age\"][data[\"Age\"] <= 60][data[\"Age\"] > 40].count()\nyoungadult = data[\"Age\"][data[\"Age\"] <= 40][data[\"Age\"] > 20].count()\nteenager = data[\"Age\"][data[\"Age\"] <= 20][data[\"Age\"] > 12].count()\nchild = data[\"Age\"][data[\"Age\"] <= 12][data[\"Age\"] > 4].count()\ntoddler = data[\"Age\"][data[\"Age\"] <= 4][data[\"Age\"] > 1].count()\nbaby = data[\"Age\"][data[\"Age\"] <= 1][data[\"Age\"] >= 0].count()\nmissing = data[\"Age\"].isna().sum()\ntotal = senior+adult+youngadult+teenager+child+toddler+baby+missing\n\nprint(\"Passengers by Age Group in train set\")\nprint(\"senior:\", senior)\nprint(\"adult:\", adult)\nprint(\"youngadult:\", youngadult)\nprint(\"teenager:\", teenager)\nprint(\"child:\", child)\nprint(\"toddler:\", toddler)\nprint(\"baby:\", baby)\nprint(\"missing:\", missing)\nprint(\"total:\", total)\n","99346f40":"print(\"Percentage of senior who survived:\", str(round(data[\"Survived\"][data[\"Age\"] > 60].value_counts(normalize = True)[1]*100, 2)),\"%\")\nprint(\"Percentage of adult who survived:\", str(round(data[\"Survived\"][data[\"Age\"] <= 60][data[\"Age\"] > 40].value_counts(normalize = True)[1]*100, 2)),\"%\")\nprint(\"Percentage of youngadult who survived:\", str(round(data[\"Survived\"][data[\"Age\"] <= 40][data[\"Age\"] > 20].value_counts(normalize = True)[1]*100, 2)),\"%\")\nprint(\"Percentage of teen who survived:\", str(round(data[\"Survived\"][data[\"Age\"] <= 20][data[\"Age\"] > 12].value_counts(normalize = True)[1]*100, 2)),\"%\")\nprint(\"Percentage of child who survived:\", str(round(data[\"Survived\"][data[\"Age\"] <= 12][data[\"Age\"] > 4].value_counts(normalize = True)[1]*100, 2)),\"%\")\nprint(\"Percentage of toddler who survived:\", str(round(data[\"Survived\"][data[\"Age\"] <= 4][data[\"Age\"] > 1].value_counts(normalize = True)[1]*100, 2)),\"%\")\nprint(\"Percentage of baby who survived:\", str(round(data[\"Survived\"][data[\"Age\"] <= 1][data[\"Age\"] > 0].value_counts(normalize = True)[1]*100, 2)),\"%\")\nprint(\"Percentage of age-missing who survived:\", str(round(data[\"Survived\"][data[\"Age\"].isna()].value_counts(normalize = True)[1]*100, 2)),\"%\")\n","68bdb418":"\nsns.barplot(x=\"Pclass\", y=\"Survived\", data=data)\n\n#print percentage of people by Pclass that survived\nprint(\"Percentage of Pclass = 1 who survived:\", data[\"Survived\"][data[\"Pclass\"] == 1].value_counts(normalize = True)[1]*100)\n\nprint(\"Percentage of Pclass = 2 who survived:\", data[\"Survived\"][data[\"Pclass\"] == 2].value_counts(normalize = True)[1]*100)\n\nprint(\"Percentage of Pclass = 3 who survived:\", data[\"Survived\"][data[\"Pclass\"] == 3].value_counts(normalize = True)[1]*100)","79f821c3":"data[\"Fare\"].min(), data[\"Fare\"].max(), data[\"Fare\"].mean()","4b14f601":"data[\"Fare\"].plot.hist();","833c2e44":"# grouping fare\nhigh_fare = data[\"Fare\"][data[\"Fare\"] > 200].count()\nmid_fare = data[\"Fare\"][data[\"Fare\"] <= 200][data[\"Fare\"] > 50].count()\nlow_fare = data[\"Fare\"][data[\"Fare\"] <= 50][data[\"Fare\"] > 25].count()\ndirt_fare = data[\"Fare\"][data[\"Fare\"] <= 25].count()\n\n# print(\"high: \", high_fare)\nhigh_fare, mid_fare, low_fare, dirt_fare\n\n","cb74b2ea":"# testing hypothesis: percentage of high-fare group who survived\nprint(\"Percentage of high-fare who survived:\", data[\"Survived\"][data[\"Fare\"] > 200].value_counts(normalize = True)[1]*100)\nprint(\"Percentage of mid-fare who survived:\", data[\"Survived\"][data[\"Fare\"] <= 200][data[\"Fare\"] > 50].value_counts(normalize = True)[1]*100)\nprint(\"Percentage of low-fare who survived:\", data[\"Survived\"][data[\"Fare\"] <= 50][data[\"Fare\"] > 25].value_counts(normalize = True)[1]*100)\nprint(\"Percentage of dirt-fare who survived:\", data[\"Survived\"][data[\"Fare\"] <= 25].value_counts(normalize = True)[1]*100)","8ae066b5":"# survival rate by no. of siblings\nsns.barplot(x=\"SibSp\", y=\"Survived\", data=data);","bcebc41b":"sns.barplot(x=\"Parch\", y=\"Survived\", data=data);","67de7d5d":"has_cabin = data[\"Cabin\"][data[\"Cabin\"] != None].count()\nno_cabin = data[\"Cabin\"].isna().sum()\nhas_cabin, no_cabin","7c63744a":"# explore survival rate of those with recorded cabins vs without\n\nprint(\"Percentage of has_cabin who survived:\", data[\"Survived\"][data[\"Cabin\"] != None].value_counts(normalize = True)[1]*100)\n\nprint(\"Percentage of no_cabin who survived:\", data[\"Survived\"][data[\"Cabin\"].isna()].value_counts(normalize = True)[1]*100)","6ed0b45d":"# explore survival rate by cabin (A to G)\n# we are filling up missing with \"Z\" since str.contains doesn't work on missing value\ndata[\"Cabin\"].fillna(\"Z\", inplace=True)","9eecd206":"cabin_class = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"Z\"]\n\nfor cabin in cabin_class:\n    print(\"% of cabin class\", cabin, \"who survived: \", \n          str(round(data[\"Survived\"][data[\"Cabin\"].str.contains(cabin)].value_counts(normalize = True)[1]*100)), \"%\",\n          \"out of \", data[\"Cabin\"].str.contains(cabin).sum(), \"passengers\")","fcfa7fba":"print(\"Number of people embarking in Southampton (S):\", data[\"Embarked\"][data[\"Embarked\"] == \"S\"].count())\n\nprint(\"Number of people embarking in Cherbourg (C):\", data[\"Embarked\"][data[\"Embarked\"] == \"C\"].count())\n\nprint(\"Number of people embarking in Queenstown (Q):\", data[\"Embarked\"][data[\"Embarked\"] == \"Q\"].count())","4693f85c":"print(\"Percentage of S who survived:\", data[\"Survived\"][data[\"Embarked\"] == \"S\"].value_counts(normalize = True)[1]*100)\nprint(\"Percentage of C who survived:\", data[\"Survived\"][data[\"Embarked\"] == \"C\"].value_counts(normalize = True)[1]*100)\nprint(\"Percentage of Q who survived:\", data[\"Survived\"][data[\"Embarked\"] == \"Q\"].value_counts(normalize = True)[1]*100)","ff50fb33":"#create a combined group of both datasets\ncombine = [data, test]\n\n#extract a title for each Name in the train and test datasets\nfor dataset in combine:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\npd.crosstab(data['Title'], data['Sex'])","5cddfc1f":"# test our hypothesis if certain titles have higher survival rate\n# note that sample size is limited, so we will only be testing a few\n\nprint(\"Master\", data[\"Survived\"][data[\"Title\"] == \"Master\"].value_counts(normalize = True)[1]*100)\nprint(\"Miss\", data[\"Survived\"][data[\"Title\"] == \"Miss\"].value_counts(normalize = True)[1]*100)\nprint(\"Mrs\", data[\"Survived\"][data[\"Title\"] == \"Mrs\"].value_counts(normalize = True)[1]*100)\nprint(\"Dr\", data[\"Survived\"][data[\"Title\"] == \"Dr\"].value_counts(normalize = True)[1]*100)","32e1101f":"#get a list of the features within the dataset\nprint(data.columns)\nprint(test.columns)","31668167":"# Split into X and y\nX = data.drop(\"Survived\", axis=1)\ny = data[\"Survived\"]","10fee8e0":"# writing our function\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\n\ndef transform_data(data):\n    # create new feature: HasCabin\n    data[\"Cabin\"].fillna(\"Z\", inplace=True) # we previously filled missing with Z on train, but haven't done for test\n    data[\"HasCabin\"] = data[\"Cabin\"].str.contains(\"Z\").astype('int')\n    \n    # create new feature: AgeGroup\n    data[\"Age\"] = data[\"Age\"].fillna(-0.5)\n    bins = [-1, 0, 1, 4, 12, 20, 40, 60, np.inf]\n    labels = ['AgeMissing', 'Baby', 'Toddler', 'Child', 'Teen', 'YoungAdult', 'Adult', 'Senior']\n    data['AgeGroup'] = pd.cut(data[\"Age\"], bins, labels = labels)    \n\n    # create new feature: FareGroup\n    data[\"Fare\"] = data[\"Fare\"].fillna(-0.5)\n    bins_2 = [-1, 0, 25, 50, 200, np.inf]\n    labels_2 = ['FareMissing', 'DirtFare', 'LowFare', 'MidFare', 'HighFare']\n    data['FareGroup'] = pd.cut(data[\"Fare\"], bins_2, labels = labels_2)     \n    \n    # remove \"PassengerId\", \"Name\", \"Ticket\", \"Title\", \"Cabin\", \"Age\", \"Fare\"\n    data = data.drop(\"PassengerId\", axis=1)\n    data = data.drop(\"Name\", axis=1)\n    data = data.drop(\"Ticket\", axis=1)\n    data = data.drop(\"Title\", axis=1)\n    data = data.drop(\"Cabin\", axis=1)\n    data = data.drop(\"Age\", axis=1)\n    data = data.drop(\"Fare\", axis=1)                                                            \n    \n    # fill na with pandas\n    # HasCabin, AgeGroup, and FareGroup already filled above, so only need to fill Embarked\n    data[\"Embarked\"].fillna(data[\"Embarked\"].mode()[0], inplace=True)\n    \n    # transform \"Sex\", \"Embark\", \"AgeGroup\", \"FareGroup\", \"Pclass\", \"HasCabin\"\n    # One Hot Encoding transforms categories into different feature columns of 1 and 0\n    # note even though \"Pclass\" data is in numbers (1,2,3), they are Categorical features (instead of numerical like Age) hence we need to encode it\n    one_hot = OneHotEncoder()\n    transformer = ColumnTransformer([(\"one_hot\",\n                                      one_hot,\n                                      [\"Sex\", \"Embarked\", \"AgeGroup\", \"FareGroup\", \"Pclass\", \"HasCabin\"])],\n                                    remainder=\"passthrough\")\n    data = transformer.fit_transform(data)\n    \n    return data","91f27893":"# applying our function to transform X_train\n\nX_tf = transform_data(X)\npd.DataFrame(X_tf)\n# note this converts to numpy array, and not pd","c87f35c6":"# number of columns we should have\n1+1+2+3+8+5+3+2","ca52f442":"# check that number of rows is intact\nlen(X_tf), len(X)","b914fd11":"# check if any missing value\npd.DataFrame(X_tf).isna().sum()","30524ed3":"# Split data into train and validation sets\nfrom sklearn.model_selection import train_test_split\nnp.random.seed(17)\nX_train, X_val, y_train, y_val = train_test_split(X_tf, y, test_size=0.2)","205406b6":"# confirm splitting is done right\nlen(X_train), len(X_val), len(y_train), len(y_val)","aa10c23d":"# Creating a function to fit and score across the models\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Put models in a dictionary\nmodels = {\"Logistic Regression\": LogisticRegression(),\n          \"KNN\": KNeighborsClassifier(),\n          \"Random Forest\": RandomForestClassifier(),\n          \"Gradient Boosting\": GradientBoostingClassifier(),\n          \"SVC\": SVC(),\n          \"SGD\": SGDClassifier(),\n          \"Decision Tree\": DecisionTreeClassifier()}\n\n\ndef fit_and_score(models, X_train, X_val, y_train, y_val):\n    \"\"\"\n    Fits and evaluates given machine learning models.\n    models : a dict of differetn Scikit-Learn machine learning models\n    \"\"\"\n    # Set random seed\n    np.random.seed(17)\n    # Make a dictionary to keep model scores\n    model_scores = {}\n    # Loop through models\n    for name, model in models.items():\n        # Fit the model to the data\n        model.fit(X_train, y_train)\n        # Evaluate the model and append its score to model_scores\n        model_scores[name] = model.score(X_val, y_val)\n    return model_scores\n","3a940317":"# Executing our function, and we will see the \"accuracy\" score (using default scoring method of each model)\nfit_and_score(models=models,\n              X_train=X_train,\n              X_val=X_val,\n              y_train=y_train,\n              y_val=y_val)","74515e07":"# creating a evaluation metrics\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\ndef evaluate_preds(y_true, y_preds):\n    \"\"\"\n    perform evaluation comparison on y_true labels vs y_preds labels\n    \"\"\"\n    accuracy = accuracy_score(y_true, y_preds)\n    precision = precision_score(y_true, y_preds)\n    recall = recall_score(y_true, y_preds)\n    f1 = f1_score(y_true, y_preds)\n    metric_dict = {\"accuracy\": round(accuracy, 2),\n                    \"precision\": round(precision, 2),\n                    \"recall\": round(recall, 2),\n                    \"f1\": round(f1, 2)}\n    print(f\"Acc: {accuracy * 100:.2f}%\")\n    print(f\"Precision: {precision:.2f}\")\n    print(f\"Recall: {recall:.2f}\")\n    print(f\"F1 score: {f1:.2f}\")\n    return metric_dict","29c93a95":"# RandomForest\n\nnp.random.seed(19)\nrf = RandomForestClassifier()\nrf.fit(X_train, y_train)\ny_preds = rf.predict(X_val)  # ML's prediction using X_val\n\n# evaluate using our evaluation function on validation set\nrf_metrics = evaluate_preds(y_val, y_preds)  # compares y_preds with y_val\/y_true\nrf_metrics","33fc3d2a":"# SVC\n\nnp.random.seed(19)\nsvc = SVC()\nsvc.fit(X_train, y_train)\ny_preds = svc.predict(X_val)  # ML's prediction using X_val\n\n# evaluate using our evaluation function on validation set\nsvc_metrics = evaluate_preds(y_val, y_preds)  # compares y_preds with y_val\/y_true\nsvc_metrics","53ae9922":"# Decision Tree\n\nnp.random.seed(19)\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, y_train)\ny_preds = decision_tree.predict(X_val)  # ML's prediction using X_val\n\n# evaluate using our evaluation function on validation set\ndecision_tree_metrics = evaluate_preds(y_val, y_preds)  # compares y_preds with y_val\/y_true\ndecision_tree_metrics\n","40b7db75":"# Lets compare our different models with new metrics\n\ncompare_metrics = pd.DataFrame({\"Random Forest\": rf_metrics,\n                                \"SVC\": svc_metrics,\n                                \"Decision Tree\": decision_tree_metrics})\ncompare_metrics.plot.bar(figsize=(10,8));","0846abb3":"# check available hyperparameters for SVC\nsvc.get_params()","56d576b8":"# tuning hyperparameters by RandomSearchCV\n\nfrom sklearn.model_selection import RandomizedSearchCV\ngrid = {\"kernel\": [\"linear\", \"rbf\", \"poly\"],\n        \"gamma\": [\"scale\", \"auto\"],\n        \"degree\": [0,1,2,3,4,5,6],\n        \"class_weight\": [\"balanced\", None],\n        \"C\": [100, 10, 1.0, 0.1, 0.001]}\n\nnp.random.seed(17)\n\nsvc = SVC()\nrs_svc = RandomizedSearchCV(estimator=svc,\n                            param_distributions=grid,  # what we defined above\n                            n_iter=10, # number of combinations to try\n                            cv=5,   # number of cross-validation split\n                            verbose=2)\nrs_svc.fit(X_train, y_train);","c646bf71":"# checking out best parameters we find from tuning\nrs_svc.best_params_","c83050d1":"# evaluating our model tuned with RandomSearchCV\nrs_y_preds = rs_svc.predict(X_val)\n\n# evaluate predictions\nrs_metrics = evaluate_preds(y_val, rs_y_preds)","af3bdb28":"# tuning hyperparameters by GridSearchCV\nfrom sklearn.model_selection import GridSearchCV\n\n\ngrid_2 = {\"kernel\": [\"linear\", \"rbf\"],\n        \"gamma\": [\"scale\"],\n        \"degree\": [1,2,3],\n        \"class_weight\": [None],\n        \"C\": [100, 10, 1.0]}\n\nnp.random.seed(17)\n\nsvc = SVC()\n\n# Setup GridSearchCV\ngs_svc = GridSearchCV(estimator=svc,\n                      param_grid=grid_2,\n                      cv=5,\n                      verbose=2)\n\n# Fit the GSCV version of clf\ngs_svc.fit(X_train, y_train);\n","68915a31":"gs_svc.best_params_","01524d44":"# evaluating our model tuned with GridSearchCV\ngs_y_preds = gs_svc.predict(X_val)\n\n# evaluate predictions\ngs_metrics = evaluate_preds(y_val, gs_y_preds)","8b89d415":"# Lets compare our different model metrics\n\ncompare_metrics = pd.DataFrame({\"baseline SVC\": svc_metrics,\n                                \"random search\": rs_metrics,\n                                \"grid search\": gs_metrics})\ncompare_metrics","cd915f7d":"# with bar graph\ncompare_metrics.plot.bar(figsize=(10,8));","5cbf42c1":"# our raw test data\ntest.head()\n","68022600":"# tranforming test data\ntest_tf = transform_data(test)\npd.DataFrame(test_tf)","df7e5364":"# check no missing value\npd.DataFrame(test_tf).isna().sum()","996df5c9":"# run our prediction with our rs_svc model\n\ntest_preds = rs_svc.predict(test_tf)\npd.DataFrame(test_preds)","e5a8917d":"submission = pd.DataFrame({\"PassengerId\": test[\"PassengerId\"], \n                          \"Survived\": test_preds})\nsubmission","62e9e718":"# check length\n\nlen(submission), len(test)","3307d873":"submission.to_csv(\"submission_v2.csv\", index=False)","21cf2170":"The sample size for each cabin size is pretty small, so it might not make sense to classify based on different classes since it might result in overfitting? So for now, we will stick to classifying by has_cabin and no_cabin.","3d204e6e":"### Data Summary \n\n1. Raw features: \n\n  * Pclass, \n  * Name, \n  * Sex, \n  * Age, \n  * Sibsp (no. of siblings), \n  * Parch (no. of parent\/child), \n  * Ticket, \n  * Fare, \n  * Cabin, \n  * Embarked\n\n \n2. We are trying to use our features to predict our label - \"Survived\"\n\n \n3. The only difference between train and test dataset is that test dataset has no \"Survived\" label, since that's what we are trying to predict.\n\n\n4. There are 891 dataset in train and 418 in test, which is on the low side.\n\n \n5. Data is relatively complete - with most (77%) missing \"Cabin\" data, and some (20%) missing \"Age\", and a handful missing \"Embarked\" and \"Fare\".\n\n6. Features Data Type:\n\n  * Numerical: Age (Continuous), Fare (Continuous), SibSp (Discrete), Parch (Discrete)\n  * Categorical: Survived, Sex, Embarked, Pclass, Ticket, Cabin","836ba9d7":"Admittedly, this is my first time tuning SVC model. So I have no idea if I'm doing this right.\n\nThe results are close between without tuning and tuning wih RandomSearchCV. We'll be using the latter since it gives a higher F1-score.","87e25bba":"## 4) Feature Analysis\n\nWe want to go through each feature to get a sense of the data and form some baseline hypothesis","7e25b1e1":"## 8) Tuning Hyperparameters of our Model\n\nTuning hyperparameters helps to optimise the performance of a model. \n\nHyperparameters are parameters that are external to our model's learning that can potentially helps improve its ability to predict.\n\nWe will be using RandomSearchCV and GridSearchCV for tuning.","33a8c820":"### 10) After thoughts\n\nOur submission has an accuracy score of 0.77033.\n\nInterestingly, our previous submission using RandomForest without doing any feature engineering received a higher score of 0.78947.\n\nSo looks like all our deliberate tinkering has actually made a worse model!\n\nCredits to Nadin Tamer for sharing her very beginner-friendly notebook: https:\/\/www.kaggle.com\/nadintamer\/titanic-survival-predictions-beginner ","7a25ffa4":"### Build data transform pipeline\nWe want to build a function that can transform our data - so that we can use the same function to apply to train, validation, and eventually test set,","7b81eeb4":"## 1) Import libraries","a6890033":"### Name feature\n\nOn first glance, name should not matter in a passenger's survival rate.\n\nHowever, our name data contains titles such as \"Countess, Rev, Don, Master\" should might be a signifer for social status.","c8f1c5b9":"since the majority of people embarked from S, we will fill up missing value with S as well","707c523e":"### Age Feature","1b2066a0":"### Parch feature\nNo. of Parent\/Child\n","f2c0519b":"### Transform on X to get X_tf","f2c88012":"## 7) Evaluation \n\n### Additional Metrics to evaluate different models\n\nWe want to have more metrics in addition to \"accuracy\" to give us better assess performance of each model.\n\nFor now, we will evaluate our 3 most accurate models - Random Forest, SVC, Decision Tree","4e05d963":"## 2) Import train and test dataset","1c6b645a":"### Result:\n\nThere is a whole science in making sense of the different metrics.\n\n\n1)\n\nIn general, if there is **class imbalance**, precision and recall are more important than accuracy.\n\nClass imbalance means e.g. in \"Sex\", if our data has 95% male and 5% female, it makes it difficult for machine learning to predict accurately especially for minority class.\n\nIn our data, due to our small sample size, there are quite a few features that have imbalanced classes, e.g. Pclass, FareGroup, etc.\n\n\n2)\n\nIf false positive predictions are worse than false negatives (depend on the project), aim for higher precision. If false negative predictions are worse than false positives, aim for higher recall.\n\nFor us, we are no preference on false positive or false negative, since we are not predicting if a patient has heart disease, for instance.\n\n\n3)\nIn our case, both Random Forest and SVC are very close in metrics.\n\n* they are equal in F1-score. F1-score is a combination of precision and recall.\n* SVC has higher accuracy.\n\nThus, we will be using SVC for further tuning.","75c17115":"### Splitting data train and validation\n\nIn general, it's good practice to split data into training and validation set **before** applying any feature engineering.\n\nBut since our sample size is small, we decide to split after feature engineering is applied.","c60858c2":"From a quick look, it seems that female titles have higher survival rate. \n\nSince this information is already captured by \"Sex\" feature, we will be dropping title and name.","6ed533db":"\"Sex\" appears to be an important feature. Female has a significantly higher rate of survival.","64924ae0":"### Fare feature\nWe can test if passengers paying higher fare has higher rate of survival.\nWe can also see if it makes sense to group fare into different categories; it is now a continuous variable","b77f1e45":"### Sex Feature\nWe want to see if there is a difference in survival rate between sexes","c48559f8":"There seems to be a difference between those with recorded cabins and those without. We can consider grouping them into boolean of 1 and 0.","42781862":"There's a marked difference between those with 1-2 siblings vs those with more or 0 sibling.\n\nPerhaps when a family has many siblings, someone needed to sacrifice.","9380cfde":"## 6) Modeling\n\nWe're going to try 4 different machine learning models that are used for Classification problem:\n\n1. Logistic Regression\n2. K-Nearest Neighbours Classifier\n3. Random Forest Classifier\n4. Gradient Boosting Classifier\n5. Support Vector Machines\n6. Stochastic Gradient Descent\n7. Decision Tree\n\nWe are using training data (X_train, y_train) for **fitting \/ training** the model \n\nThen, we use validation data (X_val, y_val) to **validate and evaluate** how accuracy our model is.\n* we use default `score` method of each model to get accuracy score\n* in essense, what `score` method does is it predicts on X_val data and produces a y_prediction data. \n* and then, it compares y_prediction (Machine's guess) with y_val (actual answer) to see how accurate is the prediction","4c69336f":"A few things to note:\n1. younger passenger has a higher rate of survival - which makes sense since younger passengers might be prioritised in rescue boat\n2. those with age missing has lower survival rate than other age group except for senior. Perhaps we can put them in a category on their own.","ebf5403e":"### Embarked feature\nEmbarked means where did the passenger come on board - there are 3 categories: Southampton (S), Cherbourg (C), and Queenstown (Q).\n\n","60179881":"### Splitting our data into X and y \n\n* y is our label - \"Survived\"\n* X is our features","b4078bf8":"## 5) Feature Engineering\n\nFeature Engineering is the process of transforming raw data into features that better represent the underlying problem to the predictive model\n\nNow that we have explored our data, we need to prepare our data for analysis:\n1. Decide what features we want to drop \/ add\n2. Decide what features we want to make changes: e.g. grouping of cabin, fare, age \n3. fill missing values for Cabin, Age, Embarked, Fare\n4. change non-numerical values to numerical values","15d129b3":"Higher-fared passengers have significantly higher survival rate","abb5d40b":"## 9) Testing and Submitting\n\nOur final objective is use our best model to predict on test data (with no labels), and upload our prediction to Kaggle to evaluate.\n\nOur model is choice is SVC tuned with RandomSearchCV.","7c31d91f":"### our data\nthe only difference between train and test dataset is that test dataset has **no** \"Survived\" label, since that's what we are trying to predict.","57b2f946":"# Titanic Challenge for kaggle.com\n\nAttempt number 2.\n\nOur previous attempt has a score of 0.78947. We did not do any feature engineering, and simply run RandomForest classifier tuned with RandomSearchCV,\n\nAfter reading around, especially https:\/\/www.kaggle.com\/nadintamer\/titanic-survival-predictions-beginner, I am inspired to try a few ways to improve the features and model.\n\n","e3a954ac":"### SibSp features\nNumber of siblings. Not sure what hypothesis we can form out of this, lets explore!","b019bac7":"## 3) Initial Data Analysis","aee6f23f":"Lets see our transformed data is done correctly.\n\nFrom above, our transformed data has 25 columns (starting from 0).\n\nIt should have:\n\n* 1x \"SibSp\",\n* 1x \"Parch\",\n* 2x \"Sex\", \n* 3x \"Embarked\", \n* 8x \"AgeGroup\", \n* 5x \"FareGroup\", \n* 3x \"Pclass\",\n* 2x \"HasCabin\"","72587ad4":"### Pclass feature\nPclass refers to the passenger class - 1,2,3 with 1 being the highest.","0ac7061f":"#### Cabin feature\n\nCabin has the most missing data. And its data might overlap with \"Pclass\""}}