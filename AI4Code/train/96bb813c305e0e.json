{"cell_type":{"11e4ced5":"code","004790f7":"code","f8aae61e":"code","bf8fa893":"code","7d39e52b":"code","82e0bf10":"code","3157a4da":"code","0d5c1dbc":"code","477b8461":"code","5816e0b8":"code","c4360bfb":"code","ac8aa01d":"code","1094ec92":"code","fc3bb6fb":"code","75497cf4":"code","620c6de2":"code","c95f1b4b":"code","c3f7c80d":"code","92e4a6eb":"code","4cd6c6e1":"code","472caaab":"code","51d62778":"code","9535a504":"code","eb13abcc":"code","140b94cd":"code","52e86f62":"code","65c5a920":"code","441a50c5":"code","f2e359c5":"code","0a7dc89e":"code","3f71a988":"code","943910e7":"code","cb1e6f13":"code","049156ab":"code","7c62adb7":"code","241bffc5":"code","ddcfd617":"code","5402af76":"code","4ee8f258":"code","51e9506f":"code","85adfbae":"code","6da7e80c":"code","ad3e5603":"code","6da7d66f":"code","7aa7f387":"code","09ef039b":"code","34584755":"code","f999a1aa":"code","63a27e35":"code","4e928117":"code","8e691cfc":"code","09d53c41":"code","46689306":"code","0b30874d":"code","30231529":"code","b8038f65":"code","98fc211b":"code","f9e03c45":"code","2aa8f410":"code","996fb5ad":"code","41cf78ab":"code","a011a1f4":"code","50956719":"code","2ab3a0da":"code","e19f7ee4":"code","8a34f25f":"code","81353c52":"code","c54659cc":"code","2dea3e8e":"code","2dc08b8c":"code","9e89210a":"code","c91917cc":"code","c1669c6f":"code","c9e8863d":"code","f78e7489":"code","d9d2437f":"code","17987a3a":"code","30bdc1e0":"code","7915284f":"code","9bbd7b8b":"code","9142a93a":"code","dded5b2b":"code","1d82aec5":"code","740314b2":"code","603b505e":"code","82dcae85":"code","0257b920":"code","6649a57b":"code","42502c00":"markdown","550d6c28":"markdown","1834e5c2":"markdown","08210f98":"markdown","6db88c9a":"markdown","a9107c26":"markdown","15f92656":"markdown","fc2d252b":"markdown","e0a529fb":"markdown","20626aed":"markdown","54cd723c":"markdown","94339e4a":"markdown","aaf511ed":"markdown","acc25858":"markdown","fb8dbc6c":"markdown","d23e587a":"markdown","cad80b86":"markdown","5d461c5b":"markdown","30b5c297":"markdown","2dd2f485":"markdown","2ebbe27b":"markdown","805a251b":"markdown","84c71872":"markdown","9859a30c":"markdown","3b1d639a":"markdown","25eb88ca":"markdown","3b7c5b28":"markdown","b6bc21ec":"markdown","3f3baddd":"markdown","273e1357":"markdown","240073de":"markdown","c44965b6":"markdown","dd4f756d":"markdown","ff4d5895":"markdown","c6e093ee":"markdown","0553ab00":"markdown","261b3cf4":"markdown","aa6ed118":"markdown"},"source":{"11e4ced5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","004790f7":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt","f8aae61e":"df = pd.read_csv(\"\/kaggle\/input\/heart-disease-uci\/heart.csv\")","bf8fa893":"df.head()","7d39e52b":"pd.set_option(\"display.float\", \"{:.2f}\".format)\ndf.describe()","82e0bf10":"df.shape","3157a4da":"df.isnull().sum()","0d5c1dbc":"df[df.ca == 4]","477b8461":"df[df.thal == 0]","5816e0b8":"df.duplicated(subset=None, keep=\"first\").sum()","c4360bfb":"#df.drop_duplicates(subset =None, keep = 'first', inplace = True)  ","ac8aa01d":"#df = df[df.ca != 4]","1094ec92":"#df = df[df.thal != 0]","fc3bb6fb":"#replace false value with most appropriate value\ndf['ca'] = df['ca'].replace([4],3)","75497cf4":"df[df.ca == 4]","620c6de2":"#replace false value with most appropriate value\ndf['thal'] = df['thal'].replace([0],7) ","c95f1b4b":"df[df.thal == 0]","c3f7c80d":"df.shape","92e4a6eb":"df.describe()","4cd6c6e1":"import pandas_profiling","472caaab":"profile = pandas_profiling.ProfileReport(df)\nprofile","51d62778":"#X= df.drop(['target'], axis=1)\n#y= df['target']\nx = df.iloc[:,0:13]\ny = df.iloc[:,-1]","9535a504":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif","eb13abcc":"BestFeature = SelectKBest(score_func=f_classif, k=10)\nfit = BestFeature.fit(x,y)","140b94cd":"dfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(x.columns)","52e86f62":"featureScores= pd.concat([dfcolumns,dfscores], axis=1)\nfeatureScores.columns = ['Column','Score']","65c5a920":"featureScores","441a50c5":"print(featureScores.nlargest(13,'Score'))","f2e359c5":"featureScores.nlargest(10,'Score').plot(kind='bar',figsize=(15,6),color=sns.color_palette(\"husl\"))\nplt.xlabel('Columns')\nplt.ylabel('Score')\nplt.title('Feature Score')\nplt.show()","0a7dc89e":"from sklearn.feature_selection import chi2","3f71a988":"BestFeature = SelectKBest(score_func=chi2, k=13)\nfit = BestFeature.fit(x,y)","943910e7":"dfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(x.columns)","cb1e6f13":"featureScores= pd.concat([dfcolumns,dfscores], axis=1)\nfeatureScores.columns = ['Column','Score']","049156ab":"featureScores","7c62adb7":"print(featureScores.nlargest(13,'Score'))","241bffc5":"from sklearn.feature_selection import mutual_info_classif","ddcfd617":"BestFeature = SelectKBest(score_func=mutual_info_classif, k=13)\nfit = BestFeature.fit(x,y)","5402af76":"dfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(x.columns)","4ee8f258":"featureScores= pd.concat([dfcolumns,dfscores], axis=1)\nfeatureScores.columns = ['Column','Score']","51e9506f":"featureScores","85adfbae":"print(featureScores.nlargest(13,'Score'))","6da7e80c":"# from sklearn.feature_selection import RFE\n# from sklearn.linear_model import LogisticRegression","ad3e5603":"# model = LogisticRegression(solver='lbfgs')\n# rfe = RFE(model, 3)\n# fit = rfe.fit(x, y)\n# print(\"Num Features: %s\" % (fit.n_features_))\n# print(\"Selected Features: %s\" % (fit.support_))\n# print(\"Feature Ranking: %s\" % (fit.ranking_))","6da7d66f":"# # decision tree for feature importance on a regression problem\n# from sklearn.datasets import make_regression\n# from sklearn.tree import DecisionTreeRegressor\n# from matplotlib import pyplot\n# # define dataset\n# X, y = make_regression(n_samples=303, n_features=13, n_informative=13, random_state=12)\n# # define the model\n# model = DecisionTreeRegressor()\n# # fit the model\n# model.fit(X, y)\n# # get importance\n# importance = model.feature_importances_\n# # summarize feature importance\n# for i,v in enumerate(importance):\n# \tprint('Feature: %0d, Score: %.5f' % (i,v))\n# # plot feature importance\n# pyplot.bar([x for x in range(len(importance))], importance)\n# pyplot.show()","7aa7f387":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nStandardScaler = StandardScaler()  \ncolumns_to_scale = ['age','sex','cp','trestbps','chol','fbs','restecg','thalach','exang','oldpeak','slope','ca','thal'] \ndf[columns_to_scale] = StandardScaler.fit_transform(df[columns_to_scale])","09ef039b":"df.head()","34584755":"#from sklearn.preprocessing import MinMaxScaler\n#scaler = MinMaxScaler()\n\n#columns_to_scale = ['age','sex','cp','thalach','exang','oldpeak','slope','ca','thal','trestbps','chol','fbs','restecg']\n#df[columns_to_scale] = scaler.fit_transform(df[columns_to_scale])","f999a1aa":"# df.head()","63a27e35":"X= df.drop(['target'], axis=1) #,'trestbps','chol','fbs','restecg'\ny= df['target']","4e928117":"#devide Dataset into test and train\nX_train, X_test,y_train, y_test=train_test_split(X,y,test_size=0.3,random_state=40)","8e691cfc":"#check those dataset\nprint('X_train-', X_train.size)\nprint('X_test-',X_test.size)\nprint('y_train-', y_train.size)\nprint('y_test-', y_test.size)","09d53c41":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(random_state=999, C =  1, tol = 5)\nmodel1 = lr.fit(X_train,y_train)\nprediction1 = model1.predict(X_test)\n\ny_pred_quant1 = model1.predict_proba(X_test)[:, 1]","46689306":"from sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_test,prediction1)\ncm","0b30874d":"plt.subplots(figsize=(5, 4))\nsns.heatmap(cm, annot=True, cmap=\"BuPu\")\n","30231529":"#Accuricy using matrics\nTP=cm[0][0]\nTN=cm[1][1]\nFN=cm[1][0]\nFP=cm[0][1]\nprint('Testing Accuracy:',(TP+TN)\/(TP+TN+FN+FP))","b8038f65":"from sklearn.metrics import accuracy_score\n\naccuracies = {}\n\nacc = accuracy_score(y_test,prediction1)*100\naccuracies['Logistic Regration'] = acc\nacc","98fc211b":"from sklearn.metrics import classification_report\nprint(classification_report(y_test,prediction1))","f9e03c45":"from sklearn.tree import DecisionTreeClassifier\n\ndtc = DecisionTreeClassifier(random_state=999)\nmodel2 = dtc.fit(X_train,y_train)\nprediction2 = model2.predict(X_test)\n\ny_pred_quant2 = model2.predict_proba(X_test)[:, 1]\n\n# probs = model.predict_proba(X_test)\n# probs = probs[:, 1]\n# loss = log_loss(testy, probs)\n# loss","2aa8f410":"cm2 = confusion_matrix(y_test,prediction2)\ncm2","996fb5ad":"acc = accuracy_score(y_test,prediction2)*100\naccuracies['Dicision Tree'] = acc\nacc","41cf78ab":"print(classification_report(y_test,prediction2))","a011a1f4":"from sklearn.ensemble import RandomForestClassifier\n\nrfc=RandomForestClassifier(random_state=20)\nmodel3 = rfc.fit(X_train, y_train)\nprediction3 = model3.predict(X_test)\n\ny_pred_quant3 = model3.predict_proba(X_test)[:, 1]\n\ncm3 = confusion_matrix(y_test, prediction3)\ncm3","50956719":"acc = accuracy_score(y_test, prediction3)*100\naccuracies['Random Forest'] = acc\nacc","2ab3a0da":"print(classification_report(y_test, prediction3))","e19f7ee4":"from sklearn.svm import SVC\n\nsvm=SVC(probability=True, C = 0.1)\nmodel4=svm.fit(X_train,y_train)\nprediction4=model4.predict(X_test)\n\ny_pred_quant4 = model4.predict_proba(X_test)[:, 1]\n\ncm4= confusion_matrix(y_test,prediction4)\ncm4","8a34f25f":"acc = accuracy_score(y_test, prediction4)*100\naccuracies['Support Vector Machine'] = acc\nacc","81353c52":"print(classification_report(y_test, prediction4))","c54659cc":"from sklearn.naive_bayes import GaussianNB\n\nNB = GaussianNB()\nmodel5 = NB.fit(X_train, y_train)\nprediction5 = model5.predict(X_test)\n\ny_pred_quant5 = model5.predict_proba(X_test)[:, 1]\n\ncm5= confusion_matrix(y_test, prediction5)\ncm5","2dea3e8e":"acc = accuracy_score(y_test, prediction5)*100\naccuracies['Gaussion NB'] = acc\nacc","2dc08b8c":"print(classification_report(y_test, prediction5))","9e89210a":"from sklearn.neighbors import KNeighborsClassifier\n\nKNN = KNeighborsClassifier(n_neighbors = 5)\nmodel6 = KNN.fit(X_train, y_train)\nprediction6 = model6.predict(X_test)\n\ny_pred_quant6 = model6.predict_proba(X_test)[:, 1]\n\ncm6= confusion_matrix(y_test, prediction6)\ncm6","c91917cc":"acc = accuracy_score(y_test, prediction6)*100\naccuracies['K nearest neighbor'] = acc\nacc","c1669c6f":"print(classification_report(y_test, prediction6))","c9e8863d":"print('Logistic Regration - lr :', accuracy_score(y_test, prediction1) * 100)\nprint('Dicission Tree - dtc :', accuracy_score(y_test, prediction2) * 100)\nprint('Random Forrest - rfc :', accuracy_score(y_test, prediction3) * 100)\nprint('Support vector Machine - svm: ', accuracy_score(y_test, prediction4) * 100)\nprint('Gaussion NB - NB :', accuracy_score(y_test, prediction5) * 100)\nprint('K Nearest - KNN :', accuracy_score(y_test, prediction6) * 100)","f78e7489":"colors = [\"purple\", \"green\", \"orange\", \"magenta\",\"#CFC60E\",\"#0FBBAE\"]\n\nsns.set_style(\"whitegrid\")\nplt.figure(figsize=(16,8))\nplt.yticks(np.arange(0,100,10))\nplt.ylabel(\"Accuracy %\")\nplt.xlabel(\"Algorithms\")\nplt.title(\"Accuracy of Different Classifier\")\nsns.barplot(x=list(accuracies.keys()), y=list(accuracies.values()), palette=colors)\nplt.show()","d9d2437f":"plt.figure(figsize=(24,12))\n\nplt.suptitle(\"Confusion Matrixes\",fontsize=24)\nplt.subplots_adjust(wspace = 0.4, hspace= 0.4)\n\nplt.subplot(2,3,1)\nplt.title(\"Logistic Regression Confusion Matrix\")\nsns.heatmap(cm,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n\nplt.subplot(2,3,5)\nplt.title(\"Decision Tree Classifier Confusion Matrix\")\nsns.heatmap(cm2,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n\nplt.subplot(2,3,6)\nplt.title(\"Random Forest Confusion Matrix\")\nsns.heatmap(cm3,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n\nplt.subplot(2,3,3)\nplt.title(\"Support Vector Machine Confusion Matrix\")\nsns.heatmap(cm4,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n\nplt.subplot(2,3,4)\nplt.title(\"Naive Bayes Confusion Matrix\")\nsns.heatmap(cm5,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n\nplt.subplot(2,3,2)\nplt.title(\"K Nearest Neighbors Confusion Matrix\")\nsns.heatmap(cm6,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n\n\n\nplt.show()","17987a3a":"sensitivitys = {}\n\nsensitivity = (cm[0,0]\/(cm[0,0]+cm[1,0]))*100\nprint('Sensitivity of Logistic Regression : ', sensitivity )\nsensitivitys['Logistic Regration'] = sensitivity\n\nsensitivity = (cm2[0,0]\/(cm2[0,0]+cm2[1,0]))*100\nprint('Sensitivity of Decision Tree Classifier : ', sensitivity )\nsensitivitys['Decision Tree Classifier'] = sensitivity\n\nsensitivity = (cm3[0,0]\/(cm3[0,0]+cm3[1,0]))*100\nprint('Sensitivity of Random Forest : ', sensitivity )\nsensitivitys['Random Forest'] = sensitivity\n\nsensitivity = (cm4[0,0]\/(cm4[0,0]+cm4[1,0]))*100\nprint('Sensitivity of Support Vector Machine: ', sensitivity )\nsensitivitys['Support Vector Machine'] = sensitivity\n\nsensitivity = (cm5[0,0]\/(cm5[0,0]+cm5[1,0]))*100\nprint('Sensitivity of Naive Bayes : ', sensitivity )\nsensitivitys['Naive Bayes'] = sensitivity\n\nsensitivity = (cm6[0,0]\/(cm6[0,0]+cm6[1,0]))*100\nprint('Sensitivity of K Nearest Neighbors : ', sensitivity )\nsensitivitys['K Nearest Neighbors'] = sensitivity","30bdc1e0":"colors = [\"purple\", \"green\", \"orange\", \"magenta\",\"#CFC60E\",\"#0FBBAE\"]\n\nsns.set_style(\"whitegrid\")\nplt.figure(figsize=(16,8))\nplt.yticks(np.arange(0,100,10))\nplt.ylabel(\"Sensitivity %\", fontsize = 15)\nplt.xlabel(\"Algorithms\", fontsize = 15)\nplt.title(\"Sensitivity of Different Classifier\")\nsns.barplot(x=list(sensitivitys.keys()), y=list(sensitivitys.values()), palette=colors)\n\nplt.show()","7915284f":"specificitys = {}\n\nspecificity = (cm[1,1]\/(cm[1,1]+cm[0,1]))*100\nprint('Specificity of Logistic Regression : ', specificity)\nspecificitys['Logistic Regression'] = specificity\n\nspecificity = (cm2[1,1]\/(cm2[1,1]+cm2[0,1]))*100\nprint('specificity of Decision Tree Classifier : ', specificity )\nspecificitys['Decision Tree'] = specificity\n\nspecificity = (cm3[1,1]\/(cm3[1,1]+cm3[0,1]))*100\nprint('specificity of Random Forest : ', specificity )\nspecificitys['Random Forest'] = specificity\n\nspecificity = (cm4[1,1]\/(cm4[1,1]+cm4[0,1]))*100\nprint('specificity of Support Vector Machine: ', specificity )\nspecificitys['Support Vector Machine'] = specificity\n\nspecificity = (cm5[1,1]\/(cm5[1,1]+cm5[0,1]))*100\nprint('specificity of Naive Bayes : ', specificity )\nspecificitys['Naive Bayes'] = specificity\n\nspecificity = (cm6[1,1]\/(cm6[1,1]+cm6[0,1]))*100\nprint('specificity of K Nearest Neighbors : ', specificity )\nspecificitys['K Nearest Neighbors'] = specificity","9bbd7b8b":"colors = [\"purple\", \"green\", \"orange\", \"magenta\",\"#CFC60E\",\"#0FBBAE\"]\n\nsns.set_style(\"whitegrid\")\nplt.figure(figsize=(16,8))\nplt.yticks(np.arange(0,100,10))\nplt.ylabel(\"specificity %\", fontsize = 15)\nplt.xlabel(\"Algorithms\", fontsize = 15)\nplt.title(\"specificity of Different Classifier\")\nsns.barplot(x=list(specificitys.keys()), y=list(specificitys.values()), palette=colors)\nplt.show()","9142a93a":"from sklearn.metrics import roc_curve, roc_auc_score, auc","dded5b2b":"fpr, tpr, thresholds = roc_curve(y_test, y_pred_quant1)\n\nfig, ax = plt.subplots()\nax.plot(fpr, tpr)\nax.plot([0, 1], [0, 1], transform=ax.transAxes, ls=\"--\", c=\".3\")\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.rcParams['font.size'] = 12\nplt.title('ROC curve analysis')\nplt.xlabel('False Positive Rate (1 - Specificity)')\nplt.ylabel('True Positive Rate (Sensitivity)')\nplt.grid(True)","1d82aec5":"ROC_Curve = {}\n\n# Instantiate the classifiers and make a list\nclassifiers = [LogisticRegression(),\n               DecisionTreeClassifier(),\n               RandomForestClassifier(),\n               SVC(probability=True),\n               GaussianNB(),\n               KNeighborsClassifier()]\n\n# Define a result table as a DataFrame\nresult_table = pd.DataFrame(columns=['classifiers', 'fpr', 'tpr', 'auc'])\n\n# Train the models and record the results\nfor cls in classifiers:\n    model = cls.fit(X_train, y_train)\n    yproba = model.predict_proba(X_test)[::, 1]\n\n    fpr, tpr, _ = roc_curve(y_test, yproba)\n    auc = roc_auc_score(y_test, yproba)\n    \n    print(\"AUC of\", cls ,\" is : \", auc * 100)\n    ROC_Curve[cls] = auc * 100\n    \n    result_table = result_table.append({'classifiers': cls.__class__.__name__,\n                                        'fpr': fpr,\n                                        'tpr': tpr,\n                                        'auc': auc}, ignore_index=True)\n    \n\n# Set name of the classifiers as index labels\nresult_table.set_index('classifiers', inplace=True)\n\n\nfig = plt.figure(figsize=(8, 6))\n\nfor i in result_table.index:\n    plt.plot(result_table.loc[i]['fpr'],\n             result_table.loc[i]['tpr'],\n             label=\"{}, AUC={:.3f}\".format(i, result_table.loc[i]['auc']))\n\nplt.plot([0, 1], [0, 1], color='orange', linestyle='--')\n\nplt.xticks(np.arange(0.0, 1.1, step=0.1))\nplt.xlabel(\"False Positive Rate\", fontsize=11, fontname = 'Times New Roman')\n\nplt.yticks(np.arange(0.0, 1.1, step=0.1))\nplt.ylabel(\"True Positive Rate\", fontsize=11, fontname = 'Times New Roman')\n\n# plt.title('ROC Curve Analysis', fontweight='bold', fontsize=11, fontname = 'Times New Roman')\nplt.legend(prop={'size': 13}, loc='lower right')\n\nplt.savefig('ROC Curve Sf1.pdf')  \nplt.show()","740314b2":"colors = [\"purple\", \"green\", \"orange\", \"magenta\",\"#CFC60E\",\"#0FBBAE\"]\n\nsns.set_style(\"whitegrid\")\nplt.figure(figsize=(16,8))\nplt.yticks(np.arange(0,100,10))\nplt.ylabel(\"Accuracy % \", fontsize = 16)\nplt.xlabel(\"Algorithms\", fontsize = 16)\nplt.title(\"ROC Curve of Different Classifier\")\nsns.barplot(x=list(ROC_Curve.keys()), y=list(ROC_Curve.values()), palette=colors)\n\nplt.show()","603b505e":"from sklearn.metrics import log_loss\n\nlogLoss = {}\n\nloss = log_loss(y_test, y_pred_quant1)\nprint('Loss of Logistic Regression : ', loss)\nlogLoss['Logistic Regression'] = loss\n\nloss = log_loss(y_test, y_pred_quant2)\nprint('Loss of Decision Tree Classifier : ', loss )\nlogLoss['Decision Tree'] = loss\n\nloss = log_loss(y_test, y_pred_quant3)\nprint('Loss of Random Forest : ', loss )\nlogLoss['Random Forest'] = loss\n\nloss = log_loss(y_test, y_pred_quant4)\nprint('Loss of Support Vector Machine: ', loss )\nlogLoss['Support Vector Machine'] = loss\n\nloss = log_loss(y_test, y_pred_quant5)\nprint('Loss of Naive Bayes : ', loss )\nlogLoss['Naive Bayes'] = loss\n\nloss = log_loss(y_test, y_pred_quant6)\nprint('Loss of K Nearest Neighbors : ', loss )\nlogLoss['K Nearest Neighbors'] = loss","82dcae85":"colors = [\"purple\", \"green\", \"orange\", \"magenta\",\"#CFC60E\",\"#0FBBAE\"]\n\nsns.set_style(\"whitegrid\")\nplt.figure(figsize=(16,8))\nplt.yticks(np.arange(0,100,10))\nplt.ylabel(\"Accuracy % \", fontsize = 15)\nplt.xlabel(\"Algorithms\", fontsize = 15)\nplt.title(\"Log Loss of Different Classifier\")\nsns.barplot(x=list(logLoss.keys()), y=list(logLoss.values()), palette=colors)\nplt.show()","0257b920":"from sklearn.metrics import cohen_kappa_score","6649a57b":"#cohen_score = cohen_kappa_score(y_test, y_pred_quant6,  labels=None, weights=None)\n#cohen_score","42502c00":"And also a duplicate row, It also should be removed. \n\n### Date Cleaning","550d6c28":"# **Please Upvote If it helps you**","1834e5c2":"# ROC curve","08210f98":"# Random Forest","6db88c9a":"# Dicision Tree","a9107c26":"# Gaussion NB","15f92656":"# Support Vector machine","fc2d252b":"# Data Description\n\n **1. age**: The person's age in years\n\n **2. sex**: The person's sex (1 = male, 0 = female)\n\n **3. cp**: Chest pain type \n            * 0: Typical angina: chest pain related decrease blood supply to the heart\n            * 1: Atypical angina: chest pain not related to heart\n            * 2: Non-anginal pain: typically esophageal spasms (non heart related)\n            * 3: Asymptomatic: chest pain not showing signs of disease\n            \n**4. trestbps**: resting blood pressure (in mm Hg on admission to the hospital)\n\n**5. chol**: The person's cholesterol measurement in mg\/dl\n\n**6. fbs**: the perdon's fasting blood sugar > 120 mg\/dl. (1 = true; 0 = false)\n\n**7. restecg**: Resting electrocardiographic results\n\n     0: Nothing to note\n     1: ST-T Wave abnormality\n        * can range from mild symptoms to severe problems\n        * signals non-normal heart beat\n     2: Possible or definite left ventricular hypertrophy\n        * Enlarged heart's main pumping chamber\n        \n**8. thalach**: Maximum heart rate achieved\n\n**9. exang**: Exercise induced angina (1 = yes; 0 = no)\n\n**10. oldpeak**: - ST depression induced by exercise relative to rest looks at stress of heart during excercise unhealthy heart will stress more\n\n**11. slope**: - the slope of the peak exercise ST segment\n    * 0: Upsloping: better heart rate with excercise (uncommon)\n    * 1: Flatsloping: minimal change (typical healthy heart)\n    * 2: Downslopins: signs of unhealthy heart\n    \n**12. ca**: Number of major vessels (0-3) colored by flourosopy\n    * colored vessel means the doctor can see the blood passing through\n    * the more blood movement the better (no clots)\n    \n**13. thal**: Thalium stress result\n    * 1,3: normal\n    * 6: fixed defect: used to be defect but ok now\n    * 7: reversable defect: no proper blood movement when excercising\n    \n**14. target**: Person have disease or not (1=yes, 0=no) (= the predicted attribute)","e0a529fb":"# Sensitivity","20626aed":"# Comparing Model","54cd723c":"# Logistic regration","94339e4a":"### mutual_info_classif method","aaf511ed":"Diagnostic tests are often sold, marketed, cited and used with sensitivity and specificity as the headline metrics. Sensitivity and specificity are defined as","acc25858":"# K nearest neighbor","fb8dbc6c":"Sensitivity = TruePositives \/ True Positives + False Negatives \n\nSpecificity = TrueNegatives \/ True Negatives + False Positives","d23e587a":"# Scalling The Data\nNormalize the data into 0 and 1\n\n\n### Standard Scalar","cad80b86":"also data #49 and 282 have thal = 0, also incorrect.","5d461c5b":"### Dataset problems\n\nThanks to the post of InitPic we noted that this dataset is a bit different from the original one while the description is the same.\n\nPart of these differences is that there were a few null values in the original dataset that have taken some values here:\n\nA few more things to consider:\n\nIn this Dataset data #93, 159, 164, 165 and 252 have ca=4 which is incorrect. In the original Cleveland dataset they are NaNs (so they should be removed)\n\nalso data #49 and 282 have thal = 0, also incorrect. They are also NaNs in the original dataset.\n\nBecause these are just a few instances, I decided to drop them.","30b5c297":"Details of confusion matricx\n![confusionMatrxiUpdated.jpg](attachment:confusionMatrxiUpdated.jpg)","2dd2f485":"Here top 3 freature is 'cp', 'oldpeak' and 'ca' ","2ebbe27b":"# Log loss\n\nLog loss, also called \u201clogistic loss,\u201d \u201clogarithmic loss,\u201d or \u201ccross entropy\u201d can be used as a measure for evaluating predicted probabilities.\n\nEach predicted probability is compared to the actual class output value (0 or 1) and a score is calculated that penalizes the probability based on the distance from the expected value. The penalty is logarithmic, offering a small score for small differences (0.1 or 0.2) and enormous score for a large difference (0.9 or 1.0).\n\nA model with perfect skill has a log loss score of 0.0.\n\nIn order to summarize the skill of a model using log loss, the log loss is calculated for each predicted probability, and the average loss is reported.","805a251b":"# Kappa Statistics\n\nCohen\u2019s kappa: a statistic that measures inter-annotator agreement.\n\nThis function computes Cohen\u2019s kappa [1], a score that expresses the level of agreement between two annotators on a classification problem. It is defined as\n\nwhere  is the empirical probability of agreement on the label assigned to any sample (the observed agreement ratio), and  is the expected agreement when both annotators assign labels randomly.  is estimated using a per-annotator empirical prior over the class labels [2].\n\nClassification metrics can't handle a mix of binary and continuous targets -- \n\nMeans it only works in regrassion in classification it is worthless","84c71872":"### Univariate selection","9859a30c":"In this Dataset data #93, 159, 164, 165 and 252 have ca=4 which is incorrect.","3b1d639a":"Another common metric is the Area Under the Curve, or AUC. This is a convenient way to capture the performance of a model in a single number, although it's not without certain issues. As a rule of thumb, an AUC can be classed as follows,\n\n - 0.90 - 1.00 = excellent\n - 0.80 - 0.90 = good\n - 0.70 - 0.80 = fair\n - 0.60 - 0.70 = poor\n - 0.50 - 0.60 = fail","25eb88ca":"### chi2 Method","3b7c5b28":"# Feature Selection","b6bc21ec":"# Specificity","3f3baddd":"### Min Max scallar","273e1357":"### Recursive Feature Elimination\nThe Recursive Feature Elimination (or RFE) works by recursively removing attributes and building a model on those attributes that remain.\n\nIt uses the model accuracy to identify which attributes (and combination of attributes) contribute the most to predicting the target attribute.","240073de":"# Load the data","c44965b6":"# ** Heart Disease EDA and correlation visualization Notebook: ** [https:\/\/www.kaggle.com\/niloyswe\/heart-disease-eda-correlation-visualization](http:\/\/)\n\n# **Please Upvote If it helps you**","dd4f756d":"# Heart disease analysis Using different model","ff4d5895":"Univariate feature selection examines each feature individually to determine the strength of the relationship of the feature with the response variable.","c6e093ee":"#  Using all feature","0553ab00":"### Feature Importance\nBagged decision trees like Random Forest and Extra Trees can be used to estimate the importance of features.","261b3cf4":"# Data Profiling","aa6ed118":"In the dataset we have some patient clinical report, We have to predict they have heart disease or not?"}}