{"cell_type":{"7be65096":"code","e8a5d43a":"code","a84d5393":"code","a94f58dc":"code","58c2edc9":"code","ebccf1d1":"code","eaf16da0":"code","d70db677":"code","9b8b04b0":"code","dbffc944":"code","a8956cf7":"code","6337b5ee":"code","92234c31":"code","3fdaea94":"code","b431036a":"code","d09391fa":"code","9555c21b":"code","0e412aef":"code","47c7e6bf":"code","1304ca6a":"code","0424b874":"code","45e98dfc":"code","6f6928c4":"code","1d0fe1ed":"code","a6b046f9":"code","91226308":"code","9d067432":"code","540e23a9":"code","a87409df":"code","25c0ece6":"code","a69396d2":"code","ae7ca1e1":"code","9b62fcf4":"code","55f167ab":"code","0f931f72":"code","dd0b118a":"code","8cc742df":"code","42a710be":"code","971cdd7b":"markdown","2e543aa7":"markdown","c0f0b26c":"markdown","f1503fee":"markdown","cc88f622":"markdown","d26bff15":"markdown","76b563c4":"markdown","4cbd3d25":"markdown","a6a09221":"markdown","1d1af5b2":"markdown"},"source":{"7be65096":"import pandas as pd\nimport numpy as np","e8a5d43a":"df = pd.read_csv(\"..\/input\/iris\/Iris.csv\")\ndf.head()","a84d5393":"#separate features and target variable\nx = df.drop(['Id','Species'],axis = 1)\ny = df['Species']","a94f58dc":"#create train and test set\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.3,random_state = 50)\n#x_train, x_test, y_train,y_test","58c2edc9":"from sklearn.naive_bayes import GaussianNB\nnaive_bayes = GaussianNB()","ebccf1d1":"naive_bayes.fit(x_train,y_train)\npredictions = naive_bayes.predict(x_test)","eaf16da0":"#calculating accuracy\nfrom sklearn.metrics import accuracy_score\naccuracy_score(y_test,predictions)","d70db677":"tweets_df = pd.read_csv(\"..\/input\/tweets\/tweets.csv\")\ntweets_df.head()","9b8b04b0":"tweets_df.info()","dbffc944":"x = tweets_df['tweet']\ny = tweets_df['label']\n#x,y","a8956cf7":"#create train and test set\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.3,random_state = 50)\n#X_train, x_test, y_train,y_test","6337b5ee":"#creating bag-of-words\nfrom sklearn.feature_extraction.text import CountVectorizer\ncount_vector = CountVectorizer()","92234c31":"train_data = count_vector.fit_transform(x_train)\ntest_data = count_vector.transform(x_test)","3fdaea94":"from sklearn.naive_bayes import MultinomialNB\nnaive_bayes = MultinomialNB()","b431036a":"naive_bayes.fit(train_data,y_train)\npredictions = naive_bayes.predict(test_data)","d09391fa":"from sklearn.metrics import accuracy_score\naccuracy_score(y_test,predictions)","9555c21b":"tweets_data = pd.read_csv('..\/input\/tweets\/tweets.csv')\ntweets_data.head()","0e412aef":"#separate features and target variable\nx = tweets_data['tweet']\ny = tweets_data['label']","47c7e6bf":"#create train and test sets\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=42)","1304ca6a":"from sklearn.feature_extraction.text import CountVectorizer","0424b874":"# Generating Binary Features using countvectorizer\ncount_vector = CountVectorizer(stop_words = 'english', binary=True)","45e98dfc":"# Fit the training data \ntraining_data = count_vector.fit_transform(x_train)\n\n# Transform testing data\ntesting_data = count_vector.transform(x_test)","6f6928c4":"from sklearn.naive_bayes import BernoulliNB\nnaive_bayes = BernoulliNB()","1d0fe1ed":"naive_bayes.fit(training_data, y_train)\npredictions = naive_bayes.predict(testing_data)","a6b046f9":"from sklearn.metrics import accuracy_score\naccuracy_score(y_test, predictions)","91226308":"# Make Predictions with Naive Bayes On The Iris Dataset\nfrom csv import reader\nfrom math import sqrt\nfrom math import exp\nfrom math import pi","9d067432":"# Load a CSV file\ndef load_csv(filename):\n    #data = pd.read_csv(filename)\n    #return data\n\tdataset = list()\n\twith open(filename, 'r') as file:\n\t\tcsv_reader = reader(file)\n\t\tfor row in csv_reader:\n\t\t\tif not row:\n\t\t\t\tcontinue\n\t\t\tdataset.append(row)\n\treturn dataset\n","540e23a9":"# Convert string column to float\ndef str_column_to_float(dataset, column):\n\tfor row in dataset:\n\t\trow[column] = float(row[column].strip())","a87409df":"# Convert string column to integer\ndef str_column_to_int(dataset, column):\n\tclass_values = [row[column] for row in dataset]\n\tunique = set(class_values)\n\tlookup = dict()\n\tfor i, value in enumerate(unique):\n\t\tlookup[value] = i\n\t\tprint('[%s] => %d' % (value, i))\n\tfor row in dataset:\n\t\trow[column] = lookup[row[column]]\n\treturn lookup","25c0ece6":"# Split the dataset by class values, returns a dictionary\ndef separate_by_class(dataset):\n\tseparated = dict()\n\tfor i in range(len(dataset)):\n\t\tvector = dataset[i]\n\t\tclass_value = vector[-1]\n\t\tif (class_value not in separated):\n\t\t\tseparated[class_value] = list()\n\t\tseparated[class_value].append(vector)\n\treturn separated","a69396d2":"# Calculate the mean of a list of numbers\ndef mean(numbers):\n\treturn sum(numbers)\/float(len(numbers))","ae7ca1e1":"# Calculate the standard deviation of a list of numbers\ndef stdev(numbers):\n\tavg = mean(numbers)\n\tvariance = sum([(x-avg)**2 for x in numbers]) \/ float(len(numbers)-1)\n\treturn sqrt(variance)","9b62fcf4":"# Calculate the mean, stdev and count for each column in a dataset\ndef summarize_dataset(dataset):\n\tsummaries = [(mean(column), stdev(column), len(column)) for column in zip(*dataset)]\n\tdel(summaries[-1])\n\treturn summaries","55f167ab":"# Split dataset by class then calculate statistics for each row\ndef summarize_by_class(dataset):\n\tseparated = separate_by_class(dataset)\n\tsummaries = dict()\n\tfor class_value, rows in separated.items():\n\t\tsummaries[class_value] = summarize_dataset(rows)\n\treturn summaries","0f931f72":"# Calculate the Gaussian probability distribution function for x\ndef calculate_probability(x, mean, stdev):\n\texponent = exp(-((x-mean)**2 \/ (2 * stdev**2 )))\n\treturn (1 \/ (sqrt(2 * pi) * stdev)) * exponent","dd0b118a":"# Calculate the probabilities of predicting each class for a given row\ndef calculate_class_probabilities(summaries, row):\n\ttotal_rows = sum([summaries[label][0][2] for label in summaries])\n\tprobabilities = dict()\n\tfor class_value, class_summaries in summaries.items():\n\t\tprobabilities[class_value] = summaries[class_value][0][2]\/float(total_rows)\n\t\tfor i in range(len(class_summaries)):\n\t\t\tmean, stdev, _ = class_summaries[i]\n\t\t\tprobabilities[class_value] *= calculate_probability(row[i], mean, stdev)\n\treturn probabilities","8cc742df":"# Predict the class for a given row\ndef predict(summaries, row):\n\tprobabilities = calculate_class_probabilities(summaries, row)\n\tbest_label, best_prob = None, -1\n\tfor class_value, probability in probabilities.items():\n\t\tif best_label is None or probability > best_prob:\n\t\t\tbest_prob = probability\n\t\t\tbest_label = class_value\n\treturn best_label","42a710be":"# Make a prediction with Naive Bayes on Iris Dataset\nfilename = '..\/input\/iris-data-git\/iris.csv'\ndataset = load_csv(filename)\n#print(f'Before converting string column to float: {dataset}')\n\nfor i in range(len(dataset[0])-1):\n\tstr_column_to_float(dataset, i)\n    \n#print(f'After converting string column to float: {dataset}')\n\n# convert class column to integers\nstr_column_to_int(dataset, len(dataset[0])-1)\n\n#print('converting class column to integers',str_column_to_int(dataset, len(dataset[0])-1))\n\n# fit model\nmodel = summarize_by_class(dataset)\n\n# define a new record\nrow = [5.6,3.0,3.9,1.1]\n\n# predict the label\nlabel = predict(model, row)\nprint(f'Data : {row}\\nPredicted value for the given data is : {label}')","971cdd7b":"# Bernoulli Naive bayes","2e543aa7":"# Implement Gaussian Naive bayes","c0f0b26c":"***Gaussian Probability Density Function\nOne way we can do this is to assume that X1 values are drawn from a distribution, such as a bell curve or Gaussian distribution.***\n\n***A Gaussian distribution can be summarized using only two numbers: the mean and the standard deviation. Therefore, with a little math, we can estimate the probability of a given value. This piece of math is called a Gaussian Probability*** \n\n***Distribution Function (or Gaussian PDF) and can be calculated as:***\n\n*f(x) = (1 \/ sqrt(2 * PI) * sigma) * exp(-((x-mean)^2 \/ (2 * sigma^2)))*\n\n***Where sigma is the standard deviation for x, mean is the mean for x and PI is the value of pi.\nBelow is a function that implements this. I tried to split it up to make it more readable.***","f1503fee":"*[Code by machinelearningmastery.com](https:\/\/machinelearningmastery.com\/naive-bayes-classifier-scratch-python\/)*","cc88f622":"# Implementing Multinomial Naive bayes","d26bff15":"***Naive bayes - It works on Bayes theorem of probability to predict the class of unknown data sets.***\n\n***What are the Pros and Cons of Naive Bayes?***\n\n**Pros:**\n\n***It is easy and fast to predict class of test data set. It also perform well in multi class prediction\nWhen assumption of independence holds, a Naive Bayes classifier performs better compare to other models like logistic regression and you need less training data.\nIt perform well in case of categorical input variables compared to numerical variable(s). For numerical variable, normal distribution is assumed (bell curve, which is a strong assumption).***\n\n**Cons:**\n\n***If categorical variable has a category (in test data set), which was not observed in training data set, then model will assign a 0 (zero) probability and will be unable to make a prediction. This is often known as \u201cZero Frequency\u201d. To solve this, we can use the smoothing technique. One of the simplest smoothing techniques is called Laplace estimation.\nOn the other side naive Bayes is also known as a bad estimator, so the probability outputs from predict_proba are not to be taken too seriously.\nAnother limitation of Naive Bayes is the assumption of independent predictors. In real life, it is almost impossible that we get a set of predictors which are completely independent.***","76b563c4":"# Multinomial Naive Bayes","4cbd3d25":"*The probability for a class is calculated as follows:\nP(class|data) = P(X|class) * P(class)*","a6a09221":"# Implementing Bernoulli naive bayes","1d1af5b2":"# Applications of Naive Bayes Algorithms\n**1) Real time Prediction: Naive Bayes is an eager learning classifier and it is sure fast. Thus, it could be used for making predictions in real time.**\n\n**2) Multi class Prediction: This algorithm is also well known for multi class prediction feature. Here we can predict the probability of multiple classes of target variable.**\n\n**3) Text classification\/ Spam Filtering\/ Sentiment Analysis: Naive Bayes classifiers mostly used in text classification (due to better result in multi class problems and independence rule) have higher success rate as compared to other algorithms. As a result, it is widely used in Spam filtering (identify spam e-mail) and Sentiment Analysis (in social media analysis, to identify positive and negative customer sentiments)**\n\n**4) Recommendation System: Naive Bayes Classifier and Collaborative Filtering together builds a Recommendation System that uses machine learning and data mining techniques to filter unseen information and predict whether a user would like a given resource or not**"}}