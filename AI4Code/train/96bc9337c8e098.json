{"cell_type":{"dc641cfc":"code","059a717c":"code","27b61954":"code","fca18bde":"code","821ba45a":"code","4215b647":"code","b5dfe942":"code","de78970c":"code","a27d3ec8":"code","1c89b47d":"code","c8d6d0de":"code","f53bffe9":"code","20df769e":"code","fad5fd6e":"code","e15e441b":"code","3ad83c44":"code","5add6532":"code","2deb6174":"code","b987d73d":"code","7cb0b62f":"code","9019c8e6":"code","13ceacff":"markdown","fc32c875":"markdown","8af87ca6":"markdown","79e81cc3":"markdown","a18dab70":"markdown","6873c0ab":"markdown","1ea0c305":"markdown","ce26a46e":"markdown","ede4ecf8":"markdown","913c2f75":"markdown","e20fda4b":"markdown","f1622f59":"markdown","acc88cfe":"markdown","6fea5083":"markdown","ea6a1249":"markdown","df81dd55":"markdown","72388fe5":"markdown","adcdc818":"markdown","c5e11839":"markdown","33da0b3e":"markdown","85ff2e7a":"markdown","67624803":"markdown"},"source":{"dc641cfc":"import numpy as np\nimport matplotlib.pylab as plt\nfrom scipy.stats import uniform, norm, laplace, beta, bernoulli, expon\n#import pandas as pd\nfrom statsmodels.graphics.tsaplots import plot_acf\nimport warnings \nimport pystan\nimport arviz\nwarnings.filterwarnings('ignore')","059a717c":"ndat = 1000\np = 0.7\nx = bernoulli.rvs(p=p, size=ndat)\n\nprint(f'Number of ones: {x.sum()}.')\nprint(f'Number of zeros: {ndat - x.sum()}')\nprint(f'Estimated probability: {x.sum()\/ndat:.3f}')","27b61954":"model_code = \"\"\"\ndata {\n    int<lower=0> N;                    \/\/ number of data items\n    int<lower=0,upper=1> x[N];         \/\/ data (observations)\n}\n\nparameters {\n    real<lower=0,upper=1> p;           \/\/ estimated variable\n}\n\nmodel {\n    p ~ beta(1, 1);  \/\/ prior for the mean\n    \n    for(n in 1:N) {\n        x[n] ~ bernoulli(p); \/\/ data model\n    }\n}\n\"\"\"\n\nsm = pystan.StanModel(model_code=model_code)","fca18bde":"data = {\n    'x': x,\n    'N': ndat\n}","821ba45a":"fit = sm.sampling(data=data, iter=100000, chains=1, verbose=True)","4215b647":"print(fit)","b5dfe942":"inferencedata = arviz.from_pystan(posterior=fit)","de78970c":"arviz.plot_trace(inferencedata)","a27d3ec8":"trace = fit.extract(pars='p')","1c89b47d":"a = b = 1\na1 = a + x.sum()\nb1 = b + (ndat - x.sum())\n\nxticks = np.linspace(0.68, 0.8, 1000)\nfrom scipy.stats import gaussian_kde\nkde_res = gaussian_kde(trace['p'])\nplt.hist(trace['p'], density=True, color='lightblue')\nplt.plot(xticks, kde_res(xticks))\n\nplt.plot(xticks, beta.pdf(xticks, a1, b1))","c8d6d0de":"y = np.loadtxt('..\/input\/ucuss\/housefly.txt')\nplt.hist(y, density=True)\nplt.title(f'Housefly wing lengths, N={y.size}, mean={y.mean():.2f}, std={y.std():.2f}')\nplt.show()","f53bffe9":"model_code = \"\"\"\ndata {\n    int<lower=0> N;      \/\/ number of data items\n    vector[N] y;         \/\/ data (observations)\n}\n\nparameters {\n    real mu;             \/\/ estimated mean\n    real<lower=0> std;   \/\/ estimated standard dev.\n}\n\nmodel {\n    mu ~ normal(50, 100); \/\/ prior for the mean\n    std ~ gamma(2, 0.1);  \/\/ prior for st. dev.\n    y ~ normal(mu, std);  \/\/ data model\n}\n\"\"\"\n\nsm_housefly = pystan.StanModel(model_code=model_code)","20df769e":"data = {\n    'y': y,\n    'N': y.size\n}\n\nfit_housefly = sm_housefly.sampling(data=data, iter=100000, chains=1, verbose=True)\nprint(fit_housefly)","fad5fd6e":"inferencedata_housefly = arviz.from_pystan(posterior=fit_housefly)\narviz.plot_trace(inferencedata_housefly)","e15e441b":"data = np.load('..\/input\/ucuss\/time-to-event_1.npz')\ny = data['y']\nplt.hist(y, density=True)","3ad83c44":"model_code = \"\"\"\ndata {\n    int<lower=0> N;          \/\/ number of data items\n    vector[N] y;             \/\/ data (observations)\n}\n\nparameters {\n    real<lower=0> lambda;    \/\/ estimated rate parameter\n}\n\nmodel {\n    lambda ~ gamma(0.001, 0.001);  \/\/ prior for lambda\n    y ~ exponential(lambda);       \/\/ data model\n}\n\ngenerated quantities {             \/\/ here we calculate posterior predictive\n    real yhat[N];                  \/\/ this is the predicted observation\n    \n    for (n in 1:N) {\n         yhat[n] = exponential_rng(lambda);    \/\/ we sample it from the rng with lambda est.\n    }\n}\n\"\"\"\n\nsm_tte = pystan.StanModel(model_code=model_code)","5add6532":"data = {\n    'y': y,\n    'N': y.size\n}\n\nfit_tte = sm_tte.sampling(data=data, iter=100000, chains=1, verbose=True)","2deb6174":"print(fit_tte.stansummary(pars='lambda'))","b987d73d":"inferencedata_tte = arviz.from_pystan(posterior=fit_tte, posterior_predictive='yhat', observed_data='y')","7cb0b62f":"arviz.plot_trace(inferencedata_tte)","9019c8e6":"arviz.plot_ppc(inferencedata_tte, data_pairs = {'y': 'yhat'}, alpha=0.9)","13ceacff":"Above we obtained a (frequentist) estimate of $p$, but cannot directly say, how much uncertain it is. For this purpose, we exploit the Bayesian paradigm and set a prior. Although we won't rely on *conjugate priors* in the sequel, we will stick with the beta distribution just for illustration. Recall, that the beta distribution has two positive real parameters $a$ and $b$, that represent the number of successes and failures.\n\nWe have\n\n$$\np \\sim \\mathrm{beta}(a, b).\n$$\n\nIf $a=b=1$, the distribution coincides with the uniform distribution over the interval $[0, 1]$. The posterior is defined by\n\n$$\n\\begin{aligned}\na_1 &= a + \\text{number of successes}, \\\\\nb_1 &= b + \\text{number of failures}.\n\\end{aligned}\n$$\n\nWe will check it later.","fc32c875":"A wise question could be: are the estimates precise? What is our uncertainty about them? We have only a finite and relatively small number of data, after all.\n\nThis is, where the Bayesian approach to modeling becomes handy. First, we define the model and prior distributions:\n- data `y` will follow a normal distribution with unknown mean `mu` and standard deviation `std`\n- the mean `mu` will be estimated using a normal prior distribution\n- the standard deviation `std` will be estimated using a gamma prior\n\nTo summarize:\n$$\n\\begin{aligned}\ny &\\sim \\mathcal{N}(\\mu, \\sigma^2), \\\\\n\\mu &\\sim \\mathcal{N}(\\cdot, \\cdot), \\\\\n\\sigma &\\sim \\mathcal{G}(\\cdot, \\cdot).\n\\end{aligned}\n$$\n\nA natural question is how to decide the prior. There are some guidelines, e.g.\n- is the estimated parameter bounded? E.g., is it strictly positive?\n    - if yes, then we need either a convenient transformation so that it's unbounded, or a bounded prior. A frequent choice is the gamma prior.\n- how solid is our knowledge about the parameter? If I choose a gamma prior, how do I know it is vague enought?\n    - P. Lambert, A. Sutton, P. Burton, K. Abrams, and D. Jones, \u201cHow vague is vague? A simulation study of the impact of the use of vague prior distributions in MCMC using WinBUGS,\u201d Stat. Med., vol. 24, no. 15, pp. 2401\u20132428, Aug. 2005.","8af87ca6":"#### Back to the conjugate prior :)\n\n...and plot the true beta posterior density:","79e81cc3":"Let's look at the summary. If we want to filter out the predictive distributions, we have to limit ourselves a bit...","a18dab70":"### (py)stan\n\nShortly: [stan](https:\/\/mc-stan.org) is a software for statistical modeling. It is incredibly powerful, we will only superficially explore its qualities.\n\nOur first (py)stan code is fairly simple. We need to define three blocks:\n- `data` - properties of data and its type, size, and limits,\n- `parameters` - properties of inferred parameters (in our case the probability). Type, size, and limits.\n- `model` - the probabilistic form of the model, i.e., the data model (here Bernoulli) and the prior distributions (here the beta distribution).\n\nThe definition serves for compilation to C++ which is obvious from the code:","6873c0ab":"Let's look at the result! We see:\n- the number of iterations and chains\n- `thin` and `warmup` will be explained tomorrow\n- the table depicts\n    - `mean` - the mean of the posterior samples\n    - `se_mean` - MC standard error: the estimated standard deviation of a parameter divided by the square root of the number of effective samples (tricky, isn't it? :))\n    - `sd` - standard deviation of the samples\n    - `2.5` and `97.5` percentiles delimiting 95% credibility regions\n    - all three `quartiles`\n    - `n_eff` - number of effective samples - the higher the better\n    - `Rhat` - the Gelman-Rubin diagnostic. A rule of thumb for a good convergence is value smaller than 1.1.\n    \nIf the chain(s) do not converge, `stan` will inform us.","1ea0c305":"...and posterior predictive. The true observations should be well covered.","ce26a46e":"...and plot the trace and the estimate of the posterior distribution. Arviz can produce a lot of nice plots, it is worth to explore a bit :)","ede4ecf8":"## Example 2: Housefly wing lengths\n\nIn this example we aim at estimation of the mean and standard deviation of a normal (Gaussian) distribution. The dataset contains the housefly wing lengths originating from\n\n*Sokal, R.R. and P.E. Hunter. 1955. A morphometric analysis of DDT-resistant and non-resistant housefly strains Ann. Entomol. Soc. Amer. 48: 499-507*\n\nThis set is considered an excelent real example of normally distributed biometric data.\n\nFirst, we will load the data and plot their histogram, including the point estimates of the mean a standard deviation:","913c2f75":"Next, we look at the posterior distribution using the `arviz` package. First, we transform the result into the arviz-friendly form:","e20fda4b":"If we obtain new measurements, we can use the posterior distributions and update them by new data! :)\n\n**Misleading priors:** Try setting a conceptually wrong prior distribution for the mean. E.g., very concentrated on a region far from the (true) mean.","f1622f59":"## Example 3: Time to event modeling (50)\n**(Checking the the posterior predictive)**\n\nIn this example we will study the problem of time-to-event modeling. We have a detector that observes a particle flux. The particles arrive independently at a constant rate. The time between two consecutive detections is described by data in the file. The goal is to estimate the rate parameter.\n\nIt is well known that the waiting times of such processes follow the [exponential distribution](https:\/\/en.wikipedia.org\/wiki\/Exponential_distribution) with the positive real **rate** parameter $\\lambda$:\n\n$$\ny \\sim \\mathrm{Exp}(\\lambda), \\qquad \\lambda>0.\n$$\n\nYour task is to estimate $\\lambda$.","acc88cfe":"Generally, we will proceed in three steps:\n1. We construct a model. That is, we need to decide the distribution of the data.\n2. We choose a convenient prior distribution for inference of unknown model parameters.\n3. We run estimation and analyze results.\n\nTwo additional software packages will be used:\n- *stan* and its python interface *pystan*\n- *arviz* for visualization of the results.","6fea5083":"Well, the task is to fit the parameter $\\lambda$. In addition, we want to calculate the [posterior predictive distribution](https:\/\/en.wikipedia.org\/wiki\/Posterior_predictive_distribution), which allows to more investigate the quality of fit. It is given by\n\n$$\nf(\\tilde{y}_i|y_1,\\ldots,y_N) = \\int f(y_i|\\lambda) \\pi(\\lambda) \\mathrm{d}\\lambda,\n$$\n\ni.e., the posterior predictive is obtained from **averaging over the parameter**.","ea6a1249":"Try to set an inconvenient prior, e.g., uniform on [0, 10], and check.","df81dd55":"Inference is performed by the `sampling()` method. It accepts a bunch of arguments, here we see:\n- reference to the `data` dictionary\n- `iter` stands for the number of MCMC iterations (i.e., samples from the posterior distribution)\n- `chains` is the number of MCMC chains. Will become clear tomorrow\n- `verbose` is clear :)","72388fe5":"Next, we define a pythonic dictionary for the `data` block:","adcdc818":"# Introduction to Bayesian Modeling (Part 1b)","c5e11839":"Now, we're ready for inference on real data:","33da0b3e":"Let's assess the traceplots...","85ff2e7a":"In this session, we will try to perform the Bayesian inference on simple examples. We won't rely on *conjugate* priors, but will explore the possibilities of *MCMC*-based inference. Tomorrow, we will learn more about MCMC.","67624803":"## Example 1: Coin tossing\n\nThe first example introduces the basics of *stan* and *pystan*. The task is:\n1. simulate `ndat` tosses of a coin (pings, detections of something...) using a known probability of head (success, presences of a phenomenon...);\n2. try to infer this (here known) probability from the data.\n\nWe already know that the binary 0-1 data where 1 occurs with probability $p$ follows the Bernoulli distribution\n\n$$\nx \\sim \\mathrm{Bernoulli}(p).\n$$\n\nThe `scipy.stats` package contains the `bernoulli` class, whose `rvs()` method produces (pseudo)random sampling:"}}