{"cell_type":{"5e9272c6":"code","719115f5":"code","f9083ad6":"code","fa8b1485":"code","928e1311":"code","cde88703":"code","a94a69fe":"code","8e919c96":"code","26d55575":"code","0b44ce5f":"code","bb7c119a":"code","14dc45ea":"code","6a061500":"code","0387c990":"code","ee3559c1":"code","05c5186a":"code","67a62e14":"code","5f554337":"code","e72cd3d6":"code","a13b14b6":"code","bd05e3ba":"markdown","eaab73d3":"markdown","6e5ce8c8":"markdown","d7e0b3a1":"markdown","0df819bc":"markdown","05bff02f":"markdown","5626563d":"markdown","8864c41f":"markdown","b8c97fd7":"markdown"},"source":{"5e9272c6":"filename_train = '..\/input\/nlp-getting-started\/train.csv'\nfilename_test = '..\/input\/nlp-getting-started\/test.csv'","719115f5":"import pandas as pd\n\ndf_train = pd.read_csv(filename_train, index_col='id')\ndf_train","f9083ad6":"df_test = pd.read_csv(filename_test, index_col='id')\ndf_test","fa8b1485":"import re, string\n\ndef clean_text(text):\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text\n\ndf_train['text'] = df_train['text'].apply(lambda x: clean_text(x))\ndf_test['text'] = df_test['text'].apply(lambda x: clean_text(x))","928e1311":"from nltk.stem import WordNetLemmatizer\n\nlem = WordNetLemmatizer()\ndf_train['text'] = df_train['text'].apply(lambda x: ' '.join([lem.lemmatize(token) for token in x.split()]))\ndf_test['text'] = df_test['text'].apply(lambda x: ' '.join([lem.lemmatize(token) for token in x.split()]))","cde88703":"from nltk.tokenize import word_tokenize\n\ndef create_corpus(texts):\n    \"\"\"Decompose text to corpus (e.g. `This is a pen` to [ `This`, `is`, `a`, `pen` ])\n    \n    Arguments:\n        texts: list(str) \/ Text list.\n        \n    Returns:\n        list(str) \/ Corpus list.\n    \"\"\"\n    \n    corpus = []\n    for tweet in texts:\n        words = [ word.lower() for word in word_tokenize(tweet) ]\n        corpus.append(words)\n        \n    return corpus","a94a69fe":"corpus_train = create_corpus(df_train['text'])\ncorpus_test = create_corpus(df_test['text'])","8e919c96":"from tensorflow.keras.preprocessing.text import Tokenizer\n\n# Torkenize corpus to integer list\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(corpus_train)\nseq_train = tokenizer.texts_to_sequences(corpus_train)\nseq_test = tokenizer.texts_to_sequences(corpus_test)","26d55575":"word_index = tokenizer.word_index\nprint('Number of unique words:',len(word_index))","0b44ce5f":"from tensorflow.keras.preprocessing.sequence import pad_sequences\n\nnum_words = 50\nX_train = pad_sequences(seq_train, maxlen=num_words)\nX_test = pad_sequences(seq_test, maxlen=num_words)","bb7c119a":"y_train = df_train['target']\ny_train","14dc45ea":"import numpy as np\n\nembedding_dict = {}\nwith open('..\/input\/glove6b\/glove.6B.100d.txt','r') as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        vectors = np.asarray(values[1:], 'float32')\n        embedding_dict[word] = vectors","6a061500":"max_words = len(word_index) + 1\nembedding_dims = 100\n\nembedding_matrix = np.zeros((max_words, embedding_dims))\nfor word, i in word_index.items():\n    if i > max_words:\n        continue\n        \n    emb_vec = embedding_dict.get(word)    \n    if emb_vec is not None:\n        embedding_matrix[i] = emb_vec            ","0387c990":"import tensorflow as tf\n\ninput = tf.keras.layers.Input(shape=(num_words,))\n\nx = input\nx = tf.keras.layers.Embedding(\n    max_words, embedding_dims, \n    embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n    input_length=num_words, trainable=False)(x)\nx = tf.keras.layers.LSTM(64)(x)\nx = tf.keras.layers.Dropout(0.2)(x)\n\noutput = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n\nmodel = tf.keras.models.Model(input, output)\nmodel.summary()","ee3559c1":"model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])","05c5186a":"epochs = 100\nbatch_size = 64\n\ncallbacks = [\n    tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)\n]\n\nmodel.fit(X_train, \n          y_train, \n          epochs=epochs,\n          batch_size=batch_size,\n          validation_split=0.1,\n          callbacks=callbacks)","67a62e14":"y_pred = model.predict(X_test)\ny_pred","5f554337":"y_pred = np.where(y_pred > 0.5, 1, 0).squeeze()\ny_pred = pd.Series(y_pred, name='target').astype(int)\ny_pred","e72cd3d6":"answer = pd.concat([df_test.index.to_series().reset_index(drop=True), y_pred], axis=1)\nanswer","a13b14b6":"filename_output = '.\/submission.csv'\nanswer.to_csv(filename_output, index=False)","bd05e3ba":"# Modeling","eaab73d3":"## Get objective variable","6e5ce8c8":"# Load data","d7e0b3a1":"## Lemmatization","0df819bc":"## Clean text","05bff02f":"## Get explanatory variable","5626563d":"# Prediction","8864c41f":"# Preprocessing","b8c97fd7":"### Get GloVe embedding matrix"}}