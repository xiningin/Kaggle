{"cell_type":{"24b95a6d":"code","b84dfebe":"code","80ffdf58":"code","d6e2b1d9":"code","cca73f0c":"code","c7eebd34":"code","eee31428":"code","58447ef0":"code","9776a04d":"code","864c5619":"code","1040d968":"code","03f72cf7":"code","68552935":"code","d483aa0f":"code","50c1cbb3":"code","2e61e78b":"code","5af67caa":"code","e3d372e1":"code","84d21d27":"code","5a10b1ed":"code","5efff0fb":"code","5bedd333":"code","8f4e16a7":"code","757dbdfc":"code","191f0030":"code","70a6d135":"code","9a3a722f":"code","f33034c7":"code","ad86236f":"code","6f219bd7":"code","e4a0830f":"code","2b37edf5":"code","5e629be9":"code","c1f9c41f":"code","f18a7a55":"code","916b6068":"code","103a3632":"code","8404aa3e":"code","332eabb3":"code","2e4ca488":"code","78e85d76":"code","26104935":"code","ce92c250":"code","cc5f1928":"code","bcd059dd":"code","cffc261b":"code","c0fdd264":"code","098c84ee":"code","536ec5f8":"code","ebbd3e07":"code","85155ed4":"code","0695db57":"code","3f3447c7":"code","7cdd72e7":"code","cc897f44":"code","046818e9":"code","93875340":"code","faaac903":"code","9cc454b4":"code","1e14ef98":"code","437109a3":"code","d42ccb53":"code","fed1e225":"code","862042cb":"code","f36805bf":"code","3ff7340c":"code","cc6e93b4":"code","0c58d376":"code","f703bf4f":"code","35e92928":"code","b3937a04":"code","1efdfb76":"code","5baa6352":"code","d030f68f":"code","eea8c469":"code","01e800ee":"code","422fa598":"code","211ace6d":"code","88c7562a":"code","1363bba8":"code","07c8c0fe":"code","44fd9d3c":"code","c04134f2":"markdown","23cb626a":"markdown","2ba6dca3":"markdown","eb650d3d":"markdown","11e8580d":"markdown","3856393b":"markdown","6dd83f84":"markdown","dc67b7d2":"markdown","21c81993":"markdown","707e1af6":"markdown","087ce79c":"markdown","bbe31067":"markdown","e2912626":"markdown","c8f38219":"markdown","77a81b4b":"markdown","ded50c29":"markdown","7951bb0c":"markdown","b4ce1203":"markdown","7fe4702a":"markdown","03c9772c":"markdown","17eecaee":"markdown","bc3556f9":"markdown","5200d4fb":"markdown","aedf8833":"markdown","d19e30a5":"markdown","ed51ccd1":"markdown","34d54070":"markdown","c59d9fad":"markdown","9d4139ef":"markdown","e5cb3833":"markdown","364fd38b":"markdown","f3d94646":"markdown","0d37ff2b":"markdown","3695c4c9":"markdown","b3f2961b":"markdown","0aeebf92":"markdown","7ab8c0f0":"markdown","e56554b2":"markdown","6e0171a3":"markdown","dc5ca063":"markdown","901aeff9":"markdown","f1213ad2":"markdown","87d39f73":"markdown","a80179b9":"markdown","ba88f0c1":"markdown","ae1afc59":"markdown","473b3838":"markdown","bb7461d6":"markdown","8d92041c":"markdown","cb08a9b8":"markdown","60fc5865":"markdown","87d3fcae":"markdown","fb9d6f73":"markdown","1075693f":"markdown","86fc1833":"markdown","3eab8bcc":"markdown","edccdae9":"markdown","e0e173cd":"markdown","5e3e39ee":"markdown","2cc4be5a":"markdown","8cd9d3db":"markdown","880d8895":"markdown","0f0e4a8b":"markdown","179b7c54":"markdown"},"source":{"24b95a6d":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('seaborn-whitegrid')\n\nfrom collections import Counter\nimport os\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","b84dfebe":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","80ffdf58":"train_df = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntest_passengerId=test_df[\"PassengerId\"]","d6e2b1d9":"train_df.columns","cca73f0c":"train_df.head()","c7eebd34":"train_df.describe()","eee31428":"train_df.info()","58447ef0":"variable = \"Survived\"\ncolumnvalues = train_df[variable]\ncolumncounts = columnvalues.value_counts()\n#plot graph\nplt.figure(figsize=(9,3))\nplt.bar(columncounts.index,columncounts, color=['red', 'green'])\nplt.xticks(columncounts.index,columncounts.index.values)\nplt.ylabel(\"Frequency\")\nplt.title(variable)\nplt.show()\nprint(\"{} \\n {}\".format(\"Survived\",train_df[\"Survived\"].value_counts()))\n","9776a04d":"variable = \"Sex\"\ncolumnvalues = train_df[variable]\ncolumncounts = columnvalues.value_counts()\n#plot graph\nplt.figure(figsize=(9,3))\nplt.bar(columncounts.index,columncounts, color=['blue', 'red'])\nplt.xticks(columncounts.index,columncounts.index.values)\nplt.ylabel(\"Frequency\")\nplt.title(variable)\nplt.show()\nprint(\"{} \\n {}\".format(\"Sex\",train_df[\"Survived\"].value_counts()))","864c5619":"variable = \"Pclass\"\ncolumnvalues = train_df[variable]\ncolumncounts = columnvalues.value_counts()\n#plot graph\nplt.figure(figsize=(9,3))\nplt.bar(columncounts.index,columncounts, color=['red', 'green','blue'])\nplt.xticks(columncounts.index,columncounts.index.values)\nplt.ylabel(\"Frequency\")\nplt.title(variable)\nplt.show()\nprint(\"{} \\n {}\".format(\"Passenger Class\",train_df[\"Pclass\"].value_counts()))","1040d968":"variable = \"Embarked\"\ncolumnvalues = train_df[variable]\ncolumncounts = columnvalues.value_counts()\n#plot graph\nplt.figure(figsize=(9,3))\nplt.bar(columncounts.index,columncounts, color=['red', 'green','blue'])\nplt.xticks(columncounts.index,columncounts.index.values)\nplt.ylabel(\"Frequency\")\nplt.title(variable)\nplt.show()\nprint(\"{} \\n {}\".format(\"Embarked\",train_df[\"Embarked\"].value_counts()))","03f72cf7":"variable = \"SibSp\"\ncolumnvalues = train_df[variable]\ncolumncounts = columnvalues.value_counts()\n#plot graph\nplt.figure(figsize=(9,3))\nplt.bar(columncounts.index,columncounts, color=['red', 'green','blue','yellow','magenta','cyan'])\nplt.xticks(columncounts.index,columncounts.index.values)\nplt.ylabel(\"Frequency\")\nplt.title(variable)\nplt.show()\nprint(\"{} \\n {}\".format(\"Siblings\/Spouses\",train_df[\"SibSp\"].value_counts()))","68552935":"variable = \"Parch\"\ncolumnvalues = train_df[variable]\ncolumncounts = columnvalues.value_counts()\n#plot graph\nplt.figure(figsize=(9,3))\nplt.bar(columncounts.index,columncounts, color=['red', 'green','blue','yellow','magenta','cyan'])\nplt.xticks(columncounts.index,columncounts.index.values)\nplt.ylabel(\"Frequency\")\nplt.title(variable)\nplt.show()\nprint(\"{} \\n {}\".format(\"Parent\/Children\",train_df[\"Parch\"].value_counts()))","d483aa0f":"#Cabin Variable\nprint('{} \\n',format(train_df[\"Cabin\"].value_counts()))","50c1cbb3":"#Name Variable\nprint('{} \\n',format(train_df[\"Name\"].value_counts()))","2e61e78b":"#Name Variable\nprint('{} \\n',format(train_df[\"Ticket\"].value_counts()))","5af67caa":"variable = \"Fare\"\nplt.figure(figsize=(9,3))\nplt.hist(train_df[variable],bins=10)\nplt.xlabel(variable)\nplt.ylabel(\"Frequency\")\nplt.title(\"{} distribution with Histogram\".format(variable))\nplt.show()","e3d372e1":"variable = \"Age\"\nplt.figure(figsize=(9,3))\nplt.hist(train_df[variable])\nplt.xlabel(variable)\nplt.ylabel(\"Frequency\")\nplt.title(\"{} distribution with Histogram\".format(variable))\nplt.show()","84d21d27":"variable = \"PassengerId\"\nplt.figure(figsize=(9,3))\nplt.hist(train_df[variable],bins=891)\nplt.xlabel(variable)\nplt.ylabel(\"Frequency\")\nplt.title(\"{} distribution with Histogram\".format(variable))\nplt.show()","5a10b1ed":"featurelist = [\"SibSp\", \"Parch\", \"Age\", \"Fare\", \"Survived\"]\nsns.heatmap(train_df[featurelist].corr(), annot = True, cmap=\"RdYlGn\")\nplt.show()","5efff0fb":"train_df[[\"Pclass\",\"Survived\"]].groupby([\"Pclass\"], as_index = False).mean().sort_values(by=\"Survived\",ascending = False)","5bedd333":"#Plot Survived Percentage\ng = sns.catplot(x = \"Pclass\", y = \"Survived\", data = train_df, kind = \"bar\", height = 6)\ng.set_ylabels(\"Percent of Survived\")\nplt.show()","8f4e16a7":"train_df[[\"Sex\",\"Survived\"]].groupby([\"Sex\"], as_index = False).mean().sort_values(by=\"Survived\",ascending = False)","757dbdfc":"#print Survived Percentage\ntrain_df[[\"SibSp\",\"Survived\"]].groupby([\"SibSp\"], as_index = False).mean().sort_values(by=\"Survived\",ascending = False)","191f0030":"#Plot Survived Percentage\ng = sns.catplot(x = \"SibSp\", y = \"Survived\", data = train_df, kind = \"bar\", height = 6)\ng.set_ylabels(\"Percent of Survived\")\nplt.show()","70a6d135":"#Print Survived Percentage\ntrain_df[[\"Parch\",\"Survived\"]].groupby([\"Parch\"], as_index = False).mean().sort_values(by=\"Survived\",ascending = False)","9a3a722f":"#Plot Survived Percentage\ng = sns.catplot(x = \"Parch\", y = \"Survived\", data = train_df, kind = \"bar\", height = 6)\ng.set_ylabels(\"Percent of Survived\")\nplt.show()","f33034c7":"g = sns.FacetGrid(train_df, col = \"Survived\")\ng.map(sns.distplot, \"Age\", bins = 25)\nplt.show()","ad86236f":"def find_outliers(dataframe,variables,threshold):\n    outliers = []\n    \n    for c in variables:\n        # First Part\n        Quartile1 = np.percentile(dataframe[c],25)\n        # Last Part\n        Quartile3 = np.percentile(dataframe[c],75)\n        # IQR\n        IQR = Quartile3 - Quartile1\n        outlier_step = IQR * 1.5\n        # Find outlier indexes\n        outlier_list_col = dataframe[(dataframe[c] < Quartile1 - outlier_step) | (dataframe[c] > Quartile3 + outlier_step)].index\n        # store indexes\n        outliers.extend(outlier_list_col)\n    \n    outliers = Counter(outliers)\n    multiple_outliers = list(i for i, v in outliers.items() if v > threshold)\n    \n    return multiple_outliers","6f219bd7":"train_df.loc[find_outliers(train_df,[\"Age\",\"SibSp\",\"Parch\",\"Fare\"],1)]","e4a0830f":"train_df.loc[find_outliers(train_df,[\"Age\",\"SibSp\",\"Parch\",\"Fare\"],2)]","2b37edf5":"# drop 2 threshold outlier records \ntrain_df = train_df.drop(find_outliers(train_df,[\"Age\",\"SibSp\",\"Parch\",\"Fare\"],2),axis = 0).reset_index(drop = True)","5e629be9":"#train dataframe missing value columns\ntrain_df.columns[train_df.isnull().any()]","c1f9c41f":"#missing values count in train data\ntrain_df.isnull().sum()","f18a7a55":"#Find which passengers embarked ports are unknown...\ntrain_df[train_df[\"Embarked\"].isnull()]","916b6068":"#draw graph: Embarked Vs. Fare\ntrain_df.boxplot(column=\"Fare\",by = \"Embarked\")\nplt.show()","103a3632":"#Fill Missing Embarked Value with C\ntrain_df[\"Embarked\"] = train_df[\"Embarked\"].fillna(\"C\")\n# Control filling\ntrain_df[train_df[\"Embarked\"].isnull()]","8404aa3e":"#Find which passengers don't have age data\ntrain_df[train_df[\"Age\"].isnull()]","332eabb3":"sns.heatmap(train_df[[\"Age\",\"Sex\",\"SibSp\",\"Parch\",\"Pclass\"]].corr(), annot = True, cmap=\"RdYlGn\")\nplt.show()","2e4ca488":"index_nan_age = list(train_df[\"Age\"][train_df[\"Age\"].isnull()].index)\nfor i in index_nan_age:\n    age_pred = train_df[\"Age\"][((train_df[\"SibSp\"] == train_df.iloc[i][\"SibSp\"]) &(train_df[\"Parch\"] == train_df.iloc[i][\"Parch\"])& (train_df[\"Pclass\"] == train_df.iloc[i][\"Pclass\"]))].median()\n    age_med = train_df[\"Age\"].median()\n    if not np.isnan(age_pred):\n        train_df[\"Age\"].iloc[i] = age_pred\n    else:\n        train_df[\"Age\"].iloc[i] = age_med","78e85d76":"train_df[train_df[\"Age\"].isnull()]","26104935":"#test dataframe missing value columns\ntest_df.columns[test_df.isnull().any()]","ce92c250":"#missing values count in train data\ntest_df.isnull().sum()","cc5f1928":"#Find which passenger ticket price is Null\ntest_df[test_df[\"Fare\"].isnull()]","bcd059dd":"#Average value of Pclass=3\nnp.mean(test_df[test_df[\"Pclass\"] == 3][\"Fare\"])","cffc261b":"test_df[\"Fare\"] = test_df[\"Fare\"].fillna(np.mean(test_df[test_df[\"Pclass\"] == 3][\"Fare\"]))\ntest_df[test_df[\"Fare\"].isnull()]","c0fdd264":"#Find which passengers don't have age data\ntest_df[test_df[\"Age\"].isnull()]","098c84ee":"sns.heatmap(test_df[[\"Age\",\"Sex\",\"SibSp\",\"Parch\",\"Pclass\"]].corr(), annot = True, cmap=\"RdYlGn\")\nplt.show()","536ec5f8":"index_nan_age = list(test_df[\"Age\"][test_df[\"Age\"].isnull()].index)\nfor i in index_nan_age:\n    age_pred = test_df[\"Age\"][((test_df[\"SibSp\"] == test_df.iloc[i][\"SibSp\"]) &(test_df[\"Parch\"] == test_df.iloc[i][\"Parch\"])& (test_df[\"Pclass\"] == test_df.iloc[i][\"Pclass\"]))].median()\n    age_med = test_df[\"Age\"].median()\n    column = test_df[\"Age\"]\n    if not np.isnan(age_pred):\n        column.iloc[i] = age_pred\n    else:\n        column.iloc[i] = age_med","ebbd3e07":"test_df[test_df[\"Age\"].isnull()]","85155ed4":"justrainlen = len(train_df)\ntrain_df = pd.concat([train_df,test_df],axis = 0).reset_index(drop = True)","0695db57":"train_df[\"Name\"].head(10)","3f3447c7":"name = train_df[\"Name\"]\ntrain_df[\"Title\"] = [i.split(\".\")[0].split(\",\")[-1].strip() for i in name]\nsns.countplot(x=\"Title\", data = train_df)\nplt.xticks(rotation = 60)\nplt.show()","7cdd72e7":"train_df[\"Title\"] = train_df[\"Title\"].replace([\"Lady\",\"the Countess\",\"Capt\",\"Col\",\"Don\",\"Dr\",\"Major\",\"Rev\",\"Sir\",\"Jonkheer\",\"Dona\"],\"other\")\ntrain_df[\"Title\"] = [0 if i == \"Master\" else 1 if i == \"Miss\" or i == \"Ms\" or i == \"Mlle\" or i == \"Mrs\" else 2 if i == \"Mr\" else 3 for i in train_df[\"Title\"]]\ntrain_df[\"Title\"].head()","cc897f44":"g = sns.catplot(x = \"Title\", y = \"Survived\", data = train_df, kind = \"bar\")\ng.set_xticklabels([\"Master\",\"Mrs\",\"Mr\",\"Other\"])\ng.set_ylabels(\"Percentage of Survival\")\nplt.show()","046818e9":"train_df.drop(labels = [\"Name\"], axis = 1, inplace = True)\ntrain_df = pd.get_dummies(train_df,columns=[\"Title\"])\ntrain_df.head()","93875340":"train_df[\"tempFamilySize\"] = train_df[\"SibSp\"] + train_df[\"Parch\"]\ntrain_df.head()","faaac903":"g = sns.catplot(x = \"tempFamilySize\", y = \"Survived\", data = train_df, kind = \"bar\")\ng.set_ylabels(\"Percentage of Survived\")\nplt.show()","9cc454b4":"train_df[\"familySize\"] = [1 if i < 4 else 0 for i in train_df[\"tempFamilySize\"]]","1e14ef98":"g = sns.catplot(x = \"familySize\", y = \"Survived\", data = train_df, kind = \"bar\")\ng.set_ylabels(\"Percentage of Survived\")\nplt.show()","437109a3":"train_df.drop(labels = [\"tempFamilySize\"], axis = 1, inplace = True)\ntrain_df = pd.get_dummies(train_df, columns= [\"familySize\"])\ntrain_df.head()","d42ccb53":"train_df[\"Ticket\"].head(20)","fed1e225":"tickets = []\nfor i in list(train_df.Ticket):\n    if not i.isdigit():\n        tickets.append(i.replace(\".\",\"\").replace(\"\/\",\"\").strip().split(\" \")[0])\n    else:\n        tickets.append(\"X\")\ntrain_df[\"Ticket\"] = tickets","862042cb":"train_df = pd.get_dummies(train_df, columns= [\"Ticket\"], prefix = \"T\")\ntrain_df.head(10)","f36805bf":"#For embarked \ntrain_df = pd.get_dummies(train_df, columns=[\"Embarked\"])\ntrain_df.head()","3ff7340c":"#for Pclass\ntrain_df[\"Pclass\"] = train_df[\"Pclass\"].astype(\"category\")\ntrain_df = pd.get_dummies(train_df, columns= [\"Pclass\"])\ntrain_df.head()","cc6e93b4":"#For Sex\ntrain_df[\"Sex\"] = train_df[\"Sex\"].astype(\"category\")\ntrain_df = pd.get_dummies(train_df, columns=[\"Sex\"])\ntrain_df.head()","0c58d376":"train_df.drop(labels = [\"PassengerId\", \"Cabin\"], axis = 1, inplace = True)\ntrain_df.columns","f703bf4f":"TEST = train_df[justrainlen:]\nTEST.drop(labels = [\"Survived\"],axis = 1, inplace = True)\nTEST.head()","35e92928":"from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\nTRAIN = train_df[:justrainlen]\nTRAINX = TRAIN.drop(labels = \"Survived\", axis = 1)\nTRAINY = TRAIN[\"Survived\"]\nX_train, X_test, y_train, y_test = train_test_split(TRAINX, TRAINY, test_size = 0.30)\nprint(\"X_train\",len(X_train))\nprint(\"X_test\",len(X_test))\nprint(\"y_train\",len(y_train))\nprint(\"y_test\",len(y_test))\nprint(\"test\",len(TEST))","b3937a04":"#for same results, fix the random value\nrandom_state = 7","1efdfb76":"from sklearn.linear_model import LogisticRegression\n#default settings\nlogreg = LogisticRegression(random_state=random_state)\nlogreg.fit(X_train, y_train)\nacc_log_train = round(logreg.score(X_train, y_train)*100,2) \nacc_log_test = round(logreg.score(X_test,y_test)*100,2)\nprint(\"DEFAULT:\")\nprint(\"Training Accuracy: % {}\".format(acc_log_train))\nprint(\"Testing Accuracy: % {}\".format(acc_log_test))\n\n#parameter penalty\nlogreg = LogisticRegression(random_state=random_state, penalty='none',C=0.1)\nlogreg.fit(X_train, y_train)\nacc_log_train = round(logreg.score(X_train, y_train)*100,2) \nacc_log_test = round(logreg.score(X_test,y_test)*100,2)\nprint(\"PARAMETERS:\")\nprint(\"Training Accuracy: % {}\".format(acc_log_train))\nprint(\"Testing Accuracy: % {}\".format(acc_log_test))","5baa6352":"from sklearn.tree import DecisionTreeClassifier\ndectre = DecisionTreeClassifier(random_state=random_state)\ndectre.fit(X_train, y_train)\nacc_log_train = round(dectre.score(X_train, y_train)*100,2) \nacc_log_test = round(dectre.score(X_test,y_test)*100,2)\nprint(\"DEFAULT:\")\nprint(\"Training Accuracy: % {}\".format(acc_log_train))\nprint(\"Testing Accuracy: % {}\".format(acc_log_test))\n\ndectre = DecisionTreeClassifier(random_state=random_state, criterion=\"entropy\", min_samples_split=0.5, max_depth=1.5)\ndectre.fit(X_train, y_train)\nacc_log_train = round(dectre.score(X_train, y_train)*100,2) \nacc_log_test = round(dectre.score(X_test,y_test)*100,2)\nprint(\"PARAMETERS:\")\nprint(\"Training Accuracy: % {}\".format(acc_log_train))\nprint(\"Testing Accuracy: % {}\".format(acc_log_test))","d030f68f":"from sklearn.svm import SVC\nsvc = SVC()\nsvc.fit(X_train, y_train)\nacc_log_train = round(svc.score(X_train, y_train)*100,2) \nacc_log_test = round(svc.score(X_test,y_test)*100,2)\nprint(\"DEFAULT\")\nprint(\"Training Accuracy: % {}\".format(acc_log_train))\nprint(\"Testing Accuracy: % {}\".format(acc_log_test))\n\nsvc = SVC(kernel=\"poly\",degree=5)\nsvc.fit(X_train, y_train)\nacc_log_train = round(svc.score(X_train, y_train)*100,2) \nacc_log_test = round(svc.score(X_test,y_test)*100,2)\nprint(\"PARAMETERS\")\nprint(\"Training Accuracy: % {}\".format(acc_log_train))\nprint(\"Testing Accuracy: % {}\".format(acc_log_test))","eea8c469":"from sklearn.ensemble import RandomForestClassifier\nrndfor = RandomForestClassifier(random_state = random_state)\nrndfor.fit(X_train, y_train)\nacc_log_train = round(rndfor.score(X_train, y_train)*100,2) \nacc_log_test = round(rndfor.score(X_test,y_test)*100,2)\nprint(\"DEFAULT\")\nprint(\"Training Accuracy: % {}\".format(acc_log_train))\nprint(\"Testing Accuracy: % {}\".format(acc_log_test))\n\nrndfor = RandomForestClassifier(random_state = random_state,n_estimators=200,max_depth=5)\nrndfor.fit(X_train, y_train)\nacc_log_train = round(rndfor.score(X_train, y_train)*100,2) \nacc_log_test = round(rndfor.score(X_test,y_test)*100,2)\nprint(\"PARAMETERS\")\nprint(\"Training Accuracy: % {}\".format(acc_log_train))\nprint(\"Testing Accuracy: % {}\".format(acc_log_test))","01e800ee":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier()\nknn.fit(X_train, y_train)\nacc_log_train = round(knn.score(X_train, y_train)*100,2) \nacc_log_test = round(knn.score(X_test,y_test)*100,2)\nprint(\"DEFAULT\")\nprint(\"Training Accuracy: % {}\".format(acc_log_train))\nprint(\"Testing Accuracy: % {}\".format(acc_log_test))\n\nknn = KNeighborsClassifier(n_neighbors=10,radius=2.0)\nknn.fit(X_train, y_train)\nacc_log_train = round(knn.score(X_train, y_train)*100,2) \nacc_log_test = round(knn.score(X_test,y_test)*100,2)\nprint(\"PARAMETERS\")\nprint(\"Training Accuracy: % {}\".format(acc_log_train))\nprint(\"Testing Accuracy: % {}\".format(acc_log_test))","422fa598":"classifier = [DecisionTreeClassifier(random_state = random_state),\n             SVC(random_state = random_state),\n             RandomForestClassifier(random_state = random_state),\n             LogisticRegression(random_state = random_state),\n             KNeighborsClassifier()]\n\ndt_param_grid = {\"min_samples_split\" : range(10,500,20),\n                \"max_depth\": range(1,20,2)}\n\nsvc_param_grid = {\"kernel\" : [\"rbf\"],\n                 \"gamma\": [0.001, 0.01, 0.1, 1],\n                 \"C\": [1,10,50,100,200,300,1000]}\n\nrf_param_grid = {\"max_features\": [1,3,10],\n                \"min_samples_split\":[2,3,10],\n                \"min_samples_leaf\":[1,3,10],\n                \"bootstrap\":[False],\n                \"n_estimators\":[100,300],\n                \"criterion\":[\"gini\"]}\n\nlogreg_param_grid = {\"C\":np.logspace(-3,3,7),\n                    \"penalty\": [\"l1\",\"l2\"]}\n\nknn_param_grid = {\"n_neighbors\": np.linspace(1,19,10, dtype = int).tolist(),\n                 \"weights\": [\"uniform\",\"distance\"],\n                 \"metric\":[\"euclidean\",\"manhattan\"]}\nclassifier_param = [dt_param_grid,\n                   svc_param_grid,\n                   rf_param_grid,\n                   logreg_param_grid,\n                   knn_param_grid]","211ace6d":"cv_result = []\nbest_estimators = []\nfor i in range(len(classifier)):\n    clf = GridSearchCV(classifier[i], param_grid=classifier_param[i], cv = StratifiedKFold(n_splits = 10), scoring = \"accuracy\", n_jobs = -1,verbose = 1)\n    clf.fit(X_train,y_train)\n    cv_result.append(clf.best_score_)\n    best_estimators.append(clf.best_estimator_)\n    print(clf.best_estimator_)\n    print(cv_result[i])","88c7562a":"from sklearn.ensemble import VotingClassifier\ncv_results = pd.DataFrame({\"Cross Validation Means\":cv_result, \"ML Models\":[\"DecisionTreeClassifier\", \"SVM\",\"RandomForestClassifier\",\n             \"LogisticRegression\",\n             \"KNeighborsClassifier\"]})\n\ng = sns.barplot(\"Cross Validation Means\", \"ML Models\", data = cv_results)\ng.set_xlabel(\"Accuracy\")\ng.set_title(\"Scores\")","1363bba8":"from sklearn.metrics import accuracy_score\nvotingC = VotingClassifier(estimators = [(\"dt\",best_estimators[0]),\n                                        (\"rfc\",best_estimators[2]),\n                                        (\"lr\",best_estimators[3])],\n                                        voting = \"soft\", n_jobs = -1)\nvotingC = votingC.fit(X_train, y_train)\nprint(accuracy_score(votingC.predict(X_test),y_test))","07c8c0fe":"test_survived = pd.Series(votingC.predict(TEST), name = \"Survived\").astype(int)\n\ntest_PassengerId = test_df[\"PassengerId\"]\nresults = pd.concat([test_PassengerId, test_survived],axis = 1)\nresults.to_csv(\"titanic.csv\", index = False)","44fd9d3c":"test_survived = pd.Series(dectre.predict(TEST), name = \"Survived\").astype(int)\nacc_log_train = round(dectre.score(X_train, y_train)*100,2) \nacc_log_test = round(dectre.score(X_test,y_test)*100,2)\ntest_PassengerId = test_df[\"PassengerId\"]\nresults = pd.concat([test_PassengerId, test_survived],axis = 1)\nresults.to_csv(\"titanic-basic-dt.csv\", index = False)","c04134f2":"<h4><a name=\"categoricalvariablespclass\">Pclass<\/a><\/h4>\n<p>Passangers are accepted to Titanic in 3 Comfort Classes.<br>\n1. Class: 216, 2.Class: 184, 3.Class:491<br>\nMost of passengers are from 3. class.<\/p>","23cb626a":"<h4><a name=\"numericvariablesfare\">Fare<\/a><\/h4>\n<p>Most of Passengers has a ticket price below 50 Dolar. There are fares over 200 Dollar, but these can be multiple price for multiple tickets.<\/p>","2ba6dca3":"<b>Now, we should drop Name feature, because title is more useful than name and categorized Title feature...<\/b>","eb650d3d":"<h1><a name=\"modelling\">MODELLING<\/a><\/h1>","11e8580d":"<h4><a name=\"numericvariablespassengerid\">Passenger ID<\/a><\/h4>\n<p>Every passenger has different passenger ID, so graph will draw another bar for every passenger. We can't figure out any helful result with this variable.<\/p>","3856393b":"<h4><a name=\"numericvariablesage\">Age<\/a><\/h4>\n<p>Ages are 20-30 most commonly. Older passengers are rare.<\/p>","6dd83f84":"When we plot graph for family size, we examine that small family size is absolutely higher chance to survive.","dc67b7d2":"<h4><a name=\"categoricalvariablessibsp\">Siblings \/ Spouses<\/a><\/h4>\n<p>It has 7 category, most of passengers does not have any siblings or spouses.<\/p>","21c81993":"<h1><a name=\"deployment\"><\/a>DEPLOYMENT<\/h1>","707e1af6":"<table><tr><td style=\"background-image:url(http:\/\/mehmetsehirli.com.tr\/out\/advanced-neural-network\/final\/banner.jpg); background-size:cover; background-position:center top;\" width=\"50%\">&nbsp;<\/td><td width=\"50%\"><a name=\"information\"><\/a><h1 style=\"text-align:left;\">Titanic Accident Data Analysis<\/h1><p style=\"font-size:1em;\">Titanic was a transatlantic steam cruise ship, the largest Olympic class of its time. On the night of April 15, 1912, he first hit an iceberg and was buried in the ice of the North Atlantic in about two hours and forty minutes.\nIts sinking resulted in the death of 1,514 people and went down in history as one of the biggest sea disasters.\n\nThe great loss of life caused by the fall of Titanic was attributed to many reasons, but the fact that became prominent over time was that the ship did not carry enough lifeboats for everyone.\nAlthough Titanic's full capacity was 3,547 people, the total capacity of the lifeboats on the ship was 1,178 people. In addition, the number of men who died in total is very disproportionate because women and children are given priority during the accident.\nIn the design phase of Titanic, if the proposed new generation wattles were used, enough lifeboats would be able to carry 48 lifeboats and enough for everyone on board, even if not all of the ship.\nThe company of Titanic decided to put 16 wooden lifeboats (1-16) (This figure was the legal minimum for a ship in this tonnage) and in addition 4 folding lifeboats (A-D).\n\nThe most advanced technologies available at the time were used in Titanic. It was believed by many people to be a \"buoyant ship,\" and this belief was defined and launched in this way before sinking.\nAt the time of the ship, the Titanic was above all its competitors in luxury, wealth and splendor. On the ship, a swimming pool, a gym, a Turkish bath, a library and a tennis court in both first and second class were offered as standard. First-class common rooms were decorated with very special woodworking, expensive furniture and other decorations.\nDespite this extremely advanced technology and trained crew, its sinking shocked many people. The media continued to bring up Titanic's famous victims and legends about the sunset.\n<\/p><\/td><\/tr><\/table>\n","087ce79c":"<h2><a name=\"datavariablesaspair\"><\/a>Discover Data Variables as Pair<\/h2>\n<p>Single feautes are good for understanding data but insufficient for prediction. Within two variables together will be useful. Compare some features to discover connections and new features.\n    <ul>\n        <li><a href=\"#datavariablesaspairpclasssurvived\">PClass vs. Survived<\/a><\/li>\n        <li><a href=\"#datavariablesaspairsexsurvived\">Sex vs. Survived<\/a><\/li>\n        <li><a href=\"#datavariablesaspairsibspsurvived\">Siblings\/Spouses vs. Survived<\/a><\/li>\n        <li><a href=\"#datavariablesaspairparchsurvived\">Parent\/Children vs. Survived<\/a><\/li>\n        <li><a href=\"#datavariablesaspairagesurvived\">Age vs. Survived<\/a><\/li>\n    <\/ul>\n<\/p>","bbe31067":".head() from dataframe returns head lines of data set. <br>\nAs an idea we can see the plain data...<br>\n<a href=\"https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.head.html\" target=\"_blank\">Head Function Details<\/a>","e2912626":"<h2><a name=\"traintestdata\">Train and Test Data<\/a><\/h2>\n<p>We have combined our train data and test data together to evaluate new features together. Now we need to split the data: We need some data sets:<br>\n    <ol>\n        <li><span style=\"color:red;\">TRAIN<\/span> Dataset (881)\n            <ol>\n                <li><span style=\"color:red;\">X_train<\/span>: for training without survived column 70% (616)<\/li>\n                <li><span style=\"color:red;\">y_train<\/span>: for answers of X_train only survived column (616)<\/li>\n                <li><span style=\"color:red;\">X_test<\/span>: for testing without survived column 30% (265)<\/li>\n                <li><span style=\"color:red;\">y_test<\/span>: for answers of X_test only survived column (265)<\/li>\n            <\/ol>\n        <\/li>\n        <li><span style=\"color:red;\">TEST<\/span> Dataset - separated or test (418)<\/li>\n    <\/ol>\n<\/p>","c8f38219":"<h3><a name=\"decisiontree\">Decision Tree Classifier (Best by Default)<\/a><\/h3>\n<p>Decision tree is a decision support tool that uses a tree-like model of decisions and their possible consequences, including chance event outcomes, resource costs, and utility. <a href=\"https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.tree.DecisionTreeClassifier.html?highlight=decision%20tree#sklearn.tree.DecisionTreeClassifier\" target=\"_blank\">Details<\/a><\/p>\n<p>Decision Tree supports lots of initial settings like criterion, min_samples_split or max_depth...<\/p>","77a81b4b":"<h3><a name=\"newfeaturesdrop\">Drop Passenger ID and Cabin<\/a><\/h3>\n<p>\n    PassengerId and Cabin data are messy and not useful for model so I drop these columns.\n<\/p>","ded50c29":"<h3><a name=\"missingvaluetrain\">Train Data Missing Values<\/a><\/h3>","7951bb0c":"<h4><a name=\"categoricalvariablessex\">Sex<\/a><\/h4>\n<p>At 891 Passengers, Gender is also an imbalanced variable, female number is about half of male counts.<\/p>","b4ce1203":"<h4><a name=\"missingvaluetrainembarked\">Embarked Train Data Missing Values<\/a><\/h4>","7fe4702a":"<h4><a name=\"categoricalvariablescabin\">Cabin<\/a><\/h4>\n<p>Cabin variable has 174 category for Titanic Train dataset. Graph will incomprehensible, so just print results.<\/p>","03c9772c":"<h3><a name=\"newfeaturestitle\">New Feature: Title<\/a><\/h3>\n<p>Normally name feature is not useful for prediction but with titanic data it also has title inside.<br>\nFirst I need split title from name, it has standart format, we can split it with .(dot):<br>\n    Example: <span style=\"color:blue;\">Braund, Mr<\/span><b>.<\/b><span style=\"color:red;\"> Owen Harris<\/span>\n    <br><br>\n    We take the first element and split it with ,(comma)\n    <br>\n    Example: <span style=\"color:blue;\">Braund<\/span><b>,<\/b><span style=\"color:red;\"> Mr<\/span>\n    <br><br>\n    We take second part and trim one character from start for blank.\n<\/p>","17eecaee":"<h4><a name=\"categoricalvariablesembarked\">Embarked<\/a><\/h4>\n<p>Passangers are embarked from 3 ports, <br>\nSouthampton:644, Cherbourg:168 Queenstown:77\n<br>\nMost of passengers are marked from Southampton.<\/p>","bc3556f9":"<h3><a name=\"randomforest\">Random Forest<\/a><\/h3>\n<p>Random decision forests are an ensemble learning method for classification, regression and other tasks that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification). <a href=\"https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html\" target=\"_blank\">Details<\/a><\/p>\n<p>Random Forest Classifier supports lots of initial settings like n_estimators or max_depth...<\/p>","5200d4fb":"<h3><a name=\"missingvaluetest\">Test Data Missing Values<\/a><\/h3>","aedf8833":"<h4><a name=\"datavariablesaspairsibspsurvived\">Siblings\/Spouses vs. Survived<\/a><\/h4>\n<p>When we look survivor as their siblings and spouses, we realize that 0,1,2 has high percentage for survive and others are low. So I can provide a new feature at threshold below\/over 2<\/p>","d19e30a5":"<p>\nIt has 17 titles, we can group these titles, Mr, Mrs, Master and Others...<br><br>\n0 For Master<br>\n1 For Miss, Ms, Mlle, Mrs<br>\n2 For Mr<br>\n3 For Other<br>\n<\/p>\n","ed51ccd1":"<h3><a name=\"svc\">Support Vector Machine<\/a><\/h3>\n<p>Support-vector machines (SVM) are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis. <a href=\"https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.svm.SVC.html\" target=\"_blank\">Details<\/a><\/p>\n<p>Support Vector Classifier supports lots of initial settings like kerne or degree...<\/p>","34d54070":"<b>Data Types for Variables<\/b>\n<table width=\"100%\" >\n    <tr>\n        <th style=\"text-align:center;\">\n            Float64 (2)\n        <\/th>\n        <th style=\"text-align:center;\">\n            Int65 (5)\n        <\/th>\n        <th style=\"text-align:center;\">\n            Object (5)\n        <\/th>\n    <\/tr>\n    <tr>\n        <td style=\"text-align:center;\">\n            Age<br>Fare\n        <\/td>\n        <td style=\"text-align:center;\">\n            PassengerId<br>\n            Survived<br>\n            Pclass<br>\n            Sibsp<br>\n            Parch\n        <\/td>\n         <td style=\"text-align:center;\">\n             Name<br>Sex<br>Ticket<br>Cabin<br>Embarked\n        <\/td>\n    <\/tr>\n<\/table>","c59d9fad":"<h4><a name=\"datavariablesaspairsexsurvived\">Sex vs. Survived<\/a><\/h4>\n<p>When comparing gender in dataset, Female passengers are very high chance to survive. Although male passengers are much more in count(<a name=\"categoricalvariablessex\">look at graph<\/a>)<\/p>","9d4139ef":"<h2><a name=\"modeldecisionbyvoting\">Model Decision By Voting<\/a><\/h2>\n<p>\n    For decision we can choose more than one model. Ensemble methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone.<a href=\"https:\/\/en.wikipedia.org\/wiki\/Ensemble_learning\" target=\"_blank\">Details<\/a><br><br>\n    I will add successful models to Voting Classifier, as above, 0: Decision Tree, 1: Random Forest and 2: Logistic Regression.\n    <br>\n    There is an important parameter for Voting Classifier as <b>\"voting\"<\/b>. It can be soft or hard. When is it hard each model returns only 0 or 1. Voting Classifier choose by majority. When parameter is soft, each model returns probability with 0 to 1.0  and Voting Classifier takes an average and returns a percentage.\n<\/p>","e5cb3833":"<h4><a name=\"datavariablesaspairpclasssurvived\">PClass vs. Survived<\/a><\/h4>\n<p>Possibility of Class 1 Passengers have more chance to survive as 62%. Class 2 has a chance of nearly half to half. Lastly Class 3 has a little chance to survive as 24%.<\/p>\n<p>Class1 is the most expensive class, class 2 is medium and class 3 is the cheapest.<\/p>\n<p>This feature ise important and powerful feautre for prediction as shown in <a href=\"#correlationmatrix\">Correlation Matrix<\/a><\/p>","364fd38b":"<h3><a name=\"newfeaturesmodify\">Modify Embarked, Pclass and Sex<\/a><\/h3>\n<p>\n    Embarked, PClass and Sex features don't need any processing so only we only turn them into categorical values and separate each category to different column.\n<\/p>","f3d94646":"<h3><a name=\"logisticregression\">Logistic Regression<\/a><\/h3>\n<p>Logistic model (or logit model) is used to model the probability of a certain class or event existing such as pass\/fail, win\/lose, alive\/dead or healthy\/sick. <a href=\"https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegression.html\" target=\"_blank\">Details<\/a><\/p>\n<p>Logistic Regression supports lots of initial settings like penalty or C...<\/p>","0d37ff2b":"<h2><a name=\"imports\"><\/a>Import Libraries<\/h2>\n<p>\n    <b>Numpy:<\/b> Famous Python Library for linear algebra operations<br>\n    <b>Pandas:<\/b> Another Famous Python Library for data processing, csv\/excel reading, datasets, etc.<br>\n    <b>MatPlot:<\/b> is a plotting library for the Python programming language and its numerical mathematics extension NumPy.<br>\n    <b>SeaBorn:<\/b>  is a Python data visualization library based on matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics. \n<\/p>","3695c4c9":"<h2><a name=\"newfeatures\">FEATURE OPERATIONS<\/a><\/h2>\n<p>\n    We examine data both one by one and pairs. Learned some helpful connections about features.<br>\n    Now we can create some new features with combining features, proccessing over features\n    Also drop some useless features<br>\n    <ul>\n        <li><a href=\"#newfeaturestitle\">Title<\/a><\/li>\n        <li><a href=\"#newfeaturesfamilysize\">Family Size<\/a><\/li>\n        <li><a href=\"#newfeaturesembarked\">Embarked<\/a><\/li>\n        <li><a href=\"#newfeaturesticket\">Ticket<\/a><\/li>\n        <li><a href=\"#newfeaturespclass\">PClass<\/a><\/li>\n        <li><a href=\"#newfeaturessex\">Sex<\/a><\/li>\n        <li><a href=\"#newfeaturesdroppassengeridcabin\">Drop PassengerID and Cabin<\/a><\/li>\n    <\/ul>\n<\/p>\n","b3f2961b":"<h2><a name=\"datavariables\"><\/a>Discover Data Variables<\/h2>\n<p>Variables of Titanic data are purely the columns of dataset:<br>\n<ol>\n    <li><b>PassengerId:<\/b><br>\n        Unique identification number for each passenger\n    <\/li>\n    <li><b style=\"color:red\">Survived:<\/b><br>\n        Information whether passenger survived from disaster or died. For survive:1, died:0\n    <\/li>\n    <li><b>Pclass:<\/b><br>\n        Passenger Class in Journey, Titanic has 3 classes.\n    <\/li>\n    <li><b>Name:<\/b><br>\n        Name of each passenger with title. \n    <\/li>\n    <li><b>Sex:<\/b><br>\n        Gender of passenger (male or female)\n    <\/li>\n    <li><b>Age:<\/b><br>\n        Age of each passenger (dtype: float64)\n    <\/li>\n    <li><b>SibSp:<\/b><br>\n        Sib for Siblings and Sp for Spouses, this feature stores number of siblings and spouses in the ship\n    <\/li>\n    <li><b>Parch:<\/b><br>\n        Par for Parents and Ch for Children, this feature stores number of parents and children in the ship\n    <\/li>\n     <li><b>Ticket:<\/b><br>\n        Ticket number of the passenger, it can contain numbers, letters, slash and etc.\n    <\/li>\n     <li><b>Fare:<\/b><br>\n         Ticket price, the amount of money spent on ticket\n    <\/li>\n     <li><b>Cabin:<\/b><br>\n         Cabin category of passenger, also include cabin number\n    <\/li>\n     <li><b>Embarked:<\/b><br>\n         abbreviation of the port where the passenger embarked<br>\n         Three Ports:\n         <ul>\n             <li>(C)herbourg<\/li>\n             <li>(Q)ueenstown<\/li>\n             <li>(S)outhampton<\/li>\n         <\/ul>\n    <\/li>\n<\/ol>\n\n<\/p>\n","0aeebf92":"Load <br>\ntrain.csv to **train_df**<br>\nand <br>\ntest.csv to **test_df**<br>\nvariables.","7ab8c0f0":"<h3><a name=\"newfeaturesticcket\">New Feature: Ticket<\/a><\/h3>\n<p>\n   In Titanic data, ticket consist of two part, all tickets have second part and its unique number; so it is not very useful for models. Some passengers have first part and it can repeat; so we can make a category from ticket:<br>\n    Example:<br><span style=\"color:red;\">A\/5.<\/span> <span style=\"color:blue\">2151<\/span><br><br>\n    At first part there can be \/ and .<br>\n    So first extract these characters, then split from blank character and takes the first part.<br>\n<\/p>","e56554b2":"<h4><a name=\"missingvaluetestage\">Age Test Data Missing Values<\/a><\/h4>\n<p>\n    For filling age for test data same properties as train data you can see <a href=\"#missingvaluetrainage\">details...<\/a><br>\n    \n<\/p>","6e0171a3":"<h2><a name=\"loaddata\"><\/a>Load and Discover Data<\/h2>\n<p>\nKaggle is a super storage for data and you can find processed or plain data about various of subjects.<br> For this project you can found detailed information about data in Competition Data Tab. For Titanic dataset the default directory is \u201c\/kaggle\/input\/titanic\u201d. It has 3 files:<br>\n    <u>\/kaggle\/input\/titanic\/train.csv<\/u><br>\n    <u>\/kaggle\/input\/titanic\/gender_submission.csv<\/u><br>\n    <u>\/kaggle\/input\/titanic\/test.csv<\/u><br>\n    For this project <b>train.csv and test.csv<\/b> will be used.\n    <br>\n    <br>\n    First we will load data,<br>\n    Then we try to understand data with basic features and statistics.\n<\/p>","dc5ca063":"<h2><a name=\"missingvalue\">HANDLE MISSING VALUES<\/a><\/h2>\n<p>1. Find Columns which has missing value<br>\n    2. How many passenger has missing values with which feature?<br>\n    3. How to Fill the missing values?<br>\n<\/p>\n\n<p>\n    <b>Training and testing data frames has different missing values. I will process same operations for both.<\/b><br>\n    <u>For train:<\/u><br>\n    Missing values are located at:<br>\n    Age (170), Cabin(680), Embarked(2)\n    <br>\n    <b>Embarked<\/b> has only 2 missing value. I evalute from data that Cherbourg will be suitable for filling.(<u>Fill with static value<\/u>)\n    <br><br>\n    <u>For Test:<\/u><br>\n    Age(86), Fare(1), Cabin(327)<br>\n    <b>Fare<\/b> feature has only one missing value in test data. I evaluate that passenger is from Class 3 and fill missing value with average of class3 passengers' fare: 12.45 (<u>Fill with average value<\/u>)\n<\/p>\n","901aeff9":"<h4><a name=\"datavariablesaspairagesurvived\">Age vs. Survived<\/a><\/h4>\n<p>\n    There will be 2 graphs, First is died and Second is Survived<br>\n    From Graphs some assumptions can be extracted:\n    <ol>\n        <li>Most of passengers are between 15-40<\/li>\n        <li>15-25 ages have lower chance to survive<\/li>\n        <li>25-35 ages have higher chance to survive<\/li>\n        <li>Age lower than 10 have higher survival rate<\/li>\n        <li>Very old passengers survived(~80)<\/li>\n    <\/ol>\n<\/p>","f1213ad2":"<h2><a name=\"models\">Models<\/a><\/h2>\n\nWe will try 5 different Machine Learning Classifier:\n<ol>\n    <li><a href=\"#logisticregression\">LogisticRegression<\/a><\/li>\n    <li><a href=\"#decisiontree\">Decision Tree<\/a><\/li>\n    <li><a href=\"#svc\">Support Vector Machine<\/a><\/li>\n    <li><a href=\"#randomforest\">Random Forest<\/a><\/li>\n    <li><a href=\"#knn\">K Nearest Neigbors<\/a><\/li>\n<\/ol>\n\nFirst I will evaluate models by default settings, then try to improve with settings' variables.","87d39f73":"<h3><a name=\"categoricalvariables\"><\/a>Categorical Variables<\/h3>\n<p>A Categorical Variable can take on one of a limited, and usually fixed, number of possible values, assigning each individual or other unit of observation to a particular group or nominal category on the basis of some qualitative property. <a href=\"https:\/\/en.wikipedia.org\/wiki\/Categorical_variable\" target=\"_blank\">Details...<\/a><\/p>\n\n<p>Examining single feature may not give a direct relation about data and prediction, but it will give a brief for all columns.<\/p>\n\n<p>For categorical variables Titanic dataset has:\n    <ul>\n        <li><a href=\"#categoricalvariablessurvived\">Survived<\/a><\/li>\n        <li><a href=\"#categoricalvariablessex\">Sex<\/a><\/li>\n        <li><a href=\"#categoricalvariablespclass\">Passenger Class<\/a><\/li>\n        <li><a href=\"#categoricalvariablesembarked\">Embarked<\/a><\/li>\n        <li><a href=\"#categoricalvariablessibsp\">Siblings \/ Spouses<\/a><\/li>\n        <li><a href=\"#categoricalvariablesparch\">Parents \/ Children<\/a><\/li>\n    <\/ul>\n<\/p>","a80179b9":"<h2><a name=\"correlationmatrix\"><\/a>Discover Data with Correlation Matrix<\/h2>\n<p>A correlation matrix is a table showing correlation coefficients between variables. Each cell in the table shows the correlation between two variables. A correlation matrix is used to summarize data, as an input into a more advanced analysis, and as a diagnostic for advanced analyses.<a href=\"https:\/\/www.displayr.com\/what-is-a-correlation-matrix\/\" target=\"_blank\">Correlation Matrix Details<\/a><br><br>\nHigher values determines better connections. We are interested in \"Survived\" feature at last row:<br>\nSibSp: 0,0033<br>\nParch: 0,093<br>\nAge: -0,077<br>\n<span style=\"color:red\">Fare: 0,26<\/span><br>\nSo survived feautre is directly relation with ticket price, we can say that:<br>\n<span style=\"background:pink;padding:5px;\">The higher you pay for ticket, the higher chance to survive!<\/span>\n\nWe can use seaborn library to plot.\n<\/p>","ba88f0c1":"<h4><a name=\"missingvaluetestfare\">Fare Test Data Missing Values<\/a><\/h4>","ae1afc59":"<h4><a name=\"categoricalvariablesticket\">Ticket<\/a><\/h4>\n<p>Ticket numbers also can't combine into large categories. Some tickets are for more than one passenger.<\/p>","473b3838":"<h4><a name=\"missingvaluetrainage\">Age Train Data Missing Values<\/a><\/h4>\n<p>\n    For filling age data can be challening, <br>\n    it can be related with gender(examine average ages for each gender)...<br>\n    it can be related with classes(older passengers can buy comfortable classes, younger can only buy standart class)...<br>\n    it can be related siblings, parents, children(if passenger has children, he\/she should be older)...<br>\n    Possibilities may vary, we could examine features with heatmap:\n<\/p>","bb7461d6":"<h1><a name=\"datapreparation\">DATA PREPARATION<\/a><\/h1>","8d92041c":"For submit a file to competition, I prepare voting model and basic decision tree classifier results.\nAnd submit to competition.","cb08a9b8":"<h4><a name=\"datavariablesaspairparchsurvived\">Parent\/Children vs. Survived<\/a><\/h4>\n<p>Small families are more chance to survive rather than large families and singles. If passenger has children and\/or parent over 3, change to survive decrease dramatically. So new feature also can be generated from 0-3 and 4-6 for Parch.<br>Another feature can be extracted from combining Parch and Sibsp.<\/p>","60fc5865":"Now we have a new feature, examine the percentage of survived. This graph deduces a good relations for prediction.<br>\nMrs title has high survival rate.<br>\nMr title has low survival rate.","87d3fcae":"<h4><a name=\"categoricalvariablessurvived\">Survived<\/a><\/h4>\n<p>Normally Dataset has 891 passengers, 342 passenger survived but 549 passengers died in disaster.<br>This feature is imbalanced, dead probability is higher for dataset<\/p>","fb9d6f73":"<h4><a name=\"categoricalvariablesname\">Name<\/a><\/h4>\n<p>Name should change for every one.<br>Finding categories don't give any valuable result<\/p>","1075693f":"<h2><a name=\"outlierdetection\">Outlier detection<\/a><\/h2>\n<p><img src=\"http:\/\/mehmetsehirli.com.tr\/out\/advanced-neural-network\/final\/outlier.jpg\" alt=\"\"style=\"float:left;margin-right:5px;\">Outlier detection is the process of detecting and subsequently excluding outliers-a piece of data or observation that deviates drastically from the given norm or average of the data set- from a given set of data.<a href=\"https:\/\/www.techopedia.com\/definition\/30351\/outlier-detection\" target=\"_blank\">Outlier Details...<\/a>\n<br>\n    It is important that single outlier data for only one feature can be acceptable. So I will remove passengers who has more than 1 outlier data...\n    <br>\n    81 passenger records has outlier value but dropping a record with a single outlier variable will lost useful data, soI run for threshold for 2. With 2 threshold there 10 records, it is acceptable to delete these records for more accurate precisions.\n<\/p>\n","86fc1833":"<h2><a name=\"menu\"><\/a>Navigation<\/h2>\n<p style=\"color:#777; font-style:italic;\">Welcome to Titanic Disaster Data Analysis, this notebook has been created for Akdeniz University - Advanced Neural Network Lecture!<br>Enjoy!<\/p>\n<ol>\n    <li><b>BUSSINESS UNDERSTANDING<\/b>\n        <ol>\n            <li><a href=\"#menu\">Navigation<\/a><\/li>\n            <li><a href=\"#information\">Information<\/a><\/li>\n            <li><a href=\"#imports\">Import External Libraries<\/a><\/li>\n        <\/ol>\n    <\/li>\n    <li><b>DATA UNDERSTANDING<\/b>\n        <ol>\n            <li><a href=\"#loaddata\">Load and Discover Data<\/a><\/li>\n            <li><a href=\"#datavariables\">Discover Data Variables<\/a>\n                <ul>\n                    <li><a href=\"#categoricalvariables\">Catecgorical Variables<\/a><br>\n                            <ul>\n                                <li><a href=\"#categoricalvariablessurvived\">Survived<\/a><\/li>\n                                <li><a href=\"#categoricalvariablessex\">Sex<\/a><\/li>\n                                <li><a href=\"#categoricalvariablespclass\">Passenger Class<\/a><\/li>\n                                <li><a href=\"#categoricalvariablesembarked\">Embarked<\/a><\/li>\n                                <li><a href=\"#categoricalvariablessibsp\">Siblings \/ Spouses<\/a><\/li>\n                                <li><a href=\"#categoricalvariablesparch\">Parents \/ Children<\/a><\/li>\n                            <\/ul>\n                    <\/li>\n                    <li><a href=\"#numericvariables\">Numeric Variables<\/a>\n                        <ul>\n                            <li><a href=\"#numericvariablesfare\">Fare<\/a><\/li>\n                            <li><a href=\"#numericvariablesage\">Age<\/a><\/li>\n                            <li><a href=\"#numericvariablespassengerid\">Passenger ID<\/a><\/li>\n                        <\/ul>\n                    <\/li>\n                <\/ul>\n            <\/li>\n            <li><a href=\"#correlationmatrix\">Correlation Matrix<\/a><\/li>\n            <li><a href=\"#datavariablesaspair\">Discover Data Variables as Pair<\/a>\n                    <ul>\n                        <li><a href=\"#datavariablesaspairpclasssurvived\">PClass vs. Survived<\/a><\/li>\n                        <li><a href=\"#datavariablesaspairsexsurvived\">Sex vs. Survived<\/a><\/li>\n                        <li><a href=\"#datavariablesaspairsibspsurvived\">Siblings\/Spouses vs. Survived<\/a><\/li>\n                        <li><a href=\"#datavariablesaspairparchsurvived\">Parent\/Children vs. Survived<\/a><\/li>\n                        <li><a href=\"#datavariablesaspairagesurvived\">Age vs. Survived<\/a><\/li>\n                    <\/ul>\n            <\/li>\n        <\/ol>\n    <\/li>\n    <li><b>DATA PREPARATION<\/b>\n        <ol>\n            <li><a href=\"#outlierdetection\">Outlier Detection<\/a><\/li>\n            <li><a href=\"#missingvalue\">Handle Missing Value<\/a><\/li>\n            <li><a href=\"#newfeatures\">Feature Operations<\/a>\n                <ul>\n                    <li><a href=\"#newfeaturestitle\">Title(Modify)<\/a><\/li>\n                    <li><a href=\"#newfeaturesfamilysize\">Family Size(New)<\/a><\/li>\n                    <li><a href=\"#newfeaturesticket\">Ticket(Modify)<\/a><\/li>\n                    <li><a href=\"#newfeaturesmodify\">Embarked, PClass, Sex(Categorized)<\/a><\/li>\n                    <li><a href=\"#newfeaturesdrop\">PassengerID, Cabin(Dropped)<\/a><\/li>\n                <\/ul>\n            <\/li>\n        <\/ol>\n    <\/li>\n    <li><b>MODELLING<\/b>\n        <ol>\n            <li><a href=\"#definetraintestdata\">Define Train and Test Data<\/a><\/li>\n            <li><a href=\"#models\">Models<\/a>\n                <ol>\n                    <li><a href=\"#logisticregression\">LogisticRegression<\/a><\/li>\n                    <li><a href=\"#decisiontree\">Decision Tree<\/a><\/li>\n                    <li><a href=\"#svc\">Support Vector Machine<\/a><\/li>\n                    <li><a href=\"#randomforest\">Random Forest<\/a><\/li>\n                    <li><a href=\"#knn\">K Nearest Neigbors<\/a><\/li>\n                <\/ol>\n            <\/li>\n        <\/ol>\n    <\/li>\n    <li><b>EVALUATION<\/b>\n        <ol>\n                    <li><a href=\"#parameterdecision\">Parameter Decision<\/a><\/li>\n                    <li><a href=\"#modeldecisionbyvoting\">Model Decision By Voting<\/a><\/li>\n                <\/ol>\n    <\/li>\n    <li><b>DEPLOYMENT<\/b><\/li>\n<\/ol>\n","3eab8bcc":".columns property returns columns of dataset as a list, to explore features...","edccdae9":"<h3><a name=\"knn\">K Nearest Neighbors<\/a><\/h3>\n<p>K-NN is a type of instance-based learning, or lazy learning, where the function is only approximated locally and all computation is deferred until function evaluation. <a href=\"https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neighbors.NearestNeighbors.html\" target=\"_blank\">Details<\/a><\/p>\n<p>K Nearest Neighbors supports lots of initial settings like n-neighbors or radius...<\/p>","e0e173cd":"<h4><a name=\"categoricalvariablesparch\">Parents \/ Children<\/a><\/h4>\n<p>It has also 7 category, most of passengers does not have any parents or children.<\/p>","5e3e39ee":"<h1>MODEL EVOLUATION<\/h1>\n<h2><a name=\"parameterdecision\">Parameter Decision<\/a><\/h2>\n<p>\n    <img src=\"http:\/\/mehmetsehirli.com.tr\/out\/advanced-neural-network\/final\/hyperparameters.png\" alt=\"\" width=\"250px\" style=\"float:left; margin-right:5px;\">In machine learning, hyperparameter optimization or tuning is the problem of choosing a set of optimal hyperparameters for a learning algorithm. A hyperparameter is a parameter whose value is used to control the learning process. By contrast, the values of other parameters (typically node weights) are learned.<a href=\"https:\/\/en.wikipedia.org\/wiki\/Hyperparameter_optimization\" target=\"_blank\">Details<\/a><br>\n    To evaluate Hyperparamter Optimization:<br>\n    1. We prepare an array for Classifiers' Constructors: classifier<br>\n    2. We prepare an array of settings with key and values at same order: classifie_params\n    3. Iterate for each classifier and GridSearchCV fit and find best accuracy and estimator\n    <hr>\n    <b>Results:<\/b><br>\n    <b style=\"color:green\">RandomForestClassifier(bootstrap=False, max_features=10, min_samples_leaf=3, random_state=7)<\/b>: 0.8328662083553674<br>\n    <b style=\"color:green\">DecisionTreeClassifier(max_depth=9, min_samples_split=30, random_state=7)<\/b>: 0.8164198836594394<br>\n    <b style=\"color:orange\">LogisticRegression(C=1000.0, random_state=7)<\/b>: 0.8083553675304073<br>\n    <b style=\"color:orange\">SVC(C=200, gamma=0.001, random_state=7)<\/b>: 0.800158646218932<br>\n    <b style=\"color:red\">KNeighborsClassifier(metric='manhattan', n_neighbors=19, weights='distance')<\/b>:0.7597567424643046<br>\n    \n    \n    \n<\/p>","2cc4be5a":"<b>Filling Embarked Missing Values<\/b>\nWe search a relation to determine embarked port. We compare with a box plot with Embarked vs. Fare.<br>\nIn the graph, Q and S ports has low ticket fare but the missing passengers has $80 so that they should embarked from (C)herbourg.","8cd9d3db":"Dataset .describe() function returns statistical information about numeric columns. <br>\nExample mean of ages of passenger, older and younger passengers age...<br>\n<a href=\"https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.describe.html\" target=\"_blank\">Describe Fuction Details<\/a>","880d8895":"<h3><a name=\"newfeaturesfamilysize\">New Feature: Family Size<\/a><\/h3>\n<p>\n    Sibsp and Parch determines number of people in family. <a name=\"missingvaluetrainage\">In previous heatmap<\/a> we realize that these features are connected, We can combine these two features and generate a new feautre.\n<\/p>","0f0e4a8b":"<h3><a name=\"numericvariables\"><\/a>Numeric Variables<\/h3>\n<p>A numerical variable is a variable where the measurement or number has a numerical meaning.  <a href=\"https:\/\/socratic.org\/questions\/what-is-a-numerical-variable-and-what-is-a-categorical-variable\" target=\"_blank\">Details...<\/a><\/p>\n\n<p>Numeric values can be any range, to understand data distributions, histogram will best way.<\/p>\n\n<p>For categorical variables Titanic dataset has:\n    <ul>\n        <li><a href=\"#numericvariablesfare\">Fare<\/a><\/li>\n        <li><a href=\"#numericvariablesage\">Age<\/a><\/li>\n        <li><a href=\"#numericvariablespassengerid\">Passenger ID<\/a><\/li>\n    <\/ul>\n<\/p>","179b7c54":"Dataset .info() function, prints information about a DataFrame including the index dtype and column dtypes, non-null values and memory usage.<br>\n<a href=\"https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.info.html\" target=\"?_blank\">Info Function Details<\/a>\n"}}