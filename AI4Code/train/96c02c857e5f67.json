{"cell_type":{"8296f814":"code","4ccfc8df":"code","cb76f418":"code","852fad44":"code","f3e3061b":"code","b436fe90":"code","16fb6680":"code","ceeb3206":"code","a2421eaa":"code","f0125c40":"code","6fd95193":"code","6727b59b":"code","17f1538f":"code","9db1a87a":"code","faa4b226":"code","11c48309":"code","87fd4735":"code","41fc9406":"code","1bef201a":"code","7f7c2d91":"markdown","d0531645":"markdown","fea1fdfc":"markdown","00a85434":"markdown","fb8487d3":"markdown","3b3bfdfb":"markdown","630712de":"markdown","41789d1d":"markdown","00ca5e06":"markdown","1236d6da":"markdown","b8f3e88d":"markdown","84d571f9":"markdown"},"source":{"8296f814":"!pip install ..\/input\/python-datatable\/datatable-0.11.0-cp37-cp37m-manylinux2010_x86_64.whl > \/dev\/null 2>&1","4ccfc8df":"import numpy as np\nimport pandas as pd\nfrom collections import defaultdict\nimport datatable as dt\nimport lightgbm as lgb\nfrom matplotlib import pyplot as plt\nimport riiideducation\nimport torch\n\n# Error handling, ignore all\nnp.seterr(divide = 'ignore', invalid = 'ignore')","cb76f418":"data_types_dict = {\n    'user_id': 'int32', \n    'content_id': 'int16', \n    'answered_correctly': 'int8', \n    'prior_question_elapsed_time': 'float32', \n    'prior_question_had_explanation': 'bool'\n}\n\ntarget = 'answered_correctly'","852fad44":"train_df = dt.fread('..\/input\/riiid-test-answer-prediction\/train.csv', columns = set(data_types_dict.keys())).to_pandas()","f3e3061b":"print('Training dataset detailed information')\nprint('*' * 50)\nprint('Columns:', train_df.columns)\nprint('*' * 50)\nprint('Shape:', train_df.shape)\nprint('*' * 50)\nprint('NA values in each column:', sum(train_df.isna().sum()))\nprint('*' * 50)","b436fe90":"# Exclude lectures\ntrain_df = train_df[train_df[target] != -1].reset_index(drop = True, inplace = False)\n# Fill NaN values in the 'prior_question_had_explanation' columns\ntrain_df['prior_question_had_explanation'].fillna(False, inplace = True)\n# Set type\ntrain_df = train_df.astype(data_types_dict)","16fb6680":"# Answer for the previous questions of users\ntrain_df['lag'] = train_df.groupby('user_id')[target].shift()\n# For each user (groupby('user_id')), compute the cummulative number of correct answers and number answers in general\ncum = train_df.groupby('user_id')['lag'].agg(['cumsum', 'cumcount'])\n# User correctness (measure the users' learning progress)\ntrain_df['user_correctness'] = cum['cumsum'] \/ cum['cumcount']\n# Drop the 'lag' feature\ntrain_df.drop(columns = ['lag'], inplace = True)","ceeb3206":"# Overall correctness of users\nuser_agg = train_df.groupby('user_id')[target].agg(['sum', 'count'])\n# Overall difficulty of questions\ncontent_agg = train_df.groupby('content_id')[target].agg(['sum', 'count'])","a2421eaa":"# Take only 24 last observations of each user\ntrain_df = train_df.groupby('user_id').tail(24).reset_index(drop = True)","f0125c40":"train_df","6fd95193":"questions_df = pd.read_csv(\n    '..\/input\/riiid-test-answer-prediction\/questions.csv', \n    usecols = [0, 3],\n    dtype = {'question_id': 'int16', 'part': 'int8'}\n)\ntrain_df = pd.merge(train_df, questions_df, left_on = 'content_id', right_on = 'question_id', how = 'left')\ntrain_df.drop(columns = ['question_id'], inplace = True)","6727b59b":"# How many questions have been answered in each content ID?\ntrain_df['content_count'] = train_df['content_id'].map(content_agg['count']).astype('int32')\n# How hard are questions in each content ID?\ntrain_df['content_id'] = train_df['content_id'].map(content_agg['sum'] \/ content_agg['count'])","17f1538f":"# Ratio is 6\/24 = 25%\nvalid_df = train_df.groupby('user_id').tail(6)\ntrain_df.drop(valid_df.index, inplace = True)","9db1a87a":"features = ['content_id', 'prior_question_elapsed_time', \n            'prior_question_had_explanation', 'user_correctness', \n            'part', 'content_count']\n\nparams = {\n    'loss_function': 'Logloss',\n    'eval_metric': 'AUC',\n    'task_type': 'GPU' if torch.cuda.is_available() else 'CPU',\n    'grow_policy': 'Lossguide',\n    'iterations': 2500,\n    'learning_rate': 4e-2,\n    'random_seed': 0,\n    'l2_leaf_reg': 1e-1,\n    'depth': 15,\n    'max_leaves': 10,\n    'border_count': 128,\n    'verbose': 50,\n}","faa4b226":"from catboost import CatBoostClassifier, Pool\n\n# Training and validating data\ntrain_set = Pool(train_df[features], label = train_df[target])\nval_set = Pool(valid_df[features], label = valid_df[target])","11c48309":"# Model definition\nmodel = CatBoostClassifier(**params)\n\n# Fitting\nmodel.fit(train_set, eval_set = val_set, use_best_model = True)","87fd4735":"user_sum_dict = user_agg['sum'].astype('int16').to_dict(defaultdict(int))\nuser_count_dict = user_agg['count'].astype('int16').to_dict(defaultdict(int))\ncontent_sum_dict = content_agg['sum'].astype('int32').to_dict(defaultdict(int))\ncontent_count_dict = content_agg['count'].astype('int32').to_dict(defaultdict(int))","41fc9406":"try:\n    env = riiideducation.make_env()\nexcept:\n    pass\niter_test = env.iter_test()\nprior_test_df = None","1bef201a":"%%time\n\nfor (test_df, sample_prediction_df) in iter_test:\n    if prior_test_df is not None:\n        prior_test_df[target] = eval(test_df['prior_group_answers_correct'].iloc[0])\n        prior_test_df = prior_test_df[prior_test_df[target] != -1].reset_index(drop = True)\n        \n        user_ids = prior_test_df['user_id'].values\n        content_ids = prior_test_df['content_id'].values\n        targets = prior_test_df[target].values\n        \n        for user_id, content_id, answered_correctly in zip(user_ids, content_ids, targets):\n            user_sum_dict[user_id] += answered_correctly\n            user_count_dict[user_id] += 1\n            content_sum_dict[content_id] += answered_correctly\n            content_count_dict[content_id] += 1\n\n    prior_test_df = test_df.copy()\n    \n    test_df = test_df[test_df['content_type_id'] == 0].reset_index(drop = True)\n    test_df = pd.merge(test_df, questions_df, left_on = 'content_id', right_on = 'question_id', how = 'left')\n    test_df['prior_question_had_explanation'] = test_df['prior_question_had_explanation'].fillna(False).astype('bool')    \n    \n    user_sum = np.zeros(len(test_df), dtype = np.int16)\n    user_count = np.zeros(len(test_df), dtype = np.int16)\n    content_sum = np.zeros(len(test_df), dtype = np.int32)\n    content_count = np.zeros(len(test_df), dtype = np.int32)\n    \n    for i, (user_id, content_id) in enumerate(zip(test_df['user_id'].values, test_df['content_id'].values)):\n        user_sum[i] = user_sum_dict[user_id]\n        user_count[i] = user_count_dict[user_id]\n        content_sum[i] = content_sum_dict[content_id]\n        content_count[i] = content_count_dict[content_id]\n\n    test_df['user_correctness'] = user_sum \/ user_count\n    test_df['content_count'] = content_count\n    test_df['content_id'] = content_sum \/ content_count\n       \n    test_df[target] = model.predict_proba(test_df[features])[:,1]\n    env.predict(test_df[['row_id', target]])","7f7c2d91":"# Training","d0531645":"* Construct data","fea1fdfc":"# Preprocessing","00a85434":"* Construct new features","fb8487d3":"* Question dataset comes into play","3b3bfdfb":"* Import data","630712de":"# Necessary packages","41789d1d":"# Inference","00ca5e06":"# Use the package 'datatable' for fast handling","1236d6da":"* Data config","b8f3e88d":"# Extract the validation set","84d571f9":"* Information of the training dataset"}}