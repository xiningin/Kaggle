{"cell_type":{"73ab3978":"code","30a8c884":"code","54145e47":"code","b6c4dac1":"code","172c5167":"code","3288a5fd":"code","836b6731":"code","bdb64ae8":"code","12dfbe84":"code","054d8d77":"code","4d7f87cf":"code","9a672397":"code","130433cc":"code","559306ad":"code","bc8064ad":"code","78682113":"code","439d8cf7":"code","3eb82b90":"code","bbee4f11":"code","213b47b1":"code","c321b670":"code","e41cd76e":"code","92032f6b":"code","1bd8652b":"code","15339a5a":"code","1a0f4b12":"code","4e30c997":"code","c559a3c5":"code","a609a3b1":"code","c536c252":"code","6597219e":"code","5d634b69":"code","90bffdf0":"code","134ad709":"code","1e371cc0":"code","61550ca8":"code","c7984805":"code","405ba072":"code","80f27814":"code","9b8811c2":"code","5a15a63f":"code","622516e5":"code","d179769e":"code","f8926e34":"code","97172abd":"code","de179bb7":"code","ed23f521":"code","14245e80":"code","36a709ee":"code","b950ae51":"code","8d51427f":"code","8e070572":"code","4815cc81":"code","1b724f55":"code","e0b7dd53":"markdown","df03d79c":"markdown","0d44731f":"markdown","7ae1cb75":"markdown","b8a097f2":"markdown","3a1a16b2":"markdown","6b7c928c":"markdown","610da419":"markdown","8671e54f":"markdown","2db2b734":"markdown","430b76d1":"markdown","7f95f46c":"markdown","2cd73d54":"markdown","ae88eeba":"markdown","0aee3d89":"markdown","b445c2b6":"markdown","678c6c0b":"markdown","a5e6bf6d":"markdown","fc9348b4":"markdown","34dc1a16":"markdown","23c080f5":"markdown","f8d8a908":"markdown","278f2933":"markdown","0fa3de29":"markdown","a891e697":"markdown","38ecadff":"markdown","8e3c5123":"markdown","b0407434":"markdown","68435113":"markdown","24332de1":"markdown","b89dd4ed":"markdown","f011ca41":"markdown","80051535":"markdown","0deb20b5":"markdown"},"source":{"73ab3978":"#importing base libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n\n#magic function to view plot inside jupyter notebook\n%matplotlib inline \n\n#loading datasets from Kaggle\ntrain=pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest=pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\n","30a8c884":"import missingno as msno #for visualizing missing data\n#train dataset missing data and preview\nprint(\"Train data rows x columns: \",train.shape,\"\\n\")\nprint(train.isnull().sum())\ntrain.head()","54145e47":"#test dataset missing data and preview\nprint(\"Test data rows x columns: \",train.shape,\"\\n\")\nprint(test.isnull().sum())\ntest.head()\ntrain.head()","b6c4dac1":"#visualize missing data in train as matrix\nmsno.matrix(train)","172c5167":"#visualize missing data in train as matrix\nmsno.matrix(test)","3288a5fd":"#visualize missing data in train as bar chart\nmsno.bar(train)","836b6731":"#visualize missing data in test as bar chart\nmsno.bar(test)","bdb64ae8":"#check if more than 40% of information is missing in columns for train data\nprint(train.isnull().sum()>int(0.40*train.shape[0]))","12dfbe84":"#check if more than 40% of information is missing in columns for test data\nprint(train.isnull().sum()>int(0.40*train.shape[0]))","054d8d77":"#Data is mssing in Age columns. Histogram plot for train data.\nsns.distplot(train['Age'].dropna(),hist=True, kde=True,rug=True, bins=40)","4d7f87cf":"#Histogram plot for test data\nsns.distplot(test['Age'].dropna(),hist=True, kde=True, bins=40, rug=True)","9a672397":"sns.countplot(x=\"Survived\",data=train,palette=\"deep\")","130433cc":"sns.countplot(x=\"Survived\",hue=\"Pclass\",data=train,palette=\"deep\")","559306ad":"corr =train.corr()\nsns.heatmap(corr,annot=True)","bc8064ad":"#Check the data type of each feature\ntrain.info()","78682113":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.impute import SimpleImputer","439d8cf7":"#intantiate label encoder\nle=LabelEncoder()","3eb82b90":"#Combining both datasets\ndataset=[train,test]","bbee4f11":"#Dropping the PassengerId column only in train as we equire passengerid in our test dataset for submission\ntrain.drop(\"PassengerId\",axis=1,inplace=True)","213b47b1":"train[\"Name\"]","c321b670":"for data in dataset:\n    data[\"Title\"]=data[\"Name\"].str.extract('([A-Za-z]+)\\.',expand=False)","e41cd76e":"train[\"Title\"].value_counts()","92032f6b":"test[\"Title\"].value_counts()","1bd8652b":"#create mapping\ntitle_mapping={\"Mr\":0,\"Miss\":1,\"Mrs\":2,\"Master\":3,\"Col\":3,\"Rev\":3,\"Ms\":3,\"Dr\":3,\"Dona\":3,\"Major\":3,\n         \"Mlle\":3,\"Countess\":3,\"Sir\":3,\"Jonkheer\":3,\"Don\":3,\"Mme\":3,\"Capt\":3,\"Lady\":3}","15339a5a":"for data in dataset:\n    data[\"Title\"]=data[\"Title\"].map(title_mapping)","1a0f4b12":"#check the count of survived according ot titles\nsns.countplot(x=\"Survived\",hue=\"Title\",data=train,palette=\"deep\")","4e30c997":"#Dropping the column Names as it is no longer required\nfor data in dataset:\n    data.drop(\"Name\",axis=1,inplace=True)","c559a3c5":"for data in dataset:\n    data[\"Sex\"]=le.fit_transform(data[\"Sex\"])","a609a3b1":"Skewness = 3*(train[\"Age\"].mean()-train[\"Age\"].median())\/train[\"Age\"].std()\nSkewness","c536c252":"for data in dataset:\n    data[\"Age\"].fillna(data.groupby(\"Title\")[\"Age\"].transform(\"median\"),inplace=True)","6597219e":"for data in dataset:\n    data[\"Family Size\"]=data[\"SibSp\"]+data[\"Parch\"]+1","5d634b69":"\n#dropping the Parch and SibSp columns\nfor data in dataset:\n    data.drop([\"SibSp\",\"Parch\"],axis=1,inplace=True)","90bffdf0":"#dropping Ticket column\nfor data in dataset:\n    data.drop([\"Ticket\"],axis=1,inplace=True)","134ad709":"for data in dataset:\n    data[\"Fare\"].fillna(data[\"Fare\"].mean(),inplace=True)","1e371cc0":"test[\"Cabin\"].value_counts()","61550ca8":"for data in dataset:\n    data[\"Cabin\"] = data[\"Cabin\"].str[:1]","c7984805":"cabin_mapping={\"A\":0,\"B\":0.4,\"C\":0.8,\"D\":1.2,\"E\":1.6,\"F\":2,\"G\":2.4,\"T\":2.8} #This is called feature scaling, please explore more on this advanced topic","405ba072":"for data in dataset:\n    data[\"Cabin\"] = data[\"Cabin\"].map(cabin_mapping)","80f27814":"for data in dataset:\n    data[\"Cabin\"].fillna(data.groupby(\"Pclass\")[\"Cabin\"].transform(\"median\"),inplace=True)","9b8811c2":"for data in dataset:\n    data[\"Embarked\"].fillna(data[\"Embarked\"].mode()[0],inplace=True)","5a15a63f":"#Label encoding Embarked\nfor data in dataset:\n    data[\"Embarked\"]=le.fit_transform(data[\"Embarked\"])","622516e5":"#for train dataset\nsns.heatmap(train.isnull(),cmap = 'magma' )","d179769e":"#For test dataset\nsns.heatmap(test.isnull(),cmap = 'magma' )","f8926e34":"#separating labels\ny_train = train[\"Survived\"]\n#separating features\ntrain.drop(\"Survived\", axis=1,inplace=True)\nX_train=train","97172abd":"#checking train dataset\nX_train.head()","de179bb7":"#checking test dataset after removing passengerid as a copy of test data, as we reuire passengerid in final submission on Kaggle\nX_test = test.drop(\"PassengerId\",axis=1).copy()\nX_test.head()","ed23f521":"#Finally check the shapes\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)","14245e80":"#import classifiers\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC","36a709ee":"from sklearn.model_selection import KFold, cross_val_score\n#set values for K-folds\nfolds= KFold(n_splits=10,shuffle=True,random_state=0)\nmetric=\"accuracy\"","b950ae51":"gnb=GaussianNB()\nscore= cross_val_score(gnb,X_train,y_train,cv=folds,n_jobs=1,scoring=metric)\nprint(score)","8d51427f":"#mean score rounded to 2 decimal points\nround(np.mean(score)*100,2)","8e070572":"#using the classifier that gave highest average accuracy on train dataset\nclf=GaussianNB()\nclf.fit(X_train,y_train)\ny_test = clf.predict(X_test)","4815cc81":"#wrapping up into a submission dataframe\nsubmission=pd.DataFrame({\"PassengerId\": test[\"PassengerId\"],\"Survived\":y_test})\n\n#converting to submission csv\nsubmission.to_csv(\"submission.csv\",index=False)","1b724f55":"submission=pd.read_csv(\"submission.csv\")\nsubmission.head()","e0b7dd53":"### h. Fare\nSince Fare has very few missing values, we can replace fare with its mean value","df03d79c":"## 5. Modelling\nWe will start with some basic models and then go through more advanced ensemble models. Lets drive straight into it!","0d44731f":"# Titanic: Machine Learning from Disaster","7ae1cb75":"### e. SibSp\n\nit contains information about the family, So we will combine this feature along with Parch to have a new feature called \"Family Size\"","b8a097f2":"### II. Walk Through Each Feature One by One\nHere we explore each feature one by one and check if we can directly drop it, use it or extract some information from it.","3a1a16b2":"### b. Name\nName looks like it does not contain much information but, we can still create new feature \"Title\" and extract titles from each name in test and train data set which can be informative going further. To create this new feature we can use regular expressions","6b7c928c":"## 2. Loading Datasets\nLoad the dataset from Kaggle","610da419":"### II. Data Interpretation and Visualization\nReference for seaborn: https:\/\/towardsdatascience.com\/data-visualization-using-seaborn-fc24db95a850","8671e54f":"### IV. Feature Relationships\nTo get an idea about how features are related to each other, we can generate a correlation matrix and check the pearson correlation coefficient\n\nReference: https:\/\/www.statisticshowto.com\/probability-and-statistics\/correlation-coefficient-formula\/","2db2b734":"### a. PassengerId\nThis is just the serial number identification for passengers and doe not contain much infromation. we can drop it directly","430b76d1":"## 4. Data Pre-processing and Feature Engineering","7f95f46c":"### II. Survival Prediction on Test Data","2cd73d54":"Import libraries and functions for cross validation and metrics for accuracy","ae88eeba":"### III. Count Plot for Features","0aee3d89":"#### 7 Stages for any machine learning project\n1. Problem Defination\n2. Data Collection\n3. Data Preparation\n4. Data Visulization\n5. Feature Engineering\n6. ML Modelling\n7. Model Deployment\n\n","b445c2b6":"### Using Gaussian NB Classifier","678c6c0b":"### j. Embarked\nSince Embarked has we missing values and is Categorical, we will just simply fill it by its mode from values S,C,Q.","a5e6bf6d":"Checking for null values if any:","fc9348b4":"### c. Sex \nSince sex is a categorical variable we need to encode it using some sort of encoding, maybe manually using a dictionary or by the use of label encoder. Here I have used Label Encoder.","34dc1a16":"### I. Exploring Missing Values","23c080f5":"## 3. Exploratory Data Analysis\nData Preparation & collection is important step in Machine learning.Data is not uniform sometime data is missing. For missing data we need to do EDA analysis.first find the missing data & modified it to suitable format.","f8d8a908":"## 6. Conclusion\nThis is my first submission to kaggle. I refer some codes on kaggle & applied machine learning algorithm for this problem.this is my begineer phase in machine learning world.Please upvote & support me to learn new things. \n\n","278f2933":"### I. Check Feature Data Types","0fa3de29":"## Contents:\n\n1. 7 Stages of Machine learning Project\n2. Dataset (Downloaded from Kaggle)\n3. Exploratory Data Analysis\n4. Data Pre-processing and Feature Engineering\n5. Modelling\n6. Conclusion","a891e697":"### f. Parch\nThis also contains information about no of parents and children so let us combine it with SibSip to have new feature as \"Family Size\"","38ecadff":"## 1. Project Steps\nBefore startng any machine learning project there are some basic steps that we should focus.They are as follows:","8e3c5123":"### g. Ticket\nThis feature does not contain much data so we can drop it directly","b0407434":"Fill the missing value by grouping by Pclass, since cabins are related to class of booking","68435113":"Since most of the values are in Mr, Miss, Mrs, we can include the others in separate category and hence have four categories. We could have used label encoder but lot of small categories can be clubbed together into single category","24332de1":"Skewness is positive so we can use medium to fill the missing values. But in our dataset we can do something better. We have feature \"Names\" from which we have extract titles like Mr,Mrs, master, miss, and now we can use the median of each group to fill the missing values for better accuracy of our model. ","b89dd4ed":"Check the Titles that were extracted","f011ca41":"### i. Cabin\nSince more than 40% of value in cabin data is missing, we can drop it directly if we want. But here I will extract the first letter of each cabin name and apply a numerical mapping on it and hence fill the missing values later. ","80051535":"### III. Re-Check Datasets \nit is good practice to re-check datasets before actually applying models. Check if both train and test datasets have no null values. Here we must also separate labels and features from train dataset","0deb20b5":"### d. Age\nSince has lots of missing values lets check how we can fill it. Lets first check the skewness"}}