{"cell_type":{"acebaecf":"code","6f7548b0":"code","2751d61e":"code","064aa2f8":"code","3f99585a":"code","a6c89326":"code","1e8336d6":"code","2469c79f":"code","75ed348d":"markdown","41ab482d":"markdown"},"source":{"acebaecf":"import numpy as np\nimport random\nimport gym\n\nenv = gym.make(\"FrozenLake8x8-v1\") \nenv.reset()\nenv.render()","6f7548b0":"actions = env.action_space.n\nstates = env.observation_space.n\neposides = 100000\nepsilon = 0.8\ngamma = 0.9\nalpha = 0.01","2751d61e":"# Create Q table with all rewards = 0\nq_table = np.zeros((states, actions))","064aa2f8":"# Training\navg_reward = 0\navg_steps = 0\nfor i in range(eposides):\n    env.reset()\n    done = False\n    state = 0\n    steps = 0\n    total_reward = 0\n    while not done:\n        # epsilon-greedy\n        if random.random() < epsilon:\n            action = env.action_space.sample() # Explore\n        else:\n            action = np.argmax(q_table[state,:]) # Exploit\n        \n        # Move one step\n        next_state, reward, done, _ = env.step(action)\n        \n        # Update Q table\n        q_table[state, action] = q_table[state, action] + alpha*(reward + gamma*np.max(q_table[next_state, :]) - q_table[state, action])\n        state = next_state\n        \n        # Update statistics\n        steps = steps + 1\n        total_reward = total_reward + reward\n    \n    avg_reward += total_reward\n    avg_steps += steps\n    \n    if i % 10000 == 0:\n        avg_reward \/= 1000\n        avg_steps \/= 1000\n        print(\"At Episode {}, the average steps {}, reward {}\".format(i, avg_steps, avg_reward))\n        avg_reward = 0","3f99585a":"q_table # Q table after learning","a6c89326":"# Testing: Calculating the average reward of 1000 eposides\ntest_episodes = 1000 # DON'T CHANGE THIS VALUE\nsteps = 0\ntotal_reward = 0\nfor i in range(test_episodes):\n    env.reset()\n    done = False\n    while not done:\n        action = np.argmax(q_table[state,:])\n        next_state, reward, done, _ = env.step(action)\n        state = next_state\n        steps = steps + 1\n        total_reward = total_reward + reward\n    \nprint(\"The average results of {} episodes are steps {}, reward {}\".format(test_episodes, steps\/test_episodes, total_reward\/test_episodes))","1e8336d6":"total_avg_reward = total_reward\/test_episodes\n# Print results in CSV format and upload to Kaggle\nwith open('rewards.csv', 'w') as f:\n    f.write('Id,Predicted\\n')\n    f.write('FrozenLake8x8_public,{}\\n'.format(total_avg_reward))\n    f.write('FrozenLake8x8_private,{}\\n'.format(total_avg_reward))","2469c79f":"# Download your results!\nfrom IPython.display import FileLink\nFileLink('rewards.csv')","75ed348d":"We use $Q$-learning to learn the expected values of all action-state pairs. ([Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Q-learning)). The update formula is shown below:\n\n![Q Table](https:\/\/wikimedia.org\/api\/rest_v1\/media\/math\/render\/svg\/678cb558a9d59c33ef4810c9618baf34a9577686)\n\nWe also adopt $\\epsilon$-greedy strategy to explore. ","41ab482d":"## Learning Goal\n\u200b\nIn this homework, we choose \"FrozenLake8x8-v0\" as our learning environment. The goal of FrozenLake is to go from the starting state (S) to the goal state (G) by walking only on frozen tiles (F) and avoid holes (H). However, the ice is slippery, so you won't always move in the direction you intend (stochastic environment).\n\u200b\n### Observation\nType: Discrete (64)\n\u200b\nNum | Observation (State)\n---|---\n0 - 63 | For 8x8 square, counting each position from left to right, top to bottom\n\u200b\n### Actions\nType: Discrete(4)\n\u200b\nNum | Action\n--- | ---\n0 | Move Left\n1 | Move Down\n2 | Move Right\n3 | Move Up\n\u200b\n### Reward\nReward is 0 for every step taken, 0 for falling in the hole, 1 for reaching the final goal.\n\u200b\n### Starting State\nStarting state is at the top left corner.\n\u200b\n### Episode Termination\nReaching the goal or fall into one of the holes."}}