{"cell_type":{"ba14c388":"code","d2c26eca":"code","f4ebf94f":"code","188d5e77":"code","5f1f93c3":"code","1e446e69":"code","b47946f3":"code","29db0ba2":"code","06397cd7":"code","756573d8":"code","8c74ad79":"code","118d97bc":"code","0b527adc":"code","ed5d921c":"code","9b8856c2":"code","5abe72bb":"code","0c7ac336":"code","c2226d6d":"code","df3814fb":"code","416701e0":"code","1cb2ead6":"code","267aaa9d":"code","575faa1c":"code","e0368907":"code","00529a33":"code","d662a80f":"code","a4b03893":"code","ea41ab53":"code","4ca07c69":"code","f3dc363e":"code","529f7345":"code","81f5f5dc":"code","a21bfb14":"code","bdc286c7":"code","2a73443c":"code","78df0930":"code","26e733d9":"code","5219045d":"code","b2ea48fa":"code","572e0df8":"code","028ae6d7":"code","9b7fa0e7":"code","037f2bbc":"code","7bbabf01":"code","52ddb4da":"markdown","dbd41dae":"markdown","92bf37ff":"markdown","5d26b814":"markdown","0901c155":"markdown","6a0ecfd9":"markdown","40534a2d":"markdown","c2f3daa0":"markdown","3b01a935":"markdown","40e2561e":"markdown","5aecddfe":"markdown","041639cd":"markdown","38ed329b":"markdown","2529b72f":"markdown","ae47bc2f":"markdown","8a381482":"markdown","7eb78369":"markdown","fc28ec9a":"markdown","991b4a55":"markdown","3e8721b4":"markdown","543b9bba":"markdown","22ccec80":"markdown","a4193512":"markdown","b3c68ec9":"markdown","37525caf":"markdown","fe87b055":"markdown","3883e3a1":"markdown","e34691f0":"markdown","e5b0dd59":"markdown","b99e32e8":"markdown","16bebcdd":"markdown","de11e1fe":"markdown","75bde8d1":"markdown","838f6a42":"markdown","e8e53852":"markdown","6d561672":"markdown"},"source":{"ba14c388":"import pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import cross_val_score, KFold, train_test_split\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.metrics import mean_squared_error\n\nfrom scipy.stats import mode\nfrom scipy.special import boxcox1p\n\nfrom sklearn.linear_model import Lasso\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom xgboost import XGBRegressor\n\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')","d2c26eca":"train_data = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')  # reading train data\n\n\ndata_test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')  # reading test data","f4ebf94f":"print(f'Shape of the train data:{train_data.shape}')\nprint(f'Shape of the test data:{data_test.shape}')","188d5e77":"train_data.head()","5f1f93c3":"data_test.head()","1e446e69":"features_with_nan_train = [feature for feature in train_data.columns if train_data[feature].isnull().sum() > 1]\nfeatures_with_nan_test = [feature for feature in data_test.columns if data_test[feature].isnull().sum() > 1]","b47946f3":"# Number of missing values fo each feature in the train data\nfor feature in features_with_nan_train:\n    print(f'{feature}: {train_data[feature].isnull().sum() \/ len(train_data[feature]) * 100} % missing values')","29db0ba2":"# Number of missing values fo each feature in the test data\nfor feature in features_with_nan_test:\n    print(f'{feature}: {data_test[feature].isnull().sum() \/ len(train_data[feature]) * 100} % missing values')","06397cd7":"fig, ax = plt.subplots(len(features_with_nan_train) \/\/ 3, 3, figsize=(15, 50))\ni = 0\nfor feature in features_with_nan_train:\n    column_with_feature = train_data[feature]\n    sns.distplot(train_data.query('@column_with_feature == @column_with_feature')[['SalePrice']],\n                color='b',\n                ax=ax[i \/\/ 3, i % 3])\n    sns.distplot(train_data.query('@column_with_feature != @column_with_feature')[['SalePrice']],\n                color='g', \n                ax=ax[i \/\/ 3, i % 3])\n    ax[i \/\/ 3, i % 3].set_title(feature)\n    ax[i \/\/ 3, i % 3].tick_params(axis='x', rotation=45)\n    ax[i \/\/ 3, i % 3].legend(labels=['Without_NAN', 'With_NAN'])\n    i += 1\n    \n","756573d8":"train_data = train_data.drop(['LotFrontage', 'Alley', 'Fence', 'MiscFeature', 'PoolQC'], axis=1)\ndata_test = data_test.drop(['LotFrontage', 'Alley', 'Fence', 'MiscFeature', 'PoolQC'], axis=1)","8c74ad79":"numeric_data = train_data.select_dtypes(exclude = object)","118d97bc":"year_data_col = [year_col for year_col in numeric_data.columns if 'Year' in year_col or 'Yr' in year_col]\nprint(f'Columns with year information: {year_data_col}')\nyear_data = train_data[year_data_col]","0b527adc":"descrete_data_col = [descr_col for descr_col in numeric_data.columns if numeric_data[descr_col].nunique() < 30 and \n                    descr_col not in year_data_col]\nprint(f'Columns with descrete variables: {descrete_data_col}')\ndescrete_data = train_data[descrete_data_col]","ed5d921c":"continuous_data = numeric_data.drop(year_data_col + descrete_data_col + ['Id'], axis=1)\ncontinuous_data_columns = continuous_data.columns\nprint(f'Columns with continous variables: {continuous_data_columns}')","9b8856c2":"# Histigrams of continuous features\nfig, axes = plt.subplots(len(continuous_data.columns) \/\/ 3, 3, figsize=(13, 21))\ni = 0\nfor feature in continuous_data.columns:\n    sns.histplot(continuous_data[[feature]], ax = axes[i \/\/ 3, i % 3])\n    i += 1","5abe72bb":"# Histigrams of descrete features\nfig, axes = plt.subplots(len(descrete_data.columns) \/\/ 3 + 1 , 3, figsize=(13, 21))\ni = 0\nfor feature in descrete_data.columns:\n    sns.histplot(descrete_data[[feature]], ax = axes[i \/\/ 3, i % 3])\n    i += 1","0c7ac336":"train_data = train_data.drop(['MiscVal', 'PoolArea'], axis=1)\ndata_test = data_test.drop(['MiscVal', 'PoolArea'], axis=1)","c2226d6d":"fig, axes = plt.subplots(len(continuous_data.columns) \/\/ 3, 3, figsize=(20, 35))\ni = 0\nfor feature in continuous_data.columns:\n    axes[i \/\/ 3, i % 3].scatter(continuous_data[[feature]], train_data['SalePrice'])\n    axes[i \/\/ 3, i % 3].set_xlabel(feature)\n    axes[i \/\/ 3, i % 3].set_ylabel('SalePrice')\n    i += 1","df3814fb":"fig, axes = plt.subplots(len(descrete_data.columns) \/\/ 3 + 1 , 3, figsize=(20, 35))\ni = 0\nfor feature in descrete_data.columns:\n    axes[i \/\/ 3, i % 3].scatter(descrete_data[[feature]], train_data['SalePrice'])\n    axes[i \/\/ 3, i % 3].set_xlabel(feature)\n    axes[i \/\/ 3, i % 3].set_ylabel('SalePrice')\n    i += 1","416701e0":"cat_data = train_data.select_dtypes(include=object)\ncat_data_columns = cat_data.columns\nprint(f'Columns with categorical variables: {cat_data_columns}')","1cb2ead6":"fig, axes = plt.subplots(len(cat_data_columns) \/\/ 3, 3, figsize=(20, 100))\ni = 0\nfor feature in cat_data_columns:\n    sns.boxplot(cat_data[feature], train_data['SalePrice'], ax=axes[i \/\/ 3, i % 3])\n    axes[i \/\/ 3, i % 3].tick_params(axis='x', rotation=45)\n    i += 1","267aaa9d":"# Let's change some NaN objects in the test data on new category data according to data_description.txt\n\ntrain_data['FireplaceQu'] = train_data.FireplaceQu.fillna('NF')  # House with no fireplace\ntrain_data['GarageQual'] = train_data.GarageQual.fillna('NG')  # House with no garage\ntrain_data['GarageCond'] = train_data.GarageCond.fillna('NG')  # House with no garage\ntrain_data['GarageType'] = train_data.GarageType.fillna('NG')  # House with no garage\ntrain_data['GarageFinish'] = train_data.GarageFinish.fillna('NG')  # House with no garage\ntrain_data['GarageArea'] = train_data.GarageArea.fillna(0)  # because in this particular house there is no garage\ntrain_data['GarageCars'] = train_data.GarageCars.fillna(0)  # because in this particular house there is no garage\ntrain_data['BsmtQual'] = train_data.BsmtQual.fillna('NB')  # House with no basement\ntrain_data['BsmtCond'] = train_data.BsmtCond.fillna('NB')  # House with no basement\ntrain_data['BsmtExposure'] = train_data.BsmtExposure.fillna('NB')  # House with no basement\ntrain_data['BsmtFinType1'] = train_data.BsmtFinType1.fillna('NB')  # House with no basement\ntrain_data['BsmtFinSF1'] = train_data.BsmtFinSF1.fillna(0)  # because in this particular house there is no basement\ntrain_data['BsmtFinType2'] = train_data.BsmtFinType2.fillna('NB')  # House with no basement\ntrain_data['BsmtFinSF2'] = train_data.BsmtFinSF2.fillna(0)  # because in this particular house there is no basement\ntrain_data['BsmtUnfSF'] = train_data.BsmtUnfSF.fillna(0)  # because in this particular house there is no basement\ntrain_data['TotalBsmtSF'] = train_data.TotalBsmtSF.fillna(0)  # because in this particular house there is no basement\n\n\n\n\n# Let's fill Nan values of the next features with mode value\n\ntrain_data['SaleType'] = train_data.SaleType.fillna(mode(train_data['SaleType'])[0][0]) \ntrain_data['GarageYrBlt'] = train_data.GarageYrBlt.fillna(mode(train_data['GarageYrBlt'])[0][0]) \ntrain_data['MSZoning'] = train_data.MSZoning.fillna(mode(train_data['MSZoning'])[0][0]) \ntrain_data['Functional'] = train_data.Functional.fillna(mode(train_data['Functional'])[0][0]) \ntrain_data['KitchenQual'] = train_data.KitchenQual.fillna(mode(train_data['KitchenQual'])[0][0])\ntrain_data['BsmtFullBath'] = train_data.BsmtFullBath.fillna(mode(train_data['BsmtFullBath'])[0][0]) \ntrain_data['BsmtHalfBath'] = train_data.BsmtHalfBath.fillna(mode(train_data['BsmtHalfBath'])[0][0]) \ntrain_data['Utilities'] = train_data.Utilities.fillna(mode(train_data['Utilities'])[0][0]) \ntrain_data['Exterior1st'] = train_data.Exterior1st.fillna(mode(train_data['Exterior1st'])[0][0])\ntrain_data['Exterior2nd'] = train_data.Exterior2nd.fillna(mode(train_data['Exterior2nd'])[0][0])\ntrain_data['MasVnrType'] = train_data.MasVnrType.fillna(mode(train_data['MasVnrType'])[0][0]) \ntrain_data['MasVnrArea'] = train_data.MasVnrArea.fillna(mode(train_data['MasVnrArea'])[0][0])\ntrain_data['Electrical'] = train_data.Electrical.fillna(mode(train_data['Electrical'])[0][0])","575faa1c":"# Let's change some NaN objects in the test data on new category data according to data_description.txt\n\ndata_test['FireplaceQu'] = data_test.FireplaceQu.fillna('NF')  # House with no fireplace\ndata_test['GarageQual'] = data_test.GarageQual.fillna('NG')  # House with no garage\ndata_test['GarageCond'] = data_test.GarageCond.fillna('NG')  # House with no garage\ndata_test['GarageType'] = data_test.GarageType.fillna('NG')  # House with no garage\ndata_test['GarageFinish'] = data_test.GarageFinish.fillna('NG')  # House with no garage\ndata_test['GarageArea'] = data_test.GarageArea.fillna(0)  # because in this particular house there is no garage\ndata_test['GarageCars'] = data_test.GarageCars.fillna(0)  # because in this particular house there is no garage\ndata_test['BsmtQual'] = data_test.BsmtQual.fillna('NB')  # House with no basement\ndata_test['BsmtCond'] = data_test.BsmtCond.fillna('NB')  # House with no basement\ndata_test['BsmtExposure'] = data_test.BsmtExposure.fillna('NB')  # House with no basement\ndata_test['BsmtFinType1'] = data_test.BsmtFinType1.fillna('NB')  # House with no basement\ndata_test['BsmtFinSF1'] = data_test.BsmtFinSF1.fillna(0)  # because in this particular house there is no basement\ndata_test['BsmtFinType2'] = data_test.BsmtFinType2.fillna('NB')  # House with no basement\ndata_test['BsmtFinSF2'] = data_test.BsmtFinSF2.fillna(0)  # because in this particular house there is no basement\ndata_test['BsmtUnfSF'] = data_test.BsmtUnfSF.fillna(0)  # because in this particular house there is no basement\ndata_test['TotalBsmtSF'] = data_test.TotalBsmtSF.fillna(0)  # because in this particular house there is no basement\n\n\n\n\n# Let's fill Nan values of the next features with mode value\n\ndata_test['SaleType'] = data_test.SaleType.fillna(mode(pd.concat([train_data['SaleType'],              \n                                                                  data_test['SaleType']], axis=0))[0][0]) \ndata_test['GarageYrBlt'] = data_test.GarageYrBlt.fillna(mode(pd.concat([train_data['GarageYrBlt'],              \n                                                                  data_test['GarageYrBlt']], axis=0))[0][0]) \ndata_test['MSZoning'] = data_test.MSZoning.fillna(mode(pd.concat([train_data['MSZoning'],               \n                                                                  data_test['MSZoning']], axis=0))[0][0]) \ndata_test['Functional'] = data_test.Functional.fillna(mode(pd.concat([train_data['Functional'],           \n                                                                  data_test['Functional']], axis=0))[0][0]) \ndata_test['KitchenQual'] = data_test.KitchenQual.fillna(mode(pd.concat([train_data['KitchenQual'],        \n                                                                  data_test['KitchenQual']], axis=0))[0][0])\ndata_test['BsmtFullBath'] = data_test.BsmtFullBath.fillna(mode(pd.concat([train_data['BsmtFullBath'],       \n                                                                  data_test['BsmtFullBath']], axis=0))[0][0]) \ndata_test['BsmtHalfBath'] = data_test.BsmtHalfBath.fillna(mode(pd.concat([train_data['BsmtHalfBath'],      \n                                                                  data_test['BsmtHalfBath']], axis=0))[0][0]) \ndata_test['Utilities'] = data_test.Utilities.fillna(mode(pd.concat([train_data['Utilities'],     \n                                                                  data_test['Utilities']], axis=0))[0][0]) \ndata_test['Exterior1st'] = data_test.Exterior1st.fillna(mode(pd.concat([train_data['Exterior1st'],       \n                                                                  data_test['Exterior1st']], axis=0))[0][0])\ndata_test['Exterior2nd'] = data_test.Exterior2nd.fillna(mode(pd.concat([train_data['Exterior2nd'],     \n                                                                  data_test['Exterior2nd']], axis=0))[0][0])\ndata_test['MasVnrType'] = data_test.MasVnrType.fillna(mode(pd.concat([train_data['MasVnrType'],      \n                                                                  data_test['MasVnrType']], axis=0))[0][0]) \ndata_test['MasVnrArea'] = data_test.MasVnrArea.fillna(mode(pd.concat([train_data['MasVnrArea'],      \n                                                                  data_test['MasVnrArea']], axis=0))[0][0])","e0368907":"print(f'Remaning missing values in train data: {train_data.isnull().sum().sum()}')\nprint(f'Remaning missing values in test data: {data_test.isnull().sum().sum()}')","00529a33":"# Distribution of the target variable before and after log scaling\nsns.displot(train_data.SalePrice, kde=True)\nplt.title('Dependent variable distribution')\n\nsns.displot(train_data.SalePrice, log_scale=True, kde=True)\nplt.title('Dependent variable distribution after log scaling')","d662a80f":"target = np.log1p(train_data.SalePrice)\ntrain_data = train_data.drop(['SalePrice'], axis=1)","a4b03893":"# sapmles for test data without missing values\nnumeric_train_data = train_data.select_dtypes(exclude = object)\ncat_train_data = train_data.drop(numeric_train_data.columns, axis=1)\n\n# sapmles for test data without missing values\nnumeric_test_data = data_test.select_dtypes(exclude = object)\ncat_test_data = data_test.drop(numeric_test_data.columns, axis=1)","ea41ab53":"numeric_train_transf = pd.DataFrame()\nnumeric_test_transf = pd.DataFrame()\n\n# Using boxcox1p transformation transform numeric data of train and test datasets\nfor feature in numeric_train_data.columns:\n    numeric_train_transf[feature] = pd.Series(boxcox1p(np.array(numeric_train_data[feature]), 0.15))\n    numeric_test_transf[feature] = pd.Series(boxcox1p(np.array(numeric_test_data[feature]), 0.15))","4ca07c69":"encode_cat_train_data = pd.get_dummies(cat_train_data)\nencode_cat_test_data = pd.get_dummies(cat_test_data)","f3dc363e":"# selecting columns which are in the encode_cat_train_data and in encode_cat_train_data\nencode_cat_train_data = encode_cat_train_data[encode_cat_test_data.columns]","529f7345":"train = pd.concat([numeric_train_transf.reset_index(drop=True),\n                  encode_cat_train_data.reset_index(drop=True)], \n                  axis=1).drop(['Id'], axis=1)\n\ntest = pd.concat([numeric_test_transf.reset_index(drop=True),\n                  encode_cat_test_data.reset_index(drop=True)], \n                  axis=1).drop(['Id'], axis=1)","81f5f5dc":"# this function will evaluate a model\ndef score(model):\n    kf_splits = KFold(5, shuffle=True)  # using KFold to shuffle dataset\n    splits = kf_splits.split(train, target)\n    score = cross_val_score(model, train, target, cv=splits).mean()\n    return score","a21bfb14":"xgb_reg = XGBRegressor(n_estimators=4000,\n                      max_depth=5, \n                      learning_rate=0.03)\nscore(xgb_reg)","bdc286c7":"lasso_reg = Lasso(alpha=0.000552)\nscore(lasso_reg)","2a73443c":"gb_reg = GradientBoostingRegressor(learning_rate=0.03,\n                                  n_estimators=4000,\n                                  min_samples_split=8,\n                                  min_samples_leaf=3,\n                                  max_depth=5)\nscore(gb_reg)","78df0930":"X_train, X_test, y_train, y_test = train_test_split(train, target, test_size=0.2)","26e733d9":"xgb_reg.fit(X_train, y_train)\nlasso_reg.fit(X_train, y_train)\ngb_reg.fit(X_train, y_train)","5219045d":"pred_xgb_reg = np.expm1(xgb_reg.predict(X_test))\npred_lasso_reg = np.expm1(lasso_reg.predict(X_test))\npred_gb_reg = np.expm1(gb_reg.predict(X_test))","b2ea48fa":"final_pred = (pred_xgb_reg * 0.35 + pred_gb_reg * 0.3 + 0.35 * pred_lasso_reg) ","572e0df8":"# Using rmse to evaluate the model\nprint(f'RMSE of the combination of models:{np.sqrt(mean_squared_error(np.expm1(y_test), final_pred)) \/ np.expm1(y_test).mean()}')\nprint(f'RMSE of the XGBRegreesor:{np.sqrt(mean_squared_error(np.expm1(y_test), pred_xgb_reg)) \/ np.expm1(y_test).mean()}')\nprint(f'RMSE of the GradientBoostingRegressor:{np.sqrt(mean_squared_error(np.expm1(y_test), pred_gb_reg)) \/ np.expm1(y_test).mean()}')\nprint(f'RMSE of the Lasso:{np.sqrt(mean_squared_error(np.expm1(y_test), pred_lasso_reg)) \/ np.expm1(y_test).mean()}')\n","028ae6d7":"xgb_reg.fit(train, target)\nlasso_reg.fit(train, target)\ngb_reg.fit(train, target)\n\npred_xgb_reg = np.expm1(xgb_reg.predict(test))\npred_lasso_reg = np.expm1(lasso_reg.predict(test))\npred_gb_reg = np.expm1(gb_reg.predict(test))\n\nfinal_pred = (pred_xgb_reg * 0.35 + pred_gb_reg * 0.3 + 0.35 * pred_lasso_reg) \nsubmission = final_pred","9b7fa0e7":"subm_data = {'Id':data_test.Id,\n                         'SalePrice':pd.Series(submission)}\nsubmission = pd.DataFrame(subm_data)","037f2bbc":"submission","7bbabf01":"submission.to_csv('submission.csv', index=False)","52ddb4da":"Find out distribution of the categorical data, using box plot","dbd41dae":"### GradientBoostingRegressor","92bf37ff":"### Plan for the step:\n* filling missing values;\n* data transformation;\n* categorical data encoding;","5d26b814":"## Categorical data","0901c155":"Also in numeric variables there are descrete variables. Let descrete variables are variables which have less than 30 unique values.","6a0ecfd9":"### XGBRegressor","40534a2d":"### Transformation of remaning data","c2f3daa0":"### Plan:\n* 1. Missing values;\n* 2. Splitting datasets on samples with numeric variables and categorical variables;\n* 3. Plotting distributions of numeric varibles;\n* 4. Exploration of categorical variables;\n* 5. Outliers exploration.","3b01a935":"### Conclusion of data exploration:\n* find out some defference in distributions between data with missing values and the same data without missing values;\n* find out that distribution of many features has heavy tails;\n* find out that dependent have linear dependance with some numeric features;\n* there are a lot of outliers in data.","40e2561e":"There are some heavy tailed distribution. On feature engineering step data transformation will be used.\n<br> Also there are two features (MiscVal, PoolArea), mosly having only one value. This features won't contribute to model, so let's drop this columns.","5aecddfe":"##  Feature engineering","041639cd":"# Data Description","38ed329b":"As we can see some of the features with NAN objects have different SalePrice distribution from the SalePrice distribution of this features without NAN objects. So we should fill missing values on feature engineering step. \n<br> Also some of the features with NAN values have the same distribution as this features without NAN objects: LotFrontage, Alley, Fence, MiscFeature. This features have high rate of missing values, so let's drop them. \n<br> There is one feature with almost 100% rate of missing values (PoolQC). So let's drop this one too.","2529b72f":"### Filling missing values","ae47bc2f":"This dataset contains information about houses. There are 79 variables describing different aspects of a house, such as number of bedrooms, living area, rate of overall conditions. \n<br> The dependent variable is sale price of a house. So the goal is to predict price of a house.","8a381482":"Let's plot distribution for SalePrice for features_with_nan_train with and without missing values","7eb78369":"Let's apply boxcox1p transformation for the numeric data, except SalePrice.","fc28ec9a":"Let's plot distribution of descrete and continuous data","991b4a55":"## Numeric Variables","3e8721b4":"And in numeric data there are continuous variables. These are all the remaining variables except Id.","543b9bba":"Let's fill missing values of categorical data with new category 'No_Feature'. And fill missing values of numeric data with mode value.","22ccec80":"### Lasso","a4193512":"Error of the final model (combination of GradientBoostingRegressor, XGBRegreesor and Lasso) in the most cases will be less, than the error of one of this models.","b3c68ec9":"Let's plot graphs, showing dependance between descrete variables and dependent variable (SalePrice).","37525caf":"There are a lot of outliers in categorical data. On the feature engineering step categorical data encode and data tranformation will be used.","fe87b055":"# Model creating ","3883e3a1":"### Target variable transformation","e34691f0":"In dataset there are several features demonstrating information about date. Let's put this data in another dataframe","e5b0dd59":"As we can see all the models have practicaly the same score. So we can combine the final prediction of the predictions of these three models.","b99e32e8":"### Categorical data encoding","16bebcdd":"### Data joining","de11e1fe":"## Missing values","75bde8d1":"Let's plot graphs, showing dependance between continuous variables and dependent variable (SalePrice).","838f6a42":"Lets' create some base model and select some for future improving ","e8e53852":"As we can see after log-scaling distribution became more like normal and doesn't have heavy tales. Let's apply log1p transformation to target.","6d561672":"# EDA "}}