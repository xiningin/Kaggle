{"cell_type":{"9d3d3b2d":"code","a5c1f423":"code","a8a1d836":"code","90dc421a":"code","086e4ed0":"code","45d08275":"code","6730483d":"code","0dbe2576":"code","1f814be0":"code","ad9480ba":"code","2e82cd14":"code","e966a68a":"code","9b14a634":"code","c799209c":"code","3d696a31":"code","e88f76c6":"code","ae2b4e1a":"code","c2e87940":"code","d4630e94":"code","da806796":"code","c6929495":"code","43be1f6b":"code","e827bffb":"code","cb2a65e9":"code","ea6147b5":"code","55cdbc4d":"code","5bb5097e":"markdown","da018de0":"markdown","0838ded2":"markdown","24ab3f70":"markdown","8964a325":"markdown","a00a0301":"markdown","d36da578":"markdown","8208e0f8":"markdown"},"source":{"9d3d3b2d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport scipy.stats as stat\nimport warnings\nwarnings.simplefilter(\"ignore\")\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a5c1f423":"data = pd.read_csv(\"..\/input\/insurance\/insurance.csv\")\n#First Look\ndata.head(10)","a8a1d836":"data.info()","90dc421a":"# \u0130nfo Data\ncategoric = data.select_dtypes(\"object\")\nnumeric = data.select_dtypes([\"int\",\"float\"])\nprint(\"Number of NA observations in the data : {}\".format(data.isna().sum().sum()))\nprint(\"Number of categorical column : {}\".format(len(data.select_dtypes(\"object\").columns)))\nprint(\"Number of numeric column : {}\".format(len(data.select_dtypes([\"int\",\"float\"]).columns)))","086e4ed0":"# Describe Data\ndata.describe().T","45d08275":"import seaborn as sns\nimport matplotlib.pyplot as plot","6730483d":"sns.pairplot(data)","0dbe2576":"# Dist. of BMI\nsns.distplot(data.bmi,color= \"red\")\nprint(\"P Value for Shapiro-Wilks test : {}\".format(stat.shapiro(data.bmi)[1]))\nprint(\"Skewness : {}\".format(stat.skew(data.bmi)))\nprint(\"BMI is not normal.\")","1f814be0":"# Charges-Bmi\nsns.scatterplot(data.charges,data.bmi,alpha= 1)","ad9480ba":"data = data[data[\"bmi\"]<=50]\nsns.distplot(data.bmi,color= \"red\")\nprint(\"P Value for Shapiro-Wilks test : {}\".format(stat.shapiro(data.bmi)[1]))\nprint(\"Skewness : {}\".format(stat.skew(data.bmi)))\nprint(\"BMI still not normal but now closer to normal.\")","2e82cd14":"# Charges\nsns.distplot(data.charges,color = \"red\")\nprint(\"P Value for Shapiro-Wilks test : {}\".format(stat.shapiro(data.charges)[1]))\nprint(\"Skewness : {}\".format(stat.skew(data.charges)))\nprint(\"Charges is not normal.\")","e966a68a":"# Since the target variable is not normal, I will apply transforming techniques\n# Log Transform\nlog_trans = np.log1p(data.charges)\nsns.distplot(log_trans,color = \"red\")\nprint(\"P Value for Shapiro-Wilks test : {}\".format(stat.shapiro(log_trans)[1]))\nprint(\"Skewness : {}\".format(stat.skew(log_trans)))\nprint(\"Charges is not normal.\")","9b14a634":"# Root Square Transform\nsqrt_trans = np.sqrt(data.charges)\nsns.distplot(sqrt_trans,color = \"red\")\nprint(\"P Value for Shapiro-Wilks test : {}\".format(stat.shapiro(sqrt_trans)[1]))\nprint(\"Skewness : {}\".format(stat.skew(sqrt_trans)))\nprint(\"Charges is not normal.\")","c799209c":"categoric = data.select_dtypes(\"object\")\nnumeric = data.select_dtypes([\"int\",\"float\"])\ndummy = pd.get_dummies(categoric,drop_first=True)\ndata.drop(categoric.columns,axis = 1,inplace = True)\ndata  = pd.concat([dummy,data],axis = 1)","3d696a31":"X =data.drop(\"charges\",axis = 1)\ny = np.sqrt(data.charges)","e88f76c6":"from sklearn.metrics import mean_squared_error,mean_absolute_error\nfrom sklearn.preprocessing import StandardScaler,PolynomialFeatures\nfrom sklearn.model_selection import KFold,cross_val_predict\nfrom sklearn.linear_model import LinearRegression,Lasso,Ridge\nfrom sklearn.metrics import r2_score\nfrom sklearn.pipeline import Pipeline\nfrom scipy.special import inv_boxcox\nkf = KFold(shuffle=True,random_state=42,n_splits=5)\nscale = StandardScaler()","ae2b4e1a":"# Simple Linear Regression\nscores = []\nlr = LinearRegression()\nfor train_index,test_index in kf.split(X):\n    X_train,X_test,y_train,y_test = (X.iloc[train_index,:],X.iloc[test_index,:],\n                                     y.iloc[train_index],y.iloc[test_index])\n    \n    model = lr.fit(X_train,y_train)\n    pred = model.predict(X_test)\n    #pred = inv_boxcox(pred,lambd)\n    scores.append(r2_score(pred,y_test))\n    plot.scatter(y_test.values,pred)\n    plot.show()\nprint(scores)","c2e87940":"# Lasso\nalphas = np.geomspace(0.00001,10,num = 20) \nscores = []\nfor alpha in alphas:\n    lasso = Lasso(alpha=alpha,max_iter = 100000)\n    estimator = Pipeline([(\"scaler\",scale),(\"lasso_regression\",lasso)])\n    predictions = cross_val_predict(estimator,X,y,cv=kf)\n    scores.append(r2_score(y,predictions))\n\n\nplot.semilogx(alphas,scores,\"-*\")\nr2_lasso = pd.DataFrame(zip(alphas,scores),columns=[\"Alpha\",\"R2_Score\"])","d4630e94":"r2_lasso","da806796":"# Lasso with add Polynomial Features\npf = PolynomialFeatures(degree = 2)\nscores = []\nalphas = np.geomspace(0.00001,1,num = 10) \n\nfor alpha in alphas:\n    lasso = Lasso(alpha=alpha,max_iter = 100000)\n    estimator = Pipeline([(\"polynomial_feature\",pf),(\"scaler\",scale),(\"lasso_regression\",lasso)])\n    predictions = cross_val_predict(estimator,X,y,cv=kf)\n    scores.append(r2_score(y,predictions))\n    print(\"For Alpha :: {}\".format(alpha),\"----> Root Mean Squared Error : {}\".format(np.sqrt(mean_squared_error(y,predictions))))\n\n\nplot.semilogx(alphas,scores,\"-*\")\npf_lasso_r2 = pd.DataFrame(list(zip(alphas,scores)),columns=[\"Alpha\",\"R2_Score\"])","c6929495":"pf_lasso_r2","43be1f6b":"# Ridge\n\npf = PolynomialFeatures(degree = 2)\nscores = []\nalphas = np.geomspace(0.0001,20,num = 15) \n\nfor alpha in alphas:\n    ridge = Ridge(alpha=alpha,max_iter = 100000)\n    estimator = Pipeline([(\"polynomial_feature\",pf),(\"scaler\",scale),(\"ridge_regression\",ridge)])\n    predictions = cross_val_predict(estimator,X,y,cv=kf)\n    print(\"For Alpha :: {}\".format(alpha),\"----> Root Mean Squared Error : {}\".format(np.sqrt(mean_squared_error(y,predictions))))\n    scores.append(r2_score(y,predictions))\nplot.semilogx(alphas,scores,\"-o\")\nridge_r2 = pd.DataFrame(list(zip(alphas,scores)),columns=[\"Alpha\",\"R2_Score\"])","e827bffb":"ridge_r2","cb2a65e9":"from sklearn.model_selection import GridSearchCV\nestimator = Pipeline([(\"polynomial\",PolynomialFeatures(include_bias=False)),\n                      (\"scale\",scale),\n                      (\"ridge_regression\",Ridge())])\nparams = {\"polynomial__degree\":[1,2,3],\n          \"ridge_regression__alpha\":np.geomspace(0.001,10,20)}\ngrid = GridSearchCV(estimator,params,cv = kf)\ngrid.fit(X,y)\ngrid.best_score_","ea6147b5":"grid.best_params_","55cdbc4d":"estimator = Pipeline([(\"polynomial\",PolynomialFeatures(degree = 2,include_bias=False)),\n                      (\"scale\",scale),\n                      (\"ridge_regression\",Ridge(alpha=1.438449888287663))])\nestimator.fit(X_train,y_train)\npredict = estimator.predict(X_test)\n\nprint(\"R2 Score for Ridge Regression : {}\".format(r2_score(predict,y_test)))\nprint(\"Root Mean Squared Error : {}\".format(np.sqrt(mean_squared_error(y_test,predict))))\nprint(\"Mean Absolute Error : {}\".format(mean_absolute_error(predict,y_test)))","5bb5097e":"## Visualization","da018de0":"* Conclusion ::\nAlthough Lasso gave a higher R2 score than ridge, I set up the main model with Ridge because Lasso is running very slow. The optimal parameter for the Ridge was found to be alpha = 1.4384988, and the model established with optimal parameters could be explained by about 83%. If you find it useful, please do not be afraid to give ops and we can speak in the comments to guide me.\nThanks :)","0838ded2":"# Main Model","24ab3f70":"* We discard those with more than 50 variables from BMI","8964a325":"* As alpha values \u200b\u200bincrease, while exchanging variance and deviation, as the alpha increases, the model will be more biased and the R2 score decreases. I'm going to keep the alpha as small as possible to achieve this unwanted state balance.","a00a0301":"* Still not compatible enough but close. I will apply the sqrt transform","d36da578":"## Encoding","8208e0f8":"# Model"}}