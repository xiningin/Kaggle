{"cell_type":{"01f29dc2":"code","5a2af207":"code","5384b85a":"code","01ccf638":"code","63a0921b":"code","6e360b06":"code","8590942f":"code","19ef9de4":"code","b7fefb1f":"code","ade08bfd":"code","5ed85623":"code","e249a279":"code","13f7bd0f":"code","60867480":"code","2d040601":"code","40cd254e":"code","5649cb70":"code","939ee62f":"code","ce2be2b6":"code","b14645e1":"code","283b7a0f":"code","5e0524b6":"code","c8885dfe":"code","c0bc0f27":"code","6e7a098a":"code","37362003":"code","93cc0cc2":"code","5e86e341":"code","1fb485e4":"code","22d6b58a":"code","d14f140a":"code","6e793087":"code","01b756f0":"code","7a0e436b":"code","3af8257e":"code","c0dd8a51":"code","4f076f22":"code","b325220a":"code","dcf0ef9d":"code","9a423dbd":"code","45aac185":"code","cb70eb80":"code","c313b00f":"code","07948a94":"code","e4d52a29":"code","14741ff2":"code","23af52c4":"code","bd897c9e":"code","83d3b2ed":"code","5e37d45a":"code","727c1eb1":"code","316ab62b":"code","a57df332":"code","f3e73e0f":"code","ab0bccc7":"code","3a4d1963":"markdown","3932f0c8":"markdown","bc4736a9":"markdown","329ae869":"markdown","b2e030a1":"markdown","90861813":"markdown","63e6c539":"markdown","5dc13b07":"markdown","e7afd43f":"markdown","c95f3d15":"markdown","55cc4424":"markdown","be566996":"markdown","48fc6491":"markdown","6de23479":"markdown","65b2c275":"markdown","67ca0b4c":"markdown","cd96a2d4":"markdown"},"source":{"01f29dc2":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline \n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier","5a2af207":"diabetes_df=pd.read_csv('..\/input\/pima-indians-diabetes-database\/diabetes.csv')\ndiabetes_df","5384b85a":"diabetes_df.info()","01ccf638":"# Check for Correlation \nsns.heatmap(diabetes_df.corr(), annot=True)","63a0921b":"#Pair plot\nsns.pairplot(diabetes_df)","6e360b06":"# I feek There is a relationship between Glucose and Outcome\n\nsns.regplot(x=\"Glucose\", y=\"Outcome\", data=diabetes_df)","8590942f":"sns.regplot(x=\"Insulin\", y=\"Outcome\", data=diabetes_df)","19ef9de4":"data=diabetes_df\ndata","b7fefb1f":"print(\"total number of rows : {0}\".format(len(data)))\nprint(\"number of rows having 0 as Value for glucose: {0}\".format(len(data.loc[data['Glucose'] == 0])))\nprint(\"number of rows having 0 as Value for Pregnancies: {0}\".format(len(data.loc[data['Pregnancies'] == 0])))\nprint(\"number of rows having 0 as Value for BloodPressure: {0}\".format(len(data.loc[data['BloodPressure'] == 0])))\nprint(\"number of rows having 0 as Value for insulin: {0}\".format(len(data.loc[data['Insulin'] == 0])))\nprint(\"number of rows having 0 as Value for bmi: {0}\".format(len(data.loc[data['BMI'] == 0])))\nprint(\"number of rows having 0 as Value for DiabetesPedigreeFunction: {0}\".format(len(data.loc[data['DiabetesPedigreeFunction'] == 0])))\nprint(\"number of rows having 0 as Value for age: {0}\".format(len(data.loc[data['Age'] == 0])))\nprint(\"number of rows having 0 as Value for SkinThickness: {0}\".format(len(data.loc[data['SkinThickness'] == 0])))","ade08bfd":"from sklearn.impute import SimpleImputer\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean') # in case wanna replace NaN with Mean","5ed85623":"X = data.drop('Outcome' , axis=1)\ny = data['Outcome']\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state=10)\n\n\nfill_values = SimpleImputer(missing_values=0, strategy=\"mean\")  # In case replace 0 with mean\n\nX_train = fill_values.fit_transform(X_train)\nX_test = fill_values.fit_transform(X_test)","e249a279":"target_df =diabetes_df['Outcome']\n\ntrain_df = diabetes_df.drop(columns={'Outcome'}, axis =1)","13f7bd0f":"# Splits the dataset\nX_train2,X_test2,y_train2,y_test2=train_test_split(train_df,target_df,test_size=0.3,random_state=0)","60867480":"# pipeline 1\npipeline_lr=Pipeline([('scalar1',StandardScaler()),\n                     ('pca1',PCA(n_components=2)),\n                     ('lr_classifier',LogisticRegression(random_state=0))])","2d040601":"# Pipeline 2\n\npipeline_dt=Pipeline([('scalar2',StandardScaler()),\n                     ('pca2',PCA(n_components=2)),\n                     ('dt_classifier',DecisionTreeClassifier())])","40cd254e":"# Pipeline 3\n\npipeline_randomforest=Pipeline([('scalar3',StandardScaler()),\n                     ('pca3',PCA(n_components=2)),\n                     ('rf_classifier',RandomForestClassifier())])","5649cb70":"## LEts make the list of pipelines\npipelines = [pipeline_lr, pipeline_dt, pipeline_randomforest]","939ee62f":"best_accuracy=0.0\nbest_classifier=0\nbest_pipeline=\"\"","ce2be2b6":"# Dictionary of pipelines and classifier types for ease of reference\npipe_dict = {0: 'Logistic Regression', 1: 'Decision Tree', 2: 'RandomForest'}\n\n# Fit the pipelines\nfor pipe in pipelines:\n\tpipe.fit(X_train2, y_train2)","b14645e1":"for i,model in enumerate(pipelines):\n    print(\"{} Test Accuracy: {}\".format(pipe_dict[i],model.score(X_test2,y_test2)))","283b7a0f":"# Print most accurate model\n\nfor i,model in enumerate(pipelines):\n    if model.score(X_test2,y_test2)>best_accuracy:\n        best_accuracy=model.score(X_test2,y_test2)\n        best_pipeline=model\n        best_classifier=i\nprint('Classifier with best accuracy:{}'.format(pipe_dict[best_classifier]))","5e0524b6":"# Import the GridSerachCV\n\nfrom sklearn.model_selection import GridSearchCV","c8885dfe":"# Create a pipeline\npipe = Pipeline([(\"classifier\", RandomForestClassifier())])\n# Create dictionary with candidate learning algorithms and their hyperparameters\ngrid_param = [\n                {\"classifier\": [LogisticRegression()],\n                 \"classifier__penalty\": ['l2','l1'],\n                 \"classifier__C\": np.logspace(0, 4, 10)\n                 },\n                {\"classifier\": [LogisticRegression()],\n                 \"classifier__penalty\": ['l2'],\n                 \"classifier__C\": np.logspace(0, 4, 10),\n                 \"classifier__solver\":['newton-cg','saga','sag','liblinear'] ##This solvers don't allow L1 penalty\n                 },\n                {\"classifier\": [RandomForestClassifier()],\n                 \"classifier__n_estimators\": [10, 100, 1000],\n                 \"classifier__max_depth\":[5,8,15,25,30,None],\n                 \"classifier__min_samples_leaf\":[1,2,5,10,15,100],\n                 \"classifier__max_leaf_nodes\": [2, 5,10]}]\n\n\n# create a gridsearch of the pipeline, the fit the best model\ngridsearch = GridSearchCV(pipe, grid_param, cv=5, verbose=0,n_jobs=-1) # Fit grid search\nbest_model2 = gridsearch.fit(X_train2,y_train2)","c0bc0f27":"print(best_model2.best_estimator_)\nprint(\"The mean accuracy of the model is:\",best_model2.score(X_test2,y_test2))","6e7a098a":"print('Prediction score for original Data')\n\n# Dictionary of pipelines and classifier types for ease of reference\npipe_dict = {0: 'Logistic Regression', 1: 'Decision Tree', 2: 'RandomForest'}\n\n# Fit the pipelines\nfor pipe in pipelines:\n\tpipe.fit(X_train2, y_train2)\n    \nfor i,model in enumerate(pipelines):\n    print(\"{} Test Accuracy: {}\".format(pipe_dict[i],model.score(X_test2,y_test2)))","37362003":"print('Prediction score for Imputed Data')\n\n# Dictionary of pipelines and classifier types for ease of reference\npipe_dict = {0: 'Logistic Regression', 1: 'Decision Tree', 2: 'RandomForest'}\n\n# Fit the pipelines\nfor pipe in pipelines:\n\tpipe.fit(X_train, y_train)\n    \nfor i,model in enumerate(pipelines):\n    print(\"{} Test Accuracy: {}\".format(pipe_dict[i],model.score(X_test,y_test)))","93cc0cc2":"# Appling it on imputed data\n\n# Create a pipeline\npipe = Pipeline([(\"classifier\", RandomForestClassifier())])\n# Create dictionary with candidate learning algorithms and their hyperparameters\ngrid_param = [\n                {\"classifier\": [LogisticRegression()],\n                 \"classifier__penalty\": ['l2','l1'],\n                 \"classifier__C\": np.logspace(0, 4, 10)\n                 },\n                {\"classifier\": [LogisticRegression()],\n                 \"classifier__penalty\": ['l2'],\n                 \"classifier__C\": np.logspace(0, 4, 10),\n                 \"classifier__solver\":['newton-cg','saga','sag','liblinear'] ##This solvers don't allow L1 penalty\n                 },\n                {\"classifier\": [RandomForestClassifier()],\n                 \"classifier__n_estimators\": [10, 100, 1000],\n                 \"classifier__max_depth\":[5,8,15,25,30,None],\n                 \"classifier__min_samples_leaf\":[1,2,5,10,15,100],\n                 \"classifier__max_leaf_nodes\": [2, 5,10]}]\n\n\n# create a gridsearch of the pipeline, the fit the best model\ngridsearch = GridSearchCV(pipe, grid_param, cv=5, verbose=0,n_jobs=-1) # Fit grid search\nbest_model = gridsearch.fit(X_train,y_train)","5e86e341":"print('Original Data')\nprint(best_model2.best_estimator_)\nprint(\"The mean accuracy of the model is:\",best_model2.score(X_test2,y_test2))","1fb485e4":"\nprint('Imputed Data')\nprint(best_model.best_estimator_)\nprint(\"The mean accuracy of the model is:\",best_model.score(X_test,y_test))","22d6b58a":"## Apply Algorithm\n\nfrom sklearn.ensemble import RandomForestClassifier\nrandom_forest_model = RandomForestClassifier(random_state=10)\n\nrandom_forest_model.fit(X_train, y_train.ravel())","d14f140a":"predict_train_data = random_forest_model.predict(X_test)\n\nfrom sklearn import metrics\n\nprint(\"Accuracy = {0:.3f}\".format(metrics.accuracy_score(y_test, predict_train_data)))","6e793087":"## Hyperparameter optimization using RandomizedSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nimport xgboost","01b756f0":"classifier=xgboost.XGBClassifier()","7a0e436b":"## Hyper Parameter Optimization\n\nparams={\n \"learning_rate\"    : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ] ,\n \"max_depth\"        : [ 3, 4, 5, 6, 8, 10, 12, 15],\n \"min_child_weight\" : [ 1, 3, 5, 7 ],\n \"gamma\"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],\n \"colsample_bytree\" : [ 0.3, 0.4, 0.5 , 0.7 ]\n    \n}","3af8257e":"random_search=RandomizedSearchCV(classifier,param_distributions=params,n_iter=5,scoring='roc_auc',n_jobs=-1,cv=5,verbose=3)","c0dd8a51":"def timer(start_time=None):\n    if not start_time:\n        start_time = datetime.now()\n        return start_time\n    elif start_time:\n        thour, temp_sec = divmod((datetime.now() - start_time).total_seconds(), 3600)\n        tmin, tsec = divmod(temp_sec, 60)\n        print('\\n Time taken: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))","4f076f22":"from datetime import datetime\n# Here we go\nstart_time = timer(None) # timing starts from this point for \"start_time\" variable\nrandom_search.fit(X,y.ravel())\ntimer(start_time) # timing ends here for \"start_time\" variable","b325220a":"random_search.best_estimator_","dcf0ef9d":"classifier=xgboost.XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=0.5, gamma=0.3, gpu_id=-1,\n              importance_type='gain', interaction_constraints='',\n              learning_rate=0.05, max_delta_step=0, max_depth=3,\n              min_child_weight=3,monotone_constraints='()',\n              n_estimators=100, n_jobs=4, num_parallel_tree=1, random_state=0,\n              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n              tree_method='exact', validate_parameters=1, verbosity=None)","9a423dbd":"\nfrom sklearn.model_selection import cross_val_score\nscore=cross_val_score(classifier,X_train,y_train.ravel(),cv=10)\n\n# For original data\nscore_o=cross_val_score(classifier,X_train2,y_train2.ravel(),cv=10)\n","45aac185":"score","cb70eb80":"print('Original')\nprint(score_o.mean())\nprint(\"Imputed df\")\nprint(score.mean())","c313b00f":"classifier.fit(X_train,y_train)\nclassifier.fit(X_train2,y_train2)","07948a94":"y_pred=classifier.predict(X_test)\ny_pred2=classifier.predict(X_test2)","e4d52a29":"from sklearn.metrics import confusion_matrix, accuracy_score\n\ncm=confusion_matrix(y_test,y_pred)\nscore= accuracy_score(y_test,y_pred)\n\ncm2=confusion_matrix(y_test2,y_pred2)\nscore2= accuracy_score(y_test2,y_pred2)\nprint(\"Score on Original Data\")\nprint(cm2)\nprint(score2)\n\nprint(\"Score on Imputed Data\")\nprint(cm)\nprint(score)","14741ff2":"from sklearn.metrics import f1_score, accuracy_score","23af52c4":"from catboost import CatBoostClassifier","bd897c9e":"model_cb = CatBoostClassifier(task_type='GPU', iterations=100, \n                              random_state = 2021, \n                              eval_metric=\"F1\")","83d3b2ed":"features = list(X_train2.columns)","5e37d45a":"X_train2","727c1eb1":"\ncat_features = data[\"Outcome\"]","316ab62b":"model_cb.fit(X_train2, y_train2,plot=True, \n             eval_set=(X_test2, y_test2))","a57df332":"y_pred = model_cb.predict(X_test)","f3e73e0f":"f1_score(y_test, y_pred)","ab0bccc7":"accuracy_score(y_test, y_pred)","3a4d1963":"# How to find the length of each column of Data Frame\n## Checking value count of 0 for columns","3932f0c8":"# Pipelines Perform Hyperparameter Tuning Using Grid SearchCV","bc4736a9":"## After Hyper Parameter ","329ae869":"<a id=\"1\"><\/a>\n<p style = \"font-size : 25px; color : black ; font-family : 'Comic Sans MS'; text-align : center; background-color : #ff6666; border-radius: 5px 5px;\"><strong>Accuracy CatBoost = 0.8225108225108225<\/strong><ul style = \"font-size : 20px; color : black ; font-family : 'Comic Sans MS'; text-align : center; background-color : #ff6666; border-radius: 5px 5px;\"><li>XGBoost Accuracy : 0.7922077922077922 <\/li><\/ul><\/p>","b2e030a1":"# Same Process but for Imputed data\n## X_train, X_test, y_train, y_test","90861813":"<a id=\"3\"><\/a>\n<p style = \"font-size : 25px; color : black ; font-family : 'Comic Sans MS'; text-align : center; background-color : #ff6666; border-radius: 5px 5px;\"><strong>XGBoost<\/strong><\/p>","63e6c539":"# We will be doing\n1. pipeline\n2. Hyper Parameters\n\n## Models to be used\n1. Logistic Classification\n2. Random Forest\n3. XGBoost\n4. CatBoost\n5. LGB\n","5dc13b07":"# Notes for Better understanding\n![pipeline1.jpg](attachment:f8d62a08-790a-4820-b620-b06f37b104ae.jpg)\n\n![pip.jpg](attachment:31c14994-a9d5-4e43-92b6-e6e7ed57b40b.jpg)\n\n## Few Key points:\n* No of n_jobs don't affect our accuracy","e7afd43f":"# Reference \n* [Source 1 Krish Naik github](https:\/\/github.com\/krishnaik06\/Pipelines-Using-Sklearn\/blob\/master\/SklearnPipeline.ipynb)\n* [Source 2 Krish explaining about Hyper Parameter Tuning](https:\/\/www.youtube.com\/watch?v=DHxsNrL7Zfw)\n* [Intro to CatBoosting](https:\/\/www.youtube.com\/watch?v=zAXG1nUTdp0)","c95f3d15":"## Preparing Data for training","55cc4424":"# Boosting\n## Now will Try boosting for prediction\n1. XGBoost\n2. Light Gradient Boost\n3. CatBoost","be566996":"# 82% Accuracy without Hyper Parameter Tuning.\n### Will Check This again after Hyper Parameter Tuning","48fc6491":"\n# A.K.A. CatBoosting\n* CatBoost mostly takes care of **Most of the Data Preprocessing** like Missing values, ","6de23479":"### Will Check difference in result after Hyperparameter and before\n","65b2c275":"<a id=\"0\"><\/a>\n<p style = \"font-size : 25px; color : black ; font-family : 'Comic Sans MS'; text-align : center; background-color : #ff6666; border-radius: 5px 5px;\"><strong>Categorical Boosting with RandomSearch CV <\/strong><\/p>","67ca0b4c":"## Pipelines Creation\n1. Data Preprocessing by using Standard Scaler\n2. Reduce Dimension using PCA\n3. Apply  Classifier","cd96a2d4":"<a id='20'><\/a>\n<font color = '#F08841'>\n    Jump Direct to:\n\n1. [CATBoosting 82.25% Accuracy](#0)    \n    *          [Prediction Accuracy without Tunning](#2)\n1. [XGBoost 79.22% Accuracy](#3)"}}