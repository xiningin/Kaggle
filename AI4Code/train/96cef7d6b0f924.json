{"cell_type":{"3b97a69c":"code","9c8e3446":"code","3aae5e8c":"code","17407ed0":"code","3f8eece0":"code","a17885af":"code","89c78261":"code","e56906f0":"code","9275fbf1":"code","512e8cc7":"code","f5f657b7":"code","653c0d17":"code","46a7e6a0":"code","4c2552e1":"code","13da130a":"code","64690114":"code","8c081d70":"markdown","88653a80":"markdown","d46dd1ca":"markdown","bb84e45c":"markdown","7c7d4622":"markdown","5abe5788":"markdown","53171e8b":"markdown","42ec54b2":"markdown","1fa225ca":"markdown","201de1a9":"markdown","dfbfb89b":"markdown"},"source":{"3b97a69c":"from __future__ import division, print_function\nimport warnings\nimport os\nimport re\nimport pickle\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom scipy.sparse import csr_matrix, hstack\nfrom sklearn.model_selection import StratifiedKFold, TimeSeriesSplit\nfrom sklearn.linear_model import LogisticRegression, LogisticRegressionCV\nfrom sklearn.metrics import roc_auc_score\nfrom scipy.stats.mstats import gmean\nfrom sklearn.model_selection import cross_val_score\n%matplotlib inline\nwarnings.filterwarnings('ignore')","9c8e3446":"PATH_TO_DATA = '..\/input'\ntrain_df = pd.read_csv(os.path.join(PATH_TO_DATA, 'train_sessions.csv'),\n                       index_col='session_id')\ntest_df = pd.read_csv(os.path.join(PATH_TO_DATA, 'test_sessions.csv'),\n                      index_col='session_id')\nwith open(os.path.join(PATH_TO_DATA, 'site_dic.pkl'), 'rb') as input_file:\n    site_dict = pickle.load(input_file)\nsite_dict['Unknown'] = 0\n\n#Let's sort the data by time. This is useful for cross-validation over time.\ntrain_df = train_df.sort_values(by='time1')","3aae5e8c":"def get_sites_dict_(data):\n    '''\n    Gets dataframe with site names (10 columns) as input.\n    Returns a dictionary of sites ordered by frequency.\n    '''\n    m, n = data.shape #num of rows and columns\n    data = pd.DataFrame(data.values.reshape(m*n, 1), columns=['site']) #transform to 1 column\n    freq = data.site.value_counts().reset_index()\n    key_value_df = pd.DataFrame() #contains a pair of site-frequency\n    key_value_df['site'] = freq['index']\n    key_value_df['count'] = freq['site']\n    key_value_df.sort_values(by='count', inplace=True, ascending=False) \n    sites_dict = {} \n    sites_dict['Unknown'] = 0\n    for i in np.arange(key_value_df.shape[0]):\n        if key_value_df.iloc[i,0]!='Unknown':\n            sites_dict[key_value_df.iloc[i,0]] = i+1\n    return sites_dict\n\ndef inverse_dict(sites_dict):\n    '''\n    Gets a key-value dictionary. \n    Returns the dictionary by swapping the key and value.\n    '''\n    code_sites_dict = {}\n    sites = list(sites_dict.items())\n    for site in sites:\n        code_sites_dict[site[1]] = site[0]\n    return code_sites_dict\n\ndef make_sparse_data(data):\n    '''\n    Makes sparse matrix\n    '''\n    n = data.shape[1]\n    X = data.values\n    flatten_X = X.flatten()\n    new_X = ([1]*flatten_X.shape[0], flatten_X, range(0, flatten_X.shape[0]+10, 10))\n    X_sparse = csr_matrix(new_X)[:, 1:]\n    return X_sparse\n\ndef write_to_submission_file(predicted_labels, out_file,\n                             target='target', index_label=\"session_id\"):\n    predicted_df=pd.DataFrame(predicted_labels,index=np.arange(1,predicted_labels.shape[0]+1),columns=[target])\n    predicted_df.to_csv(out_file, index_label=index_label)","17407ed0":"train_test_df = pd.concat([train_df, test_df])\n\nsites = ['site%s' % i for i in range(1, 11)]\ntimes = ['time%s' % i for i in range(1, 11)]\nall_features = sites + times\n\ntrain_test_df_sites = train_test_df[sites].fillna(0).astype('int')\ntrain_test_df_times = train_test_df[times].fillna(0).astype('datetime64')\ntrain_test_df_sites.head()","3f8eece0":"#the index by which we will separate the training sample from the test sample\nidx_split = train_df.shape[0]\n\ny = train_df['target']\ntrain_test_sparse = make_sparse_data(train_test_df_sites)\nX_train_sparse = train_test_sparse[:idx_split, :]\nX_test_sparse = train_test_sparse[idx_split:,:]\n\nprint('Train DF size: {0}\\nTest DF size: {1}\\nTarget size: {2}'.format(\n    str(X_train_sparse.shape), str(X_test_sparse.shape), str(y.shape)))","a17885af":"#Spoiler: this conversion gives a good boost. If you have not done it yet, be sure to use it :)\ngeneral_df = pd.concat([train_test_df_sites, train_test_df_times], axis=1) #main df\n\ngeneral_sites = general_df[sites].apply(lambda ts: ts.map(inverse_dict(site_dict))) #instead of numbers the names of sites\ngeneral_sites = general_sites.applymap(lambda site: re.sub(\"^\\S*?\\.*?www\\S*?\\.\", '', site)) \n\nnew_site_dict = get_sites_dict_(general_sites)\n\n#on a thousand unique sites became less.\nprint(len(list(site_dict.keys())), len(list(new_site_dict.keys())))\ngeneral_sites.head()","89c78261":"general_df[sites] = general_sites.apply(lambda ts: ts.map(new_site_dict)) #new coding\ngeneral_sites_sparse = make_sparse_data(general_df[sites]) #make the first half of the sparse matrix (sites).","e56906f0":"#1 morning\ngeneral_df['morning'] = general_df['time1'].apply(lambda ts: 1 if (ts.hour>=7) & (ts.hour<=11) else 0)\n#2 evening\ngeneral_df['evening'] = general_df['time1'].apply(lambda ts: 1 if (ts.hour>=19) & (ts.hour<=23) else 0)\n#3 start hour\ngeneral_df['start_hour'] = general_df['time1'].apply(lambda ts: int(ts.hour))\n#4 month\ngeneral_df['month'] = general_df['time1'].apply(lambda ts: ts.month)\n#5 day of week\ngeneral_df['day_of_week'] = general_df['time1'].apply(lambda ts: ts.isoweekday())\n\ngeneral_df.head()","9275fbf1":"other_features = list(set(general_df.columns) - set(times) - set(sites))\nvisual_df = general_df[other_features]\nvisual_df[\"target\"] = y\n\n#correlations\nsns.heatmap(visual_df.corr())","512e8cc7":"#start_hour\nplt.figure(figsize=(20, 8))\nplt.subplot(1, 2, 1)\nplt.title(\"Alice's activity by start hour\")\nplt.ylim(0, 40)\nvalues_perc = visual_df[visual_df['target']==1]['start_hour'].value_counts(normalize=True)*100\nax = sns.barplot(values_perc.index, values_perc.values, palette=\"YlGnBu\")\nax.set(ylabel=\"Percent\")\n\nplt.subplot(1, 2, 2)\nplt.title(\"Activity of others by start hour\")\nplt.ylim(0, 40)\nvalues_perc = visual_df[visual_df['target']==0]['start_hour'].value_counts(normalize=True)*100\nax = sns.barplot(values_perc.index, values_perc.values, palette=\"YlGnBu\")\nax.set(ylabel=\"Percent\")\nplt.tight_layout()","f5f657b7":"#month\nplt.figure(figsize=(20, 8))\nplt.subplot(1, 2, 1)\nplt.title(\"Alice's activity by start month\")\nplt.ylim(0, 30)\nvalues_perc = visual_df[visual_df['target']==1]['month'].value_counts(normalize=True)*100\nax = sns.barplot(values_perc.index, values_perc.values, palette=\"OrRd\")\nax.set(ylabel=\"Percent\")\n\nplt.subplot(1, 2, 2)\nplt.title(\"Activity of others by start month\")\nplt.ylim(0, 30)\nvalues_perc = visual_df[visual_df['target']==0]['month'].value_counts(normalize=True)*100\nax = sns.barplot(values_perc.index, values_perc.values, palette=\"OrRd\")\nax.set(ylabel=\"Percent\")\nplt.tight_layout()","653c0d17":"#day of week\nplt.figure(figsize=(20, 8))\nplt.subplot(1, 2, 1)\nplt.title(\"Alice's activity by days of week\")\nplt.ylim(0, 35)\nvalues_perc = visual_df[visual_df['target']==1]['day_of_week'].value_counts(normalize=True)*100\nax = sns.barplot(values_perc.index, values_perc.values)\nax.set(ylabel=\"Percent\")\n\nplt.subplot(1, 2, 2)\nplt.title(\"Activity of others by days of week\")\nplt.ylim(0, 35)\nvalues_perc = visual_df[visual_df['target']==0]['day_of_week'].value_counts(normalize=True)*100\nax = sns.barplot(values_perc.index, values_perc.values)\nax.set(ylabel=\"Percent\")\nplt.tight_layout()","46a7e6a0":"general_df = pd.get_dummies(general_df, columns=['start_hour'])\ngeneral_df = pd.get_dummies(general_df, columns=['month'])\ngeneral_df = pd.get_dummies(general_df, columns=['day_of_week'])\nother_features = list(set(general_df.columns) - set(times) - set(sites))\nprint(other_features)","4c2552e1":"general_sparse = csr_matrix(hstack([general_sites_sparse, csr_matrix(general_df[other_features])]))\n\nX_train_notimes = general_sparse[:idx_split, :]\nX_test_notimes = general_sparse[idx_split:,:]\nprint(X_train_notimes.shape, X_test_notimes.shape)","13da130a":"time_split = TimeSeriesSplit(n_splits=10)\nskf = StratifiedKFold(n_splits=3, shuffle=True, random_state=97)\n\nlogit = LogisticRegression(random_state=97) #add n_jobs=-1\n\n#Use the geometric mean of the two results on cross-validation.\nprint(gmean([round(cross_val_score(logit, X_train_notimes, y, cv=skf, scoring='roc_auc').mean(),5),\n      round(cross_val_score(logit, X_train_notimes, y, cv=time_split, scoring='roc_auc').mean(),5)]))","64690114":"logit.fit(X_train_notimes, y)\nlogit_pred_proba = logit.predict_proba(X_test_notimes)[:,1]\nwrite_to_submission_file(logit_pred_proba, 'submission_LOGIT.csv')","8c081d70":"### Step 6. Building a model.","88653a80":"### Step 4. Make the binary features with one-hot-encoding.","d46dd1ca":"### Let's combine training and test samples \u2013 it will be necessary to bring them together then to the sparse matrix format.","bb84e45c":"### Loading training and test samples. In addition, download the dictionary with sites.","7c7d4622":"### Let's define functions that will be useful in the course of work.","5abe5788":"### Step 1. Get rid of the subdomains of 'www.', to bring all sites to one kind. \n\nObviously, sites *www.google.com* and *google.com* it's the same. However, such situations in the data generate unnecessary features and make noise.","53171e8b":"### Step 3. Visual analysis of the constructed features.\nLet's compare the behavior of Alice and all the others","42ec54b2":"    - We see that Alice often comes to rest on Monday and much less often comes other Wednesday (as a percentage). \n    - In addition, the timing of the session is very different.\n    - But the distribution of the month is almost the same, except that Alice abnormally high activity in September.","1fa225ca":"### Step 2. Create features.","201de1a9":"### Step 5. Building samples.\nTotal number of features is 47173.","dfbfb89b":"### Create a sparse matrix"}}