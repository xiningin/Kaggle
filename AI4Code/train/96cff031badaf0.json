{"cell_type":{"968a7492":"code","08c189e5":"code","c40ae426":"code","16e88444":"code","4a981bbc":"code","ca30e7b2":"code","788ed882":"code","3b4c68e0":"code","3db4005d":"code","b5cc8fb3":"code","151af8fe":"code","44cf16ff":"markdown","63c62be0":"markdown","3ee40f2b":"markdown","c05e7782":"markdown","f20a4c9d":"markdown","d67af84d":"markdown","eff42dde":"markdown","6171b75c":"markdown"},"source":{"968a7492":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","08c189e5":"# helper functions for plotting and drawing lines\n\ndef plot_points(X, y):\n    admitted = X[np.argwhere(y==1)]\n    rejected = X[np.argwhere(y==0)]\n    plt.scatter([s[0][0] for s in rejected], [s[0][1] for s in rejected], s = 25, color = 'blue', edgecolor = 'k')\n    plt.scatter([s[0][0] for s in admitted], [s[0][1] for s in admitted], s = 25, color = 'red', edgecolor = 'k')\n\ndef display(m, b, color='g--'):\n    plt.xlim(-0.05,1.05)\n    plt.ylim(-0.05,1.05)\n    x = np.arange(-10, 10, 0.1)\n    plt.plot(x, m*x+b, color)","c40ae426":"labels = ['grade1', 'grade2', 'accepted']\ndata = pd.read_csv('..\/input\/admissions\/data.csv', header=None, names=labels)\n\nX = np.array(data[['grade1', 'grade2']])\ny = np.array(data['accepted'])\nplot_points(X,y)\nplt.show()","16e88444":"data.head()","4a981bbc":"# dimensions of our dataset\ndata.shape","ca30e7b2":"# Activation (sigmoid) function\ndef sigmoid(x):\n    return 1\/ ( 1 + np.exp(-x) )\n\n# Output (prediction) formula\ndef output_formula(features, weights, bias):\n    return sigmoid(np.matmul(features,weights) + bias)\n\n# Error (log-loss) formula\ndef error_formula(y, output):\n    return -y*np.log(output) - (1- y)*np.log(1-output)\n\n# Gradient descent step\ndef update_weights(x, y, weights, bias, learnrate):\n    output = output_formula(x, weights, bias)\n    d_error = y - output\n    weights += learnrate * d_error * x\n    bias += learnrate * d_error\n    return weights, bias","788ed882":"def eq_quiz(w1, w2, b):\n    return w1*0.4 + w2*0.6 + b","3b4c68e0":"# probabilities\nsigmoid(eq_quiz(2,6,-2)), sigmoid(eq_quiz(3,5,-2.2)), sigmoid(eq_quiz(5,4,-3))","3db4005d":"n_records, n_features = X.shape\nnp.random.normal(scale=1 \/ n_features**.5, size=n_features)","b5cc8fb3":"np.random.seed(44)\n\nepochs = 100\nlearnrate = 0.01\n\ndef train(features, targets, epochs, learnrate, graph_lines=False):\n    \n    # the error for each epoch\n    errors = []\n    # the number of records and the number of features\n    n_records, n_features = features.shape\n    last_loss = None\n    # start with random weights\n    weights = np.random.normal(scale=1 \/ n_features**.5, size=n_features)\n    print('Initial Random Weights: ', weights[0], weights[1])\n    # start with a bias of 0\n    bias = 0\n    for e in range(epochs):\n        del_w = np.zeros(weights.shape)\n        for x, y in zip(features, targets):\n            # calculate the output\n            output = output_formula(x, weights, bias)\n            # calculate the error\n            error = error_formula(y, output)\n            # update the weights and bias\n            weights, bias = update_weights(x, y, weights, bias, learnrate)\n        # printing out the log-loss error on the training set\n        out = output_formula(features, weights, bias)\n        loss = np.mean(error_formula(targets, out))\n        errors.append(loss)\n        if e % (epochs \/ 10) == 0:\n            print(\"\\n========== Epoch\", e,\"==========\")\n            if last_loss and last_loss < loss:\n                print(\"Train loss: \", loss, \"  WARNING - Loss Increasing\")\n            else:\n                print(\"Train loss: \", loss)\n            last_loss = loss\n            predictions = out > 0.5\n            accuracy = np.mean(predictions == targets)\n            print(\"Accuracy: \", accuracy)\n        if graph_lines and e % (epochs \/ 100) == 0:\n            display(-weights[0]\/weights[1], -bias\/weights[1])\n            \n\n    # Plotting the solution boundary\n    plt.title(\"Solution boundary\")\n    display(-weights[0]\/weights[1], -bias\/weights[1], 'black')\n\n    # Plotting the data\n    plot_points(features, targets)\n    plt.show()\n\n    # Plotting the error\n    plt.title(\"Error Plot\")\n    plt.xlabel('Number of epochs')\n    plt.ylabel('Error')\n    plt.plot(errors)\n    plt.show()","151af8fe":"train(X, y, epochs, learnrate, True)","44cf16ff":"#### #5 Time to train the algorithm!\nWhen we run the function, we'll obtain the following:\n- 10 updates with the current training loss and accuracy\n- A plot of the data and some of the boundary lines obtained. The final one is in black. Notice how the lines get closer and closer to the best fit, as we go through more epochs.\n- A plot of the error function. Notice how it decreases as we go through more epochs.","63c62be0":"#### #3 Implementing the basic functions\n\nThese are the formulas we'll nedd to implement.\n\n1. Sigmoid activation function\n\n$$\\sigma(x) = \\frac{1}{1+e^{-x}}$$\n\n2. Output (prediction) formula\n\n$$\\hat{y} = \\sigma(w_1 x_1 + w_2 x_2 + b)$$\n\n3. Error function\n\n$$Error(y, \\hat{y}) = - y \\log(\\hat{y}) - (1-y) \\log(1-\\hat{y})$$\n\n4. The function that updates the weights\n\n$$ w_i \\longrightarrow w_i + \\alpha (y - \\hat{y}) x_i$$\n\n$$ b \\longrightarrow b + \\alpha (y - \\hat{y})$$","3ee40f2b":"## Implementing the Gradient Descent Algorithm\n\nLet's implement the basic functions of the Gradient Descent algorithm to find the boundary in a small dataset with two classes. \n\n\n### Instructions\n\nImplement the functions that build the gradient descent algorithm, namely:\n\n1. The *sigmoid activation* function: `sigmoid`;\n2. The formula for the *prediction*: `output_formula`;\n3. The formula for the *error* at a point: `error_formula`;\n4. The function that *updates the parameters* with one gradient descent step: `update_weights`.\n\n**When you implement them**\n        \n* Run the `train` function and this will graph the several of the lines that are drawn in successive gradient descent steps. ","c05e7782":"##### From Scores to Probabilities\n\nLet's define the combination of two new perceptrons as `0.4*w1 + 0.6*w2 + b`. Which of the following values for the `weights` and the `bias` would result in the `final probability` of the point to be 0.88?\n* 2, 6, -2\n* 3, 5, -2.2 **[+]**\n* 5, 4, -3\n\nFirst, we need to get the score for each of the inputs values. Then, we have to apply the `sigmoid` function to transform the score into a probability. ","f20a4c9d":"#### #2 Reading and plotting the data","d67af84d":"#### #4 Training function\nThis function will help us iterate the gradient descent algorithm through all the data, for a number of epochs. It will also plot the data, and some of the boundary lines obtained as we run the algorithm.","eff42dde":"#### #1 Visualize the data\n\nLet's start with some functions that will help us plot and visualize the data.","6171b75c":"We have 100 observations, 2 input features and the output variable"}}