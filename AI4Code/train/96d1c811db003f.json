{"cell_type":{"ee79ed4e":"code","6faecb46":"code","5ec22598":"code","492d6612":"code","27ca69b7":"code","6059ca40":"code","3f120b44":"code","53b0ddaa":"code","1b44cd90":"markdown","a1260efe":"markdown","5a52e2ce":"markdown","583a190b":"markdown","e04c077c":"markdown","43f715f8":"markdown","c42ec7fb":"markdown","2cf71c9b":"markdown"},"source":{"ee79ed4e":"import numpy as np\nimport pandas as pd \nfrom sklearn.model_selection import KFold\n\nimport numpy as np\nimport pandas as pd \n\nimport os\nimport gc\nimport psutil\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, Normalizer,MinMaxScaler\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score\n\nfrom xgboost import XGBClassifier, XGBRegressor\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.preprocessing import PolynomialFeatures\n\nfrom optuna.integration import LightGBMPruningCallback\n\n# get skewed features to impute median instead of mean\nfrom scipy.stats import skew\nfrom sklearn.ensemble import AdaBoostClassifier\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import linear_model\nfrom sklearn.linear_model import Ridge,Lasso\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import roc_auc_score\nfrom xgboost import XGBRegressor, XGBRFRegressor\n\nimport itertools\nimport optuna\nfrom lightgbm import LGBMClassifier,LGBMRegressor\nimport lightgbm as lgb\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom catboost import CatBoostRegressor, CatBoostClassifier\n\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import HistGradientBoostingClassifier\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","6faecb46":"train_data = pd.read_csv('..\/input\/titanic-create-folds\/TITANIC_Folds.csv') # Read TITANIC_Folds as train_data\ntest_data = pd.read_csv(\"..\/input\/titanic\/test.csv\")","5ec22598":"train_data.drop('Cabin',axis= 1,inplace= True)\ntest_data.drop('Cabin',axis= 1,inplace= True)\ntrain_data['Age'] = train_data['Age'].fillna(int(train_data['Age'].mean()))\ntest_data['Age'] = test_data['Age'].fillna(int(test_data['Age'].mean()))\ntrain_data['Embarked'] = train_data['Embarked'].fillna(train_data['Embarked'].mode()[0])\ntest_data['Fare'] = test_data['Fare'].fillna(test_data['Fare'].median())\n\ntrain_data.drop('Ticket',axis= 1,inplace= True)\ntest_data.drop('Ticket',axis= 1,inplace= True)\ntrain_data['Family_size']=train_data.SibSp + train_data.Parch + 1\ntest_data['Family_size']=test_data.SibSp + test_data.Parch + 1\ntrain_data['IsAlone'] = 1\ntrain_data[['IsAlone']][train_data.Family_size >1] = 0\ntest_data['IsAlone'] = 1\ntest_data[['IsAlone']][test_data.Family_size >1] = 0\n#-----------------------------------------------\ntrain_data['isTrain'] = 1\ntest_data['isTrain'] = 0\ntt = pd.concat([train_data,test_data])\ntt['Title']= tt.Name.apply(lambda x: x.split(',')[1].split('.')[0])\nstat_min = 10\ntitle_names = (tt['Title'].value_counts() >= stat_min)\nt=title_names.reset_index()# most common titles\nmost_freq_titles = list(t[t.Title == True]['index'])\ntt['Title']= tt.Title.apply(lambda x: x if x in most_freq_titles else 'other')\ntt= pd.get_dummies(data= tt,columns=['Title'],drop_first= True)\ntt['Fare_bins']=pd.qcut(tt.Fare, 4)\ntt['Age_bins']= pd.cut(tt.Age.astype(int), 5)\n\nlabel = LabelEncoder()\nfor rows in ['Age_bins','Fare_bins']:\n    tt[rows]=label.fit_transform(tt[rows])\n\ntt.drop(['Name','Fare','Age'],axis=1,inplace=True) # we have created 'Title' , 'Fare_bins', 'Age_bins' \ntt = pd.get_dummies(tt,columns= ['Sex','Embarked'],drop_first = True)\ntrain_data = tt[tt.isTrain == 1]\ntest_data = tt[tt.isTrain == 0]\ntest_data.drop(['Survived','isTrain','fold'],axis=1,inplace = True)\ntrain_data.drop(['isTrain'],axis=1,inplace= True)\n\n# Note don't drop PassengerId column of train_data and test_data as it is used later. Instead drop it only for test and my_folds\nuseful_features = test_data.drop('PassengerId',axis=1).columns.tolist() #########################################\ntest = test_data[useful_features]\nmy_folds = train_data.copy()","492d6612":"test.shape, my_folds.shape, useful_features","27ca69b7":"def obj(trial,xtrain,ytrain,xvalid,yvalid):\n    \n    params={\n    \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.5),\n    \"max_depth\": trial.suggest_categorical(\"max_depth\", [3,5,7,10]),\n    \"min_child_weight\": trial.suggest_categorical(\"min_child_weight\", [1,3,5]),\n    \"subsample\": trial.suggest_float(\"subsample\", 0.01, 0.5),\n    \"n_estimators\": trial.suggest_categorical(\"n_estimators\", [100,200,300,400,500,1000,1200,1500]),\n    \"objective\": trial.suggest_categorical(\"objective\", ['reg:squarederror']),\n    \"tree_method\": trial.suggest_categorical(\"tree_method\", ['gpu_hist']),\n    \"gpu_id\": trial.suggest_categorical(\"gpu_id\", [0]),\n    \"predictor\": trial.suggest_categorical(\"predictor\", ['gpu_predictor'])\n    }\n    model = XGBRegressor(**params,random_state=141)\n    model.fit(xtrain, ytrain,\n            eval_set=[(xvalid, yvalid)],\n            eval_metric=\"auc\",\n            early_stopping_rounds=100,\n            verbose=0\n     )\n    valid_preds = model.predict(xvalid)\n    #test_preds = model.predict(xtest)\n    score = roc_auc_score(yvalid, valid_preds)\n\n\n    return score\n\n# create trial function\ndef run(my_folds1):   \n \n    my_folds1 = my_folds.copy()\n    #test1  = test.copy()\n\n    fold=0\n    xtrain = my_folds[my_folds1.fold != fold].reset_index(drop=True)\n    xvalid = my_folds[my_folds1.fold == fold].reset_index(drop=True)\n    #xtest = test1.copy()\n\n    ytrain = xtrain.Survived\n    yvalid = xvalid.Survived\n\n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n\n    ## preprocess\n    si = SimpleImputer(strategy='median')\n    xtrain = si.fit_transform(xtrain)\n    xvalid = si.transform(xvalid)\n    #xtest = si.transform(xtest)\n\n    # scale\n    ss = RobustScaler()\n    xtrain = ss.fit_transform(xtrain)\n    xvalid = ss.transform(xvalid)\n    #xtest = ss.transform(xtest)\n\n    xtrain = pd.DataFrame(xtrain, columns=useful_features)\n    xvalid = pd.DataFrame(xvalid, columns=useful_features)\n    #xtest = pd.DataFrame(xtest, columns=useful_features)\n    \n#     for col in useful_features:\n#         xtrain[col] = np.log1p(xtrain[col])\n#         xvalid[col] = np.log1p(xvalid[col])\n#         #xtest[col] = np.log1p(xtest[col])\n        \n    #create optuna study\n    study = optuna.create_study(\n        direction='maximize',\n        study_name='XGBREGRESSOR'\n    )\n\n    study.optimize( lambda trial: obj(trial,xtrain,ytrain,xvalid,yvalid),n_trials= 50 ) # it tries 50 different values to find optimal hyperparameter\n\n    print(f\"Best Params: {study.best_trial.params}\")\n    print(f\"Best Trial: {study.best_trial.value}\")\n    \n    return study.best_trial.params, study.best_trial.value","6059ca40":"bp,bv=run(my_folds)","3f120b44":"bp ","53b0ddaa":"bv","1b44cd90":"<p style=\"font-family:newtimeroman;font-size:120%;color:#444160;\"><i>bv<\/i> returns best roc_auc_score <\/p> ","a1260efe":"<a id=\"4\"><\/a>\n# <p style=\"background-color:#B721FF;font-family:newtimeroman;color:#444160;font-size:150%;text-align:center;border-radius:20px 60px;\">HYPERPARAMETER OPTIMIZATION<\/p>\n<p style=\"font-family:newtimeroman;font-size:120%;color:#444160;\">This step is very crucial, here we are using OPTUNA to find optimal hyperparameter.\n<br>We have created two functions <b>obj()<\/b> and <b>run()<\/b>. <br><b>run()<\/b> takes <b>my_folds<\/b> as input which we have created then it creates a OPTUNA study and feeds this data. \n    <br> This <b>obj()<\/b> method is created by OPTUNA which takes xtrain, ytrain, xvalid, yvalid as input and trains the model then make predictions on xvalid and then return roc_auc_score. To know more about OPTUNA you can look at working of OPTUNA from their official doccument.\n    <br>[If you do not understand anything you can ask me in the comment]<\/p> ","5a52e2ce":"<p style=\"font-family:newtimeroman;font-size:120%;color:#444160;\"><i>bp<\/i> returns best parameters <\/p> ","583a190b":"<a id=\"5\"><\/a>\n<p style=\"background-color:#B721FF;font-family:newtimeroman;color:#444160;font-size:150%;text-align:center;border-radius:20px 60px;\">CONCLUSION<\/p>\n<p style=\"font-family:newtimeroman;font-size:120%;color:#444160;\">Now we will use this optimized model to make predictions. In the next notebook we will find optimal hyperparameter for another model. \n<br> That is all for now, If you have any doubt feel free to ask me in the comment. <br>\n    If you appreciate my effort please do <b>UPVOTE\ud83d\udc4d<\/b> and I will see you in the next Notebook\ud83d\udcd2. <\/p>\n\n**<span style=\"color:#444160;\"> Thanks!<\/span>**\n<a id=\"6\"><\/a>\n<p style=\"background-color:#B721FF;font-family:newtimeroman;color:#444160;font-size:150%;text-align:center;border-radius:20px 60px;\">END<\/p>\n    <a href=\"#top\" role=\"button\" aria-pressed=\"true\" >\u2b06\ufe0fBack to Table of Contents \u2b06\ufe0f<\/a>","e04c077c":"<a id=\"2\"><\/a>\n# <p style=\"background-color:#B721FF;font-family:newtimeroman;color:#444160;font-size:150%;text-align:center;border-radius:20px 60px;\">READ DATASETS<\/p>","43f715f8":"<a id=\"3\"><\/a>\n# <p style=\"background-color:#B721FF;font-family:newtimeroman;color:#444160;font-size:150%;text-align:center;border-radius:20px 60px;\">PREPROCESSING<\/p>\n","c42ec7fb":"\n<a id=\"0\"><\/a>\n# <p style=\"background-color:#FFCC70;font-family:newtimeroman;color:#444160;font-size:150%;text-align:center;border-radius:5px 5px;\">LEVEL1 ROUND2 XGBREGRESSOR<br><p style=\"background-color:#B721FF;font-family:newtimeroman;color:#444160;font-size:150%;text-align:center;border-radius:20px 60px;\">INTRODUCTION<\/p>\n<p style=\"font-family:newtimeroman;font-size:120%;color:#444160;\">This is a part of the notebook series <i>\"My_Complete_Pipeline_for_any_ML_Competition\"<\/i> where we are building complete pipeline.<\/p> \n\n\ud83d\udcccLink of first notebook of the series <a href=\"https:\/\/www.kaggle.com\/raj401\/my-complete-pipeline-for-any-ml-competition\">https:\/\/www.kaggle.com\/raj401\/my-complete-pipeline-for-any-ml-competition<\/a><br>\n\ud83d\udcccLink of notebook where we have created folds <a href=\"https:\/\/www.kaggle.com\/raj401\/titanic-create-folds\">https:\/\/www.kaggle.com\/raj401\/titanic-create-folds<\/a><br>\n<p style=\"font-family:newtimeroman;font-size:120%;color:#444160;\">\n    If you like my effort please do <b><span style=\"color:crimson; font-size:20px\">UPVOTE\ud83d\udc4d<\/span><\/b>, it really keeps me motivated. <\/p>\n\n\n<p style=\"background-color:#B721FF;font-family:newtimeroman;color:#444160;font-size:50%;text-align:center;border-radius:20px 60px;\">\"\"<\/p> \n<p style=\"font-family:newtimeroman;font-size:120%;color:#444160;\">In the <b>TITANIC_Create_Folds<\/b> notebook we have modified our training set by adding new column named 'fold' and then saved it as <i>TITANIC_folds.csv<\/i>. In this notebook we will use this modified training set instead of original training set and do hyperparameter tuning of XGBRegressor using OPTUNA. I am providing the link of <i>TITANIC_folds.csv<\/i> you can just add it to your notebook and you are good to go.<b><br>[Make sure you have added it before moving further. If you have TITANIC_Create_Folds notebook you can also add that notebook instead.]<br><\/b>\n\ud83d\udcccLink of Dataset containing <i>TITANIC_folds.csv<\/i> <a href=\"https:\/\/www.kaggle.com\/raj401\/titanic-all-datasets\">https:\/\/www.kaggle.com\/raj401\/titanic-all-datasets<\/a><br><\/p> \n\n\n<a id='top'><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<p style=\"background-color:#B721FF;font-family:newtimeroman;color:#444160;font-size:150%;text-align:center;border-radius:20px 60px;\">TABLE OF CONTENTS<\/p>   \n    \n* [1. IMPORTING LIBRARIES](#1)\n    \n* [2. READ DATASETS](#2)\n\n* [3. PREPROCESSING](#3)\n    \n    \n* [4. HYPERPARAMETER OPTIMIZATION](#4)\n    \n* [5. CONCLUSION](#5)\n    \n* [6. END](#6)\n\n<a id=\"1\"><\/a>\n# <p style=\"background-color:#B721FF;font-family:newtimeroman;color:#444160;font-size:150%;text-align:center;border-radius:20px 60px;\">IMPORTING LIBRARIES<\/p>","2cf71c9b":"<p style=\"font-family:newtimeroman;font-size:120%;color:#444160;\">Either you can add the dataset whoose link I have given above or if you have TITANIC_Create_Folds notebook you can add it's output from <code>Add data<\/code> option. Both contains TITANIC_Folds.csv (modified train set). <br>\nNow read it as train_data.<\/p>"}}