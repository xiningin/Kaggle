{"cell_type":{"4667f970":"code","7e250e90":"code","bd4d18ea":"code","bef6a26c":"code","6bae11f2":"code","06f1bd9b":"code","6fb81b9e":"code","1de071f9":"code","d6c3b64c":"code","7186d333":"code","a9a0349a":"code","9830117c":"code","45053d5f":"code","3fe73b53":"code","dccb1ba9":"code","f58e15fd":"code","44ea31c9":"code","2f0543e8":"code","75c67c15":"code","eaec2264":"code","a0e181f2":"code","f7006b74":"code","be2fc03b":"code","54550c8a":"code","38036efe":"code","893a36f0":"code","49f2270a":"code","4f604b7d":"code","6ed770b9":"code","99c65cd5":"code","53e03fbb":"code","2a334eea":"markdown","a077edde":"markdown","2a8c0a84":"markdown","8d6e9e92":"markdown","cb494b75":"markdown","0547e765":"markdown","bb00df1c":"markdown","2ecc7752":"markdown","ea705e22":"markdown","328fbdf0":"markdown","8924c452":"markdown","54d5fdad":"markdown","bd5f9de7":"markdown","869dfdf6":"markdown","d9f8ff5d":"markdown"},"source":{"4667f970":"import re\nimport time\nimport math\nimport random\n\nimport numpy as np\nimport pandas as pd\nimport spacy\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchtext import data\n\nfrom tqdm import notebook\npd.set_option('display.max_colwidth', 200)\n\nimport warnings\nwarnings.filterwarnings('ignore')","7e250e90":"# dependency for spaCy Russian tokenizer\n!pip install pymorphy2","bd4d18ea":"# check GPU availability\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)","bef6a26c":"# import Russian spacy model to tokenize Russian text\nfrom spacy.lang.ru import Russian","6bae11f2":"# spacy object for Russian\nnlp_ru = Russian()\n\n# spacy object for English\nnlp_en = spacy.load(\"en_core_web_sm\", disable = [\"parser\", \"tagger\", \"ner\"])","06f1bd9b":"## functions to perform tokenization\n\n# tokenizes Russian text from a string into a list of tokens\ndef tokenize_ru(text):\n  return [tok.text for tok in nlp_ru.tokenizer(text)]\n\n# tokenizes English text from a string into a list of tokens\ndef tokenize_en(text):\n  return [tok.text for tok in nlp_en.tokenizer(text)]","6fb81b9e":"## Create Field objects\n\n# Field object for Russian\nSRC = data.Field(tokenize = tokenize_ru, \n                 include_lengths = True, \n                 lower = True)\n\n# Field object for English\nTRG = data.Field(tokenize = tokenize_en, \n                 init_token = '<sos>', # \"start\" token\n                 eos_token = '<eos>', # \"\" token\n                 include_lengths = True, \n                 lower = True)\n\nfields = [('rus', SRC), ('eng', TRG)]","1de071f9":"# importing data from csv\nnmt_data = data.TabularDataset(path=\"..\/input\/englishrussiansentencepairs\/data\/train.csv\", format='csv', fields=fields)","d6c3b64c":"# build vocabulary for Russian sequences\nSRC.build_vocab(nmt_data, max_size=4000)\n\n# build vocabulary for English sequences\nTRG.build_vocab(nmt_data, max_size=4000)","7186d333":"# check size of vocabulary\nlen(SRC.vocab), len(TRG.vocab)","a9a0349a":"# Split our dialogue data into training, validation, and test sets\ntrain_data, val_data = nmt_data.split(split_ratio=0.8)","9830117c":"# Create a set of iterators for each split\ntrain_iterator, valid_iterator = data.BucketIterator.splits(\n    (train_data, val_data), \n    batch_size = 64, \n    sort_within_batch = True, \n    sort_key = lambda x:len(x.rus),\n    device = device)","45053d5f":"class Encoder(nn.Module):\n  \n  def __init__(self, hidden_size, embedding_size, num_layers=2, dropout=0.3):\n    \n    super(Encoder, self).__init__()\n    \n    # Basic network params\n    self.hidden_size = hidden_size\n    self.embedding_size = embedding_size\n    self.num_layers = num_layers\n    self.dropout = dropout\n    \n    # Embedding layer that will be shared with Decoder\n    self.embedding = nn.Embedding(len(SRC.vocab), embedding_size)\n    # GRU layer\n    self.gru = nn.GRU(embedding_size, hidden_size,\n                      num_layers=num_layers,\n                      dropout=dropout)\n      \n  def forward(self, input_sequence):\n      \n    # Convert input_sequence to word embeddings\n    embedded = self.embedding(input_sequence)\n            \n    outputs, hidden = self.gru(embedded)\n    \n    # The ouput of a GRU has shape -> (seq_len, batch, hidden_size)\n    return outputs, hidden","3fe73b53":"class Attention(nn.Module):\n  def __init__(self, hidden_size):\n    super(Attention, self).__init__()        \n    self.hidden_size = hidden_size\n      \n    \n  def dot_score(self, hidden_state, encoder_states):\n    return torch.sum(hidden_state * encoder_states, dim=2)\n  \n          \n  def forward(self, hidden, encoder_outputs, mask):\n      \n    attn_scores = self.dot_score(hidden, encoder_outputs)\n    \n    # Transpose max_length and batch_size dimensions\n    attn_scores = attn_scores.t()\n    \n    # Apply mask so network does not attend <pad> tokens        \n    attn_scores = attn_scores.masked_fill(mask == 0, -1e5)\n    \n    # Return softmax over attention scores      \n    return F.softmax(attn_scores, dim=1).unsqueeze(1)","dccb1ba9":"class Decoder(nn.Module):\n  def __init__(self, embedding_size, hidden_size, output_size, n_layers=2, dropout=0.3):\n      \n    super(Decoder, self).__init__()\n    \n    # Basic network params\n    self.hidden_size = hidden_size\n    self.output_size = output_size\n    self.n_layers = n_layers\n    self.dropout = dropout\n    self.embedding = nn.Embedding(output_size, embedding_size)\n            \n    self.gru = nn.GRU(embedding_size, hidden_size, n_layers, \n                      dropout=dropout)\n    \n    self.concat = nn.Linear(hidden_size * 2, hidden_size)\n    self.out = nn.Linear(hidden_size, output_size)\n    self.attn = Attention(hidden_size)\n      \n  def forward(self, current_token, hidden_state, encoder_outputs, mask):\n    \n    # convert current_token to word_embedding\n    embedded = self.embedding(current_token)\n    \n    # Pass through GRU\n    gru_output, hidden_state = self.gru(embedded, hidden_state)\n    \n    # Calculate attention weights\n    attention_weights = self.attn(gru_output, encoder_outputs, mask)\n    \n    # Calculate context vector (weigthed average)\n    context = attention_weights.bmm(encoder_outputs.transpose(0, 1))\n    \n    # Concatenate  context vector and GRU output\n    gru_output = gru_output.squeeze(0)\n    context = context.squeeze(1)\n    concat_input = torch.cat((gru_output, context), 1)\n    concat_output = torch.tanh(self.concat(concat_input))\n    \n    # Pass concat_output to final output layer\n    output = self.out(concat_output)\n    \n    # Return output and final hidden state\n    return output, hidden_state","f58e15fd":"class seq2seq(nn.Module):\n  def __init__(self, embedding_size, hidden_size, vocab_size, device, pad_idx, eos_idx, sos_idx):\n    super(seq2seq, self).__init__()\n    \n    # Embedding layer shared by encoder and decoder\n    self.embedding = nn.Embedding(vocab_size, embedding_size)\n    \n    # Encoder network\n    self.encoder = Encoder(hidden_size, \n                            embedding_size,\n                            num_layers=2,\n                            dropout=0.3)\n    \n    # Decoder network        \n    self.decoder = Decoder(embedding_size,\n                            hidden_size,\n                            vocab_size,\n                            n_layers=2,\n                            dropout=0.3)\n    \n    \n    # Indices of special tokens and hardware device \n    self.pad_idx = pad_idx\n    self.eos_idx = eos_idx\n    self.sos_idx = sos_idx\n    self.device = device\n      \n  def create_mask(self, input_sequence):\n    return (input_sequence != self.pad_idx).permute(1, 0)\n      \n      \n  def forward(self, input_sequence, output_sequence):\n    \n    # Unpack input_sequence tuple\n    input_tokens = input_sequence[0]\n  \n    # Unpack output_tokens, or create an empty tensor for text generation\n    if output_sequence is None:\n      inference = True\n      output_tokens = torch.zeros((100, input_tokens.shape[1])).long().fill_(self.sos_idx).to(self.device)\n    else:\n      inference = False\n      output_tokens = output_sequence[0]\n    \n    vocab_size = self.decoder.output_size\n    batch_size = len(input_sequence[1])\n    max_seq_len = len(output_tokens)\n    \n    # tensor to store decoder outputs\n    outputs = torch.zeros(max_seq_len, batch_size, vocab_size).to(self.device)        \n    \n    # pass input sequence to the encoder\n    encoder_outputs, hidden = self.encoder(input_tokens)\n    \n    # first input to the decoder is the <sos> tokens\n    output = output_tokens[0,:]\n    \n    # create mask\n    mask = self.create_mask(input_tokens)\n    \n    \n    # Step through the length of the output sequence one token at a time\n    for t in range(1, max_seq_len):\n      output = output.unsqueeze(0)\n      \n      output, hidden = self.decoder(output, hidden, encoder_outputs, mask)\n      outputs[t] = output\n      \n      if inference:\n        output = output.max(1)[1]\n      else:\n        output = output_tokens[t]\n      \n      # If we're in inference mode, keep generating until we produce an\n      # <eos> token\n      if inference and output.item() == self.eos_idx:\n        return outputs[:t]\n        \n    return outputs","44ea31c9":"# extract special tokens\npad_idx = TRG.vocab.stoi['<pad>']\neos_idx = TRG.vocab.stoi['<eos>']\nsos_idx = TRG.vocab.stoi['<sos>']\n\n# Size of embedding_dim should match the dim of pre-trained word embeddings!\nembedding_dim = 100\nhidden_dim = 256\nvocab_size = len(TRG.vocab)","2f0543e8":"model = seq2seq(embedding_dim,\n                hidden_dim, \n                vocab_size, \n                device, pad_idx, eos_idx, sos_idx).to(device)","75c67c15":"# print model architecture\nmodel","eaec2264":"# Adam optimizer\noptimizer = optim.Adam(model.parameters())\n\n# cross entropy loss with softmax\ncriterion = nn.CrossEntropyLoss(ignore_index = pad_idx)","a0e181f2":"def train(model, iterator, criterion, optimizer):\n  # Put the model in training mode!\n  model.train()\n  \n  epoch_loss = 0\n  \n  for idx, batch in notebook.tqdm(enumerate(iterator), total=len(iterator)):\n    input_sequence = batch.rus\n    output_sequence = batch.eng\n\n    target_tokens = output_sequence[0]\n\n    # zero out the gradient for the current batch\n    optimizer.zero_grad()\n\n    # Run the batch through our model\n    output = model(input_sequence, output_sequence)\n\n    # Throw it through our loss function\n    output = output[1:].view(-1, output.shape[-1])\n    target_tokens = target_tokens[1:].view(-1)\n\n    loss = criterion(output, target_tokens)\n\n    # Perform back-prop and calculate the gradient of our loss function\n    loss.backward()\n\n    # Update model parameters\n    optimizer.step()\n\n    epoch_loss += loss.item()\n      \n  return epoch_loss \/ len(iterator)","f7006b74":"def evaluate(model, iterator, criterion):\n  # Put the model in training mode!\n  model.eval()\n  \n  epoch_loss = 0\n  \n  for idx, batch in notebook.tqdm(enumerate(iterator), total=len(iterator)):\n    input_sequence = batch.rus\n    output_sequence = batch.eng\n\n    target_tokens = output_sequence[0]\n\n    # Run the batch through our model\n    output = model(input_sequence, output_sequence)\n\n    # Throw it through our loss function\n    output = output[1:].view(-1, output.shape[-1])\n    target_tokens = target_tokens[1:].view(-1)\n\n    loss = criterion(output, target_tokens)\n\n    epoch_loss += loss.item()\n      \n  return epoch_loss \/ len(iterator)","be2fc03b":"# function to compute time taken by an epoch (in mm:ss)\ndef epoch_time(start_time, end_time):\n  elapsed_time = end_time - start_time\n  elapsed_mins = int(elapsed_time \/ 60)\n  elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n  return elapsed_mins, elapsed_secs","54550c8a":"N_EPOCHS = 30\n\nbest_valid_loss = float('inf')\n\n# start model training\nfor epoch in range(N_EPOCHS):\n    \n  start_time = time.time()\n  \n  train_loss = train(model, train_iterator, criterion, optimizer)\n  valid_loss = evaluate(model, valid_iterator, criterion)\n  \n  end_time = time.time()\n  \n  epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n  \n  # compare validation loss\n  if valid_loss < best_valid_loss:\n    best_valid_loss = valid_loss\n    torch.save(model.state_dict(), 'best_model.pt')\n  \n  print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n  print(f'\\tTrain Loss: {train_loss:.3f}')\n  print(f'\\t Val. Loss: {valid_loss:.3f}')","38036efe":"# load saved model weights\npath = 'best_model.pt'\nmodel.load_state_dict(torch.load(path))","893a36f0":"def translate_sentence(model, sentence):\n    model.eval()\n    \n    # tokenization\n    tokenized = nlp_ru(sentence) \n    # convert tokens to lowercase\n    tokenized = [t.lower_ for t in tokenized]\n    # convert tokens to integers\n    int_tokenized = [SRC.vocab.stoi[t] for t in tokenized] \n    \n    # convert list to tensor\n    sentence_length = torch.LongTensor([len(int_tokenized)]).to(model.device) \n    tensor = torch.LongTensor(int_tokenized).unsqueeze(1).to(model.device) \n    \n    # get predictions\n    translation_tensor_logits = model((tensor, sentence_length), None) \n    \n    # get token index with highest score\n    translation_tensor = torch.argmax(translation_tensor_logits.squeeze(1), 1)\n    # convert indices (integers) to tokens\n    translation = [TRG.vocab.itos[t] for t in translation_tensor]\n \n    # Start at the first index.  We don't need to return the <sos> token...\n    translation = translation[1:]\n    return \" \".join(translation)","49f2270a":"sentence = \"\u044d\u0442\u043e \u043d\u043e\u0432\u044b\u0439\"\nresponse = translate_sentence(model, sentence)\nprint(response)","4f604b7d":"# read test file \ntest_df = pd.read_csv('..\/input\/englishrussiansentencepairs\/data\/translation.csv')","6ed770b9":"# attention based translations\nattn_translations = [translate_sentence(model, sent) for sent in notebook.tqdm(test_df[\"rus\"])]","99c65cd5":"test_df[\"attn_translations\"] = attn_translations","53e03fbb":"# check translations\ntest_df.sample(20)","2a334eea":"# Import Libraries","a077edde":"# Create Field Objects","2a8c0a84":"# Train Seq2Seq Model","8d6e9e92":"# Sequence-to-Sequence Architecture","cb494b75":"# Encoder Architecture","0547e765":"# Attention Mechanism","bb00df1c":"# Translate Russian Sentences in the Test Dataset","2ecc7752":"# Introduction\n> To build a neural machine translation model to translate Russian text to English. A Seq2Seq model is a model that takes a sequence of items (words, letters, time series, etc) and outputs another sequence of items.The encoder captures the context of the input sequence in the form of a hidden state vector and sends it to the decoder, which then produces the output sequence.\n\n> The dataset is taken from the tatoeba Project.\n\n> This notebook implements GRU's with Attention mechanism in a Encoder-Decoder architecture on Russian-English sentence pairs. Framework used is Pytorch\/Torchtext and Spacy.\n\n## The entire code can also be found on [github](https:\/\/github.com\/jelifysh\/Seq2seq-Machine-Translation-with-Attention)","ea705e22":"# Data Preparation\n\n**Build Vocabulary**","328fbdf0":"# Model Inference","8924c452":"# Create Dataloaders","54d5fdad":"# Define Model Architecture","bd5f9de7":"# Decoder Architecture","869dfdf6":"# *Please upvote the kernel if you find it insightful*","d9f8ff5d":"# Build Inference Function"}}