{"cell_type":{"912fa682":"code","d3eb4f4c":"code","fc92254a":"code","567c3959":"code","5e2c3b03":"code","b1364d96":"code","3cd82248":"code","01df666e":"code","9160c1ed":"code","74f4b5cb":"code","461a6703":"code","f5be0622":"code","43cd9557":"markdown"},"source":{"912fa682":"import numpy as np\nimport pandas as pd\nimport os, sys, gc, warnings, random\n\n## Sklearn utils\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import KFold\n\n## LGB\nimport lightgbm as lgb\n\n## Turn off warnings\nwarnings.filterwarnings('ignore')\n\n## SEEDer\ndef seed_everything(seed=0):\n    random.seed(seed)\n    np.random.seed(seed)","d3eb4f4c":"########################### Initial Vars\n###########################################################\nTARGET    = 'target'   # Our Target\nSEED      = 42         # Base SEED\nN_SPLITS  = 5          # Number of Kfold Splits\nPATH      = '..\/input\/tabular-playground-series-mar-2021\/'\n\ncat_cols = ['cat'+str(i) for i in range(19)]  # Categorial Columns\ncnt_cols = ['cont'+str(i) for i in range(11)] # Continuous Columns \n\nremove_features = ['id',TARGET] # Features that we will not use for training","fc92254a":"########################### Data\n###########################################################\n\n# Main data\ntrain_df = pd.read_csv(PATH+'train.csv')\ntest_df  = pd.read_csv(PATH+'test.csv')\n\n# Combine train and test\n# and assign new target\ntrain_df[TARGET] = 1\ntest_df[TARGET]  = 0\nall_df = pd.concat([train_df, test_df]).reset_index(drop=True)\n\ndel train_df, test_df","567c3959":"########################### Categorical encoding\n###########################################################\n# For the Adversarial Validation we will not\n# do any \"fancy\" encoding\nfor col in cat_cols:   \n    all_df[col] = all_df[col].astype('category')","5e2c3b03":"########################### Models params and Features\n###########################################################\nfeatures_columns = [col for col in list(all_df) if col not in remove_features]\n\nlgb_params = {\n                'boosting_type': 'gbdt',\n                'objective': 'binary',\n                'metric': 'auc',\n                'n_estimators': 200,\n                'learning_rate': 0.05,\n                'num_leaves': 2**7,\n                'min_data_in_leaf': 2**8,\n                'feature_fraction': 0.7,\n                'subsample': 0.7,\n                'subsample_freq': 1,\n                'early_stopping_rounds': 100,\n                'boost_from_average': True,\n                'seed': SEED,\n                'verbose': -1\n            }","b1364d96":"########################### LGB Model\n###########################################################\n\n# We have enough data to use normal Kfold split\nfolds = KFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n\n# Separate train features and target\nX,y   = all_df[features_columns], all_df[TARGET]\n\n# Create column to store predictions\nall_df['preds'] = 0\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(X,y)):\n        \n    print('Fold:',fold_+1)\n        \n    # Creating lgb train\/valid data\n    tr_x, tr_y = X.iloc[trn_idx,:], y[trn_idx] \n    vl_x, vl_y = X.iloc[val_idx,:], y[val_idx] \n        \n    train_data = lgb.Dataset(tr_x, label=tr_y)\n    valid_data = lgb.Dataset(vl_x, label=vl_y)\n        \n    # Train Model\n    seed_everything(SEED)\n    estimator = lgb.train(\n                          lgb_params,\n                          train_data,\n                          valid_sets = [train_data,valid_data],\n                          verbose_eval = 100,\n                        )\n        \n    all_df.iloc[val_idx, len(list(all_df))-1] += (estimator.predict(vl_x)) \n\nprint(roc_auc_score(all_df[TARGET], all_df['preds']))","3cd82248":"# 0.5010550371750001 is a good result\n# and seems that you don't have to do anything special\n# with train\/test values and their distributions","01df666e":"# If the score is greater than 0.6 you may want to see \n# importances chart to find \"traitor\" feature\nlgb.plot_importance(estimator, figsize=(20,20))","9160c1ed":"########################### Fast example from other competition\n###########################################################\nTARGET    = 'target'   # Our Target\nSEED      = 42         # Base SEED\nN_SPLITS  = 2          # Number of Kfold Splits\nPATH      = '..\/input\/ieee-fraud-detection\/'\ntrain_df = pd.read_csv(PATH+'train_transaction.csv')\ntest_df  = pd.read_csv(PATH+'test_transaction.csv')\ntrain_df[TARGET] = 1\ntest_df[TARGET]  = 0\ndel train_df['isFraud']\n\nall_df = pd.concat([train_df, test_df]).reset_index(drop=True)\ndel all_df['TransactionDT'], all_df['TransactionID'] # obvious \"leakers\"\n\ndel train_df, test_df\n\nfor col in list(all_df):\n    if all_df[col].dtype=='O':\n        all_df[col] = all_df[col].astype('category')","74f4b5cb":"########################### LGB Model\n###########################################################\nfeatures_columns = [col for col in list(all_df) if col not in [TARGET]]\nfolds = KFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\nall_df['preds'] = 0\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(all_df[TARGET],all_df[TARGET])):\n    print('Fold:',fold_+1)\n    train_data = lgb.Dataset(all_df[features_columns].iloc[trn_idx,:], \n                             label=all_df[TARGET][trn_idx])\n    valid_data = lgb.Dataset(all_df[features_columns].iloc[val_idx,:], \n                             label=all_df[TARGET][val_idx])\n    estimator = lgb.train(lgb_params,train_data,\n                          valid_sets = [train_data,valid_data],\n                          verbose_eval = 100)\n    all_df.iloc[val_idx, len(list(all_df))-1] += (estimator.predict(all_df[features_columns].iloc[val_idx,:])) \n    break # we will run only 1 fold - hust for fast check\n    \nprint(roc_auc_score(all_df[TARGET], all_df['preds']))\n","461a6703":"# Score 0.88+ is toooo big \n# It means that something wrong with values\n# or distributions in train\/test sets","f5be0622":"lgb.plot_importance(estimator, figsize=(20,20))","43cd9557":"### The Adversarial validation.\n\nOr in other words, we will try to see if our classification model will be able to distinguish the train set from the test set and if yes - we can see features importances to understand how it managed to do it.\n\nThe main idea of this technic is very simple:\n- Set a binary target for the train\/test set (train 1 \/ test 0 for example)\n- Combine train and test in one dataset\n- Run any Classification model to see if there is a significant difference in train\/test sets.\n\nIf we got roc auc result near 0.5 (0.5-0.6) - all good, and there are no significant differences. It also means that overfitting most likely will not come from features values differences.\n\nIf we have roc auc score >0.6 - it's a sign of some \"leaky\" feature or values distributions in train\/test sets and you should look closer and do some cleaning. "}}