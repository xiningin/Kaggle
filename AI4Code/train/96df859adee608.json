{"cell_type":{"cbddbb64":"code","ada46cc7":"code","7932a7fe":"code","bac2ea6c":"code","9fc852b8":"code","893a9c07":"code","735547f8":"code","4274e0f3":"code","74347e25":"code","9ffd6ebf":"code","2ab56393":"code","97842c06":"code","d8d0f8c5":"code","644060a3":"code","c3e1ff12":"code","2ef79295":"code","a101c502":"code","8aede3a2":"code","a0009775":"code","a5ef562f":"code","f6ce904c":"code","4b1ddb3c":"code","e48b37ef":"code","3cf70b28":"code","5fa0545a":"code","0719f233":"code","fa8993c7":"code","2aabda9c":"code","71573480":"code","c552b757":"code","e27a3cac":"code","7911abd6":"code","54ecc791":"code","78f0ef71":"code","e476a15b":"code","91b56cae":"code","75de9e03":"code","08888d1a":"code","992fabf5":"code","56493ced":"code","b2d77696":"code","3d58cfa1":"code","65800b80":"code","3613752f":"code","fdfc4318":"code","00455996":"code","492e2c30":"code","60904470":"code","cfbc6d7a":"code","d1f9ae9d":"code","d09289d1":"code","972d0ea1":"code","3ad15483":"code","67d5a560":"code","8f9a4206":"code","b91cdf3e":"code","3fdabe44":"code","c99d2f84":"code","d29b6040":"code","4cb1f58e":"code","25e0a14e":"code","ca829f3e":"code","54fb391b":"code","18692e72":"code","f5f52cd1":"code","9cf9fe10":"code","62d88dce":"code","c6f3631e":"code","a32fd148":"code","446d782f":"code","21020f44":"code","51b74766":"code","9837438e":"code","87c1c081":"code","44e3ab82":"code","6f6c19a2":"code","4a782878":"code","4d4d456b":"code","8f2c1fc7":"markdown","be468825":"markdown","396efbd8":"markdown","371bb71a":"markdown","7555f256":"markdown","116d2fae":"markdown","c7710ba0":"markdown","e7ed0d9f":"markdown","405a5932":"markdown","62a8736f":"markdown","5f5598d4":"markdown","3efc3104":"markdown","b31c0786":"markdown","acf550ed":"markdown","660c47c8":"markdown","b43768a4":"markdown","436a55c0":"markdown","99c6fd71":"markdown","12a7da7a":"markdown","9d9792ac":"markdown","ec86dbf2":"markdown","65d9b4b7":"markdown","13cbc109":"markdown","6335b846":"markdown","d7739e22":"markdown","1a558812":"markdown","a985e920":"markdown","e6767dc1":"markdown","aee810b7":"markdown","47cb938d":"markdown","b4cb663b":"markdown","9ba56394":"markdown","85abe76c":"markdown","b4128b30":"markdown","f4e08240":"markdown","257d93e4":"markdown"},"source":{"cbddbb64":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ada46cc7":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom scipy.stats import norm, skew \nfrom scipy import stats\nimport seaborn as sns\nfrom sklearn.preprocessing import RobustScaler\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import cross_val_score, GridSearchCV, KFold, RandomizedSearchCV, train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import GridSearchCV\n","7932a7fe":"test_data=pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\n\ntrain_data=pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\n\ny_train = train_data.SalePrice\nX_train = train_data.drop(['SalePrice'],axis=1).copy().set_index('Id')\n\nX_test=pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv').set_index('Id')","bac2ea6c":"all_x = pd.concat([X_test,X_train])","9fc852b8":"all_x.shape","893a9c07":"train_data.SalePrice","735547f8":"df=all_x.copy()","4274e0f3":"df.info()","74347e25":"df.describe()","9ffd6ebf":"y_train.describe()","2ab56393":"# Sale prices are not normally distributed.\n(mu, sigma) = norm.fit(y_train)\nsns.distplot(y_train , fit=norm);\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\nplt.ylabel('Frequency')\nplt.title('Sale Price distribution (Original)')","97842c06":"#Correlation map to see how features are correlated with SalePrice\ndf_xytrain = pd.concat((df.reset_index(), y_train.rename('SalePrice')), axis=1)\n\ncorrmat = df_xytrain.corr()\nplt.subplots(figsize=(12,9))\nsns.heatmap(corrmat, vmax=0.9, square=True)","d8d0f8c5":"import seaborn as sns\n#correlation matrix\ncorrmat = df_xytrain.corr()\nf, ax = plt.subplots(figsize=(20, 10))\nsns.heatmap(corrmat[(corrmat >= 0.5) | (corrmat <= -0.5)], vmax=.8, square=True, annot=True);\nplt.show()","644060a3":"numeric = df_xytrain.select_dtypes('number')\nmissing=numeric.columns[numeric.isna().any()].tolist()\nnumeric[missing].isna().sum()","c3e1ff12":"# df=all_x.copy()\ndf.MasVnrArea.fillna(0,inplace=True)\ndf.GarageYrBlt.fillna(df.YearBuilt,inplace=True)","2ef79295":"from sklearn.linear_model import ElasticNet\nen = ElasticNet()\ndata_4_fill = X_train[['LotFrontage','1stFlrSF','LotArea']].dropna()\nen.fit(data_4_fill[['LotArea','1stFlrSF']],data_4_fill.LotFrontage)\n\ndf.LotFrontage.fillna(pd.Series(en.predict(df[['LotArea','1stFlrSF']])),inplace=True)","a101c502":"numeric = df.select_dtypes('number')\nmissing=numeric.columns[numeric.isna().any()].tolist()\nnumeric[missing].isna().sum()","8aede3a2":"df[missing]=df[missing].fillna(0)","a0009775":"numeric = df.select_dtypes('number')\nmissing=numeric.columns[numeric.isna().any()].tolist()\nnumeric[missing].isna().sum()","a5ef562f":"vars_2_fill_with_mode = ['Electrical']+['Utilities', 'Exterior1st', 'Exterior2nd', 'KitchenQual',\n        'Functional', 'SaleType']\nfor i in vars_2_fill_with_mode:\n    df[i].fillna(df[i].mode().iloc[0],inplace=True)","f6ce904c":"vars_2_fill_with_none = ['BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'\n                          ]+['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'\n                            ]+['FireplaceQu', 'PoolQC', 'Fence', 'MasVnrType']\nfor i in vars_2_fill_with_none:\n    df[i].fillna('None',inplace=True)","4b1ddb3c":"\ndf['TotalSF']=df['TotalBsmtSF'] + df['1stFlrSF'] + df['2ndFlrSF']\n\ndf['Total_Bathrooms'] = (df['FullBath'] + (0.5 * df['HalfBath']) +\n                               df['BsmtFullBath'] + (0.5 * df['BsmtHalfBath']))\n\ndf['Total_porch_sf'] = (df['OpenPorchSF'] + df['3SsnPorch'] +\n                              df['EnclosedPorch'] + df['ScreenPorch'] +\n                              df['WoodDeckSF'])","e48b37ef":"# with umber what is likely categirie or number\n# select feature for sure numerical \nlikely_cat = []\nlikely_num = []\nfor var in list(df.select_dtypes('number').columns):\n    if 1.*df[var].nunique()\/df[var].count() < 0.007: #or some other threshold\n        likely_cat.append(var)\n    else:\n        likely_num.append(var)","3cf70b28":"%matplotlib inline\nimport matplotlib.pyplot as plt\ndf[likely_num].hist(bins=50, figsize=(20,15))\n# save_fig(\"attribute_histogram_plots\")\nplt.show()","5fa0545a":"## select most skewed features\n\nnumeric_feats = pd.Index(likely_num)\n\n# Check the skew of all numerical features\nskewed_feats = df[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness.head(10)","0719f233":"## Box Cox Transformation of (highly) skewed features\n\nskewness = skewness[abs(skewness) > 0.75]\nprint(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n\nfrom scipy.special import boxcox1p\nskewed_features = skewness.index\nlam = 0.15\nfor feat in skewed_features:\n    #all_data[feat] += 1\n    df[feat] = boxcox1p(df[feat], lam)\n    # df[feat] = np.log1p(df[feat])\n","fa8993c7":"%matplotlib inline\nimport matplotlib.pyplot as plt\ndf[likely_num].hist(bins=50, figsize=(20,15))\n# save_fig(\"attribute_histogram_plots\")\nplt.show()","2aabda9c":"# from sklearn.neighbors import LocalOutlierFactor\n\n# lof = LocalOutlierFactor(contamination=0.01)\n# yhat = lof.fit_predict(X_train)\n\n# # select all rows that are not outliers\n# mask = yhat != -1\n\n# #select all rows that are outliers\n# non_mask = yhat == -1\n# X_outlier, y_outlier = X_train[non_mask, :], y_train[non_mask]\n# print(X_outlier, \"\\n\\n\", y_outlier)","71573480":"def outlier_treatment(feature): \n    sorted(feature) \n    q1,q3 = np.percentile(feature , [25,75]) \n    IQR = q3 - q1 \n    lower_range = q1 - (3 * IQR) \n    upper_range = q3 + (3 * IQR) \n    return lower_range,upper_range","c552b757":"# lower_range, upper_range = outlier_treatment(train['OverallQual'])\n# overall_qual_drop = train[(train['OverallQual'] < lower_range) | (train['OverallQual'] > upper_range)]\n# print('Number of outliers: {}'.format(len(overall_qual_drop)))","e27a3cac":"# lower_range, upper_range = outlier_treatment(train['OverallQual'])\n# overall_qual_drop = train[(train['OverallQual'] < lower_range) | (train['OverallQual'] > upper_range)]\n# print('Number of outliers: {}'.format(len(overall_qual_drop)))","7911abd6":"\ndf['KitchenQual'] = df['KitchenQual'].map({\"Fa\":1, \"TA\":2, \"Gd\":3, \"Ex\":4})\n# all_data['LotShape'] = all_data['LotShape'].map({\"Reg\":1, \"IR1\":2, \"IR2\":3, \"IR3\":4})\ndf['LandContour'] = df['LandContour'].map({\"Bnk\":1, \"Lvl\":2, \"Low\":3, \"HLS\":4})\ndf['FireplaceQu'] = df['FireplaceQu'].map({\"None\":0, \"Po\":1, \"Fa\":2, \"TA\":3, \"Gd\":4, \"Ex\":5})\ndf['Functional'] = df['Functional'].map({\"Sev\":1, \"Maj2\":2, \"Maj1\":3, \"Mod\":4, \"Min2\":5, \"Min1\":6, \"Typ\":7})\ndf['ExterQual'] = df['ExterQual'].map({\"Fa\":1, \"TA\":2, \"Gd\":3, \"Ex\":4})\ndf['GarageQual'] = df['GarageQual'].map({\"None\":0, \"Po\":1, \"Fa\":2, \"TA\":3, \"Gd\":4, \"Ex\":5})\ndf['GarageCond'] = df['GarageCond'].map({\"None\":0, \"Po\":1, \"Fa\":2, \"TA\":3, \"Gd\":4, \"Ex\":5})\ndf['LandSlope'] = df['LandSlope'].map({\"Gtl\":1, \"Mod\":2, \"Sev\":2})\ndf['HeatingQC'] = df['HeatingQC'].map({\"Po\":1, \"Fa\":2, \"TA\":3, \"Gd\":4, \"Ex\":5})\ndf['BsmtQual'] = df['BsmtQual'].map({'None':0, 'Fa':1, 'TA':2, 'Gd':3, 'Ex':4})\ndf['BsmtCond'] = df['BsmtCond'].map({'None':0, 'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5})\ndf['BsmtExposure'] = df['BsmtExposure'].map({'None':0, 'No':1, 'Mn':2, 'Av':3, 'Gd':4})","54ecc791":"df = df.replace({\"MSSubClass\" : {20 : \"SC20\", 30 : \"SC30\", 40 : \"SC40\", 45 : \"SC45\", \n                                       50 : \"SC50\", 60 : \"SC60\", 70 : \"SC70\", 75 : \"SC75\", \n                                       80 : \"SC80\", 85 : \"SC85\", 90 : \"SC90\", 120 : \"SC120\", \n                                       150 : \"SC150\", 160 : \"SC160\", 180 : \"SC180\", \n                                       190 : \"SC190\"},\n                       \"MoSold\" : {1 : \"Jan\", 2 : \"Feb\", 3 : \"Mar\", 4 : \"Apr\", 5 : \"May\", \n                                   6 : \"Jun\", 7 : \"Jul\", 8 : \"Aug\", 9 : \"Sep\", 10 : \"Oct\", \n                                   11 : \"Nov\", 12 : \"Dec\"}\n                      })","78f0ef71":"for i in [i for i in df.columns if 'Yr' in i]:\n    df[i]=2011-df[i]","e476a15b":"# df[['GarageYrBlt', 'YrSold']]","91b56cae":"def make_top_categories(data,col_name,min_ratio=0.1):\n    ### makes a new variable with the top categories and all small categories grouped into other.\n    ### example: data2[\"new_PRD_CLASS\"]=make_top_categories(data2,\"PRD_PROD_CLASS_NAME\",0.01)\n    import pandas as pd\n    import numpy as np\n    counts = data[col_name].value_counts()\n    counts_df = pd.DataFrame({col_name:counts.index,\"count\":counts.values})\n    min_count=len(data)*min_ratio\n    counts_df.loc[:,\"new_name\"]=np.where([counts_df[\"count\"]>min_count],counts_df[col_name],'other')[0]\n    jj = data.merge(counts_df,on=col_name,how='left')\n    return jj[\"new_name\"]\ndef make_top_categories_df(data,min_ratio=0.1):\n    data2=data.copy()\n    dtypes_to_encode = ['object', 'category']\n    data_to_encode =data2.select_dtypes(include=dtypes_to_encode)\n    for (col) in data_to_encode.iteritems():\n            data2.loc[:,col[0]] = make_top_categories(data2,col[0],min_ratio=min_ratio)\n    return data2\n","75de9e03":"df2= make_top_categories_df(df,.05)","08888d1a":"pd.get_dummies(df2,drop_first=True).shape,pd.get_dummies(df,drop_first=True).shape","992fabf5":"df3 =pd.get_dummies(df2,drop_first=True)","56493ced":"corrs = df3.corr().reset_index().melt(id_vars='index')\nhi_corr_vars = corrs.query('index>variable and value>.9 ').variable.unique()\ndf4=df3.drop(hi_corr_vars,axis=1)","b2d77696":"df4.columns = [i.replace('<','_').replace(',','_') for i in df4.columns]","3d58cfa1":"df4","65800b80":"train =df4.loc[X_train.index,:]\ntest =df4.loc[X_test.index,:]\ny=np.log(y_train)","3613752f":"# fit the training set only, then transform both the training and test sets\nscaler = RobustScaler()\ntrain[likely_num]= scaler.fit_transform(train[likely_num])\ntest[likely_num]= scaler.transform(test[likely_num])\n\ntrain.shape, test.shape # now we have 220 features!","fdfc4318":"test.to_pickle('test')\ntrain.to_pickle('train')\ny.to_pickle('y')","00455996":"len(y)","492e2c30":"train.shape","60904470":"all_train = pd.concat((train.reset_index(), y.rename('SalePrice')), axis=1)\nall_train","cfbc6d7a":"plt.figure(figsize=(5,20))\nsns.set(font_scale=1)\nsns.heatmap(all_train.corr()[['SalePrice']].sort_values('SalePrice', ascending=False), annot=True);","d1f9ae9d":"#Correlation with output variable\ncor = all_train.corr()\ncor_target = abs(cor[\"SalePrice\"])\ncor_target","d09289d1":"#Selecting highly correlated features\nrelevant_features = cor_target[cor_target>0.5]\nfeature_pearson = list(relevant_features.index)\nfeature_pearson","972d0ea1":"num_feats = 15\ndf_y=pd.DataFrame(y, columns=['SalePrice']) ","3ad15483":"from sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\n\n\nrfe_selector = RFE(estimator=LinearRegression(), n_features_to_select=num_feats, step=10, verbose=5)\nrfe_selector.fit(train.reset_index(drop=True), df_y)\nrfe_support = rfe_selector.get_support()\nrfe_feature = train.reset_index(drop=True).loc[:,rfe_support].columns.tolist()\nprint(str(len(rfe_feature)), 'selected features')","67d5a560":"rfe_feature","8f9a4206":"from sklearn.feature_selection import SelectFromModel\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import RandomForestRegressor as RClfe\n\n\nembeded_rf_selector = SelectFromModel(RandomForestRegressor(n_estimators=100), max_features=num_feats)\nembeded_rf_selector.fit(train.reset_index(drop=True), df_y)\n\nembeded_rf_support = embeded_rf_selector.get_support()\nembeded_rf_feature = train.reset_index(drop=True).loc[:,embeded_rf_support].columns.tolist()\nprint(str(len(embeded_rf_feature)), 'selected features')\nembeded_rf_feature","b91cdf3e":"from lightgbm import LGBMRegressor\nfrom sklearn.feature_selection import SelectFromModel\nfrom xgboost import XGBRegressor, plot_importance\n\nlgbc=LGBMRegressor(n_estimators=500, learning_rate=0.05, num_leaves=32, colsample_bytree=0.2,\n            reg_alpha=3, reg_lambda=1, min_split_gain=0.01, min_child_weight=40)\n\nembeded_lgb_selector = SelectFromModel(lgbc, max_features=num_feats)\nembeded_lgb_selector.fit(train.reset_index(drop=True), df_y)\n\n\nembeded_lgb_support = embeded_lgb_selector.get_support()\nembeded_lgb_feature = train.reset_index(drop=True).loc[:,embeded_lgb_support].columns.tolist()\nprint(str(len(embeded_lgb_feature)), 'selected features')\n\n\nembeded_lgb_feature ","3fdabe44":"xgb=XGBRegressor(n_estimators=500, learning_rate=0.05, num_leaves=32, colsample_bytree=0.2,\n            reg_alpha=3, reg_lambda=1, min_split_gain=0.01, min_child_weight=40)\n\nembeded_xgb_selector = SelectFromModel(xgb, max_features=num_feats)\nembeded_xgb_selector.fit(train.reset_index(drop=True), df_y)\nembeded_xgb_support = embeded_xgb_selector.get_support()\nembeded_xgb_feature = train.reset_index(drop=True).loc[:,embeded_xgb_support].columns.tolist()\nprint(str(len(embeded_lgb_feature)), 'selected features')\nembeded_xgb_feature","c99d2f84":"\n# fit model to training data\nxgb_model = XGBRegressor(random_state = 0 )\nxgb_model.fit(train.reset_index(drop=True).reset_index(drop=True), df_y)\n\nprint(\"Feature Importances : \", xgb_model.feature_importances_)\n\n# plot feature importance\nplot_importance(xgb_model)\nplt.show()","d29b6040":"# once we selected all features with differents technique take features that have been observed in all selections\nfinal_list_feature = list(set.intersection(*map(set, [embeded_lgb_feature, embeded_rf_feature, \n                                                      feature_pearson, rfe_feature, embeded_xgb_feature])))\n\nfinal_list_feature","4cb1f58e":"imp = final_list_feature","25e0a14e":"Feature_selected =  final_list_feature","ca829f3e":"xgb = XGBRegressor()\n# xgb.fit(train, y)","54fb391b":"df_train_x = train.reset_index(drop=True)[Feature_selected]\ndf_train_x.head()","18692e72":"X_train = df_train_x.values\ny_train = y.values\ny_train","f5f52cd1":"df_test_x = X_test.reset_index(drop=True)\nX_test = df_test_x.values\nX_test","9cf9fe10":"from sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.33, random_state=42)","62d88dce":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom sklearn.model_selection import GridSearchCV\nparam_grid = [\n{'n_estimators': [3, 10], 'max_features': [2, 4]},\n{'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3]},\n]\nforest_reg = RandomForestRegressor()\ngrid_search = GridSearchCV(forest_reg, param_grid, cv=5,\nscoring='neg_mean_squared_error',\nreturn_train_score=True)\ngrid_search.fit(X_train, y_train)\n\n\n\n# param_grid = [\n# {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n# {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n# ]\n# forest_reg = RandomForestRegressor()\n# grid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n# scoring='neg_mean_squared_error',\n# return_train_score=True)\n# grid_search.fit(X_train, y_train)\n\nprint('best_params ', grid_search.best_params_)\nprint('best_estimator ', grid_search.best_estimator_)","c6f3631e":"cvres = grid_search.cv_results_\nfor mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n    print(np.sqrt(-mean_score), params)\n    \n\nfinal_model = grid_search.best_estimator_\n\n# X_test_prepared = full_pipeline.transform(X_val)\nfinal_predictions = final_model.predict(X_val)\n\nfinal_mse = mean_squared_error(y_val, final_predictions)\nfinal_rmse = np.sqrt(final_mse)\nfinal_rmse","a32fd148":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport xgboost\n\n\nparam_grid = [\n{'n_estimators': [3, 200, 300], 'max_features': [2, 4, 6, 8]},\n{'bootstrap': [False, True], 'n_estimators': [3, 5, 10], 'max_features': [2, 3]},\n]\nforest_reg = xgboost.XGBRegressor(verbosity = 0)\ngrid_search = GridSearchCV(forest_reg, param_grid, cv=5,\nscoring='neg_mean_squared_error',\nreturn_train_score=True)\ngrid_search.fit(X_train, y_train)\n\n\n\n# param_grid = [\n# {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n# {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n# ]\n# forest_reg = RandomForestRegressor()\n# grid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n# scoring='neg_mean_squared_error',\n# return_train_score=True)\n# grid_search.fit(X_train, y_train)\n\nprint('best_params ', grid_search.best_params_)\nprint('best_estimator ', grid_search.best_estimator_)","446d782f":"cvres = grid_search.cv_results_\nfor mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n    print(np.sqrt(-mean_score), params)\n    \n\nfinal_model = grid_search.best_estimator_\n\n# X_test_prepared = full_pipeline.transform(X_val)\nfinal_predictions = final_model.predict(X_val)\n\nfinal_mse = mean_squared_error(y_val, final_predictions)\nfinal_rmse = np.sqrt(final_mse)\nfinal_rmse","21020f44":"from sklearn.svm import SVR\n\nfrom sklearn.svm import SVR\nimport numpy as np\n\nparameters = {'kernel': ('linear', 'rbf','poly'), 'C':[1.5, 3, 5 , 7.5,10],'gamma': [1e-7, 1e-4, 1e-2, 1e-1],'epsilon':[0.1,0.2,0.5,0.3]}\nsvr = SVR()\ngrid_search = GridSearchCV(svr, parameters)\ngrid_search.fit(X_train, y_train)\n\n\nprint('best_params ', grid_search.best_params_)\nprint('best_estimator ', grid_search.best_estimator_)","51b74766":"# cvres = grid_search.cv_results_\n# for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n#     print(np.sqrt(-mean_score), params)\n    \n\nfinal_model = grid_search.best_estimator_\n\n# X_test_prepared = full_pipeline.transform(X_val)\nfinal_predictions = final_model.predict(X_val)\n\n\nfinal_mse = mean_squared_error(y_val, final_predictions)\nfinal_rmse = np.sqrt(final_mse)\nfinal_rmse\n\n\nfinal_mse = mean_squared_error(np.exp(y_val),  np.exp(final_predictions))\nfinal_rmse = np.sqrt(final_mse)\nfinal_rmse","9837438e":"plt.figure(figsize=(10, 5))\nplt.scatter(y_val, final_predictions, s=20)\nplt.title('log Predicted vs. log Actual')\nplt.xlabel('log Actual Sale Price')\nplt.ylabel('log Predicted Sale Price')\n\nplt.plot([min(y_val), max(y_val)], [min(y_val), max(y_val)])\nplt.tight_layout()","87c1c081":"\nplt.figure(figsize=(10, 5))\nplt.scatter(np.exp(y_val), np.exp(final_predictions), s=20)\nplt.title('Predicted vs. Actual')\nplt.xlabel('Actual Sale Price')\nplt.ylabel('Predicted Sale Price')\n\nplt.plot([min(np.exp(y_val)), max(np.exp(y_val))], [min(np.exp(y_val)), max(np.exp(y_val))])\nplt.tight_layout()","44e3ab82":" range(20, 200)","6f6c19a2":"from scipy.stats import expon\n\n# Note: gamma is ignored when kernel is \"linear\"\n# param_distribs = {\"eta\" :[0.1, 10],\n#         'kernel': ['linear', 'rbf'],\n#         'C': range(20, 200000),\n#         'gamma': expon(scale=1.0),\n#     }\n\n\n# param_distribs = { \n#     \"eta\" :[0.1, 10],\n#     \"C\": [.01, .1, 1, 5, 10, 100],\n#     \"gamma\": [.01, .1, 1, 5, 10, 100],\n#     \"kernel\": ['linear', 'rbf'],\n#     \"random_state\": [1]\n# }\n\nparam_distribs = [\n    {\n        'kernel': ['rbf','poly'],\n        'C': [.01, .1, 1, 5, 10, 100],\n        'gamma': expon(scale=1.0),\n        'degree': expon(scale=1.0)\n    },\n    {\n        'kernel': ['linear','sigmoid'],\n        'C':  [.01, .1, 1, 5, 10, 100]\n    }\n]\n\nsvm_reg = SVR()\nrnd_search = RandomizedSearchCV(svm_reg, param_distributions=param_distribs,\n                                n_iter=50, cv=5, scoring='neg_mean_squared_error',\n                                verbose=0, random_state=0)\nrnd_search.fit(X_train, y_train)","4a782878":"rnd_search.best_estimator_","4d4d456b":"negative_mse = rnd_search.best_score_\nrmse = np.sqrt(-negative_mse)\nrmse","8f2c1fc7":"Lets deal with missing values","be468825":"# Missing Values","396efbd8":"## Categorical","371bb71a":"Some features have a lot of missing values, it can be treat like following:\n\n- for MasVnrArea : replace by 0\n- GarageYrBlt : replace by YearBuilt feature\n- LotFrontage : use Elasticnet usinf 1stFlrSF','LotArea' features\n- for the rest replace by 0 (usually if no garage are indicated it is mean that there is no garage)\n\n","7555f256":"It is possible to \n\n- sum the SF in alls the floors\n- sum the Bathroom\n- sum porches","116d2fae":"**lgbc feature selection**","c7710ba0":"## Numeric","e7ed0d9f":"## numeric","405a5932":"Date can be replace by how old is the house","62a8736f":"Make diffrente features selections with different algorythm","5f5598d4":"Onnly some feature are really correlted with the price of the appartement","3efc3104":"## 2. Xgboost","b31c0786":"### Numeric which are really categorical","acf550ed":"**xboost feature selection**","660c47c8":"## categorical","b43768a4":"Fill other categorical Nan with 'None' string","436a55c0":"# Feature Engineering","99c6fd71":"### yr -> yrs ago","12a7da7a":"Get all categories in dummies features","9d9792ac":"Also sometimes number have no meaning so it can be replace by categories","ec86dbf2":"### Total sq. ft. variables","65d9b4b7":"**Pearson feature selection**","13cbc109":"# EDA","6335b846":"### remove numerical outliers ","d7739e22":"#### remove x outliers","1a558812":"**Tree based feature selection random forest**","a985e920":"Some features are categorical but can be transforme to numerical according the standard of the feature, for example according mark out of a certain number.","e6767dc1":"## adds RandomizedSearchCV","aee810b7":"## 3 svm","47cb938d":"### Create \"other\" category for infrequent categories","b4cb663b":"# Feature selection","9ba56394":"# Modelling ","85abe76c":"### Order quality data\n","b4128b30":"#### remove y outliers","f4e08240":"### Skewed features","257d93e4":"**RFE feature selection**"}}