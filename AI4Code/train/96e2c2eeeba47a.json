{"cell_type":{"e05647b3":"code","dd76f75c":"code","9817887e":"code","0cfa415f":"code","df5c3507":"code","789c8670":"code","a11d2513":"code","7d4882b4":"code","85e74047":"code","bb957ad3":"code","d5c063c2":"code","ff8cd9c7":"code","db71a119":"code","5f9d9e37":"code","67261a4a":"code","53b65e3e":"code","295830fa":"code","62e727cc":"code","88196551":"code","7f33c6e4":"code","0a99abf9":"code","747588fe":"code","74b013fd":"code","824d9479":"code","90da677b":"code","54a5b2d7":"code","968863b5":"code","f1488a9d":"code","91fc3ee1":"code","3176512d":"code","cb5244e3":"code","2bbd5688":"code","12027f2b":"code","a916ad35":"code","c0751a3b":"code","f254934a":"code","c6a978f1":"code","f35de999":"markdown","8502d8ba":"markdown","78dd7740":"markdown","518b4bfe":"markdown","3573e0ec":"markdown","fa97c25f":"markdown","14443832":"markdown","0e68a29f":"markdown","bb5a29b0":"markdown","95c5a808":"markdown","fc06525f":"markdown","048fd2f8":"markdown","632dea21":"markdown","b8f4afc9":"markdown","2947b5de":"markdown","ac45b456":"markdown","eaf28949":"markdown","9c9269ea":"markdown","26b49cb3":"markdown","0e011e34":"markdown","85b9f462":"markdown","d6545ce2":"markdown","39b93276":"markdown"},"source":{"e05647b3":"import sys #access to sysem parameters\nimport pandas as pd #collection of functions for data processing and alaysis \npd.set_option('display.max_columns',None)\nimport numpy as np #foundation package for scientific computing\nnp.set_printoptions(threshold=np.inf)\nimport sklearn #collection of machine learning algorithms\nimport matplotlib.pyplot as plt #collection of functions for visualization\nimport scipy as sp #collection of functions for scientific computing and advance mathematics\n\n#import IPython\n#from Iphython import display # pretty printing of dataframes in Jupyter notebook\n#misc libraries\nimport random\nimport time\n#ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n###load data modelling libraries\n#Common model algorithms\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\n#from xgboost import XGBClassifier\n#Common model helpers\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\n\n###Visualization\nimport matplotlib as mpl\n#import matplotlib.pylot as plt\nimport matplotlib.pylab as pylab\nimport seaborn as sns\nfrom pandas.tools.plotting import scatter_matrix\n#Configure visualization defaults\n#%matplotlib inline\nmpl.style.use('ggplot')\nsns.set_style('white')\npylab.rcParams['figure.figsize']=12,8","dd76f75c":"data_raw = pd.read_csv('..\/input\/train.csv')\n#891 passengers in the training set\ndata_val = pd.read_csv('..\/input\/test.csv')\n#418 passengers in the test set\n\n#print (train.info())\n#create a copy for the train data\n#data1 = data_raw.copy(deep = True)\ncombine = [data_raw, data_val]\n\ndata_raw.describe()","9817887e":"# Check for the missing values\nprint('Train columns with null values:\\n', data_raw.isnull().sum())\nprint(\"-\"*10)\n\nprint('Test\/Validation columns with null values:\\n', data_val.isnull().sum())\nprint(\"-\"*10)","0cfa415f":"###Visualize embarkment, Pclass, fare\nplt.figure(figsize=[15,7])\n\nplt.subplot(121)\nsns.boxplot(x = 'Embarked', y = 'Fare', data = data_raw)\n#Passengers embarked at C tend to pay higher fare\nplt.subplot(122)\nsns.boxplot(x = 'Pclass', y = 'Fare', data = data_raw)\n#Passengers from higher class tend to pay higher fare","df5c3507":"for dataset in combine:\n    dataset['Cabin_null'] =data_raw['Cabin'].isnull()*1\nage_survive = sns.barplot(x='Cabin_null', y='Survived', data=data_raw)\nage_survive = plt.xlabel('Cabin is Missing')\nage_survive = plt.ylabel('Survived')\nplt.figure(figsize=[7,7])\nplt.show()","789c8670":"###Complete the missing values\n# The missing Embarked data is filed in with the most frequent value\nfor dataset in combine:\n    dataset['Embarked'].fillna(dataset['Embarked'].mode()[0],inplace = True) # The missing embarked data is filled in with mode\n    #dataset['Fare'].fillna(dataset['Fare'].median(),inplace = True) # The missing fare data is filled in with median \n    \n# The missing Fare data is filled in with the mean fare value for the pclass the passenger belongs to \nfor x in range(len(data_val[\"Fare\"])):\n    if pd.isnull(data_val[\"Fare\"][x]):\n        pclass = data_val[\"Pclass\"][x] #Pclass = 3\n        data_val[\"Fare\"][x] = round(data_raw[data_raw[\"Pclass\"] == pclass][\"Fare\"].mean(), 4)","a11d2513":"###Feature engineering\n#Create the FareBin feature\n#Keep the first letter for Cabin data\nFare_Bins=[-1,7.91,14.454,31,10000]\nFare_Labels=['cheap','medium','medium high','high']\n\nfor dataset in combine:\n    dataset[\"Cabin\"] = pd.Series([i[0] if not pd.isnull(i) else 'X' for i in dataset['Cabin'] ])\n    dataset['FareBin']=pd.cut(dataset['Fare'], Fare_Bins, labels=Fare_Labels)\n\ndata_raw.head()","7d4882b4":"###Data visualization\n\nplt.figure(figsize=[12,10])\n\nplt.subplot(221)\nsns.barplot(x = 'Pclass', y = 'Survived', data = data_raw)\n#People with higher socieconomic class had a higher rate of survival\n\nplt.subplot(222)\nsns.barplot(x = 'Embarked', y = 'Survived', data = data_raw)\n#People embarked at C are more likely to survive\n\nplt.subplot(223)\nsns.barplot(x='Cabin',y='Survived',data = data_raw)\n#People with a recorded Cabin number are more likely to survive\n\nplt.subplot(224)\nsns.barplot(x='FareBin',y='Survived',data = data_raw)\n#People who pay a higher fare are more likely to survive","85e74047":"###Convert the features into numbers\n#cleanup_Embarked = {'S':0,'C':1,'Q':2}\ncleanup_FareBin ={'cheap':0,'medium':1,'medium high':2,'high':3}\nfor dataset in combine:\n    #dataset['Embarked']=dataset['Embarked'].map(cleanup_Embarked).astype(int)\n    dataset['FareBin']=dataset['FareBin'].map(cleanup_FareBin).astype(int)\n\ndata_raw.head()","bb957ad3":"# For ticket feature we only keep the prefix part\nfor dataset in combine:\n    Ticket = []\n    for i in list(dataset.Ticket):\n        if not i.isdigit() :\n            Ticket.append(i.replace(\".\",\"\").replace(\"\/\",\"\").strip().split(' ')[0]) #Take prefix\n        else:\n            Ticket.append(\"X\")\n    dataset[\"Ticket\"] = Ticket\ndataset[\"Ticket\"].head()","d5c063c2":"age_bins=[0,5,12,18,50,65,120]\nage_labels=['baby','kid','teenager','adult','aging','elderly']\nfor dataset in combine:\n    dataset['AgeGroup']=pd.cut(dataset['Age'], age_bins, labels=age_labels)\nage_survive = sns.barplot(x='AgeGroup', y='Survived', data=data_raw)\nage_survive = plt.xlabel('Age')\nage_survive = plt.ylabel('Survived')\nplt.figure(figsize=[7,5])\nplt.show()","ff8cd9c7":"for dataset in combine:\n    dataset['Age_null'] =data_raw['Age'].isnull()*1\nage_survive = sns.barplot(x='Age_null', y='Survived', data=data_raw)\nage_survive = plt.xlabel('Age is Missing')\nage_survive = plt.ylabel('Survived')\nplt.show()","db71a119":"# Complete missing age Values. we noticed that in the name, there is also title information, this can be helpful to predict the age\nfor dataset in combine:\n    dataset['Title'] = dataset.Name.str.extract('([A-Za-z]+)\\.', expand=False)\npd.crosstab(data_raw['Title'], data_raw['Sex'])","5f9d9e37":"# Group different title (especially rare titles) into common groups which are more closely related to the age\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].replace(['Capt', 'Col', 'Sir', 'Don', 'Dr', 'Major', 'Rev', 'Jonkheer', 'Rev', 'Countess', 'Lady', 'Dona'], 'Rare')\n#     dataset['Title'] = dataset['Title'].replace(['Countess', 'Lady', 'Dona'], 'Rare_F')\n    dataset['Title'] = dataset['Title'].replace(['Mlle', 'Ms'], 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    dataset['FamilySize'] = dataset['SibSp']+dataset['Parch'] + 1\n    dataset['IsAlone'] = 1 #initialize to yes\/1 is alone\n    dataset['IsAlone'].loc[dataset['FamilySize'] > 1] = 0 # now update to no\/0 if family size is greater than 1\npd.crosstab(data_raw['Title'], data_raw['Sex'])","67261a4a":"# According to our common sense, the title should be related to the age, e.g. title Miss should be younger than Mrs. To identify if the title is related to the title, visualize their relationship\nplt.figure(figsize=[15,7])\nplt.subplot(121)\nage_title = sns.boxplot(x=\"Title\", y=\"Age\",hue=\"Survived\", data=data_raw)\nage_title = plt.xlabel('Title')\nage_title = plt.ylabel('Age')\n# We assume the age is also related to the pclass so we plot the corresponding diagram to confirm\nplt.subplot(122)\nage_title = sns.boxplot(x=\"Pclass\", y=\"Age\",hue=\"Survived\", data=data_raw)\nage_title = plt.xlabel('PClass')\nage_title = plt.ylabel('Age')\nplt.show()","53b65e3e":"# assign the median of each [title, pclass] group if the age value is missed\ndef impute_age(dataset):\n   for pclass in [1,2,3]:\n        for title in ['Master','Miss','Mr','Mrs','Rare']:\n            ds=dataset[dataset['Pclass']==pclass]\n            ds=ds[ds['Title']==title]\n            median=ds['Age'].median()\n            dataset.loc[\n                (dataset['Age'].isnull())&\n                (dataset['Pclass']==pclass)&\n                (dataset['Title']==title),\n                'Age'\n            ]=median\nimpute_age(data_raw)\nimpute_age(data_val)","295830fa":"for dataset in combine:\n    dataset['FamilySize'] = dataset['SibSp']+dataset['Parch'] + 1\n\n# to study the influence of the family size to the survival chance and the distributino of different family size, \n# we use the bubble plot to visualize it. The size of each bubble is count of each family size.\nfs_count = data_raw['FamilySize'].value_counts()\nfs_prob_survived = data_raw['FamilySize'][data_raw['Survived'] == 1].value_counts()\nfs_prob = fs_prob_survived\/fs_count\nfamilysize = sns.relplot(fs_prob.index, fs_prob.values, size = fs_count.values, sizes = (100,1000), data=data_raw)\nplt.xlabel('FamilySize')\nplt.ylabel('Prob of Survived')\nplt.show()","62e727cc":"sns.set_style(\"white\")\nsex_sur = sns.barplot(x=\"Sex\", y=\"Survived\", data=data_raw)\nsex_sur = plt.xlabel('Sex')\nsex_sur = plt.ylabel('Survived')\nplt.show()","88196551":"#for dataset in combine:\n #   dataset[\"Age\"] = dataset[\"Age\"].fillna(-0.5)\n\nage_bins=[0,5,12,18,50,65,120]\nage_labels=['baby','kid','teenager','adult','aging','elderly']\nfor dataset in combine:\n    dataset['AgeGroup']=pd.cut(dataset['Age'], age_bins, labels=age_labels)\n\ncleanup_agegroup = {'baby':1,'kid':2,'teenager':3,'adult':4,'aging':5,'elderly':6}\ncleanup_sex = {'male':0,'female':1}\nfor dataset in combine:\n    dataset['AgeGroup']=dataset['AgeGroup'].map(cleanup_agegroup).astype(int)\n    dataset['Sex']=dataset['Sex'].map(cleanup_sex).astype(int)\ndata_raw.head()","7f33c6e4":"# drop_column = ['Name','Age','PassengerId','Fare','Cabin']\n# data_raw.drop(drop_column, axis=1, inplace=True)\n# data_val.drop(drop_column, axis=1, inplace=True)\n# data_val.head()","0a99abf9":"# First we combine the training data and test dat in order to get the same number of dummy features when using one hot encoded\n# to deal with the training data and test data together, we firstly separate features and labels in training data into data_raw and Y_train \nY_train = data_raw[\"Survived\"]\ndrop_column = ['Survived']\ndata_raw.drop(drop_column, axis=1, inplace=True)\n\ndata_raw_len = len(data_raw)\ndataset_comb =  pd.concat(objs=[data_raw, data_val], axis=0).reset_index(drop=True)","747588fe":"# in feature group1, we drop ['Name','AgeGroup','PassengerId','FareBin','Ticket','Cabin'] to use numerical age and fare for the model\n# in feature group2, instead we drop ['Name','Age','PassengerId','Fare','FamilySize','Cabin']\n\nFeature_Group1 = ['Pclass','Sex','Age','Age_null', 'FamilySize','SibSp', 'Parch','Fare','Cabin_null','Embarked', 'Title', 'IsAlone']\nFeature_Group2 = ['Pclass','Sex', 'AgeGroup','Age_null','SibSp', 'Parch', 'Ticket','FareBin','Cabin_null','Cabin','Embarked', 'Title', 'IsAlone']\n\ndataset1 = dataset_comb[Feature_Group1]\ndataset2 = dataset_comb[Feature_Group2]\ndataset1.head()","74b013fd":"# As for title and embarked features, \n# each different types of values should be of the same weight, \n# we encode them by one hot encoder to transfer each possible value into a boolean type\nfrom sklearn.preprocessing import OneHotEncoder\none_hot_encoded_dataset1 = pd.get_dummies(dataset1)\none_hot_encoded_dataset2 = pd.get_dummies(dataset2)\none_hot_encoded_dataset1.head()","824d9479":"from sklearn.preprocessing import MinMaxScaler\n# scaler_age = MinMaxScaler()\n# scaler_fare = MinMaxScaler()\n\n# # print([dataset1['Age']].size)\n# scaled_age = scaler_age.fit_transform(dataset1[['Age']])\n# scaled_fare = scaler_fare.fit_transform(dataset1[['Fare']])\n# dataset1['Age'] = scaled_age\n# dataset1['Fare'] = scaled_fare\n\none_hot_encoded_dataset1[['Age', 'Fare']] = MinMaxScaler().fit_transform(dataset1[['Age', 'Fare']])\n#one_hot_encoded_dataset1[['Age']] = MinMaxScaler().fit_transform(dataset1[['Age']])\n#one_hot_encoded_dataset1.head()\n# dataset2['Fare'] = preprocessing.normalize(dataset1['Fare'])\n\n# preprocessing.normalize(dataset1['Age'])\n# dataset2['Fare'] = preprocessing.normalize(dataset1['Fare'])","90da677b":"#Y_train = one_hot_encoded_data_raw[\"Survived\"]\n#drop_column = ['Survived']\n#one_hot_encoded_data_raw.drop(drop_column, axis=1, inplace=True)\n\nX_train = one_hot_encoded_dataset1[:data_raw_len]\nX_test = one_hot_encoded_dataset1[data_raw_len:]\n\nX_train.head()","54a5b2d7":"from sklearn.model_selection import train_test_split\nx_train, x_dev, y_train, y_dev = train_test_split(X_train, Y_train, test_size = 0.25, random_state = 1)","968863b5":"## Logistic classification\nfrom sklearn import linear_model\nfrom sklearn.metrics import accuracy_score, fbeta_score\nfrom sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression(penalty = 'l2', C = 0.2,random_state = 0)\nlogreg.fit(x_train, y_train)\ny_pred = logreg.predict(x_dev)\ny_train_pred = logreg.predict(x_train)\nacc_log = round(accuracy_score(y_pred, y_dev) * 100, 2)\nacc_log_train = round(accuracy_score(y_train_pred, y_train) * 100, 2)\nprint(\"the test error is\", acc_log, \"the training error is\", acc_log_train)","f1488a9d":"## Linear supporter vector classifier\nfrom sklearn.svm import SVC, LinearSVC\nlinear_svc = LinearSVC()\nlinear_svc.fit(x_train, y_train)\ny_pred = linear_svc.predict(x_dev)\ny_train_pred = linear_svc.predict(x_train)\nacc_linear_svc = round(accuracy_score(y_pred,y_dev) * 100, 2)\nacc_linear_train = round(accuracy_score(y_train_pred, y_train) * 100, 2)\nprint(\"the test error is\", acc_linear_svc, \"the training error is\", acc_linear_train)","91fc3ee1":"## Perceptron\nfrom sklearn.linear_model import Perceptron\nperceptron = Perceptron()\n\nperceptron.fit(x_train, y_train)\ny_pred = perceptron.predict(x_dev)\ny_train_pred = perceptron.predict(x_train)\nacc_perceptron = round(accuracy_score(y_pred,y_dev) * 100, 2)\nacc_perceptron_train = round(accuracy_score(y_train_pred, y_train) * 100, 2)\nprint(\"the test error is\", acc_perceptron, \"the training error is\", acc_perceptron_train)","3176512d":"## Gaussian Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\ngaussian = GaussianNB()\ngaussian.fit(x_train, y_train)\ny_pred = gaussian.predict(x_dev)\ny_train_pred = gaussian.predict(x_train)\nacc_gaussian = round(accuracy_score(y_pred, y_dev) * 100, 2)\nacc_gaussian_train = round(accuracy_score(y_train_pred, y_train) * 100, 2)\nprint(\"the test error is\", acc_gaussian, \"the training error is\", acc_gaussian_train)","cb5244e3":"# Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\ndecisiontree = DecisionTreeClassifier(max_depth=8, min_samples_leaf=10, min_samples_split=15)\ndecisiontree.fit(x_train, y_train)\ny_pred = decisiontree.predict(x_dev)\ny_train_pred = decisiontree.predict(x_train)\nacc_dt = round(accuracy_score(y_pred, y_dev) * 100, 2)\nacc_dt_train = round(accuracy_score(y_train_pred, y_train) * 100, 2)\nprint(\"the test error is\", acc_dt, \"the training error is\", acc_dt_train)","2bbd5688":"# Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\nrandomforest = RandomForestClassifier(max_depth=4, max_features = 5 , min_samples_leaf=2, min_samples_split=5)\nrandomforest.fit(x_train, y_train)\ny_pred = randomforest.predict(x_dev) \ny_train_pred = randomforest.predict(x_train)\nacc_rf = round(accuracy_score(y_pred, y_dev) * 100, 2)\nacc_rf_train = round(accuracy_score(y_train_pred, y_train) * 100, 2)\nprint(\"the test error is\", acc_rf, \"the training error is\", acc_rf_train)","12027f2b":"# Support Vector Machines\nfrom sklearn.svm import SVC\nsvc = SVC(kernel='rbf',gamma=0.001,C=10)\nsvc.fit(x_train, y_train)\ny_pred = svc.predict(x_dev)\ny_train_pred = svc.predict(x_train)\nacc_svc = round(accuracy_score(y_pred, y_dev) * 100, 2)\nacc_svc_train = round(accuracy_score(y_train_pred, y_train) * 100, 2)\n\n# x_dev_copy = x_dev\n# x_dev_copy['Label'] = y_dev.values\n# x_dev_copy['Pred'] = y_pred\n# x_dev_copy[x_dev_copy['Label']!=x_dev_copy['Pred']].head(100)\n# # for i in range(0,len(y_dev)):\n# # #     print(y_pred[i], y_dev.values[i])\n# # #     print(i)\n# #     if y_pred[i] != y_dev.values[i]:\n# # #         print(y_dev.index[i])\n# # #         print(x_dev.loc[680])\n# #         errors.append(x_dev.loc[y_dev.index[i]])\n# # errors\nprint(\"the test error is\", acc_svc, \"the training error is\", acc_svc_train)\n","a916ad35":"# K-nearest Neighbors\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier()\nknn.fit(x_train, y_train)\ny_pred = knn.predict(x_dev)\ny_train_pred = knn.predict(x_train)\nacc_knn = round(accuracy_score(y_pred, y_dev) * 100, 2)\nacc_knn_train = round(accuracy_score(y_train_pred, y_train) * 100, 2)\nprint(\"the test error is\", acc_knn, \"the training error is\", acc_knn_train)","c0751a3b":"#Gradient Boosting Classifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve\n\n#kfold = StratifiedKFold(n_splits=10)\n\nGBC = GradientBoostingClassifier(max_depth=4,min_samples_leaf=100,max_features=0.2)\n#gbc_param_grid = {'loss':[\"deviance\"],\n #               'n_estimators':[100,200,300],\n  #              'learning_rate':[0.1,0.05,0.01],\n   #             'max_depth':[4,8],\n    #            'min_samples_leaf':[100,150],\n     #           'max_features':[0.3,0.1]\n      #          }\n#gsGBC = GridSearchCV(GBC,param_grid = gbc_param_grid, cv=kfold, scoring =\"accuracy\",n_jobs = 4,verbose =1)\nGBC.fit(x_train, y_train)\ny_pred = GBC.predict(x_dev)\ny_train_pred = GBC.predict(x_train)\nacc_gbc = round(accuracy_score(y_pred,y_dev)*100,2)\nacc_gbc_train = round(accuracy_score(y_train_pred, y_train) * 100, 2)\nprint(\"the test error is\", acc_gbc, \"the training error is\", acc_gbc_train)","f254934a":"from sklearn.ensemble import VotingClassifier\n\nvotingC = VotingClassifier(estimators=[('rf', randomforest), ('lr', logreg), ('sv',svc),('gb',GBC)], voting='hard')","c6a978f1":"votingC.fit(X_train, Y_train)\nY_pred = votingC.predict(X_test)\n\ndata_val1 = pd.read_csv('..\/input\/test.csv')\nsubmission = pd.DataFrame({\n        \"PassengerId\": data_val1[\"PassengerId\"],\n        \"Survived\": Y_pred\n    })\nsubmission.to_csv('submission.csv', index=False)","f35de999":"**7. Prediction and submit results**","8502d8ba":"**6. Model ensembling**\n\nWe chose a voting classifer to combine the predictions coming from the 4 classifers which have the highest scores.","78dd7740":"**5. Model implementation**\n\nWe compare the result of following result:\n\n* Logistic regression\n* linear supporter vector classifier\n* PerceptronNaive Bayes\n* Support vector classifier\n* K-nearest neighbors\n* Decision tree\n* Random forest\n* Gradient boost classifier","518b4bfe":"The figures showing survival rate ploted as functions of Pclass, Embarked, Cabin, FareBin are shown as below.","3573e0ec":"To establish our model, we split the training data into training set and development set with the ratio 75:25","fa97c25f":"The prefix part of the ticket feature is extracted and stored as the ticket feature.","14443832":"According to our common sense, the title should be related to the age, e.g. title Miss should be younger than Mrs. To identify if the title is related to the title, we visualize their relationship.","0e68a29f":"The 2 missing Embarked values are filled with the most frequent value for the Embarked feature. \n\nSince Fare feature is very related to the pclass feature, the 1 missing Fare value is filled in with the mean Fare value for the pclass the passenger belongs to.","bb5a29b0":"**Feature SibSp and Parch**\n\nBoth these features represent the number of relatives on the Titanic, so we create a new feature to represent the size of the family for each passengers.","95c5a808":"According to the above analysis, the age value is related to both the title and the pclass, as we hypothesized initially. It is also interesting to discovered that in all pclass group, survived people are of younger median age than dead ones. So we assign the median of each [title, pclass] group if the age value is missed.","fc06525f":"**2. Import data**\n\nHere, we can see that the data contains features including:\n\n* PassingerID: which is used to index the data and do not have obvious relationship with other features, data is complete\n* Survived: the classification in this project, data is complete for training data\n* Pclass: ticket class, which is largely determined by the social class, data is complete\n* Sex and Age: which can potentially influence the survival rate as women and kids may have higher priority to get help in ethics. Sex feature is complete while the age feature lacks 177 values in the training data and 86 in the test data.\n* Sibsp and Parch: # of siblins\/sponses and parents\/children aboard the Titanic, influence to the survival unknown, data is complete\n* Ticket: relationship unkown, data is complete\n* Fare: fare should be determined by the pclass, cabin, and embarked, data is complete in training data while have 1 missing value in the test data\n* Cabin: may provide insight together with pclass, fare, embarked, however, this features have 687 missing values in the training data, so we will not use it directly in the model\n* Embarked: have 2 missing values in the training data, will be filled in later","048fd2f8":"Here we can learn that if you have a family size of 2-4, you will in geneneral have a higher chance to survive. However, it is noted that as passengers with family size 4 or higher are limisted, there might be bias here. \n","632dea21":"A new feature FareBin is created.\n\nThe first letter of the Cabin is extracted and stored as the Cabin feature.","b8f4afc9":"**3. Data cleaning, feature engineering, data visualization**\n\nIn this section we are going to do some exploratory data analysis with descriptive and graphical methods. The features of interest fall into the following categories:\n\n1. Pclass, Fare, Embarked, Cabin, Ticket: which can be used to fill in missing embarked and fare values\n2. Sex, Name (with Title), Age, SibSp, Parch: which can be used to fill in missing age values\n\n**Category 1: Pclass, Fare, Embarked, Cabin, Ticket**\n\nMissing values: 2 Embarked values, 1 Fare value, 1014 Cabin values\n","2947b5de":"The FareBin feature is converted into numbers.","ac45b456":"**Feature Sex**\n\nAs can be seen below, over 70% of female passengers are survived while only ~20% male are survived. Thank you gentlemen!","eaf28949":"#Titanic Survival Prediction\n**Introduction**\n\nRMS Titanic was a British passenger liner that sank in the North Atlantic Ocean on 15th April, 1912. In this accident, more than 1500 out of the estimated total 2224 passengers and crew died, making it one of the most severe peacetime maritime disasters in modern history. This story was also filmed by director Steven Spielberg in to the movie Titanic, which was considered as one of the best movie in the 20th centery. As one of the major reasons of a such loss of life was the lack of the lifeboats, the survival chance was not determined randomly, but mostly the opportunity to acquire a spot on the lifeboats, which is influenced by several parameters. In this study, we estimate how do parameters such as sex, age, social class influence the survival chance.\n\n**Table of Contents:**\n\n1. Import libraries\n2. Import data\n3. Data cleaning, feature engineeirng, data visualization\n4. Establishing the model\n5. Model Implementation\n   * Logistic regression\n   * linear supporter vector classifier\n   * PerceptronNaive Bayes\n   * Support vector classifier\n   * K-nearest neighbors\n   * Decision tree\n   * Random forest\n   * Gradient boost classifier\n6. Model ensembling\n7. Prediction and submit the data\n\n**1. Import libraries**","9c9269ea":"We noticed that several age features are missing, we assume that one important reason for the value missing is that they did not survived so there is no record. To prove our hypothesis, we establish a new feature to indicate if the age feature is missing and to study their relationship with the survival rate.","26b49cb3":"We standardardize the age and fare features in type 1 model","0e011e34":"**Category 2: Sex, Age, SibSp, Parch**\n\nMissing values: 177 Age values\n\n**Feature: Age**\n\nFirstly, let us have a look at the relationship between the age and the survival rate. It is interesting to discovered that babies of age 0-5 have a higher chance to survive, while at the same time, elderly passengers who are order than 65 have the survival rate as low as only ~10%.","85b9f462":"We noticed that in the name, there is title information. As different titles usually refer to different age ranges, the title information can help us to predict the age features. So we extract this informatino from the name and save it in a new feature named 'title'.","d6545ce2":"**4. Establishing the model**\n\nTo establish our model, we drop features including name, passenger ID  as they can hardly provide new insights (we already took out the title features)\nWe establish two types of model dealing with the age and fare information: in type 1, we keep the numerical age and fare and drop the agegroup and farebin features. in type 2, we instead use the agegroup and farebin to represent the age and fare.\nas we use agegroup instead, sibsp and parch as we used family size instead, and Fare as we think it is determined by the pclass and embarked features. ","39b93276":"We noticed that several cabin features are missing, we assume that one important reason for the value missing is that they did not survived so there is no record. To prove our hypothesis, we establish a new feature to indicate if the cabin feature is missing and to study their relationship with the survival rate."}}