{"cell_type":{"791cc796":"code","9369360b":"code","ad35f3cd":"code","b1ba9513":"code","dbe7bce6":"code","489ab605":"code","59ba98c4":"code","ebbcf25f":"code","ad11b1cb":"code","1bdb9004":"code","3ae48909":"code","a2f2a77d":"code","199d764d":"code","c6a35352":"code","a72af5e4":"code","7a292db1":"code","48fc1039":"code","e005cddb":"code","f9a94bf9":"code","e75b911e":"code","6ae06fcc":"code","3108db30":"code","8164226b":"code","c2f7a079":"code","ff2861bb":"code","1e8d4428":"code","30a7c27e":"code","2beb401b":"code","5d8e823a":"code","5c6f55d4":"code","18938965":"code","cd9bdf48":"code","91c3a614":"code","9ed61b13":"code","c518d3f5":"code","e7518e50":"code","c79f29ed":"code","5c3b7df6":"code","b0167877":"code","ad057aca":"code","9a015526":"code","39abf13f":"code","ab3027de":"code","9f28c6ef":"code","9e9c8903":"markdown","faba47a7":"markdown","2d60bce9":"markdown","4b7c80fe":"markdown","48db620d":"markdown","dfa171ec":"markdown","09e26ff9":"markdown","89ea7a4a":"markdown","bd7a14c7":"markdown","779be826":"markdown","062344a9":"markdown","53cccc33":"markdown","33bbccc7":"markdown","4240c74e":"markdown","261ffaf5":"markdown","aec9a200":"markdown","f9bc0dce":"markdown","3c5074e1":"markdown","20061f26":"markdown","baafd97a":"markdown","d7566200":"markdown","8db1694c":"markdown","a18ff72b":"markdown","126394e7":"markdown"},"source":{"791cc796":"import numpy as np\nimport pandas as pd\nimport os","9369360b":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n    \n#read the data with all default parameters\ntrain_df = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\", index_col = 'PassengerId')   \ntest_df = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\", index_col = 'PassengerId')\n    \ntest_df['Survived'] = -888\ndf = pd.concat((train_df, test_df), axis=0, sort=True) ","ad35f3cd":"# extract rows with Embarked as Null\ndf[df.Embarked.isnull()]","b1ba9513":"# how may people embarked at differnt placess and find out the most common embarkment point\ndf.Embarked.value_counts()\n\n# we can fill the missing values with the S as most people embarked from S.","dbe7bce6":"# we can analyze further and see both of the passenger survived the disaster. So we should find out from which\n#Embarked point most people survived that will be more logical. \npd.crosstab(df[df.Survived!=-888].Survived, df[df.Survived!=-888].Embarked)","489ab605":"#option 2 : explore the fare of each class of each embarkment point \ndf.groupby(['Pclass', 'Embarked']).Fare.median()","59ba98c4":"#see the row where fare is missing. \ndf[df.Fare.isnull()]","ebbcf25f":"df.groupby(['Embarked', 'Pclass']).Fare.median()","ad11b1cb":"#calculate the median fare for Embarked in S and Pclass is 3.\nmedian_fare = df.loc[(df.Embarked == 'S') & (df.Pclass == 3),'Fare'].median()","1bdb9004":"median_fare","3ae48909":"df.Name","a2f2a77d":"# Function to extract the title from the name. There is pattern in the titel. Starts with last name, title. first name middle name. \ndef getTitle(name):\n    first_name_with_title = name.split(',')[1]\n    title = first_name_with_title.split('.')[0]\n    title = title.strip().lower()  # strip out all of the whitespaces and coverting the title to lower case\n    return title","199d764d":"#Binning - qcut() performs quantile based binning. We are splitting the Fare in 4 bins here, Where each bins contains almost equal number of observations.\npd.qcut(df.Fare, 4)","c6a35352":"#Specify the name of each bins\npd.qcut(df.Fare, 4, labels=['very_low', 'low', 'high', 'very_high'])  # discretization","a72af5e4":"# lets see the number of obervations in each bins\npd.qcut(df.Fare, 4, labels=['very_low', 'low', 'high', 'very_high']).value_counts().plot(kind='bar', color='c', rot=0);","7a292db1":"def read_data():\n\n    for dirname, _, filenames in os.walk('\/kaggle\/input'):\n        for filename in filenames:\n            print(os.path.join(dirname, filename))\n    \n    #read the data with all default parameters\n    train_df = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\", index_col = 'PassengerId')   \n    test_df = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\", index_col = 'PassengerId')\n    \n    test_df['Survived'] = -888\n    df = pd.concat((train_df, test_df), axis=0, sort=True) \n    return df","48fc1039":"# use map function to apply the function on each Name value row i\ndf.Name.map(lambda x : getTitle(x))   #alternatively we can use : df.Name.map(getTitle)","e005cddb":"# find out how many unique title we have\ndf.Name.map(lambda x : getTitle(x)).unique()","f9a94bf9":"# We will modify our getTitle function here and we will introduce the dictionary to create custome tile.\n# this is to club some of the title\ndef getTitle(name):\n    title_group = {'mr' : 'Mr',\n                'mrs' : 'Mrs',\n                'miss' : 'Miss',\n                'master' : 'Master',\n                'don' : 'Sir',\n                'rev' : 'Sir', \n                'dr' : 'Officer',\n                'mme' : 'Mrs',\n                'ms' : 'Mrs',\n                'major' : 'Officer',\n                'lady' : 'Lady',\n                'sir' : 'Sir', \n                'mlle' : 'Miss',\n                'col' : 'Officer',\n                'capt' : 'Officer',\n                'the countess' : 'Lady',\n                'jonkheer' : 'Sir', \n                'dona' : 'Lady'\n    }\n    first_name_with_title = name.split(',')[1]\n    title = first_name_with_title.split('.')[0]\n    title = title.strip().lower()\n    return title_group[title]","e75b911e":"# Create title feature\ndf['Title'] = df.Name.map(lambda x : getTitle(x))","6ae06fcc":"df.head()","3108db30":"#Box plot of Age with title\ndf[df.Age.notnull()].boxplot('Age', 'Title');","8164226b":"def fill_missing_values(df):\n    # Embarked\n    df.Embarked.fillna('C', inplace=True)\n    # Fare\n    median_fare = df.loc[(df.Embarked == 'S') & (df.Pclass == 3),'Fare'].median()\n    df.Fare.fillna(median_fare, inplace=True)        \n    # age\n    title_age_median = df.groupby('Title').Age.transform('median')\n    df.Age.fillna(title_age_median, inplace=True)   \n    return df","c2f7a079":"def get_deck(cabin):\n    return np.where(pd.notnull(cabin), str(cabin)[0].upper(),'z')      ","ff2861bb":"def reorder_columns(df):\n    columns = [column for column in df.columns if column != 'Survived']\n    columns = ['Survived'] + columns\n    df = df[columns]\n    return df","1e8d4428":"def process_data(df):\n    #using the method chaining concept\n    return (df\n         # create title attribute - then add this\n         .assign(Title = lambda x: x.Name.map(get_title))\n         # working missing values - start with this\n         .pipe(fill_missing_values)   \n         # Create Fare_Bin Feature\n         .assign(Fare_Bin = lambda x: pd.qcut(x.Fare, 4, labels=['very_low', 'low', 'high', 'very_high']))\n         # Create ageState\n         .assign(AgeState = lambda x : np.where(x.Age >= 18, 'Adult', 'Child'))\n         # Creat FamilySize\n         .assign(FamilySize = lambda x : x.Parch + x.SibSp + 1)\n         # Create IsMother   \n         .assign(IsMother = lambda x : np.where(((x.Sex == 'female') & (x.Parch > 0) & (x.Age > 18) & (x.Title != 'Miss')), 1, 0))  \n         # Create Deck feature\n         .assign(Cabin = lambda x: np.where(x.Cabin == 'T', np.nan, x.Cabin))\n         .assign(Deck = lambda x : x.Cabin.map(get_deck))   \n         # Feature Encoding\n         .assign(IsMale = lambda x : np.where(x.Sex == 'male', 1,0))\n         .pipe(pd.get_dummies, columns=['Deck', 'Pclass', 'Title', 'Fare_Bin', 'Embarked', 'AgeState'])\n         # Add code to drop unnecessarey columns\n         .drop(['Cabin', 'Name', 'Ticket', 'Parch', 'SibSp', 'Sex'], axis = 1)   \n         # Reorder columns\n         .pipe(reorder_columns)\n           )","30a7c27e":"    df = read_data()","2beb401b":"    df = process_data(df)","5d8e823a":"#train data\ntrain_df = df.loc[df.Survived != -888]","5c6f55d4":"#test data\ncolumns = [column for column in df.columns if column != 'Survived']\ntest_df = df.loc[df.Survived == -888, columns]","18938965":"train_df.head()","cd9bdf48":"test_df.head()","91c3a614":"import pandas as pd\nimport numpy as np\nimport os","9ed61b13":"X = train_df.loc[:,'Age':].as_matrix().astype('float')\ny = train_df['Survived'].ravel()","c518d3f5":"print(X.shape, y.shape)","e7518e50":"# train test split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\nprint(X_train.shape, y_train.shape)\nprint(X_test.shape, y_test.shape)","c79f29ed":"# Average survival in train and test \nprint(f\"Mean Survival in train : {np.mean(y_train)}\")\nprint(f\"Mean Survival in test : {np.mean(y_test)}\")  ","5c3b7df6":"from sklearn.dummy import DummyClassifier\n# performance matrics\nfrom sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score","b0167877":"# create Model\nmodel_dummy = DummyClassifier(strategy='most_frequent', random_state=0)\n\n# Most frequest will output the majority class. ","ad057aca":"# train model\nmodel_dummy.fit(X_train, y_train)","9a015526":"print(f\"Score for baseline Model: {model_dummy.score(X_test, y_test)}\")\nprint(f\"Accurancy for baseline model: {accuracy_score(y_test, model_dummy.predict(X_test))}\")\nprint(f\"Confusion Matrix for baseline model: {confusion_matrix(y_test, model_dummy.predict(X_test))}\")\nprint(f\"Precision for baseline model: {precision_score(y_test, model_dummy.predict(X_test))}\")\nprint(f\"Recall for baseline model: {recall_score(y_test, model_dummy.predict(X_test))}\")","39abf13f":"#import function\nfrom sklearn.linear_model import LogisticRegression","ab3027de":"# Create model\nmodel_lr_1 = LogisticRegression(random_state=0)\n# train model\nmodel_lr_1.fit(X_train, y_train)","9f28c6ef":"print(f\"Score for LogisticRegression Model: {model_lr_1.score(X_test, y_test)}\")\nprint(f\"Accurancy for LogisticRegression model: {accuracy_score(y_test, model_lr_1.predict(X_test))}\")\nprint(f\"Confusion Matrix for LogisticRegression model: {confusion_matrix(y_test, model_lr_1.predict(X_test))}\")\nprint(f\"Precision for LogisticRegression model: {precision_score(y_test, model_lr_1.predict(X_test))}\")\nprint(f\"Recall for LogisticRegression model: {recall_score(y_test, model_lr_1.predict(X_test))}\")","9e9c8903":"## Binning Fare_Bin","faba47a7":"## Logistic Regression ","2d60bce9":"### Understanding titanic dataset\n\n|#|Feature        |Meaning          | \n|:---:|:---:|:---:|\n|1|Passenger ID  | Unique passenger id |\n|2| Survived     | If Survived(1-yes, 0-no) |\n|3| Name         | Name of the passenger |\n|4| Sex          | Gender |\n|5| Age          | Age of passenger |\n|6| SibSp        | Number of siblings \/ spouses aboard |\n|7| Parch        | Number of parents \/ childer aobard |\n|8| Ticket       | Ticket Number |\n|9| Fare         | Passenger fare |\n|10| Cabin       | Cabin number |\n|11| Embarked    | Point of embarkment (C=Cherbourg; Q=Queenstown; S= Southampton) |","4b7c80fe":"## Feature: Fare missing value issues","48db620d":"# Preprocessed data\n  - Training data Set\n  - Test Data Set\n  \n  You can apply machine learning algorithm now. ","dfa171ec":"## Feature: Age Missing value\nReplace with median age of title\nExtract the title from the name column and create a separate column title. Then check median age of each title. Title of a name mostly gives us the age information. if its master then it is kid and its sir or dr means its old person","09e26ff9":"***Clearly we have converted a numerical feature Fare to categorical feature, wher each bin is one category. Such techniques are called Discritization.***","89ea7a4a":"## Read Data Function \n  - This function reads the data from the file.\n  - create train_df and test_df with index column as PassengerID\n  - we also create a column Survived in the test_df as its not given in the original data so that we can add that to the   train_df to organize all the data at once. This way we do not have to perform same function twice on different data set. \n  - merge both data and return the dataframe as df. \n  ","bd7a14c7":"## Process Data\nThis function process all the data. fill null values, create new features drop columns and on hot encoding for catgorical feature. all included in this function. \n  - follwoing new features has been created. \n  -  Fare_Bin =  Devided the fare into 4 categories 'very_low', 'low', 'high', 'very_high'. \n  -  Age_State = 'Adult', 'Child' - As child has the more chances of survival than adults. They get priority in the life  boats.\n  -  FamilySize - Small family has better chance of survival than big families.\n  -  IsMother - Mother has better chances of survival as they get preference in boat. \n  -  Deck column - Which place passenger is situated in the deck can also reveal how far they are from the lifeboat. ","779be826":"# Data Orgnizantion (Data Preprocessing Script)","062344a9":"***We can see that each title has a different median value and they also have different Age range. It makes most sense to replace the Age median to the median of each title.***","53cccc33":"***We can see both the missing passenger Fare is 80 and both are 1st class passenger. We can explore that from which Embarked value people have paid near 80 fare in first class.***","33bbccc7":"## Get Title function. \n  - Fetched the title from the name column and created a categorical feature column title. \n  - This is required to fill the null age value. \n  - Median age value for every title can be different. Like master title is for kids so the median age for kids can be smaller than the median age of old person. \n  -  Also clubbed the title to make it compact. Mr, Mrs, Miss, Master, Sir, Officer, Lady titles used. ","4240c74e":"- First observation: Survival rate in both train and test data are almost same that means training and test data. We want positive cases to be distributed evenly in both trainig and test data.\n- Second obeservation: Only 39 % of data are positive cases and 61% are the negative classes. In this scenario 39 vs 61 is still good scenario however in some case like marketing - Attract, Engage and Convert ratio can be very small. 2-3% cutomer can be attracted. This highly imbalance problem has different approach to build the model.","261ffaf5":"# Machine Learning \n  - Performance Matrices. \n      - Performance Matrics, Which Classifier is better?\n          - Accuracy\n          - Precision\n          - Recall\n\n   - Accuracy - Compare predicted output with actual output.\n        -  Accuracy = Correct \/ Total Count\n   - Precision & Recall:\n     \n    **Confution Metrix**\n \n| | Predicted Negtive  |  Predicted Possitive |\n|:---:|:---:|:---:|\n|Actual Negative | True Negative (TN) | False Positive (FP)  |\n|Actual Possitve | False Negative (FN)| True Positive (TP)   |\n        \n   - Precision: What fraction of positive predictions are correct? TP \/ Total Positive Prediction = TP \/ TP + FP\n   - Recall : What fraction of positive cases you predicted correctly? TP \/ Total Positive Cases = TP \/ TP + FN","aec9a200":"### Fare will be updated with median fare. ","f9bc0dce":"***We can see here that class 1 passenger who embarked from the C has paid 76.7292 Fare which is very close to 80. There may be possibility that those two passenger who does not have embarked data have boarded from C.***","3c5074e1":"## Reorder function\n    - Put the column survived at the beginnig. \n    - Its good practice to include the target column either at the begining or at the end. ","20061f26":"## Missing Value Embarked Handeling. ","baafd97a":"## This Kernal is to show the initial data Analysis and build the baseline model and machine learning model using Titanic Data Set. ","d7566200":"## Baseline Model\n\n- First Step - Create baseline Model which does not use the machine leanring at all - It is a best practice to do that. It will help to compare our machine learning model. If Machine leanring model gives the better result then it makes sense to build machine learning model other wise it does not make sense at all. \n\n#### Baseline Model for classification \n\n- Always give the output as majority class\n\n|Class | Count |\n|:---:|:---:|\n|1|60|\n|0|40|\n\nIf training data set has the majority class 60 percent as 1. Then our model will always return class 1 as a result.\nBaseline Model Accuracy = 60 \/ (60+40) = 0.6  = 60%\n\n***Predictive model should have better performance than baseline otherwise it does not make sense to build the machine learning model.***","8db1694c":"### Missing value \n -  Function missing value to fill all missing values. \n -  Embarked missing value is filled with 'C'\n -  Age missing value is filled with title midean. ","a18ff72b":"## We can apply other machine algorithms as above. ","126394e7":"## Get Deck function\n   - Create a new feature deck from cabin. fill all the missing value with value z. "}}