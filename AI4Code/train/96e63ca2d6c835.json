{"cell_type":{"d2f540f8":"code","6eb908f6":"code","cc7117ab":"code","e62125c7":"code","44fbf780":"code","565fa483":"code","209fb7f3":"code","ad2f6184":"code","e414be62":"code","46f75431":"code","9648fb03":"code","131d502b":"code","afbad14a":"code","ef02e39c":"code","f292d4be":"code","7729d9a3":"code","2452b1b0":"code","3d6d2b67":"code","09288238":"code","f1506a23":"code","2a5ca0b3":"code","48c7145d":"code","a45fc00a":"code","1115c382":"code","d83e87c2":"code","6ce77b34":"code","8b3124af":"code","b4e56456":"code","75dc29af":"code","d7cd640a":"code","0135a08d":"code","3c5441c5":"code","91c045eb":"code","5cf63e1d":"code","47da5a33":"code","020051b1":"code","d96509c4":"code","9e4e256b":"code","04e38a8d":"code","c8a1f05e":"code","39038c68":"code","638aa398":"code","4e9886dc":"code","101e10b7":"code","d98e9d08":"code","65c0f60e":"code","72c545e3":"code","a15b275c":"code","9f192655":"code","85452ac3":"code","ecba8f7a":"code","f4f776e7":"code","df902666":"code","f5e0ee62":"code","30bc8aa3":"code","ea3f4a84":"code","a6a32f7d":"code","f5cad843":"code","78393c12":"code","ff307d5c":"code","dbcd4801":"code","10c36ab0":"code","9b8ef203":"code","9edd5c9a":"code","4d4886a1":"code","82abc062":"code","fa292108":"code","c7b326b4":"code","92cb1416":"code","a4339f81":"code","e423f32c":"code","9b9f5d00":"code","2e763fe1":"code","e4efd57e":"code","482c074e":"code","27b387ef":"code","224dca3d":"code","9186b848":"code","da9c7854":"code","d1ff7986":"code","691499cd":"code","6e025ed5":"code","36d4f9ff":"code","f159f1d2":"code","b75869d0":"code","c1bac314":"code","4da91b02":"code","8967533f":"code","2d589700":"code","7b6bcfc5":"code","f4eb6239":"code","7f158972":"code","fbb6ca2f":"code","1f3f71c2":"code","045915bd":"code","2b9f7d51":"code","a03ee49d":"code","3683ab68":"code","697e68c0":"code","7224282d":"code","674df640":"code","44e53938":"code","980d7875":"code","cc21b4b7":"code","3456516d":"code","7d9216fd":"code","f126216b":"code","b6da2f04":"code","8156393a":"code","c7ffb1f8":"code","2ef448d8":"code","bd920d7d":"code","4839993f":"code","37dd5b42":"markdown","4b1f34ca":"markdown","13594c00":"markdown","515dbd07":"markdown","fdb97a9a":"markdown","ef593054":"markdown","bfacd115":"markdown","2ba9de8e":"markdown","30bc6981":"markdown","ed7259bc":"markdown","f8d58c12":"markdown","53169915":"markdown","fc3be7a0":"markdown","bae9aafc":"markdown","9b78177f":"markdown","602a620d":"markdown","dcf06e23":"markdown","dcb2bcb7":"markdown","e5e8c7ac":"markdown","38b38332":"markdown","0281da80":"markdown","8271f17b":"markdown","3f6fa6fb":"markdown","e4b167b8":"markdown","99a2910e":"markdown","40d13f99":"markdown","72ef79ba":"markdown","74690c59":"markdown","5a654909":"markdown"},"source":{"d2f540f8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# for data visulization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# for modeling estimators\nfrom sklearn.ensemble import RandomForestClassifier as rf\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier as gbm\nfrom xgboost.sklearn import XGBClassifier\nimport lightgbm as lgb\n\n#for data processing\nfrom sklearn.model_selection import train_test_split\n\n#for tuning parameters\nfrom bayes_opt import BayesianOptimization\nfrom skopt import BayesSearchCV\nfrom eli5.sklearn import PermutationImportance\n\n\n# Misc.\nimport os\nimport time\nimport gc\n\n","6eb908f6":"# Read in data\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\n\nids=test['Id']","cc7117ab":"train.head()","e62125c7":"train.shape, test.shape","44fbf780":"train.info() ","565fa483":"train.plot(figsize = (12,10))","209fb7f3":"sns.countplot(\"Target\", data=train)","ad2f6184":" sns.countplot(x=\"r4t3\",hue=\"Target\",data=train)","e414be62":"sns.countplot(x=\"hhsize\",hue=\"Target\",data=train)","46f75431":"from pandas.plotting import scatter_matrix\nscatter_matrix(train.select_dtypes('float'), alpha=0.2, figsize=(26, 20), diagonal='kde')","9648fb03":"from collections import OrderedDict\n\nplt.figure(figsize = (20, 16))\nplt.style.use('fivethirtyeight')\n\n# Color mapping\ncolors = OrderedDict({1: 'red', 2: 'orange', 3: 'blue', 4: 'green'})\npoverty_mapping = OrderedDict({1: 'extreme', 2: 'moderate', 3: 'vulnerable', 4: 'non vulnerable'})\n\n# Iterate through the float columns\nfor i, col in enumerate(train.select_dtypes('float')):\n    ax = plt.subplot(4, 2, i + 1)\n    # Iterate through the poverty levels\n    for poverty_level, color in colors.items():\n        # Plot each poverty level as a separate line\n        sns.kdeplot(train.loc[train['Target'] == poverty_level, col].dropna(), \n                    ax = ax, color = color, label = poverty_mapping[poverty_level])\n        \n    plt.title(f'{col.capitalize()} Distribution'); plt.xlabel(f'{col}'); plt.ylabel('Density')\n\nplt.subplots_adjust(top = 2)","131d502b":"train.select_dtypes('object').head()","afbad14a":"yes_no_map = {'no':0,'yes':1}\ntrain['dependency'] = train['dependency'].replace(yes_no_map).astype(np.float32)\ntrain['edjefe'] = train['edjefe'].replace(yes_no_map).astype(np.float32)\ntrain['edjefa'] = train['edjefa'].replace(yes_no_map).astype(np.float32)","ef02e39c":"yes_no_map = {'no':0,'yes':1}\ntest['dependency'] = test['dependency'].replace(yes_no_map).astype(np.float32)\ntest['edjefe'] = test['edjefe'].replace(yes_no_map).astype(np.float32)\ntest['edjefa'] = test['edjefa'].replace(yes_no_map).astype(np.float32)","f292d4be":"train[[\"dependency\",\"edjefe\",\"edjefa\"]].describe()","7729d9a3":"train[[\"dependency\",\"edjefe\",\"edjefa\"]].hist()","2452b1b0":"plt.figure(figsize = (16, 12))\n\n# Iterate through the float columns\nfor i, col in enumerate(['dependency', 'edjefa', 'edjefe']):\n    ax = plt.subplot(3, 1, i + 1)\n    # Iterate through the poverty levels\n    for poverty_level, color in colors.items():\n        # Plot each poverty level as a separate line\n        sns.kdeplot(train.loc[train['Target'] == poverty_level, col].dropna(), \n                    ax = ax, color = color, label = poverty_mapping[poverty_level])\n        \n    plt.title(f'{col.capitalize()} Distribution'); plt.xlabel(f'{col}'); plt.ylabel('Density')\n\nplt.subplots_adjust(top = 2)","3d6d2b67":"# Number of missing in each column\nmissing = pd.DataFrame(train.isnull().sum()).rename(columns = {0: 'total'})\n\n# Create a percentage missing\nmissing['percent'] = missing['total'] \/ len(train)\n\nmissing.sort_values('percent', ascending = False).head(10)","09288238":"train['v18q1'] = train['v18q1'].fillna(0)\ntest['v18q1'] = test['v18q1'].fillna(0)\ntrain['v2a1'] = train['v2a1'].fillna(0)\ntest['v2a1'] = test['v2a1'].fillna(0)\n\ntrain['rez_esc'] = train['rez_esc'].fillna(0)\ntest['rez_esc'] = test['rez_esc'].fillna(0)\ntrain['SQBmeaned'] = train['SQBmeaned'].fillna(0)\ntest['SQBmeaned'] = test['SQBmeaned'].fillna(0)\ntrain['meaneduc'] = train['meaneduc'].fillna(0)\ntest['meaneduc'] = test['meaneduc'].fillna(0)","f1506a23":"#Checking for missing values again to confirm that no missing values present\n# Number of missing in each column\nmissing = pd.DataFrame(train.isnull().sum()).rename(columns = {0: 'total'})\n\n# Create a percentage missing\nmissing['percent'] = missing['total'] \/ len(train)\n\nmissing.sort_values('percent', ascending = False).head(10)","2a5ca0b3":"#Checking for missing values again to confirm that no missing values present\n# Number of missing in each column\nmissing = pd.DataFrame(test.isnull().sum()).rename(columns = {0: 'total'})\n\n# Create a percentage missing\nmissing['percent'] = missing['total'] \/ len(train)\n\nmissing.sort_values('percent', ascending = False).head(10)","48c7145d":"train.drop(['Id','idhogar'], inplace = True, axis =1)\n\ntest.drop(['Id','idhogar'], inplace = True, axis =1)\n","a45fc00a":"train.shape, test.shape\n","1115c382":"y = train.iloc[:,140]\ny.unique()","d83e87c2":"X = train.iloc[:,1:141]\nX.shape","6ce77b34":"X_train, X_test, y_train, y_test = train_test_split(\n                                                    X,\n                                                    y,\n                                                    test_size = 0.2)","8b3124af":"modelgbm=gbm()","b4e56456":"start = time.time()\nmodelgbm = modelgbm.fit(X_train, y_train)\nend = time.time()\n(end-start)\/60","75dc29af":"classes = modelgbm.predict(X_test)\n\nclasses","d7cd640a":"(classes == y_test).sum()\/y_test.size ","0135a08d":"bayes_cv_tuner = BayesSearchCV(\n    #  Place your estimator here with those parameter values\n    #      that you DO NOT WANT TO TUNE\n    gbm(\n               # No need to tune this parameter value\n      ),\n\n    # 2.12 Specify estimator parameters that you would like to change\/tune\n    {\n        'n_estimators': (100, 500),           # Specify integer-values parameters like this\n        \n        'max_depth': (4, 100),                # integer valued parameter\n        'max_features' : (10,64),             # integer-valued parameter\n        'min_weight_fraction_leaf' : (0,0.5, 'uniform')   # Float-valued parameter\n    },\n\n    # 2.13\n    n_iter=32,            # How many points to sample\n    cv = 2                # Number of cross-validation folds\n)","3c5441c5":"# Start optimization\nbayes_cv_tuner.fit(X_train, y_train)","91c045eb":"#  Get list of best-parameters\nbayes_cv_tuner.best_params_","5cf63e1d":"modelgbmTuned=gbm(\n               max_depth=31,\n               max_features=29,\n               min_weight_fraction_leaf=0.02067,\n               n_estimators=489)","47da5a33":"start = time.time()\nmodelgbmTuned = modelgbmTuned.fit(X_train, y_train)\nend = time.time()\n(end-start)\/60","020051b1":"ygbm=modelgbmTuned.predict(X_test)\nygbmtest=modelgbmTuned.predict(test)","d96509c4":"#  Get what average accuracy was acheived during cross-validation\nbayes_cv_tuner.best_score_","9e4e256b":"#  What accuracy is available on test-data\nbayes_cv_tuner.score(X_test, y_test)","04e38a8d":"#  And what all sets of parameters were tried?\nbayes_cv_tuner.cv_results_['params']","c8a1f05e":"modelrf = rf()","39038c68":"start = time.time()\nmodelrf = modelrf.fit(X_train, y_train)\nend = time.time()\n(end-start)\/60","638aa398":"classes = modelrf.predict(X_test)","4e9886dc":"(classes == y_test).sum()\/y_test.size ","101e10b7":"bayes_cv_tuner = BayesSearchCV(\n    #  Place your estimator here with those parameter values\n    #      that you DO NOT WANT TO TUNE\n    rf(\n       n_jobs = 2         # No need to tune this parameter value\n      ),\n\n    # 2.12 Specify estimator parameters that you would like to change\/tune\n    {\n        'n_estimators': (100, 500),           # Specify integer-values parameters like this\n        'criterion': ['gini', 'entropy'],     # Specify categorical parameters as here\n        'max_depth': (4, 100),                # integer valued parameter\n        'max_features' : (10,64),             # integer-valued parameter\n        'min_weight_fraction_leaf' : (0,0.5, 'uniform')   # Float-valued parameter\n    },\n\n    # 2.13\n    n_iter=32,            # How many points to sample\n    cv = 3                # Number of cross-validation folds\n)","d98e9d08":"# Start optimization\nbayes_cv_tuner.fit(X_train, y_train)","65c0f60e":"#  Get list of best-parameters\nbayes_cv_tuner.best_params_","72c545e3":"modelrfTuned=rf(criterion=\"gini\",\n               max_depth=88,\n               max_features=41,\n               min_weight_fraction_leaf=0.1,\n               n_estimators=285)","a15b275c":"start = time.time()\nmodelrfTuned = modelrfTuned.fit(X_train, y_train)\nend = time.time()\n(end-start)\/60","9f192655":"yrf=modelrfTuned.predict(X_test)\nyrf","85452ac3":"#  Get what average accuracy was acheived during cross-validation\nbayes_cv_tuner.best_score_","ecba8f7a":"#  What accuracy is available on test-data\nbayes_cv_tuner.score(X_test, y_test)","f4f776e7":"#  And what all sets of parameters were tried?\nbayes_cv_tuner.cv_results_['params']","df902666":"modelneigh = KNeighborsClassifier(n_neighbors=7)","f5e0ee62":"start = time.time()\nmodelneigh = modelneigh.fit(X_train, y_train)\nend = time.time()\n(end-start)\/60","30bc8aa3":"classes = modelneigh.predict(X_test)\n\nclasses","ea3f4a84":"(classes == y_test).sum()\/y_test.size ","a6a32f7d":"bayes_cv_tuner = BayesSearchCV(\n    #  Place your estimator here with those parameter values\n    #      that you DO NOT WANT TO TUNE\n    KNeighborsClassifier(\n       n_neighbors=7         # No need to tune this parameter value\n      ),\n    {\"metric\": [\"euclidean\", \"cityblock\"]},\n    n_iter=32,            # How many points to sample\n    cv = 2            # Number of cross-validation folds\n   )","f5cad843":"# Start optimization\nbayes_cv_tuner.fit(X_train, y_train)","78393c12":"#  Get list of best-parameters\nbayes_cv_tuner.best_params_","ff307d5c":"modelneighTuned = KNeighborsClassifier(n_neighbors=7,\n               metric=\"cityblock\")","dbcd4801":"start = time.time()\nmodelneighTuned = modelneighTuned.fit(X_train, y_train)\nend = time.time()\n(end-start)\/60","10c36ab0":"yneigh=modelneighTuned.predict(X_test)","9b8ef203":"yneightest=modelneighTuned.predict(test)","9edd5c9a":"#  Get what average accuracy was acheived during cross-validation\nbayes_cv_tuner.best_score_","4d4886a1":"#  What accuracy is available on test-data\nbayes_cv_tuner.score(X_test, y_test)","82abc062":"#  And what all sets of parameters were tried?\nbayes_cv_tuner.cv_results_['params']","fa292108":"modeletf = ExtraTreesClassifier()","c7b326b4":"start = time.time()\nmodeletf = modeletf.fit(X_train, y_train)\nend = time.time()\n(end-start)\/60","92cb1416":"classes = modeletf.predict(X_test)\n\nclasses","a4339f81":"(classes == y_test).sum()\/y_test.size","e423f32c":"bayes_cv_tuner = BayesSearchCV(\n    #  Place your estimator here with those parameter values\n    #      that you DO NOT WANT TO TUNE\n    ExtraTreesClassifier( ),\n\n    # 2.12 Specify estimator parameters that you would like to change\/tune\n    {   'n_estimators': (100, 500),           # Specify integer-values parameters like this\n        'criterion': ['gini', 'entropy'],     # Specify categorical parameters as here\n        'max_depth': (4, 100),                # integer valued parameter\n        'max_features' : (10,64),             # integer-valued parameter\n        'min_weight_fraction_leaf' : (0,0.5, 'uniform')   # Float-valued parameter\n    },\n\n    n_iter=32,            # How many points to sample\n    cv = 2            # Number of cross-validation folds\n)","9b9f5d00":"# Start optimization\nbayes_cv_tuner.fit(X_train, y_train)","2e763fe1":"#  Get list of best-parameters\nbayes_cv_tuner.best_params_","e4efd57e":"modeletfTuned=ExtraTreesClassifier(criterion=\"entropy\",\n               max_depth=100,\n               max_features=64,\n               min_weight_fraction_leaf=0.0,\n               n_estimators=100)","482c074e":"start = time.time()\nmodeletfTuned = modeletfTuned.fit(X_train, y_train)\nend = time.time()\n(end-start)\/60","27b387ef":"yetf=modeletfTuned.predict(X_test)\nyetftest=modeletfTuned.predict(test)","224dca3d":"#  Get what average accuracy was acheived during cross-validation\nbayes_cv_tuner.best_score_","9186b848":"#  What accuracy is available on test-data\nbayes_cv_tuner.score(X_test, y_test)","da9c7854":"#  And what all sets of parameters were tried?\nbayes_cv_tuner.cv_results_['params']","d1ff7986":"modelxgb=XGBClassifier()","691499cd":"start = time.time()\nmodelxgb = modelxgb.fit(X_train, y_train)\nend = time.time()\n(end-start)\/60","6e025ed5":"classes = modelxgb.predict(X_test)\n\nclasses","36d4f9ff":"(classes == y_test).sum()\/y_test.size ","f159f1d2":"bayes_cv_tuner = BayesSearchCV(\n    #  Place your estimator here with those parameter values\n    #      that you DO NOT WANT TO TUNE\n    XGBClassifier(\n       n_jobs = 2         # No need to tune this parameter value\n      ),\n\n    # 2.12 Specify estimator parameters that you would like to change\/tune\n    {\n        'n_estimators': (100, 300),           # Specify integer-values parameters like this\n        'criterion': ['gini', 'entropy'],     # Specify categorical parameters as here\n        'max_depth': (4, 100),                # integer valued parameter\n        'max_features' : (10,64),             # integer-valued parameter\n        'min_weight_fraction_leaf' : (0,0.5, 'uniform')   # Float-valued parameter\n    },\n\n    # 2.13\n    n_iter=32,            # How many points to sample\n    cv = 3                # Number of cross-validation folds\n)","b75869d0":"# Start optimization\nbayes_cv_tuner.fit(X_train, y_train)","c1bac314":"#  Get list of best-parameters\nbayes_cv_tuner.best_params_","4da91b02":"modelxgbTuned=XGBClassifier(criterion=\"gini\",\n               max_depth=85,\n               max_features=47,\n               min_weight_fraction_leaf=0.035997,\n               n_estimators=178)","8967533f":"start = time.time()\nmodelxgbTuned = modelxgbTuned.fit(X_train, y_train)\nend = time.time()\n(end-start)\/60","2d589700":"#yxgb=modelxgbTuned.predict(X_test)\n#yxgbtest=modelxgbTuned.predict(test)","7b6bcfc5":"#  Get what average accuracy was acheived during cross-validation\nbayes_cv_tuner.best_score_","f4eb6239":"#  What accuracy is available on test-data\nbayes_cv_tuner.score(X_test, y_test)","7f158972":"#  And what all sets of parameters were tried?\nbayes_cv_tuner.cv_results_['params']","fbb6ca2f":"modellgb = lgb.LGBMClassifier(max_depth=-1, learning_rate=0.1, objective='multiclass',\n                             random_state=None, silent=True, metric='None', \n                             n_jobs=4, n_estimators=5000, class_weight='balanced',\n                             colsample_bytree =  0.93, min_child_samples = 95, num_leaves = 14, subsample = 0.96)","1f3f71c2":"start = time.time()\nmodellgb = modellgb.fit(X_train, y_train)\nend = time.time()\n(end-start)\/60","045915bd":"classes = modellgb.predict(X_test)\n\nclasses","2b9f7d51":"(classes == y_test).sum()\/y_test.size ","a03ee49d":"bayes_cv_tuner = BayesSearchCV(\n    #  Place your estimator here with those parameter values\n    #      that you DO NOT WANT TO TUNE\n    lgb.LGBMClassifier(\n       n_jobs = 2         # No need to tune this parameter value\n      ),\n\n    # 2.12 Specify estimator parameters that you would like to change\/tune\n    {\n        'n_estimators': (100, 500),           # Specify integer-values parameters like this\n        'criterion': ['gini', 'entropy'],     # Specify categorical parameters as here\n        'max_depth': (4, 100),                # integer valued parameter\n        'max_features' : (10,64),             # integer-valued parameter\n        'min_weight_fraction_leaf' : (0,0.5, 'uniform')   # Float-valued parameter\n    },\n\n    # 2.13\n    n_iter=32,            # How many points to sample\n    cv = 3                # Number of cross-validation folds\n)","3683ab68":"# Start optimization\nbayes_cv_tuner.fit(X_train, y_train)","697e68c0":"#  Get list of best-parameters\nbayes_cv_tuner.best_params_","7224282d":"modellgbTuned = lgb.LGBMClassifier(criterion=\"entropy\",\n               max_depth=35,\n               max_features=14,\n               min_weight_fraction_leaf=0.18611,\n               n_estimators=148)","674df640":"start = time.time()\nmodellgbTuned = modellgbTuned.fit(X_train, y_train)\nend = time.time()\n(end-start)\/60","44e53938":"ylgb=modellgbTuned.predict(X_test)\nylgbtest=modellgbTuned.predict(test)","980d7875":"#  Get what average accuracy was acheived during cross-validation\nbayes_cv_tuner.best_score_","cc21b4b7":"#  What accuracy is available on test-data\nbayes_cv_tuner.score(X_test, y_test)","3456516d":"#  And what all sets of parameters were tried?\nbayes_cv_tuner.cv_results_['params']","7d9216fd":"NewTrain = pd.DataFrame()\n#NewTrain['yrf'] = yrf.tolist()\nNewTrain['yetf'] = yetf.tolist()\nNewTrain['yneigh'] = yneigh.tolist()\nNewTrain['ygbm'] = ygbm.tolist()\n#NewTrain['yxgb'] = yxgb.tolist()\nNewTrain['ylgb'] = ylgb.tolist()\n\nNewTrain.head(5), NewTrain.shape","f126216b":"NewTest = pd.DataFrame()\n#NewTest['yrf'] = yrftest.tolist()\nNewTest['yetf'] = yetftest.tolist()\nNewTest['yneigh'] = yneightest.tolist()\nNewTest['ygbm'] = ygbmtest.tolist()\n#NewTest['yxgb'] = yxgbtest.tolist()\nNewTest['ylgb'] = ylgbtest.tolist()\nNewTest.head(5), NewTest.shape","b6da2f04":"NewModel=rf(criterion=\"entropy\",\n               max_depth=87,\n               max_features=4,\n               min_weight_fraction_leaf=0.0,\n               n_estimators=600)","8156393a":"start = time.time()\nNewModel = NewModel.fit(NewTrain, y_test)\nend = time.time()\n(end-start)\/60","c7ffb1f8":"ypredict=NewModel.predict(NewTest)\nypredict","2ef448d8":"\n#submit=pd.DataFrame({'Id': ids, 'Target': ylgbtest})\nsubmit=pd.DataFrame({'Id': ids, 'Target': ypredict})\nsubmit.head(5)","bd920d7d":"submit.to_csv('submit.csv', index=False)","4839993f":"sub = pd.read_csv('..\/input\/sample_submission.csv')\nsub['target'] = ypredict\nsub.drop(sub.columns[[1]], axis=1, inplace=True)\nsub.to_csv('submission.csv',index=False)","37dd5b42":".... Contine Tuning, Analysing ... ","4b1f34ca":"**Splitting the data into train & test**","13594c00":"**Converting categorical objects into numericals**","515dbd07":"**BUILDING a new dataset with predicted results with all these models**","fdb97a9a":"**Fill in missing values (NULL values) using 1 for yes and 0 for no**","ef593054":"**Modelling with XGBoosterClassifier**","bfacd115":"**Feature Distributions**\nThe first and easy property to review is the distribution of each attribute.\n\n","2ba9de8e":"**Feature-Feature Relationships**\n\nThe final important relationship to explore is that of the relationships between the attributes.\n\nWe can review the relationships between attributes by looking at the distribution of the interactions of each pair of attributes.\n\nThis uses a built function to create a matrix of scatter plots of all attributes versus all attributes. The diagonal where each attribute would be plotted against itself shows the Kernel Density Estimation of the attribute instead.","30bc6981":"**Performing tuning using Bayesian Optimization.**","ed7259bc":"**Dropping unnecesary columns**","f8d58c12":"**Modelling with GradientBoostingClassifier**","53169915":"\n\n**Modeling with Random Forest**","fc3be7a0":"**Performing tuning using Bayesian Optimization.**","bae9aafc":"**Object Columns**\nThe last column type is object which we can view as follows.","9b78177f":"**Dividing the data into predictors & target**","602a620d":"The below are Distribution plots using seaborn","dcf06e23":"**Explore data**","dcb2bcb7":"**Modelling with KNeighborsClassifier**","e5e8c7ac":"**Modelling with Light Gradient Booster**","38b38332":"**Performing tuning using Bayesian Optimization.**","0281da80":"**Perform data visualization**\n\nA graph is a lot more telling about the distribution and relationships of attributes.\n\nNevertheless, it is important to take your time and review the statistics first. Each time you review the data a different way, you open yourself up to noticing different aspects and potentially achieving different insights into the problem.\n\nWe can refer about [data visualization in Pandas](http:\/\/pandas.pydata.org\/pandas-docs\/stable\/user_guide\/visualization.html).","8271f17b":"**Summarize Data**\nWe will start out by understanding the data that we have by looking at it\u2019s structure.\n\n**Load Data**\nStart by loading the CSV data from file into memory as a data frame. We know the names of the data provided, so we will set those names when loading the data from the file.","3f6fa6fb":"**Performing tuning using Bayesian Optimization.**","e4b167b8":"**Feature-Target Relationships**\n\nThe next important relationship to explore is that of each attribute to the \"Target\" attribute.","99a2910e":"**Performing tuning using Bayesian Optimization.**","40d13f99":"**Costa Rican Household Poverty Level Prediction**\n\nProblem and Data Explanation The data for this competition is provided in two files: train.csv and test.csv. The training set has 9557 rows and 143 columns while the testing set has 23856 rows and 142 columns. Each row represents one individual and each column is a feature, either unique to the individual, or for the household of the individual. The training set has one additional column, Target, which represents the poverty level on a 1-4 scale and is the label for the competition. A value of 1 is the most extreme poverty.\n\nThe Target values represent poverty levels as follows:\n\n1 = extreme poverty \n2 = moderate poverty \n3 = vulnerable households \n4 = non vulnerable households\n\nThe explanations for all 143 columns can be found in the [competition documentation](https:\/\/www.kaggle.com\/c\/costa-rican-household-poverty-prediction\/data), but a few to note are below:\n\n**Id:** a unique identifier for each individual, this should not be a feature that we use!\n**idhogar:** a unique identifier for each household. This variable is not a feature, but will be used to group individuals by household as all individuals in a household will have the same identifier. \n**parentesco1:** indicates if this person is the head of the household. \n**Target:** the label, which should be equal for all members in a household\n\nThis is a supervised multi-class classification machine learning problem:\n\n**Supervised:** provided with the labels for the training data\n**Multi-class classification: ** Labels are discrete values with 4 classes\n\n![![Screen%20Shot%202019-05-03%20at%201.07.30%20PM.png](attachment:Screen%20Shot%202019-05-03%20at%201.07.30%20PM.png)](https:\/\/www.habitatforhumanity.org.uk\/wp-content\/uploads\/2017\/10\/Housing-poverty-Costa-Rica--1200x600-c-default.jpg)\n\n**Objectives:**\n\nObjective of this kernel is to perform modeling with the following estimators with default parameters & get accuracy\n\n**Modeling Estimaters**\n    1. GradientBoostingClassifier\n    2. RandomForestClassifier\n    3. KNeighborsClassifier\n    4. ExtraTreesClassifier\n    5. XGBoost\n    6. LightGBM\n\n**Tuning:**\nPerform tuning using Bayesian Optimization & compare the accuracy of the estimators.","72ef79ba":"**Modelling with ExtraTreeClassifier**","74690c59":"**Performing tuning using Bayesian Optimization.**","5a654909":"**Modeling**"}}