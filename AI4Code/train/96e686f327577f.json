{"cell_type":{"96cd6458":"code","bf7ecf99":"code","65867fad":"code","80f9e71a":"code","3bbc343f":"code","40107a83":"code","b16df4f5":"code","c725b298":"code","ac6a67f5":"code","e138b66f":"code","98226309":"code","8e916a79":"code","e5d25761":"code","d8dce4e2":"code","72fe7d3e":"code","dab8cb45":"code","21cadbb7":"code","e155fa2c":"markdown"},"source":{"96cd6458":"!pip install ffmpeg-python","bf7ecf99":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom tqdm.auto import tqdm\nfrom PIL import Image, ImageDraw\nfrom scipy.spatial import distance_matrix\nimport cv2\nimport torch\nfrom IPython.display import Video\nimport ffmpeg","65867fad":"# Get the frames\n\ndef get_frames(filepath):\n    if not os.path.exists(filepath):\n        raise OSError(f\"The filepath {filepath} does not exist.\")\n    vidcap = cv2.VideoCapture(filepath)\n    frames = []\n    success, frame = vidcap.read()\n    while success:\n        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Change to rgb\n        frames.append(frame)\n        success, frame = vidcap.read()  # Get the next frame\n    return frames\n\n\nroot = \"\/kaggle\/input\/nfl-health-and-safety-helmet-assignment\"\nexample_filepath = f\"{root}\/train\/57784_001741_Sideline.mp4\"\n\nframes = get_frames(example_filepath)\n\nframe_height, frame_width = frames[0].shape[:2]","80f9e71a":"# Get the yolov5 model. Luckily it has been trained to detect humans (along other things as well).\nmodel = torch.hub.load('ultralytics\/yolov5', 'yolov5s')\n\n# Fix the plotting...\n%matplotlib inline","3bbc343f":"def get_predictions(frames):\n    predictions = []\n    for frame in tqdm(frames):\n        df = model(frame).pandas().xyxy[0]  # Predict\n        df = df[df[\"class\"] == 0]  # Restrict to persons\n        df = df.reset_index(drop=True)\n\n        # Calculate some properties\n        df[[\"center_x\", \"center_y\"]] = df.apply(\n            lambda row: [row[\"xmin\"] + (row[\"xmax\"] - row[\"xmin\"]) \/\/ 2, row[\"ymin\"] + (row[\"ymax\"] - row[\"ymin\"]) \/\/ 2], \n            axis=1, \n            result_type=\"expand\"\n        )\n        df[\"area\"] = df.apply(lambda row: (row[\"xmax\"] - row[\"xmin\"]) * (row[\"ymax\"] - row[\"ymin\"]), axis=1)\n        predictions.append(df)\n    return predictions\n\n\ndef draw_bounding_boxes(frames, predictions, color):\n    frames_with_bbox = []\n    # Draw rectangles\n    for frame, df in tqdm(zip(frames, predictions)):\n        im = Image.fromarray(frame)\n        im_draw = ImageDraw.Draw(im)\n        for _, row in df.iterrows():  # Iterate persons\n            im_draw.rectangle([(row[\"xmin\"], row[\"ymin\"]), (row[\"xmax\"], row[\"ymax\"])], outline=color)\n        frames_with_bbox.append(np.array(im))\n    return frames_with_bbox\n\n\npredictions = get_predictions(frames)\nframes_with_bbox = draw_bounding_boxes(frames, predictions, \"black\")","40107a83":"# Test that the bounding boxes are there\nplt.figure(figsize=(10, 8))\nplt.imshow(frames_with_bbox[0])","b16df4f5":"# Make a video with the bounding boxes\n\n# Code copied from https:\/\/github.com\/kkroening\/ffmpeg-python\/issues\/246#issuecomment-520200981\ndef vidwrite(fn, images, framerate, vcodec='libx264'):\n    if not isinstance(images, np.ndarray):\n        images = np.asarray(images)\n    n,height,width,channels = images.shape\n    process = (\n        ffmpeg\n            .input('pipe:', format='rawvideo', pix_fmt='rgb24', s='{}x{}'.format(width, height))\n            .output(fn, pix_fmt='yuv420p', vcodec=vcodec, r=framerate)\n            .overwrite_output()\n            .run_async(pipe_stdin=True)\n    )\n    for frame in tqdm(images):\n        process.stdin.write(\n            frame\n                .astype(np.uint8)\n                .tobytes()\n        )\n    process.stdin.close()\n    process.wait()\n\n# The framerate might be incorrect but does not matter at this point\nvidwrite(\"output.mp4\", frames_with_bbox, framerate=60)  ","c725b298":"Video(\"\/kaggle\/working\/output.mp4\", embed=True, width=800)","ac6a67f5":"def update_bounding_boxes(df_current, df_next, last_changed):\n    \"\"\"Update bounding boxes for the next frame.\n    \n    Using simple tracking by choosing the nearest bounding box \n    from the current bounding boxes.\n    \"\"\"\n    # Get the closest bounding box in the next frame from the \n    # current bounding boxes.\n    d_matrix = distance_matrix(\n        df_current[[\"center_x\", \"center_y\"]].values, \n        df_next[[\"center_x\", \"center_y\"]].values\n    )\n    sorted_indexes = np.argsort(d_matrix.flatten())\n    mappings = {}\n    count = 0\n    for k, index in enumerate(sorted_indexes):\n        i = index \/\/ d_matrix.shape[1]\n        j = index % d_matrix.shape[1]\n        if j in mappings:\n            continue\n        else:\n            mappings[j] = i\n            count += 1\n            if count == d_matrix.shape[1]:\n                break\n    \n    # Update the current bounding boxes with the new bounding boxes.\n    for j, i in mappings.items():\n        df_current.loc[i] = df_next.loc[j]\n        \n    # Remove bounding boxes that try to leave the frame.\n    remove_these = []\n    for i, n in last_changed.items():\n        if i in mappings.values():\n            last_changed[i] = 0\n        else:\n            last_changed[i] += 1\n        if n > 3:\n            if (df_current.loc[i, \"ymin\"] < 5 \n                or df_current.loc[i, \"xmin\"] < 5 \n                or df_current.loc[i, \"ymax\"] > frame_height - 5 \n                or df_current.loc[i, \"xmin\"] > frame_width - 5\n               ):\n                df_current = df_current.drop(i)\n                remove_these.append(i)\n    for i in remove_these:\n        del last_changed[i]\n\n    return df_current, last_changed\n\n\n# Let's try to follow each player\ndf_current = predictions[0].copy()\nhistory = [df_current.copy()]\nlast_changed = {i: 0 for i in range(len(df_current))}\n\ntmps = []\nfor i in tqdm(range(1, len(predictions))):\n    df = predictions[i]\n    df_current, last_changed = update_bounding_boxes(df_current, df, last_changed)\n    history.append(df_current.copy())","e138b66f":"# Draw blue bounding boxes. Each blue box tries to present each individual player.\nframes_with_bbox2 = draw_bounding_boxes(frames_with_bbox, history, \"blue\")\n\nvidwrite(\"output2.mp4\", frames_with_bbox2, framerate=60)","98226309":"Video(\"\/kaggle\/working\/output2.mp4\", embed=True, width=800)","8e916a79":"# TODO: Handle multiple boxes trying to present a single player. \n# This seems to be caused by some bounding box leaving another player.","e5d25761":"# Draw blue bounding boxes. Each blue box tries to present each individual player.\nframes_with_bbox3 = draw_bounding_boxes(frames, [df.loc[0:0] for df in history], \"blue\")\n\nvidwrite(\"output3.mp4\", frames_with_bbox3, framerate=60)","d8dce4e2":"Video(\"\/kaggle\/working\/output3.mp4\", embed=True, width=800)","72fe7d3e":"# The beginning goes well, but at the end the bounding boxes jump everywhere.\n# Let's investigate a single player and its speed.\n\nplayer_bbox = [df.loc[0] for df in history if 0 in df.index]\ndf_player = pd.DataFrame([s[[\"center_x\", \"center_y\"]].to_list() for s in player_bbox], columns=[\"center_x\", \"center_y\"])\n\ndf_player[[\"speed_x\", \"speed_y\"]] = df_player[[\"center_x\", \"center_y\"]] - df_player.shift(1)[[\"center_x\", \"center_y\"]]\ndf_player.loc[0, [\"speed_x\", \"speed_y\"]] = 0\ndf_player[\"speed\"] = df_player[\"speed_x\"] ** 2 + df_player[\"speed_y\"] ** 2\ndf_player","dab8cb45":"fig, axs = plt.subplots(1, 2, figsize=(14, 6))\ndf_player[\"speed\"].plot(ax=axs[0])\ndf_player.loc[:190, \"speed\"].plot(ax=axs[1])","21cadbb7":"# I guess this could be smoothed out to get rid of the huge jumps.","e155fa2c":"# Player tracker (WIP)\n\nUsing the pretrained yolov5 model to detect persons. With the detections trying to follow each individual player by choosing the nearest bounding box in the next frame. This approach has multiple issues e.g. sometimes two bounding boxes are merged to one, players leave the frame, etc."}}