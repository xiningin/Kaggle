{"cell_type":{"96da2866":"code","d6e130d8":"code","e37a1353":"code","c44cee7e":"code","e8e2b736":"code","bf03bd87":"code","818fabed":"code","9e071a5a":"code","b32fee77":"code","8ef1611e":"code","509b8404":"code","d6ba1256":"code","27789880":"markdown","d3839660":"markdown","510fcb0e":"markdown","8bb72567":"markdown","a490dd73":"markdown","31970a10":"markdown","524857b4":"markdown","81aadf41":"markdown"},"source":{"96da2866":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set_style(\"whitegrid\")\nsns.set_palette(\"deep\")","d6e130d8":"train = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\nsubmission = pd.read_csv(\"\/kaggle\/input\/titanic\/gender_submission.csv\")\ntrain.head()","e37a1353":"train.info() ","c44cee7e":"tmp = train.drop(columns=\"Cabin\").drop(index=train.loc[train[\"Embarked\"].isna()].index) # i delete the uninsightful columns and indices temporarily\nmean_tmp = tmp[[\"Pclass\", \"Sex\", \"Survived\"]].groupby([\"Pclass\", \"Sex\"], as_index=False).mean()\n\nfig = plt.figure(figsize=(20, 15))\nsubfig = fig.subfigures(2, 1)\n\ng1 = sns.FacetGrid(mean_tmp, col=\"Pclass\", aspect=1, height=4)\ng1.map(sns.barplot, \"Sex\", \"Survived\", order=[\"male\", \"female\"])\n\ng2 = sns.FacetGrid(tmp, col=\"Pclass\", aspect=1, height=4)\ng2.map(sns.countplot, \"Sex\", order=[\"male\", \"female\"])\n\nsubfig[0] = g1.figure\nsubfig[0].suptitle(\"Count and Ratio of Survivers\", y=1.1, fontsize=16)\nsubfig[1] = g2.figure","e8e2b736":"tmp = train.drop(columns=\"Cabin\").drop(index=train.loc[train[\"Embarked\"].isna()].index) # i delete the uninsightful columns and indices temporarily\n\nfig, (ax1, ax2) =plt.subplots(2, 1, figsize=(20,10))\n\nsns.kdeplot(data=tmp, x=\"Age\", shade=True, hue=\"Sex\", ax=ax1)\nsns.kdeplot(data=tmp, x=\"Age\", shade=True, hue=\"Survived\", ax=ax2)","bf03bd87":"tmp_mean = tmp[[\"Sex\", \"Age\", \"Survived\"]].groupby([\"Age\", \"Sex\"], as_index=False).mean()\nsns.lmplot(data=tmp_mean, x=\"Age\", y=\"Survived\", hue=\"Sex\", aspect=6, height=4)","818fabed":"from sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.base import TransformerMixin, BaseEstimator\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\nfrom sklearn.linear_model import LinearRegression\n\ndrop_labels = [\"PassengerId\", \"Cabin\", \"Ticket\", \"Name\"]\nclass FullPrep(TransformerMixin, BaseEstimator):\n    def __init__(self):\n        pass\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        return X.drop(columns=drop_labels)\n\ndrop_na_labels = list(train) # I know that this isn't nececary\nnum_labels = [\"Pclass\", \"Age\", \"SibSp\", \"Parch\", \"Fare\"]\n\nnum_pipeline = Pipeline([\n    (\"imputer\", SimpleImputer(strategy=\"median\"))\n])\n\nembarked_pipeline = Pipeline([\n    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n    (\"cat_enc\", OneHotEncoder())\n])\n\ncolumn_t = ColumnTransformer([\n    (\"num\", num_pipeline, num_labels),\n    (\"embarked\", embarked_pipeline, [\"Embarked\"]),\n    (\"sex\", OrdinalEncoder(), [\"Sex\"])\n])\n\nfull_pipeline = Pipeline([\n    (\"dropna\", FullPrep()),\n    (\"columns\", column_t),\n])\n\ntrain_X, test_X = full_pipeline.fit_transform(train.drop(columns=\"Survived\")), full_pipeline.fit_transform(test)\ntrain_y = train[\"Survived\"]\ntrain_X[0], train_y[0]","9e071a5a":"from sklearn.model_selection import cross_val_score\n\nlin_reg = LinearRegression()\nresult = cross_val_score(lin_reg, train_X, train_y, cv=10)\nresult.mean()","b32fee77":"from sklearn.ensemble import RandomForestClassifier\n\nreg = RandomForestClassifier(n_estimators=100)\nscore = cross_val_score(reg, train_X, train_y, cv=10)\nscore.mean()\n# ...","8ef1611e":"# Final training\nreg.fit(train_X, train_y)","509b8404":"submission = pd.DataFrame({\n    \"PassengerId\": test[\"PassengerId\"],\n    \"Survived\": reg.predict(test_X)\n})\nsubmission","d6ba1256":"# Yaaaaaay\nsubmission.to_csv(\"submission.csv\", index=False)","27789880":"# Thanks for reading my first Notebook! :)\n\nFeel free to comment any improvements you would make that i didn't mention.","d3839660":"First I tried *LinearRegression* but then I noticed that I need a classifier. ","510fcb0e":"The last plot shows a linear correlation between age, sex and probability of survival. As you can see, not only were women much more likely to survive but the age didn't play as big of a role as for male passengers.\n\nAs you can see, the *Cabin* attribute is too incomplete as to beeing useful. Which is unfortunate, because the attribute might have given some insight. One possibility would be to transform the cabins into areas and then derive the area from the ticket price for the missing values. And I could look at surnames of the passengers to find the missing *Embarked* values. But I decided not to bother. The missing values in *Age* can be easily replaced by the sklearn *SimpleImputer* class with `strategy=\"median\"`. The *PassengerID* doesn't have any meaning too. And some Columns are dropped for simplicity. *Sex* is converted to a binary numeric attribute.","8bb72567":"These plots show, that children are much more likely to survive than middle-aged people. The first plot is just for basic visualization of the age distribtion on the titanic.","a490dd73":"Obviously the classifier works much better. At this point I could test some other models, but not for now.","31970a10":"# Titanic predictions and visualization\n\nThis is in fact my first submission","524857b4":"Lets clean up the data a bit and then look at it closer. ","81aadf41":"There is a strong correlation between the passengerclass, the sex and the rate of survival. In the following we will look at the correlation between the age and survival rate."}}