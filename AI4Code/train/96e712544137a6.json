{"cell_type":{"89f87d21":"code","1f1dacaf":"code","60bc25a7":"code","67425ff8":"code","c1d63f21":"code","00cb4e2a":"code","58410295":"code","77f33bea":"code","559e2cb5":"code","9e4dacff":"code","aa0188ed":"code","58a0e1b6":"code","68f18cc5":"code","6983c89b":"code","b650c028":"code","bb5677a0":"code","72df8dc8":"code","f768242a":"code","0849a49f":"code","14682843":"code","c2c2a539":"code","b4253c15":"code","404e4f25":"code","ebbd6640":"code","1d99d552":"code","92b5228e":"code","bdc374c5":"code","5d77925c":"code","7ff07470":"markdown","3e62a2b7":"markdown","d783629b":"markdown","30582c9d":"markdown","839fea7e":"markdown","c6a8c6df":"markdown","79ddfd7f":"markdown","9265c29a":"markdown","d3fea3c2":"markdown","4431db30":"markdown","0aa6e792":"markdown","fc7b7e29":"markdown","f72d7357":"markdown","ef74d4ed":"markdown","5495e858":"markdown","e80ee750":"markdown"},"source":{"89f87d21":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans, DBSCAN\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import silhouette_score, silhouette_samples\nimport matplotlib.cm as cm\nfrom sklearn.metrics.cluster import adjusted_rand_score","1f1dacaf":"data_cls_1 = pd.read_csv(\"..\/input\/clustering-dataset\/data_cls_1.csv\")\ndata_cls_1.head()","60bc25a7":"data_cls_1.isna().sum()","67425ff8":"data_cls_1.shape","c1d63f21":"data_cls_1.describe()","00cb4e2a":"standardscaler = StandardScaler()\nscaled_data = standardscaler.fit_transform(data_cls_1)\nscaled_cls1 = pd.DataFrame(scaled_data, columns = data_cls_1.columns)\nscaled_cls1.head()","58410295":"scaled_cls1.describe()","77f33bea":"pca = PCA()\npca_data = pca.fit_transform(scaled_cls1)","559e2cb5":"plt.axhline(y=1\/len(scaled_cls1.columns),color=\"r\")\nplt.plot(pca.explained_variance_ratio_)","9e4dacff":"pca_data = pd.DataFrame(pca_data, columns=[\"pc1\",\"pc2\",\"pc3\",\"pc4\",\"pc5\",\"pc6\",\"pc7\",\"pc8\",\"pc9\",\"pc10\",\"pc11\"])\nprint(\"pca.explained variance ratio:\\n\", pca.explained_variance_ratio_)","aa0188ed":"wss = []\nK = range(2,11)\nfor k in K:\n    kmeans = KMeans(n_clusters=k, random_state=0)\n    kmeans = kmeans.fit(scaled_data)\n    wss.append(kmeans.inertia_)\nplt.plot(K, wss, \"b*-\")\nplt.xlabel(\"Number of clusters k\")\nplt.ylabel(\"Total Within Sum of Squares\")\nplt.title(\"Optimal number of clusters\")\n# plt.axis([0, 11, 0, 10000])\nplt.show()","58a0e1b6":"sil_avg = [0]\nK = range(2,11)\nfor k in K:\n    kmeans = KMeans(n_clusters=k, random_state=23)\n    cluster_labels=kmeans.fit_predict(scaled_data) ###same with>> cluster_labels=kmeans.labels_\n    silhouette_avg = silhouette_score(scaled_data, cluster_labels)\n    sil_avg.append(silhouette_avg)\n    print(\"n_clusters =\", k, \"The average silhouette_score is : %.4f\" % silhouette_avg)\n\n\nplt.plot(range(1,len(sil_avg)+1), sil_avg, \"b.-\")\nplt.xlabel(\"Number of clusters k\")\nplt.ylabel(\"Average silhouette width \")\nplt.title(\"Optimal number of clusters\")\nplt.axvline( color='r', x=sil_avg.index(max(sil_avg))+1 )\nplt.show()","68f18cc5":"kmeans = KMeans(n_clusters=4, random_state=0)\ndf = pca_data[[\"pc1\",\"pc2\"]].copy()\ndf[\"clusters\"] = kmeans.fit_predict(scaled_data)\nplt.scatter(df[\"pc1\"], df[\"pc2\"],c=df.clusters)","6983c89b":"data_cls_2 = pd.read_csv(\"..\/input\/clustering-dataset\/data_cls_2.csv\")\ndata_cls_2.head()","b650c028":"data_cls_2.shape","bb5677a0":"data_cls_2.describe()","72df8dc8":"data_cls_2.isna().sum()","f768242a":"standardscaler = StandardScaler()\nscaled_data = standardscaler.fit_transform(data_cls_2)\nscaled_cls2 = pd.DataFrame(scaled_data, columns = data_cls_2.columns)\nscaled_cls2.head()","0849a49f":"scaled_cls2.describe()","14682843":"true_cls2 = pd.read_csv(\"..\/input\/clustering-dataset\/true_label_cls2.csv\")","c2c2a539":"epsilon = np.arange(0.1,2.6,0.1)\nmin_samples = [5,10,15,20]","b4253c15":"scores2 = []\n\nfor i in min_samples:\n    for j in epsilon:\n        dbmodel2 = DBSCAN(eps=j, min_samples = i).fit(scaled_cls2)\n        adjustscore = adjusted_rand_score(true_cls2.iloc[:,0], dbmodel2.labels_)\n        \"\"\"print(\"For epsilon = {}, and, min samples = {}, score is: {}\".format(i,j,adjustscore))\"\"\"\n        tup = tuple([j,i,adjustscore])\n        scores2.append(tup)","404e4f25":"sorted_by_rand_score2 = sorted(scores2, key=lambda tup: tup[2],reverse = True)","ebbd6640":"sorted_by_rand_score2[0:10]","1d99d552":"bestclustering2 = DBSCAN(eps=0.1, min_samples = 5).fit(scaled_cls2)","92b5228e":"clusters2 = bestclustering2.labels_","bdc374c5":"def viz_cluster(scaled_cls2,clusters2):\n    df = pd.DataFrame(dict(x= scaled_cls2.iloc[:,0], y =scaled_cls2.iloc[:,1],label=clusters2))\n    colors = {-1:\"red\",0:\"blue\",1:\"orange\",2:\"green\",3:\"skyblue\"}\n    fig,ax = plt.subplots(figsize=(8,8))\n    grouped = df.groupby(\"label\")\n    for key, group in grouped:\n        group.plot(ax=ax, kind=\"scatter\",x=\"x\",y=\"y\",label=key,color=colors[key])\n    \n    plt.xlabel(\"x_1\")\n    plt.ylabel(\"x_2\")\n    plt.show()","5d77925c":"viz_cluster(scaled_cls2,clusters2)","7ff07470":"PCA can be used as dimensionality reduction step to improve model performance with using principal components instead of attributes. In this case, we need to reduce dimensionality to 2, to enable plotting the data.","3e62a2b7":"Below, using true class labels, I make a grid-search to find best epsilon and min_samples parameters.","d783629b":"After defining the best parameters (eps=0.1, min_samples = 5), DBSCAN algorithm is applied to this second dataset as well. As we can see from the plot, it does an impressive performance for dealing with non-linearly seperable data. 4 clusters are created very similar to their corresponding true class labels. Red dots are also assigned as outliers.","30582c9d":"Silhouette scores also show that model performs best when we use 4 clusters.\n\nSince we decide what is the optimal number of clusters for kmeans model(4), we can create a model for clustering. To visualize the seperability of each cluster we can use scatter plots with dimensions PC1 and PC2.","839fea7e":"This is another artificially generated dataset. However, data points in this dataset are not linearly seperable, thus, different kind of approach is required.","c6a8c6df":"Before applying k-means, we need to detect optimal K parameter. Scree plot and silhouette scores are demonstrated below to find k parameter.","79ddfd7f":"In this final result, it can be seen that kmeans performed well in this scenario.","9265c29a":"**K-means**","d3fea3c2":"**Preprocessing**","4431db30":"**DBSCAN**","0aa6e792":"**PCA**","fc7b7e29":"This is a randomly generated artificial dataset,to understand and apply 2 different clustering methods. Kmeans is a partitioning based clustering algorithm, while DBSCAN is a density based algorithm. Therefore, these two have different use case scenarios. In this work, these scenarios are discovered.\n\nAt first, exploratory data analysis and preprocessing steps are applied.","f72d7357":"Scree plot indicates that optimal value of k for kmeans model is 4.","ef74d4ed":"First, I inspect the data. It contains 11 features and 3000 instances. It does not have any missing value, however, each attribute have different range, std and mean.\n\nTo apply kmeans clustering, we need to set all attribute means to 0 and std to 1.","5495e858":"After printing rand scores, I find out that 0.1 and 5 are best values for epsilon and min_samples parameters, respectively.","e80ee750":"Now, as we can see from the description table, all attributes are in similar range, with same means and standard deviations."}}