{"cell_type":{"980a55b0":"code","1e4ee6a4":"code","8b127960":"code","0c7e5980":"code","e9d33741":"markdown","15fb8e4f":"markdown","0f953f0d":"markdown","7fb37b8f":"markdown","cf6c731f":"markdown"},"source":{"980a55b0":"from mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt # plotting\nimport numpy as np # linear algebra\nimport os # accessing directory structure\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport json\nimport math","1e4ee6a4":"with open('\/kaggle\/input\/circle_gestures.json') as data_file:    \n    circle_gestures = json.load(data_file)\n\n#change index to plot other data in the list.\nindex_to_show = 0\n    \nsample = circle_gestures[index_to_show]['gestureData']\nx = [coord['x'] for coord in sample]\ny = [coord['y'] for coord in sample]\n\nplt.scatter(x,y)\nplt.show()","8b127960":"INF = 999999\nNEGINF = -999999\n\ndef discretize_2D(gesture_list):\n    \n    PADDING_FACTOR = 5\n    cube_x = 29\n    cube_y = 29\n    \n    #cube size is cube_y + 2*PADDING_FACTOR + 1!!!\n    \n    cubes = []\n    for i in gesture_list:\n        gestureData = i['gestureData']\n        gestureRange = GestureRange()\n        #get maximum and minimum x & y values\n        for j in gestureData:\n            if(j['x'] > gestureRange.x_max):\n                gestureRange.x_max = j['x']\n            if(j['x'] < gestureRange.x_min):\n                gestureRange.x_min = j['x']\n            \n            if(j['y'] > gestureRange.y_max):\n                gestureRange.y_max = j['y']\n            if(j['y'] < gestureRange.y_min):\n                gestureRange.y_min = j['y']\n                \n        #get range of x and y data\n        x_range = gestureRange.x_max - gestureRange.x_min\n        y_range = gestureRange.y_max - gestureRange.y_min\n        \n        #get factor for discretization. Itution: How many X or Y Values fit in one discretized \"step\"\n        discretization_factor_x  = x_range\/cube_x\n        discretization_factor_y = y_range\/cube_y\n                \n        #The bigger factor counts.\n        step_max  = max([discretization_factor_x, discretization_factor_y])\n        \n        #Set data to all 0\n        cubeData = np.zeros((cube_x+1,cube_y+1))\n           \n        frame_count = 0\n        for j in gestureData:\n            #zero center data\n            x_zeroed = j['x'] - gestureRange.x_min\t\n            y_zeroed = j['y'] - gestureRange.y_min\n            \n            #get discretized value\n            x_discretized = int(x_zeroed \/ step_max)      \n            y_discretized = int(y_zeroed \/ step_max)\n            \n            frame_count = frame_count +1\n            cubeData[x_discretized,y_discretized] = 1\n            \n            \n        #create padding with zeros\n        padded_cube_data = np.lib.pad(cubeData, PADDING_FACTOR, 'minimum')\n        \n        #wire it all together\n        original_cube = {}\n        original_cube['gestureData'] = padded_cube_data\n        original_cube['gestureClass'] = i['gestureClass']\n        \n        cubes.append(original_cube)\n      \n    return cubes\n\nclass GestureRange:\n    x_min = INF\n    x_max = NEGINF\n    y_min = INF\n    y_max = NEGINF\n    z_min = INF\n    z_max = NEGINF","0c7e5980":"discrete_circle_gestures = discretize_2D(circle_gestures)\nsample_discrete_circle_gesture = discrete_circle_gestures[0]\n\nplt.imshow(sample_discrete_circle_gesture['gestureData'])","e9d33741":"# Data preprocessing\n\nThe first and most important step I took in data preprocessing to discretize the raw data. \nThe raw data is a list of sequential (x,y,z) coordinate pairs. The goal is to map those coordinate pairs onto a discrete scale.\n\nBelow you can find the algorithm:","15fb8e4f":"... And a plot of a discretized 2D gesture","0f953f0d":"# Exploratory Analysis\n\nLets first have a look on the raw data. In the following example you can find a 2D plot of the raw data of a circle gesture. You can simply use this code to explore the other gestures (line, V and wave) as well. Just import the gestures from the other `.json` files to do so.","7fb37b8f":"# Introduction\n\nWelcome to the HoloLens Gesture recognition dataset which was created during my Bachelor Thesis. The goal of the thesis was to classify between 4 different hand gestures done on the Microsoft HoloLens 2 using convolutional neural networks. The different gestures are a line, circle, wave and a V. The HoloLens was used to capture the hand drawing those gestures into the air.\n\nThis Notebook contains some code to help you getting started with the HoloLens gesture data set. For simplicity reasons the code in this notebook only handles two dimensional Data. \n\n**This Means:** Even though the dataset contains a lists of (X,Y,Z) coordinates of the hand performing gestures, only (X,Y) coordinate pairs will be handled here.","cf6c731f":"## Your turn\n\nusing the discretization method we reducing the variety of the input features. Doing so enables us to effectively apply classification alorithms on the data. In my Bachelor thesis I used a CNN for classification.\n\nHave fun :)"}}