{"cell_type":{"b103026a":"code","39a3a598":"code","3fee4c99":"code","0a00a8c4":"code","ac3eeadf":"code","5cba286d":"code","e35edd3a":"code","e967c266":"code","870ae421":"code","ed5a183b":"code","b7f8aa35":"code","c06b95b5":"code","1efbb04b":"code","10ae541b":"code","3472c22b":"code","72ec5c72":"code","1fea4ed8":"code","5315ece7":"code","891d662d":"code","d345d550":"code","5666b1d8":"code","ad86bc2a":"code","b07bf41e":"code","2b568cc4":"code","223e0e23":"code","39ad91c0":"code","694d5088":"markdown","7949ac15":"markdown","d3c55678":"markdown","4c1e6531":"markdown","c07e5e87":"markdown","d45e45c0":"markdown","9aebbf2a":"markdown","6bd9eaab":"markdown","9765e9f3":"markdown","924a00b4":"markdown","fe6bc503":"markdown","543152bc":"markdown","af991a9a":"markdown","e70a2830":"markdown","ca6f47f8":"markdown","e9e72df3":"markdown","f883d358":"markdown","ae46cccd":"markdown","ee7e401c":"markdown","bccb78f2":"markdown","0bf76b82":"markdown","8c52b966":"markdown","744ec2b4":"markdown","1db62572":"markdown","63da1e8e":"markdown"},"source":{"b103026a":"import pandas as pd # dataframe manipulation\nimport numpy as np # linear algebra\nimport matplotlib.pyplot as plt # data visualization\n%matplotlib inline\n\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\nfrom keras.datasets import fashion_mnist # dataset\nfrom sklearn.model_selection import train_test_split\nimport umap","39a3a598":"(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()","3fee4c99":"# create dictionary for labels and their respective descriptions\nlabels = np.arange(10)\nclass_names = ['T-shirt\/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n              'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\nlabel_dict = dict(zip(labels, class_names))\n\n# display images\nn_images = 25\nfig, axes = plt.subplots(5, 5, figsize=(15, 15))\naxes = axes.ravel()\nfor i, (ax, image, label) in enumerate(zip(axes, train_images[:n_images], train_labels[:n_images])):\n    ax.imshow(image, cmap='binary')\n    ax.set_title(f'{label_dict[label]}', fontweight='bold')\n    ax.set_xticks([])\n    ax.set_yticks([])","0a00a8c4":"# data types\nprint(f\"Data type of train features - {train_images.dtype}\")\nprint(f\"Data type of train labels - {train_labels.dtype}\")\nprint(f\"Data type of test features - {test_images.dtype}\")\nprint(f\"Data type of test labels - {test_labels.dtype}\")","ac3eeadf":"# shape\nprint(f\"Shape of train features - {train_images.shape}\")\nprint(f\"Shape of train labels - {train_labels.shape}\")\nprint(f\"Shape of test features - {test_images.shape}\")\nprint(f\"Shape of test labels - {test_labels.shape}\")","5cba286d":"# unique labels\nprint(f\"Unique labels in train set - {np.unique(train_labels)}\")\nprint(f\"Unique labels in test set - {np.unique(test_labels)}\")","e35edd3a":"print('Label and its respective description:\\n')\nfor k, v in label_dict.items():\n    print(f\"{k} : {v}\")","e967c266":"print(f\"Number of train images - {train_images.shape[0]}\")\nprint(f\"Number of test images - {test_images.shape[0]}\")","870ae421":"# shape of first image in the dataset\ntrain_images[0].shape","ed5a183b":"memory_train_image = 8 * 28 * 28 * 60_000\nmemory_train_labels = 8 * 60_000\nprint(f\"Memory required for train images - {memory_train_image} bits\")\nprint(f\"Memory required for train labels - {memory_train_labels} bits\")\nprint(f\"Total Memory required for train data - {(memory_train_image + memory_train_labels)\/(1e6 * 8)} Megabytes(MB)\")","b7f8aa35":"training_images, val_images, training_labels, val_labels = train_test_split(\n    train_images, \n    train_labels, \n    test_size=0.15,\n    stratify=train_labels, \n    random_state=2021\n)","c06b95b5":"print(f\"Shape of training set - {training_images.shape}\")\nprint(f\"Shape of validation set - {val_images.shape}\")\nprint(f\"Shape of test set - {test_images.shape}\")","1efbb04b":"def normalize_img(image, label):\n    \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n    return tf.cast(image, tf.float32) \/ 255., label","10ae541b":"# training set pipeline\ndef train_pipeline(images, labels):\n    data = tf.data.Dataset.from_tensor_slices((images, labels))\n    data = data.map(normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n    data = data.cache()\n    data = data.shuffle(len(images))\n    data = data.batch(128)\n    data = data.prefetch(tf.data.experimental.AUTOTUNE)\n    return data\n\n# validation and test set pipeline\ndef val_test_pipeline(images, labels):\n    data = tf.data.Dataset.from_tensor_slices((images, labels))\n    data = data.map(normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n    data = data.batch(128)\n    data = data.cache()\n    data = data.prefetch(tf.data.experimental.AUTOTUNE)\n    return data","3472c22b":"# tensorflow data object\ntrain_ds = train_pipeline(training_images, training_labels)\nval_ds = val_test_pipeline(val_images, val_labels)\ntest_ds = val_test_pipeline(test_images, test_labels)","72ec5c72":"# model architecture\nmodel = tf.keras.models.Sequential(\n    [tf.keras.layers.Flatten(input_shape=(28, 28)),\n     #tf.keras.layers.AlphaDropout(0.05),\n     tf.keras.layers.Dense(128, \n                           activation='selu', \n                           kernel_initializer='lecun_normal'),\n     tf.keras.layers.AlphaDropout(0.1),\n     tf.keras.layers.Dense(64, \n                           activation='selu', \n                           kernel_initializer='lecun_normal'), \n                           #kernel_regularizer=tf.keras.regularizers.L2(0.01)),\n     tf.keras.layers.Dense(10, \n                           activation='softmax', \n                           kernel_initializer='glorot_uniform')]\n)\n\n# model compilation\nmodel.compile(\n    optimizer=tf.keras.optimizers.Nadam(learning_rate=1e-4), \n    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]\n)","1fea4ed8":"tf.keras.utils.plot_model(model, show_shapes=True)","5315ece7":"model.summary()","891d662d":"def plot_weights(model):\n    # select first hidden layer\n    hidden1 = model.layers[1]\n    \n    # get wights method return the layer weights and biases\n    weights, biases = hidden1.get_weights()\n    \n    # dimentionality reduction using umap\n    embedding = umap.UMAP(n_neighbors=5).fit_transform(weights)\n    \n    # plot\n    plt.figure(figsize=(10, 6))\n    plt.scatter(embedding[:, 0], embedding[:, 1])\n    plt.title('UMAP projection of hidden layer initial weights')\n    plt.show()","d345d550":"plot_weights(model)","5666b1d8":"# setup callback early stoping\ncallbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)]","ad86bc2a":"history = model.fit(\n    train_ds, \n    epochs=22, \n    validation_data=val_ds, \n    callbacks=callbacks\n)","b07bf41e":"# layers weights after fitting model\nplot_weights(model)","2b568cc4":"def plot_loss(history):\n    \"\"\"\n    This function plots the training and validation loss\n    \"\"\"\n    history_dict = history.history\n    loss_values = history_dict['loss']\n    val_loss_values = history_dict['val_loss']\n    epochs = range(1, len(loss_values) + 1)\n    plt.figure(figsize=(15, 7))\n    plt.plot(epochs, loss_values, 'go', label='Training loss')           \n    plt.plot(epochs, val_loss_values, 'r', label='Validation loss')\n    #plt.xticks(np.arange(0, len(epochs)+1))      \n    plt.title('Training and validation loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.show()","223e0e23":"plot_loss(history)","39ad91c0":"model.evaluate(test_ds)","694d5088":"### 4.1.3 Model summary and parameters \nThere are total 112,202 trainable parameters in the model. It is computed as follows:\n- For a hidden layer total parameters  = (input shape * no. of units\/neurons for that layer) + bians neurons. For example layer 1, total parametrs are 784*128 + 128.\n- Similarly, the parameters of all layers are added to get the total parameters.","7949ac15":"### 4.2.3 Training Settings\n- Training batch size is 128\n- The number of epochs are set to 1000. However, to save model training time, early stopping method is implemented. This will monitor the validation loss and if does not reduces in 20 epochs, the training will stop.\n- The learning rate is 0.0001 or 1e-3","d3c55678":"### 4.1.1 Network Configurations\n- Problem Description\n  - This problem is an instance of a `single-label` `multiclass` classification since each data point can be classified into one of the `10 categories`.\n\n- Output layer\n  - The output layer consists of 10 neurons. Therefore, for each input sample, the neural network will output a 10 dimentional vector. \n  - Also, this output layer is using a `Softmax` activation function that will output a probability distribution over the 10 different output classes.\n\n- Input Layer\n  - The flatten layer (first layer) is the input layer of the neural network. Its purpose is to flatten a 28 by 28 pixel image into a array of size 784.\n- Loss Function\n  - The loss function used in this case is `sparse_categorical_crossentropy`. This is because the lables are a list of integers. This function will measure the distance between two probability distribution i.e., the probability distribution output by the network and the true distributions of labels.\n  - Now, this loss function is similar to the `categorical_crossentropy` but the latter expects the labels to be one-hot encoded or categorical encoded. \n\n- Metrics\n  - Accuracy is used as the metric for model evaluation between the train and validation set.\n\n- Optimizer\n  - Nadam optimizer is selected.\n  - Since Adam optimizer merges the ideas of momentum optimization and RMSProp, however, Nadam is slightly faster than Adam. ","4c1e6531":"## 2.5 Numeric range of input and reason for rescaling\nThe numeric range of input is 0-255. Yes, the input needs to be scaled or normalized, otherwise, during the optimization of the network, it might compute large gradients and these large gradients updates can prevent the neural network to converge (Chollet, F., 2018).","c07e5e87":"# 2. Understand the data","d45e45c0":"## 2.2 Unique Labels in Dataset\nThere are 10 unique labels in the dataset were each label is specified by a digit between 0-9. The label description is shown below.","9aebbf2a":"## 4.3 Test Accuracy","6bd9eaab":"## 4.1  Model Setup","9765e9f3":"### Training Loss Changes","924a00b4":"## Display first 25 label images","fe6bc503":"## 3.2 Input Pipelines using tf.data","543152bc":"## 2.3 Number of train and test images","af991a9a":"### 4.1.2 Network architecture and compilation","e70a2830":"# 4. Construct Deep Forward Neural Network","ca6f47f8":"# Import Libraries","e9e72df3":"## 2.1 Data types and shapes of features and labels\n- The data type of features and labels is an 8-bit integer, as shown below in code.\n- Overall, the dataset consists of 70,000 samples (including train and test set) of 28*28 pixel grayscale images of 10 types of clothing. The shape of features is (70000, 28, 28) and shape of label is (70000,).","f883d358":"### 4.2.2 Visualize hidden layer initial weights","ae46cccd":"### 4.2.1 Initialization Method\n- By default `keras` uses `Glorot` initialization with uniform distribution. Since, I have selected activation is `ELU` as the activation function for every layer, it requires the layers weight to be initialized with `He normal distribution` (G\u00e9ron, A. 2019). \n- `ELU` or `Exponential Linear Unit` was selected as the activation function because it outperforms all the variants of `RELU` (G\u00e9ron, A. 2019).","ee7e401c":"### 4.2.4 Model fit","bccb78f2":"# 3. Construct Input Pipeline","0bf76b82":"The below figure the shows the model weights after fitting the model. It can be observed that the weights have changed. This s because, during the training, the model weights were updated during the backpropagation by the optimizer.","8c52b966":"## 3.1 Split train, validation and test sets\nThe train and validation set are created using the sklearn train_test_split in order to get better data representation i.e., balanced set of samples for each label class. A validation set created from a simple slicing method might contain fewer samples for a particular class to be statistically representative of the data at hand. For example val_set = train_set[:10000]","744ec2b4":"# 1. Load Data","1db62572":"## 4.2 Fitting the model","63da1e8e":"## 2.4 Size of each image and memory required to hold train set"}}