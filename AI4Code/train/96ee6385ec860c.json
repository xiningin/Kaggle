{"cell_type":{"fe3b2db0":"code","f5093b6d":"code","df84a196":"code","00b396d2":"code","92dc8ec5":"code","30c1db7c":"code","1156ce2a":"code","a8455aa5":"code","18ff6b6b":"code","9b30fc9a":"code","c911edf5":"code","a23e4a77":"code","2ce7c338":"code","6e92a8ca":"code","ad7c325c":"code","132b9268":"code","57e88347":"code","4a434afc":"code","c99bd6c4":"code","25423425":"code","0e6137b8":"code","fd0c3565":"code","1e9686fb":"code","ffcaf6a0":"code","bee37379":"code","f90785da":"code","add1371c":"code","935f5218":"code","a65a8d5b":"code","bc0978ef":"markdown","6d5ad8df":"markdown","a4953c3e":"markdown","d0a0f10b":"markdown","940f4802":"markdown","8765e426":"markdown","5e7a3027":"markdown","2ebcbbd5":"markdown","4cf5d5e0":"markdown","29c99355":"markdown"},"source":{"fe3b2db0":"shutil.rmtree('.\/train\/')\nshutil.rmtree('.\/val\/')\nshutil.rmtree('.\/test\/')","f5093b6d":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport keras\nimport time\nimport keras.backend as K\nimport os\nimport shutil\nimport random\n\nfrom keras.optimizers.schedules import InverseTimeDecay,ExponentialDecay\nfrom tensorflow.keras.applications.resnet50 import ResNet50\nfrom tensorflow.keras.applications import VGG16\nfrom tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\nfrom tensorflow.keras.utils import Progbar\n\nfrom keras.losses import CategoricalCrossentropy,BinaryCrossentropy\nfrom keras.optimizers import Adam,RMSprop\nfrom keras.applications.densenet import DenseNet121\nfrom keras.applications import InceptionV3\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Model, Sequential\nfrom keras.metrics import CategoricalAccuracy, Precision, Recall\nfrom keras.layers import Dense, Conv2D, BatchNormalization, MaxPool2D, AveragePooling2D, Flatten, Input,GlobalAveragePooling2D\nfrom keras.utils.vis_utils import plot_model\nfrom keras.optimizers.schedules import InverseTimeDecay,ExponentialDecay\n\nfrom sklearn.metrics import classification_report,confusion_matrix,ConfusionMatrixDisplay","df84a196":"path = \"..\/input\/covid19-radiography-database\/COVID-19 Radiography Database\/\"\n\nclassesPath = ['COVID', 'Viral Pneumonia', 'NORMAL']\n\nval_ratio = 0.1\ntest_ratio = 0.05\n\nfor cls in classesPath:\n    os.makedirs('.\/train\/' + cls)\n    os.makedirs('.\/val\/' + cls)\n    os.makedirs('.\/test\/' + cls)\n\n    src = path + cls\n\n    allFileNames = os.listdir(src)\n    np.random.shuffle(allFileNames)\n    train_FileNames, val_FileNames, test_FileNames = np.split(np.array(allFileNames),\n                                                              [int(len(allFileNames) * (1 - (val_ratio + test_ratio))), \n                                                               int(len(allFileNames) * (1 - test_ratio))])\n\n    train_FileNames = [src + '\/' + name for name in train_FileNames.tolist()]\n    val_FileNames = [src + '\/' + name for name in val_FileNames.tolist()]\n    test_FileNames = [src + '\/' + name for name in test_FileNames.tolist()]\n\n    print('Total ', str(cls), len(allFileNames))\n    print('Training', len(train_FileNames))\n    print('Validation', len(val_FileNames))\n    print('Testing', len(test_FileNames))\n    print(\"\\n\")\n\n    for name in train_FileNames:\n        shutil.copy(name, '.\/train\/' + cls)\n\n    for name in val_FileNames:\n        shutil.copy(name, '.\/val\/' + cls)\n\n    for name in test_FileNames:\n        shutil.copy(name, '.\/test\/' + cls)","00b396d2":"path_train = \".\/train\"\npath_val = \".\/val\"\npath_test = \".\/test\"\n\nbatch_size = 128\ninterpolation = \"bicubic\"\nseed = 69\ntarget_size = (100, 100)\nepochs = 20\n\ntrain_data_gen = ImageDataGenerator(rescale = 1.\/99,\n                                    rotation_range = 12,\n                                    zoom_range = 0.1)\n\ntest_data_gen = ImageDataGenerator(rescale = 1.\/99)\n\nds_train = train_data_gen.flow_from_directory(directory = path_train,\n                                              color_mode = \"rgb\",\n                                              batch_size = batch_size,\n                                              target_size = target_size,\n                                              shuffle = True,\n                                              interpolation = interpolation,\n                                              seed = seed)\n\nds_val = test_data_gen.flow_from_directory( directory = path_val,\n                                            color_mode = \"rgb\",\n                                            batch_size = batch_size,\n                                            target_size = target_size,\n                                            shuffle = True,\n                                            interpolation = interpolation,\n                                            seed = seed)   \n\nds_test = test_data_gen.flow_from_directory(directory = path_test,\n                                            color_mode = \"rgb\",\n                                            batch_size = 16,\n                                            target_size = target_size,\n                                            shuffle = False,\n                                            interpolation = interpolation,\n                                            seed = seed)","92dc8ec5":"@tf.function\ndef train_one_step(x_batch, y_batch, metric, model, loss_function):\n    # Gradient for derivative:\n    with tf.GradientTape() as tape:\n        # Prediction for each batch:\n        y_hat = model(x_batch, training = True)\n        # Categorical Cross Entropy Loss:\n        loss = loss_function(y_batch, y_hat)\n    # Get derivative of all trainable weights to loss function:\n    gradients = tape.gradient(loss, model.trainable_weights)\n    # Optimizer = ADAM.\n    optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n    metric.update_state(y_batch, y_hat)\n    return loss\n\n@tf.function\ndef val_one_step(x_batch, y_batch, metric, model, loss_function):\n    y_hat = model(x_batch,training = False)\n    loss = loss_function(y_batch, y_hat)\n    metric.update_state(y_batch, y_hat)\n    return loss","30c1db7c":"def train(model, optimizer, metric, metric_val,epochs):\n    losses = []\n    losses_val = []\n    accuracies = []\n    accuracies_val = []\n    best_loss=999999\n    loss_function = CategoricalCrossentropy()\n    for epoch in range(epochs):\n        print(\"\\nepoch {}\/{}\".format(epoch + 1, epochs))\n        batch_num = 0\n        metric_names = [metric.name, \"loss\"]\n        loss_train=0\n        for batch_num, (x_batch_train, y_batch_train) in enumerate(ds_train):\n            loss_train += train_one_step(x_batch_train, y_batch_train, metric, model, loss_function)\n            values = [('acc',metric.result().numpy()), ('loss', loss_train)]\n            accuracy = metric.result().numpy()\n            accuracies.append(accuracy)\n            losses.append(loss_train)\n            if batch_num == (3108 \/\/ batch_size):\n                break\n\n        print(\"\\nepoch {}\/{}\".format(epoch + 1, epochs))\n        metric_val_names = [metric_val.name, \"val_loss\"]\n        loss_val=0\n        for val_batch_num,(x_batch_val,y_batch_val) in enumerate(ds_val):\n            loss_val += val_one_step(x_batch_val, y_batch_val, metric_val, model, loss_function)\n            values = [('val_acc',metric_val.result().numpy()), ('val_loss', loss_val)]\n            accuracy_val = metric_val.result().numpy()\n            accuracies_val.append(accuracy_val)\n            losses_val.append(loss_val)\n            if val_batch_num == (388\/\/batch_size):\n                break\n\n        if(best_loss>loss_val):\n            best_weights=model.get_weights()\n            model.save_weights(model.name+\".h5\")\n            best_loss=loss_val\n        \n            \n        \n        print(metric.name + \" over epoch \" + str(epoch + 1) + \" = \" + str(accuracy) + \" and \" + metric_val.name + \" = \" + str(accuracy_val))\n        print(\"loss over epoch \" + str(epoch + 1) + \" = \" + str(loss_train.numpy()) + \" and val_loss = \" + str(loss_val.numpy()))\n        \n        metric.reset_states()\n        metric_val.reset_states()\n        \n        \n    model.set_weights(best_weights)\n    return losses, loss_val, accuracies, accuracies_val","1156ce2a":"def evaluate(model, metric):\n    losses = []\n    accuracies = []\n    loss_function = CategoricalCrossentropy()\n    batch_num = 0\n    y_pred=np.array([])\n    y_test=np.array([])\n    metric_names = [metric.name, \"loss\"]\n    for batch_num, (x_batch_test,y_batch_test) in enumerate(ds_test):\n        loss_test = val_one_step(x_batch_test, y_batch_test, metric, model, loss_function)\n        values = [('acc',metric.result().numpy()), ('loss', loss_test)]\n        y_pred=np.append(y_pred,model.predict(x_batch_test))\n        y_test=np.append(y_test,y_batch_test)\n        if batch_num == (196 \/\/ 16):\n            break\n    y_pred = np.reshape(y_pred,(196,3))\n    y_test = np.reshape(y_test,(196,3))\n    y_pred = np.argmax(y_pred,axis=1)\n    y_test = np.argmax(y_test,axis=1)\n    report = classification_report(y_true=y_test,y_pred=y_pred)\n    metric.reset_states()\n    confusion_mat = confusion_matrix(y_pred = y_pred, y_true = y_test)\n    return report, confusion_mat\n","a8455aa5":"## We have tried to use sigmoid and tanh functions but the accurracy wasn't good enough but the relu was the best one.\nmodel_fc_3 = Sequential([\n    Flatten(input_shape = (100, 100, 3)),\n    Dense(128, 'relu'),\n    Dense(256, 'relu'),\n    Dense(128, 'relu'),\n    Dense(64, 'relu'),\n    Dense(3, 'softmax')],name=\"ShallowFullyConnected\"\n)","18ff6b6b":"optimizer = Adam()\nmetric = CategoricalAccuracy(name = \"Accuracy\")\nmetric_val = CategoricalAccuracy(name = \"Val_Accuracy\")\nlosses, accuracy, losses_val, accuracies_val = train(model_fc_3, optimizer, metric, metric_val, 50)","9b30fc9a":"report, confusion_mat = evaluate(model_fc_3, metric)\n\nprint(report)\nConfusionMatrixDisplay(confusion_matrix = confusion_mat).plot()","c911edf5":"model_fc_10 = Sequential([\n    Flatten(input_shape = (100,100,3)),\n    Dense(512, 'relu'),\n    Dense(512, 'relu'),\n    Dense(1024, 'relu'),\n    Dense(1024, 'relu'),\n    Dense(1024, 'relu'),\n    Dense(512, 'relu'),\n    Dense(512, 'relu'),\n    Dense(256, 'relu'),\n    Dense(128, 'relu'),\n    Dense(3, 'softmax')],name=\"DeepFullyConnected\")","a23e4a77":"optimizer = Adam()\nmetric = CategoricalAccuracy(name = \"Accuracy\")\nmetric_val = CategoricalAccuracy(name = \"Val_Accuracy\")\nlosses, accuracy,losses_val, accuracies_val = train(model_fc_10, optimizer, metric,metric_val, 50)","2ce7c338":"report, confusion_mat = evaluate(model_fc_10, metric)\n\nprint(report)\nConfusionMatrixDisplay(confusion_matrix = confusion_mat).plot()","6e92a8ca":"model_LeNet5=Sequential([\n    Conv2D(6,(5,5),input_shape=(100,100,3),activation=\"tanh\",strides=1),\n    AveragePooling2D((2,2),strides=2),\n    Conv2D(16,(5,5),activation=\"tanh\",strides=1),\n    AveragePooling2D((2,2),strides=2),\n    Flatten(),\n    Dense(120,\"tanh\"),\n    Dense(84,\"tanh\"),\n    Dense(3,\"softmax\")],name=\"ShallowConvolutionalNetwork\")","ad7c325c":"optimizer = Adam()\nmetric = CategoricalAccuracy(name = \"Accuracy\")\nmetric_val = CategoricalAccuracy(name = \"Val_Accuracy\")\nlosses, accuracy, losses_val, accuracies_val = train(model_LeNet5, optimizer, metric,metric_val, 50)","132b9268":"report, confusion_mat = evaluate(model_LeNet5, metric)\n\nprint(report)\nConfusionMatrixDisplay(confusion_matrix = confusion_mat).plot()","57e88347":"model_VGG8= Sequential([\n    Conv2D(64,(3,3),input_shape=(100,100,3),activation =\"relu\",strides=1),\n    MaxPool2D((3,3),strides=2),\n    Conv2D(256,(3,3),activation=\"relu\",strides=1),\n    MaxPool2D((3,3),strides=2),\n    Conv2D(512,(3,3),activation=\"relu\",strides=1),\n    MaxPool2D((3,3),strides=2),\n    Conv2D(512,(3,3),activation=\"relu\",strides=1),\n    MaxPool2D((3,3),strides=2),\n    Flatten(),\n    Dense(4096,\"relu\"),\n    Dense(4096,\"relu\"),\n    Dense(3,\"softmax\")],name=\"DeepCovolutionalNetwork\")","4a434afc":"optimizer = Adam()\nmetric = CategoricalAccuracy(name = \"Accuracy\")\nmetric_val = CategoricalAccuracy(name = \"Val_Accuracy\")\nlosses, accuracy,losses_val,accuracies_val = train(model_VGG8, optimizer, metric,metric_val, 50)","c99bd6c4":"report, confusion_mat = evaluate(model_VGG8, metric)\n\nprint(report)\nConfusionMatrixDisplay(confusion_matrix = confusion_mat).plot()","25423425":"model_resnet50 = ResNet50(include_top = False, weights = \"imagenet\", input_shape = (100, 100, 3))\nmodel_resnet50.trainable = False","0e6137b8":"x = model_resnet50.output\nx = GlobalAveragePooling2D()(x)\nx = Dense(512, activation = 'relu')(x)\nx = Dense(256, activation = 'relu')(x)\nx = Dense(128, activation = 'relu')(x)\ny_hat = Dense(3,'softmax')(x)\nmodel_resnet50 = Model(inputs = model_resnet50.input, outputs = y_hat,name=\"ResNet50\")","fd0c3565":"optimizer = Adam()\nmetric = CategoricalAccuracy(name = \"Accuracy\")\nmetric_val = CategoricalAccuracy(name = \"Val_Accuracy\")\nlosses, accuracy, losses_val, accuracies_val = train(model_resnet50, optimizer, metric,metric_val, 50)","1e9686fb":"report, confusion_mat = evaluate(model_resnet50, metric)\n\nprint(report)\nConfusionMatrixDisplay(confusion_matrix = confusion_mat).plot()","ffcaf6a0":"model_DenseNet121 = DenseNet121(include_top = False, weights = \"imagenet\", input_shape = (100, 100, 3))\nmodel_DenseNet121.trainable = False\n\nx = model_DenseNet121.output\nx = GlobalAveragePooling2D()(x)\nx = Dense(512, activation = 'relu')(x)\nx = Dense(128, activation = 'relu')(x)\n\ny_hat = Dense(3,'softmax')(x)\n\nmodel_DenseNet121 = Model(inputs = model_DenseNet121.input, outputs = y_hat,name=\"DenseNet121\")","bee37379":"optimizer = Adam()\nmetric = CategoricalAccuracy(name = \"Accuracy\")\nmetric_val = CategoricalAccuracy(name = \"Val_Accuracy\")\n\nlosses, accuracy, losses_val, accuracies_val = train(model_DenseNet121, optimizer, metric, metric_val, 50)","f90785da":"\nreport, confusion_mat = evaluate(model_DenseNet121, metric)\n\nprint(report)\nConfusionMatrixDisplay(confusion_matrix = confusion_mat).plot()","add1371c":"model_InceptionV3 = InceptionV3(include_top = False, weights = \"imagenet\", input_shape = (100, 100, 3))\nmodel_InceptionV3.trainable = False\n\nx = model_InceptionV3.output\nx = GlobalAveragePooling2D()(x)\nx = Dense(512, activation = 'relu')(x)\nx = Dense(128, activation = 'relu')(x)\n\ny_hat = Dense(3,'softmax')(x)\n\nmodel_InceptionV3 = Model(inputs = model_InceptionV3.input, outputs = y_hat,name=\"InceptionV3\")","935f5218":"optimizer = Adam()\nmetric = CategoricalAccuracy(name = \"Accuracy\")\nmetric_val = CategoricalAccuracy(name = \"Val_Accuracy\")\n\nlosses, accuracy, losses_val, accuracies_val = train(model_InceptionV3, optimizer, metric, metric_val, 50)","a65a8d5b":"report, confusion_mat = evaluate(model_InceptionV3, metric)\n\nprint(report)\nConfusionMatrixDisplay(confusion_matrix = confusion_mat).plot()","bc0978ef":"# 3.2. DenseNet121 Model","6d5ad8df":"# 2.1. Shallow-Convolutional Network Model","a4953c3e":"# 3.3. InceptionV3 Model","d0a0f10b":"# Creating the custom training loop","940f4802":"# 1.2. Deep-Fully Connected Network Model","8765e426":"# 1.1. Shallow-Fully Connected Network Model","5e7a3027":"# 2.2. Deep-Convolutional Network Model","2ebcbbd5":"# Splitting Data \n### Training (85) : Validation (10) : and Testing (5)","4cf5d5e0":"# Loading the data with Image Augmentation using real-time augmentation with Keras Image Data Generator","29c99355":"# 3.1. ResNet50 Model"}}