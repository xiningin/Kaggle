{"cell_type":{"b7e06c7d":"code","c858a20e":"code","2a8a41ca":"code","c2b3a4aa":"code","976bb6c7":"code","867b2377":"code","df71e289":"code","66d0d0fe":"code","8b08147d":"code","51dc3793":"code","e43ce1d2":"code","7bc1df27":"code","66d625be":"code","fdf19be9":"code","aedfec3e":"code","517d0d05":"code","61c0ef98":"code","23391e49":"code","483a8241":"code","be58281d":"code","b30f6344":"code","1b3e602b":"code","23fcf141":"code","3d3a0efd":"code","a4c86734":"code","8eb0d58e":"code","4268c130":"code","0c4ab6c1":"code","259f30b1":"code","9db51cd7":"code","2379d2cf":"code","dcf609b5":"code","74ce71e7":"code","e5fec605":"code","1c9dfefc":"code","c8c27722":"code","4cbf767f":"code","27412ef8":"code","b0d82a59":"code","92d80832":"code","dcb1d2ec":"code","18bb1bf4":"code","0ca200f6":"code","b6f63eda":"code","0cca5848":"markdown","26ce22d2":"markdown","805304e6":"markdown","26784c11":"markdown","1276b4fa":"markdown","9cfe5fc5":"markdown","f379bcad":"markdown","90f02fc2":"markdown","a2ebbe9d":"markdown","70eb00f1":"markdown","cdfa5120":"markdown","e5e50951":"markdown","f3dd3f17":"markdown","5c107460":"markdown","8910c10f":"markdown","278cfc37":"markdown","ffb55bf8":"markdown","4304a63b":"markdown","42f45a7a":"markdown","d7f4407e":"markdown","fb4de69d":"markdown","ec339f09":"markdown","72409378":"markdown","12c35f51":"markdown","12a23e19":"markdown","5021cfea":"markdown","70065908":"markdown","0af53d4b":"markdown","16715178":"markdown","459db339":"markdown"},"source":{"b7e06c7d":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Import train test split from sklearn\nfrom sklearn.model_selection import train_test_split\n\n# Import Standard Scaler\nfrom sklearn.preprocessing import StandardScaler\n\n# Import Gradient Boost Classifier for modeling\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n# Import evaluation metrics\nfrom sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score\n\n# Import scorer\nfrom sklearn.metrics import make_scorer\n\n# Import GridSearchCV\nfrom sklearn.model_selection import GridSearchCV","c858a20e":"import warnings\nwarnings.filterwarnings('ignore')","2a8a41ca":"pd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)","c2b3a4aa":"plt.style.use('ggplot')","976bb6c7":"df = pd.read_csv(\"..\/input\/employee-attrition\/WA_Fn-UseC_-HR-Employee-Attrition.csv\")","867b2377":"# Checking the shape\ndf.shape","df71e289":"# Checking the memory consumed by the data\nprint(f\"The dataset uses {round(df.memory_usage().sum()\/1024 ** 2, 2)} MB of space\")","66d0d0fe":"# Display memory used by each column in the dataset\ndf.memory_usage()","8b08147d":"# Checking top 5 rows\ndf.head()","51dc3793":"# Checking datatypes and Nullability of the columns\ndf.info()","e43ce1d2":"missing_columns = df.columns[ df.isna().any() ]\nprint(f\"The missing columns in the dataset are: {missing_columns}\")","7bc1df27":"sns.countplot(df['Attrition'])\nplt.xlabel('Attrition')\nplt.ylabel('Number of Employees')\nplt.title('Attrition V\/S Number of Employees')\nplt.show()","66d625be":"# Getting numeric features\ndf_num = df.select_dtypes(include='int64')\nprint(f\"Numeric feature shape is {df_num.shape}\")","fdf19be9":"# Getting categorical features\ndf_cat = df.select_dtypes(include='object')\nprint(f\"Categorical feature shape is {df_cat.shape}\")","aedfec3e":"# Drop the target column 'Attrition' from df_cat before encoding\ndf_cat.drop(columns='Attrition', axis=1, inplace=True)","517d0d05":"# One Hot Encoding using pd.get_dummies()\ndf_cat_encoded = pd.get_dummies(df_cat, drop_first=True)\ndf_cat_encoded.shape","61c0ef98":"df.drop(list(df_cat.columns), axis=1, inplace=True)","23391e49":"df.shape","483a8241":"df = pd.concat([df, df_cat_encoded], axis=1)","be58281d":"df.shape","b30f6344":"# Function for converting to 1s and 0s\ndef binary_map(col):\n    return col.map({'Yes':1, 'No':0})\n\n# Applying map function to \"Attrition\"\ndf[['Attrition']] = df[['Attrition']].apply(binary_map)","1b3e602b":"df.head()","23fcf141":"X = df.drop('Attrition', axis=1)\ny = df['Attrition']","3d3a0efd":"X.head()","a4c86734":"y.head()","8eb0d58e":"# Splitting the data into 80-20 ratio\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0, \n                                                    stratify=y, shuffle=True)","4268c130":"print(\"Shape of X Train: \",X_train.shape)\nprint(\"Shape of X Test: \",X_test.shape)\nprint(\"Shape of y Train: \",y_train.shape)\nprint(\"Shape of y Test: \",y_test.shape)","0c4ab6c1":"#Using Gradient Boosting to predict 'Attrition' and create the Trees to identify important features\ngbm = GradientBoostingClassifier()\n\n# Fitting the model\ngbm.fit(X_train, y_train)","259f30b1":"# Predict default model on train\ny_train_pred = gbm.predict(X_train)\n\n# Train Accuracy\nprint(f\"Train Accuracy is: {round(accuracy_score(y_train, y_train_pred), 4)}\")","9db51cd7":"# Predict default model on test\ny_test_pred = gbm.predict(X_test)\n\n# Train Accuracy\nprint(f\"Test Accuracy is: {round(accuracy_score(y_test, y_test_pred), 4)}\")","2379d2cf":"y_train_score = gbm.predict_proba(X_train)[:,1]\nprint(f\"AUC on train data: {roc_auc_score(y_train, y_train_score)}\")","dcf609b5":"y_test_score = gbm.predict_proba(X_test)[:,1]\nprint(f\"AUC on test data: {roc_auc_score(y_test, y_test_score)}\")","74ce71e7":"# Initialize model\ngbm_model = GradientBoostingClassifier()\n\n# Default Run with default Hyperparameters\nparameters = {\n'learning_rate': [0.1], 'max_depth': [3], 'n_estimators': [100], 'min_samples_leaf': [1],\n'min_samples_split': [2], 'subsample': [1.0]\n}\n\n# Make a Scorer from a performance metric\nscorer = make_scorer(roc_auc_score, greater_is_better=True,\n                             needs_proba=True, needs_threshold=False)\n\ngridsearch_gbm = GridSearchCV(estimator=gbm_model, param_grid=parameters, scoring=scorer, \n                              n_jobs=-1, cv=5, refit=True)\n\ngridsearch_gbm.fit(X_train, y_train)","e5fec605":"# Best Parameters\nprint(gridsearch_gbm.best_params_)\nprint(\"*\"*150)\n\n# Best Score\nprint(gridsearch_gbm.best_score_)\nprint(\"*\"*150)\n\n# Best Estimator\nprint(gridsearch_gbm.best_estimator_)\nprint(\"*\"*150)","1c9dfefc":"# 1st Run with default Hyperparameters\nparameters = {\n'learning_rate': [0.1, 0.2, 0.3, 0.4, 0.5], \n'max_depth': [2, 4, 6, 8, 10], \n'n_estimators': [50, 100, 150, 200, 300, 500]\n}\n\n# Make a Scorer from a performance metric\nscorer = make_scorer(roc_auc_score, greater_is_better=True,\n                             needs_proba=True, needs_threshold=False)\n\ngridsearch_gbm = GridSearchCV(estimator=gbm_model, param_grid=parameters, scoring=scorer, \n                              n_jobs=-1, cv=5, refit=True)\n\ngridsearch_gbm.fit(X_train, y_train)","c8c27722":"# Best Parameters\nprint(gridsearch_gbm.best_params_)\nprint(\"*\"*150)\n\n# Best Score\nprint(gridsearch_gbm.best_score_)\nprint(\"*\"*150)\n\n# Best Estimator\nprint(gridsearch_gbm.best_estimator_)\nprint(\"*\"*150)","4cbf767f":"# 2nd Run with default Hyperparameters\nparameters = {\n'learning_rate': [0.0001, 0.001, 0.01, 0.05, 0.099, 0.1], \n'max_depth': [2], \n'n_estimators': [150, 200, 300, 500]\n}\n\n# Make a Scorer from a performance metric\nscorer = make_scorer(roc_auc_score, greater_is_better=True,\n                             needs_proba=True, needs_threshold=False)\n\ngridsearch_gbm = GridSearchCV(estimator=gbm_model, param_grid=parameters, scoring=scorer, \n                              n_jobs=-1, cv=5, refit=True)\n\ngridsearch_gbm.fit(X_train, y_train)","27412ef8":"# Best Parameters\nprint(gridsearch_gbm.best_params_)\nprint(\"*\"*150)\n\n# Best Score\nprint(gridsearch_gbm.best_score_)\nprint(\"*\"*150)\n\n# Best Estimator\nprint(gridsearch_gbm.best_estimator_)\nprint(\"*\"*150)","b0d82a59":"gbm = GradientBoostingClassifier(learning_rate=0.1, max_depth=2, n_estimators=150)\n\n# Fitting the model\ngbm.fit(X_train, y_train)","92d80832":"# Predict final model on train\ny_train_pred = gbm.predict(X_train)\n\n# Train Accuracy\nprint(f\"Train Accuracy is: {round(accuracy_score(y_train, y_train_pred), 4)}\")","dcb1d2ec":"# Predict final model on test\ny_test_pred = gbm.predict(X_test)\n\n# Train Accuracy\nprint(f\"Test Accuracy is: {round(accuracy_score(y_test, y_test_pred), 4)}\")","18bb1bf4":"plt.figure(figsize=[15,4])\nsns.scatterplot(x=X_train.columns.values, y=gbm.feature_importances_, palette=\"deep\")\nplt.title(\"Model Feature Importance\")\nplt.ylabel(\"Feature Importance\")\nplt.xticks(rotation=90)\nplt.show()","0ca200f6":"# Creating dictionary for feature importance\ndict_importance = {'Columns': X_train.columns.values, 'Feature Importance': gbm.feature_importances_}","b6f63eda":"# Sorting the features by their order of importance\ndf_importance = pd.DataFrame(data=dict_importance)\ndf_importance.sort_values(by='Feature Importance', ascending=False)","0cca5848":"## Divide data into X and y for building the model","26ce22d2":"## Dropping the original categorical columns from the dataframe","805304e6":"<a href=\"https:\/\/colab.research.google.com\/github\/Venkat-dev-cloud\/Employee-Attrition\/blob\/main\/Attrition_Prediction_using_Gradient_Boost_follow_along.ipynb\" target=\"_parent\"><img src=\"https:\/\/colab.research.google.com\/assets\/colab-badge.svg\" alt=\"Open In Colab\"\/><\/a>","26784c11":"There are no columns with missing values in the dataframe","1276b4fa":"# Modeling","9cfe5fc5":"The model is definitely overfitting as we can see the vast difference between the train and test accuracy and AUC scores","f379bcad":"## Identify columns with missing information","90f02fc2":"# Expected Outcome\nTo understand and determine how these factors relate to workforce attrition.","a2ebbe9d":"## 2nd run for best hyper parameters","70eb00f1":"# Important Features","cdfa5120":"## Extracting Numeric and Categorical features for separate treatment","e5e50951":"## Train test split","f3dd3f17":"## Ignore Warnings","5c107460":"## See all rows and columns","8910c10f":"## Set Plot display style","278cfc37":"# Feature Engineering","ffb55bf8":"## Importing Data","4304a63b":"# Final Model","42f45a7a":"## Checking data imbalance for the target column 'Attrition'","d7f4407e":"# Problem Statement\nThe issue of keeping one's employees happy and satisfied is a perennial and age-old challenge. If an employee you have invested so much time and money leaves for \"greener pastures\", then this would mean that you would have to spend even more time and money to hire somebody else.\n\nThis project is based on a hypothetical dataset downloaded from IBM HR Analytics Employee Attrition & Performance. It has 1,470 data points (rows) and 35 features (columns) describing each employee\u2019s background and characteristics; and labelled (supervised learning) with whether they are still in the company or whether they have gone to work somewhere else.","fb4de69d":"## Merge `df` and `df_cat_encoded` into `df` dataframe","ec339f09":"## Converting binary categorical variables to 1 and 0","72409378":"# Understanding and Exploring the data","12c35f51":"## Tuning with Default Model using GridSearchCV","12a23e19":"**Default modeling parameters**\n\n- learning_rate=0.1\n- max_depth=3\n- n_estimators=100\n- min_samples_leaf=1\n- min_samples_split=2\n- subsample=1.0","5021cfea":"## 1st run for best hyperparameters","70065908":"# Kaggle link and dataset\nhttps:\/\/www.kaggle.com\/patelprashant\/employee-attrition","0af53d4b":"## Importing required libraries","16715178":"## Perform One Hot Encoding for all categorical features","459db339":"# HyperParameter Tuning"}}