{"cell_type":{"291c8767":"code","868e233e":"code","2c66ee0e":"code","518f7e3c":"code","25e9c358":"code","80c8581f":"code","cf8793f5":"code","8990fccb":"code","99ebab92":"code","6af5852c":"code","56265011":"code","cddd2f5e":"code","9d47c7b8":"markdown","01a6c0f0":"markdown","617daad4":"markdown","783946df":"markdown","50c01f5f":"markdown","e0f3c4c3":"markdown","70e08579":"markdown","f72cd2ce":"markdown","1e38d621":"markdown","7875700f":"markdown","dc23265e":"markdown","53e8d998":"markdown","01785836":"markdown"},"source":{"291c8767":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom pandas import read_csv\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder\nimport matplotlib\nmatplotlib.use('Agg')\nfrom matplotlib import pyplot","868e233e":"data = read_csv('..\/input\/train.csv')\ndataset = data.values","2c66ee0e":"X = dataset[:,0:94]\ny = dataset[:,94]","518f7e3c":"label_encoded_y = LabelEncoder().fit_transform(y)\nmodel = XGBClassifier()","25e9c358":"subsample = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 1.0]\nparam_grid = dict(subsample=subsample)\nkfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)\ngrid_search = GridSearchCV(model, param_grid, scoring=\"neg_log_loss\", n_jobs=-1, cv=kfold)\ngrid_result = grid_search.fit(X, label_encoded_y)","80c8581f":"print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n\tprint(\"%f (%f) with: %r\" % (mean, stdev, param))","cf8793f5":"pyplot.errorbar(subsample, means, yerr=stds)\npyplot.title(\"XGBoost subsample vs Log Loss\")\npyplot.xlabel('subsample')\npyplot.ylabel('Log Loss')","8990fccb":"colsample_bytree = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 1.0]\nparam_grid = dict(colsample_bytree=colsample_bytree)\nkfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)\ngrid_search = GridSearchCV(model, param_grid, scoring=\"neg_log_loss\", n_jobs=-1, cv=kfold)\ngrid_result = grid_search.fit(X, label_encoded_y)","99ebab92":"print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n\tprint(\"%f (%f) with: %r\" % (mean, stdev, param))","6af5852c":"pyplot.errorbar(colsample_bytree, means, yerr=stds)\npyplot.title(\"XGBoost colsample_bytree vs Log Loss\")\npyplot.xlabel('colsample_bytree')\npyplot.ylabel('Log Loss')","56265011":"# grid search\nmodel = XGBClassifier()\ncolsample_bylevel = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 1.0]\nparam_grid = dict(colsample_bylevel=colsample_bylevel)\nkfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)\ngrid_search = GridSearchCV(model, param_grid, scoring=\"neg_log_loss\", n_jobs=-1, cv=kfold)\ngrid_result = grid_search.fit(X, label_encoded_y)\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n\tprint(\"%f (%f) with: %r\" % (mean, stdev, param))","cddd2f5e":"pyplot.errorbar(colsample_bylevel, means, yerr=stds)\npyplot.title(\"XGBoost colsample_bylevel vs Log Loss\")\npyplot.xlabel('colsample_bylevel')\npyplot.ylabel('Log Loss')","9d47c7b8":"# If you like this kernel Greatly Appreciate to UPVOTE .\n\n\n\n# Stochastic Gradient Boosting with XGBoost \n\nA simple technique for ensembling decision trees involves training trees on subsamples of the training dataset.\n\nSubsets of the the rows in the training data can be taken to train individual trees called bagging. When subsets of rows of the training data are also taken when calculating each split point, this is called random forest.\n\nThese techniques can also be used in the gradient tree boosting model in a technique called stochastic gradient boosting.\n\nIn this kernel I will be demonstrating stochastic gradient boosting and how to tune the sampling parameters using XGBoost with scikit-learn in Python.\n\nAfter reading this kernel we will get to know the following point in detail:\n\n- The rationale behind training trees on subsamples of data and how this can be used in gradient boosting.\n- How to tune row-based subsampling in XGBoost using scikit-learn?\n- How to tune column-based subsampling by both tree and split-point in XGBoost?\n\n## What is Stochastic Gradient Boosting? Let's understand the concept in detail\n\nGradient boosting is a greedy procedure.\n\nNew decision trees are added to the model to correct the residual error of the existing model.\n\nEach decision tree is created using a greedy search procedure to select split points that best minimize an objective function. This can result in trees that use the same attributes and even the same split points again and again.\n\nBagging is a technique where a collection of decision trees are created, each from a different random subset of rows from the training data. The effect is that better performance is achieved from the ensemble of trees because the randomness in the sample allows slightly different trees to be created, adding variance to the ensembled predictions.\n\nRandom forest takes this one step further, by allowing the features (columns) to be subsampled when choosing split points, adding further variance to the ensemble of trees.\n\nThese same techniques can be used in the construction of decision trees in gradient boosting in a variation called stochastic gradient boosting.\n\nIt is common to use aggressive sub-samples of the training data such as 40% to 80%.\n\n# Overview\n\nIn this kernel we are going to look at the effect of different subsampling techniques in gradient boosting.\n\nWe will tune three different flavors of stochastic gradient boosting supported by the XGBoost library in Python, specifically:\n\n**1.  Subsampling of rows in the dataset when creating each tree.**\n\n**2. Subsampling of columns in the dataset when creating each tree.**\n\n**3. Subsampling of columns for each split in the dataset when creating each tree.**\n\n## Dataset\n\nWe will use the **Otto Group Product Classification Challenge** dataset available in Kaggle which is available for free.\nThis dataset describes the 93 obfuscated details of more than 61,000 products grouped into 10 product categories (e.g. fashion, electronics, etc.). Input attributes are counts of different events of some kind.The Otto Group is one of the world\u2019s biggest e-commerce companies, with subsidiaries in more than 20 countries, including Crate & Barrel (USA), Otto.de (Germany) and 3 Suisses (France) selling millions of products worldwide every day, with several thousand products being added to our product line.\n\n#### Data fields\n- id - an anonymous id unique to a product\n- feat_1, feat_2, ..., feat_93 - the various features of a product\n- target - the class of a product\n\n## Problem Description\n\nThe goal is to make predictions for new products as an array of probabilities for each of the 10 categories and models are evaluated using multiclass logarithmic loss (also called cross entropy).\n\n## Solution Approach\n\nAs mentioned above let us look at the appraoches one by one.","01a6c0f0":"Encode string class values as integers","617daad4":"### 1.  Subsampling of rows in the dataset when creating each tree.\n\nRow subsampling involves selecting a random sample of the training dataset without replacement.\n\nRow subsampling can be specified in the scikit-learn wrapper of the XGBoost class in the subsample parameter. The default is 1.0 which is no sub-sampling.\n\nWe can use the grid search capability built into scikit-learn to evaluate the effect of different subsample values from 0.1 to 1.0 on the Otto dataset.\n\nThere are 9 variations of subsample and each model will be evaluated using 10-fold cross validation, meaning that 9\u00d710 or 90 models need to be trained and tested.\n\nPerform K Fold cross validation using the GridSearchCV ","783946df":"Summarize results","50c01f5f":"We can see that the best performance for the model was **colsample_bytree**=1.0. This suggests that subsampling columns on this problem does not add value.","e0f3c4c3":"Split the data into X and y","70e08579":"We can see that indeed 30% has the best mean performance, but we can also see that as the ratio increased, the variance in performance grows quite markedly.\n\nIt is interesting to note that the mean performance of all subsample values outperforms the mean performance without subsampling (**subsample**=1.0).\n\n**2. Subsampling of columns in the dataset when creating each tree.**\n\nWe can also create a random sample of the features (or columns) to use prior to creating each decision tree in the boosted model.\n\nIn the XGBoost wrapper for scikit-learn, this is controlled by the **colsample_bytree** parameter.\n\nThe default value is 1.0 meaning that all columns are used in each decision tree. We can evaluate values for **colsample_bytree** between 0.1 and 1.0 incrementing by 0.1.\n\nPerform K Fold cross validation using the GridSearchCV","f72cd2ce":"Plotting the results, we can see the performance of the model plateau (at least at this scale) with values between 0.5 to 1.0.\n\n\n\n**3. Subsampling of columns for each split in the dataset when creating each tree.**\n\n\nRather than subsample the columns once for each tree, we can subsample them at each split in the decision tree. In principle, this is the approach used in random forest.\n\nWe can set the size of the sample of columns used at each split in the colsample_bylevel parameter in the XGBoost wrapper classes for scikit-learn.\n\nAs before, we will vary the ratio from 10% to the default of 100%.","1e38d621":"We can see that the best results achieved were 0.3, or training trees using a 30% sample of the training dataset.\n\nWe can plot these mean and standard deviation log loss values to get a better understanding of how performance varies with the subsample value.","7875700f":"We can see that the best results were achieved by setting colsample_bylevel to 70%, resulting in an (inverted) log loss of -0.001062, which is better than -0.001239 seen when setting the per-tree column sampling to 100%.\n\nThis suggest to not give up on column subsampling if per-tree results suggest using 100% of columns, and to instead try per-split column subsampling.\n\nWe can plot the performance of each colsample_bylevel variation. The results show relatively low variance and seemingly a plateau in performance after a value of 0.3 at this scale.","dc23265e":"Load data","53e8d998":"# Summary\n\nIn this kernel we have discovered what is stochastic gradient boosting with XGBoost in Python.\n\nSpecifically, we learned:\n\n- About stochastic boosting and how you can subsample your training data to improve the generalization of your model\n- How to tune row subsampling with XGBoost in Python and scikit-learn.\n- How to tune column subsampling with XGBoost both per-tree and per-split.\n\n# If you like this kernel Greatly Appreciate to UPVOTE .","01785836":"Summarize results"}}