{"cell_type":{"70550cd1":"code","f52a6183":"code","064777ab":"code","c5fce6ac":"code","34447f9d":"markdown","5fffdf1d":"markdown","271322d6":"markdown","6a30c605":"markdown"},"source":{"70550cd1":"import numpy as np\nfrom keras.datasets import mnist\nfrom keras.utils import np_utils\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import SGD\nfrom keras.regularizers import l2 #l1","f52a6183":"(x_train, y_train), (x_test, y_test) = mnist.load_data()\nprint('x_shape:',x_train.shape)\nprint('y_shape:',y_train.shape)\n\nx_train = x_train.reshape(x_train.shape[0],-1)\/255.0\nx_test = x_test.reshape(x_test.shape[0],-1)\/255.0\n\ny_train = np_utils.to_categorical(y_train, num_classes=10)\ny_test = np_utils.to_categorical(y_test, num_classes=10)\n\n#  coefficient \u03bb = 0.0003\nmodel = Sequential([\n    Dense(units=200,input_dim=784,bias_initializer='one',activation='tanh',kernel_regularizer=l2(0.0003)),\n    Dense(units=100,bias_initializer='one',activation='tanh',kernel_regularizer=l2(0.0003)),\n    Dense(units=10,bias_initializer='one',activation='softmax',kernel_regularizer=l2(0.0003)),\n])\n\nsgd=SGD(lr=0.2)\n\nmodel.compile(optimizer='sgd',loss='categorical_crossentropy',metrics='accuracy')\nmodel.fit(x_train, y_train, batch_size=32, epochs=10)\n\nloss, accuracy = model.evaluate(x_test, y_test)\nprint('\\ntest loss', loss)\nprint('test accuracy', accuracy)\n\nloss, accuracy = model.evaluate(x_train, y_train)\nprint('\\ntest loss', loss)\nprint('test accuracy', accuracy)","064777ab":"from keras.regularizers import l1","c5fce6ac":"(x_train, y_train), (x_test, y_test) = mnist.load_data()\nprint('x_shape:',x_train.shape)\nprint('y_shape:',y_train.shape)\n\nx_train = x_train.reshape(x_train.shape[0],-1)\/255.0\nx_test = x_test.reshape(x_test.shape[0],-1)\/255.0\n\ny_train = np_utils.to_categorical(y_train, num_classes=10)\ny_test = np_utils.to_categorical(y_test, num_classes=10)\n\n#  coefficient \u03bb = 0.0003\nmodel = Sequential([\n    Dense(units=200,input_dim=784,bias_initializer='one',activation='tanh',kernel_regularizer=l1(0.0003)),\n    Dense(units=100,bias_initializer='one',activation='tanh',kernel_regularizer=l1(0.0003)),\n    Dense(units=10,bias_initializer='one',activation='softmax',kernel_regularizer=l1(0.0003)),\n])\n\nsgd=SGD(lr=0.2)\n\nmodel.compile(optimizer='sgd',loss='categorical_crossentropy',metrics='accuracy')\nmodel.fit(x_train, y_train, batch_size=32, epochs=10)\n\nloss, accuracy = model.evaluate(x_test, y_test)\nprint('\\ntest loss', loss)\nprint('test accuracy', accuracy)\n\nloss, accuracy = model.evaluate(x_train, y_train)\nprint('\\ntest loss', loss)\nprint('test accuracy', accuracy)","34447f9d":"By adding regularized, we overcome the over fitting effectively.","5fffdf1d":"Let's see what L1 will happen","271322d6":"L2 & L1:![image.png](attachment:image.png)\n\n\nLASSO lost function = L1\nRidge Regression lost function = L2","6a30c605":"After using L1, although it can effectively reduce the over fitting phenomenon, the accuracy of the model also decreases"}}