{"cell_type":{"7a338d9a":"code","58b4aaab":"code","ab8ea22b":"code","d7423bc4":"code","18cad8da":"code","2a275fda":"code","4254af1a":"code","e6f3d8ea":"code","71f0915b":"code","e3e017b1":"code","f67e7fb6":"code","f394873f":"code","a239cb34":"code","2320515f":"code","ee0e9ede":"code","ef7319ce":"code","35054ea8":"code","359a845d":"code","af94d6af":"code","2df8f608":"code","7cc55690":"code","528b7c1b":"code","242900dd":"code","702c8f74":"markdown","a31127e9":"markdown","f98f7770":"markdown","d3b21be5":"markdown","ca355d9d":"markdown","46821d55":"markdown","a3bbc296":"markdown","c30f5c47":"markdown","a3e11715":"markdown","2c1f8c2b":"markdown"},"source":{"7a338d9a":"%%capture\n!pip install --upgrade wandb","58b4aaab":"import wandb\nfrom wandb.keras import WandbCallback\nwandb.login()","ab8ea22b":"wandb.init(entity=\"authors\", project=\"kaggle_license\")","d7423bc4":"%%capture\nimport numpy as np\nimport pandas as pd\n\nimport tensorflow as tf\nfrom tensorflow import keras\n\nimport matplotlib.pyplot as plt\n\nimport os\nimport cv2","18cad8da":"# Check the version of tf\nprint(tf.__version__)","2a275fda":"df = pd.read_json(\"..\/input\/vehicle-number-plate-detection\/Indian_Number_plates.json\", lines=True)\ndf.head()","4254af1a":"df.columns","e6f3d8ea":"%%capture\n# Make a directory where the downloaded cars are kept\n%mkdir Cars\n\ndataset = dict()\ndataset[\"image_name\"] = list()\ndataset[\"top_x\"] = list()\ndataset[\"top_y\"] = list()\ndataset[\"bottom_x\"] = list()\ndataset[\"bottom_y\"] = list()\n\ncounter = 0\nfor index, row in df.iterrows():\n    path = tf.keras.utils.get_file('\/kaggle\/working\/Cars\/car{}.jpg'.format(counter),\n                            row[\"content\"])\n    \n    dataset[\"image_name\"].append(path)\n    \n    data_points = row[\"annotation\"]\n    \n    dataset[\"top_x\"].append(data_points[0][\"points\"][0][\"x\"])\n    dataset[\"top_y\"].append(data_points[0][\"points\"][0][\"y\"])\n    dataset[\"bottom_x\"].append(data_points[0][\"points\"][1][\"x\"])\n    dataset[\"bottom_y\"].append(data_points[0][\"points\"][1][\"y\"])\n    \n    counter += 1","71f0915b":"df_store = pd.DataFrame(dataset)\ndf_store.head()","e3e017b1":"def load_img(path):\n    img = tf.io.read_file(path)\n    img = tf.image.decode_image(img, channels=3, dtype=tf.float32)\n    img = tf.image.resize(img, (224,224))\n    if len(img.shape) == 4:\n        img = tf.squeeze(img,0)\n    img = tf.expand_dims(img, axis=0)\n    return img","f67e7fb6":"# For traininig\nimgs = []\nlabels = []\nfor index, row in df_store[:int(0.9*len(df_store))].iterrows():\n    imgs.append(load_img(row['image_name']))\n    labels.append(tf.constant([[row['top_x'],row['top_y'],row['bottom_x'],row['bottom_y']]]))\ntrain_images = tf.concat(imgs,axis=0)\ntrain_labels = tf.concat(labels,axis=0)\n\n# For testing\nimgs = []\nlabels = []\nfor index, row in df_store[int(0.9*len(df_store)):].iterrows():\n    imgs.append(load_img(row['image_name']))\n    labels.append(tf.constant([[row['top_x'],row['top_y'],row['bottom_x'],row['bottom_y']]]))\ntest_images = tf.concat(imgs,axis=0)\ntest_labels = tf.concat(labels,axis=0)","f394873f":"print('train_images: {}'.format(train_images.shape))\nprint('train_labels: {}'.format(train_labels.shape))\nprint('test_images: {}'.format(test_images.shape))\nprint('test_labels: {}'.format(test_labels.shape))","a239cb34":"def show_img_bbox(img, label):\n    img = img.numpy()\n    y_hat = label.numpy()*224\n    xt, yt = int(y_hat[0]), int(y_hat[1])\n    xb, yb = int(y_hat[2]), int(y_hat[3])\n    image = cv2.rectangle(img, (xt, yt), (xb, yb), (0, 0, 255), 3)\n    plt.imshow(image)\n    plt.show()\n\n# Use the function\nshow_img_bbox(train_images[13], train_labels[13])","2320515f":"train_ds = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).shuffle(buffer_size=64).batch(32)\ntest_ds = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).shuffle(buffer_size=64).batch(32)","ee0e9ede":"for i,l in train_ds.take(1):\n    show_img_bbox(i[0], l[0])\n    show_img_bbox(i[2], l[2])","ef7319ce":"tf.keras.backend.clear_session()\n\ni = tf.keras.layers.Input(shape=(224, 224, 3))\nx = tf.keras.layers.Conv2D(64, (5,5), activation='relu')(i)\nx = tf.keras.layers.MaxPool2D()(x)\nx = tf.keras.layers.Conv2D(128, (5,5), activation='relu')(x)\nx = tf.keras.layers.MaxPool2D()(x)\nx = tf.keras.layers.Conv2D(256, (7,7), activation='relu')(x)\nx = tf.keras.layers.MaxPool2D()(x)\nx = tf.keras.layers.Conv2D(512, (7,7), activation='relu')(x)\nx = tf.keras.layers.GlobalAveragePooling2D()(x)\nx = tf.keras.layers.Dense(256,activation='relu')(x)\nx = tf.keras.layers.Dense(128,activation='relu')(x)\nx = tf.keras.layers.Dense(64,activation='relu')(x)\no = tf.keras.layers.Dense(4,activation='sigmoid')(x)\n\nmodel = tf.keras.Model(inputs=[i], outputs=[o])\n\nmodel.summary()","35054ea8":"model.compile(optimizer='adam', loss=\"binary_crossentropy\", metrics=['accuracy'])\ncallback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)","359a845d":"history = model.fit(train_ds,\n                    validation_data=test_ds,\n                    callbacks=[callback,\n                               WandbCallback()\n                              ],\n                    epochs=50)","af94d6af":"model.evaluate(test_ds)","2df8f608":"for i,l in test_ds.take(1):\n    pred = tf.constant(model.predict(i))\n    for j in range(10):\n        show_img_bbox(i[j], pred[j])","7cc55690":"df_store.head()","528b7c1b":"def change_name(string):\n    name = string.split('\/')[-1]\n    name = '..\/input\/license\/Cars\/'+name\n    return name\ndf_store['image_name'] = df_store['image_name'].apply(change_name)\ndf_store.head()","242900dd":"df_store.to_csv('\/kaggle\/working\/license.csv')","702c8f74":"The columns of the dataframe are:\n1. content: The urls to the vehicle images\n2. annotation: The license plate co-ordinates\n3. extras: NaN","a31127e9":"We use the `tf.keras.utils.get_file` method to download the images. We also create a dictionary called `dataset` that has the following keys:\n1. image_name: The path where the image is downloaded; str\n2. top_x: The % value of the topx bbox point; float\n3. top_y: The % value of the topy bbox point; float\n4. bottom_x: The % value of the bottomx bbox point; float\n5. bottom_y: The % value of the bottomy bbox point; float","f98f7770":"A util function to show the bbox and the image.","d3b21be5":"This is the code which is used to load the images in memory. The training images and testing images are transformed in tensors and then are used to build `tf.data.Dataset`","ca355d9d":"## Image Utils\nThe load_img function is a util function that helps in reading, decoding and resing the image.","46821d55":"## Model\nI have tried keeping this as simple as possible with leraning from scratch","a3bbc296":"Let's visually check the datasets formation","c30f5c47":"## Data\nThe data is provided in the form of a json. The important fields in the json are `content` and the `annotation`. The `content` field consists of the urls to the images. The `annotation` field consists of all the metadata of the image along with the co-ordinates of the bounding boxes.","a3e11715":"# Object Detection\nIn this kernel, we will dive into the problem of object detection. Here we will provide images of cars and try to put a bounding box around the license plates. This is a very simple task, as there is only one kind of object (license plate). The idea is to use a CNN and then project the flattened layer to 4 output values. These 4 values will be the co-ordinates of the license plates.\n\nThis version of the code will have [wandb](https:\/\/wandb.ai) integration along with it. This is a tool for ML and DL developers to quiclky log their necessary metrics and to reproduce and convey results.\n\n## Imports\nThe following packages are used:\n1. numpy\n2. pandas\n3. tensorflow\n4. matplotlib\n5. os\n6. cv2\n7. wandb","2c1f8c2b":"## Store the dataframe"}}