{"cell_type":{"dc5adeb5":"code","41d33a4b":"code","dd8d9d3c":"code","6d16ea70":"code","5dcc244c":"code","88744f08":"code","ebfe677a":"code","016b5425":"code","a53381e1":"code","6e36ef60":"code","3e240258":"code","4c62cde4":"code","205b5862":"code","c1eacb64":"code","fc95655c":"code","692622e6":"code","bb5eef8f":"code","07f5b6c4":"code","14a7ec83":"code","308d7783":"code","cc5f9d8c":"code","37cee501":"code","ec20dc62":"code","b56d7f0b":"code","5f86147c":"code","889ff899":"code","ef24248b":"code","717b0562":"code","a340c35f":"code","95fc3615":"code","e6ae6d08":"code","2f295cb0":"code","5b6b4a20":"code","fe49acfb":"code","0c1f059b":"code","c9c5e9a2":"code","3f278048":"code","c0d2c727":"code","aa04799f":"code","cc6e9a1f":"code","f7288ffc":"code","b0866df0":"code","0f7c6741":"code","b897a0a1":"code","a7eeb9c2":"code","07b5b6dd":"code","b6d6356f":"markdown","6a88ad27":"markdown","ea826de6":"markdown","074004ce":"markdown","50e587cc":"markdown","aeb8330e":"markdown","74eddbfa":"markdown","e931f1dc":"markdown","33cf6715":"markdown","f88a2893":"markdown","c733bed0":"markdown","8d28c90e":"markdown","fa285d52":"markdown","d0e05576":"markdown","06f9a305":"markdown","c5a38fc6":"markdown","83ea25dc":"markdown","583408ad":"markdown","e4924f1a":"markdown","757d9093":"markdown","658301f6":"markdown","c64c2008":"markdown","0bcb865c":"markdown","94590c46":"markdown","1994bd3b":"markdown","379d41cd":"markdown","830f5bf3":"markdown","10781a50":"markdown","2681aa86":"markdown","d2dc624c":"markdown","adc659b3":"markdown","86e2262e":"markdown","d463b4e2":"markdown","6e8018e6":"markdown","36ca1d70":"markdown","b138880b":"markdown","271716a4":"markdown","b13483a4":"markdown","afcbc028":"markdown","0905c82e":"markdown","1031cc66":"markdown","e5ce7f65":"markdown","5a8b1c25":"markdown","aebab617":"markdown","82130d58":"markdown"},"source":{"dc5adeb5":"import pandas as pd\nimport numpy as np\nprint(pd.__version__)","41d33a4b":"household = pd.read_csv(\"\/kaggle\/input\/algoritma-academy-data-analysis\/household.csv\")\nhousehold.head()","dd8d9d3c":"household.dtypes","6d16ea70":"household['purchase_time'] = pd.to_datetime(household['purchase_time'])\nhousehold.head()","5dcc244c":"date = pd.Series(['30-01-2020', '31-01-2020', '01-02-2020','02-02-2020'])\ndate","88744f08":"pd.to_datetime(date)","ebfe677a":"# Solution 1\npd.to_datetime(date, format=\"%d-%m-%Y\")\n\n\n# Solution 2\npd.to_datetime(date, dayfirst=True)","016b5425":"## Your code below\n\n## -- Solution code","a53381e1":"## Your code below\n\n\n## -- Solution code","6e36ef60":"# Reference answer for Knowledge Check\nhousehold = pd.read_csv(\"\/kaggle\/input\/algoritma-academy-data-analysis\/household.csv\", index_col=1, parse_dates=['purchase_time'])\nhousehold.drop(['receipt_id', 'yearmonth', 'sub_category'], axis=1, inplace=True)\nhousehold['weekday'] = household['purchase_time'].dt.weekday_name\npd.crosstab(index=household['weekday'], columns='count')","3e240258":"household.dtypes","4c62cde4":"household['weekday'] = household['weekday'].astype('category', errors='raise')\nhousehold.dtypes","205b5862":"## Your code below\n\n\n## -- Solution code","c1eacb64":"household.select_dtypes(exclude='object').head()","fc95655c":"pd.concat([\n    household.select_dtypes(exclude='object'),\n    household.select_dtypes(include='object').apply(\n        pd.Series.astype, dtype='category'\n    )\n], axis=1).dtypes","692622e6":"objectcols = household.select_dtypes(include='object')\nhousehold[objectcols.columns] = objectcols.apply(lambda x: x.astype('category'))\nhousehold.head()","bb5eef8f":"household.dtypes","07f5b6c4":"household = pd.read_csv(\"\/kaggle\/input\/algoritma-academy-data-analysis\/household.csv\")\nhousehold.shape","14a7ec83":"## Your code below\n\n\n## -- Solution code","308d7783":"household.sub_category.value_counts(sort=False, ascending=True)","cc5f9d8c":"## Your code below\n\n\n## -- Solution code","37cee501":"pd.crosstab(index=household['sub_category'], columns=\"count\")","ec20dc62":"pd.crosstab(index=household['sub_category'], columns=\"count\", normalize='columns')","b56d7f0b":"catego = pd.crosstab(index=household['sub_category'], columns=\"count\")\ncatego \/ catego.sum()","5f86147c":"pd.crosstab(index=household['sub_category'], columns=household['format'])","889ff899":"household.head()","ef24248b":"pd.crosstab(index=household['sub_category'], \n            columns=household['format'], \n            margins=True)","717b0562":"## Your code below\n\n\n## -- Solution code","a340c35f":"pd.crosstab(index=household['sub_category'], \n            columns='mean', \n            values=household['unit_price'],\n            aggfunc='mean')","95fc3615":"pd.crosstab(index=household['sub_category'],\n           columns=household['format'],\n           values=household['unit_price'],\n           aggfunc='median', margins=True)","e6ae6d08":"## Your code below\n\n\n## -- Solution code","2f295cb0":"pd.crosstab(index=household['yearmonth'], \n            columns=[household['format'], household['sub_category']], \n            values=household['unit_price'],\n            aggfunc='median')","5b6b4a20":"pd.pivot_table(\n    data=household,\n    index='yearmonth',\n    columns=['format','sub_category'],\n    values='unit_price',\n    aggfunc='median'\n)","fe49acfb":"pd.pivot_table(\n    data=household, \n    index='sub_category',\n    columns='yearmonth',\n    values='quantity'\n)","0c1f059b":"## Your code below\n\n\n## -- Solution code","c9c5e9a2":"import math\nx=[i for i in range(32000000, 32000005)]\nx.insert(2,32030785)\nx","3f278048":"import math\nx=[i for i in range(32000000, 32000005)]\nx.insert(2,32030785)\n\nhousehold2 = household.head(6).copy()\nhousehold2 = household2.reindex(x)\nhousehold2 = pd.concat([household2, household.head(14)])\nhousehold2.loc[31885876, \"weekday\"] = math.nan\nhousehold2.iloc[2:8,]","c0d2c727":"household2['weekday'].isna()","aa04799f":"household2[household2['weekday'].isna()]","cc6e9a1f":"## Your code below\n\n\n## -- Solution code","f7288ffc":"household2.isna().sum()","b0866df0":"household2.dropna(thresh=6).head()","0f7c6741":"print(household2.shape)\nprint(household2.drop_duplicates(keep=\"first\").shape)","b897a0a1":"## Your code below\n\n\n## -- Solution code","a7eeb9c2":"household3 = household2.copy()\nhousehold3.head()","07b5b6dd":"# convert NA categories to 'Missing'\nhousehold3[['category', 'format','discount']] = household3[['category', 'format','discount']].fillna('Missing')\n\n# convert NA unit_price to 0\nhousehold3.unit_price = household3.unit_price.fillna(0)\n\n# convert NA purchase_time with 'bfill'\nhousehold3.purchase_time = household3.fillna(method='bfill')\nhousehold3.purchase_time = pd.to_datetime(household3.purchase_time)\n\n# convert NA weekday\nhousehold3.weekday = household3.purchase_time.dt.weekday_name\n\n# convert NA quantity with -1\nhousehold3.quantity = household3.quantity.replace(np.nan, -1)\n\nhousehold3.head()","b6d6356f":"Take a look on the third observation; `pd.to_datetime` converts it to 2nd January while the actual data represents February 2nd. The function may find the string pattern automatically and smartly, but note that for dates with multiple representations, it will infer it as a month first order by default.\n\nThat's why it's important to know that `pd.to_datetime` accepts other parameters, `format` and `dayfirst`:","6a88ad27":"A common way of using the `.isna()` method is to combine it with the subsetting methods we've learned in previous lessons:","ea826de6":"From the output of `dtypes`, we see that there are three variables currently stored as `object` type where a `category` is more appropriate. This is a common diagnostic step, and one that you will employ in almost every data analysis project.","074004ce":"**Knowledge Check:** Duplicates and Missing Value  \n_Est. Time required: 20 minutes_\n\n1. Duplicates may mean a different thing from a data point-of-view and a business analyst's point-of-view. You want to be extra careful about whether the duplicates is an intended characteristic of your data, or whether it poses a violation to the business logic. \n\n    - a. A medical center collects anonymized heart rate monitoring data from patients. It has duplicate observations collected across a span of 3 months\n    - b. An insurance company uses machine learning to deliver dynamic pricing to its customers. Each row contains the customer's name, occupation \/ profession and historical health data. It has duplicate observations collected across a span of 3 months\n    - c. On our original `household` data, check for duplicate observations. Would you have drop the duplicated rows?\n\n---\n\n2. Once you've identified the missing values, there are 3 common ways to deal with it:\n\n    - a. Use `dropna` with a reasonable threshold to remove any rows that contain too little values rendering it unhelpful to your analysis\n    - b. Replace the missing values with a central value (mean or median)\n    - c. Imputation through a predictive model\n        - In a dataframe where `salary` is missing but the bank has data about the customer's occupation \/ profession, years of experience, years of education, seniority level, age, and industry, then a machine learning model such as regression or nearest neighbor can offer a viable alternative to the mean imputation approach\n \nGoing back to `household2`: what is a reasonable strategy? List them down or in pseudo-code.","50e587cc":"If you've managed the above exercises, well done! Run the following cell anyway to make sure we're at the same starting point as we go into the next chapter of working with categorical data (factors). ","aeb8330e":"Notice that all columns are in the right data types, except for `purchase_time`. The correct data type for this column would have to be a `datetime`.\n\nTo convert a column `x` to a datetime, we would use:\n\n    `x = pd.to_datetime(x)`\n    ","74eddbfa":"`crosstab` is a very versatile solution to producing frequency tables on a `DataFrame` object. Its utility really goes further than that but we'll start with a simple use-case.\n\nConsider the following code: we use `pd.crosstab()` passing in the values to group by in the rows (`index`) and columns (`columns`) respectively. ","e931f1dc":"When we are certain that the rows with `NA`s can be safely dropped, we can use `dropna()`, optionally specifying a threshold. By default, this method drops the row if any NA value is present (`how='any'`), but it can be set to do this only when all values are NA in that row (`how='all'`).\n\n```\n    # drops row if all values are NA\n    household2.dropna(how='all')\n    \n    # drops row if it doesn't have at least 5 non-NA values\n    household2.dropna(thresh=5) \n```","33cf6715":"In `pandas` we call a higher-dimensional tables as Multi-Index Dataframe. We are going to dive deeper into the structure of the object on the the next chapter.","f88a2893":"Other than `to_datetime`, `pandas` has a number of machineries to work with `datetime` objects. These are convenient for when we need to extract the `month`, or `year`, or `weekday_name` from `datetime`. Some common applications in business analysis include:\n\n- `household['purchase_time'].dt.month`\n- `household['purchase_time'].dt.year`\n- `household['purchase_time'].dt.day`\n- `household['purchase_time'].dt.dayofweek`\n- `household['purchase_time'].dt.hour`\n- `household['purchase_time'].dt.weekday_name` or `household['purchase_time'].dt.day_name()` for pandas v.1.x.x","c733bed0":"There are also other functions that can be helpful in certain situations. Supposed we want to transform the existing `datetime` column into values of periods we can use the `.to_period` method:\n\n- `household['purchase_time'].dt.to_period('D')`\n- `household['purchase_time'].dt.to_period('W')`\n- `household['purchase_time'].dt.to_period('M')`\n- `household['purchase_time'].dt.to_period('Q')`","8d28c90e":"This is intuitive in a way: We use `crosstab()` which, we recall, computes the count and we pass in `index` and `columns` which correspond to the row and column respectively.\n\nWhen we add `margins=True` to our method call, then an extra row and column of margins (subtotals) will be included in the output:","fa285d52":"## Working with Categories","d0e05576":"In `pandas`, each column of a `DataFrame` is a `Series`. To get the counts of each unique levels in a categorical column, we can use `.value_counts()`. The resulting object is a `Series` and in descending order so that the most frequent element is on top. \n\nTry and perform `.value_counts()` on the `format` column, adding either:\n\n- `sort=False` as a parameter to prevent any sorting of elements, or\n- `ascending=True` as a parameter to sort in ascending order instead","06f9a305":"Go ahead and use `notna()` to extract all the rows where `weekday` column is not missing:","c5a38fc6":"# Background\n\n## Top-Down Approach \n\nThe coursebook is part of the **Data Analytics Specialization** offered by [Algoritma](https:\/\/algorit.ma). It takes a more accessible approach compared to Algoritma's core educational products, by getting participants to overcome the \"how\" barrier first, rather than a detailed breakdown of the \"why\". \n\nThis translates to an overall easier learning curve, one where the reader is prompted to write short snippets of code in frequent intervals, before being offered an explanation on the underlying theoretical frameworks. Instead of mastering the syntactic design of the Python programming language, then moving into data structures, and then the `pandas` library, and then the mathematical details in an imputation algorithm, and its code implementation; we would do the opposite: Implement the imputation, then a succinct explanation of why it works and applicational considerations (what to look out for, what are assumptions it made, when _not_ to use it etc).\n\n## Training Objectives\n\nThis coursebook is intended for participants who have completed the preceding courses offered in the **Data Analytics Developer** Specialization. This is the second course, **Exploratory Data Analysis**\n\nThe coursebook focuses on:\n- Date Time objects\n- Categorical data types\n- Why and What: Exploratory Data Analysis\n- Cross Tabulation and Pivot Table\n- Treating Duplicates and Missing Values \n\nAt the end of this course is a Learn-by-Building section, where you are expected to apply all that you've learned on a new dataset, and attempt the given questions.","83ea25dc":"We can also use the same `crosstab` method to compute a cross-tabulation of two factors. In the following cell, the `index` references the sub-category column while the `columns` references the format column:","583408ad":"In the cell above, I used `reindex` to \"inject\" some rows where values don't exist (receipts item id 32000000 through 32000004) and also set `math.nan` on one of the values for `weekday`. Notice from the output that between row 3 to 8 there are at least a few rows with missing data. We can use `isna()` and `notna()` to detect missing values. An example code is as below:","e4924f1a":"Reference answer:\n\n```\npd.crosstab(index=household['sub_category'], \n            columns=household['format'], \n            values=household['unit_price'],\n            aggfunc='median', margins=True)\n```","757d9093":"In the following cell, the technique is demonstrably repetitive or even verbose. This is done to give us an idea of all the different options we can pick from. \n\nYou may observe, for example that the two lines of code are functionally identical:\n- `.fillna(0)`\n- `.replace(np.nan, 0)`","658301f6":"We'll convert the `weekday` column to a categorical type using `.astype()`. `astype('int64')` converts a Series to an integer type, and `.astype(category)` logically, converts a Series to a categorical.\n\nBy default, `.astype()` will raise an error if the conversion is not successful (we call them \"exceptions\"). In an analysis-driven environment, this is what we usually prefer. However, in certain production settings, you don't want the exception to be raised and rather return the original object (`errors='ignore'`).","c64c2008":"## Working with Datetime\n\nGiven the program's special emphasis on business-driven analytics, one data type of particular interest to us is the `datetime`. In the first part of this coursebook, we've seen an example of `datetime` in the section introducing data types (`employees.joined`).\n\nA large portion of data science work performed by business executives involve time series and\/or dates (think about the kind of data science work done by computer vision researchers, and compare that to the work done by credit rating analysts or marketing executives and this special relationship between business and datetime data becomes apparent), so adding a level of familiarity with this format will serve you well in the long run. \n\nAs a start, let's read our data,`household.csv`:","0bcb865c":"A key difference between `crosstab` and `pivot_table` is that `crosstab` uses `len` (or `count`) as the default aggregation function while `pivot_table` using the mean. Copy the cdoe from the cell above and make a change: use `sum` as the aggregation function instead: ","94590c46":"Realize that in the code above, we're setting the row (index) to be `sub_category` and the function will by default compute a frequency table. ","1994bd3b":"The legal and cultural expectations for datetime format may vary between countries. In Indonesia for example, most people are used to storing dates in DMY order. Why it matters? Let's see what happen next when we convert our `date` to datetime object:","379d41cd":"### Alternative Solutions (optional)","830f5bf3":"# Missing Values and Duplicates\n\nDuring the data exploration and preparation phase, it is likely we come across some problematic details in our data. This could be the value of _-1_ for the _age_ column, a value of _blank_ for the _customer segment_ column, or a value of _None_ for the _loan duration_ column. All of these are examples of \"untidy\" data, which is rather common depending on the data collection and recording process in a company.\n\nIn `pandas`, we use `NaN` (not a number) to denote missing data; The equivalent for datetime is `NaT` but both are essentially compatible with each other. From the docs:\n> The choice of using `NaN` internally to denote missing data was largely for simplicity and performance reasons. We are hopeful that NumPy will soon be able to provide a native NA type solution (similar to R) performant enough to be used in pandas.","10781a50":"When we have data where duplicated observations are recorded, we can use `.drop_duplicates()` specifying whether the first occurence or the last should be kept:","2681aa86":"# Contingency Tables\n\nOne of the simplest EDA toolkit is the frequency table (contingency tables) and cross-tabulation tables. It is highly familiar, convenient, and practical for a wide array of statistical tasks. The simplest form of a table is to display counts of a `categorical` column. Let's start by reading our dataset in; Create a new cell and peek at the first few rows of the data.","d2dc624c":"**Knowledge Check:** Date time types  \n_Est. Time required: 20 minutes_\n\n1. In the following cell, start again by reading in the `household.csv` dataset \n2. Convert `purchase_time` to `datetime`. Use `pd.to_datetime()` for this.\n3. Use `x.dt.weekday_name`, assuming `x` is a datetime object to get the day of week. Assign this to a new column in your `household` Data Frame, name it `weekday`\n4. Print the first 5 rows of your data to verify that your preprocessing steps are correct","adc659b3":"# Data Preparation and Exploration\n\nAbout 60 years ago, John Tukey defined data analysis as the \"procedures for analyzing data, techniques for interpreting the results of such procedures ... and all the machinery of mathematical statistics which apply to analyzing dsta\". His championing of EDA encouraged the development of statsitical computing packages, especially S at Bell Labs (which later inspired R).\n\nHe wrote a book titled _Exploratory Data Analysis_ arguing that too much emphasis in statistics was placed on hypothesis testing (confirmatory data analysis) while not enough was placed on the discovery of the unexpected. \n\n> Exploratory data analysis isolates patterns and features of the data and reveals these forcefully to the analyst.\n\nThis course aims to present a selection of EDA techniques -- some developed by John Tukey himself -- but with a special emphasis on its application to modern business analytics.\n\nIn the previous course, we've got our hands on a few common techniques:\n\n- `.head()` and `.tail()`\n- `.describe()`\n- `.shape` and `.size`\n- `.axes`\n- `.dtypes`\n\nIn the following chapters, we'll expand our EDA toolset with the following additions:  \n\n- Tables\n- Cross-Tables and Aggregates\n- Using `aggfunc` for aggregate functions\n- Pivot Tables","86e2262e":"## Pivot Tables\n\nIf our data is already in a `DataFrame` format, using `pd.pivot_table` can sometimes be more convenient compared to a `pd.crosstab`. \n\nFortunately, much of the parameters in a `pivot_table()` function is the same as `pd.crosstab()`. The noticable difference is the use of an additional `data` parameter, which allow us to specify the `DataFrame` that is used to construct the pivot table.\n\nWe create a `pivot_table` by passing in the following:\n- `data`: our `DataFrame`\n- `index`: the column to be used as rows\n- `columns`: the column to be used as columns\n- `values`: the values used to fill in the table\n- `aggfunc`: the aggregation function","d463b4e2":"## Aggregation Table\n\nIn the following section, we will introduce another parameter to perform aggregation on our table. The `aggfunc` parameter when present, required the `values` parameter to be specified as well. `values` is the values to aggregate according to the factors in our index and columns:","6e8018e6":"If you want an extra challenge, try and modify your code above to include a `normalize` parameter. \n\n`normalize` accepts a boolean value, or one of `all`, `index` or `columns`. Since we want it to normalize across each row, we will set this parameter to the value of `index`.","36ca1d70":"Go ahead and perform the other conversions in the following cell. When you're done, use `dtypes` to check that you have the categorical columns stored as `category`.","b138880b":"Another common use-case in missing values treatment is to count the number of `NAs` across each column:","271716a4":"As you can see from the code above,`pd.to_datetime()` could do the conversion to datetime in a smart way without datetime format string required. Convenient for sure, but for some situation, this manner of `pandas` can be a little tricky.\n\nSuppose we have a column which stores a daily sales data from end of January to the beginning of February:","b13483a4":"In the cell above, we set the values to be normalized over each columns, and this will divide each values in place over the sum of all values. This is equivalent to a manual calculation:","afcbc028":"**Coursebook: Exploratory Data Analysis**\n- Part 2 of Data Analytics Specialization\n- Course Length: 12 hours\n- Last Updated: February 2020\n\n___\n\n- Author: [Samuel Chan](https:\/\/github.com\/onlyphantom)\n- Developed by [Algoritma](https:\/\/algorit.ma)'s product division and instructors team","0905c82e":"In the following cell, use `pd.crosstab()` with `yearmonth` as the row and `format` as the column. Set `margins=True` to get a total across the row and columns. ","1031cc66":"Tips: In the cell above, start from:\n\n`household = pd.read_csv(\"data_input\/household.csv\")`\n\nInspect the first 5 rows of your data and pay close attention to the `weekday` column. \n\nBonus challenge: How many transactions happen on each day of the week? Use `pd.crosstab(index=__, columns=\"count\")` or `x.value_counts()`.","e5ce7f65":"**Knowledge Check**: Cross tabulation  \n\nCreate a cross-tab using `sub_category` as the index (row) and `format` as the column. Fill the values with the median of `unit_price` across each row and column. Add a subtotal to both the row and column by setting `margins=True`.\n\n1. On average, Sugar is cheapest at...?\n2. On average, Detergent is most expensive at...?\n\nCreate a new cell for your code and answer the questions above.","5a8b1c25":"Using Python's `datetime` module, `pandas` pass the date string to `.strptime()` and follows by what's called Python's strptime directives. The full list of directives can be found in this [Documentation](https:\/\/strftime.org\/).","aebab617":"### Higher-dimensional Tables\n\nIf we need to inspect our data in higher resolution, we can create cross-tabulation using more than one factor. This allows us to yield insights on a more granular level yet have our output remain relatively compact and structured:","82130d58":"## Missing Values Treatment\n\nSome common methods when working with missing values are demonstrated in the following section. We make a copy of the NA-included DataFrame, and name it `household3`:"}}