{"cell_type":{"f0c3a453":"code","613a5902":"code","9b37b622":"code","6e862677":"code","40bfd6a9":"code","94132cb5":"code","d3bf5458":"code","d42cdd33":"code","522331e4":"code","1a58d4fe":"code","a1887910":"code","19b51ff7":"code","e882a3e2":"code","cba70e05":"code","4be3efae":"code","9040191d":"code","92d3e752":"code","f77030ca":"code","5acc4494":"code","60364215":"code","7fe7f746":"code","83a0398f":"code","90da4f9f":"code","aa0e1eac":"markdown"},"source":{"f0c3a453":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","613a5902":"import pandas as pd\nimport numpy as np\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport itertools\n\nimport matplotlib.pyplot as plt\n\n%matplotlib inline","9b37b622":"test = pd.read_csv(\"\/kaggle\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv\")\ntrain = pd.read_csv(\"\/kaggle\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv.zip\")\ntest.head(3)","6e862677":"train","40bfd6a9":"train.rename(columns={'comment_text':'text'}, inplace=True)","94132cb5":"test.shape, train.shape","d3bf5458":"train.isna().sum()","d42cdd33":"train['toxic'].value_counts()","522331e4":"def identity_hate_to_cat(X):\n    X['identity_hate_cat'] = 0\n    X.loc[X['identity_hate'] == 1, 'identity_hate_cat'] = 0.2\n    return X\n\ndef insult_to_cat(X):\n    X['insult_cat'] = 0\n    X.loc[X['insult'] == 1, 'insult_cat'] = 0.4\n    return X\n\ndef threat_to_cat(X):\n    X['threat_cat'] = 0\n    X.loc[X['threat'] == 1, 'threat_cat'] = 0.6\n    return X\n\ndef obscene_to_cat(X):\n    X['obscene_cat'] = 0\n    X.loc[X['obscene'] == 1, 'obscene_cat'] = 0.8\n    return X\n\ndef severe_toxic_to_cat(X):\n    X['severe_toxic_cat'] = 0\n    X.loc[X['severe_toxic'] == 1, 'severe_toxic_cat'] = 1\n    return X\n\ndef toxic_to_cat(X):\n    X['toxic_cat'] = X['severe_toxic_cat'] + X['obscene_cat'] + X['threat_cat'] + X['insult_cat'] + X['identity_hate_cat']\n    return X","1a58d4fe":"identity_hate_to_cat(train)\ninsult_to_cat(train)\nthreat_to_cat(train)\nobscene_to_cat(train)\nsevere_toxic_to_cat(train)\ntoxic_to_cat(train)\ntrain","a1887910":"train['toxic_cat'].value_counts()","19b51ff7":"train['toxic'].value_counts()","e882a3e2":"x = train.drop('toxic', axis=1)\nx","cba70e05":"y = train['toxic']\ny","4be3efae":"#\u0440\u0430\u0437\u0434\u0435\u043b\u0438\u043c \u0434\u0430\u043d\u043d\u044b\u0435 \u043d\u0430 train\/test\nX_train, X_test, y_train, y_test = train_test_split(x, y, random_state=0)","9040191d":"from sklearn.ensemble import GradientBoostingRegressor\n\n#\u0441\u043e\u0431\u0435\u0440\u0435\u043c \u043d\u0430\u0448 \u043f\u0440\u043e\u0441\u0442\u043e\u0439 pipeline, \u043d\u043e \u043d\u0430\u043c \u043f\u043e\u043d\u0430\u0434\u043e\u0431\u0438\u0442\u0441\u044f \u043d\u0430\u043f\u0438\u0441\u0430\u0442\u044c \u043a\u043b\u0430\u0441\u0441 \u0434\u043b\u044f \u0432\u044b\u0431\u043e\u0440\u0430 \u043d\u0443\u0436\u043d\u043e\u0433\u043e \u043f\u043e\u043b\u044f\nclass FeatureSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, column):\n        self.column = column\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X, y=None):\n        return X[self.column]\n\nmodel = Pipeline([('comment_text_selector', FeatureSelector(column='text')), \n                     ('comment_text_tfidf', TfidfVectorizer(sublinear_tf=True,\n                                                            strip_accents='unicode',\n                                                            analyzer='word',\n                                                            token_pattern=r'\\w{1,}',\n                                                            stop_words='english',\n                                                            ngram_range=(1, 1),\n                                                            max_features=6000)), \n                     ('clf', LogisticRegression(C=0.1, solver='sag'))])\n\n#\u0437\u0430\u043f\u0443\u0441\u0442\u0438\u043c \u043a\u0440\u043e\u0441\u0441-\u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u044e\ncv_scores = cross_val_score(model, X_train, y_train, cv=3, scoring='roc_auc')\ncv_score = np.mean(cv_scores)\nprint('CV score is {}'.format(cv_score))\n\n","92d3e752":"#\u043e\u0431\u0443\u0447\u0438\u043c \u043f\u0430\u0439\u043f\u043b\u0430\u0439\u043d \u043d\u0430 \u0432\u0441\u0435\u043c \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043e\u0447\u043d\u043e\u043c \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0435\nmodel.fit(X_train, y_train)\ny_score = model.predict_proba(X_test)[:, 1]","f77030ca":"import numpy as np\nfrom sklearn.metrics import precision_recall_curve, roc_curve, roc_auc_score, confusion_matrix\n\nb=1\n\nprecision, recall, thresholds = precision_recall_curve(y_test.values, y_score)\nfscore = (1+b**2)*(precision * recall) \/ (b**2*precision + recall)\n# locate the index of the largest f score\nix = np.argmax(fscore)\nprint('Best Threshold=%f, F-Score=%.3f, Precision=%.3f, Recall=%.3f' % (thresholds[ix], \n                                                                        fscore[ix],\n                                                                        precision[ix],\n                                                                        recall[ix]))","5acc4494":"import seaborn as sns\nimport itertools\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\nsns.set(font_scale=1.5)\nsns.set_color_codes(\"muted\")\n\nplt.figure(figsize=(10, 8))\n# fpr, tpr, thresholds_ = roc_curve(y_test, y_score, pos_label=1) # \u043f\u0440\u0438 max_features=100\n# fpr1, tpr1, thresholds_ = roc_curve(y_test, y_score, pos_label=1) # \u043f\u0440\u0438 max_features=1000\n# fpr2, tpr2, thresholds_ = roc_curve(y_test, y_score, pos_label=1) # \u043f\u0440\u0438 max_features=2000\n# fpr3, tpr3, thresholds_ = roc_curve(y_test, y_score, pos_label=1) # \u043f\u0440\u0438 max_features=4000\nfpr4, tpr4, thresholds_ = roc_curve(y_test, y_score, pos_label=1) # \u043f\u0440\u0438 max_features=6000\nlw = 2\n# plt.plot(fpr, tpr, lw=lw,color = 'r', label='ROC curve max_features=100')\n# plt.plot(fpr1, tpr1, lw=1,color = 'b', label='ROC curve max_features=1000')\n# plt.plot(fpr2, tpr2, lw=1,color = 'g', label='ROC curve max_features=2000')\n# plt.plot(fpr3, tpr3, lw=1,color = 'y', label='ROC curve max_features=4000')\nplt.plot(fpr4, tpr4, lw=1,color = 'r', label='ROC curve max_features=6000')\nplt.plot([0, 1], [0, 1])\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC curve')\nplt.legend()\nplt.savefig(\"ROC.png\")\nplt.show()","60364215":"predicts = model.predict_proba(test)[:, 1]\npredicts","7fe7f746":"submit=pd.read_csv('\/kaggle\/input\/jigsaw-toxic-severity-rating\/sample_submission.csv', sep=',')\nsubmit.head()","83a0398f":"submit['score'] = predicts","90da4f9f":"submit.to_csv('submission.csv', index=False)","aa0e1eac":"**\u041e\u0431\u0443\u0447\u0435\u043d\u0438\u0435**"}}