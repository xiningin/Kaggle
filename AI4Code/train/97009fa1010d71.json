{"cell_type":{"bd9df0df":"code","06ad5479":"code","6ccb02f0":"code","c0059482":"code","42132249":"code","6e9d2c8c":"code","6cee2f41":"code","35bfe5b9":"code","6abe3fcf":"code","454bf16d":"code","dbeece5a":"code","d1e19161":"code","5eae4f21":"code","ee416eb8":"code","1c7100fa":"code","fc116df8":"code","aee5f576":"code","92d69af5":"code","2a73f4ef":"code","5d958c72":"code","16a8f0f1":"code","9f266e12":"code","07001c08":"code","2d6db88a":"markdown","d68f605d":"markdown","1598ddf6":"markdown","79358f78":"markdown","2824c1e1":"markdown","b7fed112":"markdown","670e008f":"markdown","fa2c1fb2":"markdown","005b3b30":"markdown","3297801b":"markdown","b3a5239f":"markdown","b09e680a":"markdown","b94f1866":"markdown","9347c83c":"markdown"},"source":{"bd9df0df":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.tree import plot_tree\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import plot_confusion_matrix","06ad5479":"df=pd.read_csv('..\/input\/breast-cancer-wisconsin-prognostic-data-set\/data 2.csv')\ndf.head()","6ccb02f0":"#Weird part of the data a full column of nan values, and getting rid of the id columns which is going to be probably uselessf\ndf.drop(['Unnamed: 32'],axis=1,inplace=True)\ndf.drop(['id'],axis=1,inplace=True)\ndf.dtypes","c0059482":"#Checking for null values in the float columns doesnt seen to have any, neither in diagnosis but lets check empty strings\ndf.isnull().sum()# Lovely line of code","42132249":"df.describe().T","6e9d2c8c":"df['diagnosis'].unique()\n#Lovely result","6cee2f41":"X=df.drop('diagnosis',axis=1).copy()\nX.dtypes","35bfe5b9":"y=df['diagnosis'].copy()\ny.head()","6abe3fcf":"y.replace('M','1',regex=True,inplace=True)\ny.replace('B','0',regex=True,inplace=True)\ny=pd.to_numeric(y)\ny.dtypes","454bf16d":"X_train,X_test,y_train,y_test = train_test_split(X,y)\nclf_dt=DecisionTreeClassifier()\nclf_dt=clf_dt.fit(X_train,y_train)","dbeece5a":"plt.figure(figsize=(15,7.5))\nplot_tree(clf_dt,\n         filled=True,\n         rounded=True,\n         class_names=['No to cancer',' Yes to cancer'],\n         feature_names=X.columns);","d1e19161":"plot_confusion_matrix(clf_dt,X_test,y_test,display_labels=['No to cancer','Yes to cancer ):'])","5eae4f21":"path=clf_dt.cost_complexity_pruning_path(X_train,y_train)#Determining the alphas\nccp_alphas=path.ccp_alphas#Extracting different values\nccp_alphas=ccp_alphas[:-1]# Exclude the maximum values of alhpas\n\nclf_dts=[]\n# BUilding a tree per alpha, value and stocking into an array\nfor ccp_alpha in ccp_alphas:\n    clf_dt=DecisionTreeClassifier(ccp_alpha=ccp_alpha)\n    clf_dt.fit(X_train,y_train)\n    clf_dts.append(clf_dt)","ee416eb8":"train_scores=[clf_dt.score(X_train,y_train) for clf_dt in clf_dts]\ntest_scores= [clf_dt.score(X_test,y_test) for clf_dt in clf_dts]\n# Getting the precisions of each tree build according to the increase in the alpha value","1c7100fa":"fig,ax=plt.subplots()\nax.set_xlabel('alpha')\nax.set_ylabel('accuracy')\nax.set_title('Accuracy according to the increase of the alpha value for training set and testing set')\nax.plot(ccp_alphas,train_scores,marker='o',label='train',drawstyle='steps-post')\nax.plot(ccp_alphas,test_scores,marker='o',label='test',drawstyle='steps-post')\nax.legend()\nplt.show()","fc116df8":"clf_dt=DecisionTreeClassifier(ccp_alpha=0.0170)\n#just creating one tree with improved alpha value maybe who knows we get a better one\nscores=cross_val_score(clf_dt,X_train,y_train,cv=10)\ndf=pd.DataFrame(data={'tree':range(10),'accuracy': scores})\ndf.plot(x='tree',y='accuracy',marker='o',linestyle='--')","aee5f576":"alpha_loop_values=[]\n\nfor ccp_alpha in ccp_alphas:\n    clf_dt=DecisionTreeClassifier(ccp_alpha=ccp_alpha)\n    scores=cross_val_score(clf_dt,X_train,y_train,cv=10)\n    alpha_loop_values.append([ccp_alpha,np.mean(scores),np.std(scores)])\n\nalpha_results=pd.DataFrame(alpha_loop_values,columns=['alpha','mean_accuracy','standard deviation'])\n\nalpha_results.plot(x='alpha',\n                  y='mean_accuracy',\n                  yerr='standard deviation',\n                  marker='o',\n                  linestyle='--')\n    ","92d69af5":"alpha_results[(alpha_results['alpha']>0.002)\n              &\n             (alpha_results['alpha']<0.02)]\n# Getting every single alpha in beetween the lines, turns out i found out how to do it :>\n#0.003130\n#Might combeback i","2a73f4ef":"ideal_alpha=alpha_results[(alpha_results['alpha']>0.002339)\n              &\n             (alpha_results['alpha']<0.003131)]['alpha']\nideal_alpha\n# Little bit of selection, to easy up the dtype tranformation sorry you guys couldnt see is just too many cells","5d958c72":"ideal_ccp_alpha =0.004607","16a8f0f1":"clf_dt_pruned=DecisionTreeClassifier(ccp_alpha=ideal_alpha)\nclf_dt_pruned=clf_dt_pruned.fit(X_train,y_train)","9f266e12":"plot_confusion_matrix(clf_dt_pruned,X_test,y_test,display_labels=['No cancer','Yes cancer'])","07001c08":"plt.figure(figsize=(15,7.5))\nplot_tree(clf_dt_pruned,\n         filled=True,\n         rounded=True,\n         class_names=['No to cancer',' Yes to cancer'],\n         feature_names=X.columns);","2d6db88a":"### Imports","d68f605d":"# Introduction to the project\n\n* Hello everyone! It is a me, yappy or sirmicolau or Francisco.\n* This is a personal attempt of mine, when it come to the breast cancer dataset\n* I am going to be building up a cliche like decision tree, that is going to define whethere the cancer is bening or malignant\n* She is going to have a few hyperparameter such as alpha to make it a little more precise\n* Oh also also this is going to have a little bit of one hot encoding\n* Keep it up into the end and there is also going to be a drawing\n* This is my second project posted here in kaggle still trying to adapt to it so please be gentle\n* Shotout to Joshua Starmer whos video helped me out a lot in this project\n* Hope you guys enjoy, also thank you UCI for the lovely dataset","1598ddf6":"* Not gonna lie that legend screwed me up if there is anyway to actually check that value please help me,\n* Anyways i am guessing that is about 0.0170, so ccp_alpha=0.0170\n* Oh wait actually, i gotta an idea lets do a little bit of cross validation and get a bunch of alphas\n* This one is just the best one for that datatype aha. Nice way of circunventing the issue yappy.","79358f78":"# Formating the data, into dependent variables and independent variables\n* We going conventional notation X upper case and y lower case\n* Using copy so it doent modify the dataset","2824c1e1":"* Yep turns out it is, well i be dammed\n* Well lets build up more trees, but instead of the same alpha value for each and every dataset. Lets get the best alpha, from each dataset. And select the one that does the best job in most cases\n","b7fed112":"# CCP, BUT WITH CROSS VALIDATION\n* First lets check with alpha is heavily influenced by the part of the data being taken","670e008f":"## Checking for outliers\n* It seen we have a few outliers\n* I am not going take off right now i am going to build the model and on a later ocasion we see if we get better precision","fa2c1fb2":"* Damm i got it the ideal alpha better be worth it","005b3b30":"## Okay lets run that tree thorough the testing set\n* Quite good you know, taking in consideration the issue here (cancer) we need more precision maybe we can get that by pruning","3297801b":"# Building,evaluating and drawing\n* So now we going to build up a pruned tree, and we going check out we got a better precision when we compare it to our preliminary tree which had no alpha value\n","b3a5239f":"# Building up the preliminary model\n* What we are going to do here is build the early and check the best parameter and the rebuild it again, with the best parameter\n* That get more precision","b09e680a":"# Cost complexity pruning\n* Lets find the best alpha value","b94f1866":"# Analysing the internal parts of the dataset\n* First a little bit of Nan checking, and column selection\n* It seens this dataset doesnt have categorical data at first look\n* Turns out data was pretty clear. Nice job!\n* Really clean dataset :)\n","9347c83c":"## Formating the data, of the label\n* Basically the conversion to numerical value\n* I am not going to enter into the details of each column because really dont know anything about medicine :)\n* Just hoping they have some correlation, but i cant see any data that seens to be categorical so no one hoting yay?\n* Only the y columns is going to be converted"}}