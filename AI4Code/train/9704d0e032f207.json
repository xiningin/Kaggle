{"cell_type":{"225f1a15":"code","09aac63b":"code","29ec0ec2":"code","4290297f":"code","65966b39":"code","2000d583":"code","22fdb053":"code","a99bc58f":"code","6f761a5c":"code","3d848e62":"code","6f8398f9":"code","ec85b6dc":"code","971b78d9":"code","2c99984d":"code","8259a62a":"code","65969ac7":"code","3a44091d":"code","ab795a37":"code","7cd763ad":"code","6ebb342f":"code","92bad027":"code","d8a28cb3":"code","da678257":"code","cb4ec6d9":"code","0e0f6844":"code","81e323ae":"code","ffb80d90":"code","8d1729cc":"code","2370f911":"code","71072431":"code","a2068611":"code","ec9605d4":"code","76ea6b53":"code","87fbbfd7":"code","48a624a7":"code","ebe1476c":"code","60480f53":"code","344e428e":"code","b3ffa7a3":"code","4a383365":"code","8c2b7cdf":"code","5f2e3318":"code","6c38f3d0":"code","7b618e76":"code","2c8b5075":"code","8a0b2b22":"code","12d180fb":"code","eb9a9236":"code","f4df7eb9":"code","9a464bca":"code","94e3d962":"code","d790c2db":"code","bf8389c9":"code","f6ca9c69":"markdown","7b023f6a":"markdown","3eed160d":"markdown","918a003c":"markdown","3f7da73f":"markdown","0432eb9d":"markdown","84449ab6":"markdown","4343792e":"markdown","2b03e178":"markdown","23ebfba2":"markdown","a7fa3233":"markdown","f3e846e7":"markdown","b5e37b38":"markdown","6dd9b0ba":"markdown","902ee82a":"markdown","e6080df1":"markdown","355186fa":"markdown","344b797a":"markdown","d882c1cd":"markdown","df9f8612":"markdown","36cd53f2":"markdown","dd10873e":"markdown","9e82eb23":"markdown","f20621f2":"markdown","9438e232":"markdown","4ee4d7ed":"markdown","2e53ada4":"markdown","a8661b7b":"markdown","1a656d55":"markdown","dd28e011":"markdown","32b3a355":"markdown","d86be7f0":"markdown","0ccff7de":"markdown","2851024d":"markdown"},"source":{"225f1a15":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","09aac63b":"# Packages installed\n\n!pip install xmltodict #Used for transforming xml to dictionary\n# !pip install pycountry-convert #Optional package(Used to map the Location with ISO coutry codes)","29ec0ec2":"#Libraries used\nimport os\nimport xmltodict\nimport json #json file type analysis \nimport pandas as pd #data analysis\nimport numpy as np #data manipulation\nfrom datetime import date ## date fields transformstions\nfrom dateutil.relativedelta import relativedelta \n\n#data visualization\nimport plotly.graph_objects as go\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n# Plotly - Interactive Graphs\nimport plotly.express as px\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n\n\n#Model\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import ElasticNet\nfrom xgboost import XGBRegressor\n\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn import metrics\nfrom sklearn.model_selection import GridSearchCV\n\n#from pycountry_convert import country_name_to_country_alpha3 #tested for mapping location with ISO codes\n#pd.set_option('display.max_columns',100)","4290297f":"# input paths\ninput_dir_path = '\/kaggle\/input\/moneystackexchange'\ndata_dir_name = 'money.stackexchange.com'\nusers_data_file_path = os.path.join(input_dir_path,data_dir_name, 'Users.xml')","65966b39":"# Function to extract data from xml and save it to a dataframe\ndef xml_to_df(xml_path,root_tag_name, row_tag_name):\n    \n    with open(xml_path) as xml_file:\n        data_dict = xmltodict.parse(xml_file.read())\n    json_data = json.dumps(data_dict)\n    json_row_data = json.loads(json_data)\n\n    return pd.DataFrame(json_row_data[root_tag_name][row_tag_name])\n    \ndata_df = xml_to_df(users_data_file_path, 'users', 'row')\ndata_df.head(5)","2000d583":"# Rename the columns by replacing the special charater @ with empty string\ncolumn_names = [col.replace('@','') for col in data_df.columns]\ndata_df.columns = column_names","22fdb053":"# Get the dataframe information in a glance.\ndata_df.info()","a99bc58f":"# Lets check the AccountId field missing information\ndata_df[data_df['AccountId'].isna() == True]","6f761a5c":"# Dropping the rows whose AccoundId's are null\ndata_df = data_df.drop(data_df[data_df['AccountId'].isna() == True].index)","3d848e62":"# Identify & Drop the Duplicated rows if any\nduplicate = data_df[data_df.duplicated()] \nprint(f'Duplicated data frame length : {len(duplicate)} (Zero means there are no duplicated records)')\n","6f8398f9":"# Check the Duplicated Accounts\ndata_df[data_df.duplicated(subset=['AccountId'])==True]","ec85b6dc":"# Duplicated Account records\ndata_df[(data_df['AccountId'] ==  '4089929') | (data_df['AccountId'] ==  '4869405')]","971b78d9":"data_df[data_df.duplicated(subset=['AccountId'], keep='first')==True]","2c99984d":"# Drop the duplicated account rows\ndata_df = data_df.drop(data_df[data_df.duplicated(subset=['AccountId'], keep='first')==True].index)","8259a62a":"# function to create bar plot and display the values on the bars  \ndef plot_bar(labels, values, fig_title, x_axis_name, fig_size, rotation):\n    plt.figure(figsize=fig_size)\n    plt.xticks(rotation=rotation)\n    #plt.axis('off')\n    ax = sns.barplot(x=labels, y=values)\n    ax.tick_params(labelsize=12)\n    ax.set_yticks([])\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.spines['bottom'].set_visible(False)\n    ax.spines['left'].set_visible(False)\n    ax.set_title(fig_title, fontsize=20, color=\"purple\")\n    ax.set_xlabel(x_axis_name, fontsize=12)\n    for p in ax.patches:\n      ax.annotate(\"%.f\" % p.get_height(), (p.get_x() + p.get_width() \/ 2., p.get_height()),\n          ha='center', va='center', fontsize=14, color='purple', xytext=(0, 8),\n          textcoords='offset points')\n    plt.tight_layout()","65969ac7":"# total number of unique observations for each featue\nuniq_cnt = []\nfor col in data_df.columns:\n    uniq_cnt.append(len(data_df[col].unique()))\n\nlabels = data_df.columns\nplot_bar(labels, uniq_cnt, '#Uniqueness with respect to feature ', '', [15,6], 90)","3a44091d":"# Lets visually see how much percentage of missing values exists\n\n# Function to plot bar chart for missing fields with respect to total records\ndef plot_missing_perc(df, fig_title, fig_size):\n    missing_data_sum = df.isnull().sum()\n    missing_data = pd.DataFrame({'total_missing_values': missing_data_sum,'(%)': (missing_data_sum\/df.shape[0])*100})\n    plt.figure(figsize=fig_size)\n    plt.title(fig_title, fontsize=20, color=\"purple\")\n    chart = sns.barplot(x =missing_data['(%)'] , y = df.columns, orientation='horizontal')\n    return missing_data\nmissing_data = plot_missing_perc(data_df, \"Missing Data interms of percentages with respect to total number of observations\", [12,6])","ab795a37":"# Lets look into another way with respect to missing fields instead of total number of observations\nmissing_data = missing_data[missing_data['(%)'] > 0]\nfig = go.Figure([go.Pie(labels=missing_data.index, values=missing_data['(%)'])])\nfig.update_traces(hole=.6, hoverinfo=\"label+percent\")\n\nfig.update_layout(title_text=\"Donut Pie Chart for Missing Data with respect to missing fields percentages(%)\",\n                  annotations=[dict(text='Missing field data %', x=0.5, y=0.5, \n                                    font_size=18, showarrow=False)])\nfig.show()\n","7cd763ad":"# Take a look into url content\ndata_df.WebsiteUrl.unique()[0:30]","6ebb342f":"# Check the unique url counts\ndata_df.WebsiteUrl.value_counts()","92bad027":"data_df.Location.unique()[0:30]","d8a28cb3":"# copying the original data frame into new dataframe to apply EDA on location field\ndata_loc_df = data_df.copy()","da678257":"# Function to extract the Country name from Location content\ndef get_country(location):\n    if ',' in location:\n        loc_name = location.split(',')[1].strip()\n        if loc_name == 'Karnataka India':\n            loc_name = 'India'\n            \n        if len(loc_name) == 2: \n            loc_name = location.split(',')[0].strip()\n            \n        return loc_name\n    \n    return location","cb4ec6d9":"# Adding Country column to location data frame\ndata_loc_df['Country'] = data_loc_df['Location'].apply(lambda x : get_country(str(x)))","0e0f6844":"temp = data_loc_df.Country.value_counts(ascending = False).head(6)\ndata = go.Bar(\n    y = temp.values,\n    x = temp.index,\n    marker_color=px.colors.qualitative.D3[2],\n    marker = dict(\n        colorscale = 'Greens',\n    )\n    \n)\nlayout = dict(\n    title = 'Top 5 Locations ',\n\n    height = 300,\n    margin = dict(\n    )\n)\nfig = go.Figure(data = data, layout = layout)\nfig.show()","81e323ae":"# Function to convert datatypes for the list of columns in a given dataframe\ndef convert_dtype(df,col_list,data_type):\n    for col in col_list:\n        df[col] = df[col].astype(data_type)\n    return None","ffb80d90":"# Identifying list of different data types columns list\nnumeric_cols = ['Id','Reputation', 'Views', 'UpVotes', 'DownVotes', 'AccountId' ]\ndate_cols = ['CreationDate','LastAccessDate']\nstring_cols = [col for col in data_df if (col not in date_cols and col not in numeric_cols)]","8d1729cc":"# convert data types to numeric, string and date types\n\nconvert_dtype(data_df, numeric_cols, 'int')\nconvert_dtype(data_df, string_cols, 'str')\nconvert_dtype(data_df, date_cols, 'datetime64[ns]')","2370f911":"data_df.info()","71072431":"# Assuming model related features\ndata_md_df = data_df.copy()\ndata_md_df = data_md_df[['Id', 'Reputation', 'CreationDate', 'LastAccessDate', 'Views', 'UpVotes', 'DownVotes']]","a2068611":"data_md_df.describe()","ec9605d4":"# differece in months & days between createdate and lastaccesssdate\ndata_md_df['MonthsDiff'] = ((data_md_df.LastAccessDate - data_md_df.CreationDate)\/np.timedelta64(1, 'M')).astype(int)\n#data_md_df['DaysDiff'] = ((data_md_df.LastAccessDate - data_md_df.CreationDate)\/np.timedelta64(1, 'D')).astype(int)","76ea6b53":"data_md_df['CreationDateYear'] = data_md_df['CreationDate'].dt.year\ndata_md_df['CreationDateMonth'] = data_md_df['CreationDate'].dt.month\ndata_md_df['LastAccessDateYear'] = data_md_df['LastAccessDate'].dt.year\ndata_md_df['LastAccessDateMonth'] = data_md_df['LastAccessDate'].dt.month\ndata_md_df['LastAccessYearMonth'] = data_md_df['LastAccessDate'].dt.date.astype('str').apply(lambda x : x[0:7])\ndata_md_df['CreationYearMonth'] = data_md_df['CreationDate'].dt.date.astype('str').apply(lambda x : x[0:7])\n","87fbbfd7":"# Function to create yearly map\ndef year_map(year_feature, user_ids, title):\n    year_df = data_md_df.groupby([year_feature])[user_ids].count().to_frame()\n\n    trace = go.Scatter(\n        y= year_df[user_ids], x= year_df.index,\n        mode= 'lines+markers', line=dict(width=6), line_color='Green'\n    )\n\n    layout = go.Layout(autosize=True, title= title,xaxis_title=\"Year\", yaxis_title=\"Number of Users \",showlegend=False)\n    fig = go.Figure(data=[trace], layout=layout)\n    iplot(fig)\n    return None","48a624a7":"# Check how many user profiles were being created yearly\nyear_map('CreationDateYear', 'Id', 'Number Of User Profiles Created Per Year')","ebe1476c":"# Check how many user profiles were being accessed by yearly\nyear_map('LastAccessDateYear', 'Id', 'Number Of User Profiles Accessed Per Year')","60480f53":"# function to get previous n months of data into a dataframe for given date field\ndef get_previous_months_df(_date, num_months):\n    previous_n_months = date.today() - relativedelta(months=+num_months)\n    previous_months_df = data_md_df[data_md_df[_date].dt.date >= previous_n_months]\n    return previous_months_df\n","344e428e":"# How many user profiles were accessed from the previous n number of months?\nnum_months = 12\nprevious_months_last_acess_df = get_previous_months_df('LastAccessDate', num_months)\n\nprevious_months_last_acess_data = previous_months_last_acess_df['LastAccessYearMonth'].value_counts()\nplot_bar(previous_months_last_acess_data.index, previous_months_last_acess_data.values, f'Number of users accessing their profiles from previous {num_months} months ', '', [15,6], 0)","b3ffa7a3":"# How many user profiles were created recentely from the previous n number of months? \nnum_months = 12\nprevious_months_creation_df = get_previous_months_df('CreationDate', num_months)\n\nprevious_months_creation_data = previous_months_creation_df['LastAccessYearMonth'].value_counts()\nplot_bar(previous_months_creation_data.index, previous_months_creation_data.values, f'Number of user profiles were created from previous {num_months} months ', '', [15,6], 0)","4a383365":"# Function to plot regression graphs\ndef plot_reg(df, indipendent_features):\n  plt.figure(figsize=(25, 20))\n  for loc, feature in enumerate(indipendent_features):\n    ax = plt.subplot(3, 2, loc+1)\n    ax.set_title(f'Target Feature vs {feature}')\n    sns.regplot(x=df[feature], y=df['MonthsDiff'], color='green');\n    \n  return None","8c2b7cdf":"plot_reg(data_md_df, ['Views','UpVotes' ,'Reputation','DownVotes', 'LastAccessDateYear', 'CreationDateYear'])","5f2e3318":"# Look into the Highest observations for each feature\nbase_lst = [[data_md_df['Views'].max(), data_md_df['UpVotes'].max(), \n      data_md_df['DownVotes'].max(), data_md_df['Reputation'].max(),\n      data_md_df['CreationDate'].max(), data_md_df['LastAccessDate'].max()]] \n    \nbase_stats = pd.DataFrame(base_lst, columns =['Views', 'UpVotes','DownVotes', 'Reputation', 'CreationDate', 'LastAccessDate']) \nbase_stats","6c38f3d0":"data_md_df[data_md_df['Views'] == base_stats.Views[0]]","7b618e76":"# Function to derive the month order based on creation date month\ndef get_month_order(_date):\n\n    month_order = int((_date - data_md_df['CreationDate'].min())\/(np.timedelta64(1,'M')))\n    \n    return month_order\n\ndata_md_df['MonthOrder'] = data_md_df['CreationDate'].apply(lambda _date : get_month_order(_date))    ","2c8b5075":"fig = px.scatter(data_md_df, x=\"Reputation\", y=\"Views\", animation_frame=\"CreationYearMonth\", \n           size=\"UpVotes\", color=\"LastAccessDateYear\",\n           log_x=True, size_max=200, range_x=[1,base_stats.Reputation[0]], range_y=[0,2000], \n                 title='Range Slider Graph to see Views, UpVotes, Reputation by user profile creation date ')\n\nfig[\"layout\"].pop(\"updatemenus\")\nfig.show()","8a0b2b22":"# Function to set color based on target feature value range\ndef SetColor(x):\n    if(x < 50):\n        return \"green\"\n    elif(50<= x <=100):\n        return \"orange\"\n    elif(x > 100):\n        return \"red\"","12d180fb":"fig=go.Figure(go.Scatter(x=data_md_df['CreationDateYear'], y=data_md_df['MonthsDiff'], # Data\n                    mode='markers', name='Show1',\n                    marker = dict(size=8, color=list(map(SetColor,data_md_df['MonthsDiff']))),\n                    line=dict(color='rgb(200,200,200)'\n                       )))\nfig.update_layout(xaxis_title=\"Creation Date Year\", yaxis_title=\"Difference in Months\",\n                  title=\"Colored based on Target Values\")\n\nfig.show() ","eb9a9236":"plt.figure(figsize=[15,12])\nsns.heatmap(data_md_df.corr(), annot=True, cmap='Greens');","f4df7eb9":"data_md_df.head()","9a464bca":"# Considering required features to build model\nmd_df  = data_md_df[['Reputation', 'Views', 'UpVotes', 'DownVotes', 'CreationDateYear', 'LastAccessDateYear', 'MonthOrder',  'MonthsDiff' ]]","94e3d962":"# MonthsDiff is the target feature \n# Split the dataset \n\nX_data = md_df.values[:,:-1]\nY_data = md_df.values[:,7]\nprint(f'Shape of Independent features : {X_data.shape}')\nprint(f'Shape of Dependent features : {Y_data.shape}')\n\nprint('Spliting data into 70-20-10 (train-Cross Validation-test) ratio  ')\nx_train, x_test, y_train, y_test = train_test_split(X_data, Y_data, test_size=0.1, random_state=0)\nx_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=0)\n\nprint(f'Shape of Independent features Train data : {x_train.shape}')\nprint(f'Shape of Dependent features Train data : {y_train.shape}')\nprint(f'Shape of Independent features Cross Validation data : {x_val.shape}')\nprint(f'Shape of Dependent features Cross Validation data : {y_val.shape}')\nprint(f'Shape of Independent features Test data: {x_test.shape}')\nprint(f'Shape of Dependent features Test data: {y_test.shape}')","d790c2db":"models = {'Simple Linear Regression ': LinearRegression(),\n         'Lasso Regression': Lasso(),\n         'Ridge Regression': Ridge(),\n         'ElasticNet Regression': ElasticNet(),\n         'XGB':XGBRegressor()}\n\nres_col_names = []\n\nfor key, v in models.items():\n  res_col_names.append(key)\n\ndef plot_results(x, y, title, x_label, y_label ):\n  fig = plt.figure(figsize=(14, 6))\n  c = [i for i in range(1,x.shape[0] + 1)]\n  plt.plot(c,x, color = 'green', linewidth = 4, label=x_label)\n  plt.plot(c,y, color = 'yellow', linewidth = 1, label=y_label)\n  plt.grid(alpha = 0.3)\n  plt.legend()\n  plt.title(title, fontsize=20)\n\n\ndef model_fit_predict_evaluate(models, X_train, X_test, y_train, y_test, X_val, y_val ):\n    results = {}\n    i = 0\n    for key, model in models.items():\n        predictions = model.fit(X_train, y_train).predict(X_test)\n        r2_test_score = metrics.r2_score(y_test, predictions)\n        mse = metrics.mean_squared_error(y_test, predictions)\n        val_data_score = model.score(X_val, y_val)\n        train_data_score = model.score(X_train, y_train)\n\n        results[i] = [r2_test_score, mse, train_data_score, val_data_score]\n        plot_results(y_test, predictions, 'Actual(Test) vs Predicted using ' + key, 'Test', 'Predicted' )\n        i +=1\n    return pd.DataFrame(results)\n\nresult_df = model_fit_predict_evaluate(models, x_train, x_test, y_train, y_test, x_val, y_val)\nresult_df = result_df.rename(index={0: 'R2_Score', 1: 'MSE', 2:'Train Score', 3:'Val Score'})\nresult_df.columns = res_col_names\n\nresult_df","bf8389c9":"scores_df = result_df.T\nscores_df['model'] = scores_df.index\nscores_df.set_index('model', inplace=True)\nscores_df['R2_Score'].plot(kind='barh', figsize=(10, 6), color='Green');","f6ca9c69":"> Observation(s):\nUser profiles are highly Created in the month of April 2020","7b023f6a":"### Highest Views\/UpVotes\/DownVotes & Recent Creation\/LastAccess Dates","3eed160d":"### New Feature Creation\n#### \"MonthOrder\" : Which is the derived field based on the user profile creation date. Instead of months in between 1 to 12 , i have derived a feature that gives the order how far it is from the starting of the profile creation month.","918a003c":"> Observation(s):\n* CreationDate is almost same for these duplicated accounts, and the ProfileImageUrl also same. But the LastAccessDate & Views fields data is bit different.\n* Hence will proceed to drop the records which are lesser information","3f7da73f":"## Regression plots againt target feature","0432eb9d":"> Observation(s):\n* Accessing the Profiles are use to increasing in trend by year by year\n","84449ab6":"> Observatio(s): \n* Like we discussed earlier, there is lot of sparse data because of this 40k+ records are having location as NaN\/unKnown. \n* as per the rest of the locations we can understand that most of the user profiles are created from UK, US, India, Germany and Canada countries.","4343792e":"> Lets go into deep in each of these missing fields data","2b03e178":"> Observation(s):\n* there is a linear relation we can see between the traget varaible Vs views, Upvotes, Reputation and LastAccessDateYearfeatures","23ebfba2":"## Concise Summary","a7fa3233":"## DTypes","f3e846e7":"## AboutMe Field\n> For now we can ignore this feature and look back if the model does not converge","b5e37b38":"## Drop Duplicated Accounts","6dd9b0ba":"> Observations(s):\n1. Looks like this field is not a good feature for further data processing because some of the data is not at all related to the user. Example: \n    * \"http:\/\/none\" \n    * \"http:\/\/None\"\n    * \"http:\/\/example.com\"\n    * \"http:\/\/somethingelsetodo\"\n    * \"http:\/\/perfectD0TpsychoATgmailD0Tc0m\"\n    * \"http:\/\/localhost\"\n    * http:\/\/----------------------------------\n    \n\n2. Also there are some genuine website urls exists in the data but its a tedious job to extract the data from those sites. Because there are 13k unique web urls with differemt html hierarchy. However if the client requests then we need to do\n\n* Hence not applying any missing data handling technique at this point of time for this field","902ee82a":"> Observation(s) : \n1. There are 74377 user profiles exists. By default all the fields are representing \"object\" data type. Hence we need to reassign to correct data type such as Date\/String\/Numeric\n2. Total 13 fields\/columns are exists. In which, \"WebsiteUrl\", \"Location\", \"AboutMe\", \"AccountId\" and \"ProfileImageUrl\" are having missing values. Hence we need to  dig into the data to understand more on this.\n3. Fix the AccoundId Null values issue immediately because I've assumed AccoundId is the unique id for each user and see the correlation between these features\n","e6080df1":"# XML to DataFrame","355186fa":"## WebsiteUrl Field","344b797a":"## Statistics","d882c1cd":"# Modelling","df9f8612":"> Observation(s): \n* However there are two duplicated Accounts 4089929 & 4869405. Check the data and remove the records if there are no major differences","36cd53f2":"## ProfileImageUrl Field\n> Since this is an image url, we are not extracting any information from this images. Hence we can ignore this feature for further ","dd10873e":"\n# Input \n### Problem Statement : Goal is to predict \"LastAccessDate\" from the domain specific user profile data \n### Data Source :  https:\/\/archive.org\/download\/stackexchange\n### Domain : astronomy, chemistry, law, wordpress so on \n#### Note: I am playing with finance domain data in which file name starts with \"money\". I've manually downloaded data from above mentioned url and added into \"Kaggle\" notebook instance\/session\n\n> Users.xml Schema\n\n* Id (It is the Id of the user with respect to specific site\/domain)\n* Reputation (It is a way to measure user expertise)\n* CreationDate (when the user is created)\n* DisplayName \n* LastAccessDate (Datetime user last loaded a page; updated every 30 min at most)\n* WebsiteUrl\n* Location\n* AboutMe\n* Views (Number of times the profile is viewed by X number of profilers)\n* UpVotes (How many upvotes the user has cast)\n* DownVotes (How many downvotes the user has cast)\n* ProfileImageUrl\n* AccountId (User's Stack Exchange Network profile ID)\n","9e82eb23":"> Observation(s):\n* Higheist Users created in the year 2017 \n* From 2012 to 2017 , there is a positive upward trend in terms of users creation\n* After 2017 onwards the trend is moving downwards","f20621f2":"> Observation(s):\n* Looks like these two rows are having null values for most of the fields such as WebsiteUrl, Location, AboutMe, Views, UpVotes, DownVotes and obviously \"AccountId\".\n* If we closely look into the date fields both the creation and LastAccess dates are same, seems like it is derived field. And the DisplayName and ProfileImageUrl also generated in the runtime. \\\n* Hence there is no much insights from these two rows of data. So proceeding for dropping these rows from the dataframe.","9438e232":"> Observation(s):\nUser profiles are highly accessed in the month of November 2020","4ee4d7ed":"## Uniqueness","2e53ada4":"> Observation(s) :\n* 60% of data is missing  \n* Some pre-processing is required to split the Location content into Country format. However this is not following specific format. For example there are the records like USA \/ US \/ United States \/ United States of America and sometimes the content just refering to city names. So its again required lot of pre-processing to get this content into Country format.\n* Because of lot of sparse data I'm not interested to pre-processing but we can identify the locations in which most of the users exists. ","a8661b7b":"# EDA","1a656d55":"> Observation(s):\n* WebsiteUrl, Location and AboutMe fields are having more than 60% of missing values","dd28e011":"> > > Observation(s) : \n* Views , upvotes and reputation are positively correlated. ","32b3a355":"# Missing Data","d86be7f0":"## Location Field ","0ccff7de":"# EDA on Date & Numeric Features \n","2851024d":"## Delete NaN Accounts"}}