{"cell_type":{"6820fb9c":"code","141a9819":"code","4b4bbf22":"code","11f4c588":"code","0a6797ac":"code","7f163933":"code","d9d233c1":"code","c2699e27":"code","46c804bd":"code","8dc8d660":"code","b524242b":"code","695dbdad":"code","b0697423":"code","125e0f6e":"code","9027a772":"code","7ad0a917":"code","d8ce3ef4":"code","cf8f1eab":"code","ec871e63":"code","520ddd64":"code","5064374e":"code","bc48f956":"code","3cca8797":"code","ce10c265":"code","85471ecd":"code","a81c473d":"code","f9c6147f":"code","88625ffc":"code","d1e10f3c":"code","ad875d81":"code","f18ec2ae":"code","5593040e":"code","7baf3644":"code","454c25d4":"code","2b5a07c5":"code","582c76fc":"code","c7df6634":"code","7ebf0444":"code","d6016b6b":"code","9540f17b":"code","2feded86":"code","7fbd9055":"code","0db9cb02":"code","d32de964":"code","c5051c55":"code","feac8503":"code","0159e671":"code","7f6a40c0":"code","5c302eb3":"code","fc74494a":"code","03a7b498":"code","52fef625":"code","639dd2fd":"code","f8feb26b":"code","e2d267df":"code","21aa4aa1":"code","a2cb1a51":"code","a6b8cb2f":"code","542a1ad1":"markdown","7dcde266":"markdown","1d182723":"markdown","e457f51f":"markdown","3f988905":"markdown","7c9ce411":"markdown","34922366":"markdown","bc4e565e":"markdown","cba0a2fe":"markdown","be9da1d5":"markdown","c106e9e0":"markdown","bbcbda5a":"markdown","ed51b32a":"markdown","761568db":"markdown","76ec3bd6":"markdown","925d57de":"markdown","60b4a735":"markdown","c92c85c7":"markdown","cfb0d0bb":"markdown","d9a2743e":"markdown","16493659":"markdown","55dddae2":"markdown","89ec1e99":"markdown","582ca254":"markdown"},"source":{"6820fb9c":"# Import libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport os\nimport os.path\nimport matplotlib.pyplot as plt\n!pip install klib\nimport klib\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve, auc, precision_recall_curve, classification_report, average_precision_score\n\nfrom sklearn.ensemble import ExtraTreesClassifier\n\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import precision_score,f1_score\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.linear_model import LogisticRegression\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom sklearn import preprocessing\nfrom sklearn.metrics import mean_squared_error\n\n# import packages for hyperparameters tuning\nfrom hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n\n\nimport warnings\nwarnings.filterwarnings(action='ignore')","141a9819":"#input data\nroot_dir = '\/kaggle\/input\/widsdatathon2021\/'\n\nsample_submission = pd.read_csv(os.path.join(root_dir, \n                    'SampleSubmissionWiDS2021.csv'))\nsolution_template = pd.read_csv(os.path.join(root_dir, \n                    'SolutionTemplateWiDS2021.csv'))\nData_dictionary = pd.read_csv(os.path.join(root_dir,   \n                   'DataDictionaryWiDS2021.csv'))\nUnlabled_data = pd.read_csv(os.path.join(root_dir,     \n                'UnlabeledWiDS2021.csv'))\nTraining_data = pd.read_csv(os.path.join(root_dir,     \n                'TrainingWiDS2021.csv'))","4b4bbf22":"display(Data_dictionary.shape)\ndisplay(Unlabled_data.shape)\ndisplay(Training_data.shape)\ndisplay(solution_template.shape)","11f4c588":"Training_data.isna().any().any()","0a6797ac":"Training_data.drop('Unnamed: 0', axis = 1, inplace = True)","7f163933":"display(Training_data.shape)","d9d233c1":"display(Unlabled_data.shape)\nUnlabled_data.head()","c2699e27":"Unlabled_data.isna().any().any()","46c804bd":"Unlabled_data.drop('Unnamed: 0', axis = 1, inplace = True)","8dc8d660":"display(Unlabled_data.shape)\nUnlabled_data.head()","b524242b":"Training_data.info(verbose=True, null_counts=True)","695dbdad":"Training_data.dtypes.value_counts()","b0697423":"Training_data.isnull().sum()","125e0f6e":"def calc_missing_values(df_name):\n    \n    '''\n    Returns total number and percentage of missing value in each column of a\n    given dataframe.    \n    '''\n    # sum of missing values in each column\n    missing_values = df_name.isnull().sum() \n    \n    # percentage of missing values in each column\n    per_missing = df_name.isnull().sum() * 100 \/ len(df_name)\n    \n    # Table with sum and percentage of missing values\n    missing_table = pd.concat([missing_values, per_missing],axis = 1)\n        \n    # Assign column names\n    missing_table_rename = missing_table.rename(columns ={0: 'Missing Values', 1:'% of missing values'})\n    \n    # Sort it by percentage of missing values\n    \n    sorted_table = missing_table_rename[missing_table_rename.iloc[:,1] !=0].\\\n    sort_values('% of missing values', ascending = False).round(1)\n    \n    print('Out of ' + str(df_name.shape[1])+ ' columns in this dataframe '+ str(sorted_table.shape[0])+ \\\n                         ' columns have missing values')\n    \n    return sorted_table\n        ","9027a772":"# Training data\nmissing_train = calc_missing_values(Training_data)\nmissing_train[:20].style.background_gradient(cmap='viridis')","7ad0a917":"# Test data\nmissing_test = calc_missing_values(Unlabled_data)\nmissing_test[:20].style.background_gradient(cmap='cividis')","d8ce3ef4":"train_df = klib.data_cleaning(Training_data) # removes duplicate and empty row\/col","cf8f1eab":"test_df = klib.data_cleaning(Unlabled_data) # removes duplicate and empty row\/cols#","ec871e63":"#train_df = Training_data\n#test_df = Unlabled_data","520ddd64":"train_df['diabetes_mellitus'].value_counts(normalize = True)","5064374e":"train_df['diabetes_mellitus'].astype(int).plot.hist();","bc48f956":" train_df.dtypes","3cca8797":"cat_col_train = Training_data.select_dtypes('object').columns\ndisplay(len(cat_col_train))\ndisplay(cat_col_train)","ce10c265":"cat_col_test = Unlabled_data.select_dtypes('object').columns\ndisplay(len(cat_col_test))\ndisplay(cat_col_test)","85471ecd":"#klib.cat_plot(test_df)","a81c473d":"cat_list = Training_data.select_dtypes('object').columns\ndisplay(cat_list)","f9c6147f":"# Creating Label Encoder object\nle = LabelEncoder()\nfor ob in cat_list:\n    train_df[ob] = le.fit_transform(train_df[ob].astype(str))\n    test_df[ob] = le.fit_transform(test_df[ob].astype(str))\nprint(train_df.info())    \nprint(test_df.info()) ","88625ffc":"train_df.fillna(-9999,inplace = True)\ntrain_df.isnull().sum()","d1e10f3c":"test_df.fillna(-9999,inplace = True)\ntest_df.isnull().sum()","ad875d81":"Target = 'diabetes_mellitus'\ntrain_labels = train_df[Target]\ntrain_df_NT = train_df.drop(columns = [Target])\nfeatures = list(train_df_NT.columns)\nprint('Training data shape:', train_df_NT.shape)\nprint('Test data shape:', test_df.shape)","f18ec2ae":"X, y = train_df_NT, train_labels","5593040e":"#create the train and validation set for cross-validation\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=123)","7baf3644":"#Instantiate an XGBoost classifier object by calling the XGBClassifier() class from the XGBoost \n#library with the hyper-parameters passed as arguments.\nxgb_cls = xgb.XGBClassifier(\n    max_depth = 5,\n    subsample = 1,\n    colsample_bytree = 0.7,\n    colsample_bylevel = 0.7,\n    scale_pos_weight = 1,\n    min_child_weight = 1,\n    reg_alpha = 4,\n    n_jobs = 4, \n    objective = 'binary:logistic',\n    gamma = 0.01,\n    nthread=20,\n    seed = 27,\n    n_estimators=1000,\n)","454c25d4":"#Fit the classifier to the training set and make predictions on the \n#test set using the familiar .fit() and .predict() methods\nxgb_cls.fit(X_train,y_train)\n\ny_pred = xgb_cls.predict_proba(X_val)","2b5a07c5":"y_scores = y_pred[:,1]","582c76fc":"fpr, tpr, thres = roc_curve(y_val, y_scores)\nroc_auc = auc(fpr, tpr)\naverage_precision = average_precision_score(y_val, y_scores)\nprecision, recall, _ = precision_recall_curve(y_val, y_scores)\nprint(roc_auc)","c7df6634":"def roc_curve_plot(fpr, tpr):\n    plt.plot(fpr, tpr, color='purple', label='ROC')\n    plt.plot([0, 1], [0, 1], color='yellow', linestyle='--')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic (ROC) Curve')\n    plt.legend()\n    plt.show()","7ebf0444":"roc_curve_plot(fpr, tpr)\nprint('AUC :',roc_auc_score(y_val,y_scores))","d6016b6b":"xgb.plot_tree(xgb_cls,num_trees=0)\nplt.rcParams['figure.figsize'] = [50, 20]\nplt.show()","9540f17b":"# a bar chart of the relative importances.\nplt.bar(range(len(xgb_cls.feature_importances_)), xgb_cls.feature_importances_)\nplt.show()","2feded86":"xgb.plot_importance(xgb_cls)\nplt.rcParams['figure.figsize'] = [50, 15]\nplt.show()","7fbd9055":"df_feature_importance = pd.DataFrame(xgb_cls.feature_importances_, index=features, columns=['feature importance']).sort_values('feature importance', ascending=False)\ndf_feature_importance","0db9cb02":"#relevant_features = [ele for i,ele in enumerate(df_feature_importance.index) if  i <=116]\nrelevant_features = [ele for i,ele in enumerate(df_feature_importance.index) if df_feature_importance.iloc[i].values[0] != 0.0]","d32de964":"# dataframe with relevant features only\nX_nf = pd.DataFrame()\nfor col in relevant_features:\n    X_nf[col] = train_df_NT[col]   ","c5051c55":"# test dataframe with relevant features only\nX_test_nf = pd.DataFrame()\nfor col in relevant_features:\n    X_test_nf[col] = test_df[col] ","feac8503":"#create the train and validation set for cross-validation\nX_nf_train, X_nf_val, y_nf_train, y_nf_val = train_test_split(X_nf, y, test_size=0.2, random_state=123)","0159e671":"#Instantiate an XGBoost classifier object by calling the XGBClassifier() class from the XGBoost \n#library with the hyper-parameters passed as arguments.\nxgb_cls_nf = xgb.XGBClassifier(\n    max_depth = 5,\n    subsample = 1,\n    colsample_bytree = 0.7,\n    colsample_bylevel = 0.7,\n    scale_pos_weight = 1,\n    min_child_weight = 1,\n    reg_alpha = 4,\n    n_jobs = 4, \n    objective = 'binary:logistic',\n    nthread=20,\n    gamma = 0.01,\n    seed = 27,\n    n_estimators=1000,\n)","7f6a40c0":"#Fit the classifier to the training set and make predictions on the \n#test set using the familiar .fit() and .predict() methods\nxgb_cls_nf.fit(X_nf_train,y_nf_train)\n\ny_pred_nf = xgb_cls_nf.predict_proba(X_nf_val)","5c302eb3":"y_scores_nf = pd.Series(y_pred_nf[:, 1])","fc74494a":"fpr_nf, tpr_nf, thres_nf = roc_curve(y_nf_val, y_scores_nf)\nroc_auc_nf = auc(fpr_nf, tpr_nf)\naverage_precision_nf = average_precision_score(y_nf_val, y_scores_nf)\nprecision_nf, recall_nf, _ = precision_recall_curve(y_nf_val, y_scores_nf)\nprint(roc_auc_nf)","03a7b498":"dtrain = xgb.DMatrix(X_nf_train, label=y_nf_train)\ndval = xgb.DMatrix(X_nf_val, label=y_nf_val)\ndtest = xgb.DMatrix(X_test_nf)","52fef625":"# parameters# parameters\nparam = {\n    'max_depth': 5,\n    'subsample': 1,\n    'colsample_bytree': 0.7,\n    'colsample_bylevel': 0.7,\n    'scale_pos_weight': 1,\n    'min_child_weight': 1,\n    'reg_alpha': 4,\n    'n_jobs': 4, \n    'objective': 'binary:logistic',\n    'nthread': 4,\n    'gamma': 0.01,\n    'seed': 27,\n    }\nnum_round=100   # the number of training iterations","639dd2fd":"bst = xgb.train(param, dtrain, num_round)","f8feb26b":"preds = bst.predict(dval)","e2d267df":"#For each line you need to select that column where the probability is the highest\nbest_preds = np.asarray([np.argmax(line) for line in preds])","21aa4aa1":"from sklearn.metrics import precision_score\nprint(precision_score(y_nf_val, best_preds, average='macro'))","a2cb1a51":"ypred = bst.predict(dtest)","a6b8cb2f":"# Submission dataframe\nsubmit = test_df[['encounter_id']]\nsubmit['diabetes_mellitus'] = ypred\nsubmit.to_csv('xgb_cls.csv',index=False)\nsubmit.head()","542a1ad1":"## Target Column in Training data","7dcde266":"## Test data","1d182723":"# Missing  data handling","e457f51f":"Training data has 130157 entries and 181 variables. ","3f988905":"## <span style='color:purple'>  Import libraries <\/span>","7c9ce411":"There are some missing data in training data.","34922366":"There is class imbalance in this dataset.","bc4e565e":"# <span style='color:purple'>  Women in Data Science Datathon 2021       <\/span>\n\n<div style=\"text-align: justify;\n             font-size:18px\">\nObjective of <span style='color:purple'> WiDS Datathon 2021  <\/span> is to develop models and make predictions to determine whether a patient admitted to ICU has been diagnosed with a particular type of diabetes, Diabetes Mellitus, using labeled training data from the first 24 hours of intensive care.\n    <\/div>","cba0a2fe":"### Columns","be9da1d5":"## <span style='color:purple'> Input data <\/span>","c106e9e0":"### Number of unique column datatypes ","bbcbda5a":"### Column types in training and test data","ed51b32a":"## Training data","761568db":"# Using only relevant features","76ec3bd6":"## Missing values","925d57de":"## <span style='color:purple'> Data Exploration <\/span>\n","60b4a735":"Training data has 130157 entries and 180 variables. ","c92c85c7":"Test data has 10234 entries and 179 variables which is 1 less than the training data due to the presence of TARGET column.","cfb0d0bb":"This column do not contain useful information so let's drop it.","d9a2743e":"# Categorical features","16493659":"Selecting only relevant features.","55dddae2":"# Missing Values","89ec1e99":"There are three unique datatypes.","582ca254":"# Feature Selection"}}