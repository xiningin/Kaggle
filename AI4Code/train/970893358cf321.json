{"cell_type":{"66014589":"code","c9e8bd3b":"code","a63d8bc2":"code","7d1c444e":"code","8ee13146":"code","91648ece":"code","9034a863":"code","ca7f8098":"code","931d8dbc":"code","c0e12ebc":"code","374a2ae5":"code","c8145494":"code","9fa116ba":"code","da07c74c":"code","aae14382":"code","aa4c4a2d":"code","d970bb32":"code","a19be61e":"code","5361aa0e":"code","6a56ead6":"code","f5a04f9a":"code","f3944b81":"code","3acfb9e1":"code","010307a6":"code","17e980d4":"code","942986d1":"code","dd598c53":"code","808deb67":"code","3252a3f9":"code","538cf472":"code","721b9bf0":"code","5d8532c4":"code","cbbd3cf7":"code","4cc040a4":"code","d1570d0d":"code","9b078b71":"code","6752245d":"code","ba17c5cd":"code","c85c4fa9":"code","0a24dedd":"code","77265520":"code","1e60bd98":"code","947d59d8":"code","6a3c7967":"code","a3b63f67":"code","7693df96":"code","d2e60cb2":"code","08d88abd":"code","b26908a7":"code","d84d8c6b":"code","c524e372":"code","7612d4ca":"code","8c46ed3b":"code","19144bdc":"code","83b3eaa0":"markdown","b30f659e":"markdown","decd79f4":"markdown","12bc7052":"markdown","10aac52b":"markdown","b6894feb":"markdown","63c36fdd":"markdown","17573bc8":"markdown","1efea4ac":"markdown","e1bd49ee":"markdown","8f4889ae":"markdown","f9e4a005":"markdown","20b5a0bd":"markdown","489eab5d":"markdown","2fe38a84":"markdown","eaac3468":"markdown","66e22c49":"markdown","f47f6f81":"markdown","25c66990":"markdown","f40b309b":"markdown","ac7e86db":"markdown","3e926481":"markdown","7ddd13de":"markdown","e1a72a4a":"markdown","3b47bbcf":"markdown","a16d6355":"markdown","840923d4":"markdown","8346b90c":"markdown","ec69b3ec":"markdown","07df1620":"markdown","31280649":"markdown","b61511cd":"markdown","1c993a58":"markdown","4817162a":"markdown","9d50f4bc":"markdown","0aaa9af9":"markdown","8a17463f":"markdown","5f15f272":"markdown","d4c85aad":"markdown","0728076d":"markdown","fbddc699":"markdown","0fcbee28":"markdown","00d7fccd":"markdown","a894751a":"markdown","33a5803c":"markdown","1bc87ae3":"markdown","ab0eae1c":"markdown"},"source":{"66014589":"# General libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import mean_squared_error \nfrom math import sqrt\n\n# Models \nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score \nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import svm\nfrom sklearn import neighbors\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Visualization\nimport seaborn as sns \nimport matplotlib.pyplot as plt \nimport graphviz\nfrom sklearn import tree\n\n\n%matplotlib inline\nsns.set(color_codes=True)","c9e8bd3b":"# read file\ndf = pd.read_csv(\"..\/input\/mushroom-classification\/mushrooms.csv\")","a63d8bc2":"# display top 5 rows \ndf.head()","7d1c444e":"# display bottom 5 rows\ndf.tail()","8ee13146":"# checking the data type\ndf.dtypes","91648ece":"# checking duplicated data\ndf.shape\nduplicate_rows = df[df.duplicated()]\nprint(\"number of duplicated rows: \", duplicate_rows) \n\n### no duplicated rows","9034a863":"# checking null values\nprint(df.isnull().sum())\n\n### no missing values ","ca7f8098":"# find unique values for each variable  \ndf.nunique()","931d8dbc":"df = df.drop(['veil-type'], axis=1)","c0e12ebc":"# use labelencoding to convert categorical data to numerical \nfrom sklearn.preprocessing import LabelEncoder\ndf_encoded = df.copy()\nle=LabelEncoder()\nfor col in df_encoded.columns:\n    df_encoded[col] = le.fit_transform(df_encoded[col])\n\ndf_encoded.head()","374a2ae5":"df_encoded.describe()","c8145494":"# barplots with hus of class (e,p)\nsns.catplot(x='class',kind='count',palette='ch:.25',data=df)\nsns.catplot(x='cap-shape',kind='count',palette='ch:.25',data=df,hue='class')\nsns.catplot(x='cap-surface',kind='count',palette='ch:.25',data=df,hue='class')\nsns.catplot(x='bruises',kind='count',palette='ch:.25',data=df,hue='class')\nsns.catplot(x='odor',kind='count',palette='ch:.25',data=df,hue='class')\nsns.catplot(x='gill-attachment',kind='count',palette='ch:.25',data=df,hue='class')\nsns.catplot(x='gill-spacing',kind='count',palette='ch:.25',data=df,hue='class')\nsns.catplot(x='gill-size',kind='count',palette='ch:.25',data=df,hue='class')\nsns.catplot(x='gill-color',kind='count',palette='ch:.25',data=df,hue='class')\nsns.catplot(x='stalk-shape',kind='count',palette='ch:.25',data=df,hue='class')\nsns.catplot(x='stalk-root',kind='count',palette='ch:.25',data=df,hue='class')\nsns.catplot(x='stalk-surface-above-ring',kind='count',palette='ch:.25',data=df,hue='class')\nsns.catplot(x='stalk-surface-below-ring',kind='count',palette='ch:.25',data=df,hue='class')\nsns.catplot(x='stalk-color-above-ring',kind='count',palette='ch:.25',data=df,hue='class')\nsns.catplot(x='stalk-color-below-ring',kind='count',palette='ch:.25',data=df,hue='class')\nsns.catplot(x='veil-color',kind='count',palette='ch:.25',data=df,hue='class')\nsns.catplot(x='ring-number',kind='count',palette='ch:.25',data=df,hue='class')\nsns.catplot(x='ring-type',kind='count',palette='ch:.25',data=df,hue='class')\nsns.catplot(x='spore-print-color',kind='count',palette='ch:.25',data=df,hue='class')\nsns.catplot(x='population',kind='count',palette='ch:.25',data=df,hue='class')\nsns.catplot(x='habitat',kind='count',palette='ch:.25',data=df,hue='class')","9fa116ba":"matrix = np.triu(df_encoded.corr())\nplt.subplots(figsize=(20,15))\nsns.heatmap(df_encoded.corr(), annot=True, mask=matrix, xticklabels=True, yticklabels=True)","da07c74c":"# split data into x and y\nx = df_encoded.iloc[:, 1:]\ny = df_encoded.iloc[:, 0:1]","aae14382":"X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.20)","aa4c4a2d":"rmse_val = [] #to store rmse values for different k\nfor K in range(20):\n    K = K+1\n    model = neighbors.KNeighborsRegressor(n_neighbors = K)\n\n    model.fit(X_train, y_train)  #fit the model\n    pred=model.predict(X_test) #make prediction on test set\n    error = sqrt(mean_squared_error(y_test,pred)) #calculate rmse\n    rmse_val.append(error) #store rmse values\n    print('RMSE value for k= ' , K , 'is:', error)","d970bb32":"curve = pd.DataFrame(rmse_val) \ncurve.plot()","a19be61e":"classifier = KNeighborsClassifier(n_neighbors=2)\nclassifier.fit(X_train, y_train)\ny_pred = classifier.predict(X_test)","5361aa0e":"print(\"KNN accuracy on testing set is:\",metrics.accuracy_score(y_test, y_pred).round(3))\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))","6a56ead6":"logreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\ny_pred = logreg.predict(X_test)","f5a04f9a":"print(\"Logistic Regression accuracy on testing set is:\",metrics.accuracy_score(y_test, y_pred).round(3))\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))","f3944b81":"clf_scm = svm.SVC(kernel='linear') # Linear Kernel\nclf_scm.fit(X_train, y_train)\ny_pred = clf_scm.predict(X_test)","3acfb9e1":"print(\"SVM accuracy on testing set is:\",metrics.accuracy_score(y_test, y_pred).round(3))\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))","010307a6":"def print_results(results):\n    print('BEST PARAMS: {}\\n'.format(results.best_params_))\n\n    means = results.cv_results_['mean_test_score']\n    stds = results.cv_results_['std_test_score']\n    for mean, std, params in zip(means, stds, results.cv_results_['params']):\n        print('{} (+\/-{}) for {}'.format(round(mean, 3), round(std * 2, 3), params))","17e980d4":"tree_clf = DecisionTreeClassifier()\n\nparameters = {\"min_samples_split\":[50,100,200],\n             \"max_depth\":[1,5,10,50,100]}\n\ntree_clf.fit(X_train,y_train) # fit the model","942986d1":"# find the best model use scoring = balanced_accuracy\ngrid_cv = GridSearchCV(estimator=tree_clf, param_grid=parameters, cv=5,scoring=\"balanced_accuracy\") \ngrid_cv.fit(X_train,y_train) \n\nprint_results(grid_cv)","dd598c53":"print(\"SVM accuracy on testing set is:\",metrics.accuracy_score(y_test, y_pred).round(3))\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))","808deb67":"dot_data = tree.export_graphviz(grid_cv.best_estimator_, out_file=None, filled=True)\n\ngraph = graphviz.Source(dot_data, format=\"png\") \ngraph","3252a3f9":"rf = RandomForestClassifier()\nrf.fit(X_train,y_train)\ny_pred = rf.predict(X_test)","538cf472":"print(\"Random Forest accuracy on testing set is:\",metrics.accuracy_score(y_test, y_pred).round(3))\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))","721b9bf0":"clf_SGD = SGDClassifier()\nclf_SGD.fit(X_train, y_train)\ny_pred = clf_SGD.predict(X_test)","5d8532c4":"print(\"SGD accuracy on testing set is:\",metrics.accuracy_score(y_test, y_pred).round(3))\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))","cbbd3cf7":"ac = AdaBoostClassifier()\nac.fit(X_train,y_train)\ny_pred = ac.predict(X_test)","4cc040a4":"print(\"AdaBoost accuracy on testing set is:\",metrics.accuracy_score(y_test, y_pred).round(3))\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))","d1570d0d":"lr_clf = LogisticRegression()\n\nparameters = {\"solver\":['newton-cg', 'lbfgs', 'liblinear'],\n              \"penalty\": ['l2'],\n              \"C\": [100, 10, 1.0, 0.1, 0.01]}\n\nlr_clf.fit(X_train,y_train) \n\ngrid_cv1 = GridSearchCV(estimator=lr_clf, param_grid=parameters, cv=5,scoring=\"balanced_accuracy\") \ngrid_cv1.fit(X_train,y_train) \n\nprint_results(grid_cv1)","9b078b71":"lr_clf = LogisticRegression(C = 100,penalty = 'l2',solver='newton-cg')\nlr_clf= lr_clf.fit(X_train,y_train)\ny_pred1 = lr_clf.predict(X_test)","6752245d":"from sklearn.svm import SVC","ba17c5cd":"svm_clf = SVC()\n\nparameters = {\n    'C': [0.1, 0.5, 1, 2, 5, 10, 20],\n    'gamma': ['scale', 'auto', 1, 2, 3]\n    }\n\nsvm_clf.fit(X_train,y_train) \n\ngrid_cv2 = GridSearchCV(estimator=svm_clf, param_grid=parameters, cv=5,scoring=\"balanced_accuracy\") \ngrid_cv2.fit(X_train,y_train) \n\nprint_results(grid_cv2)","c85c4fa9":"svm_clf = SVC(kernel ='rbf', C = 2, gamma = 'auto')\nsvm_clf= svm_clf.fit(X_train,y_train)\ny_pred2 = svm_clf.predict(X_test)","0a24dedd":"tree_clf = DecisionTreeClassifier()\n\nparameters = {\"min_samples_split\":[50,100,200],\n             \"max_depth\":[1,5,10,50,100]}\n\ntree_clf.fit(X_train,y_train) ","77265520":"grid_cv1 = GridSearchCV(estimator=tree_clf, param_grid=parameters, cv=5,scoring=\"balanced_accuracy\") \ngrid_cv1.fit(X_train,y_train) \n\nprint_results(grid_cv1)","1e60bd98":"clf1 = DecisionTreeClassifier(max_depth=10,min_samples_split=50)\nclf1= clf1.fit(X_train,y_train)\ny_pred3 = clf1.predict(X_test)","947d59d8":"rf = RandomForestClassifier()\nrf.fit(X_train,y_train)\n\nrf_parameters = {\"min_samples_split\":[50,100,200],\n             \"max_depth\":[1,5,10,50,100],\n             \"n_estimators\":[5, 50, 250, 500]}\n\ngrid_cv4 = GridSearchCV(estimator=rf, param_grid=rf_parameters, cv=5,scoring=\"balanced_accuracy\") \ngrid_cv4.fit(X_train,y_train) \n\nprint_results(grid_cv4)","6a3c7967":"rf_clf = RandomForestClassifier(max_depth=10,min_samples_split=50,n_estimators=50)\nrf_clf = rf_clf.fit(X_train,y_train)\ny_pred4 = rf_clf.predict(X_test)","a3b63f67":"sgd = SGDClassifier()\nsgd.fit(X_train,y_train)\n\nparameters = {'alpha': [1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3], \n                'penalty': ['l2'],\n                'n_jobs': [-1]}\n\ngrid_cv5 = GridSearchCV(estimator=sgd, param_grid=parameters, cv=5,scoring=\"balanced_accuracy\") \ngrid_cv5.fit(X_train,y_train) \n\nprint_results(grid_cv5)","7693df96":"sgd_clf = SGDClassifier(alpha=0.001, n_jobs=-1, penalty= 'l2')\nsgd_clf = sgd_clf.fit(X_train,y_train)\ny_pred5 = sgd_clf.predict(X_test)","d2e60cb2":"ab = AdaBoostClassifier()\nab.fit(X_train,y_train)\n\nparameters = {'n_estimators':[500,1000,2000],\n              'learning_rate':[.001,0.01,.1]}\n\ngrid_cv6 = GridSearchCV(estimator=ab, param_grid=parameters, cv=5,scoring=\"balanced_accuracy\") \ngrid_cv6.fit(X_train,y_train) \n\nprint_results(grid_cv6)","08d88abd":"ab = AdaBoostClassifier(n_estimators=1000,learning_rate=0.1)\nab = ab.fit(X_train,y_train)\ny_pred6 = ab.predict(X_test)","b26908a7":"print(\"Logistic Regression accuracy on testing set is:\",metrics.accuracy_score(y_test, y_pred1))\nprint(classification_report(y_test, y_pred1))","d84d8c6b":"print(\"SVM accuracy on testing set is:\",metrics.accuracy_score(y_test, y_pred2))\nprint(classification_report(y_test, y_pred2))","c524e372":"print(\"Decision Tree accuracy on testing set is:\",metrics.accuracy_score(y_test, y_pred3))\nprint(classification_report(y_test, y_pred3))","7612d4ca":"print(\"Logistic Regression accuracy on testing set is:\",metrics.accuracy_score(y_test, y_pred1))\nprint(classification_report(y_test, y_pred4))","8c46ed3b":"print(\"SGD accuracy on testing set is:\",metrics.accuracy_score(y_test, y_pred5))\nprint(classification_report(y_test, y_pred5))","19144bdc":"print(\"Adaboost accuracy on testing set is:\",metrics.accuracy_score(y_test, y_pred6))\nprint(classification_report(y_test, y_pred6))","83b3eaa0":"#### 7 a d. Random Forest","b30f659e":"When k = 2, RMSE has the smallest value of 0.021. It is safe to say that k=3 will give us the best model.","decd79f4":"### 3c. Correlation Matrix","12bc7052":"### Decision Tree","10aac52b":"In this part, I created a correlation matrix heatmap to get a better understanding on the relationships between each feature. Based on the heatmap, veil-color and gill-attachment are highly positively correlated with a correlation of 0.9. Gill-spacing and population, gill-color and target variable are highly negatively correlated with a correlation of -0.53. Ring-type and bruises also have a high negative correlation with our target variable. ","b6894feb":"### Support Vector Machine","63c36fdd":"In this part, I'm using Bar Plots to understand the difference between each features based on our target variable (hue = 'class').\n\nAccording to these plots, the amount of edible and poisonous are very similar, which helps us to prevent bias towards one class. Mushrooms with a cap-shape b has a higher count on being edible, whereas cap-shape k has a higher count on being poisonous. Cap-surface with a f has a higher count for being edible. \n\nA significanr difference in gill-color = buff between edible and poisonous. It shows us mushrooms with gill-color = buff might have a more chance to be considered for poisonous. The assumption applies to when stalk-surface-above-ring = k. \n\nSpore-print-color also has significant difference between edible and posnonous. k and n have a higher edible count, where h and w have a higher poisonous count.","17573bc8":"I will remove veil-type since it only has 1 unique value. It will not be very useful for this project. ","1efea4ac":"# Mushroom Classification\n\n1. Introduction\n2. Libraries\n3. Exploratory Analysis\n    * 3a. Statistical Summary\n    * 3b. Bar Plot\n    * 3c. Correlation Heatmap\n4. Test & Train Split\n5. Machine Learning Models\n6. Experiment Results & Next Steps\n7. Methods to Improve Models\n    * 7a. Parameter Tunning\n    * 7b. Comparing results before and after Parameter Tunning\n8. Conclusion","e1bd49ee":"### KNN Classification","8f4889ae":"Accuracy and f1-score stayed the same.","f9e4a005":"f1-score and accuracy increased by 0.03. ","20b5a0bd":"#### 7 b e. AdaBoost","489eab5d":"#### 7 a c. Decision Tree","2fe38a84":"## 1. Introduction","eaac3468":"#### 7 b b. Logistic Regression","66e22c49":"## 4. Train and Test Split","f47f6f81":"### Stochastic Gradient Descent","25c66990":"## 3. Exploratory Data Analysis ","f40b309b":"### 7 a. Applying Hyperparameter Tunning","ac7e86db":"#### 7 b e. Random Forest ","3e926481":"After applying these models, I got some great results. KNN, Random Forest, and AdaBoost have perfect prediction on the testing data set, with accruacy and macro average of 1. Stochastic Gradient Descent has the worst performance among all, with an accruacy and macro average of 0.94.\n\nNext step, I will apply hyperparamter tunning on the models and comparing the results between different methods. My goal is to discover if these methods will help my models to perform better.","7ddd13de":"#### 7 a b. Support Vector Machine","e1a72a4a":"Accuracy and f1-score stayed the same.","3b47bbcf":"#### 7 a a. Logistic Regression","a16d6355":"## 7. Methods to Improve Models","840923d4":"### AdaBoost ","8346b90c":"## 8. Conclusion","ec69b3ec":"#### 7 a e. Stochastic Gradient Descent","07df1620":"**Xintong Li**\n\n**Python Classification Project**\n\n**11\/24\/2020**","31280649":"#### 7 b d. Decision Tree","b61511cd":"After applying parameter tunning, f1-score and accuracy for Logistic Regression increased 0.02. ","1c993a58":"Accuracy score on testing set decreased by 0.03. However, f1-score stayed the same.","4817162a":"#### 7 a f. Adaboost","9d50f4bc":"As someome who loves mushroom, I want to explore what the key features of a poisonous mushroom. I will be doing Exploratory Analysis to detect any interesting patterns or relationships between each features and its target variable (classes: edible=e, poisonous=p). On top of that, I will apply different Machine Learning Algrithoms to find the best model to predict whether a mushroom is edible or poisonous. Some models I will be using are KNN, Logistic Regression, Support Vector Machine, Decision Tree, Random Forest, Stochastic Gradient Descentn and AdaBoost.\n\nThis mushroom data set I found is from Kaggle: https:\/\/www.kaggle.com\/uciml\/mushroom-classification. It includes 23 features and 8124 rows. Here is a list of the features' description from Kaggle:\n\n**Attribute Information: (classes: edible=e, poisonous=p)**\n* cap-shape: bell=b,conical=c,convex=x,flat=f, knobbed=k,sunken=s\n* cap-surface: fibrous=f,grooves=g,scaly=y,smooth=s\n* cap-color: brown=n,buff=b,cinnamon=c,gray=g,green=r,pink=p,purple=u,red=e,white=w,yellow=y\n* bruises: bruises=t,no=f\n* odor: almond=a,anise=l,creosote=c,fishy=y,foul=f,musty=m,none=n,pungent=p,spicy=s\n* gill-attachment: attached=a,descending=d,free=f,notched=n\n* gill-spacing: close=c,crowded=w,distant=d\n* gill-size: broad=b,narrow=n\n* gill-color: black=k,brown=n,buff=b,chocolate=h,gray=g, green=r,orange=o,pink=p,purple=u,red=e,white=w,yellow=y\n* stalk-shape: enlarging=e,tapering=t\n* stalk-root: bulbous=b,club=c,cup=u,equal=e,rhizomorphs=z,rooted=r,missing=?\n* stalk-surface-above-ring: fibrous=f,scaly=y,silky=k,smooth=s\n* stalk-surface-below-ring: fibrous=f,scaly=y,silky=k,smooth=s\n* stalk-color-above-ring: brown=n,buff=b,cinnamon=c,gray=g,orange=o,pink=p,red=e,white=w,yellow=y\n* stalk-color-below-ring: brown=n,buff=b,cinnamon=c,gray=g,orange=o,pink=p,red=e,white=w,yellow=y\n* veil-type: partial=p,universal=u\n* veil-color: brown=n,orange=o,white=w,yellow=y\n* ring-number: none=n,one=o,two=t\n* ring-type: cobwebby=c,evanescent=e,flaring=f,large=l,none=n,pendant=p,sheathing=s,zone=z\n* spore-print-color: black=k,brown=n,buff=b,chocolate=h,green=r,orange=o,purple=u,white=w,yellow=y\n* population: abundant=a,clustered=c,numerous=n,scattered=s,several=v,solitary=y\n* habitat: grasses=g,leaves=l,meadows=m,paths=p,urban=u,waste=w,woods=d","0aaa9af9":"### Logistic Regression","8a17463f":"### 3a. Statistical Summary","5f15f272":"## 5. Machine Learning Models","d4c85aad":"### 7 b. Comparing Results: before and after parameter tunning ","0728076d":"#### 7 b f. SGD","fbddc699":"### 3b. Bar Plots","0fcbee28":"## 6. Experiment Results","00d7fccd":"## 2. Libraries","a894751a":"#### 7 b c. Support Vector Machine","33a5803c":"### Random Forest","1bc87ae3":"f1-score and accuracy increased by 0.02. ","ab0eae1c":"The best models I got were AdaBoost and Support Vector Machine after parameter tunning, with a perfect accuracy and f1-score. The features that effect the target variable the most are: gill-size and gill-color. "}}