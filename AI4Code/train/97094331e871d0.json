{"cell_type":{"731c059f":"code","2dee658d":"code","9dbf0e94":"code","ea2126b9":"code","3468270f":"code","a571c490":"code","feacfdf8":"code","42e249c9":"code","f09dba0c":"code","b69596bb":"code","d48fb098":"code","da5d0fe9":"code","b22d8452":"code","60ef785e":"code","3b45d26b":"code","b987ac87":"code","23e045e4":"code","2b6669f4":"code","fd0b564f":"code","fc0e9d84":"code","047e644d":"code","8b6d3c84":"code","d4a28f86":"code","90de4e29":"code","dbb8743f":"markdown","63f621e4":"markdown","aa955e20":"markdown","c9193bff":"markdown","5db043b0":"markdown","62fac412":"markdown","4eee3941":"markdown","a4062fd8":"markdown","a9d40045":"markdown","dcf122f2":"markdown","410430d8":"markdown","246d3175":"markdown","cad43c56":"markdown","1b657d11":"markdown","6da25409":"markdown","c81ae0d0":"markdown","af17938a":"markdown","890870c9":"markdown","7fff32f9":"markdown","4c475e8d":"markdown","b17e4f5e":"markdown","44e7bf8e":"markdown","46a880f1":"markdown"},"source":{"731c059f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2dee658d":"\n#now read the json file using pandas\n\ndf = pd.read_json(\"..\/input\/news-headlines-dataset-for-sarcasm-detection\/Sarcasm_Headlines_Dataset_v2.json\", lines=True)\ndf.head()","9dbf0e94":"#lets find if there is any NaN valus, because NaN values give wrong visualization","ea2126b9":"df.isna().sum()","3468270f":"del df['article_link']","a571c490":"import seaborn as sns\n\nsns. set_style(\"dark\")\nsns.countplot(df.is_sarcastic)","feacfdf8":"#first we import all necessary libs\nimport matplotlib.pyplot as plt\nimport nltk\nfrom sklearn.preprocessing import LabelBinarizer\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom wordcloud import WordCloud,STOPWORDS\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize,sent_tokenize\nfrom bs4 import BeautifulSoup\nimport re,string,unicodedata\nfrom keras.preprocessing import text, sequence\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom string import punctuation\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Embedding,LSTM,Dropout,Bidirectional,GRU\nimport tensorflow as tf","42e249c9":"# set and define stop word\n\nstopwd = set(stopwords.words('english'))\npunctuation = list(string.punctuation)\nstopwd.update(punctuation)","f09dba0c":"#use beautifulsoup library to extract text from html data\ndef clean_html(text):\n    soup=BeautifulSoup(text, \"html.parser\")\n    return soup.get_text()\n\n#remove [], *\/ etc html tags from text using \"re\" lib\n\ndef remove_betwn_square_brackets(text):\n    return re.sub('\\[[^]]*\\]', '', text)\n\n#remove urls from data\n\ndef remove_betwn_square_brackets(text):\n    return re.sub(r'http\\S+', '', text)\n\n#lets remove stopwords we counted before\ndef remove_stopwords(text):\n    final_text = []\n    for i in text.split():\n        if i.strip().lower() not in stopwd:\n            final_text.append(i.strip())\n    return \" \".join(final_text)\n\n#finally we remove all noisy data\ndef remove_noisy_text(text):\n    text = clean_html(text)\n    text = remove_betwn_square_brackets(text)\n    text = remove_stopwords(text)\n    return text\n\n\n","b69596bb":"#lets clean the dataset and view\n\ndf['headline']=df['headline'].apply(remove_noisy_text)\ndf['headline']","d48fb098":"plt.figure(figsize = (15,15)) # non-sarcastic words wordcloud\nwordcld = WordCloud(max_words = 3000 , width = 1400 , height = 600).generate(\" \".join(df[df.is_sarcastic == 0].headline))\nplt.imshow(wordcld , interpolation = 'bilinear')","da5d0fe9":"plt.figure(figsize = (15,15)) # non-sarcastic words wordcloud\nwordcld = WordCloud(max_words = 3000 , width = 1400 , height = 600).generate(\" \".join(df[df.is_sarcastic == 1].headline))\nplt.imshow(wordcld , interpolation = 'bilinear')","b22d8452":"#number of words\nfig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntext_len=df[df['is_sarcastic']==1]['headline'].str.split().map(lambda x: len(x))\nax1.hist(text_len,color='red')\nax1.set_title('Sarcastic text')\ntext_len=df[df['is_sarcastic']==0]['headline'].str.split().map(lambda x: len(x))\nax2.hist(text_len,color='green')\nax2.set_title('Not Sarcastic text')\nfig.suptitle('Words in texts')\nplt.show()\n\n#average word length\nfig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\nword=df[df['is_sarcastic']==1]['headline'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax1,color='red')\nax1.set_title('Sarcastic text')\nword=df[df['is_sarcastic']==0]['headline'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax2,color='green')\nax2.set_title('Not Sarcastic text')\nfig.suptitle('Average word length in each text')\n","60ef785e":"#lets convert our text data into more acceptable format\n\n#split words from a sentence and keep is sentence in the list which will help us for tokenization\nwords = []\nfor i in df.headline.values:\n    words.append(i.split())\nprint(\"splitted words:\",words[:5])\n\n# use genism lib for word2vec wordembedding\nimport gensim\n#Dim for max embedding\nEMBEDDING_DIM = 200\n\n#lets create word2vec model\nw2v_model = gensim.models.Word2Vec(sentences = words , size=EMBEDDING_DIM , window = 5 , min_count = 1)","3b45d26b":"# import keras.preprocessing lib for token\ntokenizer = text.Tokenizer(num_words=38071)\ntokenizer.fit_on_texts(words)\ntokenized_traindata = tokenizer.texts_to_sequences(words)\nx = sequence.pad_sequences(tokenized_traindata, maxlen = 20)","b987ac87":"print(\"before tokenization:\",len(w2v_model.wv.vocab))\n#  vocab size increases by 1\nvocab_size = len(tokenizer.word_index) + 1\nprint(\"after tokenization, vocab_size:\", vocab_size)","23e045e4":"#generate weightmatrix using numpy zeros \nweight_matrix=np.zeros((vocab_size, EMBEDDING_DIM))\n\n#lets fill each zeros with value model\nfor word, k in tokenizer.word_index.items():\n        weight_matrix[k] = w2v_model[word]\n","2b6669f4":"#lets split dataset into train and test\n\nx_train, x_test, y_train, y_test = train_test_split(x, df.is_sarcastic , test_size = 0.3 , random_state = 0)\n","fd0b564f":"#define dnn model\nmodel = Sequential()\n#adding embeddidng layers using bidirectional LSTM\nmodel.add(Embedding(vocab_size, output_dim=EMBEDDING_DIM, weights=[weight_matrix], input_length=20, trainable=True))\n\nmodel.add(Bidirectional(LSTM(units=128 , recurrent_dropout = 0.2 , dropout = 0.2,return_sequences = True)))\nmodel.add(Bidirectional(GRU(units=64 , recurrent_dropout = 0.1 , dropout = 0.1)))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(optimizer=keras.optimizers.Adam(lr = 0.04), loss='binary_crossentropy', metrics=['acc'])\n\ndel weight_matrix\nmodel.summary()\n","fc0e9d84":"history = model.fit(x_train, y_train, batch_size = 128 , validation_data = (x_test,y_test) , epochs = 5)","047e644d":"epochs = [i for i in range(5)]\nfig , ax = plt.subplots(1,2)\ntrain_acc = history.history['acc']\ntrain_loss = history.history['loss']\nval_acc = history.history['val_acc']\nval_loss = history.history['val_loss']\nfig.set_size_inches(20,10)\n\nax[0].plot(epochs , train_acc , 'go-' , label = 'Training Accuracy')\nax[0].plot(epochs , val_acc , 'ro-' , label = 'Testing Accuracy')\nax[0].set_title('Training & Testing Accuracy')\nax[0].legend()\nax[0].set_xlabel(\"Epochs\")\nax[0].set_ylabel(\"Accuracy\")\n\nax[1].plot(epochs , train_loss , 'go-' , label = 'Training Loss')\nax[1].plot(epochs , val_loss , 'ro-' , label = 'Testing Loss')\nax[1].set_title('Training & Testing Loss')\nax[1].legend()\nax[1].set_xlabel(\"Epochs\")\nax[1].set_ylabel(\"Loss\")\nplt.show()","8b6d3c84":"predict = model.predict_classes(x_test)\npredict[:10]","d4a28f86":"#lets use confusion matrix to check on predictions\nconmat=confusion_matrix(y_test,predict)\nconmat = pd.DataFrame(conmat , index = ['Not Sarcastic','Sarcastic'] , columns = ['Not Sarcastic','Sarcastic'])\nplt.figure(figsize = (5,5))\nsns.heatmap(conmat,cmap= \"Blues\", linecolor = 'black' , linewidth = 1 , annot = True, fmt='' , xticklabels = ['Not Sarcastic','Sarcastic'] , yticklabels = ['Not Sarcastic','Sarcastic'])","90de4e29":"print(classification_report(y_test, predict, target_names = ['Not Sarcastic','Sarcastic']))","dbb8743f":"In the dataset, if headline is sarcastic, \"is_sarcastic\" column value 1. If not sarcastic, value=0. Also article value is provided. But our main focus is to build a model that can detect sarcastic news based on \"headline\" and \"is_sarcastic\" column.","63f621e4":"# Now we will use word2vec, tokenization, padding using keras text and word preprocessing libraries. To learn details about wordembedding, visit:(https:\/\/towardsdatascience.com\/introduction-to-word-embedding-and-word2vec-652d0c2060fa)","aa955e20":"# lets perform precission, recall, f1 on our model performance","c9193bff":"# Please upvote this notebook if you find this useful**","5db043b0":"# Now we have to clean unnecessary characters like [],_, htmls etc to clean dataset","62fac412":"# Lets create a worldcloud from non-sarcastic news which will give us a view of sights which words are used here most","4eee3941":"# LETS BUILD DEEP LEARNING MODEL AND TRAIN****","a4062fd8":"here we see after tokenization, size increases by 1 because of extra index for unknown words","a9d40045":"# Lets make predictions","dcf122f2":"## Lets analyze training and validation accuracy\/loss","410430d8":"#Lets create a barplot\/countplot to compare between sarcastic or non-sarcastic news","246d3175":"# The model needs tuning.. Its overfitting.","cad43c56":"## Predictions are one hot encoded.. we need to decode it to see in original form. We can decode using decoder.","1b657d11":"# lets find vocab length\n# after tokenization, new length\n","6da25409":"# Lets create a worldcloud from non-sarcastic news which will give us a view of sights which words are used here most","c81ae0d0":"##Lets findsome somewords and clean them because stopwords have no meaning for a sentence. we will use python library to detect them","af17938a":"## so there are almost same number of sarcastic and non-sarcastic news.\n\n","890870c9":"## Lets train our model","7fff32f9":"#Lets drop the \"article link\" column from dataframe as it is not needed","4c475e8d":"#  Lets visualize number of words and each word length in dataset","b17e4f5e":"# Lets create word vectors by creating weight matrix for non-embedding keras layers","44e7bf8e":"## now we will use tokenizer which keep tracks of every word in dataset by assigning an unique token for each word. Also to match length of each sentence, padding can be used","46a880f1":"# Dataset Exploratory Data Analysis"}}