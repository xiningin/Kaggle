{"cell_type":{"332dc56b":"code","1442e241":"code","0a2bef33":"code","6f348a3b":"code","3b492e12":"code","6ddf1303":"code","e573fa54":"code","8a80e904":"code","33f8d0ec":"code","9a2941bc":"code","dc7a4f17":"code","99360adb":"code","6d111056":"code","f5f14c51":"code","aeeb62a5":"code","ac2582f4":"code","0fd89a09":"code","dc5416cb":"code","d0bae125":"code","f2d47394":"code","2653c747":"code","2b7983c1":"code","3137a094":"code","c55c173e":"code","528c651b":"code","c24a1a0d":"code","a2dbe182":"code","a059afae":"code","74cf77a2":"code","4f448393":"markdown","719f50bf":"markdown","a5be1cc1":"markdown","35dafe48":"markdown","b0fa82a1":"markdown","8a3006a5":"markdown","e0ca85d4":"markdown","63b493f1":"markdown","04a72f25":"markdown","7d395944":"markdown","d382408b":"markdown","e95aaf27":"markdown","ef9198ef":"markdown","9ff1e61f":"markdown"},"source":{"332dc56b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom tqdm import tqdm_notebook as tqdm\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","1442e241":"# Load data\ndf_train = pd.read_csv('..\/input\/cat-in-the-dat\/train.csv')\ndf_test = pd.read_csv('..\/input\/cat-in-the-dat\/test.csv')\n\nprint(df_train.shape)\nprint(df_test.shape)\n\ndf_train.info()","0a2bef33":"# Binary data\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfig, axs = plt.subplots(2, 5, figsize=(20, 8))\n\nfor i in range(5):\n    col = 'bin_{}'.format(i)\n    ax = axs[0, i]\n    sns.countplot(df_train[col], hue=df_train['target'], ax=ax)\n    ax.set_title(col, fontsize=14, fontweight='bold')\n    ax.legend(title=\"target\", loc='upper center')\n    \n    ax = axs[1, i]\n    sns.barplot(x=col, y='target', data=df_train, ax=ax)","6f348a3b":"# Categorical count\nfor i in range(10):\n    col = 'nom_{}'.format(i)\n    print(col, df_train[col].nunique())\n\n# Ordinal count\nfor i in range(6):\n    col = 'ord_{}'.format(i)\n    print(col, df_train[col].nunique())","3b492e12":"# Categorical with few uniques\n\nfig, axs = plt.subplots(1, 5, figsize=(20, 4))\n\nfor i in range(5):\n    col = 'nom_{}'.format(i)\n    ax = axs[i]\n    #sns.countplot(train[col], hue=target, ax=ax)\n    sns.barplot(x=col, y='target', data=df_train.iloc[:10000], ax=ax)\n    ax.set_title(col, fontsize=14, fontweight='bold')\n    ax.legend(title=\"target\", loc='upper center')","6ddf1303":"# Ordinal\n\nfig, axs = plt.subplots(1, 6, figsize=(20, 4))\n\nfor i in range(6):\n    col = 'ord_{}'.format(i)\n    ax = axs[i]\n    sns.barplot(x=col, y='target', data=df_train, ax=ax)\n    ax.set_title(col, fontsize=14, fontweight='bold')\n    ax.legend(title=\"target\", loc='upper center')","e573fa54":"# Ordinal sorted by label\nfig, axs = plt.subplots(1, 6, figsize=(20, 4))\n\nfor i in range(6):\n    col = 'ord_{}'.format(i)\n    \n    order = sorted(df_train[col].unique(), key=lambda val: df_train[df_train[col] == val]['target'].mean())\n    df_train[col + '_sort'] = df_train[col].map({val: i for (i, val) in enumerate(order)})\n    df_test[col + '_sort'] = df_test[col].map({val: i for (i, val) in enumerate(order)})\n    \n    ax = axs[i]\n    sns.barplot(x=col + '_sort', y='target', data=df_train, ax=ax)\n    ax.set_title(col + '_sort', fontsize=14, fontweight='bold')\n    ax.legend(title=\"target\", loc='upper center')","8a80e904":"cyclic_cols = ['day','month']\n\nfig, axs = plt.subplots(1, len(cyclic_cols), figsize=(8, 4))\n\nfor i in range(len(cyclic_cols)):\n    col = cyclic_cols[i]\n    ax = axs[i]\n    sns.barplot(x=col, y='target', data=df_train, ax=ax)\n    ax.set_title(col, fontsize=14, fontweight='bold')\n    ax.legend(title=\"target\", loc='upper center')","33f8d0ec":"for df in [df_train, df_test]:\n    for col in cyclic_cols:\n        df[col+'_sin'] = np.sin((2 * np.pi * df[col]) \/ max(df[col]))\n        df[col+'_cos'] = np.cos((2 * np.pi * df[col]) \/ max(df[col]))","9a2941bc":"sns.pairplot(df_train.loc[:1000], vars=['day_sin', 'day_cos', 'day'], hue='target')","dc7a4f17":"sns.pairplot(df_train.loc[:1000], vars=['month_sin', 'month_cos', 'month'], hue='target')","99360adb":"# Subset\ntarget = df_train['target']\ntrain_id = df_train['id']\ntest_id = df_test['id']\ntrain0 = df_train.copy().drop(['target', 'id'], axis=1)\ntest0 = df_test.copy().drop('id', axis=1)\n\nprint(train0.shape)\nprint(test0.shape)\n\nfor df in [train0, test0]:\n    # binary str to num\n    df['bin_3'] = df['bin_3'].map({'T': 1, 'F': 0})\n    df['bin_4'] = df['bin_4'].map({'Y': 1, 'N': 0})\n    \n    # drop cyclic features, leaving their sin and cos\n    #df.drop(cyclic_cols, axis=1, inplace=True)\n    df[cyclic_cols] \/= df[cyclic_cols].max(axis=0)\n    \n    # drop unsorted ordinal features\n    df.drop(['ord_{}'.format(i) for i in range(6)], axis=1, inplace=True)\n    # scale sorted ordinal features\n    df[['ord_{}_sort'.format(i) for i in range(6)]] \/= df[['ord_{}_sort'.format(i) for i in range(6)]].max(axis=0)\n\ntrain0.head()","6d111056":"%%time\nfrom sklearn.decomposition import TruncatedSVD, PCA\n\n# One hot for features with few categories\ncol_names = ['nom_{}'.format(i) for i in range(5)] + ['ord_{}'.format(i) for i in range(5)] + ['day', 'month']\ntrain_ohe = df_train[col_names]\ntest_ohe = df_test[col_names]\n\ntraintest = pd.concat([train_ohe, test_ohe])\ndummies = pd.get_dummies(traintest, columns=traintest.columns, drop_first=True, sparse=True)\n\ntrain_ohe1 = dummies.iloc[:df_train.shape[0], :]\ntest_ohe1 = dummies.iloc[df_train.shape[0]:, :]\nprint('train_ohe1.shape', train_ohe1.shape)\n\n# One hot + SVD  for features with many categories\ncol_names = ['nom_{}'.format(i) for i in range(5, 10)] + ['ord_{}'.format(i) for i in range(5, 6)]\ntrain_ohe = df_train[col_names]\ntest_ohe = df_test[col_names]\n\ntraintest = pd.concat([train_ohe, test_ohe])\ndummies = pd.get_dummies(traintest, columns=traintest.columns, drop_first=True, sparse=True)\n\ntrain_ohe2 = dummies.iloc[:df_train.shape[0], :].sparse.to_coo().tocsr()\ntest_ohe2 = dummies.iloc[df_train.shape[0]:, :].sparse.to_coo().tocsr()\nprint('train_ohe2.shape', train_ohe2.shape)\n\n# Lower dimensionality\nn_components = 200\nsvd = TruncatedSVD(n_components=n_components)\ntrain_svd = svd.fit_transform(train_ohe2)\ntest_svd = svd.transform(test_ohe2)\nprint('train_svd.shape', train_svd.shape)\n\n# Join features\ntrain = train0.drop(['nom_{}'.format(i) for i in range(10)], axis=1)\ntest = test0.drop(['nom_{}'.format(i) for i in range(10)], axis=1)\ncol_names = ['pca_{}'.format(i) for i in range(n_components)]\ntrain = pd.concat([train, train_ohe1, pd.DataFrame(train_svd, columns=col_names)], axis=1)\ntest = pd.concat([test, test_ohe1, pd.DataFrame(test_svd, columns=col_names)], axis=1)\n\nprint('train.shape', train.shape) # feature set #1","f5f14c51":"# One hot\ncol_names = ['bin_{}'.format(i) for i in range(5)] \\\n        + ['nom_{}'.format(i) for i in range(10)] \\\n        + ['ord_{}'.format(i) for i in range(6)] \\\n        + ['day', 'month']\ntrain_ohe = df_train[col_names]\ntest_ohe = df_test[col_names]\n\ntraintest = pd.concat([train_ohe, test_ohe])\ndummies = pd.get_dummies(traintest, columns=traintest.columns, drop_first=True, sparse=True)\n\ntrain_ohe = dummies.iloc[:df_train.shape[0], :].sparse.to_coo().tocsr()\ntest_ohe = dummies.iloc[df_train.shape[0]:, :].sparse.to_coo().tocsr()\n\nprint('train_ohe.shape', train_ohe.shape)  # feature set #2","aeeb62a5":"from sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.utils.testing import ignore_warnings\n\nlr1 = LogisticRegression(solver='lbfgs', C=0.1)\nlr2 = LogisticRegression(solver='lbfgs', C=0.1)\n\nwith ignore_warnings(category=FutureWarning):\n    %time print('Dense CV:', cross_val_score(lr1, train, target, cv=2, scoring='roc_auc', n_jobs=-1).mean())\n    %time print('OHE CV:', cross_val_score(lr2, train_ohe, target, cv=2, scoring='roc_auc', n_jobs=-1).mean())","ac2582f4":"%%time\nlr1.fit(train, target)\nlr2.fit(train_ohe, target)","0fd89a09":"from sklearn.utils.testing import ignore_warnings\n\nwith ignore_warnings(category=FutureWarning):\n    lr1_predictions = lr1.predict_proba(test)\nlr1_submission = pd.DataFrame({'id': test_id, 'target': lr1_predictions[:, 1]})\nlr1_submission.to_csv('lr1.csv', index=False)\nlr1_submission.head()","dc5416cb":"with ignore_warnings(category=FutureWarning):\n    lr2_predictions = lr2.predict_proba(test_ohe)\nlr2_submission = pd.DataFrame({'id': test_id, 'target': lr2_predictions[:, 1]})\nlr2_submission.to_csv('lr2.csv', index=False)\nlr2_submission.head()","d0bae125":"# %%time\n\n# from sklearn.svm import SVC\n\n# svm = SVC(kernel='rbf', probability=True, gamma='scale', C=0.1)\n# print('CV:', cross_val_score(svm, train.iloc[:10000], target.iloc[:10000], cv=2, n_jobs=-1, scoring='roc_auc').mean())","f2d47394":"# svm.fit(train.iloc[:10000], target.iloc[:10000])","2653c747":"# %%time\n# svm_predictions = svm.predict_proba(test)\n# svm_submission = pd.DataFrame({'id': test_id, 'target': svm_predictions[:, 0]})\n# svm_submission.to_csv('svm.csv', index=False)\n# svm_submission.head()","2b7983c1":"%%time\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom skorch import NeuralNetClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nclass MyModule(nn.Module):\n    def __init__(self, num_units=20):\n        super(MyModule, self).__init__()\n\n        self.dense1 = nn.Linear(train.shape[1], num_units)\n        self.dropout = nn.Dropout(0.5)\n        self.dense2 = nn.Linear(num_units, 2)\n\n    def forward(self, X, **kwargs):\n        X = self.dense1(X)\n        X = torch.tanh(X)\n        X = self.dropout(X)\n        X = self.dense2(X)\n        X = F.softmax(X, dim=-1)\n        return X\n\nnet = NeuralNetClassifier(\n    MyModule,\n    max_epochs=20,\n    lr=0.01,\n    iterator_train__shuffle=True,\n)\n\npipe = Pipeline([\n    ('scale', StandardScaler()),\n    ('net', net),\n])\n\nprint('CV:', cross_val_score(pipe,\n                             train.values.astype('float32'),\n                             target.values,\n                             cv=2,\n                             n_jobs=-1,\n                             scoring='roc_auc').mean())","3137a094":"pipe.fit(train.values.astype('float32'), target.values)","c55c173e":"net_predictions = pipe.predict_proba(test.values.astype('float32'))\nnet_submission = pd.DataFrame({'id': test_id, 'target': net_predictions[:, 1]})\nnet_submission.to_csv('net.csv', index=False)\nnet_submission.head()","528c651b":"predictions = pd.DataFrame({'lr1': lr1_predictions[:, 1],\n                            'lr2': lr2_predictions[:, 1],\n                            'net': net_predictions[:, 1]})\ncorr = predictions.corr()\ncorr","c24a1a0d":"sns.heatmap(corr, annot=True, square=True);","a2dbe182":"vote_predictions = (lr1_predictions + lr2_predictions) \/ 2\nvote_submission = pd.DataFrame({'id': test_id, 'target': vote_predictions[:, 1]})\nvote_submission.to_csv('vote1.csv', index=False)\nvote_submission.head()","a059afae":"vote_predictions = (lr1_predictions * 0.1 + lr2_predictions) \/ 1.1\nvote_submission = pd.DataFrame({'id': test_id, 'target': vote_predictions[:, 1]})\nvote_submission.to_csv('vote2.csv', index=False)\nvote_submission.head()","74cf77a2":"vote_predictions = (lr1_predictions * 0.1 + net_predictions * 0.1 + lr2_predictions) \/ 1.2\nvote_submission = pd.DataFrame({'id': test_id, 'target': vote_predictions[:, 1]})\nvote_submission.to_csv('vote3.csv', index=False)\nvote_submission.head()","4f448393":"# EDA and training models\nData preparation techniques used:\n* onehot encoding for categorical features\n* sorting and numerical encoding for ordinal features\n* sin\/cos encoding for cyclic data\n* onehot+SVD for categorical features with too many categories\n\nModels tested:\n* Linear regression\n* Two-layer perceptron\n* Simple voting\n\nReferences:\n* https:\/\/www.kaggle.com\/shahules\/an-overview-of-encoding-techniques\n* https:\/\/www.kaggle.com\/peterhurford\/why-not-logistic-regression","719f50bf":"### 2 Layer Perceptron\nNeural network is usually worse than SVM on this kind of data.  \nBut it trains faster when data is big, like this.  \nI used skorch (https:\/\/github.com\/skorch-dev\/skorch) for easy compatibility with torch and sklearn.","a5be1cc1":"### Onehot encoding + SVD\nEncode categorical features into sparse matrix, then lower the dimension  \nThis will be feature set #1","35dafe48":"### Logistic regression","b0fa82a1":"### Binary","8a3006a5":"## Data preparation","e0ca85d4":"### SVM\nIt takes too long to train, so I commented it out","63b493f1":"## EDA and Feature Engineering","04a72f25":"### Onehot encoding to sparse matrix\nEncode everything to onehot  \nThis is feature set #2","7d395944":"## Voting","d382408b":"Logregression and neural network output almost the same results when trained on the same data  \nSo, we can conclude that there might be no need for nonlinearity","e95aaf27":"## Model training","ef9198ef":"### Cyclic","9ff1e61f":"### Categorical and ordinal"}}