{"cell_type":{"fc767908":"code","51a61a52":"code","3464d092":"code","38c9e269":"code","30b1250b":"code","4463ff62":"code","68073503":"markdown","c0018b3e":"markdown","2f5c457b":"markdown","7ac72af9":"markdown","5d652ea2":"markdown","e337c376":"markdown"},"source":{"fc767908":"import numpy as np\nimport pandas as pd\n\ndata_full = pd.read_csv(\"..\/input\/train.csv\", index_col='PassengerId')\ndata_test = pd.read_csv(\"..\/input\/test.csv\", index_col='PassengerId')","51a61a52":"data_full.head()","3464d092":"data_full.describe()","38c9e269":"cols_with_missing = [col for col in data_full.columns\n                     if data_full[col].isnull().any()]\nprint(cols_with_missing)","30b1250b":"from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder, FunctionTransformer\nfrom sklearn.model_selection import GridSearchCV\nfrom xgboost import XGBClassifier\n\n# Select our features\nfeature_names = [\"Age\", \"Sex\", \"Pclass\", \"Fare\", \"SibSp\", \"Parch\"]\nX = data_full[feature_names]\ny = data_full.Survived\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# Setup a transformer to clean up our data.\npreprocessor = ColumnTransformer(\n    transformers=[\n        # We will fill in missing numerical data with their mean.\n        ('numerical', SimpleImputer(), [\"Age\", \"Fare\"]),\n        # We will fill in missing categorical values with the mode value.\n        ('categorical', categorical_transformer, [\"Sex\", \"Pclass\"]),\n        # Let's also create a family size feature by adding the SibSp and Parch counts.\n        ('family-size',\n            FunctionTransformer(\n                lambda v: pd.DataFrame({\n                    'FamilySize': np.apply_along_axis(lambda row: row[0] + row[1], 1, v)\n                }),\n            # Set validate to false to disable sklearn 0.22 deprecation warning.\n            validate=False),\n            [\"SibSp\", \"Parch\"])\n    ])\n\npipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('model', XGBClassifier(random_state=0))\n])\n\n# Perform a grid search to identify the best model hyperparameters.\nparameters = {\n    'model__learning_rate': [0.0375, 0.04, 0.0425],\n    'model__n_estimators': [500, 550]\n}\n\nCV = GridSearchCV(pipeline, parameters, cv=5, scoring='accuracy')\nCV.fit(X, y)\n\nprint('Best score and parameter combination')\n\nprint(CV.best_score_)\nprint(CV.best_params_)\n","4463ff62":"# Get our test predictions\nX_test = data_test[feature_names]\n\n# Create our final model\nfinal_model = XGBClassifier(random_state=0, learning_rate=0.0375, n_estimators=550)\n\nfinal_pipeline = Pipeline(steps=[\n        ('preprocessor', preprocessor),\n        ('model', final_model)\n    ])\n\n# Fit our model with all the available data.\nfinal_pipeline.fit(X, y)\n\n# Generate and output our test predictions.\npreds_test = final_pipeline.predict(X_test)\noutput = pd.DataFrame({'PassengerId': X_test.index, 'Survived': preds_test})\noutput.to_csv('submission.csv', index=False)","68073503":"Let's inspect the data and see what we have to work with.","c0018b3e":"# Final Model and Submission\nLet's create the final model with the parameters we identified above and generate the final competition output.\n\nWe will fit our model with all the available data to be more accurate.","2f5c457b":"# Thoughts and Future Improvements\nThis was an interesting dive into data science using the skills I learnt from the Kaggle microcourses.\n\nThere are various improvements I can make to the above model through better understanding of techniques such as:\n* Data visualisation (for identifying correlations and useful features)\n* Data engineering (for creating better, more useful features)\n* More models (such as neural networks)\n\nbut I feel that this was a good and fun beginner introduction to data science.","7ac72af9":"This is my attempt at independent machine learning after completing the Kaggle Intro and Intermediate Machine Learning microcourses.\n\n# Setup and Data Exploration\nSetup the environment and load in the dataset here.","5d652ea2":"# Model Selection and Tuning\nLet's now select our features and model.\n\nWe will set up a pipeline to preprocess our data's missing values and add a feature to identify the size of the passenger's family.\n\nWe will then perform grid search and cross validation to identify the best hyperparameters for our model.","e337c376":"Let's identify the columns with missing data so we know which to impute."}}