{"cell_type":{"f1b3a52e":"code","d874a7e2":"code","7f22c39b":"code","ed79170b":"code","08a8078a":"code","86ceedaa":"code","768c413b":"code","6a21936c":"code","e20b6656":"code","d036d6fc":"code","f4c87864":"code","091f4ca8":"code","90a13a92":"code","f6b61009":"code","d5a07a24":"code","72148f4f":"code","5e9e88b8":"code","d26a65e7":"code","69aa8232":"code","3f885512":"code","0d3f1d07":"code","15af158e":"code","3cf27868":"code","de5a0cf7":"code","0b8f7199":"code","1f1edcd3":"code","d71d0363":"code","e48bd892":"code","2a060e77":"code","b6f2595c":"code","babde364":"code","f7625d47":"code","4a99e95e":"code","72bc510f":"code","2f4d26ee":"code","0cf204c1":"code","f2216af4":"code","149f2328":"code","e7966218":"code","14a92244":"code","4e952301":"code","58827faf":"code","1a6cf264":"code","15e9cfdc":"code","cb9468e3":"code","b664b061":"code","65ac6272":"code","3be8fd16":"code","fc4110d5":"code","a660d842":"code","7aabfeae":"code","e6e07169":"code","0a158ce0":"code","1c8bbe6e":"code","a60f6ccd":"code","94ba5f2d":"code","d198b442":"code","74baee98":"code","013367ef":"code","ca1d3b57":"code","d79f3948":"code","41dbe257":"code","eaa0590a":"code","594d034b":"code","41db1043":"code","96c212c6":"code","3e163b8b":"code","41c2c66e":"code","b7c86d22":"code","3964d8ac":"code","73dfcc13":"code","92316882":"markdown","26d3b7ff":"markdown","eff35d63":"markdown","64e2fdf0":"markdown","4e2508e3":"markdown","d4a2f476":"markdown","9e436045":"markdown","0e57d194":"markdown","f34add97":"markdown","220c2c58":"markdown","98eafe11":"markdown","6db578e6":"markdown","96bde70e":"markdown","54099ef9":"markdown","0b372996":"markdown","8ade537a":"markdown","7b0024e4":"markdown","726b7ed2":"markdown","451a30d7":"markdown","b382b490":"markdown","35f9490c":"markdown","51470a23":"markdown","6ec9dc28":"markdown","2fad8ee4":"markdown","1a0982df":"markdown","140252c9":"markdown","a67f99f2":"markdown","dcd7f6a0":"markdown","fb876d2f":"markdown","7d375815":"markdown","442c8d26":"markdown","70089985":"markdown","ad9f008a":"markdown","3ad292e1":"markdown","1f7bea66":"markdown","2dea9634":"markdown","ce2e9da4":"markdown","c164564a":"markdown","4d055d46":"markdown","3cc4659a":"markdown","3163012a":"markdown","864ed944":"markdown","3c506e9a":"markdown","620a2bd1":"markdown","bfbce5f0":"markdown","aff41080":"markdown","53653747":"markdown","badf2f52":"markdown","073afbb4":"markdown","4bd8614f":"markdown","8e256318":"markdown","abcd64dc":"markdown","cb2cc54e":"markdown","af30788c":"markdown","9fee2661":"markdown","0223991e":"markdown","3479733e":"markdown","baffea8b":"markdown","6b963b3d":"markdown","fbfc21ee":"markdown","f03843af":"markdown","ba90e27f":"markdown","fc8fffd7":"markdown","4c982fda":"markdown","397c49fb":"markdown"},"source":{"f1b3a52e":"!pip install scikit-learn==0.20.3 --upgrade","d874a7e2":"# Si desea utilizar la biblioteca ** xgboost **, instale la versi\u00f3n 0.71.\n!pip install xgboost==0.71 --upgrade","7f22c39b":"import json\nimport requests\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\nfrom datetime import datetime\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import KFold\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nimport matplotlib.pyplot as plt\nimport seaborn as sns","ed79170b":"from sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.utils import resample\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler\nfrom sklearn.impute import KNNImputer\n\nclass TransformData(BaseEstimator, TransformerMixin):\n    def __init__(self, n_components,target):\n        self.n_components = n_components\n        self.target = target\n        self.mean_acept = 0\n        self.mean_sospe = 0\n        self.min_values = 0\n        self.mean_glob  = 0\n        self.max_log_values = 0\n\n    def fit(self, X, y):\n        # Guardando los valores medios\n        #X = X.join(y)\n        columns = X.columns.tolist()\n        #self.mean_glob = X.mean()\n        self.knnimputer = KNNImputer(n_neighbors=4, weights=\"uniform\")\n        self.knnimputer.fit(X)\n        X = self.knnimputer.transform(X)\n        \n        X = pd.DataFrame(data=X,columns = columns).join(y)\n        \n        count4bal = X[self.target].value_counts().sort_values(ascending=True)\n        class_minor = count4bal.index[0]\n        class_mayor = count4bal.index[-1]\n        mayority_sample = count4bal[-1]\n        \n        df_minority_upsampled = resample(X.loc[X[self.target]==class_minor], \n                                 replace=True,     # sample with replacement\n                                 n_samples=mayority_sample,    # to match majority class\n                                 random_state=17) # reproducible results\n        \n        df_temp = pd.concat([df_minority_upsampled,X.loc[X[self.target]==class_mayor]],ignore_index=True)\n        \n        self.scaler0 = MinMaxScaler()\n        self.scaler0.fit(df_temp[columns])\n        df_temp[columns] = self.scaler0.transform(df_temp[columns])\n        \n        df_balanced, label = df_temp.drop(columns=[self.target ]), df_temp[self.target ]\n        self.pca = PCA(n_components=self.n_components)  # df_balanced, label\n        self.scaler = MinMaxScaler()\n        self.pca.fit(df_balanced) ## justa para todo el espectro de datos\n        self.scaler.fit(self.pca.transform(df_balanced))\n        return self\n    \n    \n    def transform(self, X ):\n        df_temp = self.knnimputer.transform(X) #X.fillna(self.mean_glob)\n        df_temp = self.scaler0.transform(df_temp)\n\n        df_transformed = self.pca.transform(df_temp)\n        return self.scaler.transform(df_transformed)\n    \n### ONLY FOR PRETRAIN NOT PIPELINE\nclass PreTrainData( ):\n    def __init__(self,target):\n        self.target = target\n        self.mean_acept = 0\n        self.mean_sospe = 0\n        self.min_values = 0\n        self.mean_glob  = 0\n        self.max_log_values = 0\n\n    def fit(self, X, minNan2drop=5):\n        # Guardando los valores medios\n        \n        #self.mean_glob = X.mean()\n        df_temp_acep = X.loc[X[self.target ]=='Aceptado'].dropna(thresh=len(X.columns)-(minNan2drop-1))\n        #self.mean_acep = df_temp_acep.mean()\n        \n        df_temp_sosp = X.loc[X[self.target ]=='Sospechoso']\n        #self.mean_sosp = df_temp_sosp.mean()\n        \n        #df_temp_acep = df_temp_acep.fillna(self.mean_acep) \n        #df_temp_sosp = df_temp_sosp.fillna(self.mean_sosp) \n        df_temp = pd.concat([df_temp_acep,df_temp_sosp])\n        \n        y = X['OBJETIVO']\n        df_temp = X.drop(columns = ['OBJETIVO'])\n        columns = df_temp.columns.tolist()\n        self.knnimputer = KNNImputer(n_neighbors=6, weights=\"uniform\")\n        self.knnimputer.fit(df_temp)\n        df_temp = self.knnimputer.transform(df_temp)\n        df_temp = pd.DataFrame(data=df_temp,columns = columns).join(y)\n        \n        \n        count4bal = df_temp[self.target].value_counts().sort_values(ascending=True)\n        class_minor = count4bal.index[0]\n        class_mayor = count4bal.index[-1]\n        mayority_sample = count4bal[-1]\n        \n        df_minority_upsampled = resample(df_temp.loc[df_temp[self.target]==class_minor], \n                                 replace=True,     # sample with replacement\n                                 n_samples=mayority_sample,    # to match majority class\n                                 random_state=17) # reproducible results\n        \n        df_temp = pd.concat([df_minority_upsampled,df_temp.loc[df_temp[self.target]==class_mayor]],ignore_index=True)\n        #df_temp.reset_index(inplace=True)\n        print(df_temp[self.target].value_counts())\n        \n        return df_temp\n    \nimport itertools\n%matplotlib inline\ndef plot_confusion_matrix(y_true, y_pred, class_names,title=\"Confusion matrix\",normalize=False,onehot = False, size=4):\n    \"\"\"\n    Returns a matplotlib figure containing the plotted confusion matrix.\n\n    Args:\n    cm (array, shape = [n, n]): a confusion matrix of integer classes\n    class_names (array, shape = [n]): String names of the integer classes\n    \"\"\"\n    if onehot :\n        cm = confusion_matrix([y_i.argmax() for y_i in y_true], [y_ip.argmax() for y_ip in y_pred])\n    else:\n        cm = confusion_matrix(y_true, y_pred)\n    figure = plt.figure(figsize=(size, size))\n    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(class_names))\n    plt.xticks(tick_marks, class_names, rotation=45)\n    plt.yticks(tick_marks, class_names)\n\n    # Normalize the confusion matrix.\n    cm = np.around(cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis], decimals=2) if normalize else cm\n\n    # Use white text if squares are dark; otherwise black.\n    threshold = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        color = \"red\" if cm[i, j] > threshold else \"black\"\n        plt.text(j, i, cm[i, j], horizontalalignment=\"center\", color=color)\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.show()\n    #return figure","08a8078a":"!wget --no-check-certificate --content-disposition https:\/\/raw.githubusercontent.com\/vanderlei-test\/dataset\/master\/reto-4-compu-train.csv","86ceedaa":"df = pd.read_csv('reto-4-compu-train.csv')\ndf.head()","768c413b":"df.info()","6a21936c":"df.describe(include=['object'])","e20b6656":"#df_local = pd.concat([df_temp_acep,df_temp_sosp])\npretraindata = PreTrainData(target = 'OBJETIVO')\ndf_local = pretraindata.fit(df, minNan2drop=3)\nprint(len(df_local['OBJETIVO']))\n\nprint(\"#############################\")","d036d6fc":"df_mod = df.copy() #.loc[df['OBJETIVO']=='Aceptado']\ndf_mod.dropna(thresh=13, inplace= True,axis=0)\ndf_mod.loc[df_mod['OBJETIVO']=='Aceptado'].isnull().sum(axis=0) #Sospechoso","f4c87864":"#df.pivot_table(index='OBJETIVO',values='OBJETIVO', aggfunc='count')\ndf['count_temp'] = 1\n_ = pd.pivot_table(df, index= 'OBJETIVO',values= 'count_temp',aggfunc='count')\ndf.drop(columns=['count_temp'], inplace=True)\n_","091f4ca8":"columns = df_local.columns.tolist()\ncolumns.remove('OBJETIVO')\nfor column in columns:\n    g = sns.FacetGrid(df_local[['OBJETIVO',column]], col='OBJETIVO')\n    g.map(plt.hist, column, bins=None,log=True)","90a13a92":"import seaborn as sns\n# compute correlation matrix using pandas corr() function\nimport matplotlib.pyplot as plt\n\n\nfig, ax = plt.subplots(figsize=(15,15))         # Sample figsize in inches\n#sns.heatmap(df1.iloc[:, 1:6:], annot=True, linewidths=.5, ax=ax)\ncorr_df = df_local.corr(method='pearson') \nmatrix = np.triu(corr_df)\nhmap=sns.heatmap(corr_df,annot=True, ax=ax, mask=matrix, vmin=-.8,vmax=.8, center=0)","f6b61009":"from xgboost.sklearn import XGBClassifier\nfrom sklearn.model_selection import train_test_split, GridSearchCV , RandomizedSearchCV,StratifiedKFold\n#from imblearn.ensemble import BalancedBaggingClassifier\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier,AdaBoostClassifier,GradientBoostingClassifier,BaggingClassifier,ExtraTreesClassifier#StackingClassifier\nfrom sklearn.model_selection import StratifiedKFold, KFold,cross_val_score\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.metrics import accuracy_score,confusion_matrix\nfrom sklearn import svm\nfrom sklearn.decomposition import PCA\n#from sklearn.neighbors import NeighborhoodComponentsAnalysis as NCA\nfrom sklearn.neighbors import KNeighborsClassifier as KNC\nfrom collections import defaultdict\nfrom sklearn.metrics import classification_report","d5a07a24":"pretraindata = PreTrainData(target = 'OBJETIVO')\ndf_local = pretraindata.fit(df, minNan2drop=3)\nprint(len(df_local['OBJETIVO']))\n\ntransformData = TransformData(n_components=10,target='OBJETIVO')\ntransformData.fit(df_local.drop(columns=['OBJETIVO']),df_local['OBJETIVO'])\n\nX_train       = transformData.transform( df_local.drop(columns=['OBJETIVO']) ).copy()\ny_train       = df_local['OBJETIVO'].copy()\n\n\nskf = StratifiedKFold(n_splits=10)\ncross_cal = []\n# Parameters to tune\n# SVC\ntuned_parameters_svc = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4, 'auto','scale'],'C': [1, 10, 100, 1000]},\n                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]},\n                    {'kernel':['poly'], 'degree':[3,4,5,6,7]},\n                     {'kernel':['sigmoid']}]\n# XGB\ntuned_parameters_xgb = [{'learning_rate':[0.01,0.001],'n_estimators':[5,40,60,70],'min_child_weight':[.05],'subsample':[.5,.4],'colsample_bytree':[0.5],\n                    'objective':['binary:logistic'],'n_jobs':[-1] },\n                    {'learning_rate':[0.02,0.005],'n_estimators':[5,20,40,70],'min_child_weight':[.01],'subsample':[.1,.2,.4],'colsample_bytree':[0.5],\n                    'objective':['binary:logistic'],'n_jobs':[-1] }]\n#KNC\ntuned_parameters_knc = [{'n_neighbors':[2,4,6,8,10,12,20],'n_jobs':[-1],'algorithm':['auto', 'ball_tree', 'kd_tree', 'brute'],'p':[1,2]}]\n\n#RFC\ntuned_parameters_rfc = [{'n_estimators':[10,40,50,80,100],'criterion':['gini','entropy'],'min_samples_split':[2,3,4,5],'n_jobs':[-1],'random_state':[15]}]\n\n#GBC\ntuned_parameters_gbc = [{'loss':['deviance','exponential'],'learning_rate':[0.1,0.001,0.2],'n_estimators':[20,80,120,150],'subsample':[1.0],'criterion':['friedman_mse', 'mse', 'mae'],'max_depth':[2,3,6,8,10],'random_state':[15]}]\n\n#ETC\ntuned_parameters_etc = [{ 'min_samples_split':[1.0,.2,.4,.8], 'n_estimators':[50,80,100,140,160],'warm_start':[True, False],'bootstrap':[True, False],\n                         'n_jobs':[-1], 'random_state':[10,15], 'min_samples_leaf':[1,2,3,4] ,'criterion':['gini', 'entropy'],'max_features':[ 'sqrt','log2',None]   }]\n\n# Parameter tunning\nscores = ['f1']\n\n# Best parameters\nbest_parameters = defaultdict(list)\nmodels = { 'xgb': XGBClassifier(),'svc':svm.SVC(), 'rfc':RandomForestClassifier(), 'knc': KNC(),'gbc':GradientBoostingClassifier(),'etc':ExtraTreesClassifier() }\nparameters = {'xgb': tuned_parameters_xgb, 'knc': tuned_parameters_knc,'rfc':tuned_parameters_rfc,'svc':tuned_parameters_svc, 'gbc':tuned_parameters_gbc, 'etc':tuned_parameters_etc }\n\n#;ista = ['etc']\nfor model_name in models.keys():\n    print(\"######### MODEL tunning hyper-parameters for %s\" % model_name)\n    for score in scores:\n        print(\"# %s - Tuning hyper-parameters for %s\" % (model_name, score))\n        clf_i = GridSearchCV(models[model_name], parameters[model_name], scoring='%s_macro' % score, n_jobs=-1, cv=skf)\n        clf_i.fit(X_train,y_train)\n        print(\"Best parameters set found on development set:\")\n        print()\n        print(clf_i.best_params_)\n        best_parameters[model_name] = clf_i.best_params_\n        print(\"Grid scores on development set:\")\n        print()\n        means = clf_i.cv_results_['mean_test_score']\n        stds = clf_i.cv_results_['std_test_score']\n#         for mean, std, params in zip(means, stds, clf_i.cv_results_['params']):\n#             print(\"%s_macro - %0.3f (+\/-%0.03f) for %r\"% (score, mean, std * 2, params))\n        print(\"Detailed classification report:\")\n        print()\n        y_true, y_pred = y_train, clf_i.predict(X_train)\n        print(classification_report(y_true, y_pred))","72148f4f":"best_parameters = defaultdict(list)\n#### XGB\n\nbest_parameters['xgb']={'colsample_bytree': 0.5, 'learning_rate': 0.02, 'min_child_weight': 0.01, 'n_estimators': 70, 'n_jobs': -1, 'objective': 'binary:logistic', 'subsample': 0.4}\n\n#### KNC \n\nbest_parameters['knc']={'algorithm': 'auto', 'n_jobs': -1, 'n_neighbors': 8, 'p': 1}#{'algorithm': 'auto', 'n_jobs': -1, 'n_neighbors': 4, 'p': 1}\n\n#### RFC\nbest_parameters['rfc']={'criterion': 'entropy', 'min_samples_split': 2, 'n_estimators': 100, 'n_jobs': -1}#{'criterion': 'entropy', 'min_samples_split': 2, 'n_estimators': 80, 'n_jobs': -1}\n\n\n#### SVC\nbest_parameters['svc']={'C': 10, 'gamma': 'scale', 'kernel': 'rbf'}#{'C': 10, 'gamma': 'scale', 'kernel': 'rbf'}\n\n#### GBC\nbest_parameters['gbc']={'criterion': 'mse', 'learning_rate': 0.1, 'loss': 'deviance', 'max_depth': 3, 'n_estimators': 150, 'subsample': 1.0}#{'criterion': 'mse', 'learning_rate': 0.2, 'loss': 'exponential', 'max_depth': 6, 'n_estimators': 100, 'subsample': 1.0}\n\n#### ETC\nbest_parameters['etc']={'bootstrap': True, 'class_weight': 'balanced', 'criterion': 'entropy', 'max_features': 'auto', 'min_samples_leaf': 3, 'min_samples_split': 0.2, 'n_estimators': 100, 'n_jobs': -1, 'random_state': 10, 'warm_start': True}","5e9e88b8":"from collections import defaultdict\nbest_parameters = defaultdict(list,\n            {'xgb': {'colsample_bytree': 0.5, 'learning_rate': 0.02, 'min_child_weight': 0.01, 'n_estimators': 70, 'n_jobs': -1, 'objective': 'binary:logistic', 'subsample': 0.4},\n             'knc': {'algorithm': 'auto',\n              'n_jobs': -1,\n              'n_neighbors': 8,\n              'p': 1},\n             'rfc': {'criterion': 'entropy',\n              'min_samples_split': 2,\n              'n_estimators': 100,\n              'n_jobs': -1},\n             'svc': {'C': 10, 'gamma': 'scale', 'kernel': 'rbf'},\n             'gbc': {'criterion': 'mse',\n              'learning_rate': 0.1,\n              'loss': 'deviance',\n              'max_depth': 3,\n              'n_estimators': 150,\n              'subsample': 1.0},\n             'etc': {'bootstrap': True,\n              'class_weight': 'balanced',\n              'criterion': 'entropy',\n              'max_features': 'auto',\n              'min_samples_leaf': 3,\n              'min_samples_split': 0.2,\n              'n_estimators': 100,\n              'n_jobs': -1,\n              'random_state': 10,\n              'warm_start': True}})","d26a65e7":"clf_gbc = GradientBoostingClassifier(**best_parameters['gbc'] )#n_estimators=80, n_jobs=-1)# 20 - 80\nclf_RFC = RandomForestClassifier(**best_parameters['rfc'] )#\nclf_KNC = KNC(**best_parameters['knc'] )\nxgb_model = XGBClassifier(**best_parameters['xgb'])\nsvc = svm.SVC(**best_parameters['svc'])\netc = ExtraTreesClassifier(**best_parameters['etc'])\n\nmodels = { 'xgb': xgb_model,'svc':svc, 'rfc':clf_RFC, 'knc': clf_KNC,'gbc':clf_gbc,'etc':etc }\n\nskf = StratifiedKFold(n_splits=5)\n# Parameter tunning\nscores = ['f1']\n# Best parameters\nbest_parameters_bagging = defaultdict(list)\n#lista = ['etc']\nfor model_name in models.keys():\n    params_bagging = [{'n_estimators': [10,40,80,100,120], 'max_samples':[.5,.8,1.0],'base_estimator': [models[model_name]],'n_jobs':[-1] }]\n    print(\"######### Bagging MODEL tunning hyper-parameters for %s\" % model_name)\n    for score in scores:\n        print(\"# %s - Tuning hyper-parameters for %s\" % (model_name, score))\n        clf_i = GridSearchCV(BaggingClassifier(), params_bagging, scoring='%s_macro' % score, n_jobs=-1, cv=skf)\n        clf_i.fit(X_train,y_train)\n        print(\"Best parameters set found on development set:\")\n        print()\n        print(clf_i.best_params_)\n        best_parameters_bagging[model_name] = clf_i.best_params_\n        print(\"Grid scores on development set:\")\n        print()\n        means = clf_i.cv_results_['mean_test_score']\n        stds = clf_i.cv_results_['std_test_score']\n#         for mean, std, params in zip(means, stds, clf_i.cv_results_['params']):\n#             print(\"%s_macro - %0.3f (+\/-%0.03f) for %r\"% (score, mean, std * 2, params))\n        print(\"Detailed Bagging classification report:\")\n        print()\n        y_true, y_pred = y_train, clf_i.predict(X_train)\n        print(classification_report(y_true, y_pred))","69aa8232":"from sklearn.model_selection import StratifiedKFold, KFold,cross_val_score #( X_train_PCA , label)\nfrom sklearn.metrics import f1_score\nskf = StratifiedKFold(n_splits=5)\n\nclf_gbc = GradientBoostingClassifier(**best_parameters['gbc'] )#n_estimators=80, n_jobs=-1)# 20 - 80\nclf_RFC = RandomForestClassifier(**best_parameters['rfc'] )#\nclf_KNC = KNC(**best_parameters['knc'] )\nxgb_model = XGBClassifier(**best_parameters['xgb'])\nsvc = svm.SVC(**best_parameters['svc'])\netc = ExtraTreesClassifier(**best_parameters['etc'])\n\nbest_parameters_vc = defaultdict(list)\nweights = [ [1,1,1,1,1,1],[0.8,1,0.8,0.8,0.8,.8],[0.8,0.8,1,0.8,0.8,.8],[0.8,0.8,0.8,1,0.8,.8],[0.8,0.8,0.8,0.8,1,.8],[1,0.8,0.6,0.4,0.4,.4],[0.8,1,0.8,0.6,0.4,.4],[0.6,0.8,1,0.8,0.6,.4],[0.4,.4,0.6,0.8,1,0.8],\n              [1,1,.8,.8,.8,1]]\n\n\n#Cross Validation to chose the best parameter\nfor ind_,weight in enumerate(weights):\n    cross_f1 = []\n    model_VC = VotingClassifier (estimators=[ ('xgb', xgb_model), ('knc', clf_KNC),('rfc',clf_RFC),('svc',svc),('gbc',clf_gbc),('etc',etc)], voting='hard', weights=weight,n_jobs=-1)\n    for train_k, test_k in skf.split( X_train_PCA , label):\n        model_VC.fit(X_train_PCA[train_k], label.to_numpy()[train_k])\n        y_pred = model_VC.predict(X_train_PCA[test_k])\n        cross_f1.append(f1_score(label.to_numpy()[test_k], y_pred, average='macro'))\n    cross_f1 = np.stack(cross_f1).mean()\n    print('Iteraci\u00f3n {} F1_score: {}'.format(ind_, cross_f1 ))\n    best_parameters_vc[str(ind_)+'weight']={'weight':weight, 'f1_score' : cross_f1  }\n","3f885512":"model_VC = VotingClassifier (estimators=[ ('xgb', xgb_model), ('knc', clf_KNC),('rfc',clf_RFC),('svc',svc),('gbc',clf_gbc),('etc',etc)], voting='hard', weights=weights[0],n_jobs=-1)\nmodel_VC.fit(X_train_PCA, label)\ny_pred = model_VC.predict(X_train_PCA)\nprint('F1 Score for Voting model = {}'.format(f1_score(label, y_pred, average='macro')))\nplot_confusion_matrix(y_true=label, y_pred=y_pred, class_names=['No', 'Yes'],title=\"VotingClassifier\",normalize=True,size=4)","0d3f1d07":"from sklearn.ensemble import BaggingClassifier\nBclf_gbc = BaggingClassifier ( GradientBoostingClassifier(**best_parameters['gbc']),max_samples=1.0,n_jobs=-1,n_estimators= 100 )#n_estimators=80, n_jobs=-1)# 20 - 80\nBclf_RFC = BaggingClassifier ( RandomForestClassifier(**best_parameters['rfc'] ),max_samples=1.0,n_jobs=-1,n_estimators= 100  )#\nBclf_KNC = BaggingClassifier ( KNC(**best_parameters['knc'] ),max_samples=1.0,n_jobs=-1,n_estimators= 100 )\nBxgb_model = BaggingClassifier ( XGBClassifier(**best_parameters['xgb']),max_samples=1.0,n_jobs=-1,n_estimators= 100 )\nBsvc = BaggingClassifier ( svm.SVC(**best_parameters['svc']),max_samples=1.0,n_jobs=-1,n_estimators= 100 )\nBetc = BaggingClassifier ( ExtraTreesClassifier(**best_parameters['etc']),max_samples=1.0,n_jobs=-1,n_estimators= 100 )\n\nbest_parameters_vc = defaultdict(list)\nweights = [ [1,1,1,1,1,1],[0.8,1,0.8,0.8,0.8,.8],[0.8,0.8,1,0.8,0.8,.8],[0.8,0.8,0.8,1,0.8,.8],[0.8,0.8,0.8,0.8,1,.8],[.8,0.8,0.8,0.8,0.8,1],[0.8,1,0.8,0.6,0.4,.4],[0.6,0.8,1,0.8,0.6,.4],[0.5,.5,0.6,0.8,1,0.8],\n              [1,1,.8,.8,.8,1]]\n\n#Cross Validation to chose the best parameter\nfor ind_,weight in enumerate(weights):\n    cross_f1 = []\n    Bmodel_VC = VotingClassifier (estimators=[ ('xgb', Bxgb_model), ('knc', Bclf_KNC),('rfc',Bclf_RFC),('svc',Bsvc),('gbc',Bclf_gbc),('etc',Betc)], voting='hard', weights=weight,n_jobs=-1)\n    for train_k, test_k in skf.split( X_train_PCA , label):\n        Bmodel_VC.fit(X_train_PCA[train_k], label.to_numpy()[train_k])\n        y_pred = Bmodel_VC.predict(X_train_PCA[test_k])\n        cross_f1.append(f1_score(label.to_numpy()[test_k], y_pred, average='macro'))\n    cross_f1 = np.stack(cross_f1).mean()\n    print('Iteraci\u00f3n {} F1_score: {}'.format(ind_, cross_f1 ))\n    best_parameters_vc[str(ind_)+'weight']={'weight':weight, 'f1_score' : cross_f1  }","15af158e":"best_weight = 3\nBmodel_VC = VotingClassifier (estimators=[ ('xgb', Bxgb_model), ('knc', Bclf_KNC),('rfc',Bclf_RFC),('svc',Bsvc),('gbc',Bclf_gbc),('etc',Betc)], voting='hard', weights=weights[best_weight],n_jobs=-1)\nBmodel_VC.fit(X_train_PCA, label)\ny_pred = Bmodel_VC.predict(X_train_PCA)\nprint('F1 Score for B Voting model = {}'.format(f1_score(label, y_pred, average='macro')))\nplot_confusion_matrix(y_true=label, y_pred=y_pred, class_names=['No', 'Yes'],title=\"B VotingClassifier\",normalize=True,size=4)","3cf27868":"from sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.utils import resample\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler\n\n# All sklearn Transforms must have the `transform` and `fit` methods\nclass DropColumns(BaseEstimator, TransformerMixin):\n    def __init__(self, columns):\n        self.columns = columns\n\n    def fit(self, X, y=None):\n        return self\n    \n    \n    def transform(self, X):\n        # Primero copiamos el dataframe de datos de entrada 'X'\n        data = X.copy()\n        # Devolvemos un nuevo dataframe de datos sin las columnas no deseadas\n        return data.drop(labels=self.columns, axis='columns')\n    ","de5a0cf7":"#df_local = pd.concat([df_temp_acep,df_temp_sosp])\npretraindata = PreTrainData(target = 'OBJETIVO')\ndf_local = pretraindata.fit(df, minNan2drop=3)\nprint(len(df_local['OBJETIVO']))\n\nprint(\"#############################\")\n\ntransformData = TransformData(n_components=10,target='OBJETIVO')\ntransformData.fit(df_local.drop(columns=['OBJETIVO']),df_local['OBJETIVO'])\naaa = transformData.transform( df_local.drop(columns=['OBJETIVO']) )\nprint(aaa.shape)","0b8f7199":"from sklearn.pipeline import Pipeline\nfrom sklearn.tree import DecisionTreeClassifier\n\n\n\npipeline_input = df[\n    [\n        'EFECTIVO',\n        'CXC',\n        'INVENTARIO',\n        'EQ_OFICINA',\n        'EQ_TRANSPORTE',\n        'TERRENOS_Y_CONSTRUCCIONES',\n        'CXP',\n        'CONTRIBUCIONES_X_PAGAR',\n        'ANTICIPOS_CTE',\n        'CAP_SOCIAL',\n        'UTILIDADES_ACUMULADAS',\n        'UTILIDAD_O_PERDIDA',\n        'TOTAL_VENTAS',\n        'TOTAL_COMPRAS',\n        'UTILIDAD_BRUTA',\n        'TOTAL_GASTOS',\n    ]\n]\n\npipeline_target = df['OBJETIVO']\n\nX_train, X_test, y_train, y_test = train_test_split(\n    pipeline_input,\n    pipeline_target,\n    test_size=0.8,\n    random_state=27,\n    stratify=pipeline_target,\n)\n\n#df_local = X_train.join(y_train)\n\n\n#df_temp_acep = df.loc[df['OBJETIVO' ]=='Aceptado']#.dropna(thresh=len(df.columns)-0)\n#df_temp_sosp = df.loc[df['OBJETIVO' ]=='Sospechoso']\n\n#df_local = pd.concat([df_temp_acep,df_temp_sosp])\npretraindata = PreTrainData(target = 'OBJETIVO')\ndf_local = pretraindata.fit(df, minNan2drop=3)\n\n\ntransformData = TransformData(n_components=10,target='OBJETIVO')\n############### MODEL\nBclf_gbc = BaggingClassifier ( GradientBoostingClassifier(**best_parameters['gbc']),max_samples=1.0,n_jobs=-1,n_estimators= 100 )#n_estimators=80, n_jobs=-1)# 20 - 80\nBclf_RFC = BaggingClassifier ( RandomForestClassifier(**best_parameters['rfc'] ),max_samples=1.0,n_jobs=-1,n_estimators= 100  )#\nBclf_KNC = BaggingClassifier ( KNC(**best_parameters['knc'] ),max_samples=1.0,n_jobs=-1,n_estimators= 100 )\nBxgb_model = BaggingClassifier ( XGBClassifier(**best_parameters['xgb']),max_samples=1.0,n_jobs=-1,n_estimators= 100 )\nBsvc = BaggingClassifier ( svm.SVC(**best_parameters['svc']),max_samples=1.0,n_jobs=-1,n_estimators= 100 )\nBetc = BaggingClassifier ( ExtraTreesClassifier(**best_parameters['etc']),max_samples=1.0,n_jobs=-1,n_estimators= 100 )\n\n\nBmodel_VC = VotingClassifier (estimators=[ ('xgb', Bxgb_model), ('knc', Bclf_KNC),('rfc',Bclf_RFC),('svc',Bsvc),('gbc',Bclf_gbc),('etc',Betc)], voting='hard', weights=[1,1,1,1,1,1],n_jobs=-1)\nBmodel_VC =  svm.SVC(**best_parameters['svc'])\n#Bmodel_VC.fit(X_train_PCA, label)\n#y_pred = Bmodel_VC.predict(X_train_PCA)\n#print('F1 Score for B Voting model = {}'.format(f1_score(label, y_pred, average='macro')))\n#plot_confusion_matrix(y_true=label, y_pred=y_pred, class_names=['No', 'Yes'],title=\"B VotingClassifier\",normalize=True,size=4)\n\n#####################################\n#transformData.fit(X=pipeline_input, y=pipeline_target)\n#X, y = transformData.transform(X=pipeline_input, y=pipeline_target)\n# Separaci\u00f3n de los datos en un set de entrenamiento y otro de prueba (PARA CREACION DEL PIPELINE)\n\n# Creaci\u00f3n de nuestro para almacenamiento en Watson Machine Learning:\nmy_pipeline = Pipeline(\n    steps=[\n        ('preproces_transform', transformData),\n\n        ('modelo_BaggingClassfier', Bmodel_VC),\n    ]\n)\n\nmy_pipeline.fit(df_local.drop(columns=['OBJETIVO']),df_local['OBJETIVO'])\n","1f1edcd3":"from sklearn.metrics import confusion_matrix\n\ny_pred = my_pipeline.predict(df_local.drop(columns=['OBJETIVO']))\ncf_matrix = confusion_matrix(df_local['OBJETIVO'], y_pred)\ngroup_names = ['Aprobado `Aceptado`', 'Refused `Sospechoso`', 'Refused `Aceptado`', 'Aprobado `Sospechoso`']\ngroup_counts = ['{0:0.0f}'.format(value) for value in cf_matrix.flatten()]\ngroup_percentages = ['{0:.2%}'.format(value) for value in cf_matrix.flatten()\/np.sum(cf_matrix)]\nlabels = [f'{v1}\\n{v2}\\n{v3}' for v1, v2, v3 in zip(group_names, group_counts, group_percentages)]\nlabels = np.asarray(labels).reshape(2,2)\naccuracy  = np.trace(cf_matrix) \/ float(np.sum(cf_matrix))\nprecision = cf_matrix[1,1] \/ sum(cf_matrix[:,1])\nrecall    = cf_matrix[1,1] \/ sum(cf_matrix[1,:])\nf1_score  = 2*precision*recall \/ (precision + recall)\nsns.heatmap(cf_matrix, annot=labels, fmt='')\nstats_text = \"\\n\\nAccuracy={:0.3f}\\nPrecision={:0.3f}\\nRecall={:0.3f}\\nF1 Score={:0.3f}\".format(accuracy, precision, recall, f1_score)\nplt.ylabel('True label')\nplt.xlabel('Predicted label' + stats_text)","d71d0363":"cf_matrix[:,1]","e48bd892":"from sklearn.metrics import f1_score\nf1_score(y_test, y_pred, average='macro')","2a060e77":"# Creaci\u00f3n de instancias de una transformaci\u00f3n DropColumns\nrm_columns = DropColumns(\n    columns=[\"CXC\", \"CXP\"]  # Esta transformaci\u00f3n toma como par\u00e1metro una lista con los nombres de las columnas no deseadas\n)\n\nprint(rm_columns)","b6f2595c":"# Ver las columnas del conjunto de datos original\nprint(\"Columnas del conjunto de datos original: \\n\")\nprint(df.columns)\n\n# Aplicar la transformaci\u00f3n ``DropColumns`` al conjunto de datos base\nrm_columns.fit(X=df)\n\n# Reconstruyendo un DataFrame de Pandas con el resultado de la transformaci\u00f3n\ndf2 = pd.DataFrame.from_records(\n    data=rm_columns.transform(X=df)\n)\n\n# Ver las columnas del conjunto de datos transformado\nprint(\"\\n\\nColumnas del conjunto de datos despu\u00e9s de la transformaci\u00f3n ``DropColumns``: \\n\")\nprint(df2.columns)","babde364":"# Crear un objeto ``SimpleImputer``\nsi = SimpleImputer(\n    missing_values=np.nan,  # los valores que faltan son del tipo ``np.nan`` (Pandas est\u00e1ndar)\n    strategy='constant',  # la estrategia elegida es cambiar el valor faltante por una constante\n    fill_value=0,  # la constante que se usar\u00e1 para completar los valores faltantes es un int64 = 0\n    verbose=0,\n    copy=True\n)\n\nprint(si)","f7625d47":"# Ver los datos faltantes del conjunto de datos antes de la primera transformaci\u00f3n (df_data_2)\nprint(\"Valores nulos antes de la transformaci\u00f3n SimpleImputer: \\n\\n{}\\n\".format(df2.isnull().sum(axis = 0)))\n\n# Aplicamos el SimpleImputer ``si`` al conjunto de datos df_data_2 (resultado de la primera transformaci\u00f3n)\nsi.fit(X=df2)\n\n# Reconstrucci\u00f3n de un nuevo DataFrame de Pandas con el conjunto imputado (df_data_3)\ndf3 = pd.DataFrame.from_records(\n    data=si.transform(\n        X=df2\n    ),  # el resultado SimpleImputer.transform (<< pandas dataframe >>) es lista lista\n    columns=df2.columns  # las columnas originales deben conservarse en esta transformaci\u00f3n\n)\n\n# Ver los datos faltantes del conjunto de datos despu\u00e9s de la segunda transformaci\u00f3n (SimpleImputer) (df_data_3)\nprint(\"\\n\\nValores nulos en el conjunto de datos despu\u00e9s de la transformaci\u00f3n SimpleImputer: \\n\\n{}\\n\".format(df3.isnull().sum(axis = 0)))","4a99e95e":"# Definiendo las variables features y target (removed CXC and CXP)\n\nfeatures = df3[\n    [\n        'EFECTIVO',\n        'INVENTARIO',\n        'EQ_OFICINA',\n        'EQ_TRANSPORTE',\n        'TERRENOS_Y_CONSTRUCCIONES',\n        'CONTRIBUCIONES_X_PAGAR',\n        'ANTICIPOS_CTE',\n        'CAP_SOCIAL',\n        'UTILIDADES_ACUMULADAS',\n        'UTILIDAD_O_PERDIDA',\n        'TOTAL_VENTAS',\n        'TOTAL_COMPRAS',\n        'UTILIDAD_BRUTA',\n        'TOTAL_GASTOS',\n    ]\n]\ntarget = df3[\"OBJETIVO\"]  ## No cambie la variable target!","72bc510f":"X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=None, stratify = target)","2f4d26ee":"from sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\n\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","0cf204c1":"# M\u00e9todo para creacion de modelos basados en arbol de desici\u00f3n\nfrom sklearn.tree import DecisionTreeClassifier\n\ndtc = DecisionTreeClassifier(max_depth=3)\nmodel = dtc.fit(X_train, y_train)","f2216af4":"y_pred = dtc.predict(X_test)\nprint(y_pred)","149f2328":"\nfrom sklearn.metrics import confusion_matrix\n\ncf_matrix = confusion_matrix(y_test, y_pred)\ngroup_names = ['Aprobado `Aceptado`', 'Refused `Sospechoso`', 'Refused `Aceptado`', 'Aprobado `Sospechoso`']\ngroup_counts = ['{0:0.0f}'.format(value) for value in cf_matrix.flatten()]\ngroup_percentages = ['{0:.2%}'.format(value) for value in cf_matrix.flatten()\/np.sum(cf_matrix)]\nlabels = [f'{v1}\\n{v2}\\n{v3}' for v1, v2, v3 in zip(group_names, group_counts, group_percentages)]\nlabels = np.asarray(labels).reshape(2,2)\naccuracy  = np.trace(cf_matrix) \/ float(np.sum(cf_matrix))\nprecision = cf_matrix[1,1] \/ sum(cf_matrix[:,1])\nrecall    = cf_matrix[1,1] \/ sum(cf_matrix[1,:])\nf1_score  = 2*precision*recall \/ (precision + recall)\nsns.heatmap(cf_matrix, annot=labels, fmt='')\nstats_text = \"\\n\\nAccuracy={:0.3f}\\nPrecision={:0.3f}\\nRecall={:0.3f}\\nF1 Score={}\".format(accuracy, precision, recall, f1_score)\nplt.ylabel('True label')\nplt.xlabel('Predicted label' + stats_text)","e7966218":"# Susbtituya el link de abajo por el link de su repositorio git (se es necesario)\n!git clone https:\/\/github.com\/vnderlev\/sklearn_transforms.git","14a92244":"!cd sklearn_transforms && ls -ltr","4e952301":"!zip -r sklearn_transforms.zip sklearn_transforms","58827faf":"!pip install sklearn_transforms.zip","1a6cf264":"from my_custom_sklearn_transforms.sklearn_transformers import DropColumns","15e9cfdc":"pipeline_input = df[\n    [\n        'EFECTIVO',\n        'CXC',\n        'INVENTARIO',\n        'EQ_OFICINA',\n        'EQ_TRANSPORTE',\n        'TERRENOS_Y_CONSTRUCCIONES',\n        'CXP',\n        'CONTRIBUCIONES_X_PAGAR',\n        'ANTICIPOS_CTE',\n        'CAP_SOCIAL',\n        'UTILIDADES_ACUMULADAS',\n        'UTILIDAD_O_PERDIDA',\n        'TOTAL_VENTAS',\n        'TOTAL_COMPRAS',\n        'UTILIDAD_BRUTA',\n        'TOTAL_GASTOS',\n    ]\n]\n\npipeline_target = df['OBJETIVO']","cb9468e3":"# Separaci\u00f3n de los datos en un set de entrenamiento y otro de prueba (PARA CREACION DEL PIPELINE)\nX_train, X_test, y_train, y_test = train_test_split(\n    pipeline_input,\n    pipeline_target,\n    test_size=0.3,\n    random_state=21233,\n    stratify=pipeline_target,\n)","b664b061":"# Creacion de la Transformaci\u00f3n Personalizada ``DropColumns``\n\nrm_columns = DropColumns(\n    columns=['CXC', 'CXP']\n)","65ac6272":"# Crear un objeto ``SimpleImputer``\n\nsi = SimpleImputer(\n    missing_values=np.nan,  # los valores que faltan son del tipo ``np.nan`` (Pandas est\u00e1ndar)\n    strategy='constant',  # la estrategia elegida es cambiar el valor faltante por una constante (Ejemplo)\n    fill_value=0,  # la constante que se usar\u00e1 para completar los valores faltantes es un int64 = 0\n    verbose=0,\n    copy=True\n)","3be8fd16":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()","fc4110d5":"# Creaci\u00f3n de nuestro para almacenamiento en Watson Machine Learning:\nmy_pipeline = Pipeline(\n    steps=[\n        ('paso_1_remove_cols', rm_columns),\n        ('paso_2_imputer', si),\n        ('paso_3_standard_scaler', sc),\n        ('su_modelo', DecisionTreeClassifier(max_depth=3)),\n    ]\n)","a660d842":"# Inicializando el Pipeline (pre-procesamiento y entrenamiento del modelo)\nmy_pipeline.fit(X_train, y_train)","7aabfeae":"### Haciendo una predicci\u00f3n con el set de prueba\n\ny_pred = my_pipeline.predict(X_test)\ncf_matrix = confusion_matrix(y_test, y_pred)\ngroup_names = ['Aprobado `Aceptado`', 'Refused `Sospechoso`', 'Refused `Aceptado`', 'Aprobado `Sospechoso`']\ngroup_counts = ['{0:0.0f}'.format(value) for value in cf_matrix.flatten()]\ngroup_percentages = ['{0:.2%}'.format(value) for value in cf_matrix.flatten()\/np.sum(cf_matrix)]\nlabels = [f'{v1}\\n{v2}\\n{v3}' for v1, v2, v3 in zip(group_names, group_counts, group_percentages)]\nlabels = np.asarray(labels).reshape(2,2)\naccuracy  = np.trace(cf_matrix) \/ float(np.sum(cf_matrix))\nprecision = cf_matrix[1,1] \/ sum(cf_matrix[:,1])\nrecall    = cf_matrix[1,1] \/ sum(cf_matrix[1,:])\nf1_score  = 2*precision*recall \/ (precision + recall)\nsns.heatmap(cf_matrix, annot=labels, fmt='')\nstats_text = \"\\n\\nAccuracy={:0.3f}\\nPrecision={:0.3f}\\nRecall={:0.3f}\\nF1 Score={:0.3f}\".format(accuracy, precision, recall, f1_score)\nplt.ylabel('True label')\nplt.xlabel('Predicted label' + stats_text)","e6e07169":"# Precisi\u00f3n simple\nmy_pipeline.score(X_test, y_test)","0a158ce0":"!rm -r WMLC_mod -f\n!git clone https:\/\/github.com\/DavidCastilloAlvarado\/WMLC_mod.git\n!zip -r watson_machine_learning_client_mod.zip WMLC_mod\n!pip install watson_machine_learning_client_mod.zip","1c8bbe6e":"# Biblioteca Python con implementaci\u00f3n de un cliente HTTP para la API de WML\nfrom watson_machine_learning_client import WatsonMachineLearningAPIClient","a60f6ccd":"wml_credentials = {\n    \"instance_id\":\"\",\n  \"apikey\": \"dPoaJ1H9KN8kC2-xYj_N48beLXG9SAcXffls3x-cMAhj\",\n  \"iam_apikey_description\": \"Auto-generated for key a41689c6-8075-4bb4-b51b-61bfb88b1368\",\n  \"iam_apikey_name\": \"Auto-generated service credentials\",\n  \"iam_role_crn\": \"crn:v1:bluemix:public:iam::::serviceRole:Manager\",\n  \"iam_serviceid_crn\": \"crn:v1:bluemix:public:iam-identity::a\/2673a5220da64e36902b5e85a921bae2::serviceid:ServiceId-058227ac-c1ae-48ec-8d79-84ff94a864a1\",\n  \"url\": \"https:\/\/api.us-south.natural-language-understanding.watson.cloud.ibm.com\/instances\/474ed705-6893-4c7b-b09e-f997fd17d734\"\n}","94ba5f2d":"# Instanciando un objeto cliente de Watson Machine Learning a partir de las credenciales\n\nclientWML = WatsonMachineLearningAPIClient(wml_credentials)","d198b442":"# Extrayendo los detalles de su de Watson Machine Learning\n\ninstance_details = clientWML.service_instance.get_details()\nprint(json.dumps(instance_details, indent=4))","74baee98":"# Listando todos los artefatos actualmente almacenados en su instancia de WML\n\nclientWML.repository.list()","013367ef":"#Celda para borrar los Deployments:\n\nfor uid in clientWML.deployments.get_uids():\n    if clientWML.deployments.get_details(uid)['entity']['name'] == 'deployment_meta_4' : \n        print('Deleted ' + clientWML.deployments.get_details(uid)['entity']['name'] )\n        clientWML.deployments.delete(uid)","ca1d3b57":"#Celda para borrar todos los recursos:\n\nd = clientWML.repository.get_details()\nfor k in d:\n    for res in d[k][\"resources\"]:\n        if res['entity']['name'] in ['package_meta_4', 'runtime_meta_4', 'pipeline_meta_4']:\n            clientWML.repository.delete(res[\"metadata\"][\"guid\"])\n            print('Deleted ' + res['entity']['name'])","d79f3948":"# Listando todos los artefatos actualmente almacenados en su instancia de WML\n\nclientWML.repository.list()","41dbe257":"# Definici\u00f3n de metadatos del paquete con las Transformaciones personalizadas\npkg_meta = {\n    clientWML.runtimes.LibraryMetaNames.NAME: \"package_meta_4\",\n    clientWML.runtimes.LibraryMetaNames.DESCRIPTION: \"A custom sklearn transform package\",\n    clientWML.runtimes.LibraryMetaNames.FILEPATH: \"sklearn_transforms.zip\",  # Note que estamos utilizando o .zip criado anteriormente!\n    clientWML.runtimes.LibraryMetaNames.VERSION: \"1.0\",\n    clientWML.runtimes.LibraryMetaNames.PLATFORM: { \"name\": \"python\", \"versions\": [\"3.6\"] }\n}\ncustom_package_details = clientWML.runtimes.store_library( pkg_meta )\ncustom_package_uid = clientWML.runtimes.get_library_uid( custom_package_details )\n\nprint(\"\\n Lista de artefactos de runtime almacenados en WML:\")\nclientWML.repository.list()","eaa0590a":"runtime_meta = {\n    clientWML.runtimes.ConfigurationMetaNames.NAME: \"runtime_meta_4\",\n    clientWML.runtimes.ConfigurationMetaNames.DESCRIPTION: \"A Python runtime with custom sklearn Transforms\",\n    clientWML.runtimes.ConfigurationMetaNames.PLATFORM: {\n        \"name\": \"python\",\n        \"version\": \"3.6\"\n    },\n    clientWML.runtimes.ConfigurationMetaNames.LIBRARIES_UIDS: [ custom_package_uid ]\n}\nruntime_details = clientWML.runtimes.store( runtime_meta )\ncustom_runtime_uid = clientWML.runtimes.get_uid( runtime_details )\n\nprint(\"\\n Detalles del runtime almacenados:\")\nprint(json.dumps(runtime_details, indent=4))","594d034b":"# Listando todos los runtimes almacenados en su WML:\nclientWML.runtimes.list()","41db1043":"model_meta = {\n    clientWML.repository.ModelMetaNames.NAME: 'pipeline_meta_4',\n    clientWML.repository.ModelMetaNames.DESCRIPTION: \"my pipeline for submission\",\n    clientWML.repository.ModelMetaNames.RUNTIME_UID: custom_runtime_uid\n}","96c212c6":"# Funci\u00f3n para almacenar una definici\u00f3n de Pipeline en WML\nstored_model_details = clientWML.repository.store_model(\n    model=my_pipeline,  # `my_pipeline` es la variable creada anteriormente que contiene nuestro Pipeline ya entrenado :)\n    meta_props=model_meta,  # Metadatos definidos en la celda anterior\n    training_data=None  # No altere este parametro\n)\n\nprint(\"\\n Lista de artefatos almacenados en WML:\")\nclientWML.repository.list()\n\n# Datalles del modelo hospedado en Watson Machine Learning\nprint(\"\\n Metadatos del modelo almacenado:\")\nprint(json.dumps(stored_model_details, indent=4))","3e163b8b":"# El deployment del modelo es finalmente realizado por medio del m\u00e9todo ``deployments.create()``\n\nmodel_deployment_details = clientWML.deployments.create(\n    artifact_uid=stored_model_details[\"metadata\"][\"guid\"],  # No altere este parametro\n    name=\"deployment_meta_4\",\n    description=\"Desafio 4 MBTC\",\n    asynchronous=False,  # No altere este parametro\n    deployment_type='online',  # No altere este parametro\n    deployment_format='Core ML',  # No altere este parametro\n    meta_props=model_meta  # No altere este parametro\n)","41c2c66e":"# Recuperando la URL endpoint dl modelo hospedado en la celda anterior\n\nmodel_endpoint_url = clientWML.deployments.get_scoring_url(model_deployment_details)\nprint(\"La URL de llamada de su API es: {}\".format(model_endpoint_url))","b7c86d22":"# Detalles del deployment realizado\n\ndeployment_details = clientWML.deployments.get_details(\n    deployment_uid=model_deployment_details[\"metadata\"][\"guid\"]  # Este es el ID de su deployment!\n)\n\nprint(\"Metadatos del deployment realizado: \\n\")\nprint(json.dumps(deployment_details, indent=4))","3964d8ac":"scoring_payload = {\n    'fields': [\n        'EFECTIVO',\n        'CXC',\n        'INVENTARIO',\n        'EQ_OFICINA',\n        'EQ_TRANSPORTE',\n        'TERRENOS_Y_CONSTRUCCIONES',\n        'CXP',\n        'CONTRIBUCIONES_X_PAGAR',\n        'ANTICIPOS_CTE',\n        'CAP_SOCIAL',\n        'UTILIDADES_ACUMULADAS',\n        'UTILIDAD_O_PERDIDA',\n        'TOTAL_VENTAS',\n        'TOTAL_COMPRAS',\n        'UTILIDAD_BRUTA',\n        'TOTAL_GASTOS',\n    ],\n    'values': [\n        [\n            968866.8993,\n            102102.000,\n            8539205.63,\n            3898282.548,\n            416873.3265,\n            1420050.089,\n            -629717.8548,\n            14613560.64,\n            7620711.462,\n            116647.7396,\n            1798064.624,\n            9535423.826,\n            3657339.603,\n            770284.5004,\n            -102101.201,\n            -711032.0155\n        ]\n    ]\n}\n\nprint(\"\\n Payload de datos a ser clasificado:\")\nprint(json.dumps(scoring_payload, indent=4))","73dfcc13":"result = clientWML.deployments.score(\n    model_endpoint_url,\n    scoring_payload\n)\n\nprint(\"\\n Resultados:\")\nprint(json.dumps(result, indent=4))","92316882":"Las pr\u00f3ximas celdas realizaran el despliegue del pipeline declarada en este notebook en WML. Solo prosiga si usted ya est\u00e1 satisfecho con su modelo y cree que ya es hora de hacer el despliegue de su soluci\u00f3n.\n\nCopie las credenciales de su instancia de Watson Machine Learning en la variable de la celda de abajo.\n\nEs importante que la variable que contenga los valores de la credencial se llame ``wml_credentials`` para que las proximas celdas de este notebook se ejecuten corretamente.","26d3b7ff":"#### Creando una nueva definici\u00f3n de runtime Python personalizado en WML\n\nEl segundo paso es almacenar una definici\u00f3n de runtime Python para utilizar en nuestra biblioteca personalizada.\n\nEsto puede hacerse de la siguiente manera:","eff35d63":"En el reto 2 (Tortuga Code), se mostr\u00f3 c\u00f3mo crear una transformaci\u00f3n personalizada, declarando una clase Python con los m\u00e9todos ``fit`` y ``transform``.\n\n    - C\u00f3digo de transformaci\u00f3n personalizada DropColumns():\n    \n    from sklearn.base import BaseEstimator, TransformerMixin\n    # All sklearn Transforms must have the `transform` and `fit` methods\n    class DropColumns(BaseEstimator, TransformerMixin):\n        def __init__(self, columns):\n            self.columns = columns\n        def fit(self, X, y=None):\n            return self\n        def transform(self, X):\n            # Primero copiamos el dataframe de entrada 'X' de entrada\n            data = X.copy()\n            # Devolvemos un nuevo marco de datos sin las columnas no deseadas\n            return data.drop(labels=self.columns, axis='columns')\n\nPara integrar estos tipos de transformaciones personalizadas con Pipelines en Watson Machine Learning, primero debe empaquetar su c\u00f3digo personalizado como una biblioteca de Python. Esto se puede hacer f\u00e1cilmente usando la herramienta *setuptools*.\n\nEn el siguiente repositorio de git: https:\/\/github.com\/vnderlev\/sklearn_transforms tenemos todos los archivos necesarios para crear un paquete de Python, llamado **my_custom_sklearn_transforms**.\nEste paquete tiene la siguiente estructura de archivos:\n\n    \/my_custom_sklearn_transforms.egg-info\n        dependency_links.txt\n        not-zip-safe\n        PKG-INFO\n        SOURCES.txt\n        top_level.txt\n    \/my_custom_sklearn_transforms\n        __init__.py\n        sklearn_transformers.py\n    PKG-INFO\n    README.md\n    setup.cfg\n    setup.py\n    \nEl archivo principal, que contendr\u00e1 el c\u00f3digo para nuestras transformaciones personalizadas, es el archivo **\/my_custom_sklearn_transforms\/sklearn_transformers.py**. Si accedes a \u00e9l en el repositorio, notar\u00e1s que contiene exactamente el mismo c\u00f3digo declarado en el primer paso (la clase DropColumns).\n\nSi has declarado sus propias transformaciones (adem\u00e1s de la DropColumn proporcionada), debes agregar todas las clases de esas transformaciones creadas en este mismo archivo. Para hacer esto, debes hacer fork de este repositorio (esto se puede hacer en la propia interfaz web de Github, haciendo clic en el bot\u00f3n como se muestra en la imagen a continuaci\u00f3n) y agregue sus clases personalizadas al archivo **sklearn_transformers.py**.\n\n![alt text](https:\/\/i.imgur.com\/2lZ4Ty2.png \"forking-a-repo\")\n\nSi solo hizo uso de la transformaci\u00f3n proporcionada (DropColumns), puede omitir este paso de fork y continuar usando el paquete base provisto. :)\n\nDespu\u00e9s de preparar su paquete de Python con sus transformaciones personalizadas, reemplace el enlace del repositorio de git en la celda a continuaci\u00f3n y ejec\u00fatelo. Si no ha preparado ninguna transformaci\u00f3n nueva, ejecute la celda con el enlace del repositorio ya proporcionado.\n\n<hr>\n    \n**OBSERVACI\u00d3N**\n\nSi la ejecuci\u00f3n de la celda a continuaci\u00f3n devuelve un error de que el repositorio ya existe, ejecute:\n\n**!rm -r -f sklearn_transforms**","64e2fdf0":"### Encapsulando un Pipeline personalizado de Watson Machine Learning","4e2508e3":"#### Preparando transformaciones personalizadas para cargar en WML","d4a2f476":"### Pre-entrenando el modelo para definir el pipeline","9e436045":"## Reducci\u00f3n de dimensionalidad","0e57d194":"### Estandarizando los valores numericos","f34add97":"### Entrenamiento y evaluaci\u00f3n de un modelo de clasificaci\u00f3n binaria","220c2c58":"En este desaf\u00edo, CompuSoluciones usar\u00e1 herramientas de IBM como Watson Studio (o Cloud Pack for Data) y Watson Machine Learning para construir un modelo de Machine Learning natural capaz de predecir la probabilidad de cumplimiento de pago.\n\nLa idea esencial del Desaf\u00edo 4 es crear un modelo basado en machine learning capaz de identificar el comportamiento financiero del asociado de negocio, permitiendo una probabilidad de cumplimiento o incumplimiento del cr\u00e9dito.","98eafe11":"## Construyendo el model Pipeline","6db578e6":"<hr>","96bde70e":"Ahora que tenemos un Pipeline completo, con etapas de pre-procesamiento configuradas y tambien un modelo por \u00c1rbol de Desici\u00f3n entrenado, podemos realizar la integraci\u00f3n con Watson Machine Learning!","54099ef9":"Para subir o c\u00f3digo no WML, precisamos enviar um arquivo .zip com todo o c\u00f3digo fonte, ent\u00e3o iremos zipar o diret\u00f3rio clonado em seguida:","0b372996":"#### Definici\u00f3n de features (Variables Independientes)\n\nEn este * ejemplo * usaremos todas las columnas. (Usted debe decidir cuales variables utilizar)","8ade537a":"##### \u00a1ATENCI\u00d3N! Su puntuaci\u00f3n en este desaf\u00edo de clasificaci\u00f3n se basar\u00e1 en la puntuaci\u00f3n F1 del modelo.","7b0024e4":"<hr>","726b7ed2":"Podemos observar que la cantidad de casos sospechosos es inferior en ~x16 a los casos aceptados, por lo que nos confirma la posibilidad de eeliminar datos sin perder datos valiosos para el an\u00e1lisis.<br>\nSin embargo debemos curar los datos 'Sospechosos' para no perder informaci\u00f3n","451a30d7":"<hr>","b382b490":"### Descargando el dataset csv desde Github","35f9490c":"### Borrando las columnas del dataset original\n\nDebe elminiar todas las columnas que no esta usando","51470a23":"En la celda de abajo se declara un objeto **Pipeline** de scikit-learn, donde es declarado como parametros *steps*, que es una lista de etapas a ejecutar el pipeline:\n\n    'paso_1_remove_cols'     - Transformaci\u00f3n personalizada DropColumns\n    'paso_2_imputer'         - Transformaci\u00f3n embebida de scikit-learn para remplazar los valores faltantes\n    'paso_3_standard_scaler'          - Transformaci\u00f3n embebida de scikit-learn para escalar las variables num\u00e9ricas\n    'su_modelo'              - Un \u00e1rbol de desici\u00f3n simple\n    \nNote que pasamos como pasos las transformaciones instanciadas anteriormente, con nombres `rm_columns` y `si`.","6ec9dc28":"#### Transformaci\u00f3n 2: estandarizaci\u00f3n de Features","2fad8ee4":"Las variables son todas numericas. Solo nuestra variable TARGET (Prestamo aprobado o posible incumplimiento financiero) es the tipo float.\n\nLa funci\u00f3n describe() de abajo muestra varias estadisticas del dataset.","1a0982df":"#### Estableciendo conexi\u00f3n entre el cliente Python de WML y su instancia del servicio en la nube","140252c9":"## ATENCI\u00d3N!, SI UD CORRE LAS CELDAS DE ABAJO TODOS LOS DESPLIEGUES ANTERIORS SERAN BORRADOS","a67f99f2":"<hr>","dcd7f6a0":"## Parameter optimization","fb876d2f":"#### Entrenando un modelo ``DecisionTreeClassifier()``","7d375815":"#### Creando una nueva definici\u00f3n de Pipeline personalizado en WML\n\nFinalmente creando una definici\u00f3n (metadatos) para que nuestro Pipeline sea hospedada en WML.\n\nDefinimos como parametros el nombre para el artefacto y el ID de runtime creado anteriormente.","442c8d26":"### Construcci\u00f3n del Pipeline completo para el encapsulamiento en WML","70089985":"### Acerca del Dataset","ad9f008a":"El primer paso para realizar su deploy y almacenar el c\u00f3digo de las transformaciones personalizadas creadas por usted.\n\nPara esta etapa solo necesitamos el archivo .zip del paquete creado por usted (Que ya tenemos cargados en el Kernel!)","3ad292e1":"## Best parameters","1f7bea66":"#### Haciendo una predicci\u00f3n con el set de prueba","2dea9634":"### ATENCI\u00d3N: UD necesitar\u00e1 de la URL de arriba para entregar su modelo :)","ce2e9da4":"## Instalaci\u00f3n de Librerias","c164564a":"#### Analizar la calidad del modelo a trav\u00e9s de la matriz de confusi\u00f3n","4d055d46":"Para listar todos los artefatos almacenados en su Watson Machine Learning, usted puede usar la seguinte funci\u00f3n:\n\n    clientWML.repository.list()","3cc4659a":"# ATENCI\u00d3N, NO CAMBIE LA CELDA DE ABAJO O SU MODELO NO SERA EVALUADO","3163012a":"#### Probando el modelo hospedado en Watson Machine Learning","864ed944":"Podemos observar que solo utilizando un comportamiento logaritmico en el conteo, podemos visualizar informaci\u00f3n de utilidad.\n\n##### **Hipotesis**\n\n* Existe una sobrepoblaci\u00f3n de ciertos valores por lo que se podr\u00eda presindir de ellos\n\n* Es posible librarnos de la inmensa cantidad de valores nulos, con solo eliminar toda la muestra\n\n* Para no peder muestrar del tipo 'sospechoso' es necesario curar los datos perdidos\n\n* Se observa de los histogramas que todos los valores tiene un factor comun de 1e6 a 1e9, por lo que podemos normalizarlos sin incombenientes, para trabajar con n\u00fameros m\u00e1s interpretables","3c506e9a":"<hr>\n\n## \u00a1Felicitaciones! \n\nSi todo fue ejecutado sin errores, \u00a1usted ya tiene un predictor basado en clasificac\u00edon binaria encapsulado como una API REST!\n\nPara enviar su soluci\u00f3n, accede a la p\u00e1gina:\n\n# https:\/\/compusoluciones.maratona.dev\n\nUsted necesitar\u00e1 del endpoint url de su modelo y las credenciales de WML :)","620a2bd1":"### Matriz de Correlaci\u00f3n\n\nCon la finalidad de averiguar que variables guardan correlaci\u00f3n y en que magnitud, obtamos por evaluar la correlaci\u00f3n existente","bfbce5f0":"## Introducci\u00f3n","aff41080":"Ahora podemos realizar la importaci\u00f3n de nuestro paquiete personalizado en nuestro notabook!\n\nVamos a importan la transformaci\u00f3n DropColumns. Si usted posee otras transformaciones personalizadas, ahora es que debe importarlas","53653747":"En seguida llamamos el m\u00e9todo para almacenar una nueva definici\u00f3n:","badf2f52":"### Observando Histogramas de las caracteristicas","073afbb4":"En seguida ejecutaremos el m\u00e9todo `fit()` del Pipeline, realizando el pr\u00e9-procesamiento y el entrenamiento del modelo de una sola vez.","4bd8614f":"#### Listando todos los artefatos almacenados en su WML","8e256318":"### Realizando una llamada de API para su modelo almacenado en WML","abcd64dc":"\n#### Transformaci\u00f3n 1: excluir columnas del conjunto de datos\n\nPara la creaci\u00f3n de una transformaci\u00f3n de datos personalizada en scikit-learn, es necesario crear una clase con los m\u00e9todos transform y fit. En el m\u00e9todo de 'transform', se ejecutar\u00e1 la l\u00f3gica de nuestra transformaci\u00f3n.\n\nLa siguiente celda muestra el c\u00f3digo completo de una transformaci\u00f3n DropColumns para eliminar columnas de un pandas DataFrame.\n","cb2cc54e":"**\u00a1\u00a1ATENCI\u00d3N!!**\n\n\u00a1Este atento de los limites de consumo de su instancia de Watson Machine Learning!\n\nEn caso de que acabe la capa gratuita, no sera posible evualuar su modelo (Pues es necesario para la realizaci\u00f3n de algunas llamadasal API con sus predicciones!)","af30788c":"#### Datos Nulos","9fee2661":"### MODEL PREDICTIC TIME","0223991e":"## Bagging Classifier MODEL","3479733e":"Con el archivo zip de nuestro paquete cargado en el Kernel de este notebook, podemos utiliar la herramienta pip para instalarlo conforme a la siguiente celda:","baffea8b":"### Reemplazando con zeros en lugar de valores nulos\n\nUd puede usar otras estrategias, pero deben ser con Transforms","6b963b3d":"## Voting Classifier Model","fbfc21ee":"### Feature Engineering","f03843af":"#### Creando una nueva definici\u00f3n de paquete Python personalizado en WML","ba90e27f":"#### Transformaci\u00f3n 3: tratamiento de datos faltantes\n\nPara manejar los datos que faltan en nuestro conjunto de datos, ahora usaremos una transformaci\u00f3n lista para usar de la biblioteca scikit-learn, llamada SimpleImputer.\n\nEsta transformaci\u00f3n permite varias estrategias para el tratamiento de datos faltantes. La documentaci\u00f3n oficial se puede encontrar en: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.impute.SimpleImputer.html\n\nEn este ejemplo, simplemente haremos cero todos los valores faltante usted puede escoger otra estrategia ;).","fc8fffd7":"#### Realizando un deployment de su modelo para consumo inmediato por otras aplicaciones","4c982fda":"#### Divisi\u00f3n en 80% entrenamiento y 20% pruebas","397c49fb":"# MARATONA BEHIND THE CODE 2020\n\n## DESAFIO 4 - CompuSoluciones"}}