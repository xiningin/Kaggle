{"cell_type":{"8a849ddf":"code","592f57bc":"code","9cde5032":"code","bbac8458":"code","c77aa5c3":"code","89a7f29a":"code","a2e7bcde":"code","be81ea8a":"code","06600ccb":"code","1bd5077f":"markdown","c2de8015":"markdown","89b35d72":"markdown","806b0418":"markdown","ca43ab2f":"markdown"},"source":{"8a849ddf":"import numpy as np\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport matplotlib.pyplot as plt\n%matplotlib inline","592f57bc":"# Model \/ data parameters\nnum_classes = 10\ninput_shape = (28, 28, 1)\n\n# the data, split between train and test sets\n(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n\n# Scale images to the [0, 1] range\nx_train = x_train.astype(\"float32\") \/ 255\nx_test = x_test.astype(\"float32\") \/ 255\n# Make sure images have shape (28, 28, 1)\nx_train = np.expand_dims(x_train, -1)\nx_test = np.expand_dims(x_test, -1)\nprint(\"x_train shape:\", x_train.shape)\nprint(x_train.shape[0], \"train samples\")\nprint(x_test.shape[0], \"test samples\")\n\n\n# convert class vectors to binary class matrices\ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)","9cde5032":"model = keras.Sequential(\n    [\n        keras.Input(shape=input_shape),\n        layers.Conv2D(6, kernel_size=(5, 5), padding='same', activation='tanh'),\n        layers.AveragePooling2D(pool_size=(2, 2)),\n        layers.Conv2D(16, kernel_size=(5, 5), padding='valid', activation=\"tanh\"),\n        layers.AveragePooling2D(pool_size=(2, 2)),\n        layers.Flatten(),\n        layers.Dense(120, activation=\"tanh\"),\n        layers.Dense(84, activation=\"tanh\"),\n        layers.Dense(num_classes, activation=\"softmax\")\n    ]\n)\n\nmodel.summary()","bbac8458":"lr_list = [0.0005, 0.0005, 0.0002, 0.0002, 0.0002, 0.00005, \n           0.00005, 0.00005, 0.00005, 0.00001, 0.00001, 0.00001, \n           0.00001, 0.00001, 0.00001, 0.00001, 0.00001, 0.00001, \n           0.00001, 0.00001]\n\ndef calc_lr():\n    elem = lr_list[0]\n    del lr_list[0]\n    return(elem)\n\noptimizer = keras.optimizers.SGD(learning_rate=calc_lr)\nmodel.compile(loss=\"mean_squared_error\", optimizer=optimizer, metrics=[\"accuracy\"])\n    \nmodel.fit(x_train, y_train, batch_size=1, epochs=20, validation_split=0.0)","c77aa5c3":"score = model.evaluate(x_test, y_test, verbose=0)\nprint(\"Test loss:\", score[0])\nprint(\"Test accuracy:\", score[1])","89a7f29a":"# Compile the model\nmodel.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n\nhistory = model.fit(x_train, y_train, validation_split=0.0833, epochs=20)","a2e7bcde":"score = model.evaluate(x_test, y_test, verbose=0)\nprint(\"Test loss:\", score[0])\nprint(\"Test accuracy:\", score[1])","be81ea8a":"# Compile the model\nmodel.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n\nhistory = model.fit(x_train, y_train, batch_size=1, validation_split=0.0, epochs=20)","06600ccb":"score = model.evaluate(x_test, y_test, verbose=0)\nprint(\"Test loss:\", score[0])\nprint(\"Test accuracy:\", score[1])","1bd5077f":"Training takes about 130s per epoch on CPU in the Kaggle notebook. Accuracy varies slightly, one example is 0.9257 on training data an 0.9292 on test data. These are not good results, the original paper was above 99%.\n\nBy taking some hyperparameters from [this source](https:\/\/github.com\/3outeille\/Research-Paper-Summary\/blob\/master\/src\/architecture\/lenet-5\/tensorflow_2\/lenet_5_tf2.ipynb) which uses the same network structure we get significantly better results. \n\nNote: To clear the weights we have to run the section defining the model again. Otherweise you start from where the last training stopped.","c2de8015":"Training takes about 130s per epoch on CPU in the Kaggle notebook. Accuracy varies slightly, one example is 1.000 on training data an 0.9888 on test data. These are also reasonably good results compared to the original paper slightly above 99%. The accuracy of 1 on the training set sugggests overfitting but since accuracy on the test set is also good it is not critical.","89b35d72":"## Trying to Reproduce the Original Paper\n\nWhen reading the paper you will realise that an exact replication is not easily possible. If using Keras and trying to stick to the paper as close as easily possible you will end up with this network.","806b0418":"Training takes about 23s per epoch on CPU in the Kaggle notebook. Accuracy varies slightly, one example is 0.9887 on training data an 0.9868 on test data. These are reasonably good results compared to the original paper slightly above 99%.\n\nFor the last experiment we go back to batch size 1 and not crossvalidation.","ca43ab2f":"# Reimplementing the LeNet-5 Paper\n\nLeNet-5 sometimes is referred to as the \u201cHello World!\u201d of CNNs. We will reimplement a simplified version of it. We will base the implementation on the [original paper](http:\/\/yann.lecun.com\/exdb\/publis\/pdf\/lecun-98.pdf).\n\nWe work on MNIST which we have to adapt a little bit for our network structure. We use the classical train \/ test split of 50.000 to 10.000."}}