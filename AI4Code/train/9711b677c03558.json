{"cell_type":{"9b0b8087":"code","fe51a6ea":"code","954b41e2":"code","4c947e8c":"code","d2be07a1":"code","e019ffc7":"code","40e6d764":"code","86ca7e0b":"code","b477655d":"code","3a577432":"code","a8fa268f":"code","0354bd48":"code","017a43d8":"code","41631ebd":"code","56d19da8":"code","eee0884d":"code","26fbb1e5":"code","06d671f7":"code","500e08b0":"code","053e4454":"code","8da88555":"code","eef43bec":"code","74a72d1c":"code","02ec2947":"code","083f8e60":"code","9613f9a8":"code","864df8d3":"code","b4da9eb5":"code","3d70491d":"code","c5eb861e":"code","5c0981d8":"code","4daab0c2":"code","bdc68796":"code","85d8958f":"code","ab7a185a":"code","3a74d54d":"code","a5d11947":"markdown","25d8a2f2":"markdown","31a5aed5":"markdown","cc5657ed":"markdown","7105f058":"markdown","2d18e69a":"markdown","c444d67d":"markdown","11337c10":"markdown"},"source":{"9b0b8087":"import numpy as np\nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport warnings  \nwarnings.filterwarnings('ignore')","fe51a6ea":"df = pd.read_csv('..\/input\/StudentsPerformance.csv')\ndf.head()","954b41e2":"#Check for missing values\nif df.isnull().any().unique() == False:\n    print(\"No missing values\")\nelse: \n    print(df.isnull().any())","4c947e8c":"sns.pairplot(df)","d2be07a1":"#Skewness & Kurtosis\n#The values for asymmetry and kurtosis between -2 and +2 are considered acceptable in order to prove normal univariate distribution (George & Mallery, 2010)\nprint(\"Skewness \\n\", df.skew(), \"\\n\\nKurtosis\\n\", df.kurtosis())","e019ffc7":"#Outliers\nf, axes = plt.subplots(1, 3, figsize=(15,5))\nsns.boxplot(df[\"math score\"], orient='v' ,ax = axes[0])\nsns.boxplot(df[\"reading score\"],orient='v' , ax = axes[1])\nsns.boxplot(df[\"writing score\"],orient='v' , ax = axes[2])","40e6d764":"#Remove Outliers\ndef remove_outlier(col):\n    q1 = df[col].quantile(0.25)\n    q3 = df[col].quantile(0.75)\n    iqr = q3-q1\n    #Cut-Off\n    fence_low  = q1-1.5*iqr\n    fence_high = q3+1.5*iqr\n    #Remove Outliers\n    df2 = df.loc[(df[col] > fence_low) & (df[col] < fence_high)]\n    return df2\nnum_rows_org = df.shape[0]\ncolumns = [\"math score\", \"reading score\", \"writing score\"]\nfor i in columns:\n    df = remove_outlier(i) \nnum_rows_new = df.shape[0]\nprint(num_rows_org-num_rows_new,\"Outliers Removed\")","86ca7e0b":"f, axes = plt.subplots(1, 3, figsize=(15,5))\nsns.boxplot(df[\"math score\"], orient='v' ,ax = axes[0])\nsns.boxplot(df[\"reading score\"],orient='v' , ax = axes[1])\nsns.boxplot(df[\"writing score\"],orient='v' , ax = axes[2])","b477655d":"#Skewness & Kurtosis\n#The values for asymmetry and kurtosis between -2 and +2 are considered acceptable in order to prove normal univariate distribution (George & Mallery, 2010)\nprint(\"Skewness \\n\", df.skew(), \"\\n\\nKurtosis\\n\", df.kurtosis())","3a577432":"sns.pairplot(df)","a8fa268f":"#Encode Variables\nfrom sklearn.preprocessing import LabelEncoder\ndf['gender'] = LabelEncoder().fit_transform(df[\"gender\"]) #'male' or 'female'\ndf['race\/ethnicity'] = LabelEncoder().fit_transform(df[\"race\/ethnicity\"]) #Group A to E\ndf['parental level of education'] = LabelEncoder().fit_transform(df[\"parental level of education\"])#'bachelor's degree', 'some college', \"master's degree\",\"associate's degree\", 'high school' or 'some high school'\ndf['lunch'] = LabelEncoder().fit_transform(df[\"lunch\"]) #'standard' or 'free\/reduced'\ndf['test preparation course'] = LabelEncoder().fit_transform(df[\"test preparation course\"])#'none' or 'completed'\n\ndf.head(33)","0354bd48":"df[\"overall_score\"] = ((df[\"math score\"] + df[\"reading score\"] + df[\"writing score\"]) \/ 3).astype(int)\nmedian = df[\"overall_score\"].median()\ndf.head()","017a43d8":"f, axes = plt.subplots(1, 5, figsize=(25,5))\nsns.countplot(df[\"gender\"], ax = axes[0])\nsns.countplot(df[\"race\/ethnicity\"], ax = axes[1])\nsns.countplot(df[\"parental level of education\"], ax = axes[2])\nsns.countplot(df[\"lunch\"], ax = axes[3])\nsns.countplot(df[\"test preparation course\"], ax = axes[4])","41631ebd":"corr = df.corr()\nsns.heatmap(corr)","56d19da8":"print(\"Boys with Lunch:\", len(df[(df[\"gender\"] == 0)  & (df[\"lunch\"] == 0)]) \/ len(df[df[\"gender\"] == 0]))\nprint(\"Girls with Lunch:\", len(df[(df[\"gender\"] == 1)  & (df[\"lunch\"] == 0)]) \/ len(df[df[\"gender\"] == 1]))\n\nprint(\"Boys with Preparation:\", len(df[(df[\"gender\"] == 0)  & (df[\"test preparation course\"] == 0)]) \/ len(df[df[\"gender\"] == 0]))\nprint(\"Girls with Preparation:\", len(df[(df[\"gender\"] == 1)  & (df[\"test preparation course\"] == 0)]) \/ len(df[df[\"gender\"] == 1]))\n\nprint(\"Preparation with Lunch:\", len(df[(df[\"lunch\"] == 0)  & (df[\"test preparation course\"] == 0)]) \/ len(df[df[\"test preparation course\"] == 0]))\nprint(\"Preparation without Lunch:\", len(df[(df[\"lunch\"] == 1)  & (df[\"test preparation course\"] == 0)]) \/ len(df[df[\"test preparation course\"] == 1]))","eee0884d":"from scipy.stats import ttest_ind\nw_lunch_mean = df[(df[\"lunch\"] == 0)]\nwo_lunch_mean = df[(df[\"lunch\"] == 1)]\n\nprint(ttest_ind(w_lunch_mean['overall_score'], wo_lunch_mean['overall_score'], nan_policy='omit'))\n\nprint('With Lunch:', df[(df[\"lunch\"] == 0)].overall_score.mean())\nprint('Without Lunch:', df[(df[\"lunch\"] == 1)].overall_score.mean())","26fbb1e5":"from scipy.stats import ttest_ind\nw_course_mean = df[(df[\"test preparation course\"] == 0)]\nwo_course_mean = df[(df[\"test preparation course\"] == 1)]\n\nprint(ttest_ind(w_course_mean['overall_score'], wo_course_mean['overall_score'], nan_policy='omit'))\n\nprint('With Course:', df[(df[\"test preparation course\"] == 0)].overall_score.mean())\nprint('Without Course:', df[(df[\"test preparation course\"] == 1)].overall_score.mean())","06d671f7":"from scipy.stats import ttest_ind\nn_lunch_n_course = df[(df[\"lunch\"] == 1)  & (df[\"test preparation course\"] == 1)]\nlunch_course = df[(df[\"lunch\"] == 0)  & (df[\"test preparation course\"] == 0)]\n\nprint(ttest_ind(n_lunch_n_course['overall_score'], lunch_course['overall_score'], nan_policy='omit'))\n\nprint('With Course:', df[(df[\"lunch\"] == 0)  & (df[\"test preparation course\"] == 0)].overall_score.mean())\nprint('Without Course:', df[(df[\"lunch\"] == 0)  & (df[\"test preparation course\"] == 0)].overall_score.mean())","500e08b0":"from sklearn.model_selection import cross_val_score, RepeatedKFold, train_test_split\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom sklearn.linear_model import LinearRegression\n\nx = df.drop([\"math score\",\"overall_score\"], axis = 1)\ny = df[\"math score\"]\n\nscore_lr = cross_val_score(LinearRegression(), x, y, cv = RepeatedKFold(n_repeats = 10))\nscore_rf = cross_val_score(RandomForestRegressor(n_estimators = 100), x, y, cv = RepeatedKFold(n_repeats = 10))\nprint(\"Math Score:\\nLinear Regression:\",np.mean(score_lr),\"\\nRandom Forest:\",np.mean(score_rf))","053e4454":"x = df.drop([\"math score\", \"writing score\", \"reading score\", \"overall_score\"], axis = 1)\ny = df[\"overall_score\"]\n\nscore_lr = cross_val_score(LinearRegression(), x, y, cv = RepeatedKFold(n_repeats = 10))\nscore_rf = cross_val_score(RandomForestRegressor(n_estimators = 100), x, y, cv = RepeatedKFold(n_repeats = 10))\nprint(\"Overall_Score:\\nLinear Regression:\",np.mean(score_lr),\"\\nRandom Forest:\",np.mean(score_rf))","8da88555":"df[\"overall_median\"] = df[\"overall_score\"]\ndf.overall_median[(df[\"overall_score\"] >= median)] = 1\ndf.overall_median[(df[\"overall_score\"] < median)] = 0\ndf.head()","eef43bec":"x = df.drop([\"math score\", \"writing score\", \"reading score\", \"overall_score\", \"overall_median\"], axis = 1)\ny = df[\"overall_median\"]\n\nscore_lr = cross_val_score(LinearRegression(), x, y, cv = RepeatedKFold(n_repeats = 10))\nscore_rf = cross_val_score(RandomForestClassifier(n_estimators = 100), x, y, cv = RepeatedKFold(n_repeats = 10))\nprint(\"Good\/Bad Student:\\nLinear Regression:\",np.mean(score_lr),\"\\nRandom Forest:\",np.mean(score_rf))","74a72d1c":"x = df.drop([\"math score\", \"writing score\", \"reading score\", \"overall_score\", \"overall_median\", \"parental level of education\"], axis = 1)\ny = df[\"overall_median\"]\n\nscore_lr = cross_val_score(LinearRegression(), x, y, cv = RepeatedKFold(n_repeats = 10))\nscore_rf = cross_val_score(RandomForestClassifier(n_estimators = 100), x, y, cv = RepeatedKFold(n_repeats = 10))\nprint(\"Good\/Bad Student:\\nLinear Regression:\",np.mean(score_lr),\"\\nRandom Forest:\",np.mean(score_rf))","02ec2947":"sns.countplot(x = \"overall_median\", hue = \"race\/ethnicity\", data = df)","083f8e60":"df[\"ethnicity\"] = df[\"race\/ethnicity\"]\ndf.ethnicity[(df[\"race\/ethnicity\"] <= 2)] = 0\ndf.ethnicity[(df[\"race\/ethnicity\"] > 2)] = 1\ndf.head()","9613f9a8":"x = df.drop([\"math score\", \"writing score\", \"reading score\", \"overall_score\", \"overall_median\", \"parental level of education\", \"race\/ethnicity\"], axis = 1)\ny = df[\"overall_median\"]\n\nscore_lr = cross_val_score(LinearRegression(), x, y, cv = RepeatedKFold(n_repeats = 10))\nscore_rf = cross_val_score(RandomForestClassifier(n_estimators = 100), x, y, cv = RepeatedKFold(n_repeats = 10))\nprint(\"Good\/Bad Student:\\nLinear Regression:\",np.mean(score_lr),\"\\nRandom Forest:\",np.mean(score_rf))","864df8d3":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nx = df.drop([\"math score\", \"writing score\", \"reading score\", \"overall_score\", \"overall_median\", \"parental level of education\", \"race\/ethnicity\"], axis = 1)\ny = df[\"overall_median\"]\n\nfrom sklearn import preprocessing\n\nscaler = preprocessing.MinMaxScaler(feature_range=(-1, 1)).fit(x)\nx = scaler.transform(x)\nx = np.clip(x, a_min=-1, a_max=1)\n\nmodels = [LogisticRegression(), GradientBoostingClassifier(), RandomForestClassifier(),AdaBoostClassifier()]\n\nscores = []\nnames = []\n\nfor alg in models: \n    score = cross_val_score(alg, x, y, cv = RepeatedKFold(n_repeats = 3))\n    scores.append(np.mean(score))    \n    names.append(alg.__class__.__name__)\n    print(alg.__class__.__name__, \"trained\")\n    \nsns.set_color_codes(\"muted\")\nsns.barplot(x=scores, y=names, color=\"g\")\n\nplt.xlabel('Accuracy')\nplt.title('Classifier Scores')\nplt.show()","b4da9eb5":"lr_params = dict(     \n    C = [n for n in range(1, 10)],     \n    tol = [0.0001, 0.001, 0.001, 0.01, 0.1, 1],  \n)\n\nlr = LogisticRegression(solver='liblinear')\nlr_cv = GridSearchCV(estimator=lr, param_grid=lr_params, cv=5) \nlr_cv.fit(x, y)\nlr_est = lr_cv.best_estimator_\nprint(lr_cv.best_score_)","3d70491d":"gb_params = dict(     \n    max_depth = [n for n in range(9, 14)],     \n    min_samples_split = [n for n in range(2, 5)], \n    min_samples_leaf = [n for n in range(2, 5)],     \n    n_estimators = [n for n in range(10, 40, 5)],\n)\n\ngb = GradientBoostingClassifier()\ngb_cv = GridSearchCV(estimator=gb, param_grid=gb_params, cv=5) \ngb_cv.fit(x, y)\ngb_est = gb_cv.best_estimator_\nprint(gb_cv.best_score_)","c5eb861e":"forest_params = dict(     \n    max_depth = [n for n in range(9, 14)],     \n    min_samples_split = [n for n in range(4, 11)], \n    min_samples_leaf = [n for n in range(2, 5)],     \n    n_estimators = [n for n in range(10, 40, 5)],\n)\n\nforest = RandomForestClassifier()\nforest_cv = GridSearchCV(estimator=forest, param_grid=forest_params, cv=5) \nforest_cv.fit(x, y)\nforest_est = forest_cv.best_estimator_\nprint(forest_cv.best_score_)","5c0981d8":"ada_params = dict(          \n    learning_rate = [0.05, 0.1, 0.15, 0.2],  \n    n_estimators = [n for n in range(10, 40, 5)],\n)\n\nada = AdaBoostClassifier()\nada_cv = GridSearchCV(estimator=ada, param_grid=ada_params, cv=5) \nada_cv.fit(x, y)\nada_est = ada_cv.best_estimator_\nprint(ada_cv.best_score_)","4daab0c2":"cv_models = [lr_est, gb_est, forest_est, ada_est]\n\nscores_cv = []\nnames_cv = []\n\nfor alg in cv_models: \n    score = cross_val_score(alg, x, y, cv = RepeatedKFold(n_repeats = 3))\n    scores_cv.append(np.mean(score))    \n    names_cv.append(alg.__class__.__name__)\n    print(alg.__class__.__name__, \"trained\")\n    \nsns.set_color_codes(\"muted\")\nsns.barplot(x=scores_cv, y=names_cv, color=\"b\")\nsns.barplot(x=scores, y=names, color=\"g\")\n\nplt.xlabel('Accuracy')\nplt.title('Classifier Scores')\nplt.show()","bdc68796":"col = df.drop([\"math score\", \"writing score\", \"reading score\", \"overall_score\", \"overall_median\", \"parental level of education\", \"race\/ethnicity\"], axis = 1)\nf = plt.figure(figsize=(5,10))\n\nplt.subplot(3, 1, 1)\n(pd.Series(forest_est.feature_importances_, index=col.columns).nlargest(10).plot(kind='barh') )\n\nplt.subplot(3, 1, 2)\n(pd.Series(gb_est.feature_importances_, index=col.columns).nlargest(10).plot(kind='barh') )\n\nplt.subplot(3, 1, 3)\n(pd.Series(ada_est.feature_importances_, index=col.columns).nlargest(10).plot(kind='barh') )","85d8958f":"#ROC Curve\nfrom sklearn.metrics import roc_curve, precision_recall_curve\nfpr_model, tpr_model, thresholds_model = roc_curve(y, lr_cv.predict_proba(x)[:,1])\nplt.plot(fpr_model, tpr_model, label = \"LogisticRegression\")\n\nfpr_knn, tpr_knn, thresholds_knn = roc_curve(y, gb_cv.predict_proba(x)[:,1])\nplt.plot(fpr_knn, tpr_knn, label = \"GB\")\n\nfpr_knn, tpr_knn, thresholds_knn = roc_curve(y, forest_cv.predict_proba(x)[:,1])\nplt.plot(fpr_knn, tpr_knn, label = \"RFC\")\n\nfpr_rfc, tpr_rfc, thresholds_rfc = roc_curve(y, ada_cv.predict_proba(x)[:,1])\nplt.plot(fpr_rfc, tpr_rfc, label = \"ADA\")\nplt.xlabel(\"P(FP)\")\nplt.ylabel(\"P(TP)\")\nplt.legend(loc = \"best\")","ab7a185a":"#Recall Curve\nprecision_model, recall_model, thresholds_model = precision_recall_curve(y, lr_cv.predict_proba(x)[:,1])\nplt.plot(precision_model, recall_model, label = \"LogisticRegression\")\n\nprecision_gb, recall_gb, thresholds_gb = precision_recall_curve(y, gb_cv.predict_proba(x)[:,1])\nplt.plot(precision_gb, recall_gb, label = \"GB\")\n\nprecision_rfc, recall_rfc, thresholds_rfc = precision_recall_curve(y, forest_cv.predict_proba(x)[:,1])\nplt.plot(precision_rfc, recall_rfc, label = \"RFC\")\n\nprecision_ada, recall_ada, thresholds_ada = precision_recall_curve(y, ada_cv.predict_proba(x)[:,1])\nplt.plot(precision_ada, recall_ada, label = \"Ada\")","3a74d54d":"#Learning Curve\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.utils import shuffle\nx, y = shuffle(x, y)\n\ntrain_sizes_abs, train_scores, test_scores = learning_curve(RandomForestClassifier(), x, y)\nplt.plot(train_sizes_abs, np.mean(train_scores, axis = 1))\nplt.plot(train_sizes_abs, np.mean(test_scores, axis = 1))","a5d11947":"# Students Performance in Exams: Quick EDA & Easy Models\n\nIn this kernel I would like to demonstrate some of the techniques for EDA and consolidate my knowledge in this area as an MBA. \n\nThe starting point is a data set with a sample size of 1000 children, whose results have been recorded in some tests. In addition, their ethnic group, gender, education of parents and participation in the preparation course were recorded. An interesting variable is \"Lunch\", which indicates whether the child participates in the reduced lunch program. This is, if I have understood it correctly, a program that supports socially disadvantaged families by providing lunch for the children at school.","25d8a2f2":"### Correlation\nNow let's take a look at the correlation. As already suspected, the grades of the individual subjects correlate strongly with each other, which is why we can assume that good students perform well in all subjects. Unfortunately, the other variables hardly correlate with each other, making it difficult to predict what will affect the notes later on.\n\n### Let's Combine Some Attributes\nIf we now combine different attributes, we see that slightly more boys get lunch allowance as girls. Gender also has no great influence on participation in the preparation course. Whereby we could check this with a t-test.\n","31a5aed5":"## Prediction\n\nNow let's try to find a simple model that helps us predict how the student will perform. \nAs you can see, if you keep the other scores in the model, you can predict the math score. This is logical because we have a high correlation between these variables. If we take out the scores, however, the accuracy of the model deteriorates.\n\n### Dummy Variable\nNow we simplify the problem and let only the model predict whether the student is good or bad. To do this, we create a dummy variable that matches the division using the median. I like to use the median because it is more robust than the arithmetic mean.As you can see now, the Random Forest model is a bit better now. I only left the Linear Regressor for comparison, but it rarely works with binary variables.\nIf we take a quick look at the median again, we see that the student's performance could also depend on the ethnicity, which has a variety of reasons.\nWithout going into these reasons, since I am not a sociologist, I try to form groups to improve the model. As we can see, however, it improves only slightly\n","cc5657ed":"First of all we do the boring stuff like importing plugins, reading dataframes, checking for missing values,, and so on. You know the game.","7105f058":"## Compare Different Models\n\n### Parameter Tuning\nNow I played around with the models and tried to improve them by parameter tuning. However, you can clearly see that with an accuracy of 66% it's over. Nevertheless, you can see that some estimators could improve, but didn't cross the threshold. \n\n### Feature Importance\nIf we look at the feature importance we can clearly see that lunch and the income of the parents expressed through it as well as the participation in the preparation course have an important influence on the predictability of the grade.\n\n### Model Fit\nThe ROC curve rises a bit before the error rate rises and looks relatively good. We can therefore assume that the models perform relatively well.\nOn the other hand, there are clear differences between the models in the recall curve. If we were to deal with a real model, we could now prevent false-positives and select the best model.\nFinally, we consider the learning curve for the RFC. This clearly shows that with increasing sample size the model accuracy decreases again, which probably causes an overfitting in our case. ","2d18e69a":"### t-test\nIf we do an independent t-test, we see that the children who do not receive a lunch allowance achieve better results. This could be explained, for example, by the fact that children from socially disadvantaged families tend to do not receive as much help with homework or learning at home, for example by private tutoring.\n\nOn the other hand, it can be seen that children who attend the course do better than children who do not.\n\nIf we now compare the children who do not receive a subsidy and do not take part in the course because they receive help at home with the preparation for the exam, for example, with the performance of the children who come from socially disadvantaged families and receive help in school, we see that the results are the same. Thus, at first glance, the offers of help seem to work.","c444d67d":"## EDA\nIn the next step we would like to have a look at the descriptive statistics and do a quick EDA.\n\n### Encoding\nIn order to be able to use the nominally and ordinally scaled data, we will first encode them. Since I would like to plot them in a bar chart, I will not encode them as dummy variables. \nFurthermore, I have another variable calculated, which indicates the average score of all three subjects. I do this on the assumption that students who are good in one subject may also be successful in other subjects. \n\n### Characteristics Distribution\nAs you can see in the count plots, the classes are imbalanced. We have some more boys attending school. Also, different ethnic groups are represented differently, which can be due to different reasons. In addition, the parents have very different levels of education. It is clear that most children receive a lunch allowance and most also attend the preparation course.","11337c10":"## Distribution\n\nNow let's draw the pair plot and have a look at the distribution of the data. We immediately recognize that we have a skewness in the data and in the scatterplot we also see some outliers. Let's take a closer look at this\n\n### Skewness & Kurtosis\nFirst we look at the skewness. On the plot we can already see that we have to do with left-skewed data. A look at the values confirms this. If we look at the kurtosis we also see that it deviates strongly from the normal distribution, which is also confirmed by the values. However, it should be noted that real data is never completely normally distributed and the values are within a normal tolerance range. However, we can try to normalize them a bit. Especially since we have some outliers, the distance of the outliers can bring us closer to a normal distribution.\n\n### Outliers\nIn addition to the scatterplot, we also look at a boxplot for the three variables in order to be able to recognize outliers. The box plot also confirms our assumption. We remove the outliers using the interquantile distance between the 1st and 3rd quantiles. This is a frequently used method.\n\n### Results\nAs you can see, there are no more outliers on the box plots. Also, the values for skewness and kurtosis have improved. On the pairplot the distribution doesn't look as left-skewed anymore."}}