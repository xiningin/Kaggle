{"cell_type":{"283fa83e":"code","485eead5":"code","ff294a4a":"code","e6131558":"code","3ecd55d1":"code","b27837ef":"code","1f69cd06":"code","8ba2731b":"code","cedc2f37":"code","15cd0dd4":"code","110bc0c5":"code","91c05a64":"code","936b7e89":"code","4b9bd95b":"code","c7b2bb1e":"code","f4d015ed":"code","194d5251":"code","7b882cfc":"code","bab3e076":"code","d4a9a6aa":"code","e2e516a4":"code","02c4de27":"code","1fc96f93":"code","ba36c6bc":"code","0dba5344":"code","a92c0fe1":"code","3988a628":"code","e8918d7d":"code","f32d86d2":"code","7735cfe8":"code","a0e0eb6e":"code","a756533b":"code","3c2f21e9":"code","4bf6eb76":"code","80a941cf":"code","5da5b48a":"code","b4e861a9":"markdown"},"source":{"283fa83e":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport librosa\nimport matplotlib.pyplot as plt\nimport gc\n\nfrom tqdm import tqdm, tqdm_notebook\nfrom sklearn.metrics import label_ranking_average_precision_score\nfrom sklearn.model_selection import train_test_split\n\ntqdm.pandas()","485eead5":"def calculate_overall_lwlrap_sklearn(truth, scores):\n    \"\"\"Calculate the overall lwlrap using sklearn.metrics.lrap.\"\"\"\n    # sklearn doesn't correctly apply weighting to samples with no labels, so just skip them.\n    sample_weight = np.sum(truth > 0, axis=1)\n    nonzero_weight_sample_indices = np.flatnonzero(sample_weight > 0)\n    overall_lwlrap = label_ranking_average_precision_score(\n        truth[nonzero_weight_sample_indices, :] > 0, \n        scores[nonzero_weight_sample_indices, :], \n        sample_weight=sample_weight[nonzero_weight_sample_indices])\n    return overall_lwlrap","ff294a4a":"def split_and_label(rows_labels):\n    \n    row_labels_list = []\n    for row in rows_labels:\n        row_labels = row.split(',')\n        labels_array = np.zeros((80))\n        \n        for label in row_labels:\n            index = label_mapping[label]\n            labels_array[index] = 1\n        \n        row_labels_list.append(labels_array)\n    \n    return row_labels_list","e6131558":"train_curated = pd.read_csv('..\/input\/train_curated.csv')\ntrain_noisy = pd.read_csv('..\/input\/train_noisy.csv')\ntest = pd.read_csv('..\/input\/sample_submission.csv')","3ecd55d1":"print(train_curated.shape, train_noisy.shape, test.shape)","b27837ef":"label_columns = test.columns[1:]","1f69cd06":"label_mapping = dict((label, index) for index, label in enumerate(label_columns))","8ba2731b":"label_mapping","cedc2f37":"for col in tqdm(label_columns):\n    train_curated[col] = 0\n    train_noisy[col] = 0\n    \nprint(train_curated.shape, train_noisy.shape)","15cd0dd4":"train_curated_labels = split_and_label(train_curated['labels'])\ntrain_noisy_labels = split_and_label(train_noisy['labels'])","110bc0c5":"train_curated[label_columns] = train_curated_labels\ntrain_noisy[label_columns] = train_noisy_labels","91c05a64":"train_curated['num_labels'] = train_curated[label_columns].sum(axis=1)\ntrain_noisy['num_labels'] = train_noisy[label_columns].sum(axis=1)","936b7e89":"plt.figure(figsize=(18,6))\n\nplt.subplot(121)\nax1 = train_curated['num_labels'].value_counts().plot(kind='bar')\nplt.xlabel('Number of labels')\nplt.ylabel('Counts')\nplt.xticks(rotation=0)\nplt.title('Curated Training Set')\n\nfor p in ax1.patches:\n    ax1.annotate(str(p.get_height()), \n                (p.get_x() + p.get_width()\/2., p.get_height() * 1.005), \n                ha='center',\n                va='center',\n                xytext=(0,5), \n                textcoords='offset points')\n\nplt.subplot(122)\nax2 = train_noisy['num_labels'].value_counts().sort_index().plot(kind='bar', )\nplt.xlabel('Number of labels')\nplt.ylabel('Counts')\nplt.xticks(rotation=0)\nplt.title('Noisy Training Set')\n\nfor p in ax2.patches:\n    ax2.annotate(str(p.get_height()), \n                (p.get_x() + p.get_width()\/2., p.get_height() * 1.005), \n                ha='center',\n                va='center',\n                xytext=(0,5), \n                textcoords='offset points')\n\n    \nplt.show()","4b9bd95b":"# Special thanks to https:\/\/github.com\/makinacorpus\/easydict\/blob\/master\/easydict\/__init__.py\n\nclass EasyDict(dict):\n\n    def __init__(self, d=None, **kwargs):\n        if d is None:\n            d = {}\n        if kwargs:\n            d.update(**kwargs)\n        for k, v in d.items():\n            setattr(self, k, v)\n        # Class attributes\n        for k in self.__class__.__dict__.keys():\n            if not (k.startswith('__') and k.endswith('__')) and not k in ('update', 'pop'):\n                setattr(self, k, getattr(self, k))\n\n    def __setattr__(self, name, value):\n        if isinstance(value, (list, tuple)):\n            value = [self.__class__(x)\n                     if isinstance(x, dict) else x for x in value]\n        elif isinstance(value, dict) and not isinstance(value, self.__class__):\n            value = self.__class__(value)\n        super(EasyDict, self).__setattr__(name, value)\n        super(EasyDict, self).__setitem__(name, value)\n\n    __setitem__ = __setattr__\n\n    def update(self, e=None, **f):\n        d = e or dict()\n        d.update(f)\n        for k in d:\n            setattr(self, k, d[k])\n\n    def pop(self, k, d=None):\n        delattr(self, k)\n        return super(EasyDict, self).pop(k, d)","c7b2bb1e":"conf = EasyDict()\nconf.sampling_rate = 44100\nconf.duration = 5\nconf.hop_length = 347 # to make time steps 128\nconf.fmin = 20\nconf.fmax = conf.sampling_rate \/\/ 2\nconf.n_mels = 128\nconf.n_fft = conf.n_mels * 20\n\nconf.samples = conf.sampling_rate * conf.duration\n\ntrain_curated_path = '..\/input\/train_curated\/'\ntrain_noisy_path = '..\/input\/train_noisy\/'\ntest_path = '..\/input\/test\/'","f4d015ed":"def read_audio(conf, pathname, trim_long_data):\n    y, sr = librosa.load(pathname, sr=conf.sampling_rate)\n    # trim silence\n    if 0 < len(y): # workaround: 0 length causes error\n        y, _ = librosa.effects.trim(y) # trim, top_db=default(60)\n    # make it unified length to conf.samples\n    if len(y) > conf.samples: # long enough\n        if trim_long_data:\n            y = y[0:0+conf.samples]\n    else: # pad blank\n        padding = conf.samples - len(y)    # add padding at both ends\n        offset = padding \/\/ 2\n        y = np.pad(y, (offset, conf.samples - len(y) - offset), 'constant')\n    return y\n\ndef audio_to_melspectrogram(conf, audio):\n    spectrogram = librosa.feature.melspectrogram(audio, \n                                                 sr=conf.sampling_rate,\n                                                 n_mels=conf.n_mels,\n                                                 hop_length=conf.hop_length,\n                                                 n_fft=conf.n_fft,\n                                                 fmin=conf.fmin,\n                                                 fmax=conf.fmax)\n    spectrogram = librosa.power_to_db(spectrogram)\n    spectrogram = spectrogram.astype(np.float32)\n    return spectrogram\n\ndef read_as_melspectrogram(conf, pathname, trim_long_data, debug_display=False):\n    x = read_audio(conf, pathname, trim_long_data)\n    mels = audio_to_melspectrogram(conf, x)\n    if debug_display:\n        IPython.display.display(IPython.display.Audio(x, rate=conf.sampling_rate))\n        show_melspectrogram(conf, mels)\n    return mels\n\ndef convert_wav_to_image(df, source):\n    X = []\n    for i, row in tqdm_notebook(df.iterrows()):\n        try:\n            x = read_as_melspectrogram(conf, f'{source[0]}\/{str(row.fname)}', trim_long_data=True)\n        except:\n            x = read_as_melspectrogram(conf, f'{source[1]}\/{str(row.fname)}', trim_long_data=True)\n\n        #x_color = mono_to_color(x)\n        X.append(x.transpose())\n        #df.loc[i, 'length'] = x.shape[1]\n    return X","194d5251":"#For baseline, noisy set is not used.\n#train = pd.concat([train_curated, train_noisy],axis=0)","7b882cfc":"#del train_curated, train_noisy\n\n#gc.collect()","bab3e076":"%%time\n\n#X = np.array(convert_wav_to_image(train, source=[train_curated_path, train_noisy_path]))\nX = np.array(convert_wav_to_image(train_curated, source=[train_curated_path]))","d4a9a6aa":"Y = train_curated[label_columns].values","e2e516a4":"#del train\n#gc.collect()","02c4de27":"from keras import backend as K\nfrom keras.engine.topology import Layer\nfrom keras import initializers, regularizers, constraints, optimizers, layers\nfrom keras.layers import Embedding, Input, Dense, CuDNNGRU,concatenate, Bidirectional, SpatialDropout1D, Conv1D, GlobalAveragePooling1D, GlobalMaxPooling1D, Dropout\nfrom keras.models import Model\nfrom keras.optimizers import Adam\nfrom keras.callbacks import EarlyStopping","1fc96f93":"class Attention(Layer):\n    def __init__(self, step_dim,\n                 W_regularizer=None, b_regularizer=None,\n                 W_constraint=None, b_constraint=None,\n                 bias=True, **kwargs):\n        self.supports_masking = True\n        self.init = initializers.get('glorot_uniform')\n\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n\n        self.W_constraint = constraints.get(W_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n\n        self.bias = bias\n        self.step_dim = step_dim\n        self.features_dim = 0\n        super(Attention, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert len(input_shape) == 3\n\n        self.W = self.add_weight((input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_W'.format(self.name),\n                                 regularizer=self.W_regularizer,\n                                 constraint=self.W_constraint)\n        self.features_dim = input_shape[-1]\n\n        if self.bias:\n            self.b = self.add_weight((input_shape[1],),\n                                     initializer='zero',\n                                     name='{}_b'.format(self.name),\n                                     regularizer=self.b_regularizer,\n                                     constraint=self.b_constraint)\n        else:\n            self.b = None\n\n        self.built = True\n\n    def compute_mask(self, input, input_mask=None):\n        return None\n\n    def call(self, x, mask=None):\n        features_dim = self.features_dim\n        step_dim = self.step_dim\n\n        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n\n        if self.bias:\n            eij += self.b\n\n        eij = K.tanh(eij)\n\n        a = K.exp(eij)\n\n        if mask is not None:\n            a *= K.cast(mask, K.floatx())\n\n        a \/= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n\n        a = K.expand_dims(a)\n        weighted_input = x * a\n        return K.sum(weighted_input, axis=1)\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0],  self.features_dim","ba36c6bc":"sequence_input = Input(shape=(636,128), dtype='float32')\nx = CuDNNGRU(128, return_sequences=True)(sequence_input) \natt = Attention(636)(x)\navg_pool = GlobalAveragePooling1D()(x)\nmax_pool = GlobalMaxPooling1D()(x) \n\nx = concatenate([att, avg_pool, max_pool])\n\npreds = Dense(80, activation='softmax')(x)\n\nmodel = Model(sequence_input, preds)\nmodel.summary()","0dba5344":"model.compile(loss='categorical_crossentropy',optimizer=Adam(0.005),metrics=['acc'])","a92c0fe1":"x_train, x_val, y_train, y_val = train_test_split(X, Y, test_size=0.2, random_state=123)","3988a628":"del X, Y\ngc.collect()","e8918d7d":"assert len(x_train) == len(y_train)\nassert len(x_val) == len(y_val)","f32d86d2":"es = EarlyStopping(monitor='val_acc', mode='max', verbose=1, patience=3)","7735cfe8":"model.fit(np.array(x_train),\n          y_train,\n          batch_size=1024,\n          epochs=100,\n          validation_data=(np.array(x_val), y_val),\n          callbacks = [es])","a0e0eb6e":"y_train_pred = model.predict(np.array(x_train))\ny_val_pred = model.predict(np.array(x_val))","a756533b":"train_lwlrap = calculate_overall_lwlrap_sklearn(y_train, y_train_pred)\nval_lwlrap = calculate_overall_lwlrap_sklearn(y_val, y_val_pred)\n\nprint(f'Training LWLRAP : {train_lwlrap:.4f}')\nprint(f'Validation LWLRAP : {val_lwlrap:.4f}')","3c2f21e9":"%%time\nX_test = np.array(convert_wav_to_image(test, source=[test_path]))","4bf6eb76":"predictions = model.predict(np.array(X_test))","80a941cf":"test[label_columns] = predictions","5da5b48a":"test.to_csv('submission.csv', index=False)","b4e861a9":"### References:\n* https:\/\/www.kaggle.com\/maxwell110\/beginner-s-guide-to-audio-data-2\n* https:\/\/www.kaggle.com\/daisukelab\/cnn-2d-basic-solution-powered-by-fast-ai\n* https:\/\/www.kaggle.com\/christofhenkel\/keras-baseline-lstm-attention-5-fold\n* https:\/\/yerevann.github.io\/2016\/06\/26\/combining-cnn-and-rnn-for-spoken-language-identification\/\n\n### In this kernel, only train curated will be used.\n\nI'm taking 5 seconds of spectrograms for each video -> likely an overkill, to be fine-tuned later.\n\nTo use the noisy set for training, a data generator is required, as the complete spectograms won't fit into the memory.\n"}}