{"cell_type":{"b53c9498":"code","2215b3b1":"code","eca16e91":"code","968bdb8c":"code","c0f7b795":"code","d9bce0c2":"code","ab27a505":"code","d25f45ff":"code","96f99bb1":"code","7d5baba7":"code","5eb00ac4":"code","217f0cd3":"code","2a514bbf":"code","400194b5":"code","2258c59e":"code","62623261":"code","d9a404e7":"code","432ae205":"code","c4434d09":"code","410a2cd3":"code","2b102969":"code","8e04cb5c":"code","40e7cd97":"code","928e104c":"code","1380939d":"code","ff1df311":"code","773e75be":"code","8a7e6a9d":"code","c7d01cb9":"code","dd8591e2":"code","891d0637":"code","d96be3ff":"code","a05a4a98":"code","e41c7dd1":"code","1377eff2":"code","eb196d12":"code","3f514762":"code","9ff4ebb0":"code","3f85f1b9":"code","67b6148c":"code","be6d28be":"code","0dad55a5":"code","efee230c":"code","2795e04a":"code","1b5e8d71":"code","243e4483":"code","508f51cc":"code","06f5c3bf":"code","a9148b38":"code","74db2a66":"code","03f2621f":"code","cb543187":"code","4f264d37":"code","a7988946":"code","2331a960":"code","4f5e18d4":"code","f886463a":"code","a5448018":"code","29004ec9":"code","75e83f11":"markdown","2294b91f":"markdown","08452134":"markdown","8e6704bf":"markdown","2670901a":"markdown","52c5a136":"markdown","7d2d9b47":"markdown","8710c79f":"markdown","eab4b614":"markdown","cb349aa1":"markdown","47906b6d":"markdown","6099e41f":"markdown","1286c618":"markdown","c1150442":"markdown"},"source":{"b53c9498":"import numpy as np \nimport pandas as pd \nimport string\nimport re\nimport nltk\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\n","2215b3b1":"df_train = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ndf_train","eca16e91":"df_test = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\ndf_test","968bdb8c":"sample_submission = pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')\nsample_submission","c0f7b795":"df_train.shape   # There are 7613 rows and 5 columns","d9bce0c2":"df_train.text.isnull().values.any()   # There is no null value in text column","ab27a505":"df_train.location.value_counts()","d25f45ff":"df_train.keyword.value_counts()","96f99bb1":"df_train.text.describe()","7d5baba7":"df_train.keyword.describe()","5eb00ac4":"df_train.location.describe()","217f0cd3":"df_train.drop(['location' , 'keyword'] , axis = 1 , inplace = True)","2a514bbf":"df_train","400194b5":"df_train.text[:3]","2258c59e":"df_train.columns","62623261":"df_train['text']=df_train['text'].str.replace('.' , '')\ndf_train['text']=df_train['text'].str.replace(',' , '')\ndf_train['text']=df_train['text'].str.replace('&' , '')\ndf_train['text']=df_train['text'].str.lower()","d9a404e7":"string.punctuation","432ae205":"def remove_punctuation(text):\n    without_punct=\"\".join([i for i in text if i not in string.punctuation])\n    return without_punct","c4434d09":"df_train['text']= df_train['text'].apply(lambda x:remove_punctuation(x))","410a2cd3":"def tokenize(string):\n    '''\n    Tokenizes the string to a list of words\n    '''\n    word_tokens = string.split()\n    return word_tokens","2b102969":"df_train['text']= df_train['text'].apply(lambda x: tokenize(x))","8e04cb5c":"df_train.head()","40e7cd97":"stopwords = nltk.corpus.stopwords.words('english')","928e104c":"def remove_stopwords(text):\n    output= [i for i in text if i not in stopwords]\n    return output","1380939d":"df_train['text']= df_train['text'].apply(lambda x:remove_stopwords(x))","ff1df311":"porter_stemmer = PorterStemmer()","773e75be":"def stemming(text):\n    stem_text = [porter_stemmer.stem(word) for word in text]\n    return stem_text\ndf_train['text']=df_train['text'].apply(lambda x: stemming(x))","8a7e6a9d":"wordnet_lemmatizer = WordNetLemmatizer()","c7d01cb9":"#defining the function for lemmatization\ndef lemmatizer(text):\n    lemm_text = [wordnet_lemmatizer.lemmatize(word) for word in text]\n    return lemm_text","dd8591e2":"nltk.download('wordnet')","891d0637":"df_train['text']=df_train['text'].apply(lambda x: lemmatizer(x))","d96be3ff":"## Converting the tokens back to strings to feed it into Count Vectorizer\n\ndf_train['text_strings'] = df_train['text'].apply(lambda x: ' '.join([str(word) for word in x]))","a05a4a98":"# df_train","e41c7dd1":"df_train['text_strings'].head()\n","1377eff2":"vectorizer = CountVectorizer()\nX = vectorizer.fit_transform(df_train['text_strings'])\n","eb196d12":"x_train = X.toarray()","3f514762":"x_train = np.array(x_train)","9ff4ebb0":"x_train.shape","3f85f1b9":"y_train = df_train['target']","67b6148c":"y_train.shape","be6d28be":"clf = LogisticRegression(random_state=42)\nclf.fit(x_train,y_train)","0dad55a5":"pred = clf.predict(x_train)","efee230c":"accuracy_score(y_train, pred)","2795e04a":"classification_report(y_train, pred)\n","1b5e8d71":"clf2 = RandomForestClassifier(n_estimators=100, random_state=42, max_depth = 700)  #500\nclf2.fit(x_train,y_train)","243e4483":"pred2 = clf2.predict(x_train)","508f51cc":"accuracy_score(y_train, pred2)","06f5c3bf":"df_test.fillna('',inplace=True)\ndf_test.drop(columns=['id','keyword','location'],inplace=True)","a9148b38":"df_test['text']= df_test['text'].apply(lambda x:remove_punctuation(x))\ndf_test['text']= df_test['text'].apply(lambda x: tokenize(x))\ndf_test['text']= df_test['text'].apply(lambda x:remove_stopwords(x))\ndf_test['text']= df_test['text'].apply(lambda x: stemming(x))","74db2a66":"df_test['text_strings'] = df_test['text'].apply(lambda x: ' '.join([str(elem) for elem in x]))","03f2621f":"x_test = vectorizer.transform(df_test['text_strings'])\nx_test = x_test.toarray()\n","cb543187":"x_test = np.array(x_test)","4f264d37":"y_test_pred = clf2.predict(x_test)","a7988946":"y_test_pred","2331a960":"submission = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')","4f5e18d4":"submission['target'] = y_test_pred","f886463a":"submission.head()","a5448018":"final_submission = submission[['id','target']]","29004ec9":"final_submission.to_csv('final_submission.csv', index = False)","75e83f11":"### Dropping columns :","2294b91f":"### Text vectorization :","08452134":"### Cleaning the data :","8e6704bf":"### Removing punctuations :","2670901a":"### Applying on test data :","52c5a136":"### Saving output to csv :","7d2d9b47":"### Lematization :","8710c79f":"### Removing stop words :","eab4b614":"## Text vectorization :","cb349aa1":"### EDA :","47906b6d":"### Stemming :","6099e41f":"### Tokenization :","1286c618":"### Fitting model: (Logistic regression)","c1150442":"### Fitting model : (Random Forest)"}}