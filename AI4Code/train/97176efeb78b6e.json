{"cell_type":{"7ef1ee2f":"code","3a13cf89":"code","9295b79e":"code","af9096f2":"code","890c31f7":"code","43443345":"code","0c35a953":"code","9bc9c9e8":"code","b6593f24":"code","4ce65997":"code","2bbd4e43":"code","c0ef7d68":"code","ef0b1117":"code","3a50283f":"code","ccbff456":"code","c727fd6f":"code","0bb68e1b":"code","44494bff":"code","285ff278":"markdown","1744e71d":"markdown","956da48d":"markdown","9f09cdf5":"markdown","25fb2030":"markdown","a9cbf9d7":"markdown","6c42391c":"markdown","8e500fbe":"markdown","1af0e611":"markdown","b6ac29af":"markdown","1b8d6533":"markdown","12ca00ad":"markdown","35aa4947":"markdown","926b71be":"markdown","b2fe41c5":"markdown","9486cf52":"markdown","b2bfc39c":"markdown"},"source":{"7ef1ee2f":"# Base\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Hypothesis Testing, Measures of Shape\nfrom scipy.stats import kruskal, skew, kurtosis\n\n# K-Means\nfrom yellowbrick.cluster import KElbowVisualizer\nfrom sklearn.cluster import KMeans\n\n# Clustering\nfrom sklearn.cluster import AgglomerativeClustering\n\n# Principle Component Analysis\nfrom sklearn.decomposition import PCA\n\n# Pre-Processing\nfrom sklearn.preprocessing import StandardScaler\n\n# Model\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import StratifiedKFold, KFold, RandomizedSearchCV\nfrom sklearn.metrics import log_loss\n# Shap\nimport shap\n\n# Configuration\nimport warnings\nwarnings.filterwarnings('ignore')\npd.set_option('display.max_columns', None)\npd.options.display.float_format = '{:,.2f}'.format","3a13cf89":"# Grab Column Names\ndef grab_col_names(dataframe, cat_th=10, car_th=20, show_date=False):\n    date_cols = [col for col in dataframe.columns if dataframe[col].dtypes == \"datetime64[ns]\"]\n\n    #cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == \"O\"]\n    cat_cols = dataframe.select_dtypes([\"object\", \"category\"]).columns.tolist()\n    \n    \n    \n    num_but_cat = [col for col in dataframe.select_dtypes([\"float\", \"integer\"]).columns if dataframe[col].nunique() < cat_th]\n\n    cat_but_car = [col for col in dataframe.select_dtypes([\"object\", \"category\"]).columns if dataframe[col].nunique() > car_th]\n\n    cat_cols = cat_cols + num_but_cat\n    cat_cols = [col for col in cat_cols if col not in cat_but_car]\n\n    num_cols = dataframe.select_dtypes([\"float\", \"integer\"]).columns\n    num_cols = [col for col in num_cols if col not in num_but_cat]\n\n    print(f\"Observations: {dataframe.shape[0]}\")\n    print(f\"Variables: {dataframe.shape[1]}\")\n    print(f'date_cols: {len(date_cols)}')\n    print(f'cat_cols: {len(cat_cols)}')\n    print(f'num_cols: {len(num_cols)}')\n    print(f'cat_but_car: {len(cat_but_car)}')\n    print(f'num_but_cat: {len(num_but_cat)}')\n\n    # cat_cols + num_cols + cat_but_car = de\u011fi\u015fken say\u0131s\u0131.\n    # num_but_cat cat_cols'un i\u00e7erisinde zaten.\n    # dolay\u0131s\u0131yla t\u00fcm \u015fu 3 liste ile t\u00fcm de\u011fi\u015fkenler se\u00e7ilmi\u015f olacakt\u0131r: cat_cols + num_cols + cat_but_car\n    # num_but_cat sadece raporlama i\u00e7in verilmi\u015ftir.\n\n    if show_date == True:\n        return date_cols, cat_cols, cat_but_car, num_cols, num_but_cat\n    else:\n        return cat_cols, cat_but_car, num_cols, num_but_cat\n    \n    \n# Missing Value\ndef missing_values(data, plot=False):\n    mst = pd.DataFrame(\n        {\"Num_Missing\": data.isnull().sum(), \"Missing_Ratio\": data.isnull().sum() \/ data.shape[0]}).sort_values(\n        \"Num_Missing\", ascending=False)\n    mst[\"DataTypes\"] = data[mst.index].dtypes.values\n    mst = mst[mst.Num_Missing > 0].reset_index().rename({\"index\": \"Feature\"}, axis=1)\n\n    print(\"Number of Variables include Missing Values:\", mst.shape[0], \"\\n\")\n\n    if mst[mst.Missing_Ratio >= 1.0].shape[0] > 0:\n        print(\"Full Missing Variables:\", mst[mst.Missing_Ratio >= 1.0].Feature.tolist())\n        data.drop(mst[mst.Missing_Ratio >= 1.0].Feature.tolist(), axis=1, inplace=True)\n\n        print(\"Full missing variables are deleted!\", \"\\n\")\n\n    if plot:\n        plt.figure(figsize=(25, 8))\n        p = sns.barplot(mst.Feature, mst.Missing_Ratio)\n        for rotate in p.get_xticklabels():\n            rotate.set_rotation(90)\n        plt.show()\n\n    print(mst, \"\\n\")\n    \n    \n# Categorical Variables & Target\ndef cat_analyzer(dataframe, variable, target = None):\n    print(variable)\n    if target == None:\n        print(pd.DataFrame({\n            \"COUNT\": dataframe[variable].value_counts(),\n            \"RATIO\": dataframe[variable].value_counts() \/ len(dataframe)}), end=\"\\n\\n\\n\")\n    else:\n        temp = dataframe[dataframe[target].isnull() == False]\n        print(pd.DataFrame({\n            \"COUNT\":dataframe[variable].value_counts(),\n            \"RATIO\":dataframe[variable].value_counts() \/ len(dataframe),\n            \"TARGET_COUNT\":dataframe.groupby(variable)[target].count(),\n            \"TARGET_MEAN\":temp.groupby(variable)[target].mean(),\n            \"TARGET_MEDIAN\":temp.groupby(variable)[target].median(),\n            \"TARGET_STD\":temp.groupby(variable)[target].std()}), end=\"\\n\\n\\n\")\n        \n        \n# Numerical Variables\ndef corr_plot(data, remove=[\"Id\"], corr_coef = \"pearson\", figsize=(20, 20)):\n    if len(remove) > 0:\n        num_cols2 = [x for x in data.columns if (x not in remove)]\n\n    sns.set(font_scale=1.1)\n    c = data[num_cols2].corr(method = corr_coef)\n    mask = np.triu(c.corr(method = corr_coef))\n    plt.figure(figsize=figsize)\n    sns.heatmap(c,\n                annot=True,\n                fmt='.1f',\n                cmap='coolwarm',\n                square=True,\n                mask=mask,\n                linewidths=1,\n                cbar=False)\n    plt.show()\n\n# Plot numerical variables\ndef num_plot(data, num_cols, remove=[\"Id\"], hist_bins=10, figsize=(20, 4)):\n\n    if len(remove) > 0:\n        num_cols2 = [x for x in num_cols if (x not in remove)]\n\n    for i in num_cols2:\n        fig, axes = plt.subplots(1, 3, figsize=figsize)\n        data.hist(str(i), bins=hist_bins, ax=axes[0])\n        data.boxplot(str(i), ax=axes[1], vert=False);\n        try:\n            sns.kdeplot(np.array(data[str(i)]))\n        except:\n            ValueError\n\n        axes[1].set_yticklabels([])\n        axes[1].set_yticks([])\n        axes[0].set_title(i + \" | Histogram\")\n        axes[1].set_title(i + \" | Boxplot\")\n        axes[2].set_title(i + \" | Density\")\n        plt.show()\n\n# Get high correlated variables\ndef high_correlation(data, remove=['SK_ID_CURR', 'SK_ID_BUREAU'], corr_coef=\"pearson\", corr_value = 0.7):\n    if len(remove) > 0:\n        cols = [x for x in data.columns if (x not in remove)]\n        c = data[cols].corr(method=corr_coef)\n    else:\n        c = data.corr(method=corr_coef)\n\n    for i in c.columns:\n        cr = c.loc[i].loc[(c.loc[i] >= corr_value) | (c.loc[i] <= -corr_value)].drop(i)\n        if len(cr) > 0:\n            print(i)\n            print(\"-------------------------------\")\n            print(cr.sort_values(ascending=False))\n            print(\"\\n\")\n            \n            \n# CART FEATURE GENERATOR\ndef cart_feature_gen(model_type, dataframe, X, y, threshold = 1, suffix = None):\n    # Remove NaN\n    temp = dataframe[[X,y]].dropna()\n    \n    # Model Type\n    if model_type == \"reg\":\n        from sklearn.tree import DecisionTreeRegressor\n        model = DecisionTreeRegressor()\n    elif model_type == \"class\":\n        temp[y] = temp[y].astype(int)\n        from sklearn.tree import DecisionTreeClassifier\n        model = DecisionTreeClassifier()\n    else:\n        print(\"Give a model type! model_type argument should be equal to 'reg' or 'class'\")\n        return None\n    \n    # Fit a tree\n    rules = model.fit(temp[[X]], temp[y])\n  \n    # First Decision Rule\n    print(X)\n    print(\"Threshold - Head(5):\", rules.tree_.threshold[[rules.tree_.threshold > temp[X].min()]][0:5])\n    print(\"Range:\", \"[\"+str(dataframe[X].min())+\" - \"+str(dataframe[X].max()) +\"]\", \"\\n\")\n    if suffix == None:\n        new_colname = \"DTREE_THRESH\"+str(threshold)+\"_\"+X.upper()\n    else:\n        new_colname = \"DTREE_THRESH\"+str(threshold)+\"_\"+X.upper()\n    dataframe[new_colname] = np.where(dataframe[X] <= rules.tree_.threshold[threshold - 1], 1, 0)   \n\n\n# Feature Importance\ndef plot_lgb_importances(model, plot=False, num=10):\n    # K-Meansdef plot_lgb_importances(model, plot=False, num=10):\n    from matplotlib import pyplot as plt\n    import seaborn as sns\n    \n    # LGBM API\n    #gain = model.feature_importance('gain')\n    #feat_imp = pd.DataFrame({'feature': model.feature_name(),\n    #                         'split': model.feature_importance('split'),\n    #                         'gain': 100 * gain \/ gain.sum()}).sort_values('gain', ascending=False)\n    \n    # SKLEARN API\n    gain = model.booster_.feature_importance(importance_type='gain')\n    feat_imp = pd.DataFrame({'feature': model.feature_name_,\n                             'split': model.booster_.feature_importance(importance_type='split'),\n                             'gain': 100 * gain \/ gain.sum()}).sort_values('gain', ascending=False)\n    if plot:\n        plt.figure(figsize=(10, 10))\n        sns.set(font_scale=1)\n        sns.barplot(x=\"gain\", y=\"feature\", data=feat_imp[0:25])\n        plt.title('feature')\n        plt.tight_layout()\n        plt.show()\n    else:\n        print(feat_imp.head(num))\n        return feat_imp","9295b79e":"train = pd.read_csv(\"..\/input\/tabular-playground-series-jun-2021\/train.csv\")\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-jun-2021\/test.csv\")\n\ndf = train.append(test)\n\ntrain.shape, test.shape, df.shape","af9096f2":"# Columns\ncat_cols, cat_but_car, num_cols, num_but_cat = grab_col_names(df, car_th=10)\ndel cat_cols, cat_but_car, num_cols, num_but_cat","890c31f7":"missing_values(df, plot = False)","43443345":"df.drop(\"id\",axis = 1).describe([0.01, 0.05, 0.25, 0.75,0.80, 0.90, 0.95, 0.99])","0c35a953":"# Quick Visualization for numerical variables\nnum_plot(df, num_cols=num_cols, remove=['id'], figsize = (15,3))","9bc9c9e8":"high_correlation(df, remove=['id', 'target'], corr_coef = \"spearman\", corr_value = 0.5)","b6593f24":"cat_analyzer(df, \"target\")","4ce65997":"# Apply the test all variables.\nkwallis = pd.DataFrame()\nfor i in df.drop([\"id\", \"target\"],axis = 1).columns:\n    pvalue = kruskal(*[group[i].values for name, group in train.groupby(\"target\")])[1]\n    if pvalue < 0.05:\n        result = \"H0 rejected\"\n        comment = \"One of the all groups is different at least.\"\n    else:\n        result = \"H0 not rejected\"\n        comment = \"All of the groups are similar.\"\n    kwallis = pd.concat([kwallis, pd.DataFrame({\"Feature\":[i], \"Result\":result, \"Comment\":comment})])\n# Results\nprint(kwallis.Comment.value_counts())\ndel kwallis","2bbd4e43":"for i in train.drop([\"id\", \"target\"],axis = 1).columns:\n    print('#',i.upper())\n    print('----------------------------------------')\n    print(train.groupby([\"target\"])[i].agg({\"mean\", \"median\", \"std\", \"max\"}).sort_values(\"mean\", ascending = False), \"\\n\\n\")","c0ef7d68":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\npcadf = StandardScaler().fit_transform(df.drop([\"id\",\"target\"],axis = 1))\npca = PCA()\npcadf = pca.fit_transform(pcadf)\npcadf = pd.DataFrame(\n    pcadf,\n    columns = list(map(lambda x: str(x).replace(\"feature_\", \"PC\"), df.columns[1:-1]))\n)\n\nprint(\"Explained Variance must be over than 1! \\n \",pca.explained_variance_[:10], \"\\n\")\nprint(\"Sum of Explained Variance Ratio:\", np.sum(pca.explained_variance_ratio_[:4]))","ef0b1117":"kmeansdf = StandardScaler().fit_transform(df.drop([\"id\",\"target\"],axis = 1))\nkmeans = KMeans()\nvisualizer = KElbowVisualizer(kmeans, k = (2,10))\nvisualizer.fit(kmeansdf)\nvisualizer.poof()","3a50283f":"df_prepared = df.copy()\n\n# Target\ndf_prepared[\"target\"] = df_prepared[\"target\"].apply(lambda x: str(x).replace(\"Class_\", \"\"))\ndf_prepared[\"target\"] = pd.to_numeric(df_prepared[\"target\"],errors='coerce').astype(pd.Int64Dtype())\n\n# Summary Stats\ndf_prepared[\"features_sum\"] = df.drop([\"id\", \"target\"], axis = 1).sum(axis = 1)\ndf_prepared[\"features_mean\"] = df.drop([\"id\", \"target\"], axis = 1).mean(axis = 1)\ndf_prepared[\"features_std\"] = df.drop([\"id\", \"target\"], axis = 1).std(axis = 1)\ndf_prepared[\"features_skew\"] = skew(df.drop([\"id\", \"target\"], axis = 1),axis = 1)\ndf_prepared[\"features_kurtosis\"] = kurtosis(df.drop([\"id\", \"target\"], axis = 1), axis = 1)\ndf_prepared[\"features_nunique\"] = df.nunique(axis=1)\n\ndf_prepared[\"features_zero_sum\"] = (df.drop([\"id\", \"target\"], axis = 1) == 0).sum(axis = 1)\ndf_prepared[\"features_nonzero_sum\"] = (df.drop([\"id\", \"target\"], axis = 1) != 0).sum(axis = 1)\ndf_prepared[\"features_zero_mean\"] = (df.drop([\"id\", \"target\"], axis = 1) == 0).mean(axis = 1)\ndf_prepared[\"features_nonzero_mean\"] = (df.drop([\"id\", \"target\"], axis = 1) != 0).mean(axis = 1)\ndf_prepared[\"features_zero_std\"] = (df.drop([\"id\", \"target\"], axis = 1) == 0).std(axis = 1)\ndf_prepared[\"features_nonzero_std\"] = (df.drop([\"id\", \"target\"], axis = 1) != 0).std(axis = 1)\n\n# CART Feature Generator\nfor i in df_prepared.drop([\"id\", \"target\"], axis = 1).columns:\n    for j in [1,2]:    \n        cart_feature_gen(model_type = \"class\", dataframe = df_prepared, X = i, y = \"target\", threshold = j)\n        \nthreshold_cols = df_prepared.columns[df_prepared.columns.str.contains(\"DTREE_THRESH1\")]\ndf_prepared[\"DTREE_THRESH1_SUM\"] = df_prepared[threshold_cols].sum(axis = 1)\ndf_prepared[\"DTREE_THRESH1_MEAN\"] = df_prepared[threshold_cols].mean(axis = 1)\ndf_prepared[\"DTREE_THRESH1_STD\"] = df_prepared[threshold_cols].std(axis = 1)\ndf_prepared[\"DTREE_THRESH1_SKEW\"] = skew(df_prepared[threshold_cols], axis = 1)\ndf_prepared[\"DTREE_THRESH1_KURTOSIS\"] = kurtosis(df_prepared[threshold_cols], axis = 1)\nthreshold_cols = df_prepared.columns[df_prepared.columns.str.contains(\"DTREE_THRESH2\")]\ndf_prepared[\"DTREE_THRESH2_SUM\"] = df_prepared[threshold_cols].sum(axis = 1)\ndf_prepared[\"DTREE_THRESH2_MEAN\"] = df_prepared[threshold_cols].mean(axis = 1)\ndf_prepared[\"DTREE_THRESH2_STD\"] = df_prepared[threshold_cols].std(axis = 1)\ndf_prepared[\"DTREE_THRESH2_SKEW\"] = skew(df_prepared[threshold_cols], axis = 1)\ndf_prepared[\"DTREE_THRESH2_KURTOSIS\"] = kurtosis(df_prepared[threshold_cols], axis = 1)\ndel threshold_cols\n\n# Components of PCA \nfor i in [0,1,2]:\n    df_prepared[\"PC\"+str(i)] = pcadf[\"PC\"+str(0)]\n\n# K-Means Cluster    \nkmeansdf = StandardScaler().fit_transform(df.drop([\"id\",\"target\"],axis = 1))\nfor i in range(2,10):\n    kmeans = KMeans()\n    kfit = kmeans.fit(kmeansdf)\n    df_prepared[\"KMEANS_CL\"+str(i)] = kfit.labels_\n    \n    \n# QCut\nqcut_cols = pd.qcut(df.drop([\"id\", \"target\"], axis = 1).max().sort_values(), q=15, labels=range(1,16)).reset_index()\nfor i in range(1,16):\n    df_prepared[\"QCUTMAX\"+str(i)+\"_SUM\"] = df[qcut_cols[qcut_cols[0] == i][\"index\"]].sum(axis = 1)\n    df_prepared[\"QCUTMAX\"+str(i)+\"_MEAN\"] = df[qcut_cols[qcut_cols[0] == i][\"index\"]].mean(axis = 1)\n    df_prepared[\"QCUTMAX\"+str(i)+\"_STD\"] = df[qcut_cols[qcut_cols[0] == i][\"index\"]].std(axis = 1)\n    df_prepared[\"QCUTMAX\"+str(i)+\"_SKEW\"] = skew(df[qcut_cols[qcut_cols[0] == i][\"index\"]],axis = 1)\n    df_prepared[\"QCUTMAX\"+str(i)+\"_KURTOSIS\"] = kurtosis(df[qcut_cols[qcut_cols[0] == i][\"index\"]],axis = 1)\n    \nqcut_cols = pd.qcut(df.drop([\"id\", \"target\"], axis = 1).mean().sort_values(), q=15, labels=range(1,16)).reset_index()\nfor i in range(1,16):\n    df_prepared[\"QCUTMEAN\"+str(i)+\"_SUM\"] = df[qcut_cols[qcut_cols[0] == i][\"index\"]].sum(axis = 1)\n    df_prepared[\"QCUTMEAN\"+str(i)+\"_MEAN\"] = df[qcut_cols[qcut_cols[0] == i][\"index\"]].mean(axis = 1)\n    df_prepared[\"QCUTMEAN\"+str(i)+\"_STD\"] = df[qcut_cols[qcut_cols[0] == i][\"index\"]].std(axis = 1)\n    df_prepared[\"QCUTMEAN\"+str(i)+\"_SKEW\"] = skew(df[qcut_cols[qcut_cols[0] == i][\"index\"]],axis = 1)\n    df_prepared[\"QCUTMEAN\"+str(i)+\"_KURTOSIS\"] = kurtosis(df[qcut_cols[qcut_cols[0] == i][\"index\"]],axis = 1)\n    \nqcut_cols = pd.qcut(df.drop([\"id\", \"target\"], axis = 1).std().sort_values(), q=15, labels=range(1,16)).reset_index()\nfor i in range(1,16):\n    df_prepared[\"QCUTSTD\"+str(i)+\"_SUM\"] = df[qcut_cols[qcut_cols[0] == i][\"index\"]].sum(axis = 1)\n    df_prepared[\"QCUTSTD\"+str(i)+\"_MEAN\"] = df[qcut_cols[qcut_cols[0] == i][\"index\"]].mean(axis = 1)\n    df_prepared[\"QCUTSTD\"+str(i)+\"_STD\"] = df[qcut_cols[qcut_cols[0] == i][\"index\"]].std(axis = 1)\n    df_prepared[\"QCUTSTD\"+str(i)+\"_SKEW\"] = skew(df[qcut_cols[qcut_cols[0] == i][\"index\"]],axis = 1)\n    df_prepared[\"QCUTSTD\"+str(i)+\"_KURTOSIS\"] = kurtosis(df[qcut_cols[qcut_cols[0] == i][\"index\"]],axis = 1)\n    \n","ccbff456":"# LightGBM GBDT with KFold or Stratified KFold\ndef kfold_lightgbm(df, num_folds, stratified=False):\n    # Divide in training\/validation and test data\n    train_df = df[df['target'].notnull()]\n    train_df[\"target\"] = train_df[\"target\"].astype(int)\n    test_df = df[df['target'].isnull()]\n    print(\"Starting LightGBM. Train shape: {}, test shape: {} \\n\\n\".format(train_df.shape, test_df.shape))\n    \n    # Cross validation model\n    if stratified:\n        folds = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=1001)\n    else:\n        folds = KFold(n_splits=num_folds, shuffle=True, random_state=1001)\n   \n    # Create arrays and dataframes to results\n    oof_preds = np.zeros(train_df.shape[0])\n    sub_preds1 = np.zeros(test_df.shape[0])\n    sub_preds2 = np.zeros(test_df.shape[0])\n    sub_preds3 = np.zeros(test_df.shape[0])\n    sub_preds4 = np.zeros(test_df.shape[0])\n    sub_preds5 = np.zeros(test_df.shape[0])\n    sub_preds6 = np.zeros(test_df.shape[0])\n    sub_preds7 = np.zeros(test_df.shape[0])\n    sub_preds8 = np.zeros(test_df.shape[0])\n    sub_preds9 = np.zeros(test_df.shape[0])\n\n    # Independent Variables\n    feats = [f for f in train_df.columns if f not in ['target', 'id']]\n\n    train_error = []\n    valid_error = []\n    \n    # CV\n    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats], train_df['target'])):\n        train_x, train_y = train_df[feats].iloc[train_idx], train_df['target'].iloc[train_idx]\n\n        valid_x, valid_y = train_df[feats].iloc[valid_idx], train_df['target'].iloc[valid_idx]\n\n        # LightGBM parameters found by Bayesian optimization\n        clf = LGBMClassifier(\n            nthread=4,\n            n_estimators=10000,\n            learning_rate=0.02,\n            num_leaves=34,\n            colsample_bytree=0.9497036,\n            subsample=0.8715623,\n            max_depth=8,\n            reg_alpha=0.041545473,\n            reg_lambda=0.0735294,\n            min_split_gain=0.0222415,\n            min_child_weight=39.3259775,\n            silent=-1,\n            verbose=-1)\n\n        print(\"FOLD:\", n_fold+1)\n        print(\"-------------------------------------------------------------------\")\n        clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)],\n                verbose=200, early_stopping_rounds=200)\n        \n        train_error.append(clf.best_score_[\"training\"][\"multi_logloss\"])\n        valid_error.append(clf.best_score_[\"valid_1\"][\"multi_logloss\"])\n\n        \n        print(\"\\n\")\n        \n        # Predictions for Test\n        proba =  clf.predict_proba(test_df[feats], num_iteration=clf.best_iteration_)\n        sub_preds1 += proba[:, 0] \/ folds.n_splits\n        sub_preds2 += proba[:, 1] \/ folds.n_splits\n        sub_preds3 += proba[:, 2] \/ folds.n_splits\n        sub_preds4 += proba[:, 3] \/ folds.n_splits\n        sub_preds5 += proba[:, 4] \/ folds.n_splits\n        sub_preds6 += proba[:, 5] \/ folds.n_splits\n        sub_preds7 += proba[:, 6] \/ folds.n_splits\n        sub_preds8 += proba[:, 7] \/ folds.n_splits\n        sub_preds9 += proba[:, 8] \/ folds.n_splits\n        del clf, train_x, train_y, valid_x, valid_y\n        \n    print(\"MODEL RESULT\")\n    print(\"-------------------------------------------------------------------\")\n    print(\"Train Errors:\", train_error)\n    print(\"Valid Errors:\", valid_error, \"\\n\")\n    print(\"Train CV Log-Loss Mean:\", np.mean(train_error))\n    print(\"Valid CV Log-Loss Mean:\", np.mean(valid_error), \"\\n\\n\")\n\n    # Submission\n    print(\"SUBMISSION FILE IS CREATED!\")\n    test_sub = pd.DataFrame({\n        \"id\":test_df.id,\"Class_1\":sub_preds1,\"Class_1\":sub_preds1,\"Class_2\":sub_preds2,\"Class_3\":sub_preds3,\n        \"Class_4\":sub_preds4,\"Class_5\":sub_preds5,\"Class_6\":sub_preds6,\"Class_7\":sub_preds7,\n        \"Class_8\":sub_preds8, \"Class_9\":sub_preds9\n    })\n    test_sub.to_csv(\"submission.csv\", index=False)\n    \n\n    \nkfold_lightgbm(df_prepared, num_folds = 10, stratified=True)","c727fd6f":"train = df_prepared[df_prepared.target.isnull() == False]\ntrain[\"target\"] = train[\"target\"].astype(int)\ntest = df_prepared[df_prepared.target.isnull()]\n\ntrain_x = train.drop([\"target\", \"id\"], axis = 1)\ntrain_y = train.target\ntest_x = test.drop([\"target\", \"id\"], axis = 1)\n\nmodel = LGBMClassifier(\n            nthread=4,\n            n_estimators=350,\n            learning_rate=0.02,\n            num_leaves=34,\n            colsample_bytree=0.9497036,\n            subsample=0.8715623,\n            max_depth=8,\n            reg_alpha=0.041545473,\n            reg_lambda=0.0735294,\n            min_split_gain=0.0222415,\n            min_child_weight=39.3259775,\n            silent=-1,\n            verbose=-1)\nmodel.fit(X = train_x, y = train_y)","0bb68e1b":"# Feature Importance \nplot_lgb_importances(model, plot=True, num=15)","44494bff":"preds = pd.DataFrame(model.predict_proba(test_x))\npreds.columns = list(map(lambda x: \"Class_\" + str(x+1), preds.columns))\nprobs = preds.columns.tolist()\npreds[\"id\"] = test[\"id\"]\npreds = preds[[\"id\"]+probs]\npreds.to_csv(\"submission_final.csv\", index = None)","285ff278":"# 1. Packages","1744e71d":"# 5. K-Means","956da48d":"### ANOVA - Kruskal Wallis for Target\n\n- H0: M0 = M1 = ... = Mn\n- H1: One of the all groups is different at least.\n\nThe result of the hypothesis testing is shows us, all of groups in the target variable are different each other for every variables. You can see the results below.","9f09cdf5":"# 2. Functions","25fb2030":"![](https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/26480\/logos\/header.png?t=2021-04-09-00-57-05)","a9cbf9d7":"# 3. Data & EDA","6c42391c":"### Missing Values\n\nThe problem does not include any missing value!","8e500fbe":"### Numerical Variables\n\nDescribe function gives us a chance to understand the numerical variables. Also if you look at maximum and 99th percentile values, you might realize outliers in a variable. \n\n**Note: All of the numerical features contain outliers :)**","1af0e611":"### Data Types\n\nThere are 77 variables in the data and 75 variables are numerical also independent variables.","b6ac29af":"# 4. Principle Component Analysis\n\nThe result of PCA is not enough to explain all data with more less component!\n\n**Sum of Explained Variance Ratio is 13%!**","1b8d6533":"# Tabular Playground Series - Jun 2021","12ca00ad":"# 7. Stratified 10 Fold Cross Validation","35aa4947":"### Correlation\n\nAll variables are not correlated each other.","926b71be":"# 6. Feature Engineering","b2fe41c5":"### Target Count","9486cf52":"# 8. Final Model","b2bfc39c":"### Summary Stats for Target"}}