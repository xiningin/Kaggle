{"cell_type":{"3d4ed4dd":"code","f5f4d26d":"code","4c0856a1":"code","671c894d":"code","3fb9ff78":"code","f5927948":"code","8dc64ee0":"code","6ac861ac":"code","008df39e":"code","bce9502a":"code","ea5226f7":"code","255731f7":"code","7592ef59":"code","5baa2c17":"code","f508002d":"code","2f80cd50":"code","35a9a441":"code","13de3a72":"code","79d1aafb":"code","770c6b83":"markdown","643e5802":"markdown","9dd568bb":"markdown","8b7aa241":"markdown","bdc35018":"markdown","cd44786b":"markdown","2ae25b6c":"markdown","95766eb5":"markdown","dc6b8e44":"markdown","8e66bf14":"markdown","7bdd06cd":"markdown","6b20a229":"markdown","50521923":"markdown","ce045de2":"markdown"},"source":{"3d4ed4dd":"# ---------------------------------------------------------------------------\n# Import des librairies\n# ---------------------------------------------------------------------------\n\n# librairies communes\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Librairies tensorflow\nfrom tensorflow.python.keras.models import Model\nfrom tensorflow.python.keras.layers import Flatten, Dense, Dropout\nfrom tensorflow.python.keras.optimizers import Adam\nfrom tensorflow.python.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.python.keras.applications.vgg16 import VGG16, preprocess_input\nfrom tensorflow.python.keras.utils.np_utils import to_categorical\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint","f5f4d26d":"# ---------------------------------------------------------------------------\n# Configuration des param\u00e8tres principaux du mod\u00e8le\n# ---------------------------------------------------------------------------\n# Le chemin vers la sauvegarde du r\u00e9seau\nmodel_savepath    = 'cnn_vgg16_model_trained_2.h5'     \n\n# Les chemins vers les jeu de donn\u00e9es\nTRAINING_DIR      = '..\/input\/dogs-vs-cats-redux-kernels-edition\/train'\nTESTING_DIR       = '..\/input\/dogs-vs-cats-redux-kernels-edition\/test'\n\nIMGSIZE       = 224    # Taille de l'image en input\nEPOCH         = 22     # nombre d'epoch \nBATCH_SIZE    = 16     # traitement par batch d'images avant la descente de gradient\nFREEZE_LAYERS = 15     # pour un VGG16 freeze de r\u00e9apprentissage de certaines couches\nTRAIN         = True   # Entrainement ou utilisation d'un r\u00e9seau d\u00e9j\u00e0 entrain\u00e9","4c0856a1":"# ---------------------------------------------------------------------------\n#  Constitution des jeux de donn\u00e9es\n# ---------------------------------------------------------------------------    \n\n# -------\n#  Jeu d'entrainement\n# -------\n\n# Dataframe de deux colonnes contenant les id des fichiers et leur label\ntrain_files = os.listdir(TRAINING_DIR)\ntrain_labels = []\n\nfor file in train_files:\n    train_labels.append(file.split(\".\")[0])\n\ndf_train = pd.DataFrame({\"id\": train_files, \"label\": train_labels})\n","671c894d":"df_train.head()","3fb9ff78":"# Image generator: attention il est pr\u00e9f\u00e9rable de ne pas utiliser d'augmentation de donn\u00e9es\n# Nous utilisons \u00e9galement un processing sp\u00e9cifique au VGG16 et non pas un rescale 1.\/255\ntrain_datagen =  \\\n        ImageDataGenerator(\n            preprocessing_function=preprocess_input,\n            validation_split=0.20)\n\ntrain_generator = \\\n        train_datagen.flow_from_dataframe(\n            df_train,\n            TRAINING_DIR,\n            x_col='id',\n            y_col='label',\n            has_ext=True,\n            shuffle=True,\n            target_size=(IMGSIZE, IMGSIZE),\n            batch_size=BATCH_SIZE,\n            subset='training',\n            class_mode='categorical')\n\ntrain_labels = to_categorical(train_generator.classes)","f5927948":"# -------\n#  Jeu de validation\n# -------\n\nvalid_generator = \\\n        train_datagen.flow_from_dataframe(\n            df_train,\n            TRAINING_DIR,\n            x_col='id',\n            y_col='label',\n            has_ext=True,\n            shuffle=True,\n            target_size=(IMGSIZE, IMGSIZE),\n            batch_size=BATCH_SIZE,\n            subset='validation',\n            class_mode='categorical')","8dc64ee0":"# -------\n# Jeu de test\n# -------\n\ntest_files = os.listdir(TESTING_DIR)\ndf_test = pd.DataFrame({\"id\": test_files, 'label': 'nan'})\n\ntest_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)    \ntest_generator = \\\n    test_datagen.flow_from_dataframe(\n        df_test, \n        TESTING_DIR, \n        x_col='id',\n        y_col=None, \n        has_ext=True, \n        target_size=(IMGSIZE, IMGSIZE), \n        class_mode=None, \n        seed=42,\n        batch_size=1, \n        shuffle=False\n    )","6ac861ac":"# -----------\n# VGG16 pre-entrain\u00e9 sans le classifier final\n# https:\/\/github.com\/keras-team\/keras\/issues\/4465\n# -----------\n\n# D\u00e9claration du mod\u00e8le VGG16 (sans le top qui est le classifier)\nbase_model = VGG16(include_top=False, weights=None, input_tensor=None,\n            input_shape=(IMGSIZE, IMGSIZE, 3))\n\n# Chargement des points pr\u00e9entrain\u00e9s du mod\u00e8le\n# Dataset = Full Keras Pretrained No Top\nbase_model.load_weights('..\/input\/full-keras-pretrained-no-top\/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5')","008df39e":"# Classifier\nx = base_model.output\nx = Flatten()(x)\nx = Dropout(0.5)(x)\nx = Dense(128, activation='relu', name='top-fc1')(x)\nx = Dense(128, activation='relu', name='top-fc2')(x)\nx = Dropout(0.3)(x)\n\n# output layer: nombre de neurones de sortie = nombre de classe a pr\u00e9dire\noutput_layer = Dense(2, activation='softmax', name='softmax')(x)","bce9502a":"# Assemblage du mod\u00e8le final\nnet_final = Model(inputs=base_model.input, outputs=output_layer)\n\n# freeze de certains layers (sp\u00e9cifique au mod\u00e8le utilis\u00e9)\nfor layer in net_final.layers[:FREEZE_LAYERS]:\n    layer.trainable = False\n    \n# Entrainement des derniers layers de classification\nfor layer in net_final.layers[FREEZE_LAYERS:]:\n    layer.trainable = True\n\n# compilation du modele\nnet_final.compile(optimizer=Adam(lr=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\nprint(net_final.summary())","ea5226f7":"# G\u00e9n\u00e9ration des STEPS_SIZE (comme nous utilisons des g\u00e9n\u00e9rateurs infinis)\n# Ceci est n\u00e9cessaire pour d\u00e9terminer \u00e0 quel moment nous avons parcouru enti\u00e9rement nos jeu de donn\u00e9es\nSTEP_SIZE_TRAIN = train_generator.n \/\/ train_generator.batch_size\nSTEP_SIZE_VALID = valid_generator.n \/\/ valid_generator.batch_size\nSTEP_SIZE_TEST  = test_generator.n  \/\/ test_generator.batch_size","255731f7":"if (TRAIN):\n    \n    # Cr\u00e9ation des Callbacks \u00e0 appeler apr\u00e9s chaque epoch\n    #   pour sauvegarde des r\u00e9sultats\n    checkpoint = ModelCheckpoint(\"model_1.h5\", monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n    #   pour arr\u00eat pr\u00e9matur\u00e9\n    early = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=1, mode='auto')\n    \n    # Entrainement du mod\u00e8le\n    history = net_final.fit_generator(\n                    generator=train_generator,\n                    steps_per_epoch = STEP_SIZE_TRAIN,\n                    validation_data = valid_generator,\n                    validation_steps = STEP_SIZE_VALID,\n                    callbacks = [checkpoint, early],\n                    epochs = EPOCH)\n    \n    # Sauvegarde du r\u00e9seau apr\u00e8s entrainement\n    net_final.save(model_savepath)    \n    \nelse:\n    net_final.load_weights('..\/input\/cnn-vgg16-model-trained-2\/cnn_vgg16_model_trained_2\/'+model_savepath)   \n","7592ef59":"# --------------------------------------\n# Affichage des courbes accuracy et Loss\n# --------------------------------------\nif (TRAIN):\n    plt.figure(1)\n    plt.subplot(211)\n    plt.plot(history.history['acc'])\n    plt.plot(history.history['val_acc'])\n    plt.title('model accuracy')\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n\n    plt.subplot(212)\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('model loss')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.show() ","5baa2c17":"# Evaluation du mod\u00e8le\n(eval_loss, eval_accuracy) = net_final.evaluate_generator(generator=valid_generator, steps=STEP_SIZE_TEST)\nprint(\"[INFO] accuracy: {:.2f}%\".format(eval_accuracy * 100))\nprint(\"[INFO] Loss: {}\".format(eval_loss))","f508002d":"# Affichage des classes du jeu d'entrainement\ntrain_generator.class_indices","2f80cd50":"# G\u00e9n\u00e9ration des pr\u00e9dictions\ntest_generator.reset()\npred = net_final.predict_generator(test_generator, steps=STEP_SIZE_TEST, verbose=1)","35a9a441":"# La colonne 1 repr\u00e9sente la probabilit\u00e9 que l'image soit un chien (la colonne 0 que l'image soit un chat)\npred[:,1]","13de3a72":"# Enregistrement fichier soumission\nsoumission=pd.DataFrame({\"id\":test_generator.filenames,\"label\":pred[:,1]})\nsoumission['id'] = soumission['id'].str[:-4].astype('int')\nsoumission = soumission.sort_values(by=['id'])\nsoumission.to_csv('soumission.csv', index=False)","79d1aafb":"# ---------------------------------------------------------\n# Affichage al\u00e9atoires images pr\u00e9dites 0 = chat \/ 1 = chien\n# ---------------------------------------------------------\nimport random\n\nn = soumission.shape[0]\nf = list(np.arange(1,n))\n\nc = 20\nr =random.sample(f, c)\nnrows = 4\nncols = 5\nfig, ax = plt.subplots(nrows=nrows, ncols=ncols, figsize=(nrows*5, ncols*5))    \nfor i in range(c):\n    file = str(soumission['id'][r[i]]) + '.jpg'\n    path = TESTING_DIR + \"\/\" + file\n    img = plt.imread(path)\n    plt.subplot(4, 5, i+1)\n    plt.imshow(img, aspect='auto')\n    plt.xticks([])\n    plt.yticks([])\n    plt.title(str(soumission['id'][r[i]])+\"\\n\"+str(soumission['label'][r[i]]))\nplt.show()","770c6b83":"**D\u00e9tails sur le Classifier**\n\nNous ajoutons la **couche de classification** (Top Layer) sur le r\u00e9seau de Convolution:\n* applatissement de l'image de sortie en un vecteur\n* ajout de 2 couches de 128 neurones\n* ajout d'une couche de r\u00e9gularisation pour \u00e9viter l'overfitting\n* ajour de la couche de sortie \u00e0 deux neurones pour la pr\u00e9diction (probabilistique)\n\nCette couche effectue la **decente de gradient** pour le calcul num\u00e9rique de l'optimisation de la fonction de co\u00fbt\n* **le feed forward** des input vers la couche de sortie\n* **la backpropagation** en fonction de l'erreur de pr\u00e9diction\n\n![image.png](attachment:image.png)\n\nL'image ci-dessus repr\u00e9sente le classifier. L'entr\u00e9e du classifier est un vecteur applati issue de la sortie du mod\u00e8le de convolution pour chaque image.","643e5802":"## Passons \u00e0 l'impl\u00e9mentation !","9dd568bb":"Pour donner une illustration du Dataframe que nous obtenons donc 2 colonnes :\n1. id avec le nom du fichier\n2. label avec la cat\u00e9gorie de l'image","8b7aa241":"**Assemblage du mod\u00e8le final**\n\nCette \u00e9tape permet simplement d'ajouter notre couche de sortie sur le mod\u00e8le CNN.\nNous nous occupons aussi de **Freezer certaines couches** du mod\u00e8les pour ne pas qu'elles soient r\u00e9entrain\u00e9es.","bdc35018":"## Entrainement du mod\u00e8le : r\u00e9gression logistique\nNous sommes rendus \u00e0 l'\u00e9tape ou nous pouvons entrainer notre mod\u00e8le.\n\nLes librairies de keras fit_generator vont donc effectuer la partie de descente de gradient pour optimiser la fonction de cout du r\u00e9seau.\nComme nous sommes dans une probl\u00e8matique de classification c'est la r\u00e9gression logistique qui va \u00eatre mise en oeuvre, d\u00e9tail de l'\u00e9quation ci-dessous:\n\n![image.png](attachment:image.png)\n\nCette \u00e9quation pr\u00e9sente la fonction de **co\u00fbt de la regression logistique** que le r\u00e9seau cherche \u00e0 optimiser. Tous les calculs num\u00e9riques sont inclues dans les librairies que nous utilisons, c'est pourquoi nous n'avons rien \u00e0 faire, l'algorithme est d\u00e9j\u00e0 impl\u00e9ment\u00e9 et optimis\u00e9.","cd44786b":"## CNN: Utilisation d'un mod\u00e8le VGG 16 pr\u00e9entrain\u00e9 pour Reconnaissance d'image\n\n![image.png](attachment:image.png)\n\n\nDans ce Chapitre nous allons voir comment utiliser un r\u00e9seau CNN **VGG16 d\u00e9j\u00e0 entrain\u00e9**.\nNous allons \u00e9galement cr\u00e9er une **couche de classification sp\u00e9cifique** \u00e0 nos donn\u00e9es. \n\nLe mod\u00e8le VGG 16 que nous allons utiliser est un mod\u00e8le pr\u00e9entrain\u00e9 sur des millions d'images de la base imagenet. \n\n**Les avantages sont multiples** : cel\u00e0 permet de r\u00e9pondre a un probl\u00e8me sur un dataset sp\u00e9cifique sans partir de zero et d'autre part les temps d'entrainement sont \u00e9galement bien plus court qu'un mod\u00e8le complet.\n\nDans l'exemple pr\u00e9sent\u00e9 le mod\u00e8le pourrait r\u00e9ussir sans doute tr\u00e9s bien pour la reconnaissance de chien\/chat sans entrainement sp\u00e9cifique.\nCependant nous allons voir comment reprendre ce type de r\u00e9seau et de mettre en oeuvre un entrainement sur un classifieur sp\u00e9cifique \u00e0 nos jeux de donn\u00e9es.\n\nA noter que pour changer nous allons cette fois utiliser les librairies Keras **inclues** de tensorflow (et non les lib sp\u00e9cifiques). \nCe point m\u00e9rite d'\u00eatre soulign\u00e9 car la syntaxe de la construction du r\u00e9seau sera l\u00e9gerement diff\u00e9rente des deux pr\u00e9c\u00e9dents chapitres.\n\nNous allons \u00e9galement revoir et **approfondir certaines notions** vues dans les pr\u00e9c\u00e9dents chapitres.\n\nR\u00e9f\u00e9rences:\nhttps:\/\/github.com\/aidiary\/keras-examples\/blob\/master\/vgg16\/17flowers\/finetuning_with_preprocess.py\nhttps:\/\/github.com\/aditya9898\/transfer-learning\/blob\/master\/transfer-learning.py\nhttp:\/\/cs231n.github.io\/convolutional-networks\/","2ae25b6c":"Nous allons maintenant parcourir toutes les images pour le **jeu d'entrainement** gr\u00e0ace \u00e0 une librairie Keras.\n\nNous sp\u00e9cifions 80% pour le jeu d'entrainement (Il faut veiller \u00e0 appeler le subset: training)\n\n**Fonctionnement du g\u00e9n\u00e9rateur** : pour pr\u00e9ciser de fa\u00e7on synth\u00e9tique le mode de fonctionnement : le g\u00e9n\u00e9rateur va aller ouvrir chaque image sous forme d'un tableau de 3 dimensions (numpy array) puis lui appliquer une fonction de traitement (preprocess_input) sp\u00e9cifique au r\u00e9seau VGG16 dans notre cas. Nous ne devons pas utiliser d'autres transformations d'augmentation car le r\u00e9seau VGG16 se comporte mal dans ce cas (c'est un r\u00e9seau d\u00e9j\u00e0 pr\u00e9-entrain\u00e9)\n\nL'image sera labellis\u00e9e avec la colonne 'label' pour permettre l'apprentissage supervis\u00e9.","95766eb5":"### Architecture du mod\u00e8le VGG16\n\nLe mod\u00e8le pr\u00e9sent\u00e9 ci-dessous est un mod\u00e8le de r\u00e9seau CNN propos\u00e9 par K. Simonyan et A. Zisserman de l'universit\u00e9 d'Oxford.\nIl permet d'atteindre 92.7% sur la banque de donn\u00e9es ImageNet qui contient **14 Millions d'image appartenant \u00e0 1000 classes**.\n\n\nDans ce mod\u00e8le nous n'allons pas utiliser la couche de classification du VGG16 mais nous allons ajouter et **entrainer notre propre classifier**, sp\u00e9cifique \u00e0 la r\u00e9sponse au probl\u00e8me de nos propres donn\u00e9es.\n\n\n![image.png](attachment:image.png)","dc6b8e44":"## Combinaison du VGG16 et de notre classifier\nC'est dans cette \u00e9tape que nous utilisons le r\u00e9seau VGG16 en ajoutant notre **propre couche de classification**. \n\nCette couche FC (enti\u00e8rement connect\u00e9e) est un r\u00e9seau de neurones classique qui va permettre \u00e0 partir de la sortie du VGG16 de faire une pr\u00e9diction. \n\nSa couche de sortie contient n neurones (n repr\u00e9sentant le nombre de classes que nous souhaitons pr\u00e9dire) dans ce cas : 2.\n\nCommen\u00e7ons par d\u00e9clarer le r\u00e9seau qui va nous servir de **mod\u00e8le de base** :","8e66bf14":"L'image en entr\u00e9e \u00e0 une taille de 224 * 224 * 3(RVB). Les 3 dimensions \u00e9tant les couches RVB constituant les images.\n\nEnsuite les layers de convolution de par leur m\u00e9chanisme cr\u00e9ent un ensemble de couches suppl\u00e9mentaires (features) qui \"augmentent\" les dimensions de l'image (\u00e9paisseur) en ajoutant les r\u00e9sultats des filtres de convolution. \n\nLa couche de pooling r\u00e9duit la largeur\/hauteur de l'image. \n\nAu final avant la couche enti\u00e8rement connect\u00e9e nous avons une image tr\u00e8s \u00e9paisse mais tr\u00e9s petite.\n\nUn rapide calcul montre qu'il y a 224 * 224 * 3 = 150 528 pixels en entr\u00e9e pour 1 835 008 en sortie ( 7 * 7 * 512). \n\nEn passant cette partie d'extraction de features du r\u00e9seau \"l'image\" r\u00e9sultante s'est donc enrichie en information.\n\n**Exemple de filtres de convolution:\n**\n![image.png](attachment:image.png)","7bdd06cd":"### Pr\u00e9paration des jeux de donn\u00e9es\n\nCes \u00e9tapes sont similaires \u00e0 celles vues dans le cours pr\u00e9c\u00e9dent, n'h\u00e9sitez pas \u00e0 vous y reporter.\n\n#### Rappel Split Train \/ Validation\n\nNous allons comme d'habitude commencer par la pr\u00e9paration des jeux de donn\u00e9es permettant d'entrainer notre r\u00e9seau. \n\nPour cel\u00e0 nous allons splitter dans un ratio 80\/20% les images permettant l'apprentissage et les images permettant la validation.\n\nLe mod\u00e8le va donc apprendre sur le jeu d'entrainement puis valider les pr\u00e9dictions sur le jeu de validation.\nA noter que ces deux jeux de donn\u00e9es sont labellis\u00e9s : c'est \u00e0 dire que l'on connait les classes auquelles appartiennent les images\n\nLe sch\u00e9ma suivant donne une illustration de la r\u00e9partition entre le jeu d'entrainement et le jeu de validation: \n\n\n![image.png](attachment:image.png)","6b20a229":"M\u00eame chose pour le **jeu de validation** \u00e0 la seule diff\u00e9rence du nom du subset : validation","50521923":"## G\u00e9n\u00e9ration des pr\u00e9dictions\nMaintenant que notre mod\u00e8le est entrain\u00e9 nous allons pouvoir l'\u00e9valuer et lui demander de faire les pr\u00e9dictions sur nos images de tests","ce045de2":"Pour le jeu de test c'est un peu diff\u00e9rent : on pr\u00e9cise class_mode et y_col \u00e0 None car nous n'avons pas de label.\nNous mettons batch size \u00e0 1."}}