{"cell_type":{"6cdebb37":"code","4f8789d6":"code","ac3c32db":"code","6dc48950":"code","beef7e93":"code","fbd0df04":"code","26eee891":"code","246c4aab":"code","c9f76cc5":"code","cc14218b":"code","074b6325":"code","a8d1f8e1":"code","6534ee61":"code","60426a54":"code","44c2304e":"code","8c53d1ba":"code","c4a4604e":"markdown","6f39fd2a":"markdown","2bb6918d":"markdown","5cf128c7":"markdown","a4d9af85":"markdown"},"source":{"6cdebb37":"%matplotlib inline\n#\u9019\u662fjupyter notebook\u7684magic word\u02d9\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom IPython import display\n","4f8789d6":"import os\n#\u5224\u65b7\u662f\u5426\u5728jupyter notebook\u4e0a\ndef is_in_ipython():\n    \"Is the code running in the ipython environment (jupyter including)\"\n    program_name = os.path.basename(os.getenv('_', ''))\n\n    if ('jupyter-notebook' in program_name or # jupyter-notebook\n        'ipython'          in program_name or # ipython\n        'jupyter' in program_name or  # jupyter\n        'JPY_PARENT_PID'   in os.environ):    # ipython-notebook\n        return True\n    else:\n        return False\n\n\n#\u5224\u65b7\u662f\u5426\u5728colab\u4e0a\ndef is_in_colab():\n    if not is_in_ipython(): return False\n    try:\n        from google import colab\n        return True\n    except: return False\n\n#\u5224\u65b7\u662f\u5426\u5728kaggke_kernal\u4e0a\ndef is_in_kaggle_kernal():\n    if 'kaggle' in os.environ['PYTHONPATH']:\n        return True\n    else:\n        return False\n\nif is_in_colab():\n    from google.colab import drive\n    drive.mount('\/content\/gdrive')\n\nos.environ['TRIDENT_BACKEND'] = 'pytorch'\nkaggle_kernal=None\nif is_in_kaggle_kernal():\n    os.environ['TRIDENT_HOME'] = '.\/trident'\n    \nelif is_in_colab():\n    os.environ['TRIDENT_HOME'] = '\/content\/gdrive\/My Drive\/trident'\n  \n","ac3c32db":"#\u70ba\u78ba\u4fdd\u5b89\u88dd\u6700\u65b0\u7248 \n\n!pip uninstall tridentx -y\n!pip install ..\/input\/trident\/tridentx-0.7.3.21-py3-none-any.whl --upgrade\n\nimport re\nimport pandas\nimport json\nimport copy\n\nimport numpy as np\n#\u8abf\u7528trident api\n\nimport random\nfrom tqdm import tqdm\nimport scipy\nimport time\nimport glob\nimport trident as T\nfrom trident import *","6dc48950":"if is_gpu_available():\n    !pip install onnxruntime-gpu\nelse:\n    !pip install onnxruntime","beef7e93":"\nimg=image2array('..\/input\/facer-recognition-examples\/similar_faces\/TW_crop\/song\/song01.jpg')\nimg=Resize((112,112),True)(img)\nimg=Normalize(127.5,127.5)(img)\nimg=np.expand_dims(image_backend_adaption(img),0)\n\nprint(img.shape)\narray2image(image2array('..\/input\/facer-recognition-examples\/similar_faces\/TW_crop\/song\/song01.jpg'))\n#\u9019\u662f\u5b8b\u82b8\u6a3a","fbd0df04":"array2image(image2array('..\/input\/facer-recognition-examples\/similar_faces\/TW_crop\/xia\/xia02.jpg'))\n#\u9019\u662f\u590f\u96e8\u55ac","26eee891":"import onnxruntime as ort\nort_session = ort.InferenceSession('..\/input\/facer-recognition-examples\/new_arcface_with_mask.onnx')\ninput_name = ort_session.get_inputs()[0].name\nprint(ort_session.get_inputs()[0])","246c4aab":"\nembedd=ort_session.run(None, {input_name: img})[0][0]\nprint(embedd.shape)","c9f76cc5":"def tile_images(images,max_row=3,name=''):\n    allimages=OrderedDict()\n    for i in range(max_row):\n        allimages[i]=[]\n    for i in range(len(images)):\n        img=image2array(images[i])\n        img=Resize((112,112),True)(img)\n        allimages[i%max_row].append(img)\n        if i==len(images)-1:\n            if (i+1)%max_row==0:\n                pass\n            else:\n                for n in range(1,max_row):\n                    allimages[(i+n)%max_row].append(np.ones((112,112,3))*255)\n                    if (i+n+1)%max_row==0:\n                        break\n    merge_images=[]\n    for i in range(max_row):\n        concate_image=np.concatenate(allimages[i],axis=1)\n        merge_images.append(concate_image)\n    merge_image=np.concatenate(merge_images,axis=0)\n    print(name)\n    display.display(array2image(merge_image))\n    print('')\n    print('')\n        \n    ","cc14218b":"def face2embedd(img_path):\n    img=image2array(img_path)\n    img=Resize((112,112),True)(img)\n    img=Normalize(127.5,127.5)(img)\n    img=np.expand_dims(image_backend_adaption(img),0)\n    embedd=ort_session.run(None, {input_name: img})[0][0]\n    return embedd\n    ","074b6325":"songs=[]#\u5b8b\u82b8\u6a3a\nxias=[]#\u590f\u96e8\u55ac\n\nsong_imgs=glob.glob('..\/input\/facer-recognition-examples\/similar_faces\/TW_crop\/song\/*.jpg')\nxia_imgs=glob.glob('..\/input\/facer-recognition-examples\/similar_faces\/TW_crop\/xia\/*.jpg')\nprint(len(song_imgs))\nprint(len(xia_imgs))\n\ntile_images(song_imgs,max_row=3,name='\u5b8b\u82b8\u6a3a')\ntile_images(xia_imgs,max_row=3,name='\u590f\u96e8\u55ac')","a8d1f8e1":"\n\nfor img_path in song_imgs:\n    songs.append(face2embedd(img_path)) \n    \nfor img_path in xia_imgs:\n    xias.append(face2embedd(img_path)) \n\nsong_array=random.choice(songs)\nxia_array=random.choice(xias)\n\nsongs=np.stack(songs,0)\nxias=np.stack(xias,0)\n\ninternal_song=[]\nprint('\u5b8b\u82b8\u6a3a')\nfor n in range(len(songs)-1):\n    internal_song.extend(element_cosine_distance(songs[n:n+1,:],songs,axis=1)[n+1:].tolist()) \ninternal_song=np.array(internal_song)\nprint('\u6700\u5927\u503c:{0:.2%} \u6700\u5c0f\u503c:{1:.2%} \u5e73\u5747\u503c:{2:.2%}'.format(internal_song.max(),internal_song.min(),internal_song.mean()))\n\n\ninternal_xia=[]\nprint('\u590f\u96e8\u55ac')\nfor n in range(len(xias)-1):\n    internal_xia.extend(element_cosine_distance(xias[n:n+1,:],xias,axis=1)[n+1:].tolist()) \ninternal_xia=np.array(internal_xia)\nprint('\u6700\u5927\u503c:{0:.2%} \u6700\u5c0f\u503c:{1:.2%} \u5e73\u5747\u503c:{2:.2%}'.format(internal_xia.max(),internal_xia.min(),internal_xia.mean()))\n\n","6534ee61":"\nprint('\u6bd4\u8f03\u5169\u8005\u4ea4\u4e92\u76f8\u4f3c\u6027')\nintra_ab=[]\nfor n in range(len(songs)):\n    intra_ab.extend(element_cosine_distance(songs[n:n+1,:],xias,axis=1).tolist()) \n\nintra_ab=np.array(intra_ab)\nprint('\u6700\u5927\u503c:{0:.2%} \u6700\u5c0f\u503c:{1:.2%} \u5e73\u5747\u503c:{2:.2%}'.format(intra_ab.max(),intra_ab.min(),intra_ab.mean()))\nprint('\u5169\u4eba\u8aa4\u5224\u6a5f\u7387:{0:.2%}'.format(np.greater(intra_ab,0.6).astype(np.float32).mean()))","60426a54":"\n#\u9078\u53d6\u5e7e\u500b\u77e5\u540d\u4eba\u7269\nfaces=['Yao_Ming','Bill_Gates','Bill_Clinton','Madonna','Michael_Jackson','Keanu_Reeves','Queen_Elizabeth_II','Warren_Buffett']\n\n        \n#\u4ee5\u4e0b\u662f\u8457\u8272\u7684\u8abf\u8272\u76e4\ncolors = ['#ff0000', '#ffff00', '#00ff00', '#00ffff', '#0000ff',\n         '#ff00ff', '#990000', '#999900', '#009900', '#009999',\n         '#000000']\n","44c2304e":"from sklearn import manifold\nfrom umap import umap_\ndef visualize():\n    features=[]\n    labels=[]\n    \n    #\u9010\u4e00\u53d6\u51fa\u6bcf\u500b\u540d\u4eba\u7684\u6240\u6709\u7167\u7247\uff0c\u628a\u7167\u7247\u4e1f\u5165\u6a21\u578b\uff0c\u53bb\u8a08\u7b97output_layer\u8f38\u51fa\u7684\u8868\u5fb5\u5411\u91cf\n    for i in range(len(faces)):\n        face=faces[i]\n        imgs=glob.glob('..\/input\/facer-recognition-examples\/crop_famous_faces\/{0}\/*.*g'.format(face))\n        tile_images(imgs,max_row=3,name=face)\n        for im in imgs:\n            features.append(face2embedd(im))\n            labels.append(i)\n    labels=np.array(labels)\n    features=np.array(features)\n    \n    #\u5229\u7528TSNE\u964d\u7dad\u62102\u7dad\u5f8c\uff0c\u7e6a\u88fd\u6210\u6563\u5e03\u5716\n    \n\n    fig = plt.figure(figsize=(18,8))\n    \n    ax1= fig.add_subplot(1, 2, 1)\n    #\u4f7f\u7528TSNE\u964d\u7dad\u81f32\u7dad\n    tsne2 = manifold.TSNE(n_components=2, init='pca',metric='cosine',perplexity=15,angle=0.2, random_state=0)  # \u5229\u7528t-sne\u5c07512\u7279\u5fb5\u5411\u91cf\u964d\u7dad\u81f32\n    features_tsne2 = tsne2.fit_transform(features)\n    features_tsne2=l2_normalize(features_tsne2)\n    \n    for i in range(len(faces)):\n        x_i = features_tsne2[:,0][labels==i]\n        y_i = features_tsne2[:,1][labels==i]\n        ax1.scatter(x_i,y_i,s=100,marker='o',c=colors[i])\n        \n    ax2 = fig.add_subplot(122, projection='3d')\n    #\u4f7f\u7528UMAP\u5c07\u7dad\u5ea6\u62d3\u5c55\u70ba\u7403\u578b\u5ea7\u6a19\n    sphere_mapper = umap_.UMAP(output_metric='haversine', random_state=42).fit(features)\n    \n    R = 3 # Size of the doughnut circle\n    r = 1 # Size of the doughnut cross-section\n\n    x = (R + r * np.cos(sphere_mapper.embedding_[:, 0])) * np.cos(sphere_mapper.embedding_[:, 1])\n    y = (R + r * np.cos(sphere_mapper.embedding_[:, 0])) \n    z = r * np.sin(sphere_mapper.embedding_[:, 0])\n\n    for i in range(len(faces)):\n        x_i = x[labels==i]\n        y_i = y[labels==i]\n        z_i = z[labels==i]\n        ax2.scatter(x_i,y_i,z_i,s=100,marker='o',c=colors[i])\n\n\n    plt.legend(faces, loc ='lower left')\n    plt.title('arcface_visualization')\n    make_dir_if_need('Results\/arcface_visualization.jpg')\n    plt.savefig('Results\/arcface_visualization.jpg', bbox_inches='tight')\n    plt.show()","8c53d1ba":"visualize()","c4a4604e":"\u4e00\u5f35\u5716\u7247\u8981\u9001\u5165\u6a21\u578b\u505a\u4eba\u81c9\u8b58\u5225\u524d\uff0c\u9700\u8981\u7d93\u904e\u4ee5\u4e0b\u9810\u8655\u7406\u6b65\u9a5f:  \n1. \u5f9e\u5716\u6a94\u8def\u5f91\u8b80\u53d6\u8f49\u70ba\u5411\u91cf  \n2. \u5c07\u5716\u7247\u7e2e\u653e\u81f3(112,112,3)  \n3. \u5c07\u5716\u7247\u6e1b\u53bb127.5\u7136\u5f8c\u518d\u9664\u4ee5127.5  \n4. \u5c07\u5716\u7247\u8f49\u6210\u901a\u9053\u5728\u524d\uff0c\u7136\u5f8c\u52a0\u5165\u6279\u6b21\u8ef8\uff0c\u53cb\u5c31\u662f\u5f62\u72c0\u8b8a\u6210(1,3,112,112)  \n\n\u4ee5\u4e0b\u6211\u5011\u9996\u5148\u62ff\u64da\u8aaa\u662f\u53f0\u7063\u9577\u5f97\u6700\u76f8\u4f3c\u7684\u5169\u500b\u5973\u660e\u661f\u5b8b\u82b8\u6a3a\u4ee5\u53ca\u590f\u96e8\u55ac\u4f86\u4f5c\u7bc4\u4f8b","6f39fd2a":"\u70ba\u4e86\u5f8c\u7e8c\u8655\u7406\u65b9\u4fbf\uff0c\u6211\u5011\u628a\u6574\u500b\u6d41\u7a0b\u5305\u88dd\u6210face2embedd\u9019\u500b\u51fd\u6578\uff0c\u76f4\u63a5\u5305\u62ec\u9810\u8655\u7406\u6d41\u7a0b\u4ee5\u53caonnx\u63a8\u8ad6\u70ba\u9577\u5ea6512\u7279\u5fb5\u5411\u91cf\u7684\u904e\u7a0b","2bb6918d":"\u6211\u5011\u9996\u5148\u8b80\u53d6\u5b8b\u82b8\u6a3a\u8207\u590f\u96e8\u55ac\u5169\u4eba\u7684\u6240\u6709\u81c9\u7167\u4e26\u8f49\u6210\u7279\u5fb5\u5411\u91cf\uff0c\u4e26\u8a08\u7b97\u5404\u81ea\u5167\u90e8\u4ee5\u53ca\u76f8\u4e92\u7684\u76f8\u4f3c\u6027\u3002","5cf128c7":"\u8981\u4f7f\u7528onnx\u505a\u63a8\u8ad6\uff0c\u9996\u5148\u8981\u5c07\u6a21\u578b\u8b80\u53d6\u81f3InferenceSession\uff0c\u7136\u5f8c\u6211\u5011\u53ef\u4ee5\u5229\u7528ort_session.get_inputs()\u53d6\u5f97\u8f38\u5165\u8b8a\u6578\u540d\u7a31\u4ee5\u53ca\u5f62\u72c0\u7684\u4fe1\u606f\u3002","a4d9af85":"\u7136\u5f8c\u53ea\u9700\u8981\u900f\u904eort_session.run\u5373\u53ef\u5b8c\u6210\u63a8\u8ad6\u3002"}}