{"cell_type":{"5362daf0":"code","47bc856b":"code","7bddcd2c":"code","7b7c28dd":"code","2dc9aedb":"code","dfc1db18":"code","904c6786":"code","a8cdd405":"code","478fe4b0":"code","f63b7e34":"code","34f5c7e1":"code","2c929d96":"code","482103bf":"code","a134fcd6":"code","79eed28d":"code","359263b6":"code","9380a858":"code","3d70e94c":"code","575b9ba1":"code","081dc05f":"code","5f0f4ba2":"code","bf4fd023":"code","eae22e9d":"code","b5356682":"code","6fc786c4":"code","973b3a87":"code","29e47c53":"code","7bea0a37":"code","8eb01e9f":"code","bf0408db":"code","6b7212e4":"markdown","a0d2ce42":"markdown","1b7ad3dc":"markdown","84d4163b":"markdown","3f0ea985":"markdown","7c06323d":"markdown","68971e2f":"markdown","072f9eba":"markdown","32246243":"markdown","f474828b":"markdown","ea8cab40":"markdown","a636d899":"markdown","cd6b3a66":"markdown","7afb1875":"markdown","231bb007":"markdown"},"source":{"5362daf0":"import pandas as pd\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns","47bc856b":"# Plots display settings\nplt.rcParams['figure.figsize'] = 12, 8\nplt.rcParams.update({'font.size': 14})","7bddcd2c":"# Original data\nFILE_PATH = '..\/input\/brazilian-vehicle-prices-july-2021-fipe\/tabela_fipe.csv'","7b7c28dd":"# Tensorflow settings\nBATCH_SIZE = 512\nEPOCHS = 1000\nSTOP_PATIENCE = 10\nWEIGHTED_SAMPLES = False","2dc9aedb":"def get_data():\n    \"\"\"Function loads and cleans the data.\n    :return: DataFrame with car features and prices.\n    \"\"\"\n    data = pd.read_csv(FILE_PATH)\n    # Replace 'Zero KM' by year 2022 assuming it's a new car\n    data['Ano'] = data['Ano'].str.replace('Zero KM', '2021').replace('2022', '2021')\n    data['Ano'] = data['Ano'].astype(int)\n    data['Autom\u00e1tico'] = data['Autom\u00e1tico'].astype(int)\n    return data\n\n\ndef df_to_dataset(df: pd.DataFrame, shuffle=True, weighted=False, batch_size=32):\n    \"\"\"Function transforms a pd.DataFrame into tf.data.Dataset.\n    :param df: Original DataFrame with price and car features.\n    :param shuffle: Boolean argument specifying if the data should be shuffled.\n    :param weighted: Boolean argument specifying if the dataset should contain sample weights\n    :param batch_size: Batch size\n    :return: Dataset for neural network\n    \"\"\"\n    labels = df.pop('Valor')\n\n    if weighted:  # Weight sample according to frequency of it's combustion type\n        weights_combustion = {'Gasolina': 1, 'Diesel': 3.9, '\u00c1lcool': 39.}\n        weights = df['Combust\u00edvel'].apply(lambda x: weights_combustion[x])\n        ds = tf.data.Dataset.from_tensor_slices((dict(df), labels, weights))\n    else:\n        ds = tf.data.Dataset.from_tensor_slices((dict(df), labels))\n\n    if shuffle:\n        ds = ds.shuffle(buffer_size=len(df))\n\n    ds = ds.batch(batch_size)\n\n    return ds\n\n\ndef get_normalization_layer(name: str, ds: tf.data.Dataset, weighted=False):\n    \"\"\"Function creates a normalization layer for the specified numeric feature.\n    :param name: Name of the numeric column (feature)\n    :param ds: Tensorflow Dataset object containing x and y values\n    :param weighted: Boolean argument specifying if the dataset contains sample weights\n    :return: Normalization layer adapted to the feature scale\n    \"\"\"\n    # Normalization layer for the feature\n    normalizer = tf.keras.layers.experimental.preprocessing.Normalization(axis=None)\n    # Dataset that only yields specified feature\n    if weighted:\n        feature_ds = ds.map(lambda x, y, w: x[name])\n    else:\n        feature_ds = ds.map(lambda x, y: x[name])\n    # Adapt the layer to the data scale\n    normalizer.adapt(feature_ds)\n    return normalizer\n\n\ndef get_category_encoding_layer(name: str, ds: tf.data.Dataset, dtype: str, max_tokens=None, weighted=False):\n    \"\"\"Function creates category encoding layers\n    with string or integer lookup index.\n    :param name: Name of the categorical feature\n    :param ds: Tensorflow Dataset object containing x and y values\n    :param dtype: String describing data type of the categorical feature (one of 'string' or 'int64')\n    :param max_tokens: Maximum number of tokens in the lookup index\n    :param weighted: Boolean argument specifying if the dataset contains sample weights\n    :return: Lambda function with categorical encoding layers and lookup index\n    \"\"\"\n    # Lookup layer which turns strings or integers into integer indices\n    if dtype == 'string':\n        index = tf.keras.layers.experimental.preprocessing.StringLookup(max_tokens=max_tokens)\n    else:  # 'int64'\n        index = tf.keras.layers.experimental.preprocessing.IntegerLookup(max_tokens=max_tokens)\n\n    # Dataset that only yields specified feature\n    if weighted:\n        feature_ds = ds.map(lambda x, y, w: x[name])\n    else:\n        feature_ds = ds.map(lambda x, y: x[name])\n\n    # Learn the set of possible values and assign them a fixed integer index\n    index.adapt(feature_ds)\n\n    # Create a discretization for integer indices\n    encoder = tf.keras.layers.experimental.preprocessing.CategoryEncoding(max_tokens=index.vocab_size())\n\n    # Apply one-hot encoding to the indices. The lambda function captures the\n    # layer and index so they could be used later in the functional model.\n    return lambda feature: encoder(index(feature))\n\n\ndef plot_history(hist):\n    \"\"\"Function plots a chart with training and validation metrics.\n    :param hist: Tensorflow history object from model.fit()\n    \"\"\"\n    # Losses\n    mae = hist.history['loss']\n    val_mae = hist.history['val_loss']\n\n    # Epochs to plot along x axis\n    x_axis = range(1, len(mae) + 1)\n\n    plt.plot(x_axis, mae, 'bo', label='Training')\n    plt.plot(x_axis, val_mae, 'ro', label='Validation')\n    plt.title('Training and validation MAE')\n    plt.ylabel('Loss (MAE)')\n    plt.xlabel('Epochs')\n    plt.legend()\n    plt.tight_layout()\n    plt.show()","dfc1db18":"data = get_data()\nprint(f'DataFrame shape: {data.shape}')\ndata.head()","904c6786":"data.describe()","a8cdd405":"correlation = data.corr()\nax = sns.heatmap(correlation, annot=True, cmap=plt.cm.Reds, fmt='0.3f')\nl, r = ax.get_ylim()\nax.set_ylim(l + 0.5, r - 0.5)\nplt.yticks(rotation=0)\nplt.title('Correlation Matrix')\nplt.show()","478fe4b0":"data.hist(bins=20, figsize=(14, 10))\nplt.show()","f63b7e34":"for feature in ('Marca', 'Modelo', 'Combust\u00edvel'):\n    print('-' * 40)\n    print(f'{feature}: {data[feature].nunique()} unique values')\n    groups = data[feature].value_counts(normalize=True)[:10]\n    plt.barh(groups.index, groups.values)\n    plt.title(f'Largest groups in {feature}')\n    plt.show()","34f5c7e1":"plt.scatter(data['Ano'], data['Valor'])\nplt.title('Car Age - Price Correlation')\nplt.xlabel('Year of production')\nplt.ylabel('Car price')\nplt.show()","2c929d96":"# Price depending on availability of automatic transmission\nautomat_price = data.groupby(by='Autom\u00e1tico')['Valor'].mean()\nprint('Price premium for automatic transmission:', automat_price.max() \/ automat_price.min())\nplt.bar(['No-automatic transmission', 'Automatic transmission'], automat_price.values)\nplt.title('Average prices')\nplt.show()","482103bf":"# Price depending on combustion type\ncombustion_price = data.groupby(by='Combust\u00edvel')['Valor'].mean()\ncombustion_premiums = combustion_price \/ combustion_price.min()\nfor c_type, premium in zip(combustion_price.index, combustion_premiums):\n    print(f'{c_type}: {premium}')\nplt.bar(combustion_price.index, combustion_price.values)\nplt.title('Average prices')\nplt.show()","a134fcd6":"# Price range for all brands and models\nmin_price = data['Valor'].min()\nmax_price = data['Valor'].max()\nmean_price = data['Valor'].mean()\nmedian_price = data['Valor'].median()\n\nprint(f'Price range: {min_price} - {max_price}')\nprint(f'Mean price: {mean_price}\\nMedian price: {median_price}')","79eed28d":"# Max\/min price ratio for car brands\nbrands_price_ratio = data.groupby(by='Marca')['Valor'].agg(['min', 'max', 'count'])\nbrands_price_ratio['price_ratio'] = brands_price_ratio['max'] \/ brands_price_ratio['min']\nmin_ratio = brands_price_ratio['price_ratio'].min()\nmax_ratio = brands_price_ratio['price_ratio'].max()\nmean_ratio = brands_price_ratio['price_ratio'].mean()\nmedian_ratio = brands_price_ratio['price_ratio'].median()\nprint(f'Max\/min price ratio: {min_ratio} - {max_ratio}\\nAverage price ratio = {mean_ratio}'\n      f'\\nMedian price ratio = {median_ratio}')\nbrands_price_ratio['price_ratio'].hist(bins=20)\nplt.title('Max\/min Price Ratio for Car Brands')\nplt.xlabel('Price ratio')\nplt.ylabel('Frequency')\nplt.show()","359263b6":"# Brands with large spread between minimum and maximum price\nbrands_price_ratio[brands_price_ratio['price_ratio'] >= 50]","9380a858":"# Brands with small spread between minimum and maximum price\nprint(brands_price_ratio[brands_price_ratio['price_ratio'] <= 5])","3d70e94c":"# Split original DataFrame into train, validation and test parts\n# proportionally distributing car models between the groups\ntrain, test = train_test_split(data, test_size=0.1, \n                               stratify=data['Marca'], random_state=0)\ntrain, val = train_test_split(train, test_size=0.2, \n                              stratify=train['Marca'], random_state=0)\nprint(len(train), 'train examples')\nprint(len(val), 'validation examples')\nprint(len(test), 'test examples')","575b9ba1":"# Create Dataset objects\ntrain_ds = df_to_dataset(train, weighted=WEIGHTED_SAMPLES, batch_size=BATCH_SIZE)\nval_ds = df_to_dataset(val, shuffle=False, weighted=WEIGHTED_SAMPLES, batch_size=BATCH_SIZE)\ntest_ds = df_to_dataset(test, shuffle=False, batch_size=BATCH_SIZE)","081dc05f":"# Lists to append numeric and categorical input features\nall_inputs = []  # Input layers\nencoded_features = []  # Results of preprocessing","5f0f4ba2":"# Input layer for 1 numeric value (car age)\nnumeric_col = tf.keras.Input(shape=(1,), name='Ano')\n# Normalization layer\nnormalization_layer = get_normalization_layer('Ano', train_ds, \n                                              weighted=WEIGHTED_SAMPLES)\n# Scaled down input value\nencoded_numeric_col = normalization_layer(numeric_col)\n# Add the objects to lists\nall_inputs.append(numeric_col)\nencoded_features.append(encoded_numeric_col)","bf4fd023":"# Create layers for categorical encoding\ncategorical_cols = ['Marca', 'Modelo', 'Combust\u00edvel']\nfor feature in categorical_cols:\n    # Input layer for 1 string value\n    categorical_col = tf.keras.Input(shape=(1,), name=feature, dtype='string')\n    # Index and encode the value\n    encoding_layer = get_category_encoding_layer(feature, train_ds, dtype='string', \n                                                 weighted=WEIGHTED_SAMPLES)\n    encoded_categorical_col = encoding_layer(categorical_col)\n    # Add the objects to lists\n    all_inputs.append(categorical_col)\n    encoded_features.append(encoded_categorical_col)","eae22e9d":"# Add input layer for a binary feature describing automatic \/ no-automatic type\nnumeric_col = tf.keras.Input(shape=(1,), name='Autom\u00e1tico')\nall_inputs.append(numeric_col)\nencoded_features.append(numeric_col)","b5356682":"# Create a model\nall_features = tf.keras.layers.concatenate(encoded_features)\nx = tf.keras.layers.Dense(\n    128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)\n)(all_features)\nx = tf.keras.layers.Dense(\n    64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)\n)(x)\nx = tf.keras.layers.Dense(\n    32, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)\n)(x)\noutput = tf.keras.layers.Dense(1)(x)\n\nmodel = tf.keras.Model(all_inputs, output)\n\nmodel.compile(optimizer='adam', loss='mae')","6fc786c4":"# Visualize the model graph\ntf.keras.utils.plot_model(model, show_shapes=True, rankdir='LR')","973b3a87":"model.summary()","29e47c53":"# Train the model\nearly_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n                                              patience=STOP_PATIENCE,\n                                              restore_best_weights=True)\n\nhistory = model.fit(train_ds, epochs=EPOCHS, validation_data=val_ds,\n                    verbose=2, callbacks=[early_stop],\n                    use_multiprocessing=True, workers=-1)","7bea0a37":"plot_history(history)","8eb01e9f":"# Evaluate the model on the test set\nmae = model.evaluate(test_ds)\nprint(f'Test MAE = {mae}')","bf0408db":"print(f'Overall mean price: {mean_price}\\nOverall median price: {median_price}')\nprint(f'MAE \/ Mean price: {mae\/mean_price}')\nprint(f'MAE \/ Median price: {mae\/median_price}')","6b7212e4":"# Neural Network for Pricing Cars\nOriginal data includes numeric and categorical features for cars in a csv format: a total of over 171,000 samples belonging to 90 brands. Neural network is used to predict car price based on it's features: brand, model, type of combustion engine, production year and availability of automatic transmission. keras preprocessing layers are used for scaling down numeric data and one-hot encoding categorical data. Optionally, sample weights could be applied during training to take into account disproportional representation of various types of cars in the dataset.","a0d2ce42":"## Create a Neural Network","1b7ad3dc":"The dataset has 5 features (most of them categorical) and the target value (Valor - car price).\n\nAvailable parameters include:\n- Marca - car brand\n- Modelo - car model\n- Ano - year of production\n- Combust\u00edvel - type of combustion\n- Autom\u00e1tico - availability of automatic transmission","84d4163b":"#### Distribution of numeric features and target values\nCar prices vary in wide range starting from couple of thousands to millions. Vast majority of cars are in a lower price segment. Year of production varies between 1985 and 2021. Newer cars are more frequent in the dataset than older ones. Cars without automatic transmission are twice as frequent as cars with an automatic transmission.","3f0ea985":"## Exploratory Data Analysis","7c06323d":"For various car brands price range from cheapest to most expensive in the dataset is not similar. To illustrate this more clearly we introduce a Max \/ Min Price Ratio = Max Price \/ Min Price.\n\nAs we can see below, price ratio varies from 1.05 to 281. Avearge price ratio stands at 38, and median value is 8.\n\nWe can conclude that car brand along would not be enough to accurately predict the price. We would have to take into account all other features including the model, which has about 6,200 unique values, car age, combustion and transmission type.","68971e2f":"#### Categorical features have underrepresented groups:\n- Dataset contains over 171,000 samples of cars belonging to 90 brands. Brands differ in popularity. Most popular brands account for up to 20% of the samples. Less popular and more exotic brands are represented by a few samples, which could make accurate pricing for these brands a hard task.\n- Total number of car models is about 6,200. If we apply One-Hot encoding, we would get high-dimensional input data without enough samples for each car model to make reasonable assumptions about price based on this feature. However, without this feature pricing cars would be less accurate, since number of features in the dataset is limited.\n- Combustion types are also unevenly distributed with gasoline cars prevailing and alcohol fuelled cars being in the minority.","072f9eba":"## Functions","32246243":"#### Correlation of numeric features with the target\nDataset has only one numeric feature (year of production), which demonstrates moderate correlation with the car price. For that reason using simple regression model for car pricing is not possible.\n\nBinary feature (0 or 1) for car transmission type has small impact on price. It's influence is less significant than the impact of other features.","f474828b":"## Praparing Data for Neural Network","ea8cab40":"#### Impact of categorical features\nOn average, price of cars with automatic transmission is higher by 43% than the price of cars without automatic transmission.","a636d899":"Below we can see car brands filtered by the Max \/ Min Price Ratio: first group contans brands with large price spread and the second group contains brands with less diverse prices.\n\nBoth high tier brands and mass market brands can have extremely wide price range. For that reason we cannot easily cluster car brands into groups that have similar price characteristics.\n\nWe can only conclude that the more samples of a particular brand there is in the dataset, the higher is the change that price spread for that brand would be large. Brands with limited number of samples mostly have narrow price range.","cd6b3a66":"#### Car age - price relationship\nNewer cars are mostly more expensive than the old ones. However, car age - price relationship is not linear, and lower price bound is almost identical for all years in the dataset.","7afb1875":"Overall price range is very big because the dataset includes different types of cars by various producers differing in age.","231bb007":"On average, diesel fuelled cars are 10 times more expensive than alcohol fuelled vehicles. Gasoline cars are 14 times more expensive."}}