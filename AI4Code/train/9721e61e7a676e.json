{"cell_type":{"de7c547f":"code","a14ad8a9":"code","0ae21fb1":"code","edb1e23e":"code","055cfd0e":"code","cfc42110":"code","209f722b":"code","f51eff54":"code","0ec8f2f5":"code","314af70c":"code","2ffb12fc":"code","34bf64b2":"code","09262d56":"code","b93471cb":"code","1a00df7d":"code","92a14a1e":"code","b26e7959":"code","1abe1b8f":"code","d503fc1f":"code","20770ace":"markdown","4c2848ae":"markdown","0babd9c7":"markdown","50c1e424":"markdown","6162d715":"markdown","71a08227":"markdown","1dd0b614":"markdown","4b23fd6a":"markdown","1f81ebfe":"markdown"},"source":{"de7c547f":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree\n\nimport nltk\nfrom nltk import word_tokenize\nfrom nltk.tokenize import sent_tokenize\nfrom nltk.corpus import PlaintextCorpusReader\n\nimport plotly as py\nimport plotly.graph_objs as go\nimport matplotlib.pyplot as plt \nimport plotly.express as pex\nimport holoviews as hv\nhv.extension('bokeh')\n\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator","a14ad8a9":"df = pd.read_csv(r'\/kaggle\/input\/federalist-papers\/fedPapers85.csv')\ndf.head(5)","0ae21fb1":"df.author.value_counts().plot(kind='pie')","edb1e23e":"# I mispelled some file names\ndf = df.replace({'HM_fed_18.txt':'Hamilton_and_Madion_fed_18.txt','HM_fed_19.txt':'Hamilton_and_Madion_fed_19.txt','HM_fed_20.txt':'Hamilton_and_Madion_fed_20.txt'})","055cfd0e":"# Read all papers into a corpus using NLTK\npaper_corpus = PlaintextCorpusReader(r'\/kaggle\/input\/federalist-papers\/FedPapersCorpus\/FedPapersCorpus', '.*')","cfc42110":"# Using loops here because the order of files in the corpus and dataframe are different\n# There may be a more optimized way to vectorize this operation\n#    by joining the dataframe with the corpus if required\n\ndf['chars'] = [len(paper_corpus.raw(fileids=[f])) for f in df.filename]\ndf['words'] = [len(paper_corpus.words(fileids=[f])) for f in df.filename]\ndf['sents'] = [len(paper_corpus.sents(fileids=[f])) for f in df.filename]\ndf['word_len'] = df.chars\/df.words\ndf['sent_len'] = df.words\/df.sents","209f722b":"auths = ['Hamilton','Madison','Jay','dispt']","f51eff54":"fig, ax = plt.subplots(2, 2)\nfor i, auth in enumerate(auths):\n    ax[i\/\/2, i%2].hist(df[df.author == auth].words)\n    ax[i\/\/2, i%2].set_title(auth + \" paper length\", weight='bold', size=12)\n    ax[i\/\/2, i%2].set_xlabel(\"Number of Words\")\n    ax[i\/\/2, i%2].set_ylabel(\"Quantity of Papers\")\n    ax[i\/\/2, i%2].set_xlim(0, 7000)\nplt.tight_layout()","0ec8f2f5":"fig, ax = plt.subplots(2, 2)\nfor i, auth in enumerate(auths):\n    ax[i\/\/2, i%2].hist(df[df.author == auth].sent_len)\n    ax[i\/\/2, i%2].set_title(auth + \" avg sent length\", weight='bold', size=12)\n    ax[i\/\/2, i%2].set_xlabel(\"Avg Sentence Length\")\n    ax[i\/\/2, i%2].set_ylabel(\"Quantity of Papers\")\n    ax[i\/\/2, i%2].set_xlim(25, 45)\nplt.tight_layout()","314af70c":"# Wordclouds for each author\nfor auth in auths:\n    print(auth)\n    text = paper_corpus.raw(fileids = list(df[df.author == auth].filename))\n    wc = WordCloud().generate(text)\n    plt.figure(figsize=(15,10))\n    plt.imshow(wc, interpolation='bilinear')\n    plt.axis(\"off\")\n    plt.show()","2ffb12fc":"# Scaling\nscaler = MinMaxScaler()\nc = df.drop(['author', 'filename'], axis=1).columns\ndf[c] = scaler.fit_transform(df[c])","34bf64b2":"vals = df.drop(['author', 'filename'], axis=1).values\ndoc_cluster = KMeans(n_clusters = 4)\ndoc_cluster.fit(vals)\nlabs = doc_cluster.labels_\ncentroids = doc_cluster.cluster_centers_","09262d56":"df['label'] =  labs\ndf.head(2)","b93471cb":"counter_df = df[['author','label']]\ncounter_df['count'] = 1\ncounter_df = counter_df.groupby(['author','label']).agg('count').reset_index()\ncounter_df.label = counter_df.label.astype(str)","1a00df7d":"plot = hv.Sankey(counter_df, kdims=[\"author\", \"label\"], vdims=[\"count\"])\nplot.opts(label_position='left',edge_color='author')","92a14a1e":"test_df = df[df.author == 'dispt']\ntrain_df= df[df.author != 'dispt']\n\nX_test = test_df.drop(['author','filename'],axis=1)\nX_train = train_df.drop(['author','filename'],axis=1)\ny_test = test_df.author\ny_train = train_df.author","b26e7959":"paper_tree = DecisionTreeClassifier()\npaper_tree.fit(X_train, y_train)","1abe1b8f":"fig = plt.figure(figsize=(25,20))\n_ = tree.plot_tree(paper_tree,\n                  feature_names = X_train.columns,\n                  class_names = np.sort(y_train.unique()))","d503fc1f":"y_pred = paper_tree.predict(X_test)\nlist(y_pred)","20770ace":"## Creating new features from raw text files","4c2848ae":"## Exploratory Analysis","0babd9c7":"## Using Decision Trees to classify","50c1e424":"## Viewing a pre-creating matrix of functional word frequencies","6162d715":"## K means clustering with all functional words","71a08227":"## Madison is historically credited as the author of the disputed papers!","1dd0b614":"### Sentences, Words, Characters, Word Length, Sentence Length","4b23fd6a":"## Visualizing with Sankey","1f81ebfe":"## Visualizing Decision Tree"}}