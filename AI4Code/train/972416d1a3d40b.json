{"cell_type":{"386c279a":"code","0f4cd8ed":"code","34807a53":"code","15f106c1":"code","8174a955":"code","83841733":"code","b9d6e363":"code","6c5377f0":"code","eecafee7":"code","a02a5416":"code","9cec162d":"code","8aa7ef96":"code","9b233465":"code","ff74b36b":"code","158d3ec8":"code","b0ccf030":"code","8fd07a14":"code","fcf3d9c7":"code","d7d9d96e":"code","557bee0c":"markdown","a447bf5b":"markdown","df83b7cd":"markdown","98436de7":"markdown","e1d491a1":"markdown","e6d5d25a":"markdown","938eb1ba":"markdown","b58eb54e":"markdown","856d2118":"markdown","a866960a":"markdown","ba5ced95":"markdown","301c5022":"markdown","6a22fc8d":"markdown"},"source":{"386c279a":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","0f4cd8ed":"df = pd.read_csv('..\/input\/protein-secondary-structure\/2018-06-06-ss.cleaned.csv')\ndf.head()","34807a53":"df = df[['seq','len','sst3']][(df['len']<40)&(df['has_nonstd_aa']==False)]\ndf.drop_duplicates(subset='seq',inplace=True)\ndf.head()","15f106c1":"def clear_asterisks(df, seqs_column_name, pct=30):\n    indices = []\n    for i, seq in enumerate(df[seqs_column_name]):\n        if (seq.count('*')*100\/len(seq)) <= pct: indices.append(i)\n    return df.iloc[indices]\n\ndf = clear_asterisks(df,'seq')\ndf.info()","8174a955":"plt.hist(x='len',data=df,bins=40,histtype='bar');\nplt.title('histogram of sequence lengths');","83841733":"dict_a = {'C':[],'H':[],'E':[]}\ndict_f = {'C':[],'H':[],'E':[]}\ndict_p = {'C':[],'H':[],'E':[]}\ndict_s = {'C':[],'H':[],'E':[]}\n\nfor se,sst in zip(df['seq'],df['sst3']):\n    for s,ss in zip(se,sst):\n        if s == 'A':\n            if ss == 'C': dict_a['C'].append(1)\n            elif ss == 'H': dict_a['H'].append(1)\n            elif ss == 'E': dict_a['E'].append(1)\n        elif s == 'F':        \n            if ss == 'C': dict_f['C'].append(1)\n            elif ss == 'H': dict_f['H'].append(1)\n            elif ss == 'E': dict_f['E'].append(1)\n        elif s == 'P':        \n            if ss == 'C': dict_p['C'].append(1)\n            elif ss == 'H': dict_p['H'].append(1)\n            elif ss == 'E': dict_p['E'].append(1)\n        elif s == 'S':        \n            if ss == 'C': dict_s['C'].append(1)\n            elif ss == 'H': dict_s['H'].append(1)\n            elif ss == 'E': dict_s['E'].append(1)\n                \n                \nfor k in dict_a.keys(): dict_a[k] = sum(dict_a[k])\nfor k in dict_f.keys(): dict_f[k] = sum(dict_f[k])\nfor k in dict_p.keys(): dict_p[k] = sum(dict_p[k])\nfor k in dict_s.keys(): dict_s[k] = sum(dict_s[k])\nprint('dict_a:  ',dict_a)\nprint('dict_f:  ',dict_f)\nprint('dict_p:  ',dict_p)\nprint('dict_s:  ',dict_s)\n\n\nplt.figure(figsize=(14,4));\nplt.subplot(1,2,1);\nsns.barplot(x=list(dict_a.keys()),y=list(dict_a.values()),color='gray');\nplt.title('Secondary Structure character counts for aminoacid A');\nplt.subplot(1,2,2);\nsns.barplot(x=list(dict_f.keys()),y=list(dict_f.values()),color='gray');\nplt.title('Secondary Structure character counts for aminoacid F');\nplt.figure(figsize=(14,4));\nplt.subplot(1,2,1);\nsns.barplot(x=list(dict_p.keys()),y=list(dict_p.values()),color='gray');\nplt.title('Secondary Structure character counts for aminoacid P');\nplt.subplot(1,2,2);\nsns.barplot(x=list(dict_s.keys()),y=list(dict_s.values()),color='gray');\nplt.title('Secondary Structure character counts for aminoacid S');","b9d6e363":"def ngrams(seq,n=3):\n    return ([seq[i:i+n] for i in range(len(seq)-n+1)])\n\ndf['seqss']=df['seq'].apply(ngrams)\ndf.head()","6c5377f0":"# function to calculate maximum length\n# we could simply use df['len'].max(), but let's compute it just in case\ndef max_length(series):\n    l = []\n    [l.append(len(s)) for s in series]\n    return max(l)\n\nmaxlen = max_length(df['seq'])\nmaxlen","eecafee7":"# tokenize, then pad sequences into uniform length\nfrom keras.preprocessing import text, sequence\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.utils import to_categorical\n\ntok_x = Tokenizer(lower=False)\ntok_x.fit_on_texts(df['seqss'].values)\nx = tok_x.texts_to_sequences(df['seqss'].values)\nx = sequence.pad_sequences(x, maxlen=maxlen, padding='post')\n\ntok_y = Tokenizer(char_level=True)\ntok_y.fit_on_texts(df['sst3'].values)\ny = tok_y.texts_to_sequences(df['sst3'].values)\ny = sequence.pad_sequences(y, maxlen=maxlen, padding='post')\ny = to_categorical(y)\n\nx.shape, y.shape","a02a5416":"# split data into train and test sets \nfrom sklearn.model_selection import train_test_split\n\nx_tr, x_ts, y_tr, y_ts = train_test_split(x,y,test_size=0.3, random_state=33)\nx_tr.shape, y_tr.shape, x_ts.shape, y_ts.shape","9cec162d":"#import libraries and calculate hyperparameters about train-data dimensions\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, Dense, TimeDistributed, Bidirectional, GRU, LSTM\nn_ngrams = len(tok_x.word_index) + 1\nn_tags = len(tok_y.word_index) + 1","8aa7ef96":"gru = Sequential()\ngru.add(Embedding(input_dim=n_ngrams,output_dim=x.shape[0]\/\/12,input_length=maxlen))\ngru.add(Bidirectional(GRU(x.shape[0]\/\/24,return_sequences=True, recurrent_dropout=0.1)))\ngru.add(TimeDistributed(Dense(n_tags,input_dim=y.shape[2],activation='softmax')))\ngru.summary()","9b233465":"gru.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['acc'])\ngru.fit(x_tr,y_tr,epochs=15,validation_split=0.1,verbose=1)","ff74b36b":"#regular accuracy\nevals = gru.evaluate(x_ts,y_ts)\nprint('accuracy:', evals[1])","158d3ec8":"# q3 accuracy\nfrom keras import backend  as K\nimport tensorflow as tf\n\n#q3_acc func taken almost verbatim from helmehelmuto, special thanks\ndef q3_acc(y_true, y_pred):\n    y = tf.argmax(y_true, axis=-1)\n    y_ = tf.argmax(y_pred, axis=-1)\n    mask = tf.greater(y, 0)\n    q3 = K.cast(K.equal(tf.boolean_mask(y, mask), tf.boolean_mask(y_, mask)), K.floatx())\n    return sum(q3) \/ q3.shape[0]\n\npreds = gru.predict(x_ts)\nprint(q3_acc(y_ts,preds))","b0ccf030":"lstm = Sequential()\nlstm.add(Embedding(input_dim=n_ngrams,output_dim=x.shape[0]\/\/12,input_length=maxlen))\nlstm.add(Bidirectional(LSTM(x.shape[0]\/\/24,return_sequences=True, recurrent_dropout=0.1)))\nlstm.add(TimeDistributed(Dense(n_tags,input_dim=y.shape[2],activation='softmax')))\nlstm.summary()","8fd07a14":"lstm.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['acc'])\nlstm.fit(x_tr,y_tr,epochs=10,validation_split=0.1,verbose=1)","fcf3d9c7":"#regular accuracy\nevals = lstm.evaluate(x_ts,y_ts)\nprint('accuracy:', evals[1])","d7d9d96e":"# q3 accuracy\nfrom keras import backend  as K\nimport tensorflow as tf\n\n\n#q3_acc func taken almost verbatim from helmehelmuto, special thanks\ndef q3_acc(y_true, y_pred):\n    y = tf.argmax(y_true, axis=-1)\n    y_ = tf.argmax(y_pred, axis=-1)\n    mask = tf.greater(y, 0)\n    q3 = K.cast(K.equal(tf.boolean_mask(y, mask), tf.boolean_mask(y_, mask)), K.floatx())\n    return sum(q3) \/ q3.shape[0]\n\npreds = lstm.predict(x_ts)\nprint(q3_acc(y_ts,preds))","557bee0c":"Now we prepare the data representations for the models. We will first break the sequences into 3-grams, then apply a tokenizer.\n\nThe first layer of the neural networks will be an embedding layer, thus optimizing the data representation by making its final step learnable.","a447bf5b":"### Bidirectional LSTM","df83b7cd":"We will remove entries with more than 30% asterisks (the threshold in the function is configurable)","98436de7":"From the above barplots we see that although all the examined amino-acids are biased towards the sst3 character C (some more heavily than others), they often map to the other two sst3 characters. What does this mean?\n\nIt means that *protein secondary structure depends not on individual amino acids, but on the sequential order of the amino acids*.\n\nIt means we will use sequential models for the predictions, namely, RNNs.","e1d491a1":"Special thanks goes to *helmehelmuto* for the ideas in his notebook: [https:\/\/www.kaggle.com\/helmehelmuto\/secondary-structure-prediction-with-keras]","e6d5d25a":"As noted earlier, each amino acid (20 characters) corresponds to one sst3 character (3 characters).\n\nWe will arbitrarily pick 4 of the 20 amino-acids and plot the distributions of sst3 characters corresponding to each of the 4 amino-acids in the dataset.","938eb1ba":"## Classification with ngrams and RNNs","b58eb54e":"A protein's secondary structure is represented as a string of character of length equal to the length of the amino-acid sequence. These characters signify whether the protein at that point (amino-acid) folds, forms a loop, forms a bridge, or a helix, etc. Strictly speaking, there are eight characters used for secondary structure representations, but they are usually grouped into three:\n\n**E** signifies \u03b2-strands and \u03b2-bridges\n\n**H** signifies helices, and\n\n**C** signifies loops, bends, turns, and irregularcharacters.\n\nGiven amino-acid sequences (comprised of 20 amino-acids) and corresponding secondary structures (comprised of the 3 characters mentioned above), we want to train a model to infere secondary structure from amino-acid sequence.","856d2118":"We are left with 5425 sequences of length up to 40. Let's plot a histogram to see the distributions of the lengths","a866960a":"### Bidirectional GRU","ba5ced95":"The datasets contains many duplicates, so let's drop them. \n\nWe will also drop un-needed columns, as well as samples that contain unspecified ('non-standard') amino-acids.\n\nFinally, since this project is a demonstration, we will only work with protein sequences that are up to 40 aa long, to keep the demo computationally efficient.","301c5022":"# Protein Secondary Structure Prediction","6a22fc8d":"## Data Preparation and Exploration"}}