{"cell_type":{"030c817c":"code","3ab76782":"code","7147de54":"code","756a9196":"code","40d25ed8":"code","4e36784c":"code","4f25bbf3":"code","03c5051c":"code","0009afd4":"code","e72d0a1c":"code","3eb385d9":"code","0a1f7bd7":"code","ffbdac30":"code","5dfa5348":"code","2681a143":"code","c83253ab":"code","d32d3884":"code","e5bcea23":"code","1e5fc9a0":"code","6886292b":"code","696a93c9":"code","82f3d957":"code","8492f8cc":"code","1789526f":"code","e9ea5646":"code","0bea1167":"code","c69f9938":"code","140aceb7":"code","96a6846e":"code","62604257":"code","155406c0":"code","8b466fcc":"code","fd313dcf":"code","012620be":"code","912a3c5a":"code","c3b0a9cc":"code","49328a7f":"code","e0f3b316":"code","7d313ecc":"code","6a910bdd":"code","858ba065":"code","4dfda012":"code","2a441448":"code","973fa38b":"code","6087b157":"code","33eb6e36":"code","bdea9d94":"code","669dade8":"code","43337268":"code","e329fa97":"code","2c2b8aff":"code","4cef317c":"code","e75f1ec4":"code","d4ff42cd":"code","09dbbc62":"code","02526053":"code","bedf80c6":"code","f5082b86":"code","fcfa8e1c":"code","7572913c":"code","5a7ff4ff":"code","fc91ba1d":"code","541b8874":"code","558f4c16":"code","6da88265":"code","a50938ad":"code","386cadff":"code","4424b48d":"code","0d951909":"code","f40ddfca":"code","07ce7eb8":"code","d3cca4a3":"code","e51edaab":"code","50e211c4":"code","ec7fefc2":"code","02d425db":"code","7f441045":"code","a53d6796":"code","f14c1017":"markdown","232624cd":"markdown","d2cb9b1e":"markdown","88a62c03":"markdown","6bebddf4":"markdown","6ebc9021":"markdown","7d2424db":"markdown","91b66b4e":"markdown","b769e55a":"markdown","9e3ae3cf":"markdown","3ba02b85":"markdown","bac7e1be":"markdown","aff57448":"markdown","1e87bf86":"markdown","7b842216":"markdown","ea01bd30":"markdown","83d33cfa":"markdown","d523bada":"markdown","5c2ccda1":"markdown","9dbf0d63":"markdown","2941b433":"markdown","254ed6a5":"markdown","e5c696f7":"markdown","2df28d3d":"markdown","404927ae":"markdown","4239fee0":"markdown","d0d53fe4":"markdown","e69153a6":"markdown","8f6a357c":"markdown","b9d9993c":"markdown","b8e06fd3":"markdown","5c766e7e":"markdown","a0a06127":"markdown","41f6707e":"markdown","462770b1":"markdown","e34d5f82":"markdown","2893a221":"markdown","63a18b3d":"markdown","42b0843f":"markdown"},"source":{"030c817c":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","3ab76782":"pd.set_option('display.max_columns', None)\ndf = pd.read_csv('..\/input\/car-price-prediction\/CarPrice_Assignment.csv')\ndf","7147de54":"df.head()","756a9196":"df.tail()","40d25ed8":"df.shape","4e36784c":"df.size","4f25bbf3":"df.dtypes","03c5051c":"df.columns","0009afd4":"df.info()","e72d0a1c":"df.describe()","3eb385d9":"df.isnull().sum()","0a1f7bd7":"df.duplicated().sum()","ffbdac30":"df.skew()","5dfa5348":"df.corr()","2681a143":"df.drop(['car_ID','CarName'],axis = 1,inplace = True)\ndf","c83253ab":"cat = ['symboling','fueltype', 'aspiration', 'doornumber',\n       'carbody', 'drivewheel', 'enginelocation','enginetype','cylindernumber','fuelsystem']\nprint('No. of categorical columns are:',len(cat))","d32d3884":"num = []\nfor i in df.columns:\n    if i not in cat:\n        num.append(i)\nprint('No. of numerical\/float columns are:',len(num))","e5bcea23":"df['symboling'].value_counts()","1e5fc9a0":"fig, ax = plt.subplots(1, 2, figsize=(15,8))\nx=df['symboling'].value_counts().index\ny=df['symboling'].value_counts().values.tolist()\ndata = df.groupby(\"symboling\").size()\nsns.set(style=\"dark\", color_codes=True)\npal = sns.color_palette(\"magma\", len(data))\nrank = data.argsort().argsort() \nsns.barplot(x=x,y=y,palette=np.array(pal[::-1])[rank],ax = ax[0])\nfor p in ax[0].patches:\n        ax[0].annotate('{:.0f}'.format(p.get_height()), (p.get_x()+0.3, p.get_height()),\n                    ha='center', va='bottom',\n                    color= 'black')\nax[0].set_xlabel('Types of Symboling', weight='semibold', fontname = 'monospace')\n_, _, autotexts= ax[1].pie(y, labels = x, colors = pal, autopct='%1.1f%%',\n        explode=[0.03 for i in df['symboling'].value_counts().index])\nfor autotext in autotexts:\n    autotext.set_color('white')\nplt.legend(bbox_to_anchor=(1, 1))\nplt.suptitle ('Types of Symboling',weight = 'bold')\nplt.show()","6886292b":"df['fueltype'].value_counts()","696a93c9":"fig, ax = plt.subplots(1, 2, figsize=(15,8))\nx=df['fueltype'].value_counts().index\ny=df['fueltype'].value_counts().values.tolist()\ndata = df.groupby(\"fueltype\").size()\nsns.set(style=\"dark\", color_codes=True)\npal = sns.color_palette(\"magma\", len(data))\nrank = data.argsort().argsort() \nsns.barplot(x=x,y=y,palette=np.array(pal[::-1])[rank],ax = ax[0])\nfor p in ax[0].patches:\n        ax[0].annotate('{:.0f}'.format(p.get_height()), (p.get_x()+0.3, p.get_height()),\n                    ha='center', va='bottom',\n                    color= 'black')\nax[0].set_xlabel('fueltype (gas\/diesel)', weight='semibold', fontname = 'monospace')\n_, _, autotexts= ax[1].pie(y, labels = x, colors = pal, autopct='%1.1f%%',\n        explode=[0.03 for i in df['fueltype'].value_counts().index])\nfor autotext in autotexts:\n    autotext.set_color('white')\nplt.legend(bbox_to_anchor=(1, 1))\nplt.suptitle ('fueltype(gas\/diesel)',weight = 'bold')\nplt.show()","82f3d957":"df['aspiration'].value_counts()","8492f8cc":"fig, ax = plt.subplots(1, 2, figsize=(15,8))\nx=df['aspiration'].value_counts().index\ny=df['aspiration'].value_counts().values.tolist()\ndata = df.groupby(\"aspiration\").size()\nsns.set(style=\"dark\", color_codes=True)\npal = sns.color_palette(\"magma\", len(data))\nrank = data.argsort().argsort() \nsns.barplot(x=x,y=y,palette=np.array(pal[::-1])[rank],ax = ax[0])\nfor p in ax[0].patches:\n        ax[0].annotate('{:.0f}'.format(p.get_height()), (p.get_x()+0.3, p.get_height()),\n                    ha='center', va='bottom',\n                    color= 'black')\nax[0].set_xlabel('aspiration (std\/turbo)', weight='semibold', fontname = 'monospace')\n_, _, autotexts= ax[1].pie(y, labels = x, colors = pal, autopct='%1.1f%%',\n        explode=[0.03 for i in df['aspiration'].value_counts().index])\nfor autotext in autotexts:\n    autotext.set_color('white')\nplt.legend(bbox_to_anchor=(1, 1))\nplt.suptitle ('aspiration (std\/turbo)',weight = 'bold')\nplt.show()","1789526f":"df['doornumber'].value_counts()","e9ea5646":"fig, ax = plt.subplots(1, 2, figsize=(15,8))\nx=df['doornumber'].value_counts().index\ny=df['doornumber'].value_counts().values.tolist()\ndata = df.groupby(\"doornumber\").size()\nsns.set(style=\"dark\", color_codes=True)\npal = sns.color_palette(\"magma\", len(data))\nrank = data.argsort().argsort() \nsns.barplot(x=x,y=y,palette=np.array(pal[::-1])[rank],ax = ax[0])\nfor p in ax[0].patches:\n        ax[0].annotate('{:.0f}'.format(p.get_height()), (p.get_x()+0.3, p.get_height()),\n                    ha='center', va='bottom',\n                    color= 'black')\nax[0].set_xlabel('doornumber (four\/two)', weight='semibold', fontname = 'monospace')\n_, _, autotexts= ax[1].pie(y, labels = x, colors = pal, autopct='%1.1f%%',\n        explode=[0.03 for i in df['doornumber'].value_counts().index])\nfor autotext in autotexts:\n    autotext.set_color('white')\nplt.legend(bbox_to_anchor=(1, 1))\nplt.suptitle ('doornumber (four\/two)',weight = 'bold')\nplt.show()","0bea1167":"df['carbody'].value_counts()","c69f9938":"fig, ax = plt.subplots(1, 2, figsize=(15,8))\nx=df['carbody'].value_counts().index\ny=df['carbody'].value_counts().values.tolist()\ndata = df.groupby(\"carbody\").size()\nsns.set(style=\"dark\", color_codes=True)\npal = sns.color_palette(\"magma\", len(data))\nrank = data.argsort().argsort() \nsns.barplot(x=x,y=y,palette=np.array(pal[::-1])[rank],ax = ax[0])\nfor p in ax[0].patches:\n        ax[0].annotate('{:.0f}'.format(p.get_height()), (p.get_x()+0.3, p.get_height()),\n                    ha='center', va='bottom',\n                    color= 'black')\nax[0].set_xlabel('Types of carbody', weight='semibold', fontname = 'monospace')\n_, _, autotexts= ax[1].pie(y, labels = x, colors = pal, autopct='%1.1f%%',\n        explode=[0.03 for i in df['carbody'].value_counts().index])\nfor autotext in autotexts:\n    autotext.set_color('white')\nplt.legend(bbox_to_anchor=(1, 1))\nplt.suptitle ('Types of carbody',weight = 'bold')\nplt.show()","140aceb7":"df['drivewheel'].value_counts()","96a6846e":"fig, ax = plt.subplots(1, 2, figsize=(15,8))\nx=df['drivewheel'].value_counts().index\ny=df['drivewheel'].value_counts().values.tolist()\ndata = df.groupby(\"drivewheel\").size()\nsns.set(style=\"dark\", color_codes=True)\npal = sns.color_palette(\"magma\", len(data))\nrank = data.argsort().argsort() \nsns.barplot(x=x,y=y,palette=np.array(pal[::-1])[rank],ax = ax[0])\nfor p in ax[0].patches:\n        ax[0].annotate('{:.0f}'.format(p.get_height()), (p.get_x()+0.3, p.get_height()),\n                    ha='center', va='bottom',\n                    color= 'black')\nax[0].set_xlabel('Types of drivewheel', weight='semibold', fontname = 'monospace')\n_, _, autotexts= ax[1].pie(y, labels = x, colors = pal, autopct='%1.1f%%',\n        explode=[0.03 for i in df['drivewheel'].value_counts().index])\nfor autotext in autotexts:\n    autotext.set_color('white')\nplt.legend(bbox_to_anchor=(1, 1))\nplt.suptitle ('Types of drivewheel',weight = 'bold')\nplt.show()","62604257":"df['enginelocation'].value_counts()","155406c0":"fig, ax = plt.subplots(1, 2, figsize=(15,8))\nx=df['enginelocation'].value_counts().index\ny=df['enginelocation'].value_counts().values.tolist()\ndata = df.groupby(\"enginelocation\").size()\nsns.set(style=\"dark\", color_codes=True)\npal = sns.color_palette(\"magma\", len(data))\nrank = data.argsort().argsort() \nsns.barplot(x=x,y=y,palette=np.array(pal[::-1])[rank],ax = ax[0])\nfor p in ax[0].patches:\n        ax[0].annotate('{:.0f}'.format(p.get_height()), (p.get_x()+0.3, p.get_height()),\n                    ha='center', va='bottom',\n                    color= 'black')\nax[0].set_xlabel('enginelocation (front\/rear)', weight='semibold', fontname = 'monospace')\n_, _, autotexts= ax[1].pie(y, labels = x, colors = pal, autopct='%1.1f%%',\n        explode=[0.03 for i in df['enginelocation'].value_counts().index])\nfor autotext in autotexts:\n    autotext.set_color('white')\nplt.legend(bbox_to_anchor=(1, 1))\nplt.suptitle ('enginelocation (front\/rear)',weight = 'bold')\nplt.show()","8b466fcc":"df['enginetype'].value_counts()","fd313dcf":"fig, ax = plt.subplots(1, 2, figsize=(15,8))\nx=df['enginetype'].value_counts().index\ny=df['enginetype'].value_counts().values.tolist()\ndata = df.groupby(\"enginetype\").size()\nsns.set(style=\"dark\", color_codes=True)\npal = sns.color_palette(\"magma\", len(data))\nrank = data.argsort().argsort() \nsns.barplot(x=x,y=y,palette=np.array(pal[::-1])[rank],ax = ax[0])\nfor p in ax[0].patches:\n        ax[0].annotate('{:.0f}'.format(p.get_height()), (p.get_x()+0.3, p.get_height()),\n                    ha='center', va='bottom',\n                    color= 'black')\nax[0].set_xlabel('Types of engine', weight='semibold', fontname = 'monospace')\n_, _, autotexts= ax[1].pie(y, labels = x, colors = pal, autopct='%1.1f%%',\n        explode=[0.03 for i in df['enginetype'].value_counts().index])\nfor autotext in autotexts:\n    autotext.set_color('white')\nplt.legend(bbox_to_anchor=(1, 1))\nplt.suptitle ('Types of engine',weight = 'bold')\nplt.show()","012620be":"df['cylindernumber'].value_counts()","912a3c5a":"fig, ax = plt.subplots(1, 2, figsize=(15,8))\nx=df['cylindernumber'].value_counts().index\ny=df['cylindernumber'].value_counts().values.tolist()\ndata = df.groupby(\"cylindernumber\").size()\nsns.set(style=\"dark\", color_codes=True)\npal = sns.color_palette(\"magma\", len(data))\nrank = data.argsort().argsort() \nsns.barplot(x=x,y=y,palette=np.array(pal[::-1])[rank],ax = ax[0])\nfor p in ax[0].patches:\n        ax[0].annotate('{:.0f}'.format(p.get_height()), (p.get_x()+0.3, p.get_height()),\n                    ha='center', va='bottom',\n                    color= 'black')\nax[0].set_xlabel('Number of Cylinder(s)', weight='semibold', fontname = 'monospace')\n_, _, autotexts= ax[1].pie(y, labels = x, colors = pal, autopct='%1.1f%%',\n        explode=[0.03 for i in df['cylindernumber'].value_counts().index])\nfor autotext in autotexts:\n    autotext.set_color('white')\nplt.legend(bbox_to_anchor=(1, 1))\nplt.suptitle ('Number of Cylinder(s)',weight = 'bold')\nplt.show()","c3b0a9cc":"df['fuelsystem'].value_counts()","49328a7f":"fig, ax = plt.subplots(1, 2, figsize=(15,8))\nx=df['fuelsystem'].value_counts().index\ny=df['fuelsystem'].value_counts().values.tolist()\ndata = df.groupby(\"fuelsystem\").size()\nsns.set(style=\"dark\", color_codes=True)\npal = sns.color_palette(\"magma\", len(data))\nrank = data.argsort().argsort() \nsns.barplot(x=x,y=y,palette=np.array(pal[::-1])[rank],ax = ax[0])\nfor p in ax[0].patches:\n        ax[0].annotate('{:.0f}'.format(p.get_height()), (p.get_x()+0.3, p.get_height()),\n                    ha='center', va='bottom',\n                    color= 'black')\nax[0].set_xlabel('Types of fuelsystem', weight='semibold', fontname = 'monospace')\n_, _, autotexts= ax[1].pie(y, labels = x, colors = pal, autopct='%1.1f%%',\n        explode=[0.03 for i in df['fuelsystem'].value_counts().index])\nfor autotext in autotexts:\n    autotext.set_color('white')\nplt.legend(bbox_to_anchor=(1, 1))\nplt.suptitle ('Types of fuelsystem',weight = 'bold')\nplt.show()","e0f3b316":"sns.set_theme(rc = {'figure.dpi': 250, 'axes.labelsize': 7, \n                    'axes.facecolor': '#f0eee9', 'grid.color': '#fffdfa', \n                    'figure.facecolor': '#e8e6e1'}, font_scale = 0.55)\n\nfig, ax = plt.subplots(5, 2, figsize = (6.5, 10))\n\nfor indx, (column, axes) in list(enumerate(list(zip(cat, \n                                                    ax.flatten())))):\n    \n    sns.countplot(ax = axes, x = df[column], hue = df['doornumber'], \n                  palette = 'magma', alpha = 0.8)\n    \nelse:\n    [axes.set_visible(False) for axes in ax.flatten()[indx + 1:]]\n    \naxes_legend = ax.flatten()\n\naxes_legend[1].legend(title = 'doornumber', loc = 'upper right')\naxes_legend[2].legend(title = 'doornumber', loc = 'upper right')\n\nplt.tight_layout()\nplt.show()","7d313ecc":"sns.set_theme(rc = {'figure.dpi': 250, 'axes.labelsize': 7, \n                    'axes.facecolor': '#f0eee9', 'grid.color': '#fffdfa', \n                    'figure.facecolor': '#e8e6e1'}, font_scale = 0.55)\n\nfig, ax = plt.subplots(5, 2, figsize = (6.5, 10))\n\nfor indx, (column, axes) in list(enumerate(list(zip(cat, \n                                                    ax.flatten())))):\n    \n    sns.countplot(ax = axes, x = df[column], hue = df['fueltype'], \n                  palette = 'magma', alpha = 0.8)\n    \nelse:\n    [axes.set_visible(False) for axes in ax.flatten()[indx + 1:]]\n    \naxes_legend = ax.flatten()\n\naxes_legend[1].legend(title = 'fueltype', loc = 'upper right')\naxes_legend[2].legend(title = 'fueltype', loc = 'upper right')\n\nplt.tight_layout()\nplt.show()","6a910bdd":"sns.set_theme(rc = {'figure.dpi': 120, 'axes.labelsize': 8, \n                    'axes.facecolor': '#f0eee9', 'grid.color': '#fffdfa', \n                    'figure.facecolor': '#e8e6e1'}, font_scale = 0.65)\n\nfig, ax = plt.subplots(14, 1, figsize = (7, 30))\n\nfor indx, (column, axes) in list(enumerate(list(zip(num, ax.flatten())))):\n    \n    sns.scatterplot(ax = axes, y = df[column].index, x = df[column],hue = df['doornumber'],\n                    palette = 'magma', alpha = 0.8)\n    \nelse:\n    [axes.set_visible(False) for axes in ax.flatten()[indx + 1:]]\n    \nplt.tight_layout()\nplt.show()","858ba065":"sns.set_theme(rc = {'figure.dpi': 120, 'axes.labelsize': 8, \n                    'axes.facecolor': '#f0eee9', 'grid.color': '#fffdfa', \n                    'figure.facecolor': '#e8e6e1'}, font_scale = 0.65)\n\nfig, ax = plt.subplots(14, 1, figsize = (6, 35))\n\nfor indx, (column, axes) in list(enumerate(list(zip(num, ax.flatten())))):\n    \n    sns.histplot(ax = axes, x = df[column],hue = df['doornumber'],\n                    palette = 'magma', alpha = 0.8, multiple = 'stack')\n    \n    legend = axes.get_legend() # sns.hisplot has some issues with legend\n    handles = legend.legendHandles\n    legend.remove()\n    axes.legend(handles, ['0', '1'], title = 'Survived', loc = 'upper right')\n    Quantiles = np.quantile(df[column], [0, 0.25, 0.50, 0.75, 1])\n    \n    for q in Quantiles: axes.axvline(x = q, linewidth = 0.5, color = 'r')\n        \nplt.tight_layout()\nplt.show()","4dfda012":"sns.set_theme(rc = {'figure.dpi': 250, 'axes.labelsize': 7, \n                    'axes.facecolor': '#f0eee9', 'grid.color': '#fffdfa', \n                    'figure.facecolor': '#e8e6e1'}, font_scale = 0.55)\n\nfig, ax = plt.subplots(5, 2, figsize = (6.5, 10))\n\nfor indx, (column, axes) in list(enumerate(list(zip(cat, ax.flatten())))):\n    \n    sns.violinplot(ax = axes, x = df[column], \n                   y = df['price'],\n                   scale = 'width', linewidth = 0.5, \n                   palette = 'magma', inner = None)\n    \n    plt.setp(axes.collections, alpha = 0.3)\n    \n    sns.stripplot(ax = axes, x = df[column], \n                  y = df['price'],\n                  palette = 'magma', alpha = 0.9, \n                  s = 1.5, jitter = 0.07)\n    sns.pointplot(ax = axes, x = df[column],\n                  y = df['price'],\n                  color = '#ff5736', scale = 0.25,\n                  estimator = np.mean, ci = 'sd',\n                  errwidth = 0.5, capsize = 0.15, join = True)\n    \n    plt.setp(axes.lines, zorder = 100)\n    plt.setp(axes.collections, zorder = 100)\n    \nelse:\n    [axes.set_visible(False) for axes in ax.flatten()[indx + 1:]]\n    \nplt.tight_layout()\nplt.show()","2a441448":"sns.set(rc={\"axes.facecolor\":\"#a1c45a\" , \"axes.grid\" : False})\nplt.figure(figsize=(11,9))\nplt.gcf().text(.51, .84, \"Box Plot\", fontsize = 40, color='Black' ,ha='center', va='center')\nsns.boxenplot(x=df['symboling'] , y = df['price'],palette=\"Set1\")\nplt.show()","973fa38b":"# Facet along the columns to show a categorical variable using \"col\" parameter\nplt.figure(figsize=(11,9))\nsns.catplot(x=\"symboling\" , y = \"price\", hue= \"fueltype\",\n            col=\"doornumber\", kind=\"boxen\",palette=\"Set2\" , height=8, aspect=1 ,data=df)\nplt.show()","6087b157":"# Change the size of outlier markers\nsns.set(rc={\"axes.facecolor\":\"#b0deff\",\"axes.grid\":False,\n            'xtick.labelsize':15,'ytick.labelsize':15,\n            'axes.labelsize':20,'figure.figsize':(20.0, 9.0)})\nsns.boxplot(x= df['symboling'] , y= df['price'] , \n            width=.7 , hue=df['doornumber'] , \n            palette= {\"two\":'#FFB74D' , \"four\":'#9CCC65'} ,linewidth = 2  , fliersize= 15)\nsns.despine(left=True)\nplt.xticks(rotation=45)\nplt.show()","33eb6e36":"plt.figure(figsize=(16,11))\nsns.set(rc={\"axes.facecolor\":\"#b0deff\",\"axes.grid\":False,\n            'xtick.labelsize':15,'ytick.labelsize':15,\n            'axes.labelsize':20,'figure.figsize':(20.0, 9.0)})\nparams = dict(data=df ,x = df.symboling ,y = df.price ,hue=df.doornumber,dodge=True)\nsns.stripplot(**params , size=8,jitter=0.35,palette=['#33FF66','#FF6600'],edgecolor='black',linewidth=1)\nsns.boxplot(**params ,palette=['#BDBDBD','#E0E0E0'],linewidth=6)\nplt.show()","bdea9d94":"# Drawing stripplot on top of a box plot\nplt.figure(figsize=(11,8))\nsns.stripplot(x=df.symboling, y = df.price , jitter=True , color=\"black\" , size=4)\nsns.boxplot(x=df.symboling ,palette=\"Set1\", y = df.price , color='black')\nplt.show()","669dade8":"df1 = df[df['drivewheel'].isin(['rwd', 'fwd', '4wd'])]\ndf1.head()","43337268":"# Plot a subset of variables\ng = sns.PairGrid(df1 , hue='drivewheel' ,x_vars=[\"carlength\" , \"carwidth\"],y_vars=[\"carlength\" , \"carwidth\"],\n                 height=6, aspect=1)\ng = g.map_offdiag(plt.scatter , edgecolor=\"w\", s=130)\ng = g.map_diag(plt.hist , edgecolor ='w', linewidth=2)\ng = g.add_legend()\nplt.show()","e329fa97":"# Plot a subset of variables\ng = sns.PairGrid(df1 , hue='drivewheel' ,vars=[\"carlength\" , \"carwidth\",\"carheight\"],height=5, aspect=1)\ng = g.map_offdiag(plt.scatter , edgecolor=\"w\", s=130)\ng = g.map_diag(plt.hist , edgecolor ='w', linewidth=2)\ng = g.add_legend()\nplt.show()","2c2b8aff":"# Plot a subset of variables\ng = sns.PairGrid(df1 , hue='drivewheel' ,vars=[\"horsepower\",\"peakrpm\",\"price\"],height=5, aspect=1)\ng = g.map_offdiag(plt.scatter , edgecolor=\"w\", s=130)\ng = g.map_diag(plt.hist , edgecolor ='w', linewidth=2)\ng = g.add_legend()\nplt.show()","4cef317c":"''' Plot a Shifted Correlation Matrix '''\n# Diagonal correlation is always unity & less relevant, shifted variant shows only relevant cases\ndef corrMat(df,id=False):\n    \n    corr_mat = df.corr().round(2)\n    f, ax = plt.subplots(figsize=(12,7))\n    mask = np.triu(np.ones_like(corr_mat, dtype=bool))\n    mask = mask[1:,:-1]\n    corr = corr_mat.iloc[1:,:-1].copy()\n    sns.heatmap(corr,mask=mask,vmin=-0.3,vmax=0.3,center=0, \n                cmap='RdPu_r',square=False,lw=2,annot=True,cbar=False)\n#     bottom, top = ax.get_ylim() \n#     ax.set_ylim(bottom + 0.5, top - 0.5) \n    ax.set_title('Shifted Linear Correlation Matrix')\n    \ncorrMat(df)","e75f1ec4":"def LABEL_ENCODING(c1):\n    from sklearn import preprocessing\n    label_encoder = preprocessing.LabelEncoder()\n    df[c1]= label_encoder.fit_transform(df[c1])\n    df[c1].unique()","d4ff42cd":"LABEL_ENCODING(\"fueltype\")\nLABEL_ENCODING(\"aspiration\")\nLABEL_ENCODING(\"doornumber\")\nLABEL_ENCODING(\"carbody\")\nLABEL_ENCODING(\"drivewheel\")\nLABEL_ENCODING(\"enginelocation\")\nLABEL_ENCODING(\"enginetype\")\nLABEL_ENCODING(\"cylindernumber\")\nLABEL_ENCODING(\"fuelsystem\")\ndf","09dbbc62":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import classification_report,confusion_matrix\nscaler = StandardScaler()\nscaler.fit(df.drop(['price'],axis = 1))\nscaled_features = scaler.transform(df.drop('price',axis = 1))","02526053":"X = scaled_features\nY = df['price']\nX= pd.DataFrame(data=X,columns = df.drop(columns=['price']).columns)","bedf80c6":"X.corr()","f5082b86":"## pair of independent ariables with correlation greater than 0.5 \nk=X.corr()\nz=[[str(i),str(j)] for i in k.columns for j in k.columns if(k.loc[i,j]>abs(0.5))&(i!=j)]\nz,len(z)","fcfa8e1c":"#importing varience inflation factor from the stats model\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nvif_data=X\n## calculating VIF for every column\nVIF=pd.Series([variance_inflation_factor(vif_data.values,i) for i in range(vif_data.shape[1])],index=vif_data.columns)\nVIF","7572913c":"def MC_remover(data):\n    vif=pd.Series([variance_inflation_factor(data.values,i)for i in range(data.shape[1])],index=data.columns)\n    if vif.max()>5:\n        print(vif[vif == vif.max()].index[0],'has been removed')\n        data = data.drop(columns=[vif[vif==vif.max()].index[0]])\n        return data\n    else:\n        print(\"No multicollinearity present anymore\")\n        return data","5a7ff4ff":"for i in range(10):\n    vif_data=MC_remover(vif_data)\nvif_data.head()","fc91ba1d":"## calculating VIF for remaining columns\nVIF=pd.Series([variance_inflation_factor(vif_data.values,i) for i in range(vif_data.shape[1])],index=vif_data.columns)\nVIF,len(vif_data.columns)","541b8874":"from sklearn.model_selection import train_test_split\nX = vif_data\nY = df['price']\nX_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size = 0.3,random_state=44)","558f4c16":"from sklearn.linear_model import LinearRegression\nreg=LinearRegression(normalize =True)\nreg.fit(X_train,Y_train)","6da88265":"reg.coef_","a50938ad":"pred = reg.predict(X_test)\npred","386cadff":"# Recover default matplotlib settings\nplt.rcParams.update(plt.rcParamsDefault)\n%matplotlib inline\nplt.figure(figsize=(9,9))\nsns.regplot(x = Y_test,y = pred, color='#FF6600')\nplt.xlabel('Y Test (True Values)')\nplt.ylabel('Predicted values')\nplt.show()","4424b48d":"reg.score(X_test,Y_test)","0d951909":"sns.displot(Y_test-pred,bins = 50,kde = True)\nplt.show()","f40ddfca":"cdf = pd.DataFrame(reg.coef_,X.columns,columns = ['coef'])\ncdf","07ce7eb8":"coefficients_table=pd.DataFrame({'column':X_train.columns,\n                                'coefficients':reg.coef_})\ncoefficients_table=coefficients_table.sort_values(by=\"coefficients\")","d3cca4a3":"plt.figure(figsize=(8,6),dpi=120)\nx=coefficients_table['column']\ny=coefficients_table['coefficients']\nplt.barh(x,y)\nplt.xlabel(\"coefficients\")\nplt.ylabel(\"variables\")\nplt.title(\"Normalized coefficients\")","e51edaab":"import xgboost as xg\n# create an xgboost regression model\nmodel = xg.XGBRegressor(n_estimators=1000, max_depth=7, eta=0.1, subsample=0.7, colsample_bytree=0.8)","50e211c4":"model.fit(X_train, Y_train)\npred = model.predict(X_test)\nprint(model.score(X_test, Y_test))","ec7fefc2":"from sklearn.linear_model import Ridge,Lasso,ElasticNet\nR=Ridge(alpha=0.1)\nR.fit(X_train,Y_train)\ny_pred1=R.predict(X_test)","02d425db":"from sklearn.metrics import r2_score,mean_squared_error\nprint(\"R2 score\",r2_score(Y_test,y_pred1))\nprint(\"RMSE\",np.sqrt(mean_squared_error(Y_test,y_pred1)))","7f441045":"model_lasso = Lasso(alpha=0.1)\nmodel_lasso.fit(X_train, Y_train) \npred_test_lasso= model_lasso.predict(X_test)\nprint(\"RMSE\",np.sqrt(mean_squared_error(Y_test,pred_test_lasso))) \nprint(\"R2 score\",r2_score(Y_test, pred_test_lasso))","a53d6796":"model_enet = ElasticNet(alpha = 0.1)\nmodel_enet.fit(X_train, Y_train) \npred_test_enet= model_enet.predict(X_test)\nprint(\"RMSE\",np.sqrt(mean_squared_error(Y_test,pred_test_enet)))\nprint(\"R2 score\",r2_score(Y_test, pred_test_enet))","f14c1017":"<h1 style=\"color:#fce444;font-size:30px;font-family:Georgia;text-align:center;\"><strong>\u270f\ufe0fModeling and Prediction <strong style=\"color:black;font-size:25px;font-family:Georgia;\">using XGBoost Regression <strong style=\"color:#fce444;font-size:25px;font-family:Georgia;\"><strong style=\"color:black;font-size:30px;font-family:Georgia;\">:- <\/strong><\/strong><\/strong><\/strong><\/h1>","232624cd":"Gradient boosting refers to a class of ensemble machine learning algorithms that can be used for classification or regression predictive modeling problems.\n\nEnsembles are constructed from decision tree models. Trees are added one at a time to the ensemble and fit to correct the prediction errors made by prior models. This is a type of ensemble machine learning model referred to as boosting.\n\nModels are fit using any arbitrary differentiable loss function and gradient descent optimization algorithm. This gives the technique its name, \u201cgradient boosting,\u201d as the loss gradient is minimized as the model is fit, much like a neural network.","d2cb9b1e":"<h1 style=\"color:#fce444;font-size:30px;font-family:Georgia;text-align:center;\"><strong>\u270f\ufe0fImporting <strong style=\"color:black;font-size:25px;font-family:Georgia;\">Libraries <strong style=\"color:#fce444;font-size:25px;font-family:Georgia;\"><strong style=\"color:black;font-size:30px;font-family:Georgia;\">:- <\/strong><\/strong><\/strong><\/strong><\/h1>","88a62c03":"<h1 style=\"color:#fce444;font-size:30px;font-family:Georgia;text-align:center;\"><strong>\u270f\ufe0fModeling and Prediction <strong style=\"color:black;font-size:25px;font-family:Georgia;\">using Elastic Regression <strong style=\"color:#fce444;font-size:25px;font-family:Georgia;\"><strong style=\"color:black;font-size:30px;font-family:Georgia;\">:- <\/strong><\/strong><\/strong><\/strong><\/h1>","6bebddf4":"<h1 style=\"color:#fce444;font-size:30px;font-family:Georgia;text-align:center;\"><strong>\u270f\ufe0fBasic <strong style=\"color:black;font-size:25px;font-family:Georgia;\">Data Cleaning <strong style=\"color:#fce444;font-size:25px;font-family:Georgia;\"><strong style=\"color:black;font-size:30px;font-family:Georgia;\">:- <\/strong><\/strong><\/strong><\/strong><\/h1>","6ebc9021":"# Regularization Techniques\n\n* Linear regression works by selecting coefficients for each independent variable that minimizes a loss function. However, if the coefficients are too large, it can lead to model over-fitting on the training dataset. Such a model will not generalize well on the unseen data. To overcome this shortcoming, we do regularization which penalizes large coefficients. The following are the regularization algorithms.\n\n#### Pros of Regularization\n\n--> We can use a regularized model to reduce the dimensionality of the training dataset. Dimensionality reduction is important because of three main reasons:\n\n--> Prevents Overfitting: A high-dimensional dataset having too many features can sometimes lead to overfitting (model captures both real and random effects).\n\n--> Simplicity: An over-complex model having too many features can be hard to interpret especially when features are correlated with each other.\n\n--> Computational Efficiency: A model trained on a lower dimensional dataset is computationally efficient (execution of algorithm requires less computational time).\n\n\n#### Cons of Regularization\n\n--> Regularization leads to dimensionality reduction, which means the machine learning model is built using a lower dimensional dataset. This generally leads to a high bias errror.\n\n--> If regularization is performed before training the model, a perfect balance between bias-variance tradeoff must be used.\n\n## 1) Ridge Regression\n\n* Ridge regression is an extension of linear regression where the loss function is modified to minimize the complexity of the model. This modification is done by adding a penalty parameter that is equivalent to the square of the magnitude of the coefficients.\n\n     #### Loss function = OLS + alpha * summation (squared coefficient values)\n\n\n* In the above loss function, alpha is the parameter we need to select. A low alpha value can lead to over-fitting, whereas a high alpha value can lead to under-fitting.\n\n* A general linear or polynomial regression will fail if there is high collinearity between the independent variables, so to solve such problems, Ridge regression can be used.\n* It helps to solve the problems if we have more parameters than samples.\n\n#### Pros\n\n--> Avoids overfitting a model.\n\n--> The ridge estimator is preferably good at improving the least-squares estimate when there is multicollinearity.\n\n\n#### Cons\n\n--> They include all the predictors in the final model.\n\n--> They are unable to perform feature selection.\n\n--> They shrink the coefficients towards zero.\n\n--> They trade the variance for bias.\n\n## 2) Lasso Regression\n\n* Lasso regression, or the Least Absolute Shrinkage and Selection Operator, is also a modification of linear regression. In Lasso, the loss function is modified to minimize the complexity of the model by limiting the sum of the absolute values of the model coefficients (also called the l1-norm).\n\n* The loss function for Lasso Regression can be expressed as below:\n\n     #### Loss function = OLS + alpha * summation (absolute values of the magnitude of the coefficients)\n     \n\n* In the above loss function, alpha is the penalty parameter we need to select. Using an l1 norm constraint forces some weight values to zero to allow other coefficients to take non-zero values.\n* Since it takes absolute values, hence, it can shrink the slope to 0, whereas Ridge Regression can only shrink it near to 0.\n\n#### Pros\n--> Select features, by shrinking co-efficient towards zero.\n\n--> Avoids over fitting\n\n\n#### Cons\n--> Selected features will be highly biased.\n\n--> For n<<p (n-number of data points, p-number of features), LASSO selects at most n features.\n\n--> LASSO will select only one feature from a group of correlated features, the selection is arbitrary in nature.\n\n--> For different boot strapped data, the feature selected can be very different.\n\n--> Prediction performance is worse than Ridge regression.\n\n\n\n## 3) ElasticNet Regression\n\n* ElasticNet combines the properties of both Ridge and Lasso regression. It works by penalizing the model using both the l2-norm and the l1-norm.\n\n#### Key Difference between Ridge Regression and Lasso Regression\n* Ridge regression is mostly used to reduce the overfitting in the model, and it includes all the features present in the model. It reduces the complexity of the model by shrinking the coefficients.\n* Lasso regression helps to reduce the overfitting in the model as well as feature selection.\n\n\n#### Pros\n--> Doesn\u2019t have the problem of selecting more than n predictors when n<<p, whereas LASSO saturates when n<<p.\n\n#### Cons\n--> Computationally more expensive than LASSO or Ridge.\n\n","7d2424db":"<h1 style=\"color:#fce444;font-size:30px;font-family:Georgia;text-align:center;\"><strong>\u270f\ufe0fLoading <strong style=\"color:black;font-size:25px;font-family:Georgia;\">Dataset <strong style=\"color:#fce444;font-size:25px;font-family:Georgia;\"><strong style=\"color:black;font-size:30px;font-family:Georgia;\">:- <\/strong><\/strong><\/strong><\/strong><\/h1>","91b66b4e":"<h1 style=\"color:#fce444;font-size:30px;font-family:Georgia;text-align:center;\"><strong>\u270f\ufe0fModeling and Prediction <strong style=\"color:black;font-size:25px;font-family:Georgia;\">using Linear Regression <strong style=\"color:#fce444;font-size:25px;font-family:Georgia;\"><strong style=\"color:black;font-size:30px;font-family:Georgia;\">:- <\/strong><\/strong><\/strong><\/strong><\/h1>","b769e55a":" ### Linear regression:-\nLinear regression performs to predict a dependent variable(y) based on a given independent variable(x).\n\nSo, this regression technique finds out a linear relationship between x and y.\n\nlinear regression in a statistical way that is used for predictive analysis.\n\nLinear regression makes predictions for continuous\/real or numeric variables \nsuch as sales, salary, age, product price, etc..\n\nit shows the linear relationship which means it finds how the value of the dependent variable \nis changing according to the value of the independent variable.\n\nThe linear regression model provides a sloped straight line representing the relationship between the variables.\n\n\n<img src=\"https:\/\/static.javatpoint.com\/tutorial\/machine-learning\/images\/linear-regression-in-machine-learning.png\" width=\"300px\">\n\n\n\n\n\n\n\nLinear Regression in Mathematically, we can represent as:\n\n### y= a0+a1x+\u03b5\n\nY= Dependent Variable (Target Variable)\n\nX= Independent Variable (predictor Variable)\n\na0= intercept of the line\n\na1 = Linear regression coefficient\n\n\u03b5 = random error\n\n#### to find the a1 value we are using  least square method(slope of the estimated regression)\n \n ##### formula:-\n a1=m , a0=c\n<img src=\"https:\/\/i.stack.imgur.com\/OjlaY.png\" width=\"300px\">","9e3ae3cf":"Extreme Gradient Boosting (XGBoost) is an open-source library that provides an efficient and effective implementation of the gradient boosting algorithm.\n\nShortly after its development and initial release, XGBoost became the go-to method and often the key component in winning solutions for a range of problems in machine learning competitions.\n\nRegression predictive modeling problems involve predicting a numerical value such as a dollar amount or a height. XGBoost can be used directly for regression predictive modeling.","3ba02b85":"<h1 style=\"color:#fce444;font-size:30px;font-family:Georgia;text-align:center;\"><strong>\u270f\ufe0fContents <strong style=\"color:black;font-size:25px;font-family:Georgia;\">of notebook <strong style=\"color:#fce444;font-size:25px;font-family:Georgia;\"><strong style=\"color:black;font-size:30px;font-family:Georgia;\">:- <\/strong><\/strong><\/strong><\/strong><\/h1>","bac7e1be":"ElasticNet is a linear regression model trained with both l1 and l2-norm regularization of the coefficients. This combination allows for learning a sparse model where few of the weights are non-zero like Lasso, while still maintaining the regularization properties of Ridge. We control the convex combination of l1 and l2 using the l1_ratio parameter.\n\nElastic-net is useful when there are multiple features that are correlated with one another. Lasso is likely to pick one of these at random, while elastic-net is likely to pick both.\n\nA practical advantage of trading-off between Lasso and Ridge is that it allows Elastic-Net to inherit some of Ridge\u2019s stability under rotation.","aff57448":"#### Observation:\nMost of the people are using gas as fuel for their cars","1e87bf86":"## Exploring Numerical\/Float variables","7b842216":"<h1 style=\"color:#fce444;font-size:30px;font-family:Georgia;text-align:center;\"><strong>\u270f\ufe0fExploratory <strong style=\"color:black;font-size:25px;font-family:Georgia;\">Data Analysis (EDA)<strong style=\"color:#fce444;font-size:25px;font-family:Georgia;\"><strong style=\"color:black;font-size:30px;font-family:Georgia;\">:- <\/strong><\/strong><\/strong><\/strong><\/h1>","ea01bd30":"#### First I will store all categorical columns in one list and all numerical,float columns in another list","83d33cfa":"### Lets us remove Car_Id column since it is not useful","d523bada":"### Linear Regression Line\nA linear line showing the relationship between the dependent and independent variables is called a regression line. A regression line can show two types of relationship:\n\n##### Positive Linear Relationship:\nIf the dependent variable increases on the Y-axis and independent variable increases on X-axis, then such a relationship is termed as a Positive linear relationship.\n\n\n##### Negative Linear Relationship:\nIf the dependent variable decreases on the Y-axis and independent variable increases on the X-axis, then such a relationship is called a negative linear relationship.\n\n<img src=\"https:\/\/datalabbd.com\/wp-content\/uploads\/2019\/06\/12b.png\" width=\"600px\">","5c2ccda1":"## References:","9dbf0d63":"<h1 style=\"color:#fce444;font-size:30px;font-family:Georgia;text-align:center;\"><strong>\u270f\ufe0fModeling and Prediction <strong style=\"color:black;font-size:25px;font-family:Georgia;\">using Lasso Regression <strong style=\"color:#fce444;font-size:25px;font-family:Georgia;\"><strong style=\"color:black;font-size:30px;font-family:Georgia;\">:- <\/strong><\/strong><\/strong><\/strong><\/h1>","2941b433":"<h1 style=\"color:#fce444;font-size:30px;font-family:Georgia;text-align:center;\"><strong>\u270f\ufe0fModel <strong style=\"color:black;font-size:25px;font-family:Georgia;\">Coefficients <strong style=\"color:#fce444;font-size:25px;font-family:Georgia;\"><strong style=\"color:black;font-size:30px;font-family:Georgia;\">:- <\/strong><\/strong><\/strong><\/strong><\/h1>","254ed6a5":"## Finding the best fit line:\nWhen working with linear regression, our main goal is to find the best fit line\nthat means the error between predicted values and actual values should be minimized.\n\nThe best fit line will have the least error.\n\n#### when cost function is used:\n\nThe different values for weights or the coefficient of lines (a0, a1) gives a different line of regression,\n\nso we need to calculate the best values for a0 and a1 to find the best fit line, so to calculate this we use cost function.\n\n### Cost function(mean square error):-\n\nThe different values for weights or coefficient of lines (a0, a1) gives the different line of regression,and the cost function is used to estimate the values of the coefficient for the best fit line.\n\nIt measures how a linear regression model is performing.\n\nWe can use the cost function to find the accuracy of the mapping function, which maps the input variable to the output variable.\n\nThis mapping function is also known as Hypothesis function.\n\nFor Linear Regression, we use the Mean Squared Error (MSE) cost function, \nwhich is the average of squared error occurred between the predicted values and actual values.\n\n<img src=\"https:\/\/datavedas.com\/wp-content\/uploads\/2018\/04\/image003-1.png\" width=\"300px\">\n\nN=Total number of observation\n\nYi = Actual value\n\n(a1xi+a0)= Predicted value.\n\n##### Residuals: \nThe distance between the actual value and predicted values is called residual.\n\n#### Gradient Descent:\nGradient descent is used to minimize the MSE by calculating the gradient of the cost function\n\nA regression model uses gradient descent to update the coefficients of the line by reducing the cost function.\n\nIt is done by a random selection of values of coefficient and then iteratively update the values to reach the minimum cost function.\n\n\n<img src=\"https:\/\/miro.medium.com\/max\/1838\/1*HrFZV7pKPcc5dzLaWvngtQ.png\" width=\"700px\">\n","e5c696f7":"<h1 style=\"color:#fce444;font-size:30px;font-family:Georgia;text-align:center;\"><strong>\u270f\ufe0fFeature <strong style=\"color:black;font-size:25px;font-family:Georgia;\">Scaling <strong style=\"color:#fce444;font-size:25px;font-family:Georgia;\"><strong style=\"color:black;font-size:30px;font-family:Georgia;\">:- <\/strong><\/strong><\/strong><\/strong><\/h1>","2df28d3d":"The coefficient estimates for Ordinary Least Squares rely on the independence of the features. When features are correlated and the columns of the design matrix  have an approximately linear dependence, the design matrix becomes close to singular and as a result, the least-squares estimate becomes highly sensitive to random errors in the observed target, producing a large variance. This situation of multicollinearity can arise, for example, when data are collected without an experimental design.","404927ae":"<h1 style=\"color:#fce444;font-size:30px;font-family:Georgia;text-align:center;\"><strong>\u270f\ufe0fData <strong style=\"color:black;font-size:25px;font-family:Georgia;\">Visualization <strong style=\"color:#fce444;font-size:25px;font-family:Georgia;\"><strong style=\"color:black;font-size:30px;font-family:Georgia;\">:- <\/strong><\/strong><\/strong><\/strong><\/h1>","4239fee0":"Good hyperparameter values can be found by trial and error for a given dataset, or systematic experimentation such as using a grid search across a range of values.\n\nRandomness is used in the construction of the model. This means that each time the algorithm is run on the same data, it may produce a slightly different model.\n\nWhen using machine learning algorithms that have a stochastic learning algorithm, it is good practice to evaluate them by averaging their performance across multiple runs or repeats of cross-validation. When fitting a final model, it may be desirable to either increase the number of trees until the variance of the model is reduced across repeated evaluations, or to fit multiple final models and average their predictions.","d0d53fe4":"<h1 style=\"color:#fce444;font-size:30px;font-family:Georgia;text-align:center;\"><strong>\u270f\ufe0fData <strong style=\"color:black;font-size:25px;font-family:Georgia;\">Preprocessing <strong style=\"color:#fce444;font-size:25px;font-family:Georgia;\"><strong style=\"color:black;font-size:30px;font-family:Georgia;\">:- <\/strong><\/strong><\/strong><\/strong><\/h1>","e69153a6":"<h1 style=\"color:#fce444;font-size:30px;font-family:Georgia;text-align:center;\"><strong>\u270f\ufe0fModeling and Prediction <strong style=\"color:black;font-size:25px;font-family:Georgia;\">using Ridge Regression <strong style=\"color:#fce444;font-size:25px;font-family:Georgia;\"><strong style=\"color:black;font-size:30px;font-family:Georgia;\">:- <\/strong><\/strong><\/strong><\/strong><\/h1>","8f6a357c":"<h1 style=\"color:#fce444;font-size:30px;font-family:Georgia;text-align:center;\"><strong>\u270f\ufe0fFeature <strong style=\"color:black;font-size:25px;font-family:Georgia;\">Selection <strong style=\"color:#fce444;font-size:25px;font-family:Georgia;\"><strong style=\"color:black;font-size:30px;font-family:Georgia;\">:- <\/strong><\/strong><\/strong><\/strong><\/h1>","b9d9993c":"### Types of Linear Regression\n\nLinear regression can be divided into two types of the algorithm:\n\n##### Simple Linear Regression:\nIf a single independent variable is used to predict the value of a numerical dependent variable, then such a Linear Regression algorithm is called Simple Linear Regression.\n\n##### Multiple Linear regression:\nIf more than one independent variables is used to predict the value of a numerical dependent variable, then such a Linear Regression algorithm is called Multiple Linear Regression.","b8e06fd3":"The Lasso is a linear model that estimates sparse coefficients. It is useful in some contexts due to its tendency to prefer solutions with fewer non-zero coefficients, effectively reducing the number of features upon which the given solution is dependent. For this reason, Lasso and its variants are fundamental to the field of compressed sensing. Under certain conditions, it can recover the exact set of non-zero coefficients","5c766e7e":"You can specify hyperparameter values to the class constructor to configure the model.\n\nPerhaps the most commonly configured hyperparameters are the following:\n\n1. n_estimators: The number of trees in the ensemble, often increased until no further improvements are seen.\n2. max_depth: The maximum depth of each tree, often values are between 1 and 10.\n3. eta: The learning rate used to weight each model, often set to small values such as 0.3, 0.1, 0.01, or smaller.\n4. subsample: The number of samples (rows) used in each tree, set to a value between 0 and 1, often 1.0 to use all samples.\n5. colsample_bytree: Number of features (columns) used in each tree, set to a value between 0 and 1, often 1.0 to use all features.","a0a06127":"Ridge regression addresses some of the problems of Ordinary Least Squares by imposing a penalty on the size of the coefficients.\n\nThe complexity parameter <strong> alpha>\/0 <\/strong> controls the amount of shrinkage: the larger the value of , the greater the amount of shrinkage and thus the coefficients become more robust to collinearity.","41f6707e":"### R-squared method:\n\n\n<img src=\"https:\/\/cdn.rapidinsight.com\/wp-content\/uploads\/2019\/11\/r-squared.png\" width=\"700px\">\n\n\nR-squared is a statistical method that determines the goodness of fit.\n\nIt measures the strength of the relationship between the dependent and independent variables on a scale of 0-1.\nwhen the scale is negative then it comes under worst model.\n\nThe high value of R-square determines the less difference between the predicted values and actual values and hence represents a good model.\n\nIt is also called a coefficient of determination, or coefficient of multiple determination for multiple regression.","462770b1":"## Calculating variance inflation factor","e34d5f82":"1. seaborn tutorial: https:\/\/www.kaggle.com\/gaganmaahi224\/seaborn-tutorial-17-plots\n2. Linear Regression Reference : https:\/\/www.kaggle.com\/rajeshwarraothota\/car-price-prediction-using-linear-regression","2893a221":"1. Importing Libraries\n2. Exploratory Data Analysis\n3. Basic Data Cleaning\n4. Data Visulaization\n5. Data preprocessing\n5. Feature Scaling\n    1. Calculating variance inflation factor\n    2. Treating Multicollinearity\n6. Feature Selection\n7. Prediction and Detailed explantion of Linear Regression, Lasso Regression, Ridge Regression, Elastic Regression, XGBoost Regression","63a18b3d":"## Treating Multicollinearity","42b0843f":"## Exploring Categorical Columns\/Features"}}