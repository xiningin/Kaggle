{"cell_type":{"a40c497b":"code","bbd65255":"code","32808645":"code","983c9ba3":"code","60e61949":"code","597c1e0f":"code","0e3129f7":"code","dc487a4e":"code","a72590d5":"code","8248f0dd":"code","58e9ed8a":"code","a9d8aa9c":"code","56466632":"code","518a831c":"code","f18c1ceb":"code","73fb8f42":"markdown","f18237bc":"markdown","4dc42ffb":"markdown","37f69984":"markdown","0c1a3c14":"markdown","4cf1181c":"markdown","cc7acee5":"markdown","948827b7":"markdown","48c89b8a":"markdown","eb89fff8":"markdown","44839a52":"markdown","5f2b029e":"markdown","acdae796":"markdown","68828627":"markdown","d3f9e817":"markdown","64c964ac":"markdown","636e6c20":"markdown","cf809571":"markdown"},"source":{"a40c497b":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","bbd65255":"import numpy as np \nimport pandas as pd\nfrom textblob import TextBlob\nfrom tqdm import tqdm\nfrom urllib.error import HTTPError\nimport textblob\nimport time\nimport re","32808645":"dev_tcn = pd.read_csv('\/kaggle\/input\/shopee-product-title-translation-open\/dev_tcn.csv')\ndev_en  = pd.read_csv('\/kaggle\/input\/shopee-product-title-translation-open\/dev_en.csv' )\ndev = pd.concat([dev_en, dev_tcn], axis=1)","983c9ba3":"dev.head()","60e61949":"translation = []\nfor k in tqdm(range(len(dev))):\n\n    try:\n        one_translation = TextBlob(dev['text'][k]).translate(to=\"en\")\n        translation.append(one_translation)\n        time.sleep(0.4)\n        \n    except (textblob.exceptions.NotTranslated, HTTPError) as e:\n        print(k, e)\n        if isinstance(e, textblob.exceptions.NotTranslated):\n            translation.append(\"\")\n        else:\n            break\n            ","597c1e0f":"textblob_translation = [x.string if isinstance(x, textblob.blob.TextBlob) else x for x in translation]\ndev['textblob_translation'] = textblob_translation\ndev.head()","0e3129f7":"!pip install sacrebleu","dc487a4e":"import sacrebleu\nimport matplotlib.pyplot as plt","a72590d5":"refs = [list(dev['translation_output'])]\npreds = list(dev['textblob_translation'])\nbleu = sacrebleu.corpus_bleu(preds, refs)\nprint(bleu.score)","8248f0dd":"refs = [list(dev['translation_output'].str.lower())]\npreds = list(dev['textblob_translation'].str.lower())\nbleu = sacrebleu.corpus_bleu(preds, refs)\nprint(bleu.score)","58e9ed8a":"def cleaning_string(my_string):\n    my_string = re.sub(r\"[^a-z0-9 ]+\", ' ', my_string.lower()) # lowercase then change special char to '' \n    my_string = \" \".join(my_string.split()) # remove white space\n\n    return my_string","a9d8aa9c":"refs = [list(dev['translation_output'].map(cleaning_string))]\npreds = list(dev['textblob_translation'].map(cleaning_string))\nbleu = sacrebleu.corpus_bleu(preds, refs)\nprint(bleu.score)","56466632":"refs = [list(dev['translation_output'])]\npreds = list(dev['translation_output'])\nbleu = sacrebleu.corpus_bleu(preds, refs)\nprint(bleu.score)","518a831c":"list_score_one_sample = []\nfor i in range(len(dev)):\n    refs = [[list(dev['translation_output'].str.lower())[i]]]\n    preds = list(dev['textblob_translation'].str.lower())[i]\n    bleu = sacrebleu.corpus_bleu(preds, refs)\n    list_score_one_sample.append(bleu.score)","f18c1ceb":"plt.plot(list_score_one_sample);","73fb8f42":"# Still Curious on the scoring?\n\nSee the score one by one","f18237bc":"Hope this useful,\n\nif so, please upvote this notebook :D \n\nThanks!!!","4dc42ffb":"By this [paper](https:\/\/research.fb.com\/wp-content\/uploads\/2016\/11\/bilingual_methods_for_adaptive_training_data_selection_for_machine_translation.pdf) \non Table 4, appeared their score on zh2en is around `24.6-25.6`\n\nBut it is BLEU score, I do not sure if it can be compared","37f69984":"## Score for translation removed special char and strange space = 43.21","0c1a3c14":"# How good is 43.21?","4cf1181c":"This notebook suposed to be kind of \"baseline\" on metrics score we are going to reach.\n\nSince by today the submission still got an error, I will try it on dev dataset which already got paired\n\n","cc7acee5":"thanks to @EnJun who remind me at comment section","948827b7":"# Evaluate using Sacrebleu","48c89b8a":"## Score for identical inputs = 100 (obviously, just check :))","eb89fff8":"# UPDATED V2: \n* include score with lower case only (from 18.79 to 39.42)\n* include score with removing special character and unwanted space (from 39.42 to 43.21)","44839a52":"By this tranlation API, we get score 43.21 \/ 100 \n\nBut I am still confused how to calculated it actually and how good we should reach.","5f2b029e":"Source:\n* https:\/\/github.com\/mjpost\/sacrebleu\n* https:\/\/blog.machinetranslation.io\/compute-bleu-score\/\n","acdae796":"## Score for translation without modification = 18.79","68828627":"**On this Notebook we will:**\n1. Translate Dev dataset using Textblob package\n2. Evaluate using Sacrebleu\n3. And See some \"benchmark\" on research paper","d3f9e817":"# Translate","64c964ac":"inspired from this thread https:\/\/www.kaggle.com\/c\/shopee-product-title-translation-open\/discussion\/166251 findings ","636e6c20":"## Score for translation with lowercase = 39.42","cf809571":"several translation hit perfect score"}}