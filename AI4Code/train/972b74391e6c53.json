{"cell_type":{"a34d930b":"code","f120673e":"code","c2ac93c6":"code","50916240":"code","93c36c99":"code","a931effe":"code","65ee7e00":"code","6a37daca":"code","a0ff24db":"code","5d19a406":"code","506503e2":"code","8f47b733":"code","5ca63e69":"code","b63c24fe":"code","dfb1b1b0":"code","ca10e84d":"code","15d08aaa":"code","9666eaae":"markdown","4571f671":"markdown","df326694":"markdown","2db8058c":"markdown","c3f24a27":"markdown","5e48c4cf":"markdown","c1a014d6":"markdown","fb96b807":"markdown"},"source":{"a34d930b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","f120673e":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport keras\nfrom keras.utils import to_categorical\nimport matplotlib.pyplot as plt\n\nimport h5py\n\n%matplotlib inline","c2ac93c6":"with h5py.File('..\/input\/3d-mnist\/full_dataset_vectors.h5', 'r') as hf:\n    x_train_ = hf[\"X_train\"][:]\n    y_train_ = hf[\"y_train\"][:]\n    x_test_ = hf[\"X_test\"][:]\n    y_test_ = hf[\"y_test\"][:]","50916240":"# 1D vector to rgb values, provided by ..\/input\/plot3d.py\ndef array_to_color(array, cmap=\"Oranges\"):\n    s_m = plt.cm.ScalarMappable(cmap=cmap)\n    return s_m.to_rgba(array)[:,:-1]\n\n# Transform data from 1d to 3d rgb\ndef rgb_data_transform(data):\n    data_t = []\n    for i in range(data.shape[0]):\n        data_t.append(array_to_color(data[i]).reshape(16, 16, 16, 3))\n    return np.asarray(data_t, dtype=np.float32)\nn_classes = 10 # from 0 to 9, 10 labels totally\n\nx_train = rgb_data_transform(x_train_)\nx_test = rgb_data_transform(x_test_)\n\ny_train = to_categorical(y_train_, n_classes)\ny_test = to_categorical(y_test_, n_classes)","93c36c99":"print('\\nTrain')\nunique_elements, counts_elements = np.unique(y_train_, return_counts=True)\nprint(np.asarray((unique_elements, counts_elements)))\n\nprint('\\nTest')\nunique_elements, counts_elements = np.unique(y_test_, return_counts=True)\nprint(np.asarray((unique_elements, counts_elements)))","a931effe":"# !pip install keras-rectified-adam","65ee7e00":"from keras.models import Sequential\nfrom keras.layers import Conv3D, MaxPool3D, Dense, Flatten, Dropout, BatchNormalization, Activation\nfrom keras.optimizers import Adadelta, Adam, RMSprop\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom keras_radam import RAdam","6a37daca":"filepath=\"best_model.hdf5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=2, save_best_only=True, mode='min')\nearly_stopping = EarlyStopping(patience = 10,monitor='val_loss', verbose=0, mode='min')\ncallbacks_list = [checkpoint, early_stopping]","a0ff24db":"models= Sequential()\nmodels.add(Dense(256, input_shape=(4096,)))\nmodels.add(BatchNormalization())\nmodels.add(Activation('elu'))\nmodels.add(Dropout(0.25))\n\nmodels.add(Dense(256))\nmodels.add(BatchNormalization())\nmodels.add(Activation('elu'))\nmodels.add(Dropout(0.25))\n\nmodels.add(Dense(256))\nmodels.add(BatchNormalization())\nmodels.add(Activation('elu'))\nmodels.add(Dropout(0.25))\n\nmodels.add(Dense(128))\nmodels.add(BatchNormalization())\nmodels.add(Activation('elu'))\nmodels.add(Dropout(0.5))\n\nmodels.add(Dense(10, activation='softmax'))","5d19a406":"models.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(lr=0.0008), metrics=['acc'])\nmodels.fit(x_train_, y_train_, batch_size=32, epochs=150, validation_split=0.2, verbose=2,\\\n          callbacks=callbacks_list)","506503e2":"models.evaluate(x_test_,y_test_,batch_size=32)","8f47b733":"model = Sequential()\n\nmodel.add(Conv3D(64,(3,3,3), input_shape=(16,16,16,3), padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.25))\n\nmodel.add(MaxPool3D(pool_size=(2, 2, 2)))\n\n# model.add(Conv3D(32,(3,3,3), padding='same'))\n# model.add(Activation('relu'))\n\n# model.add(MaxPool3D(pool_size=(2, 2, 2)))\n\nmodel.add(Flatten())\n\n# model.add(Dense(256))\n# model.add(Activation('relu'))\n\n# model.add(Dense(256))\n# model.add(Activation('relu'))\n# model.add(Dropout(0.25))\n\nmodel.add(Dense(10, activation='softmax'))","5ca63e69":"model.summary()","b63c24fe":"model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.0005), metrics=['acc'])\n#model.compile(RAdam(),loss='categorical_crossentropy', metrics=['acc'])\nhist=model.fit(x_train, y_train, batch_size=64, epochs=80, validation_split=0.01, verbose=2)","dfb1b1b0":"model.evaluate(x_test,y_test,batch_size=64)","ca10e84d":"loss = hist.history['loss']\nval_loss = hist.history['val_loss']\n\nacc = hist.history['acc']\nval_acc = hist.history['val_acc']","15d08aaa":"fig, ax = plt.subplots(2,1, figsize=(12, 12))\nax[0].plot((loss), 'bo', label=\"Loss\")\nax[0].plot((val_loss), 'b', label=\"Valid_Loss\",axes =ax[0])\nlegend = ax[0].legend(loc='best', shadow=True)\nax[0].set_xlabel('Epochs')\nax[0].set_ylabel('Loss')\n\nax[1].plot((acc), 'bo', label=\"Accuracy\")\nax[1].plot((val_acc), 'b',label=\"Valid_Accuracy\")\nlegend = ax[1].legend(loc='best', shadow=True)\nax[1].set_xlabel('Epochs')\nax[1].set_ylabel('Accuracy')","9666eaae":"### Keras Callbacks\nModelCheckpoint -> Save best model.  \nEarlyStopping -> If not improve reference value, model stop.  ","4571f671":"# Simple NN Model","df326694":"# Conv3D Model","2db8058c":"In modelling, test accurracy approached about 0.97. Validation accuracy is almost 0.69. However, this is not accruate because of valiation loss is not stable and very fluctuated. This model has also overfitting problem. I tried add dropout layer and change dropout ratio and Batchnormalization. It is difficulty to prevent overfitting. \n\nLoss is about 1.6 and Accuracy is about 69.1%.\n\nI changed simple Conv3D model. In this case, test accuracy approached about 1.0. Validation accuracy is about 0.7. It has also overfitting problem.\n\nLoss is about 1.8 and Accuracy is about 67.1%.","c3f24a27":"In modelling, test accurracy approach 0.8. Validation accuracy approch about 0.63. This model is overfitting. I found loss and validation loss is big difference and it is overfitting. I tried add dropout layer and change dropout ratio. But it is difficulty to prevent overfitting in NN model.\n\nLoss is about 1.280 and Accuracy is about 62.6%.","5e48c4cf":"## Counting the Zero(0) to Nine(9)\nCategorical values is uniformly distributed. This is not imbalanced data.","c1a014d6":"# 3D MNIST","fb96b807":"## Plot Loss & Valid_Loss and Accuracy & Valid_Accuracy"}}