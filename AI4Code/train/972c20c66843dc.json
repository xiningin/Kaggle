{"cell_type":{"0ad1e6a3":"code","84ecc05f":"code","e487db85":"code","f957f699":"code","bb65b725":"code","fcb8d1aa":"code","f752adb7":"code","9630d1df":"code","555a1a21":"code","1872d9a5":"code","270ff0a1":"code","8b5a651f":"code","d07b5931":"code","f76f991a":"code","63a09869":"code","3a028f24":"code","454df0dc":"code","648569c1":"code","d24eabd8":"code","87026c52":"code","0443848b":"code","5391420b":"code","2fcb1f7d":"markdown","002d869e":"markdown","237c287f":"markdown","b998ec94":"markdown","40e351c8":"markdown","ed533f16":"markdown","2ae31fa3":"markdown","4c2853a9":"markdown","adc8a76a":"markdown"},"source":{"0ad1e6a3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","84ecc05f":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import StandardScaler","e487db85":"from sklearn.datasets import load_breast_cancer\n\ncancer = load_breast_cancer()","f957f699":"df = pd.DataFrame(cancer.data, columns=cancer.feature_names)\ndf['class'] = cancer.target\n\ndf.tail()","bb65b725":"df.describe()","fcb8d1aa":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim","f752adb7":"data = torch.from_numpy(df.values).float()\n\ntype(data), data.shape","9630d1df":"# In DNN, use all columns\nx = data[:, :-1]\ny = data[:, -1:]\n\nx.shape, y.shape","555a1a21":"# Train, Valid, Test ratio\nratios = [.6, .2, .2]","1872d9a5":"train_cnt = int(data.size(0) * ratios[0])\nvalid_cnt = int(data.size(0) * ratios[1])\ntest_cnt = data.size(0) - train_cnt - valid_cnt\ncnts = [train_cnt, valid_cnt, test_cnt]\n\nprint('Train %d \/ Valid %d \/ Test %d samples' % (train_cnt, valid_cnt, test_cnt))","270ff0a1":"indices = torch.randperm(data.size(0))\n\nx = torch.index_select(x, dim=0, index=indices)\ny = torch.index_select(y, dim=0, index=indices)\n\nx = x.split(cnts, dim=0) # Return tuple\ny = y.split(cnts, dim=0)\n\nfor x_i, y_i in zip(x, y):\n    print(x_i.shape, y_i.shape)","8b5a651f":"scaler = StandardScaler()\n\n# Only train data set is used for calculating mu and sigma\nscaler.fit(x[0].numpy()) \n\n# Transform Data by applyng train data set's mu and sigma\nx = [torch.from_numpy(scaler.transform(x[0].numpy())).float(),\n    torch.from_numpy(scaler.transform(x[1].numpy())).float(),\n    torch.from_numpy(scaler.transform(x[2].numpy())).float()]\n\n# Check standardized data\ndf = pd.DataFrame(x[0].numpy(), columns=cancer.feature_names)\ndf.tail()\n","d07b5931":"model = nn.Sequential(\n    nn.Linear(x[0].size(-1), 25),\n    nn.LeakyReLU(),\n    nn.Linear(25, 20),\n    nn.LeakyReLU(),\n    nn.Linear(20, 15),\n    nn.LeakyReLU(),\n    nn.Linear(15, 10),\n    nn.LeakyReLU(),\n    nn.Linear(10, 5),\n    nn.LeakyReLU(),\n    nn.Linear(5, y[0].size(-1)),\n    nn.Sigmoid()\n)\n\nmodel","f76f991a":"optimizer = optim.Adam(model.parameters()) # No learning_rate","63a09869":"n_epochs = 10000\nbatch_size = 32\nprint_interval = 100\nearly_stop = 1000","3a028f24":"from copy import deepcopy\n\nlowest_loss = np.inf\nbset_model = None\n\nlowest_epochs = np.inf","454df0dc":"train_history, valid_history = [], []\n\nfor i in range(n_epochs):\n    # Shuffle indices before split\n    indices = torch.randperm(x[0].size(0))\n    x_ = torch.index_select(x[0], dim=0, index=indices)\n    y_ = torch.index_select(y[0], dim=0, index=indices)\n    \n    x_ = x_.split(batch_size, dim=0)\n    y_ = y_.split(batch_size, dim=0)\n    \n    train_loss, valid_loss = 0, 0\n    y_hat = []\n    \n    ## For train set mini-batch:: START\n    for x_i, y_i in zip(x_, y_):\n        y_hat_i = model(x_i)\n        loss = F.binary_cross_entropy(y_hat_i, y_i)\n        \n        optimizer.zero_grad()\n        loss.backward()\n        \n        optimizer.step() # parameter update\n        \n        train_loss += float(loss) # disconnected with gradient\n    ## END\n    \n    train_loss = train_loss \/ len(x_)\n    \n    ## For validate set mini-batch :: START\n    with torch.no_grad():\n        x_ = x[1].split(batch_size, dim=0)\n        y_ = y[1].split(batch_size, dim=0)\n        \n        valid_loss = 0 # initialize valid_loss every epoch\n        \n        for x_i, y_i in zip(x_, y_):\n            y_hat_i = model(x_i)\n            loss = F.binary_cross_entropy(y_hat_i, y_i)\n            \n            valid_loss += float(loss)\n            \n            y_hat += [y_hat_i]\n    ## END\n    \n    valid_loss = valid_loss \/ len(x_)\n    \n    train_history += [train_loss]\n    valid_history += [valid_loss]\n    \n    if (i + 1) % print_interval == 0:\n        print('Epoch %d: train loss=%.4e valid_loss=%.4e lowest_loss=%.4e' % (\n            i + 1,\n            train_loss,\n            valid_loss,\n            lowest_loss\n        ))\n    \n    if valid_loss <= lowest_loss:\n        lowest_loss = valid_loss\n        lowest_epoch = i\n        \n        # best model snapshot\n        best_model = deepcopy(model.state_dict())\n    else :\n        if early_stop > 0 and lowest_epoch + early_stop < i + 1:\n            print('There is no imporvement during last %d epochs.' % early_stop)\n            break\n        \nprint('The best validation loss from epoch %d: %.4e' % (lowest_epoch + 1, lowest_loss))\nmodel.load_state_dict(best_model)","648569c1":"plot_from = 2\n\nplt.figure(figsize=(20,10))\nplt.grid(True)\nplt.title('Train \/ Valid Loss History')\nplt.plot(\n    range(plot_from, len(train_history)), train_history[plot_from:],\n    range(plot_from, len(valid_history)), valid_history[plot_from:]\n)\nplt.yscale('log')\nplt.show()","d24eabd8":"test_loss = 0\ny_hat = []\n\nwith torch.no_grad():\n    x_ = x[2].split(batch_size, dim=0)\n    y_ = y[2].split(batch_size, dim=0)\n    \n    for x_i, y_i in zip(x_, y_):\n        y_hat_i = model(x_i)\n        loss = F.binary_cross_entropy(y_hat_i, y_i)\n        \n        test_loss += loss # Gradient is already detached\n        \n        y_hat += [y_hat_i]\n\ntest_loss = test_loss \/ len(x_)\ny_hat = torch.cat(y_hat, dim=0)\n\nprint('Test loss: %.4e' % test_loss)\n        ","87026c52":"correct_cnt = (y[2] == (y_hat > 0.5)).sum()\ntotal_cnt = float(y[2].size(0))\n\nprint('Test Accuracy: %.4f' % (correct_cnt \/ total_cnt))","0443848b":"df = pd.DataFrame(torch.cat([y[2], y_hat], dim=1).detach().numpy(), \n                  columns=['y', 'y_hat'])\n\nsns.histplot(df, x='y_hat', hue='y', bins=50, stat='probability')\nplt.show()","5391420b":"from sklearn.metrics import roc_auc_score\n\nroc_auc_score(df.values[:,0], df.values[:, 1])","2fcb1f7d":"# **Binary Classification**","002d869e":"## **Load Dataset from sklearn**","237c287f":"## **Loss History**","b998ec94":"## **Let's see the result!**","40e351c8":"## **Split into Train, Valid, Test data set**","ed533f16":"## **Train**","2ae31fa3":"## **Convert to PyTorch Tensor**","4c2853a9":"## **Preprocessing**","adc8a76a":"## **Build Model & Optimizer**"}}