{"cell_type":{"de2bb84c":"code","20c0e94f":"code","b7bcb56d":"code","44e976d6":"code","edee68a6":"code","28332474":"code","dac3f5a6":"code","80a45d28":"code","19f5a78d":"code","ce2c2627":"code","26356427":"markdown","79776c97":"markdown"},"source":{"de2bb84c":"import os\nimport time\nimport numpy as np \nimport pandas as pd \n\nimport math\nfrom sklearn.model_selection import train_test_split\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n","20c0e94f":"train_df = pd.read_csv(\"..\/input\/train.csv\")\n\nprint(\"Train shape : \",train_df.shape)\n","b7bcb56d":"X_train = train_df[\"question_text\"].fillna(\"_na_\").values\n\ntokenizer = Tokenizer(num_words=100_000)\ntokenizer.fit_on_texts(list(X_train))","44e976d6":"EMBEDDING_FILE = '..\/input\/embeddings\/glove.840B.300d\/glove.840B.300d.txt'\ndef get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n\nall_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nembed_size = all_embs.shape[1]","edee68a6":"nb_words = 100_000\nembedding_matrix_glove = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n\nfor word, i in tokenizer.word_index.items():\n    if i >= 100_000:\n        continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: \n        embedding_matrix_glove[i] = embedding_vector","28332474":"# adapted from https:\/\/stackoverflow.com\/questions\/37558899\/efficiently-finding-closest-word-in-tensorflow-embedding\nimport tensorflow as tf\n\nbatch_size = 10_000\nn_neighbors = 10\nclosest_words = np.zeros((nb_words, n_neighbors+1))\n\nembedding = tf.placeholder(tf.float32, [nb_words, embed_size])\nbatch_array = tf.placeholder(tf.float32, [batch_size, embed_size])\nnormed_embedding = tf.nn.l2_normalize(embedding, dim=1)\nnormed_array = tf.nn.l2_normalize(batch_array, dim=1)\ncosine_similarity = tf.matmul(normed_array, tf.transpose(normed_embedding))\nclosest_k_words = tf.nn.top_k(cosine_similarity,k=n_neighbors+1)\n\nwith tf.Session() as session:\n    start_idx = 0\n    for end_idx in range(batch_size, nb_words, batch_size):\n        print(end_idx)\n        result = session.run(closest_k_words, feed_dict={embedding: embedding_matrix_glove, batch_array: embedding_matrix_glove[start_idx:end_idx]})\n        closest_words[start_idx:end_idx] = result[1]\n\n        start_idx = end_idx","dac3f5a6":"index_to_word = {v:k for k,v in tokenizer.word_index.items()}\nindex_to_word[0] = \"<PAD>\"","80a45d28":"synonyms = {index_to_word[int(x[0])]: [index_to_word[int(y)] for y in x[1:]] for x in closest_words}","19f5a78d":"synonyms[\"king\"]","ce2c2627":"synonyms[\"quora\"]","26356427":"Simple kernel for how to quickly calculate similarity of words from embeddings using GPU and tensorflow.","79776c97":"I have tried a few things for data augmentation, without any luck. Maybe someone has some ideas how to use it."}}