{"cell_type":{"74e12cb9":"code","a97ac315":"code","c999c52f":"code","f6dab7ab":"code","57aa3552":"code","90046e0c":"code","2172585b":"code","f26f67f5":"code","910c029f":"code","30030f7f":"code","e6eaf2b6":"code","6d6fe517":"code","5b517dcc":"code","66dfb411":"code","0bc49ae7":"code","948804e6":"code","120a26da":"code","cb337dfd":"code","b57ed065":"code","982eee4b":"code","50bec651":"code","c105e8e3":"code","e431706b":"code","20360629":"code","b2cc3a8d":"code","d3164d60":"code","cf4974a6":"code","e1fb238d":"code","6f35d3e0":"code","09784673":"markdown","aedef7b7":"markdown","0f044c37":"markdown","36d18bd4":"markdown","bab9e735":"markdown","462f3948":"markdown","1267d1ed":"markdown"},"source":{"74e12cb9":"def gettweets(search):\n    import pandas as pd\n    import tweepy as tw\n\n    consumer_key=input(\"Insert your API_key: \")\n    consumer_secret=input(\"Insert your API_Secret: \")\n    access_token=input(\"Insert your Access_Key: \")\n    access_secret=input(\"Insert your Access_Secret: \")\n\n    auth = tw.OAuthHandler(consumer_key, consumer_secret)\n    auth.set_access_token(access_token, access_secret)\n    api = tw.API(auth, wait_on_rate_limit=True)\n\n\n    # Define the search term and the date_since date as variables\n    search_words = search\n    #date_since = \"2018-11-16\"\n    #filter Retweets\n    search_words = search_words+\" -filter:retweets\"\n    # Collect tweets\n\n    tweets = tw.Cursor(api.search,\n                  q=search_words,\n                  lang=\"en\").items(500)\n\n    all_tweets = [tweet.text for tweet in tweets]\n    print(\"Succesfully worked!\",all_tweets[:5])\n\n    return all_tweets","a97ac315":"tweets = gettweets(\"#covid\")","c999c52f":"len(tweets)","f6dab7ab":"import pandas as pd\nimport numpy as np","57aa3552":"df = pd.DataFrame(columns=[\"Tweets\"])","90046e0c":"df[\"Tweets\"] = tweets","2172585b":"df.iloc[4,0]","f26f67f5":"# Use regular expression to clean unwanted words.\n#remove links\ndf[\"clean_step0\"]=df[\"Tweets\"].str.replace(r\"http\\S+\",\"\")","910c029f":"df","30030f7f":"#remove symbols\ndf[\"clean_step1\"]=df[\"clean_step0\"].str.replace(r\"[^a-zA-Z\\s]\",\"\")","e6eaf2b6":"df[\"clean_step1\"]=df[\"clean_step1\"].str.lower()","6d6fe517":"df.iloc[4,2]","5b517dcc":"df[\"clean_step2\"]=df[\"clean_step1\"].str.replace(r\"\\n\",\"\")","66dfb411":"df[\"clean_step2\"]=df[\"clean_step2\"].str.strip()","0bc49ae7":"df[\"clean_step2\"][4]","948804e6":"from textblob import TextBlob","120a26da":"class sentiment:\n    global TextBlob\n    def __init__(self,word):\n        self.word = word\n    \n    def getsentiment(self):\n        textscore = TextBlob(self.word)\n        return textscore.sentiment\n    \n    def gettokens(self):\n        sen = TextBlob(self.word)\n        return sen.words\n    \n    def gettags(self):\n        sen = TextBlob(self.word)\n        return sen.tags\n    def getnouns(self):\n        sen = TextBlob(self.word)\n        return sen.noun_phrases\n\n    ","cb337dfd":"txt = df[\"clean_step2\"]","b57ed065":"sentence = txt[4]","982eee4b":"sentence","50bec651":"senti = sentiment(sentence)","c105e8e3":"senti.gettags()","e431706b":"senti.getnouns()","20360629":"senti.gettokens()","b2cc3a8d":"senti.getsentiment()","d3164d60":"senti_lst = []\nsubj_lst = []\nfor txt in txt:\n    senti = sentiment(txt)\n    score = senti.getsentiment()[0]\n    subj = senti.getsentiment()[1]\n    \n    senti_lst.append(score)\n    subj_lst.append(subj)\n    ","cf4974a6":"df[\"Sentiment_Score\"] = senti_lst","e1fb238d":"df[\"Sentiment_Subjectivity\"] = subj_lst ","6f35d3e0":"df.iloc[:,3:]","09784673":"## Sentiment Modeling","aedef7b7":"### Required Libraries\n- tweepy (pip install tweepy)\n- TextBlob (pip install textblob)\n- pandas (inbuilt)\n- regex  (inbuilt)","0f044c37":"## Combine","36d18bd4":"## Data Extraction","bab9e735":"<img src=\"https:\/\/media.giphy.com\/media\/k4ZItrTKDPnSU\/giphy.gif\" alt=\"img\" title=\"Using Twitter API to extract and build a sentiment model.\" \/>","462f3948":"# <font color=blue|red|green|pink|yellow>Twitter Sentiment Analysis<\/font>\n","1267d1ed":"## Cleaning"}}