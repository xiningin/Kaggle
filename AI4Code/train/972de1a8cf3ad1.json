{"cell_type":{"ff6e69b5":"code","ddc46714":"code","98ef5c67":"code","91e6f9cf":"code","c1efc13d":"code","327f2b46":"code","8e1b90ac":"code","aaef600f":"code","86fe1dc2":"code","f37faef7":"code","49e816a9":"code","b2ca6b96":"code","699392f6":"code","e87d9c13":"code","06be582f":"code","16da613d":"code","05e723de":"code","5a8b0eb0":"code","0ffd96bb":"code","9a0f2bd2":"code","e60202c9":"code","f47c2a20":"code","0d448047":"code","de920257":"code","b360c581":"code","5ef98a76":"code","327f080b":"code","8e6d1ca2":"code","d4b1a306":"code","c74e1c39":"code","617f8270":"code","9cd60738":"code","72659795":"markdown","aa1309aa":"markdown","60b23cf7":"markdown","9d8f37ba":"markdown","9685d26d":"markdown","0ee558eb":"markdown"},"source":{"ff6e69b5":"import os\nimport random\nimport seaborn as sns\nimport cv2\n\n# General packages\nimport pandas as pd\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport PIL\nimport IPython.display as ipd\nimport glob\nimport h5py\nimport plotly.graph_objs as go\nimport plotly.express as px\nfrom PIL import Image\nfrom tempfile import mktemp\n\nfrom bokeh.layouts import column, row\nfrom bokeh.models import ColumnDataSource, LinearAxis, Range1d\nfrom bokeh.models.tools import HoverTool\nfrom bokeh.palettes import BuGn4\nfrom bokeh.plotting import figure, output_notebook, show\nfrom bokeh.transform import cumsum\nfrom math import pi\n\noutput_notebook()\n\nfrom IPython.display import Image, display\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom keras.models import load_model\nfrom keras.preprocessing import image\nfrom PIL import Image","ddc46714":"print(os.listdir('..\/input\/landmark-recognition-2020\/'))","98ef5c67":"BASE_PATH = '..\/input\/landmark-recognition-2020'\n\nTRAIN_DIR = f'{BASE_PATH}\/train'\nTEST_DIR = f'{BASE_PATH}\/test'\n\nprint('Reading data...')\ntrain = pd.read_csv(f'{BASE_PATH}\/train.csv')\nsub = pd.read_csv(f'{BASE_PATH}\/sample_submission.csv')\nprint('Reading data completed')","91e6f9cf":"train.head()","c1efc13d":"sub.head()","327f2b46":"train.shape[0]","8e1b90ac":"sub.shape","aaef600f":"landmarks = len(train['landmark_id'].unique())\nlandmarks","86fe1dc2":"print('Top few landmark_ids by count')\n\nz = train.landmark_id.value_counts().head(10).to_frame()\nz.reset_index(inplace=True)\nz.columns=['landmark_id','count']\nz.landmark_id = z.landmark_id.apply(lambda x: f'id_{x}')\n\nz.style.background_gradient(cmap='Oranges')","f37faef7":"# displaying only top 30 landmark\nlandmark = train.landmark_id.value_counts()\nlandmark_df = pd.DataFrame({'landmark_id':landmark.index, 'frequency':landmark.values}).head(30)\n\nlandmark_df['landmark_id'] =   landmark_df.landmark_id.apply(lambda x: f'landmark_id_{x}')\n\nfig = px.bar(landmark_df, x=\"frequency\", y=\"landmark_id\",color='landmark_id', orientation='h',\n             hover_data=[\"landmark_id\", \"frequency\"],\n             height=1000,\n             title='Number of images per landmark_id (Top 30 landmark_ids)')\nfig.show()","49e816a9":"import PIL\nfrom PIL import Image, ImageDraw\n\n\ndef display_images(images, title=None): \n    f, ax = plt.subplots(5,5, figsize=(18,22))\n    if title:\n        f.suptitle(title, fontsize = 30)\n\n    for i, image_id in enumerate(images):\n        image_path = os.path.join(TRAIN_DIR, f'{image_id[0]}\/{image_id[1]}\/{image_id[2]}\/{image_id}.jpg')\n        image = Image.open(image_path)\n        \n        ax[i\/\/5, i%5].imshow(image) \n        image.close()       \n        ax[i\/\/5, i%5].axis('off')\n\n        landmark_id = train[train.id==image_id.split('.')[0]].landmark_id.values[0]\n        ax[i\/\/5, i%5].set_title(f\"ID: {image_id.split('.')[0]}\\nLandmark_id: {landmark_id}\", fontsize=\"12\")\n\n    plt.show() ","b2ca6b96":"samples = train.sample(25).id.values\ndisplay_images(samples)","699392f6":"samples = train[train.landmark_id == 138982].sample(25).id.values\ndisplay_images(samples)","e87d9c13":"from collections import Counter\nlandmark_counts = dict(Counter(train['landmark_id']))\nlandmark_dict = {'landmark_id': list(landmark_counts.keys()), 'count': list(landmark_counts.values())}\n\nlandmark_count_df = pd.DataFrame.from_dict(landmark_dict)\nlandmark_count_sorted = landmark_count_df.sort_values('count', ascending = False)\nlandmark_count_sorted.head(20)","06be582f":"fig_count = px.histogram(landmark_count_df, x = 'landmark_id', y = 'count')\nfig_count.update_layout(\n    title_text='Distribution of Landmarks',\n    xaxis_title_text='Landmark ID',\n    yaxis_title_text='Count'\n)\n\nfig_count.show()","16da613d":"from tensorflow.keras.applications import(\n                vgg16,\n                resnet50,\n                mobilenet,\n                inception_v3)","05e723de":"vgg_model = vgg16.VGG16(weights = 'imagenet')","5a8b0eb0":"resnet_model = resnet50.ResNet50(weights = 'imagenet')","0ffd96bb":"mobilenet_model = mobilenet.MobileNet(weights = 'imagenet') ","9a0f2bd2":"train_list = glob.glob('..\/input\/landmark-recognition-2020\/train\/*\/*\/*\/*')\ntest_list = glob.glob('..\/input\/landmark-recognition-2020\/test\/*\/*\/*\/*')","e60202c9":"train_list","f47c2a20":"test_list","0d448047":"filename = '..\/input\/landmark-recognition-2020\/train\/1\/1\/1\/11172998c813fe6f.jpg'","de920257":"original = image.load_img(filename,target_size=(224,224))\nprint('PIL image size',original.size)\nplt.imshow(original)\nplt.show()","b360c581":"from tensorflow.keras.preprocessing.image import img_to_array\nnumpy_image = img_to_array(original)\nplt.imshow(np.uint8(numpy_image))\nplt.show()\nprint('numpy array size',numpy_image.shape)","5ef98a76":"image_batch = np.expand_dims(numpy_image, axis=0)\nprint('image batch size', image_batch.shape)\nplt.imshow(np.uint8(image_batch[0]))","327f080b":"# prepare the image for the VGG model\nfrom keras.applications.imagenet_utils import decode_predictions\nprocessed_image = vgg16.preprocess_input(image_batch.copy())\n# get the predicted probabilities for each class\npredictions = vgg_model.predict(processed_image)\n# print predictions\n# convert the probabilities to class labels\n# we will get top 5 predictions which is the default\nlabel_vgg = decode_predictions(predictions)\n# print VGG16 predictions\nfor prediction_id in range(len(label_vgg[0])):\n    print(label_vgg[0][prediction_id])\n","8e6d1ca2":"from keras.applications.resnet50 import preprocess_input\nfrom keras.applications.imagenet_utils import decode_predictions","d4b1a306":"img = image.load_img(filename,target_size=(224,224))\nimg = image.img_to_array(img)\nimg = np.expand_dims(img,axis=0)\nimg = preprocess_input(img)","c74e1c39":"preds = resnet_model.predict(img)","617f8270":"print( decode_predictions(preds, top=1)[0])","9cd60738":"pred = mobilenet_model.predict(img)\nprint(decode_predictions(pred))","72659795":"# VGG16","aa1309aa":"## Distribution of Landmarks with their counts","60b23cf7":"# MobileNet ","9d8f37ba":"## Most Occuring Landmarks","9685d26d":"# Pre-trained models","0ee558eb":"# ResNet50"}}