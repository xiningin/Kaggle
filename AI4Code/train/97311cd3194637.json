{"cell_type":{"ec7892a7":"code","e356f51c":"code","7b4f027b":"code","da6c4580":"code","888013fd":"code","1d0e5af1":"code","d4d177d0":"code","d5c5059f":"code","1ff99c22":"code","7a7ec3dc":"code","a6f2d332":"code","741d65ec":"code","970fd86b":"code","9e316537":"code","d8ea58bb":"code","f8bc2ede":"code","8ad2d715":"code","698cb153":"code","8507687c":"code","f393e6b1":"code","f3f5e33f":"code","ddebca86":"code","60791b38":"code","616896e3":"code","f7d77fca":"code","0c4ee93c":"code","fbd150da":"code","2d6a3920":"code","66bd6662":"code","13b82c0f":"code","bd7be436":"code","b56eda8b":"code","8385ae5a":"code","75d74f08":"code","c665e592":"code","fdef18fa":"code","e5d2260b":"code","65706ac8":"code","d60dc34b":"code","89b93ee8":"code","acb77071":"code","7caedbfb":"code","3aaa5e6b":"code","3624b7a6":"code","f43d032e":"markdown","f47b2c68":"markdown","272d26ac":"markdown","4c587c30":"markdown","ac94ab3e":"markdown","92b1bd13":"markdown","8d5bbb3a":"markdown","aafb4700":"markdown"},"source":{"ec7892a7":"import warnings\nwarnings.filterwarnings('ignore')","e356f51c":"!mkdir ~\/.kaggle","7b4f027b":"! cp \"..\/input\/kaggle\/kaggle.json\" ~\/.kaggle\/","da6c4580":"!kaggle competitions download -c heberhackathon","888013fd":"import zipfile\n\ndata = zipfile.ZipFile('.\/heberhackathon.zip')\ndata.extractall()","1d0e5af1":"!dir","d4d177d0":"import pandas as pd \nimport numpy as np\ndata = pd.read_csv('.\/sample_submission.csv')\ndata.head()","d5c5059f":"train_model = pd.read_pickle('.\/images_array_train.pkl')","1ff99c22":"train_data = np.array(train_model)\ntrain_data.shape","7a7ec3dc":"test_model = pd.read_pickle('.\/images_array_test.pkl')","a6f2d332":"test_data = np.array(test_model)\ntest_data.shape","741d65ec":"target_model = pd.read_pickle('.\/target_train.pkl')","970fd86b":"target_data = np.array(target_model)\ntarget_data.shape","9e316537":"train_data.dtype","d8ea58bb":"target_data.dtype","f8bc2ede":"x_train = train_data.astype('float32')\nx_test = test_data.astype('float32')\nx_train \/= 255\nx_test \/= 255","8ad2d715":"x_train.dtype","698cb153":"y_train = target_data","8507687c":"from tensorflow.keras.utils import to_categorical\n\nnum_classes = 10\ny_train = to_categorical(y_train, num_classes)","f393e6b1":"print(y_train.shape)\nprint(y_train.dtype)\nprint(y_train)","f3f5e33f":"print(x_train.shape)\nprint(x_test.shape)\nprint(y_train.shape)","ddebca86":"import random\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(60,60))\nfor i in range(200):\n    plt.subplot(20,20,i+1)\n    plt.xticks([])\n    plt.yticks([])\n    rand_no = random.randint(0,len(x_train))     \n    plt.imshow(x_train[rand_no], cmap='gray')\n    plt.xlabel([y_train[rand_no]])","60791b38":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\nfrom tensorflow.keras.layers import Conv2D\nfrom tensorflow.keras.layers import MaxPooling2D\nfrom tensorflow.keras.optimizers import RMSprop,SGD,Adam,Nadam\nfrom tensorflow.keras.utils import to_categorical\nimport matplotlib.pyplot as plt\n%matplotlib inline","616896e3":"import warnings\nwarnings.filterwarnings('ignore')","f7d77fca":"model1 = Sequential()\nmodel1.add(Conv2D(filters=224, kernel_size=(3,3), activation='relu', padding='same', input_shape=(28,28,3)))\nmodel1.add(MaxPooling2D(pool_size=(3,3)))\nmodel1.add(Conv2D(filters=192, kernel_size=(3,3), activation='relu', padding='same'))\nmodel1.add(MaxPooling2D(pool_size=(2,2)))\nmodel1.add(Conv2D(filters=160, kernel_size=(3,3), activation='relu', padding='same'))\nmodel1.add(MaxPooling2D(pool_size=(1,1)))\nmodel1.add(Flatten())\nmodel1.add(Dense(134, activation='tanh'))\nmodel1.add(Dense(10, activation='softmax'))\n\nmodel1.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(0.001), metrics=['accuracy'])\nmodel1.summary()","0c4ee93c":"history = model1.fit(x_train,y_train,validation_split=0.03,batch_size=13,epochs=50)","fbd150da":"score = model1.predict(x_test,verbose=1)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])","2d6a3920":"sc = model1.evaluate(x_train, y_train, verbose=1)\nprint('Test loss:', sc[0])\nprint('Test accuracy:',sc[1])","66bd6662":"erm =[]\nfor i in range(0,len(score)):\n    erm.append((np.argmax((score[i]>0.5)*1)))\ny_prediction = np.array(erm)\ny_prediction","13b82c0f":"a = []\nfor i in range(3000):\n    a.append(i)","bd7be436":"submission = pd.DataFrame()\nsubmission['id'] = a\nsubmission['class'] = y_prediction\nsubmission.to_csv('submission23oct.csv', index=False)\nprint(submission.head(8))\nprint(data)","b56eda8b":"submission.nunique()","8385ae5a":"submission['class'].plot(kind='hist')","75d74f08":"from tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Sequential","c665e592":"data_augmentation = keras.Sequential(\n  [\n    layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\", \n                                                 input_shape=(28, \n                                                              28,\n                                                              3)),\n    layers.experimental.preprocessing.RandomRotation(0.1),\n    layers.experimental.preprocessing.RandomZoom(0.1),\n  ]\n)","fdef18fa":"model2 = Sequential([\n    data_augmentation,\n    layers.Conv2D(filters=224, kernel_size=(3,3), activation='relu', padding='same'),\n    layers.MaxPooling2D(pool_size=(2,2)),\n    layers.Conv2D(filters=192, kernel_size=(3,3), activation='relu', padding='same'),\n    layers.MaxPooling2D(pool_size=(2,2)),\n    layers.Conv2D(filters=160, kernel_size=(3,3), activation='relu', padding='same'),\n    layers.MaxPooling2D(pool_size=(2,2)),\n    layers.Conv2D(filters=134, kernel_size=(3,3), activation='relu', padding='same'),\n    layers.MaxPooling2D(pool_size=(2,2)),\n    layers.Conv2D(filters=134, kernel_size=(3,3), activation='relu', padding='same'),\n    layers.Flatten(),\n    layers.Dense(134, activation='tanh'),\n    layers.Dense(10, activation='softmax')\n])\n\nmodel2.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(0.001), metrics=['accuracy'])\nmodel2.summary()","e5d2260b":"history2 = model2.fit(x_train,y_train,batch_size=13,epochs=60)","65706ac8":"sc = model2.evaluate(x_train, y_train, verbose=1)\nprint('Test loss:', sc[0])\nprint('Test accuracy:',sc[1])","d60dc34b":"score = model2.predict(x_test,verbose=1)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])","89b93ee8":"erm =[]\nfor i in range(0,len(score)):\n    erm.append((np.argmax((score[i]>0.5)*1)))\ny_predictionn = np.array(erm)\ny_predictionn","acb77071":"a = []\nfor i in range(3000):\n    a.append(i)","7caedbfb":"submission = pd.DataFrame()\nsubmission['id'] = a\nsubmission['class'] = y_predictionn\nsubmission.to_csv('submissionaug.csv', index=False)\nprint(submission.head(8))\nprint(data)","3aaa5e6b":"submission.nunique()","3624b7a6":"submission['class'].plot(kind='hist')","f43d032e":"## Data augmentation","f47b2c68":"## Understand what kind of images","272d26ac":"## Converting datatype","4c587c30":"# Creating Model","ac94ab3e":"# Target Data","92b1bd13":"### Maheshvaran S\n### 205229119","8d5bbb3a":"# Test model","aafb4700":"# Train model"}}