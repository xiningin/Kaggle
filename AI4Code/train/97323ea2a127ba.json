{"cell_type":{"e3587761":"code","06c2c6e7":"code","967f62f0":"code","93d9f2b2":"code","a2bca174":"code","0aa4193f":"code","98b4f09f":"code","0ea1e25c":"code","6e05db0c":"code","a2426554":"code","74f77c8c":"code","d8402f5f":"code","2f0a985f":"code","b62106af":"code","84f42009":"code","553f0915":"code","ad43ff4e":"code","c9da75b3":"code","e3560e0f":"code","170dfb44":"code","9fb04008":"code","04885bb2":"code","6defe318":"code","ae8b5608":"code","61fa6afc":"code","c9802d47":"code","6ffbd3a7":"code","915a9421":"code","2731a8a3":"code","d86c561c":"code","4a709eff":"code","6cd6bd3d":"code","651a8ec5":"code","cb42a82a":"code","8808ce2f":"code","2ee809d0":"code","439a9dca":"code","f2f92182":"code","8ade58bc":"code","e010d2e9":"code","19c52aea":"code","3bd02e8f":"code","7f1c93a5":"code","5d3476c8":"code","a4b7643e":"code","c161f92b":"code","5a256fa4":"code","f77e3e07":"code","b49603d0":"code","de73ed64":"code","2060a575":"code","997ed555":"code","97c71a6a":"code","95685ae2":"code","027c5f02":"code","83ac2a57":"code","95d6d242":"code","e0934bc7":"markdown","388eb77c":"markdown","7e5019f9":"markdown","e33904c9":"markdown","16a4f1c5":"markdown","07a8ef5c":"markdown","aef11025":"markdown","dbbb3347":"markdown"},"source":{"e3587761":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","06c2c6e7":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\nfrom keras.layers import Dense, Input, GlobalMaxPooling1D, LSTM\nfrom keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout, Activation, Flatten\nfrom keras.models import Model\nfrom keras.initializers import Constant\nfrom keras.models import Sequential\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score\nfrom gensim.models.keyedvectors import KeyedVectors","967f62f0":"from nltk.corpus import stopwords\nstop_words = set(stopwords.words('english'))\nfrom nltk.tokenize import word_tokenize\nimport nltk \nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nwordnet_lemmatizer = WordNetLemmatizer()","93d9f2b2":"df_real = pd.read_csv('\/kaggle\/input\/fake-and-real-news-dataset\/True.csv')\ndf_fake = pd.read_csv('\/kaggle\/input\/fake-and-real-news-dataset\/Fake.csv')","a2bca174":"df_real.head()","0aa4193f":"df_fake.head()","98b4f09f":"df_real.isnull().sum()","0ea1e25c":"df_fake.isnull().sum()","6e05db0c":"df_real.head()","a2426554":"df_real['text'][0]","74f77c8c":"df_real.shape","d8402f5f":"df_fake.shape","2f0a985f":"df_real['sentiment'] = 1\ndf_fake['sentiment'] = 0","b62106af":"df_real.shape","84f42009":"import re","553f0915":"def decontracted(phrase):\n    # specific\n    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase","ad43ff4e":"def get_cleaned_data(input_data, mode='df'):\n    stop = stopwords.words('english')\n    \n    input_df = ''\n    \n    if mode != 'df':\n        input_df = pd.DataFrame([input_data], columns=['text'])\n    else:\n        input_df = input_data\n        \n    #lowercase the text\n    input_df['text'] = input_df['text'].str.lower()\n    \n    input_df['text'] = input_df['text'].apply(lambda elem: decontracted(elem))\n    \n    #remove special characters\n    input_df['text'] = input_df['text'].apply(lambda elem: re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\\/\\\/\\S+)|^rt|http.+?\", \"\", elem))\n    \n    # remove numbers\n    input_df['text'] = input_df['text'].apply(lambda elem: re.sub(r\"\\d+\", \"\", elem))\n    \n    #remove stopwords\n    input_df['text'] = input_df['text'].apply(lambda x: ' '.join([word.strip() for word in x.split() if word not in (stop)]))\n    \n   \n    input_df['text'] = input_df['text'].apply(lambda words: (wordnet_lemmatizer.lemmatize(words)))\n\n    \n    return input_df","c9da75b3":"df_real = get_cleaned_data(df_real)\ndf_fake = get_cleaned_data(df_fake)","e3560e0f":"df_real.head()","170dfb44":"df_real['text'][0]","9fb04008":"data=pd.concat([df_real,df_fake],axis=0,ignore_index=True)","04885bb2":"data.tail(10)","6defe318":"data.head()","ae8b5608":"data.isnull().sum()","61fa6afc":"data.shape","c9802d47":"g=[]","6ffbd3a7":"for i in data['text']:\n    g.append(i)","915a9421":"maxl = max([len(s) for s in g])\nprint ('Maximum sequence length in the list of sentences:', maxl)","2731a8a3":"X=data['text']\nY=data['sentiment']","d86c561c":"tokenizer=Tokenizer(num_words=10000)\ntokenizer.fit_on_texts(X)\n\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))","4a709eff":"X = tokenizer.texts_to_sequences(X.values)\nX = pad_sequences(X, maxlen=2000)","6cd6bd3d":"Y = pd.get_dummies(data['sentiment'],columns=data[\"sentiment\"]).values\nY","651a8ec5":"Y.shape","cb42a82a":"X.shape","8808ce2f":"X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.2,random_state=42)","2ee809d0":"X_train","439a9dca":"path='\/kaggle\/input\/googlenewsvectorsnegative300\/GoogleNews-vectors-negative300.bin'","f2f92182":" wv_from_bin = KeyedVectors.load_word2vec_format(path, binary=True, limit=500000) \n  #extracting word vectors from google news vector\n embeddings_index = {}\n for word, vector in zip(wv_from_bin.vocab, wv_from_bin.vectors):\n      coefs = np.asarray(vector, dtype='float32')\n      embeddings_index[word] = coefs","8ade58bc":"print('Found %s word vectors.' % len(embeddings_index))","e010d2e9":"vocab_size = len(tokenizer.word_index) + 1","19c52aea":"print(vocab_size)","3bd02e8f":"# embedding_matrix = np.zeros((vocab_size, 300))\n# for word, i in word_index.items():\n#     try:\n#         embedding_vector = embeddings_index[word]\n#         embedding_matrix[i] = embedding_vector\n#     except KeyError:\n#         embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),300)","7f1c93a5":"embedding_matrix = np.zeros((len(word_index) + 1, 300))\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        # words not found in embedding index will be all-zeros.\n        embedding_matrix[i] = embedding_vector","5d3476c8":"embedding_matrix","a4b7643e":" model = Sequential()\n\n#Non-trainable embeddidng layer\nmodel.add(Embedding(vocab_size, output_dim=300, weights=[embedding_matrix], input_length=2000, trainable=False))\n    \nmodel.add(LSTM(units=128 , return_sequences = True))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(units=64))\nmodel.add(Dropout(0.1))\nmodel.add(Dense(units = 32 , activation = 'relu'))\nmodel.add(Dense(2, activation='sigmoid'))","c161f92b":"model.summary()","5a256fa4":"model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])","f77e3e07":"model.fit(X_train,Y_train,batch_size = 256 , validation_data = (X_test,Y_test) , epochs = 5)","b49603d0":"# prediction = model.predict_classes(X_test)\n# cf_matrix = confusion_matrix(Y_test,prediction)\n# sns.heatmap(cf_matrix, annot=True, fmt='g', xticklabels = ['Fake','Real'] , yticklabels = ['Fake','Real'])","de73ed64":"def get_pred_output(text_to_check):\n    sequences = tokenizer.texts_to_sequences([text_to_check])\n    data = pad_sequences(sequences, maxlen=2000)\n    predicted_val = model.predict_classes(data)\n#     predicted_val = model.predict(data)    \n#     if predicted_val.max() > 0.7:\n#         output = 1\n#     else:\n#          output = 0\n    \n    return predicted_val","2060a575":"unseen_real_data = \"\"\"\nTwenty-three more people have tested positive for COVID-19 in Tripura, taking the total number of cases in the state to 232.\n\nThe number of active cases stands at 65 while 165 people have recovered and have been discharged and two have migrated to other states.\n\nChief Minister Biplab Kumar Deb said, among the new cases, 18 people have come from Maharashtra by train.\n\"\"\"","997ed555":"unseen_fake_data = \"\"\"\nAmericans to fund killing babies in abortion that she has been caught trying to add taxpayer financing of abortions to the bill to combat the Coronavirus and provide economic stimulus to the nation as it deals with the COVD-19 outbreak.\nNancy Pelosi has a long history of promoting abortion and her first act after becoming Speaker in 2019 was pushing legislation to use tax money for abortions. So it\u2019s no surprise she is trying to exploit the Coronavirus pandemic to push abortion funding again.\nAs The Daily Caller reports: House Speaker Nancy Pelosi sought to include a potential way to guarantee federal funding for abortion into the coronavirus economic stimulus plan, according to multiple senior White House officials.\nSpeaking to the Daily Caller, those officials alleged that while negotiating the stimulus with U.S. Treasury Secretary Steve Mnuchin, Pelosi tried to lobby for \u201cseveral\u201d provisions that stalled bipartisan commitment to the effort. One was a mandate for up to $1 billion to reimburse laboratory claims, which White House officials say would set a precedent of health spending without protections outlined in the Hyde Amendment.\nLifeNews depends on the support of readers like you to combat the pro-abortion media. Please donate now.\n\u201cA New mandatory funding stream that does not have Hyde protections would be unprecedented,\u201d one White House official explained. \u201cUnder the guise of protecting people, Speaker Pelosi is working to make sure taxpayer dollars are spent covering abortion\u2014which is not only backwards, but goes against historical norms.\u201d\nA second White House official referred to the provision as a \u201cslush fund\u201d and yet another questioned \u201cwhat the Hyde Amendment and abortion have to do with protecting Americans from coronavirus?\u201d\nAmericans should insist to their members of Congress that we need a clean bill that provides aggressive action to help patients and spur the economy. Killing babies with our tax dollars is not the answer to the coronavirus and the situation should not be exploited for political gain.\n\"\"\"","97c71a6a":"text_to_check = unseen_real_data\npred = get_pred_output(text_to_check)\nprint('Unseen real data prediction {} '.format(pred[0]))\n\ntext_to_check = unseen_fake_data\npred = get_pred_output(text_to_check)\nprint('Unseen fake data prediction {} '.format(pred[0]))","95685ae2":"data.iloc[1000:1500]","027c5f02":"data.iloc[31000:31500]","83ac2a57":"text_to_check = data.text[1500]\npred = get_pred_output(text_to_check)\nprint('Seen Real data prediction {} '.format(pred[0]))\n\ntext_to_check = data.text[31500]\npred = get_pred_output(text_to_check)\nprint('Seen Fake data prediction {} '.format(pred[0]))","95d6d242":"model.save('final_lstm_model(word2vec).h5')","e0934bc7":"# Padding Sequences","388eb77c":"# Now saving our model as a h5 model..-->","7e5019f9":"As word2vec has 300 dimensions do we are choosing output dimension as 300 units..","e33904c9":"# Now importing the pretrained embedding index from Google index..","16a4f1c5":"# Splitting the Dataset..","07a8ef5c":"# Model-->","aef11025":"# So our model is predicting quite well..","dbbb3347":"As the maxlength is very big we will be selecting 2000 as our maxlength.."}}