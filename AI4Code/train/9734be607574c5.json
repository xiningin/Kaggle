{"cell_type":{"77d5bad8":"code","5bb8e559":"code","b9330141":"code","6f2e2e27":"code","f26692aa":"code","bc08808f":"code","a7657c23":"code","5b5e14be":"code","2108eea5":"code","c34a773c":"code","345342f6":"code","70c6cb4f":"code","1bce6142":"code","75b11eca":"code","c166f69b":"code","a4fd5002":"code","15cd319d":"code","19164004":"markdown","55c7275f":"markdown","8e42cdf5":"markdown"},"source":{"77d5bad8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5bb8e559":"import pandas as pd\ndf = pd.read_csv('\/kaggle\/input\/winequalityred\/winequality-red.csv')\ndf.head()","b9330141":"df.info()","6f2e2e27":"import seaborn as sns\nsns.countplot(x=\"quality\", data=df)","f26692aa":"df.groupby('quality')['alcohol'].mean().plot(kind='bar')","bc08808f":"prows = 6\npcols = 2\ncols  = df.columns\nfig, axs = plt.subplots(prows, pcols, figsize=(pcols*3.5, prows*3))\nfor r in range(0,prows):\n    for c in range(0,pcols):  \n        i = r*pcols+c\n        col=cols[i]\n        sns.barplot(x=\"quality\", y=col, data=df, ax = axs[r][c])\nfig.tight_layout()","a7657c23":"df['quality'].value_counts()","5b5e14be":"select_quality = [5, 6, 7]\ndf = df[df['quality'].isin(select_quality)]\ndf['quality'].value_counts()","2108eea5":"y = df['quality']\nX = df.drop('quality', axis=1)","c34a773c":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)","345342f6":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\n\nC = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\nmulti_class = ['multinomial']\nsolver = ['lbfgs']\npenalty = ['l2']\ntol = [0.5]\n\nlogreg_gscv = GridSearchCV(estimator=LogisticRegression(), \n                                param_grid={'C': C,\n                                            'multi_class': multi_class,\n                                            'solver': solver,\n                                            'penalty': penalty,\n                                            'tol':tol},   \n                                scoring='neg_log_loss',\n                                cv=5)","70c6cb4f":"logreg_gscv.fit(X_scaled, y)\nlogreg_gscv.best_params_","1bce6142":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2,  random_state=0)","75b11eca":"logreg = LogisticRegression(C = logreg_gscv.best_params_['C'],\n                           multi_class = 'multinomial',\n                           solver = 'lbfgs',\n                           penalty = logreg_gscv.best_params_['penalty'],\n                           tol = logreg_gscv.best_params_['tol'])\nlogreg.fit(X_train, y_train)\ny_pred=logreg.predict(X_test)","c166f69b":"from sklearn import metrics\nmetrics.accuracy_score(y_test,y_pred)","a4fd5002":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\n\ncm=confusion_matrix(y_test,y_pred)\nprint(cm)","15cd319d":"print(metrics.classification_report(y_test, y_pred))","19164004":"## Build model","55c7275f":"## Evaluate model","8e42cdf5":"## selecting top 3 classes for prediction"}}