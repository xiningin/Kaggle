{"cell_type":{"ccb96b41":"code","041b84b8":"code","73080575":"code","efaf4d7b":"code","a8c70cd6":"code","c33c0d2f":"code","5d273657":"code","c9320d22":"code","2100e8e8":"code","2c950361":"code","8404bc53":"code","e1806db1":"markdown","e903f91e":"markdown","7cf828ad":"markdown","7f547016":"markdown","c2ea8deb":"markdown","bfa27774":"markdown","8153ed81":"markdown","c835234d":"markdown","442f3248":"markdown","a4c9dc98":"markdown","45c1eadc":"markdown","740b0c71":"markdown","d407a98d":"markdown","f317654b":"markdown","ef4fd035":"markdown","cf7e4101":"markdown","1c387f9b":"markdown","32b41056":"markdown","c5466ec0":"markdown"},"source":{"ccb96b41":"%%capture\n!pip install --upgrade wandb\n!pip install tensorflow_datasets==3.2.1","041b84b8":"import numpy as np # For Linear Algebra\nimport pandas as pd # For I\/O, Data Transformation\nimport tensorflow as tf # Tensorflow\nimport tensorflow_datasets as tfds # For the SubTextEncoder\nimport os","73080575":"import wandb\nfrom wandb.keras import WandbCallback\nfrom kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\nWANDB_KEY = user_secrets.get_secret(\"WANDB_API_KEY\")\nwandb.login(key=WANDB_KEY);\n\n\nwandb.init(project=\"fakenewsclassification-an-overview\")\n\nconfig = wandb.config\n\nconfig.activation = 'relu'\nconfig.optimizer = 'adam'","efaf4d7b":"fakedataset = pd.read_csv(\"\/kaggle\/input\/fake-and-real-news-dataset\/Fake.csv\") # Make a DataFrame for Fake News\nrealdataset = pd.read_csv(\"\/kaggle\/input\/fake-and-real-news-dataset\/True.csv\") # Make a DataFrame for Real News\n\nrealdataset[\"class\"] = 1 # Adding Class to Real News\nfakedataset[\"class\"] = 0 # Adding Class to Fake News\n\nrealdataset[\"text\"] = realdataset[\"title\"] + \" \" + realdataset[\"text\"] # Concatenating Text and Title into a single column for Real News DataFrame\nfakedataset[\"text\"] = fakedataset[\"title\"] + \" \" + fakedataset[\"text\"] # Concatenating Text and Title into a single column for Fake News DataFrame\n\nrealdataset = realdataset.drop([\"subject\", \"date\", \"title\"], axis = 1) # Removing Redundant features from Real News DataFrame\nfakedataset = fakedataset.drop([\"subject\", \"date\", \"title\"], axis = 1) # Removing Redundant features from Fake News DataFrame\n\ndataset = realdataset.append(fakedataset, ignore_index = True) # Making a Single DataFrame \n\ndel realdataset, fakedataset ","a8c70cd6":"vocab_size = 10000\nencoder = tfds.features.text.SubwordTextEncoder.build_from_corpus(dataset[\"text\"], vocab_size)","c33c0d2f":"def enc(dataframe):\n    tokenized = []\n    for sentence in dataframe[\"text\"].values:\n        tokenized.append(encoder.encode(sentence))\n    out = tf.keras.preprocessing.sequence.pad_sequences(tokenized, padding = \"post\")\n    return out\nx = enc(dataset)","5d273657":"y = dataset[\"class\"]","c9320d22":"# Model Definition\nBiLSTM = tf.keras.Sequential([\n    tf.keras.layers.Embedding(encoder.vocab_size, 64), # Embedding Layer using the vocab-size from encoder\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,  return_sequences=True)), # Create the first Bidirectional layer with 64 LSTM units\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)), # Second Bidirectional layer witth 32 LSTM units\n    tf.keras.layers.Dense(64, activation=config.activation), # A Dense Layer with 64 units\n    tf.keras.layers.Dropout(0.5), # 50% Dropout\n    tf.keras.layers.Dense(1) # Final Dense layer with a single unit\n])","2100e8e8":"BiLSTM.compile(optimizer=config.optimizer, loss='binary_crossentropy', metrics= ['acc']) # Compiling the Model\nBiLSTM_history = BiLSTM.fit(x,y, epochs = 10, callbacks=[WandbCallback(log_weights = True)])","2c950361":"def pad_to_size(vec, size):\n  zero = [0] * (size - len(vec))\n  vec.extend(zeros)\n  return vec\n\ndef sample_predict(sample_pred_text, pad):\n  encoded_sample_pred_text = encoder.encode(sample_pred_text)\n\n  if pad:\n    encoded_sample_pred_text = pad_to_size(encoded_sample_pred_text, 64)\n  encoded_sample_pred_text = tf.cast(encoded_sample_pred_text, tf.float32)\n  predictions = model.predict(tf.expand_dims(encoded_sample_pred_text, 0))\n\n  return (predictions)\n\nsample_pred_text = ('Enter your Text Here')\npredictions = sample_predict(sample_pred_text, pad=False)","8404bc53":"BiLSTM.save(os.path.join(wandb.run.dir, \"model.h5\"))","e1806db1":"# Encoding the Corpus","e903f91e":"# Predicting with the Model","7cf828ad":"# Pre-Processing and Cleaning","7f547016":"In this notebook I'd like to continue on the work of [Atish Adhikari](https:\/\/www.kaggle.com\/atishadhikari). In his [notebook](https:\/\/www.kaggle.com\/atishadhikari\/fake-news-cleaning-word2vec-lstm-99-accuracy), he proposes a novel approach for News Classification.\n\nWe'll use the following modules, \n* [numpy](https:\/\/numpy.org\/doc\/stable\/reference\/index.html)\n* [pandas](https:\/\/pandas.pydata.org\/docs\/reference\/index.html)\n* [tensorflow](https:\/\/www.tensorflow.org\/api_docs\/python\/tf)\n* [tensorflow_datasets](https:\/\/www.tensorflow.org\/datasets\/overview?hl=en)","c2ea8deb":"# Model Definition","bfa27774":" # Downloading the Model to WandB","8153ed81":"Here's a screenrecording of the Model in action. I copied a article from a authentic and reputed news source, pasted it on the text block and ran inference. As you can see the model gave the correct prediction of the article being Real. \n\n![demo.gif](attachment:demo.gif)\n","c835234d":"# Training the Model","442f3248":"Here, we write 2 functions to predict using the model. A pad_to_size function to pad our vectors and a sample_predict function to encode a string and predict using the model.","a4c9dc98":"# Demo","45c1eadc":"Here, we create a function to encode the DataFrame by looping through all the sentences in the corpus, with \"**post**\" padding using the [**tf.keras.preprocessing.sequence.pad_sequences()**](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/preprocessing\/sequence\/pad_sequences?hl=en) function.","740b0c71":"The original dataset doesn't have any class variables associated with the instances. Thus, to enable supervised learning we add another \"**class**\" variable to the DataFrames. Also, to get a reliable and authentic score for classification we concatenate the \"**text**\" and \"**title**\" columns. We then drop the redundant columns from both the DataFrames. Then, we just make a single DataFrame out of both the DataFrames.","d407a98d":"# Importing Libraries","f317654b":"Using the \"**class**\" column of the Dataset for Supervised Training of the Model","ef4fd035":"We have all seen fake news forwards on our WhatsApp messages. Generally, these articles are generated by bots and internet trollers and are used with an intent to intrigue the audience and mislead them. Fake news can be very dangerous as it can spread misinformation and inflict rage in public. It is now becoming a serious problem in India due to more and more people using social media and lower levels of digital awareness.\n\n","cf7e4101":"Here, we define our Model with the following layers:\n\n* [Embedding Layer](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/layers\/Embedding?hl=en)\n* [Bidirectional LSTM Layer](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/layers\/Bidirectional?hl=en) with 64 units\n* [Bidirectional LSTM Layer](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/layers\/Bidirectional?hl=en) with 32 units\n* [Dense Layer](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/layers\/Dense?hl=en) with 64 units\n* [Dropout Layer](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/layers\/Dropout?hl=en) with a 50% droprate\n* [Dense](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/layers\/Dense?hl=en) with a single output unit\n\nWe then compile the model using:\n\n* [Adam Optimiser](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/optimizers\/Adam?hl=en)\n* [Binary Crossentropy Loss](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/losses\/BinaryCrossentropy?hl=en)\n* Metrics as [Accuracy](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/metrics\/Accuracy?hl=en)","1c387f9b":"To encode the corpus, we use the [**SubwordTextEncoder**](https:\/\/www.tensorflow.org\/datasets\/api_docs\/python\/tfds\/features\/text\/SubwordTextEncoder) from tfds.features.text's **build_from_corpus** function. We declare a novel vocab_size of 10,000 and then use the \"**text**\" column from the DataFrame.","32b41056":"# Approach Used\n\nBidirectional Recurrent Neural Networks (BRNN) connect two hidden layers of opposite directions to the same output. With this form of generative deep learning, the output layer can get information from past (backwards) and future (forward) states simultaneously. Invented in 1997 by Schuster and Paliwal, BRNNs were introduced to increase the amount of input information available to the network. Standard recurrent neural network (RNNs) also have restrictions as the future input information cannot be reached from the current state. On the contrary, BRNNs do not require their input data to be fixed. Moreover, their future input information is reachable from the current state.","c5466ec0":"We train the model for a novel 10 epochs"}}