{"cell_type":{"3a988f0d":"code","369aedf2":"code","e942120e":"code","06747304":"code","15ff7a77":"code","1fbf1a58":"code","2f0c0fee":"code","7115f51a":"code","920dc3d6":"code","eb299726":"code","ff15e353":"code","a6019138":"code","fc0f96e7":"code","cbaf0cef":"code","56082f9f":"code","35acfbda":"code","f8313d9d":"code","be6c8da3":"code","a4e3daf0":"code","8988b203":"markdown","62b3ab0c":"markdown","203a2358":"markdown","4032fed5":"markdown","e3f9c5d5":"markdown","f0609984":"markdown","3407994d":"markdown","f6d67269":"markdown","9ec90010":"markdown","17a8813a":"markdown","f488e9b3":"markdown","9fec56b9":"markdown","c9a67dbb":"markdown"},"source":{"3a988f0d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","369aedf2":"df= pd.read_csv('..\/input\/heart-attack-analysis-prediction-dataset\/heart.csv')\n","e942120e":"df.shape","06747304":"df.head()","15ff7a77":"df.info()","1fbf1a58":"df.isnull().sum()","2f0c0fee":"import matplotlib.pyplot as plt\nimport seaborn as sns\ncolors = ['#003f5c','#58508d','#bc5090','#ff6361','#ffa600']\nsns.set(palette=colors, font='Serif', style='white', rc={'axes.facecolor':'whitesmoke', 'figure.facecolor':'whitesmoke'})","7115f51a":"df.describe()","920dc3d6":"fig, ax = plt.subplots(ncols=4, nrows=4 ,figsize=(15,8), constrained_layout=True )\nax=ax.flatten()\nplt.suptitle(\"Univariated Analysis\", size=20, weight='bold')\nfor i,a in enumerate(df.columns):\n    if df[a].nunique() >5:\n        sns.kdeplot(x=df[a],ax=ax[i], fill=True)\n    else:\n        sns.countplot(data=df, x=a, ax=ax[i])\n    for s in ['left','right','top','bottom']:\n        ax[i].spines[s].set_visible(False)\n        ax[14].spines[s].set_visible(False)\n        ax[15].spines[s].set_visible(False)\n    ax[14].set_yticks([])\n    ax[15].set_yticks([])\n    ax[14].set_xticks([])\n    ax[15].set_xticks([])","eb299726":"num_cols=df.loc[:,df.nunique()>5].columns\nfig, ax = plt.subplots(ncols=2, nrows=5 ,figsize=(15,10), constrained_layout=True )\nplt.suptitle(\"Bivariated Analysis (comparing numeric features with target feature)\", size=20, weight='bold')\nax=ax.flatten()\ni=0\nfor a in num_cols:\n    if df[a].nunique() >5:\n        sns.boxplot(y=df[a],ax=ax[i], x=df['output'])\n        i=i+1\n        sns.kdeplot(x=df[a],ax=ax[i], hue=df['output'], fill=True, linewidth=2)\n        i=i+1\n    \n        \n   ","ff15e353":"def clean_outliers(df1, features):\n    for i in features:\n        Q1=df1[i].quantile(0.25)\n        Q2=df1[i].quantile(0.75)\n        IQR= (Q2-Q1)\n        print(\"Feature {} has min value: {} max value: {}\".format(i, Q1-IQR*1.5,Q2+IQR*1.5))\n        df1=df1[((df1[i]>(Q1-IQR*1.5))&(df1[i]<(Q2+IQR*1.5)))]\n        #df1=df_c\n    return df1","a6019138":"df_clean=clean_outliers(df, num_cols)","fc0f96e7":"df_clean.shape","cbaf0cef":"fig, ax = plt.subplots(ncols=2, nrows=5 ,figsize=(15,10), constrained_layout=True )\nax=ax.flatten()\nplt.suptitle(\"Bivariated Analysis after outlier removal (comparing numeric features with target feature)\", size=20, weight='bold')\ni=0\nfor a in num_cols:\n    if df[a].nunique() >5:\n        sns.boxplot(y=df_clean[a],ax=ax[i], x=df_clean['output'])\n        i=i+1\n        sns.kdeplot(x=df_clean[a],ax=ax[i], hue=df_clean['output'], fill=True, linewidth=2)\n        i=i+1","56082f9f":"sns.heatmap(df_clean[num_cols].corr(), annot=True)","35acfbda":"from scipy.stats import f_oneway, ttest_ind\nfrom statsmodels.stats.multicomp import pairwise_tukeyhsd \n\n\nfor i in num_cols:\n    yes = df_clean[df_clean['output']==1][i]\n    no = df_clean[df_clean['output']==0][i]\n    stat,p_value=ttest_ind(yes, no)\n   \n    if p_value < 0.05:\n        print(f\"Feature {i} has significant difference in Output feature with p_value {np.round(p_value,3)}\")\n    else:\n        print(f\"Feature {i} has no significant difference in Output feature with p_value {np.round(p_value,3)}\")","f8313d9d":"cat_cols=df.loc[:,df.nunique()<5].columns\nfrom scipy.stats import f_oneway, ttest_ind, chi2_contingency\n\n\nfor i in cat_cols:\n    crosstab = pd.crosstab(df_clean['output'], df[i])\n    #print(crosstab)\n    stat,p_value,_,_=chi2_contingency(crosstab)\n   \n    if p_value < 0.05:\n        print(f\"Feature {i} has significant difference in Output feature with p_value {np.round(p_value,3)}\")\n    else:\n        print(f\"Feature {i} has no significant difference in Output feature with p_value {np.round(p_value,3)}\")","be6c8da3":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report, confusion_matrix\n\nscale = StandardScaler()\nX=df_clean.drop(['output'], axis=1)\ny=df_clean['output']\n\nfrom sklearn.model_selection import cross_val_score, KFold, GridSearchCV,train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state = 42)\n#X_train[num_cols]=scale.fit_transform(X_train[num_cols])\n#X_test[num_cols]=scale.transform(X_test[num_cols])\n\nfrom sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nmodel = [DecisionTreeClassifier(), RandomForestClassifier(), GradientBoostingClassifier()]\nfor i in model:\n    params={'max_leaf_nodes':[i for i in range(2,20)]}\n    score = GridSearchCV(i, param_grid=params, scoring='recall')\n    score.fit(X_train,y_train)\n    print(score.best_params_)\n    print(score.best_estimator_)\n    print(f\"Recall score: {score.best_score_}\")\n    pred=score.predict(X_test)\n    print(f\"Recall score of test data: {recall_score(y_test, pred)}\")\n    print(classification_report(y_test,pred))\n    sns.heatmap(confusion_matrix(y_test,pred), annot=True)\n    plt.show()\n","a4e3daf0":"from sklearn.tree import plot_tree\nmodel = DecisionTreeClassifier(max_leaf_nodes=4)\nmodel.fit(X_train,y_train)\nprint(f\"Recall score of Train Data: {model.score(X_train,y_train)}\")\npred=model.predict(X_test)\nprint(f\"Recall score of Test Data: {recall_score(y_test, pred)}\")\nprint(f\"Accuracy Score: {accuracy_score(y_test,pred)}\")\nprint(classification_report(y_test,pred))\nsns.heatmap(confusion_matrix(y_test,pred), annot=True)\nplot_tree(model);\nfeat_imp=pd.DataFrame(columns=['feature','value'])\nfeat_imp['feature']=X_train.columns\nfeat_imp['value']=model.feature_importances_\nfeat_imp.sort_values(by=['value'], ascending=False)","8988b203":"### Statistical Hypotheis testing for Contineous features","62b3ab0c":"No Null values, seems the dataset is well managed","203a2358":"**Observation:**\nFrom the above trained models,the Decision tree with node=4 perfrom well compared to other model on the Recall value. our objective here is to minimse the False negative than the False positive. so, let us train the model with full Train data set.","4032fed5":"**Problem statement: Predict the Heart Attack based on the features provided.**\n\n**Objective: The features are provided with output feature as 0 & 1, 0 is the no heart attack and 1 is yes to heart attack. since it is medical related information. Our object is to minimize the False Negative (model predicting the actual patient as \"no\")**\n\nPlease review and provide your feedback","e3f9c5d5":"# Read Data","f0609984":"![](https:\/\/cdn-images-1.medium.com\/max\/800\/1*tGeiO5zee6exueRC8iBuaQ.jpeg)","3407994d":"# Model Creation","f6d67269":"### Statistical Hypotesis testing for Categorical variable","9ec90010":"# Exploratory Data Analysis","17a8813a":"\n## Training RandomForest with full trainset","f488e9b3":"# Hypothesis Testing","9fec56b9":"**Observation:**\nAccuracy may be less - 78%, howeverfor the Medical related data, the model should avoid the false negative than the accuracy. So, i have considered Recall as the metrics to consider for my model. Here the recall value is 91%, means that ther is less False negative than the False positive.  \n**So, may i consider the efficiency of my model is 90%** :)\n\nThere are rooms for further improvement, but i hope this will be right starting point.  \n\nPlease provide your valuable feedback. ","c9a67dbb":"All features are either float or int, which is convenient for modeling, however there are feature which are discreate."}}