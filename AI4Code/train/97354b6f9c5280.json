{"cell_type":{"8a14c5b2":"code","86555a6b":"code","aea65c89":"code","e21baad4":"code","40736cc7":"code","9a039e1c":"code","761b3b09":"code","6160b122":"code","f82e16fe":"code","19fbf5bc":"code","4de06718":"code","9f812928":"code","9fb4d845":"code","0fd274eb":"code","ebc9f4b5":"code","bbc6690a":"code","b748f8f2":"code","ae0f5b52":"code","36fa8549":"code","50a780ba":"code","35b91f48":"code","3bc17262":"code","e3965e0e":"code","acc00513":"code","aafcf8d2":"code","13c59da8":"code","983b11a6":"code","a3e1b712":"code","1de27c2c":"code","81aa6cb3":"code","444fddbc":"code","426e6410":"code","81b30833":"code","87a46e63":"code","cfc702c0":"code","b7135027":"code","694af5ce":"code","f999fa35":"code","2f668cda":"code","77c62e85":"code","b67c85af":"code","27894ff7":"markdown","f2502aa8":"markdown","e680853d":"markdown","246a877d":"markdown","8b3d6286":"markdown","610a914d":"markdown"},"source":{"8a14c5b2":"# !pip install autograd --quiet","86555a6b":"import datetime\nimport pandas as pd\nfrom time import time\nimport tensorflow as tf\n# from autograd import grad\n# import autograd.numpy as np\nimport numpy as np\nnp.set_printoptions(suppress = True)\nfrom numba import njit\nfrom scipy.optimize import minimize, fsolve\nfrom tqdm.notebook import tqdm\n\nimport lightgbm as lgb\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator\n%matplotlib inline\n%config InlineBackend.figure_format = 'svg'","aea65c89":"def log_loss_metric(y_true, y_pred):\n    y_pred_clip = np.clip(y_pred, 1e-15, 1 - 1e-15)\n    return - np.mean(y_true * np.log(y_pred_clip) + (1 - y_true) * np.log(1 - y_pred_clip))\n\ndef log_loss_numpy(y_pred):\n    y_true_ravel = np.asarray(y_true).ravel()\n    y_pred = np.asarray(y_pred).ravel()\n    y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n    loss = np.where(y_true_ravel == 1, - np.log(y_pred), - np.log(1 - y_pred))\n    return loss.mean()","e21baad4":"train_targets = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv', index_col = 'sig_id').values\ntrain = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\ny_true = train_targets.copy()\n# y_true = train_targets[train['cp_type'] == 'trt_cp']\nprint(y_true.shape)","40736cc7":"# oof_dict = {\n#             'MLP 2': '..\/input\/groupcvbestmlp\/MLP_2_oof.npy',##\n#             'MLP 3L 2': '..\/input\/groupcvbestmlp\/MLP_3L_2_oof.npy',##\n#             'MLP 4L 0': '..\/input\/groupcvbestmlp\/MLP_4L_0_oof.npy',##\n#             'RTN 2': '..\/input\/groupcvbestmlp\/RTN_oof.npy',##\n#             'TabNet': '..\/input\/groupcv-tabnet\/tabnet_oof.npy',##\n#            }\n\noof_dict = {\n#             'MLP V2': '..\/input\/groupcv-v2\/EModel_oof.npy',##\n#             'MLP V3': '..\/input\/groupcv-v3\/EModel_Stack_oof.npy', \n#             'MLP': '..\/input\/moa-mlp\/EModel_Stack_oof.npy',##\n#             'ResNet': '..\/input\/groupcv-v4\/EModel_Stack_oof.npy',\n#             'RTNS': '..\/input\/groupcv-rethinknet-single\/Model0_oof.npy',##\n#             'TabNet': '..\/input\/groupcv-tabnet\/tabnet_oof.npy',##\n#             'TabNet': '..\/input\/groupcv-1803pre-tabnet\/tabnet_oof.npy',##\n#             'GrowNet': '..\/input\/groupcv-5foldgrownet\/grownet_oof.npy',\n            'MLP 2': '..\/input\/groupcv-pbestpre-mlp\/MLP_2_oof.npy',##\n            'MLP 3L 2': '..\/input\/groupcv-pbestpre-mlp\/MLP_3L_2_oof.npy',##\n            'MLP 4L 0': '..\/input\/groupcv-pbestpre-mlp\/MLP_4L_0_oof.npy',##\n            'RTN': '..\/input\/groupcv-pbestpre-mlp\/RTN_oof.npy',##\n            'TabNet': '..\/input\/groupcv-pbestpre-tabnet\/tabnet_oof.npy',##\n            'GrowNet': '..\/input\/groupcv-7foldpbestpre-grownet\/grownet_oof.npy',\n            'ResDT': '..\/input\/groupcv-pbestpre-decisiontree\/Net_oof.npy',\n           }\n\noof = np.zeros((len(oof_dict), train_targets.shape[0], train_targets.shape[1]))\nfor i in range(oof.shape[0]):\n    oof[i][train['cp_type'] == 'trt_cp'] = np.load(list(oof_dict.values())[i])","9a039e1c":"log_loss_scores = {}\nfor n, key in enumerate(oof_dict.keys()):\n    score_oof = log_loss_numpy(oof[n])\n    log_loss_scores[key] = score_oof\n    print(f'{key} OOF:\\t', score_oof)","761b3b09":"for w in sorted(log_loss_scores, key = log_loss_scores.get, reverse = False):\n    print(w, log_loss_scores[w])","6160b122":"def func(weights):\n    oof_blend = np.tensordot(weights, oof, axes = ((0), (0))) \n    return log_loss_numpy(oof_blend)\n\n@njit\ndef grad_func(weights):\n    oof_clip = np.minimum(1 - 1e-15, np.maximum(oof, 1e-15))\n    gradients = np.zeros(oof.shape[0])\n    for i in range(oof.shape[0]):\n        a, b, c = y_true, oof_clip[i], np.zeros((oof.shape[1], oof.shape[2]))\n        for j in range(oof.shape[0]):\n            if j != i:\n                c += weights[j] * oof_clip[j]\n        gradients[i] = -np.mean((-a*b+(b**2)*weights[i]+b*c)\/((b**2)*(weights[i]**2)+2*b*c*weights[i]-b*weights[i]+(c**2)-c))\n    return gradients","f82e16fe":"tol = 1e-10\ninit_guess = [1 \/ oof.shape[0]] * oof.shape[0]\nbnds = [(0, 1) for _ in range(oof.shape[0])]\ncons = {'type': 'eq', \n        'fun': lambda x: np.sum(x) - 1,\n        'jac': lambda x: [1] * len(x),\n       }\n\nprint('Inital Blend OOF:', func(init_guess))\nstart_time = time()\nres_scipy = minimize(fun = func, \n                     x0 = init_guess, \n                     method = 'SLSQP', \n                     jac = grad_func, \n                     bounds = bnds, \n                     constraints = cons, \n                     tol = tol, \n                     options = {'disp': True})\nprint(f'[{str(datetime.timedelta(seconds = time() - start_time))[2:7]}] Optimised Blend OOF:', res_scipy.fun)\nprint('Optimised Weights:', res_scipy.x)","19fbf5bc":"weights = np.array(res_scipy.x)\nweights = np.where(weights <= 1e-6, 0., weights)\nprint(weights.sum())\nweights = weights \/ np.sum(weights)\nprint(weights)","4de06718":"# weights = np.array([0.05, 0.05, 0.25, 0.15, 0.2, 0.3])\nprint(weights.sum())","9f812928":"for n, key in enumerate(oof_dict.keys()):\n    print(f'{key} OOF:\\t', weights[n])","9fb4d845":"oof_blend = np.tensordot(weights, oof, axes = ((0), (0))) \nscore = log_loss_numpy(oof_blend)\nprint(score)","0fd274eb":"# 0.015273820378635507","ebc9f4b5":"# train_features = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\n# tar_nonctr = train_targets[train_features['cp_type'] == 'trt_cp']\n# print(log_loss_metric(tar_nonctr, oof_blend[train_features['cp_type'] == 'trt_cp']))","bbc6690a":"def post_process(pred, low, high):\n    pred_copy = pred.copy()\n    idx = []\n    for i in range(pred_copy.shape[0]):\n        flag = np.zeros(pred_copy.shape[1])\n        array = pred_copy[i].copy()\n        for j in range(pred_copy.shape[1]):\n            if (pred_copy[i, j] <= low) or (pred_copy[i, j] >= high):\n                flag[j] = 1\n            array[j] = round(array[j])\n        if flag.all() and pred_copy[i].any(): #array.any()\n            pred_copy[i] = array\n            idx.append(i)\n    return pred_copy, idx\n\n@njit\ndef post_process_jit(pred, low, high):\n    pred_copy = pred.copy()\n    for i in range(pred_copy.shape[0]):\n        flag = np.zeros(pred_copy.shape[1])\n        array = pred_copy[i].copy()\n        for j in range(pred_copy.shape[1]):\n            if (pred_copy[i, j] <= low) or (pred_copy[i, j] >= high):\n                flag[j] = 1\n            array[j] = round(array[j])\n        if flag.all() and pred_copy[i].any(): #array.any()\n            pred_copy[i] = array\n    return pred_copy","b748f8f2":"best_low = np.inf\nbest_high = 0\nbest_score = score\nfor low in tqdm(np.arange(0.001, 0.021, 0.001)):\n    for high in np.arange(0.98, 1, 0.001):\n#         oof_blend_pp, idx = post_process(oof_blend, low, high)\n        oof_blend_pp = post_process_jit(oof_blend, low, high)\n        score_pp = log_loss_numpy(oof_blend_pp)\n        if score_pp < best_score:\n            best_score = score_pp\n            best_low = low\n            best_high = high\n            print(best_low, best_high, best_score, best_score - score)","ae0f5b52":"print(best_low, best_high, best_score, best_score - score)","36fa8549":"# 0.015257296886519013","50a780ba":"# oof_blend_pp = post_process_jit(oof_blend, 0.015, 0.987)\n# score_pp = log_loss_numpy(oof_blend_pp)\n# print(score_pp)","35b91f48":"# from sklearn.metrics import roc_auc_score\n\n# print(score_pp)\n# for average in ['micro', 'macro', 'weighted']:\n#     roc_auc = roc_auc_score(y_true, oof_blend_pp, average = average)\n#     print(f'{average} ROC AUC Score:\\t', roc_auc)","3bc17262":"# @njit\n# def pp_bycol(pred, low, high):\n#     pred_copy = pred.copy()\n#     pred_copy[pred_copy <= low] = 0\n#     pred_copy[pred_copy >= high] = 1\n#     return pred_copy\n\n# @njit\n# def pp(pred, low, high):\n#     pred_copy = pred.copy()\n#     for i in range(pred_copy.shape[1]):\n#         pred_copy[:, i] = pp_bycol(pred_copy[:, i], low[i], high[i])\n#     return pred_copy\n\n# def log_loss(y_t, y_p):\n#     y_pred_clip = np.clip(y_p, 1e-15, 1 - 1e-15)\n#     loss = - np.mean(y_t * np.log(y_pred_clip) + (1 - y_t) * np.log(1 - y_pred_clip))\n#     return loss","e3965e0e":"# bins = 50\n\n# best_low = np.zeros(oof_blend.shape[1])\n# best_high = np.ones(oof_blend.shape[1])\n# for col in tqdm(range(oof_blend.shape[1])):\n#     start_time = time()\n#     best_score = log_loss(y_true[:, col], oof_blend[:, col])\n#     low_bound = oof_blend[:, col].min()\n#     high_bound = oof_blend[:, col].max()\n#     gap = high_bound - low_bound\n#     for low in np.arange(low_bound, high_bound, gap \/ bins):\n#         for high in np.arange(low, high_bound, gap \/ bins):\n#             oof_blend_col_pp = pp_bycol(oof_blend[:, col], low, high)\n#             score_pp = log_loss(y_true[:, col], oof_blend_col_pp)\n#             if score_pp < best_score:\n#                 best_score = score_pp\n#                 best_low[col] = low\n#                 best_high[col] = high\n# #                 print(f'Column {col}:', best_low[col], best_high[col])\n#     print(f'[{str(datetime.timedelta(seconds = time() - start_time))[2:7]}] Column {col}:', best_low[col], best_high[col])","acc00513":"# np.save('best_low.npy', best_low)\n# np.save('best_high.npy', best_high)","aafcf8d2":"# oof_blend_col_pp = pp(oof_blend, best_low, best_high)\n# score_bol_pp = log_loss_numpy(oof_blend_col_pp)\n# print(score)\n# print(score_bol_pp)\n# print(score_bol_pp - score)","13c59da8":"# def preprocess(df):\n#     df.loc[:, 'cp_type'] = df.loc[:, 'cp_type'].map({'trt_cp': 0, 'ctl_vehicle': 1})\n#     df.loc[:, 'cp_time'] = df.loc[:, 'cp_time'].map({24: 0, 48: 1, 72: 2})\n#     df.loc[:, 'cp_dose'] = df.loc[:, 'cp_dose'].map({'D1': 0, 'D2': 1})\n#     del df['sig_id']\n#     return df","983b11a6":"# x_train = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\n# x_test = pd.read_csv('..\/input\/lish-moa\/test_features.csv')\n\n# x_train = preprocess(x_train)\n# x_test = preprocess(x_test)\n\n# GENES = [col for col in x_train.columns if col.startswith('g-')]\n# CELLS = [col for col in x_train.columns if col.startswith('c-')]","a3e1b712":"# from sklearn.preprocessing import QuantileTransformer\n\n# qt = QuantileTransformer(output_distribution = 'normal', random_state = 42)\n# data0 = pd.concat([x_train, x_test])\n# qt.fit(data0[GENES+CELLS])","1de27c2c":"# x_train[GENES+CELLS] = qt.transform(x_train[GENES+CELLS])\n# x_test[GENES+CELLS] = qt.transform(x_test[GENES+CELLS])","81aa6cb3":"# np.random.seed(42)\n\n# std = 0.05\n# data0[GENES+CELLS] += np.random.normal(0, std, size = data0[GENES+CELLS].shape)\n# data0[GENES+CELLS] = qt.transform(data0[GENES+CELLS])","444fddbc":"# param = {'objective': 'binary', \n#          'metric': 'binary_logloss', \n#          'device_type': 'cpu', \n#          'num_thread': 4, \n#          'verbosity': -1, \n#          'bagging_fraction': 0.9811046327087707, \n#          'feature_fraction': 0.8933337503617897, \n#          'learning_rate': 0.010969545979403403, \n#          'max_bin': 24, \n#          'max_depth': 26, \n#          'min_data_in_leaf': 51, \n#          'min_sum_hessian_in_leaf': 7.765264256486626, \n#          'num_leaves': 54,                   \n#         }\n\n# data = pd.concat([x_train, x_test, data0]).reset_index(drop = True)\n# targets = np.zeros(x_train.shape[0] + x_test.shape[0] + data0.shape[0])\n# targets[x_train.shape[0] + x_test.shape[0]:] = 1\n\n# res = targets.copy()\n\n# skf = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 42)\n# for n, (tr, te) in enumerate(skf.split(targets, targets)):\n#     x_tr, x_val = data.values[tr], data.values[te]\n#     y_tr, y_val = targets[tr], targets[te]\n    \n#     lgb_tr = lgb.Dataset(x_tr, label = y_tr, categorical_feature = [0, 1, 2])\n#     lgb_val = lgb.Dataset(x_val, label = y_val, categorical_feature = [0, 1, 2])\n    \n#     clf = lgb.train(param, lgb_tr, 1000, [lgb_val], ['eval'], early_stopping_rounds = 25, \n#                     verbose_eval = 0, categorical_feature = [0, 1, 2])\n    \n#     res[te] = clf.predict(x_val)\n#     fold_score = roc_auc_score(y_val, res[te])\n#     print(f'Fold {n}:\\t', fold_score)\n    \n# oof_score = roc_auc_score(targets, res)\n# print('-' * 30)\n# print('OOF:\\t', oof_score)","426e6410":"# plt.hist(res)\n# plt.show()","81b30833":"# length = x_test.shape[0]\n# # length = 1000\n# res_tr = -res[targets == 1]\n# idx = res_tr.argsort()[-length:][::-1]\n# print(idx.shape)","87a46e63":"# np.save('res.npy', res)\n# data.to_csv('data_all.csv', index = False)","cfc702c0":"# def log_loss_numpy2(y_true2, y_pred2):\n#     loss2 = 0\n#     y_pred_clip2 = np.clip(y_pred2, 1e-15, 1 - 1e-15)\n#     for i in range(y_pred2.shape[1]):\n#         loss2 += - np.mean(y_true2[:, i] * np.log(y_pred_clip2[:, i]) + (1 - y_true2[:, i]) * np.log(1 - y_pred_clip2[:, i]))\n#     return loss2 \/ y_pred2.shape[1]","b7135027":"# for i in range(oof.shape[0]):\n#     print(log_loss_numpy2(y_true[idx], oof[i][idx]))","694af5ce":"# def Lagrange_func(params):\n#     w1, w2, w3, w4, w5, _lambda = params\n#     oof_blend = w1 * oof1 + w2 * oof2 + w3 * oof3 + w4 * oof4 + w5 * oof5\n#     return log_loss_numpy(oof_blend) - _lambda * (w1 + w2 + w3 + w4 + w5 - 1)","f999fa35":"# grad_L = grad(Lagrange_func)","2f668cda":"# def Lagrange_obj(params):\n#     w1, w2, w3, w4, w5, _lambda = params\n#     dLdw1, dLdw2, dLdw3, dLdw4, dLdw5, dLdlam = grad_L(params)\n#     return [dLdw1, dLdw2, dLdw3, dLdw4, dLdw5, w1 + w2 + w3 + w4 + w5 - 1]","77c62e85":"# start_time = time()\n# w1, w2, w3, w4, w5, _lambda = fsolve(Lagrange_obj, [0.1, 0.1, 0.3, 0.3, 0.2, 1.0])\n# print(f'[{str(datetime.timedelta(seconds = time() - start_time))[2:7]}] Optimised Weights:', [w1, w2, w3, w4, w5])\n# oof_b = w1 * oof1 + w2 * oof2 + w3 * oof3 + w4 * oof4 + w5 * oof5\n# print('Optimised Blend OOF:', log_loss_numpy(oof_b))","b67c85af":"# print('Check Condition (1a):', w1 + w2 + w3 + w4 + w5 + w6)\n# if w1 + w2 + w3 + w4 + w5 + w6 - 1 <= 1e-10:\n#     print('Great! The sum of all weights equals to 1!')\n# else:\n#     print('Manual adjustion is needed to modify the weights.')","27894ff7":"# Adversarial Validation","f2502aa8":"# Blending Weights Optimisation","e680853d":"# Scipy","246a877d":"# Model OOF Scores","8b3d6286":"# Objective Function","610a914d":"# Test"}}