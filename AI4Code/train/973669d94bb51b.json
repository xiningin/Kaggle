{"cell_type":{"3f173902":"code","7677ecc9":"code","059b9e27":"code","3daaa583":"code","5ffabebb":"code","db921564":"code","7190cf28":"code","da50c5d3":"code","b1c61bfe":"code","fd1fd2cc":"code","e3dc11a2":"code","7d707f96":"code","63fe2f42":"code","94c53b24":"code","541ed338":"code","4df9cf9c":"code","13183bc1":"code","2d9077c3":"code","2db7884d":"code","80878e64":"code","c8688f98":"code","d2cb659a":"code","592bc458":"code","2b2eb570":"code","896b55b7":"code","378bce42":"code","4f9aa8d9":"code","99eb1899":"code","d5d0be06":"code","d1af9835":"code","74267dfd":"code","0a472cb4":"code","8dc97a2c":"code","ddef5616":"code","e7295f8d":"code","ee92236a":"code","f4da0add":"markdown"},"source":{"3f173902":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7677ecc9":"traindf=pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv')\ntraindf.shape","059b9e27":"traindf.columns","3daaa583":"train_features=traindf.drop('label', axis=1)\ntrain_features.shape","5ffabebb":"train_label=traindf['label']","db921564":"train_features.head(5)","7190cf28":"#scale data\nfrom sklearn import preprocessing\nMinMaxScaler = preprocessing.MinMaxScaler()\ntrain_features = MinMaxScaler.fit_transform(train_features)","da50c5d3":"#lets reshape data to required dimension\ntrain_features = train_features.reshape(-1, 28, 28, 1)\n\ntrain_features.shape","b1c61bfe":"#load test data\ntestdf = pd.read_csv('\/kaggle\/input\/digit-recognizer\/test.csv')\n","fd1fd2cc":"testdf.shape","e3dc11a2":"#scale and reshaping test data\ntestdf = MinMaxScaler.fit_transform(testdf)\ntestdf = testdf.reshape(-1, 28, 28, 1)\ntestdf.shape","7d707f96":"#split train data for training\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(train_features, train_label.values, test_size=0.3, random_state=1)\n\n","63fe2f42":"#build categorical data from split\n\nfrom keras.utils.np_utils import to_categorical\n\ny_train_cat = to_categorical(y_train, 10)\ny_test_cat = to_categorical(y_test, 10)","94c53b24":"#lets call image data generator\n\nfrom keras.preprocessing.image import ImageDataGenerator\n\ngenerator = ImageDataGenerator(#rescale = 1.\/255,\n                               width_shift_range=0.1,\n                               height_shift_range=0.1,\n                               rotation_range = 20,\n                               shear_range = 0.3,\n                               zoom_range = 0.3,\n                               horizontal_flip = True)\ngenerator.fit(X_train)\n\n","541ed338":"#lest build model\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv2D, MaxPool2D, Flatten, Dropout,  BatchNormalization\nimport keras.backend as K\n\nmodel = Sequential()\n\n#model.add(generator())\n\nmodel.add(Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n#model.add(Conv2D(64, (3, 3), activation='tanh'))\nmodel.add(Conv2D(32, (3, 3), activation='relu'))\nBatchNormalization()\nmodel.add(MaxPool2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\n#model.add(Conv2D(128, (3, 3), activation='tanh'))\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\nBatchNormalization()\nmodel.add(MaxPool2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv2D(128, (3, 3), activation='relu'))\n#model.add(Conv2D(128, (3, 3), activation='relu'))\nBatchNormalization()\n\nmodel.add(MaxPool2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Flatten())\n\nmodel.add(Dense(784, activation='relu'))\nmodel.add(Dense(256, activation='relu'))\nBatchNormalization()\n\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.25))\n\nmodel.add(Dense(10, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\n\n","4df9cf9c":"#model summary\nmodel.summary()","13183bc1":"# Reduce learning by measuring \"validation accuracy\"\nfrom keras.callbacks import ReduceLROnPlateau\nlrd = ReduceLROnPlateau(monitor='val_acc', \n                                            patience=2, \n                                            verbose=1, \n                                            factor=0.1, \n                                            min_lr=0.00001)","2d9077c3":"#train model\nmodel.fit(generator.flow(X_train, y_train_cat, batch_size=1024), validation_data=(X_test, y_test_cat),\n          steps_per_epoch=len(X_train) \/ 1024, epochs=30, verbose=1, callbacks=[lrd])","2db7884d":"#lets save weights\ntrained_weights = model.get_weights()\n# assign trained model weights\nmodel.set_weights(trained_weights)","80878e64":"#model.fit(X_train, y_train_cat,\n          #validation = 0.3 , epochs=30, verbose=1)\nmodel.fit(X_train, y_train_cat,\n                      batch_size = 256,\n                      validation_data=(X_test, y_test_cat),\n                      epochs=50,\n                      verbose=1, callbacks=[lrd])","c8688f98":"model.evaluate(X_test, y_test_cat)","d2cb659a":"predictions = model.predict(testdf)","592bc458":"pred =  np.argmax(predictions, axis = 1)\npred.shape","2b2eb570":"sample = pd.read_csv('\/kaggle\/input\/digit-recognizer\/sample_submission.csv')","896b55b7":"output = pd.DataFrame({'ImageId': sample.ImageId, 'Label': pred})\noutput.to_csv('my_submission1.csv', index=False)\nprint(\"submission successfully saved!\")","378bce42":"import numpy as np\nimport pandas as pd\n\nnp.random.seed(0) \nimport random\n\nimport tensorflow.keras as keras\nfrom tensorflow.keras.models import Sequential, load_model\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten\nfrom tensorflow.keras.layers import Conv2D, MaxPool2D, BatchNormalization\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n\nfrom sklearn.model_selection import train_test_split\n","4f9aa8d9":"    train_aug = ImageDataGenerator( \n        featurewise_center = False,\n        samplewise_center = False,\n        featurewise_std_normalization = False, \n        samplewise_std_normalization = False,\n        zca_whitening = False,\n        horizontal_flip = False,\n        vertical_flip = False,\n        fill_mode = 'nearest',\n        rotation_range = 10,  \n        zoom_range = 0.1, \n        width_shift_range = 0.1, \n        height_shift_range = 0.1)\n    \n    \ntrain_aug.fit(X_train)\ntrain_gen = train_aug.flow(X_train, y_train_cat, batch_size=64)\ninput_shape = (28, 28, 1)","99eb1899":"def get_newtriplecnn():\n    return Sequential([\n        Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same', input_shape = input_shape),\n        Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same'),\n        BatchNormalization(),\n        #MaxPool2D(pool_size=(2, 2)),\n        #Dropout(0.3),\n        \n        Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same'),\n        Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same' ),\n        BatchNormalization(),\n        #MaxPool2D(pool_size=(2, 2)),\n        #Dropout(0.3),\n        \n        Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same' ),\n        Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same' ),\n        BatchNormalization(),\n        #MaxPool2D(pool_size=(2, 2)),\n        #Dropout(0.3),\n         \n        Flatten(),\n          \n        Dense(512, activation='relu'),\n        BatchNormalization(),\n        #Dropout(0.4),\n        \n        Dense(256, activation='relu'),\n        BatchNormalization(),\n        #Dropout(0.4),\n        \n        Dense(128, activation='relu'),\n        BatchNormalization(),\n        #Dropout(0.3),\n        \n        Dense(10, activation = \"softmax\")\n        \n    ])","d5d0be06":"model = get_newtriplecnn()\nmodel.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\nmodel.summary()","d1af9835":"callbacks1 = [ \n    EarlyStopping(monitor = 'loss', patience = 6), \n    ReduceLROnPlateau(monitor = 'loss', patience = 3), \n    ModelCheckpoint('..\/working\/model.best3.hdf5', save_best_only=True) # saving the best model\n]","74267dfd":"history = model.fit_generator((train_gen), epochs = 100, \n                               steps_per_epoch = X_train.shape[0] \/\/ 64,\n                               validation_data = (X_test, y_test_cat),\n                               callbacks = callbacks1,\n                             )\n\n","0a472cb4":"model = load_model('..\/working\/model.best3.hdf5')","8dc97a2c":"score = model.evaluate(X_test, y_test_cat, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])\n\nprint(\"CNN Error: %.2f%%\" % (100-score[1]*100))","ddef5616":"predictions = model.predict(testdf)\n\npred =  np.argmax(predictions, axis = 1)\npred.shape","e7295f8d":"sample=pd.read_csv(\"..\/input\/digit-recognizer\/sample_submission.csv\")","ee92236a":"output = pd.DataFrame({'ImageId': sample.ImageId, 'Label': pred})\noutput.to_csv('my_submission4.csv', index=False)\nprint(\"submission successfully saved!\")","f4da0add":"second model to achieve 100%"}}