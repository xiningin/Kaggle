{"cell_type":{"a885330a":"code","0671de3e":"code","5cd038b4":"code","fc8aee59":"code","2ce688ad":"code","7bc6cc87":"code","1f681887":"code","daafed06":"code","f100b8fa":"code","23bb7c71":"code","fba5c496":"code","622b626a":"code","56d4fc61":"code","bd3ce1ca":"code","af826a32":"code","4a02b42b":"code","40e46f1f":"code","382e7cdc":"code","6993a6d9":"code","637ba895":"code","8329599d":"code","cdaeeb97":"markdown","787b4ae4":"markdown","24fb38ee":"markdown","cd57915e":"markdown","3fe42825":"markdown","4b81b966":"markdown","031a3059":"markdown","7c5778b9":"markdown","a5cd1cdc":"markdown","be233158":"markdown","4e885b57":"markdown"},"source":{"a885330a":"# General \nimport os \nimport pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt \nimport pandas_profiling as pf\n\n\n#preprocessing\nfrom category_encoders import CatBoostEncoder\nfrom sklearn.preprocessing import OneHotEncoder as ohe\nfrom sklearn.model_selection import StratifiedKFold,train_test_split\nfrom sklearn.preprocessing import StandardScaler as Scaler\n\n\n#modelling\n#metrics\n\nfrom sklearn.metrics import f1_score as auc\n\n\nfrom catboost import CatBoostClassifier as cb\n\n#hyperparam optimization\nimport optuna\nfrom optuna import Trial\n\n#thresholding\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import confusion_matrix\n\nimport warnings \nwarnings.filterwarnings('ignore')\n","0671de3e":"# Loading Data \ntrain = pd.read_csv('..\/input\/jobathon-2021-analytics-vidya\/train.csv',date_parser =['MMM-YY','Dateofjoining'])\ntest  = pd.read_csv('..\/input\/jobathon-2021-analytics-vidya\/test.csv')\nsample_sub = pd.read_csv('..\/input\/jobathon-2021-analytics-vidya\/sample_submission_znWiLZ4.csv')","5cd038b4":"pf.profile_report.ProfileReport(train)","fc8aee59":"train['MMM-YY']= pd.to_datetime(train['MMM-YY'])\ntrain['Dateofjoining']= pd.to_datetime(train['Dateofjoining'])\ntrain['LastWorkingDate']= pd.to_datetime(train['LastWorkingDate'],errors='coerce')\n\n","2ce688ad":"def get_training_data(train):\n    \n    '''\n    For each unique Employee ID  \n    1) Get the first row for each employee Id \n    2) Save the last row designation as final designation\n    3) Last row salary as last salary .\n    4) Last row Quaterly ratings as last rating.\n    5) FRom last row for each employee ID , Months Since joining as months.\n    6) Perform Aggregation(min,max,avg) on these columns : Total Business Value,Salary, rating '''\n    df = train.copy()\n    \n    \n    groupby_obj = df.groupby('Emp_ID')\n    \n    \n    # step1 :\n    df1= groupby_obj.first().reset_index()\n    df1.drop('LastWorkingDate',axis=1,inplace=True)\n    \n    \n    # Step 2 \n    df2 = groupby_obj.last().reset_index()\n    df2.rename(columns = {'Salary':'LastSalary','Age':'LastAge','City':'LastCity','Designation':'LastDesignation','Quarterly Rating':'LastRating'},\n               inplace=True)\n    # months since joining \n    df2['days_joined'] =(df2['MMM-YY'] - df2['Dateofjoining']).dt.days\n    \n    # if last working date\n    cols = ['LastSalary','LastAge','LastCity','LastDesignation','LastRating','days_joined','LastWorkingDate','Emp_ID']\n     #subset on given cols\n    df2= df2[cols]\n    \n    \n    # step3 : aggregate features \n    \n    df3 = groupby_obj.agg(\n                       B_val_min = ('Total Business Value','min'),\n                       B_val__max = ('Total Business Value','max'),\n                       B_val__avg = ('Total Business Value','mean'),\n                       Sal_min    = ('Salary','min'),\n                       Sal_max    = ('Salary','max'),\n                       Sal_avg    = ('Salary','mean'),\n                       QRating_min   = ('Quarterly Rating','min'),\n                       QRating_max   = ('Quarterly Rating','max'),\n                       QRating_avg   =('Quarterly Rating','mean')\n                                                    ).reset_index()\n    \n    \n    # merge the created DFS \n    \n    df_final = df1.merge(df2,on='Emp_ID',how='inner')\n    \n    df_final = df_final.merge(df3,on='Emp_ID',how='inner')\n    \n    \n    # difference vars \n    # salary\n    df_final['Salary_increase'] = df_final['LastSalary'] - df_final['Salary']\n    \n    # age increase \n    df_final['age_inc'] = df_final['LastAge'] - df_final['Age']\n    \n    # designation increase \n    df_final['Des_inc'] = df_final['LastDesignation'] - df_final['Designation']\n    \n    df_final.drop(['LastSalary','LastAge','LastDesignation'],axis=1,inplace=True)\n    \n    return df_final\n\n    \n    \n    ","7bc6cc87":"# get training data \ntrain2 = get_training_data(train)","1f681887":"# Assiiging Target values\n\nattrition_index=train2[train2['LastWorkingDate'].notnull()].index\nnot_attrition_index= train2[train2['LastWorkingDate'].isnull()].index\n\ntrain2.loc[attrition_index,'Target'] = 1\ntrain2.loc[not_attrition_index,'Target'] = 0","daafed06":"# encode gender\ntrain2.Gender.replace({'Male':0,'Female':1},inplace=True)\n\n\n# Not encoding here \ncategorical_columns = ['Education_Level','City']\n\n\n# one hot encode education\n# ed=pd.get_dummies(train2['Education_Level'])\n\n# one hot encode cities\n# city = pd.get_dummies(train2['City'])\n\n#merge\n# train2=train2.merge(ed,right_index=True,left_index=True)\n# train2=train2.merge(city,right_index=True,left_index=True)","f100b8fa":"# columns to drop\ndrop_cols =['MMM-YY','LastWorkingDate','LastCity','Dateofjoining']\ntrain2.drop(drop_cols,axis=1,inplace=True)","23bb7c71":"[col for col in train2.columns if train2[col].dtype not in ['int64','float64','uint8']]","fba5c496":"# reset index \n\n\ntrain2.reset_index(inplace=True,drop=True)\n\nemp_ids= train2.pop('Emp_ID')\nX = train2.drop('Target',axis=1)\ny = train2.Target\n\nassert len(emp_ids)==len(X) == len(y), 'Lenghts Do not Match'","622b626a":"def objective(trial:Trial):\n    \n    #splitting training data \n    x_train,x_test,y_train,y_test=train_test_split(X,y,\n                                                   random_state=7,\n                                                   shuffle=True,\n                                                   train_size=0.6,\n                                                   stratify=y)\n    \n    #hyperparam_grid\n    params={   'verbose'        : 0,\n               'loss_function'  :'Logloss',\n               'depth'          :trial.suggest_int('depth',4,10),\n               'learning_rate'  :trial.suggest_loguniform('learning_rate', 1e-3, 1e-1),\n               'l2_leaf_reg'    :trial.suggest_loguniform('l2_leaf_reg', 1e-2, 10.0),\n               'random_strength':trial.suggest_uniform('random_strength',1e-2,0.3),\n               'max_bin'        :trial.suggest_int('max_bin',64,254),\n#                'grow_policy'    :trial.suggest_categorical('grow_policy',\n#                                                            ['SymmetricTree','Depthwise','Lossguide']),\n               'iterations'     :trial.suggest_int('iterations',200,1000),\n#                'max_leaves'     :trial.suggest_int('max_leaves',2,64),\n               \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.1, 0.8),\n               \"boosting_type\": trial.suggest_categorical(\"boosting_type\", [\"Ordered\", \"Plain\"]),\n               \"bootstrap_type\": 'MVS',\n#             trial.suggest_categorical(\"bootstrap_type\",\n#                                                            [\"Bayesian\", \"MVS\",'Bernoulli']),\n               'eval_metric': 'AUC'\n                }\n    \n    \n    try:\n        model = cb(**params)\n\n        model.fit(x_train,y_train,\n                 eval_set=[(x_test,y_test)],\n                 verbose=0,\n                 cat_features=categorical_columns,\n                 early_stopping_rounds=300)\n\n        \n        preds=model.predict(x_test)\n\n        acc = auc(y_test,preds)\n        \n        return acc\n\n    except Exception as e:\n        print(e)\n        return None","56d4fc61":"def get_best_params(time_out=3000):\n    '''time_out: time out in seconds'''\n    sampler = optuna.samplers.TPESampler(seed=7)  # Make the sampler behave in a deterministic way.\n    study=optuna.create_study(direction='maximize',sampler=sampler)\n    study.optimize(objective, n_trials=20, timeout=time_out)\n    \n    print(\"Number of finished trials: {}\".format(len(study.trials)))\n    \n    return study.best_trial.params\n\n\nbest_params=get_best_params()","bd3ce1ca":"best_params['verbose']       = 0\nbest_params['loss_function'] ='Logloss'\nbest_params[\"bootstrap_type\"]= 'MVS'\nbest_params['eval_metric']   = 'AUC'\nbest_params['iterations']    = 1000\n\n\nbest_params","af826a32":"def k_fold_predict(k,\n                  params=best_params):\n    '''predict mean probablity of Attrition using strtified K fold cross validation'''\n    \n    \n    skf=StratifiedKFold(n_splits=k,\n                       shuffle=True,\n                       random_state=7)\n    \n    mean_preds=np.zeros(shape=(X.shape[0]))\n    \n    \n    i=0\n    for train_idx,val_idx in skf.split(X,y):\n        x_t,x_v=X.iloc[train_idx],X.iloc[val_idx]\n        y_t,y_v=y.iloc[train_idx],y.iloc[val_idx]\n        \n        model=cb(**params)        \n        model.fit(x_t,y_t,\n                 cat_features=categorical_columns\n                 )\n        \n        \n        print('Validation F1 Score score for fold {} = {}'.format(i,\n                                                         auc(\n                                                            y_v,\n                                                            model.predict(x_v))))\n        i+=1\n        \n        #predictions\n        test_p=np.array([i[1] for i in model.predict_proba(X)])\n        mean_preds+=test_p\n        \n        \n        \n    mean_preds=mean_preds\/k \n    \n    return mean_preds","4a02b42b":"preds = k_fold_predict(10)","40e46f1f":"# assigning Ids to preds \npreds_df = pd.DataFrame(preds,columns=['target_proba'])\n\n\npreds_df['Emp_ID'] = emp_ids","382e7cdc":"def roc_threshold(y_true,y_pred):\n    '''get best threshold from a roc_auc_curve\n    y_true: ground truth\n    y_pred: predicted probabilities'''\n\n\n    fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n    # get the best threshold\n    J = tpr - fpr\n    ix = np.argmax(J)\n    best_thresh = thresholds[ix]\n    thresh_tpr=tpr[ix]\n    thresh_fpr=fpr[ix]\n    print(f'Best Threshold (TPR-FPR)= {best_thresh} \\n TPR :{thresh_tpr} \\n FPR :{thresh_fpr}')\n\n    #best threshold by geometric mean :\n    # calculate the g-mean for each threshold\n    gmeans = np.sqrt(tpr * (1-fpr))\n\n    # locate the index of the largest g-mean\n    ix1 = np.argmax(gmeans)\n    \n    \n    print('Best Threshold (geometric mean) = %f \\n G-Mean=%.3f' % (thresholds[ix1], gmeans[ix1]))\n\n    return best_thresh\n\nbest_thresh=roc_threshold(y,preds)","6993a6d9":"def get_bool(predictions_df,test_df,threshold):\n    '''get 1\/0 based on thresholds'''\n    \n    \n    df = predictions_df.copy()\n    \n    # getting predictions for each test Id \n    df = df.merge(test_df,on='Emp_ID',how='inner')\n    \n    \n    df['Target']=df['target_proba'].apply(lambda x: int(x>=threshold))\n    \n    df=df[['Emp_ID','Target']]\n    \n    return df\n\nsubmissions=get_bool(predictions_df=preds_df,\n                     test_df=test,\n                     threshold=0.4)","637ba895":"submissions['Target'].value_counts()","8329599d":"submissions.to_csv('sub.csv',index=False)","cdaeeb97":"# Simple Feature Engineering on data ","787b4ae4":"# Splitting the data into Predictor and target","24fb38ee":"# Deciding Threshold for prediction\n* we have predicted probablities for each class, we will have to convert them into boolean values.","cd57915e":"# Dropping other columns","3fe42825":"# Hyperparam Optimization ","4b81b966":"# Loading Data","031a3059":"# Getting a target variable \n* A target variable is not implicitly given in the train set. The target Variable is 1 if employee leaves the organisation.\n* From the given data, we can use \"LastWorkingDate\" to determine the target.","7c5778b9":"# Import Modules","a5cd1cdc":"# Simple data description","be233158":"**Encoding Categorical Columns**","4e885b57":"# More Feature Engineering"}}