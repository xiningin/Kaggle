{"cell_type":{"cff59a8e":"code","4671f918":"code","e564731a":"code","78d76de6":"code","d913db3b":"code","8a9eda70":"code","0501588f":"code","7537eacc":"code","6d7023f9":"code","0dfac4c1":"code","90f118be":"code","c67fb392":"code","bbbead48":"code","77f3dbad":"code","c3317b44":"code","9a46604f":"code","99086c3f":"code","2f7afca3":"code","be6dd8a7":"code","32e91306":"code","3f005e3d":"code","60fa8304":"code","52d4c768":"code","ff36ea64":"code","627b6b32":"code","90cdf685":"code","52dcfe39":"code","5ede81b3":"code","26273f17":"code","f8064fec":"code","19989c81":"code","639b3b90":"code","66c84ac9":"code","21d689f7":"markdown","1ae62cc3":"markdown","6f90ac62":"markdown","a86949e8":"markdown","20e80b6a":"markdown","cc912d36":"markdown"},"source":{"cff59a8e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4671f918":"import pandas as pd\nheart = pd.read_csv('\/kaggle\/input\/health-care-data-set-on-heart-attack-possibility\/heart.csv')\nheart.head()","e564731a":"age_sort=heart['age'].sort_values()\nage_sort.value_counts(ascending=True).plot(kind='bar',figsize=(10,10))\n","78d76de6":"Gender=heart['sex'].value_counts()\nprint(Gender)\nGender.plot(kind='bar', title='Count of Male and Female patients')","d913db3b":"#gender_male=heart.loc[heart['sex']==1]\n#gender_female=heart.loc[heart['sex']==0]\n\n#heart[(heart.sex == 1) & (heart.target == 1)].count()\ngrouping=heart.groupby(['sex','target'])['target'].agg(['count'])\ngrouping","8a9eda70":"grouping.plot(kind='bar')","0501588f":"#The analysis is based on the target class 0 = less chance of heart attack and the target class 1 = higher chance of heart attack..\nhigh_chance = heart.loc[heart['target']==1]\nless_chance = heart.loc[heart['target'] == 0]\nprint(\"Higher chance of Heart attack numbered \" + str(len(high_chance)))\nprint(\"Less chance of Heart attack numbered \" + str(len(less_chance)))","7537eacc":"heart['target'].value_counts().plot(kind='bar')","6d7023f9":"\nhighrisk = heart.loc[heart['target'] == 1]\nlowrisk = heart.loc[heart['target'] == 0]\n\nhigh,low = heart.target.value_counts()     #Finding the count of classes\n\nnormal_target_0_over = lowrisk.sample(high, replace=True)\nCombining_together_over = pd.concat([highrisk, normal_target_0_over], axis = 0)\n\nprint(Combining_together_over.target.value_counts())\nCombining_together_over.target.value_counts().plot(kind='bar', title = 'Data Over Sampling')","0dfac4c1":"#SEPERATING INDEPENDENT VARIABLES AND TARGET VARIABLE\n#REFERENCE = FROM WWW.KAGGLE.COM\/RENJITHMADHAVAN\/CREDIT-CARD-FRAUD-DETECTION-USING-PYTHON\nindependent_var = Combining_together_over.iloc[:, 0:13].columns\ntarget_var = Combining_together_over.iloc[:0, 13:].columns\nprint(independent_var)\nprint(target_var)\nfrom sklearn.model_selection import train_test_split\nindependent_var = Combining_together_over.iloc[:, 0:13].columns\ntarget_var = Combining_together_over.iloc[:0, 13:].columns\nprint(independent_var)\nprint(target_var)\n","90f118be":"data_independent = Combining_together_over[independent_var]\ndata_target = Combining_together_over[target_var]","c67fb392":"print(data_target)\nprint(data_independent)","bbbead48":"from sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix, roc_auc_score\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()\npredict_NB= cross_val_predict(gnb, data_independent, data_target, cv=10)\nconfmat_NB = confusion_matrix(data_target, predict_NB)","77f3dbad":"import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nClass_Names = ['LowRisk', 'HighRisk']\ndataframe = pd.DataFrame(confmat_NB, index=Class_Names, columns=Class_Names)\n# Create heatmap\nsns.heatmap(dataframe, annot=True, cbar=None, cmap=\"rocket_r\", fmt = 'g')\nplt.title(\"Confusion Matrix - Naive Bayes - Cross Validation\"), plt.tight_layout()\nplt.ylabel(\"True Class\"), plt.xlabel(\"Predicted Class\")\nplt.show()","c3317b44":"print(classification_report(data_target,predict_NB))\nprint(accuracy_score(data_target,predict_NB))\nprint(roc_auc_score(data_target,predict_NB))\nprint(f1_score(data_target,predict_NB))","9a46604f":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=5)\nclf_KNN = cross_validate(knn, data_independent,data_target, cv=10)\npredict_KNN= cross_val_predict(knn, data_independent, data_target, cv=10)\nconfmat_KNN = confusion_matrix(data_target, predict_KNN)","99086c3f":"Class_Names = ['Low Risk', 'High Risk']\ndataframe = pd.DataFrame(confmat_KNN, index=Class_Names, columns=Class_Names)\n# Create heatmap\nsns.heatmap(dataframe, annot=True, cbar=None, cmap=\"rocket_r\", fmt = 'g')\nplt.title(\"Confusion Matrix - K-Nearest Neighbours - Cross Validation\"), plt.tight_layout()\nplt.ylabel(\"True Class\"), plt.xlabel(\"Predicted Class\")\nplt.show()","2f7afca3":"print(classification_report(data_target,predict_KNN))\nprint(accuracy_score(data_target,predict_KNN))\nprint(roc_auc_score(data_target,predict_KNN))\nprint(f1_score(data_target,predict_KNN))","be6dd8a7":"from sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nAda_gbc = GradientBoostingClassifier()\nAdaboostgbc_classifier = AdaBoostClassifier(base_estimator = Ada_gbc, n_estimators=100, random_state=0)\npredict_Adagbc= cross_val_predict(Adaboostgbc_classifier, data_independent, data_target, cv=10)\nconfmat_Adagbc = confusion_matrix(data_target, predict_Adagbc)\n\nprint(classification_report(data_target,predict_Adagbc))\nprint(accuracy_score(data_target,predict_Adagbc))\nprint(roc_auc_score(data_target,predict_Adagbc))\nprint(f1_score(data_target,predict_Adagbc))\n","32e91306":"Class_Names = ['Low Risk', 'High Risk']\ndataframe = pd.DataFrame(confmat_Adagbc, index=Class_Names, columns=Class_Names)\n# Create heatmap\nsns.heatmap(dataframe, annot=True, cbar=None, cmap=\"rocket_r\", fmt = 'g')\nplt.title(\"Confusion Matrix - Adaboost Gradient Boost Classifier - Cross Validation\"), plt.tight_layout()\nplt.ylabel(\"True Class\"), plt.xlabel(\"Predicted Class\")\nplt.show()","3f005e3d":"from xgboost import XGBClassifier\nmodel = XGBClassifier()\npredict_xgb= cross_val_predict(model, data_independent, data_target, cv=10)\nconfmat_xgb = confusion_matrix(data_target, predict_xgb)\n\nprint(classification_report(data_target,predict_xgb))\nprint(accuracy_score(data_target,predict_xgb))\nprint(roc_auc_score(data_target,predict_xgb))\nprint(f1_score(data_target,predict_xgb))","60fa8304":"Class_Names = ['Low Risk', 'High Risk']\ndataframe = pd.DataFrame(confmat_xgb, index=Class_Names, columns=Class_Names)\n# Create heatmap\nsns.heatmap(dataframe, annot=True, cbar=None, cmap=\"rocket_r\", fmt = 'g')\nplt.title(\"Confusion Matrix - XGboost - Cross Validation\"), plt.tight_layout()\nplt.ylabel(\"True Class\"), plt.xlabel(\"Predicted Class\")\nplt.show()","52d4c768":"from sklearn.tree import DecisionTreeClassifier\n\nAda_dtc = DecisionTreeClassifier(random_state=0)\nAdaboostDTC_classifier = AdaBoostClassifier(base_estimator = Ada_dtc, n_estimators=100, random_state=0)\nAdaclf_dtc = cross_validate(AdaboostDTC_classifier, data_independent, data_target, cv=10)\npredict_Adadtc= cross_val_predict(AdaboostDTC_classifier, data_independent, data_target, cv=10)\nconfmat_Adadtc = confusion_matrix(data_target, predict_Adadtc)\n","ff36ea64":"Class_Names = ['Low Risk', 'High Risk']\ndataframe = pd.DataFrame(confmat_Adadtc, index=Class_Names, columns=Class_Names)\n# Create heatmap\nsns.heatmap(dataframe, annot=True, cbar=None, cmap=\"rocket_r\", fmt = 'g')\nplt.title(\"Confusion Matrix - Adaboost Decision Tree - Cross Validation\"), plt.tight_layout()\nplt.ylabel(\"True Class\"), plt.xlabel(\"Predicted Class\")\nplt.show()","627b6b32":"print(classification_report(data_target,predict_Adadtc))\nprint(accuracy_score(data_target,predict_Adadtc))\nprint(roc_auc_score(data_target,predict_Adadtc))\nprint(f1_score(data_target,predict_Adadtc))\n","90cdf685":"from sklearn.svm import SVC\nsvc =  SVC(kernel='linear', probability =True)\nsvc_cv = cross_validate(svc, data_independent, data_target, cv=10)\npredict_svc= cross_val_predict(svc, data_independent, data_target, cv=10)\nconfmat_svc = confusion_matrix(data_target, predict_svc)","52dcfe39":"Class_Names = ['Low Risk', 'High Risk']\ndataframe = pd.DataFrame(confmat_svc, index=Class_Names, columns=Class_Names)\n# Create heatmap\nsns.heatmap(dataframe, annot=True, cbar=None, cmap=\"rocket_r\", fmt = 'g')\nplt.title(\"Confusion Matrix - SVC- Cross Validation\"), plt.tight_layout()\nplt.ylabel(\"True Class\"), plt.xlabel(\"Predicted Class\")\nplt.show()","5ede81b3":"print(classification_report(data_target,predict_svc))\nprint(accuracy_score(data_target,predict_svc))\nprint(roc_auc_score(data_target,predict_svc))\nprint(f1_score(data_target,predict_svc))\n","26273f17":"from sklearn.svm import SVC\nfrom sklearn.ensemble import AdaBoostClassifier\nAdaboostSVM_classifier = AdaBoostClassifier(SVC(probability=True, kernel='linear'), n_estimators = 100, random_state=0)\nAdaclf_svm = cross_validate(AdaboostSVM_classifier, data_independent, data_target, cv=10)\npredict_Adasvm= cross_val_predict(AdaboostSVM_classifier, data_independent, data_target, cv=10)\nconfmat_Adasvm = confusion_matrix(data_target, predict_Adasvm)","f8064fec":"Class_Names = ['Low Risk', 'High Risk']\ndataframe = pd.DataFrame(confmat_Adasvm, index=Class_Names, columns=Class_Names)\n# Create heatmap\nsns.heatmap(dataframe, annot=True, cbar=None, cmap=\"rocket_r\", fmt = 'g')\nplt.title(\"Confusion Matrix - Adaboost Support Vector - Cross Validation\"), plt.tight_layout()\nplt.ylabel(\"True Class\"), plt.xlabel(\"Predicted Class\")\nplt.show()","19989c81":"print(classification_report(data_target,predict_Adasvm))\nprint(accuracy_score(data_target,predict_Adasvm))\nprint(roc_auc_score(data_target,predict_Adasvm))\nprint(f1_score(data_target,predict_Adasvm))\n","639b3b90":"Eval_result = pd.DataFrame({'Model': ['Naive Bayes','K-NearestNeighbour','ADAGradient Boost',\n                    'XGBClassifier','ADADecision Tree','Support Vector Machine','ADASVC'], 'Recall-HighRisk': [recall_score(data_target,predict_NB)*100,\n                    recall_score(data_target, predict_KNN)*100,recall_score(data_target, predict_Adagbc)*100,recall_score(data_target,predict_xgb)*100,recall_score(data_target,predict_Adadtc)*100,recall_score(data_target, predict_svc)*100,recall_score(data_target,predict_Adasvm)*100]})\nEval_result","66c84ac9":"import plotly.graph_objects as go\n\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=Eval_result['Model'], y=Eval_result['Recall-HighRisk'],line=dict(color='black', width=2, dash='dashdot')))","21d689f7":"The dataset consists of 165 High Risk Class with only 138 observation on Low Risk Class. When the dataset seems tohave a higher rate of transaction in one class compared to other, there comes the bias in the resulting measures. To rectify the class imbalance issue, \u201cOver-Sampling Technique\u201d has been implemented.   ","1ae62cc3":"Conclusion: - \n\nConsidering the main objective of the model predicting the 'High Risk' cases, SVC shows higher recall rate of 87.27% when compared with other models. SVC is the best model among the other classifiers shown in the experiment. ","6f90ac62":"Now both the Classes have been equally distributed and its ready to fit into the model.","a86949e8":"Objective:-\nModel predicting maximum High-Risk Heart Attack cases\n\nSampling Technique: -\nOver-Sampling Technique has been incorporated to equally distributing the sample and to avoid any bias in the results.\n\nEvaluation Criteria:- \nRecall Rate of the Class 1 - High Risk [True Positive\/True Positive+False Negative]\n\nClassification report has been given for each of the Models considered in this experiment. Precision, Recall and the F score for each of the models along with the Accuracy rate have been given in the Classification report. But here, I have considered the recall rate (True Positive\/True Positive+False Negative) of the 'High Risk' class as a evaluation criteria. Because Recall rate shows the performance of the model for example how the model performed in evaluating the True 'High-Risk' as 'High-Risk' and True 'High-Risk' as 'Low-Risk'.\n","20e80b6a":"So the maximum number of people involved in this examination are the age group of 58 (more than 18 numbers)","cc912d36":"In the Female category the individual who are at higher risk of getting heart attack are counted to 72 which is way higher than the count of low risk category (24). In the Male category, the count of lower risk is larger than the count of 'high risk'.\n\nFemale - 0 -category - out of 96 25% is on low risk category 75% is on high risk category\n\nMale -1 out of 207 55% is on low risk category 44.92% is on high risk category"}}