{"cell_type":{"f5d5e634":"code","fe2dddfa":"code","0cbcf559":"code","7208ab33":"code","9bc3b599":"code","49fd3e13":"code","04b270b4":"code","670ea409":"code","8283fd1b":"code","eb507fc5":"code","79ad45a0":"code","7ee4f88f":"code","51f62d05":"code","87c9e27d":"code","f6265c1d":"code","69ea077f":"code","b0d0d87f":"code","b7728d98":"code","59549a21":"code","0178e07c":"code","97921479":"code","a07301c0":"code","110343a0":"code","b723027f":"code","65244cd1":"code","588ea2ce":"code","1341bfea":"code","2c6d99e8":"code","01fc0627":"code","65b90fed":"code","bc469711":"code","9c62dfb2":"markdown","5c97947c":"markdown","2331aadc":"markdown","5303478e":"markdown","3a26c30b":"markdown","24c1e719":"markdown","59847b95":"markdown","23533c03":"markdown","211a73cd":"markdown","fe8729b2":"markdown","b63d6a92":"markdown","4b1178be":"markdown","99a6982f":"markdown","42e22b20":"markdown","43038c1d":"markdown","c1ffd513":"markdown","2809f025":"markdown","fcf73da0":"markdown","0d32fc21":"markdown","3339a6e1":"markdown","1ebb8680":"markdown","80fa8683":"markdown","3f014ac9":"markdown","04cb7a06":"markdown","cb417cf5":"markdown","92445ba9":"markdown","41c05c15":"markdown","48769548":"markdown","c84c53a4":"markdown","9d609197":"markdown","a05082ff":"markdown","40b87ab4":"markdown","b85ed940":"markdown","40f5a904":"markdown","a3ac761d":"markdown","20cce741":"markdown","7befc625":"markdown","7b58fd76":"markdown","d6396b4d":"markdown","20bbb085":"markdown","5f0ebcf9":"markdown","9a92aad6":"markdown","c3accc78":"markdown","8a43f2a5":"markdown","54c3c509":"markdown"},"source":{"f5d5e634":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","fe2dddfa":"# Just to read from google colab\n#from google.colab import files\n\n#uploaded = files.upload()","0cbcf559":"# Lectura de data (archivo .csv) y visualizaci\u00f3n\n\nfile = pd.read_csv('..\/input\/employee-turnover\/turnover.csv',encoding='ISO-8859-1')\ndf = pd.DataFrame(file)\ndf.head()","7208ab33":"#Informacion principal;\ndf.info()","9bc3b599":"#Informaci\u00f3n acerca de datos nulos\ndf.isnull().any()","49fd3e13":"# Identificamos las variables categoricas\n\ndf_objects = df.select_dtypes(include=['object']).copy() # La guardamos como variable para su posterior uso.\nprint(df_objects.columns.values,'\\n')","04b270b4":"# Luego vemos los valores \u00fanicos de tales variables\n\nfor i in df_objects.columns.values:\n    print(i,':', df[i].unique(),'\\n')","670ea409":"# Identificamos las variables num\u00e9ricas\n\ndf_numerical = df.select_dtypes(exclude=['object']).copy()\nprint(df_numerical.columns.values,'\\n')","8283fd1b":"# Rotaci\u00f3n de empleados\n\nevent = df['event'].value_counts()\nporcentaje = [round(i*100\/event.sum(),1) for i in df['event'].value_counts()]\nprint(event,'\\n')\nprint('Trabajadores que renuncian : %',porcentaje[0])\nprint('Trabajadores que se quendan : %',porcentaje[1])","eb507fc5":"# Promedios de valos n\u00famericos en funci\u00f3n de la variable 'event'\ndf.groupby('event').mean()","79ad45a0":"rows    = 3\ncolumns = 3\nc       = 1 # Inicializar plot counter\n\n# Histograma categorical variables\nfig = plt.figure(figsize=(20,20))\nfor i in df_objects.columns.values:\n\n    ax = plt.subplot(rows,columns,c)\n    pd.crosstab(df[i],df.event).plot(kind='bar',ax=ax)\n    plt.title('Frecuencia de rotaci\u00f3n por {}'.format(i))\n    plt.ylabel('Frecuencia de rotaci\u00f3n')\n    plt.xlabel('{}'.format(i))\n    plt.tight_layout(pad=4.0)\n    c = c + 1","7ee4f88f":"num_bins = 10\nhistograma = df.hist(bins=num_bins, figsize=(20,20))","51f62d05":"from sklearn.linear_model import LinearRegression, LogisticRegression\nfrom sklearn.feature_selection import RFE, RFECV\nfrom sklearn import metrics, preprocessing, model_selection\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.svm import SVC\n\nfrom sklearn.model_selection import StratifiedKFold, train_test_split,cross_val_score,ShuffleSplit\nfrom sklearn.ensemble import RandomForestClassifier","87c9e27d":"# Agreamos 1\/0 para las variables categ\u00f3ricas\n\nfor i in df_objects.columns.values:\n    object_list = 'var'+' '+ i\n    object_list = pd.get_dummies(df[i],prefix = i)\n    df_1 = df.join(object_list)\n    df = df_1","f6265c1d":"# A continuaci\u00f3n eliminamos las variables categ\u00f3ricas\ndf = df.drop(df_objects,axis=1)","69ea077f":"df_1 = df.drop('event', axis=1)  # Se elimina la variable objetivo ('event')\nX = df_1.values                  # Se define la matriz X\nY = df['event'].values           # Se define el vector Y","b0d0d87f":"scaler = preprocessing.StandardScaler()\nX2 = scaler.fit_transform(X)   ### Con este me quedo (MEAN NORMALIZATION)","b7728d98":"# PRIORIZANDO ACCURACY\n\nrfc = RandomForestClassifier()\nrfecv = RFECV(estimator= rfc, step= 1, cv=StratifiedKFold(10), scoring='accuracy')\nrfecv.fit(X2,Y)\n","59549a21":"print('N\u00famero \u00f3ptimo de features: {}'.format(rfecv.n_features_))\n\nplt.figure(figsize=(16, 9))\nplt.title('Recursive Feature Elimination with Cross-Validation', fontsize=18, fontweight='bold', pad=20)\nplt.xlabel('Number of features selected', fontsize=14, labelpad=20)\nplt.ylabel('% Correct Classification', fontsize=14, labelpad=20)\nplt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_, color='#303F9F', linewidth=3)\n\nplt.show()","0178e07c":"df_2 = df_1 # Se crea una variable auxiliar\ndf_2.drop(df_2.columns[np.where(rfecv.support_ == False)[0]], axis=1, inplace=True)","97921479":"dset = pd.DataFrame()\ndset['attr'] = df_2.columns\ndset['importance'] = rfecv.estimator_.feature_importances_\n\ndset = dset.sort_values(by='importance', ascending=False)\n\n\nplt.figure(figsize=(16, 14))\nplt.barh(y=dset['attr'], width=dset['importance'], color='#1976D2')\nplt.title('RFECV - Feature importantes', fontsize=20, fontweight='bold', pad=20)\nplt.xlabel('Importancia', fontsize=14, labelpad=20)\nplt.show()","a07301c0":"test_size = 0.3\nrandom_state = 42\n\nX_train, X_test, Y_train, Y_test = train_test_split(X2, Y, test_size= test_size, random_state= random_state)\n\nprint('Training Features Shape:', X_train.shape)\nprint('Training Labels Shape:', Y_train.shape)\nprint('Testing Features Shape:', X_test.shape)\nprint('Testing Labels Shape:', Y_test.shape)","110343a0":"model_logreg = LogisticRegression(max_iter=100, C= 10)\nmodel_logreg.fit(X_train, Y_train)\n\nprint('Precisi\u00f3n Logistic Regression: {:.3f}'.format(accuracy_score(Y_test, model_logreg.predict(X_test))))","b723027f":"model_rf = RandomForestClassifier(n_estimators= 100, random_state=10, max_depth=13)\nmodel_rf.fit(X_train,Y_train)\n\nprint('Precisi\u00f3n Random Forest: {:.3f}'.format(accuracy_score(Y_test, model_rf.predict(X_test))))","65244cd1":"model_svc = SVC(C=1)\nmodel_svc.fit(X_train,Y_train)\n\nprint('Precisi\u00f3n Support vector machine: {:.3f}'.format(accuracy_score(Y_test, model_svc.predict(X_test))))","588ea2ce":"kfold = model_selection.KFold(n_splits=20)\nmodelCV = RandomForestClassifier()\nscoring = 'accuracy'\nresults = model_selection.cross_val_score(modelCV, X_train, Y_train, cv=kfold, scoring=scoring)\nprint(\"Precisi\u00f3n promedio 10-fold cross validation: %.3f\" % (results.mean()))","1341bfea":"print(classification_report(Y_test, model_logreg.predict(X_test)))","2c6d99e8":"print(classification_report(Y_test, model_rf.predict(X_test)))","01fc0627":"print(classification_report(Y_test, model_svc.predict(X_test)))","65b90fed":"from sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\n\nlogit_roc_auc = roc_auc_score(Y_test, model_logreg.predict(X_test))\nfpr, tpr, thresholds = roc_curve(Y_test, model_logreg.predict_proba(X_test)[:,1])\n\nrf_roc_auc = roc_auc_score(Y_test, model_rf.predict(X_test))\nrf_fpr, rf_tpr, rf_thresholds = roc_curve(Y_test, model_rf.predict_proba(X_test)[:,1])\n\n\nplt.figure()\nplt.figure(figsize=(10, 10))\nplt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\nplt.plot(rf_fpr, rf_tpr, label='Random Forest (area = %0.2f)' % rf_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('Tasa False Positive')\nplt.ylabel('Tasa True Positive')\nplt.title('ROC')\nplt.legend(loc=\"lower right\")\nplt.savefig('ROC')","bc469711":"##### EVALUATING A LEARNING ALGORITHM\n## VALIDATION CURVES -> En funci\u00f3n de alg\u00fan \"hyperparameters\"\nfrom sklearn.model_selection import validation_curve, learning_curve\nfrom sklearn.naive_bayes import GaussianNB\n\ndef plot_learning_curve(estimator, title, X, y, axes=None, ylim=None, cv=None,\n                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n    \"\"\"\n    Generate 3 plots: the test and training learning curve, the training\n    samples vs fit times curve, the fit times vs score curve.\n\n    Parameters\n    ----------\n    estimator : estimator instance\n        An estimator instance implementing `fit` and `predict` methods which\n        will be cloned for each validation.\n\n    title : str\n        Title for the chart.\n\n    X : array-like of shape (n_samples, n_features)\n        Training vector, where ``n_samples`` is the number of samples and\n        ``n_features`` is the number of features.\n\n    y : array-like of shape (n_samples) or (n_samples, n_features)\n        Target relative to ``X`` for classification or regression;\n        None for unsupervised learning.\n\n    axes : array-like of shape (3,), default=None\n        Axes to use for plotting the curves.\n\n    ylim : tuple of shape (2,), default=None\n        Defines minimum and maximum y-values plotted, e.g. (ymin, ymax).\n\n    cv : int, cross-validation generator or an iterable, default=None\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n          - None, to use the default 5-fold cross-validation,\n          - integer, to specify the number of folds.\n          - :term:`CV splitter`,\n          - An iterable yielding (train, test) splits as arrays of indices.\n\n        For integer\/None inputs, if ``y`` is binary or multiclass,\n        :class:`StratifiedKFold` used. If the estimator is not a classifier\n        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validators that can be used here.\n\n    n_jobs : int or None, default=None\n        Number of jobs to run in parallel.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    train_sizes : array-like of shape (n_ticks,)\n        Relative or absolute numbers of training examples that will be used to\n        generate the learning curve. If the ``dtype`` is float, it is regarded\n        as a fraction of the maximum size of the training set (that is\n        determined by the selected validation method), i.e. it has to be within\n        (0, 1]. Otherwise it is interpreted as absolute sizes of the training\n        sets. Note that for classification the number of samples usually have\n        to be big enough to contain at least one sample from each class.\n        (default: np.linspace(0.1, 1.0, 5))\n    \"\"\"\n    if axes is None:\n        _, axes = plt.subplots(1, 3, figsize=(20, 5))\n\n    axes[0].set_title(title)\n    if ylim is not None:\n        axes[0].set_ylim(*ylim)\n    axes[0].set_xlabel(\"Training examples\")\n    axes[0].set_ylabel(\"Score\")\n\n    train_sizes, train_scores, test_scores, fit_times, _ = \\\n        learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,\n                       train_sizes=train_sizes,\n                       return_times=True)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    fit_times_mean = np.mean(fit_times, axis=1)\n    fit_times_std = np.std(fit_times, axis=1)\n\n    # Plot learning curve\n    axes[0].grid()\n    axes[0].fill_between(train_sizes, train_scores_mean - train_scores_std,\n                         train_scores_mean + train_scores_std, alpha=0.1,\n                         color=\"r\")\n    axes[0].fill_between(train_sizes, test_scores_mean - test_scores_std,\n                         test_scores_mean + test_scores_std, alpha=0.1,\n                         color=\"g\")\n    axes[0].plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n                 label=\"Training score\")\n    axes[0].plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n                 label=\"Cross-validation score\")\n    axes[0].legend(loc=\"best\")\n\n    # Plot n_samples vs fit_times\n    axes[1].grid()\n    axes[1].plot(train_sizes, fit_times_mean, 'o-')\n    axes[1].fill_between(train_sizes, fit_times_mean - fit_times_std,\n                         fit_times_mean + fit_times_std, alpha=0.1)\n    axes[1].set_xlabel(\"Training examples\")\n    axes[1].set_ylabel(\"fit_times\")\n    axes[1].set_title(\"Scalability of the model\")\n\n    # Plot fit_time vs score\n    axes[2].grid()\n    axes[2].plot(fit_times_mean, test_scores_mean, 'o-')\n    axes[2].fill_between(fit_times_mean, test_scores_mean - test_scores_std,\n                         test_scores_mean + test_scores_std, alpha=0.1)\n    axes[2].set_xlabel(\"fit_times\")\n    axes[2].set_ylabel(\"Score\")\n    axes[2].set_title(\"Performance of the model\")\n\n    return plt\n\n\nfig, axes = plt.subplots(3, 2, figsize=(10, 15))\n\ntitle = \"Learning Curves (Naive Bayes)\"\n# Cross validation with 100 iterations to get smoother mean test and train\n# score curves, each time with 20% data randomly selected as a validation set.\ncv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=0)\n\nestimator = GaussianNB()\nplot_learning_curve(estimator, title, X2, Y, axes=axes[:, 0], ylim=(0.0, 1.01),\n                    cv=cv, n_jobs=4)\n\ntitle = r\"Learning Curves (SVM, RBF kernel, $\\gamma=0.001$)\"\n# SVC is more expensive so we do a lower number of CV iterations:\ncv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\nestimator = SVC(gamma=0.001)\nplot_learning_curve(estimator, title, X2, Y, axes=axes[:, 1], ylim=(0.0, 1.01),\n                    cv=cv, n_jobs=4)\n\nplt.show()\n\n","9c62dfb2":"**Curva ROC**\n\n---\n\nEsta curva muestra la tasa de verdaderos positivos en funci\u00f3n de falsos positivos para distintos umbrales de clasificaci\u00f3n.\n\n","5c97947c":"* Con la funci\u00f3n ***.info()*** se puede obtener informaci\u00f3n general acercas de los *features* (columnas), tama\u00f1o de la muestra 1129x16,datos nulos (en este caso, no existen datos nulos), y adem\u00e1s, el tipo de variable que representa, en este caso, se pueden encontrar: *float64, int64 y object*, lo que se traduce a que existen variables n\u00famericas y categ\u00f3ricas.","2331aadc":"**Informaci\u00f3n general**\n\n---\n\n","5303478e":"Logistic Regression\n","3a26c30b":"Como la precisi\u00f3n promedio result\u00f3 ser bastante cercana a la predicci\u00f3n del modelo, se puede decir que este est\u00e1 bien adaptado.","24c1e719":"* ***isnull().any()*** Nos ayuda a corrobar la verificaci\u00f3n de datos nulos.","59847b95":"# II. An\u00e1lisis estad\u00edstico","23533c03":"Logistic Regression","211a73cd":"---\n* El an\u00e1lisis estad\u00edstico como los histogramas, nos brindan importantes observaciones a analizar a prior al modelo de predicci\u00f3n; d\u00e1ndonos una idea de que variables \"podrian\" tener mayor impacto en la rotaci\u00f3n de trabajadores. Para este caso se observa que las variables, tales como, **gender, industry y profession**, tienen patrones muy marcados en la rotaci\u00f3n. Adem\u00e1s, los histogramas nos dan informaci\u00f3n acerca de que tan sesgada es la data recolectada, lo cual no siempre es beneficioso para el modelo de predicci\u00f3n.\n\n* Con respecto a Machine learning los modelos arrojaron los siguientes resultados: Logistic regression 63.7%, Random Forest 69% y SVM 67%, con Random Forest como el modelo que m\u00e1s se ajusta la data. La elecci\u00f3n de estos fue debido a su bajo coste computacional (tiempo de ejecuci\u00f3n) y adem\u00e1s porque soy ampliamente utilizados. Si bien soy diferentes entre si, se infiere que pudieron adaptarse de manera simil a los datos entregados. Cabe destacar, que existen otros modelos m\u00e1s precisos y complejos, como Neural Network, pero computacionalmente es m\u00e1s costoso. \n\n  Volviendo a los resultados arrojados por los modelos de predicci\u00f3n, se observa que son bastante bajos (entre 60-70%), por lo que se debe analizar el desempe\u00f1o de estos con lo siguiente:\n\n* Evaluar tu algoritmo de aprendizaje\n    * Iniciando con la evaluaci\u00f3n de tu hip\u00f3tesis; que tu modelo tenga bajo error, no significa que es necesariamente una buen hip\u00f3tesis (ejemplo: overfitting). Para ello se utiliza la separaci\u00f3n de data (la cual se utiliz\u00f3 en este an\u00e1lisis).\n    * En segunda instancia, aumentar el grado de tu selecci\u00f3n de modelo y escoger la que tenga menor error.\n\n* An\u00e1lisis de Bias vs Variance\n    * Para minimizar el error de un modelo (ya sea, under\/over fitting), se diagn\u00f3stica como var\u00eda el error de tu modelo en funci\u00f3n de parametros, tales como, grado, regularizaci\u00f3n y cantidad de ejemplos.\n    * Si necesito ajustar la varianza, puedo agregar m\u00e1s datos o incrementar el parametro de regularizaci\u00f3n. Por otro lado, para arreglar el bias, es posible obtener m\u00e1s caracter\u00edsticas.\n\n  Con lo nombrado anteriormente, es posible disminuir el error y por ende aumentar la calidad del modelo. Lo ideal es encontrar un equilibrio entre bias y variance.  Si con estos an\u00e1lisis de modelos, no se logra un aumento significativo, cabe la opci\u00f3n es que los modelos seleccionados no se ajustan bien a la data (recordar que en esta existen variables n\u00famericas y categ\u00f3ricas) o tamb\u00eden cabe la posibilidad que los datos recolectados no tengan un correlaci\u00f3n con la tasa de rotaci\u00f3n.\n\n* Ahora, para evaluar el desempe\u00f1o de los modelos predictivos se utilizan los par\u00e1metros Precision\/Recall y F1 score. De los resultados entregados se observa que Random Forest obtuvo los mayores resultados (73%\/65%  Precision\/Recall, respectivamente. Y 69% F1 score), y Logistic regression lo m\u00e1s bajos (68%, 57% y 64%).\n\n  Cabe recordar que los par\u00e1metros Precision\/Recall funcionan como un trade-off (como se muestra en el gr\u00e1fico de ROC). Por lo que si quiero mejorar la precision del modelo (en este caso, predecir un empleado con riesgo de rotaci\u00f3n) debo sacrificar el recall (a modo general). Esto significa que se debe restringir las predicciones positivas a aquellas con mayor certeza en el modelo, lo que se traduce en predecir menos resultados positivos (con esto se refiere a que, si el modelo predice una cierta cantidad de rotaciones, lo m\u00e1s probable es que sea conrrectas). Lo que adem\u00e1s significa, que al tener un menor recall tendr\u00e9 menos aciertos de la realidad (no podr\u00e9 identificar a las personas con alto riesgo de rotaci\u00f3n antes de contratarla).\n\n* Por \u00faltimo se encontraron que las variables m\u00e1s relacionadas con la rotaci\u00f3n son: Los parametros de medici\u00f3n de salud (anxiety, extraversion, selfcontrol, novator, independ), age, stage, industry retail y profession_HR, los cuales tiene un rango de importancia entre 2 y 14%.","fe8729b2":"**Recursive Feature Elimination (RFE)**\n\n---\n\n\n\nSe utiliza este m\u00e9todo para escoger la cantidad de features m\u00e1s relevantes en el modelo predictivo.","b63d6a92":"**Conversi\u00f3n de variables**\n\n---\n\n\n\nPara poder iniciar el an\u00e1lisis de datos mediante machine learning, debemos tener una matriz solo de variables n\u00famericas; por lo que debe transformarse las variables categ\u00f3ricas en \"*dummy variables*\".\n\n\n","4b1178be":"**Importaci\u00f3n de librerias utilizadas**\n\n---\n\n","99a6982f":"En la gr\u00e1fica reciente se muestran dos modelos de predicci\u00f3n, los cuales est\u00e1n alejados de la curva roja (representa una curva pura aleatoria). Mientr\u00e1s m\u00e1s alejados de esta, mayor precision y recall presentar\u00e1 el modelo.","42e22b20":"Luego de calcular la precisi\u00f3n de los modelos, se hace uso de *Cross Validation* para poder generalizar el modelo y confirmar que est\u00e1 bien adaptado.","43038c1d":"#IV. Evaluaci\u00f3n de un algoritmo de aprendizaje\n\n\n\n\n","c1ffd513":"A continuaci\u00f3n configuramos las variables para poder ingresarlas a los modelos de predicci\u00f3n.","2809f025":"**Normalizaci\u00f3n de variables mediante**\n\n\n---\n\n\nSe normalizan los valores mediante *Mean Normalization* para evitar diferencias de ordenes de magnitud.","fcf73da0":"# III. Aplicaci\u00f3n a Machine Learning\n\n","0d32fc21":"**Aplicaci\u00f3n a modelos predictivos**\n\n---\n\n\n\nSe utilizaron 3 modelos de predicci\u00f3n: Logistic Regression, Random Forest y Support Vector Machine (SVM).\n\nPrimero que todo, se debe dividir los datos para alcanzar la m\u00e1xima precisi\u00f3n.","3339a6e1":"# Librerias importadas\n","1ebb8680":"**Muestro de varibles categ\u00f3ricas\/n\u00famericas**\n\n---\n\n","80fa8683":"Visualizaci\u00f3n de variables m\u00e1s importantes.","3f014ac9":"De los modelos estudiados, Random Forest es el que tiene un mayor precision, lo que indica que al momento de predecir la rotaci\u00f3n de un trabajador, este realmente abandona su trabajo. Por otro lado Loggistic regression present\u00f3 el menor valor. Lo mismo pasa con la variable Recall, que indica cuando un trabajador abandona, cuan a menudo el modelo lo predice correctamente.","04cb7a06":"**Estad\u00edsticas**\n\n---\n\n","cb417cf5":"Se observa que de los datos observados existe una mayoria de personas que renuncian a sus trabajos, siendo este del % 50.6.","92445ba9":"El objetivo de analizar los valores \u00fanicos es intentar reducir las categorias para un mejor modelado.","41c05c15":"# **Glosario de la data**\n\n* Stag        : Experiencia (tiempo).\n* Event       : Deserci\u00f3n de trabajo (1: Abandona, 0: Se queda).\n*    Gender      : Feminino(f), M\u00e1sculino(m).\n*    Age         : Edad del empleado (a\u00f1os).\n*    Industry    : Industria del empleado.\n*    Profession  : Profesi\u00f3n del empleado.\n*    Traffic     : Si el empleado vino por recomendaci\u00f3n, paginas, etc.\n*    Coach       : Presencia de un entrenador en el periodo de prueba (No,Yes,My head).\n*    Head gender : Sexo del supervisor.\n*    greywage    : White(salario m\u00ednimo) \/Gray (peque\u00f1a cantidad por encima del m\u00ednimo).\n*    Way         : Tranporte para ir a trabajar.\n*    Parametros de medici\u00f3n de salud: extraversion, selfcontrol, etc.\n---","48769548":"# Conclusiones\n\n\n","c84c53a4":"Ahora seleccionamos y eliminamos las variables menos importantes.","9d609197":"**Challenge Data Science - Genomawork**\n\n*Nombre: V\u00edctor Estrada*\n\nEl cliente Stark Industries se ha planteado el objetivo de mejorar sus contrataciones. Dentro de sus principales problemas, este identifica la rotaci\u00f3n de personal, es decir, los colaboradores que abandonan la compa\u00f1ia. Para ello, se ha recolectado datos de esta empresa, para poder analizar tal fen\u00f3meno.\n\n*   **Objetivo principal** : \n    * Hacer uso de t\u00e9cnicas de ciencias de datos para resolver problemas de la vida.\n\n* **Objectivos segundarios**: \n    * Manejo de datos (leer archivos .csv, filtrarlos, etc).\n    * Enfoque cient\u00edfico (planteamiento de hip\u00f3tesis y validaci\u00f3n o refutaci\u00f3n de datos).\n    * Uso de estad\u00edstica en problemas reales.\n    * Uso de herramienta de visualizaci\u00f3n de datos.\n    * T\u00e9cnicas de elecci\u00f3n y entrenamiento de modelos simples de machine learning.\n    * Capacidad de sintetizar los principales resultados.\n\n\n---\n\n\n\n","a05082ff":"SVM","40b87ab4":"Random Forest","b85ed940":"**Observaciones generales**\n(Repecto a la tabla anterior)\n* Las personas con menor tiempo en sus trabajos tienden a irse de ellos.\n* Los trabajadores m\u00e1s extrovertidos, independientes, novatos y con menos autocntrol tienden a renuciar.\n* La edad igual parece ser influente en la estad\u00eda de trabajo, mostr\u00e1ndose que las personas de mayor edad tienden a la estabilidad.\n\n\n\n\n\n\n","40f5a904":"Por \u00faltimo, es bueno representar las variables n\u00famericas en gr\u00e1ficos de histogramas.","a3ac761d":"---\n**Learning Curves**","20cce741":"A continuac\u00edon se hace una visualizaci\u00f3n de las variables categoricas en funci\u00f3n de la variable objetivo 'event'","7befc625":"---\n\nEl objetivo principal fue estudiar la tasas de rotaci\u00f3n mediante Machine learning dada una cierta base de datos con datos n\u00fameros y no n\u00famericos. Para ello se utilizaron distintos modelos de predicci\u00f3n los cuales dieron como resultado una exactitud entre 64-70%, lo cual parece bastante bajo; una de las posibilidades es que los modelos no se ajustan bien a este tipo de datos (datos cuantitativos y cualitativos) o bien los datos no tienen correlaci\u00f3n alguna con la rotaci\u00f3n de trabajadores. Una observaci\u00f3n importante a destacar es el sesgo de los datos; lo m\u00e1s \u00f3ptimo es distribuir de manera equitativa, a modo de ejemplo, se tiene la profesi\u00f3n HR, la cual tiene la mayoria de los datos.\n\nPara poder mejorar la predicci\u00f3n de los modelos se debe tener en consideraci\u00f3n lo anteriormente mencionado, y adem\u00e1s, lo ideal es obtener datos m\u00e1s influyentes en la toma decisi\u00f3n de rotaci\u00f3n de los empleados; a modo de sugerencia se pueden examinar las siguientes caracter\u00edstics: Salario (bajo-medio-alto), Promoci\u00f3n (aumento de sueldo\/cargo) y lejan\u00eda (distancia hogar-trabajo).","7b58fd76":"SVM","d6396b4d":"**Visualizaci\u00f3n de datos**\n\n---\n\n","20bbb085":"---\nPrimero que todo se importa la data (ya sea desde el computador, o en este caso, *google drive*; para luego hacer lectura de esta y visualizarla.","5f0ebcf9":"# Observaciones\n\n\n\n\n\n","9a92aad6":"**Precision\/Recall\/F1 score**\n\n---\n\n\n\nEstas variables nos ayudan a corrobar la calidad de las predicciones de los modelos establecidos.\n\n\n","c3accc78":"# I. Carga y visualizaci\u00f3n de los datos\n","8a43f2a5":"Random Forest","54c3c509":"De la gr\u00e1fica anterior se puede observar lo siguiente:\n* Las m\u00fajeres son m\u00e1s probables de abandonar su lugar de trabajo\n* Es claro que la frecuencia de rotaci\u00f3n depende en gran parte de la profesi\u00f3n e industria, por lo que pueden ser buenas variables para el modelo predictor."}}