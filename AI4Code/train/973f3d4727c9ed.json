{"cell_type":{"a7a645f4":"code","e2d98220":"code","02707b2c":"code","66061e7d":"code","2b9755f8":"code","e1ccc494":"code","d6472818":"code","26d831db":"code","85aac630":"code","355a0cd6":"code","7adfad19":"code","3282681f":"code","cf6b02c1":"code","bf1bac2f":"code","344fc369":"code","f61a24e8":"code","3eac5994":"code","a469bea7":"code","05a4adcb":"code","2cecf582":"markdown","93a87c73":"markdown","979ecfb9":"markdown","33975482":"markdown","4838d2ee":"markdown","369b018c":"markdown","25bbacb6":"markdown","55da7a28":"markdown","d3005dab":"markdown","5dfbdf63":"markdown","bb58d4e1":"markdown","8712cd59":"markdown","59e2f719":"markdown","cfb6f391":"markdown","06822487":"markdown","63878e4d":"markdown","316af17b":"markdown","7faa1305":"markdown"},"source":{"a7a645f4":"import time\nimport numpy as np\n\nimport jax\nimport jax.numpy as jnp\nfrom jax import random\nfrom jax import make_jaxpr\nfrom jax import vmap, pmap, jit\nfrom jax import grad, value_and_grad\nfrom jax.test_util import check_grads\n\n\n%config IPCompleter.use_jedi = False","e2d98220":"def product(x, y):\n    z = x * y\n    return z\n\n\nx = 3.0\ny = 4.0\n\nz = product(x, y)\n\nprint(f\"Input Variable x: {x}\")\nprint(f\"Input Variable y: {y}\")\nprint(f\"Product z: {z}\\n\")\n\n# dz \/ dx\ndx = grad(product, argnums=0)(x, y)\nprint(f\"Gradient of z wrt x: {dx}\")\n\n# dz \/ dy\ndy = grad(product, argnums=1)(x, y)\nprint(f\"Gradient of z wrt y: {dy}\")","02707b2c":"z, dx = value_and_grad(product, argnums=0)(x, y)\nprint(\"Product z:\", z)\nprint(f\"Gradient of z wrt x: {dx}\")","66061e7d":"# Differentiating wrt first positional argument `x`\nprint(\"Differentiating wrt x\")\nprint(make_jaxpr(grad(product, argnums=0))(x, y))\n\n\n# Differentiating wrt second positional argument `y`\nprint(\"\\nDifferentiating wrt y\")\nprint(make_jaxpr(grad(product, argnums=1))(x, y))","2b9755f8":"# Modified product function. Explicity stopping the\n# flow of the gradients through `y`\ndef product_stop_grad(x, y):\n    z = x * jax.lax.stop_gradient(y)\n    return z","e1ccc494":"# Differentiating wrt y. This should return 0\ngrad(product_stop_grad, argnums=1)(x, y)","d6472818":"def activate(x):\n    \"\"\"Applies tanh activation.\"\"\"\n    return jnp.tanh(x)\n\n\n# Check if we can compute the gradients for a single example\ngrads_single_example = grad(activate)(0.5)\nprint(\"Gradient for a single input x=0.5: \", grads_single_example)\n\n\n# Now we will generate a batch of random inputs, and will pass\n# those inputs to our activate function. And we will also try to\n# calculate the grads on the same batch in the same way as above\n\n# Always use the PRNG\nkey = random.PRNGKey(1234)\nx = random.normal(key=key, shape=(5,))\nactivations = activate(x)\n\nprint(\"\\nTrying to compute gradients on a batch\")\nprint(\"Input shape: \", x.shape)\nprint(\"Output shape: \", activations.shape)\n\ntry:\n    grads_batch = grad(activate)(x)\n    print(\"Gradients for the batch: \", grads_batch)\nexcept Exception as ex:\n    print(type(ex).__name__, ex)","26d831db":"grads_batch = vmap(grad(activate))(x)\nprint(\"Gradients for the batch: \", grads_batch)","85aac630":"make_jaxpr(vmap(grad(activate)))(x)","355a0cd6":"jitted_grads_batch = jit(vmap(grad(activate)))\n\nfor _ in range(3):\n    start_time = time.time()\n    print(\"Gradients for the batch: \", jitted_grads_batch(x))\n    print(f\"Time taken: {time.time() - start_time:.2f} seconds\")\n    print(\"=\"*50)\n    print()","7adfad19":"try:\n    check_grads(jitted_grads_batch, (x,),  order=1)\n    print(\"Gradient match with gradient calculated using finite differences\")\nexcept Exception as ex:\n    print(type(ex).__name__, ex)","3282681f":"x = 0.5\n\nprint(\"First order derivative: \", grad(activate)(x))\nprint(\"Second order derivative: \", grad(grad(activate))(x))\nprint(\"Third order derivative: \", grad(grad(grad(activate)))(x))","cf6b02c1":"# An example of a mathematical operation in your workflow\ndef log1pexp(x):\n    \"\"\"Implements log(1 + exp(x))\"\"\"\n    return jnp.log(1. + jnp.exp(x))","bf1bac2f":"# This works fine\nprint(\"Gradients for a small value of x: \", grad(log1pexp)(5.0))","344fc369":"# But what about for very large values of x for which the\n# exponent operation will explode\nprint(\"Gradients for a large value of x: \", grad(log1pexp)(500.0))","f61a24e8":"make_jaxpr(grad(log1pexp))(500.0)","3eac5994":"from jax import custom_jvp\n\n@custom_jvp\ndef log1pexp(x):\n    \"\"\"Implements log(1 + exp(x))\"\"\"\n    return jnp.log(1. + jnp.exp(x))\n\n@log1pexp.defjvp\ndef log1pexp_jvp(primals, tangents):\n    \"\"\"Tells JAX to differentiate the function in the way we want.\"\"\"\n    x, = primals\n    x_dot, = tangents\n    ans = log1pexp(x)\n    # This is where we define the correct way to compute gradients\n    ans_dot = (1 - 1\/(1 + jnp.exp(x))) * x_dot\n    return ans, ans_dot","a469bea7":"# Let's now compute the gradients for large values\nprint(\"Gradients for a small value of x: \", grad(log1pexp)(500.0))","05a4adcb":"# What about the Jaxpr?\nmake_jaxpr(grad(log1pexp))(500.0)","2cecf582":"If you take a closer look, you will notice that the computation is equivalent to this:\n\n<div align=\"center\">$\\frac{1}{1 + e^{x}} * e^{x}$<\/div><br>\n\nFor large values, the term on the right-hand side will be rounded off to `inf`, and the `grad` computation will return `nan` as we saw above. A human knows how to compute the gradient correctly in this case but JAX doesn't. It is working on the standard autodiff rules. So, how do we tell JAX that our function should be differentiated in the way we want? We can achieve this using `custom_vjp` or `custom_vjp` functions in JAX. Let's see it in action","93a87c73":"# Validate finite differences\nMany times we want to verify the computation of the gradients with finite differences to double-check if everything we did is right. Because this is a pretty-common sanity check while working with derivatives, JAX provides a convenient function `check_grads` that checks the finite differences to any order of gradients. Let's take a look","979ecfb9":"# Gradients\n\nI will try to include all the examples we saw in the [TensorFlow Autodiff Notebook](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part3) so that you can compare them both side by side and learn the differences between them\n\nThe `grad` function in JAX is used for computing the gradients. As we know that the basic idea behind JAX is to work with function compositions,`grad` also takes a callable as input and returns a callable. So, whenever we want to do the computation of the gradients, we need to pass a callable to `grad` first. Let's take an example to make it more clear","33975482":"Let's break down all the modifications we did above to achieve the desired results.\n\n1. `grad(activate)(...)` works for a single example\n2. Adding `vmap` composition adds a batch dimension (defaults to 0) to our inputs and outputs\n\nIts' that simple to go from a single example to a batch and vice-versa. All you need is to focus on using `vmap`. Let's see how the `jaxpr` for this transformation looks like","4838d2ee":"\n<img src=\"https:\/\/raw.githubusercontent.com\/google\/jax\/main\/images\/jax_logo_250px.png\" width=\"300\" height=\"300\" align=\"center\"\/><br>\n\nWelcome to another JAX tutorial. I hope you all have been enjoying the JAX Tutorials so far. If you haven't gone through the previous tutorials, I highly suggest going through them. Here are the links:\n\n1. [TF_JAX_Tutorials - Part 1](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part1)\n2. [TF_JAX_Tutorials - Part 2](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part2)\n3. [TF_JAX_Tutorials - Part 3](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part3)\n4. [TF_JAX_Tutorials - Part 4 (JAX and DeviceArray)](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part-4-jax-and-devicearray)\n5. [TF_JAX_Tutorials - Part 5 (Pure Functions in JAX)](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part-5-pure-functions-in-jax\/)\n6. [TF_JAX_Tutorials - Part 6 (PRNG in JAX)](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part-6-prng-in-jax\/)\n7. [TF_JAX_Tutorials - Part 7 (JIT in JAX)](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part-7-jit-in-jax)\n8. [TF_JAX_Tutorials - Part 8 (Vmap and Pmap)](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part-8-vmap-pmap)\n\n\nToday, we are going to look into another important concept `Automatic Differentiation`. We already have seen [automatic differentiation in TensorFlow](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part3). The idea of automatic differentiation is pretty similar in all the frameworks, but IMO JAX does it better than all of them.\nOne of the questions that people ask me generally is: If the framework already takes care of all autodiff parts, why should I bother learning about it? \n\nIt is important to learn these concepts because in order to implement something that is derived from these concepts or something that isn't provided out of the box,  then you need to have a clear understanding of the underlying mechanism first.\n\n**Note:** I have already covered the fundamentals of automatic differentiation in [this](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part3) notebook, hence I will skip those details here and we will directly work on examples","369b018c":"Let us break down the above example and try to understand the gradients calculation step by step.\n\n1. We have a function named `product(...)` that takes two-position arguments as input and returns the product of these arguments\n2. We pass the `product(...)` function to `grad`  to compute the gradients. The `argnums` argument in `grad` tells `grad` to differentiate the function wrt `ith` positional argument. Hence we pass `0` and `1` for calculating the gradients wrt `x` and `y` correspondingly.\n\nYou can also calculate the value of the function and the gradients in one go. For this we will use the `value_and_grad(...)` function","25bbacb6":"# Higher Order Gradients\n\n`grad` function takes a callable as an input and returns another function. We can compose the function returned by the transformation with `grad` again and again to compute higher-order derivates of any order. Let's take an example to see it in action. We will use our `activate(...)` function to demonstrate this","55da7a28":"# Gradients per sample\n\nIn reverse mode, the gradients are defined only for a function that outputs a scalar e.g. backpropagating on your loss value to update the parameters of your machine learning model. The loss is always a scalar value. What if your function returns a batch and you want to calculate the gradients per sample for that batch? \n\nThese things are pretty straightforward in JAX (thanks to `vmap` and `pmap`). In the next example, we will perform these steps in order:\n\n1. Write a function that takes an input and applies `tanh` on the input. This function is written in a way that works on a single example (Remember the philosophy behind `vmap` from the last tutorial?)\n2. We will check if we can compute the gradients on a single example\n3. We will then pass a batch of inputs and compute the gradients for the whole batch","d3005dab":"# References\n\n1. https:\/\/jax.readthedocs.io\/en\/latest\/notebooks\/autodiff_cookbook.html\n2. https:\/\/jax.readthedocs.io\/en\/latest\/jax-101\/04-advanced-autodiff.html","5dfbdf63":"Let's break down the steps we did to achieve the expected results.\n\n1. We decorated our `log1pexp(...)` with `custom_vjp` that computes the Jacobian-vector product (forward-mode)\n2. we then defined `log1pexp_jvp(...)` that defines how the gradients should be computed. Focus on this line of code in that function: `ans_dot = (1 - 1\/(1 + jnp.exp(x))) * x_dot`. Simply written, all we are doing is to rearrange the derivative in this way:\n\n<div align=\"center\">$\\frac{\\mathrm{d} }{\\mathrm{d} x}(log(1 + e^{x}) = 1 - \\frac {1} {1 + e^{x}}$<\/div><br>\n\n\n3. We decorate the `logp1exp_jvp(...)` function with `log1pexp.defjvp` to tell JAX that for calculating JVP, please consume the function we have defined and return the expected output\n\n\nThat's it for this tutorial folks! Many things were left out of this tutorial on purpose. For example, we didn't cover the forward-mode, and reverse-mode in detail because the scope of those concepts is outside of this notebook. If you want to understand those concepts, I highly suggest going through this [Advanced Autodiff Documentation](https:\/\/jax.readthedocs.io\/en\/latest\/jax-101\/04-advanced-autodiff.html). There is one last fundamental concept remaining for the JAX series that we will be covering in the upcoming tutorial.","bb58d4e1":"# Composition of other transformations\n\nWe can combine any other transformation with `grad`. We already saw `vmap` applied with `grad`. Let's apply `jit` to the above transformation to make it more efficient.","8712cd59":"# Gradients and numerical stability\n\n`Underflow` and `Overflow` are common problems that we run into many times especially while computing the gradients. We will take an example (this one is straight from the JAX docs, and it's a pretty good example) to illustrate how we can run into numerical instability and how JAX tries to aid you to overcome it.","59e2f719":"Woah! What just happened? Let's break it down to understand the expected output and what is gpoing on behind the scene in JAX that returned `nan`. We know that derivative of the above function can be written like this:\n\n<div align=\"center\">$\\frac{\\mathrm{d} }{\\mathrm{d} x}(log(1 + e^{x}) = \\frac {e^{x}} {1 + e^{x}}$<\/div><br>\n\nFor very large values, you would expect the value of the derivative to be 1 but when we combined `grad` with our function implementation, it returned `nan`. To get more insights, we can break down the gradients computation by looking at the `jaxpr` of the transformation ","cfb6f391":"What happens when you compute the gradients for some value?","06822487":"So what's the solution then? Well `vmap` and `pmap` is the solution to almost everything, Let's see it in action","63878e4d":"# Jaxprs and `grad`\n\nAs we can combine function transforms in JAX, we can make `jaxprs` from the grad function to understand what is going on behind the scene. Let's take an example.","316af17b":"Notice that the argument other than the one wrt which we are differentiating is a constant with a value of `1`.\n\n# Stopping Gradients computation\n\nSometimes we do not want the gradients to flow through some of the variables involved in a specific computation. In that case, we need to tell JAX explicitly that we don't want the gradients to flow through the specified set of variables. We will look into complex examples of this later on, but for now, I will modify our `product(...)` function where we do not want the gradients to flow through `y`","7faa1305":"**Update - 23rd Dec, 2021**\n\nWe have completed the TF-JAX tutorials series. 10 notebooks that covers every fundamental aspect of both TensorFlow and JAX. Here are the links to the notebooks along with the Github repo details:\n\n### TensorFlow Notebooks:\n\n* [TF_JAX_Tutorials - Part 1](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part1)\n* [TF_JAX_Tutorials - Part 2](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part2)\n* [TF_JAX_Tutorials - Part 3](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part3)\n\n### JAX Notebooks:\n\n* [TF_JAX_Tutorials - Part 4 (JAX and DeviceArray)](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part-4-jax-and-devicearray)\n* [TF_JAX_Tutorials - Part 5 (Pure Functions in JAX)](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part-5-pure-functions-in-jax\/)\n* [TF_JAX_Tutorials - Part 6 (PRNG in JAX)](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part-6-prng-in-jax\/)\n* [TF_JAX_Tutorials - Part 7 (JIT in JAX)](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part-7-jit-in-jax)\n* [TF_JAX_Tutorials - Part 8 (Vmap and Pmap)](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part-8-vmap-pmap)\n* [TF_JAX_Tutorials - Part 9 (Autodiff in JAX)](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part-9-autodiff-in-jax)\n* [TF_JAX_Tutorials - Part 10 (Pytrees in JAX)](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part-10-pytrees-in-jax)\n\n### Github Repo with all notebooks in one place\nhttps:\/\/github.com\/AakashKumarNain\/TF_JAX_tutorials\n\n---"}}