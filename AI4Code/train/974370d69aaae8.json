{"cell_type":{"c195daa7":"code","096cbf91":"code","117d7073":"code","a5af45ae":"code","a46e611e":"code","2c41ce79":"code","d85988b2":"code","d464c83f":"code","a8f9113a":"code","2ea0de59":"code","afb86bba":"code","10e74f86":"code","7912ac2a":"code","2a7fc219":"code","3fb0c704":"code","46950cd1":"code","a31ca6a7":"code","3dacf5eb":"code","53148b61":"code","5461d158":"code","9807f9fc":"code","9c9a4d94":"code","b905b3c0":"code","4d465441":"code","124f1709":"code","645e516f":"code","557d3557":"code","38632808":"code","e0927cc4":"code","469d1b28":"code","5b5874af":"markdown","b02f4ec9":"markdown","71b33843":"markdown","ae800d58":"markdown","7951178b":"markdown","74a564b6":"markdown"},"source":{"c195daa7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","096cbf91":"# !unzip \/kaggle\/input\/nyc-taxi-trip-duration\/test.zip","117d7073":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport math\nfrom geopy.distance import great_circle\nimport numpy as np\nimport gc\nimport datetime\nimport numpy as np\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nimport statsmodels.api as sm\n%matplotlib inline\nfrom sklearn.metrics import accuracy_score, mean_squared_log_error\nfrom collections import OrderedDict\nfrom sklearn.metrics import auc, accuracy_score, confusion_matrix, mean_squared_error\nfrom scipy.stats import uniform, randint","a5af45ae":"# def distance(lon1, lat1, lon2, lat2):\n#     pick_up = (lat1, lon1)\n#     drop_off = (lat2, lon2)\n#     return great_circle(pick_up, drop_off).miles\n\n# def reduce_mem_usage(df):\n#     \"\"\" iterate through all the columns of a dataframe and modify the data type\n#         to reduce memory usage.        \n#     \"\"\"\n#     #start_mem = df.memory_usage().sum() \/ 1024**2\n#     #print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n\n#     for col in df.columns:\n#         col_type = df[col].dtype\n\n#         if col_type != object:\n#             c_min = df[col].min()\n#             c_max = df[col].max()\n#             if str(col_type)[:3] == 'int':\n#                 if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n#                     df[col] = df[col].astype(np.int8)\n#                 elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n#                     df[col] = df[col].astype(np.int16)\n#                 elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n#                     df[col] = df[col].astype(np.int32)\n#                 elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n#                     df[col] = df[col].astype(np.int64)  \n#             else:\n#                 if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n#                     df[col] = df[col].astype(np.float16)\n#                 elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n#                     df[col] = df[col].astype(np.float32)\n#                 else:\n#                     df[col] = df[col].astype(np.float64)\n\n#     #end_mem = df.memory_usage().sum() \/ 1024**2\n#     #print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n#     #print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n\n#     return df","a46e611e":"# train_data = pd.read_csv('train.csv')\n\n# train_data = reduce_mem_usage(train_data)\n# train_data.info()","2c41ce79":"# train_data['distance'] = train_data.apply(lambda x: distance(x['pickup_longitude'], x['pickup_latitude'], x['dropoff_longitude'], x['dropoff_latitude']) , axis=1)\n# train_data.head()","d85988b2":"# train_data['dow'] = train_data['pickup_datetime'].apply(lambda x : datetime.datetime.strptime(x, '%Y-%m-%d %H:%M:%S').weekday())\n# train_data['hour'] = train_data['pickup_datetime'].apply(lambda x : datetime.datetime.strptime(x, '%Y-%m-%d %H:%M:%S').hour) \n# train_data.head()","d464c83f":"# v99 = train_data['trip_duration'].quantile(0.99)\n# train_data_v99 = train_data[train_data['trip_duration']<=v99]","a8f9113a":"# sns.set(style=\"ticks\", color_codes=True)\n# import matplotlib.pyplot as plt\n# g = sns.FacetGrid(train_data, col=\"vendor_id\", height=4)\n# g = g.map(plt.hist, \"trip_duration\")","2ea0de59":"# store_y = train_data[train_data[\"store_and_fwd_flag\"]==\"Y\"]\n# store_n = train_data[train_data[\"store_and_fwd_flag\"]==\"N\"]\n# store_y['trip_duration'].mean(), store_n['trip_duration'].mean()","afb86bba":"# store_y_v99 = store_y['trip_duration'].quantile(0.99)\n# store_n_v99 = store_n['trip_duration'].quantile(0.99)\n\n# sns.distplot(store_n[store_n['trip_duration']<store_n_v99]['trip_duration'])\n# sns.distplot(store_y[store_y['trip_duration']<store_y_v99]['trip_duration'])\n\n# plt.show()","10e74f86":"# dist_v99 = train_data['distance'].quantile(0.99)\n# train_dist_trip_v99 = train_data_v99[train_data_v99['distance'] < dist_v99] ","7912ac2a":"# train_dist_trip_v99['distance_1'] = train_dist_trip_v99['distance'].apply(lambda x : round(x * 2) \/ 2)\n# train_dist_trip_v99 = train_dist_trip_v99.groupby([\"distance_1\"])['trip_duration'].mean().to_frame(name = 'trip_dur').reset_index()\n# train_dist_trip_v99.head()\n","2a7fc219":"# ax = sns.scatterplot(x=\"distance_1\", y=\"trip_dur\", data=train_dist_trip_v99)","3fb0c704":"# train_data_dow_v99 = train_data_v99.groupby([\"dow\"])['trip_duration'].mean().to_frame(name = 'trip_dur').reset_index()\n# ax = sns.scatterplot(x=\"dow\", y=\"trip_dur\", data=train_data_dow_v99)","46950cd1":"# train_data_hour_v99 = train_data_v99.groupby([\"hour\"])['trip_duration'].mean().to_frame(name = 'trip_dur').reset_index()\n# ax = sns.scatterplot(x=\"hour\", y=\"trip_dur\", data=train_data_hour_v99)","a31ca6a7":"# bins = [-1, 3, 6, 9, 12, 15, 18, 21, 24]\n# labels = [1,2,3,4,5,6,7,8]\n# train_data_v99['hour_binned'] = pd.cut(train_data_v99['hour'], bins=bins, labels=labels)\n# train_data_v99.head()","3dacf5eb":"# train_data_hour_v99 = train_data_v99.groupby([\"hour_binned\"])['trip_duration'].mean().to_frame(name = 'trip_dur').reset_index()\n# ax = sns.scatterplot(x=\"hour_binned\", y=\"trip_dur\", data=train_data_hour_v99)","53148b61":"# plt.figure(figsize=(15,8))\n# train_data_v99[\"dow_hour_binned\"] = train_data_v99.apply(lambda x : f\"{x['dow']}_{x['hour_binned']}\", axis=1)\n# train_data_hour_dow_v99 = train_data_v99.groupby([\"dow_hour_binned\"])['trip_duration'].mean().to_frame(name = 'trip_dur').reset_index()\n# ax = sns.scatterplot(x=\"dow_hour_binned\", y=\"trip_dur\", data=train_data_hour_dow_v99)","5461d158":"# train_data_hour_v99 = train_data_v99.groupby(\"hour\").agg({'trip_duration': 'mean', 'id': 'count'})\n# train_data_hour_v99.head()\n","9807f9fc":"# train_data_passen_v99 = train_data_v99.groupby([\"passenger_count\"])['trip_duration'].mean().to_frame(name = 'trip_dur').reset_index()\n# ax = sns.scatterplot(x=\"passenger_count\", y=\"trip_dur\", data=train_data_passen_v99)","9c9a4d94":"train_data = pd.read_csv('..\/input\/featureadded\/train_feat_add.csv')","b905b3c0":"def display_scores(scores):\n    print(\"Scores: {0}\\nMean: {1:.3f}\\nStd: {2:.3f}\".format(scores, np.mean(scores), np.std(scores)))\n\ndef report_best_scores(results, n_top=3):\n    for i in range(1, n_top + 1):\n        candidates = np.flatnonzero(results['rank_test_score'] == i)\n        for candidate in candidates:\n            print(\"Model with rank: {0}\".format(i))\n            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n                  results['mean_test_score'][candidate],\n                  results['std_test_score'][candidate]))\n            print(\"Parameters: {0}\".format(results['params'][candidate]))\n            print(\"\")","4d465441":"train_data.head()","124f1709":"y = train_data['trip_duration']\n\nfeatures = ['vendor_id','passenger_count','pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','store_and_fwd_flag','distance','dow','hour']\nX = train_data[features]\ncategorical_features = ['store_and_fwd_flag']\nX = pd.get_dummies(X,columns =categorical_features)\nX.pickup_longitude = X.pickup_longitude*(-1)\nX.dropoff_longitude = X.dropoff_longitude*(-1)","645e516f":"X.head()","557d3557":"X_trim = X.reset_index(drop=True)\ny_trim = y.reset_index(drop=True)\n\ntrip_dur_v99 = train_data['trip_duration'].quantile(0.99)\ndistance_v99 = train_data['distance'].quantile(0.99)\n\nprint(f\"Originally X : {len(X_trim)}, Y : {len(y_trim)}\")\nX_trim = X_trim[list(y_trim)<=trip_dur_v99]\ny_trim = y_trim[list(y_trim)<=trip_dur_v99]\nprint(f\"After trip_duration X : {len(X_trim)}, Y : {len(y_trim)}\")\n\nX_trim = X_trim.reset_index(drop=True)\ny_trim = y_trim.reset_index(drop=True)\n\ny_trim = y_trim[(X_trim['distance']<=distance_v99)]\nX_trim = X_trim[(X_trim['distance']<=distance_v99)]\nprint(f\"After distance X : {len(X_trim)}, Y : {len(y_trim)}\")\n\nX_trim = X_trim.reset_index(drop=True)\ny_trim = y_trim.reset_index(drop=True)\n\ny_trim = y_trim[(X_trim['passenger_count']<=4)&(X_trim['passenger_count']>=1)]\nX_trim = X_trim[(X_trim['passenger_count']<=4)&(X_trim['passenger_count']>=1)]\nprint(f\"After passenger count X : {len(X_trim)}, Y : {len(y_trim)}\")","38632808":"xgb_model = xgb.XGBRegressor(objective=\"reg:squaredlogerror\", random_state=42, verbosity=3)\n\nxgb_model.fit(X_trim, y_trim, eval_metric='rmsle')\n\ny_pred = xgb_model.predict(X_trim)\n\nmse=mean_squared_log_error(y_trim, y_pred)\n\nprint(np.sqrt(mse))","e0927cc4":"xgb.plot_importance(xgb_model)","469d1b28":"xgb_model = xgb.XGBRegressor(objective=\"reg:squaredlogerror\", verbosity=2)\n\nparams = {\n    \"colsample_bytree\": uniform(0.7, 0.3),\n    \"gamma\": uniform(0, 0.5),\n    \"learning_rate\": uniform(0.03, 0.3), # default 0.1 \n    \"max_depth\": randint(2, 6), # default 3\n    \"n_estimators\": randint(100, 150), # default 100\n    \"subsample\": uniform(0.6, 0.4)\n}\n\nsearch = RandomizedSearchCV(xgb_model, param_distributions=params, random_state=42, n_iter=20, cv=3, verbose=1, n_jobs=1, return_train_score=True)\n\nsearch.fit(X_trim, y_trim, eval_metric='rmsle')\n\nreport_best_scores(search.cv_results_, 1)","5b5874af":"Hour and Day of week","b02f4ec9":"Random Search","71b33843":"Feature extraction","ae800d58":"### EDA","7951178b":"## Model","74a564b6":"Helper Functions"}}