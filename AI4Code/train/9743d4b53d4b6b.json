{"cell_type":{"a72dbbab":"code","8949d6af":"code","726547ac":"code","8a6f7d1a":"code","846cebab":"code","41b94260":"code","3d4c1f2a":"code","6c1dba54":"code","2a7fa96f":"code","b1d553a2":"code","d06faa2b":"code","3ccb832f":"code","a9e5cb1a":"code","90cb67fb":"code","5d9bf7c0":"code","e430f54f":"code","09d73983":"code","a8bfe5c3":"code","51f568f9":"code","b5c97556":"code","3eb98c13":"code","638ebabf":"code","cd9afe7a":"code","67987aab":"code","39e97a62":"code","50201200":"code","af2b8973":"code","2920eb97":"code","aff3c561":"code","5db47030":"code","e7ed7f31":"markdown","95d738aa":"markdown","0ebdec09":"markdown","53beca20":"markdown","d5583a6c":"markdown","0f9faac6":"markdown","b302bb24":"markdown","2a6faf85":"markdown","a5f95077":"markdown","5c18d162":"markdown","d35d15e7":"markdown","46b868d3":"markdown","e83eef7a":"markdown","5ed1692f":"markdown","5478cf28":"markdown","9f443816":"markdown","c2009720":"markdown","7c3b91ba":"markdown","d993be69":"markdown","23bc39da":"markdown","2e17ace2":"markdown","df5fa80c":"markdown","ff2d4bfa":"markdown"},"source":{"a72dbbab":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-whitegrid')\n\nfrom collections import Counter\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","8949d6af":"data = pd.read_csv(\"\/kaggle\/input\/health-care-data-set-on-heart-attack-possibility\/heart.csv\")","726547ac":"# Let's look at the first 5 rows of the data\ndata.head()","8a6f7d1a":"# last 5 rows.\ndata.tail()","846cebab":"# Column names in the data\ndata.columns","41b94260":"data.info()","3d4c1f2a":"def bar_plot(variable):\n    \n    # get feature\n    var = data[variable]\n    \n    # count number of the cateegorical variable (sample)\n    varValue = var.value_counts()\n    \n    # visualize\n    plt.figure(figsize = (10,10))\n    plt.bar(varValue.index,varValue)\n    plt.xticks(varValue.index,varValue.index.values)\n    plt.ylabel(\"Frequency\")\n    plt.title(variable)\n    plt.show()\n    print(\"{} : \\n {}\",variable,varValue)\n    ","6c1dba54":"categorical_variables = [\"sex\" ,\"cp\" , \"fbs\" , \"restecg\" , \"exang\" , \"slope\" , \"ca\" , \"thal\" ,\"target\"]\nfor c in categorical_variables:\n    bar_plot(c)","2a7fa96f":"def plot_hist(variable):\n    plt.figure(figsize = (10,10))\n    plt.hist(data[variable],bins = 75,color = \"green\")\n    plt.xlabel(variable)\n    plt.ylabel(\"Frequency\")\n    plt.title(\"{} distribution with hist\".format(variable))\n    plt.show()","b1d553a2":"numerical_variables = [\"age\" , \"trestbps\" , \"chol\" , \"thalach\" , \"oldpeak\"]\n\nfor x in numerical_variables:\n    plot_hist(x)","d06faa2b":"def detect_outliers(df,features):\n    outlier_indices = []\n    for c in features:\n        # 1 st quartile\n        Q1 = np.percentile(df[c],25)\n        \n        # 3 rd quartile\n        Q3 = np.percentile(df[c],75)\n        \n        # IQR\n        IQR = Q3 - Q1\n        \n        # Outlier step\n        outlier_step = IQR * 1.5\n   \n        # detect outlier and their indeces\n        outlier_list_col = df[(df[c] < Q1-outlier_step) | (df[c] > Q3 + outlier_step)].index\n        \n        # store indeces\n        outlier_indices.extend(outlier_list_col)\n\n    outlier_indices = Counter(outlier_indices)\n    multiple_outliers = list(i for i, v in outlier_indices.items() if v > 2)\n\n    return multiple_outliers","3ccb832f":"data.loc[detect_outliers(data,[\"age\",\"trestbps\",\"chol\",\"thalach\",\"oldpeak\"])]","a9e5cb1a":"data.columns[data.isnull().any()]","90cb67fb":"data.isnull().sum() # Here , how many missing values are in the dataset ?","5d9bf7c0":"data.head()","e430f54f":"y = data.target.values","09d73983":"# axis = 1 , which means column\n# axis = 0 , which means row\nx_data = data.drop([\"target\"],axis = 1)","a8bfe5c3":"x = (x_data - np.min(x_data))\/(np.max(x_data) - np.min(x_data)).values","51f568f9":"from sklearn.model_selection import train_test_split\nx_test,x_train,y_test,y_train = train_test_split(x,y,test_size = 0.2,random_state = 42)","b5c97556":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(x_train,y_train)\nprint(\"Test accuracy: {}\",lr.score(x_test,y_test))","3eb98c13":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=3) # n_neighbors = k value\nknn.fit(x_train,y_train)\nprediction = knn.predict(x_test)\nprint(\"KNN score: {}\".format(knn.score(x_test,y_test)))","638ebabf":"# visualize to find best K value\nscore_list = []\n\nfor each in range(1,61):\n    knn2 = KNeighborsClassifier(n_neighbors=each)\n    knn2.fit(x_train,y_train)\n    score_list.append(knn2.score(x_test,y_test))\n\n\nplt.plot(range(1,61),score_list)\nplt.title(\"K-Value & Accuracy\")\nplt.xlabel(\"K-Value\")\nplt.ylabel(\"Accuracy\")\nplt.show()","cd9afe7a":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=6) # n_neighbors = k value\nknn.fit(x_train,y_train)\nprediction = knn.predict(x_test)\nprint(\"KNN score: {}\".format(knn.score(x_test,y_test)))","67987aab":"from sklearn.svm import SVC\nsvm = SVC(random_state = 1) # random state => to randomly divide at the same rate each time.\nsvm.fit(x_train,y_train)\n\nprint(\"Accuracy of the Support Vector Machines : \",svm.score(x_test,y_test))","39e97a62":"from sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(x_train,y_train)\nprint(\"Accuracy of the Naive Bayes Classification: \",nb.score(x_test,y_test))","50201200":"from sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier()\ndt.fit(x_train,y_train)\nprint(\"Accuracy of the Decision Tree Classification: \",dt.score(x_test,y_test))\n","af2b8973":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators = 100,random_state = 1) # n_estimators = number of trees\nrf.fit(x_train,y_train)\n\nprint(\"Accuracy of the Random Forest Classification: \",rf.score(x_test,y_test))","2920eb97":"y_pred = rf.predict(x_test)\ny_true = y_test","aff3c561":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_true,y_pred)","5db47030":"import seaborn as sns\n\nf,ax = plt.subplots(figsize=(15,15))\nsns.heatmap(cm,annot = True,linewidths = 0.5,linecolor = \"green\",fmt = \".0f\",ax = ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show()","e7ed7f31":"<a id = '10' ><a><br>\n## Normalization Operation\n* To scale from 0 to 1","95d738aa":"<a id = '15' ><a><br>\n# Naive Bayes Classification","0ebdec09":"<a id = '19' ><a><br>\n## Visualize of the Confusion Matrix","53beca20":"When we look at this graph , best k value is 6.\nLet's do it again...","d5583a6c":"<a id = '4' ><a><br>\n## Categorical Variables","0f9faac6":"As you can see , our accuracy decreased.We can incerasing accuracy by changing the k value (n_neighbors)","b302bb24":"<a id = '12' ><a><br>\n# Logistic Regression Classfication","2a6faf85":"<a id = '3' ><a><br>\n## Univarite Variable Analysis\n\n* Categorical Variables = sex ,cp , fbs , restecg , exang , slope , ca , thal ,target\n \n* Numerical Variables = age , trestbps , chol , thalach , oldpeak","a5f95077":"<a id = '1' ><a><br>\n## Load and Check Data","5c18d162":"<a id = '13' ><a><br>\n## K-Nearest Neighbour (KNN) Classification","d35d15e7":"<a id = '9' ><a><br>\n## X and Y Coordinates","46b868d3":"<a id = '11' ><a><br>\n# Train - Test Split","e83eef7a":"<a id = '14' ><a><br>\n# Support Vector Machines","5ed1692f":"* int64(13) = age , sex , cp , trestbps , chol ,fbs ,restecg , thalach , exang , slope , ca , thal , target\n* float64(1) = oldpeak","5478cf28":"# INTRODUCTION\n\n\n\n1 - [Load and Check Data](#1)\n\n2 - [Variable Description](#2)\n     \n3 - [Univarite Variable Analysis](#3)\n        \n   * [Categorial Variable](#4)\n   * [Numerical Variable](#5)\n\n4 - [Outlier Detection](#6)\n\n5 - [Missing Values](#7)\n    \n   * [Find Missing Values](#8)\n   * [Fill Missing Values](#9)\n\n6 - [X and Y coordinates](#10)\n\n7 - [Normalization Operation](#11)\n\n8 - [Train - Test Split](#12)\n\n9 - [Logistic Regression Classfication](#13)\n\n10 - [K-Nearest Neighbour (KNN) Classification](#14)\n\n10 - [Support Vector Machines](#15)\n\n11 - [Naive Bayes Classification](#16)\n\n12 - [Decision Tree Classification](#17)\n\n13 - [Random Forest Classification](#18)\n\n14 - [Confusion Matrix](#19)\n    \n   * [Visualize of the Confusion Matrix](#20)\n","9f443816":"<a id = '6' ><a><br>\n## Outlier Detection","c2009720":"<a id = '7' ><a><br>\n## Find Missing Values","7c3b91ba":"<a id = '18' ><a><br>\n# Confusion Matrix","d993be69":"<a id = '5' ><a><br>\n## Numerical Variables","23bc39da":"<a id = '17' ><a><br>\n# Random Forest Classification","2e17ace2":"<a id = '8' ><a><br>\n# Fill Missing Values\n* Dataset hasn't any missing value.","df5fa80c":"<a id = '2' ><a><br>\n## Variable Description\n\n1) age = age of the patient\n\n2) sex = gender of the patient\n\n3) cp = chest pain type (4 values) \n\n4) trestbps = resting blood pressure\n\n5) chol = serum cholestoral in mg\/dl\n\n6) fbs = fasting blood sugar > 120 mg\/dl\n\n7) restecg = resting electrocardiographic results (values 0,1,2)\n\n8) thalach =  maximum heart rate achieved\n\n9) exang = exercise induced angina\n\n10) oldpeak = ST depression induced by exercise relative to rest\n\n11) slope = the slope of the peak exercise ST segment\n\n12) ca = number of major vessels (0-3) colored by flourosopy\n\n13) thal: 0 = normal; 1 = fixed defect; 2 = reversable defect\n\n14) target --->   0 = less chance of heart attack ,  1 = more chance of heart attack","ff2d4bfa":"<a id = '16' ><a><br>\n# Decision Tree Classification\n    "}}