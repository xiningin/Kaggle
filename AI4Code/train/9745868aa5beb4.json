{"cell_type":{"5a173a60":"code","b25ead15":"code","d85a545c":"code","ea0a0708":"code","9a1b87af":"code","abfb6a47":"code","5e49c6f2":"code","6ecd77e7":"markdown"},"source":{"5a173a60":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Read the data\nX_full = pd.read_csv('..\/input\/housingpricesdata\/train.csv', index_col='Id')\nX_test_full = pd.read_csv('..\/input\/housingpricesdata\/test.csv', index_col='Id')\n\ny = X_full.SalePrice\nX_full.drop(['SalePrice'], axis=1, inplace=True)\n\n# Break off validation set from training data\nX_train_full, X_valid_full, y_train, y_valid = train_test_split(X_full, y, \n                                                                train_size=0.8, test_size=0.2,\n                                                                random_state=0)\n\n# Select categorical columns with low cardinality (<10)\ncategorical_cols = [cname for cname in X_train_full.columns if\n                    X_train_full[cname].nunique() < 10 and \n                    X_train_full[cname].dtype == \"object\"]\n\n# Select numerical columns\nnumerical_cols = [cname for cname in X_train_full.columns if \n                X_train_full[cname].dtype in ['int64', 'float64']]\n\n# Keep selected columns only\nmy_cols = categorical_cols + numerical_cols\nX_train = X_train_full[my_cols].copy()\nX_valid = X_valid_full[my_cols].copy()\nX_test = X_test_full[my_cols].copy()","b25ead15":"from IPython.display import display\n\n# lets take a look at the shape of full, train and test data\ndisplay(X_full.shape, X_train.shape, X_test.shape)\n\n# Three categorical columns (Neighborhood, Exterior1st, Exterior2nd) were removed\n# from X_train and X_test because they had high cardinality (>9).\ndisplay(X_full.select_dtypes(include=['object']).nunique())","d85a545c":"# lets take a look at NA values in each column of X_train\nseries1 = X_train.isnull().sum()\ndisplay(series1[series1>0])\n\n# lets take a look at NA values in each column of X_test\nseries2 = X_test.isnull().sum()\ndisplay(series2[series2>0])","ea0a0708":"# Data preprocessing\n# Impute missing values (NA) in numerical and categorical columns\n# OneHoteEncode categorical columns\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\n\n# Preprocessing for numerical data\nnumerical_transformer = SimpleImputer(strategy='median')\n\n# Preprocessing for categorical data\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# Bundle preprocessing for numerical and categorical data\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_cols),\n        ('cat', categorical_transformer, categorical_cols)\n    ])","9a1b87af":"# Define Random Forest model\nmodel_1 = RandomForestRegressor(n_estimators=500, random_state=0)\n\n# Bundle preprocessing and modeling code in a pipeline\nclf = Pipeline(steps=[('preprocessor', preprocessor), ('model', model_1)])\n\n# Preprocessing of training data, fit model \nclf.fit(X_train, y_train)\n\n# Preprocessing of validation data, get predictions\npreds = clf.predict(X_valid)\n\nprint('MAE:', mean_absolute_error(y_valid, preds))","abfb6a47":"# Define XGBoost model\nfrom xgboost import XGBRegressor\n\nmodel_2 = XGBRegressor(n_estimators=1000, learning_rate=0.05, random_state=0)\nclf = Pipeline(steps=[('preprocessor', preprocessor), ('model', model_2)])\nclf.fit(X_train, y_train)\npreds = clf.predict(X_valid)\nprint('MAE:', mean_absolute_error(y_valid, preds))","5e49c6f2":"# Preprocessing of test data, fit model, get predictions\npreds_test = clf.predict(X_test)\n\n# Save test predictions to file\noutput = pd.DataFrame({'Id': X_test.index, 'SalePrice': preds_test})\noutput.to_csv('submission.csv', index=False)","6ecd77e7":"In this Kaggle competition, the goal is to predict the final price of a house based on 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa.\n\nI tried two most common regression techniques:\n1. Random Forest using sklearn.ensemble.RandomForestRegressor()\n2. Gradient Boosting using xgboost.XGBRegressor()\n\nAfter some parameter tuning, I found that Gradient Boosting gave better results (lower MAE, see below).\n\nI applied my optimized XGBoost model on the test data and uploaded results (submission.csv file) on Kaggle competitions site and found that my predictions ranked in the top **7%** among more than 65,000 submissions."}}