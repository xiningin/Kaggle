{"cell_type":{"c134f418":"code","6817061f":"code","e74e5820":"code","7cd50387":"code","f5a590d8":"code","5b6b2fcf":"code","2f73bff1":"code","b46e5025":"code","28cf5b7f":"code","5417c469":"code","be85e253":"code","7d16557e":"code","ab04d6e6":"code","292deba1":"code","9e3be4a3":"code","2842835c":"code","79de4773":"code","ee0e3d69":"code","692fb8e5":"code","d18a3968":"code","1a971605":"code","23eb3f1f":"code","198e7f71":"code","9188eab7":"code","96676bf0":"code","e58b5e68":"code","25f87b69":"code","c7d2c464":"code","f0894793":"code","d8544db5":"code","a4c38b62":"code","62565bf8":"code","883323a2":"code","249c9247":"code","e321af37":"code","55adb218":"code","7d6e91b1":"code","a8ee3a83":"code","b63de327":"code","dcea0dee":"code","47dde05b":"code","edae292d":"code","21047264":"code","943b2996":"code","9d43197b":"code","bdadea5f":"code","f630a27c":"code","92a867b7":"code","f18b9340":"code","48e4690d":"code","5f0c9f5e":"code","2d582c9f":"code","79230bb7":"code","3cf4e448":"code","ec10e5cc":"code","9d30ddf6":"code","ca94cb0c":"code","30e49333":"code","f8a5c837":"code","0de3d106":"code","95e727df":"code","0d642da8":"code","f2119253":"code","1dff3710":"code","59372b47":"markdown","f4cdbc50":"markdown","e1e59120":"markdown","b465f24c":"markdown","12912455":"markdown","adcfff58":"markdown","ab0a8c3c":"markdown","5e5bc32c":"markdown","fd07e9c1":"markdown","b0c0a117":"markdown","19f09b80":"markdown","9683d957":"markdown","f075e2e8":"markdown","b8d30e90":"markdown","a101ac6d":"markdown","37f05cba":"markdown","ad64991a":"markdown","64b46c82":"markdown","f86b72f3":"markdown","d5a1baa1":"markdown","2da4a254":"markdown","308ee6ef":"markdown","002f1b4b":"markdown","3d97893a":"markdown","b3534819":"markdown","f78f7a31":"markdown","04b4944f":"markdown","2578b332":"markdown","cf7069e4":"markdown","6f269c7e":"markdown","83b46dac":"markdown","80e2f9d9":"markdown","1d4ceb22":"markdown","f7f72104":"markdown","88928e27":"markdown","c2d56ebc":"markdown","a2aeb4c4":"markdown","b3d4c2e0":"markdown","8cf7c1c9":"markdown","c6cedf34":"markdown","79b25909":"markdown"},"source":{"c134f418":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n#To display the rows and columns without getting truncated\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\npd.set_option('display.max_colwidth', -1) # this is to view complete text data in the column rather truncated","6817061f":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","e74e5820":"import os\n\npath = os.path.join(dirname)\nos.chdir(path)\n\ntrain_data = pd.read_csv('train.csv',encoding = 'cp1252', index_col= False)\ntest_data = pd.read_csv('test.csv',encoding = 'cp1252', index_col= False)\n\ntrain_data.head(10)","7cd50387":"train_data.info()","f5a590d8":"train_data.describe(include = 'all')","5b6b2fcf":"train_data.shape","2f73bff1":"# percentage of missing values in each column\nround(train_data.isnull().sum()\/len(train_data.index), 2)*100","b46e5025":"# Imputing Missing values in Age (Numeric Column) Using simpleImputer\nfrom sklearn.impute import SimpleImputer\n\nimputer = SimpleImputer(missing_values= np.nan, strategy='median')\n\nimputer = imputer.fit(train_data[['Age']])\ntrain_data['Age'] = imputer.transform(train_data[['Age']]).ravel()\n\ntrain_data.info()","28cf5b7f":"# checking for Missing values again\nround(train_data.isnull().sum()\/len(train_data.index), 2)*100","5417c469":"#convert data type of age to int\ntrain_data['Age']= train_data['Age'].astype(int)\ntrain_data['Age'].dtype","be85e253":"#Fill the categorical variable (Cabin, Embarked Feature) with the most frequently occuring value\n#idxmax() function returns index of first occurrence of maximum over\ntrain_data['Cabin'] = train_data['Cabin'].fillna(train_data['Cabin'].value_counts().idxmax())\ntrain_data['Embarked'] = train_data['Embarked'].fillna(train_data['Embarked'].value_counts().idxmax())","7d16557e":"# checking for Missing values again\nround(train_data.isnull().sum()\/len(train_data.index), 2)*100","ab04d6e6":"# checking whether some rows have more than 50 missing values\nlen(train_data[train_data.isnull().sum(axis=1) > 50].index)","292deba1":"train_data.columns","9e3be4a3":"#checking for all numeric columns -.columns.tolist() - Excluding the number of siblings, spouse, parents and children\nnum_col = [col for col in train_data.select_dtypes(include=np.number) if col not in ['PassengerId', 'Survived', 'Age', 'SibSp','Parch']]\nprint(num_col)","2842835c":"## Check for outliers in Numerical columns- calling the outlier function\nfig = plt.subplots(figsize=(9,6))\ntrain_data.boxplot(['Fare'])\n\nplt.show()","79de4773":"#Binning AGE GROUP of applicants\ndef Age_Group(n):\n    if n < 5:\n        return 'Infants'\n    elif n >=5 and n < 13:\n        return 'Children'\n    elif n >= 13 and n < 20:\n        return 'Teenager'\n    elif n >= 20 and n < 26:\n        return 'Student'\n    elif n >= 26 and n < 41:\n        return 'Youth'\n    elif n >= 41 and n < 61:\n        return 'Mid-Aged'\n    else:\n        return 'Senior Citizens'\n        \ntrain_data['Age_group'] = train_data['Age'].apply(lambda x: Age_Group(x))\n#Implementing the changes to test data as well\ntest_data['Age_group'] = test_data['Age'].apply(lambda x: Age_Group(x))\n\n#plotting the continous variable (AGE) using distplt and Categorical variable (Age_group) using barplot\nplt.figure(figsize=(10,4))\n    \nplt.subplot(1, 2, 1) # This subplot will show the age group distribution\nsns.distplot(train_data['Age'])\n\nplt.subplot(1, 2, 2) # This Subplot will show how default rates vary across continous variables \nsns.barplot('Age_group', 'Survived', data=train_data).set(title = 'Distribution of Age-Group survived', ylabel = 'Survived' )\nplt.xticks(rotation=90)\n","ee0e3d69":"age_group = train_data.Age_group.value_counts()\nprint('\\033[1m'+'\\033[94m'+\"Count per Age Group \\n\"+'\\033[0m', age_group)\n#print(age_group)\nage_group.plot.bar()","692fb8e5":"#This is for the Fair paid\ndef totalfair_Group(n):\n    if n < 15:\n        return 'Low Fare'\n    elif n >=15 and n < 40:\n        return 'Avg Fare'\n    else:\n        return 'High Fare'\n        \ntrain_data['Fare_group'] = train_data['Fare'].apply(lambda x: totalfair_Group(x))\ntest_data['Fare_group'] = test_data['Fare'].apply(lambda x: totalfair_Group(x))\n\n#plotting the continous variable using distplt and Categorical variable (Fare_group) using barplot\nplt.figure(figsize=(10,4))\n    \nplt.subplot(1, 2, 1) # This subplot will show the spread of Fare\nsns.distplot(train_data['Fare'])\n\nplt.subplot(1, 2, 2) # This Subplot will show how default rates vary across continous variables \nsns.barplot('Fare_group', 'Survived', data=train_data).set(title = 'Distribution of fare per survived', ylabel = 'Survived' )\nplt.xticks(rotation=90)","d18a3968":"fare_group = train_data.Fare_group.value_counts()\nprint('\\033[1m'+'\\033[94m'+\"Count per Age Group \\n\"+'\\033[0m', fare_group)\n#print(age_group)\nfare_group.plot.bar()","1a971605":"# summarising the values\n#print(train_data['Survived'].value_counts())\nprint(\"Survival Ratio: \\n\", train_data.Survived.value_counts()*100\/train_data.shape[0])","23eb3f1f":"plt.figure(figsize=(10, 6))\nplt.title('Titanic Survival (Survived Vs Deceased) distribution')\nsns.set_color_codes(\"pastel\")\nsns.countplot(x='Survived', data=train_data)\nlocs, labels = plt.xticks()\nplt.show()","198e7f71":"corr = train_data.corr(method ='pearson').abs() # mapping features to their absolute correlation values\n\n#cor_target = corr[corr>=0.8]\nplt.figure(figsize=(10,6))\nsns.heatmap(corr, linewidths=0.5, vmin=-1, vmax=1, cmap='coolwarm',annot=True)","9188eab7":"# Correlation of Survived with other columns\nplt.figure(figsize=(10,6))\ntrain_data.corr()['Survived'].sort_values(ascending = False).plot(kind='bar')\nplt.show()","96676bf0":"#map each Age value to a numerical value\nage_mapping = {'Infants': 1, 'Children': 2, 'Teenager': 3, 'Student': 4, 'Youth': 5, 'Mid-Aged': 6, 'Senior Citizens': 7}\ntrain_data['Age_group'] = train_data['Age_group'].map(age_mapping)\ntest_data['Age_group'] = test_data['Age_group'].map(age_mapping)\n\ntrain_data.head()\n\n#dropping the Age feature for now, might change\ntrain_data = train_data.drop(['Age'], axis = 1)\ntest_data = test_data.drop(['Age'], axis = 1)","e58b5e68":"train_data.columns","25f87b69":"#map each Fare value to a numerical value\n\nfare_mapping = {'Low Fare': 0, 'Avg Fare': 1, 'High Fare': 2}\ntrain_data['Fare_group'] = train_data['Fare_group'].map(fare_mapping)\ntest_data['Fare_group'] = test_data['Fare_group'].map(fare_mapping)\n\ntrain_data.head()\n\n#dropping the Age feature for now, might change\ntrain_data = train_data.drop(['Fare'], axis = 1)\ntest_data = test_data.drop(['Fare'], axis = 1)","c7d2c464":"#map each Sex value to a numerical value\nsex_mapping = {\"male\": 0, \"female\": 1}\ntrain_data['Sex'] = train_data['Sex'].map(sex_mapping)\ntest_data['Sex'] = test_data['Sex'].map(sex_mapping)","f0894793":"#map each Embarked value to a numerical value\nEmbarked_mapping = {\"C\": 0, \"Q\": 1,\"S\":2}\ntrain_data['Embarked'] = train_data['Embarked'].map(Embarked_mapping)\ntest_data['Embarked'] = test_data['Embarked'].map(Embarked_mapping)","d8544db5":"#Dropping Name, Ticket and Cabin from train and test dataset\ntrain_data.drop(['Name','Cabin','Ticket'], axis=1, inplace = True)\ntest_data.drop(['Name','Cabin','Ticket'], axis=1, inplace = True)","a4c38b62":"train_data.head()","62565bf8":"train_data.info()","883323a2":"test_data.head()","249c9247":"##Function to plot grahps for various single features\/variables\ndef plot_surv(var):\n    plt.figure(figsize=(10,4))\n    \n    plt.subplot(1, 2, 1)\n    x=train_data[var].value_counts()\n    sns.barplot(x.index, x.values, order=x.index,alpha=0.8)\n    plt.xlabel(var, labelpad=14)\n    plt.ylabel(\"Total count\", labelpad=14)\n    plt.xticks(rotation=90)\n    \n    \n    plt.subplot(1, 2, 2)\n    #sorting the values in descending order\n    target_perc = train_data[[var, 'Survived']].groupby([var],as_index=False).mean()\n    target_perc.sort_values(by='Survived', ascending=False, inplace=True)\n    sns.barplot(x=var, y='Survived',order=target_perc[var], data=target_perc, alpha=0.8)\n    plt.xlabel(var, labelpad=14)\n    plt.ylabel(\"Percent of Survived (%)\")\n    plt.xticks(rotation=90)\n    \n    fig = plt.figure()\n    fig.subplots_adjust(right = 0.9, hspace=0.5, wspace=0.5)\n    \n    plt.show()","e321af37":"#plotting the gender distribution\nplot_surv('Sex')","55adb218":"#Plotting by ticket class\nplot_surv('Pclass')","7d6e91b1":"plot_surv('SibSp')","a8ee3a83":"plot_surv('Parch')","b63de327":"#Checking Correlation of dataset\n\ncorr = train_data.corr(method ='pearson').abs() # mapping features to their absolute correlation values\n\nplt.figure(figsize=(10,6))\nsns.heatmap(corr, linewidths=0.5, vmin=-1, vmax=1, cmap='coolwarm',annot=True)","dcea0dee":"# Taking copy of the original dataset\n\ntrain_data_cpy = train_data[:].copy()\ntrain_data_cpy.shape","47dde05b":"train_data_cpy.info()","edae292d":"train_data_cpy.head()","21047264":"#Dropping PassengerId from the dataset\ntrain_data_cpy.drop(['PassengerId'], axis=1, inplace = True)","943b2996":"# X & y dataset for model building, X will obviously not have \"Survived\" and y will only have \"Survived\"\nX = train_data_cpy.drop(['Survived'], axis=1)\ny = train_data_cpy['Survived']\n\ntrain_data_cpy.drop('Survived', axis=1, inplace=True)","9d43197b":"from sklearn.model_selection import train_test_split\n\n#Splitting Train_data\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size = .25 ,random_state = 0)","bdadea5f":"from sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\n\nlgr = LogisticRegression()\nlgr.fit(X_train, y_train)\n\n# make the predictions\ny_pred = lgr.predict(X_test)\n\n# convert prediction array into a dataframe\ny_pred_df = pd.DataFrame(y_pred)","f630a27c":"from sklearn.metrics import confusion_matrix, accuracy_score\n\nprint(\"Confusion Matrix: \\n\", confusion_matrix(y_test,y_pred))\nacc_logistic = round(accuracy_score(y_test,y_pred)*100, 2)\nprint(\"Accuracy of the logistic regression model is\",acc_logistic)","92a867b7":"# Importing decision tree classifier from sklearn library\nfrom sklearn.tree import DecisionTreeClassifier\n\n# max_depth = 5 so that we can plot and read the tree.\ndt_default = DecisionTreeClassifier(max_depth=5)\ndt_default.fit(X_train, y_train)","f18b9340":"# Let's check the evaluation metrics of our default model\n\n# Importing classification report and confusion matrix from sklearn metrics\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\n# Making predictions\ny_pred_default = dt_default.predict(X_test)\n\n# Printing classification report\nprint(classification_report(y_test, y_pred_default))","48e4690d":"print(confusion_matrix(y_test,y_pred_default))\nacc_decision = round(accuracy_score(y_test,y_pred_default)*100, 2)\nprint(\"Accuracy of the Decsion Tree model is\",acc_decision)","5f0c9f5e":"# Importing random forest classifier from sklearn library\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Running the random forest with default parameters.\nrfc = RandomForestClassifier()\n\n# fit\nrfc.fit(X_train,y_train)","2d582c9f":"# Making predictions\ny_pred = rfc.predict(X_test)\n\n# Let's check the report of our default model\nprint(classification_report(y_test,y_pred))","79230bb7":"# Printing confusion matrix\nprint(confusion_matrix(y_test,y_pred))\n\nacc_randomF = round(accuracy_score(y_test,y_pred)*100, 2)\nprint(\"Accuracy of the Random Forest model is\",acc_randomF)","3cf4e448":"from sklearn.naive_bayes import GaussianNB\ngaussian = GaussianNB()\ngaussian.fit(X_train, y_train)","ec10e5cc":"y_pred = gaussian.predict(X_test)\n# Let's check the report of our default model\nprint(classification_report(y_test,y_pred))","9d30ddf6":"# Printing confusion matrix\nprint(confusion_matrix(y_test,y_pred))\n\nacc_gaussian = round(accuracy_score(y_pred, y_test) * 100, 2)\nprint(\"Accuracy of the Random Forest model is\", acc_gaussian)","ca94cb0c":"from sklearn.svm import SVC\n\nsvc = SVC()\nsvc.fit(X_train, y_train)","30e49333":"y_pred = svc.predict(X_test)\n# Let's check the report of our default model\nprint(classification_report(y_test,y_pred))","f8a5c837":"# Printing confusion matrix\nprint(confusion_matrix(y_test,y_pred))\n\nacc_svm = round(accuracy_score(y_pred, y_test) * 100, 2)\nprint(\"Accuracy of the Support Vector Machine model is\", acc_svm)","0de3d106":"from sklearn.ensemble import GradientBoostingClassifier\n\ngbk = GradientBoostingClassifier()\ngbk.fit(X_train, y_train)","95e727df":"y_pred = gbk.predict(X_test)\n# Let's check the report of our default model\nprint(classification_report(y_test,y_pred))","0d642da8":"# Printing confusion matrix\nprint(confusion_matrix(y_test,y_pred))\n\nacc_gradientBoost = round(accuracy_score(y_pred, y_test) * 100, 2)\nprint(\"Accuracy of the Gradient Boosting model is\", acc_gradientBoost)","f2119253":"model_comp = pd.DataFrame({ 'Model': ['Logistic Regression','Decision Tree', 'Random Forest', 'Gaussian Naive Bayes', 'Support Vector Machines', 'Gradient Boosting'],\n                                        'Score': [acc_logistic, acc_decision, acc_randomF, acc_gaussian, acc_svm, acc_gradientBoost]})\nmodel_comp.sort_values(by='Score', ascending=False)","1dff3710":"# Predicting Survival using RF\nPassengerId = test_data['PassengerId']\ntest_pred = rfc.predict(test_data.drop('PassengerId', axis=1))\n\n#set the output as a dataframe and convert to csv file named submission.csv\noutput = pd.DataFrame({ 'PassengerId' : PassengerId, 'Survived': test_pred })\nprint(output)\n#output.to_csv('submission.csv', index=False)","59372b47":"#### Univariant Analysis on the dataset","f4cdbc50":"- Fare, number of Parents and Children are positively correlated with Survived where as total number of Siblings & spouse , Age and Ticket class are negatively correlated with Survived","e1e59120":"#### Ratio of defaulters to non-Defaulters","b465f24c":"<b>Accuracy on the training set using Support Vector Machine came out to be  81.61 %<b>","12912455":"### Support Vector Machine Model","adcfff58":"## Model Building\n\n#### Will be using the below models to predict the survival of the passengers.\n- Logistic Regression\n- Decision Tree\n- Gaussian Naive Bayes\n- Random Forest\n- Gradient Boosting\n- Support Vector Machine\n\nWe will them compare the highest accuracy from these models and implement the best model on our Test data","ab0a8c3c":"<b>Accuracy on the training set using Gradient Boosting came out to be 81.61 %<b>","5e5bc32c":"- From the above graph, it's clear that people with more siblings or spouses aboard were less likely to survive\n- People travelling alone are less likely to survive than those with 1-2 siblings or spouses","fd07e9c1":"<b>Accuracy on the training set using Gaussian Naive Bayes came out to be  78.92%<b>","b0c0a117":"### BINNING of Continous variables","19f09b80":"### **Further Data Analysis**\n_Identify variables that help in predicting the Survival data._","9683d957":"- The graph shows that the 1st class ticket travellers have the highest survived percentage when compared to 3rd Class travellers. Though there were maximum number of 3rd class travellers than 2nd and 1st class","f075e2e8":"### Understanding the Data","b8d30e90":"<b>Accuracy on the training set using Decision Tree came out to be  80.27 %<b>","a101ac6d":"### Outlier Analysis","37f05cba":"#### Mapping Numerical values to categorical variables to both Train and Test dataset","ad64991a":"- There are no missing values in rows","64b46c82":"- From the above Age-group distribution it is seen that Youngsters aged 15 to 30 were the most among the travellers followed by Mid-Aged 31 to 50.\n- Infants were the most likely to survive in this tragedy when compared to Senior-Citizens and youngsters","f86b72f3":"- The Graph shows that total number of male travellers were more when compared to female travellers but the Survival percentage is more for females than male","d5a1baa1":"- There are missing values in Cabin and Age","2da4a254":"### Decision Tree Model","308ee6ef":"- The Variables are not highly correlated with each other","002f1b4b":"## Comparing Accuracy for each model","3d97893a":"# Titanic Survival Prediction\n\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\n\nOn April 15, 1912, during her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren\u2019t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n\nIn this challenge, we ask you to build a predictive model that answers the question: \u201cwhat sorts of people were more likely to survive?\u201d using passenger data (ie name, age, gender, socio-economic class, etc).","b3534819":"### Correlation among variables","f78f7a31":"### Splitting the data before building the prediction model","04b4944f":"## Data Cleaning\n### Handling Missing Values","2578b332":"<b>Accuracy on the training set came out to be  80 % which is  good<b>","cf7069e4":"- People with less than four parents or children aboard are more likely to survive than those with four or more. \n- People traveling alone (0) are less likely to survive than those with 1-3 parents or children.","6f269c7e":"### Logistic Regression Model","83b46dac":"### Random Forest Model","80e2f9d9":"### Gradient Boosting Model","1d4ceb22":"- <b>Overall Survival rate as per the data is about 38.4%<\/b>. There are 61.6% Deceased","f7f72104":"- From the above chart it shows that the number of travellers per fare group is inverse to the survival rate. i.e. the total number of low fare travellers are least likely to survive in this incident when compared to the High Fare travellers","88928e27":"- All the missing values are imputed. There are no Missing values in the data","c2d56ebc":"<b>Accuracy on the training set using Random Forest came out to be  83.41 %<b>","a2aeb4c4":"### Imputing missing values - Numeric column (Using SimpleImputer)","b3d4c2e0":"### Importing Libraries","8cf7c1c9":"### Gaussian Naive Bayes Model","c6cedf34":"- Will be using Random Forest for predicting the Survival on Test dataset","79b25909":"### Imputing the Categorical Variable with mode "}}