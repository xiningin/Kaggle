{"cell_type":{"e6a34873":"code","fa402b98":"code","4ac7629a":"code","81daa80d":"code","6df633bb":"code","497624a5":"code","e70a786b":"code","c75c6a0f":"code","848f8d03":"code","45b74040":"code","255b7530":"code","9e9e7bc0":"code","f679566e":"code","1b6819a2":"code","4338901d":"code","e55ff462":"code","ab37c23d":"code","8e3448b9":"code","2da2d7c5":"code","b9796640":"code","2b5dcfbd":"code","29022c12":"code","d71f0983":"code","aa438e32":"code","15eb8ac8":"code","89adc17a":"code","57396ab9":"code","4d63624a":"code","0717fe45":"code","c8475132":"code","569ee428":"code","0f3bea8d":"code","7e9d5ec1":"code","cbf147d7":"code","3737a135":"code","efd2dac1":"code","7bfe7ef7":"code","5f2a1f09":"markdown","e3c53361":"markdown"},"source":{"e6a34873":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fa402b98":"import cv2\nimport sklearn.preprocessing\nfrom os import listdir, mkdir\nfrom os.path import isfile, join, isdir\nfrom PIL import Image","4ac7629a":"frame_height = 256\nframe_width = 256\nchannels = 1\nframe_count = 5\nfeatures_per_bag = 20\nnormal_videos_path='..\/input\/ucsddataset\/UCSD_Anomaly_Dataset.v1p2\/UCSDped1\/Train\/'\nabnormal_videos_path='..\/input\/ucsddataset\/UCSD_Anomaly_Dataset.v1p2\/UCSDped1\/Test\/'\ntest_set='..\/input\/ucsddataset\/UCSD_Anomaly_Dataset.v1p2\/UCSDped1\/Test\/Test001\/'","81daa80d":"def sliding_window(arr, size, stride):\n    \"\"\"Apply sliding window to an array, getting chunks of\n    of specified size using the specified stride\n    :param arr: Array to be divided\n    :param size: Size of the chunks\n    :param stride: Number of frames to skip for the next chunk\n    :returns: Tensor with the resulting chunks\n    :rtype: np.ndarray\n    \"\"\"\n    num_chunks = int((len(arr) - size) \/ stride) + 2\n    result = []\n    for i in range(0,  num_chunks * stride, stride):\n        if len(arr[i:i + size]) > 0:\n            result.append(arr[i:i + size])\n    return np.array(result)\n\n\ndef interpolate(features, features_per_bag):\n    \"\"\"Transform a bag with an arbitrary number of features into a bag\n    with a fixed amount, using interpolation of consecutive features\n    :param features: Bag of features to pad\n    :param features_per_bag: Number of features to obtain\n    :returns: Interpolated features\n    :rtype: np.ndarray\n    \"\"\"\n    feature_size = np.array(features).shape[1]\n    interpolated_features = np.zeros((features_per_bag, feature_size))\n    interpolation_indices = np.round(np.linspace(0, len(features) - 1, num=features_per_bag + 1))\n    count = 0\n    for index in range(0, len(interpolation_indices)-1):\n        start = int(interpolation_indices[index])\n        end = int(interpolation_indices[index + 1])\n\n        assert end >= start\n\n        if start == end:\n            temp_vect = features[start, :]\n        else:\n            temp_vect = np.mean(features[start:end+1, :], axis=0)\n\n        temp_vect = temp_vect \/ np.linalg.norm(temp_vect)\n\n        if np.linalg.norm(temp_vect) == 0:\n            print(\"Error\")\n\n        interpolated_features[count,:]=temp_vect\n        count = count + 1\n\n    return np.array(interpolated_features)\n\n\ndef extrapolate(outputs, num_frames):\n    \"\"\"Expand output to match the video length\n    :param outputs: Array of predicted outputs\n    :param num_frames: Expected size of the output array\n    :returns: Array of output size\n    :rtype: np.ndarray\n    \"\"\"\n\n    extrapolated_outputs = []\n    extrapolation_indices = np.round(np.linspace(0, len(outputs) - 1, num=num_frames))\n    for index in extrapolation_indices:\n        extrapolated_outputs.append(outputs[int(index)])\n    return np.array(extrapolated_outputs)","6df633bb":"def get_video_clips(video_path):\n    \"\"\"Divides the input video into non-overlapping clips\n    :param video_path: Path to the video\n    :returns: Array with the fragments of video\n    :rtype: np.ndarray\n    \"\"\"\n    frames = get_video_frames(video_path)\n    clips = sliding_window(frames, frame_count, frame_count)\n    return clips, len(frames)\n\n\ndef get_video_frames(video_path):\n    \"\"\"Reads the video given a file path\n    :param video_path: Path to the video\n    :returns: Video as an array of frames\n    :rtype: np.ndarray\n    \"\"\"\n    frames=[]\n    for c in sorted(listdir(video_path)):\n        img_path = join(video_path, c).replace(\"\\\\\", '\/')\n        if str(img_path)[-3:] == \"tif\":\n            img = cv2.imread(img_path)\n           # img = np.array(img, dtype=np.float32) \/ 256.0\n            frames.append(img)\n\n    return frames","497624a5":"# def get_video_clips(video_path):\n#     \"\"\"Divides the input video into non-overlapping clips\n#     :param video_path: Path to the video\n#     :returns: Array with the fragments of video\n#     :rtype: np.ndarray\n#     \"\"\"\n#     frames = get_video_frames(video_path)\n#     clips = sliding_window(frames, frame_count, frame_count)\n#     return clips, len(frames)\n\n\n# def get_video_frames(video_path):\n#     \"\"\"Reads the video given a file path\n#     :param video_path: Path to the video\n#     :returns: Video as an array of frames\n#     :rtype: np.ndarray\n#     \"\"\"\n#     cap = cv2.VideoCapture(video_path)\n#     frames = []\n#     while (cap.isOpened()):\n#         ret, frame = cap.read()\n#         if ret == True:\n#             frames.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n#         else:\n#             break\n#     cap.release()\n#     return frames","e70a786b":"import keras.backend as K\nfrom keras.models import Sequential\nfrom keras.models import Model\nfrom keras.layers.core import Dense, Dropout, Flatten\n\nfrom keras.layers.convolutional import Conv3D, MaxPooling3D, ZeroPadding3D\n\nfrom keras.utils.data_utils import get_file\n\nC3D_MEAN_PATH = 'https:\/\/github.com\/adamcasson\/c3d\/releases\/download\/v0.1\/c3d_mean.npy'\n\n\ndef preprocess_input(video):\n    \"\"\"Preprocess video input to make it suitable for feature extraction.\n    The video is resized, cropped, resampled and training mean is substracted\n    to make it suitable for the network\n    :param video: Video to be processed\n    :returns: Preprocessed video\n    :rtype: np.ndarray\n    \"\"\"\n\n    intervals = np.ceil(np.linspace(0, video.shape[0] - 1, 16)).astype(int)\n    frames = video[intervals]\n\n    # Reshape to 128x171\n    reshape_frames = np.zeros((frames.shape[0], 128, 171, frames.shape[3]))\n    for i, img in enumerate(frames):\n        img = cv2.resize(img, (171, 128), cv2.INTER_CUBIC)\n        reshape_frames[i, :, :, :] = img\n\n    mean_path = get_file('c3d_mean.npy',\n                         C3D_MEAN_PATH,\n                         cache_subdir='models',\n                         md5_hash='08a07d9761e76097985124d9e8b2fe34')\n\n    mean = np.load(mean_path)\n    reshape_frames -= mean\n    # Crop to 112x112\n    reshape_frames = reshape_frames[:, 8:120, 30:142, :]\n    # Add extra dimension for samples\n    reshape_frames = np.expand_dims(reshape_frames, axis=0)\n\n    return reshape_frames\n\n\ndef C3D(weights='sports1M'):\n    \"\"\"Creation of the full C3D architecture\n    :param weights: Weights to be loaded into the network. If None,\n    the network is randomly initialized.\n    :returns: Network model\n    :rtype: keras.model\n    \"\"\"\n\n    if weights not in {'sports1M', None}:\n        raise ValueError('weights should be either be sports1M or None')\n\n    if K.image_data_format() == 'channels_last':\n        shape = (16, 112, 112, 3)\n    else:\n        shape = (3, 16, 112, 112)\n\n    model = Sequential()\n    model.add(\n        Conv3D(64,3,activation='relu',padding='same',name='conv1', input_shape=shape))\n    model.add( MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2),padding='same', name='pool1'))\n\n    model.add(Conv3D(128, 3, activation='relu', padding='same', name='conv2'))\n    model.add(MaxPooling3D(pool_size=(2, 2, 2),strides=(2, 2, 2),padding='valid',name='pool2'))\n\n    model.add(Conv3D(256, 3, activation='relu', padding='same', name='conv3a'))\n    model.add(Conv3D(256, 3, activation='relu', padding='same', name='conv3b'))\n    model.add(\n        MaxPooling3D(pool_size=(2, 2, 2),\n                     strides=(2, 2, 2),\n                     padding='valid',\n                     name='pool3'))\n\n    model.add(Conv3D(512, 3, activation='relu', padding='same', name='conv4a'))\n    model.add(Conv3D(512, 3, activation='relu', padding='same', name='conv4b'))\n    model.add(\n        MaxPooling3D(pool_size=(2, 2, 2),\n                     strides=(2, 2, 2),\n                     padding='valid',\n                     name='pool4'))\n\n    model.add(Conv3D(512, 3, activation='relu', padding='same', name='conv5a'))\n    model.add(Conv3D(512, 3, activation='relu', padding='same', name='conv5b'))\n    model.add(ZeroPadding3D(padding=(0, 1, 1)))\n    model.add(\n        MaxPooling3D(pool_size=(2, 2, 2),\n                     strides=(2, 2, 2),\n                     padding='valid',\n                     name='pool5'))\n\n    model.add(Flatten())\n\n    model.add(Dense(4096, activation='relu', name='fc6'))\n    model.add(Dropout(0.5))\n    model.add(Dense(4096, activation='relu', name='fc7'))\n    model.add(Dropout(0.5))\n    model.add(Dense(487, activation='softmax', name='fc8'))\n\n    if weights == 'sports1M':\n        model.load_weights('..\/input\/c3d-sport1m-weights-keras-224\/C3D_Sport1M_weights_keras_2.2.4.h5')\n\n    return model\n\n\ndef c3d_feature_extractor():\n    \"\"\"Creation of the feature extraction architecture. This network is\n    formed by a subset of the original C3D architecture (from the\n    beginning to fc6 layer)\n    :returns: Feature extraction model\n    :rtype: keras.model\n    \"\"\"\n    model = C3D(weights='sports1M')\n    layer_name = 'fc6'\n    feature_extractor_model = Model(inputs=model.input,\n                                    outputs=model.get_layer(layer_name).output)\n    return feature_extractor_model","c75c6a0f":"raw_normal_train_features=\".\/raw_normal_train_features\"\nos.makedirs('.\/raw_normal_train_features')","848f8d03":"os.makedirs('.\/raw_abnormal_train_features')\nraw_abnormal_train_features='.\/raw_abnormal_train_features'","45b74040":"#Raw Normal Train _ features \nfeature_extractor = c3d_feature_extractor()\nnormal_videos = os.listdir(normal_videos_path)\nnormal_videos.sort()\n\nprint(\"Processing normal videos...\")\nfor vid_name in normal_videos:\n    print(\"Processing {}\".format(vid_name))\n    vid_path = os.path.join(normal_videos_path, vid_name)\n    feats_path = os.path.join(\n        raw_normal_train_features, vid_name + \".npy\"\n    )\n\n    clips, frames = get_video_clips(vid_path)\n\n    # Remove last clip if number of frames is not equal to 16\n    if frames % 16 != 0:\n        clips = clips[:-1]\n\n    prep_clips = [preprocess_input(np.array(clip)) for clip in clips]\n    prep_clips = np.vstack(prep_clips)\n\n    features = feature_extractor.predict(prep_clips)\n    features = sklearn.preprocessing.normalize(features, axis=1)\n\n    with open(feats_path, \"wb\") as f:\n        np.save(f, features)","255b7530":"abnormal_videos = os.listdir(abnormal_videos_path)\nabnormal_videos.sort()\nfor vid_name in abnormal_videos[1:47]:\n    if  vid_name == '._UCSDped1.m':\n        pass\n    if vid_name[-3:]=='_gt':\n        pass\n    else:\n        print(\"Processing {}\".format(vid_name))\n        vid_path = os.path.join(abnormal_videos_path, vid_name)\n        feats_path = os.path.join(\n        raw_abnormal_train_features, vid_name + \".npy\"\n    )\n\n        clips, frames = get_video_clips(vid_path)\n\n    # Remove last clip if number of frames is not equal to 16\n        if frames % 16 != 0:\n            clips = clips[:-1]\n\n        prep_clips = [preprocess_input(np.array(clip)) for clip in clips]\n        prep_clips = np.vstack(prep_clips)\n\n        features = feature_extractor.predict(prep_clips)\n        features = sklearn.preprocessing.normalize(features, axis=1)\n\n        with open(feats_path, \"wb\") as f:\n            np.save(f, features)","9e9e7bc0":"# #Test set\n# test_videos = os.listdir(cfg.test_set)\n# test_videos.sort()\n# print(\"Processing test videos...\")\n# for vid_name in test_videos:\n#     print(\"Processing {}\".format(vid_name))\n#     vid_path = os.path.join(cfg.test_set, vid_name)\n#     feats_path = os.path.join(cfg.raw_test_features, vid_name[:-9] + \".npy\")\n\n#     clips, frames = video_util.get_video_clips(vid_path)\n\n#     # Remove last clip if number of frames is not equal to 16\n#     if frames % 16 != 0:\n#         clips = clips[:-1]\n\n#     prep_clips = [c3d.preprocess_input(np.array(clip)) for clip in clips]\n#     prep_clips = np.vstack(prep_clips)\n\n#     features = feature_extractor.predict(prep_clips)\n#     features = sklearn.preprocessing.normalize(features, axis=1)\n\n#     with open(feats_path, \"wb\") as f:\n#         np.save(f, features)","f679566e":"processed_normal_train_features=os.makedirs('.\/processed_normal_train_features')\nprocessed_normal_train_features='.\/processed_normal_train_features'","1b6819a2":"processed_abnormal_train_features=os.makedirs('.\/processed_abnormal_train_features')\nprocessed_abnormal_train_features='.\/processed_abnormal_train_features'","4338901d":"import numpy as np\nimport os\nimport sklearn.preprocessing\n\n\ndef transform_into_segments(features, n_segments=32):\n    if features.shape[0] < n_segments:\n        raise RuntimeError(\n            \"Number of prev segments lesser than expected output size\"\n        )\n\n    cuts = np.linspace(0, features.shape[0], n_segments,\n                       dtype=int, endpoint=False)\n\n    new_feats = []\n    for i, j in zip(cuts[:-1], cuts[1:]):\n        new_feats.append(np.mean(features[i:j,:], axis=0))\n\n    new_feats.append(np.mean(features[cuts[-1]:,:], axis=0))\n\n    new_feats = np.array(new_feats)\n    new_feats = sklearn.preprocessing.normalize(new_feats, axis=1)\n    return new_feats\n\nfor filename in os.listdir(raw_normal_train_features):\n    print(\"Processing {}\".format(filename))\n    raw_file_path = os.path.join(\n        raw_normal_train_features, filename\n    )\n    processed_file_path = os.path.join(\n        processed_normal_train_features, filename\n    )\n\n    with open(raw_file_path, \"rb\") as f:\n        feats = np.load(f, allow_pickle=True)\n\n    try:\n        new_feats = transform_into_segments(feats)\n        with open(processed_file_path, \"wb\") as f:\n            np.save(f, new_feats, allow_pickle=True)\n    except RuntimeError:\n        print(\"Video {} too short\".format(filename))\n\nfor filename in os.listdir(raw_abnormal_train_features):\n    print(\"Processing {}\".format(filename))\n    raw_file_path = os.path.join(\n        raw_abnormal_train_features, filename\n    )\n    processed_file_path = os.path.join(\n        processed_abnormal_train_features, filename\n    )\n    with open(raw_file_path, \"rb\") as f:\n        feats = np.load(f, allow_pickle=True)\n\n    try:\n        new_feats = transform_into_segments(feats)\n        with open(processed_file_path, \"wb\") as f:\n            np.save(f, new_feats, allow_pickle=True)\n    except RuntimeError:\n        print(\"Video {} too short\".format(filename))\n\n# for filename in os.listdir(cfg.raw_test_features):\n#     print(\"Processing {}\".format(filename))\n#     raw_file_path = os.path.join(\n#         cfg.raw_test_features, filename\n#     )\n#     processed_file_path = os.path.join(\n#         cfg.processed_test_features, filename\n#     )\n#     with open(raw_file_path, \"rb\") as f:\n#         feats = np.load(f, allow_pickle=True)\n\n#     try:\n#         new_feats = transform_into_segments(feats)\n#         with open(processed_file_path, \"wb\") as f:\n#             np.save(f, new_feats, allow_pickle=True)\n#     except RuntimeError:\n#         print(\"Video {} too short\".format(filename))","e55ff462":"import keras\nimport scipy.io as sio\nfrom keras import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.regularizers import l2\nimport tensorflow as tf \n\ndef classifier_model():\n    \"\"\"Build the classifier\n    :returns: Classifier model\n    :rtype: keras.Model\n    \"\"\"\n    model = Sequential()\n    model.add(Dense(512, input_dim=4096, kernel_initializer='glorot_normal',\n                    kernel_regularizer=l2(0.001), activation='relu'))\n    model.add(Dropout(0.6))\n    model.add(Dense(32, kernel_initializer='glorot_normal',\n                    kernel_regularizer=l2(0.001)))\n    model.add(Dropout(0.6))\n    model.add(Dense(1, kernel_initializer='glorot_normal',\n                    kernel_regularizer=l2(0.001), activation='sigmoid'))\n    return model\n\ndef build_classifier_model():\n    \"\"\"Build the classifier and load the pretrained weights\n    :returns:\n    :rtype:\n    \"\"\"\n    model = classifier_model()\n    model = load_weights(model, '.\/weights.mat')\n    return model\n\n\ndef conv_dict(dict2):\n    \"\"\"Prepare the dictionary of weights to be loaded by the network\n    :param dict2: Dictionary to format\n    :returns: The dictionary properly formatted\n    :rtype: dict\n    \"\"\"\n    dict = {}\n    for i in range(len(dict2)):\n        if str(i) in dict2:\n            if dict2[str(i)].shape == (0, 0):\n                dict[str(i)] = dict2[str(i)]\n            else:\n                weights = dict2[str(i)][0]\n                weights2 = []\n                for weight in weights:\n                    if weight.shape in [(1, x) for x in range(0, 5000)]:\n                        weights2.append(weight[0])\n                    else:\n                        weights2.append(weight)\n                dict[str(i)] = weights2\n    return dict\n\n\ndef load_weights(model, weights_file):\n    \"\"\"Loads the pretrained weights into the network architecture\n    :param model: keras model of the network\n    :param weights_file: Path to the weights file\n    :returns: The input model with the weights properly loaded\n    :rtype: keras.model\n    \"\"\"\n    dict2 = sio.loadmat(weights_file)\n    dict = conv_dict(dict2)\n    i = 0\n    for layer in model.layers:\n        weights = dict[str(i)]\n        layer.set_weights(weights)\n        i += 1\n    return model\n\nif __name__ == '__main__':\n    model = build_classifier_model()\n    model.summary()","ab37c23d":"import keras.optimizers\nimport scipy.io\nfrom keras.models import model_from_json\nimport os\n\nimport numpy as np\nimport keras.backend as K\n\n\nfrom datetime import datetime\n\ndef save_model(model, json_path, weight_path):\n    json_string = model.to_json()\n    open(json_path, 'w').write(json_string)\n    \n    dict = {}\n    i = 0\n    for layer in model.layers:\n        weights = layer.get_weights()\n        my_list = np.zeros(len(weights), dtype=np.object)\n        my_list[:] = weights\n        dict[str(i)] = my_list\n        i += 1\n    scipy.io.savemat(weight_path, dict)\n\ndef load_model(json_path):\n    model = model_from_json(open(json_path).read())\n    return model\n\ndef load_batch_train(normal_path, normal_list, abnormal_path, abnormal_list):\n\n    batchsize=32\n    n_exp = int(batchsize\/2)\n\n    num_normal = len(normal_list)\n    num_abnormal = len(abnormal_list)\n\n    abnor_list_idx = np.random.permutation(num_abnormal)\n    abnor_list = abnor_list_idx[:n_exp]\n    norm_list_idx = np.random.permutation(num_normal)\n    norm_list = norm_list_idx[:n_exp]\n\n    abnormal_feats = []\n    for video_idx in abnor_list:\n        video_path = os.path.join(abnormal_path, abnormal_list[video_idx])\n        with open(video_path, \"rb\") as f:\n            feats = np.load(f)\n        abnormal_feats.append(feats)\n\n    normal_feats = []\n    for video_idx in norm_list:\n        video_path = os.path.join(normal_path, normal_list[video_idx])\n        with open(video_path, \"rb\") as f:\n            feats = np.load(f)\n        normal_feats.append(feats)\n        \n\n    all_feats = np.vstack((*abnormal_feats, *normal_feats))\n    all_labels = np.zeros(32*batchsize, dtype='uint8')\n\n    all_labels[:32*n_exp] = 1\n\n    return  all_feats, all_labels\n\n\ndef custom_objective(y_true, y_pred):\n\n    y_true = K.reshape(y_true, [-1])\n    y_pred = K.reshape(y_pred, [-1])\n    n_seg = 32\n    nvid = 50\n    n_exp = int(nvid \/ 2)\n\n    max_scores_list = []\n    z_scores_list = []\n    temporal_constrains_list = []\n    sparsity_constrains_list = []\n\n    for i in range(0, n_exp, 1):\n\n        video_predictions = y_pred[i*n_seg:(i+1)*n_seg]\n\n        max_scores_list.append(K.max(video_predictions))\n        temporal_constrains_list.append(\n            K.sum(K.pow(video_predictions[1:] - video_predictions[:-1], 2))\n        )\n        sparsity_constrains_list.append(K.sum(video_predictions))\n\n    for j in range(n_exp, 2*n_exp, 1):\n\n        video_predictions = y_pred[j*n_seg:(j+1)*n_seg]\n        max_scores_list.append(K.max(video_predictions))\n\n    max_scores = K.stack(max_scores_list)\n    temporal_constrains = K.stack(temporal_constrains_list)\n    sparsity_constrains = K.stack(sparsity_constrains_list)\n\n    for ii in range(0, n_exp, 1):\n        max_z = K.maximum(1 - max_scores[:n_exp] + max_scores[n_exp+ii], 0)\n        z_scores_list.append(K.sum(max_z))\n\n    z_scores = K.stack(z_scores_list)\n    z = K.mean(z_scores)\n\n    return z + \\\n        0.00008*K.sum(temporal_constrains) + \\\n        0.00008*K.sum(sparsity_constrains)\n\n# output_dir = \".\/\"\n# normal_dir = \"..\/processed_c3d_features\/train\/normal\"\n# abnormal_dir = \"..\/processed_c3d_features\/train\/abnormal\"\noutput_dir = \".\/\"\nnormal_dir = \"..\/input\/processed-normal-train-features\/\"\nabnormal_dir = \"..\/input\/processed-abnormal-train-features1\/\"\n\nnormal_list = os.listdir(normal_dir)\nnormal_list.sort()\nabnormal_list = os.listdir(abnormal_dir)\nabnormal_list.sort()\n\nweights_path = output_dir + 'weights.mat'\n\nmodel_path = output_dir + 'model.json'\n\n#Create Full connected Model\nmodel = classifier_model()\n\nadagrad = tf.keras.optimizers.Adagrad(lr=0.001, epsilon=1e-08)\nmodel.compile(loss=custom_objective, optimizer=adagrad)\n\nif not os.path.exists(output_dir):\n       os.makedirs(output_dir)\n\nloss_graph =[]\nnum_iters = 20000\ntotal_iterations = 0\nbatchsize=60\ntime_before = datetime.now()\n\n\nfor it_num in range(num_iters):\n    inputs, targets = load_batch_train(\n        normal_dir, normal_list, abnormal_dir, abnormal_list\n    )\n    batch_loss = model.train_on_batch(inputs, targets)\n    loss_graph = np.hstack((loss_graph, batch_loss))\n    total_iterations += 1\n    if total_iterations % 20 == 0:\n        print (\"Iteration={} took: {}, loss: {}\".format(\n            total_iterations, datetime.now() - time_before, batch_loss)\n        )\n\nprint(\"Train Successful - Model saved\")\nsave_model(model, '.\/', '.\/')","8e3448b9":"def load_test_set(videos_path, videos_list):\n    feats = []\n    \n    for vid in videos_list:\n        vid_path = os.path.join(videos_path, vid)\n        with open(vid_path, \"rb\") as f:\n            feat = np.load(f)\n        feats.append(feat)\n\n    feats = np.array(feats)\n    return feats\n\nclassifier_model = build_classifier_model()\nprocessed_test_features='..\/input\/processed-test-features\/'\nvid_list = os.listdir(processed_test_features)\nvid_list.sort()\n\ntest_set = load_test_set(processed_test_features, vid_list)\npreds_folder=os.makedirs('.\/preds_folder')\npreds_folder='.\/preds_folder'\nfor filename, example in zip(vid_list, test_set):\n    predictions_file = filename + '.npy'\n    pred_path = os.path.join(preds_folder, predictions_file)\n    pred = classifier_model.predict_on_batch(example)\n    with open(pred_path, \"wb\") as f:\n        np.save(pred_path, pred, allow_pickle=True)","2da2d7c5":"sample_video_path='..\/input\/ucsddataset\/UCSD_Anomaly_Dataset.v1p2\/UCSDped1\/Test\/Test024\/'\nvideo_name = os.path.basename(sample_video_path).split('.')[0]\nvideo_name","b9796640":"def run_demo():\n\n    video_name = os.path.basename(sample_video_path).split('.')[0]\n\n    # read video\n    video_clips, num_frames = get_video_clips(sample_video_path)\n\n    print(\"Number of clips in the video : \", len(video_clips))\n\n    # build models\n    feature_extractor = c3d_feature_extractor()\n    classifier_model = build_classifier_model()\n\n    print(\"Models initialized\")\n\n    # extract features\n    rgb_features = []\n    for i, clip in enumerate(video_clips):\n        clip = np.array(clip)\n        if len(clip) < frame_count:\n            continue\n\n        clip = preprocess_input(clip)\n        rgb_feature = feature_extractor.predict(clip)[0]\n        rgb_features.append(rgb_feature)\n\n        print(\"Processed clip : \", i)\n\n    rgb_features = np.array(rgb_features)\n    rgb_feature_bag = interpolate(rgb_features, features_per_bag)\n    \n    # classify using the trained classifier model\n    predictions = classifier_model.predict(rgb_feature_bag)\n\n    predictions = np.array(predictions).squeeze()\n\n    predictions = extrapolate(predictions, num_frames)\n    \n    save_path = os.path.join('.\/', video_name + '.gif1')\n    # visualize predictions\n    print('Executed Successfully - '+video_name + '.gif saved')\n    visualize_predictions(sample_video_path, predictions, save_path)\n\n\nif __name__ == '__main__':\n    run_demo()","2b5dcfbd":"import matplotlib\n#matplotlib.use('Agg')\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import FuncAnimation\n\n\n\ndef visualize_clip(clip, convert_bgr=False, save_gif=False, file_path=None):\n    num_frames = len(clip)\n    fig, ax = plt.subplots()\n    fig.set_tight_layout(True)\n\n    def update(i):\n        if convert_bgr:\n            frame = cv2.cvtColor(clip[i], cv2.COLOR_BGR2RGB)\n        else:\n            frame = clip[i]\n        plt.imshow(frame)\n        return plt\n\n    # FuncAnimation will call the 'update' function for each frame; here\n    # animating over 10 frames, with an interval of 20ms between frames.\n    anim = FuncAnimation(fig, update, frames=np.arange(0, num_frames), interval=1)\n    if save_gif:\n        anim.save(file_path, dpi=80, writer='imagemagick')\n    else:\n        # plt.show() will just loop the animation forever.\n        plt.show()\n\n\ndef visualize_predictions(video_path, predictions, save_path):\n    frames = get_video_frames(video_path)\n    assert len(frames) == len(predictions)\n\n    fig, ax = plt.subplots(figsize=(5, 5))\n    fig.set_tight_layout(True)\n\n    fig_frame = plt.subplot(2, 1, 1)\n    fig_prediction = plt.subplot(2, 1, 2)\n    fig_prediction.set_xlim(0, len(frames))\n    fig_prediction.set_ylim(0, 1.15)\n\n    def update(i):\n        frame = frames[i]\n        x = range(0, i)\n        y = predictions[0:i]\n        fig_prediction.plot(x, y, '-')\n        fig_frame.imshow(frame)\n        return plt\n\n    # FuncAnimation will call the 'update' function for each frame; here\n    # animating over 10 frames, with an interval of 20ms between frames.\n\n    anim = FuncAnimation(fig, update, frames=np.arange(0, len(frames), 10), interval=1, repeat=False)\n\n    if save_path:\n           anim.save(save_path, dpi=200, writer='imagemagick')\n    else:\n           plt.show()\n   \n\n   ","29022c12":"img_array = []\nfor filename in glob.glob('..\/input\/ucsddataset\/UCSD_Anomaly_Dataset.v1p2\/UCSDped1\/Test\/Test024\/*.tif'):\n    img = cv2.imread(filename)\n    height, width, layers = img.shape\n    size = (width,height)\n    img_array.append(img)","d71f0983":"import cv2\nimport numpy as np\nimport glob\n\nimg_array = []\nfor filename in sorted(glob.glob('..\/input\/ucsddataset\/UCSD_Anomaly_Dataset.v1p2\/UCSDped1\/Test\/Test005\/*.tif')):\n    img = cv2.imread(filename)\n    height, width , layers = img.shape\n    size = (width,height)\n    img_array.append(img)\n\n\nout = cv2.VideoWriter('project.avi',cv2.VideoWriter_fourcc(*'mp4v'), 6, size)\n \nfor i in range(len(img_array)):\n    out.write(img_array[i])\n    \nout.release()","aa438e32":"\nfrom IPython.display import FileLink\nFileLink(r'.\/project.avi')","15eb8ac8":"\ndef run_demo():\n    sample_video_path='..\/input\/ucsddataset\/UCSD_Anomaly_Dataset.v1p2\/UCSDped1\/Test\/Test024\/'\n    video_name = os.path.basename(sample_video_path).split('.')[0]\n\n    # read video\n    video_clips, num_frames = get_video_clips(sample_video_path)\n\n    print(\"Number of clips in the video : \", len(video_clips))\n\n    # build models\n    feature_extractor = c3d_feature_extractor()\n    classifier_model = build_classifier_model()\n\n    print(\"Models initialized\")\n\n    # extract features\n    rgb_features = []\n    for i, clip in enumerate(video_clips):\n        clip = np.array(clip)\n        if len(clip) < frame_count:\n            continue\n\n        clip = preprocess_input(clip)\n        rgb_feature = feature_extractor.predict(clip)[0]\n        rgb_features.append(rgb_feature)\n\n        print(\"Processed clip : \", i)\n\n    rgb_features = np.array(rgb_features)\n\n    # bag features\n    rgb_feature_bag = interpolate(rgb_features, features_per_bag)\n\n    # classify using the trained classifier model\n    predictions = classifier_model.predict(rgb_feature_bag)\n\n    predictions = np.array(predictions).squeeze()\n\n    predictions = extrapolate(predictions, num_frames)\n\n    save_path = os.path.join('.\/', video_name + '.gif')\n    # visualize predictions\n    visualize_predictions(sample_video_path, predictions, save_path)\n\n\nif __name__ == '__main__':\n    run_demo()","89adc17a":"\nfrom IPython.display import FileLink\nFileLink(r'.\/Test024.gif')","57396ab9":"\ndef run_demo():\n    sample_video_path='..\/input\/ucsddataset\/UCSD_Anomaly_Dataset.v1p2\/UCSDped1\/Test\/Test032'\n    video_name = os.path.basename(sample_video_path).split('.')[0]\n\n    # read video\n    video_clips, num_frames = get_video_clips(sample_video_path)\n\n    print(\"Number of clips in the video : \", len(video_clips))\n\n    # build models\n    feature_extractor = c3d_feature_extractor()\n    classifier_model = build_classifier_model()\n\n    print(\"Models initialized\")\n\n    # extract features\n    rgb_features = []\n    for i, clip in enumerate(video_clips):\n        clip = np.array(clip)\n        if len(clip) < frame_count:\n            continue\n\n        clip = preprocess_input(clip)\n        rgb_feature = feature_extractor.predict(clip)[0]\n        rgb_features.append(rgb_feature)\n\n        print(\"Processed clip : \", i)\n\n    rgb_features = np.array(rgb_features)\n\n    # bag features\n    rgb_feature_bag = interpolate(rgb_features, features_per_bag)\n\n    # classify using the trained classifier model\n    predictions = classifier_model.predict(rgb_feature_bag)\n\n    predictions = np.array(predictions).squeeze()\n\n    predictions = extrapolate(predictions, num_frames)\n\n    save_path = os.path.join('.\/', video_name + '.gif')\n    # visualize predictions\n    visualize_predictions(sample_video_path, predictions, save_path)\n\n\nif __name__ == '__main__':\n    run_demo()","4d63624a":"\nfrom IPython.display import FileLink\nFileLink(r'.\/Test032.gif')","0717fe45":"\ndef run_demo():\n    sample_video_path='..\/input\/ucsddataset\/UCSD_Anomaly_Dataset.v1p2\/UCSDped1\/Test\/Test004'\n    video_name = os.path.basename(sample_video_path).split('.')[0]\n\n    # read video\n    video_clips, num_frames = get_video_clips(sample_video_path)\n\n    print(\"Number of clips in the video : \", len(video_clips))\n\n    # build models\n    feature_extractor = c3d_feature_extractor()\n    classifier_model = build_classifier_model()\n\n    print(\"Models initialized\")\n\n    # extract features\n    rgb_features = []\n    for i, clip in enumerate(video_clips):\n        clip = np.array(clip)\n        if len(clip) < frame_count:\n            continue\n\n        clip = preprocess_input(clip)\n        rgb_feature = feature_extractor.predict(clip)[0]\n        rgb_features.append(rgb_feature)\n\n        print(\"Processed clip : \", i)\n\n    rgb_features = np.array(rgb_features)\n\n    # bag features\n    rgb_feature_bag = interpolate(rgb_features, features_per_bag)\n\n    # classify using the trained classifier model\n    predictions = classifier_model.predict(rgb_feature_bag)\n\n    predictions = np.array(predictions).squeeze()\n\n    predictions = extrapolate(predictions, num_frames)\n\n    save_path = os.path.join('.\/', video_name + '.gif')\n    # visualize predictions\n    visualize_predictions(sample_video_path, predictions, save_path)\n\n\nif __name__ == '__main__':\n    run_demo()","c8475132":"\nfrom IPython.display import FileLink\nFileLink(r'.\/Test004.gif')","569ee428":"\ndef run_demo():\n    sample_video_path='..\/input\/ucsddataset\/UCSD_Anomaly_Dataset.v1p2\/UCSDped1\/Test\/Test024'\n    video_name = os.path.basename(sample_video_path).split('.')[0]\n\n    # read video\n    video_clips, num_frames = get_video_clips(sample_video_path)\n\n    print(\"Number of clips in the video : \", len(video_clips))\n\n    # build models\n    feature_extractor = c3d_feature_extractor()\n    classifier_model = build_classifier_model()\n\n    print(\"Models initialized\")\n\n    # extract features\n    rgb_features = []\n    for i, clip in enumerate(video_clips):\n        clip = np.array(clip)\n        if len(clip) < frame_count:\n            continue\n\n        clip = preprocess_input(clip)\n        rgb_feature = feature_extractor.predict(clip)[0]\n        rgb_features.append(rgb_feature)\n\n        print(\"Processed clip : \", i)\n\n    rgb_features = np.array(rgb_features)\n\n    # bag features\n    rgb_feature_bag = interpolate(rgb_features, features_per_bag)\n\n    # classify using the trained classifier model\n    predictions = classifier_model.predict(rgb_feature_bag)\n\n    predictions = np.array(predictions).squeeze()\n\n    predictions = extrapolate(predictions, num_frames)\n\n    save_path = os.path.join('.\/', video_name + '.gif')\n    # visualize predictions\n    visualize_predictions(sample_video_path, predictions, save_path)\n\n\nif __name__ == '__main__':\n    run_demo()","0f3bea8d":"\nfrom IPython.display import FileLink\nFileLink(r'.\/Test024.gif')","7e9d5ec1":"\ndef run_demo():\n    sample_video_path='..\/input\/ucsddataset\/UCSD_Anomaly_Dataset.v1p2\/UCSDped1\/Test\/Test012'\n    video_name = os.path.basename(sample_video_path).split('.')[0]\n\n    # read video\n    video_clips, num_frames = get_video_clips(sample_video_path)\n\n    print(\"Number of clips in the video : \", len(video_clips))\n\n    # build models\n    feature_extractor = c3d_feature_extractor()\n    classifier_model = build_classifier_model()\n\n    print(\"Models initialized\")\n\n    # extract features\n    rgb_features = []\n    for i, clip in enumerate(video_clips):\n        clip = np.array(clip)\n        if len(clip) < frame_count:\n            continue\n\n        clip = preprocess_input(clip)\n        rgb_feature = feature_extractor.predict(clip)[0]\n        rgb_features.append(rgb_feature)\n\n        print(\"Processed clip : \", i)\n\n    rgb_features = np.array(rgb_features)\n\n    # bag features\n    rgb_feature_bag = interpolate(rgb_features, features_per_bag)\n\n    # classify using the trained classifier model\n    predictions = classifier_model.predict(rgb_feature_bag)\n\n    predictions = np.array(predictions).squeeze()\n\n    predictions = extrapolate(predictions, num_frames)\n\n    save_path = os.path.join('.\/', video_name + '.gif')\n    # visualize predictions\n    visualize_predictions(sample_video_path, predictions, save_path)\n\n\nif __name__ == '__main__':\n    run_demo()","cbf147d7":"\ndef run_demo():\n    sample_video_path='..\/input\/ucsddataset\/UCSD_Anomaly_Dataset.v1p2\/UCSDped1\/Test\/Test036'\n    video_name = os.path.basename(sample_video_path).split('.')[0]\n\n    # read video\n    video_clips, num_frames = get_video_clips(sample_video_path)\n\n    print(\"Number of clips in the video : \", len(video_clips))\n\n    # build models\n    feature_extractor = c3d_feature_extractor()\n    classifier_model = build_classifier_model()\n\n    print(\"Models initialized\")\n\n    # extract features\n    rgb_features = []\n    for i, clip in enumerate(video_clips):\n        clip = np.array(clip)\n        if len(clip) < frame_count:\n            continue\n\n        clip = preprocess_input(clip)\n        rgb_feature = feature_extractor.predict(clip)[0]\n        rgb_features.append(rgb_feature)\n\n        print(\"Processed clip : \", i)\n\n    rgb_features = np.array(rgb_features)\n\n    # bag features\n    rgb_feature_bag = interpolate(rgb_features, features_per_bag)\n\n    # classify using the trained classifier model\n    predictions = classifier_model.predict(rgb_feature_bag)\n\n    predictions = np.array(predictions).squeeze()\n\n    predictions = extrapolate(predictions, num_frames)\n\n    save_path = os.path.join('.\/', video_name + '.gif')\n    # visualize predictions\n    visualize_predictions(sample_video_path, predictions, save_path)\n\n\nif __name__ == '__main__':\n    run_demo()","3737a135":"\ndef run_demo():\n    sample_video_path='..\/input\/ucsddataset\/UCSD_Anomaly_Dataset.v1p2\/UCSDped1\/Test\/Test035'\n    video_name = os.path.basename(sample_video_path).split('.')[0]\n\n    # read video\n    video_clips, num_frames = get_video_clips(sample_video_path)\n\n    print(\"Number of clips in the video : \", len(video_clips))\n\n    # build models\n    feature_extractor = c3d_feature_extractor()\n    classifier_model = build_classifier_model()\n\n    print(\"Models initialized\")\n\n    # extract features\n    rgb_features = []\n    for i, clip in enumerate(video_clips):\n        clip = np.array(clip)\n        if len(clip) < frame_count:\n            continue\n\n        clip = preprocess_input(clip)\n        rgb_feature = feature_extractor.predict(clip)[0]\n        rgb_features.append(rgb_feature)\n\n        print(\"Processed clip : \", i)\n\n    rgb_features = np.array(rgb_features)\n\n    # bag features\n    rgb_feature_bag = interpolate(rgb_features, features_per_bag)\n\n    # classify using the trained classifier model\n    predictions = classifier_model.predict(rgb_feature_bag)\n\n    predictions = np.array(predictions).squeeze()\n\n    predictions = extrapolate(predictions, num_frames)\n\n    save_path = os.path.join('.\/', video_name + '.gif')\n    # visualize predictions\n    visualize_predictions(sample_video_path, predictions, save_path)\n\n\nif __name__ == '__main__':\n    run_demo()","efd2dac1":"\ndef run_demo():\n    sample_video_path='..\/input\/ucsddataset\/UCSD_Anomaly_Dataset.v1p2\/UCSDped1\/Test\/Test033'\n    video_name = os.path.basename(sample_video_path).split('.')[0]\n\n    # read video\n    video_clips, num_frames = get_video_clips(sample_video_path)\n\n    print(\"Number of clips in the video : \", len(video_clips))\n\n    # build models\n    feature_extractor = c3d_feature_extractor()\n    classifier_model = build_classifier_model()\n\n    print(\"Models initialized\")\n\n    # extract features\n    rgb_features = []\n    for i, clip in enumerate(video_clips):\n        clip = np.array(clip)\n        if len(clip) < frame_count:\n            continue\n\n        clip = preprocess_input(clip)\n        rgb_feature = feature_extractor.predict(clip)[0]\n        rgb_features.append(rgb_feature)\n\n        print(\"Processed clip : \", i)\n\n    rgb_features = np.array(rgb_features)\n\n    # bag features\n    rgb_feature_bag = interpolate(rgb_features, features_per_bag)\n\n    # classify using the trained classifier model\n    predictions = classifier_model.predict(rgb_feature_bag)\n\n    predictions = np.array(predictions).squeeze()\n\n    predictions = extrapolate(predictions, num_frames)\n\n    save_path = os.path.join('.\/', video_name + '.gif')\n    # visualize predictions\n    visualize_predictions(sample_video_path, predictions, save_path)\n\n\nif __name__ == '__main__':\n    run_demo()","7bfe7ef7":"\nfrom IPython.display import FileLink\nFileLink(r'.\/Test035.gif')","5f2a1f09":"## PAPER : \n## Real-world Anomaly Detection in Surveillance Videos\nhttps:\/\/arxiv.org\/pdf\/1801.04264v3.pdf\n\n","e3c53361":"### Training:"}}