{"cell_type":{"eecf95b1":"code","c2cb9633":"code","6ec3ee93":"code","4b09126a":"code","486cf6a7":"code","2364e507":"code","e27814ab":"code","5b70ede4":"code","394e085c":"code","2cebe923":"code","f2244fd8":"code","722ddc90":"code","1ac32cb7":"code","568a1e0d":"code","cd91abd4":"code","cb3c43be":"code","08999fa9":"code","60f1d253":"code","c2d59797":"code","b527661a":"code","68ddd356":"code","8971d54e":"code","e493123f":"code","021f868a":"code","c418db94":"code","e52c1861":"code","1540bcf6":"code","60822d32":"code","1c8bc47a":"code","a9e18118":"markdown","a0794322":"markdown","87c7123a":"markdown","5358c57d":"markdown","608b1c5c":"markdown","2ca83f6e":"markdown","985d46de":"markdown","faad32ab":"markdown","f49cbd36":"markdown","13a2982b":"markdown","d63c83d3":"markdown","00c020f5":"markdown","384fadef":"markdown","19eefcaf":"markdown","009f7bce":"markdown","17b7615a":"markdown","8a134975":"markdown","33878ab0":"markdown","bd9255e7":"markdown","f8a009e7":"markdown","d0eebd34":"markdown"},"source":{"eecf95b1":"import numpy as np\nimport pandas as pd\nfrom pandas_profiling import ProfileReport\nfrom scipy.stats import zscore\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom yellowbrick.features.pca import PCADecomposition\nfrom sklearn.feature_selection import SelectKBest, mutual_info_regression, RFE\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler","c2cb9633":"df=pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")","6ec3ee93":"profile_report = ProfileReport(df, title='Profile Report', html={'style':{'full_width':True}})","4b09126a":"profile_report.to_notebook_iframe()","486cf6a7":"# basement features\ndf.filter(regex='\\Bsmt').columns","2364e507":"# 5 higher square feet\ndfbs=df.nlargest(5, 'TotalBsmtSF')\ndfbs","e27814ab":"# excluding the bigger basement and selecting some features\ndfbs2=dfbs.loc[[332, 496, 523,440], [\"LotArea\", \"YearBuilt\", \"TotalBsmtSF\", \"BsmtCond\",\"SalePrice\" ]]\ndfbs2","5b70ede4":"# other options for selecting\/slicing\nprint(dfbs2.equals(dfbs.loc[332:440, [\"LotArea\", \"YearBuilt\", \"TotalBsmtSF\", \"BsmtCond\",\"SalePrice\" ]]))\nprint(dfbs2.equals(dfbs.iloc[[1,2,3,4],[4, 19, 38, 31, 80]]))\nprint(dfbs2.equals(dfbs.iloc[1:5,[4, 19, 38, 31, 80]]))","394e085c":"# checking about Basement Condition\nprint(df[\"BsmtCond\"].value_counts())","2cebe923":"# checking about Basement Condition\ndf.groupby(by=\"BsmtCond\")['SalePrice'].mean().sort_values(ascending=False)","f2244fd8":"print(\"The dataset has {} rows and {} columns. {} duplicated rows\".format(df.shape[0], df.shape[1],df.duplicated().sum()))","722ddc90":"s_types= df.dtypes\ns_head= df.apply(lambda x: x[0:3].tolist())\n\nexplo1 = pd.DataFrame({'Types': s_types,\n                      'Head': s_head}).sort_values(by=['Types'],ascending=False)\nexplo1.transpose()","1ac32cb7":"s_missing= df.isnull().sum()\ns_missingper= (df.isnull().sum()\/df.shape[0])*100\n\nexplo2 = pd.DataFrame({'Types': s_types,\n                       'Missing': s_missing,\n                      'Missing%': s_missingper,}).sort_values(by=['Missing%','Types'],ascending=False)\nexplo2.transpose()","568a1e0d":"for col in ('Alley','Utilities','MasVnrType','BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1',\n            'BsmtFinType2','FireplaceQu','GarageType','GarageFinish','GarageQual','GarageCond',\n           'PoolQC','Fence','MiscFeature'):\n    df[col]=df[col].fillna('None')","cd91abd4":"for col in ('Electrical','MSZoning','Exterior1st','Exterior2nd','KitchenQual','SaleType','Functional'):\n    df[col]=df[col].fillna(df[col].mode()[0])","cb3c43be":"for col in ('MasVnrArea','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF','BsmtFullBath','BsmtHalfBath',\n            'GarageYrBlt','GarageCars','GarageArea'):\n    df[col]=df[col].fillna(0) ","08999fa9":"df['LotFrontage']=df['LotFrontage'].fillna(df['LotFrontage'].mean())","60f1d253":"print(df.isnull().sum().sum())","c2d59797":"#npo= number of possible outliers\nlist_of_numerics=df.select_dtypes(include=['float','int']).columns\ns_npo= df.apply(lambda x: sum(i>3 for i in np.abs(zscore(x)))if x.name in list_of_numerics else '') \ns_npo2= df.apply(lambda x: sum((x < (x.quantile(0.25) - 1.5 * (x.quantile(0.75)- x.quantile(0.25))))|\n                               (x > (x.quantile(0.75) + 1.5 * (x.quantile(0.75)- x.quantile(0.25)))))\n                 if x.name in list_of_numerics else '')\n\nexplo3 = pd.DataFrame({'NPO': s_npo,\n                       'NPO2': s_npo2}).sort_values(by=['NPO', 'NPO2'])\nexplo3.transpose()","b527661a":"fig, axes = plt.subplots(1,2, figsize=(12,5))\n\nax1= sns.scatterplot(x='GrLivArea', y='SalePrice', data= df,ax=axes[0])\nax2= sns.boxplot(x='GrLivArea', data= df,ax=axes[1])","68ddd356":"#removing outliers recomended by author\ndf= df[df['GrLivArea']<4000]","8971d54e":"plt.style.use('classic')\nfig, axes = plt.subplots(1,4, figsize=(22,5))\n\nax1= sns.distplot(df.LotFrontage, bins= 30, hist_kws={'edgecolor':'k'},ax=axes[0])\nax1.set_title('LotFrontage')\n\nax2= sns.distplot(MinMaxScaler().fit_transform(df[['LotFrontage']]), bins= 30, hist_kws={'edgecolor':'k'},ax=axes[1])\nax2.set_title('LotFrontage with normalization')\n\nax3= sns.distplot(StandardScaler().fit_transform(df[['LotFrontage']]), bins= 30, hist_kws={'edgecolor':'k'},ax=axes[2])\nax3.set_title('LotFrontage with standardization')\n\nax4= sns.distplot(np.log(df[['LotFrontage']]), bins= 30, hist_kws={'edgecolor':'k'},ax=axes[3])\nax4.set_title('LotFrontage with log transformation')","e493123f":"df_num=pd.get_dummies(df)\nx= df_num.drop(['SalePrice'], axis=1)\ny= df_num.SalePrice\n\nvisu= PCADecomposition(scale=True)\nvisu.fit_transform(x,y)\nvisu.show()","021f868a":"np.random.seed(1)\nuni=SelectKBest( mutual_info_regression, k=5).fit(x,y)\nprint(x.columns[uni.get_support(indices=True)].tolist())","c418db94":"est= DecisionTreeRegressor(random_state=1)\neli=RFE(est, 5).fit(x,y)\nprint(x.columns[eli.support_].tolist())","e52c1861":"RF= RandomForestRegressor(random_state=1)\nRF.fit(x,y)\nimportances=RF.feature_importances_\ns_importances=pd.Series(importances, index=x.columns).sort_values(ascending=False)","1540bcf6":"plt.style.use('dark_background')\nfig, axes = plt.subplots(1,2, figsize=(13,5), sharey=True) \n\nax1= sns.barplot(x=s_importances.index[0:5], y= s_importances[0:5], ax=axes[0])\nax1.set_title('Feature Importance')\n\ntrees=pd.DataFrame(data=[tree.feature_importances_ for tree in RF], columns=x.columns)[s_importances[0:5].index.tolist()]\nax2= sns.boxplot(data=trees, ax=axes[1])\nax2.set_title('Feature Importance Distributions')","60822d32":"df[\"LotFrontage_bin_interval\"]= pd.cut(df.LotFrontage, 3, labels=[\"small\",\"medium\",\"large\"])\n\ndf[\"LotFrontage_bin_frequency\"]= pd.qcut(df.LotFrontage, 3, labels=[\"small\",\"medium\",\"large\"])","1c8bc47a":"plt.style.use('default')\nfig, axes= plt.subplots(1,3, figsize=(13,4),sharey=True)\n\nax1= sns.distplot(df.LotFrontage, bins=40, hist_kws={'edgecolor':'k'}, color='darkorchid',kde=False,ax=axes[0])\nax1.set_title('Histogram')\nax1= sns.despine()\n\nax2= sns.countplot(x='LotFrontage_bin_frequency', data=df, palette=\"Purples\", ax=axes[1])\nax2.set_title('Binning with equal frequency')\nax2= sns.despine()\n\nax3= sns.countplot(x='LotFrontage_bin_interval', data=df, palette=\"Purples\", ax=axes[2])\nax3.set_title('Binning with equal interval')\nax3= sns.despine();","a9e18118":"# 3- Transforming\nFeature scaling is an important phase in data preparation. Two common methods are normalization (in general about rescale to range 0-1) and standardization (in general about rescale to have mean 0 and standard deviation 1).\nThese transformations doesn't change data shape as the log transformation does. Examples below.\nOn this kernel, the transformations will be done according to the need.","a0794322":"# Data Preparation with Python\nOn this notebook I intend to explore the topic data preparation with python.","87c7123a":"While checking \"Types\" and \"Head\" I didn't find inconsistent types","5358c57d":"#### 4.2.3 Feature Importance\nLet's use Random Forest algorithm to get the feature importances ","608b1c5c":"# 1- Knowing the dataset","2ca83f6e":"#### 4.2.2 Recursive Feature Elimination\nThis method will remove less important features while building different models and checking the accuracy. ","985d46de":"### 2.2 - Inconsistent types?","faad32ab":"## 1.1 - pandas_profiling\npandas_profiling produces reports from pandas dataframe providing a great exploratory analysis.  \nhttps:\/\/pandas-profiling.github.io\/pandas-profiling\/docs\/","f49cbd36":"Although Zscore and IQR methods suggest several outliers, for while I'm going to focus on outliers with remotion recommended by the dataset author.","13a2982b":"### 4.1- Feature Extraction\nFeature extraction is about to get new features and lower dimensionality. Here, the idea is to transform the dataset.  Principal Component Analysis is one of the main technique.  \nI'm going to use Yellowbrick to visualize PCA Decomposition. All will be based on official documentation: http:\/\/www.scikit-yb.org\/en\/latest\/api\/index.html","d63c83d3":"# 4- Feature Engineering","00c020f5":"### 2.1 - Any duplicated rows?","384fadef":"# 2- Cleaning\nHere, I am going to build functions to check, more directly, some possible problems: duplicate rows, inconsistent variable types, missing values and outliers.","19eefcaf":"### 2.4 - Outliers?","009f7bce":"### 4.3.1 - Discretization\nDiscretization is about transforming numeric variables into intervals.  \nIt's useful when the exact value is not so important and we can simplify. This action can improve model performance.\nBinning is a kind of discretization which has mainly 2 types: equal interval and equal frequency.","17b7615a":"### 2.3 - Missing values?","8a134975":"### 4.2- Feature Selection\nFeature Selection is about select the most important features. This selection can help improving model performance and results.  \nI'm going to try:\n* Univariate Selection\n* Recursive Feature Elimination\n* Selection by Feature Importance\n\n(Selecting only 5 features for better visualizations)","33878ab0":"## 1.2 - Knowing more about the dataset subsetting and groupping\n\nLet's know more about the basements. I'm still on Parasite movie mood \ud83d\udc40  \n\nGoals:  \n* Check basement related features\n* Check bigger basements\n* Exclude the bigger basement and select some features\n* Check about Basement Condition","bd9255e7":"## 4.3- Feature Creation","f8a009e7":"#### 4.2.1 Univariate Selection \nThe features will be selected based on univariate statistical test. What variables have stronger relationship with the response?","d0eebd34":"Depending on the categorical variable, missing value can means \"None\" (which I will fill with \"None\") or \"Not Available\" (which I will fill with the mode).  \nDepending on the numeric variable, missing value can means 0 (which I will fill with 0) or \"Not Available\" (which I will fill with the mean)."}}