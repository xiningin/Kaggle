{"cell_type":{"18be13a8":"code","3758d05c":"code","ef59fb2a":"code","791f6a5e":"code","22d3859b":"code","cccb57a6":"code","3d604a16":"code","d3a4b980":"code","11de9ba9":"code","7735119f":"code","a584317e":"code","245fff61":"markdown","d929993b":"markdown","38cf11e9":"markdown","ca5139bc":"markdown","a5aeca67":"markdown","1770ba65":"markdown","7049dfdf":"markdown","bb393d59":"markdown","53808755":"markdown","66224dd1":"markdown"},"source":{"18be13a8":"import numpy as np\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\nimport time\nimport shutil\nimport PIL\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\nfrom tensorflow import keras\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.layers import Dense, Activation,Dropout,BatchNormalization, Input\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.optimizers import Adam, Adamax\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport logging\nlogging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\nfrom IPython.core.display import display, HTML","3758d05c":"def tr_plot(tr_data, start_epoch):\n    #Plot the training and validation data\n    tacc=tr_data.history['accuracy']\n    tloss=tr_data.history['loss']\n    vacc=tr_data.history['val_accuracy']\n    vloss=tr_data.history['val_loss']\n    Epoch_count=len(tacc)+ start_epoch\n    Epochs=[]\n    for i in range (start_epoch ,Epoch_count):\n        Epochs.append(i+1)   \n    index_loss=np.argmin(vloss)#  this is the epoch with the lowest validation loss\n    val_lowest=vloss[index_loss]\n    index_acc=np.argmax(vacc)\n    acc_highest=vacc[index_acc]\n    plt.style.use('fivethirtyeight')\n    sc_label='best epoch= '+ str(index_loss+1 +start_epoch)\n    vc_label='best epoch= '+ str(index_acc + 1+ start_epoch)\n    fig,axes=plt.subplots(nrows=1, ncols=2, figsize=(20,8))\n    axes[0].plot(Epochs,tloss, 'r', label='Training loss')\n    axes[0].plot(Epochs,vloss,'g',label='Validation loss' )\n    axes[0].scatter(index_loss+1 +start_epoch,val_lowest, s=150, c= 'blue', label=sc_label)\n    axes[0].set_title('Training and Validation Loss')\n    axes[0].set_xlabel('Epochs')\n    axes[0].set_ylabel('Loss')\n    axes[0].legend()\n    axes[1].plot (Epochs,tacc,'r',label= 'Training Accuracy')\n    axes[1].plot (Epochs,vacc,'g',label= 'Validation Accuracy')\n    axes[1].scatter(index_acc+1 +start_epoch,acc_highest, s=150, c= 'blue', label=vc_label)\n    axes[1].set_title('Training and Validation Accuracy')\n    axes[1].set_xlabel('Epochs')\n    axes[1].set_ylabel('Accuracy')\n    axes[1].legend()\n    plt.tight_layout\n    #plt.style.use('fivethirtyeight')\n    plt.show()\n","ef59fb2a":"def print_in_color(txt_msg,fore_tupple,back_tupple,):\n    #prints the text_msg in the foreground color specified by fore_tupple with the background specified by back_tupple \n    #text_msg is the text, fore_tupple is foregroud color tupple (r,g,b), back_tupple is background tupple (r,g,b)\n    rf,gf,bf=fore_tupple\n    rb,gb,bb=back_tupple\n    msg='{0}' + txt_msg\n    mat='\\33[38;2;' + str(rf) +';' + str(gf) + ';' + str(bf) + ';48;2;' + str(rb) + ';' +str(gb) + ';' + str(bb) +'m' \n    print(msg .format(mat), flush=True)\n    print('\\33[0m', flush=True) # returns default print color to back to black\n    return","791f6a5e":"class SOT(keras.callbacks.Callback):\n    def __init__(self, model,  train_thold, valid_thold):\n        super(SOT, self).__init__()\n        self.model=model        \n        self.train_thold=train_thold\n        self.valid_thold=valid_thold\n        \n    def on_train_begin(self, logs=None):\n        print('Starting Training - training will halt if training accuracy achieves or exceeds ', self.train_thold)\n        print ('and validation accuracy meets or exceeds ', self.valid_thold) \n            \n    def on_train_batch_end(self, batch, logs=None):\n        acc=logs.get('accuracy')* 100  # get training accuracy \n        loss=logs.get('loss')\n        msg='{0:1s}processed batch {1:4s}  training accuracy= {2:8.3f}  loss: {3:8.5f}'.format(' ', str(batch),  acc, loss)\n        print(msg, '\\r', end='') # prints over on the same line to show running batch count       \n        \n    def on_epoch_end(self,epoch, logs=None):             \n        tacc=logs.get('accuracy')           \n        vacc=logs.get('val_accuracy')\n        print(f'for epoch {epoch+1} training accuracy = {tacc:6.4f} and validation accuracy = {vacc:6.3f}')\n        if tacc>= self.train_thold and vacc>= self.valid_thold:\n            print( f'\\training accuracy and validation accuracy reached the thresholds on {epoch + 1}' )\n            self.model.stop_training = True # stop training","22d3859b":"train_dir=r'..\/input\/verticalhorizontal\/dataset\/train'\ntest_dir=r'..\/input\/verticalhorizontal\/dataset\/test'","cccb57a6":"img_shape=(128,128,3) # use 128 X128 versus 224 X224 to reduce training time\nimg_size=(img_shape[0], img_shape[1])\nmsg='For training set'\nprint_in_color(msg, (0,255,255), (55,65,80))\ntrain_ds=tf.keras.preprocessing.image_dataset_from_directory(validation_split=.1, subset='training',\n            directory=train_dir, image_size=img_size, seed=123, batch_size=30, shuffle=True)\nmsg=' For validation set'\nprint_in_color(msg, (0,255,255), (55,65,80))\nvalid_ds=tf.keras.preprocessing.image_dataset_from_directory(validation_split=.1, subset='validation',\n            directory=train_dir, image_size=img_size, seed=123, batch_size=30, shuffle=True)\nmsg='For the test set'\nprint_in_color(msg, (0,255,255), (55,65,80))\ntest_ds=tf.keras.preprocessing.image_dataset_from_directory(\n            directory=test_dir, image_size=img_size, shuffle=False, batch_size=30) # set shuffle=False to keep file order","3d604a16":"class_names=train_ds.class_names\nprint (class_names)\nclass_count=len(class_names)\nplt.figure(figsize=(20,20))\nfor images, labels in train_ds.take(1):\n    for i in range (25):\n        plt.subplot(5,5,i +1)\n        img=images[i]\/255         \n        plt.title(class_names[labels[i]], color='blue', fontsize=12)\n        plt.imshow(img)\n        plt.axis('off')\n    plt.show()\n","d3a4b980":"base_model=tf.keras.applications.EfficientNetB3(include_top=False, weights=\"imagenet\",input_shape=img_shape, pooling='max') \nx=base_model.output\nbase_model.trainable=True # Set base model as  trainable \nx=keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001 )(x)\nx = Dense(256, kernel_regularizer = regularizers.l2(l = 0.016),activity_regularizer=regularizers.l1(0.006),\n                bias_regularizer=regularizers.l1(0.006) ,activation='relu')(x)\nx=Dropout(rate=.45, seed=123)(x)        \noutput=Dense(class_count, activation='softmax')(x)\nmodel=Model(inputs=base_model.input, outputs=output)\nmodel.compile(Adamax(lr=.001), loss='sparse_categorical_crossentropy', metrics=['accuracy']) ","11de9ba9":"epochs=20\ntrain_thold=.98\nvalid_thold=.96\n# halt training when training accuracy meets or exceeds .98 AND validaton accuracy meets or exceeds .96\nrlronp=tf.keras.callbacks.ReduceLROnPlateau( monitor=\"val_loss\", factor=0.5,  patience=1, verbose=1)\ncallbacks=[rlronp,SOT(model,train_thold, valid_thold )]\nhistory=model.fit( train_ds, validation_data=valid_ds, epochs=epochs, verbose=0, callbacks=callbacks)\n  ","7735119f":"tr_plot(history, 0)","a584317e":"ytrue=[]\nfor images, label in test_ds:   \n    for e in label:\n        ytrue.append(class_names[e]) # list of class names associated with each image file in test dataset \nypred=[]\nerrors=0\ncount=0\npreds=model.predict(test_ds, verbose=1) # predict on the test data\nfor i, p in enumerate(preds):\n    count +=1\n    index=np.argmax(p) # get index of prediction with highest probability\n    klass=class_names[index] \n    ypred.append(klass)  \n    if klass != ytrue[i]:\n        errors +=1\nacc= (count-errors)* 100\/count\nmsg=f'there were {count-errors} correct predictions in {count} tests for an accuracy of {acc:6.2f} % '\nprint_in_color(msg, (0,255,255), (55,65,80)) \nypred=np.array(ypred)\nytrue=np.array(ytrue)\nclr = classification_report(ytrue, ypred, target_names=class_names)\nprint(\"Classification Report:\\n----------------------\\n\", clr)    ","245fff61":"### create reduce learning rate on plateau callback and instantiate TRC callback\n### then train the model","d929993b":"### show some  images- ","38cf11e9":"### create the model","ca5139bc":"### create a function to plot training data from model.fit","a5aeca67":"### define function to print text in RGB foreground and background colors","1770ba65":"### make predictions on test set, compute accuracy and create classification report","7049dfdf":"### define the directories ","bb393d59":"### create the Datasets","53808755":"### define a sub class of keras callbacks\nThe SOT custom callback is useful when you are using training your model and you wish to halt training the metrics\ntraining accuracy meets or exceeds a specified training threshold AND the metric validation accuracy meets or exceeds\na specified validation threshold\n\ncallbacks=[SOC(model, train_thold, valid_thold)] where:\n\n - model is the variable name for your compiled model \n - train_thold is a float specifying the threshold level for training accuracy\n - valid_thold is a float specifying the threshold level for validation accuracy\n \n Note: Both metrics have to exceed the threshold for training to halt. If you want to stop training based soley on the training accuracy set valid_thold=0.0. Similarly if you want to stop training soley on validation accuracy set train_thold=0.0.","66224dd1":"### plot the training data"}}