{"cell_type":{"5e73dba9":"code","ab1ade84":"code","9a525d89":"code","7818732a":"code","62a48713":"code","614f06ee":"code","525c56cc":"code","7347bca8":"code","2ac15ddc":"code","14fc0ba1":"code","4bba9db5":"code","e69bc850":"code","b55c1009":"code","b2ad7a93":"code","707b7526":"code","f4994158":"code","0c238180":"code","3eedba7d":"code","f3d4fd2a":"markdown","690e1562":"markdown","8f8bf04a":"markdown","e3ca0c8d":"markdown","d9d1d071":"markdown","8798779d":"markdown","b3fec9f2":"markdown","19bb4297":"markdown","38fbe501":"markdown","de41b953":"markdown","22823487":"markdown","128a1889":"markdown","de19256f":"markdown","e9b437d1":"markdown","6feb86e3":"markdown","0b02f72f":"markdown","af7748ab":"markdown","3bf04ab7":"markdown"},"source":{"5e73dba9":"# Importing libraries\n\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\nimport numpy as np\nimport pandas as pd\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.utils import to_categorical\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom mlxtend.plotting import plot_decision_regions","ab1ade84":"df = pd.read_csv('..\/input\/lrb-data\/L_R data.csv')\ndf.info()\ndf.describe()","9a525d89":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(df.Traget.drop_duplicates()) \ndf.Traget= le.transform(df.Traget)\n\nsns.heatmap(df.corr(), cmap=\"YlGnBu\", annot = True)\nplt.show()","7818732a":"plt.figure(figsize=(25,10))\ndf.corr()['Traget'].sort_values(ascending = False).plot(kind='bar')\nplt.show()","62a48713":"plt.figure(figsize = (12,8))\nplt.grid(True)\nax = sns.countplot(x='Traget', data=df, palette='Spectral_r')\nfor p in ax.patches:\n        ax.annotate('{:.0f}'.format(p.get_height()), (p.get_x()+0.3, p.get_height()+0.5))","614f06ee":"df01 = df.melt('Traget', var_name='cols',  value_name='vals')\ng = sns.catplot(x=\"Traget\", y=\"vals\", hue='cols', data=df01, kind='strip')","525c56cc":"X=df.iloc[:,1:]\ny=df.iloc[:,0]\n\nscaler = MinMaxScaler()\nX = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=.3,random_state=2,stratify=y)","7347bca8":"model=SVC()\nmodel.fit(X_train,y_train)\nprint(f' Training Accuracy is:- {model.score(X_train,y_train)}')\nf'Test Accuracy is:- {model.score(X_test,y_test)}'","2ac15ddc":"k = range(1,20)\ntrainingAccuracy = []\ntestAccuracy=[]\nfor i in k:\n    knn = KNeighborsClassifier(n_neighbors=i,n_jobs=15,p=1,weights='distance')\n    knn.fit(X_train,y_train)\n    trainingacc = knn.score(X_train,y_train)\n    trainingAccuracy.append(trainingacc)\n    testAccuracy.append(knn.score(X_test,y_test))\n\nfig, axes = plt.subplots(nrows=1, ncols=1, figsize=(10,4))\nplt.xlabel(\"value of K\")\nplt.ylabel(\"Accuracy of test and training\")\nplt.title(\"Select best value of k\")\nplt.plot(k,trainingAccuracy)\nplt.plot(k,testAccuracy)\n  #axes[0].legend(['loss','val_loss'])\naxes.legend([\"Training Accurracy\",\"Test Accuracy\"])\nprint(\"\\n Best Test accuracy is:- \",max(testAccuracy))","14fc0ba1":"from sklearn.neighbors import KNeighborsClassifier\nknn_model=KNeighborsClassifier(n_jobs=15,n_neighbors=36,p=1,weights='distance')\nknn_model.fit(X_train,y_train)\nprint(f' Training Accuracy {knn_model.score(X_train,y_train)}')\nf' Testing Accuracy {knn_model.score(X_test,y_test)}'","4bba9db5":"k_range = list(range(1,50))\nweight_options = [\"uniform\", \"distance\"]\npe=[1,2]\n\nparam_grid = dict(n_neighbors = k_range, weights = weight_options,p=pe)\nknn = KNeighborsClassifier()\nknngrid = GridSearchCV(knn, param_grid, cv = 10, scoring = 'accuracy',n_jobs=15)\nknngrid.fit(X_train,y_train)\n\nprint (\"Best score on 10 folds split Data on Train split is :- \",knngrid.best_score_)\nprint (\"\\n Best Param:- \",knngrid.best_params_)\nprint (\"\\n Best KNN Metric:- \", knngrid.best_estimator_)\n\nprint(f' \\n Training Accuracy {knngrid.score(X_train,y_train)}')\nf'Test Accuracy {knngrid.score(X_test,y_test)}'","e69bc850":"y_predicted = knngrid.predict(X_test)\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_predicted)\nimport seaborn as sn\nplt.figure(figsize = (10,7))\nsn.heatmap(cm, annot=True,fmt='g')\nplt.xlabel('Predicted')\nplt.ylabel('Truth')","b55c1009":"from sklearn.linear_model import LogisticRegression\nlog_model = LogisticRegression(solver='lbfgs', max_iter=1000)\n\nlog_model.fit(X_train, y_train)\nprint(f' Training Accuracy {log_model.score(X_train,y_train)}')\nf'Test Accuracy {log_model.score(X_test,y_test)}'","b2ad7a93":"folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=4)\nparam_grid = [\n        {\n            'activation' : ['identity', 'logistic', 'tanh', 'relu'],\n            'solver' : ['lbfgs', 'sgd', 'adam'],            \n        }\n       ]\nclf = GridSearchCV(MLPClassifier(), param_grid, cv=folds,\n                           scoring='accuracy',n_jobs=-1,verbose = 1,\n)\nclf.fit(X_train, y_train)\nprint(f' Training Accuracy {clf.score(X_train,y_train)}')\nf'Test Accuracy {clf.score(X_test,y_test)}'","707b7526":"cv_results = pd.DataFrame(clf.cv_results_)\ncv_results[cv_results.rank_test_score<5]\n#print the optimum value of hyperparameters\nprint('Best hyperparameters: ', clf.best_params_)","f4994158":"class myCallback(tf.keras.callbacks.Callback):\n  def on_epoch_end(self, epoch, logs={}):\n    if(logs.get('accuracy')>= 1.0):\n      print(\"\\nReached 99% accuracy so cancelling training!\")\n      self.model.stop_training = True\ncallbacks = myCallback()\n\nmodel = tf.keras.models.Sequential([\n  tf.keras.layers.Flatten(),\n  tf.keras.layers.Dense(800, activation=tf.nn.relu),\n  tf.keras.layers.Dense(256, activation=tf.nn.relu),\n  tf.keras.layers.Dense(256, activation=tf.nn.relu),\n  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n])\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy',metrics=['accuracy'])\nmodel.fit(X_train, y_train, epochs=50,batch_size=32,callbacks=[callbacks])\nprint(\"accuracy on test data is\", model.evaluate(X_test, y_test))","0c238180":"print(\"Accuracy on test data is \",model.evaluate(X_test, y_test))\nprint(\"Predicted value of ytest[4] is \",np.argmax(y_predicted[4]))","3eedba7d":"from sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier, Perceptron, RidgeClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB, BernoulliNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\nfrom sklearn.svm import SVC,NuSVC\nfrom sklearn.neighbors import KNeighborsClassifier, NearestCentroid\nfrom sklearn.linear_model import PassiveAggressiveClassifier\nfrom sklearn.metrics import precision_score\n\nmodels =[(\"LR\", LogisticRegression()),(\"SVC\", SVC()),\n         ('KNN',KNeighborsClassifier()),(\"DTC\", DecisionTreeClassifier()),\n         (\"GNB\", GaussianNB()),(\"SGDC\", SGDClassifier()),(\"Perc\", Perceptron()),\n         (\"NC\",NearestCentroid()),(\"Ridge\", RidgeClassifier()),\n         (\"BNB\", BernoulliNB()),('RF',RandomForestClassifier()),('ADA',AdaBoostClassifier()),\n         ('XGB',GradientBoostingClassifier()),('PAC',PassiveAggressiveClassifier())]\npred = []\nnames = []\nmodelsprecision = []\n\nfor name,model in models:\n    model.fit(X_train, y_train)\n    prediction = model.predict(X_test)\n    score = precision_score(y_test, prediction,average = 'macro')\n    pred.append(score)\n    names.append(name)\n    modelsprecision.append((name,score))\n    \nmodelsprecision.sort(key=lambda k:k[1],reverse=True)\n\nmodelsprecision\n","f3d4fd2a":"#### <b>Best params used for MLPClassifier<\/b>","690e1562":"## Section 03:- Dividing the data in to Train and Test","8f8bf04a":"### Checking the correlation","e3ca0c8d":"### <b>Model 02:- KNN<\/b>","d9d1d071":"#### <b>KNN With Hyper parameter Tuning<\/b>","8798779d":"### Verifing Hue on many columns and checking on the Target column","b3fec9f2":"## <b><u>Conclusion<\/u><\/b>\n* ### Neural network with Dense layers is giving the best result with 96.8% accuracy on test model.","19bb4297":"### <b>Modal 03:- Logistic Regression<\/b>","38fbe501":"#### Neural network with Dense layer and defining the Stop criteria on reachign the Accuracy threshold on Train data ","de41b953":"### Model 05-- Spot check without Tuning Hyper parameters\n","22823487":"### <b>Model 01:- Support Vector Machine (SVM)<\/b>","128a1889":"## Section02- EDA","de19256f":"### From the Above Target classification looks bit imablanced Target column B--> 0 (encoded) is having less data.","e9b437d1":"### From the above we see very less correlation of each features on the Target column.","6feb86e3":"## Section 01:- Reading Data and Checking the Meta information","0b02f72f":"## Section 04:- Model Building","af7748ab":"### Checking the Correlation on the Target variable using barchart","3bf04ab7":"### <b>Model 04:- Neural Network <\/b>"}}