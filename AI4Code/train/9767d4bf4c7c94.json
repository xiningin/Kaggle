{"cell_type":{"ec0101fb":"code","8afa54d9":"code","d6c6e1ef":"code","cfea1207":"code","6d23d2dd":"code","4cc493c7":"code","de68d9cf":"code","0641ff0a":"code","11b47a32":"code","656b1ada":"code","5af5c0bd":"code","6f50bed5":"code","ee91f656":"code","85cc0d6f":"code","fb8de538":"code","8fbd2f3a":"code","7b05bc55":"markdown","4e73a229":"markdown","0248a9d0":"markdown"},"source":{"ec0101fb":"import pandas as pd \nimport numpy as np\nimport re\nimport nltk\nimport spacy\nimport string\n!pip install pyspellchecker\nTWEET_DATA = pd.read_csv(\"..\/input\/covid19-tweet-indonesia-positif-dan-negatif\/dataset_tweet_covid-19.csv\")\n\n","8afa54d9":"print(TWEET_DATA.head(6))","d6c6e1ef":"def hapus_hashtag_dan_mentions(text):\n    mentions = re.findall(\"@([a-zA-Z0-9_]{1,50})\", text)\n    hashtags = re.findall(\"#([a-zA-Z0-9_]{1,50})\", text)\n    clean_tweet = re.sub(\"@[A-Za-z0-9_]+\",\"\", text)\n    clean_tweet = re.sub(\"#[A-Za-z0-9_]+\",\"\", clean_tweet)\n    return clean_tweet\n\nTWEET_DATA[\"text_remove_hashtag_and_mentions\"] = TWEET_DATA[\"Tweet\"].apply(lambda text: hapus_hashtag_dan_mentions(text))\nTWEET_DATA.head(6)","cfea1207":"def remove_urls(text):\n    url_pattern = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url_pattern.sub(r'', text)\n\nTWEET_DATA[\"text_remove_url\"] = TWEET_DATA[\"text_remove_hashtag_and_mentions\"].apply(lambda text: remove_urls(text))\nTWEET_DATA.head(6)","6d23d2dd":"PUNCT_TO_REMOVE = string.punctuation\ndef remove_punctuation(text):\n    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n\nTWEET_DATA[\"text_remove_punc\"] = TWEET_DATA[\"text_remove_url\"].apply(lambda text: remove_punctuation(text))\nTWEET_DATA.head(6)","4cc493c7":"def remove_emoji(string):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', string)\n\nTWEET_DATA[\"text_remove_emojis\"] = TWEET_DATA[\"text_remove_punc\"].apply(lambda text: remove_emoji(text))\nTWEET_DATA.head(6)","de68d9cf":"EMOTICONS = {\n    u\":\u2011\\)\":\"Happy face or smiley\",\n    u\":\\)\":\"Happy face or smiley\",\n    u\":-\\]\":\"Happy face or smiley\",\n    u\":\\]\":\"Happy face or smiley\",\n    u\":-3\":\"Happy face smiley\",\n    u\":3\":\"Happy face smiley\",\n    u\":->\":\"Happy face smiley\",\n    u\":>\":\"Happy face smiley\",\n    u\"8-\\)\":\"Happy face smiley\",\n    u\":o\\)\":\"Happy face smiley\",\n    u\":-\\}\":\"Happy face smiley\",\n    u\":\\}\":\"Happy face smiley\",\n    u\":-\\)\":\"Happy face smiley\",\n    u\":c\\)\":\"Happy face smiley\",\n    u\":\\^\\)\":\"Happy face smiley\",\n    u\"=\\]\":\"Happy face smiley\",\n    u\"=\\)\":\"Happy face smiley\",\n    u\":\u2011D\":\"Laughing, big grin or laugh with glasses\",\n    u\":D\":\"Laughing, big grin or laugh with glasses\",\n    u\"8\u2011D\":\"Laughing, big grin or laugh with glasses\",\n    u\"8D\":\"Laughing, big grin or laugh with glasses\",\n    u\"X\u2011D\":\"Laughing, big grin or laugh with glasses\",\n    u\"XD\":\"Laughing, big grin or laugh with glasses\",\n    u\"=D\":\"Laughing, big grin or laugh with glasses\",\n    u\"=3\":\"Laughing, big grin or laugh with glasses\",\n    u\"B\\^D\":\"Laughing, big grin or laugh with glasses\",\n    u\":-\\)\\)\":\"Very happy\",\n    u\":\u2011\\(\":\"Frown, sad, andry or pouting\",\n    u\":-\\(\":\"Frown, sad, andry or pouting\",\n    u\":\\(\":\"Frown, sad, andry or pouting\",\n    u\":\u2011c\":\"Frown, sad, andry or pouting\",\n    u\":c\":\"Frown, sad, andry or pouting\",\n    u\":\u2011<\":\"Frown, sad, andry or pouting\",\n    u\":<\":\"Frown, sad, andry or pouting\",\n    u\":\u2011\\[\":\"Frown, sad, andry or pouting\",\n    u\":\\[\":\"Frown, sad, andry or pouting\",\n    u\":-\\|\\|\":\"Frown, sad, andry or pouting\",\n    u\">:\\[\":\"Frown, sad, andry or pouting\",\n    u\":\\{\":\"Frown, sad, andry or pouting\",\n    u\":@\":\"Frown, sad, andry or pouting\",\n    u\">:\\(\":\"Frown, sad, andry or pouting\",\n    u\":'\u2011\\(\":\"Crying\",\n    u\":'\\(\":\"Crying\",\n    u\":'\u2011\\)\":\"Tears of happiness\",\n    u\":'\\)\":\"Tears of happiness\",\n    u\"D\u2011':\":\"Horror\",\n    u\"D:<\":\"Disgust\",\n    u\"D:\":\"Sadness\",\n    u\"D8\":\"Great dismay\",\n    u\"D;\":\"Great dismay\",\n    u\"D=\":\"Great dismay\",\n    u\"DX\":\"Great dismay\",\n    u\":\u2011O\":\"Surprise\",\n    u\":O\":\"Surprise\",\n    u\":\u2011o\":\"Surprise\",\n    u\":o\":\"Surprise\",\n    u\":-0\":\"Shock\",\n    u\"8\u20110\":\"Yawn\",\n    u\">:O\":\"Yawn\",\n    u\":-\\*\":\"Kiss\",\n    u\":\\*\":\"Kiss\",\n    u\":X\":\"Kiss\",\n    u\";\u2011\\)\":\"Wink or smirk\",\n    u\";\\)\":\"Wink or smirk\",\n    u\"\\*-\\)\":\"Wink or smirk\",\n    u\"\\*\\)\":\"Wink or smirk\",\n    u\";\u2011\\]\":\"Wink or smirk\",\n    u\";\\]\":\"Wink or smirk\",\n    u\";\\^\\)\":\"Wink or smirk\",\n    u\":\u2011,\":\"Wink or smirk\",\n    u\";D\":\"Wink or smirk\",\n    u\":\u2011P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\":P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\"X\u2011P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\"XP\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\":\u2011\u00de\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\":\u00de\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\":b\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\"d:\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\"=p\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\">:P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\":\u2011\/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\":\/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\":-[.]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\">:[(\\\\\\)]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\">:\/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\":[(\\\\\\)]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\"=\/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\"=[(\\\\\\)]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\":L\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\"=L\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\":S\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\":\u2011\\|\":\"Straight face\",\n    u\":\\|\":\"Straight face\",\n    u\":$\":\"Embarrassed or blushing\",\n    u\":\u2011x\":\"Sealed lips or wearing braces or tongue-tied\",\n    u\":x\":\"Sealed lips or wearing braces or tongue-tied\",\n    u\":\u2011#\":\"Sealed lips or wearing braces or tongue-tied\",\n    u\":#\":\"Sealed lips or wearing braces or tongue-tied\",\n    u\":\u2011&\":\"Sealed lips or wearing braces or tongue-tied\",\n    u\":&\":\"Sealed lips or wearing braces or tongue-tied\",\n    u\"O:\u2011\\)\":\"Angel, saint or innocent\",\n    u\"O:\\)\":\"Angel, saint or innocent\",\n    u\"0:\u20113\":\"Angel, saint or innocent\",\n    u\"0:3\":\"Angel, saint or innocent\",\n    u\"0:\u2011\\)\":\"Angel, saint or innocent\",\n    u\"0:\\)\":\"Angel, saint or innocent\",\n    u\":\u2011b\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\"0;\\^\\)\":\"Angel, saint or innocent\",\n    u\">:\u2011\\)\":\"Evil or devilish\",\n    u\">:\\)\":\"Evil or devilish\",\n    u\"\\}:\u2011\\)\":\"Evil or devilish\",\n    u\"\\}:\\)\":\"Evil or devilish\",\n    u\"3:\u2011\\)\":\"Evil or devilish\",\n    u\"3:\\)\":\"Evil or devilish\",\n    u\">;\\)\":\"Evil or devilish\",\n    u\"\\|;\u2011\\)\":\"Cool\",\n    u\"\\|\u2011O\":\"Bored\",\n    u\":\u2011J\":\"Tongue-in-cheek\",\n    u\"#\u2011\\)\":\"Party all night\",\n    u\"%\u2011\\)\":\"Drunk or confused\",\n    u\"%\\)\":\"Drunk or confused\",\n    u\":-###..\":\"Being sick\",\n    u\":###..\":\"Being sick\",\n    u\"<:\u2011\\|\":\"Dump\",\n    u\"\\(>_<\\)\":\"Troubled\",\n    u\"\\(>_<\\)>\":\"Troubled\",\n    u\"\\(';'\\)\":\"Baby\",\n    u\"\\(\\^\\^>``\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n    u\"\\(\\^_\\^;\\)\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n    u\"\\(-_-;\\)\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n    u\"\\(~_~;\\) \\(\u30fb\\.\u30fb;\\)\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n    u\"\\(-_-\\)zzz\":\"Sleeping\",\n    u\"\\(\\^_-\\)\":\"Wink\",\n    u\"\\(\\(\\+_\\+\\)\\)\":\"Confused\",\n    u\"\\(\\+o\\+\\)\":\"Confused\",\n    u\"\\(o\\|o\\)\":\"Ultraman\",\n    u\"\\^_\\^\":\"Joyful\",\n    u\"\\(\\^_\\^\\)\/\":\"Joyful\",\n    u\"\\(\\^O\\^\\)\uff0f\":\"Joyful\",\n    u\"\\(\\^o\\^\\)\uff0f\":\"Joyful\",\n    u\"\\(__\\)\":\"Kowtow as a sign of respect, or dogeza for apology\",\n    u\"_\\(\\._\\.\\)_\":\"Kowtow as a sign of respect, or dogeza for apology\",\n    u\"<\\(_ _\\)>\":\"Kowtow as a sign of respect, or dogeza for apology\",\n    u\"<m\\(__\\)m>\":\"Kowtow as a sign of respect, or dogeza for apology\",\n    u\"m\\(__\\)m\":\"Kowtow as a sign of respect, or dogeza for apology\",\n    u\"m\\(_ _\\)m\":\"Kowtow as a sign of respect, or dogeza for apology\",\n    u\"\\('_'\\)\":\"Sad or Crying\",\n    u\"\\(\/_;\\)\":\"Sad or Crying\",\n    u\"\\(T_T\\) \\(;_;\\)\":\"Sad or Crying\",\n    u\"\\(;_;\":\"Sad of Crying\",\n    u\"\\(;_:\\)\":\"Sad or Crying\",\n    u\"\\(;O;\\)\":\"Sad or Crying\",\n    u\"\\(:_;\\)\":\"Sad or Crying\",\n    u\"\\(ToT\\)\":\"Sad or Crying\",\n    u\";_;\":\"Sad or Crying\",\n    u\";-;\":\"Sad or Crying\",\n    u\";n;\":\"Sad or Crying\",\n    u\";;\":\"Sad or Crying\",\n    u\"Q\\.Q\":\"Sad or Crying\",\n    u\"T\\.T\":\"Sad or Crying\",\n    u\"QQ\":\"Sad or Crying\",\n    u\"Q_Q\":\"Sad or Crying\",\n    u\"\\(-\\.-\\)\":\"Shame\",\n    u\"\\(-_-\\)\":\"Shame\",\n    u\"\\(\u4e00\u4e00\\)\":\"Shame\",\n    u\"\\(\uff1b\u4e00_\u4e00\\)\":\"Shame\",\n    u\"\\(=_=\\)\":\"Tired\",\n    u\"\\(=\\^\\\u00b7\\^=\\)\":\"cat\",\n    u\"\\(=\\^\\\u00b7\\\u00b7\\^=\\)\":\"cat\",\n    u\"=_\\^=\t\":\"cat\",\n    u\"\\(\\.\\.\\)\":\"Looking down\",\n    u\"\\(\\._\\.\\)\":\"Looking down\",\n    u\"\\^m\\^\":\"Giggling with hand covering mouth\",\n    u\"\\(\\\u30fb\\\u30fb?\":\"Confusion\",\n    u\"\\(?_?\\)\":\"Confusion\",\n    u\">\\^_\\^<\":\"Normal Laugh\",\n    u\"<\\^!\\^>\":\"Normal Laugh\",\n    u\"\\^\/\\^\":\"Normal Laugh\",\n    u\"\\\uff08\\*\\^_\\^\\*\uff09\" :\"Normal Laugh\",\n    u\"\\(\\^<\\^\\) \\(\\^\\.\\^\\)\":\"Normal Laugh\",\n    u\"\\(^\\^\\)\":\"Normal Laugh\",\n    u\"\\(\\^\\.\\^\\)\":\"Normal Laugh\",\n    u\"\\(\\^_\\^\\.\\)\":\"Normal Laugh\",\n    u\"\\(\\^_\\^\\)\":\"Normal Laugh\",\n    u\"\\(\\^\\^\\)\":\"Normal Laugh\",\n    u\"\\(\\^J\\^\\)\":\"Normal Laugh\",\n    u\"\\(\\*\\^\\.\\^\\*\\)\":\"Normal Laugh\",\n    u\"\\(\\^\u2014\\^\\\uff09\":\"Normal Laugh\",\n    u\"\\(#\\^\\.\\^#\\)\":\"Normal Laugh\",\n    u\"\\\uff08\\^\u2014\\^\\\uff09\":\"Waving\",\n    u\"\\(;_;\\)\/~~~\":\"Waving\",\n    u\"\\(\\^\\.\\^\\)\/~~~\":\"Waving\",\n    u\"\\(-_-\\)\/~~~ \\($\\\u00b7\\\u00b7\\)\/~~~\":\"Waving\",\n    u\"\\(T_T\\)\/~~~\":\"Waving\",\n    u\"\\(ToT\\)\/~~~\":\"Waving\",\n    u\"\\(\\*\\^0\\^\\*\\)\":\"Excited\",\n    u\"\\(\\*_\\*\\)\":\"Amazed\",\n    u\"\\(\\*_\\*;\":\"Amazed\",\n    u\"\\(\\+_\\+\\) \\(@_@\\)\":\"Amazed\",\n    u\"\\(\\*\\^\\^\\)v\":\"Laughing,Cheerful\",\n    u\"\\(\\^_\\^\\)v\":\"Laughing,Cheerful\",\n    u\"\\(\\(d[-_-]b\\)\\)\":\"Headphones,Listening to music\",\n    u'\\(-\"-\\)':\"Worried\",\n    u\"\\(\u30fc\u30fc;\\)\":\"Worried\",\n    u\"\\(\\^0_0\\^\\)\":\"Eyeglasses\",\n    u\"\\(\\\uff3e\uff56\\\uff3e\\)\":\"Happy\",\n    u\"\\(\\\uff3e\uff55\\\uff3e\\)\":\"Happy\",\n    u\"\\(\\^\\)o\\(\\^\\)\":\"Happy\",\n    u\"\\(\\^O\\^\\)\":\"Happy\",\n    u\"\\(\\^o\\^\\)\":\"Happy\",\n    u\"\\)\\^o\\^\\(\":\"Happy\",\n    u\":O o_O\":\"Surprised\",\n    u\"o_0\":\"Surprised\",\n    u\"o\\.O\":\"Surpised\",\n    u\"\\(o\\.o\\)\":\"Surprised\",\n    u\"oO\":\"Surprised\",\n    u\"\\(\\*\uffe3m\uffe3\\)\":\"Dissatisfied\",\n    u\"\\(\u2018A`\\)\":\"Snubbed or Deflated\"\n}\n\ndef remove_emoticons(text):\n    emoticon_pattern = re.compile(u'(' + u'|'.join(k for k in EMOTICONS) + u')')\n    return emoticon_pattern.sub(r'', text)\n\nTWEET_DATA[\"text_remove_emoticons\"] = TWEET_DATA[\"text_remove_emojis\"].apply(lambda text: remove_emoticons(text))\nTWEET_DATA.head(6)","0641ff0a":"def remove_html(text):\n    html_pattern = re.compile('<.*?>')\n    return html_pattern.sub(r'', text)\n\nTWEET_DATA[\"cleansing_tweets\"] = TWEET_DATA[\"text_remove_emoticons\"].apply(lambda text: remove_html(text))\nTWEET_DATA.head(6)","11b47a32":"#Case Foldiig\n\n# ------ Case Folding --------\n# gunakan fungsi Series.str.lower() pada Pandas\nTWEET_DATA['case_folding_tweets'] = TWEET_DATA['cleansing_tweets'].str.lower()\n\n\nprint('Hasil Case Folding : \\n')\nprint(TWEET_DATA['case_folding_tweets'].head(6))\nprint('\\n\\n\\n')","656b1ada":"import string \nimport re #regex library\n\n# import word_tokenize & FreqDist from NLTK\nfrom nltk.tokenize import word_tokenize \nfrom nltk.probability import FreqDist\n\n# ------ Tokenizing ---------\n\ndef remove_tweet_special(text):\n    # remove tab, new line, ans back slice\n    text = text.replace('\\\\t',\" \").replace('\\\\n',\" \").replace('\\\\u',\" \").replace('\\\\',\"\")\n    # remove non ASCII (emoticon, chinese word, .etc)\n    text = text.encode('ascii', 'replace').decode('ascii')\n    # remove mention, link, hashtag\n    text = ' '.join(re.sub(\"([@#][A-Za-z0-9]+)|(\\w+:\\\/\\\/\\S+)\",\" \", text).split())\n    # remove incomplete URL\n    return text.replace(\"http:\/\/\", \" \").replace(\"https:\/\/\", \" \")\n                \nTWEET_DATA['case_folding_tweets'] = TWEET_DATA['case_folding_tweets'].apply(remove_tweet_special)\n\n#remove number\ndef remove_number(text):\n    return  re.sub(r\"\\d+\", \"\", text)\n\nTWEET_DATA['case_folding_tweets'] = TWEET_DATA['case_folding_tweets'].apply(remove_number)\n\n#remove punctuation\ndef remove_punctuation(text):\n    return text.translate(str.maketrans(\"\",\"\",string.punctuation))\n\nTWEET_DATA['case_folding_tweets'] = TWEET_DATA['case_folding_tweets'].apply(remove_punctuation)\n\n#remove whitespace leading & trailing\ndef remove_whitespace_LT(text):\n    return text.strip()\n\nTWEET_DATA['case_folding_tweets'] = TWEET_DATA['case_folding_tweets'].apply(remove_whitespace_LT)\n\n#remove multiple whitespace into single whitespace\ndef remove_whitespace_multiple(text):\n    return re.sub('\\s+',' ',text)\n\nTWEET_DATA['case_folding_tweets'] = TWEET_DATA['case_folding_tweets'].apply(remove_whitespace_multiple)\n\n# remove single char\ndef remove_singl_char(text):\n    return re.sub(r\"\\b[a-zA-Z]\\b\", \"\", text)\n\nTWEET_DATA['case_folding_tweets'] = TWEET_DATA['case_folding_tweets'].apply(remove_singl_char)\n\n# NLTK word rokenize \ndef word_tokenize_wrapper(text):\n    return word_tokenize(text)\n\nTWEET_DATA['tweet_tokens'] = TWEET_DATA['case_folding_tweets'].apply(word_tokenize_wrapper)\n\n\nprint('Tokenizing Result : \\n') \nprint(TWEET_DATA['tweet_tokens'].head(6))\nprint('\\n\\n\\n')\nTWEET_DATA.head(6)\n\n","5af5c0bd":"#Stopword\n\nfrom nltk.corpus import stopwords\n\n# ----------------------- get stopword from NLTK stopword -------------------------------\n# get stopword indonesia\nlist_stopwords = stopwords.words('indonesian')\n\n\n# ---------------------------- manualy add stopword  ------------------------------------\n# append additional stopword\nlist_stopwords.extend([\"yg\", \"dg\", \"rt\", \"dgn\", \"ny\", \"d\", 'klo', \n                       'kalo', 'amp', 'biar', 'bikin', 'bilang', \n                       'gak', 'ga', 'krn', 'nya', 'nih', 'sih', \n                       'si', 'tau', 'tdk', 'tuh', 'utk', 'ya', \n                       'jd', 'jgn', 'sdh', 'aja', 'n', 't', \n                       'nyg', 'hehe', 'pen', 'u', 'nan', 'loh', 'rt',\n                       '&amp', 'yah'])\n\n# ----------------------- add stopword from txt file ------------------------------------\n# read txt stopword using pandas\ntxt_stopword = pd.read_csv(\"..\/input\/covid19-tweet-indonesia-positif-dan-negatif\/stopword.txt\", names= [\"stopwords\"], header = None)\n\n# convert stopword string to list & append additional stopword\nlist_stopwords.extend(txt_stopword[\"stopwords\"][0].split(' '))\n\n# ---------------------------------------------------------------------------------------\n\n# convert list to dictionary\nlist_stopwords = set(list_stopwords)\n\n\n#remove stopword pada list token\ndef stopwords_removal(words):\n    return [word for word in words if word not in list_stopwords]\n\nTWEET_DATA['tweet_tokens_WSW'] = TWEET_DATA['tweet_tokens'].apply(stopwords_removal) \n\n\nprint(TWEET_DATA['tweet_tokens_WSW'].head())","6f50bed5":"!pip install xlrd\n!pip install openpyxl\nnormalizad_word = pd.read_excel(\"..\/input\/covid19-tweet-indonesia-positif-dan-negatif\/normalisasi.xlsx\", engine='openpyxl')\n\nnormalizad_word_dict = {}\n\nfor index, row in normalizad_word.iterrows():\n    if row[0] not in normalizad_word_dict:\n        normalizad_word_dict[row[0]] = row[1] \n\ndef normalized_term(document):\n    return [normalizad_word_dict[term] if term in normalizad_word_dict else term for term in document]\n\nTWEET_DATA['tweet_normalized'] = TWEET_DATA['tweet_tokens_WSW'].apply(normalized_term)\n\nTWEET_DATA.head(1000)","ee91f656":"!mkdir data\n!wget -P data\/ https:\/\/raw.githubusercontent.com\/taudata-indonesia\/eLearning\/master\/data\/slang.txt\n!wget -P data\/ https:\/\/raw.githubusercontent.com\/taudata-indonesia\/eLearning\/master\/data\/stopwords_id.txt\n!wget -P data\/ https:\/\/raw.githubusercontent.com\/taudata-indonesia\/eLearning\/master\/data\/stopwords_en.txt\n!wget -P data\/ https:\/\/raw.githubusercontent.com\/taudata-indonesia\/eLearning\/master\/data\/corpus_sederhana.txt\n!wget -P \/ https:\/\/raw.githubusercontent.com\/taudata-indonesia\/eLearning\/master\/lib\/taudataNlpTm.py\n!pip install unidecode textblob sastrawi\nnltk.download('popular')","85cc0d6f":"# import Sastrawi package\n!pip install swifter\nfrom Sastrawi.Stemmer.StemmerFactory import StemmerFactory\nimport swifter\n\n\n# create stemmer\nfactory = StemmerFactory()\nstemmer = factory.create_stemmer()\n\n# stemmed\ndef stemmed_wrapper(term):\n    return stemmer.stem(term)\n\nterm_dict = {}\n\nfor document in TWEET_DATA['tweet_normalized']:\n    for term in document:\n        if term not in term_dict:\n            term_dict[term] = ' '\n            \nprint(len(term_dict))\nprint(\"------------------------\")\n\nfor term in term_dict:\n    term_dict[term] = stemmed_wrapper(term)\n#     print(term,\":\" ,term_dict[term])\n    \n# print(term_dict)\nprint(\"------------------------\")\n\n\n# apply stemmed term to dataframe\ndef get_stemmed_term(document):\n    return [term_dict[term] for term in document]\n\nTWEET_DATA['tweet_tokens_stemmed'] = TWEET_DATA['tweet_normalized'].swifter.apply(get_stemmed_term)\nprint(TWEET_DATA.head())","fb8de538":"TWEET_DATA.head(1000)","8fbd2f3a":"TWEET_DATA.to_csv(\"text_preprocessing_new.csv\")","7b05bc55":"# MENGHAPUS TANDA BACA ?!&^ - STEP I CLEANSING","4e73a229":"# MENGHAPUS EMOTICON :) - STEP III CLEANSING","0248a9d0":"# MENGAPUS EMOTICON \ud83d\ude00 - STEP II CLEANSING"}}