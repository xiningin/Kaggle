{"cell_type":{"4311512e":"code","1ef12d4e":"code","572285fb":"code","22930163":"code","6d43b8e9":"code","993eff1f":"code","2b640448":"code","cd2a40f6":"code","035cda6f":"code","a9af944d":"code","e39050b7":"markdown","3a2b9f81":"markdown","0ce6bf72":"markdown","b779257e":"markdown","90258bd4":"markdown","18e4d13e":"markdown","95232fb6":"markdown","cc5a53e5":"markdown","96b1227f":"markdown","9feb2645":"markdown","b55693ae":"markdown","17d6cded":"markdown","5442c041":"markdown","8190b4b6":"markdown","65022e37":"markdown","f1f5ade9":"markdown","bc218563":"markdown","98691f99":"markdown","b30e0872":"markdown","1389ff2a":"markdown","d690c751":"markdown"},"source":{"4311512e":"import tensorflow as tf\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\niris = load_iris()\n\n","1ef12d4e":"# Load data into a DataFrame\ndf = pd.DataFrame(iris.data, columns=iris.feature_names)\nprint(f\"Number of samples --> {df.shape}\")\n# Convert datatype to float\ndf = df.astype(float)\n# append \"target\" and name it \"label\"\ndf['label'] = iris.target\n# Use string label instead\ndf['label'] = df.label.replace(dict(enumerate(iris.target_names)))\nprint(f\"\\n## Target class distribution --> \\n{df['label'].value_counts()}\")\ndf.head()\n","572285fb":"# label -> one-hot encoding\nlabel = pd.get_dummies(df['label'], prefix='label')\ndf = pd.concat([df, label], axis=1)\n# drop old label\ndf.drop(['label'], axis=1, inplace=True)\n\ndf.head()","22930163":"# Creating X and y\nX = df[['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']]\n# Convert DataFrame into np array\nX = np.asarray(X)\ny = df[['label_setosa', 'label_versicolor', 'label_virginica']]\n# Convert DataFrame into np array\ny = np.asarray(y)\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.20)\n","6d43b8e9":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\n\ndef create_model(): \n    model = Sequential([\n        Dense(64, activation='relu', input_shape=(4,)),\n        Dense(128, activation='relu'),\n        Dense(128, activation='relu'),\n        Dense(128, activation='relu'),\n        Dense(64, activation='relu'),\n        Dense(64, activation='relu'),\n        Dense(64, activation='relu'),\n        Dense(3, activation='softmax')\n    ])\n    return model\n\nmodel = create_model()\nmodel.summary()","993eff1f":"# In order to train a model, we have ti configure our model using compile()\n# categorical_crossentropy) for our multiple-class classification problem\n\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# call model.fit() to fit our model to the training data.\nhistory = model.fit(X_train, y_train, epochs=200, validation_split=0.25, batch_size=40, verbose=2)\n\n","2b640448":"%matplotlib inline\n%config InlineBackend.figure_format = 'svg'\n\n# helper function to plot metrics\ndef plot_metric(history, metric):\n    train_metrics = history.history[metric]\n    val_metrics = history.history['val_'+metric]\n    epochs = range(1, len(train_metrics) + 1)\n    plt.plot(epochs, train_metrics)\n    plt.plot(epochs, val_metrics)\n    plt.title('Training and validation '+ metric)\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(metric)\n    plt.legend([\"train_\"+metric, 'val_'+metric])\n    plt.show()\n\nplot_metric(history, 'accuracy') # plots accuracy for train and validation\nplot_metric(history, 'loss') # plots loss for train and validation","cd2a40f6":"from tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.regularizers import l2\n\ndef create_regularized_model(factor, rate):\n    model = Sequential([\n        Dense(64, kernel_regularizer=l2(factor), activation=\"relu\", input_shape=(4,)),\n        Dropout(rate),\n        Dense(128, kernel_regularizer=l2(factor), activation=\"relu\"),\n        Dropout(rate),\n        Dense(128, kernel_regularizer=l2(factor), activation=\"relu\"),\n        Dropout(rate),\n        Dense(128, kernel_regularizer=l2(factor), activation=\"relu\"),\n        Dropout(rate),\n        Dense(64, kernel_regularizer=l2(factor), activation=\"relu\"),\n        Dropout(rate),\n        Dense(64, kernel_regularizer=l2(factor), activation=\"relu\"),\n        Dropout(rate),\n        Dense(64, kernel_regularizer=l2(factor), activation=\"relu\"),\n        Dropout(rate),\n        Dense(3, activation='softmax')\n    ])\n    return model\n\nmodel = create_regularized_model(1e-5, 0.3)\nmodel.summary()\n","035cda6f":"# First configure model using model.compile()\nmodel.compile(\n    optimizer='adam', \n    loss='categorical_crossentropy', \n    metrics=['accuracy']\n)\n# Then, train the model with fit()\nhistory = model.fit(\n    X_train, \n    y_train, \n    epochs=200, \n    validation_split=0.25, \n    batch_size=40, \n    verbose=2\n)","a9af944d":"plot_metric(history, 'accuracy') # plots accuracy for train and validation\nplot_metric(history, 'loss') # plots loss for train and validation","e39050b7":"# Build a regularized NN (L2 regularization and Dropout)","3a2b9f81":"# Regularization techniques","0ce6bf72":"**In this notebook, we are going to incorporate L2 regularization and Dropout to reduce overfitting of a neural network model.**","b779257e":"## Training - Unregularized NN","90258bd4":"# Model Evaluation - Regularized NN","18e4d13e":"## **From the above graphs, we can see that the model has overfitted the training data, so it outperforms the validation set.**","95232fb6":"# Load packages","cc5a53e5":"## **In this notebook, we will focus on incorporating Regularization into our ANN model and look at an example of how we do this in practice with Keras and TensorFlow 2.0.**","96b1227f":"# Model Evaluation - Unregularized NN","9feb2645":"# Load data","b55693ae":"# Introduction to Regularization","17d6cded":"- Before applying regularization, let\u2019s build a neural network without regularization and take a look at the overfitting issue.","5442c041":"# Do upvote if the notebook was helpful. Thanks!","8190b4b6":"# Build an Unregularized Neural Network","65022e37":"# Train Test Split","f1f5ade9":"- Dataset: (iris)\n- Dataset Description: The dataset contains a set of 150 records under five attributes: sepal length, sepal width, petal length, petal width, and class (known as target from sklearn datasets).","bc218563":"- Regularization is a technique to combat the overfitting issue in machine learning. \n- Overfitting, also known as High Variance, refers to a model that learns the training data too well but fail to generalize to new data\n- Overfitting can be diagnosed by plotting training and validation loss.","98691f99":"## Training - Regularized NN","b30e0872":"The most widely used regularization techniques are:\n\n- **L1 regularization** adds \u201cabsolute value of magnitude\u201d as penalty term to the loss function. Some people say L1 can help with compressing the model. But in practice, L1 regularization makes your model sparse, helps only a little bit. L2 regularization is just used much more often.\n- **L2 regularization** (also known as weight decay) adds \u201csquared magnitude\u201d as penalty term to the loss function and it is used much more often than L1.\n- **Dropout** is widely used regularization technique in deep learning. It randomly shuts down some neurons in each iteration.\n- **Data Augmentation** adds extra fake training data. More training also helps with reducing overfitting.\n- **Early Stopping** is another widely used regularization technique in deep learning. It stops training when generalization error increases.","1389ff2a":"Link to other notebooks I have recently published:\n- https:\/\/www.kaggle.com\/snooptosh\/pycaret-topic-modeling\n- https:\/\/www.kaggle.com\/snooptosh\/intro-oop-in-python","d690c751":"From the above graph, it can be seen that the overfitting is not completely fixed, but there is a significant improvement when we compare it to the unregularized model.\n"}}