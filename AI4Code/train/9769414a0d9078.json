{"cell_type":{"83736c97":"code","15325c12":"code","89e4774e":"code","4aa670ec":"code","2897eb21":"code","a49fbabe":"code","1c2c1b4b":"code","65e8efb1":"code","95c74109":"code","b08d0b3b":"code","a3fba2a8":"code","eab3012c":"code","d380b20e":"code","9de3e95d":"code","7ff71360":"code","2e99704b":"code","e853bd04":"code","4184ff95":"code","ee53c83f":"code","d177632b":"code","48070f22":"code","9afa688a":"code","336cdebb":"code","c5651e21":"code","b3660af1":"code","d3dd17fc":"code","96c5d621":"code","153f569c":"code","50d361a5":"markdown","3f7fdd37":"markdown","b5542dec":"markdown","bb4f545b":"markdown","04b1173f":"markdown","55df5afe":"markdown"},"source":{"83736c97":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n# add glove6b300dtxt dataset \n\n# Any results you write to the current directory are saved as output.","15325c12":"train = pd.read_csv('..\/input\/movie-review-sentiment-analysis-kernels-only\/train.tsv',delimiter='\\t')\ntest =pd.read_csv('..\/input\/movie-review-sentiment-analysis-kernels-only\/test.tsv',delimiter='\\t')","89e4774e":"train.head(10)","4aa670ec":"from sklearn.preprocessing import OneHotEncoder","2897eb21":"# -1 for len(train)  One shape dimension can be -1. In this case, the value is inferred from the length of the array and remaining dimensions.\ntrain.Sentiment.values.reshape(-1,1)","a49fbabe":"train.Sentiment.values.shape","1c2c1b4b":"train.iloc[192]","65e8efb1":"ohe=OneHotEncoder(sparse=False)\n# -1 for len(train)  One shape dimension can be -1. In this case, the value is inferred from the length of the array and remaining dimensions.\n# OneHotEncoder Expected 2D array\nohe=ohe.fit(train.Sentiment.values.reshape(-1,1))\nprint(train.Sentiment.values[192])\n","95c74109":"print('sentimental distribution\\n{}'.format(train.Sentiment.value_counts()\/len(train)*100))","b08d0b3b":"len(set(train['Phrase']).intersection(set(test['Phrase'])))\/len(test)*100","a3fba2a8":"from sklearn.feature_extraction.text import  CountVectorizer\n\ncv1= CountVectorizer()\ncv2= CountVectorizer()\n\ncv1.fit(train.Phrase)\ncv2.fit(test.Phrase)\nprint('train total vocabulary size = {}'.format(len(cv1.vocabulary_)))\nprint('test total vocabulary size = {}'.format(len(cv2.vocabulary_)))\n\nprint('comman word in both ={}'.format(len(set(cv1.vocabulary_.keys()).intersection(set(cv2.vocabulary_.keys())))))\n","eab3012c":"groupby=train.groupby(\"SentenceId\")\ngroupby.count()[:3]\n","d380b20e":"def tranfer(data):\n    data['Phrase_count']=data.groupby(\"SentenceId\")['Phrase'].transform('count')\n    data['word_count']=data['Phrase'].apply(lambda x :len(x.split(' ')) )\n    data['upper_char']=data['Phrase'].apply(lambda x : x.lower()!=x)\n    data['start_comma']=data['Phrase'].apply(lambda x : x.startswith(','))\n    data['sentence_end']=data['Phrase'].apply(lambda x :x.endswith('.') )\n    data['sentence_start']=data['Phrase'].apply(lambda x :x[0].upper()==x[0] )\n    data[\"Phrase\"] = data[\"Phrase\"].apply(lambda x: x.lower())\n    return data\ntrain = tranfer(train)\ntest = tranfer(test)\n        ","9de3e95d":"NUM_FOLDS = 5\n\ntrain[\"fold_id\"] = train[\"SentenceId\"].apply(lambda x: x%NUM_FOLDS)","7ff71360":"train.groupby(\"Sentiment\")[train.columns[4:]].mean()","2e99704b":"NUM_FOLDS = 5\n\ntrain[\"fold_id\"] = train[\"SentenceId\"].apply(lambda x: x%NUM_FOLDS)\n","e853bd04":"# tranfer knowlege from pre train databset with 300 dimention , with diff features like gender , royal etc \n# glove Embeddig\nEMBEDDING_FILE =  '..\/input\/glove6b300dtxt\/glove.6B.300d.txt'\n","4184ff95":"f=open(EMBEDDING_FILE)\nfor line in f:\n    # split each iteam in first line \n    value=line.split(' ')\n    # first time is wrord \n    word = value[0]\n    print('1st word=',word)\n    print(value[:20])\n    print(line[:20])\n    break;\n    \nEMBEDDING_DIM=300","ee53c83f":"f=open(EMBEDDING_FILE)\n# unin test and train unique\nall_word = set(cv1.vocabulary_.keys()).union(set(cv2.vocabulary_.keys()))\n# store \nembedding_index={}\nfor line in f:\n    value=line.split(' ')\n    word = value[0]\n    if word in all_word:\n        coef =value[1:]\n        embedding_index[word]=coef;\n    f.close\nprint('word not in Glove ={}'.format(len(set(all_word)-set(embedding_index))))\n\n    ","d177632b":"train.head()","48070f22":"max(max(train.Phrase.apply(lambda x : len(x.split(' ')))),max(test.Phrase.apply(lambda x : len(x.split(' ')))))","9afa688a":"MAX_SEQUENCE_LENGTH=56","336cdebb":"test.head()","c5651e21":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nimport textblob\nMAX_SEQUENCE_LENGTH = 60\n\ntokenizer = Tokenizer()\n# numbering each word in vocab\ntokenizer.fit_on_texts(np.append(train[\"Phrase\"].values, test[\"Phrase\"].values))\nword_index = tokenizer.word_index\n\nnb_words = len(word_index) + 1\nembedding_matrix = np.random.rand(nb_words, EMBEDDING_DIM + 2)\n\nfor word, i in word_index.items():\n    embedding_vector = embedding_index.get(word)\n    sent = textblob.TextBlob(word).sentiment\n    if embedding_vector is not None:\n        embedding_matrix[i] = np.append(embedding_vector, [sent.polarity, sent.subjectivity])\n    else:\n        embedding_matrix[i, -2:] = [sent.polarity, sent.subjectivity]\n        \nseq = pad_sequences(tokenizer.texts_to_sequences(train[\"Phrase\"]), maxlen=MAX_SEQUENCE_LENGTH)\ntest_seq = pad_sequences(tokenizer.texts_to_sequences(test[\"Phrase\"]), maxlen=MAX_SEQUENCE_LENGTH)","b3660af1":"textblob.TextBlob('good').sentiment","d3dd17fc":"from keras.layers import  *\nfrom keras.models import Model\nfrom keras.callbacks import EarlyStopping\n\n\ndense_features = train.columns[4:10]\n\ndef build_model():\n    embading_layer =Embedding(input_dim=nb_words,output_dim=EMBEDDING_DIM+2,\n                              weights=[embedding_matrix],\n                              input_length=MAX_SEQUENCE_LENGTH,\n                              trainable=True)\n    dropout = Dropout(0.2)\n    mask =Masking()\n    lstm= LSTM(50)\n    \n    seq_input =Input(shape=(MAX_SEQUENCE_LENGTH,),dtype='int32')\n    dense_input =Input(shape=(len(dense_features),))\n    dense_vect =BatchNormalization()(dense_input)\n    phrase_vect=lstm(mask(dropout(embading_layer(seq_input))))\n    \n    f_vec= concatenate([dense_vect,phrase_vect])\n    f_vec= Dense(50,activation='relu')(f_vec)\n    f_vec=Dense(20,activation='relu')(f_vec)\n    output = Dense(5,activation='softmax')(f_vec)\n    \n    model = Model(inputs=[seq_input,dense_input],outputs=[output])\n    \n    return model","96c5d621":"test_preds = np.zeros((test.shape[0], 5))\n\nfor i in range(NUM_FOLDS):\n    print(\"FOLD\", i+1)\n    \n    print(\"Splitting the data into train and validation...\")\n    train_seq, val_seq = seq[train[\"fold_id\"] != i], seq[train[\"fold_id\"] == i]\n    train_dense, val_dense = train[train[\"fold_id\"] != i][dense_features], train[train[\"fold_id\"] == i][dense_features]\n    y_train = ohe.transform(train[train[\"fold_id\"] != i][\"Sentiment\"].values.reshape(-1, 1))\n    y_val = ohe.transform(train[train[\"fold_id\"] == i][\"Sentiment\"].values.reshape(-1, 1))\n    \n    print(\"Building the model...\")\n    model = build_model()\n    model.compile(loss=\"categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"acc\"])\n    \n    early_stopping = EarlyStopping(monitor=\"val_acc\", patience=2, verbose=1)\n    \n    print(\"Training the model...\")\n    model.fit([train_seq, train_dense], y_train, validation_data=([val_seq, val_dense], y_val),\n              epochs=15, batch_size=1024, shuffle=True, callbacks=[early_stopping], verbose=1)\n    \n    print(\"Predicting...\")\n    test_preds += model.predict([test_seq, test[dense_features]], batch_size=1024, verbose=1)\n    print()\n    \ntest_preds \/= NUM_FOLDS","153f569c":"dense_features","50d361a5":"ADD POLARITY AND SUBJECTIVITY\n\n* This tells us that the English phrase \u201cnot a very great calculation\u201d has a polarity of about -0.3, meaning it is slightly negative, and a subjectivity of about 0.6, meaning it is fairly subjective.","3f7fdd37":"phrase present in test ","b5542dec":" OneHotEncoder requires that all values are integers\n ","bb4f545b":"The sentiment labels are:\n\n* 0 - negative\n* 1 - somewhat negative\n* 2 - neutral\n* 3 - somewhat positive\n* 4 - positive","04b1173f":"# input sequence for LSTM","55df5afe":"****Model****"}}