{"cell_type":{"5af420fa":"code","544db22f":"code","74e5c340":"code","5232a69c":"code","039ffe23":"markdown","cbe7c03c":"markdown","4d63ee02":"markdown","1ad11c3a":"markdown","6681916a":"markdown","9421602b":"markdown"},"source":{"5af420fa":"import numpy as np\nimport pandas as pd\nimport matplotlib.pylab as plt\nimport cv2\nimport os\n\nDATASET_DIR = '\/kaggle\/input\/pku-autonomous-driving\/'\ntrain = pd.read_csv(os.path.join(DATASET_DIR, 'train.csv'))\n \n\n","544db22f":"# Average Image\nfname = train['ImageId'][0]\nfpath = os.path.join(DATASET_DIR, 'train_images', '{}.{}'.format(fname, 'jpg'))\nimg = cv2.imread(fpath)\n(R, C) = img.shape[:2]\ntotFrames = len(train)\naverImg = np.zeros((R,C))\n\nfor fname in train['ImageId']:\n    fpath = os.path.join(DATASET_DIR, 'train_images', '{}.{}'.format(fname, 'jpg'))\n    img = cv2.imread(fpath)\n    averImg += cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\/totFrames\n\nplt.imshow(averImg, cmap = 'gray')\nplt.show  ","74e5c340":"#  Y\/Z Analysis\nprediction_strings_expanded = train['PredictionString'].str.split(' ',expand = True).values.reshape(-1,7).astype(float)\ny = prediction_strings_expanded[:,5]\ny = y[~np.isnan(y)]\nz = prediction_strings_expanded[:,6]\nz = z[~np.isnan(z)]\ny = y[z<100]\nz = z[z<100]\nline_fit = np.polyfit(z,y,1)\ny_fit = z*line_fit[0] + line_fit[1]\nplt.plot(z,y,'b.')\nplt.plot(z, y_fit,'r')\nplt.grid(b = True)\nplt.title(f' y_fit = {line_fit[0]} * z + {line_fit[1]}')\nplt.ylabel('y')\nplt.xlabel('z')\nplt.show","5232a69c":"from math import sin, cos\n# Utility functions\ncamera_matrix = np.array([[2304.5479, 0,  1686.2379],\n                          [0, 2305.8757, 1354.9849],\n                          [0, 0, 1]], dtype=np.float32)\ncamera_matrix_inv = np.linalg.inv(camera_matrix)\ndef euler_to_Rot(yaw, pitch, roll):\n    Y = np.array([[cos(yaw), 0, sin(yaw)],\n                  [0, 1, 0],\n                  [-sin(yaw), 0, cos(yaw)]])\n    P = np.array([[1, 0, 0],\n                  [0, cos(pitch), -sin(pitch)],\n                  [0, sin(pitch), cos(pitch)]])\n    R = np.array([[cos(roll), -sin(roll), 0],\n                  [sin(roll), cos(roll), 0],\n                  [0, 0, 1]])\n    return np.dot(Y, np.dot(P, R))\ndef str2coords(s, names=['id', 'yaw', 'pitch', 'roll', 'x', 'y', 'z']):\n    '''\n    Input:\n        s: PredictionString (e.g. from train dataframe)\n        names: array of what to extract from the string\n    Output:\n        list of dicts with keys from `names`\n    '''\n    coords = []\n    for l in np.array(s.split()).reshape([-1, 7]):\n        coords.append(dict(zip(names, l.astype('float'))))\n        if 'id' in coords[-1]:\n            coords[-1]['id'] = int(coords[-1]['id'])\n    return coords\n\ndef get_img_coords(s):\n    '''\n    Input is a PredictionString (e.g. from train dataframe)\n    Output is two arrays:\n        xs: x coordinates in the image\n        ys: y coordinates in the image\n    '''\n    coords = str2coords(s)\n    xs = [c['x'] for c in coords]\n    ys = [c['y'] for c in coords]\n    zs = [c['z'] for c in coords]\n    P = np.array(list(zip(xs, ys, zs))).T\n    img_p = np.dot(camera_matrix, P).T\n    img_p[:, 0] \/= img_p[:, 2]\n    img_p[:, 1] \/= img_p[:, 2]\n    img_xs = img_p[:, 0]\n    img_ys = img_p[:, 1]\n    return img_xs, img_ys\n   \ndef draw_line(image, points):\n    color = (255, 0, 0)\n    cv2.line(image, tuple(points[0][:2]), tuple(points[3][:2]), color, 16)\n    cv2.line(image, tuple(points[0][:2]), tuple(points[1][:2]), color, 16)\n    cv2.line(image, tuple(points[1][:2]), tuple(points[2][:2]), color, 16)\n    cv2.line(image, tuple(points[2][:2]), tuple(points[3][:2]), color, 16)\n    return image\n\ndef draw_points(image, points):\n    for (p_x, p_y, p_z) in points:\n        cv2.circle(image, (p_x, p_y), int(1000 \/ p_z), (0, 255, 0), -1)\n    return image\n\ndef visualize(img, coords):\n    x_l = 1.02\n    y_l = 0.80\n    z_l = 2.31\n    \n    img = img.copy()\n    for point in coords:\n        # Get values\n        x, y, z = point['x'], point['y'], point['z']\n        yaw, pitch, roll = -point['pitch'], -point['yaw'], -point['roll']\n        # Math\n        Rt = np.eye(4)\n        t = np.array([x, y, z])\n        Rt[:3, 3] = t\n        Rt[:3, :3] = euler_to_Rot(yaw, pitch, roll).T\n        Rt = Rt[:3, :]\n        P = np.array([[x_l, -y_l, -z_l, 1],\n                      [x_l, -y_l, z_l, 1],\n                      [-x_l, -y_l, z_l, 1],\n                      [-x_l, -y_l, -z_l, 1],\n                      [0, 0, 0, 1]]).T\n        img_cor_points = np.dot(camera_matrix, np.dot(Rt, P))\n        img_cor_points = img_cor_points.T\n        img_cor_points[:, 0] \/= img_cor_points[:, 2]\n        img_cor_points[:, 1] \/= img_cor_points[:, 2]\n        img_cor_points = img_cor_points.astype(int)\n        # Drawing\n#        img = draw_line(img, img_cor_points)\n        img = draw_points(img, img_cor_points[-1:])\n        cv2.putText(img, str(int(z)), (img_cor_points[-1,0], img_cor_points[-1,1]), cv2.FONT_HERSHEY_PLAIN, 3, (0, 0, 255), 3, cv2.LINE_AA)\n    \n    return img\n\nfname = 'ID_0a1d250a1' \nfpath = os.path.join(DATASET_DIR, 'train_images', '{}.{}'.format(fname, 'jpg'))\nimg = cv2.imread(fpath)\nimage_idx = train.loc[train['ImageId'] == fname].index[0]\nimg_vis = visualize(img, str2coords(train['PredictionString'][image_idx]))\nplt.figure(figsize=(20,20))\nplt.imshow(img_vis)\nplt.title('integer part of Z values')\nplt.show","039ffe23":"I saved and croped details from a few images. I'll just leave them here, with no comments\n![Zval1s.jpg](attachment:Zval1s.jpg)\n![Zval2s.jpg](attachment:Zval2s.jpg)\n![Zval3.jpg](attachment:Zval3.jpg)","cbe7c03c":"My aim in this notebook (besides playing around for the first time with a Kaggle notebook :-) ) is to get a sense of what precision can be obtained by using simple optics\/geometry (assuming the detection itself will be achieved by DL)\nSince there is no description of how the Ground Truth dabase was built, I can think of two options:\n1. By means of a LIDAR - in this case, since its spatial resolution is low, there is uncertainty of which point on a target vehicle was actually measured (top, front, side door, etc.). Add to that the measurement precision itself => the challenge requirements are quite strict\n2. Estimated by some algorithm => resulting in this challenge being about the ability to learn\/imitate its behavior ","4d63ee02":"If the optical axis of the camera would have been parallel to the road plane, I would expect the Y values to have low variability, certainly not correclated with Z. The correlation indicates the pitch of the camera relative to the road plane. Also indicated by the location of the horizon.","1ad11c3a":"What I learn from this:\n* The fact that the test vehicle appears sharp in the average image indicates that the camera is quite stable and there is no systematic left-right image flipping in the database\n* A good indication about the location of the horizon\/vanishing point, usable for 'old school' distance estimation","6681916a":"The average image:","9421602b":"In order to get a subjective impression of the precision of the ground truth, let's use some 'quick and dirty' modification of the code from https:\/\/www.kaggle.com\/zstusnoopy\/visualize-the-location-and-3d-bounding-box-of-car, to display Z values of the labeled cars in some images"}}