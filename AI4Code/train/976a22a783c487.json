{"cell_type":{"4c683920":"code","483293d3":"code","f55d5b4f":"code","968ae837":"code","1a1a3a77":"code","699c9af0":"code","50521afd":"code","1041dde7":"code","1252fc53":"code","4ed35dbf":"code","c89e6be7":"code","58b48d99":"code","8d420c7a":"code","bdf9b83a":"code","49ae49b6":"markdown","55999644":"markdown","d6c4b9ca":"markdown","34c6f477":"markdown","4b8181ec":"markdown","bdf294d0":"markdown","eb1ea84c":"markdown","d3e6b639":"markdown","985e5e74":"markdown","5eedcc7a":"markdown","556cf8f1":"markdown"},"source":{"4c683920":"from sklearn.model_selection import cross_val_score, cross_val_predict, train_test_split\nfrom sklearn import datasets, metrics, tree\nfrom sklearn.ensemble import  RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import preprocessing\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n%matplotlib inline","483293d3":"import os\nprint(os.listdir(\"..\/input\"))","f55d5b4f":"Dtrain = pd.read_csv('..\/input\/train1.csv',header=0, sep=\",\",index_col=0)\nDtest = pd.read_csv('..\/input\/test1.csv',header=0, sep=\",\",index_col=False)\nprint(Dtrain.shape,Dtest.shape)\nDtest.head()","968ae837":"DD=Dtrain\nk = 0\nfor j in DD.columns:\n    k = k+1\n    print(\"%4d\" % k,\"%28s\" % j,\"\\t%12.1f\" % DD[j].min(),\" %12.1f\" % DD[j].max(), \n                          \" Range: %12.1f\" % (DD[j].max()-DD[j].min()),\" STD: %12.2f\" % DD[j].std())","1a1a3a77":"if Dtrain.isnull().values.any():\n    print(Dtrain.isnull().sum())","699c9af0":"# individuiamo la riga in  base al valore di ID\nZ = Dtrain['TARGET'].isna()\na = Dtrain[Z]\nprint(a['TARGET'])\nprint(a.index.values)\n# eliminiamo la riga individuata\nDtrain = Dtrain.drop(index=a.index.values)\n# controlliamo che non ci siano pi\u00f9 valori missing\nprint(\"Are there missing values?\",Dtrain.isnull().values.any())","50521afd":"Dtest.isnull().values.any()","1041dde7":"XX = Dtrain.values\nXT = Dtest.values\nxtrain = XX[:,:-1]\nytrain = XX[:,-1]\n# attenzione: XT contiene anche l'indice che ci servir\u00e0 per scrivere il file\n# con la soluzione finale\nX_test = XT[:,1:]\nprint(xtrain.shape,ytrain.shape,X_test.shape)","1252fc53":"X_train, X_val, y_train, y_val = train_test_split(xtrain,ytrain,test_size=0.25, random_state=33)\n\n\nprint(X_train.shape, X_val.shape, y_train.shape, y_val.shape)\n\n\n","4ed35dbf":"clf =  RandomForestClassifier(n_estimators=100, criterion='gini', max_depth=10, min_samples_split=2, \n                              min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=10, \n                              max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, \n                              bootstrap=True, oob_score=False, n_jobs=1, random_state=33, verbose=0, \n                              warm_start=False, class_weight=None)\nclf.fit(X_train,y_train)\n","c89e6be7":"# calcolo dell'indice AUC e dell'errore di classificazione sul training e sul test\nfrom sklearn.metrics import roc_auc_score\nscore = clf.score(X_train,y_train)\nprint(\"Score on training=\",score)\nys = clf.predict_proba(X_train)\nauc = roc_auc_score(y_train,ys[:,1])\nprint(\"AUC on training=\",auc)\n\nscore = clf.score(X_val,y_val)\nprint(\"Score on validation=\",score)\nyprob = clf.predict_proba(X_val)\nauc = roc_auc_score(y_val,yprob[:,1])\nprint(\"AUC on validation=\",auc)","58b48d99":"# Calculation of confusion matrix and other validation indexes\npredicted = clf.predict(X_val)\nprint(\"TEST: \\n Classification report for classifier %s:\\n\\n%s\\n\"\n      % (clf, metrics.classification_report(y_val, predicted)))\nprint(\"Confusion matrix sul Validation:\\n%s\" % metrics.confusion_matrix(y_val, predicted))","8d420c7a":"# Build the confusion matrix with a threshold different from 0.5\n\nix = (yprob[:,1]>0.1)\nprint(ix)\nyyy = np.zeros((len(y_val)))\nyyy[ix] = 1\nprint(\"TEST: \\n Classification report for classifier %s:\\n\\n%s\\n\"\n      % (clf, metrics.classification_report(y_val, yyy)))\nprint(\"Confusion matrix sul Validation:\\n%s\" % metrics.confusion_matrix(y_val, yyy))","bdf9b83a":"Y_test = clf.predict_proba(X_test)\nsoluz = pd.DataFrame({'ID':XT[:,0].astype(int),'TARGET':Y_test[:,1]})\nsoluz.to_csv('fine.csv', sep=\",\",index=False)","49ae49b6":"### Separate train from validation. The training will be called X_train, the validation X_val ","55999644":"### attention: call the model 'clf'\n#### is the name that is used in subsequent instructions ...","d6c4b9ca":"#### How can we interpret these indices?","34c6f477":"## Preprocessing (remember that it must also be done on the test-set)\nHere you have to implement all the operations of correction, transformation, imputation that can be useful to improve the result of the model, for example:\n\nscaler = preprocessing.StandardScaler().fit(X_train)\n\nscaler.transform(X_train)\n\n**MaxAbsScaler** and **maxabs_scale** were specifically designed for scaling sparse data,\n\n**QuantileTransformer** puts all features into the same range by performing a rank transformation, it smooths out unusual distributions and is less influenced by outliers","4b8181ec":"## Model choice\nHere you have to choose the model and set the parameters that increase its predictive capacity, you can consider all the models you want (even neural networks if you want), but at least the following three must be applied.\n\n#### 3 models:\nclass sklearn.tree.DecisionTreeClassifier(criterion=\u2019gini\u2019, splitter=\u2019best\u2019, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None, presort=False)\n\nclass sklearn.ensemble.RandomForestClassifier(n_estimators=10, criterion=\u2019gini\u2019, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=\u2019auto\u2019, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=1, random_state=None, verbose=0, warm_start=False, class_weight=None)\n\nclass sklearn.ensemble.GradientBoostingClassifier(loss=\u2019deviance\u2019, learning_rate=0.1, n_estimators=100, subsample=1.0, criterion=\u2019friedman_mse\u2019, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, min_impurity_split=None, init=None, random_state=None, max_features=None, verbose=0, max_leaf_nodes=None, warm_start=False, presort=\u2019auto\u2019)","bdf294d0":"#### After choosing the model, we have to estimate the value of the probability of target = 1 in the Test, so we have to create a file that for each unit reports the identifier and the probability of event 1, which we then have to upload to Kaggle","eb1ea84c":"## Start of program","d3e6b639":"### Let's analyze the characteristics of the explanatory variables","985e5e74":"#### If the missing is on the TARGET it is necessary to delete the line","5eedcc7a":"### Prediction on the validation dataset","556cf8f1":"### We check if there are missing in the variables"}}