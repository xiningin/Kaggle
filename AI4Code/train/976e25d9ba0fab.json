{"cell_type":{"ec7248e8":"code","9b25de77":"code","ddb4a49f":"code","cba97daf":"code","52a724ea":"code","962129c7":"code","74edd15f":"code","47e23f0b":"code","dbb14252":"code","0f7b03b1":"code","3f19d56c":"code","8982b4d2":"code","272e43eb":"code","6a32ed08":"markdown","6daa0aaa":"markdown","98e2fac7":"markdown","d37caab5":"markdown","ac0ff227":"markdown","44fb99d1":"markdown","d0f852ae":"markdown"},"source":{"ec7248e8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9b25de77":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n%matplotlib inline\n","ddb4a49f":"#resd csv file\ndf=pd.read_csv('\/kaggle\/input\/mushroom-classification\/mushrooms.csv')\ndf.head()","cba97daf":"#check null values in Dataset\ndf.isnull().sum()","52a724ea":"dummies = pd.get_dummies(df, drop_first=True)\ndummies.head()","962129c7":"# Split data features into dependent and independent\n# X is Independent variables\n# y is dependent variables\nX = dummies.iloc[:,1:]\ny = dummies.iloc[:,0]","74edd15f":"model = ExtraTreesClassifier()\nmodel.fit(X,y)","47e23f0b":"model.feature_importances_","dbb14252":"plt.figure(figsize=(10, 25))\nfeature_rank = pd.Series(model.feature_importances_, index = X.columns)\nfeature_rank.nlargest(95).plot(kind = \"barh\")","0f7b03b1":"drop_column = feature_rank.nsmallest(17).index\nX.drop(drop_column, axis = 1, inplace=True )","3f19d56c":"from sklearn.linear_model import LogisticRegression","8982b4d2":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\nlogistic = LogisticRegression()\n\nlogistic.fit(X_train, y_train)\n#To display the fitting function attributes such as coef,intercept etc..\nprint(logistic.coef_)\nprint(logistic.intercept_)\n#prediction from the test data\nprediction=logistic.predict(X_test)\nprint(\"Logistic Regression 20 classified values:\")\nprint(prediction[0:20])\n","272e43eb":"#model evaluation using classification metrics\n#confusion metrics - To display correctly classified data \n#and wrongly classified data\n#importing performance metrics\nfrom sklearn.metrics import accuracy_score,confusion_matrix\nconfus_matrix=confusion_matrix(y_test,prediction)\nprint(\"Confusion matrix\")\nprint(confus_matrix)\n#Calculate the accuracy\naccu_score=accuracy_score(y_test,prediction)\nprint(\"Accuracy\")\nprint(accu_score)\n#To display missclassified values from the prediction\nprint(\"Missclassified\")\nprint((y_test!=prediction).sum())","6a32ed08":"### feature Score","6daa0aaa":"### In above Grraph We include, First 17 Columns have less importance so Drop this columns","98e2fac7":"> ## Check Accurracy of Model","d37caab5":"> ## Feature Selection","ac0ff227":"> ## Essential Libraries","44fb99d1":"> ## Model Building","d0f852ae":"> ## Features Encoding using One Hot Encoding Technique"}}