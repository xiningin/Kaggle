{"cell_type":{"e0252c8e":"code","f31218e8":"code","a747c261":"code","73fe32b0":"code","a3b3eaac":"code","82ad27dc":"code","7de4d886":"code","0526038e":"code","75fc7794":"code","9f80904d":"code","d48377e4":"code","f639c926":"code","f435b732":"code","a05caea2":"code","906cf270":"code","a7cd2ebd":"code","c7d219f8":"code","76d6e97e":"code","2d635607":"code","12b48ac1":"code","b9b9b294":"code","fc00f6a9":"code","5e329dd4":"code","85a77b50":"markdown","ee5f1cf5":"markdown","0f8c0874":"markdown","8527a0f3":"markdown","3cf81b30":"markdown","84245ba4":"markdown","1fbcfc86":"markdown","9d3c07e7":"markdown","cab1ce90":"markdown","c6ddabf2":"markdown","83e65ac2":"markdown","0fc2e4ff":"markdown","1e03eea2":"markdown","585058f7":"markdown","f3e58970":"markdown","19b573f9":"markdown","5123db58":"markdown","fdaccbf1":"markdown","a4e3c3d3":"markdown","caa1727c":"markdown","4b9cafcb":"markdown","70338982":"markdown","b04a042e":"markdown","65467410":"markdown","c09becb2":"markdown","070b3490":"markdown","17e62aaa":"markdown","492151ef":"markdown","d6837ad7":"markdown","03d00d41":"markdown","0db7aa35":"markdown","cf942e9c":"markdown","f8e2ceed":"markdown","d0d64866":"markdown","211a10a9":"markdown","d122c5fa":"markdown","39bb77e1":"markdown","d0de8341":"markdown","0698aa3c":"markdown","2a6c08f0":"markdown","76e41957":"markdown"},"source":{"e0252c8e":"import os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nfrom collections import OrderedDict\n\nfrom sklearn import datasets\nfrom sklearn.preprocessing import label_binarize, LabelBinarizer\nfrom sklearn.model_selection import train_test_split, GridSearchCV\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_curve, auc\n\nDISPLAY_PRECISION = 4\npd.set_option(\"display.precision\", DISPLAY_PRECISION)","f31218e8":"dat = datasets.load_breast_cancer()\n# print(dat.DESCR)  # detailed description","a747c261":"print(\"The sklearn breast cancer dataset keys:\")\nprint(dat.keys()) # dict_keys(['target_names', 'target', 'feature_names', 'data', 'DESCR'])\nprint(\"---\")\n\n# Note that we need to reverse the original '0' and '1' mapping in order to end up with this mapping:\n# Benign = 0 (negative class)\n# Malignant = 1 (positive class)\n\nli_classes = [dat.target_names[1], dat.target_names[0]]\nli_target = [1 if x==0 else 0 for x in list(dat.target)]\nli_ftrs = list(dat.feature_names)\n\nprint(\"There are 2 target classes:\")\nprint(\"li_classes\", li_classes)\nprint(\"---\")\nprint(\"Target class distribution from a total of %d target values:\" % len(li_target))\nprint(pd.Series(li_target).value_counts())\nprint(\"---\")\n\ndf_all = pd.DataFrame(dat.data[:,:], columns=li_ftrs)\nprint(\"Describe dataframe, first 6 columns:\")\nprint(df_all.iloc[:,:6].describe().to_string())","73fe32b0":"TEST_SIZE_RATIO = 0.5  # split into 2 equally sized train and test sets\n\n# Setup X and y\nX = df_all\ny = pd.Series(li_target)\n\nX_train_0, X_test_0, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE_RATIO, random_state=0)\nprint(\"X_train_0.shape, y_train.shape\", X_train_0.shape, y_train.shape)\nprint(\"X_test_0.shape, y_test.shape\", X_test_0.shape, y_test.shape)","a3b3eaac":"def correlation_matrix(y, X, is_plot=False):\n  # Calculate and plot the correlation symmetrical matrix\n  # Return:\n  # yX - concatenated data\n  # yX_corr - correlation matrix, pearson correlation of values from -1 to +1\n  # yX_abs_corr - correlation matrix, absolute values\n  \n  yX = pd.concat([y, X], axis=1)\n  yX = yX.rename(columns={0: 'TARGET'})  # rename first column\n\n  print(\"Function correlation_matrix: X.shape, y.shape, yX.shape:\", X.shape, y.shape, yX.shape)\n  print()\n\n  # Get feature correlations and transform to dataframe\n  yX_corr = yX.corr(method='pearson')\n\n  # Convert to abolute values\n  yX_abs_corr = np.abs(yX_corr) \n  \n  if is_plot:\n    plt.figure(figsize=(10, 10))\n    plt.imshow(yX_abs_corr, cmap='RdYlGn', interpolation='none', aspect='auto')\n    plt.colorbar()\n    plt.xticks(range(len(yX_abs_corr)), yX_abs_corr.columns, rotation='vertical')\n    plt.yticks(range(len(yX_abs_corr)), yX_abs_corr.columns);\n    plt.suptitle('Pearson Correlation Heat Map (absolute values)', fontsize=15, fontweight='bold')\n    plt.show()\n  \n  return yX, yX_corr, yX_abs_corr\n\n# Build the correlation matrix for the train data\nyX, yX_corr, yX_abs_corr = correlation_matrix(y_train, X_train_0, is_plot=True)  ","82ad27dc":"CORRELATION_MIN = 0.1\n\n# Sort features by their pearson correlation with the target value\ns_corr_target = yX_abs_corr['TARGET']\ns_corr_target_sort = s_corr_target.sort_values(ascending=False)\n\n# Only use features with a minimum pearson correlation with the target of 0.1\ns_low_correlation_ftrs = s_corr_target_sort[s_corr_target_sort <= CORRELATION_MIN]\n\n# Print\nprint(\"Removed %d low correlation features:\" % len(s_low_correlation_ftrs))\nfor i,v in enumerate(s_low_correlation_ftrs):\n  print(i,np.round(v, DISPLAY_PRECISION), s_low_correlation_ftrs.index[i])\n  \nprint(\"---\")\n\ns_corr_target_sort = s_corr_target_sort[s_corr_target_sort > CORRELATION_MIN]\n\nprint(\"Remaining %d feature correlations:\" % (len(s_corr_target_sort)-1))\nfor i,v in enumerate(s_corr_target_sort):\n  ftr = s_corr_target_sort.index[i]\n  if ftr == 'TARGET':\n    continue\n    \n  print(i,np.round(v, DISPLAY_PRECISION), ftr)","7de4d886":"CORRELATION_MAX = 0.8\n\n# Remove features that have a low correlation with the target\nli_X1_cols = list(set(s_corr_target_sort.index) - set(s_low_correlation_ftrs.index)) \nli_X1_cols.remove('TARGET')\n\n# Build the correlation matrix for the reduced X\nX1 = X_train_0[li_X1_cols]\nyX1, yX_corr1, yX_abs_corr1 = correlation_matrix(y_train, X1, is_plot=False)  \n\n# Get all the feature pairs\nXcorr1 = yX_abs_corr1.iloc[1:,1:]\ns_pairs = Xcorr1.unstack()\nprint(\"s_pairs.shape\", s_pairs.shape)\ns_pairs = np.round(s_pairs, decimals=DISPLAY_PRECISION)\n\n# Sort all the pairs by highest correlation values\ns_pairs_sorted = s_pairs.sort_values(ascending=False) \ns_pairs_sorted = s_pairs_sorted[(s_pairs_sorted != 1) & (s_pairs_sorted > CORRELATION_MAX)]  # leave only the top matches that are not identical features\n\n# Convert to a list of name tuples e.g. ('mean radius', 'mean perimeter')\nli_corr_pairs = s_pairs_sorted.index.tolist()\n\nprint(\"len(li_corr_pairs):\", len(li_corr_pairs))\nprint(\"li_corr_pairs[:10]\", li_corr_pairs[:10])","0526038e":"# Build list of features to remove\nli_remove_pair_ftrs = []\nli_remove_scores = []\nfor tup in li_corr_pairs:\n    s0 = s_corr_target_sort.loc[tup[0]]\n    s1 = s_corr_target_sort.loc[tup[1]]\n    remove_ftr = tup[1] if s1 < s0 else tup[0]  # get the feature that is less correlated with the target\n    if remove_ftr not in li_remove_pair_ftrs:\n        li_remove_pair_ftrs.append(remove_ftr)\n        di = {'ftr_0':tup[0], 'ftr_1':tup[1], 'score_0':s0, 'score_1':s1, 'FEATURE_TO_REMOVE':remove_ftr}\n        li_remove_scores.append(OrderedDict(di))\n   \ndf_remove_scores = pd.DataFrame(li_remove_scores)\nprint(\"Removing %d features (see last column):\" % len(li_remove_pair_ftrs))\nprint(df_remove_scores.to_string())\nprint(\"---\")\n\n# Remove the features that were found in the above procedure\nli_X2_cols = list(set(li_X1_cols) - set(li_remove_pair_ftrs)) \nli_X2_cols.sort()\n\nprint(\"Remaining %d features:\" % (len(li_X2_cols)))\nfor i,v in enumerate(s_corr_target_sort):\n  ftr = s_corr_target_sort.index[i]\n\n  if ftr in li_X2_cols:\n    print(i,np.round(v, DISPLAY_PRECISION), ftr)","75fc7794":"# Calculate correlation matrix on the subset of features\nX2 = X1[li_X2_cols]\nprint(\"After the pair feature reduction, X2.shape:\", X2.shape)\nyX2, yX_corr2, yX_abs_corr2 = correlation_matrix(y_train, X2)\n\n# Recalculate the correlation matrix in order to plot the TARGET values in order of correlation\ns_X3_cols = yX_abs_corr2['TARGET'].sort_values(ascending=False)\nli_X3_cols = s_X3_cols.index.tolist()\nprint(\"Remaining features:\")\nprint(s_X3_cols)\nprint(\"---\")\nli_X3_cols.remove('TARGET')\n\nX3 = X2[li_X3_cols]\nprint(\"After the pair feature reduction, X2.shape:\", X3.shape)\nyX3, yX_corr3, yX_abs_corr3 = correlation_matrix(y_train, X3, is_plot=True)\n\nX_train = X3\nX_test = X_test_0[li_X3_cols]","9f80904d":"sns.jointplot(yX3['worst concave points'], yX3['TARGET'], kind='scatter', marginal_kws=dict(bins=12, rug=True))\nplt.suptitle('Example scatter matrix pair')","d48377e4":"color_map = {0: '#0392cf', 1: '#7bc043'}  # 0 (negative class): blue, 1 (positive class): green\ncolors = yX3['TARGET'].map(lambda x: color_map.get(x))\npd.plotting.scatter_matrix(yX3, alpha=0.5, color=colors, figsize=(12,14), diagonal='hist', hist_kwds={'bins':12})\nplt.suptitle('Scatter matrix of target and model features')\nplt.show()","f639c926":"# Train data\nprint(\"X_train.shape, y_train.shape\", X_train.shape, y_train.shape)\n\n## Print the class distribution of the TARGET for both train and test sets\nval_cnts = y_train.value_counts()\nprint(\"Class distribution of positive and negative samples in the train set:\")\nprint(val_cnts)\nprint(\"Percentage of positive class samples: %s\" % \"%2f%%\" % (100 * val_cnts[1] \/ len(y_train)))\n\nprint(\"---\")\nprint(\"X_test.shape, y_test.shape\", X_test.shape, y_test.shape)\n\nval_cnts = y_test.value_counts()\nprint(\"Class distribution of positive and negative samples in the test set:\")\nprint(val_cnts)\nprint(\"Percentage of positive class samples: %s\" % \"%2f%%\" % (100 * val_cnts[1] \/ len(y_test)))","f435b732":"def plot_2d_grid_search_heatmap(grid_search, grid_params, x_param, y_param, is_verbose=True):\n  '''\n  Plot 2d grid search heatmap\n  \n  Parameters:\n  grid_search: instance of sklearn's GridSearchCV\n  grid_params: dictionary of grid search parameters  \n  x_param: name of x-axis parameter in grid_params\n  y_param: name of y-axis parameter in grid_params\n  is_verbose (optional): print results\n  \n  Return:\n  grid_search.best_score_: best score found\n  grid_search.best_estimator_: best estimator found\n  '''\n  \n  grid_params_x = grid_params[x_param]\n  grid_params_y = grid_params[y_param]\n  \n  df_results = pd.DataFrame(grid_search.cv_results_)\n  ar_scores = np.array(df_results.mean_test_score).reshape(len(grid_params_y), len(grid_params_x))\n  sns.heatmap(ar_scores, annot=True, fmt='.3f', xticklabels=grid_params_x, yticklabels=grid_params_y)\n  print()\n  plt.suptitle('Grid search heatmap')\n  plt.xlabel(x_param)\n  plt.ylabel(y_param)\n  \n  if is_verbose:\n    print(\"grid_search.best_score_:\")\n    print(grid_search.best_score_)\n    print()\n    print(\"grid_search.best_estimator_:\")\n    print(grid_search.best_estimator_)\n    \n  return grid_search.best_score_, grid_search.best_estimator_","a05caea2":"# Note that we will often need to normalize or standardize the data prior to doing Logistic Regression, \n# however this is not always necessary (preprocessing would have lost the direct interpretation of each feature's value)\ngrid_lr = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000], 'penalty': ['l1', 'l2']}\nclf_lr = LogisticRegression(class_weight='balanced', dual=False, \n          fit_intercept=True, intercept_scaling=1, max_iter=200,\n          n_jobs=1, random_state=0, tol=0.0001, verbose=0, warm_start=False)\n\n# Cross validated grid search\ngs_lr = GridSearchCV(clf_lr, grid_lr, return_train_score=True)  # note that by default, 3-fold cross validation is performed\n\n# Fit the model\ngs_lr.fit(X_train, y_train)\n\n# Plot\nbest_score_lr, clf_lr = plot_2d_grid_search_heatmap(gs_lr, grid_lr, 'C', 'penalty')\n\n# Print Logistic Regression specific attributes\nprint(\"intercept_:\")\nprint(clf_lr.intercept_ )\nprint()\nprint(\"coef_:\")\nprint(clf_lr.coef_)","906cf270":"# Note that for tree based classifiers like GBM, \n# we do not need to normalize or standardize the data first \n# since tree based models are not affected by a linear transformation.\n# Note that only 2 grid search parameters are demonstrated here (usually we will need to search across more parameters). \ngrid_gb = {'min_samples_leaf': [2, 4, 8, 16], 'learning_rate': [0.001, 0.01, 0.1]}\n\nclf_gbm = GradientBoostingClassifier(criterion='friedman_mse',\n              init=None, loss='deviance', max_features='sqrt', max_leaf_nodes=None, \n              max_depth=3, min_impurity_decrease=0.0, min_impurity_split=None,\n              min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=800,\n              presort='auto', random_state=0, subsample=1.0, verbose=0, warm_start=False)\n\ngs_gb = GridSearchCV(clf_gbm, grid_gb, verbose=0, return_train_score=True)  # note that by default, 3-fold cross validation is performed\n\n# Fit the model\ngs_gb.fit(X_train, y_train)\n\n# Plot\nbest_score_gb, clf_gb = plot_2d_grid_search_heatmap(gs_gb, grid_gb, 'min_samples_leaf', 'learning_rate')","a7cd2ebd":"# For a discussion on how to find the 'importance' of the features for a logistic regression model, see:\n# https:\/\/stackoverflow.com\/questions\/34052115\/how-to-find-the-importance-of-the-features-for-a-logistic-regression-model\ndfimp = pd.DataFrame(np.std(X_train, 0), columns=['std'])\n\n# Logistic regression\ndfimp['coef'] = clf_lr.coef_.T\ndfimp['lr_sign_imp'] = (np.std(X_train, 0).ravel() * clf_lr.coef_).T\ndfimp['lr_imp'] = np.abs(dfimp['lr_sign_imp'])\n\n# GBM\ndfimp['gb_imp'] = pd.Series(clf_gb.feature_importances_, X_train.columns)\n\n# Add a column for the correlation with target\ndfimp['target_corr'] = s_X3_cols.drop('TARGET')\n\n# Rank from high to low values of importance\ndfimp['lr_rank'] = dfimp['lr_imp'].rank(ascending=False)\ndfimp['gb_rank'] = dfimp['gb_imp'].rank(ascending=False)\ndfimp['target_corr_rank'] = dfimp['target_corr'].rank(ascending=False)\n\ndfsort = dfimp.sort_values('target_corr_rank', ascending=True)\nprint(dfsort.iloc[:20,:].to_string())","c7d219f8":"cols_top = dfsort.index[:7]\nprint(\"[y_test, X_test]\")\nyX_test = pd.concat([y_test.iloc[-10:], X_test[cols_top].iloc[-10:,:]], axis=1)\nyX_test = yX_test.rename(columns={0: 'TARGET'})\nprint(yX_test.to_string())","76d6e97e":"li_clfs = ['clf_lr', 'clf_gb']\ndfp = pd.DataFrame(index=['TARGET'], data=[y_test]).T\n\nfor s_clf in li_clfs:\n    print(\"MODEL: \" + s_clf)\n    print(\"--------------\")\n    clf = eval(s_clf)\n    \n    y_pred = clf.predict(X_test).astype(int)  # returns a class decision based on the value of the predicted probability\n    y_score = clf.predict_proba(X_test)  # returns the value of the predicted probability\n\n    s_class = \"%s_class\" % s_clf\n    s_proba = \"%s_proba\" % s_clf\n    s_rank = \"%s_rank\" % s_clf\n    \n    dfp[s_class] = y_pred\n    dfp[s_proba] = y_score[:,1]\n    dfp[s_rank] = dfp[s_proba].rank(ascending=1).astype(int)\n    \n    # Print confusion matrix & classification report\n    # from sklearn.metrics import confusion_matrix\n    # cm = confusion_matrix(y_test, y_pred)\n    \n    # Pandas 'crosstab' displays a better formatted confusion matrix than the one in sklearn\n    cm = pd.crosstab(y_test, y_pred, rownames=['Reality'], colnames=['Predicted'], margins=True)\n    print(cm) \n    \n    print()\n    print(\"Classification report:\")\n    print(classification_report(y_test, y_pred))\n    \n    if s_clf == 'clf_lr':\n        y_score_lr = y_score.copy() \n    elif s_clf == 'clf_gb':\n        y_score_gb = y_score.copy()\n    else:\n        print('Error')\n        break","2d635607":"print(dfp.tail(10).to_string())   ","12b48ac1":"RANK_BINS = 10\nrank_cols = ['i_bin', 'i_min_bin', 'i_max_bin', 'score_min', 'score_max', 'bin_cnt', 'pos_cnt', 'pos_rate']\nrank_cols2 = ['i_bin', 'score_min', 'score_max', 'tnr', 'fpr', 'fnr', 'tpr', 'tn', 'fp', 'fn', 'tp'] \n\nprint(dfp.shape)\nlen_test = dfp.shape[0]\nlen_bin = int(len_test \/ RANK_BINS)\n\nfor s_clf in li_clfs:\n    i_min_bin = 0\n    i_max_bin = 0\n    dfr = pd.DataFrame(columns=rank_cols)\n    dfr2 = pd.DataFrame(columns=rank_cols2)\n    \n    s_class = \"%s_class\" % s_clf\n    s_proba = \"%s_proba\" % s_clf\n    s_rank = \"%s_rank\" % s_clf\n    \n    for i in range(RANK_BINS):\n        if i == RANK_BINS - 1:\n            i_max_bin = len_test\n        else:\n            i_max_bin += len_bin\n\n        # Range used for *each* score bin\n        df_rng = dfp[(dfp[s_rank] >= i_min_bin) & (dfp[s_rank] < i_max_bin)]\n        \n        score_min = np.min(df_rng[s_proba])\n        score_max = np.max(df_rng[s_proba])\n        bin_cnt = df_rng.shape[0]\n            \n        pos_cnt = len(df_rng[df_rng['TARGET'] == 1])\n        pos_rate = pos_cnt \/ bin_cnt\n        \n        # Range used for *all* score bins up to i_max_bin\n        df_rng_0 = dfp[dfp[s_rank] < i_max_bin]\n        df_rng_1 = dfp[dfp[s_rank] >= i_max_bin]\n        \n        # Positive targets (positive customers)\n        tp = len(df_rng_1[df_rng_1['TARGET'] == 1])\n        fn = len(df_rng_0[df_rng_0['TARGET'] == 1])\n        pos = tp + fn\n        tpr = tp \/ pos\n        fnr = 1 - tpr\n        \n        # Negative targets\n        tn = len(df_rng_0[df_rng_0['TARGET'] == 0])\n        fp = len(df_rng_1[df_rng_1['TARGET'] == 0])\n        neg = tn + fp\n        fpr = fp \/ neg      \n        tnr = 1 - fpr\n        \n        # Build the dataframe for the summary stats per bin\n        row = [i, i_min_bin, i_max_bin, score_min, score_max, bin_cnt, pos_cnt, pos_rate]\n        dfr.loc[i] = row\n        \n        # Build the dataframe for the ROC stats per bin\n        row2 = [i, score_min, score_max, tnr, fpr, fnr, tpr, tn, fp, fn, tp]\n        dfr2.loc[i] = row2\n        \n        # Prep for next iteration\n        i_min_bin = i_max_bin \n              \n    if s_clf == 'clf_lr':\n        dfr_lr = dfr.copy() \n        dfr2_lr = dfr2.copy() \n    elif s_clf == 'clf_gb':\n        dfr_gb = dfr.copy()\n        dfr2_gb = dfr2.copy()\n    else:\n        print('Error')\n        break\n \nprint(\"-------\")\nprint(\"Results of positive count stats per bin\")\nprint(\"Logistic Regression stats:\")\nprint(dfr_lr.to_string())\nprint()\nprint(\"GBM stats:\")\nprint(dfr_gb.to_string())\n\nprint()\nprint(\"---\")\nprint(\"Overall stats in the test set:\")\npos_cnt = len(dfp[dfp['TARGET'] == 1])\npos_rate = pos_cnt \/ len_test\n\nprint(\"pos_cnt\", pos_cnt)\nprint(\"total rows\", len_test)\nprint(\"pos_rate\", pos_rate)","b9b9b294":"s_titles = ['Logistic Regression', 'GBM']\n\nplt.figure(figsize=(12, 4))\nfor i, s_clf in enumerate(li_clfs):\n    s_title = s_titles[i]\n    \n    if s_clf == 'clf_lr':\n        dfr = dfr_lr \n        dfr2 = dfr2_lr \n    elif s_clf == 'clf_gb':\n        dfr = dfr_gb\n        dfr2 = dfr2_gb\n    else:\n        print('Error')\n        break\n    \n    plt.subplot(1, 2, i+1)\n    plt.bar(dfr['i_bin'], dfr['pos_rate'])\n    plt.title('Percentiles for ' + s_title)\n\n    # Format charts\n    plt.grid()\n    plt.xlabel('Score bin (from low to high scoring bins)')\n    plt.ylabel('Ratio of positive classes per bin')\n    plt.legend(loc=\"lower right\")\n    \n    plt.xlim([0, RANK_BINS])\n    plt.ylim([0.0, 1])\n    \nplt.show()","fc00f6a9":"print(\"Results of ROC stats per bin\")\nprint(\"Logistic Regression stats:\")\nprint(dfr2_lr.to_string())\nprint()\nprint(\"GBM stats:\")\nprint(dfr2_gb.to_string())","5e329dd4":"def plot_roc_and_precision_recall(y_true, y_score):\n  # Get ROC curve FPR and TPR from true labels vs score values\n  fpr, tpr, _ = roc_curve(y_true, y_score)\n\n  # Calculate ROC Area Under the Curve (AUC) from FPR and TPR data points\n  roc_auc = auc(fpr, tpr)\n\n  # Calculate precision and recall from true labels vs score values\n  precision, recall, _ = precision_recall_curve(y_true, y_score)\n\n  plt.figure(figsize=(8, 3))\n\n  plt.subplot(1,2,1)\n  lw = 2\n  plt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve (AUC = %0.4f)' % roc_auc)\n  plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n  plt.xlim([0.0, 1.0])\n  plt.ylim([0.0, 1.05])\n  plt.xlabel('False Positive Rate')\n  plt.ylabel('True Positive Rate')\n  plt.title('ROC Curve')\n  plt.legend(loc=\"lower right\")\n  plt.grid(True)\n\n  plt.subplot(1,2,2)\n  plt.step(recall, precision, color='orange', where='post')\n  # plt.fill_between(recall, precision, step='post', alpha=0.5, color='orange')\n  plt.xlabel('Recall')\n  plt.ylabel('Precision')\n  plt.ylim([0.0, 1.05])\n  plt.xlim([0.0, 1.0])\n  plt.title('Precision Recall Curve')\n  plt.grid(True)\n\n  left  = 0.125  # the left side of the subplots of the figure\n  right = 0.9    # the right side of the subplots of the figure\n  bottom = 0.1   # the bottom part of the subplots of the figure\n  top = 0.9      # the top part of the subplots of the figure\n  wspace = 0.5   # the amount of width reserved for blank space between subplots\n  hspace = 0.2   # the amount of height reserved for white space between subplots\n  plt.subplots_adjust(left, bottom, right, top, wspace, hspace)\n  plt.show()\n\n  \nfor s_clf in li_clfs:\n    print(\"MODEL: \" + s_clf)\n    print(\"--------------\")\n    clf = eval(s_clf)\n    \n    y_pred = clf.predict(X_test).astype(int)\n    y_score = clf.predict_proba(X_test)\n    \n    plot_roc_and_precision_recall(y_test, y_score[:,1])  # provide the column for the scores belonging only to the positive class\n    print()\n    ","85a77b50":"## Build GBM Classifier","ee5f1cf5":"Classify the data using two different types of models:\n1. Logistic Regression Model\n2. GBM - Gradient Boosting Model","0f8c0874":"# Example using scikit-learn: Breast cancer prediction","8527a0f3":"In the heatmap shown above, the optimal 'learning_rate' is 0.01 with indifference to the 'min_samples_leaf' parameter. (If we further increase the 'n_estimators' then in this case the performance of learning_rate=0.001 is on par with learning_rate=0.01, however, this won't always be the case and we usually need to search across a wide range of parameters in order to find the optimum values. Using a high value for  'n_estimators' will slow down the search).","3cf81b30":"The 'worst concave points' feature shown above has a high pearson correlation with the target (0.81) and therefore we see good separation of the points that correspond to the negative class (at the bottom, where TARGET=0) vs the points that correspond to the positive class (at the top, where TARGET=1).","84245ba4":"## Data\n\nThe tagged data set is from the \"Breast Cancer Wisconsin (Diagnostic) Database\" freely available in python's sklearn library, for details see:  \nhttps:\/\/archive.ics.uci.edu\/ml\/datasets\/Breast+Cancer+Wisconsin+%28Diagnostic%29\n\n* Number of Samples: 569  \n* Number of Features: 30 numeric, predictive attributes  \n* Number of Classes: 2 \n\nFeatures are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image. Ten real-valued features are computed for each cell nucleus. The mean, standard error and 'worst' or largest (mean of the three largest values) of these features were computed for each image, resulting in 30 features. For instance, the radius measurements are for the 'mean radius',  'standard error of the radius', and 'worst radius'. All feature values are recoded with four significant digits.\n\nThe two target classes correspond to negative outcomes (Benign) and positive outcomes (Malignant).\n\n**This original data set will be randomly split into two sets for train and test purposes.**","1fbcfc86":"In general, when using GridSearchCV in sklearn, we should use the 'Pipeline' function in order to correctly 'preprocess' and then 'model' the data when doing cross-validation. The syntax looks like this:\n\npipe = Pipeline([('preprocess', preprocess), ('model', LogisticRegression())])\n\nThis enforces that during the k-fold cross-validation, the preprocessing step 'sees' the exact same hold out sets as the model step.\n\nIn this notebook, as we'll see below, a separate 50% of the data was designated as the independent, random test set and we end up achieving good results despite not taking advantage of sklearn's 'Pipeline' function.\n\n\n\n","9d3c07e7":"Note that in some cases it is easier to train a model if we maintain an exact balance of train and test target values (a stratified sample). Here the positive to negative class ratio happens to already be quite well balanced in the train and test sets.","cab1ce90":"##  1 Univariate feature reduction (remove low correlations with the target)\n\n","c6ddabf2":"## Get the breast cancer dataset from sklearn","83e65ac2":"Note the high correlation values between features which are directly related to each other e.g. mean radius, mean perimeter, mean area.","0fc2e4ff":"## Plot all the scatter matrix pairs in a single plot","1e03eea2":"## 3 Compare two different model types, including the testing and ranking of feature importance","585058f7":"## Methodology\n\nThe following data science techniques will be demonstrated:\n\n1. Univariate feature reduction (remove low correlations with the target).\n2.  Feature reduction based on collinearity (for each highly correlated pair of features, leave only the feature that correlates better with the target value).\n3. Compare two different model types for supervised learning (Logistic Regression and GBM), including the testing and ranking of feature importance.\n4. Calculate percentile bins for each model in order to determine the ratio of positive classes for each percentile bin.\n5. Plot ROC and Precision-Recall curves.","f3e58970":"## Split the data into train and test sets","19b573f9":"## Describe the feature statistics","5123db58":"Note above that for the feature reduction, features that have similar predictive power are removed e.g.  for this feature pair: mean radius (0.7108) vs mean perimeter (0.7240), the mean radius feature is removed.","fdaccbf1":"Often we prefer models that give consistent results as the score value increases. For this kind of analysis, we prefer models that result in score bins that are monotonically increasing as a function of the ratio of positive classes per bin - as is the case of the Logistic Regression model demonstrated here.","a4e3c3d3":"## Feature importances","caa1727c":"## Create a correlation heatmap of absolute correlation scores","4b9cafcb":"##  4. Calculate percentile bins for each model in order to determine the ratio of positive classes for each percentile bin","70338982":"## Feature reduction\nThere are various popular methods for feature reduction; the feature selection technique demonstrated here comprises two methods applied in sequence:\n\n1. Univariate feature reduction (remove low correlations with the target).\n2. Feature reduction based on collinearity (for each highly correlated pair, use only the feature that correlates better with the target value).","b04a042e":"## Plot specific scatter matrix pair for one of the features vs the target","65467410":"Note that each of the equally sized 10 bins represents 10% of the data in the test set. The range of scores allocated to each bin is limited from 'score_min' to 'score_max'.","c09becb2":"## 5. Plot ROC and Precision-Recall curves","070b3490":"## Predict overall scores and print Confusion Matrix","17e62aaa":"## Important note on sklearn's GridSearchCV function","492151ef":"## Print ROC stats per bin","d6837ad7":"## Summary  \n\n* A correlation technique for feature reduction was demonstrated resulting in a reduced subset of 9 out of the original 30 features.\n\n* Two different model types were demonstrated, Logistic Regression and GBM. Optimal parameters were found using sklearn's GridSearchCV.\n\n* The ranking of the feature importance of the two different models resulted in slightly different feature importance rankings. Note that in general, if we are testing many model types, we can subsequently use ensemble techniques in order to improve overall model performance.\n\n* Often we prefer models that give consistent results as the score value increases. For this kind of analysis, we prefer models that result in score bins that are monotonically increasing as a function of the ratio of positive classes per bin - as is the case for the Logistic Regression model demonstrated here.\n\n* The Logistic Regression's ROC AUC (Area Under the Curve) score was slightly better than that of the GBM. Note that this result was the outcome of using a relatively small train set (50% of all the data).\n\n* By setting TEST_SIZE_RATIO = 0.2 (which means that the train size ratio is 0.8), we can train on more data and this results in improved ROC AUC metrics for the GBM. Note that the smaller test set will result in a less stable ROC AUC result. To further validate our findings, we could average multiple random train\/test splits (using either bootstrapped samples or averaging over independent test data splits).\n","03d00d41":"## Function to plot a heatmap for the model's grid search","0db7aa35":"In the heatmap shown above, the x-axis is for the Logistic Regression model's 'C\" parameter: It is the inverse of regularization strength and must be a positive float. Like in support vector machines, smaller values specify stronger regularization. \n\nHere, models with an L2 penalty (for the squared errors) perform better than models with an L1 penalty (for the absolute errors). Only the highest 'C\" value in the heatmap (where almost no regularization is applied) results in an L1 score that approaches the L2 score.","cf942e9c":"## Print importance feature for a small sample of the data","f8e2ceed":"## Setup","d0d64866":"## First rows in X and y ","211a10a9":"## Plot the correlation matrix for the final dataframe columns","d122c5fa":"## For each of the highest correlated feature pairs, remove the feature that is less correlated with the target\n","39bb77e1":"## 2  Feature reduction based on collinearity (for each highly correlated pair of features, leave only the feature that correlates better with the target value).","d0de8341":"Only the subset of features shown above will be used for the model. The scatter matrix helps to understand the pairwise relationships between the features. Note that the blue dots represent negative targets and the green dots represent positive targets. The diagonal contains a histogram plot per feature.\n","0698aa3c":"The importance associated with each feature somewhat varies between the models (see the '_rank' columns on the right). As might be expected, the above feature importance rankings correlate quite well with the ranking order found previously by simply ordering the pearson correlation of each feature with the target.\n","2a6c08f0":"## Plot ratio of positive classes for each percentile bin","76e41957":"## Build Logistic Regression Classifier"}}