{"cell_type":{"50e81e56":"code","0af7d9ee":"code","3b192a65":"code","72636185":"code","666ce680":"code","01014734":"code","864cfe5a":"code","8a268a92":"code","7e8a135d":"code","e8bd65b3":"code","55291b44":"code","a2c0503a":"code","39a5c72e":"code","25ce464e":"code","5bddbd4e":"code","ef4d6131":"code","fff67bbd":"code","eec0802e":"code","1ea5d395":"code","ce77c68e":"code","c3e0237b":"code","c62ba1e3":"code","e4a36acd":"code","da2ee434":"code","eeaff748":"code","5f6d5a40":"code","f7980f30":"code","ebb1e378":"code","48c83f90":"code","bf3f9152":"code","4a3ede57":"code","e9552256":"code","d35b0179":"code","8b243bc7":"code","ee86d091":"code","24dcf6b4":"code","11ff2ce0":"code","508f24b3":"code","6d444767":"code","6792c956":"code","92ab0140":"code","4290de79":"code","24eb1373":"code","514f90e5":"code","3b130171":"code","dbfc06f6":"code","2499e0be":"code","892f215b":"code","dd6d5e96":"code","d24ccb56":"code","94eeb4d6":"code","3e19e212":"code","292bf10c":"code","5a9072a8":"code","1ca9f48e":"code","09a8354d":"code","daa4fd9e":"code","6f793f3a":"code","f1815a18":"code","427e3020":"code","79813f7e":"code","3edb57cc":"code","26bc6a77":"code","6ffa798b":"code","9da2a672":"code","12e85855":"code","076570a7":"code","bac3f629":"code","7b6b1f87":"code","5825cf0f":"code","8827e763":"code","bb9a8d54":"code","02112ff3":"code","a8daf406":"code","40b7d778":"code","64808c8a":"code","9aefbb6e":"code","bde9f6c5":"code","b3bb685d":"code","90b92d9c":"code","610e9838":"code","4af2822b":"code","cccc6973":"code","abdb551c":"code","224dcbad":"code","2f1d8267":"markdown","980c02b6":"markdown","9d070fc8":"markdown","316e0c3e":"markdown","3a125bbd":"markdown","6ffffb8d":"markdown","4c27063e":"markdown","56c601b8":"markdown","72b8dd45":"markdown","7ab015bd":"markdown","814ec6fe":"markdown","babf5a52":"markdown","9a1762ee":"markdown","5a2c3d3d":"markdown","4c764022":"markdown","f11d577b":"markdown","81fa2272":"markdown","5dec81ae":"markdown","3ddc5b16":"markdown","5d19b11a":"markdown","9687a374":"markdown","b3bf45a8":"markdown","a55ab460":"markdown","f312cf1f":"markdown","08eca105":"markdown","7dcd429e":"markdown","df3c39d3":"markdown","cedd66d2":"markdown","dcb6ce2d":"markdown","a515520b":"markdown","7b570bd1":"markdown","469d3fc2":"markdown","8e698c23":"markdown","9b1bcceb":"markdown","7b43f9ba":"markdown","58cd04e0":"markdown","d57df3c6":"markdown","b96646ef":"markdown","9fec776a":"markdown","3a50dd63":"markdown","72145bf9":"markdown","8549cf08":"markdown","91baf78f":"markdown","b09c7574":"markdown","c19314f8":"markdown","0fcb91f5":"markdown","64c41fc1":"markdown","be55a44d":"markdown","0935c3de":"markdown","97c50453":"markdown","d639894d":"markdown","3027003b":"markdown","776edf18":"markdown","4f0ac27a":"markdown","e50f6705":"markdown","5c4b2f0e":"markdown","c29ffcad":"markdown","9a6e7e03":"markdown","37fa9766":"markdown","92fbbc74":"markdown","9950b245":"markdown","e59274db":"markdown","7d0d7e10":"markdown","efdfd59e":"markdown","44710629":"markdown"},"source":{"50e81e56":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0af7d9ee":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport math as math\nfrom scipy import stats\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import ParameterGrid\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, fbeta_score\n\nfrom imblearn.over_sampling import SMOTE\nnp.random.seed(11)","3b192a65":"df_ = pd.read_csv('\/kaggle\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv',index_col=0)","72636185":"df_.head()","666ce680":"df = df_.sample(n = len(df_),)\ndf.index = range(0,len(df))\ndf.head()","01014734":"df.info()","864cfe5a":"def clean_df():\n    df_ = pd.read_csv('\/kaggle\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv',index_col=0)\n    df = df_.sample(n = len(df_),)\n    df.index = range(0,len(df))\n    # puts all nan bmi values == mean\n    df[\"bmi\"] = df[\"bmi\"].apply(lambda x: round(df[\"bmi\"].mean(),1) if math.isnan(x) else x)\n    return df","8a268a92":"df = clean_df()\ndf.head()","7e8a135d":"sns.pairplot(df, hue=\"stroke\", corner=True, kind=\"hist\", palette=\"coolwarm\")","e8bd65b3":"sns.pairplot(df, hue=\"stroke\", corner=True, kind=\"reg\", palette=\"coolwarm\")","55291b44":"df_corr = df.corr()\ndf_corr","a2c0503a":"mask = np.zeros_like(df_corr)\nmask[np.triu_indices_from(mask)] = True\nwith sns.axes_style(\"white\"):\n    f, ax = plt.subplots(figsize=(7, 5))\n    ax = sns.heatmap(df_corr, mask=mask, vmax=.3, vmin=0, square=True, cmap=\"coolwarm\", annot=True)","39a5c72e":"corr = np.array(df_corr.iloc[-1][:-1]).reshape(1,-1)\nplt.figure(figsize=(10,1))\n_=plt.imshow(corr, cmap=\"coolwarm\", aspect=\"auto\", extent=[0.5,5.5,0,1])\n_=plt.xticks(ticks=[1,2,3,4,5], labels=list(df_corr.index[:-1]))\n_=plt.yticks(ticks=[0.5], labels=[\"Stroke Correlation\"])\n_=plt.title(\"Feature Correlation with Strokes\")","25ce464e":"x=[\"No Hypertension\",\"Yes Hypertension\"]\n_=plt.title(\"Stokes compared with Hypertension:\")\n_=plt.bar(x, height = df[(df[\"stroke\"]==0)][\"hypertension\"].value_counts(), color=\"blue\")\n_=plt.bar(x, height = df[(df[\"stroke\"]==1)][\"hypertension\"].value_counts(), color=\"red\")","5bddbd4e":"strokes = df[df[\"hypertension\"]==0][\"stroke\"].value_counts()\nprint(\"The percentage of (hypertension = no) with strokes is {:.1f}%\".format(100*strokes[1]\/sum(strokes)))\n\nstrokes = df[df[\"hypertension\"]==1][\"stroke\"].value_counts()\nprint(\"The percentage of (hypertension = yes) with strokes is {:.1f}%\".format(100*strokes[1]\/sum(strokes)))","ef4d6131":"df[\"heart_disease\"].value_counts()","fff67bbd":"x=[\"No Heart Disease\",\"Yes Heart Disease\"]\n_=plt.title(\"Stokes compared with Heart Disease:\")\n_=plt.bar(x, height = df[(df[\"stroke\"]==0)][\"heart_disease\"].value_counts(), color=\"blue\")\n_=plt.bar(x, height = df[(df[\"stroke\"]==1)][\"heart_disease\"].value_counts(), color=\"red\")","eec0802e":"strokes = df[df[\"heart_disease\"]==0][\"stroke\"].value_counts()\nprint(\"The percentage of (heart disease = no) with strokes is {:.1f}%\".format(100*strokes[1]\/sum(strokes)))\n\nstrokes = df[df[\"heart_disease\"]==1][\"stroke\"].value_counts()\nprint(\"The percentage of (heart disease = yes) with strokes is {:.1f}%\".format(100*strokes[1]\/sum(strokes)))","1ea5d395":"sns.jointplot(x=\"age\", y=\"bmi\", data=df, hue=\"stroke\", palette=\"coolwarm\", kind=\"kde\")","ce77c68e":"df[\"stroke\"].value_counts()","c3e0237b":"df[(df[\"age\"] >= 60) & (df[\"bmi\"] > 22) & (df[\"bmi\"] < 38)][\"stroke\"].value_counts()","c62ba1e3":"100*df[(df[\"age\"] >= 60) & (df[\"bmi\"] > 22) & (df[\"bmi\"] < 38)][\"stroke\"].value_counts()[1]\/sum(df[(df[\"age\"] >= 60) & (df[\"bmi\"] > 22) & (df[\"bmi\"] < 38)][\"stroke\"].value_counts())","e4a36acd":"sns.jointplot(x=\"age\", y=\"heart_disease\", data=df, hue=\"stroke\", palette=\"coolwarm\", kind=\"kde\")","da2ee434":"df[(df[\"age\"] >= 70) & (df[\"heart_disease\"] ==0)][\"stroke\"].value_counts()","eeaff748":"100*df[(df[\"age\"] >= 70) & (df[\"heart_disease\"] ==0)][\"stroke\"].value_counts()[1]\/sum(df[(df[\"age\"] >= 70) & (df[\"heart_disease\"] ==0)][\"stroke\"].value_counts())","5f6d5a40":"df.head()","f7980f30":"df[\"gender\"].value_counts()","ebb1e378":"x=[\"Female\",\"Male\"]\n_=plt.title(\"Gender and Strokes\")\n_=plt.bar(x, height = df[(df[\"stroke\"]==0) & (df[\"gender\"]!=\"Other\")][\"gender\"].value_counts(), color=\"blue\")\n_=plt.bar(x, height = df[(df[\"stroke\"]==1) & (df[\"gender\"]!=\"Other\")][\"gender\"].value_counts(), color=\"red\")","48c83f90":"male_strokes = df[df[\"gender\"]==\"Male\"][\"stroke\"].value_counts()\nprint(\"The percentage of males with strokes is {:.1f}%\".format(100*male_strokes[1]\/sum(male_strokes)))\nfemale_strokes =df[df[\"gender\"]==\"Female\"][\"stroke\"].value_counts()\nprint(\"The percentage of females with strokes is {:.1f}%\".format(100*female_strokes[1]\/sum(female_strokes)))","bf3f9152":"df[\"ever_married\"].value_counts()","4a3ede57":"x=[\"Yes\",\"No\"]\n_=plt.title(\"Marrige and Strokes\")\n_=plt.bar(x, height = df[(df[\"stroke\"]==0)][\"ever_married\"].value_counts(), color=\"blue\")\n_=plt.bar(x, height = df[(df[\"stroke\"]==1)][\"ever_married\"].value_counts(), color=\"red\")","e9552256":"married_strokes = df[df[\"ever_married\"]==\"Yes\"][\"stroke\"].value_counts()\nprint(\"The percentage of Married with strokes is {:.1f}%\".format(100*married_strokes[1]\/sum(married_strokes)))\nnot_married_strokes = df[df[\"ever_married\"]==\"No\"][\"stroke\"].value_counts()\nprint(\"The percentage of Not Married with strokes is {:.1f}%\".format(100*not_married_strokes[1]\/sum(not_married_strokes)))","d35b0179":"x=[\"Gov Job\", \"Private\", \"Self-employed\", \"Children\"]\nplt.figure(figsize=(10,5))\n_=plt.title(\"Strokes for each Work Type\")\n_=plt.bar(x, height = df[(df[\"stroke\"]==0) & (df[\"work_type\"]!=\"Never_worked\")][\"work_type\"].value_counts().sort_index(), color=\"blue\")\n_=plt.bar(x, height = df[(df[\"stroke\"]==1)][\"work_type\"].value_counts().sort_index(), color=\"red\")","8b243bc7":"private_strokes = df[df[\"work_type\"]==\"Private\"][\"stroke\"].value_counts()\nprint(\"The percentage of (work type = Private) with strokes is {:.1f}%\".format(100*private_strokes[1]\/sum(private_strokes)))\nchil_strokes = df[df[\"work_type\"]==\"children\"][\"stroke\"].value_counts()\nprint(\"The percentage of (work type = Children) with strokes is {:.1f}%\".format(100*chil_strokes[1]\/sum(chil_strokes)))\nself_strokes = df[df[\"work_type\"]==\"Self-employed\"][\"stroke\"].value_counts()\nprint(\"The percentage of (work type = self employed) with strokes is {:.1f}%\".format(100*self_strokes[1]\/sum(self_strokes)))\ngov_strokes = df[df[\"work_type\"]==\"Govt_job\"][\"stroke\"].value_counts()\nprint(\"The percentage of (work type = Govt_job) with strokes is {:.1f}%\".format(100*gov_strokes[1]\/sum(gov_strokes)))","ee86d091":"high_strokes = df[(df[\"work_type\"]!=\"children\") | (df[\"work_type\"]!=\"Never_worked\")][\"stroke\"].value_counts()\nprint(\"The percentage of (work type = high pressure) with strokes is {:.1f}%\".format(100*high_strokes[1]\/sum(high_strokes)))\nlow_strokes = df[(df[\"work_type\"]==\"children\") | (df[\"work_type\"]==\"Never_worked\")][\"stroke\"].value_counts()\nprint(\"The percentage of (work type = low pressure) with strokes is {:.1f}%\".format(100*low_strokes[1]\/sum(low_strokes)))","24dcf6b4":"df[\"Residence_type\"].value_counts()","11ff2ce0":"x=[\"Urban\",\"Rural\"]\nplt.figure(figsize=(10,5))\n_=plt.title(\"Strokes for Residence Type\")\n_=plt.bar(x, height = df[(df[\"stroke\"]==0)][\"Residence_type\"].value_counts(), color=\"blue\")\n_=plt.bar(x, height = df[(df[\"stroke\"]==1)][\"Residence_type\"].value_counts(), color=\"red\")","508f24b3":"Urban_strokes = df[df[\"Residence_type\"]==\"Urban\"][\"stroke\"].value_counts()\nprint(\"The percentage of (Residence = Urban) with strokes is {:.1f}%\".format(100*Urban_strokes[1]\/sum(Urban_strokes)))\nUrban_strokes = df[df[\"Residence_type\"]==\"Rural\"][\"stroke\"].value_counts()\nprint(\"The percentage of (Residence = Rural) with strokes is {:.1f}%\".format(100*Urban_strokes[1]\/sum(Urban_strokes)))","6d444767":"df[\"smoking_status\"].value_counts().sort_index()","6792c956":"x=[\"Unknown\", \"formerly smoked\", \"never smoked\", \"smokes\"]\nplt.figure(figsize=(10,5))\n_=plt.title(\"Strokes for Smokers\/Non-smokers\")\n_=plt.bar(x, height = df[df[\"stroke\"]==0][\"smoking_status\"].value_counts().sort_index(), color=\"blue\")\n_=plt.bar(x, height = df[df[\"stroke\"]==1][\"smoking_status\"].value_counts().sort_index(), color=\"red\")","92ab0140":"strokes = df[df[\"smoking_status\"]==\"Unknown\"][\"stroke\"].value_counts()\nprint(\"The percentage of (smoked = unknown) with strokes is {:.1f}%\".format(100*strokes[1]\/sum(strokes)))\n\nstrokes = df[df[\"smoking_status\"]==\"formerly smoked\"][\"stroke\"].value_counts()\nprint(\"The percentage of (smoked = formerly) with strokes is {:.1f}%\".format(100*strokes[1]\/sum(strokes)))\n\nstrokes = df[df[\"smoking_status\"]==\"never smoked\"][\"stroke\"].value_counts()\nprint(\"The percentage of (smoked = never) with strokes is {:.1f}%\".format(100*strokes[1]\/sum(strokes)))\n\nstrokes = df[df[\"smoking_status\"]==\"smokes\"][\"stroke\"].value_counts()\nprint(\"The percentage of (smoked = smokes) with strokes is {:.1f}%\".format(100*strokes[1]\/sum(strokes)))","4290de79":"strokes = df[df[\"smoking_status\"]!=\"Unknown\"][\"stroke\"].value_counts()\nprint(\"The percentage of (smoked = known) with strokes is {:.1f}%\".format(100*strokes[1]\/sum(strokes)))\n\nstrokes = df[df[\"smoking_status\"]==\"Unknown\"][\"stroke\"].value_counts()\nprint(\"The percentage of (smoked = unknown) with strokes is {:.1f}%\".format(100*strokes[1]\/sum(strokes)))","24eb1373":"df.head()","514f90e5":"df[\"ever_married\"] = df[\"ever_married\"].apply(lambda x: 1 if x==\"Yes\" else 0)","3b130171":"def age_bmi(x):\n    if x[0] >= 60 and x[1] >= 22 and x[1] <= 38:\n        return 1\n    else:\n        return 0\n\ndf[\"age bmi\"] = df[[\"age\", \"bmi\"]].apply(lambda x: age_bmi(x), axis=1)","dbfc06f6":"def age_heart(x):\n    if x[0] >= 70 and x[1] == 1:\n        return 1\n    else:\n        return 0\n\ndf[\"over 70 no heart disease\"] = df[[\"age\", \"heart_disease\"]].apply(lambda x: age_heart(x), axis=1)","2499e0be":"def employment(x):\n    if x == \"Private\" or x == \"Self-employed\" or x == \"Govt_job\":\n        return 1\n    else:\n        return 0 \n\ndf[\"job type\"] = df[\"work_type\"].apply(lambda x: employment(x))\n# remove work type featrue.\ndf.drop(labels=\"work_type\", axis=1, inplace = True)","892f215b":"df[\"smoking habbit\"] = df[\"smoking_status\"].apply(lambda x: 1 if x != \"Unknown\" else 0)\n# remove work type featrue.\ndf.drop(labels=\"smoking_status\", axis=1, inplace = True)","dd6d5e96":"df[\"gender\"] = df[\"gender\"].apply(lambda x: 1 if x == \"Male\" else 0)\ndf[\"Residence_type\"] = df[\"Residence_type\"].apply(lambda x: 1 if x == \"Urban\" else 0)","d24ccb56":"# 1 always has increase stroke chance except for gender and residence_type\ndf.head()","94eeb4d6":"df[(df[\"age\"] < 30) & (df[\"stroke\"]==1)]","3e19e212":"_=sns.boxplot(y=\"age\", x=\"stroke\", data=df)","292bf10c":"# how to find the different quartiles for the data.\ndef qaurtiles(data):\n    q25, q50, q75 = np.percentile(data, [25, 50, 75])\n    iqr = q75 - q25\n\n    q0 = q25 - 1.5*iqr\n    q100 = q75 + 1.5*iqr\n    \n    return q0, q25, q50, q75, q100","5a9072a8":"# finds the data points that lie outside of our quartiles.\ndata = df[df[\"stroke\"]==1][\"age\"]\n[x for x in data if x < qaurtiles(data)[0]]","1ca9f48e":"# code below removes outliers from the age stroke dataset:\nthreshold = qaurtiles(data)[0]\n\ndef filter_age(x):\n    if x[1] == 1 and x[0] < threshold:\n        x[0] = threshold\n        return x\n    else:\n        return x\n    \n# create apply threshold to data\nage_thresh = df[[\"age\",\"stroke\"]].apply(lambda x: filter_age(x), axis=1)\nage_thresh_df = pd.DataFrame(age_thresh)\nage_thresh_df[\"stroke\"] = df[\"stroke\"]\nage_thresh_df = age_thresh_df.rename(columns={0:\"bmi\"})\n\n# visualize the new distrabution\n_=plt.title(\"Age vs Stroke Outliers Removed\")\n_=sns.boxplot(y=\"age\", x=\"stroke\", data=age_thresh_df)","09a8354d":"_=sns.boxplot(y=\"avg_glucose_level\", x=\"stroke\", data=df)","daa4fd9e":"data = df[df[\"stroke\"]==0][\"avg_glucose_level\"]\n[x for x in data if x > qaurtiles(data)[-1]][0:5]","6f793f3a":"threshold = qaurtiles(data)[-1]\n\ndef filter_glu(x):\n    if x[1] == 0 and x[0] > threshold:\n        x[0] = threshold\n        return x\n    else:\n        return x\n    \n# create apply threshold to data\nglu_thresh = df[[\"avg_glucose_level\",\"stroke\"]].apply(lambda x: filter_glu(x), axis=1)\nglu_thresh_df = pd.DataFrame(glu_thresh)\nglu_thresh_df[\"stroke\"] = df[\"stroke\"]\nglu_thresh_df = glu_thresh_df.rename(columns={0:\"bmi\"})\n\n# visualize the new distrabution\n_=plt.title(\"Glucose vc Stroke Outliers Removed\")\n_=sns.boxplot(y=\"avg_glucose_level\", x=\"stroke\", data=glu_thresh_df)","f1815a18":"_=sns.boxplot(y=\"bmi\", x=\"stroke\", data=df)","427e3020":"data = df[df[\"stroke\"]==0][\"bmi\"]\nprint(\"bmi no-stroke outliers: {}\".format([x for x in data if x > qaurtiles(data)[-1]][0:5]))\n\ndata = df[df[\"stroke\"]==1][\"bmi\"]\nprint(\"bmi stroke outliers: {}\".format([x for x in data if x > qaurtiles(data)[-1]][0:5]))  ","79813f7e":"# remove outliers for stroke = 0 BMI:\ndata_no_stroke = df[df[\"stroke\"]==0][\"bmi\"]\ndata_stroke = df[df[\"stroke\"]==1][\"bmi\"]\n\nthreshold = qaurtiles(data_no_stroke)[-1]\nthreshold_up = qaurtiles(data_stroke)[-1]\nthreshold_low = qaurtiles(data_stroke)[0]\n\ndef filter_bmi_noStroke(x):\n    if x[1] == 0 and x[0] > threshold:\n        x[0] = threshold\n        return x\n    else:\n        return x\n\ndef filter_bmi_Stroke(x):\n    if x[1] == 1 and x[0] > threshold_up:\n        x[0] = threshold_up\n        return x\n    elif x[1] == 1 and x[0] < threshold_low:\n        x[0] = threshold_low\n        return x\n    else:\n        return x\n    \n# create apply threshold to data\nbmi_thresh = df[[\"bmi\",\"stroke\"]].apply(lambda x: filter_bmi_noStroke(x), axis=1)\nbmi_thresh = bmi_thresh.apply(lambda x: filter_bmi_Stroke(x), axis=1)\nbmi_thresh_df = pd.DataFrame(bmi_thresh)\nbmi_thresh_df[\"stroke\"] = df[\"stroke\"]\nbmi_thresh_df = bmi_thresh_df.rename(columns={0:\"bmi\"})\n\n# visualize the new distrabution\n_=plt.title(\"BMI vs Stroke Outliers Removed\")\n_=sns.boxplot(y=\"bmi\", x=\"stroke\", data=bmi_thresh_df)","3edb57cc":"data = df[df[\"stroke\"]==1][\"age\"]\nthreshold = qaurtiles(data)[0]\ndf[\"age\"] = df[[\"age\",\"stroke\"]].apply(lambda x: filter_age(x), axis=1)\n\ndata = df[df[\"stroke\"]==0][\"avg_glucose_level\"]\nthreshold = qaurtiles(data)[-1]\ndf[\"avg_glucose_level\"] = df[[\"avg_glucose_level\",\"stroke\"]].apply(lambda x: filter_glu(x), axis=1)\n\ndata_no_stroke = df[df[\"stroke\"]==0][\"bmi\"]\ndata_stroke = df[df[\"stroke\"]==1][\"bmi\"]\nthreshold = qaurtiles(data_no_stroke)[-1]\nthreshold_up = qaurtiles(data_stroke)[-1]\nthreshold_low = qaurtiles(data_stroke)[0]\ndf[\"bmi\"] = df[[\"bmi\",\"stroke\"]].apply(lambda x: filter_bmi_noStroke(x), axis=1)\ndf[\"bmi\"] = df[[\"bmi\",\"stroke\"]].apply(lambda x: filter_bmi_Stroke(x), axis=1)","26bc6a77":"# The below code takes any numerical column that has a skew > 0.75 and transforms it using a log fucntion\n\nskew_limit = 0.75\nskew_values = df.skew()\nskew_cols = skew_values[abs(skew_values)>skew_limit].sort_values(ascending=False)\n\nfor col in skew_cols.index:\n    if col == \"stroke\":\n        continue\n    df[col] = df[col].apply(np.log1p)","6ffa798b":"X = df.drop(labels=\"stroke\",axis=1)\nY = df[\"stroke\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=42)\n\n# scaling for a decision tree might not have much impact, as it does not calculate any euclidean distance.\n# however, this will be useful for some of the unsupervised learning methods like k-mean-clusters.\nscaler = MinMaxScaler().fit(X_train)\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# loop throught some random states and append the feature importance to a dataframe.\ndf_decision_tree = pd.DataFrame()\nfor i in range(0,10):\n    Dtree = DecisionTreeClassifier(random_state=i)\n    model = Dtree.fit(X_train_scaled, y_train)\n    importance = model.feature_importances_\n    importance_labels = X.columns\n\n    import_series = pd.Series(data=importance, index=importance_labels).sort_values(ascending=False)\n    df_decision_tree[i] = import_series\n\n# refind the dataframe to a series of the mean results from the for loop above\nmean_feature = df_decision_tree.T.describe().loc[\"mean\"].sort_values(ascending=False)\n_=plt.figure(figsize=(10,6))\n_=plt.bar(x=mean_feature.index, height = mean_feature.values)\n_=plt.xticks(ticks=range(0,len(list(mean_feature.index))), labels = mean_feature.index, rotation='60')\n_=plt.ylabel(\"Importance\")\n_=plt.title(\"Decision Tree Feature Importance for Strokes: Random state: {}\".format(i))","9da2a672":"GXDtree = GradientBoostingClassifier()\nGXmodel = GXDtree.fit(X_train_scaled, y_train)\nGXimportance = GXmodel.feature_importances_\nGXimportance_labels = X.columns\nGXimport_series = pd.Series(data=GXimportance, index=GXimportance_labels).sort_values(ascending=False)\n\n_=plt.figure(figsize=(10,6))\n_=plt.bar(x=GXimport_series.index, height = GXimport_series.values)\n_=plt.xticks(ticks=range(0,len(list(GXimport_series.index))), labels = GXimport_series.index, rotation='60')\n_=plt.ylabel(\"Importance\")\n_=plt.title(\"Gradient Boosted Decision Tree Feature Importance for Strokes\")","12e85855":"df.stroke.value_counts()","076570a7":"# try using train_test_split\nX = df.drop(\"stroke\",axis=1)\nY = df[\"stroke\"]\n\n# train is 60%, test is 20% and validation is 20%\nX_train, X_test_validation, y_train, y_test_validation = train_test_split(X, Y, test_size=0.4, random_state=13)\nX_test, X_validation, y_test, y_validation = train_test_split(X_test_validation, y_test_validation, test_size=0.5, random_state=13)\n\n# SMOTE to balance the minority class, THIS SHOULD ONLY BE APPLIED TO TRAINING DATA!\nsm = SMOTE(random_state=1)\nX_train, y_train = sm.fit_resample(X_train, y_train)\n\n# scale data\nscaler = MinMaxScaler().fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)\nX_validation = scaler.transform(X_validation)","bac3f629":"def evaluate(model, test=False, train=False, validation=False):\n    # returns train results    \n    if train == True:\n        name = model\n        pred = model.predict(X_train)\n        confuction = confusion_matrix(y_train, pred)\n        print(\"-----------------------TRAINING SCORES-----------------------\")\n        print(\"\")\n        print(confuction)\n        print(str(model)+\" accuracy score: {:.2f}\".format(accuracy_score(y_train, pred)))\n        print(str(model)+\" precision score: {:.2f}\".format(precision_score(y_train, pred)))\n        print(str(model)+\" recall score: {:.2f}\".format(recall_score(y_train, pred)))\n        print(str(model)+\" f1 (beta = 4) score: {:.2f}\".format(fbeta_score(y_train, pred, beta = 4)))\n        print(\"\")\n        \n    # returns test results\n    if test == True:\n        name = model\n        pred = model.predict(X_test)\n        confuction = confusion_matrix(y_test, pred)\n        print(\"-----------------------TESTING SCORES-----------------------\")\n        print(\"\")\n        print(confuction)\n        print(str(model)+\" accuracy score: {:.2f}\".format(accuracy_score(y_test, pred)))\n        print(str(model)+\" precision score: {:.2f}\".format(precision_score(y_test, pred)))\n        print(str(model)+\" recall score: {:.2f}\".format(recall_score(y_test, pred)))\n        print(str(model)+\" f1 (beta = 4) score: {:.2f}\".format(fbeta_score(y_test, pred, beta = 4)))\n        print(\"\")\n        \n    # completely unseen data\n    if validation == True:\n        name = model\n        pred = model.predict(X_validation)\n        confuction = confusion_matrix(y_validation, pred)\n        print(\"-----------------------VALIDATION SCORES-----------------------\")\n        print(\"\")\n        print(confuction)\n        print(str(model)+\" accuracy score: {:.2f}\".format(accuracy_score(y_validation, pred)))\n        print(str(model)+\" precision score: {:.2f}\".format(precision_score(y_validation, pred)))\n        print(str(model)+\" recall score: {:.2f}\".format(recall_score(y_validation, pred)))\n        print(str(model)+\" f1 (beta = 4) score: {:.2f}\".format(fbeta_score(y_validation, pred, beta = 4)))","7b6b1f87":"dummy = DummyClassifier(strategy=\"stratified\").fit(X_train, y_train)\nevaluate(dummy, test = True, train= True, validation=True)","5825cf0f":"lr = LogisticRegression(max_iter=300).fit(X_train,y_train) # 50\/50\nevaluate(lr, test = True, train= True)","8827e763":"svc = SVC().fit(X_train, y_train)\nevaluate(svc, test = True, train= True)","bb9a8d54":"rf = RandomForestClassifier().fit(X_train,y_train)\nevaluate(rf, test = True, train= True)","02112ff3":"gbdt = GradientBoostingClassifier().fit(X_train, y_train)\nevaluate(gbdt, test = True, train= True)","a8daf406":"knc = KNeighborsClassifier().fit(X_train, y_train)\nevaluate(knc, test = True, train= True)","40b7d778":"nbc = GaussianNB().fit(X_train, y_train)\nevaluate(nbc, test = True, train= True)","64808c8a":"dtree = DecisionTreeClassifier().fit(X_train, y_train)\nevaluate(dtree, test = True, train= True)","9aefbb6e":"# Using this for CV\n\ndef reshuffle():\n    # train is 60%, test is 20% and validation is 20% (no random_state to allow for shuffling)\n    X_train, X_test_validation, y_train, y_test_validation = train_test_split(X, Y, test_size=0.4)\n    X_test, X_validation, y_test, y_validation = train_test_split(X_test_validation, y_test_validation, test_size=0.5, random_state=13)\n\n    # SMOTE to balance the minority class, THIS SHOULD ONLY BE APPLIED TO TRAINING DATA!\n    sm = SMOTE()\n    X_train, y_train = sm.fit_resample(X_train, y_train)\n\n    # scale data\n    scaler = MinMaxScaler().fit(X_train)\n    X_train = scaler.transform(X_train)\n    X_test = scaler.transform(X_test)\n    X_validation = scaler.transform(X_validation)\n    \n    return X_train, y_train, X_test, y_test","bde9f6c5":"# hyperparameters im interested in using\nn_trees = [10,20,50,100,300,600]\ncriterion = [\"gini\", \"entropy\"]\ndepth = [3,5,8,10]\n\n# create a list of dictionaries to loop through\ngrid = []\n\n# populate the grid with hyperparameters\nfor n in n_trees:\n    for c in criterion:\n        for d in depth:\n            grid_dic = {}\n            grid_dic[\"n_estimators\"] = n\n            grid_dic[\"criterion\"] = c\n            grid_dic[\"max_depth\"] = d\n            # append to grid list    \n            grid.append(grid_dic)\n\n# creat dictionaries for the train and test scores\ntrain_cache = {}\ntest_cache = {}\n\n# create lists to populate the train and test dictionaries\naccuarcy_train = []\naccuarcy_test = []\nprecision_train = []\nprecision_test = []\nrecall_train = []\nrecall_test = []\nf1_train = []\nf1_test = []\n\n# loop through the grid to get scores for all hyperparameter combinations\nfor i in grid:\n    \n    # create lists for cross validation\n    accuarcy_train_mean = []\n    accuarcy_test_mean = []\n    precision_train_mean = []\n    precision_test_mean = []\n    recall_train_mean = []\n    recall_test_mean = []\n    f1_train_mean = []\n    f1_test_mean = []\n    \n    # perform cross validation\n    for kfolds in range(1,6):\n        # reshuffle data to allow for CV\n        X_train, y_train, X_test, y_test = reshuffle()\n        RS = RandomForestClassifier(n_estimators=i[\"n_estimators\"], max_depth=i[\"max_depth\"], criterion=i[\"criterion\"], n_jobs=-2).fit(X_train, y_train)\n    \n        pred_train = RS.predict(X_train)\n        pred_test = RS.predict(X_test)\n\n        accuarcy_train_mean.append(accuracy_score(y_train, pred_train))\n        precision_train_mean.append(precision_score(y_train, pred_train))\n        recall_train_mean.append(recall_score(y_train, pred_train))\n        f1_train_mean.append(fbeta_score(y_train, pred_train, beta = 3))\n\n        accuarcy_test_mean.append(accuracy_score(y_test, pred_test))\n        precision_test_mean.append(precision_score(y_test, pred_test))\n        recall_test_mean.append(recall_score(y_test, pred_test))\n        f1_test_mean.append(fbeta_score(y_test, pred_test, beta = 3))\n    \n    # append the mean of the CV values\n    accuarcy_train.append(np.mean(accuarcy_train_mean))\n    precision_train.append(np.mean(precision_train_mean))\n    recall_train.append(np.mean(recall_train_mean))\n    f1_train.append(np.mean(f1_train_mean))\n    \n    accuarcy_test.append(np.mean(accuarcy_test_mean))\n    precision_test.append(np.mean(precision_test_mean))\n    recall_test.append(np.mean(recall_test_mean))\n    f1_test.append(np.mean(f1_test_mean))\n    \n    \n# populate the dictionaries with all the scores from the above hyperparameters    \ntrain_cache[\"accurary\"] = accuarcy_train\ntrain_cache[\"precision\"] = precision_train\ntrain_cache[\"recall\"] = recall_train\ntrain_cache[\"f1(beta=4)\"] = f1_train\ntrain_cache[\"grid\"] = grid\n\ntest_cache[\"accurary\"] = accuarcy_test\ntest_cache[\"precision\"] = precision_test\ntest_cache[\"recall\"] = recall_test\ntest_cache[\"f1(beta=4)\"] = f1_test\ntest_cache[\"grid\"] = grid\n\n# create dataframe to easily work with these scores\ntrain_df_rf = pd.DataFrame(train_cache).set_index(\"grid\")\ntest_df_rf = pd.DataFrame(test_cache).set_index(\"grid\")\n\ncomp_rf = pd.DataFrame(test_df_rf).sort_values(\"f1(beta=4)\", ascending=False)\ncomp_rf.head(5)","b3bb685d":"RF_best = RandomForestClassifier(n_estimators=comp_rf.index[0][\"n_estimators\"], criterion=comp_rf.index[0][\"criterion\"], max_depth=comp_rf.index[0][\"max_depth\"]).fit(X_train, y_train)\nevaluate(RF_best, test=True, train=True, validation=True)","90b92d9c":"# hyperparameters im interested in using\nC = [0.01, 0.1, 0.5, 0.1, 2, 5, 10, 50, 100]\npenalty = [\"l2\"]\n\n# create a list of dictionaries to loop through\ngrid = []\n\n# populate the grid with hyperparameters\nfor p in penalty:\n    for c in C:\n        grid_dic = {}\n        grid_dic[\"penalty\"] = p\n        grid_dic[\"C\"] = c\n        # append to grid list\n        grid.append(grid_dic)\n\n# creat dictionaries for the train and test scores\ntrain_cache = {}\ntest_cache = {}\n\n# create lists to populate the train and test dictionaries\naccuarcy_train = []\naccuarcy_test = []\nprecision_train = []\nprecision_test = []\nrecall_train = []\nrecall_test = []\nf1_train = []\nf1_test = []\n\n# loop through the grid to get scores for all hyperparameter combinations\nfor i in grid:\n    \n    # create lists for cross validation\n    accuarcy_train_mean = []\n    accuarcy_test_mean = []\n    precision_train_mean = []\n    precision_test_mean = []\n    recall_train_mean = []\n    recall_test_mean = []\n    f1_train_mean = []\n    f1_test_mean = []\n    \n    # perform cross validation\n    for kfolds in range(1,6):\n        # reshuffle data to allow for CV\n        X_train, y_train, X_test, y_test = reshuffle()\n        LR = LogisticRegression(penalty=i[\"penalty\"], C=i['C'], max_iter=1000).fit(X_train, y_train)\n    \n        pred_train = LR.predict(X_train)\n        pred_test = LR.predict(X_test)\n\n        accuarcy_train_mean.append(accuracy_score(y_train, pred_train))\n        precision_train_mean.append(precision_score(y_train, pred_train))\n        recall_train_mean.append(recall_score(y_train, pred_train))\n        f1_train_mean.append(fbeta_score(y_train, pred_train, beta = 3))\n\n        accuarcy_test_mean.append(accuracy_score(y_test, pred_test))\n        precision_test_mean.append(precision_score(y_test, pred_test))\n        recall_test_mean.append(recall_score(y_test, pred_test))\n        f1_test_mean.append(fbeta_score(y_test, pred_test, beta = 3))\n    \n    # append the mean of the CV values\n    accuarcy_train.append(np.mean(accuarcy_train_mean))\n    precision_train.append(np.mean(precision_train_mean))\n    recall_train.append(np.mean(recall_train_mean))\n    f1_train.append(np.mean(f1_train_mean))\n    \n    accuarcy_test.append(np.mean(accuarcy_test_mean))\n    precision_test.append(np.mean(precision_test_mean))\n    recall_test.append(np.mean(recall_test_mean))\n    f1_test.append(np.mean(f1_test_mean))\n    \n# populate the dictionaries with all the scores from the above hyperparameters    \ntrain_cache[\"accurary\"] = accuarcy_train\ntrain_cache[\"precision\"] = precision_train\ntrain_cache[\"recall\"] = recall_train\ntrain_cache[\"f1(beta=4)\"] = f1_train\ntrain_cache[\"grid\"] = grid\n\ntest_cache[\"accurary\"] = accuarcy_test\ntest_cache[\"precision\"] = precision_test\ntest_cache[\"recall\"] = recall_test\ntest_cache[\"f1(beta=4)\"] = f1_test\ntest_cache[\"grid\"] = grid\n\n# create dataframe to easily work with these scores\ntrain_df_lr = pd.DataFrame(train_cache).set_index(\"grid\")\ntest_df_lr = pd.DataFrame(test_cache).set_index(\"grid\")\n\n# find the hyperparamters with the smallest errors\ncomp_lr = pd.DataFrame(test_df_lr).sort_values(\"f1(beta=4)\", ascending=False)\ncomp_lr.head(5)","610e9838":"LR_best_model = LogisticRegression(C=comp_lr.index[0][\"C\"]).fit(X_train, y_train)\nevaluate(LR_best_model, test=True, train=True, validation=True)","4af2822b":"C_range = [0.01, 100]\nC_start = np.log10(min(C_range))\nC_end = np.log10(max(C_range))\nr_large = C_end*np.random.rand(4)\nr_small = C_start*np.random.rand(4)\nr = np.concatenate((r_small,r_large))\n\nC = [round(x,2) for x in sorted(np.power(10,r))]\nkernel = [\"poly\", \"rbf\", \"sigmoid\"]\ngamma = [\"scale\", \"auto\"]\ngrid = []\n\nfor k in kernel:\n    for g in gamma:\n        for c in C:\n            grid_dic = {}\n            grid_dic[\"kernel\"] = k\n            grid_dic[\"gamma\"] = g\n            grid_dic[\"C\"] = c\n\n            grid.append(grid_dic)\n\ntrain_cache = {}\ntest_cache = {}\n\naccuarcy_train = []\naccuarcy_test = []\nprecision_train = []\nprecision_test = []\nrecall_train = []\nrecall_test = []\nf1_train = []\nf1_test = []\n    \n# loop through the grid to get scores for all hyperparameter combinations\nfor i in grid:\n    \n    # create lists for cross validation\n    accuarcy_train_mean = []\n    accuarcy_test_mean = []\n    precision_train_mean = []\n    precision_test_mean = []\n    recall_train_mean = []\n    recall_test_mean = []\n    f1_train_mean = []\n    f1_test_mean = []\n    \n    # perform cross validation\n    for kfolds in range(1,6):\n        # reshuffle data to allow for CV\n        X_train, y_train, X_test, y_test = reshuffle()\n        svc = SVC(kernel=i[\"kernel\"], C=i['C'], gamma=i[\"gamma\"]).fit(X_train, y_train)\n    \n        pred_train = svc.predict(X_train)\n        pred_test = svc.predict(X_test)\n\n        accuarcy_train_mean.append(accuracy_score(y_train, pred_train))\n        precision_train_mean.append(precision_score(y_train, pred_train))\n        recall_train_mean.append(recall_score(y_train, pred_train))\n        f1_train_mean.append(fbeta_score(y_train, pred_train, beta = 3))\n\n        accuarcy_test_mean.append(accuracy_score(y_test, pred_test))\n        precision_test_mean.append(precision_score(y_test, pred_test))\n        recall_test_mean.append(recall_score(y_test, pred_test))\n        f1_test_mean.append(fbeta_score(y_test, pred_test, beta = 3))\n    \n    # append the mean of the CV values\n    accuarcy_train.append(np.mean(accuarcy_train_mean))\n    precision_train.append(np.mean(precision_train_mean))\n    recall_train.append(np.mean(recall_train_mean))\n    f1_train.append(np.mean(f1_train_mean))\n    \n    accuarcy_test.append(np.mean(accuarcy_test_mean))\n    precision_test.append(np.mean(precision_test_mean))\n    recall_test.append(np.mean(recall_test_mean))\n    f1_test.append(np.mean(f1_test_mean))\n    \ntrain_cache[\"accurary\"] = accuarcy_train\ntrain_cache[\"precision\"] = precision_train\ntrain_cache[\"recall\"] = recall_train\ntrain_cache[\"f1(beta=4)\"] = f1_train\ntrain_cache[\"grid\"] = grid\n\ntest_cache[\"accurary\"] = accuarcy_test\ntest_cache[\"precision\"] = precision_test\ntest_cache[\"recall\"] = recall_test\ntest_cache[\"f1(beta=4)\"] = f1_test\ntest_cache[\"grid\"] = grid\n\n# create dataframe to easily work with these scores\ntrain_df_svc = pd.DataFrame(train_cache).set_index(\"grid\")\ntest_df_svc = pd.DataFrame(test_cache).set_index(\"grid\")\n\n# find the hyperparamters with the smallest errors\ncomp_svc = pd.DataFrame(test_df_svc).sort_values(\"f1(beta=4)\", ascending=False)\ncomp_svc.head(5)","cccc6973":"svc_best_model = SVC(kernel=comp_svc.index[0][\"kernel\"], C= comp_svc.index[0][\"C\"], gamma=comp_svc.index[0][\"gamma\"]).fit(X_train, y_train)\nevaluate(svc_best_model, test=True, train=True, validation=True)","abdb551c":"# hyperparameters im interested in using\nlearning_rate = [0.001, 0.01, 0.1, 0.2]\nn_trees = [10,20,50,100,300]\ndepth = [3,5,8]\n\n# create a list of dictionaries to loop through\ngrid = []\n\n# populate the grid with hyperparameters\nfor n in n_trees:\n    for l in learning_rate:\n        for d in depth:\n            grid_dic = {}\n            grid_dic[\"n_estimators\"] = n\n            grid_dic[\"learning_rate\"] = l\n            grid_dic[\"max_depth\"] = d\n            # append to grid list    \n            grid.append(grid_dic)\n\n# creat dictionaries for the train and test scores\ntrain_cache = {}\ntest_cache = {}\n\n# create lists to populate the train and test dictionaries\naccuarcy_train = []\naccuarcy_test = []\nprecision_train = []\nprecision_test = []\nrecall_train = []\nrecall_test = []\nf1_train = []\nf1_test = []\n\n# loop through the grid to get scores for all hyperparameter combinations\nfor i in grid:\n    \n    # create lists for cross validation\n    accuarcy_train_mean = []\n    accuarcy_test_mean = []\n    precision_train_mean = []\n    precision_test_mean = []\n    recall_train_mean = []\n    recall_test_mean = []\n    f1_train_mean = []\n    f1_test_mean = []\n    \n    # perform cross validation\n    for kfolds in range(1,6):\n        # reshuffle data to allow for CV\n        X_train, y_train, X_test, y_test = reshuffle()\n        BG = GradientBoostingClassifier(n_estimators=i[\"n_estimators\"], max_depth=i[\"max_depth\"], learning_rate=i[\"learning_rate\"]).fit(X_train, y_train)\n    \n        pred_train = BG.predict(X_train)\n        pred_test = BG.predict(X_test)\n\n        accuarcy_train_mean.append(accuracy_score(y_train, pred_train))\n        precision_train_mean.append(precision_score(y_train, pred_train))\n        recall_train_mean.append(recall_score(y_train, pred_train))\n        f1_train_mean.append(fbeta_score(y_train, pred_train, beta = 3))\n\n        accuarcy_test_mean.append(accuracy_score(y_test, pred_test))\n        precision_test_mean.append(precision_score(y_test, pred_test))\n        recall_test_mean.append(recall_score(y_test, pred_test))\n        f1_test_mean.append(fbeta_score(y_test, pred_test, beta = 3))\n    \n    # append the mean of the CV values\n    accuarcy_train.append(np.mean(accuarcy_train_mean))\n    precision_train.append(np.mean(precision_train_mean))\n    recall_train.append(np.mean(recall_train_mean))\n    f1_train.append(np.mean(f1_train_mean))\n    \n    accuarcy_test.append(np.mean(accuarcy_test_mean))\n    precision_test.append(np.mean(precision_test_mean))\n    recall_test.append(np.mean(recall_test_mean))\n    f1_test.append(np.mean(f1_test_mean))\n    \n    \n# populate the dictionaries with all the scores from the above hyperparameters    \ntrain_cache[\"accurary\"] = accuarcy_train\ntrain_cache[\"precision\"] = precision_train\ntrain_cache[\"recall\"] = recall_train\ntrain_cache[\"f1(beta=4)\"] = f1_train\ntrain_cache[\"grid\"] = grid\n\ntest_cache[\"accurary\"] = accuarcy_test\ntest_cache[\"precision\"] = precision_test\ntest_cache[\"recall\"] = recall_test\ntest_cache[\"f1(beta=4)\"] = f1_test\ntest_cache[\"grid\"] = grid\n\n# create dataframe to easily work with these scores\ntrain_df_gb = pd.DataFrame(train_cache).set_index(\"grid\")\ntest_df_gb = pd.DataFrame(test_cache).set_index(\"grid\")\n\ncomp_gb = pd.DataFrame(test_df_gb).sort_values(\"f1(beta=4)\", ascending=False)\ncomp_gb.head(5)","224dcbad":"GBC_best = GradientBoostingClassifier(n_estimators=comp_gb.index[0][\"n_estimators\"], learning_rate=comp_gb.index[0][\"learning_rate\"], max_depth=comp_gb.index[0][\"max_depth\"]).fit(X_train, y_train)\nevaluate(GBC_best, test=True, train=True, validation=True)","2f1d8267":"### Evaluate Helper  Function","980c02b6":"# Results:\n\nThe Gradient Boosting Classifier shows the best results on the unseen validation data with **recall = 87%, f1 = 66%, and accuracy = 73%**, the trade off here is that precision is very low. This means that the model will manage to catch 87% of patients that are going to have strokes. However, out of all the patients told they're going to have a stroke only 13% of them actually will. ","9d070fc8":"### Library imports:","316e0c3e":"**Features to Create\/Edit:**\n- age over 60, and has a bmi bewtween 22 - 38.\n- age over 70 and no heart disease (seems counter intuative, but data doesnt lie).\n- marrige as yes = 1, no = 0.\n- Self-employed\/private\/Gov_job or Children-work\/never worked\n- known and unknown smoking habits.","3a125bbd":"## Create New Features From Findings:","6ffffb8d":"**The below looks at low stress jobs and high stress jobs**","4c27063e":"### Scale skewed data to further remove outliers:","56c601b8":"- This will be used as a comparison for the final model","72b8dd45":"## Gradient Boosting Classifier Tuning","7ab015bd":"- both decision trees show that age, glucose levels, and bmi are the most importatn features.\n- the over 60 and bmi between 22-38, as well as the over 70 features seem to be more important in the xgboosted tree.","814ec6fe":"### i want to check the counts of the binary data and plot it with stroke and non-stroke","babf5a52":"- **could make a new feature if someone is over 60, and has a bmi bewtween 22 - 38**.","9a1762ee":"### Create Marrige Feature:","5a2c3d3d":"- **new feature could be to make over 70 and no heart disease?**","4c764022":"### Residency and Gender:","f11d577b":"### Create Feature for age over 60 and bmi 22-38","81fa2272":"## Logistic Regression","5dec81ae":"#### Age vs Stroke Outliers:","3ddc5b16":"### Some missing data in the BMI column","5d19b11a":"## How  to tune model with imbalanced Classes?\n\nTo accurately see how the model performs with real world data the testing and validation set should be representative of the real world data (e.g. imbalanced classes). However the training data needs to be balanced to allow for the model to better learn who will get a stroke and who wont. These different balanced classes mean that KFolds CV and GridsearchCV cant be used as these would train and test on one data set, either balanced classes (not representative of data) or imbalanced classes which would result in poor performance. \n\nWhat i've done bellow is create my own grid search and CV to allow for the model to be trained on the balanced data and then tested on the imbalanced data. \n\nNote: if i didnt use SMOTE or any other form of sampling then i could have used GridSearchCV.","9687a374":"### Age vs Heart Disease:","b3bf45a8":"**I have selected a few models that show promising results**\n- I have taken Random forests forwards as they seem to be massively over fitting the data, so perhaps a large improvement could be found using some form of regularization\n- Logistic regression scored well so this will be taken forwards\n- SVM shows good results and will be carried forwards.\n- Gradient boosted decision trees will be carried forwards.\n- Gaussian Naive Bayes shows good results, however, this is a very simple model that assumes each feature have no carry over. There is also very limited hyperparameter tuning avaliable for this model.","a55ab460":"## Pair Plots to check Clusters and Groupings","f312cf1f":"## Machine Learning Section:","08eca105":"## Outliers:","7dcd429e":"### Apply Outlier Changes to df","df3c39d3":"#### Glucose Levels vs Stroke Outliers:","cedd66d2":"- **Perhaps include feature of known and unknown smoking habits**\n- Unknown smoking habits have far less stokes than known smoking habits","dcb6ce2d":"Im assuming that we want to detect as many people who will get a stroke as possible, only then can we act to try and prevent a stroke from happening. This means we want **recall** to be as high as possible, but whenever we ask about recall we should really ask \"recall at what precision?\". If we have the maximum recall value then we are likely to have very low precision (for this dataset). Low precision would mean that we would tell many people they are going to have a stroke, but they wont. Obvisouly this would be distressing for the person so we dont want precision too low either. Luckly we have a greate eveluation metric called the **harmonic f1 score**, if beta > 1 then we are more concerned with recall and if beta < 1 we are more concerned with precision. I will set beta = 4 as stopping people from having strokes is much more important than stressing people. Changing the value of beta is the same as moving the threshold of the model.","a515520b":"- check for distribution of classes for all labels","7b570bd1":"### Known smoking habit\/ unknown smoking habit:","469d3fc2":"### Gradient Boosted Decision Trees:","8e698c23":"#### Create Random C regularization and then Scale:","9b1bcceb":"In this notebook I have built a machine learning model that is able to predict who will have a stroke in the future based on some features. ","7b43f9ba":"### Decision Trees:","58cd04e0":"## Supervised Feature Importance:","d57df3c6":"- **13.6% have a stroke in this category! with 22% of people falling into this group.**","b96646ef":"## EDA","9fec776a":"## Random Forest Hyperparameter Tuning","3a50dd63":"- 4% have a stroke in the data","72145bf9":"- looking at the bottom row we can see what features have the largest positive correlation with having a stroke","8549cf08":"- has far more females than males, and one other.\n- **males and females seem to have very little diffence in terms of strokes.**","91baf78f":"### smoking_status","b09c7574":"- regression plot shows how a linear line would be fit to the data.\n- a low bmi and high age seems to have a high correlation and the most different in terms of gradient of stroke and non-stroke.\n- could create new feature of age and bmi combined?","c19314f8":"### Employment:","0fcb91f5":"- **Could create feature of Self-employed\/private\/Gov_job or Children-work\/never worked**\n- self-employed shows the highest stroke percentage\n- Never worked and children work shows the lowest","64c41fc1":"### Over 70 and no heart disease:","be55a44d":"### Age vs BMI:","0935c3de":"### Gender:","97c50453":"- marrige seems to induce more strokes.","d639894d":"**Interestingly some of the features i thougth would divide the data better didnt, and some that i thought would have no impact has**\n\n- gender has a much larger impact than i thought\n- people over 60 and bmi between 22 and 38 has a much smaller importance (could be because of the small number of people it applies to)\n- average glucose levels had the largest importance, meaning it was probably the root node in the tree.","3027003b":"**using gradient boosted decision trees we are able to see importance of the feature with more complex behaviour**","776edf18":"### Train, Test, Validation Split","4f0ac27a":"### Null Metric Basline","e50f6705":"### Work Type","5c4b2f0e":"## Investigate Text Data:","c29ffcad":"- we can see that as age inceases so does the frequency of strokes, this is true for all other features.","9a6e7e03":"#### BMI vs Stroke Outliers:","37fa9766":"### Residence_type","92fbbc74":"### Ever Married","9950b245":"### All the people who had strokes are at the top of the dataset, i will reshuffle the data:","e59274db":"# Predicting If Someone Will have a Stroke","7d0d7e10":"### Default Models:","efdfd59e":"## Support Vector Machine Hyperparameter Tuning","44710629":"- **17.5% of people over 70 and with no heart disease had a stoke! however, only 12% of people fall into this category**."}}