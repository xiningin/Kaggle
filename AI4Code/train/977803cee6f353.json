{"cell_type":{"11012668":"code","32758b1b":"code","6ba21cae":"code","90afd6e3":"code","7e42c8bb":"code","07d78717":"code","bd0041fb":"code","54ce02c7":"code","d50a6318":"code","1afd4fb2":"code","371bb17d":"code","bb6ad843":"code","cc07f8ad":"code","9eb17faa":"code","17832850":"code","1d24a45a":"code","63a6961e":"code","7b56535d":"code","e5c18eb5":"code","265831f3":"code","e160d7fc":"code","a5c4db08":"code","35fb0bc3":"code","89fa081b":"code","3249d383":"code","b700dbc1":"markdown","7329d35c":"markdown","81b3430f":"markdown","377b8800":"markdown","c2df715a":"markdown","6183453e":"markdown","5003d30d":"markdown","5547744d":"markdown","55d0483f":"markdown","b75fadb5":"markdown","ff2200c9":"markdown","86cfd1b1":"markdown","8c9aaf54":"markdown","904f4b23":"markdown","193be32e":"markdown","724b4242":"markdown","2ab75c1f":"markdown"},"source":{"11012668":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","32758b1b":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\nimport warnings\nimport time\nimport datetime\nfrom wordcloud import WordCloud\nimport re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nfrom sklearn import model_selection\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import f1_score\nfrom sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\nfrom sklearn.model_selection import GridSearchCV,StratifiedKFold,RandomizedSearchCV\n%matplotlib inline","6ba21cae":"trainv=pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntestv=pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\ntrainv['Word Count']=trainv['text'].apply(len)\ntrainv.head()","90afd6e3":"trainv.shape","7e42c8bb":"testv.shape","07d78717":"#Missing values in train set\ntrainv.isnull().sum()","bd0041fb":"#Counting the number of events and classifying them as 1:real disaster and 0: not a disaster.\ntrainv['target'].value_counts()","54ce02c7":"#Displaying the Target Distribution\nfig, axes = plt.subplots(ncols=2, nrows=1, figsize=(18, 6), dpi=100)\nsns.countplot(trainv['target'], ax=axes[0])\naxes[1].pie(trainv['target'].value_counts(),\n            labels=['Not Disaster', 'Disaster'],\n            autopct='%1.2f%%',\n            shadow=True,\n            explode=(0.06, 0),\n            startangle=60)\nfig.suptitle('Distribution of the Tweets', fontsize=24)\nplt.show()","d50a6318":"g=sns.FacetGrid(trainv,col='target')\ng.map(plt.hist,'Word Count',bins=50)","1afd4fb2":"sns.barplot(y=trainv['keyword'].value_counts()[:30].index,x=trainv['keyword'].value_counts()[:30])","371bb17d":"disaster_tweets = trainv[trainv['target']==1]['text']\ndisaster_tweets.values[5]","bb6ad843":"non_disaster_tweets = trainv[trainv['target']==0]['text']\nnon_disaster_tweets.values[1]","cc07f8ad":"fig, (ax1) = plt.subplots(1,figsize=[20, 8])\nwordcloud1 = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(disaster_tweets))\nax1.imshow(wordcloud1)\nax1.axis('off')\nax1.set_title('Disaster Tweets',fontsize=30);","9eb17faa":"fig, (ax2) = plt.subplots(1,figsize=[20, 8])\nwordcloud2 = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(non_disaster_tweets))\nax2.imshow(wordcloud2)\nax2.axis('off')\nax2.set_title('Non Disaster Tweets',fontsize=30);","17832850":"def cleaner(text):\n   #Make text lowercase, remove text in square brackets,remove links,remove punctuation and remove words containing numbers.\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = text.lower()\n    return text\n\n# Applying the cleaning function to both test and training datasets\ntrainv['text'] = trainv['text'].apply(lambda x: cleaner(x))\ntestv['text'] = testv['text'].apply(lambda x: cleaner(x))\n\n# Let's take a look at the updated text\ntrainv['text'].head()","1d24a45a":"tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\ntrainv['text'] = trainv['text'].apply(lambda x: tokenizer.tokenize(x))\ntestv['text'] = testv['text'].apply(lambda x: tokenizer.tokenize(x))\ntrainv['text'].head()","63a6961e":"def remove_stopwords(text):\n    words = [w for w in text if w not in stopwords.words('english')]\n    return words\n\n\ntrainv['text'] = trainv['text'].apply(lambda x : remove_stopwords(x))\ntestv['text'] = testv['text'].apply(lambda x : remove_stopwords(x))\ntrainv.head()","7b56535d":"# The text format after preprocessing \ndef combiner(list_of_text):\n    text = ' '.join(list_of_text)\n    return text\n\ntrainv['text'] = trainv['text'].apply(lambda x : combiner(x))\ntestv['text'] = testv['text'].apply(lambda x : combiner(x))\ntrainv['text']\ntrainv.head()","e5c18eb5":"count_vectorizer = CountVectorizer()\ntrain_vectors = count_vectorizer.fit_transform(trainv['text'])\ntest_vectors = count_vectorizer.transform(testv[\"text\"])\n\n## Keeping only non-zero elements to preserve space \nprint(train_vectors[3].todense())","265831f3":"tfidf = TfidfVectorizer(min_df=2, max_df=0.5, ngram_range=(1, 2))\ntrain_tfidf = tfidf.fit_transform(trainv['text'])\ntest_tfidf = tfidf.transform(testv[\"text\"])","e160d7fc":"# Fitting a simple Naive Bayes on Counts\nclf_NB = MultinomialNB()\nscores = model_selection.cross_val_score(clf_NB, train_vectors, trainv[\"target\"], cv=5, scoring=\"f1\")\nscores","a5c4db08":"clf_NB.fit(train_vectors,trainv[\"target\"])","35fb0bc3":"# Fitting a simple Naive Bayes on TFIDF\nclf_NB_TFIDF = MultinomialNB()\nscores = model_selection.cross_val_score(clf_NB_TFIDF, train_tfidf, trainv[\"target\"], cv=5, scoring=\"f1\")\nscores","89fa081b":"clf_NB_TFIDF.fit(train_tfidf, trainv[\"target\"])","3249d383":"def submission(submission_file_path,model,test_vectors):\n    sample_submission = pd.read_csv(submission_file_path)\n    sample_submission[\"target\"] = model.predict(test_vectors)\n    sample_submission.to_csv(\"submission.csv\", index=False)\nsubmission_file_path = \"..\/input\/nlp-getting-started\/sample_submission.csv\"\ntest_vectors=test_tfidf\nsubmission(submission_file_path,clf_NB_TFIDF,test_vectors)","b700dbc1":"**Features of Bag of Words**\n* [CountVectorizer](https:\/\/www.kaggle.com\/matleonard\/intro-to-nlp)\n* [TfidfVectorizer](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.TfidfVectorizer.html?highlight=tfidf#sklearn.feature_extraction.text.TfidfVectorizer)","7329d35c":"# 3.Exploratoy Data Analysis\nData analysis is a very useful way to both enhance the project as well as understand the features needed to make a model.","81b3430f":"Cool WordClouds to Make This More Colourful and Interesting ..","377b8800":"# 1.Setting Up The Kernel\nImporting all necessary libraries needed for this project...","c2df715a":"# 6.Text Classification Model\nFinally we arrive at the final stage where we create a text classification model and predict using test data. ","6183453e":"# 2.Reading The Dataset..\n   Self explanatory heading","5003d30d":"Another interesting column other than target is **keyword** which can either be very **useful** or **misleading.**","5547744d":"**REMOVING STOPWORDS**","55d0483f":"**TOKENIZING**","b75fadb5":"**This Marks the end to My First NLP Project**","ff2200c9":"**DATA CLEANING**","86cfd1b1":"Exploring various aspects of the training data..\nMainly the no. of null values and the target column.","8c9aaf54":"Next we can plot a difference in word count of \"non disaster\" and \"disaster tweets","904f4b23":"The Best Model to Use in this Scenario is [**Naive Bayes.**](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.naive_bayes.MultinomialNB.html?highlight=multinomial%20nb#sklearn.naive_bayes.MultinomialNB)","193be32e":"# 4.Data **Preprocessing**\nBefore we jump knee deep into algorithms and models we need to pre-process the data to get it all in a consistent format.We need to clean, tokenize and convert our data into a matrix. Some of the basic text pre-processing techniques includes:\n\n* Make text all **lower case or uppercase** so that the algorithm does not treat the same words in different cases as different\n* **Removing Noise**  i.e everything that isn\u2019t in a standard number or letter i.e Punctuation, Numerical values, common non-sensical text (\/n)\n* Tokenization is just the term used to describe the process of converting the **normal text strings into a list of tokens** i.e words that we actually want. Sentence tokenizer can be used to find the list of sentences and Word tokenizer can be used to find the list of words in strings.\n* Stopword Removal: Sometimes, some **extremely common words** which would appear to be of little value in helping select documents matching a user **need are excluded from the vocabulary entirely**. These words are called stop words\n\nOne can get more info at[ Kaggle Courses>NLP](https:\/\/www.kaggle.com\/matleonard\/intro-to-nlp)","724b4242":"# 5.Transforming Tokens into a Vector\nAfter the initial preprocessing phase, we need to transform text into a meaningful vector (or array) of numbers. This can be done by a number of tecniques:\n\nBag of Words\nThe bag-of-words is a representation of text that describes the occurrence of words within a document. It involves two things:\n\n* A vocabulary of known words.\n* A measure of the presence of known words.\n\nOne can get more info at[ Kaggle Courses>NLP](https:\/\/www.kaggle.com\/matleonard\/text-classification)","2ab75c1f":"# **Beginner's Guide to NLP** \nMy First Project dealing with Natural Language Preprocessing.\nThe Basic gist off this project is to differentiate between legit disaster tweets and false ones."}}