{"cell_type":{"631330d4":"code","9d7fe200":"code","04018d59":"code","09ebd6f9":"code","2281bb4e":"code","bce65125":"code","571fd845":"code","5a951f1a":"code","28bfc69a":"code","eb86b42f":"code","810b5302":"code","d240b5ce":"code","22566536":"code","1c962c0c":"code","cd0a8ff5":"code","c81eab5a":"code","841ec7e2":"code","b58ae9d0":"code","a3e902d8":"code","196cda03":"code","341294c2":"code","fac5fb30":"code","87eb29ec":"code","6a3c7138":"code","5c8d55aa":"code","dd59dfda":"code","b7bc51e9":"code","bb62a5f8":"code","e2b4d9d1":"code","f80e2d8c":"code","55d4c1dc":"code","f704c606":"code","8fe6cae2":"code","c73b389e":"code","4b5aabbf":"code","08829110":"code","dc2598d0":"code","38fb0fc3":"code","f9487674":"code","9a38c096":"code","c2f9bd3f":"code","8d7edc9f":"code","a3a225d4":"code","82645fdc":"markdown","c1aad640":"markdown","a14bd485":"markdown","e757852e":"markdown","217214c1":"markdown","954c8013":"markdown","8b54904f":"markdown","949cb900":"markdown","6d26a685":"markdown","75c31f4b":"markdown","f923d938":"markdown","cf9dc59a":"markdown","40e90bfe":"markdown","34af034e":"markdown","739a6b5f":"markdown","89022647":"markdown","7fd6d934":"markdown","24e0a9df":"markdown","9e0a4f76":"markdown","2407b18e":"markdown","ec545555":"markdown","a617fc8f":"markdown","1ebf5228":"markdown","c95a06d8":"markdown","fcbebb38":"markdown","2b5e9c08":"markdown","9cb7b18c":"markdown","551227b5":"markdown","9e7104ce":"markdown","b5dfae6c":"markdown","a2ae7721":"markdown","af2d398d":"markdown","e7fcde00":"markdown","bec08573":"markdown","fca00c9d":"markdown","8dd95d99":"markdown","a9f4b946":"markdown","e87d23c8":"markdown","6b26de1a":"markdown","e10b6d93":"markdown","b0133e18":"markdown","606159df":"markdown","65a47af4":"markdown","46ae3a82":"markdown","cf2adf70":"markdown","9b555fe2":"markdown","628ad3d5":"markdown","659afffa":"markdown","9ac2c750":"markdown","a291910c":"markdown","4e7b47cd":"markdown","ef207b8a":"markdown","efa4afa9":"markdown","79447e2c":"markdown","c4f8c581":"markdown"},"source":{"631330d4":"from sklearn import __version__ as sklearn_version\nfrom distutils.version import LooseVersion\n\nif LooseVersion(sklearn_version) < LooseVersion('0.18'):\n    raise ValueError('Please use scikit-learn 0.18 or newer')","9d7fe200":"from IPython.display import Image\n%matplotlib inline\n","04018d59":"from sklearn import datasets\nimport numpy as np\n\niris = datasets.load_iris()\nX = iris.data[:, [2, 3]]\ny = iris.target\n\nprint('Class labels:', np.unique(y))","09ebd6f9":"X.shape","2281bb4e":"X[0:10,:]","bce65125":"y[0:10]","571fd845":"print(np.sum(y==0))\nprint(np.sum(y==1))\nprint(np.sum(y==2))","5a951f1a":"# Check if the first 50 values in y are from Class 0 (setosa), the next 50 from Class 1 (Versicolor) and the last 50 from Class 2 (Virginica)","28bfc69a":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=1, stratify=y)","eb86b42f":"print('Labels counts in y:', np.bincount(y))\nprint('Labels counts in y_train:', np.bincount(y_train))\nprint('Labels counts in y_test:', np.bincount(y_test))","810b5302":"from sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\nsc.fit(X_train)\nX_train_std = sc.transform(X_train)\nX_test_std = sc.transform(X_test)","d240b5ce":"from matplotlib.colors import ListedColormap\nimport matplotlib.pyplot as plt\n\n\ndef plot_decision_regions(X, y, classifier, test_idx=None, resolution=0.02):\n\n    # setup marker generator and color map\n    markers = ('s', 'x', 'o', '^', 'v')\n    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n    cmap = ListedColormap(colors[:len(np.unique(y))])\n\n    # plot the decision surface\n    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n                           np.arange(x2_min, x2_max, resolution))\n    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n    Z = Z.reshape(xx1.shape)\n    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)\n    plt.xlim(xx1.min(), xx1.max())\n    plt.ylim(xx2.min(), xx2.max())\n\n    for idx, cl in enumerate(np.unique(y)):\n        plt.scatter(x=X[y == cl, 0], \n                    y=X[y == cl, 1],\n                    alpha=0.8, \n                    c=colors[idx],\n                    marker=markers[idx], \n                    label=cl, \n                    edgecolor='black')\n\n    # highlight test samples\n    if test_idx:\n        # plot all samples\n        X_test, y_test = X[test_idx, :], y[test_idx]\n\n        plt.scatter(X_test[:, 0],\n                    X_test[:, 1],\n                    c='',\n                    edgecolor='black',\n                    alpha=1.0,\n                    linewidth=1,\n                    marker='o',\n                    s=100, \n                    label='test set')","22566536":"X_combined_std = np.vstack((X_train_std, X_test_std))\ny_combined = np.hstack((y_train, y_test))","1c962c0c":"import matplotlib.pyplot as plt\nimport numpy as np\n\n\ndef sigmoid(z):\n    return 1.0 \/ (1.0 + np.exp(-z))\n\nz = np.arange(-7, 7, 0.1)\nphi_z = sigmoid(z)\n\nplt.plot(z, phi_z)\nplt.axvline(0.0, color='k')\nplt.ylim(-0.1, 1.1)\nplt.xlabel('z')\nplt.ylabel('$\\phi (z)$')\n\n# y axis ticks and gridline\nplt.yticks([0.0, 0.5, 1.0])\nax = plt.gca()\nax.yaxis.grid(True)\n\nplt.tight_layout()\n#plt.savefig('images\/03_02.png', dpi=300)\nplt.show()","cd0a8ff5":"Image('..\/input\/python-ml-ch03-images\/03_03.png',width=700)","c81eab5a":"Image(filename='..\/input\/regularization\/LR-cost.png')","841ec7e2":"def cost_1(z):\n    return - np.log(sigmoid(z))\n\n\ndef cost_0(z):\n    return - np.log(1 - sigmoid(z))\n\nz = np.arange(-10, 10, 0.1)\nphi_z = sigmoid(z)\n\nc1 = [cost_1(x) for x in z]\nplt.plot(phi_z, c1, label='J(w) if y=1')\n\nc0 = [cost_0(x) for x in z]\nplt.plot(phi_z, c0, linestyle='--', label='J(w) if y=0')\n\nplt.ylim(0.0, 5.1)\nplt.xlim([0, 1])\nplt.xlabel('$\\phi$(z)')\nplt.ylabel('J(w)')\nplt.legend(loc='best')\nplt.tight_layout()\n#plt.savefig('images\/03_04.png', dpi=300)\nplt.show()","b58ae9d0":"from sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression(C=100,penalty='l2', random_state=1)\nlr.fit(X_train_std, y_train)\n\nplot_decision_regions(X_combined_std, y_combined,\n                      classifier=lr, test_idx=range(105, 150))\nplt.xlabel('petal length [standardized]')\nplt.ylabel('petal width [standardized]')\nplt.legend(loc='upper left')\nplt.tight_layout()\n#plt.savefig('images\/03_06.png', dpi=300)\nplt.show()","a3e902d8":"#Probability estimate\nlr.predict_proba(X_test_std[:3, :])","196cda03":"lr.predict_proba(X_test_std[:3, :]).sum(axis=1)","341294c2":"lr.predict_proba(X_test_std[:3, :]).argmax(axis=1)","fac5fb30":"lr.predict(X_test_std[:3, :])","87eb29ec":"lr.predict(X_test_std[0, :].reshape(1, -1))","6a3c7138":"#Returns the mean accuracy on the given test data and labels.\nlr.score(X_test_std, y_test)\n","5c8d55aa":"lr.score(X_train_std,y_train)","dd59dfda":"# Find the weights b0 and b1. \nprint(lr.coef_)\nprint(lr.intercept_)","b7bc51e9":"#Image(filename='..\/input\/regularization\/04_04.png',width=700)","bb62a5f8":"#Image(filename='..\/input\/regularization\/04_06.png',width=700)","e2b4d9d1":"Image(filename='..\/input\/regularization\/l2-term.png', width=700)","f80e2d8c":"#Image(filename='..\/input\/regularization\/04_05.png',width=300)","55d4c1dc":"weights, params = [], []\nfor c in np.arange(-5, 5):\n    lr = LogisticRegression(C=10.**c, random_state=1)\n    lr.fit(X_train_std, y_train)\n    weights.append(lr.coef_[1])\n    params.append(10.**c)\n\nweights = np.array(weights)\nplt.plot(params, weights[:, 0],\n         label='petal length')\nplt.plot(params, weights[:, 1], linestyle='--',\n         label='petal width')\nplt.ylabel('weight coefficient')\nplt.xlabel('C')\nplt.legend(loc='upper left')\nplt.xscale('log')\n#plt.savefig('images\/03_08.png', dpi=300)\nplt.show()","f704c606":"Image(filename='..\/input\/python-ml-ch03-images\/03_09.png', width=700) ","8fe6cae2":"#Image(filename='..\/input\/python-ml-ch03-images\/03_10.png', width=600) ","c73b389e":"from sklearn.svm import SVC\n\nsvm = SVC(kernel='linear', C=0.1, random_state=1)\nsvm.fit(X_train_std, y_train)\n\nplot_decision_regions(X_combined_std, \n                      y_combined,\n                      classifier=svm, \n                      test_idx=range(105, 150))\nplt.xlabel('petal length [standardized]')\nplt.ylabel('petal width [standardized]')\nplt.legend(loc='upper left')\nplt.tight_layout()\n#plt.savefig('images\/03_11.png', dpi=300)\nplt.show()","4b5aabbf":"svm.score(X_test_std,y_test)","08829110":"svm.score(X_train_std,y_train)","dc2598d0":"from sklearn.linear_model import SGDClassifier\n\nppn = SGDClassifier(loss='perceptron', n_iter=1000)\nlr = SGDClassifier(loss='log', n_iter=1000)\nsvm = SGDClassifier(loss='hinge', n_iter=1000)","38fb0fc3":"import matplotlib.pyplot as plt\nimport numpy as np\n\nnp.random.seed(1)\nX_xor = np.random.randn(200, 2)\ny_xor = np.logical_xor(X_xor[:, 0] > 0,\n                       X_xor[:, 1] > 0)\ny_xor = np.where(y_xor, 1, -1)\n\nplt.scatter(X_xor[y_xor == 1, 0],\n            X_xor[y_xor == 1, 1],\n            c='b', marker='x',\n            label='1')\nplt.scatter(X_xor[y_xor == -1, 0],\n            X_xor[y_xor == -1, 1],\n            c='r',\n            marker='s',\n            label='-1')\n\nplt.xlim([-3, 3])\nplt.ylim([-3, 3])\nplt.legend(loc='best')\nplt.tight_layout()\n#plt.savefig('images\/03_12.png', dpi=300)\nplt.show()","f9487674":"Image(filename='..\/input\/python-ml-ch03-images\/03_13.png', width=700) ","9a38c096":"svm = SVC(kernel='rbf', random_state=1, gamma=0.10, C=10.0)\nsvm.fit(X_xor, y_xor)\nplot_decision_regions(X_xor, y_xor,\n                      classifier=svm)\n\nplt.legend(loc='upper left')\nplt.tight_layout()\n#plt.savefig('images\/03_14.png', dpi=300)\nplt.show()","c2f9bd3f":"svm = SVC(kernel='rbf', random_state=1, gamma=100.0, C=1.0)\nsvm.fit(X_train_std, y_train)\n\nplot_decision_regions(X_combined_std, y_combined, \n                      classifier=svm, test_idx=range(105, 150))\nplt.xlabel('petal length [standardized]')\nplt.ylabel('petal width [standardized]')\nplt.legend(loc='upper left')\nplt.tight_layout()\n#plt.savefig('images\/03_16.png', dpi=300)\nplt.show()","8d7edc9f":"from sklearn.svm import SVC\n\nsvm = SVC(kernel='rbf', random_state=1, gamma=0.2, C=1.0)\nsvm.fit(X_train_std, y_train)\n\nplot_decision_regions(X_combined_std, y_combined,\n                      classifier=svm, test_idx=range(105, 150))\nplt.xlabel('petal length [standardized]')\nplt.ylabel('petal width [standardized]')\nplt.legend(loc='upper left')\nplt.tight_layout()\n#plt.savefig('images\/03_15.png', dpi=300)\nplt.show()","a3a225d4":"! python ..\/.convert_notebook_to_script.py --input ch03.ipynb --output ch03.py","82645fdc":"*Python Machine Learning 2nd Edition* by [Sebastian Raschka](https:\/\/sebastianraschka.com), Packt Publishing Ltd. 2017\n\nCode Repository: https:\/\/github.com\/rasbt\/python-machine-learning-book-2nd-edition\n\nCode License: [MIT License](https:\/\/github.com\/rasbt\/python-machine-learning-book-2nd-edition\/blob\/master\/LICENSE.txt)","c1aad640":"Exercise: Change penalty from default \"L2\" to \"L1\". Note change in the weights. Try C = 0.1 and see the difference in scores.","a14bd485":"What is overfitting?\nModel performs well on training data but not on unseen data (test data). Has high variance that can be due to too many parameters yielding a model with high complexity. Variance measures the stability or consistency of the model if we rebuild model multiple times with different subsets of training data.\n\nWhat is underfitting?\nModel does not capture the pattern in the training data and hence performs poorly on both training and test data. Low complexity model with low performance. High bias model.\nBias measures how far off the predictions are from the true values if we train the model multiple times with different subsets of training data.\n\nSolution: Tune the complexity of the model using regularization. It handles high collinearity, filters out noise from data and prevents overfitting. Add a bias term to penalize extreme values for weights. Feature scaling such as standardization are very important for regularization to work properly.","e757852e":"Logistic Regression is a classificaton method that predicts the probability that an input X belongs to Class y. The probability P(y =1 \/ X) is calculated and converted to 0 or 1 for classifying y. \n\nIt is named for the logistic (sigmoid) function that is an S curve that can map any real number to a value between 0 and 1, but not at those limits. \n\ny = e^(b0 + b1 * x) \/ (1 + e^(b0 + b1 * x))\nwhere b0 and b1 are the weights. \n\nP(y = 1 \/ X) = P(X) = e^(b0 + b1 * X) \/ (1 + e^(b0 + b1 * X))\n\nThis can be written as\nln(p(X) \/ 1 \u2013 p(X)) = b0 + b1 * X\n\nThe log odds of the default class is a linear combination of the input X. The coefficients are estimated using Maximum Likelihood Estimation.\nP(X) >= 0.5 => y = 1\n\nP(X) < 0.5 => y = 0\n\nhttps:\/\/machinelearningmastery.com\/logistic-regression-for-machine-learning\/","217214c1":"<br>\n<br>","954c8013":"## Maximum margin intuition","8b54904f":"Checking the distribution of classes in the dataset.","949cb900":"https:\/\/towardsdatascience.com\/l1-and-l2-regularization-methods-ce25e7fc831c\n\nhttps:\/\/www.knime.com\/blog\/regularization-for-logistic-regression-l1-l2-gauss-or-laplace\n\nhttp:\/\/www.chioka.in\/differences-between-l1-and-l2-as-loss-function-and-regularization\/\n\nL1 regularization or LASSO adds the absolute value of the weights as the penalty term to the loss\/cost function modulated by the lambda. It is computationally inefficient, yields sparse outputs (shrinks less important features' coefficients to zero) and hence has feature selection built in. ","6d26a685":"<br>\n<br>","75c31f4b":"...","f923d938":"*The use of `watermark` is optional. You can install this IPython extension via \"`pip install watermark`\". For more information, please see: https:\/\/github.com\/rasbt\/watermark.*","cf9dc59a":"# Summary","40e90bfe":"L2 regularization adds the squared magnitude of the coefficients to the cost function. It is computationally efficient, yields nonsparse outputs and does not help in feature selection.  ","34af034e":"![](http:\/\/)Standardizing the features:\nUsing the fit method, StandardScaler estimated the\nparameters \u03bc (sample mean) and \u03c3 (standard deviation) for each feature dimension\nfrom the training data. By calling the transform method, we then standardized the\ntraining data using those estimated parameters \u03bc and \u03c3 .","739a6b5f":"# Modeling class probabilities via logistic regression","89022647":"<br>\n<br>","7fd6d934":"Exercise: Recalculate scores by changing the value of C. C = 0.1, 1, 10, 100\n","24e0a9df":"Splitting data into 70% training and 30% test data:\nNote that the train_test_split function already shuffles the training sets\ninternally before splitting; otherwise, all class 0 and class 1 samples would have\nended up in the training set, and the test set would consist of 45 samples from\nclass 2. Via the random_state parameter, we provided a fixed random seed\n( random_state=1 ) for the internal pseudo-random number generator that is used\nfor shuffling the datasets prior to splitting. Using such a fixed random_state ensures\nthat our results are reproducible.\nLastly, we took advantage of the built-in support for stratification via stratify=y . In\nthis context, stratification means that the train_test_split method returns training\nand test subsets that have the same proportions of class labels as the input dataset.","9e0a4f76":"### Logistic regression intuition and conditional probabilities","2407b18e":"Note that the optional watermark extension is a small IPython notebook plugin that I developed to make the code reproducible. You can just skip the following line(s).","ec545555":"# Python Machine Learning - Code Examples","a617fc8f":"# Choosing a classification algorithm","1ebf5228":"...","c95a06d8":"<br>\n<br>","fcbebb38":"<br>\n<br>","2b5e9c08":"- [Choosing a classification algorithm](#Choosing-a-classification-algorithm)\n- [Modeling class probabilities via logistic regression](#Modeling-class-probabilities-via-logistic-regression)\n    - [Logistic regression intuition and conditional probabilities](#Logistic-regression-intuition-and-conditional-probabilities)\n    - [Learning the weights of the logistic cost function](#Learning-the-weights-of-the-logistic-cost-function)\n    - [Training a logistic regression model with scikit-learn](#Training-a-logistic-regression-model-with-scikit-learn)\n    - [Tackling overfitting via regularization](#Tackling-overfitting-via-regularization)\n- [Maximum margin classification with support vector machines](#Maximum-margin-classification-with-support-vector-machines)\n    - [Maximum margin intuition](#Maximum-margin-intuition)\n    - [Dealing with the nonlinearly separable case using slack variables](#Dealing-with-the-nonlinearly-separable-case-using-slack-variables)\n    - [Alternative implementations in scikit-learn](#Alternative-implementations-in-scikit-learn)\n- [Solving nonlinear problems using a kernel SVM](#Solving-nonlinear-problems-using-a-kernel-SVM)\n    - [Using the kernel trick to find separating hyperplanes in higher dimensional space](#Using-the\n- [Summary](#Summary)","9cb7b18c":"<br>\n<br>","551227b5":"# Chapter 3 - A Tour of Machine Learning Classifiers Using Scikit-Learn","9e7104ce":"Parameters\n\nclass sklearn.linear_model.LogisticRegression(penalty=\u2019l2\u2019, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver=\u2019warn\u2019, max_iter=100, multi_class=\u2019warn\u2019, verbose=0, warm_start=False, n_jobs=None)\n\nC : float, default: 1.0\nInverse of regularization strength; must be a positive float. Smaller values specify stronger regularization.","b5dfae6c":"Redefining the `plot_decision_region` function from chapter 2:","a2ae7721":"Looking at the shape of the arrays and the data.","af2d398d":"<br>\n<br>","e7fcde00":"...","bec08573":"## Using the kernel trick to find separating hyperplanes in higher dimensional space","fca00c9d":"<br>\n<br>","8dd95d99":"<br>\n<br>","a9f4b946":"# Maximum margin classification with support vector machines\nMaximize the margin. Margin is the distance between the decision boundary and the training samples closest to the decision boundary, the support vectors.","e87d23c8":"### Overview","6b26de1a":"Loading the Iris dataset from scikit-learn. Here, the third column represents the petal length, and the fourth column the petal width of the flower samples. The classes are already converted to integer labels where 0=Iris-Setosa, 1=Iris-Versicolor, 2=Iris-Virginica.","e10b6d93":"### Learning the weights of the logistic cost function\nCost function\n\nhttps:\/\/sebastianraschka.com\/faq\/docs\/probablistic-logistic-regression.html","b0133e18":"...","606159df":"The original solution to minimize the cost function is below.","65a47af4":"---\n\nReaders may ignore the next cell.","46ae3a82":"### Training a logistic regression model with scikit-learn","cf2adf70":"## Alternative implementations in scikit-learn","9b555fe2":"## Dealing with the nonlinearly separable case using slack variables","628ad3d5":"### Tackling overfitting via regularization","659afffa":"<br>\n<br>","9ac2c750":"<br>\n<br>","a291910c":"# First steps with scikit-learn","4e7b47cd":"where z = b0 + b1 * x","ef207b8a":"# Solving non-linear problems using a kernel SVM","efa4afa9":"**Note**\n\n- You can replace `Perceptron(n_iter, ...)` by `Perceptron(max_iter, ...)` in scikit-learn >= 0.19. The `n_iter` parameter is used here deriberately, because some people still use scikit-learn 0.18.","79447e2c":"There are equal number of flowers in each class. As an exercise, it will be interesting to check if the first 50 values of y are in class 0, the next 50 in class 1 and the last 50 in class 2.","c4f8c581":"<br>\n<br>"}}