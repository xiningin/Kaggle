{"cell_type":{"9e680aa9":"code","fbb74c82":"markdown"},"source":{"9e680aa9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","fbb74c82":"                                        **METERICS in LOGISTIC REGRESSION***\n\nPRECISION:\n----------\nPrecision means the percentage of your results which are relevant. - TP\/(TP+FP) . Measured for total actual with positive\n\nRECALL :\n--------\nRecall or Sensitivity or True Positive rate refers to the percentage of total relevant results correctly classified by your algorithm - TP\/(TP+FN) . Measured for total prediction with positive\n\n\nACCURACY:\n---------\nAccuracy refers to ratio of correct classification with incorrect classification\n\nConsider in amazon site, if you search product and it able to list 20 products out of which 10 are relevant then we see Recall is 100% (10\/10)  whereas precision is 50% (10\/20)\n\nSo there should be trade-off between Precision and Recall to have model optimal model for some cases which is provided by F1 score which is hormonic mean of Precision and recall. for other model we can maximise either of this\n\nSPECIFICITY:\n-----------\nSpecificity or TNR (True Negative Rate): Number of items correctly identified as negative out of total negatives- TN\/(TN+FP) . Measured for total prediction\n\nFalse Positive Rate or Type I Error: Number of items wrongly identified as positive out of total true negatives- FP\/(FP+TN). Measured for total prediction\nFalse Negative Rate or Type II Error: Number of items wrongly identified as negative out of total true positives- FN\/(FN+TP). Measured for total prediction\n\nROC curve:\n----------\n\nIt is curve used to measure relationship between true positive rate vs true negative rate.\n\nThe ROC curve is a useful tool for a few reasons:\n\n\nThe curves of different models can be compared directly in general or for different thresholds.\nThe area under the curve (AUC) can be used as a metric which falls between 0 and 1 with a higher number indicating better classification performance.\nHelps to identify threshold.\n\n\n\nReference:\n\nhttps:\/\/towardsdatascience.com\/precision-vs-recall-386cf9f89488\n\nhttps:\/\/towardsdatascience.com\/accuracy-precision-recall-or-f1-331fb37c5cb9\n\nhttps:\/\/towardsdatascience.com\/beyond-accuracy-precision-and-recall-3da06bea9f6c\n"}}