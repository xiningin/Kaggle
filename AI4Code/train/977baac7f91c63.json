{"cell_type":{"ec3ec361":"code","45d9007d":"code","c18b07e7":"code","6efa8b7f":"code","bcf2503b":"code","316602ae":"code","8e0fb138":"code","74563f4c":"code","d2a785e6":"code","1d563205":"code","9b6a39e6":"code","1fcf527d":"code","adf33956":"code","034e6829":"code","18d111c4":"code","2eac94cd":"code","de5bf886":"code","46695088":"code","8cdec733":"code","d7c7cfd9":"code","63c61f2a":"code","17187b4f":"code","74c91d9d":"code","fb454efc":"code","92edc021":"code","b0c9d198":"code","6c4ad1d9":"code","98f32ad8":"code","92335827":"code","0b785a00":"code","7e8ee213":"code","a9f8be5e":"code","00a436e0":"code","f5b6aae3":"code","feaa32e7":"code","a1c1eb89":"code","5c83f554":"code","1c75d07a":"code","fbd84aba":"code","2d0b287b":"code","2ce81c1d":"code","1dfb17d3":"code","6b005d46":"code","0d72e519":"code","ab878099":"code","2915e9aa":"code","d0217232":"code","699d90b5":"code","7eef4be8":"code","ef04befb":"code","aaafd60d":"code","88768a03":"markdown","aff12f0c":"markdown","79767b86":"markdown","6acad238":"markdown","14be5654":"markdown","503552c2":"markdown","15fb0ae5":"markdown"},"source":{"ec3ec361":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow as tf\nimport keras\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns\nsns.set(palette=\"Set2\")\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import (accuracy_score, f1_score,average_precision_score, confusion_matrix,\n                             average_precision_score, precision_score, recall_score, roc_auc_score, )\nfrom mlxtend.plotting import plot_confusion_matrix\n\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import tree\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n\n\nfrom xgboost import XGBClassifier, plot_importance\nfrom imblearn.over_sampling import SMOTE\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","45d9007d":"# read dataset\ndataset = pd.read_csv(\"\/kaggle\/input\/deep-learning-az-ann\/Churn_Modelling.csv\")","c18b07e7":"dataset.head()","6efa8b7f":"dataset.shape","bcf2503b":"sns.countplot(x=\"Gender\", data=dataset)","316602ae":"dataset.describe()","8e0fb138":"# checking datatypes and null values\ndataset.info()","74563f4c":"dataset.drop([\"RowNumber\",\"CustomerId\",\"Surname\"], axis=1, inplace=True)","d2a785e6":"_, ax = plt.subplots(1, 3, figsize=(18, 6))\nplt.subplots_adjust(wspace=0.3)\nsns.countplot(x = \"NumOfProducts\", hue=\"Exited\", data = dataset, ax= ax[0])\nsns.countplot(x = \"HasCrCard\", hue=\"Exited\", data = dataset, ax = ax[1])\nsns.countplot(x = \"IsActiveMember\", hue=\"Exited\", data = dataset, ax = ax[2])","1d563205":"_, ax = plt.subplots(1, 3, figsize=(18, 6))\nplt.subplots_adjust(wspace=0.3)\nsns.swarmplot(x = \"NumOfProducts\", y = \"Age\", hue=\"Exited\", data = dataset, ax= ax[0])\nsns.swarmplot(x = \"HasCrCard\", y = \"Age\", data = dataset, hue=\"Exited\", ax = ax[1])\nsns.swarmplot(x = \"IsActiveMember\", y = \"Age\", hue=\"Exited\", data = dataset, ax = ax[2])","9b6a39e6":"encoder = LabelEncoder()\ndataset[\"Geography\"] = encoder.fit_transform(dataset[\"Geography\"])\ndataset[\"Gender\"] = encoder.fit_transform(dataset[\"Gender\"])","1fcf527d":"dataset[\"Age\"].value_counts().plot.bar(figsize=(20,6))","adf33956":"facet = sns.FacetGrid(dataset, hue=\"Exited\", aspect=3)\nfacet.map(sns.kdeplot,\"Age\",shade= True)\nfacet.set(xlim=(0, dataset[\"Age\"].max()))\nfacet.add_legend()\n\nplt.show()","034e6829":"_, ax =  plt.subplots(1, 2, figsize=(15, 7))\ncmap = sns.cubehelix_palette(light=1, as_cmap=True)\nsns.scatterplot(x = \"Age\", y = \"Balance\", hue = \"Exited\", cmap = cmap, sizes = (10, 200), data = dataset, ax=ax[0])\nsns.scatterplot(x = \"Age\", y = \"CreditScore\", hue = \"Exited\", cmap = cmap, sizes = (10, 200), data = dataset, ax=ax[1])\n","18d111c4":"plt.figure(figsize=(8,8))\nsns.swarmplot(x=\"HasCrCard\",y = \"Age\", data=dataset, hue=\"Exited\")","2eac94cd":"facet = sns.FacetGrid(dataset, hue=\"Exited\",aspect=3)\nfacet.map(sns.kdeplot,\"Balance\",shade= True)\nfacet.set(xlim=(0, dataset[\"Balance\"].max()))\nfacet.add_legend()\n\nplt.show()","de5bf886":"_, ax = plt.subplots(1, 2, figsize=(15, 6))\nsns.scatterplot(x = \"Balance\", y = \"Age\", data = dataset, hue=\"Exited\", ax = ax[0])\nsns.scatterplot(x = \"Balance\", y = \"CreditScore\", data = dataset, hue=\"Exited\", ax = ax[1])\n\n","46695088":"facet = sns.FacetGrid(dataset, hue=\"Exited\",aspect=3)\nfacet.map(sns.kdeplot,\"CreditScore\",shade= True)\nfacet.set(xlim=(0, dataset[\"CreditScore\"].max()))\nfacet.add_legend()\n\nplt.show()","8cdec733":"plt.figure(figsize=(12,6))\nbplot = dataset.boxplot(patch_artist=True)\nplt.xticks(rotation=90)       \nplt.show()","d7c7cfd9":"plt.subplots(figsize=(11,8))\nsns.heatmap(dataset.corr(), annot=True, cmap=\"RdYlBu\")\nplt.show()","63c61f2a":"X = dataset.drop(\"Exited\", axis=1)\ny = dataset[\"Exited\"]\n","17187b4f":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)","74c91d9d":"clf = GaussianNB()\nclf.fit(X_train, y_train)\npred = clf.predict(X_test)\naccuracy_score(pred, y_test)","fb454efc":"clf = LogisticRegression()\nclf.fit(X_train, y_train)\npred = clf.predict(X_test)\naccuracy_score(pred, y_test)","92edc021":"sns.regplot(pred,y_test )","b0c9d198":"clf = tree.DecisionTreeClassifier()\nclf.fit(X_train, y_train)\npred = clf.predict(X_test)\naccuracy_score(pred, y_test)","6c4ad1d9":"sns.regplot(pred,y_test )","98f32ad8":"clf = RandomForestClassifier(n_estimators = 200, random_state=200)\nclf.fit(X_train, y_train)\npred = clf.predict(X_test)\naccuracy_score(pred, y_test)\n","92335827":"sns.regplot(pred,y_test, color=\"red\")","0b785a00":"clf  = XGBClassifier(max_depth = 10,random_state = 10, n_estimators=220, eval_metric = 'auc', min_child_weight = 3,\n                    colsample_bytree = 0.75, subsample= 0.9)\n\nclf.fit(X_train, y_train)\npred = clf.predict(X_test)\naccuracy_score(pred, y_test)","7e8ee213":"sns.regplot(pred,y_test, color=\"red\")","a9f8be5e":"scaler = MinMaxScaler() \n\nbumpy_features = [\"CreditScore\", \"Age\", \"Balance\",'EstimatedSalary']\n\ndf_scaled = pd.DataFrame(data = X)\ndf_scaled[bumpy_features] = scaler.fit_transform(X[bumpy_features])","00a436e0":"df_scaled.head()","f5b6aae3":"X = df_scaled\nsm  = SMOTE(random_state=42)\nX_res, y_res = sm.fit_sample(X, y)\nX_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size= 0.2, random_state=7)\n","feaa32e7":"clf = XGBClassifier(max_depth = 12,random_state=7, n_estimators=100, eval_metric = 'auc', min_child_weight = 3,\n                    colsample_bytree = 0.75, subsample= 0.8)\nclf.fit(X_train, y_train)\n\ny_pred = clf.predict(X_test)\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\nprint(\"Precision:\", precision_score(y_test, y_pred))\nprint(\"Recall:\", recall_score(y_test, y_pred))\nprint(\"F1:\", f1_score(y_test, y_pred))\nprint(\"Area under precision (AUC) Recall:\", average_precision_score(y_test, y_pred))","a1c1eb89":" #Confusion Matrix\nconfusion_matrix(y_test, y_pred)","5c83f554":"#splitting data into Train, DEV, test\nfrom sklearn.model_selection import train_test_split\ny=dataset.Exited # pulling values into another array so that we can drop\nX=dataset.drop(['Exited'],axis='columns')\nX_train, X_Dev, y_train, y_Dev = train_test_split(X,y,test_size=0.3,random_state=0,shuffle=False)\nX_train, X_test, y_train, y_test = train_test_split(X_train,y_train,test_size=0.2,random_state=0,shuffle=False)\n","1c75d07a":"#[Train] divide train data into categories , numerical and binary\n\nbinary_columns=[\"HasCrCard\",\"IsActiveMember\"]\nbinary_df=pd.DataFrame(X_train[binary_columns])\n\nnumerical_columns =[\"CreditScore\",\"Age\",\"Tenure\",\"Balance\",\"NumOfProducts\",\"EstimatedSalary\"]\nnumerical_df=pd.DataFrame(X_train[numerical_columns])\n\ncategory_columns=['Geography','Gender']\ncategory_df=pd.DataFrame(X_train[category_columns])","fbd84aba":"#[TRAIN] Encode Categorical Data\n\ncategory_df['Geography'] = category_df['Geography'].astype('category')\ncategory_df['Gender'] = category_df['Gender'].astype('category')\ncategory_df_Final = pd.get_dummies(category_df)\n","2d0b287b":"#[TRAIN] feature scaling\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nnumerical_df_train_mean=numerical_df.mean()\nnumerical_df_train_std=numerical_df.std(axis=0)\nnumerical_df_scale =pd.DataFrame(scaler.fit_transform(numerical_df),columns=numerical_columns)\n","2ce81c1d":"# [TRAIN] Concatenate Columns\nX_train = pd.concat([numerical_df_scale, category_df_Final,binary_df], axis=1)","1dfb17d3":"#is there any NULL row ?\ndataset.isnull().any().any(), dataset.shape","6b005d46":" df = dataset.copy","0d72e519":"#customers churn \nchurn= dataset[dataset[\"Exited\"]==1]\n#customers retention\nretention = dataset[dataset[\"Exited\"]==0]\n\nprint (\"Total Churn          :\", len(churn))\nprint (\"Total Retention      :\", len(retention))\n## return total length \"size\"\ntotal= len(dataset)\nprint (\"Churn Rate           :\",round((float(len(churn)) \/ float(total))*100,2),\"%\" )\nprint (\"Retention Rate       :\",(float(len(retention)) \/ float(total))*100,\"%\")\n","ab878099":"## Churn By Gender\nfemale = churn[churn['Gender']=='Female']\nmale   = churn[churn['Gender']=='Male']\nprint (\"Feramle Churn     :\",round((float(len(female)) \/ float(len(churn)))*100,2),\"%\" )\nprint (\"Male Churn        :\",round((float(len(male)) \/ float(len(churn)))*100,2),\"%\")\n\n","2915e9aa":"from imblearn.over_sampling import SMOTE\n\nX = dataset.drop(\"Exited\",axis = 1)\ny = dataset['Exited']\nsm  = SMOTE(random_state=42)\nX_res, y_res = sm.fit_sample(X, y)","d0217232":"\nX_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size= 0.2, random_state=7)\nprint(\"The split of the under_sampled data is as follows\")\nprint(\"X_train: \", len(X_train))\nprint(\"X_test: \", len(X_test))\nprint(\"y_train: \", len(y_train))\nprint(\"y_test: \", len(y_test))","699d90b5":"import xgboost as xgb\n\n\nmodel = xgb.XGBClassifier(max_depth = 12,random_state=7,n_estimators=100,eval_metric = 'auc' ,min_child_weight = 3\n                          ,colsample_bytree = 0.75, subsample= 0.8)\nmodel.fit(X_train, y_train)","7eef4be8":"y_pred = model.predict(X_test)\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\nprint(\"Precision:\", precision_score(y_test, y_pred))\nprint(\"Recall:\", recall_score(y_test, y_pred))\nprint(\"F1:\", f1_score(y_test, y_pred))\nprint(\"Area under precision Recall:\", average_precision_score(y_test, y_pred))\n","ef04befb":"cm = confusion_matrix(y_test, y_pred)","aaafd60d":"fig, ax = plot_confusion_matrix(conf_mat=cm)\nplt.title(\"The Confusion Matrix\")\nplt.ylabel(\"Actual\")\nplt.xlabel(\"Predicted\")\nplt.show()\nprint(\"The Accuracy is : \"+str((float(cm[1,1])+float(cm[0,0]))\/(float(cm[0,0]) + float(cm[0,1])+float(cm[1,0]) + float(cm[1,1]))*100) + \"%\")\nprint(\"The Recall   is : \"+ str(float(cm[1,1])\/(float(cm[1,0]) + float(cm[1,1]))*100) +\"%\")","88768a03":"\n## Bank Customer Churn Prediction\n\nIn this kernel I am going to make an Exploratory Data Analysis (EDA) on this dataset. Also I am going to make different predictive models and find out the best one with highest prediction accuracy.\n\n\n**Kernel Outlines:**\n\n*     Importing Necessary Packages\n*     Statistical Summary of the Dataset\n*     Dropping Irrelevant Features\n*     One Hot Encoding\n*     Data Visualization\n*     Detecting Outliers using Tukey Boxplot\n*     Hand written function for detecting and removing outliers\n*     Checking Correlation with Heatmap\n*     Different ML predictive models\n*         Gaussian Naive Bayes\n*         Logistic Regression\n*         Decision Tree\n*         Random Forest\n*         Extra Gradient Boosting Tree (XGBoost)\n*     Improve the Predictive Model\n*         Feature Scaling\n*         Over Sampling\n\n#### Importing Necessary Packages\n\n","aff12f0c":"### Prediction with ML models:","79767b86":"\nDropping Irrelevant Feature\u00b6\n\n`RowNumber`, `CustomerId` and `Surname` are irrelivant, so we drop those features.\n","6acad238":"\n\n**Customer with 3 or 4 products are higher chances to Churn\n**","14be5654":"#### Detecting Outliers using Tukey Boxplot","503552c2":"\n\n*         **40 to 70 years old customers are higher chances to churn**\n*         **Customer with CreditScore less then 400 are higher chances to churn**\n\n","15fb0ae5":"### Checking Correlation"}}