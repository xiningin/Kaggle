{"cell_type":{"8f53f9f1":"code","d64c88ce":"code","e0369093":"code","ca8509d2":"code","4068cbf3":"code","3325d2fb":"code","98d6a4f8":"code","3c7f3555":"code","bd16029e":"code","f590bf49":"code","51962b0b":"markdown","07c416c4":"markdown","c110ba3b":"markdown","751b1d19":"markdown","efd63c00":"markdown","e99748f6":"markdown","d6120d43":"markdown","0611100c":"markdown","3e8022d3":"markdown"},"source":{"8f53f9f1":"%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt","d64c88ce":"from sklearn.datasets import make_regression\n\nX, y = make_regression(n_samples = 100, n_features=5,\n                            n_informative=1, bias = 150.0,\n                            noise = 30, random_state=0)","e0369093":"X[:5], y[:5]","ca8509d2":"def cost(x, y, th, loc=None):\n    m = len(x)\n    if loc != 'in':\n        x = np.hstack((np.array([1]*m).reshape(-1,1), x))\n    y = y.reshape(-1,1)\n    return (np.sum((x.dot(th) - y)**2)) \/ (2*m)","4068cbf3":"def gd(x, y, a=0.1, itr=100, graph=0):\n    m, n = x.shape \n    # m -> no of datapoints\n    # n -> no of features\n    \n    th = np.array([0]*(n+1)).reshape(-1,1)\n    # th -> set intercept and coefficient values to 0 initially\n    \n    x = np.hstack((np.ones((m,1)), x))\n    # add a row of 1s to the dataset to mulitply with the intercept term in th\n    \n    y = y.reshape(-1,1)\n    cst = [cost(x, y, th, loc='in')]\n    # list to store the cost to check for convergence\n    \n    for i in range(itr):\n        der = (x.T).dot(x.dot(th) - y) \/ m\n        # gradient of the cost function\n        \n        th = th - (a * der) #updated th\n        cst.append(cost(x, y, th, loc='in')) #cost for updated th\n    if graph == 1:\n        return th, cst, a, itr\n    return th","3325d2fb":"out = gd(X, y, a=1.6, itr=100, graph=1)\nplt.figure()\nplt.plot(out[1]) #plot the cost wrt iterations\nplt.title('Convergence curve for alpha = {} and max_iteration = {}'.format(out[2], out[3]))\nplt.ylabel('Mean square error (Cost function)')\nplt.xlabel('Iteration')\nplt.show()","98d6a4f8":"print('Intercept: ', out[0][0,0])\nprint('Coefficients: ', out[0][1:].ravel()) # Gives bad estimates","3c7f3555":"out = gd(X, y, a=0.1, itr=500, graph=1)\nplt.figure()\nplt.plot(out[1]) #plot the cost wrt iterations\nplt.title('Convergence curve for alpha = {} and max_iteration = {}'.format(out[2], out[3]))\nplt.ylabel('Mean square error (Cost function)')\nplt.xlabel('Iteration')\nplt.show()","bd16029e":"print('Intercept: ', out[0][0,0])\nprint('Coefficients: ', out[0][1:].ravel()) # Gives proper estimates","f590bf49":"from sklearn.linear_model import LinearRegression\nlr = LinearRegression().fit(X, y)\nprint('Intercept: ', lr.intercept_)\nprint('Coefficients: ', lr.coef_)","51962b0b":"### A perfectly converging model for appropriate learning rate(a)","07c416c4":"### A diverging model for high learning rate(a)","c110ba3b":"### Specify loc='in' while using cost function inside Gradient Descent(gd function) ","751b1d19":"### Mean Square Cost Function for linear regression","efd63c00":"## Linear Regression","e99748f6":"### Verifying Results with Scikit-learn","d6120d43":"# Gradient Descent \n\n   Gradient descent is one of the most basic and easy to implement algorithm for finding minima of the cost function. For a mean square cost function there exists only one minima which it's global minima. This is because the mean square cost function is a Quadratic Function.\n            \n   For a complex cost function with many local minimas, the algorithm that I have given below might not work effectively.\n            \n   Always check the convergence curve when you're training the model for the first time. If it doesn't converge try decreasing the learning rate(a) and increase the iterations. This is one of the downsides of gradient descent, but in more complex solvers like BGFS, L-BGFS, etc we need not specify the learning rate.","0611100c":"## Creating synthetic datasets for testing","3e8022d3":"### Intercept and Coefficient calculating Function for linear regression using Gradient Descent\n\nx     -> the training dataset, a 2D array.\n\ny     -> target values, 1D array.\n\na     -> learning rate.\n\nitr   -> maximum number of iterations to be performed."}}