{"cell_type":{"799d38a6":"code","7226efe9":"code","cff2bfeb":"code","a985b78d":"code","6726a7a4":"code","5f62988e":"code","13c70348":"code","02c37a11":"markdown","da4b833d":"markdown","27d19acb":"markdown","9f839599":"markdown","710b7dee":"markdown","2f7a46aa":"markdown","992ad1d0":"markdown","e0c05f53":"markdown","eecacdb1":"markdown","eccc4572":"markdown","e3825885":"markdown","3a466e0f":"markdown","24c7e345":"markdown","50c5d370":"markdown","86e09100":"markdown","05b7d077":"markdown","1f8d844e":"markdown","f96efb97":"markdown","7935ca19":"markdown","0fa83c7f":"markdown","93675fcb":"markdown","eb2094a9":"markdown"},"source":{"799d38a6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n#Function to turn non numeric values into categories\ndef categorize_df(df):\n    for column,series in df.items():\n        if is_string_dtype(series): \n            df[column] = series.astype('category').cat.as_ordered()\n    return df\n\n# function to let me know how many numeric and non numeric columns are there\nfrom pandas.api.types import is_string_dtype, is_numeric_dtype\n\ndef numeric_count(df):\n    non_numericals = 0\n    numericals = 0\n    for c in df.columns:\n        if is_numeric_dtype(df[c].dtype):\n            numericals +=1\n        else:\n            df[c] = pd.to_numeric(df[c],errors='ignore')\n            if is_numeric_dtype(df[c].dtype):\n                numericals +=1\n            else:\n                non_numericals +=1\n    return print('{} numbers and {} non numbers'.format(numericals,non_numericals))\n\n#function to replace categories for codes in dataframe\n# adds 1 as pandas replaces missing values with -1\ndef replace_cats(df):\n    for column in df.columns:\n        if df[column].dtype.name == 'category':\n            df[column] = df[column].cat.codes +1\n    return df\n\n#function to add na_columns when missing values are present\ndef na_columns(df):\n    for column, series in df.items():\n        if df[column].isnull().sum():\n            df[column+'_na'] = df[column].isnull()\n    return df\n\n#function to fill null values with the mean of the column\ndef mean_df(df): return df.fillna(df.mean())\n\n#function to separate training set from evaluation variable\ndef x_and_y(df,target):\n    y = df[target].values\n    X = df.drop(target, axis=1)\n    return X, y\n\n# create a forest regressor\nfrom sklearn.ensemble import RandomForestRegressor\n\nm = RandomForestRegressor(n_estimators=80,n_jobs=-1,oob_score=True)\n\n#split a training set\nimport math as math\ndef split_df(X,y,split):\n    Xt = X[:math.floor(len(X)*split)]\n    Xv = X[math.floor(len(X)*split):]\n    yt = y[:math.floor(len(y)*split)]\n    yv = y[math.floor(len(y)*split):]\n    return Xt,Xv,yt,yv\n\n#function to get log root mean square error\ndef lrmse(predicted,y): return math.sqrt(((np.log(predicted)-np.log(y))**2).mean())\n\n#function to get root mean square error\ndef rmse(predicted,y): return math.sqrt(((predicted-y)**2).mean())\n\n#calculate and print errors\ndef print_errors (Xt,Xv,yt,yv):\n    errors = []\n    errors.append(rmse(m.predict(Xt),yt))\n    errors.append(rmse(m.predict(Xv),yv))\n    errors.append(m.score(Xt,yt))\n    errors.append(m.score(Xv,yv))\n    if hasattr(m, 'oob_score_'): errors.append(m.oob_score_)\n    print(errors)\n\ndef training_pipeline(df):\n    df = categorize_df(df)\n    df = replace_cats(df)\n    df = na_columns(df)\n    #df = df.select_dtypes([np.number])\n    df = mean_df(df)\n    X,y = x_and_y(df,'SalePrice')\n    #activate log of y during modeling as the evaluation is based on the lrmse\n    #y = np.log(y)\n    Xt,Xv,yt,yv = split_df(X,y,0.7)\n    m.fit(Xt,yt)\n    print_errors(Xt,Xv,yt,yv)\n    return X, y, Xt, Xv, yt, yv\n\ndef output_pipeline(df,X,output):\n    df = categorize_df(df)\n    df = replace_cats(df)\n    df = na_columns(df)\n    #df = df.select_dtypes([np.number])\n    df = mean_df(df)\n    X, X_test = X.align(df,join='inner',axis=1)\n    predictions = m.predict(X_test)\n    my_submission = pd.DataFrame({'Id': df.Id, 'SalePrice': predictions})\n    # you could use any filename. We choose submission here\n    my_submission.to_csv(\"submission{}.csv\".format(output), index=False)\n\n# draw a tree, taken from fastai library\nfrom IPython.display import Image\nfrom sklearn.tree import export_graphviz\ndef draw_tree(t, df, size=10, ratio=0.6, precision=0):\n    \"\"\" Draws a representation of a random forest in IPython.\n\n    Parameters:\n    -----------\n    t: The tree you wish to draw\n    df: The data used to train the tree. This is used to get the names of the features.\n    \"\"\"\n    s=export_graphviz(t, out_file=\"simple-tree.dot\", feature_names=df.columns, filled=True,\n                      special_characters=True, rotate=True, precision=precision)\n    !dot -Tpng simple-tree.dot -o simple-tree.png -Gdpi=600\n\n#draw a plot of R2 score for each additional tree added to the forrest\nfrom sklearn import metrics\nimport matplotlib.pyplot as plt\n\ndef plot_r2_evolution(Xv,yv):\n    predictions = np.stack([t.predict(Xv) for t in m.estimators_])\n    r2 = []\n    last_r2 = 0\n    right_tree = 0\n    for i in range (len(predictions)):\n        r2.append(metrics.r2_score(yv, np.mean(predictions[:i+1], axis=0)))\n        if right_tree == 0:\n            if abs(r2[-1] - last_r2) > 0.00003:\n                last_r2 = r2[-1]\n            else:\n                print(\"after {} trees, no significant improvement\".format(i))\n                right_tree = i\n    plt.plot(r2)\n    #print(r2)","7226efe9":"df_train = pd.read_csv('..\/input\/train.csv',low_memory=False)\ndf_test = pd.read_csv('..\/input\/test.csv',low_memory=False)","cff2bfeb":"df_train = pd.read_csv('..\/input\/train.csv',low_memory=False)\n#df_train.head()\n# we remove the ID column, we don't need it\nId_column = df_train['Id']\ndf_train = df_train.drop('Id',axis=1)\n#print(df_train.head())\ndf_train.isnull().sum().sort_values(ascending=False)\ndf_train.PoolQC.fillna(\"None\", inplace=True)\n#df_train.PoolQC.head()\ndf_train.MiscFeature.fillna(\"None\", inplace=True)\n#df_train.MiscFeature.head()\n#df_train['Neighborhood'].head(100)\nimport seaborn as sns\n#sns.countplot(df_train.Neighborhood)\nNone_columns = ['Alley','MasVnrType','MasVnrArea','FireplaceQu','Fence','GarageQual','GarageYrBlt','GarageCond','GarageFinish','GarageType','BsmtFinType2','BsmtExposure','BsmtQual','BsmtFinType1','BsmtCond']\ndf_train[None_columns] = df_train[None_columns].fillna(\"None\")\ndf_train['Electrical'].fillna(df_train['Electrical'].mode()[0],inplace=True)\n#len(df_train.columns)\n\n# How to replace missing values from column LotFrontage with the mean of LotFrontage based on the Neighborhood: Houses must have similar LotFrontage if they're from the same neighborhood\n# # Drop all rows where LotFrontage is null\ndf_no_lot = df_train.dropna(subset=['LotFrontage'])\n# # Dataframe with Neighborhood and the mean for each neighborhood\ndf_Neighmeans = df_no_lot.groupby('Neighborhood', as_index=False)['LotFrontage'].mean()\n# # Make a dictionary out of it\ndict = df_Neighmeans.set_index('Neighborhood').T.to_dict('list')\n# # The mean is stored in a list. Turn the list into float, since it's only one value\ndict = {k:float(dict[k][0]) for k in dict}\n# # Replace!\ndf_train.LotFrontage = df_train.LotFrontage.fillna(df_train.Neighborhood.map(dict))\n\n","a985b78d":"X_test.columns","6726a7a4":"print(\"baseline\")\nm = RandomForestRegressor(n_jobs=-1)\nX, y, Xt, Xv, yt, yv = training_pipeline(df_train)\n#output_pipeline(df_test,X,10)\nprint(\"tiny tree\")\nm = RandomForestRegressor(n_estimators=1, max_depth=3, bootstrap=False, n_jobs=-1)\nm.fit(Xt,yt)\nprint_errors (Xt,Xv,yt,yv)\nprint(\"only one tree\")\nm = RandomForestRegressor(n_estimators=1, bootstrap=False, n_jobs=-1)\nm.fit(Xt,yt)\nprint_errors (Xt,Xv,yt,yv)\nprint(\"baseline\")\nm = RandomForestRegressor(n_jobs=-1)\nm.fit(Xt,yt)\nprint_errors (Xt,Xv,yt,yv)\nprint(\"50 trees\")\nm = RandomForestRegressor(n_estimators=50,n_jobs=-1)\nm.fit(Xt,yt)\nprint_errors (Xt,Xv,yt,yv)\nprint(\"50 trees, 3 samples per leaf\")\nm = RandomForestRegressor(n_estimators=50,min_samples_leaf=3,n_jobs=-1)\nm.fit(Xt,yt)\nprint_errors (Xt,Xv,yt,yv)\nprint(\"50 trees, 5 samples per leaf\")\nm = RandomForestRegressor(n_estimators=50,min_samples_leaf=5,n_jobs=-1)\nm.fit(Xt,yt)\nprint_errors (Xt,Xv,yt,yv)\nprint(\"80 trees, 3 samples per leaf, 0.5 features\")\nm = RandomForestRegressor(n_estimators=80,max_features=0.5,min_samples_leaf=3,n_jobs=-1)\nX, y, Xt, Xv, yt, yv = training_pipeline(df_train)\noutput_pipeline(df_test,X,13)\nprint(\"50 trees, 5 samples per leaf, sqrt features\")\nm = RandomForestRegressor(n_estimators=50,max_features=\"sqrt\",min_samples_leaf=3,n_jobs=-1)\nX, y, Xt, Xv, yt, yv = training_pipeline(df_train)\nprint(\"50 trees, 5 samples per leaf, log2 features\")\nm = RandomForestRegressor(n_estimators=50,max_features=\"log2\",min_samples_leaf=3,n_jobs=-1)\nX, y, Xt, Xv, yt, yv = training_pipeline(df_train)\nprint(\"gridsearch result top rank\")\nm = RandomForestRegressor(bootstrap=False, max_depth=None, max_features=10, min_samples_split= 2, n_estimators=100,n_jobs=-1)\nX, y, Xt, Xv, yt, yv = training_pipeline(df_train)\noutput_pipeline(df_test,X,15)\n","5f62988e":"# from sklearn.model_selection import GridSearchCV\n# from time import time\n\n# # build a classifier\n# clf = RandomForestRegressor(n_jobs=-1)\n\n# # Utility function to report best scores\n# def report(results, n_top=3):\n#     for i in range(1, n_top + 1):\n#         candidates = np.flatnonzero(results['rank_test_score'] == i)\n#         for candidate in candidates:\n#             print(\"Model with rank: {0}\".format(i))\n#             print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n#                   results['mean_test_score'][candidate],\n#                   results['std_test_score'][candidate]))\n#             print(\"Parameters: {0}\".format(results['params'][candidate]))\n#             print(\"\")\n\n# # use a full grid over all parameters\n# param_grid = {\"n_estimators\": [10, 20,40,50,80,100],\n#               \"max_depth\": [3, None],\n#               \"max_features\": [1, 3, 10, \"sqrt\", \"log2\"],\n#               \"min_samples_split\": [2, 3, 10],\n#               \"bootstrap\": [True, False]}\n# # run grid search\n# grid_search = GridSearchCV(clf, param_grid=param_grid, cv=5)\n# start = time()\n# grid_search.fit(X, y)\n\n\n# print(\"GridSearchCV took %.2f seconds for %d candidate parameter settings.\"\n#       % (time() - start, len(grid_search.cv_results_['params'])))\n# report(grid_search.cv_results_)\n","13c70348":"for i in range(len(df_test.select_dtypes(np.object).columns)):\n    print(df_test.select_dtypes(np.object).columns[:i+1])","02c37a11":"And we train the model!","da4b833d":"we have 91 features for the test set and 83 for the train set. what happenned?","27d19acb":"We now split the y (value to predict) from X (values to train)","9f839599":"badly overfitting, and likely messing up terribly when evaluating the test data set. let's try only with numerical variables, see what happens","710b7dee":"We see there's a lot of stuff. 2 distint values that are an issue for modeling: text and dates","2f7a46aa":"We got our predictions! now we format properly and submit, see how it went","992ad1d0":"We might have dropped columns from the original model, so we have to train it again, and then we predict right away","e0c05f53":"Now we replace the strings with the category codes assigned by pandas. it will no longer be FV, RH, etc. but 1,2,3, etc. We add +1 at the end because pandas assigns -1 to non values, so it'll be zero","eecacdb1":"First part: read the data","eccc4572":"we have 3 fields with missing values. Only numerical though, as pandas put -1 on string values. ","e3825885":"we're good to go! the data is proper to be processed.\n\nOne last thing: because this competition evaluates the root square mean error between logarithms, we'll change the SalePrice column to log(SalePrice)","3a466e0f":"And we predict our values:","24c7e345":"we have to process the test data just like we did with the training data","50c5d370":"0.97 is pretty awesome! 1 is the best. We might be overfitting though. Let's just submit and see how we fare without doing any research into how to improve it","86e09100":"there were 3 columns with missing values on the train set and 11 columns with missing values on the test set. So engineering the test set we added 8 extra columns. So we have to align by removing those extra 8 columns","05b7d077":"can't convert automatically, lots of unconvertable text. we go to categories then","1f8d844e":"all good. we now check if there are missing values in the set","f96efb97":"We check if the conversion worked ","7935ca19":"aNext, we have a look at the data: what columns, what does it look like the first rows,","0fa83c7f":"let's see what happens if we split 70\/30 our training set and compare","93675fcb":"We check that the transformation went through with one category. we can see the categories and the codes","eb2094a9":"No dates, but lots of text. let's put in numbers everything we can first"}}