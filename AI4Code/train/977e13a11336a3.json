{"cell_type":{"8bea6fb2":"code","c5c9ab90":"code","b0defff8":"code","83f87361":"code","40fc541f":"code","0e16df7e":"code","b17947bb":"code","b00c9262":"code","e72edabd":"code","644a0c2c":"code","348abbd4":"code","0a526dd2":"code","be02b8d4":"code","deb0f82c":"code","19f9bd8c":"code","07bab66e":"code","ca3e953f":"code","dfc6b55f":"code","559f95a1":"code","ff50aa61":"code","7d5f7c24":"code","4896ef52":"code","b48029dd":"markdown","fe601538":"markdown","5110ae7e":"markdown","29fb8e9b":"markdown","8e65e596":"markdown","c95b4948":"markdown","32433819":"markdown","ca06390c":"markdown","30144c81":"markdown","893a392e":"markdown","38918683":"markdown","c307ef1a":"markdown","1df1ba93":"markdown","784cb2d8":"markdown","5c84326c":"markdown","fea65ef3":"markdown","ddc6652f":"markdown","7f973a3b":"markdown","a0247ef2":"markdown","56cea0ee":"markdown","4dc0da35":"markdown","8b4f9a15":"markdown","96c7085b":"markdown","430bdd33":"markdown","7ca54016":"markdown","2362dee7":"markdown","830639a7":"markdown","8c38e630":"markdown","9bc6bc52":"markdown","2e333eb9":"markdown","fd3c8f2b":"markdown","d2a06984":"markdown","3e0cff42":"markdown","4bd3af0d":"markdown","a2262ff6":"markdown","23f0ec58":"markdown","3a789a4d":"markdown","0d31ad77":"markdown","3faf96b5":"markdown"},"source":{"8bea6fb2":"import pip._internal as pip\npip.main(['install', '--upgrade', 'numpy==1.17.2'])\nimport numpy as np\nimport pandas as pd\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.model_selection import GridSearchCV\nfrom xgboost import XGBClassifier\n\nfrom lwoku import RANDOM_STATE, N_JOBS, VERBOSE, get_prediction\nfrom grid_search_utils import plot_grid_search, table_grid_search\n\nimport pickle","c5c9ab90":"VERBOSE=1","b0defff8":"# Read training and test files\nX_train = pd.read_csv('..\/input\/learn-together\/train.csv', index_col='Id', engine='python')\nX_test = pd.read_csv('..\/input\/learn-together\/test.csv', index_col='Id', engine='python')\n\n# Define the dependent variable\ny_train = X_train['Cover_Type'].copy()\n\n# Define a training set\nX_train = X_train.drop(['Cover_Type'], axis='columns')","83f87361":"xg_clf = XGBClassifier(verbosity=VERBOSE,\n                       random_state=RANDOM_STATE,\n                       n_jobs=N_JOBS)","40fc541f":"parameters = {\n    'max_depth': [1, 2, 3, 5, 8, 13, 21, 34, 55]\n}\nclf = GridSearchCV(xg_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf)","0e16df7e":"parameters = {\n    'learning_rate': [0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1, 2, 5]\n}\nclf = GridSearchCV(xg_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf)","b17947bb":"parameters = {\n    'n_estimators': [20, 50, 100, 200, 500, 1000, 1500, 2000, 2500]\n}\nclf = GridSearchCV(xg_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf)","b00c9262":"parameters = {\n    'booster': ['gbtree', 'gblinear', 'dart']\n}\nclf = GridSearchCV(xg_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf, all_ranks=True)","e72edabd":"parameters = {\n    'gamma': [0, 1, 2, 3, 5, 8]\n}\nclf = GridSearchCV(xg_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf)","644a0c2c":"parameters = {\n    'min_child_weight': [1e0, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7, 1e-8]\n}\nclf = GridSearchCV(xg_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf)","348abbd4":"parameters = {\n    'max_delta_step': [0, 1, 2, 3, 5, 8]\n}\nclf = GridSearchCV(xg_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf)","0a526dd2":"parameters = {\n    'max_delta_step': [x \/ 10 for x in range(1, 11)]\n}\nclf = GridSearchCV(xg_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf)","be02b8d4":"parameters = {\n    'colsample_bytree': [x \/ 10 for x in range(0, 11)]\n}\nclf = GridSearchCV(xg_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf)","deb0f82c":"parameters = {\n    'colsample_bylevel': [x \/ 10 for x in range(0, 11)]\n}\nclf = GridSearchCV(xg_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf)","19f9bd8c":"parameters = {\n    'colsample_bynode': [x \/ 10 for x in range(0, 11)]\n}\nclf = GridSearchCV(xg_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf)","07bab66e":"parameters = {\n    'reg_alpha': [x \/ 10 for x in range(0, 11)]\n}\nclf = GridSearchCV(xg_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf)","ca3e953f":"parameters = {\n    'reg_lambda': [x \/ 10 for x in range(0, 11)]\n}\nclf = GridSearchCV(xg_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf)","dfc6b55f":"parameters = {\n    'scale_pos_weight': [-0.5 + x \/ 10 for x in range(0, 11)]\n}\nclf = GridSearchCV(xg_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf)","559f95a1":"parameters = {\n    'importance_type': ['gain', 'weight', 'cover', 'total_gain', 'total_cover']\n}\nclf = GridSearchCV(xg_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf)","ff50aa61":"xg_clf = XGBClassifier(verbosity=VERBOSE,\n                       random_state=RANDOM_STATE,\n                       n_jobs=N_JOBS)\nxg_clf.n_estimators = 500\nxg_clf.max_depth = 21\nxg_clf.learning_rate = 1\nxg_clf.gamma = 2\nxg_clf.min_child_weight = 2\nxg_clf.max_delta_step = 2\nxg_clf.subsample = 0.7\nxg_clf.colsample_bytree = 0.5\nxg_clf.colsample_bylevel = 0.4\nxg_clf.colsample_bynode = 0.4\nxg_clf.reg_lambda = 0\nparameters = {\n    'n_estimators': [500],\n    'learning_rate': [0.1, 1],\n    'gamma': [0, 2],\n#     'min_child_weight': [1, 2],\n#     'max_delta_step': [0, 2],\n#     'subsample': [0.7, 1],\n#     'colsample_bytree': [0.5, 1],\n#     'colsample_bylevel': [0.4, 1],\n#     'colsample_bynode': [0.4, 1],\n#     'reg_lambda': [0, 1]\n}\nclf = GridSearchCV(xg_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf)","7d5f7c24":"with open('clf.pickle', 'wb') as fp:\n    pickle.dump(clf, fp)","4896ef52":"clf.best_estimator_","b48029dd":"The type has no influence on the score.","fe601538":"# base_score\n##### (default=0.5)\n\nThe initial prediction score of all instances, global bias.","5110ae7e":"# colsample_bylevel\n##### : float (default=1)\n\nSubsample ratio of columns for each level.","29fb8e9b":"`gbtree` and `dart` scores the same.\nBut `dart` needs more fit and score times.\n\n`gblinear` has a very low score.","8e65e596":"The optimum value for the ratio of columns for tress is 0.5.","c95b4948":"# Exhaustive search","32433819":"It has some chaotic behaviour,\nwith a clear downward trend.","ca06390c":"# gamma\n##### : float (default=0)\n\nMinimum loss reduction required to make a further partition on a leaf node of the tree.","30144c81":"# booster\n##### : string (default='gbtree')\n\nSpecify which booster to use: gbtree, gblinear or dart.","893a392e":"# colsample_bynode\n##### : float (default=1)\n\nSubsample ratio of columns for each split.","38918683":"# Introduction\n\nThe aim of this notebook is to optimize the Extra-trees model.\n\nFirst, all [XGBoost](https:\/\/xgboost.readthedocs.io\/en\/latest\/python\/python_api.html?#xgboost.XGBClassifier) parameters are analysed separately.\n\nThen, a grid search is carried out.\nThis is a search through all the combinations of parameters,\nwhich optimize the internal score in the train set.\n\nThe results are collected at [Tactic 03. Hyperparameter optimization](https:\/\/www.kaggle.com\/juanmah\/tactic-03-hyperparameter-optimization).","c307ef1a":"# reg_alpha\n##### : float (xgb's alpha) (default=0)\n\nL1 regularization term on weights","1df1ba93":"It has some chaotic behaviour.","784cb2d8":"The more subsamples, the higher the score, up to a limit: 0.7.","5c84326c":"**Note**: Not evaluated","fea65ef3":"The score increases up to `learning_rate` 1, then decays.","ddc6652f":"# max_depth\n##### : int (default=3)\n\nMaximum tree depth for base learners.","7f973a3b":"The score increases up to `max_depth` 21, then estabilizes.","a0247ef2":"# reg_lambda\n##### : float (xgb's lambda) (default=1)\n\nL2 regularization term on weights","56cea0ee":"# objective\n##### : string or callable (default='binary:logistic')\n\nSpecify the learning task and the corresponding learning objective or a custom objective function to be used (see note below).","4dc0da35":"# Prepare data","8b4f9a15":"# importance_type\n##### : string (default='gain')\n\nThe feature importance type for the feature_importances_ property: either \u201cgain\u201d, \u201cweight\u201d, \u201ccover\u201d, \u201ctotal_gain\u201d or \u201ctotal_cover\u201d.","96c7085b":"# min_child_weight\n##### : int (default=1)\n\nMinimum sum of instance weight(hessian) needed in a child.","430bdd33":"# learning_rate\n##### : float (default=0.1)\n\nBoosting learning rate (xgb\u2019s \u201ceta\u201d)","7ca54016":"# Search over parameters","2362dee7":"**Note**: not evaluated","830639a7":"This parameter has no influence in score.","8c38e630":"The more estimators, the higher the score and the greater the fit and score time.","9bc6bc52":"# scale_pos_weight\n##### : float (default=1)\n\nBalancing of positive and negative weights.","2e333eb9":"The `max_delta_step` value that maximizes the score is 2.","fd3c8f2b":"## Export grid search results","d2a06984":"The gamma that makes the score maximum is 2.\nFrom then, the score decays.","3e0cff42":"The optimum value for the ratio of columns for trees is 0.4.","4bd3af0d":"This parameter has no influence on the score.","a2262ff6":"# n_estimators\n##### : int (default=100)\n\nNumber of trees to fit.","23f0ec58":"# max_delta_step\n##### : int (default=0)\n\nMaximum delta step we allow each tree\u2019s weight estimation to be.","3a789a4d":"# subsample\n##### : float (default=1)\n\nSubsample ratio of the training instance.","0d31ad77":"The optimum value for the ratio of columns for split is 0.4.","3faf96b5":"# colsample_bytree\n##### : float (default=1)\n\nSubsample ratio of columns when constructing each tree."}}