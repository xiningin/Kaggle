{"cell_type":{"9b276fe6":"code","9dc8c227":"code","cdb0e351":"code","c7a5a84e":"code","8931a42b":"code","b4ed06af":"code","0c31b806":"code","d350a187":"code","a29f9284":"code","1e2b386a":"code","9624a4cf":"code","ae8b4d19":"code","a3939f63":"code","26f261bb":"code","40ba7e44":"code","46279d4f":"code","d750fd06":"code","332ef316":"code","a6f27869":"code","2d1a47ba":"code","a287420f":"code","cf65324d":"code","03a4ebf7":"code","73688a2b":"code","1b658e67":"code","d5ee52a7":"code","cc7c0d38":"code","b19c5060":"code","7b2b1ed3":"code","04156b36":"code","a214e9fc":"code","a5c00345":"code","73320d46":"code","009bdb5e":"code","88b8402c":"code","150625d9":"code","5efcf668":"code","d03f639f":"markdown"},"source":{"9b276fe6":"# !pip install vit-pytorch","9dc8c227":"!pip install einops","cdb0e351":"import torch\n# from vit_pytorch import ViT, MAE\nfrom glob import glob\nfrom torch.utils.data import Dataset, DataLoader\nimport os\nfrom skimage import io, transform\nfrom tqdm import tqdm_notebook as tqdm\nimport pandas as pd\nimport torchvision\nfrom torchvision import transforms\nimport numpy as np\nfrom PIL import Image\nimport cv2\nimport matplotlib.pyplot as plt\nfrom torch import nn\nimport torch.nn.functional as F\nfrom einops import rearrange, repeat\nfrom einops.layers.torch import Rearrange\nfrom matplotlib import pyplot as plt\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split","c7a5a84e":"from skimage import io\nim = io.imread('..\/input\/neuron\/train-input.tif')\nprint(im.shape)","8931a42b":"d_imag = im[3]","b4ed06af":"plt.imshow(cv2.resize(d_imag, dsize=(256, 256), interpolation=cv2.INTER_CUBIC))","0c31b806":"!mkdir seperated","d350a187":"for x in range(im.shape[0]):\n    with open('.\/seperated\/train-input'+str(x)+'.npy', 'wb') as f:\n        np.save(f, im[x])\n####################################################","a29f9284":"image = glob(\".\/seperated\/*.npy\")\n","1e2b386a":"# image = glob(\"..\/input\/imagenetmini-1000\/imagenet-mini\/train\/n01440764\/*\/*.jpeg\")","9624a4cf":"# img_opti = []\n# for i in image:\n#     img = Image.open(i)\n# #     img = img.resize((256, 256))\n#     if np.asarray(img).shape[-1] ==3:\n#         print(img.size)\n# #         img_opti.append(i)\n# #     if(len(img_opti)==1000):\n# #         break","ae8b4d19":"image = pd.DataFrame(image)","a3939f63":"train, val = train_test_split(image, test_size=0.2)","26f261bb":"# img = Image.open(\"..\/input\/imagenetmini-1000\/imagenet-mini\/train\/n01440764\/n01440764_11170.JPEG\")","40ba7e44":"# np.asarray(img).shape","46279d4f":"# image = np.load(\".\/seperated\/train-input2.npy\")\n# image = np.resize(image, (256, 256))","d750fd06":"# image = image.reshape((256,256,1))","332ef316":"# np.stack((image, image, image), axis=2).shape","a6f27869":"# image.shape","2d1a47ba":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","a287420f":"class FaceLandmarksDataset(Dataset):\n    \"\"\"Face Landmarks dataset.\"\"\"\n\n    def __init__(self,input_dir, transform=None):\n        \"\"\"\n        Args:\n            csv_file (string): Path to the csv file with annotations.\n            root_dir (string): Directory with all the images.\n            transform (callable, optional): Optional transform to be applied\n                on a sample.\n        \"\"\"\n        self.image_list = input_dir\n#         self.landmarks_frame = pd.read_csv(csv_file)\n#         self.root_dir = root_dir\n        self.transform = transforms.Compose([\n                                               transforms.ToTensor(),\n                                           ])\n\n    def __len__(self):\n        return len(self.image_list)\n\n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n\n        img_name = os.path.join(self.image_list.iloc[idx, 0])\n        \n        image = np.load(img_name)\n        image = cv2.resize(image, dsize=(256, 256), interpolation=cv2.INTER_CUBIC)\n#         image = np.resize(image, (256, 256))\n#         image = image.reshape((256,256,1))\n#         image = np.stack((image, image, image), axis=2)\n#         print(np.asarray(image).shape)\n#         image = io.imread(img_name)\n#         image = np.asarray(image)\n#         image = (image - np.amin(image))-(np.amax(image)-np.amin(image)+0.0001)\n#         landmarks = self.landmarks_frame.iloc[idx, 1:]\n#         landmarks = np.array([landmarks])\n#         landmarks = landmarks.astype('float').reshape(-1, 2)\n#         sample = {'image': image, 'landmarks': landmarks}\n\n        if self.transform:\n            image = self.transform(image)\n            \n#         print(image.shape)\n        return image","cf65324d":"# train_set = FaceLandmarksDataset(image)\ntrain_set = FaceLandmarksDataset(train)\nval_set = FaceLandmarksDataset(val)\n# val_set = FaceLandmarksDataset(glob(\"..\/input\/imagenetmini-1000\/imagenet-mini\/val\/*\/*.JPEG\"))","03a4ebf7":"BATCH_SIZE = 1\nWORKERS = 0\nSHUFFLE = True\ntrain_loader = DataLoader(train_set,  batch_size=BATCH_SIZE,\n                        shuffle=SHUFFLE, num_workers=WORKERS)\nval_loader = DataLoader(val_set,  batch_size=BATCH_SIZE,\n                        shuffle=SHUFFLE, num_workers=WORKERS)","73688a2b":"# helpers\n\ndef pair(t):\n    return t if isinstance(t, tuple) else (t, t)\n\n# classes\n\nclass PreNorm(nn.Module):\n    def __init__(self, dim, fn):\n        super().__init__()\n        self.norm = nn.LayerNorm(dim)\n        self.fn = fn\n    def forward(self, x, **kwargs):\n        return self.fn(self.norm(x), **kwargs)\n\nclass FeedForward(nn.Module):\n    def __init__(self, dim, hidden_dim, dropout = 0.):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(dim, hidden_dim),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim, dim),\n            nn.Dropout(dropout)\n        )\n    def forward(self, x):\n        return self.net(x)\n\nclass Attention(nn.Module):\n    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n        super().__init__()\n        inner_dim = dim_head *  heads\n        project_out = not (heads == 1 and dim_head == dim)\n\n        self.heads = heads\n        self.scale = dim_head ** -0.5\n\n        self.attend = nn.Softmax(dim = -1)\n        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim),\n            nn.Dropout(dropout)\n        ) if project_out else nn.Identity()\n\n    def forward(self, x):\n        qkv = self.to_qkv(x).chunk(3, dim = -1)\n        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n\n        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n\n        attn = self.attend(dots)\n\n        out = torch.matmul(attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\nclass Transformer(nn.Module):\n    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n        super().__init__()\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)),\n                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))\n            ]))\n    def forward(self, x):\n        for attn, ff in self.layers:\n            x = attn(x) + x\n            x = ff(x) + x\n        return x\n\nclass ViT(nn.Module):\n    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, pool = 'cls', channels = 1, dim_head = 64, dropout = 0., emb_dropout = 0.):\n        super().__init__()\n        image_height, image_width = pair(image_size)\n        patch_height, patch_width = pair(patch_size)\n        print(patch_height, patch_width)\n\n        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n\n        num_patches = (image_height \/\/ patch_height) * (image_width \/\/ patch_width)\n        patch_dim = channels * patch_height * patch_width\n        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n\n        self.to_patch_embedding = nn.Sequential(\n            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width),\n            nn.Linear(patch_dim, dim),\n        )\n\n        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n        self.dropout = nn.Dropout(emb_dropout)\n\n        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n\n        self.pool = pool\n        self.to_latent = nn.Identity()\n\n        self.mlp_head = nn.Sequential(\n            nn.LayerNorm(dim),\n            nn.Linear(dim, num_classes)\n        )\n\n    def forward(self, img):\n        x = self.to_patch_embedding(img)\n        \n        b, n, _ = x.shape\n    \n        cls_tokens = repeat(self.cls_token, '() n d -> b n d', b = b)\n        x = torch.cat((cls_tokens, x), dim=1)\n        x += self.pos_embedding[:, :(n + 1)]\n        x = self.dropout(x)\n\n        x = self.transformer(x)\n\n        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n\n        x = self.to_latent(x)\n        return self.mlp_head(x)","1b658e67":"v = ViT(\n    image_size = 256,\n    patch_size = 8,\n    num_classes = 1000,\n    dim = 1024,\n    depth = 6,\n    heads = 8,\n    channels=1,\n    mlp_dim = 2048\n)\n\nif torch.cuda.is_available():\n    v = v.cuda()","d5ee52a7":"class MAE(nn.Module):\n    def __init__(\n        self,\n        *,\n        encoder,\n        decoder_dim,\n        masking_ratio = 0.75,\n        decoder_depth = 1,\n        decoder_heads = 8,\n        decoder_dim_head = 64\n    ):\n        super().__init__()\n        assert masking_ratio > 0 and masking_ratio < 1, 'masking ratio must be kept between 0 and 1'\n        self.masking_ratio = masking_ratio\n\n        # extract some hyperparameters and functions from encoder (vision transformer to be trained)\n        \n        self.encoder = encoder\n        num_patches, encoder_dim = encoder.pos_embedding.shape[-2:]\n        self.to_patch, self.patch_to_emb = encoder.to_patch_embedding[:2]  ## seperating the patch embedding list\n        pixel_values_per_patch = self.patch_to_emb.weight.shape[-1] ##3072\n\n        # decoder parameters\n\n        self.enc_to_dec = nn.Linear(encoder_dim, decoder_dim) if encoder_dim != decoder_dim else nn.Identity()\n        self.mask_token = nn.Parameter(torch.randn(decoder_dim))\n        self.decoder = Transformer(dim = decoder_dim, depth = decoder_depth, heads = decoder_heads, dim_head = decoder_dim_head, mlp_dim = decoder_dim * 4)\n        self.decoder_pos_emb = nn.Embedding(num_patches, decoder_dim)\n        self.to_pixels = nn.Linear(decoder_dim, pixel_values_per_patch)\n#         self.to_image = nn.Sequential(\n#             Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width),\n#             nn.Linear(patch_dim, dim),\n#         )\n\n    def forward(self, img):\n        device = img.device\n\n        # get patches\n\n        patches = self.to_patch(img) #convert image to patches [100, 64, 3072]\n#         print(patches.shape, img.shape)\n        batch, num_patches, *_ = patches.shape\n        #batches = 100, num_patches = 64\n\n        # patch to encoder tokens and add positions\n\n        tokens = self.patch_to_emb(patches) #change num_patches to dim i.e from 3072 to 1024\n        #(100,64,1024)\n        tokens = tokens + self.encoder.pos_embedding[:, 1:(num_patches + 1)] # (1, 64, 1024) of pos_embedding is added to token\n#         print(1, (self.encoder.pos_embedding.shape),(self.encoder.pos_embedding[:, 1:(num_patches + 1)].shape))\n        # add embedded pathces to ;to randomly initialised positional embeddings\n        #(100,64,1024)\n\n        # calculate of patches needed to be masked, and get random indices, dividing it up for mask vs unmasked\n\n        num_masked = int(self.masking_ratio * num_patches) #patches that has to masked \n        rand_indices = torch.rand(batch, num_patches, device = device).argsort(dim = -1) # indices are arranged in by the order in which they are appearing\n        #rand_indices shape is (batch, num_patches) i.e. 100,64 \n        masked_indices, unmasked_indices = rand_indices[:, :num_masked], rand_indices[:, num_masked:]\n        #masked indices = (batch, masked), unmasked indices = (batch, unmasked)\n        # get the unmasked tokens to be encoded\n#         print(masked_indices)\n        batch_range = torch.arange(batch, device = device)[:, None]\n        tokens = tokens[batch_range, unmasked_indices]\n#         print(tokens)\n        # get the patches to be masked for the final reconstruction loss\n\n        masked_patches = patches[batch_range, masked_indices]\n#         print(1, masked_patches.shape) \n\n        # attend with vision transformer\n\n        encoded_tokens = self.encoder.transformer(tokens)\n        \n        # project encoder to decoder dimensions, if they are not equal - the paper says you can get away with a smaller dimension for decoder\n\n        decoder_tokens = self.enc_to_dec(encoded_tokens)\n\n        # repeat mask tokens for number of masked, and add the positions using the masked indices derived above\n\n        mask_tokens = repeat(self.mask_token, 'd -> b n d', b = batch, n = num_masked)\n        mask_tokens = mask_tokens + self.decoder_pos_emb(masked_indices)\n\n        # concat the masked tokens to the decoder tokens and attend with decoder\n\n        decoder_tokens = torch.cat((mask_tokens, decoder_tokens), dim = 1)\n        decoded_tokens = self.decoder(decoder_tokens)\n\n        # splice out the mask tokens and project to pixel values\n\n        mask_tokens = decoded_tokens[:, :num_masked]\n        pred_pixel_values = self.to_pixels(mask_tokens)\n        return pred_pixel_values, masked_patches, patches, masked_indices\n\n#         calculate reconstruction loss\n\n#         recon_loss = F.mse_loss(pred_pixel_values, masked_patches)\n#         return recon_loss","cc7c0d38":"\nmae = MAE(\n    encoder = v,\n    masking_ratio = 0.75,   # the paper recommended 75% masked patches\n    decoder_dim = 512,      # paper showed good results with just 512\n    decoder_depth = 6       # anywhere from 1 to 8\n)\nif torch.cuda.is_available():\n    mae = mae.cuda()","b19c5060":"criterion = nn.MSELoss()\n# if torch.cuda.is_available():\n#     criterion = criterion.cuda()\noptimizer = torch.optim.Adam(mae.parameters(), lr=0.001)","7b2b1ed3":"var = np.empty(())","04156b36":"# loss = mae(images)\n# # loss.backward()\npred = np.array\npatchess = np.array\n\nfor epoch in range(300):  # loop over the dataset multiple times\n    i=0\n    running_loss = 0.0\n    train_loss = 0.0\n    for data in tqdm(train_loader):\n        # get the inputs; data is a list of [inputs, labels]\n        data = data.to(device)\n        images = data\n        i+=1\n\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n#         outputs = net(inputs)\n#         print(images.shape)\n#         print(images.shape)\n        pred, masked,patchess, indices = mae(images)\n#         print(pred.shape)\n        loss = criterion(pred, masked)\n        loss.backward()\n        optimizer.step()\n        if epoch%60==0:\n            torch.save(v.state_dict(), '.\/trained-vit'+str(epoch)+'.pt')\n            torch.save(mae, \".\/model\"+str(epoch))\n        # print statistics\n        running_loss += loss.item()\n    print('[%d] loss: %.3f' %\n            (epoch + 1, running_loss))\n    \n    for data in tqdm(val_loader):\n        # get the inputs; data is a list of [inputs, labels]\n        data = data.to(device)\n        images = data\n        i+=1\n\n\n        pred, masked,patchess, indices = mae(images)\n        loss = criterion(pred, masked)\n        if epoch%60==0:\n            torch.save(v.state_dict(), '.\/trained-vit'+str(epoch)+'.pt')\n            torch.save(mae, \".\/model\"+str(epoch))\n        # print statistics\n        running_loss += loss.item()\n        \n    print('[%d] loss: %.3f' %\n            (epoch + 1, train_loss))\n\nprint('Finished Training')\n# that's all!\n# do the above in a for loop many times with a lot of images and your vision transformer will learn\n\n# save your improved vision transformer\ntorch.save(v.state_dict(), '.\/trained-vit.pt')","a214e9fc":"you = Rearrange('b (h w) (p1 p2 c) -> b c (h p1) (w p2)', h=32, w = 32, p1 = 8, p2 = 8, c = 1)","a5c00345":"orig = patchess.cpu().detach().numpy()\norig1 = you(orig)","73320d46":"\n# orig = orig.reshape((100, 1, 256,256))\n\nplt.imshow(orig1[0][0], cmap='gray')\nnp.save(\".\/original.npy\",orig1)","009bdb5e":"batch_range = torch.arange(BATCH_SIZE)[:, None]\npatchess[batch_range, indices] = pred","88b8402c":"imga = patchess.cpu().detach().numpy()\nimga = you(imga)\n# img = imga.reshape((100, 1, 256,256))\nplt.imshow(imga[0][0], cmap='gray')\nnp.save(\".\/outputImage.npy\",imga)","150625d9":"# running_loss*2","5efcf668":"# for data in train_loader:\n# #     print(data[0].shape)\n#     image = np.array(data[0])\n#     print(image.shape)\n# #     image = image.reshape((256,256,3))\n# #     image = Image.fromarray(image)\n#     im = Image.fromarray(np.uint8(image*255))\n#     im.save(\".\/file.jpeg\" + \".jpg\", \"JPEG\")","d03f639f":"## 2D slices splitting"}}