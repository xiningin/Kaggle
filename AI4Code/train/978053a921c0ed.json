{"cell_type":{"347db5bf":"code","9eaf94ba":"code","f2fe2344":"code","c5a91f35":"code","a4ff7c01":"code","0acf0fb9":"code","72684aa4":"code","8e6af721":"code","f6e33dd5":"markdown","b78a78ce":"markdown","cde75c09":"markdown","4729bb9e":"markdown","01b39e2e":"markdown","93c99e3c":"markdown"},"source":{"347db5bf":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport torchvision\nimport torchvision.transforms as transforms\nfrom tqdm.auto import tqdm","9eaf94ba":"transform = transforms.Compose([\n   transforms.ToTensor(),\n   transforms.Normalize((0.1307,), (0.3081,))\n])\n\ntrainset = torchvision.datasets.MNIST(root='.\/data', train=True, download=True, transform=transform)\ntestset = torchvision.datasets.MNIST(root='.\/data', train=False, download=True, transform=transform)","f2fe2344":"def train_mnist(args, reporter):\n    # get variables from args\n    lr = args.lr\n    wd = args.wd\n    epochs = args.epochs\n    net = args.net\n    print('lr: {}, wd: {}'.format(lr, wd))\n\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    # Model\n    net = net.to(device)\n\n    if device == 'cuda':\n        net = nn.DataParallel(net)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.SGD(net.parameters(), lr=args.lr, momentum=0.9, weight_decay=wd)\n\n    # datasets and dataloaders\n    trainset = torchvision.datasets.MNIST(root='.\/data', train=True, download=False, transform=transform)\n    trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n\n    testset = torchvision.datasets.MNIST(root='.\/data', train=False, download=False, transform=transform)\n    testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False, num_workers=2)\n\n    # Training\n    def train(epoch):\n        net.train()\n        train_loss, correct, total = 0, 0, 0\n        for batch_idx, (inputs, targets) in enumerate(trainloader):\n            inputs, targets = inputs.to(device), targets.to(device)\n            optimizer.zero_grad()\n            outputs = net(inputs)\n            loss = criterion(outputs, targets)\n            loss.backward()\n            optimizer.step()\n\n    def test(epoch):\n        net.eval()\n        test_loss, correct, total = 0, 0, 0\n        with torch.no_grad():\n            for batch_idx, (inputs, targets) in enumerate(testloader):\n                inputs, targets = inputs.to(device), targets.to(device)\n                outputs = net(inputs)\n                loss = criterion(outputs, targets)\n\n                test_loss += loss.item()\n                _, predicted = outputs.max(1)\n                total += targets.size(0)\n                correct += predicted.eq(targets).sum().item()\n\n        acc = 100.*correct\/total\n        # 'epoch' reports the number of epochs done\n        reporter(epoch=epoch+1, accuracy=acc)\n\n    for epoch in tqdm(range(0, epochs)):\n        train(epoch)\n        test(epoch)","c5a91f35":"import autogluon.core as ag\n\n@ag.obj(\n    hidden_conv=ag.space.Int(6, 12),\n    hidden_fc=ag.space.Categorical(80, 120, 160),\n)\nclass Net(nn.Module):\n    def __init__(self, hidden_conv, hidden_fc):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, hidden_conv, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(hidden_conv, 16, 5)\n        self.fc1 = nn.Linear(16 * 4 * 4, hidden_fc)\n        self.fc2 = nn.Linear(hidden_fc, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(-1, 16 * 4 * 4)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x","a4ff7c01":"@ag.args(\n    lr = ag.space.Real(0.01, 0.2, log=True),\n    wd = ag.space.Real(1e-4, 5e-4, log=True),\n    net = Net(),\n    epochs=5,\n)\ndef ag_train_mnist(args, reporter):\n    return train_mnist(args, reporter)","0acf0fb9":"myscheduler = ag.scheduler.FIFOScheduler(\n    ag_train_mnist,\n    num_trials=5,\n    time_attr='epoch',\n    reward_attr='accuracy')\nprint(myscheduler)","72684aa4":"myscheduler.run()\nmyscheduler.join_jobs()","8e6af721":"myscheduler.get_training_curves(plot=True,use_legend=False)\nprint('The Best Configuration and Accuracy are: {}, {}'.format(myscheduler.get_best_config(), myscheduler.get_best_reward()))","f6e33dd5":"We can simply add a decorator autogluon.args() to convert the train_mnist function argument values to be tuned by AutoGluon\u2019s hyperparameter optimizer.","b78a78ce":"The following train_mnist function represents normal training code a user would write for training on MNIST dataset.","cde75c09":"[AutoGluon](https:\/\/github.com\/awslabs\/autogluon) is a famous AutoML library from AWS.","4729bb9e":"# Pytorch AutoML on MNIST with AutoGluon [~99%]","01b39e2e":"Let\u2019s define a \u2018dynamic\u2019 network with searchable configurations by simply adding a decorator autogluon.obj().","93c99e3c":"Download MNIST from the `torchvision.datasets`. We first apply standard image transforms to our training and validation data."}}