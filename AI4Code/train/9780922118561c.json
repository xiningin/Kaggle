{"cell_type":{"6d6bfc35":"code","1ea8fa5e":"code","916e3142":"code","37663e2e":"code","4eafef14":"code","6fa305bc":"code","9c35cb16":"code","14a9264a":"code","c18fd116":"code","95bcce40":"code","2af4e8ce":"code","409ad80b":"code","82386fea":"markdown"},"source":{"6d6bfc35":"import sys\nimport numpy as np\nimport pandas as pd\nfrom math import ceil\nfrom tqdm import trange\nfrom subprocess import call\nfrom itertools import islice\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import normalize\nfrom sklearn.neighbors import NearestNeighbors\nfrom scipy.sparse import csr_matrix, dok_matrix\nfrom sklearn.preprocessing import LabelEncoder","1ea8fa5e":"class BPR:\n    \"\"\"\n    Bayesian Personalized Ranking (BPR) for implicit feedback data\n\n    Parameters\n    ----------\n    learning_rate : float, default 0.01\n        learning rate for gradient descent\n\n    n_factors : int, default 20\n        Number\/dimension of user and item latent factors\n\n    n_iters : int, default 15\n        Number of iterations to train the algorithm\n        \n    batch_size : int, default 1000\n        batch size for batch gradient descent, the original paper\n        uses stochastic gradient descent (i.e., batch size of 1),\n        but this can make the training unstable (very sensitive to\n        learning rate)\n\n    reg : int, default 0.01\n        Regularization term for the user and item latent factors\n\n    seed : int, default 1234\n        Seed for the randomly initialized user, item latent factors\n\n    verbose : bool, default True\n        Whether to print progress bar while training\n\n    Attributes\n    ----------\n    user_factors : 2d ndarray, shape [n_users, n_factors]\n        User latent factors learnt\n\n    item_factors : 2d ndarray, shape [n_items, n_factors]\n        Item latent factors learnt\n\n    References\n    ----------\n    S. Rendle, C. Freudenthaler, Z. Gantner, L. Schmidt-Thieme \n    Bayesian Personalized Ranking from Implicit Feedback\n    - https:\/\/arxiv.org\/abs\/1205.2618\n    \"\"\"\n    def __init__(self, learning_rate = 0.01, n_factors = 15, n_iters = 10, \n                 batch_size = 1000, reg = 0.01, seed = 1234, verbose = True):\n        self.reg = reg\n        self.seed = seed\n        self.verbose = verbose\n        self.n_iters = n_iters\n        self.n_factors = n_factors\n        self.batch_size = batch_size\n        self.learning_rate = learning_rate\n        \n        # to avoid re-computation at predict\n        self._prediction = None\n        \n    def fit(self, ratings):\n        \"\"\"\n        Parameters\n        ----------\n        ratings : scipy sparse csr_matrix, shape [n_users, n_items]\n            sparse matrix of user-item interactions\n        \"\"\"\n        indptr = ratings.indptr\n        indices = ratings.indices\n        n_users, n_items = ratings.shape\n        \n        # ensure batch size makes sense, since the algorithm involves\n        # for each step randomly sample a user, thus the batch size\n        # should be smaller than the total number of users or else\n        # we would be sampling the user with replacement\n        batch_size = self.batch_size\n        if n_users < batch_size:\n            batch_size = n_users\n            sys.stderr.write('WARNING: Batch size is greater than number of users,'\n                             'switching to a batch size of {}\\n'.format(n_users))\n\n        batch_iters = n_users \/\/ batch_size\n        \n        # initialize random weights\n        rstate = np.random.RandomState(self.seed)\n        self.user_factors = rstate.normal(size = (n_users, self.n_factors))\n        self.item_factors = rstate.normal(size = (n_items, self.n_factors))\n        \n        # progress bar for training iteration if verbose is turned on\n        loop = range(self.n_iters)\n        if self.verbose:\n            loop = trange(self.n_iters, desc = self.__class__.__name__)\n        \n        for _ in loop:\n            for _ in range(batch_iters):\n                sampled = self._sample(n_users, n_items, indices, indptr)\n                sampled_users, sampled_pos_items, sampled_neg_items = sampled\n                self._update(sampled_users, sampled_pos_items, sampled_neg_items)\n\n        return self\n    \n    def _sample(self, n_users, n_items, indices, indptr):\n        \"\"\"sample batches of random triplets u, i, j\"\"\"\n        sampled_pos_items = np.zeros(self.batch_size, dtype = np.int)\n        sampled_neg_items = np.zeros(self.batch_size, dtype = np.int)\n        sampled_users = np.random.choice(\n            n_users, size = self.batch_size, replace = False)\n\n        for idx, user in enumerate(sampled_users):\n            pos_items = indices[indptr[user]:indptr[user + 1]]\n            pos_item = np.random.choice(pos_items)\n            neg_item = np.random.choice(n_items)\n            while neg_item in pos_items:\n                neg_item = np.random.choice(n_items)\n\n            sampled_pos_items[idx] = pos_item\n            sampled_neg_items[idx] = neg_item\n\n        return sampled_users, sampled_pos_items, sampled_neg_items\n                \n    def _update(self, u, i, j):\n        \"\"\"\n        update according to the bootstrapped user u, \n        positive item i and negative item j\n        \"\"\"\n        user_u = self.user_factors[u]\n        item_i = self.item_factors[i]\n        item_j = self.item_factors[j]\n        \n        # decompose the estimator, compute the difference between\n        # the score of the positive items and negative items; a\n        # naive implementation might look like the following:\n        # r_ui = np.diag(user_u.dot(item_i.T))\n        # r_uj = np.diag(user_u.dot(item_j.T))\n        # r_uij = r_ui - r_uj\n        \n        # however, we can do better, so\n        # for batch dot product, instead of doing the dot product\n        # then only extract the diagonal element (which is the value\n        # of that current batch), we perform a hadamard product, \n        # i.e. matrix element-wise product then do a sum along the column will\n        # be more efficient since it's less operations\n        # http:\/\/people.revoledu.com\/kardi\/tutorial\/LinearAlgebra\/HadamardProduct.html\n        # r_ui = np.sum(user_u * item_i, axis = 1)\n        #\n        # then we can achieve another speedup by doing the difference\n        # on the positive and negative item up front instead of computing\n        # r_ui and r_uj separately, these two idea will speed up the operations\n        # from 1:14 down to 0.36\n        r_uij = np.sum(user_u * (item_i - item_j), axis = 1)\n        sigmoid = np.exp(-r_uij) \/ (1.0 + np.exp(-r_uij))\n        \n        # repeat the 1 dimension sigmoid n_factors times so\n        # the dimension will match when doing the update\n        sigmoid_tiled = np.tile(sigmoid, (self.n_factors, 1)).T\n\n        # update using gradient descent\n        grad_u = sigmoid_tiled * (item_j - item_i) + self.reg * user_u\n        grad_i = sigmoid_tiled * -user_u + self.reg * item_i\n        grad_j = sigmoid_tiled * user_u + self.reg * item_j\n        self.user_factors[u] -= self.learning_rate * grad_u\n        self.item_factors[i] -= self.learning_rate * grad_i\n        self.item_factors[j] -= self.learning_rate * grad_j\n        return self\n\n    def predict(self):\n        \"\"\"\n        Obtain the predicted ratings for every users and items\n        by doing a dot product of the learnt user and item vectors.\n        The result will be cached to avoid re-computing it every time\n        we call predict, thus there will only be an overhead the first\n        time we call it. Note, ideally you probably don't need to compute\n        this as it returns a dense matrix and may take up huge amounts of\n        memory for large datasets\n        \"\"\"\n        if self._prediction is None:\n            self._prediction = self.user_factors.dot(self.item_factors.T)\n\n        return self._prediction\n\n    def _predict_user(self, user):\n        \"\"\"\n        returns the predicted ratings for the specified user,\n        this is mainly used in computing evaluation metric\n        \"\"\"\n        user_pred = self.user_factors[user].dot(self.item_factors.T)\n        return user_pred\n\n    def recommend(self, ratings, N = 5):\n        \"\"\"\n        Returns the top N ranked items for given user id,\n        excluding the ones that the user already liked\n        \n        Parameters\n        ----------\n        ratings : scipy sparse csr_matrix, shape [n_users, n_items]\n            sparse matrix of user-item interactions \n        \n        N : int, default 5\n            top-N similar items' N\n        \n        Returns\n        -------\n        recommendation : 2d ndarray, shape [number of users, N]\n            each row is the top-N ranked item for each query user\n        \"\"\"\n        n_users = ratings.shape[0]\n        recommendation = np.zeros((n_users, N), dtype = np.uint32)\n        for user in range(n_users):\n            top_n = self._recommend_user(ratings, user, N)\n            recommendation[user] = top_n\n\n        return recommendation\n\n    def _recommend_user(self, ratings, user, N):\n        \"\"\"the top-N ranked items for a given user\"\"\"\n        scores = self._predict_user(user)\n\n        # compute the top N items, removing the items that the user already liked\n        # from the result and ensure that we don't get out of bounds error when \n        # we ask for more recommendations than that are available\n        liked = set(ratings[user].indices)\n        count = N + len(liked)\n        if count < scores.shape[0]:\n\n            # when trying to obtain the top-N indices from the score,\n            # using argpartition to retrieve the top-N indices in \n            # unsorted order and then sort them will be faster than doing\n            # straight up argort on the entire score\n            # http:\/\/stackoverflow.com\/questions\/42184499\/cannot-understand-numpy-argpartition-output\n            ids = np.argpartition(scores, -count)[-count:]\n            best_ids = np.argsort(scores[ids])[::-1]\n            best = ids[best_ids]\n        else:\n            best = np.argsort(scores)[::-1]\n\n        top_n = list(islice((rec for rec in best if rec not in liked), N))\n        return top_n\n    \n    def get_similar_items(self, N = 5, item_ids = None):\n        \"\"\"\n        return the top N similar items for itemid, where\n        cosine distance is used as the distance metric\n        \n        Parameters\n        ----------\n        N : int, default 5\n            top-N similar items' N\n            \n        item_ids : 1d iterator, e.g. list or numpy array, default None\n            the item ids that we wish to find the similar items\n            of, the default None will compute the similar items\n            for all the items\n        \n        Returns\n        -------\n        similar_items : 2d ndarray, shape [number of query item_ids, N]\n            each row is the top-N most similar item id for each\n            query item id\n        \"\"\"\n        # cosine distance is proportional to normalized euclidean distance,\n        # thus we normalize the item vectors and use euclidean metric so\n        # we can use the more efficient kd-tree for nearest neighbor search;\n        # also the item will always to nearest to itself, so we add 1 to \n        # get an additional nearest item and remove itself at the end\n        normed_factors = normalize(self.item_factors)\n        knn = NearestNeighbors(n_neighbors = N + 1, metric = 'euclidean')\n        knn.fit(normed_factors)\n\n        # returns a distance, index tuple,\n        # we don't actually need the distance\n        if item_ids is not None:\n            normed_factors = normed_factors[item_ids]\n\n        _, items = knn.kneighbors(normed_factors)\n        similar_items = items[:, 1:].astype(np.uint32)\n        return similar_items\n\n\ndef create_matrix(data, users_col, items_col, ratings_col, threshold = None):\n    \"\"\"\n    creates the sparse user-item interaction matrix,\n    if the data is not in the format where the interaction only\n    contains the positive items (indicated by 1), then use the \n    threshold parameter to determine which items are considered positive\n    \n    Parameters\n    ----------\n    data : DataFrame\n        implicit rating data\n\n    users_col : str\n        user column name\n\n    items_col : str\n        item column name\n    \n    ratings_col : str\n        implicit rating column name\n\n    threshold : int, default None\n        threshold to determine whether the user-item pair is \n        a positive feedback\n\n    Returns\n    -------\n    ratings : scipy sparse csr_matrix, shape [n_users, n_items]\n        user\/item ratings matrix\n\n    data : DataFrame\n        implict rating data that retains only the positive feedback\n        (if specified to do so)\n    \"\"\"\n    if threshold is not None:\n        data = data[data[ratings_col] >= threshold]\n        data[ratings_col] = 1\n    \n    for col in (items_col, users_col, ratings_col):\n        data[col] = data[col].astype('category')\n\n    ratings = csr_matrix((data[ratings_col],\n                          (data[users_col].cat.codes, data[items_col].cat.codes)))\n    ratings.eliminate_zeros()\n    return ratings, data\n\nprint('Class BPR is defined')\nprint('Creating Matrix is defined')","916e3142":"data_aday_log = pd.read_csv('..\/input\/datathon-guess-the-last-one\/data_aday_log.csv')\ndata_job_details = pd.read_csv('..\/input\/datathon-guess-the-last-one\/data_job_details.csv')\ndata_cv_details = pd.read_csv('..\/input\/datathon-guess-the-last-one\/data_cv_details.csv')\ndata_aday_log = pd.read_csv('..\/input\/datathon-guess-the-last-one\/data_aday_log.csv')\nsample_ = pd.read_csv('..\/input\/datathon-guess-the-last-one\/sample_.csv')\nson2_basvurular_test = pd.read_csv('..\/input\/datathon-guess-the-last-one\/son2_basvurular_test.csv')\n\ndata_aday_log = data_aday_log.iloc[:,1:3]\ndata_aday_log.loc[:,'rating'] = 1\ndata_aday_log['jobId'] = 'J' + data_aday_log['jobId'].astype(str)","37663e2e":"user_label_encoder = LabelEncoder()\nuser_ids = user_label_encoder.fit_transform(data_aday_log.jobseekerId)\nproduct_label_encoder = LabelEncoder()\nproduct_ids = product_label_encoder.fit_transform(data_aday_log.jobId)\njobIdJobseekerIdMatrix = csr_matrix(([1]*len(user_ids), (product_ids, user_ids)))\njobseekerIdjobIdMatrix = csr_matrix(jobIdJobseekerIdMatrix.T)","4eafef14":"jobseekerIdjobIdMatrix","6fa305bc":"# parameters were randomly chosen\nbpr_params = {'reg': 0.01,\n              'learning_rate': 0.1,\n              'n_iters': 160,\n              'n_factors': 15,\n              'batch_size': 100}\n\nbpr = BPR(**bpr_params)\nbpr.fit(jobseekerIdjobIdMatrix)","9c35cb16":"recommendation_array = bpr.recommend(jobseekerIdjobIdMatrix, N = 10)","14a9264a":"df_final = pd.DataFrame(recommendation_array)","c18fd116":"df_final.index = user_label_encoder.inverse_transform(df_final.index)\nfor i in range(df_final.shape[1]):\n    df_final.iloc[:, i] = product_label_encoder.inverse_transform(df_final.iloc[:, i])","95bcce40":"submission_ = pd.melt(df_final,ignore_index=False)\nsubmission_ = submission_.reset_index().iloc[:,[0,2]]\nsubmission_['value'] = submission_['value'].str[1:]\nsubmission_ = submission_.rename(columns={\"index\": \"jobseekerId\", \"value\": \"jobId\"})","2af4e8ce":"submission_ = submission_[submission_.jobseekerId.isin(son2_basvurular_test.jobseekerId.unique())]","409ad80b":"submission_.to_csv('submission_bpr.csv',index=False)\npd.read_csv('submission_bpr.csv')","82386fea":"#### \"Bayesian Personalized Ranking\" denerken en \u00e7ok umutlu oldu\u011fum algoritmalardan birisiydi. Bir \u00e7ok benchmarkta \u00e7ok iyiydi. Implement ederken bir hata yapm\u0131\u015f da olabilirim.\n\n#### \u0130lgili linkten hakk\u0131nda okuyabilirsiniz:\n\n#### https:\/\/arxiv.org\/ftp\/arxiv\/papers\/1205\/1205.2618.pdf"}}