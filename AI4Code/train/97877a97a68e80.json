{"cell_type":{"247be217":"code","078df365":"code","bb4e2c02":"code","52d75bf6":"code","db86874f":"code","663fffa8":"code","a87aefcc":"code","5ae3be13":"code","4c3ddbdd":"code","8b81d4c2":"code","9bc1baae":"code","efed5183":"code","754e8e5d":"code","b2b526e7":"code","13a3e8ef":"code","f4950013":"code","65dc8a8e":"code","4a081f2a":"code","c8b36751":"code","3ec5c6c1":"code","5c485167":"code","6e182adc":"code","1e0c89b2":"code","3b88b1dc":"code","e8555189":"code","e7c5ed2d":"code","bc068642":"code","f59d7092":"code","9000c847":"code","a7a3369e":"code","96333927":"code","4ad75dab":"code","15804263":"code","68e30477":"code","02dc3058":"code","96b40819":"code","fdc84ff9":"code","0bf38960":"code","ad13a547":"code","cbc53613":"code","d611ca4a":"code","8c60601b":"code","4325c7ab":"code","bccc56d1":"code","8fbeb3e8":"code","418ffbe8":"code","810f504a":"code","1ba61ba7":"code","2d07f267":"code","11f85231":"code","60f7cc9b":"code","93f56ff3":"code","dbfcc72f":"code","f4349189":"code","c07f0a38":"code","3342bb4f":"code","09c4ddea":"code","9efc4208":"code","6d6b81c3":"code","1ae769e0":"code","980d558a":"code","a6657e72":"code","dfb71351":"code","1a82e6be":"code","54ab82f0":"code","2cc31645":"code","1aefebd2":"code","2e4686b3":"code","0163b3a7":"markdown"},"source":{"247be217":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","078df365":"titanic=pd.read_csv('\/kaggle\/input\/titanic\/train.csv',index_col='PassengerId')\ntitanic_test=pd.read_csv('\/kaggle\/input\/titanic\/test.csv',index_col='PassengerId')\n\ntitanic.head(10)","bb4e2c02":"titanic_test.head()","52d75bf6":"titanic.info()","db86874f":"titanic.describe()","663fffa8":"titanic_test.info()","a87aefcc":"titanic_full=pd.concat([titanic,titanic_test],axis=0)","5ae3be13":"print(titanic.shape)\nprint(titanic_test.shape)\nprint(titanic_full.shape)","4c3ddbdd":"titanic_full.isnull().sum()","8b81d4c2":"# Removing Cabin column becuase out of 891 rows, the \"cabin\" column has 687 missing values.\ntitanic_full=titanic_full.drop('Cabin',axis=1)\ntitanic_full.Embarked.value_counts()\n# Filling the two missing values with the most frequent value for Embarked column\ntitanic_full[['Embarked']]=titanic_full[['Embarked']].fillna('S')","9bc1baae":"# Creating a list for saving the title of people form the name column\nvar=titanic_full['Name'].str.split()\nlist_of_title=[]\nfor x in range(1,(len(titanic_full)+1)):\n    list_of_title.append(var[x][1])","efed5183":"titanic_full['Title']=list_of_title\ntitanic_full.head()","754e8e5d":"titanic_full['Title'].unique()","b2b526e7":"corrections=['Planke,','Billiard,', 'der', 'Walle,',  'Pelsmaeker,', 'Mulder,', 'y',\n       'Steen,', 'Carlo,',  'Ms.',  'Gordon,','Messemaeker,',  'Velde,', 'the',\n       'Shawah,', 'Jonkheer.', 'Melkebeke,', 'Cruyssen,', 'Khalil,',\n       'Palmquist,', 'Brito,']\n\nfor x in corrections:\n    print(titanic_full.loc[titanic_full['Title']==x,'Name'])","13a3e8ef":"#Some titles needed some modifications based on their names\n\na=[19,39,334,1037,154,1084,1236,201,283,287,356,362,907,420,596,800,560,1152,753,760,799,869,874,911,995,1228]\nb=[171,308,506,548,867,965,1112,1261,1306]\nfor x in a:\n    titanic_full.loc[x,'Title']=var[x][2]\n    \nfor y in b:\n    titanic_full.loc[y,'Title']=var[y][3]\n    \n    \ntitanic_full.loc[557,'Title']='Mrs'\ntitanic_full.loc[600,'Title']='Mr'\ntitanic_full.loc[760,'Title']='Mrs'\n\n#Some of the titles like Jonkheer,Col,Capt,etc are being left as it is.","f4950013":"titanic_full['Title']=titanic_full['Title'].str.replace('.','').str.replace(',','')","65dc8a8e":"titanic_full.Title.unique()","4a081f2a":"#Calculating missing age for title - 'Mr' \nfrom sklearn.impute import SimpleImputer\n\nmr=pd.DataFrame(titanic_full.loc[titanic_full['Title']=='Mr','Age'])\n\nmy_imputer = SimpleImputer(strategy='mean')\nimputed_mr=pd.DataFrame(my_imputer.fit_transform(mr[['Age']]))\nimputed_mr.index=mr.index\nimputed_mr.columns=mr.columns","c8b36751":"# Substituting the age for people with title as 'Mr' after finding the average value with the help of simple imputer\ntitanic_full.loc[titanic_full['Title']=='Mr','Age']=imputed_mr['Age']","3ec5c6c1":"# Simple imputer didn't work for title 'Master' so finding the rows and avg age manually.\nmt=pd.DataFrame(titanic_full.loc[titanic_full['Title']=='Master','Age'])\nmaster_rows=list((mt[mt['Age'].isnull()]).index)\navg_age_master=titanic_full[titanic_full['Title']=='Master']['Age'].mean()\nprint(avg_age_master)\nfor row in master_rows:\n    titanic_full.loc[row,'Age']=avg_age_master","5c485167":"#Calculating missing age for title - 'Mrs' \nfrom sklearn.impute import SimpleImputer\n\nmrs=pd.DataFrame(titanic_full.loc[titanic_full['Title']=='Mrs','Age'])\n\nmy_imputer = SimpleImputer(strategy='mean')\nimputed_mrs=pd.DataFrame(my_imputer.fit_transform(mrs[['Age']]))\nimputed_mrs.index=mrs.index\nimputed_mrs.columns=mrs.columns","6e182adc":"# Substituting the age for people with title as 'Mrs' after finding the average value with the help of simple imputer\ntitanic_full.loc[titanic_full['Title']=='Mrs','Age']=imputed_mrs['Age']","1e0c89b2":"#Calculating missing age for title - 'Miss' \nfrom sklearn.impute import SimpleImputer\n\nmiss=pd.DataFrame(titanic_full.loc[titanic_full['Title']=='Miss','Age'])\n\nmy_imputer = SimpleImputer(strategy='mean')\nimputed_miss=pd.DataFrame(my_imputer.fit_transform(miss[['Age']]))\nimputed_miss.index=miss.index\nimputed_miss.columns=miss.columns","3b88b1dc":"# Substituting the age for people with title as 'Miss' after finding the average value with the help of simple imputer\ntitanic_full.loc[titanic_full['Title']=='Miss','Age']=imputed_miss['Age']","e8555189":"# Taking the mean of the rows with title 'Dr' because it has only one missing row \ntitanic_full[titanic_full['Title']=='Dr']","e7c5ed2d":"titanic_full[titanic_full['Title']=='Dr']['Age'].mean()","bc068642":"titanic_full.loc[767,'Age']=43.57","f59d7092":"# The 'Ms' title has only two rows and one of them is missing, so assigning the value of the first row to the missing one.\ntitanic_full[titanic_full['Title']=='Ms']","9000c847":"titanic_full.loc[980,'Age']=28.0","a7a3369e":"titanic_full.loc[titanic_full['Fare'].isnull()]","96333927":"#Finding the mean of fare for plcass3 as it has only one missing row\np=titanic_full.groupby('Pclass').Fare\np.describe()","4ad75dab":"titanic_full.loc[1044,'Fare']=13.302889","15804263":"# No more missing rows\ntitanic_full.isnull().sum()","68e30477":"titanic_full['Age']=round(titanic_full['Age'],1)","02dc3058":"titanic_full.tail()","96b40819":"titanic=titanic_full.loc[1:891]\nprint(titanic.shape)\ntitanic_test=titanic_full.loc[892:1309]\nprint(titanic_test.shape)\ntitanic_test","fdc84ff9":"titanic['Survived']=titanic['Survived'].astype(int)\ntitanic_test=titanic_test.drop('Survived',axis=1)\ntitanic_test.shape","0bf38960":"titanic.head()","ad13a547":"plt.figure(figsize=(9,5))\nsns.set_style('darkgrid')\nsns.countplot(x=titanic['Sex'],hue=titanic['Survived'])\nplt.title('Survival comparision based on gender')","cbc53613":"plt.figure(figsize=(9,5))\nsns.countplot(x=titanic['Pclass'],hue=titanic['Survived'])\nplt.title('Survival comparision based on Pclass')","d611ca4a":"plt.figure(figsize=(9,5))\nsns.countplot(x=titanic['Pclass'],hue=titanic['Sex'])\nplt.title('Population based on Pclass')","8c60601b":"plt.figure(figsize=(9,5))\nsns.countplot(x=titanic['Pclass'],hue=titanic['Embarked'])\nplt.title('Embarked')","4325c7ab":"plt.figure(figsize=(9,5))\nsns.countplot(x=titanic['Sex'],hue=titanic['Embarked'])\nplt.title('Number of male and female embarked')","bccc56d1":"plt.figure(figsize=(9,5))\nsns.stripplot(x=titanic['Sex'],y=titanic['Age'])\nplt.title('Age vs Gender')","8fbeb3e8":"plt.figure(figsize=(9,5))\nsns.stripplot(x=titanic['Pclass'],y=titanic['Fare'])\nplt.title('Fare vs Pclass')","418ffbe8":"# Now finding how many survived from a particular class based on gender","810f504a":"df=titanic[titanic['Pclass']==1]\nplt.figure(figsize=(9,5))\nsns.countplot(x=df['Sex'],hue=df['Survived'])\nplt.title('Survival comparision based on Gender from Class 1')","1ba61ba7":"df=titanic[titanic['Pclass']==2]\nplt.figure(figsize=(9,5))\nsns.countplot(x=df['Sex'],hue=df['Survived'])\nplt.title('Survival comparision based on Gender from Class 2')","2d07f267":"df=titanic[titanic['Pclass']==3]\nplt.figure(figsize=(9,5))\nsns.countplot(x=df['Sex'],hue=df['Survived'])\nplt.title('Survival comparision based on Gender from Class 3')","11f85231":"# Survival comparision based on Title\ntitles=titanic['Title'].unique()\nfor title in titles:\n    plt.figure(figsize=(7,4))\n    df=titanic[titanic['Title']==title]\n\n    sns.countplot(x=df['Title'],hue=df['Survived'])\n    plt.title('Survival of people with title - {}'.format(title))","60f7cc9b":"titanic.head()","93f56ff3":"titanic=titanic.drop(['Name','Ticket','Title','Fare'],axis=1)\ntitanic_test=titanic_test.drop(['Name','Ticket','Title','Fare'],axis=1)\nt=titanic['Sex'].map({'male':0,'female':1})\ntitanic['Sex']=t\n","dbfcc72f":"test=titanic_test['Sex'].map({'male':0,'female':1})\ntitanic_test['Sex']=test","f4349189":"embarked_dummies=pd.get_dummies(titanic.Embarked,prefix='Embarked')\nembarked_dummies_test=pd.get_dummies(titanic_test.Embarked,prefix='Embarked')\n\ntitanic=pd.concat([titanic,embarked_dummies],axis=1)\ntitanic_test=pd.concat([titanic_test,embarked_dummies_test],axis=1)\n\ntitanic=titanic.drop('Embarked',axis=1)\ntitanic_test=titanic_test.drop('Embarked',axis=1)","c07f0a38":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(titanic.loc[:,titanic.columns!='Survived'],titanic['Survived'], \n                test_size = 0.18, random_state = 78)","3342bb4f":"#Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score,classification_report, confusion_matrix,mean_absolute_error\nlr=LogisticRegression(penalty='l2',max_iter=1000)\nlr.fit(X_train,y_train)\npredictions=lr.predict(X_test)\nacc=accuracy_score(y_test,predictions)\nprint(acc)\nmae=mean_absolute_error(y_test,predictions)\nprint(mae)\nprint(classification_report(y_test,predictions))\nprint(confusion_matrix(y_test,predictions))","09c4ddea":"#SVC\nfrom sklearn.svm import SVC\nsvc=SVC(C= 100, gamma= 0.01,kernel= 'rbf')\nsvc.fit(X_train,y_train)\npredictions=svc.predict(X_test)\nacc=accuracy_score(y_test,predictions)\nprint(acc)\nprint(classification_report(y_test,predictions))\nprint(confusion_matrix(y_test,predictions))","9efc4208":"#Using GridSearch for svc \nfrom sklearn.model_selection import GridSearchCV\n  \n\nparam_grid = {'C': [0.1, 1, 10, 100, 1000], \n              'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n              'kernel': ['rbf']} \n  \ngrid = GridSearchCV(SVC(), param_grid, refit = True, verbose = False)\n\ngrid.fit(X_train, y_train)\n\nprint(grid.best_params_)\nprint(grid.best_estimator_)","6d6b81c3":"svc=SVC(C= 100, gamma= 0.01,kernel= 'rbf')\nsvc.fit(X_train,y_train)\npredictions=svc.predict(X_test)\nacc=accuracy_score(y_test,predictions)\nprint(acc)\nmae=mean_absolute_error(y_test,predictions)\nprint(mae)\nprint(classification_report(y_test,predictions))\nprint(confusion_matrix(y_test,predictions))","1ae769e0":"#Random Forest Classifier\nfrom sklearn.ensemble import RandomForestClassifier\nrfc=RandomForestClassifier(random_state=0,max_features='log2',criterion='gini',\n                                 n_estimators=100)\nrfc.fit(X_train,y_train)\npredictions=rfc.predict(X_test)\nacc=accuracy_score(y_test,predictions)\nprint(acc)\nmae=mean_absolute_error(y_test,predictions)\nprint(mae)\nprint(classification_report(y_test,predictions))\nprint(confusion_matrix(y_test,predictions))","980d558a":"#Using GridSearch for RFC \nparam_grid = {\n    'criterion' : ['gini', 'entropy'],\n     'n_estimators': [90,100,110],\n    'max_features' : ['auto', 'sqrt', 'log2'],\n             } \n  \ngrid = GridSearchCV(RandomForestClassifier(), param_grid, refit = True, verbose = False)\n\ngrid.fit(X_train, y_train)\n\nprint(grid.best_params_)\nprint(grid.best_estimator_)","a6657e72":"rfc=RandomForestClassifier(random_state=0,max_features='log2',criterion='gini',\n                                 n_estimators=100)\nrfc.fit(X_train,y_train)\npredictions=rfc.predict(X_test)\nacc=accuracy_score(y_test,predictions)\nprint(acc)\nmae=mean_absolute_error(y_test,predictions)\nprint(mae)","dfb71351":"# XGB Classifier\nfrom xgboost import XGBClassifier\nxgbc=XGBClassifier(n_estimators=90,learning_rate=0.1,random_state=0,gamma=0.1,\n                   eval_metric='logloss',use_label_encoder=False)\nxgbc.fit(X_train, y_train, \n             early_stopping_rounds=5, \n             eval_set=[(X_test, y_test)], \n             verbose=False)\npredictions=xgbc.predict(X_test)\nacc=accuracy_score(y_test,predictions)\nprint(acc)\nprint(classification_report(y_test,predictions))\nprint(confusion_matrix(y_test,predictions))","1a82e6be":"#Using GridSearch for XGBC \nparam_grid = {'learning_rate': [0.1,0.01,0.5], \n              'n_estimators': [90,100,110],\n              'gamma': [ 1,0.1, 0.01]\n             } \n  \ngrid = GridSearchCV(XGBClassifier(use_label_encoder=False,eval_metric='logloss'), param_grid, refit = True,\n                    verbose = False)\n  \n\ngrid.fit(X_train, y_train)\n\nprint(grid.best_params_)\nprint(grid.best_estimator_)","54ab82f0":"xgbc=XGBClassifier(n_estimators=90,learning_rate=0.1,random_state=0,gamma=0.1,\n                   eval_metric='logloss',use_label_encoder=False)\nxgbc.fit(X_train, y_train, \n             early_stopping_rounds=5, \n             eval_set=[(X_test, y_test)], \n             verbose=False)\npredictions=xgbc.predict(X_test)\nacc=accuracy_score(y_test,predictions)\nprint(acc)\nmae=mean_absolute_error(y_test,predictions)\nprint(mae)","2cc31645":"#Gradient Boosting Classifier \nfrom sklearn.ensemble import GradientBoostingClassifier\ngbc=GradientBoostingClassifier(random_state=0,learning_rate= 0.05, max_depth= 4, n_estimators= 100)\ngbc.fit(X_train,y_train)\npredictions=gbc.predict(X_test)\nacc=accuracy_score(y_test,predictions)\nprint(acc)\nprint(classification_report(y_test,predictions))\nprint(confusion_matrix(y_test,predictions))\nmae=mean_absolute_error(y_test,predictions)\nprint(mae)","1aefebd2":"#Using GridSearch for GBC\nparam_grid = {'learning_rate': [0.1,0.05,0.5], \n              'n_estimators': [90,100,500],\n              'max_depth':[2,3,4],\n             } \n  \ngrid = GridSearchCV(GradientBoostingClassifier(), param_grid, refit = True, verbose = False)\n\ngrid.fit(X_train, y_train)\n\nprint(grid.best_params_)\nprint(grid.best_estimator_)","2e4686b3":"# Implementing SVC\npreds_test=svc.predict(titanic_test)\nprint(preds_test)\noutput = pd.DataFrame({'PassengerId': titanic_test.index,\n                       'Survived': preds_test})\noutput.to_csv('submissions.csv', index=False)","0163b3a7":"# Models"}}