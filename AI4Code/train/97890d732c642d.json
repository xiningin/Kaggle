{"cell_type":{"173d1d12":"code","20d6ef3d":"code","c7f1b014":"code","5b6e3a25":"code","6dd24573":"markdown","f7a94282":"markdown","5d7728a9":"markdown","7aba71ee":"markdown"},"source":{"173d1d12":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\ntarget = 'SalePrice'\nX_train = pd.read_csv('..\/input\/home-data-for-ml-course\/train.csv', index_col = 'Id')\nX_test = pd.read_csv('..\/input\/home-data-for-ml-course\/test.csv', index_col = 'Id')\n\n# Remove rows with missing target data and drop target column\nX_train.dropna(axis = 0, subset = [target], inplace = True)\ny_train = X_train[target]\nX_train.drop([target], axis = 1, inplace = True)","20d6ef3d":"from matplotlib import pyplot as plt\nimport numpy as np\n\n# Get columns with categorical data\ncols_by_is_object = (X_train.dtypes == 'object')\ncols_by_is_object = cols_by_is_object.where(lambda is_object: is_object)\ncat_cols = list(cols_by_is_object.dropna().index)\n\ncat_data = X_train[cat_cols]\n\n# Get columns with high cardinality.\ncardinality_threshold = 10\nhigh_card_cols = [c for c in cat_data.columns if len(cat_data[c].unique()) > cardinality_threshold]\n# Drop the ones that have NaNs\ncat_cols_to_drop = [c for c in high_card_cols if X_train[c].isnull().any()] + [c for c in high_card_cols if X_test[c].isnull().any()]\nif len(cat_cols_to_drop) > 0:\n    print(f'Dropping the columns {cat_cols_to_drop} due to cardinalities that are above {cardinality_threshold}\\n')\n    X_train.drop(columns = cat_cols_to_drop, inplace = True)\n    X_test.drop(columns = cat_cols_to_drop, inplace = True)\n\n# Set columns to ordinal encode. Do this by manually going through the column values and deciding if there's an order to them or not.\ncat_cols_to_ordinal_encode = [\n    'ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', 'HeatingQC', 'CentralAir', 'KitchenQual',\n    'FireplaceQu', 'GarageQual', 'GarageCond', 'PoolQC', 'Fence'\n]\nprint(f'Ordinal encoding the columns {cat_cols_to_ordinal_encode}\\n')\n\ncat_cols_to_one_hot_encode = list(set(cat_cols) - set(high_card_cols) - set(cat_cols_to_ordinal_encode))\nprint(f'Remaining columns to one-hot encode: {cat_cols_to_one_hot_encode}')\n\n# Plot for funsies\nfig, ax = plt.subplots(figsize = (8, 8))\nax.pie(\n    [len(cat_cols_to_drop), len(cat_cols_to_ordinal_encode), len(cat_cols_to_one_hot_encode)],\n    autopct = lambda pct: f'{pct:.0f}%',\n    labels = ['Dropped', 'Ordinal Encoded', 'One-Hot Encoded'],\n    shadow = True,\n    explode = (0.03, 0.03, 0.03)\n)\nax.set_title('% of columns that are...')\nplt.show()","c7f1b014":"from sklearn.pipeline import make_pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Define transformers for numerical data.\nnumber_transformer_mean = SimpleImputer(strategy = 'mean')\nnumber_transformer_median = SimpleImputer(strategy = 'median')\nnumber_transformer_zero = SimpleImputer(strategy = 'constant', fill_value = 0)\n\n# Most null numerical data is set to zero, with some exceptions.\nnumerical_cols = (X_train.dtypes != 'object')\nnumerical_cols = numerical_cols.loc[numerical_cols].index.tolist()\n\nnum_cols_to_mean = ['MSSubClass', 'OverallQual', 'OverallCond']\nnum_cols_to_median = ['YearBuilt', 'YearRemodAdd', 'GarageYrBlt']\nnum_cols_to_zero = list(set(numerical_cols) - set(num_cols_to_median) - set(num_cols_to_mean))\n\n# Define transformers for categorical data.\n\n# Cross validation can cause some column values to appear in ONLY the training or validation set, causing ordinal encoder to mad.\n# Pass in all categories beforehand to fix.\nordinal = X_train[cat_cols_to_ordinal_encode].fillna('');\nordinal_cats = [ordinal[c].unique() for c in ordinal.columns]\nordinal_transformer = make_pipeline(\n    SimpleImputer(strategy = 'constant', fill_value = ''),\n    # Setting unknown_value to np.nan can cause an error when using the test set.\n    OrdinalEncoder(handle_unknown = 'use_encoded_value', unknown_value = -1)\n)\n\nonehot_transformer = make_pipeline(\n    SimpleImputer(strategy = 'constant', fill_value = 'N\/A'),\n    OneHotEncoder(handle_unknown = 'ignore')\n)\n\npreprocessor = ColumnTransformer(transformers = [\n    ('num_cols_mean', number_transformer_mean, num_cols_to_mean),\n    ('num_cols_median', number_transformer_median, num_cols_to_median),\n    ('num_cols_zero', number_transformer_zero, numerical_cols),\n    ('ordinal', ordinal_transformer, cat_cols_to_ordinal_encode),\n    ('onehot', onehot_transformer, cat_cols_to_one_hot_encode)\n])\n","5b6e3a25":"from sklearn.model_selection import train_test_split, cross_val_score\nfrom xgboost import XGBRegressor\n\nnum_trees = [100, 300, 500, 800, 1000]\nlearning_rates = [0.02, 0.03, 0.05, 0.08, 0.13]\n\n# Results in the form [(n_estimators, learning_rate, score)]\nresults = []\nfor trees in num_trees:\n    for rate in learning_rates:\n        pipeline = make_pipeline(\n            preprocessor,\n            XGBRegressor(n_estimators = trees, learning_rate = rate)\n        )\n\n        scores = -1 * cross_val_score(\n            pipeline,\n            X_train,\n            y_train,\n            scoring = 'neg_mean_absolute_error'\n        )\n        results.append((trees, rate, scores.mean()))\n\n        \nn_estimators, learning_rate, score = min(results, key = lambda x: x[2])\nprint(f'Best parameters:\\nn_estimators = {n_estimators}\\nlearning_rate = {learning_rate}\\nMAE: {score}')\n\n# Run using the best model parameters\npipeline = make_pipeline(\n    preprocessor,\n    XGBRegressor(n_estimators = n_estimators, learning_rate = learning_rate)\n)\n\npipeline.fit(X_train, y_train)\npreds = pipeline.predict(X_test)\n\noutput = pd.DataFrame({\n    'Id': X_test.index,\n    'SalePrice': preds\n})\noutput.to_csv('submission.csv', index = False)","6dd24573":"## Choose strategies for handling columns with categorical data\nHere, we process categorical values on a column-by-column basis. For each categorical column, we have one of three different approaches (thanks [intermediate machine learning course](https:\/\/www.kaggle.com\/alexisbcook\/categorical-variables#Three-Approaches)!).\n\n### Drop the column\nThis is done if the cardinality of the column is too high. The definition of \"too high\" is completely arbitrary for now. This can be improved by running different models with different cardinality thresholds and seeing which one performs the best.\n\n### Ordinal encoding\nDo this for values that have a clear ordering. This pretty much only involves columns with \"quality\" or \"condition\" ratings.\n\n### One-hot encoding\nDo this for the remaining categorical columns.","f7a94282":"## Import data and create data sets","5d7728a9":"## Set up preprocessors","7aba71ee":"# Run and test the model"}}