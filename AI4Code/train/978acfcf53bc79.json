{"cell_type":{"d3d55651":"code","31e9f5dd":"code","cf1f8633":"code","8f10d822":"code","3a981975":"code","9ad06836":"code","2ae686d6":"code","2ce68923":"code","3d474476":"code","d6376803":"code","a79b4333":"code","7fe5d028":"code","4c904cdb":"code","4ddd8e21":"code","eb246c93":"code","22c33d12":"code","a8f484ba":"markdown","905cad26":"markdown","866cb45a":"markdown","7b754548":"markdown","aee4f416":"markdown","ff45bd15":"markdown","a5058a90":"markdown","38921304":"markdown","194b64dc":"markdown","2c16d102":"markdown","f74927f3":"markdown","7afe0ee9":"markdown","cfef57bf":"markdown","def6ecff":"markdown","bd62ca68":"markdown","df43fd4c":"markdown","99cb11fb":"markdown","a145ef87":"markdown","2384e061":"markdown","fea84766":"markdown","3453bf96":"markdown"},"source":{"d3d55651":"# we don't like warnings\n# you can comment the following 2 lines if you'd like to\nimport warnings\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LogisticRegression, LogisticRegressionCV\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold\nfrom sklearn.model_selection import GridSearchCV","31e9f5dd":"# loading data\ndata = pd.read_csv('..\/input\/microchip_tests.txt',\n                   header=None, names = ('test1','test2','released'))\n# getting some info about dataframe\ndata.info()","cf1f8633":"data.head(5)","8f10d822":"data.tail(5)","3a981975":"X = data.iloc[:,:2].values\ny = data.iloc[:,2].values","9ad06836":"plt.scatter(X[y == 1, 0], X[y == 1, 1], c='blue', label='Released')\nplt.scatter(X[y == 0, 0], X[y == 0, 1], c='orange', label='Faulty')\nplt.xlabel(\"Test 1\")\nplt.ylabel(\"Test 2\")\nplt.title('2 tests of microchips. Logit with C=1')\nplt.legend();","2ae686d6":"def plot_boundary(clf, X, y, grid_step=.01, poly_featurizer=None):\n    x_min, x_max = X[:, 0].min() - .1, X[:, 0].max() + .1\n    y_min, y_max = X[:, 1].min() - .1, X[:, 1].max() + .1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, grid_step),\n                         np.arange(y_min, y_max, grid_step))\n\n\n    # to every point from [x_min, m_max]x[y_min, y_max]\n    # we put in correspondence its own color\n    Z = clf.predict(poly_featurizer.transform(np.c_[xx.ravel(), yy.ravel()]))\n    Z = Z.reshape(xx.shape)\n    plt.contour(xx, yy, Z, cmap=plt.cm.Paired)","2ce68923":"poly = PolynomialFeatures(degree=7)\nX_poly = poly.fit_transform(X)","3d474476":"X_poly.shape","d6376803":"C = 1e-2\nlogit = LogisticRegression(C=C, random_state=17)\nlogit.fit(X_poly, y)\n\nplot_boundary(logit, X, y, grid_step=.01, poly_featurizer=poly)\n\nplt.scatter(X[y == 1, 0], X[y == 1, 1], c='blue', label='Released')\nplt.scatter(X[y == 0, 0], X[y == 0, 1], c='orange', label='Faulty')\nplt.xlabel(\"Test 1\")\nplt.ylabel(\"Test 2\")\nplt.title('2 tests of microchips. Logit with C=%s' % C)\nplt.legend();\n\nprint(\"Accuracy on training set:\", \n      round(logit.score(X_poly, y), 3))","a79b4333":"C = 1\nlogit = LogisticRegression(C=C, random_state=17)\nlogit.fit(X_poly, y)\n\nplot_boundary(logit, X, y, grid_step=.005, poly_featurizer=poly)\n\nplt.scatter(X[y == 1, 0], X[y == 1, 1], c='blue', label='Released')\nplt.scatter(X[y == 0, 0], X[y == 0, 1], c='orange', label='Faulty')\nplt.xlabel(\"Test 1\")\nplt.ylabel(\"Test 2\")\nplt.title('2 tests of microchips. Logit with C=%s' % C)\nplt.legend();\n\nprint(\"Accuracy on training set:\", \n      round(logit.score(X_poly, y), 3))","7fe5d028":"C = 1e4\nlogit = LogisticRegression(C=C, random_state=17)\nlogit.fit(X_poly, y)\n\nplot_boundary(logit, X, y, grid_step=.005, poly_featurizer=poly)\n\nplt.scatter(X[y == 1, 0], X[y == 1, 1], c='blue', label='Released')\nplt.scatter(X[y == 0, 0], X[y == 0, 1], c='orange', label='Faulty')\nplt.xlabel(\"Test 1\")\nplt.ylabel(\"Test 2\")\nplt.title('2 tests of microchips. Logit with C=%s' % C)\nplt.legend();\n\nprint(\"Accuracy on training set:\", \n      round(logit.score(X_poly, y), 3))","4c904cdb":"skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=17)\n\nc_values = np.logspace(-2, 3, 500)\n\nlogit_searcher = LogisticRegressionCV(Cs=c_values, cv=skf, verbose=1, n_jobs=-1)\nlogit_searcher.fit(X_poly, y)","4ddd8e21":"logit_searcher.C_","eb246c93":"plt.plot(c_values, np.mean(logit_searcher.scores_[1], axis=0))\nplt.xlabel('C')\nplt.ylabel('Mean CV-accuracy');","22c33d12":"plt.plot(c_values, np.mean(logit_searcher.scores_[1], axis=0))\nplt.xlabel('C')\nplt.ylabel('Mean CV-accuracy');\nplt.xlim((0,10));","a8f484ba":"Finalmente, seleccione el \u00e1rea con los \"mejores\" valores de $C$.","905cad26":"**Subtotales**:\n- cuanto mayor sea el par\u00e1metro $C$, m\u00e1s complejas ser\u00e1n las relaciones en los datos que el modelo puede recuperar (intuitivamente $C$ corresponde a la \"complejidad\" del modelo - capacidad del modelo)\n- Si la regularizaci\u00f3n es demasiado fuerte, es decir, los valores de $C$ son peque\u00f1os, la soluci\u00f3n al problema de minimizar la funci\u00f3n de p\u00e9rdida log\u00edstica puede ser aquella en la que muchas de las ponderaciones son demasiado peque\u00f1as o est\u00e1n en cero. El modelo tampoco est\u00e1 suficientemente \"penalizado\" por los errores (es decir, en la funci\u00f3n $J$, la suma de los cuadrados de los pesos \"supera\", y el error $\\mathcal{L}$ puede ser relativamente grande). En este caso, el modelo se adaptar\u00e1 como vimos en nuestro primer caso.\n- por el contrario, si la regularizaci\u00f3n es demasiado d\u00e9bil, es decir, los valores de $C$ son grandes, un vector $w$ con componentes de alto valor absoluto puede convertirse en la soluci\u00f3n al problema de optimizaci\u00f3n. En este caso, $\\mathcal{L}$ tiene una mayor contribuci\u00f3n a la $J$ funcional optimizada. En t\u00e9rminos generales, el modelo es demasiado \"temeroso\" para confundirse con los objetos del conjunto de entrenamiento y, por lo tanto, se adaptar\u00e1 como lo vimos en el tercer caso.\n- la regresi\u00f3n log\u00edstica no \"entender\u00e1\" (o \"aprender\u00e1\") qu\u00e9 valor de $C$ elegir, como lo hace con los pesos $w$. Es decir, no se puede determinar resolviendo el problema de optimizaci\u00f3n en la regresi\u00f3n log\u00edstica. Hemos visto una situaci\u00f3n similar anteriormente: un \u00e1rbol de decisiones no puede \"aprender\" qu\u00e9 l\u00edmite de profundidad elegir durante el proceso de capacitaci\u00f3n. Por lo tanto, $C$ es un hiperpar\u00e1metro modelo que se ajusta a la validaci\u00f3n cruzada; as\u00ed es el max_depth en un \u00e1rbol.","866cb45a":"<center>\n<img src=\"https:\/\/mlcourse.ai\/notebooks\/blob\/master\/img\/ods_stickers.jpg\" \/>\n    \n## [mlcourse.ai](mlcourse.ai), open Machine Learning course \n\nAuthor: [Yury Kashnitskiy](https:\/\/yorko.github.io). Translated and edited by [Christina Butsko](https:\/\/www.linkedin.com\/in\/christinabutsko\/), [Nerses Bagiyan](https:\/\/www.linkedin.com\/in\/nersesbagiyan\/), [Yulia Klimushina](https:\/\/www.linkedin.com\/in\/yuliya-klimushina-7168a9139), and [Yuanyuan Pao](https:\/\/www.linkedin.com\/in\/yuanyuanpao\/). This material is subject to the terms and conditions of the [Creative Commons CC BY-NC-SA 4.0](https:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/) license. Free use is permitted for any non-commercial purpose.","7b754548":"Entonces, \u00bfpor qu\u00e9 no aumentamos $C$ a\u00fan m\u00e1s - hasta 10,000? Ahora, la regularizaci\u00f3n claramente no es lo suficientemente fuerte, y vemos el sobreajuste. Tenga en cuenta que, con $C$=1 y un l\u00edmite \"suave\", la proporci\u00f3n de respuestas correctas en el conjunto de entrenamiento no es mucho menor que aqu\u00ed. Pero uno puede imaginar f\u00e1cilmente c\u00f3mo nuestro segundo modelo funcionar\u00e1 mucho mejor con los nuevos datos.","aee4f416":"Para ver c\u00f3mo var\u00eda la calidad del modelo (porcentaje de respuestas correctas en los conjuntos de entrenamiento y validaci\u00f3n) con el hiperpar\u00e1metro $C$, podemos trazar la gr\u00e1fica.","ff45bd15":"Usaremos la implementaci\u00f3n de regresi\u00f3n log\u00edstica de `sklearn`. Entonces, creamos un objeto que agregar\u00e1 caracter\u00edsticas polinomiales hasta el grado 7 a la matriz $X$.","a5058a90":"**Ajuste de par\u00e1metros de regularizaci\u00f3n**","38921304":"Para discutir los resultados, reescribamos la funci\u00f3n que est\u00e1 optimizada en regresi\u00f3n log\u00edstica con la forma:\n\n$$\\large J(X,y,w) = \\mathcal{L} + \\frac{1}{C}||w||^2,$$\n\nDonde\n\n- $\\mathcal{L}$ es la funci\u00f3n de p\u00e9rdida log\u00edstica sumada en todo el conjunto de datos\n- $C$ es el coeficiente de regularizaci\u00f3n inverso (el mismo $C$ de la implementaci\u00f3n de `sklearn` de` LogisticRegression`)","194b64dc":"Entrenemos la regresi\u00f3n log\u00edstica con el par\u00e1metro de regularizaci\u00f3n. $C = 10^{-2}$.","2c16d102":"Vamos a inspeccionar en la primera y \u00faltima 5 l\u00edneas.","f74927f3":"Ahora podr\u00edamos intentar aumentar $C$ a 1. Al hacer esto, debilitamos la regularizaci\u00f3n, y la soluci\u00f3n ahora puede tener mayores valores (en valor absoluto) de pesos de modelo que anteriormente. Ahora la precisi\u00f3n del clasificador en el conjunto de entrenamiento mejora a 0.831.","7afe0ee9":"Ahora debemos guardar el conjunto de entrenamiento y las etiquetas de la clase objetivo en matrices NumPy separadas.","cfef57bf":"En el primer art\u00edculo, demostramos c\u00f3mo las caracter\u00edsticas polinomiales permiten que los modelos lineales construyan superficies de separaci\u00f3n no lineales. Ahora vamos a mostrar esto visualmente.\n\nVeamos c\u00f3mo la regularizaci\u00f3n afecta la calidad de la clasificaci\u00f3n en un conjunto de datos en pruebas de microchip del curso de Andrew Ng sobre aprendizaje autom\u00e1tico. Utilizaremos la regresi\u00f3n log\u00edstica con caracter\u00edsticas polin\u00f3micas y variaremos el par\u00e1metro de regularizaci\u00f3n $C$. Primero, veremos c\u00f3mo la regularizaci\u00f3n afecta el borde de separaci\u00f3n del clasificador y reconocer\u00e1 de manera intuitiva las faltas y sobreajustes. Luego, elegiremos el par\u00e1metro de regularizaci\u00f3n para que est\u00e9 num\u00e9ricamente cerca del valor \u00f3ptimo a trav\u00e9s de (`cross-validation`) y (`GridSearch`).","def6ecff":"Definimos las siguientes caracter\u00edsticas polinomiales de grado. $d$ para dos variables $x_1$ and $x_2$:\n\n$$\\large \\{x_1^d, x_1^{d-1}x_2, \\ldots x_2^d\\} =  \\{x_1^ix_2^j\\}_{i+j=d, i,j \\in \\mathbb{N}}$$\n\nPor ejemplo, para $d=3$, estas ser\u00e1n las siguientes caracter\u00edsticas:\n\n$$\\large 1, x_1, x_2,  x_1^2, x_1x_2, x_2^2, x_1^3, x_1^2x_2, x_1x_2^2, x_2^3$$\n\nDibujar un Tri\u00e1ngulo de Pit\u00e1goras mostrar\u00eda cu\u00e1ntas de estas caracter\u00edsticas habr\u00e1 por $d=4,5...$ y as\u00ed sucesivamente.\nEl n\u00famero de tales caracter\u00edsticas es exponencialmente grande, y puede ser costoso construir caracter\u00edsticas polinomiales de gran grado (por ejemplo, $d=10$) para 100 variables. M\u00e1s importante a\u00fan, no es necesario.\n","bd62ca68":"### Recursos \u00fatiles\n- Medium [\"historia\"](https:\/\/medium.com\/open-machine-learning-course\/open-machine-learning-course-topic-4-linear-classification-and-regression-44a41b9b5220) basado en este cuaderno\n- Materiales del curso como un [conjunto de datos de Kaggle](https:\/\/www.kaggle.com\/kashnitsky\/mlcourse)\n- Si lees ruso: un [art\u00edculo] (https:\/\/habrahabr.ru\/company\/ods\/blog\/323890\/) en Habrahabr con ~ el mismo material. Y una [conferencia](https:\/\/youtu.be\/oTXGQ-_oqvI) en YouTube\n- En el libro [\"Aprendizaje profundo\"](http:\/\/www.deeplearningbook.org) (I. Goodfellow, Y. Bengio y A. Courville) se ofrece una descripci\u00f3n general agradable y concisa de los modelos lineales.\n- Los modelos lineales est\u00e1n cubiertos pr\u00e1cticamente en todos los libros de ML. Recomendamos \u201cReconocimiento de patrones y aprendizaje autom\u00e1tico\u201d (C. Bishop) y \u201cAprendizaje autom\u00e1tico: una perspectiva probabil\u00edstica\u201d (K. Murphy).\n- Si prefiere una visi\u00f3n general del modelo lineal desde el punto de vista de un estad\u00edstico, mire \"Los elementos del aprendizaje estad\u00edstico\" (T. Hastie, R. Tibshirani y J. Friedman).\n- El libro \u201cAprendizaje de m\u00e1quinas en acci\u00f3n\u201d (P. Harrington) lo guiar\u00e1 a trav\u00e9s de las implementaciones de algoritmos ML cl\u00e1sicos en Python puro.\n- [Scikit-learn](http:\/\/scikit-learn.org\/stable\/documentation.html) biblioteca. Estos chicos trabajan duro para escribir documentaci\u00f3n muy clara.\n- Scipy 2017 [tutorial de scikit-learn](https:\/\/github.com\/amueller\/scipy-2017-sklearn) por Alex Gramfort y Andreas Mueller.\n- Uno m\u00e1s [curso de ML](https:\/\/github.com\/diefimov\/MTH594_MachineLearning) con muy buenos materiales.\n- [Implementaciones](https:\/\/github.com\/rushter\/MLAlgorithms) de muchos algoritmos ML. B\u00fasqueda de regresi\u00f3n lineal y regresi\u00f3n log\u00edstica.","df43fd4c":"Carguemos los datos usando `read_csv` de la biblioteca `pandas`. En este conjunto de datos de 118 microchips (objetos), hay resultados para dos pruebas de control de calidad (dos variables num\u00e9ricas) e informaci\u00f3n sobre si el microchip entr\u00f3 en producci\u00f3n. Las variables ya est\u00e1n centradas, lo que significa que los valores de columna han tenido sus propios valores medios restados. Por lo tanto, el microchip \"promedio\" corresponde a un valor de cero en los resultados de la prueba.","99cb11fb":"Usando este ejemplo, identifiquemos el valor \u00f3ptimo del par\u00e1metro de regularizaci\u00f3n $C$. Esto se puede hacer usando `LogisticRegressionCV` - una b\u00fasqueda de par\u00e1metros de la rejilla seguida de una validaci\u00f3n cruzada. Esta clase est\u00e1 dise\u00f1ada espec\u00edficamente para regresi\u00f3n log\u00edstica (algoritmos efectivos con par\u00e1metros de b\u00fasqueda conocidos). Para un modelo arbitrario, use `GridSearchCV`,` RandomizedSearchCV`, o algoritmos especiales para la optimizaci\u00f3n de hiperpar\u00e1metros como el implementado en `hyperopt`.","a145ef87":"Vamos a definir una funci\u00f3n para mostrar la curva de separaci\u00f3n del clasificador.","2384e061":"Recuerde que estas curvas se llaman curvas de validaci\u00f3n. Anteriormente, los constru\u00edamos manualmente, pero sklearn tiene m\u00e9todos especiales para construirlos que usaremos en el futuro.","fea84766":"# <center>Topic 4. Clasificaci\u00f3n lineal y regresi\u00f3n\n## <center> Part 3. Un ejemplo ilustrativo de regularizaci\u00f3n log\u00edstica de regresi\u00f3n.","3453bf96":"Como paso intermedio, podemos trazar los datos. Los puntos naranjas corresponden a chips defectuosos, azules a los normales."}}