{"cell_type":{"ebc6db23":"code","8e20ff04":"code","81abfde4":"code","90192500":"code","b8049b15":"code","70be3e98":"code","55a4ba60":"code","b3917b64":"code","7aa8b459":"code","f57ecd3a":"code","8f0f3a7e":"code","6cf32e74":"code","17cfef55":"code","0ef675dc":"code","e9ae1967":"code","6d8d3166":"code","926853f4":"code","d84a832c":"code","eb4ab71d":"code","95cb6b13":"code","47579d22":"code","4d5d76c1":"code","4eae220b":"code","a4a0cdb4":"code","773100da":"code","85ec1f1f":"code","a8688641":"code","51d30797":"code","e7bb9593":"code","704eae97":"code","b8a4c5e7":"code","9265a6b9":"code","908fd135":"code","b1427386":"code","5214bb80":"code","1af59943":"code","fbf0e476":"code","2de4c6ea":"code","b63f6b45":"code","25ca10de":"markdown","3ba0d117":"markdown","71378ebd":"markdown","24fcdcef":"markdown","da4e8907":"markdown","3f68e135":"markdown","8ff2b5da":"markdown","8cf81241":"markdown","bbb98f9f":"markdown","fef3ff02":"markdown","a5038a2f":"markdown","8bfd0801":"markdown"},"source":{"ebc6db23":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # data visualization library\n%matplotlib inline\nimport seaborn as sns # interactive visualization library built on top on matplotlib\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\ndf1 = pd.read_csv('\/kaggle\/input\/train.csv') # importing training data","8e20ff04":"df1.head() #checking the head of the data","81abfde4":"#Missing Values If Any\ndf1.isna().sum()","90192500":"print(len(df1[df1.label == 0]), 'Non-Hatred Tweets')\nprint(len(df1[df1.label == 1]), 'Hatred Tweets')\n# Class distribution in this data seems to be imbalanced.\n# F1 score should be used fot model performance evaluation in such situation. ","b8049b15":"#importing different libraries for analysis, processing and classification\nimport nltk\nfrom sklearn import re #regular expression for text processing\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer #word stemmer class\nlemma = WordNetLemmatizer()\nfrom wordcloud import WordCloud, STOPWORDS\nfrom nltk import FreqDist \n# vectorizer \nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression #classification model\nfrom sklearn.metrics import confusion_matrix, classification_report, f1_score # performance evaluation criteria","70be3e98":"def normalizer(tweet):\n    tweets = \" \".join(filter(lambda x: x[0]!= '@' , tweet.split()))\n    tweets = re.sub('[^a-zA-Z]', ' ', tweets)\n    tweets = tweets.lower()\n    tweets = tweets.split()\n    tweets = [word for word in tweets if not word in set(stopwords.words('english'))]\n    tweets = [lemma.lemmatize(word) for word in tweets]\n    tweets = \" \".join(tweets)\n    return tweets\n","55a4ba60":"df1['normalized_text'] = df1.tweet.apply(normalizer)","b3917b64":"def extract_hashtag(tweet):\n    tweets = \" \".join(filter(lambda x: x[0]== '#', tweet.split()))\n    tweets = re.sub('[^a-zA-Z]',' ',  tweets)\n    tweets = tweets.lower()\n    tweets = [lemma.lemmatize(word) for word in tweets]\n    tweets = \"\".join(tweets)\n    return tweets","7aa8b459":"df1['hashtag'] = df1.tweet.apply(extract_hashtag)","f57ecd3a":"df1.head()","8f0f3a7e":"# all tweets \nall_words = \" \".join(df1.normalized_text)\n#print(all_all_words)\n","6cf32e74":"#Hatred tweets\nhatred_words = \" \".join(df1[df1['label']==1].normalized_text)\n#print(hatred_words)","17cfef55":"wordcloud = WordCloud(height=2000, width=2000, stopwords=STOPWORDS, background_color='white')\nwordcloud = wordcloud.generate(all_words)\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","0ef675dc":"wordcloud = WordCloud(height=2000, width=2000, stopwords=STOPWORDS, background_color='white')\nwordcloud = wordcloud.generate(hatred_words)\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","e9ae1967":"freq_all_hashtag = FreqDist(list((\" \".join(df1.hashtag)).split())).most_common(10)\nfreq_all_hashtag","6d8d3166":"freq_hatred_hashtag = FreqDist(list((\" \".join(df1[df1['label']==1]['hashtag'])).split())).most_common(10)\nfreq_hatred_hashtag","926853f4":"df_allhashtag = pd.DataFrame(freq_all_hashtag, columns=['words', 'frequency'])\ndf_hatredhashtag = pd.DataFrame(freq_hatred_hashtag, columns=['words', 'frequency'])\nprint(df_allhashtag.head())\nprint(df_allhashtag.head())","d84a832c":"sns.barplot(x='words', y='frequency', data=df_allhashtag)\nplt.xticks(rotation = 45)\nplt.title('hashtag words frequency')\nplt.show()","eb4ab71d":"sns.barplot(x='words', y='frequency', data=df_hatredhashtag)\nplt.xticks(rotation = 45)\nplt.title('hatred hashtag frequency')\nplt.show()","95cb6b13":"# to create sparse matrix corpus is created to pass to vectorizer\nlen(df1)\ncorpus = []\nfor i in range(0,31962):\n    corpus.append(df1['normalized_text'][i])\n#corpus","47579d22":"cv = CountVectorizer(stop_words=stopwords.words('english'))\ncv.fit(corpus)","4d5d76c1":"# creating dense matrix\nX = cv.transform(corpus).toarray()\ny = df1.iloc[:,1].values","4eae220b":"# train test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","a4a0cdb4":"\nclassifier1 = LogisticRegression(C=10)\nclassifier1.fit(X_train, y_train)\n","773100da":"y_pred = classifier1.predict(X_test)\ny_prob = classifier1.predict_proba(X_test)","85ec1f1f":"print(f1_score(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))","a8688641":"tfidf = TfidfVectorizer(ngram_range=(1,3), min_df=10, stop_words=stopwords.words('english'))\nX1 = tfidf.fit_transform(corpus)","51d30797":"X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y, test_size=0.33, random_state=42)\nclassifier2 = LogisticRegression(C=10)\nclassifier2.fit(X1_train, y1_train)","e7bb9593":"y1_pred = classifier2.predict(X1_test)\ny1_prob = classifier2.predict_proba(X1_test)","704eae97":"print(f1_score(y1_test, y1_pred))\nprint(classification_report(y1_test, y1_pred))\nprint(confusion_matrix(y1_test, y1_pred))","b8a4c5e7":"threshold = np.arange(0.1,0.9,0.1)\nscore = [f1_score(y1_test, ((y1_prob[:,1] >= x).astype(int))) for x in threshold]","9265a6b9":"plt.plot(threshold, score)\nplt.xlabel('Threshold Probability')\nplt.ylabel('F1 score')\nplt.show()","908fd135":"df2 = pd.read_csv('\/kaggle\/input\/test.csv')\ndf2.head()","b1427386":"df2['normalized_text'] = df2['tweet'].apply(normalizer)\n","5214bb80":"# creating corpus\ncorpus_test = []\nfor i in range(0,17197):\n    corpus_test.append(df2.normalized_text[i])\n#corpus_test","1af59943":"Test_X = tfidf.transform(corpus_test)","fbf0e476":"pred_Y = classifier2.predict(Test_X)\nprob_Y = classifier2.predict_proba(Test_X)","2de4c6ea":"df2['pred_label'] = pred_Y\nscores = (prob_Y[:,1] >= 0.5).astype(int)\ndf2['score'] = scores\n","b63f6b45":"df2[df2.pred_label == 1]","25ca10de":"**Analysing Hashtag words**\n\n**plotting the most common hashtag used in tweets**\n**all hashtag \nhatred hashtag**","3ba0d117":"**hatred tweets cloudword**","71378ebd":"**To Create Cloud of words for all words and hatred words**","24fcdcef":"**a maximum f1 score of 0.63 is achieved at threshold of 0.5 \nthus tweet with probability greater than or equal to 0.5 will be classified as hatred\nsince class distribution is imabalance we cannot use accurace as model performance evaluation method.**","da4e8907":"**Classification**\n\n**Logistic Regression (Linear Model)******","3f68e135":"**Extracting words with hashtag for further analysis**","8ff2b5da":"**checking with TF-IDF vectorizer**\n*** Unigram, bi gram is used wih min_df = 10*******","8cf81241":"**Preprocessing the tweet column**","bbb98f9f":"**performing classification model on our test data** ","fef3ff02":"**It can be seen that our model performed quiet good on the test data as well and made a quiet good prediction on class labels, this performance can probably be enhanced if we use SVM linear classifier which more powerfull than Logistic Regression and is also a good classifier for sentiment analysis.**","a5038a2f":"**Preprocessing**","8bfd0801":"**All tweets cloudword**"}}