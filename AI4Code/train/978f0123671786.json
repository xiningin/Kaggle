{"cell_type":{"34aaddc6":"code","b27ee71e":"code","a79b0e93":"code","7c7c8ac8":"code","30fd79bd":"code","e7f49111":"code","de2f2e4a":"code","3728508d":"code","9fa7ce3b":"code","31bb0dbd":"code","9bca1c69":"code","df382b5c":"code","5ddacaf0":"code","5c9ed749":"code","687b96c0":"code","be4243b3":"code","c7a95dca":"code","66762a51":"code","d57f0b6d":"code","f5a62a9c":"code","e3e516f3":"code","854baf8d":"code","ec22263b":"markdown","80e6318e":"markdown","769eb015":"markdown"},"source":{"34aaddc6":"!git clone https:\/\/github.com\/zzh8829\/yolov3-tf2","b27ee71e":"\n%cd yolov3-tf2\n!ls","a79b0e93":"!ls data\nprint('coco.names')\n!cat data\/coco.names | wc\nprint('voc2012.names')\n!cat data\/voc2012.names | wc\n!cat data\/coco.names","7c7c8ac8":"!pip install -r requirements-gpu.txt","30fd79bd":"import tensorflow as tf\n\n# You'll generate plots of attention in order to see which parts of an image\n# our model focuses on during captioning\nimport matplotlib.pyplot as plt\n\n# Scikit-learn includes many helpful utilities\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle\n\nimport numpy as np\nimport os\nimport time\nimport json\nfrom glob import glob\nfrom PIL import Image\nimport pickle\n\nprint(tf.__version__)","e7f49111":"!cat convert.py # this function use to convert official yolov3 weights to Keras model","de2f2e4a":"!wget https:\/\/pjreddie.com\/media\/files\/yolov3.weights -O data\/yolov3.weights\n!python convert.py","3728508d":"import sys\nfrom absl import app, logging, flags\nfrom absl.flags import FLAGS\nimport time\nimport cv2\nfrom yolov3_tf2.models import (\n    YoloV3, YoloV3Tiny\n)\nfrom yolov3_tf2.dataset import transform_images, load_tfrecord_dataset\nfrom yolov3_tf2.utils import draw_outputs\nfrom IPython.display import Image, display","9fa7ce3b":"\n\nflags.DEFINE_string('classes', '.\/data\/coco.names', 'path to classes file')\nflags.DEFINE_string('weights', '.\/checkpoints\/yolov3.tf',\n                    'path to weights file')\nflags.DEFINE_boolean('tiny', False, 'yolov3 or yolov3-tiny')\nflags.DEFINE_integer('size', 416, 'resize images to')\nflags.DEFINE_string('image', '.\/data\/girl.png', 'path to input image')\nflags.DEFINE_string('tfrecord', None, 'tfrecord instead of image')\nflags.DEFINE_string('output', '.\/output.jpg', 'path to output image')\nflags.DEFINE_integer('num_classes', 80, 'number of classes in the model')\n\napp._run_init(['yolov3'], app.parse_flags_with_usage)\n\n","31bb0dbd":"# trick to better allocate GPU memory, otherwise, we will get OOM\nphysical_devices = tf.config.experimental.list_physical_devices('GPU')\ntf.config.experimental.set_memory_growth(physical_devices[0], True) ","9bca1c69":"FLAGS.tiny, FLAGS.classes","df382b5c":"\n\nif FLAGS.tiny:\n    yolo = YoloV3Tiny(classes=FLAGS.num_classes)\nelse:\n    yolo = YoloV3(classes=FLAGS.num_classes)\n      \nyolo.load_weights(FLAGS.weights).expect_partial() # expect_partial just suppress some loading warning\nlogging.info('weights loaded')\n\nclass_names = [c.strip() for c in open(FLAGS.classes).readlines()]\nlogging.info('classes loaded')\n\n","5ddacaf0":"print(class_names)","5c9ed749":"flickr_path = '\/kaggle\/input\/flickr8k-sau\/Flickr_Data\/Images\/'\npaths2 = sorted(os.listdir(flickr_path))\nprint(len(paths2))","687b96c0":"'''\ngirl.png  meme2.jpeg    street.jpg\t    \nmeme.jpg  meme_out.jpg  street_out.jpg \n'''\nFLAGS.image = 'data\/meme.jpg'\nFLAGS.image = 'data\/meme2.jpeg'\nFLAGS.image = 'data\/girl.png'\nFLAGS.image = 'data\/street.jpg'\n\nfor jj in range(20):\n    FLAGS.image = flickr_path + paths2[jj]\n\n    img_raw = tf.image.decode_image(\n        open(FLAGS.image, 'rb').read(), channels=3)\n\n    img = tf.expand_dims(img_raw, 0)\n    img = transform_images(img, FLAGS.size)\n\n    t1 = time.time()\n    boxes, scores, classes, nums = yolo(img)\n    t2 = time.time()\n#     logging.info('time: {}'.format(t2 - t1))\n\n#     logging.info('detections:')\n    for i in range(nums[0]):\n        logging.info('\\t{}, {}, {}'.format(class_names[int(classes[0][i])],\n                                        np.array(scores[0][i]),\n                                        np.array(boxes[0][i])))\n\n    img = cv2.cvtColor(img_raw.numpy(), cv2.COLOR_RGB2BGR)\n    img = draw_outputs(img, (boxes, scores, classes, nums), class_names)\n\n    \n    display(Image(data=bytes(cv2.imencode('.jpg', img)[1]), width=800))","be4243b3":"nums_np = nums[0].numpy()\nprint(nums_np)\n\nscore_np = scores[0].numpy()\nprint(score_np[:(nums[0].numpy()+1)])\n\nclasses_np = classes[0].numpy().astype(int)\nprint(classes_np)","c7a95dca":"## see the details of drawing function\n!cat yolov3_tf2\/utils.py","66762a51":"for i in range(nums_np):\n    print(class_names[classes_np[i]]) # Note that Person = class0","d57f0b6d":"annotation_file = '\/kaggle\/input\/coco2014\/captions\/annotations\/captions_train2014.json'\nCOCOPATH = '\/kaggle\/input\/coco2014\/train2014\/train2014\/'\n!ls {COCOPATH} | wc","f5a62a9c":"with open(annotation_file, 'r') as f:\n    annotations = json.load(f)\n\n# Store captions and image names in vectors\nall_captions = {}\nall_img_name_vector = []\n\nfor annot in annotations['annotations']:\n    caption = '<start> ' + annot['caption'] + ' <end>'\n    image_id = annot['image_id']\n    full_coco_image_path = COCOPATH + 'COCO_train2014_' + '%012d.jpg' % (image_id)\n\n    all_img_name_vector.append(full_coco_image_path)\n    \n    if all_captions.get(all_img_name_vector[-1]) is None:\n        all_captions[all_img_name_vector[-1]] = []\n    \n    all_captions[all_img_name_vector[-1]].append(caption)\n","e3e516f3":"len(all_captions), len(all_img_name_vector), image_id\nprint(all_img_name_vector[:5])\nprint(all_captions[list(all_captions.keys())[0]])","854baf8d":"\n\npaths2 = sorted(os.listdir(COCOPATH))\nprint(len(paths2))\n\nfor jj in range(20):\n    FLAGS.image =  COCOPATH + paths2[jj] # all_img_name_vector[jj] is repeated\n    print('\\n***',all_captions[COCOPATH + paths2[jj]],'***\\n')\n    img_raw = tf.image.decode_image(\n        open(FLAGS.image, 'rb').read(), channels=3)\n\n    img = tf.expand_dims(img_raw, 0)\n    img = transform_images(img, FLAGS.size)\n\n    t1 = time.time()\n    boxes, scores, classes, nums = yolo(img)\n    t2 = time.time()\n#     logging.info('time: {}'.format(t2 - t1))\n\n#     logging.info('detections:')\n    for i in range(nums[0]):\n        logging.info('\\t{}, {}, {}'.format(class_names[int(classes[0][i])],\n                                        np.array(scores[0][i]),\n                                        np.array(boxes[0][i])))\n\n    img = cv2.cvtColor(img_raw.numpy(), cv2.COLOR_RGB2BGR)\n    img = draw_outputs(img, (boxes, scores, classes, nums), class_names)\n\n    \n    display(Image(data=bytes(cv2.imencode('.jpg', img)[1]), width=800))","ec22263b":"# COCO dataset","80e6318e":"# Flickr dataset","769eb015":"# Introduction\nHi guys, this notebook illustrates how to employ yoloV3 to perform Object Detection in Flickr images and COCO 2014 dataset. 99% of the codes are from this amazing repo https:\/\/github.com\/zzh8829\/yolov3-tf2 please star him.\n"}}