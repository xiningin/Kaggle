{"cell_type":{"26d2bfa3":"code","6dfc564f":"code","63ce7ed4":"code","1e554c1f":"code","0d6b567f":"code","840bdd0f":"code","678dd89c":"code","52ffeb25":"code","9bb27c5a":"code","87d04629":"code","6924bd1b":"code","f26d85ac":"code","3d2fc70d":"code","aa6c6ca6":"code","49f2a8c4":"code","50960cb6":"code","6e93f05f":"code","368fc4c7":"code","1f0a3f05":"code","b94f1048":"code","d27509be":"code","2329e6dc":"code","9eb63e54":"code","7167cb68":"code","583d397f":"code","868f986a":"code","6e19d358":"code","42a94d22":"code","0a17ee2c":"code","73f352cc":"code","96f01865":"code","0a4c71b0":"code","e64f0a57":"code","75cf68a0":"code","5edee0a4":"code","89d204dd":"code","cb02b873":"code","84b0bd37":"code","17f8a167":"code","3f11347a":"code","124a6202":"code","4bf07741":"code","4dcbc734":"code","9bfb0b00":"code","ea75c7cd":"code","4e0682af":"code","400be5ff":"code","67d995a5":"code","25dcb611":"code","0472f201":"code","1de5d357":"code","7d4a0e36":"code","6cf476be":"code","22cf4cfa":"code","812a68e7":"code","0b95ba7f":"code","ec20a421":"code","78c5b1e7":"code","d0056e4f":"code","3b823bf3":"code","d8140129":"code","59964489":"code","77479040":"code","ad760123":"code","251f02fe":"code","472877f7":"code","b70b1d59":"markdown","3d0faf35":"markdown","b2016d44":"markdown","7497fc8d":"markdown","45362812":"markdown","b8a5c770":"markdown","8bfd4ea5":"markdown","f6e76d7e":"markdown","bda40b4d":"markdown","a9f01397":"markdown","df0d0096":"markdown","272944cc":"markdown","6f3a6656":"markdown","3d930a11":"markdown","2ddc37f0":"markdown"},"source":{"26d2bfa3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.ensemble import RandomForestRegressor\nfrom IPython.display import display\nfrom fastai.imports import *\nimport xgboost as xgb\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n# Any results you write to the current directory are saved as output.\n#setting display height,max_rows,max_columns and width to desired\npd.set_option('display.max_rows',500)\npd.set_option('display.max_columns',500)\n","6dfc564f":"# function to print Mean Absolute Error\ndef rmse(X,Y,t,m: RandomForestRegressor):\n    print (\" Mean Absolute Error in {} is {}\".format(t,mean_absolute_error(m.predict(X),Y)))\n#     res = ['mae train: ',mean_absolute_error(m.predict(X_train),Y_train),'mae value: ',mean_absolute_error(m.predict(X_test),Y_test)]\n#     if hasattr(m,'oooob score '):\n#         res.append(m.oob_score_)\n#     print(res)\ndef rootmse(Y_pred,Y_test):\n    print(\" Mean Absolute Error is {}\".format(mean_absolute_error(Y_pred,Y_test)))","63ce7ed4":"train  = pd.read_csv('..\/input\/train_V2.csv')\ntest = pd.read_csv('..\/input\/test_V2.csv')\nprint (\"Train Head -->\")\ndisplay(train.head())\nprint (\"Test Head -->\")\ndisplay(test.head())","1e554c1f":"print (train.shape)","0d6b567f":"print (test.shape)","840bdd0f":"train.info()","678dd89c":"train.isna().sum()","52ffeb25":"train[train['winPlacePerc'].isnull()]","9bb27c5a":"#lets drop that NaN entry\ntrain.drop(2744604,inplace = True)","87d04629":"test.isna().sum()","6924bd1b":"print (\"Longest Kill Recored {} Average Kill Distance {}\".format(train['longestKill'].max(),train['longestKill'].mean()))\nprint (\"Max Assists Recorded {} Average Assists {}\".format(train['assists'].max(),train['assists'].mean()))\nprint (\"Max Boost Items used {} Average Boost Items Used {}\".format(train['boosts'].max(),train['boosts'].mean()))\nprint (\"Maximum DamageDealt  {} Average Damage Dealt {}\".format(train['damageDealt'].max(),train['damageDealt'].mean()))\nprint (\"Max Boost Items used {} Average Boost Items Used {}\".format(train['boosts'].max(),train['boosts'].mean()))\nprint (\"Max Heal Items used {} Average Heal Items Used {}\".format(train['heals'].max(),train['heals'].mean()))\nprint (\"Longest Kill Streak {} Average Kill Streak Used {}\".format(train['killStreaks'].max(),train['killStreaks'].mean()))\nprint (\"Maximum Kills {} Average Kills {}\".format(train['kills'].max(),train['kills'].mean()))\nprint (\"Maximum Revives {} Average Revives {}\".format(train['revives'].max(),train['revives'].mean()))\nprint (\"Maximum Team Kills {} Average Team Kills {}\".format(train['teamKills'].max(),train['teamKills'].mean()))\nprint (\"Maximum vehicleDestroys {} Average vehicleDestroys {}\".format(train['vehicleDestroys'].max(),train['vehicleDestroys'].mean()))","f26d85ac":"train['playerJoined'] = train.groupby('matchId')['matchId'].transform('count')\nplt.figure(figsize = (15,10))\nsns.countplot(train[train['playerJoined']>=60]['playerJoined'])\nplt.title(\"Players Joined\")\nplt.show()\nplt.figure(figsize = (15,10))\nsns.countplot(train[train['playerJoined']<=60]['playerJoined'])\nplt.title(\"Players Joined\")\nplt.show()","3d2fc70d":"# I think matches with less than 50 are not worth considering\n# so gonna drop those rows\ntrain.drop(train[train['playerJoined']<=50].index,inplace = True)\ntrain.shape","aa6c6ca6":"train['playersInGroup'] = train.groupby('groupId')['groupId'].transform('count')\nplt.figure(figsize=(15,10))\nsns.countplot(train[train['playersInGroup']>=0]['playersInGroup'])\nplt.title(\"Number of Players in Single Group\")\nplt.show()\ntrain[train['playersInGroup']>4].shape","49f2a8c4":"#Groups with players greater than 4 are not valid\n# as in PUBG max size of Group is 4 so we remove them\ntrain.drop(train[train['playersInGroup']>4].index, inplace = True)\n","50960cb6":"#lets find some interesting things from data\nprint ('Max Kills Recored {} Average Kills person kills {} while 99% people kills {}'.format(train['kills'].max(),train['kills'].mean(),train['kills'].quantile(0.99)))\n# 72 kills seems suspicious lets Plot","6e93f05f":"plt.figure(figsize=(15,10))\nsns.countplot(train[train['kills']>=1]['kills'])\nplt.title(\"Number of Kills\")\nplt.show()","368fc4c7":"#lets check killing with winPlacePerc\n# plt.figure(figsize = (15,10))\nsns.jointplot(x=\"winPlacePerc\",y=\"kills\",data=train,height=10,ratio=3)\nplt.title(\"WinplacePerc vs Number of Kills\")\nplt.show()","1f0a3f05":"#Team kills cannot be 4 or more so we have to remove this\nplt.figure(figsize=(15,10))\nsns.countplot(train[train['teamKills']>=4]['teamKills'])\nplt.title(\"TeamMate Kills\")\nplt.show()\ntrain[train['teamKills']>=4].shape","b94f1048":"#removing teamKills outliers\ntrain.drop(train[train['teamKills']>=4].index, inplace = True)","d27509be":"print(\"Max number of HeadShots by Single Person {} Average Headshots {} While 99% percent people {} \".format(train['headshotKills'].max(),train['headshotKills'].mean(),train['headshotKills'].quantile(0.99)))\n### remove  outlier headshots ###","2329e6dc":"######### has to do something with MatchDuration for match duration with less than 5min to 10 minutes######\n# train['check'] = train[train['matchDuration']<600]\nplt.figure(figsize=(15,10))\nsns.countplot(train[train['matchDuration']<600]['matchDuration'])\nplt.title(\"Match With Duration less tha 10 Minutes\")\nplt.show()\ntrain[train['matchDuration']<600].shape","9eb63e54":"#we will drop the rows with match Duration less than 10 minutes","7167cb68":"print (\"Unique id counts {} while data shape {}\".format(train['Id'].nunique(),train.shape))","583d397f":"print(\"Max Number of Weapons acquired by individual {} Average Number of Weapons Acquired {} while 99% percentile {}\".format(train['weaponsAcquired'].max(),train['weaponsAcquired'].mean(),train['weaponsAcquired'].quantile(0.99)))","868f986a":"#236 weapons acquired by an individual is of course an outlier\n#lets find outliers using weapons acquired\nplt.figure(figsize=(15,10))\nsns.countplot(train[train['weaponsAcquired']>50]['weaponsAcquired'])\nplt.show()\ntrain[train['weaponsAcquired']>50].shape","6e19d358":"#we will remove rows with weapons acquired more than 40 as they seems suspicious\ntrain.drop(train[train['weaponsAcquired']>50].index, inplace = True)\n#lets plot WeaponsAcquired vs winPlacePerc\n# plt.figure(figsize=(15,10))\nsns.jointplot(x=\"winPlacePerc\",y=\"weaponsAcquired\",data=train,height=10,ratio=3,color=\"blue\")\nplt.title(\"WinPlacePerc vs WeaponsAcquired Realtion\")\nplt.show()","42a94d22":"# train.sort_values(by = ['groupId']).head()","0a17ee2c":"# train['checkSolo'] = [1 if (x == \"solo\" or x == \"solo-fpp\") else 0 for x in train['matchType']]\ntrain['assistsCheck'] = [\"false\" if ((x == \"solo\" or x == \"solo-fpp\") and y!=0) else \"true\" for x,y in zip(train['matchType'],train['assists'])]\nprint (\"Number of assists in Solo :\",train[train['assistsCheck']==\"false\"].shape)\ntrain['DBNOCheck'] = [\"false\" if ((x == \"solo\" or x == \"solo-fpp\") and y!=0) else \"true\" for x,y in zip(train['matchType'],train['DBNOs'])]\nprint (\"Number of Knocks in Solos\",train[train['DBNOCheck']==\"false\"].shape)\n","73f352cc":"#lets remove these outliers from dataset\ntrain.drop(train[train['assistsCheck']==\"false\"].index, inplace = True)","96f01865":"# #plot winPlacePerc with Vehicle Destroyed\n# plt.figure(figsize = (15,10))\n# sns.countplot(train[train['vehicleDestroys']>0]['vehicleDestroys'])\n# plt.title(\"Vehicle Destroyed\")\n# plt.show()\n# # plt.figure(figsize = (15,10))\n# sns.jointplot(x=\"winPlacePerc\",y=\"vehicleDestroys\",data=train,height=10,ratio=3,color=\"lime\")\n# plt.title(\"Vehicle Destroyed jointplot\")\n# plt.show()\n","0a4c71b0":"print (\"Maximum walk Distance Tracelled {} Average walk Distance Travelled {}\".format(train['walkDistance'].max(),train['walkDistance'].mean()))\nprint (\"Maximum ride Distance Tracelled {} Average ride Distance Travelled {}\".format(train['rideDistance'].max(),train['rideDistance'].mean()))\nprint (\"Maximum swim Distance Tracelled {} Average swim Distance Travelled {}\".format(train['swimDistance'].max(),train['swimDistance'].mean()))\n","e64f0a57":"print (\"Maximum Total Distance Travelled by a Person in Single Match {} Average Total Distance Travelled {}\".format((train['walkDistance']+train['swimDistance']+train['rideDistance']).max(),(train['walkDistance']+train['swimDistance']+train['rideDistance']).mean()))","75cf68a0":"plt.figure(figsize = (15,10))\nsns.countplot(train[train['walkDistance']>5000]['walkDistance'])\nplt.title(\"Distance Covered by Foot\")\nplt.show()\n","5edee0a4":"plt.figure(figsize = (15,10))\nsns.countplot(train[train['rideDistance']>1200]['rideDistance'])\nplt.title(\"Distance Covered by Ride\")\nplt.show()","89d204dd":"plt.figure(figsize = (15,10))\nsns.countplot(train[train['swimDistance']>1200]['swimDistance'])\nplt.title(\"Distance Covered by Swimming\")\nplt.show()","cb02b873":"#lets find jointplot for WinPlacePerc vs all distances\n#first Walk Distance\nsns.jointplot(x=\"winPlacePerc\",y=\"walkDistance\",data=train,height=10,ratio=3)\nplt.show()","84b0bd37":"#now check ride distance\nsns.jointplot(x=\"winPlacePerc\",y=\"rideDistance\",data=train,height=10,ratio=3,color=\"black\")\nplt.show()","17f8a167":"#now check swim distance\nsns.jointplot(x=\"winPlacePerc\",y=\"swimDistance\",data=train,height=10,ratio=3,color=\"pink\")\nplt.show()","3f11347a":"#at last total distancce\ntrain['totalDistance'] = train['walkDistance'] + train['rideDistance'] + train['swimDistance']\nsns.jointplot(x=\"winPlacePerc\",y=\"totalDistance\",data=train,height=10,ratio=3,color=\"green\")\nplt.show()","124a6202":"df = train.copy()\ndf = df[(df['totalDistance']==0) & (df['weaponsAcquired']>=4)]\nprint (\"{} peoples cheated who donot move a bit but acquired weapons \".format(df['Id'].count()))\n# df.sort_values(by=['groupId']).head()\ntrain[train['groupId']==\"082950bbdd1d97\"].head()","4bf07741":"# df = train.copy()\n# df = df[(df['totalDistance']==0) & (df['kills']!=0)]\n# df.shape\n# df.head()\ndf = train.copy()\ndf = df[(df['totalDistance']==0) & (df['kills']!=0)]\nprint (\"{} peoples cheated who donot move a bit but acquired weapons \".format(df['Id'].count()))\ndf.sort_values(by=['groupId']).head()","4dcbc734":"#lets see an entry and observe\ntrain[train['groupId']==\"0000a9f58703c5\"].head()","9bfb0b00":"plt.figure(figsize=(15,10))\nsns.countplot(train[train['numGroups']>1]['numGroups'])\nplt.title(\"Number of Groups\")\nplt.show()","ea75c7cd":"# train['matchType'] = [1 if match ==\"solos\" 2 elif match ==\"duos\" else 3 for match in train['matchType']]\n# df.loc[df.set_of_numbers <= 4, 'equal_or_lower_than_4?'] = 'True' \n# df.loc[df.set_of_numbers > 4, 'equal_or_lower_than_4?'] = 'False' \ntrain.loc[train.matchType == \"solo\",'matchType'] = 1\ntrain.loc[train.matchType == \"duo\",'matchType'] = 2\ntrain.loc[train.matchType == \"squad\",'matchType'] = 3\ntrain.loc[train.matchType == \"solo-fpp\",'matchType'] = 4\ntrain.loc[train.matchType == \"duo-fpp\",'matchType'] = 5\ntrain.loc[train.matchType == \"squad-fpp\",'matchType'] = 6\ntrain.loc[(train.matchType != \"solo\") & (train.matchType != \"duo\") & (train.matchType != \"squad\") & (train.matchType != \"solo-fpp\") & (train.matchType != \"duo-fpp\") & (train.matchType != \"squad-fpp\"),'matchType'] = 7\ntrain.head()","4e0682af":"#drop unneccesary columns from dataset\ntrain = train.drop(['assistsCheck','DBNOCheck'], axis=1)\ntrain.head()","400be5ff":"#Let's turn groupId and matchId in category values\ntrain['groupId'] = train['groupId'].astype('category')\ntrain['matchId'] = train['matchId'].astype('category')\n\n#category codinf for groupId and matchId\ntrain['groupId'] = train['groupId'].cat.codes\ntrain['matchId'] = train['matchId'].cat.codes\n\ntrain.head()","67d995a5":"#Let's finally drop Id Column and do some Machine Learning\ntrain = train.drop(['Id'],axis = 1)\ntrain.shape","25dcb611":"train.head()","0472f201":"#Let's split the dataset into Training and cross validation set\ntrain,test = train_test_split(train,test_size=0.3)\nX_train = train.copy()\nX_train = X_train.drop(['winPlacePerc'], axis = 1)\nY_train = train['winPlacePerc']\nX_test = test.copy()\nX_test = X_test.drop(['winPlacePerc'], axis = 1)\nY_test = test['winPlacePerc']","1de5d357":"print (\"Training Data Shape {} and Test Data Shape {} \".format(train.shape,test.shape))","7d4a0e36":"m1 = RandomForestRegressor(n_estimators = 40, min_samples_leaf = 3, max_features = 'sqrt', n_jobs = -1)\nm1.fit(X_train,Y_train)\nrmse(X_train,Y_train,\"Train\",m1)\nrmse(X_test,Y_test,\"Test\",m1)","6cf476be":"m2 = RandomForestRegressor(n_estimators = 70, min_samples_leaf = 4, max_features = 0.5, n_jobs = -1)\nm2.fit(X_train,Y_train)\nrmse(X_train,Y_train,\"Train\",m2)\nrmse(X_test,Y_test,\"Test\",m2)","22cf4cfa":"m3 = RandomForestRegressor(n_estimators = 100, min_samples_leaf = 5, max_features = 0.5, n_jobs = -1)\nm3.fit(X_train,Y_train)\nrmse(X_train,Y_train,\"Train\",m3)\nrmse(X_test,Y_test,\"Test\",m3)","812a68e7":"# #XGboost for regression\nregr = xgb.XGBRegrssor(colsample_bytree=0.2,gamm=0.0,learning_rate=0.01,max_depth=4,min_child_weight=1.5,n_estimators=7200,reg_alpha=0.9,reg_lambda=0.6,subsample=0.2,seed=42,silent=1)\nregr.fit(X_train,Y_train)\nY_pred = regr.predict(X_test)\nrootmse(Y_pred,Y_test)","0b95ba7f":"from sklearn.model_selection import RandomizedSearchCV","ec20a421":"#let's do hyperParameter Tuning\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\nprint(random_grid)","78c5b1e7":"##########has to do one hot encoding############\n#let's try new Setting\n# Use the random grid to search for best hyperparameters\n# First create the base model to tune\nrf = RandomForestRegressor()\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n# Fit the random search model\nrf_random.fit(X_train,Y_train)","d0056e4f":"#Let's check best Random Parameters\n# rf_random.best_params_","3b823bf3":"#Let's Compare Accuracy of Different Models\ndef evaluate(model, test_features, test_labels):\n    predictions = model.predict(test_features)\n    errors = abs(predictions - test_labels)\n    mape = 100 * np.mean(errors \/ test_labels)\n    accuracy = 100 - mape\n    print('Model Performance')\n    print('Average Error: {:0.4f} degrees.'.format(np.mean(errors)))\n    print('Accuracy = {:0.2f}%.'.format(accuracy))\n    \n    return accuracy","d8140129":"#Base Model given\nbase_model = RandomForestRegressor(n_estimators = 10, random_state = 42)\nbase_model.fit(X_train,Y_train)\nbase_accuracy_given = evaluate(base_model, X_test, Y_test)","59964489":"#Model Created\nbase_accuracy_m1 = evaluate(m1,X_test,Y_test)\nbase_accuracy_m2 = evaluate(m2,X_test,Y_test)\nbase_accuracy_m3 = evaluate(m3,X_test,Y_test)\n# base_accuracy_regr = evaluate(regr,X_test,Y_test)","77479040":"#radnom Best Estimator\ntrain_features = X_train\ntrain_labels = Y_train\ntest_features = X_test\ntest_labels = Y_test\n# best_random = rf_random.best_estimator_\n# random_accuracy = evaluate(best_random, test_features, test_labels)\n# print (\" Hyper Random Accracy {} \\n m1 Accuracy {} \\n m2 Accuracy {} \\n m3 Accuracy {} \\n xGBoost {} \\n best_random {}\".format(base_accuracy_given,base_accuracy_m1,base_accuracy_m2,base_accuracy_m3,base_accuracy_regr,random_accuracy))","ad760123":"#GridSearch Model\nfrom sklearn.model_selection import GridSearchCV\n# Create the parameter grid based on the results of random search \nparam_grid = {\n    'bootstrap': [True],\n    'max_depth': [80, 90, 100, 110],\n    'max_features': [2, 3],\n    'min_samples_leaf': [3, 4, 5],\n    'min_samples_split': [8, 10, 12],\n    'n_estimators': [100, 200, 300, 1000]\n}\n# Create a based model\nrf = RandomForestRegressor()\n# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n                          cv = 3, n_jobs = -1, verbose = 2)","251f02fe":"# Fit the grid search to the data\ngrid_search.fit(train_features, train_labels)\n# grid_search.best_params_\n#chagne into one hot encoding","472877f7":"best_grid = grid_search.best_estimator_\ngrid_accuracy = evaluate(best_grid, X_test, Y_test)\n# print('Improvement of {:0.2f}%.'.format( 100 * (grid_accuracy - base_accuracy_given) \/ base_accuracy_given))","b70b1d59":"Let's check people with Total Distance travelled 0 and weapons Acquired and Kills for outliers in the data","3d0faf35":"These data are no outliers as there teammates increased thier winPlacePerc not cheated","b2016d44":"Let's Check WinplacePerc with Vehicle Destroys","7497fc8d":"**Match Duration**","45362812":"Let's Change Categorical Data","b8a5c770":"from all those distance plot we can observe more you travel the higher is your WinPlacePerc i.e winning chances increaase the more you travel","8bfd4ea5":"Let's Plot the distance travelled","f6e76d7e":"There seems to be relation between kills and winplaceperc . the more the number of kills more the winplacePerc","bda40b4d":"The models are as follows:\n\n1. average: original baseline computed by predicting historical average max temperature for each day in test set\n2. one_year: model trained using a single year of data\n3. four_years_all: model trained using 4.5 years of data and expanded features (see Part One for details)\n4. four_years_red: model trained using 4.5 years of data and subset of most important features\n5. best_random: best model from random search with cross validation\n6. first_grid: best model from first grid search with cross validation (selected as the final model)\n7. second_grid: best model from second grid search","a9f01397":"**Check Number of Players Joined in Game**","df0d0096":"**Killing**","272944cc":"There are 38848 assists in solo that can't be possible as there are no teammates in solo.","6f3a6656":"Lets Check If There are no entries with MatchType solo but has assists and Players Knocked","3d930a11":"**Number of Player in Group**","2ddc37f0":"Let's Look for Players with Kills but has not travelled a bit in the match and Players won the match with no distance Travelled"}}