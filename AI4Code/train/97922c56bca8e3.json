{"cell_type":{"e61bd88c":"code","2fbe2ff5":"code","b92f41c6":"code","45894449":"code","61e70db8":"code","1a2a1a8b":"code","5f9ee7ec":"code","9356dcc2":"code","01ecf92b":"code","cf22d4fe":"code","cf29f56a":"code","edf1a7a2":"code","addc9a13":"code","95e45f1a":"code","9dfb41c4":"code","f77f7252":"code","52a5770c":"code","7fa7c91c":"code","3134c671":"code","979d1161":"code","79a9b7fb":"code","117fceae":"code","67971e47":"code","5164ef7e":"markdown","0780880d":"markdown","e50b4eda":"markdown","0605268c":"markdown","9bd1d75d":"markdown","46f39014":"markdown","0c411cd7":"markdown","adffcdda":"markdown","9239dd49":"markdown","24f22856":"markdown","d846c15b":"markdown","ffd645a7":"markdown","0405db0e":"markdown","888a4cd8":"markdown","5e5a8145":"markdown","bcb1b92a":"markdown","5a473c8b":"markdown","c0625464":"markdown","6d763fda":"markdown","0d658bfe":"markdown","dbabc7ee":"markdown"},"source":{"e61bd88c":"!conda install -c conda-forge gdcm -y","2fbe2ff5":"import numpy as np\nimport pandas as pd\nimport os\nfrom glob import glob\nfrom PIL import Image\nimport cv2\nimport random\nimport math\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nimport seaborn as sns\nimport json\nimport pydicom\nimport glob\nfrom tqdm.notebook import tqdm\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut\nfrom skimage import exposure\nimport warnings\nfrom fastai.vision.all import *\nfrom fastai.medical.imaging import *\nwarnings.filterwarnings('ignore')","b92f41c6":"from albumentations import (\n    BboxParams,\n    HorizontalFlip,\n    VerticalFlip,\n    Resize,\n    CenterCrop,\n    RandomCrop,\n    Crop,\n    Compose,\n    Rotate,\n    CropNonEmptyMaskIfExists,\n    RandomSizedBBoxSafeCrop,\n    Blur,\n    ChannelDropout,\n    ChannelShuffle,\n    FancyPCA,\n    GaussNoise,\n    GaussianBlur,\n    GlassBlur,\n    HueSaturationValue,\n    ImageCompression,\n    InvertImg,\n    MedianBlur,\n    MotionBlur,\n    MultiplicativeNoise,\n    Normalize,\n    Posterize,\n    RGBShift,\n    RandomBrightnessContrast,\n    RandomFog,\n    RandomGamma,\n    RandomRain,\n    RandomSnow,\n    RandomSunFlare,\n    Solarize\n)","45894449":"train_folder = '..\/input\/siim-covid19-detection\/train'\ntest_folder = '..\/input\/siim-covid19-detection\/test'\ntrain_image_level = '..\/input\/siim-covid19-detection\/train_image_level.csv'\ntrain_study_level = '..\/input\/siim-covid19-detection\/train_study_level.csv'\nsample_submission = '..\/input\/siim-covid19-detection\/sample_submission.csv'","61e70db8":"train_data = pd.read_csv(train_image_level)","1a2a1a8b":"train_data.describe()","5f9ee7ec":"# DICOM to NP.array conversion\ndef dicom2array(path, voi_lut=True, fix_monochrome=True):\n    '''\n    function to convert DICOM images into np.array.\n    :param path: path of the DICOM image.\n    :param VOI LUT: used to transform DICOM data to more simpler data.\n    :param fix_monochrome: used to fix the inversion of X-Ray images.\n    :return: the Grayscale image converted to np.array format from DICOM format. \n    '''\n    dicom = pydicom.read_file(path)\n    # VOI LUT (if available by DICOM device) is used to\n    # transform raw DICOM data to \"human-friendly\" view\n    if voi_lut:\n        data = apply_voi_lut(dicom.pixel_array, dicom)\n    else:\n        data = dicom.pixel_array\n    # depending on this value, X-ray may look inverted - fix that:\n    if fix_monochrome and dicom.PhotometricInterpretation == \"MONOCHROME1\":\n        data = np.amax(data) - data\n    data = data - np.min(data)\n    data = data \/ np.max(data)\n    data = (data * 255).astype(np.uint8)\n    \n    return data\n\n# Image Path Extraction\ndef image_path(row):\n    '''\n    function to retrieve DICOM image path from the given directory structure.\n    :param row: ith row of a dataframe which would be parsed to get the image's path.\n    :return: image path w.r.t. the given directory.\n    '''\n    study_path = train_folder + '\/' + row.StudyInstanceUID\n    for i in get_dicom_files(study_path):\n        if row.id.split('_')[0] == i.stem: return i\n        \n# Image Visualization         \ndef plot_imgs(annotations, cols=4, size = 7, thickness = 13, img_resize = (500, 500)):\n    '''\n    function to display image using matplot-lib.\n    :param annotations: List of dictionaries containg the image, bboxes and category_ids to plot.\n    :param cols: number of columns to use for displaying the images.\n    :param size: to determine the size of the total plot w.r.t columns and rows.\n    :param thickness: the thickness of the rectangle border [bboxes] line in px. \n    :param img_resize: the new values for changing the size of the image\n    '''\n    rows = len(annotations)\/\/cols + 1\n    fig = plt.figure(figsize=(cols*size, rows*size))\n    for i, annotation in enumerate(annotations):\n        img = annotation['image']\n        bboxes = annotation['bboxes']\n        for box in bboxes:    \n            '''\n            xmin = xmin\n            ymin = ymin\n            xmax = xmin + w\n            ymax = ymin + h\n            '''\n            xmin = int(box[0])\n            ymin = int(box[1])\n            xmax = int((box[0]+box[2]))\n            ymax = int((box[1]+box[3]))\n            cv2.rectangle(img, (xmin, ymin), (xmax, ymax), [255, 0, 0], thickness)\n        img = cv2.resize(img, img_resize)\n        ax = fig.add_subplot(rows, cols, i+1)\n        plt.imshow(img, cmap = 'gray')\n        ax.title.set_text('Image_'+str(i))\n    plt.show()\n    \n# Data Cleaning\ndef clean_data(data):\n    '''\n    function to format the dataframe in order to make the images and bounding boxes more accessible.\n    :param data: Dataframe containing the original image ids and bounding boxes.\n    :return: Cleaned Up dataframe of the dataset containing.\n    '''\n    # Drop images with no BBoxes\n    data = data.dropna()\n    #Forming all the image paths for easy access.\n    data['image_path'] = data.apply(image_path, axis=1)\n    #Formatting bboxes = [[x, y, w, h], [x, y, w, h]] for data augmentation.\n    data['bboxes'] = data.boxes.apply(lambda x: list(list(x.values()) for x in eval(str(x))))\n    \n    return data\n\n# Annotation Creation\ndef get_annotations(data , no_images=4):\n    '''\n    function to retrieve annotations i.e. list of dictionaries containing the image, bboxes. \n    and category_id for randomly selected 'no_images' images.\n    :param data: Dataframe from which the image path and bboxes are selected.\n    :param no_images: Number of images to be converted into annotations.\n    :return: Annotations as List of Dictionaries containing image as an np.array, list of bboxes, and category_id.\n    '''\n    # These Images will be used for augmentation display. \n    # We randomly select n images and then convert them into np.array format.\n    # We then create an annotation list of dictionaries, as the Albumentation takes values as annotations.\n    image_paths = random.choices(data['image_path'].values, k=no_images)\n    imgs = [dicom2array(path) for path in image_paths]\n    annotations = []\n    for im in range(len(image_paths)):\n        image_path = image_paths[im]\n        # Get all the bound boxes as values to 'bboxes'.\n        bboxes_imgs = data.loc[data['image_path'] == image_path].bboxes.values.tolist()[0]\n        # Get all the images as values to 'image' in RGB 3 channels.\n        image = cv2.cvtColor(imgs[im], cv2.COLOR_GRAY2RGB)\n        annotations.append({'image': image, 'bboxes': bboxes_imgs, 'category_id': [1]*len(bboxes_imgs)})\n    return annotations","9356dcc2":"# def saveImage(path,annotations):\n    # Takes path and annotations and saves the images at that path.\n    # img = annotations['image'].copy()\n    # cv2.imwrite(path, img)\n\ndef get_albumentation_aug(aug, min_area=0., min_visibility=0.):\n    '''\n    function to get the transform object to apply albumentation transformation and handle all \n    transformations regarding bounding boxes.\n    :param aug: type of transformation object to create.\n    :param min_area: \n    :return: transform object \n    '''\n    return Compose(aug, bbox_params=BboxParams(format='coco', min_area=min_area, min_visibility=min_visibility, label_fields=['category_id']))\n\ndef augmentation(annotations, filter = 'VerticalFlip'):\n    '''\n    function to perform different augmentation on the images and then display them.\n    :param annotations: Image annotation containing image as np.array, bboxes and category id.\n    :param count: Number of Images to be augmentated \n    :param filter: Type of Filter to apply on images (RGB, Contrast, VerticalFlip, etc...)\n    '''\n    # Change default parameters for filters to see different effects on the data.\n    \n    # Save Augmented BBoxes and paths of data\n    # augmented_BBoxes = []\n    # path = []\n    augmented_annotations = []\n    for i, annotation in enumerate(annotations):\n        # Use this for further saving images\n        # filter_title = 'image_'+str(i)+'_'+filter\n        if filter == 'VerticalFlip':\n            aug = get_albumentation_aug([VerticalFlip(p=1)])\n        elif filter == 'HorizontalFlip':\n            aug = get_albumentation_aug([HorizontalFlip(p=1)])\n        elif filter == 'Blur':\n            aug = get_albumentation_aug([Blur(p=1)])\n        elif filter == 'ChannelDropout':\n            aug = get_albumentation_aug([ChannelDropout(p=1)])\n        elif filter == 'GaussianBlur':\n            aug = get_albumentation_aug([GaussianBlur(p=1)])\n        elif filter == 'GlassBlur':\n            aug = get_albumentation_aug([GlassBlur(p=1)])\n        elif filter == 'InvertImg':\n            aug = get_albumentation_aug([InvertImg(p=1)])\n        elif filter == 'RandomFog':\n            aug = get_albumentation_aug([RandomFog(p=1)])\n        elif filter == 'RandomGamma':\n            aug = get_albumentation_aug([RandomGamma(p=1)])\n        elif filter == 'RandomSunFlare':\n            aug = get_albumentation_aug([RandomSunFlare(p=1)])\n        elif filter == 'RGBShift':\n            aug = get_albumentation_aug([RGBShift(p=1)])\n        elif filter == 'RandomSizedBBoxSafeCrop':    \n            aug = get_albumentation_aug([RandomSizedBBoxSafeCrop(height=350,width=350,p=1)])\n        elif filter == 'GaussNoise':\n            aug = get_albumentation_aug([GaussNoise(p=1)])\n        elif filter == 'MultiplicativeNoise':\n            aug = get_albumentation_aug([MultiplicativeNoise(p=1)])\n        elif filter == 'RandomBrightnessContrast':\n            aug = get_albumentation_aug([RandomBrightnessContrast(p=1)])\n        augmented_annotation = aug(**annotation)\n        # saveImage(\".\/\"+ filter_title + \".png\", augmented_annotation)\n        # augmented_BBoxes.append(augmented_annotation['bboxes'])\n        # path.append(\".\/\"+ filter_title + \".png\")\n        augmented_annotations.append(augmented_annotation)\n    # pd.DataFrame({\"bboxes\": augmented_BBoxes, \"path\": path}).to_csv(\".\/Augmented_Data_\"+filter+\".csv\")\n    plot_imgs(augmented_annotations)","01ecf92b":"train_data = clean_data(train_data)\nannotations = get_annotations(train_data, 8)","cf22d4fe":"# Visualizing Base Images that will be used for augmentation\nplot_imgs(annotations)","cf29f56a":"augmentation(annotations, filter = 'VerticalFlip')","edf1a7a2":"augmentation(annotations, filter = 'HorizontalFlip')","addc9a13":"augmentation(annotations, filter = 'Blur')","95e45f1a":"augmentation(annotations, filter = 'GlassBlur')","9dfb41c4":"augmentation(annotations, filter = 'GaussianBlur')","f77f7252":"augmentation(annotations, filter = 'InvertImg')","52a5770c":"augmentation(annotations, filter = 'RandomFog')","7fa7c91c":"augmentation(annotations, filter = 'RandomGamma')","3134c671":"augmentation(annotations, filter = 'ChannelDropout')","979d1161":"augmentation(annotations, filter = 'RGBShift')","79a9b7fb":"augmentation(annotations, filter = 'RandomSizedBBoxSafeCrop')","117fceae":"augmentation(annotations, filter = 'RandomSunFlare')","67971e47":"augmentation(annotations, filter = 'GaussNoise')","5164ef7e":"## [RandomFog](https:\/\/albumentations.ai\/docs\/api_reference\/augmentations\/transforms\/#albumentations.augmentations.transforms.RandomFog)","0780880d":"Even though the dataset size seems large [119 GB] it is mostly because of DICOM images, if those images are stored as PNG, the dataset size would be 3 to 4 GB as it only contains about 6K images. \nThus, data augmentation will play as a game changer for training different model architectures in this competition.\n\n* As part of the article on [data augmentation techniques](https:\/\/www.kaggle.com\/c\/siim-covid19-detection\/discussion\/242760), this notebook shows different simple augmentation applied to different images.\n\n* Each section can be accessed from the right side by it's specific name.\n\n* Each Augmentation is also marked with it's Albumentation Definition so that one can change it's default parameters and play around with the data.\n\n* After applying the augmentation, one can save the augmented images [Code Present in Augmentation Utility Section] or can integrate these augmentation techniques on the fly with their models.\n\nPlease ask any questions in the comments below.\n\nThank You!!\n","e50b4eda":"## [InvertImg](https:\/\/albumentations.ai\/docs\/api_reference\/augmentations\/transforms\/#albumentations.augmentations.transforms.InvertImg)","0605268c":"## [RandomSunFlare](https:\/\/albumentations.ai\/docs\/api_reference\/augmentations\/transforms\/#albumentations.augmentations.transforms.RandomSunFlare)","9bd1d75d":"## Base Images","46f39014":"## [RandomGamma](https:\/\/albumentations.ai\/docs\/api_reference\/augmentations\/transforms\/#albumentations.augmentations.transforms.RandomGamma)","0c411cd7":"## [Blur](https:\/\/albumentations.ai\/docs\/api_reference\/augmentations\/transforms\/#albumentations.augmentations.transforms.Blur)","adffcdda":"## [GaussianBlur](https:\/\/albumentations.ai\/docs\/api_reference\/augmentations\/transforms\/#albumentations.augmentations.transforms.GaussianBlur)","9239dd49":"## [Horizontal Flip](https:\/\/albumentations.ai\/docs\/api_reference\/augmentations\/transforms\/#albumentations.augmentations.transforms.HorizontalFlip)","24f22856":"## [RandomSizedBBoxSafeCrop](https:\/\/albumentations.ai\/docs\/api_reference\/augmentations\/crops\/transforms\/#albumentations.augmentations.crops.transforms.RandomSizedBBoxSafeCrop)","d846c15b":"## Albumentation Utility Functions","ffd645a7":"## INIT","0405db0e":"## [Vertical Flip](https:\/\/albumentations.ai\/docs\/api_reference\/augmentations\/transforms\/#albumentations.augmentations.transforms.VerticalFlip)","888a4cd8":"## Simple Data Augmentation Techniques Using Albumentation","5e5a8145":"## [Reference for Helper Functions](https:\/\/www.kaggle.com\/tanlikesmath\/siim-covid-19-detection-a-simple-eda#A-look-at-the-images)","bcb1b92a":"## [RGBShift](https:\/\/albumentations.ai\/docs\/api_reference\/augmentations\/transforms\/#albumentations.augmentations.transforms.RGBShift)","5a473c8b":"## [GaussianNoise](https:\/\/albumentations.ai\/docs\/api_reference\/augmentations\/transforms\/#albumentations.augmentations.transforms.GaussNoise)","c0625464":"* There are several other [augmentation techniques](https:\/\/albumentations.ai\/docs\/) as well as these techniques can be combined with each other and passed to a model for training. \n\n* Furthermore, one can also use [AutoAlbument](https:\/\/albumentations.ai\/docs\/autoalbument\/) which learns image augmentation policies from image itself using Faster AutoAugment Algorithm. This allows us to automatically select the best augmentation techniques for the given task.\n\nPlease ask any questions in the comments below.\n\nThank You!!","6d763fda":"## [GlassBlur](https:\/\/albumentations.ai\/docs\/api_reference\/augmentations\/transforms\/#albumentations.augmentations.transforms.GlassBlur)","0d658bfe":"## End Note","dbabc7ee":"## [ChannelDropout](https:\/\/albumentations.ai\/docs\/api_reference\/augmentations\/transforms\/#albumentations.augmentations.transforms.ChannelDropout)"}}