{"cell_type":{"b77bdca0":"code","2acbb808":"code","80676cd2":"code","cfe69c4b":"code","60672856":"code","ca590ef5":"code","dab6a8d8":"code","d56145c2":"code","103fd309":"code","f68e46fe":"code","554407d5":"code","03062b79":"code","353fac4b":"code","f35bb5a2":"code","98e654f5":"code","37280378":"code","e3a72080":"code","13c11261":"code","28b3fbee":"code","2fb27532":"code","88504a0a":"code","ab2ad3dd":"code","3650d66b":"code","82c796c3":"code","6e5f9a71":"code","da105226":"code","cbc01a15":"code","9e67ab61":"code","1e68cbd5":"code","04fdec0a":"code","0c3e7328":"code","d4d0db4d":"code","482645bb":"code","28ad44fb":"code","445d7fbd":"code","cd5551dd":"code","fa2b8c1e":"code","cea25b5a":"code","508e03ae":"code","ee5118e2":"code","a4893134":"code","ed25752d":"code","777dca0a":"markdown","06e4da96":"markdown","dd3e9329":"markdown","052071e7":"markdown","d5d73248":"markdown","fcc0cafe":"markdown","02897dec":"markdown","025a327a":"markdown","cd57c05c":"markdown","bc25d895":"markdown","f264161d":"markdown","a694ce2d":"markdown","18a6b0b6":"markdown","573fec40":"markdown","8cff1768":"markdown","0e70ef12":"markdown","292a8554":"markdown","cdee7603":"markdown","ecbc94aa":"markdown","2f510b4a":"markdown","dfa375e3":"markdown","3c221e50":"markdown","b9bcf215":"markdown","9c4decf4":"markdown","0f9a40b0":"markdown","b8c60228":"markdown","6f7870c7":"markdown","3fc6879f":"markdown","22cd0b36":"markdown","ecc3b86f":"markdown","c6f3526a":"markdown","9f000469":"markdown","d12436e6":"markdown","c30e7e2f":"markdown","da0d5257":"markdown","6b367364":"markdown","1ce381e2":"markdown","ced15a70":"markdown"},"source":{"b77bdca0":"# BASIC LIBRARIES\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\n\n# SKLEARN TRANSFORMATIONS AND PIPELINES\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.decomposition import PCA\n\n# NON-SUPERVISED LEARNING\nfrom sklearn.cluster import DBSCAN\n\n# SUPERVISED LEARNING - NEURAL NETWORKS LIBS\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Dropout, AlphaDropout, Input, Dense, concatenate\nfrom keras.utils.vis_utils import plot_model\n\n# SUPERVISED LEARNING - PART 1 LIBS\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\n\n# MODEL SELECTION LIBS\nimport tensorflow.keras.backend as K\n\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.callbacks import TensorBoard, EarlyStopping, LearningRateScheduler\nfrom keras.optimizers import Nadam\n\n# PLOTTING LIBS\nimport seaborn as sns\nimport matplotlib as mpl\n\n# ACESSORY LIBRARIES\nimport re\nimport os\nimport dill\nimport time\nimport math\nimport random\n\nfrom collections import Counter\nfrom IPython.display import Image, FileLinks, display","2acbb808":"tf.random.set_random_seed(42)\nrandom.seed(42)\nnp.random.seed(42)","80676cd2":"data_dir = '\/kaggle\/input\/titanic'\nK.clear_session()\n%load_ext tensorboard.notebook","cfe69c4b":"%tensorboard --logdir logs","60672856":"def get_title(dataframe_in):\n    dataframe_in['Title'] = dataframe_in['Name'].apply(lambda X: re.search('[A-Z]{1}[a-z]+\\.', X).group(0))\n    return dataframe_in\n\ndef classify_title(dataframe_in):\n    dataframe_in.loc[:, ['Title']] = dataframe_in['Title'].apply(lambda X: X if X in ['Mr.', 'Miss.', 'Mrs.', 'Master.'] else 'Rare')\n    return dataframe_in","ca590ef5":"def get_family_name(dataframe_in):\n    dataframe_in['FamilyName'] = dataframe_in['Name'].apply(lambda X: X.split(',')[0])\n    return dataframe_in","dab6a8d8":"def get_cabin_letter(dataframe_in):\n    def first_letter_if_exists(str_in):\n        if pd.isnull(str_in):\n            return '?'\n        return str_in[0]\n    dataframe_in['Cabin'] = dataframe_in['Cabin'].apply(first_letter_if_exists)\n    return dataframe_in","d56145c2":"def get_cabins_per_family(dataframe_in):\n    dataframe_in = get_family_name(dataframe_in)\n    dict_cabins_per_family = dict()\n    for current_family_name in dataframe_in.FamilyName.unique().tolist():\n    \n        filter_family_name = (dataframe_in['FamilyName'] == current_family_name)\n        filter_known_cabin = (dataframe_in['Cabin'] != '?')\n        listCabinsFromFamily = dataframe_in.loc[(filter_family_name) & (filter_known_cabin)].Cabin.unique().tolist()\n    \n        if len(listCabinsFromFamily) > 0:\n            max_v, mode = 0, None\n            for curr_cabin, v in Counter(listCabinsFromFamily).items():\n                if v > max_v:\n                    max_v, mode = v, curr_cabin\n            dict_cabins_per_family[current_family_name] = mode\n    \n    return dict_cabins_per_family","103fd309":"def get_family_cabin_per_row(df_row, dict_family_in):\n    dict_cabins_per_family = dict_family_in\n    \n    if df_row.FamilyName in dict_cabins_per_family and df_row.Cabin == '?':\n        out = dict_cabins_per_family[df_row.FamilyName]\n    else:\n        out = df_row.Cabin\n    return out","f68e46fe":"def get_family_cabin(df, dict_family_in):\n    \n    df['Cabin'] = df.apply(lambda X: get_family_cabin_per_row(X, dict_family_in), axis = 1)\n    df['Cabin'] = df['Cabin'].fillna('?')\n    \n    return df","554407d5":"def get_ticket_prefix(dataframe_in):\n    dataframe_in['Ticket'] = dataframe_in['Ticket'].apply(lambda X: X.split(' ')[0] if len(X.split(' ')) > 1 else '?')\n    return dataframe_in","03062b79":"def get_family_info(df_in):\n    df_in['FamilyMembers'] = df_in['Parch'] + df_in['SibSp'] + 1\n    df_in['Is_Mother'] = np.where((df_in.Title=='Mrs.') & (df_in.Parch >0), 1, 0)\n    return df_in","353fac4b":"def fill_age_from_masters(df, strategy_in = 'median'):\n    is_master = (df['Title'] == 'Master.')\n    imp = SimpleImputer(missing_values = np.nan, strategy = strategy_in)\n    df.loc[is_master, 'Age'] = imp.fit_transform(df.loc[is_master][['Age']])\n    return df","f35bb5a2":"def fill_age_from_non_masters(df, strategy_in = 'median'):\n    is_not_master = (df['Title'] != 'Master.')\n    imp = SimpleImputer(missing_values = np.nan, strategy = strategy_in)\n    df.loc[is_not_master, 'Age'] = imp.fit_transform(df.loc[is_not_master][['Age']])\n    return df","98e654f5":"class TransformerSignificantData(BaseEstimator, TransformerMixin):\n    \n    def __init__(self):\n        return\n        \n    def fit(self, X, y = None):\n        return self\n    \n    def transform(self, X, y = None):\n        \n        out = get_cabin_letter(X)\n        out = get_ticket_prefix(out)\n        out = get_title(out)\n        out = get_family_info(out)\n        out = classify_title(out)\n        out = get_family_name(out)\n        dict_cabins_per_family = get_cabins_per_family(out)\n        out = get_family_cabin(out, dict_cabins_per_family)\n        \n        return out","37280378":"class TransformerDummify(BaseEstimator, TransformerMixin):\n    \n    def __init__(self):\n        return\n    \n    def fit(self, X, y = None):\n        return self\n    \n    def transform(self, X, y = None):\n        \n        columns_to_dummify = ['Sex', 'Cabin', 'Embarked', 'Title', 'Ticket']\n        useless_columns = ['PassengerId', 'Name', 'FamilyName']\n        dim_redundant_cols = ['Sex_male', 'Cabin_?', 'Embarked_S', 'Title_Rare', 'Ticket_?']\n        \n        out_dummies = pd.get_dummies(X[columns_to_dummify], prefix = columns_to_dummify)\n        \n        out = pd.concat([X, out_dummies], axis = 1)\n        out = out.drop(useless_columns + columns_to_dummify + dim_redundant_cols, axis = 1)\n        \n        return out","e3a72080":"class TransformerMissingData(BaseEstimator, TransformerMixin):\n    \n    def __init__(self, missing_age_masters_strategy='mean', missing_age_non_masters_strategy='cluster'):\n        self.missing_age_masters_strategy = missing_age_masters_strategy\n        self.missing_age_non_masters_strategy = missing_age_non_masters_strategy\n        \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n\n        is_not_master = (X['Title_Master.'] == 0)\n        is_master = (X['Title_Master.'] == 1)\n        \n        imp_non_master = SimpleImputer(missing_values = np.nan, \n                                       strategy = self.missing_age_non_masters_strategy)\n        \n        imp_master = SimpleImputer(missing_values = np.nan, \n                                   strategy = self.missing_age_masters_strategy)\n        \n        imp_fare = SimpleImputer(missing_values = np.nan,\n                                 strategy = 'mean')\n        \n        X.loc[is_master, 'Age'] = imp_master.fit_transform(X.loc[is_master, ['Age']])\n        X.loc[:, 'Fare'] = imp_fare.fit_transform(X.loc[:, ['Fare']])\n        \n        if self.missing_age_non_masters_strategy in ['mean', 'median']:\n            X.loc[is_not_master, 'Age'] = imp_non_master.fit_transform(X.loc[is_not_master, ['Age']])\n            \n        elif self.missing_age_non_masters_strategy == 'cluster':\n            \n            X_without_age = X.drop(['Age', 'Survived'], axis = 1)\n            pca_fit_vec = PCA(n_components = 3).fit_transform(X_without_age)\n            \n            X['ClusterLabel'] = DBSCAN(eps = 0.7, min_samples = 20).fit_predict(pca_fit_vec)\n            dict_means_per_cluster = dict()\n            for cluster_label in X['ClusterLabel'].tolist():\n                dict_means_per_cluster[cluster_label] = X.loc[X['ClusterLabel'] == cluster_label, :]['Age'].mean()\n            \n            age_list = X['Age'].tolist()\n            cluster_list = X['ClusterLabel'].tolist()\n            for i, curr_age in enumerate(age_list):\n                if np.isnan(curr_age):\n                    age_list[i] = dict_means_per_cluster[cluster_label]\n            X['Age'] = age_list\n            X.drop('ClusterLabel', axis=1, inplace=True)\n                \n        return X","13c11261":"class TransformerNormalize(BaseEstimator, TransformerMixin):\n    \n    def __init__(self):\n        return\n    \n    def fit(self, X, y = None):\n        return self\n    \n    def transform(self, X, y = None):\n        \n        out = X\n        out.loc[:, ['Pclass']] = MinMaxScaler().fit_transform(out[['Pclass']])\n        out.loc[:, ['Age', 'Fare']] = StandardScaler().fit_transform(out[['Age', 'Fare']])\n        \n        return out","28b3fbee":"feature_engineering_pipeline = Pipeline([\n    ('prepare', TransformerSignificantData()),\n    ('dummify', TransformerDummify()),\n    ('normalize', TransformerNormalize()),\n    ('missing', TransformerMissingData())\n])","2fb27532":"def split_cv_test(df_all_processed):\n    df_ans = df_all_processed[df_all_processed['Survived'].isnull()]\n    df_ans = df_ans.loc[:, df_ans.columns != 'Survived']\n    df_cv_and_test = df_all_processed[df_all_processed['Survived'].notnull()]\n    df_cv_and_test_X = df_cv_and_test.drop(['Survived'], axis = 1)\n    df_cv_and_test_Y = df_cv_and_test['Survived']\n    df_cv_X, df_test_X, df_cv_Y, df_test_Y = train_test_split(df_cv_and_test_X,\n                                                              df_cv_and_test_Y,\n                                                              test_size = 0.1)\n    return df_cv_X, df_cv_Y, df_test_X, df_test_Y, df_ans, df_cv_and_test_X, df_cv_and_test_Y","88504a0a":"df_in = [pd.read_csv(data_dir + '\/train.csv'), pd.read_csv(data_dir + '\/test.csv')]\ndf_in = pd.concat(df_in, ignore_index = True, sort = False)\ndf_in.head()","ab2ad3dd":"df_in_norm = feature_engineering_pipeline.transform(df_in)\nprint(df_in_norm.describe())","3650d66b":"n_col = len(df_in_norm.columns)\ndef nn_layers_and_activation(n_hidden_layers, activation_func, dropout=None):\n    \n    in_layer_args = dict(\n        units = n_col - 1,\n        activation = activation_func,\n        input_dim = n_col - 1,\n        kernel_initializer = 'lecun_uniform'\n    )\n    \n    out_layer_args = dict(\n        units = 1,\n        activation = 'sigmoid',\n        input_dim = n_col - 1,\n        kernel_initializer = 'lecun_uniform'\n    )\n    \n    model_args = dict(\n        loss = 'binary_crossentropy',\n        optimizer = 'nadam',\n        metrics = ['binary_accuracy']\n    )\n    \n    SelectedDropout = Dropout if dropout == 'Dropout' else AlphaDropout\n    out_model = Sequential()\n    for i in range(0, n_hidden_layers + 1):\n        out_model.add(Dense(**in_layer_args))\n        if dropout is not None:\n            out_model.add(SelectedDropout(rate=0.5))\n        \n    out_model.add(Dense(**out_layer_args))\n    out_model.compile(**model_args)\n    return out_model\n    \n\ntest_nn = nn_layers_and_activation(1, 'relu')\nprint(test_nn)","82c796c3":"df_cv_X, df_cv_Y, df_test_X, df_test_Y, df_ans, df_cv_and_test_X, df_cv_and_test_Y = split_cv_test(df_in_norm)\nprint(df_cv_X.head())","6e5f9a71":"wrapped_tune_layers_and_activation = KerasClassifier(build_fn=nn_layers_and_activation, \n                                                     batch_size=64, \n                                                     verbose=0)\n\nnn_pipeline = Pipeline([\n    ('tune_layers_and_activation', wrapped_tune_layers_and_activation)\n])\n\ndef gen_scheduler(eta=0.1, epoch0=100.0):\n    def exp_decay_fn(epoch, lr):\n        return lr * eta ** (epoch \/ epoch0)\n    return LearningRateScheduler(exp_decay_fn)\n\ndict_params_standard = dict(\n    tune_layers_and_activation__n_hidden_layers = [1, 3, 5, 10],\n    tune_layers_and_activation__epochs = [100],\n    tune_layers_and_activation__activation_func = ['sigmoid', 'relu', 'tanh', 'selu'],\n    tune_layers_and_activation__dropout = [None],\n    tune_layers_and_activation__callbacks = [[gen_scheduler(epoch0=X), \n                                              EarlyStopping(patience=10, restore_best_weights=False)]\\\n                                              for X in [1000]]\n)\ndict_params_dropout = dict(\n    tune_layers_and_activation__n_hidden_layers = [1, 3, 5, 10],\n    tune_layers_and_activation__epochs = [100],\n    tune_layers_and_activation__activation_func = ['sigmoid', 'relu', 'tanh', 'selu'],\n    tune_layers_and_activation__dropout = ['Dropout'],\n    tune_layers_and_activation__callbacks = [[gen_scheduler(epoch0=X), \n                                              EarlyStopping(patience=10, restore_best_weights=False)]\\\n                                              for X in [1000]]\n)\ndict_params_alphadropout = dict(\n    tune_layers_and_activation__n_hidden_layers = [1, 3, 5, 10],\n    tune_layers_and_activation__epochs = [100],\n    tune_layers_and_activation__activation_func = ['sigmoid', 'relu', 'tanh', 'selu'],\n    tune_layers_and_activation__dropout = ['AlphaDropout'],\n    tune_layers_and_activation__callbacks = [[gen_scheduler(epoch0=X), \n                                              EarlyStopping(patience=10, restore_best_weights=False)]\\\n                                              for X in [1000]]\n)","da105226":"cv_pipeline_standard = GridSearchCV(estimator=nn_pipeline, param_grid=dict_params_standard, \n                                    scoring='accuracy', cv=3, verbose=10, n_jobs=-1)\n\ncv_pipeline_dropout = GridSearchCV(estimator=nn_pipeline, param_grid=dict_params_dropout, \n                                   scoring='accuracy', cv=3, verbose=10, n_jobs=-1)\n\ncv_pipeline_alphadropout = GridSearchCV(estimator=nn_pipeline, param_grid=dict_params_alphadropout, \n                                        scoring='accuracy', cv=3, verbose=10, n_jobs=-1)\n\nprint('Fitting model without Dropout')\ncv_pipeline_standard.fit(df_cv_X, df_cv_Y)\nprint('Fitting model with Dropout')\ncv_pipeline_dropout.fit(df_cv_X, df_cv_Y)\nprint('Fitting model with AlphaDropout')\ncv_pipeline_alphadropout.fit(df_cv_X, df_cv_Y)","cbc01a15":"for i, cv_pipeline in enumerate([cv_pipeline_standard, cv_pipeline_dropout, cv_pipeline_alphadropout]):\n    list_models = ['No Dropout', 'Std Dropout', 'AlphaDropout']\n    \n    print('----')\n    print(list_models[i] + '- Results:')\n    print('----')\n    print('Paramaters:')\n    dict_params = cv_pipeline.best_estimator_.named_steps['tune_layers_and_activation'].get_params()\n    print('Act. Fn: ' + dict_params['activation_func'])\n    print('Model: ' + (dict_params['dropout'] if dict_params['dropout'] is not None else 'No Dropout'))\n    print('N. Hidden Layers: ' + str(dict_params['n_hidden_layers']))\n    \n    print('Optimal Exp. Decay:')\n    dict_params = cv_pipeline.best_estimator_.named_steps['tune_layers_and_activation'].get_params()\n    opt_scheduler = dict_params['callbacks'][0]\n    opt_scheduler_fn = opt_scheduler.__dict__['schedule']\n    opt_epoch0 = 1 \/ (math.log10(opt_scheduler_fn(0, 1)) - math.log10(opt_scheduler_fn(1, 1)))\n    print(f'Epoch0 = %f' % opt_epoch0)\n    \n    print('Best Score:')\n    print('{0:.0%}'.format(cv_pipeline.best_score_))\n    print('\\n')","9e67ab61":"cv_pipeline_standard.cv_results_['split0_test_score']\n\ndef get_cv_compared_results(cv_obj_list, cv_obj_names):\n    \n    obj_cv0 = cv_obj_list[0]\n    n_splits = obj_cv0.n_splits_\n    param_list = obj_cv0.param_grid.keys()\n    dict_df_cv = dict()\n    \n    for param in param_list:\n        dict_df_cv[param.split('__')[1]] = []\n        dict_df_cv['OPT__' + param.split('__')[1]] = []\n        \n    dict_df_cv['Score'] = []\n    dict_df_cv['Split'] = []\n    dict_df_cv['Model'] = []\n    \n    df_compared_results = pd.DataFrame(dict_df_cv)\n    for idx, obj_cv in enumerate(cv_obj_list):\n        \n        obj_name = cv_obj_names[idx]\n        opt_dict = obj_cv.best_estimator_.named_steps['tune_layers_and_activation'].get_params()\n        \n        for i, param in enumerate(obj_cv.cv_results_['params']):\n            for j in range(0, n_splits):\n                \n                dict_new_row = dict()\n                    \n                dict_new_row['Score'] = [obj_cv.cv_results_['split' + str(j) + '_test_score'][i]]\n                dict_new_row['Split'] = [j]\n                dict_new_row['Model'] = [obj_name]\n                \n                for param_id, param_val in param.items():\n                    \n                    param_name = param_id.split('__')[1]\n                    \n                    dict_new_row[param_name] = [param_val]\n                    dict_new_row['OPT__' + param_name] = [opt_dict[param_name]]\n                    \n            df_compared_results = pd.concat([df_compared_results, pd.DataFrame(dict_new_row)], axis=0, ignore_index=True)\n\n    return df_compared_results\n        \n    \ncv_obj_list = [cv_pipeline_standard, cv_pipeline_dropout, cv_pipeline_alphadropout]\ncv_names_list = ['No Dropout', 'Simple Dropout', 'AlphaDropout']\n\ndf_compared_results = get_cv_compared_results(cv_obj_list, cv_names_list)\ndf_compared_results.head()","1e68cbd5":"act_fn_used = df_compared_results['activation_func'] == df_compared_results['OPT__activation_func']\ndf_compared_results_fn_filtered = df_compared_results.loc[act_fn_used, :]\ndf_compared_results_fn_filtered.head()","04fdec0a":"df_compared_results_fn_filtered.dtypes","0c3e7328":"mpl.rcParams['figure.figsize'] = [10, 10]\nsns.set_style(\"darkgrid\")\ndf_compared_results_fn_filtered['n_hidden_layers_jitter'] = df_compared_results_fn_filtered['n_hidden_layers'].apply(lambda X: X + 0.1 * np.random.normal())\nsns.scatterplot(data=df_compared_results_fn_filtered, x='n_hidden_layers_jitter', y='Score', hue='Model',  linewidth=1, s=1000, alpha=0.4, edgecolor='k')","d4d0db4d":"df_submission = pd.read_csv(data_dir + '\/test.csv')\ndf_submission = df_submission.loc[:, ['PassengerId', 'Survived']]\n\nfor i, cv_obj in enumerate([cv_pipeline_standard, cv_pipeline_dropout, cv_pipeline_alphadropout]):\n    tensor_board_callback = [TensorBoard('.\/logs\/Model-' + str(i + 1), histogram_freq=0, write_graph=False)]\n    cbcks_list = cv_obj.best_estimator_.steps[0][1].__dict__['sk_params']['callbacks'] + tensor_board_callback\n    cv_obj.best_estimator_.fit(df_cv_X, df_cv_Y, \n                               tune_layers_and_activation__callbacks=cbcks_list,\n                               tune_layers_and_activation__validation_data=(df_test_X, df_test_Y))\n    \n    cv_obj.best_estimator_.fit(df_cv_and_test_X, df_cv_and_test_Y, \n                               tune_layers_and_activation__callbacks=cv_obj.best_estimator_.steps[0][1].__dict__['sk_params']['callbacks'])\n\n    \n    df_submission['Survived'] = cv_obj.best_estimator_.predict(df_ans)\n    df_submission['Survived'] = df_submission['Survived'].apply(int)\n    df_submission.to_csv('Model-' + str(i + 1) + '-submission.csv', index=False)","482645bb":"with K.learning_phase_scope(1):\n    np_mc_probs_per_model = np.stack(cv_pipeline_dropout.best_estimator_.predict(df_ans) for _ in range(100))\n    \nnp_mc_probs = np.mean(np_mc_probs_per_model, axis=0)\nnp_mc_ans = np.round(np_mc_probs)\n\ndf_submission['Survived'] = np_mc_ans\ndf_submission['Survived'] = df_submission['Survived'].apply(int)\ndf_submission = df_submission.loc[:, ['PassengerId', 'Survived']]\ndf_submission.to_csv('Model-MCDropOut-submission.csv', index=False)","28ad44fb":"dict_rf = dict(\n    n_estimators = 85,\n    max_depth = 8\n)\n\ndict_svm = dict(\n    C = 7.0,\n    kernel = 'rbf',\n    gamma = 'auto',\n    probability = True\n)\n\ndict_logit = dict(\n    C = 1.0,\n    solver = 'lbfgs'\n)\n\ndict_adab = dict(\n    n_estimators = 54,\n    learning_rate = 0.35\n)\n\ndict_xgboost = dict(\n    gamma =0.05,\n    max_depth = 3,\n    probability = True\n)","445d7fbd":"rf_model = RandomForestClassifier(**dict_rf).fit(df_cv_and_test_X, df_cv_and_test_Y)\nsvm_model = SVC(**dict_svm).fit(df_cv_and_test_X, df_cv_and_test_Y)\nlogit_model = LogisticRegression(**dict_logit).fit(df_cv_and_test_X, df_cv_and_test_Y)\nadab_model = AdaBoostClassifier(**dict_adab).fit(df_cv_and_test_X, df_cv_and_test_Y)\nxgboost_model = XGBClassifier(**dict_xgboost).fit(df_cv_and_test_X, df_cv_and_test_Y)","cd5551dd":"dict_train_X_wide = dict()\ndict_valid_X_wide = dict()\ndict_ans_wide = dict()\n\ndf_list = [df_cv_X, df_test_X, df_ans]\nsimple_models_names = ['RF', 'SVM', 'LOGISTIC', 'ADABOOST', 'XGBTREE']\nfor i, mod in enumerate([rf_model, svm_model, logit_model, adab_model, xgboost_model]):\n    for j, curr_dict in enumerate([dict_train_X_wide, dict_valid_X_wide, dict_ans_wide]):\n        curr_dict[simple_models_names[i]] = [X[0] for X in mod.predict_proba(df_list[j])[:, [0]]]\n        \ndf_cv_X_wide = pd.DataFrame(dict_train_X_wide)\ndf_test_X_wide = pd.DataFrame(dict_valid_X_wide)\ndf_ans_wide = pd.DataFrame(dict_ans_wide)","fa2b8c1e":"n_wide = len(simple_models_names)\nn_deep = len(df_cv_X.columns)\n\ndeep_input = Input(shape=(n_deep,))\nwide_input = Input(shape=(n_wide,))\n\nopt_n_hidden = cv_pipeline_dropout.best_estimator_.named_steps['tune_layers_and_activation'].get_params()['n_hidden_layers']\nopt_act_fn = cv_pipeline_dropout.best_estimator_.named_steps['tune_layers_and_activation'].get_params()['activation_func']\n\nif opt_n_hidden > 0:\n    hidden = Dense(n_deep, activation=opt_act_fn, kernel_initializer='lecun_uniform')(deep_input)\n    hidden = Dropout(rate=0.5)(hidden)\n    for _ in range(opt_n_hidden):\n        hidden = Dense(n_deep, activation=opt_act_fn, kernel_initializer='lecun_uniform')(hidden)\n        hidden = Dropout(rate=0.5)(hidden)\n        \noutput_layer = Dense(1, activation='sigmoid',  kernel_initializer='lecun_uniform')(concatenate([wide_input, hidden]))\n\nmodel = Model(inputs=[wide_input, deep_input], output=output_layer)\nmodel.compile(optimizer='nadam', \n              loss='binary_crossentropy', \n              metrics=['accuracy'])\n\nprint(model.summary())","cea25b5a":"plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\ndisplay(Image(filename='model_plot.png'))","508e03ae":"tb_wide_and_deep = TensorBoard(log_dir='.\/logs\/wide_and_deep', histogram_freq=1, write_graph=False, write_images=False)\ncbcks_list = [tb_wide_and_deep, EarlyStopping(patience=10, restore_best_weights=False)]\nmodel.fit([df_cv_X_wide, df_cv_X], df_cv_Y, validation_data=([df_test_X_wide, df_test_X], df_test_Y), epochs=100, callbacks=cbcks_list, verbose=0)","ee5118e2":"cbcks_list = [EarlyStopping(patience=10, restore_best_weights=False)]\nmodel.fit([pd.concat([df_cv_X_wide, df_test_X_wide], axis=0, ignore_index=True), \n           pd.concat([df_cv_X, df_test_X], axis=0, ignore_index=True)], \n          pd.concat([df_cv_Y, df_test_Y], axis=0, ignore_index=True), epochs=100, callbacks=cbcks_list, verbose=0)","a4893134":"df_submission['Survived'] = model.predict([df_ans_wide, df_ans])\ndf_submission['Survived'] = df_submission['Survived'].apply(int)\ndf_submission.to_csv('Model-DropoutWideAndDeep-submission.csv', index=False)","ed25752d":"FileLinks('.')","777dca0a":"## 2.C.2. Train and Test Splitting Procedure","06e4da96":"Separe ticket prefixes","dd3e9329":"## 2.B.2. Transformer Dummify\nAs the name suggests, it's the transformation responsible to the \"dummification\" of variables by using one-hot encoding and removind one category per feature hot-encoded (to avoid the curse of dimentionality).","052071e7":"We saw that all the people with a \"Master\" title were really young. Then, the variance of that group was small and it was possible to separe the algorithm in two partes: (1) filling the missing age of the \"Masters\" (which can be done easily by simply filling the NA's with the median or average value of the master people) and (2) filling the age missing values of the \"Non Masters\", which can be done with the same procedure.\n\nFilling the ages with means and medians to the second case can lead us many mistakes since the variance is not small to that group. That's why we propose to use a clustering algorithm to separe the people in different groups.","d5d73248":"Let's create dictionaries with the optimal results already found in the previous notebook.","fcc0cafe":"Finally, let's check the training curves of the model using the TensorFlow backend:","02897dec":"Functions to separe and classify the titles - Classifying the titles is important to reduce the number of classifications after aggregating the rare occurrences.","025a327a":"## 2.A.2. Missing Data Transformations","cd57c05c":"Get informations about the family structure of each passenger","bc25d895":"# 3. Starting the Game - Titanic - A NN Approach\nApplying Preprocessing Pipeline\nLet's read and pre-process our data just like before:","f264161d":"# 2. Taking Previous Results\nWhat do we already have? Simple:\n* 2.A The functions used in the transformations\n* 2.B The transformations used in the pipelines\n* 2.C The feature importance obtained during the Random Forest Algorithm (which can serve as a subside for us)\n\n# 2.A Functions Used in the Transformations","a694ce2d":"Let's take the code to split the train \/ validation set and the testing set as a function. Remember that the training and validating occur in the same set because we are using a Cross Validation K-Fold procedure:","18a6b0b6":"Then apply the function over all the dataframe...","573fec40":"Let's print the model graph...","8cff1768":"Training the model, just like in the first part...","0e70ef12":"## 2.A.1. Significant Data Transformations","292a8554":"# 2.C. Pipelines","cdee7603":"Adding the models' outputs as features to the test and validation sets. The new features will be used in the wide layer of the NN while the other features will be present on the \"deep\" part.","ecbc94aa":"## 2.C.1. Feature Engineering Pipeline\nThe pipeline where the data is preprocessed according to the transformations exposed in the previous step. The hyper-parameters of those transformations can, with this instance, be tunned:","2f510b4a":"Time to create the wide and deep model. Previously, we have used the \"Sequencial Keras API\". This time, we will use the \"Functional API\":","dfa375e3":"Function to extract the family from the \"Name\" feature.","3c221e50":"# 3.A. Defyning Neural Network Model to Apply\nTo create our neural network model, we will use the \"Keras Library\". The idea here is to test different architectures, enhancing the hyperparameters little by little. It's impossible to GridSearch over all possible neural networks that we can apply. We are going to tune the number of hidden layers and the activation function for three different models:\n\n* An ordinary Feed Foward Neural Network\n* A Neural Network using a standard DropOut function\n* A Neural Network using an AlphaDropOut function\n\nThe DropOut's will employ a DropOut rate of $50\\%$.","b9bcf215":"# 3.B. Prepare a TensorBoard and inspect the Neural Networks in a Separated Validation Set\nThe Cross-Validation procedure executes the hyper-parameter tunning over different splits of the same set. For that reason, to inspect the generalization power of our models we are going to look out of that set and inspect the Learning Curves of the $3$ optimal models (without Dropping Out, with DropOut and with AlphaDropout) using the TensorBoard tool. The model fitting will be executed over all the Cross Validation data to inspect the results when we increase the number of samples for each Neural Network.","9c4decf4":"# 5. A Final Try: Using the Results of Part 1\nThe best model found during the validation step (when looking at the TensorBoard results) were:\n* The ordinary feed foward neural network using 5 layers and\n* The simple DropOut with 3 layers using the $tanh(x)$ activation function\nThe AlphaDropout Neural Network is generally combined with the $selu$ activation function, which make sense with the found results but it's commonly more suitable to Deep Learning problems (which is not our case sice it was not necessary to increase our hidden layers number to a huge quantity, like 100 or even 1000).\n\nSo, our last try will be to develop a Wide and Deep network using the DropOut model [which produced a slightly better result]: the \"Deep\" part will use 5 layers or 3 layers (depending on the model) and we will add a \"Wide\" part using the results found in each one of the individual models used in the last notebook (check [the Part 1](https:\/\/www.kaggle.com\/guidant\/corvus-part-1-top-15-combining-5-models#5.-Final-Submission) for more details). \n\nTraining those models with the optimal parameters found on the previous Notebook...","0f9a40b0":"## Previous Optimal Results:\n\n---\n* Random Forest\n    * n_estimators: 85 -- FROM [85, 86, 87, 88, 89]\n    * max_depth: 8 -- FROM [3, 4, 5, 6, 7, 8, 9, 10]\n\n---\n* SVM\n    * C: 7.0 -- FROM [6.0, 7.0, 8.0]\n    * kernel: rbf -- FROM ['linear', 'poly', 'rbf', 'sigmoid']\n    * gamma: auto -- FROM ['auto', 'scale']\n    * probability: True -- FROM [True]\n\n---\n* Logistic Regression\n    * C: 1.0 -- FROM [0.1, 1.0, 10.0]\n    * solver: lbfgs -- FROM ['lbfgs']\n\n---\n* Ada Boost\n    * n_estimators: 54 -- FROM [54, 55, 56]\n    * learning_rate: 0.35 -- FROM [0.35, 0.4, 0.45]\n\n---\n* XGBTree\n    * eta: 0 -- [0, 1e-05]\n    * gamma: 0.05 -- [0.001, 0.05, 0.1]\n    * max_depth: 3 -- [3, 4, 5]\n    * probability: True -- FROM [True]\n\n---","b8c60228":"# 4. Could We Perform Better? - Testing the Monte Carlo DropOut\nMaybe. When we talk about DropOut, we can have an immediate alternative to improve the model even more: the Monte Carlo Dropout is a technique that can help us to achieve better results in dropping out neural networks. After training the Neural Network using the Dropout, we also drop out some weights randomly during the test $N$ times, obtaining $N$ different models. Then, we blend them in a Soft Voting Model.\n\nIt can be proven that it's equivalent to realize a Bayesian Simulation on our Neural Networks, which is a formal justificative to get almost aways a better result :).","6f7870c7":"## 2.B.1. Transform Significant Data\nA transformation that represents the extraction of significant variables, defined during the feature exploration step.","3fc6879f":"## 2.B.3. Transformer Missing Data\nTransformer that inputs all the missing data. A special attention was given to the \"Age\" feature and the procedure of label propagation over DBSCAN clusters was used:","22cd0b36":"After tunning the parameters, we have just saved all variables from our session. Now, it's time to check out the optimal neural network we just got.","ecc3b86f":"# Titanic Corvus Guide - Part II - Neural Networks Solution\nThis notebook is continuation of the [Part 1 Notebook](https:\/\/www.kaggle.com\/guidant\/corvus-part-1-top-15-combining-5-models#5.-Final-Submission), where we used simple Scikit Learn models to write an ensemble solution and get to the top 15%. The question is: can we use the knowledge we already got about the Titanic Dataset and develop a new solution using neural networks? Let's see!\n\n# 1. Once Again - The Libraries\nLet-me say it again - It's a good practice to start by importing all used libraries at once.","c6f3526a":"(To check the TensorBoard, it's necessary to edit this notebook and run the code!)","9f000469":"Seems to produce similar or even better results. Let's save this model then...of course, we will use all data this time.","d12436e6":"First, let's try to find the optimal activation function using common values to all the others hyperparameters.","c30e7e2f":"Functions to relate each family to a cabin. First we define the function to be applied to each row:","da0d5257":"# 2.B. Transformations Used in Pipelines","6b367364":"Let's compare the result along each fold for each one of the models in a BoxPlot:","1ce381e2":"And after all these transformations, we just normalize the data:","ced15a70":"Notice that:\n* We are defining the number of hidden layers, not the number of total layers ($N_{Total Layers} = N_{Hidden Layers} + 2$)\n* Since the output can get values equal to zero or one, we are defining the output function as a simple sigmoid (generally, the output function is defined by the type of problem - regression or classification - and by the number of outputs - for problems with multiple classes, we use the softmax function and for binary classification, it's common to employ the logistic function as we have done here).\nLet's try to tune our hyperparameters, testing different numbers of $N_{Hidden Layers}$ and creating a pipeline to realize our GridSearch. But, before, let's separe out training and test sets:"}}