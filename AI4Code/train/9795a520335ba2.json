{"cell_type":{"db937ef2":"code","e6024aca":"code","ad04e359":"code","64c56f87":"code","7df5ebcf":"code","3fbed1cc":"code","74c0bc9e":"code","51e38a0e":"code","00a04385":"code","296385b5":"code","6fb468ea":"code","949f6309":"code","31eeefa8":"code","7d27ebdf":"code","aa855188":"code","29af7dfc":"code","bbf0a075":"code","7a3539e0":"code","334e488d":"code","77cbbc6f":"code","727a3082":"code","c4f30739":"code","787167de":"markdown","15a88434":"markdown","d0ae01c4":"markdown","3e2ace0e":"markdown","7c765be8":"markdown","8e0fc5f8":"markdown","b3e4156c":"markdown","7f027667":"markdown","1ee5eaf4":"markdown","dc9873bb":"markdown","936ac5c4":"markdown","be62f152":"markdown","04fb1b14":"markdown","a6a0dbb9":"markdown","bdf006eb":"markdown","0e116242":"markdown","1262b54c":"markdown","b502a95e":"markdown","fc3a1072":"markdown","0c7a48aa":"markdown","07f1c97f":"markdown","d65db523":"markdown","0369868d":"markdown","79a6333c":"markdown","25fe7fa4":"markdown","f5fdc6c6":"markdown","a50be1d1":"markdown","3bc30a5d":"markdown","b2fd51e3":"markdown","1c72498d":"markdown","44872847":"markdown","dad7000d":"markdown"},"source":{"db937ef2":"###### bibliotecas padrao ######\nimport pandas as pd #csv, dataframes\nimport numpy as np\n\n###### bibliotecas para preparacao dos dados ######\n#expressoes regulares\nimport re \n#vetorizacao via BoW\nfrom sklearn.feature_extraction.text import CountVectorizer \n\n\n###### bibliotecas para o modelo de classificacao ######\n#divis\u00e3o de dados para treino\/teste\nfrom sklearn.model_selection import train_test_split\n#aprendizagem supervisionada com multiplas classes\/funciona bem com BoW\nfrom sklearn.naive_bayes import MultinomialNB ","e6024aca":"#importando os dados de interesse\n#df = pd.read_csv('\/kaggle\/input\/brazilian-ecommerce\/olist_order_reviews_dataset.csv', \n#                 usecols = ['review_score','review_comment_message', 'order_id'])\n\ndf_reviews = pd.read_csv('\/kaggle\/input\/brazilian-ecommerce\/olist_order_reviews_dataset.csv', \n                 usecols = ['review_score','review_comment_message'])\n\ndf_prod = pd.read_csv('\/kaggle\/input\/brazilian-ecommerce\/olist_products_dataset.csv', usecols = ['product_id', 'product_category_name'])\n\ndf_order = pd.read_csv('\/kaggle\/input\/brazilian-ecommerce\/olist_order_payments_dataset.csv', \n                 usecols = ['order_id'])\n\n#renomeando colunas\ndf_reviews.columns = ['score', 'comentarios']\ndf_prod.columns = ['prod_id', 'prod_categoria']\n#df_order.columns = ['order_id']\n\n#concatenando os dados de interesse\ndf = pd.concat([df_reviews, df_prod, df_order], axis = 1, join = 'inner' )\n\ndf","ad04e359":"#removendo dados ausentes do dataframe\ndf.dropna(inplace = True)\n\n#resetando \u00edndice\ndf.reset_index(inplace = True, drop = True)\n\n#imprime tuple(#linhas,#colunas)\ndf.shape","64c56f87":"def reQuebraDeLinha(texto_lista):\n    #quebras de linha - \\n\\r\n    #re.sub -> express\u00e3o regular que substitui um padr\u00e3o por outro\n    \n    texto_lista = [re.sub('\\r\\n',' ', i) for i in texto_lista]   # list comprehension para substituir\n                                                                 # as quebras de linha por espa\u00e7os em branco\n    return texto_lista","7df5ebcf":"def numTransform(texto_lista):\n    #[] indica quais caracteres estou considerando\n    #0-9 -> numeros no intervalo (0,9)\n    #'+' -> na documenta\u00e7\u00e3o diz que significa que o que estiver imediatamente antes dele precisa aparecer 1 ou mais vezes,\n    #       por\u00e9m n\u00e3o entendi muito bem. Mas sem o + na RegEx a palavra 'numero' se repete\n    \n    texto_lista = [re.sub('[0-9]+', 'numero ', i) for i in texto_lista]\n    return texto_lista","3fbed1cc":"def charEsp(texto_lista):\n    #\\W caracter nao alfa-numerico (^[a-zA-Z0-9])\n    texto_lista = [re.sub('\\W', ' ', i) for i in texto_lista]\n    return texto_lista","74c0bc9e":"def negacao(texto_lista):\n    #[] indica quais caracteres estou considerando\n    texto_lista = [re.sub('([nN][\u00e3\u00c3aA][oO]|[\u00f1\u00d1]| [nN] )', 'negacao', i) for i in texto_lista]\n    return texto_lista","51e38a0e":"def preparacao(texto_lista):\n    #aplicando a funcao requebraDeLinha()\n    texto_lista = reQuebraDeLinha(texto_lista)\n\n    #aplicando a fun\u00e7\u00e3o numTransform()\n    texto_lista = numTransform(texto_lista)\n\n    #aplicando a fun\u00e7\u00e3o negacao()\n    texto_lista = negacao(texto_lista)\n    \n    #aplicando a fun\u00e7\u00e3o charEsp()\n    texto_lista = charEsp(texto_lista)\n    \n    return texto_lista","00a04385":"#aplicando a preparacao dos dados\ndf.comentarios = preparacao(df['comentarios'])","296385b5":"# 0 = negativo e 1 = positivo\nmapa_score = {1: 0, 2: 0, 3: 0, 4: 1, 5: 1} #mapa 0\/1 -> neg\/pos\n\ndf['sentimento'] = df['score'].map(mapa_score) #mapeando a positividade\/negatividade baseado no score\ndf['sentimento_nome'] = pd.Categorical.from_codes(df['sentimento'],['negativo','positivo']) #categoriza\u00e7\u00e3o 0\/1 para neg\/pos\ndf.head()","6fb468ea":"#qtd de positivos e negativos no dataset\ndf.sentimento_nome.value_counts()","949f6309":"#instanciando o vetor\nvectorizer = CountVectorizer(max_features = 300, min_df = 7, max_df = 0.8)  #300 palavras mais usadas, termos com freq min 7 coments, termos com freq max 80% \n\n#vetorizando e transformando em array\ncomentarios_vectorized = vectorizer.fit_transform(df['comentarios']).toarray()\n\n#extraindo o nome das features\ncomentarios_features = vectorizer.get_feature_names()\n\n\nprint(comentarios_vectorized.shape)\n#print(comentarios_vectorized.A) #vizualiza\u00e7\u00e3o da matriz","31eeefa8":"#matriz BoW\ndf_comentarios_vectorized = pd.DataFrame(data = comentarios_vectorized, columns = comentarios_features)\ndf_comentarios_vectorized","7d27ebdf":"'''\ny = f(X)\nX -> comentarios, y -> labels\/targets\/classes (neg,pos)(0,1)\n'''\n\n#train_test_split params(X vetorizados, y array(labels), tamanho do conj teste, random_state ???)\nX_treino, X_teste, y_treino, y_teste = train_test_split(comentarios_vectorized, df['sentimento'].values, test_size = 0.20, random_state = 42)\n\nprint('X_treino shape ',X_treino.shape)\nprint('X_teste shape ',X_teste.shape)\nprint('y_treino shape ',y_treino.shape)\nprint('y_teste shape',y_teste.shape)","aa855188":"#treinando o modelo \nclassificador = MultinomialNB() #instanciando o modelo de ML navie bayes-multinomial\nclassificador.fit(X_treino, y_treino)","29af7dfc":"predicao = classificador.predict(X_teste) #predicoes feitas pelo modelo\nproba = classificador.predict_proba(X_teste) #probabilidade de cada classe proba[0] = negatividade, proba[1] = positividade\n\nproba","bbf0a075":"predicao = classificador.predict(comentarios_vectorized) #predicoes feitas pelo modelo\nproba = classificador.predict_proba(comentarios_vectorized) #probabilidade de cada classe","7a3539e0":"#criando dataFrame para expressar os resultados\nresult = pd.DataFrame(columns = ['comentarios', 'an\u00e1lise sentimental', r'negatividade $\\%$', r'positividade $\\%$', 'prod_id', 'prod_categoria', 'order_id'])\n\n#alimentando as colunas\nresult['order_id'] = df['order_id']\nresult['prod_id'] = df['prod_id']\nresult['prod_categoria'] = df['prod_categoria']\nresult['an\u00e1lise sentimental'] = predicao\nresult.comentarios = df['comentarios']\nresult['an\u00e1lise sentimental'] = ['negative' if i == 0 else 'positive' for i in result['an\u00e1lise sentimental']] #list comprehension para trocar 0\/1 por neg\/pos\n\n#list comprehension para pegar a positividade\/negatividade de cada predicao\nresult[r'negatividade $\\%$'] = [100 * round(proba[i][0],3) for i in range(len(proba))] #round(valor, casas decimais) -> arredonda valores\nresult[r'positividade $\\%$']= [100 * round(proba[i][1],3) for i in range(len(proba))]\nprint(result['an\u00e1lise sentimental'].value_counts())\n\n#proba = np.array([round(i,2) * 100 for i in proba])\n#proba\nresult.head()","334e488d":"print('M\u00e9dia de positividade {}%'.format(round(result[r'positividade $\\%$'].values.mean(), 2)))\nprint('M\u00e9dia de negatividade {}%'.format(round(result[r'negatividade $\\%$'].values.mean(), 2)))\nresult[['comentarios','an\u00e1lise sentimental', r'positividade $\\%$', r'negatividade $\\%$']].head()","77cbbc6f":"df_8 = result.groupby('prod_categoria').mean()\ndf_8.columns = [r'm\u00e9dia de negatividade $\\%$',r'm\u00e9dia de positividade $\\%$']\ndf_8[r'm\u00e9dia de negatividade $\\%$'] = [round(i,2) for i in df_8[r'm\u00e9dia de negatividade $\\%$'].values]\ndf_8[r'm\u00e9dia de positividade $\\%$'] = [round(i,2) for i in df_8[r'm\u00e9dia de positividade $\\%$'].values]\ndf_8.head()","727a3082":"df_9 = result.groupby('prod_id').mean()\ndf_9.columns = [r'm\u00e9dia de negatividade $\\%$',r'm\u00e9dia de positividade $\\%$']\ndf_9[r'm\u00e9dia de negatividade $\\%$'] = [round(i,2) for i in df_9[r'm\u00e9dia de negatividade $\\%$'].values]\ndf_9[r'm\u00e9dia de positividade $\\%$'] = [round(i,2) for i in df_9[r'm\u00e9dia de positividade $\\%$'].values]\ndf_9.head()","c4f30739":"df_10 = result.groupby('order_id').mean()\ndf_10.columns = [r'm\u00e9dia de negatividade $\\%$',r'm\u00e9dia de positividade $\\%$']\ndf_10[r'm\u00e9dia de negatividade $\\%$'] = [round(i,2) for i in df_10[r'm\u00e9dia de negatividade $\\%$'].values]\ndf_10[r'm\u00e9dia de positividade $\\%$'] = [round(i,2) for i in df_10[r'm\u00e9dia de positividade $\\%$'].values]\ndf_10.head()","787167de":"V\u00e1rios processos de RegEx s\u00e3o necess\u00e1rios para preparar os dados para an\u00e1lise, bem como processos como stemming (redu\u00e7\u00e3o da palavra ao seu tronco) e remo\u00e7\u00e3o de stopwords (palavras irrelevantes). Aqui estou reproduzindo apenas alguns\n\n* Criar uma fun\u00e7\u00e3o que substitua as quebras de linhas encontradas nos textos por espa\u00e7os em branco\n* Criar uma fun\u00e7\u00e3o que substitua os valores num\u00e9ricos pela palavra 'numero' (Mas, por qual motivo exatamente ?)\n* Criar uma fun\u00e7\u00e3o que substitua caracteres especiais por espa\u00e7os em branco","15a88434":"<h4>Term Frequency Inverse Document Frequency (TF - IDF)<\/h4>\n\nO modelo TF-IDF consiste em quantificar a import\u00e2ncia de cada palavra no conjunto de dados e represent\u00e1-las como um vetor, onde cada elemento destes \u00e9 um n\u00famero do tipo float que est\u00e1 associado a frequ\u00eancia e ao peso de cada termo no documento, e quanto maior este n\u00famero, mais relevante \u00e9 a palavra.<br>\n\nSegundo <a href = \"https:\/\/www.analyticsvidhya.com\/blog\/2020\/02\/quick-introduction-bag-of-words-bow-tf-idf\/\">Analytics Vidhya<\/a>, o TF-IDF score \u00e9 dado por: <br><br>\n\n$(TF - IDF)_{t,d} = TF_{t,d} * IDF_{t,d}$<br><br>\nonde \"t\" e \"d\" representam o termo analisado em um dado documento, respectivamente. Por defini\u00e7\u00e3o, tamb\u00e9m temos a frequ\u00eancia do termo \"t\" no documento \"d\", dada por:<br>\n* $TF_{t,d} = \\frac{\\textrm{n\u00ba de vezes que o termo \"t\" aparece no documento \"d\"}}{\\textrm{n\u00famero total de termos no documento \"d\"}}$<br><br>\n\nE a import\u00e2ncia do termo \"t\", definida como:<br>\n\n* $IDF_{t,d} = \\Big(\\frac{\\textrm{n\u00famero de documentos}}{\\textrm{n\u00famero de documentos que cont\u00e9m o termo \"t\"}}\\Big)$<br><br>","d0ae01c4":"Os <a href = \"https:\/\/www.kaggle.com\/thiagopanini\/e-commerce-sentiment-analysis-eda-viz-nlp\">dados<\/a> referem-se ao e-commerce no territ\u00f3rio brasileiro entre 09\/2016 at\u00e9 10\/2018. Os arquivos de interesse chamam-se (1) 'olist_order_reviews_dataset.csv', (2) 'olist_products_dataset.csv' e (3) 'olist_order_payments_dataset.csv' e cont\u00e9m informa\u00e7\u00f5es sobre os reviews, sobre as ordens de compra e sobre os produtos vendidos no e-commerce no Brasil no mesmo intervalo temporal.\n* As colunas de interesse (review_score, reviews_message, product_id, product_category_name, order_id), e seus respectivos datasets podem ser encontrados do Data Schemma","3e2ace0e":"<h1>Modelo<\/h1>","7c765be8":"Segundo <a href = \"https:\/\/www.kaggle.com\/thiagopanini\/e-commerce-sentiment-analysis-eda-viz-nlp\"> Panini<\/a>, para fazer a an\u00e1lise sentimental \u00e9 necess\u00e1rio preparar os dados e transform\u00e1-los em vetores, para que por fim sejam interpretados por um modelo de Machine Learning. A prepara\u00e7\u00e3o consiste, dentre outros passos, em aplicar Express\u00f5es Regulares (RegEx) no conjunto de dados, afim de identificar padr\u00f5es e extra\u00ed-los. A vetoriza\u00e7\u00e3o depende do modelo utilizado, e consiste em representar palavras vetorialmente em fun\u00e7\u00e3o da sua frequ\u00eancia em um documento e\/ou de sua import\u00e2ncia.","8e0fc5f8":"Database Schema de um banco de dados corresponde a estrutura formal do mesmo e refere-se \u00e0 a organiza\u00e7\u00e3o do banco de dados em forma de diagrama, e determina como o banco de dados \u00e9 constru\u00eddo (<a href = 'https:\/\/pt.wikipedia.org\/wiki\/Esquema_de_banco_de_dados'>Wikipedia<\/a>).","b3e4156c":"2 - Como o autor chegar\u00e1 a este objetivo?\n\nR: Primeiro ele far\u00e1 uma an\u00e1lise explorat\u00f3ria desses dados, utilizando ferramentas gr\u00e1ficas para melhor compreens\u00e3o sobre o cen\u00e1rio de vendas online no Brasil. Em seguida, implementar\u00e1 uma An\u00e1lise Sentimental dos coment\u00e1rios dos clientes usando ferramentas do Processamento de Linguagem Natural.","7f027667":"<h1>Importando e limpando os dados<\/h1>","1ee5eaf4":"7 - Realize AS em todas as reviews contidas no dataset, apontando se s\u00e3o Positivas ou Negativas, e qual percentual de positividade ou negatividade de cada uma. Calcule a m\u00e9dia para as ASs Negativas e a m\u00e9dia para as ASs Positivas.","dc9873bb":"<h1>Sum\u00e1rio<\/h1>","936ac5c4":"<h3>Modelos de vetoriza\u00e7\u00e3o<\/h3>","be62f152":"<h1>Database Schema Design<\/h1>","04fb1b14":"<h1>Teste<\/h1>","a6a0dbb9":"* Observa\u00e7\u00e3o 1: Notebook de refer\u00eancia: https:\/\/www.kaggle.com\/thiagopanini\/e-commerce-sentiment-analysis-eda-viz-nlp\n* Observa\u00e7\u00e3o 2: Fui escrevendo este notebook de forma simult\u00e2nea aos estudos. Achei essa metodologia necess\u00e1ria uma vez que este \u00e9 meu primeiro contato com RegEx, NLP e ML. O que n\u00e3o foi totalmente entendido possui um \"?\"","bdf006eb":"<h1>Respostas<\/h1>","0e116242":"10 - Calcule a m\u00e9dia para as ASs Negativas e a m\u00e9dia para as ASs Positivas por ordem efetuada (campo order_id). Uma ordem de compra pode ser composta por distintos produtos","1262b54c":"<h2>RegEx<\/h2>","b502a95e":"5 - Defina a biblioteca pandas em 1 (uma) linha, em uma \u00fanica frase.\n\nR: Uma poderosa ferramenta de manipula\u00e7\u00e3o e an\u00e1lise de dados e de arquivos tipo .csv, .excel, .txt etc.","fc3a1072":"Ainda segundo <a href = \"https:\/\/www.analyticsvidhya.com\/blog\/2020\/02\/quick-introduction-bag-of-words-bow-tf-idf\/\">Analytics Vidhya<\/a>, apesar do modelo BoW ser mais simples no que se refere a interpreta\u00e7\u00e3o, o modelo TD-IDF possui um desempenho maior quando comparado com o modelo Bag of Words em modelos de Machine Learning, uma vez que o TD-IDF traz informa\u00e7\u00f5es sobre a import\u00e2ncia das palavras e o modelo Bag of Words apenas traz informa\u00e7\u00f5es sobre a ocorr\u00eancia destas no documento. Neste notebook, por quest\u00f5es de praticidade e de melhor desempenho com o modelo de classifica\u00e7\u00e3o, usarei o modelo BoW","0c7a48aa":"<h1>An\u00e1lise de sentimento com todas as reviews<\/h1>","07f1c97f":"* Importando e limpando os dados<br> \n* Prepara\u00e7\u00e3o dos dados<br>\n * RegEx<br>\n* Modelo<br>\n * Vetoriza\u00e7\u00e3o<br> \n   * Modelos de vetoriza\u00e7\u00e3o\n     * BoW<br>\n     * TD - IDF<br>\n* Treinamento<br>\n* Teste<br>\n* An\u00e1lise de Sentimento com todas as reviews\n* Respostas das Tarefas","d65db523":"1 - Qual \u00e9 o objetivo deste notebook?\n\nR: O objetivo deste notebook \u00e9 olhar para o e-commerce brasileiro a partir de uma vis\u00e3o anal\u00edtica","0369868d":"8 - Calcule a m\u00e9dia para as ASs Negativas e a m\u00e9dia para as ASs Positivas por g\u00eanero do produto comprado (campo product_category_name)","79a6333c":"<h2>Vetoriza\u00e7\u00e3o<\/h2>","25fe7fa4":"6 - Qual o shape do dataset utilizado para a parte \"4. Natural Language Processing\" do notebook? Quantas linhas? Quantas colunas?\n\nR: 41753 linhas e 2 colunas","f5fdc6c6":"![image.png](attachment:image.png)[](http:\/\/)","a50be1d1":"<h4>Bag of Words (BoW)<\/h4>\n\nSegundo <a href = \"https:\/\/www.analyticsvidhya.com\/blog\/2020\/02\/quick-introduction-bag-of-words-bow-tf-idf\/\">Analytics Vidhya<\/a>, o modelo BoW \u00e9 definido como uma forma simples de fazer uma representa\u00e7\u00e3o vetorial de um dado termo \"t\" presente em um dado documento \"d\", a partir de sua frequ\u00eancia de ocorr\u00eancia no mesmo. O vetor \u00e9 gerado de forma tal, que para cada termo presente no texto est\u00e1 associado um valor bin\u00e1rio (0 ou 1), onde 0 significa n\u00e3o ocorr\u00eancia e 1 indica ocorr\u00eancia do termo em um dado documento.","3bc30a5d":"<h1>Treinamento<\/h1>","b2fd51e3":"4 - Defina a biblioteca numpy em 1 (uma) linha, em uma \u00fanica frase.\n\nR: Uma poderosa ferramenta de manipula\u00e7\u00e3o de entidades matem\u00e1ticas como matrizes,vetores e tensores, e que possui alta efici\u00eancia no que diz respeto a tempo de execu\u00e7\u00e3o.","1c72498d":"9 - Calcule a m\u00e9dia para as ASs Negativas e a m\u00e9dia para as ASs Positivas por produto comprado (campo product_id)","44872847":"3 - O autor agradece a um colaborador antes de iniciar a explora\u00e7\u00e3o de dados. Qual o nome desse colaborador?\n\nR: O colaborador chama-se Raeshid David, e \u00e9 diretor fundador da Pathways in Technology Early College (via LinkedIn)","dad7000d":"<h1>Prepara\u00e7\u00e3o dos dados<\/h1>"}}