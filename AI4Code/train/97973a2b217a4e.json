{"cell_type":{"774cee1c":"code","d29a9200":"code","4e71e637":"code","ee3ae10f":"code","363387c7":"code","2e79e8f3":"code","8a16419d":"code","ad190d63":"code","b0a9dfd3":"code","96e41a9b":"code","773b121e":"code","4d74bde3":"code","f2436a20":"code","7f3c277b":"code","fea8ad97":"code","b5e379cf":"code","0b8b56e3":"code","5ebf81fa":"code","0b4927d3":"markdown","d38f7cfd":"markdown","47c88dd5":"markdown","550a52d0":"markdown","557e8f0a":"markdown","338cd3dc":"markdown","8748a93d":"markdown","34e60c30":"markdown","7fe14870":"markdown","0692dfce":"markdown","dca88d5b":"markdown","c6cd58b3":"markdown","07cd76ee":"markdown"},"source":{"774cee1c":"# Importing Libraries\nimport pandas as pd # Intermediate DS\nimport numpy as np # Scientific Operations\nimport copy\n\nfrom sklearn import preprocessing # Preprocesing library for Encoding, etc.\nfrom sklearn.model_selection import KFold # Segregating the train data to fragments for predorming Cross Validation\nfrom sklearn.metrics import mean_squared_error # For Calculating Mean Squared Error\nfrom sklearn.model_selection import RandomizedSearchCV\nimport xgboost as xgb # Light GBM Regressor\n\nfrom datetime import datetime # For storing date and timers for operations\n\nimport warnings\nwarnings.filterwarnings('ignore')","d29a9200":"# Load the training data\ntrain = pd.read_csv(\"..\/input\/30-days-of-ml\/train.csv\", index_col=0)\ntest = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\", index_col=0)\n\n# Preview the data\ntrain.head()\ntest.head()","4e71e637":"y_train = train.target\nX_train = train.drop(['target'], axis=1)\nX_test = test.copy()\n\n# Preview features\nX_train.head()","ee3ae10f":"# # Extract the Categorical Columns\n# cat_cols = [feature for feature in train.columns if 'cat' in feature]\n# print(cat_cols)\n\n# # Copy of original data to prevent overwwritting them\n# label_X_train = X_train.copy()\n# label_X_test = X_test.copy()\n\n# # Apply ordinal encoder to each column with categorical data\n# ord_encoder = preprocessing.OrdinalEncoder()\n# label_X_train[cat_cols] = ord_encoder.fit_transform(label_X_train[cat_cols])\n# label_X_test[cat_cols] = ord_encoder.transform(label_X_test[cat_cols])\n\n# del ord_encoder","363387c7":"# Extract the Categorical Columns\ncat_cols = [feature for feature in X_train.columns if 'cat' in feature]\nprint(cat_cols)\nlow_cardinality_cols = [col for col in cat_cols if X_train[col].nunique() < 8]\nprint(low_cardinality_cols)\nhigh_cardinality_cols = list(set(cat_cols) - set(low_cardinality_cols))\nprint(high_cardinality_cols)\n\n# Copy of original data to prevent overwwritting them\nlabel_X_train = X_train.copy()\nlabel_X_test = X_test.copy()\n\nprint('Original Train Shape: %s' %str(label_X_train.shape))\nprint('Original Test Shape: %s' %str(label_X_test.shape))\n\n'''\nOne Hot Encoding for low cardinality cols\n'''\nOH_encoder = preprocessing.OneHotEncoder(handle_unknown='ignore', sparse=False)\n\nOH_cols_X = pd.DataFrame(OH_encoder.fit_transform(label_X_train[low_cardinality_cols]))\nOH_cols_X_test = pd.DataFrame(OH_encoder.fit_transform(label_X_test[low_cardinality_cols]))\n\n# One-hot encoding removed index; put it back\nOH_cols_X.index = label_X_train.index\nOH_cols_X_test.index = label_X_test.index\n\n# Remove categorical columns (will replace with one-hot encoding)\nnum_X = label_X_train.drop(low_cardinality_cols, axis=1)\nnum_X_test = label_X_test.drop(low_cardinality_cols, axis=1)\n\n# Add one-hot encoded columns to numerical features\nlabel_X_train = pd.concat([num_X, OH_cols_X], axis=1)\nlabel_X_test = pd.concat([num_X_test, OH_cols_X_test], axis=1)\n\nprint('Train Shape After OH Encoding: %s' %str(label_X_train.shape))\nprint('Test Shape After OH Encoding: %s' %str(label_X_test.shape))\n\n'''\nOrdinal Encoding for high cardinality cols\n'''\nord_encoder = preprocessing.OrdinalEncoder()\n\nlabel_X_train[high_cardinality_cols] = ord_encoder.fit_transform(label_X_train[high_cardinality_cols])\nlabel_X_test[high_cardinality_cols] = ord_encoder.fit_transform(label_X_test[high_cardinality_cols])\n\nprint('Train Shape After Ordinal Encoding: %s' %str(label_X_train.shape))\nprint('Test Shape After Ordinal Encoding: %s' %str(label_X_test.shape))\n\ndel OH_encoder, OH_cols_X, OH_cols_X_test, num_X, num_X_test\ndel ord_encoder","2e79e8f3":"# # Extract the Categorical Columns\n# cat_cols = [feature for feature in X_train.columns if 'cat' in feature]\n# print(cat_cols)\n# low_cardinality_cols = [col for col in cat_cols if X_train[col].nunique() < 8]\n# print(low_cardinality_cols)\n# high_cardinality_cols = list(set(cat_cols) - set(low_cardinality_cols))\n# print(high_cardinality_cols)\n# numerical_cols = [feature for feature in X_train.columns if feature.startswith(\"cont\")]\n# print(numerical_cols)\n\n# # Copy of original data to prevent overwwritting them\n# label_X_train = X_train.copy()\n# label_X_test = X_test.copy()\n\n# print('Original Train Shape: %s' %str(label_X_train.shape))\n# print('Original Test Shape: %s' %str(label_X_test.shape))\n\n# '''\n# Polynomial Encoding for numerical cols\n# '''\n# poly = preprocessing.PolynomialFeatures(degree=3, interaction_only=True, include_bias=False)\n\n# train_poly = poly.fit_transform(label_X_train[numerical_cols])\n# test_poly = poly.fit_transform(label_X_test[numerical_cols])\n\n# df_poly = pd.DataFrame(train_poly, columns=[f\"poly_{i}\" for i in range(train_poly.shape[1])])\n# df_test_poly = pd.DataFrame(test_poly, columns=[f\"poly_{i}\" for i in range(test_poly.shape[1])])\n\n# df_poly['id'] = list(label_X_train.index)\n# df_test_poly['id'] = list(label_X_test.index)\n# df_poly = df_poly.set_index('id')\n# df_test_poly = df_test_poly.set_index('id')\n\n# label_X_train = pd.concat([label_X_train, df_poly], axis=1)\n# label_X_test = pd.concat([label_X_test, df_test_poly], axis=1)\n\n# print('Train Shape After Polynomial Encoding: %s' %str(label_X_train.shape))\n# print('Test Shape After Polynomial Encoding: %s' %str(label_X_test.shape))\n\n# '''\n# One Hot Encoding for low cardinality cols\n# '''\n# OH_encoder = preprocessing.OneHotEncoder(handle_unknown='ignore', sparse=False)\n\n# OH_cols_X = pd.DataFrame(OH_encoder.fit_transform(label_X_train[low_cardinality_cols]))\n# OH_cols_X_test = pd.DataFrame(OH_encoder.fit_transform(label_X_test[low_cardinality_cols]))\n\n# # One-hot encoding removed index; put it back\n# OH_cols_X.index = label_X_train.index\n# OH_cols_X_test.index = label_X_test.index\n\n# # Remove categorical columns (will replace with one-hot encoding)\n# num_X = label_X_train.drop(low_cardinality_cols, axis=1)\n# num_X_test = label_X_test.drop(low_cardinality_cols, axis=1)\n\n# # Add one-hot encoded columns to numerical features\n# label_X_train = pd.concat([num_X, OH_cols_X], axis=1)\n# label_X_test = pd.concat([num_X_test, OH_cols_X_test], axis=1)\n\n# print('Train Shape After OH Encoding: %s' %str(label_X_train.shape))\n# print('Test Shape After OH Encoding: %s' %str(label_X_test.shape))\n\n# '''\n# Ordinal Encoding for high cardinality cols\n# '''\n# ord_encoder = preprocessing.OrdinalEncoder()\n\n# label_X_train[high_cardinality_cols] = ord_encoder.fit_transform(label_X_train[high_cardinality_cols])\n# label_X_test[high_cardinality_cols] = ord_encoder.fit_transform(label_X_test[high_cardinality_cols])\n\n# print('Train Shape After Ordinal Encoding: %s' %str(label_X_train.shape))\n# print('Test Shape After Ordinal Encoding: %s' %str(label_X_test.shape))\n\n# del poly, train_poly, test_poly, df_poly, df_test_poly\n# del OH_encoder, OH_cols_X, OH_cols_X_test, num_X, num_X_test\n# del ord_encoder","8a16419d":"label_X_train.head()","ad190d63":"y_train.head()","b0a9dfd3":"label_X_test.head()","96e41a9b":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df","773b121e":"# label_X_train = reduce_mem_usage(label_X_train)\n# label_X_test = reduce_mem_usage(label_X_test)","4d74bde3":"!nvidia-smi","f2436a20":"def timer(start_time=None):\n    if not start_time:\n        start_time = datetime.now()\n        return start_time\n    elif start_time:\n        thour, temp_sec = divmod((datetime.now() - start_time).total_seconds(), 3600)\n        tmin, tsec = divmod(temp_sec, 60)\n        print('\\n Time taken: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))","7f3c277b":"# Hyper Parameter Optimization\nxgb_params={\n    'learning_rate'    : [0.003, 0.05, 0.10, 0.15],\n    'max_depth'        : [3, 4, 5],\n    'min_child_weight' : [1, 1.5, 3, 5, 7 ],\n    'gamma'            : [0.0, 0.1, 0.2 , 0.3, 0.4 ],\n    'colsample_bytree' : [0.1, 0.3, 0.4, 0.5 , 0.7 ],\n    'n_estimators'     : [1000, 1500, 2000, 2500, 3500, 4000, 5000, 10000],\n    'reg_lambda'       : [5, 10, 15, 20, 25, 30, 40, 50],\n    'reg_alpha'        : [5, 10, 15, 20, 25, 30, 40, 50],\n}","fea8ad97":"split = KFold(n_splits=10, random_state=91, shuffle=True)\n\nxgb_regressor=xgb.XGBRegressor(random_state=91, \n                               tree_method='gpu_hist', \n                               nthread=1)\nrandom_search=RandomizedSearchCV(xgb_regressor,\n                                 param_distributions=xgb_params,\n                                 n_iter=50,\n                                 scoring='neg_root_mean_squared_error',\n                                 n_jobs=-1,\n                                 cv=split.split(label_X_train, y_train),\n                                 verbose=3)\nstart_time = timer(None) # timing starts from this point for \"start_time\" variable\nrandom_search.fit(label_X_train, y_train)\ntimer(start_time) # timing ends here for \"start_time\" variable","b5e379cf":"random_search.best_estimator_","0b8b56e3":"random_search.best_params_","5ebf81fa":"'''\nCONDITION 1\nOptimized Hyper Parameters for RandomizedSearchCV on Training Dataset where Categorical columns \nare converted to Ordinal Columns\n'''\noptimized_xgb_params={\n    'colsample_bytree': 0.1, \n    'gamma': 0.0, \n    'learning_rate': 0.15, \n    'max_depth': 5,\n    'min_child_weight': 3, \n    'n_estimators': 2500,\n    'n_jobs': -1, \n    'random_state': 91, \n    'reg_alpha': 10, \n    'reg_lambda': 20, \n    #'nthread': 1, \n    #'tree_method': 'gpu_hist', \n}\n\n'''\nCONDITION 2\nptimized Hyper Parameters for RandomizedSearchCV on Training Dataset where Categorical columns \nare converted to Ordinal Columns for high cardinality columns, Categorical columns are converted \nto One Hot Columns for low cardinality columns\n'''\noptimized_xgb_params={\n    'colsample_bytree': 0.1, \n    'gamma': 0.0, \n    'learning_rate': 0.05, \n    'max_depth': 3,\n    'min_child_weight': 1, \n    'n_estimators': 10000,\n    'n_jobs': -1, \n    'random_state': 91, \n    'reg_alpha': 15, \n    'reg_lambda': 30, \n    #'nthread': 1, \n    #'tree_method': 'gpu_hist', \n}\n\n'''\nCONDITION 3\nOptimized Hyper Parameters for RandomizedSearchCV on Training Dataset where Categorical columns \nare converted to Ordinal Columns for high cardinality columns, Categorical columns are converted \nto One Hot Columns for low cardinality columns and Numerical columns are converted to Polynomial \ncolumns\n'''\noptimized_xgb_params={\n    'colsample_bytree': 0.7, \n    'gamma': 0.2, \n    'learning_rate': 0.05, \n    'max_depth': 3,\n    'min_child_weight': 7, \n    'n_estimators': 4000,\n    'n_jobs': -1, \n    'random_state': 91, \n    'reg_alpha': 50, \n    'reg_lambda': 25, \n    'nthread': 1, \n    'tree_method': 'gpu_hist', \n}","0b4927d3":"# Blending Optimization Condition 2\nOH Encoder - Low Cardinality Categorical Columns\n\nOrdinal Encoder - High Cardinality Categorical Columns","d38f7cfd":"## Condition 1: Run with Ordinal Encoder only for Categorical Columns\n10k fold split with random state as 91 and n_iter for RandomizeedSearchCV being 50\n\n![image.png](attachment:725633d2-9d2d-499d-bc26-c1a403c0d7fa.png)","47c88dd5":"## Condition 2: Run with Ordinal Encoder for high cardinality columns and OH Encoder for low cardinality columns\n\n10k fold split with random state as 91 and n_iter for RandomozedSearchCV being 50\n\n![image.png](attachment:ba6a2014-021c-422b-b33d-bcb689ce4681.png)","550a52d0":"# Step 1: Import helpful libraries","557e8f0a":"# Blending Optimization Condition 1\n\nOrdinal Encoder - Categorical Columns","338cd3dc":"## Condition 1: Best Regressor with Ordinal Encoder only for Categorical Columns\n![image.png](attachment:70400766-e718-4c00-bf43-394c43e0f0be.png)","8748a93d":"## Condition 3: Run with Ordinal Encoder for high cardinality columns, OH Encoder for low cardinality columns and polynomial Encoder for numerical colums\n10k fold split with random state as 91 and n_iter for RandomizeedSearchCV being 10\n\n![image.png](attachment:4898065c-881e-4c8c-9488-3fda357787a2.png)","34e60c30":"## Condition 3: Best Regressor with Ordinal Encoder for high cardinality columns, OH Encoder for low cardinality columns and polynomial Encoder for numerical colums\n\n![image.png](attachment:8761acdd-9047-4dbf-99b1-668c9da54b71.png)","7fe14870":"The next code cell separates the target (which we assign to y) from the training features.","0692dfce":"# Step 3: Optimize the XG Boost Classifier","dca88d5b":"## Condition 2: Best Regressor with Ordinal Encoder for high cardinality columns and OH Encoder for low cardinality columns \n\n![image.png](attachment:d1d2932f-8895-4687-ac3d-bd26cbc5ae1a.png)","c6cd58b3":"# Blending Optimization Condition 3\nOH Encoder - Low Cardinality Categorical Columns\n\nOrdinal Encoder - High Cardinality Categorical Columns\n\nPolynomial Encoder - Numerical Columns","07cd76ee":"# Step 2: Load the data\n\nNext, we'll load the training and test data.  "}}