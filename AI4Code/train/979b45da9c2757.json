{"cell_type":{"fcc5ae53":"code","8dc1d553":"code","b0c78def":"code","ddd3e400":"code","5ad0000a":"code","3cc441f6":"code","084e1196":"code","2ea6314c":"code","d5d208c1":"code","5a8525c6":"code","6f3235ca":"code","b38739e2":"markdown","72ba8e2b":"markdown"},"source":{"fcc5ae53":"!pip install git+https:\/\/github.com\/huggingface\/transformers.git\n","8dc1d553":"import logging\nlogging.getLogger().setLevel(logging.CRITICAL)\n\nimport torch\nimport numpy as np\n\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel\n\ndevice = 'cpu'\nif torch.cuda.is_available():\n    device = 'cuda'","b0c78def":"tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')\nmodel = GPT2LMHeadModel.from_pretrained('gpt2-medium')\nmodel = model.to(device)","ddd3e400":"\n\n# Function to first select topN tokens from the probability list and then based on the selected N word distribution\n# get random token ID\ndef choose_from_top(probs, n=5):\n    ind = np.argpartition(probs, -n)[-n:]\n    top_prob = probs[ind]\n    top_prob = top_prob \/ np.sum(top_prob) # Normalize\n    choice = np.random.choice(n, 1, p = top_prob)\n    token_id = ind[choice][0]\n    return int(token_id)\n\n","5ad0000a":"def generate_some_text(input_str, text_len = 50):\n\n    cur_ids = torch.tensor(tokenizer.encode(input_str)).unsqueeze(0).long().to(device)\n\n    model.eval()\n    with torch.no_grad():\n\n        for i in range(text_len):\n            outputs = model(cur_ids, labels=cur_ids)\n            loss, logits = outputs[:2]\n            softmax_logits = torch.softmax(logits[0,-1], dim=0) #Take the first(only one) batch and the last predicted embedding\n            next_token_id = choose_from_top(softmax_logits.to('cpu').numpy(), n=10) #Randomly(from the given probability distribution) choose the next word from the top n words\n            cur_ids = torch.cat([cur_ids, torch.ones((1,1)).long().to(device) * next_token_id], dim = 1) # Add the last word\n\n        output_list = list(cur_ids.squeeze().to('cpu').numpy())\n        output_text = tokenizer.decode(output_list)\n        print(output_text)","3cc441f6":"generate_some_text('''i am just paranoid''')","084e1196":"generate_some_text(\" Artificial intelligence is \")","2ea6314c":"# from pytorch_transformers import WEIGHTS_NAME, CONFIG_NAME\n\n# output_dir = \".\/models\/\"\n\n# # Step 1: Save a model, configuration and vocabulary that you have fine-tuned\n\n# # If we have a distributed model, save only the encapsulated model\n# # (it was wrapped in PyTorch DistributedDataParallel or DataParallel)\n# model_to_save = model.module if hasattr(model, 'module') else model\n\n# # If we save using the predefined names, we can load using `from_pretrained`\n# output_model_file = os.path.join(output_dir, WEIGHTS_NAME)\n# output_config_file = os.path.join(output_dir, CONFIG_NAME)\n\n# torch.save(model_to_save.state_dict(), output_model_file)\n# model_to_save.config.to_json_file(output_config_file)\n# tokenizer.save_vocabulary(output_dir)\n\n# # Step 2: Re-load the saved model and vocabulary\n\n# # Example for a Bert model\n# model = BertForQuestionAnswering.from_pretrained(output_dir)\n# tokenizer = BertTokenizer.from_pretrained(output_dir, do_lower_case=args.do_lower_case)  # Add specific options if needed\n# # Example for a GPT model\n# model = OpenAIGPTDoubleHeadsModel.from_pretrained(output_dir)\n# tokenizer = OpenAIGPTTokenizer.from_pretrained(output_dir)","d5d208c1":"from torch.utils.data import Dataset\nfrom torch.utils.data import Dataset, DataLoader\nimport os\nimport json\nimport csv\n\nclass JokesDataset(Dataset):\n    def __init__(self, jokes_dataset_path = '..\/input\/short-jokes\/'):\n        super().__init__()\n\n        short_jokes_path = os.path.join(jokes_dataset_path, 'shortjokes.csv')\n\n        self.joke_list = []\n        self.end_of_text_token = \"<|endoftext|>\"\n        \n        with open(short_jokes_path) as csv_file:\n            csv_reader = csv.reader(csv_file, delimiter=',')\n            \n            x = 0\n            for row in csv_reader:\n                joke_str = f\"JOKE:{row[1]}{self.end_of_text_token}\"\n                self.joke_list.append(joke_str)\n        \n    def __len__(self):\n        return len(self.joke_list)\n\n    def __getitem__(self, item):\n        return self.joke_list[item]\n\n\ndataset = JokesDataset()\njoke_loader = DataLoader(dataset, batch_size=1, shuffle=True)\n\n\nBATCH_SIZE = 16\nEPOCHS = 2\nLEARNING_RATE = 3e-5\nWARMUP_STEPS = 5000\nMAX_SEQ_LEN = 400\nfrom transformers import AdamW, get_linear_schedule_with_warmup\n\ndevice = 'cpu'\nif torch.cuda.is_available():\n    device = 'cuda'\n\nmodel = model.to(device)\nmodel.train()\noptimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=WARMUP_STEPS, num_training_steps = -1)\nproc_seq_count = 0\nsum_loss = 0.0\nbatch_count = 0\n\ntmp_jokes_tens = None\nmodels_folder = \"trained_models\"\nif not os.path.exists(models_folder):\n    os.mkdir(models_folder)","5a8525c6":"\n\nfor epoch in range(EPOCHS):\n    \n    print(f\"EPOCH {epoch} started\" + '=' * 30)\n    \n    for idx,joke in enumerate(joke_loader):\n        \n        #################### \"Fit as many joke sequences into MAX_SEQ_LEN sequence as possible\" logic start ####\n        joke_tens = torch.tensor(tokenizer.encode(joke[0])).unsqueeze(0).to(device)\n        #Skip sample from dataset if it is longer than MAX_SEQ_LEN\n        if joke_tens.size()[1] > MAX_SEQ_LEN:\n            continue\n        \n        #The first joke sequence in the sequence\n        if not torch.is_tensor(tmp_jokes_tens):\n            tmp_jokes_tens = joke_tens\n            continue\n        else:\n            #The next joke does not fit in so we process the sequence and leave the last joke \n            #as the start for next sequence \n            if tmp_jokes_tens.size()[1] + joke_tens.size()[1] > MAX_SEQ_LEN:\n                work_jokes_tens = tmp_jokes_tens\n                tmp_jokes_tens = joke_tens\n            else:\n                #Add the joke to sequence, continue and try to add more\n                tmp_jokes_tens = torch.cat([tmp_jokes_tens, joke_tens[:,1:]], dim=1)\n                continue\n        ################## Sequence ready, process it trough the model ##################\n            \n        outputs = model(work_jokes_tens, labels=work_jokes_tens)\n        loss, logits = outputs[:2]                        \n        loss.backward()\n        sum_loss = sum_loss + loss.detach().data\n                       \n        proc_seq_count = proc_seq_count + 1\n        if proc_seq_count == BATCH_SIZE:\n            proc_seq_count = 0    \n            batch_count += 1\n            optimizer.step()\n            scheduler.step() \n            optimizer.zero_grad()\n            model.zero_grad()\n\n        if batch_count == 100:\n            print(f\"sum loss {sum_loss}\")\n            batch_count = 0\n            sum_loss = 0.0\n    \n    # Store the model after each epoch to compare the performance of them\n    torch.save(model.state_dict(), os.path.join(models_folder, f\"gpt2_medium_joker_{epoch}.pt\"))\n\n\nmodels_folder = \"trained_models\"\n\n","6f3235ca":"model_path = os.path.join(models_folder, f\"gpt2_medium_joker_{1}.pt\")\nmodel.load_state_dict(torch.load(model_path))\n\njokes_output_file_path = f'generated_{1}.jokes'\n\nmodel.eval()\nif os.path.exists(jokes_output_file_path):\n    os.remove(jokes_output_file_path)\n    \njoke_num = 0\nwith torch.no_grad():\n   \n        for joke_idx in range(1000):\n        \n            joke_finished = False\n\n            cur_ids = torch.tensor(tokenizer.encode(\"JOKE:\")).unsqueeze(0).to(device)\n\n            for i in range(100):\n                outputs = model(cur_ids, labels=cur_ids)\n                loss, logits = outputs[:2]\n                softmax_logits = torch.softmax(logits[0,-1], dim=0) #Take the first(from only one in this case) batch and the last predicted embedding\n                if i < 3:\n                    n = 20\n                else:\n                    n = 3\n                next_token_id = choose_from_top(softmax_logits.to('cpu').numpy(), n=n) #Randomly(from the topN probability distribution) select the next word\n                cur_ids = torch.cat([cur_ids, torch.ones((1,1)).long().to(device) * next_token_id], dim = 1) # Add the last word to the running sequence\n\n                if next_token_id in tokenizer.encode('<|endoftext|>'):\n                    joke_finished = True\n                    break\n\n            \n            if joke_finished:\n                \n                joke_num = joke_num + 1\n                \n                output_list = list(cur_ids.squeeze().to('cpu').numpy())\n                output_text = tokenizer.decode(output_list)\n\n                with open(jokes_output_file_path, 'a') as f:\n                    f.write(f\"{output_text} \\n\\n\")\n\n","b38739e2":"for saving and loading a bert or gpt model","72ba8e2b":"references:-https:\/\/towardsdatascience.com\/teaching-gpt-2-a-sense-of-humor-fine-tuning-large-transformer-models-on-a-single-gpu-in-pytorch-59e8cec40912"}}