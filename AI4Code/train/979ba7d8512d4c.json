{"cell_type":{"e4c47b46":"code","4898b33a":"code","0f5177f7":"code","7f7be47a":"code","3eae8ed4":"code","172da557":"code","fc3a0bf7":"code","7900527d":"code","0415c181":"code","a5d3e2fd":"code","5b2959d0":"code","57e136a0":"code","4ba7f491":"code","0c6a6ebf":"code","901b14f6":"code","9a202dcc":"code","91990174":"code","f774601f":"code","ac161c8c":"code","7d978717":"code","fedea6ad":"markdown","a8fa140c":"markdown","102888a1":"markdown","f4fe4009":"markdown","3940d88f":"markdown","01bdc09c":"markdown","be47e253":"markdown","5f2f8a9e":"markdown","5b1fa4c4":"markdown","160a9800":"markdown","8acbd18b":"markdown","b7c021b0":"markdown","e5c7b3e7":"markdown","8ea2c82d":"markdown","7f4e2585":"markdown","02e0be12":"markdown","37d7e55e":"markdown","b2aca74b":"markdown","cc7906c2":"markdown","830b3d82":"markdown","29076540":"markdown","2e10b9f6":"markdown"},"source":{"e4c47b46":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport pandas as pd\nimport numpy as np\nimport pdpbox, lime, shap, eli5\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, recall_score\nfrom sklearn.model_selection import train_test_split\nfrom imblearn.combine import SMOTETomek\n\n%matplotlib inline","4898b33a":"data = pd.read_csv('..\/input\/adult-census-income\/adult.csv')\ndata.shape","0f5177f7":"data.columns","7f7be47a":"data.head()","3eae8ed4":"data['target']=data['income'].map({'<=50K':0,'>50K':1})\ndata.drop(\"income\",axis=1,inplace=True)\ndata['target'].value_counts()","172da557":"# Let's drop \"education.num\" feature. We will use one-hot encoding instead.\ndata.drop(\"education.num\",axis=1,inplace=True)","fc3a0bf7":"# Since this example is for educational purposes, we'll also drop 'native-country' feature to decrease our data dimensionality.\ndata.drop('native.country',axis=1,inplace=True)","7900527d":"# Now we will encode categorical features using one-hot encoding, i.e. each category will now be represented by a separate column\n# containing only 0 and 1, depending on whether this category is relevant in a sample (row in our data) \ndata=pd.get_dummies(data, drop_first = True)","0415c181":"data.head()","a5d3e2fd":"y = data['target'].values\nfeatures = [col for col in data.columns if col not in ['target']]\nX = data[features]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, test_size=0.3, stratify=y)\nprint(X_train.shape, y_train.shape)\nprint(X_test.shape, y_test.shape)","5b2959d0":"model = RandomForestClassifier(random_state=1).fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\nprint(\"Accuracy: %.2f\" %accuracy_score(y_test, y_pred))\nprint(\"Recall: %.2f\" %recall_score(y_test, y_pred))","57e136a0":"import eli5\nfrom eli5.sklearn import PermutationImportance\n\nimp = PermutationImportance(model, random_state=1).fit(X_test, y_test)\neli5.show_weights(imp, feature_names = X_test.columns.tolist())","4ba7f491":"from pdpbox import pdp, get_dataset, info_plots\n\nfeat_name = 'capital.gain'\ncapital_gain_pdp = pdp.pdp_isolate(model=model, dataset=X_test, \n                                   model_features=X_test.columns, feature=feat_name)\n\npdp.pdp_plot(capital_gain_pdp, feat_name)\nplt.show()","0c6a6ebf":"feat_name = 'hours.per.week'\n\nhours_per_week_pdp = pdp.pdp_isolate(model=model, dataset=X_test, \n                                   model_features=X_test.columns, feature=feat_name)\n\npdp.pdp_plot(hours_per_week_pdp, feat_name)\nplt.show()","901b14f6":"# check the target. 1? perfect!\ny_test[69]","9a202dcc":"# taking a quick look on a sample\npd.DataFrame(X_test.iloc[69]).T","91990174":"# First, create a prediction on this sample\nrow = X_test.iloc[69]\nto_predict = row.values.reshape(1, -1)\n\nmodel.predict_proba(to_predict)","f774601f":"import shap \n# create object that can calculate shap values\nexplainer = shap.TreeExplainer(model)\n\n# calculate Shap values\nshap_values = explainer.shap_values(row)","ac161c8c":"# draw a plot\nshap.initjs()\nshap.force_plot(explainer.expected_value[1], shap_values[1], row)","7d978717":"import lime.lime_tabular\n\nexplainer = lime.lime_tabular.LimeTabularExplainer(X_train.values, feature_names=X_test.columns,\n                                                    discretize_continuous=True)\n\nexp = explainer.explain_instance(row, model.predict_proba, num_features=8)\nexp.show_in_notebook(show_table=True)","fedea6ad":"Let's read the plot above:\n- Basically, the base value is the mean of the model output over the train set. This means that accoding to our model, the probability of earning more than \\$50K over is on average 24%.\n- The red arrows show us **what** features and **how much** \"pushed\" the probability for a given person to earn more than people on general. Here the capital-gain amounts for more than \\$7K. Quite probable this person earns at least 50 grand a year. \n- The opposite goes for the blue arrows. Ouch, it seems like our model identified one of the trends of the job market: women earn less than men somehow. Since the model was trained on this kind of data, it identifies one's gender as one of the features affecting one's income.\n\n\nLet's also create a local explanation for that prediction using LIME:","a8fa140c":"Let's split our data into train and test in proportions 70\/30. We will also fix ```random_state``` for reproducability and use `stratify` to preserve the same class distribution.","102888a1":"Looks about right: capital-gain is identified as the most important feature for predicting the target as `1`.\n\n\n# <a id='wrapping_up'><\/a> 9. Wrapping up\n\nHope that this notebook was useful and now you know how to turn your black box into a explainable and trustworthy model. Cheers!\n\n**Bonus:** a quick (but totally not comprehensive!) overview of some tools for interpretable ML.\n\n![interpret.png](attachment:interpret.png)","f4fe4009":"# <a id='shap'><\/a> 7. SHAP\n\n[SHapley Additive exPlantions (SHAP)](https:\/\/github.com\/slundberg\/shap) is an method based on the concept of the Shapley values from the game theory.\n\nIdea: a feature is a \"player\", a prediction is a \"gain\". Then the Shapley value is the contribution of a feature averaged over all possible combinations of a \"team\":\n\n\n$$\\phi_i(v) = \\sum_{ S \\subseteq N \\setminus \\lbrace i \\rbrace } {{|S| ! ( N - |S| - 1 )!} \\over {N!}} ( v( S \\cup \\lbrace i \\rbrace) - v( S ))$$\n\n\n$N$ - all players.\n\n$S$ - the \"team\" of $N$ players.\n\n$v(S)$ - the gain of $S$.\n\n$v( S \\cup \\lbrace i \\rbrace) - v(S)$ - the \"player's\" contribution when joining $S$.\n\n**Advantages:**\n- Global and local interpretation.\n- Intuitively clear local explanations: the prediction is represented as a game outcome where the features are the team players.\n\n**Disadvantages:**\n- Shap returns only one value for each feature, not an interpretable model as LIME does.\n- Slow when creating a global interpretation.\n","3940d88f":"# <a id='practice'><\/a> 8. Practice! Explaining your ML model \ud83d\udd0d\n\n## Loading data\n\nOur __task__ is to predict whether a person earns more than 50,000$ a year.","01bdc09c":"# <a id='dependency_plots'><\/a> 5. Dependency plots\n\nPartial Dependency Plots will help you to answer the question \"**How** does the feature affect the predictions?\"\nPDP provides a quick look on the global relationship between the feature and the prediction. The plot below shows that the more money is at one's bank account, the higher the probability of one signing a term deposit during [a bank campaign](http:\/\/https:\/\/archive.ics.uci.edu\/ml\/datasets\/Bank+Marketing).\n\n![PDP.svg](attachment:PDP.svg)\n\nLet's look at how this plot is created:\n1. Take one sample: a single student, no loans, balance is around \\$1000.\n2. Increase the latter feature up to 5000.\n3. Make a prediction on an updated sample.\n4. What is the model output if `balance==10`? And so on.\n5. Moving along the x axis, from smaller to larger values, plot the resulting predictions on the y axis.\n\nNow, we considered only one sample. To create a PDP, we need to repeat this procedure for all the samples, i.e. all the rows in our dataset, and then draw the average prediction.\n\n**Advantages:**\n- Easy to interpret\n- Enables the interpretation of causality\n\n**Disadvantages:**\n- One plot can give you the analysis of only one or two features. Plots with more features would be difficult for humans to comprehend.\n- An assumption of the independent features. However, this assumption is often violated in real life. \n\n    Why is this a problem? Imagine that we want to draw a PDP for the data with correlated features. While we change the values of one feature, the values of the related feature stay the same. As a result, we can get unrealistic data points. For instance, we are interested in the feature `Weight`, but the dataset also contains such a feature as `Height`. As we change the value of `Weight`, the value of `Height` is fixed so we can end up having a sample with `Weight==200 kg` and `Height==150 cm`.\n- Opposite effects can cancel out the feature's impact.\n    \n    Imagine that a half of the values of a particular feature is positively correlated with the target: the higher the value, the higher the model's outcome. On the other hand, a half of the values is negatively correlated with the target: the lower the feature's value, the higher the prediction. In this case, a PDP may be a horizontal line since the positive effects got cancelled out by the negative ones.\n","be47e253":"[1. What's in a black box?](#black_box)\n\n[2. Different types of interpretation](#types)\n\n[3. Trade-off between Accuracy and Interpretability](#tradeoff)\n\n[4. Feature Importance](#feature_importance)\n\n[5. Dependency plots](#dependency_plots)\n\n[6. Local interpretation](#local)\n\n[7. SHAP](#shap)\n\n[8. Practice! Explaining your ML model \ud83d\udd0d](#practice)\n\n[9. Wrapping up](#wrapping_up)\n\n[10. Great resources to learn more about interpretable ML \ud83d\udcda](#resources)\n","5f2f8a9e":"## <font color='purple'> Textual description\n\nA brief text explanations is also an option.    \n    \n![image.png](attachment:image.png)\n    \nSource: [Generating Visual Explanations](https:\/\/arxiv.org\/pdf\/1603.08507.pdf)","5b1fa4c4":"# <a id='resources'><\/a> 10. Great resources to learn more about interpretable ML \ud83d\udcda\n\n- [Ch. Molnar, Interpretable ML book](https:\/\/christophm.github.io\/interpretable-ml-book\/)\n- [Kaggle course by Dan Becker: Machine Learning Explainability](https:\/\/www.kaggle.com\/learn\/machine-learning-explainability)\n- [LIME repo](https:\/\/github.com\/marcotcr\/lime)\n- [Shap repo](https:\/\/github.com\/slundberg\/shap)\n- [InterpretML: Open-source project by Microsoft](https:\/\/github.com\/interpretml\/interpret)\n","160a9800":"## <font color='purple'> Formulae\nAnd sometimes an old, good formula is worth a thousand of words:    \n\nHouse price = $\\$2800 * room + \\$10000 * {swimming pool} + \\$5000 * garage$","8acbd18b":"Our model predicts that this person earns more than \\$50K a year with the probabiltiy over 90%. Let's find out what affected this prediction.","b7c021b0":"Unsurprisingly, our model show that there's a positive correlation: people who have some capital-gains are more probable to earn more than $50,000.\n\nThe more money a person makes, the more they work. Kind of a logical thought, right? But let's check if that's true.","e5c7b3e7":"### Explore what features are important for the model. \n\nHere we are going to use permutation feature importance.","8ea2c82d":"# <a id='types'><\/a> 2. Different types of interpretation\n\nModel's predictions can be explained in different ways. The choice of media relies on what would be the most appropriate for a given problem.\n\n## <font color='purple'> Visualization\n    \nFor example, visualized interpretations are perfect for explaining the image classifier predictions.    \n![image.png](attachment:image.png)\n\nSource: [LIME Tutorial](https:\/\/marcotcr.github.io\/lime\/tutorials\/Tutorial%20-%20images.html)","7f4e2585":"# <a id='local'><\/a> 6. Local interpretation\n\nFor now, we have considered two methods of global interpretation: feature importance and dependecy plots. These approaches help us to explain our model's behaviour, well, at a global level which is surely nice. However, we often need to explain a particular prediction for an individual sample. To achieve this goal, we may turn to local interpretation. One technique that can be used here is [LIME, Local Interpretable Model-agnostic Explanations](https:\/\/github.com\/marcotcr\/lime) \n\nThe idea is as follows: instead of interpreting predictions of the black box we have at hand, we create a local surrogate model which is interpretable by its nature (e.g. a linear regression or a decision tree), use it to predict on an interesting data point and finally explain the prediction.\n\n![lime.png](attachment:lime.png)\n\nOn the picture above, the prediction to explain is a big red cross. Blue and pink areas represent the complex decision function of the black box model. Surely, this cannot be approximated by a linear model. However, as we see, the dashed line that represents the learned explanation is locally faithful.\n\nSource: [Why Should I Trust You?](https:\/\/arxiv.org\/pdf\/1602.04938.pdf)\n\n**Advantages:**\n- Concise and clear explanations.\n- Compatible with most of data types: texts, images, tabular data.\n- The speed of computation as we focus on one sample at a time.\n\n**Disadvantages:**\n- Only linear models are used to approximate the model's local behaviour.\n- No global explanations.","02e0be12":"# Interpret your ML model","37d7e55e":"# <a id='tradeoff'><\/a> 3. Trade-off between Accuracy and Interpretability\n\nThe thing is that not all kinds of machine learning models are equally interpretable. As a rule, more accurate and advanced algorithms, e.g. neural networks, are hard to explain. Imagine making sense of all these layers' weights!\n\nThus, it is a job of a data scientist to:\n1. Find a trade-off between accuracy and interpretability.\n\n    One may use a linear regression which predictions are easy to explain. But the price for a high interpretability may be a lower metric as compared to a more complicated boosting.\n\n2. Explain a choice of a particular algorightm to a client.\n\n![tradeoff.png](attachment:tradeoff.png)\n","b2aca74b":"# <a id='black_box'><\/a> 1. What's in a black box?\n\nThe more companies are interested in using machine learning and big data in their work, the more they care about the interpretability of the models. This is understandable: asking questions and looking for explanations is human. \n\nWe want to know not only \"What's the prediction?\", but \"Why so?\" as well. Thus, interpretation of ML models is important and helps us to:\n\n   - Explain individual predictions\n   - Understand models' behaviour\n   - Detect errors & biases\n   - Generate insights about data & create new features\n   \n![ml_model_lifecycle.png](attachment:ml_model_lifecycle.png)","cc7906c2":"# <a id='feature_importance'><\/a> 4. Feature importance\nFeature importance helps to answer the question \"**What features** affect the model's prediction?\"\n\nOne of the methods used to estimate the importance of features is Permutation importance.\n\n*Idea*: if we permute the values of an important feature, the data won't reflect the real world anymore and the accuracy of the model will go down.\n\nThe method work as follows:\n\n- Train the model \n- Mix up all values of the feature `X`. Make a prediction on an updated data.\n- Compute $Importance(X) = Accuracy_{actual} - Accuracy_{permutated}$.\n- Restore the actual order of the feature's values. Repeat steps 2-3 with a next feature.\n\n\n**Advantages:**\n- Concise global explanation of the model's behaviour.\n- Easy to interpret.\n- No need to re-train a model again and again.\n\n**Disadvantages:**\n- Need the ground truth values for the target.\n- Connection to a model's error. It's not always bad, simply not something we need in some cases.\n\n    Sometimes we want to know how much the prediction will change depending on the feature's value without taking into account how much the metric will change.","830b3d82":"Well, actually this logic is totally wrong. The plots shows us the following:\n- If a person works <20 hours a week, the chance of gaining \\$50K is around a zero. That's plausible because it's probably a part-time job.\n- The possibility of earning more than \\$50K is increasing linearly when working from 20 up to 40 hours a week.\n- However, working more hours won't make you richer on general. This could be explained by the fact that those extra hours (over standard 40) probably represent some side hastle which may be not stable. Another scenario might be that a person has several low-paying part-time jobs.  \n\nNow let's see how we can explain individual predictions of our model. In order to do that we'll find a person earning more than \\$50K from the test set and draw some plots with SHAP and LIME.","29076540":"## Data Preprocessing and Modeling\n\nSince we are going to tackle this case as a classification problem, let's encode the variable `income` into a binary target.","2e10b9f6":"Ok, looks like the most important feature in our case if `capital.gain`. Let's see **how** exactly it influences the target."}}