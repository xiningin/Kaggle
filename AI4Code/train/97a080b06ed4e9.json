{"cell_type":{"9e53adb0":"code","dc853b79":"code","17a9bec5":"code","815b6ca9":"code","20679c41":"code","6bb20c0e":"code","8158670c":"code","e2b85222":"code","e19e3a4f":"code","fc780101":"code","6e02e1a3":"code","aab5ed98":"code","49c75201":"code","a70b632f":"code","1f8bcd30":"code","d695599e":"code","dc6a5739":"code","d7b1021d":"code","29ad06a0":"code","719146c9":"code","62081e38":"code","56d495ac":"code","9519a8f4":"code","a0e08e3d":"code","a1e0c788":"code","cd67592c":"code","ba5f5627":"code","df6d4b77":"code","36eeb21c":"code","6032d2b8":"code","a6da0efd":"code","a0c16163":"code","edd9c3a1":"code","7896dd5c":"code","9f1663bb":"code","e6ce250d":"code","bc4f04ff":"code","21b9a701":"code","503179e7":"code","60b9936a":"code","61cea99a":"code","14eef208":"code","c414d0a9":"code","ee207b3d":"code","210500b7":"code","caddeb2c":"code","34f99a91":"code","c702156c":"code","af27017e":"code","fcc5af6e":"code","73ea6d3e":"code","59797a17":"code","8f5a1ecd":"code","9d381b28":"code","fbd500ee":"code","ba1bbc87":"code","1ac5b53d":"code","579940e0":"code","92a101b6":"code","2e3b325d":"code","dae78f41":"code","d2b7d450":"code","8a46451f":"code","67719308":"code","d10de3ac":"code","09111f6b":"code","b60d16c5":"code","02702abb":"code","2335a5f7":"code","a1b02ddb":"markdown","3fd40380":"markdown","6f697337":"markdown","2f830f29":"markdown","70e2a77b":"markdown","ec63dc08":"markdown"},"source":{"9e53adb0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","dc853b79":"import matplotlib.pyplot as plt # Data visualizations\nimport seaborn as sns #high-level interface for drawing attractive and informative statistical graphics\nimport pickle","17a9bec5":"drugs = pd.read_csv(\"\/kaggle\/input\/medicine-review\/train.csv\")\ndrugs","815b6ca9":"drugs.isnull().sum() ## there is 899 null values in condition column which is 0.005% of entire value#\n","20679c41":"print(\"Missing value (%):\", 1200\/drugs.shape[0] *100)\n#Missing value (%): 7%\n## droping all NA values from Train  data..##\ndrugs.dropna(inplace=True)","6bb20c0e":"drugs.isnull().sum()","8158670c":"len(set(drugs['uniqueID'].values))","e2b85222":"drugs['uniqueID'].unique()\n","e19e3a4f":"total_drugs = drugs['drugName'].value_counts()\nsns.countplot(total_drugs)","fc780101":"plt.hist(drugs.drugName)\n","6e02e1a3":"plt.hist(drugs.condition)\n","aab5ed98":"plt.plot(drugs.rating,drugs.drugName,\"bo\");plt.xlabel(\"Rating\");plt.ylabel(\"Drug name\")\n","49c75201":"plt.plot(drugs.rating,drugs.condition,\"bo\");plt.xlabel(\"Rating\");plt.ylabel(\"Drug_Condition\")\n","a70b632f":"plt.plot(drugs.rating,drugs.usefulCount,\"bo\");plt.xlabel(\"Rating\");plt.ylabel(\"UsefulCount\")","1f8bcd30":"plt.hist(drugs.rating)\nplt.boxplot(drugs.rating,0,\"rs\",0)","d695599e":"import re\ndef cleaning_text(i):\n    i = re.sub(\"[^A-Za-z\" \"]+\",\" \",i).lower()\n    i = re.sub(\"[0-9\" \"]+\",\" \",i)\n    i= re.sub(\"[\\W+\"\"]\", \" \",i)        \n    w = []\n    for word in i.split(\" \"):\n        if len(word)>3:\n            w.append(word)\n    return (\" \".join(w))","dc6a5739":"drugs.review= drugs.review.apply(cleaning_text)\ndrugs.review","d7b1021d":"drugs.condition=drugs.condition.apply(cleaning_text)\ndrugs.condition","29ad06a0":"# removing the date column as date has not that significance in output##\ndrugs.drop([\"date\"],axis=1,inplace=True)\ndrugs","719146c9":"\n## Removing specific words which dont have much significance and higher frequency..##\n#n_req= ['one','first','effect','side','taking','day', 'month','year','week','im','ive','mg','time','hour','could','lb','two','sideeffect','started','still']\n\n#drugs['review']=drugs['review'].apply(lambda x: \" \".join(word for word in x.split() if word not in n_req))","62081e38":"## subjectvity & polarity of each review rows...##\nfrom textblob import TextBlob\n\ndrugs['polarity'] = drugs['review'].apply(lambda x: TextBlob(x).sentiment.polarity)\n#%%time\ndrugs['subjectivity'] = drugs['review'].apply(lambda x: TextBlob(x).sentiment.subjectivity)","56d495ac":"drugs['polarity'],drugs['subjectivity']","9519a8f4":"#Categorisation of sentimental score on the basis of rating distribution we can divide the sentimental score in to 5 categories\n#compute sentiment scores (polarity) and labels","a0e08e3d":"## Finding sentiment through VADER sentiment Analyzer..##\nimport nltk\nnltk.download('vader_lexicon')\n","a1e0c788":"from nltk.sentiment.vader import SentimentIntensityAnalyzer\nsid = SentimentIntensityAnalyzer()\nsid.polarity_scores(drugs.iloc[4]['review'])\n","cd67592c":"drugs['vad_scores'] = drugs['review'].apply(lambda review:sid.polarity_scores(review))\ndrugs['vad_compound'] = drugs['vad_scores'].apply(lambda d:d['compound'])\ndrugs['vad_scores']","ba5f5627":"drugs['vad_scores']","df6d4b77":"drugs['vad_compound']","36eeb21c":"test_data = pd.read_csv(\"\/kaggle\/input\/medicine\/test.csv\")\ntest_data","6032d2b8":"test_data.drop([\"date\"],axis=1,inplace=True)","a6da0efd":"test_data","a0c16163":"test_data.isnull().sum()","edd9c3a1":"print(\"Missing value (%):\", 1200\/drugs.shape[0] *100)\n#Missing value (%): 2.9758952484872534\n## droping all NA values from Train  data..##\ntest_data.dropna(inplace=True)","7896dd5c":"test_data.review=test_data.review.apply(cleaning_text)\ntest_data.review","9f1663bb":"test_data.drugName=test_data.drugName.apply(cleaning_text)\ntest_data.drugName","e6ce250d":"test_data.condition=test_data.condition.apply(cleaning_text)\ntest_data.condition","bc4f04ff":"test_data['polarity'] = test_data['review'].apply(lambda x: TextBlob(x).sentiment.polarity)\n#%%time\ntest_data['subjectivity'] = test_data['review'].apply(lambda x: TextBlob(x).sentiment.subjectivity)","21b9a701":"sid = SentimentIntensityAnalyzer()\nsid.polarity_scores(test_data.iloc[4]['review'])","503179e7":"test_data['vad_scores'] =test_data['review'].apply(lambda review:sid.polarity_scores(review))\ntest_data['vad_compound'] = test_data['vad_scores'].apply(lambda d:d['compound'])\ntest_data['vad_scores']","60b9936a":"drugs['rating']= drugs['rating'].apply(lambda x: 1 if x>5 else 0)","61cea99a":"drugs['rating']","14eef208":"sns.countplot(drugs['rating'])\ndrugs['rating'].value_counts(normalize=True)*100","c414d0a9":"from imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder \nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score\n","ee207b3d":"cv = TfidfVectorizer()\nle = LabelEncoder() \ndrugs['drugName']= le.fit_transform(drugs['drugName']) \ndrugs['condition']= le.fit_transform(drugs['condition']) ","210500b7":"X= drugs[['vad_compound','polarity','subjectivity','condition','drugName']]\ny=drugs.rating\n\nX,y","caddeb2c":"Y","34f99a91":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2)\n\nX_train.shape,X_test.shape,y_train.shape,y_test.shape\n","c702156c":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(X_train)","af27017e":"# Now apply the transformations to the data:\nX_train =scaler.transform(X_train)\nX_test = scaler.transform(X_test)\nX_train,X_test","fcc5af6e":"from sklearn.neural_network import MLPClassifier\n\nmlp = MLPClassifier(hidden_layer_sizes=(10,10))\n\nmlp.fit(X_train,y_train)\nprediction_train=mlp.predict(X_train)\nprediction_test = mlp.predict(X_test)\n\nfrom sklearn.metrics import classification_report,confusion_matrix\nprint(confusion_matrix(y_test,prediction_test))\nnp.mean(y_train==prediction_train)*100\nnp.mean(y_test==prediction_test)*100\n\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, prediction_test ))\n\n## ACCURACY=73.53%....##","73ea6d3e":"from sklearn.metrics import cohen_kappa_score\ncohen_kappa_score(y_test,prediction_test)","59797a17":"from imblearn.over_sampling import SMOTE\nsm = SMOTE(random_state = 2) \nX_train_oversampled,y_train_oversampled = sm.fit_sample(X_train, y_train)\nX_train_oversampled.shape\ny_train_oversampled.shape","8f5a1ecd":"sns.countplot(y_train_oversampled)\ny_train_oversampled.value_counts(normalize=True)*100","9d381b28":"mlp = MLPClassifier(hidden_layer_sizes=(10,10))\n\nmlp.fit(X_train_oversampled,y_train_oversampled)\nprediction_train=mlp.predict(X_train_oversampled)\nprediction_test = mlp.predict(X_test)\n\nfrom sklearn.metrics import classification_report,confusion_matrix\nprint(confusion_matrix(y_test,prediction_test))\nnp.mean(y_train_oversampled==prediction_train)*100\nnp.mean(y_test==prediction_test)*100\n\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, prediction_test ))\n","fbd500ee":"#### appling SVM Model....###","ba1bbc87":"from sklearn import svm\nfrom sklearn.exceptions import NotFittedError\nfrom sklearn.metrics import classification_report","1ac5b53d":"\n#X= dataset[['polarity','vad_compound']]\n#Y=dataset['output']\n\n#X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)\n\n#X_test.shape\n#y_test.shape\n\n#%%time\n#classifier=svm.SVC(kernel='rbf',gamma='auto', C=2)\n#model=classifier.fit(X_train,y_train)  #... time 35  mins\n\n#y_pred=model.predict(X_test)\n\n#print(classification_report(y_test,y_pred))\n#print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n\n## with input polarity & subjectivity accuracy is 72.44% and with polarity and review_type accuracy is 45.54%..##\n\n#cohen_kappa_score(y_pred,y_test)\n\n#from sklearn.metrics import confusion_matrix\n#confusion_matrix = confusion_matrix(y_test, y_pred)\n#print(confusion_matrix)","579940e0":"## Using Naive Bays Classifier...##\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n\n\nX= drugs.review\ny=drugs.rating\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n\nnb = Pipeline([('tfidf', TfidfVectorizer()),\n               ('clf', MultinomialNB())])\n              \n\nmodel=nb.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\n\nprint(classification_report(y_test,y_pred))\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n\n## accuracy =75.75%.... precision 0 ..91   1----73","92a101b6":"from sklearn.metrics import cohen_kappa_score\n\ncohen_kappa_score(y_test, y_pred)\n","2e3b325d":"#########Logistic Regression############################","dae78f41":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline","d2b7d450":"log_re=Pipeline([('tfidf', TfidfVectorizer()),\n                ('clf', LogisticRegression()),\n               ])\n\nLR_Model=log_re.fit(X_train,y_train)\n\ny_pred = LR_Model.predict(X_test)\n\nprint(classification_report(y_test,y_pred))\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n\n## accuracy =75.75%.... precision 0 ..91   1----73\n","8a46451f":"### kappa score...##\ncohen_kappa_score(y_test,y_pred)","67719308":"print(classification_report(y_test,y_pred)) ## precision 0-> 78, 1-> 86... recall 0--> 63, 1-->> .92\n# confusion Matrix...##\nfrom sklearn.metrics import confusion_matrix\nconfusion_matrix = confusion_matrix(y_test, y_pred)\nprint(confusion_matrix)","d10de3ac":"## ROC Curve...\nfrom sklearn.metrics import roc_curve, roc_auc_score\nlogit_roc_auc = roc_auc_score(y_test, log_re.predict(X_test)) ## 77.51\nfpr, tpr, thresholds = roc_curve(y_test, log_re.predict_proba(X_test)[:,1])\nplt.figure()\nplt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Review Classification')\nplt.legend(loc=\"lower right\")\nplt.savefig('Log_ROC')\nplt.show() \n","09111f6b":"##........RANDOM FOREST CLAssifier Implementation...... ######\n\n## Using Model ..###\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import roc_curve, roc_auc_score\nrf = Pipeline([('tfidf', TfidfVectorizer()),\n                ('clf', RandomForestClassifier(n_estimators=100, n_jobs=-1)),\n               ]) #we can implement this code also result will be same...##   \n\n#%%time\nmodel=rf.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\n\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n #### Accuracy of the model is 88.24%...###\n\n","b60d16c5":"### kappa score...##\ncohen_kappa_score(y_test,y_pred) ##.70","02702abb":"print(classification_report(y_test,y_pred)) ## precision 0-> .96, 1-> .87... recall 0--> .64, 1-->> .99\n# confusion Matrix...##\nfrom sklearn.metrics import confusion_matrix\nconfusion_matrix = confusion_matrix(y_test, y_pred)\nprint(confusion_matrix)","2335a5f7":"## ROC Curve...\nrf_roc_auc = roc_auc_score(y_test, rf.predict(X_test)) #print(rf_roc_auc ) ## .8122\nfpr, tpr, thresholds = roc_curve(y_test, rf.predict_proba(X_test)[:,1])\nplt.figure()\nplt.plot(fpr, tpr, label='Random Forest(area = %0.2f)' % rf_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Review Classification')\nplt.legend(loc=\"lower right\")\nplt.savefig('Log_ROC')\nplt.show() ","a1b02ddb":"#vadar Score","3fd40380":"split data in train & test for SMOTE","6f697337":"###################.... Creating a NN Model...############################","2f830f29":"let's calculate with smote","70e2a77b":"indicate data is highly imbalance","ec63dc08":"#there is no need to SMOTE here bcz data is already balanced"}}