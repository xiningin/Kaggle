{"cell_type":{"639d28a5":"code","2c474779":"code","f560e2d1":"code","e3545b85":"code","9809c8d2":"code","9b2d883c":"code","80acfd8d":"code","6c46430b":"code","9d0759ad":"code","7506d2bb":"code","4540ee20":"code","f269e4ab":"code","fde7ee77":"code","77654cfe":"code","e6835b9e":"code","78d088dc":"code","a4b5f10d":"code","c96d9d01":"code","85095114":"code","fa0e29c2":"code","536346f8":"code","2bbf137e":"code","8a8050ff":"code","9d6881bb":"code","b6942829":"code","6874b311":"code","d74154b1":"markdown","f53a77f9":"markdown","0a88d75d":"markdown","a733bfe6":"markdown"},"source":{"639d28a5":"#Lets Import Important Libraries\nimport pandas as pd\nimport numpy as np\nimport warnings\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom imblearn.over_sampling import RandomOverSampler\n#from imblearn.combine import SMOTETome - Class to perform over-sampling using SMOTE (Synthetic Minority Oversampling Technique).\nwarnings.simplefilter(\"ignore\")","2c474779":"df = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')\ndf.drop(['Time'],axis=1,inplace=True)\ndf.head()","f560e2d1":"from sklearn.preprocessing import StandardScaler\ndf['norm_amount'] = StandardScaler().fit_transform(df['Amount'].values.reshape(-1, 1))\ndf.head(1)","e3545b85":"df.drop(['Amount'],axis=1,inplace=True)","9809c8d2":"print(\"The missing values in different columns are: \")\nprint(df.isnull().sum())","9b2d883c":"show = df['Class'].value_counts()\nshow.plot(kind='bar',figsize=(6,6),color='y')","80acfd8d":"show","6c46430b":"print('The percentage of no frauds is in the provided data is : ',show[0]\/show.sum() * 100,'%')\nprint('The percentage of frauds in the provided data is: ',show[1]\/show.sum() * 100,'%')","9d0759ad":"frauds = df[df['Class'] == 1]\nnon_frauds = df[df['Class'] == 0]\nprint(frauds.shape)\nprint(non_frauds.shape)","7506d2bb":"X = df.drop(['Class'],axis=1)\nX[:5]","4540ee20":"Y = df[['Class']]\nY[:5]","f269e4ab":"ros =  RandomOverSampler(sampling_strategy=0.5) #To perform Oversampling\nros","fde7ee77":"Xs, ys = ros.fit_sample(X, Y)\nprint(Xs.shape)\nprint(ys.shape)","77654cfe":"ys['Class'].value_counts()","e6835b9e":"Xs = Xs.values\nys = ys.values\nprint(Xs[:1])\nprint(ys[:3])","78d088dc":"from sklearn.model_selection import train_test_split","a4b5f10d":"X_train, X_test, y_train, y_test = train_test_split(Xs, ys, test_size = 0.25, random_state = 16)\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","c96d9d01":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\ntrain_X = scaler.fit_transform(X_train)\ntest_X = scaler.transform(X_test)","85095114":"from sklearn.svm import SVC","fa0e29c2":"classifier = SVC(kernel = 'linear')\nclassifier","536346f8":"classifier.fit(train_X, y_train)","2bbf137e":"y_pred = classifier.predict(test_X)\ny_pred","8a8050ff":"from sklearn.metrics import accuracy_score \nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report","9d6881bb":"print(\"Accuracy of Model on test data:  \", accuracy_score(y_test, y_pred) *  100)","b6942829":"print(classification_report(y_test, y_pred))","6874b311":"cm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm,annot=True,fmt=\"d\")","d74154b1":"Here the values of Amount column are not scaled so  lets scale them first.","f53a77f9":"The data is clearly imbalanced. TO solve this we can either perfom undersampling or oversampling. In this notebook we will perform oversampling.","0a88d75d":"Now we will perform classification using Support Vector Machine(SVM).","a733bfe6":"Lets see if there is any missing value."}}