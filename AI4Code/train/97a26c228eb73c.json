{"cell_type":{"f0639c68":"code","b73251c6":"code","925304f7":"code","3d00d041":"code","a3810c0a":"code","4f005815":"code","e1544df5":"code","dc23bd0a":"code","4445ad7c":"code","96920ccc":"code","0054a118":"code","b12f4f36":"code","06e420e9":"code","f5e79b03":"code","fb091d88":"code","3c913dc3":"code","6f041595":"code","de123bc1":"code","357dfe4f":"code","e5d616ad":"code","1f455efa":"code","1ae65a2d":"code","dbcb49d5":"code","d9ef3ce3":"code","0675a8ea":"code","787459c7":"code","b963f138":"code","faa681a5":"markdown","53610067":"markdown","5775c1f1":"markdown","b339712f":"markdown","f7f8d9b5":"markdown","7b5ab09c":"markdown","5b7f4462":"markdown","39e349d1":"markdown","c0d76bf5":"markdown","7ad4625f":"markdown","c4701c9d":"markdown","67b1abc5":"markdown","95522375":"markdown","50a5818e":"markdown","6ede810f":"markdown","0adccdf3":"markdown","28e679f0":"markdown","c1341ebd":"markdown","8c8aeaee":"markdown","4c2a7486":"markdown"},"source":{"f0639c68":"import math\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import cm\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import FuncFormatter as FF\nfrom matplotlib.ticker import StrMethodFormatter as SMF\nimport seaborn as sns\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.model_selection import KFold\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import LinearRegression as lr\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import train_test_split,cross_val_score,GridSearchCV\nfrom sklearn.preprocessing import StandardScaler","b73251c6":"df = pd.read_csv('..\/input\/ames-housing-dataset\/AmesHousing.csv')\npd.set_option(\"display.max_columns\", None)\ndf.head()","925304f7":"df.describe()","3d00d041":"df.info()","a3810c0a":"df_na = df.isna().sum().to_frame().sort_values(by = 0, axis = 0)\ndf_na = df_na.rename(columns={0: 'NA Count'})\ndf_na.T","4f005815":"clean_columns = df_na[df_na['NA Count'] == 0].index.to_list()\ndf_clean = df[clean_columns]\ndf_clean.shape","e1544df5":"sns.set()\n#Graph 1\ng1 = sns.lmplot(data = df_clean, x = 'Lot Area', y = 'SalePrice', hue = 'Bedroom AbvGr',\n                  palette = 'viridis_r',ci = None, height = 9, aspect = 16 \/ 9)\n\n#Axes.\nax = plt.gca()\n\n#Title setup.\nax.set_title('Price vs Lot Area \/ # of Bedrooms', fontsize = 32)\n\n#X-axis setup.\nax.set_xlabel(\"Lot Area (sq. ft.)\", fontsize = 24)\nax.set_xscale('log')\nxlabels = [2500, 5000, 10000, 20000, 40000, 80000, 160000, 320000]\nax.set_xticks(xlabels)\nax.set_xticklabels(xlabels, rotation = 45,ha = 'right')\nax.get_xaxis().set_major_formatter(FF(lambda x, p: format(int(x), ',')))\n\n#Y-axis setup.\nax.set_ylabel(\"Price\", fontsize = 24)\nax.set_ylim(0,800000)\nax.yaxis.set_major_formatter(SMF('${x:,.0f}'))\nax.tick_params(axis = 'both', which = 'major', labelsize = 16)\n\n#Legend setup.\ng1._legend.remove()\nax.legend(loc = 'upper left', title = 'Bedrooms', ncol = 2, title_fontsize = 18, fontsize = 16);\n\n#Bedroom count\nbedroom_count = df_clean['Bedroom AbvGr'].value_counts().sort_index().to_frame().rename(columns = {'Bedroom AbvGr': \"# of Houses\"})\nbedroom_count.index.name = \"Number of Bedrooms\"\nbedroom_count.T","dc23bd0a":"#Graph 2\ng2 = sns.lmplot(data = df_clean, x = 'Year Built', y = 'SalePrice', hue = 'Overall Qual',\n                  palette = 'viridis_r', ci=None, height = 9, aspect = 16 \/ 9)\n\n#Axes\nax = plt.gca()\n\n#Title\nax.set_title('Price vs Year Built \/ Overall Quality', fontsize = 32)\n\n#X-axis\nax.set_xlabel(\"Year Built\", fontsize = 24)\nax.set_xlim(1870, 2015)\nax.set_xticklabels(ax.get_xticklabels(), rotation = 45, ha = 'right')\n\n#Y-axis\nax.set_ylabel(\"Price\", fontsize = 24)\nax.set_ylim(0, 800000)\nax.yaxis.set_major_formatter(SMF('${x:,.0f}'))\nax.tick_params(axis = 'both', which = 'major', labelsize = 16)\n\n#Legend\ng2._legend.remove()\nax.legend(loc = 'upper left', title = 'Overall House Quality', ncol = 2, title_fontsize = 18, fontsize = 16)\n\n#Overall house quality count\nneighborhood_count = df_clean['Overall Qual'].value_counts().sort_index().to_frame().rename(columns = {'Overall Qual': \"# of Houses\"})\nneighborhood_count.index.name = \"Overall Quality\"\nneighborhood_count.T","4445ad7c":"#Graph 3\nxlabels = df_clean.groupby(['Neighborhood'])['SalePrice'].median().sort_values().index\ng3 = sns.boxplot(data = df_clean, x = 'Neighborhood', y = 'SalePrice', palette = 'viridis_r', order = xlabels)\nplt.gcf().set_size_inches(16, 9)\n\n#Axes\nax = plt.gca()\n\n#Title\nax.set_title('Price vs Neighborhood', fontsize = 24)\n\n#X-axis\nax.set_xlabel(\"Neighborhood\", fontsize = 24)\nax.set_xticklabels(ax.get_xticklabels(), rotation = 45, ha = 'right')\n\n#Y-axis\nax.set_ylabel(\"Price\", fontsize = 24)\nax.set_ylim(0, 800000)\nax.yaxis.set_major_formatter(SMF('${x:,.0f}'))\nax.tick_params(axis = 'both', which = 'major', labelsize = 16)\n\n#Neighborhood count\nneighborhood_count = df_clean['Neighborhood'].value_counts().sort_index().to_frame().rename(columns = {'Neighborhood': \"# of Houses\"})\nneighborhood_count.index.name = \"Neighborhood\"\nneighborhood_count.T","96920ccc":"#Graph 4\ng4 = sns.violinplot(data = df_clean, x = 'Neighborhood', y = 'SalePrice', palette = 'viridis_r', scale = 'width', order = xlabels)\nplt.gcf().set_size_inches(16, 9)\n\n#Axes\nax = plt.gca()\n\n#Title\nax.set_title('Price vs Neighborhood', fontsize = 24)\n\n#X-axis\nax.set_xlabel(\"Neighborhood\", fontsize = 24)\nax.set_xticklabels(ax.get_xticklabels(), rotation = 45, ha = 'right')\n\n#Y-axis\nax.set_ylabel(\"Price\", fontsize = 24)\nax.set_ylim(0, 800000)\nax.yaxis.set_major_formatter(SMF('${x:,.0f}'))\nax.tick_params(axis = 'both', which = 'major', labelsize = 16)\n\n#Neighborhood count\nneighborhood_count = df_clean['Neighborhood'].value_counts().sort_index().to_frame().rename(columns = {'Neighborhood': \"# of Houses\"})\nneighborhood_count.index.name = \"Neighborhood\"\nneighborhood_count.T","0054a118":"#Graph 5\ng5 = sns.heatmap(df_clean[['Lot Area','Overall Qual','Bedroom AbvGr','Overall Cond','Full Bath','Half Bath','1st Flr SF','2nd Flr SF','Pool Area','Open Porch SF','TotRms AbvGrd','Year Built','SalePrice']].corr(),cmap='Blues')","b12f4f36":"#Graph 6\ng6 = sns.histplot(data=df_clean,x='SalePrice')\nplt.gcf().set_size_inches(16, 9)","06e420e9":"#Graph 7\n#Housing prices log normalized\nnp.log1p(df_clean['SalePrice'])\ng7 = sns.histplot(data=df_clean,x=np.log1p(df_clean['SalePrice']))\nplt.gcf().set_size_inches(16, 9)","f5e79b03":"df_final = df_clean.drop(['Order','PID'],axis=1)\ndf_final['SalePrice']=np.log1p(df_clean['SalePrice'])\ndf_final.set_index('SalePrice',inplace=True)","fb091d88":"one_hot_encode_cols = df_final.dtypes[df_final.dtypes == np.object]\none_hot_encode_cols = one_hot_encode_cols.index.tolist()\n\ndf_final = pd.get_dummies(df_final, columns = one_hot_encode_cols, drop_first=True)\ndf_final.shape","3c913dc3":"float_cols = df_final.columns[df_final.dtypes == np.float]\nskew_limit = 0.75\nskew_vals = df_final[float_cols].skew()\nskew_cols = (skew_vals\n             .sort_values(ascending = False)\n             .to_frame()\n             .rename(columns = {0: 'Skew'})\n             .query('abs(Skew) > {}'.format(skew_limit)))\n#skew_cols","6f041595":"X = df_final.reset_index().drop('SalePrice', axis = 1)\ny = df_final.index\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)","de123bc1":"X_train1 = X_train.copy()\ny_train1 = y_train.copy()\nX_test1 = X_test.copy()\ny_test1 = y_test.copy()\n\nlm = lr().fit(X_train1, y_train)\ny_pred = lm.predict(X_test1)\nlm.score(X_test1,y_test1)","357dfe4f":"kf = KFold(shuffle = True, random_state = 42, n_splits = 5)","e5d616ad":"def optimize_alpha(alphas, x, y, model, kf):\n    \n    #Scale and transform x.\n    s = StandardScaler()\n    x = s.fit_transform(x)\n    \n    #List of R2.\n    r2_scores = []\n    \n    for alpha in alphas:\n        \n        reg = model(alpha = alpha, max_iter = 500000)\n        y_pred = cross_val_predict(reg, x, y, cv = kf)\n        score = r2_score(y, y_pred)\n        r2_scores.append(score)\n    \n    return(r2_scores)","1f455efa":"def alpha_r2_graph(alphas, R2s, xlabels, model):\n    \n    df = pd.DataFrame(data = {'alpha': alphas,'R2': R2s})\n    sns.set()\n    #Scatter Plot\n    sns.lineplot(data = df, x = 'alpha', y = 'R2', marker = 'o')\n    \n    #Size\n    plt.gcf().set_size_inches(15, 6.92)\n    paper_rc = {'lines.linewidth': 2, 'lines.markersize': 6}  \n    \n    #Axes\n    ax = plt.gca()\n    \n    #Title\n    ax.set_title(\"Hyperparameter Optimization for {} Regression\".format(model), fontsize = 24)\n\n    #X-axis\n    ax.set_xlabel(\"\u03b1\", fontsize = 22)\n    ax.set_xscale('log')\n    ax.set_xticks(xlabels)\n    ax.set_xticklabels(xlabels, rotation = 45, ha = 'right')\n    if (model == 'Ridge') :\n        ax.get_xaxis().set_major_formatter(FF(lambda x, p: format(int(x), ',')))\n\n    #Y-axis\n    ax.set_ylabel(\"R2\", fontsize = 22)\n    ylabels = [0, 0.2, 0.4, 0.6, 0.8, 1]\n    ax.set_xticks(xlabels)\n    \n    ax.tick_params(axis = 'both', which = 'major', labelsize = 16)","1ae65a2d":"alphas = list(pd.core.common.flatten([[a \/ 2, a, 2 * a] for a in np.geomspace(1e-5, 1e1, 7)]))\nxlabels = [a for a in np.geomspace(1e-5, 1e1, 7)]\n\ns = StandardScaler()\nX_train2 = s.fit_transform(X_train)\ny_train2 = y_train.copy()\nX_test2 = s.fit_transform(X_test)\ny_test2 = y_test.copy()\n\n#R2s and graph\nr2_lasso = optimize_alpha(alphas, X_train2, y_train2, Lasso, kf)\nalpha_r2_graph(alphas, r2_lasso, xlabels, 'Lasso')\n\n#Lasso Regression\nlm_lasso = Lasso(alpha=0.005).fit(X_train2,y_train2)","dbcb49d5":"alphas = list(pd.core.common.flatten([[a \/ 2, a, 2 * a] for a in np.geomspace(1, 1e6, 7)]))\nxlabels = [a for a in np.geomspace(1, 1e6, 7)]\n\ns = StandardScaler()\nX_train3 = s.fit_transform(X_train)\ny_train3 = y_train.copy()\nX_test3 = s.fit_transform(X_test)\ny_test3 = y_test.copy()\n\n#Determine R2s and graph.\nr2_ridge = optimize_alpha(alphas, X_train3, y_train3, Ridge, kf)\nalpha_r2_graph(alphas, r2_ridge, xlabels, 'Ridge')\n\n#Ridge Regression\nlm_ridge = Ridge(alpha=500).fit(X_train3,y_train3)","d9ef3ce3":"def summary_table(models, Xs, Y) :\n\n    index = ['Linear','Lasso', 'Ridge']\n    R2 = []\n    ADJUSTED_R2 = []\n    RMSE = []\n    MAE = []\n\n    for i in range(3):\n        y_pred = models[i].predict(Xs[i])\n        \n        #R2.\n        r2 = r2_score(Y, y_pred)\n        R2.append(r2)\n        \n        #Adj R2\n        adjusted_r2 = 1.0 - (1.0 - r2) * (len(Y) - 1.0) \/ (len(Y) - Xs[i].shape[1] - 1.0)\n        ADJUSTED_R2.append(adjusted_r2)\n        \n        #RMSE\n        rmse = math.sqrt(mean_squared_error(Y, y_pred))\n        RMSE.append(rmse)\n                         \n        #MAE\n        mae = mean_absolute_error(Y, y_pred)\n        MAE.append(mae)\n\n    df_summary = pd.DataFrame(data = {'R2': R2,'Adjusted R2': ADJUSTED_R2,'RMSE': RMSE,'MAE': MAE},index = index)\n    \n    return(df_summary)","0675a8ea":"linear_models = [lm, lm_lasso, lm_ridge]\nX_trains = [X_train1, X_train2, X_train3]\nsummary_table(linear_models, X_trains, y_train)","787459c7":"X_tests = [X_test1, X_test2, X_test3]\nsummary_table(linear_models, X_tests, y_test)","b963f138":"for i in linear_models:\n    print(len(X_train.columns[i.coef_ != 0]))","faa681a5":"I am using the Ames, Iowa dataset containing 2930 observations and 81 features related to house sale prices in Ames, Iowa. If you'd like to browse the various features, take a look at the features [here](http:\/\/jse.amstat.org\/v19n3\/decock\/DataDocumentation.txt).\nThe plan is to train test split the housing data. Afterwards, regression and regularization will be used to compare and analyze the model predicting house prices for Amex, IA.","53610067":"### Regularized Lasso Regression\n#### K fold ","5775c1f1":"#### Alpha Graph","b339712f":"#### Test Data","f7f8d9b5":"## 1) Load relevant packages","7b5ab09c":"## Predicting Sale Price for Houses in Ames, IA","5b7f4462":"## 5) Data Modeling","39e349d1":"#### Lasso Regression maintains accuracy while reducing complexity by more than half.","c0d76bf5":"#### Summary Function","7ad4625f":"<b>Dropped Order and PID features because they're ID variables and log transformed the SalePrice.","c4701c9d":"<b>One-Hot encoding dummy variables","67b1abc5":"## 3) Data Cleaning and Feature Engineering\nAfter browsing the various features, the initial plan is to use the columns with little to no missing values. The features with a lot of missing values don't look like variables that have a huge impact on sale price. Various variables like bedroom count, lot area, year built, overall quality, and neighborhood will be plotted against sale price.","95522375":"<b>Train Test Split","50a5818e":"## 2) Load Data","6ede810f":"#### Lasso with L1 Regularization","0adccdf3":"#### Ridge with L2 Regularization","28e679f0":"#### Train Data","c1341ebd":"## 4) Exploratory Data Analysis","8c8aeaee":"#### Optimization Function","4c2a7486":"<b>Log transforming skew variables"}}