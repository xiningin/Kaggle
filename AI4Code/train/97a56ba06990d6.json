{"cell_type":{"d210addf":"code","6c743425":"code","0e2fccfd":"code","0c9f764e":"code","e869291b":"code","d36174c5":"code","4e1c7966":"code","1d7deef9":"code","621a44bd":"markdown","0f19fff0":"markdown"},"source":{"d210addf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6c743425":"!pip install transformers\n","0e2fccfd":"!pip install -U pytorch-pretrained-bert","0c9f764e":"import torch\nfrom pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n\n# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\nimport logging\nlogging.basicConfig(level=logging.INFO)\n\n# Load pre-trained model tokenizer (vocabulary)\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\ntext = '[CLS] I want to [MASK] the car because it is cheap . Malaria is a [MASK] disease [SEP]'\ntokenized_text = tokenizer.tokenize(text)\nindexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n\n# Create the segments tensors.\nsegments_ids = [0] * len(tokenized_text)\n\n# Convert inputs to PyTorch tensors\ntokens_tensor = torch.tensor([indexed_tokens])\nsegments_tensors = torch.tensor([segments_ids])\n\n# Load pre-trained model (weights)\nmodel = BertForMaskedLM.from_pretrained('bert-base-uncased')\nmodel.eval()\nmasked_index = [i for i, x in enumerate(tokenized_text) if x == '[MASK]']\n# Predict all tokens\nwith torch.no_grad():\n    predictions = model(tokens_tensor, segments_tensors)\nfor i in masked_index:\n      #masked_index = tokenized_text.index(word)\n      predicted_index = torch.argmax(predictions[0, i]).item()\n      predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n      print(predicted_token)\n","e869291b":"!pip install pytorch_transformers","d36174c5":"import torch\nimport numpy as np\nimport nltk\nfrom pytorch_transformers import BertTokenizer, BertModel, BertForMaskedLM\n\n# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\nimport logging\nlogging.basicConfig(level=logging.INFO)\n\n# Load pre-trained model tokenizer (vocabulary)\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertForMaskedLM.from_pretrained('bert-base-uncased', output_attentions=True)\nmodel.eval()\n\ntext = '[CLS] that is trained [MASK] predict the word in sentence . [SEP]'\ntokenized_text = tokenizer.tokenize(text)\nindexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\nmasked_index = tokenized_text.index('[MASK]')\n\n# Create the segments tensors.\nsegments_ids = [0] * len(tokenized_text)\n\n# Convert inputs to PyTorch tensors\ntokens_tensor = torch.tensor([indexed_tokens])\nsegments_tensors = torch.tensor([segments_ids])\n\n# Load pre-trained model (weights)\n\n\n# Predict all tokens\nwith torch.no_grad():\n    outputs = model(tokens_tensor, token_type_ids=segments_tensors)\n    predictions = outputs[0]\n    attention = outputs[-1]\npredicted_index = torch.argmax(predictions[0, masked_index]).item()\npredicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n\nprint(predicted_token)\n","4e1c7966":"dim = attention[2][0].shape[-1]*attention[2][0].shape[-1]\na = attention[2][0].reshape(12, dim)\nb = a.mean(axis=0)\nc = b.reshape(attention[2][0].shape[-1],attention[2][0].shape[-1])\navg_wgts = c[masked_index]\n#print (avg_wgts, tokenized_text)\nfocus = [tokenized_text[i] for i in avg_wgts.argsort().tolist()[::-1] if tokenized_text[i] not in ['[SEP]', '[CLS]', '[MASK]']][:5]","1d7deef9":"print(focus)","621a44bd":"If we write a sentence and replace the missing word with MASK token, bert pretrained model can be used to predict those missing words","0f19fff0":"We can also calculate the attention weight and find out the words which got the maximum score to predict the respective sentence\n"}}