{"cell_type":{"82d08796":"code","3e0c4513":"code","5cf57a2e":"code","061c2f15":"code","415e3c73":"code","10842ff7":"code","60dd27d1":"code","5963409d":"code","30e4c721":"code","8f579fc4":"code","f74545ae":"code","03d1c2fa":"code","274f65dd":"code","6069ceec":"code","1272ca1f":"code","fc702327":"code","1d9536e9":"code","4443c60c":"code","bbd8a34d":"code","52748e8e":"code","9e274d1e":"code","71a29bfd":"code","6792f36e":"code","74e3aff3":"code","a922f920":"code","23ca75fc":"code","969b4789":"code","7c9c1607":"code","6fbdfc28":"code","64fa8b96":"code","78e37460":"code","5e34f5c7":"code","defc5bca":"code","c5d71565":"code","b4830361":"code","ff9f60ba":"code","1a7a7c54":"code","42081e72":"code","a62d5c12":"code","3ba7f5e1":"code","e0dacdbf":"code","dd5ba6b8":"code","230135d5":"code","04d53f5b":"code","bbe08fca":"code","9dec7570":"code","7f12274b":"code","3aaa67e9":"code","29831bae":"code","8233ed4e":"code","64ef0498":"code","e6470da2":"code","c7db30b9":"code","6b21c09d":"code","6c551199":"code","6976ac39":"code","219c1b7d":"code","8f277dc3":"code","6afb5052":"markdown","0a2d6255":"markdown","858e2dd4":"markdown","67df986a":"markdown","de2f4f8c":"markdown","eb86e85e":"markdown","0b350789":"markdown","f2971259":"markdown","e42030ac":"markdown","cee9ca1d":"markdown","b692b411":"markdown","bde86aef":"markdown","ed3b0473":"markdown","a2db0f71":"markdown","017b6c53":"markdown","a1ca77a3":"markdown","724647b7":"markdown","28e92f6d":"markdown","191bf516":"markdown","7b3c99f9":"markdown","3f4108a0":"markdown","01639bf1":"markdown","f1782672":"markdown","e961bd9a":"markdown","3db10a0e":"markdown","91a879f1":"markdown","39941172":"markdown","5c7ff9cb":"markdown","1d1b7bd6":"markdown","31570a7b":"markdown","e89728ac":"markdown","86ae2490":"markdown","48b4dce7":"markdown","30147282":"markdown","57386588":"markdown","1373ab6f":"markdown","e4b76162":"markdown","ff379d38":"markdown","2fe9c6b4":"markdown","7c8d9224":"markdown","6853ff1f":"markdown","79d95413":"markdown","b93b8747":"markdown","364aeb34":"markdown","10222716":"markdown"},"source":{"82d08796":"import time\nimport gc  #garbage collection\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' #suppressing GPU warnings\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.feature_selection import mutual_info_classif\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.preprocessing import StandardScaler, RobustScaler \nfrom sklearn.preprocessing import MinMaxScaler, MaxAbsScaler\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.pipeline import Pipeline\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\n\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-darkgrid')   #for plotting learning curves\nplt.rc('axes', labelweight='bold', labelsize='medium',\n       titleweight='bold', titlesize=12, titlepad=10)\n\nSEED = 2311\n\n%pprint","3e0c4513":"train = pd.read_csv('..\/input\/tps-nov21-data\/tps-nov21-6folds.csv')","5cf57a2e":"test = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/test.csv')","061c2f15":"features = [f for f in train.columns if f not in ('id', 'target', 'fold')] #original feature set","415e3c73":"#extracting selected features based on boolean mask returned by the feature selector\ndef get_selected_features(mask, original_features):\n    return [b for a, b in zip(mask, original_features) if a]\n\n#another way of selecting the relevant data would be train.loc[:, mask]\n#but we want the feature list for our 'custom_cross_val_predict' function","10842ff7":"scaled_train = train[features] \/ train[features].mean()","60dd27d1":"np.asarray(sorted(scaled_train.var()))","5963409d":"vt_selector = VarianceThreshold(threshold=0.5).fit(scaled_train)\nvt_features = get_selected_features(vt_selector.get_support(), features) #feature-set 0\n#get_support() returns a boolean mask depending on which features were selected\n#list with same length as original features\n\nprint(f'Number of features selected by Variance Threshold: {len(vt_features)}')","30e4c721":"vt_features","8f579fc4":"del scaled_train\ngc.collect()","f74545ae":"# %%time\n# mi_scores = mutual_info_classif(train[features], train.target,\n#                                 discrete_features=False, random_state=SEED)","03d1c2fa":"mi_scores = [0.00000000e+00, 1.96434784e-04, 0.00000000e+00, 3.24873570e-04,\n           0.00000000e+00, 4.75964111e-04, 0.00000000e+00, 0.00000000e+00,\n           5.03380171e-03, 0.00000000e+00, 1.27742161e-03, 0.00000000e+00,\n           1.26559691e-04, 0.00000000e+00, 0.00000000e+00, 1.01206862e-03,\n           6.84650663e-04, 2.16435823e-03, 0.00000000e+00, 0.00000000e+00,\n           1.07352768e-03, 1.03902893e-03, 3.12189166e-03, 0.00000000e+00,\n           1.88988660e-03, 1.58894727e-03, 0.00000000e+00, 4.67464802e-03,\n           0.00000000e+00, 3.03387459e-04, 4.15842780e-05, 1.43445676e-03,\n           0.00000000e+00, 4.67015321e-04, 9.41982692e-03, 0.00000000e+00,\n           3.28791846e-04, 0.00000000e+00, 5.31774984e-04, 3.19115729e-04,\n           2.35229287e-03, 3.59450827e-03, 6.85933487e-06, 6.69911708e-03,\n           1.22169000e-03, 6.21526524e-05, 0.00000000e+00, 0.00000000e+00,\n           8.13240439e-04, 1.56451963e-03, 3.19821423e-03, 1.23502574e-03,\n           1.07206457e-04, 3.17651844e-04, 1.25395409e-03, 6.67993524e-03,\n           2.60800294e-04, 2.02716563e-03, 0.00000000e+00, 6.46768333e-04,\n           9.63983707e-04, 0.00000000e+00, 3.60474773e-04, 1.50197749e-03,\n           4.46106477e-04, 3.49257353e-05, 1.04204391e-03, 6.29323354e-04,\n           0.00000000e+00, 0.00000000e+00, 5.43070916e-04, 5.81930670e-03,\n           2.44215992e-04, 0.00000000e+00, 0.00000000e+00, 9.65990836e-04,\n           4.35211308e-04, 3.22449569e-04, 7.98747472e-04, 0.00000000e+00,\n           5.81364980e-03, 8.88931821e-04, 2.36474749e-03, 6.93652415e-06,\n           4.94263510e-04, 8.49602201e-04, 6.17923826e-04, 0.00000000e+00,\n           5.64878468e-04, 7.77078867e-05, 9.66702475e-04, 4.64267011e-03,\n           1.93027681e-05, 6.49350870e-04, 2.57520012e-04, 5.91060864e-04,\n           2.00171166e-03, 2.30227132e-03, 3.19943381e-04, 0.00000000e+00]","274f65dd":"mi_scores.count(0.0)","6069ceec":"mi_features = get_selected_features(mi_scores, features) \nprint(f'Number of features selected by Mutual Information scores: {len(mi_features)}')\n\n#the utility function still works since 0 is interpreted as False so our mi_scores act as a mask","1272ca1f":"mi_features","fc702327":"%%time\nestimator = ExtraTreesClassifier(n_estimators=150, \n                                 criterion='gini',\n                                 n_jobs=-1,\n                                 random_state=SEED)\n\nestimator.fit(train[features], train.target)","1d9536e9":"np.asarray(sorted(estimator.feature_importances_))","4443c60c":"sfm_selector = SelectFromModel(estimator=estimator, prefit=True, threshold='median')\n#default threshold is 'mean'\n\nsfm_features = get_selected_features(sfm_selector.get_support(), features)\nprint(f'Number of features selected by embedded estimator: {len(sfm_features)}')","bbd8a34d":"sfm_features","52748e8e":"vt_set = set(vt_features)\nmi_set = set(mi_features)\nsfm_set = set(sfm_features)","9e274d1e":"#features that should definitely be included\ncommon = mi_set.intersection(vt_set, sfm_set)\ncommon","71a29bfd":"#features present in vt and sfm but not in mi\nnot_mi = vt_set.union(sfm_set) - mi_set\nnot_mi","6792f36e":"sfm_but_not_mi = sfm_set - mi_set\nsfm_but_not_mi","74e3aff3":"high_mi = set(get_selected_features(np.asarray(mi_scores) > 0.002, features))\nhigh_mi_extra = high_mi - sfm_set\nhigh_mi_extra","a922f920":"selected_features = sfm_set - sfm_but_not_mi\nlen(selected_features)","23ca75fc":"pca_pipeline1 = Pipeline([\n    ('StandardScaler', StandardScaler()),\n    ('PCA1', PCA(n_components='mle', svd_solver='full'))\n])","969b4789":"%%time\nxtemp = pca_pipeline1.fit_transform(train[features])\nxtemp.shape[1]","7c9c1607":"del xtemp\ngc.collect()","6fbdfc28":"pca_pipeline2 = Pipeline([\n    ('StandardScaler', StandardScaler()),\n    ('PCA2', PCA(n_components=50, svd_solver='full'))\n])","64fa8b96":"#using Keras Functional API instead of Sequential API\n#since we need to separate out the encoder for later use\n\nnum_features = len(features)\n\nvisible = layers.Input(shape=(num_features,))\n#encoder hidden layer 1\nencoder = layers.Dense(num_features * 2)(visible)\nencoder = layers.BatchNormalization()(encoder)\nencoder = layers.LeakyReLU()(encoder)\n#encoder hidden layer 2\nencoder = layers.Dense(num_features)(encoder)\nencoder = layers.BatchNormalization()(encoder)\nencoder = layers.LeakyReLU()(encoder)\n\n#bottleneck - number of nodes here will determine reduced feature dimensions\nbottleneck = layers.Dense(num_features \/\/ 2)(encoder) #we will go with 100\/2 = 50\n\n#decoder hidden layer 1 - reconstruction begins, mirrored version of encoder\ndecoder = layers.Dense(num_features)(bottleneck)\ndecoder = layers.BatchNormalization()(decoder)\ndecoder = layers.LeakyReLU()(decoder)\n#decoder hidden layer 2\ndecoder = layers.Dense(num_features * 2)(decoder)\ndecoder = layers.BatchNormalization()(decoder)\ndecoder = layers.LeakyReLU()(decoder)\noutput = layers.Dense(num_features, activation='linear')(decoder)\n\n#complete autoencoder which we will train and then use the encoder part for feature extraction\nautoencoder = keras.models.Model(inputs=visible, outputs=output)\n\nautoencoder.compile(\n    optimizer='adam', \n    loss='binary_crossentropy'\n)","78e37460":"xtrain, xval, ytrain, yval = train_test_split(train[features], train.target, \n                                              stratify=train.target, \n                                              test_size=0.3, \n                                              random_state=SEED)\n\nscaler = QuantileTransformer(output_distribution='normal', random_state=SEED)\nxtrain = scaler.fit_transform(xtrain)\nxval = scaler.transform(xval)","5e34f5c7":"gc.collect()","defc5bca":"%%time\n#autoencoder training\nhistory = autoencoder.fit(\n    xtrain, ytrain,\n    validation_data=(xval, yval),\n    batch_size=256,\n    epochs=50,\n    verbose=0\n)","c5d71565":"history_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot(title=\"Loss\")","b4830361":"del xtrain, ytrain, xval, yval","ff9f60ba":"gc.collect()","1a7a7c54":"trained_encoder = keras.models.Model(inputs=visible, outputs=bottleneck)","42081e72":"preprocessors = {\n    0: Pipeline([\n        ('Standard-Scaler', StandardScaler()),  #baseline-pipeline\n        ('MinMax-Scaler', MinMaxScaler())\n    ]),\n    \n    1: Pipeline([\n        ('Normal-Quantile-Transformer', QuantileTransformer(output_distribution='normal',\n                                                            random_state=SEED))\n    ]),\n    \n    2: Pipeline([\n        ('Robust-Scaler', RobustScaler()),\n        ('MaxAbs-Scaler', MaxAbsScaler())\n    ]),\n    \n    3: pca_pipeline1,\n    \n    4: pca_pipeline2\n}","a62d5c12":"#custom model based on number of features in different experiments\ndef get_model(input_shape):\n    model = keras.Sequential([\n                layers.BatchNormalization(input_shape=[input_shape]),\n                layers.Dense(units=64, activation='relu'),\n                layers.BatchNormalization(),\n                layers.Dropout(0.5),\n                layers.Dense(units=64, activation='relu'),\n                layers.BatchNormalization(),\n                layers.Dropout(0.5),\n                layers.Dense(units=1, activation='sigmoid')\n            ])\n    \n    return model    ","3ba7f5e1":"N_SPLITS = 6\nPATIENCE = 10\nMIN_DELTA = 0.0005\nBATCH_SIZE = 1024\nEPOCHS = 100  #will be managed by early-stopping mechanism\n\ndef custom_cross_val_predict(train, test, features, preprocessor):\n    oof_preds = {}  #out-of-fold predictions on validation set for each fold\n    test_preds = []  #predictions on complete test set using model trained in each fold\n    scores = []  #evaluation metric score for each fold\n    \n    cv_start = time.time()\n    \n    for fold in range(N_SPLITS):\n        xtrain = train[train.fold != fold].reset_index(drop=True)\n        ytrain = xtrain.target\n\n        xval = train[train.fold == fold].reset_index(drop=True)\n        yval = xval.target\n        val_idx = xval.id.values.tolist()\n        \n        #since we need to apply preprocessor for each fold depending on new xtrain\n        xtest = test[features].copy()\n        \n        #fit preprocessor on training set, use it to transform val and test sets\n        xtrain = preprocessor.fit_transform(xtrain[features])\n        xval = preprocessor.transform(xval[features])\n        xtest = preprocessor.transform(xtest)\n        \n        #Building network architecture here since we have feature extraction in\n        #our preprocessor pipeline, which will change the shape of data\n        model = get_model(xtrain.shape[1])\n        model.compile(\n            optimizer='adam', \n            loss='binary_crossentropy', \n            metrics=['AUC']\n        )\n        early_stopping = keras.callbacks.EarlyStopping(\n            patience=PATIENCE,\n            min_delta=MIN_DELTA,\n            restore_best_weights=True,\n        )\n        \n        fold_start = time.time()\n        \n        history = model.fit(\n            xtrain, ytrain,\n            validation_data=(xval, yval),\n            batch_size=BATCH_SIZE,\n            epochs=EPOCHS,\n            callbacks=[\n                early_stopping\n            ],\n            verbose=0\n        )\n        history_df = pd.DataFrame(history.history)\n        \n        #side-by-side plots for loss-function and eval-metric\n        fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(10, 3))\n        ax1.plot(history_df.loc[:, ['loss', 'val_loss']])\n        ax1.set_title('Loss')\n        ax2.plot(history_df.loc[:, ['auc', 'val_auc']])\n        ax2.set_title('AUC')\n        plt.show()\n        \n        val_preds = model.predict(xval).ravel()     \n        oof_preds.update(dict(zip(val_idx, val_preds)))\n        auc = roc_auc_score(yval, val_preds)\n        scores.append(auc)\n        \n        fold_end = time.time()\n        print(f'Fold #{fold}: AUC = {auc:.5f}\\\n                [Time: {fold_end - fold_start:.2f} secs]')\n        \n        test_preds.append(model.predict(xtest).ravel())\n        \n        #models can be saved first if needed\n        del xtrain, ytrain, xval, yval, xtest, model\n        gc.collect()\n        \n    cv_end = time.time()\n    print(f'\\nAverage AUC = {np.mean(scores):.5f} (std. dev. = {np.std(scores):.5f})')\n    print(f'[Total time: {cv_end - cv_start:.2f} secs]\\n')\n    \n    oof_preds = pd.DataFrame.from_dict(oof_preds, orient='index').reset_index()\n    test_preds = np.mean(np.column_stack(test_preds), axis=1)\n    \n    return oof_preds, test_preds","e0dacdbf":"oof_preds, test_preds = custom_cross_val_predict(train, test, \n                                                 features, \n                                                 preprocessors[0])\n\noof_preds.columns = ['id', 'oof14']\noof_preds.to_csv('oof14.csv', index=False)\n\noutput = pd.DataFrame({'id': test.id, 'target': test_preds})\noutput.to_csv('submission14.csv', index=False)","dd5ba6b8":"gc.collect()","230135d5":"oof_preds, test_preds = custom_cross_val_predict(train, test, \n                                                 features, \n                                                 preprocessors[1])\n\noof_preds.columns = ['id', 'oof15']\noof_preds.to_csv('oof15.csv', index=False)\n\noutput = pd.DataFrame({'id': test.id, 'target': test_preds})\noutput.to_csv('submission15.csv', index=False)","04d53f5b":"gc.collect()","bbe08fca":"oof_preds, test_preds = custom_cross_val_predict(train, test, \n                                                 features, \n                                                 preprocessors[2])\n\noof_preds.columns = ['id', 'oof16']\noof_preds.to_csv('oof16.csv', index=False)\n\noutput = pd.DataFrame({'id': test.id, 'target': test_preds})\noutput.to_csv('submission16.csv', index=False)","9dec7570":"gc.collect()","7f12274b":"oof_preds, test_preds = custom_cross_val_predict(train, test, \n                                                 selected_features, \n                                                 preprocessors[0])\n\noof_preds.columns = ['id', 'oof17']\noof_preds.to_csv('oof17.csv', index=False)\n\noutput = pd.DataFrame({'id': test.id, 'target': test_preds})\noutput.to_csv('submission17.csv', index=False)","3aaa67e9":"gc.collect()","29831bae":"oof_preds, test_preds = custom_cross_val_predict(train, test, \n                                                 selected_features, \n                                                 preprocessors[1])\n\noof_preds.columns = ['id', 'oof18']\noof_preds.to_csv('oof18.csv', index=False)\n\noutput = pd.DataFrame({'id': test.id, 'target': test_preds})\noutput.to_csv('submission18.csv', index=False)","8233ed4e":"gc.collect()","64ef0498":"oof_preds, test_preds = custom_cross_val_predict(train, test, \n                                                 selected_features, \n                                                 preprocessors[2])\n\noof_preds.columns = ['id', 'oof19']\noof_preds.to_csv('oof19.csv', index=False)\n\noutput = pd.DataFrame({'id': test.id, 'target': test_preds})\noutput.to_csv('submission19.csv', index=False)","e6470da2":"gc.collect()","c7db30b9":"oof_preds, test_preds = custom_cross_val_predict(train, test, \n                                                 features, \n                                                 preprocessors[3])\n\noof_preds.columns = ['id', 'oof20']\noof_preds.to_csv('oof20.csv', index=False)\n\noutput = pd.DataFrame({'id': test.id, 'target': test_preds})\noutput.to_csv('submission20.csv', index=False)","6b21c09d":"gc.collect()","6c551199":"oof_preds, test_preds = custom_cross_val_predict(train, test, \n                                                 features, \n                                                 preprocessors[4])\n\noof_preds.columns = ['id', 'oof21']\noof_preds.to_csv('oof21.csv', index=False)\n\noutput = pd.DataFrame({'id': test.id, 'target': test_preds})\noutput.to_csv('submission21.csv', index=False)","6976ac39":"oof_preds = {}  #out-of-fold predictions on validation set for each fold\ntest_preds = []  #predictions on complete test set using model trained in each fold\nscores = []  #evaluation metric score for each fold\n    \ncv_start = time.time()\n    \nfor fold in range(N_SPLITS):\n    xtrain = train[train.fold != fold].reset_index(drop=True)\n    ytrain = xtrain.target\n\n    xval = train[train.fold == fold].reset_index(drop=True)\n    yval = xval.target\n    val_idx = xval.id.values.tolist()\n        \n    #since we need to apply preprocessor for each fold depending on new xtrain\n    xtest = test[features].copy()\n    \n    #'scaler' is the same preprocessing we used before training the autoencoder\n    xtrain = scaler.fit_transform(xtrain[features])\n    xval = scaler.transform(xval[features])\n    xtest = scaler.transform(xtest)\n    \n    #compressed representation created by the encoder\n    xtrain = trained_encoder.predict(xtrain)\n    xval = trained_encoder.predict(xval)\n    xtest = trained_encoder.predict(xtest)\n        \n    model = get_model(xtrain.shape[1])\n    model.compile(\n        optimizer='adam', \n        loss='binary_crossentropy', \n        metrics=['AUC']\n    )\n    early_stopping = keras.callbacks.EarlyStopping(\n        patience=PATIENCE,\n        min_delta=MIN_DELTA,\n        restore_best_weights=True,\n    )\n        \n    fold_start = time.time()\n        \n    history = model.fit(\n        xtrain, ytrain,\n        validation_data=(xval, yval),\n        batch_size=BATCH_SIZE,\n        epochs=EPOCHS,\n        callbacks=[early_stopping],\n        verbose=0\n    )\n    history_df = pd.DataFrame(history.history)\n\n    fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(10, 3))\n    ax1.plot(history_df.loc[:, ['loss', 'val_loss']])\n    ax1.set_title('Loss')\n    ax2.plot(history_df.loc[:, ['auc', 'val_auc']])\n    ax2.set_title('AUC')\n    plt.show()\n        \n    val_preds = model.predict(xval).ravel()     \n    oof_preds.update(dict(zip(val_idx, val_preds)))\n    auc = roc_auc_score(yval, val_preds)\n    scores.append(auc)\n        \n    fold_end = time.time()\n    print(f'Fold #{fold}: AUC = {auc:.5f}\\\n            [Time: {fold_end - fold_start:.2f} secs]')\n        \n    test_preds.append(model.predict(xtest).ravel())\n        \n    #models can be saved first if needed\n    del xtrain, ytrain, xval, yval, xtest, model\n    gc.collect()\n        \ncv_end = time.time()\nprint(f'\\nAverage AUC = {np.mean(scores):.5f} (std. dev. = {np.std(scores):.5f})')\nprint(f'[Total time: {cv_end - cv_start:.2f} secs]\\n')\n    \noof_preds = pd.DataFrame.from_dict(oof_preds, orient='index').reset_index()\noof_preds.columns = ['id', 'oof22']\noof_preds.to_csv('oof22.csv', index=False)\n\ntest_preds = np.mean(np.column_stack(test_preds), axis=1)\noutput = pd.DataFrame({'id': test.id, 'target': test_preds})\noutput.to_csv('submission22.csv', index=False)","219c1b7d":"gc.collect()","8f277dc3":"!head submission16.csv","6afb5052":"It was my first time working with an autoencoder, but the result is impressive, atleast in comparison with the other preprocessing we did here. Yet to see how it holds up on the public LB but the CV-AUC here is great. Definitely something I will experiment with separately.  \nAnother thing to remember is that time is required to train the autoencoder separately first and then use it on our dataset to generate input for our training network. The 'Total time' visible here does not include autoencoder training.","0a2d6255":"**Time to submit!**","858e2dd4":"# Feature Extraction\/Projection  \n(building derived features intended to be informative and non-redundant)  \n\nSince the feature extraction procedure will transform the data and remove our original features, **we will keep it as part of our preprocessing pipeline**.","67df986a":"Contrary to my expectations, QuantileTransformer did not improve performance over the baseline-pipeline and also took more time to train. However, the **RobustScaler + MaxAbsScaler pipeline performed great** both in terms of training time and AUC.","de2f4f8c":"**Full feature set + Quantile Transformer**","eb86e85e":"I think that's amazing for having half the dimensionality of our original data. Maybe it can maintain these kind of results for even fewer components. That should be a separate experiment.","0b350789":"### 2. Autoencoder  \nA neural network which can learn a compressed representation of raw data by trying to reconstruct the data after reducing its dimensionality. Consists of two parts - Encoder and Decoder.  \n* Encoder is the restricting part of the neural network architecture where the feature set is reduced. The last hidden layer of the encoder is the 'bottleneck' which will decide the dimensions of the reduced\/extracted features. Decoder is the reconstruction part, where the reduced feature vectors are used to get back the original data.  \n* Once the complete architecture is trained, we can use the encoder half to extract a reduced feature set since we have already determined that it is a good enough representation to recreate the original data (assuming the model training shows good results).  \n\n[A more detailed explanation](https:\/\/machinelearningmastery.com\/autoencoder-for-classification\/) (also the reference for autoencoder code)","f2971259":"While creating a [neural network baseline](https:\/\/www.kaggle.com\/stiwar1\/tps-nov-21-neural-network-baseline) at the start of the competition, I directly used StandardScaler followed by MinMaxScaler to standardize and normalize the data.  \n\nFrom the scikit-learn documentation, I looked at examples of how different distributions are affected by different scalers\/transformers. Our dataset features have two kinds of distributions -  \n1. Bimodal\n2. Skewed 'Log-normal-ish'(?) with outliers  \n\nQuantileTransformer manages to handle both these types by converting them to a normal distribution, so we will add it to our preprocessing pipeline and compare it with our baseline.  \nAnother combination I wanted to experiment with was RobustScaler (which reduces the effect of outliers by using median for centering and interquartile range for scaling) and MaxAbsScaler (which uses maximum absolute value of a feature for scaling). Hence, I have added it to the preprocessors dict.","e42030ac":"**Selected feature set + Quantile Transformer**","cee9ca1d":"Our selected features have performed relatively well ([slightly better than the 26 features we selected for logistic regression](https:\/\/www.kaggle.com\/stiwar1\/feature-selection-preprocessing-experiments)).  \n\nA slight drop of around 0.015-0.02 in AUC compared to the full feature set (and a drop to the bottom of public LB).  \nIt is acceptable since we chose only 40 features and managed to train much faster (great for practical use I would say).  \n\n**I will explore more feature selection methods and update any improvements in performance.**","b692b411":"# Preprocessing pipelines","bde86aef":"Sorted variances:","ed3b0473":"That's quite a lot of chosen features with MI = 0  \nOur variance threshold was a bit arbitrary. So let's just check features chosen by our embedded estimator but with MI = 0","a2db0f71":"  \nReasons for ignoring the Variance Threshold features:  \n* there are no zero variance features.  \n* we can choose a threshold arbitrarily but the features with low variance are in a very close range so it would be like a lottery. I will try to find a rigourous method for incorporating them.","017b6c53":"**Full feature set + Encoder**  \n(here we couldn't call our *custom_cross_val_predict* function since the encoder does not have *fit* and *transform* methods. The code is copy-pasted, with the preprocessor part modified for the encoder.)","a1ca77a3":"### 1. PCA  \nDerives a linear relationship between features. The scikit-learn implementation provides an option for automatic choice of dimensionality, which we will be using.  \nPCA requires data to be scaled, so our pipeline will include StandardScaler before PCA.","724647b7":"# Imports","28e92f6d":"**Selected feature set + RobustScaler + MaxAbsScaler**","191bf516":"Ten features . We will remove them from our final set.  \n\nLet's check if some of our high MI features were ignored by the embedded estimator.","7b3c99f9":"Before comparing variance of different features, we will first bring them on the same scale. To do this, we divide each feature by its mean value.","3f4108a0":"**Finally, only 40 features! Let us see how they perform with a neural network.**","01639bf1":"Finally, we can use the Encoder as a separate model for feature extraction:","f1782672":"**Selected feature set + StandardScaler + MinMaxScaler**","e961bd9a":"Saved the mutual information scores (*mi_scores*) since it takes about 19-20 mins to run. Expand the cell below for the values.","3db10a0e":"### 2. Univariate selection  \nUsing a scoring function between a feature and target variable to determine rankings. Does not handle redundant features but can determine which features are most relevant to the target variable.  \nHere we will use Mutual Information as the scoring function.","91a879f1":"Great performance, but expected from 98 components. We did not manage to reduce the feature set much with automatic PCA so let's see how manual PCA performs (we arbitrarily chose 50 components).","39941172":"Let's check how many components remain after applying automatic PCA:","5c7ff9cb":"**Let us try to come up with a single feature set which combines the best of our feature selection methods.**","1d1b7bd6":"Some features have MI scores equal to 0, which means the target is independent of those features. We should remove them.","31570a7b":"So our embedded estimator managed to find all the high MI features.  ","e89728ac":"Training data with cross-validation folds is imported from a private dataset.  \nBut it can also be found in the data section here: [Feature Selection & Preprocessing experiments](https:\/\/www.kaggle.com\/stiwar1\/feature-selection-preprocessing-experiments)","86ae2490":"### 1. Variance Threshold  \nLow variance features have low impact on the target variable. Features below a certain variance threshold can be removed. However, it is not guaranteed that high-variance features have high correlation with the target variable.","48b4dce7":"**Any feedback for improving efficiency and readability of code would be greatly appreciated!**","30147282":"I have included the plots so that we can see the subtle differences in model training for different folds.","57386588":"### 3. SelectFromModel with embedded estimator  \nUsing a classifier we can determine feature importances and select those above a particular threshold.  \nThe classifier and its training parameters can be of your choice. Here we will use ExtraTreesClassifier (Extremely Randomized Trees).  \nIt needs to be fit on the training set **only once** to determine feature importances.","1373ab6f":"After getting great results with a [neural network baseline](https:\/\/www.kaggle.com\/stiwar1\/tps-nov-21-neural-network-baseline) and exploring feature selection methods, I decided to experiment with them together and check out the results. Additionally, I explored two feature extraction methods - PCA and Autoencoder for linear and non-linear projections of the features respectively.","e4b76162":"# Cross-validation + Inference","ff379d38":"**Full feature set + PCA Pipeline 2 (manual)**","2fe9c6b4":"# Feature Selection  \n(subset of original features)","7c8d9224":"**Full feature set + PCA Pipeline 1 (automatic)**","6853ff1f":"**Quick summary before model training:**  \n* From our feature selection methods we have a reduced set of features: *selected_features*  \n* From our feature extraction methods, we have two PCA pipelines in our *preprocessors* dict, and the Encoder which we will use separately since it requires modified code. PCA pipeline-1 will automatically determine number of components but PCA pipeline-2 and the Encoder will reduce the features to 50 components as chosen by us.  \n* We added two feature scaling pipelines in addition to the one we used in the baseline. They have special properties to handle the distributions of the features.","79d95413":"98 components...  \nLet us also do another PCA with manual input for 50 components. ","b93b8747":"I will be submitting the results obtained from our RobustScaler + MaxAbsScaler pipeline on the full feature set.\nSaving the autoencoder for another day :D","364aeb34":"**Full feature set + StandardScaler + MinMaxScaler** (baseline)","10222716":"**Full feature set + RobustScaler + MaxAbsScaler**"}}