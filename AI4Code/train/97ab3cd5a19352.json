{"cell_type":{"07ec70a3":"code","7305f30e":"code","57855b93":"code","b9c4c19d":"code","cb22b6af":"code","14713072":"code","301d2aec":"code","51d39337":"code","a4ac51e9":"code","41db6278":"code","30f7b35c":"code","a4e0604d":"code","feebf43f":"code","cf106ebc":"code","729d5289":"code","b1d2318a":"code","136f3e66":"code","5a880033":"code","c15b416a":"code","0071a943":"code","9b1e5c24":"code","08e7a931":"code","a00c66ba":"code","0a0c04ae":"code","89b418dd":"code","3dbb1357":"code","f83b0814":"code","cc35f2a4":"code","bfaa06f7":"code","b77527fd":"code","a487f499":"code","935961bb":"code","4e68704b":"code","999eed60":"markdown","27c58749":"markdown","805c6027":"markdown"},"source":{"07ec70a3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import Ridge, LinearRegression\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.base import TransformerMixin, BaseEstimator\nimport re \nimport scipy\nfrom scipy import sparse\nimport gc \nfrom IPython.display import display, HTML\nfrom pprint import pprint\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7305f30e":"df_train = pd.read_csv('\/kaggle\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv.zip')\ndf_test = pd.read_csv('\/kaggle\/input\/jigsaw-toxic-comment-classification-challenge\/test.csv.zip')\ndf_train.head(10)","57855b93":"#No null values in the dataset\ndf_train.info()","b9c4c19d":"df_train.columns","cb22b6af":"#Counts for each category\ntotal_rows = df_train.shape[0]\nfor col_name in ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']:\n    print(col_name, \"%.2f\" % (df_train[df_train[col_name]>0].shape[0]\/total_rows*100))","14713072":"#threats, identity hate and severe_toxic comments are less in the distributions. \n#Their weights need to be increased to rebalance the data. \n\ndf_train['threat'] = df_train['threat']*4\ndf_train['identity_hate'] = df_train['identity_hate']*3\ndf_train['severe_toxic'] = df_train['severe_toxic']*2","301d2aec":"#Scoring the toxicity of data by summing up  the attributes\ndf_train['toxicity_score'] = df_train[['toxic', 'severe_toxic', 'obscene', 'threat',\n       'insult', 'identity_hate']].sum(axis = 1)","51d39337":"#unique toxicity_score value\ndf_train['toxicity_score'].unique()","a4ac51e9":"#Statements that are highly toxic\ndf_train[df_train['toxicity_score']>11].head(5)","41db6278":"#Counting occurance of each type of 'toxicity_score'\ndf_train_count = df_train['toxicity_score'].value_counts().sort_index()\nplt.bar(df_train_count.index, df_train_count.values)\nplt.title(\"Count of statements with toxicity score\")","30f7b35c":"#Normalizing toxicity score\ndf_train['toxicity_score_norm'] = df_train['toxicity_score']\/df_train['toxicity_score'].max()\ndf_train['toxicity_score_norm'].value_counts().sort_index()","a4e0604d":"#Remove punctuations\nimport string\nstring.punctuation","feebf43f":"#Example to understand to check the code working\nexample_sentence = \"I love to wear 'adidas' shoes\"\nget_words = [words for words in example_sentence if words not in string.punctuation]\nprint(''.join(get_words))","cf106ebc":"def remove_punctuation(sentence):\n    get_words=[words for words in sentence if words not in string.punctuation]\n    sentences_wo_punct=''.join(get_words)\n    return sentences_wo_punct\ndf_train['text_wo_punct']=df_train['comment_text'].apply(lambda x: remove_punctuation(x))\ndf_train.head(5)","729d5289":"#Removing stop words\nimport nltk\nstopword = nltk.corpus.stopwords.words('english')\nprint(stopword[:11])","b1d2318a":"def remove_stopwords(sentence):\n    get_words=[words for words in sentence if words not in string.punctuation]\n    sentences_wo_stop_words =''.join(get_words)\n    return sentences_wo_stop_words\ndf_train['text_wo_punct_and_stop']=df_train['text_wo_punct'].apply(lambda x: remove_stopwords(x))\ndf_train['text_wo_punct_and_stop'].head(10)\n\n","136f3e66":"#Tokenazing and Lemmatizing the sentences\ntokenizer = nltk.tokenize.WhitespaceTokenizer()\nlemmatizer = nltk.stem.WordNetLemmatizer()\n\n","5a880033":"def lemmatize_sent(sentence):\n    lemmatized_words = [lemmatizer.lemmatize(words) for words in tokenizer.tokenize(sentence)]\n    sentences_lemmatized =' '.join(lemmatized_words)\n    return sentences_lemmatized","c15b416a":"df_train['text_lemmatized']=df_train['text_wo_punct_and_stop'].apply(lambda x: lemmatize_sent(x))\ndf_train['text_lemmatized'].head(10)\n","0071a943":"vec_tokens = TfidfVectorizer(ngram_range=(3,5))","9b1e5c24":"token_words = vec_tokens.fit_transform(df_train['text_lemmatized'].values)\n#vec_tokens.vocabulary_","08e7a931":"from sklearn.linear_model import LinearRegression\nfrom sklearn import metrics","a00c66ba":"#training the algorithm\nregressor = LinearRegression()  \nregressor.fit(token_words,df_train['toxicity_score_norm']) ","0a0c04ae":"y_pred = regressor.predict(X)\ny_pred.to_csv()","89b418dd":"RFC_classifier = RandomForestRegressor(n_estimators = 5, random_state = 0)\nRFC_classifier.fit(token_words,df_train['toxicity_score_norm'])","3dbb1357":"y_pred = RFC_classifier.predict(X)\ny_pred.to_csv()","f83b0814":"from tensorflow.keras.layers import Dense, Dropout, GRU, Embedding, LSTM, Bidirectional\nfrom tensorflow.keras.models import Sequential, Model, load_model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","cc35f2a4":"#Setting up hyperparameters\nvocab_size = 50000   \nmax_seq_length = 100 \nembedding_dim = 128 \nepochs = 10 \nbatch_size = 64\nlr = 0.001","bfaa06f7":"#Defining LSTM model\n\nmodel = Sequential([\n        Embedding(vocab_size, embedding_dim, input_length = max_seq_length),\n        LSTM(128, return_sequences=True),\n        Dropout(0.1),    \n        LSTM(64, return_sequences = True),\n        Dropout(0.1),\n        LSTM(32, return_sequences = False),\n        Dropout(0.1),\n        Dense(1, activation=\"relu\")\n    ])","b77527fd":"tokenizer = Tokenizer(num_words = vocab_size)\ntokenizer.fit_on_texts(df_train['text_lemmatized'].values)\ntoken_words_lstm = tokenizer.texts_to_sequences(X_train.values)\ntoken_words_lstm = pad_sequences(token_words_lstm, maxlen = max_seq_length)","a487f499":"model = lstm_model()\n        model.compile(\n            optimizer= RMSProp(learning_rate= lr),\n            loss='mean_squared_error',\n            metrics=['MSE'],\n    )","935961bb":"history = model.fit(token_words_lstm, df_train['toxicity_score_norm'], \n                batch_size = batch_size, \n                epochs = epochs)","4e68704b":"df_test['text_wo_punct']=df_test['comment_text'].apply(lambda x: remove_punctuation(x))\ndf_test['text_wo_punct_and_stop']=df_test['text_wo_punct'].apply(lambda x: remove_stopwords(x))\ndf_test['text_lemmatized']=df_test['text_wo_punct_and_stop'].apply(lambda x: lemmatize_sent(x))\n\n    \ntoken_words_lstm_test = tokenizer.texts_to_sequences(df_test['text_lemmatized'].values)\ntoken_words_lstm_test = pad_sequences(oken_words_lstm_test, maxlen = max_seq_length)\npred = model.predict(token_words_lstm_test)","999eed60":"#Usign deep learning LSTM network to improve the model R2 score ","27c58749":"**The text needs to be cleaned so as to improve model performance.**\nReferance - \nhttps:\/\/towardsdatascience.com\/nlp-in-python-data-cleaning-6313a404a470","805c6027":"**Approach#1: Using TF-IDF and Linear Regression anlays**is"}}