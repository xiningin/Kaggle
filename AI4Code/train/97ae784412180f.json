{"cell_type":{"5e715872":"code","262a4e95":"code","27cc38b5":"code","7a89f915":"code","90e80e11":"code","c682a30a":"code","c3417af2":"code","7679c586":"code","b85f37b0":"code","49ac718b":"code","60f61822":"code","ac09dccc":"code","25baf741":"code","7796498f":"code","f32be0b0":"markdown","35cfdf56":"markdown","4832802b":"markdown","254e1729":"markdown","f2aed844":"markdown","fbe10d53":"markdown","cb5e7c78":"markdown"},"source":{"5e715872":"# data analysis and wrangling\nimport pandas as pd\nimport numpy as np\n\n# visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# machine learning\nfrom sklearn import preprocessing\nmin_max_scaler = preprocessing.MinMaxScaler()\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\n\n\nimport os\nprint(os.listdir(\"..\/input\"))","262a4e95":"# Read data\ntrain = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')\n\n# Concat train & test set\ntest['Survived'] = \"0\"\ntest['train_test'] = \"test\"\ntest = test[['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked', 'train_test']]\n\ntrain['train_test'] = \"train\"\ndata = train.append(test)","27cc38b5":"# \u0417\u043e\u0440\u0447\u0438\u0433\u0434\u0438\u0439\u043d \u043d\u044d\u0440\u043d\u044d\u044d\u0441 Title \u0433\u044d\u0441\u044d\u043d \u0448\u0438\u043d\u044d \u0445\u0443\u0432\u044c\u0441\u0430\u0433\u0447 \u0433\u0430\u0440\u0433\u0430\u0436 \u0430\u0432\u0430\u0445\ndata['Title'] = data['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\nstat_min = 10 \ntitle_names = (data['Title'].value_counts() < stat_min) \ndata['Title'] = data['Title'].apply(lambda x: 'Misc' if title_names.loc[x] == True else x)\nprint(data['Title'].value_counts())\nprint(\"-\"*10)","7a89f915":"# Title \u0445\u0443\u0432\u044c\u0441\u0430\u0433\u0447\u0438\u0439\u0433 \u0442\u043e\u043e \u0440\u0443\u0443 \u0445\u04e9\u0440\u0432\u04af\u04af\u043b\u044d\u0445\nlabel = LabelEncoder()\ndata['Title_Code'] = label.fit_transform(data['Title'])\n\n# Family Size \u0433\u044d\u0441\u044d\u043d \u0448\u0438\u043d\u044d \u0445\u0443\u0432\u044c\u0441\u0430\u0433\u0447\ndata['FamilySize'] = data['SibSp'] + data['Parch'] + 1\ndata2 = data[['PassengerId', 'Survived', 'Pclass', 'Sex', 'Age', 'FamilySize',\n        'Fare', 'Embarked', 'Title_Code', 'train_test']]\n\n# Cabin \u0448\u0438\u043d\u044d \u0445\u0443\u0432\u044c\u0441\u0430\u0433\u0447\nCabin = data['Cabin']\ncabin = pd.DataFrame(Cabin.str.slice(stop = 1)) \ncabin = cabin.fillna('cabin_NA')\ndata3 = pd.concat([data2, cabin], axis=1, ignore_index=False)\n\n# One hot encoding - \u041a\u0430\u0442\u0435\u0433\u043e\u0440\u0438 \u0445\u0443\u0432\u044c\u0441\u0430\u0433\u0447\u0434\u044b\u0433 \u043a\u043e\u0434\u043b\u043e\u0445\ncat_feat = data3[['Cabin', 'Embarked', 'Sex','Pclass','Title_Code']]\nnum_feat = data3[['Age', 'Fare', 'FamilySize']]\ntrain_test = data3[['PassengerId', 'Survived','train_test']]\ncat_feat['Pclass'] = cat_feat['Pclass'].astype(object)\ncat_feat['Title_Code'] = cat_feat['Title_Code'].astype(object)\ntransformed_data = pd.get_dummies(cat_feat)\n\ntransformed_data = transformed_data[['Cabin_A', 'Cabin_B', 'Cabin_C', 'Cabin_D', 'Cabin_E', 'Cabin_F',\n       'Cabin_G', 'Cabin_T', 'Cabin_cabin_NA', 'Embarked_C', 'Embarked_Q', 'Embarked_S',\n       'Sex_male', 'Pclass_1', 'Pclass_2', 'Pclass_3', 'Title_Code_0', 'Title_Code_1', 'Title_Code_2', 'Title_Code_3',\n       'Title_Code_4']]","90e80e11":"# Scaling       \nfrom sklearn.preprocessing import MinMaxScaler\ndef min_max_scale_data(numerical_data):\n    scaler = MinMaxScaler().fit(numerical_data)\n    data = pd.DataFrame(scaler.transform(numerical_data), columns=numerical_data.columns.values, index=numerical_data.index)\n    return data\n\n# Missing Value \u043d\u04e9\u0445\u04e9\u0445\nnum_feat = num_feat.fillna(num_feat.mean())\nnum_feat2 = min_max_scale_data(num_feat)\n\ndata4 = pd.concat([num_feat2, transformed_data], axis=1, ignore_index=False)\ndata5 = pd.concat([train_test, data4], axis=1, ignore_index=False)\n\n# Training data\ntrain_F = (data5.train_test == 'train')\ntrain = data5.loc[train_F]\ntrain = train.drop(\"train_test\", axis = 1)\ntrain['Survived'] = train['Survived'].astype(int)\n\n# Test data\ntest_F = (data5.train_test == 'test')\ntest_KAGGLE = data5.loc[test_F]\ntest_KAGGLE = test_KAGGLE.drop(\"train_test\", axis = 1)","c682a30a":"# X \u0431\u043e\u043b\u043e\u043d y-\u0433 \u0441\u0430\u043b\u0433\u0430\u0445 \nModel_subset = train.drop(\"PassengerId\", axis = 1)\nX = Model_subset.drop(\"Survived\", axis=1)\ny = Model_subset[\"Survived\"].copy()\n\n# Test, train \u0445\u0443\u0432\u0430\u0430\u043b\u0442\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train,  y_test = train_test_split(X, y, test_size=0.3, random_state = 0)\n\n# RF fit \u0445\u0438\u0439\u0445\nclf = RandomForestClassifier(n_estimators=720, max_leaf_nodes=32, max_features = 16, n_jobs=-1)\nclf.fit(X_train, y_train)","c3417af2":"# \u0427\u0443\u0445\u0430\u043b \u043d\u04e9\u043b\u04e9\u04e9\u0442\u044d\u0439 \u0445\u0443\u0432\u044c\u0441\u0430\u0433\u0447\u0438\u0434\nimportances = pd.DataFrame({\"feature\": X_train.columns, \"importance\": clf.feature_importances_})\nimportances.head()   ","7679c586":"sns.barplot(data=importances.sort_values(\"importance\", ascending=False).head(10), x=\"importance\", y=\"feature\")","b85f37b0":"# pred on training data\ny_train_pred = clf.predict(X_train)\naccuracy_score(y_train, y_train_pred)","49ac718b":"# pred on test set unseen data\ny_test_pred = clf.predict(X_test)\naccuracy_score(y_test, y_test_pred)","60f61822":"print (y_test_pred [0:5])\nprint (y_test [0:5])","ac09dccc":"sns.heatmap(confusion_matrix(y_test, y_test_pred),annot=True,fmt='2.0f')","25baf741":"# Submission\ntest_org = test_KAGGLE[['Age', 'Fare', 'FamilySize', 'Cabin_A', 'Cabin_B', 'Cabin_C', \n       'Cabin_D', 'Cabin_E', 'Cabin_F', 'Cabin_G', 'Cabin_T', 'Cabin_cabin_NA',\n       'Embarked_C', 'Embarked_Q', 'Embarked_S', 'Sex_male', 'Pclass_1',\n       'Pclass_2', 'Pclass_3', 'Title_Code_0', 'Title_Code_1',\n       'Title_Code_2', 'Title_Code_3', 'Title_Code_4']]\ny_pred_subm = clf.predict(test_org)\n\ny_pred_subm = pd.DataFrame(y_pred_subm)\ny_pred_subm.columns = ['pred']\nprediction = pd.concat([y_pred_subm, test], axis=1, ignore_index=False)\nprediction = prediction[['PassengerId', 'pred']]\nprediction.columns = ['PassengerId','Survived']\nprediction.head()","7796498f":"prediction.to_csv('.\/submission.csv', index=False)","f32be0b0":"## Feature Importance","35cfdf56":"## Prediction","4832802b":"# Titanic Survival Prediction","254e1729":"## Data","f2aed844":"## Data Preprocessing & Feature Engineering","fbe10d53":"## Model Training","cb5e7c78":"## Evaluation"}}