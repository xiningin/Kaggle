{"cell_type":{"596e3d7d":"code","16c7f3b0":"code","1f053d53":"code","db214c87":"code","5d3d7214":"code","69c6cab4":"code","d33fdd48":"code","424df57c":"code","d65fad46":"code","3ea15cc5":"code","b735fb12":"code","022c6fe0":"code","8091ed3a":"code","2afee4a9":"code","9d6e9d48":"code","f13cff10":"code","5c2b988f":"code","5139dc4c":"code","c5f3f952":"code","69508a68":"code","933f89d1":"code","73da9960":"code","70d2d0dd":"code","e512ae61":"markdown","80d2edf4":"markdown","d5ad33e1":"markdown","444cae42":"markdown","3974d016":"markdown","c51c3e0c":"markdown","2ad8536e":"markdown","6b4533fb":"markdown","2c2bdf4a":"markdown","fe79986e":"markdown","e628d37e":"markdown","9e1075c9":"markdown","a440d81a":"markdown","d18636fb":"markdown","8ffdcede":"markdown","8eeb06b9":"markdown","4ad1b70b":"markdown","0a4439eb":"markdown","3815c186":"markdown","8d971b95":"markdown"},"source":{"596e3d7d":"!pip install -q efficientnet_pytorch\nfrom efficientnet_pytorch import EfficientNet\nfrom albumentations.pytorch import ToTensor\nfrom albumentations import (\n    Compose, HorizontalFlip, CLAHE, HueSaturationValue,\n    RandomBrightness, RandomContrast, RandomGamma, OneOf, Resize,\n    ToFloat, ShiftScaleRotate, GridDistortion, ElasticTransform, JpegCompression, HueSaturationValue,\n    RGBShift, RandomBrightness, RandomContrast, Blur, MotionBlur, MedianBlur, GaussNoise, CenterCrop,\n    IAAAdditiveGaussianNoise, GaussNoise, OpticalDistortion, RandomSizedCrop, VerticalFlip\n)\nimport os\nimport torch\nimport pandas as pd\nimport numpy as np\nimport random\nimport torch.nn as nn\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom glob import glob\nimport torchvision\nfrom torch.utils.data import Dataset\nimport time\nfrom tqdm.notebook import tqdm\n# from tqdm import tqdm\nfrom sklearn import metrics\nimport cv2\nimport gc\nimport torch.nn.functional as F","16c7f3b0":"seed = 42\nprint(f'setting everything to seed {seed}')\nrandom.seed(seed)\nos.environ['PYTHONHASHSEED'] = str(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True","1f053d53":"data_dir = '..\/input\/alaska2-image-steganalysis'\nfolder_names = ['JMiPOD\/', 'JUNIWARD\/', 'UERD\/']\nclass_names = ['Normal', 'JMiPOD_75', 'JMiPOD_90', 'JMiPOD_95', \n               'JUNIWARD_75', 'JUNIWARD_90', 'JUNIWARD_95',\n                'UERD_75', 'UERD_90', 'UERD_95']\nclass_labels = { name: i for i, name in enumerate(class_names)}","db214c87":"train_df = pd.read_csv('..\/input\/alaska2trainvalsplit\/alaska2_train_df.csv')\nval_df = pd.read_csv('..\/input\/alaska2trainvalsplit\/alaska2_val_df.csv')","5d3d7214":"class Alaska2Dataset(Dataset):\n\n    def __init__(self, df, augmentations=None):\n\n        self.data = df\n        self.augment = augmentations\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        fn, label = self.data.loc[idx]\n        im = cv2.imread(fn)[:, :, ::-1]\n        if self.augment:\n            # Apply transformations\n            im = self.augment(image=im)\n        return im, label\n\n\nimg_size = 512\nAUGMENTATIONS_TRAIN = Compose([\n    VerticalFlip(p=0.5),\n    HorizontalFlip(p=0.5),\n    ToFloat(max_value=255),\n    ToTensor()\n], p=1)\n\n\nAUGMENTATIONS_TEST = Compose([\n    ToFloat(max_value=255),\n    ToTensor()\n], p=1)\n","69c6cab4":"class Net(nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n        self.model = EfficientNet.from_pretrained('efficientnet-b0')\n        # 1280 is the number of neurons in last layer. is diff for diff. architecture\n        self.dense_output = nn.Linear(1280, num_classes)\n\n    def forward(self, x):\n        feat = self.model.extract_features(x)\n        feat = F.avg_pool2d(feat, feat.size()[2:]).reshape(-1, 1280)\n        return self.dense_output(feat)","d33fdd48":"!ls ..\/input\/alaska2-efficientnet-trained-model-weights","424df57c":"models = os.listdir('..\/input\/alaska2-efficientnet-trained-model-weights\/')\nmodels ","d65fad46":"tpu_models = [fn for fn in models if 'model_effnet' in fn]\ntpu_models","3ea15cc5":"gpu_models =  [fn for fn in models if 'efficientnetb0_lb' in fn]\ngpu_models","b735fb12":"# we will not load these here as the TPU weights provided do not help in the stacking","022c6fe0":"loaded_gpu_models = []\nfor fn in gpu_models:\n    device = 'cuda'\n    model = Net(num_classes=len(class_labels)).to(device)\n    model.load_state_dict(torch.load(('..\/input\/alaska2-efficientnet-trained-model-weights\/' + fn)))\n    loaded_gpu_models.append(model)","8091ed3a":"# # Inference\nclass Alaska2TestDataset(Dataset):\n\n    def __init__(self, df, augmentations=None):\n\n        self.data = df\n        self.augment = augmentations\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        fn = self.data.loc[idx][0]\n        im = cv2.imread(fn)[:, :, ::-1]\n\n        if self.augment:\n            # Apply transformations\n            im = self.augment(image=im)\n\n        return im\n\n\ntest_filenames = sorted(glob(f\"{data_dir}\/Test\/*.jpg\"))\ntest_df = pd.DataFrame({'ImageFileName': list(\n    test_filenames)}, columns=['ImageFileName'])\n\nbatch_size = 16\nnum_workers = 4\ntest_dataset = Alaska2TestDataset(test_df, augmentations=AUGMENTATIONS_TEST)\ntest_loader = torch.utils.data.DataLoader(test_dataset,\n                                          batch_size=batch_size,\n                                          num_workers=num_workers,\n                                          shuffle=False,\n                                          drop_last=False)","2afee4a9":"test_df['Id'] = test_df['ImageFileName'].apply(lambda x: x.split(os.sep)[-1])\n\nfor k, model in enumerate(loaded_gpu_models):\n    model.eval()\n\n    preds = []\n    tk0 = tqdm(test_loader)\n    with torch.no_grad():\n        for i, im in enumerate(tk0):\n            inputs = im[\"image\"].to(device)\n            # flip vertical\n            im = inputs.flip(2)\n            outputs = model(im)\n            # fliplr\n            im = inputs.flip(3)\n            outputs = (0.25*outputs + 0.25*model(im))\n            outputs = (outputs + 0.5*model(inputs))        \n            preds.extend(F.softmax(outputs, 1).cpu().numpy())\n\n    preds = np.array(preds)\n    labels = preds.argmax(1)\n    new_preds = np.zeros((len(preds),))\n    new_preds[labels != 0] = preds[labels != 0, 1:].sum(1)\n    new_preds[labels == 0] = 1 - preds[labels == 0, 0]\n    \n    test_df['Label' + str(k)] = new_preds\n\ntest_df = test_df.drop('ImageFileName', axis=1)","9d6e9d48":"test_df.head()","f13cff10":"ncol = test_df.shape[1]\ntest_df.iloc[:,1:ncol].corr()","5c2b988f":"corr = test_df.iloc[:,1:10].corr()\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0, square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","5139dc4c":"test_df['max'] = test_df.iloc[:, 1:ncol].max(axis=1)\ntest_df['min'] = test_df.iloc[:, 1:ncol].min(axis=1)\ntest_df['mean'] = test_df.iloc[:, 1:ncol].mean(axis=1)\ntest_df['median'] = test_df.iloc[:, 1:ncol].median(axis=1)","c5f3f952":"cutoff_lo = 0.3\ncutoff_hi = 0.7","69508a68":"test_df['Label'] = test_df['mean']\ntest_df[['Id', 'Label']].to_csv('stack_mean.csv', index=False, float_format='%.6f')\ntest_df['Label'] = test_df['median']\ntest_df[['Id', 'Label']].to_csv('stack_median.csv', index=False, float_format='%.6f')","933f89d1":"test_df['Label'] = np.where(np.all(test_df.iloc[:,1:ncol] > cutoff_lo, axis=1), 1, \n                                    np.where(np.all(test_df.iloc[:,1:ncol] < cutoff_hi, axis=1),\n                                             0, test_df['median']))\ntest_df[['Id', 'Label']].to_csv('stack_pushout_median.csv', index=False, float_format='%.6f')","73da9960":"test_df['Label'] = np.where(np.all(test_df.iloc[:,1:ncol] > cutoff_lo, axis=1), \n                                    test_df['max'], \n                                    np.where(np.all(test_df.iloc[:,1:ncol] < cutoff_hi, axis=1),\n                                             test_df['min'], \n                                             test_df['mean']))\ntest_df[['Id', 'Label']].to_csv('stack_minmax_mean.csv', index=False, float_format='%.6f')","70d2d0dd":"test_df['Label'] = np.where(np.all(test_df.iloc[:,1:ncol] > cutoff_lo, axis=1), \n                                    test_df['max'], \n                                    np.where(np.all(test_df.iloc[:,1:ncol] < cutoff_hi, axis=1),\n                                             test_df['min'], \n                                             test_df['median']))\ntest_df[['Id', 'Label']].to_csv('stack_minmax_median.csv',  index=False, float_format='%.6f')","e512ae61":"# Load GPU Models","80d2edf4":"## All Model Weights","d5ad33e1":"# Load Libraries","444cae42":"## Quod erat demonstrandum (Q.E.D.)","3974d016":"# CNN Model for multiclass classification","c51c3e0c":"# Inference on Each Model","2ad8536e":"# Seed everything","6b4533fb":"# Model Weights","2c2bdf4a":"# Acknowledgements\n\n- Many thanks to Siddhartha for publishing his 'Alaska2 CNN Multiclass Classifier' notebook at https:\/\/www.kaggle.com\/meaninglesslives\/alaska2-cnn-multiclass-classifier\n- The inference pipeline and training for EfficientNetb0 with GPU in this notebook was done with Siddhartha's notebook above\n- TPU-trained EfficientNetb0 to b7 weights are from my private notebooks, built-upon xhlulu's notebook at https:\/\/www.kaggle.com\/xhlulu\/alaska2-efficientnet-on-tpus\n- For training\/inference on TPU-trained models in TensorFlow, you may refer to xhlulu's notebook above\n- For TPU blending, you may refer to my other notebook at https:\/\/www.kaggle.com\/khoongweihao\/alaska2-blending-efficientnets-on-tpus","fe79986e":"## TPU-Trained Model Weights\n\nI'm not stacking my TPU-trained model weights here as their LB scores are between 0.750 and 0.790+. Feel free to include your own weights here to stack with GPU ones.","e628d37e":"# Check Correlations","9e1075c9":"# Some Tips\n\n- The weights for GPU have a 'lb' suffix in their filename. This reflects the LB scores I obtained for the weights respectively\n- TPU LB scores are not included\n- How I obtained higher LB scores with Siddhartha's notebook above:\n    - modified training parameters like epochs, learning rate, lr_scheduler params, etc\n    - due to GPU runtime limit, weights were repeatedly loaded and 'transfer-learnt', while varying training parameters","a440d81a":"# Overview\n\nThis notebook provides a baseline stacking framework with TPU and GPU models. I did not include my best weights here as there is no free lunch in this world. An **important point** to note is that the TPU weights are trained with TensorFlow wheres the GPU ones are with PyTorch!\n\nThe stacking methods shown here are:\n- mean\n- median\n- min-max mean\n- min-max median\n- pushout-median\n\nMy public GPU and TPU weights for EfficientNetb0 to b7 can be found here:https:\/\/www.kaggle.com\/khoongweihao\/alaska2-efficientnet-trained-model-weights. The weights will be updated periodically to reflect my progress in the competition.","d18636fb":"# Stacking (Mean\/Median\/Minmax\/Pushout)","8ffdcede":"## GPU-Trained Model Weights","8eeb06b9":"# Load TPU Models","4ad1b70b":"# Create Inference Dataset","0a4439eb":"## This notebook will be updated periodically with better private\/public submissions!\n## Hope this will help you out and happy Kaggling! :)","3815c186":"# Create dataset for training and Validation","8d971b95":"# Pytorch Dataset"}}