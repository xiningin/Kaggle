{"cell_type":{"6a426077":"code","305459c1":"code","b9ac6e77":"code","0a9c1c41":"code","96b53b8c":"code","228b9737":"code","dcc80fa2":"code","7f41a433":"code","b474d5c3":"code","8498eede":"code","87076810":"code","5539aba6":"code","699edbdc":"code","d1fb0e40":"code","f7fd3018":"code","092f2881":"code","33a36bf6":"code","82eec2b4":"code","019750bb":"code","1b635580":"code","b7fc7d17":"code","0bc4114d":"code","7f52c737":"code","a2e63855":"markdown","05761d6a":"markdown","a69f7f14":"markdown","510cd7f0":"markdown","509bb19b":"markdown"},"source":{"6a426077":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')","305459c1":"train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","b9ac6e77":"# Missing features\nmissing = train.isnull().sum()\nmissing = missing[missing > 0]\nmissing.sort_values(inplace=True)\nmissing.plot.bar()","0a9c1c41":"# Function to handle missing values in a df\ndef data_cleaning(df):\n    df_clean = df.copy()\n    \n    # More than 50% missing values\n    df_clean['PoolQC'] = df_clean.PoolQC.isnull().astype(int)\n    df_clean['MiscFeature'] = df_clean.MiscFeature.isnull().astype(int)\n    df_clean.Alley.replace([np.nan, 'Grvl', 'Pave'], [0, 1, 2], inplace=True)\n    df_clean['Fence'] = df_clean.Fence.isnull().astype(int)\n    df_clean.FireplaceQu.replace([np.nan, 'Po', 'Fa', 'TA', 'Gd', 'Ex'], \n                              [0, 1, 2, 3, 4, 5], inplace=True)\n    \n    # LotFrontage - log + linear relationship + outlier\n    df_clean.drop(df_clean.loc[df_clean.LotFrontage > 300].index, axis=0, inplace=True)\n    df_clean.reset_index(drop=True, inplace=True)\n    df_clean.LotFrontage.fillna(df_clean.LotFrontage.mean(), inplace=True)\n\n    # Missing Garage values means the house does not have a garage\n    df_clean['HasGarage'] = df_clean.GarageType.isnull().astype(int)\n\n    # GarageType - Categorical\n    df_clean.GarageType.replace([np.nan, 'CarPort', 'Detchd', 'Basment', '2Types', 'Attchd', 'BuiltIn'], \n                              [0, 1, 2, 3, 4, 5, 6], inplace=True)\n\n    # GarageYrBlt - Garage was never built. log + linear relationship\n    df_clean.GarageYrBlt.fillna(round(df_clean.GarageYrBlt.max()), inplace=True)\n\n    # GarageType - Categorical\n    df_clean.GarageFinish.replace([np.nan, 'Unf', 'RFn', 'Fin'], \n                              [0, 1, 2, 3], inplace=True)\n\n    # GarageQual - Categorical\n    df_clean.GarageQual.replace([np.nan, 'Po', 'Fa', 'TA', 'Gd', 'Ex'], \n                              [0, 1, 2, 3, 4, 5], inplace=True)\n\n    # GarageCond - Duplicate of GarageQual\n    df_clean.drop(['GarageCond'], axis=1, inplace=True)\n\n    # Missing Basement values means the house does not have a basement\n    df_clean['HasBsmt'] = df_clean.BsmtQual.isnull().astype(int)\n\n    # BsmtQual[Height] - Categorical\n    df_clean.BsmtQual.replace([np.nan, 'Po', 'Fa', 'TA', 'Gd', 'Ex'], \n                              [0, 1, 2, 3, 4, 5], inplace=True)\n\n    # BsmtCond - Categorical\n    df_clean.BsmtCond.replace([np.nan, 'Po', 'Fa', 'TA', 'Gd', 'Ex'], \n                              [0, 1, 2, 3, 4, 5], inplace=True)\n\n    # BsmtExposure - Categorical\n    df_clean.BsmtExposure.replace([np.nan, 'No', 'Mn', 'Av', 'Gd'], \n                              [0, 1, 2, 3, 4], inplace=True)\n\n\n    # BsmtFinType1 - Categorical\n    df_clean.BsmtFinType1.replace([np.nan, 'Unf', 'LwQ', 'Rec', 'BLQ', 'ALQ', 'GLQ'], \n                              [0, 1, 2, 3, 4, 5, 6], inplace=True)\n\n    # BsmtFinType2 - Categorical\n    df_clean.BsmtFinType2.replace([np.nan, 'Unf', 'LwQ', 'Rec', 'BLQ', 'ALQ', 'GLQ'], \n                              [0, 1, 2, 3, 4, 5, 6], inplace=True)\n\n    # MasVnrType - Categorical\n    df_clean.MasVnrType.replace([np.nan, 'None', 'BrkCmn', 'BrkFace', 'Stone'], \n                              [0, 0, 1, 2, 3], inplace=True)\n\n    # MasVnrArea - Remove\n    df_clean.drop(['MasVnrArea'], axis=1, inplace=True)\n\n    # Electrical - Only 1 missing value. Fill with most common SBrkr\n    df_clean.Electrical.fillna('SBrkr', inplace=True)\n    \n    return df_clean","96b53b8c":"# Train\ntrain_cleaned = data_cleaning(train)\nmissing_val_count_by_column = train_cleaned.isnull().sum()\nprint(missing_val_count_by_column[missing_val_count_by_column > 0])\n\n# Test\ntest_cleaned = data_cleaning(test)\nmissing_val_count_by_column = test_cleaned.isnull().sum()\nprint(missing_val_count_by_column[missing_val_count_by_column > 0])","228b9737":"test_cleaned.fillna(test_cleaned.mode().iloc[0], inplace=True)\nmissing_val_count_by_column = test_cleaned.isnull().sum()\nprint(missing_val_count_by_column[missing_val_count_by_column > 0])","dcc80fa2":"# Heatmap\nplt.figure(dpi=100, figsize=(30, 12))\nsns.heatmap(train_cleaned.corr(),annot=True,cmap='RdYlGn',linewidths=0.2)","7f41a433":"# Save cleaned datasets\ntrain_cleaned.to_csv('house_train.csv', index=False)\ntest_cleaned.to_csv('house_test.csv', index=False)\n\n# Kfold\nnumber_of_folds = 5\nfrom sklearn.model_selection import KFold\n\n# Add a column for fold number\ntrain_cleaned[\"kfold\"] = -1\n\n# Split the data into folds\nkf = KFold(n_splits=number_of_folds, shuffle=True, random_state=42)\nfor fold, (train_indices, valid_indices) in enumerate(kf.split(X=train_cleaned)):\n    train_cleaned.loc[valid_indices, \"kfold\"] = fold\n\n# Save the new train dataset\ntrain_cleaned.to_csv(\"house_5folds.csv\", index=False)","b474d5c3":"# Load datasets\ntrain = pd.read_csv('.\/house_train.csv')\ntest = pd.read_csv('.\/house_test.csv')\n\n# List of categorial features\ncat_features = [col for col in train.columns if train[col].dtypes == 'object']\nprint(len(cat_features), cat_features)\n\n# List of numerical features\nnum_features = [col for col in train.columns if train[col].dtypes != 'object']\n# Remove id and target\nnum_features = [i for i in num_features if i not in ['Id', 'SalePrice']]\nprint(len(num_features), num_features)","8498eede":"test[cat_features]","87076810":"from sklearn.preprocessing import OrdinalEncoder\n\nordinal_encoder = OrdinalEncoder()\ntrain[cat_features] = ordinal_encoder.fit_transform(train[cat_features])\ntest[cat_features] = ordinal_encoder.transform(test[cat_features])","5539aba6":"import matplotlib.pyplot as plt\nfrom sklearn.feature_selection import mutual_info_regression\n\ny = train['SalePrice']\nX = train.drop(['Id', 'SalePrice'], axis=1)\n\n# Calculate MI Scores\nmi_scores = mutual_info_regression(X, y)\nmi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\nscores = mi_scores.sort_values(ascending=False)\n\n# Plot MI scores\nplt.figure(dpi=100, figsize=(30, 12))\nscores = scores.sort_values(ascending=True)\nwidth = np.arange(len(scores))\nticks = list(scores.index)\nplt.barh(width, scores)\nplt.yticks(width, ticks)\nplt.title(\"Mutual Information Scores\")","699edbdc":"scores.loc[scores > 0.1]","d1fb0e40":"from sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_log_error\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nimport warnings\nwarnings.filterwarnings('ignore')","f7fd3018":"def fi(model_fi):\n    # Feature importances\n    df = pd.DataFrame()\n    df[\"Feature\"] = X.columns\n    # Extracting feature importances from the trained model\n    df[\"Importance\"] = model_fi \/ model_fi.sum()\n    # Sorting the dataframe by feature importance\n    df.sort_values(\"Importance\", axis=0, ascending=False, inplace=True)\n\n    fig, ax = plt.subplots(figsize=(40, 20))\n    bars = ax.barh(df[\"Feature\"], df[\"Importance\"], height=0.4,\n                   color=\"mediumorchid\", edgecolor=\"black\")\n    ax.set_title(\"Feature importances\", fontsize=30, pad=15)\n    ax.set_ylabel(\"Feature name\", fontsize=20, labelpad=15)\n    ax.set_xlabel(\"Feature importance\", fontsize=20, labelpad=15)\n    ax.set_yticks(df[\"Feature\"])\n    ax.set_yticklabels(df[\"Feature\"], fontsize=15)\n    ax.tick_params(axis=\"x\", labelsize=15)\n    ax.grid(axis=\"x\")\n    # Adding labels on top\n    ax2 = ax.secondary_xaxis('top')\n    ax2.set_xlabel(\"Feature importance\", fontsize=20, labelpad=15)\n    ax2.tick_params(axis=\"x\", labelsize=15)\n\n    # Inverting y axis direction so the values are decreasing\n    plt.gca().invert_yaxis()","092f2881":"def model_predict(model):\n    splits = 5\n    kf = KFold(n_splits=splits, shuffle=True, random_state=8)\n\n    model_fi = 0\n    oof_rmsle = []\n    \n    for fold, (train_indices, valid_indices) in enumerate(kf.split(X)):\n        # Divide train and validation data using folds\n        X_train, X_valid = X.iloc[train_indices], X.iloc[valid_indices]\n        y_train, y_valid = y.iloc[train_indices], y.iloc[valid_indices]\n        \n        # Fit the model\n        model.fit(X_train, y_train, \n                  early_stopping_rounds=300,\n                  eval_set=[(X_valid, y_valid)],\n                  verbose=False)\n\n        valid_preds = model.predict(X_valid)\n        msle = mean_squared_log_error(y_valid, valid_preds)\n        oof_rmsle.append(np.sqrt(msle))\n        print(fold, np.sqrt(msle))\n        \n        # Getting mean feature importances (i.e. devided by number of splits)\n        model_fi += model.feature_importances_ \/ splits\n\n    print('Mean RMSLE:', np.mean(oof_rmsle), 'STD:', np.std(oof_rmsle))\n    fi(model_fi)","33a36bf6":"%%time\n# Simple model without GPU\nmodel = XGBRegressor(random_state=64)\n\n# Get CV score\nmodel_predict(model)","82eec2b4":"%%time\n# Simple model without GPU\nmodel = LGBMRegressor(random_state=64)\n\n# Get CV score\nmodel_predict(model)","019750bb":"%%time\n# Simple model without GPU\nmodel = CatBoostRegressor(random_state=64)\n\n# Get CV score\nmodel_predict(model)","1b635580":"from sklearn.preprocessing import StandardScaler\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom sklearn.model_selection import train_test_split","b7fc7d17":"# Standard Scaling\nscaler = StandardScaler()\nX = pd.DataFrame(scaler.fit_transform(X))\nX_test = pd.DataFrame(scaler.transform(test.drop(['Id'], axis=1)))","0bc4114d":"X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)","7f52c737":"model = keras.Sequential([\n    layers.Dense(1024, activation='relu', input_shape=[79]),\n    layers.Dropout(0.3),\n    layers.BatchNormalization(),\n    layers.Dense(1024, activation='relu'),\n    layers.Dropout(0.3),\n    layers.BatchNormalization(),\n    layers.Dense(1024, activation='relu'),\n    layers.Dropout(0.3),\n    layers.BatchNormalization(),\n    layers.Dense(1),\n])\n\nearly_stopping = EarlyStopping(\n    min_delta=0.0001, # minimium amount of change to count as an improvement\n    patience=20, # how many epochs to wait before stopping\n    restore_best_weights=True,\n)\n\nmodel.compile(\n    optimizer='adam',\n    loss='msle',\n)\n\nhistory = model.fit(\n    X_train, y_train,\n    validation_data=(X_valid, y_valid),\n    batch_size=128,\n    epochs=1000,\n    callbacks=[early_stopping], # put your callbacks in a list\n    verbose=0,  # turn off training log\n)\n\nhistory_df = pd.DataFrame(history.history)\n# Start the plot at epoch 5\nhistory_df.loc[5:, ['loss', 'val_loss']].plot()\n\nprint(\"Best Validation Loss: {:0.4f}\".format(np.sqrt(history_df['val_loss'].min())))","a2e63855":"No missing values in train dataset. Test has some additonal missing data. Fill them with the most frequent values present in each column.","05761d6a":"## Feature Importance + Boosting models","a69f7f14":"## Ordinal Encoding","510cd7f0":"## Mutual Information","509bb19b":"## Bonus: Deep Learning model"}}