{"cell_type":{"7347736d":"code","86e161b0":"code","3236ac63":"code","16bb0c46":"code","d9fce614":"code","88071830":"code","b16d7745":"code","b0b51d82":"code","88f6f749":"code","55d2f0ae":"code","460bd428":"code","a1ce91c5":"code","1260606f":"code","1ed9516b":"code","df50e628":"code","5704a7d9":"code","3a954878":"code","189cac49":"code","e138fdfa":"markdown","2e9b3d7d":"markdown","03cf9938":"markdown","45129a08":"markdown","172ceba6":"markdown","a7cf79e1":"markdown","19f8db16":"markdown","99039cef":"markdown","4fd1860b":"markdown"},"source":{"7347736d":"# necessary imports\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport tensorflow as tf\nfrom tensorflow import keras\n","86e161b0":"# loading data\n\nfashion_mnist = tf.keras.datasets.fashion_mnist\n(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n\n#scaling data to range between 0 to 1 (unit normalization)\n\nX_train = X_train.astype('float32') \/ 255\nX_test = X_test.astype('float32') \/ 255\n","3236ac63":"plt.figure()\nplt.imshow(X_train[0])","16bb0c46":"# shape of data\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)\n\n\n","d9fce614":"from sklearn.model_selection import train_test_split\nX_train, X_valid, y_train, y_valid = train_test_split(X_train,y_train, test_size=0.2, random_state=30)","88071830":"model1 = tf.keras.Sequential([\n    tf.keras.layers.Flatten(input_shape=(28, 28)),\n    tf.keras.layers.Dense(180, activation = 'relu'),#input_layer\n    tf.keras.layers.Dense(180, activation = 'relu'),#hidden_layer\n    tf.keras.layers.Dense(100, activation = 'relu'),\n    tf.keras.layers.Dense(10, activation = 'softmax'),#output_layer\n])\n\nmodel1.compile(optimizer='adam',\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n# fitting model\nmodel_history = model1.fit(X_train, y_train, validation_data=(X_valid,y_valid),epochs = 20)","b16d7745":"plt.figure(figsize = (12, 6))\n\ntrain_loss = model_history.history['accuracy']\nval_loss = model_history.history['val_accuracy'] \nepoch = range(1,21)\nsns.lineplot(epoch, train_loss, label = 'Training accuracy')\nsns.lineplot(epoch, val_loss, label = 'Validation accuracy')\nplt.title('Training and Validation Accuracy\\n')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend(loc = 'best')\nplt.show()","b0b51d82":"plt.figure(figsize = (12, 6))\nplt.style.use('fivethirtyeight')\n\ntrain_loss = model_history.history['loss']\nval_loss = model_history.history['val_loss'] \nepoch = range(1, 21)\nsns.lineplot(epoch, train_loss, label = 'Training Loss')\nsns.lineplot(epoch, val_loss, label = 'Validation Loss')\nplt.title('Training and Validation Loss\\n')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend(loc = 'best')\nplt.show()","88f6f749":"\nvalues = [1e-2, 1e-3, 1e-4]\ntrain_list, test_list = list(), list()\n\n\nfor i in values:\n    model12 = tf.keras.Sequential([\n    tf.keras.layers.Flatten(input_shape=(28, 28)),\n    tf.keras.layers.Dense(180, activation = 'relu', kernel_regularizer=tf.keras.regularizers.L1(i)),#input_layer\n\n    tf.keras.layers.Dense(180, activation = 'relu', kernel_regularizer=tf.keras.regularizers.L1(i)),#hidden_layer\n \n    tf.keras.layers.Dense(100, activation = 'relu', kernel_regularizer=tf.keras.regularizers.L1(i)),\n   \n    tf.keras.layers.Dense(10, activation = 'softmax', kernel_regularizer=tf.keras.regularizers.L1(i)),#output_layer\n    ])\n    model12.compile(optimizer='Adam',\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n\n    model_history = model12.fit(X_train, y_train, epochs = 25,verbose=0)\n    _, train_acc = model12.evaluate(X_train, y_train, verbose=0)\n    _, test_acc = model12.evaluate(X_test, y_test, verbose=0)\n    print('Param: %f, Train: %.3f, Test: %.3f' % (i, train_acc, test_acc))\n    train_list.append(train_acc)\n    test_list.append(test_acc)\n","55d2f0ae":"# plot train and test accuracy with respect to L2 values\nplt.semilogx(values, train_list, label='train', marker='o')\nplt.semilogx(values, test_list, label='test', marker='o')\nplt.legend()\nplt.show()","460bd428":"dropout_ratio = [0.1,0.2,0.3]\ntrain_dropout_list, test_dropout_list = list(), list()\n\n\nfor i in dropout_ratio:\n    model2 = tf.keras.Sequential([\n    tf.keras.layers.Flatten(input_shape=(28, 28)),\n    tf.keras.layers.Dense(180, activation = 'relu'),#input_layer\n    tf.keras.layers.Dropout(i),\n    tf.keras.layers.Dense(180, activation = 'relu'),#hidden_layer\n    tf.keras.layers.Dropout(i),\n    tf.keras.layers.Dense(100, activation = 'relu'),\n    tf.keras.layers.Dropout(i),\n    tf.keras.layers.Dense(10, activation = 'softmax'),#output_layer\n    ])\n    model2.compile(optimizer='Adam',\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n\n    model_history = model2.fit(X_train, y_train, epochs = 25,verbose=0)\n    _, train_acc = model2.evaluate(X_train, y_train, verbose=0)\n    _, test_acc = model2.evaluate(X_test, y_test, verbose=0)\n    print('dropout_ratio: %f, Train: %.3f, Test: %.3f' % (i, train_acc, test_acc))\n    train_dropout_list.append(train_acc)\n    test_dropout_list.append(test_acc)\n    \n\n","a1ce91c5":"# plot train and test accuracy with respect to dropout values\nplt.semilogx(dropout_ratio,train_dropout_list, label='train', marker='o')\nplt.semilogx(dropout_ratio,test_dropout_list, label='test', marker='o')\nplt.legend()\nplt.show()","1260606f":"train_size=[15000, 20000, 25000, 30000, 35000, 40000]\ntrain_size_list, test_size_list = list(), list()\n\nfor i in train_size:\n    model3 = tf.keras.Sequential([\n    tf.keras.layers.Flatten(input_shape=(28, 28)),\n    tf.keras.layers.Dense(180, activation = 'relu'),#input_layer\n    tf.keras.layers.Dropout(0.3),\n    tf.keras.layers.Dense(180, activation = 'relu'),#hidden_layer\n    tf.keras.layers.Dropout(0.3),\n    tf.keras.layers.Dense(100, activation = 'relu'),\n    tf.keras.layers.Dropout(0.3),\n    tf.keras.layers.Dense(10, activation = 'softmax'),#output_layer\n    ])\n    model3.compile(optimizer='Adam',\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n\n    model_history = model3.fit(X_train[:i], y_train[:i], epochs = 25,verbose=0)\n    _, train_acc = model3.evaluate(X_train[:i], y_train[:i], verbose=0)\n    _, test_acc = model3.evaluate(X_test, y_test, verbose=0)\n    print('train_size: %.1f, Train: %.3f, Test: %.3f' % (i, train_acc, test_acc))\n    train_size_list.append(train_acc)\n    test_size_list.append(test_acc)","1ed9516b":"# plot different train size with respect to test accuracy\nplt.plot(train_size,test_size_list, label='test', marker='o')\nplt.title('test accuracy vs different train size')\nplt.xlabel('Different train sizes')\nplt.ylabel('Test accuracy ')\nplt.legend()\nplt.show()\n\n","df50e628":"import numpy as np\nX_train = np.expand_dims(X_train, axis=-1)\nX_test = np.expand_dims(X_test, axis=-1)","5704a7d9":"#regularisation experiment\nvalues = [1e-2, 1e-3, 1e-4]\ntrain_list, test_list = list(), list()\n\nfor i in values:\n    \n    model = tf.keras.Sequential()\n\n    model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=2, padding='same', activation='relu', input_shape=(28,28,1),kernel_regularizer=tf.keras.regularizers.L1(i))) \n    model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n\n    model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=2, padding='same', activation='relu',kernel_regularizer=tf.keras.regularizers.L1(i)))\n    model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n\n\n    model.add(tf.keras.layers.Flatten())\n    model.add(tf.keras.layers.Dense(256, activation='relu',kernel_regularizer=tf.keras.regularizers.L1(i)))\n    model.add(tf.keras.layers.Dense(10, activation='softmax'))\n\n    #compile our model\n    model.compile(loss='sparse_categorical_crossentropy',\n             optimizer='adam',\n             metrics=['accuracy'])\n\n    #fit our model\n    history = model.fit(X_train,y_train,epochs=10,verbose=0)\n    \n    \n    _, train_acc = model.evaluate(X_train, y_train, verbose=0)\n    _, test_acc = model.evaluate(X_test, y_test, verbose=0)\n    print('lambda: %f,, Train: %.3f, Test: %.3f' % (i, train_acc, test_acc))\n    train_list.append(train_acc)\n    test_list.append(test_acc)\n\n\n\n","3a954878":"#Learning curve experiment\ntrain_size=[15000, 20000, 25000, 30000, 35000, 40000]\ntrain_size_list2, test_size_list2 = list(), list()\n\nfor i in train_size:\n    model = tf.keras.Sequential()\n\n    model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=2, padding='same', activation='relu', input_shape=(28,28,1))) \n    model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n    model.add(tf.keras.layers.Dropout(0.3))\n    model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=2, padding='same', activation='relu'))\n    model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n    model.add(tf.keras.layers.Dropout(0.3))\n\n    model.add(tf.keras.layers.Flatten())\n    model.add(tf.keras.layers.Dense(256, activation='relu'))\n    model.add(tf.keras.layers.Dropout(0.5))\n    model.add(tf.keras.layers.Dense(10, activation='softmax'))\n\n    #compile our model\n    model.compile(optimizer='Adam',\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n\n    model_history = model.fit(X_train[:i], y_train[:i], epochs = 25,verbose=0)\n    _, train_acc = model.evaluate(X_train[:i], y_train[:i], verbose=0)\n    _, test_acc = model.evaluate(X_test, y_test, verbose=0)\n    print('train_size: %.1f, Train: %.3f, Test: %.3f' % (i, train_acc, test_acc))\n    train_size_list2.append(train_acc)\n    test_size_list2.append(test_acc)","189cac49":"plt.plot(train_size,test_size_list, label='ANN_test', marker='o')\nplt.plot(train_size,test_size_list2, label='CNN_test', marker='o')\nplt.title('test accuracy vs different train size')\nplt.xlabel('Different train sizes')\nplt.ylabel('Test accuracy ')\nplt.legend()\nplt.show()","e138fdfa":"**For CNN**\n\n1. lambda: 0.010000,, Train: 0.100, Test: 0.100\n1. lambda: 0.001000,, Train: 0.856, Test: 0.846\n1. lambda: 0.000100,, Train: 0.913, Test: 0.891","2e9b3d7d":"# CNN","03cf9938":"# For dropout","45129a08":"# ANN","172ceba6":"AS you can see CNN gives better permance in image processing as expected(which is explained in theoretical part as well)","a7cf79e1":"# Different trian size","19f8db16":"**For ANN**\n\n1. lambda: 0.010000, Train: 0.475, Test: 0.473\n1. lambda: 0.001000, Train: 0.849, Test: 0.833\n1. lambda: 0.000100, Train: 0.899, Test: 0.867\n\n","99039cef":"**Conclusion:** \n\nAfter evaluting test performance CNN gives higher accuracy than the ANN model, as you can see above figure. \nMoreover, see if size of data set increase then the accuracy is also increase , it means more data gives better accuracy or perfomance\n","4fd1860b":"# L2 regularisation"}}