{"cell_type":{"08d79cfa":"code","f6f3b403":"code","e28db1e0":"code","e220a179":"code","03193c3a":"code","a8e4ba66":"code","cfa7a28e":"code","9aab4275":"code","e62e41ac":"code","2695c4c8":"code","18b9a945":"code","ac15bd93":"code","d08a949f":"code","df49cf9b":"code","62c9dd6a":"code","fb5593a5":"code","ebfc4a67":"code","a0e698e0":"code","3ba7c571":"code","ce877dc3":"code","5e0e2d42":"code","ee39e59b":"code","66762618":"code","1f09fb43":"code","8d9557ea":"code","a71b781a":"code","199ecdcd":"code","f0bcdb01":"code","b5b9d9b8":"code","3a553ba6":"code","d3736358":"code","0c727785":"code","7cdef6c8":"code","6428bdc9":"code","14e1dbb0":"code","24cb1add":"code","93bce4f9":"code","81bb79a6":"code","ecf80206":"code","4f0185ea":"code","ad997e5a":"code","dff96157":"code","01e3bba7":"code","c6d0c33c":"code","97db4bee":"code","15405324":"code","693c4d0e":"code","a4d1a9f4":"markdown","da9b09df":"markdown","57c8d43f":"markdown","4381837a":"markdown","42b0f861":"markdown","1c42f839":"markdown","2e90439d":"markdown","5897bac9":"markdown","d6abf676":"markdown","385f92c6":"markdown","fce1c3a4":"markdown","837feab8":"markdown","c0c6e32e":"markdown","5f427cc5":"markdown","358450e6":"markdown","574df610":"markdown","24c1448d":"markdown","30c5d509":"markdown","d844458e":"markdown","c57b28e5":"markdown","e885c349":"markdown","e346a177":"markdown","8177d3b0":"markdown"},"source":{"08d79cfa":"import numpy as np\nimport pandas as pd\nimport os\nimport random\nimport math\nimport gc\nimport glob\n\nfrom sklearn.model_selection import KFold, GroupKFold, train_test_split\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_squared_error\n\n\nimport operator\nimport typing as tp\nfrom logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\nfrom functools import partial\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom tqdm.notebook import tqdm\n\nimport pydicom\n\nprint(os.listdir('\/kaggle\/input\/osic-pulmonary-fibrosis-progression\/'))\n\nfrom time import time, strftime, gmtime\n\nstart = time()\n#print(start)\n\nimport datetime\nprint(str(datetime.datetime.now()))","f6f3b403":"def seed_everything(seed=777):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)","e28db1e0":"seed = 2019\nseed_everything(seed)","e220a179":"input_path = '\/kaggle\/input\/osic-pulmonary-fibrosis-progression\/'","03193c3a":"train = pd.read_csv(input_path + 'train.csv')\ntrain","a8e4ba66":"test = pd.read_csv(input_path + 'test.csv')\ntest","cfa7a28e":"sample = pd.read_csv(input_path + 'sample_submission.csv')\nsample","9aab4275":"print('Number of patients in the train set: ', train['Patient'].nunique())","e62e41ac":"plt.figure(figsize = (8, 8))\n\nplt.title('Age')\nsns.distplot(train['Age'])","2695c4c8":"plt.figure(figsize = (8, 8))\n\nplt.title('Weeks')\nsns.distplot(train['Weeks'])","18b9a945":"plt.figure(figsize = (8, 8))\n\nplt.title('FVC')\nsns.distplot(train['FVC'], bins = int(1 + math.log2(train['Patient'].nunique())))\n#use Sturgess's formula to find the appropriate number of classes in the histogram","ac15bd93":"print('Max. FVC: ', train['FVC'].max())\nprint('Min. FVC: ', train['FVC'].min())\nprint('Mean FVC: ', train['FVC'].mean())","d08a949f":"train['SmokingStatus'].value_counts()","df49cf9b":"fig, ax = plt.subplots(1, 2, figsize = (15, 15))\n#plt.figure(figsize = (10, 10))\nplt.suptitle('Smoking Status')\nsns.countplot(train['SmokingStatus'], ax = ax[0])\n\nlbls, freqs = np.unique(train['SmokingStatus'].values, return_counts = True)\n\nax[1].pie(freqs, labels = lbls, autopct = '%1.1f%%', shadow = False, startangle = 90)\nplt.show()","62c9dd6a":"fig, ax = plt.subplots(1, 2, figsize = (15, 15))\n\nplt.suptitle('Sex')\nsns.countplot(train['Sex'], ax = ax[0])\n\nlbls, freqs = np.unique(train['Sex'].values, return_counts = True)\n\nax[1].pie(freqs, labels = lbls, autopct = '%1.1f%%', shadow = False, startangle = 90)\nplt.show()","fb5593a5":"plt.figure(figsize = (8, 8))\nplt.title('Smoking Status by Sex')\nsns.countplot(train['SmokingStatus'], hue = train['Sex'])","ebfc4a67":"plt.figure(figsize = (8, 8))\nplt.title('Smoking Status by Sex')\nsns.countplot(train['Age'], hue = train['SmokingStatus'])","a0e698e0":"train.isnull().sum()","3ba7c571":"plt.figure(figsize = (8, 8))\nsns.heatmap(train.corr(), annot = True)","ce877dc3":"print('Correlation coeff between Age and FVC is: ', train.corr()['Age']['FVC'])","5e0e2d42":"plt.figure(figsize = (8, 8))\nsns.scatterplot(data = train, x = 'Age', y = 'FVC')","ee39e59b":"#Corr for smokers\ntrain_cs = train.loc[train['SmokingStatus'] == 'Currently smokes']\n\nplt.figure(figsize = (8, 8))\nsns.scatterplot(data = train_cs, x = 'Age', y = 'FVC')\n\nprint('Correlation coeff between Age and FVC (Current Smokers) is: ', train_cs.corr()['Age']['FVC'])","66762618":"train_dcm = glob.glob(input_path + 'train\/*\/*')\ntest_dcm = glob.glob(input_path + 'test\/*\/*')\n\nprint('Num of train dicom: ', len(train_dcm))\nprint('Num of test dicom: ', len(test_dcm))","1f09fb43":"num_imgs_pid = [len(os.listdir(input_path + 'train\/' + path)) for path in os.listdir(input_path + 'train\/')]\nplt.figure(figsize = (8, 8))\nplt.hist(num_imgs_pid)\nplt.ylabel('Number of patients')\nplt.xlabel('DICOM files')\nplt.title('DICOM Images per patient')\nplt.show()\nprint('Max. number of dicom images per patient: ', max(num_imgs_pid))\nprint('Min. number of dicom images per patient: ', min(num_imgs_pid))\nprint('Mean. number of dicom images per patient: ', np.mean(num_imgs_pid))","8d9557ea":"pydicom.dcmread(train_dcm[10])","a71b781a":"random_dcm_train = np.random.choice(train_dcm, 6)\nfig, ax = plt.subplots(2, 3, figsize = (15, 10))\n\nax = ax.ravel()\n\nfor i, file in enumerate(random_dcm_train):\n    img = pydicom.dcmread(file)\n    img = img.pixel_array\n    # Since the scanning equipment is cylindrical in nature and image output is square,\n    # we set the out-of-scan pixels to 0\n    img[img == -2000] = 0\n    ax[i].imshow(img, cmap = plt.cm.bone)\nplt.show()","199ecdcd":"fig, ax = plt.subplots(2, 3, figsize = (15, 10))\n\nax = ax.ravel()\n\nfor i, file in enumerate(random_dcm_train):\n    img = pydicom.dcmread(file)\n    img = img.pixel_array\n    img[img == -2000] = 0\n    ax[i].imshow(img, cmap = plt.cm.Reds)\nplt.show()","f0bcdb01":"def plot_dicom_images(pid = None, df = None, feature = None, dcm_path = None):\n    fig, ax = plt.subplots(2, 3, figsize = (15, 10))\n    ax = ax.ravel()\n    for i in range(len(ax)):\n        dcm = pydicom.dcmread(input_path + '\/train\/' + pid + '\/' + str(i + 1) + '.dcm')\n        img = dcm.pixel_array\n        img[img == -2000] = 0\n        ax[i].imshow(img, cmap = plt.cm.bone)\n    return None","b5b9d9b8":"plot_dicom_images(pid = np.random.choice(train['Patient'].values, 1)[0], df = train)","3a553ba6":"train['Patient_Week'] = train['Patient'].astype(str) + '_' + train['Weeks'].astype(str)\ntrain","d3736358":"train_out = pd.DataFrame()\n\ntrain_grp = train.groupby('Patient')\n\nfor _, df_out in tqdm(train_grp):\n    df_pid = pd.DataFrame()\n    for wk, temp in df_out.groupby('Weeks'):\n        rename_cols = {'Weeks': 'base_Week', 'FVC': 'base_FVC', 'Percent': 'base_Percent', 'Age': 'base_Age'}\n        temp = temp.drop(columns = 'Patient_Week').rename(columns = rename_cols)\n        drop_cols = ['Age', 'Sex', 'SmokingStatus', 'Percent']\n        _df_pid = df_out.drop(columns = drop_cols).rename(columns = {'Weeks': 'predict_Week'}).merge(temp, on = 'Patient')\n        _df_pid['Week_passed'] = _df_pid['predict_Week'] - _df_pid['base_Week']\n        df_pid = pd.concat([df_pid, _df_pid])\n    train_out = pd.concat([train_out, df_pid])\n\ntrain = train_out[train_out['Week_passed'] != 0].reset_index(drop = True)\ntrain","0c727785":"test = test.rename(columns = {'Weeks': 'base_Week', 'FVC': 'base_FVC', 'Percent': 'base_Percent', 'Age': 'base_Age'})\n\nsample['Patient'] = sample['Patient_Week'].apply(lambda x: x.split('_')[0])\nsample['predict_Week'] = sample['Patient_Week'].apply(lambda x: x.split('_')[1]).astype(int)\ntest = sample.drop(columns = ['FVC', 'Confidence']).merge(test, on = 'Patient')\ntest['Week_passed'] = test['predict_Week'] - test['base_Week']\ntest","7cdef6c8":"folds = train[['Patient_Week', 'Patient', 'FVC']].copy()\ngkf = GroupKFold(n_splits = 5)\ngroups = folds['Patient'].values\nfor i, (trn_idx, val_idx) in enumerate(gkf.split(folds, folds['FVC'], groups)):\n    folds.loc[val_idx, 'fold'] = int(i)\nfolds['fold'] = folds['fold'].astype(int)\nfolds","6428bdc9":"import lightgbm as lgbm\n\nparams = {\n    'num_class': 2,\n    'metric': 'None',\n    'boosting_type': 'gbdt',\n    'learning_rate': 5e-02,\n    'seed': seed,\n    \"subsample\": 0.4,\n    \"subsample_freq\": 1,\n    'max_depth': 1,\n    'verbosity': -1,\n}","14e1dbb0":"#https:\/\/www.kaggle.com\/ttahara\/osic-baseline-lgbm-with-custom-metric\nclass OSICLossForLGBM:\n    \"\"\"\n    Custom Loss for LightGBM.\n    \n    * Objective: return grad & hess of NLL of gaussian\n    * Evaluation: return competition metric\n    \"\"\"\n    \n    def __init__(self, epsilon: float=1) -> None:\n        \"\"\"Initialize.\"\"\"\n        self.name = \"osic_loss\"\n        self.n_class = 2  # FVC & Confidence\n        self.epsilon = epsilon\n    \n    def __call__(self, preds: np.ndarray, labels: np.ndarray, weight: tp.Optional[np.ndarray]=None) -> float:\n        \"\"\"Calc loss.\"\"\"\n        sigma_clip = np.maximum(preds[:, 1], 70)\n        Delta = np.minimum(np.abs(preds[:, 0] - labels), 1000)\n        loss_by_sample = - np.sqrt(2) * Delta \/ sigma_clip - np.log(np.sqrt(2) * sigma_clip)\n        loss = np.average(loss_by_sample, weight)\n        \n        return loss\n    \n    def _calc_grad_and_hess(\n        self, preds: np.ndarray, labels: np.ndarray, weight: tp.Optional[np.ndarray]=None\n    ) -> tp.Tuple[np.ndarray]:\n        \"\"\"Calc Grad and Hess\"\"\"\n        mu = preds[:, 0]\n        sigma = preds[:, 1]\n        \n        sigma_t = np.log(1 + np.exp(sigma))\n        grad_sigma_t = 1 \/ (1 + np.exp(- sigma))\n        hess_sigma_t = grad_sigma_t * (1 - grad_sigma_t)\n        \n        grad = np.zeros_like(preds)\n        hess = np.zeros_like(preds)\n        grad[:, 0] = - (labels - mu) \/ sigma_t ** 2\n        hess[:, 0] = 1 \/ sigma_t ** 2\n        \n        tmp = ((labels - mu) \/ sigma_t) ** 2\n        grad[:, 1] = 1 \/ sigma_t * (1 - tmp) * grad_sigma_t\n        hess[:, 1] = (\n            - 1 \/ sigma_t ** 2 * (1 - 3 * tmp) * grad_sigma_t ** 2\n            + 1 \/ sigma_t * (1 - tmp) * hess_sigma_t\n        )\n        if weight is not None:\n            grad = grad * weight[:, None]\n            hess = hess * weight[:, None]\n        return grad, hess\n    \n    def return_loss(self, preds: np.ndarray, data: lgbm.Dataset) -> tp.Tuple[str, float, bool]:\n        \"\"\"Return Loss for lightgbm\"\"\"\n        labels = data.get_label()\n        weight = data.get_weight()\n        n_example = len(labels)\n        \n        # # reshape preds: (n_class * n_example,) => (n_class, n_example) =>  (n_example, n_class)\n        preds = preds.reshape(self.n_class, n_example).T\n        # # calc loss\n        loss = self(preds, labels, weight)\n        \n        return self.name, loss, True\n    \n    def return_grad_and_hess(self, preds: np.ndarray, data: lgbm.Dataset) -> tp.Tuple[np.ndarray]:\n        \"\"\"Return Grad and Hess for lightgbm\"\"\"\n        labels = data.get_label()\n        weight = data.get_weight()\n        n_example = len(labels)\n        \n        # # reshape preds: (n_class * n_example,) => (n_class, n_example) =>  (n_example, n_class)\n        preds = preds.reshape(self.n_class, n_example).T\n        # # calc grad and hess.\n        grad, hess =  self._calc_grad_and_hess(preds, labels, weight)\n\n        # # reshape grad, hess: (n_example, n_class) => (n_class, n_example) => (n_class * n_example,) \n        grad = grad.T.reshape(n_example * self.n_class)\n        hess = hess.T.reshape(n_example * self.n_class)\n        \n        return grad, hess","24cb1add":"train.dtypes","93bce4f9":"cat_features = ['Sex', 'SmokingStatus']\nnum_features = [col for col in train.columns if (train[col].dtype != 'object') & (col not in cat_features)]\nprint(cat_features, num_features)\nfeatures = cat_features + num_features\nfeatures = [col for col in features if col not in ['Patient_Week', 'FVC', 'predict_Week', 'base_Week']]\nfeatures","81bb79a6":"import category_encoders as catenc\n\ntest['FVC'] = np.nan\n\nordenc = catenc.OrdinalEncoder(cols = cat_features, handle_unknown = 'impute')\nordenc.fit(train)\ntrain = ordenc.transform(train)\ntest = ordenc.transform(test)\nprint('Categorical features encoded..')","ecf80206":"nb_splits = 5\noof = np.zeros((len(train), 2))\npredictions = np.zeros((len(test), 2))\nfeature_importance_df = pd.DataFrame()\nosic_loss = OSICLossForLGBM()\n\nfor n_folds in range(nb_splits):\n    print()\n    print('Fold No: ', n_folds + 1)\n    trn_idx = folds[folds['fold'] != n_folds].index\n    val_idx = folds[folds['fold'] == n_folds].index\n    #print(trn_idx, val_idx)\n    ltrain = lgbm.Dataset(train.iloc[trn_idx][features], label = train.iloc[trn_idx]['FVC'])\n    lvalid = lgbm.Dataset(train.iloc[val_idx][features], label = train.iloc[val_idx]['FVC'])\n    \n    clf = lgbm.train(params, ltrain, \n                    num_boost_round = 10000, \n                    verbose_eval = 100, \n                    early_stopping_rounds = 400, \n                    valid_sets = [ltrain, lvalid], \n                    fobj = osic_loss.return_grad_and_hess,\n                    feval = osic_loss.return_loss\n                    )\n    oof[val_idx] = clf.predict(train.iloc[val_idx][features], num_iteration = clf.best_iteration)\n    \n    # RMSE\n    print(\"CV RMSE score: {:<8.5f}\".format(np.sqrt(mean_squared_error(train['FVC'], oof[:, 0]))))\n    # Metric\n    print(\"CV Metric: {:<8.5f}\".format(osic_loss(oof, train['FVC'])))\n    \n    fold_imp_df = pd.DataFrame()\n    fold_imp_df['feature'] = train[features].columns\n    fold_imp_df['importance'] = clf.feature_importance()\n    fold_imp_df['fold'] = n_folds + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_imp_df], axis = 0)\n    \n    predictions += clf.predict(test[features], num_iteration = clf.best_iteration) \/ nb_splits","4f0185ea":"cols = (feature_importance_df[['feature', 'importance']]\n        .groupby('feature')\n        .mean()\n        .sort_values(by = 'importance', ascending = False).index)\nbest_features = feature_importance_df.loc[feature_importance_df.feature.isin(cols)]\n\nplt.figure(figsize = (10,14))\nsns.barplot(x = 'importance', y = 'feature', data = best_features.sort_values(by = 'importance', ascending = False))\nplt.title('LightGBM Features')\nplt.tight_layout()","ad997e5a":"predictions, oof","dff96157":"train[\"FVC_pred\"] = oof[:, 0]\ntrain[\"Confidence\"] = oof[:, 1]\ntest[\"FVC_pred\"] = predictions[:, 0]\ntest[\"Confidence\"] = predictions[:, 1]","01e3bba7":"sub = pd.read_csv(input_path + 'sample_submission.csv')\nsub","c6d0c33c":"submission = sub.drop(columns = ['FVC', 'Confidence']).merge(test[['Patient_Week', 'FVC_pred', 'Confidence']], \n                                                           on = 'Patient_Week')\nsubmission.columns = sub.columns\nsubmission.to_csv('.\/submission.csv', index = False)\nsubmission.head()","97db4bee":"sns.distplot(submission['FVC'])","15405324":"sns.distplot(submission['Confidence'])","693c4d0e":"finish = time()\nprint(strftime(\"%H:%M:%S\", gmtime(finish - start)))","a4d1a9f4":"- CT Scan images of patients are in *dcm* format available in train\/test folders\n- FVC is in ml","da9b09df":"__Sex Countplot__","57c8d43f":"__Weeks Distribution__","4381837a":"__Correlation between Age and FVC__","42b0f861":"__Check for NaNs__","1c42f839":"- Looks like there is no correlation between Age and FVC","2e90439d":"- No Correlation","5897bac9":"__Smoking Status by Sex__","d6abf676":"__Competetion Goal__\n\n- The aim of this competition is to predict a patient\u2019s severity of decline in lung function based on a CT scan of their lungs, **FVC and its Confidence**\n\n- Lung function is assessed based on output from a spirometer, which measures the **forced vital capacity (FVC)**, i.e. the volume of air exhaled.\n\n- In the training set, we are provided with an anonymized, baseline CT scan and the entire history of FVC measurements.\n\n- In the test set, we are provided with a baseline CT scan and only the initial FVC measurement. We are asked to predict the **final three FVC measurements for each patient, as well as a confidence value in your prediction**.","385f92c6":"__Competetion Metric__\n\nThis competition is evaluated on a modified version of the Laplace Log Likelihood","fce1c3a4":"__Baseline Model__","837feab8":"__Age Distribution__","c0c6e32e":"__Dicom Images by Patient ID__","5f427cc5":"__Forced Vital Capacity (FVC) Distribution__","358450e6":"__Code Requirements__\n\n- No TPU available for making submissions but can be used for training.\n- GPU <= 4 hours\n- No Internet access\n- Submission file name *submission.csv*","574df610":"__Plot dcm Images__","24c1448d":"__Preparing folds for GroupK Fold__","30c5d509":"__Smoking Status__","d844458e":"__Change CMAP__","c57b28e5":"__Random Train dicom Images__","e885c349":"__Ready the testset__","e346a177":"__Preparing input train set__","8177d3b0":"__Sample dicom data__"}}