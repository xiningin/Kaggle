{"cell_type":{"4af6c99c":"code","5f93bb51":"code","71ea6d08":"code","d3fafb64":"code","fa886ccb":"code","8bbf29fc":"code","41987b4d":"markdown","eeb7d41b":"markdown","1bc6b723":"markdown","a24ca334":"markdown","57f22b24":"markdown","3887df85":"markdown","a49815ca":"markdown","7c1ec287":"markdown","e050573c":"markdown","c3542e1e":"markdown"},"source":{"4af6c99c":"from __future__ import print_function, division\nfrom builtins import range, input\n# Note: you may need to update your version of future\n# sudo pip install -U future\n\nfrom keras.models import Model\nfrom tensorflow.keras import layers\n\nfrom tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input, decode_predictions\nfrom keras.preprocessing import image\n\nimport numpy as np\nimport scipy as sp\nimport matplotlib.pyplot as plt\n\nfrom glob import glob\nimport itertools\nimport warnings\nwarnings.filterwarnings(\"ignore\")","5f93bb51":"image_path = '..\/input\/imagenetmini-1000\/imagenet-mini\/val'\nimage_files = glob(image_path +  '\/*\/*.JP*G')","71ea6d08":"# look at an image for fun\nplt.imshow(image.load_img(np.random.choice(image_files)));\n","d3fafb64":"# add preprocessing layer to the front of VGG\nresnet = ResNet50(input_shape=(224, 224, 3), weights='imagenet', include_top=True)\n\n# view the structure of the model\n# if you want to confirm we need activation_49\nresnet.summary()","fa886ccb":"# make a model to get output before flatten\nactivation_layer = resnet.get_layer('conv5_block3_out')\n\n# create a model object\nmodel = Model(inputs=resnet.input, outputs=activation_layer.output)","8bbf29fc":"# get the feature map weights\nfinal_dense = resnet.get_layer('predictions')\nW = final_dense.get_weights()[0]\n\n\n#while True:\ni = 0\nfor i in range(10):\n  img = image.load_img(np.random.choice(image_files), target_size=(224, 224))\n  x = preprocess_input(np.expand_dims(img, 0))\n  fmaps = model.predict(x)[0] # 7 x 7 x 2048\n\n  # get predicted class\n  probs = resnet.predict(x)\n  classnames = decode_predictions(probs)[0]\n  print(classnames)\n  classname = classnames[0][1]\n  pred = np.argmax(probs[0])\n\n  # get the 2048 weights for the relevant class\n  w = W[:, pred]\n\n  # \"dot\" w with fmaps\n  cam = fmaps.dot(w)\n\n  # upsample to 224 x 224\n  # 7 x 32 = 224\n  cam = sp.ndimage.zoom(cam, (32, 32), order=1)\n\n  plt.subplot(1,2,1)\n  plt.imshow(img, alpha=0.8)\n  plt.imshow(cam, cmap='jet', alpha=0.5)\n  plt.subplot(1,2,2)\n  plt.imshow(img)\n  plt.title(classname)\n  plt.show()\n  \n  \n\n  #ans = input(\"Continue? (Y\/n)\")\n  #if ans and ans[0].lower() == 'n':\n    #break\n\n\n","41987b4d":"# Introduction","eeb7d41b":"![](https:\/\/github.com\/yvtsanlevy\/KaggleProjects\/blob\/main\/images\/%E2%80%8F%E2%80%8F%D7%9C%D7%9B%D7%99%D7%93%D7%94.JPG?raw=true)","1bc6b723":"What we can see that we've overlaid a sort of heat map over the original image and we can see that the area around the main object that is being identified is hotter than the rest.\n\nSo in some sense, we've done object localization.\n\nWe figured out where the object is in the image and not just what the object is, which is what classification would tell us.\n\nto make a picture like this, we don't have to do any task other than just playing classification.","a24ca334":"# Build the model","57f22b24":"# Import ibrary","3887df85":"![](https:\/\/github.com\/yvtsanlevy\/KaggleProjects\/blob\/main\/images\/2%E2%80%8F%E2%80%8F%D7%9C%D7%9B%D7%99%D7%93%D7%94.JPG?raw=true)","a49815ca":"# get the image files","7c1ec287":"**What is class activation maps?**\n\nThe best way to understand is by looking at the following picture\n\nis exactly the kind of picture we're going to learn how to produce.","e050573c":"The key point is thinking about what happens to an image as it travels through the neural network.\n\nAs an image passes through a CNN, it shrinks or more accurately, the image dimensions shrink, but the number of features increases and we can also imagine that we're using a relu activation function. So all the values are either positive or zero.\n\nAfter the final convolution block, we can imagine what happens is a global max or average pooling so that you only have one value per feature.\n\nAnd then we pass each feature through a logistic regression to predict the upper level.\n\n\nSuppose we did find some feature in an image and its feature value was positive since we did a global max pool, that feature must have been found somewhere in the image.\n\nAnd before we did the max pool, we would have known the location of that feature.\nSo if we imagine what the picture looks like before doing Max Pool, we can imagine that it's just a bunch of zeros and some positive values indicating where in the image that feature was found.","c3542e1e":"[B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba. Learning Deep Features for Discriminative Localization. CVPR'16 (arXiv:1512.04150, 2015).](https:\/\/arxiv.org\/abs\/1512.04150)"}}