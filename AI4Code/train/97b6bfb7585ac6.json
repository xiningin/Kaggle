{"cell_type":{"4d9484a7":"code","cb6fcbc6":"code","7fd2f760":"code","a1346bbc":"code","4d7f34c1":"code","afeac279":"code","87bc4ba4":"code","ddaa7a94":"code","4ea85da7":"code","acc61deb":"code","b8bb24b5":"code","a02ebc70":"code","04a73c77":"code","af2af82e":"code","02e14e45":"code","6c4973fa":"code","b01d4a0a":"code","e6ae5ba2":"code","15c932ea":"code","df6ecfc1":"code","9ea37f82":"code","b57f4047":"code","a4ea4d00":"code","85417bf4":"code","3a4a60d6":"code","92608245":"code","83683df0":"code","a2468911":"code","65cc250d":"code","02c87636":"code","5f86e337":"code","31db3d40":"code","691c9bb9":"code","65d4d3a7":"code","c9ece364":"code","e42e0929":"code","688154b4":"code","30f6a906":"code","4a1f6f0c":"code","e433a201":"code","bf935af3":"code","92e83813":"code","393dc841":"code","18934c82":"code","3c642435":"code","2eb005b4":"code","a21e1f10":"markdown","19413f5a":"markdown","4015cc92":"markdown","d3491e65":"markdown","c611787f":"markdown","0b104982":"markdown","798e69a8":"markdown","e7fa4d83":"markdown","f06a8691":"markdown","f8323e9c":"markdown","d66f10f9":"markdown","9868aa51":"markdown","c87cd9eb":"markdown","6144f9fd":"markdown","8a4bcaf0":"markdown","786fd73d":"markdown","d284357e":"markdown","5596c88a":"markdown","904bc8b2":"markdown"},"source":{"4d9484a7":"import pandas as pd","cb6fcbc6":"#Charger les donn\u00e9es du fichier covid_faq.csv sous forme de dataframe:\ndata = pd.read_csv('..\/input\/covid19-related-faqs\/covid_faq.csv', delimiter=',')","7fd2f760":"data.head()","a1346bbc":"data.shape","4d7f34c1":"data.dtypes","afeac279":"#changer la cha\u00eene de caract\u00e8res en minuscules\ndata['questions']=data['questions'].str.lower()","87bc4ba4":"#supprimer les valeurs manquantes\ndata.dropna(inplace=True)","ddaa7a94":"#supprimer les doublons\ndata =data.drop_duplicates()","4ea85da7":"data.questions=data.questions.str.replace('covid-19','coronavirus')","acc61deb":"#telecharger la fonction word_tokenize de la Biblioth\u00e8ques nltk.tokenize\nfrom nltk.tokenize import word_tokenize","b8bb24b5":"exemple=\"I am a king!\"\nexemple_tok = word_tokenize(exemple)\nexemple_tok","a02ebc70":"#assembler les mots par un separateur \" \" pour former une cha\u00eene de caract\u00e8re\n\" \".join(exemple_tok)","04a73c77":"import string\nstring.punctuation","af2af82e":"def remove_punct(text):\n    \n    return resultat","02e14e45":"remove_punct(exemple)","6c4973fa":"exemple","b01d4a0a":"exemple=remove_punct(exemple)\nexemple","e6ae5ba2":"data['questions']=data.questions.apply(remove_punct)","15c932ea":"from nltk.corpus import stopwords\n#T\u00e9l\u00e8charger les stopwords\nstop=set(stopwords.words('english'))\nstop","df6ecfc1":"def remove_stopword(text):\n    \n    return resultat","9ea37f82":"remove_stopword('i am a strong king')","b57f4047":"data.questions","a4ea4d00":"import nltk\nfrom nltk.stem import WordNetLemmatizer\nlemmatizer=WordNetLemmatizer()","85417bf4":"lemmatizer.lemmatize(\"kids\")","3a4a60d6":"lemmatizer.lemmatize('facilities')","92608245":"def lemm(text):\n    \n    return resultat","83683df0":"data.questions=data.questions.apply(lemm)","a2468911":"corpus=data['questions'].values\ncorpus","65cc250d":"from sklearn.feature_extraction.text import CountVectorizer\nbw_vect = CountVectorizer()\n# tokenize et construire le vocabulaire\nbw_fit=bw_vect.fit(corpus)\n# vectoriser les mots\nbw_corpus = bw_fit.transform(corpus)","02c87636":"bw_corpus.shape","5f86e337":"bw_fit.get_feature_names()","31db3d40":"bw_corpus.toarray()","691c9bb9":"cv_data=pd.DataFrame(bw_corpus.toarray(),columns=bw_fit.get_feature_names())\ncv_data","65d4d3a7":"from sklearn.feature_extraction.text import TfidfVectorizer","c9ece364":"#Initialiser les param\u00e8tres du vectoriseur\ntf_vect = TfidfVectorizer(max_features=5000)\n#Apprendre le vocabulaire du vectoriseur bas\u00e9 sur le param\u00e8tre initialis\u00e9\ntfidf_fit=tf_vect.fit(corpus)\n#Vectoriser le corpus\ntfidf_corpus= tfidf_fit.transform(corpus)","e42e0929":"tfidf_fit.get_feature_names()","688154b4":"tfidf_data=pd.DataFrame(tfidf_corpus.toarray(),columns=tfidf_fit.get_feature_names())\ntfidf_data","30f6a906":"test='does the weather have any impact on the propagation of the virus'\ntfidf_test=tfidf_fit.transform([test])","4a1f6f0c":"tfidf_test.shape","e433a201":"mask=tfidf_test.toarray()!=0\nm=mask[0]\nm","bf935af3":"tfidf_test.toarray()[mask]","92e83813":"tfidf_data.columns[m]","393dc841":"from sklearn.metrics.pairwise import cosine_similarity\ncm=cosine_similarity(tfidf_test, tfidf_corpus)\ncm[0]","18934c82":"import numpy as np\npos=np.argmax(cm[0])\ndata.iloc[pos]","3c642435":"data.answers[pos]","2eb005b4":"def reponse(text):\n    \n    return resultat\n\n","a21e1f10":"Appliquer cette fonction sur la colonne **questions**","19413f5a":"le type de donn\u00e9es **objet** peut contenir plusieurs types. Par exemple, la colonne peut inclure des entiers, des r\u00e9els et des cha\u00eenes de caract\u00e9re qui sont collectivement \u00e9tiquet\u00e9s comme un objet","4015cc92":"### Fonction reponse\n\n","d3491e65":"Afficher les ponctuations","c611787f":"### TF-IDF\nC'est une mesure statistique permet d'\u00e9valuer l'importance d'un terme contenu dans un document, relativement \u00e0 une collection ou un corpus. Le poids augmente proportionnellement au nombre d'occurrences du mot dans le document. Il varie \u00e9galement en fonction de la fr\u00e9quence du mot dans le corpus.\n\n- TF : le Term Frequency d\u00e9crit la fr\u00e9quence \u00e0 laquelle un certain terme appara\u00eet dans un document par rapport \u00e0 tous les autres termes contenus dans le document. \n$$TF(m,p)=\\frac{f_{m,p}}{f_p}$$\n$f_{m,p}$: frequence du mot $m$ dans la phrase $p$, $f_p$ nombre de mots dans la phrase $p$\n\n- IDF : l\u2019Inverse Document Frequency est une valeur qui mesure la signification d'un terme non pas en fonction de sa fr\u00e9quence dans un document particulier, mais en fonction de sa distribution et de son utilisation dans l'ensemble des documents \n$$IDF(m)=\\log (\\frac{L}{L_m})$$\n$L$ : nombre de phrases dans le corpus, $L_m$ : nombre de phrases dans le corpus o\u00f9 le mot $m$ apparait\n\n- TF-IDF : la multiplication des deux valeurs. Etant donn\u00e9 que le Term Frequency repr\u00e9sente la pertinence d\u2019un terme dans un document donn\u00e9 et que l\u2019Inverse Document Frequency peut refl\u00e9ter le r\u00f4le d\u2019un terme par rapport \u00e0 tous les documents d\u2019un corpus, la combinaison des deux valeurs permet de bien comprendre la fr\u00e9quence r\u00e9elle des termes et le potentiel de chaque terme.","0b104982":"### Chatbot\n","798e69a8":"### Supprimer les Stop words\nCe sont les mots tr\u00e8s courants dans la langue \u00e9tudi\u00e9e (\"et\", \"\u00e0\", \"le\"... en fran\u00e7ais) qui n'apportent pas de valeur informative pour la compr\u00e9hension du \"sens\" d'un document. Ils sont tr\u00e8s fr\u00e9quents et ralentissent notre travail : nous souhaitons donc les supprimer.","e7fa4d83":"### Tokenization","f06a8691":"### Qu'est-ce qu'un chatbot ?\n\u00ab Chat \u00bb comme discussion en ligne et \u00ab bot \u00bb comme robot. Le chatbot, connu aussi sous le nom d'\u00ab agent conversationnel \u00bb, est un logiciel programm\u00e9 pour simuler une conversation en langage naturel.","f8323e9c":"Parfois, le m\u00eame mot peut avoir plusieurs lemmes en fonction du sens \/ du contexte.","d66f10f9":"### Supprimer les ponctuations","9868aa51":"### C'est quoi NLP\n\nLe traitement naturel du langage, aussi appel\u00e9 Natural Language Processing ou NLP en anglais, est une technologie permettant aux machines de comprendre le langage humain gr\u00e2ce \u00e0 l\u2019intelligence artificielle.\n\n### Vectorisation des mots (Word embedding)\nC'est une m\u00e9thode d'apprentissage d'une repr\u00e9sentation des mots utilis\u00e9e notamment en traitement automatique des langues. Cette technique permet de repr\u00e9senter chaque mot d'un dictionnaire par un vecteur de nombres r\u00e9els.\nLes mots apparaissant dans des contextes similaires poss\u00e8dent des vecteurs correspondants qui sont relativement proches.\n\n### Similarit\u00e9 entre vecteurs\nOn peut quantifier la similarit\u00e9 entre diff\u00e9rents mots ou texts.\nDeux mesures de similarit\u00e9: \n- la similarit\u00e9 cosinus, qui consiste \u00e0 quantifier la similarit\u00e9 entre deux text en calculant le cosinus entre leurs vecteurs : $$\\cos {\\alpha }={\\frac  {{\\mathbf  {v_{1}}}\\cdot {\\mathbf  {v_{2}}}}{\\left\\|{\\mathbf  {v_{1}}}\\right\\|\\left\\|{\\mathbf  {v_{2}}}\\right\\|}}$$\n- le carr\u00e9 de la norme L2 de la difference entre deux vecteurs exprim\u00e9e par: $$\\|{\\mathbf  {v_{1}-v_{2}}}\\|_{2}^{2}=\\|{\\mathbf  {v_{1}}}\\|_{2}^{2}+\\|{\\mathbf  {v_{2}}}\\|_{2}^{2}-2{\\mathbf  {v_{2}}}\\cdot {\\mathbf  {v_{1}}}$$","c87cd9eb":"### Lemmetizing\nhttps:\/\/www.machinelearningplus.com\/nlp\/lemmatization-examples-python\/\n\nOn remplace les mots par leur forme canonique","6144f9fd":"Couper les questions en unit\u00e9s linguistiques individuelles (Tokenization) .","8a4bcaf0":"## les donn\u00e9es\n","786fd73d":"Definir une fonction qui supprime les ponctuations ","d284357e":"## Data cleaning","5596c88a":"### Vectoriser","904bc8b2":"### Bag of words"}}