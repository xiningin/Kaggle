{"cell_type":{"9fedcdd8":"code","deee0fec":"code","7ed13de5":"code","d527473e":"code","053d8e9f":"code","47150b4a":"code","c47aec0e":"code","7ee12669":"code","679f0168":"code","9ece8fce":"code","7a638588":"markdown","ddfb2a88":"markdown","505374d3":"markdown","88feaf55":"markdown","04edcba8":"markdown","0cee5096":"markdown","d8fa5d55":"markdown","0d8435a7":"markdown"},"source":{"9fedcdd8":"import numpy as np\nfrom matplotlib import pyplot as plt","deee0fec":"# base class for neural network layers\nclass Layer():\n    def __init__(self):\n        pass\n    \n    # forward pass\n    # compute output value y given input x\n    def forward(self, x):\n        raise NotImplementedError\n    \n    # backward pass\n    # compute gradient for weight variables dE\/dW\n    # and compute gradient for input dE\/dx\n    # given `node_grad` which is the gradient passed from previous layers dE\/dy\n    def backward(self, node_grad):\n        raise NotImplementedError\n    \n    # update weight variables based on gradients\n    def update(self, learning_rate):\n        raise NotImplementedError","7ed13de5":"class Sigmoid(Layer):    \n    def forward(self, x):\n        self.y = 1 \/ (1 + np.exp(-x))\n        return self.y\n    \n    def backward(self, node_grad):\n        return node_grad * (self.y * (1-self.y))\n    \n    # no parameters to train\n    def update(self, learning_rate):\n        pass","d527473e":"class Relu():        \n    def forward(self, x):\n        self.x = x\n        return np.maximum(x, 0)\n    \n    def backward(self, node_grad):\n        return node_grad * (self.x > 0)\n    \n    def update(self, learning_rate):\n        pass","053d8e9f":"class Softmax_Cross_Entropy():\n    # forward pass with softmax\n    def forward(self, x):\n        exps = np.exp(x - np.max(x)) # float64 is 10^308, exp with too large x will yield inf\n        self.y = exps \/ np.sum(exps)\n        return self.y\n    \n    # backward pass\n    # compute gradient of softmax+crossentropy simultaneously\n    def backward(self, label):\n        self.out_grad = self.y - label\n        return self.out_grad\n    \n    def update(self, learning_rate):\n        pass","47150b4a":"class Linear(Layer):\n    def __init__(self, size_in, size_out, with_bias):\n        self.size_in = size_in\n        self.size_out = size_out\n        self.with_bias = with_bias\n        self.W = self.initialize_weight()\n        if with_bias:\n            self.b = np.zeros(size_out)\n    \n    # initialization is important\n    # dangerous if set to 0\n    def initialize_weight(self):\n        epsilon = np.sqrt(2.0 \/ (self.size_in + self.size_out))\n        return epsilon * (np.random.rand(self.size_in, self.size_out) * 2 - 1) \n        \n    def forward(self, x):\n        self.x = x\n        self.y = x @ self.W + self.b  # @ is shortcut for matrix multiplication\n        return self.y\n    \n    def backward(self, node_grad):\n        self.G_W = np.outer(self.x, node_grad)\n        if self.with_bias:\n            self.G_b = node_grad\n        return node_grad @ self.W.T\n    \n    # update weight parameters, perform gradient descent\n    def update(self, learning_rate):\n        self.W -= learning_rate * self.G_W\n        if self.with_bias:\n            self.b -= learning_rate * self.G_b","c47aec0e":"class MLP():\n    def __init__(self, layer_size, with_bias=True, activation=\"sigmoid\", learning_rate=1):\n        assert len(layer_size) >= 2\n        self.layer_size = layer_size\n        self.with_bias = with_bias\n        if activation == \"sigmoid\":\n            self.activation = Sigmoid\n        elif activation == \"relu\":\n            self.activation = Relu\n        else:\n            raise Exception(\"activation not implemented\")\n        self.learning_rate = learning_rate\n        self.build_model()\n        \n    def build_model(self):\n        self.layers = []\n        \n        size_in = self.layer_size[0]\n        for hu in self.layer_size[1:-1]:\n            self.layers.append(Linear(size_in, hu, self.with_bias))\n            self.layers.append(self.activation())\n            size_in = hu\n            \n        # final layer uses softmax+crossentropy\n        self.layers.append(Linear(size_in, self.layer_size[-1], self.with_bias))\n        self.layers.append(Softmax_Cross_Entropy())\n        \n    def forward(self, x):\n        for layer in self.layers:\n            x = layer.forward(x)\n        return x\n    \n    def backward(self, label):\n        node_grad = label\n        for layer in reversed(self.layers):\n            node_grad = layer.backward(node_grad)\n            \n    def update(self, learning_rate):\n        for layer in self.layers:\n            layer.update(learning_rate)\n            \n    def train(self, x, label):\n        y = self.forward(x)\n        self.backward(label)\n        self.update(self.learning_rate)\n    \n    def predict(self, x):\n        for layer in self.layers:\n            x = layer.forward(x)\n        return np.argmax(x)\n    \n    def loss(self, x, label):\n        y = self.forward(x)\n        return -np.log(y) @ label # cross entropy loss","7ee12669":"X = np.array([\n    [0, 0],\n    [0, 1],\n    [1, 0],\n    [1, 1]\n])\nY = np.array([\n    [0, 1],\n    [1, 0],\n    [1, 0],\n    [0, 1]\n])","679f0168":"np.random.seed(1007) # set random seed\nEPOCH = 1000 # set training epochs\nN = X.shape[0] # get number of training data\n\n# craete a MLP with layer size [2,4,2] and learning_rate 0.1\nmlp = MLP([2, 4, 2], learning_rate=.1, activation=\"relu\")\n\nloss = np.zeros(EPOCH) # store losses\nfor epoch in range(EPOCH):\n    # train on each training data\n    for i in range(N):\n        mlp.train(X[i], Y[i])\n        \n    # compute loss\n    for i in range(N):\n        loss[epoch] += mlp.loss(X[i], Y[i])\n        \n    loss[epoch] \/= N\n    \n# plot loss curve\nplt.figure()\nix = np.arange(EPOCH)\nplt.plot(ix, loss)","9ece8fce":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import SGD\n\nmodel = Sequential()\nmodel.add(Dense(8, input_dim=2, activation=\"relu\"))\nmodel.add(Dense(2, activation=\"softmax\"))\n\nsgd = SGD(lr=.1)\n\nmodel.compile(optimizer=sgd,\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\nhistory = model.fit(X, Y, epochs=200)","7a638588":"create a simple dataset (XOR)\n![xor.png](attachment:xor.png)","ddfb2a88":"Softmax + CrossEntropy\n\nFor forward and backward computation, please check https:\/\/deepnotes.io\/softmax-crossentropy","505374d3":"## Using Keras","88feaf55":"Linear Layer\n\nFor forward and backward computation, please check http:\/\/cs231n.stanford.edu\/handouts\/linear-backprop.pdf","04edcba8":"# back propogation, activation function and multi-layer perceptron","0cee5096":"A layer in neural network\n![layer.png](attachment:layer.png)","d8fa5d55":"Relu Function\n\n$$\ny= \n\\begin{cases}\n    x, & \\text{if } x\\gt 0\\\\\n    0, & \\text{otherwise}\n\\end{cases}\n$$\n\nGradients \n\n$$\n\\frac{\\partial{y}}{\\partial{x}}= \n\\begin{cases}\n    1, & \\text{if } x\\gt 0\\\\\n    0, & \\text{otherwise}\n\\end{cases}\n$$\n\nBackpropogation\n\n$$\n\\frac{\\partial{E}}{\\partial{x}}= \n\\begin{cases}\n    \\frac{\\partial{E}}{\\partial{y}}, & \\text{if } x\\gt 0\\\\\n    0, & \\text{otherwise}\n\\end{cases}\n$$","0d8435a7":"Sigmoid Function\n\n$$y = \\frac{1}{1+e^{-x}}$$\n\nGradients \n\n$$\\frac{\\partial{y}}{\\partial{x}}=y*(1-y)$$\n\nBackpropogation\n\n$$\\frac{\\partial{E}}{\\partial{x}}=\\frac{\\partial{E}}{\\partial{y}}*y*(1-y)$$"}}