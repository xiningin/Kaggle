{"cell_type":{"94b8b446":"code","ac8cb1e6":"code","e1f1b957":"code","6b664903":"code","e7780334":"code","ad39a3e0":"code","14ecd271":"code","503e0c47":"code","f8e52835":"code","63f125e6":"code","7ffca4ed":"code","d3a0eae1":"code","213c667e":"code","121933f8":"code","e7582975":"code","28f484e8":"code","5229fc79":"code","d3d362f1":"code","94106857":"code","4c2ecb4a":"code","2db34fba":"code","0ada0c80":"code","9edbf491":"code","1a78efa0":"code","27b8ef29":"markdown","85151f90":"markdown","5c934d4a":"markdown","acef479f":"markdown","eaff5d2c":"markdown","54ac987a":"markdown","6e0f5172":"markdown","0813f3f4":"markdown","e7e07360":"markdown","ea08dd46":"markdown","cc4cec90":"markdown","b7a9bd23":"markdown","f31d9a62":"markdown","d4b35bb2":"markdown","3ed68a63":"markdown","500a1777":"markdown","389a1c8c":"markdown"},"source":{"94b8b446":"import nltk \nfrom nltk.corpus import stopwords\n\nimport pandas as pd \nimport numpy as np \nfrom numpy import array \n\nimport re \nimport keras\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.text import one_hot\nfrom keras.preprocessing.sequence import pad_sequences\n\nfrom keras.models import Sequential\n\nfrom keras.layers.core import Dropout\nfrom keras.layers.core import Dense\nfrom keras.layers.core import Activation\n\nfrom keras.layers import LeakyReLU\nfrom keras.layers import LSTM\nfrom keras.layers import Bidirectional\nfrom keras.layers import Flatten\nfrom keras.layers import GlobalMaxPooling1D\nfrom keras.layers.embeddings import Embedding\n\nfrom sklearn.model_selection import train_test_split\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns \n","ac8cb1e6":"NUMWORDS = 5000\nMAXLEN = 100","e1f1b957":"df= pd.read_csv('..\/input\/imdb-dataset-of-50k-movie-reviews\/IMDB Dataset.csv')\ndf.shape","6b664903":"df.head()","e7780334":"sns.countplot(x='sentiment', data=df)","ad39a3e0":"# we remove any HTML tage with removing the opening tage bracket '<' then remove anything is not the closing to this bracket '>' \n# lastly we remove the close bracket >. \nTAG_RE= re.compile(r'<[^>]+>' )\ndef remove_tags(text): \n    return TAG_RE.sub('', text)","14ecd271":"def cleanData(text):\n    sentence = remove_tags(text)\n    \n    # removing any thing that is not in the engilsh letter (simples, numbers, punctuations and so on)\n    sentence = re.sub('[^a-zA-Z]', ' ', sentence )\n    \n    # remove any single charachter \n    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", \" \", sentence)\n    \n    # replace the multiple spaces with a single one \n    sentence = re.sub('\\s+', ' ', sentence)\n    \n    sw = stopwords.words('english')# array of all english stop words \n    # remove all stop words from the data and save the result in array \n    sentence = [word.lower() for word in sentence.split() if word.lower() not in sw]\n    sentence = \" \".join(sentence) # joinning the array to a sentence again \n    \n    return sentence","503e0c47":"reviews = []\nsentences = list(df['review'])\nfor s in sentences: \n    reviews.append(cleanData(s))\nreviews[4]","f8e52835":"labels = df.sentiment\nlabels = array(list (map(lambda x: 1 if x == 'positive' else 0, labels)))\nlabels[:5]","63f125e6":"X_train, X_test, y_train, y_test = train_test_split(reviews, labels, test_size = .2, random_state = 69)\n","7ffca4ed":"tokenizer = Tokenizer(num_words= NUMWORDS)\ntokenizer.fit_on_texts(X_train)\n\nX_train = tokenizer.texts_to_sequences(X_train)\nX_test = tokenizer.texts_to_sequences(X_test)","d3a0eae1":"vocab_size = len(tokenizer.word_index) + 1 \nX_train[7][:10]","213c667e":"\nX_train = pad_sequences(X_train, padding = 'post', maxlen= MAXLEN)\nX_test = pad_sequences(X_test, padding = 'post', maxlen= MAXLEN)","121933f8":"embedding_dictionary = dict()\nwith open('..\/input\/glove6b100dtxt\/glove.6B.100d.txt','r') as glove_file:\n    for line in glove_file:\n        values=line.split()\n        word=values[0]\n        vectors=np.asarray(values[1:],'float32')\n        embedding_dictionary[word]=vectors\nglove_file.close()","e7582975":"embedding_matrix = np.zeros((vocab_size, MAXLEN))\nfor word, index in tokenizer.word_index.items():\n    embedding_vector = embedding_dictionary.get(word)\n    if embedding_vector is not None: \n        embedding_matrix[index] = embedding_vector\n        ","28f484e8":"embedding_matrix.shape","5229fc79":"model = Sequential()\nembedding_layer = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=MAXLEN, trainable=False)\n\nmodel.add(embedding_layer)\nmodel.add(Bidirectional(LSTM(128,recurrent_dropout=0.4,return_sequences=True)))\nmodel.add(Bidirectional(LSTM(64,recurrent_dropout=0.3,return_sequences=True)))\nmodel.add(Bidirectional(LSTM(40,recurrent_dropout=0.3,return_sequences=False)))\n\nmodel.add(Dense(64, activation=LeakyReLU(alpha=0.2)))\nmodel.add(Dense(32, activation=LeakyReLU(alpha=0.2)))\n\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.summary()\nmodel.compile(optimizer= 'adam', loss= 'binary_crossentropy', metrics=['acc'])\n","d3d362f1":"import tensorflow as tf\n\nkeras.utils.vis_utils.plot_model(model, show_shapes=True)","94106857":"history = model.fit(X_train, y_train,\n                    batch_size=128,\n                    epochs = 10,\n                    workers=50,\n                    use_multiprocessing=True,\n                    validation_split=0.2 )\n","4c2ecb4a":"score = model.evaluate(X_test, y_test)","2db34fba":"#print(labels[60])\ninstance = reviews[60]\ninstance","0ada0c80":"instance = tokenizer.texts_to_sequences(instance)\nflat_list = []\nfor sublist in instance:\n    for item in sublist:\n        flat_list.append(item)\nflat_list = [flat_list]\ninstance = pad_sequences(flat_list, padding='post', maxlen=MAXLEN)\nmodel.predict(instance)","9edbf491":"\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train','test'], loc='upper left')\nplt.show()","1a78efa0":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train','test'], loc='upper left')\nplt.show()","27b8ef29":"# define the variables ","85151f90":"# model predicting the test data","5c934d4a":"# spliting the data \nWe need to split the data into train, and test sets. the train data we will use it to train the data the the test set we will use it to evaluate the data. ","acef479f":"# model fitting ","eaff5d2c":"\n# data cleaning","54ac987a":"# Tokenize the data \n","6e0f5172":"# converting the labels to be numeric ","0813f3f4":"# loading Glove model after importing the data from Kaggel data \nyou can import this data using + sign \"add data\" under the save button  ","e7e07360":"# ploting histogram for the labels ","ea08dd46":"# ploting the history of the model for the training and the testing ","cc4cec90":"# reading the data ","b7a9bd23":"# creating the model ","f31d9a62":"# checking the model in a random review \nhere is a sample of data to check out model this is an instance from the data that is in row 70 and this review is a **negative review** ","d4b35bb2":"# ploting the model ","3ed68a63":"# analyising IMBD dataset\n\nThe aim is to develop an algorithm to predict whether a review  about movies is positive or not.","500a1777":"# creating embedding matrix ","389a1c8c":"#  the libraries we needed \n>+ we need nltk in text cleaning\n>+ pandas for data reading\n>+ numpy we need in preprocessing \n>+ re we needing it to clean the text data to remove some tages, spaces, simples, numbers, and punctuations. \n>+ keras we needed to tokenize the text data, encode the data, apply the model, and padding the sequences\n>+ sklearn we need to split the data into train test \n>+ matplotlib visualization library used to plot the history of the model. \n>+ seaborn visualization library used to plot histogram to the positive and negative data. "}}