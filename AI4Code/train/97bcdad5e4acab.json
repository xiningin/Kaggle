{"cell_type":{"c995dd98":"code","dc8c793e":"code","86b27b30":"code","99f60de0":"code","98609acc":"code","a396db44":"code","231937cd":"code","91151892":"code","40829985":"code","7f8dfc46":"code","03012109":"code","0314d03f":"code","106e35d6":"code","0dbfcc39":"code","0c2d0e99":"code","ff7cbd27":"code","f32b7924":"code","6bffa64e":"code","01f7ffe2":"code","81984c99":"code","79e41c1e":"markdown","259ec09d":"markdown","a56097dc":"markdown","cf5360c9":"markdown","1bf62df0":"markdown","c5062616":"markdown","30278296":"markdown","57da8af2":"markdown","6cb55d82":"markdown","c524ac9c":"markdown","498cbfa0":"markdown","d45f2f03":"markdown"},"source":{"c995dd98":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\n\nfrom xgboost import XGBClassifier\n\nfrom pathlib import Path","dc8c793e":"class Config:\n    debug = False\n    competition = \"TPS_202110\"\n    seed = 42\n    NFOLDS = 5\n    EPOCHS = 10\n","86b27b30":"data_dir = Path('..\/input\/tabular-playground-series-oct-2021') # Change me every month","99f60de0":"%%time\ntrain_df = pd.read_csv(data_dir \/ \"train.csv\",\n#                       nrows=100000\n                      )\n\ntest_df = pd.read_csv(data_dir \/ \"test.csv\")\nsample_submission = pd.read_csv(data_dir \/ \"sample_submission.csv\")\n\nprint(f\"train data: Rows={train_df.shape[0]}, Columns={train_df.shape[1]}\")\nprint(f\"test data : Rows={test_df.shape[0]}, Columns={test_df.shape[1]}\")\n","98609acc":"# this function will help to reduce momory \n# data will be smaller with the same value\n\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df\n","a396db44":"%%time\ntrain_df = reduce_mem_usage(train_df)\ntest_df  = reduce_mem_usage(test_df)","231937cd":"train_df['std'] = train_df.std(axis=1)\ntrain_df['min'] = train_df.min(axis=1)\ntrain_df['max'] = train_df.max(axis=1)\n\ntest_df['std'] = test_df.std(axis=1)\ntest_df['min'] = test_df.min(axis=1)\ntest_df['max'] = test_df.max(axis=1)","91151892":"from sklearn.preprocessing import StandardScaler\n\n# scaler = StandardScaler()\n# train_df = pd.DataFrame (data=scaler.fit_transform(train_df), columns=train_df.columns)\n# test_df = pd.DataFrame (data=scaler.transform(test_df), columns=test_df.columns)\n","40829985":"features = [col for col in train_df.columns if col not in ('id', 'target')]","7f8dfc46":"y = train_df.target\n\ntest_df = test_df.drop([\"id\"], axis=1)\nX = train_df.drop([\"id\", \"target\"], axis=1)","03012109":"# X['std'] = X.std(axis=1)\n# X['min'] = X.min(axis=1)\n# X['max'] = X.max(axis=1)\n\n# test_df['std'] = test_df.std(axis=1)\n# test_df['min'] = test_df.min(axis=1)\n# test_df['max'] = test_df.max(axis=1)","0314d03f":"X.head()","106e35d6":"# Run Quickly\n\nxgb_params000 = {\n    'device_type':'gpu',  # Use cpu\/gpu\n    'gpu_id':0,\n    'gpu_platform_id':0,\n#     'objective':'binary:logistic',\n    'use_label_encoder': False,\n    'tree_method': 'gpu_hist',\n\n    'metric': 'auc',\n#     'num_leaves': 150,\n    'learning_rate': 0.05,\n    'max_depth': 3,\n\n#     'n_estimators': 10000,\n    }\n","0dbfcc39":"xgb_params = {\n    'max_depth': 6,\n    'n_estimators': 9500,\n    'subsample': 0.7,\n    'colsample_bytree': 0.2,\n    'colsample_bylevel': 0.6000000000000001,\n    'min_child_weight': 56.41980735551558,\n    'reg_lambda': 75.56651890088857,\n    'reg_alpha': 0.11766857055687065,\n    'gamma': 0.6407823221122686,\n    'booster': 'gbtree',\n    'eval_metric': 'auc',\n    'tree_method': 'gpu_hist',\n    'predictor': 'gpu_predictor',\n    'use_label_encoder': False\n    }","0c2d0e99":"final_test_predictions = []\nfinal_valid_predictions = {}\nscores = []\n\nkf = StratifiedKFold(n_splits=Config.NFOLDS, shuffle=True, random_state=Config.seed)\n\nfor fold, (train_idx, valid_idx) in enumerate(kf.split(X = X, y = y)):\n\n    print(10*\"=\", f\"Fold={fold}\", 10*\"=\")\n\n    x_train = X.loc[train_idx, :]\n    x_valid = X.loc[valid_idx, :]\n    \n    y_train = y[train_idx]\n    y_valid = y[valid_idx]\n\n    xgb_params['learning_rate']=0.007\n    model1 = XGBClassifier(**xgb_params)\n\n    model1.fit(x_train, y_train,\n          early_stopping_rounds=100,\n          eval_set=[(x_valid, y_valid)],\n          verbose=0)\n\n    xgb_params['learning_rate']=0.01\n    model2 = XGBClassifier(**xgb_params)\n    \n    model2.fit(x_train, y_train,\n          early_stopping_rounds=100,\n          eval_set=[(x_valid, y_valid)],\n          verbose=0,\n          xgb_model=model1)\n\n    xgb_params['learning_rate']=0.05\n    model3 = XGBClassifier(**xgb_params)\n    \n    model3.fit(x_train, y_train,\n          early_stopping_rounds=100,\n          eval_set=[(x_valid, y_valid)],\n          verbose=0,\n          xgb_model=model2)\n\n    \n    preds_valid = model3.predict_proba(x_valid)[:, -1]\n    # Want probability or classification?\n    final_valid_predictions.update(dict(zip(valid_idx, preds_valid)))\n\n    auc = roc_auc_score(y_valid,  preds_valid)\n    print('auc: ', auc)\n    scores.append(auc)\n    \n#     test_preds = model.predict_proba(test_df[features])[:, -1]\n    test_preds = model3.predict_proba(test_df)[:, -1]\n\n    final_test_predictions.append(test_preds)\n","ff7cbd27":"print(f\"scores -> mean: {np.mean(scores)}, std: {np.std(scores)}\")","f32b7924":"final_valid_predictions = pd.DataFrame.from_dict(final_valid_predictions, orient=\"index\").reset_index()\nfinal_valid_predictions.columns = [\"id\", \"pred_3\"]\nfinal_valid_predictions.to_csv(\"train_pred_3.csv\", index=False)","6bffa64e":"# print(model.objective)","01f7ffe2":"df = pd.DataFrame(np.column_stack(final_test_predictions))\ndf['mean'] = df.mean(axis=1)\ndf","81984c99":"sample_submission['target'] = np.mean(np.column_stack(final_test_predictions), axis=1)\nsample_submission.to_csv(\"test_pred_3.csv\",index=None)\nsample_submission.to_csv(\"basic_xgb_cv_fe.csv\",index=None)\nsample_submission\n","79e41c1e":"# Model","259ec09d":"# Submission File","a56097dc":"# Reduce Memory\n\nToo many memory issues. Got a function to reduce the float and int types by checking the max column value and setting column to minimum necessary type.\n\n- https:\/\/www.kaggle.com\/hrshuvo\/tps-oct-21-xgb-kfold\n- https:\/\/www.kaggle.com\/rinnqd\/reduce-memory-usage","cf5360c9":"# Load Data","1bf62df0":"# Predict on Test Data","c5062616":"## XGBoost\n\n3 Chained Models, slowly relaxing the learning rate","30278296":"# Save OOF Predictions\n\nSave the dictionary that we created for all the training predictions that were made when each fold was used for validation","57da8af2":"# Extract Target and Drop Unused Columns","6cb55d82":"# Configuration","c524ac9c":"# Load Libraries","498cbfa0":"# Feature Engineering","d45f2f03":"# Third Notebook for my Blending Notebook\n\n- [TPS Oct 2021 - The Melling Blend](https:\/\/www.kaggle.com\/mmellinger66\/tps-oct-2021-the-melling-blend)\n\n### Files needed for the blend\n\n- train_pred_3.csv\n- test_pred_3.csv\n\n# References\n\n- https:\/\/www.kaggle.com\/mohammadkashifunique\/single-xgboost-model-featureengineering\n- https:\/\/www.kaggle.com\/joecooper\/tsp-single-xgboost-model\n"}}