{"cell_type":{"60df2298":"code","0ea999b3":"code","9e74dd14":"code","c0cf7bc6":"code","588e5c49":"code","382965f5":"code","db2a6e82":"code","2b91ee92":"code","7d1a6051":"code","aebfa8ff":"code","d7ca0bab":"code","b3b240c6":"code","a777147e":"code","84733715":"code","2114a016":"code","681a7c86":"code","e90048dd":"code","dd0fa1a9":"code","c48b4c88":"code","761e94d5":"code","7e98db38":"code","c17b5e4d":"markdown","468d2358":"markdown","1909e9b3":"markdown"},"source":{"60df2298":"!pip install kaggle-environments --upgrade","0ea999b3":"import gc\nimport os\nimport sys\nfrom time import time, sleep\nimport json\nimport math\nimport pickle\nfrom collections import defaultdict\n\nimport numpy as np\nimport numba\nimport pandas as pd\nimport seaborn as sns\nimport lightgbm as lgb\nimport matplotlib.pyplot as plt\nfrom kaggle_environments import (\n    evaluate, make, utils,\n    get_episode_replay, list_episodes, list_episodes_for_submission\n)\n\nos.listdir(\"..\/input\")","9e74dd14":"df_episode_agents = pd.read_csv(\"..\/input\/meta-kaggle\/EpisodeAgents.csv\")\nrecent_submissions = [19352359, 19352272, 19352181, 19352079, 19351928,\n                      19335271, 19334971, 19334470, 19334131, 19334038,\n                      19325005, 19324861, 19324344, 19324095, 19323940,\n                      19311665, 19311618, 19311565, 19311503, 19311396,\n                      19287465, 19287233, 19286948, 19286747, 19286415, \n                      19278412, 19278217, 19278083, 19277961, 19277822, \n                      19252525, 19252439, 19252356, 19252202, 19252130,\n                      19237820, 19237695, 19237490, 19237372, 19237108,\n                      19224197, 19224008, 19223754, 19223504, 19223418,\n                      19208120, 19207921, 19207843, 19207741, 19207633,]\nset_recent_submissions = set(recent_submissions)\ndf_episode_agents = df_episode_agents[df_episode_agents[\"SubmissionId\"].isin(set_recent_submissions)]\ndf_episode_agents.reset_index(drop=True, inplace=True)\nepisodes = sorted(df_episode_agents[\"EpisodeId\"].unique().tolist())\nset_recent_submission_episode_id_and_indexes = set((df_episode_agents[\"EpisodeId\"] * 2 + df_episode_agents[\"Index\"]).tolist())\ndel df_episode_agents\ngc.collect()\nprint(f\"len(episodes)={len(episodes)}\")","c0cf7bc6":"data = []\nseen_episodes = set()\n\nfor episode_id in episodes[-3000:]:\n    replay = get_episode_replay(episode_id)\n    sleep(1)\n    if not replay[\"wasSuccessful\"]:\n        continue\n    d = json.loads(replay[\"result\"][\"replay\"])\n    if d[\"statuses\"] != ['DONE', 'DONE']:\n        continue\n\n    final_rewards = d[\"rewards\"]\n    thresholds = d[\"steps\"][0][0][\"observation\"][\"thresholds\"]\n    actions = []\n    last_reward_0 = 0\n    last_reward_1 = 0\n    rewards = []\n    for step in d[\"steps\"][1:]:\n        actions.append([step[0][\"action\"], step[1][\"action\"]])\n        reward_0 = step[0][\"reward\"]\n        reward_1 = step[1][\"reward\"]\n        rewards.append([\n            reward_0 - last_reward_0,\n            reward_1 - last_reward_1,\n        ])\n        last_reward_0 = reward_0\n        last_reward_1 = reward_1\n    dat = {\n        \"episode_id\": episode_id,\n        \"final_rewards\": final_rewards,\n        \"rewards\": rewards,\n        \"actions\": actions,\n        \"thresholds\": thresholds,\n    }\n    data.append(dat)","588e5c49":"print(len(data))\nwith open(\"data.json\", \"w\") as f:\n    json.dump(data, f)","382965f5":"N_FEATURES = 10\nTRANSFORM_BASE = 1.02","db2a6e82":"_rand = np.array([42], dtype=np.uint64)\n\n@numba.njit(\"f4[:,:](i8[:],i8[:,:],i8[:,:],i8[:],u8[:])\", cache=True)\ndef proc(thresholds, actions, rewards, teams, _rand):\n    def rand():\n        _rand[0] ^= _rand[0] << np.uint64(7)\n        _rand[0] ^= _rand[0] >> np.uint64(9)\n        return _rand[0]\n    res = np.empty((len(actions)*100*2, N_FEATURES+1), dtype=np.float32)\n    idx_res = 0\n    likelihoods1 = np.ones((100, 101), dtype=np.float64)\n    likelihoods2 = np.ones((100, 101), dtype=np.float64)\n    e1 = np.full(101, (TRANSFORM_BASE**np.arange(101.0)).mean(), dtype=np.float64)\n    e2 = np.full(101, (TRANSFORM_BASE**np.arange(101.0)).mean(), dtype=np.float64)\n    p1 = np.zeros((100, 2), dtype=np.int64)\n    p2 = np.zeros((100, 2), dtype=np.int64)\n    last1 = np.full(100, -1, dtype=np.int64)\n    last2 = np.full(100, -1, dtype=np.int64)\n    last_bandit_1 = -1\n    last_bandit_2 = -1\n    n_consecutive_1 = 0\n    n_consecutive_2 = 0\n    n_tried_bandits_1 = 0\n    n_tried_bandits_2 = 0\n    max_tried_1 = 0\n    max_tried_2 = 0\n    tried_counts_sorted_1 = np.zeros(100, dtype=np.int64)\n    tried_counts_sorted_2 = np.zeros(100, dtype=np.int64)\n    jini_sum_1 = 0\n    jini_sum_2 = 0\n    jini_sum_ideal = 1e-100\n    tried_counts_modified_1 = np.zeros(100, dtype=np.float64)\n    tried_counts_modified_2 = np.zeros(100, dtype=np.float64)\n    for step in range(len(actions)):\n        a1, a2 = actions[step]\n        r1, r2 = rewards[step]\n        for bandit, thr in enumerate(thresholds):  # 100\n            if teams[0] == 1 and rand() % 5 == 0:\n                res[idx_res] = float(step), float(n_consecutive_2), float(n_tried_bandits_2), float(max_tried_2),\\\n                                jini_sum_2\/jini_sum_ideal, float(p1[bandit].sum()), float(e1[bandit]), float(p2[bandit].sum()), float(tried_counts_modified_2[bandit]), float(step-last2[bandit]), float(thr)\n                idx_res += 1\n            if teams[1] == 1 and rand() % 5 == 0:\n                res[idx_res] = float(step), float(n_consecutive_1), float(n_tried_bandits_1), float(max_tried_1),\\\n                                jini_sum_1\/jini_sum_ideal, float(p2[bandit].sum()), float(e2[bandit]), float(p1[bandit].sum()), float(tried_counts_modified_1[bandit]), float(step-last1[bandit]), float(thr)\n                idx_res += 1\n        decay1 = 0.97 ** (p1[a1].sum() + p2[a1].sum())\n        decay2 = 0.97 ** (p1[a2].sum() + p2[a2].sum())\n        for thr in range(101):\n            prob = np.ceil(thr * decay1) \/ 101.0\n            likelihoods1[a1, thr] *= prob if r1 == 1 else 1 - prob\n            prob = np.ceil(thr * decay2) \/ 101.0\n            likelihoods2[a2, thr] *= prob if r2 == 1 else 1 - prob\n        e1[a1] = (likelihoods1[a1] * TRANSFORM_BASE**np.arange(101.0)).sum() \/ likelihoods1[a1].sum()\n        e2[a2] = (likelihoods2[a2] * TRANSFORM_BASE**np.arange(101.0)).sum() \/ likelihoods2[a2].sum()\n        if p1[a1].sum() == 0:\n            n_tried_bandits_1 += 1\n        if p2[a2].sum() == 0:\n            n_tried_bandits_2 += 1\n        idx = np.searchsorted(tried_counts_sorted_1, p1[a1].sum(), side=\"right\") - 1\n        jini_sum_1 += 99 - idx\n        tried_counts_sorted_1[idx] += 1\n        idx = np.searchsorted(tried_counts_sorted_2, p2[a2].sum(), side=\"right\") - 1\n        jini_sum_2 += 99 - idx\n        tried_counts_sorted_2[idx] += 1\n        jini_sum_ideal += step % 100\n        assert jini_sum_1 <= jini_sum_ideal and jini_sum_2 <= jini_sum_ideal\n        tried_counts_modified_1[a1] += 1.0 \/ decay1\n        tried_counts_modified_2[a2] += 1.0 \/ decay2\n        p1[a1, r1] += 1\n        p2[a2, r2] += 1\n        max_tried_1 = max(max_tried_1, p1[a1].sum())\n        max_tried_2 = max(max_tried_2, p2[a2].sum())\n        last1[a1] = step\n        last2[a2] = step\n        if a1 == last_bandit_1:\n            n_consecutive_1 += 1\n        if a2 == last_bandit_2:\n            n_consecutive_2 += 1\n        last_bandit_1 = a1\n        last_bandit_2 = a2\n    return res[:idx_res]\n\n\ntrain = np.empty((3000*2000*100*2, N_FEATURES+1), dtype=np.float32)\nvalid = np.empty((800*2000*100*2, N_FEATURES+1), dtype=np.float32)\nidx_train = idx_valid = 0\nids = set()\n\ndef rand():\n    _rand[0] ^= _rand[0] << np.uint64(7)\n    _rand[0] ^= _rand[0] >> np.uint64(9)\n    return _rand[0]\nfor dat in data:\n    episode_id = dat[\"episode_id\"]\n    if episode_id in ids:\n        continue\n    ids.add(episode_id)\n    thresholds = np.array(dat[\"thresholds\"])\n    actions = np.array(dat[\"actions\"])\n    rewards = np.array(dat[\"rewards\"])\n    teams = np.array([episode_id * 2 + 0 in set_recent_submission_episode_id_and_indexes,\n                      episode_id * 2 + 1 in set_recent_submission_episode_id_and_indexes], dtype=np.int64)\n    d = proc(thresholds, actions, rewards, teams, _rand)\n    if rand() % 5 != 0:\n        train[idx_train:idx_train+len(d)] = d\n        idx_train += len(d)\n    else:\n        valid[idx_valid:idx_valid+len(d)] = d\n        idx_valid += len(d)\n\ntrain = train[:idx_train]\nvalid = valid[:idx_valid]\n\nprint(train.shape, valid.shape)","2b91ee92":"n_train = len(train)\nX_train, y_train = train[:, :N_FEATURES], TRANSFORM_BASE ** train[:, N_FEATURES]\nX_valid, y_valid = valid[:, :N_FEATURES], TRANSFORM_BASE ** valid[:, N_FEATURES]\n\nlgb_train = lgb.Dataset(X_train.copy(), y_train.copy())\ndel X_train, y_train\nlgb_valid = lgb.Dataset(X_valid.copy(), y_valid.copy(), reference=lgb_train)\ndel X_valid, y_valid\ndel train, valid\ngc.collect()","7d1a6051":"%%time\n\nparams = {\n    'boosting_type': \"gbdt\",\n    'objective': \"rmse\",\n    'metric': \"rmse\",\n    'num_leaves': 4095,\n    'learning_rate': 0.05,\n    'feature_fraction': 0.9,\n    'bagging_fraction': 0.5,\n    'bagging_freq': 5,\n    'verbose': 1,\n    \"seed\": 42,\n}\n\ngbm = lgb.train(\n    params,\n    lgb_train,\n    num_boost_round=1024,\n    valid_sets=lgb_valid,\n    early_stopping_rounds=50\n)","aebfa8ff":"feature_names = \"STEP, OPPONENT_N_CONSECUTIVE, OPPONENT_N_TRIED_BANDITS, OPPONENT_MAX_TRIED, OPPONENT_JINI, N_TRIED, EXPECTED_THRESHOLD, OPPONENT, OPPONENT_MODIFIED, OPPONENT_NOT_USED_TURNS\".split(\", \")\n\ndf_importance = pd.DataFrame()\ndf_importance[\"importance\"] = gbm.feature_importance()\ndf_importance[\"feature\"] = feature_names\ndf_importance.sort_values(\"importance\", ascending=False, inplace=True)\n\nplt.figure(figsize=(10,6)) \nsns.barplot(y=\"feature\", x=\"importance\", data=df_importance, palette=sns.hls_palette(10, l=0.7, s=1))\nplt.show()","d7ca0bab":"model_filename = \"model062_1.pickle\"\nwith open(model_filename, \"wb\") as f:\n    pickle.dump(gbm,  f)","b3b240c6":"@numba.njit(\"f4[:,:](i8[:],i8[:,:],i8[:,:],i8[:],u8[:])\", cache=True)\ndef proc(thresholds, actions, rewards, teams, _rand):\n    def rand():\n        _rand[0] ^= _rand[0] << np.uint64(7)\n        _rand[0] ^= _rand[0] >> np.uint64(9)\n        return _rand[0]\n    res = np.empty((len(actions)*100*2, N_FEATURES+1), dtype=np.float32)\n    idx_res = 0\n    likelihoods1 = np.ones((100, 101), dtype=np.float64)\n    likelihoods2 = np.ones((100, 101), dtype=np.float64)\n    e1 = np.full(101, 50.0, dtype=np.float64)\n    e2 = np.full(101, 50.0, dtype=np.float64)\n    p1 = np.zeros((100, 2), dtype=np.int64)\n    p2 = np.zeros((100, 2), dtype=np.int64)\n    last1 = np.full(100, -1, dtype=np.int64)\n    last2 = np.full(100, -1, dtype=np.int64)\n    last_bandit_1 = -1\n    last_bandit_2 = -1\n    n_consecutive_1 = 0\n    n_consecutive_2 = 0\n    n_tried_bandits_1 = 0\n    n_tried_bandits_2 = 0\n    max_tried_1 = 0\n    max_tried_2 = 0\n    tried_counts_sorted_1 = np.zeros(100, dtype=np.int64)\n    tried_counts_sorted_2 = np.zeros(100, dtype=np.int64)\n    jini_sum_1 = 0\n    jini_sum_2 = 0\n    jini_sum_ideal = 1e-100\n    tried_counts_modified_1 = np.zeros(100, dtype=np.float64)\n    tried_counts_modified_2 = np.zeros(100, dtype=np.float64)\n    for step in range(len(actions)):\n        a1, a2 = actions[step]\n        r1, r2 = rewards[step]\n        for bandit, thr in enumerate(thresholds):  # 100\n            if teams[0] == 1 and rand() % 5 == 0:\n                res[idx_res] = float(step), float(n_consecutive_2), float(n_tried_bandits_2), float(max_tried_2),\\\n                                jini_sum_2\/jini_sum_ideal, float(p1[bandit].sum()), float(e1[bandit]), float(p2[bandit].sum()), float(tried_counts_modified_2[bandit]), float(step-last2[bandit]), float(thr)\n                idx_res += 1\n            if teams[1] == 1 and rand() % 5 == 0:\n                res[idx_res] = float(step), float(n_consecutive_1), float(n_tried_bandits_1), float(max_tried_1),\\\n                                jini_sum_1\/jini_sum_ideal, float(p2[bandit].sum()), float(e2[bandit]), float(p1[bandit].sum()), float(tried_counts_modified_1[bandit]), float(step-last1[bandit]), float(thr)\n                idx_res += 1\n        decay1 = 0.97 ** (p1[a1].sum() + p2[a1].sum())\n        decay2 = 0.97 ** (p1[a2].sum() + p2[a2].sum())\n        for thr in range(101):\n            prob = np.ceil(thr * decay1) \/ 101.0\n            likelihoods1[a1, thr] *= prob if r1 == 1 else 1 - prob\n            prob = np.ceil(thr * decay2) \/ 101.0\n            likelihoods2[a2, thr] *= prob if r2 == 1 else 1 - prob\n        e1[a1] = (likelihoods1[a1] * np.arange(101.0)).sum() \/ likelihoods1[a1].sum()\n        e2[a2] = (likelihoods2[a2] * np.arange(101.0)).sum() \/ likelihoods2[a2].sum()\n        if p1[a1].sum() == 0:\n            n_tried_bandits_1 += 1\n        if p2[a2].sum() == 0:\n            n_tried_bandits_2 += 1\n        idx = np.searchsorted(tried_counts_sorted_1, p1[a1].sum(), side=\"right\") - 1\n        jini_sum_1 += 99 - idx\n        tried_counts_sorted_1[idx] += 1\n        idx = np.searchsorted(tried_counts_sorted_2, p2[a2].sum(), side=\"right\") - 1\n        jini_sum_2 += 99 - idx\n        tried_counts_sorted_2[idx] += 1\n        jini_sum_ideal += step % 100\n        assert jini_sum_1 <= jini_sum_ideal and jini_sum_2 <= jini_sum_ideal\n        tried_counts_modified_1[a1] += 1.0 \/ decay1\n        tried_counts_modified_2[a2] += 1.0 \/ decay2\n        p1[a1, r1] += 1\n        p2[a2, r2] += 1\n        max_tried_1 = max(max_tried_1, p1[a1].sum())\n        max_tried_2 = max(max_tried_2, p2[a2].sum())\n        last1[a1] = step\n        last2[a2] = step\n        if a1 == last_bandit_1:\n            n_consecutive_1 += 1\n        if a2 == last_bandit_2:\n            n_consecutive_2 += 1\n        last_bandit_1 = a1\n        last_bandit_2 = a2\n    return res[:idx_res]\n\n\ntrain = np.empty((3000*2000*100*2, N_FEATURES+1), dtype=np.float32)\nvalid = np.empty((800*2000*100*2, N_FEATURES+1), dtype=np.float32)\nidx_train = idx_valid = 0\nids = set()\n\ndef rand():\n    _rand[0] ^= _rand[0] << np.uint64(7)\n    _rand[0] ^= _rand[0] >> np.uint64(9)\n    return _rand[0]\nfor dat in data:\n    episode_id = dat[\"episode_id\"]\n    if episode_id in ids:\n        continue\n    ids.add(episode_id)\n    thresholds = np.array(dat[\"thresholds\"])\n    actions = np.array(dat[\"actions\"])\n    rewards = np.array(dat[\"rewards\"])\n    teams = np.array([episode_id * 2 + 0 in set_recent_submission_episode_id_and_indexes,\n                      episode_id * 2 + 1 in set_recent_submission_episode_id_and_indexes], dtype=np.int64)\n    d = proc(thresholds, actions, rewards, teams, _rand)\n    if rand() % 5 != 0:\n        train[idx_train:idx_train+len(d)] = d\n        idx_train += len(d)\n    else:\n        valid[idx_valid:idx_valid+len(d)] = d\n        idx_valid += len(d)\n\nprint(train.shape, valid.shape)\ntrain = train[:idx_train]\nvalid = valid[:idx_valid]","a777147e":"n_train = len(train)\nX_train, y_train = train[:, :N_FEATURES], train[:, N_FEATURES]\nX_valid, y_valid = valid[:, :N_FEATURES], valid[:, N_FEATURES]\n\nlgb_train = lgb.Dataset(X_train.copy(), y_train.copy())\ndel X_train, y_train\nlgb_valid = lgb.Dataset(X_valid.copy(), y_valid.copy(), reference=lgb_train)\ndel X_valid, y_valid\ndel train, valid\ndel data\ngc.collect()","84733715":"%%time\n\nparams = {\n    'boosting_type': \"gbdt\",\n    'objective': \"rmse\",\n    'metric': \"rmse\",\n    'num_leaves': 4095,\n    'learning_rate': 0.05,\n    'feature_fraction': 0.9,\n    'bagging_fraction': 0.5,\n    'bagging_freq': 5,\n    'verbose': 1,\n    \"seed\": 42,\n}\n\ngbm = lgb.train(\n    params,\n    lgb_train,\n    num_boost_round=1024,\n    valid_sets=lgb_valid,\n    early_stopping_rounds=50\n)","2114a016":"feature_names = \"STEP, OPPONENT_N_CONSECUTIVE, OPPONENT_N_TRIED_BANDITS, OPPONENT_MAX_TRIED, OPPONENT_JINI, N_TRIED, EXPECTED_THRESHOLD, OPPONENT, OPPONENT_MODIFIED, OPPONENT_NOT_USED_TURNS\".split(\", \")\n\ndf_importance = pd.DataFrame()\ndf_importance[\"importance\"] = gbm.feature_importance()\ndf_importance[\"feature\"] = feature_names\ndf_importance.sort_values(\"importance\", ascending=False, inplace=True)\n\nplt.figure(figsize=(10,6)) \nsns.barplot(y=\"feature\", x=\"importance\", data=df_importance, palette=sns.hls_palette(10, l=0.7, s=1))\nplt.show()","681a7c86":"model_filename = \"model062_2.pickle\"\nwith open(model_filename, \"wb\") as f:\n    pickle.dump(gbm,  f)","e90048dd":"!mkdir \/kaggle_simulations\n!mkdir \/kaggle_simulations\/agent","dd0fa1a9":"transform_model_filename = \"model062_1.pickle\"\nnormal_model_filename = \"model062_2.pickle\"\n\n!cp {transform_model_filename} \/kaggle_simulations\/agent\/\n!cp {normal_model_filename} \/kaggle_simulations\/agent\/\n\nwith open(\"main.py\", \"w\") as f:\n    f.write(f\"TRANSFORM_BASE = {TRANSFORM_BASE}\\n\")\n    f.write(f\"N_FEATURES = {N_FEATURES}\\n\")\n    f.write(f\"transform_model_filename = '\/kaggle_simulations\/agent\/{transform_model_filename}'\\n\")\n    f.write(f\"normal_model_filename = '\/kaggle_simulations\/agent\/{normal_model_filename}'\\n\")","c48b4c88":"%%writefile -a main.py\n\nimport sys\nimport math\nfrom time import time\nimport gzip\nimport base64\nimport random\nimport pickle\n\nimport numpy as np\n\nwith open(transform_model_filename, \"rb\") as f:\n    transform_model = pickle.load(f)\nwith open(normal_model_filename, \"rb\") as f:\n    normal_model = pickle.load(f)\n\nstate = {}\n\nrng = np.random.default_rng(int(time()*1000))\n\nSTEP, OPPONENT_N_CONSECUTIVE, OPPONENT_N_TRIED_BANDITS, OPPONENT_MAX_TRIED, OPPONENT_JINI, N_TRIED, EXPECTED_THRESHOLD, OPPONENT, OPPONENT_MODIFIED, OPPONENT_NOT_USED_TURNS = range(N_FEATURES)\n\ndef get_monotonic_func(a, b):\n    # a \u304c\u6b63\u306a\u3089\u5358\u8abf\u5897\u52a0\u3001\u8ca0\u306a\u3089\u5358\u8abf\u6e1b\u5c11\n    assert a != 0.0\n    x_left = -b\n    x_right = x_left + 1.0\n    def sigmoid(x):\n        return 1.0 \/ (1.0 + np.exp(-a * x))\n    left, right = sigmoid(x_left), sigmoid(x_right)\n    if a > 0:\n        def res(x):\n            y = sigmoid(x + x_left)\n            return (y - left) \/ (right - left)\n    else:\n        def res(x):\n            y = sigmoid(x + x_left)\n            return (y - right) \/ (left - right)\n    return res\n\nmonotonic_func = get_monotonic_func(-10.0, 0.0)\ndef get_transform_model_ratio(step):\n    return monotonic_func(step \/ 2000.0) * 1.2\n\ndef agent(observation, configuration):\n    if observation.step == 0:\n        state[\"X\"] = X = np.zeros((configuration[\"banditCount\"], N_FEATURES), dtype=np.float32)\n        X[:, OPPONENT_NOT_USED_TURNS] = 1\n        X[:, EXPECTED_THRESHOLD] = (TRANSFORM_BASE**np.arange(101.0)).mean()\n        state[\"normal_expected_threshold\"] = normal_expected_threshold = np.full(configuration[\"banditCount\"], 50.0, dtype=np.float32)\n        state[\"n_selections\"] = n_selections = [0] * configuration[\"banditCount\"]\n        state[\"total_reward\"] = 0\n        state[\"likelihoods\"] = np.ones((configuration[\"banditCount\"], 101), dtype=np.float64)\n        state[\"jini_sum\"] = 0\n        state[\"trial_counts\"] = np.zeros(100, dtype=np.int64)\n        state[\"trial_counts_sorted\"] = np.zeros(100, dtype=np.float64)\n        state[\"opponent_last_bandit\"] = -999\n        state[\"opponent_trial_counts\"] = np.zeros(100, dtype=np.int64)\n        state[\"opponent_jini_sum\"] = 0\n        state[\"jini_sum_ideal\"] = 1e-100\n        state[\"opponent_trial_counts_sorted\"] = np.zeros(100, dtype=np.float64)\n    else:\n        X = state[\"X\"]\n        normal_expected_threshold = state[\"normal_expected_threshold\"]\n        last_bandit = state[\"last_bandit\"]\n        n_selections = state[\"n_selections\"]\n        total_reward = state[\"total_reward\"]\n        likelihoods = state[\"likelihoods\"]\n        trial_counts = state[\"trial_counts\"]\n        trial_counts_sorted = state[\"trial_counts_sorted\"]\n        opponent_trial_counts = state[\"opponent_trial_counts\"]\n        opponent_trial_counts_sorted = state[\"opponent_trial_counts_sorted\"]\n        reward = observation.reward - total_reward\n        state[\"total_reward\"] = observation.reward\n        \n        decay = 0.97 ** (n_selections[last_bandit])\n        a1, a2 = observation.lastActions\n        opponent_last_bandit = a1 + a2 - last_bandit\n        opponent_decay = 0.97 ** n_selections[opponent_last_bandit]\n        for thr in range(101):\n            prob = np.ceil(thr * decay) \/ 101.0\n            likelihoods[last_bandit, thr] *= prob if reward == 1 else 1 - prob\n        X[last_bandit, EXPECTED_THRESHOLD] = (likelihoods[last_bandit] * TRANSFORM_BASE**np.arange(101.0)).sum() \/ likelihoods[last_bandit].sum()\n        normal_expected_threshold[last_bandit] = (likelihoods[last_bandit] * np.arange(101.0)).sum() \/ likelihoods[last_bandit].sum()\n        \n        for b in observation[\"lastActions\"]:  # take care\n            n_selections[b] += 1\n        \n        X[:, STEP] = observation.step\n        \n        X[last_bandit, N_TRIED] += 1\n#         if reward == 1:\n#             X[last_bandit, WIN] += 1\n#         else:\n#             X[last_bandit, LOSE] += 1\n        X[:, OPPONENT_N_CONSECUTIVE] += state[\"opponent_last_bandit\"] == opponent_last_bandit\n        if opponent_trial_counts[opponent_last_bandit] == 0:\n            X[:, OPPONENT_N_TRIED_BANDITS] += 1\n        \n        # jini\n        idx = np.searchsorted(trial_counts_sorted, trial_counts[last_bandit], side=\"right\") - 1\n        state[\"jini_sum\"] += 99 - idx\n        trial_counts_sorted[idx] += 1\n        idx = np.searchsorted(opponent_trial_counts_sorted, opponent_trial_counts[opponent_last_bandit], side=\"right\") - 1\n        state[\"opponent_jini_sum\"] += 99 - idx\n        opponent_trial_counts_sorted[idx] += 1\n        state[\"jini_sum_ideal\"] += (observation.step-1) % 100\n        #X[:, JINI] = state[\"jini_sum\"] \/ state[\"jini_sum_ideal\"]\n        X[:, OPPONENT_JINI] = state[\"opponent_jini_sum\"] \/ state[\"jini_sum_ideal\"]\n        \n        opponent_trial_counts[opponent_last_bandit] += 1  ### take care\n        trial_counts[last_bandit] += 1\n        \n        if X[0, OPPONENT_MAX_TRIED] < opponent_trial_counts[opponent_last_bandit]:\n            X[:, OPPONENT_MAX_TRIED] = opponent_trial_counts[opponent_last_bandit]\n        \n        X[opponent_last_bandit, OPPONENT] += 1\n        X[opponent_last_bandit, OPPONENT_MODIFIED] += 1.0 \/ opponent_decay\n        X[:, OPPONENT_NOT_USED_TURNS] += 1\n        X[opponent_last_bandit, OPPONENT_NOT_USED_TURNS] = 1\n        state[\"opponent_last_bandit\"] = opponent_last_bandit\n    \n    \n    y_transform = np.log(transform_model.predict(X)) \/ np.log(TRANSFORM_BASE)\n    X_normal = X.copy()\n    X_normal[:, EXPECTED_THRESHOLD] = normal_expected_threshold\n    y_normal = normal_model.predict(X_normal)\n    \n    r = get_transform_model_ratio(observation.step)\n    y = r * y_transform + (1 - r) * y_normal\n    \n    order = list(range(configuration[\"banditCount\"]))\n    rng.shuffle(order)\n    ama = 0\n    ma = 0\n    for bandit in order:\n        theta = y[bandit]\n        decay = 0.97 ** (n_selections[bandit])\n        theta *= decay\n        if theta > ma:\n            ma = theta\n            ama = bandit\n    bandit = ama\n    \n    state[\"last_bandit\"] = bandit\n    return bandit","761e94d5":"!tar -czvf submission.tar.gz main.py {transform_model_filename} {normal_model_filename}","7e98db38":"!cp ..\/input\/santa2020-061-f057-d0128\/model059.pickle \/kaggle_simulations\/agent\/\n!cp ..\/input\/santa2020-061-f057-d0128\/model060.pickle \/kaggle_simulations\/agent\/\nopponent_filename = \"..\/input\/santa2020-061-f057-d0128\/main.py\"\n\nenv = make(\"mab\", debug=True)\nenv.reset()\nenv.run([\"main.py\", opponent_filename])\nprint(env.toJSON()[\"rewards\"])\nenv.render(mode=\"ipython\", width=800, height=700)","c17b5e4d":"# Agent","468d2358":"# Model","1909e9b3":"# Data"}}