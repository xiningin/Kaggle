{"cell_type":{"601eda69":"code","896719b9":"code","ea2da030":"code","ecfb7fb5":"code","74fdb639":"code","985440a4":"code","6fe58f4d":"code","85013775":"code","f20e451d":"code","3df9376b":"code","1a2709f8":"code","f89c6920":"code","8c9a87a1":"code","45d42a51":"code","7f580c18":"code","a003dd00":"code","9c02d0e9":"code","80408c82":"code","7fe5af86":"code","b03e06dc":"code","00e5d1cb":"code","15a1aec9":"code","843e625e":"code","0be4de1e":"code","521bf1f0":"code","2c636ddf":"markdown","7ba816b6":"markdown","9a8386b3":"markdown","e99ccb4c":"markdown","c2a4ee7b":"markdown","b5632ead":"markdown","2c907d64":"markdown","42b3fef9":"markdown"},"source":{"601eda69":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt \n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n\nfrom sklearn.utils import shuffle\n\nimport re\nimport time\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem.porter import *\nfrom nltk.stem import LancasterStemmer\nfrom wordcloud import WordCloud ,STOPWORDS\n\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom catboost import CatBoostClassifier\n\nimport keras \nfrom keras.models import Sequential, Model \nfrom keras import layers\nfrom keras.layers import Dense, Dropout, Input, Embedding","896719b9":"cols = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\ndata = pd.read_csv('..\/input\/sentiment140\/training.1600000.processed.noemoticon.csv', header=None, encoding='ISO-8859-1', names=cols)","ea2da030":"data.shape","ecfb7fb5":"data.head()","74fdb639":"word_bank = []\n\ndef preprocess(text):\n    stemr = PorterStemmer()\n    tidy = re.sub('[^a-zA-Z#]',' ',text) \n    tidy = tidy.lower()\n    tidy = tidy.split()\n    all_stopwords = stopwords.words('english')\n    all_stopwords.remove('not')\n    review = [stemr.stem(word) for word in tidy if not word in set(all_stopwords)]\n    return ' '.join(tidy)","985440a4":"df = shuffle(data,random_state=17)\ndf = df[1:600000]","6fe58f4d":"df['target'].value_counts()","85013775":"df['text'] = df['text'].apply(lambda x: preprocess(x))","f20e451d":"all_words = df['text'].to_string()\nwordcloud = WordCloud(width=800, height=500, stopwords = STOPWORDS,max_font_size=110, random_state=17).generate(all_words)\n\nplt.figure(figsize=(10, 7))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.title('Most common words in tweets.')\nplt.show()","3df9376b":"positive_words = ' '.join([text for text in df['text'][df['target'] == 4]])\nwordcloud = WordCloud(width=800, height=500,stopwords = STOPWORDS,max_font_size=110, random_state=17).generate(positive_words)\nplt.figure(figsize=(10, 7))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.title('Most common words in positive tweets')\nplt.axis('off')\nplt.show()","1a2709f8":"negative_words = ' '.join([text for text in df['text'][df['target'] == 0]])\nwordcloud = WordCloud(width=800, height=500,stopwords = STOPWORDS,max_font_size=110, random_state=17).generate(negative_words)\nplt.figure(figsize=(10, 7))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.title('Most common words in negative tweets')\nplt.axis('off')\nplt.show()","f89c6920":"y = df['target']\nlabelE = LabelEncoder()\ny = labelE.fit_transform(y)","8c9a87a1":"X_train, X_test, y_train, y_test = train_test_split(df['text'], y, test_size = 0.20, random_state = 17)","45d42a51":"length_train = X_train.str.len()\nlength_test = X_test.str.len()\n\nplt.hist(length_train, bins=20, label=\"train_tweets\")\nplt.hist(length_test, bins=20, label=\"test_tweets\")\nplt.legend()\nplt.show()","7f580c18":"tfidf = TfidfVectorizer(max_features = 500)\nX_train_tf = tfidf.fit_transform(X_train).toarray() \nX_test_tf = tfidf.transform(X_test).toarray()","a003dd00":"X_train_tf.shape, X_test_tf.shape, y_train.shape, y_test.shape","9c02d0e9":"model_NB = MultinomialNB()\nmodel_NB.fit(X_train_tf,y_train)","80408c82":"y_pred_nb = model_NB.predict(X_test_tf)\nprint(\"Accuracy:\\n\", accuracy_score(y_test, y_pred_nb))\nprint(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_nb))\nprint(\"Classification Report:\\n\", classification_report(y_test, y_pred_nb))","7fe5af86":"model_DT= DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\nmodel_DT.fit(X_train_tf, y_train)","b03e06dc":"y_pred_dt = model_DT.predict(X_test_tf)\nprint(\"Accuracy:\\n\", accuracy_score(y_test, y_pred_dt))\nprint(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_dt))\nprint(\"Classification Report:\\n\", classification_report(y_test, y_pred_dt))","00e5d1cb":"from catboost import CatBoostClassifier\nmodel_CatB = CatBoostClassifier(verbose=0, n_estimators=200)\nmodel_CatB.fit(X_train_tf, y_train)","15a1aec9":"y_pred_catb = model_CatB.predict(X_test_tf)\ny_pred_catb[y_pred_catb > 0.5] = 1\ny_pred_catb[y_pred_catb <= 0.5] = 0\nprint(\"Accuracy:\\n\", accuracy_score(y_test, y_pred_catb))\nprint(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_catb))\nprint(\"Classification Report:\\n\", classification_report(y_test, y_pred_catb))","843e625e":"model = Sequential()\n\nmodel.add(Dense(512, activation='relu', input_dim=500))\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(16, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(8, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\nmodel.summary()","0be4de1e":"history = model.fit(X_train_tf, y_train, epochs=20, batch_size=64,validation_data=(X_test_tf,y_test))\nloss, accuracy = model.evaluate(X_train_tf, y_train, verbose=False)\nprint(\"Training Accuracy: {:.4f}\".format(accuracy))\nloss, accuracy = model.evaluate(X_test_tf, y_test, verbose=False)\nprint(\"Testing Accuracy:  {:.4f}\".format(accuracy))","521bf1f0":"y_pred_new = model.predict(X_test_tf)\ny_pred_nn = np.where(y_pred_new>0.5,1,0)\nprint(\"Accuracy:\\n\", accuracy_score(y_test, y_pred_nn))\nprint(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_nn))\nprint(\"Classification Report:\\n\", classification_report(y_test, y_pred_nn))","2c636ddf":"### Decision Tree","7ba816b6":"### Nerul Network","9a8386b3":"### Conclusion","e99ccb4c":"Naive Bayes Classifier : 0.7306666666666667\n\nDecision Tree : 0.6804166666666667\n\nCat Boost :  0.751575\n\nNerul Network : 0.7302333333333333\n","c2a4ee7b":"### CAT BOOST","b5632ead":"### Naive Bayes Classifier","2c907d64":"Tweets distribution is similar in the train and test data.","42b3fef9":"# Data Preprocessing "}}