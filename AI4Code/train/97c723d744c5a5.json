{"cell_type":{"93151e12":"code","9a4ea24d":"code","3ffe9653":"code","19ec1f9f":"code","aedb803a":"code","a458a505":"code","37c3962a":"code","16265db1":"code","c35543b7":"code","ae402221":"code","6b1136a2":"code","34459f03":"code","dbafc9a4":"code","988ee726":"code","6d78157f":"code","c2c12f4e":"code","bb4f1ea1":"code","45befec9":"code","766c8a5d":"markdown","a7121a89":"markdown","8f207ef9":"markdown","ec2bab9d":"markdown","2da0a4ea":"markdown","496d9cc0":"markdown","a194ba08":"markdown","9161effa":"markdown","d67b6901":"markdown","94779a72":"markdown","c080f102":"markdown","2a73290b":"markdown","71b1ec6b":"markdown","87419980":"markdown","75771454":"markdown","9592edf5":"markdown","fd1765c6":"markdown"},"source":{"93151e12":"%%bash\npip install pytorch-pfn-extras\npip install timm","9a4ea24d":"import os\nimport gc\nimport sys\nimport copy\nimport yaml\nimport random\nimport shutil\nimport typing as tp\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\n\nfrom tqdm.notebook import tqdm\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\n\nimport torch\nfrom torch import nn\nfrom torch import optim\nfrom torch.optim import lr_scheduler\nfrom torch.cuda import amp\n\nimport timm\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nimport pytorch_pfn_extras as ppe\nfrom pytorch_pfn_extras.config import Config\nfrom pytorch_pfn_extras.training import extensions as ppe_exts, triggers as ppe_triggers\n\nsys.path.append(\"..\/input\/volo-package\")\nfrom volo.models import volo_d1, volo_d2, volo_d3, volo_d4, volo_d5  # register models to timm\nfrom volo.utils import load_pretrained_weights as volo_load_weights","3ffe9653":"ROOT = Path.cwd().parent\nINPUT = ROOT \/ \"input\"\nOUTPUT = ROOT \/ \"output\"\n# DATA = INPUT \/ \"seti-breakthrough-listen\"\nDATA = INPUT \/ \"c\" \/ \"seti-breakthrough-listen\"\nTRAIN = DATA \/ \"train\"\nTEST = DATA \/ \"test\"\nTRAINING_OUTPUT_PATH = INPUT \/ \"seti-breakthrough-listen-weights-for-volo-d1\"\n\nRANDAM_SEED = 1086\nCLASSES = [\"target\",]\nN_CLASSES = len(CLASSES)\nFOLDS = [0, 1, 2, 3, 4]\nN_FOLDS = len(FOLDS)","19ec1f9f":"train = pd.read_csv(DATA \/ \"train_labels.csv\")\nsmpl_sub = pd.read_csv(DATA \/ \"sample_submission.csv\")","aedb803a":"skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDAM_SEED)\ntrain[\"fold\"] = -1\nfor fold_id, (_, val_idx) in enumerate(skf.split(train[\"id\"], train[\"target\"])):\n    train.loc[val_idx, \"fold\"] = fold_id","a458a505":"train.groupby(\"fold\").agg(total=(\"id\", len), pos=(\"target\", sum))","37c3962a":"VOLO_CHECHPOINTS = {\n    \"volo_d1\": \"..\/input\/volo-package\/d1_224_84.2.pth.tar\",\n    \"volo_d1-384\": \"..\/input\/volo-package\/d1_384_85.2.pth.tar\",\n    \"volo_d2\": \"..\/input\/volo-package\/d2_224_85.2.pth.tar\",\n    \"volo_d2-384\": \"..\/input\/volo-package\/d2_384_86.0.pth.tar\",\n    \"volo_d3\": \"..\/input\/volo-package\/d3_224_85.4.pth.tar\",\n    \"volo_d3-448\": \"..\/input\/volo-package\/d3_448_86.3.pth.tar\",\n    \"volo_d4\": \"..\/input\/volo-package\/d4_224_85.7.pth.tar\",\n    \"volo_d4-448\": \"..\/input\/volo-package\/d4_448_86.79.pth.tar\",\n    \"volo_d5\": \"..\/input\/volo-package\/d5_224_86.10.pth.tar\",\n    \"volo_d5-448\": \"..\/input\/volo-package\/d5_448_87.0.pth.tar\",\n    \"volo_d5-512\": \"..\/input\/volo-package\/d5_512_87.07.pth.tar\",\n}\n\nclass BasicImageModel(nn.Module):\n    \n    def __init__(\n        self, base_name: str, dims_head: tp.List[int],\n        pretrained=False, in_channels: int=3, image_size: int=224 \n    ):\n        \"\"\"Initialize\"\"\"\n        self.base_name = base_name\n        super().__init__()\n        model_name = base_name.split(\"-\")[0]\n        assert timm.is_model(model_name), \"you can use only models in timm.\"\n        \n        if model_name[:4] == \"volo\":\n            base_model = timm.create_model(\n                model_name, img_size=image_size, mix_token=False, return_dense=False)\n            in_features = base_model.head.in_features\n            if pretrained:\n                volo_load_weights(base_model, VOLO_CHECHPOINTS[base_name], strict=False)\n            \n            if in_channels != 3:\n                # # change input channel\n                # # I follow the manner used in timm.\n                first_conv = base_model.patch_embed.conv[0]\n                w_t = first_conv.weight.data  # shape: (out_ch, 3, 7, 7)\n                if in_channels == 1:\n                    new_w_t = w_t.sum(axis=1, keepdims=True)  # shape: (out_ch, 1, 7, 7)\n                else:\n                    n_repeats = (in_channels + 3 - 1) \/\/ 3\n                    new_w_t = w_t.repeat((1, n_repeats, 1, 1))\n                    new_w_t = new_w_t[:, :in_channels]\n                    new_w_t = new_w_t * 3 \/ in_channels  # shape: (out_ch, in_channels, 7, 7)\n\n                first_conv.weight.data = new_w_t\n        else:\n            base_model = timm.create_model(\n                base_name, pretrained=pretrained, in_chans=in_channels)\n            in_features = base_model.num_features\n            print(\"load imagenet pretrained:\", pretrained)\n            \n        base_model.reset_classifier(num_classes=0)\n        self.backbone = base_model\n        print(f\"{base_name}: {in_features}\")\n        \n        # # prepare head clasifier\n        if dims_head[0] is None:\n            dims_head[0] = in_features\n\n        layers_list = []\n        for i in range(len(dims_head) - 2):\n            in_dim, out_dim = dims_head[i: i + 2]\n            layers_list.extend([\n                nn.Linear(in_dim, out_dim),\n                nn.ReLU(), nn.Dropout(0.5),])\n        layers_list.append(\n            nn.Linear(dims_head[-2], dims_head[-1]))\n        self.head = nn.Sequential(*layers_list)\n\n    def forward(self, x):\n        \"\"\"Forward\"\"\"\n        h = self.backbone(x)\n        h = self.head(h)\n        return h","16265db1":"FilePath = tp.Union[str, Path]\nLabel = tp.Union[int, float, np.ndarray]\n\n\nclass SetiSimpleDataset(torch.utils.data.Dataset):\n    \"\"\"\n    Dataset using 6 channels by stacking them along time-axis\n\n    Attributes\n    ----------\n    paths : tp.Sequence[FilePath]\n        Sequence of path to cadence snippet file\n    labels : tp.Sequence[Label]\n        Sequence of label for cadence snippet file\n    transform: albumentations.Compose\n        composed data augmentations for data\n    \"\"\"\n\n    def __init__(\n        self,\n        paths: tp.Sequence[FilePath],\n        labels: tp.Sequence[Label],\n        transform: A.Compose,\n    ):\n        \"\"\"Initialize\"\"\"\n        self.paths = paths\n        self.labels = labels\n        self.transform = transform\n\n    def __len__(self):\n        \"\"\"Return num of cadence snippets\"\"\"\n        return len(self.paths)\n\n    def __getitem__(self, index: int):\n        \"\"\"Return transformed image and label for given index.\"\"\"\n        path, label = self.paths[index], self.labels[index]\n        img = self._read_cadence_array(path)\n        img = self.transform(image=img)[\"image\"]\n        return {\"image\": img, \"target\": label}\n\n    def _read_cadence_array(self, path: Path):\n        \"\"\"Read cadence file and reshape\"\"\"\n        img = np.load(path)  # shape: (6, 273, 256)\n        img = np.vstack(img)  # shape: (1638, 256)\n        img = img.transpose(1, 0)  # shape: (256, 1638)\n        img = img.astype(\"f\")[..., np.newaxis]  # shape: (256, 1638, 1)\n        return img\n\n    def lazy_init(self, paths=None, labels=None, transform=None):\n        \"\"\"Reset Members\"\"\"\n        if paths is not None:\n            self.paths = paths\n        if labels is not None:\n            self.labels = labels\n        if transform is not None:\n            self.transform = transform\n\n\nclass SetiAObsDataset(SetiSimpleDataset):\n    \"\"\"Use only on-target observation\"\"\"\n\n    def _read_cadence_array(self, path: Path):\n        \"\"\"Read cadence file and reshape\"\"\"\n        img = np.load(path)[[0, 2, 4]]  # shape: (3, 273, 256)\n        img = np.vstack(img)  # shape: (819, 256)\n        img = img.transpose(1, 0)  # shape: (256, 819)\n        img = img.astype(\"f\")[..., np.newaxis]  # shape: (256, 819, 1)\n        return img","c35543b7":"Batch = tp.Union[tp.Tuple[torch.Tensor], tp.Dict[str, torch.Tensor]]\nModelOut = tp.Union[tp.Tuple[torch.Tensor], tp.Dict[str, torch.Tensor], torch.Tensor]\n\n\nclass ROCAUC(nn.Module):\n    \"\"\"ROC AUC score\"\"\"\n\n    def __init__(self, average=\"macro\") -> None:\n        \"\"\"Initialize.\"\"\"\n        self.average = average\n        super(ROCAUC, self).__init__()\n\n    def forward(self, y, t) -> float:\n        \"\"\"Forward.\"\"\"\n        if isinstance(y, torch.Tensor):\n            y = y.detach().cpu().numpy()\n        if isinstance(t, torch.Tensor):\n            t = t.detach().cpu().numpy()\n\n        return roc_auc_score(t, y, average=self.average)\n\n\ndef micro_average(\n    metric_func: nn.Module,\n    report_name: str, prefix=\"val\",\n    pred_index: int=-1, label_index: int=-1,\n    pred_key: str=\"logit\", label_key: str=\"target\",\n) -> tp.Callable:\n    \"\"\"Return Metric Wrapper for Simple Mean Metric\"\"\"\n    metric_sum = [0.]\n    n_examples = [0]\n    \n    def wrapper(batch: Batch, model_output: ModelOut, is_last_batch: bool):\n        \"\"\"Wrapping metric function for evaluation\"\"\"\n        if isinstance(batch, tuple): \n            t = batch[label_index]\n        elif isinstance(batch, dict):\n            t = batch[label_key]\n        else:\n            raise NotImplementedError\n\n        if isinstance(model_output, tuple):\n            y = model_output[pred_index]\n        elif isinstance(model_output, dict):\n            y = model_output[pred_key]\n        else:\n            y = model_output\n\n        metric = metric_func(y, t).item()\n        metric_sum[0] += metric * y.shape[0]\n        n_examples[0] += y.shape[0]\n\n        if is_last_batch:\n            final_metric = metric_sum[0] \/ n_examples[0]\n            ppe.reporting.report({f\"{prefix}\/{report_name}\": final_metric})\n            # # reset state\n            metric_sum[0] = 0.\n            n_examples[0] = 0\n\n    return wrapper\n\n\ndef calc_across_all_batchs(\n    metric_func: nn.Module,\n    report_name: str, prefix=\"val\",\n    pred_index: int=-1, label_index: int=-1,\n    pred_key: str=\"logit\", label_key: str=\"target\",\n) -> tp.Callable:\n    \"\"\"\n    Return Metric Wrapper for Metrics caluculated on all data\n    \n    storing predictions and labels of evry batch, finally calculating metric on them.\n    \"\"\"\n    pred_list = []\n    label_list = []\n    \n    def wrapper(batch: Batch, model_output: ModelOut, is_last_batch: bool):\n        \"\"\"Wrapping metric function for evaluation\"\"\"\n        if isinstance(batch, tuple):\n            t = batch[label_index]\n        elif isinstance(batch, dict):\n            t = batch[label_key]\n        else:\n            raise NotImplementedError\n\n        if isinstance(model_output, tuple):\n            y = model_output[pred_index]\n        elif isinstance(model_output, dict):\n            y = model_output[pred_key]\n        else:\n            y = model_output\n\n        pred_list.append(y.numpy())\n        label_list.append(t.numpy())\n\n        if is_last_batch:\n            pred = np.concatenate(pred_list, axis=0)\n            label = np.concatenate(label_list, axis=0)\n            final_metric = metric_func(pred, label)\n            ppe.reporting.report({f\"{prefix}\/{report_name}\": final_metric})\n            # # reset state\n            pred_list[:] = []\n            label_list[:] = []\n\n    return wrapper","ae402221":"def set_random_seed(seed: int = 42, deterministic: bool = False):\n    \"\"\"Set seeds\"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)  # type: ignore\n    torch.backends.cudnn.deterministic = deterministic  # type: ignore\n\n\ndef to_device(\n    tensors: tp.Union[tp.Tuple[torch.Tensor], tp.Dict[str, torch.Tensor]],\n    device: torch.device, *args, **kwargs\n):\n    if isinstance(tensors, tuple):\n        return (t.to(device, *args, **kwargs) for t in tensors)\n    elif isinstance(tensors, dict):\n        return {\n            k: t.to(device, *args, **kwargs) for k, t in tensors.items()}\n    else:\n        return tensors.to(device, *args, **kwargs)","6b1136a2":"CONFIG_TYPES = {\n    # # utils\n    \"__len__\": lambda obj: len(obj),\n    \"method_call\": lambda obj, method: getattr(obj, method)(),\n\n    # # Dataset, DataLoader\n    \"SetiSimpleDataset\": SetiSimpleDataset,\n    \"SetiAObsDataset\": SetiAObsDataset,\n    \"DataLoader\": torch.utils.data.DataLoader,\n\n    # # Data Augmentation\n    \"Compose\": A.Compose, \"OneOf\": A.OneOf,\n    \"Resize\": A.Resize,\n    \"HorizontalFlip\": A.HorizontalFlip, \"VerticalFlip\": A.VerticalFlip,\n    \"ShiftScaleRotate\": A.ShiftScaleRotate,\n    \"RandomResizedCrop\": A.RandomResizedCrop,\n    \"Cutout\": A.Cutout,\n    \"ToTensorV2\": ToTensorV2,\n\n    # # Model\n    \"BasicImageModel\": BasicImageModel,\n\n    # # Optimizer\n    \"AdamW\": optim.AdamW,\n\n    # # Scheduler\n    \"OneCycleLR\": lr_scheduler.OneCycleLR,\n\n    # # Loss,Metric\n    \"BCEWithLogitsLoss\": nn.BCEWithLogitsLoss,\n    \"ROCAUC\": ROCAUC,\n\n    # # Metric Wrapper\n    \"micro_average\": micro_average,\n    \"calc_across_all_batchs\": calc_across_all_batchs,\n\n    # # PPE Extensions\n    \"ExtensionsManager\": ppe.training.ExtensionsManager,\n\n    \"observe_lr\": ppe_exts.observe_lr,\n    \"LogReport\": ppe_exts.LogReport,\n    \"PlotReport\": ppe_exts.PlotReport,\n    \"PrintReport\": ppe_exts.PrintReport,\n    \"PrintReportNotebook\": ppe_exts.PrintReportNotebook,\n    \"ProgressBar\": ppe_exts.ProgressBar,\n    \"ProgressBarNotebook\": ppe_exts.ProgressBarNotebook,\n    \"snapshot\": ppe_exts.snapshot,\n    \"LRScheduler\": ppe_exts.LRScheduler, \n\n    \"MinValueTrigger\": ppe_triggers.MinValueTrigger,\n    \"MaxValueTrigger\": ppe_triggers.MaxValueTrigger,\n    \"EarlyStoppingTrigger\": ppe_triggers.EarlyStoppingTrigger,\n}","34459f03":"def get_path_label(cfg: Config, train_all: pd.DataFrame):\n    \"\"\"Get file path and target info.\"\"\"\n    use_fold = cfg[\"\/globals\/val_fold\"]\n\n    train_df = train_all[train_all[\"fold\"] != use_fold]\n    val_df = train_all[train_all[\"fold\"] == use_fold]\n    \n    train_path_label = {\n        \"paths\": [TRAIN \/ f\"{img_id[0]}\/{img_id}.npy\" for img_id in train_df[\"id\"].values],\n        \"labels\": train_df[CLASSES].values.astype(\"f\")}\n    val_path_label = {\n        \"paths\": [TRAIN \/ f\"{img_id[0]}\/{img_id}.npy\" for img_id in val_df[\"id\"].values],\n        \"labels\": val_df[CLASSES].values.astype(\"f\")\n    }\n    return train_path_label, val_path_label\n\n\ndef run_inference_loop(cfg, model, loader, device):\n    model.to(device)\n    model.eval()\n    pred_list = []\n    with torch.no_grad():\n        for batch in tqdm(loader):\n            x = to_device(batch[\"image\"], device)\n            y = model(x)\n            pred_list.append(y.sigmoid().detach().cpu().numpy())\n        \n    pred_arr = np.concatenate(pred_list)\n    del pred_list\n    return pred_arr","dbafc9a4":"# # debug\n# smpl_sub = smpl_sub.iloc[:32 * 10, :].copy()","988ee726":"torch.backends.cudnn.benchmark = True\nset_random_seed(1086, deterministic=True)\ndevice = torch.device(\"cuda\")\n\nlabel_arr = train[CLASSES].values\noof_pred_arr = np.zeros((len(train), N_CLASSES))\ntest_pred_arr = np.zeros((N_FOLDS, len(smpl_sub), N_CLASSES))\nscore_list = []\n\ntest_path_label = {\n    \"paths\": [DATA \/ f\"test\/{img_id[0]}\/{img_id}.npy\" for img_id in smpl_sub[\"id\"].values],\n    \"labels\": smpl_sub[CLASSES].values.astype(\"f\")\n}","6d78157f":"for fold_id in FOLDS:\n    print(f\"\\n[fold {fold_id}]\")\n    tmp_dir = TRAINING_OUTPUT_PATH \/ f\"fold{fold_id}\"\n    with open(tmp_dir \/ \"config.yml\", \"r\") as fr:\n        cfg = Config(yaml.safe_load(fr), types=CONFIG_TYPES)\n    val_idx = train.query(\"fold == @fold_id\").index.values\n\n    # # get_dataloader\n    _, val_path_label = get_path_label(cfg, train)\n    cfg[\"\/dataset\/val\"].lazy_init(**val_path_label)\n    cfg[\"\/dataset\/test\"].lazy_init(**test_path_label)\n    val_loader = cfg[\"\/loader\/val\"]\n    test_loader = cfg[\"\/loader\/test\"]\n    \n    # # get model\n    model_path = TRAINING_OUTPUT_PATH \/ f\"best_metric_model_fold{fold_id}.pth\"\n    model = cfg[\"\/model\"]\n    model.load_state_dict(torch.load(model_path, map_location=device))\n    \n    # # inference\n    val_pred = run_inference_loop(cfg, model, val_loader, device)\n    val_score = roc_auc_score(label_arr[val_idx], val_pred)\n    oof_pred_arr[val_idx] = val_pred\n    score_list.append([fold_id, val_score])\n    \n    test_pred_arr[fold_id] = run_inference_loop(cfg, model, test_loader, device)\n    \n    del cfg, val_idx, val_path_label\n    del model, val_loader, test_loader\n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    print(f\"[fold {fold_id}] val score: {val_score:.4f}\\n\")","c2c12f4e":"oof_score = roc_auc_score(label_arr, oof_pred_arr)\nscore_list.append([\"oof\", oof_score])\npd.DataFrame(score_list, columns=[\"fold\", \"metric\"])","bb4f1ea1":"oof_df = train.copy()\noof_df[CLASSES] = oof_pred_arr\noof_df.to_csv(\".\/oof_prediction.csv\", index=False)","45befec9":"sub_df = smpl_sub.copy()\nsub_df[CLASSES] = test_pred_arr.mean(axis=0)\nsub_df.to_csv(\".\/submission.csv\", index=False)\nsub_df.head()","766c8a5d":"## Check oof score","a7121a89":"### Utils","8f207ef9":"# EOF","ec2bab9d":"### Dataset","2da0a4ea":"## Read Data, Split folds","496d9cc0":"# About\n\nHere I'm trying to train _VOLO_: Vision Outlooker for Visual Recognition.  \nIt is a new vison transformer which achinved good results with fewer parameters than others.   \n\npaper: https:\/\/arxiv.org\/abs\/2106.13112v1  \ncode: https:\/\/github.com\/sail-sg\/volo  \n\nI uploaded official code and pretrained weights on Kaggle Datasets:  \nhttps:\/\/www.kaggle.com\/ttahara\/volo-package\n\nI make some changes for simply training binary classification task.\n\n<br>\nI have prepared the training and inference notebooks because the training time per fold is longer than resnet18d.\n  \n  \nThis is an **inference** notebook.\n\n* model:\n  * backbone: volo_d1\n  * head classifier: one linear layer\n  * num of input channels: **1**\n* input image:\n  * size: 1x256x256\n  * use only on-target ('A') observations\n  \n  \n  ```python\n  img = np.load(path)[[0, 2, 4]]          # shape: (3, 273, 256)\n  img = np.vstack(img)                    # shape: (819, 256)\n  img = img.transpose(1, 0)               # shape: (256, 819)\n  ```\n\n\n* CVStrategy: Stratified KFold(K=5)\n\nIf you want to know more details of experimental settings, see training notebooks:  \n[fold0](https:\/\/www.kaggle.com\/ttahara\/rerun-seti-e-t-volo-d1-baseline-training?scriptVersionId=68249988), [fold1](https:\/\/www.kaggle.com\/ttahara\/rerun-seti-e-t-volo-d1-baseline-training?scriptVersionId=68250008), [fold2](https:\/\/www.kaggle.com\/ttahara\/rerun-seti-e-t-volo-d1-baseline-training?scriptVersionId=68281727), [fold3](https:\/\/www.kaggle.com\/ttahara\/rerun-seti-e-t-volo-d1-baseline-training?scriptVersionId=68281737), [fold4](https:\/\/www.kaggle.com\/ttahara\/rerun-seti-e-t-volo-d1-baseline-training?scriptVersionId=68335611)","a194ba08":"# Prapere","9161effa":"## Import","d67b6901":"### Metric","94779a72":"## Make submission","c080f102":"## ","2a73290b":"### Model","71b1ec6b":"## Install","87419980":"## Definition","75771454":"## config_types for evaluating configuration\n\nI use [pytorch-pfn-extras](https:\/\/github.com\/pfnet\/pytorch-pfn-extras) for training NNs. This library has useful config systems but requires some preparation.\n\nFor more details, see [docs](https:\/\/github.com\/pfnet\/pytorch-pfn-extras\/blob\/master\/docs\/config.md).","9592edf5":"## Run inference for each fold","fd1765c6":"# Inference"}}