{"cell_type":{"3a9dda56":"code","67860d9f":"code","75cff542":"code","2756ed8e":"code","4d18da32":"code","0e5c1cae":"code","b818354f":"code","2dcc2866":"code","e0ef512a":"code","88289b9e":"code","b1ff3b51":"code","957ddcdf":"code","d0d732a0":"code","43f3c3b9":"code","807bc8d7":"code","98d2d1cb":"code","0ee22849":"code","0a8ee3d3":"code","c856ace5":"code","6f93e11b":"code","c1e85063":"markdown","b2f75ada":"markdown","959c580b":"markdown","81fa91ad":"markdown","be24861f":"markdown","dd56149f":"markdown","2a990ac0":"markdown","1b9c9339":"markdown","07a26335":"markdown"},"source":{"3a9dda56":"# Bibliotecas necess\u00e1rias\n# Manipula\u00e7\u00e3o de dados\nimport pandas as pd\n# Redes Neurais\nfrom tensorflow import keras\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten,AveragePooling2D\nfrom tensorflow.keras.optimizers import RMSprop\n# Plot\nimport matplotlib.pyplot as plt\n%matplotlib inline\n# Avalia\u00e7\u00e3o\nfrom sklearn.metrics import classification_report, confusion_matrix\n","67860d9f":"# Lendo o dataset Kaggle\ntrain = pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\")\n\n","75cff542":"# Analisando o dataset\nprint(\"Quantidade de elementos de treino: {}\". format(len(train)))\nprint(train.head())","2756ed8e":"# Separando x_train e y_train\nY = train[\"label\"]\nX = train.drop(labels = [\"label\"],axis = 1)\nprint(X.head())\n","4d18da32":"print(X.shape)","0e5c1cae":"# com matplotlib\n#vamos exemplificar com um dos dados\nplt.imshow(X.values[3].reshape(28,28), cmap=plt.cm.binary)\nplt.show()\nprint('Label: {}'.format(Y[3]))","b818354f":"# Transformando a imagem 2d em um numpy array (imagem 28*28)\nx = X.values.reshape(42000, 28, 28, 1)\n\n#Normalizando para valores entre 0 e 1\nx = x.astype('float32')\nx \/= 255\n\n","2dcc2866":"# Vamos ajustar o formato da saida\nnum_classes = 10\n\n# Convertendo para um vetor de saida com 10 dimensoes\n# ex. 8 => [0,0,0,0,0,0,0,0,1,0]\ny = keras.utils.to_categorical(Y, num_classes)\nprint(y[0])","e0ef512a":"# Separando uma parte para treino (90%) e outra para valida\u00e7\u00e3o (10%)\nfrom sklearn.model_selection import train_test_split\nx_train, x_val, y_train, y_val = train_test_split(x, y, test_size = 0.1, random_state=5)\nprint('Qtde de treinos: {}'.format(len(x_train)))\nprint('Qtde de valida\u00e7\u00f5es: {}'.format(len(x_val)))","88289b9e":"# Criando o modelo Sequential\n# Sequential: Modelo Keras de ir adicionando camadas (como um lego)\n# Conv2D: Camada com kernels (filtros) que percorrem a imagem extraindo caracter\u00edsitcas (mapas de caracte\u00edsticas)\n# MaxPooling2D: Camada que reduz a dimensionalidade dos mapas de caracter\u00edsticas 2D\n# Flatten: Camada que transforma um mapa de caracter\u00edsticas 2D num vetor para classficador final\n# Dense: Camada onde todas as entradas est\u00e3o conectadas em cada neur\u00f4nio (totalmente conectada)\n# Dropout: Camada usa durante treino que descarta aleatoriamente um percentual de conex\u00f5es (reduz overfitting)\n\nmodel = Sequential()\nmodel.add(Conv2D(25, kernel_size=(2,2),\n                 activation='relu',\n                 input_shape=(28,28,1),padding='same',dilation_rate=(2)))\n\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Conv2D(30, kernel_size=(2,2), activation='relu',dilation_rate=(2)))\n\nmodel.add(Conv2D(45, kernel_size=(2,2), activation='relu',padding='same',dilation_rate=(2)))\n\nmodel.add(Conv2D(25, kernel_size=(2,2), activation='relu',dilation_rate=(2)))\n\nmodel.add(Flatten())\nmodel.add(Dense(50, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(num_classes, activation='softmax'))\nmodel.summary()","b1ff3b51":"# Compila o modelo\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=RMSprop(),\n              metrics=['accuracy'])","957ddcdf":"# Treina com os parte dos dados\nbatch_size = 32\nepochs = 20\n\n#Salvar o melhor modelo\ncallbacks_list = [\n    keras.callbacks.ModelCheckpoint(\n        filepath='model.h5',\n        monitor='val_loss', save_best_only=True, verbose=1),\n    keras.callbacks.EarlyStopping(monitor='val_loss', patience=10,verbose=1)\n]\n\nhistory = model.fit(x_train, y_train,\n                    batch_size=batch_size,\n                    epochs=epochs,\n                    callbacks = callbacks_list,\n                    verbose=1,\n                    validation_data=(x_val, y_val))","d0d732a0":"#Vamos ver como foi o treino?\n\nfig, ax = plt.subplots(1,2, figsize=(16,8))\nax[0].plot(history.history['loss'], color='b', label=\"Training loss\")\nax[0].plot(history.history['val_loss'], color='r', label=\"validation loss\",axes =ax[0])\nlegend = ax[0].legend(loc='best', shadow=True)\n\nax[1].plot(history.history['accuracy'], color='b', label=\"Training accuracy\")\nax[1].plot(history.history['val_accuracy'], color='r',label=\"Validation accuracy\")\nlegend = ax[1].legend(loc='best', shadow=True)","43f3c3b9":"from tensorflow.keras.models import load_model\n# Load the best saved model\nmodel = load_model('model.h5')","807bc8d7":"# Testa\nscore = model.evaluate(x_val, y_val, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])","98d2d1cb":"# Testando uma entrada qualquer\nprint(y_train[10])\nprint(model.predict(x_train[10].reshape((1,28,28,1))))\nprint(model.predict_classes(x_train[10].reshape((1,28,28,1))))","0ee22849":"import itertools\n\n#Plot the confusion matrix. Set Normalize = True\/False\ndef plot_confusion_matrix(cm, classes, normalize=True, title='Confusion matrix', cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.figure(figsize=(10,10))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        cm = np.around(cm, decimals=2)\n        cm[np.isnan(cm)] = 0.0\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","0a8ee3d3":"# Vendo alguns reports# Vendo alguns reports\n# Usando sklearn\nimport numpy as np\n\n# Classificando toda base de teste\ny_pred = model.predict_classes(x_val)\n# voltando pro formato de classes\ny_test_c = np.argmax(y_val, axis=1)\ntarget_names = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n\n#Confution Matrix\ncm = confusion_matrix(y_test_c, y_pred)\nplot_confusion_matrix(cm, target_names, normalize=False, title='Confusion Matrix')\n\nprint('Classification Report')\nprint(classification_report(y_test_c, y_pred, target_names=target_names))","c856ace5":"# Gerando sa\u00edda para dataset de teste\n\n#Carrega dataset de teste\ntest = pd.read_csv(\"..\/input\/digit-recognizer\/test.csv\")\nprint(\"Qtde de testes: {}\".format(len(test)))\n# Bota no formato numpy e normaliza\nx_test = test.values.reshape(len(test),28,28,1)\nx_test = x_test.astype('float32')\nx_test \/= 255\n\n# Faz classifica\u00e7\u00e3o para dataset de teste\ny_pred = model.predict_classes(x_test)\n\n# Verficando algum exemplo\ni = 0\nplt.imshow(test.values[i].reshape(28,28), cmap=plt.cm.binary)\nplt.show()\nprint('Previsto: {}'.format(y_pred[i]))\n\n# Botando no formato de sa\u00edda (competi\u00e7\u00e3o Kaggle)\nresults = pd.Series(y_pred,name=\"Label\")\nsubmission = pd.concat([pd.Series(range(1,len(y_pred)+1),name = \"ImageId\"),results],axis = 1)\nprint(submission.head(10))\n#Salvando Arquivo\nsubmission.to_csv(\"mlp_mnist_v1.csv\",index=False)","6f93e11b":"#introduzindo ruido\nimport numpy as np\nmean = 0.1\nstddev = 0.21\nnoise = np.random.normal(mean, stddev, (4200, 28, 28,1))\nx_te = x_val + noise\nx_te = np.clip(x_te, 0., 1.)\n\nplt.imshow(x_te.reshape(4200, 28,28)[0], cmap=plt.cm.binary)\nplt.show()\n\n# Testa\nscore = model.evaluate(x_te, y_val, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])","c1e85063":"# Atividade realizada\nAtividade voltada para obten\u00e7\u00e3o parcial de nota na disciplina de T\u00f3picos Especiais II, ministrada pelo Prof. Maur\u00edcio Figueiredo. \n\nForam realizadas modifica\u00e7\u00f5es no c\u00f3digo apresentado pelo professor em aula, com intuito de testar outras estruturas e par\u00e2metros para aferir seus resultados.\n\nAluno: Wilbert Lu\u00eds E. Marins","b2f75ada":"# Teste Adicional: Com ru\u00eddo","959c580b":"# Gerando Sa\u00edda","81fa91ad":"# Modifica\u00e7\u00f5es efetuadas\n\n---\n* Inser\u00e7\u00e3o de novas convolu\u00e7\u00f5es, amplia\u00e7\u00e3o de par\u00e2metros utilizados com adi\u00e7\u00e3o do processo de \"padding\" com preenchimento do tipo Zero nos dados analisados; \n* Uso do filtro 2x2;\n* Poolings alternados de 2 tipos diferentes (Max e Avg)\n* Uso de dilata\u00e7\u00e3o no filtro \n* Em resumo o processo de cria\u00e7\u00e3o do modelo ficou: Conv->MaxPooling->Conv->AvgPooling->Conv->MaxPooling->Conv\n","be24861f":"# Bibliotecas e Dados","dd56149f":"# Considera\u00e7\u00f5es sobre o resultado obtido","2a990ac0":"# Criando e treinando o Modelo","1b9c9339":"Foram realizados v\u00e1rios testes at\u00e9 chegar a essa acur\u00e1cia ,a partir da altern\u00e2ncia da fun\u00e7\u00e3o de ativa\u00e7\u00e3o, filtros, uso ou n\u00e3o do padding, modifica\u00e7\u00e3o de outros par\u00e2metros e a realiza\u00e7\u00e3o de mais convolu\u00e7\u00f5es at\u00e9 que convergisse a esse resultado.\n\nNo caso especial do teste adicional com grande ru\u00eddo, teve um queda consider\u00e1vel na acur\u00e1cia dos exemplos de valida\u00e7\u00e3o, mas que reflete um bom valor para o dado nesse estado.\n","07a26335":"# Avaliando o Modelo"}}