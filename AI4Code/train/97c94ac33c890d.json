{"cell_type":{"29c4db53":"code","1f9b47c0":"code","2fdc6065":"code","53399222":"code","5f25e8db":"code","b3059018":"code","fec4b7e9":"code","25ba6814":"code","04e083c5":"code","5c5e828e":"code","98c7f7de":"code","706e05f9":"code","0286ddaf":"code","72d136fd":"code","1453fc57":"code","e4aff45a":"code","5aa84075":"code","2bbf0990":"code","0577a85c":"code","156d4d34":"code","291743dc":"code","090a100f":"code","fbbaf90f":"code","d8edd5c7":"code","d72072c4":"code","e781f58d":"code","15d9cf0f":"code","1e60b87d":"code","fd34071b":"code","2d9bab12":"code","63612bb3":"markdown","0a92123f":"markdown","870ad72a":"markdown","3991854e":"markdown","d04b92f3":"markdown","826ca88a":"markdown","63891ddb":"markdown","fde4dfb6":"markdown","854e9f60":"markdown","caa5dac2":"markdown","01f4bd3c":"markdown","26bf98f1":"markdown","03146267":"markdown","94229278":"markdown","3e1c0bfc":"markdown","790e18e9":"markdown","679d67e1":"markdown","9f5622b8":"markdown","eeb8a11f":"markdown","3112a8e7":"markdown","a719d19c":"markdown","eda1939e":"markdown","a9e1d6eb":"markdown","eff64972":"markdown","a5242939":"markdown","7396a719":"markdown","bfb9a539":"markdown","5784cde1":"markdown","0a8bfd49":"markdown","435e649c":"markdown","5a7695b4":"markdown","c24f5489":"markdown","c7ddcd0a":"markdown","d8d90b0c":"markdown","8febc3b4":"markdown"},"source":{"29c4db53":"from __future__ import absolute_import, division, print_function\n\n# TensorFlow and tf.keras\nimport tensorflow as tf\nfrom tensorflow import keras\n\n# Helper libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nprint(tf.__version__)","1f9b47c0":"fashion_mnist = keras.datasets.fashion_mnist\n\n(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()","2fdc6065":"class_names = ['T-shirt\/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']","53399222":"train_images.shape","5f25e8db":"len(train_labels)","b3059018":"train_labels","fec4b7e9":"test_images.shape","25ba6814":"len(test_labels)","04e083c5":"plt.figure()\nplt.imshow(train_images[0])\nplt.colorbar()\nplt.grid(False)\nplt.show()","5c5e828e":"train_images = train_images \/ 255.0\n\ntest_images = test_images \/ 255.0","98c7f7de":"plt.figure(figsize=(10,10))\nfor i in range(25):\n    plt.subplot(5,5,i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    plt.imshow(train_images[i], cmap=plt.cm.binary)\n    plt.xlabel(class_names[train_labels[i]])\nplt.show()","706e05f9":"model = keras.Sequential([\n    keras.layers.Flatten(input_shape=(28, 28)),\n    keras.layers.Dense(128, activation=tf.nn.relu),\n    keras.layers.Dense(10, activation=tf.nn.softmax)\n])","0286ddaf":"model.compile(optimizer='adam', \n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])","72d136fd":"model.fit(train_images, train_labels, epochs=5)","1453fc57":"test_loss, test_acc = model.evaluate(test_images, test_labels)\n\nprint('Test accuracy:', test_acc)","e4aff45a":"predictions = model.predict(test_images)","5aa84075":"predictions[0]","2bbf0990":"np.argmax(predictions[0])","0577a85c":"test_labels[0]","156d4d34":"def plot_image(i, predictions_array, true_label, img):\n  predictions_array, true_label, img = predictions_array[i], true_label[i], img[i]\n  plt.grid(False)\n  plt.xticks([])\n  plt.yticks([])\n  \n  plt.imshow(img, cmap=plt.cm.binary)\n\n  predicted_label = np.argmax(predictions_array)\n  if predicted_label == true_label:\n    color = 'blue'\n  else:\n    color = 'red'\n  \n  plt.xlabel(\"{} {:2.0f}% ({})\".format(class_names[predicted_label],\n                                100*np.max(predictions_array),\n                                class_names[true_label]),\n                                color=color)\n\ndef plot_value_array(i, predictions_array, true_label):\n  predictions_array, true_label = predictions_array[i], true_label[i]\n  plt.grid(False)\n  plt.xticks([])\n  plt.yticks([])\n  thisplot = plt.bar(range(10), predictions_array, color=\"#777777\")\n  plt.ylim([0, 1]) \n  predicted_label = np.argmax(predictions_array)\n \n  thisplot[predicted_label].set_color('red')\n  thisplot[true_label].set_color('blue')","291743dc":"i = 0\nplt.figure(figsize=(6,3))\nplt.subplot(1,2,1)\nplot_image(i, predictions, test_labels, test_images)\nplt.subplot(1,2,2)\nplot_value_array(i, predictions,  test_labels)\nplt.show()","090a100f":"i = 12\nplt.figure(figsize=(6,3))\nplt.subplot(1,2,1)\nplot_image(i, predictions, test_labels, test_images)\nplt.subplot(1,2,2)\nplot_value_array(i, predictions,  test_labels)\nplt.show()","fbbaf90f":"# Plot the first X test images, their predicted label, and the true label\n# Color correct predictions in blue, incorrect predictions in red\nnum_rows = 5\nnum_cols = 3\nnum_images = num_rows*num_cols\nplt.figure(figsize=(2*2*num_cols, 2*num_rows))\nfor i in range(num_images):\n  plt.subplot(num_rows, 2*num_cols, 2*i+1)\n  plot_image(i, predictions, test_labels, test_images)\n  plt.subplot(num_rows, 2*num_cols, 2*i+2)\n  plot_value_array(i, predictions, test_labels)\nplt.show()","d8edd5c7":"# Grab an image from the test dataset\nimg = test_images[0]\n\nprint(img.shape)","d72072c4":"# Add the image to a batch where it's the only member.\nimg = (np.expand_dims(img,0))\n\nprint(img.shape)","e781f58d":"predictions_single = model.predict(img)\n\nprint(predictions_single)","15d9cf0f":"plot_value_array(0, predictions_single, test_labels)\n_ = plt.xticks(range(10), class_names, rotation=45)","1e60b87d":"np.argmax(predictions_single[0])","fd34071b":"##### Copyright 2018 The TensorFlow Authors.\n\n#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https:\/\/www.apache.org\/licenses\/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.","2d9bab12":"#@title MIT License\n#\n# Copyright (c) 2017 Fran\u00e7ois Chollet\n#\n# Permission is hereby granted, free of charge, to any person obtaining a\n# copy of this software and associated documentation files (the \"Software\"),\n# to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n# and\/or sell copies of the Software, and to permit persons to whom the\n# Software is furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL\n# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n# DEALINGS IN THE SOFTWARE.","63612bb3":"## License","0a92123f":"`model.predict` returns a list of lists, one for each image in the batch of data. Grab the predictions for our (only) image in the batch:","870ad72a":"## Import the Fashion MNIST dataset\n\nThis guide uses the [Fashion MNIST](https:\/\/github.com\/zalandoresearch\/fashion-mnist) dataset which contains 70,000 grayscale images in 10 categories. The images show individual articles of clothing at low resolution (28 by 28 pixels), as seen here:\n\n<table>\n  <tr><td>\n    <img src=\"https:\/\/tensorflow.org\/images\/fashion-mnist-sprite.png\"\n         alt=\"Fashion MNIST sprite\"  width=\"600\">\n  <\/td><\/tr>\n  <tr><td align=\"center\">\n    <b>Figure 1.<\/b> <a href=\"https:\/\/github.com\/zalandoresearch\/fashion-mnist\">Fashion-MNIST samples<\/a> (by Zalando, MIT License).<br\/>&nbsp;\n  <\/td><\/tr>\n<\/table>\n\nFashion MNIST is intended as a drop-in replacement for the classic [MNIST](http:\/\/yann.lecun.com\/exdb\/mnist\/) dataset\u2014often used as the \"Hello, World\" of machine learning programs for computer vision. The MNIST dataset contains images of handwritten digits (0, 1, 2, etc) in an identical format to the articles of clothing we'll use here.\n\nThis guide uses Fashion MNIST for variety, and because it's a slightly more challenging problem than regular MNIST. Both datasets are relatively small and are used to verify that an algorithm works as expected. They're good starting points to test and debug code. \n\nWe will use 60,000 images to train the network and 10,000 images to evaluate how accurately the network learned to classify images. You can access the Fashion MNIST directly from TensorFlow, just import and load the data:","3991854e":"Let's plot several images with their predictions. Correct prediction labels are blue and incorrect prediction labels are red. The number gives the percent (out of 100) for the predicted label. Note that it can be wrong even when very confident. ","d04b92f3":"# Basic Fashion MNIST Classification from TensorFlow Docs\n## A look at neural network: basic classification using tf.keras\n\nThis notebook trains a neural network model to classify images of clothing, like sneakers and shirts. It's okay if you don't understand all the details, this is a fast-paced overview of a complete TensorFlow program with the details explained as we go.\n\nThis notebook uses [tf.keras](https:\/\/www.tensorflow.org\/guide\/keras), a high-level API to build and train models in TensorFlow.","826ca88a":"\n  ## Awesome Computer Vision CV with Deep Learning - Part I\n\n    The part 1 has a curated list of deep learning resources for computer vision as follows,\n\n## Table of Contents -  Part I\n\n- [Papers](#papers)\n  - [ImageNet Classification](#imagenet-classification)\n  - [Object Detection](#object-detection)\n  - [Object Tracking](#object-tracking)\n  - [Low-Level Vision](#low-level-vision)\n    - [Super-Resolution](#super-resolution)\n    - [Other Applications](#other-applications)\n  - [Edge Detection](#edge-detection)\n  - [Semantic Segmentation](#semantic-segmentation)\n  - [Visual Attention and Saliency](#visual-attention-and-saliency)\n  - [Object Recognition](#object-recognition)\n  - [Human Pose Estimation](#human-pose-estimation)\n  - [Understanding CNN](#understanding-cnn)\n  - [Image and Language](#image-and-language)\n    - [Image Captioning](#image-captioning)\n    - [Video Captioning](#video-captioning)\n    - [Question Answering](#question-answering)\n  - [Image Generation](#image-generation)\n  - [Other Topics](#other-topics)\n- [Courses](#courses)\n- [Books](#books)\n- [Videos](#videos)\n- [Software](#software)\n  - [Framework](#framework)\n  - [Applications](#applications)\n- [Tutorials](#tutorials)\n- [Blogs](#blogs)\n\n\n## Papers\n\n### ImageNet Classification\n![classification](https:\/\/cloud.githubusercontent.com\/assets\/5226447\/8451949\/327b9566-2022-11e5-8b34-53b4a64c13ad.PNG)\n(from Alex Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton, ImageNet Classification with Deep Convolutional Neural Networks, NIPS, 2012.)\n* Microsoft (Deep Residual Learning) [[Paper](http:\/\/arxiv.org\/pdf\/1512.03385v1.pdf)][[Slide](http:\/\/image-net.org\/challenges\/talks\/ilsvrc2015_deep_residual_learning_kaiminghe.pdf)]\n  * Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Deep Residual Learning for Image Recognition, arXiv:1512.03385.\n* Microsoft (PReLu\/Weight Initialization) [[Paper]](http:\/\/arxiv.org\/pdf\/1502.01852)\n  * Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification, arXiv:1502.01852.\n* Batch Normalization [[Paper]](http:\/\/arxiv.org\/pdf\/1502.03167)\n  * Sergey Ioffe, Christian Szegedy, Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift, arXiv:1502.03167.\n* GoogLeNet [[Paper]](http:\/\/arxiv.org\/pdf\/1409.4842)\n  * Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich, CVPR, 2015.\n* VGG-Net [[Web]](http:\/\/www.robots.ox.ac.uk\/~vgg\/research\/very_deep\/) [[Paper]](http:\/\/arxiv.org\/pdf\/1409.1556)\n  * Karen Simonyan and Andrew Zisserman, Very Deep Convolutional Networks for Large-Scale Visual Recognition, ICLR, 2015.\n* AlexNet [[Paper]](http:\/\/papers.nips.cc\/book\/advances-in-neural-information-processing-systems-25-2012)\n  * Alex Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton, ImageNet Classification with Deep Convolutional Neural Networks, NIPS, 2012.\n\n### Object Detection\n![object_detection](https:\/\/cloud.githubusercontent.com\/assets\/5226447\/8452063\/f76ba500-2022-11e5-8db1-2cd5d490e3b3.PNG)\n(from Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun, Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks, arXiv:1506.01497.)\n\n* PVANET [[Paper]](https:\/\/arxiv.org\/pdf\/1608.08021) [[Code]](https:\/\/github.com\/sanghoon\/pva-faster-rcnn)\n  * Kye-Hyeon Kim, Sanghoon Hong, Byungseok Roh, Yeongjae Cheon, Minje Park, PVANET: Deep but Lightweight Neural Networks for Real-time Object Detection, arXiv:1608.08021\n* OverFeat, NYU [[Paper]](http:\/\/arxiv.org\/pdf\/1312.6229.pdf)\n  * OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks, ICLR, 2014.\n* R-CNN, UC Berkeley [[Paper-CVPR14]](http:\/\/www.cv-foundation.org\/openaccess\/content_cvpr_2014\/papers\/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf) [[Paper-arXiv14]](http:\/\/arxiv.org\/pdf\/1311.2524)\n  * Ross Girshick, Jeff Donahue, Trevor Darrell, Jitendra Malik, Rich feature hierarchies for accurate object detection and semantic segmentation, CVPR, 2014.\n* SPP, Microsoft Research [[Paper]](http:\/\/arxiv.org\/pdf\/1406.4729)\n  * Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition, ECCV, 2014.\n* Fast R-CNN, Microsoft Research [[Paper]](http:\/\/arxiv.org\/pdf\/1504.08083)\n  * Ross Girshick, Fast R-CNN, arXiv:1504.08083.\n* Faster R-CNN, Microsoft Research [[Paper]](http:\/\/arxiv.org\/pdf\/1506.01497)\n  * Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun, Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks, arXiv:1506.01497.\n* R-CNN minus R, Oxford [[Paper]](http:\/\/arxiv.org\/pdf\/1506.06981)\n  * Karel Lenc, Andrea Vedaldi, R-CNN minus R, arXiv:1506.06981.\n* End-to-end people detection in crowded scenes [[Paper]](http:\/\/arxiv.org\/abs\/1506.04878)\n  * Russell Stewart, Mykhaylo Andriluka, End-to-end people detection in crowded scenes, arXiv:1506.04878.\n* You Only Look Once: Unified, Real-Time Object Detection [[Paper]](http:\/\/arxiv.org\/abs\/1506.02640), [[Paper Version 2]](https:\/\/arxiv.org\/abs\/1612.08242), [[C Code]](https:\/\/github.com\/pjreddie\/darknet), [[Tensorflow Code]](https:\/\/github.com\/thtrieu\/darkflow)\n  * Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi, You Only Look Once: Unified, Real-Time Object Detection, arXiv:1506.02640\n  * Joseph Redmon, Ali Farhadi (Version 2)\n* Inside-Outside Net [[Paper]](http:\/\/arxiv.org\/abs\/1512.04143)\n  * Sean Bell, C. Lawrence Zitnick, Kavita Bala, Ross Girshick, Inside-Outside Net: Detecting Objects in Context with Skip Pooling and Recurrent Neural Networks\n* Deep Residual Network (Current State-of-the-Art) [[Paper]](http:\/\/arxiv.org\/abs\/1512.03385)\n  * Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Deep Residual Learning for Image Recognition\n* Weakly Supervised Object Localization with Multi-fold Multiple Instance Learning [[Paper](http:\/\/arxiv.org\/pdf\/1503.00949.pdf)]\n* R-FCN [[Paper]](https:\/\/arxiv.org\/abs\/1605.06409) [[Code]](https:\/\/github.com\/daijifeng001\/R-FCN)\n  * Jifeng Dai, Yi Li, Kaiming He, Jian Sun, R-FCN: Object Detection via Region-based Fully Convolutional Networks\n* SSD [[Paper]](https:\/\/arxiv.org\/pdf\/1512.02325v2.pdf) [[Code]](https:\/\/github.com\/weiliu89\/caffe\/tree\/ssd)\n  * Wei Liu1, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, Alexander C. Berg, SSD: Single Shot MultiBox Detector, arXiv:1512.02325\n* Speed\/accuracy trade-offs for modern convolutional object detectors [[Paper]](https:\/\/arxiv.org\/pdf\/1611.10012v1.pdf)\n  * Jonathan Huang, Vivek Rathod, Chen Sun, Menglong Zhu, Anoop Korattikara, Alireza Fathi, Ian Fischer, Zbigniew Wojna, Yang Song, Sergio Guadarrama, Kevin Murphy, Google Research, arXiv:1611.10012\n\n### Video Classification\n* Nicolas Ballas, Li Yao, Pal Chris, Aaron Courville, \"Delving Deeper into Convolutional Networks for Learning Video Representations\", ICLR 2016. [[Paper](http:\/\/arxiv.org\/pdf\/1511.06432v4.pdf)]\n* Michael Mathieu, camille couprie, Yann Lecun, \"Deep Multi Scale Video Prediction Beyond Mean Square Error\", ICLR 2016. [[Paper](http:\/\/arxiv.org\/pdf\/1511.05440v6.pdf)]\n\n### Object Tracking\n* Seunghoon Hong, Tackgeun You, Suha Kwak, Bohyung Han, Online Tracking by Learning Discriminative Saliency Map with Convolutional Neural Network, arXiv:1502.06796. [[Paper]](http:\/\/arxiv.org\/pdf\/1502.06796)\n* Hanxi Li, Yi Li and Fatih Porikli, DeepTrack: Learning Discriminative Feature Representations by Convolutional Neural Networks for Visual Tracking, BMVC, 2014. [[Paper]](http:\/\/www.bmva.org\/bmvc\/2014\/files\/paper028.pdf)\n* N Wang, DY Yeung, Learning a Deep Compact Image Representation for Visual Tracking, NIPS, 2013. [[Paper]](http:\/\/winsty.net\/papers\/dlt.pdf)\n* Chao Ma, Jia-Bin Huang, Xiaokang Yang and Ming-Hsuan Yang, Hierarchical Convolutional Features for Visual Tracking, ICCV 2015 [[Paper](http:\/\/www.cv-foundation.org\/openaccess\/content_iccv_2015\/papers\/Ma_Hierarchical_Convolutional_Features_ICCV_2015_paper.pdf)] [[Code](https:\/\/github.com\/jbhuang0604\/CF2)]\n* Lijun Wang, Wanli Ouyang, Xiaogang Wang, and Huchuan Lu, Visual Tracking with fully Convolutional Networks, ICCV 2015  [[Paper](http:\/\/202.118.75.4\/lu\/Paper\/ICCV2015\/iccv15_lijun.pdf)] [[Code](https:\/\/github.com\/scott89\/FCNT)]\n* Hyeonseob Namand Bohyung Han, Learning Multi-Domain Convolutional Neural Networks for Visual Tracking, [[Paper](http:\/\/arxiv.org\/pdf\/1510.07945.pdf)] [[Code](https:\/\/github.com\/HyeonseobNam\/MDNet)] [[Project Page](http:\/\/cvlab.postech.ac.kr\/research\/mdnet\/)]\n\n### Low-Level Vision\n\n#### Super-Resolution\n* Iterative Image Reconstruction\n  * Sven Behnke: Learning Iterative Image Reconstruction. IJCAI, 2001. [[Paper]](http:\/\/www.ais.uni-bonn.de\/behnke\/papers\/ijcai01.pdf)\n  * Sven Behnke: Learning Iterative Image Reconstruction in the Neural Abstraction Pyramid. International Journal of Computational Intelligence and Applications, vol. 1, no. 4, pp. 427-438, 2001. [[Paper]](http:\/\/www.ais.uni-bonn.de\/behnke\/papers\/ijcia01.pdf)\n* Super-Resolution (SRCNN) [[Web]](http:\/\/mmlab.ie.cuhk.edu.hk\/projects\/SRCNN.html) [[Paper-ECCV14]](http:\/\/personal.ie.cuhk.edu.hk\/~ccloy\/files\/eccv_2014_deepresolution.pdf) [[Paper-arXiv15]](http:\/\/arxiv.org\/pdf\/1501.00092.pdf)\n  * Chao Dong, Chen Change Loy, Kaiming He, Xiaoou Tang, Learning a Deep Convolutional Network for Image Super-Resolution, ECCV, 2014.\n  * Chao Dong, Chen Change Loy, Kaiming He, Xiaoou Tang, Image Super-Resolution Using Deep Convolutional Networks, arXiv:1501.00092.\n* Very Deep Super-Resolution\n  * Jiwon Kim, Jung Kwon Lee, Kyoung Mu Lee, Accurate Image Super-Resolution Using Very Deep Convolutional Networks, arXiv:1511.04587, 2015. [[Paper]](http:\/\/arxiv.org\/abs\/1511.04587)\n* Deeply-Recursive Convolutional Network\n  * Jiwon Kim, Jung Kwon Lee, Kyoung Mu Lee, Deeply-Recursive Convolutional Network for Image Super-Resolution, arXiv:1511.04491, 2015. [[Paper]](http:\/\/arxiv.org\/abs\/1511.04491)\n* Casade-Sparse-Coding-Network\n  * Zhaowen Wang, Ding Liu, Wei Han, Jianchao Yang and Thomas S. Huang, Deep Networks for Image Super-Resolution with Sparse Prior. ICCV, 2015. [[Paper]](http:\/\/www.ifp.illinois.edu\/~dingliu2\/iccv15\/iccv15.pdf) [[Code]](http:\/\/www.ifp.illinois.edu\/~dingliu2\/iccv15\/)\n* Perceptual Losses for Super-Resolution\n  * Justin Johnson, Alexandre Alahi, Li Fei-Fei, Perceptual Losses for Real-Time Style Transfer and Super-Resolution, arXiv:1603.08155, 2016. [[Paper]](http:\/\/arxiv.org\/abs\/1603.08155) [[Supplementary]](http:\/\/cs.stanford.edu\/people\/jcjohns\/papers\/fast-style\/fast-style-supp.pdf)\n* SRGAN\n  * Christian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, Wenzhe Shi, Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network, arXiv:1609.04802v3, 2016. [[Paper]](https:\/\/arxiv.org\/pdf\/1609.04802v3.pdf)\n* Others\n  * Osendorfer, Christian, Hubert Soyer, and Patrick van der Smagt, Image Super-Resolution with Fast Approximate Convolutional Sparse Coding, ICONIP, 2014. [[Paper ICONIP-2014]](http:\/\/brml.org\/uploads\/tx_sibibtex\/281.pdf)\n\n#### Other Applications\n* Optical Flow (FlowNet) [[Paper]](http:\/\/arxiv.org\/pdf\/1504.06852)\n  * Philipp Fischer, Alexey Dosovitskiy, Eddy Ilg, Philip H\u00e4usser, Caner Haz\u0131rba\u015f, Vladimir Golkov, Patrick van der Smagt, Daniel Cremers, Thomas Brox, FlowNet: Learning Optical Flow with Convolutional Networks, arXiv:1504.06852.\n* Compression Artifacts Reduction [[Paper-arXiv15]](http:\/\/arxiv.org\/pdf\/1504.06993)\n  * Chao Dong, Yubin Deng, Chen Change Loy, Xiaoou Tang, Compression Artifacts Reduction by a Deep Convolutional Network, arXiv:1504.06993.\n* Blur Removal\n  * Christian J. Schuler, Michael Hirsch, Stefan Harmeling, Bernhard Sch\u00f6lkopf, Learning to Deblur, arXiv:1406.7444 [[Paper]](http:\/\/arxiv.org\/pdf\/1406.7444.pdf)\n  * Jian Sun, Wenfei Cao, Zongben Xu, Jean Ponce, Learning a Convolutional Neural Network for Non-uniform Motion Blur Removal, CVPR, 2015 [[Paper]](http:\/\/arxiv.org\/pdf\/1503.00593)\n* Image Deconvolution [[Web]](http:\/\/lxu.me\/projects\/dcnn\/) [[Paper]](http:\/\/lxu.me\/mypapers\/dcnn_nips14.pdf)\n  * Li Xu, Jimmy SJ. Ren, Ce Liu, Jiaya Jia, Deep Convolutional Neural Network for Image Deconvolution, NIPS, 2014.\n* Deep Edge-Aware Filter [[Paper]](http:\/\/jmlr.org\/proceedings\/papers\/v37\/xub15.pdf)\n  * Li Xu, Jimmy SJ. Ren, Qiong Yan, Renjie Liao, Jiaya Jia, Deep Edge-Aware Filters, ICML, 2015.\n* Computing the Stereo Matching Cost with a Convolutional Neural Network [[Paper]](http:\/\/www.cv-foundation.org\/openaccess\/content_cvpr_2015\/papers\/Zbontar_Computing_the_Stereo_2015_CVPR_paper.pdf)\n  * Jure \u017dbontar, Yann LeCun, Computing the Stereo Matching Cost with a Convolutional Neural Network, CVPR, 2015.\n* Colorful Image Colorization Richard Zhang, Phillip Isola, Alexei A. Efros, ECCV, 2016 [[Paper]](http:\/\/arxiv.org\/pdf\/1603.08511.pdf), [[Code]](https:\/\/github.com\/richzhang\/colorization)\n* Ryan Dahl, [[Blog]](http:\/\/tinyclouds.org\/colorize\/)\n* Feature Learning by Inpainting[[Paper]](https:\/\/arxiv.org\/pdf\/1604.07379v1.pdf)[[Code]](https:\/\/github.com\/pathak22\/context-encoder)\n  * Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, Alexei A. Efros, Context Encoders: Feature Learning by Inpainting, CVPR, 2016\n\n### Edge Detection\n![edge_detection](https:\/\/cloud.githubusercontent.com\/assets\/5226447\/8452371\/93ca6f7e-2025-11e5-90f2-d428fd5ff7ac.PNG)\n(from Gedas Bertasius, Jianbo Shi, Lorenzo Torresani, DeepEdge: A Multi-Scale Bifurcated Deep Network for Top-Down Contour Detection, CVPR, 2015.)\n\n* Holistically-Nested Edge Detection [[Paper]](http:\/\/arxiv.org\/pdf\/1504.06375) [[Code]](https:\/\/github.com\/s9xie\/hed)\n  * Saining Xie, Zhuowen Tu, Holistically-Nested Edge Detection, arXiv:1504.06375.\n* DeepEdge [[Paper]](http:\/\/arxiv.org\/pdf\/1412.1123)\n  * Gedas Bertasius, Jianbo Shi, Lorenzo Torresani, DeepEdge: A Multi-Scale Bifurcated Deep Network for Top-Down Contour Detection, CVPR, 2015.\n* DeepContour [[Paper]](http:\/\/mc.eistar.net\/UpLoadFiles\/Papers\/DeepContour_cvpr15.pdf)\n  * Wei Shen, Xinggang Wang, Yan Wang, Xiang Bai, Zhijiang Zhang, DeepContour: A Deep Convolutional Feature Learned by Positive-Sharing Loss for Contour Detection, CVPR, 2015.\n\n### Semantic Segmentation\n![semantic_segmantation](https:\/\/cloud.githubusercontent.com\/assets\/5226447\/8452076\/0ba8340c-2023-11e5-88bc-bebf4509b6bb.PNG)\n(from Jifeng Dai, Kaiming He, Jian Sun, BoxSup: Exploiting Bounding Boxes to Supervise Convolutional Networks for Semantic Segmentation, arXiv:1503.01640.)\n* PASCAL VOC2012 Challenge Leaderboard (01 Sep. 2016)\n  ![VOC2012_top_rankings](https:\/\/cloud.githubusercontent.com\/assets\/3803777\/18164608\/c3678488-7038-11e6-9ec1-74a1542dce13.png)\n  (from PASCAL VOC2012 [leaderboards](http:\/\/host.robots.ox.ac.uk:8080\/leaderboard\/displaylb.php?challengeid=11&compid=6))\n* SEC: Seed, Expand and Constrain\n  *  Alexander Kolesnikov, Christoph Lampert, Seed, Expand and Constrain: Three Principles for Weakly-Supervised Image Segmentation, ECCV, 2016. [[Paper]](http:\/\/pub.ist.ac.at\/~akolesnikov\/files\/ECCV2016\/main.pdf) [[Code]](https:\/\/github.com\/kolesman\/SEC)\n* Adelaide\n  * Guosheng Lin, Chunhua Shen, Ian Reid, Anton van dan Hengel, Efficient piecewise training of deep structured models for semantic segmentation, arXiv:1504.01013. [[Paper]](http:\/\/arxiv.org\/pdf\/1504.01013) (1st ranked in VOC2012)\n  * Guosheng Lin, Chunhua Shen, Ian Reid, Anton van den Hengel, Deeply Learning the Messages in Message Passing Inference, arXiv:1508.02108. [[Paper]](http:\/\/arxiv.org\/pdf\/1506.02108) (4th ranked in VOC2012)\n* Deep Parsing Network (DPN)\n  * Ziwei Liu, Xiaoxiao Li, Ping Luo, Chen Change Loy, Xiaoou Tang, Semantic Image Segmentation via Deep Parsing Network, arXiv:1509.02634 \/ ICCV 2015 [[Paper]](http:\/\/arxiv.org\/pdf\/1509.02634.pdf) (2nd ranked in VOC 2012)\n* CentraleSuperBoundaries, INRIA [[Paper]](http:\/\/arxiv.org\/pdf\/1511.07386)\n  * Iasonas Kokkinos, Surpassing Humans in Boundary Detection using Deep Learning, arXiv:1411.07386 (4th ranked in VOC 2012)\n* BoxSup [[Paper]](http:\/\/arxiv.org\/pdf\/1503.01640)\n  * Jifeng Dai, Kaiming He, Jian Sun, BoxSup: Exploiting Bounding Boxes to Supervise Convolutional Networks for Semantic Segmentation, arXiv:1503.01640. (6th ranked in VOC2012)\n* POSTECH\n  * Hyeonwoo Noh, Seunghoon Hong, Bohyung Han, Learning Deconvolution Network for Semantic Segmentation, arXiv:1505.04366. [[Paper]](http:\/\/arxiv.org\/pdf\/1505.04366) (7th ranked in VOC2012)\n  * Seunghoon Hong, Hyeonwoo Noh, Bohyung Han, Decoupled Deep Neural Network for Semi-supervised Semantic Segmentation, arXiv:1506.04924. [[Paper]](http:\/\/arxiv.org\/pdf\/1506.04924)\n  * Seunghoon Hong,Junhyuk Oh,\tBohyung Han, and\tHonglak Lee, Learning Transferrable Knowledge for Semantic Segmentation with Deep Convolutional Neural Network, arXiv:1512.07928 [[Paper](http:\/\/arxiv.org\/pdf\/1512.07928.pdf)] [[Project Page](http:\/\/cvlab.postech.ac.kr\/research\/transfernet\/)]\n* Conditional Random Fields as Recurrent Neural Networks [[Paper]](http:\/\/arxiv.org\/pdf\/1502.03240)\n  * Shuai Zheng, Sadeep Jayasumana, Bernardino Romera-Paredes, Vibhav Vineet, Zhizhong Su, Dalong Du, Chang Huang, Philip H. S. Torr, Conditional Random Fields as Recurrent Neural Networks, arXiv:1502.03240. (8th ranked in VOC2012)\n* DeepLab\n  * Liang-Chieh Chen, George Papandreou, Kevin Murphy, Alan L. Yuille, Weakly-and semi-supervised learning of a DCNN for semantic image segmentation, arXiv:1502.02734. [[Paper]](http:\/\/arxiv.org\/pdf\/1502.02734) (9th ranked in VOC2012)\n* Zoom-out [[Paper]](http:\/\/www.cv-foundation.org\/openaccess\/content_cvpr_2015\/papers\/Mostajabi_Feedforward_Semantic_Segmentation_2015_CVPR_paper.pdf)\n  * Mohammadreza Mostajabi, Payman Yadollahpour, Gregory Shakhnarovich, Feedforward Semantic Segmentation With Zoom-Out Features, CVPR, 2015\n* Joint Calibration [[Paper]](http:\/\/arxiv.org\/pdf\/1507.01581)\n  * Holger Caesar, Jasper Uijlings, Vittorio Ferrari, Joint Calibration for Semantic Segmentation, arXiv:1507.01581.\n* Fully Convolutional Networks for Semantic Segmentation [[Paper-CVPR15]](http:\/\/www.cv-foundation.org\/openaccess\/content_cvpr_2015\/papers\/Long_Fully_Convolutional_Networks_2015_CVPR_paper.pdf) [[Paper-arXiv15]](http:\/\/arxiv.org\/pdf\/1411.4038)\n  * Jonathan Long, Evan Shelhamer, Trevor Darrell, Fully Convolutional Networks for Semantic Segmentation, CVPR, 2015.\n* Hypercolumn [[Paper]](http:\/\/www.cv-foundation.org\/openaccess\/content_cvpr_2015\/papers\/Hariharan_Hypercolumns_for_Object_2015_CVPR_paper.pdf)\n  * Bharath Hariharan, Pablo Arbelaez, Ross Girshick, Jitendra Malik, Hypercolumns for Object Segmentation and Fine-Grained Localization, CVPR, 2015.\n* Deep Hierarchical Parsing\n  * Abhishek Sharma, Oncel Tuzel, David W. Jacobs, Deep Hierarchical Parsing for Semantic Segmentation, CVPR, 2015. [[Paper]](http:\/\/www.cv-foundation.org\/openaccess\/content_cvpr_2015\/papers\/Sharma_Deep_Hierarchical_Parsing_2015_CVPR_paper.pdf)\n* Learning Hierarchical Features for Scene Labeling [[Paper-ICML12]](http:\/\/yann.lecun.com\/exdb\/publis\/pdf\/farabet-icml-12.pdf) [[Paper-PAMI13]](http:\/\/yann.lecun.com\/exdb\/publis\/pdf\/farabet-pami-13.pdf)\n  * Clement Farabet, Camille Couprie, Laurent Najman, Yann LeCun, Scene Parsing with Multiscale Feature Learning, Purity Trees, and Optimal Covers, ICML, 2012.\n  * Clement Farabet, Camille Couprie, Laurent Najman, Yann LeCun, Learning Hierarchical Features for Scene Labeling, PAMI, 2013.\n* University of Cambridge [[Web]](http:\/\/mi.eng.cam.ac.uk\/projects\/segnet\/)\n  * Vijay Badrinarayanan, Alex Kendall and Roberto Cipolla \"SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation.\" arXiv preprint arXiv:1511.00561, 2015. [[Paper]](http:\/\/arxiv.org\/abs\/1511.00561)\n* Alex Kendall, Vijay Badrinarayanan and Roberto Cipolla \"Bayesian SegNet: Model Uncertainty in Deep Convolutional Encoder-Decoder Architectures for Scene Understanding.\" arXiv preprint arXiv:1511.02680, 2015. [[Paper]](http:\/\/arxiv.org\/abs\/1511.00561)\n* Princeton\n  * Fisher Yu, Vladlen Koltun, \"Multi-Scale Context Aggregation by Dilated Convolutions\", ICLR 2016, [[Paper](http:\/\/arxiv.org\/pdf\/1511.07122v2.pdf)]\n* Univ. of Washington, Allen AI\n  * Hamid Izadinia, Fereshteh Sadeghi, Santosh Kumar Divvala, Yejin Choi, Ali Farhadi, \"Segment-Phrase Table for Semantic Segmentation, Visual Entailment and Paraphrasing\", ICCV, 2015, [[Paper](http:\/\/www.cv-foundation.org\/openaccess\/content_iccv_2015\/papers\/Izadinia_Segment-Phrase_Table_for_ICCV_2015_paper.pdf)]\n* INRIA\n  * Iasonas Kokkinos, \"Pusing the Boundaries of Boundary Detection Using deep Learning\", ICLR 2016, [[Paper](http:\/\/arxiv.org\/pdf\/1511.07386v2.pdf)]\n* UCSB\n  * Niloufar Pourian, S. Karthikeyan, and B.S. Manjunath, \"Weakly supervised graph based semantic segmentation by learning communities of image-parts\", ICCV, 2015, [[Paper](http:\/\/www.cv-foundation.org\/openaccess\/content_iccv_2015\/papers\/Pourian_Weakly_Supervised_Graph_ICCV_2015_paper.pdf)]\n\n### Visual Attention and Saliency\n![saliency](https:\/\/cloud.githubusercontent.com\/assets\/5226447\/8492362\/7ec65b88-2183-11e5-978f-017e45ddba32.png)\n(from Nian Liu, Junwei Han, Dingwen Zhang, Shifeng Wen, Tianming Liu, Predicting Eye Fixations using Convolutional Neural Networks, CVPR, 2015.)\n\n* Mr-CNN [[Paper]](http:\/\/www.cv-foundation.org\/openaccess\/content_cvpr_2015\/papers\/Liu_Predicting_Eye_Fixations_2015_CVPR_paper.pdf)\n  * Nian Liu, Junwei Han, Dingwen Zhang, Shifeng Wen, Tianming Liu, Predicting Eye Fixations using Convolutional Neural Networks, CVPR, 2015.\n* Learning a Sequential Search for Landmarks [[Paper]](http:\/\/www.cv-foundation.org\/openaccess\/content_cvpr_2015\/papers\/Singh_Learning_a_Sequential_2015_CVPR_paper.pdf)\n  * Saurabh Singh, Derek Hoiem, David Forsyth, Learning a Sequential Search for Landmarks, CVPR, 2015.\n* Multiple Object Recognition with Visual Attention [[Paper]](http:\/\/arxiv.org\/pdf\/1412.7755.pdf)\n  * Jimmy Lei Ba, Volodymyr Mnih, Koray Kavukcuoglu, Multiple Object Recognition with Visual Attention, ICLR, 2015.\n* Recurrent Models of Visual Attention [[Paper]](http:\/\/papers.nips.cc\/paper\/5542-recurrent-models-of-visual-attention.pdf)\n  * Volodymyr Mnih, Nicolas Heess, Alex Graves, Koray Kavukcuoglu, Recurrent Models of Visual Attention, NIPS, 2014.\n\n### Object Recognition\n* Weakly-supervised learning with convolutional neural networks [[Paper]](http:\/\/www.cv-foundation.org\/openaccess\/content_cvpr_2015\/papers\/Oquab_Is_Object_Localization_2015_CVPR_paper.pdf)\n  * Maxime Oquab, Leon Bottou, Ivan Laptev, Josef Sivic, Is object localization for free? \u2013 Weakly-supervised learning with convolutional neural networks, CVPR, 2015.\n* FV-CNN [[Paper]](http:\/\/www.cv-foundation.org\/openaccess\/content_cvpr_2015\/papers\/Cimpoi_Deep_Filter_Banks_2015_CVPR_paper.pdf)\n  * Mircea Cimpoi, Subhransu Maji, Andrea Vedaldi, Deep Filter Banks for Texture Recognition and Segmentation, CVPR, 2015.\n\n### Human Pose Estimation\n* Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser Sheikh, Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields, CVPR, 2017.\n* Leonid Pishchulin, Eldar Insafutdinov, Siyu Tang, Bjoern Andres, Mykhaylo Andriluka, Peter Gehler, and Bernt Schiele, Deepcut: Joint subset partition and labeling for multi person pose estimation, CVPR, 2016.\n* Shih-En Wei, Varun Ramakrishna, Takeo Kanade, and Yaser Sheikh, Convolutional pose machines, CVPR, 2016.\n* Alejandro Newell, Kaiyu Yang, and Jia Deng, Stacked hourglass networks for human pose estimation, ECCV, 2016.\n* Tomas Pfister, James Charles, and Andrew Zisserman, Flowing convnets for human pose estimation in videos, ICCV, 2015.\n* Jonathan J. Tompson, Arjun Jain, Yann LeCun, Christoph Bregler, Joint training of a convolutional network and a graphical model for human pose estimation, NIPS, 2014.\n\n### Understanding CNN\n![understanding](https:\/\/cloud.githubusercontent.com\/assets\/5226447\/8452083\/1aaa0066-2023-11e5-800b-2248ead51584.PNG)\n(from Aravindh Mahendran, Andrea Vedaldi, Understanding Deep Image Representations by Inverting Them, CVPR, 2015.)\n\n* Karel Lenc, Andrea Vedaldi, Understanding image representations by measuring their equivariance and equivalence, CVPR, 2015. [[Paper]](http:\/\/www.cv-foundation.org\/openaccess\/content_cvpr_2015\/papers\/Lenc_Understanding_Image_Representations_2015_CVPR_paper.pdf)\n* Anh Nguyen, Jason Yosinski, Jeff Clune, Deep Neural Networks are Easily Fooled:High Confidence Predictions for Unrecognizable Images, CVPR, 2015. [[Paper]](http:\/\/www.cv-foundation.org\/openaccess\/content_cvpr_2015\/papers\/Nguyen_Deep_Neural_Networks_2015_CVPR_paper.pdf)\n* Aravindh Mahendran, Andrea Vedaldi, Understanding Deep Image Representations by Inverting Them, CVPR, 2015. [[Paper]](http:\/\/www.cv-foundation.org\/openaccess\/content_cvpr_2015\/papers\/Mahendran_Understanding_Deep_Image_2015_CVPR_paper.pdf)\n* Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, Antonio Torralba, Object Detectors Emerge in Deep Scene CNNs, ICLR, 2015. [[arXiv Paper]](http:\/\/arxiv.org\/abs\/1412.6856)\n* Alexey Dosovitskiy, Thomas Brox, Inverting Visual Representations with Convolutional Networks, arXiv, 2015. [[Paper]](http:\/\/arxiv.org\/abs\/1506.02753)\n* Matthrew Zeiler, Rob Fergus, Visualizing and Understanding Convolutional Networks, ECCV, 2014. [[Paper]](https:\/\/www.cs.nyu.edu\/~fergus\/papers\/zeilerECCV2014.pdf)\n\n\n### Image and Language\n\n#### Image Captioning\n![image_captioning](https:\/\/cloud.githubusercontent.com\/assets\/5226447\/8452051\/e8f81030-2022-11e5-85db-c68e7d8251ce.PNG)\n(from Andrej Karpathy, Li Fei-Fei, Deep Visual-Semantic Alignments for Generating Image Description, CVPR, 2015.)\n\n* UCLA \/ Baidu [[Paper]](http:\/\/arxiv.org\/pdf\/1410.1090)\n  * Junhua Mao, Wei Xu, Yi Yang, Jiang Wang, Alan L. Yuille, Explain Images with Multimodal Recurrent Neural Networks, arXiv:1410.1090.\n* Toronto [[Paper]](http:\/\/arxiv.org\/pdf\/1411.2539)\n  * Ryan Kiros, Ruslan Salakhutdinov, Richard S. Zemel, Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models, arXiv:1411.2539.\n* Berkeley [[Paper]](http:\/\/arxiv.org\/pdf\/1411.4389)\n  * Jeff Donahue, Lisa Anne Hendricks, Sergio Guadarrama, Marcus Rohrbach, Subhashini Venugopalan, Kate Saenko, Trevor Darrell, Long-term Recurrent Convolutional Networks for Visual Recognition and Description, arXiv:1411.4389.\n* Google [[Paper]](http:\/\/arxiv.org\/pdf\/1411.4555)\n  * Oriol Vinyals, Alexander Toshev, Samy Bengio, Dumitru Erhan, Show and Tell: A Neural Image Caption Generator, arXiv:1411.4555.\n* Stanford [[Web]](http:\/\/cs.stanford.edu\/people\/karpathy\/deepimagesent\/) [[Paper]](http:\/\/cs.stanford.edu\/people\/karpathy\/cvpr2015.pdf)\n  * Andrej Karpathy, Li Fei-Fei, Deep Visual-Semantic Alignments for Generating Image Description, CVPR, 2015.\n* UML \/ UT [[Paper]](http:\/\/arxiv.org\/pdf\/1412.4729)\n  * Subhashini Venugopalan, Huijuan Xu, Jeff Donahue, Marcus Rohrbach, Raymond Mooney, Kate Saenko, Translating Videos to Natural Language Using Deep Recurrent Neural Networks, NAACL-HLT, 2015.\n* CMU \/ Microsoft [[Paper-arXiv]](http:\/\/arxiv.org\/pdf\/1411.5654) [[Paper-CVPR]](http:\/\/www.cs.cmu.edu\/~xinleic\/papers\/cvpr15_rnn.pdf)\n  * Xinlei Chen, C. Lawrence Zitnick, Learning a Recurrent Visual Representation for Image Caption Generation, arXiv:1411.5654.\n  * Xinlei Chen, C. Lawrence Zitnick, Mind\u2019s Eye: A Recurrent Visual Representation for Image Caption Generation, CVPR 2015\n* Microsoft [[Paper]](http:\/\/arxiv.org\/pdf\/1411.4952)\n  * Hao Fang, Saurabh Gupta, Forrest Iandola, Rupesh Srivastava, Li Deng, Piotr Doll\u00e1r, Jianfeng Gao, Xiaodong He, Margaret Mitchell, John C. Platt, C. Lawrence Zitnick, Geoffrey Zweig, From Captions to Visual Concepts and Back, CVPR, 2015.\n* Univ. Montreal \/ Univ. Toronto [[Web](http:\/\/kelvinxu.github.io\/projects\/capgen.html)] [[Paper](http:\/\/www.cs.toronto.edu\/~zemel\/documents\/captionAttn.pdf)]\n  * Kelvin Xu, Jimmy Lei Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhutdinov, Richard S. Zemel, Yoshua Bengio, Show, Attend, and Tell: Neural Image Caption Generation with Visual Attention, arXiv:1502.03044 \/ ICML 2015\n* Idiap \/ EPFL \/ Facebook [[Paper](http:\/\/arxiv.org\/pdf\/1502.03671)]\n  * Remi Lebret, Pedro O. Pinheiro, Ronan Collobert, Phrase-based Image Captioning, arXiv:1502.03671 \/ ICML 2015\n* UCLA \/ Baidu [[Paper](http:\/\/arxiv.org\/pdf\/1504.06692)]\n  * Junhua Mao, Wei Xu, Yi Yang, Jiang Wang, Zhiheng Huang, Alan L. Yuille, Learning like a Child: Fast Novel Visual Concept Learning from Sentence Descriptions of Images, arXiv:1504.06692\n* MS + Berkeley\n  * Jacob Devlin, Saurabh Gupta, Ross Girshick, Margaret Mitchell, C. Lawrence Zitnick, Exploring Nearest Neighbor Approaches for Image Captioning, arXiv:1505.04467 [[Paper](http:\/\/arxiv.org\/pdf\/1505.04467.pdf)]\n  * Jacob Devlin, Hao Cheng, Hao Fang, Saurabh Gupta, Li Deng, Xiaodong He, Geoffrey Zweig, Margaret Mitchell, Language Models for Image Captioning: The Quirks and What Works, arXiv:1505.01809 [[Paper](http:\/\/arxiv.org\/pdf\/1505.01809.pdf)]\n* Adelaide [[Paper](http:\/\/arxiv.org\/pdf\/1506.01144.pdf)]\n  * Qi Wu, Chunhua Shen, Anton van den Hengel, Lingqiao Liu, Anthony Dick, Image Captioning with an Intermediate Attributes Layer, arXiv:1506.01144\n* Tilburg [[Paper](http:\/\/arxiv.org\/pdf\/1506.03694.pdf)]\n  * Grzegorz Chrupala, Akos Kadar, Afra Alishahi, Learning language through pictures, arXiv:1506.03694\n* Univ. Montreal [[Paper](http:\/\/arxiv.org\/pdf\/1507.01053.pdf)]\n  * Kyunghyun Cho, Aaron Courville, Yoshua Bengio, Describing Multimedia Content using Attention-based Encoder-Decoder Networks, arXiv:1507.01053\n* Cornell [[Paper](http:\/\/arxiv.org\/pdf\/1508.02091.pdf)]\n  * Jack Hessel, Nicolas Savva, Michael J. Wilber, Image Representations and New Domains in Neural Image Captioning, arXiv:1508.02091\n* MS + City Univ. of HongKong [[Paper](http:\/\/www.cv-foundation.org\/openaccess\/content_iccv_2015\/papers\/Yao_Learning_Query_and_ICCV_2015_paper.pdf)]\n  * Ting Yao, Tao Mei, and Chong-Wah Ngo, \"Learning Query and Image Similarities\n    with Ranking Canonical Correlation Analysis\", ICCV, 2015\n\n#### Video Captioning\n* Berkeley [[Web]](http:\/\/jeffdonahue.com\/lrcn\/) [[Paper]](http:\/\/arxiv.org\/pdf\/1411.4389.pdf)\n  * Jeff Donahue, Lisa Anne Hendricks, Sergio Guadarrama, Marcus Rohrbach, Subhashini Venugopalan, Kate Saenko, Trevor Darrell, Long-term Recurrent Convolutional Networks for Visual Recognition and Description, CVPR, 2015.\n* UT \/ UML \/ Berkeley [[Paper]](http:\/\/arxiv.org\/pdf\/1412.4729)\n  * Subhashini Venugopalan, Huijuan Xu, Jeff Donahue, Marcus Rohrbach, Raymond Mooney, Kate Saenko, Translating Videos to Natural Language Using Deep Recurrent Neural Networks, arXiv:1412.4729.\n* Microsoft [[Paper]](http:\/\/arxiv.org\/pdf\/1505.01861)\n  * Yingwei Pan, Tao Mei, Ting Yao, Houqiang Li, Yong Rui, Joint Modeling Embedding and Translation to Bridge Video and Language, arXiv:1505.01861.\n* UT \/ Berkeley \/ UML [[Paper]](http:\/\/arxiv.org\/pdf\/1505.00487)\n  * Subhashini Venugopalan, Marcus Rohrbach, Jeff Donahue, Raymond Mooney, Trevor Darrell, Kate Saenko, Sequence to Sequence--Video to Text, arXiv:1505.00487.\n* Univ. Montreal \/ Univ. Sherbrooke [[Paper](http:\/\/arxiv.org\/pdf\/1502.08029.pdf)]\n  * Li Yao, Atousa Torabi, Kyunghyun Cho, Nicolas Ballas, Christopher Pal, Hugo Larochelle, Aaron Courville, Describing Videos by Exploiting Temporal Structure, arXiv:1502.08029\n* MPI \/ Berkeley [[Paper](http:\/\/arxiv.org\/pdf\/1506.01698.pdf)]\n  * Anna Rohrbach, Marcus Rohrbach, Bernt Schiele, The Long-Short Story of Movie Description, arXiv:1506.01698\n* Univ. Toronto \/ MIT [[Paper](http:\/\/arxiv.org\/pdf\/1506.06724.pdf)]\n  * Yukun Zhu, Ryan Kiros, Richard Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, Sanja Fidler, Aligning Books and Movies: Towards Story-like Visual Explanations by Watching Movies and Reading Books, arXiv:1506.06724\n* Univ. Montreal [[Paper](http:\/\/arxiv.org\/pdf\/1507.01053.pdf)]\n  * Kyunghyun Cho, Aaron Courville, Yoshua Bengio, Describing Multimedia Content using Attention-based Encoder-Decoder Networks, arXiv:1507.01053\n* TAU \/ USC [[paper](https:\/\/arxiv.org\/pdf\/1612.06950.pdf)]\n  * Dotan Kaufman, Gil Levi, Tal Hassner, Lior Wolf, Temporal Tessellation for Video Annotation and Summarization, arXiv:1612.06950.\n\n#### Question Answering\n![question_answering](https:\/\/cloud.githubusercontent.com\/assets\/5226447\/8452068\/ffe7b1f6-2022-11e5-87ab-4f6d4696c220.PNG)\n(from Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence Zitnick, Devi Parikh, VQA: Visual Question Answering, CVPR, 2015 SUNw:Scene Understanding workshop)\n\n* Virginia Tech \/ MSR [[Web]](http:\/\/www.visualqa.org\/) [[Paper]](http:\/\/arxiv.org\/pdf\/1505.00468)\n  * Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence Zitnick, Devi Parikh, VQA: Visual Question Answering, CVPR, 2015 SUNw:Scene Understanding workshop.\n* MPI \/ Berkeley [[Web]](https:\/\/www.mpi-inf.mpg.de\/departments\/computer-vision-and-multimodal-computing\/research\/vision-and-language\/visual-turing-challenge\/) [[Paper]](http:\/\/arxiv.org\/pdf\/1505.01121)\n  * Mateusz Malinowski, Marcus Rohrbach, Mario Fritz, Ask Your Neurons: A Neural-based Approach to Answering Questions about Images, arXiv:1505.01121.\n* Toronto [[Paper]](http:\/\/arxiv.org\/pdf\/1505.02074) [[Dataset]](http:\/\/www.cs.toronto.edu\/~mren\/imageqa\/data\/cocoqa\/)\n  * Mengye Ren, Ryan Kiros, Richard Zemel, Image Question Answering: A Visual Semantic Embedding Model and a New Dataset, arXiv:1505.02074 \/ ICML 2015 deep learning workshop.\n* Baidu \/ UCLA [[Paper]](http:\/\/arxiv.org\/pdf\/1505.05612) [[Dataset]]()\n  * Hauyuan Gao, Junhua Mao, Jie Zhou, Zhiheng Huang, Lei Wang, Wei Xu, Are You Talking to a Machine? Dataset and Methods for Multilingual Image Question Answering, arXiv:1505.05612.\n* POSTECH [[Paper](http:\/\/arxiv.org\/pdf\/1511.05756.pdf)] [[Project Page](http:\/\/cvlab.postech.ac.kr\/research\/dppnet\/)]\n  * Hyeonwoo Noh, Paul Hongsuck Seo, and Bohyung Han, Image Question Answering using Convolutional Neural Network with Dynamic Parameter Prediction, arXiv:1511.05765\n* CMU \/ Microsoft Research [[Paper](http:\/\/arxiv.org\/pdf\/1511.02274v2.pdf)]\n  * Yang, Z., He, X., Gao, J., Deng, L., & Smola, A. (2015). Stacked Attention Networks for Image Question Answering. arXiv:1511.02274.\n* MetaMind [[Paper](http:\/\/arxiv.org\/pdf\/1603.01417v1.pdf)]\n  * Xiong, Caiming, Stephen Merity, and Richard Socher. \"Dynamic Memory Networks for Visual and Textual Question Answering.\" arXiv:1603.01417 (2016).\n* SNU + NAVER [[Paper](http:\/\/arxiv.org\/abs\/1606.01455)]\n  * Jin-Hwa Kim, Sang-Woo Lee, Dong-Hyun Kwak, Min-Oh Heo, Jeonghee Kim, Jung-Woo Ha, Byoung-Tak Zhang, *Multimodal Residual Learning for Visual QA*, arXiv:1606:01455\n* UC Berkeley + Sony [[Paper](https:\/\/arxiv.org\/pdf\/1606.01847)]\n  * Akira Fukui, Dong Huk Park, Daylen Yang, Anna Rohrbach, Trevor Darrell, and Marcus Rohrbach, *Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding*, arXiv:1606.01847\n* Postech [[Paper](http:\/\/arxiv.org\/pdf\/1606.03647.pdf)]\n  * Hyeonwoo Noh and Bohyung Han, *Training Recurrent Answering Units with Joint Loss Minimization for VQA*, arXiv:1606.03647\n* SNU + NAVER [[Paper](http:\/\/arxiv.org\/abs\/1610.04325)]\n  * Jin-Hwa Kim, Kyoung Woon On, Jeonghee Kim, Jung-Woo Ha, Byoung-Tak Zhang, *Hadamard Product for Low-rank Bilinear Pooling*, arXiv:1610.04325.\n\n### Image Generation\n* Convolutional \/ Recurrent Networks\n  * A\u00e4ron van den Oord, Nal Kalchbrenner, Oriol Vinyals, Lasse Espeholt, Alex Graves, Koray Kavukcuoglu. \"Conditional Image Generation with PixelCNN Decoders\"[[Paper]](https:\/\/arxiv.org\/pdf\/1606.05328v2.pdf)[[Code]](https:\/\/github.com\/kundan2510\/pixelCNN)\n  * Alexey Dosovitskiy, Jost Tobias Springenberg, Thomas Brox, \"Learning to Generate Chairs with Convolutional Neural Networks\", CVPR, 2015. [[Paper]](http:\/\/www.cv-foundation.org\/openaccess\/content_cvpr_2015\/papers\/Dosovitskiy_Learning_to_Generate_2015_CVPR_paper.pdf)\n  * Karol Gregor, Ivo Danihelka, Alex Graves, Danilo Jimenez Rezende, Daan Wierstra, \"DRAW: A Recurrent Neural Network For Image Generation\", ICML, 2015. [[Paper](https:\/\/arxiv.org\/pdf\/1502.04623v2.pdf)] \n* Adversarial Networks\n  * Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio, Generative Adversarial Networks, NIPS, 2014. [[Paper]](http:\/\/arxiv.org\/abs\/1406.2661)\n  * Emily Denton, Soumith Chintala, Arthur Szlam, Rob Fergus, Deep Generative Image Models using a \ufffcLaplacian Pyramid of Adversarial Networks, NIPS, 2015. [[Paper]](http:\/\/arxiv.org\/abs\/1506.05751)\n  * Lucas Theis, A\u00e4ron van den Oord, Matthias Bethge, \"A note on the evaluation of generative models\", ICLR 2016. [[Paper](http:\/\/arxiv.org\/abs\/1511.01844)]\n  * Zhenwen Dai, Andreas Damianou, Javier Gonzalez, Neil Lawrence, \"Variationally Auto-Encoded Deep Gaussian Processes\", ICLR 2016. [[Paper](http:\/\/arxiv.org\/pdf\/1511.06455v2.pdf)]\n  * Elman Mansimov, Emilio Parisotto, Jimmy Ba, Ruslan Salakhutdinov, \"Generating Images from Captions with Attention\", ICLR 2016, [[Paper](http:\/\/arxiv.org\/pdf\/1511.02793v2.pdf)]\n  * Jost Tobias Springenberg, \"Unsupervised and Semi-supervised Learning with Categorical Generative Adversarial Networks\", ICLR 2016, [[Paper](http:\/\/arxiv.org\/pdf\/1511.06390v1.pdf)]\n  * Harrison Edwards, Amos Storkey, \"Censoring Representations with an Adversary\", ICLR 2016, [[Paper](http:\/\/arxiv.org\/pdf\/1511.05897v3.pdf)]\n  * Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, Ken Nakae, Shin Ishii, \"Distributional Smoothing with Virtual Adversarial Training\", ICLR 2016, [[Paper](http:\/\/arxiv.org\/pdf\/1507.00677v8.pdf)]\n  * Jun-Yan Zhu, Philipp Krahenbuhl, Eli Shechtman, and Alexei A. Efros, \"Generative Visual Manipulation on the Natural Image Manifold\", ECCV 2016. [[Paper](https:\/\/arxiv.org\/pdf\/1609.03552v2.pdf)] [[Code](https:\/\/github.com\/junyanz\/iGAN)] [[Video](https:\/\/youtu.be\/9c4z6YsBGQ0)]\n* Mixing Convolutional and Adversarial Networks\n  * Alec Radford, Luke Metz, Soumith Chintala, \"Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks\", ICLR 2016. [[Paper](http:\/\/arxiv.org\/pdf\/1511.06434.pdf)]\n\n### Other Topics\n* Visual Analogy [[Paper](https:\/\/web.eecs.umich.edu\/~honglak\/nips2015-analogy.pdf)]\n  * Scott Reed, Yi Zhang, Yuting Zhang, Honglak Lee, Deep Visual Analogy Making, NIPS, 2015\n* Surface Normal Estimation [[Paper]](http:\/\/www.cv-foundation.org\/openaccess\/content_cvpr_2015\/papers\/Wang_Designing_Deep_Networks_2015_CVPR_paper.pdf)\n  * Xiaolong Wang, David F. Fouhey, Abhinav Gupta, Designing Deep Networks for Surface Normal Estimation, CVPR, 2015.\n* Action Detection [[Paper]](http:\/\/www.cv-foundation.org\/openaccess\/content_cvpr_2015\/papers\/Gkioxari_Finding_Action_Tubes_2015_CVPR_paper.pdf)\n  * Georgia Gkioxari, Jitendra Malik, Finding Action Tubes, CVPR, 2015.\n* Crowd Counting [[Paper]](http:\/\/www.cv-foundation.org\/openaccess\/content_cvpr_2015\/papers\/Zhang_Cross-Scene_Crowd_Counting_2015_CVPR_paper.pdf)\n  * Cong Zhang, Hongsheng Li, Xiaogang Wang, Xiaokang Yang, Cross-scene Crowd Counting via Deep Convolutional Neural Networks, CVPR, 2015.\n* 3D Shape Retrieval [[Paper]](http:\/\/www.cv-foundation.org\/openaccess\/content_cvpr_2015\/papers\/Wang_Sketch-Based_3D_Shape_2015_CVPR_paper.pdf)\n  * Fang Wang, Le Kang, Yi Li, Sketch-based 3D Shape Retrieval using Convolutional Neural Networks, CVPR, 2015.\n* Weakly-supervised Classification\n  * Samaneh Azadi, Jiashi Feng, Stefanie Jegelka, Trevor Darrell, \"Auxiliary Image Regularization for Deep CNNs with Noisy Labels\", ICLR 2016, [[Paper](http:\/\/arxiv.org\/pdf\/1511.07069v2.pdf)]\n* Artistic Style [[Paper]](http:\/\/arxiv.org\/abs\/1508.06576) [[Code]](https:\/\/github.com\/jcjohnson\/neural-style)\n  * Leon A. Gatys, Alexander S. Ecker, Matthias Bethge, A Neural Algorithm of Artistic Style.\n* Human Gaze Estimation\n  * Xucong Zhang, Yusuke Sugano, Mario Fritz, Andreas Bulling, Appearance-Based Gaze Estimation in the Wild, CVPR, 2015. [[Paper]](http:\/\/www.cv-foundation.org\/openaccess\/content_cvpr_2015\/papers\/Zhang_Appearance-Based_Gaze_Estimation_2015_CVPR_paper.pdf) [[Website]](https:\/\/www.mpi-inf.mpg.de\/departments\/computer-vision-and-multimodal-computing\/research\/gaze-based-human-computer-interaction\/appearance-based-gaze-estimation-in-the-wild-mpiigaze\/)\n* Face Recognition\n  * Yaniv Taigman, Ming Yang, Marc'Aurelio Ranzato, Lior Wolf, DeepFace: Closing the Gap to Human-Level Performance in Face Verification, CVPR, 2014. [[Paper]](https:\/\/www.cs.toronto.edu\/~ranzato\/publications\/taigman_cvpr14.pdf)\n  * Yi Sun, Ding Liang, Xiaogang Wang, Xiaoou Tang, DeepID3: Face Recognition with Very Deep Neural Networks, 2015. [[Paper]](http:\/\/arxiv.org\/abs\/1502.00873)\n  * Florian Schroff, Dmitry Kalenichenko, James Philbin, FaceNet: A Unified Embedding for Face Recognition and Clustering, CVPR, 2015. [[Paper]](http:\/\/arxiv.org\/abs\/1503.03832)\n* Facial Landmark Detection\n  * Yue Wu, Tal Hassner, KangGeon Kim, Gerard Medioni, Prem Natarajan, Facial Landmark Detection with Tweaked Convolutional Neural Networks, 2015. [[Paper]](http:\/\/arxiv.org\/abs\/1511.04031) [[Project]](http:\/\/www.openu.ac.il\/home\/hassner\/projects\/tcnn_landmarks\/)\n\n## Courses\n* Deep Vision\n  * [Stanford] [CS231n: Convolutional Neural Networks for Visual Recognition](http:\/\/cs231n.stanford.edu\/)\n  * [CUHK] [ELEG 5040: Advanced Topics in Signal Processing(Introduction to Deep Learning)](https:\/\/piazza.com\/cuhk.edu.hk\/spring2015\/eleg5040\/home)\n* More Deep Learning\n  * [Stanford] [CS224d: Deep Learning for Natural Language Processing](http:\/\/cs224d.stanford.edu\/)\n  * [Oxford] [Deep Learning by Prof. Nando de Freitas](https:\/\/www.cs.ox.ac.uk\/people\/nando.defreitas\/machinelearning\/)\n  * [NYU] [Deep Learning by Prof. Yann LeCun](http:\/\/cilvr.cs.nyu.edu\/doku.php?id=courses:deeplearning2014:start)\n\n## Books\n* Free Online Books\n  * [Deep Learning by Ian Goodfellow, Yoshua Bengio, and Aaron Courville](http:\/\/www.iro.umontreal.ca\/~bengioy\/dlbook\/)\n  * [Neural Networks and Deep Learning by Michael Nielsen](http:\/\/neuralnetworksanddeeplearning.com\/)\n  * [Deep Learning Tutorial by LISA lab, University of Montreal](http:\/\/deeplearning.net\/tutorial\/deeplearning.pdf)\n\n## Videos\n* Talks\n  * [Deep Learning, Self-Taught Learning and Unsupervised Feature Learning By Andrew Ng](https:\/\/www.youtube.com\/watch?v=n1ViNeWhC24)\n  * [Recent Developments in Deep Learning By Geoff Hinton](https:\/\/www.youtube.com\/watch?v=vShMxxqtDDs)\n  * [The Unreasonable Effectiveness of Deep Learning by Yann LeCun](https:\/\/www.youtube.com\/watch?v=sc-KbuZqGkI)\n  * [Deep Learning of Representations by Yoshua bengio](https:\/\/www.youtube.com\/watch?v=4xsVFLnHC_0)\n\n\n## Software\n### Framework\n* Tensorflow: An open source software library for numerical computation using data flow graph by Google [[Web](https:\/\/www.tensorflow.org\/)]\n* Torch7: Deep learning library in Lua, used by Facebook and Google Deepmind [[Web](http:\/\/torch.ch\/)]\n  * Torch-based deep learning libraries: [[torchnet](https:\/\/github.com\/torchnet\/torchnet)],\n* Caffe: Deep learning framework by the BVLC [[Web](http:\/\/caffe.berkeleyvision.org\/)]\n* Theano: Mathematical library in Python, maintained by LISA lab [[Web](http:\/\/deeplearning.net\/software\/theano\/)]\n  * Theano-based deep learning libraries: [[Pylearn2](http:\/\/deeplearning.net\/software\/pylearn2\/)], [[Blocks](https:\/\/github.com\/mila-udem\/blocks)], [[Keras](http:\/\/keras.io\/)], [[Lasagne](https:\/\/github.com\/Lasagne\/Lasagne)]\n* MatConvNet: CNNs for MATLAB [[Web](http:\/\/www.vlfeat.org\/matconvnet\/)]\n* MXNet: A flexible and efficient deep learning library for heterogeneous distributed systems with multi-language support [[Web](http:\/\/mxnet.io\/)]\n* Deepgaze: A computer vision library for human-computer interaction based on CNNs [[Web](https:\/\/github.com\/mpatacchiola\/deepgaze)]\n\n### Applications\n* Adversarial Training\n  * Code and hyperparameters for the paper \"Generative Adversarial Networks\" [[Web]](https:\/\/github.com\/goodfeli\/adversarial)\n* Understanding and Visualizing\n  * Source code for \"Understanding Deep Image Representations by Inverting Them,\" CVPR, 2015. [[Web]](https:\/\/github.com\/aravindhm\/deep-goggle)\n* Semantic Segmentation\n  * Source code for the paper \"Rich feature hierarchies for accurate object detection and semantic segmentation,\" CVPR, 2014. [[Web]](https:\/\/github.com\/rbgirshick\/rcnn)\n  * Source code for the paper \"Fully Convolutional Networks for Semantic Segmentation,\" CVPR, 2015. [[Web]](https:\/\/github.com\/longjon\/caffe\/tree\/future)\n* Super-Resolution\n  * Image Super-Resolution for Anime-Style-Art [[Web]](https:\/\/github.com\/nagadomi\/waifu2x)\n* Edge Detection\n  * Source code for the paper \"DeepContour: A Deep Convolutional Feature Learned by Positive-Sharing Loss for Contour Detection,\" CVPR, 2015. [[Web]](https:\/\/github.com\/shenwei1231\/DeepContour)\n  * Source code for the paper \"Holistically-Nested Edge Detection\", ICCV 2015. [[Web]](https:\/\/github.com\/s9xie\/hed)\n\n## Tutorials\n* [CVPR 2014] [Tutorial on Deep Learning in Computer Vision](https:\/\/sites.google.com\/site\/deeplearningcvpr2014\/)\n* [CVPR 2015] [Applied Deep Learning for Computer Vision with Torch](https:\/\/github.com\/soumith\/cvpr2015)\n\n## Blogs\n* [Deep down the rabbit hole: CVPR 2015 and beyond@Tombone's Computer Vision Blog](http:\/\/www.computervisionblog.com\/2015\/06\/deep-down-rabbit-hole-cvpr-2015-and.html)\n* [CVPR recap and where we're going@Zoya Bylinskii (MIT PhD Student)'s Blog](http:\/\/zoyathinks.blogspot.kr\/2015\/06\/cvpr-recap-and-where-were-going.html)\n* [Facebook's AI Painting@Wired](http:\/\/www.wired.com\/2015\/06\/facebook-googles-fake-brains-spawn-new-visual-reality\/)\n* [Inceptionism: Going Deeper into Neural Networks@Google Research](http:\/\/googleresearch.blogspot.kr\/2015\/06\/inceptionism-going-deeper-into-neural.html)\n* [Implementing Neural networks](http:\/\/peterroelants.github.io\/) \n\n\n## Awesome Computer Vision Resources - Part II\n    The part 2 has a curated list of awesome computer vision resources as follows,\n\n## Table of Contents - Part II\n\n - [Books](#books)\n - [Courses](#courses)\n - [Papers](#papers)\n - [Software](#software)\n - [Datasets](#datasets)\n - [Tutorials and Talks](#tutorials-and-talks)\n - [Resources for students](#resources-for-students)\n - [Blogs](#blogs)\n - [Links](#links)\n - [Songs](#songs)    \n\n## Books\n\n#### Computer Vision\n* [Computer Vision:  Models, Learning, and Inference](http:\/\/www.computervisionmodels.com\/) - Simon J. D. Prince 2012\n* [Computer Vision: Theory and Application](http:\/\/szeliski.org\/Book\/) - Rick Szeliski 2010\n* [Computer Vision: A Modern Approach (2nd edition)](http:\/\/www.amazon.com\/Computer-Vision-Modern-Approach-2nd\/dp\/013608592X\/ref=dp_ob_title_bk) - David Forsyth and Jean Ponce 2011\n* [Multiple View Geometry in Computer Vision](http:\/\/www.robots.ox.ac.uk\/~vgg\/hzbook\/) - Richard Hartley and Andrew Zisserman 2004\n* [Computer Vision](http:\/\/www.amazon.com\/Computer-Vision-Linda-G-Shapiro\/dp\/0130307963) - Linda G. Shapiro 2001\n* [Vision Science: Photons to Phenomenology](http:\/\/www.amazon.com\/Vision-Science-Phenomenology-Stephen-Palmer\/dp\/0262161834\/) - Stephen E. Palmer 1999\n* [Visual Object Recognition synthesis lecture](http:\/\/www.morganclaypool.com\/doi\/abs\/10.2200\/S00332ED1V01Y201103AIM011) - Kristen Grauman and Bastian Leibe 2011\n* [Computer Vision for Visual Effects](http:\/\/cvfxbook.com\/) - Richard J. Radke, 2012\n* [High dynamic range imaging: acquisition, display, and image-based lighting](http:\/\/www.amazon.com\/High-Dynamic-Range-Imaging-Second\/dp\/012374914X) - Reinhard, E., Heidrich, W., Debevec, P., Pattanaik, S., Ward, G., Myszkowski, K 2010\n* [Numerical Algorithms: Methods for Computer Vision, Machine Learning, and Graphics](https:\/\/people.csail.mit.edu\/jsolomon\/share\/book\/numerical_book.pdf) - Justin Solomon 2015\n\n#### OpenCV Programming\n* [Learning OpenCV: Computer Vision with the OpenCV Library](http:\/\/www.amazon.com\/Learning-OpenCV-Computer-Vision-Library\/dp\/0596516134) - Gary Bradski and Adrian Kaehler\n* [Practical Python and OpenCV](https:\/\/www.pyimagesearch.com\/practical-python-opencv\/) - Adrian Rosebrock\n* [OpenCV Essentials](http:\/\/www.amazon.com\/OpenCV-Essentials-Oscar-Deniz-Suarez\/dp\/1783984244\/ref=sr_1_1?s=books&ie=UTF8&qid=1424594237&sr=1-1&keywords=opencv+essentials#) - Oscar Deniz Suarez, M\u00aa del Milagro Fernandez Carrobles, Noelia Vallez Enano, Gloria Bueno Garcia, Ismael Serrano Gracia\n\n#### Machine Learning\n* [Pattern Recognition and Machine Learning](http:\/\/research.microsoft.com\/en-us\/um\/people\/cmbishop\/prml\/index.htm) - Christopher M. Bishop 2007\n* [Neural Networks for Pattern Recognition](http:\/\/www.engineering.upm.ro\/master-ie\/sacpi\/mat_did\/info068\/docum\/Neural%20Networks%20for%20Pattern%20Recognition.pdf) - Christopher M. Bishop 1995\n* [Probabilistic Graphical Models: Principles and Techniques](http:\/\/pgm.stanford.edu\/) - Daphne Koller and Nir Friedman 2009\n* [Pattern Classification](http:\/\/www.amazon.com\/Pattern-Classification-2nd-Richard-Duda\/dp\/0471056693) - Peter E. Hart, David G. Stork, and Richard O. Duda 2000\n* [Machine Learning](http:\/\/www.amazon.com\/Machine-Learning-Tom-M-Mitchell\/dp\/0070428077\/) - Tom M. Mitchell 1997\n* [Gaussian processes for machine learning](http:\/\/www.gaussianprocess.org\/gpml\/) - Carl Edward Rasmussen and Christopher K. I. Williams 2005\n* [Learning From Data](https:\/\/work.caltech.edu\/telecourse.html)- Yaser S. Abu-Mostafa, Malik Magdon-Ismail and Hsuan-Tien Lin 2012\n* [Neural Networks and Deep Learning](http:\/\/neuralnetworksanddeeplearning.com\/) - Michael Nielsen 2014\n* [Bayesian Reasoning and Machine Learning](http:\/\/www.cs.ucl.ac.uk\/staff\/d.barber\/brml\/) - David Barber, Cambridge University Press, 2012\n\n#### Fundamentals\n * [Linear Algebra and Its Applications](http:\/\/www.amazon.com\/Linear-Algebra-Its-Applications-4th\/dp\/0030105676\/ref=sr_1_4?ie=UTF8&qid=1421433773&sr=8-4&keywords=Linear+Algebra+and+Its+Applications) - Gilbert Strang 1995\n\n## Courses\n\n#### Computer Vision\n * [EENG 512 \/ CSCI 512 - Computer Vision](http:\/\/inside.mines.edu\/~whoff\/courses\/EENG512\/) - William Hoff (Colorado School of Mines)\n * [Visual Object and Activity Recognition](https:\/\/sites.google.com\/site\/ucbcs29443\/) - Alexei A. Efros and Trevor Darrell (UC Berkeley)\n * [Computer Vision](http:\/\/courses.cs.washington.edu\/courses\/cse455\/12wi\/) - Steve Seitz (University of Washington)\n * Visual Recognition [Spring 2016](http:\/\/vision.cs.utexas.edu\/381V-spring2016\/), [Fall 2016](http:\/\/vision.cs.utexas.edu\/381V-fall2016\/) - Kristen Grauman (UT Austin)\n * [Language and Vision](http:\/\/www.tamaraberg.com\/teaching\/Spring_15\/) - Tamara Berg (UNC Chapel Hill)\n * [Convolutional Neural Networks for Visual Recognition](http:\/\/vision.stanford.edu\/teaching\/cs231n\/) - Fei-Fei Li and Andrej Karpathy (Stanford University)\n * [Computer Vision](http:\/\/cs.nyu.edu\/~fergus\/teaching\/vision\/index.html) - Rob Fergus (NYU)\n * [Computer Vision](https:\/\/courses.engr.illinois.edu\/cs543\/sp2015\/) - Derek Hoiem (UIUC)\n * [Computer Vision: Foundations and Applications](http:\/\/vision.stanford.edu\/teaching\/cs131_fall1415\/index.html) - Kalanit Grill-Spector and Fei-Fei Li (Stanford University)\n * [High-Level Vision: Behaviors, Neurons and Computational Models](http:\/\/vision.stanford.edu\/teaching\/cs431_spring1314\/) - Fei-Fei Li (Stanford University)\n * [Advances in Computer Vision](http:\/\/6.869.csail.mit.edu\/fa15\/) - Antonio Torralba and Bill Freeman (MIT)\n * [Computer Vision](http:\/\/www.vision.rwth-aachen.de\/course\/11\/) - Bastian Leibe (RWTH Aachen University)\n * [Computer Vision 2](http:\/\/www.vision.rwth-aachen.de\/course\/9\/) - Bastian Leibe (RWTH Aachen University)\n * [Computer Vision](http:\/\/klewel.com\/conferences\/epfl-computer-vision\/) Pascal Fua (EPFL):\n * [Computer Vision 1](http:\/\/cvlab-dresden.de\/courses\/computer-vision-1\/) Carsten Rother (TU Dresden):\n * [Computer Vision 2](http:\/\/cvlab-dresden.de\/courses\/CV2\/) Carsten Rother (TU Dresden):\n * [Multiple View Geometry](https:\/\/youtu.be\/RDkwklFGMfo?list=PLTBdjV_4f-EJn6udZ34tht9EVIW7lbeo4) Daniel Cremers (TU Munich):\n\n\n\n\n#### Computational Photography\n* [Image Manipulation and Computational Photography](http:\/\/inst.eecs.berkeley.edu\/~cs194-26\/fa14\/) - Alexei A. Efros (UC Berkeley)\n* [Computational Photography](http:\/\/graphics.cs.cmu.edu\/courses\/15-463\/2012_fall\/463.html) - Alexei A. Efros (CMU)\n* [Computational Photography](https:\/\/courses.engr.illinois.edu\/cs498dh3\/) - Derek Hoiem (UIUC)\n* [Computational Photography](http:\/\/cs.brown.edu\/courses\/csci1290\/) - James Hays (Brown University)\n* [Digital & Computational Photography](http:\/\/stellar.mit.edu\/S\/course\/6\/sp12\/6.815\/) - Fredo Durand (MIT)\n* [Computational Camera and Photography](http:\/\/ocw.mit.edu\/courses\/media-arts-and-sciences\/mas-531-computational-camera-and-photography-fall-2009\/) - Ramesh Raskar (MIT Media Lab)\n* [Computational Photography](https:\/\/www.udacity.com\/course\/computational-photography--ud955) - Irfan Essa (Georgia Tech)\n* [Courses in Graphics](http:\/\/graphics.stanford.edu\/courses\/) - Stanford University\n* [Computational Photography](http:\/\/cs.nyu.edu\/~fergus\/teaching\/comp_photo\/index.html) - Rob Fergus (NYU)\n* [Introduction to Visual Computing](http:\/\/www.cs.toronto.edu\/~kyros\/courses\/320\/) - Kyros Kutulakos (University of Toronto)\n* [Computational Photography](http:\/\/www.cs.toronto.edu\/~kyros\/courses\/2530\/) - Kyros Kutulakos (University of Toronto)\n* [Computer Vision for Visual Effects](https:\/\/www.ecse.rpi.edu\/~rjradke\/cvfxcourse.html) - Rich Radke (Rensselaer Polytechnic Institute)\n* [Introduction to Image Processing](https:\/\/www.ecse.rpi.edu\/~rjradke\/improccourse.html) - Rich Radke (Rensselaer Polytechnic Institute)\n\n#### Machine Learning and Statistical Learning\n * [Machine Learning](https:\/\/www.coursera.org\/learn\/machine-learning) - Andrew Ng (Stanford University)\n * [Learning from Data](https:\/\/work.caltech.edu\/telecourse.html) - Yaser S. Abu-Mostafa (Caltech)\n * [Statistical Learning](https:\/\/class.stanford.edu\/courses\/HumanitiesandScience\/StatLearning\/Winter2015\/about) - Trevor Hastie and Rob Tibshirani (Stanford University)\n * [Statistical Learning Theory and Applications](http:\/\/www.mit.edu\/~9.520\/fall14\/) - Tomaso Poggio, Lorenzo Rosasco, Carlo Ciliberto, Charlie Frogner, Georgios Evangelopoulos, Ben Deen (MIT)\n * [Statistical Learning](http:\/\/www.stat.rice.edu\/~gallen\/stat640.html) - Genevera Allen (Rice University)\n * [Practical Machine Learning](http:\/\/www.cs.berkeley.edu\/~jordan\/courses\/294-fall09\/) - Michael Jordan (UC Berkeley)\n * [Course on Information Theory, Pattern Recognition, and Neural Networks](http:\/\/videolectures.net\/course_information_theory_pattern_recognition\/) - David MacKay (University of Cambridge)\n * [Methods for Applied Statistics: Unsupervised Learning](http:\/\/web.stanford.edu\/~lmackey\/stats306b\/) - Lester Mackey (Stanford)\n * [Machine Learning](http:\/\/www.robots.ox.ac.uk\/~az\/lectures\/ml\/index.html) - Andrew Zisserman (University of Oxford)\n * [Intro to Machine Learning](https:\/\/www.udacity.com\/course\/intro-to-machine-learning--ud120) - Sebastian Thrun (Stanford University)\n * [Machine Learning](https:\/\/www.udacity.com\/course\/machine-learning--ud262) - Charles Isbell, Michael Littman (Georgia Tech)\n * [(Convolutional) Neural Networks for Visual Recognition](https:\/\/cs231n.github.io\/) - Fei-Fei Li, Andrej Karphaty, Justin Johnson (Stanford University)\n * [Machine Learning for Computer Vision](https:\/\/youtu.be\/QZmZFeZxEKI?list=PLTBdjV_4f-EIiongKlS9OKrBEp8QR47Wl) - Rudolph Triebel (TU Munich)\n\n\n\n#### Optimization\n * [Convex Optimization I](http:\/\/stanford.edu\/class\/ee364a\/) - Stephen Boyd (Stanford University)\n * [Convex Optimization II](http:\/\/stanford.edu\/class\/ee364b\/) - Stephen Boyd (Stanford University)\n * [Convex Optimization](https:\/\/class.stanford.edu\/courses\/Engineering\/CVX101\/Winter2014\/about) - Stephen Boyd (Stanford University)\n * [Optimization at MIT](http:\/\/optimization.mit.edu\/classes.php) - (MIT)\n * [Convex Optimization](http:\/\/www.stat.cmu.edu\/~ryantibs\/convexopt\/) - Ryan Tibshirani (CMU)\n\n## Papers\n\n#### Conference papers on the web\n * [CVPapers](http:\/\/www.cvpapers.com\/) - Computer vision papers on the web\n * [SIGGRAPH Paper on the web](http:\/\/kesen.realtimerendering.com\/) - Graphics papers on the web\n * [NIPS Proceedings](http:\/\/papers.nips.cc\/) - NIPS papers on the web\n * [Computer Vision Foundation open access](http:\/\/www.cv-foundation.org\/openaccess\/menu.py)\n * [Annotated Computer Vision Bibliography](http:\/\/iris.usc.edu\/Vision-Notes\/bibliography\/contents.html) - Keith Price (USC)\n * [Calendar of Computer Image Analysis, Computer Vision Conferences](http:\/\/iris.usc.edu\/Information\/Iris-Conferences.html) - (USC)\n\n#### Survey Papers\n * [Visionbib Survey Paper List](http:\/\/surveys.visionbib.com\/index.html)\n * [Foundations and Trends\u00ae in Computer Graphics and Vision](http:\/\/www.nowpublishers.com\/CGV)\n * [Computer Vision: A Reference Guide](http:\/\/link.springer.com\/book\/10.1007\/978-0-387-31439-6)\n\n## Tutorials and talks\n\n#### Computer Vision\n * [Computer Vision Talks](http:\/\/www.computervisiontalks.com\/) - Lectures, keynotes, panel discussions on computer vision\n * [The Three R's of Computer Vision](https:\/\/www.youtube.com\/watch?v=Mqg6eorYRIQ) - Jitendra Malik (UC Berkeley) 2013\n * [Applications to Machine Vision](http:\/\/videolectures.net\/epsrcws08_blake_amv\/) - Andrew Blake (Microsoft Research) 2008\n * [The Future of Image Search](http:\/\/videolectures.net\/kdd08_malik_fis\/?q=image) - Jitendra Malik (UC Berkeley) 2008\n * [Should I do a PhD in Computer Vision?](https:\/\/www.youtube.com\/watch?v=M17oGxh3Ny8) - Fatih Porikli (Australian National University)\n - [Graduate Summer School 2013: Computer Vision](http:\/\/www.ipam.ucla.edu\/programs\/summer-schools\/graduate-summer-school-computer-vision\/?tab=schedule) - IPAM, 2013\n\n#### Recent Conference Talks\n- [CVPR 2015](http:\/\/www.pamitc.org\/cvpr15\/) - Jun 2015\n- [ECCV 2014](http:\/\/videolectures.net\/eccv2014_zurich\/) - Sep 2014\n- [CVPR 2014](http:\/\/techtalks.tv\/cvpr-2014-oral-talks\/) - Jun 2014\n- [ICCV 2013](http:\/\/techtalks.tv\/iccv2013\/) - Dec 2013\n- [ICML 2013](http:\/\/techtalks.tv\/icml\/2013\/) - Jul 2013\n- [CVPR 2013](http:\/\/techtalks.tv\/cvpr2013\/) - Jun 2013\n- [ECCV 2012](http:\/\/videolectures.net\/eccv2012_firenze\/) - Oct 2012\n- [ICML 2012](http:\/\/techtalks.tv\/icml\/2012\/orals\/) - Jun 2012\n- [CVPR 2012](http:\/\/techtalks.tv\/cvpr2012webcast\/) - Jun 2012\n\n#### 3D Computer Vision\n * [3D Computer Vision: Past, Present, and Future](https:\/\/www.youtube.com\/watch?v=kyIzMr917Rc) - Steve Seitz (University of Washington) 2011\n * [Reconstructing the World from Photos on the Internet](https:\/\/www.youtube.com\/watch?v=04Kgg3QEXFI) - Steve Seitz (University of Washington) 2013\n\n#### Internet Vision\n * [The Distributed Camera](http:\/\/www.technologyreview.com\/video\/426265\/meet-2011-tr35-winner-noah-snavely\/) - Noah Snavely (Cornell University) 2011\n * [Planet-Scale Visual Understanding](https:\/\/www.youtube.com\/watch?v=UHkCa9-Z1Ps) - Noah Snavely (Cornell University) 2014\n * [A Trillion Photos](https:\/\/www.youtube.com\/watch?v=6MWEfpKUfRc) - Steve Seitz (University of Washington) 2013\n\n#### Computational Photography\n * [Reflections on Image-Based Modeling and Rendering](https:\/\/www.youtube.com\/watch?v=j90_0Ndk7XM) - Richard Szeliski (Microsoft Research) 2013\n * [Photographing Events over Time](https:\/\/www.youtube.com\/watch?v=ZvPaHZZVPRk) - William T. Freeman (MIT) 2011\n * [Old and New algorithm for Blind Deconvolution](http:\/\/videolectures.net\/nipsworkshops2011_weiss_deconvolution\/) -  Yair Weiss (The Hebrew University of Jerusalem) 2011\n * [A Tour of Modern \"Image Processing\"](http:\/\/videolectures.net\/nipsworkshops2010_milanfar_tmi\/) -  Peyman Milanfar (UC Santa Cruz\/Google) 2010\n * [Topics in image and video processing](http:\/\/videolectures.net\/mlss07_blake_tiivp\/) Andrew Blake (Microsoft Research) 2007\n * [Computational Photography](https:\/\/www.youtube.com\/watch?v=HJVNI0mkmqk) - William T. Freeman (MIT) 2012\n * [Revealing the Invisible](https:\/\/www.youtube.com\/watch?v=_BWnIQY_X98) - Fr\u00e9do Durand (MIT) 2012\n * [Overview of Computer Vision and Visual Effects](https:\/\/www.youtube.com\/watch?v=rE-hVtytT-I) - Rich Radke (Rensselaer Polytechnic Institute) 2014\n\n#### Learning and Vision\n * [Where machine vision needs help from machine learning](http:\/\/videolectures.net\/colt2011_freeman_help\/?q=computer%20vision) - William T. Freeman (MIT) 2011\n * [Learning in Computer Vision](http:\/\/videolectures.net\/mlss08au_lucey_linv\/) - Simon Lucey (CMU) 2008\n * [Learning and Inference in Low-Level Vision](http:\/\/videolectures.net\/nips09_weiss_lil\/?q=computer%20vision) - Yair Weiss (The Hebrew University of Jerusalem) 2009\n\n#### Object Recognition\n * [Object Recognition](http:\/\/research.microsoft.com\/apps\/video\/dl.aspx?id=231358) - Larry Zitnick (Microsoft Research)\n * [Generative Models for Visual Objects and Object Recognition via Bayesian Inference](http:\/\/videolectures.net\/mlas06_li_gmvoo\/?q=Fei-Fei%20Li) - Fei-Fei Li (Stanford University)\n\n#### Graphical Models\n * [Graphical Models for Computer Vision](http:\/\/videolectures.net\/uai2012_felzenszwalb_computer_vision\/?q=computer%20vision) - Pedro Felzenszwalb (Brown University) 2012\n * [Graphical Models](http:\/\/videolectures.net\/mlss09uk_ghahramani_gm\/) - Zoubin Ghahramani (University of Cambridge) 2009\n * [Machine Learning, Probability and Graphical Models](http:\/\/videolectures.net\/mlss06tw_roweis_mlpgm\/) - Sam Roweis (NYU) 2006\n * [Graphical Models and Applications](http:\/\/videolectures.net\/mlss09us_weiss_gma\/?q=Graphical%20Models) -  Yair Weiss (The Hebrew University of Jerusalem) 2009\n\n#### Machine Learning\n * [A Gentle Tutorial of the EM Algorithm](https:\/\/nikola-rt.ee.washington.edu\/people\/bulyko\/papers\/em.pdf) - Jeff A. Bilmes (UC Berkeley) 1998\n * [Introduction To Bayesian Inference](http:\/\/videolectures.net\/mlss09uk_bishop_ibi\/) - Christopher Bishop (Microsoft Research) 2009\n * [Support Vector Machines](http:\/\/videolectures.net\/mlss06tw_lin_svm\/) - Chih-Jen Lin (National Taiwan University) 2006\n * [Bayesian or Frequentist, Which Are You? ](http:\/\/videolectures.net\/mlss09uk_jordan_bfway\/) - Michael I. Jordan (UC Berkeley)\n\n#### Optimization\n * [Optimization Algorithms in Machine Learning](http:\/\/videolectures.net\/nips2010_wright_oaml\/) - Stephen J. Wright (University of Wisconsin-Madison)\n * [Convex Optimization](http:\/\/videolectures.net\/mlss07_vandenberghe_copt\/?q=convex%20optimization) - Lieven Vandenberghe (University of California, Los Angeles)\n * [Continuous Optimization in Computer Vision](https:\/\/www.youtube.com\/watch?v=oZqoWozVDVg) - Andrew Fitzgibbon (Microsoft Research)\n * [Beyond stochastic gradient descent for large-scale machine learning](http:\/\/videolectures.net\/sahd2014_bach_stochastic_gradient\/) - Francis Bach (INRIA)\n * [Variational Methods for Computer Vision](https:\/\/www.youtube.com\/playlist?list=PLTBdjV_4f-EJ7A2iIH5L5ztqqrWYjP2RI) - Daniel Cremers (Technische Universit\u00e4t M\u00fcnchen) ([lecture 18 missing from playlist](https:\/\/www.youtube.com\/watch?v=GgcbVPNd3SI))\n\n#### Deep Learning\n * [A tutorial on Deep Learning](http:\/\/videolectures.net\/jul09_hinton_deeplearn\/) - Geoffrey E. Hinton (University of Toronto)\n * [Deep Learning](http:\/\/videolectures.net\/kdd2014_salakhutdinov_deep_learning\/?q=Hidden%20Markov%20model#) -  Ruslan Salakhutdinov (University of Toronto)\n * [Scaling up Deep Learning](http:\/\/videolectures.net\/kdd2014_bengio_deep_learning\/) - Yoshua Bengio (University of Montreal)\n * [ImageNet Classification with Deep Convolutional Neural Networks](http:\/\/videolectures.net\/machine_krizhevsky_imagenet_classification\/?q=deep%20learning) -  Alex Krizhevsky (University of Toronto)\n * [The Unreasonable Effectivness Of Deep Learning](http:\/\/videolectures.net\/sahd2014_lecun_deep_learning\/) Yann LeCun (NYU\/Facebook Research) 2014\n * [Deep Learning for Computer Vision](https:\/\/www.youtube.com\/watch?v=qgx57X0fBdA) - Rob Fergus (NYU\/Facebook Research)\n * [High-dimensional learning with deep network contractions](http:\/\/videolectures.net\/sahd2014_mallat_dimensional_learning\/) - St\u00e9phane Mallat (Ecole Normale Superieure)\n * [Graduate Summer School 2012: Deep Learning, Feature Learning](http:\/\/www.ipam.ucla.edu\/programs\/summer-schools\/graduate-summer-school-deep-learning-feature-learning\/?tab=schedule) - IPAM, 2012\n * [Workshop on Big Data and Statistical Machine Learning](http:\/\/www.fields.utoronto.ca\/programs\/scientific\/14-15\/bigdata\/machine\/)\n * [Machine Learning Summer School](https:\/\/www.youtube.com\/channel\/UC3ywjSv5OsDiDAnOP8C1NiQ) - Reykjavik, Iceland 2014\n\t* [Deep Learning Session 1](https:\/\/www.youtube.com\/watch?v=JuimBuvEWBg) - Yoshua Bengio (Universtiy of Montreal)\n\t* [Deep Learning Session 2](https:\/\/www.youtube.com\/watch?v=Fl-W7_z3w3o) - Yoshua Bengio (University of Montreal)\n\t* [Deep Learning Session 3](https:\/\/www.youtube.com\/watch?v=_cohR7LAgWA) - Yoshua Bengio (University of Montreal)\n\n## Software\n\n#### External Resource Links\n * [Computer Vision Resources](https:\/\/sites.google.com\/site\/jbhuang0604\/resources\/vision) - Jia-Bin Huang (UIUC)\n * [Computer Vision Algorithm Implementations](http:\/\/www.cvpapers.com\/rr.html) - CVPapers\n * [Source Code Collection for Reproducible Research](http:\/\/www.csee.wvu.edu\/~xinl\/reproducible_research.html) - Xin Li (West Virginia University)\n * [CMU Computer Vision Page](http:\/\/www.cs.cmu.edu\/afs\/cs\/project\/cil\/ftp\/html\/v-source.html)\n\n#### General Purpose Computer Vision Library\n* [Open CV](http:\/\/opencv.org\/)\n* [mexopencv](http:\/\/kyamagu.github.io\/mexopencv\/)\n* [SimpleCV](http:\/\/simplecv.org\/)\n* [Open source Python module for computer vision](https:\/\/github.com\/jesolem\/PCV)\n* [ccv: A Modern Computer Vision Library](https:\/\/github.com\/liuliu\/ccv)\n* [VLFeat](http:\/\/www.vlfeat.org\/)\n* [Matlab Computer Vision System Toolbox](http:\/\/www.mathworks.com\/products\/computer-vision\/)\n* [Piotr's Computer Vision Matlab Toolbox](http:\/\/vision.ucsd.edu\/~pdollar\/toolbox\/doc\/index.html)\n* [PCL: Point Cloud Library](http:\/\/pointclouds.org\/)\n* [ImageUtilities](https:\/\/gitorious.org\/imageutilities)\n\n#### Multiple-view Computer Vision\n* [MATLAB Functions for Multiple View Geometry](http:\/\/www.robots.ox.ac.uk\/~vgg\/hzbook\/code\/)\n* [Peter Kovesi's Matlab Functions for Computer Vision and Image Analysis](http:\/\/staffhome.ecm.uwa.edu.au\/~00011811\/Research\/MatlabFns\/index.html)\n* [OpenGV ](http:\/\/laurentkneip.github.io\/opengv\/) - geometric computer vision algorithms\n* [MinimalSolvers](http:\/\/cmp.felk.cvut.cz\/mini\/) - Minimal problems solver\n* [Multi-View Environment](http:\/\/www.gcc.tu-darmstadt.de\/home\/proj\/mve\/)\n* [Visual SFM](http:\/\/ccwu.me\/vsfm\/)\n* [Bundler SFM](http:\/\/www.cs.cornell.edu\/~snavely\/bundler\/)\n* [openMVG: open Multiple View Geometry](http:\/\/imagine.enpc.fr\/~moulonp\/openMVG\/) - Multiple View Geometry; Structure from Motion library & softwares\n* [Patch-based Multi-view Stereo V2](http:\/\/www.di.ens.fr\/pmvs\/)\n* [Clustering Views for Multi-view Stereo](http:\/\/www.di.ens.fr\/cmvs\/)\n* [Floating Scale Surface Reconstruction](http:\/\/www.gris.informatik.tu-darmstadt.de\/projects\/floating-scale-surface-recon\/)\n* [Large-Scale Texturing of 3D Reconstructions](http:\/\/www.gcc.tu-darmstadt.de\/home\/proj\/texrecon\/)\n* [Awesome 3D reconstruction list](https:\/\/github.com\/openMVG\/awesome_3DReconstruction_list)\n\n\n#### Feature Detection and Extraction\n* [VLFeat](http:\/\/www.vlfeat.org\/)\n* [SIFT](http:\/\/www.cs.ubc.ca\/~lowe\/keypoints\/)\n  * David G. Lowe, \"Distinctive image features from scale-invariant keypoints,\" International Journal of Computer Vision, 60, 2 (2004), pp. 91-110.\n* [SIFT++](http:\/\/www.robots.ox.ac.uk\/~vedaldi\/code\/siftpp.html)\n* [BRISK](http:\/\/www.asl.ethz.ch\/people\/lestefan\/personal\/BRISK)\n  * Stefan Leutenegger, Margarita Chli and Roland Siegwart, \"BRISK: Binary Robust Invariant Scalable Keypoints\", ICCV 2011\n* [SURF](http:\/\/www.vision.ee.ethz.ch\/~surf\/)\n  * Herbert Bay, Andreas Ess, Tinne Tuytelaars, Luc Van Gool, \"SURF: Speeded Up Robust Features\", Computer Vision and Image Understanding (CVIU), Vol. 110, No. 3, pp. 346--359, 2008\n* [FREAK](http:\/\/www.ivpe.com\/freak.htm)\n  * A. Alahi, R. Ortiz, and P. Vandergheynst, \"FREAK: Fast Retina Keypoint\", CVPR 2012\n* [AKAZE](http:\/\/www.robesafe.com\/personal\/pablo.alcantarilla\/kaze.html)\n  * Pablo F. Alcantarilla, Adrien Bartoli and Andrew J. Davison, \"KAZE Features\", ECCV 2012\n* [Local Binary Patterns](https:\/\/github.com\/nourani\/LBP)\n\n#### High Dynamic Range Imaging\n* [HDR_Toolbox](https:\/\/github.com\/banterle\/HDR_Toolbox)\n\n#### Semantic Segmentation\n* [List of Semantic Segmentation algorithms](http:\/\/www.it-caesar.com\/list-of-contemporary-semantic-segmentation-datasets\/)\n\n#### Low-level Vision\n\n###### Stereo Vision\n * [Middlebury Stereo Vision](http:\/\/vision.middlebury.edu\/stereo\/)\n * [The KITTI Vision Benchmark Suite](http:\/\/www.cvlibs.net\/datasets\/kitti\/eval_stereo_flow.php?benchmark=stero)\n * [LIBELAS: Library for Efficient Large-scale Stereo Matching](http:\/\/www.cvlibs.net\/software\/libelas\/)\n * [Ground Truth Stixel Dataset](http:\/\/www.6d-vision.com\/ground-truth-stixel-dataset)\n\n###### Optical Flow\n * [Middlebury Optical Flow Evaluation](http:\/\/vision.middlebury.edu\/flow\/)\n * [MPI-Sintel Optical Flow Dataset and Evaluation](http:\/\/sintel.is.tue.mpg.de\/)\n * [The KITTI Vision Benchmark Suite](http:\/\/www.cvlibs.net\/datasets\/kitti\/eval_stereo_flow.php?benchmark=flow)\n * [HCI Challenge](http:\/\/hci.iwr.uni-heidelberg.de\/Benchmarks\/document\/Challenging_Data_for_Stereo_and_Optical_Flow\/)\n * [Coarse2Fine Optical Flow](http:\/\/people.csail.mit.edu\/celiu\/OpticalFlow\/) - Ce Liu (MIT)\n * [Secrets of Optical Flow Estimation and Their Principles](http:\/\/cs.brown.edu\/~dqsun\/code\/cvpr10_flow_code.zip)\n * [C++\/MatLab Optical Flow by C. Liu (based on Brox et al. and Bruhn et al.)](http:\/\/people.csail.mit.edu\/celiu\/OpticalFlow\/)\n * [Parallel Robust Optical Flow by S\u00e1nchez P\u00e9rez et al.](http:\/\/www.ctim.es\/research_works\/parallel_robust_optical_flow\/)\n\n###### Image Denoising\nBM3D, KSVD,\n\n###### Super-resolution\n * [Multi-frame image super-resolution](http:\/\/www.robots.ox.ac.uk\/~vgg\/software\/SR\/)\n    * Pickup, L. C. Machine Learning in Multi-frame Image Super-resolution, PhD thesis 2008\n * [Markov Random Fields for Super-Resolution](http:\/\/people.csail.mit.edu\/billf\/project%20pages\/sresCode\/Markov%20Random%20Fields%20for%20Super-Resolution.html)\n    * W. T Freeman and C. Liu. Markov Random Fields for Super-resolution and Texture Synthesis. In A. Blake, P. Kohli, and C. Rother, eds., Advances in Markov Random Fields for Vision and Image Processing, Chapter 10. MIT Press, 2011\n * [Sparse regression and natural image prior](https:\/\/people.mpi-inf.mpg.de\/~kkim\/supres\/supres.htm)\n    * K. I. Kim and Y. Kwon, \"Single-image super-resolution using sparse regression and natural image prior\", IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 32, no. 6, pp. 1127-1133, 2010.\n * [Single-Image Super Resolution via a Statistical Model](http:\/\/www.cs.technion.ac.il\/~elad\/Various\/SingleImageSR_TIP14_Box.zip)\n    * T. Peleg and M. Elad, A Statistical Prediction Model Based on Sparse Representations for Single Image Super-Resolution, IEEE Transactions on Image Processing, Vol. 23, No. 6, Pages 2569-2582, June 2014\n * [Sparse Coding for Super-Resolution](http:\/\/www.cs.technion.ac.il\/~elad\/Various\/Single_Image_SR.zip)\n    * R. Zeyde, M. Elad, and M. Protter On Single Image Scale-Up using Sparse-Representations, Curves & Surfaces, Avignon-France, June 24-30, 2010 (appears also in Lecture-Notes-on-Computer-Science - LNCS).\n * [Patch-wise Sparse Recovery](http:\/\/www.ifp.illinois.edu\/~jyang29\/ScSR.htm)\n    * Jianchao Yang, John Wright, Thomas Huang, and Yi Ma. Image super-resolution via sparse representation. IEEE Transactions on Image Processing (TIP), vol. 19, issue 11, 2010.\n * [Neighbor embedding](http:\/\/www.jdl.ac.cn\/user\/hchang\/doc\/code.rar)\n    * H. Chang, D.Y. Yeung, Y. Xiong. Super-resolution through neighbor embedding. Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR), vol.1, pp.275-282, Washington, DC, USA, 27 June - 2 July 2004.\n * [Deformable Patches](https:\/\/sites.google.com\/site\/yuzhushome\/single-image-super-resolution-using-deformable-patches)\n    *  Yu Zhu, Yanning Zhang and Alan Yuille, Single Image Super-resolution using Deformable Patches, CVPR 2014\n * [SRCNN](http:\/\/mmlab.ie.cuhk.edu.hk\/projects\/SRCNN.html)\n    * Chao Dong, Chen Change Loy, Kaiming He, Xiaoou Tang, Learning a Deep Convolutional Network for Image Super-Resolution, in ECCV 2014\n * [A+: Adjusted Anchored Neighborhood Regression](http:\/\/www.vision.ee.ethz.ch\/~timofter\/ACCV2014_ID820_SUPPLEMENTARY\/index.html)\n    * R. Timofte, V. De Smet, and L. Van Gool. A+: Adjusted Anchored Neighborhood Regression for Fast Super-Resolution, ACCV 2014\n * [Transformed Self-Exemplars](https:\/\/sites.google.com\/site\/jbhuang0604\/publications\/struct_sr)\n    * Jia-Bin Huang, Abhishek Singh, and Narendra Ahuja, Single Image Super-Resolution using Transformed Self-Exemplars, IEEE Conference on Computer Vision and Pattern Recognition, 2015\n\n###### Image Deblurring\n\nNon-blind deconvolution\n * [Spatially variant non-blind deconvolution](http:\/\/homes.cs.washington.edu\/~shanqi\/work\/spvdeconv\/)\n * [Handling Outliers in Non-blind Image Deconvolution](http:\/\/cg.postech.ac.kr\/research\/deconv_outliers\/)\n * [Hyper-Laplacian Priors](http:\/\/cs.nyu.edu\/~dilip\/research\/fast-deconvolution\/)\n * [From Learning Models of Natural Image Patches to Whole Image Restoration](http:\/\/people.csail.mit.edu\/danielzoran\/epllcode.zip)\n * [Deep Convolutional Neural Network for Image Deconvolution](http:\/\/lxu.me\/projects\/dcnn\/)\n * [Neural Deconvolution](http:\/\/webdav.is.mpg.de\/pixel\/neural_deconvolution\/)\n\nBlind deconvolution\n * [Removing Camera Shake From A Single Photograph](http:\/\/www.cs.nyu.edu\/~fergus\/research\/deblur.html)\n * [High-quality motion deblurring from a single image](http:\/\/www.cse.cuhk.edu.hk\/leojia\/projects\/motion_deblurring\/)\n * [Two-Phase Kernel Estimation for Robust Motion Deblurring](http:\/\/www.cse.cuhk.edu.hk\/leojia\/projects\/robust_deblur\/)\n * [Blur kernel estimation using the radon transform](http:\/\/people.csail.mit.edu\/taegsang\/Documents\/RadonDeblurringCode.zip)\n * [Fast motion deblurring](http:\/\/cg.postech.ac.kr\/research\/fast_motion_deblurring\/)\n * [Blind Deconvolution Using a Normalized Sparsity Measure](http:\/\/cs.nyu.edu\/\/~dilip\/research\/blind-deconvolution\/)\n * [Blur-kernel estimation from spectral irregularities](http:\/\/www.cs.huji.ac.il\/~raananf\/projects\/deblur\/)\n * [Efficient marginal likelihood optimization in blind deconvolution](http:\/\/www.wisdom.weizmann.ac.il\/~levina\/papers\/LevinEtalCVPR2011Code.zip)\n * [Unnatural L0 Sparse Representation for Natural Image Deblurring](http:\/\/www.cse.cuhk.edu.hk\/leojia\/projects\/l0deblur\/)\n * [Edge-based Blur Kernel Estimation Using Patch Priors](http:\/\/cs.brown.edu\/~lbsun\/deblur2013\/deblur2013iccp.html)\n * [Blind Deblurring Using Internal Patch Recurrence](http:\/\/www.wisdom.weizmann.ac.il\/~vision\/BlindDeblur.html)\n\nNon-uniform Deblurring\n * [Non-uniform Deblurring for Shaken Images](http:\/\/www.di.ens.fr\/willow\/research\/deblurring\/)\n * [Single Image Deblurring Using Motion Density Functions](http:\/\/grail.cs.washington.edu\/projects\/mdf_deblurring\/)\n * [Image Deblurring using Inertial Measurement Sensors](http:\/\/research.microsoft.com\/en-us\/um\/redmond\/groups\/ivm\/imudeblurring\/)\n * [Fast Removal of Non-uniform Camera Shake](http:\/\/webdav.is.mpg.de\/pixel\/fast_removal_of_camera_shake\/)\n\n\n###### Image Completion\n * [GIMP Resynthesizer](http:\/\/registry.gimp.org\/node\/27986)\n * [Priority BP](http:\/\/lafarren.com\/image-completer\/)\n * [ImageMelding](http:\/\/www.ece.ucsb.edu\/~psen\/melding)\n * [PlanarStructureCompletion](https:\/\/sites.google.com\/site\/jbhuang0604\/publications\/struct_completion)\n\n###### Image Retargeting\n * [RetargetMe](http:\/\/people.csail.mit.edu\/mrub\/retargetme\/)\n\n###### Alpha Matting\n * [Alpha Matting Evaluation](http:\/\/www.alphamatting.com\/)\n * [Closed-form image matting](http:\/\/people.csail.mit.edu\/alevin\/matting.tar.gz)\n * [Spectral Matting](http:\/\/www.vision.huji.ac.il\/SpectralMatting\/)\n * [Learning-based Matting](http:\/\/www.mathworks.com\/matlabcentral\/fileexchange\/31412-learning-based-digital-matting)\n * [Improving Image Matting using Comprehensive Sampling Sets](http:\/\/www.alphamatting.com\/ImprovingMattingComprehensiveSamplingSets_CVPR2013.zip)\n\n###### Image Pyramid\n* [The Steerable Pyramid](http:\/\/www.cns.nyu.edu\/~eero\/steerpyr\/)\n* [CurveLab](http:\/\/www.curvelet.org\/)\n\n###### Edge-preserving image processing\n * [Fast Bilateral Filter](http:\/\/people.csail.mit.edu\/sparis\/bf\/)\n * [O(1) Bilateral Filter](http:\/\/www.cs.cityu.edu.hk\/~qiyang\/publications\/code\/qx.cvpr09.ctbf.zip)\n * [Recursive Bilateral Filtering](http:\/\/www.cs.cityu.edu.hk\/~qiyang\/publications\/eccv-12\/)\n * [Rolling Guidance Filter](http:\/\/www.cse.cuhk.edu.hk\/leojia\/projects\/rollguidance\/)\n * [Relative Total Variation](http:\/\/www.cse.cuhk.edu.hk\/leojia\/projects\/texturesep\/index.html)\n * [L0 Gradient Optimization](http:\/\/www.cse.cuhk.edu.hk\/leojia\/projects\/L0smoothing\/index.html)\n * [Domain Transform](http:\/\/www.inf.ufrgs.br\/~eslgastal\/DomainTransform\/)\n * [Adaptive Manifold](http:\/\/inf.ufrgs.br\/~eslgastal\/AdaptiveManifolds\/)\n * [Guided image filtering](http:\/\/research.microsoft.com\/en-us\/um\/people\/kahe\/eccv10\/)\n\n#### Intrinsic Images\n\n* [Recovering Intrinsic Images with a global Sparsity Prior on Reflectance](http:\/\/people.tuebingen.mpg.de\/mkiefel\/projects\/intrinsic\/)\n* [Intrinsic Images by Clustering](http:\/\/giga.cps.unizar.es\/~elenag\/projects\/EGSR2012_intrinsic\/)\n\n#### Contour Detection and Image Segmentation\n * [Mean Shift Segmentation](http:\/\/coewww.rutgers.edu\/riul\/research\/code\/EDISON\/)\n * [Graph-based Segmentation](http:\/\/cs.brown.edu\/~pff\/segment\/)\n * [Normalized Cut](http:\/\/www.cis.upenn.edu\/~jshi\/software\/)\n * [Grab Cut](http:\/\/grabcut.weebly.com\/background--algorithm.html)\n * [Contour Detection and Image Segmentation](http:\/\/www.eecs.berkeley.edu\/Research\/Projects\/CS\/vision\/grouping\/resources.html)\n * [Structured Edge Detection](http:\/\/research.microsoft.com\/en-us\/downloads\/389109f6-b4e8-404c-84bf-239f7cbf4e3d\/)\n * [Pointwise Mutual Information](http:\/\/web.mit.edu\/phillipi\/pmi-boundaries\/)\n * [SLIC Super-pixel](http:\/\/ivrl.epfl.ch\/research\/superpixels)\n * [QuickShift](http:\/\/www.vlfeat.org\/overview\/quickshift.html)\n * [TurboPixels](http:\/\/www.cs.toronto.edu\/~babalex\/research.html)\n * [Entropy Rate Superpixel](http:\/\/mingyuliu.net\/)\n * [Contour Relaxed Superpixels](http:\/\/www.vsi.cs.uni-frankfurt.de\/research\/current-projects\/research\/superpixel-segmentation\/)\n * [SEEDS](http:\/\/www.mvdblive.org\/seeds\/)\n * [SEEDS Revised](https:\/\/github.com\/davidstutz\/seeds-revised)\n * [Multiscale Combinatorial Grouping](http:\/\/www.eecs.berkeley.edu\/Research\/Projects\/CS\/vision\/grouping\/mcg\/)\n * [Fast Edge Detection Using Structured Forests](https:\/\/github.com\/pdollar\/edges)\n\n#### Interactive Image Segmentation\n * [Random Walker](http:\/\/cns.bu.edu\/~lgrady\/software.html)\n * [Geodesic Segmentation](http:\/\/www.tc.umn.edu\/~baixx015\/)\n * [Lazy Snapping](http:\/\/research.microsoft.com\/apps\/pubs\/default.aspx?id=69040)\n * [Power Watershed](http:\/\/powerwatershed.sourceforge.net\/)\n * [Geodesic Graph Cut](http:\/\/www.adobe.com\/technology\/people\/san-jose\/brian-price.html)\n * [Segmentation by Transduction](http:\/\/www.cs.cmu.edu\/~olivierd\/)\n\n#### Video Segmentation\n * [Video Segmentation with Superpixels](http:\/\/www.mpi-inf.mpg.de\/departments\/computer-vision-and-multimodal-computing\/research\/image-and-video-segmentation\/video-segmentation-with-superpixels\/)\n * [Efficient hierarchical graph-based video segmentation](http:\/\/www.cc.gatech.edu\/cpl\/projects\/videosegmentation\/)\n * [Object segmentation in video](http:\/\/lmb.informatik.uni-freiburg.de\/Publications\/2011\/OB11\/)\n * [Streaming hierarchical video segmentation](http:\/\/www.cse.buffalo.edu\/~jcorso\/r\/supervoxels\/)\n\n#### Camera calibration\n * [Camera Calibration Toolbox for Matlab](http:\/\/www.vision.caltech.edu\/bouguetj\/calib_doc\/)\n * [Camera calibration With OpenCV](http:\/\/docs.opencv.org\/trunk\/doc\/tutorials\/calib3d\/camera_calibration\/camera_calibration.html#)\n * [Multiple Camera Calibration Toolbox](https:\/\/sites.google.com\/site\/prclibo\/toolbox)\n\n#### Simultaneous localization and mapping\n\n###### SLAM community:\n * [openSLAM](https:\/\/www.openslam.org\/)\n * [Kitti Odometry: benchmark for outdoor visual odometry (codes may be available)](http:\/\/www.cvlibs.net\/datasets\/kitti\/eval_odometry.php)\n\n###### Tracking\/Odometry:\n * [LIBVISO2: C++ Library for Visual Odometry 2](http:\/\/www.cvlibs.net\/software\/libviso\/)\n * [PTAM: Parallel tracking and mapping](http:\/\/www.robots.ox.ac.uk\/~gk\/PTAM\/)\n * [KFusion: Implementation of KinectFusion](https:\/\/github.com\/GerhardR\/kfusion)\n * [kinfu_remake: Lightweight, reworked and optimized version of Kinfu.](https:\/\/github.com\/Nerei\/kinfu_remake)\n * [LVR-KinFu: kinfu_remake based Large Scale KinectFusion with online reconstruction](http:\/\/las-vegas.uni-osnabrueck.de\/related-projects\/lvr-kinfu\/)\n * [InfiniTAM: Implementation of multi-platform large-scale depth tracking and fusion](http:\/\/www.robots.ox.ac.uk\/~victor\/infinitam\/)\n * [VoxelHashing: Large-scale KinectFusion](https:\/\/github.com\/nachtmar\/VoxelHashing)\n * [SLAMBench: Multiple-implementation of KinectFusion](http:\/\/apt.cs.manchester.ac.uk\/projects\/PAMELA\/tools\/SLAMBench\/)\n * [SVO: Semi-direct visual odometry](https:\/\/github.com\/uzh-rpg\/rpg_svo)\n * [DVO: dense visual odometry](https:\/\/github.com\/tum-vision\/dvo_slam)\n * [FOVIS: RGB-D visual odometry](https:\/\/code.google.com\/p\/fovis\/)\n\n###### Graph Optimization:\n * [GTSAM: General smoothing and mapping library for Robotics and SFM](https:\/\/collab.cc.gatech.edu\/borg\/gtsam?destination=node%2F299) -- Georgia Institute of Technology\n * [G2O: General framework for graph optomization](https:\/\/github.com\/RainerKuemmerle\/g2o)\n\n###### Loop Closure:\n * [FabMap: appearance-based loop closure system](http:\/\/www.robots.ox.ac.uk\/~mjc\/Software.htm) - also available in [OpenCV2.4.11](http:\/\/docs.opencv.org\/2.4\/modules\/contrib\/doc\/openfabmap.html)\n * [DBoW2: binary bag-of-words loop detection system](http:\/\/webdiis.unizar.es\/~dorian\/index.php?p=32)\n\n###### Localization & Mapping:\n * [RatSLAM](https:\/\/code.google.com\/p\/ratslam\/)\n * [LSD-SLAM](https:\/\/github.com\/tum-vision\/lsd_slam)\n * [ORB-SLAM](https:\/\/github.com\/raulmur\/ORB_SLAM)\n\n#### Single-view Spatial Understanding\n * [Geometric Context](http:\/\/web.engr.illinois.edu\/~dhoiem\/projects\/software.html) - Derek Hoiem (CMU)\n * [Recovering Spatial Layout](http:\/\/web.engr.illinois.edu\/~dhoiem\/software\/counter.php?Down=varsha_spatialLayout.zip) - Varsha Hedau (UIUC)\n * [Geometric Reasoning](http:\/\/www.cs.cmu.edu\/~.\/dclee\/code\/index.html) - David C. Lee (CMU)\n * [RGBD2Full3D](https:\/\/github.com\/arron2003\/rgbd2full3d) - Ruiqi Guo (UIUC)\n\n#### Object Detection\n * [INRIA Object Detection and Localization Toolkit](http:\/\/pascal.inrialpes.fr\/soft\/olt\/)\n * [Discriminatively trained deformable part models](http:\/\/www.cs.berkeley.edu\/~rbg\/latent\/)\n * [VOC-DPM](https:\/\/github.com\/rbgirshick\/voc-dpm)\n * [Histograms of Sparse Codes for Object Detection](http:\/\/www.ics.uci.edu\/~dramanan\/software\/sparse\/)\n * [R-CNN: Regions with Convolutional Neural Network Features](https:\/\/github.com\/rbgirshick\/rcnn)\n * [SPP-Net](https:\/\/github.com\/ShaoqingRen\/SPP_net)\n * [BING: Objectness Estimation](http:\/\/mmcheng.net\/bing\/comment-page-9\/)\n * [Edge Boxes](https:\/\/github.com\/pdollar\/edges)\n * [ReInspect](https:\/\/github.com\/Russell91\/ReInspect)\n\n#### Nearest Neighbor Search\n\n###### General purpose nearest neighbor search\n * [ANN: A Library for Approximate Nearest Neighbor Searching](http:\/\/www.cs.umd.edu\/~mount\/ANN\/)\n * [FLANN - Fast Library for Approximate Nearest Neighbors](http:\/\/www.cs.ubc.ca\/research\/flann\/)\n * [Fast k nearest neighbor search using GPU](http:\/\/vincentfpgarcia.github.io\/kNN-CUDA\/)\n\n###### Nearest Neighbor Field Estimation\n * [PatchMatch](http:\/\/gfx.cs.princeton.edu\/gfx\/pubs\/Barnes_2009_PAR\/index.php)\n * [Generalized PatchMatch](http:\/\/gfx.cs.princeton.edu\/pubs\/Barnes_2010_TGP\/index.php)\n * [Coherency Sensitive Hashing](http:\/\/www.eng.tau.ac.il\/~simonk\/CSH\/)\n * [PMBP: PatchMatch Belief Propagation](https:\/\/github.com\/fbesse\/pmbp)\n * [TreeCANN](http:\/\/www.eng.tau.ac.il\/~avidan\/papers\/TreeCANN_code_20121022.rar)\n\n#### Visual Tracking\n* [Visual Tracker Benchmark](https:\/\/sites.google.com\/site\/trackerbenchmark\/benchmarks\/v10)\n* [Visual Tracking Challenge](http:\/\/www.votchallenge.net\/)\n* [Kanade-Lucas-Tomasi Feature Tracker](http:\/\/www.ces.clemson.edu\/~stb\/klt\/)\n* [Extended Lucas-Kanade Tracking](http:\/\/www.eng.tau.ac.il\/~oron\/ELK\/ELK.html)\n* [Online-boosting Tracking](http:\/\/www.vision.ee.ethz.ch\/boostingTrackers\/)\n* [Spatio-Temporal Context Learning](http:\/\/www4.comp.polyu.edu.hk\/~cslzhang\/STC\/STC.htm)\n* [Locality Sensitive Histograms](http:\/\/www.shengfenghe.com\/visual-tracking-via-locality-sensitive-histograms.html)\n* [Enhanced adaptive coupled-layer LGTracker++](http:\/\/www.cv-foundation.org\/openaccess\/content_iccv_workshops_2013\/W03\/papers\/Xiao_An_Enhanced_Adaptive_2013_ICCV_paper.pdf)\n* [TLD: Tracking - Learning - Detection](http:\/\/personal.ee.surrey.ac.uk\/Personal\/Z.Kalal\/tld.html)\n* [CMT: Clustering of Static-Adaptive Correspondences for Deformable Object Tracking](http:\/\/www.gnebehay.com\/cmt\/)\n* [Kernelized Correlation Filters](http:\/\/home.isr.uc.pt\/~henriques\/circulant\/)\n* [Accurate Scale Estimation for Robust Visual Tracking](http:\/\/www.cvl.isy.liu.se\/en\/research\/objrec\/visualtracking\/scalvistrack\/index.html)\n* [Multiple Experts using Entropy Minimization](http:\/\/cs-people.bu.edu\/jmzhang\/MEEM\/MEEM.html)\n* [TGPR](http:\/\/www.dabi.temple.edu\/~hbling\/code\/TGPR.htm)\n* [CF2: Hierarchical Convolutional Features for Visual Tracking](https:\/\/sites.google.com\/site\/jbhuang0604\/publications\/cf2)\n* [Modular Tracking Framework](http:\/\/webdocs.cs.ualberta.ca\/~vis\/mtf\/index.html)\n\n#### Saliency Detection\n\n#### Attributes\n\n#### Action Reconition\n\n#### Egocentric cameras\n\n#### Human-in-the-loop systems\n\n#### Image Captioning\n * [NeuralTalk](https:\/\/github.com\/karpathy\/neuraltalk\ufeff) -\n\n#### Optimization\n * [Ceres Solver](http:\/\/ceres-solver.org\/) - Nonlinear least-square problem and unconstrained optimization solver\n * [NLopt](http:\/\/ab-initio.mit.edu\/wiki\/index.php\/NLopt)- Nonlinear least-square problem and unconstrained optimization solver\n * [OpenGM](http:\/\/hci.iwr.uni-heidelberg.de\/opengm2\/) - Factor graph based discrete optimization and inference solver\n * [GTSAM](https:\/\/collab.cc.gatech.edu\/borg\/gtsam\/) - Factor graph based lease-square optimization solver\n\n#### Deep Learning\n * [Awesome Deep Vision](https:\/\/github.com\/kjw0612\/awesome-deep-vision)\n\n#### Machine Learning\n * [Awesome Machine Learning](https:\/\/github.com\/josephmisiti\/awesome-machine-learning)\n * [Bob: a free signal processing and machine learning toolbox for researchers](http:\/\/idiap.github.io\/bob\/)\n * [LIBSVM -- A Library for Support Vector Machines](https:\/\/www.csie.ntu.edu.tw\/~cjlin\/libsvm\/)\n\n## Datasets\n\n#### External Dataset Link Collection\n * [CV Datasets on the web](http:\/\/www.cvpapers.com\/datasets.html) - CVPapers\n * [Are we there yet?](http:\/\/rodrigob.github.io\/are_we_there_yet\/build\/) - Which paper provides the best results on standard dataset X?\n * [Computer Vision Dataset on the web](http:\/\/www.cvpapers.com\/datasets.html)\n * [Yet Another Computer Vision Index To Datasets](http:\/\/riemenschneider.hayko.at\/vision\/dataset\/)\n * [ComputerVisionOnline Datasets](http:\/\/www.computervisiononline.com\/datasets)\n * [CVOnline Dataset](http:\/\/homepages.inf.ed.ac.uk\/cgi\/rbf\/CVONLINE\/entries.pl?TAG363)\n * [CV datasets](http:\/\/clickdamage.com\/sourcecode\/cv_datasets.php)\n * [visionbib](http:\/\/datasets.visionbib.com\/info-index.html)\n * [VisualData](http:\/\/www.visualdata.io\/)\n\n#### Low-level Vision\n\n###### Stereo Vision\n * [Middlebury Stereo Vision](http:\/\/vision.middlebury.edu\/stereo\/)\n * [The KITTI Vision Benchmark Suite](http:\/\/www.cvlibs.net\/datasets\/kitti\/eval_stereo_flow.php?benchmark=stero)\n * [LIBELAS: Library for Efficient Large-scale Stereo Matching](http:\/\/www.cvlibs.net\/software\/libelas\/)\n * [Ground Truth Stixel Dataset](http:\/\/www.6d-vision.com\/ground-truth-stixel-dataset)\n\n###### Optical Flow\n * [Middlebury Optical Flow Evaluation](http:\/\/vision.middlebury.edu\/flow\/)\n * [MPI-Sintel Optical Flow Dataset and Evaluation](http:\/\/sintel.is.tue.mpg.de\/)\n * [The KITTI Vision Benchmark Suite](http:\/\/www.cvlibs.net\/datasets\/kitti\/eval_stereo_flow.php?benchmark=flow)\n * [HCI Challenge](http:\/\/hci.iwr.uni-heidelberg.de\/Benchmarks\/document\/Challenging_Data_for_Stereo_and_Optical_Flow\/)\n\n###### Video Object Segmentation\n * [DAVIS: Densely Annotated VIdeo Segmentation](http:\/\/davischallenge.org\/)\n * [SegTrack v2](http:\/\/web.engr.oregonstate.edu\/~lif\/SegTrack2\/dataset.html)\n\n\n###### Change Detection\n * [Labeled and Annotated Sequences for Integral Evaluation of SegmenTation Algorithms](http:\/\/www.gti.ssr.upm.es\/data\/LASIESTA)\n * [ChangeDetection.net](http:\/\/www.changedetection.net\/)\n\n###### Image Super-resolutions\n * [Single-Image Super-Resolution: A Benchmark](https:\/\/eng.ucmerced.edu\/people\/cyang35\/ECCV14\/ECCV14.html)\n\n#### Intrinsic Images\n * [Ground-truth dataset and baseline evaluations for intrinsic image algorithms](http:\/\/www.mit.edu\/~kimo\/publications\/intrinsic\/)\n * [Intrinsic Images in the Wild](http:\/\/opensurfaces.cs.cornell.edu\/intrinsic\/)\n * [Intrinsic Image Evaluation on Synthetic Complex Scenes](http:\/\/www.cic.uab.cat\/Datasets\/synthetic_intrinsic_image_dataset\/)\n\n#### Material Recognition\n * [OpenSurface](http:\/\/opensurfaces.cs.cornell.edu\/)\n * [Flickr Material Database](http:\/\/people.csail.mit.edu\/celiu\/CVPR2010\/)\n * [Materials in Context Dataset](http:\/\/opensurfaces.cs.cornell.edu\/publications\/minc\/)\n\n#### Multi-view Reconsturction\n* [Multi-View Stereo Reconstruction](http:\/\/vision.middlebury.edu\/mview\/)\n\n#### Saliency Detection\n\n#### Visual Tracking\n * [Visual Tracker Benchmark](https:\/\/sites.google.com\/site\/trackerbenchmark\/benchmarks\/v10)\n * [Visual Tracker Benchmark v1.1](https:\/\/sites.google.com\/site\/benchmarkpami\/)\n * [VOT Challenge](http:\/\/www.votchallenge.net\/)\n * [Princeton Tracking Benchmark](http:\/\/tracking.cs.princeton.edu\/)\n * [Tracking Manipulation Tasks (TMT)](http:\/\/webdocs.cs.ualberta.ca\/~vis\/trackDB\/)\n\n#### Visual Surveillance\n * [VIRAT](http:\/\/www.viratdata.org\/)\n * [CAM2](https:\/\/cam2.ecn.purdue.edu\/)\n\n#### Saliency Detection\n\n#### Change detection\n * [ChangeDetection.net](http:\/\/changedetection.net\/)\n\n#### Visual Recognition\n\n###### Image Classification\n * [The PASCAL Visual Object Classes](http:\/\/pascallin.ecs.soton.ac.uk\/challenges\/VOC\/)\n * [ImageNet Large Scale Visual Recognition Challenge](http:\/\/www.image-net.org\/challenges\/LSVRC\/2014\/)\n\n###### Scene Recognition\n * [SUN Database](http:\/\/groups.csail.mit.edu\/vision\/SUN\/)\n * [Place Dataset](http:\/\/places.csail.mit.edu\/)\n\n###### Object Detection\n * [The PASCAL Visual Object Classes](http:\/\/pascallin.ecs.soton.ac.uk\/challenges\/VOC\/)\n * [ImageNet Object Detection Challenge](http:\/\/www.image-net.org\/challenges\/LSVRC\/2014\/)\n * [Microsoft COCO](http:\/\/mscoco.org\/)\n\n###### Semantic labeling\n * [Stanford background dataset](http:\/\/dags.stanford.edu\/projects\/scenedataset.html)\n * [CamVid](http:\/\/mi.eng.cam.ac.uk\/research\/projects\/VideoRec\/CamVid\/)\n * [Barcelona Dataset](http:\/\/www.cs.unc.edu\/~jtighe\/Papers\/ECCV10\/)\n * [SIFT Flow Dataset](http:\/\/www.cs.unc.edu\/~jtighe\/Papers\/ECCV10\/siftflow\/SiftFlowDataset.zip)\n\n###### Multi-view Object Detection\n * [3D Object Dataset](http:\/\/cvgl.stanford.edu\/resources.html)\n * [EPFL Car Dataset](http:\/\/cvlab.epfl.ch\/data\/pose)\n * [KTTI Dection Dataset](http:\/\/www.cvlibs.net\/datasets\/kitti\/eval_object.php)\n * [SUN 3D Dataset](http:\/\/sun3d.cs.princeton.edu\/)\n * [PASCAL 3D+](http:\/\/cvgl.stanford.edu\/projects\/pascal3d.html)\n * [NYU Car Dataset](http:\/\/nyc3d.cs.cornell.edu\/)\n\n###### Fine-grained Visual Recognition\n * [Fine-grained Classification Challenge](https:\/\/sites.google.com\/site\/fgcomp2013\/)\n * [Caltech-UCSD Birds 200](http:\/\/www.vision.caltech.edu\/visipedia\/CUB-200.html)\n\n###### Pedestrian Detection\n * [Caltech Pedestrian Detection Benchmark](http:\/\/www.vision.caltech.edu\/Image_Datasets\/CaltechPedestrians\/)\n * [ETHZ Pedestrian Detection](https:\/\/data.vision.ee.ethz.ch\/cvl\/aess\/dataset\/)\n\n#### Action Recognition\n\n###### Image-based\n\n###### Video-based\n * [HOLLYWOOD2 Dataset](http:\/\/www.di.ens.fr\/~laptev\/actions\/hollywood2\/)\n * [UCF Sports Action Data Set](http:\/\/crcv.ucf.edu\/data\/UCF_Sports_Action.php)\n\n###### Image Deblurring\n * [Sun dataset](http:\/\/cs.brown.edu\/~lbsun\/deblur2013\/deblur2013iccp.html)\n * [Levin dataset](http:\/\/www.wisdom.weizmann.ac.il\/~levina\/papers\/LevinEtalCVPR09Data.rar)\n\n#### Image Captioning\n * [Flickr 8K](http:\/\/nlp.cs.illinois.edu\/HockenmaierGroup\/Framing_Image_Description\/KCCA.html)\n * [Flickr 30K](http:\/\/shannon.cs.illinois.edu\/DenotationGraph\/)\n * [Microsoft COCO](http:\/\/mscoco.org\/)\n\n#### Scene Understanding\n \n [SUN RGB-D](http:\/\/rgbd.cs.princeton.edu\/) - A RGB-D Scene Understanding Benchmark Suite\n \n [NYU depth v2](http:\/\/cs.nyu.edu\/~silberman\/datasets\/nyu_depth_v2.html) - Indoor Segmentation and Support Inference from RGBD Images\n\n#### Aerial images\n \n [Aerial Image Segmentation](https:\/\/zenodo.org\/record\/1154821#.WmN9kHWnHIp) - Learning Aerial Image Segmentation From Online Maps\n\n\n## Resources for students\n\n#### Resource link collection\n * [Resources for students](http:\/\/people.csail.mit.edu\/fredo\/student.html) - Fr\u00e9do Durand (MIT)\n * [Advice for Graduate Students](http:\/\/www.dgp.toronto.edu\/~hertzman\/advice\/) - Aaron Hertzmann (Adobe Research)\n * [Graduate Skills Seminars](http:\/\/www.dgp.toronto.edu\/~hertzman\/courses\/gradSkills\/2010\/) - Yashar Ganjali, Aaron Hertzmann (University of Toronto)\n * [Research Skills](http:\/\/research.microsoft.com\/en-us\/um\/people\/simonpj\/papers\/giving-a-talk\/giving-a-talk.htm) - Simon Peyton Jones (Microsoft Research)\n * [Resource collection](http:\/\/web.engr.illinois.edu\/~taoxie\/advice.htm) - Tao Xie (UIUC) and Yuan Xie (UCSB)\n\n#### Writing\n * [Write Good Papers](http:\/\/people.csail.mit.edu\/fredo\/FredoGoodWriting.pdf) - Fr\u00e9do Durand (MIT)\n * [Notes on writing](http:\/\/people.csail.mit.edu\/fredo\/PUBLI\/writing.pdf) - Fr\u00e9do Durand (MIT)\n * [How to Write a Bad Article](http:\/\/people.csail.mit.edu\/fredo\/FredoBadWriting.pdf) - Fr\u00e9do Durand (MIT)\n * [How to write a good CVPR submission](http:\/\/billf.mit.edu\/sites\/default\/files\/documents\/cvprPapers.pdf) - William T. Freeman (MIT)\n * [How to write a great research paper](https:\/\/www.youtube.com\/watch?v=g3dkRsTqdDA) - Simon Peyton Jones (Microsoft Research)\n * [How to write a SIGGRAPH paper](http:\/\/www.slideshare.net\/jdily\/how-to-write-a-siggraph-paper) - SIGGRAPH ASIA 2011 Course\n * [Writing Research Papers](http:\/\/www.dgp.toronto.edu\/~hertzman\/advice\/writing-technical-papers.pdf) - Aaron Hertzmann (Adobe Research)\n * [How to Write a Paper for SIGGRAPH](http:\/\/www.computer.org\/csdl\/mags\/cg\/1987\/12\/mcg1987120062.pdf) - Jim Blinn\n * [How to Get Your SIGGRAPH Paper Rejected](http:\/\/www.siggraph.org\/sites\/default\/files\/kajiya.pdf) - Jim Kajiya (Microsoft Research)\n * [How to write a SIGGRAPH paper](www.liyiwei.org\/courses\/how-siga11\/liyiwei.pptx) - Li-Yi Wei (The University of Hong Kong)\n * [How to Write a Great Paper](http:\/\/www-hagen.informatik.uni-kl.de\/~bertram\/talks\/getpublished.pdf) - Martin Martin Hering Hering--Bertram (Hochschule Bremen University of Applied Sciences)\n * [How to have a paper get into SIGGRAPH?](http:\/\/www-ui.is.s.u-tokyo.ac.jp\/~takeo\/writings\/siggraph.html) - Takeo Igarashi (The University of Tokyo)\n * [Good Writing](http:\/\/www.cs.cmu.edu\/~pausch\/Randy\/Randy\/raibert.htm) - Marc H. Raibert (Boston Dynamics, Inc.)\n * [How to Write a Computer Vision Paper](http:\/\/web.engr.illinois.edu\/~dhoiem\/presentations\/How%20to%20Write%20a%20Computer%20Vison%20Paper.ppt) - Derek Hoiem (UIUC)\n * [Common mistakes in technical writing](http:\/\/www.cs.dartmouth.edu\/~wjarosz\/writing.html) - Wojciech Jarosz (Dartmouth College)\n\n\n#### Presentation\n * [Giving a Research Talk](http:\/\/people.csail.mit.edu\/fredo\/TalkAdvice.pdf) - Fr\u00e9do Durand (MIT)\n * [How to give a good talk](http:\/\/www.dgp.toronto.edu\/~hertzman\/courses\/gradSkills\/2010\/GivingGoodTalks.pdf) - David Fleet (University of Toronto) and Aaron Hertzmann (Adobe Research)\n * [Designing conference posters](http:\/\/colinpurrington.com\/tips\/poster-design) - Colin Purrington\n\n#### Research\n * [How to do research](http:\/\/people.csail.mit.edu\/billf\/www\/papers\/doresearch.pdf) - William T. Freeman (MIT)\n * [You and Your Research](http:\/\/www.cs.virginia.edu\/~robins\/YouAndYourResearch.html) - Richard Hamming\n * [Warning Signs of Bogus Progress in Research in an Age of Rich Computation and Information](http:\/\/yima.csl.illinois.edu\/psfile\/bogus.pdf) - Yi Ma (UIUC)\n * [Seven Warning Signs of Bogus Science](http:\/\/www.quackwatch.com\/01QuackeryRelatedTopics\/signs.html) - Robert L. Park\n * [Five Principles for Choosing Research Problems in Computer Graphics](https:\/\/www.youtube.com\/watch?v=v2Qaf8t8I6c) - Thomas Funkhouser (Cornell University)\n * [How To Do Research In the MIT AI Lab](http:\/\/www.cs.indiana.edu\/mit.research.how.to.html) - David Chapman (MIT)\n * [Recent Advances in Computer Vision](http:\/\/www.slideshare.net\/antiw\/recent-advances-in-computer-vision) - Ming-Hsuan Yang (UC Merced)\n * [How to Come Up with Research Ideas in Computer Vision?](http:\/\/www.slideshare.net\/jbhuang\/how-to-come-up-with-new-research-ideas-4005840) - Jia-Bin Huang (UIUC)\n * [How to Read Academic Papers](http:\/\/www.slideshare.net\/jbhuang\/how-to-read-academic-papers) - Jia-Bin Huang (UIUC)\n\n#### Time Management\n * [Time Management](https:\/\/www.youtube.com\/watch?v=oTugjssqOT0) - Randy Pausch (CMU)\n\n## Blogs\n * [Learn OpenCV](http:\/\/www.learnopencv.com\/) - Satya Mallick\n * [Tombone's Computer Vision Blog](http:\/\/www.computervisionblog.com\/) - Tomasz Malisiewicz\n * [Computer vision for dummies](http:\/\/www.visiondummy.com\/) - Vincent Spruyt\n * [Andrej Karpathy blog](http:\/\/karpathy.github.io\/) - Andrej Karpathy\n * [AI Shack](http:\/\/aishack.in\/) - Utkarsh Sinha\n * [Computer Vision Talks](http:\/\/computer-vision-talks.com\/) - Eugene Khvedchenya\n * [Computer Vision Basics with Python Keras and OpenCV](https:\/\/github.com\/jrobchin\/Computer-Vision-Basics-with-Python-Keras-and-OpenCV) - Jason Chin (University of Western Ontario)\n\n\n## Links\n* [The Computer Vision Industry](http:\/\/www.cs.ubc.ca\/~lowe\/vision.html) - David Lowe\n* [German Computer Vision Research Groups & Companies](http:\/\/hci.iwr.uni-heidelberg.de\/Links\/German_Vision\/)\n* [awesome-deep-learning](https:\/\/github.com\/ChristosChristofidis\/awesome-deep-learning)\n* [awesome-machine-learning](https:\/\/github.com\/josephmisiti\/awesome-machine-learning)\n* [Cat Paper Collection](http:\/\/www.eecs.berkeley.edu\/~junyanz\/cat\/cat_papers.html)\n* [Computer Vision News](http:\/\/www.rsipvision.com\/computer-vision-news\/)\n*\n## Songs\n* [The Fundamental Matrix Song](http:\/\/danielwedge.com\/fmatrix\/)\n* [The RANSAC Song](http:\/\/danielwedge.com\/ransac\/)\n* [Machine Learning A Cappella - Overfitting Thriller](https:\/\/www.youtube.com\/watch?v=DQWI1kvmwRg)","63891ddb":"## Make predictions\n\nWith the model trained, we can use it to make predictions about some images.","fde4dfb6":"A prediction is an array of 10 numbers. These describe the \"confidence\" of the model that the image corresponds to each of the 10 different articles of clothing. We can see which label has the highest confidence value:","854e9f60":"## Train the model\n\nTraining the neural network model requires the following steps:\n\n1. Feed the training data to the model\u2014in this example, the `train_images` and `train_labels` arrays.\n2. The model learns to associate images and labels.\n3. We ask the model to make predictions about a test set\u2014in this example, the `test_images` array. We verify that the predictions match the labels from the `test_labels` array. \n\nTo start training,  call the `model.fit` method\u2014the model is \"fit\" to the training data:","caa5dac2":"## Explore the data\n\nLet's explore the format of the dataset before training the model. The following shows there are 60,000 images in the training set, with each image represented as 28 x 28 pixels:","01f4bd3c":"Let's look at the 0th image, predictions, and prediction array. ","26bf98f1":"Loading the dataset returns four NumPy arrays:\n\n* The `train_images` and `train_labels` arrays are the *training set*\u2014the data the model uses to learn.\n* The model is tested against the *test set*, the `test_images`, and `test_labels` arrays.\n\nThe images are 28x28 NumPy arrays, with pixel values ranging between 0 and 255. The *labels* are an array of integers, ranging from 0 to 9. These correspond to the *class* of clothing the image represents:\n\n<table>\n  <tr>\n    <th>Label<\/th>\n    <th>Class<\/th> \n  <\/tr>\n  <tr>\n    <td>0<\/td>\n    <td>T-shirt\/top<\/td> \n  <\/tr>\n  <tr>\n    <td>1<\/td>\n    <td>Trouser<\/td> \n  <\/tr>\n    <tr>\n    <td>2<\/td>\n    <td>Pullover<\/td> \n  <\/tr>\n    <tr>\n    <td>3<\/td>\n    <td>Dress<\/td> \n  <\/tr>\n    <tr>\n    <td>4<\/td>\n    <td>Coat<\/td> \n  <\/tr>\n    <tr>\n    <td>5<\/td>\n    <td>Sandal<\/td> \n  <\/tr>\n    <tr>\n    <td>6<\/td>\n    <td>Shirt<\/td> \n  <\/tr>\n    <tr>\n    <td>7<\/td>\n    <td>Sneaker<\/td> \n  <\/tr>\n    <tr>\n    <td>8<\/td>\n    <td>Bag<\/td> \n  <\/tr>\n    <tr>\n    <td>9<\/td>\n    <td>Ankle boot<\/td> \n  <\/tr>\n<\/table>\n\nEach image is mapped to a single label. Since the *class names* are not included with the dataset, store them here to use later when plotting the images:","03146267":"We can graph this to look at the full set of 10 channels","94229278":"And the test set contains 10,000 images labels:","3e1c0bfc":"So the model is most confident that this image is an ankle boot, or `class_names[9]`. And we can check the test label to see this is correct:","790e18e9":"Now predict the image:","679d67e1":"Here, the model has predicted the label for each image in the testing set. Let's take a look at the first prediction:","9f5622b8":"Each label is an integer between 0 and 9:","eeb8a11f":"## Build the model\n\nBuilding the neural network requires configuring the layers of the model, then compiling the model.","3112a8e7":"Display the first 25 images from the *training set* and display the class name below each image. Verify that the data is in the correct format and we're ready to build and train the network.","a719d19c":"## Preprocess the data\n\nThe data must be preprocessed before training the network. If you inspect the first image in the training set, you will see that the pixel values fall in the range of 0 to 255:","eda1939e":"And, as before, the model predicts a label of 9.","a9e1d6eb":"`tf.keras` models are optimized to make predictions on a *batch*, or collection, of examples at once. So even though we're using a single image, we need to add it to a list:","eff64972":"The first layer in this network, `tf.keras.layers.Flatten`, transforms the format of the images from a 2d-array (of 28 by 28 pixels), to a 1d-array of 28 * 28 = 784 pixels. Think of this layer as unstacking rows of pixels in the image and lining them up. This layer has no parameters to learn; it only reformats the data.\n\nAfter the pixels are flattened, the network consists of a sequence of two `tf.keras.layers.Dense` layers. These are densely-connected, or fully-connected, neural layers. The first `Dense` layer has 128 nodes (or neurons). The second (and last) layer is a 10-node *softmax* layer\u2014this returns an array of 10 probability scores that sum to 1. Each node contains a score that indicates the probability that the current image belongs to one of the 10 classes.\n\n### Compile the model\n\nBefore the model is ready for training, it needs a few more settings. These are added during the model's *compile* step:\n\n* *Loss function* \u2014This measures how accurate the model is during training. We want to minimize this function to \"steer\" the model in the right direction.\n* *Optimizer* \u2014This is how the model is updated based on the data it sees and its loss function.\n* *Metrics* \u2014Used to monitor the training and testing steps. The following example uses *accuracy*, the fraction of the images that are correctly classified.","a5242939":"# Awesome Computer Vision CV Resources","7396a719":"We scale these values to a range of 0 to 1 before feeding to the neural network model. For this, we divide the values by 255. It's important that the *training set* and the *testing set* are preprocessed in the same way:","bfb9a539":"There are 10,000 images in the test set. Again, each image is represented as 28 x 28 pixels:","5784cde1":"Likewise, there are 60,000 labels in the training set:","0a8bfd49":"## Credits (Reference)\n\n> * [TensorFlow Docs - GitHub](https:\/\/github.com\/tensorflow\/docs)\n> * [GitHub Deep Learning Topic](https:\/\/github.com\/topics\/deep-learning)\n> * [Jiwon Kim](https:\/\/github.com\/kjw0612\/awesome-deep-vision)\n> * [Jia-Bin Huang](https:\/\/github.com\/jbhuang0604\/awesome-computer-vision)\n\n## License\n\n[![CC0](http:\/\/mirrors.creativecommons.org\/presskit\/buttons\/88x31\/svg\/cc-zero.svg)](https:\/\/creativecommons.org\/publicdomain\/zero\/1.0\/)\n\n### Please ***UPVOTE*** my kernel if you like it or wanna fork it.\n\n##### Feedback: If you have any ideas or you want any other content to be added to this curated list, please feel free to make any comments to make it better.\n#### I am open to have your *feedback* for improving this ***kernel***\n###### Hope you enjoyed this kernel!\n\n### Thanks for visiting my *Kernel* and please *UPVOTE* to stay connected and follow up the *further updates!*","435e649c":"# Awesome Computer Vision CV Resources and Basic Fashion MNIST Classification\n\n    This Kernel has the basic Fashion MNIST Classification and the curated lists of collection of awesome computer vision resources and deep learning resources for computer vision with two different parts as follows\n\n> #### **Credits**: Thanks to **TensorFlow Team** for Fashion MNIST Classification using tf.keras, **Jiwon Kim** and **Jia-Bin Huang** and other contributers for such wonderful curated collections!\n\n### Here are some of *my kernel notebooks* for **Machine Learning and Data Science** as follows, ***Upvote*** them if you *like* them\n\n> * [Awesome Deep Learning Basics and Resources](https:\/\/www.kaggle.com\/arunkumarramanan\/awesome-deep-learning-resources)\n> * [Data Science with R - Awesome Tutorials](https:\/\/www.kaggle.com\/arunkumarramanan\/data-science-with-r-awesome-tutorials)\n> * [Data Science and Machine Learning Cheetcheets](https:\/\/www.kaggle.com\/arunkumarramanan\/data-science-and-machine-learning-cheatsheets)\n> * [Awesome ML Frameworks and MNIST Classification](https:\/\/www.kaggle.com\/arunkumarramanan\/awesome-machine-learning-ml-frameworks)\n> * [Awesome Data Science for Beginners with Titanic Exploration](https:\/\/kaggle.com\/arunkumarramanan\/awesome-data-science-for-beginners)\n> * [Tensorflow Tutorial and House Price Prediction](https:\/\/www.kaggle.com\/arunkumarramanan\/tensorflow-tutorial-and-examples)\n> * [Practical Machine Learning with PyTorch](https:\/\/www.kaggle.com\/arunkumarramanan\/practical-machine-learning-with-pytorch)\n> * [Data Science with Python - Awesome Tutorials](https:\/\/www.kaggle.com\/arunkumarramanan\/data-science-with-python-awesome-tutorials)\n> * [Machine Learning and Deep Learning - Awesome Tutorials](https:\/\/www.kaggle.com\/arunkumarramanan\/awesome-deep-learning-ml-tutorials)\n> * [Machine Learning Engineer's Toolkit with Roadmap](https:\/\/www.kaggle.com\/arunkumarramanan\/machine-learning-engineer-s-toolkit-with-roadmap) \n> * [Awesome TensorFlow and PyTorch Resources](https:\/\/www.kaggle.com\/arunkumarramanan\/awesome-tensorflow-and-pytorch-resources)\n> * [Hands-on ML with scikit-learn and TensorFlow](https:\/\/www.kaggle.com\/arunkumarramanan\/hands-on-ml-with-scikit-learn-and-tensorflow)\n> * [Awesome Data Science IPython Notebooks](https:\/\/www.kaggle.com\/arunkumarramanan\/awesome-data-science-ipython-notebooks)\n> * [Data Scientist's Toolkits - Awesome Data Science Resources](https:\/\/www.kaggle.com\/arunkumarramanan\/data-scientist-s-toolkits-awesome-ds-resources)\n> * [Awesome Computer Vision Resources (TBU)](https:\/\/www.kaggle.com\/arunkumarramanan\/awesome-computer-vision-resources-to-be-updated)\n","5a7695b4":"It turns out, the accuracy on the test dataset is a little less than the accuracy on the training dataset. This gap between training accuracy and test accuracy is an example of *overfitting*. Overfitting is when a machine learning model performs worse on new data than on their training data. ","c24f5489":"### Setup the layers\n\nThe basic building block of a neural network is the *layer*. Layers extract representations from the data fed into them. And, hopefully, these representations are more meaningful for the problem at hand.\n\nMost of deep learning consists of chaining together simple layers. Most layers, like `tf.keras.layers.Dense`, have parameters that are learned during training.","c7ddcd0a":"## Evaluate accuracy\n\nNext, compare how the model performs on the test dataset:","d8d90b0c":"Finally, use the trained model to make a prediction about a single image. ","8febc3b4":"As the model trains, the loss and accuracy metrics are displayed. This model reaches an accuracy of about 0.88 (or 88%) on the training data."}}