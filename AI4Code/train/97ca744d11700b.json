{"cell_type":{"073b19b7":"code","41ce75a8":"code","9611ff89":"code","fbd7845b":"code","e53e77b7":"code","efeac151":"code","5783b765":"code","213d3923":"code","05bdf2c3":"code","774da027":"code","82a54d19":"code","fa23dc17":"code","7ca3c443":"code","558ab365":"code","6d1bf09a":"code","d278d4f2":"code","1cffd8b6":"code","9b9f61af":"code","33d83a04":"code","435b42d7":"code","e5563b82":"code","226f3751":"code","73bc9bfb":"code","a76aedf7":"code","0a1d55aa":"code","f740fc4f":"code","cf8ab2ab":"code","f4d75a6a":"code","14c35107":"code","fc09bf5c":"code","20738300":"code","70f343c9":"code","ea3f7217":"code","55ea2aed":"code","2e4113e8":"code","25d2eb39":"code","7a711fd8":"code","06ff3679":"code","e9d50f99":"code","844d52bd":"code","b84589cb":"code","06c22345":"code","e0faaef8":"code","3cfbb669":"code","4a0c1a39":"code","d6d908c7":"code","c04f07c9":"code","3183abd2":"code","55d54155":"code","d4ffe2dd":"code","704505d7":"code","cf0bb2fa":"code","e1914277":"code","459412bd":"code","89440855":"markdown","46aa15c9":"markdown","4e6c56b5":"markdown","84819466":"markdown","dd79f911":"markdown","16f9a74d":"markdown","0e41718a":"markdown","72f0cdfd":"markdown","b4d3999d":"markdown","5c9b3493":"markdown","76b22828":"markdown","581fc92b":"markdown","53a6aa9b":"markdown","e2e8d81d":"markdown","b5ff56da":"markdown","7655d6e7":"markdown","bbb4d52b":"markdown","55a588e1":"markdown","4c86f0e6":"markdown","9a24cc72":"markdown","254fdef1":"markdown","8fc527b2":"markdown","81b02b2f":"markdown","eaa8cc9c":"markdown","717cf648":"markdown","9588b461":"markdown","f566ab26":"markdown","abd99c2d":"markdown","39357e4f":"markdown","da83fd2d":"markdown","1ac0d86c":"markdown","95c153d5":"markdown","6491705a":"markdown","de069cbf":"markdown","f929d1f0":"markdown","2f9f8f59":"markdown","9472efbe":"markdown","8eeff595":"markdown","391bbd5a":"markdown","f92088fe":"markdown","6bc9c264":"markdown","4f831772":"markdown","b88df844":"markdown","46f6a128":"markdown","6c655256":"markdown","d9862d23":"markdown","9d536716":"markdown","fa8465de":"markdown","b9302441":"markdown","816f7bd1":"markdown","566e3d41":"markdown","4b5c21e0":"markdown","28cac65f":"markdown","11e6190c":"markdown","c5e1f007":"markdown","f6127f26":"markdown","ba09c04f":"markdown","b8a0c6d5":"markdown"},"source":{"073b19b7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Plots\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport plotly.offline as py\nimport plotly.graph_objs as go\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nimport plotly.tools as tls\nimport plotly.figure_factory as ff\npy.init_notebook_mode(connected=True)\nimport squarify\n\n# Data processing, metrics and modeling\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split, GridSearchCV, RandomizedSearchCV\nfrom sklearn.metrics import precision_score, recall_score, confusion_matrix,  roc_curve, precision_recall_curve, accuracy_score, roc_auc_score\nimport lightgbm as lgbm\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import roc_curve,auc\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_predict\nfrom yellowbrick.classifier import DiscriminationThreshold\n\n# Stats\nimport scipy.stats as ss\nfrom scipy import interp\nfrom scipy.stats import randint as sp_randint\nfrom scipy.stats import uniform as sp_uniform\n\n# Time\nfrom contextlib import contextmanager\n@contextmanager\ndef timer(title):\n    t0 = time.time()\n    yield\n    print(\"{} - done in {:.0f}s\".format(title, time.time() - t0))\n\n#ignore warning messages \nimport warnings\nwarnings.filterwarnings('ignore') \n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","41ce75a8":"# Python libraries\n# Classic,data manipulation and linear algebra\nimport pandas as pd\nimport numpy as np\n\n# Plots\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n","9611ff89":"data = pd.read_csv('..\/input\/pima-indians-diabetes-database\/diabetes.csv')","fbd7845b":"data.head()","e53e77b7":"data.info()","efeac151":"# 2 datasets\nD = data[(data['Outcome'] != 0)]\nH = data[(data['Outcome'] == 0)]","5783b765":"missing_values = (data.isnull().sum() \/ len(data) * 100).round(2)\nmissing_values = missing_values[missing_values > 0]\nmissing_values = missing_values[missing_values > 0]\nmissing_values.sort_values(inplace=True)\nmissing_values","213d3923":"data[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']] = data[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']].replace(0,np.NaN)","05bdf2c3":"missing_values = (data.isnull().sum() \/ len(data) * 100).round(2)\nmissing_values = missing_values[missing_values > 0]\nmissing_values.sort_values(inplace=True)\nmissing_values","774da027":"missing_values = missing_values.to_frame()\nmissing_values.columns = ['Percentage Missing']\nmissing_values.index.names = ['Attributes']\nmissing_values['Attributes'] = missing_values.index","82a54d19":"import seaborn as sns\nsns.set(style=\"whitegrid\", color_codes=True)","fa23dc17":"sns.barplot(x = 'Attributes', y = 'Percentage Missing', data=missing_values)\nplt.xticks(rotation = 90)\nplt.show()","7ca3c443":"data.describe()","558ab365":"data.hist(bins=25, figsize=(20, 15));","6d1bf09a":"plt.style.use('ggplot') # Using ggplot2 style visuals \n\nf, ax = plt.subplots(figsize=(10, 20))\n\nax.set_facecolor('#fafafa')\nax.set(xlim=(-1, 500))\nplt.ylabel('Variables')\nplt.title(\"Box Plot of Variables\")\nax = sns.boxplot(data = data, \n  orient = 'h', \n  palette = 'Set2')","d278d4f2":"ax = sns.catplot(x=\"Outcome\", kind=\"count\", data=data).set(title = \"Outcome Distribution\");","1cffd8b6":"corr_matrix = data.corr()","9b9f61af":"corr_matrix[\"Outcome\"].sort_values(ascending=False)","33d83a04":"from pandas.plotting import scatter_matrix\nattributes =  ['Outcome','Glucose', 'BMI', 'Insulin', 'SkinThickness', 'Age',  'Pregnancies', 'DiabetesPedigreeFunction', 'BloodPressure']\n\nscatter_matrix(data[attributes], figsize=(20, 20));","435b42d7":"\nfig, ax = plt.subplots(figsize=(8, 8))\nsns.heatmap(corr_matrix, annot=True);","e5563b82":"def median_target(var):   \n    temp = data[data[var].notnull()]\n    temp = temp[[var, 'Outcome']].groupby(['Outcome'])[[var]].median().reset_index()\n    return temp","226f3751":"def plot_distribution(data_select, size_bin) :  \n    # 2 datasets\n    tmp1 = D[data_select]\n    tmp2 = H[data_select]\n    hist_data = [tmp1, tmp2]\n    \n    group_labels = ['diabetic', 'healthy']\n    colors = ['red', 'lime']\n\n    fig = ff.create_distplot(hist_data, group_labels, colors = colors, show_hist = True, bin_size = size_bin, curve_type='kde')\n    \n    fig['layout'].update(title = data_select)\n\n    py.iplot(fig, filename = 'Density plot')","73bc9bfb":"plot_distribution('Insulin', 0)","a76aedf7":"median_target('Insulin')","0a1d55aa":"data.loc[(data['Outcome'] == 0 ) & (data['Insulin'].isnull()), 'Insulin'] = 102.5\ndata.loc[(data['Outcome'] == 1 ) & (data['Insulin'].isnull()), 'Insulin'] = 169.5","f740fc4f":"plot_distribution('Glucose', 0)","cf8ab2ab":"median_target('Glucose')","f4d75a6a":"data.loc[(data['Outcome'] == 0 ) & (data['Glucose'].isnull()), 'Glucose'] = 107\ndata.loc[(data['Outcome'] == 1 ) & (data['Glucose'].isnull()), 'Glucose'] = 140","14c35107":"plot_distribution('SkinThickness', 10)","fc09bf5c":"median_target('SkinThickness')","20738300":"data.loc[(data['Outcome'] == 0 ) & (data['SkinThickness'].isnull()), 'SkinThickness'] = 27\ndata.loc[(data['Outcome'] == 1 ) & (data['SkinThickness'].isnull()), 'SkinThickness'] = 32","70f343c9":"plot_distribution('BloodPressure', 5)","ea3f7217":"median_target('BloodPressure')","55ea2aed":"data.loc[(data['Outcome'] == 0 ) & (data['BloodPressure'].isnull()), 'BloodPressure'] = 70\ndata.loc[(data['Outcome'] == 1 ) & (data['BloodPressure'].isnull()), 'BloodPressure'] = 74.5","2e4113e8":"plot_distribution('BMI', 0)","25d2eb39":"median_target('BMI')","7a711fd8":"data.loc[(data['Outcome'] == 0 ) & (data['BMI'].isnull()), 'BMI'] = 30.1\ndata.loc[(data['Outcome'] == 1 ) & (data['BMI'].isnull()), 'BMI'] = 34.3","06ff3679":"#plot distribution \nplot_distribution('Age', 0)\nplot_distribution('Pregnancies', 0)\nplot_distribution('DiabetesPedigreeFunction', 0)","e9d50f99":"def set_bmi(x):\n    if x < 18.5:\n        return \"Under Weight\"\n    elif x >= 18.5 and x <= 24.9:\n        return \"Healthy\"\n    elif x >= 25 and x <= 29.9:\n        return \"Over Weight\"\n    elif x >= 30:\n        return \"Obese\"","844d52bd":"data[\"BMI_CAT\"] = data.BMI.apply(set_bmi)\ndata.head()","b84589cb":"def set_insulin(x):\n    # Normal\n    if x >= 16 and x <= 166:\n        return \"Normal\"\n    # Abnormal\n    else:\n        return \"Abnormal\"","06c22345":"data[\"INSULIN_CAT\"] = data.Insulin.apply(set_insulin)\ndata.head()","e0faaef8":"ax = sns.catplot(x=\"INSULIN_CAT\", kind=\"count\", data=data).set(title = \"Insulin Levels Distribution\");","3cfbb669":"ax = sns.catplot(x=\"BMI_CAT\", kind=\"count\", data=data).set(title = \"BMI Categorical Distribution\");","4a0c1a39":"target_col = [\"Outcome\"]\ncat_cols = [\"BMI_CAT\", \"INSULIN_CAT\"]\n\n#numeric columns\nnum_cols = [x for x in data.columns if x not in cat_cols + target_col]\n\n#Binary columns with 2 values\nbin_cols   = data.nunique()[data.nunique() == 2].keys().tolist()\n#Columns more than 2 values\nmulti_cols = [i for i in cat_cols if i not in bin_cols]\n\n#Label encoding Binary columns\nle = LabelEncoder()\nfor i in bin_cols :\n    data[i] = le.fit_transform(data[i])\n    \n#Duplicating columns for multi value columns\ndata = pd.get_dummies(data = data,columns = multi_cols )\n\n#Scaling Numerical columns\nstd = StandardScaler()\nscaled = std.fit_transform(data[num_cols])\nscaled = pd.DataFrame(scaled,columns=num_cols)\n\n#dropping original values merging scaled values for numerical columns\ndf_data_og = data.copy()\ndata = data.drop(columns = num_cols,axis = 1)\ndata = data.merge(scaled,left_index=True,right_index=True,how = \"left\")","d6d908c7":"def correlation_plot():\n    #correlation\n    correlation = data.corr()\n    #tick labels\n    matrix_cols = correlation.columns.tolist()\n    #convert to array\n    corr_array  = np.array(correlation)\n    trace = go.Heatmap(z = corr_array,\n                       x = matrix_cols,\n                       y = matrix_cols,\n                       colorscale='Viridis',\n                       colorbar   = dict() ,\n                      )\n    layout = go.Layout(dict(title = 'Correlation Matrix for variables',\n                            #autosize = False,\n                            #height  = 1400,\n                            #width   = 1600,\n                            margin  = dict(r = 0 ,l = 100,\n                                           t = 0,b = 100,\n                                         ),\n                            yaxis   = dict(tickfont = dict(size = 9)),\n                            xaxis   = dict(tickfont = dict(size = 9)),\n                           )\n                      )\n    fig = go.Figure(data = [trace],layout = layout)\n    py.iplot(fig)","c04f07c9":"correlation_plot()","3183abd2":"# Def X and Y\nX = data.drop('Outcome', 1)\ny = data['Outcome']","55d54155":"def model_performance(model, subtitle) :   \n    #Kfold\n    cv = KFold(n_splits=5,shuffle=False, random_state = 42)\n    y_real = []\n    y_proba = []\n    tprs = []\n    aucs = []\n    mean_fpr = np.linspace(0,1,100)\n    i = 1\n    \n    for train,test in cv.split(X,y):\n        model.fit(X.iloc[train], y.iloc[train])\n        pred_proba = model.predict_proba(X.iloc[test])\n        precision, recall, _ = precision_recall_curve(y.iloc[test], pred_proba[:,1])\n        y_real.append(y.iloc[test])\n        y_proba.append(pred_proba[:,1])\n        fpr, tpr, t = roc_curve(y[test], pred_proba[:, 1])\n        tprs.append(interp(mean_fpr, fpr, tpr))\n        roc_auc = auc(fpr, tpr)\n        aucs.append(roc_auc) \n    \n    # Confusion matrix\n    y_pred = cross_val_predict(model, X, y, cv=5)\n    conf_matrix = confusion_matrix(y, y_pred)\n    trace1 = go.Heatmap(z = conf_matrix  ,x = [\"0 (pred)\",\"1 (pred)\"],\n                        y = [\"0 (true)\",\"1 (true)\"],xgap = 2, ygap = 2, \n                        colorscale = 'Viridis', showscale  = False)\n    \n    #Show metrics\n    tp = conf_matrix[1,1]\n    fn = conf_matrix[1,0]\n    fp = conf_matrix[0,1]\n    tn = conf_matrix[0,0]\n    Accuracy  =  ((tp+tn)\/(tp+tn+fp+fn))\n    Precision =  (tp\/(tp+fp))\n    Recall    =  (tp\/(tp+fn))\n    F1_score  =  (2*(((tp\/(tp+fp))*(tp\/(tp+fn)))\/((tp\/(tp+fp))+(tp\/(tp+fn)))))\n\n    show_metrics = pd.DataFrame(data=[[Accuracy , Precision, Recall, F1_score]])\n    show_metrics = show_metrics.T\n\n    colors = ['gold', 'lightgreen', 'lightcoral', 'lightskyblue']\n    trace2 = go.Bar(x = (show_metrics[0].values), \n                    y = ['Accuracy', 'Precision', 'Recall', 'F1_score'], text = np.round_(show_metrics[0].values,4),\n                    textposition = 'auto', textfont=dict(color='black'),\n                    orientation = 'h', opacity = 1, marker=dict(\n            color=colors,\n            line=dict(color='#000000',width=1.5)))\n\n    #Roc curve\n    mean_tpr = np.mean(tprs, axis=0)\n    mean_auc = auc(mean_fpr, mean_tpr)\n\n    trace3 = go.Scatter(x=mean_fpr, y=mean_tpr,\n                        name = \"Roc : \" ,\n                        line = dict(color = ('rgb(22, 96, 167)'),width = 2), fill='tozeroy')\n    trace4 = go.Scatter(x = [0,1],y = [0,1],\n                        line = dict(color = ('black'),width = 1.5,\n                        dash = 'dot'))\n    \n    #Precision - recall curve\n    y_real = y\n    y_proba = np.concatenate(y_proba)\n    precision, recall, _ = precision_recall_curve(y_real, y_proba)\n\n    trace5 = go.Scatter(x = recall, y = precision,\n                        name = \"Precision\" + str(precision),\n                        line = dict(color = ('lightcoral'),width = 2), fill='tozeroy')\n    \n    mean_auc=round(mean_auc,3)\n    #Subplots\n    fig = tls.make_subplots(rows=2, cols=2, print_grid=False,\n                          specs=[[{}, {}], \n                                 [{}, {}]],\n                          subplot_titles=('Confusion Matrix',\n                                          'Metrics',\n                                          'ROC curve'+\" \"+ '('+ str(mean_auc)+')',\n                                          'Precision - Recall curve',\n                                          ))\n    #Trace and layout\n    fig.append_trace(trace1,1,1)\n    fig.append_trace(trace2,1,2)\n    fig.append_trace(trace3,2,1)\n    fig.append_trace(trace4,2,1)\n    fig.append_trace(trace5,2,2)\n    \n    fig['layout'].update(showlegend = False, title = '<b>Model performance report (5 folds)<\/b><br>'+subtitle,\n                        autosize = False, height = 830, width = 830,\n                        plot_bgcolor = 'black',\n                        paper_bgcolor = 'black',\n                        margin = dict(b = 195), font=dict(color='white'))\n    fig[\"layout\"][\"xaxis1\"].update(color = 'white')\n    fig[\"layout\"][\"yaxis1\"].update(color = 'white')\n    fig[\"layout\"][\"xaxis2\"].update((dict(range=[0, 1], color = 'white')))\n    fig[\"layout\"][\"yaxis2\"].update(color = 'white')\n    fig[\"layout\"][\"xaxis3\"].update(dict(title = \"false positive rate\"), color = 'white')\n    fig[\"layout\"][\"yaxis3\"].update(dict(title = \"true positive rate\"),color = 'white')\n    fig[\"layout\"][\"xaxis4\"].update(dict(title = \"recall\"), range = [0,1.05],color = 'white')\n    fig[\"layout\"][\"yaxis4\"].update(dict(title = \"precision\"), range = [0,1.05],color = 'white')\n    for i in fig['layout']['annotations']:\n        i['font'] = titlefont=dict(color='white', size = 14)\n    py.iplot(fig)","d4ffe2dd":"def scores_table(model, subtitle):\n    scores = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n    res = []\n    for sc in scores:\n        scores = cross_val_score(model, X, y, cv = 5, scoring = sc)\n        res.append(scores)\n    df = pd.DataFrame(res).T\n    df.loc['mean'] = df.mean()\n    df.loc['std'] = df.std()\n    df= df.rename(columns={0: 'accuracy', 1:'precision', 2:'recall',3:'f1',4:'roc_auc'})\n\n    trace = go.Table(\n        header=dict(values=['<b>Fold', '<b>Accuracy', '<b>Precision', '<b>Recall', '<b>F1 score', '<b>Roc auc'],\n                    line = dict(color='#7D7F80'),\n                    fill = dict(color='#a1c3d1'),\n                    align = ['center'],\n                    font = dict(size = 15)),\n        cells=dict(values=[('1','2','3','4','5','mean', 'std'),\n                           np.round(df['accuracy'],3),\n                           np.round(df['precision'],3),\n                           np.round(df['recall'],3),\n                           np.round(df['f1'],3),\n                           np.round(df['roc_auc'],3)],\n                   line = dict(color='#7D7F80'),\n                   fill = dict(color='#EDFAFF'),\n                   align = ['center'], font = dict(size = 15)))\n\n    layout = dict(width=800, height=400, title = '<b>Cross Validation - 5 folds<\/b><br>'+subtitle, font = dict(size = 15))\n    fig = dict(data=[trace], layout=layout)\n\n    py.iplot(fig, filename = 'styled_table')","704505d7":"# Load libraries\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier","cf0bb2fa":"X_train, X_test, y_train, y_test =train_test_split(X, y, test_size=0.25, random_state=42, stratify=data['Outcome'])","e1914277":"def GetBasedModel():\n    basedModels = []\n    basedModels.append(('LR'   , LogisticRegression()))\n    basedModels.append(('LDA'  , LinearDiscriminantAnalysis()))\n    basedModels.append(('KNN'  , KNeighborsClassifier()))\n    basedModels.append(('CART' , DecisionTreeClassifier()))\n    basedModels.append(('NB'   , GaussianNB()))\n    basedModels.append(('SVM'  , SVC(probability=True)))\n    basedModels.append(('AB'   , AdaBoostClassifier()))\n    basedModels.append(('GBM'  , GradientBoostingClassifier()))\n    basedModels.append(('RF'   , RandomForestClassifier()))\n    basedModels.append(('ET'   , ExtraTreesClassifier()))\n\n    \n    return basedModels","459412bd":"models = GetBasedModel()\n\nfor name, model in models:\n    clf = model.fit(X, y)\n    model_performance(clf, name)\n    scores_table(clf, name)\n    \n    \n","89440855":"## <a id='7.1'> 7.1 Standard Scaler and Label Encoder <\/a>","46aa15c9":"Let's calculate and plot the Percentage of data missing in each attribute","4e6c56b5":"* **Glucose** : Plasma glucose concentration a 2 hours in an oral glucose tolerance test","84819466":"We can create a correlation matrix with new features","dd79f911":"## <a id='5.1.e'>5.1.e BMI<\/a> ","16f9a74d":"### Correlation Heat Map","0e41718a":"## <a id='8.1'>8.1 Train Test Split<\/a> ","72f0cdfd":"It looks like there's no data is missing, Hurray!\nIs it?\nWe saw on data.head() that some features like BloodPressure contain 0, it doesn't make sense here and this indicates missing values.\nBelow we replace 0 value by NaN.","b4d3999d":"### What is DiabetesPedigreeFunction Column signifying?\nA function which scores the likelihood of diabetes based on family history. It provided some data on diabetes mellitus history in relatives and the genetic relationship of those relatives to the patient.","5c9b3493":"## <a id='2.1'>2.1 Head and  Attributes<\/a>","76b22828":"## <a id='5.1'> 5.1 Handling Missing Data <\/a>","581fc92b":"A **correlation matrix** is a table showing correlation coefficients between sets of variables. Each random variable (Xi) in the table is correlated with each of the other values in the table (Xj). This allows you to see which pairs have the highest correlation.","53a6aa9b":"# <a id='2'>2. Overview of each Attribute and its Characteristics<\/a>","e2e8d81d":"* **BMI** : Body mass index (weight in kg\/(height in m)^2)","b5ff56da":"The dataset has 768 entries and 9 colums(8 Predictor variables and 1 Target variable). Only BMI and DaiabetesPedigreeFunction are floating numbers, rest are integers.","7655d6e7":" ## <a id='2.5'> 2.5 Data Visualization<\/a>","bbb4d52b":"We need to add important features to the dataset discover some effective features before training the data to machine learning models.\n\n**Feature 1 : BMI Descriptor** Adding BMI Descriptor feature as we know all the participants were adult female.\nThe features are encoded as below\n**BMI**\n* Under - Under 18.5 \u2013 are considered underweight and possibly malnourished.\n* Healthy - 18.5 to 24.9 \u2013 are within a healthy weight range for young and middle-aged adults.\n* OverWeight - 25.0 to 29.9 \u2013 are considered overweight.\n* Obese - Over 30 \u2013 are considered obese.","55a588e1":"This dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage.\n\n","4c86f0e6":"### What are these columns signifying?\n\nThis datasets consist of eight medical predictor (independent) variables and one target (dependent) variable, Outcome Class variable whether paitent has diabetes or not (encoded as 0 or 1). Predictor variables are  includes the number of pregnancies the patient has had, their BMI, insulin level, age, and so on.\n* Pregnancies - Number of times Pregnant. \n* Glucose - Plasma glucose concentration a 2 hours in an oral glucose tolerance test. \n* BloodPressure - Diastolic blood pressure (mm Hg).\n* SkinThickness - Triceps skin fold thickness (mm).\n* Insulin - 2-Hour serum insulin (mu U\/ml).\n* BMI - Body mass index ( BMI weight in kg\/(height in m)^2).\n* DiabetesPedigreeFunction - Diabetes pedigree function.\n* Age - Age in year.\n* Outcome - 1 if Patient has diabetes, 0 otherwise.","9a24cc72":"* **Age** : Age (years)\n* **DiabetesPedigreeFunction** : Diabetes pedigree function\n* **Pregnancies** : Number of times pregnant","254fdef1":"## <a id='2.3'>2.3 Missing Values<\/a>","8fc527b2":"All features are filled the missing values. Now, we can create new features.","81b02b2f":"Attributes with Missing Values and percentage missing\n\n- Glucose           0.65%\n- BMI               1.43%\n- BloodPressure     4.56%\n- SkinThickness    29.56%\n- Insulin          48.70%","eaa8cc9c":"# <a id='8'>8. Machine Learning<\/a> ","717cf648":"What's target's distribution ? ","9588b461":"# <a id='6'> 6. Feature Engineering & Selection <\/a>","f566ab26":"# <a id='3'> 3. Target<\/a> ","abd99c2d":"# <a id='7'> 7. Data Preparation <\/a>","39357e4f":"To measure the performance of a model, we need several elements :\n\nThis part is essential\n\n* **Confusion matrix** : also known as the error matrix, allows visualization of the performance of an algorithm :\n\n    * true positive (TP) : Diabetic correctly identified as diabetic\n    * true negative (TN) : Healthy correctly identified as healthy\n    * false positive (FP) : Healthy incorrectly identified as diabetic\n    * false negative (FN) : Diabetic incorrectly identified as healthy\n\n![](https:\/\/miro.medium.com\/max\/2102\/1*fxiTNIgOyvAombPJx5KGeA.png)\n\n* **Metrics ** :\n\n    * Accuracy : (TP +TN) \/ (TP + TN + FP +FN)\n    * Precision : TP \/ (TP + FP)\n    * Recall : TP \/ (TP + FN)\n    * F1 score : 2 x ((Precision x Recall) \/ (Precision + Recall))\n\n* **Roc Curve** : The ROC curve is created by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings.\n\n![](https:\/\/miro.medium.com\/max\/722\/1*pk05QGzoWhCgRiiFbz-oKQ.png)\n\n* **Precision Recall Curve** :  shows the tradeoff between precision and recall for different threshold\n![](https:\/\/machinelearningblogcom.files.wordpress.com\/2018\/04\/bildschirmfoto-2018-04-03-um-11-31-16.png)","da83fd2d":"107 for a healthy person and 140 for a diabetic person","1ac0d86c":"### <a id='5.1.b'> 5.1.b Glucose <\/a>","95c153d5":"### <a id='5.1.a'> 5.1.a Insulin <\/a>","6491705a":"We can complete model performance report with a table contain all results by fold","de069cbf":"## <a id='7.4'>7.4. Model Performance<\/a> ","f929d1f0":"## <a id='1.2'>1.2 Read Data <\/a>","2f9f8f59":"## <a id='7.5'>7.5. Scores Tables<\/a> ","9472efbe":" ## <a id='2.4'> 2.4 Identify Noise in the Data and Type of Noise<\/a>","8eeff595":"Insulin's medians by the target are really different ! 102.5 for a healthy person and 169.5 for a diabetic person","391bbd5a":"From the above information we can make the following obserevations for the attributes \n\n- The max Pregnacies is 17 which is quite an outlier.\n- The Glucose data is normally distributed.\n- Blood Pressure is also normally distributed with a considerable number of outliers but still is not skewed.\n- Skin Thickness is positvely skewed with some outliers.\n- Insulin also has considerable number of outliers with abnormal values around 800 and is positively skewed.\n- BMI also has few outliers and is positively skewed.\n- DiabetesPedigreeFunction is also highly positively skewed with few outliers.\n- Age data has very few outliers.  \n","f92088fe":"# Who are Pima Indians ?\n\n\"The Pima (or Akimel O'odham, also spelled Akimel O'otham, \"River People\", formerly known as Pima) are a group of Native Americans living in an area consisting of what is now central and southern Arizona. The majority population of the surviving two bands of the Akimel O'odham are based in two reservations: the Keli Akimel O'otham on the Gila River Indian Community (GRIC) and the On'k Akimel O'odham on the Salt River Pima-Maricopa Indian Community (SRPMIC).\" Wikipedia\n\n![](https:\/\/cdn.britannica.com\/42\/93542-050-E2B32DAB\/women-Pima-shinny-game-field-hockey.jpg)","6bc9c264":"Since all the data is in the form of integers and floating points we can draw histograms to see the data distribution ","4f831772":"## <a id='2.2'>2.2 Size and Type of Data<\/a>","b88df844":"# <a id='4'> 4. Correlations between the attributes<\/a>","46f6a128":"* ** SkinThickness** : Triceps skin fold thickness (mm)","6c655256":"* **Insulin** : 2-Hour serum insulin (mu U\/ml)","d9862d23":"We define X and y:","9d536716":"## <a id='1.1'>1.1. Load libraries Pandas, Numpy and Matplotlib<\/a> ","fa8465de":"Many of the attributes have significant outliers so we shall replace the missing values with the median by target, instead of the mean.","b9302441":"### <a id='5.1.c'> 5.1.c SkinThickness<\/a>","816f7bd1":"**Feature 2**:\n* Insulin Indicative Range If insulin level (2-Hour serum insulin (mu U\/ml)) is >= 16 and <= 166, then it is **normal** range else it is considered as **Abnormal**.","566e3d41":"## What is diabetes ?\nAcccording to NIH, \"Diabetes is a disease that occurs when your blood glucose, also called blood sugar, is too high. Blood glucose is your main source of energy and comes from the food you eat. Insulin, a hormone made by the pancreas, helps glucose from food get into your cells to be used for energy. Sometimes your body doesn\u2019t make enough\u2014or any\u2014insulin or doesn\u2019t use insulin well. Glucose then stays in your blood and doesn\u2019t reach your cells.\n\nOver time, having too much glucose in your blood can cause health problems. Although diabetes has no cure, you can take steps to manage your diabetes and stay healthy.\n\nSometimes people call diabetes \u201ca touch of sugar\u201d or \u201cborderline diabetes.\u201d These terms suggest that someone doesn\u2019t really have diabetes or has a less serious case, but every case of diabetes is serious.\n\n**What are the different types of diabetes?** The most common types of diabetes are type 1, type 2, and gestational diabetes.\n\n**Type 1 diabetes** If you have type 1 diabetes, your body does not make insulin. Your immune system attacks and destroys the cells in your pancreas that make insulin. Type 1 diabetes is usually diagnosed in children and young adults, although it can appear at any age. People with type 1 diabetes need to take insulin every day to stay alive.\n\n**Type 2 diabetes** If you have type 2 diabetes, your body does not make or use insulin well. You can develop type 2 diabetes at any age, even during childhood. However, this type of diabetes occurs most often in middle-aged and older people. Type 2 is the most common type of diabetes.\n\n**Gestational diabetes** Gestational diabetes develops in some women when they are pregnant. Most of the time, this type of diabetes goes away after the baby is born. However, if you\u2019ve had gestational diabetes, you have a greater chance of developing type 2 diabetes later in life. Sometimes diabetes diagnosed during pregnancy is actually type 2 diabetes.\n\n**Other types of diabetes** Less common types include monogenic diabetes, which is an inherited form of diabetes, and cystic fibrosis-related diabetes .\"","4b5c21e0":"# <a id='5'> 5. Data Cleaning & EDA <\/a>","28cac65f":"27 for a healthy person and 32 for a diabetic person","11e6190c":"## <a id='7.3'>7.3. X and y<\/a> ","c5e1f007":"## <a id='7.2'> 7.2 Correlation Matrix <\/a>","f6127f26":"* **StandardScaler** :\n\nStandardize features by removing the mean and scaling to unit variance : \n\n![](https:\/\/i.stack.imgur.com\/PZgJ2.png)\n\nCentering and scaling happen independently on each feature by computing the relevant statistics on the samples in the set. Mean and standard deviation are then stored to be used on later data using the transform method.\n\nStandardization of a dataset is a common requirement for many machine learning estimators: they might behave badly if the individual features do not more or less look like standard normally distributed data (e.g. Gaussian with 0 mean and unit variance).\n\n* **LabelEncoder** : Encode labels with value between 0 and n_classes-1.\n\nBellow we encode the data to feed properly to our algorithm","ba09c04f":"# <a id='1'>1. Load Libraries and read the data<a>","b8a0c6d5":"### <a id='5.1.d'>5.1.d BloodPressure<\/a> "}}