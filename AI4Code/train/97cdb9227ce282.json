{"cell_type":{"d1fc6a16":"code","7331203c":"code","dbf71cfd":"code","7b1d1124":"code","faeb19c0":"code","3500a108":"code","967c41d0":"code","386f857e":"code","a03abb51":"code","b4e9d6e0":"code","460acadf":"code","643c9357":"code","9735b44f":"code","0b25fd68":"code","55ea6542":"code","264f24e3":"code","a7d3003f":"code","1ca7ad8b":"code","bfaf2b82":"markdown","42981b4a":"markdown","368ebd27":"markdown","4232f70d":"markdown","1b03f0b6":"markdown","82c11618":"markdown","8c219549":"markdown","884f390a":"markdown","e3aab9dd":"markdown","bf802e4c":"markdown","5bfeda51":"markdown"},"source":{"d1fc6a16":"# Import des libraries classique (numpy, pandas, ...)\nimport pandas as pd\nimport numpy as np\nimport re\nimport sklearn as sk\nfrom matplotlib import pyplot as plt\nplt.style.use('ggplot')","7331203c":"!pip install seaborn --upgrade","dbf71cfd":"import seaborn as sb\n\nprint(sb.__version__)","7b1d1124":"# Import du dataframe \"data.csv\"\ndf = pd.read_csv(\"..\/input\/prediction-of-data-scientist-profiles\/data.csv\")\ndf_or = df.copy()\ndf.head(10)","faeb19c0":"print(\"Total number of observations: {}  \\n\".format(df.shape[0]))\nprint(\"The sum of missing values by variables:  \\n\")\nprint(df.isna().sum())","3500a108":"# remplacer les ',' par des '.' pour bien convertir les chiffres apr\u00e8s la virgule dans la variable \"Experience\"\ndf['Experience'] = df.Experience.str.replace(',', '.').astype(float)\n\n# remplire les valeurs manquantes dans la variable \"Experience\" par la valeur m\u00e9diane pour les data scientists\ndf_DS =  df.loc[df['Metier']=='Data scientist','Experience']\ndf_DS = df_DS.fillna(df_DS.median())\ndf.loc[df['Metier']=='Data scientist','Experience'] = df_DS.copy() \nprint('Missing values in Experience for data scientists', df.loc[df['Metier']=='Data scientist','Experience'].isnull().sum())\n\n# remplire les valeurs manquantes dans la variable \"Experience\" par la valeur m\u00e9diane pour les data engineers\ndf_DS =  df.loc[df['Metier']=='Data scientist','Experience']\ndf_DS = df_DS.fillna(df_DS.median())\ndf.loc[df['Metier']=='Data scientist','Experience'] = df_DS.copy() \nprint('Missing values in Experience for data engineers', df.loc[df['Metier']=='Data scientist','Experience'].isnull().sum())\n","967c41d0":"df_mean =  df[df['Metier'].isin(['Data scientist','Data engineer','Lead data scientist'])]\ndf_mean.groupby('Metier')['Experience'].mean()","386f857e":"\ndf_Ex = df.groupby('Metier')['Experience'].mean()\ndf_mean = [df_Ex.get(k) for k in  df['Metier'].value_counts().keys()] \n#df_Ex = df.groupby('Metier')['Experience'].mean()\nx=-0.6; y=0.25\nax = pd.Series(df_mean, index= df['Metier'].value_counts().keys()).plot(kind='barh')\ntotals =[] \nfor p in ax.patches:\n    totals.append(p.get_width())\n    total = np.sum(totals)    \nfor ix,i in enumerate(ax.patches, 0):\n    ax.text(i.get_width()+x, i.get_y()+y, str(round(df_mean[ix],3)), weight='bold', color='black')\nplt.xlabel(\"Ann\u00e9e d'experiences en moyenne\")\nplt.show()","a03abb51":"# Plot histogramme de variable Exp\u00e9rience pour d\u00e9terminer sa distribution et choisi la bonne m\u00e9thode de d\u00e9coupage\nplt.figure(figsize=(10,8))\nsb.histplot(df,x='Experience', kde=True, ) \nplt.show()","b4e9d6e0":"labels = ['d\u00e9butant', 'confirm\u00e9', 'avanc\u00e9', 'expert']\nExp_level = pd.cut(df['Experience'], bins=4, labels= labels)\ndf[\"Exp_level\"] = pd.Series(Exp_level, index=df.index)\n# Affichage des r\u00e9sultats\nax = pd.Series(df['Exp_level'].value_counts().values, index=df['Exp_level'].value_counts().keys()).plot.barh(figsize=(10,7))\nplt.xlabel(\"# de profile\")\nplt.subplots_adjust(wspace=0.3, hspace=0.2)\nax.invert_yaxis()","460acadf":"#Split la variable technologies pour voir chaque technologie\ndf_Tq = pd.DataFrame(df.Technologies.str.split('\/').tolist()).stack()\ndf_Tq  = pd.DataFrame(df_Tq)\ndf_Tq.columns = ['Technologies']\n\nplt.figure(figsize=(15,12))\ndf_Tq['Technologies'].value_counts().plot(kind='bar')\nplt.show()\n","643c9357":"print('Les 5 technologies les plus utilis\u00e9es sont:')\ndf_Tq['Technologies'].value_counts()[0:5]\n    ","9735b44f":"#step1: Transformation les variables cat\u00e9gorielles\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ndf_enc = df.loc[:,['Experience']]\n#df_enc['Ville_enc'] = pd.Series(le.fit_transform(np.squeeze(df.loc[:,['Ville']])[:]), index = df_enc.index)\ndf_enc['Technologies_enc'] = pd.Series(le.fit_transform(np.squeeze(df.loc[:,['Technologies']])[:]), index = df_enc.index)\ndf_enc['Diplome_enc'] = pd.Series(le.fit_transform(np.squeeze(df.loc[:,['Diplome']])[:]), index = df_enc.index)\ndf_enc = np.round(df_enc,2)\n\nprint(df_enc.head(10))\nprint(df_enc.info())","0b25fd68":"from sklearn.cluster import KMeans\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn import metrics\n\n#remplire les valeurs qui sont encore encore manquantes dans la variable \"Experience\" \nmean_da = df_enc['Experience'].dropna().mean()\ndf_enc['Experience'] = df_enc['Experience'].fillna(mean_da)\n\n\nX = df_enc.astype(float)\nscaler = MinMaxScaler()\nX_scaled = scaler.fit_transform(X)\n\nkmeans = KMeans(n_clusters=2, n_init=100 , max_iter=5000,tol=1e-8,random_state=0).fit(X_scaled)\nlabels_ = kmeans.labels_\n#Ajout des clusters identifi\u00e9s aux donn\u00e9es d'origine\ndf_enc['Cluster'] = pd.Series(labels_)\ndf_plot = df.copy()\ndf_plot['Cluster'] = pd.Series(labels_)\n# Evaluation de l'erreur de clustering par: Indicateur de compacit\u00e9 des classes(la dispersion \u00e0 l\u2019int\u00e9rieur de chaque groupe).\nc1, c2 = kmeans.cluster_centers_\nDist = lambda i,j: 100 * ((i-j)**2).sum() \/ ((i)**2).sum()\nquad_dist1 = Dist(X_scaled[labels_==0], c1) \nquad_dist2 = Dist(X_scaled[labels_==1], c2) \nprint('La m\u00e9trique utilis\u00e9e est indicateur de compacit\u00e9 des classes pour chaque cluster: {0:2.2f}% et {1:2.2f} %'.format(quad_dist1, quad_dist2))\nprint(\"les erreurs de clustering obtenus sont < 20% \")\n# Pour l\u2019\u00e9valuation intrins\u00e8que, je choisis le coefficient de silhouette :\nprint(\"Silhouette Coefficient: %0.3f\" % metrics.silhouette_score(X, labels_))\nprint(\"On remarque Coefficient Silhouette ~1 et <<-1\")\n\nprint(\"On remarque aussi que deux technologies revient pus souvent dans le cluster 1 (resp. cluster 2) qui sont : \")\nprint(df.loc[labels_==0,'Technologies'].value_counts()[0:2])\nprint(df.loc[labels_==1,'Technologies'].value_counts()[0:2])\n\nprint(\"Les caracteristiques imortantes des deux clusters sont le 'Diplome':\\\n  \\n  'Master' & 'Bachelor' pour le deuxieme cluster et 'No diplome' & 'PhD' pour le  premier cluster.\")\n# Affichage des r\u00e9sultats\nprint(\"\\n Affichage des caract\u00e9ristiques du cahque cluster : \\n\")      \na = ['Experience', 'Technologies',  'Diplome']# ,'Ville']\nsb.set_theme(style=\"ticks\", color_codes=True)\n\nfor n,i in enumerate(a,1):\n    plt.figure(figsize=(20,30))\n    plt.subplot(521),\n    ax = df_plot.loc[labels_==0,i].value_counts().plot(kind='bar', color = '#5c3a97ff', label='cluster_2')\n    plt.legend()\n    plt.xlabel(i)\n    plt.subplot(522),\n    ax = df_plot.loc[labels_==1,i].value_counts().plot(kind='bar', color = '#fdb861ff', label='cluster_1')\n    plt.ylabel(\"# de profile\")\n    plt.xlabel(i)\n    plt.legend()\nplt.show()\n    ","55ea6542":"from sklearn.linear_model import LogisticRegressionCV\nfrom sklearn import model_selection\nfrom sklearn.metrics import classification_report, accuracy_score, confusion_matrix\nle = LabelEncoder()\nmask = df.loc[:,'Metier'].isna() \nX = np.array(df_enc[~mask])\ny_m = df.loc[~mask,'Metier']\ny = le.fit_transform(df.loc[~mask,'Metier'].dropna())\nMetier_test = np.array(df_enc[mask])\ny_lab=[]\nfor i in [0,1,2,3]:\n    y_lab.append(y_m[y==i].unique()[0])\n\nX_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=.25, shuffle=True, random_state=40)\nx_ax = range(len(y_test))\n\n#LogisticRegressionCV\nclf = LogisticRegressionCV(cv=5, random_state=0, max_iter=400, multi_class='multinomial').fit(X_train, y_train)\n\ny_predicted = clf.predict(X_test)\nprint('Precision de classification par LogisticRegression : '+str(np.round(accuracy_score(y_test, y_predicted),2)*100)+'%   \\n')\nprint(classification_report(y_test, y_predicted)) \ncm = confusion_matrix(y_test, y_predicted)\nprint('Matrice de confusion\\n')\nprint(cm)\n\nsb.set_theme(style=\"ticks\", color_codes=True)\n\nlabels = pd.Series(y_predicted)\nmetier_pred = [y_m[y==i].unique()[0] for i in labels]\nmetier_pred = pd.Series(metier_pred)\ndata = pd.DataFrame({'index': x_ax, 'original': y_test, 'predicted': labels,\n                     'Metie_pred':labels, 'Metie_original': y_test })  \ncleanup_nums = {\"Metie_pred\": {i : y_lab[i]+'_pred' for i in range(len(y_lab))},\n               \"Metie_original\": {i : y_lab[i]+'_original' for i in range(len(y_lab))}\n               }\ndata.replace(cleanup_nums, inplace=True)\nplt.figure(figsize=(15,10))\ng =sb.scatterplot(y=\"Metie_pred\", x=\"index\",   data=data, hue=\"Metie_original\")\n#plt.yticks([0,1,2,3], y_lab)\nplt.title(\"Classification par LogisticRegression\\n\\n\")\nplt.legend()\nplt.show()\n\n\n# training a DescisionTreeClassifier \nfrom sklearn.tree import DecisionTreeClassifier \ndtree_model = DecisionTreeClassifier(max_depth = 4).fit(X_train, y_train) \ndtree_predictions = dtree_model.predict(X_test) \ncm = confusion_matrix(y_test, dtree_predictions) \nprint('Precision de classification DescisionTreeClassifier: '+str(np.round(accuracy_score(y_test, dtree_predictions),2)*100)+'%   \\n')\nprint(classification_report(y_test, dtree_predictions))\nprint('Matrice de confusion\\n')\nprint(cm)\n\nlabels = pd.Series(dtree_predictions)\nmetier_pred = [y_m[y==i].unique()[0] for i in labels]\nmetier_pred = pd.Series(metier_pred)\ndata = pd.DataFrame({'index': x_ax, 'original': y_test , 'predicted': labels,\n                     'Metie_pred':labels, 'Metie_original':y_test })  \ncleanup_nums = {\"Metie_pred\": {i : y_lab[i]+'_pred' for i in range(len(y_lab))},\n               \"Metie_original\": {i : y_lab[i]+'_original' for i in range(len(y_lab))}\n               }\ndata.replace(cleanup_nums, inplace=True)\nplt.figure(figsize=(15,10))\ng =sb.scatterplot(y=\"Metie_pred\", x=\"index\",   data=data, hue=\"Metie_original\")\nplt.title(\"Classification par DescisionTreeClassifier\\n\\n\")\nplt.legend()\nplt.show()\n","264f24e3":"def report_best_scores(results, n_top=3):\n    for i in range(1, n_top + 1):\n        candidates = np.flatnonzero(results['rank_test_score'] == i)\n        for candidate in candidates:\n            print(\"Model with rank: {0}\".format(i))\n            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n                  results['mean_test_score'][candidate],\n                  results['std_test_score'][candidate]))\n            print(\"Parameters: {0}\".format(results['params'][candidate]))\n            print(\"\")    \n            \nimport xgboost as xgb\n\nfrom sklearn.metrics import auc, accuracy_score, confusion_matrix, mean_squared_error\n\n\nclf_xgb = xgb.XGBClassifier(objective=\"multi:softprob\", random_state=42)\n\nclf_xgb.fit(X_train, y_train)\n\ny_predicted = clf_xgb.predict(X_test)\n\nprint('Precision de classification par Gradient boosting est de: '+str(np.round(accuracy_score(y_test, y_predicted),2)*100)+'%   \\n')\nprint(classification_report(y_test, y_predicted))\nprint('Matrice de confusion\\n')\nprint(confusion_matrix(y_test, y_predicted))\n\nlabels = pd.Series(y_predicted)\nmetier_pred = [y_m[y==i].unique()[0] for i in labels]\nmetier_pred = pd.Series(metier_pred)\ndata = pd.DataFrame({'index': x_ax, 'original': y_test , 'predicted': labels,\n                     'Metier_pred':labels, 'Metier_original': y_test })  \ncleanup_nums = {\"Metier_pred\": {i : y_lab[i]+'_pred' for i in range(len(y_lab))},\n               \"Metier_original\": {i : y_lab[i]+'_original' for i in range(len(y_lab))}\n               }\ndata.replace(cleanup_nums, inplace=True)\nplt.figure(figsize=(15,10))\nsb.set_theme(style=\"ticks\", color_codes=True)\ng =sb.scatterplot(y=\"Metier_pred\", x=\"index\",   data=data, hue=\"Metier_original\")\nplt.title(\"Classification par XGBClassifier\\n\\n\")\nplt.legend()\nplt.show()\n","a7d3003f":"\nmetier_predicted= clf_xgb.predict(Metier_test)\nmetier_predicted\nx_ax = range(len(metier_predicted))\nlabels = pd.Series(metier_predicted)\nmetier_pred = [y_m[y==i].unique()[0] for i in labels]\nmetier_pred = pd.Series(metier_pred)\ndata = pd.DataFrame({'index': x_ax, 'metier_manquants': labels,\n                     'Metie_labels': labels })  \ncleanup_nums = {\"metier_manquants\": {i : y_lab[i]+'_pred' for i in range(len(y_lab))}\n               }\ndata.replace(cleanup_nums, inplace=True)\nplt.figure(figsize=(15,10))\ng =sb.scatterplot(y=\"metier_manquants\", x=\"index\",   data=data, hue='metier_manquants')\nplt.title(\"Pr\u00e9diction des m\u00e9tiers manquants par XGBClassifier \\n\\n\")\nplt.legend()\nplt.show()","1ca7ad8b":"df.Metier.isnull().sum()\ndata.shape\ndata","bfaf2b82":"8. **Realize the prediction of missing trades in the database by the following algorithms:**","42981b4a":"4. **Graphic representation of the average number of years of experience for each profession**","368ebd27":"5. **Transform the continuous variable 'Experience' into a new categorical variable 'Exp_label' with 4 modalities: beginner, confirmed, advanced and expert.**","4232f70d":"6. **The 5 most used technologies**","1b03f0b6":"1. **Import the data table into a dataframe**","82c11618":"9. The prediction of missing trades","8c219549":"* Gradient boosting","884f390a":"7. **Une m\u00e9thode de clustering non supervis\u00e9e pour faire appara\u00eetre 2 clusters que vous jugerez pertinents.**","e3aab9dd":"2. **Perform missing value imputation for the variable \"Experience\" with :**\n\n\n- The median value for the data scientists\n- The average value for data engineers","bf802e4c":"# Data science project\n## Objectives:\n\nFrom a dataset, we will realize :\n\n- An unsupervised clustering in order to identify 2 groups of distinct technical profiles.\n- A prediction of profiles whose job is not labeled.\n\n","5bfeda51":"3. **Years of experience have, on average, each of the profiles: the data scientist, the lead data scientist and the data engineer on average.**"}}