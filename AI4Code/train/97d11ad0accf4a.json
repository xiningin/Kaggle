{"cell_type":{"5048c60a":"code","2c689e35":"code","3b6fb00a":"code","ef1eaece":"code","ccc965c2":"code","e8c7e681":"code","2f971c77":"code","817cb4bb":"code","1af71cab":"code","038b3f49":"code","3a6a65ab":"code","93659106":"code","c3d3caaf":"code","aaa0de2e":"code","13fda78d":"code","c5eade76":"code","fed6a202":"code","9b321c75":"code","cf051b77":"code","2b7fd17f":"code","3f85b829":"code","ce04caf2":"code","0c9d5cfa":"code","131a16fa":"code","1d06df33":"code","c4574f25":"code","d5e2fecc":"code","c83f3aee":"code","9df11778":"code","fb5c59eb":"code","3073cd37":"code","f597dfcb":"code","cde2704d":"code","df5d30fd":"code","5789a813":"code","0b0341c5":"code","f000c0cc":"code","af7fdfcf":"code","9a1f36d1":"code","0503914a":"code","2f242807":"code","dd0b0d4f":"code","67ce03bb":"code","0b08f9af":"code","0e7e1afd":"code","853c93d8":"code","4f603bb0":"code","a70e74f1":"markdown","9404073e":"markdown","e66a74ab":"markdown","00fb572a":"markdown","6a658fd2":"markdown","0cd3286a":"markdown","3c21eb50":"markdown","923f79f0":"markdown","0b1b0689":"markdown","52c464af":"markdown","5f08abbe":"markdown","9da2fb9d":"markdown","4670f283":"markdown","f309677b":"markdown","a537a9fb":"markdown","53aec737":"markdown","afb59388":"markdown","8d264113":"markdown","2066ff4e":"markdown","9fe3e8cc":"markdown","14ec1b5a":"markdown","3837a4c5":"markdown","c094ff8f":"markdown","96e4adb7":"markdown","574e473c":"markdown","786682d0":"markdown","afc86480":"markdown","46ce3d40":"markdown","92c781a5":"markdown","f994a3f5":"markdown","08aa8ac4":"markdown","d840476e":"markdown","19245a4f":"markdown","539bb738":"markdown","b5c8cf46":"markdown","725325dc":"markdown","14585029":"markdown","f6ecff54":"markdown","cb8d2c17":"markdown","e5b5a7f4":"markdown","51e396d0":"markdown","112764ac":"markdown","fa4fae01":"markdown"},"source":{"5048c60a":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","2c689e35":"df = pd.read_csv('..\/input\/ireland-historical-news\/irishtimes-date-text.csv')\ndf.head()","3b6fb00a":"try:\n    df = df.drop('publish_date', axis=1)\nexcept:\n    #already dropped\n    pass\n\ndf.head()","ef1eaece":"df['headline_category'].value_counts()","ccc965c2":"print(len(df['headline_category'].value_counts()))","e8c7e681":"filtered_df = df[df.groupby('headline_category').headline_category.transform(len)>10000]\nprint(len(filtered_df['headline_category'].value_counts()))","2f971c77":"filtered_df['headline_category'].value_counts()","817cb4bb":"plt.figure(figsize=(20,20))\nf = sns.countplot(filtered_df['headline_category'])\nf.set_xticklabels(f.get_xticklabels(), rotation=20, ha=\"right\");","1af71cab":"def sampling_k_elements(group, k=10000):\n    return group.sample(k)\n\n#Apply the function to all groups\nbalanced_df = filtered_df.groupby('headline_category').apply(sampling_k_elements).reset_index(drop=True)\nbalanced_df['headline_category'].value_counts()","038b3f49":"balanced_df['category'] = balanced_df['headline_category'].astype(\"category\").cat.codes\nbalanced_df.head()","3a6a65ab":"np.random.seed(123)\nbalanced_df = balanced_df.iloc[np.random.permutation(len(balanced_df))]\ncut1 = int(0.8 * len(balanced_df)) + 1\ntry:\n    dropped_balanced_df = balanced_df.drop('headline_category', axis=1)\nexcept:\n    pass\n\ndf_train, df_valid = dropped_balanced_df[:cut1], dropped_balanced_df[cut1:]","93659106":"print(df_train.shape)\ndf_train.head()","c3d3caaf":"print(df_valid.shape)\ndf_valid.head()","aaa0de2e":"df_valid['category'].value_counts()","13fda78d":"category_numbers = dict(enumerate(balanced_df['headline_category'].astype(\"category\").cat.categories))\nprint (category_numbers)","c5eade76":"from fastai.text import *","fed6a202":"data_lm = TextLMDataBunch.from_df(path=\"\", train_df=df_train, valid_df = df_valid, text_cols=\"headline_text\", label_cols=\"category\")","9b321c75":"data_lm.save('irish.pkl')","cf051b77":"data_lm.show_batch(5)","2b7fd17f":"data_lm.vocab.itos[:40]","3f85b829":"learner = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.1)","ce04caf2":"learner.save_encoder('irish_encoder')","0c9d5cfa":"data_clas = TextClasDataBunch.from_df(path=\"\", train_df=df_train, valid_df = df_valid, vocab=data_lm.train_ds.vocab, text_cols=\"headline_text\",label_cols=\"category\")","131a16fa":"clas = text_classifier_learner(data_clas, AWD_LSTM, drop_mult=0.1)","1d06df33":"clas.load_encoder('irish_encoder')","c4574f25":"clas.lr_find()\nclas.recorder.plot()","d5e2fecc":"clas.fit_one_cycle(5, 1e-02)","c83f3aee":"clas.freeze_to(-2)\nclas.lr_find()\nclas.recorder.plot(suggestion=True)","9df11778":"clas.fit_one_cycle(5, 5e-04, moms=(0.9,0.8))","fb5c59eb":"clas.save_encoder('freeze_2_encoder')","3073cd37":"clas.freeze_to(-3)\nclas.lr_find()\nclas.recorder.plot(suggestion=True)","f597dfcb":"clas.fit_one_cycle(5, 3.2e-05, moms=(0.95,0.85))","cde2704d":"clas.save_encoder('freeze_3_encoder')","df5d30fd":"clas.unfreeze()\nclas.lr_find()\nclas.recorder.plot(suggestion=True)","5789a813":"clas.fit_one_cycle(3, 5e-05, moms=(0.95, 0.85))","0b0341c5":"clas.save_encoder('final_encoder')","f000c0cc":"clas.predict(\"Artist A's latest album is soaring through the charts\")","af7fdfcf":"print(category_numbers.get(7))","9a1f36d1":"clas.predict(\"Beatles' latest album is soaring through the charts\")[2].sum()","0503914a":"clas.predict(\"An underdog wins the worldcup 2-0\")","2f242807":"category_numbers.get(18) #To know what category 18 belongs to, let us see...","dd0b0d4f":"def pred_classes(text):\n    print(category_numbers.get(int(clas.predict(text)[1])))    ","67ce03bb":"pred_classes(\"NIFTY falls down by 100 rupees\")","0b08f9af":"pred_classes(\"Eggs and cholestrol - is eating many eggs really unhealthy for your heart?\")","0e7e1afd":"pred_classes(\"We need to do something now, or else our planet is doomed\")","853c93d8":"pred_classes(\"10 ways to improve your kitchen\")","4f603bb0":"pred_classes(\"A couple gets 10 years for killing a teddy bear\")","a70e74f1":"> \"Transfer learning will be the next drive of ML success\" - Andrew Ng","9404073e":"There are many ways of fixing a class imbalance problem - \n* Using metrics like AUC (Area Under the Curve) instead of accuracy for weights updation.\n* Downsampling the classes that have a lot of samples (also called majority classes)\n* Upsampling the classes that have very less samples (also called minority classes)\n* Using an algorithm that is robust to class imbalance (such as decision trees)\n* Generate synthetic samples \n* Penalize the model if the performance on minority classes is low.\n\nHere, we will downsample all the categories to 10000. ","e66a74ab":"Let's check how data_lm looks now.","00fb572a":"# Unbalanced Data\n\nLooks like there are a lot of overlapping categories. 5 business categories, lots of news categories and 3 sports categories. You can combine them and implement the model and compare the results (you will probably get a higher accuracy since similar classes are merged into one, which are the ones our model will often get wrong), but here I have kept them separate.\n\nThat issue aside, we have a big problem here that is clearly shown by the count plot. There are way too many headlines classified as news (and to some extent, sports and business), compared to the others. \n\n**Why is this a problem?**\n\nWell, suppose we want to predict the class without knowing any data -- the chance is 1\/21 i.e. 4.76%. Pretty low. Your model can predict a random class and it'll only get 4.76% accuracy.\n\nNow, suppose we feed this data to our model with a validation test that is also so skewed. Most of the models will start to predict news or sports or business on every headline. \nWhy is that? For that, let's see how much % of our headlines is 'news'. According to the csv, we have 1.43 million rows. And from the above value_counts, we have found that news corresponds to 574774 of headlines. \n\n574774\/1430000 = 40.19%\nSo even if the model were to predict that every single headline belongs to the 'news' category, it would have 40.19% accuracy. Now, if it learns the difference between only the major 3 categories -- 'news', 'sports' and 'business', it can reach about 60% accuracy.\n\nWhat this means is that our model potentially won't learn anything about other classes and if that happens, it will not be able to classify the less frequent categories. ","6a658fd2":"Here we use another parameter, moms, which is another parameter that Leslie Smith advices to use. It stands for momentum. Since I'm picking an aggressive learning rate here, the momentum will help slow down the overshoot if there is any. \n\nIntuitively, we want to have\n\n1) a higher momentum with a low learning rate\n\n2) a low momentum with a high learning rate\n\n> To accompany the movement toward larger learning rates, Leslie found in his experiments that decreasing the momentum led to better results. This supports the intuition that in that part of the training, we want the SGD to quickly go in new directions to find a flatter area, so the new gradients need to be given more weight. In practice, he recommends to pick two values likes 0.85 and 0.95, and decrease from the higher one to the lower one when we increase the learning rate, then go back to the higher momentum as the learning rate goes down.\n\nCited from [Sylvain Gugger's post](https:\/\/sgugger.github.io\/the-1cycle-policy.html) ","0cd3286a":"Pretty inconvenient to predict and find out manually each time. Let's make a function to predict and try predicting something not related to Ireland. I'll take the NIFTY stock, which is not related to Ireland.","3c21eb50":"Fast ai provides an easy way to finding the ideal learning rate for the mode that works most of the time.","923f79f0":"The objective of this notebook is to serve as an introduction to go about building a headlines classifier using transfer learning (more specifically, ULMFiT).\n\nIn this notebook, we will go over the following topics\n1. What is imbalanced data and why is it bad?\n2. What is transfer learning?\n3. How to use fast.ai to train a headline classifier model quickly and easily with the help of transfer learning?","0b1b0689":"We can check that the sum of all of the probabilities is 1.","52c464af":"Let's predict some stuff!","5f08abbe":"Now, there is a new column with numerical categories.\nWe can now proceed to split our dataset into two - train and validation. We do this in order to ensure that our model does not overfit the dataset. Comparising train accuracy vs validation loss and accuracy tells us how well the model performs for seen data (train) vs unseen data (validation).\n\nWe split 80% of dataset in train and 20% of dataset in valid in this case. We also drop the headline_category column, which is no longer required since we have converted the categories to numbers.","9da2fb9d":"Mostly the tokens, and the common words.\n\nThe nouns are Ireland, Irish and Dublin. There's also a \u20ac.\nUntil recently many believed that removing stopwords was an essential step before modelling, but views have changed. In this case, I am not going to remove them, because stop words have been shown to contribute a lot to the meaning of a sentence.\n\nFor those who are familiar with stopwords, they are words that always occur in texts, regardless of their nature. For eg, \"an\", \"a\", \"in\", \"the\", etc. compared to (say), a word like \"vindicate\" that is probably mostly present in only legal matters.","4670f283":"The y axis is the loss, and the x axis is the learning rate. If you're thinking that we should pick the learning rate that corresponds to the lowest loss, then you are mistaken in your thinking. We want our model to learn as fast as possible but if the loss is minimum, then what our model will learn is also minimum.\n\nMost of the losses vs learning rate graphs look quite similar - They go down slowly to a certain point, stay for a few values, and then shoot up very rapidly because of overfitting. \n\nA good thumb of rule is to pick the loss as LR(Loss_min)\/10, where LR(Loss_min) refers to the learning rate where the loss is minimum, but this does not always work. In cases like these, you might have to experiment a bit.\n\nWe specify the number of epochs(5 here), and the maximum learning rate to the classifier and train.","f309677b":"### One cycle fit \n\nfastai uses 1 cycle fit, which works very well. You can check out the original paper  by Leslie Smith [here](https:\/\/arxiv.org\/abs\/1803.09820). \n\n![image.png](attachment:image.png)\nImage from [Sylvain Gugger's post](https:\/\/sgugger.github.io\/the-1cycle-policy.html) on 1cycle fit, which you can refer to if you want to understand 1cycle in detail.\n\nEssentially, we start slow (low learning rate), then keep raising the learning rate until it hits maximum, and then slow down again. When all of this is done once, one cycle is complete.\n\n# Fine-tuning\n\nAfter 5 epochs on a pretrained model, we have already got nearly half chance of predicting the correct class amongst 21 (almost 10 times better than picking the correct class at random). In less than 5 minutes, too.\n\nBut we can surely improve the accuracy further. Let us keep all the layers of the pretrained model frozen(as it is) and train again.","a537a9fb":"Some more testing and fun...","53aec737":"References - \n1. [fastai text docs](https:\/\/docs.fast.ai\/text.html)\n2. [Towards Data Science - Machine Learning \u2014 Text Classification, Language Modelling using fast.ai by Javaid Nabi](https:\/\/towardsdatascience.com\/machine-learning-text-classification-language-modelling-using-fast-ai-b1b334f2872d)\n3. [Towards Data Science -Transfer Learning in NLP for Tweet Stance Classification by Prashanth Rao](https:\/\/towardsdatascience.com\/transfer-learning-in-nlp-for-tweet-stance-classification-8ab014da8dde)\n4. [Sylvain Gugger's excellent post on one cycle fit](https:\/\/sgugger.github.io\/the-1cycle-policy.html)\n\n\n\n","afb59388":"Let's save the databunch, so we can directly use it the next time directly.","8d264113":"Import the csv file as a pandas dataframe, and check what the data looks like - ","2066ff4e":"Import fast ai's text API","9fe3e8cc":"And we're done. Near to 60% accuracy is pretty good for training for less than 20 minutes of training on a 21 classes problem with quite a few similar classes. This is the power of transfer learning. I'm sure the accuracy can be improved by merging similar classes, taking more gradual approach and having more epochs or simply getting more data of each class, but I'm quite satisfied with this. ","14ec1b5a":"Finally, we will unfreeze the entire model and train.","3837a4c5":"Begin with importing the libraries that we will for preliminary data analysis.","c094ff8f":"Now we define our language model learner, in which we pass our DataBunch and specify the pretrained network that we want to use - AWD_LSTM that was used on WT-103 (Wikitext-103). \n\ndrop_mult is a hyper-parameter that defines by how much we want to multiply the dropout layers of our network. Dropout refers to dropping out units in the network(both hidden and visible) to avoid overfitting the network. \n\ndrop_mult = 0.3 means all dropout layers's probability will be multiplied by 0.3. drop_mult = 1.5 means all dropout layers's probability will be multiplied by 1.5. AWD_LSTM has a dropout in every layer, so this parameter is quite important.\n\nTo understand it simply, if model is overfitting, increase drop_mult.\nIf model is underfitting, decrease drop_mult.","96e4adb7":"Now we have a relatively balanced validation dataframe (all categories are close to 2000).","574e473c":"We create a dictionary for future use so we know which number corresponds to which category","786682d0":"There are two tensors here - The first tensor (7) tells us which class it probably belongs to, and the long, second tensor is a list of probabilities.\n\nBut well, what category did 7 belong to, again?\nRemember the dictionary that we made when we numericalized the categories? We can use that to find out.","afc86480":"That's certainly a lot, but most of them probably have very less frequencies. Let's see how many categories exist that have occured at least 10000 times in the sample.","46ce3d40":"### What does Language Modelling mean and what are all those xx terms?\n\nLanguage modelling, in simple terms, is to understand a language using various techniques, such as its word representations(individual words) or its semantic meaning(whole text\/sentence\/paragraph).\n\nWe use the xx tokens so that the model can understand better and differentiate between two similar texts. For example, \"I need a hotdog now\" vs \"I NEED a hotdog now\". The second one implies more urgency, more hunger than the first. However, to have less features and reduce complexity, since all text is converted to lower case, both of them end up as \"i need a hotdog now\", in which we have definitely lost some understanding after tokenization. \"China\" is a country, \"china\" is porcelain. So you can see, proper tokenization is a very important prequisite for language modelling.\n\nUsing xx tokens we retain a lot of the original meaning despite converting everything to lowercase.\nHere are all of the xx tokens and what they mean - \n\n1. xxbos = Beggining Of Sentence = This xx token represents the beggining of a sentence.\n\n2. xxpad = Padding = Used for padding if needed to regroup several texts of different lengths into one batch.\n\n3. xxmaj = Denotes that the following word begins with uppercase. So \"Hello World\" becomes \"xxbos xxmaj hello xxmaj world\"\n\n4. xxfld = Separates different fields of text (when we have more than one column in dataframe for text classification, for eg, if we had both headlines and article text in two seperate columns, we would have 2 fields).\n\n5. xxup = Denotes that the following word is entirely uppercase. \"... NEED... \" becomes \"...xxup need...\"\n\n6. xxrep = Repetitions = Denotes how many times a CHARACTER has been repeated consecutively, if it has been repeated consecutively. \"...oooo ...\" becomes \"...xxrep 4 o ...\"\n\n7. xxwrep = Same thing as xxrep but for words. \"...muda muda muda muda muda...\" becomes \"...xxwrep 5 muda...\"\n\n8. xxunk = Denotes an unknown word, that is not in our vocabulary. ","92c781a5":"Now that we have tried our language modeller and made an encoder that can understand text, we need to create a text classifier that separates the 21 classes we had. For that, first, we need to create a TextClasDataBunch, which is another type of DataBunch.\n\nIn this we specify vocab = data_lm.train_ds.vocab to ensure that the vocabularly that  we had in language modelling is the same as what we will have for the text classifier.","f994a3f5":"We now see that each category is of 10000 counts only, giving us a perfectly balanced dataset.\n\nLet us numericalize the categories (instead of 'news', 'culture', 'health', we convert them to some corresponding numbers such as '9', '5', '2', etc). It can be easily done by converting the column to a category and finding out the category codes.","08aa8ac4":"Let's see how many unique categories are there, and how many samples each of those have using value_counts (returns the values and their counts) of a pandas Series.","d840476e":"Save the encoder for the classifier so we don't have to train next time. We can simply load clas.load_encoder('freeze_2_encoder').\n\nYou can see that the training loss actually went up the last iteration, so we will need to unfreeze more layers to get better results.","19245a4f":"We save the encoded learner, so we can use it in our classifier. This encoder is what understands the text.","539bb738":"We'll keep the model frozen except the last three layers and train again.","b5c8cf46":"Let's check the top 40 words used in the headlines, since we are done with the tokenization.\n\nThe default size of the vocabulary is 60000. A word has to occur at least 2 times to be added to the vocabulary.","725325dc":"21 seems to be a lot more reasonable for news classifications. Let's check what they are.","14585029":"Load the encoder that was obtained from language model learner...\nYou can check out the details of the entire network if you wish. ","f6ecff54":"Publish_date doesn't seem like it would be very useful in classifying the headline_text as headline_category, so we'll drop that column and see what our new dataframe looks like. Besides, the goal of this kernel is to classify headlines only based on the text.","cb8d2c17":"# Transfer Learning\n\nNow that our data is ready, we will dive into modelling our classifier. \n\nFrom Wikipedia, \n> Transfer learning is a research problem in machine learning that focuses on storing knowledge gained while solving one problem and applying it to a different but related problem. For example, knowledge gained while learning to recognize cars could apply when trying to recognize trucks. \n\nMost of machine learning models are very specific. They are very good at handling one problem, and only that one problem. Transfer learning allows us to use the knowledge that a model has learned in domain A (suppose cars) in training a model for a similar problem in domain B (suppose trucks).\n\nWhat we essentially do is take a pretrained model that was used for a task T1 in certain domain A, and fine tune it to use it on a similar task T2 in domain B.\n\nfast ai features ULMFiT (Universal Language Model Fine-tuning for Text Classification) which as the name suggests, is a universal language model for text classification. It is a language modeller that has been trained on the Wikitext-103 (103 million tokens_ dataset, which is basically the entire wikipedia scraped. There are also other models like this, such as Google's BERT, Open AI's GPT-2 (Generative Pre-Training). ","e5b5a7f4":"### Tokenization \nNext, we need to create a TextLMDataBunch, which fast ai uses for all of its NLP operations. It tokenizes the data while trying to retain most of the meaning of the sentence(we will discuss this more further down). Tokenization refers to breaking down of text into tokens - for example, \"This is a sentence\" is separated into 4 tokens and forced into lower case - 'this', 'is', 'a', 'sentence'.\n\nDataBunch is what is fed to the neural network. It has 5 components (for supervised learning) - \n\n1. Training Dataset\n2. Training DataLoader\n3. Validation Dataset\n4. Validation DataLoader\n5. (Optional) Test set, which we are not going to use in this kernel.\n\nDataset and DataLoader are PyTorch classes, so you can read more about them in their docs if you want to know more in detail.\n\nThe combination of all these 5 is bunched in the fast ai DataBunch.\n\nTextDataBunch is just a type of DataBunch, however, it has a big limitation. It only works directly if ALL texts have the same length. So to convert our headlines, which are of uneven lengths, we use TextLMDataBunch that concatenates the text without their target labels so our headlines can be used as a TextDataBunch. This TextLMDataBunch is essentially the data that we can feed into language modeler later.\n\nThis might take some time...","51e396d0":"Here we create the text classifier, using the pretrained AWD_LSTM model that was used for WT-103.","112764ac":"Seems like a lot. Let's find out how many exactly...","fa4fae01":"Checking the shapes and heads of the new dataframes. There are total 210000 samples (21 categories * 10000 headlines per category)."}}