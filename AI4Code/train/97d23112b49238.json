{"cell_type":{"c1d10c76":"code","4cbd0be8":"code","4b31c0e8":"code","bd409792":"code","ab1c757e":"code","1a48e3f4":"code","b359dcee":"code","7e9bd8ad":"code","50030b9d":"code","b59e27d3":"code","bf6d79bb":"code","79170936":"code","58e45be7":"code","9e931125":"code","f0c3fbc4":"code","c70e74f0":"code","09844de7":"code","6af3facc":"code","14338cc5":"code","e5193699":"code","8cb4588c":"code","0106e4fd":"code","d758ff87":"code","b2db73d3":"code","3a86517a":"code","913fac3b":"code","9fe0bc8f":"code","7b0afc54":"code","f42fb5ea":"code","5750f11f":"code","109cf288":"code","1de8bc8e":"code","bd4acd77":"code","5e5cdca4":"code","15f93f0b":"code","ecee7151":"code","4ed16b23":"code","3be96717":"code","becfb619":"code","c2619c38":"code","2d71e4f8":"code","7be55876":"code","5d38bab1":"code","683626b2":"code","d2f6f17e":"markdown","684f4fe2":"markdown","a96c2c57":"markdown","f6e5995e":"markdown","68f22a4f":"markdown","21f0c46d":"markdown","42801394":"markdown","40224875":"markdown","06bad79d":"markdown","44d7113f":"markdown","1218103e":"markdown"},"source":{"c1d10c76":"!pip install ipython-autotime\n%load_ext autotime","4cbd0be8":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score, classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import resample\nfrom sklearn.model_selection import KFold\nimport multiprocessing as mp\nfrom xgboost import XGBClassifier\nfrom bayes_opt import BayesianOptimization\nfrom catboost import CatBoostClassifier","4b31c0e8":"import warnings\nwarnings.filterwarnings('ignore')","bd409792":"mp.cpu_count()","ab1c757e":"draw = False","1a48e3f4":"df = pd.read_csv(\"..\/input\/weather-dataset-rattle-package\/weatherAUS.csv\")\n# df = pd.read_csv(\"weatherAUS.csv\")\ndf","b359dcee":"df.isnull().sum()","7e9bd8ad":"# adding features\n# split year month day\ndf[\"Year\"] = df[\"Date\"].map(lambda x: x.split(\"-\")[0])\ndf[\"Month\"] = df[\"Date\"].map(lambda x: x.split(\"-\")[1])\ndf[\"Day\"] = df[\"Date\"].map(lambda x: x.split(\"-\")[2])\n\n# wind dir to point on circle\nfor col in [\"WindGustDir\", \"WindDir9am\", \"WindDir3pm\"]:\n    df[col + \"Sin\"] = df[col].map({\n        \"E\": 0.0, \"N\": 1.0, \"W\": 0.0, \"S\": -1.0,\n        \"NE\": np.sqrt(2), \"NW\": np.sqrt(2), \"SW\": -np.sqrt(2), \"SE\": -np.sqrt(2),\n        \"ENE\": np.sqrt(2 - np.sqrt(2)) \/ 2, \"NNE\": np.sqrt(2 + np.sqrt(2)) \/ 2, \"NNW\": np.sqrt(2 + np.sqrt(2)) \/ 2, \"WNW\": np.sqrt(2 - np.sqrt(2)) \/ 2,\n        \"WSW\": - np.sqrt(2 - np.sqrt(2)) \/ 2, \"SSW\": - np.sqrt(2 + np.sqrt(2)), \"SSE\": - np.sqrt(2 + np.sqrt(2)), \"ESE\": - np.sqrt(2 - np.sqrt(2)) \/ 2,\n        float('nan'): 0.0,\n    })\n    df[col + \"Cos\"] = df[col].map({\n        \"E\": 1.0, \"N\": 0.0, \"W\": -1.0, \"S\": 0.0,\n        \"NE\": np.sqrt(2), \"NW\": -np.sqrt(2), \"SW\": -np.sqrt(2), \"SE\": np.sqrt(2),\n        \"ENE\": np.sqrt(2 + np.sqrt(2)) \/ 2, \"NNE\": np.sqrt(2 - np.sqrt(2)) \/ 2, \"NNW\": - np.sqrt(2 - np.sqrt(2)) \/ 2, \"WNW\": - np.sqrt(2 + np.sqrt(2)) \/ 2,\n        \"WSW\": - np.sqrt(2 + np.sqrt(2)) \/ 2, \"SSW\": - np.sqrt(2 - np.sqrt(2)), \"SSE\": np.sqrt(2 - np.sqrt(2)), \"ESE\": np.sqrt(2 + np.sqrt(2)) \/ 2,\n        float('nan'): 0.0,\n    })\ndf\n\n# 1daybefore\nfor col in df.columns:\n    df[col + \"1daybefore\"] = df[col]\n    df[col + \"1daybefore\"][1:len(df[col])] = df[col][0:len(df[col])-1]\n","50030b9d":"df.columns","b59e27d3":"categorical_features = [\"Location\", \"Year\", \"Month\", \"Day\", \"RainToday\", \"WindGustDir\", \"WindDir9am\", \"WindDir3pm\"]\nnumerical_features = [\"MinTemp\", \"MaxTemp\", \"Rainfall\", \"Evaporation\", \"Sunshine\", \"WindGustSpeed\", \"WindSpeed9am\", \"WindSpeed3pm\", \"Humidity9am\", \"Humidity3pm\", \"Pressure9am\", \"Pressure3pm\", \"Cloud9am\", \"Cloud3pm\", \"Temp9am\", \"Temp3pm\", \"WindDir9amSin\", \"WindDir9amCos\", \"WindDir3pmSin\", \"WindDir3pmCos\", \"WindGustDirSin\", \"WindGustDirCos\",]\ntarget_feature = \"RainTomorrow\"\n\ncategorical_features.extend(list(map(lambda col: col + \"1daybefore\", categorical_features)))\nfor discard in [\"Location1daybefore\", \"Year1daybefore\", \"Month1daybefore\", \"Day1daybefore\"]:\n    categorical_features.remove(discard)\n    \nnumerical_features.extend(list(map(lambda col: col + \"1daybefore\", numerical_features)))","bf6d79bb":"categorical_features","79170936":"numerical_features","58e45be7":"# drop nan on target_feature\ndf = df[~df[target_feature].isnull()]","9e931125":"df[categorical_features].isnull().sum()","f0c3fbc4":"for col in categorical_features:\n    value = df[col].mode()[0]\n    df[col].fillna(value, inplace=True)\n    \ndf[categorical_features].isnull().sum()","c70e74f0":"df[numerical_features].isnull().sum()","09844de7":"for col in numerical_features:\n    value = df[col].median()\n    df[col].fillna(value, inplace=True)\n    \ndf[numerical_features].isnull().sum()","6af3facc":"if draw:\n    _, ax = plt.subplots(nrows=len(categorical_features), ncols=1, figsize=(12.8, 5.4*len(categorical_features)))\n    ax = ax.flatten()\n    for i, feature in enumerate(categorical_features):\n        sns.countplot(x=feature, hue=target_feature , data=df, ax=ax[i])\n    plt.tight_layout()","14338cc5":"if draw:\n    _, ax = plt.subplots(nrows=len(numerical_features), ncols=1, figsize=(12.8, 5.4*len(numerical_features)))\n    ax = ax.flatten()\n    for i, feature in enumerate(numerical_features):\n        sns.histplot(x=feature, hue=target_feature , data=df, ax=ax[i])\n    plt.tight_layout()","e5193699":"if draw:\n    df[target_feature + \"_01\"] = df[target_feature].map({\"No\": 0.0, \"Yes\": 1.0})\n    _, ax = plt.subplots(nrows=1, ncols=1, figsize=(20, 20))\n    sns.heatmap(df[numerical_features + [target_feature + \"_01\"]].corr(), annot=True, ax=ax)","8cb4588c":"# TRAIN TEST SPLIT\ndef to_Xy(df, categorical_features, numerical_features, target_feature, onehot=False):\n    \"\"\"one hot encoding for categorical features\"\"\"\n    df = df.copy()\n    # X\n    X = []\n    for col in categorical_features:\n        df[col] = LabelEncoder().fit_transform(df[col])\n        if onehot:\n            X.append(pd.get_dummies(df[col], prefix=col))\n        else:\n            X.append(df[col])\n    for col in numerical_features:\n        df[col] = df[col].astype(np.float64)\n        X.append(df[col])\n            \n    X = pd.concat(X, axis=1)\n    \n    # y\n    if target_feature in df.columns:\n        df[target_feature] = LabelEncoder().fit_transform(df[target_feature])\n        y = df[target_feature]\n    else:\n        y = None\n    return X, y\n\nX, y = to_Xy(df, categorical_features, numerical_features, target_feature, onehot=False)","0106e4fd":"X","d758ff87":"y","b2db73d3":"def cross_validation(clf, features, k=10, random_state=0):\n    kf = KFold(n_splits=k, shuffle=True, random_state=random_state)\n    \n    auc_test_avg = []\n    for train_idx, test_idx in kf.split(np.arange(len(y))):\n        # resampling\n        X_ = []\n        y_ = []\n        for label in y.iloc[train_idx].unique():\n            X_.append(X.iloc[train_idx, :][y.iloc[train_idx] == label])\n            y_.append(y.iloc[train_idx][y.iloc[train_idx] == label])\n        n_samples = max(yy.shape[0] for yy in y_)\n        for i in range(len(y_)):\n            if len(y_[i]) < n_samples:\n                X_[i], y_[i] = resample(X_[i], y_[i], n_samples=n_samples, replace=True)\n\n        X_train = pd.concat(X_)\n        y_train = pd.concat(y_)\n        X_test = X.iloc[test_idx]\n        y_test = y.iloc[test_idx]\n\n        # train model\n        clf.fit(X_train[features] , y_train)\n        y_train_pred = clf.predict_proba(X_train[features])[:, 1]\n        auc_train = roc_auc_score(y_train, y_train_pred)\n        y_test_pred = clf.predict_proba(X_test[features])[:, 1]\n        auc_test = roc_auc_score(y_test, y_test_pred)\n\n        auc_test_avg.append(auc_test)\n\n    auc_test_avg = sum(auc_test_avg) \/ len(auc_test_avg)\n    return auc_test_avg","3a86517a":"# logistic regression - baseline\ncategorical_features = [\"Location\", \"Year\", \"Month\", \"Day\", \"RainToday\", \"WindGustDir\", \"WindDir9am\", \"WindDir3pm\"]\nnumerical_features = [\"MinTemp\", \"MaxTemp\", \"Rainfall\", \"Evaporation\", \"Sunshine\", \"WindGustSpeed\", \"WindSpeed9am\", \"WindSpeed3pm\", \"Humidity9am\", \"Humidity3pm\", \"Pressure9am\", \"Pressure3pm\", \"Cloud9am\", \"Cloud3pm\", \"Temp9am\", \"Temp3pm\", \"WindDir9amSin\", \"WindDir9amCos\", \"WindDir3pmSin\", \"WindDir3pmCos\", \"WindGustDirSin\", \"WindGustDirCos\"]\nfeatures = categorical_features + numerical_features    \n\nauc = cross_validation(\n    clf=make_pipeline(StandardScaler(), LogisticRegression(\n        max_iter=100,\n        random_state=0,\n        n_jobs=mp.cpu_count(),\n    )),\n    features=features,\n)\nauc","913fac3b":"# logistic regression - without categorical dir\ncategorical_features = [\"Location\", \"Year\", \"Month\", \"Day\", \"RainToday\"]\nnumerical_features = [\"MinTemp\", \"MaxTemp\", \"Rainfall\", \"Evaporation\", \"Sunshine\", \"WindGustSpeed\", \"WindSpeed9am\", \"WindSpeed3pm\", \"Humidity9am\", \"Humidity3pm\", \"Pressure9am\", \"Pressure3pm\", \"Cloud9am\", \"Cloud3pm\", \"Temp9am\", \"Temp3pm\", \"WindDir9amSin\", \"WindDir9amCos\", \"WindDir3pmSin\", \"WindDir3pmCos\", \"WindGustDirSin\", \"WindGustDirCos\"]\nfeatures = categorical_features + numerical_features    \n\nauc = cross_validation(\n    clf=make_pipeline(StandardScaler(), LogisticRegression(\n        max_iter=100,\n        random_state=0,\n        n_jobs=mp.cpu_count(),\n    )),\n    features=features,\n)\nauc","9fe0bc8f":"# logistic regression - without sin cos dir\ncategorical_features = [\"Location\", \"Year\", \"Month\", \"Day\", \"RainToday\", \"WindGustDir\", \"WindDir9am\", \"WindDir3pm\"]\nnumerical_features = [\"MinTemp\", \"MaxTemp\", \"Rainfall\", \"Evaporation\", \"Sunshine\", \"WindGustSpeed\", \"WindSpeed9am\", \"WindSpeed3pm\", \"Humidity9am\", \"Humidity3pm\", \"Pressure9am\", \"Pressure3pm\", \"Cloud9am\", \"Cloud3pm\", \"Temp9am\", \"Temp3pm\"]\nfeatures = categorical_features + numerical_features    \n\nauc = cross_validation(\n    clf=make_pipeline(StandardScaler(), LogisticRegression(\n        max_iter=100,\n        random_state=0,\n        n_jobs=mp.cpu_count(),\n    )),\n    features=features,\n)\nauc","7b0afc54":"# logistic regression - without year month date\ncategorical_features = [\"Location\", \"RainToday\", \"WindGustDir\", \"WindDir9am\", \"WindDir3pm\"]\nnumerical_features = [\"MinTemp\", \"MaxTemp\", \"Rainfall\", \"Evaporation\", \"Sunshine\", \"WindGustSpeed\", \"WindSpeed9am\", \"WindSpeed3pm\", \"Humidity9am\", \"Humidity3pm\", \"Pressure9am\", \"Pressure3pm\", \"Cloud9am\", \"Cloud3pm\", \"Temp9am\", \"Temp3pm\", \"WindDir9amSin\", \"WindDir9amCos\", \"WindDir3pmSin\", \"WindDir3pmCos\", \"WindGustDirSin\", \"WindGustDirCos\"]\nfeatures = categorical_features + numerical_features    \n\nauc = cross_validation(\n    clf=make_pipeline(StandardScaler(), LogisticRegression(\n        max_iter=100,\n        random_state=0,\n        n_jobs=mp.cpu_count(),\n    )),\n    features=features,\n)\nauc  ","f42fb5ea":"# logistic regression - 1daybefore\ncategorical_features = [\"Location\", \"Year\", \"Month\", \"Day\", \"RainToday\", \"WindGustDir\", \"WindDir9am\", \"WindDir3pm\"]\nnumerical_features = [\"MinTemp\", \"MaxTemp\", \"Rainfall\", \"Evaporation\", \"Sunshine\", \"WindGustSpeed\", \"WindSpeed9am\", \"WindSpeed3pm\", \"Humidity9am\", \"Humidity3pm\", \"Pressure9am\", \"Pressure3pm\", \"Cloud9am\", \"Cloud3pm\", \"Temp9am\", \"Temp3pm\", \"WindDir9amSin\", \"WindDir9amCos\", \"WindDir3pmSin\", \"WindDir3pmCos\", \"WindGustDirSin\", \"WindGustDirCos\",]\ntarget_feature = \"RainTomorrow\"\n\ncategorical_features.extend(list(map(lambda col: col + \"1daybefore\", categorical_features)))\nfor discard in [\"Location1daybefore\", \"Year1daybefore\", \"Month1daybefore\", \"Day1daybefore\"]:\n    categorical_features.remove(discard)\n\nnumerical_features.extend(list(map(lambda col: col + \"1daybefore\", numerical_features)))\n\nfeatures = categorical_features + numerical_features    \n\nauc = cross_validation(\n    clf=make_pipeline(StandardScaler(), LogisticRegression(\n        max_iter=100,\n        random_state=0,\n        n_jobs=mp.cpu_count(),\n    )),\n    features=features,\n)\nauc","5750f11f":"# xgboost - 1daybefore\ncategorical_features = [\"Location\", \"Year\", \"Month\", \"Day\", \"RainToday\", \"WindGustDir\", \"WindDir9am\", \"WindDir3pm\"]\nnumerical_features = [\"MinTemp\", \"MaxTemp\", \"Rainfall\", \"Evaporation\", \"Sunshine\", \"WindGustSpeed\", \"WindSpeed9am\", \"WindSpeed3pm\", \"Humidity9am\", \"Humidity3pm\", \"Pressure9am\", \"Pressure3pm\", \"Cloud9am\", \"Cloud3pm\", \"Temp9am\", \"Temp3pm\", \"WindDir9amSin\", \"WindDir9amCos\", \"WindDir3pmSin\", \"WindDir3pmCos\", \"WindGustDirSin\", \"WindGustDirCos\",]\ntarget_feature = \"RainTomorrow\"\n\ncategorical_features.extend(list(map(lambda col: col + \"1daybefore\", categorical_features)))\nfor discard in [\"Location1daybefore\", \"Year1daybefore\", \"Month1daybefore\", \"Day1daybefore\"]:\n    categorical_features.remove(discard)\n    \nnumerical_features.extend(list(map(lambda col: col + \"1daybefore\", numerical_features)))\n\nfeatures = categorical_features + numerical_features    \n\nauc = cross_validation(\n    clf=XGBClassifier(\n        n_estimators=100,\n        random_state=0,\n        n_jobs=mp.cpu_count(),\n        eval_metric=\"auc\",\n    ),\n    features=features,\n)\nauc","109cf288":"# logistic regression - 1daybefore\ncategorical_features = [\"Location\", \"Year\", \"Month\", \"Day\", \"RainToday\", \"WindGustDir\", \"WindDir9am\", \"WindDir3pm\"]\nnumerical_features = [\"MinTemp\", \"MaxTemp\", \"Rainfall\", \"Evaporation\", \"Sunshine\", \"WindGustSpeed\", \"WindSpeed9am\", \"WindSpeed3pm\", \"Humidity9am\", \"Humidity3pm\", \"Pressure9am\", \"Pressure3pm\", \"Cloud9am\", \"Cloud3pm\", \"Temp9am\", \"Temp3pm\", \"WindDir9amSin\", \"WindDir9amCos\", \"WindDir3pmSin\", \"WindDir3pmCos\", \"WindGustDirSin\", \"WindGustDirCos\",]\ntarget_feature = \"RainTomorrow\"\n\ncategorical_features.extend(list(map(lambda col: col + \"1daybefore\", categorical_features)))\nfor discard in [\"Location1daybefore\", \"Year1daybefore\", \"Month1daybefore\", \"Day1daybefore\"]:\n    categorical_features.remove(discard)\n    \nnumerical_features.extend(list(map(lambda col: col + \"1daybefore\", numerical_features)))\n\nfeatures = categorical_features + numerical_features    \n\n# train\nclf = XGBClassifier(\n    n_estimators=1000,\n    random_state=0,\n    n_jobs=mp.cpu_count(),\n    eval_metric=\"auc\",\n)\nclf.fit(X , y)\n\ny_pred = clf.predict_proba(X)[:, 1]\nauc = roc_auc_score(y, y_pred)\nauc","1de8bc8e":"feature_importance = clf.feature_importances_\n_, ax = plt.subplots()\nax.plot(np.arange(len(feature_importance)), feature_importance)","bd4acd77":"feature_importance = clf.feature_importances_\n_, ax = plt.subplots()\nax.plot(np.arange(len(feature_importance)), np.sort(feature_importance))","5e5cdca4":"features = np.array(features)[np.argsort(feature_importance)[::-1]]\nfeatures","15f93f0b":"def objective(num_features: int, n_estimators: int, max_depth: int, reg_alpha: float, reg_lambda: float):\n    chosen_features = features[0: int(round(num_features))]\n    auc = cross_validation(\n        clf=XGBClassifier(\n            n_estimators=int(round(n_estimators)),\n            max_depth=int(round(max_depth)),\n            reg_alpha=reg_alpha,\n            reg_lambda=reg_lambda,\n            random_state=0,\n            n_jobs=mp.cpu_count(),\n            eval_metric=\"auc\",\n        ),\n        features=chosen_features,\n    )\n    return auc","ecee7151":"pbounds = {\n    \"num_features\": (1, 10),\n    \"n_estimators\": (50, 100),\n    \"max_depth\": (2, 5),\n    \"reg_alpha\": (0.0, 1.0),\n    \"reg_lambda\": (0.0, 1.0),\n}\n\noptimizer = BayesianOptimization(\n    f=objective,\n    pbounds=pbounds,\n    random_state=0,\n)\n\noptimizer.maximize(n_iter=50)","4ed16b23":"optimizer.max","3be96717":"# logistic regression - 1daybefore\ncategorical_features = [\"Location\", \"Year\", \"Month\", \"Day\", \"RainToday\", \"WindGustDir\", \"WindDir9am\", \"WindDir3pm\"]\nnumerical_features = [\"MinTemp\", \"MaxTemp\", \"Rainfall\", \"Evaporation\", \"Sunshine\", \"WindGustSpeed\", \"WindSpeed9am\", \"WindSpeed3pm\", \"Humidity9am\", \"Humidity3pm\", \"Pressure9am\", \"Pressure3pm\", \"Cloud9am\", \"Cloud3pm\", \"Temp9am\", \"Temp3pm\", \"WindDir9amSin\", \"WindDir9amCos\", \"WindDir3pmSin\", \"WindDir3pmCos\", \"WindGustDirSin\", \"WindGustDirCos\",]\ntarget_feature = \"RainTomorrow\"\n\ncategorical_features.extend(list(map(lambda col: col + \"1daybefore\", categorical_features)))\nfor discard in [\"Location1daybefore\", \"Year1daybefore\", \"Month1daybefore\", \"Day1daybefore\"]:\n    categorical_features.remove(discard)\n    \nnumerical_features.extend(list(map(lambda col: col + \"1daybefore\", numerical_features)))\n\nfeatures = categorical_features + numerical_features    \n\n# train\nclf = CatBoostClassifier(\n    iterations=5000,\n    random_state=0,\n    thread_count=mp.cpu_count(),\n    eval_metric=\"AUC\",\n    verbose=False,\n)\nclf.fit(X , y)\n\ny_pred = clf.predict_proba(X)[:, 1]\nauc = roc_auc_score(y, y_pred)\nauc","becfb619":"feature_importance = clf.feature_importances_\n_, ax = plt.subplots()\nax.plot(np.arange(len(feature_importance)), feature_importance)","c2619c38":"feature_importance = clf.feature_importances_\n_, ax = plt.subplots()\nax.plot(np.arange(len(feature_importance)), np.sort(feature_importance))","2d71e4f8":"features = np.array(features)[np.argsort(feature_importance)[::-1]]\nfeatures","7be55876":"def objective(num_features: int, iterations: int, depth: int, l2_leaf_reg: float, model_size_reg: float):\n    chosen_features = features[0: int(round(num_features))]\n    auc = cross_validation(\n        clf=CatBoostClassifier(\n            iterations=int(round(iterations)),\n            depth=int(round(depth)),\n            l2_leaf_reg=l2_leaf_reg,\n            model_size_reg=model_size_reg,\n            random_state=0,\n            thread_count=mp.cpu_count(),\n            eval_metric=\"AUC\",\n            verbose=False,\n        ),\n        features=chosen_features,\n    )\n    return auc","5d38bab1":"pbounds = {\n    \"num_features\": (1, 10),\n    \"iterations\": (250, 500),\n    \"depth\": (2, 5),\n    \"l2_leaf_reg\": (0.0, 1.0),\n    \"model_size_reg\": (0.0, 1.0),\n}\n\noptimizer = BayesianOptimization(\n    f=objective,\n    pbounds=pbounds,\n    random_state=0,\n)\n\noptimizer.maximize(n_iter=50)","683626b2":"optimizer.max","d2f6f17e":"# CROSS VALIDATION","684f4fe2":"# FILL NAN ON CATEGORICAL FEATURES","a96c2c57":"# DROP NAN ON TARGET","f6e5995e":"# FEATURE IMPORTANCE FROM CATBOOST","68f22a4f":"# FEATURE IMPORTANCE FROM XGBOOST","21f0c46d":"# DRAW","42801394":"- Date: date of observation (*)\n- Location: common name of the weather station\n- MinTemp: minimum temperature\n- MaxTemp: maximum temperature\n- Rainfall: amount of rainfall recorded\n- Evaporation: evaporation\n- Sunshine: number of hours of bright sunshine\n- WindGustDir: direction of strongest wind gust (*)\n- WindGustSpeed: speed of strongest wind gust\n- WindDir9am: direction of wind at 9am (*)\n- WindDir3pm: direction of wind at 3pm (*)\n- WindSpeed9am: speed of wind at least 10 mins prior to 9am \n- WindSpeed3pm: speed of wind at least 10 mins prior to 3pm \n- Humidity9am: humidity at 9am\n- Humidity3pm: humidity at 3pm\n- Pressure9am: pressure at 9am\n- Pressure3pm: pressure at 3pm\n- Cloud9am: fraction of cloud at 9am\n- Cloud3pm: fraction of cloud at 3pm\n- Temp9am: temperature at 9am\n- Temp3pm: temperature at 3pm\n- RainToday: binary\n- RiskMM: millimeter of rain ","40224875":"# ADDING NEW FEATURES\n- Date -> Year, Month, Day\n- Dir -> Sin, Cos\n- Added yesterday data","06bad79d":"# FILL NAN ON NUMERICAL FEATURES","44d7113f":"# ALL FEATURES","1218103e":"# LOAD DATA"}}