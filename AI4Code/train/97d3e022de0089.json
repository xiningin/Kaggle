{"cell_type":{"141e0ef7":"code","5aa0d55f":"code","558057c6":"code","989287e9":"code","d4c5bb28":"code","25e3bd65":"code","6b6f3711":"code","57dababc":"code","f75d8181":"code","d96c4345":"code","6741ccfc":"code","b48ebab6":"code","e8f79ddf":"code","88cf64a3":"code","04bfcf8d":"code","599a14bd":"code","6e7ed2a1":"code","db9b8f70":"code","df776db1":"code","b635cdf0":"code","5dd7850d":"code","1895d7db":"code","acc47d66":"code","5f77b990":"code","46007fa7":"code","172933f7":"code","4ec53038":"code","f221db0b":"code","6fe07be1":"code","c8660436":"code","3b16cd29":"code","6afc9bf5":"code","85375a47":"code","d440bb20":"code","d81d22a6":"markdown","10346832":"markdown","36d0e934":"markdown","ebaf7643":"markdown","f986bf06":"markdown","f1d4731b":"markdown","b59e966b":"markdown","20b6f857":"markdown","4365d756":"markdown","cf089098":"markdown","194f4c97":"markdown","bb992021":"markdown","25052e95":"markdown","2063acca":"markdown","be7c89f7":"markdown","8a8dfe31":"markdown","8499e6e1":"markdown","774740c3":"markdown","585a9b1b":"markdown","f262baa3":"markdown","e4c0264e":"markdown","2839f8c6":"markdown","45c457b7":"markdown","6026417d":"markdown","4c7050e0":"markdown"},"source":{"141e0ef7":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")","5aa0d55f":"#read the data\n\ndata_path = 'countries of the world.csv'  #the path where you downloaded the data\ndf = pd.read_csv(data_path)\n\nprint('The shape of the dataset is:', df.shape)","558057c6":"# Let's see the data types and non-null values for each column\ndf.info()","989287e9":"df.isnull().sum()","d4c5bb28":"# This will print basic statistics for numerical columns\ndf.describe()","25e3bd65":"df.head()","6b6f3711":"# for col in df.columns[2:]:\n#     sns.boxplot(df[col])\n#     plt.show()","57dababc":"#make a copy for the original dataset\ndf_copy=df.copy()","f75d8181":"df_copy.columns","d96c4345":"#solution \nfor col in df_copy.columns[4:]:\n    if col == 'GDP ($ per capita)':\n        pass\n    else:\n        df_copy[col] = df_copy[col].str.replace(',','.')","6741ccfc":"#test \ndf_copy.columns","b48ebab6":"df_copy.head()","e8f79ddf":"#solution \nfor col in df_copy.columns[4:]:\n    if col == 'GDP ($ per capita)':\n        pass\n    else:\n        df_copy[col] = df_copy[col].astype('float64')","88cf64a3":"#test \ndf_copy.info()","04bfcf8d":"#fill null values with median\ndf_copy = df_copy.fillna(df_copy.median()) ","599a14bd":"#test\ndf_copy.info()","6e7ed2a1":"df_copy.duplicated().sum()","db9b8f70":"#show statistics\ndf_copy.describe()","df776db1":"for col in df_copy.columns[2:]:\n    sns.boxplot(df_copy[col])\n    plt.show()","b635cdf0":"df_copy = df_copy[df_copy['Deathrate'] < 18]\ndf_copy = df_copy[df_copy['Infant mortality (per 1000 births)']<125]\ndf_copy = df_copy[df_copy['Agriculture'] < 0.48]\ndf_copy = df_copy[df_copy['Other (%)'] > 40]\ndf_copy.info()","5dd7850d":"from sklearn.preprocessing import LabelEncoder\n\n# creating instance of labelencoder\nlabelencoder = LabelEncoder()\n# Assigning numerical values and storing in another column\ndf_copy['Country'] = labelencoder.fit_transform(df_copy['Country'])\ndf_copy['Region'] = labelencoder.fit_transform(df_copy['Region'])\ndf_copy","1895d7db":"from sklearn import preprocessing","acc47d66":"from sklearn.preprocessing import MinMaxScaler\n\ndf_copy_scaled = df_copy.copy() #scaled min max\n#cols to be scaled\ncols = df_copy.columns\nfeatures = df_copy[cols]\n#MinMax scaling\nscaler = MinMaxScaler()\ndf_copy_scaled[cols] = scaler.fit_transform(features.values)\ndf_copy_scaled","5f77b990":"import scipy.cluster.hierarchy as shc\n\nplt.figure(figsize=(10, 7))\nplt.title(\"Counters Dendograms\")\ndend = shc.dendrogram(shc.linkage(y=df_copy_scaled , method='single',metric='euclidean'),orientation='right') #fill y with your dataframe\n                                                                                      #and method with linkage criteria\n                                                                                      #and metric with distance function","46007fa7":"dend = shc.dendrogram(shc.linkage(y=df_copy_scaled , method='complete',metric='euclidean'),orientation='right')","172933f7":"dend = shc.dendrogram(shc.linkage(y=df_copy_scaled , method='average',metric='euclidean'),orientation='right')","4ec53038":"dend = shc.dendrogram(shc.linkage(y=df_copy_scaled , method='centroid',metric='euclidean'),orientation='right')","f221db0b":"#training\nfrom sklearn.cluster import AgglomerativeClustering\nclustering = AgglomerativeClustering(n_clusters=6).fit(df_copy_scaled)\nlabels = clustering.labels_\nprint(labels)","6fe07be1":"from sklearn import metrics\nmetrics.silhouette_score(df_copy_scaled, labels)","c8660436":"from sklearn.decomposition import PCA\npca = PCA(n_components=2)\nprincipalComponents = pca.fit_transform(df_copy_scaled)\nprincipalDf = pd.DataFrame(data = principalComponents , columns = ['principal component 1', 'principal component 2'])\n\nprincipalDf.head()","3b16cd29":"from sklearn.cluster import KMeans\nkmeans = KMeans(n_clusters=4, random_state=0).fit(principalDf)\nlabels = kmeans.labels_\n\nfrom sklearn import metrics\nmetrics.silhouette_score(principalDf, labels)","6afc9bf5":"from sklearn.preprocessing import MaxAbsScaler\nscaler = MaxAbsScaler()\n\ndf_copy_scaled[cols] = scaler.fit_transform(features.values)\n\nfrom sklearn.cluster import KMeans\nkmeans = KMeans(n_clusters=4, random_state=0).fit(df_copy_scaled)\nlabels = kmeans.labels_\n\nfrom sklearn import metrics\nmetrics.silhouette_score(df_copy_scaled, labels)","85375a47":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\ndf_copy_scaled[cols] = scaler.fit_transform(features.values)\ndf_copy_scaled.head()","d440bb20":"from sklearn.cluster import KMeans\nkmeans = KMeans(n_clusters=4, random_state=0).fit(principalDf)\nlabels = kmeans.labels_\n\nfrom sklearn import metrics\nmetrics.silhouette_score(principalDf, labels)","d81d22a6":"# PCA IS RECOMMENDED","10346832":"## 5. <a name=\"5\">Training and hyperparamter tuning<\/a>\n(<a href=\"#0\">Go to top<\/a>)\n","36d0e934":"1. <a href=\"#1\">Read the dataset<\/a>\n2. <a href=\"#2\">Data investigation<\/a>\n3. <a href=\"#3\">Data preprocessing <\/a>\n4. <a href=\"#4\">Features transformation <\/a>\n4. <a href=\"#5\">Training datasets<\/a>\n5. <a href=\"#6\">Improvement ideas<\/a>\n\n","ebaf7643":"# MaxAbsScaler","f986bf06":"# <a name=\"0\">Hierarchical Clustering<\/a>","f1d4731b":"Before we start the training process we need to specify 3 paramters:<br>\n1- Linkage criteria : The linkage criterion determines the distance between two clusters\n    - Complete-Linkage Clustering\n    - Single-Linkage Clustering\n    - Average-Linkage Clustering\n    - Centroid Linkage Clustering\n2- Distance function:\n    - Euclidean Distance \n    - Manhattan Distance \n    - Mahalanobis distance \n3- Number of clusters\n","b59e966b":"##  Silhouette_Score or any other evalution method ","20b6f857":"## 2. <a name=\"2\">Data investigation<\/a>\n(<a href=\"#0\">Go to top<\/a>)\n\nin this part you need to check the data quality and assess any issues in the data as:\n- null values in each column \n- each column has the proper data type\n- outliers\n- duplicate rows\n- distribution for each column (skewness)\n<br>\n\n**comment each issue you find** ","4365d756":"- Try to use PCA to reduce the number of features and compare how this will affect the clustring process\n- Try to run your code again but with different tranformation technique\n- Implement gap statistics method and use it as evaluation metric and compare the result with what you did before https:\/\/www.datanovia.com\/en\/lessons\/determining-the-optimal-number-of-clusters-3-must-know-methods\/#gap-statistic-method ","cf089098":"### for each issue adapt this methodology \n- start by defining the solution\n- apply this solution onn the data\n- test the solution to make sure that you have solved the issue","194f4c97":"**Forth issue**\n# Dealing with dupplicates","bb992021":"## 3. <a name=\"3\">Data preprocessing<\/a>\n(<a href=\"#0\">Go to top<\/a>)\n","25052e95":"# StandardScaler","2063acca":"## Lets look at outliers","be7c89f7":"### Define below all the issues that you had found in the previous part\n1-     Null values to be dealt with      <br>\n2-     Change object datatype to float datatype      <br>\n3-     Dealing with duplictes <br>\n4-     Changing comma to dot <br>","8a8dfe31":"# Lets use LabelEncoder for Country and Region","8499e6e1":"## 6. <a name=\"6\">improvement ideas<\/a>\n(<a href=\"#0\">Go to top<\/a>)","774740c3":"*What is the feature scaling technique that would use and why?* <br>\n*return to this section again and try another technique and see how that will impact your result*<br>\nfor more details on different methods for scaling check these links\n- https:\/\/scikit-learn.org\/stable\/modules\/preprocessing.html#preprocessing\n- https:\/\/scikit-learn.org\/stable\/modules\/classes.html#module-sklearn.preprocessing\n- https:\/\/www.analyticsvidhya.com\/blog\/2020\/07\/types-of-feature-transformation-and-scaling\/","585a9b1b":"# THANKS","f262baa3":"**First issue**\n# Changing comma in columns to dot for space","e4c0264e":"**Second issue**\n# Change object datatype to float datatype","2839f8c6":"### *Number of clusters*\nUse Dendograms to specify the optimum number of clusters\n- Compare how changing linkage criteria or distance function would affect the optimum number of clusters\n- you can use silhouette_score or any other evalution method to help you determine the optimum number of clusters\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.silhouette_score.html","45c457b7":"## 4. <a name=\"4\">Features transformation<\/a>\n(<a href=\"#0\">Go to top<\/a>)","6026417d":"## 1. <a name=\"1\">Read the dataset<\/a>\n(<a href=\"#0\">Go to top<\/a>)\n\nFirst dowmload the data set from this link https:\/\/www.kaggle.com\/fernandol\/countries-of-the-world\nthen import it in python.","4c7050e0":"**Third issue**\n# Null values to be dealt with"}}