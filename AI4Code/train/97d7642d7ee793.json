{"cell_type":{"db69235d":"code","d2b56652":"code","a80971b8":"code","1b4ce314":"code","79c2442b":"code","171a1d27":"code","230aa595":"code","cdd5e038":"code","d9715146":"code","b69b3cdc":"code","9a9f6e21":"code","1a670452":"code","3446d801":"code","5344d96b":"code","4902f565":"code","dc18385c":"code","38616ceb":"code","bf353cc3":"code","0124b5dc":"code","6450c2b9":"code","c94c885e":"code","25818a15":"code","09bb957d":"code","0e9f1457":"code","8ba756e3":"code","4a2ebaa8":"code","d9c82719":"code","2a11c678":"code","7927eff4":"code","63ea7364":"code","52b7f39a":"code","3e202ca2":"code","31d714b2":"markdown","934c8a96":"markdown","e8cedadf":"markdown","5bc45462":"markdown","19ae3924":"markdown","aa97c459":"markdown","3fd19e0d":"markdown","f99ca4ea":"markdown","935e37e8":"markdown","ac0231ac":"markdown","5ce313db":"markdown","bdf1962d":"markdown","7ad219f7":"markdown"},"source":{"db69235d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport re #I will use regular  expression library to get rid of unnecessary characters lik \/.-\u00bd$...\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport nltk as nlp # Main Library for NLP\nfrom nltk.corpus import stopwords # stopwords\nfrom sklearn.feature_extraction.text import CountVectorizer # to create sparce matrix\nfrom sklearn.model_selection import train_test_split # train-test split to model\nfrom sklearn.naive_bayes import GaussianNB # for the model\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d2b56652":"fake_data=pd.read_csv(\"..\/input\/fake-and-real-news-dataset\/Fake.csv\")\ntrue_data=pd.read_csv(\"..\/input\/fake-and-real-news-dataset\/True.csv\")","a80971b8":"fake_data.head()","1b4ce314":"true_data.head()","79c2442b":"true_news=pd.concat([true_data.text],axis=1)\nfake_news=pd.concat([fake_data.text],axis=1)","171a1d27":"true_news.head()","230aa595":"fake_news.head()","cdd5e038":"fake_news.shape","d9715146":"true_news.shape","b69b3cdc":"fake_news.dtypes","9a9f6e21":"true_news.dtypes","1a670452":"true_news[\"status\"]=\"true\"\nfake_news[\"status\"]=\"fake\"","3446d801":"true_news.head()","5344d96b":"df=pd.concat([true_news,fake_news],axis=0,ignore_index=True)","4902f565":"df.head()","dc18385c":"df.tail()","38616ceb":"df.status=[1 if status==\"fake\" else 0 for status in df.status]","bf353cc3":"df.tail()","0124b5dc":"df.head()","6450c2b9":"df = df.sample(frac = 1)","c94c885e":"df.head()","25818a15":"df.isna().sum()","09bb957d":"df.isnull().sum()","0e9f1457":"plt.figure(figsize=(10,5))\nsns.countplot(\"status\",data=df)\nplt.title(\"distribution of news\")\nplt.grid()","8ba756e3":"nlp.download(\"stopwords\") # in order to extract stopwords\nlemma=nlp.WordNetLemmatizer()#for Lemmatisation","4a2ebaa8":"data=df.sample(n=10000)","d9c82719":"text_list=[]\nfor text in data.text:\n    text=re.sub(\"[^a-zA-Z]\",\" \",text) # extracting unnecesary characters\n    text=text.lower() #makes characters lowercase\n    text=nlp.word_tokenize(text) # splits all the words\n    text=[word for word in text if not word in set(stopwords.words(\"english\"))] # extract stopwords\n    text=[lemma.lemmatize(word) for word in text] # Lemmatisation\n    text=\" \".join(text) \n    text_list.append(text)","2a11c678":"max_features=10000\ncount_vectorizer=CountVectorizer(max_features=max_features,stop_words=\"english\")\nsparce_matrix=count_vectorizer.fit_transform(text_list).toarray()","7927eff4":"y=data.iloc[:,1]\nx=sparce_matrix","63ea7364":"x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.1,random_state=5)","52b7f39a":"nb=GaussianNB()\nnb.fit(x_train,y_train)\ny_pred= nb.predict(x_test)\n","3e202ca2":"print(\"accuracy\",nb.score(y_pred.reshape(-1,1),y_test))","31d714b2":"Fake and true news distributed equally which is good for us.","934c8a96":"**Fake News Detector**\n\nHello everyone\nI will create a model that will detect if a new is fake or not. I am new at NLP so that will be my first project. Your feedbacks are very important for me, I am waiting for your comments to see my faults,lacks and diffusiveness.","e8cedadf":"I only took 5000 samples out of 45000 samples since my computer is not powerfull enough, when I try more than this number I have to wait hours. If you have better computer you can try.","5bc45462":"I created a status column that will act as Dependent Variable for the project.","19ae3924":"Count Vectorizer is a modul that turns out texts into matrices. It fits words into a matrix, if that a text contain that word that place turns into 1, if it doesn't it is 0. This is how we can train our model.","aa97c459":"**Cleaning Data**","3fd19e0d":"Accuracy is 88% it is enough but with bigger sample it would be more.","f99ca4ea":"**Modelling**","935e37e8":"stopwords are the words that has no effect on the subject, to give an example I can say \"or,on,and,the\".\n\nLemmatisation is the process that takes root of a word, it is important because model without Lemmatisation features will be really high and model will be executed in a long time and without Lemmatisation for the word that will used for same purpose can be read different by the model.","ac0231ac":"**Reading the Data**","5ce313db":"We just need text column for this project.","bdf1962d":"I just took text column for the dataset.","7ad219f7":"I wanted to shuffle the dataset rows considering if a need to take some portion from data."}}