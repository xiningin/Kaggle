{"cell_type":{"5f7c4336":"code","a655d5fb":"code","f8b190ad":"code","efa73f14":"code","cd1c7725":"code","5bd10656":"code","b167adc8":"code","6ce3fd80":"code","e7c8a13c":"code","05c2392b":"code","0e080929":"code","2b6c753f":"code","d091ef5c":"code","7562f39a":"code","3e179000":"code","6961e391":"code","ee475e55":"code","70c1b46c":"code","5260d8f5":"code","da10326e":"code","271deefb":"code","6e506b36":"code","c554fe8f":"code","029f996f":"code","7c91107c":"code","b65a2cca":"code","71c3333f":"code","e3abee4f":"code","081d1741":"code","530cac6a":"code","842798c7":"code","057b2833":"code","ec6537cc":"code","3bf56452":"code","798d847d":"code","dd790eb9":"code","2e74de0b":"code","001df4f8":"code","81fb855e":"code","04ad809a":"code","cc70dc8c":"code","fededc41":"code","4473f539":"code","7060e3e4":"code","0c524d62":"code","4337b2c5":"code","8fa7a348":"code","943fb211":"code","e0297e69":"code","5ab81d17":"code","66d0633f":"code","a2cf749e":"code","64d6583b":"code","b7a54a95":"code","c638b7f2":"code","193c0414":"code","e5125590":"code","60e54129":"code","84f01b9d":"code","b702a88f":"code","393a5af4":"code","a129e098":"code","0fcb819b":"code","b2c2336c":"code","7b218e54":"code","6a804430":"code","0eb91f3f":"code","41ebf1e1":"code","f7056bc3":"code","afcba4f8":"code","44df0810":"code","b2029c4e":"code","5e7faa21":"code","645b6541":"code","8a8d70b4":"code","80b36756":"code","77a25e74":"code","b5fca7de":"code","725f2241":"code","54e4c9ed":"code","5133c93a":"code","96a3064f":"code","d30f363e":"code","d976fc79":"code","7696127f":"code","7a6d8aed":"code","bc722b0b":"code","605d2dcd":"code","4387b87b":"code","fdf53f13":"code","9725fb1f":"code","10af4b58":"code","0452d74c":"code","c5cfb8e7":"code","c76aa263":"code","bab08b7f":"code","c4abed29":"code","5a28280f":"code","ec72a8a0":"code","770731af":"code","49cbe5ae":"markdown","a16b40c7":"markdown","183c1419":"markdown","a9e8e3d3":"markdown","aa665398":"markdown","8f40a444":"markdown","bb4d485c":"markdown","f3b88d8e":"markdown","e3587f29":"markdown","66a31355":"markdown","6101e08a":"markdown","db7df28c":"markdown","cb30b05e":"markdown","6e187377":"markdown","b7db81c4":"markdown","3f08318e":"markdown","07acacc1":"markdown","cadfa05d":"markdown","899d1cc9":"markdown"},"source":{"5f7c4336":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a655d5fb":"%matplotlib inline\n\n# Start Python Imports\nimport math, time, random, datetime\n\n# Data Manipulation\nimport numpy as np\nimport pandas as pd\n\n# Visualization \nimport matplotlib.pyplot as plt\nimport missingno\nimport seaborn as sns\nplt.style.use('seaborn-whitegrid')\n\n# Preprocessing\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, label_binarize\n\n# Machine learning\nimport catboost\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import model_selection, tree, preprocessing, metrics, linear_model\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LinearRegression, LogisticRegression, SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom catboost import CatBoostClassifier, Pool, cv\n\n# Let's be rebels and ignore warnings for now\nimport warnings\nwarnings.filterwarnings('ignore')\n","f8b190ad":"tr=pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest=pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ngender_sb=pd.read_csv('\/kaggle\/input\/titanic\/gender_submission.csv')\ntr.head()","efa73f14":"tr.Age.plot.hist()","cd1c7725":"test.head()","5bd10656":"gender_sb.head()","b167adc8":"tr.describe()","6ce3fd80":"missingno.matrix(tr, figsize = (30,10))","e7c8a13c":"tr.isnull().sum()","05c2392b":"df_bin = pd.DataFrame() # for discretised continuous variables\ndf_con = pd.DataFrame() # for continuous variables\n","0e080929":"tr.dtypes","2b6c753f":"fig = plt.figure(figsize=(20,1))\nsns.countplot(y='Survived', data=tr);\nprint(tr.Survived.value_counts())","d091ef5c":"df_bin['Survived'] = tr['Survived']\ndf_con['Survived'] = tr['Survived']","7562f39a":"df_bin.head()","3e179000":"df_con.head()","6961e391":"sns.distplot(tr.Pclass)","ee475e55":"tr.Pclass.isnull().sum()","70c1b46c":"df_bin['Pclass'] = tr['Pclass']\ndf_con['Pclass'] = tr['Pclass']","5260d8f5":"tr.Name.value_counts()","da10326e":"plt.figure(figsize=(20, 5))\nsns.countplot(y=\"Sex\", data=tr);","271deefb":"tr.Sex.isnull().sum()","6e506b36":"tr.Sex.head()","c554fe8f":"df_bin['Sex'] = tr['Sex']\ndf_bin['Sex'] = np.where(df_bin['Sex'] == 'female', 1, 0) # change sex to 0 for male and 1 for female\n\ndf_con['Sex'] = tr['Sex']","029f996f":"fig = plt.figure(figsize=(10, 10))\nsns.distplot(df_bin.loc[df_bin['Survived'] == 1]['Sex'], kde_kws={'label': 'Survived'});\nsns.distplot(df_bin.loc[df_bin['Survived'] == 0]['Sex'], kde_kws={'label': 'Did not survive'});","7c91107c":"tr.Age.isnull().sum()","b65a2cca":"tr['Age']=tr['Age'].fillna(tr['Age'].mean())","71c3333f":"tr.Age.isnull().sum()","e3abee4f":"# Once the Age values have been fixed up, we can add them to our sub dataframes.\ndf_bin['Age'] = pd.cut(tr['Age'], 10) # bucketed\/binned into different categories\ndf_con['Age'] = tr['Age'] # non-bucketed","081d1741":"def plot_count_dist(data, bin_df, label_column, target_column, figsize=(20, 5), use_bin_df=False):\n    \"\"\"\n    Function to plot counts and distributions of a label variable and \n    target variable side by side.\n    ::param_data:: = target dataframe\n    ::param_bin_df:: = binned dataframe for countplot\n    ::param_label_column:: = binary labelled column\n    ::param_target_column:: = column you want to view counts and distributions\n    ::param_figsize:: = size of figure (width, height)\n    ::param_use_bin_df:: = whether or not to use the bin_df, default False\n    \"\"\"\n    if use_bin_df: \n        fig = plt.figure(figsize=figsize)\n        plt.subplot(1, 2, 1)\n        sns.countplot(y=target_column, data=bin_df);\n        plt.subplot(1, 2, 2)\n        sns.distplot(data.loc[data[label_column] == 1][target_column], \n                     kde_kws={\"label\": \"Survived\"});\n        sns.distplot(data.loc[data[label_column] == 0][target_column], \n                     kde_kws={\"label\": \"Did not survive\"});\n    else:\n        fig = plt.figure(figsize=figsize)\n        plt.subplot(1, 2, 1)\n        sns.countplot(y=target_column, data=data);\n        plt.subplot(1, 2, 2)\n        sns.distplot(data.loc[data[label_column] == 1][target_column], \n                     kde_kws={\"label\": \"Survived\"});\n        sns.distplot(data.loc[data[label_column] == 0][target_column], \n                     kde_kws={\"label\": \"Did not survive\"});","530cac6a":"tr.SibSp.isnull().sum()","842798c7":"tr.SibSp.value_counts()","057b2833":"# Add SibSp to subset dataframes\ndf_bin['SibSp'] = tr['SibSp']\ndf_con['SibSp'] = tr['SibSp']","ec6537cc":"# Visualise the counts of SibSp and the distribution of the values\n# against Survived\nplot_count_dist(tr, \n                bin_df=df_bin, \n                label_column='Survived', \n                target_column='SibSp', \n                figsize=(20, 10))","3bf56452":"tr.Parch.isnull().sum()","798d847d":"tr.Parch.value_counts()","dd790eb9":"df_bin['Parch'] = tr['Parch']\ndf_con['Parch'] = tr['Parch']","2e74de0b":"# Visualise the counts of Parch and the distribution of the values\n# against Survived\nplot_count_dist(tr, \n                bin_df=df_bin,\n                label_column='Survived', \n                target_column='Parch', \n                figsize=(20, 10))","001df4f8":"df_con.head()","81fb855e":"tr.Ticket.isnull().sum()","04ad809a":"sns.countplot(y=\"Ticket\", data=tr);","cc70dc8c":"# How many kinds of ticket are there?\ntr.Ticket.value_counts()","fededc41":"# How many unique kinds of Ticket are there?\nprint(\"There are {} unique Ticket values.\".format(len(tr.Ticket.unique())))","4473f539":"# How many missing values does Fare have?\ntr.Fare.isnull().sum()","7060e3e4":"# How many different values of Fare are there?\nsns.countplot(y=\"Fare\", data=tr);","0c524d62":"# What kind of variable is Fare?\ntr.Fare.dtype","4337b2c5":"# How many unique kinds of Fare are there?\nprint(\"There are {} unique Fare values.\".format(len(tr.Fare.unique())))","8fa7a348":"# Add Fare to sub dataframes\ndf_con['Fare'] = tr['Fare'] \ndf_bin['Fare'] = pd.cut(tr['Fare'], bins=6) # discretised","943fb211":"# What do our Fare bins look like?\ndf_bin.Fare.value_counts()","e0297e69":"# Visualise the Fare bin counts as well as the Fare distribution versus Survived.\nplot_count_dist(data=tr,\n                bin_df=df_bin,\n                label_column='Survived', \n                target_column='Fare', \n                figsize=(20,10), \n                use_bin_df=True)","5ab81d17":"tr.Cabin.isnull().sum()","66d0633f":"tr.Cabin.value_counts()","a2cf749e":"tr.Embarked.isnull().sum()","64d6583b":"tr.Embarked.value_counts()","b7a54a95":"# What do the counts look like?\nsns.countplot(y='Embarked', data=tr);","c638b7f2":"# Add Embarked to sub dataframes\ndf_bin['Embarked'] = tr['Embarked']\ndf_con['Embarked'] = tr['Embarked']","193c0414":"print(len(df_con))\ndf_con = df_con.dropna(subset=['Embarked'])\ndf_bin = df_bin.dropna(subset=['Embarked'])\nprint(len(df_con))","e5125590":"df_bin.head()","60e54129":"df_con.head()","84f01b9d":"# One-hot encode binned variables\none_hot_cols = df_bin.columns.tolist()\none_hot_cols.remove('Survived')\ndf_bin_enc = pd.get_dummies(df_bin, columns=one_hot_cols)\n\ndf_bin_enc.head()","b702a88f":"# One hot encode the categorical columns\ndf_embarked_one_hot = pd.get_dummies(df_con['Embarked'], \n                                     prefix='embarked')\n\ndf_sex_one_hot = pd.get_dummies(df_con['Sex'], \n                                prefix='sex')\n\ndf_plcass_one_hot = pd.get_dummies(df_con['Pclass'], \n                                   prefix='pclass')","393a5af4":"# Combine the one hot encoded columns with df_con_enc\ndf_con_enc = pd.concat([df_con, \n                        df_embarked_one_hot, \n                        df_sex_one_hot, \n                        df_plcass_one_hot], axis=1)\n\n# Drop the original categorical columns (because now they've been one hot encoded)\ndf_con_enc = df_con_enc.drop(['Pclass', 'Sex', 'Embarked'], axis=1)","a129e098":"# Let's look at df_con_enc\ndf_con_enc.head(20)","0fcb819b":"# Seclect the dataframe we want to use first for predictions\nselected_df = df_con_enc","b2c2336c":"selected_df.head()","7b218e54":"# Split the dataframe into data and labels\nX_train = selected_df.drop('Survived', axis=1) # data\ny_train = selected_df.Survived # labels","6a804430":"X_train.shape","0eb91f3f":"X_train.head()","41ebf1e1":"y_train.shape","f7056bc3":"# Function that runs the requested algorithm and returns the accuracy metrics\ndef fit_ml_algo(algo, X_train, y_train, cv):\n    \n    # One Pass\n    model = algo.fit(X_train, y_train)\n    acc = round(model.score(X_train, y_train) * 100, 2)\n    \n    # Cross Validation \n    train_pred = model_selection.cross_val_predict(algo, \n                                                  X_train, \n                                                  y_train, \n                                                  cv=cv, \n                                                  n_jobs = -1)\n    # Cross-validation accuracy metric\n    acc_cv = round(metrics.accuracy_score(y_train, train_pred) * 100, 2)\n    \n    return train_pred, acc, acc_cv","afcba4f8":"# Logistic Regression\nstart_time = time.time()\ntrain_pred_log, acc_log, acc_cv_log = fit_ml_algo(LogisticRegression(), \n                                                               X_train, \n                                                               y_train, \n                                                                    10)\nlog_time = (time.time() - start_time)\nprint(\"Accuracy: %s\" % acc_log)\nprint(\"Accuracy CV 10-Fold: %s\" % acc_cv_log)\nprint(\"Running Time: %s\" % datetime.timedelta(seconds=log_time))","44df0810":"# k-Nearest Neighbours\nstart_time = time.time()\ntrain_pred_knn, acc_knn, acc_cv_knn = fit_ml_algo(KNeighborsClassifier(), \n                                                  X_train, \n                                                  y_train, \n                                                  10)\nknn_time = (time.time() - start_time)\nprint(\"Accuracy: %s\" % acc_knn)\nprint(\"Accuracy CV 10-Fold: %s\" % acc_cv_knn)\nprint(\"Running Time: %s\" % datetime.timedelta(seconds=knn_time))","b2029c4e":"# Gaussian Naive Bayes\nstart_time = time.time()\ntrain_pred_gaussian, acc_gaussian, acc_cv_gaussian = fit_ml_algo(GaussianNB(), \n                                                                      X_train, \n                                                                      y_train, \n                                                                           10)\ngaussian_time = (time.time() - start_time)\nprint(\"Accuracy: %s\" % acc_gaussian)\nprint(\"Accuracy CV 10-Fold: %s\" % acc_cv_gaussian)\nprint(\"Running Time: %s\" % datetime.timedelta(seconds=gaussian_time))","5e7faa21":"# Linear SVC\nstart_time = time.time()\ntrain_pred_svc, acc_linear_svc, acc_cv_linear_svc = fit_ml_algo(LinearSVC(),\n                                                                X_train, \n                                                                y_train, \n                                                                10)\nlinear_svc_time = (time.time() - start_time)\nprint(\"Accuracy: %s\" % acc_linear_svc)\nprint(\"Accuracy CV 10-Fold: %s\" % acc_cv_linear_svc)\nprint(\"Running Time: %s\" % datetime.timedelta(seconds=linear_svc_time))","645b6541":"# Stochastic Gradient Descent\nstart_time = time.time()\ntrain_pred_sgd, acc_sgd, acc_cv_sgd = fit_ml_algo(SGDClassifier(), \n                                                  X_train, \n                                                  y_train,\n                                                  10)\nsgd_time = (time.time() - start_time)\nprint(\"Accuracy: %s\" % acc_sgd)\nprint(\"Accuracy CV 10-Fold: %s\" % acc_cv_sgd)\nprint(\"Running Time: %s\" % datetime.timedelta(seconds=sgd_time))","8a8d70b4":"# Decision Tree Classifier\nstart_time = time.time()\ntrain_pred_dt, acc_dt, acc_cv_dt = fit_ml_algo(DecisionTreeClassifier(), \n                                                                X_train, \n                                                                y_train,\n                                                                10)\ndt_time = (time.time() - start_time)\nprint(\"Accuracy: %s\" % acc_dt)\nprint(\"Accuracy CV 10-Fold: %s\" % acc_cv_dt)\nprint(\"Running Time: %s\" % datetime.timedelta(seconds=dt_time))","80b36756":"# Gradient Boosting Trees\nstart_time = time.time()\ntrain_pred_gbt, acc_gbt, acc_cv_gbt = fit_ml_algo(GradientBoostingClassifier(), \n                                                                       X_train, \n                                                                       y_train,\n                                                                       10)\ngbt_time = (time.time() - start_time)\nprint(\"Accuracy: %s\" % acc_gbt)\nprint(\"Accuracy CV 10-Fold: %s\" % acc_cv_gbt)\nprint(\"Running Time: %s\" % datetime.timedelta(seconds=gbt_time))","77a25e74":"# Define the categorical features for the CatBoost model\ncat_features = np.where(X_train.dtypes != np.float)[0]\ncat_features","b5fca7de":"# Use the CatBoost Pool() function to pool together the training data and categorical feature labels\ntrain_pool = Pool(X_train, \n                  y_train,\n                  cat_features)","725f2241":"y_train.head()","54e4c9ed":"# CatBoost model definition\ncatboost_model = CatBoostClassifier(iterations=1000,\n                                    custom_loss=['Accuracy'],\n                                    loss_function='Logloss')\n\n# Fit CatBoost model\ncatboost_model.fit(train_pool,\n                   plot=True)\n\n# CatBoost accuracy\nacc_catboost = round(catboost_model.score(X_train, y_train) * 100, 2)","5133c93a":"# How long will this take?\nstart_time = time.time()\n\n# Set params for cross-validation as same as initial model\ncv_params = catboost_model.get_params()\n\n# Run the cross-validation for 10-folds (same as the other models)\ncv_data = cv(train_pool,\n             cv_params,\n             fold_count=10,\n             plot=True)\n\n# How long did it take?\ncatboost_time = (time.time() - start_time)\n\n# CatBoost CV results save into a dataframe (cv_data), let's withdraw the maximum accuracy score\nacc_cv_catboost = round(np.max(cv_data['test-Accuracy-mean']) * 100, 2)","96a3064f":"# Print out the CatBoost model metrics\nprint(\"---CatBoost Metrics---\")\nprint(\"Accuracy: {}\".format(acc_catboost))\nprint(\"Accuracy cross-validation 10-Fold: {}\".format(acc_cv_catboost))\nprint(\"Running Time: {}\".format(datetime.timedelta(seconds=catboost_time)))","d30f363e":"models = pd.DataFrame({\n    'Model': ['KNN', 'Logistic Regression', 'Naive Bayes', \n              'Stochastic Gradient Decent', 'Linear SVC', \n              'Decision Tree', 'Gradient Boosting Trees',\n              'CatBoost'],\n    'Score': [\n        acc_knn, \n        acc_log,  \n        acc_gaussian, \n        acc_sgd, \n        acc_linear_svc, \n        acc_dt,\n        acc_gbt,\n        acc_catboost\n    ]})\nprint(\"---Reuglar Accuracy Scores---\")\nmodels.sort_values(by='Score', ascending=False)","d976fc79":"cv_models = pd.DataFrame({\n    'Model': ['KNN', 'Logistic Regression', 'Naive Bayes', \n              'Stochastic Gradient Decent', 'Linear SVC', \n              'Decision Tree', 'Gradient Boosting Trees',\n              'CatBoost'],\n    'Score': [\n        acc_cv_knn, \n        acc_cv_log,      \n        acc_cv_gaussian, \n        acc_cv_sgd, \n        acc_cv_linear_svc, \n        acc_cv_dt,\n        acc_cv_gbt,\n        acc_cv_catboost\n    ]})\nprint('---Cross-validation Accuracy Scores---')\ncv_models.sort_values(by='Score', ascending=False)","7696127f":"# Feature Importance\ndef feature_importance(model, data):\n    \"\"\"\n    Function to show which features are most important in the model.\n    ::param_model:: Which model to use?\n    ::param_data:: What data to use?\n    \"\"\"\n    fea_imp = pd.DataFrame({'imp': model.feature_importances_, 'col': data.columns})\n    fea_imp = fea_imp.sort_values(['imp', 'col'], ascending=[True, False]).iloc[-30:]\n    _ = fea_imp.plot(kind='barh', x='col', y='imp', figsize=(20, 10))\n    return fea_imp\n    #plt.savefig('catboost_feature_importance.png')","7a6d8aed":"# Plot the feature importance scores\nfeature_importance(catboost_model, X_train)","bc722b0b":"metrics = ['Precision', 'Recall', 'F1', 'AUC']\n\neval_metrics = catboost_model.eval_metrics(train_pool,\n                                           metrics=metrics,\n                                           plot=True)\n\nfor metric in metrics:\n    print(str(metric)+\": {}\".format(np.mean(eval_metrics[metric])))","605d2dcd":"# We need our test dataframe to look like this one\nX_train.head()","4387b87b":"# Our test dataframe has some columns our model hasn't been trained on\ntest.head()","fdf53f13":"# One hot encode the columns in the test data frame (like X_train)\ntest_embarked_one_hot = pd.get_dummies(test['Embarked'], \n                                       prefix='embarked')\n\ntest_sex_one_hot = pd.get_dummies(test['Sex'], \n                                prefix='sex')\n\ntest_plcass_one_hot = pd.get_dummies(test['Pclass'], \n                                   prefix='pclass')","9725fb1f":"# Combine the test one hot encoded columns with test\ntest = pd.concat([test, \n                  test_embarked_one_hot, \n                  test_sex_one_hot, \n                  test_plcass_one_hot], axis=1)","10af4b58":"# Let's look at test, it should have one hot encoded columns now\ntest.head()","0452d74c":"# Create a list of columns to be used for the predictions\nwanted_test_columns = X_train.columns\nwanted_test_columns","c5cfb8e7":"# Make a prediction using the CatBoost model on the wanted columns\npredictions = catboost_model.predict(test[wanted_test_columns])","c76aa263":"# Our predictions array is comprised of 0's and 1's (Survived or Did Not Survive)\npredictions[:20]","bab08b7f":"# Create a submisison dataframe and append the relevant columns\nsubmission = pd.DataFrame()\nsubmission['PassengerId'] = test['PassengerId']\nsubmission['Survived'] = predictions # our model predictions on the test dataset\nsubmission.head()","c4abed29":"# Let's convert our submission dataframe 'Survived' column to ints\nsubmission['Survived'] = submission['Survived'].astype(int)\nprint('Converted Survived column to integers.')","5a28280f":"# How does our submission dataframe look?\nsubmission.head()","ec72a8a0":"# Are our test and submission dataframes the same length?\nif len(submission) == len(test):\n    print(\"Submission dataframe is the same length as test ({} rows).\".format(len(submission)))\nelse:\n    print(\"Dataframes mismatched, won't be able to submit to Kaggle.\")","770731af":"# Check the submission csv to make sure it's in the right format\nsubmissions_check = pd.read_csv(\"\/output\/kaggle\/working\/catboost_info.csv\")\nsubmissions_check.head()","49cbe5ae":"### Precision & Recall","a16b40c7":"**Function to create count and distribution visualisations******","183c1419":"> **Model Results\nWhich model had the best cross-validation accuracy?\n\nNote: We care most about cross-validation metrics because the metrics we get from .fit() can randomly score higher than usual.**","a9e8e3d3":"**Stochastic Gradient Descent","aa665398":"Define a function to fit machine learning algorithms","8f40a444":"**Why would you want feature importance?\nFeatrue importance shows how much each feature contributed to the model.\n\nYou could take this information and remove features which don't contribute much to reduce dimenstionality (and save compute).\n\nYou could improve features which don't offer much to the overall model.\n\nOr you could improve features which offer more to the model. In this case, there aren't many ways you could improve sex as it's already a binary.\n\nFeature importance figures also show people who may not be familiar with the problem what features of their data are most important when it comes to making predictions with machine learning models.**","bb4d485c":"**CatBoost Algorithm**","f3b88d8e":"### Feature Importance","e3587f29":"**Gaussian Naive Bayes**","66a31355":"**Gradient Boost Trees**","6101e08a":"**Linear Support Vector Machines (SVC)**","db7df28c":"**Perform CatBoost cross-validation**","cb30b05e":"CatBoost is a state-of-the-art open-source gradient boosting on decision trees library.It deal with Categorical Variable.","6e187377":"**Decision Tree Classifier**","b7db81c4":"**Logistic Regression**","3f08318e":"### Regular accuracy scores****","07acacc1":"**K-Nearest Neighbours******","cadfa05d":"**Recall = a metric which measures a models ability to find all the relevant cases in a dataset.\n\nRecall would be the models ability to find the 1 person in 100,000 who has the disease.\n\nPrecision = a metric which measures a models ability to correctly identify only relevant instances.\n\nIn our example, Precision would be if the model found the 1 person who had the disease, did they actually have the disease.\n\nCombining the precision and recall, gives an F1 score.\n\nThese metrics will all fall between 0 and 1, with a higher value being better.**","899d1cc9":"### Feature Importance\nWhich features of the best model were most important for making predictions?"}}