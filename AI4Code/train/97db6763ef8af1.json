{"cell_type":{"3e062a94":"code","12964e00":"code","cc62c8e0":"code","9581628a":"code","5aa2fbd5":"code","c99809e7":"code","095176a5":"code","72487b5d":"code","1fd87bfd":"code","2f0c1d52":"code","ee45828e":"code","c0aa88a0":"code","ac32ab88":"code","fc963a52":"code","029454ed":"code","6e02dc03":"code","44cacf54":"code","5703894d":"code","902984c4":"code","591b96b0":"code","bcc4345f":"code","b596b4a7":"code","be1cecf1":"code","56473b9f":"code","d0e42edf":"code","10023347":"code","d5ecd976":"code","be7635fa":"code","848a2590":"code","ced3d59a":"code","f3598d7a":"code","8fe109c2":"code","a65adcbc":"code","04b3142d":"code","babf607c":"code","81b9e8a4":"code","59d802c6":"code","8fb75d6e":"code","cca25de4":"code","de1332de":"code","a14be8e5":"code","51181b22":"code","879ba5f8":"code","1f4b6079":"code","84afa530":"code","3c5899f4":"code","17995b8a":"code","b1f156ba":"code","2dbbe313":"code","b918ffa8":"code","cce35064":"code","50f9f18b":"code","42bf15aa":"code","0aa0ff6d":"code","972e8ce0":"code","20c91afc":"code","92520377":"code","e4b2eae2":"code","e6e2e2d0":"code","ce9c025d":"code","beaec77e":"code","2dead2d0":"code","7ab39653":"code","bf9379af":"code","a463e6d0":"code","9ff93bfd":"code","41c5927f":"code","301b88bb":"code","e466f005":"code","a2b0d374":"code","a37ac4f4":"code","7780954d":"code","9d95c466":"code","afa13e99":"code","a0cc2b17":"code","d962a3d7":"code","3436bfef":"code","09f62ca0":"code","76cc3c4d":"code","cebd9d35":"code","ff45ee0b":"code","87408475":"code","4b0a253e":"code","54f62895":"code","04e535af":"code","21f9c8e0":"code","fdfa6af3":"code","9f10a619":"markdown","ddc87f4d":"markdown","86794548":"markdown","4be7c0de":"markdown","1a6ec142":"markdown","e98e9385":"markdown","3867aa46":"markdown","04c952b2":"markdown","b60c6c18":"markdown","49904f39":"markdown","1673071d":"markdown","7d2f0e46":"markdown","d75c67a9":"markdown","b2784094":"markdown","f94142e3":"markdown","a24fdaf9":"markdown","193fce61":"markdown","c86620a7":"markdown","c2048733":"markdown","72f04d99":"markdown","91c7d464":"markdown","832250c3":"markdown","8d1e72d7":"markdown","0a70f469":"markdown","a57c86bf":"markdown","6c3e180e":"markdown"},"source":{"3e062a94":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport datetime as dt \nfrom sklearn.cluster import KMeans","12964e00":"data = pd.read_csv('\/kaggle\/input\/ecommerce-data\/data.csv', encoding = 'unicode_escape')\n","cc62c8e0":"data.head()","9581628a":"data.describe()","5aa2fbd5":"data.info()","c99809e7":"data.isnull().sum()","095176a5":"pd.to_datetime(data.InvoiceDate.max())-pd.to_datetime(data.InvoiceDate.min()) ","72487b5d":"data.duplicated().sum()","1fd87bfd":"data.drop_duplicates(inplace = True)","2f0c1d52":"data.duplicated().sum()","ee45828e":"data = data.loc[(data.CustomerID.isnull()==False) & (data.Description.isnull()==False)].copy()","c0aa88a0":"data.isnull().sum()","ac32ab88":"data.info()","fc963a52":"data.min()","029454ed":"data.max()","6e02dc03":"data.nunique()","44cacf54":"data['Quantity'][data['Quantity'] < 0].nunique()","5703894d":"data = data[data['Quantity'] > 0]","902984c4":"data.min()","591b96b0":"data.Quantity.describe()","bcc4345f":"data.head()","b596b4a7":"data.describe()","be1cecf1":"data['Sales'] = data['Quantity'] * data['UnitPrice']\ndata[:5]","56473b9f":"data[data['InvoiceNo'].str.startswith('c')]","d0e42edf":"data['Sales'].describe()","10023347":"print('Duplicate invoice = ',data['InvoiceNo'].duplicated().sum())\nprint('Unique invoce = ',data['InvoiceNo'].nunique())","d5ecd976":"print('Unique Values :- ')\nprint('Country : ',data['Country'].nunique())\nprint('Quantity : ',data['Quantity'].nunique())\nprint('Items : ',data['Description'].nunique())","be7635fa":"print(\"Most Occured :- \")\nprint('Country = ', data['Country'].mode()[0])\nprint('Description = ', data['Description'].mode()[0])","848a2590":"data.groupby(['Country']).sum().head()","ced3d59a":"data['InvoiceDate'] = pd.to_datetime(data.InvoiceDate, format='%m\/%d\/%Y %H:%M')","f3598d7a":"data.insert(loc=4, column='Day', value=data.InvoiceDate.dt.day)\ndata.insert( loc = 5,column='Month', value=data.InvoiceDate.dt.month)\ndata.insert( loc = 6,column='Year', value=data.InvoiceDate.dt.year)\ndata.insert( loc = 7,column='WeekDay', value=data.InvoiceDate.dt.weekday)\ndata.insert( loc = 8,column='Hour', value=data.InvoiceDate.dt.hour)\ndata.insert( loc = 9,column='Minute', value=data.InvoiceDate.dt.minute)\ndata.insert( loc = 10,column='Date', value=data.InvoiceDate.dt.date)\n","8fe109c2":"data.head()","a65adcbc":"sns.catplot(data=data, x= 'Month', kind = 'count')\nplt.title('month vs orders')","04b3142d":"sns.catplot(data=data, x= 'Month', y='Sales', kind = 'bar')\nplt.title('Month wise Sales ')","babf607c":"sns.catplot(data=data, x= 'WeekDay', y='Sales', kind = 'bar')\nplt.title('Sales vs WeekDay ')\n# Monday = 0 to Sunday = 6","81b9e8a4":"data['InvoiceNo'].value_counts().head(10)","59d802c6":"data['CustomerID'].value_counts().head(10)","8fb75d6e":"data['StockCode'].value_counts().head()","cca25de4":"plt.figure(figsize=(15,8))\n#sns.countplot(data['Country'])\nsns.countplot(data[data['Country'] != 'United Kingdom']['Country'] , order = data[data['Country'] != 'United Kingdom']['Country'].value_counts().index)\n\nplt.xticks(rotation=90)\nplt.title('Order Count Abroad (Outside UK) ')","de1332de":"descrip_count =  data.Description.value_counts().sort_values(ascending=False).iloc[0:15]","a14be8e5":"plt.figure(figsize=(15,8))\nsns.barplot(y = descrip_count.values, x=descrip_count.index )\nplt.xticks(rotation=90)\nplt.title('Top 10 Products ')","51181b22":"sns.catplot(data=data, x = 'Hour', kind = 'count')\nplt.title('Order count wrt Hour')","879ba5f8":"data['InvoiceDate'].max()","1f4b6079":"now = dt.date(2011,12,9) ","84afa530":"new_df = data.groupby(by='CustomerID', as_index=False)['Date'].max()\nnew_df.columns = ['CustomerID', 'LastPurchaseDate']\nnew_df[:5]","3c5899f4":"new_df['Recency'] =  new_df['LastPurchaseDate'].apply(lambda x : (now-x).days)\nnew_df.drop('LastPurchaseDate',axis = 1, inplace = True)\nnew_df[:5]","17995b8a":"new_df2 = data.groupby(by = 'CustomerID', as_index=False)['InvoiceNo'].count()\nnew_df2.columns = ['CustomerID','Frequency']\nnew_df2[:4]","b1f156ba":"new_df3 = data.groupby(by='CustomerID',as_index=False).agg({'Sales': 'sum'})\nnew_df3.columns = ['CustomerID','Monetary']\nnew_df3[:4]","2dbbe313":"temp = new_df.merge(new_df2, on = 'CustomerID')\nrfm_df = temp.merge(new_df3, on = 'CustomerID')\nrfm_df.set_index('CustomerID',inplace = True)\nrfm_df.head()","b918ffa8":"rfm_df['R_quartile'] = pd.qcut(rfm_df['Recency'], 4, ['1','2','3','4'])\nrfm_df['F_quartile'] = pd.qcut(rfm_df['Frequency'], 4, ['4','3','2','1'])\nrfm_df['M_quartile'] = pd.qcut(rfm_df['Monetary'], 4, ['4','3','2','1'])\nrfm_df.head()","cce35064":"rfm_df['RFM_Score'] = rfm_df.R_quartile.astype(str)+ rfm_df.F_quartile.astype(str) + rfm_df.M_quartile.astype(str)\nrfm_df.head()","50f9f18b":"rfm_df[rfm_df['RFM_Score']==str(111)].head()","42bf15aa":"rfm_df[rfm_df['F_quartile']=='1'].head()","0aa0ff6d":"rfm_df[rfm_df['M_quartile']=='1'].head()","972e8ce0":"rfm_df[rfm_df['RFM_Score']==str(444)].head()","20c91afc":"rfm_df[rfm_df['RFM_Score']==str(111)].shape","92520377":"temp2 = rfm_df[rfm_df['RFM_Score']==str(111)]\ntemp2.head()","e4b2eae2":"temp3 = pd.DataFrame()","e6e2e2d0":"temp2.reset_index(level=0, inplace=True)\ntemp2.head()","ce9c025d":"print(data.shape)\nprint(temp2.shape)","beaec77e":"temp3 =  pd.merge(temp2,data.drop_duplicates(),on='CustomerID',how='right')","2dead2d0":"temp3.shape","7ab39653":"temp3['CustomerID'].nunique()","bf9379af":"temp2['CustomerID'].nunique()","a463e6d0":"data['CustomerID'].nunique()","9ff93bfd":"temp3.dropna(inplace=True)\ntemp3['CustomerID'].nunique()","41c5927f":"temp3.shape","301b88bb":"temp3.head()","e466f005":"#Fetch wordcount for each Description\ntemp3['word_count'] = temp3['Description'].apply(lambda x: len(str(x).split(\" \")))\ntemp3[['Description','word_count']].head()","a2b0d374":"temp4 = temp3[['Description','word_count']]","a37ac4f4":"temp4.head()","7780954d":"temp4.word_count.describe()","9d95c466":"#Identify common words\nfreq = pd.Series(' '.join(temp4['Description']).split()).value_counts()[:20]\nfreq","afa13e99":"#Identify uncommon words\nfreq1 =  pd.Series(' '.join(temp4 ['Description']).split()).value_counts()[-20:]\nfreq1\n","a0cc2b17":"import re\nimport nltk\nnltk.download('stopwords')\nnltk.download('wordnet')\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.stem.wordnet import WordNetLemmatizer","d962a3d7":"stop_words = set(stopwords.words(\"english\"))\nnew_words = ['RED','PINK', 'BLUE', 'OF', 'BROWN',\"BLACK\"]\nstop_words = stop_words.union(new_words)\n","3436bfef":"for i in new_words:\n  if i in stop_words:\n    print(i)\n","09f62ca0":"corpus = []\nfor i in range(0, 164373):\n    #Remove punctuations\n    text = re.sub('[^a-zA-Z]', ' ', temp4['Description'][i])\n    \n    #Convert to lowercase\n    text = text.lower()\n    \n    #remove tags\n    text=re.sub(\"&lt;\/?.*?&gt;\",\" &lt;&gt; \",text)\n    \n    # remove special characters and digits\n    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n    \n    ##Convert to list from string\n    text = text.split()\n    \n    ##Stemming\n    ps=PorterStemmer()\n    #Lemmatisation\n    lem = WordNetLemmatizer()\n    text = [lem.lemmatize(word) for word in text if word not in  \n            stop_words] \n    text = \" \".join(text)\n    corpus.append(text)","76cc3c4d":"corpus[:10]","cebd9d35":"from PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator","ff45ee0b":"wordcloud = WordCloud(    #background_color='white',\n                          stopwords=stop_words,\n                          max_words=200,\n                          max_font_size=50, \n                          random_state=42\n                         ).generate(str(corpus))","87408475":"plt.figure(figsize=(25,10))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.title('Word Cloud for Best Customer\\'s Products')","4b0a253e":"from sklearn.feature_extraction.text import CountVectorizer\nimport re\ncv=CountVectorizer(max_df=0.8,stop_words=stop_words, max_features=10000, ngram_range=(1,3))\nX=cv.fit_transform(corpus)","54f62895":"list(cv.vocabulary_.keys())[:20]","04e535af":"#Most frequently occuring words\ndef get_top_n_words(corpus, n=None):\n    vec = CountVectorizer().fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in      \n                   vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], \n                       reverse=True)\n    return words_freq[:n]","21f9c8e0":"top_words = get_top_n_words(corpus, n=20)\ntop_df = pd.DataFrame(top_words)\ntop_df.columns=[\"Word\", \"Freq\"]\ntop_df[:20]","fdfa6af3":"\nsns.catplot(data=top_df,x='Word',y='Freq',kind='bar')\nplt.xticks(rotation = 60)\n","9f10a619":"Right Join on temp2 and data \n","ddc87f4d":"Score best : 1 worst : 4","86794548":"Customer who spent most","4be7c0de":"Sales Column created\n","1a6ec142":"Here we see the values of quantity which are less than 0 and there are no quantity which is values as 0.","e98e9385":"\n\n---\n\n\n# EDA Result:\n\n\n1.   Max Orders in month (Sep to Dec)\n2.   WeekDay wise sales increases till Thursday then decreses \n3.   No Transaction on Saturday\n4.   Maximum Sale in UK (as it is UK based company)\n5.   Abroad Max Sales in Germany, France, etc. (refer above graph)\n6.   Top Products (refer above graph)  \n7.   Orders Increases till 12 then decreases \n\n\n\n\n---\n\n\n\n\n","3867aa46":"# Segmentations below using score","04c952b2":"Now dropping customers and descriptions missing fields from the dataset","b60c6c18":"temp4 contains description of products bought by best customers, this list of description will be used for wordcloud","49904f39":"# Word Cloud for Best Customer","1673071d":"# Thank you for your time \n## Do upvote and comment if you find this notebook helpful \n## Please comment any suggestion","7d2f0e46":"Top\/Best Customers","d75c67a9":"# RFM Model","b2784094":"Checking for duplicates now and will remove those duplicates\n","f94142e3":"### Dataset after wrangling and preprocessing","a24fdaf9":"## preparing dataset of best customer for product description analysis","193fce61":"# Getting top 20 words in top_df dataframe","c86620a7":"### This notebook aims at analyzing the content of an E-commerce database that lists purchases made by \u223c4000 customers over a period of one year (from 2010\/12\/01 to 2011\/12\/09).\n\n### This notebook includes :\n\n  * Data Visualization and Analysis\n  * Customer Segmentation using RFM Method\n  * Analysis of product description for a particular segment of customers (example of segments : best cusotmers, loyal cusotmers, lost cusotmers, etc.)\n  * Word Cloud for words in Product Description after cleaning (reomving unnecessay words or stop words)\n","c2048733":"Now main object is to remove outliers that is negative and 0 quantity is not at all possible so now we have to remove that and also the returns orders are to be seperated as the prediciton is for orders placements and not the returs.","72f04d99":"## EDA on the modified dataset","91c7d464":" New Columns inserted related to Date & Time","832250c3":"So now the Quantity outliers have been removed now we are left with the return orders","8d1e72d7":"# Word Count Summary","0a70f469":"## temp4 will be used for word cloud and product description analysis","a57c86bf":"Loyal Customers","6c3e180e":"Lost Customers"}}