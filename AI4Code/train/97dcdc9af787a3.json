{"cell_type":{"4081dd36":"code","c242b15f":"code","d9da9c2a":"code","e4bf9b04":"code","0d4bd7dd":"code","8bb6219c":"code","6cb6d0e6":"code","da80ad4b":"code","7028b646":"code","0e021131":"code","5ebbfd19":"code","1e8a770b":"code","12d0c1c9":"code","e632c131":"code","deb9cb5d":"code","9175b3f2":"code","9524d9fe":"code","cea8e614":"code","d3ef1c6a":"code","bb511621":"code","975c662c":"code","172e4b64":"code","6820d612":"code","d9178383":"code","8e9758ef":"code","f7802109":"code","0fc02efe":"code","0dab4c90":"code","7bfcef04":"code","daf40a8a":"code","e8700c27":"code","3c275a46":"code","6e286eb9":"code","b5018870":"code","535b6f2c":"code","47872307":"code","0a326569":"code","411f56aa":"code","8575e55e":"code","bf6a3ff6":"code","cd27da2d":"code","a32bd4e0":"code","0571374d":"code","a03dab89":"code","e6676bef":"code","6ea88c85":"code","8bbd93b5":"code","784e11b4":"code","eeed1705":"code","cc9a24e7":"code","23c59084":"code","b5361ad9":"code","af7e927a":"code","bc788b37":"code","aaa746a4":"code","9c819a1a":"code","e4be844c":"code","00067b11":"code","f34293a9":"code","35ba89fa":"code","9212cc71":"code","b1ca1f64":"code","7e4cfae3":"code","663187f6":"code","c38531cb":"code","d8142d44":"code","911965ce":"code","dc16f3c8":"code","0121b24e":"code","b991bec4":"markdown","4db8f9b1":"markdown","be9f8c6a":"markdown","288e10da":"markdown","f7b1677a":"markdown","c8d5c9a1":"markdown","697222d3":"markdown","d1f02edc":"markdown","0e313430":"markdown","8812fe17":"markdown","de7605b1":"markdown","137aa84a":"markdown","2eea5f50":"markdown","c37b71c4":"markdown","9ef672f6":"markdown","35327154":"markdown","1453625e":"markdown","c0d915d6":"markdown","9cdcfea9":"markdown","d0cbe630":"markdown","0137d5cb":"markdown","96d75318":"markdown","e03d954a":"markdown","031dc486":"markdown","65a2ef8e":"markdown","bbaedd92":"markdown","5337a684":"markdown","2ffeccb0":"markdown","07473e93":"markdown","5f489d94":"markdown","7704b179":"markdown","7549dd66":"markdown","beb0e3f6":"markdown","656ad082":"markdown","108605f4":"markdown","bd0e185d":"markdown","bade6c9f":"markdown","6c5c1b3e":"markdown","a2cadfd5":"markdown"},"source":{"4081dd36":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\ntrain_df = pd.read_csv(\"..\/input\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/test.csv\")\ncombine = [train_df, test_df]","c242b15f":"print(\"Variables:\", train_df.columns)\ntrain_df.head()","d9da9c2a":"#lets see some statistic in our dataset.\ntrain_df.describe()","e4bf9b04":"# graphics are better\nf, ax = plt.subplots(3, 3, figsize = (20,10))\n\nsns.countplot(\"Survived\", data = train_df, ax = ax[0,0])\nsns.countplot(\"Pclass\", data = train_df, ax = ax[0,1])\nsns.countplot(\"Sex\", data = train_df, ax = ax[0,2])\ntrain_df[\"Age\"].plot.hist(x= \"Age\", ax = ax [1,0])\nsns.countplot(\"SibSp\", data = train_df, ax = ax[1,1])\nsns.countplot(\"Parch\", data = train_df, ax = ax[1,2])\ntrain_df[\"Fare\"].plot.hist(x=\"Fare\", ax = ax [2,0])\nsns.countplot(\"Embarked\", data = train_df, ax = ax[2,1])\n\nplt.show()","0d4bd7dd":"sns.countplot(\"Pclass\", hue=\"Survived\", data = train_df)\n\nprint(\"How many people survived in each class: \\n\", \n      pd.crosstab(train_df[\"Pclass\"], train_df[\"Survived\"]), \"\\n\")\n\nprint(\"The survive-rate in each class: \\n\",\n      train_df[[\"Pclass\", \"Survived\"]].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False))","8bb6219c":"# print(\"Here are no missing data and because the data is categorical there are also no outliers. NEXT!\")\n# train_df.Pclass.describe()","6cb6d0e6":"print(\"The survived rate of male vs female.\\n\",\n      train_df[[\"Sex\", \"Survived\"]].groupby([\"Sex\"], as_index = False).mean().sort_values(by=\"Survived\", ascending=False))\n\nf, ax = plt.subplots(1, 2, figsize =(10, 3))\n\nsns.countplot(\"Sex\", hue = \"Survived\", data = train_df, ax = ax[0])\ntrain_df[[\"Sex\", \"Survived\"]].groupby([\"Sex\"]).mean().plot.bar(ax = ax[1])\nplt.show()","da80ad4b":"# Both graphics represents the same. The Age distribution\nprint(train_df.Age.describe())\n\nf, ax = plt.subplots(1,2, figsize = (20,5))\n\ntrain_df.Age.plot.hist(bins = 20, ax= ax[0])\nsns.violinplot(train_df.Age, ax=ax[1])\nplt.show()","7028b646":"grid = sns.FacetGrid(train_df, col=\"Survived\", size=4)\ngrid.map(plt.hist, \"Age\", bins = 10)\nplt.show()","0e021131":"print(\"In our data we have\", train_df.Survived.count() - train_df.Age.count(), \"Ages missing from the Data\")\nprint(\"And also some outliers. We have to clean this data later!\")\ntrain_df.Age.plot.box()","5ebbfd19":"# train_df.SibSp.value_counts().plot.bar()\nprint(\"Let's see what is the survived rate\/avarage of number of people with relatives on Titanic \\n\",\n     train_df[[\"SibSp\", \"Survived\"]].groupby([\"SibSp\"], as_index=False).mean().sort_values(by = \"Survived\", ascending = False))\n\n\nf, ax = plt.subplots(1, 2, figsize = (15, 5))\nsns.countplot(\"SibSp\", hue = \"Survived\", data = train_df, ax = ax [1])\n# ax[1].set_title(\"asad\")\n\n# train_df[[\"SibSp\", \"Survived\"]].groupby([\"SibSp\"], as_index=False).count().sort_values(by = \"Survived\", ascending = False).plot.bar(ax = ax[1], title = \"People survived by nr of SibSp\")\ntrain_df[[\"SibSp\", \"Survived\"]].groupby([\"SibSp\"], as_index=False).mean().sort_values(by = \"Survived\", ascending = False).plot.bar(ax = ax[0], title = \"Survive rate by nr of SibSp\")","1e8a770b":"print(\"We have\", train_df.SibSp.count(), \"Observation in our Dataset, so no missing Data, \\n but maybe we shoud simplify this variable. Later\")","12d0c1c9":"print(\"Lets see how the Survived-rate in each Parch category is distributed: \\n\",\n      train_df[[\"Parch\", \"Survived\"]].groupby([\"Parch\"], as_index = False).mean().sort_values(by = \"Survived\", ascending = False))\n\nf, ax = plt.subplots(1, 2, figsize = (15, 5))\n\ntrain_df[[\"Parch\", \"Survived\"]].groupby([\"Parch\"]).mean().plot.bar(ax = ax[1])\nsns.countplot(\"Parch\", hue = \"Survived\", data = train_df, ax = ax[0])\n\nax[0].set_title(\"No of Parch\")\nax[0].set_xlabel(\"Parch\")\nax[0].set_ylabel(\"Count of Parch\")\n\nplt.show()\n","e632c131":"print(\"We dont have any missing data here and now outliers\")\nprint(train_df.Parch.count(), \"total obserivation in the database\")","deb9cb5d":"print(train_df.Fare.describe())\n\nf, ax = plt.subplots(1, 2, figsize=(15,5))\n\ntrain_df.Fare.plot.box(ax =ax[0])\ntrain_df.Fare.plot.hist(bins = 10 , ax = ax[1])","9175b3f2":"grid = sns.FacetGrid(train_df, col = \"Survived\", size = 5)\ngrid.map(plt.hist, \"Fare\", bins = 10)\n\nplt.show()","9524d9fe":"print(\"I am just curious. Who paid more. Women or men?\")\n\ngrid = sns.FacetGrid(train_df,  size = 3)\ngrid.map(sns.barplot, \"Sex\", \"Fare\", ci=None)","cea8e614":"print(\"We dont have any missing data here ->\", train_df.Fare.count())\nprint(\"We have some outliers at the first sight\")\n\ntrain_df.Fare.plot.box()\nplt.show()","d3ef1c6a":"print(\" Missing Data:\", train_df.Survived.count() - train_df.Embarked.count())\n\nprint(train_df[[\"Embarked\", \"Survived\"]].groupby([\"Embarked\"], as_index = False).mean().sort_values(by = \"Survived\", ascending = False))\n\nf, ax = plt.subplots(1, 2, figsize = (20, 5))\n\ntrain_df.Embarked.value_counts().plot.bar(ax = ax[0])\nsns.countplot(\"Embarked\", hue=\"Survived\", data = train_df, ax = ax[1])\nplt.show()","bb511621":"print(\"Before we remove we have\", train_df.shape, \"in Train Set and\", test_df.shape, \"in Test Set\")\n\ntrain_df = train_df.drop([\"Cabin\", \"Ticket\"], axis = 1)\ntest_df = test_df.drop([\"Cabin\", \"Ticket\"], axis = 1)\ncombine = [train_df, test_df]\n\nprint(\"Now we should have\", train_df.shape, \"in Train set and,\", test_df.shape, \"in Test Set\")","975c662c":"for dataset in combine:\n    dataset[\"Title\"] = dataset.Name.str.extract(\" ([A-Za-z]+)\\.\", expand = False)\n\npd.crosstab(train_df[\"Title\"], train_df[\"Sex\"]).T","172e4b64":"pd.crosstab(test_df[\"Title\"], train_df[\"Sex\"]).T","6820d612":"train_df[[\"Title\", \"Age\"]].groupby([\"Title\"]).mean()","d9178383":"for dataset in combine:\n    dataset[\"Title\"] = dataset[\"Title\"].replace(\"Dona\", \"Other\")\n    dataset[\"Title\"] = dataset[\"Title\"].replace(\"Mlle\", \"Miss\")\n    dataset[\"Title\"] = dataset[\"Title\"].replace(\"Mme\", \"Miss\")\n    dataset[\"Title\"] = dataset[\"Title\"].replace(\"Ms\", \"Miss\")\n    dataset[\"Title\"] = dataset[\"Title\"].replace(\"Capt\", \"Mr\")\n    dataset[\"Title\"] = dataset[\"Title\"].replace(\"Col\", \"Mr\")\n    dataset[\"Title\"] = dataset[\"Title\"].replace(\"Countess\", \"Mrs\")\n    dataset[\"Title\"] = dataset[\"Title\"].replace(\"Don\", \"Mr\")\n    dataset[\"Title\"] = dataset[\"Title\"].replace(\"Dr\", \"Mr\")\n    dataset[\"Title\"] = dataset[\"Title\"].replace(\"Jonkheer\", \"Other\")\n    dataset[\"Title\"] = dataset[\"Title\"].replace(\"Lady\", \"Mrs\")\n    dataset[\"Title\"] = dataset[\"Title\"].replace(\"Major\", \"Mr\")\n    dataset[\"Title\"] = dataset[\"Title\"].replace(\"Rev\", \"Other\")\n    dataset[\"Title\"] = dataset[\"Title\"].replace(\"Sir\", \"Mr\")\n\ntrain_df[[\"Title\", \"Age\"]].groupby([\"Title\"]).mean()\n# train_df[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()\n","8e9758ef":"for dataset in combine:\n    dataset.loc[(dataset.Age.isnull()) & (dataset.Title == \"Mr\"), \"Age\"] = 32\n    dataset.loc[(dataset.Age.isnull()) & (dataset.Title == \"Master\"), \"Age\"] = 4\n    dataset.loc[(dataset.Age.isnull()) & (dataset.Title == \"Miss\"), \"Age\"] = 22\n    dataset.loc[(dataset.Age.isnull()) & (dataset.Title == \"Mrs\"), \"Age\"] = 36\n    dataset.loc[(dataset.Age.isnull()) & (dataset.Title == \"Other\"), \"Age\"] = 42\n    \nprint(\"Age missing values after we matched the missing values from the Name Title is:\",\n      train_df.Age.isnull().sum())","f7802109":"print(test_df.info())\nprint(\"-\"*40)\n\ntest_df[\"Fare\"].fillna(test_df[\"Fare\"].dropna().median(), inplace = True)","0fc02efe":"test_df.info()","0dab4c90":"train_df.Embarked.fillna(\"S\", inplace = True)\ntrain_df.Embarked.isnull().sum()","7bfcef04":"train_df.head()","daf40a8a":"# Outlier detection \nfrom collections import Counter\n#Once initialized, counters are accessed just like dictionaries.\n#Also, it does not raise the KeyValue error (if key is not present) instead the value\u2019s count is shown as 0.\ndef detect_outliers(df,n,features):\n    outlier_indices = []\n    # iterate over features(columns)\n    for col in features:\n        # 1st quartile (25%)\n        Q1 = np.percentile(df[col],25)\n        # 3rd quartile (75%)\n        Q3 = np.percentile(df[col],75)\n        # Interquartile range (IQR)\n        IQR = Q3 - Q1\n        # outlier step\n        outlier_step = 1.5 * IQR\n        # Determine a list of indices of outliers for feature col\n        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index       \n        # append the found outlier indices for col to the list of outlier indices \n        outlier_indices.extend(outlier_list_col)\n        \n    # select observations containing more than 2 outliers\n    outlier_indices = Counter(outlier_indices)        \n    multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )\n    return multiple_outliers   \n# detect outliers from Age, SibSp , Parch and Fare\nOutliers_to_drop = detect_outliers(train_df,2,[\"Age\",\"SibSp\",\"Parch\",\"Fare\"])\ntrain_df.loc[Outliers_to_drop] # Show the outliers rows","e8700c27":"# lets delete this outliers\ntrain_df = train_df.drop(Outliers_to_drop, axis = 0).reset_index(drop=True)","3c275a46":"print(\"So if the oldest Age was 80. We just devide this into 5 groups ->\",80\/5 )\n\ncombine = [train_df, test_df]\n\nfor dataset in combine:\n    dataset.loc[dataset[\"Age\"] <= 16, \"Age_group\"] = 0\n    dataset.loc[(dataset[\"Age\"] > 16) & (dataset[\"Age\"] <=32), \"Age_group\"] = 1\n    dataset.loc[(dataset[\"Age\"] > 32) & (dataset[\"Age\"] <=48), \"Age_group\"] = 2\n    dataset.loc[(dataset[\"Age\"] > 48) & (dataset[\"Age\"] <=64), \"Age_group\"] = 3\n    dataset.loc[(dataset[\"Age\"] > 64), \"Age_group\"] = 4\n\ntrain_df.head()","6e286eb9":"# train_df[[\"Age_group\", \"Survived\"]].groupby([\"Age_group\"]).mean().sort_values(by = \"Survived\")\nprint(\"How many passangers are in the age_groups each:\\n\",\n      train_df.Age_group.value_counts(ascending = False))\n\nf, ax = plt.subplots(1,2, figsize = (20,5))\ntrain_df.Age_group.value_counts().plot.bar( ax = ax [0])\nsns.countplot(\"Age_group\", hue = \"Survived\", data = train_df, ax = ax[1])\n","b5018870":"train_df['Fare_range'] = pd.qcut(train_df['Fare'], 4)\n\ntrain_df[[\"Fare_range\", \"Survived\"]].groupby([\"Fare_range\"], as_index = False).mean().sort_values(by = \"Fare_range\", ascending = True)","535b6f2c":"#Now we can create the fare categories with the the values above.\n    \nfor dataset in combine:\n    dataset.loc[(dataset[\"Fare\"] <= 7.91), \"Fare_cat\"] = 0\n    dataset.loc[(dataset[\"Fare\"] > 7.91) & (dataset[\"Fare\"] <= 14.454), \"Fare_cat\"] = 1\n    dataset.loc[(dataset[\"Fare\"] > 14.454) & (dataset[\"Fare\"] <= 31), \"Fare_cat\"] = 2\n    dataset.loc[(dataset[\"Fare\"] > 31), \"Fare_cat\"] = 3\n    \nf, ax = plt.subplots(1, 2, figsize = (20, 5))\nsns.countplot(\"Fare_cat\", hue = \"Survived\", data = train_df, ax = ax[0])\nsns.factorplot(\"Fare_cat\", \"Survived\", hue = \"Sex\", data = train_df, ax = ax [1])\nplt.show()","47872307":"# lets make from sibsp and parch family sets and a new variable of is_single\n\nfor dataset in combine:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n    \ntrain_df.head()","0a326569":"# Now we have the family size. 1 is alone and above 1 is with family\n\nfor dataset in combine:\n    dataset[\"Single\"] = 0\n    dataset.loc[dataset[\"FamilySize\"] == 1, \"Single\"] = 1\n\nprint(train_df[[\"Single\", \"Survived\"]].groupby([\"Single\"]).mean())    \n\nsns.countplot(\"Single\", hue = \"Survived\", data = train_df)","411f56aa":"train_df = train_df.drop([\"PassengerId\", \"Name\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Fare_range\", \"FamilySize\"], axis = 1)\ntest_df = test_df.drop([\"Name\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"FamilySize\"], axis = 1)\ncombine = [train_df, test_df]","8575e55e":"sns.violinplot(\"Sex\",\"Age_group\", hue=\"Survived\", data=train_df, split=True)","bf6a3ff6":"grid = sns.FacetGrid(train_df, col = \"Embarked\", size = 4)\ngrid.map(sns.pointplot, \"Pclass\", \"Survived\", \"Sex\")\ngrid.add_legend()","cd27da2d":"grid = sns.FacetGrid(train_df, col = \"Embarked\", row = \"Survived\", size = 3)\ngrid.map(sns.barplot, \"Sex\", \"Fare_cat\", ci=None)","a32bd4e0":"grid = sns.FacetGrid(train_df, col = \"Embarked\", row = \"Pclass\", size = 4)\ngrid.map(plt.hist, \"Fare_cat\", bins = 5)","0571374d":"grid = sns.FacetGrid(train_df, col =\"Embarked\", row = \"Survived\", size = 3, margin_titles = \"xx\")\ngrid.map(plt.hist, \"Age_group\",  bins=20)","a03dab89":"grid = sns.FacetGrid(train_df, row = \"Survived\", col = \"Pclass\", size = 3)\ngrid.map(plt.hist, \"Age_group\")\n\ntrain_df[[\"Age_group\", \"Survived\", \"Pclass\", \"Fare_cat\"]].groupby([\"Pclass\"], as_index = False).mean().sort_values(by = \"Age_group\", ascending = False)","e6676bef":"grid = sns.FacetGrid(train_df, col = \"Single\", size = 3)\ngrid.map(sns.pointplot, \"Survived\", \"Sex\", ci = None)","6ea88c85":"# g = (train_df.loc[: , [\"Survived\", \"Pclass\"]]).corr()\n# sns.heatmap(train_df.corr())\nsns.heatmap(train_df.corr(),annot=True)","8bbd93b5":"test_df.head()","784e11b4":"train_df.head()","eeed1705":"for dataset in combine:\n    dataset[\"Sex\"].replace([\"male\", \"female\"], [0, 1], inplace = True)\n    dataset[\"Embarked\"].replace([\"S\", \"C\", \"Q\"], [0, 1, 2], inplace = True)\n    dataset[\"Title\"].replace([\"Master\", \"Miss\", \"Mr\", \"Mrs\", \"Other\"], [0, 1, 2, 3, 4], inplace = True)\n\ntrain_df.head()","cc9a24e7":"#Lets see the correlation diagriam\n\nsns.heatmap(train_df.corr(), annot = True)\nplt.show()","23c59084":"#Another one correlation map, which I find it ugly\n\nsns.pairplot(train_df, hue=\"Survived\", size=1.5)\nplt.show()","b5361ad9":"print(\"Lets import what we need\")\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import confusion_matrix #for confusion matrix\nfrom sklearn.model_selection import cross_val_predict","af7e927a":"test_df.head()","bc788b37":"X_train = train_df.drop(\"Survived\", axis = 1)\nY_train = train_df[\"Survived\"]\nX_test = test_df.drop(\"PassengerId\", axis = 1).copy()\n\nX_train.shape, Y_train.shape, X_test.shape","aaa746a4":"# Logistic Regression\n# Logistic Regression is a useful model to run early in the workflow. \n# Logistic regression measures the relationship between the categorical dependent variable (feature) and \n# one or more independent variables (features) by estimating probabilities using a logistic function, which is the cumulative logistic distribution.\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\nY_pred = logreg.predict(X_test)\nacc_log = round(logreg.score(X_train, Y_train) * 100, 2)\nprint(\"The accuracy of Logistic Regression is:\", acc_log)","9c819a1a":"#lets check the coefficient. As higher the coefficient variable is, as higher the probality of the output\ncoeff_df = pd.DataFrame(train_df.columns.delete(0)) #remove the survival variable\ncoeff_df.columns = [\"Features\"] # create a new column\ncoeff_df[\"Correlation\"] = pd.Series(logreg.coef_[0])\n\ncoeff_df.sort_values(by = \"Correlation\", ascending = False)","e4be844c":"# Support Vector Machine (SVM)\n# Support Vector Machines which are supervised learning models with associated learning algorithms \n# that analyze data used for classification and regression analysis. \n# Given a set of training samples, each marked as belonging to one or the other of two categories, \n# an SVM training algorithm builds a model that assigns new test samples to one category or the other, making it a non-probabilistic binary linear classifier.\n\nsvc = SVC()\nsvc.fit(X_train, Y_train)\nY_pred = svc.predict(X_test)\nacc_svc = round(svc.score(X_train, Y_train) * 100, 2)\nprint(\"The accuracy of SVM is:\", acc_svc)","00067b11":"# KNN\n# In pattern recognition, the k-Nearest Neighbors algorithm (or k-NN for short) is a non-parametric method used for classification and regression. \n# A sample is classified by a majority vote of its neighbors, with the sample being assigned to the class most common among its k nearest neighbors \n# (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor. \n\nknn = KNeighborsClassifier(n_neighbors = 7)\nknn.fit(X_train, Y_train)\nY_pred = knn.predict(X_test)\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\nprint(\"The accuracy with KNN is:\", acc_knn)","f34293a9":"# lets find the best neighbor nr.\n\nfor n in range (1, 20, 2):\n    knn = KNeighborsClassifier (n_neighbors = n)\n    knn.fit(X_train, Y_train)\n    Y_pred = knn.predict(X_test)\n    acc_knn = round(knn.score(X_train, Y_train) * 100, 2)\n    print(\"The accuracy with\", n, \"neihbors is:\", acc_knn)","35ba89fa":"# Gaussian Naive Bayes\n# In machine learning, naive Bayes classifiers are a family of simple probabilistic classifiers \n# based on applying Bayes' theorem with strong (naive) independence assumptions between the features. \n# Naive Bayes classifiers are highly scalable, requiring a number of parameters linear in the number of variables (features) in a learning problem.\n\ngaussian = GaussianNB()\ngaussian.fit(X_train, Y_train)\nY_pred = gaussian.predict(X_test)\nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\nprint(\"The accuracy with Gaussian Naive Bayes is\", acc_gaussian)","9212cc71":"# Perceptron \n# The perceptron is an algorithm for supervised learning of binary classifiers \n# (functions that can decide whether an input, represented by a vector of numbers, belongs to some specific class or not). \n# It is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor \n# function combining a set of weights with the feature vector. The algorithm allows for online learning, in that it processes elements in the training set one at a time.\n\nperceptron = Perceptron()\nperceptron.fit(X_train, Y_train)\nY_pred = perceptron.predict(X_test)\nacc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)\nprint(\"The Accuracy with Perceptron is:\", acc_perceptron)","b1ca1f64":"# Linear SVC\n\nlinear_svc = LinearSVC()\nlinear_svc.fit(X_train, Y_train)\nY_pred = linear_svc.predict(X_test)\nacc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)\nprint(\"The accuracy of Linear SVC is:\", acc_linear_svc)","7e4cfae3":"# Stochastic Gradient Descent\n\nsgd = SGDClassifier()\nsgd.fit(X_train, Y_train)\nY_pred = sgd.predict(X_test)\nacc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\nprint(\"The Accuracy of SGD is:\", acc_sgd)","663187f6":"# Decision Tree\n\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, Y_train)\nY_pred = decision_tree.predict(X_test)\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) *100, 2)\nprint(\"The accuracy of Decision Tree is\", acc_decision_tree)","c38531cb":"# Random Forest\n# The next model Random Forests is one of the most popular. \n# Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks, \n# that operate by constructing a multitude of decision trees (n_estimators=100) at training time and outputting the class \n# that is the mode of the classes (classification) or mean prediction (regression) of the individual trees.\n\nrandom_forest = RandomForestClassifier(n_estimators = 100)\nrandom_forest.fit(X_train, Y_train)\nY_pred = random_forest.predict(X_test)\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\nprint(\"The accuracy of random forest ist:\", acc_random_forest)\n","d8142d44":"print(\"now lets see the scores togher\")\n\nmodels = pd.DataFrame({\n    \"Model\": [\"Support Vector Machine\", \"KNN\", \"Logistic Regression\", \"Random Forest\", \"Naive Bayes\", \"Perceptron\", \"Stochastic Gradiant Decent\", \"Linear SVC\", \"Decision Tree\"],\n    \"Score\": [acc_svc, acc_knn, acc_log, acc_random_forest, acc_gaussian, acc_perceptron, acc_sgd, acc_linear_svc, acc_decision_tree]\n})\n\nmodels.sort_values(by = \"Score\", ascending = False)","911965ce":"print(\"\"\"Now lets see a confusion Matrix, which give a summary of the predictions made. Great for classification problem (1 vs 0)\nThe number of correct and incorrect predictions are counted and broken down by each class.\n\"\"\")\n\n# Y_pred = cross_val_predict(RandomForestClassifier(n_estimators = 100), X, Y, cv = 10)\n# sns.heatmap(confusion_matrix(Y, Y_pred))\n\nf,ax=plt.subplots(2,4,figsize=(20,10))\n\nY_pred = cross_val_predict(SVC(gamma = \"auto\"), X_train, Y_train, cv = 10)\nsns.heatmap(confusion_matrix(Y_train, Y_pred), ax=ax[0,0], annot=True, fmt='2.0f')\nax[0,0].set_title('Matrix for SVM')\n\nY_pred = cross_val_predict(KNeighborsClassifier(n_neighbors = 7), X_train, Y_train, cv = 10)\nsns.heatmap(confusion_matrix(Y_train, Y_pred), ax=ax[0,1], annot=True, fmt='2.0f')\nax[0,1].set_title('Matrix for KNN')\n\nY_pred = cross_val_predict(GaussianNB(), X_train, Y_train, cv = 10)\nsns.heatmap(confusion_matrix(Y_train, Y_pred), ax=ax[0,2], annot=True, fmt='2.0f')\nax[0,2].set_title('Matrix for Naive Bayes')\n\nY_pred = cross_val_predict(Perceptron(), X_train, Y_train, cv = 10)\nsns.heatmap(confusion_matrix(Y_train, Y_pred), ax=ax[0,3], annot=True, fmt='2.0f')\nax[0,3].set_title('Matrix for Perceptron')\n\nY_pred = cross_val_predict(RandomForestClassifier(n_estimators=100), X_train, Y_train, cv = 10)\nsns.heatmap(confusion_matrix(Y_train, Y_pred), ax=ax[1,0], annot=True, fmt='2.0f')\nax[1,0].set_title('Matrix for Random-Forests')\n\nY_pred = cross_val_predict(LinearSVC(), X_train, Y_train, cv = 10)\nsns.heatmap(confusion_matrix(Y_train, Y_pred), ax=ax[1,1], annot=True, fmt='2.0f')\nax[1,1].set_title('Matrix for Linear SVC')\n\nY_pred = cross_val_predict(SGDClassifier(), X_train, Y_train, cv = 10)\nsns.heatmap(confusion_matrix(Y_train, Y_pred), ax=ax[1,2], annot=True, fmt='2.0f')\nax[1,2].set_title('Matrix for SGDClassifier')\n\nY_pred = cross_val_predict(DecisionTreeClassifier(), X_train, Y_train, cv = 10)\nsns.heatmap(confusion_matrix(Y_train, Y_pred), ax=ax[1,3], annot=True, fmt='2.0f')\nax[1,3].set_title('Matrix for DecisionTreeClassifier')\n\nplt.subplots_adjust(hspace=0.2,wspace=0.2)\nplt.show()","dc16f3c8":"print(\"\"\"\nLets take the last matrix:\n493 -> correct nr. of predictions for Dead\n236 -> correct nr. of prediction for Survived\n56 -> Wrongly classified 56 as survived \n106 -> Wrong classified as dead \n\"\"\")","0121b24e":"submission = pd.DataFrame({\n#         \"PassengerId\": test_df[\"PassengerId\"],\n        \"Survived\": Y_pred\n    })\nsubmission.to_csv(\"submission.csv\", index=False)","b991bec4":"**Embarked** - the last one.","4db8f9b1":"**SibSp** - the number of relatives on the boot.","be9f8c6a":"First I want to get rid of some variables. \"Cabin\" and \"Tickets seems to not have any impact on the outcome. Cabin has to many missing values and tickts seems to be a random number. ","288e10da":"An outlier is an observation point that is distant from other observations. An outlier may be due to variability in the measurement or it may indicate experimental error; the latter are sometimes excluded from the data set. reference https:\/\/www.analyticsvidhya.com\/blog\/2016\/01\/guide-data-exploration\/\nMethods of detecting Outlier -> Refer for more details http:\/\/colingorrie.github.io\/outlier-detection.html\n\n* Z-score method\n* Modified Z-score method\n* IQR method -- I will use this method.\n\nIQR Method meens that any data points outside 1.5 time the IQR (1.5 time IQR below Q1, or 1.5 time IQR above Q3), is considered an outlier.\nThe interquartile range (IQR) is a measure of statistical dispersion, being equal to the difference between the 75th and 25th percentiles, or between upper and lower quartiles.\n\nSo, I found some outliers in Age, SibSp, Parch and Fare. Lets go ahead","f7b1677a":"# Data preparation\nNow lets convert Sex, Embarked and Title into numbers, so the machine can understand and predict them","c8d5c9a1":"Now lets see if we can find some corelations between data.","697222d3":"**Age_groups ** - Lets crate a new variable, where the ages are splitet into groups. Machine learning love this and hates to work with continous data","d1f02edc":"**We are done with cleaning the data** and creating new variables. \nLets drop the columns we dont need anymore, like... PassengerID, Name, Age, SibSp, Parch, Fare, Family Size, etc \")\n","0e313430":"**Parch** What about the number of parents with children","8812fe17":"First I had to unrderstood that we are dealing here with a classification problem. I also learned that this type of problem could be also a regression problem. We just want to know with all the variables who surived and not. We will train our machine with supervised learning algorythem, as we are giving our model input. So, with these understanding -> Supervised Learning + Classification and Regression, we can go with the following algorythem:\n* Logistic Regression\n* K-Nearest Neighbors\n* Support Vector Machines\n* Decision Tree\n* Random Forest\n* Perceptron (never used it)\n* Artificial neural network (never used it)\n* Relevance Vector Machine (never used it) ","de7605b1":"# Univariate Analysis\n\nLets start with the first independent variable. The passanager Class -> **Pclass**","137aa84a":"# Multivariate Analysis","2eea5f50":"Let's go trough the following steps:\n\n**1. Exploratory Data Analysis (EDA)**\n* Problem definition (what do we want to solve)\n* Variable Identification (what data do we have)\n* Univariate Analysis (understanding each field in the dataset)\n* Multivariate Analysis (understanding the interactions between different fields and target)\n* Missig values treatment (machine learning algorythems hates fields with missing data)\n* Outlier treatment\n\n**2. Feature Enginerring**\n* Variable transformation\n* Variable creation\n* Dimesionality reduction\n\n**3. Predictive Modeling**\n\nCredits goes to [Sunil Ray](https:\/\/www.analyticsvidhya.com\/blog\/2016\/01\/guide-data-exploration\/) and many amazing Kagglers, that showed me why and how to go with a workflow for EDA.\n\n\nNow get prepared, by sitting on you favorite chair in a room with a glas Wine or Cola, because this can be a long read.","c37b71c4":"So, of course the majority of the passangers without relatives on Titanic survived. But the chances were higher to survive, if you had childers or other family members.","9ef672f6":"**Fare** - Let's go ahead with the ticket prices.","35327154":"# Missing Values Treatment","1453625e":"**Missing Age** Now lets go ahead with the missing Ages. We will take them from the title Names.","c0d915d6":"**Fare** - In the train data, we dont have any missing values. But in the test set, we do. So lets just replace the line with the median value.","9cdcfea9":"# Outlier Treatment","d0cbe630":"So, women had about 75% chances to survive. A great variable to predict our target. NEXT!","0137d5cb":"# Problem definition\n\nI think you already know something about Titanic. However, Titanic was as a British passenger liner that sank in the North Atlantic Ocean in the early hours of 15 April 1912, after colliding with an iceberg during its maiden voyage from Southampton to New York City. There were an estimated 2,224 passengers and crew aboard, and more than 1,500 died, making it one of the deadliest commercial peacetime maritime disasters in modern history. ([Wikipedia](https:\/\/en.wikipedia.org\/wiki\/RMS_Titanic))\n\nAs we want to know who and dind't survived, we are dealing with a classification problem.","96d75318":"**Sex** What about males vs females. ","e03d954a":"Of course, there were way more passangiers in the third class. However, the passangiers in the first class, had a 62% chance to survive. \nThis variable has a significant correlation to our target variable (survival)","031dc486":"The majaroty on Titanic and which also survived were single. But if they had 1-3 Childrens, the chances to survive were more than 50%.  \nWe need to add SibSp and Parch to one \"Family\" variable.","65a2ef8e":"Yeah, so, most of the passangers paid arround 32 Dollars. and most of them survived. I cant see no correlations from this single information, so we need to treat this in the multivariatea analysis.","bbaedd92":"# 2. Feature Enginerring\n* Variable transformation\n* Variable creation\n* Dimesionality reduction","5337a684":"True story: the first movie, that I ever saw in the cinema, was \"[Titanic](https:\/\/en.wikipedia.org\/wiki\/Titanic_(1997_film)\". So I was very happy when I found out, that a lot of Data Scientist starts with the titanic dataset. So I will also used it to learn more about data exploratory analysis and features extraction techniques. \n\n\nShort story about [me](https:\/\/www.linkedin.com\/in\/vladmelonari\/): Early this year I started to learn python with the book \"learn python the hard way\". A few months ago I began to learn machine learning with different courses and tutorials, so this is my first kernel on a beginner-level.\nI understood quickly, that if you want to predict something, you need to understand your data, so this Notebook is more about data exploration and of course machine learning.\n\n","2ffeccb0":"**Fare_range ** - As the Fare variable is also continous, we need to bring it into groups, to make an ordinal variable.. just like age_groups.\nWe will use pandas qcut for this. So what qcut does is it splits or arranges the values according the number of bins we have passed. \nSo if we pass for 5 bins, it will arrange the values equally spaced into 5 seperate bins or value ranges.","07473e93":"**Age** distribution. Let's see how this looks like","5f489d94":"We have 891 observations in our training dataset and some missing data in the Age field.\nOh... also some outliers in the Age, Fare, SibSp and Parch table","7704b179":"Yeah. In Southampton came more on board, 644 Passangers from over 800. The chances were about 55 % to survive, if the passangers embarked in Cherbourg.\nWe also have 2 missing data","7549dd66":"The Kernal is still in progress.. I still have to comment few graphs and check the gramma. :)","beb0e3f6":"Before we go forward, we need to import the dataset","656ad082":"# Variable identification\n\nLet's see what information (variables) do we have in our dataset:","108605f4":"As we can see, the ages is also a good variable. Childres and a lot of passangers between 20 and 35 are saved. Also the olders on the ship is saved. ","bd0e185d":"For data vizualisation and it is important, to note down, what kind of type of variables do we have here.\n\n* **Nominal**: Survived, Sex, Embarked, SibSp, Parch\n* **Ordinal**: Pclass\n* **Numerical**: Age, Fare\n* Other: Name, Ticket, Cabin\n\nFor more information, you can read this [here](https:\/\/towardsdatascience.com\/data-types-in-statistics-347e152e8bee):","bade6c9f":"** Family size vs Single** - lets create a new variable. ","6c5c1b3e":"** Embarked** - has two missing values. We now the most passangers came from southampt, so why not add those two missing data to \"S\"?","a2cadfd5":"# Yeah. We are done with EDA \nexploring and analysing the data. Lets start with machine learning, finaly."}}