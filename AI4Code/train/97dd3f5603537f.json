{"cell_type":{"3c7df0b8":"code","785b7fb4":"code","4faa8c3d":"code","d27354d2":"code","c330ab64":"code","b7ad5682":"code","03979449":"code","f0734d77":"code","a45ed8a8":"code","00234c4c":"code","17e0a0d5":"code","cb4591ac":"code","a82ec7ee":"code","2eb24d24":"code","3b410c0a":"code","46b34c85":"code","32ebe249":"markdown","c78b7f97":"markdown","c96e13ee":"markdown","ada32c1a":"markdown","20921505":"markdown","8cb2c3e6":"markdown","8be8f8fa":"markdown","c5966d6c":"markdown","fddacb80":"markdown","fcff18f4":"markdown"},"source":{"3c7df0b8":"import gc\nimport numpy as np\nimport pandas as pd\nimport math\nimport torch\nimport copy\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.metrics import roc_auc_score\nfrom datetime import datetime\nimport torch.nn.functional as F","785b7fb4":"train = pd.read_csv('\/kaggle\/input\/tabular-playground-series-nov-2021\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/tabular-playground-series-nov-2021\/test.csv')\nsub = pd.read_csv('\/kaggle\/input\/tabular-playground-series-nov-2021\/sample_submission.csv')","4faa8c3d":"device = \"cuda\" if torch.cuda.is_available() else 'cpu'\nBATCH_SIZE = 512\nEPOCHS = 50","d27354d2":"FEATURES = [col for col in train.columns if col not in ['id','target']]\ny = train.target\ntrain_df = train.drop(columns=['id', 'target'])\ntest_df = test.drop(columns=['id'])","c330ab64":"def target_dist():\n    palette = 'Set2'\n    plt=sns.countplot(x=y,palette=palette)\n    plt.set_title('Target distribution')\n    sns.despine()\n\ntarget_dist()","b7ad5682":"# training data\nprint(f'Numerical attributes: {len(train_df._get_numeric_data().columns)}' )\nprint(f'Categorical attributes: {abs(len(train_df.columns) - len(train_df._get_numeric_data().columns))}')\n# testing data\nprint(f'Numerical attributes: {len(test_df._get_numeric_data().columns)}' )\nprint(f'Categorical attributes: {abs(len(test_df.columns) - len(test_df._get_numeric_data().columns))}')","03979449":"scaler_standard = StandardScaler()\ntrain_df[FEATURES] = scaler_standard.fit_transform(train_df[FEATURES])\ntest_df[FEATURES] = scaler_standard.transform(test_df[FEATURES])","f0734d77":"class CustomDataset:\n    def __init__(self, X, y=None):\n        self.X = X\n        self.y = y\n    def __len__(self):\n        return len(self.X)\n    def __getitem__(self, idx):\n        if self.y is None:\n            return torch.tensor(self.X.values[idx], dtype=torch.float)\n        else:\n            return torch.tensor(self.X.values[idx], dtype=torch.float), torch.tensor(self.y.values[idx], dtype=torch.float)","a45ed8a8":"# takes in a module and applies the specified weight initialization\ndef weights_init_uniform_rule(m):\n    classname = m.__class__.__name__\n    if classname.find('Linear') != -1:\n        n = m.in_features\n        y = 1.0\/np.sqrt(n)\n        m.weight.data.uniform_(-y, y)\n        m.bias.data.fill_(0)","00234c4c":"def fc_block(in_f, out_f):\n        return nn.Sequential(\n            nn.Linear(in_f, out_f),\n            nn.SiLU(),\n            nn.Dropout(0.3),\n        )    \nclass Net(nn.Module):\n    def __init__(self, n):\n        super(Net, self).__init__()\n        self.flatten = nn.Flatten()\n        self.fc1 = fc_block(n, 192)\n        self.fc2 = fc_block(192, 96)\n        self.fc3 = fc_block(96, 48)\n        self.out = nn.Sequential(\n            nn.Linear(48, 1),\n            nn.Sigmoid()\n        )    \n    def forward(self, x):\n        x = self.flatten(x)\n        x = self.fc1(x)\n        x = self.fc2(x)\n        x = self.fc3(x)\n        x = self.out(x)\n        return x\n\nnet_model = Net(len(FEATURES)).to(device)\nnet_model.apply(weights_init_uniform_rule)","17e0a0d5":"def batch_gd(model, train_loader, test_loader, epochs, val_score_best):\n    train_losses = np.zeros(epochs)\n    test_losses = np.zeros(epochs)\n    epochs_no_improve = 0\n    for it in range(epochs):\n        t0 = datetime.now()\n        model.train()\n        train_loss = []\n        train_roc = []\n        for batch_idx, (inputs, targets) in enumerate(train_loader):\n            # move data to GPU\n            inputs, targets = inputs.to(device), targets.to(device)\n            # zero the parameter gradients\n            optimizer.zero_grad()\n            # Forward pass\n            outputs = model(inputs)\n            targets = targets.unsqueeze(1)\n            loss = criterion(outputs, targets)\n            # Backward and optimize\n            loss.backward()\n            optimizer.step()\n            train_loss.append(loss.item())\n            train_roc.append(roc_auc_score(targets.cpu().data.numpy(), outputs.cpu().data.numpy()))\n            \n        else:\n            model.eval()\n            test_loss = []\n            test_roc = []\n            for inputs, targets in test_loader:\n                inputs, targets = inputs.to(device), targets.to(device)\n                outputs = model(inputs)\n                targets = targets.unsqueeze(1)\n                loss = criterion(outputs, targets)\n                test_loss.append(loss.item())\n                test_roc.append(roc_auc_score(targets.cpu().data.numpy(), outputs.cpu().data.numpy()))\n            #get train and test loss\n            test_loss = np.mean(test_loss)\n            train_loss = np.mean(train_loss)\n            lr_scheduler.step(test_loss)\n            ###    \n            print('learning_rate: {}'.format(optimizer.state_dict()['param_groups'][0]['lr']))\n            # Save losses\n            train_losses[it] = train_loss\n            test_losses[it] = test_loss\n            test_roc_auc = np.mean(test_roc)\n            # saving best weights\n            if test_loss < val_score_best:\n                epochs_no_improve = 0\n                val_score_best = test_loss\n                print(f'--- saving best weights ---')\n                torch.save(model.state_dict(), 'best_weights.pth')\n            else:\n                epochs_no_improve += 1\n            # getting the duration\n            dt = datetime.now() - t0\n            print(f'Epoch {it+1}\/{epochs}, Train Loss: {train_loss:.4f}, Train ROC: {(np.mean(train_roc)):.4f}, \\\n                    Test Loss: {test_loss:.4f}, Test ROC: {test_roc_auc:.4f}, Improvement: {epochs_no_improve}, Duration: {dt}')\n            if epochs_no_improve == 10:\n                print(f'Early Stopping..\\n')\n                break\n    return train_losses, test_losses","cb4591ac":"# garbage collection\ngc.collect()\n# creating and loading test data\ntest_dataset = CustomDataset(test_df)\ntest_loader = DataLoader(test_dataset, batch_size = 512)\n# defining folds dictionary\nfolds_train_losses = {}\nfolds_test_losses = {}\n# test data predictions\ntest_predictions = []\n# defining skfolds\nskf = StratifiedKFold(n_splits=5, random_state=47, shuffle=True)\nfor fold, (train_idx, val_idx) in enumerate(skf.split(train_df, y)):\n    X_train, y_train = train_df.iloc[train_idx], y.iloc[train_idx]\n    X_val, y_val = train_df.iloc[val_idx], y.iloc[val_idx]\n    \n    train_dataset = CustomDataset(X=X_train, y=y_train)\n    val_dataset = CustomDataset(X=X_val, y=y_val)\n    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE)\n    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n        \n    criterion = nn.BCELoss()    \n    optimizer = torch.optim.Adam(net_model.parameters(), lr=0.001)\n    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=2, \n                                                          verbose=True, min_lr=1e-7, mode='min')    \n    # training and validation\n    val_score_best = math.inf\n    train_losses, test_losses = batch_gd(net_model, train_loader, val_loader, EPOCHS, val_score_best)\n    folds_train_losses[fold] = train_losses\n    folds_test_losses[fold] = test_losses\n    \n    # loading best weights\n    # print(f'--- loading best weights ---')\n    # net_model.load_state_dict(torch.load('best_weights.pth'))\n    \n    # prediction on test data\n    test_preds = []\n    net_model.eval()\n    with torch.no_grad():\n        for idx, batch_tensor in enumerate(test_loader):\n            batch_tensor = batch_tensor.to(device)\n            preds = net_model(batch_tensor)\n            test_preds.extend(preds.cpu().detach().numpy())\n    test_predictions.append(test_preds)","a82ec7ee":"def average_loss_per_fold():\n    for (f_train, l_train), (f_test, l_test) in zip(folds_train_losses.items(), folds_test_losses.items()):\n        print(f'Fold: {f_train} \\t Average Train Loss: {np.mean(l_train)} \\t Average Test Loss: {np.mean(l_test)}')\n    \naverage_loss_per_fold()","2eb24d24":"for fold in range(0,5):\n    sns.set_style(\"darkgrid\", {\"axes.facecolor\": \".9\"})\n    fig, ax = plt.subplots(1, 1, tight_layout=True, figsize=(8,4))\n    plt.title(f'Fold: {fold+1}')\n    plt.subplot(1,1,1)\n    fold_df = pd.DataFrame(data=list(np.stack((folds_train_losses[fold], folds_test_losses[fold])).T),\n                     columns=['train_loss','val_loss'])\n    plt.plot(fold_df.loc[:, ['train_loss', 'val_loss']], label=fold_df.columns)\n    plt.xticks(np.arange(0,51,5))\n    plt.legend(fontsize=13)","3b410c0a":"tp_df=pd.DataFrame(data=[list(i) for i in zip(*test_predictions)], columns=['fold_1','fold_2','fold_3','fold_4','fold_5'])\ntp_df.head(4)","46b34c85":"test_predictions_v2 = copy.deepcopy(test_predictions)\nsub['target'] = np.mean(np.column_stack(test_predictions_v2), axis=1)\nsub.to_csv(\"submission.csv\",index=None)\nsub.head(5)","32ebe249":"* A uniform distribution has the equal probability of picking any number from a set of numbers.\n* The general rule for setting the weights in a neural network is to set them to be close to zero without being too small.","c78b7f97":"# Model","c96e13ee":"# Submission","ada32c1a":"# Losses and Plots","20921505":"# Target Distribution","8cb2c3e6":"# Imports","8be8f8fa":"# Preprocessing","c5966d6c":"# Training","fddacb80":"# Update\n* Added uniform weight initialization.\n* Loss graphs.\n* Early Stopping.","fcff18f4":"# Weights Init"}}