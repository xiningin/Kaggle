{"cell_type":{"eaf378a0":"code","13382539":"code","dc5ba478":"code","03bc3949":"code","0ba2a2d3":"code","33be58f7":"code","b4d2309c":"code","34c385ec":"code","e40c01a5":"code","5ef7ea87":"code","3b8ab99b":"code","d7acebc0":"code","d2ff5316":"code","7d30cf37":"code","f64ee911":"code","41f54217":"code","0c234f9f":"code","ddbe9e38":"code","b1ebb61a":"code","7d65669e":"code","61c1eaff":"code","be2e6217":"code","8526d713":"markdown","748bcd7c":"markdown","7bcb34b0":"markdown","24330631":"markdown","0a5bfe90":"markdown","f8279956":"markdown","39beea10":"markdown","f6be41f1":"markdown","891df6fd":"markdown","46c8271b":"markdown","f6f62033":"markdown","4a8a515e":"markdown"},"source":{"eaf378a0":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\n\nprint(\"TensorFlow version:\", tf.__version__)\n\n# set pandas\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 10)\npd.set_option('display.width', 1000)","13382539":"# When set to true, the network will use two months of data for validation.\n# Set to false to train on the full training set!\nvalidate_while_training = False","dc5ba478":"number_of_items = 30490\ntest_days = 28\nn_days_train = int(3.6*365) # Memory issues when training on more data\n\nif validate_while_training:\n    validation_days = test_days * 2  # (2 months for validation)\nelse:\n    validation_days = 0\n    \nlast_train_day = 1913 - validation_days # Last day used for training\nlast_val_day = last_train_day + validation_days # Last day used for validation\ndays_train_ini = last_train_day - n_days_train # First day used for training\n\ndays_back = 14 # Number of days to pass to the CNN (history)\ndays_predict = 1 # The network predicts one day at the time","03bc3949":"# Read the sales data\nsales_train_validation = pd.read_csv(\n    '\/kaggle\/input\/m5-forecasting-accuracy\/sales_train_evaluation.csv'\n)","0ba2a2d3":"# Transpose sales_train_validation\nsales_train_validation = sales_train_validation.T","33be58f7":"# Split the evaluation set on training, validation and test (test is here the kaggle validation set)\nsales_val = sales_train_validation.iloc[6 + last_train_day:6 + last_val_day, :]\nsales_test = sales_train_validation.iloc[6 + last_val_day:, :]\nsales_train = sales_train_validation.iloc[6 + days_train_ini:6 + last_train_day, :]\ndel sales_train_validation\nprint(sales_train.shape)\nprint(sales_val.shape)\nprint(sales_test.shape)","b4d2309c":"# Normalize the sales using MinMax\nscaler = MinMaxScaler(feature_range= (0,1))\nsales_train = scaler.fit_transform(sales_train.values)\nif validate_while_training:\n    sales_val = scaler.transform(sales_val.values)","34c385ec":"# Get the input for the first prediction of the model (i.e. day 1) \ninput_sales= sales_train[-days_back:, :]","e40c01a5":"calendar = pd.read_csv(\n    '\/kaggle\/input\/m5-forecasting-accuracy\/calendar.csv',\n)\n\n# drop the columns that are not used\ndrop_cols_cal = ['wm_yr_wk', 'date', 'weekday', 'month', 'year', 'wday']\ncalendar.drop(drop_cols_cal, axis='columns', inplace=True)","5ef7ea87":"# Make event_name_1 a categorical variable\ncalendar.loc[:, 'event_name_1'] = calendar.event_name_1.astype('category')","3b8ab99b":"# Add boolean for event 1\ncalendar['isevent1'] = 0\ncalendar.loc[~calendar.event_name_1.isna(), 'isevent1'] = 1\ncalendar.loc[calendar.event_name_1.isna(), 'isevent1'] = 0","d7acebc0":"# Separate training and test calendars\ncalendar_train = calendar.iloc[days_train_ini:last_train_day, :]\ncalendar_val = calendar.iloc[last_train_day:last_val_day, :]\ncalendar_test = calendar.iloc[last_val_day:, :]\ndel calendar\nprint(calendar_train.shape)\nprint(calendar_val.shape)\nprint(calendar_test.shape)","d2ff5316":"# Training\nX_train = []\nisevent1_train = []\ny_train = []\nfor i in range(days_back, last_train_day - days_train_ini - days_back):\n    X_train.append(sales_train[i-days_back:i])\n    isevent1_train.append(calendar_train.iloc[i:i+days_predict, -1].values)\n    y_train.append(sales_train[i:i+days_predict][0:number_of_items])","7d30cf37":"# Validation\nX_val = []\nisevent1_val = []\ny_val = []\nfor i in range(days_back, len(sales_val)):\n    X_val.append(sales_val[i-days_back:i])\n    isevent1_val.append(calendar_val.iloc[i:i+days_predict, -1].values)\n    y_val.append(sales_val[i:i+days_predict][0:number_of_items])","f64ee911":"# Convert to np array\nX_train = np.array(X_train)\nisevent1_train = np.array(isevent1_train)\ny_train = np.array(y_train)\nX_val = np.array(X_val)\nisevent1_val = np.array(isevent1_val)\ny_val = np.array(y_val)\n\nprint('Shape of X_train: ', X_train.shape)\nprint('Shape of isevent1_train: ', isevent1_train.shape)\nprint('Shape of y_train: ', y_train.shape)\nprint('Shape of X_val: ', X_val.shape)\nprint('Shape of isevent1_val: ', isevent1_val.shape)\nprint('Shape of y_val: ', y_val.shape)","41f54217":"def get_cnn_plus_dense(width=100, deep=False):\n    ''' \n    The width parameter sets the number of neurons on the dense layer just before the output\n    '''\n    # Categorical input\n    cat_input = tf.keras.Input(shape=(days_predict, 1))\n\n    # Sales branch\n    inputs_sales = tf.keras.Input(shape=(days_back, number_of_items))\n\n    sales = tf.keras.layers.Conv1D(256, 7, strides=1, activation='relu')(inputs_sales)\n    sales = tf.keras.layers.BatchNormalization()(sales)\n    sales = tf.keras.layers.Dropout(0.2)(sales)\n    sales = tf.keras.layers.Conv1D(512, 5, strides=1, activation='relu')(sales)\n    sales = tf.keras.layers.BatchNormalization()(sales)\n    sales = tf.keras.layers.AveragePooling1D(4)(sales)\n    sales = tf.keras.layers.Dropout(0.2)(sales)\n\n    # Concatenate + dense\n    concat = tf.keras.layers.Concatenate()([sales, cat_input])\n    dense = tf.keras.layers.Dense(width, activation='relu')(concat)\n    dense = tf.keras.layers.BatchNormalization()(dense)\n    if deep:\n        dense = tf.keras.layers.Dropout(0.2)(dense)\n        dense = tf.keras.layers.Dense(500, activation='relu')(dense)\n        dense = tf.keras.layers.BatchNormalization()(dense)\n        dense = tf.keras.layers.Dropout(0.2)(dense)\n        dense = tf.keras.layers.Dense(250, activation='relu')(dense)\n        dense = tf.keras.layers.BatchNormalization()(dense)\n        dense = tf.keras.layers.Dropout(0.2)(dense)\n        dense = tf.keras.layers.Dense(100, activation='relu')(dense)\n        dense = tf.keras.layers.BatchNormalization()(dense)\n\n    # Output branch\n    out = tf.keras.layers.Dense(number_of_items)(dense)\n\n    model = tf.keras.Model(inputs=[inputs_sales, cat_input], outputs=out)\n\n    opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n    model.compile(loss=\"mean_squared_error\", optimizer=opt)\n    \n    return model","0c234f9f":"# Get the relevant model and print summary and structure\nmodel = get_cnn_plus_dense(width=100, deep=False)\nprint(model.summary())\ntf.keras.utils.plot_model(model, show_shapes=True)","ddbe9e38":"# Fitting the network to the Training set\nepoch_no=25\nbatch_size=15\nif validate_while_training:\n    model.fit([X_train, isevent1_train], \n              y_train, \n              epochs = epoch_no, \n              batch_size = batch_size,\n              validation_data=([X_val, isevent1_val], y_val))\nelse:\n    model.fit([X_train, isevent1_train], \n              y_train, \n              epochs = epoch_no, \n              batch_size = batch_size)","b1ebb61a":"X_test = []\nisevent1_test = []\n\npredictions = []\n\nX_test.append(input_sales[0:days_back, :])\nX_test = np.array(X_test)\n\nfor j in range(days_back,days_back + test_days):\n    isevent1_test = np.expand_dims(np.array(calendar_test.iloc[j-days_back, -1]), axis=0)\n    test_input = [X_test[0,j - days_back:j].reshape(1, days_back, number_of_items),\n                  isevent1_test]\n    predicted_sales = model.predict(test_input)\n    testInput = np.array(predicted_sales)\n    testInput[testInput < 0] = 0\n    X_test = np.append(X_test, testInput).reshape(1, j + 1, number_of_items)\n    predicted_sales = scaler.inverse_transform(np.squeeze(testInput, axis=0))[:, 0:number_of_items]\n    predictions.append(predicted_sales)\n\npredictions = pd.DataFrame(data=np.array(predictions).reshape(28,30490))\nprint(predictions.shape)","7d65669e":"print('Predicted sales per item')\nprint(predictions.head(10))\nprint('Actual sales per item')\nprint(sales_test.head(10))","61c1eaff":"# Simple mse\nerror = mean_squared_error(sales_test.values, predictions.values, squared=True)\nprint(error)","be2e6217":"submission = pd.DataFrame(data=np.array(predictions).reshape(28,30490))\n\nsubmission[submission < 0] = 0 # Should not be any at this point, but just in case...\n\nsubmission = submission.T\n    \nsubmission = pd.concat((submission, submission), ignore_index=True)\n\nsample_submission = pd.read_csv(\"\/kaggle\/input\/m5-forecasting-accuracy\/sample_submission.csv\")\n    \nidColumn = sample_submission[[\"id\"]]\n    \nsubmission[[\"id\"]] = idColumn  \n\ncols = list(submission.columns)\ncols = cols[-1:] + cols[:-1]\nsubmission = submission[cols]\n\ncolsdeneme = [\"id\"] + [f\"F{i}\" for i in range (1,29)]\n\nsubmission.columns = colsdeneme\n\nsubmission.to_csv(\"\/kaggle\/working\/submission_avg_shallow_100.csv.csv\", index=False)","8526d713":"# Calendar data","748bcd7c":"In the following code snippet, a boolean is added to indicate whether a day is a special event as defined by event_name_1. This was inspired by the public notebook https:\/\/www.kaggle.com\/bountyhunters\/baseline-lstm-with-keras-0-7.","7bcb34b0":"## Visual comparison of the predictions with the actual sales","24330631":"# Generate the submission file\nThis piece of code was taken from the public notebook https:\/\/www.kaggle.com\/bountyhunters\/baseline-lstm-with-keras-0-7","0a5bfe90":"# Some constants","f8279956":"This is only relevant for the model testing (i.e. when we want to predict the sales in the kaggle validation period). We extract the last 14 days from the training data, since that is the input neede to predict the day 1 of the kaggle validation set.","39beea10":"The way to arrange the data is inspired by the notebook: https:\/\/www.kaggle.com\/bountyhunters\/baseline-lstm-with-keras-0-7. Parts of the code have been taken and adapted from the referred notebook.","f6be41f1":"# Construct the model\n\nAll models presented in the report are bult using the function *get_cnn_plus_dense()*. The function takes two optional parameters:\n1. The width of the dense layer before the output layer\n2. A boolean on whether the fully connected part of the model should be deep (true) or shallow (false)\n\nIn the report, the following combinations are presented:\n* CNN + dense layer: get_cnn_plus_dense(width=100, deep=False)\n* CNN + dense and wide layer: get_cnn_plus_dense(width=500, deep=False)\n* CNN + dense and narrow layer: get_cnn_plus_dense(width=50, deep=False)\n* CNN + 4 dense layers: get_cnn_plus_dense(width=500, deep=True)\n\n### Notes on model selection:\nParametric grid search was not possible, given the memory constrains of kaggle. The session had to be restarted before training each of the models, so the model selection was done manually. The effect of the following parameters was explored, using the validation error during training as a proxi for the model performance (*validate_while_training=True*): \n* depth and width of the convolutional layers\n* length of the kernels\n* effect of adding shortcuts\n* depth and width of the fully connected layers\n* max vs average pooling methods\n* Batch size\n* Number of epochs","891df6fd":"# Test the model\nThe code to test the model was taken and adapted from the public notebook https:\/\/www.kaggle.com\/bountyhunters\/baseline-lstm-with-keras-0-7 ","46c8271b":"# Sales data","f6f62033":"# Train the model","4a8a515e":"# Construct the training, validation and test data arrays"}}