{"cell_type":{"f2e7c45b":"code","d507780f":"code","915353a6":"code","1df6d1e6":"code","99f7b908":"code","fe56f2f5":"code","bd0f5dfa":"code","fbe6532c":"code","ca735f66":"code","00f3610c":"code","03e6002d":"code","c8093df5":"code","d9a05d9e":"code","d3c646da":"code","4e7e1ce0":"code","b5506e8c":"code","3a4de016":"code","78820629":"code","363b593c":"code","a25241b2":"code","b76c8525":"code","56c4d87e":"code","501b7b7c":"code","10275f82":"code","b952fb61":"code","7674569e":"code","11184ba2":"code","ec765e2d":"code","a7abd808":"code","3cad211a":"code","90c3df1a":"code","3045a11c":"code","0f0cea02":"code","ea41ff7d":"code","dca94251":"code","03b521c0":"code","4e66d2c6":"code","9b3064e7":"code","0ead9a0c":"code","7a8492f0":"code","b37e5df8":"code","b1d0149b":"code","59a49882":"code","a6557f26":"code","7016c66c":"code","1d23da52":"code","7ea7e0d1":"code","71b1e9a0":"code","e0a3cbe1":"markdown","f0f27fb2":"markdown","1fe32009":"markdown","08ead23b":"markdown","5943f241":"markdown","cf5a16cc":"markdown","a5f5cc81":"markdown","d5f76bce":"markdown","61f6223c":"markdown","e7e579ec":"markdown","33a21941":"markdown","eac82a40":"markdown","dc36c276":"markdown","ce28ed0a":"markdown","e2f7664d":"markdown"},"source":{"f2e7c45b":"import numpy as np \nimport pandas as pd \nimport re\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","d507780f":"import nltk\nfrom nltk import word_tokenize\nimport keras\nimport spacy\nspacy_nlp = spacy.load('en_core_web_lg')","915353a6":"train=pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest=pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\nsub=pd.read_csv('\/kaggle\/input\/nlp-getting-started\/sample_submission.csv')","1df6d1e6":"def conv_str(df):\n    if isinstance(df,pd.DataFrame):\n        df['text']=df['text'].astype(str)\n        return df\n    else:\n        raise TypeError(\"Expected type pd.DataFrame\")\ndef normalize(text):\n    if text is not None:\n        return text.lower()\n    else:\n        raise TypeError(\"Expected type str\")\ndef stop_words_removal(df):\n        for i in range(len(test)):\n            df['text'].iloc[i]=remove_stopwords(df['text'].iloc[i])\n        return df ['text']\ndef remove_stopwords(sentence):\n            import nltk\n            from nltk import word_tokenize\n            stop_words = stopwords = spacy.lang.en.stop_words.STOP_WORDS   \n            word_tokens = word_tokenize(sentence) \n            filtered_sentence =' '.join(map(str,[w for w in word_tokens if not w in stop_words]))\n            return filtered_sentence\ndef emoji_removal(df):\n        for i in range(len(test)):\n            df['text'].iloc[i]=remove_emoji(df['text'].iloc[i])\n        return df ['text']\ndef remove_emoji(text):\n        emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  #Emoji representations\n                               u\"\\U0001F300-\\U0001F5FF\"  \n                               u\"\\U0001F680-\\U0001F6FF\"  \n                               u\"\\U0001F1E0-\\U0001F1FF\"  \n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               \"]+\", flags=re.UNICODE)\n        return emoji_pattern.sub(r'', text)\ndef remove_characteristic(text):\n    twt=re.compile('^rt\\s?:$ |@+|#+') # rt : appeared in texts\n    return twt.sub(r'',text)\ndef remove_html(text):\n    html=re.compile(r'<.*?>|<!--.*?-->') #Some morons might add comments within tags\n    return html.sub(r'',text)\ndef remove_url(text):\n        url = re.compile(r'http[s]\\S+|www\\.\\S+') #This extremely incomprehensible regex is to prevent http : \n        return url.sub(r'',text)\ndef remove_extra_space(text):\n    exs=re.compile(r' {2,}')\n    return exs.sub(r'',text) \ndef remove_other(text):\n    regex=re.compile(r'[^a-z ]')\n    return regex.sub(r'',text)","99f7b908":"\"\"\"class NLP():\n    def __init__(self,train,test,sample_submission):\n        self.train=pd.read_csv(train)\n        self.test=pd.read_csv(test)\n        self.sample_submission=pd.read_csv(sample_submission)\n    def display(self,*args):\n        print(self.train.head(args[0]),self.test.head(args[1]),self.sample_submission.head(args[2]),sep='\\n')\n        nlp=NLP('\/kaggle\/input\/nlp-getting-started\/train.csv','\/kaggle\/input\/nlp-getting-started\/test.csv','\/kaggle\/input\/nlp-getting-started\/sample_submission.csv')\"\"\"","fe56f2f5":"def tokenize(dataframe,char_level=False):\n    if  isinstance(dataframe,pd.DataFrame) or isinstance(dataframe,pd.Series):\n        data=np.array(dataframe.values).ravel()\n        if char_level is not type(bool):\n                from keras.preprocessing.text import Tokenizer\n                if char_level==False:\n                    tk=Tokenizer(lower=True ,char_level=False)\n                    tk.fit_on_texts(data)\n                    text_sequences=tk.texts_to_sequences(data)\n                else:\n                    tk=Tokenizer(lower=True ,char_level=True)\n                    tk.fit_on_texts(data)\n                    text_sequences=tk.texts_to_sequences(data)  \n                return text_sequences,tk\n        else:\n            TypeError(\"Expected type bool got type {0}\".format(type(topx)))\n    else:\n        raise TypeError(\"Expected type pd.DataFrame got type {0}\".format(type(dataframe)))\n        \n\n        \ndef pad(x, length=None):\n    \"\"\"\n    Pad x\n    :param x: List of sequences.\n    :param length: Length to pad the sequence to.  If None, use length of longest sequence in x.\n    :return: Padded numpy array of sequences\n    \"\"\"\n    from keras.preprocessing.sequence import pad_sequences\n    return pad_sequences(x, maxlen=length, padding='post',value=0.0) #Pad at the end\n\n\ndef preprocess(x, y):\n    \"\"\"\n    Preprocess x and y\n    :param x: Feature List of sentences\n    :param y: Label List of sentences\n    :return: Tuple of (Preprocessed x, Preprocessed y, x tokenizer, y tokenizer)\n    \"\"\"\n    preprocess_x, x_tk = tokenize(x)\n    preprocess_y, y_tk = tokenize(y)\n    \n    preprocess_x = pad(preprocess_x)\n    preprocess_y = pad(preprocess_y)\n\n    # Keras's sparse_categorical_crossentropy function requires the labels to be in 3 dimensions\n    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n\n    return preprocess_x, preprocess_y, x_tk, y_tk\n\ndef words_distribution(dataframe,topx=10,stopwords=False,stopwordslist=None):\n    \"\"\"\n    Parameters\n    ----------\n    dataframe : pd.Dataframe\n        The dataframe containing text.\n    topx : int, optional\n        Display the top n words in the given distribution. The default is 10.\n    stopwords : bool, optional\n        Stopwords in various lagnuages. The default is False.\n    stopwordslist : TYPE, optional\n        DESCRIPTION. The default is None.\n\n    Raises\n    ------\n    TypeError\n        When wrong type of iterable is entered in the given fucntion\n\n    Returns\n    -------\n    None.\n    \"\"\"\n    \n    if  dataframe is not None:\n        if topx is not type(int):\n            from nltk import FreqDist\n            from nltk import word_tokenize\n            sens=[sen[0] for sen in dataframe.values if sen!=']' or sen!='[']\n            sens=[''.join(sen[0].lower()) for sen in dataframe.values if sen!=']' or sen!='[']\n            words=word_tokenize(str([sens[i] for i in range(len(sens))]))\n            freq=FreqDist(words)\n            freq.plot(topx)\n        else:\n                 raise TypeError(\"Expected type int got type {0}\".format(type(topx)))\n                \n    else:\n        raise TypeError(\"Expected type pd.DataFrame got type {0}\".format(type(dataframe)))","bd0f5dfa":"words_distribution(train['text'],10)","fbe6532c":"train=conv_str(train)\ntrain['text']=normalize(train['text'].str)\ntrain['text']=train['text'].apply(lambda x: remove_url(x))\ntrain['text']=train['text'].apply(lambda x: remove_html(x))\ntrain['text']=train['text'].apply(lambda x : remove_emoji(x))\ntrain['text']=train['text'].apply(lambda x : remove_stopwords(x))\ntrain['text']=train['text'].apply(lambda x:remove_other(x))\ntrain['text']=train['text'].apply(lambda x:remove_characteristic(x))\ntrain['text']=train['text'].apply(lambda x : remove_extra_space(x))\n\n\ntest=conv_str(test)\ntest['text']=normalize(test['text'].str)\ntest['text']=test['text'].apply(lambda x: remove_url(x))\ntest['text']=test['text'].apply(lambda x: remove_html(x))\ntest['text']=test['text'].apply(lambda x : remove_emoji(x))\ntest['text']=test['text'].apply(lambda x : remove_stopwords(x))\ntest['text']=test['text'].apply(lambda x:remove_other(x))\ntest['text']=test['text'].apply(lambda x:remove_characteristic(x))\ntest['text']=test['text'].apply(lambda x : remove_extra_space(x))","ca735f66":"test['text'].head(20)","00f3610c":"#remove_url('The link to this post is http:\/\/stackoverflow.com\/questions\/11331982\/how-to-remove-any-url-within-a-string-in-python')","03e6002d":"def remove_space(text):\n    regex=re.compile(r'%\\S?20')\n    return regex.sub(r' ',text)","c8093df5":"def append_location(x):\n    return ' '.join(train['location'].iloc[train.index[x]]+x)","d9a05d9e":"train['keyword'].fillna(value=\"None\",inplace=True)\ntrain['keyword']=train['keyword'].apply(lambda x :remove_space(x))\n#train['text']=train['keyword']+'is location'+train['text']\ntrain_id=train['id']\ntrain.drop(columns=['id','keyword','location'],axis=1,inplace=True)\ntrain['target']='__label__'+''+train['target'].astype(str)\n\ntest['keyword'].fillna(value=\"None\",inplace=True)\ntest['keyword']=test['keyword'].apply(lambda x :remove_space(x))\n#test['text']=test['keyword']+'is location '+test['text']\ntest_id=test['id']\ntest.drop(columns=['keyword','location'],axis=1,inplace=True)","d3c646da":"train['target']=train['target'].str.replace('1','disaster')\ntrain['target']=train['target'].str.replace('0','nodisaster')\n","4e7e1ce0":"#train['text']=train.text.str.encode('utf-8')\n#train['target']=train.target.str.encode('utf-8')\ntrain=train[['target','text']]","b5506e8c":"train.shape,test.shape","3a4de016":"\"\"\"from sklearn.model_selection import train_test_split\nfft_train,fft_test=train_test_split(train,test_size=0.3,random_state=42)\nfft_train.to_csv(r'fft_train.txt', index=False, sep=' ', header=False)\nfft_test.to_csv(r'fft_test.txt', index=False, sep=' ', header=False)\"\"\"","78820629":"train.to_csv(r'train.txt', index=False, sep=' ',header=False)\ntest.to_csv(r'test.txt', index=False, sep=' ',header=False)","363b593c":"n = (train.shape[0] * 7)\/10\nn = int(round(n))\n\n# Split the file into 70% train and 30% test\nfft_train = train[:n] \nfft_test = train[n:] \nprint(fft_train.shape, fft_test.shape)\nfft_train.to_csv(r'fft_train.txt', index=False, sep=' ', header=False)\nfft_test.to_csv(r'fft_test.txt', index=False, sep=' ', header=False)","a25241b2":"np=fft_train['text'].values\nlength=[]\nfor i in np:\n    length.append(len(i))\nfrom statistics import mean\nprint('Mean no. of words',mean(length))","b76c8525":"import fasttext\nimport fasttext.util","56c4d87e":"ftmodel = fasttext.train_supervised(input='\/kaggle\/working\/train.txt',label_prefix=\"__label__\",neg=5,epoch=10,dim=300,loss='hs',word_ngrams=2,ws=4,minn=2,maxn=6,pretrainedVectors='\/kaggle\/input\/wikinews300d1mvec\/wiki-news-300d-1M.vec')","501b7b7c":"#unsupervised_model = fasttext.train_supervised(input='\/kaggle\/working\/fft_train.txt',label_prefix=\"__label__\",lr=0.05,neg=2,epoch=200,dim=256,loss='softmax',word_ngrams=2,minn=2,maxn=6)","10275f82":"#ftmodel.get_output_matrix()[0].shape","b952fb61":"#ftmodel = fasttext.train_supervised(input='\/kaggle\/working\/train.txt',label_prefix=\"__label__\",neg=5,epoch=400,dim=300,loss='hs')","7674569e":"def print_results(N, p, r):\n    print(\"N\\t\" + str(N))\n    print(\"P@{}\\t{:.3f}\".format(1, p))\n    print(\"R@{}\\t{:.3f}\".format(1, r))\n\nftmodel.test('\/kaggle\/working\/train.txt')","11184ba2":"df_submit = test[['id','text']]\n#df_submit[df_submit['text'].str.isspace()==True]\n#df_submit.at[13 ,'text']='none'","ec765e2d":"ftmodel.predict('today is a good day')[0][0]","a7abd808":"embedding_matrix=ftmodel.get_output_matrix()[0]\n#embedding_matrix.shape","3cad211a":"#preproc_train_sentences, preproc_test_sentences, train_tokenizer, test_tokenizer =preprocess(fft_train['text'],fft_test['text'])\n","90c3df1a":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\n\n\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(fft_train['text'].values)\ntrainsequences = tokenizer.texts_to_sequences(fft_train['text'].values)\ntword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(tword_index))\ntraindata = pad_sequences(trainsequences,maxlen=31)\ntrain['target']=train['target'].str.replace('__label__nodisaster','0')\ntrain['target']=train['target'].str.replace('__label__disaster','1')\ntrainlabels=train['target'].iloc[:n]\n\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(fft_test['text'].values)\ntestsequences = tokenizer.texts_to_sequences(fft_test['text'].values)\ntesword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(tesword_index))\ntestdata = pad_sequences(testsequences, maxlen=31)\ntestlabels=train['target'].iloc[n:]\n","3045a11c":"\"\"\"indices = np.arange(data.shape[0])\nnp.random.shuffle(indices)\ndata = data[indices]\nlabels = labels[indices]\nnb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n\nx_train = data[:-nb_validation_samples]\ny_train = labels[:-nb_validation_samples]\nx_val = data[-nb_validation_samples:]\ny_val = labels[-nb_validation_samples:]\"\"\"","0f0cea02":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import LSTM ,Conv1D ,MaxPooling1D ,GlobalMaxPooling1D ,Dropout ,GlobalAveragePooling1D ,AveragePooling1D\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense\nimport numpy as np\nw= np.zeros((len(tword_index) + 1, 300))\n\nfor word, i in tword_index.items():\n    embedding_vector = ftmodel.get_word_vector(word)\n    if embedding_vector is not None:\n        # words not found in embedding index will be all-zeros.\n        w[i] = embedding_vector","ea41ff7d":"embeddings_index = {}\nf = open(os.path.join('\/kaggle\/input\/glove-global-vectors-for-word-representation\/', 'glove.6B.200d.txt'))\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\n\nprint('Found %s word vectors.' % len(embeddings_index))","dca94251":"wg= np.zeros((len(tword_index) + 1, 200))\n\nfor word, i in tword_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        # words not found in embedding index will be all-zeros.\n        wg[i] = embedding_vector","03b521c0":"# Preparing ","4e66d2c6":"import tensorflow\n#from tensorflow import keras\nfrom tensorflow import keras\nfrom keras import Model,Input\nfrom tensorflow.keras import optimizers ,layers,Model ,Input\nfrom tensorflow.keras.layers import Bidirectional ,Dense,Dropout,GRU, GaussianNoise ,BatchNormalization,TimeDistributed ,Bidirectional,LSTM,Conv1D,BatchNormalization,GlobalMaxPooling1D,Embedding,concatenate","9b3064e7":"from keras.utils.np_utils import to_categorical","0ead9a0c":"    #Embeddings\n    inputs=Input(shape=(31,), name=\"INPUT\")\n    ft_embedding=Embedding(len(tword_index)+1,w.shape[1],weights=[w],input_length=31,trainable=False)(inputs)\n    x1=Conv1D(filters=200,kernel_size=2, activation='relu', padding='valid')(ft_embedding)\n    x1=BatchNormalization()(x1)\n    x1=GlobalMaxPooling1D()(x1)\n\n\n    #x2=LSTM(200,)(ft_embedding)\n    #ftconcat=concatenate(inputs=[x1,x2])\n\n\n    gve_embedding=Embedding(len(tword_index)+1,wg.shape[1],weights=[wg],input_length=31,trainable=False)(inputs)\n    y1=Conv1D(filters=200,kernel_size=2, activation='relu', padding='valid')(gve_embedding)\n    y1=BatchNormalization()(y1)\n    y1=GlobalMaxPooling1D()(y1)\n\n    #y2=LSTM(200,)(gve_embedding)\n    #gveconcat=concatenate(inputs=[y1,y2])\n\n    finalconcat=concatenate(inputs=[x1,y1])\n    finalconcat=BatchNormalization()(finalconcat)\n    finalconcat=Dense(128,activation='relu')(finalconcat)\n    finalconcat=Dropout(0.4)(finalconcat)\n    finalconcat=BatchNormalization()(finalconcat)\n    finalconcat=Dense(1,activation='sigmoid')(finalconcat)\n    model=Model(inputs=[inputs],outputs=finalconcat)\n    model.compile(loss=tensorflow.keras.losses.SparseCategoricalCrossentropy(from_logits=True), optimizer=tensorflow.keras.optimizers.Adam(), metrics=['accuracy'])\n    model.summary()","7a8492f0":"keras.utils.plot_model(model)","b37e5df8":"#trainlabels=trainlabels.values\n#trainlabels=trainlabels.astype('int32')\n#testlabels=testlabels.values\n#testlabels=testlabels.astype('int32')\n","b1d0149b":"t=to_categorical(testlabels)","59a49882":"t1=to_categorical(trainlabels)","a6557f26":"trainlabels.values.shape","7016c66c":"model.fit(traindata, trainlabels.values,validation_data=(testdata,testlabels.values), epochs=1, batch_size=64)","1d23da52":"\"\"\"model = Sequential()\nmodel.add(Embedding(len(tword_index)+1,w.shape[1],weights=[w],input_length=31,trainable=False),)\n#model.add(GaussianNoise(0.9))\n#model.add(Conv1D(300, 2, activation='relu', padding='same'))\nmodel.add(BatchNormalization())\n#model.add(AveragePooling1D())\n#model.add(Conv1D(200, 3, activation='relu', padding='same'))\n#model.add(AveragePooling1D())\n#model.add(BatchNormalization())\n#model.add(GaussianNoise(-.4))\n#model.add(Conv1D(256, 7, activation='relu', padding='same'))\n#model.add(MaxPooling1D())\n#model.add(Conv1D(100, 4, activation='relu', padding='same'))\n#model.add(Conv1D(64, 4, activation='relu', padding='same'))\nmodel.add(GlobalAveragePooling1D())\n#model.add(Dropout(0.5))\n#model.add(Dense(64, activation='relu'))\n#model.add(Bidirectional(LSTM(128,activation='relu')))\n#model.add(Dropout(0.5))\n#model.add(Dense(64, activation='relu'))\n#model.add(Dropout(0.5))\nmodel.add(Dense(1, activation='sigmoid'),)\nmodel.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\nprint(model.summary())\"\"\"\n","7ea7e0d1":"#model.fit(traindata, trainlabels, validation_data=(testdata, testlabels), epochs=200, batch_size=128)","71b1e9a0":"df_submit = test[['id','text']]\ntargets=[]\nfor row in df_submit['text']:\n    pred_label=ftmodel.predict(row, k=-1)[0][0]\n    if (pred_label == '__label__nodisaster'):\n          pred_label = 0\n    else:\n          pred_label = 1\n    targets.append(pred_label)     \n\n# you add the list to the dataframe, then save the datframe to new csv\ndf_submit['target']=targets\ndf_submit=df_submit.drop(['text'],axis=1)\ndf_submit.to_csv('submission.csv',sep=',',index=False)","e0a3cbe1":"##### spacy stopwords list are larger compared to NLTK","f0f27fb2":"## Vizualization explaining fasText\n(Not mine credit to respective owner\/s)\n\n![FasText%20vizualization.jpg](attachment:FasText%20vizualization.jpg)","1fe32009":"# fasText model","08ead23b":"- I would suggest to use locally and clone the latest repo from Github ,which has features like automatic hyperparamater optimization","5943f241":"# Cleaning","cf5a16cc":"## Distribution of Data After Cleaning ","a5f5cc81":"# Load Files","d5f76bce":"# Submission","61f6223c":"## Understanding the hyperparameters:\n- **label_prefix **:The librarrty assumes a prefix to be added to classification labels\n- **lr**: The learning rate...\n- **neg **: Number of negative samples 2<neg<6\n- **epoch **: 5,10,15 works well incase of default (0.1) lr.\n- **dim **: 128,256 perform very well.\n- **loss**:softmax takes a bit longer ,hs  hierarchial softmax is good too,  ns not good score.\n- **word_ngrams**: 2=bigrams ,3=trigrams ,in this case limit it to 2 as per original paper + score_performance\n- **ws**: Size of **context window** ,here avg sentence length is not too large ,therefore we chose 3 based on experiments. \n- **bucket**: Hash length","e7e579ec":"So why fastText you may ask? Well, for starters it is fast and.\n- Provides enriched sub-word information which models such as Word2Vec may lack\n- Out of corpus words representations can be easily learned!\n- NOTE : fasText is NOT Windows compatible so beware.\n- It is CPU based model.\n- In order to tune hyperparameters pls read the paper:\n[Bag of Tricks for Efficient Text Classification](https:\/\/arxiv.org\/abs\/1607.01759) by the same person who made Word2Vec *Mikolov yeah*","33a21941":"Splitting with random shuffling makes fastText training worse","eac82a40":"I try to append location names with text to add meaning  (However this does not work very well)","dc36c276":"# Loading GloVe","ce28ed0a":"## Distribution of Data before Cleaning ","e2f7664d":"* Get ready for some cleaning ~~!"}}