{"cell_type":{"62a7ad95":"code","b9579399":"code","b45dfe71":"code","610abd1a":"code","337d897b":"code","184bb72d":"code","978c67bc":"code","05c9ca77":"code","0324fe13":"code","03815124":"code","27d265fa":"code","136dbf83":"code","1f500efe":"code","05b4f0c6":"code","c07e62a0":"code","3168d9fa":"code","cab99b60":"code","ee98ac0e":"code","a051fb20":"code","e82c09af":"markdown","295bf2fb":"markdown","9c8306b5":"markdown"},"source":{"62a7ad95":"import sys\nsys.path.append('..\/input\/iterative-stratification\/iterative-stratification-master')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold","b9579399":"import warnings\nwarnings.filterwarnings(\"ignore\")","b45dfe71":"import numpy as np\nimport pandas as pd\nimport pickle\nimport os, sys\nimport gc\nimport math\nimport random\nfrom tqdm import tqdm\nfrom typing import List, NoReturn, Union, Tuple, Optional, Text, Generic, Callable, Dict\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, QuantileTransformer\nfrom sklearn.model_selection import KFold, StratifiedKFold, TimeSeriesSplit\nfrom sklearn.decomposition import PCA\nimport umap\n\nimport lightgbm as lgb\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.metrics import log_loss\n\nfrom tqdm import tqdm\n\nimport math\n\n# visualize\nimport matplotlib.pyplot as plt\nimport matplotlib.style as style\nimport seaborn as sns\nfrom matplotlib_venn import venn2\nfrom matplotlib import pyplot\nfrom matplotlib.ticker import ScalarFormatter\nsns.set_context(\"talk\")\nstyle.use('seaborn-colorblind')\npd.options.display.max_columns = None\n\nimport warnings\nwarnings.filterwarnings('ignore')","610abd1a":"N_STARTS = 2\nN_SPLITS = 4\nSEED = 217\nVAR_THRESHOLD = 0.8\nNO_CTL = True\nN_COMPONENTS = [80, 10]\nPOSTPROCESS = True","337d897b":"%%time\ntrain_features = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\ntrain_targets = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv')\ntest_features = pd.read_csv('..\/input\/lish-moa\/test_features.csv')\n\nif NO_CTL:\n    print('no ctl')\n    train_features = train_features[train_features['cp_type']!='ctl_vehicle']\n    control_g = test_features['cp_type'] == 'ctl_vehicle'\n    test_g = test_features['cp_type'] != 'ctl_vehicle'\n    test_features = test_features[test_g]\n    train_targets = train_targets.iloc[train_features.index]\n    train_features.reset_index(drop=True, inplace=True)\n    test_features.reset_index(drop=True, inplace=True)\n    train_targets.reset_index(drop=True, inplace=True)\n    \nss = pd.read_csv('..\/input\/lish-moa\/sample_submission.csv')","184bb72d":"# variance threshold\ndata_all = pd.concat([train_features, test_features], ignore_index=True)\ncols_numeric = [feat for feat in list(data_all.columns) if feat not in ['sig_id', 'cp_type', 'cp_time', 'cp_dose']]\nmask = (data_all[cols_numeric].var() >= VAR_THRESHOLD).values\ntmp = data_all[cols_numeric].loc[:, mask]\ndata_all = pd.concat([data_all[['sig_id', 'cp_type', 'cp_time', 'cp_dose']], tmp], axis=1)","978c67bc":"def preprocess(df):\n    df = df.copy()\n    df.loc[:, 'cp_type'] = df.loc[:, 'cp_type'].map({'trt_cp': 0, 'ctl_vehicle': 1})\n    df.loc[:, 'cp_dose'] = df.loc[:, 'cp_dose'].map({'D1': 0, 'D2': 1})\n    df.loc[:, 'cp_time'] = df.loc[:, 'cp_time'].map({24: 0, 48: 0.5, 72: 1})\n#     df = pd.get_dummies(df, columns=['cp_type', 'cp_time','cp_dose'])\n    del df['sig_id']\n    return df\n\ndata_all = preprocess(data_all)\n\ndel train_targets['sig_id']","05c9ca77":"# categorize feats\ng_feats = [f for f in data_all.columns.values.tolist() if 'g-' in f]\nc_feats = [f for f in data_all.columns.values.tolist() if 'c-' in f]\ncp_feats = [f for f in data_all.columns.values.tolist() if 'cp_' in f]\nprint(len(g_feats), len(c_feats), len(cp_feats))","0324fe13":"# feature engineering\ndef q1(x):\n    return x.quantile(0.2)\n\ndef q2(x):\n    return x.quantile(0.8)\n\ndef calc_stats(df):\n    for stats in tqdm(['sum', 'mean', 'std', 'kurt', 'skew', 'max', 'min']):\n        df['g-'+stats] = getattr(df[g_feats], stats)(axis=1)\n        df['c-'+stats] = getattr(df[c_feats], stats)(axis=1)\n        df['gc-'+stats] = getattr(df[g_feats+c_feats], stats)(axis=1)\n    return df\n\ndata_all = calc_stats(data_all)","03815124":"%%time\n\n# dimensionality reduction\ndef dim_reducer(feats, n_components=N_COMPONENTS):\n    trans = PCA(n_components=n_components)\n    train_dist = trans.fit_transform(data_all[feats].values)\n    \n    return train_dist\n\ntrain_g = dim_reducer(g_feats, n_components=N_COMPONENTS[0])\ntrain_c = dim_reducer(c_feats, n_components=N_COMPONENTS[1])\n\nfor i in range(train_g.shape[1]):\n    data_all[f'g-pca{i+1}'] = train_g[:, i]\nfor i in range(train_c.shape[1]):\n    data_all[f'c-pca{i+1}'] = train_c[:, i]","27d265fa":"train = data_all.iloc[:len(train_features)]\nprint(train.shape)\ntrain.head()","136dbf83":"test = data_all.iloc[len(train_features):]\nprint(test.shape)\ntest.head()","1f500efe":"print(train_targets.shape)\ntrain_targets.tail()","05b4f0c6":"del train_features, test_features, data_all\ngc.collect()","c07e62a0":"feats = test.columns.values.tolist()\ndrops = ['sig_id', 'cp_type']\nfeats = [f for f in feats if f not in drops]\n\nprint('{:,} features'.format(len(feats)))\nprint(feats)","3168d9fa":"p_min = 0.001\np_max = 0.999\n\ndef metric(y_true, y_pred):\n    metrics = []\n    for _target in train_targets.columns:\n        metrics.append(log_loss(y_true.loc[:, _target], y_pred.loc[:, _target].astype(float), labels=[0,1]))\n    return np.mean(metrics)","cab99b60":"params = {\n    'n_estimators': 24000,\n    'objective': 'binary',\n    'boosting_type': 'gbdt',\n    'max_depth': 3,\n    'learning_rate': 0.08,\n    'subsample': 0.72,\n    'subsample_freq': 4,\n    'feature_fraction': 0.4,\n    'lambda_l1': 1,\n    'lambda_l2': 1,\n    'seed': SEED,\n    'early_stopping_rounds': 40,\n    }    \nparams[\"metric\"] = \"binary_logloss\" # other candidates: binary_logloss\n# params[\"is_unbalance\"] = True # assume unbalanced data\n\ndef fit_lgb_kfold(train, train_targets, test, features, target, n_splits=N_SPLITS, random_state=SEED):    \n    oof = np.zeros(train.shape[0])\n    y_preds = np.zeros(test.shape[0])\n    fi = pd.DataFrame()\n    fi['features'] = features\n    fi['importance'] = 0\n    params['seed'] = SEED * (random_state+1)\n    \n    cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n    for n, (train_idx, valid_idx) in enumerate(cv.split(train, train_targets[target])):\n        # train test split\n        x_train_train = train[features].iloc[train_idx]\n        y_train_train = train_targets[target].iloc[train_idx]\n        x_train_valid = train[features].iloc[valid_idx]\n        y_train_valid = train_targets[target].iloc[valid_idx]\n\n        # lgb dataset\n        lgb_train = lgb.Dataset(data=x_train_train, label=y_train_train)\n        lgb_valid = lgb.Dataset(data=x_train_valid, label=y_train_valid)\n\n        # fit\n        model = lgb.train(params, lgb_train, valid_sets=lgb_valid, verbose_eval=0)\n        fi['importance'] += model.feature_importance(importance_type=\"gain\") \/ N_SPLITS\n        \n        # save mod?p(model, open(f'model_{random_state}_{n}_{target}.pkl', 'wb'))                \n    \n        # predict\n        oof[valid_idx] = model.predict(x_train_valid, num_iteration=model.best_iteration)\n        y_preds += model.predict(test[features]) \/ N_SPLITS\n        \n    score = log_loss(train_targets[target], oof)\n    print('LogLoss Score:', score)\n\n#     model = pickle.load(open(f'model_{seed}_{n}_{targ}.pkl', 'rb'))\n    return y_preds, oof, score","ee98ac0e":"res = train_targets.copy()\nss.loc[:, train_targets.columns] = 0\nres.loc[:, train_targets.columns] = 0\n\nfor seed in range(N_STARTS):\n    res_seed = res.copy()\n    ss_seed = ss.copy()\n    for targ in tqdm(train_targets.columns):\n        print('Target = {}'.format(targ))\n        y_pred, oof, score = fit_lgb_kfold(train, train_targets, test, feats, targ, n_splits=N_SPLITS, random_state=seed)\n        res_seed[targ] = oof\n        ss_seed.loc[test_g, targ] = y_pred\n    \n    print(f'OOF Metric For SEED {seed}: {metric(train_targets, res_seed)}')\n    for targ in train_targets.columns:\n        res[targ] += res_seed[targ].values \/ N_STARTS\n        ss.loc[test_g, targ] += ss_seed.loc[test_g, targ].values \/ N_STARTS","a051fb20":"# if DO == 'training':\n#     print(f'OOF Metric: {metric(train_targets, res)}')\n    \n# elif DO == 'inference':\nprint(f'OOF Metric: {metric(train_targets, res)}')\n\nif POSTPROCESS:\n    print('post-process...')\n\n    # clip\n    ss.iloc[:,1:] = np.clip(ss.values[:, 1:], p_min, p_max)\n\n    # Set ctl_vehicle to 0\n    ss.iloc[control_g, 1:] = 0\nss.to_csv('submission.csv', index=False)","e82c09af":"Kernel still under modification.. <span style='color:red'>**Feedback**<\/span> is also very much appreciated.\nPls <span style='color:red'>**UPVOTE**<\/span>, if you find it useful. \n","295bf2fb":"# Load data","9c8306b5":"# Libraries"}}