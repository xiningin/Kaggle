{"cell_type":{"c41de886":"code","e0ad26d8":"code","9775e1bb":"code","b7aac668":"code","67727f2e":"code","7442e4a0":"code","a29376a5":"code","484c8a14":"code","0a0c44c7":"code","3ba0e175":"code","cfa39305":"code","f087f708":"code","58274b09":"code","f862946a":"code","c5fce2b2":"code","560ece3c":"code","70b1d0d2":"code","58e8c52b":"code","dc170ed4":"code","75a8c808":"code","88a4094f":"code","0cdfa027":"code","3b643eee":"code","b8713a14":"code","cb9e0042":"code","73790052":"code","b0b2c33c":"code","6dc60280":"code","ecb5121e":"code","80074f66":"code","4ebfb7e5":"code","1828ec3d":"code","b5ea963d":"code","24d63b74":"code","dc266d14":"code","96f61998":"code","2af90521":"code","c9b35805":"code","b9df490b":"code","03b8e09a":"code","9a522a4b":"code","d2425307":"code","817b675c":"code","e05d768c":"code","d6b8f37a":"code","6e79c597":"code","777771bf":"code","40e36674":"code","7d5d91bd":"code","9ab9e6ec":"code","0e78931e":"code","0f2ad805":"code","39f132ed":"code","7f26815d":"code","48dd24f5":"code","cb39e683":"code","1b629135":"code","a40aab02":"code","855eeb36":"code","90b718fb":"markdown","684a2ce1":"markdown","bd302b1a":"markdown","24e05d52":"markdown","fc85e212":"markdown","5908387f":"markdown","0cc4a78e":"markdown","58abb3fb":"markdown","66ed1b6d":"markdown","0a28ec70":"markdown","e1064ee4":"markdown","958c1caf":"markdown","84067b41":"markdown","586d1ca1":"markdown"},"source":{"c41de886":"from IPython.core.display import display, HTML\nfrom IPython.display import Image\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F","e0ad26d8":"use_gpu = True\nif use_gpu:\n    assert torch.cuda.is_available(), 'You either do not have a GPU or is not accessible to PyTorch'","9775e1bb":"Image(\"https:\/\/pouannes.github.io\/fastai-callbacks\/fastai_training_loop_vanilla.png\",height=400,width=600)","b7aac668":"# def train(train_dl, model, epoch, opt, loss_func):\n#   for _ in range(epoch):\n#     model.train() # model in training mode\n#     for xb,yb in train_dl: # loop through the batches \n#       out = model(xb) # forward pass \n#       loss = loss_func(out, yb) # compute the loss\n#       loss.backward() # backward pass gradient \n#       opt.step() # update the gradient \n#       opt.zero_grad() ","67727f2e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","7442e4a0":"def sigmoid(x):\n    return 1 \/ (1 + np.exp(-x))","a29376a5":"from torchtext.data import Field\ntokenize = lambda x: x.split()\ntxt_field = Field(sequential=True, tokenize=tokenize, lower=True)\n\nlabel_field = Field(sequential=False, use_vocab=False)","484c8a14":"train=pd.read_csv('\/kaggle\/input\/usinlppracticum\/imdb_train.csv')\ntrain.head()","0a0c44c7":"train['sentiment']=train['sentiment'].map({'negative':0,'positive':1})","3ba0e175":"print({'negative':0,'positive':1})","cfa39305":"train.head()","f087f708":"train.to_csv('train.csv',index=False)","58274b09":"from torchtext.data import TabularDataset\ntv_datafields = [\n                 (\"review\", txt_field), (\"sentiment\", label_field)]\n\ntrn = TabularDataset( path=\"train.csv\", # the root directory where the data lies\n#                train='imdb_train.csv',\n               format='csv',\n               skip_header=True, # if your csv header has a header, make sure to pass this to ensure it doesn't get proceesed as data!\n               fields=tv_datafields)","f862946a":"trn,val = trn.split(split_ratio=0.9, stratified=False, strata_field='sentiment', random_state=None)","c5fce2b2":"pd.read_csv(\"\/kaggle\/input\/usinlppracticum\/imdb_test.csv\").head()","560ece3c":"tst_datafields = [(\"id\", None), # we won't be needing the id, so we pass in None as the field\n                 (\"review\", txt_field)\n                 ]\ntst = TabularDataset(\n        path=\"\/kaggle\/input\/usinlppracticum\/imdb_test.csv\", # the file path\n        format='csv',\n        skip_header=True, # if your csv header has a header, make sure to pass this to ensure it doesn't get proceesed as data!\n        fields=tst_datafields)","70b1d0d2":"len(trn), len(val),len(tst)","58e8c52b":"trn.fields.items()","dc170ed4":"ex = trn[1]\ntype(ex)","75a8c808":"trn[0].__dict__.keys()","88a4094f":"trn[0].__dict__['review'][:10]","0cdfa027":"ex.review","3b643eee":"ex.sentiment","b8713a14":"from torchtext import vocab\n# specify the path to the localy saved vectors\nvec = vocab.Vectors('\/kaggle\/input\/glove6b\/glove.6B.300d.txt',cache= '\/kaggle\/working')\n# build the vocabulary using train and validation dataset and assign the vectors\n# print('creating Vocab')\n","cb9e0042":"txt_field.build_vocab(trn, max_size=60000, vectors=vec)\n# build vocab for labels\nlabel_field.build_vocab(trn)","73790052":"pre_trained_vectors=txt_field.vocab.vectors\npre_trained_vectors.shape","b0b2c33c":"txt_field.vocab.vectors[txt_field.vocab.stoi['the']].shape","6dc60280":"txt_field.vocab.freqs.most_common(14)","ecb5121e":"type(txt_field.vocab.itos), type(txt_field.vocab.stoi), len(txt_field.vocab.itos), len(txt_field.vocab.stoi.keys()),","80074f66":"txt_field.vocab.stoi['and'], txt_field.vocab.itos[4]","4ebfb7e5":"from torchtext.data import Iterator,BucketIterator\ntraindl, valdl = BucketIterator.splits(datasets=(trn, val), # specify train and validation Tabulardataset\n                                            batch_sizes=(64,64),  # batch size of train and validation\n                                            sort_key=lambda x: len(x.review), # on what attribute the text should be sorted\n                                            device=None, # -1 mean cpu and 0 or None mean gpu\n                                            sort_within_batch=False, \n                                            repeat=False)","1828ec3d":"test_iter = Iterator(tst, batch_size=64, sort=False, sort_within_batch=False, repeat=False)","b5ea963d":"print(len(traindl), len(valdl))","24d63b74":"batch = next(iter(traindl)) # BucketIterator return a batch object\nprint(type(batch))","dc266d14":"print(batch.sentiment) # labels of the batch\n# tensor([ 0,  0,  0], device='cuda:0')","96f61998":"print(batch.review.shape) # text index and length of the batch","2af90521":"class BatchGenerator:\n    def __init__(self, dl, x_field, y_field):\n        self.dl, self.x_field, self.y_field = dl, x_field, y_field\n        \n    def __len__(self):\n        return len(self.dl)\n    \n    def __iter__(self):\n        for batch in self.dl:\n            X = getattr(batch, self.x_field)\n            if self.y_field is not None:\n                y = getattr(batch, self.y_field)\n            else:\n                y = torch.zeros((1))\n            y= torch.tensor(y[:, np.newaxis], dtype=torch.float32)\n            if use_gpu:\n                yield (X.cuda(), y.cuda())\n            else:\n                yield (X, y)\n          ","c9b35805":"train_batch_it = BatchGenerator(traindl, 'review', 'sentiment')\nval_batch_it = BatchGenerator(valdl, 'review', 'sentiment')\ntest_dl = BatchGenerator(test_iter, \"review\", None)\n# print(next(iter(train_batch_it))shape)","b9df490b":"vocab_size = len(txt_field.vocab)\nn_out = 1\nvocab_size","03b8e09a":"trn.fields['review'].vocab.vectors.shape","9a522a4b":"# class SimpleLSTMBaseline(nn.Module):\n#     def __init__(self, hidden_dim, emb_dim=300,pretrained_vec=None,\n#                  spatial_dropout=0.05, recurrent_dropout=0.1, num_linear=2):\n#         super().__init__() # don't forget to call this!\n#         self.embedding = nn.Embedding(len(txt_field.vocab), emb_dim)\n#         self.embedding.weight.data.copy_(pretrained_vec) # load pretrained vectors\n#         self.embedding.weight.requires_grad = False # make embedding non trainable\n#         self.embedding_dropout = nn.Dropout2d(0.1)\n#         self.lstm_1 = nn.LSTM(emb_dim, hidden_dim, bidirectional=True)\n#         self.lstm_2 = nn.LSTM(hidden_dim*2, hidden_dim, bidirectional=True)\n#         self.linear = nn.Linear(hidden_dim, hidden_dim\/\/4)\n#         self.relu = nn.ReLU()\n#         self.dropout = nn.Dropout(0.1)\n#         self.out = nn.Linear(hidden_dim\/\/4, 1)\n# #         self.predictor = nn.Linear(hidden_dim, 1)\n    \n#     def forward(self, seq):\n#         h_embedding = self.embedding(x)\n#         hdn, _ = self.encoder(self.embedding(seq))\n#         feature = hdn[-1, :, :]\n#         for layer in self.linear_layers:\n#             feature = layer(feature)\n#         preds = self.predictor(feature)\n#         return preds","d2425307":"class SimpleLSTM(nn.Module):\n    def __init__(self):\n        super(SimpleLSTM, self).__init__()\n        \n        hidden_size = 128\n        max_features,embed_size=60002,300\n        self.embedding = nn.Embedding(max_features, embed_size)\n        self.embedding.weight = nn.Parameter(torch.tensor(pre_trained_vectors, dtype=torch.float32))\n        self.embedding.weight.requires_grad = False\n        \n#         self.embedding_dropout = nn.Dropout2d(0.1)\n        self.lstm_1 = nn.LSTM(embed_size, hidden_size, bidirectional=True)\n        self.lstm_2 = nn.LSTM(hidden_size*2, hidden_size, bidirectional=True)\n        self.linear = nn.Linear(hidden_size*6, 16)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(0.1)\n        self.out = nn.Linear(16, 1)\n    def forward(self, x):\n        h_embedding = self.embedding(x)\n#             h_embedding = torch.squeeze(self.embedding_dropout(torch.unsqueeze(h_embedding, 0)))\n\n        h_lstm_1, _ = self.lstm_1(h_embedding)\n        h_lstm_2, _ = self.lstm_2(h_lstm_1)\n\n        avg_pool = torch.mean(h_lstm_1, 0)\n        max_pool, _ = torch.max(h_lstm_2, 0)\n\n        conc = torch.cat((h_lstm_2[-1,:,:], avg_pool, max_pool), 1)# 128*2, 128*2, 128*2\n        conc = self.relu(self.linear(conc))\n        conc = self.dropout(conc)\n        out = self.out(conc)\n\n        return out","817b675c":"x,y=next(iter(train_batch_it))\nx.shape","e05d768c":"m=SimpleLSTM()\nm.cuda()\nm","d6b8f37a":"em_sz = 300\n# nh = 256\/\nmodel = SimpleLSTM()\nif use_gpu:\n    model = model.cuda()\nprint(model)","6e79c597":"def model_size(model: torch.nn)->int:\n    \"\"\"\n    Calculates the number of trainable parameters in any model\n    \n    Returns:\n        params (int): the total count of all model weights\n    \"\"\"\n    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n#     model_parameters = model.parameters()\n    params = sum([np.prod(p.size()) for p in model_parameters])\n    return params\n\nprint(f'{model_size(model)\/10**6} million parameters')","777771bf":"from torch import optim\nopt = optim.Adam(model.parameters(), lr=1e-2)\nloss_func = nn.BCEWithLogitsLoss().cuda()\nepochs = 10","40e36674":"next(iter(traindl))","7d5d91bd":"model=SimpleLSTM()\nmodel.cuda()","9ab9e6ec":"x.shape","0e78931e":"x,y=next(iter(train_batch_it))\nmodel(x.cuda()).shape","0f2ad805":"Image(\"https:\/\/pouannes.github.io\/fastai-callbacks\/fastai_training_loop_vanilla.png\",height=400,width=600)","39f132ed":"\nfrom tqdm import tqdm\nfor epoch in tqdm(range(1, epochs + 1)):\n    running_loss = 0.0\n    model.train() # turn on training mode\n    for x, y in train_batch_it: # thanks to our wrapper, we can intuitively iterate over our data!\n        preds = model(x)\n#         print(preds.dtype,y.dtype)\n        loss = loss_func(preds, y)\n        loss.backward()\n        opt.step()\n        opt.zero_grad()\n        #---\n        running_loss += loss.item() * x.size(0)\n    epoch_loss = running_loss \/ len(trn)\n    # calculate the validation loss for this epoch\n    val_loss = 0.0\n    val_acc=0\n    model.eval() # turn on evaluation mode\n    with torch.no_grad():\n        for x, y in val_batch_it:\n            preds = model(x)\n            y_pred = sigmoid(preds.detach().cpu().numpy()).astype(int)\n            loss = loss_func(preds, y)\n            val_loss += loss.item() * x.size(0)\n#             out = (sigmoid(preds>0.5))\n            \n            val_acc += (y_pred == y.cpu().numpy().astype(int)).sum().item()\n        val_loss \/= len(val)\n        val_acc=val_acc\/len(val)\n    print('Epoch: {}, Training Loss: {:.4f}, Validation Loss: {:.4f},val_acc:{:.4f}'.format(epoch, epoch_loss, val_loss,val_acc))","7f26815d":"next(iter(test_dl))","48dd24f5":"model.eval()","cb39e683":"with torch.no_grad():\n    test_preds = []\n    for x, y in tqdm(test_dl):\n        preds = model(x)\n        # if you're data is on the GPU, you need to move the data back to the cpu\n        # preds = preds.data.cpu().numpy()\n        preds = preds.data.cpu().numpy()\n        # the actual outputs of the model are logits, so we need to pass these values to the sigmoid function\n        preds = 1 \/ (1 + np.exp(-preds))\n    #     print(preds.shape)\n        test_preds.append(preds[:,0])\n    test_preds = np.hstack(test_preds)","1b629135":"test_df = pd.read_csv(\"\/kaggle\/input\/usinlppracticum\/imdb_test.csv\")\ntest_df['sentiment'] = np.where(test_preds>0.5,'positive','negative')\n","a40aab02":"test_df.head()","855eeb36":"test_df.sentiment.value_counts()","90b718fb":"### loading pretrained Vectors & Creating Vocab","684a2ce1":"* `Preprocessing and tokenization`\n* `Generating vocabulary of unique tokens and converting words to indices (Numericalization)`\n* `Loading pretrained vectors e.g. Glove, Word2vec, Fasttext`\n* Padding text with zeros in case of variable lengths\n* Dataloading and batching\n* Model creation and training\n* Prediction using trained Model\n","bd302b1a":"### Iterators\n* `Padding text with zeros in case of variable lengths`\n* `Dataloading and batching`","24e05d52":"### Sigmoid Function","fc85e212":"### Typical components of deep learning approach for NLP\n* `Preprocessing and tokenization`\n* `Generating vocabulary of unique tokens and converting words to indices (Numericalization)`\n* `Loading pretrained vectors e.g. Glove, Word2vec, Fasttext`\n* `Padding text with zeros in case of variable lengths`\n* `Dataloading and batching`\n* `Model creation and training`\n* `Prediction using trained Model`","5908387f":"### train and validation split ","0cc4a78e":"### Training a Text Classifier- LSTM","58abb3fb":"#### Data Loaders with torchtext","66ed1b6d":"### Model Architecture","0a28ec70":"### Prediction using trained Model","e1064ee4":"### Exploring the Dataset Objects","958c1caf":"## Text Classification- Deep Learning Approach\n*author: Vikas Kumar (vikkumar@deloitte.com)*\n\nReferences:\n* https:\/\/torchtext.readthedocs.io\/en\/latest\/vocab.html\n* https:\/\/pytorch.org\/tutorials\/beginner\/deep_learning_nlp_tutorial.html","84067b41":"### optimizer and loss function ","586d1ca1":"This is good <PAD> <PAD>\nThis is good and awesome"}}