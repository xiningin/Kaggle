{"cell_type":{"9a5b7dec":"code","0fff91b2":"code","c0239114":"code","84e7d067":"code","9ba7ba9d":"code","892e562f":"code","6fa19ccb":"code","44c73838":"code","a33b3c90":"code","2f1887c7":"code","df1d12f2":"code","e674e959":"code","528269bf":"code","fb7091fa":"code","eb723f16":"code","de9b2128":"code","0d9ce72e":"markdown","f60a7eeb":"markdown","fe6c885b":"markdown","91f25950":"markdown","95789b4e":"markdown","ab07883b":"markdown","5e463d45":"markdown","1628adbf":"markdown"},"source":{"9a5b7dec":"# install ktrain on Google Colab\n!pip3 install ktrain","0fff91b2":"# import ktrain and the ktrain.text modules\nimport ktrain\nfrom ktrain import text","c0239114":"ktrain.__version__","84e7d067":"# fetch the dataset using scikit-learn\ncategories = ['alt.atheism', 'soc.religion.christian',\n             'comp.graphics', 'sci.med']\nfrom sklearn.datasets import fetch_20newsgroups\ntrain_b = fetch_20newsgroups(subset='train',\n   categories=categories, shuffle=True, random_state=42)\ntest_b = fetch_20newsgroups(subset='test',\n   categories=categories, shuffle=True, random_state=42)\n\nprint('size of training set: %s' % (len(train_b['data'])))\nprint('size of validation set: %s' % (len(test_b['data'])))\nprint('classes: %s' % (train_b.target_names))\n\nx_train = train_b.data\ny_train = train_b.target\nx_test = test_b.data\ny_test = test_b.target","9ba7ba9d":"(x_train,  y_train), (x_test, y_test), preproc = text.texts_from_array(x_train=x_train, y_train=y_train,\n                                                                       x_test=x_test, y_test=y_test,\n                                                                       class_names=train_b.target_names,\n                                                                       preprocess_mode='bert',\n                                                                       maxlen=350, \n                                                                       max_features=35000)","892e562f":"# you can disregard the deprecation warnings arising from using Keras 2.2.4 with TensorFlow 1.14.\nmodel = text.text_classifier('bert', train_data=(x_train, y_train), preproc=preproc)\nlearner = ktrain.get_learner(model, train_data=(x_train, y_train), batch_size=6)","6fa19ccb":"learner.fit_onecycle(2e-5, 4)","44c73838":"learner.validate(val_data=(x_test, y_test), class_names=train_b.target_names)","a33b3c90":"predictor = ktrain.get_predictor(learner.model, preproc)","2f1887c7":"predictor.get_classes()","df1d12f2":"predictor.predict(test_b.data[0:1])","e674e959":"# we can visually verify that our prediction of 'sci.med' for this document is correct\nprint(test_b.data[0])","528269bf":"# we predicted the correct label\nprint(test_b.target_names[test_b.target[0]])","fb7091fa":"# let's save the predictor for later use\npredictor.save('\/tmp\/my_predictor')","eb723f16":"# reload the predictor\nreloaded_predictor = ktrain.load_predictor('\/tmp\/my_predictor')","de9b2128":"# make a prediction on the same document to verify it still works\nreloaded_predictor.predict(test_b.data[0:1])","0d9ce72e":"# Multiclass Text Classification Using BERT and Keras\nIn this example, we will use ***ktrain*** ([a lightweight wrapper around Keras](https:\/\/github.com\/amaiya\/ktrain)) to build a model using the dataset employed in the **scikit-learn** tutorial: [Working with Text Data](https:\/\/scikit-learn.org\/stable\/tutorial\/text_analytics\/working_with_text_data.html).  As in the tutorial, we will sample 4 newsgroups to create a relatively small multiclass text classification dataset.  The objective is to accurately classify each document into one of these four newsgroup topic categories.  This will provide us an opportunity to see **BERT** in action on a relatively smaller training set.  Let's fetch the [20newsgroups dataset ](http:\/\/qwone.com\/~jason\/20Newsgroups\/) using scikit-learn.","f60a7eeb":"## STEP 1:  Load and Preprocess the Data\nPreprocess the data using the `texts_from_array function` (since the data resides in an array).\nIf your documents are stored in folders or a CSV file you can use the `texts_from_folder` or `texts_from_csv` functions, respectively.","fe6c885b":"source : https:\/\/colab.research.google.com\/drive\/1ixOZTKLz4aAa-MtC6dy_sAvc9HujQmHN#scrollTo=x631IAqvIlDg","91f25950":"We can use the `learner.validate` method to test our model against the validation set.\nAs we can see, BERT achieves a **96%** accuracy, which is quite a bit higher than the 91% accuracy achieved by SVM in the [scikit-learn tutorial](https:\/\/scikit-learn.org\/stable\/tutorial\/text_analytics\/working_with_text_data.html).","95789b4e":"## How to Use Our Trained BERT Model\n\nWe can call the `learner.get_predictor` method to obtain a Predictor object capable of making predictions on new raw data.","ab07883b":"## STEP 2:  Load the BERT Model and Instantiate a Learner object","5e463d45":"## STEP 3: Train the Model\n\nWe train using one of the three learning rates recommended in the BERT paper: *5e-5*, *3e-5*, or *2e-5*.\nAlternatively, the ktrain Learning Rate Finder can be used to find a good learning rate by invoking `learner.lr_find()` and `learner.lr_plot()`, prior to training.\nThe `learner.fit_onecycle` method employs a [1cycle learning rate policy](https:\/\/arxiv.org\/pdf\/1803.09820.pdf).\n\n","1628adbf":"The `predictor.save` and `ktrain.load_predictor` methods can be used to save the Predictor object to disk and reload it at a later time to make predictions on new data."}}