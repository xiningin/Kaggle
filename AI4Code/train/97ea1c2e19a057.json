{"cell_type":{"f3199db3":"code","35952069":"code","14fd4fe9":"code","34833738":"code","63ddce13":"code","5ced9aee":"code","2fd2c7a9":"code","01dd4fc7":"code","7caaf8ba":"code","5d25295e":"code","8046e43b":"code","69dd172a":"code","e523bbde":"code","aeda4a4e":"code","f849c382":"code","be7d2739":"code","b7fc2fb2":"code","cfa3bf48":"code","5f636bcb":"code","f85a9c94":"code","f8ad058a":"code","671de9d8":"code","e067572c":"code","6ebe5c8d":"code","0c435019":"code","bb5fc1a6":"code","790f5150":"code","ef9cd7e3":"code","ff2f3683":"code","b83dabec":"code","8a88de57":"code","945bd86b":"code","08acd0dd":"code","5159fccd":"markdown","3c2ea26d":"markdown","619d84f7":"markdown","150fab3f":"markdown","a2416ba9":"markdown","1261ef85":"markdown","059f016c":"markdown","69637488":"markdown","5b338ff9":"markdown","42fc4a08":"markdown","a770c96b":"markdown","9d19a1d4":"markdown","b374caba":"markdown","528ac024":"markdown","84c8317d":"markdown","c80c892e":"markdown","75c5bd4d":"markdown","030608fd":"markdown","8c67298a":"markdown","b64a3735":"markdown","12a7fb50":"markdown","d9e32490":"markdown","8008ea8c":"markdown"},"source":{"f3199db3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","35952069":"!pwd","14fd4fe9":"os.chdir('..')\nos.listdir()","34833738":"import os, shutil\n\n#E\u011fitilecek veri seti\noriginal_dataset_dir = 'input\/train'\n#Modelleri kaydetmek i\u00e7in\nbase_dir = 'pens_and_books_small'\nos.mkdir(base_dir)\n\n#Veri train,test ve validation \u015feklinde b\u00f6l\u00fcnd\u00fc.\ntrain_dir = os.path.join(base_dir, 'train')\nos.mkdir(train_dir)\nvalidation_dir = os.path.join(base_dir, 'validation')\nos.mkdir(validation_dir)\ntest_dir = os.path.join(base_dir, 'test')\nos.mkdir(test_dir)\n\ntrain_pens_dir = os.path.join(train_dir, 'pens')\nos.mkdir(train_pens_dir)\n\n\ntrain_books_dir = os.path.join(train_dir, 'books')\nos.mkdir(train_books_dir)\n\n\nvalidation_pens_dir = os.path.join(validation_dir, 'pens')\nos.mkdir(validation_pens_dir)\n\n\nvalidation_books_dir = os.path.join(validation_dir, 'books')\nos.mkdir(validation_books_dir)\n\n\ntest_pens_dir = os.path.join(test_dir, 'pens')\nos.mkdir(test_pens_dir)\n\n\ntest_books_dir = os.path.join(test_dir, 'books')\nos.mkdir(test_books_dir)\n\n# 70 kalem resmi train_pens_dir kopyaland\u0131.\nfnames = ['kalem.{}.jpg'.format(i) for i in range(70)]\nfor fname in fnames:\n    src = os.path.join(original_dataset_dir, fname)\n    dst = os.path.join(train_pens_dir, fname)\n    shutil.copyfile(src, dst)\n\n# 30 kalem resmi validation_pens_dir kopyaland\u0131.\nfnames = ['kalem.{}.jpg'.format(i) for i in range(70, 100)]\nfor fname in fnames:\n    src = os.path.join(original_dataset_dir, fname)\n    dst = os.path.join(validation_pens_dir, fname)\n    shutil.copyfile(src, dst)\n    \n# 50 kalem resmi test_pens_dir kopyaland\u0131.\nfnames = ['kalem.{}.jpg'.format(i) for i in range(0, 50)]\nfor fname in fnames:\n    src = os.path.join(original_dataset_dir, fname)\n    dst = os.path.join(test_pens_dir, fname)\n    shutil.copyfile(src, dst)\n    \n# # 70 kitap resmi train_books_dir kopyaland\u0131.\nfnames = ['kitap.{}.jpg'.format(i) for i in range(70)]\nfor fname in fnames:\n    src = os.path.join(original_dataset_dir, fname)\n    dst = os.path.join(train_books_dir, fname)\n    shutil.copyfile(src, dst)\n    \n# # 30 kitap resmi validation_books_dir kopyaland\u0131.\nfnames = ['kitap.{}.jpg'.format(i) for i in range(70, 100)]\nfor fname in fnames:\n    src = os.path.join(original_dataset_dir, fname)\n    dst = os.path.join(validation_books_dir, fname)\n    shutil.copyfile(src, dst)\n    \n# # 50 kitap resmi test_books_dir kopyaland\u0131.\nfnames = ['kitap.{}.jpg'.format(i) for i in range(0, 50)]\nfor fname in fnames:\n    src = os.path.join(original_dataset_dir, fname)\n    dst = os.path.join(test_books_dir, fname)\n    shutil.copyfile(src, dst)","63ddce13":"print('Total training pen images:', len(os.listdir(train_pens_dir)))","5ced9aee":"print('Total training book images:', len(os.listdir(train_books_dir)))","2fd2c7a9":"print('Total validation pen images: ', len(os.listdir(validation_pens_dir)))","01dd4fc7":"print('Total validation book images: ', len(os.listdir(validation_books_dir)))","7caaf8ba":"print('Total test pen images:', len(os.listdir(test_pens_dir)))","5d25295e":"print('Total test book images:', len(os.listdir(test_books_dir)))\n","8046e43b":"from keras import layers\nfrom keras import models\n\nmodel = models.Sequential()\nmodel.add(layers.Conv2D(16, (3, 3), activation = 'relu', input_shape = (300, 300, 3)))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(32, (3, 3), activation = 'relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(64, activation = 'relu'))\nmodel.add(layers.Dense(1, activation = 'sigmoid'))","69dd172a":"model.summary()","e523bbde":"from keras import optimizers\n\nmodel.compile(loss = 'binary_crossentropy',\n             optimizer=optimizers.RMSprop(lr=1e-4),\n             metrics = ['acc'])","aeda4a4e":"\nfrom keras.preprocessing.image import ImageDataGenerator\n\ntrain_datagen = ImageDataGenerator(rescale = 1.\/255)\ntest_datagen = ImageDataGenerator(rescale = 1.\/255)\n\ntrain_generator = train_datagen.flow_from_directory(\n                    train_dir,\n                    target_size = (300, 300),\n                    batch_size = 10,\n                    class_mode = 'binary')\n\nvalidation_generator = test_datagen.flow_from_directory(\n                        validation_dir,\n                        target_size = (300, 300),\n                        batch_size = 10,\n                        class_mode = 'binary')","f849c382":"for data_batch, labels_batch in train_generator:\n    print('data batch shape:', data_batch.shape)\n    print('labels batch shape:', labels_batch.shape)\n    break","be7d2739":"history = model.fit_generator(\n                train_generator,\n                steps_per_epoch = 6,\n                epochs = 6,\n                validation_data = validation_generator,\n                validation_steps = 3)","b7fc2fb2":"model.save('pens_and_books_model_v1.h5')","cfa3bf48":"import matplotlib.pyplot as plt\n\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, acc, 'bo', label = 'Training acc')\nplt.plot(epochs, val_acc, 'b', label = 'Validation acc')\nplt.title('Training and Validation Accuracy')\nplt.legend()\n\nplt.figure()\n\nplt.plot(epochs, loss, 'bo', label = 'Training loss')\nplt.plot(epochs, val_loss, 'b', label = 'Validation loss')\nplt.title('Training and Validation Loss')\nplt.legend()\n\nplt.show()\n","5f636bcb":"from keras import regularizers\nmodel = models.Sequential()\n\nmodel.add(layers.Conv2D(16, (3, 3), activation = 'relu', input_shape = (300, 300, 3)))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(32, (3, 3), activation = 'relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation = 'relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(128, (3, 3), activation = 'relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(512, activation = 'relu'))\nmodel.add(layers.Dense(1, activation = 'sigmoid'))\n\nmodel.compile(loss = 'binary_crossentropy',\n             optimizer=optimizers.RMSprop(lr=1e-4),\n             metrics=['acc'])","f85a9c94":"train_datagen = ImageDataGenerator(rescale = 1.\/255)\ntest_datagen = ImageDataGenerator(rescale = 1.\/255)\n\ntrain_generator = train_datagen.flow_from_directory(\n                    train_dir,\n                    target_size = (300, 300),\n                    batch_size = 10,\n                    class_mode = 'binary')\n\nvalidation_generator = test_datagen.flow_from_directory(\n                        validation_dir,\n                        target_size = (300, 300),\n                        batch_size = 10,\n                        class_mode = 'binary')\nhistory = model.fit_generator(\n      train_generator,\n      steps_per_epoch=20,\n      epochs=15,\n      validation_data=validation_generator,\n      validation_steps=3)","f8ad058a":"acc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\n\nplt.figure()\n\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()","671de9d8":"model = models.Sequential()\n\nmodel.add(layers.Conv2D(16, (3, 3), activation = 'relu', input_shape = (300, 300, 3)))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(32, (3, 3), activation = 'relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation = 'relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(128, (3, 3), activation = 'relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dropout(0.5))\nmodel.add(layers.Dense(512, activation = 'relu'))\nmodel.add(layers.Dense(1, activation = 'sigmoid'))\n\nmodel.compile(loss = 'binary_crossentropy',\n             optimizer=optimizers.RMSprop(lr=1e-4),\n             metrics=['acc'])\n","e067572c":"train_datagen = ImageDataGenerator(rescale = 1.\/255)\ntest_datagen = ImageDataGenerator(rescale = 1.\/255)\n\ntrain_generator = train_datagen.flow_from_directory(\n                    train_dir,\n                    target_size = (300, 300),\n                    batch_size = 10,\n                    class_mode = 'binary')\n\nvalidation_generator = test_datagen.flow_from_directory(\n                        validation_dir,\n                        target_size = (300, 300),\n                        batch_size = 10,\n                        class_mode = 'binary')\nhistory = model.fit_generator(\n      train_generator,\n      steps_per_epoch=20,\n      epochs=15,\n      validation_data=validation_generator,\n      validation_steps=3)","6ebe5c8d":"acc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\n\nplt.figure()\n\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()","0c435019":"from keras import regularizers\nfrom keras.regularizers import l2\nmodel = models.Sequential()\nmodel.add(layers.Conv2D(16, (3, 3),kernel_regularizer=regularizers.l2(0.001),\n                       activation = 'relu', input_shape = (300, 300, 3)))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(32, (3, 3),kernel_regularizer=regularizers.l2(0.001),activation = 'relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3),kernel_regularizer=regularizers.l2(0.001),activation = 'relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(128, (3, 3),kernel_regularizer=regularizers.l2(0.001),activation = 'relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dropout(0.5))\nmodel.add(layers.Dense(512, activation = 'relu'))\nmodel.add(layers.Dense(1, activation = 'sigmoid'))\n\nmodel.compile(loss = 'binary_crossentropy',\n             optimizer=optimizers.RMSprop(lr=1e-4),\n             metrics=['acc'])","bb5fc1a6":"train_datagen = ImageDataGenerator(rescale = 1.\/255)\ntest_datagen = ImageDataGenerator(rescale = 1.\/255)\n\ntrain_generator = train_datagen.flow_from_directory(\n                    train_dir,\n                    target_size = (300, 300),\n                    batch_size = 10,\n                    class_mode = 'binary')\n\nvalidation_generator = test_datagen.flow_from_directory(\n                        validation_dir,\n                        target_size = (300, 300),\n                        batch_size = 10,\n                        class_mode = 'binary')\n\nhistory = model.fit_generator(\n                train_generator,\n                steps_per_epoch = 20,\n                epochs = 15,\n                validation_data = validation_generator,\n                validation_steps = 3)","790f5150":"acc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\n\nplt.figure()\n\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()","ef9cd7e3":"datagen = ImageDataGenerator(\n      rotation_range=40,\n      width_shift_range=0.2,\n      height_shift_range=0.2,\n      shear_range=0.2,\n      zoom_range=0.2,\n      horizontal_flip=True,\n      fill_mode='nearest')","ff2f3683":"import matplotlib.pyplot as plt\nfrom keras.preprocessing import image\n\nfnames = [os.path.join(train_pens_dir, fname) for fname in os.listdir(train_pens_dir)]\n\nimg_path = fnames[50]\n\nimg = image.load_img(img_path, target_size = (300, 300))\n\nx = image.img_to_array(img)\n\nx = x.reshape((1,) + x.shape)\n\ni = 0\nfor batch in datagen.flow(x, batch_size = 1):\n    plt.figure()\n    imgplot = plt.imshow(image.array_to_img(batch[0]))\n    i += 1\n    if i % 4 == 0:\n        break\nplt.show()","b83dabec":"from keras import regularizers\nmodel = models.Sequential()\n\nmodel.add(layers.Conv2D(16, (3, 3), activation = 'relu', input_shape = (300, 300, 3)))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(32, (3, 3), activation = 'relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation = 'relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(128, (3, 3), activation = 'relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dropout(0.5))\nmodel.add(layers.Dense(512, activation = 'relu'))\nmodel.add(layers.Dense(1, activation = 'sigmoid'))\n\nmodel.compile(loss = 'binary_crossentropy',\n             optimizer=optimizers.RMSprop(lr=1e-4),\n             metrics=['acc'])","8a88de57":"train_datagen = ImageDataGenerator(\n    rescale=1.\/255,\n    rotation_range=40,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,)\n\ntest_datagen = ImageDataGenerator(rescale=1.\/255)\n\ntrain_generator = train_datagen.flow_from_directory(\n        train_dir,\n        target_size=(300, 300),\n        batch_size=10,\n        class_mode='binary')\n\nvalidation_generator = test_datagen.flow_from_directory(\n        validation_dir,\n        target_size=(300, 300),\n        batch_size=10,\n        class_mode='binary')\n\nhistory = model.fit_generator(\n      train_generator,\n      steps_per_epoch=20,\n      epochs=15,\n      validation_data=validation_generator,\n      validation_steps=3)","945bd86b":"model.save('pens_and_books_model_v2.h5')","08acd0dd":"acc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\n\nplt.figure()\n\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()","5159fccd":"Veri setinin k\u00fc\u00e7\u00fck olmas\u0131na ba\u011fl\u0131 e\u011frilerden anla\u015f\u0131lacag\u0131 ve acc,loss sonu\u00e7lar\u0131na ba\u011fl\u0131 olarak modelin ba\u015far\u0131m\u0131 d\u00fc\u015f\u00fckt\u00fcr ve overfit olma riski vard\u0131r.Bu y\u00fczden s\u0131ras\u0131yla \n\n1.Epoch\n\n2.Dropout\n\n3.A\u011f\u0131rl\u0131k reg\u00fclarizasyonu\n\n4.Data Augmentation\ny\u00f6ntemlerini kullanarak,ayr\u0131ca katman say\u0131lar\u0131 ile oynanarak acc artt\u0131r\u0131lmaya ve loss azalt\u0131r\u0131lmaya \u00e7al\u0131\u015f\u0131l\u0131p overfitting olmas\u0131 engellenmeye \u00e7al\u0131\u015f\u0131lacakt\u0131r.","3c2ea26d":"Resimlerin normalize edilmesi, boyutlar\u0131n\u0131n ayarlanmas\u0131 ya da D\u00d6\u2019nin kabul edebilece\u011fi tens\u00f6r\u2019ler haline getirilmesi.","619d84f7":"l2(0.001) 'nin anlam\u0131 \u015fudur; katman\u0131n a\u011f\u0131rl\u0131k matrisi i\u00e7erisindeki her bir katsay\u0131, a\u011f\u0131n toplam kay\u0131p de\u011ferine 0.001 * weight_coefficient_value*2 ceza de\u011ferini ekleyecektir. Bu ceza de\u011feri sadece e\u011fitim s\u00fcrecinde eklenece\u011fi i\u00e7in, kay\u0131p de\u011ferleri e\u011fitim s\u00fcrecinde test s\u00fcrecine g\u00f6re \u00e7ok daha b\u00fcy\u00fck olacakt\u0131r.","150fab3f":"accuracy ve loss fonksiyonu e\u011frileri","a2416ba9":"Model yeni eklenen a\u011f\u0131rl\u0131klarla overfittinge daha diren\u00e7li hale geldi ve acc de\u011ferinde de art\u0131\u015f ya\u015fand\u0131.","1261ef85":"Model e\u011fitimi","059f016c":"Modelin saklanmas\u0131","69637488":"\u015eimdi jpg dosyalar\u0131n\u0131n oldu\u011fu 140 train (70 kalem, 70 kitap), 60 validation (30 kalem, 30 kitap) ve test 100 (50 kalem, 50 kitap) \u015feklinde veriler olan, iki s\u0131n\u0131fl\u0131 bir derin \u00f6\u011frenme problemi vard\u0131r. Bu resim s\u0131n\u0131fland\u0131rma problemi CNN ve Maxpooling katmanlar\u0131 ile \u00e7\u00f6z\u00fclecektir.\n","5b338ff9":"2.Dropout\n\nBir katman\u0131n \u00e7\u0131k\u0131\u015flar\u0131n\u0131n bir b\u00f6l\u00fcm\u00fcn\u00fcn train zaman\u0131nda belli bir oranda s\u0131f\u0131rlanmas\u0131 ile yap\u0131lan bir t\u00fcr reg\u00fclarizasyondur.","42fc4a08":"Optimizer, loss fonksiyonu ve ba\u015far\u0131m metri\u011fi se\u00e7imi","a770c96b":"Katman say\u0131s\u0131 artt\u0131r\u0131larak model e\u011fitime haz\u0131r hale getirilmi\u015ftir.\n\n1.Model e\u011fitimi i\u00e7in epoch artt\u0131r\u0131lm\u0131\u015ft\u0131r.","9d19a1d4":"accuracy ve loss fonksiyonu e\u011frileri","b374caba":"10\u2019luk bacth\u2019ler halinde, 300*300 boyutlu, 3 kanall\u0131 tens\u00f6rler ve 10\u2019luk s\u0131n\u0131f vekt\u00f6r\u00fc g\u00f6r\u00fclmektedir.","528ac024":"4.Data Augmentation","84c8317d":"Modelimizin giri\u015f katmanlar\u0131 i\u00e7in aktivasyon fonksiyonu olarak \"relu\" se\u00e7ilmi\u015ftir.Verimiz iki s\u0131n\u0131ftan olu\u015ftu\u011fu i\u00e7in \u00e7\u0131k\u0131\u015f katman\u0131 aktivasyonu binary classficationa uygun olarak \"sigmoid\" olarak belirlenmi\u015ftir.Flatten ile vekt\u00f6rel d\u00f6n\u00fc\u015f\u00fcm tamamlan\u0131r.Verilerin boyutu 300x300 ve renkli oldu\u011funu belirtmek i\u00e7in 3 kanall\u0131 tens\u00f6rler \u015feklinde input_shape belirlenir.","c80c892e":"Veri setinin k\u00fc\u00e7\u00fck olmas\u0131 bir overfitting nedenidir. Bu ba\u011flamda \u201caugmentation\u201d de\u011fi\u015fik transformasyonlar ile eldeki resimleri D\u00d6\u2019nin ilk kez g\u00f6rd\u00fc\u011f\u00fc resimler haline getirme i\u015flemidir.","75c5bd4d":"Optimizer, loss fonksiyonu ve ba\u015far\u0131m metri\u011fi se\u00e7imi.","030608fd":"Ayn\u0131 kalem resminin augment edilmi\u015f hali yukar\u0131dad\u0131r.\u015eimdi modeli yeni verilerle e\u011fiterek sonu\u00e7lar\u0131 kar\u015f\u0131la\u015ft\u0131ral\u0131m.","8c67298a":"Modelin veri yetersizli\u011fi y\u00fcz\u00fcnden \u00f6\u011frenerek s\u00fcrekli iyi sonu\u00e7lar vermesi engellendi yani veri \u00e7e\u015fitlili\u011fi zenginle\u015ftirilip overfit olmamas\u0131 sa\u011fland\u0131.","b64a3735":"Model epoch say\u0131s\u0131ndaki art\u0131\u015f sayesinde  acc de\u011ferini artt\u0131r\u0131p loss fonksiyonunu azaltm\u0131\u015ft\u0131r.Her ad\u0131mdaki epoch say\u0131lar\u0131 train\/batch size form\u00fcl\u00fc ile bir de\u011fer verilmi\u015ftir.Epoch say\u0131s\u0131 ise t\u00fcm veriler\/step_per_epoch say\u0131s\u0131 form\u00fcl\u00fc ile belirlenmi\u015ftir.Validation_steps ise validation verisi\/batch size form\u00fcl\u00fc ile belirlenmi\u015ftir.","12a7fb50":"Model version 2 saklanmas\u0131","d9e32490":"Bu yakla\u015f\u0131m(dropout)ezberlemeyi engellemek i\u00e7in sisteme\/ortama g\u00fcr\u00fclt\u00fc eklemek (veya d\u00fc\u011f\u00fcmlerin baz\u0131lar\u0131n\u0131 kapatmak!) gibi d\u00fc\u015f\u00fcn\u00fclebilir.E\u011frilerden de g\u00f6r\u00fcld\u00fc\u011f\u00fc \u00fczere bir \u00f6nceki e\u011fitimde acc si 1.00 olan model araya g\u00fcr\u00fclt\u00fc eklenmesiyle de\u011fer d\u00fc\u015f\u00fc\u015f\u00fc ya\u015fam\u0131\u015ft\u0131r ve \u00f6\u011frenmesi engellenmi\u015ftir.","8008ea8c":"3.A\u011f\u0131rl\u0131k Reg\u00fclarizasyonu\n\nKeras\u2019ta weight_regularization parametresi katmanlara \u201ccost\u201d olarak eklenerek overfitting\u2019i d\u00fc\u015f\u00fcr\u00fcr."}}