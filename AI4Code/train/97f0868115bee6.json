{"cell_type":{"605bdce5":"code","4c7699b0":"code","01fb037a":"code","99b3075c":"code","1f81e94a":"code","f1269a4c":"code","6e6b5001":"code","b78af685":"code","0bd30f16":"code","e420ad65":"code","d8f6488b":"code","02dc6f57":"code","8ff9d6b8":"code","5470d743":"code","e841783e":"code","05fb7c4b":"code","d817021b":"code","93d09c03":"code","c5dee670":"code","7f8eedbb":"code","46091da9":"code","4d644fbb":"code","9fa151dd":"code","5a56416a":"code","9e68ce5f":"code","ee8416c1":"code","d808b637":"code","bdb54554":"code","03f5fcb4":"code","95c93abc":"code","b88021fc":"code","6de4ab54":"code","8b823391":"code","076920de":"code","1c684882":"code","1b60c8b5":"code","02f2e01c":"code","ddb916cb":"code","dedf1095":"code","cef85959":"code","2c99b556":"code","2ff1347f":"code","53036679":"code","49be1b08":"markdown","c4da65d8":"markdown","ae04a73f":"markdown","81aba1c6":"markdown","3d1d01ba":"markdown","7d42b313":"markdown","24f5960f":"markdown","6708404a":"markdown","4fffe198":"markdown","2c81c622":"markdown","8c2634d0":"markdown","26592619":"markdown","3aae56e1":"markdown","f2664748":"markdown","f8c9b02f":"markdown","56783bd2":"markdown","3149eb90":"markdown","37251cbe":"markdown","7284cea4":"markdown","7663a5d8":"markdown","b0a29f4e":"markdown","b710c547":"markdown","8c2da0bb":"markdown","ad3688ac":"markdown","76f02c7d":"markdown","b87e0125":"markdown"},"source":{"605bdce5":"#Getting all the packages we need: \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns #statist graph package\nimport matplotlib.pyplot as plt #plot package\n\n#plt.style.use('ggplot') #choosing favorite R ggplot stype\nplt.style.use('bmh') #setting up 'bmh' as \"Bayesian Methods for Hackers\" style sheet\n\n\n#loading ML packages:\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.svm import LinearSVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\n#input files directory\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","4c7699b0":"# Read train data\ntrain = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')","01fb037a":"print(\"train shape:\",train.shape)\nprint(\"test shape :\",test.shape)","99b3075c":"test.head(5)","1f81e94a":"train.sample(5)","f1269a4c":"#The mean of the target column:\nround(np.mean(train['Survived']), 2)","6e6b5001":"train['Survived'].value_counts()\n","b78af685":"#Survival rate:\n\ncolor = ('#F5736B', '#C7F35B')\nplt.pie(train[\"Survived\"].value_counts(), data = train, explode=[0.08,0], labels=(\"Not Survived\", \"Survived\"), \n        autopct=\"%1.1f%%\", colors=color, shadow=True, startangle=400, radius=1.6, textprops = {\"fontsize\":20})\nplt.show();","0bd30f16":"train.describe()","e420ad65":"h_labels = [x.replace('_', ' ').title() for x in \n            list(train.select_dtypes(include=['number', 'bool']).columns.values)]\n\nfig, ax = plt.subplots(figsize=(10,6))\n_ = sns.heatmap(train.corr(), annot=True, xticklabels=h_labels, yticklabels=h_labels, cmap=sns.cubehelix_palette(as_cmap=True), ax=ax)","d8f6488b":"#How many people from which social class were on Titanic?\n\nsns.countplot(train['Pclass'])","02dc6f57":"#Let's group social class and survived data:\n\nsns.countplot(x = 'Survived', hue = 'Pclass', data = train)\n\n#0 - didn't survived\n#1 - survived","8ff9d6b8":"plt.figure(figsize = (16, 8))\n\nsns.distplot(train[\"Fare\"])\nplt.title(\"Fare Histogram\")\nplt.xlabel(\"Fare\")\nplt.show()","5470d743":"#Let's see the same survival chart for gender data: \n\nsns.countplot(x = 'Survived', hue = 'Sex', data = train)\n\n#0 - didn't survived\n#1 - survived","e841783e":"#Age distribution\nplt.figure(figsize = (16, 8))\n\nsns.distplot(train[\"Age\"])\nplt.title(\"Age Histogram\")\nplt.xlabel(\"Age\")\nplt.show()","05fb7c4b":"#Let's group age and survived data:\nplt.figure(figsize = (35, 8))\n\nsns.countplot(x = 'Age', hue = 'Survived', data = train)\n\nplt.title(\"Age Histogram\")\nplt.xlabel(\"Age\")\nplt.show()\n\n#0 - didn't survived\n#1 - survived\n","d817021b":"g = sns.FacetGrid(train, col = \"Survived\")\ng.map(sns.distplot, \"Age\")\nplt.show()","93d09c03":"sns.countplot(train['SibSp'])\n","c5dee670":"#Let's group family and survived data:\nplt.figure(figsize = (15, 8))\n\nsns.countplot(x = 'SibSp', hue = 'Survived', data = train)\n\nplt.title(\"Siblings\/Spouse Histogram\")\nplt.xlabel(\"Siblings\/Spouse\")\nplt.show()\n\n#0 - didn't survived\n#1 - survived\n","7f8eedbb":"#Let's group children\/parents and survived data:\nplt.figure(figsize = (12, 6))\n\nsns.countplot(x = 'Parch', hue = 'Survived', data = train)\n\nplt.title(\"Parents\/Children Histogram\")\nplt.xlabel(\"Parents\/Children\")\nplt.show()\n\n#0 - didn't survived\n#1 - survived","46091da9":"#Let's review the data types we have to work with:\n\ntest.info()","4d644fbb":"# Let's look for missing values\ntrain.isnull().sum().sort_values(ascending = False)","9fa151dd":"# And missing values for Test set:\ntest.isnull().sum().sort_values(ascending = False)","5a56416a":"#We have to fill in missing age values in our dataset. We can use Median Titanic passenger age data for this, which is 28 (as we confirmed it above in EDA)\n\ntrain['Age']=train['Age'].fillna('28')\ntest['Age']=train['Age'].fillna('28')","9e68ce5f":"#Using the same logic again and filling in \"S\" for missing Embarked values:\n\ntrain['Embarked'] = train['Embarked'].fillna('S')\ntest['Embarked'] = train['Embarked'].fillna('S')","ee8416c1":"# Convert 'Embarked' variable to integer form\ntrain[\"Embarked\"][train[\"Embarked\"] == \"S\"] = 0\ntrain[\"Embarked\"][train[\"Embarked\"] == \"C\"] = 1\ntrain[\"Embarked\"][train[\"Embarked\"] == \"Q\"] = 2\n\ntest[\"Embarked\"][test[\"Embarked\"] == \"S\"] = 0\ntest[\"Embarked\"][test[\"Embarked\"] == \"C\"] = 1\ntest[\"Embarked\"][test[\"Embarked\"] == \"Q\"] = 2","d808b637":"#Our test set has one empty value, which we will fill in with the median:  \ntest['Fare']=train['Fare'].fillna('14')","bdb54554":"#Convert categorical Gender column to numerical data:\n\ntrain[\"Sex\"][train[\"Sex\"] == \"male\"] = 0\ntrain[\"Sex\"][train[\"Sex\"] == \"female\"] = 1\n\ntest[\"Sex\"][test[\"Sex\"] == \"male\"] = 0\ntest[\"Sex\"][test[\"Sex\"] == \"female\"] = 1\n","03f5fcb4":"#Let's validate we don't have empty values left: \n\ntrain.isnull().sum().sort_values(ascending = False)","95c93abc":"#And, for the test set:\n\ntest.isnull().sum().sort_values(ascending = False)","b88021fc":"#Dropping columns which we won't use:\n\ntrain.drop(['Name', 'Ticket', 'Cabin'], axis = 1, inplace = True)\ntest.drop(['Name', 'Ticket', 'Cabin'], axis = 1, inplace = True)","6de4ab54":"#Final check for the Train data:\n\ntrain.head(5)","8b823391":"#And, for the test set:\n\ntest.head(5)","076920de":"#Running model for only 20% our test sample using test split feature:\n\nx_train, x_test, y_train, y_test = train_test_split(train.drop(['Survived'], axis = 1), \n                                                    train['Survived'], test_size = 0.2, \n                                                    random_state = 2)","1c684882":"#Logistic Regression model:\n\nlogisticRegression = LogisticRegression(max_iter = 10000)\nlogisticRegression.fit(x_train, y_train)\n\n# Predicting the values for Survived:\npredictions = logisticRegression.predict(x_test)\n\n#print(predictions)\n\nacc_logreg = round(accuracy_score(predictions, y_test) * 100, 2)\nprint(acc_logreg)","1b60c8b5":"#Decision Tree Classifier:\n\ndecisiontree = DecisionTreeClassifier()\ndecisiontree.fit(x_train, y_train)\n\n# Predicting the values for Survived:\npredictions = decisiontree.predict(x_test)\n\n#print(predictions)\n\nacc_decisiontree = round(accuracy_score(predictions, y_test) * 100, 5)\nprint(acc_decisiontree)","02f2e01c":"#Gaussian NB:\n\ngaussian = GaussianNB()\ngaussian.fit(x_train, y_train)\n\n# Predicting the values for Survived:\npredictions = gaussian.predict(x_test)\n\n#print(predictions)\n\nacc_gaussian = round(accuracy_score(predictions, y_test) * 100, 5)\nprint(acc_gaussian)","ddb916cb":"#Support Vector Machines\n\nsvc = SVC(max_iter = 10000)\nsvc.fit(x_train, y_train)\n\n# Predicting the values for Survived:\npredictions = svc.predict(x_test)\n\n#print(predictions)\n\nacc_svc = round(accuracy_score(predictions, y_test) * 100, 2)\nprint(acc_svc)","dedf1095":"# Linear SVC\n\nlinear_svc = LinearSVC(max_iter = 10000)\nlinear_svc.fit(x_train, y_train)\n\n# Predicting the values for Survived:\npredictions = linear_svc.predict(x_test)\n\n#print(predictions)\n\nacc_linear_svc = round(accuracy_score(predictions, y_test) * 100, 2)\nprint(acc_linear_svc)","cef85959":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'Logistic Regression', 'Naive Bayes', 'Linear SVC', 'Decision Tree'],\n    'Score': [acc_svc, acc_logreg, acc_gaussian, acc_linear_svc, acc_decisiontree]})\nmodels.sort_values(by='Score', ascending=False)","2c99b556":"#set ids as PassengerId and predict survival \n\nids = test['PassengerId']\nprint(len(ids))\npredictions = logisticRegression.predict(test)","2ff1347f":"#set the output file:\noutput = pd.DataFrame({ 'PassengerId' : ids, 'Survived': predictions })\n\noutput.tail(5)","53036679":"output.to_csv('kaggle_titanic_submission.csv', index=False)\nprint(\"Successfull submission\")","49be1b08":">We don't have high correlations which might affect our prediction model. Besides Age, we also might need to look into Parents\/Children and Fare values. ","c4da65d8":">As we assumed, women have significantly higher survival rate.","ae04a73f":"## <a name=\"read\"><\/a>Reading the dataset","81aba1c6":">It seems that passengers travelling with children and\/or parents have higher chances to survive.","3d1d01ba":"### <a name=\"read\"><\/a>Family size","7d42b313":"## <a name=\"read\"><\/a>So, what features we can use for prediction?","24f5960f":"* Numerical Features: Age (Continuous), Fare (Continuous), SibSp (Discrete), Parch (Discrete)\n* Categorical Features: Survived, Sex, Embarked, Pclass\n* Alphanumeric Features: Ticket, Cabin","6708404a":"## <a name=\"read\"><\/a>Feature engineering or preparing our data for modeling","4fffe198":"### <a name=\"read\"><\/a>Gender values","2c81c622":"## <a name=\"read\"><\/a>Getting a feel of a dataset","8c2634d0":"### <a name=\"read\"><\/a>Embarked values","26592619":">Our target column for prediction is \"Survived\"","3aae56e1":"## <a name=\"read\"><\/a>Prediction and submission - let's use the model with the highest score.","f2664748":"### <a name=\"read\"><\/a>Social class","f8c9b02f":"> ### <a name=\"read\"><\/a>Challenges we have to solve before data modeling:\n1. Incomplete age data (20% missing values). Assuming age is an important attribute for survival, we will have to fill in the gaps.\n1. Gender is categorical data, which we need to transfer to numerical for LR model.\n1. Cabin data is very sparse. We won't be able to use it. \n1. Embarced data has some missing values in the Train dataset. We might have to fill them in before using it.\n1. Fare data also has missing values. \n","56783bd2":"## <a name=\"read\"><\/a>Modeling","3149eb90":">Most of the Titanic passengers were travelling alone. The survival rate for people travelling with a spouse or siblings may seem to be slightly higher. ","37251cbe":">We can see above that the highest survived score has the first class. The lowest - the third class.","7284cea4":">Below we run a quick EDA on the most essential data points: Age, Gender, Social class, and family size.","7663a5d8":"### <a name=\"read\"><\/a>Gender","b0a29f4e":"# Prediction of passengers survival for Titanic data\n\n<img src=\"https:\/\/cbsnews1.cbsistatic.com\/hub\/i\/2018\/10\/23\/80b06a72-0d2f-4962-a3c1-ab0b1a18c9bc\/screen-shot-2018-10-23-at-10-45-06-am.png\" width=600>\n\n## Introduction\nThis prediction is based on the most common supervised machine learning models. It includes some EDA, feature engineering, and modeling. \n\nBelow we will try to confirm the following assumptions:\n1. Were women more likely to survive?\n1. Were people traveling with children more likely to survive?\n1. Were young children more likely to survive?\n1. Were people with higher class tickets more likely to survive?\n\n*April, 2020*","b710c547":">We note from the data above:\n\n* There are 891 entries in the train dataset. Caveat, not all columns in the dataset are complete.\n* The yongest passenger age is 4 months, the oldest - 80 years. Average Titanic passenger age is 29 years, while the median is 28 years. That said, we have age data for approximately 80% passengers.\n* The largest family had 6 people (children and parents) and 8 siblings. \n* Approximatelly half of the passengers were with siblings and\/or a spouse. And more than a half pasengers were travelling alone without children and\/or parents.  ","8c2da0bb":"### <a name=\"read\"><\/a>Age","ad3688ac":"### <a name=\"read\"><\/a>Age values ","76f02c7d":">Looks clean. We can build prediction model now!","b87e0125":"### <a name=\"read\"><\/a>Fare"}}