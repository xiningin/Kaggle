{"cell_type":{"938d2dc8":"code","dbaa3b50":"code","d8f05ece":"code","01aec21f":"code","62963790":"code","c3a6de73":"code","97ecaff9":"code","72b48ac1":"code","9cf42d51":"code","46790c26":"code","63687f96":"markdown","900f446a":"markdown","6dc42678":"markdown"},"source":{"938d2dc8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","dbaa3b50":"import numpy as np\nimport pandas as pd\nimport statistics as st","d8f05ece":"input_file=pd.read_csv('..\/input\/movie-ratings\/movieratings.csv',index_col=0, sep=\",\")","01aec21f":"#defining column names and reading the text file to get a dataframe\nrating_columns = ['User', 'movie_id', 'rating', 'unix_timestamp']\nratings = pd.read_csv('..\/input\/movielens-100k-dataset\/ml-100k\/u.data', sep='\\t', names=rating_columns)\n\nmovie_columns = ['movie_id', 'title', 'release_date', 'video_release_date', 'imdb_url']\nmovies = pd.read_csv('..\/input\/movielens-100k-dataset\/ml-100k\/u.item', sep='|', names=movie_columns, usecols=range(5),encoding='latin-1')\n\n#as we didn't have all the info in one file, we are merging two files to get movie names, user ids and ratings\nmovie_ratings = pd.merge(movies, ratings)\n\n#for further manipulations we will need only these three columns and we need the index to be the user to be able to manipulate data easier\nmovie_ratings=movie_ratings.loc[:,['title','User','rating']].set_index('User')\n\n#we also need to redesign the table to match the look of our smaller dataset so that it can be manipulated the same way, this structure is not randomly chosen, it fits the best manipulations with iterations and dataframe calculations\ndata_big=movie_ratings.reset_index().groupby(['User', 'title'])['rating'].aggregate('first').unstack()\n\n#as opposed to smaller dataset we don't have names here, hence we transform the index to strings to have names of people rather than numbers even if it's only ids\ndata_big.index=data_big.index.map(str)","62963790":"# Recommendation algorithms\n\n# Pearson correlation coefficient for person1 and person2\n\ndef pearsonSimilarity(prefs,person1,person2):\n#dropping all the columns that are not mutually shared between two people\n    df_1=prefs.loc[[person1, person2],:].dropna(axis='columns')\n#finding correlation between the ratings that are left aka for the same movies both people watched\n    scores=df_1.loc[person1].corr(df_1.loc[person2])\n    return scores","c3a6de73":"pearsonSimilarity(data_big,'308','287')","97ecaff9":"# Geting recommendations for person by using a weighted average\n# of every other user's rankings\n\ndef getRecommendations(prefs,person,similarity=pearsonSimilarity):\n#I am using np seterr as there are some NaNs and 0 values which might occur in the calculations only for the big dataset, which can be ignored and no how affect the end results, if it was crucial, certianly I would not have handled a warning by ignoring, but in Recommender we truly don't care about 0 values, which don't provide any valuable insight\n    np.seterr(divide='ignore', invalid='ignore')\n#extracting the movies that the person watched in order to take it out later from the recommendations\n    person_watched=pd.DataFrame(prefs.loc[person,:].dropna()).drop(columns=[person])\n    movieratings=prefs\n    for title in person_watched.index:\n        movieratings=movieratings.drop(columns=title)\n    list_1=[]\n    list_2=[] \n#iterating through the movies that the person did not watch and adding correlations for all other users, excluding the person we are making the recommendations for\n    for i in movieratings.index:\n         if i != person:\n                list_1=(i,pearsonSimilarity(prefs,person,i))\n                list_2.append(list_1)    \n    list_2=pd.DataFrame(list_2)\n    list_2.columns=['User','Similarity']\n    \n#merging the correlations with ratings dataframe to perform calculations\n    main_data = pd.DataFrame.merge(list_2,movieratings, on='User',left_on=None, right_on=None)\n#multiplying similiarity with rating for every user and movie\n    for col in main_data.columns[2:]:\n         main_data[col] = np.where(main_data.loc[:,col]==\"NaN\",main_data.loc[:,col],main_data.loc[:,'Similarity']*main_data.loc[:,col])\n\n#because we need to handle the negative values in the correlation, I set a threshhold of 0 and cliped it, while also filling NAs with 0, this was the most efficient way not to distort the calculations, because in the formula we have sums, zeros would not cause any issues\n    main_data.iloc[:,1:]=main_data.iloc[:,1:].astype(\"float\").clip(0).fillna(0)\n    rankings=[]\n#here we needed to take into account similiarities only for the movies-wise, meaning if for movie X the user did not have a rating(or as we filled them the rating was 0), we need to skip taking their similiarities into account. Hence, I used the trick we did during our R class to have an extra column that allows us to manipulate the data easier. \n    for u in main_data.columns:\n        if (u != 'User') and (u!='Similarity'):\n            main_data.loc[main_data[u] !=0, 'Indicator'] = 1\n            main_data.loc[main_data[u] ==0, 'Indicator'] = 0\n#we create an if in case of movies that have 0 as a result, if there was no 'if' we would get an error that we can't divide by zero, hence, we need to take into account this specific cases\n            if sum(main_data['Similarity'].mul(main_data['Indicator'])) == 0:\n                r=0\n            else:\n                r=(sum(main_data[u])\/sum(main_data['Similarity'].mul(main_data['Indicator'])))\n                if r>5:\n                    r=5\n#printing the rating and movie name and appending in the list\n            list_3=(r,u)\n            rankings.append(list_3)\n#I am applying the sorted function on the tuples and sorting by the rankings value from highest to lowest\n            rankings = sorted(rankings, key=lambda tup: tup[0],reverse=True)\n    return rankings\n","72b48ac1":"getRecommendations(data_big,'308')","9cf42d51":"# Returning the best matches for person from the prefs dictionary. \n# Number of results and similarity function are optional params.\n\ndef topMatches(prefs,person,n=5,similarity=pearsonSimilarity):\n#initializing lists where the results will be registered\n    list_1=[]\n    list_2=[]\n#basically this 'for loop' goes through all the people in our dataframe, excluding the person taken, pulls out the similiarities and if they are not NaNs then it appends them to the list, in this case we get NaNs when people don't have anything to do with each other hence their correlation is registered as NaN\n    for i in prefs.index:\n        if i != person:\n            list_1=(pearsonSimilarity(prefs,person,i),i)\n            if np.isnan(list_1[0])==False:\n                list_2.append(list_1)\n#here I am applying the sorted function on the tuples and sorting by the score value from highest to lowest, top 5\n        scores = sorted(list_2, key=lambda tup: tup[0],reverse=True)\n    return scores[0:n]","46790c26":"topMatches(data_big,'308')","63687f96":"## Collaborative Filtering Recommender System","900f446a":"## Reading the text files and transforming it to manageable dataframes\nI want to note that this changes to the 100K data should be done before using the functions, if you are testing for the small datasets just read the input_file accordingly.","6dc42678":"### Reading the small dataset, IMPORTANT to read it exactly like this.\nI could have added the line where the data will be transformed inside the function but I believe functions should not be tailored to one kind of dataframe, hence, this kind of manipulations should be done before."}}