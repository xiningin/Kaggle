{"cell_type":{"86049479":"code","d6832eee":"code","27b97233":"code","ba313826":"code","a3f40f18":"code","b85077f7":"code","adf83400":"code","9417e94f":"code","2f6e5960":"code","af3e2c86":"code","d9140897":"code","a98ab47a":"markdown","0642f7f0":"markdown","9d6c6dc0":"markdown","437dbdd0":"markdown","e2c9182a":"markdown","a6b6dde9":"markdown"},"source":{"86049479":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.metrics import classification_report, roc_auc_score, confusion_matrix","d6832eee":"df_train = pd.read_csv('..\/input\/choose-tutors\/train.csv')\ndf_test = pd.read_csv('..\/input\/choose-tutors\/test.csv')\n\nfeature_names = ['Id', 'age', 'years_of_experience', 'lesson_price', 'qualification', 'physics', 'chemistry', 'biology', 'english', 'geography', 'history', 'mean_exam_points']\ntarget_name = 'choose'\n\ndf_train = df_train[feature_names + [target_name]]\ndf_test = df_test[feature_names]","27b97233":"df_train.head()","ba313826":"df_test.head()","a3f40f18":"class Node:\n    \"\"\"\u041a\u043b\u0430\u0441\u0441 \u0443\u0437\u043b\u0430\"\"\"\n    \n    def __init__(self, index, t, true_branch, false_branch):\n        self.index = index  # \u0438\u043d\u0434\u0435\u043a\u0441 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430, \u043f\u043e \u043a\u043e\u0442\u043e\u0440\u043e\u043c\u0443 \u0432\u0435\u0434\u0435\u0442\u0441\u044f \u0441\u0440\u0430\u0432\u043d\u0435\u043d\u0438\u0435 \u0441 \u043f\u043e\u0440\u043e\u0433\u043e\u043c \u0432 \u044d\u0442\u043e\u043c \u0443\u0437\u043b\u0435\n        self.t = t  # \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u043f\u043e\u0440\u043e\u0433\u0430\n        self.true_branch = true_branch  # \u043f\u043e\u0434\u0434\u0435\u0440\u0435\u0432\u043e, \u0443\u0434\u043e\u0432\u043b\u0435\u0442\u0432\u043e\u0440\u044f\u044e\u0449\u0435\u0435 \u0443\u0441\u043b\u043e\u0432\u0438\u044e \u0432 \u0443\u0437\u043b\u0435\n        self.false_branch = false_branch  # \u043f\u043e\u0434\u0434\u0435\u0440\u0435\u0432\u043e, \u043d\u0435 \u0443\u0434\u043e\u0432\u043b\u0435\u0442\u0432\u043e\u0440\u044f\u044e\u0449\u0435\u0435 \u0443\u0441\u043b\u043e\u0432\u0438\u044e \u0432 \u0443\u0437\u043b\u0435\n\nclass Leaf:\n    \"\"\"\u041a\u043b\u0430\u0441\u0441 \u0442\u0435\u0440\u043c\u0438\u043d\u0430\u043b\u044c\u043d\u043e\u0433\u043e \u0443\u0437\u043b\u0430 (\u043b\u0438\u0441\u0442\u0430)\"\"\"\n    \n    def __init__(self, data, labels):\n        self.data = data\n        self.labels = labels\n        self.prediction = self.predict()\n        \n    def predict(self):\n        # \u043f\u043e\u0434\u0441\u0447\u0435\u0442 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u0430 \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u0432 \u0440\u0430\u0437\u043d\u044b\u0445 \u043a\u043b\u0430\u0441\u0441\u043e\u0432\n        classes = {}\n        for label in self.labels:\n            if label not in classes:\n                classes[label] = 0\n            classes[label] += 1\n        #  \u043d\u0430\u0439\u0434\u0435\u043c \u043a\u043b\u0430\u0441\u0441, \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u0432 \u043a\u043e\u0442\u043e\u0440\u043e\u0433\u043e \u0431\u0443\u0434\u0435\u0442 \u043c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u044b\u043c \u0432 \u044d\u0442\u043e\u043c \u043b\u0438\u0441\u0442\u0435 \u0438 \u0432\u0435\u0440\u043d\u0435\u043c \u0435\u0433\u043e    \n        prediction = max(classes, key=classes.get)\n        return prediction\n\nclass RandomForestClassifier():\n    \"\"\"\u041a\u043b\u0430\u0441\u0441 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u0430 RandomForestClassifier\"\"\"\n\n    @property\n    def forest_(self):\n        return self.forest\n    \n    @property\n    def n_trees_(self):\n        return self.n_trees\n\n    def __init__(self, n_trees = 1, max_depth=5, min_leaf=1, random_state=42):\n        self.n_trees = n_trees\n        self.random_state = random_state\n        self.max_depth = max_depth\n        self.min_leaf = min_leaf\n        self.forest = []\n\n    def bootstrap(self, X, y):\n        n_samples = X.shape[0]\n        np.random.seed(self.random_state)\n        \n        bootstrap = []\n        for i in range(self.n_trees):\n            b_data = np.zeros(X.shape)\n            b_labels = np.zeros(y.shape)\n            \n            for j in range(n_samples):\n                sample_index = np.random.randint(0, n_samples-1)\n                b_data[j] = X[sample_index]\n                b_labels[j] = y[sample_index]\n            bootstrap.append((b_data, b_labels))\n        \n        return bootstrap\n    \n    def subsample(self, len_sample):\n        # \u0431\u0443\u0434\u0435\u043c \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u0442\u044c \u043d\u0435 \u0441\u0430\u043c\u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438, \u0430 \u0438\u0445 \u0438\u043d\u0434\u0435\u043a\u0441\u044b\n        sample_indexes = [i for i in range(len_sample)]\n        \n        len_subsample = int(np.sqrt(len_sample))\n        \n        subsample = []\n        np.random.shuffle(sample_indexes)\n        for _ in range(len_subsample):\n            subsample.append(sample_indexes.pop())\n        \n        return subsample\n    \n    def calc_criterion(self, y):\n        \"\"\"\u0420\u0430\u0441\u0447\u0435\u0442 \u043a\u0440\u0438\u0442\u0435\u0440\u0438\u044f \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0442\u0438\u0432\u043d\u043e\u0441\u0442\u0438\"\"\"\n\n        #  \u043f\u043e\u0434\u0441\u0447\u0435\u0442 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u0430 \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u0432 \u0440\u0430\u0437\u043d\u044b\u0445 \u043a\u043b\u0430\u0441\u0441\u043e\u0432\n        classes = {}\n        for label in y:\n            if label not in classes:\n                classes[label] = 0\n            classes[label] += 1\n\n        # \u0420\u0430\u0441\u0447\u0435\u0442 \u043a\u0440\u0438\u0442\u0435\u0440\u0438\u044f \u0414\u0436\u0438\u043d\u0438\n        impurity = 1\n        for label in classes:\n            p = classes[label] \/ len(y)\n            impurity -= p ** 2\n\n        return impurity\n    \n    def quality(self, left_labels, right_labels, current_criterion):\n        \"\"\"\u0420\u0430\u0441\u0447\u0435\u0442 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0430\"\"\"\n\n        # \u0434\u043e\u043b\u044f \u0432\u044b\u0431\u043e\u043a\u0438, \u0443\u0448\u0435\u0434\u0448\u0430\u044f \u0432 \u043b\u0435\u0432\u043e\u0435 \u043f\u043e\u0434\u0434\u0435\u0440\u0435\u0432\u043e\n        p = float(left_labels.shape[0]) \/ (left_labels.shape[0] + right_labels.shape[0])\n        \n        return current_criterion - p * self.calc_criterion(left_labels) - (1 - p) * self.calc_criterion(right_labels)\n\n    @staticmethod\n    def split(X, y, index, t):\n        \"\"\"\u0420\u0430\u0437\u0431\u0438\u0435\u043d\u0438\u0435 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430 \u0432 \u0443\u0437\u043b\u0435\"\"\"\n\n        left = np.where(X[:, index] <= t)\n        right = np.where(X[:, index] > t)\n            \n        true_data = X[left]\n        false_data = X[right]\n        true_labels = y[left]\n        false_labels = y[right]\n            \n        return true_data, false_data, true_labels, false_labels\n\n    def find_best_split(self, X, y):\n        \"\"\"\u041d\u0430\u0445\u043e\u0436\u0434\u0435\u043d\u0438\u0435 \u043d\u0430\u0438\u043b\u0443\u0447\u0448\u0435\u0433\u043e \u0440\u0430\u0437\u0431\u0438\u0435\u043d\u0438\u044f\"\"\"\n\n        current_criterion = self.calc_criterion(y)\n\n        best_quality = 0\n        best_t = None\n        best_index = None\n        \n        n_features = X.shape[1]\n        \n        # \u0432\u044b\u0431\u043e\u0440 \u0438\u043d\u0434\u0435\u043a\u0441\u0430 \u0438\u0437 \u043f\u043e\u0434\u0432\u044b\u0431\u043e\u0440\u043a\u0438 \u0434\u043b\u0438\u043d\u043e\u0439 sqrt(n_features)\n        subsample = self.subsample(n_features)\n        \n        for index in subsample:\n            # \u0431\u0443\u0434\u0435\u043c \u043f\u0440\u043e\u0432\u0435\u0440\u044f\u0442\u044c \u0442\u043e\u043b\u044c\u043a\u043e \u0443\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430, \u0438\u0441\u043a\u043b\u044e\u0447\u0430\u044f \u043f\u043e\u0432\u0442\u043e\u0440\u0435\u043d\u0438\u044f\n            t_values = np.unique([row[index] for row in X])\n            \n            for t in t_values:\n                true_data, false_data, true_labels, false_labels = self.split(X, y, index, t)\n                #  \u043f\u0440\u043e\u043f\u0443\u0441\u043a\u0430\u0435\u043c \u0440\u0430\u0437\u0431\u0438\u0435\u043d\u0438\u044f, \u0432 \u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u0432 \u0443\u0437\u043b\u0435 \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u0432 \u043c\u0435\u043d\u0435\u0435 \u0437\u0430\u0434\u0430\u043d\u043d\u044b\u0445 \u0432 min_leaf\n                if len(true_data) < self.min_leaf or len(false_data) < self.min_leaf:\n                    continue\n                \n                current_quality = self.quality(true_labels, false_labels, current_criterion)\n                \n                #  \u0432\u044b\u0431\u0438\u0440\u0430\u0435\u043c \u043f\u043e\u0440\u043e\u0433, \u043d\u0430 \u043a\u043e\u0442\u043e\u0440\u043e\u043c \u043f\u043e\u043b\u0443\u0447\u0430\u0435\u0442\u0441\u044f \u043c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u044b\u0439 \u043f\u0440\u0438\u0440\u043e\u0441\u0442 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0430\n                if current_quality > best_quality:\n                    best_quality, best_t, best_index = current_quality, t, index\n\n        return best_quality, best_t, best_index\n\n    def tree(self, X, y):\n\n        def build_tree(X, y, **kwargs):\n            \"\"\"\u041f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u0435 \u0434\u0435\u0440\u0435\u0432\u0430 \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u0440\u0435\u043a\u0443\u0440\u0441\u0438\u0432\u043d\u043e\u0439 \u0444\u0443\u043d\u043a\u0446\u0438\u0438\"\"\"\n            \n            # \u043e\u0433\u0440\u0430\u043d\u0438\u0447\u0435\u043d\u0438\u0435 \u043f\u043e \u0433\u043b\u0443\u0431\u0438\u043d\u0435 \u0434\u0435\u0440\u0435\u0432\u0430\n            kwargs['depth'] += 1\n            if kwargs['depth'] > self.max_depth:\n                return Leaf(X, y)\n\n            quality, t, index = self.find_best_split(X, y)\n\n            #  \u0411\u0430\u0437\u043e\u0432\u044b\u0439 \u0441\u043b\u0443\u0447\u0430\u0439 - \u043f\u0440\u0435\u043a\u0440\u0430\u0449\u0430\u0435\u043c \u0440\u0435\u043a\u0443\u0440\u0441\u0438\u044e, \u043a\u043e\u0433\u0434\u0430 \u043d\u0435\u0442 \u043f\u0440\u0438\u0440\u043e\u0441\u0442\u0430 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0430\n            if quality == 0:\n                return Leaf(X, y)\n\n            true_data, false_data, true_labels, false_labels = self.split(X, y, index, t)\n\n            # \u0420\u0435\u043a\u0443\u0440\u0441\u0438\u0432\u043d\u043e \u0441\u0442\u0440\u043e\u0438\u043c \u0434\u0432\u0430 \u043f\u043e\u0434\u0434\u0435\u0440\u0435\u0432\u0430\n            true_branch = build_tree(true_data, true_labels, depth=kwargs['depth'])\n            false_branch = build_tree(false_data, false_labels, depth=kwargs['depth'])\n            \n            # \u0412\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u043c \u043a\u043b\u0430\u0441\u0441 \u0443\u0437\u043b\u0430 \u0441\u043e \u0432\u0441\u0435\u043c\u0438 \u043f\u043e\u0434\u0434\u0435\u0440\u0435\u0432\u044c\u044f\u043c\u0438, \u0442\u043e \u0435\u0441\u0442\u044c \u0446\u0435\u043b\u043e\u0433\u043e \u0434\u0435\u0440\u0435\u0432\u0430\n            return Node(index, t, true_branch, false_branch)\n    \n        return build_tree(X, y, depth=0)\n    \n    def predict(self, X, proba=False):\n        \"\"\"\u041f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0435 \u0433\u043e\u043b\u043e\u0441\u043e\u0432\u0430\u043d\u0438\u0435\u043c \u0434\u0435\u0440\u0435\u0432\u044c\u0435\u0432 \"\"\"\n        \n        def tree_predict(X, tree):\n            \"\"\"\u0424\u0443\u043d\u043a\u0446\u0438\u044f \u0444\u043e\u0440\u043c\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u043f\u043e \u0432\u044b\u0431\u043e\u0440\u043a\u0435 \u043d\u0430 \u043e\u0434\u043d\u043e\u043c \u0434\u0435\u0440\u0435\u0432\u0435\"\"\"\n            \n            return [classify_object(obj, tree) for obj in X]\n        \n        def classify_object(obj, node):\n            \"\"\"\u0424\u0443\u043d\u043a\u0446\u0438\u044f \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438 \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u043e\u0433\u043e \u043e\u0431\u044a\u0435\u043a\u0442\u0430\"\"\"\n\n            #  \u041e\u0441\u0442\u0430\u043d\u0430\u0432\u043b\u0438\u0432\u0430\u0435\u043c \u0440\u0435\u043a\u0443\u0440\u0441\u0438\u044e, \u0435\u0441\u043b\u0438 \u0434\u043e\u0441\u0442\u0438\u0433\u043b\u0438 \u043b\u0438\u0441\u0442\u0430\n            if isinstance(node, Leaf):\n                return node.prediction\n            \n            if obj[node.index] <= node.t:\n                return classify_object(obj, node.true_branch)\n            else:\n                return classify_object(obj, node.false_branch)\n        \n        # \u0434\u043e\u0431\u0430\u0432\u0438\u043c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u0432\u0441\u0435\u0445 \u0434\u0435\u0440\u0435\u0432\u044c\u0435\u0432 \u0432 \u0441\u043f\u0438\u0441\u043e\u043a\n        predictions = []\n        for tree in self.forest:\n            prediction = tree_predict(X, tree)\n            predictions.append(prediction)\n        \n        # \u0441\u0444\u043e\u0440\u043c\u0438\u0440\u0443\u0435\u043c \u0441\u043f\u0438\u0441\u043e\u043a \u0441 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f\u043c\u0438 \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u043e\u0431\u044a\u0435\u043a\u0442\u0430\n        predictions_per_object = list(zip(*predictions))\n        \n        answers = []\n        if proba is True:\n            for obj in predictions_per_object:\n                class_0 = 0\n                class_1 = 0\n                for itm in obj:\n                    if itm == 0:\n                        class_0 += 1\n                    else:\n                        class_1 += 1\n                probability = class_1 \/ (class_0 + class_1)\n                answers.append(probability)\n        else:\n            for obj in predictions_per_object:\n                predicted_class = max(set(obj), key=obj.count)\n                answers.append(predicted_class)\n        \n        return answers\n\n    def predict_proba(self, X):\n        \"\"\"\u041f\u043e\u043b\u0443\u0447\u0435\u043d\u0438\u0435 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u0438 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f\"\"\"\n        \n        return self.predict(X, proba=True)\n    \n    def fit(self, X, y):\n        bootstrap = self.bootstrap(X, y)\n        \n        for b_data, b_labels in bootstrap:\n            self.forest.append(self.tree(b_data, b_labels))","b85077f7":"# \u0432\u0437\u044f\u0442\u043e \u0438\u0437 \u043d\u043e\u0443\u0442\u0431\u0443\u043a\u0430 https:\/\/www.kaggle.com\/mahhets\/standard-logistic-regression\ndef balance_df_by_target(df, target_name, balancing_type='oversampling'):\n    target_counts = df[target_name].value_counts()\n\n    major_class_name = target_counts.argmax()\n    minor_class_name = target_counts.argmin()\n    \n    if balancing_type == 'oversampling':\n        \n        disbalance_coeff = int(target_counts[major_class_name] \/ target_counts[minor_class_name]) - 1\n\n        for i in range(disbalance_coeff):\n            sample = df[df[target_name] == minor_class_name].sample(target_counts[minor_class_name])\n            df = df.append(sample, ignore_index=True)\n\n        return df.sample(frac=1) \n    \n    if balancing_type == 'undersampling':\n        \n        major_class_sample = df[df[target_name] == major_class_name].sample(target_counts[minor_class_name])\n        minor_class_sample = df[df[target_name] == minor_class_name]\n        \n        df = pd.concat([major_class_sample, minor_class_sample], ignore_index=True)\n        \n        return df.sample(frac=1) \n\ndef accuracy_metric(actual, predicted):\n    \"\"\"\u041f\u043e\u0434\u0441\u0447\u0435\u0442 \u043c\u0435\u0442\u0440\u0438\u043a\u0438 Accuracy\"\"\"\n    \n    correct = 0\n    for i in range(len(actual)):\n        if actual[i] == predicted[i]:\n            correct += 1\n    return correct \/ float(len(actual)) * 100.0\n\ndef std_func(x):\n    \"\"\"\u0424\u0443\u043d\u043a\u0446\u0438\u044f \u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u0438\u0437\u0430\u0446\u0438\u0438\"\"\"\n    \n    return (x - x.mean()) \/ x.std()\n\ndef print_results(model, X_train, X_test, y_train, y_test):\n\n    y_train_pred = model.predict(X_train)\n    y_test_pred = model.predict(X_test)\n    \n    print('Train')\n    print(f'Accuracy: {accuracy_metric(y_train, y_train_pred):.3f}')\n    print(f'ROC AUC: {roc_auc_score(y_train, model.predict_proba(X_train)):.3f}')\n    print('Classification report')\n    print(classification_report(y_train, y_train_pred))\n    \n    print('\\nTest')\n    print(f'Accuracy: {accuracy_metric(y_test, y_test_pred):.3f}')\n    print(f'ROC AUC: {roc_auc_score(y_test, model.predict_proba(X_test)):.3f}')\n    print('Classification report')\n    print(classification_report(y_test, y_test_pred))\n    \n    print('\\nConfusion matrix')\n    print(confusion_matrix(y_test, y_test_pred))","adf83400":"df_train_balanced = balance_df_by_target(df_train, target_name, balancing_type='oversampling')\ndf_train_balanced[target_name].value_counts()","9417e94f":"np_train = df_train_balanced.to_numpy()\nnp_test = df_test.to_numpy()\n\nprint(np_train.shape)\nprint(np_test.shape)","2f6e5960":"X = std_func(np_train[:,1:12])\ny = np_train[:,-1].astype(int)\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X,\n    y,\n    shuffle=True,\n    test_size=0.25,\n    random_state=0,\n    stratify=y\n)","af3e2c86":"clf = RandomForestClassifier(\n    n_trees=500,\n    max_depth=8,\n    min_leaf=1,\n    random_state=0,\n)\nclf.fit(X_train, y_train)\n\nprint_results(clf, X_train, X_test, y_train, y_test)","d9140897":"X_final = std_func(np_test[:,1:12])\ny_pred_proba_final = clf.predict_proba(X_final)\n\npreds_final = pd.DataFrame()\npreds_final['Id'] = df_test['Id'].copy()\npreds_final['choose'] = y_pred_proba_final\npreds_final.to_csv('.\/predictions.csv', index=False, encoding='utf-8', sep=',')\n\npreds_final.head()","a98ab47a":"### \u0421\u043e\u0445\u0440\u0430\u043d\u0435\u043d\u0438\u0435 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u0430","0642f7f0":"### \u0420\u0430\u0437\u0431\u0438\u0435\u043d\u0438\u0435 \u043d\u0430 train\/test","9d6c6dc0":"### \u0412\u0441\u043f\u043e\u043c\u043e\u0433\u0430\u0442\u0435\u043b\u044c\u043d\u044b\u0435 \u0444\u0443\u043d\u043a\u0446\u0438\u0438","437dbdd0":"### \u0412\u044b\u0440\u0430\u0432\u043d\u0438\u0432\u0430\u043d\u0438\u0435 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0435\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438 \u043f\u043e \u043a\u043b\u0430\u0441\u0441\u0430\u043c","e2c9182a":"### \u0420\u0435\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u0430 RandomForestClassifier","a6b6dde9":"### \u041e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0438"}}