{"cell_type":{"2e296e14":"code","ae4a2ef8":"code","dff62b73":"code","9df13b69":"code","e2a5c24e":"code","07dfaebb":"code","a50fff5c":"code","39504956":"code","720fc818":"code","ee9abc00":"code","03e8b54a":"code","7116bada":"code","2605f817":"code","5fd092b3":"code","7715fb2e":"code","c854a477":"code","cbf8aac4":"code","eebf470d":"code","4b275822":"code","3fcfd998":"code","d89fd9d1":"code","55844534":"code","1d8885d6":"code","888f82d8":"code","7973a7c3":"code","a97f7c37":"code","4d4fd1cc":"code","3ee07b10":"code","70686686":"code","c898f446":"code","cd836b25":"code","ded4efd3":"code","cec465be":"code","e8390c2b":"code","5a524c1c":"code","041992ef":"code","0375ab38":"code","7321fde5":"code","b1247081":"code","0020da97":"code","f5fac2fb":"code","4eda9838":"code","18c61496":"code","ae7f5acb":"code","9c4b22f8":"code","3e72a422":"code","d6159785":"code","c3e5150b":"code","f7cc0caf":"code","5ef7289c":"code","8eb18e47":"code","e1281008":"code","c70901a1":"code","641f4f38":"code","59356955":"code","3ac9c0ae":"code","3ea940a6":"code","8fe03a92":"code","c20e4d88":"code","a23f4b08":"code","4e381901":"code","0de25488":"code","b7f8f88f":"code","37f14ef5":"code","5807d197":"code","3b4672c0":"code","3459e54a":"code","fa486beb":"code","e95b6127":"code","9b363cd7":"code","925ea3b3":"markdown","c355f718":"markdown","2cc83109":"markdown","a8519e63":"markdown","6a51d0e5":"markdown"},"source":{"2e296e14":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n#","ae4a2ef8":"import pandas as pd\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pandas.plotting import scatter_matrix\n%matplotlib inline\nfrom matplotlib.ticker import StrMethodFormatter","dff62b73":"# ignore warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","9df13b69":"df = pd.read_csv(\"..\/input\/pima-indians-diabetes-database\/diabetes.csv\")","e2a5c24e":"df","07dfaebb":"# Data Exploration\ndf.shape","a50fff5c":"df.count(0)","39504956":"df.isnull().sum()","720fc818":"df.count(0)","ee9abc00":"df.dtypes","03e8b54a":"df.describe(include='all')","7116bada":"#Check the correlation between features and plot heatmap\ncorrmat = df.corr()\ntop_corr_features = corrmat.index\nplt.figure(figsize=(20,20))\ng=sns.heatmap(df[top_corr_features].corr(),annot=True, cmap=\"YlGnBu\")","2605f817":"df.corr()","5fd092b3":"p = df.hist(figsize = (20,20))\n#There are many features having 0 as values","7715fb2e":"df['Outcome'].value_counts()","c854a477":"df['Outcome'].value_counts().plot(kind='bar')","cbf8aac4":"print(df.dtypes)","eebf470d":"sns.countplot(df.dtypes.map(str))\nplt.show()","4b275822":"df_copy = df.copy(deep = True)\ndf_copy.head()","3fcfd998":"df_copy[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']] = df_copy[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']].replace(0,np.NaN)\n","d89fd9d1":"df_copy.isnull().sum()","55844534":"df_copy.describe(include='all')","1d8885d6":"# There are too many 0 values in BloodPressure, SkinThickness, Insulin and BMI \n# We need to replace (impute) these NaN values based on the data dostribution with Mean, Median or Mode\ndf_copy['Glucose'].fillna(df_copy['Glucose'].mean(), inplace = True)\ndf_copy['BloodPressure'].fillna(df_copy['BloodPressure'].mean(), inplace = True)\ndf_copy['SkinThickness'].fillna(df_copy['SkinThickness'].median(), inplace = True)\ndf_copy['Insulin'].fillna(df_copy['Insulin'].median(), inplace = True)\ndf_copy['BMI'].fillna(df_copy['BMI'].median(), inplace = True)","888f82d8":"# There is no null value now\ndf_copy.isnull().sum()","7973a7c3":"df_copy.head()","a97f7c37":"# Lets plot the histogram after NaN removal\np = df_copy.hist(figsize = (20,20))","4d4fd1cc":"#Scatter matrix of the uncleaned data\n\np=scatter_matrix(df,figsize=(25, 25))","3ee07b10":"#Pair plot of the cleaned data\np=sns.pairplot(df_copy, hue = 'Outcome')","70686686":"# Heatmap of the cleaned data\n# There are not much correlations between features. So all features are considered\ncorrmat = df_copy.corr()\ntop_corr_features = corrmat.index\nplt.figure(figsize=(20,20))\ng=sns.heatmap(df_copy[top_corr_features].corr(),annot=True, cmap=\"YlGnBu\")","c898f446":"# Train test split\nfrom sklearn.model_selection import train_test_split\nfeature_columns = ['Pregnancies' ,'Glucose' , 'BloodPressure' ,'SkinThickness' , 'Insulin' , 'BMI', 'DiabetesPedigreeFunction' , 'Age']\npredicted_class =['Outcome']","cd836b25":"X=df_copy[feature_columns]\ny=df_copy[predicted_class]\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.70,test_size=0.30, random_state=101)","ded4efd3":"X_train.head(10)","cec465be":"y_train.head()","e8390c2b":"X_test.head()","5a524c1c":"X_train.describe(include='all')","041992ef":"#Apply Random forest classifier\nfrom sklearn.ensemble import RandomForestClassifier","0375ab38":"#instantiate the classifier\nrfc = RandomForestClassifier(n_estimators=10, random_state=0)","7321fde5":"from pprint import pprint\nprint('Parameters currently in use:\\n')\npprint(rfc.get_params())","b1247081":"#fit the model\nrfc.fit(X_train, y_train)","0020da97":"#Evaluate Random Search model\n# predict the test results\ny_pred = rfc.predict(X_test)","f5fac2fb":"#check accuracy score\nfrom sklearn.metrics import accuracy_score\n\nprint('Model accuracy score with 10 decision-trees : {0:0.4f}'. format(accuracy_score(y_test, y_pred)))","4eda9838":"# Print confusion matrix\n\nfrom sklearn.metrics import confusion_matrix\ncm=confusion_matrix(y_test,y_pred)\ncm","18c61496":"#instantiate the RandomForest classifier with Estimators=100\nrfc = RandomForestClassifier(n_estimators=100, random_state=0)","ae7f5acb":"#fit the model\nrfc.fit(X_train, y_train)","9c4b22f8":"#Evaluate RandomForest model\n# predict the test results\ny_pred1 = rfc.predict(X_test)","3e72a422":"#check accuracy score\nfrom sklearn.metrics import accuracy_score\n\nprint('Model accuracy score with 100 decision-trees : {0:0.4f}'. format(accuracy_score(y_test, y_pred1)))","d6159785":"# Print classification report,confusion matrix for RandomForest\nfrom sklearn.metrics import classification_report,confusion_matrix\nprint(classification_report(y_test,y_pred1))\nprint()\nprint(confusion_matrix(y_test,y_pred1))","c3e5150b":"#Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nlr= LogisticRegression(solver='lbfgs', max_iter=400)\nlr.fit(X_train,y_train)\npred=lr.predict(X_test)","f7cc0caf":"# Print classification report,confusion matrix for logistics regression\nfrom sklearn.metrics import classification_report,confusion_matrix\nprint(classification_report(y_test,pred))\nprint()\nprint(confusion_matrix(y_test,pred))","5ef7289c":"#Support vector machine\nfrom sklearn.svm import SVC\nclassifier = SVC(kernel = 'rbf', random_state = 0)\nclassifier.fit(X_train, y_train)\nY_pred = classifier.predict(X_test)","8eb18e47":"# Print classification report,confusion matrix for SVM\nfrom sklearn.metrics import classification_report,confusion_matrix\nprint(classification_report(y_test,Y_pred))\nprint()\nprint(confusion_matrix(y_test,Y_pred))","e1281008":"#Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.preprocessing import StandardScaler","c70901a1":"# Gaussian NB\nsc_X = StandardScaler()\nX_train1 = sc_X.fit_transform(X_train)\nX_test1 = sc_X.transform(X_test)\nclassifier=GaussianNB()\nclassifier.fit(X_train1,y_train)\nY_pred1=classifier.predict(X_test1)","641f4f38":"\n# Print classification report,confusion matrix for Gaussian NB\nfrom sklearn.metrics import classification_report,confusion_matrix\nprint(classification_report(y_test,Y_pred1))\nprint()\nprint(confusion_matrix(y_test,Y_pred1))\n              ","59356955":"#BernoulliNB\nsc_X = StandardScaler()\nX_train2 = sc_X.fit_transform(X_train)\nX_test2 = sc_X.transform(X_test)\nclassifier=BernoulliNB()\nclassifier.fit(X_train2,y_train)\ny_pred2=classifier.predict(X_test2)","3ac9c0ae":"# Print classification report,confusion matrix for BernoulliNB\nfrom sklearn.metrics import classification_report,confusion_matrix\nprint(classification_report(y_test,y_pred2))\nprint()\nprint(confusion_matrix(y_test,y_pred2))","3ea940a6":"#Perceptron\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.neighbors import KNeighborsClassifier","8fe03a92":"sc_X = StandardScaler()\nX_train4 = sc_X.fit_transform(X_train)\nX_test4 = sc_X.transform(X_test)\nclassifierNew=Perceptron()\nclassifierNew.fit(X_train4,y_train)\ny_pred4=classifierNew.predict(X_test4)","c20e4d88":"# Print classification report,confusion matrix Perceptron\nfrom sklearn.metrics import classification_report,confusion_matrix\nprint(classification_report(y_test,y_pred4))\nprint()\nprint(confusion_matrix(y_test,y_pred4))","a23f4b08":"#KNN Classification\nfrom sklearn.neighbors import KNeighborsClassifier","4e381901":"# It is better to bring all the features to the same scale for distance based algorithms like KNN using standard scaler\nsc_X = StandardScaler()\nX_train3 = sc_X.fit_transform(X_train)\nX_test3 = sc_X.transform(X_test)\nclassifier=KNeighborsClassifier(n_neighbors=18,metric='minkowski',p=2)\nclassifier.fit(X_train3,y_train)\ny_pred3=classifier.predict(X_test3)","0de25488":"# Print classification report,confusion matrix for KNN\nfrom sklearn.metrics import classification_report,confusion_matrix\nprint(classification_report(y_test,y_pred3))\nprint()\nprint(confusion_matrix(y_test,y_pred3))","b7f8f88f":"# Hyper parameter optimization for KNN \n# Grid search approcah is used here\n#import GridSearchCV\nfrom sklearn.model_selection import GridSearchCV","37f14ef5":"#In the case of classifier like knn the parameter to be tuned is n_neighbors\nparam_grid = {'n_neighbors':np.arange(1,50)}\nknn = KNeighborsClassifier()\nknn_cv= GridSearchCV(knn,param_grid,cv=5)\nknn_cv.fit(X_train3,y_train)\n\nprint(\"Best Score:\" + str(knn_cv.best_score_))\nprint(\"Best Parameters: \" + str(knn_cv.best_params_))","5807d197":"#AUC-ROC Curve plot\n# Curve tells us about how good the model can distinguish between two things (e.g If a patient has a disease or no).\n# We have already train the model for Random forest,logistic regression, KNN and XGBoost\n# predict probabilities\npred_prob1 = rfc.predict_proba(X_test)\npred_prob2 = lr.predict_proba(X_test)\npred_prob3 = classifier.predict_proba(X_test3)","3b4672c0":"# roc curve for models\nfrom sklearn.metrics import roc_curve\nfpr1, tpr1, thresh1 = roc_curve(y_test, pred_prob1[:,1], pos_label=1)\nfpr2, tpr2, thresh2 = roc_curve(y_test, pred_prob2[:,1], pos_label=1)\nfpr3, tpr3, thresh3 = roc_curve(y_test, pred_prob3[:,1], pos_label=1)","3459e54a":"# roc curve for tpr = fpr \nrandom_probs = [0 for i in range(len(y_test))]\np_fpr, p_tpr, _ = roc_curve(y_test, random_probs, pos_label=1)","fa486beb":"#Compute the AUC score\nfrom sklearn.metrics import roc_auc_score\nauc_score1 = roc_auc_score(y_test, pred_prob1[:,1])\nauc_score2 = roc_auc_score(y_test, pred_prob2[:,1])\nauc_score3 = roc_auc_score(y_test, pred_prob3[:,1])","e95b6127":"print(auc_score1, auc_score2, auc_score3)","9b363cd7":"# plot roc curves\nplt.plot(fpr1, tpr1, linestyle='--',color='green', label='Random Forest')\nplt.plot(fpr2, tpr2, linestyle='--',color='blue', label='Logistic Regression')\nplt.plot(fpr3, tpr3, linestyle='--',color='red', label='KNN')\nplt.plot(p_fpr, p_tpr, linestyle='--', color='green')\n\n# title\nplt.title('ROC curve')\n# x label\nplt.xlabel('False Positive Rate')\n# y label\nplt.ylabel('True Positive rate')\n\nplt.legend(loc='best')\nplt.savefig('ROC',dpi=300)\nplt.show();","925ea3b3":"We will run the  following classification algorithms for this problem of Diabetes prediction.\n1.RandomForest 2.Logistic Regression 3.Support Vector Machine 4.Naive Bayes (GaussianNB, BernoulliNB) 5.Perceptron 6.KNN Classification\n\n","c355f718":"Above accuracy score of Random Forest, Logistic REgression and KNN is mentioned.Here KNN has the maximum AUC(0.85440329218107)","2cc83109":"The confusion matrix is a technique used for summarizing the performance of a classification algorithm i.e. it has binary outputs. Classification Report includes Precision, Recall and F1-Score. Precision- Precision is the ratio of correctly predicted positive observations to the total predicted positive observations High precision relates to the low false positive rate Recall (Sensitivity) - Recall is the ratio of correctly predicted positive observations to the all observations in actual class - yes. A recall greater than 0.5 is good. F1 score - F1 Score is the weighted average of Precision and Recall. Therefore, this score takes both false positives and false negatives into account. F1 is usually more useful than accuracy, especially if you have an uneven class distribution.","a8519e63":"Unfortunately Gridsearch did not give an optimum accuracy. We have got a better accuracy as 79 for KNN(n_neighbors=18) So far we have run the following classification algorithms 1.RandomForest 2.Logistic Regression 3.XGBoost 4.Support Vector Machine 5.Naive Bayes (GaussianNB, BernoulliNB) 6.Perceptron 7.KNN Classification\n\nOut of all KNN has given the best accuracy of 79.","6a51d0e5":"It is evident from the plot that the AUC for the KNN ROC curve is higher than that of other classifications. Therefore, we can say that KNN did a better job of classifying the positive class in the dataset"}}