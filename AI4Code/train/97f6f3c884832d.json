{"cell_type":{"50b33df7":"code","a2da48bb":"code","3e2c0736":"code","e44c441c":"code","f9cfbdf1":"code","00df03e8":"code","6ba269ef":"code","2a012a4c":"code","d1af74c4":"code","002d5d45":"code","e78525c3":"code","5ff17f82":"code","fb6cf276":"code","bc33e421":"code","fafcfbf1":"code","af66dae1":"code","c4dc3588":"code","c0b851b5":"code","94d90eeb":"markdown","1f060b8b":"markdown","a5214b35":"markdown","87d73c32":"markdown","9df749bf":"markdown","538c7544":"markdown","1545d1bf":"markdown","6f21bbb0":"markdown","270e6f21":"markdown","d1b32cbf":"markdown","b680ae41":"markdown"},"source":{"50b33df7":"import os \nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import FuncAnimation\n%matplotlib inline\n%config InlineBackend.figure_format = 'svg'\nimport seaborn as sns","a2da48bb":"BASE_DIR = '..\/input\/lish-moa\/'\ntrain_features = pd.read_csv(BASE_DIR + 'train_features.csv')\ntrain_targets_scored = pd.read_csv(BASE_DIR + 'train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv(BASE_DIR + 'train_targets_nonscored.csv')\n\ntest_features = pd.read_csv(BASE_DIR + 'test_features.csv')\nsample_submission = pd.read_csv(BASE_DIR + 'sample_submission.csv')\n\n# TRAIN FEATURES\nINDEX = 'sig_id'\ng_cols = [col for col in train_features.columns if col.startswith('g-')]\nc_cols = [col for col in train_features.columns if col.startswith('c-')]\n\nother_cols = ['cp_type', 'cp_time', 'cp_dose']  # Categoricals","3e2c0736":"train_features.head()","e44c441c":"print(\"Q: Does the features dataframe have any null entries?\")\nif not train_features.isnull().values.any():\n    print('A: Nope, none!')","f9cfbdf1":"print('A full list of categorical features')\nprint('----'*10)\nfor col in other_cols:\n    print('Number of unique values in \"%s\": %d' % (col, train_features[col].nunique()))\n    print('Values: ', train_features[col].unique())\n    print('')","00df03e8":"fig, ax = plt.subplots(1, 3, figsize=(10, 3))\nsns.countplot(data=train_features, x=other_cols[0], ax=ax[0])\nsns.countplot(data=train_features, x=other_cols[1], ax=ax[1])\nsns.countplot(data=train_features, x=other_cols[2], ax=ax[2])\nplt.tight_layout()\nfig.suptitle('Distribution of the categorical variables', y=1.1)\nplt.show()","6ba269ef":"print('Q: Does the features dataframe have any duplicated rows?')\nif not train_features.duplicated().values.any():\n    print('A: Nope!')","2a012a4c":"print('Q: Are there any duplicated rows associated with different sig_ids?')\nif not train_features.loc[:, train_features.columns !=INDEX].duplicated().values.any():\n    print('A: Nope!')","d1af74c4":"print('Feature set includes a series of c- and g- columns.')\nprint('Number of c_cols: %d' % (len(c_cols)))\nprint('Number of g_cols: %d' % (len(g_cols)))","002d5d45":"def display_distributions(cols):\n    fig, axs = plt.subplots(nrows=5, ncols=5, figsize=(12, 10))\n    for i in range(len(cols)):\n        # print(i)\n        sns.distplot(train_features[cols[i]], ax = axs[i \/\/ 5, i % 5], norm_hist=False, kde=False)\n        #axs[i \/\/ 10, i % 10].set_title(le.inverse_transform(np.argmax([samples[0][1][i].numpy()], axis=-1))[0])\n    plt.tight_layout() # w_pad=0.01, h_pad=1\n    plt.show()\n\ndisplay_distributions(c_cols[:25])\ndisplay_distributions(c_cols[25:50])\ndisplay_distributions(c_cols[50:75])\ndisplay_distributions(c_cols[75:100])","e78525c3":"display_distributions(g_cols[:25])\ndisplay_distributions(g_cols[25:50])\ndisplay_distributions(g_cols[50:75])\ndisplay_distributions(g_cols[75:100])","5ff17f82":"train_targets_scored.head()","fb6cf276":"target_cols = train_targets_scored.columns[1:]  # 1 removes the sig_id\nprint('Number of target columns: %d' % (len(target_cols)))\nprint('We are predicting 206 columns for each sig_id')","bc33e421":"total = 0\nfor i in target_cols:\n    total += train_targets_scored[i].nunique()\n\nif total\/(len(target_cols)) == 2:\n    print('All of the target columns are binary.')","fafcfbf1":"train_targets_scored.set_index('sig_id', inplace=True)","af66dae1":"target_freq = train_targets_scored.sum(axis=0).to_frame('Counts').sort_values('Counts').reset_index()\n\nfig, ax = plt.subplots(1, 2, figsize=(8, 4))\nsns.barplot(target_freq.index, target_freq.Counts, ax=ax[0]).set(xticklabels=[])\nax[0].set_xlabel('Classes')\n\nax[1] = sns.distplot(target_freq.Counts, kde=False)\nax[1].set_ylabel('Counts of Counts')\nfig.suptitle('Class Distribution', y=1.1)\nplt.tight_layout()\nplt.show()","c4dc3588":"target_freq[:5]","c0b851b5":"target_freq[-5:]","94d90eeb":"Class names are not shown not to clutter the plot.","1f060b8b":"### 2. Targets\n\nThe targets dataset have 206 binary target columns for each of the drugs in the features set. Each drug can activate more than 1 MoA at the same time. This means the problem in this competition is a **multilabel** prediction problem, not multiclass prediction. ","a5214b35":"### The Least Frequent Classes\n\nThere are 2 least observed classes, both observed only once. The second group of least observed classes are observed 6 times. This means that a 5-fold CV will likely not to learn any of these classes if they are not augmented.","87d73c32":"# TO BE CONTINUED...","9df749bf":"#### g- columns\n\nIt seems that g- columns are also quantile-scaled similiar to c- columns. Unlike c- columns, g- columns exhibit both low and high end tails. **Again the information at these tails are the signatures of MoAs.**\n\n","538c7544":"### Class Distribution\n\nThe least frequent class is observed only once and most frequent class is observed 832 times. So, the train targets are highly imbalanced.","1545d1bf":"**c- columns**\n\nThe plots below show that c- columns follow a Gaussian-like distribution with a low end tail. Moreover, the values are quantile-scaled as seen from their Zero means and tails around +\/- 2.5. \n\n**These tails are important because if a drug is responsible for an MoA, one or some of the c- measurement diverge from the normal distribution. So our models will try to predict the critical values in these distributions where the tails start.**","6f21bbb0":"The aim of the challenge is given in the competition page. I prefer to cut to the chase.\n\nWe are given 2 dataframes for the training set: a feature set and a target set. Let's start exploring the data in the Q&A style and also check some basic statistics.","270e6f21":"Only a small portion of the data belongs to the **ctl_vehicle** group. The ctl_vehicle sample is the control group; meaning that no drugs are applied. So their MoA values are 0. We will check this in the Features Section.\n\nWe have a balanced distribution for different categories in treatment times and doses.","d1b32cbf":"### 1. Features\n\n* **`cp_type (categorical):`**  Samples treated with a compound or with a control perturbation. Categories include \"trt_cp\" and \"ctl_vehicle\", respectively.\n\n* **`cp_time (categorical):`** Treatment duration in hours. Categories include 24, 48, 72.\n\n* **`cp_dose (categorical):`** Drug dose. Categories include \"D1\", \"D2\" for low and high dose.\n\n* **`g-[0-771] (continous):`** Gene expression data - a measure of activation in a given gene after the drug is applied. \n\n* **`c-[0-99] (continous):`** Cell viability. Basically count of live cells after the drug is applied.","b680ae41":"### <center> Mechanisms of Action (MoA) Prediction <\/center>\n\n### <center> Exploratory Data Analysis <\/center>"}}