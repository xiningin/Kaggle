{"cell_type":{"eee060bb":"code","60d96f4b":"code","31039d8a":"code","25d81955":"code","f1ae3da3":"code","43b563f2":"code","493d7d32":"code","157f2307":"code","54cd65a2":"code","8e4fe476":"markdown","a836785d":"markdown","ecf1dfab":"markdown","ecc8d8bd":"markdown","17e90809":"markdown","ef69e465":"markdown","53cde453":"markdown"},"source":{"eee060bb":"# Preliminaries\nfrom tqdm import tqdm\nimport math\nimport random\nimport os\nimport pandas as pd\nimport numpy as np\n\n#torch\nimport torch\nimport torch.nn as nn\nfrom torch.nn import Parameter\nfrom torch.nn import functional as F\nfrom torch.utils.data import Dataset,DataLoader\nfrom torch.optim import Adam\nfrom torch.optim.lr_scheduler import _LRScheduler\nfrom torch.optim import Adam, lr_scheduler\n\nimport transformers\nfrom transformers import AutoModelForMaskedLM\nfrom transformers import AdamW\nfrom transformers import get_linear_schedule_with_warmup,get_cosine_schedule_with_warmup\nfrom transformers import get_cosine_with_hard_restarts_schedule_with_warmup","60d96f4b":"NUM_WORKERS = 4\nTRAIN_BATCH_SIZE = 32\nEPOCHS = 2\nSEED = 2020\nLR = 3e-5\n\ndevice = torch.device('cuda')\n\n################################################# MODEL ####################################################################\n\ntransformer_model = 'sentence-transformers\/paraphrase-xlm-r-multilingual-v1'\nTOKENIZER = transformers.AutoTokenizer.from_pretrained(transformer_model)\nCONFIG = transformers.AutoConfig.from_pretrained(transformer_model)\n\n############################################################################################################################\nif transformer_model == 'bert-base-uncased':\n    mask_tok = 103\nelif transformer_model == 'roberta-base':\n    mask_tok = 50264\nelif (transformer_model == 'xlm-roberta-base') or (transformer_model == 'sentence-transformers\/paraphrase-xlm-r-multilingual-v1'):\n    mask_tok = 250001","31039d8a":"class AverageMeter(object):\n    def __init__(self):\n        self.reset()\n    \n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n    \n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum \/ self.count","25d81955":"class ShopeeDataset(Dataset):\n    def __init__(self, csv):\n        self.csv = csv.reset_index()\n\n    def __len__(self):\n        return self.csv.shape[0]\n\n    def __getitem__(self, index):\n        row = self.csv.iloc[index]\n        text = row.title\n        \n        text = TOKENIZER(text,\n                         return_attention_mask=False,\n                         return_token_type_ids=False,\n                         padding='max_length',\n                         truncation=True,\n                         max_length=64)\n        \n        input_ids = text['input_ids']\n        \n        input_ids,labels = self.prepare_mlm_input_and_labels(np.array(input_ids))\n\n        input_ids = torch.tensor(input_ids,dtype=torch.long)\n        labels = torch.tensor(labels,dtype=torch.long)\n    \n        return input_ids,labels\n    \n    def prepare_mlm_input_and_labels(self,X):\n        # 15% BERT masking\n        inp_mask = np.random.rand(*X.shape)<0.15 \n        # do not mask special tokens\n        inp_mask[X<=2] = False\n        # set targets to -1 by default, it means ignore\n        labels = -100 * np.ones(X.shape, dtype=int)\n        # set labels for masked tokens\n        labels[inp_mask] = X[inp_mask]\n        \n        # prepare input\n        X_mlm = np.copy(X)\n        # set input to [MASK] which is the last token for the 90% of tokens\n        # this means leaving 10% unchanged\n        inp_mask_2mask = inp_mask  & (np.random.rand(*X.shape)<0.90)\n        X_mlm[inp_mask_2mask] = mask_tok\n\n        # set 10% to a random token\n        inp_mask_2random = inp_mask_2mask  & (np.random.rand(*X.shape) < 1\/9)\n        X_mlm[inp_mask_2random] = np.random.randint(3, CONFIG.vocab_size, inp_mask_2random.sum())\n\n        return X_mlm, labels","f1ae3da3":"def masked_categorical_crossentropy(output,target):\n    y_true_masked = target[target!= -100]\n    y_pred_masked = output[target!= -100]\n    loss =  nn.CrossEntropyLoss()(y_pred_masked,y_true_masked)\n    return loss","43b563f2":"def train_fn(dataloader,model,optimizer,device,scheduler,epoch):\n    model.train()\n    loss_score = AverageMeter()\n    \n    tk0 = tqdm(enumerate(dataloader), total=len(dataloader))\n    for bi,d in tk0:\n        \n        batch_size = d[0].shape[0]\n\n        input_ids = d[0]\n        targets = d[1]\n\n        input_ids = input_ids.to(device,dtype=torch.long)\n        targets = targets.to(device)\n\n        optimizer.zero_grad()\n\n        output = model(input_ids=input_ids,labels=targets)\n        \n        loss = output.loss       \n        \n        loss.backward()\n        optimizer.step()\n        \n        loss_score.update(loss.detach().item(), batch_size)\n        tk0.set_postfix(Train_Loss=loss_score.avg,Epoch=epoch,LR=optimizer.param_groups[0]['lr'])\n        \n        if scheduler is not None:\n                scheduler.step()\n        \n    return loss_score","493d7d32":"data = pd.read_csv('..\/input\/shopee-product-matching\/train.csv')","157f2307":"def run():\n    # Defining DataSet\n    train_dataset = ShopeeDataset(\n        csv=data\n    )\n        \n    train_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=TRAIN_BATCH_SIZE,\n        pin_memory=True,\n        drop_last=True,\n        num_workers=NUM_WORKERS\n    )\n    \n    \n    # Defining Model for specific fold\n    model = AutoModelForMaskedLM.from_pretrained(transformer_model)\n    print(model)\n    model.to(device)\n\n        \n    # Defining Optimizer with weight decay to params other than bias and layer norms\n    param_optimizer = list(model.named_parameters())\n    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n    optimizer_parameters = [\n        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.0001},\n        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n            ]  \n    \n    optimizer = AdamW(optimizer_parameters, lr=LR)\n    \n    #Defining LR SCheduler\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer, \n        num_warmup_steps=len(train_loader)*5, \n        num_training_steps=len(train_loader)*EPOCHS\n    )\n        \n    # THE ENGINE LOOP\n    for epoch in range(EPOCHS):\n        train_loss = train_fn(train_loader, model,optimizer, device,scheduler=scheduler,epoch=epoch)\n        \n    model.save_pretrained('.\/')","54cd65a2":"run()","8e4fe476":"# About this Notebook\n\nIn the Quest of generating of more relevant and better embeddings , here is another technique that I thought be useful. This notebooks describes <b> MASK token prediction BERT type pretraining of transformer models on our dataset <\/b> . My intuition was if we pretrain BERT or any other model using MASK word prediction task and then fine tune that model using arcface loss or simple classification task it might do a better job at creating good embeddings because it might have more idea about the words in the title. \n\nHowever it didnt give any significant boost in the performance of xlm-roberta , the reason which I feel for this is the test set is a whole lot different from train set . It was a good learning experience for me though , I am sharing it with the community as I feel many more can learn from it and also I would know if I have done anay mistake while implementing this\n\nHappy Learning","a836785d":"# Engine","ecf1dfab":"# Loss","ecc8d8bd":"# Dataset","17e90809":"# Utils","ef69e465":"# Configuration","53cde453":"# Train Function"}}