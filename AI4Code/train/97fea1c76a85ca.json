{"cell_type":{"6f94e17a":"code","2acccf4b":"code","db9d4d6b":"code","3f7f1c75":"code","01938270":"code","b11e9b13":"code","7e1320de":"code","1e937a27":"code","b299dae5":"code","d90c25ac":"code","bd0d6af3":"code","915536af":"code","aaece2e9":"code","5ab2d780":"code","fc304af7":"code","d7eaf239":"code","882ffef6":"code","5a773928":"code","6106b0b2":"code","63a72486":"code","6817bc7e":"code","ef4aea3c":"code","a0cc67d0":"code","71cf3a3b":"code","22f679fa":"code","8c59c747":"code","65446f53":"code","1ab5cdd8":"code","4fa7600a":"code","b7cc71fb":"code","b760a3bd":"code","865b63ab":"code","e2832457":"code","884171be":"code","6cda9be3":"code","68883d00":"code","b595b6c9":"code","8da94834":"code","71a0df72":"code","c9537fff":"code","1c8d6d87":"code","ff17980a":"code","a995c6a9":"code","6312a61f":"code","a9d9fc22":"code","d0b577e8":"code","e979edcb":"code","da51afef":"code","ed6b409c":"markdown","fb40b3d1":"markdown","54adf1b3":"markdown","89da9062":"markdown","2fca0932":"markdown","2eae8afd":"markdown","af926392":"markdown","9db08658":"markdown","66269137":"markdown","89d8a4f3":"markdown","40ddeff6":"markdown","1e9ed2a2":"markdown","64377ee6":"markdown","83fe2391":"markdown","b7cfa637":"markdown","b2a82a00":"markdown","19f47b8f":"markdown","69231369":"markdown","ae0f16b3":"markdown","d21e53e8":"markdown","4a071dea":"markdown","78fc17a1":"markdown","236e2fc1":"markdown","5534a799":"markdown","dcdb4b48":"markdown","33df7c19":"markdown","3974c777":"markdown","afc455de":"markdown","978feae6":"markdown","42c7db13":"markdown","969cb6b2":"markdown","4bcebbfc":"markdown","5b067327":"markdown","935504d4":"markdown","53c4623b":"markdown","618f2b58":"markdown","a02ed9db":"markdown","5d866f79":"markdown","ff138cb9":"markdown","35fedbdd":"markdown","4dcc56ce":"markdown","4d6a6f24":"markdown","2dd0fe24":"markdown","042f17b7":"markdown","232f23f7":"markdown","e3752a6b":"markdown","9c66d0db":"markdown","02bbd063":"markdown","0b9953bc":"markdown","5a384464":"markdown","8fe77827":"markdown","f5a35bb3":"markdown","40f4b6a3":"markdown","4498bdcc":"markdown","8e333cee":"markdown","ce7a45a2":"markdown","a85aa4d0":"markdown","62726188":"markdown","10d3afbf":"markdown","d7dc9a73":"markdown","6ca30af0":"markdown","3bc82854":"markdown"},"source":{"6f94e17a":"#import libraries\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#Read the dataset\ndf = pd.read_csv(\"..\/input\/uplift-modeling\/criteo-uplift-v2.1.csv\")","2acccf4b":"#Show the columns\ndf.columns","db9d4d6b":"#Check for null values\ndf.isnull().sum()","3f7f1c75":"#Quick stats of the variables\ndf.describe()","01938270":"#Boxplots of the features\ndf.drop(['treatment','conversion','visit','exposure'], axis = 1).plot.box(figsize=(10,6))","b11e9b13":"#Correlation Matrix\nf, ax = plt.subplots(figsize=(13, 8))\nplt.title('Pearson Correlation Matrix',fontsize=25)\n\ncorr = df.corr()\n\nmask = np.zeros_like(corr, dtype=np.bool) \nmask[np.triu_indices_from(mask)] = True \n\nsns.heatmap(corr, annot=True, mask = mask)","7e1320de":"#Show the proportion of treatment\nsns.countplot(x='treatment', data=df)\nprint(df['treatment'].value_counts(normalize = True))","1e937a27":"#Proportion of full exposure to the treatment\nsns.countplot(x='exposure', data=df)\nprint(df['exposure'].value_counts(normalize = True))","b299dae5":"#Proportion of conversion\nsns.countplot(x='conversion', data=df)\nprint(df['conversion'].value_counts(normalize = True))","d90c25ac":"#Proportion of visits\nsns.countplot(x='visit', data=df)\nprint(df['visit'].value_counts(normalize = True))","bd0d6af3":"#Given the treatment and exposure how is the conversion?\ntable_conv = df.groupby(['treatment','exposure']).agg({'conversion':['mean','sum','count']})\nprint(table_conv)","915536af":"#Given the treatment and exposure, how is the visits?\ntable_vis = df.groupby(['treatment','exposure']).agg({'visit':['mean','sum','count']})\nprint(table_vis)","aaece2e9":"#Given the treatment, exposure and visits, how is the conversion?\ndf.groupby(['treatment','exposure','visit']).agg({'conversion':['mean','sum','count']})","5ab2d780":"from statsmodels.stats.proportion import proportions_ztest\n\nproportions_ztest(count=table_conv.reset_index()[('conversion')]['sum'].loc[:1],\n                  nobs=table_conv.reset_index()[('conversion')]['count'].loc[:1])[1]","fc304af7":"#Get the two groups of data\nproportions_ztest(count=table_conv.reset_index()[('conversion')]['sum'].drop(1),\n                  nobs=table_conv.reset_index()[('conversion')]['count'].drop(1))[1]","d7eaf239":"#Get the two groups of data\nproportions_ztest(count=table_vis.reset_index()[('visit')]['sum'].loc[:1],\n                  nobs=table_vis.reset_index()[('visit')]['count'].loc[:1])[1]","882ffef6":"#Get the two groups of data\nproportions_ztest(count=table_vis.reset_index()[('visit')]['sum'].drop(1),\n                  nobs=table_vis.reset_index()[('visit')]['count'].drop(1))[1]","5a773928":"#Remove the users that were treated but not exposed\nusers_to_remove = df[(df['treatment'] == 1 ) & (df['exposure']==0)]\n\n#New df based on the objective of the notebook\ndf_ml = df.drop(users_to_remove.index).reset_index().drop(['index','treatment', 'visit'], axis = 1)","6106b0b2":"#Number of Treated but not exposed\nlen(users_to_remove)","63a72486":"#Proportion of Exposure\nsns.countplot(x='exposure', data=df_ml)\nprint(df_ml['exposure'].value_counts())\nprint(\"Normalized:\")\nprint(df_ml['exposure'].value_counts(normalize = True))","6817bc7e":"df_noexp = df_ml[df_ml['exposure'] == 0]\ndf_exp = df_ml[df_ml['exposure'] == 1]\n\nprint(\"Users converted given no exposure\")\nprint(df_noexp['conversion'].value_counts())\n\nprint(\"Users converted given exposure\")\nprint(df_exp['conversion'].value_counts())\n\n#Plot both graphs\nfig, ax = plt.subplots( ncols=2, figsize = (16,4))\n\nsns.countplot(x='conversion', data=df_noexp, ax=ax[0])\nax[0].set_ylabel('Number of users')\nax[0].set_title('Conversion given no exposure')\n\nsns.countplot(x='conversion', data=df_exp, ax=ax[1])\nax[1].set_ylabel('Number of users')\nax[1].set_title('Conversion given exposure')","ef4aea3c":"from sklearn import preprocessing\n\nscaled = preprocessing.scale(df_ml.drop(['conversion','exposure'], axis = 1).values)\ndf_sca = pd.DataFrame(data=scaled, columns = df_ml.drop(['conversion','exposure'], axis = 1).columns)\ndf_exposure = df_ml['exposure']\ndf_conversion = df_ml['conversion']","a0cc67d0":"from sklearn.model_selection import train_test_split\n\nstratify_cols = pd.concat([df_exposure, df_conversion], axis=1)\n\n#Setting up testing and traning sets\nX_train, X_test, trmnt_train , trmnt_test, y_train, y_test = train_test_split(df_sca,\n                                                                             df_exposure,\n                                                                             df_conversion,\n                                                                             stratify = stratify_cols,\n                                                                             test_size = 0.1,\n                                                                             random_state=42)\n\nprint(f\"Train shape: {X_train.shape}\")\nprint(f\"Test shape: {X_test.shape}\")","71cf3a3b":"#Selection of instances only exposed\ny_traincml = y_train[trmnt_train==1]\nX_traincml = X_train[trmnt_train==1]","22f679fa":"#Imbalanced in the conversion variable\nsns.countplot(x='conversion', data=y_traincml.to_frame())\nprint(y_traincml.value_counts())\nprint(\"Normalized:\")\nprint(y_traincml.value_counts(normalize = True))","8c59c747":"#Setting up testing and training sets\nX_train_cml, X_val_cml, y_train_cml, y_val_cml = train_test_split(X_traincml,y_traincml, test_size=0.3, random_state=42)\n\nprint('Train instances: {}, None_Conversion: {} and Conversion: {}'.format(len(y_train_cml),\n                                                                         y_train_cml.value_counts()[0],\n                                                                         y_train_cml.value_counts()[1]))\n\nprint('Val instances: {}, None_Conversion: {} and Conversion: {}'.format( len(y_val_cml), \n                                                                         y_val_cml.value_counts()[0],\n                                                                         y_val_cml.value_counts()[1]))","65446f53":"#SMOTE\nfrom imblearn.over_sampling import SMOTE \n\nsm = SMOTE(random_state=42 )\nX_train_sm, y_train_sm = sm.fit_resample(X_train_cml, y_train_cml)\n\nprint(y_train_sm.value_counts())","1ab5cdd8":"from sklearn.dummy import DummyClassifier\nimport xgboost as xgb\nfrom sklearn.metrics import confusion_matrix, recall_score, precision_score\nfrom sklearn.linear_model import LogisticRegression","4fa7600a":"#DummyClassifiert to predict only target 0\ndummy = DummyClassifier(strategy='uniform', random_state=42).fit(X_train_sm, y_train_sm)\ndummy_pred = dummy.predict(X_val_cml.values)\n\nprint(\"Precision: \", precision_score(y_val_cml, dummy_pred))\nprint(\"Recall: \", recall_score(y_val_cml, dummy_pred))","b7cc71fb":"#Logistic regression model\nlogreg_sm = LogisticRegression().fit(X_train_sm.values, y_train_sm.values)\nlogreg_sm_pred = logreg_sm.predict(X_val_cml.values)\n\nprint(\"Precision: \", precision_score(y_val_cml, logreg_sm_pred))\nprint(\"Recall: \", recall_score(y_val_cml, logreg_sm_pred))","b760a3bd":"#Extreme Gradient Boosting\nxgb_sm = xgb.XGBClassifier(objective = 'binary:logistic', eval_metric = 'aucpr',  use_label_encoder = False).fit(X_train_sm.values,\n                                                                                      y_train_sm.values)\nxgb_sm_pred = xgb_sm.predict(X_val_cml.values)\n\nprint(\"Precision: \", precision_score(y_val_cml, xgb_sm_pred))\nprint(\"Recall: \", recall_score(y_val_cml, xgb_sm_pred))","865b63ab":"values = {'tp':49, 'fp':1, 'fn':49}\n\ndef value_eq(values, conf_matrix): \n    \"\"\"\n    Function that computes the value generated by the model\n    \n    Parameters:\n    -----------\n    values: dict with the values of the outcomes\n    conf_matrix: confusion matrix of the models\n    \"\"\"\n    tp = conf_matrix[1,1]\n    fp = conf_matrix[0,1]\n    fn = conf_matrix[1,0]\n    \n    return tp*values['tp'] - fp*values['fp'] - fn*values['fn']","e2832457":"#Confusion Matrix of Random Pred\ncm_rand_sm = confusion_matrix(y_val_cml, dummy_pred)\n\n#Confusion Matrix of Logistic Regression\ncm_logreg_sm = confusion_matrix(y_val_cml, logreg_sm_pred)\n\n#Confusion Matrix of XGB\ncm_xgb_sm = confusion_matrix(y_val_cml, xgb_sm_pred)\n\n#Print the benefit from both models\nprint(f'The profit in USD with the Random model is $ {value_eq(values,cm_rand_sm):,.2f}')\nprint(f'The profit in USD with the Logistic Regression model is $ {value_eq(values,cm_logreg_sm):,.2f}')\nprint(f'The profit in USD with XGBoost model is $ {value_eq(values,cm_xgb_sm):,.2f}')","884171be":"def outcome_value(y_pred,y_test,values):\n    \"\"\"\n    Returns the value of the outcome\n    \n    Parameters:\n    -----------\n    y_pred: classes estimated by the classifier\n    y_test: actual classes\n    value: value of the outcome\n    \"\"\"\n    if y_pred == 1 and y_test == 1:\n        return values['tp']\n    if y_pred == 1 and y_test==0:\n        return -values['fp']\n    elif y_pred== 0 and y_test==1:\n        return -values['fn']\n    else:\n        return 0\n    \ndef compute_value_df(y_proba,y_test,values, best_thres= 0.5):\n    \"\"\"\n    Generates a df with the cummulative cost of the errors\n    \n    Parameters:\n    ------------\n    y_proba: array with the probabilities of X_test given by an estimator\n    y_test: array with the actual values of y\n    best_thres: int best threshold to define a binary outcome\n    values: dict with the value of the outcomes\n    \"\"\"\n    #Build the df for the cost curve\n    df_value_curve = pd.DataFrame({'y_proba':y_proba, \n                                         'y_pred': np.where(y_proba > best_thres,1,0),\n                                         'y_test':y_test})\n\n    #Sort df by probability\n    df_value_curve = df_value_curve.sort_values(by='y_proba'\n                                                            , ascending = False).reset_index().drop('index', axis =1)\n\n    #Compute the value of each outcome\n    df_value_curve['value'] = df_value_curve.apply(lambda x: outcome_value(x.y_pred, x.y_test,values), axis=1)\n\n    #Compute the cummulative sum of the outcomes\n    df_value_curve['cum_value'] = df_value_curve['value'].cumsum(axis=0)\n    \n    return df_value_curve","6cda9be3":"#random model \ndummy_proba = dummy.predict_proba(X_val_cml.values)[:,1]\ndf_dummy_val_curve = compute_value_df(dummy_proba,y_val_cml,values)\n\n#Get the probabilities for each instance based on the estimator\nlogreg_y_proba = logreg_sm.predict_proba(X_val_cml.values)[:,1]\ndf_logreg_val_curve = compute_value_df(logreg_y_proba,y_val_cml,values)\n\n#Get the probabilities for each instance based on the estimator\nxgb_y_proba = xgb_sm.predict_proba(X_val_cml.values)[:,1]\ndf_xgboost_val_curve = compute_value_df(xgb_y_proba,y_val_cml,values)","68883d00":"_, axes = plt.subplots(figsize=(10, 6))\n\ninstances=np.arange(0, 1, (1\/len(y_val_cml)))\n\naxes.set_title(\"Profit Curve\")\naxes.set_xlabel(\"Proportion of test instances\")\naxes.set_ylabel(\"Profit $\")\naxes.grid(b=True)\n\naxes.plot(instances, df_dummy_val_curve['cum_value'].values, '-', color=\"red\",label=\"Random Classifier\")\naxes.plot(instances, df_logreg_val_curve['cum_value'].values, '-', color=\"purple\",label=\"Logistic Regression\")\naxes.plot(instances, df_xgboost_val_curve['cum_value'].values, '-', color=\"green\",label=\"XGBoost\")\n\naxes.legend(loc=\"best\")\naxes.set_xlim(0,1)\n#axes.set_ylim(0,1)\nplt.show()","b595b6c9":"stratify_cols = pd.concat([trmnt_train, y_train], axis=1)\n\n#Setting up testing and traning sets\nX_trainup, X_valup, trmnt_trainup , trmnt_valup , y_trainup , y_valup = train_test_split(X_train,\n                                                                                         trmnt_train,\n                                                                                         y_train,\n                                                                                         stratify = stratify_cols,\n                                                                                         test_size = 0.3,\n                                                                                         random_state=42)\n\nprint(f\"Train shape: {X_trainup.shape}\")\nprint(f\"Validation shape: {X_valup.shape}\")","8da94834":"#Imbalanced in the treatment variableo\nsns.countplot(x='exposure', data=trmnt_trainup.to_frame())","71a0df72":"#Subsampling the mayority class (could be done with over-sampling the minority class but it consumes too much resources)\ntmp = X_trainup.copy()\ntmp.insert(len(tmp.columns)-1, 'conversion', y_trainup.values)\ntmp.insert(len(tmp.columns)-1, 'exposure', trmnt_trainup.values)\n\n#separte df based on the treatment\ntmp1 = tmp[tmp['exposure']==1]\ntmp2 = tmp[tmp['exposure']==0]\n\n#Subsampling\ntmp2 = tmp2.sample(n=len(tmp1))\ntmp3 = pd.concat([tmp1,tmp2], axis=0)\n\n#Updating the variables\nX_trainup_sub = tmp3.drop(['exposure','conversion'], axis = 1)\ntrmnt_trainup_sub = tmp3['exposure']\ny_trainup_sub = tmp3['conversion']","c9537fff":"#Checking the umbalance\nsns.countplot(x='exposure', data=trmnt_trainup_sub.to_frame())","1c8d6d87":"!pip install scikit-uplift","ff17980a":"from sklift.models import SoloModel, TwoModels\n\nestimator = LogisticRegression()\n\n#Single Model\nsm = SoloModel(estimator)\nsm = sm.fit(X_trainup_sub, y_trainup_sub, trmnt_trainup_sub)\nuplift_sm = sm.predict(X_valup)\n\n#Two Independent models\ntm = TwoModels(estimator_trmnt= LogisticRegression(), estimator_ctrl= LogisticRegression(), method='vanilla')\ntm = tm.fit(X_trainup_sub, y_trainup_sub, trmnt_trainup_sub)\nuplift_tm = tm.predict(X_valup)","a995c6a9":"def get_tc(treatment,target):\n    \"\"\"\n    Function that returns helps to select the users that were in the control group and have a positive response\n    \n    Parameters:\n    ------------\n    treatment = treatment\n    target = the response \n    \"\"\"    \n    if not target:\n        return 0\n    elif not treatment:\n        return 1\n    else:\n        return 0\n    \ndef compute_gain(df_x, uplift, treatment, y, rnd = False):\n    \"\"\"\n    Function that computes the cumulative gains in a dataframe\n    \n    Parameters:\n    ------------\n    uplift = arr with the predicted uplifts by the models\n    trtment = arr with the treatment indicator (test)\n    y = arr target variable (test)\n    rnd = bool indicating if random selection\n    \"\"\"\n    e = 0.0000000000001 # to avoid divisions by zero\n    base = pd.DataFrame({'uplift':uplift, 'treatment':treatment.values, 'target': y.values})\n\n    df = pd.concat([df_x,base], axis = 1)\n    \n    #sort based on the flag\n    if rnd:\n        df = df.sample(frac=1).reset_index().drop('index', axis =1)\n    else:\n        df = df.sort_values(by='uplift', ascending = False).reset_index().drop('index', axis =1)\n    \n    #Compute the cumulative treatment size\n    df['cum_trs'] = df['treatment'].cumsum(axis=0)\n\n    #Compute the cumulative control size\n    df['cum_crs'] = (df.index+1) - df['treatment'].cumsum(axis=0)\n    \n    #Compute target given treatment\n    df['ttr'] = df['treatment'] * df['target']\n    \n    #Compute target given control\n    df['tcr'] = df.apply(lambda x: get_tc(x.treatment, x.target), axis=1)\n    \n    #Compute cumulative target given treatment\n    df['cum_ttr'] = df['ttr'].cumsum(axis=0)\n    \n    #Compute cumulative target given control\n    df['cum_tcr'] = df['tcr'].cumsum(axis=0)\n    \n    #Compute cumulative gain from one to uplift [descent]\n    df['cum_gain'] = (df['cum_ttr']\/(df['cum_trs']+e)) - (df['cum_tcr']\/(df['cum_crs']+e))\n    \n    #Compute cumulative gain from zero to uplift [ascending]\n    df['cum_gain2'] = df['cum_gain']*(df['cum_trs']+df['cum_crs']+e)\/ len(df)\n    \n    return df   \n    ","6312a61f":"df_x_valup = X_valup.reset_index().drop('index',axis=1)\n\n#Compute gain for the three models\nsm_gain = compute_gain(df_x_valup, uplift_sm,trmnt_valup, y_valup)\ntm_gain = compute_gain(df_x_valup, uplift_tm,trmnt_valup, y_valup)\nrnd_gain = compute_gain(df_x_valup, uplift_tm,trmnt_valup, y_valup, True)","a9d9fc22":"#Store the results for the modeling\n\nsave_df = pd.DataFrame({'gain_twomodel':tm_gain['cum_gain2'] , 'gain_random':rnd_gain['cum_gain2']})\n\n#save_df.to_csv(\"C:\/Users\/Milara\/Desktop\/New Projects\/uplift_results.csv\", encoding=\"ISO-8859-1\", index= False)","d0b577e8":"#Plot the results\n_, axes = plt.subplots(figsize=(15, 7))\n\ninstances=np.arange(0, 1, (1\/len(y_valup)))\n\naxes.set_title(\"Uplift Curve\")\naxes.set_xlabel(\"Proportion of users targeted\")\naxes.set_ylabel(\"Cumulative Uplift\")\naxes.grid(b=True)\naxes.plot(instances, sm_gain['cum_gain2'].values, '-', color=\"r\",label=\"Single model\")\naxes.plot(instances, tm_gain['cum_gain2'].values, '-', color=\"b\",label=\"Two models\")\naxes.plot(instances, rnd_gain['cum_gain2'].values, '-', color=\"k\",label=\"Random model\")\naxes.legend(loc=\"best\")\n#axes.set_xlim(0,0.5)\n#axes.set_ylim(0,0.5)\nplt.show()","e979edcb":"cut1 = int(len(tm_gain)*0.2)\ncut2 = int(len(tm_gain)*0.6)\n\ntm_gain_variables = tm_gain[['f0','f1','f2','f3','f4','f5','f6','f7','f8','f9','f10','f11']]\n\nprofile_1 = tm_gain_variables.iloc[:cut1]\nprofile_2 = tm_gain_variables.iloc[cut1+1:cut2]\nprofile_3 = tm_gain_variables.iloc[cut2+1:]","da51afef":"import plotly.graph_objects as go\n\ncategories = profile_1.columns\n\n#Plot \nfig = go.Figure()\n\nfig.add_trace(go.Scatterpolar(\n        r= profile_1.median().to_list(),\n        theta=categories,\n        fill='toself',\n        name='Profile_1\\'s area'\n))\n\nfig.add_trace(go.Scatterpolar(\n        r= profile_2.median().to_list(),\n        theta=categories,\n        fill='toself',\n        name='Profile_2\\'s area'\n))\n\nfig.add_trace(go.Scatterpolar(\n        r= profile_3.median().to_list(),\n        theta=categories,\n        fill='toself',\n        name='Profile_3\\'s area'\n))\n\nfig.show()","ed6b409c":"In the image above shows the following:\n + The variables that are most different from the first profile to the others are *f8* and *f9* .\n + The variable that is most different from the second profile to the others is *f0*.\n + The variable that is most different from the third profile to the others is *f2*.","fb40b3d1":"### **CM: Profit\/Cost curve**\n\nAnother technique to measure the performance of the models is using a profit\/cost curve. With this technique the users are sorted by the probability of conversion provided the models and then they are compared to the ground truth (whether they bought or not). Consecutively, the cummulative profit is calculated. This technique is very helpful when the desire is to get the highest profit but the budget for targeting is not enough to target all of the users. In this sense, it is possible to target the k users with highest probability of conversion. **[3]**","54adf1b3":"From the table of above, it can be seen the following:\n+ The percentage of users who were not given the treatment (nor fully exposed) and converted is 0.19%.\n+ The percentage of users who were given the treatment but were not fully exposed and converted is 0.12%\n+ The percentage of users who were given the treatment and fully exposed and converted is 5.38%\n**It seems that it is only worth giving the full exposure instead of giving it partially.**","89da9062":"#### **Data scaling**\n\nThis step is performed to make all the variables have the mean of zero and standard deviation of 1. Some models from sklearn require it because their computations are very sensitive to the range of data points. Specifically the models that work with gradient descent. Both models used in this notebook use gradient descent.","2fca0932":"## **References**\n\n+ [1] The Criteo AI Lab, \"Uplift Modeling , Marketing Campaign Data\", https:\/\/www.kaggle.com\/arashnic\/uplift-modeling\n+ [2] Scikit-uplift,\"Uplift vs other models\", https:\/\/www.uplift-modeling.com\/en\/latest\/user_guide\/introduction\/comparison.html\n+ [3] Provost,F and Fawcett, Tom. (2013). Data science-what you need to know about analytic-thinking and decision-making. USA: O'RELLY.\n+ [4] Kane, K., Lo, V. S., und Zheng. J., 2014. Mining for the Truly Responsive Customers and Prospects Using True-Lift Modeling: Comparison of New and Existing Methods. Journal of Marketing Analytics, 82(4), pp. 218-238.\n+ [5] Scikit-uplift, \"Causal Inference: Basics\", https:\/\/www.uplift-modeling.com\/en\/latest\/user_guide\/introduction\/cate.html\n+ [6] Scikit-uplift, \"Single model approaches\", https:\/\/www.uplift-modeling.com\/en\/latest\/user_guide\/models\/solo_model.html\n+ [7] Scikit-uplift, \"Two models approaches\",https:\/\/www.uplift-modeling.com\/en\/latest\/user_guide\/models\/two_models.html\n+ [8] AMBIATA,\"How uplift modeling works\", https:\/\/ambiata.com\/blog\/2020-07-07-uplift-modeling\/\n+ [9] Susan Currie Sivek Ph.D, \"Beyond Churn: An Introduction to Uplift Modeling\", https:\/\/towardsdatascience.com\/beyond-churn-an-introduction-to-uplift-modeling-d1d9af7be\n+ [10] Shelbey Temple, \"Uplift Modeling: A Quick Introduction\", https:\/\/towardsdatascience.com\/a-quick-uplift-modeling-introduction-6e14de32bfe0\n+ [11] The Data Lab,\"Understanding Customer Behaviour Using Uplift Modelling\", https:\/\/www.thedatalab.com\/tech-blog\/understanding-customer-behaviour-using-uplift-modelling\/\n+ [12] Floris Devriendt, Jeroen Berrevoets, Wouter Verbeke, Why you should stop predicting customer churn and start using uplift models, Information Sciences (2019), doi: https:\/\/doi.org\/10.1016\/j.ins.2019.12.075","2eae8afd":"Plot the profit\/cost curves for all the models","af926392":"**In summary:**\n+ There are 12 features and 2 variables for treatment and 2 variables for target\n+ There are no null values in the 13M users\n+ The features have outliers, positive and negative skweness and different range of values\n+ Some of the features have a strong correlation between them\n    + There were no features with an absolute score correlation above 0.31 for conversion\n    + There were two feautures with an absolute score correlation above 0.31 for visit\n+ **It seems that feature engineering should be performed to:**\n    + Reduce\/remove the outliers\n    + Correct the skewness of some features\n    + Remove variables with strong correlation\n    + Scale the values to have the same range for the sake of the models\n    \n+ However, the feature engineering will be reduced to only scaling of the values. A large feature engineering is out of the scope of the notebook. But it will be intersting to apply a complete feature engineering to improve the performance of the models.","9db08658":"#### *Test between control and treatment group on visit*","66269137":"## **Problem understanding**\n\n+ The data was obtained from Kaggle **\"Uplift Modeling, Marketing and Campaing Data\" provided by AI lab of Criteo** (French advertising company that provides online display adverstisements). **The data contains 13 million instances from a randomized control trial collected in two weeks**, where 84.6% of the users where sent the treatment **[1]**. \n+ Each instance has 12 features that were anonymized plus a treatment variable and two target variables (visits and conversion). There is another extra variable called \"exposure\" which indicates whether the user was effectively exposed to the treatment.\n+ As mentioned before, there are two target variables (visits and conversion), **this notebook will only focus on the conversion variable**. Which can be understood as the indicator whether the user bought the product. \n+ **The goal is to generate a model that can identify users that are more likely to convert (or buy the product) and avoid the ones that are not.**\n+ Before the modeling approaches, it is necessary to select the data that is useful for the models. \n+ For the sake of simplicity two models will be used in this notebook. The models are Logistic Regression and XGBoost. The first model is selected because it is normally used as the baseline. And, the second model is selected because it is known that can provide pretty good results but it is complex, it requires more resources to train, more hyperparameter fine-tunning and it is not easy interpretable. ","89d8a4f3":"The variables have 13M instances each. The stats values of the features seems to be similar. To have a better picture a boxplot is shown below.","40ddeff6":"#### *Test between control and exposure group on visit*","1e9ed2a2":"### **CM: Resampling**\n\nThe proportion of the target variable is imbalanced, thus the over-sampling technique SMOTE is used to increase the number of conversion users to have half of the size of the non_conversion. Note that other ratio could be used.","64377ee6":"### **Imbalance**\n\nThe data is highly imbalanced in four features: treatment, exposure, visits and conversion.","83fe2391":"<a id='section'><\/a>","b7cfa637":"## **Data understanding**","b2a82a00":"## **Classical Modeling**\n\nIn this section the profile of the users is used to create a model that can identify new users with high probabilty of conversion if targeted. This is called the classical modeling because it uses data labeled with the ground truth. The benefit of this type of modeling is that it helps you to avoid the users who will not convert if targeted. **Thus, the data to use is only the group of users who were targeted (\"exposed\").** This approach is also called Response model and Traditional Propensity model **[2,10]**.","19f47b8f":"### **CM: Summary**\n\n+ This section elaborates on the classical modeling approach to identify users that most likely will convert if a targeted campaing is sent. The procedure followed was to scale and resample the data, generate three models and finally compare the models using a profit\/cost technique. Other techniques to improve the model results such as outlier removal, feature selection and hyperparameter tunning were not used due to scope of the analysis. The same case for the evaluation with cross-validation. \n+ **However, this approach has a pitfall. How do we know if a user who received the campaing could have been converted if the user hadn't received it? If this is possible, then the model is causing the company to lose money by targeting users that would convert without being targeted. To address this pitfall the Uplift modeling should be used instead.**","69231369":"There are 12 features and 4 variables with information about the treatments and targets","ae0f16b3":"The graph above shows the following:\n+ The x axis is the proportion of data used by the company to send the targeted marketing\n+ The y axis is the benefit obtained for targeting the users\n+ **The curves show the cummulative profit of users that were sorted by their probability of being converted. In other words, the first users targeted are the ones with highest probability of being converted**\n+ In order to know the benefit, a thershold for the probability is defined to decide if the user is targeted or not. In this case a threshold of 0.5 was used to determine to target or not, yet this value can be tunned\n\nRegarding the models:\n+ **The Logistic Regression model has a maximum of profit close to 200,000 USD**\n+ The XGBoost model has a maximum around 100,000 USD\n+ The random model never generates a benefit\n\nAs well the graph implies the following:\n+ If 0.2 proportion of users were used to send the marketing campaing, then the best model to use would be Logistic Regression.\n+ **However, if all the users were used to send the marketing campaing, based on the model results, then the best model to use would be Logisict Regression as well, because after using the 0.8 proportion of users the benefit would not be decreasing dramatically such as for the Random and XGBoost models.**\n+ The curves decrease when they fail to detect correctly the users that will convert.\n+ The ideal curve would be the one that it gets to the maximum and after that it does not decreases, because it would mean that it correctly found all the users to be converted and would not be losing money by not targeting users that could be converted if targeted.","d21e53e8":"### **CM: Data preparation**\n\nThe instances that were given the treatment are selected.","4a071dea":"From the table of above, it can be seen the following:\n+ There is no conversion if the users do not visit\n+ The percentage of users who were not given the treatment (nor fully exposed) and visited and converted is 5.07%.\n+ The percentage of users who were given the treatment but were not fully exposed and visited and converted is 3.42%\n+ The percentage of users who were given the treatment, fully exposed and visited and converted is 12.97%\n\n**It seems that even if the users receive the partial treatment and visit there is a drop in the percetage of conversion**\n\nIn brief, there is a negative impact of the partial treatment on the results, either on visits and coversions. Furthermore, it seems that not even making the users visit with the partial treatment can yield higher conversion.","78fc17a1":"As expected the model with the highest profit is the Logisit Regression because it has lesser False Postive and False Negative errors than the other two models.","236e2fc1":"The plot shows the following:\n+ The range of the features is different. \n+ Only features f0 and f2 have no outliers.\n+ The features that have the largest outliers are f6 and f9. \n+ The features with large outliers have them only on one side of the distribution. \n+ Some features such as f3, f4 and f5 seem to have a positive skeweness, while f0, f1, f4, f7 and f9 seem to have negative skewness.","5534a799":"In summary:\n+ If a p-value smaller than 0.05 is considered for the statistical significance, then for all test results there is enough evidence that there is an statistical difference between the proportions.\n+ This is good and bad:\n    + It is good because the treatment fully exposed (*exposure*) has a positive impact on conversions and visits.\n    + It is bad because the treatment not fully exposed has a negative impact on conversions and visits.\n    \n+ This should be addressed by the company. Nevertheless, as scope of this notebooks stated, the analysis continues only for the conversion.\n\n+ **As well, the two treatments (treatment and exposed) can be considered different given their characteristic (partially and fully completed), thus this notebook continues only analyzing the data of the exposed users as a treatment to increase the conversion. From now on the exposure feature will be called treatment.**","dcdb4b48":"#### **Data split**\n\nThe following step is peformed to let a portion of the data be untouched, it will be used if necessary.","33df7c19":"### **UM: Modeling**\n\nIn the following lines of code two types of uplift models are used, the single model and two models. These models are from the scikit package for uplift.","3974c777":"## **Uplift modeling**\n\nThis technique combines Machine Learning with causal inference to determine the impact of the treatment (or an action) on the target variable. In other words, it helps to know if its necessary to treat the user or not. To be clearer, the method is explained with the following table **[4]**:\n\n\n|                    | Not buy if not treated | Buy if not treated |\n|--------------------|------------------------|--------------------|\n| **Buy if treated**     | Persuadables           | Sure things        |\n|**Not buy if treated** | Lost Causes            | Sleeping dogs      |\n\nThe possible outcomes are four:\n+ The customers that will buy only if treated: persuadables\n+ The customers that will buy no matter if treated or not treated: sure things\n+ The customers that will not buy no matter if treated or not treated: lost causes\n+ The customers that will buy only if not treated: sleeping dogs\n\nThe classical modeling only avoids the lost causes while the uplift modeling avoids the sure things, lost causes and sleeping dogs. **Specifically, uplift modeling focuses on the persuadables customers.**\n","afc455de":"*From now on Exposure is the same as Treatment.*","978feae6":"### **UM: Brief explanation**\n\n*The key ideas to take are **[5]** :*\n+ The main concept behind the model is **causal inference** in which the aim is to calculate the difference between the response of a customer given that she received the treatment and response given that she did not receive the treatment . \n+ Unfortunately, to measure this is impossible because a customer can not receive and not receive the treatment, there is no ground truth. Therefore, an estimation is calculated.\n+ The estimation is nothing more that the probability of a customer responding given the treatment minus the probability of the customer responding given no treatment. And the probabilities are provided by the models.\n+ **This difference is the estimation of the Uplift, also known as Conditional Average Treatment.**\n+ The largest the uplift the better to treat the customer.\n+ The estimation has the assumption that the treatment is applied randomly, thus independent of the potential results. \n\n*And regarding the modeling:*\n+ There are different types of models such as the Direct and Meta-Learners.\n    + The Direct models aim to predict the uplift, such as Uplift trees.\n    + The Meta-Learners are a combination of models and data processing techniques to estimate the uplift, such as Class Transformation and Two models.\n+ Each of the model has their own pros, cons and assumptions. For this notebook the models used are:\n    + Single model and Two models.\n+ They are selected because they are easily interpretable. \n+ **The single model:**\n    + A usual model such as Logistic Regression or XGBoost is trained with train data and the feature indicating if the treatment was received or not. The results are computed by estimating the probability of response twice. The first time by using the test set and the treatment indicator as 1, and the second time by using the treatment indicator as 0. Lastly, the two probabilities given by the model are substracted to get the uplift **[6]**.\n+ **The Two models:**\n    +  This is similar as the single but it trains two independent models, the first model is trained with the treated customers and the second model is trained with the non-treated customers. In the end, the test set is used in both models and the results are substracted to get the uplift **[7]**.","42c7db13":"It seems the curve has 3 divisions, during the first uplift, then in the middle and finally at the end when the uplift decreases. Thus, just for fun three profiles are obtained from these 3 parts of the plot. The following plot shows the median values from each one of the profiles","969cb6b2":"The graph above shows the following:\n+ The x axis is the proportion of data used by the company to send the targeted marketing.\n+ The y axis is the uplift obtained for targeting the users.\n+ **The curves show the cummulative profit of users that were sorted by their probability of being converted. In other words, the first users targeted are the ones with highest uplift.**\n\nRegarding the models:\n+ All models get eventually to the 5% of uplift after targeting all the users.\n+ **The objective of the curves is to show that if only 50% of the users are targeted, then it would be wise to use the results of the Two models because it reaches almost 5% of the uplift, while the random selection would only reach around half of the maximum uplift.** The proportion of users can be changed based on the needs of the company.\n+ The results of the single and two models have similar behaviour, but the two models still provide better results from 0.2 to 0.8 proportion of users.\n\nAs well the graph implies the following:\n+ The first part of the curve are the users are the persudables, followed by the sure things and lost causes, and finally the sleeping dogs. \n+ The ideal curve is the one that with the first amount of instances reaches the maximum uplift, then the curve keeps the same level of uplift due to the sure things and lost causes, and finally the curve will decrease at some point due to the sleeping dogs.","4bcebbfc":"There are no null values","5b067327":"There are some variables with strong correlation that should be addressed to avoid repetition of information.","935504d4":"Surprisingly, the XGBoost did not provide the best results. Instead, the Logistic Regression model provided the best results. ","53c4623b":"The table above shows following:\n+ The percentage of users who were not given the treatment (nor exposed) and visited is 3.82%\n+ The percentage of users who were given the treatment but not fully exposed and visited is 3.49%\n+ The percentage of users who were given the full treatment and visited is 41.45%\n\n**It can be seen similar behavior as in conversion, it seems that only giving the full treatment and avoiding sending the partial treatment yield better results.**","618f2b58":"### **Results of the treatments**\n\nBefore moving on to the modeling stage it is highly important to determine if the randomized control trial provided a positive impact to the company. If the results generated a positive impact, then the modeling stage can be done but if the results were negative then there is no point of moving to the modeling stage. Thereby, the following analysis is intended to show the impact of the treatments on the conversions and visits. **In other words, to check if the treatment was useful or not.**\n\n#### **Conversions**\nAs the goal is the conversion of the users, then how the conversion is related to the treatment and exposure?","a02ed9db":"The summary of the rounded percentages is: \n+ The percetanges of **treatment** are: 85% with treatment and 15% without treatment\n+ The percetanges of **exposure** are: 3% with exposure and 97% without exposure\n+ The percentages of **conversion** are: 0.2% with conversion and 99% without conversion\n+ The percentages of **visits** are: 5% with conversion and 95% without conversion\n","5d866f79":"#### **Uni-variate analysis**","ff138cb9":"### **UM: Summary**\n\n+ This section elaborates on the uplift modeling approach to identify persuadables users. The procedure followed was to scale and resample the data, generate 3 models and finally compare the models using a Cumulative uplift curve. Other techniques to improve the model results such as outlier removal, feature selection and hyperparameter tunning were not used due to scope of the analysis. The same case for the evaluation with cross-validation. \n+ The results show that by using the uplift modeling the gains on the uplift can be doubled for a certain amount of targeted users.\n+ **A disadvatage of this approach is that the ground truth is not know because a customer can not receive and not receive a treatment (promotion). Thus, the uplift model has no error to be compared such as Classical modeling, meaning that the outputs generated by both approached cannot be compared with the same metrics. [11]**\n+ Nevertheless, there are some research articles proposing different metrics to assess both models in terms of profit and costs. But this is not in the scope of this notebook. \n","35fedbdd":"## **To try**\nThis notenbook provided an overview of the models and how to use them in an easly manner. Yet there still plenty of room for improvement such as:\n+ More Feature Engineering (outlier and correlation removal, correction of the skeness of the features distributions)\n+ Hyperparameter tunning\n+ Validation with Cross-validation \n+ Check of learning and over-fitting curves\n+ Using more Machine Learning models\n+ Try metrics from literature to compare the performance between the Classical and Uplift modeling with the hold-out data generated at the beginning of the notebook\n+ Analyse the most important features\n+ Use other packages causalML (from Uber) and pylift.","4dcc56ce":"### **UM: Cumulative uplift curve**\nOne of the principal methods to assess the performance of the uplift models is using the cumulative gain plot. The plot is similar to the profit\/cost curve in the sense that the instances are sorted based on the results of the models. But they are different in the vertical axis, one shows the cumulative profit and the other the cumulative uplift. The following functions are used to compute the cumulative uplfit based on **[8,9]**. ","4d6a6f24":"#### *Test between control and exposure group on conversion*","2dd0fe24":"### **UM: Data split**\n\nThe data is split in two train and test, but it also is necessary to split the treatment variable. ","042f17b7":"### **UM: Resampling**\n\nThe amount of data is huge, over 1 million of instances, more than the used for the Classical modeling because here the model is using the targeted and not targeted users. Nevertheless, the imbalance is notorious. Therefore, the mayority class is sub-sampled to have the same number of instances as the minority class. Recall that the models work better if they are trained with balanced classes. Notwithstanding, in the case of uplift the models the variable that is balanced is the treatment instead of the target. Because some of the models work with the assumption that the control and treatment groups have the same size. But it would be interersting to try also balancing the target variable.","232f23f7":"Plot the balance treatment feature","e3752a6b":"### **Data preparation: data cleansing to have users only exposed and not exposed.**\n\nGiven the analysis of the previous section, it is necessary to select only the data useful for the modeling","9c66d0db":"The results of the modeling in the test set are used to compute the cumulative uplift ","02bbd063":"### **CM: Data split**\n\nThe data is divided in train and validation tests.","0b9953bc":"#### **Visits**\n\nIf the visits were the goal, how are they related to the treatment and exposure?","5a384464":"This could be also be done with over-sampling the minority class, but as the instances are over a million, it requires more comptuer resources and time to make the computations. ","8fe77827":"#### **Bonus: How do the treatment, exposure and visit impact the conversion?**","f5a35bb3":"#### **Multi-variate analysis**","40f4b6a3":"Computation of the profits","4498bdcc":"### **Basic EDA**","8e333cee":"## **Conclusion_**\n\nThis notebook elaborated two approaches for Targeted Marketing. It started by selecting the information that is worth used by the models. Then, the first approach called **\"Classical modeling\"** was presented, which takes only the targeted users (from a randomize experiment for targeted marketing) to train the models. Then, the models were used to determine the users that if targeted will convert. Also, the technique \"Profit\/Cost curve\" was used to compare the performance of the models in terms of money. In the following part of the notebook the **\"Uplift modeling\"** was shown. The models were trained with the results of a randomized experiment for targeted marketing. Then, the results were used to determine the users that if targeted  will provide the highest uplift. Finally, a the Cumulative Uplift curve was used to compare the results of the models.\n\nThe downside of the Classical modeling approach it only avoids the lost causes. On the other side, the Uplift modeling is trained to identify firstly the persuadable users. **Both approaches will provide benefit to the companies that used them instead of a randomized targeting, the only difference is that the Uplift modeling will reduce the cost of targeting users that do not need it and the losses for customers that will buy if not targeted.**","ce7a45a2":"# **Uplift Modeling**\n\n*by Milara*\n\n*September 2021*\n\n**Introduction:**\n+ One of the main applications of the Machine Learning models is to improve the **Targeted Marketing**. The Targeted Marketing is used to select the customers that most likely buy a product.\n+ There are different approaches for Targeted Marketing such as the **Classical modeling** (also known as Response model). This approach is focused in training a model with customers that were sent a promotion\/offer. The model separates the customers that will buy from the ones that will not buy if targeted. **This approach generates higher profit than random selection targeting.** \n+ Nonetheless, the Classical modeling has a flaw. It does not separate customers that will buy even if not targeted from the ones that will only buy if targeted. In other words, the model wastes money by targeting customers that do not need it. The current solution to that issue is the Uplift modeling.\n+ The **Uplift modeling** can separate customers that will buy if not targeted from the ones that will buy only if targeted, as well as avoiding customers that will not buy even if targeted. **Specifically, the model identifies the customers that are worth spending money on Targeted Marketing.** \n+ This approach can also be related to the **Churn problem**, in which the goal is to avoid losing customers. It is related because it requires to send an offer to persuade the customer that the product still provide value. Therefore, it is important to send an offer to the customers that are more likely to stay if targeted. \n+ **This notebook elaborates these two approaches for Targeted Marketing using a dataset with 13 million instances provided by Criteo (French advertising company)**.\n\n\n**Objectives**\n+ To show the Classical and Uplift modeling in Targeted Marketing\n\n**Content**\n+ [Problem understanding](#Problem-understanding)\n+ [Data understanding](#Data-understanding)\n    + [Basic EDA](#Basic-EDA)\n    + [Imbalance](#Imbalance)\n    + [Results of the treatments](#Results-of-the-treatments)\n    + [Data preparation](#Data-preparation)\n+ [Classical modeling](#Classical-Modeling)\n    + [CM: Data preparation](#CM:-Data-preparation)\n    + [CM: Data split](#CM:-Data-split)\n    + [CM: Resampling](#CM:-Resampling)\n    + [CM: Modeling](#CM:-Modeling)\n    + [CM: Profit\/Cost of the results](#CM:-Profit\/Cost-of-the-results)\n    + [CM: Profit\/Cost curve](#CM:-Profit\/Cost-curve)\n    + [CM: Summary](#CM:-Summary)\n+ [Uplift modeling](#Uplift-modeling)\n    + [UM: Brief explanation](#UM:-Brief-explanation)\n    + [UM: Data split](#UM:-Data-split)\n    + [UM: Resampling](#UM:-Resampling)\n    + [UM: Modeling](#UM:-Modeling)\n    + [UM: Cumulative uplift curve](#UM:-Cumulative-uplift-curve)\n    + [UM: Summary](#UM:-Summary)\n+ [Conclusion](#Conclusion_)\n+ [To try](#To-try)\n+ [References](#References)\n","a85aa4d0":"Even that a vast number of instaces were removed the imbalancing remains. Note that from having 13M instances only around 400k have the characteristics needed for this stage.","62726188":"### **CM: Profit\/Cost of the results**\n\n**Rather than only using technical metrics to know if the models are doing it well or not such as Precision and Recall, another method that takes into account the profits generated by the models is used. Similar to the method proposed by Provost. [3]** \n\nThe model has four possible outcomes: True Positive (correctly targeted), False Positive (incorrectly targeted), True Negative (correctly not targeted) and False Negative (incorrectly not targeted). Based on the business it is necessary to define a benefit and cost for each one of them. Thus, lets assume the following: the cost of sending the campaing (or promotion) per user is 1 USD and if the user converts then the benefit is 50 USD. Another assumption, is that if the user is not targeted then the benefit is zero (which in real scenarios this may not happen but this is outside of the scope of this notebook).\n\n+ True Positives (TP): the benefit of targeting and conversion is 50-1 = 49 USD\n+ True Negatives (TN): the benefit of not targeting is 0 \n+ False Positives (FP): the cost of targeting when there is no conversion is 1 USD\n+ False Negatives (FN): the cost of not targeting to a user who would have been converted is 49 USD\n\nTherefore the profit equation is the following:\n+ $ profit = TP*49 - FP*1 - FN*49$\n\n**Note: the value of each outcome of the model can be changed based on the business information.** ","10d3afbf":"### **CM: Modeling**\n\nThree models are used, the first model is random (to have a baseline), the second is Logistic Regression and the third is XGBoost. As the target variable is imbalanced all the models are evaluated on precision and recall to focus on False Positives and False Negatives. An important note is that all of the the models are trained on balanced data to learn how to generalize and then evaluated on imbalanced  data.","d7dc9a73":"The results are plotted","6ca30af0":"### **Statistical tests for the treatments on results**\n\nIt is important to make a robust comparasion between the proportions of the results given the treatments. Therefore, statiscal tests are carried on.\n\n#### *Test between control and treatment group on conversion*","3bc82854":"Calculation of the cummulative profit "}}