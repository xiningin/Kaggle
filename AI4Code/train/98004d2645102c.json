{"cell_type":{"812a3031":"code","2955860c":"code","3b2b28e2":"code","df34939e":"code","052b087a":"code","91906bc8":"code","b2a601c0":"code","e1526ccd":"code","5ee262e7":"code","c6b742ab":"code","065a65c9":"code","b0b5427d":"code","da2399fb":"code","9a6f4e38":"code","e8a73e3c":"code","c9a75a7a":"code","9fe6ad3a":"code","27843a15":"code","48d931b2":"code","47acd47e":"code","92dddad5":"code","7cbc5b1e":"code","6f925d68":"code","9d4210b5":"code","203366c3":"code","ef8e8396":"code","849f90fc":"code","dd0e8e07":"code","14625875":"code","7f2f3138":"code","a6082be5":"code","5d3a175a":"markdown","d068a86d":"markdown","b4a59dee":"markdown","70f1d90d":"markdown","7113f06e":"markdown","c6d8ed6b":"markdown","98d11c14":"markdown","5e4d5fc7":"markdown","041006cf":"markdown","6a068a2d":"markdown"},"source":{"812a3031":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nimport math\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2955860c":"df_train = pd.read_csv('\/kaggle\/input\/GiveMeSomeCredit\/cs-training.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/GiveMeSomeCredit\/cs-test.csv')\ndf_train.head()","3b2b28e2":"df_train.info()","df34939e":"null_val_sums = df_train.isnull().sum()","052b087a":"pd.DataFrame({'Column': null_val_sums.index, 'Number of Null Values': null_val_sums.values, 'Proportions': null_val_sums.values\/len(df_train)})","91906bc8":"df_train = df_train.fillna(df_train.median())\nprint(df_train.isnull().sum())","b2a601c0":"sns.countplot(x='SeriousDlqin2yrs', data=df_train)\nprint('Default Rate: {}'.format(df_train['SeriousDlqin2yrs'].sum()\/len(df_train)))","e1526ccd":"age_bins = [-math.inf, 25, 40, 50, 60, 70, math.inf]\ndependent_bin = [-math.inf,2,4,6,8,10,math.inf]\ndpd_bins = [-math.inf,1,2,3,4,5,6,7,8,9,math.inf]\ndf_train['bin_age'] = pd.cut(df_train['age'],bins=age_bins).astype(str)\ndf_train['bin_NumberOfDependents'] = pd.cut(df_train['NumberOfDependents'],bins=dependent_bin).astype(str)\ndf_train['bin_NumberOfTimes90DaysLate'] = pd.cut(df_train['NumberOfTimes90DaysLate'],bins=dpd_bins)\ndf_train['bin_NumberOfTime30-59DaysPastDueNotWorse'] = pd.cut(df_train['NumberOfTime30-59DaysPastDueNotWorse'], bins=dpd_bins)\ndf_train['bin_NumberOfTime60-89DaysPastDueNotWorse'] = pd.cut(df_train['NumberOfTime60-89DaysPastDueNotWorse'], bins=dpd_bins)\n\n\ndf_train['bin_RevolvingUtilizationOfUnsecuredLines'] = pd.qcut(df_train['RevolvingUtilizationOfUnsecuredLines'],q=5,duplicates='drop').astype(str)\ndf_train['bin_DebtRatio'] = pd.qcut(df_train['DebtRatio'],q=5,duplicates='drop').astype(str)\ndf_train['bin_MonthlyIncome'] = pd.qcut(df_train['MonthlyIncome'],q=5,duplicates='drop').astype(str)\ndf_train['bin_NumberOfOpenCreditLinesAndLoans'] = pd.qcut(df_train['NumberOfOpenCreditLinesAndLoans'],q=5,duplicates='drop').astype(str)\ndf_train['bin_NumberRealEstateLoansOrLines'] = pd.qcut(df_train['NumberRealEstateLoansOrLines'],q=5,duplicates='drop').astype(str)","5ee262e7":"bin_cols = [c for c in df_train.columns.values if c.startswith('bin_')]","c6b742ab":"df_train.head()","065a65c9":"def calculate_IV(df_train, feature, target):\n    crstab = pd.crosstab(df_train[feature], df_train[target], normalize = 'columns')\n    \n    #crstab['Log_a_b'] = np.log(crstab[0]) - np.log(crstab[1])\n    crstab['WOE'] =   np.log(crstab[crstab.columns[0]]\/crstab[crstab.columns[1]]) \n    crstab = crstab.replace([np.inf, -np.inf], np.nan).dropna(axis=0)\n    crstab['diff_0_1'] =  crstab[crstab.columns[0]] - crstab[crstab.columns[1]]\n    crstab['IV_i'] = crstab['WOE']*crstab['diff_0_1']\n    IV = sum(crstab['IV_i'])\n    # print(crstab)\n    return IV\n\n# recalculate IV 1 4\n# f = bin_cols[2]\n# calculate_IV(df_train,f,'SeriousDlqin2yrs')","b0b5427d":"# recalculate IV\nIV_list_ = []\nfor f in bin_cols:\n#     IV_list_.append([f,cal_IV(df_train,f,'SeriousDlqin2yrs')])\n     IV_list_.append([f,calculate_IV(df_train,f,'SeriousDlqin2yrs')])\nIV_data_ = pd.DataFrame(IV_list_, columns=['features','IV'])\nprint(IV_data_)","da2399fb":"# We choose only those features with IV>0.1\nfeature_cols = ['RevolvingUtilizationOfUnsecuredLines','NumberOfTime30-59DaysPastDueNotWorse','age','NumberOfTimes90DaysLate','NumberOfTime60-89DaysPastDueNotWorse']\nbin_cols = ['bin_RevolvingUtilizationOfUnsecuredLines','bin_NumberOfTime30-59DaysPastDueNotWorse','bin_age','bin_NumberOfTimes90DaysLate','bin_NumberOfTime60-89DaysPastDueNotWorse']","9a6f4e38":"def cal_WOE(df,features,target):\n    df_new = df\n    for f in features:\n        df_woe = df_new.groupby(f).agg({target:['sum','count']})\n        df_woe.columns = list(map(''.join, df_woe.columns.values))\n        df_woe = df_woe.reset_index()\n        df_woe = df_woe.rename(columns = {target+'sum':'bad'})\n        df_woe = df_woe.rename(columns = {target+'count':'all'})\n        df_woe['good'] = df_woe['all']-df_woe['bad']\n        df_woe = df_woe[[f,'good','bad']]\n        df_woe['bad_rate'] = df_woe['bad']\/df_woe['bad'].sum()\n        df_woe['good_rate'] = df_woe['good']\/df_woe['good'].sum()\n        df_woe['woe'] = df_woe['bad_rate'].divide(df_woe['good_rate'],fill_value=1)\n        df_woe.columns = [c if c==f else c+'_'+f for c in list(df_woe.columns.values)]\n        df_new = df_new.merge(df_woe,on=f,how='left')\n    return df_new\n\ndef WOE(df_train, feature, target):\n    crstab = pd.crosstab(df_train[feature], df_train[target], normalize = 'columns')\n    crstab_ = pd.crosstab(df_train[feature], df_train[target])\n    crstab['woe'] = crstab[crstab.columns[1]].divide(crstab[crstab.columns[0]], fill_value=1) \n    crstab = crstab.replace([np.inf, -np.inf], np.nan).dropna(axis=0)\n    df_woe = pd.DataFrame(crstab.values, index=crstab.index)\n    df_woe.columns = ['good_rate', 'bad_rate', 'woe']\n    df_woe.columns = [st + \"_\" + feature for st in df_woe.columns]\n    \n    return df_woe\n\ndef calculate_WOE(df,features,target):\n    df_new = df\n    for f in features:\n        df_woe = WOE(df_new, f, target)\n        df_new = df_new.merge(df_woe, on = f, how='left')\n    woe_cols = [c for c in df_new.columns if 'woe' in c]\n    # return df_new[woe_cols]\n    return df_new # full data frame\n#print(bin_cols)\n#calculate_WOE(df_train,bin_cols,'SeriousDlqin2yrs')","e8a73e3c":"df_woe = cal_WOE(df_train,bin_cols,'SeriousDlqin2yrs')\ndf_woe.head()\n# woe_cols = [c for c in list(df_woe.columns.values) if 'woe' in c]\n# df_woe[woe_cols]\n\n# my calculation\ndf_woe = calculate_WOE(df_train,bin_cols,'SeriousDlqin2yrs')\nwoe_cols = [c for c in df_woe.columns if 'woe' in c]\ndf_woe[woe_cols].head()","c9a75a7a":"df_bin_to_woe = pd.DataFrame(columns = ['features','bin','woe'])\nfor f in feature_cols:\n    b = 'bin_'+f\n    w = 'woe_bin_'+f\n    df = df_woe[[w,b]].drop_duplicates()\n    df.columns = ['woe','bin']\n    df['features'] = f\n    df=df[['features','bin','woe']]\n    df_bin_to_woe = pd.concat([df_bin_to_woe,df])\ndf_bin_to_woe.head()","9fe6ad3a":"# split data 80\/20 for training and validating\nfrom sklearn.linear_model import LogisticRegression\nX_train, X_test, y_train, y_test = train_test_split(df_woe[woe_cols], df_woe['SeriousDlqin2yrs'], test_size=0.2, random_state=66)\nmodel = LogisticRegression().fit(X_train,y_train)","27843a15":"model.score(X_test,y_test)","48d931b2":"import sklearn.metrics as metrics\n# calculate the fpr and tpr for all thresholds of the classification\nprobs = model.predict_proba(X_test)\n\npreds = probs[:,1]\nfpr, tpr, threshold = metrics.roc_curve(y_test, preds)\nroc_auc = metrics.auc(fpr, tpr)\n\n# method I: plt\nimport matplotlib.pyplot as plt\nplt.title('Receiver Operating Characteristic (ROC)')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()\n\n# K-S value, max(tpr - fpr) to measure the discrimination of predict model\n# >=0.5 is considered good differentiation \nprint(max(tpr - fpr))","47acd47e":"# Confusion matrix\ny_pred = model.predict(X_test)\ncm = metrics.confusion_matrix(y_test, y_pred)\ncm_display = metrics.ConfusionMatrixDisplay(cm).plot()","92dddad5":"model.coef_","7cbc5b1e":"theta_0 = 1\/1\nP_0 = 650 # base score\nPDO = 50 # point of double","6f925d68":"B = PDO\/np.log(2)\nA = P_0+B*np.log(theta_0)","9d4210b5":"def generate_scorecard(model_coef,binning_df,features,B):\n    lst = []\n    cols = ['Variable','Binning','Score']\n    coef = model_coef[0]\n    for i in range(len(features)):\n        f = features[i]\n        df = binning_df[binning_df['features']==f]\n        for index,row in df.iterrows():\n            lst.append([f,row['bin'],int(round(-coef[i]*row['woe']*B))])\n    data = pd.DataFrame(lst, columns=cols)\n    return data","203366c3":"score_card = generate_scorecard(model.coef_,df_bin_to_woe,feature_cols,B)\nscore_card.head()","ef8e8396":"sort_scorecard = score_card.groupby('Variable').apply(lambda x: x.sort_values('Score', ascending=False))\nsort_scorecard.head()","849f90fc":"def str_to_int(s):\n    if s == '-inf':\n        return -999999999.0\n    elif s=='inf':\n        return 999999999.0\n    else:\n        return float(s)\n    \ndef map_value_to_bin(feature_value,feature_to_bin):\n    for idx, row in feature_to_bin.iterrows():\n        bins = str(row['Binning'])\n        left_open = bins[0]==\"(\"\n        right_open = bins[-1]==\")\"\n        binnings = bins[1:-1].split(',')\n        in_range = True\n        # check left bound\n        if left_open:\n            if feature_value<= str_to_int(binnings[0]):\n                in_range = False   \n        else:\n            if feature_value< str_to_int(binnings[0]):\n                in_range = False   \n        #check right bound\n        if right_open:\n            if feature_value>= str_to_int(binnings[1]):\n                in_range = False \n        else:\n            if feature_value> str_to_int(binnings[1]):\n                in_range = False   \n        if in_range:\n            return row['Binning']\n    return null\n\ndef map_to_score(df,score_card):\n    scored_columns = list(score_card['Variable'].unique())\n    score = 0\n    for col in scored_columns:\n        feature_to_bin = score_card[score_card['Variable']==col]\n        feature_value = df[col]\n        selected_bin = map_value_to_bin(feature_value,feature_to_bin)\n        selected_record_in_scorecard = feature_to_bin[feature_to_bin['Binning'] == selected_bin]\n        score += selected_record_in_scorecard['Score'].iloc[0]\n    return score  \n\ndef calculate_score_with_card(df,score_card,A):\n    df['score'] = df.apply(map_to_score,args=(score_card,),axis=1)\n    df['score'] = df['score']+A\n    df['score'] = df['score'].astype(int)\n    return df","dd0e8e07":"good_sample = df_train[df_train['SeriousDlqin2yrs']==0].sample(1000)\ngood_sample = good_sample[feature_cols]\nbad_sample = df_train[df_train['SeriousDlqin2yrs']==1].sample(1000)\nbad_sample = bad_sample[feature_cols]\n\ngood_candidate = calculate_score_with_card(good_sample,score_card,A)\nbad_candidate = calculate_score_with_card(bad_sample,score_card,A)\nres = pd.concat([good_candidate, bad_candidate])\nres.head()","14625875":"def approve_or_not(score):\n    if score < 450:\n        return \"Declined\"\n    elif score >= 450 and score < 650:\n        return \"Pending\"\n    else:\n        return \"Approved\"","7f2f3138":"res['Status'] = res['score'].apply(approve_or_not)\nprint(res['Status'].value_counts())\nfig, axs = plt.subplots(1,2)\naxs[0].hist(res['score']) \naxs[1].hist(res['Status']) \nfig.show()","a6082be5":"pd.crosstab(df_train['SeriousDlqin2yrs'], res['Status'])","5d3a175a":"1000 good and bad candidates are selected each, 1335 were pending.","d068a86d":"# Approve card critera\n### >= 650   Approved\n### 450-650  Pending\n### <450     Declined","b4a59dee":"# Validate score\nSample candiate from training data","70f1d90d":"Used logistc regression rather than advanced NN or xgboost model for credit scoring. \n\nLogistic is intuitively explanable and meets the model requirements under survilence in banks  ","7113f06e":"# Binning","c6d8ed6b":"# Get credit scores\ncredit score is linear transformation of log odds","98d11c14":"# Modeling and Training","5e4d5fc7":"# Features Extracting\nuse IV and WoE for extracting","041006cf":"# Replicate credit scoring.\nusing information value (IV), weight of evidence (WoE) for feature selection, logistic regression for scoring model and reverse back to credit score.","6a068a2d":"# EDA"}}