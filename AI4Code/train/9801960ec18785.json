{"cell_type":{"dea01046":"code","84099f4f":"code","1e82032b":"code","2f352aca":"code","fda52029":"code","48e7e433":"code","79041beb":"code","1b1fca46":"code","3b8cb381":"code","82f4de83":"code","d10d83ab":"code","ea5ba0ac":"code","2f486639":"code","49f58a3d":"code","83091918":"code","1cfbd8d4":"code","68cb62c5":"code","8da73865":"code","35ce8839":"code","73045c36":"code","5fd5d31d":"markdown","8d5ea61c":"markdown","429c3c6f":"markdown","5c7649d9":"markdown","d5c8d51a":"markdown","89eb1f79":"markdown"},"source":{"dea01046":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n# i do like plotly and matplotlib\nimport plotly.express as px\nimport matplotlib.pyplot as plt\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom tqdm import tqdm\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","84099f4f":"df = pd.read_json(\"\/kaggle\/input\/cryptopunks\/txn_history-2021-10-07.jsonl\", lines=True)\nlen(df)","1e82032b":"df['date'] = pd.to_datetime(df.date)","2f352aca":"df.columns","fda52029":"df = df[[\"txn_type\", \"date\", \"eth\", \"punk_id\", \"type\", \"accessories\"]]\ndf.head()","48e7e433":"df = df.explode(\"type\")\ndf.head()","79041beb":"fig = px.bar(df.drop_duplicates(\"punk_id\")['type'].value_counts().rename_axis('type').reset_index(name='counts'),\n             x=\"type\", y=\"counts\", color=\"type\", title=\"Cryptopunk Type Counts\")\nfig.show()\n","1b1fca46":"df.eth.describe()","3b8cb381":"median_price = {}\ni = 0\nall_txns = []\npunk_types = df.type.unique()\ndates = sorted(df.date.unique())\nprev_date = None\nfor date in tqdm(dates):\n    if prev_date:\n        df_up_to = df[(df.date <= date) & (df.date > prev_date)]\n    else:\n        # this only occurs for the first date!\n        df_up_to = df[(df.date <= date)]\n\n    median_price = df_up_to[df.txn_type == \"Sold\"].groupby(\"type\").agg({\"eth\": \"median\"})['eth'].to_dict()\n    for punk_type in punk_types:\n        if punk_type not in median_price:\n            # no median price\/sales, so we include all transactions\n            all_txns.append(df_up_to[df_up_to.type == punk_type])\n        else:\n            # only include transactions that are within at least 10% of the median price\n            all_txns.append(df_up_to[(df_up_to.type == punk_type) & (df.eth >= median_price[punk_type] * .1)])\n\n    prev_date = date\n    \ndf = pd.concat(all_txns)","82f4de83":"len(df)","d10d83ab":"# in case there's any duplicates\n# since accessories is a list, have to include all cols except that\ndf = df.drop_duplicates([\"txn_type\", \"date\", \"eth\", \"punk_id\", \"type\"])\nlen(df)","ea5ba0ac":"max_sold = df[df.txn_type == \"Sold\"].eth.max()\n\n# include all transactions with offers\/bids up to 1.5x larger than largest sale\ndf = df[df.eth <= max_sold * 1.5 ]","2f486639":"fig = px.histogram(df, x=\"eth\", title=\"All Transaction ETH Prices\")\nfig.show()\n","49f58a3d":"dates = df['date'].value_counts().sort_index().rename_axis('date').reset_index(name='counts')\nplt.figure(figsize=(20,10))\nplt.bar(dates['date'], dates['counts'], label=\"All Transactions\")\nplt.legend()\nplt.xticks(rotation=60)\nplt.ylim(0, 1000)\nplt.title(\"Transactions per Day\")\nplt.ylabel(\"Number of Transactions\")\nplt.xlabel(\"Date\")\nplt.show()","83091918":"fig = px.bar(df[df.txn_type == 'Sold'].groupby(\"type\").agg({\"eth\": \"max\"}).sort_values(by=\"eth\").reset_index('type'),\n             x=\"type\", y=\"eth\", color=\"type\", title=\"CryptoPunk Max Sold Price by Type\")\nfig.show()\n","1cfbd8d4":"human = df[(df.txn_type == 'Sold') & ((df.type == \"Female\") | (df.type == \"Male\")) ].groupby(\"date\").agg({\"eth\": [\"median\"]}).reset_index(\"date\")\nalien = df[(df.txn_type == 'Sold') & ((df.type == \"Alien\")) ].groupby(\"date\").agg({\"eth\": [\"median\"]}).reset_index(\"date\")\nzombie = df[(df.txn_type == 'Sold') & ((df.type == \"Zombie\")) ].groupby(\"date\").agg({\"eth\": [\"median\"]}).reset_index(\"date\")\nape = df[(df.txn_type == 'Sold') & ((df.type == \"Ape\")) ].groupby(\"date\").agg({\"eth\": [\"median\"]}).reset_index(\"date\")\nplt.figure(figsize=(20,10))\nplt.plot(human['date'], human['eth']['median'], label=\"Human Median Eth\")\nplt.plot(alien['date'], alien['eth']['median'], label=\"Alien Median Eth\")\nplt.plot(zombie['date'], zombie['eth']['median'], label=\"Zombie Median Eth\")\nplt.plot(ape['date'], ape['eth']['median'], label=\"Ape Median Eth\")\nplt.legend()\nplt.xticks(rotation=60)\nplt.title(\"Median Eth Price for Punks Sold Over Time by Type\")\nplt.show()","68cb62c5":"df['num_attributes'] = df.accessories.apply(lambda x: len(x))","8da73865":"fig = px.bar(df.drop_duplicates(\"punk_id\")['num_attributes'].value_counts().rename_axis('num_attributes').reset_index(name='counts'),\n             x=\"num_attributes\", y=\"counts\", color=\"num_attributes\", title=\"Cryptopunk Distribution of Number of Attributes\")\nfig.show()","35ce8839":"# here are the actual counts\ndf.drop_duplicates(\"punk_id\")['num_attributes'].value_counts()","73045c36":"fig = px.bar(df[(df.txn_type == \"Sold\") & ((df.type == \"Female\") | (df.type == \"Male\"))].groupby(\"num_attributes\").agg({\"eth\": \"mean\"}).reset_index(\"num_attributes\"),\n             x=\"num_attributes\", y=\"eth\", color=\"eth\", title=\"Cryptopunk Price per Number of Attributes of Human Punks Only\")\nfig.show()","5fd5d31d":"There are certainly many more avenues one could meander digging through this data. We hope that this piques your interest and you take a stab at estimating Cryptopunk values!","8d5ea61c":"As we can see most of the punks are human (Female or Male) and they make up **~98%** of all punks!","429c3c6f":"To preprocess the data, we need to exclude \"non-serious\" bids and offers. One way to do that is to exclude bids and offers that are lower than 50% of median sale value for the punk type\n\n**Note:** This takes quite a while to process!","5c7649d9":"The only columns we care about are `txn_type`, `date`, `eth`, `punk_id`, `type`, and `accessories`","d5c8d51a":"Sometimes, NFTs derive extra value by the number of attributes they have (either having none or having more). An interesting tool that's popular in the community is https:\/\/rarity.tools\/. If you are curious, here is the page for [CryptoPunks](https:\/\/rarity.tools\/cryptopunks)","89eb1f79":"# Exploratory Data Analysis of Cryptopunk Transaction History\n\nThis is a sample notebook to get folks familiar with the Cryptopunks dataset! We investigate the distributions of different features, as well as the effect of time on other features. Please leave any feeback\/comments\/questions below! And if you would like to check out price valuations, have a look at [DeepNFTValue](http:\/\/deepnftvalue.com\/), where we are leveraging ML using this very dataset!\n"}}