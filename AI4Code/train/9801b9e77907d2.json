{"cell_type":{"d08413f2":"code","9a2dc27c":"code","b4dc2c75":"code","07c86060":"code","69c025e5":"code","d4fc251f":"code","ead7653b":"code","763f299a":"code","17ac5d89":"code","a28da862":"code","51ffe611":"code","ee62155e":"code","aa09d178":"code","1b57c6ce":"code","de93d0a9":"code","af3c7e78":"code","d20fef7c":"code","6d14b32e":"code","abcf8eee":"code","92d7794d":"code","a4172b92":"code","0264a8ff":"code","28395b26":"code","409a7eee":"code","16dd34e5":"code","75e46e57":"markdown","5859c53c":"markdown","97956126":"markdown","d1525ab3":"markdown","a703cb6e":"markdown","b36f881b":"markdown","c33913cc":"markdown","ce70869a":"markdown","36c299ba":"markdown","ac53d2e5":"markdown","566e7d87":"markdown","594bd33b":"markdown","a34811e4":"markdown","0ba0531c":"markdown","8322cee7":"markdown","b4501a93":"markdown","514ef717":"markdown","ce32ed3c":"markdown","c5689479":"markdown","bac3c0df":"markdown","32f3c00b":"markdown","24af8fae":"markdown"},"source":{"d08413f2":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport copy\nimport datetime\nimport pytz\nimport time\nimport random\n\n# PyTorch stuff\nimport torch\nfrom torch import nn\nfrom torch import optim\nimport torch.nn.functional as F\nfrom torch.utils import data\n\n# Sklearn stuff\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.metrics import roc_auc_score","9a2dc27c":"SEED = 17\n\n# Input data files are available in the \"..\/input\/\" directory.\nPATH_TO_DATA = '..\/input'\nprint(os.listdir(PATH_TO_DATA))","b4dc2c75":"# Train dataset\ndf_train_features = pd.read_csv(os.path.join(PATH_TO_DATA, 'train_features.csv'), \n                                    index_col='match_id_hash')\ndf_train_targets = pd.read_csv(os.path.join(PATH_TO_DATA, 'train_targets.csv'), \n                                   index_col='match_id_hash')\n\n# Test dataset\ndf_test_features = pd.read_csv(os.path.join(PATH_TO_DATA, 'test_features.csv'), \n                                   index_col='match_id_hash')","07c86060":"df_train_features.head()","69c025e5":"# Check if there is missing data\nprint(df_train_features.isnull().values.any())\nprint(df_test_features.isnull().values.any())","d4fc251f":"df_full_features = pd.concat([df_train_features, df_test_features])\n\n# Index to split the training and test data sets\nidx_split = df_train_features.shape[0]\n\n# That is, \n# df_train_features == df_full_features[:idx_split]\n# df_test_features == df_full_features[idx_split:]","ead7653b":"df_full_features.drop(['game_time', 'game_mode', 'lobby_type', 'objectives_len', 'chat_len'],\n                      inplace=True, axis=1)","763f299a":"def write_to_submission_file(predicted_labels):\n    df_submission = pd.DataFrame({'radiant_win_prob': predicted_labels}, \n                                     index=df_test_features.index)\n\n    submission_filename = 'submission_{}.csv'.format(\n        datetime.datetime.now(tz=pytz.timezone('Europe\/Athens')).strftime('%Y-%m-%d_%H-%M-%S'))\n    \n    df_submission.to_csv(submission_filename)\n    \n    print('Submission saved to {}'.format(submission_filename))","17ac5d89":"np.sort(np.unique(df_full_features['r1_hero_id'].values.flatten()))","a28da862":"np.all(df_train_features[[f'{t}{i}_hero_id' for t in ['r', 'd'] for i in range(1, 6)]].nunique(axis=1) == 10)","51ffe611":"for t in ['r', 'd']:\n    for i in range(1, 6):\n        df_full_features = pd.get_dummies(df_full_features, columns = [f'{t}{i}_hero_id'])","ee62155e":"df_full_features_scaled = df_full_features.copy()\ndf_full_features_scaled[df_full_features.columns.tolist()] = MinMaxScaler().fit_transform(df_full_features_scaled[df_full_features.columns.tolist()])  # alternatively use StandardScaler","aa09d178":"df_full_features_scaled.head()","1b57c6ce":"X_train = df_full_features_scaled[:idx_split]\nX_test = df_full_features_scaled[idx_split:]\n\ny_train = df_train_targets['radiant_win'].map({True: 1, False: 0})","de93d0a9":"X_train.head()","af3c7e78":"mlp = nn.Sequential(nn.Linear(6, 4),\n                    nn.ReLU(),\n                    nn.Linear(4, 4),\n                    nn.ReLU(),\n                    nn.Linear(4, 1),\n                    nn.Sigmoid()\n                   )","d20fef7c":"class MLP(nn.Module):\n    ''' Multi-layer perceptron with ReLu and Softmax.\n    \n    Parameters:\n    -----------\n        n_input (int): number of nodes in the input layer \n        n_hidden (int list): list of number of nodes n_hidden[i] in the i-th hidden layer \n        n_output (int):  number of nodes in the output layer \n        drop_p (float): drop-out probability [0, 1]\n        random_state (int): seed for random number generator (use for reproducibility of result)\n    '''\n    def __init__(self, n_input, n_hidden, drop_p, random_state=SEED):\n        super().__init__()   \n        self.random_state = random_state\n        set_random_seed(SEED)\n        self.hidden_layers = nn.ModuleList([nn.Linear(n_input, n_hidden[0])])\n        self.hidden_layers.extend([nn.Linear(h1, h2) for h1, h2 in zip(n_hidden[:-1], n_hidden[1:])])\n        self.output_layer = nn.Linear(n_hidden[-1], 1)       \n        self.dropout = nn.Dropout(p=drop_p)  # method to prevent overfitting\n                \n    def forward(self, X):\n        ''' Forward propagation -- computes output from input X.\n        '''\n        for h in self.hidden_layers:\n            X = F.relu(h(X))\n            X = self.dropout(X)\n        X = self.output_layer(X)\n        return torch.sigmoid(X)\n    \n    def predict_proba(self, X_test):\n        return self.forward(X_test).detach().squeeze(1).cpu().numpy()\n    \n    \n\ndef set_random_seed(rand_seed=SEED):\n    ''' Helper function for setting random seed. Use for reproducibility of results'''\n    if type(rand_seed) == int:\n        torch.backends.cudnn.benchmark = False\n        torch.backends.cudnn.deterministic = True\n        random.seed(rand_seed)\n        np.random.seed(rand_seed)\n        torch.manual_seed(rand_seed)\n        torch.cuda.manual_seed(rand_seed)\n        ","6d14b32e":"def train(model, epochs, criterion, optimizer, scheduler, dataloaders, verbose=False):\n    ''' \n    Train the given model...\n    \n    Parameters:\n    -----------\n        model: model (MLP) to train\n        epochs (int): number of epochs\n        criterion: loss function e.g. BCELoss\n        optimizer: optimizer e.g SGD or Adam \n        scheduler: learning rate scheduler e.g. StepLR\n        dataloaders: train and validation dataloaders\n        verbose (boolean): print training details (elapsed time and losses)\n\n    '''\n    t0_tot = time.time()\n    \n    set_random_seed(model.random_state)\n    \n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f'Training on {device}...')\n    model.to(device)\n    \n    # Best model weights (deepcopy them because model.state_dict() changes during the training)\n    best_model_wts = copy.deepcopy(model.state_dict()) \n    best_loss = np.inf\n    losses = {'train': [], 'valid': []}\n    roc_auc_values = []\n    for epoch in range(epochs): \n        t0 = time.time()\n        print(f'============== Epoch {epoch + 1}\/{epochs} ==============')\n        # Each epoch has a training and validation phase\n        for phase in ['train', 'valid']:\n            if phase == 'train':\n                scheduler.step()\n                if verbose: print(f'lr: {scheduler.get_lr()}')\n                model.train()  # Set model to training mode\n            else:\n                model.eval()   # Set model to evaluate mode\n\n            running_loss = 0.0 \n            for ii, (X_batch, y_batch) in enumerate(dataloaders[phase], start=1):                               \n                # Move input and label tensors to the GPU\n                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n\n                # Reset the gradients because they are accumulated\n                optimizer.zero_grad()\n\n                with torch.set_grad_enabled(phase == 'train'):\n                    outputs = model(X_batch).squeeze(1)  # forward prop\n                    loss = criterion(outputs, y_batch)  # compute loss\n                    if phase == 'train':\n                        loss.backward()  # backward prop\n                        optimizer.step()  # update the parameters\n                        \n                running_loss += loss.item() * X_batch.shape[0]\n                \n            ep_loss = running_loss\/len(dataloaders[phase].dataset)  # average loss over an epoch\n            losses[phase].append(ep_loss)\n            if verbose: print(f' ({phase}) Loss: {ep_loss:.5f}')\n                        \n            # Best model by lowest validation loss\n            if phase == 'valid' and ep_loss < best_loss:\n                best_loss = ep_loss\n                best_model_wts = copy.deepcopy(model.state_dict())      \n                \n        roc_auc = roc_auc_score(dataloaders['valid'].dataset.tensors[1].numpy(),\n                                model.predict_proba(dataloaders['valid'].dataset.tensors[0].to(device)))\n        roc_auc_values.append(roc_auc)\n        if verbose: print('(valid) ROC AUC:', roc_auc)\n        if verbose: print(f'\\nElapsed time: {round(time.time() - t0, 3)} sec\\n')\n        \n    print(f'\\nTraining completed in {round(time.time() - t0_tot, 3)} sec')\n    \n    # Load the best model weights to the trained model\n    model.load_state_dict(best_model_wts)\n    model.losses = losses   \n    model.roc_auc_values = roc_auc_values\n    model.to('cpu')\n    model.eval()\n    return model\n\n\ndef plot_losses(train_losses, val_losses):\n    y = [train_losses, val_losses]\n    c = ['C7', 'C9']\n    labels = ['Train loss', 'Validation loss']\n    # Plot train_losses and val_losses wrt epoch\n    fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n    x = list(range(1, len(train_losses)+1))\n    for i in range(2):\n        ax.plot(x, y[i], lw=3, label=labels[i], color=c[i])\n        ax.set_xlabel('Epoch', fontsize=16)\n        ax.set_ylabel('Loss', fontsize=16)\n        ax.set_xticks(range(0, x[-1]+1, 2))  \n        ax.legend(loc='best')\n    plt.tight_layout()\n    plt.show()\n        \n        \ndef plot_roc_auc(roc_auc_values):\n    fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n    x = list(range(1, len(roc_auc_values)+1))\n    ax.plot(x, roc_auc_values, lw=3, color='C6')\n    ax.set_xlabel('Epoch', fontsize=16)\n    ax.set_ylabel('ROC AUC', fontsize=16)\n    ax.set_xticks(range(0, x[-1]+1, 2))  \n    plt.tight_layout()\n    plt.show()","abcf8eee":"# Perform a train\/validation split\nX_train_part, X_valid, y_train_part, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=SEED)\n\n# Convert to pytorch tensors\nX_train_tensor = torch.from_numpy(X_train_part.values).float()\nX_valid_tensor = torch.from_numpy(X_valid.values).float()\ny_train_tensor = torch.from_numpy(y_train_part.values).float()\ny_valid_tensor = torch.from_numpy(y_valid.values).float()\nX_test_tensor = torch.from_numpy(X_test.values).float()\n\n# Create the train and validation dataloaders\ntrain_dataset = data.TensorDataset(X_train_tensor, y_train_tensor)\nvalid_dataset = data.TensorDataset(X_valid_tensor, y_valid_tensor)\n\ndataloaders = {'train': data.DataLoader(train_dataset, batch_size=1000, shuffle=True, num_workers=2), \n               'valid': data.DataLoader(valid_dataset, batch_size=1000, shuffle=False, num_workers=2)}","92d7794d":"mlp = MLP(n_input=X_train.shape[1], n_hidden=[200, 100], drop_p=0.4)\n\ncriterion = nn.BCELoss()  # Binary cross entropy\noptimizer = optim.Adam(mlp.parameters(), lr=0.01, weight_decay=0.005)  # alternatevily torch.optim.SGD\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n\nepochs = 15\ntrain(mlp, epochs, criterion, optimizer, scheduler, dataloaders, verbose=True)","a4172b92":"plot_losses(mlp.losses['train'], mlp.losses['valid'])","0264a8ff":"plot_roc_auc(mlp.roc_auc_values)","28395b26":"print(f'Best ROC AUC: {roc_auc_score(y_valid.values, mlp.predict_proba(X_valid_tensor))}')","409a7eee":"# Save\ntorch.save(mlp.state_dict(), 'mlp.pth')\n\n# Load\nmlp =  MLP(n_input=X_train.shape[1], n_hidden=[200, 100], drop_p=0.4)\nmlp.load_state_dict(torch.load('mlp.pth'))\nmlp.eval()","16dd34e5":"mlp_pred = mlp.predict_proba(X_test_tensor)\n\nwrite_to_submission_file(mlp_pred)","75e46e57":"Finally let's scale all the player-features","5859c53c":"# Things to do\n- Feature engineering. Create new features from the given ones (e.g. `radiant_total_gold - dire_total_gold` etc.), exctract features from the provided json files, perhaps remove some features.\n- Hyperparameter tunning. Try to find optimal hyperparameters such as number of hidden layers and nodes, learning rate, number of epochs, optimizer and scheduler parameters etc.","97956126":"Helper function for writing to submission file","d1525ab3":"Also, no hero duplicates are allowed in the same match. Here's a simple check","a703cb6e":"# Multilayer Perceptron\n\n<img src='https:\/\/www.vaetas.cz\/img\/machine-learning\/multilayer-perceptron.png' >\n\nLet's build a feedforward neural network \u2013 a multilayer perceptron (MLP for short).  Pytorch provides a convinient and easy way to do it with the help of the [nn.Sequential](https:\/\/pytorch.org\/docs\/stable\/nn.html#torch.nn.Sequential) class, which is a sequential container of different modules (nn.Module\u2019s) \u2013 building blocks of a neural network. Below is a simple MLP with one input layer (4 nodes), two hidden layers (4 nodes each) with ReLU activation, and an output layer with Sigmoid activation (1 node, for a binary classification problem).","b36f881b":"Drop the game-features","c33913cc":"Let's construct X and y arrays.","ce70869a":"Clearly `hero_id` is a categorical feature so let's one-hot encode it for each player (though this might not be the best option since it introduces 1150 new features -- see this [Kernel](https:\/\/www.kaggle.com\/kuzand\/bag-of-heroes-logistic-regression) for a better approach).","36c299ba":"# Description of the player-features\n\nThere are 245 different features in the dataset. It might be daunting at first, however there are 10 different players (2 teams with 5 players in each), and there are only 24 unique player-features for each player (which gives us 240 player-features in total). The remaining 5 features are general game-features which I don't use here.\n\nThe description of each player-feature is given in the following table (source: dota2.gamepedia.com). This might be helpfull for people who have never played Dota 2 like me.\n\n\n|  Feature  | Description |\n| ------------- |:-------------| \n| **hero_id** | ID of player's hero (int64). [Heroes](https:\/\/dota2.gamepedia.com\/Heroes) are the essential element of Dota 2, as the course of the match is dependent on their intervention. During a match, two opposing teams select five out of 117 heroes that accumulate experience and gold to grow stronger and gain new abilities in order to destroy the opponent's Ancient. Most heroes have a distinct role that defines how they affect the battlefield, though many heroes can perform multiple roles. A hero's appearance can be modified with equipment.|\n| **kills** | Number of killed players (int64).|\n| **deaths** | Number of deaths of the player (int64).|\n| **gold** | Amount of gold (int64). [Gold](https:\/\/dota2.gamepedia.com\/Gold) is the currency used to buy items or instantly revive your hero. Gold can be earned from killing heroes, creeps, or buildings. |\n| **xp** | Experience points (int64). [Experience](https:\/\/dota2.gamepedia.com\/Experience) is an element heroes can gather by killing enemy units, or being present as enemy units get killed. On its own, experience does nothing, but when accumulated, it increases the hero's level, so that they grow more powerful.   |\n| **lh** | Number of last hits (int64). [Last-hitting](https:\/\/dota2.gamepedia.com\/Creep_control_techniques#Last-hitting) is a technique where you (or a creep under your control) get the 'last hit' on a neutral creep, enemy lane creep, or enemy hero. The hero that dealt the killing blow to the enemy unit will be awarded a bounty.|\n| **denies** | Number of denies (int64). [Denying](https:\/\/dota2.gamepedia.com\/Denying) is the act of preventing enemy heroes from getting the last hit on a friendly unit by last hitting the unit oneself. Enemies earn reduced experience if the denied unit is not controlled by a player, and no experience if it is a player controlled unit. Enemies gain no gold from any denied unit. |\n| **assists** | Number of [assists](https:\/\/dota2.gamepedia.com\/Gold#Assists_.28AoE_gold.29) (int64). Allied heroes within 1300 radius of a killed enemy, including the killer, receive experience and reliable gold if they assisted in the kill. To qualify for an assist, the allied hero merely has to be within the given radius of the dying enemy hero. |\n| **health** | Health points (int64). [Health](https:\/\/dota2.gamepedia.com\/Health) represents the life force of a unit. When a unit's current health reaches 0, it dies. Every hero has a base health pool of 200. This value exists for all heroes and cannot be altered. This means that a hero's maximum health cannot drop below 200. |\n| **max_health** | Hero's maximum health pool (int64).|\n| **max_mana** | Hero's maximum mana pool (float64). [Mana](https:\/\/dota2.gamepedia.com\/Mana) represents the magic power of a unit. It is used as a cost for the majority of active and even some passive abilities. Every hero has a base mana pool of 75, while most non-hero units only have a set mana pool if they have abilities which require mana, with a few exceptions. These values cannot be altered. This means that a hero's maximum mana cannot drop below 75. |\n| **level** | [Level](https:\/\/dota2.gamepedia.com\/Experience#Leveling) of player's hero (int64). Each hero begins at level 1, with one free ability point to spend. Heroes may level up by acquiring certain amounts of experience. Upon leveling up, the hero's attributes increase by fixed amounts (unique for each hero), which makes them overall more powerful. Heroes may also gain more ability points by leveling up, allowing them to learn new spells, or to improve an already learned spell, making it more powerful. Heroes can gain a total for 24 levels, resulting in level 25 as the highest possible level a hero can reach. |\n| **x** | Player's X coordinate (int64) |\n| **y** | Player's Y coordinate (int64) |\n| **stuns** | Total stun duration of all stuns (float64). [Stun](https:\/\/dota2.gamepedia.com\/Stun) is a status effect that completely locks down affected units, disabling almost all of its capabilities. |\n| **creeps_stacked** | Number of stacked creeps (int64). [Creep Stacking](https:\/\/dota2.gamepedia.com\/Creep_Stacking) is the process of drawing neutral creeps away from their camps in order to increase the number of units in an area. By pulling neutral creeps beyond their camp boundaries, the game will generate a new set of creeps for the player to interact with in addition to any remaining creeps. This is incredibly time efficient, since it effectively increases the amount of gold available for a team. |\n| **camps_stacked** | Number of stacked camps  (int64). |\n| **rune_pickups** | Number of picked up [runes](https:\/\/dota2.gamepedia.com\/Runes)  (int64).  |\n| **firstblood_claimed** | boolean feature? (int64) |\n| **teamfight_participation** |  Team fight participation rate? (float64) |\n| **towers_killed** | Number of killed\/destroyed Towers (int64). [Towers](https:\/\/dota2.gamepedia.com\/Buildings#Towers) are the main line of defense for both teams, attacking any non-neutral enemy that gets within their range. Both factions have all three lanes guarded by three towers each. Additionally, each faction's Ancient have two towers as well, resulting in a total of 11 towers per faction. Towers come in 4 different tiers. |\n| **roshans_killed** | Number of killed Roshans  (int64). [Roshan](https:\/\/dota2.gamepedia.com\/Roshan) is the most powerful neutral creep in Dota 2. It is the first unit which spawns, right as the match is loaded. During the early to mid game, he easily outmatches almost every hero in one-on-one combat. Very few heroes can take him on alone during the mid-game. Even in the late game, lots of heroes struggle fighting him one on one, since Roshan grows stronger as time passes. |\n| **obs_placed** | Number of observer-wards placed by a player (int64). [Observer Ward](https:\/\/dota2.gamepedia.com\/Observer_Ward), an invisible watcher that gives ground vision in a 1600 radius to your team. Lasts 6 minutes. |\n| **sen_placed** | Number of sentry-wards placed by a player (int64) [Sentry Ward](https:\/\/dota2.gamepedia.com\/Sentry_Ward), an invisible watcher that grants True Sight, the ability to see invisible enemy units and wards, to any existing allied vision within a radius. Lasts 6 minutes.|\n\n**Note**: I am not sure about the meaning of some features: `stuns`, `firstblood_claimed` and `teamfight_participation`. Also, the number of towers killed by a team in a few cases is 12, whereas according to wiki the number of towers of each team is 11.\nPlease correct me if I am wrong in the comments and help to clarrify the meaning of these features.","ac53d2e5":"Finally, let's make predictions on the test dataset and write them to a submission file.","566e7d87":"Let's combine train and test datasets in one dataset. This allows for addding new features for both datasets at the same time.","594bd33b":"## Training and making predictions\nLet's construct a MLP with 2 hidden layers and train it.","a34811e4":"<center> <img src='http:\/\/www.thumbnailtemplates.com\/images\/thumbs\/thumb-099-dota-2-2.jpg'>","0ba0531c":"## Data loaders\n\nPyTorch provides [tools](https:\/\/pytorch.org\/docs\/stable\/data.html#torch.utils.data.DataLoader) for loading data to a model in parallel using multiprocessing workers. It also allows batching and shuffling the data. Let's create dataloaders for our training and validation datasets.","8322cee7":"## Preprocess features\nNote that according to wiki currently there are 117 heroes, however in our dataset there are 115 heroes with ids `1, ... 23, 25, ..., 114, 119, 120`.","b4501a93":"# Load data","514ef717":"Plot the train and validation losses w.r.t. epochs to see if both are decreasing. Keep in mind that when the training loss is decreasing while the valid loss is increasing it's a sign of overfiting, so perhaps try to tune regularization hyperparameters such as dropout probability and the optimizer's weight_decay.","ce32ed3c":"Let's create a function for training our MLP and a function for plotting training and validation losses with respect to the epochs.","c5689479":"# Dota 2 winner prediction\n    \nIn this Kernel I build a simple multilayer neural network (multilayer perceptron) in Pytorch to predict which team (Radiant or Dire) will win a Dota 2 match.","bac3c0df":"It's a pretty good result obtained after just ~15 seconds of training... \n\nPytorch provides an easy way of [saving and loading](https:\/\/pytorch.org\/tutorials\/beginner\/saving_loading_models.html) our trained model (it's state-dictionary with all the learned weights and biases), so that we don't have to train  it from the beginning each time we want to use it for inference.","32f3c00b":"# Import modules","24af8fae":"Instead of using `nn.Sequential`, let's define our own MLP class which will allow us to build a MLP just by passing any number of hidden layers and nodes."}}